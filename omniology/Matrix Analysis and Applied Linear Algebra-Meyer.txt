
Contents
Preface .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ix
1.
Linear Equations
. . . . . . . . . . . . . . 1
1.1
Introduction
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1
1.2
Gaussian Elimination and Matrices
.
.
.
.
.
.
.
.
3
1.3
Gaussâ€“Jordan Method .
.
.
.
.
.
.
.
.
.
.
.
.
. 15
1.4
Two-Point Boundary Value Problems
.
.
.
.
.
.
. 18
1.5
Making Gaussian Elimination Work .
.
.
.
.
.
.
. 21
1.6
Ill-Conditioned Systems
.
.
.
.
.
.
.
.
.
.
.
.
. 33
2.
Rectangular Systems and Echelon Forms
. . .
41
2.1
Row Echelon Form and Rank .
.
.
.
.
.
.
.
.
.
. 41
2.2
Reduced Row Echelon Form
.
.
.
.
.
.
.
.
.
.
. 47
2.3
Consistency of Linear Systems
.
.
.
.
.
.
.
.
.
. 53
2.4
Homogeneous Systems .
.
.
.
.
.
.
.
.
.
.
.
.
. 57
2.5
Nonhomogeneous Systems
.
.
.
.
.
.
.
.
.
.
.
. 64
2.6
Electrical Circuits .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 73
3.
Matrix Algebra . . . . . . . . . . . . . .
79
3.1
From Ancient China to Arthur Cayley .
.
.
.
.
.
. 79
3.2
Addition and Transposition
.
.
.
.
.
.
.
.
.
.
. 81
3.3
Linearity .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 89
3.4
Why Do It This Way
.
.
.
.
.
.
.
.
.
.
.
.
.
. 93
3.5
Matrix Multiplication
.
.
.
.
.
.
.
.
.
.
.
.
.
. 95
3.6
Properties of Matrix Multiplication
.
.
.
.
.
.
.
105
3.7
Matrix Inversion
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
115
3.8
Inverses of Sums and Sensitivity
.
.
.
.
.
.
.
.
124
3.9
Elementary Matrices and Equivalence
.
.
.
.
.
.
131
3.10
The LU Factorization
.
.
.
.
.
.
.
.
.
.
.
.
.
141
4.
Vector Spaces . . . . . . . . . . . . . . . 159
4.1
Spaces and Subspaces
.
.
.
.
.
.
.
.
.
.
.
.
.
159
4.2
Four Fundamental Subspaces .
.
.
.
.
.
.
.
.
.
169
4.3
Linear Independence
.
.
.
.
.
.
.
.
.
.
.
.
.
181
4.4
Basis and Dimension
.
.
.
.
.
.
.
.
.
.
.
.
.
194

vi
Contents
4.5
More about Rank .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
210
4.6
Classical Least Squares
.
.
.
.
.
.
.
.
.
.
.
.
223
4.7
Linear Transformations
.
.
.
.
.
.
.
.
.
.
.
.
238
4.8
Change of Basis and Similarity
.
.
.
.
.
.
.
.
.
251
4.9
Invariant Subspaces .
.
.
.
.
.
.
.
.
.
.
.
.
.
259
5.
Norms, Inner Products, and Orthogonality
. . 269
5.1
Vector Norms
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
269
5.2
Matrix Norms
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
279
5.3
Inner-Product Spaces
.
.
.
.
.
.
.
.
.
.
.
.
.
286
5.4
Orthogonal Vectors
.
.
.
.
.
.
.
.
.
.
.
.
.
.
294
5.5
Gramâ€“Schmidt Procedure
.
.
.
.
.
.
.
.
.
.
.
307
5.6
Unitary and Orthogonal Matrices .
.
.
.
.
.
.
.
320
5.7
Orthogonal Reduction .
.
.
.
.
.
.
.
.
.
.
.
.
341
5.8
Discrete Fourier Transform .
.
.
.
.
.
.
.
.
.
.
356
5.9
Complementary Subspaces .
.
.
.
.
.
.
.
.
.
.
383
5.10
Range-Nullspace Decomposition
.
.
.
.
.
.
.
.
394
5.11
Orthogonal Decomposition .
.
.
.
.
.
.
.
.
.
.
403
5.12
Singular Value Decomposition
.
.
.
.
.
.
.
.
.
411
5.13
Orthogonal Projection .
.
.
.
.
.
.
.
.
.
.
.
.
429
5.14
Why Least Squares? .
.
.
.
.
.
.
.
.
.
.
.
.
.
446
5.15
Angles between Subspaces
.
.
.
.
.
.
.
.
.
.
.
450
6.
Determinants . . . . . . . . . . . . . . . 459
6.1
Determinants .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
459
6.2
Additional Properties of Determinants .
.
.
.
.
.
475
7.
Eigenvalues and Eigenvectors . . . . . . . . 489
7.1
Elementary Properties of Eigensystems
.
.
.
.
.
489
7.2
Diagonalization by Similarity Transformations
.
.
505
7.3
Functions of Diagonalizable Matrices
.
.
.
.
.
.
525
7.4
Systems of Diï¬€erential Equations
.
.
.
.
.
.
.
.
541
7.5
Normal Matrices
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
547
7.6
Positive Deï¬nite Matrices
.
.
.
.
.
.
.
.
.
.
.
558
7.7
Nilpotent Matrices and Jordan Structure
.
.
.
.
574
7.8
Jordan Form
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
587
7.9
Functions of Nondiagonalizable Matrices .
.
.
.
.
599

Contents
vii
7.10
Diï¬€erence Equations, Limits, and Summability
.
.
616
7.11
Minimum Polynomials and Krylov Methods
.
.
.
642
8.
Perronâ€“Frobenius Theory
. . . . . . . . . 661
8.1
Introduction
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
661
8.2
Positive Matrices
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
663
8.3
Nonnegative Matrices
.
.
.
.
.
.
.
.
.
.
.
.
.
670
8.4
Stochastic Matrices and Markov Chains
.
.
.
.
.
687
Index
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
705

Preface
Scaffolding
Reacting to criticism concerning the lack of motivation in his writings,
Gauss remarked that architects of great cathedrals do not obscure the beauty
of their work by leaving the scaï¬€olding in place after the construction has been
completed. His philosophy epitomized the formal presentation and teaching of
mathematics throughout the nineteenth and twentieth centuries, and it is still
commonly found in mid-to-upper-level mathematics textbooks. The inherent ef-
ï¬ciency and natural beauty of mathematics are compromised by straying too far
from Gaussâ€™s viewpoint. But, as with most things in life, appreciation is gen-
erally preceded by some understanding seasoned with a bit of maturity, and in
mathematics this comes from seeing some of the scaï¬€olding.
Purpose, Gap, and Challenge
The purpose of this text is to present the contemporary theory and applica-
tions of linear algebra to university students studying mathematics, engineering,
or applied science at the postcalculus level. Because linear algebra is usually en-
countered between basic problem solving courses such as calculus or diï¬€erential
equations and more advanced courses that require students to cope with mathe-
matical rigors, the challenge in teaching applied linear algebra is to expose some
of the scaï¬€olding while conditioning students to appreciate the utility and beauty
of the subject. Eï¬€ectively meeting this challenge and bridging the inherent gaps
between basic and more advanced mathematics are primary goals of this book.
Rigor and Formalism
To reveal portions of the scaï¬€olding, narratives, examples, and summaries
are used in place of the formal deï¬nitionâ€“theoremâ€“proof development. But while
well-chosen examples can be more eï¬€ective in promoting understanding than
rigorous proofs, and while precious classroom minutes cannot be squandered on
theoretical details, I believe that all scientiï¬cally oriented students should be
exposed to some degree of mathematical thought, logic, and rigor. And if logic
and rigor are to reside anywhere, they have to be in the textbook. So even when
logic and rigor are not the primary thrust, they are always available. Formal
deï¬nitionâ€“theoremâ€“proof designations are not used, but deï¬nitions, theorems,
and proofs nevertheless exist, and they become evident as a studentâ€™s maturity
increases. A signiï¬cant eï¬€ort is made to present a linear development that avoids
forward references, circular arguments, and dependence on prior knowledge of the
subject. This results in some ineï¬ƒcienciesâ€”e.g., the matrix 2-norm is presented

x
Preface
before eigenvalues or singular values are thoroughly discussed. To compensate,
I try to provide enough â€œwiggle roomâ€ so that an instructor can temper the
ineï¬ƒciencies by tailoring the approach to the studentsâ€™ prior background.
Comprehensiveness and Flexibility
A rather comprehensive treatment of linear algebra and its applications is
presented and, consequently, the book is not meant to be devoured cover-to-cover
in a typical one-semester course. However, the presentation is structured to pro-
vide ï¬‚exibility in topic selection so that the text can be easily adapted to meet
the demands of diï¬€erent course outlines without suï¬€ering breaks in continuity.
Each section contains basic material paired with straightforward explanations,
examples, and exercises. But every section also contains a degree of depth coupled
with thought-provoking examples and exercises that can take interested students
to a higher level. The exercises are formulated not only to make a student think
about material from a current section, but they are designed also to pave the way
for ideas in future sections in a smooth and often transparent manner. The text
accommodates a variety of presentation levels by allowing instructors to select
sections, discussions, examples, and exercises of appropriate sophistication. For
example, traditional one-semester undergraduate courses can be taught from the
basic material in Chapter 1 (Linear Equations); Chapter 2 (Rectangular Systems
and Echelon Forms); Chapter 3 (Matrix Algebra); Chapter 4 (Vector Spaces);
Chapter 5 (Norms, Inner Products, and Orthogonality); Chapter 6 (Determi-
nants); and Chapter 7 (Eigenvalues and Eigenvectors). The level of the course
and the degree of rigor are controlled by the selection and depth of coverage in
the latter sections of Chapters 4, 5, and 7. An upper-level course might consist
of a quick review of Chapters 1, 2, and 3 followed by a more in-depth treatment
of Chapters 4, 5, and 7. For courses containing advanced undergraduate or grad-
uate students, the focus can be on material in the latter sections of Chapters 4,
5, 7, and Chapter 8 (Perronâ€“Frobenius Theory of Nonnegative Matrices). A rich
two-semester course can be taught by using the text in its entirety.
What Does â€œAppliedâ€ Mean?
Most people agree that linear algebra is at the heart of applied science, but
there are divergent views concerning what â€œapplied linear algebraâ€ really means;
the academicianâ€™s perspective is not always the same as that of the practitioner.
In a poll conducted by SIAM in preparation for one of the triannual SIAM con-
ferences on applied linear algebra, a diverse group of internationally recognized
scientiï¬c corporations and government laboratories was asked how linear algebra
ï¬nds application in their missions. The overwhelming response was that the pri-
mary use of linear algebra in applied industrial and laboratory work involves the
development, analysis, and implementation of numerical algorithms along with
some discrete and statistical modeling. The applications in this book tend to
reï¬‚ect this realization. While most of the popular â€œacademicâ€ applications are
included, and â€œapplicationsâ€ to other areas of mathematics are honestly treated,

Preface
xi
there is an emphasis on numerical issues designed to prepare students to use
linear algebra in scientiï¬c environments outside the classroom.
Computing Projects
Computing projects help solidify concepts, and I include many exercises
that can be incorporated into a laboratory setting. But my goal is to write a
mathematics text that can last, so I donâ€™t muddy the development by marrying
the material to a particular computer package or language. I am old enough
to remember what happened to the FORTRAN- and APL-based calculus and
linear algebra texts that came to market in the 1970s. I provide instructors with a
ï¬‚exible environment that allows for an ancillary computing laboratory in which
any number of popular packages and lab manuals can be used in conjunction
with the material in the text.
History
Finally, I believe that revealing only the scaï¬€olding without teaching some-
thing about the scientiï¬c architects who erected it deprives students of an im-
portant part of their mathematical heritage. It also tends to dehumanize mathe-
matics, which is the epitome of human endeavor. Consequently, I make an eï¬€ort
to say things (sometimes very human things that are not always complimentary)
about the lives of the people who contributed to the development and applica-
tions of linear algebra. But, as I came to realize, this is a perilous task because
writing history is frequently an interpretation of facts rather than a statement
of facts. I considered documenting the sources of the historical remarks to help
mitigate the inevitable challenges, but it soon became apparent that the sheer
volume required to do so would skew the direction and ï¬‚avor of the text. I can
only assure the reader that I made an eï¬€ort to be as honest as possible, and
I tried to corroborate â€œfacts.â€ Nevertheless, there were times when interpreta-
tions had to be made, and these were no doubt inï¬‚uenced by my own views and
experiences.
Supplements
Included with this text is a solutions manual and a CD-ROM. The solutions
manual contains the solutions for each exercise given in the book. The solutions
are constructed to be an integral part of the learning process. Rather than just
providing answers, the solutions often contain details and discussions that are
intended to stimulate thought and motivate material in the following sections.
The CD, produced by Vickie Kearn and the people at SIAM, contains the entire
book along with the solutions manual in PDF format. This electronic version
of the text is completely searchable and linked. With a click of the mouse a
student can jump to a referenced page, equation, theorem, deï¬nition, or proof,
and then jump back to the sentence containing the reference, thereby making
learning quite eï¬ƒcient. In addition, the CD contains material that extends his-
torical remarks in the book and brings them to life with a large selection of

xii
Preface
portraits, pictures, attractive graphics, and additional anecdotes. The support-
ing Internet site at MatrixAnalysis.com contains updates, errata, new material,
and additional supplements as they become available.
SIAM
I thank the SIAM organization and the people who constitute it (the in-
frastructure as well as the general membership) for allowing me the honor of
publishing my book under their name. I am dedicated to the goals, philosophy,
and ideals of SIAM, and there is no other company or organization in the world
that I would rather have publish this book. In particular, I am most thankful
to Vickie Kearn, publisher at SIAM, for the conï¬dence, vision, and dedication
she has continually provided, and I am grateful for her patience that allowed
me to write the book that I wanted to write. The talented people on the SIAM
staï¬€went far above and beyond the call of ordinary duty to make this project
special. This group includes Lois Sellers (art and cover design), Michelle Mont-
gomery and Kathleen LeBlanc (promotion and marketing), Marianne Will and
Deborah Poulson (copy for CD-ROM biographies), Laura Helfrich and David
Comdico (design and layout of the CD-ROM), Kelly Cuomo (linking the CD-
ROM), and Kelly Thomas (managing editor for the book). Special thanks goes
to Jean Anderson for her eagle-sharp editorâ€™s eye.
Acknowledgments
This book evolved over a period of several years through many diï¬€erent
courses populated by hundreds of undergraduate and graduate students. To all
my students and colleagues who have oï¬€ered suggestions, corrections, criticisms,
or just moral support, I oï¬€er my heartfelt thanks, and I hope to see as many of
you as possible at some point in the future so that I can convey my feelings to
you in person. I am particularly indebted to Michele Benzi for conversations and
suggestions that led to several improvements. All writers are inï¬‚uenced by people
who have written before them, and for me these writers include (in no particular
order) Gil Strang, Jim Ortega, Charlie Van Loan, Leonid Mirsky, Ben Noble,
Pete Stewart, Gene Golub, Charlie Johnson, Roger Horn, Peter Lancaster, Paul
Halmos, Franz Hohn, Nick Rose, and Richard Bellmanâ€”thanks for lighting the
path. I want to oï¬€er particular thanks to Richard J. Painter and Franklin A.
Graybill, two exceptionally ï¬ne teachers, for giving a rough Colorado farm boy
a chance to pursue his dreams. Finally, neither this book nor anything else I
have done in my career would have been possible without the love, help, and
unwavering support from Bethany, my friend, partner, and wife. Her multiple
readings of the manuscript and suggestions were invaluable. I dedicate this book
to Bethany and our children, Martin and Holly, to our granddaughter, Margaret,
and to the memory of my parents, Carl and Louise Meyer.
Carl D. Meyer
April 19, 2000

CHAPTER 1
Linear
Equations
1.1
INTRODUCTION
A fundamental problem that surfaces in all mathematical sciences is that of
analyzing and solving m algebraic equations in n unknowns. The study of a
system of simultaneous linear equations is in a natural and indivisible alliance
with the study of the rectangular array of numbers deï¬ned by the coeï¬ƒcients of
the equations. This link seems to have been made at the outset.
The earliest recorded analysis of simultaneous equations is found in the
ancient Chinese book Chiu-chang Suan-shu (Nine Chapters on Arithmetic), es-
timated to have been written some time around 200 B.C. In the beginning of
Chapter VIII, there appears a problem of the following form.
Three sheafs of a good crop, two sheafs of a mediocre crop, and
one sheaf of a bad crop are sold for 39 dou. Two sheafs of
good, three mediocre, and one bad are sold for 34 dou; and one
good, two mediocre, and three bad are sold for 26 dou. What is
the price received for each sheaf of a good crop, each sheaf of a
mediocre crop, and each sheaf of a bad crop?
Today, this problem would be formulated as three equations in three un-
knowns by writing
3x + 2y + z = 39,
2x + 3y + z = 34,
x + 2y + 3z = 26,
where x, y, and z represent the price for one sheaf of a good, mediocre, and
bad crop, respectively. The Chinese saw right to the heart of the matter. They
placed the coeï¬ƒcients (represented by colored bamboo rods) of this system in

2
Chapter 1
Linear Equations
a square array on a â€œcounting boardâ€ and then manipulated the lines of the
array according to prescribed rules of thumb. Their counting board techniques
and rules of thumb found their way to Japan and eventually appeared in Europe
with the colored rods having been replaced by numerals and the counting board
replaced by pen and paper. In Europe, the technique became known as Gaussian
elimination in honor of the German mathematician Carl Gauss,
1 whose extensive
use of it popularized the method.
Because this elimination technique is fundamental, we begin the study of
our subject by learning how to apply this method in order to compute solutions
for linear equations. After the computational aspects have been mastered, we
will turn to the more theoretical facets surrounding linear systems.
1
Carl Friedrich Gauss (1777â€“1855) is considered by many to have been the greatest mathemati-
cian who has ever lived, and his astounding career requires several volumes to document. He
was referred to by his peers as the â€œprince of mathematicians.â€ Upon Gaussâ€™s death one of
them wrote that â€œHis mind penetrated into the deepest secrets of numbers, space, and nature;
He measured the course of the stars, the form and forces of the Earth; He carried within himself
the evolution of mathematical sciences of a coming century.â€ History has proven this remark
to be true.

1.2 Gaussian Elimination and Matrices
3
1.2
GAUSSIAN ELIMINATION AND MATRICES
The problem is to calculate, if possible, a common solution for a system of m
linear algebraic equations in n unknowns
a11x1 +
a12x2 + Â· Â· Â· +
a1nxn =
b1,
a21x1 +
a22x2 + Â· Â· Â· +
a2nxn =
b2,
...
am1x1 + am2x2 + Â· Â· Â· + amnxn = bm,
where the xi â€™s are the unknowns and the aij â€™s and the bi â€™s are known constants.
The aij â€™s are called the coeï¬ƒcients of the system, and the set of bi â€™s is referred
to as the right-hand side of the system. For any such system, there are exactly
three possibilities for the set of solutions.
Three Possibilities
â€¢
UNIQUE SOLUTION:
There is one and only one set of values
for the xi â€™s that satisï¬es all equations simultaneously.
â€¢
NO SOLUTION:
There is no set of values for the xi â€™s that
satisï¬es all equations simultaneouslyâ€”the solution set is empty.
â€¢
INFINITELY MANY SOLUTIONS:
There are inï¬nitely
many diï¬€erent sets of values for the xi â€™s that satisfy all equations
simultaneously. It is not diï¬ƒcult to prove that if a system has more
than one solution, then it has inï¬nitely many solutions. For example,
it is impossible for a system to have exactly two diï¬€erent solutions.
Part of the job in dealing with a linear system is to decide which one of these
three possibilities is true. The other part of the task is to compute the solution
if it is unique or to describe the set of all solutions if there are many solutions.
Gaussian elimination is a tool that can be used to accomplish all of these goals.
Gaussian elimination is a methodical process of systematically transform-
ing one system into another simpler, but equivalent, system (two systems are
called equivalent if they possess equal solution sets) by successively eliminating
unknowns and eventually arriving at a system that is easily solvable. The elimi-
nation process relies on three simple operations by which to transform one system
to another equivalent system. To describe these operations, let Ek denote the
kth equation
Ek :
ak1x1 + ak2x2 + Â· Â· Â· + aknxn = bk

4
Chapter 1
Linear Equations
and write the system as
S =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
E1
E2
...
Em
ï£¼
ï£´
ï£´
ï£½
ï£´
ï£´
ï£¾
.
For a linear system S , each of the following three elementary operations
results in an equivalent system Sâ€².
(1)
Interchange the ith and jth equations. That is, if
S =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
E1
...
Ei
...
Ej
...
Em
ï£¼
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£½
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£¾
,
then
Sâ€² =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
E1
...
Ej
...
Ei
...
Em
ï£¼
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£½
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£¾
.
(1.2.1)
(2)
Replace the ith equation by a nonzero multiple of itself. That is,
Sâ€² =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
E1
...
Î±Ei
...
Em
ï£¼
ï£´
ï£´
ï£´
ï£´
ï£´
ï£½
ï£´
ï£´
ï£´
ï£´
ï£´
ï£¾
,
where Î± Ì¸= 0.
(1.2.2)
(3)
Replace the jth equation by a combination of itself plus a multiple of
the ith equation. That is,
Sâ€² =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
E1
...
Ei
...
Ej + Î±Ei
...
Em
ï£¼
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£½
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£¾
.
(1.2.3)

1.2 Gaussian Elimination and Matrices
5
Providing explanations for why each of these operations cannot change the
solution set is left as an exercise.
The most common problem encountered in practice is the one in which there
are n equations as well as n unknownsâ€”called a square systemâ€”for which
there is a unique solution. Since Gaussian elimination is straightforward for this
case, we begin here and later discuss the other possibilities. What follows is a
detailed description of Gaussian elimination as applied to the following simple
(but typical) square system:
2x +
y + z =
1,
6x + 2y + z = âˆ’1,
âˆ’2x + 2y + z =
7.
(1.2.4)
At each step, the strategy is to focus on one position, called the pivot po-
sition, and to eliminate all terms below this position using the three elementary
operations. The coeï¬ƒcient in the pivot position is called a pivotal element (or
simply a pivot), while the equation in which the pivot lies is referred to as the
pivotal equation. Only nonzero numbers are allowed to be pivots. If a coef-
ï¬cient in a pivot position is ever 0, then the pivotal equation is interchanged
with an equation below the pivotal equation to produce a nonzero pivot. (This is
always possible for square systems possessing a unique solution.) Unless it is 0,
the ï¬rst coeï¬ƒcient of the ï¬rst equation is taken as the ï¬rst pivot. For example,
the circled âƒ
2
in the system below is the pivot for the ï¬rst step:
âƒ
2 x +
y + z =
1,
6x + 2y + z = âˆ’1,
âˆ’2x + 2y + z =
7.
Step 1. Eliminate all terms below the ï¬rst pivot.
â€¢
Subtract three times the ï¬rst equation from the second so as to produce the
equivalent system:
âƒ
2 x +
y +
z =
1,
âˆ’
y âˆ’2z = âˆ’4
(E2 âˆ’3E1),
âˆ’2x + 2y +
z =
7.
â€¢
Add the ï¬rst equation to the third equation to produce the equivalent system:
âƒ
2 x +
y +
z =
1,
âˆ’
y âˆ’2z = âˆ’4,
3y + 2z =
8
(E3 + E1).

6
Chapter 1
Linear Equations
Step 2. Select a new pivot.
â€¢
For the time being, select a new pivot by moving down and to the right.
2 If
this coeï¬ƒcient is not 0, then it is the next pivot. Otherwise, interchange
with an equation below this position so as to bring a nonzero number into
this pivotal position. In our example, âˆ’1 is the second pivot as identiï¬ed
below:
2x +
y +
z =
1,
âƒ
-1 y âˆ’2z = âˆ’4,
3y + 2z =
8.
Step 3. Eliminate all terms below the second pivot.
â€¢
Add three times the second equation to the third equation so as to produce
the equivalent system:
2x +
y +
z =
1,
âƒ
-1 y âˆ’2z = âˆ’4,
âˆ’4z = âˆ’4
(E3 + 3E2).
(1.2.5)
â€¢
In general, at each step you move down and to the right to select the next
pivot, then eliminate all terms below the pivot until you can no longer pro-
ceed. In this example, the third pivot is âˆ’4, but since there is nothing below
the third pivot to eliminate, the process is complete.
At this point, we say that the system has been triangularized. A triangular
system is easily solved by a simple method known as back substitution in which
the last equation is solved for the value of the last unknown and then substituted
back into the penultimate equation, which is in turn solved for the penultimate
unknown, etc., until each unknown has been determined. For our example, solve
the last equation in (1.2.5) to obtain
z = 1.
Substitute z = 1 back into the second equation in (1.2.5) and determine
y = 4 âˆ’2z = 4 âˆ’2(1) = 2.
2
The strategy of selecting pivots in numerical computation is usually a bit more complicated
than simply using the next coeï¬ƒcient that is down and to the right. Use the down-and-right
strategy for now, and later more practical strategies will be discussed.

1.2 Gaussian Elimination and Matrices
7
Finally, substitute z = 1 and y = 2 back into the ï¬rst equation in (1.2.5) to
get
x = 1
2(1 âˆ’y âˆ’z) = 1
2(1 âˆ’2 âˆ’1) = âˆ’1,
which completes the solution.
It should be clear that there is no reason to write down the symbols such
as â€œ x, â€ â€œ y, â€ â€œ z, â€ and â€œ = â€ at each step since we are only manipulating the
coeï¬ƒcients. If such symbols are discarded, then a system of linear equations
reduces to a rectangular array of numbers in which each horizontal line represents
one equation. For example, the system in (1.2.4) reduces to the following array:
ï£«
ï£­
2
1
1
1
6
2
1
âˆ’1
âˆ’2
2
1
7
ï£¶
ï£¸.
(The line emphasizes where = appeared.)
The array of coeï¬ƒcientsâ€”the numbers on the left-hand side of the vertical
lineâ€”is called the coeï¬ƒcient matrix for the system. The entire arrayâ€”the
coeï¬ƒcient matrix augmented by the numbers from the right-hand side of the
systemâ€”is called the augmented matrix associated with the system. If the
coeï¬ƒcient matrix is denoted by A and the right-hand side is denoted by b ,
then the augmented matrix associated with the system is denoted by [A|b].
Formally, a scalar is either a real number or a complex number, and a
matrix is a rectangular array of scalars. It is common practice to use uppercase
boldface letters to denote matrices and to use the corresponding lowercase letters
with two subscripts to denote individual entries in a matrix. For example,
A =
ï£«
ï£¬
ï£¬
ï£­
a11
a12
Â· Â· Â·
a1n
a21
a22
Â· Â· Â·
a2n
...
...
...
...
am1
am2
Â· Â· Â·
amn
ï£¶
ï£·
ï£·
ï£¸.
The ï¬rst subscript on an individual entry in a matrix designates the row (the
horizontal line), and the second subscript denotes the column (the vertical line)
that the entry occupies. For example, if
A =
ï£«
ï£­
2
1
3
4
8
6
5
âˆ’9
âˆ’3
8
3
7
ï£¶
ï£¸,
then
a11 = 2, a12 = 1, . . . , a34 = 7.
(1.2.6)
A submatrix of a given matrix A is an array obtained by deleting any
combination of rows and columns from A. For example, B =

2
4
âˆ’3
7

is a
submatrix of the matrix A in (1.2.6) because B is the result of deleting the
second row and the second and third columns of A.

8
Chapter 1
Linear Equations
Matrix A is said to have shape or size m Ã— n â€”pronounced â€œm by nâ€â€”
whenever A has exactly m rows and n columns. For example, the matrix
in (1.2.6) is a 3 Ã— 4 matrix. By agreement, 1 Ã— 1 matrices are identiï¬ed with
scalars and vice versa. To emphasize that matrix A has shape m Ã— n, subscripts
are sometimes placed on A as AmÃ—n. Whenever m = n (i.e., when A has the
same number of rows as columns), A is called a square matrix. Otherwise, A
is said to be rectangular. Matrices consisting of a single row or a single column
are often called row vectors or column vectors, respectively.
The symbol Aiâˆ—is used to denote the ith row, while Aâˆ—j denotes the jth
column of matrix A . For example, if A is the matrix in (1.2.6), then
A2âˆ—= ( 8
6
5
âˆ’9 )
and
Aâˆ—2 =
ï£«
ï£­
1
6
8
ï£¶
ï£¸.
For a linear system of equations
a11x1 +
a12x2 + Â· Â· Â· +
a1nxn =
b1,
a21x1 +
a22x2 + Â· Â· Â· +
a2nxn =
b2,
...
am1x1 + am2x2 + Â· Â· Â· + amnxn = bm,
Gaussian elimination can be executed on the associated augmented matrix [A|b]
by performing elementary operations to the rows of [A|b]. These row operations
correspond to the three elementary operations (1.2.1), (1.2.2), and (1.2.3) used
to manipulate linear systems. For an m Ã— n matrix
M =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
M1âˆ—
...
Miâˆ—
...
Mjâˆ—
...
Mmâˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
the three types of elementary row operations on M are as follows.
â€¢
Type I:
Interchange rows i and j to produce
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
M1âˆ—
...
Mjâˆ—
...
Miâˆ—
...
Mmâˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
(1.2.7)

1.2 Gaussian Elimination and Matrices
9
â€¢
Type II:
Replace row i by a nonzero multiple of itself to produce
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
M1âˆ—
...
Î±Miâˆ—
...
Mmâˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
where
Î± Ì¸= 0.
(1.2.8)
â€¢
Type III:
Replace row j by a combination of itself plus a multiple of row
i to produce
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
M1âˆ—
...
Miâˆ—
...
Mjâˆ—+ Î±Miâˆ—
...
Mmâˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
(1.2.9)
To solve the system (1.2.4) by using elementary row operations, start with
the associated augmented matrix [A|b] and triangularize the coeï¬ƒcient matrix
A by performing exactly the same sequence of row operations that corresponds
to the elementary operations executed on the equations themselves:
ï£«
ï£­
âƒ
2
1
1
1
6
2
1
âˆ’1
âˆ’2
2
1
7
ï£¶
ï£¸R2 âˆ’3R1
R3 + R1
âˆ’â†’
ï£«
ï£­
2
1
1
1
0
âƒ
-1
âˆ’2
âˆ’4
0
3
2
8
ï£¶
ï£¸
R3 + 3R2
âˆ’â†’
ï£«
ï£­
2
1
1
1
0
âˆ’1
âˆ’2
âˆ’4
0
0
âˆ’4
âˆ’4
ï£¶
ï£¸.
The ï¬nal array represents the triangular system
2x + y +
z =
1,
âˆ’y âˆ’2z = âˆ’4,
âˆ’4z = âˆ’4
that is solved by back substitution as described earlier. In general, if an n Ã— n
system has been triangularized to the form
ï£«
ï£¬
ï£¬
ï£­
t11
t12
Â· Â· Â·
t1n
c1
0
t22
Â· Â· Â·
t2n
c2
...
...
...
...
...
0
0
Â· Â· Â·
tnn
cn
ï£¶
ï£·
ï£·
ï£¸
(1.2.10)
in which each tii Ì¸= 0 (i.e., there are no zero pivots), then the general algorithm
for back substitution is as follows.

10
Chapter 1
Linear Equations
Algorithm for Back Substitution
Determine the xi â€™s from (1.2.10) by ï¬rst setting xn = cn/tnn and then
recursively computing
xi = 1
tii
(ci âˆ’ti,i+1xi+1 âˆ’ti,i+2xi+2 âˆ’Â· Â· Â· âˆ’tinxn)
for i = n âˆ’1, n âˆ’2, . . . , 2, 1.
One way to gauge the eï¬ƒciency of an algorithm is to count the number of
arithmetical operations required.
3 For a variety of reasons, no distinction is made
between additions and subtractions, and no distinction is made between multipli-
cations and divisions. Furthermore, multiplications/divisions are usually counted
separately from additions/subtractions. Even if you do not work through the de-
tails, it is important that you be aware of the operational counts for Gaussian
elimination with back substitution so that you will have a basis for comparison
when other algorithms are encountered.
Gaussian Elimination Operation Counts
Gaussian elimination with back substitution applied to an n Ã— n system
requires
n3
3 + n2 âˆ’n
3
multiplications/divisions
and
n3
3 + n2
2 âˆ’5n
6
additions/subtractions.
As n grows, the n3/3 term dominates each of these expressions. There-
fore, the important thing to remember is that Gaussian elimination with
back substitution on an n Ã— n system requires about n3/3 multiplica-
tions/divisions and about the same number of additions/subtractions.
3
Operation counts alone may no longer be as important as they once were in gauging the ef-
ï¬ciency of an algorithm. Older computers executed instructions sequentially, whereas some
contemporary machines are capable of executing instructions in parallel so that diï¬€erent nu-
merical tasks can be performed simultaneously. An algorithm that lends itself to parallelism
may have a higher operational count but might nevertheless run faster on a parallel machine
than an algorithm with a lesser operational count that cannot take advantage of parallelism.

1.2 Gaussian Elimination and Matrices
11
Example 1.2.1
Problem: Solve the following system using Gaussian elimination with back sub-
stitution:
v âˆ’
w =
3,
âˆ’2u + 4v âˆ’
w =
1,
âˆ’2u + 5v âˆ’4w = âˆ’2.
Solution: The associated augmented matrix is
ï£«
ï£­
0
1
âˆ’1
3
âˆ’2
4
âˆ’1
1
âˆ’2
5
âˆ’4
âˆ’2
ï£¶
ï£¸.
Since the ï¬rst pivotal position contains 0, interchange rows one and two before
eliminating below the ï¬rst pivot:
ï£«
ï£­
âƒ
0
1
âˆ’1
3
âˆ’2
4
âˆ’1
1
âˆ’2
5
âˆ’4
âˆ’2
ï£¶
ï£¸
Interchange R1 and R2
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’
ï£«
ï£­
âƒ
-2
4
âˆ’1
1
0
1
âˆ’1
3
âˆ’2
5
âˆ’4
âˆ’2
ï£¶
ï£¸
R3 âˆ’R1
âˆ’â†’
ï£«
ï£­
âˆ’2
4
âˆ’1
1
0
âƒ
1
âˆ’1
3
0
1
âˆ’3
âˆ’3
ï£¶
ï£¸
R3 âˆ’R2
âˆ’â†’
ï£«
ï£­
âˆ’2
4
âˆ’1
1
0
1
âˆ’1
3
0
0
âˆ’2
âˆ’6
ï£¶
ï£¸.
Back substitution yields
w = âˆ’6
âˆ’2 = 3,
v = 3 + w = 3 + 3 = 6,
u = 1
âˆ’2 (1 âˆ’4v + w) = 1
âˆ’2 (1 âˆ’24 + 3) = 10.
Exercises for section 1.2
1.2.1. Use Gaussian elimination with back substitution to solve the following
system:
x1 + x2 + x3 = 1,
x1 + 2x2 + 2x3 = 1,
x1 + 2x2 + 3x3 = 1.

12
Chapter 1
Linear Equations
1.2.2. Apply Gaussian elimination with back substitution to the following sys-
tem:
2x1 âˆ’x2
= 0,
âˆ’x1 + 2x2 âˆ’x3 = 0,
âˆ’x2 + x3 = 1.
1.2.3. Use Gaussian elimination with back substitution to solve the following
system:
4x2 âˆ’3x3 = 3,
âˆ’x1 + 7x2 âˆ’5x3 = 4,
âˆ’x1 + 8x2 âˆ’6x3 = 5.
1.2.4. Solve the following system:
x1 + x2 + x3 + x4 = 1,
x1 + x2 + 3x3 + 3x4 = 3,
x1 + x2 + 2x3 + 3x4 = 3,
x1 + 3x2 + 3x3 + 3x4 = 4.
1.2.5. Consider the following three systems where the coeï¬ƒcients are the same
for each system, but the right-hand sides are diï¬€erent (this situation
occurs frequently):
4x âˆ’8y + 5z = 1 0 0,
4x âˆ’7y + 4z = 0 1 0,
3x âˆ’4y + 2z = 0 0 1.
Solve all three systems at one time by performing Gaussian elimination
on an augmented matrix of the form

A
 b1
 b2
 b3

.
1.2.6. Suppose that matrix B is obtained by performing a sequence of row
operations on matrix A . Explain why A can be obtained by performing
row operations on B .
1.2.7. Find angles Î±, Î², and Î³ such that
2 sin Î± âˆ’
cos Î² + 3 tan Î³ = 3,
4 sin Î± + 2 cos Î² âˆ’2 tan Î³ = 2,
6 sin Î± âˆ’3 cos Î² +
tan Î³ = 9,
where 0 â‰¤Î± â‰¤2Ï€, 0 â‰¤Î² â‰¤2Ï€, and 0 â‰¤Î³ < Ï€.

1.2 Gaussian Elimination and Matrices
13
1.2.8. The following system has no solution:
âˆ’x1 + 3x2 âˆ’2x3 = 1,
âˆ’x1 + 4x2 âˆ’3x3 = 0,
âˆ’x1 + 5x2 âˆ’4x3 = 0.
Attempt to solve this system using Gaussian elimination and explain
what occurs to indicate that the system is impossible to solve.
1.2.9. Attempt to solve the system
âˆ’x1 + 3x2 âˆ’2x3 = 4,
âˆ’x1 + 4x2 âˆ’3x3 = 5,
âˆ’x1 + 5x2 âˆ’4x3 = 6,
using Gaussian elimination and explain why this system must have in-
ï¬nitely many solutions.
1.2.10. By solving a 3 Ã— 3 system, ï¬nd the coeï¬ƒcients in the equation of the
parabola y = Î±+Î²x+Î³x2 that passes through the points (1, 1), (2, 2),
and (3, 0).
1.2.11. Suppose that 100 insects are distributed in an enclosure consisting of
four chambers with passageways between them as shown below.
#1
#2
#3
#4
At the end of one minute, the insects have redistributed themselves.
Assume that a minute is not enough time for an insect to visit more than
one chamber and that at the end of a minute 40% of the insects in each
chamber have not left the chamber they occupied at the beginning of
the minute. The insects that leave a chamber disperse uniformly among
the chambers that are directly accessible from the one they initially
occupiedâ€”e.g., from #3, half move to #2 and half move to #4.

14
Chapter 1
Linear Equations
(a)
If at the end of one minute there are 12, 25, 26, and 37 insects
in chambers #1, #2, #3, and #4, respectively, determine what
the initial distribution had to be.
(b)
If the initial distribution is 20, 20, 20, 40, what is the distribution
at the end of one minute?
1.2.12. Show that the three types of elementary row operations discussed on
p. 8 are not independent by showing that the interchange operation
(1.2.7) can be accomplished by a sequence of the other two types of row
operations given in (1.2.8) and (1.2.9).
1.2.13. Suppose that [A|b] is the augmented matrix associated with a linear
system. You know that performing row operations on [A|b] does not
change the solution of the system. However, no mention of column oper-
ations was ever made because column operations can alter the solution.
(a)
Describe the eï¬€ect on the solution of a linear system when
columns Aâˆ—j and Aâˆ—k are interchanged.
(b)
Describe the eï¬€ect when column Aâˆ—j is replaced by Î±Aâˆ—j for
Î± Ì¸= 0.
(c)
Describe the eï¬€ect when Aâˆ—j is replaced by Aâˆ—j + Î±Aâˆ—k.
Hint: Experiment with a 2 Ã— 2 or 3 Ã— 3 system.
1.2.14. Consider the n Ã— n Hilbert matrix deï¬ned by
H =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
1
2
1
3
Â· Â· Â·
1
n
1
2
1
3
1
4
Â· Â· Â·
1
n+1
1
3
1
4
1
5
Â· Â· Â·
1
n+2
...
...
...
Â· Â· Â·
...
1
n
1
n+1
1
n+2
Â· Â· Â·
1
2nâˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Express the individual entries hij in terms of i and j.
1.2.15. Verify that the operation counts given in the text for Gaussian elimi-
nation with back substitution are correct for a general 3 Ã— 3 system.
If you are up to the challenge, try to verify these counts for a general
n Ã— n system.
1.2.16. Explain why a linear system can never have exactly two diï¬€erent solu-
tions. Extend your argument to explain the fact that if a system has more
than one solution, then it must have inï¬nitely many diï¬€erent solutions.

1.3 Gaussâ€“Jordan Method
15
1.3
GAUSSâ€“JORDAN METHOD
The purpose of this section is to introduce a variation of Gaussian elimination
that is known as the Gaussâ€“Jordan method.
4 The two features that dis-
tinguish the Gaussâ€“Jordan method from standard Gaussian elimination are as
follows.
â€¢
At each step, the pivot element is forced to be 1.
â€¢
At each step, all terms above the pivot as well as all terms below the pivot
are eliminated.
In other words, if
ï£«
ï£¬
ï£¬
ï£­
a11
a12
Â· Â· Â·
a1n
b1
a21
a22
Â· Â· Â·
a2n
b2
...
...
...
...
...
an1
an2
Â· Â· Â·
ann
bn
ï£¶
ï£·
ï£·
ï£¸
is the augmented matrix associated with a linear system, then elementary row
operations are used to reduce this matrix to
ï£«
ï£¬
ï£¬
ï£­
1
0
Â· Â· Â·
0
s1
0
1
Â· Â· Â·
0
s2
...
...
...
...
...
0
0
Â· Â· Â·
1
sn
ï£¶
ï£·
ï£·
ï£¸.
The solution then appears in the last column (i.e., xi = si ) so that this procedure
circumvents the need to perform back substitution.
Example 1.3.1
Problem: Apply the Gaussâ€“Jordan method to solve the following system:
2x1 + 2x2 + 6x3 =
4,
2x1 +
x2 + 7x3 =
6,
âˆ’2x1 âˆ’6x2 âˆ’7x3 = âˆ’1.
4
Although there has been some confusion as to which Jordan should receive credit for this
algorithm, it now seems clear that the method was in fact introduced by a geodesist named
Wilhelm Jordan (1842â€“1899) and not by the more well known mathematician Marie Ennemond
Camille Jordan (1838â€“1922), whose name is often mistakenly associated with the technique, but
who is otherwise correctly credited with other important topics in matrix analysis, the â€œJordan
canonical formâ€ being the most notable. Wilhelm Jordan was born in southern Germany,
educated in Stuttgart, and was a professor of geodesy at the technical college in Karlsruhe.
He was a proliï¬c writer, and he introduced his elimination scheme in the 1888 publication
Handbuch der Vermessungskunde. Interestingly, a method similar to W. Jordanâ€™s variation
of Gaussian elimination seems to have been discovered and described independently by an
obscure Frenchman named Clasen, who appears to have published only one scientiï¬c article,
which appeared in 1888â€”the same year as W. Jordanâ€™s Handbuch appeared.

16
Chapter 1
Linear Equations
Solution: The sequence of operations is indicated in parentheses and the pivots
are circled.
ï£«
ï£­
âƒ
2
2
6
4
2
1
7
6
âˆ’2
âˆ’6
âˆ’7
âˆ’1
ï£¶
ï£¸
R1/2
âˆ’â†’
ï£«
ï£­
âƒ
1
1
3
2
2
1
7
6
âˆ’2
âˆ’6
âˆ’7
âˆ’1
ï£¶
ï£¸R2 âˆ’2R1
R3 + 2R1
âˆ’â†’
ï£«
ï£­
âƒ
1
1
3
2
0
âˆ’1
1
2
0
âˆ’4
âˆ’1
3
ï£¶
ï£¸(âˆ’R2) âˆ’â†’
ï£«
ï£­
1
1
3
2
0
âƒ
1
âˆ’1
âˆ’2
0
âˆ’4
âˆ’1
3
ï£¶
ï£¸
R1 âˆ’R2
R3 + 4R2
âˆ’â†’
ï£«
ï£­
1
0
4
4
0
âƒ
1
âˆ’1
âˆ’2
0
0
âˆ’5
âˆ’5
ï£¶
ï£¸
âˆ’R3/5
âˆ’â†’
ï£«
ï£­
1
0
4
4
0
1
âˆ’1
âˆ’2
0
0
âƒ
1
1
ï£¶
ï£¸
R1 âˆ’4R3
R2 + R3
âˆ’â†’
ï£«
ï£­
1
0
0
0
0
1
0
âˆ’1
0
0
âƒ
1
1
ï£¶
ï£¸.
Therefore, the solution is
ï£«
ï£­
x1
x2
x3
ï£¶
ï£¸=
ï£«
ï£­
0
âˆ’1
1
ï£¶
ï£¸.
On the surface it may seem that there is little diï¬€erence between the Gaussâ€“
Jordan method and Gaussian elimination with back substitution because elimi-
nating terms above the pivot with Gaussâ€“Jordan seems equivalent to performing
back substitution. But this is not correct. Gaussâ€“Jordan requires more arithmetic
than Gaussian elimination with back substitution.
Gaussâ€“Jordan Operation Counts
For an n Ã— n system, the Gaussâ€“Jordan procedure requires
n3
2 + n2
2
multiplications/divisions
and
n3
2 âˆ’n
2
additions/subtractions.
In other words, the Gaussâ€“Jordan method requires about n3/2 multipli-
cations/divisions and about the same number of additions/subtractions.
Recall from the previous section that Gaussian elimination with back sub-
stitution requires only about n3/3 multiplications/divisions and about the same

1.3 Gaussâ€“Jordan Method
17
number of additions/subtractions. Compare this with the n3/2 factor required
by the Gaussâ€“Jordan method, and you can see that Gaussâ€“Jordan requires about
50% more eï¬€ort than Gaussian elimination with back substitution. For small sys-
tems of the textbook variety (e.g., n = 3 ), these comparisons do not show a great
deal of diï¬€erence. However, in practical work, the systems that are encountered
can be quite large, and the diï¬€erence between Gaussâ€“Jordan and Gaussian elim-
ination with back substitution can be signiï¬cant. For example, if n = 100, then
n3/3 is about 333,333, while n3/2 is 500,000, which is a diï¬€erence of 166,667
multiplications/divisions as well as that many additions/subtractions.
Although the Gaussâ€“Jordan method is not recommended for solving linear
systems that arise in practical applications, it does have some theoretical advan-
tages. Furthermore, it can be a useful technique for tasks other than computing
solutions to linear systems. We will make use of the Gaussâ€“Jordan procedure
when matrix inversion is discussedâ€”this is the primary reason for introducing
Gaussâ€“Jordan.
Exercises for section 1.3
1.3.1. Use the Gaussâ€“Jordan method to solve the following system:
4x2 âˆ’3x3 = 3,
âˆ’x1 + 7x2 âˆ’5x3 = 4,
âˆ’x1 + 8x2 âˆ’6x3 = 5.
1.3.2. Apply the Gaussâ€“Jordan method to the following system:
x1 + x2 + x3 + x4 = 1,
x1 + 2x2 + 2x3 + 2x4 = 0,
x1 + 2x2 + 3x3 + 3x4 = 0,
x1 + 2x2 + 3x3 + 4x4 = 0.
1.3.3. Use the Gaussâ€“Jordan method to solve the following three systems at
the same time.
2x1 âˆ’x2
= 1 0 0,
âˆ’x1 + 2x2 âˆ’x3 = 0 1 0,
âˆ’x2 + x3 = 0 0 1.
1.3.4. Verify that the operation counts given in the text for the Gaussâ€“Jordan
method are correct for a general 3 Ã— 3 system. If you are up to the
challenge, try to verify these counts for a general n Ã— n system.

18
Chapter 1
Linear Equations
1.4
TWO-POINT BOUNDARY VALUE PROBLEMS
It was stated previously that linear systems that arise in practice can become
quite large in size. The purpose of this section is to understand why this often
occurs and why there is frequently a special structure to the linear systems that
come from practical applications.
Given an interval [a, b] and two numbers Î± and Î², consider the general
problem of trying to ï¬nd a function y(t) that satisï¬es the diï¬€erential equation
u(t)yâ€²â€²(t)+v(t)yâ€²(t)+w(t)y(t) = f(t),
where
y(a) = Î± and y(b) = Î². (1.4.1)
The functions u, v, w, and f are assumed to be known functions on [a, b].
Because the unknown function y(t) is speciï¬ed at the boundary points a and
b, problem (1.4.1) is known as a two-point boundary value problem. Such
problems abound in nature and are frequently very hard to handle because it is
often not possible to express y(t) in terms of elementary functions. Numerical
methods are usually employed to approximate y(t) at discrete points inside
[a, b]. Approximations are produced by subdividing the interval [a, b] into n+1
equal subintervals, each of length h = (b âˆ’a)/(n + 1) as shown below.
h
h
h


 





Â· Â· Â·
Â· Â· Â·
t0 = a
t1 = a + h
t2 = a + 2h
tn = a + nh
tn+1 = b
Derivative approximations at the interior nodes (grid points) ti = a + ih are
made by using Taylor series expansions y(t) = âˆ
k=0 y(k)(ti)(tâˆ’ti)k/k! to write
y(ti + h) = y(ti) + yâ€²(ti)h + yâ€²â€²(ti)h2
2!
+ yâ€²â€²â€²(ti)h3
3!
+ Â· Â· Â· ,
y(ti âˆ’h) = y(ti) âˆ’yâ€²(ti)h + yâ€²â€²(ti)h2
2!
âˆ’yâ€²â€²â€²(ti)h3
3!
+ Â· Â· Â· ,
(1.4.2)
and then subtracting and adding these expressions to produce
yâ€²(ti) = y(ti + h) âˆ’y(ti âˆ’h)
2h
+ O(h3)
and
yâ€²â€²(ti) = y(ti âˆ’h) âˆ’2y(ti) + y(ti + h)
h2
+ O(h4),
where O(hp) denotes
5 terms containing pth and higher powers of h. The
5
Formally, a function f(h) is O(hp) if f(h)/hp remains bounded as h â†’0, but f(h)/hq
becomes unbounded if q > p. This means that f goes to zero as fast as hp goes to zero.

1.4 Two-Point Boundary Value Problems
19
resulting approximations
yâ€²(ti) â‰ˆy(ti+h) âˆ’y(tiâˆ’h)
2h
and yâ€²â€²(ti) â‰ˆy(tiâˆ’h) âˆ’2y(ti) + y(ti+h)
h2
(1.4.3)
are called centered diï¬€erence approximations, and they are preferred over
less accurate one-sided approximations such as
yâ€²(ti) â‰ˆy(ti + h) âˆ’y(ti)
h
or
yâ€²(ti) â‰ˆy(t) âˆ’y(t âˆ’h)
h
.
The value h = (b âˆ’a)/(n + 1) is called the step size. Smaller step sizes pro-
duce better derivative approximations, so obtaining an accurate solution usually
requires a small step size and a large number of grid points. By evaluating the
centered diï¬€erence approximations at each grid point and substituting the result
into the original diï¬€erential equation (1.4.1), a system of n linear equations in
n unknowns is produced in which the unknowns are the values y(ti). A simple
example can serve to illustrate this point.
Example 1.4.1
Suppose that f(t) is a known function and consider the two-point boundary
value problem
yâ€²â€²(t) = f(t)
on [0, 1] with y(0) = y(1) = 0.
The goal is to approximate the values of y at n equally spaced grid points
ti interior to [0, 1]. The step size is therefore h = 1/(n + 1). For the sake of
convenience, let yi = y(ti) and fi = f(ti). Use the approximation
yiâˆ’1 âˆ’2yi + yi+1
h2
â‰ˆyâ€²â€²(ti) = fi
along with y0 = 0 and yn+1 = 0 to produce the system of equations
âˆ’yiâˆ’1 + 2yi âˆ’yi+1 â‰ˆâˆ’h2fi
for i = 1, 2, . . . , n.
(The signs are chosen to make the 2â€™s positive to be consistent with later devel-
opments.) The augmented matrix associated with this system is shown below:
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
2
âˆ’1
0
Â· Â· Â·
0
0
0
âˆ’h2f1
âˆ’1
2
âˆ’1
Â· Â· Â·
0
0
0
âˆ’h2f2
0
âˆ’1
2
Â· Â· Â·
0
0
0
âˆ’h2f3
...
...
...
...
...
...
...
...
0
0
0
Â· Â· Â·
2
âˆ’1
0
âˆ’h2fnâˆ’2
0
0
0
Â· Â· Â·
âˆ’1
2
âˆ’1
âˆ’h2fnâˆ’1
0
0
0
Â· Â· Â·
0
âˆ’1
2
âˆ’h2fn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
By solving this system, approximate values of the unknown function y at the
grid points ti are obtained. Larger values of n produce smaller values of h and
hence better approximations to the exact values of the yi â€™s.

20
Chapter 1
Linear Equations
Notice the pattern of the entries in the coeï¬ƒcient matrix in the above ex-
ample. The nonzero elements occur only on the subdiagonal, main-diagonal, and
superdiagonal linesâ€”such a system (or matrix) is said to be tridiagonal. This
is characteristic in the sense that when ï¬nite diï¬€erence approximations are ap-
plied to the general two-point boundary value problem, a tridiagonal system is
the result.
Tridiagonal systems are particularly nice in that they are inexpensive to
solve. When Gaussian elimination is applied, only two multiplications/divisions
are needed at each step of the triangularization process because there is at most
only one nonzero entry below and to the right of each pivot. Furthermore, Gaus-
sian elimination preserves all of the zero entries that were present in the original
tridiagonal system. This makes the back substitution process cheap to execute
because there are at most only two multiplications/divisions required at each
substitution step. Exercise 3.10.6 contains more details.
Exercises for section 1.4
1.4.1. Divide the interval [0, 1] into ï¬ve equal subintervals, and apply the ï¬nite
diï¬€erence method in order to approximate the solution of the two-point
boundary value problem
yâ€²â€²(t) = 125t,
y(0) = y(1) = 0
at the four interior grid points. Compare your approximate values at
the grid points with the exact solution at the grid points. Note: You
should not expect very accurate approximations with only four interior
grid points.
1.4.2. Divide [0, 1] into n+1 equal subintervals, and apply the ï¬nite diï¬€erence
approximation method to derive the linear system associated with the
two-point boundary value problem
yâ€²â€²(t) âˆ’yâ€²(t) = f(t),
y(0) = y(1) = 0.
1.4.3. Divide [0, 1] into ï¬ve equal subintervals, and approximate the solution
to
yâ€²â€²(t) âˆ’yâ€²(t) = 125t,
y(0) = y(1) = 0
at the four interior grid points. Compare the approximations with the
exact values at the grid points.

1.5 Making Gaussian Elimination Work
21
1.5
MAKING GAUSSIAN ELIMINATION WORK
Now that you understand the basic Gaussian elimination technique, itâ€™s time
to turn it into a practical algorithm that can be used for realistic applications.
For pencil and paper computations where you are doing exact arithmetic, the
strategy is to keep things as simple as possible (like avoiding messy fractions) in
order to minimize those â€œstupid arithmetic errorsâ€ we are all prone to make. But
very few problems in the real world are of the textbook variety, and practical
applications involving linear systems usually demand the use of a computer.
Computers donâ€™t care about messy fractions, and they donâ€™t introduce errors of
the â€œstupidâ€ variety. Computers produce a more predictable kind of error, called
roundoï¬€error, and itâ€™s important
6 to spend a little time up front to understand
this kind of error and its eï¬€ects on solving linear systems.
Numerical computation in digital computers is performed by approximating
the inï¬nite set of real numbers with a ï¬nite set of numbers as described below.
Floating-Point Numbers
A t -digit, base-Î² ï¬‚oating-point number has the form
f = Â±.d1d2 Â· Â· Â· dt Ã— Î²Ïµ
with
d1 Ì¸= 0,
where the base Î², the exponent Ïµ, and the digits 0 â‰¤di â‰¤Î² âˆ’1
are integers. For internal machine representation, Î² = 2 (binary rep-
resentation) is standard, but for pencil-and-paper examples itâ€™s more
convenient to use Î² = 10. The value of t, called the precision, and
the exponent Ïµ can vary with the choice of hardware and software.
Floating-point numbers are just adaptations of the familiar concept of sci-
entiï¬c notation where Î² = 10, which will be the value used in our examples. For
any ï¬xed set of values for t, Î², and Ïµ, the corresponding set F of ï¬‚oating-
point numbers is necessarily a ï¬nite set, so some real numbers canâ€™t be found
in F. There is more than one way of approximating real numbers with ï¬‚oating-
point numbers. For the remainder of this text, the following common rounding
convention is adopted. Given a real number x, the ï¬‚oating-point approximation
fl(x) is deï¬ned to be the nearest element in F to x, and in case of a tie we
round away from 0. This means that for t-digit precision with Î² = 10, we need
6
The computer has been the single most important scientiï¬c and technological development
of our century and has undoubtedly altered the course of science for all future time. The
prospective young scientist or engineer who passes through a contemporary course in linear
algebra and matrix theory and fails to learn at least the elementary aspects of what is involved
in solving a practical linear system with a computer is missing a fundamental tool of applied
mathematics.

22
Chapter 1
Linear Equations
to look at digit dt+1 in x = .d1d2 Â· Â· Â· dtdt+1 Â· Â· Â· Ã— 10Ïµ (making sure d1 Ì¸= 0) and
then set
fl(x) =
 .d1d2 Â· Â· Â· dt Ã— 10Ïµ
if dt+1 < 5,
([.d1d2 Â· Â· Â· dt] + 10âˆ’t) Ã— 10Ïµ
if dt+1 â‰¥5.
For example, in 2 -digit, base-10 ï¬‚oating-point arithmetic,
fl (3/80) = fl(.0375) = fl(.375 Ã— 10âˆ’1) = .38 Ã— 10âˆ’1 = .038.
By considering Î· = 1/3 and Î¾ = 3 with t -digit base-10 arithmetic, itâ€™s
easy to see that
fl(Î· + Î¾) Ì¸= fl(Î·) + fl(Î¾)
and
fl(Î·Î¾) Ì¸= fl(Î·)fl(Î¾).
Furthermore, several familiar rules of real arithmetic do not hold for ï¬‚oating-
point arithmeticâ€”associativity is one outstanding example. This, among other
reasons, makes the analysis of ï¬‚oating-point computation diï¬ƒcult. It also means
that you must be careful when working the examples and exercises in this text
because although most calculators and computers can be instructed to display
varying numbers of digits, most have a ï¬xed internal precision with which all
calculations are made before numbers are displayed, and this internal precision
cannot be altered. Almost certainly, the internal precision of your calculator or
computer is greater than the precision called for by the examples and exercises
in this text. This means that each time you perform a t-digit calculation, you
should manually round the result to t signiï¬cant digits and reenter the rounded
number before proceeding to the next calculation. In other words, donâ€™t â€œchainâ€
operations in your calculator or computer.
To understand how to execute Gaussian elimination using ï¬‚oating-point
arithmetic, letâ€™s compare the use of exact arithmetic with the use of 3-digit
base-10 arithmetic to solve the following system:
47x + 28y = 19,
89x + 53y = 36.
Using Gaussian elimination with exact arithmetic, we multiply the ï¬rst equation
by the multiplier m = 89/47 and subtract the result from the second equation
to produce

47
28
19
0
âˆ’1/47
1/47

.
Back substitution yields the exact solution
x = 1
and
y = âˆ’1.
Using 3-digit arithmetic, the multiplier is
fl(m) = fl
89
47

= .189 Ã— 101 = 1.89.

1.5 Making Gaussian Elimination Work
23
Since
fl

fl(m)fl(47)

= fl(1.89 Ã— 47) = .888 Ã— 102 = 88.8,
fl

fl(m)fl(28)

= fl(1.89 Ã— 28) = .529 Ã— 102 = 52.9,
fl

fl(m)fl(19)

= fl(1.89 Ã— 19) = .359 Ã— 102 = 35.9,
the ï¬rst step of 3-digit Gaussian elimination is as shown below:

47
28
19
fl(89 âˆ’88.8)
fl(53 âˆ’52.9)
fl(36 âˆ’35.9)

=

47
28
19
âƒ
.2
.1
.1

.
The goal is to triangularize the systemâ€”to produce a zero in the circled
(2,1)-positionâ€”but this cannot be accomplished with 3-digit arithmetic. Unless
the circled value âƒ
.2
is replaced by 0, back substitution cannot be executed.
Henceforth, we will agree simply to enter 0 in the position that we are trying
to annihilate, regardless of the value of the ï¬‚oating-point number that might
actually appear. The value of the position being annihilated is generally not
even computed. For example, donâ€™t even bother computing
fl

89 âˆ’fl

fl(m)fl(47)

= fl(89 âˆ’88.8) = .2
in the above example. Hence the result of 3-digit Gaussian elimination for this
example is

47
28
19
0
.1
.1

.
Apply 3-digit back substitution to obtain the 3-digit ï¬‚oating-point solution
y = fl
.1
.1

= 1,
x = fl
19 âˆ’28
47

= fl
âˆ’9
47

= âˆ’.191.
The vast discrepancy between the exact solution (1, âˆ’1) and the 3-digit
solution (âˆ’.191, 1) illustrates some of the problems we can expect to encounter
while trying to solve linear systems with ï¬‚oating-point arithmetic. Sometimes
using a higher precision may help, but this is not always possible because on
all machines there are natural limits that make extended precision arithmetic
impractical past a certain point. Even if it is possible to increase the precision, it

24
Chapter 1
Linear Equations
may not buy you very much because there are many cases for which an increase
in precision does not produce a comparable decrease in the accumulated roundoï¬€
error. Given any particular precision (say, t ), it is not diï¬ƒcult to provide exam-
ples of linear systems for which the computed t-digit solution is just as bad as
the one in our 3-digit example above.
Although the eï¬€ects of rounding can almost never be eliminated, there are
some simple techniques that can help to minimize these machine induced errors.
Partial Pivoting
At each step, search the positions on and below the pivotal position for
the coeï¬ƒcient of maximum magnitude. If necessary perform the appro-
priate row interchange to bring this maximal coeï¬ƒcient into the pivotal
position. Illustrated below is the third step in a typical case:
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
0
âƒ
S
âˆ—
âˆ—
âˆ—
0
0
S
âˆ—
âˆ—
âˆ—
0
0
S
âˆ—
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£¸.
Search the positions in the third column marked â€œ S â€ for the coeï¬ƒcient
of maximal magnitude and, if necessary, interchange rows to bring this
coeï¬ƒcient into the circled pivotal position. Simply stated, the strategy
is to maximize the magnitude of the pivot at each step by using only
row interchanges.
On the surface, it is probably not apparent why partial pivoting should
make a diï¬€erence. The following example not only shows that partial pivoting
can indeed make a great deal of diï¬€erence, but it also indicates what makes this
strategy eï¬€ective.
Example 1.5.1
It is easy to verify that the exact solution to the system
âˆ’10âˆ’4x + y = 1,
x + y = 2,
is given by
x =
1
1.0001
and
y = 1.0002
1.0001.
If 3-digit arithmetic without partial pivoting is used, then the result is

1.5 Making Gaussian Elimination Work
25

âˆ’10âˆ’4
1
1
1
1
2

R2 + 104R1 âˆ’â†’

âˆ’10âˆ’4
1
1
0
104
104

because
fl(1 + 104) = fl(.10001 Ã— 105) = .100 Ã— 105 = 104
(1.5.1)
and
fl(2 + 104) = fl(.10002 Ã— 105) = .100 Ã— 105 = 104.
(1.5.2)
Back substitution now produces
x = 0
and
y = 1.
Although the computed solution for y is close to the exact solution for y, the
computed solution for x is not very close to the exact solution for x â€”the
computed solution for x is certainly not accurate to three signiï¬cant ï¬gures as
you might hope. If 3-digit arithmetic with partial pivoting is used, then the result
is

âˆ’10âˆ’4
1
1
1
1
2

âˆ’â†’

1
1
2
âˆ’10âˆ’4
1
1

R2 + 10âˆ’4R1
âˆ’â†’

1
1
2
0
1
1

because
fl(1 + 10âˆ’4) = fl(.10001 Ã— 101) = .100 Ã— 101 = 1
(1.5.3)
and
fl(1 + 2 Ã— 10âˆ’4) = fl(.10002 Ã— 101) = .100 Ã— 101 = 1.
(1.5.4)
This time, back substitution produces the computed solution
x = 1
and
y = 1,
which is as close to the exact solution as one can reasonably expectâ€”the com-
puted solution agrees with the exact solution to three signiï¬cant digits.
Why did partial pivoting make a diï¬€erence? The answer lies in comparing
(1.5.1) and (1.5.2) with (1.5.3) and (1.5.4).
Without partial pivoting the multiplier is 104, and this is so large that it
completely swamps the arithmetic involving the relatively smaller numbers 1
and 2 and prevents them from being taken into account. That is, the smaller
numbers 1 and 2 are â€œblown awayâ€ as though they were never present so that
our 3-digit computer produces the exact solution to another system, namely,

âˆ’10âˆ’4
1
1
1
0
0

,

26
Chapter 1
Linear Equations
which is quite diï¬€erent from the original system. With partial pivoting the mul-
tiplier is 10âˆ’4, and this is small enough so that it does not swamp the numbers
1 and 2. In this case, the 3-digit computer produces the exact solution to the
system
 0
1
1
1
1
2

, which is close to the original system.
7
In summary, the villain in Example 1.5.1 is the large multiplier that pre-
vents some smaller numbers from being fully accounted for, thereby resulting
in the exact solution of another system that is very diï¬€erent from the original
system. By maximizing the magnitude of the pivot at each step, we minimize
the magnitude of the associated multiplier thus helping to control the growth
of numbers that emerge during the elimination process. This in turn helps cir-
cumvent some of the eï¬€ects of roundoï¬€error. The problem of growth in the
elimination procedure is more deeply analyzed on p. 348.
When partial pivoting is used, no multiplier ever exceeds 1 in magnitude. To
see that this is the case, consider the following two typical steps in an elimination
procedure:
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
0
âƒ
p
âˆ—
âˆ—
âˆ—
0
0
q
âˆ—
âˆ—
âˆ—
0
0
r
âˆ—
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£¸R4 âˆ’(q/p)R3
R5 âˆ’(r/p)R3
âˆ’â†’
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
0
âƒ
p
âˆ—
âˆ—
âˆ—
0
0
0
âˆ—
âˆ—
âˆ—
0
0
0
âˆ—
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£¸.
The pivot is p, while q/p and r/p are the multipliers. If partial pivoting has
been employed, then |p| â‰¥|q| and |p| â‰¥|r| so that
q
p
 â‰¤1
and
r
p
 â‰¤1.
By guaranteeing that no multiplier exceeds 1 in magnitude, the possibility
of producing relatively large numbers that can swamp the signiï¬cance of smaller
numbers is much reduced, but not completely eliminated. To see that there is
still more to be done, consider the following example.
Example 1.5.2
The exact solution to the system
âˆ’10x + 105y = 105,
x +
y = 2,
7
Answering the question, â€œWhat system have I really solved (i.e., obtained the exact solution
of), and how close is this system to the original system,â€ is called backward error analysis,
as opposed to forward analysis in which one tries to answer the question, â€œHow close will a
computed solution be to the exact solution?â€
Backward analysis has proven to be an eï¬€ective
way to analyze the numerical stability of algorithms.

1.5 Making Gaussian Elimination Work
27
is given by
x =
1
1.0001
and
y = 1.0002
1.0001.
Suppose that 3-digit arithmetic with partial pivoting is used. Since | âˆ’10| > 1,
no interchange is called for and we obtain

âˆ’10
105
105
1
1
2

R2 + 10âˆ’1R1 âˆ’â†’

âˆ’10
105
105
0
104
104

because
fl(1 + 104) = fl(.10001 Ã— 105) = .100 Ã— 105 = 104
and
fl(2 + 104) = fl(.10002 Ã— 105) = .100 Ã— 105 = 104.
Back substitution yields
x = 0
and
y = 1,
which must be considered to be very badâ€”the computed 3-digit solution for y
is not too bad, but the computed 3-digit solution for x is terrible!
What is the source of diï¬ƒculty in Example 1.5.2? This time, the multi-
plier cannot be blamed. The trouble stems from the fact that the ï¬rst equation
contains coeï¬ƒcients that are much larger than the coeï¬ƒcients in the second
equation. That is, there is a problem of scale due to the fact that the coeï¬ƒcients
are of diï¬€erent orders of magnitude. Therefore, we should somehow rescale the
system before attempting to solve it.
If the ï¬rst equation in the above example is rescaled to insure that the
coeï¬ƒcient of maximum magnitude is a 1, which is accomplished by multiplying
the ï¬rst equation by 10âˆ’5, then the system given in Example 1.5.1 is obtained,
and we know from that example that partial pivoting produces a very good
approximation to the exact solution.
This points to the fact that the success of partial pivoting can hinge on
maintaining the proper scale among the coeï¬ƒcients. Therefore, the second re-
ï¬nement needed to make Gaussian elimination practical is a reasonable scaling
strategy. Unfortunately, there is no known scaling procedure that will produce
optimum results for every possible system, so we must settle for a strategy that
will work most of the time. The strategy is to combine row scalingâ€”multiplying
selected rows by nonzero multipliersâ€”with column scalingâ€”multiplying se-
lected columns of the coeï¬ƒcient matrix A by nonzero multipliers.
Row scaling doesnâ€™t alter the exact solution, but column scaling doesâ€”see
Exercise 1.2.13(b). Column scaling is equivalent to changing the units of the
kth unknown. For example, if the units of the kth unknown xk in [A|b] are
millimeters, and if the kth column of A is multiplied by . 001, then the kth
unknown in the scaled system [Ë†A | b] is Ë†xi = 1000xi, and thus the units of the
scaled unknown Ë†xk become meters.

28
Chapter 1
Linear Equations
Experience has shown that the following strategy for combining row scaling
with column scaling usually works reasonably well.
Practical Scaling Strategy
1.
Choose units that are natural to the problem and do not dis-
tort the relationships between the sizes of things. These natural
units are usually self-evident, and further column scaling past
this point is not ordinarily attempted.
2.
Row scale the system [A|b] so that the coeï¬ƒcient of maximum
magnitude in each row of A is equal to 1. That is, divide each
equation by the coeï¬ƒcient of maximum magnitude.
Partial pivoting together with the scaling strategy described above
makes Gaussian elimination with back substitution an extremely eï¬€ec-
tive tool. Over the course of time, this technique has proven to be reliable
for solving a majority of linear systems encountered in practical work.
Although it is not extensively used, there is an extension of partial pivoting
known as complete pivoting which, in some special cases, can be more eï¬€ective
than partial pivoting in helping to control the eï¬€ects of roundoï¬€error.
Complete Pivoting
If [A|b] is the augmented matrix at the kth step of Gaussian elimina-
tion, then search the pivotal position together with every position in A
that is below or to the right of the pivotal position for the coeï¬ƒcient
of maximum magnitude. If necessary, perform the appropriate row and
column interchanges to bring the coeï¬ƒcient of maximum magnitude into
the pivotal position. Shown below is the third step in a typical situation:
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
0
âƒ
S
S
S
âˆ—
0
0
S
S
S
âˆ—
0
0
S
S
S
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£¸
Search the positions marked â€œ S â€ for the coeï¬ƒcient of maximal magni-
tude. If necessary, interchange rows and columns to bring this maximal
coeï¬ƒcient into the circled pivotal position. Recall from Exercise 1.2.13
that the eï¬€ect of a column interchange in A is equivalent to permuting
(or renaming) the associated unknowns.

1.5 Making Gaussian Elimination Work
29
You should be able to see that complete pivoting should be at least as eï¬€ec-
tive as partial pivoting. Moreover, it is possible to construct specialized exam-
ples where complete pivoting is superior to partial pivotingâ€”a famous example
is presented in Exercise 1.5.7. However, one rarely encounters systems of this
nature in practice. A deeper comparison between no pivoting, partial pivoting,
and complete pivoting is given on p. 348.
Example 1.5.3
Problem:
Use 3-digit arithmetic together with complete pivoting to solve the
following system:
x âˆ’
y = âˆ’2,
âˆ’9x + 10y = 12.
Solution:
Since 10 is the coeï¬ƒcient of maximal magnitude that lies in the
search pattern, interchange the ï¬rst and second rows and then interchange the
ï¬rst and second columns:

1
âˆ’1
âˆ’2
âˆ’9
10
12

âˆ’â†’

âˆ’9
10
12
1
âˆ’1
âˆ’2

âˆ’â†’

10
âˆ’9
12
âˆ’1
1
âˆ’2

âˆ’â†’

10
âˆ’9
12
0
.1
âˆ’.8

.
The eï¬€ect of the column interchange is to rename the unknowns to Ë†x and Ë†y,
where Ë†x = y and Ë†y = x. Back substitution yields Ë†y = âˆ’8 and Ë†x = âˆ’6 so that
x = Ë†y = âˆ’8
and
y = Ë†x = âˆ’6.
In this case, the 3-digit solution and the exact solution agree. If only partial
pivoting is used, the 3-digit solution will not be as accurate. However, if scaled
partial pivoting is used, the result is the same as when complete pivoting is used.
If the cost of using complete pivoting was nearly the same as the cost of using
partial pivoting, we would always use complete pivoting. However, it is not diï¬ƒ-
cult to show that complete pivoting approximately doubles the cost over straight
Gaussian elimination, whereas partial pivoting adds only a negligible amount.
Couple this with the fact that it is extremely rare to encounter a practical system
where scaled partial pivoting is not adequate while complete pivoting is, and it
is easy to understand why complete pivoting is seldom used in practice. Gaus-
sian elimination with scaled partial pivoting is the preferred method for dense
systems (i.e., not a lot of zeros) of moderate size.

30
Chapter 1
Linear Equations
Exercises for section 1.5
1.5.1. Consider the following system:
10âˆ’3x âˆ’y = 1,
x + y = 0.
(a)
Use 3-digit arithmetic with no pivoting to solve this system.
(b)
Find a system that is exactly satisï¬ed by your solution from
part (a), and note how close this system is to the original system.
(c)
Now use partial pivoting and 3-digit arithmetic to solve the
original system.
(d)
Find a system that is exactly satisï¬ed by your solution from
part (c), and note how close this system is to the original system.
(e)
Use exact arithmetic to obtain the solution to the original sys-
tem, and compare the exact solution with the results of parts (a)
and (c).
(f)
Round the exact solution to three signiï¬cant digits, and compare
the result with those of parts (a) and (c).
1.5.2. Consider the following system:
x +
y = 3,
âˆ’10x + 105y = 105.
(a)
Use 4-digit arithmetic with partial pivoting and no scaling to
compute a solution.
(b)
Use 4-digit arithmetic with complete pivoting and no scaling to
compute a solution of the original system.
(c)
This time, row scale the original system ï¬rst, and then apply
partial pivoting with 4-digit arithmetic to compute a solution.
(d)
Now determine the exact solution, and compare it with the re-
sults of parts (a), (b), and (c).
1.5.3. With no scaling, compute the 3-digit solution of
âˆ’3x + y = âˆ’2,
10x âˆ’3y = 7,
without partial pivoting and with partial pivoting. Compare your results
with the exact solution.

1.5 Making Gaussian Elimination Work
31
1.5.4. Consider the following system in which the coeï¬ƒcient matrix is the
Hilbert matrix:
x + 1
2y + 1
3z = 1
3,
1
2x + 1
3y + 1
4z = 1
3,
1
3x + 1
4y + 1
5z = 1
5.
(a)
First convert the coeï¬ƒcients to 3-digit ï¬‚oating-point numbers,
and then use 3-digit arithmetic with partial pivoting but with
no scaling to compute the solution.
(b)
Again use 3-digit arithmetic, but row scale the coeï¬ƒcients (after
converting them to ï¬‚oating-point numbers), and then use partial
pivoting to compute the solution.
(c)
Proceed as in part (b), but this time row scale the coeï¬ƒcients
before each elimination step.
(d)
Now use exact arithmetic on the original system to determine
the exact solution, and compare the result with those of parts
(a), (b), and (c).
1.5.5. To see that changing units can aï¬€ect a ï¬‚oating-point solution, consider
a mining operation that extracts silica, iron, and gold from the earth.
Capital (measured in dollars), operating time (in hours), and labor (in
man-hours) are needed to operate the mine. To extract a pound of silica
requires $.0055, .0011 hours of operating time, and .0093 man-hours of
labor. For each pound of iron extracted, $.095, .01 operating hours, and
.025 man-hours are required. For each pound of gold extracted, $960,
112 operating hours, and 560 man-hours are required.
(a)
Suppose that during 600 hours of operation, exactly $5000 and
3000 man-hours are used. Let x, y, and z denote the number
of pounds of silica, iron, and gold, respectively, that are recov-
ered during this period. Set up the linear system whose solution
will yield the values for x, y, and z.
(b)
With no scaling, use 3-digit arithmetic and partial pivoting to
compute a solution (Ëœx, Ëœy, Ëœz) of the system of part (a). Then
approximate the exact solution (x, y, z) by using your machineâ€™s
(or calculatorâ€™s) full precision with partial pivoting to solve the
system in part (a), and compare this with your 3-digit solution
by computing the relative error deï¬ned by
er =
 
(x âˆ’Ëœx)2 + (y âˆ’Ëœy)2 + (z âˆ’Ëœz)2
 
x2 + y2 + z2
.

32
Chapter 1
Linear Equations
(c)
Using 3-digit arithmetic, column scale the coeï¬ƒcients by chang-
ing units: convert pounds of silica to tons of silica, pounds of
iron to half-tons of iron, and pounds of gold to troy ounces of
gold (1 lb. = 12 troy oz.).
(d)
Use 3-digit arithmetic with partial pivoting to solve the column
scaled system of part (c). Then approximate the exact solution
by using your machineâ€™s (or calculatorâ€™s) full precision with par-
tial pivoting to solve the system in part (c), and compare this
with your 3-digit solution by computing the relative error er as
deï¬ned in part (b).
1.5.6. Consider the system given in Example 1.5.3.
(a)
Use 3-digit arithmetic with partial pivoting but with no scaling
to solve the system.
(b)
Now use partial pivoting with scaling. Does complete pivoting
provide an advantage over scaled partial pivoting in this case?
1.5.7. Consider the following well-scaled matrix:
Wn =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
0
Â· Â· Â·
0
0
1
âˆ’1
1
0
Â· Â· Â·
0
0
1
âˆ’1
âˆ’1
1
...
0
0
1
...
...
...
...
...
...
...
âˆ’1
âˆ’1
âˆ’1
...
1
0
1
âˆ’1
âˆ’1
âˆ’1
Â· Â· Â·
âˆ’1
1
1
âˆ’1
âˆ’1
âˆ’1
Â· Â· Â·
âˆ’1
âˆ’1
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
(a)
Reduce Wn to an upper-triangular form using Gaussian elimi-
nation with partial pivoting, and determine the element of max-
imal magnitude that emerges during the elimination procedure.
(b)
Now use complete pivoting and repeat part (a).
(c)
Formulate a statement comparing the results of partial pivoting
with those of complete pivoting for Wn, and describe the eï¬€ect
this would have in determining the t -digit solution for a system
whose augmented matrix is [Wn | b].
1.5.8. Suppose that A is an n Ã— n matrix of real numbers that has been scaled
so that each entry satisï¬es |aij| â‰¤1, and consider reducing A to tri-
angular form using Gaussian elimination with partial pivoting. Demon-
strate that after k steps of the process, no entry can have a magnitude
that exceeds 2k. Note: The previous exercise shows that there are cases
where it is possible for some elements to actually attain the maximum
magnitude of 2k after k steps.

1.6 Ill-Conditioned Systems
33
1.6
ILL-CONDITIONED SYSTEMS
Gaussian elimination with partial pivoting on a properly scaled system is perhaps
the most fundamental algorithm in the practical use of linear algebra. However,
it is not a universal algorithm nor can it be used blindly. The purpose of this
section is to make the point that when solving a linear system some discretion
must always be exercised because there are some systems that are so inordinately
sensitive to small perturbations that no numerical technique can be used with
conï¬dence.
Example 1.6.1
Consider the system
.835x + .667y = .168,
.333x + .266y = .067,
for which the exact solution is
x = 1
and
y = âˆ’1.
If b2 = .067 is only slightly perturbed to become Ë†b2 = .066, then the exact
solution changes dramatically to become
Ë†x = âˆ’666
and
Ë†y = 834.
This is an example of a system whose solution is extremely sensitive to
a small perturbation. This sensitivity is intrinsic to the system itself and is
not a result of any numerical procedure. Therefore, you cannot expect some
â€œnumerical trickâ€ to remove the sensitivity. If the exact solution is sensitive to
small perturbations, then any computed solution cannot be less so, regardless of
the algorithm used.
Ill-Conditioned Linear Systems
A system of linear equations is said to be ill-conditioned when
some small perturbation in the system can produce relatively large
changes in the exact solution. Otherwise, the system is said to be well-
conditioned.
It is easy to visualize what causes a 2 Ã— 2 system to be ill-conditioned.
Geometrically, two equations in two unknowns represent two straight lines, and
the point of intersection is the solution for the system. An ill-conditioned system
represents two straight lines that are almost parallel.

34
Chapter 1
Linear Equations
If two straight lines are almost parallel and if one of the lines is tilted only
slightly, then the point of intersection (i.e., the solution of the associated 2 Ã— 2
linear system) is drastically altered.
L
L'
Original
Solution
Perturbed
Solution
Figure 1.6.1
This is illustrated in Figure 1.6.1 in which line L is slightly perturbed to
become line Lâ€². Notice how this small perturbation results in a large change
in the point of intersection. This was exactly the situation for the system given
in Example 1.6.1. In general, ill-conditioned systems are those that represent
almost parallel lines, almost parallel planes, and generalizations of these notions.
Because roundoï¬€errors can be viewed as perturbations to the original coeï¬ƒ-
cients of the system, employing even a generally good numerical techniqueâ€”short
of exact arithmeticâ€”on an ill-conditioned system carries the risk of producing
nonsensical results.
In dealing with an ill-conditioned system, the engineer or scientist is often
confronted with a much more basic (and sometimes more disturbing) problem
than that of simply trying to solve the system. Even if a minor miracle could
be performed so that the exact solution could be extracted, the scientist or
engineer might still have a nonsensical solution that could lead to totally incorrect
conclusions. The problem stems from the fact that the coeï¬ƒcients are often
empirically obtained and are therefore known only within certain tolerances. For
an ill-conditioned system, a small uncertainty in any of the coeï¬ƒcients can mean
an extremely large uncertainty may exist in the solution. This large uncertainty
can render even the exact solution totally useless.
Example 1.6.2
Suppose that for the system
.835x + .667y = b1
.333x + .266y = b2
the numbers b1 and b2 are the results of an experiment and must be read from
the dial of a test instrument. Suppose that the dial can be read to within a

1.6 Ill-Conditioned Systems
35
tolerance of Â±.001, and assume that values for b1 and b2 are read as . 168 and
. 067, respectively. This produces the ill-conditioned system of Example 1.6.1,
and it was seen in that example that the exact solution of the system is
(x, y) = (1, âˆ’1).
(1.6.1)
However, due to the small uncertainty in reading the dial, we have that
.167 â‰¤b1 â‰¤.169
and
.066 â‰¤b2 â‰¤.068.
(1.6.2)
For example, this means that the solution associated with the reading (b1, b2) =
(.168, .067) is just as valid as the solution associated with the reading (b1, b2) =
(.167, .068), or the reading (b1, b2) = (.169, .066), or any other reading falling
in the range (1.6.2). For the reading (b1, b2) = (.167, .068), the exact solution is
(x, y) = (934, âˆ’1169),
(1.6.3)
while for the other reading (b1, b2) = (.169, .066), the exact solution is
(x, y) = (âˆ’932, 1167).
(1.6.4)
Would you be willing to be the ï¬rst to ï¬‚y in the plane or drive across the bridge
whose design incorporated a solution to this problem? I wouldnâ€™t! There is just
too much uncertainty. Since no one of the solutions (1.6.1), (1.6.3), or (1.6.4)
can be preferred over any of the others, it is conceivable that totally diï¬€erent
designs might be implemented depending on how the technician reads the last
signiï¬cant digit on the dial. Due to the ill-conditioned nature of an associated
linear system, the successful design of the plane or bridge may depend on blind
luck rather than on scientiï¬c principles.
Rather than trying to extract accurate solutions from ill-conditioned sys-
tems, engineers and scientists are usually better oï¬€investing their time and re-
sources in trying to redesign the associated experiments or their data collection
methods so as to avoid producing ill-conditioned systems.
There is one other discomforting aspect of ill-conditioned systems. It con-
cerns what students refer to as â€œchecking the answerâ€ by substituting a computed
solution back into the left-hand side of the original system of equations to see
how close it comes to satisfying the systemâ€”that is, producing the right-hand
side. More formally, if
xc = ( Î¾1
Î¾2
Â· Â· Â·
Î¾n )
is a computed solution for a system
a11x1 + a12x2 + Â· Â· Â· + a1nxn = b1,
a21x1 + a22x2 + Â· Â· Â· + a2nxn = b2,
...
an1x1 + an2x2 + Â· Â· Â· + annxn = bn,

36
Chapter 1
Linear Equations
then the numbers
ri = ai1Î¾1 + ai2Î¾2 + Â· Â· Â· + ainÎ¾n âˆ’bi
for i = 1, 2, . . . , n
are called the residuals. Suppose that you compute a solution xc and substitute
it back to ï¬nd that all the residuals are relatively small. Does this guarantee that
xc is close to the exact solution? Surprisingly, the answer is a resounding â€œno!â€
whenever the system is ill-conditioned.
Example 1.6.3
For the ill-conditioned system given in Example 1.6.1, suppose that somehow
you compute a solution to be
Î¾1 = âˆ’666
and
Î¾2 = 834.
If you attempt to â€œcheck the errorâ€ in this computed solution by substituting it
back into the original system, then you ï¬ndâ€”using exact arithmeticâ€”that the
residuals are
r1 = .835Î¾1 + .667Î¾2 âˆ’.168 = 0,
r2 = .333Î¾1 + .266Î¾2 âˆ’.067 = âˆ’.001.
That is, the computed solution (âˆ’666, 834) exactly satisï¬es the ï¬rst equation
and comes very close to satisfying the second. On the surface, this might seem to
suggest that the computed solution should be very close to the exact solution. In
fact a naive person could probably be seduced into believing that the computed
solution is within Â±.001 of the exact solution. Obviously, this is nowhere close
to being true since the exact solution is
x = 1
and
y = âˆ’1.
This is always a shock to a student seeing this illustrated for the ï¬rst time because
it is counter to a noviceâ€™s intuition. Unfortunately, many students leave school
believing that they can always â€œcheckâ€ the accuracy of their computations by
simply substituting them back into the original equationsâ€”it is good to know
that youâ€™re not among them.
This raises the question, â€œHow can I check a computed solution for accu-
racy?â€ Fortunately, if the system is well-conditioned, then the residuals do indeed
provide a more eï¬€ective measure of accuracy (a rigorous proof along with more
insight appears in Example 5.12.2 on p. 416). But this means that you must be
able to answer some additional questions. For example, how can one tell before-
hand if a given system is ill-conditioned? How can one measure the extent of
ill-conditioning in a linear system?
One technique to determine the extent of ill-conditioning might be to exper-
iment by slightly perturbing selected coeï¬ƒcients and observing how the solution

1.6 Ill-Conditioned Systems
37
changes. If a radical change in the solution is observed for a small perturbation
to some set of coeï¬ƒcients, then you have uncovered an ill-conditioned situation.
If a given perturbation does not produce a large change in the solution, then
nothing can be concludedâ€”perhaps you perturbed the wrong set of coeï¬ƒcients.
By performing several such experiments using diï¬€erent sets of coeï¬ƒcients, a
feel (but not a guarantee) for the extent of ill-conditioning can be obtained. This
is expensive and not very satisfying. But before more can be said, more sophisti-
cated tools need to be developedâ€”the topics of sensitivity and conditioning are
revisited on p. 127 and in Example 5.12.1 on p. 414.
Exercises for section 1.6
1.6.1. Consider the ill-conditioned system of Example 1.6.1:
.835x + .667y = .168,
.333x + .266y = .067.
(a)
Describe the outcome when you attempt to solve the system
using 5-digit arithmetic with no scaling.
(b)
Again using 5-digit arithmetic, ï¬rst row scale the system before
attempting to solve it. Describe to what extent this helps.
(c)
Now use 6-digit arithmetic with no scaling. Compare the results
with the exact solution.
(d)
Using 6-digit arithmetic, compute the residuals for your solution
of part (c), and interpret the results.
(e)
For the same solution obtained in part (c), again compute the
residuals, but use 7-digit arithmetic this time, and interpret the
results.
(f)
Formulate a concluding statement that summarizes the points
made in parts (a)â€“(e).
1.6.2. Perturb the ill-conditioned system given in Exercise 1.6.1 above so as to
form the following system:
.835x + .667y = .1669995,
.333x + .266y = .066601.
(a)
Determine the exact solution, and compare it with the exact
solution of the system in Exercise 1.6.1.
(b)
On the basis of the results of part (a), formulate a statement
concerning the necessity for the solution of an ill-conditioned
system to undergo a radical change for every perturbation of
the original system.

38
Chapter 1
Linear Equations
1.6.3. Consider the two straight lines determined by the graphs of the following
two equations:
.835x + .667y = .168,
.333x + .266y = .067.
(a)
Use 5-digit arithmetic to compute the slopes of each of the lines,
and then use 6-digit arithmetic to do the same. In each case,
sketch the graphs on a coordinate system.
(b)
Show by diagram why a small perturbation in either of these
lines can result in a large change in the solution.
(c)
Describe in geometrical terms the situation that must exist in
order for a system to be optimally well-conditioned.
1.6.4. Using geometric considerations, rank the following three systems accord-
ing to their condition.
(a)
1.001x âˆ’y = .235,
x + .0001y = .765.
(b)
1.001x âˆ’y = .235,
x + .9999y = .765.
(c)
1.001x + y = .235,
x + .9999y = .765.
1.6.5. Determine the exact solution of the following system:
8x + 5y + 2z = 15,
21x + 19y + 16z = 56,
39x + 48y + 53z = 140.
Now change 15 to 14 in the ï¬rst equation and again solve the system
with exact arithmetic. Is the system ill-conditioned?
1.6.6. Show that the system
v âˆ’w âˆ’x âˆ’y âˆ’z = 0,
w âˆ’x âˆ’y âˆ’z = 0,
x âˆ’y âˆ’z = 0,
y âˆ’z = 0,
z = 1,

1.6 Ill-Conditioned Systems
39
is ill-conditioned by considering the following perturbed system:
v âˆ’w âˆ’x âˆ’y âˆ’z = 0,
âˆ’1
15v + w âˆ’x âˆ’y âˆ’z = 0,
âˆ’1
15v + x âˆ’y âˆ’z = 0,
âˆ’1
15v + y âˆ’z = 0,
âˆ’1
15v + z = 1.
1.6.7. Let f(x) = sin Ï€x on [0, 1]. The object of this problem is to determine
the coeï¬ƒcients Î±i of the cubic polynomial
p(x) =
3
!
i=0
Î±ixi
that is as close to f(x) as possible in the sense that
r =
" 1
0
[f(x) âˆ’p(x)]2dx
=
" 1
0
[f(x)]2dx âˆ’2
3
!
i=0
Î±i
" 1
0
xif(x)dx +
" 1
0
# 3
!
i=0
Î±ixi
$2
dx
is as small as possible.
(a)
In order to minimize r, impose the condition that âˆ‚r/âˆ‚Î±i = 0
for each i = 0, 1, 2, 3, and show this results in a system of linear
equations whose augmented matrix is [H4 | b], where H4 and
b are given by
H4 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
1
2
1
3
1
4
1
2
1
3
1
4
1
5
1
3
1
4
1
5
1
6
1
4
1
5
1
6
1
7
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
and
b =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
2
Ï€
1
Ï€
1
Ï€ âˆ’
4
Ï€3
1
Ï€ âˆ’
6
Ï€3
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Any matrix Hn that has the same form as H4 is called a
Hilbert matrix of order n.
(b)
Systems involving Hilbert matrices are badly ill-conditioned,
and the ill-conditioning becomes worse as the size increases. Use
exact arithmetic with Gaussian elimination to reduce H4 to tri-
angular form. Assuming that the case in which n = 4 is typical,
explain why a general system [Hn | b] will be ill-conditioned.
Notice that even complete pivoting is of no help.

CHAPTER 2
Rectangular Systems
and
Echelon Forms
2.1
ROW ECHELON FORM AND RANK
We are now ready to analyze more general linear systems consisting of m linear
equations involving n unknowns
a11x1 +
a12x2 + Â· Â· Â· +
a1nxn =
b1,
a21x1 +
a22x2 + Â· Â· Â· +
a2nxn =
b2,
...
am1x1 + am2x2 + Â· Â· Â· + amnxn = bm,
where m may be diï¬€erent from n. If we do not know for sure that m and n
are the same, then the system is said to be rectangular. The case m = n is
still allowed in the discussionâ€”statements concerning rectangular systems also
are valid for the special case of square systems.
The ï¬rst goal is to extend the Gaussian elimination technique from square
systems to completely general rectangular systems. Recall that for a square sys-
tem with a unique solution, the pivotal positions are always located along the
main diagonalâ€”the diagonal line from the upper-left-hand corner to the lower-
right-hand cornerâ€”in the coeï¬ƒcient matrix A so that Gaussian elimination
results in a reduction of A to a triangular matrix, such as that illustrated
below for the case n = 4:
T =
ï£«
ï£¬
ï£­
âƒ
*
âˆ—
âˆ—
âˆ—
0
âƒ
*
âˆ—
âˆ—
0
0
âƒ
*
âˆ—
0
0
0
âƒ
*
ï£¶
ï£·
ï£¸.

42
Chapter 2
Rectangular Systems and Echelon Forms
Remember that a pivot must always be a nonzero number. For square sys-
tems possessing a unique solution, it is a fact (proven later) that one can al-
ways bring a nonzero number into each pivotal position along the main diag-
onal.
8 However, in the case of a general rectangular system, it is not always
possible to have the pivotal positions lying on a straight diagonal line in the
coeï¬ƒcient matrix. This means that the ï¬nal result of Gaussian elimination will
not be triangular in form. For example, consider the following system:
x1 + 2x2 + x3 + 3x4 + 3x5 = 5,
2x1 + 4x2
+ 4x4 + 4x5 = 6,
x1 + 2x2 + 3x3 + 5x4 + 5x5 = 9,
2x1 + 4x2
+ 4x4 + 7x5 = 9.
Focus your attention on the coeï¬ƒcient matrix
A =
ï£«
ï£¬
ï£­
1
2
1
3
3
2
4
0
4
4
1
2
3
5
5
2
4
0
4
7
ï£¶
ï£·
ï£¸,
(2.1.1)
and ignore the right-hand side for a moment. Applying Gaussian elimination to
A yields the following result:
ï£«
ï£¬
ï£­
âƒ
1
2
1
3
3
2
4
0
4
4
1
2
3
5
5
2
4
0
4
7
ï£¶
ï£·
ï£¸âˆ’â†’
ï£«
ï£¬
ï£­
1
2
1
3
3
0
âƒ
0
âˆ’2
âˆ’2
âˆ’2
0
0
2
2
2
0
0
âˆ’2
âˆ’2
1
ï£¶
ï£·
ï£¸.
In the basic elimination process, the strategy is to move down and to the right
to the next pivotal position. If a zero occurs in this position, an interchange with
a row below the pivotal row is executed so as to bring a nonzero number into
the pivotal position. However, in this example, it is clearly impossible to bring
a nonzero number into the (2, 2) -position by interchanging the second row with
a lower row.
In order to handle this situation, the elimination process is modiï¬ed as
follows.
8
This discussion is for exact arithmetic. If ï¬‚oating-point arithmetic is used, this may no longer
be true. Part (a) of Exercise 1.6.1 is one such example.

2.1 Row Echelon Form and Rank
43
Modiï¬ed Gaussian Elimination
Suppose that U is the augmented matrix associated with the system
after i âˆ’1 elimination steps have been completed. To execute the ith
step, proceed as follows:
â€¢
Moving from left to right in U , locate the ï¬rst column that contains
a nonzero entry on or below the ith positionâ€”say it is Uâˆ—j.
â€¢
The pivotal position for the ith step is the (i, j) -position.
â€¢
If necessary, interchange the ith row with a lower row to bring a
nonzero number into the (i, j) -position, and then annihilate all en-
tries below this pivot.
â€¢
If row Uiâˆ—as well as all rows in U below Uiâˆ—consist entirely of
zeros, then the elimination process is completed.
Illustrated below is the result of applying this modiï¬ed version of Gaussian
elimination to the matrix given in (2.1.1).
Example 2.1.1
Problem: Apply modiï¬ed Gaussian elimination to the following matrix and
circle the pivot positions:
A =
ï£«
ï£¬
ï£­
1
2
1
3
3
2
4
0
4
4
1
2
3
5
5
2
4
0
4
7
ï£¶
ï£·
ï£¸.
Solution:
ï£«
ï£¬
ï£­
âƒ
1
2
1
3
3
2
4
0
4
4
1
2
3
5
5
2
4
0
4
7
ï£¶
ï£·
ï£¸âˆ’â†’
ï£«
ï£¬
ï£­
âƒ
1
2
1
3
3
0
0
âƒ
-2
âˆ’2
âˆ’2
0
0
2
2
2
0
0
âˆ’2
âˆ’2
1
ï£¶
ï£·
ï£¸
âˆ’â†’
ï£«
ï£¬
ï£­
âƒ
1
2
1
3
3
0
0
âƒ
-2
âˆ’2
âˆ’2
0
0
0
0
âƒ
0
0
0
0
0
3
ï£¶
ï£·
ï£¸âˆ’â†’
ï£«
ï£¬
ï£­
âƒ
1
2
1
3
3
0
0
âƒ
-2
âˆ’2
âˆ’2
0
0
0
0
âƒ
3
0
0
0
0
0
ï£¶
ï£·
ï£¸.

44
Chapter 2
Rectangular Systems and Echelon Forms
Notice that the ï¬nal result of applying Gaussian elimination in the above
example is not a purely triangular form but rather a jagged or â€œstair-stepâ€ type
of triangular form. Hereafter, a matrix that exhibits this stair-step structure will
be said to be in row echelon form.
Row Echelon Form
An m Ã— n matrix E with rows Eiâˆ—and columns Eâˆ—j is said to be in
row echelon form provided the following two conditions hold.
â€¢
If Eiâˆ—consists entirely of zeros, then all rows below Eiâˆ—are also
entirely zero; i.e., all zero rows are at the bottom.
â€¢
If the ï¬rst nonzero entry in Eiâˆ—lies in the jth position, then all
entries below the ith position in columns Eâˆ—1, Eâˆ—2, . . . , Eâˆ—j are zero.
These two conditions say that the nonzero entries in an echelon form
must lie on or above a stair-step line that emanates from the upper-
left-hand corner and slopes down and to the right. The pivots are the
ï¬rst nonzero entries in each row. A typical structure for a matrix in row
echelon form is illustrated below with the pivots circled.
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âƒ
*
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
0
âƒ
*
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
0
0
âƒ
*
âˆ—
âˆ—
âˆ—
âˆ—
0
0
0
0
0
0
âƒ
*
âˆ—
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
Because of the ï¬‚exibility in choosing row operations to reduce a matrix A
to a row echelon form E, the entries in E are not uniquely determined by A.
Nevertheless, it can be proven that the â€œformâ€ of E is unique in the sense that
the positions of the pivots in E (and A) are uniquely determined by the entries
in A .
9 Because the pivotal positions are unique, it follows that the number of
pivots, which is the same as the number of nonzero rows in E, is also uniquely
determined by the entries in A . This number is called the rank
10 of A, and it
9
The fact that the pivotal positions are unique should be intuitively evident. If it isnâ€™t, take the
matrix given in (2.1.1) and try to force some diï¬€erent pivotal positions by a diï¬€erent sequence
of row operations.
10
The word â€œrankâ€ was introduced in 1879 by the German mathematician Ferdinand Georg
Frobenius (p. 662), who thought of it as the size of the largest nonzero minor determinant
in A. But the concept had been used as early as 1851 by the English mathematician James
J. Sylvester (1814â€“1897).

2.1 Row Echelon Form and Rank
45
is extremely important in the development of our subject.
Rank of a Matrix
Suppose AmÃ—n is reduced by row operations to an echelon form E.
The rank of A is deï¬ned to be the number
rank (A) = number of pivots
= number of nonzero rows in E
= number of basic columns in A,
where the basic columns of A are deï¬ned to be those columns in A
that contain the pivotal positions.
Example 2.1.2
Problem: Determine the rank, and identify the basic columns in
A =
ï£«
ï£­
1
2
1
1
2
4
2
2
3
6
3
4
ï£¶
ï£¸.
Solution: Reduce A to row echelon form as shown below:
A =
ï£«
ï£­
âƒ
1
2
1
1
2
4
2
2
3
6
3
4
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
âƒ
1
2
1
1
0
0
0
âƒ
0
0
0
0
1
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
âƒ
1
2
1
1
0
0
0
âƒ
1
0
0
0
0
ï£¶
ï£¸= E.
Consequently, rank (A) = 2. The pivotal positions lie in the ï¬rst and fourth
columns so that the basic columns of A are Aâˆ—1 and Aâˆ—4. That is,
Basic Columns =
ï£±
ï£²
ï£³
ï£«
ï£­
1
2
3
ï£¶
ï£¸,
ï£«
ï£­
1
2
4
ï£¶
ï£¸
ï£¼
ï£½
ï£¾.
Pay particular attention to the fact that the basic columns are extracted from
A and not from the row echelon form E .

46
Chapter 2
Rectangular Systems and Echelon Forms
Exercises for section 2.1
2.1.1. Reduce each of the following matrices to row echelon form, determine
the rank, and identify the basic columns.
(a)
ï£«
ï£­
1
2
3
3
2
4
6
9
2
6
7
6
ï£¶
ï£¸(b)
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1
2
3
2
6
8
2
6
0
1
2
5
3
8
6
ï£¶
ï£·
ï£·
ï£·
ï£¸(c)
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
2
1
1
3
0
4
1
4
2
4
4
1
5
5
2
1
3
1
0
4
3
6
3
4
8
1
9
5
0
0
3
âˆ’3
0
0
3
8
4
2
14
1
13
3
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
2.1.2. Determine which of the following matrices are in row echelon form:
(a)
ï£«
ï£­
1
2
3
0
0
4
0
1
0
ï£¶
ï£¸.
(b)
ï£«
ï£­
0
0
0
0
0
1
0
0
0
0
0
1
ï£¶
ï£¸.
(c)
ï£«
ï£­
2
2
3
âˆ’4
0
0
7
âˆ’8
0
0
0
âˆ’1
ï£¶
ï£¸.
(d)
ï£«
ï£¬
ï£­
1
2
0
0
1
0
0
0
0
1
0
0
0
0
0
0
0
1
0
0
0
0
0
0
ï£¶
ï£·
ï£¸.
2.1.3. Suppose that A is an m Ã— n matrix. Give a short explanation of why
each of the following statements is true.
(a)
rank (A) â‰¤min{m, n}.
(b)
rank (A) < m if one row in A is entirely zero.
(c)
rank (A) < m if one row in A is a multiple of another row.
(d)
rank (A) < m if one row in A is a combination of other rows.
(e)
rank (A) < n if one column in A is entirely zero.
2.1.4. Let A =
ï£«
ï£­
.1
.2
.3
.4
.5
.6
.7
.8
.901
ï£¶
ï£¸.
(a)
Use exact arithmetic to determine rank (A).
(b)
Now use 3-digit ï¬‚oating-point arithmetic (without partial piv-
oting or scaling) to determine rank (A). This number might be
called the â€œ3-digit numerical rank.â€
(c)
What happens if partial pivoting is incorporated?
2.1.5. How many diï¬€erent â€œformsâ€ are possible for a 3 Ã— 4 matrix that is in
row echelon form?
2.1.6. Suppose that [A|b] is reduced to a matrix [E|c].
(a)
Is [E|c] in row echelon form if E is?
(b)
If [E|c] is in row echelon form, must E be in row echelon form?

2.2 Reduced Row Echelon Form
47
2.2
REDUCED ROW ECHELON FORM
At each step of the Gaussâ€“Jordan method, the pivot is forced to be a 1, and then
all entries above and below the pivotal 1 are annihilated. If A is the coeï¬ƒcient
matrix for a square system with a unique solution, then the end result of applying
the Gaussâ€“Jordan method to A is a matrix with 1â€™s on the main diagonal and
0â€™s everywhere else. That is,
A
Gaussâ€“Jordan
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’
ï£«
ï£¬
ï£¬
ï£­
1
0
Â· Â· Â·
0
0
1
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
1
ï£¶
ï£·
ï£·
ï£¸.
But if the Gaussâ€“Jordan technique is applied to a more general m Ã— n matrix,
then the ï¬nal result is not necessarily the same as described above. The following
example illustrates what typically happens in the rectangular case.
Example 2.2.1
Problem: Apply Gaussâ€“Jordan elimination to the following 4 Ã— 5 matrix and
circle the pivot positions. This is the same matrix used in Example 2.1.1:
A =
ï£«
ï£¬
ï£­
1
2
1
3
3
2
4
0
4
4
1
2
3
5
5
2
4
0
4
7
ï£¶
ï£·
ï£¸.
Solution:
ï£«
ï£¬
ï£­
âƒ
1
2
1
3
3
2
4
0
4
4
1
2
3
5
5
2
4
0
4
7
ï£¶
ï£·
ï£¸â†’
ï£«
ï£¬
ï£­
âƒ
1
2
1
3
3
0
0
âƒ
-2
âˆ’2
âˆ’2
0
0
2
2
2
0
0
âˆ’2
âˆ’2
1
ï£¶
ï£·
ï£¸â†’
ï£«
ï£¬
ï£­
âƒ
1
2
1
3
3
0
0
âƒ
1
1
1
0
0
2
2
2
0
0
âˆ’2
âˆ’2
1
ï£¶
ï£·
ï£¸
â†’
ï£«
ï£¬
ï£­
âƒ
1
2
0
2
2
0
0
âƒ
1
1
1
0
0
0
0
âƒ
0
0
0
0
0
3
ï£¶
ï£·
ï£¸â†’
ï£«
ï£¬
ï£­
âƒ
1
2
0
2
2
0
0
âƒ
1
1
1
0
0
0
0
âƒ
3
0
0
0
0
0
ï£¶
ï£·
ï£¸â†’
ï£«
ï£¬
ï£­
âƒ
1
2
0
2
2
0
0
âƒ
1
1
1
0
0
0
0
âƒ
1
0
0
0
0
0
ï£¶
ï£·
ï£¸
â†’
ï£«
ï£¬
ï£­
âƒ
1
2
0
2
0
0
0
âƒ
1
1
0
0
0
0
0
âƒ
1
0
0
0
0
0
ï£¶
ï£·
ï£¸.

48
Chapter 2
Rectangular Systems and Echelon Forms
Compare the results of this example with the results of Example 2.1.1, and
notice that the â€œformâ€ of the ï¬nal matrix is the same in both examples, which
indeed must be the case because of the uniqueness of â€œformâ€ mentioned in the
previous section. The only diï¬€erence is in the numerical value of some of the
entries. By the nature of Gaussâ€“Jordan elimination, each pivot is 1 and all entries
above and below each pivot are 0. Consequently, the row echelon form produced
by the Gaussâ€“Jordan method contains a reduced number of nonzero entries, so
it seems only natural to refer to this as a reduced row echelon form.
11
Reduced Row Echelon Form
A matrix EmÃ—n is said to be in reduced row echelon form provided
that the following three conditions hold.
â€¢
E is in row echelon form.
â€¢
The ï¬rst nonzero entry in each row (i.e., each pivot) is 1.
â€¢
All entries above each pivot are 0.
A typical structure for a matrix in reduced row echelon form is illustrated
below, where entries marked * can be either zero or nonzero numbers:
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âƒ
1
âˆ—
0
0
âˆ—
âˆ—
0
âˆ—
0
0
âƒ
1
0
âˆ—
âˆ—
0
âˆ—
0
0
0
âƒ
1
âˆ—
âˆ—
0
âˆ—
0
0
0
0
0
0
âƒ
1
âˆ—
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
As previously stated, if matrix A is transformed to a row echelon form
by row operations, then the â€œformâ€ is uniquely determined by A, but the in-
dividual entries in the form are not unique. However, if A is transformed by
row operations to a reduced row echelon form EA, then it can be shown
12 that
both the â€œformâ€ as well as the individual entries in EA are uniquely determined
by A. In other words, the reduced row echelon form EA produced from A is
independent of whatever elimination scheme is used. Producing an unreduced
form is computationally more eï¬ƒcient, but the uniqueness of EA makes it more
useful for theoretical purposes.
11
In some of the older books this is called the Hermite normal form
in honor of the French
mathematician Charles Hermite (1822â€“1901), who, around 1851, investigated reducing matrices
by row operations.
12
A formal uniqueness proof must wait until Example 3.9.2, but you can make this intuitively
clear right now with some experiments. Try to produce two diï¬€erent reduced row echelon forms
from the same matrix.

2.2 Reduced Row Echelon Form
49
EA Notation
For a matrix A, the symbol EA will hereafter denote the unique re-
duced row echelon form derived from A by means of row operations.
Example 2.2.2
Problem: Determine EA, deduce rank (A), and identify the basic columns of
A =
ï£«
ï£¬
ï£­
1
2
2
3
1
2
4
4
6
2
3
6
6
9
6
1
2
4
5
3
ï£¶
ï£·
ï£¸.
Solution:
ï£«
ï£¬
ï£­
âƒ
1
2
2
3
1
2
4
4
6
2
3
6
6
9
6
1
2
4
5
3
ï£¶
ï£·
ï£¸âˆ’â†’
ï£«
ï£¬
ï£­
âƒ
1
2
2
3
1
0
0
âƒ
0
0
0
0
0
0
0
3
0
0
2
2
2
ï£¶
ï£·
ï£¸âˆ’â†’
ï£«
ï£¬
ï£­
âƒ
1
2
2
3
1
0
0
âƒ
2
2
2
0
0
0
0
3
0
0
0
0
0
ï£¶
ï£·
ï£¸
âˆ’â†’
ï£«
ï£¬
ï£­
âƒ
1
2
2
3
1
0
0
âƒ
1
1
1
0
0
0
0
3
0
0
0
0
0
ï£¶
ï£·
ï£¸âˆ’â†’
ï£«
ï£¬
ï£­
âƒ
1
2
0
1
âˆ’1
0
0
âƒ
1
1
1
0
0
0
0
âƒ
3
0
0
0
0
0
ï£¶
ï£·
ï£¸
âˆ’â†’
ï£«
ï£¬
ï£­
âƒ
1
2
0
1
âˆ’1
0
0
âƒ
1
1
1
0
0
0
0
âƒ
1
0
0
0
0
0
ï£¶
ï£·
ï£¸âˆ’â†’
ï£«
ï£¬
ï£­
âƒ
1
2
0
1
0
0
0
âƒ
1
1
0
0
0
0
0
âƒ
1
0
0
0
0
0
ï£¶
ï£·
ï£¸
Therefore, rank (A) = 3, and {Aâˆ—1, Aâˆ—3, Aâˆ—5} are the three basic columns.
The above example illustrates another important feature of EA and ex-
plains why the basic columns are indeed â€œbasic.â€ Each nonbasic column is ex-
pressible as a combination of basic columns. In Example 2.2.2,
Aâˆ—2 = 2Aâˆ—1
and
Aâˆ—4 = Aâˆ—1 + Aâˆ—3.
(2.2.1)
Notice that exactly the same set of relationships hold in EA. That is,
Eâˆ—2 = 2Eâˆ—1
and
Eâˆ—4 = Eâˆ—1 + Eâˆ—3.
(2.2.2)
This is no coincidenceâ€”itâ€™s characteristic of what happens in general. Thereâ€™s
more to observe. The relationships between the nonbasic and basic columns in a

50
Chapter 2
Rectangular Systems and Echelon Forms
general matrix A are usually obscure, but the relationships among the columns
in EA are absolutely transparent. For example, notice that the multipliers used
in the relationships (2.2.1) and (2.2.2) appear explicitly in the two nonbasic
columns in EA â€”they are just the nonzero entries in these nonbasic columns.
This is important because it means that EA can be used as a â€œmapâ€ or â€œkeyâ€
to discover or unlock the hidden relationships among the columns of A .
Finally, observe from Example 2.2.2 that only the basic columns to the left
of a given nonbasic column are needed in order to express the nonbasic column
as a combination of basic columnsâ€”e.g., representing Aâˆ—2 requires only Aâˆ—1
and not Aâˆ—3 or Aâˆ—5, while representing Aâˆ—4 requires only Aâˆ—1 and Aâˆ—3.
This too is typical. For the time being, we accept the following statements to be
true. A rigorous proof is given later on p. 136.
Column Relationships in A and EA
â€¢
Each nonbasic column Eâˆ—k in EA is a combination (a sum of mul-
tiples) of the basic columns in EA to the left of Eâˆ—k. That is,
Eâˆ—k = Âµ1Eâˆ—b1 + Âµ2Eâˆ—b2 + Â· Â· Â· + ÂµjEâˆ—bj
= Âµ1
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
...
0
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
+ Âµ2
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
1
...
0
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
+ Â· Â· Â· + Âµj
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
0
...
1
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Âµ1
Âµ2
...
Âµj
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
where the Eâˆ—biâ€™s are the basic columns to the left of Eâˆ—k and where
the multipliers Âµi are the ï¬rst j entries in Eâˆ—k.
â€¢
The relationships that exist among the columns of A are exactly
the same as the relationships that exist among the columns of EA.
In particular, if Aâˆ—k is a nonbasic column in A , then
Aâˆ—k = Âµ1Aâˆ—b1 + Âµ2Aâˆ—b2 + Â· Â· Â· + ÂµjAâˆ—bj,
(2.2.3)
where the Aâˆ—biâ€™s are the basic columns to the left of Aâˆ—k, and
where the multipliers Âµi are as described aboveâ€”the ï¬rst j entries
in Eâˆ—k.

2.2 Reduced Row Echelon Form
51
Example 2.2.3
Problem: Write each nonbasic column as a combination of basic columns in
A =
ï£«
ï£­
2
âˆ’4
âˆ’8
6
3
0
1
3
2
3
3
âˆ’2
0
0
8
ï£¶
ï£¸.
Solution: Transform A to EA as shown below.
ï£«
ï£­
âƒ
2
âˆ’4
âˆ’8
6
3
0
1
3
2
3
3
âˆ’2
0
0
8
ï£¶
ï£¸â†’
ï£«
ï£­
âƒ
1
âˆ’2
âˆ’4
3
3
2
0
1
3
2
3
3
âˆ’2
0
0
8
ï£¶
ï£¸â†’
ï£«
ï£­
âƒ
1
âˆ’2
âˆ’4
3
3
2
0
âƒ
1
3
2
3
0
4
12
âˆ’9
7
2
ï£¶
ï£¸â†’
ï£«
ï£­
âƒ
1
0
2
7
15
2
0
âƒ
1
3
2
3
0
0
0
âˆ’17
âˆ’17
2
ï£¶
ï£¸â†’
ï£«
ï£­
âƒ
1
0
2
7
15
2
0
âƒ
1
3
2
3
0
0
0
âƒ
1
1
2
ï£¶
ï£¸â†’
ï£«
ï£­
âƒ
1
0
2
0
4
0
âƒ
1
3
0
2
0
0
0
âƒ
1
1
2
ï£¶
ï£¸
The third and ï¬fth columns are nonbasic. Looking at the columns in EA reveals
Eâˆ—3 = 2Eâˆ—1 + 3Eâˆ—2
and
Eâˆ—5 = 4Eâˆ—1 + 2Eâˆ—2 + 1
2Eâˆ—4.
The relationships that exist among the columns of A must be exactly the same
as those in EA, so
Aâˆ—3 = 2Aâˆ—1 + 3Aâˆ—2
and
Aâˆ—5 = 4Aâˆ—1 + 2Aâˆ—2 + 1
2Aâˆ—4.
You can easily check the validity of these equations by direct calculation.
In summary, the utility of EA lies in its ability to reveal dependencies in
data stored as columns in an array A. The nonbasic columns in A represent
redundant information in the sense that this information can always be expressed
in terms of the data contained in the basic columns.
Although data compression is not the primary reason for introducing EA,
the application to these problems is clear. For a large array of data, it may be
more eï¬ƒcient to store only â€œindependent dataâ€ (i.e., the basic columns of A )
along with the nonzero multipliers Âµi obtained from the nonbasic columns in
EA. Then the redundant data contained in the nonbasic columns of A can
always be reconstructed if and when it is called for.
Exercises for section 2.2
2.2.1. Determine the reduced row echelon form for each of the following matri-
ces and then express each nonbasic column in terms of the basic columns:
(a)
ï£«
ï£­
1
2
3
3
2
4
6
9
2
6
7
6
ï£¶
ï£¸,
(b)
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
2
1
1
3
0
4
1
4
2
4
4
1
5
5
2
1
3
1
0
4
3
6
3
4
8
1
9
5
0
0
3
âˆ’3
0
0
3
8
4
2
14
1
13
3
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.

52
Chapter 2
Rectangular Systems and Echelon Forms
2.2.2. Construct a matrix A whose reduced row echelon form is
EA =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
2
0
âˆ’3
0
0
0
0
0
1
âˆ’4
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Is A unique?
2.2.3. Suppose that A is an m Ã— n matrix. Give a short explanation of why
rank (A) < n whenever one column in A is a combination of other
columns in A .
2.2.4. Consider the following matrix:
A =
ï£«
ï£­
.1
.2
.3
.4
.5
.6
.7
.8
.901
ï£¶
ï£¸.
(a)
Use exact arithmetic to determine EA.
(b)
Now use 3-digit ï¬‚oating-point arithmetic (without partial piv-
oting or scaling) to determine EA and formulate a statement
concerning â€œnear relationshipsâ€ between the columns of A .
2.2.5. Consider the matrix
E =
ï£«
ï£­
1
0
âˆ’1
0
1
2
0
0
0
ï£¶
ï£¸.
You already know that Eâˆ—3 can be expressed in terms of Eâˆ—1 and Eâˆ—2.
However, this is not the only way to represent the column dependencies
in E . Show how to write Eâˆ—1 in terms of Eâˆ—2 and Eâˆ—3 and then
express Eâˆ—2 as a combination of Eâˆ—1 and Eâˆ—3. Note:
This exercise
illustrates that the set of pivotal columns is not the only set that can
play the role of â€œbasic columns.â€ Taking the basic columns to be the
ones containing the pivots is a matter of convenience because everything
becomes automatic that way.

2.3 Consistency of Linear Systems
53
2.3
CONSISTENCY OF LINEAR SYSTEMS
A system of m linear equations in n unknowns is said to be a consistent sys-
tem if it possesses at least one solution. If there are no solutions, then the system
is called inconsistent. The purpose of this section is to determine conditions
under which a given system will be consistent.
Stating conditions for consistency of systems involving only two or three
unknowns is easy. A linear equation in two unknowns represents a line in 2-space,
and a linear equation in three unknowns is a plane in 3-space. Consequently, a
linear system of m equations in two unknowns is consistent if and only if the m
lines deï¬ned by the m equations have at least one common point of intersection.
Similarly, a system of m equations in three unknowns is consistent if and only
if the associated m planes have at least one common point of intersection.
However, when m is large, these geometric conditions may not be easy to verify
visually, and when n > 3, the generalizations of intersecting lines or planes are
impossible to visualize with the eye.
Rather than depending on geometry to establish consistency, we use Gaus-
sian elimination. If the associated augmented matrix [A|b] is reduced by row
operations to a matrix [E|c] that is in row echelon form, then consistencyâ€”or
lack of itâ€”becomes evident. Suppose that somewhere in the process of reduc-
ing [A|b] to [E|c] a situation arises in which the only nonzero entry in a row
appears on the right-hand side, as illustrated below:
Row i âˆ’â†’
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
0
0
âˆ—
âˆ—
âˆ—
âˆ—
0
0
0
0
âˆ—
âˆ—
âˆ—
0
0
0
0
0
0
Î±
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸â†âˆ’Î± Ì¸= 0.
If this occurs in the ith row, then the ith equation of the associated system is
0x1 + 0x2 + Â· Â· Â· + 0xn = Î±.
For Î± Ì¸= 0, this equation has no solution, and hence the original system must
also be inconsistent (because row operations donâ€™t alter the solution set). The
converse also holds. That is, if a system is inconsistent, then somewhere in the
elimination process a row of the form
( 0
0
Â· Â· Â·
0
|
Î± ) ,
Î± Ì¸= 0
(2.3.1)
must appear. Otherwise, the back substitution process can be completed and
a solution is produced. There is no inconsistency indicated when a row of the
form (0 0 Â· Â· Â· 0 | 0) is encountered. This simply says that 0 = 0, and although

54
Chapter 2
Rectangular Systems and Echelon Forms
this is no help in determining the value of any unknown, it is nevertheless a true
statement, so it doesnâ€™t indicate inconsistency in the system.
There are some other ways to characterize the consistency (or inconsistency)
of a system. One of these is to observe that if the last column b in the augmented
matrix [A|b] is a nonbasic column, then no pivot can exist in the last column,
and hence the system is consistent because the situation (2.3.1) cannot occur.
Conversely, if the system is consistent, then the situation (2.3.1) never occurs
during Gaussian elimination and consequently the last column cannot be basic.
In other words, [A|b] is consistent if and only if b is a nonbasic column.
Saying that b is a nonbasic column in [A|b] is equivalent to saying that
all basic columns in [A|b] lie in the coeï¬ƒcient matrix A . Since the number of
basic columns in a matrix is the rank, consistency may also be characterized by
stating that a system is consistent if and only if rank[A|b] = rank (A).
Recall from the previous section the fact that each nonbasic column in [A|b]
must be expressible in terms of the basic columns. Because a consistent system
is characterized by the fact that the right-hand side b is a nonbasic column,
it follows that a system is consistent if and only if the right-hand side b is a
combination of columns from the coeï¬ƒcient matrix A.
Each of the equivalent
13 ways of saying that a system is consistent is sum-
marized below.
Consistency
Each of the following is equivalent to saying that [A|b] is consistent.
â€¢
In row reducing [A|b], a row of the following form never appears:
( 0
0
Â· Â· Â·
0
|
Î± ) ,
where
Î± Ì¸= 0.
(2.3.2)
â€¢
b is a nonbasic column in [A|b].
(2.3.3)
â€¢
rank[A|b] = rank (A).
(2.3.4)
â€¢
b is a combination of the basic columns in A.
(2.3.5)
Example 2.3.1
Problem: Determine if the following system is consistent:
x1 + x2 + 2x3 + 2x4 + x5 = 1,
2x1 + 2x2 + 4x3 + 4x4 + 3x5 = 1,
2x1 + 2x2 + 4x3 + 4x4 + 2x5 = 2,
3x1 + 5x2 + 8x3 + 6x4 + 5x5 = 3.
13
Statements P and Q are said to be equivalent when (P implies Q) as well as its converse (Q
implies P) are true statements. This is also the meaning of the phrase â€œP if and only if Q.â€

2.3 Consistency of Linear Systems
55
Solution: Apply Gaussian elimination to the augmented matrix [A|b] as shown:
ï£«
ï£¬
ï£­
âƒ
1
1
2
2
1
1
2
2
4
4
3
1
2
2
4
4
2
2
3
5
8
6
5
3
ï£¶
ï£·
ï£¸âˆ’â†’
ï£«
ï£¬
ï£­
âƒ
1
1
2
2
1
1
0
âƒ
0
0
0
1
âˆ’1
0
0
0
0
0
0
0
2
2
0
2
0
ï£¶
ï£·
ï£¸
âˆ’â†’
ï£«
ï£¬
ï£­
âƒ
1
1
2
2
1
1
0
âƒ
2
2
0
2
0
0
0
0
0
âƒ
1
âˆ’1
0
0
0
0
0
0
ï£¶
ï£·
ï£¸.
Because a row of the form ( 0
0
Â· Â· Â·
0
|
Î± ) with Î± Ì¸= 0 never emerges,
the system is consistent. We might also observe that b is a nonbasic column
in [A|b] so that rank[A|b] = rank (A). Finally, by completely reducing A to
EA, it is possible to verify that b is indeed a combination of the basic columns
{Aâˆ—1, Aâˆ—2, Aâˆ—5}.
Exercises for section 2.3
2.3.1. Determine which of the following systems are consistent.
(a)
x + 2y + z = 2,
2x + 4y
= 2,
3x + 6y + z = 4.
(b)
2x + 2y + 4z = 0,
3x + 2y + 5z = 0,
4x + 2y + 6z = 0.
(c)
x âˆ’y + z = 1,
x âˆ’y âˆ’z = 2,
x + y âˆ’z = 3,
x + y + z = 4.
(d)
x âˆ’y + z = 1,
x âˆ’y âˆ’z = 2,
x + y âˆ’z = 3,
x + y + z = 2.
(e)
2w + x + 3y + 5z = 1,
4w +
4y + 8z = 0,
w + x + 2y + 3z = 0,
x + y + z = 0.
(f)
2w + x + 3y + 5z = 7,
4w +
4y + 8z = 8,
w + x + 2y + 3z = 5,
x + y + z = 3.
2.3.2. Construct a 3 Ã— 4 matrix A and 3 Ã— 1 columns b and c such that
[A|b] is the augmented matrix for an inconsistent system, but [A|c] is
the augmented matrix for a consistent system.
2.3.3. If A is an m Ã— n matrix with rank (A) = m, explain why the system
[A|b] must be consistent for every right-hand side b .

56
Chapter 2
Rectangular Systems and Echelon Forms
2.3.4. Consider two consistent systems whose augmented matrices are of the
form [A|b] and [A|c]. That is, they diï¬€er only on the right-hand side.
Is the system associated with [A | b + c] also consistent? Explain why.
2.3.5. Is it possible for a parabola whose equation has the form y = Î±+Î²x+Î³x2
to pass through the four points (0, 1), (1, 3), (2, 15), and (3, 37)? Why?
2.3.6. Consider using ï¬‚oating-point arithmetic (without scaling) to solve the
following system:
.835x + .667y = .168,
.333x + .266y = .067.
(a)
Is the system consistent when 5-digit arithmetic is used?
(b)
What happens when 6-digit arithmetic is used?
2.3.7. In order to grow a certain crop, it is recommended that each square foot
of ground be treated with 10 units of phosphorous, 9 units of potassium,
and 19 units of nitrogen. Suppose that there are three brands of fertilizer
on the marketâ€” say brand X , brand Y , and brand Z . One pound of
brand X contains 2 units of phosphorous, 3 units of potassium, and 5
units of nitrogen. One pound of brand Y contains 1 unit of phosphorous,
3 units of potassium, and 4 units of nitrogen. One pound of brand Z
contains only 1 unit of phosphorous and 1 unit of nitrogen. Determine
whether or not it is possible to meet exactly the recommendation by
applying some combination of the three brands of fertilizer.
2.3.8. Suppose that an augmented matrix [A|b] is reduced by means of Gaus-
sian elimination to a row echelon form [E|c]. If a row of the form
( 0
0
Â· Â· Â·
0
|
Î± ) ,
Î± Ì¸= 0
does not appear in [E|c], is it possible that rows of this form could have
appeared at earlier stages in the reduction process? Why?

2.4 Homogeneous Systems
57
2.4
HOMOGENEOUS SYSTEMS
A system of m linear equations in n unknowns
a11x1 + a12x2 + Â· Â· Â· + a1nxn = 0,
a21x1 + a22x2 + Â· Â· Â· + a2nxn = 0,
...
am1x1 + am2x2 + Â· Â· Â· + amnxn = 0,
in which the right-hand side consists entirely of 0â€™s is said to be a homogeneous
system. If there is at least one nonzero number on the right-hand side, then the
system is called nonhomogeneous. The purpose of this section is to examine
some of the elementary aspects concerning homogeneous systems.
Consistency is never an issue when dealing with homogeneous systems be-
cause the zero solution x1 = x2 = Â· Â· Â· = xn = 0 is always one solution regardless
of the values of the coeï¬ƒcients. Hereafter, the solution consisting of all zeros is
referred to as the trivial solution. The only question is, â€œAre there solutions
other than the trivial solution, and if so, how can we best describe them?â€ As
before, Gaussian elimination provides the answer.
While reducing the augmented matrix [A|0] of a homogeneous system to
a row echelon form using Gaussian elimination, the zero column on the right-
hand side can never be altered by any of the three elementary row operations.
That is, any row echelon form derived from [A|0] by means of row operations
must also have the form [E|0]. This means that the last column of 0â€™s is just
excess baggage that is not necessary to carry along at each step. Just reduce the
coeï¬ƒcient matrix A to a row echelon form E, and remember that the right-
hand side is entirely zero when you execute back substitution. The process is
best understood by considering a typical example.
In order to examine the solutions of the homogeneous system
x1 + 2x2 + 2x3 + 3x4 = 0,
2x1 + 4x2 + x3 + 3x4 = 0,
3x1 + 6x2 + x3 + 4x4 = 0,
(2.4.1)
reduce the coeï¬ƒcient matrix to a row echelon form.
A =
ï£«
ï£­
1
2
2
3
2
4
1
3
3
6
1
4
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
1
2
2
3
0
0
âˆ’3
âˆ’3
0
0
âˆ’5
âˆ’5
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
1
2
2
3
0
0
âˆ’3
âˆ’3
0
0
0
0
ï£¶
ï£¸= E.
Therefore, the original homogeneous system is equivalent to the following reduced
homogeneous system:
x1 + 2x2 + 2x3 + 3x4 = 0,
âˆ’3x3 âˆ’3x4 = 0.
(2.4.2)

58
Chapter 2
Rectangular Systems and Echelon Forms
Since there are four unknowns but only two equations in this reduced system,
it is impossible to extract a unique solution for each unknown. The best we can
do is to pick two â€œbasicâ€ unknownsâ€”which will be called the basic variables
and solve for these in terms of the other two unknownsâ€”whose values must
remain arbitrary or â€œfree,â€ and consequently they will be referred to as the free
variables. Although there are several possibilities for selecting a set of basic
variables, the convention is to always solve for the unknowns corresponding to
the pivotal positionsâ€”or, equivalently, the unknowns corresponding to the basic
columns. In this example, the pivots (as well as the basic columns) lie in the ï¬rst
and third positions, so the strategy is to apply back substitution to solve the
reduced system (2.4.2) for the basic variables x1 and x3 in terms of the free
variables x2 and x4. The second equation in (2.4.2) yields
x3 = âˆ’x4
and substitution back into the ï¬rst equation produces
x1 = âˆ’2x2 âˆ’2x3 âˆ’3x4,
= âˆ’2x2 âˆ’2(âˆ’x4) âˆ’3x4,
= âˆ’2x2 âˆ’x4.
Therefore, all solutions of the original homogeneous system can be described by
saying
x1 = âˆ’2x2 âˆ’x4,
x2 is â€œfree,â€
x3 = âˆ’x4,
x4 is â€œfree.â€
(2.4.3)
As the free variables x2 and x4 range over all possible values, the above ex-
pressions describe all possible solutions. For example, when x2 and x4 assume
the values x2 = 1 and x4 = âˆ’2, then the particular solution
x1 = 0,
x2 = 1,
x3 = 2,
x4 = âˆ’2
is produced. When x2 = Ï€ and x4 =
âˆš
2, then another particular solution
x1 = âˆ’2Ï€ âˆ’
âˆš
2,
x2 = Ï€,
x3 = âˆ’
âˆš
2,
x4 =
âˆš
2
is generated.
Rather than describing the solution set as illustrated in (2.4.3), future de-
velopments will make it more convenient to express the solution set by writing
ï£«
ï£¬
ï£­
x1
x2
x3
x4
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
âˆ’2x2 âˆ’x4
x2
âˆ’x4
x4
ï£¶
ï£·
ï£¸= x2
ï£«
ï£¬
ï£­
âˆ’2
1
0
0
ï£¶
ï£·
ï£¸+ x4
ï£«
ï£¬
ï£­
âˆ’1
0
âˆ’1
1
ï£¶
ï£·
ï£¸
(2.4.4)

2.4 Homogeneous Systems
59
with the understanding that x2 and x4 are free variables that can range over
all possible numbers. This representation will be called the general solution
of the homogeneous system. This expression for the general solution emphasizes
that every solution is some combination of the two particular solutions
h1 =
ï£«
ï£¬
ï£­
âˆ’2
1
0
0
ï£¶
ï£·
ï£¸
and
h2 =
ï£«
ï£¬
ï£­
âˆ’1
0
âˆ’1
1
ï£¶
ï£·
ï£¸.
The fact that h1 and h2 are each solutions is clear because h1 is produced
when the free variables assume the values x2 = 1 and x4 = 0, whereas the
solution h2 is generated when x2 = 0 and x4 = 1.
Now consider a general homogeneous system [A|0] of m linear equations
in n unknowns. If the coeï¬ƒcient matrix is such that rank (A) = r, then it
should be apparent from the preceding discussion that there will be exactly r
basic variablesâ€”corresponding to the positions of the basic columns in A â€”and
exactly n âˆ’r free variablesâ€”corresponding to the positions of the nonbasic
columns in A . Reducing A to a row echelon form using Gaussian elimination
and then using back substitution to solve for the basic variables in terms of the
free variables produces the general solution, which has the form
x = xf1h1 + xf2h2 + Â· Â· Â· + xfnâˆ’rhnâˆ’r,
(2.4.5)
where xf1, xf2, . . . , xfnâˆ’r are the free variables and where h1, h2, . . . , hnâˆ’r are
n Ã— 1 columns that represent particular solutions of the system. As the free
variables xfi range over all possible values, the general solution generates all
possible solutions.
The general solution does not depend on which row echelon form is used
in the sense that using back substitution to solve for the basic variables in
terms of the nonbasic variables generates a unique set of particular solutions
{h1, h2, . . . , hnâˆ’r}, regardless of which row echelon form is used. Without going
into great detail, one can argue that this is true because using back substitution
in any row echelon form to solve for the basic variables must produce exactly
the same result as that obtained by completely reducing A to EA and then
solving the reduced homogeneous system for the basic variables. Uniqueness of
EA guarantees the uniqueness of the hiâ€™s.
For example, if the coeï¬ƒcient matrix A associated with the system (2.4.1)
is completely reduced by the Gaussâ€“Jordan procedure to EA
A =
ï£«
ï£­
1
2
2
3
2
4
1
3
3
6
1
4
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
1
2
0
1
0
0
1
1
0
0
0
0
ï£¶
ï£¸= EA,

60
Chapter 2
Rectangular Systems and Echelon Forms
then we obtain the following reduced system:
x1 + 2x2 + x4 = 0,
x3 + x4 = 0.
Solving for the basic variables x1 and x3 in terms of x2 and x4 produces
exactly the same result as given in (2.4.3) and hence generates exactly the same
general solution as shown in (2.4.4).
Because it avoids the back substitution process, you may ï¬nd it more con-
venient to use the Gaussâ€“Jordan procedure to reduce A completely to EA
and then construct the general solution directly from the entries in EA. This
approach usually will be adopted in the examples and exercises.
As was previously observed, all homogeneous systems are consistent because
the trivial solution consisting of all zeros is always one solution. The natural
question is, â€œWhen is the trivial solution the only
solution?â€ In other words,
we wish to know when a homogeneous system possesses a unique solution. The
form of the general solution (2.4.5) makes the answer transparent. As long as
there is at least one free variable, then it is clear from (2.4.5) that there will
be an inï¬nite number of solutions. Consequently, the trivial solution is the only
solution if and only if there are no free variables. Because the number of free
variables is given by n âˆ’r, where r = rank (A), the previous statement can be
reformulated to say that a homogeneous system possesses a unique solutionâ€”the
trivial solutionâ€”if and only if rank (A) = n.
Example 2.4.1
The homogeneous system
x1 + 2x2 + 2x3 = 0,
2x1 + 5x2 + 7x3 = 0,
3x1 + 6x2 + 8x3 = 0,
has only the trivial solution because
A =
ï£«
ï£­
1
2
2
2
5
7
3
6
8
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
1
2
2
0
1
3
0
0
2
ï£¶
ï£¸= E
shows that rank (A) = n = 3. Indeed, it is also obvious from E that applying
back substitution in the system [E|0] yields only the trivial solution.
Example 2.4.2
Problem: Explain why the following homogeneous system has inï¬nitely many
solutions, and exhibit the general solution:
x1 + 2x2 + 2x3 = 0,
2x1 + 5x2 + 7x3 = 0,
3x1 + 6x2 + 6x3 = 0.

2.4 Homogeneous Systems
61
Solution:
A =
ï£«
ï£­
1
2
2
2
5
7
3
6
6
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
1
2
2
0
1
3
0
0
0
ï£¶
ï£¸= E
shows that rank (A) = 2 < n = 3. Since the basic columns lie in positions
one and two, x1 and x2 are the basic variables while x3 is free. Using back
substitution on [E|0] to solve for the basic variables in terms of the free variable
produces x2 = âˆ’3x3 and x1 = âˆ’2x2 âˆ’2x3 = 4x3, so the general solution is
ï£«
ï£­
x1
x2
x3
ï£¶
ï£¸= x3
ï£«
ï£­
4
âˆ’3
1
ï£¶
ï£¸,
where
x3 is free.
That is, every solution is a multiple of the one particular solution h1 =
ï£«
ï£­
4
âˆ’3
1
ï£¶
ï£¸.
Summary
Let AmÃ—n be the coeï¬ƒcient matrix for a homogeneous system of m
linear equations in n unknowns, and suppose rank (A) = r.
â€¢
The unknowns that correspond to the positions of the basic columns
(i.e., the pivotal positions) are called the basic variables, and the
unknowns corresponding to the positions of the nonbasic columns
are called the free variables.
â€¢
There are exactly r basic variables and n âˆ’r free variables.
â€¢
To describe all solutions, reduce A to a row echelon form using
Gaussian elimination, and then use back substitution to solve for
the basic variables in terms of the free variables. This produces the
general solution that has the form
x = xf1h1 + xf2h2 + Â· Â· Â· + xfnâˆ’rhnâˆ’r,
where the terms xf1, xf2, . . . , xfnâˆ’r are the free variables and where
h1, h2, . . . , hnâˆ’r are n Ã— 1 columns that represent particular solu-
tions of the homogeneous system. The hi â€™s are independent of which
row echelon form is used in the back substitution process. As the free
variables xfi range over all possible values, the general solution gen-
erates all possible solutions.
â€¢
A homogeneous system possesses a unique solution (the trivial solu-
tion) if and only if rank (A) = n â€”i.e., if and only if there are no
free variables.

62
Chapter 2
Rectangular Systems and Echelon Forms
Exercises for section 2.4
2.4.1. Determine the general solution for each of the following homogeneous
systems.
(a)
x1 + 2x2 + x3 + 2x4 = 0,
2x1 + 4x2 + x3 + 3x4 = 0,
3x1 + 6x2 + x3 + 4x4 = 0.
(b)
2x + y + z = 0,
4x + 2y + z = 0,
6x + 3y + z = 0,
8x + 4y + z = 0.
(c)
x1 + x2 + 2x3
= 0,
3x1
+ 3x3 + 3x4 = 0,
2x1 + x2 + 3x3 + x4 = 0,
x1 + 2x2 + 3x3 âˆ’x4 = 0.
(d)
2x + y + z = 0,
4x + 2y + z = 0,
6x + 3y + z = 0,
8x + 5y + z = 0.
2.4.2. Among all solutions that satisfy the homogeneous system
x + 2y + z = 0,
2x + 4y + z = 0,
x + 2y âˆ’z = 0,
determine those that also satisfy the nonlinear constraint y âˆ’xy = 2z.
2.4.3. Consider a homogeneous system whose coeï¬ƒcient matrix is
A =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1
2
1
3
1
2
4
âˆ’1
3
8
1
2
3
5
7
2
4
2
6
2
3
6
1
7
âˆ’3
ï£¶
ï£·
ï£·
ï£·
ï£¸.
First transform A to an unreduced row echelon form to determine the
general solution of the associated homogeneous system. Then reduce A
to EA, and show that the same general solution is produced.
2.4.4. If A is the coeï¬ƒcient matrix for a homogeneous system consisting of
four equations in eight unknowns and if there are ï¬ve free variables,
what is rank (A)?

2.4 Homogeneous Systems
63
2.4.5. Suppose that A is the coeï¬ƒcient matrix for a homogeneous system of
four equations in six unknowns and suppose that A has at least one
nonzero row.
(a)
Determine the fewest number of free variables that are possible.
(b)
Determine the maximum number of free variables that are pos-
sible.
2.4.6. Explain why a homogeneous system of m equations in n unknowns
where m < n must always possess an inï¬nite number of solutions.
2.4.7. Construct a homogeneous system of three equations in four unknowns
that has
x2
ï£«
ï£¬
ï£­
âˆ’2
1
0
0
ï£¶
ï£·
ï£¸+ x4
ï£«
ï£¬
ï£­
âˆ’3
0
2
1
ï£¶
ï£·
ï£¸
as its general solution.
2.4.8. If c1 and c2 are columns that represent two particular solutions of
the same homogeneous system, explain why the sum c1 + c2 must also
represent a solution of this system.

64
Chapter 2
Rectangular Systems and Echelon Forms
2.5
NONHOMOGENEOUS SYSTEMS
Recall that a system of m linear equations in n unknowns
a11x1 +
a12x2 + Â· Â· Â· +
a1nxn =
b1,
a21x1 +
a22x2 + Â· Â· Â· +
a2nxn =
b2,
...
am1x1 + am2x2 + Â· Â· Â· + amnxn = bm,
is said to be nonhomogeneous whenever bi Ì¸= 0 for at least one i. Unlike
homogeneous systems, a nonhomogeneous system may be inconsistent and the
techniques of Â§2.3 must be applied in order to determine if solutions do indeed
exist. Unless otherwise stated, it is assumed that all systems in this section are
consistent.
To describe the set of all possible solutions of a consistent nonhomogeneous
system, construct a general solution by exactly the same method used for homo-
geneous systems as follows.
â€¢
Use Gaussian elimination to reduce the associated augmented matrix [A|b]
to a row echelon form [E|c].
â€¢
Identify the basic variables and the free variables in the same manner de-
scribed in Â§2.4.
â€¢
Apply back substitution to [E|c] and solve for the basic variables in terms
of the free variables.
â€¢
Write the result in the form
x = p + xf1h1 + xf2h2 + Â· Â· Â· + xfnâˆ’rhnâˆ’r,
(2.5.1)
where xf1, xf2, . . . , xfnâˆ’r are the free variables and p, h1, h2, . . . , hnâˆ’r are
n Ã— 1 columns. This is the general solution of the nonhomogeneous system.
As the free variables xfi range over all possible values, the general solu-
tion (2.5.1) generates all possible solutions of the system [A|b]. Just as in the
homogeneous case, the columns hi and p are independent of which row eche-
lon form [E|c] is used. Therefore, [A|b] may be completely reduced to E[A|b]
by using the Gaussâ€“Jordan method thereby avoiding the need to perform back
substitution. We will use this approach whenever it is convenient.
The diï¬€erence between the general solution of a nonhomogeneous system
and the general solution of a homogeneous system is the column p that appears

2.5 Nonhomogeneous Systems
65
in (2.5.1). To understand why p appears and where it comes from, consider the
nonhomogeneous system
x1 + 2x2 + 2x3 + 3x4 = 4,
2x1 + 4x2 + x3 + 3x4 = 5,
3x1 + 6x2 + x3 + 4x4 = 7,
(2.5.2)
in which the coeï¬ƒcient matrix is the same as the coeï¬ƒcient matrix for the
homogeneous system (2.4.1) used in the previous section. If [A|b] is completely
reduced by the Gaussâ€“Jordan procedure to E[A|b]
[A|b] =
ï£«
ï£­
1
2
2
3
4
2
4
1
3
5
3
6
1
4
7
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
1
2
0
1
2
0
0
1
1
1
0
0
0
0
0
ï£¶
ï£¸= E[A|b],
then the following reduced system is obtained:
x1 + 2x2 + x4 = 2,
x3 + x4 = 1.
Solving for the basic variables, x1 and x3, in terms of the free variables, x2
and x4, produces
x1 = 2 âˆ’2x2 âˆ’x4,
x2 is â€œfree,â€
x3 = 1 âˆ’x4,
x4 is â€œfree.â€
The general solution is obtained by writing these statements in the form
ï£«
ï£¬
ï£­
x1
x2
x3
x4
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
2 âˆ’2x2 âˆ’x4
x2
1 âˆ’x4
x4
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
2
0
1
0
ï£¶
ï£·
ï£¸+ x2
ï£«
ï£¬
ï£­
âˆ’2
1
0
0
ï£¶
ï£·
ï£¸+ x4
ï£«
ï£¬
ï£­
âˆ’1
0
âˆ’1
1
ï£¶
ï£·
ï£¸.
(2.5.3)
As the free variables x2 and x4 range over all possible numbers, this generates
all possible solutions of the nonhomogeneous system (2.5.2). Notice that the
column
ï£«
ï£¬
ï£­
2
0
1
0
ï£¶
ï£·
ï£¸in (2.5.3) is a particular solution of the nonhomogeneous system
(2.5.2)â€”it is the solution produced when the free variables assume the values
x2 = 0 and x4 = 0.

66
Chapter 2
Rectangular Systems and Echelon Forms
Furthermore, recall from (2.4.4) that the general solution of the associated
homogeneous system
x1 + 2x2 + 2x3 + 3x4 = 0,
2x1 + 4x2 + x3 + 3x4 = 0,
3x1 + 6x2 + x3 + 4x4 = 0,
(2.5.4)
is given by
ï£«
ï£¬
ï£­
âˆ’2x2 âˆ’x4
x2
âˆ’x4
x4
ï£¶
ï£·
ï£¸= x2
ï£«
ï£¬
ï£­
âˆ’2
1
0
0
ï£¶
ï£·
ï£¸+ x4
ï£«
ï£¬
ï£­
âˆ’1
0
âˆ’1
1
ï£¶
ï£·
ï£¸.
That is, the general solution of the associated homogeneous system (2.5.4) is a
part of the general solution of the original nonhomogeneous system (2.5.2).
These two observations can be combined by saying that the general solution
of the nonhomogeneous system is given by a particular solution plus the general
solution of the associated homogeneous system.
14
To see that the previous statement is always true, suppose [A|b] represents
a general m Ã— n consistent system where rank (A) = r. Consistency guarantees
that b is a nonbasic column in [A|b], and hence the basic columns in [A|b] are
in the same positions as the basic columns in [A|0] so that the nonhomogeneous
system and the associated homogeneous system have exactly the same set of basic
variables as well as free variables. Furthermore, it is not diï¬ƒcult to see that
E[A|0] = [EA|0]
and
E[A|b] = [EA|c],
where c is some column of the form c =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î¾1
...
Î¾r
0
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
. This means that if you solve
the ith equation in the reduced homogeneous system for the ith basic variable
xbi in terms of the free variables xfi, xfi+1, . . . , xfnâˆ’r to produce
xbi = Î±ixfi + Î±i+1xfi+1 + Â· Â· Â· + Î±nâˆ’rxfnâˆ’r,
(2.5.5)
then the solution for the ith basic variable in the reduced nonhomogeneous
system must have the form
xbi = Î¾i + Î±ixfi + Î±i+1xfi+1 + Â· Â· Â· + Î±nâˆ’rxfnâˆ’r.
(2.5.6)
14
For those students who have studied diï¬€erential equations, this statement should have a familiar
ring. Exactly the same situation holds for the general solution to a linear diï¬€erential equation.
This is no accidentâ€”it is due to the inherent linearity in both problems. More will be said
about this issue later in the text.

2.5 Nonhomogeneous Systems
67
That is, the two solutions diï¬€er only in the fact that the latter contains the
constant Î¾i. Consider organizing the expressions (2.5.5) and (2.5.6) so as to
construct the respective general solutions. If the general solution of the homoge-
neous system has the form
x = xf1h1 + xf2h2 + Â· Â· Â· + xfnâˆ’rhnâˆ’r,
then it is apparent that the general solution of the nonhomogeneous system must
have a similar form
x = p + xf1h1 + xf2h2 + Â· Â· Â· + xfnâˆ’rhnâˆ’r
(2.5.7)
in which the column p contains the constants Î¾i along with some 0â€™sâ€”the Î¾i â€™s
occupy positions in p that correspond to the positions of the basic columns, and
0â€™s occupy all other positions. The column p represents one particular solution
to the nonhomogeneous system because it is the solution produced when the free
variables assume the values xf1 = xf2 = Â· Â· Â· = xfnâˆ’r = 0.
Example 2.5.1
Problem: Determine the general solution of the following nonhomogeneous sys-
tem and compare it with the general solution of the associated homogeneous
system:
x1 + x2 + 2x3 + 2x4 + x5 = 1,
2x1 + 2x2 + 4x3 + 4x4 + 3x5 = 1,
2x1 + 2x2 + 4x3 + 4x4 + 2x5 = 2,
3x1 + 5x2 + 8x3 + 6x4 + 5x5 = 3.
Solution: Reducing the augmented matrix [A|b] to E[A|b] yields
A =
ï£«
ï£¬
ï£­
1
1
2
2
1
1
2
2
4
4
3
1
2
2
4
4
2
2
3
5
8
6
5
3
ï£¶
ï£·
ï£¸âˆ’â†’
ï£«
ï£¬
ï£­
1
1
2
2
1
1
0
0
0
0
1
âˆ’1
0
0
0
0
0
0
0
2
2
0
2
0
ï£¶
ï£·
ï£¸
âˆ’â†’
ï£«
ï£¬
ï£­
1
1
2
2
1
1
0
2
2
0
2
0
0
0
0
0
1
âˆ’1
0
0
0
0
0
0
ï£¶
ï£·
ï£¸âˆ’â†’
ï£«
ï£¬
ï£­
1
1
2
2
1
1
0
1
1
0
1
0
0
0
0
0
1
âˆ’1
0
0
0
0
0
0
ï£¶
ï£·
ï£¸
âˆ’â†’
ï£«
ï£¬
ï£­
1
0
1
2
0
1
0
1
1
0
1
0
0
0
0
0
1
âˆ’1
0
0
0
0
0
0
ï£¶
ï£·
ï£¸âˆ’â†’
ï£«
ï£¬
ï£­
1
0
1
2
0
1
0
1
1
0
0
1
0
0
0
0
1
âˆ’1
0
0
0
0
0
0
ï£¶
ï£·
ï£¸= E[A|b].

68
Chapter 2
Rectangular Systems and Echelon Forms
Observe that the system is indeed consistent because the last column is nonbasic.
Solve the reduced system for the basic variables x1, x2, and x5 in terms of the
free variables x3 and x4 to obtain
x1 = 1 âˆ’x3 âˆ’2x4,
x2 = 1 âˆ’x3,
x3 is â€œfree,â€
x4 is â€œfree,â€
x5 = âˆ’1.
The general solution to the nonhomogeneous system is
x =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
x1
x2
x3
x4
x5
ï£¶
ï£·
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1 âˆ’x3 âˆ’2x4
1 âˆ’x3
x3
x4
âˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1
1
0
0
âˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£¸+ x3
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’1
âˆ’1
1
0
0
ï£¶
ï£·
ï£·
ï£·
ï£¸+ x4
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’2
0
0
1
0
ï£¶
ï£·
ï£·
ï£·
ï£¸.
The general solution of the associated homogeneous system is
x =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
x1
x2
x3
x4
x5
ï£¶
ï£·
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’x3 âˆ’2x4
âˆ’x3
x3
x4
0
ï£¶
ï£·
ï£·
ï£·
ï£¸= x3
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’1
âˆ’1
1
0
0
ï£¶
ï£·
ï£·
ï£·
ï£¸+ x4
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’2
0
0
1
0
ï£¶
ï£·
ï£·
ï£·
ï£¸.
You should verify for yourself that
p =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1
1
0
0
âˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£¸
is indeed a particular solution to the nonhomogeneous system and that
h3 =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’1
âˆ’1
1
0
0
ï£¶
ï£·
ï£·
ï£·
ï£¸
and
h4 =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’2
0
0
1
0
ï£¶
ï£·
ï£·
ï£·
ï£¸
are particular solutions to the associated homogeneous system.

2.5 Nonhomogeneous Systems
69
Now turn to the question, â€œWhen does a consistent system have a unique
solution?â€ It is known from (2.5.7) that the general solution of a consistent
m Ã— n nonhomogeneous system [A|b] with rank (A) = r is given by
x = p + xf1h1 + xf2h2 + Â· Â· Â· + xfnâˆ’rhnâˆ’r,
where
xf1h1 + xf2h2 + Â· Â· Â· + xfnâˆ’rhnâˆ’r
is the general solution of the associated homogeneous system. Consequently, it
is evident that the nonhomogeneous system [A|b] will have a unique solution
(namely, p ) if and only if there are no free variablesâ€”i.e., if and only if r = n
(= number of unknowns)â€”this is equivalent to saying that the associated ho-
mogeneous system [A|0] has only the trivial solution.
Example 2.5.2
Consider the following nonhomogeneous system:
2x1 + 4x2 + 6x3 = 2,
x1 + 2x2 + 3x3 = 1,
x1
+ x3 = âˆ’3,
2x1 + 4x2
= 8.
Reducing [A|b] to E[A|b] yields
[A|b] =
ï£«
ï£¬
ï£­
2
4
6
2
1
2
3
1
1
0
1
âˆ’3
2
4
0
8
ï£¶
ï£·
ï£¸âˆ’â†’
ï£«
ï£¬
ï£­
1
0
0
âˆ’2
0
1
0
3
0
0
1
âˆ’1
0
0
0
0
ï£¶
ï£·
ï£¸= E[A|b].
The system is consistent because the last column is nonbasic. There are several
ways to see that the system has a unique solution. Notice that
rank (A) = 3 = number of unknowns,
which is the same as observing that there are no free variables. Furthermore,
the associated homogeneous system clearly has only the trivial solution. Finally,
because we completely reduced [A|b] to E[A|b], it is obvious that there is only
one solution possible and that it is given by p =
ï£«
ï£­
âˆ’2
3
âˆ’1
ï£¶
ï£¸.

70
Chapter 2
Rectangular Systems and Echelon Forms
Summary
Let [A|b] be the augmented matrix for a consistent m Ã— n nonhomo-
geneous system in which rank (A) = r.
â€¢
Reducing [A|b] to a row echelon form using Gaussian elimination
and then solving for the basic variables in terms of the free variables
leads to the general solution
x = p + xf1h1 + xf2h2 + Â· Â· Â· + xfnâˆ’rhnâˆ’r.
As the free variables xfi range over all possible values, this general
solution generates all possible solutions of the system.
â€¢
Column p is a particular solution of the nonhomogeneous system.
â€¢
The expression xf1h1 + xf2h2 + Â· Â· Â· + xfnâˆ’rhnâˆ’r is the general so-
lution of the associated homogeneous system.
â€¢
Column p as well as the columns hi are independent of the row
echelon form to which [A|b] is reduced.
â€¢
The system possesses a unique solution if and only if any of the
following is true.
â–·
rank (A) = n = number of unknowns.
â–·
There are no free variables.
â–·
The associated homogeneous system possesses only the trivial
solution.
Exercises for section 2.5
2.5.1. Determine the general solution for each of the following nonhomogeneous
systems.
(a)
x1 + 2x2 + x3 + 2x4 = 3,
2x1 + 4x2 + x3 + 3x4 = 4,
3x1 + 6x2 + x3 + 4x4 = 5.
(b)
2x + y + z = 4,
4x + 2y + z = 6,
6x + 3y + z = 8,
8x + 4y + z = 10.
(c)
x1 + x2 + 2x3
= 1,
3x1
+ 3x3 + 3x4 = 6,
2x1 + x2 + 3x3 + x4 = 3,
x1 + 2x2 + 3x3 âˆ’x4 = 0.
(d)
2x + y + z = 2,
4x + 2y + z = 5,
6x + 3y + z = 8,
8x + 5y + z = 8.

2.5 Nonhomogeneous Systems
71
2.5.2. Among the solutions that satisfy the set of linear equations
x1 + x2 + 2x3 + 2x4 + x5 = 1,
2x1 + 2x2 + 4x3 + 4x4 + 3x5 = 1,
2x1 + 2x2 + 4x3 + 4x4 + 2x5 = 2,
3x1 + 5x2 + 8x3 + 6x4 + 5x5 = 3,
ï¬nd all those that also satisfy the following two constraints:
(x1 âˆ’x2)2 âˆ’4x2
5 = 0,
x2
3 âˆ’x2
5 = 0.
2.5.3. In order to grow a certain crop, it is recommended that each square foot
of ground be treated with 10 units of phosphorous, 9 units of potassium,
and 19 units of nitrogen. Suppose that there are three brands of fertilizer
on the marketâ€”say brand X, brand Y, and brand Z. One pound of
brand X contains 2 units of phosphorous, 3 units of potassium, and 5
units of nitrogen. One pound of brand Y contains 1 unit of phosphorous,
3 units of potassium, and 4 units of nitrogen. One pound of brand Z
contains only 1 unit of phosphorous and 1 unit of nitrogen.
(a)
Take into account the obvious fact that a negative number of
pounds of any brand can never be applied, and suppose that
because of the way fertilizer is sold only an integral number of
pounds of each brand will be applied. Under these constraints,
determine all possible combinations of the three brands that can
be applied to satisfy the recommendations exactly.
(b)
Suppose that brand X costs $1 per pound, brand Y costs $6
per pound, and brand Z costs $3 per pound. Determine the
least expensive solution that will satisfy the recommendations
exactly as well as the constraints of part (a).
2.5.4. Consider the following system:
2x + 2y + 3z = 0,
4x + 8y + 12z = âˆ’4,
6x + 2y + Î±z = 4.
(a)
Determine all values of Î± for which the system is consistent.
(b)
Determine all values of Î± for which there is a unique solution,
and compute the solution for these cases.
(c)
Determine all values of Î± for which there are inï¬nitely many
diï¬€erent solutions, and give the general solution for these cases.

72
Chapter 2
Rectangular Systems and Echelon Forms
2.5.5. If columns s1 and s2 are particular solutions of the same nonhomo-
geneous system, must it be the case that the sum s1 + s2 is also a
solution?
2.5.6. Suppose that [A|b] is the augmented matrix for a consistent system of
m equations in n unknowns where m â‰¥n. What must EA look like
when the system possesses a unique solution?
2.5.7. Construct a nonhomogeneous system of three equations in four un-
knowns that has
ï£«
ï£¬
ï£­
1
0
1
0
ï£¶
ï£·
ï£¸+ x2
ï£«
ï£¬
ï£­
âˆ’2
1
0
0
ï£¶
ï£·
ï£¸+ x4
ï£«
ï£¬
ï£­
âˆ’3
0
2
1
ï£¶
ï£·
ï£¸
as its general solution.
2.5.8. Consider using ï¬‚oating-point arithmetic (without partial pivoting or
scaling) to solve the system represented by the following augmented
matrix:
ï£«
ï£­
.835
.667
.5
.168
.333
.266
.1994
.067
1.67
1.334
1.1
.436
ï£¶
ï£¸.
(a)
Determine the 4-digit general solution.
(b)
Determine the 5-digit general solution.
(c)
Determine the 6-digit general solution.

2.6 Electrical Circuits
73
2.6
ELECTRICAL CIRCUITS
The theory of electrical circuits is an important application that naturally gives
rise to rectangular systems of linear equations. Because the underlying mathe-
matics depends on several of the concepts discussed in the preceding sections,
you may ï¬nd it interesting and worthwhile to make a small excursion into the
elementary mathematical analysis of electrical circuits. However, the continuity
of the text is not compromised by omitting this section.
In a direct current circuit containing resistances and sources of electromo-
tive force (abbreviated EMF) such as batteries, a point at which three or more
conductors are joined is called a node or branch point of the circuit, and a
closed conduction path is called a loop. Any part of a circuit between two ad-
joining nodes is called a branch of the circuit. The circuit shown in Figure 2.6.1
is a typical example that contains four nodes, seven loops, and six branches.
1
2
3
4
R1
R6
R5
R4
R3
R2
E4
E3
E2
E1
I2
I4
I1
I5
I6
I3
A
B
C
Figure 2.6.1
The problem is to relate the currents Ik in each branch to the resistances Rk
and the EMFs Ek.
15 This is accomplished by using Ohmâ€™s law in conjunction
with Kirchhoï¬€â€™s rules to produce a system of linear equations.
Ohmâ€™s Law
Ohmâ€™s law states that for a current of I amps, the voltage drop (in
volts) across a resistance of R ohms is given by V = IR.
Kirchhoï¬€â€™s rulesâ€”formally stated belowâ€”are the two fundamental laws
that govern the study of electrical circuits.
15
For an EMF source of magnitude E and a current I, there is always a small internal resistance
in the source, and the voltage drop across it is V = Eâˆ’I Ã—(internal resistance). But internal
source resistance is usually negligible, so the voltage drop across the source can be taken as
V = E. When internal resistance cannot be ignored, its eï¬€ects may be incorporated into
existing external resistances, or it can be treated as a separate external resistance.

74
Chapter 2
Rectangular Systems and Echelon Forms
Kirchhoffâ€™s Rules
NODE RULE:
The algebraic sum of currents toward each node is zero.
That is, the total incoming current must equal the total
outgoing current. This is simply a statement of conser-
vation of charge.
LOOP RULE:
The algebraic sum of the EMFs around each loop must
equal the algebraic sum of the IR products in the same
loop. That is, assuming internal source resistances have
been accounted for, the algebraic sum of the voltage
drops over the sources equals the algebraic sum of the
voltage drops over the resistances in each loop. This is
a statement of conservation of energy.
Kirchhoï¬€â€™s rules may be used without knowing the directions of the currents
and EMFs in advance. You may arbitrarily assign directions. If negative values
emerge in the ï¬nal solution, then the actual direction is opposite to that assumed.
To apply the node rule, consider a current to be positive if its direction is toward
the nodeâ€”otherwise, consider the current to be negative. It should be clear that
the node rule will always generate a homogeneous system. For example, applying
the node rule to the circuit in Figure 2.6.1 yields four homogeneous equations in
six unknownsâ€”the unknowns are the Ik â€™s:
Node 1:
I1 âˆ’I2 âˆ’I5 = 0,
Node 2:
âˆ’I1 âˆ’I3 + I4 = 0,
Node 3:
I3 + I5 + I6 = 0,
Node 4:
I2 âˆ’I4 âˆ’I6 = 0.
To apply the loop rule, some direction (clockwise or counterclockwise) must
be chosen as the positive direction, and all EMFs and currents in that direction
are considered positive and those in the opposite direction are negative. It is
possible for a current to be considered positive for the node rule but considered
negative when it is used in the loop rule. If the positive direction is considered
to be clockwise in each case, then applying the loop rule to the three indicated
loops A, B, and C in the circuit shown in Figure 2.6.1 produces the three non-
homogeneous equations in six unknownsâ€”the Ik â€™s are treated as the unknowns,
while the Rkâ€™s and Ekâ€™s are assumed to be known.
Loop A:
I1R1 âˆ’I3R3 + I5R5 = E1 âˆ’E3,
Loop B:
I2R2 âˆ’I5R5 + I6R6 = E2,
Loop C:
I3R3 + I4R4 âˆ’I6R6 = E3 + E4.

2.6 Electrical Circuits
75
There are 4 additional loops that also produce loop equations thereby mak-
ing a total of 11 equations (4 nodal equations and 7 loop equations) in 6 un-
knowns. Although this appears to be a rather general 11 Ã— 6 system of equations,
it really is not. If the circuit is in a state of equilibrium, then the physics of the
situation dictates that for each set of EMFs Ek, the corresponding currents
Ik must be uniquely determined. In other words, physics guarantees that the
11 Ã— 6 system produced by applying the two Kirchhoï¬€rules must be consistent
and possess a unique solution.
Suppose that [A|b] represents the augmented matrix for the 11 Ã— 6 system
generated by Kirchhoï¬€â€™s rules. From the results in Â§2.5, we know that the system
has a unique solution if and only if
rank (A) = number of unknowns = 6.
Furthermore, it was demonstrated in Â§2.3 that the system is consistent if and
only if
rank[A|b] = rank (A).
Combining these two facts allows us to conclude that
rank[A|b] = 6
so that when [A|b] is reduced to E[A|b], there will be exactly 6 nonzero rows
and 5 zero rows. Therefore, 5 of the original 11 equations are redundant in the
sense that they can be â€œzeroed outâ€ by forming combinations of some particular
set of 6 â€œindependentâ€ equations. It is desirable to know beforehand which of
the 11 equations will be redundant and which can act as the â€œindependentâ€ set.
Notice that in using the node rule, the equation corresponding to node 4
is simply the negative sum of the equations for nodes 1, 2, and 3, and that the
ï¬rst three equations are independent in the sense that no one of the three can
be written as a combination of any other two. This situation is typical. For a
general circuit with n nodes, it can be demonstrated that the equations for
the ï¬rst n âˆ’1 nodes are independent, and the equation for the last node is
redundant.
The loop rule also can generate redundant equations. Only simple loopsâ€”
loops not containing smaller loopsâ€”give rise to independent equations. For ex-
ample, consider the loop consisting of the three exterior branches in the circuit
shown in Figure 2.6.1. Applying the loop rule to this large loop will produce
no new information because the large loop can be constructed by â€œaddingâ€ the
three simple loops A,
B, and C contained within. The equation associated
with the large outside loop is
I1R1 + I2R2 + I4R4 = E1 + E2 + E4,
which is precisely the sum of the equations that correspond to the three compo-
nent loops A, B, and C. This phenomenon will hold in general so that only
the simple loops need to be considered when using the loop rule.

76
Chapter 2
Rectangular Systems and Echelon Forms
The point of this discussion is to conclude that the more general 11 Ã— 6
rectangular system can be replaced by an equivalent 6 Ã— 6 square system that
has a unique solution by dropping the last nodal equation and using only the
simple loop equations. This is characteristic of practical work in general. The
physics of a problem together with natural constraints can usually be employed
to replace a general rectangular system with one that is square and possesses a
unique solution.
One of the goals in our study is to understand more clearly the notion of
â€œindependenceâ€ that emerged in this application. So far, independence has been
an intuitive idea, but this example helps make it clear that independence is a
fundamentally important concept that deserves to be nailed down more ï¬rmly.
This is done in Â§4.3, and the general theory for obtaining independent equations
from electrical circuits is developed in Examples 4.4.6 and 4.4.7.
Exercises for section 2.6
2.6.1. Suppose that Ri = i ohms and Ei = i volts in the circuit shown in
Figure 2.6.1.
(a)
Determine the six indicated currents.
(b)
Select node number 1 to use as a reference point and ï¬x its
potential to be 0 volts. With respect to this reference, calculate
the potentials at the other three nodes. Check your answer by
verifying the loop rule for each loop in the circuit.
2.6.2. Determine the three currents indicated in the following circuit.
5â„¦
8â„¦
1â„¦
1â„¦
10â„¦
9 volts
12 volts
I1
I2
I3
2.6.3. Determine the two unknown EMFs in the following circuit.
1 amp
2 amps
20 volts
E1
E2
6â„¦
4â„¦
2â„¦

2.6 Electrical Circuits
77
2.6.4. Consider the circuit shown below and answer the following questions.
R1
R2
R3
R4
R5
R6
I
E
(a)
How many nodes does the circuit contain?
(b)
How many branches does the circuit contain?
(c)
Determine the total number of loops and then determine the
number of simple loops.
(d)
Demonstrate that the simple loop equations form an â€œindepen-
dentâ€ system of equations in the sense that there are no redun-
dant equations.
(e)
Verify that any three of the nodal equations constitute an â€œin-
dependentâ€ system of equations.
(f)
Verify that the loop equation associated with the loop containing
R1, R2, R3, and R4 can be expressed as the sum of the two
equations associated with the two simple loops contained in the
larger loop.
(g)
Determine the indicated current I if R1 = R2 = R3 = R4 = 1
ohm, R5 = R6 = 5 ohms, and E = 5 volts.

CHAPTER 3
Matrix
Algebra
3.1
FROM ANCIENT CHINA TO ARTHUR CAYLEY
The ancient Chinese appreciated the advantages of array manipulation in dealing
with systems of linear equations, and they possessed the seed that might have
germinated into a genuine theory of matrices. Unfortunately, in the year 213
B.C., emperor Shih Hoang-ti ordered that â€œall books be burned and all scholars
be buried.â€ It is presumed that the emperor wanted all knowledge and written
records to begin with him and his regime. The edict was carried out, and it will
never be known how much knowledge was lost. The book Chiu-chang Suan-shu
(Nine Chapters on Arithmetic), mentioned in the introduction to Chapter 1, was
compiled on the basis of remnants that survived.
More than a millennium passed before further progress was documented.
The Chinese counting board with its colored rods and its applications involving
array manipulation to solve linear systems eventually found its way to Japan.
Seki Kowa (1642â€“1708), whom many Japanese consider to be one of the greatest
mathematicians that their country has produced, carried forward the Chinese
principles involving â€œrule of thumbâ€ elimination methods on arrays of numbers.
His understanding of the elementary operations used in the Chinese elimination
process led him to formulate the concept of what we now call the determinant.
While formulating his ideas concerning the solution of linear systems, Seki Kowa
anticipated the fundamental concepts of array operations that today form the
basis for matrix algebra. However, there is no evidence that he developed his
array operations to actually construct an algebra for matrices.
From the middle 1600s to the middle 1800s, while Europe was ï¬‚owering
in mathematical development, the study of array manipulation was exclusively

80
Chapter 3
Matrix Algebra
dedicated to the theory of determinants. Curiously, matrix algebra did not evolve
along with the study of determinants.
It was not until the work of the British mathematician Arthur Cayley (1821â€“
1895) that the matrix was singled out as a separate entity, distinct from the
notion of a determinant, and algebraic operations between matrices were deï¬ned.
In an 1855 paper, Cayley ï¬rst introduced his basic ideas that were presented
mainly to simplify notation. Finally, in 1857, Cayley expanded on his original
ideas and wrote A Memoir on the Theory of Matrices. This laid the foundations
for the modern theory and is generally credited for being the birth of the subjects
of matrix analysis and linear algebra.
Arthur Cayley began his career by studying literature at Trinity College,
Cambridge (1838â€“1842), but developed a side interest in mathematics, which he
studied in his spare time. This â€œhobbyâ€ resulted in his ï¬rst mathematical paper
in 1841 when he was only 20 years old. To make a living, he entered the legal
profession and practiced law for 14 years. However, his main interest was still
mathematics. During the legal years alone, Cayley published almost 300 papers
in mathematics.
In 1850 Cayley crossed paths with James J. Sylvester, and between the two
of them matrix theory was born and nurtured. The two have been referred to
as the â€œinvariant twins.â€ Although Cayley and Sylvester shared many mathe-
matical interests, they were quite diï¬€erent people, especially in their approach
to mathematics. Cayley had an insatiable hunger for the subject, and he read
everything that he could lay his hands on. Sylvester, on the other hand, could
not stand the sight of papers written by others. Cayley never forgot anything
he had read or seenâ€”he became a living encyclopedia. Sylvester, so it is said,
would frequently fail to remember even his own theorems.
In 1863, Cayley was given a chair in mathematics at Cambridge University,
and thereafter his mathematical output was enormous. Only Cauchy and Euler
were as proliï¬c. Cayley often said, â€œI really love my subject,â€ and all indica-
tions substantiate that this was indeed the way he felt. He remained a working
mathematician until his death at age 74.
Because the idea of the determinant preceded concepts of matrix algebra by
at least two centuries, Morris Kline says in his book Mathematical Thought from
Ancient to Modern Times that â€œthe subject of matrix theory was well developed
before it was created.â€ This must have indeed been the case because immediately
after the publication of Cayleyâ€™s memoir, the subjects of matrix theory and linear
algebra virtually exploded and quickly evolved into a discipline that now occupies
a central position in applied mathematics.

3.2 Addition and Transposition
81
3.2
ADDITION AND TRANSPOSITION
In the previous chapters, matrix language and notation were used simply to for-
mulate some of the elementary concepts surrounding linear systems. The purpose
now is to turn this language into a mathematical theory.
16
Unless otherwise stated, a scalar is a complex number. Real numbers are
a subset of the complex numbers, and hence real numbers are also scalar quan-
tities. In the early stages, there is little harm in thinking only in terms of real
scalars. Later on, however, the necessity for dealing with complex numbers will
be unavoidable. Throughout the text, â„œwill denote the set of real numbers,
and C will denote the complex numbers. The set of all n -tuples of real numbers
will be denoted by â„œn, and the set of all complex n -tuples will be denoted
by Cn. For example, â„œ2 is the set of all ordered pairs of real numbers (i.e.,
the standard cartesian plane), and â„œ3 is ordinary 3-space. Analogously, â„œmÃ—n
and CmÃ—n denote the m Ã— n matrices containing real numbers and complex
numbers, respectively.
Matrices A = [aij] and B = [bij] are deï¬ned to be equal matrices
when A and B have the same shape and corresponding entries are equal. That
is, aij = bij for each i = 1, 2, . . . , m and j = 1, 2, . . . , n. In particular, this
deï¬nition applies to arrays such as u =
ï£«
ï£­
1
2
3
ï£¶
ï£¸and v = ( 1
2
3 ) . Even
though u and v describe exactly the same point in 3-space, we cannot consider
them to be equal matrices because they have diï¬€erent shapes. An array (or
matrix) consisting of a single column, such as u, is called a column vector,
while an array consisting of a single row, such as v, is called a row vector.
Addition of Matrices
If A and B are m Ã— n matrices, the sum of A and B is deï¬ned to
be the m Ã— n matrix A+B obtained by adding corresponding entries.
That is,
[A + B]ij = [A]ij + [B]ij
for each i and j.
For example,

âˆ’2
x
3
z + 3
4
âˆ’y

+

2
1 âˆ’x
âˆ’2
âˆ’3
4 + x
4 + y

=

0
1
1
z
8 + x
4

.
16
The great French mathematician Pierre-Simon Laplace (1749â€“1827) said that, â€œSuch is the ad-
vantage of a well-constructed language that its simpliï¬ed notation often becomes the source of
profound theories.â€ The theory of matrices is a testament to the validity of Laplaceâ€™s statement.

82
Chapter 3
Matrix Algebra
The symbol â€œ+â€ is used two diï¬€erent waysâ€”it denotes addition between
scalars in some places and addition between matrices at other places. Although
these are two distinct algebraic operations, no ambiguities will arise if the context
in which â€œ+â€ appears is observed. Also note that the requirement that A and
B have the same shape prevents adding a row to a column, even though the two
may contain the same number of entries.
The matrix (âˆ’A), called the additive inverse of A, is deï¬ned to be
the matrix obtained by negating each entry of A. That is, if A = [aij], then
âˆ’A = [âˆ’aij]. This allows matrix subtraction to be deï¬ned in the natural way.
For two matrices of the same shape, the diï¬€erence A âˆ’B is deï¬ned to be the
matrix A âˆ’B = A + (âˆ’B) so that
[A âˆ’B]ij = [A]ij âˆ’[B]ij
for each i and j.
Since matrix addition is deï¬ned in terms of scalar addition, the familiar algebraic
properties of scalar addition are inherited by matrix addition as detailed below.
Properties of Matrix Addition
For m Ã— n matrices A, B, and C, the following properties hold.
Closure property:
A + B is again an m Ã— n matrix.
Associative property:
(A + B) + C = A + (B + C).
Commutative property:
A + B = B + A.
Additive identity:
The m Ã— n matrix 0 consisting of all ze-
ros has the property that A + 0 = A.
Additive inverse:
The m Ã— n matrix (âˆ’A) has the property
that A + (âˆ’A) = 0.
Another simple operation that is derived from scalar arithmetic is as follows.
Scalar Multiplication
The product of a scalar Î± times a matrix A, denoted by Î±A, is deï¬ned
to be the matrix obtained by multiplying each entry of A by Î±. That
is, [Î±A]ij = Î±[A]ij for each i and j.
For example,
2
ï£«
ï£­
1
2
3
0
1
2
1
4
2
ï£¶
ï£¸=
ï£«
ï£­
2
4
6
0
2
4
2
8
4
ï£¶
ï£¸
and
ï£«
ï£­
1
2
3
4
0
1
ï£¶
ï£¸= 1
2
ï£«
ï£­
2
4
6
8
0
2
ï£¶
ï£¸.
The rules for combining addition and scalar multiplication are what you
might suspect they should be. Some of the important ones are listed below.

3.2 Addition and Transposition
83
Properties of Scalar Multiplication
For m Ã— n matrices A and B and for scalars Î± and Î², the following
properties hold.
Closure property:
Î±A is again an m Ã— n matrix.
Associative property:
(Î±Î²)A = Î±(Î²A).
Distributive property:
Î±(A + B) = Î±A + Î±B. Scalar multiplica-
tion is distributed over matrix addition.
Distributive property:
(Î± + Î²)A = Î±A + Î²A. Scalar multiplica-
tion is distributed over scalar addition.
Identity property:
1A = A. The number 1 is an identity el-
ement under scalar multiplication.
Other properties such as Î±A = AÎ± could have been listed, but the prop-
erties singled out pave the way for the deï¬nition of a vector space on p. 160.
A matrix operation thatâ€™s not derived from scalar arithmetic is transposition
as deï¬ned below.
Transpose
The transpose of AmÃ—n is deï¬ned to be the n Ã— m matrix AT ob-
tained by interchanging rows and columns in A. More precisely, if
A = [aij], then [AT ]ij = aji. For example,
ï£«
ï£­
1
2
3
4
5
6
ï£¶
ï£¸
T
=

1
3
5
2
4
6

.
It should be evident that for all matrices,

AT 	T = A.
Whenever a matrix contains complex entries, the operation of complex con-
jugation almost always accompanies the transpose operation. (Recall that the
complex conjugate of z = a + ib is deï¬ned to be z = a âˆ’ib.)

84
Chapter 3
Matrix Algebra
Conjugate Transpose
For A = [aij], the conjugate matrix is deï¬ned to be A = [aij] , and
the conjugate transpose of A is deï¬ned to be Â¯AT = AT . From now
on, Â¯AT will be denoted by Aâˆ—, so [Aâˆ—]ij = aji. For example,

1 âˆ’4i
i
2
3
2 + i
0
âˆ—
=
ï£«
ï£­
1 + 4i
3
âˆ’i
2 âˆ’i
2
0
ï£¶
ï£¸.
(Aâˆ—)âˆ—= A for all matrices, and Aâˆ—= AT whenever A contains only
real entries. Sometimes the matrix Aâˆ—is called the adjoint of A.
The transpose (and conjugate transpose) operation is easily combined with
matrix addition and scalar multiplication. The basic rules are given below.
Properties of the Transpose
If A and B are two matrices of the same shape, and if Î± is a scalar,
then each of the following statements is true.
(A + B)T = AT + BT
and
(A + B)âˆ—= Aâˆ—+ Bâˆ—.
(3.2.1)
(Î±A)T = Î±AT
and
(Î±A)âˆ—= Î±Aâˆ—.
(3.2.2)
Proof.
17
We will prove that (3.2.1) and (3.2.2) hold for the transpose operation.
The proofs of the statements involving conjugate transposes are similar and are
left as exercises. For each i and j, it is true that
[(A + B)T ]ij = [A + B]ji = [A]ji + [B]ji = [AT ]ij + [BT ]ij = [AT + BT ]ij.
17
Computers can outperform people in many respects in that they do arithmetic much faster
and more accurately than we can, and they are now rather adept at symbolic computation and
mechanical manipulation of formulas. But computers canâ€™t do mathematicsâ€”people still hold
the monopoly. Mathematics emanates from the uniquely human capacity to reason abstractly
in a creative and logical manner, and learning mathematics goes hand-in-hand with learning
how to reason abstractly and create logical arguments. This is true regardless of whether your
orientation is applied or theoretical. For this reason, formal proofs will appear more frequently
as the text evolves, and it is expected that your level of comprehension as well as your ability
to create proofs will grow as you proceed.

3.2 Addition and Transposition
85
This proves that corresponding entries in (A + B)T and AT + BT are equal,
so it must be the case that (A + B)T = AT + BT . Similarly, for each i and j,
[(Î±A)T ]ij = [Î±A]ji = Î±[A]ji = Î±[AT ]ij
=â‡’
(Î±A)T = Î±AT .
Sometimes transposition doesnâ€™t change anything. For example, if
A =
ï£«
ï£­
1
2
3
2
4
5
3
5
6
ï£¶
ï£¸,
then
AT = A.
This is because the entries in A are symmetrically located about the main di-
agonalâ€”the line from the upper-left-hand corner to the lower-right-hand corner.
Matrices of the form D =
ï£«
ï£­
Î»1
0
Â· Â· Â·
0
0
Î»2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
Î»n
ï£¶
ï£¸are called diagonal matrices,
and they are clearly symmetric in the sense that D = DT . This is one of several
kinds of symmetries described below.
Symmetries
Let A = [aij] be a square matrix.
â€¢
A is said to be a symmetric matrix whenever A = AT , i.e.,
whenever aij = aji.
â€¢
A is said to be a skew-symmetric matrix whenever A = âˆ’AT ,
i.e., whenever aij = âˆ’aji.
â€¢
A is said to be a hermitian matrix
whenever A = Aâˆ—, i.e.,
whenever aij = aji. This is the complex analog of symmetry.
â€¢
A is said to be a skew-hermitian matrix when A = âˆ’Aâˆ—, i.e.,
whenever aij = âˆ’aji. This is the complex analog of skew symmetry.
For example, consider
A =
ï£«
ï£­
1
2 + 4i
1 âˆ’3i
2 âˆ’4i
3
8 + 6i
1 + 3i
8 âˆ’6i
5
ï£¶
ï£¸
and
B =
ï£«
ï£­
1
2 + 4i
1 âˆ’3i
2 + 4i
3
8 + 6i
1 âˆ’3i
8 + 6i
5
ï£¶
ï£¸.
Can you see that A is hermitian but not symmetric, while B is symmetric but
not hermitian?
Nature abounds with symmetry, and very often physical symmetry manifests
itself as a symmetric matrix in a mathematical model. The following example is
an illustration of this principle.

86
Chapter 3
Matrix Algebra
Example 3.2.1
Consider two springs that are connected as shown in Figure 3.2.1.
x1
x2
x3
k1
k2
Node 1
Node 2
Node 3
F1
-F1
F3
-F3
Figure 3.2.1
The springs at the top represent the â€œno tensionâ€ position in which no force is
being exerted on any of the nodes. Suppose that the springs are stretched or
compressed so that the nodes are displaced as indicated in the lower portion
of Figure 3.2.1. Stretching or compressing the springs creates a force on each
node according to Hookeâ€™s law
18 that says that the force exerted by a spring
is F = kx, where x is the distance the spring is stretched or compressed and
where k is a stiï¬€ness constant inherent to the spring. Suppose our springs have
stiï¬€ness constants k1 and k2, and let Fi be the force on node i when the
springs are stretched or compressed. Letâ€™s agree that a displacement to the left
is positive, while a displacement to the right is negative, and consider a force
directed to the right to be positive while one directed to the left is negative.
If node 1 is displaced x1 units, and if node 2 is displaced x2 units, then the
left-hand spring is stretched (or compressed) by a total amount of x1 âˆ’x2 units,
so the force on node 1 is
F1 = k1(x1 âˆ’x2).
Similarly, if node 2 is displaced x2 units, and if node 3 is displaced x3 units,
then the right-hand spring is stretched by a total amount of x2 âˆ’x3 units, so
the force on node 3 is
F3 = âˆ’k2(x2 âˆ’x3).
The minus sign indicates the force is directed to the left. The force on the left-
hand side of node 2 is the opposite of the force on node 1, while the force on the
right-hand side of node 2 must be the opposite of the force on node 3. That is,
F2 = âˆ’F1 âˆ’F3.
18
Hookeâ€™s law is named for Robert Hooke (1635â€“1703), an English physicist, but it was generally
known to several people (including Newton) before Hookeâ€™s 1678 claim to it was made. Hooke
was a creative person who is credited with several inventions, including the wheel barometer,
but he was reputed to be a man of â€œterrible character.â€ This characteristic virtually destroyed
his scientiï¬c career as well as his personal life. It is said that he lacked mathematical sophis-
tication and that he left much of his work in incomplete form, but he bitterly resented people
who built on his ideas by expressing them in terms of elegant mathematical formulations.

3.2 Addition and Transposition
87
Organize the above three equations as a linear system:
k1x1
âˆ’k1x2
= F1,
âˆ’k1x1 + (k1 + k2)x2 âˆ’k2x3 = F2,
âˆ’k2x2 + k2x3 = F3,
and observe that the coeï¬ƒcient matrix, called the stiï¬€ness matrix,
K =
ï£«
ï£­
k1
âˆ’k1
0
âˆ’k1
k1 + k2
âˆ’k2
0
âˆ’k2
k2
ï£¶
ï£¸,
is a symmetric matrix. The point of this example is that symmetry in the physical
problem translates to symmetry in the mathematics by way of the symmetric
matrix K. When the two springs are identical (i.e., when k1 = k2 = k ), even
more symmetry is present, and in this case
K = k
ï£«
ï£­
1
âˆ’1
0
âˆ’1
2
âˆ’1
0
âˆ’1
1
ï£¶
ï£¸.
Exercises for section 3.2
3.2.1. Determine the unknown quantities in the following expressions.
(a)
3X =

0
3
6
9

.
(b)
2

x + 2
y + 3
3
0

=

3
6
y
z
T
.
3.2.2. Identify each of the following as symmetric, skew symmetric, or neither.
(a)
ï£«
ï£­
1
âˆ’3
3
âˆ’3
4
âˆ’3
3
3
0
ï£¶
ï£¸.
(b)
ï£«
ï£­
0
âˆ’3
âˆ’3
3
0
1
3
âˆ’1
0
ï£¶
ï£¸.
(c)
ï£«
ï£­
0
âˆ’3
âˆ’3
âˆ’3
0
3
âˆ’3
3
1
ï£¶
ï£¸.
(d)

1
2
0
2
1
0

.
3.2.3. Construct an example of a 3 Ã— 3 matrix A that satisï¬es the following
conditions.
(a)
A is both symmetric and skew symmetric.
(b)
A is both hermitian and symmetric.
(c)
A is skew hermitian.

88
Chapter 3
Matrix Algebra
3.2.4. Explain why the set of all n Ã— n symmetric matrices is closed under
matrix addition. That is, explain why the sum of two n Ã— n symmetric
matrices is again an n Ã— n symmetric matrix. Is the set of all n Ã— n
skew-symmetric matrices closed under matrix addition?
3.2.5. Prove that each of the following statements is true.
(a)
If A = [aij] is skew symmetric, then ajj = 0 for each j.
(b)
If A = [aij] is skew hermitian, then each ajj is a pure imagi-
nary numberâ€”i.e., a multiple of the imaginary unit i.
(c)
If A is real and symmetric, then B = iA is skew hermitian.
3.2.6. Let A be any square matrix.
(a)
Show that A+AT is symmetric and Aâˆ’AT is skew symmetric.
(b)
Prove that there is one and only one way to write A as the
sum of a symmetric matrix and a skew-symmetric matrix.
3.2.7. If A and B are two matrices of the same shape, prove that each of the
following statements is true.
(a)
(A + B)âˆ—= Aâˆ—+ Bâˆ—.
(b)
(Î±A)âˆ—= Î±Aâˆ—.
3.2.8. Using the conventions given in Example 3.2.1, determine the stiï¬€ness
matrix for a system of n identical springs, with stiï¬€ness constant k,
connected in a line similar to that shown in Figure 3.2.1.

3.3 Linearity
89
3.3
LINEARITY
The concept of linearity is the underlying theme of our subject. In elementary
mathematics the term â€œlinear functionâ€ refers to straight lines, but in higher
mathematics linearity means something much more general. Recall that a func-
tion f is simply a rule for associating points in one set D â€”called the domain
of f â€”to points in another set R â€”the range of f. A linear function is a
particular type of function that is characterized by the following two properties.
Linear Functions
Suppose that D and R are sets that possess an addition operation as
well as a scalar multiplication operationâ€”i.e., a multiplication between
scalars and set members. A function f that maps points in D to points
in R is said to be a linear function whenever f satisï¬es the conditions
that
f(x + y) = f(x) + f(y)
(3.3.1)
and
f(Î±x) = Î±f(x)
(3.3.2)
for every x and y in D and for all scalars Î±. These two conditions
may be combined by saying that f is a linear function whenever
f(Î±x + y) = Î±f(x) + f(y)
(3.3.3)
for all scalars Î± and for all x, y âˆˆD.
One of the simplest linear functions is f(x) = Î±x, whose graph in â„œ2 is a
straight line through the origin. You should convince yourself that f is indeed
a linear function according to the above deï¬nition. However, f(x) = Î±x + Î²
does not qualify for the title â€œlinear functionâ€â€”it is a linear function that has
been translated by a constant Î². Translations of linear functions are referred to
as aï¬ƒne functions. Virtually all information concerning aï¬ƒne functions can
be derived from an understanding of linear functions, and consequently we will
focus only on issues of linearity.
In â„œ3, the surface described by a function of the form
f(x1, x2) = Î±1x1 + Î±2x2
is a plane through the origin, and it is easy to verify that f is a linear function.
For Î² Ì¸= 0, the graph of f(x1, x2) = Î±1x1 + Î±2x2 + Î² is a plane not passing
through the origin, and f is no longer a linear functionâ€”it is an aï¬ƒne function.

90
Chapter 3
Matrix Algebra
In â„œ2 and â„œ3, the graphs of linear functions are lines and planes through
the origin, and there seems to be a pattern forming. Although we cannot visualize
higher dimensions with our eyes, it seems reasonable to suggest that a general
linear function of the form
f(x1, x2, . . . , xn) = Î±1x1 + Î±2x2 + Â· Â· Â· + Î±nxn
somehow represents a â€œlinearâ€ or â€œï¬‚atâ€ surface passing through the origin 0 =
(0, 0, . . . , 0) in â„œn+1. One of the goals of the next chapter is to learn how to
better interpret and understand this statement.
Linearity is encountered at every turn. For example, the familiar operations
of diï¬€erentiation and integration may be viewed as linear functions. Since
d(f + g)
dx
= df
dx + dg
dx
and
d(Î±f)
dx
= Î± df
dx,
the diï¬€erentiation operator Dx(f) = df/dx is linear. Similarly,

(f + g)dx =

fdx +

gdx
and

Î±fdx = Î±

fdx
means that the integration operator I(f) =

fdx is linear.
There are several important matrix functions that are linear. For example,
the transposition function f(XmÃ—n) = XT is linear because
(A + B)T = AT + BT
and
(Î±A)T = Î±AT
(recall (3.2.1) and (3.2.2)). Another matrix function that is linear is the trace
function presented below.
Example 3.3.1
The trace of an n Ã— n matrix A = [aij] is deï¬ned to be the sum of the entries
lying on the main diagonal of A. That is,
trace (A) = a11 + a22 + Â· Â· Â· + ann =
n

i=1
aii.
Problem: Show that f(XnÃ—n) = trace (X) is a linear function.
Solution: Letâ€™s be eï¬ƒcient by showing that (3.3.3) holds. Let A = [aij] and
B = [bij], and write
f(Î±A + B) = trace (Î±A + B) =
n

i=1
[Î±A + B]ii =
n

i=1
(Î±aii + bii)
=
n

i=1
Î±aii +
n

i=1
bii = Î±
n

i=1
aii +
n

i=1
bii = Î± trace (A) + trace (B)
= Î±f(A) + f(B).

3.3 Linearity
91
Example 3.3.2
Consider a linear system
a11x1 +
a12x2 + Â· Â· Â· +
a1nxn =
u1,
a21x1 +
a22x2 + Â· Â· Â· +
a2nxn =
u2,
...
am1x1 + am2x2 + Â· Â· Â· + amnxn = um,
to be a function u = f(x) that maps x =
ï£«
ï£¬
ï£¬
ï£­
x1
x2
...
xn
ï£¶
ï£·
ï£·
ï£¸âˆˆâ„œn to u =
ï£«
ï£¬
ï£¬
ï£­
u1
u2
...
um
ï£¶
ï£·
ï£·
ï£¸âˆˆâ„œm.
Problem: Show that u = f(x) is linear.
Solution: Let A = [aij] be the matrix of coeï¬ƒcients, and write
f(Î±x + y) = f
ï£«
ï£¬
ï£¬
ï£­
Î±x1 + y1
Î±x2 + y2
...
Î±xn + yn
ï£¶
ï£·
ï£·
ï£¸=
n

j=1
(Î±xj + yj)Aâˆ—j =
n

j=1
(Î±xjAâˆ—j + yjAâˆ—j)
=
n

j=1
Î±xjAâˆ—j +
n

j=1
yjAâˆ—j = Î±
n

j=1
xjAâˆ—j +
n

j=1
yjAâˆ—j
= Î±f(x) + f(y).
According to (3.3.3), the function f is linear.
The following terminology will be used from now on.
Linear Combinations
For scalars Î±j and matrices Xj, the expression
Î±1X1 + Î±2X2 + Â· Â· Â· + Î±nXn =
n

j=1
Î±jXj
is called a linear combination of the Xjâ€™s.

92
Chapter 3
Matrix Algebra
Exercises for section 3.3
3.3.1. Each of the following is a function from â„œ2 into â„œ2. Determine which
are linear functions.
(a)
f

x
y

=

x
1 + y

.
(b)
f

x
y

=

y
x

.
(c)
f

x
y

=

0
xy

.
(d)
f

x
y

=

x2
y2

.
(e)
f

x
y

=

x
sin y

.
(f)
f

x
y

=

x + y
x âˆ’y

.
3.3.2. For x =
ï£«
ï£¬
ï£¬
ï£­
x1
x2
...
xn
ï£¶
ï£·
ï£·
ï£¸, and for constants Î¾i, verify that
f(x) = Î¾1x1 + Î¾2x2 + Â· Â· Â· + Î¾nxn
is a linear function.
3.3.3. Give examples of at least two diï¬€erent physical principles or laws that
can be characterized as being linear phenomena.
3.3.4. Determine which of the following three transformations in â„œ2 are linear.
Î¸
f(p)
p
f(p)
p
y = x
f(p)
p
Rotate counterclockwise
through an angle Î¸.
Reflect about
the x -axis.
Project onto
the line y = x.

3.4 Why Do It This Way
93
3.4
WHY DO IT THIS WAY
If you were given the task of formulating a deï¬nition for composing two ma-
trices A and B in some sort of â€œnaturalâ€ multiplicative fashion, your ï¬rst
attempt would probably be to compose A and B by multiplying correspond-
ing entriesâ€”much the same way matrix addition is deï¬ned. Asked then to defend
the usefulness of such a deï¬nition, you might be hard pressed to provide a truly
satisfying response. Unless a person is in the right frame of mind, the issue of
deciding how to best deï¬ne matrix multiplication is not at all transparent, es-
pecially if it is insisted that the deï¬nition be both â€œnaturalâ€ and â€œuseful.â€ The
world had to wait for Arthur Cayley to come to this proper frame of mind.
As mentioned in Â§3.1, matrix algebra appeared late in the game. Manipula-
tion on arrays and the theory of determinants existed long before Cayley and his
theory of matrices. Perhaps this can be attributed to the fact that the â€œcorrectâ€
way to multiply two matrices eluded discovery for such a long time.
Around 1855, Cayley became interested in composing linear functions.
19 In
particular, he was investigating linear functions of the type discussed in Example
3.3.2. Typical examples of two such functions are
f(x) = f

x1
x2

=

ax1 + bx2
cx1 + dx2

and
g(x) = g

x1
x2

=

Ax1 + Bx2
Cx1 + Dx2

.
Consider, as Cayley did, composing f and g to create another linear function
h(x) = f

g(x)

= f

Ax1 + Bx2
Cx1 + Dx2

=

(aA + bC)x1 + (aB + bD)x2
(cA + dC)x1 + (cB + dD)x2

.
It was Cayleyâ€™s idea to use matrices of coeï¬ƒcients to represent these linear
functions. That is, f, g, and h are represented by
F =

a
b
c
d

,
G =

A
B
C
D

,
and
H =

aA + bC
aB + bD
cA + dC
cB + dD

.
After making this association, it was only natural for Cayley to call H the
composition (or product) of F and G, and to write

a
b
c
d
 
A
B
C
D

=

aA + bC
aB + bD
cA + dC
cB + dD

.
(3.4.1)
In other words, the product of two matrices represents the composition of the
two associated linear functions. By means of this observation, Cayley brought to
life the subjects of matrix analysis and linear algebra.
19
Cayley was not the ï¬rst to compose linear functions. In fact, Gauss used these compositions
as early as 1801, but not in the form of an array of coeï¬ƒcients. Cayley was the ï¬rst to make
the connection between composition of linear functions and the composition of the associated
matrices. Cayleyâ€™s work from 1855 to 1857 is regarded as being the birth of our subject.

94
Chapter 3
Matrix Algebra
Exercises for section 3.4
Each problem in this section concerns the following three linear transformations
in â„œ2.
Rotation:
Rotate points counterclockwise
through an angle Î¸.
Î¸
f(p)
p
Reï¬‚ection:
Reï¬‚ect points about the x -axis.
f(p)
p
Projection:
Project points onto the line
y = x in a perpendicular
manner.
y = x
f(p)
p
3.4.1. Determine the matrix associated with each of these linear functions.
That is, determine the aij â€™s such that
f(p) = f

x1
x2

=

a11x1 + a12x2
a21x1 + a22x2

.
3.4.2. By using matrix multiplication, determine the linear function obtained
by performing a rotation followed by a reï¬‚ection.
3.4.3. By using matrix multiplication, determine the linear function obtained
by ï¬rst performing a reï¬‚ection, then a rotation, and ï¬nally a projection.

3.5 Matrix Multiplication
95
3.5
MATRIX MULTIPLICATION
The purpose of this section is to further develop the concept of matrix multipli-
cation as introduced in the previous section. In order to do this, it is helpful to
begin by composing a single row with a single column. If
R = ( r1
r2
Â· Â· Â·
rn )
and
C =
ï£«
ï£¬
ï£¬
ï£­
c1
c2
...
cn
ï£¶
ï£·
ï£·
ï£¸,
the standard inner product of R with C is deï¬ned to be the scalar
RC = r1c1 + r2c2 + Â· Â· Â· + rncn =
n

i=1
rici.
For example,
( 2
4
âˆ’2 )
ï£«
ï£­
1
2
3
ï£¶
ï£¸= (2)(1) + (4)(2) + (âˆ’2)(3) = 4.
Recall from (3.4.1) that the product of two 2 Ã— 2 matrices
F =
	
a
b
c
d

and
G =
	
A
B
C
D

was deï¬ned naturally by writing
FG =
	
a
b
c
d

 	
A
B
C
D

=
	
aA + bC
aB + bD
cA + dC
cB + dD

= H.
Notice that the (i, j) -entry in the product H can be described as the inner
product of the ith row of F with the jth column in G. That is,
h11 = F1âˆ—Gâˆ—1 = ( a
b )
	
A
C

,
h12 = F1âˆ—Gâˆ—2 = ( a
b )
	
B
D

,
h21 = F2âˆ—Gâˆ—1 = ( c
d )
	
A
C

,
h22 = F2âˆ—Gâˆ—2 = ( c
d )
	
B
D

.
This is exactly the way that the general deï¬nition of matrix multiplication is
formulated.

96
Chapter 3
Matrix Algebra
Matrix Multiplication
â€¢
Matrices A and B are said to be conformable for multiplication
in the order AB whenever A has exactly as many columns as B
has rowsâ€”i.e., A is m Ã— p and B is p Ã— n.
â€¢
For conformable matrices AmÃ—p = [aij] and BpÃ—n = [bij], the
matrix product AB is deï¬ned to be the m Ã— n matrix whose
(i, j) -entry is the inner product of the ith row of A with the jth
column in B. That is,
[AB]ij = Aiâˆ—Bâˆ—j = ai1b1j + ai2b2j + Â· Â· Â· + aipbpj =
p

k=1
aikbkj.
â€¢
In case A and B fail to be conformableâ€”i.e., A is m Ã— p and B
is q Ã— n with p Ì¸= q â€”then no product AB is deï¬ned.
For example, if
A =

a11
a12
a13
a21
a22
a23

2Ã—3
and
B =
ï£«
ï£­
b11
b12
b13
b14
b21
b22
b23
b24
b31
b32
b33
b34
ï£¶
ï£¸
3Ã—4
â†‘
inside ones match
â†‘
ï£¦ï£¦ï£¦
shape of the product
ï£¦ï£¦ï£¦
then the product AB exists and has shape 2 Ã— 4. Consider a typical entry of
this product, say, the (2,3)-entry. The deï¬nition says [AB]23 is obtained by
forming the inner product of the second row of A with the third column of B

a11
a12
a13
a21
a22
a23
 ï£«
ï£­
b11
b21
b31
b12
b22
b32
b13
b23
b33
b14
b24
b34
ï£¶
ï£¸,
so
[AB]23 = A2âˆ—Bâˆ—3 = a21b13 + a22b23 + a23b33 =
3

k=1
a2kbk3.

3.5 Matrix Multiplication
97
For example,
A =

2
1
âˆ’4
âˆ’3
0
5

, B =
ï£«
ï£­
1
3
âˆ’3
2
2
5
âˆ’1
8
âˆ’1
2
0
2
ï£¶
ï£¸=â‡’AB =

8
3
âˆ’7
4
âˆ’8
1
9
4

.
Notice that in spite of the fact that the product AB exists, the product BA
is not deï¬nedâ€”matrix B is 3 Ã— 4 and A is 2 Ã— 3, and the inside dimensions
donâ€™t match in this order. Even when the products AB and BA each exist
and have the same shape, they need not be equal. For example,
A=

1
âˆ’1
1
âˆ’1

, B=

1
1
1
1

=â‡’AB=

0
0
0
0

, BA=

2
âˆ’2
2
âˆ’2

. (3.5.1)
This disturbing feature is a primary diï¬€erence between scalar and matrix algebra.
Matrix Multiplication Is Not Commutative
Matrix multiplication is a noncommutative operationâ€”i.e., it is possible
for AB Ì¸= BA, even when both products exist and have the same shape.
There are other major diï¬€erences between multiplication of matrices and
multiplication of scalars. For scalars,
Î±Î² = 0
implies
Î± = 0
or
Î² = 0.
(3.5.2)
However, the analogous statement for matrices does not holdâ€”the matrices given
in (3.5.1) show that it is possible for AB = 0 with A Ì¸= 0 and B Ì¸= 0. Related
to this issue is a rule sometimes known as the cancellation law. For scalars,
this law says that
Î±Î² = Î±Î³
and
Î± Ì¸= 0
implies
Î² = Î³.
(3.5.3)
This is true because we invoke (3.5.2) to deduce that Î±(Î² âˆ’Î³) = 0 implies
Î² âˆ’Î³ = 0. Since (3.5.2) does not hold for matrices, we cannot expect (3.5.3) to
hold for matrices.
Example 3.5.1
The cancellation law (3.5.3) fails for matrix multiplication. If
A =

1
1
1
1

,
B =

2
2
2
2

,
and
C =

3
1
1
3

,
then
AB =

4
4
4
4

= AC
but
B Ì¸= C
in spite of the fact that A Ì¸= 0.

98
Chapter 3
Matrix Algebra
There are various ways to express the individual rows and columns of a
matrix product. For example, the ith row of AB is
[AB]iâˆ—=

Aiâˆ—Bâˆ—1 | Aiâˆ—Bâˆ—2 | Â· Â· Â· | Aiâˆ—Bâˆ—n

= Aiâˆ—B
= ( ai1
ai2
Â· Â· Â·
aip )
ï£«
ï£¬
ï£¬
ï£­
B1âˆ—
B2âˆ—
...
Bpâˆ—
ï£¶
ï£·
ï£·
ï£¸= ai1B1âˆ—+ ai2B2âˆ—+ Â· Â· Â· + aipBpâˆ—.
As shown below, there are similar representations for the individual columns.
Rows and Columns of a Product
Suppose that A = [aij] is m Ã— p and B = [bij] is p Ã— n.
â€¢
[AB]iâˆ—= Aiâˆ—B

( ith row of AB )=( ith row of A ) Ã—B

. (3.5.4)
â€¢
[AB]âˆ—j = ABâˆ—j

( jth col of AB )= AÃ— ( jth col of B )

.
(3.5.5)
â€¢
[AB]iâˆ—= ai1B1âˆ—+ ai2B2âˆ—+ Â· Â· Â· + aipBpâˆ—= p
k=1 aikBkâˆ—.
(3.5.6)
â€¢
[AB]âˆ—j = Aâˆ—1b1j + Aâˆ—2b2j + Â· Â· Â· + Aâˆ—pbpj = p
k=1 Aâˆ—kbkj.
(3.5.7)
These last two equations show that rows of AB are combinations of
rows of B, while columns of AB are combinations of columns of A.
For example, if A =

1
âˆ’2
0
3
âˆ’4
5

and B =
ï£«
ï£­
3
âˆ’5
1
2
âˆ’7
2
1
âˆ’2
0
ï£¶
ï£¸, then the
second row of AB is
[AB]2âˆ—= A2âˆ—B = ( 3
âˆ’4
5 )
ï£«
ï£­
3
âˆ’5
1
2
âˆ’7
2
1
âˆ’2
0
ï£¶
ï£¸= ( 6
3
âˆ’5 ) ,
and the second column of AB is
[AB]âˆ—2 = ABâˆ—2 =

1
âˆ’2
0
3
âˆ’4
5
 ï£«
ï£­
âˆ’5
âˆ’7
âˆ’2
ï£¶
ï£¸=

9
3

.
This example makes the point that it is wasted eï¬€ort to compute the entire
product if only one row or column is called for. Although itâ€™s not necessary to
compute the complete product, you may wish to verify that
AB =

1
âˆ’2
0
3
âˆ’4
5
 ï£«
ï£­
3
âˆ’5
1
2
âˆ’7
2
1
âˆ’2
0
ï£¶
ï£¸=

âˆ’1
9
âˆ’3
6
3
âˆ’5

.

3.5 Matrix Multiplication
99
Matrix multiplication provides a convenient representation for a linear sys-
tem of equations. For example, the 3 Ã— 4 system
2x1 + 3x2 + 4x3 + 8x4 = 7,
3x1 + 5x2 + 6x3 + 2x4 = 6,
4x1 + 2x2 + 4x3 + 9x4 = 4,
can be written as Ax = b, where
A3Ã—4 =
ï£«
ï£­
2
3
4
8
3
5
6
2
4
2
4
9
ï£¶
ï£¸,
x4Ã—1 =
ï£«
ï£¬
ï£­
x1
x2
x3
x4
ï£¶
ï£·
ï£¸,
and
b3Ã—1 =
ï£«
ï£­
7
6
4
ï£¶
ï£¸.
And this example generalizes to become the following statement.
Linear Systems
Every linear system of m equations in n unknowns
a11x1 +
a12x2 + Â· Â· Â· +
a1nxn =
b1,
a21x1 +
a22x2 + Â· Â· Â· +
a2nxn =
b2,
...
am1x1 + am2x2 + Â· Â· Â· + amnxn = bm,
can be written as a single matrix equation Ax = b in which
A =
ï£«
ï£¬
ï£¬
ï£­
a11
a12
Â· Â· Â·
a1n
a21
a22
Â· Â· Â·
a2n
...
...
...
...
am1
am2
Â· Â· Â·
amn
ï£¶
ï£·
ï£·
ï£¸,
x =
ï£«
ï£¬
ï£¬
ï£­
x1
x2
...
xn
ï£¶
ï£·
ï£·
ï£¸,
and
b =
ï£«
ï£¬
ï£¬
ï£­
b1
b2
...
bm
ï£¶
ï£·
ï£·
ï£¸.
Conversely, every matrix equation of the form AmÃ—nxnÃ—1 = bmÃ—1 rep-
resents a system of m linear equations in n unknowns.
The numerical solution of a linear system was presented earlier in the text
without the aid of matrix multiplication because the operation of matrix mul-
tiplication is not an integral part of the arithmetical process used to extract a
solution by means of Gaussian elimination. Viewing a linear system as a single
matrix equation Ax = b is more of a notational convenience that can be used to
uncover theoretical properties and to prove general theorems concerning linear
systems.

100
Chapter 3
Matrix Algebra
For example, a very concise proof of the fact (2.3.5) stating that a system
of equations AmÃ—nxnÃ—1 = bmÃ—1 is consistent if and only if b is a linear
combination of the columns in A is obtained by noting that the system is
consistent if and only if there exists a column s that satisï¬es
b = As = ( Aâˆ—1
Aâˆ—2
Â· Â· Â·
Aâˆ—n )
ï£«
ï£¬
ï£¬
ï£­
s1
s2
...
sn
ï£¶
ï£·
ï£·
ï£¸= Aâˆ—1s1 + Aâˆ—2s2 + Â· Â· Â· + Aâˆ—nsn.
The following example illustrates a common situation in which matrix mul-
tiplication arises naturally.
Example 3.5.2
An airline serves ï¬ve cities, say, A, B, C, D, and H, in which H is the â€œhub
city.â€ The various routes between the cities are indicated in Figure 3.5.1.
A
B
C
D
H
Figure 3.5.1
Suppose you wish to travel from city A to city B so that at least two connecting
ï¬‚ights are required to make the trip. Flights (A â†’H) and (H â†’B) provide the
minimal number of connections. However, if space on either of these two ï¬‚ights
is not available, you will have to make at least three ï¬‚ights. Several questions
arise. How many routes from city A to city B require exactly three connecting
ï¬‚ights? How many routes require no more than four ï¬‚ightsâ€”and so forth? Since
this particular network is small, these questions can be answered by â€œeyeballingâ€
the diagram, but the â€œeyeball methodâ€ wonâ€™t get you very far with the large
networks that occur in more practical situations. Letâ€™s see how matrix algebra
can be applied. Begin by creating a connectivity matrix C = [cij] (also known
as an adjacency matrix) in which
cij =
 1
if there is a ï¬‚ight from city i to city j,
0
otherwise.

3.5 Matrix Multiplication
101
For the network depicted in Figure 3.5.1,
C =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
A
B
C
D
H
A
0
0
1
0
1
B
1
0
0
0
1
C
0
0
0
1
1
D
0
1
0
0
1
H
1
1
1
1
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
The matrix C together with its powers C2, C3, C4, . . . will provide all of the
information needed to analyze the network. To see how, notice that since cik
is the number of direct routes from city i to city k, and since ckj is the
number of direct routes from city k to city j, it follows that cikckj must be
the number of 2-ï¬‚ight routes from city i to city j that have a connection at
city k. Consequently, the (i, j) -entry in the product C2 = CC is
[C2]ij =
5

k=1
cikckj = the total number of 2-ï¬‚ight routes from city i to city j.
Similarly, the (i, j) -entry in the product C3 = CCC is
[C3]ij =
5

k1,k2=1
cik1ck1k2ck2j = number of 3-ï¬‚ight routes from city i to city j,
and, in general,
[Cn]ij =
5

k1,k2,Â·Â·Â·,knâˆ’1=1
cik1ck1k2 Â· Â· Â· cknâˆ’2knâˆ’1cknâˆ’1j
is the total number of n -ï¬‚ight routes from city i to city j. Therefore, the total
number of routes from city i to city j that require no more than n ï¬‚ights
must be given by
[C]ij + [C2]ij + [C3]ij + Â· Â· Â· + [Cn]ij = [C + C2 + C3 + Â· Â· Â· + Cn]ij.
For our particular network,
C2=
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1
1
1
2
1
1
1
2
1
1
1
2
1
1
1
2
1
1
1
1
1
1
1
1
4
ï£¶
ï£·
ï£·
ï£·
ï£¸, C3=
ï£«
ï£¬
ï£¬
ï£¬
ï£­
2
3
2
2
5
2
2
2
3
5
3
2
2
2
5
2
2
3
2
5
5
5
5
5
4
ï£¶
ï£·
ï£·
ï£·
ï£¸, C4=
ï£«
ï£¬
ï£¬
ï£¬
ï£­
8
7
7
7
9
7
8
7
7
9
7
7
8
7
9
7
7
7
8
9
9
9
9
9
20
ï£¶
ï£·
ï£·
ï£·
ï£¸,

102
Chapter 3
Matrix Algebra
and
C + C2 + C3 + C4 =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
11
11
11
11
16
11
11
11
11
16
11
11
11
11
16
11
11
11
11
16
16
16
16
16
28
ï£¶
ï£·
ï£·
ï£·
ï£¸.
The fact that [C3]12 = 3 means there are exactly 3 three-ï¬‚ight routes from city
A to city B, and [C4]12 = 7 means there are exactly 7 four-ï¬‚ight routesâ€”try
to identify them. Furthermore, [C + C2 + C3 + C4]12 = 11 means there are 11
routes from city A to city B that require no more than 4 ï¬‚ights.
Exercises for section 3.5
3.5.1. For A =
ï£«
ï£­
1
âˆ’2
3
0
âˆ’5
4
4
âˆ’3
8
ï£¶
ï£¸, B =
ï£«
ï£­
1
2
0
4
3
7
ï£¶
ï£¸, and C =
ï£«
ï£­
1
2
3
ï£¶
ï£¸, compute
the following products when possible.
(a)
AB,
(b) BA,
(c) CB,
(d) CT B,
(e) A2,
(f) B2,
(g)
CT C,
(h) CCT ,
(i) BBT ,
(j) BT B,
(k) CT AC.
3.5.2. Consider the following system of equations:
2x1 +
x2 +
x3 =
3,
4x1
+ 2x3 =
10,
2x1 + 2x2
= âˆ’2.
(a)
Write the system as a matrix equation of the form Ax = b.
(b)
Write the solution of the system as a column s and verify by
matrix multiplication that s satisï¬es the equation Ax = b.
(c)
Write b as a linear combination of the columns in A.
3.5.3. Let E =
ï£«
ï£­
1
0
0
0
1
0
3
0
1
ï£¶
ï£¸and let A be an arbitrary 3 Ã— 3 matrix.
(a)
Describe the rows of EA in terms of the rows of A.
(b)
Describe the columns of AE in terms of the columns of A.
3.5.4. Let ej denote the jth unit column that contains a 1 in the jth
position and zeros everywhere else. For a general matrix AnÃ—n, describe
the following products.
(a)
Aej
(b)
eT
i A
(c)
eT
i Aej

3.5 Matrix Multiplication
103
3.5.5. Suppose that A and B are m Ã— n matrices. If Ax = Bx holds for
all n Ã— 1 columns x, prove that A = B. Hint: What happens when
x is a unit column?
3.5.6. For A =

1/2
Î±
0
1/2

, determine limnâ†’âˆAn. Hint: Compute a few
powers of A and try to deduce the general form of An.
3.5.7. If CmÃ—1 and R1Ã—n are matrices consisting of a single column and
a single row, respectively, then the matrix product PmÃ—n = CR is
sometimes called the outer product of C with R. For conformable
matrices A and B, explain how to write the product AB as a sum of
outer products involving the columns of A and the rows of B.
3.5.8. A square matrix U = [uij] is said to be upper triangular whenever
uij = 0 for i > j â€”i.e., all entries below the main diagonal are 0.
(a)
If A and B are two n Ã— n upper-triangular matrices, explain
why the product AB must also be upper triangular.
(b)
If AnÃ—n and BnÃ—n are upper triangular, what are the diagonal
entries of AB?
(c)
L is lower triangular when â„“ij = 0 for i < j. Is it true that
the product of two n Ã— n lower-triangular matrices is again
lower triangular?
3.5.9. If A = [aij(t)] is a matrix whose entries are functions of a variable t,
the derivative
of A with respect to t is deï¬ned to be the matrix of
derivatives. That is,
dA
dt =
daij
dt

.
Derive the product rule for diï¬€erentiation
d(AB)
dt
= dA
dt B + AdB
dt .
3.5.10. Let CnÃ—n be the connectivity matrix associated with a network of n
nodes such as that described in Example 3.5.2, and let e be the n Ã— 1
column of all 1â€™s. In terms of the network, describe the entries in each
of the following products.
(a)
Interpret the product Ce.
(b)
Interpret the product eT C.

104
Chapter 3
Matrix Algebra
3.5.11. Consider three tanks each containing V gallons of brine. The tanks are
connected as shown in Figure 3.5.2, and all spigots are opened at once.
As fresh water at the rate of r gal/sec is pumped into the top of the
ï¬rst tank, r gal/sec leaves from the bottom and ï¬‚ows into the next
tank, and so on down the lineâ€”there are r gal/sec entering at the top
and leaving through the bottom of each tank.
r  gal / sec
r  gal / sec
r  gal / sec
r  gal / sec
Figure 3.5.2
Let xi(t) denote the number of pounds of salt in tank i at time t, and
let
x =
ï£«
ï£¬
ï£­
x1(t)
x2(t)
x3(t)
ï£¶
ï£·
ï£¸
and
dx
dt =
ï£«
ï£¬
ï£­
dx1/dt
dx2/dt
dx3/dt
ï£¶
ï£·
ï£¸.
Assuming that complete mixing occurs in each tank on a continuous
basis, show that
dx
dt = Ax,
where
A = r
V
ï£«
ï£­
âˆ’1
0
0
1
âˆ’1
0
0
1
âˆ’1
ï£¶
ï£¸.
Hint: Use the fact that
dxi
dt = rate of change = lbs
sec coming in âˆ’lbs
sec going out.

3.6 Properties of Matrix Multiplication
105
3.6
PROPERTIES OF MATRIX MULTIPLICATION
We saw in the previous section that there are some diï¬€erences between scalar
and matrix algebraâ€”most notable is the fact that matrix multiplication is not
commutative, and there is no cancellation law. But there are also some important
similarities, and the purpose of this section is to look deeper into these issues.
Although we can adjust to not having the commutative property, the situa-
tion would be unbearable if the distributive and associative properties were not
available. Fortunately, both of these properties hold for matrix multiplication.
Distributive and Associative Laws
For conformable matrices each of the following is true.
â€¢
A(B + C) = AB + AC
(left-hand distributive law).
â€¢
(D + E)F = DF + EF
(right-hand distributive law).
â€¢
A(BC) = (AB)C
(associative law).
Proof.
To prove the left-hand distributive property, demonstrate the corre-
sponding entries in the matrices A(B + C) and AB + AC are equal. To this
end, use the deï¬nition of matrix multiplication to write
[A(B + C)]ij = Aiâˆ—(B + C)âˆ—j =

k
[A]ik[B + C]kj =

k
[A]ik ([B]kj + [C]kj)
=

k
([A]ik[B]kj + [A]ik[C]kj) =

k
[A]ik[B]kj +

k
[A]ik[C]kj
= Aiâˆ—Bâˆ—j + Aiâˆ—Câˆ—j = [AB]ij + [AC]ij
= [AB + AC]ij.
Since this is true for each i and j, it follows that A(B + C) = AB + AC. The
proof of the right-hand distributive property is similar and is omitted. To prove
the associative law, suppose that B is p Ã— q and C is q Ã— n, and recall from
(3.5.7) that the jth column of BC is a linear combination of the columns in
B. That is,
[BC]âˆ—j = Bâˆ—1c1j + Bâˆ—2c2j + Â· Â· Â· + Bâˆ—qcqj =
q

k=1
Bâˆ—kckj.

106
Chapter 3
Matrix Algebra
Use this along with the left-hand distributive property to write
[A(BC)]ij = Aiâˆ—[BC]âˆ—j = Aiâˆ—
q

k=1
Bâˆ—kckj =
q

k=1
Aiâˆ—Bâˆ—kckj
=
q

k=1
[AB]ikckj = [AB]iâˆ—Câˆ—j = [(AB)C]ij.
Example 3.6.1
Linearity of Matrix Multiplication. Let A be an m Ã— n matrix, and f be
the function deï¬ned by matrix multiplication
f(XnÃ—p) = AX.
The left-hand distributive property guarantees that f is a linear function be-
cause for all scalars Î± and for all n Ã— p matrices X and Y,
f(Î±X + Y) = A(Î±X + Y) = A(Î±X) + AY = Î±AX + AY
= Î±f(X) + f(Y).
Of course, the linearity of matrix multiplication is no surprise because it was
the consideration of linear functions that motivated the deï¬nition of the matrix
product at the outset.
For scalars, the number 1 is the identity element for multiplication because
it has the property that it reproduces whatever it is multiplied by. For matrices,
there is an identity element with similar properties.
Identity Matrix
The n Ã— n matrix with 1â€™s on the main diagonal and 0â€™s elsewhere
In =
ï£«
ï£¬
ï£¬
ï£­
1
0
Â· Â· Â·
0
0
1
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
1
ï£¶
ï£·
ï£·
ï£¸
is called the identity matrix of order n. For every m Ã— n matrix A,
AIn = A
and
ImA = A.
The subscript on In is neglected whenever the size is obvious from the
context.

3.6 Properties of Matrix Multiplication
107
Proof.
Notice that Iâˆ—j has a 1 in the jth position and 0â€™s elsewhere. Recall
from Exercise
3.5.4 that such columns were called unit columns, and they
have the property that for any conformable matrix A,
AIâˆ—j = Aâˆ—j.
Using this together with the fact that [AI]âˆ—j = AIâˆ—j produces
AI = ( AIâˆ—1
AIâˆ—2
Â· Â· Â·
AIâˆ—n ) = ( Aâˆ—1
Aâˆ—2
Â· Â· Â·
Aâˆ—n ) = A.
A similar argument holds when I appears on the left-hand side of A.
Analogous to scalar algebra, we deï¬ne the 0th power of a square matrix to
be the identity matrix of corresponding size. That is, if A is n Ã— n, then
A0 = In.
Positive powers of A are also deï¬ned in the natural way. That is,
An = AAÂ· Â· Â·A



n times
.
The associative law guarantees that it makes no diï¬€erence how matrices are
grouped for powering. For example, AA2 is the same as A2A, so that
A3 = AAA = AA2 = A2A.
Also, the usual laws of exponents hold. For nonnegative integers r and s,
ArAs = Ar+s
and
(Ar)s = Ars.
We are not yet in a position to deï¬ne negative or fractional powers, and due to
the lack of conformability, powers of nonsquare matrices are never deï¬ned.
Example 3.6.2
A Pitfall. For two n Ã— n matrices, what is (A + B)2? Be careful! Because
matrix multiplication is not commutative, the familiar formula from scalar alge-
bra is not valid for matrices. The distributive properties must be used to write
(A + B)2 = (A + B)


(A + B) = (A + B)


 A + (A + B)


 B
= A2 + BA + AB + B2,
and this is as far as you can go. The familiar form A2+2AB+B2 is obtained only
in those rare cases where AB = BA. To evaluate (A + B)k, the distributive
rules must be applied repeatedly, and the results are a bit more complicatedâ€”try
it for k = 3.

108
Chapter 3
Matrix Algebra
Example 3.6.3
Suppose that the population migration between two geographical regionsâ€”say,
the North and the Southâ€”is as follows. Each year, 50% of the population in
the North migrates to the South, while only 25% of the population in the South
moves to the North. This situation is depicted by drawing a transition diagram
such as that shown in Figure 3.6.1.
N
S
.25
.5
.5
.75
Figure 3.6.1
Problem: If this migration pattern continues, will the population in the North
continually shrink until the entire population is eventually in the South, or will
the population distribution somehow stabilize before the North is completely
deserted?
Solution: Let nk and sk denote the respective proportions of the total popula-
tion living in the North and South at the end of year k and assume nk +sk = 1.
The migration pattern dictates that the fractions of the population in each region
at the end of year k + 1 are
nk+1 = nk(.5) + sk(.25),
sk+1 = nk(.5) + sk(.75).
(3.6.1)
If pT
k = (nk, sk) and pT
k+1 = (nk+1, sk+1) denote the respective population
distributions at the end of years k and k + 1, and if
T =
 N
S
N
.5
.5
S
.25
.75

is the associated transition matrix, then (3.6.1) assumes the matrix form
pT
k+1 = pT
k T. Inducting on pT
1 = pT
0 T, pT
2 = pT
1 T = pT
0 T2, pT
3 = pT
2 T =
pT
0 T3, etc., leads to
pT
k = pT
0 Tk.
(3.6.2)
Determining the long-run behavior involves evaluating limkâ†’âˆpT
k , and itâ€™s clear
from (3.6.2) that this boils down to analyzing limkâ†’âˆTk. Later, in Example

3.6 Properties of Matrix Multiplication
109
7.3.5, a more sophisticated approach is discussed, but for now we will use the
â€œbrute forceâ€ method of successively powering P until a pattern emerges. The
ï¬rst several powers of P are shown below with three signiï¬cant digits displayed.
P2 =

.375
.625
.312
.687

P3 =

.344
.656
.328
.672

P4 =

.328
.672
.332
.668

P5 =

.334
.666
.333
.667

P6 =

.333
.667
.333
.667

P7 =

.333
.667
.333
.667

This sequence appears to be converging to a limiting matrix of the form
Pâˆ= lim
kâ†’âˆPk =

1/3
2/3
1/3
2/3

,
so the limiting population distribution is
pT
âˆ= lim
kâ†’âˆpT
k = lim
kâ†’âˆpT
0 Tk = pT
0 lim
kâ†’âˆTk = ( n0
s0 )

1/3
2/3
1/3
2/3

=
 n0 + s0
3
2(n0 + s0)
3

= ( 1/3
2/3 ) .
Therefore, if the migration pattern continues to hold, then the population dis-
tribution will eventually stabilize with 1/3 of the population being in the North
and 2/3 of the population in the South. And this is independent of the initial
distribution! The powers of P indicate that the population distribution will be
practically stable in no more than 6 yearsâ€”individuals may continue to move,
but the proportions in each region are essentially constant by the sixth year.
The operation of transposition has an interesting eï¬€ect upon a matrix
productâ€”a reversal of order occurs.
Reverse Order Law for Transposition
For conformable matrices A and B,
(AB)T = BT AT .
The case of conjugate transposition is similar. That is,
(AB)âˆ—= Bâˆ—Aâˆ—.

110
Chapter 3
Matrix Algebra
Proof.
By deï¬nition,
(AB)T
ij = [AB]ji = Ajâˆ—Bâˆ—i.
Consider the (i, j)-entry of the matrix BT AT and write

BT AT 
ij =

BT 	
iâˆ—

AT 	
âˆ—j =

k

BT 
ik

AT 
kj
=

k
[B]ki[A]jk =

k
[A]jk[B]ki
= Ajâˆ—Bâˆ—i.
Therefore, (AB)T
ij =

BT AT 
ij for all i and j, and thus (AB)T = BT AT .
The proof for the conjugate transpose case is similar.
Example 3.6.4
For every matrix AmÃ—n, the products AT A and AAT are symmetric matrices
because

AT A
	T = AT AT T = AT A
and

AAT 	T = AT T AT = AAT .
Example 3.6.5
Trace of a Product. Recall from Example 3.3.1 that the trace of a square
matrix is the sum of its main diagonal entries. Although matrix multiplication
is not commutative, the trace function is one of the few cases where the order of
the matrices can be changed without aï¬€ecting the results.
Problem: For matrices AmÃ—n and BnÃ—m, prove that
trace (AB) = trace (BA).
Solution:
trace (AB) =

i
[AB]ii =

i
Aiâˆ—Bâˆ—i =

i

k
aikbki =

i

k
bkiaik
=

k

i
bkiaik =

k
Bkâˆ—Aâˆ—k =

k
[BA]kk = trace (BA).
Note: This is true in spite of the fact that AB is m Ã— m while BA is n Ã— n.
Furthermore, this result can be extended to say that any product of conformable
matrices can be permuted cyclically without altering the trace of the product.
For example,
trace (ABC) = trace (BCA) = trace (CAB).
However, a noncyclical permutation may not preserve the trace. For example,
trace (ABC) Ì¸= trace (BAC).

3.6 Properties of Matrix Multiplication
111
Executing multiplication between two matrices by partitioning one or both
factors into submatricesâ€”a matrix contained within another matrixâ€”can be
a useful technique.
Block Matrix Multiplication
Suppose that A and B are partitioned into submatricesâ€”often referred
to as blocksâ€”as indicated below.
A =
ï£«
ï£¬
ï£¬
ï£­
A11
A12
Â· Â· Â·
A1r
A21
A22
Â· Â· Â·
A2r
...
...
...
...
As1
As2
Â· Â· Â·
Asr
ï£¶
ï£·
ï£·
ï£¸,
B =
ï£«
ï£¬
ï£¬
ï£­
B11
B12
Â· Â· Â·
B1t
B21
B22
Â· Â· Â·
B2t
...
...
...
...
Br1
Br2
Â· Â· Â·
Brt
ï£¶
ï£·
ï£·
ï£¸.
If the pairs (Aik, Bkj) are conformable, then A and B are said to
be conformably partitioned. For such matrices, the product AB is
formed by combining the blocks exactly the same way as the scalars are
combined in ordinary matrix multiplication. That is, the (i, j) -block in
AB is
Ai1B1j + Ai2B2j + Â· Â· Â· + AirBrj.
Although a completely general proof is possible, looking at some examples
better serves the purpose of understanding this technique.
Example 3.6.6
Block multiplication is particularly useful when there are patterns in the matrices
to be multiplied. Consider the partitioned matrices
A =
ï£«
ï£¬
ï£¬
ï£­
1
2
1
0
3
4
0
1
1
0
0
0
0
1
0
0
ï£¶
ï£·
ï£·
ï£¸=

C
I
I
0

,
B =
ï£«
ï£¬
ï£¬
ï£­
1
0
0
0
0
1
0
0
1
2
1
2
3
4
3
4
ï£¶
ï£·
ï£·
ï£¸=

I
0
C
C

,
where
I =

1
0
0
1

and
C =

1
2
3
4

.
Using block multiplication, the product AB is easily computed to be
AB =

C
I
I
0
 
I
0
C
C

=

2C
C
I
0

=
ï£«
ï£¬
ï£¬
ï£­
2
4
1
2
6
8
3
4
1
0
0
0
0
1
0
0
ï£¶
ï£·
ï£·
ï£¸.

112
Chapter 3
Matrix Algebra
Example 3.6.7
Reducibility. Suppose that TnÃ—nx = b represents a system of linear equa-
tions in which the coeï¬ƒcient matrix is block triangular. That is, T can be
partitioned as
T =

A
B
0
C

,
where
A is r Ã— r and C is n âˆ’r Ã— n âˆ’r.
(3.6.3)
If x and b are similarly partitioned as x =
 x1
x2

and b =
 b1
b2

, then block
multiplication shows that Tx = b reduces to two smaller systems
Ax1 + Bx2 = b1,
Cx2 = b2,
so if all systems are consistent, a block version of back substitution is possibleâ€”
i.e., solve Cx2 = b2 for x2, and substituted this back into Ax1 = b1 âˆ’Bx2,
which is then solved for x1. For obvious reasons, block-triangular systems of
this type are sometimes referred to as reducible systems, and T is said to
be a reducible matrix. Recall that applying Gaussian elimination with back
substitution to an n Ã— n system requires about n3/3 multiplications/divisions
and about n3/3 additions/subtractions. This means that itâ€™s more eï¬ƒcient to
solve two smaller subsystems than to solve one large main system. For exam-
ple, suppose the matrix T in (3.6.3) is 100 Ã— 100 while A and C are each
50 Ã— 50. If Tx = b is solved without taking advantage of its reducibility, then
about 106/3 multiplications/divisions are needed. But by taking advantage of
the reducibility, only about (250 Ã— 103)/3 multiplications/divisions are needed
to solve both 50 Ã— 50 subsystems. Another advantage of reducibility is realized
when a computerâ€™s main memory capacity is not large enough to store the entire
coeï¬ƒcient matrix but is large enough to hold the submatrices.
Exercises for section 3.6
3.6.1. For the partitioned matrices
A =
ï£«
ï£­
1
0
0
3
3
3
1
0
0
3
3
3
1
2
2
0
0
0
ï£¶
ï£¸
and
B =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ’1
âˆ’1
0
0
0
0
âˆ’1
âˆ’2
âˆ’1
âˆ’2
âˆ’1
âˆ’2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
use block multiplication with the indicated partitions to form the prod-
uct AB.

3.6 Properties of Matrix Multiplication
113
3.6.2. For all matrices AnÃ—k and BkÃ—n, show that the block matrix
L =

I âˆ’BA
B
2A âˆ’ABA
AB âˆ’I

has the property L2 = I. Matrices with this property are said to be
involutory, and they occur in the science of cryptography.
3.6.3. For the matrix
A =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
0
1/3
1/3
1/3
0
1
0
1/3
1/3
1/3
0
0
1
1/3
1/3
1/3
0
0
0
1/3
1/3
1/3
0
0
0
1/3
1/3
1/3
0
0
0
1/3
1/3
1/3
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
determine A300. Hint: A square matrix C is said to be idempotent
when it has the property that C2 = C. Make use of idempotent sub-
matrices in A.
3.6.4. For every matrix AmÃ—n, demonstrate that the products Aâˆ—A and
AAâˆ—are hermitian matrices.
3.6.5. If A and B are symmetric matrices that commute, prove that the
product AB is also symmetric. If AB Ì¸= BA, is AB necessarily sym-
metric?
3.6.6. Prove that the right-hand distributive property is true.
3.6.7. For each matrix AnÃ—n, explain why it is impossible to ï¬nd a solution
for XnÃ—n in the matrix equation
AX âˆ’XA = I.
Hint: Consider the trace function.
3.6.8. Let yT
1Ã—m be a row of unknowns, and let AmÃ—n and bT
1Ã—n be known
matrices.
(a)
Explain why the matrix equation yT A = bT represents a sys-
tem of n linear equations in m unknowns.
(b)
How are the solutions for yT
in yT A = bT
related to the
solutions for x in AT x = b?

114
Chapter 3
Matrix Algebra
3.6.9. A particular electronic device consists of a collection of switching circuits
that can be either in an ON state or an OFF state. These electronic
switches are allowed to change state at regular time intervals called clock
cycles. Suppose that at the end of each clock cycle, 30% of the switches
currently in the OFF state change to ON, while 90% of those in the ON
state revert to the OFF state.
(a)
Show that the device approaches an equilibrium in the sense
that the proportion of switches in each state eventually becomes
constant, and determine these equilibrium proportions.
(b)
Independent of the initial proportions, about how many clock
cycles does it take for the device to become essentially stable?
3.6.10. Write the following system in the form TnÃ—nx = b, where T is block
triangular, and then obtain the solution by solving two small systems as
described in Example 3.6.7.
x1 +
x2 + 3x3 + 4x4 = âˆ’1,
2x3 + 3x4 =
3,
x1 + 2x2 + 5x3 + 6x4 = âˆ’2,
x3 + 2x4 =
4.
3.6.11. Prove that each of the following statements is true for conformable ma-
trices.
(a)
trace (ABC) = trace (BCA) = trace (CAB).
(b)
trace (ABC) can be diï¬€erent from trace (BAC).
(c)
trace

AT B
	
= trace

ABT 	
.
3.6.12. Suppose that AmÃ—n and xnÃ—1 have real entries.
(a)
Prove that xT x = 0 if and only if x = 0.
(b)
Prove that trace

AT A
	
= 0 if and only if A = 0.

3.7 Matrix Inversion
115
3.7
MATRIX INVERSION
If Î± is a nonzero scalar, then for each number Î² the equation Î±x = Î² has a
unique solution given by x = Î±âˆ’1Î². To prove that Î±âˆ’1Î² is a solution, write
Î±(Î±âˆ’1Î²) = (Î±Î±âˆ’1)Î² = (1)Î² = Î².
(3.7.1)
Uniqueness follows because if x1 and x2 are two solutions, then
Î±x1 = Î² = Î±x2
=â‡’
Î±âˆ’1(Î±x1) = Î±âˆ’1(Î±x2)
=â‡’
(Î±âˆ’1Î±)x1 = (Î±âˆ’1Î±)x2
=â‡’
(1)x1 = (1)x2
=â‡’
x1 = x2.
(3.7.2)
These observations seem pedantic, but they are important in order to see how
to make the transition from scalar equations to matrix equations. In particular,
these arguments show that in addition to associativity, the properties
Î±Î±âˆ’1 = 1
and
Î±âˆ’1Î± = 1
(3.7.3)
are the key ingredients, so if we want to solve matrix equations in the same
fashion as we solve scalar equations, then a matrix analogue of (3.7.3) is needed.
Matrix Inversion
For a given square matrix AnÃ—n, the matrix BnÃ—n that satisï¬es the
conditions
AB = In
and
BA = In
is called the inverse of A and is denoted by B = Aâˆ’1. Not all square
matrices are invertibleâ€”the zero matrix is a trivial example, but there
are also many nonzero matrices that are not invertible. An invertible
matrix is said to be nonsingular, and a square matrix with no inverse
is called a singular matrix.
Notice that matrix inversion is deï¬ned for square matrices onlyâ€”the con-
dition AAâˆ’1 = Aâˆ’1A rules out inverses of nonsquare matrices.
Example 3.7.1
If
A =

a
b
c
d

,
where
Î´ = ad âˆ’bc Ì¸= 0,
then
Aâˆ’1 = 1
Î´

d
âˆ’b
âˆ’c
a

because it can be veriï¬ed that AAâˆ’1 = Aâˆ’1A = I2.

116
Chapter 3
Matrix Algebra
Although not all matrices are invertible, when an inverse exists, it is unique.
To see this, suppose that X1 and X2 are both inverses for a nonsingular matrix
A. Then
X1 = X1I = X1(AX2) = (X1A)X2 = IX2 = X2,
which implies that only one inverse is possible.
Since matrix inversion was deï¬ned analogously to scalar inversion, and since
matrix multiplication is associative, exactly the same reasoning used in (3.7.1)
and (3.7.2) can be applied to a matrix equation AX = B, so we have the
following statements.
Matrix Equations
â€¢
If A is a nonsingular matrix, then there is a unique solution for X
in the matrix equation AnÃ—nXnÃ—p = BnÃ—p, and the solution is
X = Aâˆ’1B.
(3.7.4)
â€¢
A system of n linear equations in n unknowns can be written as a
single matrix equation AnÃ—nxnÃ—1 = bnÃ—1 (see p. 99), so it follows
from (3.7.4) that when A is nonsingular, the system has a unique
solution given by x = Aâˆ’1b.
However, it must be stressed that the representation of the solution as
x = Aâˆ’1b is mostly a notational or theoretical convenience. In practice, a
nonsingular system Ax = b is almost never solved by ï¬rst computing Aâˆ’1 and
then the product x = Aâˆ’1b. The reason will be apparent when we learn how
much work is involved in computing Aâˆ’1.
Since not all square matrices are invertible, methods are needed to distin-
guish between nonsingular and singular matrices. There is a variety of ways to
describe the class of nonsingular matrices, but those listed below are among the
most important.
Existence of an Inverse
For an n Ã— n matrix A, the following statements are equivalent.
â€¢
Aâˆ’1 exists
(A is nonsingular).
(3.7.5)
â€¢
rank (A) = n.
(3.7.6)
â€¢
A
Gaussâ€“Jordan
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’I.
(3.7.7)
â€¢
Ax = 0 implies that x = 0.
(3.7.8)

3.7 Matrix Inversion
117
Proof.
The fact that (3.7.6) â‡â‡’(3.7.7) is a direct consequence of the deï¬-
nition of rank, and (3.7.6) â‡â‡’(3.7.8) was established in Â§2.4. Consequently,
statements (3.7.6), (3.7.7), and (3.7.8) are equivalent, so if we establish that
(3.7.5) â‡â‡’(3.7.6), then the proof will be complete.
Proof of (3.7.5) =â‡’(3.7.6).
Begin by observing that (3.5.5) guarantees
that a matrix X = [Xâˆ—1 | Xâˆ—2 | Â· Â· Â· | Xâˆ—n] satisï¬es the equation AX = I if and
only if Xâˆ—j is a solution of the linear system Ax = Iâˆ—j. If A is nonsingular,
then we know from (3.7.4) that there exists a unique solution to AX = I, and
hence each linear system Ax = Iâˆ—j has a unique solution. But in Â§2.5 we learned
that a linear system has a unique solution if and only if the rank of the coeï¬ƒcient
matrix equals the number of unknowns, so rank (A) = n.
Proof of (3.7.6) =â‡’(3.7.5).
If rank (A) = n, then (2.3.4) insures that
each system Ax = Iâˆ—j is consistent because rank[A | Iâˆ—j] = n = rank (A).
Furthermore, the results of Â§2.5 guarantee that each system Ax = Iâˆ—j has a
unique solution, and hence there is a unique solution to the matrix equation
AX = I. We would like to say that X = Aâˆ’1, but we cannot jump to this
conclusion without ï¬rst arguing that XA = I. Suppose this is not trueâ€”i.e.,
suppose that XA âˆ’I Ì¸= 0. Since
A(XA âˆ’I) = (AX)A âˆ’A = IA âˆ’A = 0,
it follows from (3.5.5) that any nonzero column of XAâˆ’I is a nontrivial solution
of the homogeneous system Ax = 0. But this is a contradiction of the fact that
(3.7.6) â‡â‡’(3.7.8). Therefore, the supposition that XA âˆ’I Ì¸= 0 must be false,
and thus AX = I = XA, which means A is nonsingular.
The deï¬nition of matrix inversion says that in order to compute Aâˆ’1, it is
necessary to solve both of the matrix equations AX = I and XA = I. These
two equations are necessary to rule out the possibility of nonsquare inverses. But
when only square matrices are involved, then any one of the two equations will
suï¬ƒceâ€”the following example elaborates.
Example 3.7.2
Problem: If A and X are square matrices, explain why
AX = I
=â‡’
XA = I.
(3.7.9)
In other words, if A and X are square and AX = I, then X = Aâˆ’1.
Solution: Notice ï¬rst that AX = I implies X is nonsingular because if X is
singular, then, by (3.7.8), there is a column vector x Ì¸= 0 such that Xx = 0,
which is contrary to the fact that x = Ix = AXx = 0. Now that we know Xâˆ’1
exists, we can establish (3.7.9) by writing
AX = I
=â‡’
AXXâˆ’1 = Xâˆ’1
=â‡’
A = Xâˆ’1
=â‡’
XA = I.
Caution!
The argument above is not valid for nonsquare matrices. When
m Ì¸= n, itâ€™s possible that AmÃ—nXnÃ—m = Im, but XA Ì¸= In.

118
Chapter 3
Matrix Algebra
Although we usually try to avoid computing the inverse of a matrix, there are
times when an inverse must be found. To construct an algorithm that will yield
Aâˆ’1 when AnÃ—n is nonsingular, recall from Example 3.7.2 that determining
Aâˆ’1 is equivalent to solving the single matrix equation AX = I, and due to
(3.5.5), this in turn is equivalent to solving the n linear systems deï¬ned by
Ax = Iâˆ—j
for
j = 1, 2, . . . , n.
(3.7.10)
In other words, if Xâˆ—1, Xâˆ—2, . . . , Xâˆ—n are the respective solutions to (3.7.10), then
X = [Xâˆ—1 | Xâˆ—2 | Â· Â· Â· | Xâˆ—n] solves the equation AX = I, and hence X = Aâˆ’1.
If A is nonsingular, then we know from (3.7.7) that the Gaussâ€“Jordan method
reduces the augmented matrix [A | Iâˆ—j] to [I | Xâˆ—j], and the results of Â§1.3 insure
that Xâˆ—j is the unique solution to Ax = Iâˆ—j. That is,
[A | Iâˆ—j]
Gaussâ€“Jordan
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’

I
   [Aâˆ’1]âˆ—j
!
.
But rather than solving each system Ax = Iâˆ—j separately, we can solve them
simultaneously by taking advantage of the fact that they all have the same
coeï¬ƒcient matrix. In other words, applying the Gaussâ€“Jordan method to the
larger augmented array [A | Iâˆ—1 | Iâˆ—2 | Â· Â· Â· | Iâˆ—n] produces
[A | Iâˆ—1 | Iâˆ—2 | Â· Â· Â· | Iâˆ—n]
Gaussâ€“Jordan
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’
"
I
   [Aâˆ’1]âˆ—1
   [Aâˆ’1]âˆ—2
   Â· Â· Â·
   [Aâˆ’1]âˆ—n
#
,
or more compactly,
[A | I]
Gaussâ€“Jordan
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’[I | Aâˆ’1].
(3.7.11)
What happens if we try to invert a singular matrix using this procedure?
The fact that (3.7.5) â‡â‡’(3.7.6) â‡â‡’(3.7.7) guarantees that a singular matrix
A cannot be reduced to I by Gaussâ€“Jordan elimination because a zero row will
have to emerge in the left-hand side of the augmented array at some point during
the process. This means that we do not need to know at the outset whether A
is nonsingular or singularâ€”it becomes self-evident depending on whether or not
the reduction (3.7.11) can be completed. A summary is given below.
Computing an Inverse
Gaussâ€“Jordan elimination can be used to invert A by the reduction
[A | I]
Gaussâ€“Jordan
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’[I | Aâˆ’1].
(3.7.12)
The only way for this reduction to fail is for a row of zeros to emerge
in the left-hand side of the augmented array, and this occurs if and only
if A is a singular matrix. A diï¬€erent (and somewhat more practical)
algorithm is given Example 3.10.3 on p. 148.

3.7 Matrix Inversion
119
Although they are not included in the simple examples of this section, you
are reminded that the pivoting and scaling strategies presented in Â§1.5 need to
be incorporated, and the eï¬€ects of ill-conditioning discussed in Â§1.6 must be con-
sidered whenever matrix inverses are computed using ï¬‚oating-point arithmetic.
However, practical applications rarely require an inverse to be computed.
Example 3.7.3
Problem: If possible, ï¬nd the inverse of A =
ï£«
ï£­
1
1
1
1
2
2
1
2
3
ï£¶
ï£¸.
Solution:
[A | I] =
ï£«
ï£­
1
1
1
1
0
0
1
2
2
0
1
0
1
2
3
0
0
1
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
1
1
1
1
0
0
0
1
1
âˆ’1
1
0
0
1
2
âˆ’1
0
1
ï£¶
ï£¸
âˆ’â†’
ï£«
ï£­
1
0
0
2
âˆ’1
0
0
1
1
âˆ’1
1
0
0
0
1
0
âˆ’1
1
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
1
0
0
2
âˆ’1
0
0
1
0
âˆ’1
2
âˆ’1
0
0
1
0
âˆ’1
1
ï£¶
ï£¸
Therefore, the matrix is nonsingular, and Aâˆ’1 =
ï£«
ï£­
2
âˆ’1
0
âˆ’1
2
âˆ’1
0
âˆ’1
1
ï£¶
ï£¸. If we wish
to check this answer, we need only check that AAâˆ’1 = I. If this holds, then the
result of Example 3.7.2 insures that Aâˆ’1A = I will automatically be true.
Earlier in this section it was stated that one almost never solves a nonsin-
gular linear system Ax = b by ï¬rst computing Aâˆ’1 and then the product
x = Aâˆ’1b. To appreciate why this is true, pay attention to how much eï¬€ort is
required to perform one matrix inversion.
Operation Counts for Inversion
Computing Aâˆ’1
nÃ—n by reducing [A|I] with Gaussâ€“Jordan requires
â€¢
n3 multiplications/divisions,
â€¢
n3 âˆ’2n2 + n additions/subtractions.
Interestingly, if Gaussian elimination with a back substitution process is
applied to [A|I] instead of the Gaussâ€“Jordan technique, then exactly the same
operation count can be obtained. Although Gaussian elimination with back sub-
stitution is more eï¬ƒcient than the Gaussâ€“Jordan method for solving a single
linear system, the two procedures are essentially equivalent for inversion.

120
Chapter 3
Matrix Algebra
Solving a nonsingular system Ax = b by ï¬rst computing Aâˆ’1 and then
forming the product x = Aâˆ’1b requires n3 + n2 multiplications/divisions and
n3 âˆ’n2 additions/subtractions. Recall from Â§1.5 that Gaussian elimination with
back substitution requires only about n3/3 multiplications/divisions and about
n3/3 additions/subtractions. In other words, using Aâˆ’1 to solve a nonsingular
system Ax = b requires about three times the eï¬€ort as does Gaussian elimina-
tion with back substitution.
To put things in perspective, consider standard matrix multiplication be-
tween two n Ã— n matrices. It is not diï¬ƒcult to verify that n3 multiplications
and n3âˆ’n2 additions are required. Remarkably, it takes almost exactly as much
eï¬€ort to perform one matrix multiplication as to perform one matrix inversion.
This fact always seems to be counter to a noviceâ€™s intuitionâ€”it â€œfeelsâ€ like ma-
trix inversion should be a more diï¬ƒcult task than matrix multiplication, but this
is not the case.
The remainder of this section is devoted to a discussion of some of the
important properties of matrix inversion. We begin with the four basic facts
listed below.
Properties of Matrix Inversion
For nonsingular matrices A and B, the following properties hold.
â€¢

Aâˆ’1	âˆ’1 = A.
(3.7.13)
â€¢
The product AB is also nonsingular.
(3.7.14)
â€¢
(AB)âˆ’1 = Bâˆ’1Aâˆ’1
(the reverse order law for inversion). (3.7.15)
â€¢

Aâˆ’1	T =

AT 	âˆ’1 and

Aâˆ’1	âˆ—= (Aâˆ—)âˆ’1.
(3.7.16)
Proof.
Property (3.7.13) follows directly from the deï¬nition of inversion. To
prove (3.7.14) and (3.7.15), let X = Bâˆ’1Aâˆ’1 and verify that (AB)X = I by
writing
(AB)X = (AB)Bâˆ’1Aâˆ’1 = A(BBâˆ’1)Aâˆ’1 = A(I)Aâˆ’1 = AAâˆ’1 = I.
According to the discussion in Example 3.7.2, we are now guaranteed that
X(AB) = I, and we need not bother to verify it. To prove property (3.7.16), let
X =

Aâˆ’1	T and verify that AT X = I. Make use of the reverse order law for
transposition to write
AT X = AT 
Aâˆ’1	T =

Aâˆ’1A
	T = IT = I.
Therefore,

AT 	âˆ’1 = X =

Aâˆ’1	T . The proof of the conjugate transpose case
is similar.

3.7 Matrix Inversion
121
In general the product of two rank-r matrices does not necessarily have to
produce another matrix of rank r. For example,
A =

1
2
2
4

and
B =

2
4
âˆ’1
âˆ’2

each has rank 1, but the product AB = 0 has rank 0. However, we saw in
(3.7.14) that the product of two invertible matrices is again invertible. That is, if
rank (AnÃ—n) = n and rank (BnÃ—n) = n, then rank (AB) = n. This generalizes
to any number of matrices.
Products of Nonsingular Matrices Are Nonsingular
If A1, A2, . . . , Ak are each n Ã— n nonsingular matrices, then the prod-
uct A1A2 Â· Â· Â· Ak is also nonsingular, and its inverse is given by the
reverse order law. That is,
(A1A2 Â· Â· Â· Ak)âˆ’1 = Aâˆ’1
k
Â· Â· Â· Aâˆ’1
2 Aâˆ’1
1 .
Proof.
Apply (3.7.14) and (3.7.15) inductively. For example, when k = 3 you
can write
(A1{A2A3})âˆ’1 = {A2A3}âˆ’1Aâˆ’1
1
= Aâˆ’1
3 Aâˆ’1
2 Aâˆ’1
1 .
Exercises for section 3.7
3.7.1. When possible, ï¬nd the inverse of each of the following matrices. Check
your answer by using matrix multiplication.
(a)

1
2
1
3

(b)

1
2
2
4

(c)
ï£«
ï£­
4
âˆ’8
5
4
âˆ’7
4
3
âˆ’4
2
ï£¶
ï£¸
(d)
ï£«
ï£­
1
2
3
4
5
6
7
8
9
ï£¶
ï£¸
(e)
ï£«
ï£¬
ï£­
1
1
1
1
1
2
2
2
1
2
3
3
1
2
3
4
ï£¶
ï£·
ï£¸
3.7.2. Find the matrix X such that X = AX + B, where
A =
ï£«
ï£­
0
âˆ’1
0
0
0
âˆ’1
0
0
0
ï£¶
ï£¸
and
B =
ï£«
ï£­
1
2
2
1
3
3
ï£¶
ï£¸.

122
Chapter 3
Matrix Algebra
3.7.3. For a square matrix A, explain why each of the following statements
must be true.
(a)
If A contains a zero row or a zero column, then A is singular.
(b)
If A contains two identical rows or two identical columns, then
A is singular.
(c)
If one row (or column) is a multiple of another row (or column),
then A must be singular.
3.7.4. Answer each of the following questions.
(a)
Under what conditions is a diagonal matrix nonsingular? De-
scribe the structure of the inverse of a diagonal matrix.
(b)
Under what conditions is a triangular matrix nonsingular? De-
scribe the structure of the inverse of a triangular matrix.
3.7.5. If A is nonsingular and symmetric, prove that Aâˆ’1 is symmetric.
3.7.6. If A is a square matrix such that I âˆ’A is nonsingular, prove that
A(I âˆ’A)âˆ’1 = (I âˆ’A)âˆ’1A.
3.7.7. Prove that if A is m Ã— n and B is n Ã— m such that AB = Im and
BA = In, then m = n.
3.7.8. If A, B, and A + B are each nonsingular, prove that
A(A + B)âˆ’1B = B(A + B)âˆ’1A =

Aâˆ’1 + Bâˆ’1	âˆ’1.
3.7.9. Let S be a skew-symmetric matrix with real entries.
(a)
Prove that I âˆ’S is nonsingular. Hint: xT x = 0
=â‡’
x = 0.
(b)
If A = (I + S)(I âˆ’S)âˆ’1, show that Aâˆ’1 = AT .
3.7.10. For matrices ArÃ—r, BsÃ—s, and CrÃ—s such that A and B are nonsin-
gular, verify that each of the following is true.
(a)

A
0
0
B
âˆ’1
=

Aâˆ’1
0
0
Bâˆ’1

(b)

A
C
0
B
âˆ’1
=

Aâˆ’1
âˆ’Aâˆ’1CBâˆ’1
0
Bâˆ’1


3.7 Matrix Inversion
123
3.7.11. Consider the block matrix

ArÃ—r
CrÃ—s
RsÃ—r
BsÃ—s

. When the indicated in-
verses exist, the matrices deï¬ned by
S = B âˆ’RAâˆ’1C
and
T = A âˆ’CBâˆ’1R
are called the Schur complements
20 of A and B, respectively.
(a)
If A and S are both nonsingular, verify that

A
C
R
B
âˆ’1
=

Aâˆ’1 + Aâˆ’1CSâˆ’1RAâˆ’1
âˆ’Aâˆ’1CSâˆ’1
âˆ’Sâˆ’1RAâˆ’1
Sâˆ’1

.
(b)
If B and T are nonsingular, verify that

A
C
R
B
âˆ’1
=

Tâˆ’1
âˆ’Tâˆ’1CBâˆ’1
âˆ’Bâˆ’1RTâˆ’1
Bâˆ’1 + Bâˆ’1RTâˆ’1CBâˆ’1

.
3.7.12. Suppose that A, B, C, and D are n Ã— n matrices such that ABT
and CDT are each symmetric and ADT âˆ’BCT = I. Prove that
AT D âˆ’CT B = I.
20
This is named in honor of the German mathematician Issai Schur (1875â€“1941), who ï¬rst studied
matrices of this type. Schur was a student and collaborator of Ferdinand Georg Frobenius
(p. 662). Schur and Frobenius were among the ï¬rst to study matrix theory as a discipline
unto itself, and each made great contributions to the subject. It was Emilie V. Haynsworth
(1916â€“1987)â€”a mathematical granddaughter of Schurâ€”who introduced the phrase â€œSchur
complementâ€ and developed several important aspects of the concept.

124
Chapter 3
Matrix Algebra
3.8
INVERSES OF SUMS AND SENSITIVITY
The reverse order law for inversion makes the inverse of a product easy to deal
with, but the inverse of a sum is much more diï¬ƒcult. To begin with, (A + B)âˆ’1
may not exist even if Aâˆ’1 and Bâˆ’1 each exist. Moreover, if (A + B)âˆ’1 exists,
then, with rare exceptions, (A + B)âˆ’1 Ì¸= Aâˆ’1 + Bâˆ’1. This doesnâ€™t even hold
for scalars (i.e., 1 Ã— 1 matrices), so it has no chance of holding in general.
There is no useful general formula for (A+B)âˆ’1, but there are some special
sums for which something can be said. One of the most easily inverted sums is
I + cdT in which c and d are n Ã— 1 nonzero columns such that 1 + dT c Ì¸= 0.
Itâ€™s straightforward to verify by direct multiplication that

I + cdT 	âˆ’1 = I âˆ’
cdT
1 + dT c.
(3.8.1)
If I is replaced by a nonsingular matrix A satisfying 1 + dT Aâˆ’1c Ì¸= 0, then
the reverse order law for inversion in conjunction with (3.8.1) yields
(A + cdT )âˆ’1 =

A(I + Aâˆ’1cdT )
âˆ’1
= (I + Aâˆ’1cdT )âˆ’1Aâˆ’1
=

I âˆ’
Aâˆ’1cdT
1 + dT Aâˆ’1c

Aâˆ’1 = Aâˆ’1 âˆ’Aâˆ’1cdT Aâˆ’1
1 + dT Aâˆ’1c .
This is often called the Shermanâ€“Morrison
21 rank-one update formula because
it can be shown (Exercise 3.9.9, p. 140) that rank (cdT ) = 1 when c Ì¸= 0 Ì¸= d.
Shermanâ€“Morrison Formula
â€¢
If AnÃ—n is nonsingular and if c and d are n Ã— 1 columns such
that 1 + dT Aâˆ’1c Ì¸= 0, then the sum A + cdT is nonsingular, and

A + cdT 	âˆ’1 = Aâˆ’1 âˆ’Aâˆ’1cdT Aâˆ’1
1 + dT Aâˆ’1c .
(3.8.2)
â€¢
The Shermanâ€“Morrisonâ€“Woodbury formula is a generalization. If C
and D are n Ã— k such that (I + DT Aâˆ’1C)âˆ’1 exists, then
(A + CDT )âˆ’1 = Aâˆ’1 âˆ’Aâˆ’1C(I + DT Aâˆ’1C)âˆ’1DT Aâˆ’1.
(3.8.3)
21
This result appeared in the 1949â€“1950 work of American statisticians J. Sherman and W. J.
Morrison, but they were not the ï¬rst to discover it. The formula was independently presented
by the English mathematician W. J. Duncan in 1944 and by American statisticians L. Guttman
(1946), Max Woodbury (1950), and M. S. Bartlett (1951). Since its derivation is so natural, it
almost certainly was discovered by many others along the way. Recognition and fame are often
not aï¬€orded simply for introducing an idea, but rather for applying the idea to a useful end.

3.8 Inverses of Sums and Sensitivity
125
The Shermanâ€“Morrisonâ€“Woodbury formula (3.8.3) can be veriï¬ed with di-
rect multiplication, or it can be derived as indicated in Exercise 3.8.6.
To appreciate the utility of the Shermanâ€“Morrison formula, suppose Aâˆ’1
is known from a previous calculation, but now one entry in A needs to be
changed or updatedâ€”say we need to add Î± to aij. Itâ€™s not necessary to start
from scratch to compute the new inverse because Shermanâ€“Morrison shows how
the previously computed information in Aâˆ’1 can be updated to produce the
new inverse. Let c = ei and d = Î±ej, where ei and ej are the ith and jth
unit columns, respectively. The matrix cdT has Î± in the (i, j)-position and
zeros elsewhere so that
B = A + cdT = A + Î±eieT
j
is the updated matrix. According to the Shermanâ€“Morrison formula,
Bâˆ’1 =

A + Î±eieT
j
	âˆ’1 = Aâˆ’1 âˆ’Î± Aâˆ’1eieT
j Aâˆ’1
1 + Î±eT
j Aâˆ’1ei
= Aâˆ’1 âˆ’Î±[Aâˆ’1]âˆ—i[Aâˆ’1]jâˆ—
1 + Î±[Aâˆ’1]ji
(recall Exercise 3.5.4).
(3.8.4)
This shows how Aâˆ’1 changes when aij is perturbed, and it provides a useful
algorithm for updating Aâˆ’1.
Example 3.8.1
Problem: Start with A and Aâˆ’1 given below. Update A by adding 1 to a21,
and then use the Shermanâ€“Morrison formula to update Aâˆ’1 :
A =

1
2
1
3

and
Aâˆ’1 =

3
âˆ’2
âˆ’1
1

.
Solution: The updated matrix is
B =

1
2
2
3

=

1
2
1
3

+

0
0
1
0

=

1
2
1
3

+

0
1

( 1
0 ) = A + e2eT
1 .
Applying the Shermanâ€“Morrison formula yields the updated inverse
Bâˆ’1 = Aâˆ’1 âˆ’Aâˆ’1e2eT
1 Aâˆ’1
1 + eT
1 Aâˆ’1e2
= Aâˆ’1 âˆ’[Aâˆ’1]âˆ—2[Aâˆ’1]1âˆ—
1 + [Aâˆ’1]12
=

3
âˆ’2
âˆ’1
1

âˆ’

âˆ’2
1

( 3
âˆ’2 )
1 âˆ’2
=

âˆ’3
2
2
âˆ’1

.

126
Chapter 3
Matrix Algebra
Another sum that often requires inversion is I âˆ’A, but we have to be
careful because (Iâˆ’A)âˆ’1 need not always exist. However, we are safe when the
entries in A are suï¬ƒciently small. In particular, if the entries in A are small
enough in magnitude to insure that limnâ†’âˆAn = 0, then, analogous to scalar
algebra,
(I âˆ’A)(I + A + A2 + Â· Â· Â· + Anâˆ’1) = I âˆ’An â†’I
as
n â†’âˆ,
so we have the following matrix version of a geometric series.
Neumann Series
If limnâ†’âˆAn = 0, then I âˆ’A is nonsingular and
(I âˆ’A)âˆ’1 = I + A + A2 + Â· Â· Â· =
âˆ

k=0
Ak.
(3.8.5)
This is the Neumann series. It provides approximations of (I âˆ’A)âˆ’1
when A has entries of small magnitude. For example, a ï¬rst-order ap-
proximation is (I âˆ’A)âˆ’1 â‰ˆI+A. More on the Neumann series appears
in Example 7.3.1, p. 527, and the complete statement is developed on
p. 618.
While there is no useful formula for (A + B)âˆ’1 in general, the Neumann
series allows us to say something when B has small entries relative to A, or
vice versa. For example, if Aâˆ’1 exists, and if the entries in B are small enough
in magnitude to insure that limnâ†’âˆ

Aâˆ’1B
	n = 0, then
(A + B)âˆ’1 =

A

I âˆ’

âˆ’Aâˆ’1B
	 âˆ’1
=

I âˆ’

âˆ’Aâˆ’1B
 âˆ’1
Aâˆ’1
=
 âˆ

k=0

âˆ’Aâˆ’1B
k

Aâˆ’1,
and a ï¬rst-order approximation is
(A + B)âˆ’1 â‰ˆAâˆ’1 âˆ’Aâˆ’1BAâˆ’1.
(3.8.6)
Consequently, if A is perturbed by a small matrix B, possibly resulting from
errors due to inexact measurements or perhaps from roundoï¬€error, then the
resulting change in Aâˆ’1 is about Aâˆ’1BAâˆ’1. In other words, the eï¬€ect of a
small perturbation (or error) B is magniï¬ed by multiplication (on both sides)
with Aâˆ’1, so if Aâˆ’1 has large entries, small perturbations (or errors) in A can
produce large perturbations (or errors) in the resulting inverse. You can reach

3.8 Inverses of Sums and Sensitivity
127
essentially the same conclusion from (3.8.4) when only a single entry is perturbed
and from Exercise 3.8.2 when a single column is perturbed.
This discussion resolves, at least in part, an issue raised in Â§1.6â€”namely,
â€œWhat mechanism determines the extent to which a nonsingular system Ax = b
is ill-conditioned?â€ To see how, an aggregate measure of the magnitude of the
entries in A is needed, and one common measure is
âˆ¥Aâˆ¥= max
i

j
|aij| = the maximum absolute row sum.
(3.8.7)
This is one example of a matrix norm, a detailed discussion of which is given in
Â§5.1. Theoretical properties speciï¬c to (3.8.7) are developed on pp. 280 and 283,
and one property established there is the fact that âˆ¥XYâˆ¥â‰¤âˆ¥Xâˆ¥âˆ¥Yâˆ¥for all
conformable matrices X and Y. But letâ€™s keep things on an intuitive level for
the time being and defer the details. Using the norm (3.8.7), the approximation
(3.8.6) insures that if âˆ¥Bâˆ¥is suï¬ƒciently small, then
$$Aâˆ’1 âˆ’(A + B)âˆ’1$$ â‰ˆ
$$Aâˆ’1BAâˆ’1$$ â‰¤
$$Aâˆ’1$$ âˆ¥Bâˆ¥
$$Aâˆ’1$$ ,
so, if we interpret x <âˆ¼y to mean that x is bounded above by something not
far from y, we can write
$$Aâˆ’1 âˆ’(A + B)âˆ’1$$
âˆ¥Aâˆ’1âˆ¥
<âˆ¼
$$Aâˆ’1$$ âˆ¥Bâˆ¥=
$$Aâˆ’1$$ âˆ¥Aâˆ¥
%âˆ¥Bâˆ¥
âˆ¥Aâˆ¥
&
.
The term on the left is the relative change in the inverse, and âˆ¥Bâˆ¥/ âˆ¥Aâˆ¥is the
relative change in A. The number Îº =
$$Aâˆ’1$$ âˆ¥Aâˆ¥is therefore the â€œmagniï¬-
cation factorâ€ that dictates how much the relative change in A is magniï¬ed.
This magniï¬cation factor Îº is called a condition number for A. In other
words, if Îº is small relative to 1 (i.e., if A is well conditioned), then a small
relative change (or error) in A cannot produce a large relative change (or error)
in the inverse, but if Îº is large (i.e., if A is ill conditioned), then a small rela-
tive change (or error) in A can possibly (but not necessarily) result in a large
relative change (or error) in the inverse.
The situation for linear systems is similar. If the coeï¬ƒcients in a nonsingular
system Ax = b are slightly perturbed to produce the system (A + B)Ëœx = b,
then x = Aâˆ’1b and Ëœx = (A + B)âˆ’1b so that (3.8.6) implies
x âˆ’Ëœx = Aâˆ’1b âˆ’(A + B)âˆ’1b â‰ˆAâˆ’1b âˆ’

Aâˆ’1 âˆ’Aâˆ’1BAâˆ’1	
b = Aâˆ’1Bx.
For column vectors, (3.8.7) reduces to âˆ¥xâˆ¥= maxi |xi|, and we have
âˆ¥x âˆ’Ëœxâˆ¥<âˆ¼
$$Aâˆ’1$$ âˆ¥Bâˆ¥âˆ¥xâˆ¥,

128
Chapter 3
Matrix Algebra
so the relative change in the solution is
âˆ¥x âˆ’Ëœxâˆ¥
âˆ¥xâˆ¥
<âˆ¼
$$Aâˆ’1$$ âˆ¥Bâˆ¥=
$$Aâˆ’1$$ âˆ¥Aâˆ¥
%âˆ¥Bâˆ¥
âˆ¥Aâˆ¥
&
= Îº
%âˆ¥Bâˆ¥
âˆ¥Aâˆ¥
&
.
(3.8.8)
Again, the condition number Îº is pivotal because when Îº is small, a small
relative change in A cannot produce a large relative change in x, but for larger
values of Îº, a small relative change in A can possibly result in a large relative
change in x. Below is a summary of these observations.
Sensitivity and Conditioning
â€¢
A nonsingular matrix A is said to be ill conditioned if a small
relative change in A can cause a large relative change in Aâˆ’1.
The degree of ill-conditioning is gauged by a condition number
Îº = âˆ¥Aâˆ¥âˆ¥Aâˆ’1âˆ¥, where âˆ¥â‹†âˆ¥is a matrix norm.
â€¢
The sensitivity of the solution of Ax = b to perturbations (or
errors) in A is measured by the extent to which A is an ill-
conditioned matrix. More is said in Example 5.12.1 on p. 414.
Example 3.8.2
It was demonstrated in Example 1.6.1 that the system
.835x + .667y = .168,
.333x + .266y = .067,
is sensitive to small perturbations. We can understand this in the current context
by examining the condition number of the coeï¬ƒcient matrix. If the matrix norm
(3.8.7) is employed with
A =

.835
.667
.333
.266

and
Aâˆ’1 =

âˆ’266000
667000
333000
âˆ’835000

,
then the condition number for A is
Îº = Îº = âˆ¥Aâˆ¥âˆ¥Aâˆ’1âˆ¥= (1.502)(1168000) = 1, 754, 336 â‰ˆ1.7 Ã— 106.
Since the right-hand side of (3.8.8) is only an estimate of the relative error in
the solution, the exact value of Îº is not as important as its order of magnitude.
Because Îº is of order 106, (3.8.8) holds the possibility that the relative change
(or error) in the solution can be about a million times larger than the relative

3.8 Inverses of Sums and Sensitivity
129
change (or error) in A. Therefore, we must consider A and the associated linear
system to be ill conditioned.
A Rule of Thumb. If Gaussian elimination with partial pivoting is used to
solve a well-scaled nonsingular system Ax = b using t -digit ï¬‚oating-point
arithmetic, then, assuming no other source of error exists, it can be argued that
when Îº is of order 10p, the computed solution is expected to be accurate to
at least t âˆ’p signiï¬cant digits, more or less. In other words, one expects to
lose roughly p signiï¬cant ï¬gures. For example, if Gaussian elimination with 8-
digit arithmetic is used to solve the 2 Ã— 2 system given above, then only about
t âˆ’p = 8 âˆ’6 = 2 signiï¬cant ï¬gures of accuracy should be expected. This
doesnâ€™t preclude the possibility of getting lucky and attaining a higher degree of
accuracyâ€”it just says that you shouldnâ€™t bet the farm on it.
The complete story of conditioning has not yet been told. As pointed out ear-
lier, itâ€™s about three times more costly to compute Aâˆ’1 than to solve Ax = b,
so it doesnâ€™t make sense to compute Aâˆ’1 just to estimate the condition of A.
Questions concerning condition estimation without explicitly computing an in-
verse still need to be addressed. Furthermore, liberties allowed by using the â‰ˆ
and
<âˆ¼symbols produce results that are intuitively correct but not rigorous.
Rigor will eventually be attainedâ€”see Example 5.12.1on p. 414.
Exercises for section 3.8
3.8.1. Suppose you are given that
A =
ï£«
ï£­
2
0
âˆ’1
âˆ’1
1
1
âˆ’1
0
1
ï£¶
ï£¸
and
Aâˆ’1 =
ï£«
ï£­
1
0
1
0
1
âˆ’1
1
0
2
ï£¶
ï£¸.
(a)
Use the Shermanâ€“Morrison formula to determine the inverse of
the matrix B that is obtained by changing the (3, 2)-entry in
A from 0 to 2.
(b)
Let C be the matrix that agrees with A except that c32 = 2
and c33 = 2. Use the Shermanâ€“Morrison formula to ï¬nd Câˆ’1.
3.8.2. Suppose A and B are nonsingular matrices in which B is obtained
from A by replacing Aâˆ—j with another column b. Use the Shermanâ€“
Morrison formula to derive the fact that
Bâˆ’1 = Aâˆ’1 âˆ’

Aâˆ’1b âˆ’ej
	
[Aâˆ’1]jâˆ—
[Aâˆ’1]jâˆ—b
.

130
Chapter 3
Matrix Algebra
3.8.3. Suppose the coeï¬ƒcient matrix of a nonsingular system Ax = b is up-
dated to produce another nonsingular system (A + cdT )z = b, where
b, c, d âˆˆâ„œnÃ—1, and let y be the solution of Ay = c. Show that
z = x âˆ’ydT x/(1 + dT y).
3.8.4.
(a)
Use the Shermanâ€“Morrison formula to prove that if A is non-
singular, then A + Î±eieT
j
is nonsingular for a suï¬ƒciently small
Î±.
(b)
Use part (a) to prove that I + E is nonsingular when all Ïµij â€™s
are suï¬ƒciently small in magnitude. This is an alternative to using
the Neumann series argument.
3.8.5. For given matrices A and B, where A is nonsingular, explain why
A + ÏµB is also nonsingular when the real number Ïµ is constrained to
a suï¬ƒciently small interval about the origin. In other words, prove that
small perturbations of nonsingular matrices are also nonsingular.
3.8.6. Derive the Shermanâ€“Morrisonâ€“Woodbury formula. Hint: Recall Exer-
cise 3.7.11, and consider the product
 I
C
0
I
 A
C
DT
âˆ’I
 I
0
DT
I

.
3.8.7. Using the norm (3.8.7), rank the following matrices according to their
degree of ill-conditioning:
A =
ï£«
ï£­
100
0
âˆ’100
0
100
âˆ’100
âˆ’100
âˆ’100
300
ï£¶
ï£¸,
B =
ï£«
ï£­
1
8
âˆ’1
âˆ’9
âˆ’71
11
1
17
18
ï£¶
ï£¸,
C =
ï£«
ï£­
1
22
âˆ’42
0
1
âˆ’45
âˆ’45
âˆ’948
1
ï£¶
ï£¸.
3.8.8. Suppose that the entries in A(t),
x(t), and b(t) are diï¬€erentiable
functions of a real variable t such that A(t)x(t) = b(t).
(a)
Assuming that A(t)âˆ’1 exists, explain why
dA(t)âˆ’1
dt
= âˆ’A(t)âˆ’1Aâ€²(t)A(t)âˆ’1.
(b)
Derive the equation
xâ€²(t) = A(t)âˆ’1bâ€²(t) âˆ’A(t)âˆ’1Aâ€²(t)x(t).
This shows that Aâˆ’1 magniï¬es both the change in A and the
change in b, and thus it conï¬rms the observation derived from
(3.8.8) saying that the sensitivity of a nonsingular system to
small perturbations is directly related to the magnitude of the
entries in Aâˆ’1.

3.9 Elementary Matrices and Equivalence
131
3.9
ELEMENTARY MATRICES AND EQUIVALENCE
A common theme in mathematics is to break complicated objects into more
elementary components, such as factoring large polynomials into products of
smaller polynomials. The purpose of this section is to lay the groundwork for
similar ideas in matrix algebra by considering how a general matrix might be
factored into a product of more â€œelementaryâ€ matrices.
Elementary Matrices
Matrices of the form Iâˆ’uvT , where u and v are n Ã— 1 columns such
that vT u Ì¸= 1 are called elementary matrices, and we know from
(3.8.1) that all such matrices are nonsingular and

I âˆ’uvT 	âˆ’1 = I âˆ’
uvT
vT u âˆ’1.
(3.9.1)
Notice that inverses of elementary matrices are elementary matrices.
We are primarily interested in the elementary matrices associated with the
three elementary row (or column) operations hereafter referred to as follows.
â€¢
Type I is interchanging rows (columns) i and j.
â€¢
Type II is multiplying row (column) i by Î± Ì¸= 0.
â€¢
Type III is adding a multiple of row (column) i to row (column) j.
An elementary matrix of Type I, II, or III is created by performing an elementary
operation of Type I, II, or III to an identity matrix. For example, the matrices
E1 =
ï£«
ï£­
0
1
0
1
0
0
0
0
1
ï£¶
ï£¸, E2 =
ï£«
ï£­
1
0
0
0
Î±
0
0
0
1
ï£¶
ï£¸,
and E3 =
ï£«
ï£­
1
0
0
0
1
0
Î±
0
1
ï£¶
ï£¸
(3.9.2)
are elementary matrices of Types I, II, and III, respectively, because E1 arises
by interchanging rows 1 and 2 in I3, whereas E2 is generated by multiplying
row 2 in I3 by Î±, and E3 is constructed by multiplying row 1 in I3 by Î±
and adding the result to row 3. The matrices in (3.9.2) also can be generated by
column operations. For example, E3 can be obtained by adding Î± times the
third column of I3 to the ï¬rst column. The fact that E1, E2, and E3 are of
the form (3.9.1) follows by using the unit columns ei to write
E1 = Iâˆ’uuT , where u = e1âˆ’e2,
E2 = Iâˆ’(1âˆ’Î±)e2eT
2 ,
and
E3 = I+Î±e3eT
1 .

132
Chapter 3
Matrix Algebra
These observations generalize to matrices of arbitrary size.
One of our objectives is to remove the arrows from Gaussian elimination
because the inability to do â€œarrow algebraâ€ limits the theoretical analysis. For
example, while it makes sense to add two equations together, there is no mean-
ingful analog for arrowsâ€”reducing A â†’B and C â†’D by row operations does
not guarantee that A + C â†’B + D is possible. The following properties are
the mechanisms needed to remove the arrows from elimination processes.
Properties of Elementary Matrices
â€¢
When used as a left-hand multiplier, an elementary matrix of Type
I, II, or III executes the corresponding row operation.
â€¢
When used as a right-hand multiplier, an elementary matrix of Type
I, II, or III executes the corresponding column operation.
Proof.
A proof for Type III operations is givenâ€”the other two cases are left to
the reader. Using I + Î±ejeT
i
as a left-hand multiplier on an arbitrary matrix A
produces

I + Î±ejeT
i
	
A = A + Î±ejAiâˆ—= A + Î±
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
0
Â· Â· Â·
0
...
...
...
ai1
ai2
Â· Â· Â·
ain
...
...
...
0
0
Â· Â· Â·
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
â†jth row .
This is exactly the matrix produced by a Type III row operation in which the
ith row of A is multiplied by Î± and added to the jth row. When I + Î±ejeT
i
is used as a right-hand multiplier on A, the result is
A

I + Î±ejeT
i
	
= A + Î±Aâˆ—jeT
i = A + Î±
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
ith col
â†“
0
Â· Â· Â·
a1j
Â· Â· Â·
0
0
Â· Â· Â·
a2j
Â· Â· Â·
0
...
...
...
0
Â· Â· Â·
anj
Â· Â· Â·
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
This is the result of a Type III column operation in which the jth column of A
is multiplied by Î± and then added to the ith column.

3.9 Elementary Matrices and Equivalence
133
Example 3.9.1
The sequence of row operations used to reduce A =
ï£«
ï£­
1
2
4
2
4
8
3
6
13
ï£¶
ï£¸to EA is
indicated below.
A =
ï£«
ï£­
1
2
4
2
4
8
3
6
13
ï£¶
ï£¸R2 âˆ’2R1
R3 âˆ’3R1
âˆ’â†’
ï£«
ï£­
1
2
4
0
0
0
0
0
1
ï£¶
ï£¸
Interchange R2 and R3
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’
ï£«
ï£­
1
2
4
0
0
1
0
0
0
ï£¶
ï£¸
R1 âˆ’4R2
âˆ’â†’
ï£«
ï£­
1
2
0
0
0
1
0
0
0
ï£¶
ï£¸= EA.
The reduction can be accomplished by a sequence of left-hand multiplications
with the corresponding elementary matrices as shown below.
ï£«
ï£­
1
âˆ’4
0
0
1
0
0
0
1
ï£¶
ï£¸
ï£«
ï£­
1
0
0
0
0
1
0
1
0
ï£¶
ï£¸
ï£«
ï£­
1
0
0
0
1
0
âˆ’3
0
1
ï£¶
ï£¸
ï£«
ï£­
1
0
0
âˆ’2
1
0
0
0
1
ï£¶
ï£¸A = EA.
The product of these elementary matrices is P =
ï£«
ï£­
13
0
âˆ’4
âˆ’3
0
1
âˆ’2
1
0
ï£¶
ï£¸, and you can
verify that it is indeed the case that PA = EA. Thus the arrows are eliminated
by replacing them with a product of elementary matrices.
We are now in a position to understand why nonsingular matrices are pre-
cisely those matrices that can be factored as a product of elementary matrices.
Products of Elementary Matrices
â€¢
A is a nonsingular matrix if and only if A is the product
of elementary matrices of Type I, II, or III.
(3.9.3)
Proof.
If A is nonsingular, then the Gaussâ€“Jordan technique reduces A to
I by row operations. If G1, G2, . . . , Gk is the sequence of elementary matrices
that corresponds to the elementary row operations used, then
Gk Â· Â· Â· G2G1A = I or, equivalently, A = Gâˆ’1
1 Gâˆ’1
2
Â· Â· Â· Gâˆ’1
k .
Since the inverse of an elementary matrix is again an elementary matrix of the
same type, this proves that A is the product of elementary matrices of Type I,
II, or III. Conversely, if A = E1E2 Â· Â· Â· Ek is a product of elementary matrices,
then A must be nonsingular because the Ei â€™s are nonsingular, and a product
of nonsingular matrices is also nonsingular.

134
Chapter 3
Matrix Algebra
Equivalence
â€¢
Whenever B can be derived from A by a combination of elementary
row and column operations, we write A âˆ¼B, and we say that A
and B are equivalent matrices. Since elementary row and column
operations are left-hand and right-hand multiplication by elementary
matrices, respectively, and in view of (3.9.3), we can say that
A âˆ¼B â‡â‡’PAQ = B
for nonsingular P and Q.
â€¢
Whenever B can be obtained from A by performing a sequence
of elementary row operations only, we write A
row
âˆ¼B, and we say
that A and B are row equivalent. In other words,
A
row
âˆ¼B â‡â‡’PA = B
for a nonsingular P.
â€¢
Whenever B can be obtained from A by performing a sequence of
column operations only, we write A
col
âˆ¼B, and we say that A and
B are column equivalent. In other words,
A
col
âˆ¼B â‡â‡’AQ = B
for a nonsingular Q.
If itâ€™s possible to go from A to B by elementary row and column oper-
ations, then clearly itâ€™s possible to start with B and get back to A because
elementary operations are reversibleâ€”i.e., PAQ = B
=â‡’
Pâˆ’1BQâˆ’1 = A. It
therefore makes sense to talk about the equivalence of a pair of matrices without
regard to order. In other words, A âˆ¼B â‡â‡’B âˆ¼A. Furthermore, itâ€™s not
diï¬ƒcult to see that each type of equivalence is transitive in the sense that
A âˆ¼B
and
B âˆ¼C
=â‡’
A âˆ¼C.
In Â§2.2 it was stated that each matrix A possesses a unique reduced row
echelon form EA, and we accepted this fact because it is intuitively evident.
However, we are now in a position to understand a rigorous proof.
Example 3.9.2
Problem: Prove that EA is uniquely determined by A.
Solution:
Without loss of generality, we may assume that A is squareâ€”
otherwise the appropriate number of zero rows or columns can be adjoined to A
without aï¬€ecting the results. Suppose that A
row
âˆ¼E1 and A
row
âˆ¼E2, where E1
and E2 are both in reduced row echelon form. Consequently, E1
row
âˆ¼E2, and
hence there is a nonsingular matrix P such that
PE1 = E2.
(3.9.4)

3.9 Elementary Matrices and Equivalence
135
Furthermore, by permuting the rows of E1 and E2 to force the pivotal 1â€™s to
occupy the diagonal positions, we see that
E1
row
âˆ¼T1
and
E2
row
âˆ¼T2,
(3.9.5)
where T1 and T2 are upper-triangular matrices in which the basic columns in
each Ti occupy the same positions as the basic columns in Ei. For example, if
E =
ï£«
ï£­
1
2
0
0
0
1
0
0
0
ï£¶
ï£¸, then
T =
ï£«
ï£­
1
2
0
0
0
0
0
0
1
ï£¶
ï£¸.
Each Ti has the property that T2
i = Ti because there is a permutation
matrix Qi (a product of elementary interchange matrices of Type I) such that
QiTiQT
i =

Iri
Ji
0
0

or, equivalently,
Ti = QT
i

Iri
Ji
0
0

Qi,
and QT
i = Qâˆ’1
i
(see Exercise 3.9.4) implies T2
i = Ti. It follows from (3.9.5)
that T1
row
âˆ¼T2, so there is a nonsingular matrix R such that RT1 = T2. Thus
T2 = RT1 = RT1T1 = T2T1
and
T1 = Râˆ’1T2 = Râˆ’1T2T2 = T1T2.
Because T1 and T2 are both upper triangular, T1T2 and T2T1 have the same
diagonal entries, and hence T1 and T2 have the same diagonal. Therefore, the
positions of the basic columns (i.e., the pivotal positions) in T1 agree with those
in T2, and hence E1 and E2 have basic columns in exactly the same positions.
This means there is a permutation matrix Q such that
E1Q =

Ir
J1
0
0

and
E2Q =

Ir
J2
0
0

.
Using (3.9.4) yields PE1Q = E2Q, or

P11
P12
P21
P22
 
Ir
J1
0
0

=

Ir
J2
0
0

,
which in turn implies that P11 = Ir and P11J1 = J2. Consequently, J1 = J2,
and it follows that E1 = E2.
In passing, notice that the uniqueness of EA implies the uniqueness of the
pivot positions in any other row echelon form derived from A. If A
row
âˆ¼U1
and A
row
âˆ¼U2, where U1 and U2 are row echelon forms with diï¬€erent pivot
positions, then Gaussâ€“Jordan reduction applied to U1 and U2 would lead to
two diï¬€erent reduced echelon forms, which is impossible.
In Â§2.2 we observed the fact that the column relationships in a matrix A
are exactly the same as the column relationships in EA. This observation is a
special case of the more general result presented below.

136
Chapter 3
Matrix Algebra
Column and Row Relationships
â€¢
If A
row
âˆ¼B, then linear relationships existing among columns of A
also hold among corresponding columns of B. That is,
Bâˆ—k =
n

j=1
Î±jBâˆ—j
if and only if
Aâˆ—k =
n

j=1
Î±jAâˆ—j.
(3.9.6)
â€¢
In particular, the column relationships in A and EA must be iden-
tical, so the nonbasic columns in A must be linear combinations of
the basic columns in A as described in (2.2.3).
â€¢
If A
col
âˆ¼B, then linear relationships existing among rows of A must
also hold among corresponding rows of B.
â€¢
Summary. Row equivalence preserves column relationships, and col-
umn equivalence preserves row relationships.
Proof.
If A
row
âˆ¼B, then PA = B for some nonsingular P. Recall from (3.5.5)
that the jth column in B is given by
Bâˆ—j = (PA)âˆ—j = PAâˆ—j.
Therefore, if Aâˆ—k = 
j Î±jAâˆ—j, then multiplication by P on the left produces
Bâˆ—k = 
j Î±jBâˆ—j. Conversely, if Bâˆ—k = 
j Î±jBâˆ—j, then multiplication on the
left by Pâˆ’1 produces Aâˆ—k = 
j Î±jAâˆ—j. The statement concerning column
equivalence follows by considering transposes.
The reduced row echelon form EA is as far as we can go in reducing A by
using only row operations. However, if we are allowed to use row operations in
conjunction with column operations, then, as described below, the end result of
a complete reduction is much simpler.
Rank Normal Form
If A is an m Ã— n matrix such that rank (A) = r, then
A âˆ¼Nr =

Ir
0
0
0

.
(3.9.7)
Nr is called the rank normal form for A, and it is the end product
of a complete reduction of A by using both row and column operations.

3.9 Elementary Matrices and Equivalence
137
Proof.
It is always true that A
row
âˆ¼EA so that there is a nonsingular matrix
P such that PA = EA. If rank (A) = r, then the basic columns in EA are
the r unit columns. Apply column interchanges to EA so as to move these r
unit columns to the far left-hand side. If Q1 is the product of the elementary
matrices corresponding to these column interchanges, then PAQ1 has the form
PAQ1 = EAQ1 =

Ir
J
0
0

.
Multiplying both sides of this equation on the right by the nonsingular matrix
Q2 =

Ir
âˆ’J
0
I

produces PAQ1Q2 =

Ir
J
0
0
 
Ir
âˆ’J
0
I

=

Ir
0
0
0

.
Thus A âˆ¼Nr. because P and Q = Q1Q2 are nonsingular.
Example 3.9.3
Problem: Explain why rank
 A
0
0
B

= rank (A) + rank (B).
Solution:
If rank (A) = r and rank (B) = s, then A âˆ¼Nr and B âˆ¼Ns.
Consequently,

A
0
0
B

âˆ¼

Nr
0
0
Ns

=â‡’
rank

A
0
0
B

= rank

Nr
0
0
Ns

= r + s.
Given matrices A and B, how do we decide whether or not A âˆ¼B,
A
row
âˆ¼B, or A
col
âˆ¼B? We could use a trial-and-error approach by attempting to
reduce A to B by elementary operations, but this would be silly because there
are easy tests, as described below.
Testing for Equivalence
For m Ã— n matrices A and B the following statements are true.
â€¢
A âˆ¼B if and only if rank (A) = rank (B).
(3.9.8)
â€¢
A
row
âˆ¼B if and only if EA = EB.
(3.9.9)
â€¢
A
col
âˆ¼B if and only if EAT = EBT .
(3.9.10)
Corollary. Multiplication by nonsingular matrices cannot change rank.

138
Chapter 3
Matrix Algebra
Proof.
To establish the validity of (3.9.8), observe that rank (A) = rank (B)
implies A âˆ¼Nr and B âˆ¼Nr. Therefore, A âˆ¼Nr âˆ¼B. Conversely, if A âˆ¼B,
where rank (A) = r and rank (B) = s, then A âˆ¼Nr and B âˆ¼Ns, and
hence Nr âˆ¼A âˆ¼B âˆ¼Ns. Clearly, Nr âˆ¼Ns implies r = s. To prove (3.9.9),
suppose ï¬rst that A
row
âˆ¼B. Because B
row
âˆ¼EB, it follows that A
row
âˆ¼EB. Since
a matrix has a uniquely determined reduced echelon form, it must be the case
that EB = EA. Conversely, if EA = EB, then
A
row
âˆ¼EA = EB
row
âˆ¼B
=â‡’
A
row
âˆ¼B.
The proof of (3.9.10) follows from (3.9.9) by considering transposes because
A
col
âˆ¼B â‡â‡’AQ = B â‡â‡’(AQ)T = BT
â‡â‡’QT AT = BT â‡â‡’AT row
âˆ¼BT .
Example 3.9.4
Problem: Are the relationships that exist among the columns in A the same
as the column relationships in B, and are the row relationships in A the same
as the row relationships in B, where
A =
ï£«
ï£­
1
1
1
âˆ’4
âˆ’3
âˆ’1
2
1
âˆ’1
ï£¶
ï£¸
and
B =
ï£«
ï£­
âˆ’1
âˆ’1
âˆ’1
2
2
2
2
1
âˆ’1
ï£¶
ï£¸?
Solution: Straightforward computation reveals that
EA = EB =
ï£«
ï£­
1
0
âˆ’2
0
1
3
0
0
0
ï£¶
ï£¸,
and hence A
row
âˆ¼B. Therefore, the column relationships in A and B must be
identical, and they must be the same as those in EA. Examining EA reveals
that Eâˆ—3 = âˆ’2Eâˆ—1 + 3Eâˆ—2, so it must be the case that
Aâˆ—3 = âˆ’2Aâˆ—1 + 3Aâˆ—2
and
Bâˆ—3 = âˆ’2Bâˆ—1 + 3Bâˆ—2.
The row relationships in A and B are diï¬€erent because EAT Ì¸= EBT .
On the surface, it may not seem plausible that a matrix and its transpose
should have the same rank. After all, if A is 3 Ã— 100, then A can have as
many as 100 basic columns, but AT can have at most three. Nevertheless, we
can now demonstrate that rank (A) = rank

AT 	
.

3.9 Elementary Matrices and Equivalence
139
Transposition and Rank
Transposition does not change the rankâ€”i.e., for all m Ã— n matrices,
rank (A) = rank

AT 	
and
rank (A) = rank (Aâˆ—).
(3.9.11)
Proof.
Let rank (A) = r, and let P and Q be nonsingular matrices such that
PAQ = Nr =

Ir
0rÃ—nâˆ’r
0mâˆ’rÃ—r
0mâˆ’rÃ—nâˆ’r

.
Applying the reverse order law for transposition produces QT AT PT = NT
r .
Since QT and PT are nonsingular, it follows that AT âˆ¼NT
r , and therefore
rank

AT 	
= rank

NT
r
	
= rank

Ir
0rÃ—mâˆ’r
0nâˆ’rÃ—r
0nâˆ’rÃ—mâˆ’r

= r = rank (A).
To prove rank (A) = rank (Aâˆ—), write Nr = Nr = PAQ = Â¯PÂ¯AÂ¯Q, and use the
fact that the conjugate of a nonsingular matrix is again nonsingular (because
Â¯Kâˆ’1 = Kâˆ’1 ) to conclude that Nr âˆ¼A, and hence rank (A) = rank
Â¯A
	
. It
now follows from rank (A) = rank

AT 	
that
rank (Aâˆ—) = rank
Â¯AT 	
= rank
Â¯A
	
= rank (A).
Exercises for section 3.9
3.9.1. Suppose that A is an m Ã— n matrix.
(a)
If [A|Im] is row reduced to a matrix [B|P], explain why P
must be a nonsingular matrix such that PA = B.
(b)
If

A
In
!
is column reduced to

C
Q
!
, explain why Q must be a
nonsingular matrix such that AQ = C.
(c)
Find a nonsingular matrix P such that PA = EA, where
A =
ï£«
ï£­
1
2
3
4
2
4
6
7
1
2
3
6
ï£¶
ï£¸.
(d)
Find nonsingular matrices P and Q such that PAQ is in rank
normal form.

140
Chapter 3
Matrix Algebra
3.9.2. Consider the two matrices
A =
ï£«
ï£­
2
2
0
âˆ’1
3
âˆ’1
4
0
0
âˆ’8
8
3
ï£¶
ï£¸
and
B =
ï£«
ï£­
2
âˆ’6
8
2
5
1
4
âˆ’1
3
âˆ’9
12
3
ï£¶
ï£¸.
(a)
Are A and B equivalent?
(b)
Are A and B row equivalent?
(c)
Are A and B column equivalent?
3.9.3. If A
row
âˆ¼B, explain why the basic columns in A occupy exactly the
same positions as the basic columns in B.
3.9.4. A product of elementary interchange matricesâ€”i.e., elementary matrices
of Type Iâ€”is called a permutation matrix. If P is a permutation
matrix, explain why Pâˆ’1 = PT .
3.9.5. If AnÃ—n is a nonsingular matrix, which (if any) of the following state-
ments are true?
(a)
A âˆ¼Aâˆ’1.
(b)
A
row
âˆ¼Aâˆ’1.
(c)
A
col
âˆ¼Aâˆ’1.
(d)
A âˆ¼I.
(e)
A
row
âˆ¼I.
(f)
A
col
âˆ¼I.
3.9.6. Which (if any) of the following statements are true?
(a)
A âˆ¼B
=â‡’
AT âˆ¼BT .
(b)
A
row
âˆ¼B
=â‡’
AT row
âˆ¼BT .
(c)
A
row
âˆ¼B
=â‡’
AT col
âˆ¼BT .
(d)
A
row
âˆ¼B
=â‡’
A âˆ¼B.
(e)
A
col
âˆ¼B
=â‡’
A âˆ¼B.
(f)
A âˆ¼B
=â‡’
A
row
âˆ¼B.
3.9.7. Show that every elementary matrix of Type I can be written as a product
of elementary matrices of Types II and III. Hint: Recall Exercise 1.2.12
on p. 14.
3.9.8. If rank (AmÃ—n) = r, show that there exist matrices BmÃ—r and CrÃ—n
such that A = BC, where rank (B) = rank (C) = r. Such a factor-
ization is called a full-rank factorization.
Hint: Consider the basic
columns of A and the nonzero rows of EA.
3.9.9. Prove that rank (AmÃ—n) = 1 if and only if there are nonzero columns
umÃ—1 and vnÃ—1 such that
A = uvT .
3.9.10. Prove that if rank (AnÃ—n) = 1, then A2 = Ï„A, where Ï„ = trace (A).

3.10 The LU Factorization
141
3.10
THE LU FACTORIZATION
We have now come full circle, and we are back to where the text beganâ€”solving
a nonsingular system of linear equations using Gaussian elimination with back
substitution. This time, however, the goal is to describe and understand the
process in the context of matrices.
If Ax = b is a nonsingular system, then the object of Gaussian elimination
is to reduce A to an upper-triangular matrix using elementary row operations.
If no zero pivots are encountered, then row interchanges are not necessary, and
the reduction can be accomplished by using only elementary row operations of
Type III. For example, consider reducing the matrix
A =
ï£«
ï£­
2
2
2
4
7
7
6
18
22
ï£¶
ï£¸
to upper-triangular form as shown below:
ï£«
ï£­
2
2
2
4
7
7
6
18
22
ï£¶
ï£¸R2 âˆ’2R1
R3 âˆ’3R1
âˆ’â†’
ï£«
ï£­
2
2
2
0
3
3
0
12
16
ï£¶
ï£¸
R3 âˆ’4R2
âˆ’â†’
ï£«
ï£­
2
2
2
0
3
3
0
0
4
ï£¶
ï£¸= U.
(3.10.1)
We learned in the previous section that each of these Type III operations can be
executed by means of a left-hand multiplication with the corresponding elemen-
tary matrix Gi, and the product of all of these Gi â€™s is
G3G2G1 =
ï£«
ï£­
1
0
0
0
1
0
0
âˆ’4
1
ï£¶
ï£¸
ï£«
ï£­
1
0
0
0
1
0
âˆ’3
0
1
ï£¶
ï£¸
ï£«
ï£­
1
0
0
âˆ’2
1
0
0
0
1
ï£¶
ï£¸=
ï£«
ï£­
1
0
0
âˆ’2
1
0
5
âˆ’4
1
ï£¶
ï£¸.
In other words, G3G2G1A = U, so that A = Gâˆ’1
1 Gâˆ’1
2 Gâˆ’1
3 U = LU, where
L is the lower-triangular matrix
L = Gâˆ’1
1 Gâˆ’1
2 Gâˆ’1
3
=
ï£«
ï£­
1
0
0
2
1
0
3
4
1
ï£¶
ï£¸.
Thus A = LU is a product of a lower-triangular matrix L and an upper-
triangular matrix U. Naturally, this is called an LU factorization of A.

142
Chapter 3
Matrix Algebra
Observe that U is the end product of Gaussian elimination and has the
pivots on its diagonal, while L has 1â€™s on its diagonal. Moreover, L has the
remarkable property that below its diagonal, each entry â„“ij is precisely the
multiplier used in the elimination (3.10.1) to annihilate the (i, j)-position.
This is characteristic of what happens in general. To develop the gen-
eral theory, itâ€™s convenient to introduce the concept of an elementary lower-
triangular matrix, which is deï¬ned to be an n Ã— n triangular matrix of the
form
Tk = I âˆ’ckeT
k ,
where ck is a column with zeros in the ï¬rst k positions. In particular, if
ck =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
0
...
Âµk+1
...
Âµn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
then
Tk =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
Â· Â· Â·
0
0
Â· Â· Â·
0
0
1
Â· Â· Â·
0
0
Â· Â· Â·
0
...
...
...
...
...
...
0
0
Â· Â· Â·
1
0
Â· Â· Â·
0
0
0
Â· Â· Â·
âˆ’Âµk+1
1
Â· Â· Â·
0
...
...
...
...
...
...
0
0
Â· Â· Â·
âˆ’Âµn
0
Â· Â· Â·
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
(3.10.2)
By observing that eT
k ck = 0, the formula for the inverse of an elementary matrix
given in (3.9.1) produces
Tâˆ’1
k
= I + ckeT
k =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
Â· Â· Â·
0
0
Â· Â· Â·
0
0
1
Â· Â· Â·
0
0
Â· Â· Â·
0
...
...
...
...
...
...
0
0
Â· Â· Â·
1
0
Â· Â· Â·
0
0
0
Â· Â· Â·
Âµk+1
1
Â· Â· Â·
0
...
...
...
...
...
...
0
0
Â· Â· Â·
Âµn
0
Â· Â· Â·
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
(3.10.3)
which is also an elementary lower-triangular matrix. The utility of elementary
lower-triangular matrices lies in the fact that all of the Type III row operations
needed to annihilate the entries below the kth pivot can be accomplished with
one multiplication by Tk. If
Akâˆ’1 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
Â· Â· Â·
Î±1
âˆ—
Â· Â· Â·
âˆ—
0
âˆ—
Â· Â· Â·
Î±2
âˆ—
Â· Â· Â·
âˆ—
...
...
...
...
...
...
0
0
Â· Â· Â·
Î±k
âˆ—
Â· Â· Â·
âˆ—
0
0
Â· Â· Â·
Î±k+1
âˆ—
Â· Â· Â·
âˆ—
...
...
...
...
...
...
0
0
Â· Â· Â·
Î±n
âˆ—
Â· Â· Â·
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸

3.10 The LU Factorization
143
is the partially triangularized result after k âˆ’1 elimination steps, then
TkAkâˆ’1 =

I âˆ’ckeT
k
	
Akâˆ’1 = Akâˆ’1 âˆ’ckeT
k Akâˆ’1
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
Â· Â· Â·
Î±1
âˆ—
Â· Â· Â·
âˆ—
0
âˆ—
Â· Â· Â·
Î±2
âˆ—
Â· Â· Â·
âˆ—
...
...
...
...
...
...
0
0
Â· Â· Â·
Î±k
âˆ—
Â· Â· Â·
âˆ—
0
0
Â· Â· Â·
0
âˆ—
Â· Â· Â·
âˆ—
...
...
...
...
...
...
0
0
Â· Â· Â·
0
âˆ—
Â· Â· Â·
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
where
ck =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
0
...
0
Î±k+1/Î±k
...
Î±n/Î±k
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
contains the multipliers used to annihilate those entries below Î±k. Notice that
Tk does not alter the ï¬rst k âˆ’1 columns of Akâˆ’1 because eT
k [Akâˆ’1]âˆ—j = 0
whenever j â‰¤kâˆ’1. Therefore, if no row interchanges are required, then reducing
A to an upper-triangular matrix U by Gaussian elimination is equivalent to
executing a sequence of n âˆ’1 left-hand multiplications with elementary lower-
triangular matrices. That is, Tnâˆ’1 Â· Â· Â· T2T1A = U, and hence
A = Tâˆ’1
1 Tâˆ’1
2
Â· Â· Â· Tâˆ’1
nâˆ’1U.
(3.10.4)
Making use of the fact that eT
j ck = 0 whenever j â‰¤k and applying (3.10.3)
reveals that
Tâˆ’1
1 Tâˆ’1
2
Â· Â· Â· Tâˆ’1
nâˆ’1 =

I + c1eT
1
	 
I + c2eT
2
	
Â· Â· Â·

I + cnâˆ’1eT
nâˆ’1
	
= I + c1eT
1 + c2eT
2 + Â· Â· Â· + cnâˆ’1eT
nâˆ’1.
(3.10.5)
By observing that
ckeT
k =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
0
Â· Â· Â·
0
0
Â· Â· Â·
0
0
0
Â· Â· Â·
0
0
Â· Â· Â·
0
...
...
...
...
...
...
0
0
Â· Â· Â·
0
0
Â· Â· Â·
0
0
0
Â· Â· Â·
â„“k+1,k
0
Â· Â· Â·
0
...
...
...
...
...
...
0
0
Â· Â· Â·
â„“nk
0
Â· Â· Â·
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
where the â„“ik â€™s are the multipliers used at the kth stage to annihilate the entries
below the kth pivot, it now follows from (3.10.4) and (3.10.5) that
A = LU,

144
Chapter 3
Matrix Algebra
where
L = I + c1eT
1 + c2eT
2 + Â· Â· Â· + cnâˆ’1eT
nâˆ’1 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
0
Â· Â· Â·
0
â„“21
1
0
Â· Â· Â·
0
â„“31
â„“32
1
Â· Â· Â·
0
...
...
...
...
...
â„“n1
â„“n2
â„“n3
Â· Â· Â·
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
(3.10.6)
is the lower-triangular matrix with 1â€™s on the diagonal, and where â„“ij is precisely
the multiplier used to annihilate the (i, j) -position during Gaussian elimination.
Thus the factorization A = LU can be viewed as the matrix formulation of
Gaussian elimination, with the understanding that no row interchanges are used.
LU Factorization
If A is an n Ã— n matrix such that a zero pivot is never encountered
when applying Gaussian elimination with Type III operations, then A
can be factored as the product A = LU, where the following hold.
â€¢
L is lower triangular and U is upper triangular.
(3.10.7)
â€¢
â„“ii = 1 and uii Ì¸= 0 for each i = 1, 2, . . . , n.
(3.10.8)
â€¢
Below the diagonal of L, the entry â„“ij is the multiple of row j that
is subtracted from row i in order to annihilate the (i, j) -position
during Gaussian elimination.
â€¢
U is the ï¬nal result of Gaussian elimination applied to A.
â€¢
The matrices L and U are uniquely determined by properties
(3.10.7) and (3.10.8).
The decomposition of A into A = LU is called the LU factorization
of A, and the matrices L and U are called the LU factors of A.
Proof.
Except for the statement concerning the uniqueness of the LU fac-
tors, each point has already been established. To prove uniqueness, observe
that LU factors must be nonsingular because they have nonzero diagonals. If
L1U1 = A = L2U2 are two LU factorizations for A, then
Lâˆ’1
2 L1 = U2Uâˆ’1
1 .
(3.10.9)
Notice that Lâˆ’1
2 L1 is lower triangular, while U2Uâˆ’1
1
is upper triangular be-
cause the inverse of a matrix that is upper (lower) triangular is again up-
per (lower) triangular, and because the product of two upper (lower) trian-
gular matrices is also upper (lower) triangular. Consequently, (3.10.9) implies
Lâˆ’1
2 L1 = D = U2Uâˆ’1
1
must be a diagonal matrix. However, [L2]ii = 1 =
[Lâˆ’1
2 ]ii, so it must be the case that Lâˆ’1
2 L1 = I = U2Uâˆ’1
1 , and thus L1 = L2
and U1 = U2.

3.10 The LU Factorization
145
Example 3.10.1
Once L and U are known, there is usually no need to manipulate with A. This
together with the fact that the multipliers used in Gaussian elimination occur in
just the right places in L means that A can be successively overwritten with the
information in L and U as Gaussian elimination evolves. The rule is to store
the multiplier â„“ij in the position it annihilatesâ€”namely, the (i, j)-position of
the array. For a 3 Ã— 3 matrix, the result looks like this:
ï£«
ï£­
a11
a12
a13
a21
a22
a23
a31
a32
a33
ï£¶
ï£¸
T ype III operations
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’
ï£«
ï£­
u11
u12
u13
â„“21
u22
u23
â„“31
â„“32
u33
ï£¶
ï£¸.
For example, generating the LU factorization of
A =
ï£«
ï£­
2
2
2
4
7
7
6
18
22
ï£¶
ï£¸
by successively overwriting a single 3 Ã— 3 array would evolve as shown below:
ï£«
ï£­
2
2
2
4
7
7
6
18
22
ï£¶
ï£¸R2 âˆ’2R1
R3 âˆ’3R1
âˆ’â†’
ï£«
ï£­
2
2
2
âƒ
2
3
3
âƒ
3
12
16
ï£¶
ï£¸
R3 âˆ’4R2
âˆ’â†’
ï£«
ï£­
2
2
2
âƒ
2
3
3
âƒ
3
âƒ
4
4
ï£¶
ï£¸.
Thus
L =
ï£«
ï£­
1
0
0
2
1
0
3
4
1
ï£¶
ï£¸
and
U =
ï£«
ï£­
2
2
2
0
3
3
0
0
4
ï£¶
ï£¸.
This is an important feature in practical computation because it guarantees that
an LU factorization requires no more computer memory than that required to
store the original matrix A.
Once the LU factors for a nonsingular matrix AnÃ—n have been obtained,
itâ€™s relatively easy to solve a linear system Ax = b. By rewriting Ax = b as
L(Ux) = b
and setting
y = Ux,
we see that Ax = b is equivalent to the two triangular systems
Ly = b
and
Ux = y.
First, the lower-triangular system Ly = b is solved for y by forward substi-
tution. That is, if
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
0
Â· Â· Â·
0
â„“21
1
0
Â· Â· Â·
0
â„“31
â„“32
1
Â· Â· Â·
0
...
...
...
...
...
â„“n1
â„“n2
â„“n3
Â· Â· Â·
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
y1
y2
y3
...
yn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
b1
b2
b3
...
bn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
,

146
Chapter 3
Matrix Algebra
set
y1 = b1,
y2 = b2 âˆ’â„“21y1,
y3 = b3 âˆ’â„“31y1 âˆ’â„“32y2,
etc.
The forward substitution algorithm can be written more concisely as
y1 = b1
and
yi = bi âˆ’
iâˆ’1

k=1
â„“ikyk
for
i = 2, 3, . . . , n.
(3.10.10)
After y is known, the upper-triangular system Ux = y is solved using the
standard back substitution procedure by starting with xn = yn/unn, and setting
xi = 1
uii

yi âˆ’
n

k=i+1
uikxk

for
i = n âˆ’1, n âˆ’2, . . . , 1.
(3.10.11)
It can be veriï¬ed that only n2 multiplications/divisions and n2 âˆ’n addi-
tions/subtractions are required when (3.10.10) and (3.10.11) are used to solve
the two triangular systems Ly = b and Ux = y, so itâ€™s relatively cheap to
solve Ax = b once L and U are knownâ€”recall from Â§1.2 that these operation
counts are about n3/3 when we start from scratch.
If only one system Ax = b is to be solved, then there is no signiï¬cant
diï¬€erence between the technique of reducing the augmented matrix [A|b] to
a row echelon form and the LU factorization method presented here. However,
suppose it becomes necessary to later solve other systems Ax = Ëœb with the
same coeï¬ƒcient matrix but with diï¬€erent right-hand sides, which is frequently
the case in applied work. If the LU factors of A were computed and saved
when the original system was solved, then they need not be recomputed, and
the solutions to all subsequent systems Ax = Ëœb are therefore relatively cheap
to obtain. That is, the operation counts for each subsequent system are on the
order of n2, whereas these counts would be on the order of n3/3 if we would
start from scratch each time.
Summary
â€¢
To solve a nonsingular system Ax = b using the LU factorization
A = LU, ï¬rst solve Ly = b for y with the forward substitution
algorithm (3.10.10), and then solve Ux = y for x with the back
substitution procedure (3.10.11).
â€¢
The advantage of this approach is that once the LU factors for
A have been computed, any other linear system Ax = Ëœb can
be solved with only n2 multiplications/divisions and n2 âˆ’n ad-
ditions/subtractions.

3.10 The LU Factorization
147
Example 3.10.2
Problem 1: Use the LU factorization of A to solve Ax = b, where
A =
ï£«
ï£­
2
2
2
4
7
7
6
18
22
ï£¶
ï£¸
and
b =
ï£«
ï£­
12
24
12
ï£¶
ï£¸.
Problem 2: Suppose that after solving the original system new information is
received that changes b to
Ëœb =
ï£«
ï£­
6
24
70
ï£¶
ï£¸.
Use the LU factors of A to solve the updated system Ax = Ëœb.
Solution 1: The LU factors of the coeï¬ƒcient matrix were determined in Example
3.10.1 to be
L =
ï£«
ï£­
1
0
0
2
1
0
3
4
1
ï£¶
ï£¸
and
U =
ï£«
ï£­
2
2
2
0
3
3
0
0
4
ï£¶
ï£¸.
The strategy is to set Ux = y and solve Ax = L(Ux) = b by solving the two
triangular systems
Ly = b
and
Ux = y.
First solve the lower-triangular system Ly = b by using forward substitution:
ï£«
ï£­
1
0
0
2
1
0
3
4
1
ï£¶
ï£¸
ï£«
ï£­
y1
y2
y3
ï£¶
ï£¸=
ï£«
ï£­
12
24
12
ï£¶
ï£¸
=â‡’
y1 = 12,
y2 = 24 âˆ’2y1 = 0,
y3 = 12 âˆ’3y1 âˆ’4y2 = âˆ’24.
Now use back substitution to solve the upper-triangular system Ux = y:
ï£«
ï£­
2
2
2
0
3
3
0
0
4
ï£¶
ï£¸
ï£«
ï£­
x1
x2
x3
ï£¶
ï£¸=
ï£«
ï£­
12
0
âˆ’24
ï£¶
ï£¸
=â‡’
x1 = (12 âˆ’2x2 âˆ’2x3)/2 = 6,
x2 = (0 âˆ’3x3)/3 = 6,
x3 = âˆ’24/4 = âˆ’6.
Solution 2: To solve the updated system Ax = Ëœb, simply repeat the forward
and backward substitution steps with b replaced by Ëœb. Solving Ly = Ëœb with
forward substitution gives the following:
ï£«
ï£­
1
0
0
2
1
0
3
4
1
ï£¶
ï£¸
ï£«
ï£­
y1
y2
y3
ï£¶
ï£¸=
ï£«
ï£­
6
24
70
ï£¶
ï£¸
=â‡’
y1 = 6,
y2 = 24 âˆ’2y1 = 12,
y3 = 70 âˆ’3y1 âˆ’4y2 = 4.
Using back substitution to solve Ux = y gives the following updated solution:
ï£«
ï£­
2
2
2
0
3
3
0
0
4
ï£¶
ï£¸
ï£«
ï£­
x1
x2
x3
ï£¶
ï£¸=
ï£«
ï£­
6
12
4
ï£¶
ï£¸
=â‡’
x1 = (6 âˆ’2x2 âˆ’2x3)/2 = âˆ’1,
x2 = (12 âˆ’3x3)/3 = 3,
x3 = 4/4 = 1.

148
Chapter 3
Matrix Algebra
Example 3.10.3
Computing Aâˆ’1.
Although matrix inversion is not used for solving Ax = b,
there are a few applications where explicit knowledge of Aâˆ’1 is desirable.
Problem: Explain how to use the LU factors of a nonsingular matrix AnÃ—n to
compute Aâˆ’1 eï¬ƒciently.
Solution: The strategy is to solve the matrix equation AX = I. Recall from
(3.5.5) that AAâˆ’1 = I implies A[Aâˆ’1]âˆ—j = ej, so the jth column of Aâˆ’1
is the solution of a system Axj = ej. Each of these n systems has the same
coeï¬ƒcient matrix, so, once the LU factors for A are known, each system Axj =
LUxj = ej can be solved by the standard two-step process.
(1)
Set yj = Uxj, and solve Lyj = ej for yj by forward substitution.
(2)
Solve Uxj = yj for xj = [Aâˆ’1]âˆ—j by back substitution.
This method has at least two advantages: itâ€™s eï¬ƒcient, and any code written to
solve Ax = b can also be used to compute Aâˆ’1.
Note: A tempting alternate solution might be to use the fact Aâˆ’1 = (LU)âˆ’1 =
Uâˆ’1Lâˆ’1. But computing Uâˆ’1 and Lâˆ’1 explicitly and then multiplying the
results is not as computationally eï¬ƒcient as the method just described.
Not all nonsingular matrices possess an LU factorization. For example, there
is clearly no nonzero value of u11 that will satisfy

0
1
1
1

=

1
0
â„“21
1
 
u11
u12
0
u22

.
The problem here is the zero pivot in the (1,1)-position. Our development of
the LU factorization using elementary lower-triangular matrices shows that if no
zero pivots emerge, then no row interchanges are necessary, and the LU factor-
ization can indeed be carried to completion. The converse is also true (its proof
is left as an exercise), so we can say that a nonsingular matrix A has an LU
factorization if and only if a zero pivot does not emerge during row reduction to
upper-triangular form with Type III operations.
Although it is a bit more theoretical, there is another interesting way to
characterize the existence of LU factors. This characterization is given in terms
of the leading principal submatrices of A that are deï¬ned to be those
submatrices taken from the upper-left-hand corner of A. That is,
A1 =

a11

, A2 =

a11
a12
a21
a22

, . . . , Ak =
ï£«
ï£¬
ï£¬
ï£­
a11
a12
Â· Â· Â·
a1k
a21
a22
Â· Â· Â·
a2k
...
...
...
...
ak1
ak2
Â· Â· Â·
akk
ï£¶
ï£·
ï£·
ï£¸, . . . .

3.10 The LU Factorization
149
Existence of LU Factors
Each of the following statements is equivalent to saying that a nonsin-
gular matrix AnÃ—n possesses an LU factorization.
â€¢
A zero pivot does not emerge during row reduction to upper-
triangular form with Type III operations.
â€¢
Each leading principal submatrix Ak is nonsingular.
(3.10.12)
Proof.
We will prove the statement concerning the leading principal submatri-
ces and leave the proof concerning the nonzero pivots as an exercise. Assume
ï¬rst that A possesses an LU factorization and partition A as
A = LU =

L11
0
L21
L22
 
U11
U12
0
U22

=

L11U11
âˆ—
âˆ—
âˆ—

,
where L11 and U11 are each k Ã— k. Thus Ak = L11U11 must be nonsingular
because L11 and U11 are each nonsingularâ€”they are triangular with nonzero
diagonal entries. Conversely, suppose that each leading principal submatrix in
A is nonsingular. Use induction to prove that each Ak possesses an LU fac-
torization. For k = 1, this statement is clearly true because if A1 = (a11) is
nonsingular, then A1 = (1)(a11) is its LU factorization. Now assume that Ak
has an LU factorization and show that this together with the nonsingularity
condition implies Ak+1 must also possess an LU factorization. If Ak = LkUk
is the LU factorization for Ak, then Aâˆ’1
k
= Uâˆ’1
k Lâˆ’1
k
so that
Ak+1 =

Ak
b
cT
Î±k+1

=

Lk
0
cT Uâˆ’1
k
1
 
Uk
Lâˆ’1
k b
0
Î±k+1 âˆ’cT Aâˆ’1
k b

, (3.10.13)
where cT and b contain the ï¬rst k components of Ak+1âˆ—and Aâˆ—k+1, re-
spectively. Observe that this is the LU factorization for Ak+1 because
Lk+1 =

Lk
0
cT Uâˆ’1
k
1

and
Uk+1 =

Uk
Lâˆ’1
k b
0
Î±k+1 âˆ’cT Aâˆ’1
k b

are lower- and upper-triangular matrices, respectively, and L has 1â€™s on its
diagonal while the diagonal entries of U are nonzero. The fact that
Î±k+1 âˆ’cT Aâˆ’1
k b Ì¸= 0
follows because Ak+1 and Lk+1 are each nonsingular, so Uk+1 = Lâˆ’1
k+1Ak+1
must also be nonsingular. Therefore, the nonsingularity of the leading principal

150
Chapter 3
Matrix Algebra
submatrices implies that each Ak possesses an LU factorization, and hence
An = A must have an LU factorization.
Up to this point we have avoided dealing with row interchanges because if
a row interchange is needed to remove a zero pivot, then no LU factorization is
possible. However, we know from the discussion in Â§1.5 that practical computa-
tion necessitates row interchanges in the form of partial pivoting. So even if no
zero pivots emerge, it is usually the case that we must still somehow account for
row interchanges.
To understand the eï¬€ects of row interchanges in the framework of an LU
decomposition, let Tk = I âˆ’ckeT
k be an elementary lower-triangular matrix as
described in (3.10.2), and let E = I âˆ’uuT with u = ek+i âˆ’ek+j be the Type I
elementary interchange matrix associated with an interchange of rows k +i and
k + j. Notice that eT
k E = eT
k because eT
k has 0â€™s in positions k + i and k + j.
This together with the fact that E2 = I guarantees
ETkE = E2 âˆ’EckeT
k E = I âˆ’ËœckeT
k ,
where
Ëœck = Eck.
In other words, the matrix
ËœTk = ETkE = I âˆ’ËœckeT
k
(3.10.14)
is also an elementary lower-triangular matrix, and ËœTk agrees with Tk in all
positions except that the multipliers Âµk+i and Âµk+j have traded places. As be-
fore, assume we are row reducing an n Ã— n nonsingular matrix A, but suppose
that an interchange of rows k + i and k + j is necessary immediately after the
kth stage so that the sequence of left-hand multiplications ETkTkâˆ’1 Â· Â· Â· T1 is
applied to A. Since E2 = I, we may insert E2 to the right of each T to obtain
ETkTkâˆ’1 Â· Â· Â· T1 = ETkE2Tkâˆ’1E2 Â· Â· Â· E2T1E2
= (ETkE) (ETkâˆ’1E) Â· Â· Â· (ET1E) E
= ËœTk ËœTkâˆ’1 Â· Â· Â· ËœT1E.
In such a manner, the necessary interchange matrices E can be â€œfactoredâ€ to
the far-right-hand side, and the matrices ËœT retain the desirable feature of be-
ing elementary lower-triangular matrices. Furthermore, (3.10.14) implies that
ËœTk ËœTkâˆ’1 Â· Â· Â· ËœT1 diï¬€ers from TkTkâˆ’1 Â· Â· Â· T1 only in the sense that the multipli-
ers in rows k + i and k + j have traded places. Therefore, row interchanges in
Gaussian elimination can be accounted for by writing ËœTnâˆ’1 Â· Â· Â· ËœT2 ËœT1PA = U,
where P is the product of all elementary interchange matrices used during the
reduction and where the ËœTk â€™s are elementary lower-triangular matrices in which
the multipliers have been permuted according to the row interchanges that were
implemented. Since all of the ËœTk â€™s are elementary lower-triangular matrices, we
may proceed along the same lines discussed in (3.10.4)â€”(3.10.6) to obtain
PA = LU,
where
L = ËœTâˆ’1
1
ËœTâˆ’1
2
Â· Â· Â· ËœTâˆ’1
nâˆ’1.
(3.10.15)

3.10 The LU Factorization
151
When row interchanges are allowed, zero pivots can always be avoided when the
original matrix A is nonsingular. Consequently, we may conclude that for every
nonsingular matrix A, there exists a permutation matrix P (a product of
elementary interchange matrices) such that PA has an LU factorization. Fur-
thermore, because of the observation in (3.10.14) concerning how the multipliers
in Tk and ËœTk trade places when a row interchange occurs, and because
ËœTâˆ’1
k
=

I âˆ’ËœckeT
k
	âˆ’1 = I + ËœckeT
k ,
it is not diï¬ƒcult to see that the same line of reasoning used to arrive at (3.10.6)
can be applied to conclude that the multipliers in the matrix L in (3.10.15) are
permuted according to the row interchanges that are executed. More speciï¬cally,
if rows k and k+i are interchanged to create the kth pivot, then the multipliers
( â„“k1
â„“k2
Â· Â· Â·
â„“k,kâˆ’1 )
and
( â„“k+i,1
â„“k+i,2
Â· Â· Â·
â„“k+i,kâˆ’1 )
trade places in the formation of L.
This means that we can proceed just as in the case when no interchanges are
used and successively overwrite the array originally containing A with each mul-
tiplier replacing the position it annihilates. Whenever a row interchange occurs,
the corresponding multipliers will be correctly interchanged as well. The per-
mutation matrix P is simply the cumulative record of the various interchanges
used, and the information in P is easily accounted for by a simple technique
that is illustrated in the following example.
Example 3.10.4
Problem: Use partial pivoting on the matrix
A =
ï£«
ï£¬
ï£­
1
2
âˆ’3
4
4
8
12
âˆ’8
2
3
2
1
âˆ’3
âˆ’1
1
âˆ’4
ï£¶
ï£·
ï£¸
and determine the LU decomposition PA = LU, where P is the associated
permutation matrix.
Solution: As explained earlier, the strategy is to successively overwrite the array
A with components from L and U. For the sake of clarity, the multipliers â„“ij
are shown in boldface type. Adjoin a â€œpermutation counter columnâ€ p that
is initially set to the natural order 1,2,3,4. Permuting components of p as the
various row interchanges are executed will accumulate the desired permutation.
The matrix P is obtained by executing the ï¬nal permutation residing in p to
the rows of an appropriate size identity matrix:
[A|p] =
ï£«
ï£¬
ï£­
1
2
âˆ’3
4
1
4
8
12
âˆ’8
2
2
3
2
1
3
âˆ’3
âˆ’1
1
âˆ’4
4
ï£¶
ï£·
ï£¸âˆ’â†’
ï£«
ï£¬
ï£­
4
8
12
âˆ’8
2
1
2
âˆ’3
4
1
2
3
2
1
3
âˆ’3
âˆ’1
1
âˆ’4
4
ï£¶
ï£·
ï£¸

152
Chapter 3
Matrix Algebra
âˆ’â†’
ï£«
ï£¬
ï£­
4
8
12
âˆ’8
2
1/4
0
âˆ’6
6
1
1/2
âˆ’1
âˆ’4
5
3
âˆ’3/4
5
10
âˆ’10
4
ï£¶
ï£·
ï£¸âˆ’â†’
ï£«
ï£¬
ï£­
4
8
12
âˆ’8
2
âˆ’3/4
5
10
âˆ’10
4
1/2
âˆ’1
âˆ’4
5
3
1/4
0
âˆ’6
6
1
ï£¶
ï£·
ï£¸
âˆ’â†’
ï£«
ï£¬
ï£­
4
8
12
âˆ’8
2
âˆ’3/4
5
10
âˆ’10
4
1/2
âˆ’1/5
âˆ’2
3
3
1/4
0
âˆ’6
6
1
ï£¶
ï£·
ï£¸âˆ’â†’
ï£«
ï£¬
ï£­
4
8
12
âˆ’8
2
âˆ’3/4
5
10
âˆ’10
4
1/4
0
âˆ’6
6
1
1/2
âˆ’1/5
âˆ’2
3
3
ï£¶
ï£·
ï£¸
âˆ’â†’
ï£«
ï£¬
ï£­
4
8
12
âˆ’8
2
âˆ’3/4
5
10
âˆ’10
4
1/4
0
âˆ’6
6
1
1/2
âˆ’1/5
1/3
1
3
ï£¶
ï£·
ï£¸.
Therefore,
L=
ï£«
ï£¬
ï£­
1
0
0
0
âˆ’3/4
1
0
0
1/4
0
1
0
1/2
âˆ’1/5
1/3
1
ï£¶
ï£·
ï£¸, U=
ï£«
ï£¬
ï£­
4
8
12
âˆ’8
0
5
10
âˆ’10
0
0
âˆ’6
6
0
0
0
1
ï£¶
ï£·
ï£¸, P=
ï£«
ï£¬
ï£­
0
1
0
0
0
0
0
1
1
0
0
0
0
0
1
0
ï£¶
ï£·
ï£¸.
It is easy to combine the advantages of partial pivoting with the LU decom-
position in order to solve a nonsingular system Ax = b. Because permutation
matrices are nonsingular, the system Ax = b is equivalent to
PAx = Pb,
and hence we can employ the LU solution techniques discussed earlier to solve
this permuted system. That is, if we have already performed the factorization
PA = LU â€”as illustrated in Example 3.10.4â€”then we can solve Ly = Pb for
y by forward substitution, and then solve Ux = y by back substitution.
It should be evident that the permutation matrix P is not really needed.
All that is necessary is knowledge of the LU factors along with the ï¬nal permu-
tation contained in the permutation counter column p illustrated in Example
3.10.4. The column Ëœb = Pb is simply a rearrangement of the components of
b according to the ï¬nal permutation shown in p. In other words, the strategy
is to ï¬rst permute b into Ëœb according to the permutation p, and then solve
Ly = Ëœb followed by Ux = y.
Example 3.10.5
Problem:
Use the LU decomposition obtained with partial pivoting to solve
the system Ax = b, where
A =
ï£«
ï£¬
ï£­
1
2
âˆ’3
4
4
8
12
âˆ’8
2
3
2
1
âˆ’3
âˆ’1
1
âˆ’4
ï£¶
ï£·
ï£¸
and
b =
ï£«
ï£¬
ï£­
3
60
1
5
ï£¶
ï£·
ï£¸.

3.10 The LU Factorization
153
Solution:
The LU decomposition with partial pivoting was computed in Ex-
ample 3.10.4. Permute the components in b according to the permutation
p = ( 2
4
1
3 ) , and call the result Ëœb. Now solve Ly = Ëœb by applying
forward substitution:
ï£«
ï£¬
ï£­
1
0
0
0
âˆ’3/4
1
0
0
1/4
0
1
0
1/2
âˆ’1/5
1/3
1
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
y1
y2
y3
y4
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
60
5
3
1
ï£¶
ï£·
ï£¸
=â‡’
y =
ï£«
ï£¬
ï£­
y1
y2
y3
y4
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
60
50
âˆ’12
âˆ’15
ï£¶
ï£·
ï£¸.
Then solve Ux = y by applying back substitution:
ï£«
ï£¬
ï£­
4
8
12
âˆ’8
0
5
10
âˆ’10
0
0
âˆ’6
6
0
0
0
1
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
x1
x2
x3
x4
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
60
50
âˆ’12
âˆ’15
ï£¶
ï£·
ï£¸
=â‡’
x =
ï£«
ï£¬
ï£­
12
6
âˆ’13
âˆ’15
ï£¶
ï£·
ï£¸.
LU Factorization with Row Interchanges
â€¢
For each nonsingular matrix A, there exists a permutation matrix
P such that PA possesses an LU factorization PA = LU.
â€¢
To compute L, U, and P, successively overwrite the array origi-
nally containing A. Replace each entry being annihilated with the
multiplier used to execute the annihilation. Whenever row inter-
changes such as those used in partial pivoting are implemented, the
multipliers in the array will automatically be interchanged in the
correct manner.
â€¢
Although the entire permutation matrix P is rarely called for, it
can be constructed by permuting the rows of the identity matrix
I according to the various interchanges used. These interchanges
can be accumulated in a â€œpermutation counter columnâ€ p that is
initially in natural order ( 1, 2, . . . , n )â€”see Example 3.10.4.
â€¢
To solve a nonsingular linear system Ax = b using the LU de-
composition with partial pivoting, permute the components in b to
construct Ëœb according to the sequence of interchanges usedâ€”i.e.,
according to p â€”and then solve Ly = Ëœb by forward substitution
followed by the solution of Ux = y using back substitution.

154
Chapter 3
Matrix Algebra
Example 3.10.6
The LDU factorization. Thereâ€™s some asymmetry in an LU factorization be-
cause the lower factor has 1â€™s on its diagonal while the upper factor has a nonunit
diagonal. This is easily remedied by factoring the diagonal entries out of the up-
per factor as shown below:
ï£«
ï£¬
ï£¬
ï£­
u11
u12
Â· Â· Â·
u1n
0
u22
Â· Â· Â·
u2n
...
...
...
...
0
0
Â· Â· Â·
unn
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
u11
0
Â· Â· Â·
0
0
u22
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
unn
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
1 u12/u11
Â· Â· Â· u1n/u11
0
1
Â· Â· Â· u2n/u22
...
...
...
...
0
0
Â· Â· Â·
1
ï£¶
ï£·
ï£·
ï£¸.
Setting D = diag (u11, u22, . . . , unn) (the diagonal matrix of pivots) and redeï¬n-
ing U to be the rightmost upper-triangular matrix shown above allows any LU
factorization to be written as A = LDU, where L and U are lower- and upper-
triangular matrices with 1â€™s on both of their diagonals. This is called the LDU
factorization of A. It is uniquely determined, and when A is symmetric, the
LDU factorization is A = LDLT (Exercise 3.10.9).
Example 3.10.7
The Cholesky Factorization.
22 A symmetric matrix A possessing an LU fac-
torization in which each pivot is positive is said to be positive deï¬nite.
Problem: Prove that A is positive deï¬nite if and only if A can be uniquely
factored as A = RT R, where R is an upper-triangular matrix with positive
diagonal entries. This is known as the Cholesky factorization of A, and R is
called the Cholesky factor of A.
Solution: If A is positive deï¬nite, then, as pointed out in Example 3.10.6,
it has an LDU factorization A = LDLT
in which D = diag (p1, p2, . . . , pn)
is the diagonal matrix containing the pivots pi > 0. Setting R = D1/2LT
where D1/2 = diag
âˆšp1, âˆšp2, . . . , âˆšpn
	
yields the desired factorization because
A = LD1/2D1/2LT = RT R, and R is upper triangular with positive diagonal
22
This is named in honor of the French military oï¬ƒcer Major AndrÂ´e-Louis Cholesky (1875â€“
1918). Although originally assigned to an artillery branch, Cholesky later became attached to
the Geodesic Section of the Geographic Service in France where he became noticed for his
extraordinary intelligence and his facility for mathematics. From 1905 to 1909 Cholesky was
involved with the problem of adjusting the triangularization grid for France. This was a huge
computational task, and there were arguments as to what computational techniques should be
employed. It was during this period that Cholesky invented the ingenious procedure for solving
a positive deï¬nite system of equations that is the basis for the matrix factorization that now
bears his name. Unfortunately, Choleskyâ€™s mathematical talents were never allowed to ï¬‚ower.
In 1914 war broke out, and Cholesky was again placed in an artillery groupâ€”but this time
as the commander. On August 31, 1918, Major Cholesky was killed in battle. Cholesky never
had time to publish his clever computational methodsâ€”they were carried forward by word-
of-mouth. Issues surrounding the Cholesky factorization have been independently rediscovered
several times by people who were unaware of Cholesky, and, in some circles, the Cholesky
factorization is known as the square root method.

3.10 The LU Factorization
155
entries. Conversely, if A = RRT , where R is lower triangular with a positive
diagonal, then factoring the diagonal entries out of R as illustrated in Example
3.10.6 produces R = LD, where L is lower triangular with a unit diagonal and
D is the diagonal matrix whose diagonal entries are the riiâ€™s. Consequently,
A = LD2LT
is the LDU factorization for A, and thus the pivots must be
positive because they are the diagonal entries in D2. We have now proven that
A is positive deï¬nite if and only if it has a Cholesky factorization. To see why
such a factorization is unique, suppose A = R1RT
1 = R2RT
2 , and factor out
the diagonal entries as illustrated in Example 3.10.6 to write R1 = L1D1 and
R2 = L2D2, where each Ri is lower triangular with a unit diagonal and Di
contains the diagonal of Ri so that A = L1D2
1LT
1 = L2D2
2LT
2 . The uniqueness
of the LDU factors insures that L1 = L2 and D1 = D2, so R1 = R2. Note:
More is said about the Cholesky factorization and positive deï¬nite matrices on
pp. 313, 345, and 559.
Exercises for section 3.10
3.10.1. Let A =
ï£«
ï£­
1
4
5
4
18
26
3
16
30
ï£¶
ï£¸.
(a)
Determine the LU factors of A.
(b)
Use the LU factors to solve Ax1 = b1 as well as Ax2 = b2,
where
b1 =
ï£«
ï£­
6
0
âˆ’6
ï£¶
ï£¸
and
b2 =
ï£«
ï£­
6
6
12
ï£¶
ï£¸.
(c)
Use the LU factors to determine Aâˆ’1.
3.10.2. Let A and b be the matrices
A =
ï£«
ï£¬
ï£­
1
2
4
17
3
6
âˆ’12
3
2
3
âˆ’3
2
0
2
âˆ’2
6
ï£¶
ï£·
ï£¸
and
b =
ï£«
ï£¬
ï£­
17
3
3
4
ï£¶
ï£·
ï£¸.
(a)
Explain why A does not have an LU factorization.
(b)
Use partial pivoting and ï¬nd the permutation matrix P as well
as the LU factors such that PA = LU.
(c)
Use the information in P, L, and U to solve Ax = b.
3.10.3. Determine all values of Î¾ for which A =
ï£«
ï£­
Î¾
2
0
1
Î¾
1
0
1
Î¾
ï£¶
ï£¸fails to have an
LU factorization.

156
Chapter 3
Matrix Algebra
3.10.4. If A is a nonsingular matrix that possesses an LU factorization, prove
that the pivot that emerges after (k + 1) stages of standard Gaussian
elimination using only Type III operations is given by
pk+1 = ak+1,k+1 âˆ’cT Aâˆ’1
k b,
where Ak and
Ak+1 =

Ak
b
cT
ak+1,k+1

are the leading principal submatrices of orders k and k + 1, respec-
tively. Use this to deduce that all pivots must be nonzero when an LU
factorization for A exists.
3.10.5. If A is a matrix that contains only integer entries and all of its pivots
are 1, explain why Aâˆ’1 must also be an integer matrix. Note: This fact
can be used to construct random integer matrices that possess integer
inverses by randomly generating integer matrices L and U with unit
diagonals and then constructing the product A = LU.
3.10.6. Consider the tridiagonal matrix T =
ï£«
ï£¬
ï£­
Î²1
Î³1
0
0
Î±1
Î²2
Î³2
0
0
Î±2
Î²3
Î³3
0
0
Î±3
Î²4
ï£¶
ï£·
ï£¸.
(a)
Assuming that T possesses an LU factorization, verify that it
is given by
L =
ï£«
ï£¬
ï£­
1
0
0
0
Î±1/Ï€1
1
0
0
0
Î±2/Ï€2
1
0
0
0
Î±3/Ï€3
1
ï£¶
ï£·
ï£¸, U =
ï£«
ï£¬
ï£­
Ï€1
Î³1
0
0
0
Ï€2
Î³2
0
0
0
Ï€3
Î³3
0
0
0
Ï€4
ï£¶
ï£·
ï£¸,
where the Ï€i â€™s are generated by the recursion formula
Ï€1 = Î²1
and
Ï€i+1 = Î²i+1 âˆ’Î±iÎ³i
Ï€i
.
Note:
This holds for tridiagonal matrices of arbitrary size
thereby making the LU factors of these matrices very easy to
compute.
(b)
Apply the recursion formula given above to obtain the LU fac-
torization of
T =
ï£«
ï£¬
ï£­
2
âˆ’1
0
0
âˆ’1
2
âˆ’1
0
0
âˆ’1
2
âˆ’1
0
0
âˆ’1
1
ï£¶
ï£·
ï£¸.

3.10 The LU Factorization
157
3.10.7. AnÃ—n is called a band matrix if aij = 0 whenever |i âˆ’j| > w for
some positive integer w, called the bandwidth. In other words, the
nonzero entries of A are constrained to be in a band of w diagonal lines
above and below the main diagonal. For example, tridiagonal matrices
have bandwidth one, and diagonal matrices have bandwidth zero. If
A is a nonsingular matrix with bandwidth w, and if A has an LU
factorization A = LU, then L inherits the lower band structure of A,
and U inherits the upper band structure in the sense that L has â€œlower
bandwidthâ€ w, and U has â€œupper bandwidthâ€ w. Illustrate why this
is true by using a generic 5 Ã— 5 matrix with a bandwidth of w = 2.
3.10.8.
(a)
Construct an example of a nonsingular symmetric matrix that
fails to possess an LU (or LDU) factorization.
(b)
Construct an example of a nonsingular symmetric matrix that
has an LU factorization but is not positive deï¬nite.
3.10.9.
(a)
Determine the LDU factors for A =
ï£«
ï£­
1
4
5
4
18
26
3
16
30
ï£¶
ï£¸(this is the
same matrix used in Exercise 3.10.1).
(b)
Prove that if a matrix has an LDU factorization, then the LDU
factors are uniquely determined.
(c)
If A is symmetric and possesses an LDU factorization, explain
why it must be given by A = LDLT .
3.10.10. Explain why A =
ï£«
ï£­
1
2
3
2
8
12
3
12
27
ï£¶
ï£¸is positive deï¬nite, and then ï¬nd the
Cholesky factor R.

CHAPTER 4
Vector
Spaces
4.1
SPACES AND SUBSPACES
After matrix theory became established toward the end of the nineteenth century,
it was realized that many mathematical entities that were considered to be quite
diï¬€erent from matrices were in fact quite similar. For example, objects such as
points in the plane â„œ2, points in 3-space â„œ3, polynomials, continuous functions,
and diï¬€erentiable functions (to name only a few) were recognized to satisfy the
same additive properties and scalar multiplication properties given in Â§3.2 for
matrices. Rather than studying each topic separately, it was reasoned that it
is more eï¬ƒcient and productive to study many topics at one time by studying
the common properties that they satisfy. This eventually led to the axiomatic
deï¬nition of a vector space.
A vector space involves four thingsâ€”two sets V and F, and two algebraic
operations called vector addition and scalar multiplication.
â€¢
V is a nonempty set of objects called vectors. Although V can be quite
general, we will usually consider V to be a set of n-tuples or a set of matrices.
â€¢
F is a scalar ï¬eldâ€”for us F is either the ï¬eld â„œof real numbers or the
ï¬eld C of complex numbers.
â€¢
Vector addition (denoted by x + y ) is an operation between elements of V.
â€¢
Scalar multiplication (denoted by Î±x ) is an operation between elements of
F and V.
The formal deï¬nition of a vector space stipulates how these four things relate
to each other. In essence, the requirements are that vector addition and scalar
multiplication must obey exactly the same properties given in Â§3.2 for matrices.

160
Chapter 4
Vector Spaces
Vector Space Deï¬nition
The set V is called a vector space over F when the vector addition
and scalar multiplication operations satisfy the following properties.
(A1)
x+y âˆˆV for all x, y âˆˆV. This is called the closure property
for vector addition.
(A2)
(x + y) + z = x + (y + z) for every x, y, z âˆˆV.
(A3)
x + y = y + x for every x, y âˆˆV.
(A4)
There is an element 0 âˆˆV such that x + 0 = x for every
x âˆˆV.
(A5)
For each x âˆˆV, there is an element (âˆ’x) âˆˆV such that
x + (âˆ’x) = 0.
(M1)
Î±x âˆˆV for all Î± âˆˆF and x âˆˆV. This is the closure
property for scalar multiplication.
(M2)
(Î±Î²)x = Î±(Î²x) for all Î±, Î² âˆˆF and every x âˆˆV.
(M3)
Î±(x + y) = Î±x + Î±y for every Î± âˆˆF and all x, y âˆˆV.
(M4)
(Î± + Î²)x = Î±x + Î²x for all Î±, Î² âˆˆF and every x âˆˆV.
(M5)
1x = x for every x âˆˆV.
A theoretical algebraic treatment of the subject would concentrate on the
logical consequences of these deï¬ning properties, but the objectives in this text
are diï¬€erent, so we will not dwell on the axiomatic development.
23 Neverthe-
23
The idea of deï¬ning a vector space by using a set of abstract axioms was contained in a general
theory published in 1844 by Hermann Grassmann (1808â€“1887), a theologian and philosopher
from Stettin, Poland, who was a self-taught mathematician. But Grassmannâ€™s work was origi-
nally ignored because he tried to construct a highly abstract self-contained theory, independent
of the rest of mathematics, containing nonstandard terminology and notation, and he had a
tendency to mix mathematics with obscure philosophy. Grassmann published a complete re-
vision of his work in 1862 but with no more success. Only later was it realized that he had
formulated the concepts we now refer to as linear dependence, bases, and dimension. The
Italian mathematician Giuseppe Peano (1858â€“1932) was one of the few people who noticed
Grassmannâ€™s work, and in 1888 Peano published a condensed interpretation of it. In a small
chapter at the end, Peano gave an axiomatic deï¬nition of a vector space similar to the one
above, but this drew little attention outside of a small group in Italy. The current deï¬nition is
derived from the 1918 work of the German mathematician Hermann Weyl (1885â€“1955). Even
though Weylâ€™s deï¬nition is closer to Peanoâ€™s than to Grassmannâ€™s, Weyl did not mention his
Italian predecessor, but he did acknowledge Grassmannâ€™s â€œepoch making work.â€ Weylâ€™s success
with the idea was due in part to the fact that he thought of vector spaces in terms of geometry,
whereas Grassmann and Peano treated them as abstract algebraic structures. As we will see,
itâ€™s the geometry thatâ€™s important.

4.1 Spaces and Subspaces
161
less, it is important to recognize some of the more signiï¬cant examples and to
understand why they are indeed vector spaces.
Example 4.1.1
Because (A1)â€“(A5) are generalized versions of the ï¬ve additive properties of
matrix addition, and (M1)â€“(M5) are generalizations of the ï¬ve scalar multipli-
cation properties given in Â§3.2, we can say that the following hold.
â€¢
The set â„œmÃ—n of m Ã— n real matrices is a vector space over â„œ.
â€¢
The set CmÃ—n of m Ã— n complex matrices is a vector space over C.
Example 4.1.2
The real coordinate spaces
â„œ1Ã—n = {( x1
x2
Â· Â· Â·
xn ) , xi âˆˆâ„œ}
and
â„œnÃ—1 =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
ï£«
ï£¬
ï£¬
ï£­
x1
x2
...
xn
ï£¶
ï£·
ï£·
ï£¸, xi âˆˆâ„œ
ï£¼
ï£´
ï£´
ï£½
ï£´
ï£´
ï£¾
are special cases of the preceding example, and these will be the object of most
of our attention. In the context of vector spaces, it usually makes no diï¬€erence
whether a coordinate vector is depicted as a row or as a column. When the row or
column distinction is irrelevant, or when it is clear from the context, we will use
the common symbol â„œn to designate a coordinate space. In those cases where
it is important to distinguish between rows and columns, we will explicitly write
â„œ1Ã—n or â„œnÃ—1. Similar remarks hold for complex coordinate spaces.
Although the coordinate spaces will be our primary concern, be aware that
there are many other types of mathematical structures that are vector spacesâ€”
this was the reason for making an abstract deï¬nition at the outset. Listed below
are a few examples.
Example 4.1.3
With function addition and scalar multiplication deï¬ned by
(f + g)(x) = f(x) + g(x)
and
(Î±f)(x) = Î±f(x),
the following sets are vector spaces over â„œ:
â€¢
The set of functions mapping the interval [0, 1] into â„œ.
â€¢
The set of all real-valued continuous functions deï¬ned on [0, 1].
â€¢
The set of real-valued functions that are diï¬€erentiable on [0, 1].
â€¢
The set of all polynomials with real coeï¬ƒcients.

162
Chapter 4
Vector Spaces
Example 4.1.4
Consider the vector space â„œ2, and let
L = {(x, y) | y = Î±x}
be a line through the origin. L is a subset of â„œ2, but L is a special kind
of subset because L also satisï¬es the properties (A1)â€“(A5) and (M1)â€“(M5)
that deï¬ne a vector space. This shows that it is possible for one vector space to
properly contain other vector spaces.
Subspaces
Let S be a nonempty subset of a vector space V over F (symbolically,
S âŠ†V). If S is also a vector space over F using the same addition
and scalar multiplication operations, then S is said to be a subspace of
V. Itâ€™s not necessary to check all 10 of the deï¬ning conditions in order
to determine if a subset is also a subspaceâ€”only the closure conditions
(A1) and (M1) need to be considered. That is, a nonempty subset S
of a vector space V is a subspace of V if and only if
(A1)
x, y âˆˆS
=â‡’
x + y âˆˆS
and
(M1)
x âˆˆS
=â‡’
Î±x âˆˆS for all Î± âˆˆF.
Proof.
If S is a subset of V, then S automatically inherits all of the vector
space properties of V except (A1), (A4), (A5), and (M1). However, (A1)
together with (M1) implies (A4) and (A5). To prove this, observe that (M1)
implies (âˆ’x) = (âˆ’1)x âˆˆS for all x âˆˆS so that (A5) holds. Since x and (âˆ’x)
are now both in S, (A1) insures that x + (âˆ’x) âˆˆS, and thus 0 âˆˆS.
Example 4.1.5
Given a vector space V, the set Z = {0} containing only the zero vector is
a subspace of V because (A1) and (M1) are trivially satisï¬ed. Naturally, this
subspace is called the trivial subspace.
Vector addition in â„œ2 and â„œ3 is easily visualized by using the parallelo-
gram law, which states that for two vectors u and v, the sum u + v is the
vector deï¬ned by the diagonal of the parallelogram as shown in Figure 4.1.1.

4.1 Spaces and Subspaces
163
u = (u1,u2)
v = (v1,v2)
= (u
u+v
1+v1, u2+v2)
Figure 4.1.1
We have already observed that straight lines through the origin in â„œ2 are
subspaces, but what about straight lines not through the origin? Noâ€”they can-
not be subspaces because subspaces must contain the zero vector (i.e., they must
pass through the origin). What about curved lines through the originâ€”can some
of them be subspaces of â„œ2? Again the answer is â€œNo!â€ As depicted in Figure
4.1.2, the parallelogram law indicates why the closure property (A1) cannot be
satisï¬ed for lines with a curvature because there are points u and v on the
curve for which u + v (the diagonal of the corresponding parallelogram) is not
on the curve. Consequently, the only proper subspaces of â„œ2 are the trivial
subspace and lines through the origin.
u
v
u+v
Figure 4.1.2
u
v
u+v
Î±u
P
Figure 4.1.3
In â„œ3, the trivial subspace and lines through the origin are again subspaces,
but there is also another oneâ€”planes through the origin. If P is a plane through
the origin in â„œ3, then, as shown in Figure 4.1.3, the parallelogram law guarantees
that the closure property for addition (A1) holdsâ€”the parallelogram deï¬ned by

164
Chapter 4
Vector Spaces
any two vectors in P is also in P so that if u, v âˆˆP, then u + v âˆˆP. The
closure property for scalar multiplication (M1) holds because multiplying any
vector by a scalar merely stretches it, but its angular orientation does not change
so that if u âˆˆP, then Î±u âˆˆP for all scalars Î±. Lines and surfaces in â„œ3 that
have curvature cannot be subspaces for essentially the same reason depicted in
Figure 4.1.2. So the only proper subspaces of â„œ3 are the trivial subspace, lines
through the origin, and planes through the origin.
The concept of a subspace now has an obvious interpretation in the visual
spaces â„œ2 and â„œ3 â€”subspaces are the ï¬‚at surfaces passing through the origin.
Flatness
Although we canâ€™t use our eyes to see â€œï¬‚atnessâ€ in higher dimensions,
our minds can conceive it through the notion of a subspace. From now on,
think of ï¬‚at surfaces passing through the origin whenever you encounter
the term â€œsubspace.â€
For a set of vectors S = {v1, v2, . . . , vr} from a vector space V, the set of
all possible linear combinations of the vi â€™s is denoted by
span (S) = {Î±1v1 + Î±2v2 + Â· Â· Â· + Î±rvr | Î±i âˆˆF} .
Notice that span (S) is a subspace of V because the two closure properties
(A1) and (M1) are satisï¬ed. That is, if x = 
i Î¾ivi and y = 
i Î·ivi are two
linear combinations from span (S) , then the sum x + y = 
i(Î¾i + Î·i)vi is also
a linear combination in span (S) , and for any scalar Î²,
Î²x = 
i(Î²Î¾i)vi is
also a linear combination in span (S) .
u
v
Î±u
Î²v
Î±u + Î²v
Figure 4.1.4

4.1 Spaces and Subspaces
165
For example, if u Ì¸= 0 is a vector in â„œ3, then span {u} is the straight
line passing through the origin and u. If S = {u, v}, where u and v are two
nonzero vectors in â„œ3 not lying on the same line, then, as shown in Figure 4.1.4,
span (S) is the plane passing through the origin and the points u and v. As we
will soon see, all subspaces of â„œn are of the type span (S), so it is worthwhile
to introduce the following terminology.
Spanning Sets
â€¢
For a set of vectors S = {v1, v2, . . . , vr} , the subspace
span (S) = {Î±1v1 + Î±2v2 + Â· Â· Â· + Î±rvr}
generated by forming all linear combinations of vectors from S is
called the space spanned by S.
â€¢
If V is a vector space such that V = span (S) , we say S is a
spanning set for V. In other words, S spans V whenever each
vector in V is a linear combination of vectors from S.
Example 4.1.6
(i)
In Figure 4.1.4, S = {u, v} is a spanning set for the indicated plane.
(ii)
S =

1
1

,

2
2

spans the line y = x in â„œ2.
(iii)
The unit vectors
ï£±
ï£²
ï£³e1 =
ï£«
ï£­
1
0
0
ï£¶
ï£¸, e2 =
ï£«
ï£­
0
1
0
ï£¶
ï£¸, e3 =
ï£«
ï£­
0
0
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾span â„œ3.
(iv)
The unit vectors {e1, e2, . . . , en} in â„œn form a spanning set for â„œn.
(v)
The ï¬nite set

1, x, x2, . . . , xn
spans the space of all polynomials such
that deg p(x) â‰¤n, and the inï¬nite set

1, x, x2, . . .

spans the space of all
polynomials.
Example 4.1.7
Problem: For a set of vectors S = {a1, a2, . . . , an} from a subspace V âŠ†â„œmÃ—1,
let A be the matrix containing the ai â€™s as its columns. Explain why S spans V
if and only if for each b âˆˆV there corresponds a column x such that Ax = b
(i.e., if and only if Ax = b is a consistent system for every b âˆˆV).

166
Chapter 4
Vector Spaces
Solution: By deï¬nition, S spans V if and only if for each b âˆˆV there exist
scalars Î±i such that
b = Î±1a1 + Î±2a2 + Â· Â· Â· + Î±nan =

a1 | a2 | Â· Â· Â· | an

ï£«
ï£¬
ï£¬
ï£­
Î±1
Î±2
...
Î±n
ï£¶
ï£·
ï£·
ï£¸= Ax.
Note: This simple observation often is quite helpful. For example, to test whether
or not S = {( 1
1
1 ) , ( 1
âˆ’1
âˆ’1 ) , ( 3
1
1 )} spans â„œ3, place these
rows as columns in a matrix A, and ask, â€œIs the system
ï£«
ï£­
1
1
3
1
âˆ’1
1
1
âˆ’1
1
ï£¶
ï£¸
ï£«
ï£­
x1
x2
x3
ï£¶
ï£¸=
ï£«
ï£­
b1
b2
b3
ï£¶
ï£¸
consistent for every b âˆˆâ„œ3?â€ Recall from (2.3.4) that Ax = b is consis-
tent if and only if rank[A|b] = rank (A). In this case, rank (A) = 2, but
rank[A|b] = 3 for some b â€™s (e.g., b1 = 0, b2 = 1, b3 = 0), so S doesnâ€™t span
â„œ3. On the other hand, Sâ€² = {( 1
1
1 ) , ( 1
âˆ’1
âˆ’1 ) , ( 3
1
2 )} is a
spanning set for â„œ3 because
A =
ï£«
ï£­
1
1
3
1
âˆ’1
1
1
âˆ’1
2
ï£¶
ï£¸
is nonsingular, so Ax = b is consistent for all b (the solution is x = Aâˆ’1b ).
As shown below, itâ€™s possible to â€œaddâ€ two subspaces to generate another.
Sum of Subspaces
If X and Y are subspaces of a vector space V, then the sum of X
and Y is deï¬ned to be the set of all possible sums of vectors from X
with vectors from Y. That is,
X + Y = {x + y | x âˆˆX and y âˆˆY}.
â€¢
The sum X + Y is again a subspace of V.
(4.1.1)
â€¢
If SX, SY span X, Y, then SX âˆªSY spans X + Y.
(4.1.2)

4.1 Spaces and Subspaces
167
Proof.
To prove (4.1.1), demonstrate that the two closure properties (A1) and
(M1) hold for S = X +Y. To show (A1) is valid, observe that if u, v âˆˆS, then
u = x1 + y1 and v = x2 + y2, where x1, x2 âˆˆX and y1, y2 âˆˆY. Because
X
and Y are closed with respect to addition, it follows that x1 + x2 âˆˆX
and y1 + y2 âˆˆY, and therefore u + v = (x1 + x2) + (y1 + y2) âˆˆS. To
verify (M1), observe that X
and Y are both closed with respect to scalar
multiplication so that Î±x1 âˆˆX
and Î±y1 âˆˆY for all Î±, and consequently
Î±u = Î±x1 + Î±y1 âˆˆS for all Î±. To prove (4.1.2), suppose SX = {x1, x2, . . . , xr}
and SY = {y1, y2, . . . , yt} , and write
z âˆˆspan (SX âˆªSY ) â‡â‡’z =
r

i=1
Î±ixi +
t

i=1
Î²iyi = x + y with x âˆˆX, y âˆˆY
â‡â‡’z âˆˆX + Y.
Example 4.1.8
If X âŠ†â„œ2 and Y âŠ†â„œ2 are subspaces deï¬ned by two diï¬€erent lines through
the origin, then X + Y = â„œ2. This follows from the parallelogram lawâ€”sketch
a picture for yourself.
Exercises for section 4.1
4.1.1. Determine which of the following subsets of â„œn are in fact subspaces of
â„œn
(n > 2).
(a)
{x | xi â‰¥0},
(b)
{x | x1 = 0},
(c)
{x | x1x2 = 0},
(d)

x

n
j=1
xj = 0

,
(e)

x

n
j=1
xj = 1

,
(f)
{x | Ax = b, where AmÃ—n Ì¸= 0 and bmÃ—1 Ì¸= 0} .
4.1.2. Determine which of the following subsets of â„œnÃ—n are in fact subspaces
of â„œnÃ—n.
(a)
The symmetric matrices.
(b) The diagonal matrices.
(c)
The nonsingular matrices. (d) The singular matrices.
(e)
The triangular matrices.
(f) The upper-triangular matrices.
(g)
All matrices that commute with a given matrix A.
(h)
All matrices such that A2 = A.
(i)
All matrices such that trace (A) = 0.
4.1.3. If X is a plane passing through the origin in â„œ3 and Y is the line
through the origin that is perpendicular to X, what is X + Y ?

168
Chapter 4
Vector Spaces
4.1.4. Why must a real or complex nonzero vector space contain an inï¬nite
number of vectors?
4.1.5. Sketch a picture in â„œ3 of the subspace spanned by each of the following.
(a)
ï£±
ï£²
ï£³
ï£«
ï£­
1
3
2
ï£¶
ï£¸,
ï£«
ï£­
2
6
4
ï£¶
ï£¸,
ï£«
ï£­
âˆ’3
âˆ’9
âˆ’6
ï£¶
ï£¸
ï£¼
ï£½
ï£¾, (b)
ï£±
ï£²
ï£³
ï£«
ï£­
âˆ’4
0
0
ï£¶
ï£¸,
ï£«
ï£­
0
5
0
ï£¶
ï£¸,
ï£«
ï£­
1
1
0
ï£¶
ï£¸
ï£¼
ï£½
ï£¾,
(c)
ï£±
ï£²
ï£³
ï£«
ï£­
1
0
0
ï£¶
ï£¸,
ï£«
ï£­
1
1
0
ï£¶
ï£¸,
ï£«
ï£­
1
1
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾.
4.1.6. Which of the following are spanning sets for â„œ3 ?
(a)
{( 1
1
1 )}
(b)
{( 1
0
0 ) , ( 0
0
1 )},
(c)
{( 1
0
0 ) , ( 0
1
0 ) , ( 0
0
1 ) , ( 1
1
1 )},
(d)
{( 1
2
1 ) , ( 2
0
âˆ’1 ) , ( 4
4
1 )},
(e)
{( 1
2
1 ) , ( 2
0
âˆ’1 ) , ( 4
4
0 )}.
4.1.7. For a vector space V, and for M, N âŠ†V, explain why
span (M âˆªN) = span (M) + span (N) .
4.1.8. Let X and Y be two subspaces of a vector space V.
(a)
Prove that the intersection X âˆ©Y is also a subspace of V.
(b)
Show that the union X âˆªY need not be a subspace of V.
4.1.9. For A âˆˆâ„œmÃ—n and S âŠ†â„œnÃ—1, the set A(S) = {Ax | x âˆˆS} contains
all possible products of A with vectors from S. We refer to A(S) as
the set of images of S under A.
(a)
If S is a subspace of â„œn, prove A(S) is a subspace of â„œm.
(b)
If s1, s2, . . . , sk spans S, show As1, As2, . . . , Ask spans A(S).
4.1.10. With the usual addition and multiplication, determine whether or not
the following sets are vector spaces over the real numbers.
(a)
â„œ,
(b)
C,
(c)
The rational numbers.
4.1.11. Let M = {m1, m2, . . . , mr} and N = {m1, m2, . . . , mr, v} be two sets
of vectors from the same vector space. Prove that span (M) = span (N)
if and only if v âˆˆspan (M) .
4.1.12. For a set of vectors S = {v1, v2, . . . , vn} , prove that span (S) is the
intersection of all subspaces that contain S. Hint: For M =

SâŠ†V V,
prove that span (S) âŠ†M and M âŠ†span (S) .

4.2 Four Fundamental Subspaces
169
4.2
FOUR FUNDAMENTAL SUBSPACES
The closure properties (A1) and (M1) on p. 162 that characterize the notion
of a subspace have much the same â€œfeelâ€ as the deï¬nition of a linear function as
stated on p. 89, but thereâ€™s more to it than just a â€œsimilar feel.â€ Subspaces are
intimately related to linear functions as explained below.
Subspaces and Linear Functions
For a linear function f mapping â„œn into â„œm, let R(f) denote the
range of f. That is, R(f) = {f(x) | x âˆˆâ„œn} âŠ†â„œm is the set of all
â€œimagesâ€ as x varies freely over â„œn.
â€¢
The range of every linear function f : â„œn â†’â„œm is a subspace of
â„œm, and every subspace of â„œm is the range of some linear function.
For this reason, subspaces of â„œm are sometimes called linear spaces.
Proof.
If f : â„œn â†’â„œm is a linear function, then the range of f is a subspace
of â„œm because the closure properties (A1) and (M1) are satisï¬ed. Establish
(A1) by showing that y1, y2 âˆˆR(f) â‡’y1 +y2 âˆˆR(f). If y1, y2 âˆˆR(f), then
there must be vectors x1, x2 âˆˆâ„œn such that y1 = f(x1) and y2 = f(x2), so
it follows from the linearity of f that
y1 + y2 = f(x1) + f(x2) = f(x1 + x2) âˆˆR(f).
Similarly, establish (M1) by showing that if y âˆˆR(f), then Î±y âˆˆR(f) for all
scalars Î± by using the deï¬nition of range along with the linearity of f to write
y âˆˆR(f) =â‡’y = f(x) for some x âˆˆâ„œn =â‡’Î±y = Î±f(x) = f(Î±x) âˆˆR(f).
Now prove that every subspace V of â„œm is the range of some linear function
f : â„œn â†’â„œm. Suppose that {v1, v2, . . . , vn} is a spanning set for V so that
V = {Î±1v1 + Â· Â· Â· + Î±nvn | Î±i âˆˆR}.
(4.2.1)
Stack the vi â€™s as columns in a matrix AmÃ—n =

v1 | v2 | Â· Â· Â· | vn

, and put the
Î±i â€™s in an n Ã— 1 column x = (Î±1, Î±2, . . . , Î±n)T to write
Î±1v1 + Â· Â· Â· + Î±nvn =

v1 | v2 | Â· Â· Â· | vn

ï£«
ï£­
Î±1
...
Î±n
ï£¶
ï£¸= Ax.
(4.2.2)
The function f(x) = Ax is linear (recall Example 3.6.1, p. 106), and we have
that R(f) = {Ax | x âˆˆâ„œnÃ—1} = {Î±1v1 + Â· Â· Â· + Î±nvn | Î±i âˆˆR} = V.

170
Chapter 4
Vector Spaces
In particular, this result means that every matrix A âˆˆâ„œmÃ—n generates
a subspace of â„œm by means of the range of the linear function f(x) = Ax.
Likewise, the transpose
24 of A âˆˆâ„œmÃ—n deï¬nes a subspace of â„œn by means
of the range of f(y) = AT y. These two â€œrange spacesâ€ are two of the four
fundamental subspaces deï¬ned by a matrix.
Range Spaces
The range of a matrix A âˆˆâ„œmÃ—n is deï¬ned to be the subspace
R (A) of â„œm that is generated by the range of f(x) = Ax. That is,
R (A) = {Ax | x âˆˆâ„œn} âŠ†â„œm.
Similarly, the range of AT is the subspace of â„œn deï¬ned by
R

AT 
= {AT y | y âˆˆâ„œm} âŠ†â„œn.
Because R (A) is the set of all â€œimagesâ€ of vectors x âˆˆâ„œm under
transformation by A, some people call R (A) the image space of A.
The observation (4.2.2) that every matrixâ€“vector product Ax (i.e., every
image) is a linear combination of the columns of A provides a useful character-
ization of the range spaces. Allowing the components of x = (Î¾1, Î¾2, . . . , Î¾n)T to
vary freely and writing
Ax =

Aâˆ—1 | Aâˆ—2 | Â· Â· Â· | Aâˆ—n

ï£«
ï£¬
ï£¬
ï£­
Î¾1
Î¾2
...
Î¾n
ï£¶
ï£·
ï£·
ï£¸=
n

j=1
Î¾jAâˆ—j
shows that the set of all images Ax is the same as the set of all linear combi-
nations of the columns of A. Therefore, R (A) is nothing more than the space
spanned by the columns of A. Thatâ€™s why R (A) is often called the column
space of A.
Likewise, R

AT 
is the space spanned by the columns of AT . But the
columns of AT are just the rows of A (stacked upright), so R

AT 
is simply
the space spanned by the rows
25 of A. Consequently, R

AT 
is also known as
the row space of A. Below is a summary.
24
For ease of exposition, the discussion in this section is in terms of real matrices and real spaces,
but all results have complex analogs obtained by replacing AT by Aâˆ—.
25
Strictly speaking, the range of AT
is a set of columns, while the row space of A is a set of
rows. However, no logical diï¬ƒculties are encountered by considering them to be the same.

4.2 Four Fundamental Subspaces
171
Column and Row Spaces
For A âˆˆâ„œmÃ—n, the following statements are true.
â€¢
R (A) = the space spanned by the columns of A (column space).
â€¢
R

AT 
= the space spanned by the rows of A (row space).
â€¢
b âˆˆR (A) â‡â‡’b = Ax for some x.
(4.2.3)
â€¢
a âˆˆR

AT 
â‡â‡’aT = yT A for some yT .
(4.2.4)
Example 4.2.1
Problem: Describe R (A) and R

AT 
for A =
 1
2
3
2
4
6

.
Solution: R (A) = span {Aâˆ—1, Aâˆ—2, Aâˆ—3} = {Î±1Aâˆ—1+Î±2Aâˆ—2+Î±3Aâˆ—3 | Î±i âˆˆâ„œ},
but since Aâˆ—2 = 2Aâˆ—1 and Aâˆ—3 = 3Aâˆ—1, itâ€™s clear that every linear combination
of Aâˆ—1, Aâˆ—2, and Aâˆ—3 reduces to a multiple of Aâˆ—1, so R (A) = span {Aâˆ—1} .
Geometrically, R (A) is the line in â„œ2 through the origin and the point (1, 2).
Similarly, R

AT 
= span {A1âˆ—, A2âˆ—} = {Î±1A1âˆ—+ Î±2A2âˆ—| Î±1, Î±2 âˆˆâ„œ} . But
A2âˆ—= 2A1âˆ—implies that every combination of A1âˆ—and A2âˆ—reduces to a
multiple of A1âˆ—, so R

AT 
= span {A1âˆ—} , and this is a line in â„œ3 through
the origin and the point (1, 2, 3).
There are times when it is desirable to know whether or not two matrices
have the same row space or the same range. The following theorem provides the
solution to this problem.
Equal Ranges
For two matrices A and B of the same shape:
â€¢
R

AT 
= R

BT 
if and only if A
row
âˆ¼B.
(4.2.5)
â€¢
R (A) = R (B) if and only if A
col
âˆ¼B.
(4.2.6)
Proof.
To prove (4.2.5), ï¬rst assume A
row
âˆ¼B so that there exists a nonsingular
matrix P such that PA = B. To see that R

AT 
= R

BT 
, use (4.2.4) to
write
a âˆˆR

AT 
â‡â‡’aT = yT A = yT Pâˆ’1PA
for some yT
â‡â‡’aT = zT B
for zT = yT Pâˆ’1
â‡â‡’a âˆˆR

BT 
.

172
Chapter 4
Vector Spaces
Conversely, if R

AT 
= R

BT 
, then
span {A1âˆ—, A2âˆ—, . . . , Amâˆ—} = span {B1âˆ—, B2âˆ—, . . . , Bmâˆ—} ,
so each row of B is a combination of the rows of A, and vice versa. On the
basis of this fact, it can be argued that it is possible to reduce A to B by using
only row operations (the tedious details are omitted), and thus A
row
âˆ¼B. The
proof of (4.2.6) follows by replacing A and B with AT and BT .
Example 4.2.2
Testing Spanning Sets. Two sets {a1, a2, . . . , ar} and {b1, b2, . . . , bs} in
â„œn span the same subspace if and only if the nonzero rows of EA agree with
the nonzero rows of EB, where A and B are the matrices containing the ai â€™s
and bi â€™s as rows. This is a corollary of (4.2.5) because zero rows are irrelevant
in considering the row space of a matrix, and we already know from (3.9.9) that
A
row
âˆ¼B if and only if EA = EB.
Problem: Determine whether or not the following sets span the same subspace:
A =
ï£±
ï£´
ï£²
ï£´
ï£³
ï£«
ï£¬
ï£­
1
2
2
3
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
2
4
1
3
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
3
6
1
4
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
,
B =
ï£±
ï£´
ï£²
ï£´
ï£³
ï£«
ï£¬
ï£­
0
0
1
1
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
1
2
3
4
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
.
Solution: Place the vectors as rows in matrices A and B, and compute
A =
ï£«
ï£­
1
2
2
3
2
4
1
3
3
6
1
4
ï£¶
ï£¸â†’
ï£«
ï£­
1
2
0
1
0
0
1
1
0
0
0
0
ï£¶
ï£¸= EA
and
B =

0
0
1
1
1
2
3
4

â†’

1
2
0
1
0
0
1
1

= EB.
Hence span {A} = span {B} because the nonzero rows in EA and EB agree.
We already know that the rows of A span R

AT 
, and the columns of A
span R (A), but itâ€™s often possible to span these spaces with fewer vectors than
the full set of rows and columns.
Spanning the Row Space and Range
Let A be an m Ã— n matrix, and let U be any row echelon form derived
from A. Spanning sets for the row and column spaces are as follows:
â€¢
The nonzero rows of U span R

AT 
.
(4.2.7)
â€¢
The basic columns in A span R (A).
(4.2.8)

4.2 Four Fundamental Subspaces
173
Proof.
Statement (4.2.7) is an immediate consequence of (4.2.5). To prove
(4.2.8), suppose that the basic columns in A are in positions b1, b2, . . . , br,
and the nonbasic columns occupy positions n1, n2, . . . , nt, and let Q1 be the
permutation matrix that permutes all of the basic columns in A to the left-hand
side so that AQ1 = ( BmÃ—r
NmÃ—t ) , where B contains the basic columns and
N contains the nonbasic columns. Since the nonbasic columns are linear com-
binations of the basic columnsâ€”recall (2.2.3)â€”we can annihilate the nonbasic
columns in N using elementary column operations. In other words, there is a
nonsingular matrix Q2 such that ( B
N ) Q2 = ( B
0 ) . Thus Q = Q1Q2 is
a nonsingular matrix such that AQ = AQ1Q2 = ( B
N ) Q2 = ( B
0 ) , and
hence A
col
âˆ¼( B
0 ). The conclusion (4.2.8) now follows from (4.2.6).
Example 4.2.3
Problem: Determine spanning sets for R (A) and R

AT 
, where
A =
ï£«
ï£­
1
2
2
3
2
4
1
3
3
6
1
4
ï£¶
ï£¸.
Solution: Reducing A to any row echelon form U provides the solutionâ€”the
basic columns in A correspond to the pivotal positions in U, and the nonzero
rows of U span the row space of A. Using EA =
 1
2
0
1
0
0
1
1
0
0
0
0
	
produces
R (A) = span
ï£±
ï£²
ï£³
ï£«
ï£­
1
2
3
ï£¶
ï£¸,
ï£«
ï£­
2
1
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
and
R

AT 
= span
ï£±
ï£´
ï£²
ï£´
ï£³
ï£«
ï£¬
ï£­
1
2
0
1
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
0
0
1
1
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
.
So far, only two of the four fundamental subspaces associated with each
matrix A âˆˆâ„œmÃ—n have been discussed, namely, R (A) and R

AT 
. To see
where the other two fundamental subspaces come from, consider again a general
linear function f mapping â„œn into â„œm, and focus on N(f) = {x | f(x) = 0}
(the set of vectors that are mapped to 0 ). N(f) is called the nullspace of f
(some texts call it the kernel of f), and itâ€™s easy to see that N(f) is a subspace
of â„œn because the closure properties (A1) and (M1) are satisï¬ed. Indeed, if
x1, x2 âˆˆN(f), then f(x1) = 0 and f(x2) = 0, so the linearity of f produces
f(x1 + x2) = f(x1) + f(x2) = 0 + 0 = 0
=â‡’
x1 + x2 âˆˆN(f).
(A1)
Similarly, if Î± âˆˆâ„œ, and if x âˆˆN(f), then f(x) = 0 and linearity implies
f(Î±x) = Î±f(x) = Î±0 = 0
=â‡’
Î±x âˆˆN(f).
(M1)
By considering the linear functions f(x) = Ax and g(y) = AT y, the
other two fundamental subspaces deï¬ned by A âˆˆâ„œmÃ—n are obtained. They are
N(f) = {xnÃ—1 | Ax = 0} âŠ†â„œn and N(g) =

ymÃ—1 | ATy = 0

âŠ†â„œm.

174
Chapter 4
Vector Spaces
Nullspace
â€¢
For an m Ã— n matrix A, the set N (A) = {xnÃ—1 | Ax = 0} âŠ†â„œn
is called the nullspace of A. In other words, N (A) is simply the
set of all solutions to the homogeneous system Ax = 0.
â€¢
The set N

AT 
=

ymÃ—1 | AT y = 0

âŠ†â„œm is called the left-
hand nullspace of A because N

AT 
is the set of all solutions
to the left-hand homogeneous system yT A = 0T .
Example 4.2.4
Problem: Determine a spanning set for N (A), where A =
 1
2
3
2
4
6

.
Solution:
N (A) is merely the general solution of Ax = 0, and this is deter-
mined by reducing A to a row echelon form U. As discussed in Â§2.4, any such
U will suï¬ƒce, so we will use EA =
 1
2
3
0
0
0

. Consequently, x1 = âˆ’2x2 âˆ’3x3,
where x2 and x3 are free, so the general solution of Ax = 0 is
ï£«
ï£­
x1
x2
x3
ï£¶
ï£¸=
ï£«
ï£­
âˆ’2x2 âˆ’3x3
x2
x3
ï£¶
ï£¸= x2
ï£«
ï£­
âˆ’2
1
0
ï£¶
ï£¸+ x3
ï£«
ï£­
âˆ’3
0
1
ï£¶
ï£¸.
In other words, N (A) is the set of all possible linear combinations of the vectors
h1 =
ï£«
ï£­
âˆ’2
1
0
ï£¶
ï£¸
and
h2 =
ï£«
ï£­
âˆ’3
0
1
ï£¶
ï£¸,
and therefore span {h1, h2} = N (A). For this example, N (A) is the plane in
â„œ3 that passes through the origin and the two points h1 and h2.
Example 4.2.4 indicates the general technique for determining a spanning
set for N (A). Below is a formal statement of this procedure.

4.2 Four Fundamental Subspaces
175
Spanning the Nullspace
To determine a spanning set for N (A), where rank (AmÃ—n) = r, row
reduce A to a row echelon form U, and solve Ux = 0 for the basic
variables in terms of the free variables to produce the general solution
of Ax = 0 in the form
x = xf1h1 + xf2h2 + Â· Â· Â· + xfnâˆ’rhnâˆ’r.
(4.2.9)
By deï¬nition, the set H = {h1, h2, . . . , hnâˆ’r} spans N (A). Moreover,
it can be proven that H is unique in the sense that H is independent
of the row echelon form U.
It was established in Â§2.4 that a homogeneous system Ax = 0 possesses a
unique solution (i.e., only the trivial solution x = 0 ) if and only if the rank of
the coeï¬ƒcient matrix equals the number of unknowns. This may now be restated
using vector space terminology.
Zero Nullspace
If A is an m Ã— n matrix, then
â€¢
N (A) = {0} if and only if rank (A) = n;
(4.2.10)
â€¢
N

AT 
= {0} if and only if rank (A) = m.
(4.2.11)
Proof.
We already know that the trivial solution x = 0 is the only solution to
Ax = 0 if and only if the rank of A is the number of unknowns, and this is
what (4.2.10) says. Similarly, AT y = 0 has only the trivial solution y = 0 if
and only if rank

AT 
= m. Recall from (3.9.11) that rank

AT 
= rank (A)
in order to conclude that (4.2.11) holds.
Finally, letâ€™s think about how to determine a spanning set for N

AT 
. Of
course, we can proceed in the same manner as described in Example 4.2.4 by
reducing AT to a row echelon form to extract the general solution for AT x = 0.
However, the other three fundamental subspaces are derivable directly from EA
(or any other row echelon form U
row
âˆ¼A ), so itâ€™s rather awkward to have to
start from scratch and compute a new echelon form just to get a spanning set
for N

AT 
. It would be better if a single reduction to echelon form could
produce all four of the fundamental subspaces. Note that EAT Ì¸= ET
A, so ET
A
wonâ€™t easily lead to N

AT 
. The following theorem helps resolve this issue.

176
Chapter 4
Vector Spaces
Left-Hand Nullspace
If rank (AmÃ—n) = r, and if PA = U, where P is nonsingular and
U is in row echelon form, then the last m âˆ’r rows in P span the
left-hand nullspace of A. In other words, if P =
 P1
P2

, where P2 is
(m âˆ’r) Ã— m, then
N

AT 
= R

PT
2

.
(4.2.12)
Proof.
If U =

C
0

, where CrÃ—n, then PA = U implies P2A = 0, and
this says R

PT
2

âŠ†N

AT 
. To show equality, demonstrate containment in
the opposite direction by arguing that every vector in N

AT 
must also be in
R

PT
2

. Suppose yT âˆˆN

AT 
, and let Pâˆ’1 = ( Q1
Q2 ) to conclude that
0 = yT A = yT Pâˆ’1U = yT Q1C
=â‡’
0 = yT Q1
because N

CT 
= {0} by (4.2.11). Now observe that PPâˆ’1 = I = Pâˆ’1P
insures P1Q1 = Ir and Q1P1 = Im âˆ’Q2P2, so
0 = yT Q1
=â‡’
0 = yT Q1P1 = yT (I âˆ’Q2P2)
=â‡’
yT = yT Q2P2 =

yT Q2

P2
=â‡’
y âˆˆR

PT
2

=â‡’
yT âˆˆR

PT
2

.
Example 4.2.5
Problem: Determine a spanning set for N

AT 
, where A =
 1
2
2
3
2
4
1
3
3
6
1
4

.
Solution: To ï¬nd a nonsingular matrix P such that PA = U is in row echelon
form, proceed as described in Exercise 3.9.1 and row reduce the augmented
matrix

A | I

to

U | P

. It must be the case that PA = U because P
is the product of the elementary matrices corresponding to the elementary row
operations used. Since any row echelon form will suï¬ƒce, we may use Gaussâ€“
Jordan reduction to reduce A to EA as shown below:
ï£«
ï£­
1
2
2
3
1
0
0
2
4
1
3
0
1
0
3
6
1
4
0
0
1
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
1
2
0
1
âˆ’1/3
2/3
0
0
0
1
1
2/3
âˆ’1/3
0
0
0
0
0
1/3
âˆ’5/3
1
ï£¶
ï£¸
P =
ï£«
ï£­
âˆ’1/3
2/3
0
2/3
âˆ’1/3
0
1/3
âˆ’5/3
1
ï£¶
ï£¸, so (4.2.12) implies N

AT 
= span
ï£±
ï£²
ï£³
ï£«
ï£­
1/3
âˆ’5/3
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾.

4.2 Four Fundamental Subspaces
177
Example 4.2.6
Problem: Suppose rank (AmÃ—n) = r, and let P =
 P1
P2

be a nonsingular
matrix such that PA = U =
 CrÃ—n
0

, where U is in row echelon form. Prove
R (A) = N (P2).
(4.2.13)
Solution: The strategy is to ï¬rst prove R (A) âŠ†N (P2) and then show the
reverse inclusion N (P2) âŠ†R (A). The equation PA = U implies P2A = 0, so
all columns of A are in N (P2), and thus R (A) âŠ†N (P2) . To show inclusion
in the opposite direction, suppose b âˆˆN (P2), so that
Pb =

P1
P2

b =

P1b
P2b

=

drÃ—1
0

.
Consequently, P

A | b

=

PA | Pb

=
 C
d
0
0

, and this implies
rank[A|b] = r = rank (A).
Recall from (2.3.4) that this means the system Ax = b is consistent, and thus
b âˆˆR (A) by (4.2.3). Therefore, N (P2) âŠ†R (A), and we may conclude that
N (P2) = R (A).
Itâ€™s often important to know when two matrices have the same nullspace (or
left-hand nullspace). Below is one test for determining this.
Equal Nullspaces
For two matrices A and B of the same shape:
â€¢
N (A) = N (B) if and only if A
row
âˆ¼B.
(4.2.14)
â€¢
N

AT 
= N

BT 
if and only if A
col
âˆ¼B.
(4.2.15)
Proof.
We will prove (4.2.15). If N

AT 
= N

BT 
, then (4.2.12) guarantees
R

PT
2

= N

BT 
, and hence P2B = 0. But this means the columns of B
are in N (P2). That is, R (B) âŠ†N (P2) = R (A) by using (4.2.13). If A is
replaced by B in the preceding argumentâ€”and in (4.2.13)â€” the result is that
R (A) âŠ†R (B), and consequently we may conclude that R (A) = R (B) . The
desired conclusion (4.2.15) follows from (4.2.6). Statement (4.2.14) now follows
by replacing A and B by AT and BT in (4.2.15).

178
Chapter 4
Vector Spaces
Summary
The four fundamental subspaces associated with AmÃ—n are as follows.
â€¢
The range or column space:
R (A) = {Ax} âŠ†â„œm.
â€¢
The row space or left-hand range:
R

AT 
=

AT y

âŠ†â„œn.
â€¢
The nullspace:
N (A) = {x | Ax = 0} âŠ†â„œn.
â€¢
The left-hand nullspace:
N

AT 
=

y | AT y = 0

âŠ†â„œm.
Let P be a nonsingular matrix such that PA = U, where U is in row
echelon form, and suppose rank (A) = r.
â€¢
Spanning set for R (A) = the basic columns in A.
â€¢
Spanning set for R

AT 
= the nonzero rows in U.
â€¢
Spanning set for N (A) =the hiâ€™s in the general solution of Ax = 0.
â€¢
Spanning set for N

AT 
= the last m âˆ’r rows of P.
If A and B have the same shape, then
â€¢
A
row
âˆ¼B â‡â‡’N (A) = N (B) â‡â‡’R

AT 
= R

BT 
.
â€¢
A
col
âˆ¼B â‡â‡’R (A) = R (B) â‡â‡’N

AT 
= N

BT 
.
Exercises for section 4.2
4.2.1. Determine spanning sets for each of the four fundamental subspaces
associated with
A =
ï£«
ï£­
1
2
1
1
5
âˆ’2
âˆ’4
0
4
âˆ’2
1
2
2
4
9
ï£¶
ï£¸.
4.2.2. Consider a linear system of equations AmÃ—nx = b.
(a)
Explain why Ax = b is consistent if and only if b âˆˆR (A).
(b)
Explain why a consistent system Ax = b has a unique solution
if and only if N (A) = {0}.

4.2 Four Fundamental Subspaces
179
4.2.3. Suppose that A is a 3 Ã— 3 matrix such that
R =
ï£±
ï£²
ï£³
ï£«
ï£­
1
2
3
ï£¶
ï£¸,
ï£«
ï£­
1
âˆ’1
2
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
and
N =
ï£±
ï£²
ï£³
ï£«
ï£­
âˆ’2
1
0
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
span R (A) and N (A), respectively, and consider a linear system
Ax = b, where b =

1
âˆ’7
0

.
(a)
Explain why Ax = b must be consistent.
(b)
Explain why Ax = b cannot have a unique solution.
4.2.4. If A =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’1
1
1
âˆ’2
1
âˆ’1
0
3
âˆ’4
2
âˆ’1
0
3
âˆ’5
3
âˆ’1
0
3
âˆ’6
4
âˆ’1
0
3
âˆ’6
4
ï£¶
ï£·
ï£·
ï£·
ï£¸and b =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’2
âˆ’5
âˆ’6
âˆ’7
âˆ’7
ï£¶
ï£·
ï£·
ï£·
ï£¸, is b âˆˆR (A) ?
4.2.5. Suppose that A is an n Ã— n matrix.
(a)
If R (A) = â„œn, explain why A must be nonsingular.
(b)
If A is nonsingular, describe its four fundamental subspaces.
4.2.6. Consider the matrices A =
ï£«
ï£­
1
1
5
2
0
6
1
2
7
ï£¶
ï£¸and B =
ï£«
ï£­
1
âˆ’4
4
4
âˆ’8
6
0
âˆ’4
5
ï£¶
ï£¸.
(a)
Do A and B have the same row space?
(b)
Do A and B have the same column space?
(c)
Do A and B have the same nullspace?
(d)
Do A and B have the same left-hand nullspace?
4.2.7. If A =
 A1
A2

is a square matrix such that N (A1) = R

AT
2

, prove
that A must be nonsingular.
4.2.8. Consider a linear system of equations Ax = b for which yT b = 0
for every y âˆˆN

AT 
. Explain why this means the system must be
consistent.
4.2.9. For matrices AmÃ—n and BmÃ—p, prove that
R (A | B) = R (A) + R (B).

180
Chapter 4
Vector Spaces
4.2.10. Let p be one particular solution of a linear system Ax = b.
(a)
Explain the signiï¬cance of the set
p + N (A) = {p + h | h âˆˆN (A)} .
(b)
If rank (A3Ã—3) = 1, sketch a picture of p + N (A) in â„œ3.
(c)
Repeat part (b) for the case when rank (A3Ã—3) = 2.
4.2.11. Suppose that Ax = b is a consistent system of linear equations, and
let a âˆˆR

AT 
. Prove that the inner product aT x is constant for all
solutions to Ax = b.
4.2.12. For matrices such that the product AB is deï¬ned, explain why each of
the following statements is true.
(a)
R (AB) âŠ†R (A).
(b)
N (AB) âŠ‡N (B).
4.2.13. Suppose that B = {b1, b2, . . . , bn} is a spanning set for R (B). Prove
that A(B) = {Ab1, Ab2, . . . , Abn} is a spanning set for R (AB).

4.3 Linear Independence
181
4.3
LINEAR INDEPENDENCE
For a given set of vectors S = {v1, v2, . . . , vn} there may or may not exist
dependency relationships in the sense that it may or may not be possible to
express one vector as a linear combination of the others. For example, in the set
A =
ï£±
ï£²
ï£³
ï£«
ï£­
1
âˆ’1
2
ï£¶
ï£¸,
ï£«
ï£­
3
0
âˆ’1
ï£¶
ï£¸,
ï£«
ï£­
9
âˆ’3
4
ï£¶
ï£¸
ï£¼
ï£½
ï£¾,
the third vector is a linear combination of the ï¬rst twoâ€”i.e., v3 = 3v1 + 2v2.
Such a dependency always can be expressed in terms of a homogeneous equation
by writing
3v1 + 2v2 âˆ’v3 = 0.
On the other hand, it is evident that there are no dependency relationships in
the set
B =
ï£±
ï£²
ï£³
ï£«
ï£­
1
0
0
ï£¶
ï£¸,
ï£«
ï£­
0
1
0
ï£¶
ï£¸,
ï£«
ï£­
0
0
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
because no vector can be expressed as a combination of the others. Another way
to say this is to state that there are no solutions for Î±1, Î±2, and Î±3 in the
homogeneous equation
Î±1v1 + Î±2v2 + Î±3v3 = 0
other than the trivial solution Î±1 = Î±2 = Î±3 = 0. These observations are the
basis for the following deï¬nitions.
Linear Independence
A set of vectors S = {v1, v2, . . . , vn} is said to be a linearly in-
dependent set whenever the only solution for the scalars Î±i in the
homogeneous equation
Î±1v1 + Î±2v2 + Â· Â· Â· + Î±nvn = 0
(4.3.1)
is the trivial solution Î±1 = Î±2 = Â· Â· Â· = Î±n = 0. Whenever there is a
nontrivial solution for the Î± â€™s (i.e., at least one Î±i Ì¸= 0 ) in (4.3.1), the
set S is said to be a linearly dependent set. In other words, linearly
independent sets are those that contain no dependency relationships,
and linearly dependent sets are those in which at least one vector is a
combination of the others. We will agree that the empty set is always
linearly independent.

182
Chapter 4
Vector Spaces
It is important to realize that the concepts of linear independence and de-
pendence are deï¬ned only for setsâ€”individual vectors are neither linearly inde-
pendent nor dependent. For example consider the following sets:
S1 =

1
0

,

0
1

,
S2 =

1
0

,

1
1

,
S3 =

1
0

,

0
1

,

1
1

.
It should be clear that S1 and S2 are linearly independent sets while S3 is
linearly dependent. This shows that individual vectors can simultaneously belong
to linearly independent sets as well as linearly dependent sets. Consequently, it
makes no sense to speak of â€œlinearly independent vectorsâ€ or â€œlinearly dependent
vectors.â€
Example 4.3.1
Problem: Determine whether or not the set
S =
ï£±
ï£²
ï£³
ï£«
ï£­
1
2
1
ï£¶
ï£¸,
ï£«
ï£­
1
0
2
ï£¶
ï£¸,
ï£«
ï£­
5
6
7
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
is linearly independent.
Solution:
Simply determine whether or not there exists a nontrivial solution
for the Î± â€™s in the homogeneous equation
Î±1
ï£«
ï£­
1
2
1
ï£¶
ï£¸+ Î±2
ï£«
ï£­
1
0
2
ï£¶
ï£¸+ Î±3
ï£«
ï£­
5
6
7
ï£¶
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸
or, equivalently, if there is a nontrivial solution to the homogeneous system
ï£«
ï£­
1
1
5
2
0
6
1
2
7
ï£¶
ï£¸
ï£«
ï£­
Î±1
Î±2
Î±3
ï£¶
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸.
If A =
 1
1
5
2
0
6
1
2
7

, then EA =
 1
0
3
0
1
2
0
0
0

, and therefore there exist nontrivial
solutions. Consequently, S is a linearly dependent set. Notice that one particular
dependence relationship in S is revealed by EA because it guarantees that
Aâˆ—3 = 3Aâˆ—1 +2Aâˆ—2. This example indicates why the question of whether or not
a subset of â„œm is linearly independent is really a question about whether or not
the nullspace of an associated matrix is trivial. The following is a more formal
statement of this fact.

4.3 Linear Independence
183
Linear Independence and Matrices
Let A be an m Ã— n matrix.
â€¢
Each of the following statements is equivalent to saying that the
columns of A form a linearly independent set.
â–·
N (A) = {0}.
(4.3.2)
â–·
rank (A) = n.
(4.3.3)
â€¢
Each of the following statements is equivalent to saying that the rows
of A form a linearly independent set.
â–·
N

AT 
= {0}.
(4.3.4)
â–·
rank (A) = m.
(4.3.5)
â€¢
When A is a square matrix, each of the following statements is
equivalent to saying that A is nonsingular.
â–·
The columns of A form a linearly independent set.
(4.3.6)
â–·
The rows of A form a linearly independent set.
(4.3.7)
Proof.
By deï¬nition, the columns of A are a linearly independent set when
the only set of Î± â€™s satisfying the homogeneous equation
0 = Î±1Aâˆ—1 + Î±2Aâˆ—2 + Â· Â· Â· + Î±nAâˆ—n =

Aâˆ—1 | Aâˆ—2 | Â· Â· Â· | Aâˆ—n

ï£«
ï£¬
ï£¬
ï£­
Î±1
Î±2
...
Î±n
ï£¶
ï£·
ï£·
ï£¸
is the trivial solution Î±1 = Î±2 = Â· Â· Â· = Î±n = 0, which is equivalent to saying
N (A) = {0}. The fact that N (A) = {0} is equivalent to rank (A) = n was
demonstrated in (4.2.10). Statements (4.3.4) and (4.3.5) follow by replacing A
by AT in (4.3.2) and (4.3.3) and by using the fact that rank (A) = rank

AT 
.
Statements (4.3.6) and (4.3.7) are simply special cases of (4.3.3) and (4.3.5).
Example 4.3.2
Any set {ei1, ei2, . . . , ein} consisting of distinct unit vectors is a linearly indepen-
dent set because rank

ei1 | ei2 | Â· Â· Â· | ein

= n. For example, the set of unit vec-
tors {e1, e2, e4} in â„œ4 is linearly independent because rank
ï£«
ï£­
1
0
0
0
1
0
0
0
0
0
0
1
ï£¶
ï£¸= 3.

184
Chapter 4
Vector Spaces
Example 4.3.3
Diagonal Dominance. A matrix AnÃ—n is said to be diagonally dominant
whenever
|aii| >
n

j=1
jÌ¸=i
|aij|
for each i = 1, 2, . . . , n.
That is, the magnitude of each diagonal entry exceeds the sum of the magni-
tudes of the oï¬€-diagonal entries in the corresponding row. Diagonally dominant
matrices occur naturally in a wide variety of practical applications, and when
solving a diagonally dominant system by Gaussian elimination, partial pivoting
is never requiredâ€”you are asked to provide the details in Exercise 4.3.15.
Problem: In 1900, Minkowski (p. 278) discovered that all diagonally dominant
matrices are nonsingular. Establish the validity of Minkowskiâ€™s result.
Solution: The strategy is to prove that if A is diagonally dominant, then
N (A) = {0}, so that (4.3.2) together with (4.3.6) will provide the desired
conclusion. Use an indirect argumentâ€”suppose there exists a vector x Ì¸= 0 such
that Ax = 0, and assume that xk is the entry of maximum magnitude in x.
Focus on the kth component of Ax, and write the equation Akâˆ—x = 0 as
akkxk = âˆ’
n

j=1
jÌ¸=k
akjxj.
Taking absolute values of both sides and using the triangle inequality together
with the fact that |xj| â‰¤|xk| for each j produces
|akk| |xk| =

n

j=1
jÌ¸=k
akjxj

â‰¤
n

j=1
jÌ¸=k
|akjxj| =
n

j=1
jÌ¸=k
|akj| |xj| â‰¤

n

j=1
jÌ¸=k
|akj|
 
|xk|.
But this implies that
|akk| â‰¤
n

j=1
jÌ¸=k
|akj|,
which violates the hypothesis that A is diagonally dominant. Therefore, the
assumption that there exists a nonzero vector in N (A) must be false, so we
may conclude that N (A) = {0}, and hence A is nonsingular.
Note: An alternate solution is given in Example 7.1.6 on p. 499.

4.3 Linear Independence
185
Example 4.3.4
Vandermonde Matrices. Matrices of the form
VmÃ—n =
ï£«
ï£¬
ï£¬
ï£­
1
x1
x2
1
Â· Â· Â·
xnâˆ’1
1
1
x2
x2
2
Â· Â· Â·
xnâˆ’1
2
...
...
...
Â· Â· Â·
...
1
xm
x2
m
Â· Â· Â·
xnâˆ’1
m
ï£¶
ï£·
ï£·
ï£¸
in which xi Ì¸= xj for all i Ì¸= j are called Vandermonde
26 matrices.
Problem: Explain why the columns in V constitute a linearly independent set
whenever n â‰¤m.
Solution: According to (4.3.2), the columns of V form a linearly independent
set if and only if N (V) = {0}. If
ï£«
ï£¬
ï£¬
ï£­
1
x1
x2
1
Â· Â· Â·
xnâˆ’1
1
1
x2
x2
2
Â· Â· Â·
xnâˆ’1
2
...
...
...
Â· Â· Â·
...
1
xm
x2
m
Â· Â· Â·
xnâˆ’1
m
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
Î±0
Î±1
...
Î±nâˆ’1
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
0
0
...
0
ï£¶
ï£·
ï£·
ï£¸,
(4.3.8)
then for each i = 1, 2, . . . , m,
Î±0 + xiÎ±1 + x2
i Î±2 + Â· Â· Â· + xnâˆ’1
i
Î±nâˆ’1 = 0.
This implies that the polynomial
p(x) = Î±0 + Î±1x + Î±2x2 + Â· Â· Â· + Î±nâˆ’1xnâˆ’1
has m distinct rootsâ€”namely, the xi â€™s. However, deg p(x) â‰¤n âˆ’1 and the
fundamental theorem of algebra guarantees that if p(x) is not the zero polyno-
mial, then p(x) can have at most n âˆ’1 distinct roots. Therefore, (4.3.8) holds
if and only if Î±i = 0 for all i, and thus (4.3.2) insures that the columns of V
form a linearly independent set.
26
This is named in honor of the French mathematician Alexandre-Theophile Vandermonde (1735â€“
1796). He made a variety of contributions to mathematics, but he is best known perhaps for
being the ï¬rst European to give a logically complete exposition of the theory of determinants.
He is regarded by many as being the founder of that theory. However, the matrix V (and
an associated determinant) named after him, by Lebesgue, does not appear in Vandermondeâ€™s
published work. Vandermondeâ€™s ï¬rst love was music, and he took up mathematics only after
he was 35 years old. He advocated the theory that all art and music rested upon a general
principle that could be expressed mathematically, and he claimed that almost anyone could
become a composer with the aid of mathematics.

186
Chapter 4
Vector Spaces
Example 4.3.5
Problem: Given a set of m points S = {(x1, y1),
(x2, y2), . . . , (xm, ym)} in
which the xi â€™s are distinct, explain why there is a unique polynomial
â„“(t) = Î±0 + Î±1t + Î±2t2 + Â· Â· Â· + Î±mâˆ’1tmâˆ’1
(4.3.9)
of degree m âˆ’1 that passes through each point in S.
Solution: The coeï¬ƒcients Î±i must satisfy the equations
Î±0 + Î±1x1 + Î±2x2
1 + Â· Â· Â· + Î±mâˆ’1xmâˆ’1
1
= â„“(x1) = y1,
Î±0 + Î±1x2 + Î±2x2
2 + Â· Â· Â· + Î±mâˆ’1xmâˆ’1
2
= â„“(x2) = y2,
...
Î±0 + Î±1xm + Î±2x2
m + Â· Â· Â· + Î±mâˆ’1xmâˆ’1
m
= â„“(xm) = ym.
Writing this m Ã— m system as
ï£«
ï£¬
ï£¬
ï£­
1
x1
x2
1
Â· Â· Â·
xmâˆ’1
1
1
x2
x2
2
Â· Â· Â·
xmâˆ’1
2
...
...
...
Â· Â· Â·
...
1
xm
x2
m
Â· Â· Â·
xmâˆ’1
m
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
Î±0
Î±1
...
Î±mâˆ’1
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
y1
y2
...
ym
ï£¶
ï£·
ï£·
ï£¸
reveals that the coeï¬ƒcient matrix is a square Vandermonde matrix, so the result
of Example 4.3.4 guarantees that it is nonsingular. Consequently, the system has
a unique solution, and thus there is one and only one possible set of coeï¬ƒcients
for the polynomial â„“(t) in (4.3.9). In fact, â„“(t) must be given by
â„“(t) =
m

i=1
ï£«
ï£­yi
!m
jÌ¸=i(t âˆ’xj)
!m
jÌ¸=i(xi âˆ’xj)
ï£¶
ï£¸.
Verify this by showing that the right-hand side is indeed a polynomial of degree
m âˆ’1 that passes through the points in S. The polynomial â„“(t) is known as
the Lagrange
27 interpolation polynomial of degree m âˆ’1.
If rank (AmÃ—n) < n, then the columns of A must be a dependent setâ€”
recall (4.3.3). For such matrices we often wish to extract a maximal linearly
independent subset of columnsâ€”i.e., a linearly independent set containing as
many columns from A as possible. Although there can be several ways to make
such a selection, the basic columns in A always constitute one solution.
27
Joseph Louis Lagrange (1736â€“1813), born in Turin, Italy, is considered by many to be one
of the two greatest mathematicians of the eighteenth centuryâ€”Euler is the other. Lagrange
occupied Eulerâ€™s vacated position in 1766 in Berlin at the court of Frederick the Great who
wrote that â€œthe greatest king in Europeâ€ wishes to have at his court â€œthe greatest mathe-
matician of Europe.â€ After 20 years, Lagrange left Berlin and eventually moved to France.
Lagrangeâ€™s mathematical contributions are extremely wide and deep, but he had a particularly
strong inï¬‚uence on the way mathematical research evolved. He was the ï¬rst of the top-class
mathematicians to recognize the weaknesses in the foundations of calculus, and he was among
the ï¬rst to attempt a rigorous development.

4.3 Linear Independence
187
Maximal Independent Subsets
If rank (AmÃ—n) = r, then the following statements hold.
â€¢
Any maximal independent subset of columns from A con-
tains exactly r columns.
(4.3.10)
â€¢
Any maximal independent subset of rows from A contains
exactly r rows.
(4.3.11)
â€¢
In particular, the r basic columns in A constitute one
maximal independent subset of columns from A.
(4.3.12)
Proof.
Exactly the same linear relationships that exist among the columns of
A must also hold among the columns of EA â€”by (3.9.6). This guarantees that
a subset of columns from A is linearly independent if and only if the columns
in the corresponding positions in EA are an independent set. Let
C =

c1 | c2 | Â· Â· Â· | ck

be a matrix that contains an independent subset of columns from EA so that
rank (C) = k â€”recall (4.3.3). Since each column in EA is a combination of the
r basic (unit) columns in EA, there are scalars Î²ij such that cj = r
i=1 Î²ijei
for j = 1, 2, . . . , k. These equations can be written as the single matrix equation

c1 | c2 | Â· Â· Â· | ck

=

e1 | e2 | Â· Â· Â· | er

ï£«
ï£¬
ï£¬
ï£­
Î²11
Î²12
Â· Â· Â·
Î²1k
Î²21
Î²22
Â· Â· Â·
Î²2k
...
...
...
...
Î²r1
Î²r2
Â· Â· Â·
Î²rk
ï£¶
ï£·
ï£·
ï£¸
or
CmÃ—k =

Ir
0

BrÃ—k =

BrÃ—k
0

,
where
B = [Î²ij].
Consequently, r â‰¥rank (C) = k, and therefore any independent subset of
columns from EA â€”and hence any independent set of columns from A â€”cannot
contain more than r vectors. Because the r basic (unit) columns in EA form
an independent set, the r basic columns in A constitute an independent set.
This proves (4.3.10) and (4.3.12). The proof of (4.3.11) follows from the fact that
rank (A) = rank

AT 
â€”recall (3.9.11).

188
Chapter 4
Vector Spaces
Basic Facts of Independence
For a nonempty set of vectors S = {u1, u2, . . . , un} in a space V, the
following statements are true.
â€¢
If S contains a linearly dependent subset, then S itself
must be linearly dependent.
(4.3.13)
â€¢
If S is linearly independent, then every subset of S is
also linearly independent.
(4.3.14)
â€¢
If S is linearly independent and if v âˆˆV, then the ex-
tension set Sext = S âˆª{v} is linearly independent if and
only if v /âˆˆspan (S) .
(4.3.15)
â€¢
If S âŠ†â„œm and if n > m, then S must be linearly
dependent.
(4.3.16)
Proof of (4.3.13).
Suppose that S contains a linearly dependent subset, and,
for the sake of convenience, suppose that the vectors in S have been permuted
so that this dependent subset is Sdep = {u1, u2, . . . , uk} . According to the
deï¬nition of dependence, there must be scalars Î±1, Î±2, . . . , Î±k, not all of which
are zero, such that Î±1u1 +Î±2u2 +Â· Â· Â·+Î±kuk = 0. This means that we can write
Î±1u1 + Î±2u2 + Â· Â· Â· + Î±kuk + 0uk+1 + Â· Â· Â· + 0un = 0,
where not all of the scalars are zero, and hence S is linearly dependent.
Proof of (4.3.14).
This is an immediate consequence of (4.3.13).
Proof of (4.3.15).
If Sext is linearly independent, then v /âˆˆspan (S) , for
otherwise v would be a combination of vectors from S thus forcing Sext to
be a dependent set. Conversely, suppose v /âˆˆspan (S) . To prove that Sext is
linearly independent, consider a linear combination
Î±1u1 + Î±2u2 + Â· Â· Â· + Î±nun + Î±n+1v = 0.
(4.3.17)
It must be the case that Î±n+1 = 0, for otherwise v would be a combination of
vectors from S. Consequently,
Î±1u1 + Î±2u2 + Â· Â· Â· + Î±nun = 0.
But this implies that
Î±1 = Î±2 = Â· Â· Â· = Î±n = 0
because S is linearly independent. Therefore, the only solution for the Î± â€™s in
(4.3.17) is the trivial set, and hence Sext must be linearly independent.
Proof of (4.3.16).
This follows from (4.3.3) because if the ui â€™s are placed as
columns in a matrix AmÃ—n, then rank (A) â‰¤m < n.

4.3 Linear Independence
189
Example 4.3.6
Let V be the vector space of real-valued functions of a real variable, and let S =
{f1(x), f2(x), . . . , fn(x)} be a set of functions that are nâˆ’1 times diï¬€erentiable.
The Wronski
28 matrix is deï¬ned to be
W(x) =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
f1(x)
f2(x)
Â· Â· Â·
fn(x)
f â€²
1(x)
f â€²
2(x)
Â· Â· Â·
f â€²
n(x)
...
...
...
...
f (nâˆ’1)
1
(x)
f (nâˆ’1)
2
(x)
Â· Â· Â·
f (nâˆ’1)
n
(x)
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Problem: If there is at least one point x = x0 such that W(x0) is nonsingular,
prove that S must be a linearly independent set.
Solution: Suppose that
0 = Î±1f1(x) + Î±2f2(x) + Â· Â· Â· + Î±nfn(x)
(4.3.18)
for all values of x. When x = x0, it follows that
0 = Î±1f1(x0) + Î±2f2(x0) + Â· Â· Â· + Î±nfn(x0),
0 = Î±1f â€²
1(x0) + Î±2f â€²
2(x0) + Â· Â· Â· + Î±nf â€²
n(x0),
...
0 = Î±1f (nâˆ’1)
1
(x0) + Î±2f (nâˆ’1)
2
(x0) + Â· Â· Â· + Î±nf (nâˆ’1)
n
(x0),
which means that v =
ï£«
ï£¬
ï£­
Î±1
Î±2
...
Î±n
ï£¶
ï£·
ï£¸âˆˆN

W(x0)

. But N

W(x0)

= {0} because
W(x0) is nonsingular, and hence v = 0. Therefore, the only solution for the
Î± â€™s in (4.3.18) is the trivial solution Î±1 = Î±2 = Â· Â· Â· = Î±n = 0 thereby insuring
that S is linearly independent.
28
This matrix is named in honor of the Polish mathematician Jozef Maria HÂ¨oenÂ´e Wronski
(1778â€“1853), who studied four special forms of determinants, one of which was the deter-
minant of the matrix that bears his name. Wronski was born to a poor family near Poznan,
Poland, but he studied in Germany and spent most of his life in France. He is reported to have
been an egotistical person who wrote in an exhaustively wearisome style. Consequently, almost
no one read his work. Had it not been for his lone follower, Ferdinand Schweins (1780â€“1856)
of Heidelberg, Wronski would probably be unknown today. Schweins preserved and extended
Wronskiâ€™s results in his own writings, which in turn received attention from others. Wronski
also wrote on philosophy. While trying to reconcile Kantâ€™s metaphysics with Leibnizâ€™s calculus,
Wronski developed a social philosophy called â€œMessianismâ€ that was based on the belief that
absolute truth could be achieved through mathematics.

190
Chapter 4
Vector Spaces
For example, to verify that the set of polynomials P =

1, x, x2, . . . , xn
is
linearly independent, observe that the associated Wronski matrix
W(x) =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
x
x2
Â· Â· Â·
xn
0
1
2x
Â· Â· Â·
nxnâˆ’1
0
0
2
Â· Â· Â·
n(n âˆ’1)xnâˆ’2
...
...
...
...
...
0
0
0
Â· Â· Â·
n!
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
is triangular with nonzero diagonal entries. Consequently, W(x) is nonsingular
for every value of x, and hence P must be an independent set.
Exercises for section 4.3
4.3.1. Determine which of the following sets are linearly independent. For those
sets that are linearly dependent, write one of the vectors as a linear
combination of the others.
(a)
ï£±
ï£²
ï£³
ï£«
ï£­
1
2
3
ï£¶
ï£¸,
ï£«
ï£­
2
1
0
ï£¶
ï£¸,
ï£«
ï£­
1
5
9
ï£¶
ï£¸
ï£¼
ï£½
ï£¾,
(b)
{( 1
2
3 ) , ( 0
4
5 ) , ( 0
0
6 ) , ( 1
1
1 )} ,
(c)
ï£±
ï£²
ï£³
ï£«
ï£­
3
2
1
ï£¶
ï£¸,
ï£«
ï£­
1
0
0
ï£¶
ï£¸,
ï£«
ï£­
2
1
0
ï£¶
ï£¸
ï£¼
ï£½
ï£¾,
(d)
{( 2
2
2
2 ) , ( 2
2
0
2 ) , ( 2
0
2
2 )} ,
(e)
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
2
0
4
0
3
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
2
0
4
1
3
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
2
1
4
0
3
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
2
0
4
0
3
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£¼
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£½
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£¾
.
4.3.2. Consider the matrix A =
 2
1
1
0
4
2
1
2
6
3
2
2

.
(a)
Determine a maximal linearly independent subset of columns
from A.
(b)
Determine the total number of linearly independent subsets that
can be constructed using the columns of A.

4.3 Linear Independence
191
4.3.3. Suppose that in a population of a million children the height of each one
is measured at ages 1 year, 2 years, and 3 years, and accumulate this
data in a matrix
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1 yr
2 yr
3 yr
#1
h11
h12
h13
#2
h21
h22
h23
...
...
...
...
#i
hi1
hi2
hi3
...
...
...
...
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
= H.
Explain why there are at most three â€œindependent childrenâ€ in the sense
that the heights of all the other children must be a combination of these
â€œindependentâ€ ones.
4.3.4. Consider a particular species of wildï¬‚ower in which each plant has several
stems, leaves, and ï¬‚owers, and for each plant let the following hold.
S = the average stem length (in inches).
L = the average leaf width (in inches).
F = the number of ï¬‚owers.
Four particular plants are examined, and the information is tabulated
in the following matrix:
A =
ï£«
ï£¬
ï£¬
ï£­
S
L
F
#1
1
1
10
#2
2
1
12
#3
2
2
15
#4
3
2
17
ï£¶
ï£·
ï£·
ï£¸.
For these four plants, determine whether or not there exists a linear rela-
tionship between S, L, and F. In other words, do there exist constants
Î±0, Î±1, Î±2, and Î±3 such that Î±0 + Î±1S + Î±2L + Î±3F = 0 ?
4.3.5. Let S = {0} be the set containing only the zero vector.
(a)
Explain why S must be linearly dependent.
(b)
Explain why any set containing a zero vector must be linearly
dependent.
4.3.6. If T is a triangular matrix in which each tii Ì¸= 0, explain why the rows
and columns of T must each be linearly independent sets.

192
Chapter 4
Vector Spaces
4.3.7. Determine whether or not the following set of matrices is a linearly
independent set:

1
0
0
0

,

1
1
0
0

,

1
1
1
0

,

1
1
1
1

.
4.3.8. Without doing any computation, determine whether the following ma-
trix is singular or nonsingular:
A =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
n
1
1
Â· Â· Â·
1
1
n
1
Â· Â· Â·
1
1
1
n
Â· Â· Â·
1
...
...
...
...
...
1
1
1
Â· Â· Â·
n
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
nÃ—n
.
4.3.9. In theory, determining whether or not a given set is linearly independent
is a well-deï¬ned problem with a straightforward solution. In practice,
however, this problem is often not so well deï¬ned because it becomes
clouded by the fact that we usually cannot use exact arithmetic, and con-
tradictory conclusions may be produced depending upon the precision
of the arithmetic. For example, let
S =
ï£±
ï£²
ï£³
ï£«
ï£­
.1
.4
.7
ï£¶
ï£¸,
ï£«
ï£­
.2
.5
.8
ï£¶
ï£¸,
ï£«
ï£­
.3
.6
.901
ï£¶
ï£¸
ï£¼
ï£½
ï£¾.
(a)
Use exact arithmetic to determine whether or not S is linearly
independent.
(b)
Use 3-digit arithmetic (without pivoting or scaling) to determine
whether or not S is linearly independent.
4.3.10. If AmÃ—n is a matrix such that n
j=1 aij = 0 for each i = 1, 2, . . . , m
(i.e., each row sum is 0), explain why the columns of A are a linearly
dependent set, and hence rank (A) < n.
4.3.11. If S = {u1, u2, . . . , un} is a linearly independent subset of â„œmÃ—1, and
if PmÃ—m is a nonsingular matrix, explain why the set
P(S) = {Pu1, Pu2, . . . , Pun}
must also be a linearly independent set. Is this result still true if P is
singular?

4.3 Linear Independence
193
4.3.12. Suppose that S = {u1, u2, . . . , un} is a set of vectors from â„œm. Prove
that S is linearly independent if and only if the set
Sâ€² =

u1,
2

i=1
ui,
3

i=1
ui, . . . ,
n

i=1
ui

is linearly independent.
4.3.13. Which of the following sets of functions are linearly independent?
(a)
{sin x, cos x, x sin x} .
(b)

ex, xex, x2ex
.
(c)

sin2 x, cos2 x, cos 2x

.
4.3.14. Prove that the converse of the statement given in Example 4.3.6 is false
by showing that S =

x3, |x|3
is a linearly independent set, but the
associated Wronski matrix W(x) is singular for all values of x.
4.3.15. If AT is diagonally dominant, explain why partial pivoting is not needed
when solving Ax = b by Gaussian elimination. Hint: If after one step
of Gaussian elimination we have
A =

Î±
dT
c
B

one step
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’
 Î±
dT
0
B âˆ’cdT
Î±

,
show that AT being diagonally dominant implies X =

B âˆ’cdT
Î±
T
must also be diagonally dominant.

194
Chapter 4
Vector Spaces
4.4
BASIS AND DIMENSION
Recall from Â§4.1 that S is a spanning set for a space V if and only if every
vector in V is a linear combination of vectors in S. However, spanning sets
can contain redundant vectors. For example, a subspace L deï¬ned by a line
through the origin in â„œ2 may be spanned by any number of nonzero vectors
{v1, v2, . . . , vk} in L, but any one of the vectors {vi} by itself will suï¬ƒce.
Similarly, a plane P through the origin in â„œ3 can be spanned in many diï¬€erent
ways, but the parallelogram law indicates that a minimal spanning set need only
be an independent set of two vectors from P. These considerations motivate the
following deï¬nition.
Basis
A linearly independent spanning set for a vector space V is called a
basis for V.
It can be proven that every vector space V possesses a basisâ€”details for
the case when V âŠ†â„œm are asked for in the exercises. Just as in the case of
spanning sets, a space can possess many diï¬€erent bases.
Example 4.4.1
â€¢
The unit vectors S = {e1, e2, . . . , en} in â„œn are a basis for â„œn. This is
called the standard basis for â„œn.
â€¢
If A is an n Ã— n nonsingular matrix, then the set of rows in A as well as
the set of columns from A constitute a basis for â„œn. For example, (4.3.3)
insures that the columns of A are linearly independent, and we know they
span â„œn because R (A) = â„œn â€”recall Exercise 4.2.5(b).
â€¢
For the trivial vector space Z = {0}, there is no nonempty linearly indepen-
dent spanning set. Consequently, the empty set is considered to be a basis
for Z.
â€¢
The set

1, x, x2, . . . , xn
is a basis for the vector space of polynomials
having degree n or less.
â€¢
The inï¬nite set

1, x, x2, . . .

is a basis for the vector space of all polynomi-
als. It should be clear that no ï¬nite basis is possible.

4.4 Basis and Dimension
195
Spaces that possess a basis containing an inï¬nite number of vectors are
referred to as inï¬nite-dimensional spaces, and those that have a ï¬nite basis
are called ï¬nite-dimensional spaces. This is often a line of demarcation in
the study of vector spaces. A complete theoretical treatment would include the
analysis of inï¬nite-dimensional spaces, but this text is primarily concerned with
ï¬nite-dimensional spaces over the real or complex numbers. It can be shown that,
in eï¬€ect, this amounts to analyzing â„œn or Cn and their subspaces.
The original concern of this section was to try to eliminate redundancies
from spanning sets so as to provide spanning sets containing a minimal number
of vectors. The following theorem shows that a basis is indeed such a set.
Characterizations of a Basis
Let V be a subspace of â„œm, and let B = {b1, b2, . . . , bn} âŠ†V. The
following statements are equivalent.
â€¢
B is a basis for V.
(4.4.1)
â€¢
B is a minimal spanning set for V.
(4.4.2)
â€¢
B is a maximal linearly independent subset of V.
(4.4.3)
Proof.
First argue that (4.4.1) =â‡’(4.4.2) =â‡’(4.4.1), and then show (4.4.1)
is equivalent to (4.4.3).
Proof of (4.4.1) =â‡’(4.4.2). First suppose that B is a basis for V, and
prove that B is a minimal spanning set by using an indirect argumentâ€”i.e.,
assume that B is not minimal, and show that this leads to a contradiction. If
X = {x1, x2, . . . , xk} is a basis for V in which k < n, then each bj can be
written as a combination of the xi â€™s. That is, there are scalars Î±ij such that
bj =
k

i=1
Î±ijxi
for j = 1, 2, . . . , n.
(4.4.4)
If the b â€™s and x â€™s are placed as columns in matrices
BmÃ—n =

b1 | b2 | Â· Â· Â· | bn

and
XmÃ—k =

x1 | x2 | Â· Â· Â· | xk

,
then (4.4.4) can be expressed as the matrix equation
B = XA,
where,
AkÃ—n = [Î±ij] .
Since the rank of a matrix cannot exceed either of its size dimensions, and since
k < n, we have that rank (A) â‰¤k < n, so that N (A) Ì¸= {0} â€”recall (4.2.10).
If z Ì¸= 0 is such that Az = 0, then Bz = 0. But this is impossible because

196
Chapter 4
Vector Spaces
the columns of B are linearly independent, and hence N (B) = {0} â€”recall
(4.3.2). Therefore, the supposition that there exists a basis for V containing
fewer than n vectors must be false, and we may conclude that B is indeed a
minimal spanning set.
Proof of (4.4.2) =â‡’(4.4.1). If B is a minimal spanning set, then B must
be a linearly independent spanning set. Otherwise, some bi would be a linear
combination of the other b â€™s, and the set
Bâ€² = {b1, . . . , biâˆ’1, bi+1, . . . , bn}
would still span V, but Bâ€² would contain fewer vectors than B, which is im-
possible because B is a minimal spanning set.
Proof of (4.4.3) =â‡’(4.4.1). If B is a maximal linearly independent subset
of V, but not a basis for V, then there exists a vector v âˆˆV such that
v /âˆˆspan (B) . This means that the extension set
B âˆª{v} = {b1, b2, . . . , bn, v}
is linearly independentâ€”recall (4.3.15). But this is impossible because B is a
maximal linearly independent subset of V. Therefore, B is a basis for V.
Proof of (4.4.1) =â‡’(4.4.3). Suppose that B is a basis for V, but not a
maximal linearly independent subset of V, and let
Y = {y1, y2, . . . , yk} âŠ†V,
where
k > n
be a maximal linearly independent subsetâ€”recall that (4.3.16) insures the ex-
istence of such a set. The previous argument shows that Y must be a basis
for V. But this is impossible because we already know that a basis must be a
minimal spanning set, and B is a spanning set containing fewer vectors than Y.
Therefore, B must be a maximal linearly independent subset of V.
Although a space V can have many diï¬€erent bases, the preceding result
guarantees that all bases for V contain the same number of vectors. If B1 and
B2 are each a basis for V, then each is a minimal spanning set, and thus they
must contain the same number of vectors. As we are about to see, this number
is quite important.
Dimension
The dimension of a vector space V is deï¬ned to be
dim V = number of vectors in any basis for V
= number of vectors in any minimal spanning set for V
= number of vectors in any maximal independent subset of V.

4.4 Basis and Dimension
197
Example 4.4.2
â€¢
If Z = {0} is the trivial subspace, then dim Z = 0 because the basis for
this space is the empty set.
â€¢
If L is a line through the origin in â„œ3, then dim L = 1 because a basis for
L consists of any nonzero vector lying along L.
â€¢
If P is a plane through the origin in â„œ3, then dim P = 2 because a minimal
spanning set for P must contain two vectors from P.
â€¢
dim â„œ3 = 3 because the three unit vectors
 1
0
0

,
 0
1
0

,
 0
0
1

constitute
a basis for â„œ3.
â€¢
dim â„œn = n because the unit vectors {e1, e2, . . . , en} in â„œn form a basis.
Example 4.4.3
Problem:
If V is an n -dimensional space, explain why every independent
subset S = {v1, v2, . . . , vn} âŠ‚V containing n vectors must be a basis for V.
Solution: dim V = n means that every subset of V that contains more than n
vectors must be linearly dependent. Consequently, S is a maximal independent
subset of V, and hence S is a basis for V.
Example 4.4.2 shows that in a loose sense the dimension of a space is a
measure of the amount of â€œstuï¬€â€ in the spaceâ€”a plane P in â„œ3 has more
â€œstuï¬€â€ in it than a line L, but P contains less â€œstuï¬€â€ than the entire space
â„œ3. Recall from the discussion in Â§4.1 that subspaces of â„œn are generalized
versions of ï¬‚at surfaces through the origin. The concept of dimension gives us a
way to distinguish between these â€œï¬‚atâ€ objects according to how much â€œstuï¬€â€
they containâ€”much the same way we distinguish between lines and planes in â„œ3.
Another way to think about dimension is in terms of â€œdegrees of freedom.â€ In
the trivial space Z, there are no degrees of freedomâ€”you can move nowhereâ€”
whereas on a line there is one degree of freedomâ€”length; in a plane there are
two degrees of freedomâ€”length and width; in â„œ3 there are three degrees of
freedomâ€”length, width, and height; etc.
It is important not to confuse the dimension of a vector space V with the
number of components contained in the individual vectors from V. For example,
if P is a plane through the origin in â„œ3, then dim P = 2, but the individual
vectors in P each have three components. Although the dimension of a space V
and the number of components contained in the individual vectors from V need
not be the same, they are nevertheless related. For example, if V is a subspace of
â„œn, then (4.3.16) insures that no linearly independent subset in V can contain
more than n vectors and, consequently, dim V â‰¤n. This observation generalizes
to produce the following theorem.

198
Chapter 4
Vector Spaces
Subspace Dimension
For vector spaces M and N such that M âŠ†N, the following state-
ments are true.
â€¢
dim M â‰¤dim N.
(4.4.5)
â€¢
If dim M = dim N, then M = N.
(4.4.6)
Proof.
Let dim M = m and dim N = n, and use an indirect argument to
prove (4.4.5). If it were the case that m > n, then there would exist a linearly
independent subset of N (namely, a basis for M ) containing more than n vec-
tors. But this is impossible because dim N is the size of a maximal independent
subset of N. Thus m â‰¤n. Now prove (4.4.6). If m = n but M Ì¸= N, then
there exists a vector x such that x âˆˆN but x /âˆˆM. If B is a basis for M,
then x /âˆˆspan (B) , and the extension set E = B âˆª{x} is a linearly independent
subset of N â€”recall (4.3.15). But E contains m + 1 = n + 1 vectors, which is
impossible because dim N = n is the size of a maximal independent subset of
N. Hence M = N.
Letâ€™s now ï¬nd bases and dimensions for the four fundamental subspaces
of an m Ã— n matrix A of rank r, and letâ€™s start with R (A). The entire set
of columns in A spans R (A), but they wonâ€™t form a basis when there are
dependencies among some of the columns. However, the set of basic columns in
A is also a spanning setâ€”recall (4.2.8)â€”and the basic columns always constitute
a linearly independent set because no basic column can be a combination of other
basic columns (otherwise it wouldnâ€™t be basic). So, the set of basic columns is a
basis for R (A), and, since there are r of them, dim R (A) = r = rank (A).
Similarly, the entire set of rows in A spans R

AT 
, but the set of all rows
is not a basis when dependencies exist. Recall from (4.2.7) that if U =
 CrÃ—n
0

is any row echelon form that is row equivalent to A, then the rows of C span
R

AT 
. Since rank (C) = r, (4.3.5) insures that the rows of C are linearly
independent. Consequently, the rows in C are a basis for R

AT 
, and, since
there are r of them, dim R

AT 
= r = rank (A). Older texts referred to
dim R

AT 
as the row rank of A, while dim R (A) was called the column rank
of A, and it was a major task to prove that the row rank always agrees with the
column rank. Notice that this is a consequence of the discussion above where it
was observed that dim R

AT 
= r = dim R (A).
Turning to the nullspaces, letâ€™s ï¬rst examine N

AT 
. We know from
(4.2.12) that if P is a nonsingular matrix such that PA = U is in row echelon
form, then the last m âˆ’r rows in P span N

AT 
. Because the set of rows
in a nonsingular matrix is a linearly independent set, and because any subset

4.4 Basis and Dimension
199
of an independent set is again independentâ€”see (4.3.7) and (4.3.14)â€”it follows
that the last m âˆ’r rows in P are linearly independent, and hence they con-
stitute a basis for N

AT 
. And this implies dim N

AT 
= m âˆ’r (i.e., the
number of rows in A minus the rank of A). Replacing A by AT shows that
dim N

AT T 
= dim N (A) is the number of rows in AT minus rank

AT 
.
But rank

AT 
= rank (A) = r, so dim N (A) = nâˆ’r. We deduced dim N (A)
without exhibiting a speciï¬c basis, but a basis for N (A) is easy to describe.
Recall that the set H containing the hi â€™s appearing in the general solution
(4.2.9) of Ax = 0 spans N (A). Since there are exactly n âˆ’r vectors in H,
and since dim N (A) = n âˆ’r, H is a minimal spanning set, so, by (4.4.2), H
must be a basis for N (A). Below is a summary of facts uncovered above.
Fundamental Subspacesâ€”Dimension and Bases
For an m Ã— n matrix of real numbers such that rank (A) = r,
â€¢
dim R (A) = r,
(4.4.7)
â€¢
dim N (A) = n âˆ’r,
(4.4.8)
â€¢
dim R

AT 
= r,
(4.4.9)
â€¢
dim N

AT 
= m âˆ’r.
(4.4.10)
Let P be a nonsingular matrix such that PA = U is in row echelon
form, and let H be the set of hi â€™s appearing in the general solution
(4.2.9) of Ax = 0.
â€¢
The basic columns of A form a basis for R (A).
(4.4.11)
â€¢
The nonzero rows of U form a basis for R

AT 
.
(4.4.12)
â€¢
The set H is a basis for N (A).
(4.4.13)
â€¢
The last m âˆ’r rows of P form a basis for N

AT 
.
(4.4.14)
For matrices with complex entries, the above statements remain valid
provided that AT is replaced with Aâˆ—.
Statements (4.4.7) and (4.4.8) combine to produce the following theorem.
Rank Plus Nullity Theorem
â€¢
dim R (A) + dim N (A) = n for all m Ã— n matrices.
(4.4.15)

200
Chapter 4
Vector Spaces
In loose terms, this is a kind of conservation lawâ€”it says that as the amount
of â€œstuï¬€â€ in R (A) increases, the amount of â€œstuï¬€â€ in N (A) must decrease,
and vice versa. The phrase rank plus nullity is used because dim R (A) is the
rank of A, and dim N (A) was traditionally known as the nullity of A.
Example 4.4.4
Problem: Determine the dimension as well as a basis for the space spanned by
S =
ï£±
ï£²
ï£³
ï£«
ï£­
1
2
1
ï£¶
ï£¸,
ï£«
ï£­
1
0
2
ï£¶
ï£¸,
ï£«
ï£­
5
6
7
ï£¶
ï£¸
ï£¼
ï£½
ï£¾.
Solution 1: Place the vectors as columns in a matrix A, and reduce
A =
ï£«
ï£­
1
1
5
2
0
6
1
2
7
ï£¶
ï£¸âˆ’â†’EA =
ï£«
ï£­
1
0
3
0
1
2
0
0
0
ï£¶
ï£¸.
Since span (S) = R (A), we have
dim

span (S)

= dim R (A) = rank (A) = 2.
The basic columns B =
 1
2
1

,
 1
0
2

are a basis for R (A) = span (S) .
Other bases are also possible. Examining EA reveals that any two vectors in S
form an independent set, and therefore any pair of vectors from S constitutes
a basis for span (S) .
Solution 2: Place the vectors from S as rows in a matrix B, and reduce B
to row echelon form:
B =
ï£«
ï£­
1
2
1
1
0
2
5
6
7
ï£¶
ï£¸âˆ’â†’U =
ï£«
ï£­
1
2
1
0
âˆ’2
1
0
0
0
ï£¶
ï£¸.
This time we have span (S) = R

BT 
, so that
dim

span (S)

= dim R

BT 
= rank (B) = rank (U) = 2,
and a basis for span (S) = R

BT 
is given by the nonzero rows in U.

4.4 Basis and Dimension
201
Example 4.4.5
Problem:
If Sr = {v1, v2, . . . , vr} is a linearly independent subset of an
n -dimensional space V, where r < n, explain why it must be possible to ï¬nd
extension vectors {vr+1, . . . , vn} from V such that
Sn = {v1, . . . , vr, vr+1, . . . , vn}
is a basis for V.
Solution 1:
r < n means that span (Sr) Ì¸= V, and hence there exists a vector
vr+1 âˆˆV such that vr+1 /âˆˆspan (Sr) . The extension set Sr+1 = Sr âˆª{vr+1} is
an independent subset of V containing r+1 vectorsâ€”recall (4.3.15). Repeating
this process generates independent subsets Sr+2, Sr+3, . . . , and eventually leads
to a maximal independent subset Sn âŠ‚V containing n vectors.
Solution 2:
The ï¬rst solution shows that it is theoretically possible to ï¬nd
extension vectors, but the argument given is not much help in actually computing
them. It is easy to remedy this situation. Let {b1, b2, . . . , bn} be any basis for
V, and place the given vi â€™s along with the bi â€™s as columns in a matrix
A =

v1 | Â· Â· Â· | vr | b1 | Â· Â· Â· | bn

.
Clearly, R (A) = V so that the set of basic columns from A is a basis for V.
Observe that {v1, v2, . . . , vr} are basic columns in A because no one of these is
a combination of preceding ones. Therefore, the remaining n âˆ’r basic columns
must be a subset of {b1, b2, . . . , bn} â€”say they are

bj1, bj2, . . . , bjnâˆ’r

. The
complete set of basic columns from A, and a basis for V, is the set
B =

v1, . . . , vr, bj1, . . . , bjnâˆ’r

.
For example, to extend the independent set
S =
ï£±
ï£´
ï£²
ï£´
ï£³
ï£«
ï£¬
ï£­
1
0
âˆ’1
2
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
0
0
1
âˆ’2
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
to a basis for â„œ4, append the standard basis {e1, e2, e3, e4} to the vectors in
S, and perform the reduction
A =
ï£«
ï£¬
ï£­
1
0
1
0
0
0
0
0
0
1
0
0
âˆ’1
1
0
0
1
0
2
âˆ’2
0
0
0
1
ï£¶
ï£·
ï£¸âˆ’â†’EA =
ï£«
ï£¬
ï£­
1
0
1
0
0
0
0
1
1
0
0
âˆ’1/2
0
0
0
1
0
0
0
0
0
0
1
1/2
ï£¶
ï£·
ï£¸.
This reveals that {Aâˆ—1, Aâˆ—2, Aâˆ—4, Aâˆ—5} are the basic columns in A, and there-
fore
B =
ï£±
ï£´
ï£²
ï£´
ï£³
ï£«
ï£¬
ï£­
1
0
âˆ’1
2
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
0
0
1
âˆ’2
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
0
1
0
0
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
0
0
1
0
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
is a basis for â„œ4 that contains S.

202
Chapter 4
Vector Spaces
Example 4.4.6
Rank and Connectivity. A set of points (or nodes), {N1, N2, . . . , Nm} , to-
gether with a set of paths (or edges), {E1, E2, . . . , En} , between the nodes is
called a graph. A connected graph is one in which there is a sequence of edges
linking any pair of nodes, and a directed graph is one in which each edge has been
assigned a direction. For example, the graph in Figure 4.4.1 is both connected
and directed.
E6
E5
E4
E3
E2
E1
1
2
3
4
Figure 4.4.1
The connectivity of a directed graph is independent of the directions assigned
to the edgesâ€”i.e., changing the direction of an edge doesnâ€™t change the connec-
tivity. (Exercise 4.4.20 presents another type of connectivity in which direction
matters.) On the surface, the concepts of graph connectivity and matrix rank
seem to have little to do with each other, but, in fact, there is a close relationship.
The incidence matrix associated with a directed graph containing m nodes
and n edges is deï¬ned to be the m Ã— n matrix E whose (k, j) -entry is
ekj =
ï£±
ï£²
ï£³
1
if edge Ej is directed toward node Nk.
âˆ’1
if edge Ej is directed away from node Nk.
0
if edge Ej neither begins nor ends at node Nk.
For example, the incidence matrix associated with the graph in Figure 4.4.1 is
E =
ï£«
ï£¬
ï£¬
ï£­
E1
E2
E3
E4
E5
E6
N1
1
âˆ’1
0
0
âˆ’1
0
N2
âˆ’1
0
âˆ’1
1
0
0
N3
0
0
1
0
1
1
N4
0
1
0
âˆ’1
0
âˆ’1
ï£¶
ï£·
ï£·
ï£¸.
(4.4.16)
Each edge in a directed graph is associated with two nodesâ€”the nose and the tail
of the edgeâ€”so each column in E must contain exactly two nonzero entriesâ€”a
(+1) and a (âˆ’1). Consequently, all column sums are zero. In other words, if
eT = ( 1
1
Â· Â· Â·
1 ) , then eT E = 0, so e âˆˆN

ET 
, and
rank (E) = rank

ET 
= m âˆ’dim N

ET 
â‰¤m âˆ’1.
(4.4.17)
This inequality holds regardless of the connectivity of the associated graph, but
marvelously, equality is attained if and only if the graph is connected.

4.4 Basis and Dimension
203
Rank and Connectivity
Let G be a graph containing m nodes. If G is undirected, arbitrarily
assign directions to the edges to make G directed, and let E be the
corresponding incidence matrix.
â€¢
G is connected if and only if rank (E) = m âˆ’1.
(4.4.18)
Proof.
Suppose G is connected. Prove rank (E) = m âˆ’1 by arguing that
dim N

ET 
= 1, and do so by showing e = ( 1
1
Â· Â· Â·
1 )T is a basis N

ET 
.
To see that e spans N

ET 
, consider an arbitrary x âˆˆN

ET 
, and focus on
any two components xi and xk in x along with the corresponding nodes Ni
and Nk in G. Since G is connected, there must exist a subset of r nodes,
{Nj1, Nj2, . . . , Njr} ,
where
i = j1
and
k = jr,
such that there is an edge between Njp and Njp+1 for each p = 1, 2, . . . , r âˆ’1.
Therefore, corresponding to each of the r âˆ’1 pairs

Njp, Njp+1

, there must
exist a column cp in E (not necessarily the pth column) such that components
jp and jp+1 in cp are complementary in the sense that one is (+1) while the
other is (âˆ’1) (all other components are zero). Because xT E = 0, it follows that
xT cp = 0, and hence xjp = xjp+1. But this holds for every p = 1, 2, . . . , r âˆ’1,
so xi = xk for each i and k, and hence x = Î±e for some scalar Î±. Thus {e}
spans N

ET 
. Clearly, {e} is linearly independent, so it is a basis N

ET 
,
and, therefore, dim N

ET 
= 1 or, equivalently, rank (E) = mâˆ’1. Conversely,
suppose rank (E) = mâˆ’1, and prove G is connected with an indirect argument.
If G is not connected, then G is decomposable into two nonempty subgraphs
G1 and G2 in which there are no edges between nodes in G1 and nodes in G2.
This means that the nodes in G can be ordered so as to make E have the form
E =

E1
0
0
E2

,
where E1 and E2 are the incidence matrices for G1 and G2, respectively. If
G1 and G2 contain m1 and m2 nodes, respectively, then (4.4.17) insures that
rank (E)=rank

E1 0
0 E2

=rank (E1)+rank (E1)â‰¤(m1âˆ’1)+(m2âˆ’1)=mâˆ’2.
But this contradicts the hypothesis that rank (E) = m âˆ’1, so the supposition
that G is not connected must be false.

204
Chapter 4
Vector Spaces
Example 4.4.7
An Application to Electrical Circuits. Recall from the discussion on p. 73
that applying Kirchhoï¬€â€™s node rule to an electrical circuit containing m nodes
and n branches produces m homogeneous linear equations in n unknowns (the
branch currents), and Kirchhoï¬€â€™s loop rule provides a nonhomogeneous equation
for each simple loop in the circuit. For example, consider the circuit in Figure
4.4.2 along with its four nodal equations and three loop equationsâ€”this is the
same circuit appearing on p. 73, and the equations are derived there.
1
2
3
4
R1
R6
R5
R4
R3
R2
E4
E3
E2
E1
I2
I4
I1
I5
I6
I3
A
B
C
Node 1:
I1 âˆ’I2 âˆ’I5 = 0
Node 2:
âˆ’I1 âˆ’I3 + I4 = 0
Node 3:
I3 + I5 + I6 = 0
Node 4:
I2 âˆ’I4 âˆ’I6 = 0
Loop A:
I1R1 âˆ’I3R3 + I5R5 = E1 âˆ’E3
Loop B:
I2R2 âˆ’I5R5 + I6R6 = E2
Loop C:
I3R3 + I4R4 âˆ’I6R6 = E3 + E4
Figure 4.4.2
The directed graph and associated incidence matrix E deï¬ned by this circuit
are the same as those appearing in Example 4.4.6 in Figure 4.4.1 and equation
(4.4.16), so itâ€™s apparent that the 4 Ã— 3 homogeneous system of nodal equations
is precisely the system Ex = 0. This observation holds for general circuits. The
goal is to compute the six currents I1, I2, . . . , I6 by selecting six independent
equations from the entire set of node and loop equations. In general, if a circuit
containing m nodes is connected in the graph sense, then (4.4.18) insures that
rank (E) = m âˆ’1, so there are m independent nodal equations. But Example
4.4.6 also shows that 0 = eT E = E1âˆ—+ E2âˆ—+ Â· Â· Â· + Emâˆ—, which means that
any row can be written in terms of the others, and this in turn implies that
every subset of m âˆ’1 rows in E must be independent (see Exercise 4.4.13).
Consequently, when any nodal equation is discarded, the remaining ones are
guaranteed to be independent. To determine an n Ã— n nonsingular system that
has the n branch currents as its unique solution, itâ€™s therefore necessary to ï¬nd
nâˆ’m+1 additional independent equations, and, as shown in Â§2.6, these are the
loop equations. A simple loop in a circuit is now seen to be a connected subgraph
that does not properly contain other connected subgraphs. Physics dictates that
the currents must be uniquely determined, so there must always be n âˆ’m + 1
simple loops, and the combination of these loop equations together with any
subset of m âˆ’1 nodal equations will be a nonsingular n Ã— n system that yields
the branch currents as its unique solution. For example, any three of the nodal
equations in Figure 4.4.2 can be coupled with the three simple loop equations to
produce a 6 Ã— 6 nonsingular system whose solution is the six branch currents.

4.4 Basis and Dimension
205
If X and Y are subspaces of a vector space V, then the sum of X and
Y was deï¬ned in Â§4.1 to be
X + Y = {x + y | x âˆˆX and y âˆˆY},
and it was demonstrated in (4.1.1) that X + Y is again a subspace of V. You
were asked in Exercise
4.1.8 to prove that the intersection X âˆ©Y is also a
subspace of V. We are now in a position to exhibit an important relationship
between dim (X + Y) and dim (X âˆ©Y) .
Dimension of a Sum
If X and Y are subspaces of a vector space V, then
dim (X + Y) = dim X + dim Y âˆ’dim (X âˆ©Y) .
(4.4.19)
Proof.
The strategy is to construct a basis for X + Y and count the number
of vectors it contains. Let S = {z1, z2, . . . , zt} be a basis for X âˆ©Y. Since
S âŠ†X and S âŠ†Y, there must exist extension vectors {x1, x2, . . . , xm} and
{y1, y2, . . . , yn} such that
BX = {z1, . . . , zt, x1, . . . , xm} = a basis for X
and
BY = {z1, . . . , zt, y1, . . . , yn} = a basis for Y.
We know from (4.1.2) that B = BX âˆªBY spans X + Y, and we wish show that
B is linearly independent. If
t

i=1
Î±izi +
m

j=1
Î²jxj +
n

k=1
Î³kyk = 0,
(4.4.20)
then
n

k=1
Î³kyk = âˆ’
ï£«
ï£­
t

i=1
Î±izi +
m

j=1
Î²jxj
ï£¶
ï£¸âˆˆX.
Since it is also true that 
k Î³kyk âˆˆY, we have that 
k Î³kyk âˆˆX âˆ©Y, and
hence there must exist scalars Î´i such that
n

k=1
Î³kyk =
t

i=1
Î´izi
or, equivalently,
n

k=1
Î³kyk âˆ’
t

i=1
Î´izi = 0.

206
Chapter 4
Vector Spaces
Since BY
is an independent set, it follows that all of the Î³k â€™s (as well as all
Î´i â€™s) are zero, and (4.4.20) reduces to t
i=1 Î±izi + m
j=1 Î²jxj = 0. But BX is
also an independent set, so the only way this can hold is for all of the Î±i â€™s as
well as all of the Î²j â€™s to be zero. Therefore, the only possible solution for the
Î± â€™s, Î² â€™s, and Î³ â€™s in the homogeneous equation (4.4.20) is the trivial solution,
and thus B is linearly independent. Since B is an independent spanning set, it
is a basis for X + Y and, consequently,
dim (X + Y) = t+m+n = (t+m)+(t+n)âˆ’t = dim X +dim Yâˆ’dim (X âˆ©Y) .
Example 4.4.8
Problem: Show that rank (A + B) â‰¤rank (A) + rank (B).
Solution: Observe that
R (A + B) âŠ†R (A) + R (B)
because if b âˆˆR (A + B), then there is a vector x such that
b = (A + B)x = Ax + Bx âˆˆR (A) + R (B).
Recall from (4.4.5) that if M and N are vector spaces such that M âŠ†N, then
dim M â‰¤dim N. Use this together with formula (4.4.19) for the dimension of a
sum to conclude that
rank (A + B) = dim R (A + B) â‰¤dim

R (A) + R (B)

= dim R (A) + dim R (B) âˆ’dim

R (A) âˆ©R (B)

â‰¤dim R (A) + dim R (B) = rank (A) + rank (B).
Exercises for section 4.4
4.4.1. Find the dimensions of the four fundamental subspaces associated with
A =
ï£«
ï£­
1
2
2
3
2
4
1
3
3
6
1
4
ï£¶
ï£¸.
4.4.2. Find a basis for each of the four fundamental subspaces associated with
A =
ï£«
ï£­
1
2
0
2
1
3
6
1
9
6
2
4
1
7
5
ï£¶
ï£¸.

4.4 Basis and Dimension
207
4.4.3. Determine the dimension of the space spanned by the set
S =
ï£±
ï£´
ï£²
ï£´
ï£³
ï£«
ï£¬
ï£­
1
2
âˆ’1
3
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
1
0
0
2
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
2
8
âˆ’4
8
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
1
1
1
1
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
3
3
0
6
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
.
4.4.4. Determine the dimensions of each of the following vector spaces:
(a)
The space of polynomials having degree n or less.
(b)
The space â„œmÃ—n of m Ã— n matrices.
(c)
The space of n Ã— n symmetric matrices.
4.4.5. Consider the following matrix and column vector:
A =
ï£«
ï£­
1
2
2
0
5
2
4
3
1
8
3
6
1
5
5
ï£¶
ï£¸
and
v =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’8
1
3
3
0
ï£¶
ï£·
ï£·
ï£·
ï£¸.
Verify that v âˆˆN (A), and then extend {v} to a basis for N (A).
4.4.6. Determine whether or not the set
B =
ï£±
ï£²
ï£³
ï£«
ï£­
2
3
2
ï£¶
ï£¸,
ï£«
ï£­
1
1
âˆ’1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
is a basis for the space spanned by the set
A =
ï£±
ï£²
ï£³
ï£«
ï£­
1
2
3
ï£¶
ï£¸,
ï£«
ï£­
5
8
7
ï£¶
ï£¸,
ï£«
ï£­
3
4
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾.
4.4.7. Construct a 4 Ã— 4 homogeneous system of equations that has no zero
coeï¬ƒcients and three linearly independent solutions.
4.4.8. Let B = {b1, b2, . . . , bn} be a basis for a vector space V. Prove that
each v âˆˆV can be expressed as a linear combination of the bi â€™s
v = Î±1b1 + Î±2b2 + Â· Â· Â· + Î±nbn,
in only one wayâ€”i.e., the coordinates Î±i are unique.

208
Chapter 4
Vector Spaces
4.4.9. For A âˆˆâ„œmÃ—n and a subspace S of â„œnÃ—1, the image
A(S) = {Ax | x âˆˆS}
of S under A is a subspace of â„œmÃ—1 â€”recall Exercise
4.1.9. Prove
that if S âˆ©N (A) = 0, then dim A(S) = dim(S). Hint: Use a basis
{s1, s2, . . . , sk} for S to determine a basis for A(S).
4.4.10. Explain why
rank (A) âˆ’rank (B)
 â‰¤rank (A âˆ’B).
4.4.11. If rank (AmÃ—n) = r and rank (EmÃ—n) = k â‰¤r, explain why
r âˆ’k â‰¤rank (A + E) â‰¤r + k.
In words, this says that a perturbation of rank k can change the rank
by at most k.
4.4.12. Explain why every nonzero subspace V âŠ†â„œn must possess a basis.
4.4.13. Explain why every set of m âˆ’1 rows in the incidence matrix E of a
connected directed graph containing m nodes is linearly independent.
4.4.14. For the incidence matrix E of a directed graph, explain why
"
EET #
ij =

number of edges at node i
when i = j,
âˆ’(number of edges between nodes i and j)
when i Ì¸= j.
4.4.15. If M and N are subsets of a space V, explain why
dim

span (M âˆªN)

= dim

span (M)

+ dim

span (N)

âˆ’dim

span (M) âˆ©span (N)

.
4.4.16. Consider two matrices AmÃ—n and BmÃ—k.
(a)
Explain why
rank (A | B) = rank (A) + rank (B) âˆ’dim

R (A) âˆ©R (B)

.
Hint: Recall Exercise 4.2.9.
(b)
Now explain why
dim N (A | B) = dim N (A)+dim N (B)+dim

R (A)âˆ©R (B)

.
(c)
Determine dim

R (C) âˆ©N (C)

and dim

R (C) + N (C)

for
C =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’1
1
1
âˆ’2
1
âˆ’1
0
3
âˆ’4
2
âˆ’1
0
3
âˆ’5
3
âˆ’1
0
3
âˆ’6
4
âˆ’1
0
3
âˆ’6
4
ï£¶
ï£·
ï£·
ï£·
ï£¸.

4.4 Basis and Dimension
209
4.4.17. Suppose that A is a matrix with m rows such that the system Ax = b
has a unique solution for every b âˆˆâ„œm. Explain why this means that
A must be square and nonsingular.
4.4.18. Let S be the solution set for a consistent system of linear equations
Ax = b.
(a)
If Smax = {s1, s2, . . . , st} is a maximal independent subset of
S, and if p is any particular solution, prove that
span (Smax) = span {p} + N (A).
Hint:
First show that x âˆˆS implies x âˆˆspan (Smax) , and
then demonstrate set inclusion in both directions with the aid
of Exercise 4.2.10.
(b)
If b Ì¸= 0 and rank (AmÃ—n) = r, explain why Ax = b has
n âˆ’r + 1 â€œindependent solutions.â€
4.4.19. Let rank (AmÃ—n) = r, and suppose Ax = b with b Ì¸= 0 is a consistent
system. If H = {h1, h2, . . . , hnâˆ’r} is a basis for N (A), and if p is a
particular solution to Ax = b, show that
Smax = {p, p + h1, p + h2, . . . , p + hnâˆ’r}
is a maximal independent set of solutions.
4.4.20. Strongly Connected Graphs. In Example 4.4.6 we started with a
graph to construct a matrix, but itâ€™s also possible to reverse the situation
by starting with a matrix to build an associated graph. The graph of
AnÃ—n (denoted by G(A)) is deï¬ned to be the directed graph on n
nodes {N1, N2, . . . , Nn} in which there is a directed edge leading from
Ni to Nj if and only if aij Ì¸= 0. The directed graph G(A) is said to
be strongly connected provided that for each pair of nodes (Ni, Nk)
there is a sequence of directed edges leading from Ni to Nk. The matrix
A is said to be reducible if there exists a permutation matrix P such
that PT AP =
 X
Y
0
Z

, where X and Z are both square matrices.
Otherwise, A is said to be irreducible. Prove that G(A) is strongly
connected if and only if A is irreducible. Hint: Prove the contrapositive:
G(A) is not strongly connected if and only if A is reducible.

210
Chapter 4
Vector Spaces
4.5
MORE ABOUT RANK
Since equivalent matrices have the same rank, it follows that if P and Q are
nonsingular matrices such that the product PAQ is deï¬ned, then
rank (A) = rank (PAQ) = rank (PA) = rank (AQ).
In other words, rank is invariant under multiplication by a nonsingular matrix.
However, multiplication by rectangular or singular matrices can alter the rank,
and the following formula shows exactly how much alteration occurs.
Rank of a Product
If A is m Ã— n and B is n Ã— p, then
rank (AB) = rank (B) âˆ’dim N (A) âˆ©R (B).
(4.5.1)
Proof.
Start with a basis S = {x1, x2, . . . , xs} for N (A) âˆ©R (B), and no-
tice N (A) âˆ©R (B) âŠ†R (B). If dim R (B) = s + t, then, as discussed in
Example 4.4.5, there exists an extension set Sext = {z1, z2, . . . , zt} such that
B = {x1, . . . , xs, z1, . . . , zt} is a basis for R (B). The goal is to prove that
dim R (AB) = t, and this is done by showing T = {Az1, Az2, . . . , Azt} is a
basis for R (AB). T
spans R (AB) because if b âˆˆR (AB), then b = ABy
for some y, but By âˆˆR (B) implies By = s
i=1 Î¾ixi + t
i=1 Î·izi, so
b = A
 s

i=1
Î¾ixi +
t

i=1
Î·izi
 
=
s

i=1
Î¾iAxi +
t

i=1
Î·iAzi =
t

i=1
Î·iAzi.
T
is linearly independent because if 0 = t
i=1 Î±iAzi = A t
i=1 Î±izi, then
t
i=1 Î±izi âˆˆN (A) âˆ©R (B), so there are scalars Î²j such that
t

i=1
Î±izi =
s

j=1
Î²jxj
or, equivalently,
t

i=1
Î±izi âˆ’
s

j=1
Î²jxj = 0,
and hence the only solution for the Î±i â€™s and Î²i â€™s is the trivial solution because
B is an independent set. Thus T is a basis for R (AB), so t = dim R (AB) =
rank (AB), and hence
rank (B) = dim R (B) = s + t = dim N (A) âˆ©R (B) + rank (AB).
Itâ€™s sometimes necessary to determine an explicit basis for N (A) âˆ©R (B).
In particular, such a basis is needed to construct the Jordan chains that are
associated with the Jordan form that is discussed on pp. 582 and 594. The
following example outlines a procedure for ï¬nding such a basis.

4.5 More about Rank
211
Basis for an Intersection
If A is m Ã— n and B is n Ã— p, then a basis for N (A) âˆ©R (B) can
be constructed by the following procedure.
â–·
Find a basis {x1, x2, . . . , xr} for R (B).
â–·
Set XnÃ—r =

x1 | x2 | Â· Â· Â· | xr

.
â–·
Find a basis {v1, v2, . . . , vs} for N (AX).
â–·
B = {Xv1, Xv2, . . . , Xvs} is a basis for N (A) âˆ©R (B).
Proof.
The strategy is to argue that B is a maximal linear independent sub-
set of N (A) âˆ©R (B). Since each Xvj belongs to R (X) = R (B), and since
AXvj = 0, itâ€™s clear that B âŠ‚N (A) âˆ©R (B). Let VrÃ—s =

v1 | v2 | Â· Â· Â· | vs

,
and notice that V and X each have full column rank. Consequently, N (X) = 0
so, by (4.5.1),
rank (XV)nÃ—s = rank (V) âˆ’dim N (X) âˆ©R (V) = rank (V) = s,
which insures that B is linearly independent. B is a maximal independent
subset of N (A) âˆ©R (B) because (4.5.1) also guarantees that
s = dim N (AX) = dim N (X) + dim N (A) âˆ©R (X)
(see Exercise 4.5.10)
= dim N (A) âˆ©R (B).
The utility of (4.5.1) is mitigated by the fact that although rank (A) and
rank (B) are frequently known or can be estimated, the term dim N (A)âˆ©R (B)
can be costly to obtain. In such cases (4.5.1) still provides us with useful upper
and lower bounds for rank (AB) that depend only on rank (A) and rank (B).
Bounds on the Rank of a Product
If A is m Ã— n and B is n Ã— p, then
â€¢
rank (AB) â‰¤min {rank (A), rank (B)} ,
(4.5.2)
â€¢
rank (A) + rank (B) âˆ’n â‰¤rank (AB).
(4.5.3)

212
Chapter 4
Vector Spaces
Proof.
In words, (4.5.2) says that the rank of a product cannot exceed the rank
of either factor. To prove rank (AB) â‰¤rank (B), use (4.5.1) and write
rank (AB) = rank (B) âˆ’dim N (A) âˆ©R (B) â‰¤rank (B).
This says that the rank of a product cannot exceed the rank of the right-hand
factor. To show that rank (AB) â‰¤rank (A), remember that transposition does
not alter rank, and use the reverse order law for transposes together with the
previous statement to write
rank (AB) = rank (AB)T = rank

BT AT 
â‰¤rank

AT 
= rank (A).
To prove (4.5.3), notice that N (A)âˆ©R (B) âŠ†N (A), and recall from (4.4.5) that
if M and N are spaces such that M âŠ†N, then dim M â‰¤dim N. Therefore,
dim N (A) âˆ©R (B) â‰¤dim N (A) = n âˆ’rank (A),
and the lower bound on rank (AB) is obtained from (4.5.1) by writing
rank (AB) = rank (B) âˆ’dim N (A) âˆ©R (B) â‰¥rank (B) + rank (A) âˆ’n.
The products AT A and AAT and their complex counterparts Aâˆ—A and
AAâˆ—deserve special attention because they naturally appear in a wide variety
of applications.
Products AT A and AAT
For A âˆˆâ„œmÃ—n, the following statements are true.
â€¢
rank

AT A

= rank (A) = rank

AAT 
.
(4.5.4)
â€¢
R

AT A

= R

AT 
and
R

AAT 
= R (A).
(4.5.5)
â€¢
N

AT A

= N (A)
and
N

AAT 
= N

AT 
.
(4.5.6)
For A âˆˆCmÃ—n, the transpose operation (â‹†)T must be replaced by the
conjugate transpose operation (â‹†)âˆ—.

4.5 More about Rank
213
Proof.
First observe that N

AT 
âˆ©R (A) = {0} because
x âˆˆN

AT 
âˆ©R (A)
=â‡’
AT x = 0 and x = Ay for some y
=â‡’
xT x = yT AT x = 0
=â‡’

x2
i = 0
=â‡’
x = 0.
Formula (4.5.1) for the rank of a product now guarantees that
rank

AT A

= rank (A) âˆ’dim N

AT 
âˆ©R (A) = rank (A),
which is half of (4.5.4)â€”the other half is obtained by reversing the roles of A
and AT . To prove (4.5.5) and (4.5.6), use the facts R (AB) âŠ†R (A) and
N (B) âŠ†N (AB) (see Exercise
4.2.12) to write R

AT A

âŠ†R

AT 
and
N (A) âŠ†N

AT A

. The ï¬rst half of (4.5.5) and (4.5.6) now follows because
dim R

AT A

= rank

AT A

= rank (A) = rank

AT 
= dim R

AT 
,
dim N (A) = n âˆ’rank (A) = n âˆ’rank

AT A

= dim N

AT A

.
Reverse the roles of A and AT to get the second half of (4.5.5) and (4.5.6).
To see why (4.5.4)â€”(4.5.6) might be important, consider an m Ã— n system
of equations Ax = b that may or may not be consistent. Multiplying on the
left-hand side by AT produces the n Ã— n system
AT Ax = AT b
called the associated system of normal equations, which has some ex-
tremely interesting properties. First, notice that the normal equations are always
consistent, regardless of whether or not the original system is consistent because
(4.5.5) guarantees that AT b âˆˆR

AT 
= R

AT A

(i.e., the right-hand side is
in the range of the coeï¬ƒcient matrix), so (4.2.3) insures consistency. However, if
Ax = b happens to be consistent, then Ax = b and AT Ax = AT b have the
same solution set because if p is a particular solution of the original system,
then Ap = b implies AT Ap = AT b (i.e., p is also a particular solution of
the normal equations), so the general solution of Ax = b is S = p + N (A),
and the general solution of AT Ax = AT b is
p + N

AT A

= p + N (A) = S.
Furthermore, if Ax = b is consistent and has a unique solution, then the same
is true for AT Ax = AT b, and the unique solution common to both systems is
x =

AT A
âˆ’1AT b.
(4.5.7)

214
Chapter 4
Vector Spaces
This follows because a unique solution (to either system) exists if and only if
0 = N (A) = N

AT A

, and this insures (AT A)nÃ—n must be nonsingular (by
(4.2.11)), so (4.5.7) is the unique solution to both systems. Caution!
When
A is not square, Aâˆ’1 does not exist, and the reverse order law for inversion
doesnâ€™t apply to

AT A
âˆ’1, so (4.5.7) cannot be further simpliï¬ed.
There is one outstanding questionâ€”what do the solutions of the normal
equations AT Ax = AT b represent when the original system Ax = b is not
consistent? The answer, which is of fundamental importance, will have to wait
until Â§4.6, but letâ€™s summarize what has been said so far.
Normal Equations
â€¢
For an m Ã— n system Ax = b, the associated system of normal
equations is deï¬ned to be the n Ã— n system AT Ax = AT b.
â€¢
AT Ax = AT b is always consistent, even when Ax = b is not
consistent.
â€¢
When Ax = b is consistent, its solution set agrees with that of
AT Ax = AT b. As discussed in Â§4.6, the normal equations provide
least squares solutions to Ax = b when Ax = b is inconsistent.
â€¢
AT Ax = AT b has a unique solution if and only if rank (A) = n,
in which case the unique solution is x =

AT A
âˆ’1AT b.
â€¢
When Ax = b is consistent and has a unique solution, then the
same is true for AT Ax = AT b, and the unique solution to both
systems is given by x =

AT A
âˆ’1AT b.
Example 4.5.1
Caution!
Use of the product AT A or the normal equations is not recom-
mended for numerical computation. Any sensitivity to small perturbations that
is present in the underlying matrix A is magniï¬ed by forming the product
AT A. In other words, if Ax = b is somewhat ill-conditioned, then the asso-
ciated system of normal equations AT Ax = AT b will be ill-conditioned to an
even greater extent, and the theoretical properties surrounding AT A and the
normal equations may be lost in practical applications. For example, consider
the nonsingular system Ax = b, where
A =

3
6
1
2.01

and
b =

9
3.01

.
If Gaussian elimination with 3-digit ï¬‚oating-point arithmetic is used to solve
Ax = b, then the 3-digit solution is (1, 1), and this agrees with the exact

4.5 More about Rank
215
solution. However if 3-digit arithmetic is used to form the associated system of
normal equations, the result is

10
20
20
40
 
x1
x2

=

30
60.1

.
The 3-digit representation of AT A is singular, and the associated system of
normal equations is inconsistent. For these reasons, the normal equations are
often avoided in numerical computations. Nevertheless, the normal equations
are an important theoretical idea that leads to practical tools of fundamental
importance such as the method of least squares developed in Â§4.6 and Â§5.13.
Because the concept of rank is at the heart of our subject, itâ€™s important to
understand rank from a variety of diï¬€erent viewpoints. The statement below is
one more way to think about rank.
29
Rank and the Largest Nonsingular Submatrix
The rank of a matrix AmÃ—n is precisely the order of a maximal square
nonsingular submatrix of A. In other words, to say rank (A) = r
means that there is at least one r Ã— r nonsingular submatrix in A,
and there are no nonsingular submatrices of larger order.
Proof.
First demonstrate that there exists an r Ã— r nonsingular submatrix in
A, and then show there can be no nonsingular submatrix of larger order. Begin
with the fact that there must be a maximal linearly independent set of r rows
in A as well as a maximal independent set of r columns, and prove that the
submatrix MrÃ—r lying on the intersection of these r rows and r columns is
nonsingular. The r independent rows can be permuted to the top, and the
remaining rows can be annihilated using row operations, so
A
row
âˆ¼

UrÃ—n
0

.
Now permute the r independent columns containing M to the left-hand side,
and use column operations to annihilate the remaining columns to conclude that
A
row
âˆ¼

UrÃ—n
0

col
âˆ¼

MrÃ—r
N
0
0

col
âˆ¼

MrÃ—r
0
0
0

.
29
This is the last characterization of rank presented in this text, but historically this was the
essence of the ï¬rst deï¬nition (p. 44) of rank given by Georg Frobenius (p. 662) in 1879.

216
Chapter 4
Vector Spaces
Rank isnâ€™t changed by row or column operations, so r = rank (A) = rank (M),
and thus M is nonsingular. Now suppose that W is any other nonsingu-
lar submatrix of A, and let P and Q be permutation matrices such that
PAQ =
 W
X
Y
Z

. If
E =

I
0
âˆ’YWâˆ’1
I

,
F =

I
âˆ’Wâˆ’1X
0
I

,
and
S = Z âˆ’YWâˆ’1X,
then
EPAQF =

W
0
0
S

=â‡’
A âˆ¼

W
0
0
S

,
(4.5.8)
and hence r = rank (A) = rank (W) + rank (S) â‰¥rank (W) (recall Example
3.9.3). This guarantees that no nonsingular submatrix of A can have order
greater than r = rank (A).
Example 4.5.2
Problem: Determine the rank of A =
 1
2
1
2
4
1
3
6
1

.
Solution: rank (A) = 2 because there is at least one 2 Ã— 2 nonsingular sub-
matrix (e.g., there is one lying on the intersection of rows 1 and 2 with columns
2 and 3), and there is no larger nonsingular submatrix (the entire matrix is sin-
gular). Notice that not all 2 Ã— 2 matrices are nonsingular (e.g., consider the one
lying on the intersection of rows 1 and 2 with columns 1 and 2).
Earlier in this section we saw that it is impossible to increase the rank by
means of matrix multiplicationâ€”i.e., (4.5.2) says rank (AE) â‰¤rank (A). In
a certain sense there is a dual statement for matrix addition that says that it
is impossible to decrease the rank by means of a â€œsmallâ€ matrix additionâ€”i.e.,
rank (A + E) â‰¥rank (A) whenever E has entries of small magnitude.
Small Perturbations Canâ€™t Reduce Rank
If A and E are m Ã— n matrices such that E has entries of suï¬ƒciently
small magnitude, then
rank (A + E) â‰¥rank (A).
(4.5.9)
The term â€œsuï¬ƒciently smallâ€ is further clariï¬ed in Exercise 5.12.4.

4.5 More about Rank
217
Proof.
Suppose rank (A) = r, and let P and Q be nonsingular matrices
that reduce A to rank normal formâ€”i.e., PAQ =
 Ir
0
0
0

. If P and Q are
applied to E to form PEQ =
 E11
E12
E21
E22

, where E11 is r Ã— r, then
P(A + E)Q =

Ir + E11
E12
E21
E22

.
(4.5.10)
If the magnitude of the entries in E are small enough to insure that Ek
11 â†’0
as k â†’âˆ, then the discussion of the Neumann series on p. 126 insures that
I + E11 is nonsingular. (Exercise 4.5.14 gives another condition on the size of
E11 to insure this.) This allows the right-hand side of (4.5.10) to be further
reduced by writing

I
0
âˆ’E21(I + E11)âˆ’1 I

I + E11 E12
E21
E22

I
âˆ’(I + E11)âˆ’1E12
0
I

=

I âˆ’E11 0
0
S

,
where S = E22 âˆ’E21 (I + E11)âˆ’1 E12. In other words,
A + E âˆ¼

I âˆ’E11
0
0
S

,
and therefore
rank (A + E) = rank (Ir + E11) + rank (S)
(recall Example 3.9.3)
= rank (A) + rank (S)
â‰¥rank (A).
(4.5.11)
Example 4.5.3
A Pitfall in Solving Singular Systems.
Solving Ax = b with ï¬‚oating-
point arithmetic produces the exact solution of a perturbed system whose coeï¬ƒ-
cient matrix is A+E. If A is nonsingular, and if we are using a stable algorithm
(an algorithm that insures that the entries in E have small magnitudes), then
(4.5.9) guarantees that we are ï¬nding the exact solution to a nearby system that
is also nonsingular. On the other hand, if A is singular, then perturbations of
even the slightest magnitude can increase the rank, thereby producing a system
with fewer free variables than the original system theoretically demands, so even
a stable algorithm can result in a signiï¬cant loss of information. But what are
the chances that this will actually occur in practice? To answer this, recall from
(4.5.11) that
rank (A + E) = rank (A) + rank (S),
where
S = E22 âˆ’E21 (I + E11)âˆ’1 E12.

218
Chapter 4
Vector Spaces
If the rank is not to jump, then the perturbation E must be such that S = 0,
which is equivalent to saying E22 = E21 (I + E11)âˆ’1 E12. Clearly, this requires
the existence of a very speciï¬c (and quite special) relationship among the entries
of E, and a random perturbation will almost never produce such a relation-
ship. Although rounding errors cannot be considered to be truly random, they
are random enough so as to make the possibility that S = 0 very unlikely.
Consequently, when A is singular, the small perturbation E due to roundoï¬€
makes the possibility that rank (A + E) > rank (A) very likely. The moral is
to avoid ï¬‚oating-point solutions of singular systems. Singular problems can often
be distilled down to a nonsingular core or to nonsingular pieces, and these are
the components you should be dealing with.
Since no more signiï¬cant characterizations of rank will be given, it is ap-
propriate to conclude this section with a summary of all of the diï¬€erent ways we
have developed to say â€œrank.â€
Summary of Rank
For A âˆˆâ„œmÃ—n, each of the following statements is true.
â€¢
rank (A) = The number of nonzero rows in any row echelon form
that is row equivalent to A.
â€¢
rank (A) = The number of pivots obtained in reducing A to a row
echelon form with row operations.
â€¢
rank (A) = The number of basic columns in A (as well as the num-
ber of basic columns in any matrix that is row equivalent
to A ).
â€¢
rank (A) = The number of independent columns in A â€”i.e., the size
of a maximal independent set of columns from A.
â€¢
rank (A) = The number of independent rows in A â€”i.e., the size of
a maximal independent set of rows from A.
â€¢
rank (A) = dim R (A).
â€¢
rank (A) = dim R

AT 
.
â€¢
rank (A) = n âˆ’dim N (A).
â€¢
rank (A) = m âˆ’dim N

AT 
.
â€¢
rank (A) = The size of the largest nonsingular submatrix in A.
For A âˆˆCmÃ—n, replace (â‹†)T with (â‹†)âˆ—.

4.5 More about Rank
219
Exercises for section 4.5
4.5.1. Verify that rank

AT A

= rank (A) = rank

AAT 
for
A =
ï£«
ï£­
1
3
1
âˆ’4
âˆ’1
âˆ’3
1
0
2
6
2
âˆ’8
ï£¶
ï£¸.
4.5.2. Determine dim N (A) âˆ©R (B) for
A =
ï£«
ï£­
âˆ’2
1
1
âˆ’4
2
2
0
0
0
ï£¶
ï£¸
and
B =
ï£«
ï£­
1
3
1
âˆ’4
âˆ’1
âˆ’3
1
0
2
6
2
âˆ’8
ï£¶
ï£¸.
4.5.3. For the matrices given in Exercise
4.5.2, use the procedure described
on p. 211 to determine a basis for N (A) âˆ©R (B).
4.5.4. If A1A2 Â· Â· Â· Ak is a product of square matrices such that some Ai is
singular, explain why the entire product must be singular.
4.5.5. For A âˆˆâ„œmÃ—n, explain why AT A = 0 implies A = 0.
4.5.6. Find rank (A) and all nonsingular submatrices of maximal order in
A =
ï£«
ï£­
2
âˆ’1
1
4
âˆ’2
1
8
âˆ’4
1
ï£¶
ï£¸.
4.5.7. Is it possible that rank (AB) < rank (A) and rank (AB) < rank (B)
for the same pair of matrices?
4.5.8. Is rank (AB) = rank (BA) when both products are deï¬ned? Why?
4.5.9. Explain why rank (AB) = rank (A) âˆ’dim N

BT 
âˆ©R

AT 
.
4.5.10. Explain why dim N (AmÃ—nBnÃ—p) = dim N (B) + dim R (B) âˆ©N (A).

220
Chapter 4
Vector Spaces
4.5.11. Sylvesterâ€™s law of nullity, given by James J. Sylvester in 1884, states
that for square matrices A and B,
max {Î½(A), Î½(B)} â‰¤Î½(AB) â‰¤Î½(A) + Î½(B),
where Î½(â‹†) = dim N (â‹†) denotes the nullity.
(a)
Establish the validity of Sylvesterâ€™s law.
(b)
Show Sylvesterâ€™s law is not valid for rectangular matrices be-
cause Î½(A) > Î½(AB) is possible. Is Î½(B) > Î½(AB) possible?
4.5.12. For matrices AmÃ—n and BnÃ—p, prove each of the following statements:
(a) rank (AB) = rank (A) and R (AB) = R (A) if rank (B) = n.
(b) rank (AB) = rank (B) and N (AB) = N (B) if rank (A) = n.
4.5.13. Perform the following calculations using the matrices:
A =
ï£«
ï£­
1
2
2
4
1
2.01
ï£¶
ï£¸
and
b =
ï£«
ï£­
1
2
1.01
ï£¶
ï£¸.
(a)
Find rank (A), and solve Ax = b using exact arithmetic.
(b)
Find rank

AT A

, and solve AT Ax=AT b exactly.
(c)
Find rank (A), and solve Ax = b with 3-digit arithmetic.
(d)
Find AT A,
AT b, and the solution of AT Ax = AT b with
3-digit arithmetic.
4.5.14. Prove that if the entries of FrÃ—r satisfy r
j=1 |fij| < 1 for each i (i.e.,
each absolute row sum < 1), then I + F is nonsingular. Hint: Use the
triangle inequality for scalars |Î±+Î²| â‰¤|Î±|+|Î²| to show N (I + F) = 0.
4.5.15. If A =
 W
X
Y
Z

, where rank (A) = r = rank (WrÃ—r), show that
there are matrices B and C such that
A =

W
WC
BW
BWC

=

I
B

W

I | C

.
4.5.16. For a convergent sequence {Ak}âˆ
k=1 of matrices, let A = limkâ†’âˆAk.
(a)
Prove that if each Ak is singular, then A is singular.
(b)
If each Ak is nonsingular, must A be nonsingular? Why?

4.5 More about Rank
221
4.5.17. The Frobenius Inequality. Establish the validity of Frobeniusâ€™s 1911
result that states that if ABC exists, then
rank (AB) + rank (BC) â‰¤rank (B) + rank (ABC).
Hint: If M = R (BC)âˆ©N (A) and N = R (B)âˆ©N (A), then M âŠ†N.
4.5.18. If A is n Ã— n, prove that the following statements are equivalent:
(a)
N (A) = N

A2
.
(b)
R (A) = R

A2
.
(c)
R (A) âˆ©N (A) = {0}.
4.5.19. Let A and B be n Ã— n matrices such that A = A2, B = B2, and
AB = BA = 0.
(a)
Prove that rank (A + B) = rank (A) + rank (B). Hint: Con-
sider
 A
B

(A + B)(A | B).
(b)
Prove that rank (A) + rank (I âˆ’A) = n.
4.5.20. Mooreâ€“Penrose Inverse. For A âˆˆâ„œmÃ—n such that rank (A) = r,
let A = BC be the full rank factorization of A in which BmÃ—r is the
matrix of basic columns from A and CrÃ—n is the matrix of nonzero
rows from EA (see Exercise 3.9.8). The matrix deï¬ned by
Aâ€  = CT 
BT ACT âˆ’1 BT
is called the Mooreâ€“Penrose
30 inverse of A. Some authors refer to
Aâ€  as the pseudoinverse or the generalized inverse of A. A more elegant
treatment is given on p. 423, but itâ€™s worthwhile to introduce the idea
here so that it can be used and viewed from diï¬€erent perspectives.
(a)
Explain why the matrix BT ACT is nonsingular.
(b)
Verify that x = Aâ€ b solves the normal equations AT Ax = AT b (as
well as Ax = b when it is consistent).
(c)
Show that the general solution for AT Ax = AT b (as well as Ax = b
when it is consistent) can be described as
x = Aâ€ b +

I âˆ’Aâ€ A

h,
30
This is in honor of Eliakim H. Moore (1862â€“1932) and Roger Penrose (a famous contemporary
English mathematical physicist). Each formulated a concept of generalized matrix inversionâ€”
Mooreâ€™s work was published in 1922, and Penroseâ€™s work appeared in 1955. E. H. Moore is
considered by many to be Americaâ€™s ï¬rst great mathematician.

222
Chapter 4
Vector Spaces
where h is a â€œfree variableâ€ vector in â„œnÃ—1.
Hint: Verify AAâ€ A = A, and then show R

I âˆ’Aâ€ A

= N (A).
(d)
If rank (A) = n, explain why Aâ€  =

AT A
âˆ’1AT .
(e)
If A is square and nonsingular, explain why Aâ€  = Aâˆ’1.
(f)
Verify that Aâ€  = CT 
BT ACT âˆ’1 BT satisï¬es the Penrose equations:
AAâ€ A = A,

AAâ€ T = AAâ€ ,
Aâ€ AAâ€  = Aâ€ ,

Aâ€ A
T = Aâ€ A.
Penrose originally deï¬ned Aâ€  to be the unique solution to these four
equations.

4.6 Classical Least Squares
223
4.6
CLASSICAL LEAST SQUARES
The following problem arises in almost all areas where mathematics is applied.
At discrete points ti (often points in time), observations bi of some phenomenon
are made, and the results are recorded as a set of ordered pairs
D = {(t1, b1),
(t2, b2), . . . , (tm, bm)} .
On the basis of these observations, the problem is to make estimations or predic-
tions at points (times) Ë†t that are between or beyond the observation points ti.
A standard approach is to ï¬nd the equation of a curve y = f(t) that closely ï¬ts
the points in D so that the phenomenon can be estimated at any nonobservation
point Ë†t with the value Ë†y = f(Ë†t).
Letâ€™s begin by ï¬tting a straight line to the points in D. Once this is under-
stood, it will be relatively easy to see how to ï¬t the data with curved lines.
f (t)= Î± + Î² t
(t1,b1)

t1,f (t1)

(t2,b2)

t2,f (t2)

(tm ,bm )

tm ,f (tm )

t
b
Îµ1
Îµ2
Îµm
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
Figure 4.6.1
The strategy is to determine the coeï¬ƒcients Î± and Î² in the equation of the
line f(t) = Î± + Î²t that best ï¬ts the points (ti, bi) in the sense that the sum
of the squares of the vertical
31 errors Îµ1, Îµ2, . . . , Îµm indicated in Figure 4.6.1 is
31
We consider only vertical errors because there is a tacit assumption that only the observations
bi are subject to error or variation. The ti â€™s are assumed to be errorless constantsâ€”think of
them as being exact points in time (as they often are). If the ti â€™s are also subject to variation,
then horizontal as well as vertical errors have to be considered in Figure 4.6.1, and a more
complicated theory known as total least squares (not considered in this text) emerges. The
least squares line L obtained by minimizing only vertical deviations will not be the closest
line to points in D in terms of perpendicular distance, but L is the best line for the purpose
of linear estimationâ€”see Â§5.14 (p. 446).

224
Chapter 4
Vector Spaces
minimal. The distance from (ti, bi) to a line f(t) = Î± + Î²t is
Îµi = |f(ti) âˆ’bi| = |Î± + Î²ti âˆ’bi|,
so that the objective is to ï¬nd values for Î± and Î² such that
m

i=1
Îµ2
i =
m

i=1
(Î± + Î²ti âˆ’bi)2
is minimal.
Minimization techniques from calculus tell us that the minimum value must
occur at a solution to the two equations
0 =
âˆ‚
m
i=1 (Î± + Î²ti âˆ’bi)2
âˆ‚Î±
= 2
m

i=1
(Î± + Î²ti âˆ’bi) ,
0 =
âˆ‚
m
i=1 (Î± + Î²ti âˆ’bi)2
âˆ‚Î²
= 2
m

i=1
(Î± + Î²ti âˆ’bi) ti.
Rearranging terms produces two equations in the two unknowns Î± and Î²
 m

i=1
1
 
Î± +
 m

i=1
ti
 
Î² =
m

i=1
bi,
 m

i=1
ti
 
Î± +
 m

i=1
t2
i
 
Î² =
m

i=1
tibi.
(4.6.1)
By setting
A =
ï£«
ï£¬
ï£¬
ï£­
1
t1
1
t2
...
...
1
tm
ï£¶
ï£·
ï£·
ï£¸,
b =
ï£«
ï£¬
ï£¬
ï£­
b1
b2
...
bm
ï£¶
ï£·
ï£·
ï£¸,
and
x =

Î±
Î²

,
we see that the two equations (4.6.1) have the matrix form AT Ax = AT b.
In other words, (4.6.1) is the system of normal equations associated with the
system Ax = b (see p. 213). The ti â€™s are assumed to be distinct numbers,
so rank (A) = 2, and (4.5.7) insures that the normal equations have a unique
solution given by
x =

AT A
âˆ’1AT b
=
1
m  t2
i âˆ’( ti)2

 t2
i
âˆ’ ti
âˆ’ ti
m
   bi
 tibi

=
1
m  t2
i âˆ’( ti)2
  t2
i
 bi âˆ’ ti
 tibi
m  tibi âˆ’ ti
 bi

=

Î±
Î²

.
Finally, notice that the total sum of squares of the errors is given by
m

i=1
Îµ2
i =
m

i=1
(Î± + Î²ti âˆ’bi)2 = (Ax âˆ’b)T (Ax âˆ’b).

4.6 Classical Least Squares
225
Example 4.6.1
Problem: A small company has been in business for four years and has recorded
annual sales (in tens of thousands of dollars) as follows.
Year
1
2
3
4
Sales
23
27
30
34
When this data is plotted as shown in Figure 4.6.2, we see that although the
points do not exactly lie on a straight line, there nevertheless appears to be a
linear trend. Predict the sales for any future year if this trend continues.
0
22
23
24
25
26
27
28
29
30
31
32
33
34
4
3
2
1
Year
Sales
Figure 4.6.2
Solution: Determine the line f(t) = Î± + Î²t that best ï¬ts the data in the sense
of least squares. If
A =
ï£«
ï£¬
ï£­
1
1
1
2
1
3
1
4
ï£¶
ï£·
ï£¸,
b =
ï£«
ï£¬
ï£­
23
27
30
34
ï£¶
ï£·
ï£¸,
and
x =

Î±
Î²

,
then the previous discussion guarantees that x is the solution of the normal
equations AT Ax = AT b. That is,

4
10
10
30
 
Î±
Î²

=

114
303

.
The solution is easily found to be Î± = 19.5 and Î² = 3.6, so we predict that the
sales in year t will be f(t) = 19.5 + 3.6t. For example, the estimated sales for
year ï¬ve is $375,000. To get a feel for how close the least squares line comes to

226
Chapter 4
Vector Spaces
passing through the data points, let Îµ = Ax âˆ’b, and compute the sum of the
squares of the errors to be
m

i=1
Îµ2
i = ÎµT Îµ = (Ax âˆ’b)T (Ax âˆ’b) = .2.
General Least Squares Problem
For A âˆˆâ„œmÃ—n and b âˆˆâ„œm, let Îµ = Îµ(x) = Ax âˆ’b. The general
least squares problem is to ï¬nd a vector x that minimizes the quantity
m

i=1
Îµ2
i = ÎµT Îµ = (Ax âˆ’b)T (Ax âˆ’b).
Any vector that provides a minimum value for this expression is called
a least squares solution.
â€¢
The set of all least squares solutions is precisely the set of solutions
to the system of normal equations AT Ax = AT b.
â€¢
There is a unique least squares solution if and only if rank (A) = n,
in which case it is given by x =

AT A
âˆ’1AT b.
â€¢
If Ax = b is consistent, then the solution set for Ax = b is the
same as the set of least squares solutions.
Proof.
32 First prove that if x minimizes ÎµT Îµ, then x must satisfy the normal
equations. Begin by using xT AT b = bT Ax (scalars are symmetric) to write
m

i=1
Îµ2
i = ÎµT Îµ = (Ax âˆ’b)T (Ax âˆ’b) = xT AT Ax âˆ’2xT AT b + bT b.
(4.6.2)
To determine vectors x that minimize the expression (4.6.2), we will again use
minimization techniques from calculus and diï¬€erentiate the function
f(x1, x2, . . . , xn) = xT AT Ax âˆ’2xT AT b + bT b
(4.6.3)
with respect to each xi. Diï¬€erentiating matrix functions is similar to diï¬€er-
entiating scalar functions (see Exercise 3.5.9) in the sense that if U = [uij],
then
$âˆ‚U
âˆ‚x
%
ij
= âˆ‚uij
âˆ‚x ,
âˆ‚[U + V]
âˆ‚x
= âˆ‚U
âˆ‚x + âˆ‚V
âˆ‚x ,
and
âˆ‚[UV]
âˆ‚x
= âˆ‚U
âˆ‚x V + Uâˆ‚V
âˆ‚x .
32
A more modern development not relying on calculus is given in Â§5.13 on p. 437, but the more
traditional approach is given here because itâ€™s worthwhile to view least squares from both
perspectives.

4.6 Classical Least Squares
227
Applying these rules to the function in (4.6.3) produces
âˆ‚f
âˆ‚xi
= âˆ‚xT
âˆ‚xi
AT Ax + xT AT A âˆ‚x
âˆ‚xi
âˆ’2âˆ‚xT
âˆ‚xi
AT b.
Since âˆ‚x/âˆ‚xi = ei (the ith unit vector), we have
âˆ‚f
âˆ‚xi
= eT
i AT Ax + xT AT Aei âˆ’2eT
i AT b = 2eT
i AT Ax âˆ’2eT
i AT b.
Using eT
i AT =

AT 
iâˆ—and setting âˆ‚f/âˆ‚xi = 0 produces the n equations

AT 
iâˆ—Ax =

AT 
iâˆ—b
for i = 1, 2, . . . , n,
which can be written as the single matrix equation AT Ax = AT b. Calculus
guarantees that the minimum value of f occurs at some solution of this system.
But this is not enoughâ€”we want to know that every solution of AT Ax = AT b
is a least squares solution. So we must show that the function f in (4.6.3) attains
its minimum value at each solution to AT Ax = AT b. Observe that if z is a
solution to the normal equations, then f(z) = bT b âˆ’zT AT b. For any other
y âˆˆâ„œnÃ—1, let u = y âˆ’z, so y = z + u, and observe that
f(y) = f(z) + vT v,
where
v = Au.
Since vT v = 
i v2
i â‰¥0, it follows that f(z) â‰¤f(y) for all y âˆˆâ„œnÃ—1, and
thus f attains its minimum value at each solution of the normal equations. The
remaining statements in the theorem follow from the properties established on
p. 213.
The classical least squares problem discussed at the beginning of this sec-
tion and illustrated in Example 4.6.1 is part of a broader topic known as linear
regression, which is the study of situations where attempts are made to express
one variable y as a linear combination of other variables t1, t2, . . . , tn. In prac-
tice, hypothesizing that y is linearly related to t1, t2, . . . , tn means that one
assumes the existence of a set of constants {Î±0, Î±1, . . . , Î±n} (called parameters)
such that
y = Î±0 + Î±1t1 + Î±2t2 + Â· Â· Â· + Î±ntn + Îµ,
where Îµ is a â€œrandom functionâ€ whose values â€œaverage outâ€ to zero in some
sense. Practical problems almost always involve more variables than we wish to
consider, but it is frequently fair to assume that the eï¬€ect of variables of lesser
signiï¬cance will indeed â€œaverage outâ€ to zero. The random function Îµ accounts
for this assumption. In other words, a linear hypothesis is the supposition that
the expected (or mean) value of y at each point where the phenomenon can be
observed is given by a linear equation
E(y) = Î±0 + Î±1t1 + Î±2t2 + Â· Â· Â· + Î±ntn.

228
Chapter 4
Vector Spaces
To help seat these ideas, consider the problem of predicting the amount of
weight that a pint of ice cream loses when it is stored at very low temperatures.
There are many factors that may contribute to weight lossâ€”e.g., storage tem-
perature, storage time, humidity, atmospheric pressure, butterfat content, the
amount of corn syrup, the amounts of various gums (guar gum, carob bean gum,
locust bean gum, cellulose gum), and the never-ending list of other additives and
preservatives. It is reasonable to believe that storage time and temperature are
the primary factors, so to predict weight loss we will make a linear hypothesis of
the form
y = Î±0 + Î±1t1 + Î±2t2 + Îµ,
where y = weight loss (grams), t1 = storage time (weeks), t2 = storage tem-
perature ( oF ), and Îµ is a random function to account for all other factors. The
assumption is that all other factors â€œaverage outâ€ to zero, so the expected (or
mean) weight loss at each point (t1, t2) is
E(y) = Î±0 + Î±1t1 + Î±2t2.
(4.6.4)
Suppose that we conduct an experiment in which values for weight loss are
measured for various values of storage time and temperature as shown below.
Time (weeks)
1
1
1
2
2
2
3
3
3
Temp (oF)
âˆ’10
âˆ’5
0
âˆ’10
âˆ’5
0
âˆ’10
âˆ’5
0
Loss (grams)
.15
.18
.20
.17
.19
.22
.20
.23
.25
If
A =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
1
âˆ’10
1
1
âˆ’5
1
1
0
1
2
âˆ’10
1
2
âˆ’5
1
2
0
1
3
âˆ’10
1
3
âˆ’5
1
3
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
x =
ï£«
ï£­
Î±0
Î±1
Î±2
ï£¶
ï£¸,
and
b =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
.15
.18
.20
.17
.19
.22
.20
.23
.25
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
and if we were lucky enough to exactly observe the mean weight loss each time
(i.e., if bi = E(yi) ), then equation (4.6.4) would insure that Ax = b is a
consistent system, so we could solve for the unknown parameters Î±0, Î±1, and
Î±2. However, it is virtually impossible to observe the exact value of the mean
weight loss for a given storage time and temperature, and almost certainly the
system deï¬ned by Ax = b will be inconsistentâ€”especially when the number
of observations greatly exceeds the number of parameters. Since we canâ€™t solve
Ax = b to ï¬nd exact values for the Î±i â€™s, the best we can hope for is a set of
â€œgood estimatesâ€ for these parameters.

4.6 Classical Least Squares
229
The famous Gaussâ€“Markov theorem (developed on p. 448) states that under
certain reasonable assumptions concerning the random error function Îµ, the
â€œbestâ€ estimates for the Î±i â€™s are obtained by minimizing the sum of squares
(Ax âˆ’b)T (Ax âˆ’b). In other words, the least squares estimates are the â€œbestâ€
way to estimate the Î±i â€™s.
Returning to our ice cream example, it can be veriï¬ed that b /âˆˆR (A), so, as
expected, the system Ax = b is not consistent, and we cannot determine exact
values for Î±0, Î±1, and Î±2. The best we can do is to determine least squares esti-
mates for the Î±i â€™s by solving the associated normal equations AT Ax = AT b,
which in this example are
ï£«
ï£­
9
18
âˆ’45
18
42
âˆ’90
âˆ’45
âˆ’90
375
ï£¶
ï£¸
ï£«
ï£­
Î±0
Î±1
Î±2
ï£¶
ï£¸=
ï£«
ï£­
1.79
3.73
âˆ’8.2
ï£¶
ï£¸.
The solution is
ï£«
ï£­
Î±0
Î±1
Î±2
ï£¶
ï£¸=
ï£«
ï£­
.174
.025
.005
ï£¶
ï£¸,
and the estimating equation for mean weight loss becomes
Ë†y = .174 + .025t1 + .005t2.
For example, the mean weight loss of a pint of ice cream that is stored for nine
weeks at a temperature of âˆ’35oF is estimated to be
Ë†y = .174 + .025(9) + .005(âˆ’35) = .224 grams.
Example 4.6.2
Least Squares Curve Fitting Problem:
Find a polynomial
p(t) = Î±0 + Î±1t + Î±2t2 + Â· Â· Â· + Î±nâˆ’1tnâˆ’1
with a speciï¬ed degree that comes as close as possible in the sense of least squares
to passing through a set of data points
D = {(t1, b1),
(t2, b2), . . . , (tm, bm)} ,
where the ti â€™s are distinct numbers, and n â‰¤m.

230
Chapter 4
Vector Spaces
p(t)
(t1,b1)
(t2,b2)

t2,p (t2)


t1,p (t1)

(tm ,bm )

tm ,p (tm )

t
b
Îµ1
Îµ2
Îµm
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
â€¢
Figure 4.6.3
Solution: For the Îµi â€™s indicated in Figure 4.6.3, the objective is to minimize
the sum of squares
m

i=1
Îµ2
i =
m

i=1
(p(ti) âˆ’bi)2 = (Ax âˆ’b)T (Ax âˆ’b),
where
A =
ï£«
ï£¬
ï£¬
ï£­
1
t1
t2
1
Â· Â· Â·
tnâˆ’1
1
1
t2
t2
2
Â· Â· Â·
tnâˆ’1
2
...
...
...
Â· Â· Â·
...
1
tm
t2
m
Â· Â· Â·
tnâˆ’1
m
ï£¶
ï£·
ï£·
ï£¸,
x =
ï£«
ï£¬
ï£¬
ï£­
Î±0
Î±1
...
Î±nâˆ’1
ï£¶
ï£·
ï£·
ï£¸,
and
b =
ï£«
ï£¬
ï£¬
ï£­
b1
b2
...
bm
ï£¶
ï£·
ï£·
ï£¸.
In other words, the least squares polynomial of degree nâˆ’1 is obtained from the
least squares solution associated with the system Ax = b. Furthermore, this
least squares polynomial is unique because AmÃ—n is the Vandermonde matrix
of Example 4.3.4 with n â‰¤m, so rank (A) = n, and Ax = b has a unique
least squares solution given by x =

AT A
âˆ’1AT b.
Note: We know from Example 4.3.5 on p. 186 that the Lagrange interpolation
polynomial â„“(t) of degree mâˆ’1 will exactly ï¬t the dataâ€”i.e., it passes through
each point in D. So why would one want to settle for a least squares ï¬t when
an exact ï¬t is possible? One answer stems from the fact that in practical work
the observations bi are rarely exact due to small errors arising from imprecise

4.6 Classical Least Squares
231
measurements or from simplifying assumptions. For this reason, it is the trend
of the observations that needs to be ï¬tted and not the observations themselves.
To hit the data points, the interpolation polynomial â„“(t) is usually forced to
oscillate between or beyond the data points, and as m becomes larger the oscil-
lations can become more pronounced. Consequently, â„“(t) is generally not useful
in making estimations concerning the trend of the observationsâ€”Example 4.6.3
drives this point home. In addition to exactly hitting a prescribed set of data
points, an interpolation polynomial called the Hermite polynomial (p. 607) can
be constructed to have speciï¬ed derivatives at each data point. While this helps,
it still is not as good as least squares for making estimations on the basis of
observations.
Example 4.6.3
A missile is ï¬red from enemy territory, and its position in ï¬‚ight is observed by
radar tracking devices at the following positions.
Position down range (miles)
0
250
500
750
1000
Height (miles)
0
8
15
19
20
Suppose our intelligence sources indicate that enemy missiles are programmed
to follow a parabolic ï¬‚ight pathâ€”a fact that seems to be consistent with the
diagram obtained by plotting the observations on the coordinate system shown
in Figure 4.6.4.
1000
750
500
250
0
0
5
10
15
20
t = Range
b = Height
Figure 4.6.4
Problem: Predict how far down range the missile will land.

232
Chapter 4
Vector Spaces
Solution:
Determine the parabola f(t) = Î±0 + Î±1t + Î±2t2 that best ï¬ts the
observed data in the least squares sense. Then estimate where the missile will
land by determining the roots of f (i.e., determine where the parabola crosses
the horizontal axis). As it stands, the problem will involve numbers having rela-
tively large magnitudes in conjunction with relatively small ones. Consequently,
it is better to ï¬rst scale the data by considering one unit to be 1000 miles. If
A =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1
0
0
1
.25
.0625
1
.5
.25
1
.75
.5625
1
1
1
ï£¶
ï£·
ï£·
ï£·
ï£¸,
x =
ï£«
ï£­
Î±0
Î±1
Î±2
ï£¶
ï£¸,
and
b =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
.008
.015
.019
.02
ï£¶
ï£·
ï£·
ï£·
ï£¸,
and if Îµ = Ax âˆ’b, then the object is to ï¬nd a least squares solution x that
minimizes
5

i=1
Îµ2
i = ÎµT Îµ = (Ax âˆ’b)T (Ax âˆ’b).
We know that such a least squares solution is given by the solution to the system
of normal equations AT Ax = AT b, which in this case is
ï£«
ï£­
5
2.5
1.875
2.5
1.875
1.5625
1.875
1.5625
1.3828125
ï£¶
ï£¸
ï£«
ï£­
Î±0
Î±1
Î±2
ï£¶
ï£¸=
ï£«
ï£­
.062
.04375
.0349375
ï£¶
ï£¸.
The solution (rounded to four signiï¬cant digits) is
x =
ï£«
ï£­
âˆ’2.286 Ã— 10âˆ’4
3.983 Ã— 10âˆ’2
âˆ’1.943 Ã— 10âˆ’2
ï£¶
ï£¸,
and the least squares parabola is
f(t) = âˆ’.0002286 + .03983t âˆ’.01943t2.
To estimate where the missile will land, determine where this parabola crosses
the horizontal axis by applying the quadratic formula to ï¬nd the roots of f(t)
to be t = .005755 and t = 2.044. Therefore, we estimate that the missile will
land 2044 miles down range. The sum of the squares of the errors associated with
the least squares solution is
5

i=1
Îµ2
i = ÎµT Îµ = (Ax âˆ’b)T (Ax âˆ’b) = 4.571 Ã— 10âˆ’7.

4.6 Classical Least Squares
233
Least Squares vs. Lagrange Interpolation. Instead of using least squares,
ï¬t the observations exactly with the fourth-degree Lagrange interpolation poly-
nomial
â„“(t) = 11
375t +
17
750000t2 âˆ’
1
18750000t3 +
1
46875000000t4
described in Example 4.3.5 on p. 186 (you can verify that â„“(ti) = bi for each
observation). As the graph in Figure 4.6.5 indicates, â„“(t) has only one real
nonnegative root, so it is worthless for predicting where the missile will land.
This is characteristic of Lagrange interpolation.
y = â„“(t)
Figure 4.6.5
Computational Note: Theoretically, the least squares solutions of Ax = b
are exactly the solutions of the normal equations AT Ax = AT b, but form-
ing and solving the normal equations to compute least squares solutions with
ï¬‚oating-point arithmetic is not recommended. As pointed out in Example 4.5.1
on p. 214, any sensitivities to small perturbations that are present in the under-
lying problem are magniï¬ed by forming the normal equations. In other words, if
the underlying problem is somewhat ill-conditioned, then the system of normal
equations will be ill-conditioned to an even greater extent. Numerically stable
techniques that avoid the normal equations are presented in Example 5.5.3 on
p. 313 and Example 5.7.3 on p. 346.
Epilogue
While viewing a region in the Taurus constellation on January 1, 1801, Giuseppe
Piazzi, an astronomer and director of the Palermo observatory, observed a small
â€œstarâ€ that he had never seen before. As Piazzi and others continued to watch
this new â€œstarâ€â€”which was really an asteroidâ€”they noticed that it was in fact
moving, and they concluded that a new â€œplanetâ€ had been discovered. However,
their new â€œplanetâ€ completely disappeared in the autumn of 1801. Well-known
astronomers of the time joined the search to relocate the lost â€œplanet,â€ but all
eï¬€orts were in vain.

234
Chapter 4
Vector Spaces
In September of 1801 Carl F. Gauss decided to take up the challenge of
ï¬nding this lost â€œplanet.â€ Gauss allowed for the possibility of an elliptical or-
bit rather than constraining it to be circularâ€”which was an assumption of the
othersâ€”and he proceeded to develop the method of least squares. By December
the task was completed, and Gauss informed the scientiï¬c community not only
where the lost â€œplanetâ€ was located, but he also predicted its position at fu-
ture times. They looked, and it was exactly where Gauss had predicted it would
be! The asteroid was named Ceres, and Gaussâ€™s contribution was recognized by
naming another minor asteroid Gaussia.
This extraordinary feat of locating a tiny and distant heavenly body from
apparently insuï¬ƒcient data astounded the scientiï¬c community. Furthermore,
Gauss refused to reveal his methods, and there were those who even accused
him of sorcery. These events led directly to Gaussâ€™s fame throughout the entire
European community, and they helped to establish his reputation as a mathe-
matical and scientiï¬c genius of the highest order.
Gauss waited until 1809, when he published his Theoria Motus Corporum
Coelestium In Sectionibus Conicis Solem Ambientium, to systematically develop
the theory of least squares and his methods of orbit calculation. This was in
keeping with Gaussâ€™s philosophy to publish nothing but well-polished work of
lasting signiï¬cance. When criticized for not revealing more motivational aspects
in his writings, Gauss remarked that architects of great cathedrals do not obscure
the beauty of their work by leaving the scaï¬€olds in place after the construction
has been completed. Gaussâ€™s theory of least squares approximation has indeed
proven to be a great mathematical cathedral of lasting beauty and signiï¬cance.
Exercises for section 4.6
4.6.1. Hookeâ€™s law says that the displacement y of an ideal spring is propor-
tional to the force x that is appliedâ€”i.e., y = kx for some constant k.
Consider a spring in which k is unknown. Various masses are attached,
and the resulting displacements shown in Figure 4.6.6 are observed. Us-
ing these observations, determine the least squares estimate for k.
x (lb)
y (in)
5
11.1
7
15.4
8
17.5
10
22.0
12
26.3
x
y
Figure 4.6.6

4.6 Classical Least Squares
235
4.6.2. Show that the slope of the line that passes through the origin in â„œ2 and
comes closest in the least squares sense to passing through the points
{(x1, y1), (x2, y2), . . . , (xn, yn)} is given by m = 
i xiyi/ 
i x2
i .
4.6.3. A small company has been in business for three years and has recorded
annual proï¬ts (in thousands of dollars) as follows.
Year
1
2
3
Sales
7
4
3
Assuming that there is a linear trend in the declining proï¬ts, predict the
year and the month in which the company begins to lose money.
4.6.4. An economist hypothesizes that the change (in dollars) in the price of a
loaf of bread is primarily a linear combination of the change in the price
of a bushel of wheat and the change in the minimum wage. That is, if B
is the change in bread prices, W is the change in wheat prices, and M
is the change in the minimum wage, then B = Î±W +Î²M. Suppose that
for three consecutive years the change in bread prices, wheat prices, and
the minimum wage are as shown below.
Year 1
Year 2
Year 3
B
+$1
+$1
+$1
W
+$1
+$2
0$
M
+$1
0$
âˆ’$1
Use the theory of least squares to estimate the change in the price of
bread in Year 4 if wheat prices and the minimum wage each fall by $1.
4.6.5. Suppose that a researcher hypothesizes that the weight loss of a pint of
ice cream during storage is primarily a linear function of time. That is,
y = Î±0 + Î±1t + Îµ,
where y = the weight loss in grams, t = the storage time in weeks, and
Îµ is a random error function whose mean value is 0. Suppose that an
experiment is conducted, and the following data is obtained.
Time (t)
1
2
3
4
5
6
7
8
Loss (y)
.15
.21
.30
.41
.49
.59
.72
.83
(a)
Determine the least squares estimates for the parameters Î±0
and Î±1.
(b)
Predict the mean weight loss for a pint of ice cream that is stored
for 20 weeks.

236
Chapter 4
Vector Spaces
4.6.6. After studying a certain type of cancer, a researcher hypothesizes that
in the short run the number (y) of malignant cells in a particular tissue
grows exponentially with time (t). That is, y = Î±0eÎ±1t. Determine least
squares estimates for the parameters Î±0 and Î±1 from the researcherâ€™s
observed data given below.
t (days)
1
2
3
4
5
y (cells)
16
27
45
74
122
Hint: What common transformation converts an exponential function
into a linear function?
4.6.7. Using least squares techniques, ï¬t the following data
x
âˆ’5
âˆ’4
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
4
5
y
2
7
9
12
13
14
14
13
10
8
4
with a line y = Î±0 + Î±1x and then ï¬t the data with a quadratic y =
Î±0 +Î±1x+Î±2x2. Determine which of these two curves best ï¬ts the data
by computing the sum of the squares of the errors in each case.
4.6.8. Consider the time (T) it takes for a runner to complete a marathon (26
miles and 385 yards). Many factors such as height, weight, age, previous
training, etc. can inï¬‚uence an athleteâ€™s performance, but experience has
shown that the following three factors are particularly important:
x1 = Ponderal index =
height (in.)
[weight (lbs.)]
1
3 ,
x2 = Miles run the previous 8 weeks,
x3 = Age (years).
A linear model hypothesizes that the time T (in minutes) is given by
T = Î±0 + Î±1x1 + Î±2x2 + Î±3x3 + Îµ, where Îµ is a random function
accounting for all other factors and whose mean value is assumed to
be zero. On the basis of the ï¬ve observations given below, estimate the
expected marathon time for a 43-year-old runner of height 74 in., weight
180 lbs., who has run 450 miles during the previous eight weeks.
T
x1
x2
x3
181
13.1
619
23
193
13.5
803
42
212
13.8
207
31
221
13.1
409
38
248
12.5
482
45
What is your personal predicted mean marathon time?

4.6 Classical Least Squares
237
4.6.9. For A âˆˆâ„œmÃ—n and b âˆˆâ„œm, prove that x2 is a least squares solution
for Ax = b if and only if x2 is part of a solution to the larger system
 ImÃ—m
A
AT
0nÃ—n
  x1
x2

=
 b
0

.
(4.6.5)
Note: It is not uncommon to encounter least squares problems in which
A is extremely large but very sparse (mostly zero entries). For these
situations, the system (4.6.5) will usually contain signiï¬cantly fewer
nonzero entries than the system of normal equations, thereby helping to
overcome the memory requirements that plague these problems. Using
(4.6.5) also eliminates the undesirable need to explicitly form the prod-
uct AT A â€”recall from Example 4.5.1 that forming AT A can cause
loss of signiï¬cant information.
4.6.10. In many least squares applications, the underlying data matrix AmÃ—n
does not have independent columnsâ€”i.e., rank (A) < n â€”so the corre-
sponding system of normal equations AT Ax = AT b will fail to have
a unique solution. This means that in an associated linear estimation
problem of the form
y = Î±1t1 + Î±2t2 + Â· Â· Â· + Î±ntn + Îµ
there will be inï¬nitely many least squares estimates for the parameters
Î±i, and hence there will be inï¬nitely many estimates for the mean value
of y at any given point (t1, t2, . . . , tn) â€”which is clearly an undesirable
situation. In order to remedy this problem, we restrict ourselves to mak-
ing estimates only at those points (t1, t2, . . . , tn) that are in the row
space of A. If
t =
ï£«
ï£¬
ï£¬
ï£­
t1
t2
...
tn
ï£¶
ï£·
ï£·
ï£¸âˆˆR

AT 
,
and if
x =
ï£«
ï£¬
ï£¬
ï£­
Ë†Î±1
Ë†Î±2
...
Ë†Î±n
ï£¶
ï£·
ï£·
ï£¸
is any least squares solution (i.e., AT Ax = AT b ), prove that the esti-
mate deï¬ned by
Ë†y = tT x =
n

i=1
tiË†Î±i
is unique in the sense that Ë†y is independent of which least squares
solution x is used.

238
Chapter 4
Vector Spaces
4.7
LINEAR TRANSFORMATIONS
The connection between linear functions and matrices is at the heart of our sub-
ject. As explained on p. 93, matrix algebra grew out of Cayleyâ€™s observation that
the composition of two linear functions can be represented by the multiplication
of two matrices. Itâ€™s now time to look deeper into such matters and to formalize
the connections between matrices, vector spaces, and linear functions deï¬ned on
vector spaces. This is the point at which linear algebra, as the study of linear
functions on vector spaces, begins in earnest.
Linear Transformations
Let U and V be vector spaces over a ï¬eld F ( â„œor C for us).
â€¢
A linear transformation from U into V is deï¬ned to be a linear
function T mapping U into V. That is,
T(x + y) = T(x) + T(y)
and
T(Î±x) = Î±T(x)
(4.7.1)
or, equivalently,
T(Î±x + y) = Î±T(x) + T(y) for all x, y âˆˆU, Î± âˆˆF.
(4.7.2)
â€¢
A linear operator on U is deï¬ned to be a linear transformation
from U into itselfâ€”i.e., a linear function mapping U back into U.
Example 4.7.1
â€¢
The function 0(x) = 0 that maps all vectors in a space U to the zero
vector in another space V is a linear transformation from U into V, and,
not surprisingly, it is called the zero transformation.
â€¢
The function I(x) = x that maps every vector from a space U back to itself
is a linear operator on U. I is called the identity operator on U.
â€¢
For A âˆˆâ„œmÃ—n and x âˆˆâ„œnÃ—1, the function T(x) = Ax is a linear
transformation from â„œn into â„œm because matrix multiplication satisï¬es
A(Î±x + y) = Î±Ax + Ay. T is a linear operator on â„œn if A is n Ã— n.
â€¢
If W is the vector space of all functions from â„œto â„œ, and if V is the space
of all diï¬€erentiable functions from â„œto â„œ, then the mapping D(f) = df/dx
is a linear transformation from V into W because
d(Î±f + g)
dx
= Î± df
dx + dg
dx.
â€¢
If V is the space of all continuous functions from â„œinto â„œ, then the
mapping deï¬ned by T(f) =
& x
0 f(t)dt is a linear operator on V because
' x
0
[Î±f(t) + g(t)] dt = Î±
' x
0
f(t)dt +
' x
0
g(t)dt.

4.7 Linear Transformations
239
â€¢
The rotator Q that rotates vectors u in â„œ2 counterclockwise through an
angle Î¸, as shown in Figure 4.7.1, is a linear operator on â„œ2 because the
â€œactionâ€ of Q on u can be described by matrix multiplication in the sense
that the coordinates of the rotated vector Q(u) are given by
Q(u) =

x cos Î¸ âˆ’y sin Î¸
x sin Î¸ + y cos Î¸

=

cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸
 
x
y

.
â€¢
The projector P that maps each point v = (x, y, z) âˆˆâ„œ3 to its orthogonal
projection (x, y, 0) in the xy -plane, as depicted in Figure 4.7.2, is a linear
operator on â„œ3 because if u = (u1, u2, u3) and v = (v1, v2, v3), then
P(Î±u + v)=(Î±u1+v1, Î±u2+v2, 0)=Î±(u1, u2, 0)+(v1, v2, 0)=Î±P(u)+P(v).
â€¢
The reï¬‚ector R that maps each vector v = (x, y, z) âˆˆâ„œ3 to its reï¬‚ection
R(v) = (x, y, âˆ’z) about the xy -plane, as shown in Figure 4.7.3, is a linear
operator on â„œ3.
Î¸
Q(u) = (x cos Î¸  -  y sin Î¸,  x sin Î¸  +  y cos Î¸)
u = (x, y)
y = x
P(v)
v
v = (x, y, z)
R(v) = (x, y, -z)
Figure 4.7.1
Figure 4.7.2
Figure 4.7.3
â€¢
Just as the rotator Q is represented by a matrix [Q] =
 cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸

, the
projector P and the reï¬‚ector R can be represented by matrices
[P] =
ï£«
ï£­
1
0
0
0
1
0
0
0
0
ï£¶
ï£¸
and
[R] =
ï£«
ï£­
1
0
0
0
1
0
0
0
âˆ’1
ï£¶
ï£¸
in the sense that the â€œactionâ€ of P and R on v = (x, y, z) can be accom-
plished with matrix multiplication using [P] and [R] by writing
ï£«
ï£­
1
0
0
0
1
0
0
0
0
ï£¶
ï£¸
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
x
y
0
ï£¶
ï£¸
and
ï£«
ï£­
1
0
0
0
1
0
0
0
âˆ’1
ï£¶
ï£¸
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
x
y
âˆ’z
ï£¶
ï£¸.

240
Chapter 4
Vector Spaces
It would be wrong to infer from Example 4.7.1 that all linear transformations
can be represented by matrices (of ï¬nite size). For example, the diï¬€erential and
integral operators do not have matrix representations because they are deï¬ned
on inï¬nite-dimensional spaces. But linear transformations on ï¬nite-dimensional
spaces will always have matrix representations. To see why, the concept of â€œco-
ordinatesâ€ in higher dimensions must ï¬rst be understood.
Recall that if B = {u1, u2, . . . , un} is a basis for a vector space U, then
each v âˆˆU can be written as v = Î±1u1 + Î±2u2 + Â· Â· Â· + Î±nun. The Î±i â€™s in this
expansion are uniquely determined by v because if v = 
i Î±iui = 
i Î²iui,
then 0 = 
i(Î±i âˆ’Î²i)ui, and this implies Î±i âˆ’Î²i = 0 (i.e., Î±i = Î²i) for each
i because B is an independent set.
Coordinates of a Vector
Let B = {u1, u2, . . . , un} be a basis for a vector space U, and let v âˆˆU.
The coeï¬ƒcients Î±i in the expansion v = Î±1u1 +Î±2u2 +Â· Â· Â·+Î±nun are
called the coordinates of v with respect to B, and, from now on,
[v]B will denote the column vector
[v]B =
ï£«
ï£¬
ï£¬
ï£­
Î±1
Î±2
...
Î±n
ï£¶
ï£·
ï£·
ï£¸.
Caution! Order is important. If Bâ€² is a permutation of B, then [v]Bâ€²
is the corresponding permutation of [v]B.
From now on, S = {e1, e2, . . . , en} will denote the standard basis of unit
vectors (in natural order) for â„œn (or Cn). If no other basis is explicitly men-
tioned, then the standard basis is assumed. For example, if no basis is mentioned,
and if we write
v =
ï£«
ï£­
8
7
4
ï£¶
ï£¸,
then it is understood that this is the representation with respect to S in the
sense that v = [v]S = 8e1 + 7e2 + 4e3. The standard coordinates of a vector
are its coordinates with respect to S. So, 8, 7, and 4 are the standard coordinates
of v in the above example.
Example 4.7.2
Problem: If v is a vector in â„œ3 whose standard coordinates are

4.7 Linear Transformations
241
v =
ï£«
ï£­
8
7
4
ï£¶
ï£¸,
determine the coordinates of v with respect to the basis
B =
ï£±
ï£²
ï£³u1 =
ï£«
ï£­
1
1
1
ï£¶
ï£¸, u2 =
ï£«
ï£­
1
2
2
ï£¶
ï£¸, u3 =
ï£«
ï£­
1
2
3
ï£¶
ï£¸
ï£¼
ï£½
ï£¾.
Solution: The object is to ï¬nd the three unknowns Î±1, Î±2, and Î±3 such that
Î±1u1 + Î±2u2 + Î±3u3 = v. This is simply a 3 Ã— 3 system of linear equations
ï£«
ï£­
1
1
1
1
2
2
1
2
3
ï£¶
ï£¸
ï£«
ï£­
Î±1
Î±2
Î±3
ï£¶
ï£¸=
ï£«
ï£­
8
7
4
ï£¶
ï£¸
=â‡’
[v]B =
ï£«
ï£­
Î±1
Î±2
Î±3
ï£¶
ï£¸=
ï£«
ï£­
9
2
âˆ’3
ï£¶
ï£¸.
The general rule for making a change of coordinates is given on p. 252.
Linear transformations possess coordinates in the same way vectors do be-
cause linear transformations from U to V also form a vector space.
Space of Linear Transformations
â€¢
For each pair of vector spaces U and V over F, the set L(U, V) of
all linear transformations from U to V is a vector space over F.
â€¢
Let B = {u1, u2, . . . , un} and Bâ€² = {v1, v2, . . . , vm} be bases for U
and V, respectively, and let Bji be the linear transformation from
U into V deï¬ned by Bji(u) = Î¾jvi, where (Î¾1, Î¾2, . . . , Î¾n)T = [u]B.
That is, pick oï¬€the jth coordinate of u, and attach it to vi.
â–·
BL = {Bji}i=1...m
j=1...n is a basis for L(U, V).
â–·
dim L(U, V) = (dim U) (dim V) .
Proof.
L(U, V) is a vector space because the deï¬ning properties on p. 160 are
satisï¬edâ€”details are omitted. Prove BL is a basis by demonstrating that it is a
linearly independent spanning set for L(U, V). To establish linear independence,
suppose 
j,i Î·jiBji = 0 for scalars Î·ji, and observe that for each uk âˆˆB,
Bji(uk)=

vi
if j = k
0
if j Ì¸= k =â‡’0=
 
j,i
Î·jiBji

(uk)=

j,i
Î·jiBji(uk)=
m

i=1
Î·kivi.
For each k, the independence of Bâ€² implies that Î·ki = 0 for each i, and thus
BL is linearly independent. To see that BL spans L(U, V), let T âˆˆL(U, V),

242
Chapter 4
Vector Spaces
and determine the action of T on any u âˆˆU by using u = n
j=1 Î¾juj and
T(uj) = m
i=1 Î±ijvi to write
T(u) = T

n

j=1
Î¾juj
 
=
n

j=1
Î¾jT(uj) =
n

j=1
Î¾j
m

i=1
Î±ijvi
=

i,j
Î±ijÎ¾jvi =

i,j
Î±ijBji(u).
(4.7.3)
This holds for all u âˆˆU, so T = 
i,j Î±ijBji, and thus BL spans L(U, V).
It now makes sense to talk about the coordinates of T âˆˆL(U, V) with
respect to the basis BL. In fact, the rule for determining these coordinates is
contained in the proof above, where it was demonstrated that T = 
i,j Î±ijBji
in which the coordinates Î±ij are precisely the scalars in
T(uj) =
m

i=1
Î±ijvi or, equivalently, [T(uj)]Bâ€² =
ï£«
ï£¬
ï£¬
ï£­
Î±1j
Î±2j
...
Î±mj
ï£¶
ï£·
ï£·
ï£¸,
j = 1, 2, . . . , n.
This suggests that rather than listing all coordinates Î±ij in a single column
containing mn entries (as we did with coordinate vectors), itâ€™s more logical to
arrange the Î±ij â€™s as an m Ã— n matrix in which the jth column contains the
coordinates of T(uj) with respect to Bâ€². These ideas are summarized below.
Coordinate Matrix Representations
Let B = {u1, u2, . . . , un} and Bâ€² = {v1, v2, . . . , vm} be bases for U
and V, respectively. The coordinate matrix of T âˆˆL(U, V) with
respect to the pair (B, Bâ€²) is deï¬ned to be the m Ã— n matrix
[T]BBâ€² =

[T(u1)]Bâ€²
 [T(u2)]Bâ€²
 Â· Â· Â·
 [T(un)]Bâ€²

.
(4.7.4)
In other words, if T(uj) = Î±1jv1 + Î±2jv2 + Â· Â· Â· + Î±mjvm, then
[T(uj)]Bâ€² =
ï£«
ï£¬
ï£¬
ï£­
Î±1j
Î±2j
...
Î±mj
ï£¶
ï£·
ï£·
ï£¸and [T]BBâ€² =
ï£«
ï£¬
ï£¬
ï£­
Î±11
Î±12
Â· Â· Â·
Î±1n
Î±21
Î±22
Â· Â· Â·
Î±2n
...
...
...
...
Î±m1
Î±m2
Â· Â· Â·
Î±mn
ï£¶
ï£·
ï£·
ï£¸.
(4.7.5)
When T is a linear operator on U, and when there is only one basis
involved, [T]B is used in place of [T]BB to denote the (necessarily
square) coordinate matrix of T with respect to B.

4.7 Linear Transformations
243
Example 4.7.3
Problem: If P is the projector deï¬ned in Example 4.7.1 that maps each point
v = (x, y, z) âˆˆâ„œ3 to its orthogonal projection P(v) = (x, y, 0) in the xy -plane,
determine the coordinate matrix [P]B with respect to the basis
B =
ï£±
ï£²
ï£³u1 =
ï£«
ï£­
1
1
1
ï£¶
ï£¸, u2 =
ï£«
ï£­
1
2
2
ï£¶
ï£¸, u3 =
ï£«
ï£­
1
2
3
ï£¶
ï£¸
ï£¼
ï£½
ï£¾.
Solution: According to (4.7.4), the jth column in [P]B is [P(uj)]B. Therefore,
P(u1) =
ï£«
ï£­
1
1
0
ï£¶
ï£¸= 1u1 + 1u2 âˆ’1u3
=â‡’
[P(u1)]B =
ï£«
ï£­
1
1
âˆ’1
ï£¶
ï£¸,
P(u2) =
ï£«
ï£­
1
2
0
ï£¶
ï£¸= 0u1 + 3u2 âˆ’2u3
=â‡’
[P(u2)]B =
ï£«
ï£­
0
3
âˆ’2
ï£¶
ï£¸,
P(u3) =
ï£«
ï£­
1
2
0
ï£¶
ï£¸= 0u1 + 3u2 âˆ’2u3
=â‡’
[P(u3)]B =
ï£«
ï£­
0
3
âˆ’2
ï£¶
ï£¸,
so that [P]B =
ï£«
ï£­
1
0
0
1
3
3
âˆ’1
âˆ’2
âˆ’2
ï£¶
ï£¸.
Example 4.7.4
Problem: Consider the same problem given in Example 4.7.3, but use diï¬€erent
basesâ€”say,
B =
ï£±
ï£²
ï£³u1 =
ï£«
ï£­
1
0
0
ï£¶
ï£¸,
u2 =
ï£«
ï£­
1
1
0
ï£¶
ï£¸,
u3 =
ï£«
ï£­
1
1
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
and
Bâ€² =
ï£±
ï£²
ï£³v1 =
ï£«
ï£­
âˆ’1
0
0
ï£¶
ï£¸,
v2 =
ï£«
ï£­
0
1
0
ï£¶
ï£¸,
v3 =
ï£«
ï£­
0
1
âˆ’1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾.
For the projector deï¬ned by P(x, y, z) = (x, y, 0), determine [P]BBâ€².
Solution:
Determine the coordinates of each P(uj) with respect to Bâ€², as

244
Chapter 4
Vector Spaces
shown below:
P(u1) =
ï£«
ï£­
1
0
0
ï£¶
ï£¸= âˆ’1v1 + 0v2 + 0v3
=â‡’
[P(u1)]Bâ€² =
ï£«
ï£­
âˆ’1
0
0
ï£¶
ï£¸,
P(u2) =
ï£«
ï£­
1
1
0
ï£¶
ï£¸= âˆ’1v1 + 1v2 + 0v3
=â‡’
[P(u2)]Bâ€² =
ï£«
ï£­
âˆ’1
1
0
ï£¶
ï£¸,
P(u3) =
ï£«
ï£­
1
1
0
ï£¶
ï£¸= âˆ’1v1 + 1v2 + 0v3
=â‡’
[P(u3)]Bâ€² =
ï£«
ï£­
âˆ’1
1
0
ï£¶
ï£¸.
Therefore, according to (4.7.4), [P]BBâ€² =
 âˆ’1
âˆ’1
âˆ’1
0
1
1
0
0
0

.
At the heart of linear algebra is the realization that the theory of ï¬nite-
dimensional linear transformations is essentially the same as the theory of ma-
trices. This is due primarily to the fundamental fact that the action of a linear
transformation T on a vector u is precisely matrix multiplication between the
coordinates of T and the coordinates of u.
Action as Matrix Multiplication
Let T âˆˆL(U, V), and let B and Bâ€² be bases for U and V, respectively.
For each u âˆˆU, the action of T on u is given by matrix multiplication
between their coordinates in the sense that
[T(u)]Bâ€² = [T]BBâ€²[u]B.
(4.7.6)
Proof.
Let B = {u1, u2, . . . , un} and Bâ€² = {v1, v2, . . . , vm} . If u = n
j=1 Î¾juj
and T(uj) = m
i=1 Î±ijvi, then
[u]B =
ï£«
ï£¬
ï£¬
ï£­
Î¾1
Î¾2
...
Î¾n
ï£¶
ï£·
ï£·
ï£¸
and [T]BBâ€² =
ï£«
ï£¬
ï£¬
ï£­
Î±11
Î±12
Â· Â· Â·
Î±1n
Î±21
Î±22
Â· Â· Â·
Î±2n
...
...
...
...
Î±m1
Î±m2
Â· Â· Â·
Î±mn
ï£¶
ï£·
ï£·
ï£¸,
so, according to (4.7.3),
T(u) =

i,j
Î±ijÎ¾jvi =
m

i=1

n

j=1
Î±ijÎ¾j
 
vi.

4.7 Linear Transformations
245
In other words, the coordinates of T(u) with respect to Bâ€² are the terms
n
j=1 Î±ijÎ¾j for i = 1, 2, . . . , m, and therefore
[T(u)]Bâ€² =
ï£«
ï£¬
ï£¬
ï£¬
ï£­

j Î±1jÎ¾j

j Î±2jÎ¾j
...

j Î±mjÎ¾j
ï£¶
ï£·
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
Î±11
Î±12
Â· Â· Â·
Î±1n
Î±21
Î±22
Â· Â· Â·
Î±2n
...
...
...
...
Î±m1
Î±m2
Â· Â· Â·
Î±mn
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
Î¾1
Î¾2
...
Î¾n
ï£¶
ï£·
ï£·
ï£¸= [T]BBâ€²[u]B.
Example 4.7.5
Problem: Show how the action of the operator D

p(t)

= dp/dt on the space
P3 of polynomials of degree three or less is given by matrix multiplication.
Solution: The coordinate matrix of D with respect to the basis B = {1, t, t2, t3}
is
[D]B =
ï£«
ï£¬
ï£­
0
1
0
0
0
0
2
0
0
0
0
3
0
0
0
0
ï£¶
ï£·
ï£¸.
If p = p(t) = Î±0 + Î±1t + Î±2t2 + Î±3t3, then D(p) = Î±1 + 2Î±2t + 3Î±3t2 so that
[p]B =
ï£«
ï£¬
ï£­
Î±0
Î±1
Î±2
Î±3
ï£¶
ï£·
ï£¸
and
[D(p)]B =
ï£«
ï£¬
ï£­
Î±1
2Î±2
3Î±3
0
ï£¶
ï£·
ï£¸.
The action of D is accomplished by means of matrix multiplication because
[D(p)]B =
ï£«
ï£¬
ï£­
Î±1
2Î±2
3Î±3
0
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
0
1
0
0
0
0
2
0
0
0
0
3
0
0
0
0
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
Î±0
Î±1
Î±2
Î±3
ï£¶
ï£·
ï£¸= [D]B[p]B.
For T âˆˆL(U, V) and L âˆˆL(V, W), the composition of L with T is
deï¬ned to be the function C : U â†’W such that C(x) = L

T(x)

, and this
composition, denoted by C = LT, is also a linear transformation because
C(Î±x + y) = L

T(Î±x + y)

= L

Î±T(x) + T(y)

= Î±L

T(x)

+ L

T(y)

= Î±C(x) + C(y).
Consequently, if B,
Bâ€², and Bâ€²â€² are bases for U,
V, and W, respectively,
then C must have a coordinate matrix representation with respect to (B, Bâ€²â€²),
so itâ€™s only natural to ask how [C]BBâ€²â€² is related to [L]Bâ€²Bâ€²â€² and [T]BBâ€². Re-
call that the motivation behind the deï¬nition of matrix multiplication given on
p. 93 was based on the need to represent the composition of two linear trans-
formations, so it should be no surprise to discover that [C]BBâ€²â€² = [L]Bâ€²Bâ€²â€²[T]BBâ€².
This, along with the other properties given below, makes it clear that studying
linear transformations on ï¬nite-dimensional spaces amounts to studying matrix
algebra.

246
Chapter 4
Vector Spaces
Connections with Matrix Algebra
â€¢
If T, L âˆˆL(U, V), and if B and Bâ€² are bases for U and V, then
â–·
[Î±T]BBâ€² = Î±[T]BBâ€² for scalars Î±,
(4.7.7)
â–·
[T + L]BBâ€² = [T]BBâ€² + [L]BBâ€².
(4.7.8)
â€¢
If T âˆˆL(U, V) and L âˆˆL(V, W), and if B, Bâ€², and Bâ€²â€² are bases
for U, V, and W, respectively, then LT âˆˆL(U, W), and
â–·
[LT]BBâ€²â€² = [L]Bâ€²Bâ€²â€²[T]BBâ€².
(4.7.9)
â€¢
If T âˆˆL(U, U) is invertible in the sense that TTâˆ’1 = Tâˆ’1T = I
for some Tâˆ’1 âˆˆL(U, U), then for every basis B of U,
â–·
[Tâˆ’1]B = [T]âˆ’1
B .
(4.7.10)
Proof.
The ï¬rst three properties (4.7.7)â€“(4.7.9) follow directly from (4.7.6). For
example, to prove (4.7.9), let u be any vector in U, and write
[LT]BBâ€²â€²[u]B =
"
LT(u)
#
Bâ€²â€² =
"
L

T(u)
#
Bâ€²â€² =[L]Bâ€²Bâ€²â€²"
T(u)
#
Bâ€² =[L]Bâ€²Bâ€²â€²[T]BBâ€²[u]B.
This is true for all u âˆˆU, so [LT]BBâ€²â€² = [L]Bâ€²Bâ€²â€²[T]BBâ€² (see Exercise 3.5.5).
Proving (4.7.7) and (4.7.8) is similarâ€”details are omitted. To prove (4.7.10),
note that if dim U = n, then [I]B = In for all bases B, so property (4.7.9)
implies In = [I]B = [TTâˆ’1]B = [T]B[Tâˆ’1]B, and thus [Tâˆ’1]B = [T]âˆ’1
B .
Example 4.7.6
Problem: Form the composition C = LT of the two linear transformations
T : â„œ3 â†’â„œ2 and L : â„œ2 â†’â„œ2 deï¬ned by
T(x, y, z) = (x + y, y âˆ’z)
and
L(u, v) = (2u âˆ’v, u),
and then verify (4.7.9) and (4.7.10) using the standard bases S2 and S3 for â„œ2
and â„œ3, respectively.
Solution: The composition C : â„œ3 â†’â„œ2 is the linear transformation
C(x, y, z) = L

T(x, y, z)

= L(x + y, y âˆ’z) = (2x + y + z, x + y).
The coordinate matrix representations of C, L, and T are
[C]S3S2 =

2
1
1
1
1
0

,
[L]S2 =

2
âˆ’1
1
0

,
and
[T]S3S2 =

1
1
0
0
1
âˆ’1

.

4.7 Linear Transformations
247
Property (4.7.9) is veriï¬ed because [LT]S3S2 = [C]S3S2 = [L]S2[T]S3S2. Find
Lâˆ’1 by looking for scalars Î²ij in Lâˆ’1(u, v) = (Î²11u + Î²12v, Î²21u + Î²22v) such
that LLâˆ’1 = Lâˆ’1L = I or, equivalently,
L

Lâˆ’1(u, v)

= Lâˆ’1
L(u, v)

= (u, v)
for all (u, v) âˆˆâ„œ2.
Computation reveals Lâˆ’1(u, v) = (v, 2v âˆ’u), and (4.7.10) is veriï¬ed by noting
[Lâˆ’1]S2 =

0
1
âˆ’1
2

=

2
âˆ’1
1
0
âˆ’1
= [L]âˆ’1
S2 .
Exercises for section 4.7
4.7.1. Determine which of the following functions are linear operators on â„œ2.
(a)
T(x, y) = (x, 1 + y),
(b)
T(x, y) = (y, x),
(c)
T(x, y) = (0, xy),
(d)
T(x, y) = (x2, y2),
(e)
T(x, y) = (x, sin y),
(f)
T(x, y) = (x + y, x âˆ’y).
4.7.2. For A âˆˆâ„œnÃ—n, determine which of the following functions are linear
transformations.
(a)
T(XnÃ—n) = AX âˆ’XA,
(b)
T(xnÃ—1) = Ax + b for b Ì¸= 0,
(c)
T(A) = AT ,
(d)
T(XnÃ—n) = (X + XT )/2.
4.7.3. Explain why T(0) = 0 for every linear transformation T.
4.7.4. Determine which of the following mappings are linear operators on Pn,
the vector space of polynomials of degree n or less.
(a)
T = Î¾kDk + Î¾kâˆ’1Dkâˆ’1 + Â· Â· Â· + Î¾1D + Î¾0I, where Dk is the
kth-order diï¬€erentiation operator (i.e., Dkp(t) = dkp/dtk).
(b)
T

p(t)

= tnpâ€²(0) + t.
4.7.5. Let v be a ï¬xed vector in â„œnÃ—1 and let T : â„œnÃ—1 â†’â„œbe the mapping
deï¬ned by T(x) = vTx (i.e., the standard inner product).
(a)
Is T a linear operator?
(b)
Is T a linear transformation?
4.7.6. For the operator T : â„œ2 â†’â„œ2 deï¬ned by T(x, y) = (x + y, âˆ’2x + 4y),
determine [T]B, where B is the basis B =
( 1
1

,
 1
2
)
.

248
Chapter 4
Vector Spaces
4.7.7. Let T : â„œ2 â†’â„œ3 be the linear transformation deï¬ned by
T(x, y) = (x + 3y, 0, 2x âˆ’4y).
(a)
Determine [T]SSâ€², where S and Sâ€² are the standard bases for
â„œ2 and â„œ3, respectively.
(b)
Determine [T]SSâ€²â€², where Sâ€²â€² is the basis for â„œ3 obtained by
permuting the standard basis according to Sâ€²â€² = {e3, e2, e1}.
4.7.8. Let T be the operator on â„œ3 deï¬ned by T(x, y, z) = (xâˆ’y, yâˆ’x, xâˆ’z)
and consider the vector
v =
ï£«
ï£­
1
1
2
ï£¶
ï£¸
and the basis
B =
ï£±
ï£²
ï£³
ï£«
ï£­
1
0
1
ï£¶
ï£¸,
ï£«
ï£­
0
1
1
ï£¶
ï£¸,
ï£«
ï£­
1
1
0
ï£¶
ï£¸
ï£¼
ï£½
ï£¾.
(a)
Determine [T]B and [v]B.
(b)
Compute [T(v)]B, and then verify that [T]B[v]B = [T(v)]B.
4.7.9. For A âˆˆâ„œnÃ—n, let T be the linear operator on â„œnÃ—1 deï¬ned by
T(x) = Ax. That is, T is the operator deï¬ned by matrix multiplica-
tion. With respect to the standard basis S, show that [T]S = A.
4.7.10. If T is a linear operator on a space V with basis B, explain why
[Tk]B = [T]k
B for all nonnegative integers k.
4.7.11. Let P be the projector that maps each point v âˆˆâ„œ2 to its orthogonal
projection on the line y = x as depicted in Figure 4.7.4.
y = x
P(v)
v
Figure 4.7.4
(a)
Determine the coordinate matrix of P with respect to the stan-
dard basis.
(b)
Determine the orthogonal projection of v =
 Î±
Î²

onto the line
y = x.

4.7 Linear Transformations
249
4.7.12. For the standard basis S =

1
0
0
0

,

0
1
0
0

,

0
0
1
0

,

0
0
0
1

of â„œ2Ã—2, determine the matrix representation [T]S for each of the fol-
lowing linear operators on â„œ2Ã—2, and then verify [T(U)]S = [T]S[U]S
for U =
 a
b
c
d

.
(a)
T(X2Ã—2) = X + XT
2
.
(b)
T(X2Ã—2) = AX âˆ’XA, where A =

1
1
âˆ’1
âˆ’1

.
4.7.13. For P2 and P3 (the spaces of polynomials of degrees less than or
equal to two and three, respectively), let S : P2 â†’P3 be the linear
transformation deï¬ned by S(p) =
& t
0 p(x)dx. Determine [S]BBâ€², where
B = {1, t, t2} and Bâ€² = {1, t, t2, t3}.
4.7.14. Let Q be the linear operator on â„œ2 that rotates each point counter-
clockwise through an angle Î¸, and let R be the linear operator on â„œ2
that reï¬‚ects each point about the x -axis.
(a)
Determine the matrix of the composition [RQ]S relative to the
standard basis S.
(b)
Relative to the standard basis, determine the matrix of the lin-
ear operator that rotates each point in â„œ2 counterclockwise
through an angle 2Î¸.
4.7.15. Let P : U â†’V and Q : U â†’V be two linear transformations, and let
B and Bâ€² be arbitrary bases for U and V, respectively.
(a)
Provide the details to explain why [P+Q]BBâ€² = [P]BBâ€²+[Q]BBâ€².
(b)
Provide the details to explain why [Î±P]BBâ€² = Î±[P]BBâ€², where
Î± is an arbitrary scalar.
4.7.16. Let I be the identity operator on an n -dimensional space V.
(a)
Explain why
[I]B =
ï£«
ï£¬
ï£¬
ï£­
1
0
Â· Â· Â·
0
0
1
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
1
ï£¶
ï£·
ï£·
ï£¸
regardless of the choice of basis B.
(b)
Let B = {xi}n
i=1 and Bâ€² = {yi}n
i=1 be two diï¬€erent bases for
V, and let T be the linear operator on V that maps vectors
from Bâ€² to vectors in B according to the rule T(yi) = xi for
i = 1, 2, . . . , n. Explain why
[I]BBâ€² = [T]B = [T]Bâ€² =

[x1]Bâ€²
 [x2]Bâ€²
 Â· Â· Â·
 [xn]Bâ€²

.

250
Chapter 4
Vector Spaces
(c)
When V = â„œ3, determine [I]BBâ€² for
B =
ï£±
ï£²
ï£³
ï£«
ï£­
1
0
0
ï£¶
ï£¸,
ï£«
ï£­
0
1
0
ï£¶
ï£¸,
ï£«
ï£­
0
0
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾,
Bâ€² =
ï£±
ï£²
ï£³
ï£«
ï£­
1
0
0
ï£¶
ï£¸,
ï£«
ï£­
1
1
0
ï£¶
ï£¸,
ï£«
ï£­
1
1
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾.
4.7.17. Let T : â„œ3 â†’â„œ3 be the linear operator deï¬ned by
T(x, y, z) = (2x âˆ’y, âˆ’x + 2y âˆ’z, z âˆ’y).
(a)
Determine Tâˆ’1(x, y, z).
(b)
Determine [Tâˆ’1]S, where S is the standard basis for â„œ3.
4.7.18. Let T be a linear operator on an n -dimensional space V. Show that
the following statements are equivalent.
(1)
Tâˆ’1 exists.
(2)
T is a one-to-one mapping (i.e., T(x) = T(y)
=â‡’
x = y ).
(3)
N (T) = {0}.
(4)
T is an onto mapping (i.e., for each v âˆˆV, there is an x âˆˆV
such that T(x) = v ).
Hint:
Show that (1) =â‡’(2) =â‡’(3) =â‡’(4) =â‡’(2),
and then show (2) and (4)
=â‡’
(1).
4.7.19. Let V be an n -dimensional space with a basis B = {ui}n
i=1.
(a)
Prove that a set of vectors {x1, x2, . . . , xr} âŠ†V is linearly
independent if and only if the set of coordinate vectors
(
[x1]B, [x2]B, . . . , [xr]B
)
âŠ†â„œnÃ—1
is a linearly independent set.
(b)
If T is a linear operator on V, then the range of T is the set
R (T) = {T(x) | x âˆˆV}.
Suppose that the basic columns of [T]B
occur in positions
b1, b2, . . . , br. Explain why

T(ub1), T(ub2), . . . , T(ubr)

is a
basis for R (T).

4.8 Change of Basis and Similarity
251
4.8
CHANGE OF BASIS AND SIMILARITY
By their nature, coordinate matrix representations are basis dependent. However,
itâ€™s desirable to study linear transformations without reference to particular bases
because some bases may force a coordinate matrix representation to exhibit
special properties that are not present in the coordinate matrix relative to other
bases. To divorce the study from the choice of bases itâ€™s necessary to somehow
identify properties of coordinate matrices that are invariant among all basesâ€”
these are properties intrinsic to the transformation itself, and they are the ones
on which to focus. The purpose of this section is to learn how to sort out these
basis-independent properties.
The discussion is limited to a single ï¬nite-dimensional space V and to linear
operators on V. Begin by examining how the coordinates of v âˆˆV change as
the basis for V changes. Consider two diï¬€erent bases
B = {x1, x2, . . . , xn}
and
Bâ€² = {y1, y2, . . . , yn} .
Itâ€™s convenient to regard B as an old basis for V and Bâ€² as a new basis for V.
Throughout this section T will denote the linear operator such that
T(yi) = xi for i = 1, 2, . . . , n.
(4.8.1)
T is called the change of basis operator because it maps the new basis vectors
in Bâ€² to the old basis vectors in B. Notice that [T]B = [T]Bâ€² = [I]BBâ€². To see
this, observe that
xi =
n

j=1
Î±jyj
=â‡’
T(xi) =
n

j=1
Î±jT(yj) =
n

j=1
Î±jxj,
which means [xi]Bâ€² = [T(xi)]B , so, according to (4.7.4),
[T]B =

[T(x1)]B [T(x2)]B Â· Â· Â· [T(xn)]B

=

[x1]Bâ€² [x2]Bâ€² Â· Â· Â· [xn]Bâ€²

= [T]Bâ€².
The fact that [I]BBâ€² = [T]B follows because [I(xi)]Bâ€² = [xi]Bâ€² . The matrix
P = [I]BBâ€² = [T]B = [T]Bâ€²
(4.8.2)
will hereafter be referred to as a change of basis matrix. Caution! [I]BBâ€² is
not necessarily the identity matrixâ€”see Exercise 4.7.16â€”and [I]BBâ€² Ì¸= [I]Bâ€²B.
We are now in a position to see how the coordinates of a vector change as
the basis for the underlying space changes.

252
Chapter 4
Vector Spaces
Changing Vector Coordinates
Let B = {x1, x2, . . . , xn} and Bâ€² = {y1, y2, . . . , yn} be bases for V,
and let T and P be the associated change of basis operator and change
of basis matrix, respectivelyâ€”i.e., T(yi) = xi, for each i, and
P = [T]B = [T]Bâ€² = [I]BBâ€² =

[x1]Bâ€²
 [x2]Bâ€²
 Â· Â· Â·
 [xn]Bâ€²

.
(4.8.3)
â€¢
[v]Bâ€² = P[v]B for all v âˆˆV.
(4.8.4)
â€¢
P is nonsingular.
â€¢
No other matrix can be used in place of P in (4.8.4).
Proof.
Use (4.7.6) to write [v]Bâ€² = [I(v)]Bâ€² = [I]BBâ€²[v]B = P[v]B, which is
(4.8.4). P is nonsingular because T is invertible (in fact, Tâˆ’1(xi) = yi), and
because (4.7.10) insures [Tâˆ’1]B = [T]âˆ’1
B
= Pâˆ’1. P is unique because if W is
another matrix satisfying (4.8.4) for all v âˆˆV, then (P âˆ’W)[v]B = 0 for all
v. Taking v = xi yields (P âˆ’W)ei = 0 for each i, so P âˆ’W = 0.
If we think of B as the old basis and Bâ€² as the new basis, then the change
of basis operator T acts as
T(new basis) = old basis,
while the change of basis matrix P acts as
new coordinates = P(old coordinates).
For this reason, T should be referred to as the change of basis operator from
Bâ€² to B, while P is called the change of basis matrix from B to Bâ€².
Example 4.8.1
Problem: For the space P2 of polynomials of degree 2 or less, determine the
change of basis matrix P from B to Bâ€², where
B = {1, t, t2}
and
Bâ€² = {1, 1 + t, 1 + t + t2},
and then ï¬nd the coordinates of q(t) = 3 + 2t + 4t2 relative to Bâ€².
Solution: According to (4.8.3), the change of basis matrix from B to Bâ€² is
P =

[x1]Bâ€²
 [x2]Bâ€²
 [x3]Bâ€²

.

4.8 Change of Basis and Similarity
253
In this case, x1 = 1,
x2 = t, and x3 = t2, and y1 = 1,
y2 = 1 + t, and
y3 = 1 + t + t2, so the coordinates [xi]Bâ€² are computed as follows:
1 =
1(1) + 0(1 + t) + 0(1 + t + t2) =
1y1 + 0y2 + 0y3,
t = âˆ’1(1) + 1(1 + t) + 0(1 + t + t2) = âˆ’1y1 + 1y2 + 0y3,
t2 =
0(1) âˆ’1(1 + t) + 1(1 + t + t2) =
0y1 âˆ’1y2 + 1y3.
Therefore,
P =

[x1]Bâ€²
 [x2]Bâ€²
 [x3]Bâ€²

=
ï£«
ï£­
1
âˆ’1
0
0
1
âˆ’1
0
0
1
ï£¶
ï£¸,
and the coordinates of q = q(t) = 3 + 2t + 4t2 with respect to Bâ€² are
[q]Bâ€² = P[q]B =
ï£«
ï£­
1
âˆ’1
0
0
1
âˆ’1
0
0
1
ï£¶
ï£¸
ï£«
ï£­
3
2
4
ï£¶
ï£¸=
ï£«
ï£­
1
âˆ’2
4
ï£¶
ï£¸.
To independently check that these coordinates are correct, simply verify that
q(t) = 1(1) âˆ’2(1 + t) + 4(1 + t + t2).
Itâ€™s now rather easy to describe how the coordinate matrix of a linear oper-
ator changes as the underlying basis changes.
Changing Matrix Coordinates
Let A be a linear operator on V, and let B and Bâ€² be two bases for
V. The coordinate matrices [A]B and [A]Bâ€² are related as follows.
[A]B = Pâˆ’1[A]Bâ€²P,
where
P = [I]BBâ€²
(4.8.5)
is the change of basis matrix from B to Bâ€². Equivalently,
[A]Bâ€² = Qâˆ’1[A]BQ,
where
Q = [I]Bâ€²B = Pâˆ’1
(4.8.6)
is the change of basis matrix from Bâ€² to B.

254
Chapter 4
Vector Spaces
Proof.
Let B = {x1, x2, . . . , xn} and Bâ€² = {y1, y2, . . . , yn} , and observe that
for each j, (4.7.6) can be used to write
*
A(xj)
+
Bâ€² = [A]Bâ€² [xj]Bâ€² = [A]Bâ€²Pâˆ—j =
*
[A]Bâ€²P
+
âˆ—j.
Now use the change of coordinates rule (4.8.4) together with the fact that
"
A(xj)
#
B =
"
[A]B
#
âˆ—j (see (4.7.4)) to write
*
A(xj)
+
Bâ€² = P
*
A(xj)
+
B = P
*
[A]B
+
âˆ—j =
*
P[A]B
+
âˆ—j.
Consequently,
"
[A]Bâ€²P
#
âˆ—j =
"
P[A]B
#
âˆ—j for each j, so [A]Bâ€²P = P[A]B. Since
the change of basis matrix P is nonsingular, it follows that [A]B = Pâˆ’1[A]Bâ€²P,
and (4.8.5) is proven. Setting Q = Pâˆ’1 in (4.8.5) yields [A]Bâ€² = Qâˆ’1[A]BQ.
The matrix Q = Pâˆ’1 is the change of basis matrix from Bâ€² to B because if T
is the change of basis operator from Bâ€² to B (i.e., T(yi) = xi ), then Tâˆ’1 is
the change of basis operator from B to Bâ€² (i.e., Tâˆ’1(xi) = yi ), and according
to (4.8.3), the change of basis matrix from Bâ€² to B is
[I]Bâ€²B =

[y1]B
 [y2]B
 Â· Â· Â·
 [yn]B

= [Tâˆ’1]B = [T]âˆ’1
B
= Pâˆ’1 = Q.
Example 4.8.2
Problem: Consider the linear operator A(x, y) = (y, âˆ’2x + 3y) on â„œ2 along
with the two bases
S =

1
0

,

0
1

and
Sâ€² =

1
1

,

1
2

.
First compute the coordinate matrix [A]S as well as the change of basis matrix
Q from Sâ€² to S, and then use these two matrices to determine [A]Sâ€².
Solution: The matrix of A relative to S is obtained by computing
A(e1) =A(1, 0) = (0, âˆ’2) = (0)e1 + (âˆ’2)e2,
A(e2) =A(0, 1) = (1, 3) = (1)e1 + (3)e2,
so that [A]S =

[A(e1)]S
 [A(e2)]S

=

0
1
âˆ’2
3

. According to (4.8.6), the
change of basis matrix from Sâ€² to S is
Q =

[y1]S
 [y2]S

=

1
1
1
2

,

4.8 Change of Basis and Similarity
255
and the matrix of A with respect to Sâ€² is
[A]Sâ€² = Qâˆ’1[A]SQ =

2
âˆ’1
âˆ’1
1
 
0
1
âˆ’2
3
 
1
1
1
2

=

1
0
0
2

.
Notice that [A]Sâ€² is a diagonal matrix, whereas [A]S is not. This shows that
the standard basis is not always the best choice for providing a simple matrix
representation. Finding a basis so that the associated coordinate matrix is as
simple as possible is one of the fundamental issues of matrix theory. Given an
operator A, the solution to the general problem of determining a basis B so
that [A]B is diagonal is summarized on p. 520.
Example 4.8.3
Problem: Consider a matrix MnÃ—n to be a linear operator on â„œn by deï¬ning
M(v) = Mv (matrixâ€“vector multiplication). If S is the standard basis for â„œn,
and if Sâ€² = {q1, q2, . . . , qn} is any other basis, describe [M]S and [M]Sâ€².
Solution: The jth column in [M]S is [Mej]S = [Mâˆ—j]S = Mâˆ—j, and hence
[M]S = M. That is, the coordinate matrix of M with respect to S is M itself.
To ï¬nd [M]Sâ€², use (4.8.6) to write [M]Sâ€² = Qâˆ’1[M]SQ = Qâˆ’1MQ, where
Q = [I]Sâ€²S =

[q1]S
 [q2]S
 Â· Â· Â·
 [qn]S

=

q1
 q2
 Â· Â· Â·
 qn

.
Conclusion: The matrices M and Qâˆ’1MQ represent the same linear operator
(namely, M), but with respect to two diï¬€erent bases (namely, S and Sâ€²). So,
when considering properties of M (as a linear operator), itâ€™s legitimate to replace
M by Qâˆ’1MQ. Whenever the structure of M obscures its operator properties,
look for a basis Sâ€² = {Qâˆ—1, Qâˆ—2, . . . , Qâˆ—n} (or, equivalently, a nonsingular matrix
Q) such that Qâˆ’1MQ has a simpler structure. This is an important theme
throughout linear algebra and matrix theory.
For a linear operator A, the special relationships between [A]B and [A]Bâ€²
that are given in (4.8.5) and (4.8.6) motivate the following deï¬nitions.
Similarity
â€¢
Matrices BnÃ—n and CnÃ—n are said to be similar matrices when-
ever there exists a nonsingular matrix Q such that B = Qâˆ’1CQ.
We write B â‰ƒC to denote that B and C are similar.
â€¢
The linear operator f : â„œnÃ—n â†’â„œnÃ—n deï¬ned by f(C) = Qâˆ’1CQ
is called a similarity transformation.

256
Chapter 4
Vector Spaces
Equations (4.8.5) and (4.8.6) say that any two coordinate matrices of a
given linear operator must be similar. But must any two similar matrices be
coordinate matrices of the same linear operator? Yes, and hereâ€™s why. Suppose
C = Qâˆ’1BQ, and let A(v) = Bv be the linear operator deï¬ned by matrixâ€“
vector multiplication. If S is the standard basis, then itâ€™s straightforward to see
that [A]S = B (Exercise 4.7.9). If Bâ€² = {Qâˆ—1, Qâˆ—2, . . . , Qâˆ—n} is the basis con-
sisting of the columns of Q, then (4.8.6) insures that [A]Bâ€² = [I]âˆ’1
Bâ€²S[A]S[I]Bâ€²S,
where
[I]Bâ€²S =

[Qâˆ—1]S
 [Qâˆ—2]S
 Â· Â· Â·
 [Qâˆ—n]S

= Q.
Therefore, B = [A]S and C = Qâˆ’1BQ = Qâˆ’1[A]SQ = [A]Bâ€², so B and
C are both coordinate matrix representations of A. In other words, similar
matrices represent the same linear operator.
As stated at the beginning of this section, the goal is to isolate and study
coordinate-independent properties of linear operators. They are the ones de-
termined by sorting out those properties of coordinate matrices that are ba-
sis independent. But, as (4.8.5) and (4.8.6) show, all coordinate matrices for a
given linear operator must be similar, so the coordinate-independent properties
are exactly the ones that are similarity invariant (invariant under similarity
transformations). Naturally, determining and studying similarity invariants is an
important part of linear algebra and matrix theory.
Example 4.8.4
Problem: The trace of a square matrix C was deï¬ned in Example 3.3.1 to be
the sum of the diagonal entries
trace (C) =

i
cii.
Show that trace is a similarity invariant, and explain why it makes sense to talk
about the trace of a linear operator without regard to any particular basis. Then
determine the trace of the linear operator on â„œ2 that is deï¬ned by
A(x, y) = (y, âˆ’2x + 3y).
(4.8.7)
Solution: As demonstrated in Example 3.6.5, trace (BC) = trace (CB), when-
ever the products are deï¬ned, so
trace

Qâˆ’1CQ

= trace

CQQâˆ’1
= trace (C),
and thus trace is a similarity invariant. This allows us to talk about the trace of
a linear operator A without regard to any particular basis because trace ([A]B)
is the same number regardless of the choice of B. For example, two coordinate
matrices of the operator A in (4.8.7) were computed in Example 4.8.2 to be
[A]S =

0
1
âˆ’2
3

and
[A]Sâ€² =

1
0
0
2

,
and itâ€™s clear that trace ([A]S) = trace ([A]Sâ€²) = 3. Since trace ([A]B) = 3 for
all B, we can legitimately deï¬ne trace (A) = 3.

4.8 Change of Basis and Similarity
257
Exercises for section 4.8
4.8.1. Explain why rank is a similarity invariant.
4.8.2. Explain why similarity is transitive in the sense that A â‰ƒB and B â‰ƒC
implies A â‰ƒC.
4.8.3. A(x, y, z) = (x + 2y âˆ’z, âˆ’y, x + 7z) is a linear operator on â„œ3.
(a)
Determine [A]S, where S is the standard basis.
(b)
Determine [A]Sâ€² as well as the nonsingular matrix Q such that
[A]Sâ€² = Qâˆ’1[A]SQ for Sâ€² =
 1
0
0

,
 1
1
0

,
 1
1
1

.
4.8.4. Let A =
 1
2
0
3
1
4
0
1
5

and B =
 1
1
1

,
 1
2
2

,
 1
2
3

. Consider A
as a linear operator on â„œnÃ—1 by means of matrix multiplication A(x) =
Ax, and determine [A]B.
4.8.5. Show that C =
 4
6
3
4

and B =
 âˆ’2
âˆ’3
6
10

are similar matrices, and
ï¬nd a nonsingular matrix Q such that C = Qâˆ’1BQ. Hint: Consider
B as a linear operator on â„œ2, and compute [B]S and [B]Sâ€², where S
is the standard basis, and Sâ€² =
(
2
âˆ’1

,
 âˆ’3
2
)
.
4.8.6. Let T be the linear operator T(x, y) = (âˆ’7x âˆ’15y, 6x + 12y). Find
a basis B such that [T]B =
 2
0
0
3

, and determine a matrix Q such
that [T]B = Qâˆ’1[T]SQ, where S is the standard basis.
4.8.7. By considering the rotator P(x, y) = (x cos Î¸ âˆ’y sin Î¸, x sin Î¸ + y cos Î¸)
described in Example 4.7.1 and Figure 4.7.1, show that the matrices
R =

cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸

and
D =

eiÎ¸
0
0
eâˆ’iÎ¸

are similar over the complex ï¬eld. Hint: In case you have forgotten (or
didnâ€™t know), eiÎ¸ = cos Î¸ + i sin Î¸.

258
Chapter 4
Vector Spaces
4.8.8. Let Î» be a scalar such that (C âˆ’Î»I)nÃ—n is singular.
(a)
If B â‰ƒC, prove that (B âˆ’Î»I) is also singular.
(b)
Prove that (B âˆ’Î»iI) is singular whenever BnÃ—n is similar to
D =
ï£«
ï£¬
ï£¬
ï£­
Î»1
0
Â· Â· Â·
0
0
Î»2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
Î»n
ï£¶
ï£·
ï£·
ï£¸.
4.8.9. If A â‰ƒB, show that Ak â‰ƒBk for all nonnegative integers k.
4.8.10. Suppose B = {x1, x2, . . . , xn} and Bâ€² = {y1, y2, . . . , yn} are bases for
an n -dimensional subspace V âŠ†â„œmÃ—1, and let XmÃ—n and YmÃ—n be
the matrices whose columns are the vectors from B and Bâ€², respectively.
(a)
Explain why YT Y is nonsingular, and prove that the change
of basis matrix from B to Bâ€² is P =

YT Y
âˆ’1YT X.
(b)
Describe P when m = n.
4.8.11.
(a)
N is nilpotent of index k when Nk = 0 but Nkâˆ’1 Ì¸= 0. If N
is a nilpotent operator of index n on â„œn, and if Nnâˆ’1(y) Ì¸= 0,
show B =

y, N(y), N2(y), . . . , Nnâˆ’1(y)

is a basis for â„œn,
and then demonstrate that
[N]B = J =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
0
Â· Â· Â·
0
0
1
0
Â· Â· Â·
0
0
0
1
Â· Â· Â·
0
0
...
...
...
...
...
0
0
Â· Â· Â·
1
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
(b)
If A and B are any two n Ã— n nilpotent matrices of index n,
explain why A â‰ƒB.
(c)
Explain why all n Ã— n nilpotent matrices of index n must have
a zero trace and be of rank n âˆ’1.
4.8.12. E is idempotent when E2 = E. For an idempotent operator E on â„œn,
let X = {xi}r
i=1 and Y = {yi}nâˆ’r
i=1
be bases for R (E) and N (E),
respectively.
(a)
Prove that B = X âˆªY is a basis for â„œn. Hint: Show Exi = xi
and use this to deduce that B is linearly independent.
(b)
Show that [E]B =
 Ir
0
0
0

.
(c)
Explain why two n Ã— n idempotent matrices of the same rank
must be similar.
(d)
If F is an idempotent matrix, prove that rank (F) = trace (F).

4.9 Invariant Subspaces
259
4.9
INVARIANT SUBSPACES
For a linear operator T on a vector space V, and for X âŠ†V,
T(X) = {T(x) | x âˆˆX}
is the set of all possible images of vectors from X under the transformation T.
Notice that T(V) = R (T). When X is a subspace of V, it follows that T(X)
is also a subspace of V, but T(X) is usually not related to X. However, in
some special cases it can happen that T(X) âŠ†X, and such subspaces are the
focus of this section.
Invariant Subspaces
â€¢
For a linear operator T on V, a subspace X âŠ†V is said to be an
invariant subspace under T whenever T(X) âŠ†X.
â€¢
In such a situation, T can be considered as a linear operator on X
by forgetting about everything else in V and restricting T to act
only on vectors from X. Hereafter, this restricted operator will
be denoted by T/X .
Example 4.9.1
Problem: For
A =
ï£«
ï£­
4
4
4
âˆ’2
âˆ’2
âˆ’5
1
2
5
ï£¶
ï£¸,
x1 =
ï£«
ï£­
2
âˆ’1
0
ï£¶
ï£¸,
and
x2 =
ï£«
ï£­
âˆ’1
2
âˆ’1
ï£¶
ï£¸,
show that the subspace X spanned by B = {x1, x2} is an invariant subspace
under A. Then describe the restriction A/X
and determine the coordinate
matrix of A/X relative to B.
Solution: Observe that Ax1 = 2x1 âˆˆX and Ax2 = x1 + 2x2 âˆˆX, so the
image of any x = Î±x1 + Î²x2 âˆˆX is back in X because
Ax = A(Î±x1+Î²x2) = Î±Ax1+Î²Ax2 = 2Î±x1+Î²(x1+2x2) = (2Î±+Î²)x1+2Î²x2.
This equation completely describes the action of A restricted to X, so
A/X (x) = (2Î± + Î²)x1 + 2Î²x2
for each
x = Î±x1 + Î²x2 âˆˆX.
Since A/X (x1) = 2x1 and A/X (x2) = x1 + 2x2, we have
*
A/X
+
B =
 *
A/X (x1)
+
B

*
A/X (x2)
+
B
 
=

2
1
0
2

.

260
Chapter 4
Vector Spaces
The invariant subspaces for a linear operator T are important because they
produce simpliï¬ed coordinate matrix representations of T. To understand how
this occurs, suppose X is an invariant subspace under T, and let
BX = {x1, x2, . . . , xr}
be a basis for X that is part of a basis
B = {x1, x2, . . . , xr, y1, y2, . . . , yq}
for the entire space V. To compute [T]B, recall from the deï¬nition of coordinate
matrices that
[T]B =

[T(x1)]B
 Â· Â· Â·
 [T(xr)]B
 [T(y1)]B
 Â· Â· Â·
 [T(yq)]B

.
(4.9.1)
Because each T(xj) is contained in X, only the ï¬rst r vectors from B are
needed to represent each T(xj), so, for j = 1, 2, . . . , r,
T(xj) =
r

i=1
Î±ijxi
and
[T(xj)]B =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î±1j
...
Î±rj
0
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
(4.9.2)
The space
Y = span {y1, y2, . . . , yq}
(4.9.3)
may not be an invariant subspace for T, so all the basis vectors in B may be
needed to represent the T(yj) â€™s. Consequently, for j = 1, 2, . . . , q,
T(yj) =
r

i=1
Î²ijxi +
q

i=1
Î³ijyi
and
[T(yj)]B =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î²1j
...
Î²rj
Î³1j
...
Î³qj
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
(4.9.4)
Using (4.9.2) and (4.9.4) in (4.9.1) produces the block-triangular matrix
[T]B =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î±11
Â· Â· Â·
Î±1r
Î²11
Â· Â· Â·
Î²1q
...
...
...
...
...
...
Î±r1
Â· Â· Â·
Î±rr
Î²r1
Â· Â· Â·
Î²rq
0
Â· Â· Â·
0
Î³11
Â· Â· Â·
Î³1q
...
...
...
...
...
...
0
Â· Â· Â·
0
Î³q1
Â· Â· Â·
Î³qq
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
(4.9.5)

4.9 Invariant Subspaces
261
The equations T(xj) = r
i=1 Î±ijxi in (4.9.2) mean that
*
T/X (xj)
+
BX
=
ï£«
ï£¬
ï£¬
ï£­
Î±1j
Î±2j
...
Î±rj
ï£¶
ï£·
ï£·
ï£¸,
so
*
T/X
+
BX
=
ï£«
ï£¬
ï£¬
ï£­
Î±11
Î±12
Â· Â· Â·
Î±1r
Î±21
Î±22
Â· Â· Â·
Î±2r
...
...
...
...
Î±r1
Î±r2
Â· Â· Â·
Î±rr
ï£¶
ï£·
ï£·
ï£¸,
and thus the matrix in (4.9.5) can be written as
[T]B =
 *
T/X
+
BX
BrÃ—q
0
CqÃ—q
 
.
(4.9.6)
In other words, (4.9.6) says that the matrix representation for T can be made
to be block triangular whenever a basis for an invariant subspace is available.
The more invariant subspaces we can ï¬nd, the more tools we have to con-
struct simpliï¬ed matrix representations. For example, if the space Y in (4.9.3)
is also an invariant subspace for T, then T(yj) âˆˆY for each j = 1, 2, . . . , q,
and only the yi â€™s are needed to represent T(yj) in (4.9.4). Consequently, the
Î²ij â€™s are all zero, and [T]B has the block-diagonal form
[T]B =

ArÃ—r
0
0
CqÃ—q

=
ï£«
ï£­
*
T/X
+
Bx
0
0
*
T/Y
+
By
ï£¶
ï£¸.
This notion easily generalizes in the sense that if B = BX âˆªBY âˆªÂ· Â· Â·âˆªBZ is a basis
for V, where BX , BY, . . . , BZ are bases for invariant subspaces under T that
have dimensions r1, r2, . . . , rk, respectively, then [T]B has the block-diagonal
form
[T]B =
ï£«
ï£¬
ï£¬
ï£­
Ar1Ã—r1
0
Â· Â· Â·
0
0
Br2Ã—r2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
CrkÃ—rk
ï£¶
ï£·
ï£·
ï£¸,
where
A =
*
T/X
+
Bx
,
B =
*
T/Y
+
By
,
. . . ,
C =
*
T/Z
+
Bz
.
The situations discussed above are also reversible in the sense that if the
matrix representation of T has a block-triangular form
[T]B =

ArÃ—r
BrÃ—q
0
CqÃ—q

relative to some basis
B = {u1, u2, . . . , ur, w1, w2, . . . , wq},

262
Chapter 4
Vector Spaces
then the r -dimensional subspace U = span {u1, u2, . . . , ur} spanned by the
ï¬rst r vectors in B must be an invariant subspace under T. Furthermore, if
the matrix representation of T has a block-diagonal form
[T]B =

ArÃ—r
0
0
CqÃ—q

relative to B, then both
U = span {u1, u2, . . . , ur}
and
W = span {w1, w2, . . . , wq}
must be invariant subspaces for T. The details are left as exercises.
The general statement concerning invariant subspaces and coordinate ma-
trix representations is given below.
Invariant Subspaces and Matrix Representations
Let T be a linear operator on an n-dimensional space V, and let
X, Y, . . . , Z be subspaces of V with respective dimensions r1, r2, . . . , rk
and bases BX , BY, . . . , BZ. Furthermore, suppose that 
i ri = n and
B = BX âˆªBY âˆªÂ· Â· Â· âˆªBZ is a basis for V.
â€¢
The subspace X is an invariant subspace under T if and only if
[T]B has the block-triangular form
[T]B =

Ar1Ã—r1
B
0
C

,
in which case
A =
*
T/X
+
BX
.
(4.9.7)
â€¢
The subspaces X, Y, . . . , Z are all invariant under T if and only if
[T]B has the block-diagonal form
[T]B =
ï£«
ï£¬
ï£¬
ï£­
Ar1Ã—r1
0
Â· Â· Â·
0
0
Br2Ã—r2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
CrkÃ—rk
ï£¶
ï£·
ï£·
ï£¸,
(4.9.8)
in which case
A =
*
T/X
+
Bx
,
B =
*
T/Y
+
By
,
. . . ,
C =
*
T/Z
+
Bz
.
An important corollary concerns the special case in which the linear operator
T is in fact an n Ã— n matrix and T(v) = Tv is a matrixâ€“vector multiplication.

4.9 Invariant Subspaces
263
Triangular and Diagonal Block Forms
When T is an n Ã— n matrix, the following two statements are true.
â€¢
Q is a nonsingular matrix such that
Qâˆ’1TQ =

ArÃ—r
BrÃ—q
0
CqÃ—q

(4.9.9)
if and only if the ï¬rst r columns in Q span an invariant subspace
under T.
â€¢
Q is a nonsingular matrix such that
Qâˆ’1TQ =
ï£«
ï£¬
ï£¬
ï£­
Ar1Ã—r1
0
Â· Â· Â·
0
0
Br2Ã—r2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
CrkÃ—rk
ï£¶
ï£·
ï£·
ï£¸
(4.9.10)
if and only if Q =

Q1
 Q2
 Â· Â· Â·
 Qk

in which Qi is n Ã— ri, and
the columns of each Qi span an invariant subspace under T.
Proof.
We know from Example 4.8.3 that if B = {q1, q2, . . . , qn} is a basis for
â„œn, and if Q =

q1
 q2
 Â· Â· Â·
 qn

is the matrix containing the vectors from B
as its columns, then [T]B = Qâˆ’1TQ. Statements (4.9.9) and (4.9.10) are now
direct consequences of statements (4.9.7) and (4.9.8), respectively.
Example 4.9.2
Problem: For
T =
ï£«
ï£¬
ï£­
âˆ’1
âˆ’1
âˆ’1
âˆ’1
0
âˆ’5
âˆ’16
âˆ’22
0
3
10
14
4
8
12
14
ï£¶
ï£·
ï£¸,
q1 =
ï£«
ï£¬
ï£­
2
âˆ’1
0
0
ï£¶
ï£·
ï£¸,
and
q2 =
ï£«
ï£¬
ï£­
âˆ’1
2
âˆ’1
0
ï£¶
ï£·
ï£¸,
verify that X = span {q1, q2} is an invariant subspace under T, and then ï¬nd
a nonsingular matrix Q such that Qâˆ’1TQ has the block-triangular form
Qâˆ’1TQ =
ï£«
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
0
âˆ—
âˆ—
0
0
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£¸.

264
Chapter 4
Vector Spaces
Solution: X is invariant because Tq1 = q1+3q2 and Tq2 = 2q1+4q2 insure
that for all Î± and Î², the images
T(Î±q1 + Î²q2) = (Î± + 2Î²)q1 + (3Î± + 4Î²)q2
lie in X. The desired matrix Q is constructed by extending {q1, q2} to a basis
B = {q1, q2, q3, q4} for â„œ4. If the extension technique described in Solution 2
of Example 4.4.5 is used, then
q3 =
ï£«
ï£¬
ï£­
1
0
0
0
ï£¶
ï£·
ï£¸
and
q4 =
ï£«
ï£¬
ï£­
0
0
0
1
ï£¶
ï£·
ï£¸,
and
Q =

q1
 q2
 q3
 q4

=
ï£«
ï£¬
ï£­
2
âˆ’1
1
0
âˆ’1
2
0
0
0
âˆ’1
0
0
0
0
0
1
ï£¶
ï£·
ï£¸.
Since the ï¬rst two columns of Q span a space that is invariant under T, it
follows from (4.9.9) that Qâˆ’1TQ must be in block-triangular form. This is easy
to verify by computing
Qâˆ’1 =
ï£«
ï£¬
ï£­
0
âˆ’1
âˆ’2
0
0
0
âˆ’1
0
1
2
3
0
0
0
0
1
ï£¶
ï£·
ï£¸
and
Qâˆ’1TQ =
ï£«
ï£¬
ï£¬
ï£­
1
2
0
âˆ’6
3
4
0
âˆ’14
0
0
âˆ’1
âˆ’3
0
0
4
14
ï£¶
ï£·
ï£·
ï£¸.
In passing, notice that the upper-left-hand block is
*
T/X
+
{q1,q2} =

1
2
3
4

.
Example 4.9.3
Consider again the matrices of Example 4.9.2:
T =
ï£«
ï£¬
ï£­
âˆ’1
âˆ’1
âˆ’1
âˆ’1
0
âˆ’5
âˆ’16
âˆ’22
0
3
10
14
4
8
12
14
ï£¶
ï£·
ï£¸,
q1 =
ï£«
ï£¬
ï£­
2
âˆ’1
0
0
ï£¶
ï£·
ï£¸,
and
q2 =
ï£«
ï£¬
ï£­
âˆ’1
2
âˆ’1
0
ï£¶
ï£·
ï£¸.
There are inï¬nitely many extensions of {q1, q2} to a basis B = {q1, q2, q3, q4}
for â„œ4 â€”the extension used in Example 4.9.2 is only one possibility. Another
extension is
q3 =
ï£«
ï£¬
ï£­
0
âˆ’1
2
âˆ’1
ï£¶
ï£·
ï£¸
and
q4 =
ï£«
ï£¬
ï£­
0
0
âˆ’1
1
ï£¶
ï£·
ï£¸.

4.9 Invariant Subspaces
265
This extension might be preferred over that of Example 4.9.2 because the spaces
X = span {q1, q2} and Y = span {q3, q4} are both invariant under T, and
therefore it follows from (4.9.10) that Qâˆ’1TQ is block diagonal. Indeed, it is
not diï¬ƒcult to verify that
Qâˆ’1TQ =
ï£«
ï£¬
ï£­
1
1
1
1
1
2
2
2
1
2
3
3
1
2
3
4
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
âˆ’1
âˆ’1
âˆ’1
âˆ’1
0
âˆ’5
âˆ’16
âˆ’22
0
3
10
14
4
8
12
14
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
2
âˆ’1
0
0
âˆ’1
2
âˆ’1
0
0
âˆ’1
2
âˆ’1
0
0
âˆ’1
1
ï£¶
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£­
1
2
0
0
3
4
0
0
0
0
5
6
0
0
7
8
ï£¶
ï£·
ï£·
ï£¸.
Notice that the diagonal blocks must be the matrices of the restrictions in the
sense that

1
2
3
4

=
*
T/X
+
{q1,q2}
and

5
6
7
8

=
*
T/Y
+
{q3,q4} .
Example 4.9.4
Problem: Find all subspaces of â„œ2 that are invariant under
A =

0
1
âˆ’2
3

.
Solution: The trivial subspace {0} is the only zero-dimensional invariant sub-
space, and the entire space â„œ2 is the only two-dimensional invariant subspace.
The real problem is to ï¬nd all one-dimensional invariant subspaces. If M is a
one-dimensional subspace spanned by x Ì¸= 0 such that A(M) âŠ†M, then
Ax âˆˆM
=â‡’
there is a scalar Î» such that Ax = Î»x
=â‡’
(A âˆ’Î»I) x = 0.
In other words, M âŠ†N (A âˆ’Î»I) . Since dim M = 1, it must be the case that
N (A âˆ’Î»I) Ì¸= {0}, and consequently Î» must be a scalar such that (A âˆ’Î»I) is
a singular matrix. Row operations produce
A âˆ’Î»I =

âˆ’Î»
1
âˆ’2
3 âˆ’Î»

âˆ’â†’

âˆ’2
3 âˆ’Î»
âˆ’Î»
1

âˆ’â†’

âˆ’2
3 âˆ’Î»
0
1 + (Î»2 âˆ’3Î»)/2

,
and it is clear that (A âˆ’Î»I) is singular if and only if 1 + (Î»2 âˆ’3Î»)/2 = 0 â€”i.e.,
if and only if Î» is a root of
Î»2 âˆ’3Î» + 2 = 0.

266
Chapter 4
Vector Spaces
Thus Î» = 1 and Î» = 2, and straightforward computation yields the two one-
dimensional invariant subspaces
M1 = N (A âˆ’I) = span

1
1

and
M2 = N (A âˆ’2I) = span

1
2

.
In passing, notice that B =
( 1
1

,
 1
2
)
is a basis for â„œ2, and
[A]B = Qâˆ’1AQ =

1
0
0
2

,
where
Q =

1
1
1
2

.
In general, scalars Î» for which (A âˆ’Î»I) is singular are called the eigenvalues
of A, and the nonzero vectors in N (A âˆ’Î»I) are known as the associated
eigenvectors for A. As this example indicates, eigenvalues and eigenvectors
are of fundamental importance in identifying invariant subspaces and reducing
matrices by means of similarity transformations. Eigenvalues and eigenvectors
are discussed at length in Chapter 7.
Exercises for section 4.9
4.9.1. Let T be an arbitrary linear operator on a vector space V.
(a)
Is the trivial subspace {0} invariant under T?
(b)
Is the entire space V invariant under T?
4.9.2. Describe all of the subspaces that are invariant under the identity oper-
ator I on a space V.
4.9.3. Let T be the linear operator on â„œ4 deï¬ned by
T(x1, x2, x3, x4) = (x1 + x2 + 2x3 âˆ’x4,
x2 + x4,
2x3 âˆ’x4,
x3 + x4),
and let X = span {e1, e2} be the subspace that is spanned by the ï¬rst
two unit vectors in â„œ4.
(a)
Explain why X is invariant under T.
(b)
Determine
"
T/X
#
{e1,e2}.
(c)
Describe the structure of [T]B, where B is any basis obtained
from an extension of {e1, e2} .

4.9 Invariant Subspaces
267
4.9.4. Let T and Q be the matrices
T =
ï£«
ï£¬
ï£­
âˆ’2
âˆ’1
âˆ’5
âˆ’2
âˆ’9
0
âˆ’8
âˆ’2
2
3
11
5
3
âˆ’5
âˆ’13
âˆ’7
ï£¶
ï£·
ï£¸
and
Q =
ï£«
ï£¬
ï£­
1
0
0
âˆ’1
1
1
3
âˆ’4
âˆ’2
0
1
0
3
âˆ’1
âˆ’4
3
ï£¶
ï£·
ï£¸.
(a)
Explain why the columns of Q are a basis for â„œ4.
(b)
Verify that X = span {Qâˆ—1, Qâˆ—2} and Y = span {Qâˆ—3, Qâˆ—4}
are each invariant subspaces under T.
(c)
Describe the structure of Qâˆ’1TQ without doing any compu-
tation.
(d)
Now compute the product Qâˆ’1TQ to determine
*
T/X
+
{Qâˆ—1,Qâˆ—2}
and
*
T/Y
+
{Qâˆ—3,Qâˆ—4} .
4.9.5. Let T be a linear operator on a space V, and suppose that
B = {u1, . . . , ur, w1, . . . , wq}
is a basis for V such that [T]B has the block-diagonal form
[T]B =

ArÃ—r
0
0
CqÃ—q

.
Explain why U = span {u1, . . . , ur} and W = span {w1, . . . , wq} must
each be invariant subspaces under T.
4.9.6. If TnÃ—n and PnÃ—n are matrices such that
Pâˆ’1TP =

ArÃ—r
0
0
CqÃ—q

,
explain why
U = span {Pâˆ—1, . . . , Pâˆ—r}
and
W = span {Pâˆ—r+1, . . . , Pâˆ—n}
are each invariant subspaces under T.
4.9.7. If A is an n Ã— n matrix and Î» is a scalar such that (A âˆ’Î»I) is
singular (i.e., Î» is an eigenvalue), explain why the associated space of
eigenvectors N (A âˆ’Î»I) is an invariant subspace under A.
4.9.8. Consider the matrix A =
 âˆ’9
4
âˆ’24
11

.
(a)
Determine the eigenvalues of A.
(b)
Identify all subspaces of â„œ2 that are invariant under A.
(c)
Find a nonsingular matrix Q such that Qâˆ’1AQ is a diagonal
matrix.

CHAPTER 5
Norms,
Inner Products,
and Orthogonality
5.1
VECTOR NORMS
A signiï¬cant portion of linear algebra is in fact geometric in nature because
much of the subject grew out of the need to generalize the basic geometry of
â„œ2 and â„œ3 to nonvisual higher-dimensional spaces. The usual approach is to
coordinatize geometric concepts in â„œ2 and â„œ3, and then extend statements
concerning ordered pairs and triples to ordered n-tuples in â„œn and Cn.
For example, the length of a vector u âˆˆâ„œ2 or v âˆˆâ„œ3 is obtained from
the Pythagorean theorem by computing the length of the hypotenuse of a right
triangle as shown in Figure 5.1.1.
x
y
u = (x,y)
x2 + y2
||u|| =
x
y
z
v = (x,y,z)
x2 + y2 + z2
||v|| =
Figure 5.1.1
This measure of length,
âˆ¥uâˆ¥=

x2 + y2
and
âˆ¥vâˆ¥=

x2 + y2 + z2,

270
Chapter 5
Norms, Inner Products, and Orthogonality
is called the euclidean norm in â„œ2 and â„œ3, and there is an obvious extension
to higher dimensions.
Euclidean Vector Norm
For a vector xnÃ—1, the euclidean norm of x is deï¬ned to be
â€¢
âˆ¥xâˆ¥=
 n
i=1 x2
i
1/2
=
âˆš
xT x whenever x âˆˆâ„œnÃ—1,
â€¢
âˆ¥xâˆ¥=
 n
i=1 |xi|21/2
=
âˆš
xâˆ—x whenever x âˆˆCnÃ—1.
For example, if u =
ï£«
ï£¬
ï£­
0
âˆ’1
2
âˆ’2
4
ï£¶
ï£·
ï£¸and v =
ï£«
ï£¬
ï£­
i
2
1 âˆ’i
0
1 + i
ï£¶
ï£·
ï£¸, then
âˆ¥uâˆ¥=

u2
i =
âˆš
uT u =
âˆš
0 + 1 + 4 + 4 + 16 = 5,
âˆ¥vâˆ¥=

|vi|2 =
âˆš
vâˆ—v =
âˆš
1 + 4 + 2 + 0 + 2 = 3.
There are several points to note.
33
â€¢
The complex version of âˆ¥xâˆ¥includes the real version as a special case because
|z|2 = z2 whenever z is a real number. Recall that if z = a + ib, then
Â¯z = a âˆ’ib, and the magnitude of z is |z| = âˆšÂ¯zz =
âˆš
a2 + b2. The fact that
|z|2 = Â¯zz = a2 + b2 is a real number insures that âˆ¥xâˆ¥is real even if x has
some complex components.
â€¢
The deï¬nition of euclidean norm guarantees that for all scalars Î±,
âˆ¥xâˆ¥â‰¥0,
âˆ¥xâˆ¥= 0 â‡â‡’x = 0,
and
âˆ¥Î±xâˆ¥= |Î±| âˆ¥xâˆ¥.
(5.1.1)
â€¢
Given a vector x Ì¸= 0, itâ€™s frequently convenient to have another vector
that points in the same direction as x (i.e., is a positive multiple of x) but
has unit length. To construct such a vector, we normalize x by setting
u = x/ âˆ¥xâˆ¥. From (5.1.1), itâ€™s easy to see that
âˆ¥uâˆ¥=

x
âˆ¥xâˆ¥
 =
1
âˆ¥xâˆ¥âˆ¥xâˆ¥= 1.
(5.1.2)
33
By convention, column vectors are used throughout this chapter. But there is nothing special
about columns because, with the appropriate interpretation, all statements concerning columns
will also hold for rows.

5.1 Vector Norms
271
â€¢
The distance between vectors in â„œ3 can be visualized with the aid of the
parallelogram law as shown in Figure 5.1.2, so for vectors in â„œn and Cn,
the distance between u and v is naturally deï¬ned to be âˆ¥u âˆ’vâˆ¥.
v
u
u - v
||u - v||
Figure 5.1.2
Standard Inner Product
The scalar terms deï¬ned by
xT y =
n

i=1
xiyi âˆˆâ„œ
and
xâˆ—y =
n

i=1
Â¯xiyi âˆˆC
are called the standard inner products for â„œn and Cn, respectively.
The Cauchyâ€“Bunyakovskiiâ€“Schwarz (CBS) inequality
34 is one of the most
important inequalities in mathematics. It relates inner product to norm.
34
The Cauchyâ€“Bunyakovskiiâ€“Schwarz inequality is named in honor of the three men who played
a role in its development. The basic inequality for real numbers is attributed to Cauchy in 1821,
whereas Schwarz and Bunyakovskii contributed by later formulating useful generalizations of
the inequality involving integrals of functions.
Augustin-Louis Cauchy (1789â€“1857) was a French mathematician who is generally regarded
as being the founder of mathematical analysisâ€”including the theory of complex functions.
Although deeply embroiled in political turmoil for much of his life (he was a partisan of the
Bourbons), Cauchy emerged as one of the most proliï¬c mathematicians of all time. He authored
at least 789 mathematical papers, and his collected works ï¬ll 27 volumesâ€”this is on a par with
Cayley and second only to Euler. It is said that more theorems, concepts, and methods bear
Cauchyâ€™s name than any other mathematician.
Victor Bunyakovskii (1804â€“1889) was a Russian professor of mathematics at St. Petersburg, and
in 1859 he extended Cauchyâ€™s inequality for discrete sums to integrals of continuous functions.
His contribution was overlooked by western mathematicians for many years, and his name is
often omitted in classical texts that simply refer to the Cauchyâ€“Schwarz inequality.
Hermann Amandus Schwarz (1843â€“1921) was a student and successor of the famous German
mathematician Karl Weierstrass at the University of Berlin. Schwarz independently generalized
Cauchyâ€™s inequality just as Bunyakovskii had done earlier.

272
Chapter 5
Norms, Inner Products, and Orthogonality
Cauchyâ€“Bunyakovskiiâ€“Schwarz (CBS) Inequality
|xâˆ—y| â‰¤âˆ¥xâˆ¥âˆ¥yâˆ¥
for all x, y âˆˆCnÃ—1.
(5.1.3)
Equality holds if and only if y = Î±x for Î± = xâˆ—y/xâˆ—x.
Proof.
Set Î± = xâˆ—y/xâˆ—x = xâˆ—y/ âˆ¥xâˆ¥2 (assume x Ì¸= 0 because there is nothing
to prove if x = 0) and observe that xâˆ—(Î±x âˆ’y) = 0, so
0 â‰¤âˆ¥Î±x âˆ’yâˆ¥2 = (Î±x âˆ’y)âˆ—(Î±x âˆ’y) = Â¯Î±xâˆ—(Î±x âˆ’y) âˆ’yâˆ—(Î±x âˆ’y)
= âˆ’yâˆ—(Î±x âˆ’y) = yâˆ—y âˆ’Î±yâˆ—x = âˆ¥yâˆ¥2 âˆ¥xâˆ¥2 âˆ’(xâˆ—y) (yâˆ—x)
âˆ¥xâˆ¥2
.
(5.1.4)
Since yâˆ—x = xâˆ—y, it follows that (xâˆ—y) (yâˆ—x) = |xâˆ—y|2 , so
0 â‰¤âˆ¥yâˆ¥2 âˆ¥xâˆ¥2 âˆ’|xâˆ—y|2
âˆ¥xâˆ¥2
.
Now, 0 < âˆ¥xâˆ¥2 implies 0 â‰¤âˆ¥yâˆ¥2 âˆ¥xâˆ¥2 âˆ’|xâˆ—y|2 , and thus the CBS inequality
is obtained. Establishing the conditions for equality is Exercise 5.1.9.
One reason that the CBS inequality is important is because it helps to
establish that the geometry in higher-dimensional spaces is consistent with the
geometry in the visual spaces â„œ2 and â„œ3. In particular, consider the situation
depicted in Figure 5.1.3.
x
y
x + y
||x||
||y||
||x + y||
Figure 5.1.3
Imagine traveling from the origin to the point x and then moving from x to the
point x+y. Clearly, you have traveled a distance that is at least as great as the
direct distance from the origin to x+y along the diagonal of the parallelogram.
In other words, itâ€™s visually evident that âˆ¥x + yâˆ¥â‰¤âˆ¥xâˆ¥+âˆ¥yâˆ¥. This observation

5.1 Vector Norms
273
is known as the triangle inequality. In higher-dimensional spaces we do not
have the luxury of visualizing the geometry with our eyes, and the question of
whether or not the triangle inequality remains valid has no obvious answer. The
CBS inequality is precisely what is required to prove that, in this respect, the
geometry of higher dimensions is no diï¬€erent than that of the visual spaces.
Triangle Inequality
âˆ¥x + yâˆ¥â‰¤âˆ¥xâˆ¥+ âˆ¥yâˆ¥
for every x, y âˆˆCn.
Proof.
Consider x and y to be column vectors, and write
âˆ¥x + yâˆ¥2 = (x + y)âˆ—(x + y) = xâˆ—x + xâˆ—y + yâˆ—x + yâˆ—y
= âˆ¥xâˆ¥2 + xâˆ—y + yâˆ—x + âˆ¥yâˆ¥2 .
(5.1.5)
Recall that if z = a + ib, then z + Â¯z = 2a = 2 Re (z) and |z|2 = a2 + b2 â‰¥a2,
so that |z| â‰¥Re (z) . Using the fact that yâˆ—x = xâˆ—y together with the CBS
inequality yields
xâˆ—y + yâˆ—x = 2 Re (xâˆ—y) â‰¤2 |xâˆ—y| â‰¤2 âˆ¥xâˆ¥âˆ¥yâˆ¥.
Consequently, we may infer from (5.1.5) that
âˆ¥x + yâˆ¥2 â‰¤âˆ¥xâˆ¥2 + 2 âˆ¥xâˆ¥âˆ¥yâˆ¥+ âˆ¥yâˆ¥2 = (âˆ¥xâˆ¥+ âˆ¥yâˆ¥)2 .
Itâ€™s not diï¬ƒcult to see that the triangle inequality can be extended to any
number of vectors in the sense that
 
i xi
 â‰¤
i âˆ¥xiâˆ¥. Furthermore, it follows
as a corollary that for real or complex numbers,
 
i Î±i
 â‰¤
i |Î±i| (the triangle
inequality for scalars).
Example 5.1.1
Backward Triangle Inequality. The triangle inequality produces an upper
bound for a sum, but it also yields the following lower bound for a diï¬€erence:
 âˆ¥xâˆ¥âˆ’âˆ¥yâˆ¥
 â‰¤âˆ¥x âˆ’yâˆ¥.
(5.1.6)
This is a consequence of the triangle inequality because
âˆ¥xâˆ¥= âˆ¥x âˆ’y + yâˆ¥â‰¤âˆ¥x âˆ’yâˆ¥+ âˆ¥yâˆ¥
=â‡’
âˆ¥xâˆ¥âˆ’âˆ¥yâˆ¥â‰¤âˆ¥x âˆ’yâˆ¥
and
âˆ¥yâˆ¥= âˆ¥x âˆ’y âˆ’xâˆ¥â‰¤âˆ¥x âˆ’yâˆ¥+ âˆ¥xâˆ¥
=â‡’
âˆ’(âˆ¥xâˆ¥âˆ’âˆ¥yâˆ¥) â‰¤âˆ¥x âˆ’yâˆ¥.

274
Chapter 5
Norms, Inner Products, and Orthogonality
There are notions of length other than the euclidean measure. For example,
urban dwellers navigate on a grid of city blocks with one-way streets, so they are
prone to measure distances in the city not as the crow ï¬‚ies but rather in terms
of lengths on a directed grid. For example, instead of than saying that â€œitâ€™s a
one-half mile straight-line (euclidean) trip from here to there,â€ they are more
apt to describe the length of the trip by saying, â€œitâ€™s two blocks north on Dan
Allen Drive, four blocks west on Hillsborough Street, and ï¬ve blocks south on
Gorman Street.â€ In other words, the length of the trip is 2 + | âˆ’4| + | âˆ’5| = 11
blocksâ€”absolute value is used to insure that southerly and westerly movement
does not cancel the eï¬€ect of northerly and easterly movement, respectively. This
â€œgrid normâ€ is better known as the 1-norm because it is a special case of a more
general class of norms deï¬ned below.
p-Norms
For p â‰¥1, the p-norm of x âˆˆCn is deï¬ned as âˆ¥xâˆ¥p = (n
i=1 |xi|p)1/p.
It can be proven that the following properties of the euclidean norm are in
fact valid for all p-norms:
âˆ¥xâˆ¥p â‰¥0
and
âˆ¥xâˆ¥p = 0 â‡â‡’x = 0,
âˆ¥Î±xâˆ¥p = |Î±| âˆ¥xâˆ¥p
for all scalars Î±,
âˆ¥x + yâˆ¥p â‰¤âˆ¥xâˆ¥p + âˆ¥yâˆ¥p
(see Exercise 5.1.13).
(5.1.7)
The generalized version of the CBS inequality (5.1.3) for p-norms is HÂ¨olderâ€™s
inequality (developed in Exercise 5.1.12), which states that if p > 1 and q > 1
are real numbers such that 1/p + 1/q = 1, then
|xâˆ—y| â‰¤âˆ¥xâˆ¥p âˆ¥yâˆ¥q .
(5.1.8)
In practice, only three of the p-norms are used, and they are
âˆ¥xâˆ¥1 =
n

i=1
|xi|
(the grid norm),
âˆ¥xâˆ¥2 =
 n

i=1
|xi|2
1/2
(the euclidean norm),
and
âˆ¥xâˆ¥âˆ= lim
pâ†’âˆâˆ¥xâˆ¥p = lim
pâ†’âˆ
 n

i=1
|xi|p
1/p
= max
i
|xi|
(the max norm).
For example, if x = (3, 4âˆ’3i, 1), then âˆ¥xâˆ¥1 = 9, âˆ¥xâˆ¥2 =
âˆš
35, and âˆ¥xâˆ¥âˆ= 5.

5.1 Vector Norms
275
To see that limpâ†’âˆâˆ¥xâˆ¥p = maxi |xi| , proceed as follows. Relabel the en-
tries of x by setting Ëœx1 = maxi |xi| , and if there are other entries with this
same maximal magnitude, label them Ëœx2, . . . , Ëœxk. Label any remaining coordi-
nates as Ëœxk+1 Â· Â· Â· Ëœxn. Consequently, |Ëœxi/Ëœx1| < 1 for i = k + 1, . . . , n, so, as
p â†’âˆ,
âˆ¥xâˆ¥p =
 n

i=1
|Ëœxi|p
1/p
= |Ëœx1|

k +

Ëœxk+1
Ëœx1

p
+ Â· Â· Â· +

Ëœxn
Ëœx1

p1/p
â†’|Ëœx1| .
Example 5.1.2
To get a feel for the 1-, 2-, and âˆ-norms, it helps to know the shapes and relative
sizes of the unit p-spheres Sp = {x | âˆ¥xâˆ¥p = 1} for p = 1, 2, âˆ. As illustrated
in Figure 5.1.4, the unit 1-, 2-, and âˆ-spheres in â„œ3 are an octahedron, a ball,
and a cube, respectively, and itâ€™s visually evident that S1 ï¬ts inside S2, which
in turn ï¬ts inside Sâˆ. This means that âˆ¥xâˆ¥1 â‰¥âˆ¥xâˆ¥2 â‰¥âˆ¥xâˆ¥âˆfor all x âˆˆâ„œ3.
In general, this is true in â„œn (Exercise 5.1.8).
S1
S2
Sâˆ
Figure 5.1.4
Because the p-norms are deï¬ned in terms of coordinates, their use is limited
to coordinate spaces. But itâ€™s desirable to have a general notion of norm that
works for all vector spaces. In other words, we need a coordinate-free deï¬nition
of norm that includes the standard p-norms as a special case. Since all of the p-
norms satisfy the properties (5.1.7), itâ€™s natural to use these properties to extend
the concept of norm to general vector spaces.
General Vector Norms
A norm for a real or complex vector space V is a function âˆ¥â‹†âˆ¥mapping
V into â„œthat satisï¬es the following conditions.
âˆ¥xâˆ¥â‰¥0
and
âˆ¥xâˆ¥= 0 â‡â‡’x = 0,
âˆ¥Î±xâˆ¥= |Î±| âˆ¥xâˆ¥
for all scalars Î±,
âˆ¥x + yâˆ¥â‰¤âˆ¥xâˆ¥+ âˆ¥yâˆ¥.
(5.1.9)

276
Chapter 5
Norms, Inner Products, and Orthogonality
Example 5.1.3
Equivalent Norms. Vector norms are basic tools for deï¬ning and analyzing
limiting behavior in vector spaces V. A sequence {xk} âŠ‚V is said to converge
to x (write xk â†’x ) if âˆ¥xk âˆ’xâˆ¥â†’0. This depends on the choice of the norm,
so, ostensibly, we might have xk â†’x with one norm but not with another.
Fortunately, this is impossible in ï¬nite-dimensional spaces because all norms are
equivalent in the following sense.
Problem: For each pair of norms, âˆ¥â‹†âˆ¥a , âˆ¥â‹†âˆ¥b , on an n-dimensional space V,
exhibit positive constants Î± and Î² (depending only on the norms) such that
Î± â‰¤âˆ¥xâˆ¥a
âˆ¥xâˆ¥b
â‰¤Î²
for all nonzero vectors in V.
(5.1.10)
Solution: For Sb = {y | âˆ¥yâˆ¥b = 1}, let Âµ = minyâˆˆSb âˆ¥yâˆ¥a > 0,
35 and write
x
âˆ¥xâˆ¥b
âˆˆSb
=â‡’
âˆ¥xâˆ¥a = âˆ¥xâˆ¥b

x
âˆ¥xâˆ¥b

a
â‰¥âˆ¥xâˆ¥b min
yâˆˆSb âˆ¥yâˆ¥a = âˆ¥xâˆ¥b Âµ.
The same argument shows there is a Î½ > 0 such that âˆ¥xâˆ¥b â‰¥Î½ âˆ¥xâˆ¥a , so
(5.1.10) is produced with Î± = Âµ and Î² = 1/Î½. Note that (5.1.10) insures that
âˆ¥xk âˆ’xâˆ¥a â†’0 if and only if âˆ¥xk âˆ’xâˆ¥b â†’0. Speciï¬c values for Î± and Î² are
given in Exercises 5.1.8 and 5.12.3.
Exercises for section 5.1
5.1.1. Find the 1-, 2-, and âˆ-norms of x =
ï£«
ï£­
2
1
âˆ’4
âˆ’2
ï£¶
ï£¸and x =
ï£«
ï£­
1 + i
1 âˆ’i
1
4i
ï£¶
ï£¸.
5.1.2. Consider the euclidean norm with u =
ï£«
ï£­
2
1
âˆ’4
âˆ’2
ï£¶
ï£¸and v =
ï£«
ï£­
1
âˆ’1
1
âˆ’1
ï£¶
ï£¸.
(a)
Determine the distance between u and v.
(b)
Verify that the triangle inequality holds for u and v.
(c)
Verify that the CBS inequality holds for u and v.
5.1.3. Show that (Î±1 + Î±2 + Â· Â· Â· + Î±n)2 â‰¤n

Î±2
1 + Î±2
2 + Â· Â· Â· + Î±2
n

for Î±i âˆˆâ„œ.
35
An important theorem from analysis states that a continuous function mapping a closed and
bounded subset K âŠ‚V
into â„œattains a minimum and maximum value at points in K.
Unit spheres in ï¬nite-dimensional spaces are closed and bounded, and every norm on V is
continuous (Exercise 5.1.7), so this minimum is guaranteed to exist.

5.1 Vector Norms
277
5.1.4. (a)
Using the euclidean norm, describe the solid ball in â„œn centered
at the origin with unit radius.
(b)
Describe a solid ball centered at
the point c = ( Î¾1
Î¾2
Â· Â· Â·
Î¾n ) with radius Ï.
5.1.5. If x, y âˆˆâ„œn such that âˆ¥x âˆ’yâˆ¥2 = âˆ¥x + yâˆ¥2 , what is xT y?
5.1.6. Explain why âˆ¥x âˆ’yâˆ¥= âˆ¥y âˆ’xâˆ¥is true for all norms.
5.1.7. For every vector norm on Cn, prove that âˆ¥vâˆ¥depends continuously on
the components of v in the sense that for each Ïµ > 0, there corresponds
a Î´ > 0 such that
 âˆ¥xâˆ¥âˆ’âˆ¥yâˆ¥
 < Ïµ whenever |xi âˆ’yi| < Î´ for each i.
5.1.8.
(a)
For x âˆˆCnÃ—1, explain why âˆ¥xâˆ¥1 â‰¥âˆ¥xâˆ¥2 â‰¥âˆ¥xâˆ¥âˆ.
(b)
For x âˆˆCnÃ—1, show that âˆ¥xâˆ¥i â‰¤Î± âˆ¥xâˆ¥j , where Î± is the (i, j)-
entry in the following matrix. (See Exercise 5.12.3 for a similar
statement regarding matrix norms.)
ï£«
ï£­
1
2
âˆ
1
âˆ—
âˆšn
n
2
1
âˆ—
âˆšn
âˆ
1
1
âˆ—
ï£¶
ï£¸.
5.1.9. For x, y âˆˆCn, x Ì¸= 0, explain why equality holds in the CBS inequality
if and only if y = Î±x, where Î± = xâˆ—y/xâˆ—x. Hint: Use (5.1.4).
5.1.10. For nonzero vectors x, y âˆˆCn with the euclidean norm, prove that
equality holds in the triangle inequality if and only if y = Î±x, where Î±
is real and positive. Hint: Make use of Exercise 5.1.9.
5.1.11. Use HÂ¨olderâ€™s inequality (5.1.8) to prove that if the components of
x âˆˆâ„œnÃ—1 sum to zero (i.e., xT e = 0 for eT = (1, 1, . . . , 1) ), then
|xT y| â‰¤âˆ¥xâˆ¥1
ymax âˆ’ymin
2

for all y âˆˆâ„œnÃ—1.
Note: For â€œzero sumâ€ vectors x, this is at least as sharp and usually
itâ€™s sharper than (5.1.8) because (ymax âˆ’ymin)/2 â‰¤maxi |yi| = âˆ¥yâˆ¥âˆ.

278
Chapter 5
Norms, Inner Products, and Orthogonality
5.1.12. The classical form of HÂ¨olderâ€™s inequality
36 states that if p > 1 and
q > 1 are real numbers such that 1/p + 1/q = 1, then
n

i=1
|xiyi| â‰¤
 n

i=1
|xi|p
1/p  n

i=1
|yi|q
1/q
.
Derive this inequality by executing the following steps:
(a)
By considering the function f(t) = (1 âˆ’Î») + Î»t âˆ’tÎ» for 0 < Î» < 1,
establish the inequality
Î±Î»Î²1âˆ’Î» â‰¤Î»Î± + (1 âˆ’Î»)Î²
for nonnegative real numbers Î± and Î².
(b)
Let Ë†x = x/ âˆ¥xâˆ¥p and Ë†y = y/ âˆ¥yâˆ¥q , and apply the inequality of part (a)
to obtain
n

i=1
|Ë†xiË†yi| â‰¤1
p
n

i=1
|Ë†xi|p + 1
q
n

i=1
|Ë†yi|q = 1.
(c)
Deduce the classical form of HÂ¨olderâ€™s inequality, and then explain why
this means that
|xâˆ—y| â‰¤âˆ¥xâˆ¥p âˆ¥yâˆ¥q .
5.1.13. The triangle inequality âˆ¥x + yâˆ¥p â‰¤âˆ¥xâˆ¥p + âˆ¥yâˆ¥p for a general p-norm
is really the classical Minkowski inequality,
37 which states that for
p â‰¥1,
 n

i=1
|xi + yi|p
1/p
â‰¤
 n

i=1
|xi|p
1/p
+
 n

i=1
|yi|p
1/p
.
Derive Minkowskiâ€™s inequality. Hint: For p > 1, let q be the number
such that 1/q = 1 âˆ’1/p. Verify that for scalars Î± and Î²,
|Î± + Î²|p = |Î± + Î²| |Î± + Î²|p/q â‰¤|Î±| |Î± + Î²|p/q + |Î²| |Î± + Î²|p/q,
and make use of HÂ¨olderâ€™s inequality in Exercise 5.1.12.
36
Ludwig Otto HÂ¨older (1859â€“1937) was a German mathematician who studied at GÂ¨ottingen and
lived in Leipzig. Although he made several contributions to analysis as well as algebra, he is
primarily known for the development of the inequality that now bears his name.
37
Hermann Minkowski (1864â€“1909) was born in Russia, but spent most of his life in Germany
as a mathematician and professor at KÂ¨onigsberg and GÂ¨ottingen. In addition to the inequality
that now bears his name, he is known for providing a mathematical basis for the special theory
of relativity. He died suddenly from a ruptured appendix at the age of 44.

5.2 Matrix Norms
279
5.2
MATRIX NORMS
Because CmÃ—n is a vector space of dimension mn, magnitudes of matrices
A âˆˆCmÃ—n can be â€œmeasuredâ€ by employing any vector norm on Cmn. For
example, by stringing out the entries of A =

2
âˆ’1
âˆ’4
âˆ’2

into a four-component
vector, the euclidean norm on â„œ4 can be applied to write
âˆ¥Aâˆ¥=

22 + (âˆ’1)2 + (âˆ’4)2 + (âˆ’2)21/2 = 5.
This is one of the simplest notions of a matrix norm, and it is called the Frobenius
(p. 662) norm (older texts refer to it as the Hilbertâ€“Schmidt norm or the Schur
norm). There are several useful ways to describe the Frobenius matrix norm.
Frobenius Matrix Norm
The Frobenius norm of A âˆˆCmÃ—n is deï¬ned by the equations
âˆ¥Aâˆ¥2
F =

i,j
|aij|2 =

i
âˆ¥Aiâˆ—âˆ¥2
2 =

j
âˆ¥Aâˆ—jâˆ¥2
2 = trace (Aâˆ—A).
(5.2.1)
The Frobenius matrix norm is ï¬ne for some problems, but it is not well suited
for all applications. So, similar to the situation for vector norms, alternatives need
to be explored. But before trying to develop diï¬€erent recipes for matrix norms, it
makes sense to ï¬rst formulate a general deï¬nition of a matrix norm. The goal is
to start with the deï¬ning properties for a vector norm given in (5.1.9) on p. 275
and ask what, if anything, needs to be added to that list.
Matrix multiplication distinguishes matrix spaces from more general vector
spaces, but the three vector-norm properties (5.1.9) say nothing about products.
So, an extra property that relates âˆ¥ABâˆ¥to âˆ¥Aâˆ¥and âˆ¥Bâˆ¥is needed. The
Frobenius norm suggests the nature of this extra property. The CBS inequality
insures that âˆ¥Axâˆ¥2
2 = 
i |Aiâˆ—x|2 â‰¤
i âˆ¥Aiâˆ—âˆ¥2
2 âˆ¥xâˆ¥2
2 = âˆ¥Aâˆ¥2
F âˆ¥xâˆ¥2
2 . That is,
âˆ¥Axâˆ¥2 â‰¤âˆ¥Aâˆ¥F âˆ¥xâˆ¥2 ,
(5.2.2)
and we express this by saying that the Frobenius matrix norm âˆ¥â‹†âˆ¥F and the
euclidean vector norm âˆ¥â‹†âˆ¥2 are compatible. The compatibility condition (5.2.2)
implies that for all conformable matrices A and B,
âˆ¥ABâˆ¥2
F =

j
âˆ¥[AB]âˆ—jâˆ¥2
2 =

j
âˆ¥ABâˆ—jâˆ¥2
2 â‰¤

j
âˆ¥Aâˆ¥2
F âˆ¥Bâˆ—jâˆ¥2
2
= âˆ¥Aâˆ¥2
F

j
âˆ¥Bâˆ—jâˆ¥2
2 = âˆ¥Aâˆ¥2
F âˆ¥Bâˆ¥2
F
=â‡’
âˆ¥ABâˆ¥F â‰¤âˆ¥Aâˆ¥F âˆ¥Bâˆ¥F .
This suggests that the submultiplicative property âˆ¥ABâˆ¥â‰¤âˆ¥Aâˆ¥âˆ¥Bâˆ¥should be
added to (5.1.9) to deï¬ne a general matrix norm.

280
Chapter 5
Norms, Inner Products, and Orthogonality
General Matrix Norms
A matrix norm is a function âˆ¥â‹†âˆ¥from the set of all complex matrices
(of all ï¬nite orders) into â„œthat satisï¬es the following properties.
âˆ¥Aâˆ¥â‰¥0
and
âˆ¥Aâˆ¥= 0 â‡â‡’A = 0.
âˆ¥Î±Aâˆ¥= |Î±| âˆ¥Aâˆ¥
for all scalars Î±.
âˆ¥A + Bâˆ¥â‰¤âˆ¥Aâˆ¥+ âˆ¥Bâˆ¥
for matrices of the same size.
âˆ¥ABâˆ¥â‰¤âˆ¥Aâˆ¥âˆ¥Bâˆ¥
for all conformable matrices.
(5.2.3)
The Frobenius norm satisï¬es the above deï¬nition (it was built that way),
but where do other useful matrix norms come from? In fact, every legitimate
vector norm generates (or induces) a matrix norm as described below.
Induced Matrix Norms
A vector norm that is deï¬ned on Cp for p = m, n induces a matrix
norm on CmÃ—n by setting
âˆ¥Aâˆ¥= max
âˆ¥xâˆ¥=1 âˆ¥Axâˆ¥
for A âˆˆCmÃ—n, x âˆˆCnÃ—1.
(5.2.4)
The footnote on p. 276 explains why this maximum value must exist.
â€¢
Itâ€™s apparent that an induced matrix norm is compatible with its
underlying vector norm in the sense that
âˆ¥Axâˆ¥â‰¤âˆ¥Aâˆ¥âˆ¥xâˆ¥.
(5.2.5)
â€¢
When A is nonsingular,
min
âˆ¥xâˆ¥=1 âˆ¥Axâˆ¥=
1
âˆ¥Aâˆ’1âˆ¥.
(5.2.6)
Proof.
Verifying that maxâˆ¥xâˆ¥=1 âˆ¥Axâˆ¥satisï¬es the ï¬rst three conditions in
(5.2.3) is straightforward, and (5.2.5) implies âˆ¥ABâˆ¥â‰¤âˆ¥Aâˆ¥âˆ¥Bâˆ¥(see Exercise
5.2.5). Property (5.2.6) is developed in Exercise 5.2.7.
In words, an induced norm âˆ¥Aâˆ¥represents the maximum extent to which
a vector on the unit sphere can be stretched by A, and 1/
Aâˆ’1 measures the
extent to which a nonsingular matrix A can shrink vectors on the unit sphere.
Figure 5.2.1 depicts this in â„œ3 for the induced matrix 2-norm.

5.2 Matrix Norms
281
1
max
âˆ¥xâˆ¥=1 âˆ¥Axâˆ¥= âˆ¥Aâˆ¥
min
âˆ¥xâˆ¥=1 âˆ¥Axâˆ¥=
1
âˆ¥A-1âˆ¥
A
Figure 5.2.1. The induced matrix 2-norm in â„œ3.
Intuition might suggest that the euclidean vector norm should induce the
Frobenius matrix norm (5.2.1), but something surprising happens instead.
Matrix 2-Norm
â€¢
The matrix norm induced by the euclidean vector norm is
âˆ¥Aâˆ¥2 = max
âˆ¥xâˆ¥2=1 âˆ¥Axâˆ¥2 =

Î»max,
(5.2.7)
where Î»max is the largest number Î» such that Aâˆ—A âˆ’Î»I is singular.
â€¢
When A is nonsingular,
Aâˆ’1
2 =
1
min
âˆ¥xâˆ¥2=1 âˆ¥Axâˆ¥2
=
1
âˆšÎ»min
,
(5.2.8)
where Î»min is the smallest number Î» such that Aâˆ—Aâˆ’Î»I is singular.
Note: If you are already familiar with eigenvalues, these say that Î»max
and Î»min are the largest and smallest eigenvalues of Aâˆ—A (Example
7.5.1, p. 549), while (Î»max)1/2 = Ïƒ1 and (Î»min)1/2 = Ïƒn are the largest
and smallest singular values of A (p. 414).
Proof.
To prove (5.2.7), assume that AmÃ—n is real (a proof for complex ma-
trices is given in Example 7.5.1 on p. 549). The strategy is to evaluate âˆ¥Aâˆ¥2
2 by
solving the problem
maximize f(x) = âˆ¥Axâˆ¥2
2 = xT AT Ax
subject to g(x) = xT x = 1

282
Chapter 5
Norms, Inner Products, and Orthogonality
using the method of Lagrange multipliers. Introduce a new variable Î» (the
Lagrange multiplier), and consider the function h(x, Î») = f(x) âˆ’Î»g(x). The
points at which f is maximized are contained in the set of solutions to the
equations âˆ‚h/âˆ‚xi = 0
(i = 1, 2, . . . , n) along with g(x) = 1. Diï¬€erentiating
h with respect to the xi â€™s is essentially the same as described on p. 227, and
the system generated by âˆ‚h/âˆ‚xi = 0 (i = 1, 2, . . . , n) is (AT A âˆ’Î»I)x = 0. In
other words, f is maximized at a vector x for which (AT A âˆ’Î»I)x = 0 and
âˆ¥xâˆ¥2 = 1. Consequently, Î» must be a number such that AT A âˆ’Î»I is singular
(because x Ì¸= 0 ). Since
xT AT Ax = Î»xT x = Î»,
it follows that
âˆ¥Aâˆ¥2 = max
âˆ¥xâˆ¥=1 âˆ¥Axâˆ¥= max
âˆ¥xâˆ¥2=1 âˆ¥Axâˆ¥=

max
xT x=1 xT AT Ax
1/2
=

Î»max,
where Î»max is the largest number Î» for which AT A âˆ’Î»I is singular. A similar
argument applied to (5.2.6) proves (5.2.8). Also, an independent development of
(5.2.7) and (5.2.8) is contained in the discussion of singular values on p. 412.
Example 5.2.1
Problem: Determine the induced norm âˆ¥Aâˆ¥2 as well as âˆ¥Aâˆ’1âˆ¥2 for the non-
singular matrix
A =
1
âˆš
3
 3
âˆ’1
0
âˆš
8

.
Solution: Find the values of Î» that make AT A âˆ’Î»I singular by applying
Gaussian elimination to produce
AT A âˆ’Î»I =

3 âˆ’Î»
âˆ’1
âˆ’1
3 âˆ’Î»

âˆ’â†’

âˆ’1
3 âˆ’Î»
3 âˆ’Î»
âˆ’1

âˆ’â†’

âˆ’1
3 âˆ’Î»
0
âˆ’1 + (3 âˆ’Î»)2

.
This shows that AT Aâˆ’Î»I is singular when âˆ’1+(3âˆ’Î»)2 = 0 or, equivalently,
when Î» = 2 or Î» = 4, so Î»min = 2 and Î»max = 4. Consequently, (5.2.7) and
(5.2.8) say that
âˆ¥Aâˆ¥2 =

Î»max = 2
and
âˆ¥Aâˆ’1âˆ¥2 =
1
âˆšÎ»min
=
1
âˆš
2.
Note: As mentioned earlier, the values of Î» that make AT A âˆ’Î»I singular
are called the eigenvalues of AT A, and they are the focus of Chapter 7 where
their determination is discussed in more detail. Using Gaussian elimination to
determine the eigenvalues is not practical for larger matrices.
Some useful properties of the matrix 2-norm are stated below.

5.2 Matrix Norms
283
Properties of the 2-Norm
In addition to the properties shared by all induced norms, the 2-norm
enjoys the following special properties.
â€¢
âˆ¥Aâˆ¥2 = max
âˆ¥xâˆ¥2=1 max
âˆ¥yâˆ¥2=1 |yâˆ—Ax|.
(5.2.9)
â€¢
âˆ¥Aâˆ¥2 = âˆ¥Aâˆ—âˆ¥2.
(5.2.10)
â€¢
âˆ¥Aâˆ—Aâˆ¥2 = âˆ¥Aâˆ¥2
2 .
(5.2.11)
â€¢

 A
0
0
B

2 = max

âˆ¥Aâˆ¥2 , âˆ¥Bâˆ¥2

.
(5.2.12)
â€¢
âˆ¥Uâˆ—AVâˆ¥2 = âˆ¥Aâˆ¥2 when UUâˆ—= I and Vâˆ—V = I.
(5.2.13)
You are asked to verify the validity of these properties in Exercise 5.2.6
on p. 285. Furthermore, some additional properties of the matrix 2-norm are
developed in Exercise 5.6.9 and on pp. 414 and 417.
Now that we understand how the euclidean vector norm induces the matrix
2-norm, letâ€™s investigate the nature of the matrix norms that are induced by the
vector 1-norm and the vector âˆ-norm.
Matrix 1-Norm and Matrix âˆ-Norm
The matrix norms induced by the vector 1-norm and âˆ-norm are as
follows.
â€¢
âˆ¥Aâˆ¥1 = max
âˆ¥xâˆ¥1=1 âˆ¥Axâˆ¥1 = max
j

i
|aij|
= the largest absolute column sum.
(5.2.14)
â€¢
âˆ¥Aâˆ¥âˆ=
max
âˆ¥xâˆ¥âˆ=1 âˆ¥Axâˆ¥âˆ= max
i

j
|aij|
= the largest absolute row sum.
(5.2.15)
Proof of (5.2.14).
For all x with âˆ¥xâˆ¥1 = 1, the scalar triangle inequality yields
âˆ¥Axâˆ¥1 =

i
Aiâˆ—x
 =

i


j
aijxj
 â‰¤

i

j
|aij| |xj| =

j

|xj|

i
|aij|

â‰¤
 
j
|xj|

max
j

i
|aij|

= max
j

i
|aij| .

284
Chapter 5
Norms, Inner Products, and Orthogonality
Equality can be attained because if Aâˆ—k is the column with largest absolute sum,
set x = ek, and note that âˆ¥ekâˆ¥1 = 1 and âˆ¥Aekâˆ¥1 = âˆ¥Aâˆ—kâˆ¥1 = maxj

i |aij| .
Proof of (5.2.15).
For all x with âˆ¥xâˆ¥âˆ= 1,
âˆ¥Axâˆ¥âˆ= max
i


j
aijxj
 â‰¤max
i

j
|aij| |xj| â‰¤max
i

j
|aij| .
Equality can be attained because if Akâˆ—is the row with largest absolute sum,
and if x is the vector such that
xj =

1
if akj â‰¥0,
âˆ’1
if akj < 0,
then

|Aiâˆ—x| = | 
j aijxj| â‰¤
j |aij| for all i,
|Akâˆ—x| = 
j |akj| = maxi

j |aij| ,
so âˆ¥xâˆ¥âˆ= 1, and âˆ¥Axâˆ¥âˆ= maxi |Aiâˆ—x| = maxi

j |aij| .
Example 5.2.2
Problem: Determine the induced matrix norms âˆ¥Aâˆ¥1 and âˆ¥Aâˆ¥âˆfor
A =
1
âˆš
3

3
âˆ’1
0
âˆš
8

,
and compare the results with âˆ¥Aâˆ¥2 (from Example 5.2.1) and âˆ¥Aâˆ¥F .
Solution: Equation (5.2.14) says that âˆ¥Aâˆ¥1 is the largest absolute column sum
in A, and (5.2.15) says that âˆ¥Aâˆ¥âˆis the largest absolute row sum, so
âˆ¥Aâˆ¥1 = 1/
âˆš
3 +
âˆš
8/
âˆš
3 â‰ˆ2.21
and
âˆ¥Aâˆ¥âˆ= 4/
âˆš
3 â‰ˆ2.31.
Since âˆ¥Aâˆ¥2 = 2 (Example 5.2.1) and âˆ¥Aâˆ¥F =

trace (AT A) =
âˆš
6 â‰ˆ2.45, we
see that while âˆ¥Aâˆ¥1, âˆ¥Aâˆ¥2, âˆ¥Aâˆ¥âˆ, and âˆ¥Aâˆ¥F are not equal, they are all in
the same ballpark. This is true for all n Ã— n matrices because it can be shown
that âˆ¥Aâˆ¥i â‰¤Î± âˆ¥Aâˆ¥j , where Î± is the (i, j)-entry in the following matrix
ï£«
ï£¬
ï£¬
ï£­
1
2
âˆ
F
1
âˆ—
âˆšn
n
âˆšn
2
âˆšn
âˆ—
âˆšn
1
âˆ
n
âˆšn
âˆ—
âˆšn
F
âˆšn
âˆšn
âˆšn
âˆ—
ï£¶
ï£·
ï£·
ï£¸
(see Exercise 5.1.8 and Exercise 5.12.3 on p. 425). Since itâ€™s often the case that
only the order of magnitude of âˆ¥Aâˆ¥is needed and not the exact value (e.g.,
recall the rule of thumb in Example 3.8.2 on p. 129), and since âˆ¥Aâˆ¥2 is diï¬ƒcult
to compute in comparison with âˆ¥Aâˆ¥1, âˆ¥Aâˆ¥âˆ, and âˆ¥Aâˆ¥F , you can see why any
of these three might be preferred over âˆ¥Aâˆ¥2 in spite of the fact that âˆ¥Aâˆ¥2 is
more â€œnaturalâ€ by virtue of being induced by the euclidean vector norm.

5.2 Matrix Norms
285
Exercises for section 5.2
5.2.1. Evaluate the Frobenius matrix norm for each matrix below.
A =

1
âˆ’2
âˆ’1
2

,
B =
ï£«
ï£­
0
1
0
0
0
1
1
0
0
ï£¶
ï£¸,
C =
ï£«
ï£­
4
âˆ’2
4
âˆ’2
1
âˆ’2
4
âˆ’2
4
ï£¶
ï£¸.
5.2.2. Evaluate the induced 1-, 2-, and âˆ-matrix norm for each of the three
matrices given in Exercise 5.2.1.
5.2.3.
(a)
Explain why âˆ¥Iâˆ¥= 1 for every induced matrix norm (5.2.4).
(b)
What is âˆ¥InÃ—nâˆ¥F ?
5.2.4. Explain why âˆ¥Aâˆ¥F = âˆ¥Aâˆ—âˆ¥F for Frobenius matrix norm (5.2.1).
5.2.5. For matrices A and B and for vectors x, establish the following com-
patibility properties between a vector norm deï¬ned on every Cp and
the associated induced matrix norm.
(a)
Show that âˆ¥Axâˆ¥â‰¤âˆ¥Aâˆ¥âˆ¥xâˆ¥.
(b)
Show that âˆ¥ABâˆ¥â‰¤âˆ¥Aâˆ¥âˆ¥Bâˆ¥.
(c)
Explain why âˆ¥Aâˆ¥= maxâˆ¥xâˆ¥â‰¤1 âˆ¥Axâˆ¥.
5.2.6. Establish the following properties of the matrix 2-norm.
(a)
âˆ¥Aâˆ¥2 = max
âˆ¥xâˆ¥2=1
âˆ¥yâˆ¥2=1
|yâˆ—Ax|,
(b)
âˆ¥Aâˆ¥2 = âˆ¥Aâˆ—âˆ¥2,
(c)
âˆ¥Aâˆ—Aâˆ¥2 = âˆ¥Aâˆ¥2
2 ,
(d)

 A
0
0
B

2 = max

âˆ¥Aâˆ¥2 , âˆ¥Bâˆ¥2

(take A, B to be real),
(e)
âˆ¥Uâˆ—AVâˆ¥2 = âˆ¥Aâˆ¥2 when UUâˆ—= I and Vâˆ—V = I.
5.2.7. Using the induced matrix norm (5.2.4), prove that if A is nonsingular,
then
âˆ¥Aâˆ¥=
1
min
âˆ¥xâˆ¥=1
Aâˆ’1x

or, equivalently,
Aâˆ’1 =
1
min
âˆ¥xâˆ¥=1 âˆ¥Axâˆ¥.
5.2.8. For A âˆˆCnÃ—n and a parameter z âˆˆC, the matrix R(z) = (zI âˆ’A)âˆ’1
is called the resolvent of A. Prove that if |z| > âˆ¥Aâˆ¥for any induced
matrix norm, then
âˆ¥R(z)âˆ¥â‰¤
1
|z| âˆ’âˆ¥Aâˆ¥.

286
Chapter 5
Norms, Inner Products, and Orthogonality
5.3
INNER-PRODUCT SPACES
The euclidean norm, which naturally came ï¬rst, is a coordinate-dependent con-
cept. But by isolating its important properties we quickly moved to the more
general coordinate-free deï¬nition of a vector norm given in (5.1.9) on p. 275. The
goal is to now do the same for inner products. That is, start with the standard
inner product, which is a coordinate-dependent deï¬nition, and identify proper-
ties that characterize the basic essence of the concept. The ones listed below are
those that have been distilled from the standard inner product to formulate a
more general coordinate-free deï¬nition.
General Inner Product
An inner product on a real (or complex) vector space V is a function
that maps each ordered pair of vectors x, y to a real (or complex) scalar
âŸ¨x yâŸ©such that the following four properties hold.
âŸ¨x xâŸ©is real with âŸ¨x xâŸ©â‰¥0, and âŸ¨x xâŸ©= 0 if and only if x = 0,
âŸ¨x Î±yâŸ©= Î± âŸ¨x yâŸ©for all scalars Î±,
(5.3.1)
âŸ¨x y + zâŸ©= âŸ¨x yâŸ©+ âŸ¨x zâŸ©,
âŸ¨x yâŸ©= âŸ¨y xâŸ©
(for real spaces, this becomes âŸ¨x yâŸ©= âŸ¨y xâŸ©).
Notice that for each ï¬xed value of x, the second and third properties
say that âŸ¨x yâŸ©is a linear function of y.
Any real or complex vector space that is equipped with an inner product
is called an inner-product space.
Example 5.3.1
â€¢
The standard inner products, âŸ¨x yâŸ©= xT y for â„œnÃ—1 and âŸ¨x yâŸ©= xâˆ—y
for CnÃ—1, each satisfy the four deï¬ning conditions (5.3.1) for a general inner
productâ€”this shouldnâ€™t be a surprise.
â€¢
If AnÃ—n is a nonsingular matrix, then âŸ¨x yâŸ©= xâˆ—Aâˆ—Ay is an inner product
for CnÃ—1. This inner product is sometimes called an A-inner product or
an elliptical inner product.
â€¢
Consider the vector space of m Ã— n matrices. The functions deï¬ned by
âŸ¨A BâŸ©= trace

AT B

and
âŸ¨A BâŸ©= trace (Aâˆ—B)
(5.3.2)
are inner products for â„œmÃ—n and CmÃ—n, respectively. These are referred to
as the standard inner products for matrices. Notice that these reduce
to the standard inner products for vectors when n = 1.

5.3 Inner-Product Spaces
287
â€¢
If V is the vector space of real-valued continuous functions deï¬ned on the
interval (a, b), then
âŸ¨f|gâŸ©=
 b
a
f(t)g(t)dt
is an inner product on V.
Just as the standard inner product for CnÃ—1 deï¬nes the euclidean norm on
CnÃ—1 by âˆ¥xâˆ¥2 =
âˆš
xâˆ—x, every general inner product in an inner-product space
V deï¬nes a norm on V by setting
âˆ¥â‹†âˆ¥=

âŸ¨â‹†â‹†âŸ©.
(5.3.3)
Itâ€™s straightforward to verify that this satisï¬es the ï¬rst two conditions in (5.2.3)
on p. 280 that deï¬ne a general vector norm, but, just as in the case of euclidean
norms, verifying that (5.3.3) satisï¬es the triangle inequality requires a generalized
version of CBS inequality.
General CBS Inequality
If V is an inner-product space, and if we set âˆ¥â‹†âˆ¥=

âŸ¨â‹†â‹†âŸ©, then
| âŸ¨x yâŸ©| â‰¤âˆ¥xâˆ¥âˆ¥yâˆ¥
for all x, y âˆˆV.
(5.3.4)
Equality holds if and only if y = Î±x for Î± = âŸ¨x yâŸ©/ âˆ¥xâˆ¥2 .
Proof.
Set Î± = âŸ¨x yâŸ©/ âˆ¥xâˆ¥2 (assume x Ì¸= 0, for otherwise there is nothing to
prove), and observe that âŸ¨x Î±x âˆ’yâŸ©= 0, so
0 â‰¤âˆ¥Î±x âˆ’yâˆ¥2 = âŸ¨Î±x âˆ’y Î±x âˆ’yâŸ©
= Â¯Î± âŸ¨x Î±x âˆ’yâŸ©âˆ’âŸ¨y Î±x âˆ’yâŸ©
(see Exercise 5.3.2)
= âˆ’âŸ¨y Î±x âˆ’yâŸ©= âŸ¨y yâŸ©âˆ’Î± âŸ¨y xâŸ©= âˆ¥yâˆ¥2 âˆ¥xâˆ¥2 âˆ’âŸ¨x yâŸ©âŸ¨y xâŸ©
âˆ¥xâˆ¥2
.
Since âŸ¨y xâŸ©= âŸ¨x yâŸ©, it follows that âŸ¨x yâŸ©âŸ¨y xâŸ©= |âŸ¨x yâŸ©|2 , so
0 â‰¤âˆ¥yâˆ¥2 âˆ¥xâˆ¥2 âˆ’|âŸ¨x yâŸ©|2
âˆ¥xâˆ¥2
=â‡’
| âŸ¨x yâŸ©| â‰¤âˆ¥xâˆ¥âˆ¥yâˆ¥.
Establishing the conditions for equality is the same as in Exercise 5.1.9.
Letâ€™s now complete the job of showing that âˆ¥â‹†âˆ¥=

âŸ¨â‹†â‹†âŸ©is indeed a vector
norm as deï¬ned in (5.2.3) on p. 280.

288
Chapter 5
Norms, Inner Products, and Orthogonality
Norms in Inner-Product Spaces
If V is an inner-product space with an inner product âŸ¨x yâŸ©, then
âˆ¥â‹†âˆ¥=

âŸ¨â‹†â‹†âŸ©
deï¬nes a norm on V.
Proof.
The fact that âˆ¥â‹†âˆ¥=

âŸ¨â‹†â‹†âŸ©satisï¬es the ï¬rst two norm properties in
(5.2.3) on p. 280 follows directly from the deï¬ning properties (5.3.1) for an inner
product. You are asked to provide the details in Exercise 5.3.3. To establish the
triangle inequality, use âŸ¨x yâŸ©â‰¤| âŸ¨x yâŸ©| and âŸ¨y xâŸ©= âŸ¨x yâŸ©â‰¤| âŸ¨x yâŸ©| together
with the CBS inequality to write
âˆ¥x + yâˆ¥2 = âŸ¨x + y x + yâŸ©= âŸ¨x xâŸ©+ âŸ¨x yâŸ©+ âŸ¨y xâŸ©+ âŸ¨y yâŸ©
â‰¤âˆ¥xâˆ¥2 + 2| âŸ¨x yâŸ©| + âˆ¥yâˆ¥2 â‰¤(âˆ¥xâˆ¥+ âˆ¥yâˆ¥)2.
Example 5.3.2
Problem: Describe the norms that are generated by the inner products pre-
sented in Example 5.3.1.
â€¢
Given a nonsingular matrix A âˆˆCnÃ—n, the A-norm (or elliptical norm)
generated by the A-inner product on CnÃ—1 is
âˆ¥xâˆ¥A =

âŸ¨x xâŸ©=
âˆš
xâˆ—Aâˆ—Ax = âˆ¥Axâˆ¥2 .
(5.3.5)
â€¢
The standard inner product for matrices generates the Frobenius matrix
norm because
âˆ¥Aâˆ¥=

âŸ¨A AâŸ©=

trace (Aâˆ—A) = âˆ¥Aâˆ¥F .
(5.3.6)
â€¢
For the space of real-valued continuous functions deï¬ned on (a, b), the norm
of a function f generated by the inner product âŸ¨f|gâŸ©=
 b
a f(t)g(t)dt is
âˆ¥fâˆ¥=

âŸ¨f|fâŸ©=
 b
a
f(t)2dt
1/2
.

5.3 Inner-Product Spaces
289
Example 5.3.3
To illustrate the utility of the ideas presented above, consider the proposition
trace

AT B
2 â‰¤trace

AT A

trace

BT B

for all A, B âˆˆâ„œmÃ—n.
Problem: How would you know to formulate such a proposition and, second,
how do you prove it?
Solution: The answer to both questions is the same. This is the CBS inequality
in â„œmÃ—n equipped with the standard inner product âŸ¨A BâŸ©= trace

AT B

and
associated norm âˆ¥Aâˆ¥F =

âŸ¨A AâŸ©=

trace (AT A) because CBS says
âŸ¨A BâŸ©2 â‰¤âˆ¥Aâˆ¥2
F âˆ¥Bâˆ¥2
F
=â‡’
trace

AT B
2 â‰¤trace

AT A

trace

BT B

.
The point here is that if your knowledge is limited to elementary matrix manip-
ulations (which is all that is needed to understand the statement of the propo-
sition), formulating the correct inequality might be quite a challenge to your
intuition. And then proving the proposition using only elementary matrix ma-
nipulations would be a signiï¬cant taskâ€”essentially, you would have to derive a
version of CBS. But knowing the basic facts of inner-product spaces makes the
proposition nearly trivial to conjecture and prove.
Since each inner product generates a norm by the rule âˆ¥â‹†âˆ¥=

âŸ¨â‹†â‹†âŸ©, itâ€™s
natural to ask if the reverse is also true. That is, for each vector norm âˆ¥â‹†âˆ¥
on a space V, does there exist a corresponding inner product on V such that

âŸ¨â‹†â‹†âŸ©= âˆ¥â‹†âˆ¥2 ? If not, under what conditions will a given norm be generated by
an inner product? These are tricky questions, and it took the combined eï¬€orts
of Maurice R. FrÂ´echet
38 (1878â€“1973) and John von Neumann (1903â€“1957) to
provide the answer.
38
Maurice RenÂ´e FrÂ´echet began his illustrious career by writing an outstanding Ph.D. dissertation
in 1906 under the direction of the famous French mathematician Jacques Hadamard (p. 469)
in which the concepts of a metric space and compactness were ï¬rst formulated. FrÂ´echet devel-
oped into a versatile mathematical scientist, and he served as professor of mechanics at the
University of Poitiers (1910â€“1919), professor of higher calculus at the University of Strasbourg
(1920â€“1927), and professor of diï¬€erential and integral calculus and professor of the calculus of
probabilities at the University of Paris (1928â€“1948).
Born in Budapest, Hungary, John von Neumann was a child prodigy who could divide eight-
digit numbers in his head when he was only six years old. Due to the political unrest in
Europe, he came to America, where, in 1933, he became one of the six original professors
of mathematics at the Institute for Advanced Study at Princeton University, a position he
retained for the rest of his life. During his career, von Neumannâ€™s genius touched mathematics
(pure and applied), chemistry, physics, economics, and computer science, and he is generally
considered to be among the best scientists and mathematicians of the twentieth century.

290
Chapter 5
Norms, Inner Products, and Orthogonality
Parallelogram Identity
For a given norm âˆ¥â‹†âˆ¥on a vector space V, there exists an inner product
on V such that âŸ¨â‹†â‹†âŸ©= âˆ¥â‹†âˆ¥2 if and only if the parallelogram identity
âˆ¥x + yâˆ¥2 + âˆ¥x âˆ’yâˆ¥2 = 2

âˆ¥xâˆ¥2 + âˆ¥yâˆ¥2 
(5.3.7)
holds for all x, y âˆˆV.
Proof.
Consider real spacesâ€”complex spaces are discussed in Exercise 5.3.6. If
there exists an inner product such that âŸ¨â‹†â‹†âŸ©= âˆ¥â‹†âˆ¥2 , then the parallelogram
identity is immediate because âŸ¨x + y x + yâŸ©+âŸ¨x âˆ’y x âˆ’yâŸ©= 2 âŸ¨x xâŸ©+2 âŸ¨y yâŸ©.
The diï¬ƒcult part is establishing the converse. Suppose âˆ¥â‹†âˆ¥satisï¬es the paral-
lelogram identity, and prove that the function
âŸ¨x yâŸ©= 1
4

âˆ¥x + yâˆ¥2 âˆ’âˆ¥x âˆ’yâˆ¥2 
(5.3.8)
is an inner product for V such that âŸ¨x xâŸ©= âˆ¥xâˆ¥2 for all x by showing the four
deï¬ning conditions (5.3.1) hold. The ï¬rst and fourth conditions are immediate.
To establish the third, use the parallelogram identity to write
âˆ¥x + yâˆ¥2 + âˆ¥x + zâˆ¥2 = 1
2

âˆ¥x + y + x + zâˆ¥2 + âˆ¥y âˆ’zâˆ¥2 
,
âˆ¥x âˆ’yâˆ¥2 + âˆ¥x âˆ’zâˆ¥2 = 1
2

âˆ¥x âˆ’y + x âˆ’zâˆ¥2 + âˆ¥z âˆ’yâˆ¥2 
,
and then subtract to obtain
âˆ¥x + yâˆ¥2âˆ’âˆ¥x âˆ’yâˆ¥2+âˆ¥x + zâˆ¥2âˆ’âˆ¥x âˆ’zâˆ¥2 = âˆ¥2x + (y + z)âˆ¥2 âˆ’âˆ¥2x âˆ’(y + z)âˆ¥2
2
.
Consequently,
âŸ¨x yâŸ©+ âŸ¨x zâŸ©= 1
4

âˆ¥x + yâˆ¥2 âˆ’âˆ¥x âˆ’yâˆ¥2 + âˆ¥x + zâˆ¥2 âˆ’âˆ¥x âˆ’zâˆ¥2 
= 1
8

âˆ¥2x + (y + z)âˆ¥2 âˆ’âˆ¥2x âˆ’(y + z)âˆ¥2 
= 1
2
x + y + z
2

2
âˆ’
x âˆ’y + z
2

2
= 2

x y + z
2

,
(5.3.9)
and setting z = 0 produces the statement that âŸ¨x yâŸ©= 2 âŸ¨x y/2âŸ©for all y âˆˆV.
Replacing y by y + z yields âŸ¨x y + zâŸ©= 2 âŸ¨x (y + z)/2âŸ©, and thus (5.3.9)

5.3 Inner-Product Spaces
291
guarantees that âŸ¨x yâŸ©+ âŸ¨x zâŸ©= âŸ¨x y + zâŸ©. Now prove that âŸ¨x Î±yâŸ©= Î± âŸ¨x yâŸ©
for all real Î±. This is valid for integer values of Î± by the result just established,
and it holds when Î± is rational because if Î² and Î³ are integers, then
Î³2

x Î²
Î³ y

= âŸ¨Î³x Î²yâŸ©= Î²Î³ âŸ¨x yâŸ©
=â‡’

x Î²
Î³ y

= Î²
Î³ âŸ¨x yâŸ©.
Because âˆ¥x + Î±yâˆ¥and âˆ¥x âˆ’Î±yâˆ¥are continuous functions of Î± (Exercise
5.1.7), equation (5.3.8) insures that âŸ¨x Î±yâŸ©is a continuous function of Î±. There-
fore, if Î± is irrational, and if {Î±n} is a sequence of rational numbers such that
Î±n â†’Î±, then âŸ¨x Î±nyâŸ©â†’âŸ¨x Î±yâŸ©and âŸ¨x Î±nyâŸ©= Î±n âŸ¨x yâŸ©â†’Î± âŸ¨x yâŸ©, so
âŸ¨x Î±yâŸ©= Î± âŸ¨x yâŸ©.
Example 5.3.4
We already know that the euclidean vector norm on Cn is generated by the stan-
dard inner product, so the previous theorem guarantees that the parallelogram
identity must hold for the 2-norm. This is easily corroborated by observing that
âˆ¥x + yâˆ¥2
2 + âˆ¥x âˆ’yâˆ¥2
2 = (x + y)âˆ—(x + y) + (x âˆ’y)âˆ—(x âˆ’y)
= 2 (xâˆ—x + yâˆ—y) = 2(âˆ¥xâˆ¥2
2 + âˆ¥yâˆ¥2
2).
The parallelogram identity is so named because it expresses the fact that the
sum of the squares of the diagonals in a parallelogram is twice the sum of the
squares of the sides. See the following diagram.
x
y
x + y
||x||
||y||
||x + y||
||x - y||
Example 5.3.5
Problem: Except for the euclidean norm, is any other vector p-norm generated
by an inner product?
Solution: No, because the parallelogram identity (5.3.7) doesnâ€™t hold when
p Ì¸= 2. To see that âˆ¥x + yâˆ¥2
p + âˆ¥x âˆ’yâˆ¥2
p = 2

âˆ¥xâˆ¥2
p + âˆ¥yâˆ¥2
p

is not valid for
all x, y âˆˆCn when p Ì¸= 2, consider x = e1 and y = e2. Itâ€™s apparent that
âˆ¥e1 + e2âˆ¥2
p = 22/p = âˆ¥e1 âˆ’e2âˆ¥2
p , so
âˆ¥e1 + e2âˆ¥2
p + âˆ¥e1 âˆ’e2âˆ¥2
p = 2(p+2)/p
and
2

âˆ¥e1âˆ¥2
p + âˆ¥e2âˆ¥2
p

= 4.

292
Chapter 5
Norms, Inner Products, and Orthogonality
Clearly, 2(p+2)/p = 4 only when p = 2. Details for the âˆ-norm are asked for
in Exercise 5.3.7.
Conclusion: For applications that are best analyzed in the context of an inner-
product space (e.g., least squares problems), we are limited to the euclidean
norm or else to one of its variation such as the elliptical norm in (5.3.5).
Virtually all important statements concerning â„œn or Cn with the standard
inner product remain valid for general inner-product spacesâ€”e.g., consider the
statement and proof of the general CBS inequality. Advanced or more theoretical
texts prefer a development in terms of general inner-product spaces. However,
the focus of this text is matrices and the coordinate spaces â„œn and Cn, so
subsequent discussions will usually be phrased in terms of â„œn or Cn and their
standard inner products. But remember that extensions to more general inner-
product spaces are always lurking in the background, and we will not hesitate
to use these generalities or general inner-product notation when they serve our
purpose.
Exercises for section 5.3
5.3.1. For x =
 x1
x2
x3

, y =
 y1
y2
y3

, determine which of the following are inner
products for â„œ3Ã—1.
(a)
âŸ¨x yâŸ©= x1y1 + x3y3,
(b)
âŸ¨x yâŸ©= x1y1 âˆ’x2y2 + x3y3,
(c)
âŸ¨x yâŸ©= 2x1y1 + x2y2 + 4x3y3,
(d)
âŸ¨x yâŸ©= x2
1y2
1 + x2
2y2
2 + x2
3y2
3.
5.3.2. For a general inner-product space V, explain why each of the following
statements must be true.
(a)
If âŸ¨x yâŸ©= 0 for all x âˆˆV, then y = 0.
(b)
âŸ¨Î±x yâŸ©= Î± âŸ¨x yâŸ©for all x, y âˆˆV and for all scalars Î±.
(c)
âŸ¨x + y zâŸ©= âŸ¨x zâŸ©+ âŸ¨y zâŸ©for all x, y, z âˆˆV.
5.3.3. Let V be an inner-product space with an inner product âŸ¨x yâŸ©. Explain
why the function deï¬ned by âˆ¥â‹†âˆ¥=

âŸ¨â‹†â‹†âŸ©satisï¬es the ï¬rst two norm
properties in (5.2.3) on p. 280.
5.3.4. For a real inner-product space with âˆ¥â‹†âˆ¥2 = âŸ¨â‹†â‹†âŸ©, derive the inequality
âŸ¨x yâŸ©â‰¤âˆ¥xâˆ¥2 + âˆ¥yâˆ¥2
2
.
Hint: Consider x âˆ’y.

5.3 Inner-Product Spaces
293
5.3.5. For n Ã— n matrices A and B, explain why each of the following in-
equalities is valid.
(a)
|trace (B)|2 â‰¤n [trace (Bâˆ—B)] .
(b)
trace

B2
â‰¤trace

BT B

for real matrices.
(c)
trace

AT B

â‰¤trace

AT A

+ trace

BT B

2
for real matrices.
5.3.6. Extend the proof given on p. 290 concerning the parallelogram identity
(5.3.7) to include complex spaces. Hint: If V is a complex space with
a norm âˆ¥â‹†âˆ¥that satisï¬es the parallelogram identity, let
âŸ¨x yâŸ©r = âˆ¥x + yâˆ¥2 âˆ’âˆ¥x âˆ’yâˆ¥2
4
,
and prove that
âŸ¨x yâŸ©= âŸ¨x yâŸ©r + i âŸ¨ix yâŸ©r
(the polarization identity)
(5.3.10)
is an inner product on V.
5.3.7. Explain why there does not exist an inner product on Cn (n â‰¥2) such
that âˆ¥â‹†âˆ¥âˆ=

âŸ¨â‹†â‹†âŸ©.
5.3.8. Explain why the Frobenius matrix norm on CnÃ—n must satisfy the par-
allelogram identity.
5.3.9. For n â‰¥2, is either the matrix 1-, 2-, or âˆ-norm generated by an inner
product on CnÃ—n?

294
Chapter 5
Norms, Inner Products, and Orthogonality
5.4
ORTHOGONAL VECTORS
Two vectors in â„œ3 are orthogonal (perpendicular) if the angle between them is
a right angle (90â—¦). But the visual concept of a right angle is not at our disposal in
higher dimensions, so we must dig a little deeper. The essence of perpendicularity
in â„œ2 and â„œ3 is embodied in the classical Pythagorean theorem,
u
|| u ||
v
|| v  ||
|| u - v ||
which says that u and v are orthogonal if and only if âˆ¥uâˆ¥2 +âˆ¥vâˆ¥2 = âˆ¥u âˆ’vâˆ¥2 .
But
39 âˆ¥uâˆ¥2 = uT u for all u âˆˆâ„œ3, and uT v = vT u, so we can rewrite the
Pythagorean statement as
0 = âˆ¥uâˆ¥2 + âˆ¥vâˆ¥2 âˆ’âˆ¥u âˆ’vâˆ¥2 = uT u + vT v âˆ’(u âˆ’v)T (u âˆ’v)
= uT u + vT v âˆ’

uT u âˆ’uT v âˆ’vT u + vT v

= 2uT v.
Therefore, u and v are orthogonal vectors in â„œ3 if and only if uT v = 0. The
natural extension of this provides us with a deï¬nition in more general spaces.
Orthogonality
In an inner-product space V, two vectors x, y âˆˆV are said to be
orthogonal (to each other) whenever âŸ¨x yâŸ©= 0, and this is denoted
by writing x âŠ¥y.
â€¢
For â„œn with the standard inner product, x âŠ¥y â‡â‡’xT y = 0.
â€¢
For Cn with the standard inner product, x âŠ¥y â‡â‡’xâˆ—y = 0.
Example 5.4.1
x =
ï£«
ï£­
1
âˆ’2
3
âˆ’1
ï£¶
ï£¸is orthogonal to y =
ï£«
ï£­
4
1
âˆ’2
âˆ’4
ï£¶
ï£¸because xT y = 0.
39
Throughout this section, only norms generated by an underlying inner product âˆ¥â‹†âˆ¥2 = âŸ¨â‹†â‹†âŸ©
are used, so distinguishing subscripts on the norm notation can be omitted.

5.4 Orthogonal Vectors
295
In spite of the fact that uT v = 0, the vectors u =
 i
3
1

and v =
 i
0
1

are
not orthogonal because uâˆ—v Ì¸= 0.
Now that â€œright anglesâ€ in higher dimensions make sense, how can more
general angles be deï¬ned? Proceed just as before, but use the law of cosines
rather than the Pythagorean theorem. Recall that
u
|| u ||
v
|| v  ||
|| u - v ||
Î¸
the law of cosines in â„œ2 or â„œ3 says âˆ¥u âˆ’vâˆ¥2 = âˆ¥uâˆ¥2+âˆ¥vâˆ¥2âˆ’2 âˆ¥uâˆ¥âˆ¥vâˆ¥cos Î¸.
If u and v are orthogonal, then this reduces to the Pythagorean theorem. But,
in general,
cos Î¸ = âˆ¥uâˆ¥2 + âˆ¥vâˆ¥2 âˆ’âˆ¥u âˆ’vâˆ¥2
2 âˆ¥uâˆ¥âˆ¥vâˆ¥
= uT u + vT v âˆ’(u âˆ’v)T (u âˆ’v)
2 âˆ¥uâˆ¥âˆ¥vâˆ¥
=
2uT v
2 âˆ¥uâˆ¥âˆ¥vâˆ¥=
uT v
âˆ¥uâˆ¥âˆ¥vâˆ¥.
This easily extends to higher dimensions because if x, y are vectors from any real
inner-product space, then the general CBS inequality (5.3.4) on p. 287 guarantees
that âŸ¨x yâŸ©/ âˆ¥xâˆ¥âˆ¥yâˆ¥is a number in the interval [âˆ’1, 1], and hence there is a
unique value Î¸ in [0, Ï€] such that cos Î¸ = âŸ¨x yâŸ©/ âˆ¥xâˆ¥âˆ¥yâˆ¥.
Angles
In a real inner-product space V, the radian measure of the angle be-
tween nonzero vectors x, y âˆˆV is deï¬ned to be the number Î¸ âˆˆ[0, Ï€]
such that
cos Î¸ =
âŸ¨x yâŸ©
âˆ¥xâˆ¥âˆ¥yâˆ¥.
(5.4.1)

296
Chapter 5
Norms, Inner Products, and Orthogonality
Example 5.4.2
In â„œn,
cos Î¸ = xT y/ âˆ¥xâˆ¥âˆ¥yâˆ¥. For example, to determine the angle between
x =
ï£«
ï£­
âˆ’4
2
1
2
ï£¶
ï£¸and y =
ï£«
ï£­
1
0
2
2
ï£¶
ï£¸, compute cos Î¸ = 2/(5)(3) = 2/15, and use the
inverse cosine function to conclude that Î¸ = 1.437 radians (rounded).
Example 5.4.3
Linear Correlation. Suppose that an experiment is conducted, and the result-
ing observations are recorded in two data vectors
x =
ï£«
ï£¬
ï£¬
ï£­
x1
x2
...
xn
ï£¶
ï£·
ï£·
ï£¸, y =
ï£«
ï£¬
ï£¬
ï£­
y1
y2
...
yn
ï£¶
ï£·
ï£·
ï£¸,
and let e =
ï£«
ï£¬
ï£¬
ï£­
1
1
...
1
ï£¶
ï£·
ï£·
ï£¸.
Problem: Determine to what extent the yi â€™s are linearly related to the xi â€™s.
That is, measure how close y is to being a linear combination Î²0e + Î²1x.
Solution: The cosine as deï¬ned in (5.4.1) does the job. To understand how, let
Âµx and Ïƒx be the mean and standard deviation of the data in x. That is,
Âµx =

i xi
n
= eT x
n
and
Ïƒx =

i(xi âˆ’Âµx)2
n
= âˆ¥x âˆ’Âµxeâˆ¥2
âˆšn
.
The mean is a measure of central tendency, and the standard deviation mea-
sures the extent to which the data is spread. Frequently, raw data from diï¬€erent
sources is diï¬ƒcult to compare because the units of measure are diï¬€erentâ€”e.g.,
one researcher may use the metric system while another uses American units. To
compensate, data is almost always ï¬rst â€œstandardizedâ€ into unitless quantities.
The standardization of a vector x for which Ïƒx Ì¸= 0 is deï¬ned to be
zx = x âˆ’Âµxe
Ïƒx
.
Entries in zx are often referred to as standard scores or z-scores. All stan-
dardized vectors have the properties that âˆ¥zâˆ¥= âˆšn,
Âµz = 0, and Ïƒz = 1.
Furthermore, itâ€™s not diï¬ƒcult to verify that for vectors x and y such that
Ïƒx Ì¸= 0 and Ïƒy Ì¸= 0, itâ€™s the case that
zx = zy â‡â‡’âˆƒconstants Î²0, Î²1 such that y = Î²0e + Î²1x,
where
Î²1 > 0,
zx = âˆ’zy â‡â‡’âˆƒconstants Î²0, Î²1 such that y = Î²0e + Î²1x,
where
Î²1 < 0.
â€¢
In other words, y = Î²0e+Î²1x for some Î²0 and Î²1 if and only if zx = Â±zy,
in which case we say y is perfectly linearly correlated with x.

5.4 Orthogonal Vectors
297
Since zx varies continuously with x, the existence of a â€œnearâ€ linear relationship
between x and y is equivalent to zx being â€œcloseâ€ to Â±zy in some sense. The
fact that âˆ¥zxâˆ¥= âˆ¥Â±zyâˆ¥= âˆšn means zx and Â±zy diï¬€er only in orientation,
so a natural measure of how close zx is to Â±zy is cos Î¸, where Î¸ is the angle
between zx and zy. The number
Ïxy = cos Î¸ =
zxT zy
âˆ¥zxâˆ¥âˆ¥zyâˆ¥= zxT zy
n
= (x âˆ’Âµxe)T (y âˆ’Âµye)
âˆ¥x âˆ’Âµxeâˆ¥âˆ¥y âˆ’Âµyeâˆ¥
is called the coeï¬ƒcient of linear correlation, and the following facts are now
immediate.
â€¢
Ïxy = 0 if and only if x and y are orthogonal, in which case we say that
x and y are completely uncorrelated.
â€¢
|Ïxy| = 1 if and only if y is perfectly correlated with x. That is, |Ïxy| = 1
if and only if there exists a linear relationship y = Î²0e + Î²1x.
â–·
When Î²1 > 0, we say that y is positively correlated with x.
â–·
When Î²1 < 0, we say that y is negatively correlated with x.
â€¢
|Ïxy| measures the degree to which y is linearly related to x. In other
words, |Ïxy| â‰ˆ1 if and only if y â‰ˆÎ²0e + Î²1x for some Î²0 and Î²1.
â–·
Positive correlation is measured by the degree to which Ïxy â‰ˆ1.
â–·
Negative correlation is measured by the degree to which Ïxy â‰ˆâˆ’1.
If the data in x and y are plotted in â„œ2 as points (xi, yi), then, as depicted in
Figure 5.4.1, Ïxy â‰ˆ1 means that the points lie near a straight line with positive
slope, while Ïxy â‰ˆâˆ’1 means that the points lie near a line with negative slope,
and Ïxy â‰ˆ0 means that the points do not lie near a straight line.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
. .
. .
... .
.
.. .
.
.
....
.
. .
. .
.. .
.
..
. .
.
...
. ..
.
..
.
.
....
.
.
. .
.
..
.
.
.
Positive Correlation
No Correlation
Negative Correlation
Ïxy â‰ˆ1
Ïxy â‰ˆâˆ’1
Ïxy â‰ˆ0
Figure 5.4.1
If |Ïxy| â‰ˆ1, then the theory of least squares as presented in Â§4.6 can be used
to determine a â€œbest-ï¬ttingâ€ straight line.

298
Chapter 5
Norms, Inner Products, and Orthogonality
Orthonormal Sets
B = {u1, u2, . . . , un} is called an orthonormal set whenever âˆ¥uiâˆ¥= 1
for each i, and ui âŠ¥uj for all i Ì¸= j. In other words,
âŸ¨ui ujâŸ©=

1
when i = j,
0
when i Ì¸= j.
â€¢
Every orthonormal set is linearly independent.
(5.4.2)
â€¢
Every orthonormal set of n vectors from an n-dimensional space V
is an orthonormal basis for V.
Proof.
The second point follows from the ï¬rst. To prove the ï¬rst statement,
suppose B = {u1, u2, . . . , un} is orthonormal. If 0 = Î±1u1 +Î±2u2 +Â· Â· Â·+Î±nun,
use the properties of an inner product to write
0 = âŸ¨ui 0âŸ©= âŸ¨ui Î±1u1 + Î±2u2 + Â· Â· Â· + Î±nunâŸ©
= Î±1 âŸ¨ui u1âŸ©+ Â· Â· Â· + Î±i âŸ¨ui uiâŸ©+ Â· Â· Â· + Î±n âŸ¨ui unâŸ©= Î±i âˆ¥uiâˆ¥2
= Î±i
for each i.
Example 5.4.4
The set Bâ€² =

u1 =

1
âˆ’1
0

, u2 =
 1
1
1

, u3 =
 âˆ’1
âˆ’1
2
!
is a set of mutually
orthogonal vectors because uT
i uj = 0 for i Ì¸= j, but Bâ€² is not an orthonormal
setâ€”each vector does not have unit length. However, itâ€™s easy to convert an
orthogonal set (not containing a zero vector) into an orthonormal set by simply
normalizing each vector. Since âˆ¥u1âˆ¥=
âˆš
2,
âˆ¥u2âˆ¥=
âˆš
3, and âˆ¥u3âˆ¥=
âˆš
6, it
follows that B =

u1/
âˆš
2, u2/
âˆš
3, u3/
âˆš
6

is orthonormal.
The most common orthonormal basis is S = {e1, e2, . . . , en} , the stan-
dard basis for â„œn and Cn, and, as illustrated below for â„œ2 and â„œ3, these
orthonormal vectors are directed along the standard coordinate axes.
e1
e2
x
y
e1
e2
e3
x
y
z

5.4 Orthogonal Vectors
299
Another orthonormal basis B need not be directed in the same way as S, but
thatâ€™s the only signiï¬cant diï¬€erence because itâ€™s geometrically evident that B
must amount to some rotation of S. Consequently, we should expect general
orthonormal bases to provide essentially the same advantages as the standard
basis. For example, an important function of the standard basis S for â„œn is to
provide coordinate representations by writing
x = [x]S =
ï£«
ï£¬
ï£¬
ï£­
x1
x2
...
xn
ï£¶
ï£·
ï£·
ï£¸
to mean
x = x1e1 + x2e2 + Â· Â· Â· + xnen.
With respect to a general basis B = {u1, u2, . . . , un} , the coordinates of x
are the scalars Î¾i in the representation x = Î¾1u1 + Î¾2u2 + Â· Â· Â· + Î¾nun, and, as
illustrated in Example 4.7.2, ï¬nding the Î¾i â€™s requires solving an n Ã— n system,
a nuisance we would like to avoid. But if B is an orthonormal basis, then the
Î¾i â€™s are readily available because âŸ¨ui xâŸ©= âŸ¨ui Î¾1u1 + Î¾2u2 + Â· Â· Â· + Î¾nunâŸ©=
n
j=1 Î¾j âŸ¨ui ujâŸ©= Î¾i âˆ¥uiâˆ¥2 = Î¾i. This yields the Fourier
40 expansion of x.
Fourier Expansions
If B = {u1, u2, . . . , un} is an orthonormal basis for an inner-product
space V, then each x âˆˆV can be expressed as
x = âŸ¨u1 xâŸ©u1 + âŸ¨u2 xâŸ©u2 + Â· Â· Â· + âŸ¨un xâŸ©un.
(5.4.3)
This is called the Fourier expansion of x. The scalars Î¾i = âŸ¨ui xâŸ©
are the coordinates of x with respect to B, and they are called the
Fourier coeï¬ƒcients. Geometrically, the Fourier expansion resolves x
into n mutually orthogonal vectors âŸ¨ui xâŸ©ui, each of which represents
the orthogonal projection of x onto the space (line) spanned by ui.
(More is said in Example 5.13.1 on p. 431 and Exercise 5.13.11.)
40
Jean Baptiste Joseph Fourier (1768â€“1830) was a French mathematician and physicist who,
while studying heat ï¬‚ow, developed expansions similar to (5.4.3). Fourierâ€™s work dealt with
special inï¬nite-dimensional inner-product spaces involving trigonometric functions as discussed
in Example 5.4.6. Although they were apparently used earlier by Daniel Bernoulli (1700â€“1782)
to solve problems concerned with vibrating strings, these orthogonal expansions became known
as Fourier series, and they are now a fundamental tool in applied mathematics. Born the son
of a tailor, Fourier was orphaned at the age of eight. Although he showed a great aptitude for
mathematics at an early age, he was denied his dream of entering the French artillery because
of his â€œlow birth.â€ Instead, he trained for the priesthood, but he never took his vows. However,
his talents did not go unrecognized, and he later became a favorite of Napoleon. Fourierâ€™s work
is now considered as marking an epoch in the history of both pure and applied mathematics.
The next time you are in Paris, check out Fourierâ€™s plaque on the ï¬rst level of the Eiï¬€el Tower.

300
Chapter 5
Norms, Inner Products, and Orthogonality
Example 5.4.5
Problem: Determine the Fourier expansion of x =
 âˆ’1
2
1

with respect to the
standard inner product and the orthonormal basis given in Example 5.4.4
B =
ï£±
ï£²
ï£³u1 =
1
âˆš
2
ï£«
ï£­
1
âˆ’1
0
ï£¶
ï£¸, u2 =
1
âˆš
3
ï£«
ï£­
1
1
1
ï£¶
ï£¸, u3 =
1
âˆš
6
ï£«
ï£­
âˆ’1
âˆ’1
2
ï£¶
ï£¸
ï£¼
ï£½
ï£¾.
Solution: The Fourier coeï¬ƒcients are
Î¾1 = âŸ¨u1 xâŸ©= âˆ’3
âˆš
2,
Î¾2 = âŸ¨u2 xâŸ©=
2
âˆš
3,
Î¾3 = âŸ¨u3 xâŸ©=
1
âˆš
6,
so
x = Î¾1u1 + Î¾2u2 + Î¾3u3 = 1
2
ï£«
ï£­
âˆ’3
3
0
ï£¶
ï£¸+ 1
3
ï£«
ï£­
2
2
2
ï£¶
ï£¸+ 1
6
ï£«
ï£­
âˆ’1
âˆ’1
2
ï£¶
ï£¸.
You may ï¬nd it instructive to sketch a picture of these vectors in â„œ3.
Example 5.4.6
Fourier Series.
Let V be the inner-product space of real-valued functions
that are integrable on the interval (âˆ’Ï€, Ï€) and where the inner product and
norm are given by
âŸ¨f|gâŸ©=
 Ï€
âˆ’Ï€
f(t)g(t)dt
and
âˆ¥fâˆ¥=
 Ï€
âˆ’Ï€
f 2(t)dt
1/2
.
Itâ€™s straightforward to verify that the set of trigonometric functions
Bâ€² = {1, cos t, cos 2t, . . . , sin t, sin 2t, sin 3t, . . .}
is a set of mutually orthogonal vectors, so normalizing each vector produces the
orthonormal set
B =

1
âˆš
2Ï€ , cos t
âˆšÏ€ , cos 2t
âˆšÏ€ , . . . , sin t
âˆšÏ€ , sin 2t
âˆšÏ€ , sin 3t
âˆšÏ€ , . . .
!
.
Given an arbitrary f âˆˆV, we construct its Fourier expansion
F(t) = Î±0
1
âˆš
2Ï€ +
âˆ

k=1
Î±k
cos kt
âˆšÏ€
+
âˆ

k=1
Î²k
sin kt
âˆšÏ€ ,
(5.4.4)

5.4 Orthogonal Vectors
301
where the Fourier coeï¬ƒcients are given by
Î±0 =

1
âˆš
2Ï€ f

=
1
âˆš
2Ï€
 Ï€
âˆ’Ï€
f(t)dt ,
Î±k =
cos kt
âˆšÏ€
f

=
1
âˆšÏ€
 Ï€
âˆ’Ï€
f(t) cos kt dt
for k = 1, 2, 3, . . . ,
Î²k =
sin kt
âˆšÏ€
f

=
1
âˆšÏ€
 Ï€
âˆ’Ï€
f(t) sin kt dt
for k = 1, 2, 3, . . . .
Substituting these coeï¬ƒcients in (5.4.4) produces the inï¬nite series
F(t) = a0
2 +
âˆ

n=1
(an cos nt + bn sin nt) ,
(5.4.5)
where
an = 1
Ï€
 Ï€
âˆ’Ï€
f(t) cos nt dt
and
bn = 1
Ï€
 Ï€
âˆ’Ï€
f(t) sin nt dt.
(5.4.6)
The series F(t) in (5.4.5) is called the Fourier series expansion for f(t), but,
unlike the situation in ï¬nite-dimensional spaces, F(t) need not agree with the
original function f(t). After all, F is periodic, so there is no hope of agreement
when f is not periodic. However, the following statement is true.
â€¢
If f(t) is a periodic function with period 2Ï€ that is sectionally continu-
ous
41 on the interval (âˆ’Ï€, Ï€), then the Fourier series F(t) converges to
f(t) at each t âˆˆ(âˆ’Ï€, Ï€), where f is continuous. If f is discontinuous
at t0 but possesses left-hand and right-hand derivatives at t0, then F(t0)
converges to the average value
F(t0) = f(tâˆ’
0 ) + f(t+
0 )
2
,
where f(tâˆ’
0 ) and f(t+
0 ) denote the one-sided limits f(tâˆ’
0 ) = limtâ†’tâˆ’
0 f(t)
and f(t+
0 ) = limtâ†’t+
0 f(t).
For example, the square wave function deï¬ned by
f(t) =

âˆ’1
when âˆ’Ï€ < t < 0,
1
when
0 < t < Ï€,
41
A function f is sectionally continuous on (a, b) when f has only a ï¬nite number of discon-
tinuities in (a, b) and the one-sided limits exist at each point of discontinuity as well as at the
end points a and b.

302
Chapter 5
Norms, Inner Products, and Orthogonality
and illustrated in Figure 5.4.2, satisï¬es these conditions. The value of f at t = 0
is irrelevantâ€”itâ€™s not even necessary that f(0) be deï¬ned.
Ï€
1
âˆ’Ï€
âˆ’1
Figure 5.4.2
To ï¬nd the Fourier series expansion for f, compute the coeï¬ƒcients in (5.4.6) as
an = 1
Ï€
 Ï€
âˆ’Ï€
f(t) cos nt dt = 1
Ï€
 0
âˆ’Ï€
âˆ’cos nt dt + 1
Ï€
 Ï€
0
cos nt dt
= 0,
bn = 1
Ï€
 Ï€
âˆ’Ï€
f(t) sin nt dt = 1
Ï€
 0
âˆ’Ï€
âˆ’sin nt dt + 1
Ï€
 Ï€
0
sin nt dt
= 2
nÏ€ (1 âˆ’cos nÏ€) =

0
when n is even,
4/nÏ€
when n is odd,
so that
F(t) = 4
Ï€ sin t + 4
3Ï€ sin 3t + 4
5Ï€ sin 5t + Â· Â· Â· =
âˆ

n=1
4
(2n âˆ’1)Ï€ sin(2n âˆ’1)t.
For each t âˆˆ(âˆ’Ï€, Ï€), except t = 0, it must be the case that F(t) = f(t), and
F(0) = f(0âˆ’) + f(0+)
2
= 0.
Not only does F(t) agree with f(t) everywhere f is deï¬ned, but F also pro-
vides a periodic extension of f in the sense that the graph of F(t) is the entire
square wave depicted in Figure 5.4.2â€”the values at the points of discontinuity
(the jumps) are F(Â±nÏ€) = 0.

5.4 Orthogonal Vectors
303
Exercises for section 5.4
5.4.1. Using the standard inner product, determine which of the following pairs
are orthogonal vectors in the indicated space.
(a)
x =
ï£«
ï£­
1
âˆ’3
4
ï£¶
ï£¸
and
y =
ï£«
ï£­
âˆ’2
2
2
ï£¶
ï£¸
in â„œ3,
(b)
x =
ï£«
ï£¬
ï£­
i
1 + i
2
1 âˆ’i
ï£¶
ï£·
ï£¸
and
y =
ï£«
ï£¬
ï£­
0
1 + i
âˆ’2
1 âˆ’i
ï£¶
ï£·
ï£¸
in C4,
(c)
x =
ï£«
ï£¬
ï£­
1
âˆ’2
3
4
ï£¶
ï£·
ï£¸
and
y =
ï£«
ï£¬
ï£­
4
2
âˆ’1
1
ï£¶
ï£·
ï£¸
in â„œ4,
(d)
x =
ï£«
ï£­
1 + i
1
i
ï£¶
ï£¸
and
y =
ï£«
ï£­
1 âˆ’i
âˆ’3
âˆ’i
ï£¶
ï£¸
in C3,
(e)
x =
ï£«
ï£¬
ï£¬
ï£­
0
0
...
0
ï£¶
ï£·
ï£·
ï£¸
and
y =
ï£«
ï£¬
ï£¬
ï£­
y1
y2
...
yn
ï£¶
ï£·
ï£·
ï£¸
in â„œn.
5.4.2. Find two vectors of unit norm that are orthogonal to u =

3
âˆ’2

.
5.4.3. Consider the following set of three vectors.
ï£±
ï£´
ï£²
ï£´
ï£³
x1 =
ï£«
ï£¬
ï£­
1
âˆ’1
0
2
ï£¶
ï£·
ï£¸, x2 =
ï£«
ï£¬
ï£­
1
1
1
0
ï£¶
ï£·
ï£¸, x3 =
ï£«
ï£¬
ï£­
âˆ’1
âˆ’1
2
0
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
.
(a)
Using the standard inner product in â„œ4, verify that these vec-
tors are mutually orthogonal.
(b)
Find a nonzero vector x4 such that {x1, x2, x3, x4} is a set
of mutually orthogonal vectors.
(c)
Convert the resulting set into an orthonormal basis for â„œ4.
5.4.4. Using the standard inner product, determine the Fourier expansion of
x with respect to B, where
x =
ï£«
ï£­
1
0
âˆ’2
ï£¶
ï£¸
and
B =
ï£±
ï£²
ï£³
1
âˆš
2
ï£«
ï£­
1
âˆ’1
0
ï£¶
ï£¸,
1
âˆš
3
ï£«
ï£­
1
1
1
ï£¶
ï£¸,
1
âˆš
6
ï£«
ï£­
âˆ’1
âˆ’1
2
ï£¶
ï£¸
ï£¼
ï£½
ï£¾.

304
Chapter 5
Norms, Inner Products, and Orthogonality
5.4.5. With respect to the inner product for matrices given by (5.3.2), verify
that the set
B =
 1
âˆš
2

0
1
1
0

,
1
âˆš
2

1
0
0
âˆ’1

,
1
2

1
âˆ’1
1
1

,
1
2

1
1
âˆ’1
1
!
is an orthonormal basis for â„œ2Ã—2, and then compute the Fourier expan-
sion of A =
 1
1
1
1

with respect to B.
5.4.6. Determine the angle between x =

2
âˆ’1
1

and y =
 1
1
2

.
5.4.7. Given an orthonormal basis B for a space V, explain why the Fourier
expansion for x âˆˆV is uniquely determined by B.
5.4.8. Explain why the columns of UnÃ—n are an orthonormal basis for Cn if
and only if Uâˆ—= Uâˆ’1. Such matrices are said to be unitaryâ€”their
properties are studied in a later section.
5.4.9. Matrices with the property Aâˆ—A = AAâˆ—are said to be normal. No-
tice that hermitian matrices as well as real symmetric matrices are in-
cluded in the class of normal matrices. Prove that if A is normal, then
R (A) âŠ¥N (A)â€”i.e., every vector in R (A) is orthogonal to every vec-
tor in N (A). Hint: Recall equations (4.5.5) and (4.5.6).
5.4.10. Using the trace inner product described in Example 5.3.1, determine the
angle between the following pairs of matrices.
(a)
I =

1
0
0
1

and
B =

1
1
1
1

.
(b)
A =

1
3
2
4

and
B =

2
âˆ’2
2
0

.
5.4.11. Why is the deï¬nition for cos Î¸ given in (5.4.1) not good for Cn? Explain
how to deï¬ne cos Î¸ so that it makes sense in Cn.
5.4.12. If {u1, u2, . . . , un} is an orthonormal basis for an inner-product space
V, explain why
âŸ¨x yâŸ©=

i
âŸ¨x uiâŸ©âŸ¨ui yâŸ©
holds for every x, y âˆˆV.

5.4 Orthogonal Vectors
305
5.4.13. Consider a real inner-product space, where âˆ¥â‹†âˆ¥2 = âŸ¨â‹†â‹†âŸ©.
(a)
Prove that if âˆ¥xâˆ¥= âˆ¥yâˆ¥, then (x + y) âŠ¥(x âˆ’y).
(b)
For the standard inner product in â„œ2, draw a picture of this.
That is, sketch the location of x + y and x âˆ’y for two vectors
with equal norms.
5.4.14. Pythagorean Theorem.
Let V be a general inner-product space in
which âˆ¥â‹†âˆ¥2 = âŸ¨â‹†â‹†âŸ©.
(a)
When V is a real space, prove that x âŠ¥y if and only if
âˆ¥x + yâˆ¥2 = âˆ¥xâˆ¥2 + âˆ¥yâˆ¥2 . (Something would be wrong if this
were not true because this is where the deï¬nition of orthogonal-
ity originated.)
(b)
Construct an example to show that one of the implications in
part (a) does not hold when V is a complex space.
(c)
When V is a complex space, prove that x âŠ¥y if and only if
âˆ¥Î±x + Î²yâˆ¥2 = âˆ¥Î±xâˆ¥2 + âˆ¥Î²yâˆ¥2 for all scalars Î± and Î².
5.4.15. Let B = {u1, u2, . . . , un} be an orthonormal basis for an inner-product
space V, and let x = 
i Î¾iui be the Fourier expansion of x âˆˆV.
(a)
If V is a real space, and if Î¸i is the angle between ui and x,
explain why
Î¾i = âˆ¥xâˆ¥cos Î¸i.
Sketch a picture of this in â„œ2 or â„œ3 to show why the com-
ponent Î¾iui represents the orthogonal projection of x onto
the line determined by ui, and thus illustrate the fact that a
Fourier expansion is nothing more than simply resolving x into
mutually orthogonal components.
(b)
Derive Parsevalâ€™s identity,
42 which says n
i=1 |Î¾i|2 = âˆ¥xâˆ¥2 .
5.4.16. Let B = {u1, u2, . . . , uk} be an orthonormal set in an n-dimensional
inner-product space V. Derive Besselâ€™s inequality,
43 which says that
if x âˆˆV and Î¾i = âŸ¨ui xâŸ©, then
k

i=1
|Î¾i|2 â‰¤âˆ¥xâˆ¥2 .
Explain why equality holds if and only if x âˆˆspan {u1, u2, . . . , uk} .
Hint: Consider âˆ¥x âˆ’k
i=1 Î¾iuiâˆ¥2.
42
This result appeared in the second of the ï¬ve mathematical publications by Marc-Antoine
Parseval des ChË†enes (1755â€“1836). Parseval was a royalist who had to ï¬‚ee from France when
Napoleon ordered his arrest for publishing poetry against the regime.
43
This inequality is named in honor of the German astronomer and mathematician Friedrich
Wilhelm Bessel (1784â€“1846), who devoted his life to understanding the motions of the stars.
In the process he introduced several useful mathematical ideas.

306
Chapter 5
Norms, Inner Products, and Orthogonality
5.4.17. Construct an example using the standard inner product in â„œn to show
that two vectors x and y can have an angle between them that is close
to Ï€/2 without xT y being close to 0. Hint: Consider n to be large,
and use the vector e of all 1â€™s for one of the vectors.
5.4.18. It was demonstrated in Example 5.4.3 that y is linearly correlated with
x in the sense that y â‰ˆÎ²0e + Î²1x if and only if the standardization
vectors zx and zy are â€œcloseâ€ in the sense that they are almost on the
same line in â„œn. Explain why simply measuring âˆ¥zx âˆ’zyâˆ¥2 does not
always gauge the degree of linear correlation.
5.4.19. Let Î¸ be the angle between two vectors x and y from a real inner-
product space.
(a)
Prove that cos Î¸ = 1 if and only if y = Î±x for Î± > 0.
(b)
Prove that cos Î¸ = âˆ’1 if and only if y = Î±x for Î± < 0.
Hint: Use the generalization of Exercise 5.1.9.
5.4.20. With respect to the orthonormal set
B =

1
âˆš
2Ï€ , cos t
âˆšÏ€ , cos 2t
âˆšÏ€ , . . . , sin t
âˆšÏ€ , sin 2t
âˆšÏ€ , sin 3t
âˆšÏ€ , . . .
!
,
determine the Fourier series expansion of the saw-toothed function
deï¬ned by f(t) = t for âˆ’Ï€ < t < Ï€. The periodic extension of this
function is depicted in Figure 5.4.3.
Ï€
Ï€
âˆ’Ï€
âˆ’Ï€
Figure 5.4.3

5.5 Gramâ€“Schmidt Procedure
307
5.5
GRAMâ€“SCHMIDT PROCEDURE
As discussed in Â§5.4, orthonormal bases possess signiï¬cant advantages over bases
that are not orthonormal. The spaces â„œn and Cn clearly possess orthonormal
bases (e.g., the standard basis), but what about other spaces? Does every ï¬nite-
dimensional space possess an orthonormal basis, and, if so, how can one be
produced? The Gramâ€“Schmidt
44 orthogonalization procedure developed below
answers these questions.
Let B = {x1, x2, . . . , xn} be an arbitrary basis (not necessarily orthonormal)
for an n-dimensional inner-product space S, and remember that âˆ¥â‹†âˆ¥= âŸ¨â‹†â‹†âŸ©1/2.
Objective:
Use B to construct an orthonormal basis O = {u1, u2, . . . , un}
for S.
Strategy:
Construct O sequentially so that Ok = {u1, u2, . . . , uk} is an or-
thonormal basis for Sk = span {x1, x2, . . . , xk} for k = 1, . . . , n.
For k = 1, simply take u1 = x1/ âˆ¥x1âˆ¥. Itâ€™s clear that O1 = {u1} is an
orthonormal set whose span agrees with that of S1 = {x1} . Now reason in-
ductively. Suppose that Ok = {u1, u2, . . . , uk} is an orthonormal basis for
Sk = span {x1, x2, . . . , xk} , and consider the problem of ï¬nding one additional
vector uk+1 such that Ok+1 = {u1, u2, . . . , uk, uk+1} is an orthonormal basis
for Sk+1 = span {x1, x2, . . . , xk, xk+1} . For this to hold, the Fourier expansion
(p. 299) of xk+1 with respect to Ok+1 must be
xk+1 =
k+1

i=1
âŸ¨ui xk+1âŸ©ui,
which in turn implies that
uk+1 = xk+1 âˆ’k
i=1 âŸ¨ui xk+1âŸ©ui
âŸ¨uk+1 xk+1âŸ©
.
(5.5.1)
Since âˆ¥uk+1âˆ¥= 1, it follows from (5.5.1) that
| âŸ¨uk+1 xk+1âŸ©| =
xk+1 âˆ’
k

i=1
âŸ¨ui xk+1âŸ©ui
,
44
Jorgen P. Gram (1850â€“1916) was a Danish actuary who implicitly presented the essence of or-
thogonalization procedure in 1883. Gram was apparently unaware that Pierre-Simon Laplace
(1749â€“1827) had earlier used the method. Today, Gram is remembered primarily for his de-
velopment of this process, but in earlier times his name was also associated with the matrix
product Aâˆ—A that historically was referred to as the Gram matrix of A.
Erhard Schmidt (1876â€“1959) was a student of Hermann Schwarz (of CBS inequality fame) and
the great German mathematician David Hilbert. Schmidt explicitly employed the orthogonal-
ization process in 1907 in his study of integral equations, which in turn led to the development
of what are now called Hilbert spaces. Schmidt made signiï¬cant use of the orthogonalization
process to develop the geometry of Hilbert Spaces, and thus it came to bear Schmidtâ€™s name.

308
Chapter 5
Norms, Inner Products, and Orthogonality
so âŸ¨uk+1 xk+1âŸ©= eiÎ¸xk+1 âˆ’k
i=1 âŸ¨ui xk+1âŸ©ui
 for some 0 â‰¤Î¸ < 2Ï€, and
uk+1 =
xk+1 âˆ’k
i=1 âŸ¨ui xk+1âŸ©ui
eiÎ¸
xk+1 âˆ’k
i=1 âŸ¨ui xk+1âŸ©ui

.
Since the value of Î¸ in the scalar eiÎ¸ neither aï¬€ects span {u1, u2, . . . , uk+1} nor
the facts that âˆ¥uk+1âˆ¥= 1 and âŸ¨uk+1 uiâŸ©= 0 for all i â‰¤k, we can arbitrarily
deï¬ne uk+1 to be the vector corresponding to the Î¸ = 0 or, equivalently,
eiÎ¸ = 1. For the sake of convenience, let
Î½k+1 =
xk+1 âˆ’
k

i=1
âŸ¨ui xk+1âŸ©ui

so that we can write
u1 =
x1
âˆ¥x1âˆ¥
and
uk+1 = xk+1 âˆ’k
i=1 âŸ¨ui xk+1âŸ©ui
Î½k+1
for k > 0.
(5.5.2)
This sequence of vectors is called the Gramâ€“Schmidt sequence. A straight-
forward induction argument proves that Ok = {u1, u2, . . . , uk} is indeed an or-
thonormal basis for span {x1, x2, . . . , xk} for each k = 1, 2, . . . . Details are
called for in Exercise 5.5.7.
The orthogonalization procedure deï¬ned by (5.5.2) is valid for any inner-
product space, but if we concentrate on subspaces of â„œm or Cm with the stan-
dard inner product and euclidean norm, then we can formulate (5.5.2) in terms
of matrices. Suppose that B = {x1, x2, . . . , xn} is a basis for an n-dimensional
subspace S of CmÃ—1 so that the Gramâ€“Schmidt sequence (5.5.2) becomes
u1 =
x1
âˆ¥x1âˆ¥
and
uk =
xk âˆ’kâˆ’1
i=1 (uâˆ—
i xk) ui
xk âˆ’kâˆ’1
i=1 (uâˆ—
i xk) ui

for k = 2, 3, . . . , n. (5.5.3)
To express this in matrix notation, set
U1 = 0mÃ—1
and
Uk =

u1 | u2 | Â· Â· Â· | ukâˆ’1

mÃ—kâˆ’1
for k > 1,
and notice that
Uâˆ—
kxk =
ï£«
ï£¬
ï£¬
ï£­
uâˆ—
1xk
uâˆ—
2xk
...
uâˆ—
kâˆ’1xk
ï£¶
ï£·
ï£·
ï£¸
and
UkUâˆ—
kxk =
kâˆ’1

i=1
ui (uâˆ—
i xk) =
kâˆ’1

i=1
(uâˆ—
i xk) ui.
Since
xk âˆ’
kâˆ’1

i=1
(uâˆ—
i xk) ui = xk âˆ’UkUâˆ—
kxk = (I âˆ’UkUâˆ—
k) xk,
the vectors in (5.5.3) can be concisely written as
uk =
(I âˆ’UkUâˆ—
k) xk
âˆ¥(I âˆ’UkUâˆ—
k) xkâˆ¥
for k = 1, 2, . . . , n.
Below is a summary.

5.5 Gramâ€“Schmidt Procedure
309
Gramâ€“Schmidt Orthogonalization Procedure
If B = {x1, x2, . . . , xn} is a basis for a general inner-product space S,
then the Gramâ€“Schmidt sequence deï¬ned by
u1 =
x1
âˆ¥x1âˆ¥
and
uk =
xk âˆ’kâˆ’1
i=1 âŸ¨ui xkâŸ©ui
xk âˆ’kâˆ’1
i=1 âŸ¨ui xkâŸ©ui

for k = 2, . . . , n
is an orthonormal basis for S. When S is an n-dimensional subspace
of CmÃ—1, the Gramâ€“Schmidt sequence can be expressed as
uk =
(I âˆ’UkUâˆ—
k) xk
âˆ¥(I âˆ’UkUâˆ—
k) xkâˆ¥
for
k = 1, 2, . . . , n
(5.5.4)
in which U1 = 0mÃ—1 and Uk =

u1 | u2 | Â· Â· Â· | ukâˆ’1

mÃ—kâˆ’1 for k > 1.
Example 5.5.1
Classical Gramâ€“Schmidt Algorithm. The following formal algorithm is the
straightforward or â€œclassicalâ€ implementation of the Gramâ€“Schmidt procedure.
Interpret a â†b to mean that â€œa is deï¬ned to be (or overwritten by) b.â€
For k = 1:
u1 â†
x1
âˆ¥x1âˆ¥
For k > 1:
uk â†xk âˆ’
kâˆ’1

i=1
(uâˆ—
i xk)ui
uk â†
uk
âˆ¥ukâˆ¥
(See Exercise 5.5.10 for other formulations of the Gramâ€“Schmidt algorithm.)
Problem: Use the classical formulation of the Gramâ€“Schmidt procedure given
above to ï¬nd an orthonormal basis for the space spanned by the following three
linearly independent vectors.
x1 =
ï£«
ï£¬
ï£­
1
0
0
âˆ’1
ï£¶
ï£·
ï£¸,
x2 =
ï£«
ï£¬
ï£­
1
2
0
âˆ’1
ï£¶
ï£·
ï£¸,
x3 =
ï£«
ï£¬
ï£­
3
1
1
âˆ’1
ï£¶
ï£·
ï£¸.

310
Chapter 5
Norms, Inner Products, and Orthogonality
Solution:
k = 1:
u1 â†
x1
âˆ¥x1âˆ¥=
1
âˆš
2
ï£«
ï£¬
ï£­
1
0
0
âˆ’1
ï£¶
ï£·
ï£¸
k = 2:
u2 â†x2 âˆ’(uT
1 x2)u1 =
ï£«
ï£¬
ï£­
0
2
0
0
ï£¶
ï£·
ï£¸,
u2 â†
u2
âˆ¥u2âˆ¥=
ï£«
ï£¬
ï£­
0
1
0
0
ï£¶
ï£·
ï£¸
k = 3:
u3 â†x3 âˆ’(uT
1 x3)u1 âˆ’(uT
2 x3)u2 =
ï£«
ï£¬
ï£­
1
0
1
1
ï£¶
ï£·
ï£¸,
u3 â†
u3
âˆ¥u3âˆ¥=
1
âˆš
3
ï£«
ï£¬
ï£­
1
0
1
1
ï£¶
ï£·
ï£¸
Thus
u1 =
1
âˆš
2
ï£«
ï£¬
ï£­
1
0
0
âˆ’1
ï£¶
ï£·
ï£¸,
u2 =
ï£«
ï£¬
ï£­
0
1
0
0
ï£¶
ï£·
ï£¸,
u3 =
1
âˆš
3
ï£«
ï£¬
ï£­
1
0
1
1
ï£¶
ï£·
ï£¸
is the desired orthonormal basis.
The Gramâ€“Schmidt process frequently appears in the disguised form of a
matrix factorization. To see this, let AmÃ—n =

a1 | a2 | Â· Â· Â· | an

be a matrix with
linearly independent columns. When Gramâ€“Schmidt is applied to the columns
of A, the result is an orthonormal basis {q1, q2, . . . , qn} for R (A), where
q1 = a1
Î½1
and
qk = ak âˆ’kâˆ’1
i=1 âŸ¨qi akâŸ©qi
Î½k
for k = 2, 3, . . . , n,
where Î½1 = âˆ¥a1âˆ¥and Î½k =
ak âˆ’kâˆ’1
i=1 âŸ¨qi akâŸ©qi
 for k > 1. The above
relationships can be rewritten as
a1 = Î½1q1
and
ak = âŸ¨q1 akâŸ©q1 + Â· Â· Â· + âŸ¨qkâˆ’1 akâŸ©qkâˆ’1 + Î½kqk
for k > 1,
which in turn can be expressed in matrix form by writing

a1 | a2 | Â· Â· Â· | an

=

q1 | q2 | Â· Â· Â· | qn

ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î½1
âŸ¨q1 a2âŸ©
âŸ¨q1 a3âŸ©
Â· Â· Â·
âŸ¨q1 anâŸ©
0
Î½2
âŸ¨q2 a3âŸ©
Â· Â· Â·
âŸ¨q2 anâŸ©
0
0
Î½3
Â· Â· Â·
âŸ¨q3 anâŸ©
...
...
...
...
...
0
0
0
Â· Â· Â·
Î½n
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
This says that itâ€™s possible to factor a matrix with independent columns as
AmÃ—n = QmÃ—nRnÃ—n, where the columns of Q are an orthonormal basis for
R (A) and R is an upper-triangular matrix with positive diagonal elements.

5.5 Gramâ€“Schmidt Procedure
311
The factorization A = QR is called the QR factorization for A, and it is
uniquely determined by A (Exercise 5.5.8). When A and Q are not square,
some authors emphasize the point by calling A = QR the rectangular QR
factorizationâ€”the case when A and Q are square is further discussed on p. 345.
Below is a summary of the above observations.
QR Factorization
Every matrix AmÃ—n with linearly independent columns can be uniquely
factored as A = QR in which the columns of QmÃ—n are an orthonor-
mal basis for R (A) and RnÃ—n is an upper-triangular matrix with
positive diagonal entries.
â€¢
The QR factorization is the complete â€œroad mapâ€ of the Gramâ€“
Schmidt process because the columns of Q =

q1 | q2 | Â· Â· Â· | qn

are
the result of applying the Gramâ€“Schmidt procedure to the columns
of A =

a1 | a2 | Â· Â· Â· | an

and R is given by
R =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î½1
qâˆ—
1a2
qâˆ—
1a3
Â· Â· Â·
qâˆ—
1an
0
Î½2
qâˆ—
2a3
Â· Â· Â·
qâˆ—
2an
0
0
Î½3
Â· Â· Â·
qâˆ—
3an
...
...
...
...
...
0
0
0
Â· Â· Â·
Î½n
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
where Î½1 = âˆ¥a1âˆ¥and Î½k =
ak âˆ’kâˆ’1
i=1 âŸ¨qi akâŸ©qi
 for k > 1.
Example 5.5.2
Problem: Determine the QR factors of
A =
ï£«
ï£­
0
âˆ’20
âˆ’14
3
27
âˆ’4
4
11
âˆ’2
ï£¶
ï£¸.
Solution: Using the standard inner product for â„œn, apply the Gramâ€“Schmidt
procedure to the columns of A by setting
q1 = a1
Î½1
and
qk = ak âˆ’kâˆ’1
i=1

qT
i ak

qi
Î½k
for k = 2, 3,
where Î½1 = âˆ¥a1âˆ¥and Î½k =
ak âˆ’kâˆ’1
i=1

qT
i ak

qi
. The computation of these
quantities can be organized as follows.

312
Chapter 5
Norms, Inner Products, and Orthogonality
k = 1:
r11 â†âˆ¥a1âˆ¥= 5
and
q1 â†a1
r11
=
ï£«
ï£­
0
3/5
4/5
ï£¶
ï£¸
k = 2:
r12 â†qT
1 a2 = 25
q2 â†a2 âˆ’r12q1 =
ï£«
ï£­
âˆ’20
12
âˆ’9
ï£¶
ï£¸
r22 â†âˆ¥q2âˆ¥= 25 and q2 â†q2
r22
= 1
25
ï£«
ï£­
âˆ’20
12
âˆ’9
ï£¶
ï£¸
k = 3:
r13 â†qT
1 a3 = âˆ’4 and r23 â†qT
2 a3 = 10
q3 â†a3 âˆ’r13q1 âˆ’r23q2 = 2
5
ï£«
ï£­
âˆ’15
âˆ’16
12
ï£¶
ï£¸
r33 â†âˆ¥q3âˆ¥= 10 and q3 â†q3
r33
= 1
25
ï£«
ï£­
âˆ’15
âˆ’16
12
ï£¶
ï£¸
Therefore,
Q = 1
25
ï£«
ï£­
0
âˆ’20
âˆ’15
15
12
âˆ’16
20
âˆ’9
12
ï£¶
ï£¸
and
R =
ï£«
ï£­
5
25
âˆ’4
0
25
10
0
0
10
ï£¶
ï£¸.
We now have two important matrix factorizations, namely, the LU factor-
ization, discussed in Â§3.10 on p. 141 and the QR factorization. They are not the
same, but some striking analogies exist.
â€¢
Each factorization represents a reduction to upper-triangular formâ€”LU by
Gaussian elimination, and QR by Gramâ€“Schmidt. In particular, the LU fac-
torization is the complete â€œroad mapâ€ of Gaussian elimination applied to a
square nonsingular matrix, whereas QR is the complete road map of Gramâ€“
Schmidt applied to a matrix with linearly independent columns.
â€¢
When they exist, both factorizations A = LU and A = QR are uniquely
determined by A.
â€¢
Once the LU factors (assuming they exist) of a nonsingular matrix A are
known, the solution of Ax = b is easily computedâ€”solve Ly = b by
forward substitution, and then solve Ux = y by back substitution (see
p. 146). The QR factors can be used in a similar manner. If A âˆˆâ„œnÃ—n is
nonsingular, then QT = Qâˆ’1 (because Q has orthonormal columns), so
Ax = b
â‡â‡’
QRx = b
â‡â‡’
Rx = QT b, which is also a triangular
system that is solved by back substitution.

5.5 Gramâ€“Schmidt Procedure
313
While the LU and QR factors can be used in more or less the same way to
solve nonsingular systems, things are diï¬€erent for singular and rectangular cases
because Ax = b might be inconsistent, in which case a least squares solution
as described in Â§4.6, (p. 223) may be desired. Unfortunately, the LU factors of
A donâ€™t exist when A is rectangular. And even if A is square and has an
LU factorization, the LU factors of A are not much help in solving the system
of normal equations AT Ax = AT b that produces least squares solutions. But
the QR factors of AmÃ—n always exist as long as A has linearly independent
columns, and, as demonstrated in the following example, the QR factors provide
the least squares solution of an inconsistent system in exactly the same way as
they provide the solution of a consistent system.
Example 5.5.3
Application to the Least Squares Problem. If Ax = b is a possibly in-
consistent (real) system, then, as discussed on p. 226, the set of all least squares
solutions is the set of solutions to the system of normal equations
AT Ax = AT b.
(5.5.5)
But computing AT A and then performing an LU factorization of AT A to solve
(5.5.5) is generally not advisable. First, itâ€™s ineï¬ƒcient and, second, as pointed
out in Example 4.5.1, computing AT A with ï¬‚oating-point arithmetic can result
in a loss of signiï¬cant information. The QR approach doesnâ€™t suï¬€er from either
of these objections. Suppose that rank (AmÃ—n) = n (so that there is a unique
least squares solution), and let A = QR be the QR factorization. Because the
columns of Q are an orthonormal set, it follows that QT Q = In, so
AT A = (QR)T (QR) = RT QT QR = RT R.
(5.5.6)
Consequently, the normal equations (5.5.5) can be written as
RT Rx = RT QT b.
(5.5.7)
But RT is nonsingular (it is triangular with positive diagonal entries), so (5.5.7)
simpliï¬es to become
Rx = QT b.
(5.5.8)
This is just an upper-triangular system that is eï¬ƒciently solved by back substi-
tution. In other words, most of the work involved in solving the least squares
problem is in computing the QR factorization of A. Finally, notice that
x = Râˆ’1QT b =

AT A
âˆ’1AT b
is the solution of Ax = b when the system is consistent as well as the least
squares solution when the system is inconsistent (see p. 214). That is, with the
QR approach, it makes no diï¬€erence whether or not Ax = b is consistent
because in both cases things boil down to solving the same equationâ€”namely,
(5.5.8). Below is a formal summary.

314
Chapter 5
Norms, Inner Products, and Orthogonality
Linear Systems and the QR Factorization
If rank (AmÃ—n) = n, and if A = QR is the QR factorization, then the
solution of the nonsingular triangular system
Rx = QT b
(5.5.9)
is either the solution or the least squares solution of Ax = b depending
on whether or not Ax = b is consistent.
Itâ€™s worthwhile to reemphasize that the QR approach to the least squares prob-
lem obviates the need to explicitly compute the product AT A. But if AT A is
ever needed, it is retrievable from the factorization AT A = RT R. In fact, this
is the Cholesky factorization of AT A as discussed in Example 3.10.7, p. 154.
The Gramâ€“Schmidt procedure is a powerful theoretical tool, but itâ€™s not a
good numerical algorithm when implemented in the straightforward or â€œclassi-
calâ€ sense. When ï¬‚oating-point arithmetic is used, the classical Gramâ€“Schmidt
algorithm applied to a set of vectors that is not already close to being an orthog-
onal set can produce a set of vectors that is far from being an orthogonal set. To
see this, consider the following example.
Example 5.5.4
Problem: Using 3-digit ï¬‚oating-point arithmetic, apply the classical Gramâ€“
Schmidt algorithm to the set
x1 =
ï£«
ï£­
1
10âˆ’3
10âˆ’3
ï£¶
ï£¸,
x2 =
ï£«
ï£­
1
10âˆ’3
0
ï£¶
ï£¸,
x3 =
ï£«
ï£­
1
0
10âˆ’3
ï£¶
ï£¸.
Solution:
k = 1:
fl âˆ¥x1âˆ¥= 1, so u1 â†x1.
k = 2:
fl

uT
1 x2

= 1, so
u2 â†x2 âˆ’

uT
1 x2

u1 =
ï£«
ï£­
0
0
âˆ’10âˆ’3
ï£¶
ï£¸
and
u2 â†fl
 u2
âˆ¥u2âˆ¥

=
ï£«
ï£­
0
0
âˆ’1
ï£¶
ï£¸.
k = 3:
fl

uT
1 x3

= 1 and fl

uT
2 x3

= âˆ’10âˆ’3, so
u3â†x3âˆ’

uT
1 x3

u1âˆ’

uT
2 x3

u2=
ï£«
ï£­
0
âˆ’10âˆ’3
âˆ’10âˆ’3
ï£¶
ï£¸and u3â†fl
 u3
âˆ¥u3âˆ¥

=
ï£«
ï£­
0
âˆ’.709
âˆ’.709
ï£¶
ï£¸.

5.5 Gramâ€“Schmidt Procedure
315
Therefore, classical Gramâ€“Schmidt with 3-digit arithmetic returns
u1 =
ï£«
ï£­
1
10âˆ’3
10âˆ’3
ï£¶
ï£¸,
u2 =
ï£«
ï£­
0
0
âˆ’1
ï£¶
ï£¸,
u3 =
ï£«
ï£­
0
âˆ’.709
âˆ’.709
ï£¶
ï£¸,
(5.5.10)
which is unsatisfactory because u2 and u3 are far from being orthogonal.
Itâ€™s possible to improve the numerical stability of the orthogonalization pro-
cess by rearranging the order of the calculations. Recall from (5.5.4) that
uk =
(I âˆ’UkUâˆ—
k) xk
âˆ¥(I âˆ’UkUâˆ—
k) xkâˆ¥,
where
U1 = 0 and Uk =

u1 | u2 | Â· Â· Â· | ukâˆ’1

.
If E1 = I and Ei = I âˆ’uiâˆ’1uâˆ—
iâˆ’1 for i > 1, then the orthogonality of the ui â€™s
insures that
Ek Â· Â· Â· E2E1 = I âˆ’u1uâˆ—
1 âˆ’u2uâˆ—
2 âˆ’Â· Â· Â· âˆ’ukâˆ’1uâˆ—
kâˆ’1 = I âˆ’UkUâˆ—
k,
so the Gramâ€“Schmidt sequence can also be expressed as
uk =
Ek Â· Â· Â· E2E1xk
âˆ¥Ek Â· Â· Â· E2E1xkâˆ¥
for k = 1, 2, . . . , n.
This means that the Gramâ€“Schmidt sequence can be generated as follows:
{x1, x2, . . . , xn}
Normalize 1-st
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’{u1, x2, . . . , xn}
Apply E2
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’{u1, E2x2, E2x3, . . . , E2xn}
Normalize 2-nd
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’{u1, u2, E2x3, . . . , E2xn}
Apply E3
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’{u1, u2, E3E2x3, . . . , E3E2xn}
Normalize 3-rd
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’{u1, u2, u3, E3E2x4, . . . , E3E2xn} ,
etc.
While there is no theoretical diï¬€erence, this â€œmodiï¬edâ€ algorithm is numerically
more stable than the classical algorithm when ï¬‚oating-point arithmetic is used.
The kth step of the classical algorithm alters only the kth vector, but the kth
step of the modiï¬ed algorithm â€œupdatesâ€ all vectors from the kth through the
last, and conditioning the unorthogonalized tail in this way makes a diï¬€erence.

316
Chapter 5
Norms, Inner Products, and Orthogonality
Modiï¬ed Gramâ€“Schmidt Algorithm
For a linearly independent set {x1, x2, . . . , xn} âŠ‚CmÃ—1, the Gramâ€“
Schmidt sequence given on p. 309 can be alternately described as
uk =
Ek Â· Â· Â· E2E1xk
âˆ¥Ek Â· Â· Â· E2E1xkâˆ¥with E1 = I, Ei = I âˆ’uiâˆ’1uâˆ—
iâˆ’1 for i > 1,
and this sequence is generated by the following algorithm.
For k = 1:
u1 â†x1/ âˆ¥x1âˆ¥
and
uj â†xj for j = 2, 3, . . . , n
For k > 1:
uj â†Ekuj = uj âˆ’

uâˆ—
kâˆ’1uj

ukâˆ’1 for j = k, k + 1, . . . , n
uk â†uk/ âˆ¥ukâˆ¥
(An alternate implementation is given in Exercise 5.5.10.)
To see that the modiï¬ed version of Gramâ€“Schmidt can indeed make a dif-
ference when ï¬‚oating-point arithmetic is used, consider the following example.
Example 5.5.5
Problem: Use 3-digit ï¬‚oating-point arithmetic, and apply the modiï¬ed Gramâ€“
Schmidt algorithm to the set given in Example 5.5.4 (p. 314), and then compare
the results of the modiï¬ed algorithm with those of the classical algorithm.
Solution: x1 =
ï£«
ï£­
1
10âˆ’3
10âˆ’3
ï£¶
ï£¸,
x2 =
ï£«
ï£­
1
10âˆ’3
0
ï£¶
ï£¸,
x3 =
ï£«
ï£­
1
0
10âˆ’3
ï£¶
ï£¸.
k = 1:
fl âˆ¥x1âˆ¥= 1, so {u1, u2, u3} â†{x1, x2, x3} .
k = 2:
fl

uT
1 u2

= 1 and fl

uT
1 u3

= 1, so
u2 â†u2 âˆ’

uT
1 u2

u1 =
ï£«
ï£­
0
0
âˆ’10âˆ’3
ï£¶
ï£¸,
u3 â†u3 âˆ’

uT
1 u3

u1 =
ï£«
ï£­
0
âˆ’10âˆ’3
0
ï£¶
ï£¸,
and
u2 â†
u2
âˆ¥u2âˆ¥=
ï£«
ï£­
0
0
âˆ’1
ï£¶
ï£¸.
k = 3:
uT
2 u3 = 0, so
u3 â†u3 âˆ’

uT
2 u3

u2 =
ï£«
ï£­
0
âˆ’10âˆ’3
0
ï£¶
ï£¸
and
u3 â†
u3
âˆ¥u3âˆ¥=
ï£«
ï£­
0
âˆ’1
0
ï£¶
ï£¸.

5.5 Gramâ€“Schmidt Procedure
317
Thus the modiï¬ed Gramâ€“Schmidt algorithm produces
u1 =
ï£«
ï£­
1
10âˆ’3
10âˆ’3
ï£¶
ï£¸,
u2 =
ï£«
ï£­
0
0
âˆ’1
ï£¶
ï£¸,
u3 =
ï£«
ï£­
0
âˆ’1
0
ï£¶
ï£¸,
(5.5.11)
which is as good as one can expect using 3-digit arithmetic. Comparing (5.5.11)
with the result (5.5.10) obtained in Example 5.5.4 illuminates the advantage
possessed by modiï¬ed Gramâ€“Schmidt algorithm over the classical algorithm.
Below is a summary of some facts concerning the modiï¬ed Gramâ€“Schmidt
algorithm compared with the classical implementation.
Summary
â€¢
When the Gramâ€“Schmidt procedures (classical or modiï¬ed) are ap-
plied to the columns of A using exact arithmetic, each produces an
orthonormal basis for R (A).
â€¢
For computing a QR factorization in ï¬‚oating-point arithmetic, the
modiï¬ed algorithm produces results that are at least as good as and
often better than the classical algorithm, but the modiï¬ed algorithm
is not unconditionally stableâ€”there are situations in which it fails
to produce a set of columns that are nearly orthogonal.
â€¢
For solving the least square problem with ï¬‚oating-point arithmetic,
the modiï¬ed procedure is a numerically stable algorithm in the sense
that the method described in Example 5.5.3 returns a result that is
the exact solution of a nearby least squares problem. However, the
Householder method described on p. 346 is just as stable and needs
slightly fewer arithmetic operations.
Exercises for section 5.5
5.5.1. Let S = span
ï£±
ï£´
ï£²
ï£´
ï£³
x1 =
ï£«
ï£¬
ï£­
1
1
1
âˆ’1
ï£¶
ï£·
ï£¸,
x2 =
ï£«
ï£¬
ï£­
2
âˆ’1
âˆ’1
1
ï£¶
ï£·
ï£¸,
x3 =
ï£«
ï£¬
ï£­
âˆ’1
2
2
1
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
.
(a)
Use the classical Gramâ€“Schmidt algorithm (with exact arith-
metic) to determine an orthonormal basis for S.
(b)
Verify directly that the Gramâ€“Schmidt sequence produced in
part (a) is indeed an orthonormal basis for S.
(c)
Repeat part (a) using the modiï¬ed Gramâ€“Schmidt algorithm,
and compare the results.

318
Chapter 5
Norms, Inner Products, and Orthogonality
5.5.2. Use the Gramâ€“Schmidt procedure to ï¬nd an orthonormal basis for the
four fundamental subspaces of A =
 1
âˆ’2
3
âˆ’1
2
âˆ’4
6
âˆ’2
3
âˆ’6
9
âˆ’3

.
5.5.3. Apply the Gramâ€“Schmidt procedure with the standard inner product
for C3 to
 i
i
i

,
 0
i
i

,
 0
0
i
!
.
5.5.4. Explain what happens when the Gramâ€“Schmidt process is applied to an
orthonormal set of vectors.
5.5.5. Explain what happens when the Gramâ€“Schmidt process is applied to a
linearly dependent set of vectors.
5.5.6. Let A =
ï£«
ï£­
1
0
âˆ’1
1
2
1
1
1
âˆ’3
0
1
1
ï£¶
ï£¸and b =
ï£«
ï£­
1
1
1
1
ï£¶
ï£¸.
(a)
Determine the rectangular QR factorization of A.
(b)
Use the QR factors from part (a) to determine the least squares
solution to Ax = b.
5.5.7. Given a linearly independent set of vectors S = {x1, x2, . . . , xn} in an
inner-product space, let Sk = span {x1, x2, . . . , xk} for k = 1, 2, . . . , n.
Give an induction argument to prove that if Ok = {u1, u2, . . . , uk} is
the Gramâ€“Schmidt sequence deï¬ned in (5.5.2), then Ok is indeed an or-
thonormal basis for Sk = span {x1, x2, . . . , xk} for each k = 1, 2, . . . , n.
5.5.8. Prove that if rank (AmÃ—n) = n, then the rectangular QR factorization
of A is unique. That is, if A = QR, where QmÃ—n has orthonormal
columns and RnÃ—n is upper triangular with positive diagonal entries,
then Q and R are unique. Hint: Recall Example 3.10.7, p. 154.
5.5.9.
(a)
Apply classical Gramâ€“Schmidt with 3-digit ï¬‚oating-point arith-
metic to

x1 =

1
0
10âˆ’3

, x2 =
 1
0
0

, x3 =

1
10âˆ’3
0
!
. You may
assume that fl
âˆš
2

= 1.41.
(b)
Again using 3-digit ï¬‚oating-point arithmetic, apply the modiï¬ed
Gramâ€“Schmidt algorithm to {x1, x2, x3} , and compare the re-
sult with that of part (a).

5.5 Gramâ€“Schmidt Procedure
319
5.5.10. Depending on how the inner products rij are deï¬ned, verify that the fol-
lowing code implements both the classical and modiï¬ed Gramâ€“Schmidt
algorithms applied to a set of vectors {x1, x2, . . . , xn} .
For j = 1 to n
uj â†âˆ’xj
For i = 1 to j âˆ’1
rij â†âˆ’
 âŸ¨ui xjâŸ©
(classical Gramâ€“Schmidt)
âŸ¨ui ujâŸ©
(modiï¬ed Gramâ€“Schmidt)
uj â†âˆ’uj âˆ’rijui
End
rjj â†âˆ’âˆ¥ujâˆ¥
If rjj = 0
quit
(because xj âˆˆspan {x1, x2, . . . , xjâˆ’1} )
Else uj â†âˆ’uj/rjj
End
If exact arithmetic is used, will the inner products rij be the same for
both implementations?
5.5.11. Let V be the inner-product space of real-valued continuous functions
deï¬ned on the interval [âˆ’1, 1], where the inner product is deï¬ned by
âŸ¨f gâŸ©=
 1
âˆ’1
f(x)g(x)dx,
and let S be the subspace of V that is spanned by the three linearly
independent polynomials q0 = 1,
q1 = x,
q2 = x2.
(a)
Use the Gramâ€“Schmidt process to determine an orthonormal set
of polynomials {p0, p1, p2} that spans S. These polynomials
are the ï¬rst three normalized Legendre
45 polynomials.
(b)
Verify that pn satisï¬es Legendreâ€™s diï¬€erential equation
(1 âˆ’x2)yâ€²â€² âˆ’2xyâ€² + n(n + 1)y = 0
for n = 0, 1, 2. This equation and its solutions are of consider-
able importance in applied mathematics.
45
Adrienâ€“Marie Legendre (1752â€“1833) was one of the most eminent French mathematicians of
the eighteenth century. His primary work in higher mathematics concerned number theory
and the study of elliptic functions. But he was also instrumental in the development of the
theory of least squares, and some people believe that Legendre should receive the credit that
is often aï¬€orded to Gauss for the introduction of the method of least squares. Like Gauss and
many other successful mathematicians, Legendre spent substantial time engaged in diligent
and painstaking computation. It is reported that in 1824 Legendre refused to vote for the
governmentâ€™s candidate for Institut National, so his pension was stopped, and he died in
poverty.

320
Chapter 5
Norms, Inner Products, and Orthogonality
5.6
UNITARY AND ORTHOGONAL MATRICES
The purpose of this section is to examine square matrices whose columns (or
rows) are orthonormal. The standard inner product and the euclidean 2-norm
are the only ones used in this section, so distinguishing subscripts are omitted.
Unitary and Orthogonal Matrices
â€¢
A unitary matrix is deï¬ned to be a complex matrix UnÃ—n whose
columns (or rows) constitute an orthonormal basis for Cn.
â€¢
An orthogonal matrix is deï¬ned to be a real matrix PnÃ—n whose
columns (or rows) constitute an orthonormal basis for â„œn.
Unitary and orthogonal matrices have some nice features, one of which is
the fact that they are easy to invert. To see why, notice that the columns of
UnÃ—n =

u1 | u2 | Â· Â· Â· |un

are an orthonormal set if and only if
[Uâˆ—U]ij = uâˆ—
i uj =

1
when i = j,
0
when i Ì¸= j, â‡â‡’Uâˆ—U = I â‡â‡’Uâˆ’1 = Uâˆ—.
Notice that because Uâˆ—U = I â‡â‡’UUâˆ—= I, the columns of U are orthonor-
mal if and only if the rows of U are orthonormal, and this is why the deï¬nitions
of unitary and orthogonal matrices can be stated either in terms of orthonormal
columns or orthonormal rows.
Another nice feature is that multiplication by a unitary matrix doesnâ€™t
change the length of a vector. Only the direction can be altered because
âˆ¥Uxâˆ¥2 = xâˆ—Uâˆ—Ux = xâˆ—x = âˆ¥xâˆ¥2
âˆ€x âˆˆCn.
(5.6.1)
Conversely, if (5.6.1) holds, then U must be unitary. To see this, set x = ei
in (5.6.1) to observe uâˆ—
i ui = 1 for each i, and then set x = ej + ek for j Ì¸= k
to obtain 0 = uâˆ—
juk + uâˆ—
kuj = 2 Re (uâˆ—
juk) . By setting x = ej + iek in (5.6.1)
it also follows that 0 = 2 Im (uâˆ—
juk) , so uâˆ—
juk = 0 for each j Ì¸= k, and thus
(5.6.1) guarantees that U is unitary.
In the case of orthogonal matrices, everything is real so that (â‹†)âˆ—can be
replaced by (â‹†)T . Below is a summary of these observations.

5.6 Unitary and Orthogonal Matrices
321
Characterizations
â€¢
The following statements are equivalent to saying that a complex
matrix UnÃ—n is unitary.
â–·
U has orthonormal columns.
â–·
U has orthonormal rows.
â–·
Uâˆ’1 = Uâˆ—.
â–·
âˆ¥Uxâˆ¥2 = âˆ¥xâˆ¥2 for every x âˆˆCnÃ—1.
â€¢
The following statements are equivalent to saying that a real matrix
PnÃ—n is orthogonal.
â–·
P has orthonormal columns.
â–·
P has orthonormal rows.
â–·
Pâˆ’1 = PT .
â–·
âˆ¥Pxâˆ¥2 = âˆ¥xâˆ¥2 for every x âˆˆâ„œnÃ—1.
Example 5.6.1
â€¢
The identity matrix I is an orthogonal matrix.
â€¢
All permutation matrices (products of elementary interchange matrices) are
orthogonalâ€”recall Exercise 3.9.4.
â€¢
The matrix
P =
ï£«
ï£­
1/
âˆš
2
1/
âˆš
3
âˆ’1/
âˆš
6
âˆ’1/
âˆš
2
1/
âˆš
3
âˆ’1/
âˆš
6
0
1/
âˆš
3
2/
âˆš
6
ï£¶
ï£¸
is an orthogonal matrix because PT P = PPT = I or, equivalently, because
the columns (and rows) constitute an orthonormal set.
â€¢
The matrix U =
1
2
 1 + i
âˆ’1 + i
1 + i
1 âˆ’i

is unitary because Uâˆ—U = UUâˆ—= I or,
equivalently, because the columns (and rows) are an orthonormal set.
â€¢
An orthogonal matrix can be considered to be unitary, but a unitary matrix
is generally not orthogonal.
In general, a linear operator T on a vector space V with the property that
âˆ¥Txâˆ¥= âˆ¥xâˆ¥for all x âˆˆV is called an isometry on V. The isometries on â„œn
are precisely the orthogonal matrices, and the isometries on Cn are the unitary
matrices. The term â€œisometryâ€ has an advantage in that it can be used to treat
the real and complex cases simultaneously, but for clarity we will often revert
back to the more cumbersome â€œorthogonalâ€ and â€œunitaryâ€ terminology.

322
Chapter 5
Norms, Inner Products, and Orthogonality
The geometrical concepts of projection, reï¬‚ection, and rotation are among
the most fundamental of all linear transformations in â„œ2 and â„œ3 (see Example
4.7.1 for three simple examples), so pursuing these ideas in higher dimensions
is only natural. The reï¬‚ector and rotator given in Example 4.7.1 are isometries
(because they preserve length), but the projector is not. We are about to see
that the same is true in more general settings.
Elementary Orthogonal Projectors
For a vector u âˆˆCnÃ—1 such that âˆ¥uâˆ¥= 1, a matrix of the form
Q = I âˆ’uuâˆ—
(5.6.2)
is called an elementary orthogonal projector. More general projec-
tors are discussed on pp. 386 and 429.
To understand the nature of elementary projectors consider the situation in
â„œ3. Suppose that âˆ¥u3Ã—1âˆ¥= 1, and let uâŠ¥denote the space (the plane through
the origin) consisting of all vectors that are perpendicular to u â€”we call uâŠ¥the
orthogonal complement of u (a more general deï¬nition appears on p. 403).
The matrix Q = Iâˆ’uuT is the orthogonal projector onto uâŠ¥in the sense that
Q maps each x âˆˆâ„œ3Ã—1 to its orthogonal projection in uâŠ¥as shown in Figure
5.6.1.
u âŠ¥
x
Qx = (I - uuT)x
u
(I - Q)x = uuTx
0
Figure 5.6.1
To see this, observe that each x can be resolved into two components
x = (I âˆ’Q)x + Qx,
where
(I âˆ’Q)x âŠ¥Qx.
The vector (I âˆ’Q)x = u(uT x) is on the line determined by u, and Qx is in
the plane uâŠ¥because uT Qx = 0.

5.6 Unitary and Orthogonal Matrices
323
The situation is exactly as depicted in Figure 5.6.1. Notice that (Iâˆ’Q)x =
uuT x is the orthogonal projection of x onto the line determined by u and
uuT x
 = |uT x|. This provides a nice interpretation of the magnitude of the
standard inner product. Below is a summary.
Geometry of Elementary Projectors
For vectors u, x âˆˆCnÃ—1 such that âˆ¥uâˆ¥= 1,
â€¢
(I âˆ’uuâˆ—)x is the orthogonal projection of x onto the orthogonal
complement uâŠ¥, the space of all vectors orthogonal to u;
(5.6.3)
â€¢
uuâˆ—x is the orthogonal projection of x onto the one-dimensional
space span {u} ;
(5.6.4)
â€¢
|uâˆ—x| represents the length of the orthogonal projection of x onto
the one-dimensional space span {u} .
(5.6.5)
In passing, note that elementary projectors are never isometriesâ€”they canâ€™t
be because they are not unitary matrices in the complex case and not orthogonal
matrices in the real case. Furthermore, isometries are nonsingular but elementary
projectors are singular.
Example 5.6.2
Problem: Determine the orthogonal projection of x onto span {u} , and then
ï¬nd the orthogonal projection of x onto uâŠ¥for x =
 2
0
1

and u =

2
âˆ’1
3

.
Solution: We cannot apply (5.6.3) and (5.6.4) directly because âˆ¥uâˆ¥Ì¸= 1, but
this is not a problem because

u
âˆ¥uâˆ¥
 = 1,
span {u} = span
 u
âˆ¥uâˆ¥
!
,
and
uâŠ¥=
 u
âˆ¥uâˆ¥
âŠ¥
.
Consequently, the orthogonal projection of x onto span {u} is given by
 u
âˆ¥uâˆ¥
  u
âˆ¥uâˆ¥
T
x = uuT
uT ux = 1
2
ï£«
ï£­
2
âˆ’1
3
ï£¶
ï£¸,
and the orthogonal projection of x onto uâŠ¥is

I âˆ’uuT
uT u

x = x âˆ’uuT x
uT u = 1
2
ï£«
ï£­
2
1
âˆ’1
ï£¶
ï£¸.

324
Chapter 5
Norms, Inner Products, and Orthogonality
There is nothing special about the numbers in this example. For every nonzero
vector u âˆˆCnÃ—1, the orthogonal projectors onto span {u} and uâŠ¥are
Pu = uuâˆ—
uâˆ—u
and
PuâŠ¥= I âˆ’uuâˆ—
uâˆ—u.
(5.6.6)
Elementary Reï¬‚ectors
For unÃ—1 Ì¸= 0, the elementary reï¬‚ector about uâŠ¥is deï¬ned to be
R = I âˆ’2uuâˆ—
uâˆ—u
(5.6.7)
or, equivalently,
R = I âˆ’2uuâˆ—
when
âˆ¥uâˆ¥= 1.
(5.6.8)
Elementary reï¬‚ectors are also called Householder transformations,
46 and
they are analogous to the simple reï¬‚ector given in Example 4.7.1. To understand
why, suppose u âˆˆâ„œ3Ã—1 and âˆ¥uâˆ¥= 1 so that Q = I âˆ’uuT is the orthogonal
projector onto the plane uâŠ¥. For each x âˆˆâ„œ3Ã—1, Qx is the orthogonal pro-
jection of x onto uâŠ¥as shown in Figure 5.6.1. To locate Rx = (I âˆ’2uuT )x,
notice that Q(Rx) = Qx. In other words, Qx is simultaneously the orthogo-
nal projection of x onto uâŠ¥as well as the orthogonal projection of Rx onto
uâŠ¥. This together with âˆ¥x âˆ’Qxâˆ¥= |uT x| = âˆ¥Qx âˆ’Rxâˆ¥implies that Rx
is the reï¬‚ection of x about the plane uâŠ¥, exactly as depicted in Figure 5.6.2.
(Reï¬‚ections about more general subspaces are examined in Exercise 5.13.21.)
x
Rx
Qx
|| x - Qx ||
|| Qx - Rx ||
0
u âŠ¥
u
Figure 5.6.2
46
Alston Scott Householder (1904â€“1993) was one of the ï¬rst people to appreciate and promote
the use of elementary reï¬‚ectors for numerical applications. Although his 1937 Ph.D. disserta-
tion at University of Chicago concerned the calculus of variations, Householderâ€™s passion was
mathematical biology, and this was the thrust of his career until it was derailed by the war
eï¬€ort in 1944. Householder joined the Mathematics Division of Oak Ridge National Labora-
tory in 1946 and became its director in 1948. He stayed at Oak Ridge for the remainder of his
career, and he became a leading ï¬gure in numerical analysis and matrix computations. Like
his counterpart J. Wallace Givens (p. 333) at the Argonne National Laboratory, Householder
was one of the early presidents of SIAM.

5.6 Unitary and Orthogonal Matrices
325
Properties of Elementary Reï¬‚ectors
â€¢
All elementary reï¬‚ectors R are unitary, hermitian, and involutory
( R2 = I ). That is,
R = Râˆ—= Râˆ’1.
(5.6.9)
â€¢
If xnÃ—1 is a vector whose ï¬rst entry is x1 Ì¸= 0, and if
u = x Â± Âµ âˆ¥xâˆ¥e1,
where
Âµ =

1
if x1 is real,
x1/|x1|
if x1 is not real,
(5.6.10)
is used to build the elementary reï¬‚ector R in (5.6.7), then
Rx = âˆ“Âµ âˆ¥xâˆ¥e1.
(5.6.11)
In other words, this R â€œreï¬‚ectsâ€ x onto the ï¬rst coordinate axis.
Computational Note: To avoid cancellation when using ï¬‚oating-
point arithmetic for real matrices, set u = x + sign(x1) âˆ¥xâˆ¥e1.
Proof of (5.6.9). It is clear that R = Râˆ—, and the fact that R = Râˆ’1 is
established simply by verifying that R2 = I.
Proof of (5.6.10). Observe that R = I âˆ’2Ë†uË†uâˆ—, where Ë†u = u/ âˆ¥uâˆ¥.
Proof of (5.6.11). Write Rx = x âˆ’2uuâˆ—x/uâˆ—u = x âˆ’(2uâˆ—x/uâˆ—u)u and verify
that 2uâˆ—x = uâˆ—u to conclude Rx = x âˆ’u = âˆ“Âµ âˆ¥xâˆ¥e1.
Example 5.6.3
Problem: Given x âˆˆCnÃ—1 such that âˆ¥xâˆ¥= 1, construct an orthonormal basis
for Cn that contains x.
Solution: An eï¬ƒcient solution is to build a unitary matrix that contains x as
its ï¬rst column. Set u = xÂ±Âµe1 in R = Iâˆ’2(uuâˆ—/uâˆ—u) and notice that (5.6.11)
guarantees Rx = âˆ“Âµe1, so multiplication on the left by R (remembering that
R2 = I) produces x = âˆ“ÂµRe1 = [âˆ“ÂµR]âˆ—1 . Since | âˆ“Âµ| = 1,
U = âˆ“ÂµR
is a unitary matrix with Uâˆ—1 = x, so the columns of U provide the desired
orthonormal basis. For example, to construct an orthonormal basis for â„œ4 that
includes x = (1/3) ( âˆ’1
2
0 âˆ’2 )T , set
u = x âˆ’e1 = 1
3
ï£«
ï£¬
ï£­
âˆ’4
2
0
âˆ’2
ï£¶
ï£·
ï£¸and compute R = I âˆ’2uuT
uT u = 1
3
ï£«
ï£¬
ï£­
âˆ’1
2
0
âˆ’2
2
2
0
1
0
0
3
0
âˆ’2
1
0
2
ï£¶
ï£·
ï£¸.
The columns of R do the job.

326
Chapter 5
Norms, Inner Products, and Orthogonality
Now consider rotation, and begin with a basic problem in â„œ2. If a nonzero
vector u = (u1, u2) is rotated counterclockwise through an angle Î¸ to produce
v = (v1, v2), how are the coordinates of v related to the coordinates of u? To
answer this question, refer to Figure 5.6.3, and use the fact that âˆ¥uâˆ¥= Î½ = âˆ¥vâˆ¥
(rotation is an isometry) together with some elementary trigonometry to obtain
v1 = Î½ cos(Ï† + Î¸) = Î½(cos Î¸ cos Ï† âˆ’sin Î¸ sin Ï†),
v2 = Î½ sin(Ï† + Î¸) = Î½(sin Î¸ cos Ï† + cos Î¸ sin Ï†).
(5.6.12)
u = ( u1 , u2 )
v = ( v1 , v2 )
Î¸
Ï†
Figure 5.6.3
Substituting cos Ï† = u1/Î½ and sin Ï† = u2/Î½ into (5.6.12) yields
v1 = (cos Î¸)u1 âˆ’(sin Î¸)u2,
v2 = (sin Î¸)u1 + (cos Î¸)u2,
or

v1
v2

=

cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸
 
u1
u2

. (5.6.13)
In other words, v = Pu, where P is the rotator (rotation operator)
P =

cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸

.
(5.6.14)
Notice that P is an orthogonal matrix because PT P = I. This means that if
v = Pu, then u = PT v, and hence PT is also a rotator, but in the opposite
direction of that associated with P. That is, PT is the rotator associated with
the angle âˆ’Î¸. This is conï¬rmed by the fact that if Î¸ is replaced by âˆ’Î¸ in
(5.6.14), then PT is produced.
Rotating vectors in â„œ3 around any one of the coordinate axes is similar.
For example, consider rotation around the z-axis. Suppose that v = (v1, v2, v3)
is obtained by rotating u = (u1, u2, u3) counterclockwise
47 through an angle
Î¸ around the z-axis. Just as before, the goal is to determine the relationship
between the coordinates of u and v. Since we are rotating around the z-axis,
47
This is from the perspective of looking down the z-axis onto the xy-plane.

5.6 Unitary and Orthogonal Matrices
327
it is evident (see Figure 5.6.4) that the third coordinates are unaï¬€ectedâ€”i.e.,
v3 = u3. To see how the xy-coordinates of u and v are related, consider the
orthogonal projections
up = (u1, u2, 0)
and
vp = (v1, v2, 0)
of u and v onto the xy-plane.
x
y
z
v = (v1, v2, v3)
vp = (v1, v2, 0)
u = (u1, u2, u3)
up = (u1, u2, 0)
Î¸
Î¸
Figure 5.6.4
Itâ€™s apparent from Figure 5.6.4 that the problem has been reduced to rotation
in the xy-plane, and we already know how to do this. Combining (5.6.13) with
the fact that v3 = u3 produces the equation
ï£«
ï£­
v1
v2
v3
ï£¶
ï£¸=
ï£«
ï£­
cos Î¸
âˆ’sin Î¸
0
sin Î¸
cos Î¸
0
0
0
1
ï£¶
ï£¸
ï£«
ï£­
u1
u2
u3
ï£¶
ï£¸,
so
Pz =
ï£«
ï£­
cos Î¸
âˆ’sin Î¸
0
sin Î¸
cos Î¸
0
0
0
1
ï£¶
ï£¸
is the matrix that rotates vectors in â„œ3 counterclockwise around the z-axis
through an angle Î¸. It is easy to verify that Pz is an orthogonal matrix and
that Pâˆ’1
z
= PT
z rotates vectors clockwise around the z-axis.
By using similar techniques, it is possible to derive orthogonal matrices that
rotate vectors around the x-axis or around the y-axis. Below is a summary of
these rotations in â„œ3.

328
Chapter 5
Norms, Inner Products, and Orthogonality
Rotations in R3
A vector u âˆˆâ„œ3 can be rotated counterclockwise through an angle Î¸
around a coordinate axis by means of a multiplication Pâ‹†u in which
Pâ‹†is an appropriate orthogonal matrix as described below.
Rotation around the x-Axis
Px =
ï£«
ï£­
1
0
0
0
cos Î¸
âˆ’sin Î¸
0
sin Î¸
cos Î¸
ï£¶
ï£¸
x
y
z
Î¸
Rotation around the y-Axis
Py =
ï£«
ï£­
cos Î¸
0
sin Î¸
0
1
0
âˆ’sin Î¸
0
cos Î¸
ï£¶
ï£¸
Î¸
x
y
z
Rotation around the z-Axis
Pz =
ï£«
ï£­
cos Î¸
âˆ’sin Î¸
0
sin Î¸
cos Î¸
0
0
0
1
ï£¶
ï£¸
x
y
z
Î¸
Note: The minus sign appears above the diagonal in Px and Pz, but
below the diagonal in Py. This is not a mistakeâ€”itâ€™s due to the orien-
tation of the positive x-axis with respect to the yz-plane.
Example 5.6.4
3-D Rotational Coordinates. Suppose that three counterclockwise rotations
are performed on the three-dimensional solid shown in Figure 5.6.5. First rotate
the solid in View (a) 90â—¦around the x-axis to obtain the orientation shown
in View (b). Then rotate View (b) 45â—¦around the y-axis to produce View (c)
and, ï¬nally, rotate View (c) 60â—¦around the z-axis to end up with View (d).
You can follow the process by watching how the notch, the vertex v, and the
lighter shaded face move.

5.6 Unitary and Orthogonal Matrices
329
x
y
z
View (a)
Ï€/2
v
x
y
z
View (b)
Ï€/4
v
View (c)
y
z
x
Ï€/3
v
x
y
z
View (d)
v
Figure 5.6.5
Problem: If the coordinates of each vertex in View (a) are speciï¬ed, what are
the coordinates of each vertex in View (d)?
Solution: If Px is the rotator that maps points in View (a) to corresponding
points in View (b), and if Py and Pz are the respective rotators carrying View
(b) to View (c) and View (c) to View (d), then
Px =
ï£«
ï£­
1
0
0
0
0
âˆ’1
0
1
0
ï£¶
ï£¸, Py =
1
âˆš
2
ï£«
ï£­
1
0
1
0
âˆš
2
0
âˆ’1
0
1
ï£¶
ï£¸, Pz =
ï£«
ï£­
1/2
âˆ’
âˆš
3/2
0
âˆš
3/2
1/2
0
0
0
1
ï£¶
ï£¸,
so
P = PzPyPx =
1
2
âˆš
2
ï£«
ï£­
1
1
âˆš
6
âˆš
3
âˆš
3
âˆ’
âˆš
2
âˆ’2
2
0
ï£¶
ï£¸
(5.6.15)
is the orthogonal matrix that maps points in View (a) to their corresponding
images in View (d). For example, focus on the vertex labeled v in View (a), and
let va, vb, vc, and vd denote its respective coordinates in Views (a), (b), (c),
and (d). If va = ( 1
1
0 )T , then vb = Pxva = ( 1
0
1 )T ,
vc = Pyvb = PyPxva=
ï£«
ï£­
âˆš
2
0
0
ï£¶
ï£¸,
and
vd = Pzvc = PzPyPxva=
ï£«
ï£­
âˆš
2/2
âˆš
6/2
0
ï£¶
ï£¸.

330
Chapter 5
Norms, Inner Products, and Orthogonality
More generally, if the coordinates of each of the ten vertices in View (a) are
placed as columns in a vertex matrix,
Va =
ï£«
ï£¬
ï£­
v1
â†“
v2
â†“
v10
â†“
x1
x2
Â· Â· Â·
x10
y1
y2
Â· Â· Â·
y10
z1
z2
Â· Â· Â·
z10
ï£¶
ï£·
ï£¸, then
Vd = PzPyPxVa =
ï£«
ï£¬
ï£­
Ë†v1
â†“
Ë†v2
â†“
Ë†v10
â†“
Ë†x1
Ë†x2
Â· Â· Â·
Ë†x10
Ë†y1
Ë†y2
Â· Â· Â·
Ë†y10
Ë†z1
Ë†z2
Â· Â· Â·
Ë†z10
ï£¶
ï£·
ï£¸
is the vertex matrix for the orientation shown in View (d). The polytope in
View (d) is drawn by identifying pairs of vertices (vi, vj) in Va that have an
edge between them, and by drawing an edge between the corresponding vertices
(Ë†vi, Ë†vj) in Vd.
Example 5.6.5
3-D Computer Graphics. Consider the problem of displaying and manipu-
lating views of a three-dimensional solid on a two-dimensional computer display
monitor. One simple technique is to use a wire-frame representation of the solid
consisting of a mesh of points (vertices) on the solidâ€™s surface connected by
straight line segments (edges). Once these vertices and edges have been deï¬ned,
the resulting polytope can be oriented in any desired manner as described in
Example 5.6.4, so all that remains are the following problems.
Problem: How should the vertices and edges of a three-dimensional polytope
be plotted on a two-dimensional computer monitor?
Solution: Assume that the screen represents the yz-plane, and suppose the
x-axis is orthogonal to the screen so that it points toward the viewerâ€™s eye as
shown in Figure 5.6.6.
z
y
x
Figure 5.6.6
A solid in the xyz-coordinate system appears to the viewer as the orthogonal
projection of the solid onto the yz-plane, and the projection of a polytope is
easy to draw. Just set the x-coordinate of each vertex to 0 (i.e., ignore the
x-coordinates), plot the (y, z)-coordinates on the yz-plane (the screen), and

5.6 Unitary and Orthogonal Matrices
331
draw edges between appropriate vertices. For example, suppose that the vertices
of the polytope in Figure 5.6.5 are numbered as indicated below in Figure 5.6.7,
x
y
z
2
3
4
5
6
10
7
8
9
1
Figure 5.6.7
and suppose that the associated vertex matrix is
V =
ï£«
ï£­
v1
v2
v3
v4
v5
v6
v7
v8
v9
v10
x
0
1
1
0
0
1
1
1
.8
0
y
0
0
1
1
0
0
.8
1
1
1
z
0
0
0
0
1
1
1
.8
1
1
ï£¶
ï£¸.
There are 15 edges, and they can be recorded in an edge matrix
E =
 e1
e2
e3
e4
e5
e6
e7
e8
e9
e10
e11
e12
e13
e14
e15
1
2
3
4
1
2
3
4
5
6
7
7
8
9
10
2
3
4
1
5
6
8
10
6
7
8
9
9
10
5

in which the kth column represents an edge between the indicated pair of ver-
tices. To display the image of the polytope in Figure 5.6.7 on a monitor, (i) drop
the ï¬rst row from V, (ii) plot the remaining yz-coordinates on the screen, (iii)
draw edges between appropriate vertices as dictated by the information in the
edge matrix E. To display the image of the polytope after it has been rotated
counterclockwise around the x-, y-, and z-axes by 90â—¦,
45â—¦, and 60â—¦, re-
spectively, use the orthogonal matrix P = PzPyPx determined in (5.6.15) and
compute the product
PV =
ï£«
ï£­
0
.354
.707
.354
.866
1.22
1.5
1.4
1.5
1.22
0
.612
1.22
.612
âˆ’.5
.112
.602
.825
.602
.112
0
âˆ’.707
0
.707
0
âˆ’.707
âˆ’.141
0
.141
.707
ï£¶
ï£¸.
Now proceed as beforeâ€”(i) ignore the ï¬rst row of PV, (ii) plot the points in
the second and third row of PV as yz-coordinates on the monitor, (iii) draw
edges between appropriate vertices as indicated by the edge matrix E.

332
Chapter 5
Norms, Inner Products, and Orthogonality
Problem: In addition to rotation, how can a polytope (or its image on a com-
puter monitor) be translated?
Solution: Translation of a polytope to a diï¬€erent point in space is accom-
plished by adding a constant to each of its coordinates. For example, to trans-
late the polytope shown in Figure 5.6.7 to the location where vertex 1 is at
pT = (x0, y0, z0) instead of at the origin, just add p to every point. In partic-
ular, if e is the column of 1â€™s, the translated vertex matrix is
Vtrans = Vorig +
ï£«
ï£­
x0
x0
Â· Â· Â·
x0
y0
y0
Â· Â· Â·
y0
z0
z0
Â· Â· Â·
z0
ï£¶
ï£¸= Vorig + peT
(a rank-1 update).
Of course, the edge matrix is not aï¬€ected by translation.
Problem: How can a polytope (or its image on a computer monitor) be scaled?
Solution: Simply multiply every coordinate by the desired scaling factor. For
example, to scale an image by a factor Î±, form the scaled vertex matrix
Vscaled = Î±Vorig,
and then connect the scaled vertices with appropriate edges as dictated by the
edge matrix E.
Problem: How can the faces of a polytope that are hidden from the viewerâ€™s
perspective be detected so that they can be omitted from the drawing on the
screen?
Solution: A complete discussion of this tricky problem would carry us too far
astray, but one clever solution relying on the cross product of vectors in â„œ3 is
presented in Exercise 5.6.21 for the case of convex polytopes.
Rotations in higher dimensions are straightforward generalizations of rota-
tions in â„œ3. Recall from p. 328 that rotation around any particular axis in â„œ3
amounts to rotation in the complementary plane, and the associated 3 Ã— 3 ro-
tator is constructed by embedding a 2 Ã— 2 rotator in the appropriate position
in a 3 Ã— 3 identity matrix. For example, rotation around the y-axis is rotation
in the xz-plane, and the corresponding rotator is produced by embedding

cos Î¸
sin Î¸
âˆ’sin Î¸
cos Î¸

in the â€œ xz-positionâ€ of I3Ã—3 to form
Py =
ï£«
ï£­
cos Î¸
0
sin Î¸
0
1
0
âˆ’sin Î¸
0
cos Î¸
ï£¶
ï£¸.
These observations directly extend to higher dimensions.

5.6 Unitary and Orthogonal Matrices
333
Plane Rotations
Orthogonal matrices of the form
col i
â†“
col j
â†“
Pij =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
...
c
s
1
...
âˆ’s
c
1
...
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
â†âˆ’row i
â†âˆ’row j
in which c2 +s2 = 1 are called plane rotation matrices because they
perform a rotation in the (i, j)-plane of â„œn. The entries c and s are
meant to suggest cosine and sine, respectively, but designating a rotation
angle Î¸ as is done in â„œ2 and â„œ3 is not useful in higher dimensions.
Plane rotations matrices Pij are also called Givens
48 rotations. Applying
Pij to 0 Ì¸= x âˆˆâ„œn rotates the (i, j)-coordinates of x in the sense that
Pijx =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
x1
...
cxi + sxj
...
âˆ’sxi + cxj
...
xn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
â†âˆ’i
â†âˆ’j
.
If xi and xj are not both zero, and if we set
c =
xi

x2
i + x2
j
and
s =
xj

x2
i + x2
j
,
(5.6.16)
48
J. Wallace Givens, Jr. (1910â€“1993) pioneered the use of plane rotations in the early days
of automatic matrix computations. Givens graduated from Lynchburg College in 1928, and
he completed his Ph.D. at Princeton University in 1936. After spending three years at the
Institute for Advanced Study in Princeton as an assistant of O. Veblen, Givens accepted an
appointment at Cornell University but later moved to Northwestern University. In addition to
his academic career, Givens was the Director of the Applied Mathematics Division at Argonne
National Laboratory and, like his counterpart A. S. Householder (p. 324) at Oak Ridge National
Laboratory, Givens served as an early president of SIAM.

334
Chapter 5
Norms, Inner Products, and Orthogonality
then
Pijx =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
x1
...

x2
i + x2
j
...
0
...
xn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
â†âˆ’i
â†âˆ’j
.
This means that we can selectively annihilate any componentâ€”the jth in this
caseâ€”by a rotation in the (i, j)-plane without aï¬€ecting any entry except xi and
xj. Consequently, plane rotations can be applied to annihilate all components
below any particular â€œpivot.â€ For example, to annihilate all entries below the
ï¬rst position in x, apply a sequence of plane rotations as follows:
P12x=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆš
x2
1+x2
2
0
x3
x4
...
xn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
, P13P12x=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆš
x2
1+x2
2+x2
3
0
0
x4
...
xn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
, . . . , P1nÂ· Â· Â·P13P12x=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ¥xâˆ¥
0
0
0
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
The product of plane rotations is generally not another plane rotation, but
such a product is always an orthogonal matrix, and hence it is an isometry. If
we are willing to interpret â€œrotation in â„œn â€ as a sequence of plane rotations,
then we can say that it is always possible to â€œrotateâ€ each nonzero vector onto
the ï¬rst coordinate axis. Recall from (5.6.11) that we can also do this with a
reï¬‚ection. More generally, the following statement is true.
Rotations in â„œn
Every nonzero vector x âˆˆâ„œn can be rotated to the ith coordinate
axis by a sequence of n âˆ’1 plane rotations. In other words, there is an
orthogonal matrix P such that
Px = âˆ¥xâˆ¥ei,
(5.6.17)
where P has the form
P = Pin Â· Â· Â· Pi,i+1Pi,iâˆ’1 Â· Â· Â· Pi1.

5.6 Unitary and Orthogonal Matrices
335
Example 5.6.6
Problem: If x âˆˆâ„œn is a vector such that âˆ¥xâˆ¥= 1, explain how to use plane
rotations to construct an orthonormal basis for â„œn that contains x.
Solution: This is almost the same problem as that posed in Example 5.6.3, and,
as explained there, the goal is to construct an orthogonal matrix Q such that
Qâˆ—1 = x. But this time we need to use plane rotations rather than an elementary
reï¬‚ector. Equation (5.6.17) asserts that we can build an orthogonal matrix from
a sequence of plane rotations P = P1n Â· Â· Â· P13P12 such that Px = e1. Thus
x = PT e1 = PT
âˆ—1, and hence the columns of Q = PT serve the purpose. For
example, to extend
x = 1
3
ï£«
ï£¬
ï£­
âˆ’1
2
0
âˆ’2
ï£¶
ï£·
ï£¸
to an orthonormal basis for â„œ4, sequentially annihilate the second and fourth
components of x by using (5.6.16) to construct the following plane rotations:
P12x =
ï£«
ï£¬
ï£­
âˆ’1/
âˆš
5
2/
âˆš
5
0
0
âˆ’2/
âˆš
5
âˆ’1/
âˆš
5
0
0
0
0
1
0
0
0
0
1
ï£¶
ï£·
ï£¸1
3
ï£«
ï£¬
ï£­
âˆ’1
2
0
âˆ’2
ï£¶
ï£·
ï£¸= 1
3
ï£«
ï£¬
ï£­
âˆš
5
0
0
âˆ’2
ï£¶
ï£·
ï£¸,
P14

P12x

=
ï£«
ï£¬
ï£­
âˆš
5/3
0
0
âˆ’2/3
0
1
0
0
0
0
1
0
2/3
0
0
âˆš
5/3
ï£¶
ï£·
ï£¸1
3
ï£«
ï£¬
ï£­
âˆš
5
0
0
âˆ’2
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
1
0
0
0
ï£¶
ï£·
ï£¸.
Therefore, the columns of
Q = (P14P12)T = PT
12PT
14 =
ï£«
ï£¬
ï£­
âˆ’1/3
âˆ’2/
âˆš
5
0
âˆ’2/3
âˆš
5
2/3
âˆ’1/
âˆš
5
0
4/3
âˆš
5
0
0
1
0
âˆ’2/3
0
0
âˆš
5/3
ï£¶
ï£·
ï£¸
are an orthonormal set containing the speciï¬ed vector x.
Exercises for section 5.6
5.6.1. Determine which of the following matrices are isometries.
(a)
ï£«
ï£­
1/
âˆš
2
âˆ’1/
âˆš
2
0
1/
âˆš
6
1/
âˆš
6
âˆ’2/
âˆš
6
1/
âˆš
3
1/
âˆš
3
1/
âˆš
3
ï£¶
ï£¸.
(b)
ï£«
ï£­
1
0
1
1
0
âˆ’1
0
1
0
ï£¶
ï£¸.
(c)
ï£«
ï£¬
ï£­
0
0
1
0
1
0
0
0
0
0
0
1
0
1
0
0
ï£¶
ï£·
ï£¸.
(d)
ï£«
ï£¬
ï£¬
ï£­
eiÎ¸1
0
Â· Â· Â·
0
0
eiÎ¸2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
eiÎ¸n
ï£¶
ï£·
ï£·
ï£¸.

336
Chapter 5
Norms, Inner Products, and Orthogonality
5.6.2. Is
ï£«
ï£¬
ï£¬
ï£­
1 + i
âˆš
3
1 + i
âˆš
6
i
âˆš
3
âˆ’2 i
âˆš
6
ï£¶
ï£·
ï£·
ï£¸a unitary matrix?
5.6.3.
(a)
How many 3 Ã— 3 matrices are both diagonal and orthogonal?
(b)
How many n Ã— n matrices are both diagonal and orthogonal?
(c)
How many n Ã— n matrices are both diagonal and unitary?
5.6.4.
(a)
Under what conditions on the real numbers Î± and Î² will
P =

Î± + Î²
Î² âˆ’Î±
Î± âˆ’Î²
Î² + Î±

be an orthogonal matrix?
(b)
Under what conditions on the real numbers Î± and Î² will
U =
ï£«
ï£¬
ï£­
0
Î±
0
iÎ²
Î±
0
iÎ²
0
0
iÎ²
0
Î±
iÎ²
0
Î±
0
ï£¶
ï£·
ï£¸
be a unitary matrix?
5.6.5. Let U and V be two n Ã— n unitary (orthogonal) matrices.
(a)
Explain why the product UV must be unitary (orthogonal).
(b)
Explain why the sum U+V need not be unitary (orthogonal).
(c)
Explain why
 UnÃ—n
0
0
VmÃ—m

must be unitary (orthogonal).
5.6.6. Cayley Transformation. Prove, as Cayley did in 1846, that if A is
skew hermitian (or real skew symmetric), then
U = (I âˆ’A)(I + A)âˆ’1 = (I + A)âˆ’1(I âˆ’A)
is unitary (orthogonal) by ï¬rst showing that (I + A)âˆ’1 exists for skew-
hermitian matrices, and (I âˆ’A)(I + A)âˆ’1 = (I + A)âˆ’1(I âˆ’A) (recall
Exercise 3.7.6). Note: There is a more direct approach, but it requires
the diagonalization theorem for normal matricesâ€”see Exercise 7.5.5.
5.6.7. Suppose that R and S are elementary reï¬‚ectors.
(a)
Is
 I
0
0
R

an elementary reï¬‚ector?
(b)
Is
 R
0
0
S

an elementary reï¬‚ector?

5.6 Unitary and Orthogonal Matrices
337
5.6.8.
(a)
Explain why the standard inner product is invariant under a uni-
tary transformation. That is, if U is any unitary matrix, and if
u = Ux and v = Uy, then
uâˆ—v = xâˆ—y.
(b)
Given any two vectors x, y âˆˆâ„œn, explain why the angle between
them is invariant under an orthogonal transformation. That is, if
u = Px and v = Py, where P is an orthogonal matrix, then
cos Î¸u,v = cos Î¸x,y.
5.6.9. Let UmÃ—r be a matrix with orthonormal columns, and let VkÃ—n be a
matrix with orthonormal rows. For an arbitrary A âˆˆCrÃ—k, solve the
following problems using the matrix 2-norm (p. 281) and the Frobenius
matrix norm (p. 279).
(a)
Determine the values of âˆ¥Uâˆ¥2 , âˆ¥Vâˆ¥2 , âˆ¥Uâˆ¥F , and âˆ¥Vâˆ¥F .
(b)
Show that âˆ¥UAVâˆ¥2 = âˆ¥Aâˆ¥2 . (Hint: Start with âˆ¥UAâˆ¥2 . )
(c)
Show that âˆ¥UAVâˆ¥F = âˆ¥Aâˆ¥F .
Note: In particular, these properties are valid when U and V are
unitary matrices. Because of parts (b) and (c), the 2-norm and the F -
norm are said to be unitarily invariant norms.
5.6.10. Let u =
ï£«
ï£­
âˆ’2
1
3
âˆ’1
ï£¶
ï£¸and v =
ï£«
ï£­
1
4
0
âˆ’1
ï£¶
ï£¸.
(a)
Determine the orthogonal projection of u onto span {v} .
(b)
Determine the orthogonal projection of v onto span {u} .
(c)
Determine the orthogonal projection of u onto vâŠ¥.
(d)
Determine the orthogonal projection of v onto uâŠ¥.
5.6.11. Consider elementary orthogonal projectors Q = I âˆ’uuâˆ—.
(a)
Prove that Q is singular.
(b)
Now prove that if Q is n Ã— n, then rank (Q) = n âˆ’1.
Hint: Recall Exercise 4.4.10.
5.6.12. For vectors u, x âˆˆCn such that âˆ¥uâˆ¥= 1, let p be the orthogonal
projection of x onto span {u} . Explain why âˆ¥pâˆ¥â‰¤âˆ¥xâˆ¥with equality
holding if and only if x is a scalar multiple of u.

338
Chapter 5
Norms, Inner Products, and Orthogonality
5.6.13. Let x = (1/3)

1
âˆ’2
âˆ’2

.
(a)
Determine an elementary reï¬‚ector R such that Rx lies on the
x-axis.
(b)
Verify by direct computation that your reï¬‚ector R is symmet-
ric, orthogonal, and involutory.
(c)
Extend x to an orthonormal basis for â„œ3 by using an elemen-
tary reï¬‚ector.
5.6.14. Let R = I âˆ’2uuâˆ—, where âˆ¥unÃ—1âˆ¥= 1. If x is a ï¬xed point for R in
the sense that Rx = x, and if n > 1, prove that x must be orthogonal
to u, and then sketch a picture of this situation in â„œ3.
5.6.15. Let x, y âˆˆâ„œnÃ—1 be vectors such that âˆ¥xâˆ¥= âˆ¥yâˆ¥but x Ì¸= y. Explain
how to construct an elementary reï¬‚ector R such that Rx = y.
Hint: The vector u that deï¬nes R can be determined visually in â„œ3
by considering Figure 5.6.2.
5.6.16. Let xnÃ—1 be a vector such that âˆ¥xâˆ¥= 1, and partition x as
x =

x1
Ëœx

,
where Ëœx is n âˆ’1 Ã— 1.
(a)
If the entries of x are real, and if x1 Ì¸= 1, show that
P =

x1
ËœxT
Ëœx
I âˆ’Î±ËœxËœxT

,
where
Î± =
1
1 âˆ’x1
is an orthogonal matrix.
(b)
Suppose that the entries of x are complex. If |x1| Ì¸= 1, and if
Âµ is the number deï¬ned in (5.6.10), show that the matrix
U =

x1
Âµ2Ëœxâˆ—
Ëœx
Âµ(I âˆ’Î±ËœxËœxâˆ—)

,
where
Î± =
1
1 âˆ’|x1|
is unitary. Note: These results provide an easy way to extend
a given vector to an orthonormal basis for the entire space â„œn
or Cn.

5.6 Unitary and Orthogonal Matrices
339
5.6.17. Perform the following sequence of rotations in â„œ3 beginning with
v0 =
ï£«
ï£­
1
1
âˆ’1
ï£¶
ï£¸.
1.
Rotate v0 counterclockwise 45â—¦around the x-axis to produce v1.
2.
Rotate v1 clockwise 90â—¦around the y-axis to produce v2.
3.
Rotate v2 counterclockwise 30â—¦around the z-axis to produce v3.
Determine the coordinates of v3 as well as an orthogonal matrix Q
such that Qv0 = v3.
5.6.18. Does it matter in what order rotations in â„œ3 are performed? For ex-
ample, suppose that a vector v âˆˆâ„œ3 is ï¬rst rotated counterclockwise
around the x-axis through an angle Î¸, and then that vector is rotated
counterclockwise around the y-axis through an angle Ï†. Is the result
the same as ï¬rst rotating v counterclockwise around the y-axis through
an angle Ï† followed by a rotation counterclockwise around the x-axis
through an angle Î¸?
5.6.19. For each nonzero vector u âˆˆCn, prove that dim uâŠ¥= n âˆ’1.
5.6.20. A matrix satisfying A2 = I is said to be an involution or an involu-
tory matrix, and a matrix P satisfying P2 = P is called a projector
or is said to be an idempotent matrixâ€”properties of such matrices
are developed on p. 386. Show that there is a one-to-one correspondence
between the set of involutions and the set of projectors in CnÃ—n. Hint:
Consider the relationship between the projectors in (5.6.6) and the re-
ï¬‚ectors (which are involutions) in (5.6.7) on p. 324.
5.6.21. When using a computer to generate and display a three-dimensional
convex polytope such as the one in Example 5.6.4, it is desirable to not
draw those faces that should be hidden from the perspective of a viewer
positioned as shown in Figure 5.6.6. The operation of cross product in
â„œ3 (usually introduced in elementary calculus courses) can be used to
decide which faces are visible and which are not. Recall that if
u =
ï£«
ï£­
u1
u2
u3
ï£¶
ï£¸and v =
ï£«
ï£­
v1
v2
v3
ï£¶
ï£¸,
then
u Ã— v =
ï£«
ï£­
u2v3 âˆ’u3v2
u3v1 âˆ’u1v3
u1v2 âˆ’u2v1
ï£¶
ï£¸,

340
Chapter 5
Norms, Inner Products, and Orthogonality
and u Ã— v is a vector orthogonal to both u and v. The direction of
u Ã— v is determined from the so-called right-hand rule as illustrated in
Figure 5.6.8.
Figure 5.6.8
Assume the origin is interior to the polytope, and consider a particular
face and three vertices p0, p1, and p2 on the face that are positioned
as shown in Figure 5.6.9. The vector n = (p1 âˆ’p0) Ã— (p2 âˆ’p1) is
orthogonal to the face, and it points in the outward direction.
Figure 5.6.9
Explain why the outside of the face is visible from the perspective indi-
cated in Figure 5.6.6 if and only if the ï¬rst component of the outward
normal vector n is positive. In other words, the face is drawn if and
only if n1 > 0.

5.7 Orthogonal Reduction
341
5.7
ORTHOGONAL REDUCTION
We know that a matrix A can be reduced to row echelon form by elementary row
operations. This is Gaussian elimination, and, as explained on p. 143, the basic
â€œGaussian transformationâ€ is an elementary lower triangular matrix Tk whose
action annihilates all entries below the kth pivot at the kth elimination step.
But Gaussian elimination is not the only way to reduce a matrix. Elementary
reï¬‚ectors Rk can be used in place of elementary lower triangular matrices Tk
to annihilate all entries below the kth pivot at the kth elimination step, or a
sequence of plane rotation matrices can accomplish the same purpose.
When reï¬‚ectors are used, the process is usually called Householder re-
duction, and it proceeds as follows. For AmÃ—n = [Aâˆ—1 | Aâˆ—2 | Â· Â· Â· | Aâˆ—n] , use
x = Aâˆ—1 in (5.6.10) to construct the elementary reï¬‚ector
R1 = I âˆ’2uuâˆ—
uâˆ—u,
where
u = Aâˆ—1 Â± Âµ âˆ¥Aâˆ—1âˆ¥e1,
(5.7.1)
so that
R1Aâˆ—1 = âˆ“Âµ âˆ¥Aâˆ—1âˆ¥e1 =
ï£«
ï£¬
ï£¬
ï£­
t11
0
...
0
ï£¶
ï£·
ï£·
ï£¸.
(5.7.2)
Applying R1 to A yields
R1A=[R1Aâˆ—1 | R1Aâˆ—2 | Â· Â· Â· | R1Aâˆ—n]=
ï£«
ï£¬
ï£¬
ï£¬
ï£­
t11
t12
Â· Â· Â·
t1n
0
âˆ—
Â· Â· Â·
âˆ—
...
...
...
0
âˆ—
Â· Â· Â·
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£¸=

t11
tT
1
0
A2

,
where A2 is m âˆ’1 Ã— n âˆ’1. Thus all entries below the (1, 1)-position are an-
nihilated. Now apply the same procedure to A2 to construct an elementary
reï¬‚ector Ë†R2 that annihilates all entries below the (1, 1)-position in A2. If we
set R2 =
 1
0
0
Ë†R2

, then R2R1 is an orthogonal matrix (Exercise 5.6.5) such
that
R2R1A =

t11
tT
1
0
Ë†R2A2

=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
t11
t12
t13
Â· Â· Â·
t1n
0
t22
t23
Â· Â· Â·
t2n
0
0
âˆ—
Â· Â· Â·
âˆ—
...
...
...
...
0
0
âˆ—
Â· Â· Â·
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
The result after k âˆ’1 steps is Rkâˆ’1 Â· Â· Â· R2R1A =

Tkâˆ’1
ËœTkâˆ’1
0
Ak

. At step
k an elementary reï¬‚ector
Ë†Rk is constructed in a manner similar to (5.7.1)

342
Chapter 5
Norms, Inner Products, and Orthogonality
to annihilate all entries below the (1, 1)-position in Ak, and Rk is deï¬ned
as Rk =
 Ikâˆ’1
0
0
Ë†Rk

, which is another elementary reï¬‚ector (Exercise 5.6.7).
Eventually, all of the rows or all of the columns will be exhausted, so the ï¬nal
result is one of the two following upper-trapezoidal forms:
Rn Â· Â· Â· R2R1AmÃ—n =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
Â· Â· Â·
âˆ—
0
âˆ—
Â· Â· Â·
âˆ—
...
...
...
0
0
Â· Â· Â·
âˆ—
0
0
Â· Â· Â·
0
...
...
...
0
0
Â· Â· Â·
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£¼
ï£´
ï£´
ï£´
ï£½
ï£´
ï£´
ï£´
ï£¾
nÃ—n
when
m > n,
Rmâˆ’1 Â· Â· Â· R2R1AmÃ—n =
ï£«
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
Â· Â· Â·
âˆ—
âˆ—
Â· Â· Â·
âˆ—
0
âˆ—
Â· Â· Â·
âˆ—
âˆ—
Â· Â· Â·
âˆ—
...
...
...
...
...
0
0
Â· Â· Â·
âˆ—
âˆ—
Â· Â· Â·
âˆ—
ï£¶
ï£·
ï£·
ï£¸
when
m < n.
)
*+
,
mÃ—m
If m = n, then the ï¬nal form is an upper-triangular matrix.
A product of elementary reï¬‚ectors is not necessarily another elementary re-
ï¬‚ector, but a product of unitary (orthogonal) matrices is again unitary (orthogo-
nal) (Exercise 5.6.5). The elementary reï¬‚ectors Ri described above are unitary
(orthogonal in the real case) matrices, so every product RkRkâˆ’1 Â· Â· Â· R2R1 is a
unitary matrix, and thus we arrive at the following important conclusion.
Orthogonal Reduction
â€¢
For every A âˆˆCmÃ—n, there exists a unitary matrix P such that
PA = T
(5.7.3)
has an upper-trapezoidal form. When P is constructed as a prod-
uct of elementary reï¬‚ectors as described above, the process is called
Householder reduction.
â€¢
If A is square, then T is upper triangular, and if A is real, then
the P can be taken to be an orthogonal matrix.

5.7 Orthogonal Reduction
343
Example 5.7.1
Problem:
Use Householder reduction to ï¬nd an orthogonal matrix P such
that PA = T is upper triangular with positive diagonal entries, where
A =
ï£«
ï£­
0
âˆ’20
âˆ’14
3
27
âˆ’4
4
11
âˆ’2
ï£¶
ï£¸.
Solution: To annihilate the entries below the (1, 1)-position and to guarantee
that t11 is positive, equations (5.7.1) and (5.7.2) dictate that we set
u1 = Aâˆ—1 âˆ’âˆ¥Aâˆ—1âˆ¥e1 = Aâˆ—1 âˆ’5e1 =
ï£«
ï£­
âˆ’5
3
4
ï£¶
ï£¸
and
R1 = I âˆ’2u1uT
1
uT
1 u1
.
To compute a reï¬‚ector-by-matrix product RA = [RAâˆ—1 | RAâˆ—2 | Â· Â· Â· | RAâˆ—n] ,
itâ€™s wasted eï¬€ort to actually determine the entries in R = Iâˆ’2uuT /uT u. Simply
compute uT Aâˆ—j and then
RAâˆ—j = Aâˆ—j âˆ’2
uT Aâˆ—j
uT u

u
for each j = 1, 2, . . . , n.
(5.7.4)
By using this observation we obtain
R1A = [R1Aâˆ—1 | R1Aâˆ—2 | R1Aâˆ—3] =
ï£«
ï£­
5
25
âˆ’4
0
0
âˆ’10
0
âˆ’25
âˆ’10
ï£¶
ï£¸.
To annihilate the entry below the (2, 2)-position, set
A2 =

0
âˆ’10
âˆ’25
âˆ’10

and
u2 = [A2]âˆ—1 âˆ’
 [A2]âˆ—1
e1 = 25

âˆ’1
âˆ’1

.
If Ë†R2 = I âˆ’2u2uT
2 /uT
2 u2 and R2 =
 1
0
0
Ë†R2

(neither is explicitly computed),
then
Ë†R2A2 =

25
10
0
10

and
R2R1A = T =
ï£«
ï£­
5
25
âˆ’4
0
25
10
0
0
10
ï£¶
ï£¸.
If Ë†Rk = I âˆ’2Ë†uË†uT /Ë†uT Ë†u is an elementary reï¬‚ector, then so is
Rk =

I
0
0
Ë†Rk

= I âˆ’2uuT
uT u
with
u =

0
Ë†u

,
and consequently the product of any sequence of these Rk â€™s can be formed by
using the observation (5.7.4). In this example,
P = R2R1 = 1
25
ï£«
ï£­
0
15
20
âˆ’20
12
âˆ’9
âˆ’15
âˆ’16
12
ï£¶
ï£¸.
You may wish to check that P really is an orthogonal matrix and PA = T.

344
Chapter 5
Norms, Inner Products, and Orthogonality
Elementary reï¬‚ectors are not the only type of orthogonal matrices that can
be used to reduce a matrix to an upper-trapezoidal form. Plane rotation matrices
are also orthogonal, and, as explained on p. 334, plane rotation matrices can be
used to selectively annihilate any component in a given column, so a sequence of
plane rotations can be used to annihilate all elements below a particular pivot.
This means that a matrix A âˆˆâ„œmÃ—n can be reduced to an upper-trapezoidal
form strictly by using plane rotationsâ€”such a process is usually called a Givens
reduction.
Example 5.7.2
Problem: Use Givens reduction (i.e., use plane rotations) to reduce the matrix
A =
ï£«
ï£­
0
âˆ’20
âˆ’14
3
27
âˆ’4
4
11
âˆ’2
ï£¶
ï£¸
to upper-triangular form. Also compute an orthogonal matrix P such that
PA = T is upper triangular.
Solution: The plane rotation that uses the (1,1)-entry to annihilate the (2,1)-
entry is determined from (5.6.16) to be
P12 =
ï£«
ï£­
0
1
0
âˆ’1
0
0
0
0
1
ï£¶
ï£¸
so that
P12A =
ï£«
ï£­
3
27
âˆ’4
0
20
14
4
11
âˆ’2
ï£¶
ï£¸.
Now use the (1,1)-entry in P12A to annihilate the (3,1)-entry in P12A. The
plane rotation that does the job is again obtained from (5.6.16) to be
P13 = 1
5
ï£«
ï£­
3
0
4
0
5
0
âˆ’4
0
3
ï£¶
ï£¸
so that
P13P12A =
ï£«
ï£­
5
25
âˆ’4
0
20
14
0
âˆ’15
2
ï£¶
ï£¸.
Finally, using the (2,2)-entry in P13P12A to annihilate the (3,2)-entry produces
P23 = 1
5
ï£«
ï£­
5
0
0
0
4
âˆ’3
0
3
4
ï£¶
ï£¸
so that
P23P13P12A = T =
ï£«
ï£­
5
25
âˆ’4
0
25
10
0
0
10
ï£¶
ï£¸.
Since plane rotation matrices are orthogonal, and since the product of orthogonal
matrices is again orthogonal, it must be the case that
P = P23P13P12 = 1
25
ï£«
ï£­
0
15
20
âˆ’20
12
âˆ’9
âˆ’15
âˆ’16
12
ï£¶
ï£¸
is an orthogonal matrix such that PA = T.

5.7 Orthogonal Reduction
345
Householder and Givens reductions are closely related to the results pro-
duced by applying the Gramâ€“Schmidt process (p. 307) to the columns of A.
When A is nonsingular, Householder, Givens, and Gramâ€“Schmidt each pro-
duce an orthogonal matrix Q and an upper-triangular matrix R such that
A = QR (Q = PT in the case of orthogonal reduction). The upper-triangular
matrix R produced by the Gramâ€“Schmidt algorithm has positive diagonal en-
tries, and, as illustrated in Examples 5.7.1 and 5.7.2, we can also force this to be
true using the Householder or Givens reduction. This feature makes Q and R
unique.
QR Factorization
For each nonsingular A âˆˆâ„œnÃ—n, there is a unique orthogonal matrix Q
and a unique upper-triangular matrix R with positive diagonal entries
such that
A = QR.
This â€œsquareâ€ QR factorization is a special case of the more general
â€œrectangularâ€ QR factorization discussed on p. 311.
Proof.
Only uniqueness needs to be proven. If there are two QR factorizations
A = Q1R1 = Q2R2,
let U = QT
2 Q1 = R2Râˆ’1
1 . The matrix R2Râˆ’1
1
is upper triangular with positive
diagonal entries (Exercises 3.5.8 and 3.7.4) while QT
2 Q1 is an orthogonal matrix
(Exercise 5.6.5), and therefore U is an upper-triangular matrix whose columns
are an orthonormal set and whose diagonal entries are positive. Considering the
ï¬rst column of U we see that

ï£«
ï£¬
ï£¬
ï£­
u11
0
...
0
ï£¶
ï£·
ï£·
ï£¸

= 1
=â‡’
u11 = Â±1
and
u11 > 0
=â‡’
u11 = 1,
so that Uâˆ—1 = e1. A similar argument together with the fact that the columns
of U are mutually orthogonal produces
UT
âˆ—1Uâˆ—2 = 0
=â‡’
u12 = 0
=â‡’
u22 = 1
=â‡’
Uâˆ—2 = e2.
Proceeding inductively establishes that Uâˆ—k = ek for each k (i.e., U = I ), and
therefore Q1 = Q2 and R1 = R2.

346
Chapter 5
Norms, Inner Products, and Orthogonality
Example 5.7.3
Orthogonal Reduction and Least Squares. Orthogonal reduction can be
used to solve the least squares problem associated with an inconsistent system
Ax = b in which A âˆˆâ„œmÃ—n and m â‰¥n (the most common case). If Îµ
denotes the diï¬€erence Îµ = Ax âˆ’b, then, as described on p. 226, the general
least squares problem is to ï¬nd a vector x that minimizes the quantity
m

i=1
Îµ2
i = ÎµT Îµ = âˆ¥Îµâˆ¥2 ,
where âˆ¥â‹†âˆ¥is the standard euclidean vector norm. Suppose that A is reduced
to an upper-trapezoidal matrix T by an orthogonal matrix P, and write
PA = T =

RnÃ—n
0

and
Pb =

cnÃ—1
d

in which R is an upper-triangular matrix. An orthogonal matrix is an isometryâ€”
recall (5.6.1)â€”so that
âˆ¥Îµâˆ¥2 = âˆ¥PÎµâˆ¥2 = âˆ¥P(Ax âˆ’b)âˆ¥2 =


R
0

x âˆ’

c
d

2
=


Rx âˆ’c
d

2
= âˆ¥Rx âˆ’câˆ¥2 + âˆ¥dâˆ¥2 .
Consequently, âˆ¥Îµâˆ¥2 is minimized when x is a vector such that âˆ¥Rx âˆ’câˆ¥2 is
minimal or, in other words, x is a least squares solution for Ax = b if and only
if x is a least squares solution for Rx = c.
Full-Rank Case.
In a majority of applications the coeï¬ƒcient matrix A has
linearly independent columns so rank (AmÃ—n) = n. Because multiplication by
a nonsingular matrix P does not change the rank,
n = rank (A) = rank (PA) = rank (T) = rank (RnÃ—n).
Thus R is nonsingular, and we have established the following fact.
â€¢
If A has linearly independent columns, then the (unique) least squares so-
lution for Ax = b is obtained by solving the nonsingular triangular system
Rx = c for x.
As pointed out in Example 4.5.1, computing the matrix product AT A is to be
avoided when ï¬‚oating-point computation is used because of the possible loss of
signiï¬cant information. Notice that the method based on orthogonal reduction
sidesteps this potential problem because the normal equations AT Ax = AT b
are avoided and the product AT A is never explicitly computed. Householder
reduction (or Givens reduction for sparse problems) is a numerically stable algo-
rithm (see the discussion following this example) for solving the full-rank least
squares problem, and, if the computations are properly ordered, it is an attrac-
tive alternative to the method of Example 5.5.3 that is based on the modiï¬ed
Gramâ€“Schmidt procedure.

5.7 Orthogonal Reduction
347
We now have four diï¬€erent ways to reduce a matrix to an upper-triangular
(or trapezoidal) form. (1) Gaussian elimination; (2) Gramâ€“Schmidt procedure;
(3) Householder reduction; and (4) Givens reduction. Itâ€™s natural to try to com-
pare them and to sort out the advantages and disadvantages of each.
First consider numerical stability. This is a complicated issue, but you can
nevertheless gain an intuitive feel for the situation by considering the eï¬€ect of
applying a sequence of â€œelementary reductionâ€ matrices to a small perturbation
of A. Let E be a matrix such that âˆ¥Eâˆ¥F
is small relative to âˆ¥Aâˆ¥F
(the
Frobenius norm was introduced on p. 279), and consider
Pk Â· Â· Â· P2P1(A + E) = (Pk Â· Â· Â· P2P1A) + (Pk Â· Â· Â· P2P1E) = PA + PE.
If each Pi is an orthogonal matrix, then the product P = Pk Â· Â· Â· P2P1 is also an
orthogonal matrix (Exercise 5.6.5), and consequently âˆ¥PEâˆ¥F = âˆ¥Eâˆ¥F (Exercise
5.6.9). In other words, a sequence of orthogonal transformations cannot magnify
the magnitude of E, and you might think of E as representing the eï¬€ects of
roundoï¬€error. This suggests that Householder and Givens reductions should be
numerically stable algorithms. On the other hand, if the Pi â€™s are elementary
matrices of Type I, II, or III, then the product P = Pk Â· Â· Â· P2P1 can be any
nonsingular matrixâ€”recall (3.9.3). Nonsingular matrices are not generally norm
preserving (i.e., it is possible that âˆ¥PEâˆ¥F > âˆ¥Eâˆ¥F ), so the possibility of E
being magniï¬ed is generally present in elimination methods, and this suggests
the possibility of numerical instability.
Strictly speaking, an algorithm is considered to be numerically stable
if, under ï¬‚oating-point arithmetic, it always returns an answer that is the exact
solution of a nearby problem. To give an intuitive argument that the Householder
or Givens reduction is a stable algorithm for producing the QR factorization of
AnÃ—n, suppose that Q and R are the exact QR factors, and suppose that
ï¬‚oating-point arithmetic produces an orthogonal matrix Q + E and an upper-
triangular matrix R + F that are the exact QR factors of a diï¬€erent matrix
ËœA = (Q + E)(R + F) = QR + QF + ER + EF = A + QF + ER + EF.
If E and F account for the roundoï¬€errors, and if their entries are small relative
to those in A, then the entries in EF are negligible, and
ËœA â‰ˆA + QF + ER.
But since Q is orthogonal, âˆ¥QFâˆ¥F = âˆ¥Fâˆ¥F and âˆ¥Aâˆ¥F = âˆ¥QRâˆ¥F = âˆ¥Râˆ¥F ,
and this means that neither QF nor ER can contain entries that are large
relative to those in A. Hence ËœA â‰ˆA, and this is what is required to conclude
that the algorithm is stable.
Gaussian elimination is not a stable algorithm because, as alluded to in Â§1.5,
problems arise due to the growth of the magnitude of the numbers that can occur

348
Chapter 5
Norms, Inner Products, and Orthogonality
during the process. To see this from a heuristic point of view, consider the LU
factorization of A = LU, and suppose that ï¬‚oating-point Gaussian elimination
with no pivoting returns matrices L + E and U + F that are the exact LU
factors of a somewhat diï¬€erent matrix
ËœA = (L + E)(U + F) = LU + LF + EU + EF = A + LF + EU + EF.
If E and F account for the roundoï¬€errors, and if their entries are small relative
to those in A, then the entries in EF are negligible, and
ËœA â‰ˆA + LF + EU
(using no pivoting).
However, if L or U contains entries that are large relative to those in A
(and this is certainly possible), then LF or EU can contain entries that are
signiï¬cant. In other words, Gaussian elimination with no pivoting can return the
LU factorization of a matrix
ËœA that is not very close to the original matrix
A, and this is what it means to say that an algorithm is unstable. We saw on
p. 26 that if partial pivoting is employed, then no multiplier can exceed 1 in
magnitude, and hence no entry of L can be greater than 1 in magnitude (recall
that the subdiagonal entries of L are in fact the multipliers). Consequently,
L cannot greatly magnify the entries of F, so, if the rows of A have been
reordered according to the partial pivoting strategy, then
ËœA â‰ˆA + EU
(using partial pivoting).
Numerical stability requires that ËœA â‰ˆA, so the issue boils down to the degree
to which U magniï¬es the entries in E â€”i.e., the issue rests on the magnitude of
the entries in U. Unfortunately, partial pivoting may not be enough to control
the growth of all entries in U. For example, when Gaussian elimination with
partial pivoting is applied to
Wn =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
0
Â· Â· Â·
0
0
1
âˆ’1
1
0
Â· Â· Â·
0
0
1
âˆ’1
âˆ’1
1
...
0
0
1
...
...
...
...
...
...
...
âˆ’1
âˆ’1
âˆ’1
...
1
0
1
âˆ’1
âˆ’1
âˆ’1
Â· Â· Â·
âˆ’1
1
1
âˆ’1
âˆ’1
âˆ’1
Â· Â· Â·
âˆ’1
âˆ’1
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
the largest entry in U is unn = 2nâˆ’1. However, if complete pivoting is used on
Wn, then no entry in the process exceeds 2 in magnitude (Exercises 1.5.7 and
1.5.8). In general, it has been proven that if complete pivoting is used on a well-
scaled matrix AnÃ—n for which max |aij| = 1, then no entry of U can exceed

5.7 Orthogonal Reduction
349
Î³ = n1/2 
2131/241/3 Â· Â· Â· n1/nâˆ’11/2 in magnitude. Since Î³ is a slow growing
function of n, the entries in U wonâ€™t greatly magnify the entries of E, so
ËœA â‰ˆA
(using complete pivoting).
In other words, Gaussian elimination with complete pivoting is stable, but Gaus-
sian elimination with partial pivoting is not. Fortunately, in practical work it is
rare to encounter problems such as the matrix Wn in which partial pivoting
fails to control the growth in the U factor, so scaled partial pivoting is generally
considered to be a â€œpractically stableâ€ algorithm.
Algorithms based on the Gramâ€“Schmidt procedure are more complicated.
First, the Gramâ€“Schmidt algorithms diï¬€er from Householder and Givens reduc-
tions in that the Gramâ€“Schmidt procedures are not a sequential application of
elementary orthogonal transformations. Second, as an algorithm to produce the
QR factorization even the modiï¬ed Gramâ€“Schmidt technique can return a Q
factor that is far from being orthogonal, and the intuitive stability argument
used earlier is not valid. As an algorithm to return the QR factorization of A,
the modiï¬ed Gramâ€“Schmidt procedure has been proven to be unstable, but as
an algorithm used to solve the least squares problem (see Example 5.5.3), it is
stableâ€”i.e., stability of modiï¬ed Gramâ€“Schmidt is problem dependent.
Summary of Numerical Stability
â€¢
Gaussian elimination with scaled partial pivoting is theoretically un-
stable, but it is â€œpractically stableâ€â€”i.e., stable for most practical
problems.
â€¢
Complete pivoting makes Gaussian elimination unconditionally sta-
ble.
â€¢
For the QR factorization, the Gramâ€“Schmidt procedure (classical
or modiï¬ed) is not stable. However, the modiï¬ed Gramâ€“Schmidt
procedure is a stable algorithm for solving the least squares problem.
â€¢
Householder and Givens reductions are unconditionally stable algo-
rithms for computing the QR factorization.
For the algorithms under consideration, the number of multiplicative oper-
ations is about the same as the number of additive operations, so computational
eï¬€ort is gauged by counting only multiplicative operations. For the sake of com-
parison, lower-order terms are not signiï¬cant, and when they are neglected the
following approximations are obtained.

350
Chapter 5
Norms, Inner Products, and Orthogonality
Summary of Computational Effort
The approximate number of multiplications/divisions required to reduce
an n Ã— n matrix to an upper-triangular form is as follows.
â€¢
Gaussian elimination (scaled partial pivoting) â‰ˆn3/3.
â€¢
Gramâ€“Schmidt procedure (classical and modiï¬ed) â‰ˆn3.
â€¢
Householder reduction â‰ˆ2n3/3.
â€¢
Givens reduction â‰ˆ4n3/3.
Itâ€™s not surprising that the unconditionally stable methods tend to be more
costlyâ€”there is no free lunch. No one triangularization technique can be con-
sidered optimal, and each has found a place in practical work. For example, in
solving unstructured linear systems, the probability of Gaussian elimination with
scaled partial pivoting failing is not high enough to justify the higher cost of using
the safer Householder or Givens reduction, or even complete pivoting. Although
much the same is true for the full-rank least squares problem, Householder re-
duction or modiï¬ed Gramâ€“Schmidt is frequently used as a safeguard against
sensitivities that often accompany least squares problems. For the purpose of
computing an orthonormal basis for R (A) in which A is unstructured and
dense (not many zeros), Householder reduction is preferredâ€”the Gramâ€“Schmidt
procedures are unstable for this purpose and Givens reduction is too costly.
Givens reduction is useful when the matrix being reduced is highly structured
or sparse (many zeros).
Example 5.7.4
Reduction to Hessenberg Form. For reasons alluded to in Â§4.8 and Â§4.9, it
is often desirable to triangularize a square matrix A by means of a similarity
transformationâ€”i.e., ï¬nd a nonsingular matrix P such that Pâˆ’1AP = T is
upper triangular. But this is a computationally diï¬ƒcult task, so we will try to do
the next best thing, which is to ï¬nd a similarity transformation that will reduce
A to a matrix in which all entries below the ï¬rst subdiagonal are zero. Such a
matrix is said to be in upper-Hessenberg formâ€”illustrated below is a 5 Ã— 5
Hessenberg form.
H =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
âˆ—
0
0
âˆ—
âˆ—
âˆ—
0
0
0
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£¸.

5.7 Orthogonal Reduction
351
Problem: Reduce A âˆˆâ„œnÃ—n to upper-Hessenberg form by means of an orthog-
onal similarity transformationâ€”i.e., construct an orthogonal matrix P such that
PT AP = H is upper Hessenberg.
Solution: At each step, use Householder reduction on entries below the main
diagonal. Begin by letting Ë†Aâˆ—1 denote the entries of the ï¬rst column that are
below the (1,1)-positionâ€”this is illustrated below for n = 5:
A =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
=
 a11
Ë†A1âˆ—
Ë†Aâˆ—1
A1

.
If
Ë†R1 is an elementary reï¬‚ector determined according to (5.7.1) for which
Ë†R1 Ë†Aâˆ—1 =
ï£«
ï£­
âˆ—
0
0
0
ï£¶
ï£¸, then R1 =
 1
0
0
Ë†R1

is an orthogonal matrix such that
R1AR1 =
 1
0
0
Ë†R1
 
a11
Ë†A1âˆ—
Ë†Aâˆ—1
A1
  1
0
0
Ë†R1

=

a11
Ë†A1âˆ—Ë†R1
Ë†R1 Ë†Aâˆ—1
Ë†R1A1 Ë†R1

=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
At the second step, repeat the process on A2 = Ë†R1A1 Ë†R1 to obtain an orthogo-
nal matrix Ë†R2 such that Ë†R2A2 Ë†R2 =
ï£«
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
ï£¶
ï£·
ï£¸. Matrix R2 =
 I2
0
0
Ë†R2

is an orthogonal matrix such that
R2R1AR1R2 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
âˆ—
0
0
âˆ—
âˆ—
âˆ—
0
0
âˆ—
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
After n âˆ’2 of these steps, the product P = R1R2 Â· Â· Â· Rnâˆ’2 is an orthogonal
matrix such that PT AP = H is in upper-Hessenberg form.

352
Chapter 5
Norms, Inner Products, and Orthogonality
Note:
If A is a symmetric matrix, then HT = (PT AP)T = PT AT P = H,
so H is symmetric. But as illustrated below for n = 5, a symmetric Hessenberg
form is a tridiagonal matrix,
H = PT AP =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
0
0
0
âˆ—
âˆ—
âˆ—
0
0
0
âˆ—
âˆ—
âˆ—
0
0
0
âˆ—
âˆ—
âˆ—
0
0
0
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£¸,
so the following useful corollary is obtained.
â€¢
Every real-symmetric matrix is orthogonally similar to a tridiagonal matrix,
and Householder reduction can be used to compute this tridiagonal matrix.
However, the Lanczos technique discussed on p. 651 can be much more eï¬ƒ-
cient.
Example 5.7.5
Problem: Compute the QR factors of a nonsingular upper-Hessenberg matrix
H âˆˆâ„œnÃ—n.
Solution: Due to its smaller multiplication count, Householder reduction is
generally preferred over Givens reduction. The exception is for matrices that
have a zero pattern that can be exploited by the Givens method but not by
the Householder method. A Hessenberg matrix H is such an example. The
ï¬rst step of Householder reduction completely destroys most of the zeros in
H, but applying plane rotations does not. This is illustrated below for a 5 Ã— 5
Hessenberg formâ€”remember that the action of Pk,k+1 aï¬€ects only the kth and
(k + 1)st rows.
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
âˆ—
0
0
âˆ—
âˆ—
âˆ—
0
0
0
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£¸
P12
âˆ’âˆ’âˆ’â†’
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
âˆ—
0
0
âˆ—
âˆ—
âˆ—
0
0
0
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£¸
P23
âˆ’âˆ’âˆ’â†’
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
âˆ—
0
0
âˆ—
âˆ—
âˆ—
0
0
âˆ—
âˆ—
âˆ—
0
0
0
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£¸
P34
âˆ’âˆ’âˆ’â†’
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
âˆ—
0
0
âˆ—
âˆ—
âˆ—
0
0
0
âˆ—
âˆ—
0
0
0
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£¸
P45
âˆ’âˆ’âˆ’â†’
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
âˆ—
0
0
âˆ—
âˆ—
âˆ—
0
0
0
âˆ—
âˆ—
0
0
0
0
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£¸.
In general, Pnâˆ’1,n Â· Â· Â· P23P12H = R is upper triangular in which all diagonal
entries, except possibly the last, are positiveâ€”the last diagonal can be made pos-
itive by the technique illustrated in Example 5.7.2. Thus we obtain an orthogonal
matrix P such that PH = R, or H = QR in which Q = PT .

5.7 Orthogonal Reduction
353
Example 5.7.6
Jacobi Reduction.
49 Given a real-symmetric matrix A, the result of Example
5.7.4 shows that Householder reduction can be used to construct an orthogonal
matrix P such that PT AP = T is tridiagonal. Can we do better?â€”i.e., can we
construct an orthogonal matrix P such that PT AP = D is a diagonal matrix?
Indeed we can, and much of the material in Chapter 7 concerning eigenvalues and
eigenvectors is devoted to this problem. But in the present context, this fact can
be constructively established by means of Jacobiâ€™s diagonalization algorithm.
Jacobiâ€™s Idea. If A âˆˆâ„œnÃ—n is symmetric, then a plane rotation matrix can
be applied to reduce the magnitude of the oï¬€-diagonal entries. In particular,
suppose that aij Ì¸= 0 is the oï¬€-diagonal entry of maximal magnitude, and let
Aâ€² denote the matrix obtained by setting each akk = 0. If Pij is the plane
rotation matrix described on p. 333 in which c = cos Î¸ and s = sin Î¸, where
cot 2Î¸ = (aii âˆ’ajj)/2aij, and if B = PT
ijAPij, then
(1)
bij = bji = 0
(i.e., aij is annihilated),
(2)
âˆ¥Bâ€²âˆ¥2
F = âˆ¥Aâ€²âˆ¥2
F âˆ’2a2
ij,
(3)
âˆ¥Bâ€²âˆ¥2
F â‰¤

1 âˆ’
2
n2 âˆ’n

âˆ¥Aâ€²âˆ¥2
F .
Proof.
The entries of B = PT
ijAPij that lay on the intersection of the ith and
jth rows with the ith and jth columns can be described by
Ë†B =

bii
bij
bji
bjj

=

cos Î¸
sin Î¸
âˆ’sin Î¸
cos Î¸
 
aii
aij
aij
ajj
 
cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸

= PT Ë†AP.
Use the identities cos 2Î¸ = cos2 Î¸ âˆ’sin2 Î¸ and sin 2Î¸ = 2 cos Î¸ sin Î¸ to verify
bij = bji = 0, and recall that âˆ¥Ë†Bâˆ¥F = âˆ¥PT Ë†APâˆ¥F = âˆ¥Ë†Aâˆ¥F (recall Exercise
49
Karl Gustav Jacob Jacobi (1804â€“1851) ï¬rst presented this method in 1846, and it was popular
for a time. But the twentieth-century development of electronic computers sparked tremendous
interest in numerical algorithms for diagonalizing symmetric matrices, and Jacobiâ€™s method
quickly fell out of favor because it could not compete with newer proceduresâ€”at least on the
traditional sequential machines. However, the emergence of multiprocessor parallel computers
has resurrected interest in Jacobiâ€™s method because of the inherent parallelism in the algorithm.
Jacobi was born in Potsdam, Germany, educated at the University of Berlin, and employed as
a professor at the University of KÂ¨onigsberg. During his proliï¬c career he made contributions
that are still important facets of contemporary mathematics. His accomplishments include the
development of elliptic functions; a systematic development and presentation of the theory
of determinants; contributions to the theory of rotating liquids; and theorems in the areas of
diï¬€erential equations, calculus of variations, and number theory. In contrast to his great con-
temporary Gauss, who disliked teaching and was anything but inspiring, Jacobi was regarded
as a great teacher (the introduction of the student seminar method is credited to him), and
he advocated the view that â€œthe sole end of science is the honor of the human mind, and that
under this title a question about numbers is worth as much as a question about the system
of the world.â€ Jacobi once defended his excessive devotion to work by saying that â€œOnly cab-
bages have no nerves, no worries. And what do they get out of their perfect wellbeing?â€ Jacobi
suï¬€ered a breakdown from overwork in 1843, and he died at the relatively young age of 46.

354
Chapter 5
Norms, Inner Products, and Orthogonality
5.6.9) to produce the conclusion b2
ii + b2
jj = a2
ii + 2a2
ij + a2
jj. Now use the fact
that bkk = akk for all k Ì¸= i, j together with âˆ¥Bâˆ¥F = âˆ¥Aâˆ¥F to write
âˆ¥Bâ€²âˆ¥2
F = âˆ¥Bâˆ¥2
F âˆ’

k
b2
kk = âˆ¥Bâˆ¥2
F âˆ’

kÌ¸=i,j
b2
kk âˆ’

b2
ii + b2
jj

= âˆ¥Aâˆ¥2
F âˆ’

kÌ¸=i,j
a2
kk âˆ’

a2
ii + 2a2
ij + a2
jj

= âˆ¥Aâˆ¥2
F âˆ’

k
a2
kk âˆ’2a2
ij
= âˆ¥Aâ€²âˆ¥2
F âˆ’2a2
ij.
Furthermore, since a2
pq â‰¤a2
ij for all p Ì¸= q,
âˆ¥Aâ€²âˆ¥2
F =

pÌ¸=q
a2
pq â‰¤

pÌ¸=q
a2
ij = (n2 âˆ’n)a2
ij
=â‡’
âˆ’a2
ij â‰¤âˆ’âˆ¥Aâ€²âˆ¥2
F
n2 âˆ’n,
so
âˆ¥Bâ€²âˆ¥2
F = âˆ¥Aâ€²âˆ¥2
F âˆ’2a2
ij â‰¤âˆ¥Aâ€²âˆ¥2
F âˆ’2âˆ¥Aâ€²âˆ¥2
F
n2 âˆ’n =

1 âˆ’
2
n2 âˆ’n

âˆ¥Aâ€²âˆ¥2
F .
Jacobiâ€™s Diagonalization Algorithm. Start with A0 = A, and produce a
sequence of matrices Ak = PT
k Akâˆ’1Pk, where at the kth step Pk is a plane
rotation constructed to annihilate the maximal oï¬€-diagonal entry in Akâˆ’1. In
particular, if aij is the entry of maximal magnitude in Akâˆ’1, then Pk is the
rotator in the (i, j)-plane deï¬ned by setting
s =
1
âˆš
1 + Ïƒ2
and
c =
Ïƒ
âˆš
1 + Ïƒ2 =

1 âˆ’s2,
where
Ïƒ = (aii âˆ’ajj)
2aij
.
For n > 2 we have
âˆ¥Aâ€²
kâˆ¥2
F â‰¤

1 âˆ’
2
n2 âˆ’n
k
âˆ¥Aâ€²âˆ¥2
F â†’0
as
k â†’âˆ.
Therefore, if P(k) is the orthogonal matrix deï¬ned by P(k) = P1P2 Â· Â· Â· Pk, then
lim
kâ†’âˆP(k)T AP(k) = lim
kâ†’âˆAk = D
is a diagonal matrix.
Exercises for section 5.7
5.7.1.
(a)
Using Householder reduction, compute the QR factors of
A =
ï£«
ï£­
1
19
âˆ’34
âˆ’2
âˆ’5
20
2
8
37
ï£¶
ï£¸.
(b)
Repeat part (a) using Givens reduction.

5.7 Orthogonal Reduction
355
5.7.2. For A âˆˆâ„œmÃ—n, suppose that rank (A) = n, and let P be an orthog-
onal matrix such that
PA = T =

RnÃ—n
0

,
where R is an upper-triangular matrix. If PT is partitioned as
PT = [XmÃ—n | Y] ,
explain why the columns of X constitute an orthonormal basis for
R (A).
5.7.3. By using Householder reduction, ï¬nd an orthonormal basis for R (A),
where
A =
ï£«
ï£¬
ï£­
4
âˆ’3
4
2
âˆ’14
âˆ’3
âˆ’2
14
0
1
âˆ’7
15
ï£¶
ï£·
ï£¸.
5.7.4. Use Householder reduction to compute the least squares solution for
Ax = b, where
A =
ï£«
ï£¬
ï£­
4
âˆ’3
4
2
âˆ’14
âˆ’3
âˆ’2
14
0
1
âˆ’7
15
ï£¶
ï£·
ï£¸
and
b =
ï£«
ï£¬
ï£­
5
âˆ’15
0
30
ï£¶
ï£·
ï£¸.
Hint: Make use of the factors you computed in Exercise 5.7.3.
5.7.5. If A = QR is the QR factorization for A, explain why âˆ¥Aâˆ¥F = âˆ¥Râˆ¥F ,
where âˆ¥â‹†âˆ¥F is the Frobenius matrix norm introduced on p. 279.
5.7.6. Find an orthogonal matrix P such that PT AP = H is in upper-
Hessenberg form, where
A =
ï£«
ï£­
âˆ’2
3
âˆ’4
3
âˆ’25
50
âˆ’4
50
25
ï£¶
ï£¸.
5.7.7. Let H be an upper-Hessenberg matrix, and suppose that H = QR,
where R is a nonsingular upper-triangular matrix. Prove that Q as
well as the product RQ must also be in upper-Hessenberg form.
5.7.8. Approximately how many multiplications are needed to reduce an n Ã— n
nonsingular upper-Hessenberg matrix to upper-triangular form by using
plane rotations?

356
Chapter 5
Norms, Inner Products, and Orthogonality
5.8
DISCRETE FOURIER TRANSFORM
For a positive integer n, the complex numbers

1, Ï‰, Ï‰2, . . . , Ï‰nâˆ’1
, where
Ï‰ = e2Ï€i/n = cos 2Ï€
n + i sin 2Ï€
n
are called the n throots of unity because they represent all solutions to zn = 1.
Geometrically, they are the vertices of a regular polygon of n sides as depicted
in Figure 5.8.1 for n = 3 and n = 6.
1
1
Ï‰
Ï‰
Ï‰2
Ï‰2
Ï‰3
Ï‰4
Ï‰5
n = 6
n = 3
Figure 5.8.1
The roots of unity are cyclic in the sense that if k â‰¥n, then Ï‰k = Ï‰k (mod n),
where k (mod n) denotes the remainder when k is divided by nâ€”for example,
when n = 6, Ï‰6 = 1, Ï‰7 = Ï‰, Ï‰8 = Ï‰2, Ï‰9 = Ï‰3, . . . .
The numbers

1, Î¾, Î¾2, . . . , Î¾nâˆ’1
, where
Î¾ = eâˆ’2Ï€i/n = cos 2Ï€
n âˆ’i sin 2Ï€
n = Ï‰
are also the nth roots of unity, but, as depicted in Figure 5.8.2 for n = 3 and
n = 6, they are listed in clockwise order around the unit circle rather than
counterclockwise.
1
Î¾5
Î¾4
Î¾3
Î¾2
Î¾
n = 6
1
Î¾2
Î¾
n = 3
Figure 5.8.2
The following identities will be useful in our development. If k is an integer,
then 1 = |Î¾k|2 = Î¾kÎ¾k implies that
Î¾âˆ’k = Î¾k = Ï‰k.
(5.8.1)

5.8 Discrete Fourier Transform
357
Furthermore, the fact that
Î¾k 
1 + Î¾k + Î¾2k + Â· Â· Â· + Î¾(nâˆ’2)k + Î¾(nâˆ’1)k
= Î¾k + Î¾2k + Â· Â· Â· + Î¾(nâˆ’1)k + 1
implies

1 + Î¾k + Î¾2k + Â· Â· Â· + Î¾(nâˆ’1)k 
1 âˆ’Î¾k
= 0 and, consequently,
1 + Î¾k + Î¾2k + Â· Â· Â· + Î¾(nâˆ’1)k = 0
whenever
Î¾k Ì¸= 1.
(5.8.2)
Fourier Matrix
The n Ã— n matrix whose (j, k)-entry is Î¾jk = Ï‰âˆ’jk for 0 â‰¤j, k â‰¤nâˆ’1
is called the Fourier matrix of order n, and it has the form
Fn =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
1
1
Â· Â· Â·
1
1
Î¾
Î¾2
Â· Â· Â·
Î¾nâˆ’1
1
Î¾2
Î¾4
Â· Â· Â·
Î¾nâˆ’2
...
...
...
...
...
1
Î¾nâˆ’1
Î¾nâˆ’2
Â· Â· Â·
Î¾
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
nÃ—n
.
Note. Throughout this section entries are indexed from 0 to n âˆ’1.
For example, the upper left-hand entry of Fn is considered to be in the
(0, 0) position (rather than the (1, 1) position), and the lower right-
hand entry is in the (n âˆ’1, n âˆ’1) position. When the context makes it
clear, the subscript n on Fn is omitted.
The Fourier matrix
50 is a special case of the Vandermonde matrix introduced
in Example 4.3.4. Using (5.8.1) and (5.8.2), we see that the inner product of any
two columns in Fn, say, the rth and sth, is
Fâˆ—
âˆ—rFâˆ—s =
nâˆ’1

j=0
Î¾jrÎ¾js =
nâˆ’1

j=0
Î¾âˆ’jrÎ¾js =
nâˆ’1

j=0
Î¾j(sâˆ’r) = 0.
In other words, the columns in Fn are mutually orthogonal. Furthermore, each
column in Fn has norm âˆšn because
âˆ¥Fâˆ—kâˆ¥2
2 =
nâˆ’1

j=0
|Î¾jk|2 =
nâˆ’1

j=0
1 = n,
50
Some authors deï¬ne the Fourier matrix using powers of Ï‰ rather than powers of Î¾, and some
include a scalar multiple 1/n or 1/âˆšn. These diï¬€erences are superï¬cial, and they do not
aï¬€ect the basic properties. Our deï¬nition is the discrete counterpart of the integral operator
F(f) =  âˆ
âˆ’âˆx(t)eâˆ’i2Ï€ftdt that is usually taken as the deï¬nition of the continuous Fourier
transform.

358
Chapter 5
Norms, Inner Products, and Orthogonality
and consequently every column of Fn can be normalized by multiplying by the
same scalarâ€”namely, 1/âˆšn. This means that (1/âˆšn )Fn is a unitary matrix.
Since it is also true that FT
n = Fn, we have
 1
âˆšnFn
âˆ’1
=
 1
âˆšnFn
âˆ—
=
1
âˆšnFn,
and therefore Fâˆ’1
n
= Fn/n. But (5.8.1) says that Î¾k = Ï‰k, so it must be the
case that
Fâˆ’1
n
= 1
nFn = 1
n
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
1
1
Â· Â· Â·
1
1
Ï‰
Ï‰2
Â· Â· Â·
Ï‰nâˆ’1
1
Ï‰2
Ï‰4
Â· Â· Â·
Ï‰nâˆ’2
...
...
...
...
...
1
Ï‰nâˆ’1
Ï‰nâˆ’2
Â· Â· Â·
Ï‰
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
nÃ—n
.
Example 5.8.1
The Fourier matrices of orders 2 and 4 are given by
F2 =

1
1
1
âˆ’1

and
F4 =
ï£«
ï£¬
ï£­
1
1
1
1
1
âˆ’i
âˆ’1
i
1
âˆ’1
1
âˆ’1
1
i
âˆ’1
âˆ’i
ï£¶
ï£·
ï£¸,
and their inverses are
Fâˆ’1
2
= 1
2F2 = 1
2

1
1
1
âˆ’1

and
Fâˆ’1
4
= 1
4F4 = 1
4
ï£«
ï£¬
ï£­
1
1
1
1
1
i
âˆ’1
âˆ’i
1
âˆ’1
1
âˆ’1
1
âˆ’i
âˆ’1
i
ï£¶
ï£·
ï£¸.
Discrete Fourier Transform
Given a vector xnÃ—1, the product Fnx is called the discrete Fourier
transform of x, and Fâˆ’1
n x is called the inverse transform of x.
The kth entries in Fnx and Fâˆ’1
n x are given by
[Fnx]k =
nâˆ’1

j=0
xjÎ¾jk
and
[Fâˆ’1
n x]k = 1
n
nâˆ’1

j=0
xjÏ‰jk.
(5.8.3)

5.8 Discrete Fourier Transform
359
Example 5.8.2
Problem: Computing the Inverse Transform. Explain why any algorithm
or program designed to compute the discrete Fourier transform of a vector x
can also be used to compute the inverse transform of x.
Solution: Call such an algorithm FFT (see p. 373 for a speciï¬c example). The
fact that
Fâˆ’1
n x = Fnx
n
= Fnx
n
means that FFT will return the inverse transform of x by executing the following
three steps:
(1)
x â†âˆ’x
(compute x ).
(2)
x â†âˆ’FFT(x)
(compute Fnx ).
(3)
x â†âˆ’(1/n)x
(compute nâˆ’1Fnx = Fâˆ’1
n x ).
For example, computing the inverse transform of x = ( i
0
âˆ’i
0 )T is ac-
complished as followsâ€”recall that F4 was given in Example 5.8.1.
x =
ï£«
ï£¬
ï£­
âˆ’i
0
i
0
ï£¶
ï£·
ï£¸,
F4x =
ï£«
ï£¬
ï£­
0
âˆ’2i
0
âˆ’2i
ï£¶
ï£·
ï£¸,
1
4F4x = 1
4
ï£«
ï£¬
ï£­
0
2i
0
2i
ï£¶
ï£·
ï£¸= Fâˆ’1
4 x.
You may wish to check that this answer agrees with the result obtained by
directly multiplying Fâˆ’1
4
times x, where Fâˆ’1
4
is given in Example 5.8.1.
Example 5.8.3
Signal Processing.
Suppose that a microphone is placed under a hovering
helicopter, and suppose that Figure 5.8.3 represents the sound signal that is
recorded during 1 second of time.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-6
-4
-2
0
2
4
6
Figure 5.8.3

360
Chapter 5
Norms, Inner Products, and Orthogonality
It seems reasonable to expect that the signal should have oscillatory components
together with some random noise contamination. That is, we expect the signal
to have the form
y(Ï„) =

k
Î±k cos 2Ï€fkÏ„ + Î²k sin 2Ï€fkÏ„

+ Noise.
But due to the noise contamination, the oscillatory nature of the signal is only
barely apparentâ€”the characteristic â€œchop-a chop-a chop-aâ€ is not completely
clear. To reveal the oscillatory components, the magic of the Fourier transform
is employed. Let x be the vector obtained by sampling the signal at n equally
spaced points between time Ï„ = 0 and Ï„ = 1 ( n = 512 in our case), and let
y = (2/n)Fnx = a + ib,
where
a = (2/n)Re (Fnx) and b = (2/n)Im (Fnx) .
Using only the ï¬rst n/2 = 256 entries in a and ib, we plot the points in
{(0, a0), (1, a1), . . . , (255, a255)}
and
{(0, ib0), (1, ib1), . . . , (255, ib255)}
to produce the two graphs shown in Figure 5.8.4.
0
50
100
150
200
250
300
-0.5
0
0.5
1
1.5
Real Axis
Frequency
0
50
100
150
200
250
300
-2
-1.5
-1
-0.5
0
0.5
Imaginary Axis
Frequency
Figure 5.8.4
Now there are some obvious characteristicsâ€”the plot of a in the top graph of
Figure 5.8.4 has a spike of height approximately 1 at entry 80, and the plot of
ib in the bottom graph has a spike of height approximately âˆ’2 at entry 50.
These two spikes indicate that the signal is made up primarily of two oscillatory

5.8 Discrete Fourier Transform
361
componentsâ€”the spike in the real vector a indicates that one of the oscillatory
components is a cosine of frequency 80 Hz (or period = 1/80 ) whose amplitude
is approximately 1, and the spike in the imaginary vector ib indicates there is a
sine component with frequency 50 Hz and amplitude of about 2. In other words,
the Fourier transform indicates that the signal is
y(Ï„) = cos 2Ï€(80Ï„) + 2 sin 2Ï€(50Ï„) + Noise.
In truth, the data shown in Figure 5.8.3 was artiï¬cially generated by contami-
nating the function y(Ï„) = cos 2Ï€(80Ï„) + 2 sin 2Ï€(50Ï„) with some normally dis-
tributed zero-mean noise, and therefore the plot of (2/n)Fnx shown in Figure
5.8.4 does indeed accurately reï¬‚ect the true nature of the signal. To understand
why Fn reveals the hidden frequencies, let cos 2Ï€ft and sin 2Ï€ft denote the
discrete cosine and discrete sine vectors
cos 2Ï€ft =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
cos

2Ï€f Â· 0
n

cos

2Ï€f Â· 1
n

cos

2Ï€f Â· 2
n

...
cos

2Ï€f Â· nâˆ’1
n

ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
and
sin 2Ï€ft =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
sin

2Ï€f Â· 0
n

sin

2Ï€f Â· 1
n

sin

2Ï€f Â· 2
n

...
sin

2Ï€f Â· nâˆ’1
n

ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
where t = ( 0/n
1/n
2/n
Â· Â· Â·
nâˆ’1/n )T is the discrete time vector. If
the discrete exponential vectors ei2Ï€ft and eâˆ’i2Ï€ft are deï¬ned in the natural
way as ei2Ï€ft = cos 2Ï€ft + i sin 2Ï€ft and eâˆ’i2Ï€ft = cos 2Ï€ft âˆ’i sin 2Ï€ft, and
if 0 â‰¤f < n is an integer frequency,
51 then
ei2Ï€ft =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Ï‰0f
Ï‰1f
Ï‰2f
...
Ï‰(nâˆ’1)f
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
= n

Fâˆ’1
n

âˆ—f = nFâˆ’1
n ef,
where ef is the n Ã— 1 unit vector with a 1 in the f th componentâ€”remember
that components of vectors are indexed from 0 to nâˆ’1 throughout this section.
Similarly, the fact that
Î¾kf = Ï‰âˆ’kf = 1Ï‰âˆ’kf = Ï‰knÏ‰âˆ’kf = Ï‰k(nâˆ’f)
for
k = 0, 1, 2, . . .
allows us to conclude that if 0 â‰¤n âˆ’f < n, then
eâˆ’i2Ï€ft =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î¾0f
Î¾1f
Î¾2f
...
Î¾(nâˆ’1)f
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Ï‰0(nâˆ’f)
Ï‰1(nâˆ’f)
Ï‰2(nâˆ’f)
...
Ï‰(nâˆ’1)(nâˆ’f)
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
= n

Fâˆ’1
n

âˆ—nâˆ’f = nFâˆ’1
n enâˆ’f.
51
The assumption that frequencies are integers is not overly harsh because the Fourier series for
a periodic function requires only integer frequenciesâ€”recall Example 5.4.6.

362
Chapter 5
Norms, Inner Products, and Orthogonality
Therefore, if 0 < f < n, then
Fnei2Ï€ft = nef
and
Fneâˆ’i2Ï€ft = nenâˆ’f.
(5.8.4)
Because cos Î¸ = (eiÎ¸ +eâˆ’iÎ¸)/2 and sin Î¸ = (eiÎ¸ âˆ’eâˆ’iÎ¸)/2i, it follows from (5.8.4)
that for any scalars Î± and Î²,
Fn(Î± cos 2Ï€ft) = Î±Fn
ei2Ï€ft + eâˆ’i2Ï€ft
2

= nÎ±
2 (ef + enâˆ’f)
and
Fn(Î² sin 2Ï€ft) = Î²Fn
ei2Ï€ft âˆ’eâˆ’i2Ï€ft
2i

= nÎ²
2i (ef âˆ’enâˆ’f) ,
so that
2
nFn(Î± cos 2Ï€ft) = Î±ef + Î±enâˆ’f
(5.8.5)
and
2
nFn(Î² sin 2Ï€ft) = âˆ’Î²ief + Î²ienâˆ’f.
(5.8.6)
The trigonometric functions Î± cos 2Ï€fÏ„ and Î² sin 2Ï€fÏ„ have amplitudes Î± and
Î², respectively, and their frequency is f (their period is 1/f ). The discrete
vectors Î± cos 2Ï€ft and Î² sin 2Ï€ft are obtained by evaluating Î± cos 2Ï€fÏ„ and
Î² sin 2Ï€fÏ„ at the discrete points in t = ( 0
1/n
2/n
Â· Â· Â·
(n âˆ’1)/n )T . As
depicted in Figure 5.8.5 for n = 32 and f = 4, the vectors Î±ef and Î±enâˆ’f
are interpreted as two pulses of magnitude Î± at frequencies f and n âˆ’f.
28
4
32
16
8
24
Î±
Frequency
 n = 32      f = 4
0
(1/16)F( 
)
Time
-Î±
0
Î±
1
Î±cos ïœ¸Ï€t
Î±cos ïœ¸Ï€t
Figure 5.8.5

5.8 Discrete Fourier Transform
363
The vector Î± cos 2Ï€ft is said to be in the time domain, while the pulses
Î±ef and Î±enâˆ’f are said to be in the frequency domain. The situation for
Î² sin 2Ï€ft is similarly depicted in Figure 5.8.6 in which âˆ’Î²ief and Î²ienâˆ’f are
considered two pulses of height âˆ’Î² and Î², respectively.
1
Time
-Î²
0
Î²
28
4
32
16
8
24
Î²
- Î²i
i
Frequency
 n = 32      f = 4
0
(1/16)F( 
)
sin ïœ¸Ï€t
Î²
sin ïœ¸Ï€t
Î²
Figure 5.8.6
Therefore, if a waveform is given by a ï¬nite sum
x(Ï„) =

k
(Î±k cos 2Ï€fkÏ„ + Î²k sin 2Ï€fkÏ„)
in which the fk â€™s are integers, and if x is the vector containing the values of
x(Ï„) at n equally spaced points between time Ï„ = 0 and Ï„ = 1, then, provided
that n is suï¬ƒciently large,
2
nFnx = 2
nFn

k
Î±k cos 2Ï€fkt + Î²k sin 2Ï€fkt

=

k
2
nFn (Î±k cos 2Ï€fkt) +

k
2
nFn (Î²k sin 2Ï€fkt)
=

k
Î±k (efk + enâˆ’fk) + i

k
Î²k (âˆ’efk + enâˆ’fk) ,
(5.8.7)
and this exposes the frequency and amplitude of each of the components. If n is
chosen so that max{fk} < n/2, then the pulses represented by ef and enâˆ’f are

364
Chapter 5
Norms, Inner Products, and Orthogonality
symmetric about the point n/2 in the frequency domain, and the information in
just the ï¬rst (or second) half of the frequency domain completely characterizes
the original waveformâ€”this is why only 512/2=256 points are plotted in the
graphs shown in Figure 5.8.4. In other words, if
y = 2
nFnx =

k
Î±k (efk + enâˆ’fk) + i

k
Î²k (âˆ’efk + enâˆ’fk) ,
(5.8.8)
then the information in
yn/2 =

k
Î±kefk âˆ’i

k
Î²kefk
(the ï¬rst half of y )
is enough to reconstruct the original waveform. For example, the equation of the
waveform shown in Figure 5.8.7 is
x(Ï„) = 3 cos 2Ï€Ï„ + 5 sin 2Ï€Ï„,
(5.8.9)
Amplitude
1
. 2 5
. 5
. 7 5
1
0
2
3
4
5
6
- 1
- 2
- 3
- 4
- 5
- 6
Time
Figure 5.8.7
and it is completely determined by the four values in
x =
ï£«
ï£¬
ï£­
x(0)
x(1/4)
x(1/2)
x(3/4)
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
3
5
âˆ’3
âˆ’5
ï£¶
ï£·
ï£¸.
To capture equation (5.8.9) from these four values, compute the vector y deï¬ned
by (5.8.8) to be
y = 2
4F4x =
ï£«
ï£¬
ï£­
1
1
1
1
1
âˆ’i
âˆ’1
i
1
âˆ’1
1
âˆ’1
1
i
âˆ’1
âˆ’i
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
3
5
âˆ’3
âˆ’5
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
0
3 âˆ’5i
0
3 + 5i
ï£¶
ï£·
ï£¸
=
ï£«
ï£¬
ï£­
0
3
0
3
ï£¶
ï£·
ï£¸+ i
ï£«
ï£¬
ï£­
0
âˆ’5
0
5
ï£¶
ï£·
ï£¸= 3(e1 + e3) + 5i(âˆ’e1 + e3).

5.8 Discrete Fourier Transform
365
The real part of y tells us there is a cosine component with amplitude = 3 and
frequency = 1, while the imaginary part of y says there is a sine component
with amplitude = 5 and frequency = 1. This is depicted in the frequency
domain shown in Figure 5.8.8.
Imaginary Axis
4
1
2
3
1
0
2
3
4
5
6
- 1
- 2
- 3
- 4
- 5
- 6
Frequency
Real Axis
4
1
2
3
1
0
2
3
4
5
6
Frequency
Figure 5.8.8
Putting this information together allows us to conclude that the equation of the
waveform must be x(Ï„) = 3 cos 2Ï€Ï„ + 5 sin 2Ï€Ï„. Since
1 = max{fk} < n
2 = 4
2 = 2,
the information in just the ï¬rst half of y
yn/2 =

0
3

+ i

0
âˆ’5

= 3e1 âˆ’5ie1
suï¬ƒces to completely characterize x(Ï„).
These elementary ideas help explain why applying F to a sample from a
signal can reveal the oscillatory components of the signal. But there is still a
signiï¬cant amount of theory that is well beyond the scope of this example. The
purpose here is to just hint at how useful the discrete Fourier transform is and
why it is so important in analyzing the nature of complicated waveforms.

366
Chapter 5
Norms, Inner Products, and Orthogonality
If
a =
ï£«
ï£¬
ï£¬
ï£­
Î±0
Î±1
...
Î±nâˆ’1
ï£¶
ï£·
ï£·
ï£¸
nÃ—1
and
b =
ï£«
ï£¬
ï£¬
ï£­
Î²0
Î²1
...
Î²nâˆ’1
ï£¶
ï£·
ï£·
ï£¸
nÃ—1
,
then the vector
a âŠ™b=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î±0Î²0
Î±0Î²1 + Î±1Î²0
Î±0Î²2 + Î±1Î²1 + Î±2Î²0
...
Î±nâˆ’2Î²nâˆ’1 + Î±nâˆ’1Î²nâˆ’2
Î±nâˆ’1Î²nâˆ’1
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
2nÃ—1
(5.8.10)
is called the convolution of a and b. The 0 in the last position is for con-
venience onlyâ€”it makes the size of the convolution twice the size of the origi-
nal vectors, and this provides a balance in some of the formulas involving con-
volution. Furthermore, it is sometimes convenient to pad a and b with n
additional zeros to consider them to be vectors with 2n components. Setting
Î±n = Â· Â· Â· = Î±2nâˆ’1 = Î²n = Â· Â· Â· = Î²2nâˆ’1 = 0 allows us to write the kth entry in
a âŠ™b as
[a âŠ™b]k =
k

j=0
Î±jÎ²kâˆ’j
for
k = 0, 1, 2, . . . , 2n âˆ’1.
A visual way to form a âŠ™b is to â€œslideâ€ the reversal of b â€œagainstâ€ a as
depicted in Figure 5.8.9, and then sum the resulting products.
Î±0
Î±1
...
Î±nâˆ’1
Ã—
Î²nâˆ’1
...
Î²1
Î²0
Î±0
Î±1
...
Î±nâˆ’1
Ã—
Ã—
Î²nâˆ’1
...
Î²1
Î²0
Î±0
Î±1
Î±2
...
Î±nâˆ’1
Ã—
Ã—
Ã—
Î²nâˆ’1
...
Î²2
Î²1
Î²0
Â· Â· Â·
Î±0
...
Î±nâˆ’2
Î±nâˆ’1
Ã—
Ã—
Î²nâˆ’1
Î²nâˆ’2
...
Î²0
Î±0
...
Î±nâˆ’2
Î±nâˆ’1 Ã—Î²nâˆ’1
Î²nâˆ’2
...
Î²0
Figure 5.8.9
The convolution operation is a natural occurrence in a variety of situations,
and polynomial multiplication is one such example.

5.8 Discrete Fourier Transform
367
Example 5.8.4
Polynomial Multiplication. For p(x) = nâˆ’1
k=0 Î±kxk, q(x) = nâˆ’1
k=0 Î²kxk, let
a = ( Î±0
Î±1
Â· Â· Â·
Î±nâˆ’1 )T
and b = ( Î²0
Î²1
Â· Â· Â·
Î²nâˆ’1 )T . The product
p(x)q(x) = Î³0 + Î³1x + Î³2x2 + Â· Â· Â· + Î³2nâˆ’2x2nâˆ’2 is a polynomial of degree 2n âˆ’2
in which Î³k is simply the kth component of the convolution a âŠ™b because
p(x)q(x) =
2nâˆ’2

k=0
ï£®
ï£°
k

j=0
Î±jÎ²kâˆ’j
ï£¹
ï£»xk =
2nâˆ’2

k=0
[a âŠ™b]kxk.
(5.8.11)
In other words, polynomial multiplication and convolution are equivalent opera-
tions, so if we can devise an eï¬ƒcient way to perform a convolution, then we can
eï¬ƒciently multiply two polynomials, and conversely.
There are two facets involved in eï¬ƒciently performing a convolution. The
ï¬rst is the realization that the discrete Fourier transform has the ability to
convert a convolution into an ordinary product, and vice versa. The second is
the realization that itâ€™s possible to devise a fast algorithm to compute a discrete
Fourier transform. These two facets are developed below.
Convolution Theorem
Let a Ã— b denote the entry-by-entry product
a Ã— b =
ï£«
ï£¬
ï£¬
ï£­
Î±0
Î±1
...
Î±nâˆ’1
ï£¶
ï£·
ï£·
ï£¸Ã—
ï£«
ï£¬
ï£¬
ï£­
Î²0
Î²1
...
Î²nâˆ’1
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
Î±0Î²0
Î±1Î²1
...
Î±nâˆ’1Î²nâˆ’1
ï£¶
ï£·
ï£·
ï£¸
nÃ—1
,
and let Ë†a and Ë†b be the padded vectors
Ë†a =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î±0
...
Î±nâˆ’1
0
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
2nÃ—1
and
Ë†b =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î²0
...
Î²nâˆ’1
0
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
2nÃ—1
.
If F = F2n is the Fourier matrix of order 2n, then
F(a âŠ™b) = (FË†a) Ã— (FË†b) and a âŠ™b = Fâˆ’1
(FË†a) Ã— (FË†b)

.
(5.8.12)

368
Chapter 5
Norms, Inner Products, and Orthogonality
Proof.
Observe that the tth component in Fâˆ—j Ã— Fâˆ—k is
[Fâˆ—j Ã— Fâˆ—k]t = Î¾tjÎ¾tk = Î¾t(j+k) = [Fâˆ—j+k]t ,
so that the columns of F have the property that
Fâˆ—j Ã— Fâˆ—k = Fâˆ—j+k
for each
j, k = 0, 1, . . . , (n âˆ’1).
This means that if FË†a, FË†b, and F(a âŠ™b) are expressed as combinations of
columns of F as indicated below,
FË†a =
nâˆ’1

k=0
Î±kFâˆ—k,
FË†b =
nâˆ’1

k=0
Î²kFâˆ—k,
and
F(a âŠ™b) =
2nâˆ’2

k=0
[a âŠ™b]kFâˆ—k,
then the computation of (FË†a)Ã—(FË†b) is exactly the same as forming the product
of two polynomials in the sense that
(FË†a) Ã— (FË†b) =
nâˆ’1

k=0
Î±kFâˆ—k
 nâˆ’1

k=0
Î²kFâˆ—k

=
2nâˆ’2

k=0
ï£®
ï£°
k

j=0
Î±jÎ²kâˆ’j
ï£¹
ï£»Fâˆ—k
=
2nâˆ’2

k=0
[a âŠ™b]kFâˆ—k = F(a âŠ™b).
According to the convolution theorem, the convolution of two n Ã— 1 vectors
can be computed by executing three discrete Fourier transforms of order 2n
anÃ—1 âŠ™bnÃ—1 = Fâˆ’1
2n

(F2nË†a) Ã— (F2nË†b)

.
(5.8.13)
The fact that one of them is an inverse transform is not a source of diï¬ƒcultyâ€”
recall Example 5.8.2. But it is still not clear that much has been accomplished.
Performing a convolution by following the recipe called for in deï¬nition (5.8.10)
requires n2 scalar multiplications (you are asked to verify this in the exercises).
Performing a discrete Fourier transform of order 2n by standard matrixâ€“vector
multiplication requires 4n2 scalar multiplications, so using matrixâ€“vector multi-
plication to perform the computations on the right-hand side of (5.8.13) requires
at least 12 times the number of scalar multiplications demanded by the deï¬nition
of convolution. So, if there is an advantage to be gained by using the convolution
theorem, then it is necessary to be able to perform a discrete Fourier transform
in far fewer scalar multiplications than that required by standard matrixâ€“vector
multiplication. It was not until 1965 that this hurdle was overcome. Two Ameri-
cans, J. W. Cooley and J. W. Tukey, introduced a fast Fourier transform (FFT)
algorithm that requires only on the order of (n/2) log2 n scalar multiplications
to compute Fnx. Using the FFT together with the convolution theorem requires

5.8 Discrete Fourier Transform
369
only about 3n log2 n multiplications to perform a convolution of two n Ã— 1 vec-
tors, and when n is large, this is signiï¬cantly less than the n2 factor demanded
by the deï¬nition of convolution.
The magic of the fast Fourier transform algorithm emanates from the fact
that if n is a power of 2, then a discrete Fourier transform of order n can be
executed by performing two transforms of order n/2. To appreciate exactly how
this comes about, observe that when n = 2r we have

Î¾jn =

Î¾2jn/2 , so

1, Î¾, Î¾2, Î¾3, . . . , Î¾nâˆ’1
= the nth roots of unity
if and only if

1, Î¾2, Î¾4, Î¾6, . . . , Î¾nâˆ’2
= the (n/2)th roots of unity.
This means that the (j, k)-entries in the Fourier matrices Fn and Fn/2 are
[Fn]jk = Î¾jk
and
[Fn/2]jk = (Î¾2)jk = Î¾2jk.
(5.8.14)
If the columns of Fn are permuted so that columns with even subscripts are
listed before those with odd subscripts, and if PT
n is the corresponding permu-
tation matrix, then we can partition FnPT
n as
FnPT
n = [Fâˆ—0 Fâˆ—2 Â· Â· Â· Fâˆ—nâˆ’2 | Fâˆ—1 Fâˆ—3 Â· Â· Â· Fâˆ—nâˆ’1] =
 A n
2Ã—n
2
B n
2Ã—n
2
C n
2Ã—n
2
G n
2Ã—n
2

.
By using (5.8.14) together with the facts that
Î¾nk = 1
and
Î¾n/2 = cos 2Ï€(n/2)
n
âˆ’i sin 2Ï€(n/2)
n
= âˆ’1,
we see that the entries in A, B, C, and G are
Ajk = Fj,2k = Î¾2jk = [Fn/2]jk,
Bjk = Fj,2k+1 = Î¾j(2k+1) = Î¾jÎ¾2jk = Î¾j[Fn/2]jk,
Cjk = F n
2 +j, 2k = Î¾( n
2 +j)2k = Î¾nkÎ¾2jk = Î¾2jk = [Fn/2]jk,
Gjk = F n
2 +j, 2k+1 = Î¾( n
2 +j)(2k+1) = Î¾nkÎ¾n/2Î¾jÎ¾2jk = âˆ’Î¾jÎ¾2jt = âˆ’Î¾j[Fn/2]jk.
In other words, if Dn/2 is the diagonal matrix
Dn/2 =
ï£«
ï£¬
ï£¬
ï£­
1
Î¾ Î¾2
...
Î¾
n
2 âˆ’1
ï£¶
ï£·
ï£·
ï£¸,
then
FnPT
n =
 A(n/2)Ã—(n/2)
B(n/2)Ã—(n/2)
C(n/2)Ã—(n/2)
G(n/2)Ã—(n/2)

=
 Fn/2
Dn/2Fn/2
Fn/2
âˆ’Dn/2Fn/2

.
This fundamental feature of the discrete Fourier transform is summarized below.

370
Chapter 5
Norms, Inner Products, and Orthogonality
Decomposing the Fourier Matrix
If n = 2r, then
Fn =
 Fn/2
Dn/2Fn/2
Fn/2
âˆ’Dn/2Fn/2

Pn,
(5.8.15)
where
Dn/2 =
ï£«
ï£¬
ï£¬
ï£­
1
Î¾
Î¾2
...
Î¾
n
2 âˆ’1
ï£¶
ï£·
ï£·
ï£¸
contains half of the nth roots of unity and Pn is the â€œevenâ€“oddâ€ per-
mutation matrix deï¬ned by
PT
n = [e0 e2 e4 Â· Â· Â· enâˆ’2 | e1 e3 e5 Â· Â· Â· enâˆ’1] .
The decomposition (5.8.15) says that a discrete Fourier transform of order
n = 2r can be accomplished by two Fourier transforms of order n/2 = 2râˆ’1, and
this leads to the FFT algorithm. To get a feel for how the FFT works, consider
the case when n = 8, and proceed to â€œdivide and conquer.â€ If
x8 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
x0
x1
x2
x3
x4
x5
x6
x7
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
then
P8x8 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
x0
x2
x4
x6
x1
x3
x5
x7
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£­
x(0)
4
x(1)
4
ï£¶
ï£¸,
so
F8x8 =
 F4
D4F4
F4
âˆ’D4F4
 ï£«
ï£­x(0)
4
x(1)
4
ï£¶
ï£¸=
ï£«
ï£­F4x(0)
4
+ D4F4x(1)
4
F4x(0)
4
âˆ’D4F4x(1)
4
ï£¶
ï£¸.
(5.8.16)
But
P4x(0)
4
=
ï£«
ï£¬
ï£¬
ï£­
x0
x4
x2
x6
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£­
x(0)
2
x(1)
2
ï£¶
ï£¸
and
P4x(1)
4
=
ï£«
ï£¬
ï£¬
ï£­
x1
x5
x3
x7
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£­
x(2)
2
x(3)
2
ï£¶
ï£¸,

5.8 Discrete Fourier Transform
371
so
F4x(0)
4
=
 F2
D2F2
F2
âˆ’D2F2
 ï£«
ï£­x(0)
2
x(1)
2
ï£¶
ï£¸=
ï£«
ï£­F2x(0)
2
+ D2F2x(1)
2
F2x(0)
2
âˆ’D2F2x(1)
2
ï£¶
ï£¸
and
(5.8.17)
F4x(1)
4
=
 F2
D2F2
F2
âˆ’D2F2
 ï£«
ï£­x(2)
2
x(3)
2
ï£¶
ï£¸=
ï£«
ï£­F2x(2)
2
+ D2F2x(3)
2
F2x(2)
2
âˆ’D2F2x(3)
2
ï£¶
ï£¸.
Now, since F2 =
 1
1
1
âˆ’1

, it is a trivial matter to compute the terms
F2x(0)
2 ,
F2x(1)
2 ,
F2x(2)
2 ,
F2x(3)
2 .
Of course, to actually carry out the computation, we need to work backward
through the preceding sequence of steps. That is, we start with
Ëœx8 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
x(0)
2
x(1)
2
x(2)
2
x(3)
2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
x0
x4
x2
x6
x1
x5
x3
x7
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
(5.8.18)
and use (5.8.17) followed by (5.8.16) to work downward in the following tree.
F2x(0)
2
F2x(1)
2
â†˜
â†™
F4x(0)
4
â†˜
â†˜
F2x(2)
2
F2x(3)
2
â†˜
â†™
F4x(1)
4
â†™
â†™
F8x8
But there appears to be a snag. In order to work downward through this
tree, we cannot start directly with x8â€”we must start with the permutation Ëœx8
shown in (5.8.18). So how is this initial permutation determined? Looking back
reveals that the entries in Ëœx8 were obtained by ï¬rst sorting the xj â€™s into two
groupsâ€”the entries in the even positions were separated from those in the odd

372
Chapter 5
Norms, Inner Products, and Orthogonality
positions. Then each group was broken into two more groups by again separating
the entries in the even positions from those in the odd positions.

0
1
2
3
4
5
6
7

â†™
â†˜

0
2
4
6


1
3
5
7

â†™
â†˜
â†™
â†˜

0
4


2
6


1
5


3
7

(5.8.19)
In general, this evenâ€“odd sorting process (sometimes called a perfect shuï¬„e)
produces the permutation necessary to initiate the algorithm. A clever way to
perform a perfect shuï¬„e is to use binary representations and observe that the
ï¬rst level of sorting in (5.8.19) is determined according to whether the least
signiï¬cant bit is 0 or 1, the second level of sorting is determined by the second
least signiï¬cant bit, and so onâ€”this is illustrated in Table 5.8.1 for n = 8.
Table 5.8.1
Natural order
First level
Second level
0 â†”000
0 â†”000
0 â†”000
1 â†”001
2 â†”010
4 â†”100
2 â†”010
4 â†”100
2 â†”010
3 â†”011
6 â†”110
6 â†”110
4 â†”100
1 â†”001
1 â†”001
5 â†”101
3 â†”011
5 â†”101
6 â†”110
5 â†”101
3 â†”011
7 â†”111
7 â†”111
7 â†”111
But all intermediate levels in this sorting process can be eliminated because
something very nice occurs. Examination of the last column in Table 5.8.1 reveals
that the binary bits in the perfect shuï¬„e ordering are exactly the reversal of the
binary bits in the natural ordering. In other words,
â€¢
to generate the perfect shuï¬„e of the numbers 0, 1, 2, . . . , nâˆ’1, simply reverse
the bits in the binary representation of each number.
We can summarize the fast Fourier transform by the following implementa-
tion that utilizes array operations.
52
52
There are a variety of diï¬€erent ways to implement the FFT, and choosing a practical imple-
mentation frequently depends on the hardware being used as well as the application under
consideration. The FFT ranks high on the list of useful algorithms because it provides an ad-
vantage in a large variety of applications, and there are many more facets of the FFT than
those presented here (e.g., FFT when n is not a power of 2). In fact, there are entire texts
devoted to these issues, so the interested student need only go as far as the nearest library to
ï¬nd more details.

5.8 Discrete Fourier Transform
373
Fast Fourier Transform
For a given input vector x containing n = 2r components, the discrete
Fourier transform Fnx is the result of successively creating the following
arrays.
X1Ã—n â†âˆ’rev(x)
(bit reverse the subscripts)
For j = 0, 1, 2, 3, . . . , r âˆ’1
D â†âˆ’
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
eâˆ’Ï€i/2j
eâˆ’2Ï€i/2j
eâˆ’3Ï€i/2j
...
eâˆ’(2jâˆ’1)Ï€i/2j
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
2jÃ—1
(Half of the (2j+1)th roots of 1,
perhaps from a lookup table)
X(0) â†âˆ’

Xâˆ—0
Xâˆ—2
Xâˆ—4
Â· Â· Â·
Xâˆ—2râˆ’jâˆ’2
	
2jÃ—2râˆ’jâˆ’1
X(1) â†âˆ’

Xâˆ—1
Xâˆ—3
Xâˆ—5
Â· Â· Â·
Xâˆ—2râˆ’jâˆ’1
	
2jÃ—2râˆ’jâˆ’1
X â†âˆ’

 X(0) + D Ã— X(1)
X(0) âˆ’D Ã— X(1)

2j+1Ã—2râˆ’jâˆ’1

 Deï¬ne Ã— to mean
[D Ã— M]ij = dimij

Example 5.8.5
Problem: Perform the FFT on x =
ï£«
ï£­
x0
x1
x2
x3
ï£¶
ï£¸.
Solution: Start with X â†âˆ’rev(x) = ( x0
x2
x1
x3 ) .
For j = 0 :
D â†âˆ’(1)
(Half of the square roots of 1)
X(0) â†âˆ’( x0
x1 )
X(1) â†âˆ’( x2
x3 )
and
D Ã— X(1) â†âˆ’( x2
x3 )
X â†âˆ’

 X(0) + D Ã— X(1)
X(0) âˆ’D Ã— X(1)

=

 x0 + x2
x1 + x3
x0 âˆ’x2
x1 âˆ’x3


374
Chapter 5
Norms, Inner Products, and Orthogonality
For j = 1 :
D â†âˆ’

1
âˆ’i

(Half of the 4th roots of 1)
X(0) â†âˆ’

x0 + x2
x0 âˆ’x2

X(1) â†âˆ’

x1 + x3
x1 âˆ’x3

and
D Ã— X(1) â†âˆ’

x1 + x3
âˆ’ix1 + ix3

X â†âˆ’
 X(0) + D Ã— X(1)
X(0) âˆ’D Ã— X(1)

=
ï£«
ï£¬
ï£­
x0 + x2 + x1 + x3
x0 âˆ’x2 âˆ’ix1 + ix3
x0 + x2 âˆ’x1 âˆ’x3
x0 âˆ’x2 + ix1 âˆ’ix3
ï£¶
ï£·
ï£¸= F4x
Notice that this agrees with the result obtained by using direct matrixâ€“vector
multiplication with F4 given in Example 5.8.1.
To understand why it is called the â€œfastâ€ Fourier transform, simply count
the number of multiplications the FFT requires. Observe that the jth iteration
requires 2j multiplications for each column in X(1), and there are 2râˆ’jâˆ’1
columns, so 2râˆ’1 multiplications are used for each iteration.
53 Since r iterations
are required, the total number of multiplications used by the FFT does not exceed
2râˆ’1r = (n/2) log2 n.
FFT Multiplication Count
If n is a power of 2, then applying the FFT to a vector of n components
requires at most (n/2) log2 n multiplications.
The (n/2) log2 n count represents a tremendous advantage over the n2
factor demanded by a direct matrixâ€“vector product. To appreciate the magnitude
of the diï¬€erence between n2 and (n/2) log2 n, look at Figure 5.8.10.
53
Actually, we can get by with slightly fewer multiplications if we take advantage of the fact that
the ï¬rst entry in D is always 1 and if we observe that no multiplications are necessary when
j = 0. But when n is large, these savings are relatively insigniï¬cant, so they are ignored in
the multiplication count.

5.8 Discrete Fourier Transform
375
f(n) = n2
512
256
128
0
0
100000
200000
300000
n
f(n) = (n/2) log2n
Figure 5.8.10
The small dark portion at the bottom of the graph is the area under the curve
f(n) = (n/2) log2 nâ€”it is tiny in comparison to the area under f(n) = n2.
For example, if n = 512, then n2 = 262, 144, but (n/2) log2 n = 2304. In
other words, for n = 512, the FFT is on the order of 100 times faster than
straightforward matrixâ€“vector multiplication, and for larger values of n, the
gap is even widerâ€”Figure 5.8.10 illustrates just how fast the diï¬€erence between
n2 and (n/2) log2 n grows as n increases. Since Cooley and Tukey introduced
the FFT in 1965, it has risen to a position of fundamental importance. The FFT
and the convolution theorem are extremely powerful tools, and they have been
principal components of the computational revolution that now touches our lives
countless times each day.
Example 5.8.6
Problem: Fast Integer Multiplication. Consider two positive integers whose
base-b representations are
c = (Î³nâˆ’1Î³nâˆ’2 Â· Â· Â· Î³1Î³0)b
and
d = (Î´nâˆ’1Î´nâˆ’2 Â· Â· Â· Î´1Î´0)b.
Use the convolution theorem together with the FFT to compute the product cd.
Solution: If we let
p(x) =
nâˆ’1

k=0
Î³kxk,
q(x) =
nâˆ’1

k=0
Î´kxk,
c =
ï£«
ï£¬
ï£¬
ï£­
Î³0
Î³1
...
Î³nâˆ’1
ï£¶
ï£·
ï£·
ï£¸,
and
d =
ï£«
ï£¬
ï£¬
ï£­
Î´0
Î´1
...
Î´nâˆ’1
ï£¶
ï£·
ï£·
ï£¸,

376
Chapter 5
Norms, Inner Products, and Orthogonality
then
c = Î³nâˆ’1bnâˆ’1 + Î³nâˆ’2bnâˆ’2 + Â· Â· Â· + Î³1b1 + Î³0b0 = p(b),
d = Î´nâˆ’1bnâˆ’1 + Î´nâˆ’2bnâˆ’2 + Â· Â· Â· + Î´1b1 + Î´0b0 = q(b),
and it follows from (5.8.11) that the product cd is given by
cd = p(b)q(b) = [câŠ™d]2nâˆ’2b2nâˆ’2 +[câŠ™d]2nâˆ’3b2nâˆ’3 +Â· Â· Â·+[câŠ™d]1b1 +[câŠ™d]0b0.
It looks as though the convolution c âŠ™d provides the base-b representation for
cd, but this is not quite the case because it is possible to have some [câŠ™d]k â‰¥b.
For example, if c = 20110 and d = 42510, then
c âŠ™d =
ï£«
ï£­
1
0
2
ï£¶
ï£¸âŠ™
ï£«
ï£­
5
2
4
ï£¶
ï£¸=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
5
2
14
4
8
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
so
cd = (8Ã—104) + (4Ã—103) + (14Ã—102) + (2Ã—101) + (5Ã—100).
(5.8.20)
But when numbers like 14 (i.e., greater than or equal to the base) appear in câŠ™d,
it is relatively easy to decompose them by writing 14 = (1Ã—101) + (4Ã—100), so
14Ã—102 =

(1Ã—101) + (4Ã—100)

Ã—102 = (1Ã—103) + (4Ã—102).
Substituting this in (5.8.20) and combining coeï¬ƒcients of like powers produces
the base-10 representation of the product
cd = (8Ã—104) + (5Ã—103) + (4Ã—102) + (2Ã—101) + (5Ã—100) = 8542510.
Computing c âŠ™d directly demands n2 multiplications, but using the FFT in
conjunction with the convolution theorem requires only about 3n log2 n mul-
tiplications, which is considerably less than n2 for large values of n. Thus it
is possible to multiply very long base-b integers much faster than by using di-
rect methods. Most digital computers have binary integer multiplication (usually
64-bit multiplication not requiring the FFT) built into their hardware, but for
ultra-high-precision multiplication or for more general base-b multiplication, the
FFT is a viable tool.
Exercises for section 5.8
5.8.1. Evaluate the following convolutions.
(a)
ï£«
ï£­
1
2
3
ï£¶
ï£¸âŠ™
ï£«
ï£­
4
5
6
ï£¶
ï£¸.
(b)
ï£«
ï£­
âˆ’1
0
1
ï£¶
ï£¸âŠ™
ï£«
ï£­
1
0
âˆ’1
ï£¶
ï£¸.
(c)
ï£«
ï£­
1
1
1
ï£¶
ï£¸âŠ™
ï£«
ï£­
Î±0
Î±1
Î±2
ï£¶
ï£¸.

5.8 Discrete Fourier Transform
377
5.8.2.
(a)
Evaluate the discrete Fourier transform of
ï£«
ï£¬
ï£­
1
âˆ’i
âˆ’1
i
ï£¶
ï£·
ï£¸.
(b)
Evaluate the inverse transform of
ï£«
ï£¬
ï£­
1
i
âˆ’1
âˆ’i
ï£¶
ï£·
ï£¸.
5.8.3. Verify directly that F4 =
 F2
D2F2
F2
âˆ’D2F2

P4, where the F4, P4, and
D2, are as deï¬ned in (5.8.15).
5.8.4. Use the following vectors to perform the indicated computations:
a =
 Î±0
Î±1

,
b =
 Î²0
Î²1

,
Ë†a =
ï£«
ï£¬
ï£­
Î±0
Î±1
0
0
ï£¶
ï£·
ï£¸,
Ë†b =
ï£«
ï£¬
ï£­
Î²0
Î²1
0
0
ï£¶
ï£·
ï£¸.
(a)
Compute a âŠ™b, F4(a âŠ™b), and (F4Ë†a) Ã— (F4Ë†b).
(b)
By using Fâˆ’1
4
as given in Example 5.8.1, compute
Fâˆ’1
4

(F4Ë†a) Ã— (F4Ë†b)

.
Compare this with the results guaranteed by the convolution
theorem.
5.8.5. For p(x) = 2x âˆ’3 and q(x) = 3x âˆ’4, compute the product p(x)q(x)
by using the convolution theorem.
5.8.6. Use convolutions to form the following products.
(a)
4310 Ã— 2110.
(b)
1238 Ã— 6018.
(c)
10102 Ã— 11012.
5.8.7. Let a and b be n Ã— 1 vectors, where n is a power of 2.
(a)
Show that the number of multiplications required to form aâŠ™b
by using the deï¬nition of convolution is n2.
Hint: 1 + 2 + Â· Â· Â· + k = k(k + 1)/2.
(b)
Show that the number of multiplications required to form aâŠ™b
by using the FFT in conjunction with the convolution theorem
is 3n log2 n + 7n. Sketch a graph of 3n log2 n (the 7n factor is
dropped because it is not signiï¬cant), and compare it with the
graph of n2 to illustrate why the FFT in conjunction with the
convolution theorem provides such a big advantage.

378
Chapter 5
Norms, Inner Products, and Orthogonality
5.8.8. A waveform given by a ï¬nite sum
x(Ï„) =

k
(Î±k cos 2Ï€fkÏ„ + Î²k sin 2Ï€fkÏ„)
in which the fk â€™s are integers and max{fk} â‰¤3 is sampled at eight
equally spaced points between Ï„ = 0 and Ï„ = 1. Let
x =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
x(0/8)
x(1/8)
x(2/8)
x(3/8)
x(4/8)
x(5/8)
x(6/8)
x(7/8)
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
and suppose that
y = 1
4F8x =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
âˆ’5i
1 âˆ’3i
4
0
4
1 + 3i
5i
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
What is the equation of the waveform?
5.8.9. Prove that a âŠ™b = b âŠ™a for all a, b âˆˆCnâ€”i.e., convolution is a
commutative operation.
5.8.10. For p(x) = nâˆ’1
k=0 Î±kxk and the nth roots of unity Î¾k, let
a = ( Î±0 Î±1 Î±2 Â· Â· Â· Î±nâˆ’1 )T
and
p = ( p(1) p(Î¾) p(Î¾2) Â· Â· Â· p(Î¾nâˆ’1) )T .
Explain why Fna = p and a = Fâˆ’1
n p. This says that the discrete
Fourier transform allows us to go from the representation of a polynomial
p in terms of its coeï¬ƒcients Î±k to the representation of p in terms of its
values p(Î¾k), and the inverse transform takes us in the other direction.
5.8.11. For two polynomials p(x) = nâˆ’1
k=0 Î±kxk and q(x) = nâˆ’1
k=0 Î²kxk, let
p =
ï£«
ï£¬
ï£¬
ï£­
p(1)
p(Î¾)
...
p(Î¾2nâˆ’1)
ï£¶
ï£·
ï£·
ï£¸
and
q =
ï£«
ï£¬
ï£¬
ï£­
q(1)
q(Î¾)
...
q(Î¾2nâˆ’1)
ï£¶
ï£·
ï£·
ï£¸,
where

1, Î¾, Î¾2, . . . , Î¾2nâˆ’1
are now the 2nth roots of unity. Explain
why the coeï¬ƒcients in the product
p(x)q(x) = Î³0 + Î³1x + Î³2x2 + Â· Â· Â· + Î³2nâˆ’2x2nâˆ’2
must be given by
ï£«
ï£¬
ï£¬
ï£­
Î³0
Î³1
Î³2
...
ï£¶
ï£·
ï£·
ï£¸= Fâˆ’1
2n
ï£«
ï£¬
ï£¬
ï£­
p(1)q(1)
p(Î¾)q(Î¾)
p(Î¾2)q(Î¾2)
...
ï£¶
ï£·
ï£·
ï£¸.
This says that the product p(x)q(x) is completely determined by the
values of p(x) and q(x) at the 2nth roots of unity.

5.8 Discrete Fourier Transform
379
5.8.12. A circulant matrix is deï¬ned to be a square matrix that has the form
C =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
c0
cnâˆ’1
cnâˆ’2
Â· Â· Â·
c1
c1
c0
cnâˆ’1
Â· Â· Â·
c2
c2
c1
c0
Â· Â· Â·
c3
...
...
...
...
...
cnâˆ’1
cnâˆ’2
cnâˆ’3
Â· Â· Â·
c0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
nÃ—n
.
In other words, the entries in each column are the same as the previous
column, but they are shifted one position downward and wrapped around
at the topâ€”the (j, k)-entry in C can be described as cjk = cjâˆ’k (mod n).
(Some authors use CT
rather than C as the deï¬nitionâ€”it doesnâ€™t
matter.)
(a)
If Q is the circulant matrix deï¬ned by
Q =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
0
Â· Â· Â·
0
1
1
0
Â· Â· Â·
0
0
0
1
Â· Â· Â·
0
0
...
...
...
...
...
0
0
Â· Â· Â·
1
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
nÃ—n
,
and if p(x) = c0 + c1x + Â· Â· Â· + cnâˆ’1xnâˆ’1, verify that
C = p(Q) = c0I + c1Q + Â· Â· Â· + cnâˆ’1Qnâˆ’1.
(b)
Explain why the Fourier matrix of order n diagonalizes Q in
the sense that
FQFâˆ’1 = D =
ï£«
ï£¬
ï£¬
ï£­
1
0
Â· Â· Â·
0
0
Î¾
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
Î¾nâˆ’1
ï£¶
ï£·
ï£·
ï£¸,
where the Î¾k â€™s are the nth roots of unity.
(c)
Prove that the Fourier matrix of order n diagonalizes every
n Ã— n circulant in the sense that
FCFâˆ’1 =
ï£«
ï£¬
ï£¬
ï£­
p(1)
0
Â· Â· Â·
0
0
p(Î¾)
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
p(Î¾nâˆ’1)
ï£¶
ï£·
ï£·
ï£¸,
where p(x) = c0 + c1x + Â· Â· Â· + cnâˆ’1xnâˆ’1.
(d)
If C1 and C2 are any pair of n Ã— n circulants, explain why
C1C2 = C2C1â€”i.e., all circulants commute with each other.

380
Chapter 5
Norms, Inner Products, and Orthogonality
5.8.13. For a nonsingular circulant CnÃ—n, explain how to use the FFT algo-
rithm to eï¬ƒciently perform the following operations.
(a)
Solve a system Cx = b.
(b)
Compute Câˆ’1.
(c)
Multiply two circulants C1C2.
5.8.14. For the vectors
a=
ï£«
ï£¬
ï£¬
ï£­
Î±0
Î±1
...
Î±nâˆ’1
ï£¶
ï£·
ï£·
ï£¸, b=
ï£«
ï£¬
ï£¬
ï£­
Î²0
Î²1
...
Î²nâˆ’1
ï£¶
ï£·
ï£·
ï£¸, Ë†a=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î±0
...
Î±nâˆ’1
0
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
2nÃ—1
, and Ë†b=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î²0
...
Î²nâˆ’1
0
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
2nÃ—1
,
let C be the 2n Ã— 2n circulant matrix (see Exercise 5.8.12) whose ï¬rst
column is Ë†a.
(a)
Show that the convolution operation can be described as a
matrixâ€“vector product by demonstrating that
a âŠ™b = CË†b.
(b)
Use this relationship to give an alternate proof of the convolu-
tion theorem. Hint: Use the diagonalization result of Exercise
5.8.12 together with the result of Exercise 5.8.10.
5.8.15. The Kronecker product of two matrices AmÃ—n and BpÃ—q is deï¬ned
to be the mp Ã— nq matrix
A âŠ—B =
ï£«
ï£¬
ï£¬
ï£­
a11B
a12B
Â· Â· Â·
a1nB
a21B
a22B
Â· Â· Â·
a2nB
...
...
...
...
am1B
am2B
Â· Â· Â·
amnB
ï£¶
ï£·
ï£·
ï£¸.
This is also known as the tensor product or the direct product. Al-
though there is an extensive list of properties that the tensor product
satisï¬es, this exercise requires only the following two elementary facts
(which you need not prove unless you feel up to it). The complete list of
properties is given in Exercise 7.8.11 (p. 597) along with remarks about
Kronecker, and another application appears in Exercise 7.6.10 (p. 573).
A âŠ—(B âŠ—C) = (A âŠ—B) âŠ—C.
(AâŠ—B)(CâŠ—D) = ACâŠ—BD (if AC and BD exist).

5.8 Discrete Fourier Transform
381
(a)
If n = 2r, and if Pn is the evenâ€“odd permutation matrix
described in (5.8.15), explain why
Rn = (I2râˆ’1 âŠ—P21)(I2râˆ’2 âŠ—P22) Â· Â· Â· (I21 âŠ—P2râˆ’1)(I20 âŠ—P2r)
is the permutation matrix associated with the bit reversing (or
perfect shuï¬„e) permutation described in (5.8.19) and Table
5.8.1. Hint: Work it out for n = 8 by showing
R8
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
x0
x1
x2
x3
x4
x5
x6
x7
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
x0
x4
x2
x6
x1
x5
x3
x7
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
and you will see why it holds in general.
(b)
Suppose n = 2r, and set
Bn =
 In/2
Dn/2
In/2
âˆ’Dn/2

.
According to (5.8.15), the Fourier matrix can be written as
Fn = Bn(I2 âŠ—Fn/2)Pn.
Expand on this idea by proving that Fn can be factored as
Fn = LnRn
in which
Ln = (I20 âŠ—B2r)(I21 âŠ—B2râˆ’1) Â· Â· Â· (I2râˆ’2 âŠ—B22)(I2râˆ’1 âŠ—B21),
and where Rn is the bit reversing permutation
Rn = (I2râˆ’1 âŠ—P21)(I2râˆ’2 âŠ—P22) Â· Â· Â· (I21 âŠ—P2râˆ’1)(I20 âŠ—P2r).
Notice that this says Fnx = LnRnx, so the discrete Fourier
transform of x is obtained by ï¬rst performing the bit revers-
ing permutation to x followed by r applications of the terms
(I2râˆ’k âŠ—B2k) from Ln. This in fact is the FFT algorithm in
factored form. Hint: Deï¬ne two sequences by the rules
L2k = (I2râˆ’k âŠ—B2k) L2kâˆ’1
and
R2k = R2kâˆ’1 (I2râˆ’k âŠ—P2k) ,
where
L1 = 1,
R1 = In,
B2 = F2,
P2 = I2,
and use induction on k to prove that
I2râˆ’k âŠ—F2k = L2kR2k
for
k = 1, 2, . . . , r.

382
Chapter 5
Norms, Inner Products, and Orthogonality
5.8.16. For p(x) = Î±0 + Î±1x + Î±2x2 + Â· Â· Â· + Î±nâˆ’1xnâˆ’1, prove that
1
n
nâˆ’1

k=0
p(Î¾k)
2 = |Î±0|2 + |Î±1|2 + Â· Â· Â· + |Î±nâˆ’1|2,
where

1, Î¾, Î¾2, . . . , Î¾nâˆ’1
are the nth roots of unity.
5.8.17. Consider a waveform that is given by the ï¬nite sum
x(Ï„) =

k
(Î±k cos 2Ï€fkÏ„ + Î²k sin 2Ï€fkÏ„)
in which the fk â€™s are distinct integers, and let
x =

k
(Î±k cos 2Ï€fkt + Î²k sin 2Ï€fkt)
be the vector containing the values of x(Ï„) at n > 2 max{fk} equally
spaced points between Ï„ = 0 and Ï„ = 1 as described in Example 5.8.3.
Use the discrete Fourier transform to prove that
âˆ¥xâˆ¥2
2 = n
2

k

Î±2
k + Î²2
k

.
5.8.18. Let Î· be an arbitrary scalar, and let
c =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
Î·
Î·2
...
Î·2nâˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
and
a =
ï£«
ï£¬
ï£¬
ï£­
Î±0
Î±1
...
Î±nâˆ’1
ï£¶
ï£·
ï£·
ï£¸.
Prove that cT (a âŠ™a) =

cT Ë†a
2 .
5.8.19. Apply the FFT algorithm to the vector x8 =
ï£«
ï£¬
ï£­
x0
x1
...
x7
ï£¶
ï£·
ï£¸, and then verify
that your answer agrees with the result obtained by computing F8x8
directly.

5.9 Complementary Subspaces
383
5.9
COMPLEMENTARY SUBSPACES
The sum of two subspaces X and Y of a vector space V was deï¬ned on p. 166
to be the set X + Y = {x + y | x âˆˆX and y âˆˆY}, and it was established that
X + Y is another subspace of V. For example, consider the two subspaces of
â„œ3 shown in Figure 5.9.1 in which X is a plane through the origin, and Y is a
line through the origin.
Figure 5.9.1
Notice that X and Y are disjoint in the sense that X âˆ©Y = 0. The paral-
lelogram law for vector addition makes it clear that X + Y = â„œ3 because each
vector in â„œ3 can be written as â€œsomething from X plus something from Y. â€
Thus â„œ3 is resolved into a pair of disjoint components X and Y. These ideas
generalize as described below.
Complementary Subspaces
Subspaces X, Y of a space V are said to be complementary whenever
V = X + Y
and
X âˆ©Y = 0,
(5.9.1)
in which case V is said to be the direct sum of X and Y, and this is
denoted by writing V = X âŠ•Y.
â€¢
For a vector space V with subspaces X, Y having respective bases
BX and BY, the following statements are equivalent.
â–·
V = X âŠ•Y.
(5.9.2)
â–·
For each v âˆˆV there are unique vectors x âˆˆX and y âˆˆY
such that v = x + y.
(5.9.3)
â–·
BX âˆ©BY = Ï† and BX âˆªBY is a basis for V.
(5.9.4)

384
Chapter 5
Norms, Inner Products, and Orthogonality
Prove these by arguing (5.9.2)
=â‡’
(5.9.3)
=â‡’
(5.9.4)
=â‡’
(5.9.2).
Proof of (5.9.2) =â‡’(5.9.3).
First recall from (4.4.19) that
dim V = dim (X + Y) = dim X + dim Y âˆ’dim (X âˆ©Y) .
If V = X âŠ•Y, then X âˆ©Y = 0, and thus dim V = dim X + dim Y. To prove
(5.9.3), suppose there are two ways to represent a vector v âˆˆV as â€œsomething
from X plus something from Y. â€ If v = x1 + y1 = x2 + y2, where x1, x2 âˆˆX
and y1, y2 âˆˆY, then
x1 âˆ’x2 = y2 âˆ’y1
=â‡’
ï£±
ï£²
ï£³
x1 âˆ’x2 âˆˆX
and
x1 âˆ’x2 âˆˆY
ï£¼
ï£½
ï£¾
=â‡’
x1 âˆ’x2 âˆˆX âˆ©Y.
But X âˆ©Y = 0, so x1 = x2 and y1 = y2.
Proof of (5.9.3) =â‡’(5.9.4).
The hypothesis insures that V = X + Y, and we
know from (4.1.2) that BX âˆªBY spans X +Y, so BX âˆªBY must be a spanning
set for V. To prove BX âˆªBY is linearly independent, let BX = {x1, x2, . . . , xr}
and BY = {y1, y2, . . . , ys} , and suppose that
0 =
r

i=1
Î±ixi +
s

j=1
Î²jyj.
This is one way to express 0 as â€œsomething from X plus something from Y, â€
while 0 = 0 + 0 is another way. Consequently, (5.9.3) guarantees that
r

i=1
Î±ixi = 0
and
s

j=1
Î²jyj = 0,
and hence Î±1 = Î±2 = Â· Â· Â· = Î±r = 0 and Î²1 = Î²2 = Â· Â· Â· = Î²s = 0 because
BX
and BY are both linearly independent. Therefore, BX âˆªBY is linearly
independent, and hence it is a basis for V.
Proof of (5.9.4) =â‡’(5.9.2).
If BX âˆªBY is a basis for V, then BX âˆªBY is a
linearly independent set. This together with the fact that BX âˆªBY always spans
X + Y means BX âˆªBY is a basis for X + Y as well as for V. Consequently,
V = X + Y, and hence
dim X + dim Y = dim V = dim(X + Y) = dim X + dim Y âˆ’dim (X âˆ©Y) ,
so dim (X âˆ©Y) = 0 or, equivalently, X âˆ©Y = 0.
If V = X âŠ•Y, then (5.9.3) says there is one and only one way to resolve
each v âˆˆV into an â€œX -componentâ€ and a â€œY-componentâ€ so that v = x + y.
These two components of v have a deï¬nite geometrical interpretation. Look
back at Figure 5.9.1 in which â„œ3 = X âŠ•Y, where X is a plane and Y is a line
outside the plane, and notice that x (the X -component of v ) is the result of
projecting v onto X along a line parallel to Y, and y (the Y-component of
v ) is obtained by projecting v onto Y along a line parallel to X. This leads
to the following formal deï¬nition of a projection.

5.9 Complementary Subspaces
385
Projection
Suppose that V = X âŠ•Y so that for each v âˆˆV there are unique
vectors x âˆˆX and y âˆˆY such that v = x + y.
â€¢
The vector x is called the projection of v onto X along Y.
â€¢
The vector y is called the projection of v onto Y along X.
Itâ€™s clear that if X âŠ¥Y in Figure 5.9.1, then this notion of projection
agrees with the concept of orthogonal projection that was discussed on p. 322.
The phrase â€œoblique projectionâ€ is sometimes used to emphasize the fact that
X and Y are not orthogonal subspaces. In this text the word â€œprojectionâ€ is
synonymous with the term â€œoblique projection.â€ If it is known that X âŠ¥Y, then
we explicitly say â€œorthogonal projection.â€ Orthogonal projections are discussed
in detail on p. 429.
Given a pair of complementary subspaces X and Y of â„œn and an arbitrary
vector v âˆˆâ„œn = X âŠ•Y, how can the projection of v onto X be computed?
One way is to build a projector (a projection operator) that is a matrix PnÃ—n
with the property that for each v âˆˆâ„œn, the product Pv is the projection of
v onto X along Y. Let BX = {x1, x2, . . . , xr} and BY = {y1, y2, . . . , ynâˆ’r}
be respective bases for X and Y so that BX âˆªBY is a basis for â„œnâ€”recall
(5.9.4). This guarantees that if the xi â€™s and yi â€™s are placed as columns in
BnÃ—n =

x1 x2 Â· Â· Â· xr | y1 y2 Â· Â· Â· ynâˆ’r

=

XnÃ—r | YnÃ—(nâˆ’r)

,
then B is nonsingular. If PnÃ—n is to have the property that Pv is the pro-
jection of v onto X
along Y for every v âˆˆâ„œn, then (5.9.3) implies that
Pxi = xi, i = 1, 2, . . . , r and Pyj = 0, j = 1, 2, . . . , n âˆ’r, so
PB = P

X | Y

=

PX | PY

=

X | 0

and, consequently,
P =

X | 0

Bâˆ’1 = B

Ir
0
0
0

Bâˆ’1.
(5.9.5)
To argue that Pv is indeed the projection of v onto X along Y, set x = Pv
and y = (I âˆ’P)v and observe that v = x + y, where
x = Pv =

X | 0

Bâˆ’1v âˆˆR (X) = X
(5.9.6)
and
y = (I âˆ’P)v = B

0
0
0
Inâˆ’r

Bâˆ’1v =

0 | Y

Bâˆ’1v âˆˆR (Y) = Y.
(5.9.7)

386
Chapter 5
Norms, Inner Products, and Orthogonality
Is it possible that there can be more than one projector onto X
along
Y ? No, P is unique because if P1 and P2 are two such projectors, then for
i = 1, 2, we have PiB = Pi

X | Y

=

PiX | PiY

=

X | 0

, and this implies
P1B = P2B, which means P1 = P2. Therefore, (5.9.5) is the projector onto
X along Y, and this formula for P is independent of which pair of bases for X
and Y is selected. Notice that the argument involving (5.9.6) and (5.9.7) also
establishes that the complementary projectorâ€”the projector onto Y along
X â€”must be given by
Q = I âˆ’P =

0 | Y

Bâˆ’1 = B

0
0
0
Inâˆ’r

Bâˆ’1.
Below is a summary of the basic properties of projectors.
Projectors
Let X and Y be complementary subspaces of a vector space V so that
each v âˆˆV can be uniquely resolved as v = x + y, where x âˆˆX and
y âˆˆY. The unique linear operator P deï¬ned by Pv = x is called the
projector onto X along Y, and P has the following properties.
â€¢
P2 = P
( P is idempotent).
(5.9.8)
â€¢
I âˆ’P is the complementary projector onto Y along X.
(5.9.9)
â€¢
R (P) = {x | Px = x} (the set of â€œï¬xed pointsâ€ for P ).
(5.9.10)
â€¢
R (P) = N (I âˆ’P) = X and R (I âˆ’P) = N (P) = Y.
(5.9.11)
â€¢
If V = â„œn or Cn, then P is given by
P =

X | 0

X | Y
âˆ’1 =

X | Y
 
I
0
0
0
 
X | Y
âˆ’1,
(5.9.12)
where the columns of X and Y are respective bases for X and Y.
Other formulas for P are given on p. 634.
Proof.
Some of these properties have already been derived in the context of
â„œn. But since the concepts of projections and projectors are valid for all vector
spaces, more general arguments that do not rely on properties of â„œn will be
provided. Uniqueness is evident because if P1 and P2 both satisfy the deï¬ning
condition, then P1v = P2v for every v âˆˆV, and thus P1 = P2. The linearity
of P follows because if v1 = x1 +y1 and v2 = x2 +y2, where x1, x2 âˆˆX and
y1, y2 âˆˆY, then P(Î±v1 + v2) = Î±x1 + x2 = Î±Pv1 + Pv2. To prove that P is
idempotent, write
P2v = P(Pv) = Px = x = Pv for every v âˆˆV
=â‡’
P2 = P.

5.9 Complementary Subspaces
387
The validity of (5.9.9) is established by observing that v = x + y = Pv + y
implies y = v âˆ’Pv = (I âˆ’P)v. The properties in (5.9.11) and (5.9.10) are
immediate consequences of the deï¬nition. Formula (5.9.12) is the result of the
arguments that culminated in (5.9.5), but it can be more elegantly derived by
making use of the material in Â§4.7 and Â§4.8. If BX and BY are bases for X
and Y, respectively, then B = BX âˆªBY = {x1, x2, . . . , xr, y1, y2, . . . , ynâˆ’r} is
a basis for V, and (4.7.4) says that the matrix of P with respect to B is
[P]B =
1
[Px1]B
 Â· Â· Â·
 [Pxr]B
 [Py1]B
 Â· Â· Â·
 [Pynâˆ’r]B
2
=
1
[x1]B
 Â· Â· Â·
 [xr]B
 [0]B
 Â· Â· Â·
 [0]B
2
=
1
e1
 Â· Â· Â·
 er
 0
 Â· Â· Â·
 0
2
=

Ir
0
0
0

.
If S is the standard basis, then (4.8.5) says that [P]B = Bâˆ’1[P]SB in which
B = [I]BS =
1
[x1]S
 Â· Â· Â·
 [xr]S
 [y1]S Â· Â· Â·
 [ynâˆ’r]S
2
=

X | Y

,
and therefore [P]S = B[P]BBâˆ’1 =

X | Y
 Ir
0
0
0

X | Y
âˆ’1.
In the language of Â§4.8, statement (5.9.12) says that P is similar to the
diagonal matrix
 I
0
0
0

. In the language of Â§4.9, this means that P must be
the matrix representation of the linear operator that when restricted to X is
the identity operator and when restricted to Y is the zero operator.
Statement (5.9.8) says that if P is a projector, then P is idempotent
( P2 = P ). But what about the converseâ€”is every idempotent linear operator
necessarily a projector? The following theorem says, â€œYes.â€
Projectors and Idempotents
A linear operator P on V is a projector if and only if P2 = P. (5.9.13)
Proof.
The fact that every projector is idempotent was proven in (5.9.8). The
proof of the converse rests on the fact that
P2 = P
=â‡’
R (P) and N (P) are complementary subspaces.
(5.9.14)
To prove this, observe that V = R (P) + N (P) because for each v âˆˆV,
v = Pv + (I âˆ’P)v,
where
Pv âˆˆR (P) and (I âˆ’P)v âˆˆN (P).
(5.9.15)

388
Chapter 5
Norms, Inner Products, and Orthogonality
Furthermore, R (P) âˆ©N (P) = 0 because
x âˆˆR (P) âˆ©N (P)
=â‡’
x = Py and Px = 0
=â‡’
x = Py = P2y = 0,
and thus (5.9.14) is established. Now that we know R (P) and N (P) are com-
plementary, we can conclude that P is a projector because each v âˆˆV can be
uniquely written as v = x + y, where x âˆˆR (P) and y âˆˆN (P), and (5.9.15)
guarantees Pv = x.
Notice that there is a one-to-one correspondence between the set of idem-
potents (or projectors) deï¬ned on a vector space V and the set of all pairs of
complementary subspaces of V in the following sense.
â€¢
Each idempotent P deï¬nes a pair of complementary spacesâ€”namely, R (P)
and N (P).
â€¢
Every pair of complementary subspaces X and Y deï¬nes an idempotentâ€”
namely, the projector onto X along Y.
Example 5.9.1
Problem: Let X and Y be the subspaces of â„œ3 that are spanned by
BX =
ï£±
ï£²
ï£³
ï£«
ï£­
1
âˆ’1
âˆ’1
ï£¶
ï£¸,
ï£«
ï£­
0
1
âˆ’2
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
and
BY =
ï£±
ï£²
ï£³
ï£«
ï£­
1
âˆ’1
0
ï£¶
ï£¸
ï£¼
ï£½
ï£¾,
respectively. Explain why X and Y are complementary, and then determine
the projector onto X along Y. What is the projection of v = ( âˆ’2
1
3 )T
onto X along Y? What is the projection of v onto Y along X?
Solution: BX and BY are linearly independent, so they are bases for X and
Y, respectively. The spaces X and Y are complementary because
rank [X | Y] = rank
ï£«
ï£­
1
0
1
âˆ’1
1
âˆ’1
âˆ’1
âˆ’2
0
ï£¶
ï£¸= 3
insures that BX âˆªBY is a basis for â„œ3â€”recall (5.9.4). The projector onto X
along Y is obtained from (5.9.12) as
P =

X | 0

X | Y
âˆ’1 =
ï£«
ï£­
1
0
0
âˆ’1
1
0
âˆ’1
âˆ’2
0
ï£¶
ï£¸
ï£«
ï£­
âˆ’2
âˆ’2
âˆ’1
1
1
0
3
2
1
ï£¶
ï£¸=
ï£«
ï£­
âˆ’2
âˆ’2
âˆ’1
3
3
1
0
0
1
ï£¶
ï£¸.
You may wish to verify that P is indeed idempotent. The projection of v onto
X along Y is Pv, and, according to (5.9.9), the projection of v onto Y along
X is (I âˆ’P)v.

5.9 Complementary Subspaces
389
Example 5.9.2
Angle between Complementary Subspaces.
The angle between nonzero
vectors u and v in â„œn was deï¬ned on p. 295 to be the number 0 â‰¤Î¸ â‰¤
Ï€/2 such that cos Î¸ = vT u/ âˆ¥vâˆ¥2 âˆ¥uâˆ¥2 . Itâ€™s natural to try to extend this idea
to somehow make sense of angles between subspaces of â„œn. Angles between
completely general subspaces are presently out of our reachâ€”they are discussed
in Â§5.15â€”but the angle between a pair of complementary subspaces is within
our grasp. When â„œn = R âŠ•N with R Ì¸= 0 Ì¸= N, the angle (also known as the
minimal angle) between R and N is deï¬ned to be the number 0 < Î¸ â‰¤Ï€/2
that satisï¬es
cos Î¸ = max
uâˆˆR
vâˆˆN
vT u
âˆ¥vâˆ¥2 âˆ¥uâˆ¥2
=
max
uâˆˆR, vâˆˆN
âˆ¥uâˆ¥2=âˆ¥vâˆ¥2=1
vT u.
(5.9.16)
While this is a good deï¬nition, itâ€™s not easy to useâ€”especially if one wants to
compute the numerical value of cos Î¸. The trick in making Î¸ more accessible
is to think in terms of projections and sin Î¸ = (1 âˆ’cos2 Î¸)1/2. Let P be the
projector such that R (P) = R and N (P) = N, and recall that the matrix
2-norm (p. 281) of P is
âˆ¥Pâˆ¥2 = max
âˆ¥xâˆ¥2=1 âˆ¥Pxâˆ¥2 .
(5.9.17)
In other words, âˆ¥Pâˆ¥2 is the length of a longest vector in the image of the unit
sphere under transformation by P. To understand how sin Î¸ is related to âˆ¥Pâˆ¥2 ,
consider the situation in â„œ3. The image of the unit sphere under P is obtained
by projecting the sphere onto R along lines parallel to N. As depicted in
Figure 5.9.2, the result is an ellipse in R.
x
v
Î¸
Î¸
=
max
âˆ¥xâˆ¥=1
âˆ¥Px
âˆ¥âˆ¥
P
=
âˆ¥âˆ¥
v
âˆ¥
Figure 5.9.2
The norm of a longest vector v on this ellipse equals the norm of P. That is,
âˆ¥vâˆ¥2 = maxâˆ¥xâˆ¥2=1 âˆ¥Pxâˆ¥2 = âˆ¥Pâˆ¥2 , and it is apparent from the right triangle in

390
Chapter 5
Norms, Inner Products, and Orthogonality
Figure 5.9.2 that
sin Î¸ = âˆ¥xâˆ¥2
âˆ¥vâˆ¥2
=
1
âˆ¥vâˆ¥2
=
1
âˆ¥Pâˆ¥2
.
(5.9.18)
A little reï¬‚ection on the geometry associated with Figure 5.9.2 should convince
you that in â„œ3 a number Î¸ satisï¬es (5.9.16) if and only if Î¸ satisï¬es (5.9.18)â€”a
completely rigorous proof validating this fact in â„œn is given in Â§5.15.
Note: Recall from p. 281 that âˆ¥Pâˆ¥2 = âˆšÎ»max, where Î»max is the largest
number Î» such that PT P âˆ’Î»I is a singular matrix. Consequently,
sin Î¸ =
1
âˆ¥Pâˆ¥2
=
1
âˆšÎ»max
.
Numbers Î» such that PT P âˆ’Î»I is singular are called eigenvalues of PT P
(they are the main topic of discussion in Chapter 7, p. 489), and the numbers
âˆš
Î» are the singular values of P discussed on p. 411.
Exercises for section 5.9
5.9.1. Let X and Y be subspaces of â„œ3 whose respective bases are
BX =
ï£±
ï£²
ï£³
ï£«
ï£­
1
1
1
ï£¶
ï£¸,
ï£«
ï£­
1
2
2
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
and
BY =
ï£±
ï£²
ï£³
ï£«
ï£­
1
2
3
ï£¶
ï£¸
ï£¼
ï£½
ï£¾.
(a)
Explain why X and Y are complementary subspaces of â„œ3.
(b)
Determine the projector P onto X along Y as well as the
complementary projector Q onto Y along X.
(c)
Determine the projection of v =

2
âˆ’1
1

onto Y along X.
(d)
Verify that P and Q are both idempotent.
(e)
Verify that R (P) = X = N (Q) and N (P) = Y = R (Q).
5.9.2. Construct an example of a pair of nontrivial complementary subspaces
of â„œ5, and explain why your example is valid.
5.9.3. Construct an example to show that if V = X + Y but X âˆ©Y Ì¸= 0, then
a vector v âˆˆV can have two diï¬€erent representations as
v = x1 + y1
and
v = x2 + y2,
where x1, x2 âˆˆX and y1, y2 âˆˆY, but x1 Ì¸= x2 and y1 Ì¸= y2.

5.9 Complementary Subspaces
391
5.9.4. Explain why â„œnÃ—n = S âŠ•K, where S and K are the subspaces of
n Ã— n symmetric and skew-symmetric matrices, respectively. What is
the projection of A =
 1
2
3
4
5
6
7
8
9

onto S along K? Hint: Recall
Exercise 3.2.6.
5.9.5. For a general vector space, let X and Y be two subspaces with respec-
tive bases BX = {x1, x2, . . . , xm} and BY = {y1, y2, . . . , yn} .
(a)
Prove that X âˆ©Y = 0 if and only if {x1, . . . , xm, y1, . . . , yn}
is a linearly independent set.
(b)
Does BX âˆªBY being linear independent imply X âˆ©Y = 0?
(c)
If BX âˆªBY is a linearly independent set, does it follow that X
and Y are complementary subspaces? Why?
5.9.6. Let P be a projector deï¬ned on a vector space V. Prove that (5.9.10)
is trueâ€”i.e., prove that the range of a projector is the set of its â€œï¬xed
pointsâ€ in the sense that R (P) = {x âˆˆV | Px = x}.
5.9.7. Suppose that V = X âŠ•Y, and let P be the projector onto X along
Y. Prove that (5.9.11) is trueâ€”i.e., prove
R (P) = N (I âˆ’P) = X
and
R (I âˆ’P) = N (P) = Y.
5.9.8. Explain why âˆ¥Pâˆ¥2 â‰¥1 for every projector P Ì¸= 0. When is âˆ¥Pâˆ¥2 = 1?
5.9.9. Explain why âˆ¥I âˆ’Pâˆ¥2 = âˆ¥Pâˆ¥2 for all projectors that are not zero and
not equal to the identity.
5.9.10. Prove that if u, v âˆˆâ„œnÃ—1 are vectors such that vT u = 1, then
I âˆ’uvT 
2 =
uvT 
2 = âˆ¥uâˆ¥2 âˆ¥vâˆ¥2 =
uvT 
F ,
where âˆ¥â‹†âˆ¥F is the Frobenius matrix norm deï¬ned in (5.2.1) on p. 279.
5.9.11. Suppose that X and Y are complementary subspaces of â„œn, and let
B = [X | Y] be a nonsingular matrix in which the columns of X and
Y constitute respective bases for X and Y. For an arbitrary vector
v âˆˆâ„œnÃ—1, explain why the projection of v onto X along Y can be
obtained by the following two-step process.
(1)
Solve the system Bz = v for z.
(2)
Partition z as z =
 z1
z2

, and set p = Xz1.

392
Chapter 5
Norms, Inner Products, and Orthogonality
5.9.12. Let P and Q be projectors.
(a)
Prove R (P) = R (Q) if and only if PQ = Q and QP = P.
(b)
Prove N (P) = N (Q) if and only if PQ = P and QP = Q.
(c)
Prove that if E1, E2, . . . , Ek are projectors with the same range,
and if Î±1, Î±2, . . . , Î±k are scalars such that 
j Î±j = 1, then

j Î±jEj is a projector.
5.9.13. Prove that rank (P) = trace (P) for every projector P deï¬ned on â„œn.
Hint: Recall Example 3.6.5 (p. 110).
5.9.14. Let {Xi}k
i=1 be a collection of subspaces from a vector space V, and
let Bi denote a basis for Xi. Prove that the following statements are
equivalent.
(i)
V = X1 + X2 + Â· Â· Â· + Xk and Xj âˆ©(X1 + Â· Â· Â· + Xjâˆ’1) = 0 for
each j = 2, 3, . . . , k.
(ii)
For each vector v âˆˆV, there is one and only one way to write
v = x1 + x2 + Â· Â· Â· + xk, where xi âˆˆXi.
(iii)
B = B1 âˆªB2 âˆªÂ· Â· Â· âˆªBk with Bi âˆ©Bj = Ï† for i Ì¸= j is a basis
for V.
Whenever any one of the above statements is true, V is said to be the
direct sum of the Xi â€™s, and we write V = X1 âŠ•X2 âŠ•Â· Â· Â· âŠ•Xk. Notice
that for k = 2, (i) and (5.9.1) say the same thing, and (ii) and (iii)
reduce to (5.9.3) and (5.9.4), respectively.
5.9.15. For complementary subspaces X and Y of â„œn, let P be the projec-
tor onto X along Y, and let Q = [X | Y] in which the columns of
X and Y constitute bases for X and Y, respectively. Prove that if
Qâˆ’1AnÃ—nQ is partitioned as Qâˆ’1AQ =
 A11
A12
A21
A22

, then
Q

A11
0
0
0

Qâˆ’1=PAP,
Q

0
A12
0
0

Qâˆ’1 = PA(I âˆ’P),
Q

0
0
A21
0

Qâˆ’1= (I âˆ’P)AP, Q

0
0
0
A12

Qâˆ’1=(I âˆ’P)A(I âˆ’P).
This means that if A is considered as a linear operator on â„œn, and if
B = BX âˆªBY, where BX and BY are the respective bases for X and
Y deï¬ned by the columns of X and Y, then, in the context of Â§4.8, the
matrix representation of A with respect to B is [A]B =
 A11
A12
A21
A22


5.9 Complementary Subspaces
393
in which the blocks are matrix representations of restricted operators as
shown below.
A11 =
1
PAP/X
2
BX
.
A12 =
1
PA(I âˆ’P)/Y
2
BYBX
.
A21 =
1
(I âˆ’P)AP/X
2
BX BY
.
A22 =
1
(I âˆ’P)A(I âˆ’P)/Y
2
BY
.
5.9.16. Suppose that â„œn = X âŠ•Y, where dim X = r, and let P be the
projector onto X
along Y. Explain why there exist matrices XnÃ—r
and ArÃ—n such that P = XA, where rank (X) = rank (A) = r and
AX = Ir. This is a full-rank factorization for P (recall Exercise 3.9.8).
5.9.17. For either a real or complex vector space, let E be the projector onto
X1 along Y1, and let F be the projector onto X2 along Y2. Prove
that E + F is a projector if and only if EF = FE = 0, and under this
condition, prove that R (E + F) = X1 âŠ•X2 and N (E + F) = Y1 âˆ©Y2.
5.9.18. For either a real or complex vector space, let E be the projector onto
X1 along Y1, and let F be the projector onto X2 along Y2. Prove
that E âˆ’F is a projector if and only if EF = FE = F, and under this
condition, prove that R (E âˆ’F) = X1 âˆ©Y2 and N (E âˆ’F) = Y1 âŠ•X2.
Hint: P is a projector if and only if I âˆ’P is a projector.
5.9.19. For either a real or complex vector space, let E be the projector onto
X1 along Y1, and let F be the projector onto X2 along Y2. Prove that
if EF = P = FE, then P is the projector onto X1âˆ©X2 along Y1+Y2.
5.9.20. An inner pseudoinverse for AmÃ—n is a matrix XnÃ—m such that
AXA = A, and an outer pseudoinverse for A is a matrix X satis-
fying XAX = X. When X is both an inner and outer pseudoinverse,
X is called a reï¬‚exive pseudoinverse.
(a)
If Ax = b is a consistent system of m equations in n un-
knowns, and if Aâˆ’is any inner pseudoinverse for A, explain
why the set of all solutions to Ax = b can be expressed as
Aâˆ’b + R

I âˆ’Aâˆ’A

= {Aâˆ’b + (I âˆ’Aâˆ’A)h | h âˆˆâ„œn}.
(b)
Let M and L be respective complements of R (A) and N (A)
so that Cm = R (A) âŠ•M and Cn = L âŠ•N (A). Prove that
there is a unique reï¬‚exive pseudoinverse X for A such that
R (X) = L and N (X) = M. Show that X = QAâˆ’P, where
Aâˆ’is any inner pseudoinverse for A, P is the projector onto
R (A) along M, and Q is the projector onto L along N (A).

394
Chapter 5
Norms, Inner Products, and Orthogonality
5.10
RANGE-NULLSPACE DECOMPOSITION
Since there are inï¬nitely many diï¬€erent pairs of complementary subspaces in
â„œn (or Cn ),
54 is some pair more â€œnaturalâ€ than the rest? Without reference to
anything else the question is hard to answer. But if we start with a given matrix
AnÃ—n, then there is a very natural direct sum decomposition of â„œn deï¬ned
by fundamental subspaces associated with powers of A. The rank plus nullity
theorem on p. 199 says that dim R (A)+dim N (A) = n, so itâ€™s reasonable to ask
about the possibility of R (A) and N (A) being complementary subspaces. If A
is nonsingular, then itâ€™s trivially true that R (A) and N (A) are complementary,
but when A is singular, this need not be the case because R (A) and N (A)
need not be disjoint. For example,
A =

0
1
0
0

=â‡’

1
0

âˆˆR (A) âˆ©N (A).
But all is not lost if we are willing to consider powers of A.
Range-Nullspace Decomposition
For every singular matrix AnÃ—n, there exists a positive integer k such
that R

Ak
and N

Ak
are complementary subspaces. That is,
â„œn = R

Ak
âŠ•N

Ak
.
(5.10.1)
The smallest positive integer k for which (5.10.1) holds is called the
index of A. For nonsingular matrices we deï¬ne index(A) = 0.
Proof.
First observe that as A is powered the nullspaces grow and the ranges
shrinkâ€”recall Exercise 4.2.12.
N

A0
âŠ†N (A) âŠ†N

A2
âŠ†Â· Â· Â· âŠ†N

Ak
âŠ†N

Ak+1
âŠ†Â· Â· Â·
R

A0
âŠ‡R (A) âŠ‡R

A2
âŠ‡Â· Â· Â· âŠ‡R

Ak
âŠ‡R

Ak+1
âŠ‡Â· Â· Â· .
(5.10.2)
The proof of (5.10.1) is attained by combining the four following properties.
Property 1.
There is equality at some point in each of the chains (5.10.2).
Proof.
If there is strict containment at each link in the nullspace chain in
(5.10.2), then the sequence of inequalities
dim N

A0
< dim N (A) < dim N

A2
< dim N

A3
< Â· Â· Â·
54
All statements and arguments in this section are phrased in terms of â„œn, but everything we
say has a trivial extension to Cn.

5.10 Range-Nullspace Decomposition
395
holds, and this forces n < dim N

An+1
, which is impossible. A similar
argument proves equality exists somewhere in the range chain.
Property 2.
Once equality is attained, it is maintained throughout the rest of
both chains in (5.10.2). In other words,
N

A0
âŠ‚N (A) âŠ‚Â· Â· Â· âŠ‚N

Ak
= N

Ak+1
= N

Ak+2
= Â· Â· Â·
R

A0
âŠƒR (A) âŠƒÂ· Â· Â· âŠƒR

Ak
= R

Ak+1
= R

Ak+2
= Â· Â· Â· .
(5.10.3)
To prove this for the range chain, observe that if k is the smallest nonneg-
ative integer such that R

Ak
= R

Ak+1
, then for all i â‰¥1,
R

Ai+k
= R

AiAk
= AiR

Ak
= AiR

Ak+1
= R

Ai+k+1
.
The nullspace chain stops growing at exactly the same place the ranges
stop shrinking because the rank plus nullity theorem (p. 199) insures that
dim N (Ap) = n âˆ’dim R (Ap).
Property 3.
If k is the value at which the ranges stop shrinking and the
nullspaces stop growing in (5.10.3), then R

Ak
âˆ©N

Ak
= 0.
Proof.
If x âˆˆR

Ak
âˆ©N

Ak
, then Aky = x for some y âˆˆâ„œn, and
Akx = 0. Hence A2ky = Akx = 0 â‡’y âˆˆN

A2k
= N

Ak
â‡’x = 0.
Property 4.
If k is the value at which the ranges stop shrinking and the
nullspaces stop growing in (5.10.3), then R

Ak
+ N

Ak
= â„œn.
Proof.
Use Property 3 along with (4.4.19), (4.4.15), and (4.4.6), to write
dim

R

Ak
+ N

Ak
= dim R

Ak
+ dim N

Ak
âˆ’dim R

Ak
âˆ©N

Ak
= dim R

Ak
+ dim N

Ak
= n
=â‡’
R

Ak
+ N

Ak
= â„œn.
Below is a summary of our observations concerning the index of a square matrix.
Index
The index of a square matrix A is the smallest nonnegative integer k
such that any one of the three following statements is true.
â€¢
rank

Ak
= rank

Ak+1
.
â€¢
R

Ak
= R

Ak+1
â€”i.e., the point where R

Ak
stops shrinking.
â€¢
N

Ak
= N

Ak+1
â€”i.e., the point where N

Ak
stops growing.
For nonsingular matrices,
index (A)
=
0.
For singular matrices,
index (A) is the smallest positive integer k such that either of the fol-
lowing two statements is true.
â€¢
R

Ak
âˆ©N

Ak
= 0.
(5.10.4)
â€¢
â„œn = R

Ak
âŠ•N

Ak
.

396
Chapter 5
Norms, Inner Products, and Orthogonality
Example 5.10.1
Problem: Determine the index of A =
 2
0
0
0
1
1
0
âˆ’1
âˆ’1

.
Solution: A is singular (because rank (A) = 2), so index(A) > 0. Since
A2 =
ï£«
ï£­
4
0
0
0
0
0
0
0
0
ï£¶
ï£¸
and
A3 =
ï£«
ï£­
8
0
0
0
0
0
0
0
0
ï£¶
ï£¸,
we see that rank (A) > rank

A2
= rank

A3
, so index(A) = 2. Alternately,
R (A) = span
ï£±
ï£²
ï£³
ï£«
ï£­
2
0
0
ï£¶
ï£¸,
ï£«
ï£­
0
1
âˆ’1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾, R

A2
= span
ï£«
ï£­
4
0
0
ï£¶
ï£¸, R

A3
= span
ï£«
ï£­
8
0
0
ï£¶
ï£¸,
so R (A) âŠƒR

A2
= R

A3
implies index(A) = 2.
Nilpotent Matrices
â€¢
NnÃ—n is said to be nilpotent whenever Nk = 0 for some positive
integer k.
â€¢
k = index(N) is the smallest positive integer such that Nk = 0.
(Some authors refer to index(N) as the index of nilpotency.)
Proof.
To prove that k = index(N) is the smallest positive integer such that
Nk = 0, suppose p is a positive integer such that Np = 0, but Npâˆ’1 Ì¸= 0.
We know from (5.10.3) that R

N0
âŠƒR (N) âŠƒÂ· Â· Â· âŠƒR

Nk
= R

Nk+1
=
R

Nk+2
= Â· Â· Â· , and this makes it clear that itâ€™s impossible to have p < k or
p > k, so p = k is the only choice.
Example 5.10.2
Problem: Verify that
N =
ï£«
ï£­
0
1
0
0
0
1
0
0
0
ï£¶
ï£¸
is a nilpotent matrix, and determine its index.
Solution: Computing the powers
N2 =
ï£«
ï£­
0
0
1
0
0
0
0
0
0
ï£¶
ï£¸
and
N3 =
ï£«
ï£­
0
0
0
0
0
0
0
0
0
ï£¶
ï£¸,
reveals that N is indeed nilpotent, and it shows that index(N) = 3 because
N3 = 0, but N2 Ì¸= 0.

5.10 Range-Nullspace Decomposition
397
Anytime â„œn can be written as the direct sum of two complementary sub-
spaces such that one of them is an invariant subspace for a given square matrix A
we have a block-triangular representation for A according to formula (4.9.9) on
p. 263. And if both complementary spaces are invariant under A, then (4.9.10)
says that this block-triangular representation is actually block diagonal.
Herein lies the true value of the range-nullspace decomposition (5.10.1) be-
cause it turns out that if k = index(A), then R

Ak
and N

Ak
are both
invariant subspaces under A. R

Ak
is invariant under A because
A

R

Ak
= R

Ak+1
= R

Ak
,
and N

Ak
is invariant because
x âˆˆA

N

Ak
=â‡’
x = Aw for some w âˆˆN

Ak
= N

Ak+1
=â‡’
Akx = Ak+1w = 0
=â‡’
x âˆˆN

Ak
=â‡’
A

N

Ak
âŠ†N

Ak
.
This brings us to a matrix decomposition that is an important building
block for developments that culminate in the Jordan form on p. 590.
Core-Nilpotent Decomposition
If A is an n Ã— n singular matrix of index k such that rank

Ak
= r,
then there exists a nonsingular matrix Q such that
Qâˆ’1AQ =

CrÃ—r
0
0
N

(5.10.5)
in which C is nonsingular, and N is nilpotent of index k. In other
words, A is similar to a 2 Ã— 2 block-diagonal matrix containing a non-
singular â€œcoreâ€ and a nilpotent component. The block-diagonal matrix
in (5.10.5) is called a core-nilpotent decomposition of A.
Note: When A is nonsingular, k = 0 and r = n, so N is not present,
and we can set Q = I and C = A (the nonsingular core is everything).
So (5.10.5) says absolutely nothing about nonsingular matrices.
Proof.
Let Q =

X | Y

, where the columns of XnÃ—r and YnÃ—nâˆ’r constitute
bases for R

Ak
and N

Ak
, respectively. Equation (4.9.10) guarantees that
Qâˆ’1AQ must be block diagonal in form, and thus (5.10.5) is established. To see
that N is nilpotent, let
Qâˆ’1 =

U
V

,

398
Chapter 5
Norms, Inner Products, and Orthogonality
and write

Ck
0
0
Nk

= Qâˆ’1AkQ =

U
V

Ak
X | Y

=

UAkX
0
VAkX
0

.
Therefore, Nk = 0 and Qâˆ’1AkQ =
 Ck
0
0
0
	
. Since Ck is r Ã— r and r =
rank

Ak
= rank

Qâˆ’1AkQ

= rank

Ck
, it must be the case that Ck is
nonsingular, and hence C is nonsingular. Finally, notice that index(N) = k
because if index(N) Ì¸= k, then Nkâˆ’1 = 0, so
rank

Akâˆ’1
=rank

Qâˆ’1Akâˆ’1Q

=rank

Ckâˆ’1
0
0
Nkâˆ’1

=rank

Ckâˆ’1
0
0
0

= rank

Ckâˆ’1
= r = rank

Ak
,
which is impossible because index(A) = k is the smallest integer for which there
is equality in ranks of powers.
Example 5.10.3
Problem: Let AnÃ—n have index k with rank

Ak
= r, and let
Qâˆ’1AQ =

CrÃ—r
0
0
N

with
Q =

XnÃ—r | Y

and Qâˆ’1 =
 UrÃ—n
V

be the core-nilpotent decomposition described in (5.10.5). Explain why
Q

Ir
0
0
0

Qâˆ’1 = XU = the projector onto R

Ak
along N

Ak
and
Q

0
0
0
Inâˆ’r

Qâˆ’1 = YV = the projector onto N

Ak
along R

Ak
.
Solution: Because R

Ak
and N

Ak
are complementary subspaces, and
because the columns of X and Y constitute respective bases for these spaces,
it follows from the discussion concerning projectors on p. 386 that
P =

X | Y
 
I
0
0
0
 
X | Y
âˆ’1 = Q

Ir
0
0
0

Qâˆ’1 = XU
must be the projector onto R

Ak
along N

Ak
, and
I âˆ’P =

X | Y
 
0
0
0
I
 
X | Y
âˆ’1 = Q

0
0
0
Inâˆ’r

Qâˆ’1 = YV
is the complementary projector onto N

Ak
along R

Ak
.

5.10 Range-Nullspace Decomposition
399
Example 5.10.4
Problem: Explain how each noninvertible linear operator deï¬ned on an n-
dimensional vector space V can be decomposed as the â€œdirect sumâ€ of an in-
vertible operator and a nilpotent operator.
Solution: Let T be a linear operator of index k deï¬ned on V = R âŠ•N,
where R = R

Tk
and N = N

Tk
, and let E = T/R and F = T/N be
the restriction operators as described in Â§4.9. Since R and N are invariant
subspaces for T, we know from the discussion of matrix representations on
p. 263 that the right-hand side of the core-nilpotent decomposition in (5.10.5)
must be the matrix representation of T with respect to a basis BR âˆªBN , where
BR and BN are respective bases for R and N. Furthermore, the nonsingular
matrix C and the nilpotent matrix N are the matrix representations of E and
F with respect to BR and BN , respectively. Consequently, E is an invertible
operator on R, and F is a nilpotent operator on N. Since V = R âŠ•N, each
x âˆˆV can be expressed as x = r + n with r âˆˆR and n âˆˆN. This allows
us to formulate the concept of the direct sum of E and F by deï¬ning E âŠ•F
to be the linear operator on V such that (E âŠ•F)(x) = E(r) + F(n) for each
x âˆˆV. Therefore,
T(x) = T(r + n) = T(r) + T(n) = (T/R)(r) + (T/N )(n)
= E(r) + F(n) = (E âŠ•F)(x)
for each
x âˆˆV.
In other words, T = E âŠ•F in which E = T/R is invertible and F = T/N is
nilpotent.
Example 5.10.5
Drazin Inverse.
Inverting the nonsingular core C and neglecting the nilpo-
tent part N in the core-nilpotent decomposition (5.10.5) produces a natural
generalization of matrix inversion. More precisely, if
A = Q

C
0
0
N

Qâˆ’1,
then
AD = Q

Câˆ’1
0
0
0

Qâˆ’1
(5.10.6)
deï¬nes the Drazin inverse of A. Even though the components in a core-
nilpotent decomposition are not uniquely deï¬ned by A, it can be proven that
AD is unique and has the following properties.
â€¢
AD = Aâˆ’1 when A is nonsingular (the nilpotent part is not present).
â€¢
ADAAD = AD, AAD = ADA, Ak+1AD = Ak, where k = index(A).
55
55
These three properties served as Michael P. Drazinâ€™s original deï¬nition in 1968. Initially,

400
Chapter 5
Norms, Inner Products, and Orthogonality
â€¢
If Ax = b is a consistent system of linear equations in which b âˆˆR

Ak
,
then x = ADb is the unique solution that belongs to R

Ak
(Exercise
5.10.9).
â€¢
AAD is the projector onto R

Ak
along N

Ak
, and I âˆ’AAD is the
complementary projector onto N

Ak
along R

Ak
(Exercise 5.10.10).
â€¢
If A is considered as a linear operator on â„œn, then, with respect to a basis
BR for R

Ak
, C is the matrix representation for the restricted operator
A/R(Ak) (see p. 263). Thus A/R(Ak) is invertible. Moreover,
1
AD
/R(Ak)
2
BR
= Câˆ’1 =
3
A/R(Ak)
âˆ’14
BR
,
so
AD
/R(Ak) =

A/R(Ak)
âˆ’1
.
In other words, AD is the inverse of A on R

Ak
, and AD is the zero
operator on N

Ak
, so, in the context of Example 5.10.4,
A = A/R(Ak) âŠ•A/N(Ak)
and
AD =

A/R(Ak)
âˆ’1
âŠ•0/N(Ak).
Exercises for section 5.10
5.10.1. If A is a square matrix of index k > 0, prove that index(Ak) = 1.
5.10.2. If A is a nilpotent matrix of index k, describe the components in a
core-nilpotent decomposition of A.
5.10.3. Prove that if A is a symmetric matrix, then index(A) â‰¤1.
5.10.4. A âˆˆCnÃ—n is said to be a normal matrix whenever AAâˆ—= Aâˆ—A.
Prove that if A is normal, then index(A) â‰¤1.
Note: All symmetric matrices are normal, so the result of this exercise
includes the result of Exercise 5.10.3 as a special case.
Drazinâ€™s concept attracted little interestâ€”perhaps due to Drazinâ€™s abstract algebraic pre-
sentation. But eventually Drazinâ€™s generalized inverse was recognized to be a useful tool for
analyzing nonorthogonal types of problems involving singular matrices. In this respect, the
Drazin inverse is complementary to the Mooreâ€“Penrose pseudoinverse discussed in Exercise
4.5.20 and on p. 423 because the Mooreâ€“Penrose pseudoinverse is more useful in applications
where orthogonality is somehow wired in (e.g., least squares).

5.10 Range-Nullspace Decomposition
401
5.10.5. Find a core-nilpotent decomposition and the Drazin inverse of
A =
ï£«
ï£­
âˆ’2
0
âˆ’4
4
2
4
3
2
2
ï£¶
ï£¸.
5.10.6. For a square matrix A, any scalar Î» that makes A âˆ’Î»I singular
is called an eigenvalue for A. The index of an eigenvalue Î» is de-
ï¬ned to be the index of the associated matrix A âˆ’Î»I. In other words,
index(Î») = index(A âˆ’Î»I). Determine the eigenvalues and the index of
each eigenvalue for the following matrices:
(a)
J =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
2
0
0
0
0
0
2
ï£¶
ï£·
ï£·
ï£·
ï£¸.
(b)
J =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1
1
0
0
0
0
1
1
0
0
0
0
1
0
0
0
0
0
2
1
0
0
0
0
2
ï£¶
ï£·
ï£·
ï£·
ï£¸.
5.10.7. Let P be a projector diï¬€erent from the identity.
(a)
Explain why index(P) = 1. What is the index of I?
(b)
Determine the core-nilpotent decomposition for P.
5.10.8. Let N be a nilpotent matrix of index k, and suppose that x is a vector
such that Nkâˆ’1x Ì¸= 0. Prove that the set
C = {x, Nx, N2x, . . . , Nkâˆ’1x}
is a linearly independent set. C is sometimes called a Jordan chain or
a Krylov sequence.
5.10.9. Let A be a square matrix of index k, and let b âˆˆR

Ak
.
(a)
Explain why the linear system Ax = b must be consistent.
(b)
Explain why x = ADb is the unique solution in R

Ak
.
(c)
Explain why the general solution is given by ADb + N (A).
5.10.10. Suppose that A is a square matrix of index k, and let AD be the
Drazin inverse of A as deï¬ned in Example 5.10.5. Explain why AAD
is the projector onto R

Ak
along N

Ak
. What does I âˆ’AAD
project onto and along?

402
Chapter 5
Norms, Inner Products, and Orthogonality
5.10.11. An algebraic group is a set G together with an associative operation
between its elements such that G is closed with respect to this operation;
G possesses an identity element E (which can be proven to be unique);
and every member A âˆˆG has an inverse A# (which can be proven to
be unique). These are essentially the axioms (A1), (A2), (A4), and (A5)
in the deï¬nition of a vector space given on p. 160. A matrix group is
a set of square matrices that forms an algebraic group under ordinary
matrix multiplication.
(a)
Show that the set of n Ã— n nonsingular matrices is a matrix
group.
(b)
Show that the set of n Ã— n unitary matrices is a subgroup of
the n Ã— n nonsingular matrices.
(c)
Show that the set G =
5 Î±
Î±
Î±
Î±
  Î± Ì¸= 0
6
is a matrix group.
In particular, what does the identity element E âˆˆG look like,
and what does the inverse A# of A âˆˆG look like?
5.10.12. For singular matrices, prove that the following statements are equivalent.
(a)
A is a group matrix (i.e., A belongs to a matrix group).
(b)
R (A) âˆ©N (A) = 0.
(c)
R (A) and N (A) are complementary subspaces.
(d)
index(A) = 1.
(e)
There are nonsingular matrices QnÃ—n and CrÃ—r such that
Qâˆ’1AQ =

CrÃ—r
0
0
0

,
where
r = rank (A).
5.10.13. Let A âˆˆG for some matrix group G.
(a)
Show that the identity element E âˆˆG is the projector onto
R (A) along N (A) by arguing that E must be of the form
E = Q

IrÃ—r
0
0
0

Qâˆ’1.
(b)
Show that the group inverse of A (the inverse of A in G )
must be of the form
A# = Q

Câˆ’1
0
0
0

Qâˆ’1.

5.11 Orthogonal Decomposition
403
5.11
ORTHOGONAL DECOMPOSITION
The orthogonal complement of a single vector x was deï¬ned on p. 322 to be the
set of all vectors orthogonal to x. Below is the natural extension of this idea.
Orthogonal Complement
For a subset M of an inner-product space V, the orthogonal com-
plement MâŠ¥(pronounced â€œM perpâ€) of M is deï¬ned to be the set
of all vectors in V that are orthogonal to every vector in M. That is,
MâŠ¥=

x âˆˆV
 âŸ¨m xâŸ©= 0 for all m âˆˆM

.
For example, if M = {x} is a single vector in â„œ2, then, as illustrated in
Figure 5.11.1, MâŠ¥is the line through the origin that is perpendicular to x. If
M is a plane through the origin in â„œ3, then MâŠ¥is the line through the origin
that is perpendicular to the plane.
Figure 5.11.1
Notice that MâŠ¥is a subspace of V even if M is not a subspace because MâŠ¥is
closed with respect to vector addition and scalar multiplication (Exercise 5.11.4).
But if M is a subspace, then M and MâŠ¥decompose V as described below.
Orthogonal Complementary Subspaces
If M is a subspace of a ï¬nite-dimensional inner-product space V, then
V = M âŠ•MâŠ¥.
(5.11.1)
Furthermore, if N is a subspace such that V = M âŠ•N and N âŠ¥M
(every vector in N is orthogonal to every vector in M ), then
N = MâŠ¥.
(5.11.2)

404
Chapter 5
Norms, Inner Products, and Orthogonality
Proof.
Observe that M âˆ©MâŠ¥= 0 because if x âˆˆM and x âˆˆMâŠ¥, then
x must be orthogonal to itself, and âŸ¨x xâŸ©= 0 implies x = 0. To prove that
M âŠ•MâŠ¥= V, suppose that BM and BMâŠ¥are orthonormal bases for M
and MâŠ¥, respectively. Since M and MâŠ¥are disjoint, BM âˆªBMâŠ¥is an
orthonormal basis for some subspace S = M âŠ•MâŠ¥âŠ†V. If S Ì¸= V, then
the basis extension technique of Example 4.4.5 followed by the Gramâ€“Schmidt
orthogonalization procedure of Â§5.5 yields a nonempty set of vectors E such that
BM âˆªBMâŠ¥âˆªE is an orthonormal basis for V. Consequently,
E âŠ¥BM
=â‡’
E âŠ¥M
=â‡’
E âŠ†MâŠ¥
=â‡’
E âŠ†span (BMâŠ¥) .
But this is impossible because BMâˆªBMâŠ¥âˆªE is linearly independent. Therefore,
E is the empty set, and thus V = M âŠ•MâŠ¥. To prove statement (5.11.2),
note that N âŠ¥M implies N âŠ†MâŠ¥, and coupling this with the fact that
M âŠ•MâŠ¥= V = M âŠ•N together with (4.4.19) insures
dim N = dim V âˆ’dim M = dim MâŠ¥.
Example 5.11.1
Problem: Let UmÃ—m =

U1 | U2

be a partitioned orthogonal matrix. Explain
why R (U1) and R (U2) must be orthogonal complements of each other.
Solution: Statement (5.9.4) insures that â„œm = R (U1) âŠ•R (U2), and we know
that R (U1) âŠ¥R (U2) because the columns of U are an orthonormal set.
Therefore, (5.11.2) guarantees that R (U2) = R (U1)âŠ¥.
Perp Operation
If M is a subspace of an n-dimensional inner-product space, then the
following statements are true.
â€¢
dim MâŠ¥= n âˆ’dim M.
(5.11.3)
â€¢
MâŠ¥âŠ¥= M.
(5.11.4)
Proof.
Property (5.11.3) follows from the fact that M and MâŠ¥are comple-
mentary subspacesâ€”recall (4.4.19). To prove (5.11.4), ï¬rst show that MâŠ¥âŠ¥âŠ†
M. If x âˆˆMâŠ¥âŠ¥, then (5.11.1) implies x = m + n, where m âˆˆM and
n âˆˆMâŠ¥, so
0 = âŸ¨n xâŸ©= âŸ¨n m + nâŸ©= âŸ¨n mâŸ©+ âŸ¨n nâŸ©= âŸ¨n nâŸ©
=â‡’
n = 0
=â‡’
x âˆˆM,
and thus MâŠ¥âŠ¥âŠ†M. We know from (5.11.3) that dim MâŠ¥= n âˆ’dim M and
dim MâŠ¥âŠ¥= nâˆ’dim MâŠ¥, so dim MâŠ¥âŠ¥= dim M. Therefore, (4.4.6) guarantees
that MâŠ¥âŠ¥= M.

5.11 Orthogonal Decomposition
405
We are now in a position to understand why the four fundamental subspaces
associated with a matrix A âˆˆâ„œmÃ—n are indeed â€œfundamental.â€ First consider
R (A)âŠ¥, and observe that for all y âˆˆâ„œn,
x âˆˆR (A)âŠ¥
â‡â‡’
âŸ¨Ay xâŸ©= 0
â‡â‡’
yT AT x = 0
â‡â‡’
7
y AT x
8
= 0
â‡â‡’
AT x = 0
(Exercise 5.3.2)
â‡â‡’
x âˆˆN

AT 
.
Therefore, R (A)âŠ¥= N

AT 
. Perping both sides of this equation and replac-
ing
56 A by AT produces R

AT 
= N (A)âŠ¥. Combining these observations
produces one of the fundamental theorems of linear algebra.
Orthogonal Decomposition Theorem
For every A âˆˆâ„œmÃ—n,
R (A)âŠ¥= N

AT 
and
N (A)âŠ¥= R

AT 
.
(5.11.5)
In light of (5.11.1), this means that every matrix A âˆˆâ„œmÃ—n produces
an orthogonal decomposition of â„œm and â„œn in the sense that
â„œm = R (A) âŠ•R (A)âŠ¥= R (A) âŠ•N

AT 
,
(5.11.6)
and
â„œn = N (A) âŠ•N (A)âŠ¥= N (A) âŠ•R

AT 
.
(5.11.7)
Theorems without hypotheses tend to be extreme in the sense that they
either say very little or they reveal a lot. The orthogonal decomposition theorem
has no hypothesisâ€”it holds for all matricesâ€”so, does it really say something
signiï¬cant? Yes, it does, and hereâ€™s part of the reason why.
In addition to telling us how to decompose â„œm and â„œn in terms of the
four fundamental subspaces of A, the orthogonal decomposition theorem also
tells us how to decompose A itself into more basic components. Suppose that
rank (A) = r, and let
BR(A) = {u1, u2, . . . , ur}
and
BN(AT) = {ur+1, ur+2, . . . , um}
be orthonormal bases for R (A) and N

AT 
, respectively, and let
BR(AT) = {v1, v2, . . . , vr}
and
BN(A) = {vr+1, vr+2, . . . , vn}
56
Here, as well as throughout the rest of this section, (â‹†)T
can be replaced by (â‹†)âˆ—whenever
â„œmÃ—n is replaced by CmÃ—n.

406
Chapter 5
Norms, Inner Products, and Orthogonality
be orthonormal bases for R

AT 
and N (A), respectively. It follows that
BR(A) âˆªBN(AT) and BR(AT) âˆªBN(A) are orthonormal bases for â„œm and â„œn,
respectively, and hence
UmÃ—m =

u1 | u2 | Â· Â· Â· | um

and
VnÃ—n =

v1 | v2 | Â· Â· Â· | vn

(5.11.8)
are orthogonal matrices. Now consider the product R = UT AV, and notice
that rij = uT
i Avj. However, uT
i A = 0 for i = r + 1, . . . , m and Avj = 0 for
j = r + 1, . . . , n, so
R = UT AV =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
uT
1 Av1
Â· Â· Â·
uT
1 Avr
0
Â· Â· Â·
0
...
...
...
...
...
uT
r Av1
Â· Â· Â·
uT
r Avr
0
Â· Â· Â·
0
0
Â· Â· Â·
0
0
Â· Â· Â·
0
...
...
...
...
...
0
Â· Â· Â·
0
0
Â· Â· Â·
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
(5.11.9)
In other words, A can be factored as
A = URVT = U

CrÃ—r
0
0
0

VT .
(5.11.10)
Moreover, C is nonsingular because it is r Ã— r and
rank (C) = rank

C
0
0
0

= rank

UT AV

= rank (A) = r.
For lack of a better name, we will refer to (5.11.10) as a URV factorization.
We have just observed that every set of orthonormal bases for the four
fundamental subspaces deï¬nes a URV factorization. The situation is also re-
versible in the sense that every URV factorization of A deï¬nes an orthonor-
mal basis for each fundamental subspace. Starting with orthogonal matrices
U =

U1 | U2

and V =

V1 | V2

together with a nonsingular matrix CrÃ—r
such that (5.11.10) holds, use the fact that right-hand multiplication by a non-
singular matrix does not alter the range (Exercise 4.5.12) to observe
R (A) = R (UR) = R (U1C | 0) = R (U1C) = R (U1).
By (5.11.5) and Example 5.11.1, N

AT 
= R (A)âŠ¥= R (U1)âŠ¥= R (U2).
Similarly, left-hand multiplication by a nonsingular matrix does not change the
nullspace, so the second equation in (5.11.5) along with Example 5.11.1 yields
N (A) = N

RVT 
= N

CVT
1
0

= N

CVT
1

= N

VT
1

= R (V1)âŠ¥= R (V2),
and R

AT 
= N (A)âŠ¥= R (V2)âŠ¥= R (V1). A summary is given below.

5.11 Orthogonal Decomposition
407
URV Factorization
For each A âˆˆâ„œmÃ—n of rank r, there are orthogonal matrices UmÃ—m
and VnÃ—n and a nonsingular matrix CrÃ—r such that
A = URVT = U

CrÃ—r
0
0
0

mÃ—n
VT .
(5.11.11)
â€¢
The ï¬rst r columns in U are an orthonormal basis for R (A).
â€¢
The last mâˆ’r columns of U are an orthonormal basis for N

AT 
.
â€¢
The ï¬rst r columns in V are an orthonormal basis for R

AT 
.
â€¢
The last n âˆ’r columns of V are an orthonormal basis for N (A).
Each diï¬€erent collection of orthonormal bases for the four fundamental
subspaces of A produces a diï¬€erent URV factorization of A. In the
complex case, replace (â‹†)T by (â‹†)âˆ—and â€œorthogonalâ€ by â€œunitary.â€
Example 5.11.2
Problem: Explain how to make C lower triangular in (5.11.11).
Solution: Apply Householder (or Givens) reduction to produce an orthogonal
matrix PmÃ—m such that PA =
 B
0

, where B is r Ã— n of rank r. House-
holder (or Givens) reduction applied to BT
results in an orthogonal matrix
QnÃ—n and a nonsingular upper-triangular matrix T such that
QBT =

TrÃ—r
0

=â‡’
B =

TT | 0

Q
=â‡’

B
0

=

TT
0
0
0

Q,
so A = PT  B
0

= PT  TT
0
0
0

Q is a URV factorization.
Note: C can in fact be made diagonalâ€”see (p. 412).
Have you noticed the duality that has emerged concerning the use of fun-
damental subspaces of A to decompose â„œn (or Cn )? On one hand there is
the range-nullspace decomposition (p. 394), and on the other is the orthogo-
nal decomposition theorem (p. 405). Each produces a decomposition of A. The
range-nullspace decomposition of â„œn produces the core-nilpotent decomposition
of A (p. 397), and the orthogonal decomposition theorem produces the URV
factorization. In the next section, the URV factorization specializes to become

408
Chapter 5
Norms, Inner Products, and Orthogonality
the singular value decomposition (p. 412), and in a somewhat parallel manner,
the core-nilpotent decomposition paves the way to the Jordan form (p. 590).
These two parallel tracks constitute the backbone for the theory of modern linear
algebra, so itâ€™s worthwhile to take a moment and reï¬‚ect on them.
The range-nullspace decomposition decomposes â„œn with square matrices
while the orthogonal decomposition theorem does it with rectangular matrices.
So does this mean that the range-nullspace decomposition is a special case of,
or somehow weaker than, the orthogonal decomposition theorem? No! Even for
square matrices they are not very comparable because each says something that
the other doesnâ€™t. The core-nilpotent decomposition (and eventually the Jordan
form) is obtained by a similarity transformation, and, as discussed in Â§Â§4.8â€“4.9,
similarity is the primary mechanism for revealing characteristics of A that are
independent of bases or coordinate systems. The URV factorization has little
to say about such things because it is generally not a similarity transforma-
tion. Orthogonal decomposition has the advantage whenever orthogonality is
naturally built into a problemâ€”such as least squares applications. And, as dis-
cussed in Â§5.7, orthogonal methods often produce numerically stable algorithms
for ï¬‚oating-point computation, whereas similarity transformations are generally
not well suited for numerical computations. The value of similarity is mainly on
the theoretical side of the coin.
So when do we get the best of both worldsâ€”i.e., when is a URV factoriza-
tion also a core-nilpotent decomposition? First, A must be square and, second,
(5.11.11) must be a similarity transformation, so U = V. Surprisingly, this
happens for a rather large class of matrices described below.
Range Perpendicular to Nullspace
For rank (AnÃ—n) = r, the following statements are equivalent:
â€¢
R (A) âŠ¥N (A),
(5.11.12)
â€¢
R (A) = R

AT 
,
(5.11.13)
â€¢
N (A) = N

AT 
,
(5.11.14)
â€¢
A = U

CrÃ—r
0
0
0

UT
(5.11.15)
in which U is orthogonal and C is nonsingular. Such matrices will
be called RPN matrices, short forâ€œrange perpendicular to nullspace.â€
Some authors call them range-symmetric or EP matrices. Nonsingular
matrices are trivially RPN because they have a zero nullspace. For com-
plex matrices, replace (â‹†)T by (â‹†)âˆ—and â€œorthogonalâ€ by â€œunitary.â€
Proof.
The fact that (5.11.12) â‡â‡’(5.11.13) â‡â‡’(5.11.14) is a direct conse-
quence of (5.11.5). It suï¬ƒces to prove (5.11.15) â‡â‡’(5.11.13). If (5.11.15) is a

5.11 Orthogonal Decomposition
409
URV factorization with V = U =

U1 | U2), then R (A) = R (U1) = R (V1) =
R

AT 
. Conversely, if R (A) = R

AT 
, perping both sides and using equation
(5.11.5) produces N (A) = N

AT 
, so (5.11.8) yields a URV factorization with
U = V.
Example 5.11.3
A âˆˆCnÃ—n is called a normal matrix whenever AAâˆ—= Aâˆ—A. As illustrated
in Figure 5.11.2, normal matrices ï¬ll the niche between hermitian and (complex)
RPN matrices in the sense that real-symmetric â‡’hermitian â‡’normal â‡’RPN,
with no implication being reversibleâ€”details are called for in Exercise 5.11.13.
RPN
Normal
Hermitian
Real-Symmetric
Nonsingular
Figure 5.11.2
Exercises for section 5.11
5.11.1. Verify the orthogonal decomposition theorem for A=

2
1
1
âˆ’1
âˆ’1
0
âˆ’2
âˆ’1
âˆ’1

.
5.11.2. For an inner-product space V, what is VâŠ¥? What is 0âŠ¥?
5.11.3. Find a basis for the orthogonal complement of M=span
ï£±
ï£²
ï£³
ï£«
ï£­
1
2
0
3
ï£¶
ï£¸,
ï£«
ï£­
2
4
1
6
ï£¶
ï£¸
ï£¼
ï£½
ï£¾.
5.11.4. For every inner-product space V, prove that if M âŠ†V, then MâŠ¥is
a subspace of V.
5.11.5. If M and N are subspaces of an n-dimensional inner-product space,
prove that the following statements are true.
(a)
M âŠ†N
=â‡’
N âŠ¥âŠ†MâŠ¥.
(b)
(M + N)âŠ¥= MâŠ¥âˆ©N âŠ¥.
(c)
(M âˆ©N)âŠ¥= MâŠ¥+ N âŠ¥.

410
Chapter 5
Norms, Inner Products, and Orthogonality
5.11.6. Explain why the rank plus nullity theorem on p. 199 is a corollary of the
orthogonal decomposition theorem.
5.11.7. Suppose A = URVT is a URV factorization of an m Ã— n matrix of
rank r, and suppose U is partitioned as U =

U1 | U2

, where U1
is m Ã— r. Prove that P = U1UT
1
is the projector onto R (A) along
N

AT 
. In this case, P is said to be an orthogonal projector because its
range is orthogonal to its nullspace. What is the orthogonal projector
onto N

AT 
along R (A)? (Orthogonal projectors are discussed in
more detail on p. 429.)
5.11.8. Use the Householder reduction method as described in Example 5.11.2
to compute a URV factorization as well as orthonormal bases for the
four fundamental subspaces of A =
 âˆ’4
âˆ’2
âˆ’4
âˆ’2
2
âˆ’2
2
1
âˆ’4
1
âˆ’4
âˆ’2

.
5.11.9. Compute a URV factorization for the matrix given in Exercise 5.11.8 by
using elementary row operations together with Gramâ€“Schmidt orthogo-
nalization. Are the results the same as those of Exercise 5.11.8?
5.11.10. For the matrix A of Exercise
5.11.8, ï¬nd vectors x âˆˆR (A) and
y âˆˆN

AT 
such that v = x + y, where v = ( 3
3
3 )T . Is there
more than one choice for x and y?
5.11.11. Construct a square matrix such that R (A) âˆ©N (A) = 0, but R (A) is
not orthogonal to N (A).
5.11.12. For AnÃ—n singular, explain why R (A) âŠ¥N (A) implies index(A) = 1,
but not conversely.
5.11.13. Prove that real-symmetric matrix â‡’hermitian â‡’normal â‡’(com-
plex) RPN. Construct examples to show that none of the implications
is reversible.
5.11.14. Let A be a normal matrix.
(a)
Prove that R (A âˆ’Î»I) âŠ¥N (A âˆ’Î»I) for every scalar Î».
(b)
Let Î» and Âµ be scalars such that A âˆ’Î»I and A âˆ’ÂµI are
singular matricesâ€”such scalars are called eigenvalues of A.
Prove that if Î» Ì¸= Âµ, then N (A âˆ’Î»I) âŠ¥N (A âˆ’ÂµI).

5.12 Singular Value Decomposition
411
5.12
SINGULAR VALUE DECOMPOSITION
For an m Ã— n matrix A of rank r, Example 5.11.2 shows how to build a URV
factorization
A = URVT = U

CrÃ—r
0
0
0

mÃ—n
VT
in which C is triangular. The purpose of this section is to prove that itâ€™s possible
to do even better by showing that C can be made to be diagonal. To see how,
let Ïƒ1 = âˆ¥Aâˆ¥2 = âˆ¥Câˆ¥2 (Exercise 5.6.9), and recall from the proof of (5.2.7) on
p. 281 that âˆ¥Câˆ¥2 = âˆ¥Cxâˆ¥2 for some vector x such that
(CT C âˆ’Î»I)x = 0,
where
âˆ¥xâˆ¥2 = 1 and Î» = xT CT Cx = Ïƒ2
1.
(5.12.1)
Set y = Cx/âˆ¥Cxâˆ¥2 = Cx/Ïƒ1, and let Ry =

y | Y

and Rx =

x | X

be
elementary reï¬‚ectors having y and x as their ï¬rst columns, respectivelyâ€”recall
Example 5.6.3. Reï¬‚ectors are orthogonal matrices, so xT X = 0 and YT y = 0,
and these together with (5.12.1) yield
yT CX = xT CT CX
Ïƒ1
= Î»xT X
Ïƒ1
= 0
and
YT Cx = Ïƒ1YT y = 0.
Coupling these facts with yT Cx = yT (Ïƒ1y) = Ïƒ1 and Ry = RT
y produces
RyCRx =
 yT
YT

C

x | X

=

yT Cx
yT CX
YT Cx
YT CX

=

Ïƒ1
0
0
C2

with Ïƒ1 â‰¥âˆ¥C2âˆ¥2 (because Ïƒ1 = âˆ¥Câˆ¥2 = max{Ïƒ1, âˆ¥C2âˆ¥} by (5.2.12)). Repeat-
ing the process on C2 yields reï¬‚ectors Sy, Sx such that
SyC2Sx =

Ïƒ2
0
0
C3

,
where Ïƒ2 â‰¥âˆ¥C3âˆ¥2 .
If P2 and Q2 are the orthogonal matrices
P2 =

1
0
0
Sy

Ry,
Q2 = Rx

1
0
0
Sx

, then P2CQ2 =
ï£«
ï£­
Ïƒ1
0
0
0
Ïƒ2
0
0
0
C3
ï£¶
ï£¸
in which Ïƒ1 â‰¥Ïƒ2 â‰¥âˆ¥C3âˆ¥2 . Continuing for r âˆ’1 times produces orthogonal
matrices Prâˆ’1 and Qrâˆ’1 such that Prâˆ’1CQrâˆ’1 = diag (Ïƒ1, Ïƒ2, . . . , Ïƒr) = D,
where Ïƒ1 â‰¥Ïƒ2 â‰¥Â· Â· Â· â‰¥Ïƒr. If ËœUT and ËœV are the orthogonal matrices
ËœUT =

Prâˆ’1
0
0
I

UT and ËœV = V

Qrâˆ’1
0
0
I

, then ËœUT A ËœV =

D
0
0
0

,
and thus the singular value decomposition (SVD) is derived.
57
57
The SVD has been independently discovered and rediscovered several times. Those credited
with the early developments include Eugenio Beltrami (1835â€“1899) in 1873, M. E. Camille
Jordan (1838â€“1922) in 1875, James J. Sylvester (1814â€“1897) in 1889, L. Autonne in 1913, and
C. Eckart and G. Young in 1936.

412
Chapter 5
Norms, Inner Products, and Orthogonality
Singular Value Decomposition
For each A âˆˆâ„œmÃ—n of rank r, there are orthogonal matrices UmÃ—m,
VnÃ—n and a diagonal matrix DrÃ—r = diag (Ïƒ1, Ïƒ2, . . . , Ïƒr) such that
A = U

D
0
0
0

mÃ—n
VT
with
Ïƒ1 â‰¥Ïƒ2 â‰¥Â· Â· Â· â‰¥Ïƒr > 0.
(5.12.2)
The
Ïƒi â€™s are called the nonzero singular values of
A.
When
r < p = min{m, n}, A is said to have p âˆ’r additional zero singular
values. The factorization in (5.12.2) is called a singular value decom-
position of A, and the columns in U and V are called left-hand and
right-hand singular vectors for A, respectively.
While the constructive method used to derive the SVD can be used as an
algorithm, more sophisticated techniques exist, and all good matrix computation
packages contain numerically stable SVD implementations. However, the details
of a practical SVD algorithm are too complicated to be discussed at this point.
The SVD is valid for complex matrices when (â‹†)T is replaced by (â‹†)âˆ—, and
it can be shown that the singular values are unique, but the singular vectors
are not. In the language of Chapter 7, the Ïƒ2
i â€™s are the eigenvalues of AT A,
and the singular vectors are specialized sets of eigenvectors for AT Aâ€”see the
summary on p. 555. In fact, the practical algorithm for computing the SVD is
an implementation of the QR iteration (p. 535) that is cleverly applied to AT A
without ever explicitly computing AT A.
Singular values reveal something about the geometry of linear transforma-
tions because the singular values Ïƒ1 â‰¥Ïƒ2 â‰¥Â· Â· Â· â‰¥Ïƒn of a matrix A tell us how
much distortion can occur under transformation by A. They do so by giving us
an explicit picture of how A distorts the unit sphere. To develop this, suppose
that A âˆˆâ„œnÃ—n is nonsingular (Exercise 5.12.5 treats the singular and rectangu-
lar case), and let S2 = {x | âˆ¥xâˆ¥2 = 1} be the unit 2-sphere in â„œn. The nature
of the image A(S2) is revealed by considering the singular value decompositions
A = UDVT
and
Aâˆ’1 = VDâˆ’1UT
with
D = diag (Ïƒ1, Ïƒ2, . . . , Ïƒn) ,
where U and V are orthogonal matrices. For each y âˆˆA(S2) there is an
x âˆˆS2 such that y = Ax, so, with w = UT y,
1 = âˆ¥xâˆ¥2
2 =
Aâˆ’1Ax
2
2 =
Aâˆ’1y
2
2 =
VDâˆ’1UT y
2
2 =
Dâˆ’1UT y
2
2
=
Dâˆ’1w
2
2 = w2
1
Ïƒ2
1
+ w2
2
Ïƒ2
2
+ Â· Â· Â· + w2
r
Ïƒ2r
.
(5.12.3)

5.12 Singular Value Decomposition
413
This means that UT A(S2) is an ellipsoid whose kth semiaxis has length
Ïƒk. Because orthogonal transformations are isometries (length preserving trans-
formations), UT can only aï¬€ect the orientation of A(S2) , so A(S2) is also an
ellipsoid whose kth semiaxis has length Ïƒk. Furthermore, (5.12.3) implies that
the ellipsoid UT A(S2) is in standard positionâ€”i.e., its axes are directed along
the standard basis vectors ek. Since U maps UT A(S2) to A(S2), and since
Uek = Uâˆ—k, it follows that the axes of A(S2) are directed along the left-hand
singular vectors deï¬ned by the columns of U. Therefore, the kth semiaxis of
A(S2) is ÏƒkUâˆ—k. Finally, since AV = UD implies AVâˆ—k = ÏƒkUâˆ—k, the right-
hand singular vector Vâˆ—k is a point on S2 that is mapped to the kth semiaxis
vector on the ellipsoid A(S2). The picture in â„œ3 looks like Figure 5.12.1.
1
A
Ïƒ1Uâˆ—1
Ïƒ2Uâˆ—2
Ïƒ3Uâˆ—3
Vâˆ—1
Vâˆ—2
Vâˆ—3
Figure 5.12.1
The degree of distortion of the unit sphere under transformation by A
is therefore measured by Îº2 = Ïƒ1/Ïƒn, the ratio of the largest singular value
to the smallest singular value. Moreover, from the discussion of induced ma-
trix norms (p. 280) and the unitary invariance of the 2-norm (Exercise 5.6.9),
max
âˆ¥xâˆ¥2=1 âˆ¥Axâˆ¥2 = âˆ¥Aâˆ¥2 =
UDVT 
2 = âˆ¥Dâˆ¥2 = Ïƒ1
and
min
âˆ¥xâˆ¥2=1 âˆ¥Axâˆ¥2 =
1
âˆ¥Aâˆ’1âˆ¥2
=
1
âˆ¥VDâˆ’1UT âˆ¥2
=
1
âˆ¥Dâˆ’1âˆ¥2
= Ïƒn.
In other words, longest and shortest vectors on A(S2) have respective lengths
Ïƒ1 = âˆ¥Aâˆ¥2 and Ïƒn = 1/
Aâˆ’1
2 (this justiï¬es Figure 5.2.1 on p. 281), so
Îº2 = âˆ¥Aâˆ¥2
Aâˆ’1
2 . This is called the 2-norm condition number of A. Diï¬€er-
ent norms result in condition numbers with diï¬€erent values but with more or
less the same order of magnitude as Îº2 (see Exercise 5.12.3), so the qualitative
information about distortion is the same. Below is a summary.

414
Chapter 5
Norms, Inner Products, and Orthogonality
Image of the Unit Sphere
For a nonsingular AnÃ—n having singular values Ïƒ1 â‰¥Ïƒ2 â‰¥Â· Â· Â· â‰¥Ïƒn
and an SVD A = UDVT with D = diag (Ïƒ1, Ïƒ2, . . . , Ïƒn) , the image
of the unit 2-sphere is an ellipsoid whose kth semiaxis is given by ÏƒkUâˆ—k
(see Figure 5.12.1). Furthermore, Vâˆ—k is a point on the unit sphere such
that AVâˆ—k = ÏƒkUâˆ—k. In particular,
â€¢
Ïƒ1 = âˆ¥AVâˆ—1âˆ¥2 = max
âˆ¥xâˆ¥2=1 âˆ¥Axâˆ¥2 = âˆ¥Aâˆ¥2,
(5.12.4)
â€¢
Ïƒn = âˆ¥AVâˆ—nâˆ¥2 = min
âˆ¥xâˆ¥2=1 âˆ¥Axâˆ¥2 = 1/âˆ¥Aâˆ’1âˆ¥2.
(5.12.5)
The degree of distortion of the unit sphere under transformation by A
is measured by the 2-norm condition number
â€¢
Îº2 = Ïƒ1
Ïƒn
= âˆ¥Aâˆ¥2
Aâˆ’1
2 â‰¥1.
(5.12.6)
Notice that Îº2 = 1 if and only if A is an orthogonal matrix.
The amount of distortion of the unit sphere under transformation by A
determines the degree to which uncertainties in a linear system Ax = b can be
magniï¬ed. This is explained in the following example.
Example 5.12.1
Uncertainties in Linear Systems. Systems of linear equations Ax = b aris-
ing in practical work almost always come with built-in uncertainties due to mod-
eling errors (because assumptions are almost always necessary), data collection
errors (because inï¬nitely precise gauges donâ€™t exist), and data entry errors (be-
cause numbers like
âˆš
2,
Ï€, and 2/3 canâ€™t be entered exactly). In addition,
roundoï¬€error in ï¬‚oating-point computation is a prevalent source of uncertainty.
In all cases itâ€™s important to estimate the degree of uncertainty in the solution
of Ax = b. This is not diï¬ƒcult when A is known exactly and all uncertainty
resides in the right-hand side. Even if this is not the case, itâ€™s sometimes possible
to aggregate uncertainties and shift all of them to the right-hand side.
Problem: Let Ax = b be a nonsingular system in which A is known exactly
but b is subject to an uncertainty e, and consider AËœx = b âˆ’e = Ëœb. Estimate
the relative uncertainty
58 âˆ¥x âˆ’Ëœxâˆ¥/ âˆ¥xâˆ¥in x in terms of the relative uncertainty
âˆ¥b âˆ’Ëœbâˆ¥/ âˆ¥bâˆ¥= âˆ¥eâˆ¥/ âˆ¥bâˆ¥in b. Use any vector norm and its induced matrix
norm (p. 280).
58
Knowing the absolute uncertainty âˆ¥x âˆ’Ëœxâˆ¥by itself may not be meaningful. For example, an
absolute uncertainty of a half of an inch might be ï¬ne when measuring the distance between
the earth and the moon, but itâ€™s not good in the practice of eye surgery.

5.12 Singular Value Decomposition
415
Solution: Use âˆ¥bâˆ¥= âˆ¥Axâˆ¥â‰¤âˆ¥Aâˆ¥âˆ¥xâˆ¥with x âˆ’Ëœx = Aâˆ’1e to write
âˆ¥x âˆ’Ëœxâˆ¥
âˆ¥xâˆ¥
=
Aâˆ’1e

âˆ¥xâˆ¥
â‰¤âˆ¥Aâˆ¥
Aâˆ’1 âˆ¥eâˆ¥
âˆ¥bâˆ¥
= Îº âˆ¥eâˆ¥
âˆ¥bâˆ¥,
(5.12.7)
where Îº = âˆ¥Aâˆ¥
Aâˆ’1 is a condition number as discussed earlier (Îº = Ïƒ1/Ïƒn
if the 2-norm is used). Furthermore, âˆ¥eâˆ¥= âˆ¥A(x âˆ’Ëœx)âˆ¥â‰¤âˆ¥Aâˆ¥âˆ¥(x âˆ’Ëœx)âˆ¥and
âˆ¥xâˆ¥â‰¤
Aâˆ’1 âˆ¥bâˆ¥imply
âˆ¥x âˆ’Ëœxâˆ¥
âˆ¥xâˆ¥
â‰¥
âˆ¥eâˆ¥
âˆ¥Aâˆ¥âˆ¥xâˆ¥â‰¥
âˆ¥eâˆ¥
âˆ¥Aâˆ¥âˆ¥Aâˆ’1âˆ¥âˆ¥bâˆ¥= 1
Îº
âˆ¥eâˆ¥
âˆ¥bâˆ¥.
This with (5.12.7) yields the following bounds on the relative uncertainty:
Îºâˆ’1 âˆ¥eâˆ¥
âˆ¥bâˆ¥â‰¤âˆ¥x âˆ’Ëœxâˆ¥
âˆ¥xâˆ¥
â‰¤Îº âˆ¥eâˆ¥
âˆ¥bâˆ¥,
where
Îº = âˆ¥Aâˆ¥
Aâˆ’1 .
(5.12.8)
In other words, when A is well conditioned (i.e., when Îº is smallâ€”see the rule
of thumb in Example 3.8.2 to get a feeling of what â€œsmallâ€ and â€œlargeâ€ might
mean), (5.12.8) insures that small relative uncertainties in b cannot greatly
aï¬€ect the solution, but when A is ill conditioned (i.e., when Îº is large), a
relatively small uncertainty in b might result in a relatively large uncertainty
in x. To be more sure, the following problem needs to be addressed.
Problem: Can equality be realized in each bound in (5.12.8) for every nonsin-
gular A, and if so, how?
Solution: Use the 2-norm, and let A = UDVT be an SVD so AVâˆ—k = ÏƒkUâˆ—k
for each k. If b and e are directed along left-hand singular vectors associated
with Ïƒ1 and Ïƒn, respectivelyâ€”say, b = Î²Uâˆ—1 and e = ÏµUâˆ—n, then
x = Aâˆ’1b = Aâˆ’1(Î²Uâˆ—1) = Î²Vâˆ—1
Ïƒ1
and
xâˆ’Ëœx = Aâˆ’1e = Aâˆ’1(ÏµUâˆ—n) = ÏµVâˆ—n
Ïƒn
,
so
âˆ¥x âˆ’Ëœxâˆ¥2
âˆ¥xâˆ¥2
=
 Ïƒ1
Ïƒn
 |Ïµ|
|Î²| = Îº2
âˆ¥eâˆ¥2
âˆ¥bâˆ¥2
when b = Î²Uâˆ—1 and e = ÏµUâˆ—n.
Thus the upper bound (the worst case) in (5.12.8) is attainable for all A. The
lower bound (the best case) is realized in the opposite situation when b and e
are directed along Uâˆ—n and Uâˆ—1, respectively. If b = Î²Uâˆ—n and e = ÏµUâˆ—1,
then the same argument yields x = Ïƒâˆ’1
n Î²Vâˆ—n and x âˆ’Ëœx = Ïƒâˆ’1
1 ÏµVâˆ—1, so
âˆ¥x âˆ’Ëœxâˆ¥2
âˆ¥xâˆ¥2
=
Ïƒn
Ïƒ1
 |Ïµ|
|Î²| = Îºâˆ’1
2
âˆ¥eâˆ¥2
âˆ¥bâˆ¥2
when b = Î²Uâˆ—n and e = ÏµUâˆ—1.

416
Chapter 5
Norms, Inner Products, and Orthogonality
Therefore, if A is well conditioned, then relatively small uncertainties in b canâ€™t
produce relatively large uncertainties in x. But when A is ill conditioned, itâ€™s
possible for relatively small uncertainties in b to have relatively large eï¬€ects on
x, and itâ€™s also possible for large uncertainties in b to have almost no eï¬€ect on
x. Since the direction of e is almost always unknown, we must guard against the
worst case and proceed with caution when dealing with ill-conditioned matrices.
Problem: What if there are uncertainties in both sides of Ax = b?
Solution: Use calculus to analyze the situation by considering the entries of
A = A(t) and b = b(t) to be diï¬€erentiable functions of a variable t, and
compute the relative size of the derivative of x = x(t) by diï¬€erentiating b = Ax
to obtain bâ€² = (Ax)â€² = Aâ€²x + Axâ€² (with â‹†â€² denoting d â‹†/dt ), so
âˆ¥xâ€²âˆ¥=
Aâˆ’1bâ€² âˆ’Aâˆ’1Aâ€²x
 â‰¤
Aâˆ’1bâ€² +
Aâˆ’1Aâ€²x

â‰¤
Aâˆ’1 âˆ¥bâ€²âˆ¥+
Aâˆ’1 âˆ¥Aâ€²âˆ¥âˆ¥xâˆ¥.
Consequently,
âˆ¥xâ€²âˆ¥
âˆ¥xâˆ¥â‰¤
Aâˆ’1 âˆ¥bâ€²âˆ¥
âˆ¥xâˆ¥
+
Aâˆ’1 âˆ¥Aâ€²âˆ¥
â‰¤âˆ¥Aâˆ¥
Aâˆ’1
âˆ¥bâ€²âˆ¥
âˆ¥Aâˆ¥âˆ¥xâˆ¥+ âˆ¥Aâˆ¥
Aâˆ’1 âˆ¥Aâ€²âˆ¥
âˆ¥Aâˆ¥
â‰¤Îºâˆ¥bâ€²âˆ¥
âˆ¥bâˆ¥+ Îºâˆ¥Aâ€²âˆ¥
âˆ¥Aâˆ¥= Îº
âˆ¥bâ€²âˆ¥
âˆ¥bâˆ¥+ âˆ¥Aâ€²âˆ¥
âˆ¥Aâˆ¥

.
In other words, the relative sensitivity of the solution is the sum of the relative
sensitivities of A and b magniï¬ed by Îº = âˆ¥Aâˆ¥
Aâˆ’1 . A discrete analog of
the above inequality is developed in Exercise 5.12.12.
Conclusion: In all cases, the credibility of the solution to Ax = b in the face
of uncertainties must be gauged in relation to the condition of A.
As the next example shows, the condition number is pivotal also in deter-
mining whether or not the residual r = b âˆ’AËœx is a reliable indicator of the
accuracy of an approximate solution Ëœx.
Example 5.12.2
Checking an Answer. Suppose that Ëœx is a computed (or otherwise approxi-
mate) solution for a nonsingular system Ax = b, and suppose the accuracy of
Ëœx is â€œcheckedâ€ by computing the residual r = b âˆ’AËœx. If r = 0, exactly,
then Ëœx must be the exact solution. But if r is not exactly zeroâ€”say, âˆ¥râˆ¥2 is
zero to t signiï¬cant digitsâ€”are we guaranteed that Ëœx is accurate to roughly t
signiï¬cant ï¬gures? This question was brieï¬‚y examined in Example 1.6.3, but itâ€™s
worth another look.
Problem: To what extent does the size of the residual reï¬‚ect the accuracy of
an approximate solution?

5.12 Singular Value Decomposition
417
Solution: Without realizing it, we answered this question in Example 5.12.1.
To bound the accuracy of Ëœx relative to the exact solution x, write r = b âˆ’AËœx
as AËœx = b âˆ’r, and apply (5.12.8) with e = r to obtain
Îºâˆ’1 âˆ¥râˆ¥2
âˆ¥bâˆ¥2
â‰¤âˆ¥x âˆ’Ëœxâˆ¥
âˆ¥xâˆ¥
â‰¤Îº âˆ¥râˆ¥2
âˆ¥bâˆ¥2
,
where
Îº = âˆ¥Aâˆ¥2
Aâˆ’1
2 .
(5.12.9)
Therefore, for a well-conditioned A, the residual r is relatively small if and
only if Ëœx is relatively accurate. However, as demonstrated in Example 5.12.1,
equality on either side of (5.12.9) is possible, so, when A is ill conditioned, a
very inaccurate approximation Ëœx can produce a small residual r, and a very
accurate approximation can produce a large residual.
Conclusion: Residuals are reliable indicators of accuracy only when A is well
conditionedâ€”if A is ill conditioned, residuals are nearly meaningless.
In addition to measuring the distortion of the unit sphere and gauging the
sensitivity of linear systems, singular values provide a measure of how close A
is to a matrix of lower rank.
Distance to Lower-Rank Matrices
If Ïƒ1 â‰¥Ïƒ2 â‰¥Â· Â· Â· â‰¥Ïƒr are the nonzero singular values of AmÃ—n, then
for each k < r, the distance from A to the closest matrix of rank k is
Ïƒk+1 =
min
rank(B)=k âˆ¥A âˆ’Bâˆ¥2.
(5.12.10)
Proof.
Suppose rank (BmÃ—n) = k, and let A = U
 D
0
0
0

VT be an SVD
for A with D = diag (Ïƒ1, Ïƒ2, . . . , Ïƒr) . Deï¬ne S = diag (Ïƒ1, . . . , Ïƒk+1), and
partition V =

FnÃ—k+1 | G

. Since rank (BF) â‰¤rank (B) = k (by (4.5.2)),
dim N (BF) = k+1âˆ’rank (BF) â‰¥1, so there is an x âˆˆN (BF) with âˆ¥xâˆ¥2 = 1.
Consequently, BFx = 0 and
AFx = U

D
0
0
0

VT Fx = U
ï£«
ï£­
S
0
0
0
â‹†
0
0
0
0
ï£¶
ï£¸
ï£«
ï£­
x
0
0
ï£¶
ï£¸= U
ï£«
ï£­
Sx
0
0
ï£¶
ï£¸.
Since âˆ¥A âˆ’Bâˆ¥2 = maxâˆ¥yâˆ¥2=1 âˆ¥(A âˆ’B)yâˆ¥2 , and since âˆ¥Fxâˆ¥2 = âˆ¥xâˆ¥2 = 1
(recall (5.2.4), p. 280, and (5.2.13), p. 283),
âˆ¥A âˆ’Bâˆ¥2
2 â‰¥âˆ¥(A âˆ’B)Fxâˆ¥2
2 = âˆ¥Sxâˆ¥2
2 =
k+1

i=1
Ïƒ2
i x2
i â‰¥Ïƒ2
k+1
k+1

i=1
x2
i = Ïƒ2
k+1.
Equality holds for Bk = U
 Dk
0
0
0

VT with Dk = diag (Ïƒ1, . . . , Ïƒk), and thus
(5.12.10) is proven.

418
Chapter 5
Norms, Inner Products, and Orthogonality
Example 5.12.3
Filtering Noisy Data. The SVD can be a useful tool in applications involving
the need to sort through noisy data and lift out relevant information. Suppose
that AmÃ—n is a matrix containing data that are contaminated with a certain
level of noiseâ€”e.g., the entries A might be digital samples of a noisy video or
audio signal such as that in Example 5.8.3 (p. 359). The SVD resolves the data
in A into r mutually orthogonal components by writing
A = U

DrÃ—r
0
0
0

VT =
r

i=1
ÏƒiuivT
i =
r

i=1
ÏƒiZi,
(5.12.11)
where Zi = uivT
i
and Ïƒ1 â‰¥Ïƒ2 â‰¥Â· Â· Â· â‰¥Ïƒr > 0. The matrices {Z1, Z2, . . . , Zr}
constitute an orthonormal set because
âŸ¨Zi ZjâŸ©= trace

ZT
i Zj

=

0
if i Ì¸= j,
1
if i = j.
In other words, the SVD (5.12.11) can be regarded as a Fourier expansion as
described on p. 299 and, consequently, Ïƒi = âŸ¨Zi AâŸ©can be interpreted as the
proportion of A lying in the â€œdirectionâ€ of Zi. In many applications the noise
contamination in A is random (or nondirectional) in the sense that the noise
is distributed more or less uniformly across the Ziâ€™s. That is, there is about as
much noise in the â€œdirectionâ€ of one Zi as there is in the â€œdirectionâ€ of any
other. Consequently, we expect each term ÏƒiZi to contain approximately the
same level of noise. This means that if SNR(ÏƒiZi) denotes the signal-to-noise
ratio in ÏƒiZi, then
SNR(Ïƒ1Z1) â‰¥SNR(Ïƒ2Z2) â‰¥Â· Â· Â· â‰¥SNR(ÏƒrZr),
more or less. If some of the singular values, say, Ïƒk+1, . . . , Ïƒr, are small relative to
(total noise)/r, then the terms Ïƒk+1Zk+1, . . . , ÏƒrZr have small signal-to-noise
ratios. Therefore, if we delete these terms from (5.12.11), then we lose a small part
of the total signal, but we remove a disproportionately large component of the
total noise in A. This explains why a truncated SVD Ak = k
i=1 ÏƒiZi can, in
many instances, ï¬lter out some of the noise without losing signiï¬cant information
about the signal in A. Determining the best value of k often requires empirical
techniques that vary from application to application, but looking for obvious
gaps between large and small singular values is usually a good place to start.
The next example presents an interesting application of this idea to building an
Internet search engine.

5.12 Singular Value Decomposition
419
Example 5.12.4
Search Engines. The ï¬ltering idea presented in Example 5.12.3 is widely used,
but a particularly novel application is the method of latent semantic indexing
used in the areas of information retrieval and text mining. You can think of
this in terms of building an Internet search engine. Start with a dictionary of
terms T1, T2, . . . , Tm. Terms are usually single words, but sometimes a term
may contain more that one word such as â€œlanding gear.â€ Itâ€™s up to you to decide
how extensive your dictionary should be, but even if you use the entire English
language, you probably wonâ€™t be using more than a few hundred-thousand terms,
and this is within the capacity of existing computer technology. Each document
(or web page) Dj of interest is scanned for key terms (this is called indexing the
document), and an associated document vector dj = (freq1j, freq2j, . . . , freqmj)T
is created in which
freqij = number of times term Ti occurs in document Dj.
(More sophisticated search engines use weighted frequency strategies.) After a
collection of documents D1, D2, . . . , Dn has been indexed, the associated docu-
ment vectors dj are placed as columns in a term-by-document matrix
AmÃ—n =

d1 | d2 Â· Â· Â· | dn

=
ï£«
ï£¬
ï£¬
ï£¬
ï£­
D1
D2
Â· Â· Â·
Dn
T1
freq11
freq12
Â· Â· Â·
freq1n
T2
freq21
freq22
Â· Â· Â·
freq2n
...
...
...
...
Tm
freqm1
freqm2
Â· Â· Â·
freqmn
ï£¶
ï£·
ï£·
ï£·
ï£¸.
Naturally, most entries in each document vector dj will be zero, so A is a
sparse matrixâ€”this is good because it means that sparse matrix technology can
be applied. When a query composed of a few terms is submitted to the search
engine, a query vector qT = (q1, q2, . . . , qn) is formed in which
qi =
5 1
if term Ti appears in the query,
0
otherwise.
(The qi â€™s might also be weighted.) To measure how well a query q matches a
document Dj, we check how close q is to dj by computing the magnitude of
cos Î¸j =
qT dj
âˆ¥qâˆ¥2 âˆ¥djâˆ¥2
=
qT Aej
âˆ¥qâˆ¥2 âˆ¥Aejâˆ¥2
.
(5.12.12)
If | cos Î¸j| â‰¥Ï„ for some threshold tolerance Ï„, then document Dj is con-
sidered relevant and is returned to the user. Selecting Ï„ is part art and part
science thatâ€™s based on experimentation and desired performance criteria. If the
columns of A along with q are initially normalized to have unit length, then

420
Chapter 5
Norms, Inner Products, and Orthogonality
|qT A| =

| cos Î¸1|, | cos Î¸2|, . . . , | cos Î¸n|

provides the information that allows
the search engine to rank the relevance of each document relative to the query.
However, due to things like variation and ambiguity in the use of vocabulary,
presentation style, and even the indexing process, there is a lot of â€œnoiseâ€ in
A, so the results in |qT A| are nowhere near being an exact measure of how
well query q matches the various documents. To ï¬lter out some of this noise,
the techniques of Example 5.12.3 are employed. An SVD A = r
i=1 ÏƒiuivT
i
is
judiciously truncated, and
Ak = UkDkVT
k =

u1 | Â· Â· Â· | uk

ï£«
ï£­
Ïƒ1
...
Ïƒk
ï£¶
ï£¸
ï£«
ï£¬
ï£­
vT
1
...
vT
k
ï£¶
ï£·
ï£¸=
k

i=1
ÏƒiuivT
i
is used in place of A in (5.12.12). In other words, instead of using cos Î¸j, query
q is compared with document Dj by using the magnitude of
cos Ï†j =
qT Akej
âˆ¥qâˆ¥2 âˆ¥Akejâˆ¥2
.
To make this more suitable for computation, set Sk = DkVT
k =

s1 | s2 | Â· Â· Â· | sk

,
and use
âˆ¥Akejâˆ¥2 =
UkDkVT
k ej

2 = âˆ¥Uksjâˆ¥2 = âˆ¥sjâˆ¥2
to write
cos Ï†j =
qT Uksj
âˆ¥qâˆ¥2 âˆ¥sjâˆ¥2
.
(5.12.13)
The vectors in Uk and Sk only need to be computed once (and they can be
determined without computing the entire SVD), so (5.12.13) requires very little
computation to process each new query. Furthermore, we can be generous in the
number of SVD components that are dropped because variation in the use of
vocabulary and the ambiguity of many words produces signiï¬cant noise in A.
Coupling this with the fact that numerical accuracy is not an important issue
(knowing a cosine to two or three signiï¬cant digits is suï¬ƒcient) means that we
are more than happy to replace the SVD of A by a low-rank truncation Ak,
where k is signiï¬cantly less than r.
Alternate Query Matching Strategy.
An alternate way to measuring how
close a given query q is to a document vector dj is to replace the query vector
q in (5.12.12) by the projected query 9q = PR(A)q, where PR(A) = UrUT
r is the
orthogonal projector onto R (A) along R (A)âŠ¥(Exercise 5.12.15) to produce
cos 9Î¸j =
9qT Aej
âˆ¥9qâˆ¥2 âˆ¥Aejâˆ¥2
.
(5.12.14)

5.12 Singular Value Decomposition
421
Itâ€™s proven on p. 435 that 9q = PR(A)q is the vector in R (A) (the document
space) that is closest to q, so using 9q in place of q has the eï¬€ect of using the
best approximation to q that is a linear combination of the document vectors
di. Since 9qT A = qT A and âˆ¥9qâˆ¥2 â‰¤âˆ¥qâˆ¥2 , it follows that cos 9Î¸j â‰¥cos Î¸j, so
more documents are deemed relevant when the projected query is used. Just as
in the unprojected query matching strategy, the noise is ï¬ltered out by replacing
A in (5.12.14) with a truncated SVD Ak = k
i=1 ÏƒiuivT
i . The result is
cos 9Ï†j =
qT Uksj
UT
k q

2 âˆ¥sjâˆ¥2
and, just as in (5.12.13), cos 9Ï†j is easily and quickly computed for each new
query q because Uk and sj need only be computed once.
The next example shows why singular values are the primary mechanism
for numerically determining the rank of a matrix.
Example 5.12.5
Perturbations and Numerical Rank. For A âˆˆâ„œmÃ—n with p = min{m, n},
let {Ïƒ1, Ïƒ2, . . . , Ïƒp} and {Î²1, Î²2, . . . , Î²p} be all singular values (nonzero as well
as any zero ones) for A and A + E, respectively.
Problem: Prove that
|Ïƒk âˆ’Î²k| â‰¤âˆ¥Eâˆ¥2
for each
k = 1, 2, . . . , p.
(5.12.15)
Solution: If the SVD for A given in (5.12.2) is written in the form
A =
p

i=1
ÏƒiuivT
i ,
and if we set
Akâˆ’1 =
kâˆ’1

i=1
ÏƒiuivT
i ,
then
Ïƒk = âˆ¥A âˆ’Akâˆ’1âˆ¥2 = âˆ¥A + E âˆ’Akâˆ’1 âˆ’Eâˆ¥2
â‰¥âˆ¥A + E âˆ’Akâˆ’1âˆ¥2 âˆ’âˆ¥Eâˆ¥2
(recall (5.1.6) on p. 273)
â‰¥Î²k âˆ’âˆ¥Eâˆ¥2
by (5.12.10).
Couple this with the observation that
Ïƒk = min
rank(B)=kâˆ’1 âˆ¥A âˆ’Bâˆ¥2 = min
rank(B)=kâˆ’1 âˆ¥A + E âˆ’B âˆ’Eâˆ¥2
â‰¤min
rank(B)=kâˆ’1 âˆ¥A + E âˆ’Bâˆ¥2 + âˆ¥Eâˆ¥2 = Î²k + âˆ¥Eâˆ¥2
to conclude that |Ïƒk âˆ’Î²k| â‰¤âˆ¥Eâˆ¥2.

422
Chapter 5
Norms, Inner Products, and Orthogonality
Problem: Explain why this means that computing the singular values of A
with any stable algorithm (one that returns the exact singular values Î²k of a
nearby matrix A + E) is a good way to compute rank (A).
Solution: If rank (A) = r, then p âˆ’r of the Ïƒk â€™s are exactly zero, so the
perturbation result (5.12.15) guarantees that pâˆ’r of the computed Î²k â€™s cannot
be larger than âˆ¥Eâˆ¥2. So if
Î²1 â‰¥Â· Â· Â· â‰¥Î²Ëœr > âˆ¥Eâˆ¥2 â‰¥Î²Ëœr+1 â‰¥Â· Â· Â· â‰¥Î²p,
then itâ€™s reasonable to consider Ëœr to be the numerical rank of A. For most
algorithms, âˆ¥Eâˆ¥2 is not known exactly, but adequate estimates of âˆ¥Eâˆ¥2 often
can be derived. Considerable eï¬€ort has gone into the development of stable al-
gorithms for computing singular values, but such algorithms are too involved
to discuss hereâ€”consult an advanced book on matrix computations. Gener-
ally speaking, good SVD algorithms have âˆ¥Eâˆ¥2 â‰ˆ5 Ã— 10âˆ’tâˆ¥Aâˆ¥2 when t-digit
ï¬‚oating-point arithmetic is used.
Just as the range-nullspace decomposition was used in Example 5.10.5 to
deï¬ne the Drazin inverse of a square matrix, a URV factorization or an SVD
can be used to deï¬ne a generalized inverse for rectangular matrices. For a URV
factorization
AmÃ—n = U

C
0
0
0

mÃ—n
VT ,
we deï¬ne
Aâ€ 
nÃ—m = V

Câˆ’1
0
0
0

nÃ—m
UT
to be the Mooreâ€“Penrose inverse (or the pseudoinverse) of A. (Replace
(â‹†)T by (â‹†)âˆ—when A âˆˆCmÃ—n. ) Although the URV factors are not uniquely
deï¬ned by A, it can be proven that Aâ€  is unique by arguing that Aâ€  is the
unique solution to the four Penrose equations
AAâ€ A = A,
Aâ€ AAâ€  = Aâ€ ,

AAâ€ T = AAâ€ ,

Aâ€ A
T = Aâ€ A,
so Aâ€  is the same matrix deï¬ned in Exercise 4.5.20. Since it doesnâ€™t matter
which URV factorization is used, we can use the SVD (5.12.2), in which case
C = D = diag (Ïƒ1, . . . , Ïƒr). Some â€œinverselikeâ€ properties that relate Aâ€  to
solutions and least squares solutions for linear systems are given in the following
summary. Other useful properties appear in the exercises.

5.12 Singular Value Decomposition
423
Mooreâ€“Penrose Pseudoinverse
â€¢
In terms of URV factors, the Mooreâ€“Penrose pseudoinverse of
AmÃ—n= U

CrÃ—r
0
0
0

VT
is Aâ€ 
nÃ—m= V

Câˆ’1
0
0
0

UT . (5.12.16)
â€¢
When Ax = b is consistent, x = Aâ€ b is the solution
of minimal euclidean norm.
(5.12.17)
â€¢
When Ax = b is inconsistent, x = Aâ€ b is the least
squares solution of minimal euclidean norm.
(5.12.18)
â€¢
When an SVD is used, C = D = diag (Ïƒ1, . . . , Ïƒr), so
Aâ€  = V

Dâˆ’1
0
0
0

UT =
r

i=1
viuT
i
Ïƒi
and
Aâ€ b =
r

i=1

uT
i b

Ïƒi
vi.
Proof.
To prove (5.12.17), suppose Ax0 = b, and replace A by AAâ€ A to
write b = Ax0 = AAâ€ Ax0 = AAâ€ b. Thus Aâ€ b solves Ax = b when it is
consistent. To see that Aâ€ b is the solution of minimal norm, observe that the
general solution is Aâ€ b+N (A) (a particular solution plus the general solution of
the homogeneous equation), so every solution has the form z = Aâ€ b+n, where
n âˆˆN (A). Itâ€™s not diï¬ƒcult to see that Aâ€ b âˆˆR

Aâ€ 
= R

AT 
(Exercise
5.12.16), so Aâ€ b âŠ¥n. Therefore, by the Pythagorean theorem (Exercise 5.4.14),
âˆ¥zâˆ¥2
2 =
Aâ€ b + n
2
2 =
Aâ€ b
2
2 + âˆ¥nâˆ¥2
2 â‰¥
Aâ€ b
2
2 .
Equality is possible if and only if n = 0, so Aâ€ b is the unique minimum
norm solution. When Ax = b is inconsistent, the least squares solutions are the
solutions of the normal equations AT Ax = AT b, and itâ€™s straightforward to
verify that Aâ€ b is one such solution (Exercise 5.12.16(c)). To prove that Aâ€ b
is the least squares solution of minimal norm, apply the same argument used in
the consistent case to the normal equations.
Caution! Generalized inverses are useful in formulating theoretical statements
such as those above, but, just as in the case of the ordinary inverse, generalized
inverses are not practical computational tools. In addition to being computation-
ally ineï¬ƒcient, serious numerical problems result from the fact that Aâ€  need

424
Chapter 5
Norms, Inner Products, and Orthogonality
not be a continuous function of the entries of A. For example,
A(x) =

1
0
0
x

=â‡’
Aâ€ (x) =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³

1
0
0
1/x

for x Ì¸= 0,

1
0
0
0

for x = 0.
Not only is Aâ€ (x) discontinuous in the sense that limxâ†’0 Aâ€ (x) Ì¸= Aâ€ (0), but
it is discontinuous in the worst way because as A(x) comes closer to A(0) the
matrix Aâ€ (x) moves farther away from Aâ€ (0). This type of behavior translates
into insurmountable computational diï¬ƒculties because small errors due to round-
oï¬€(or anything else) can produce enormous errors in the computed Aâ€ , and as
errors in A become smaller the resulting errors in Aâ€  can become greater. This
diabolical fact is also true for the Drazin inverse (p. 399). The inherent numeri-
cal problems coupled with the fact that itâ€™s extremely rare for an application to
require explicit knowledge of the entries of Aâ€  or AD constrains them to being
theoretical or notational tools. But donâ€™t underestimate this roleâ€”go back and
read Laplaceâ€™s statement quoted in the footnote on p. 81.
Example 5.12.6
Another way to view the URV or SVD factorizations in relation to the Mooreâ€“
Penrose inverse is to consider A/R(AT ) and Aâ€ 
/R(A), the restrictions of A and
Aâ€  to R

AT 
and R (A), respectively. Begin by making the straightforward
observations that R

Aâ€ 
= R

AT 
and N

Aâ€ 
= N

AT 
(Exercise 5.12.16).
Since â„œn = R

AT 
âŠ•N (A) and â„œm = R (A) âŠ•N

AT 
, it follows that
R (A) = A(â„œn) = A(R

AT 
) and R

AT 
= R

Aâ€ 
= Aâ€ (â„œm) = Aâ€ (R (A)).
In other words, A/R(AT ) and Aâ€ 
/R(A) are linear transformations such that
A/R(AT ) : R

AT 
â†’R (A)
and
Aâ€ 
/R(A) : R (A) â†’R

AT 
.
If B = {u1, u2, . . . , ur} and Bâ€² = {v1, v2, . . . , vr} are the ï¬rst r columns
from U =

U1 | U2

and V =

V1 | V2

in (5.11.11), then AV1 = U1C and
Aâ€ U1 = V1Câˆ’1 implies (recall (4.7.4)) that
1
A/R(AT )
2
Bâ€²B = C
and
1
Aâ€ 
/R(A)
2
BBâ€² = Câˆ’1.
(5.12.19)
If left-hand and right-hand singular vectors from the SVD (5.12.2) are used in
B and Bâ€², respectively, then C = D = diag (Ïƒ1, . . . , Ïƒr). Thus (5.12.19) reveals
the exact sense in which A and Aâ€  are â€œinverses.â€ Compare these results with
the analogous statements for the Drazin inverse in Example 5.10.5 on p. 399.

5.12 Singular Value Decomposition
425
Exercises for section 5.12
5.12.1. Following the derivation in the text, ï¬nd an SVD for
C =

âˆ’4
âˆ’6
3
âˆ’8

.
5.12.2. If Ïƒ1 â‰¥Ïƒ2 â‰¥Â· Â· Â· â‰¥Ïƒr are the nonzero singular values of A, then it can
be shown that the function Î½k(A) =

Ïƒ2
1 + Ïƒ2
2 + Â· Â· Â· + Ïƒ2
k
1/2 deï¬nes a
unitarily invariant norm (recall Exercise 5.6.9) for â„œmÃ—n (or CmÃ—n)
for each k = 1, 2, . . . , r. Explain why the 2-norm and the Frobenius
norm (p. 279) are the extreme cases in the sense that âˆ¥Aâˆ¥2
2 = Ïƒ2
1 and
âˆ¥Aâˆ¥2
F = Ïƒ2
1 + Ïƒ2
2 + Â· Â· Â· + Ïƒ2
r.
5.12.3. Each of the four common matrix norms can be bounded above and below
by a constant multiple of each of the other matrix norms. To be precise,
âˆ¥Aâˆ¥i â‰¤Î± âˆ¥Aâˆ¥j , where Î± is the (i, j)-entry in the following matrix.
ï£«
ï£¬
ï£¬
ï£­
1
2
âˆ
F
1
âˆ—
âˆšn
n
âˆšn
2
âˆšn
âˆ—
âˆšn
1
âˆ
n
âˆšn
âˆ—
âˆšn
F
âˆšn
âˆšn
âˆšn
âˆ—
ï£¶
ï£·
ï£·
ï£¸.
For analyzing limiting behavior, it therefore makes no diï¬€erence which
of these norms is used, so they are said to be equivalent matrix norms. (A
similar statement for vector norms was given in Exercise 5.1.8.) Explain
why the (2, F) and the (F, 2) entries are correct.
5.12.4. Prove that if Ïƒ1 â‰¥Ïƒ2 â‰¥Â· Â· Â· â‰¥Ïƒr are the nonzero singular values of a
rank r matrix A, and if âˆ¥Eâˆ¥2 < Ïƒr, then rank (A + E) â‰¥rank (A).
Note: This clariï¬es the meaning of the term â€œsuï¬ƒciently smallâ€ in the
assertion on p. 216 that small perturbations canâ€™t reduce rank.
5.12.5. Image of the Unit Sphere. Extend the result on p. 414 concerning
the image of the unit sphere to include singular and rectangular matrices
by showing that if Ïƒ1 â‰¥Ïƒ2 â‰¥Â· Â· Â· â‰¥Ïƒr > 0 are the nonzero singular
values of AmÃ—n, then the image A(S2) âŠ‚â„œm of the unit 2-sphere
S2 âŠ‚â„œn is an ellipsoid (possibly degenerate) in which the kth semiaxis
is ÏƒkUâˆ—k = AVâˆ—k, where Uâˆ—k and Vâˆ—k are respective left-hand and
right-hand singular vectors for A.

426
Chapter 5
Norms, Inner Products, and Orthogonality
5.12.6. Prove that if Ïƒr is the smallest nonzero singular value of AmÃ—n, then
Ïƒr =
min
âˆ¥xâˆ¥2=1
xâˆˆR(AT )
âˆ¥Axâˆ¥2 = 1/
Aâ€ 
2,
which is the generalization of (5.12.5).
5.12.7. Generalized Condition Number. Extend the bound in (5.12.8) to
include singular and rectangular matrices by showing that if x and
Ëœx are the respective minimum 2-norm solutions of consistent systems
Ax = b and AËœx = Ëœb = b âˆ’e, then
Îºâˆ’1 âˆ¥eâˆ¥
âˆ¥bâˆ¥â‰¤âˆ¥x âˆ’Ëœxâˆ¥
âˆ¥xâˆ¥
â‰¤Îº âˆ¥eâˆ¥
âˆ¥bâˆ¥,
where
Îº = âˆ¥Aâˆ¥
Aâ€  .
Can the same reasoning given in Example 5.12.1 be used to argue that
for âˆ¥â‹†âˆ¥2, the upper and lower bounds are attainable for every A?
5.12.8. Prove that if |Ïµ| < Ïƒ2
r for the smallest nonzero singular value of AmÃ—n,
then (AT A + ÏµI)âˆ’1 exists, and limÏµâ†’0(AT A + ÏµI)âˆ’1AT = Aâ€ .
5.12.9. Consider a system Ax = b in which
A =

.835
.667
.333
.266

,
and suppose b is subject to an uncertainty e. Using âˆ-norms, deter-
mine the directions of b and e that give rise to the worst-case scenario
in (5.12.8) in the sense that âˆ¥x âˆ’Ëœxâˆ¥âˆ/ âˆ¥xâˆ¥âˆ= Îºâˆâˆ¥eâˆ¥âˆ/ âˆ¥bâˆ¥âˆ.
5.12.10. An ill-conditioned matrix is suspected when a small pivot uii emerges
during the LU factorization of A because

Uâˆ’1
ii = 1/uii is then
large, and this opens the possibility of Aâˆ’1 = Uâˆ’1Lâˆ’1 having large
entries. Unfortunately, this is not an absolute test, and no guarantees
about conditioning can be made from the pivots alone.
(a)
Construct an example of a matrix that is well conditioned but
has a small pivot.
(b)
Construct an example of a matrix that is ill conditioned but has
no small pivots.

5.12 Singular Value Decomposition
427
5.12.11. Bound the relative uncertainty in the solution of a nonsingular system
Ax = b for which there is some uncertainty in A but not in b by
showing that if (Aâˆ’E)Ëœx = b, where Î± =
Aâˆ’1E
 < 1 for any matrix
norm such that âˆ¥Iâˆ¥= 1, then
âˆ¥x âˆ’Ëœxâˆ¥
âˆ¥xâˆ¥
â‰¤
Îº
1 âˆ’Î±
âˆ¥Eâˆ¥
âˆ¥Aâˆ¥,
where
Îº = âˆ¥Aâˆ¥
Aâˆ’1 .
Note: If the 2-norm is used, then âˆ¥Eâˆ¥2 < Ïƒn insures Î± < 1.
Hint: If B = Aâˆ’1E, then A âˆ’E = A(I âˆ’B), and Î± = âˆ¥Bâˆ¥< 1
=â‡’
Bk â‰¤âˆ¥Bâˆ¥k â†’0
=â‡’
Bk â†’0, so the Neumann series
expansion (p. 126) yields (I âˆ’B)âˆ’1 = âˆ
i=0 Bi.
5.12.12. Now bound the relative uncertainty in the solution of a nonsingular
system Ax = b for which there is some uncertainty in both A and b
by showing that if (A âˆ’E)Ëœx = b âˆ’e, where Î± =
Aâˆ’1E
 < 1 for any
matrix norm such that âˆ¥Iâˆ¥= 1, then
âˆ¥x âˆ’Ëœxâˆ¥
âˆ¥xâˆ¥
â‰¤
Îº
1 âˆ’Îº âˆ¥Eâˆ¥/ âˆ¥Aâˆ¥
 âˆ¥eâˆ¥
âˆ¥bâˆ¥+ âˆ¥Eâˆ¥
âˆ¥Aâˆ¥

,
where Îº = âˆ¥Aâˆ¥
Aâˆ’1 .
Note: If the 2-norm is used, then âˆ¥Eâˆ¥2 < Ïƒn insures Î± < 1. This
exercise underscores the conclusion of Example 5.12.1 stating that if A
is well conditioned, and if the relative uncertainties in A and b are
small, then the relative uncertainty in x must be small.
5.12.13. Consider the matrix A =
 âˆ’4
âˆ’2
âˆ’4
âˆ’2
2
âˆ’2
2
1
âˆ’4
1
âˆ’4
âˆ’2

.
(a)
Use the URV factorization you computed in Exercise 5.11.8 to
determine Aâ€ .
(b)
Now use the URV factorization you obtained in Exercise 5.11.9
to determine Aâ€ . Do your results agree with those of part (a)?
5.12.14. For matrix A in Exercise 5.11.8, and for b = ( âˆ’12
3
âˆ’9 )T , ï¬nd
the solution of Ax = b that has minimum euclidean norm.
5.12.15. Suppose A = URVT is a URV factorization (so it could be an SVD)
of an m Ã— n matrix of rank r, and suppose U is partitioned as U =

U1 | U2

, where U1 is m Ã— r. Prove that P = U1UT
1 = AAâ€  is the
projector onto R (A) along N

AT 
. In this case, P is said to be an or-
thogonal projector because its range is orthogonal to its nullspace. What
is the orthogonal projector onto N

AT 
along R (A)? (Orthogonal
projectors are discussed in more detail on p. 429.)

428
Chapter 5
Norms, Inner Products, and Orthogonality
5.12.16. Establish the following properties of Aâ€ .
(a)
Aâ€  = Aâˆ’1 when A is nonsingular.
(b)
(Aâ€ )
â€  = A.
(c)
(Aâ€ )
T = (AT )
â€  .
(d)
Aâ€  =

(AT A)âˆ’1AT
when rank (AmÃ—n) = n,
AT (AAT )âˆ’1
when rank (AmÃ—n) = m.
(e)
AT = AT AAâ€  = Aâ€ AAT for all A âˆˆâ„œmÃ—n.
(f)
Aâ€  = AT (AAT )â€  = (AT A)â€ AT for all A âˆˆâ„œmÃ—n.
(g)
R

Aâ€ 
= R

AT 
= R

Aâ€ A

, and
N

Aâ€ 
= N

AT 
= N

AAâ€ 
.
(h)
(PAQ)â€  = QT Aâ€ PT when P and Q are orthogonal matrices,
but in general (AB)â€  Ì¸= Bâ€ Aâ€  (the reverse-order law fails).
(i)
(AT A)â€  = Aâ€ (AT )â€  and (AAT )â€  = (AT )â€ Aâ€ .
5.12.17. Explain why Aâ€  = AD if and only if A is an RPN matrix.
5.12.18. Let X, Y âˆˆâ„œmÃ—n be such that R (X) âŠ¥R (Y).
(a)
Establish the Pythagorean theorem for matrices by proving
âˆ¥X + Yâˆ¥2
F = âˆ¥Xâˆ¥2
F + âˆ¥Yâˆ¥2
F .
(b)
Give an example to show that the result of part (a) does not
hold for the matrix 2-norm.
(c)
Demonstrate that Aâ€  is the best approximate inverse for A
in the sense that Aâ€  is the matrix of smallest Frobenius norm
that minimizes âˆ¥I âˆ’AXâˆ¥F .

5.13 Orthogonal Projection
429
5.13
ORTHOGONAL PROJECTION
As discussed in Â§5.9, every pair of complementary subspaces deï¬nes a projector.
But when the complementary subspaces happen to be orthogonal complements,
the resulting projector has some particularly nice properties, and the purpose of
this section is to develop this special case in more detail. Discussions are in the
context of real spaces, but generalizations to complex spaces are straightforward
by replacing (â‹†)T by (â‹†)âˆ—and â€œorthogonal matrixâ€ by â€œunitary matrix.â€
If M is a subspace of an inner-product space V, then V = M âŠ•MâŠ¥
by (5.11.1), and each v âˆˆV can be written uniquely as v = m + n, where
m âˆˆM and n âˆˆMâŠ¥by (5.9.3). The vector m was deï¬ned on p. 385 to be
the projection of v onto M along MâŠ¥, so the following deï¬nitions are natural.
Orthogonal Projection
For v âˆˆV, let v = m + n, where m âˆˆM and n âˆˆMâŠ¥.
â€¢
m is called the orthogonal projection of v onto M.
â€¢
The projector PM onto M along MâŠ¥is called the orthogonal
projector onto M.
â€¢
PM is the unique linear operator such that PMv = m (see p. 386).
These ideas are illustrated illustrated in Figure 5.13.1 for V = â„œ3.
Figure 5.13.1
Given an arbitrary pair of complementary subspaces M, N of â„œn, formula
(5.9.12) on p. 386 says that the projector P onto M along N is given by
P =

M | N
 
I
0
0
0
 
M | N
âˆ’1 =

M | 0

M | N
âˆ’1,
(5.13.1)
where the columns of M and N constitute bases for M and N, respectively.
So, how does this expression simplify when N = MâŠ¥? To answer the question,

430
Chapter 5
Norms, Inner Products, and Orthogonality
observe that if N = MâŠ¥, then NT M = 0 and MT N = 0. Furthermore, if
dim M = r, then MT M is r Ã— r, and rank

MT M

= rank (M) = r by
(4.5.4), so MT M is nonsingular. Therefore, if the columns of N are chosen to
be an orthonormal basis for MâŠ¥, then
ï£«
ï£­

MT M
âˆ’1 MT
NT
ï£¶
ï£¸
M | N

=

I
0
0
I

=â‡’

M | N
âˆ’1=
ï£«
ï£­

MT M
âˆ’1 MT
NT
ï£¶
ï£¸.
This together with (5.13.1) says the orthogonal projector onto M is given by
PM =

M | 0

ï£«
ï£­

MT M
âˆ’1 MT
NT
ï£¶
ï£¸= M

MT M
âˆ’1 MT .
(5.13.2)
As discussed in Â§5.9, the projector associated with any given pair of com-
plementary subspaces is unique, and it doesnâ€™t matter which bases are used to
form M and N in (5.13.1). Consequently, formula PM = M

MT M
âˆ’1 MT
is independent of the choice of M â€”just as long as its columns constitute some
basis for M. In particular, the columns of M need not be an orthonormal basis
for M. But if they are, then MT M = I, and (5.13.2) becomes PM = MMT .
Moreover, if the columns of M and N constitute orthonormal bases for M and
MâŠ¥, respectively, then U =

M | N

is an orthogonal matrix, and (5.13.1) be-
comes
PM = U

Ir
0
0
0

UT .
In other words, every orthogonal projector is orthogonally similar to a diagonal
matrix in which the diagonal entries are 1â€™s and 0â€™s.
Below is a summary of the formulas used to build orthogonal projectors.
Constructing Orthogonal Projectors
Let M be an r-dimensional subspace of â„œn, and let the columns of
MnÃ—r and NnÃ—nâˆ’r be bases for M and MâŠ¥, respectively. The or-
thogonal projectors onto M and MâŠ¥are
â€¢
PM = M

MT M
âˆ’1 MT and PMâŠ¥= N

NT N
âˆ’1 NT .
(5.13.3)
If M and N contain orthonormal bases for M and MâŠ¥, then
â€¢
PM = MMT and PMâŠ¥= NNT .
(5.13.4)
â€¢
PM = U

Ir
0
0
0

UT , where U =

M | N

.
(5.13.5)
â€¢
PMâŠ¥= I âˆ’PM in all cases.
(5.13.6)
Note: Extensions of (5.13.3) appear on p. 634.

5.13 Orthogonal Projection
431
Example 5.13.1
Problem: Let unÃ—1 Ì¸= 0, and consider the line L = span {u} . Construct the
orthogonal projector onto L, and then determine the orthogonal projection of
a vector xnÃ—1 onto L.
Solution: The vector u by itself is a basis for L, so, according to (5.13.3),
PL = u

uT u
âˆ’1 uT = uuT
uT u
is the orthogonal projector onto L. The orthogonal projection of a vector x
onto L is therefore given by
PLx = uuT
uT ux =
uT x
uT u

u.
Note: If âˆ¥uâˆ¥2 = 1, then PL = uuT , so PLx = uuT x = (uT x)u, and
âˆ¥PLxâˆ¥2 = |uT x| âˆ¥uâˆ¥2 = |uT x|.
This yields a geometrical interpretation for the magnitude of the standard inner
product. It says that if u is a vector of unit length in L, then, as illustrated
in Figure 5.13.2, |uT x| is the length of the orthogonal projection of x onto the
line spanned by u.
x
u
L
0
PLx
|uT x|
Figure 5.13.2
Finally, notice that since PL = uuT is the orthogonal projector onto L, it must
be the case that PLâŠ¥= I âˆ’PL = I âˆ’uuT is the orthogonal projection onto
LâŠ¥. This was called an elementary orthogonal projector on p. 322â€”go back
and reexamine Figure 5.6.1.
Example 5.13.2
Volume, Gramâ€“Schmidt, and QR. A solid in â„œm with parallel opposing
faces whose adjacent sides are deï¬ned by vectors from a linearly independent set
{x1, x2, . . . , xn} is called an n-dimensional parallelepiped. As shown in the
shaded portions of Figure 5.13.3, a two-dimensional parallelepiped is a parallel-
ogram, and a three-dimensional parallelepiped is a skewed rectangular box.

432
Chapter 5
Norms, Inner Products, and Orthogonality
x1
x2
âˆ¥x1âˆ¥
âˆ¥(I âˆ’P2)x2âˆ¥
x1
x2
x3
âˆ¥(I âˆ’P3)x3âˆ¥
Figure 5.13.3
Problem: Determine the volumes of a two-dimensional and a three-dimensional
parallelepiped, and then make the natural extension to deï¬ne the volume of an
n-dimensional parallelepiped.
Solution: In the two-dimensional case, volume is area, and itâ€™s evident from
Figure 5.13.3 that the area of the shaded parallelogram is the same as the area
of the dotted rectangle. The width of the dotted rectangle is Î½1 = âˆ¥x1âˆ¥2 , and
the height is Î½2 = âˆ¥(I âˆ’P2)x2âˆ¥2 , where P2 is the orthogonal projector onto
the space (line) spanned by x1, and I âˆ’P2 is the orthogonal projector onto
span {x1}âŠ¥. In other words, the area, V2, of the parallelogram is the length of
its base times its projected height, Î½2, so
V2 = âˆ¥x1âˆ¥2 âˆ¥(I âˆ’P2)x2âˆ¥2 = Î½1Î½2.
Similarly, the volume of a three-dimensional parallelepiped is the area of its
base times its projected height. The area of the base was just determined to be
V2 = âˆ¥x1âˆ¥2 âˆ¥(I âˆ’P2)x2âˆ¥2 = Î½1Î½2, and itâ€™s evident from Figure 5.13.3 that the
projected height is Î½3 = âˆ¥(I âˆ’P3)x3âˆ¥2 , where P3 is the orthogonal projector
onto span {x1, x2} . Therefore, the volume of the parallelepiped generated by
{x1, x2, x3} is
V3 = âˆ¥x1âˆ¥2 âˆ¥(I âˆ’P2)x2âˆ¥2 âˆ¥(I âˆ’P3)x3âˆ¥2 = Î½1Î½2Î½3.
Itâ€™s now clear how to inductively deï¬ne V4, V5, etc. In general, the volume of
the parallelepiped generated by a linearly independent set {x1, x2, . . . , xn} is
Vn = âˆ¥x1âˆ¥2 âˆ¥(I âˆ’P2)x2âˆ¥2 âˆ¥(I âˆ’P3)x3âˆ¥2 Â· Â· Â· âˆ¥(I âˆ’Pn)xnâˆ¥2 = Î½1Î½2 Â· Â· Â· Î½n,
where Pk is the orthogonal projector onto span {x1, x2, . . . , xkâˆ’1} , and where
Î½1 = âˆ¥x1âˆ¥2
and
Î½k = âˆ¥(I âˆ’Pk)xkâˆ¥2
for
k > 1.
(5.13.7)
Note that if {x1, x2, . . . , xn} is an orthogonal set, Vn = âˆ¥x1âˆ¥2 âˆ¥x2âˆ¥2 Â· Â· Â· âˆ¥xnâˆ¥2 ,
which is what we would expect.

5.13 Orthogonal Projection
433
Connections with Gramâ€“Schmidt and QR. Recall from (5.5.4) on p. 309
that the vectors in the Gramâ€“Schmidt sequence generated from a linearly inde-
pendent set {x1, x2, . . . , xn} âŠ‚â„œm are u1 = x1/ âˆ¥x1âˆ¥2 and
uk =

I âˆ’UkUT
k

xk

I âˆ’UkUT
k

xk

2
,
where
Uk =

u1 | u2 | Â· Â· Â· | ukâˆ’1

for k > 1.
Since {u1, u2, . . . , ukâˆ’1} is an orthonormal basis for span {x1, x2, . . . , xkâˆ’1} ,
it follows from (5.13.4) that UkUT
k
must be the orthogonal projector onto
span {x1, x2, . . . , xkâˆ’1} . Hence UkUT
k = Pk and (Iâˆ’Pk)xk = (Iâˆ’UkUT
k )xk,
so

I âˆ’UkUT
k

xk

2 = Î½k is the kth projected height in (5.13.7). This means
that when the Gramâ€“Schmidt equations are written in the form of a QR fac-
torization as explained on p. 311, the diagonal elements of the upper-triangular
matrix R are the Î½k â€™s. Consequently, the product of the diagonal entries in R
is the volume of the parallelepiped generated by the xk â€™s. But the QR factor-
ization of A =

x1 | x2 | Â· Â· Â· | xn

is unique (Exercise 5.5.8), so it doesnâ€™t matter
whether Gramâ€“Schmidt or another method is used to determine the QR factors.
Therefore, we arrive at the following conclusion.
â€¢
If AmÃ—n = QmÃ—nRnÃ—n is the (rectangular) QR factorization of a matrix
with linearly independent columns, then the volume of the n-dimensional
parallelepiped generated by the columns of A is Vn = Î½1Î½2 Â· Â· Â· Î½n, where
the Î½k â€™s are the diagonal elements of R. We will see on p. 468 what this
means in terms of determinants.
Of course, not all projectors are orthogonal projectors, so a natural question
to ask is, â€œWhat characteristic features distinguish orthogonal projectors from
more general oblique projectors?â€ Some answers are given below.
Orthogonal Projectors
Suppose that P âˆˆâ„œnÃ—n is a projectorâ€”i.e., P2 = P. The following
statements are equivalent to saying that P is an orthogonal projector.
â€¢
R (P) âŠ¥N (P).
(5.13.8)
â€¢
PT = P
(i.e., orthogonal projector â‡â‡’P2 = P = PT ). (5.13.9)
â€¢
âˆ¥Pâˆ¥2 = 1 for the matrix 2-norm (p. 281).
(5.13.10)
Proof.
Every projector projects vectors onto its range along (parallel to) its
nullspace, so statement (5.13.8) is essentially a restatement of the deï¬nition of
an orthogonal projector. To prove (5.13.9), note that if P is an orthogonal
projector, then (5.13.3) insures that P is symmetric. Conversely, if a projector

434
Chapter 5
Norms, Inner Products, and Orthogonality
P is symmetric, then it must be an orthogonal projector because (5.11.5) on
p. 405 allows us to write
P = PT
=â‡’
R (P) = R

PT 
=â‡’
R (P) âŠ¥N (P).
To see why (5.13.10) characterizes projectors that are orthogonal, refer back
to Example 5.9.2 on p. 389 (or look ahead to (5.15.3)) and note that âˆ¥Pâˆ¥2 =
1/ sin Î¸, where Î¸ is the angle between R (P) and N (P). This makes it clear
that âˆ¥Pâˆ¥2 â‰¥1 for all projectors, and âˆ¥Pâˆ¥2 = 1 if and only if Î¸ = Ï€/2, (i.e., if
and only if R (P) âŠ¥N (P) ).
Example 5.13.3
Problem: For A âˆˆâ„œmÃ—n such that rank (A) = r, describe the orthogonal
projectors onto each of the four fundamental subspaces of A.
Solution 1: Let BmÃ—r and NnÃ—nâˆ’r be matrices whose columns are bases for
R (A) and N (A), respectivelyâ€”e.g., B might contain the basic columns of
A. The orthogonal decomposition theorem on p. 405 says R (A)âŠ¥= N

AT 
and N (A)âŠ¥= R

AT 
, so, by making use of (5.13.3) and (5.13.6), we can write
PR(A) = B

BT B
âˆ’1 BT ,
PN(AT ) = PR(A)âŠ¥= I âˆ’PR(A) = I âˆ’B

BT B
âˆ’1 BT ,
PN(A) = N

NT N
âˆ’1 NT ,
PR(AT ) = PN(A)âŠ¥= I âˆ’PN(A) = I âˆ’N

NT N
âˆ’1 NT .
Note: If rank (A) = n, then all columns of A are basic and
PR(A) = A

AT A
âˆ’1 AT .
(5.13.11)
Solution 2: Another way to describe these projectors is to make use of the
Mooreâ€“Penrose pseudoinverse Aâ€  (p. 423). Recall that if A has a URV factor-
ization
A = U

C
0
0
0

VT ,
then
Aâ€  = V

Câˆ’1
0
0
0

UT ,
where U =

U1 | U2

and V =

V1 | V2

are orthogonal matrices in which the
columns of U1 and V1 constitute orthonormal bases for R (A) and R

AT 
,
respectively, and the columns of U2 and V2 are orthonormal bases for N

AT 
and N (A), respectively. Computing the products AAâ€  and Aâ€ A reveals
AAâ€  = U

I
0
0
0

UT = U1UT
1
and
Aâ€ A = V

I
0
0
0

VT = V1VT
1 ,

5.13 Orthogonal Projection
435
so, according to (5.13.4),
PR(A) = U1UT
1 = AAâ€ ,
PN(AT ) = I âˆ’PR(A) = I âˆ’AAâ€ ,
PR(AT ) = V1VT
1 = Aâ€ A,
PN(A) = I âˆ’PR(AT ) = I âˆ’Aâ€ A.
(5.13.12)
The notion of orthogonal projection in higher-dimensional spaces is consis-
tent with the visual geometry in â„œ2 and â„œ3. In particular, it is visually evident
from Figure 5.13.4 that if M is a subspace of â„œ3, and if b is a vector outside
of M, then the point in M that is closest to b is p = PMb, the orthogonal
projection of b onto M.
M
p = PMb
b
0
min
mâˆˆM âˆ¥b âˆ’mâˆ¥2
Figure 5.13.4
The situation is exactly the same in higher dimensions. But rather than using
our eyes to understand why, we use mathematicsâ€”itâ€™s surprising just how easy
it is to â€œseeâ€ such things in abstract spaces.
Closest Point Theorem
Let M be a subspace of an inner-product space V, and let b be a
vector in V. The unique vector in M that is closest to b is p = PMb,
the orthogonal projection of b onto M. In other words,
min
mâˆˆM âˆ¥b âˆ’mâˆ¥2 = âˆ¥b âˆ’PMbâˆ¥2 = dist (b, M).
(5.13.13)
This is called the orthogonal distance between b and M.
Proof.
If p = PMb, then p âˆ’m âˆˆM for all m âˆˆM, and
b âˆ’p = (I âˆ’PM)b âˆˆMâŠ¥,
so (p âˆ’m) âŠ¥(b âˆ’p). The Pythagorean theorem says âˆ¥x + yâˆ¥2 = âˆ¥xâˆ¥2 + âˆ¥yâˆ¥2
whenever x âŠ¥y (recall Exercise 5.4.14), and hence
âˆ¥b âˆ’mâˆ¥2
2 = âˆ¥b âˆ’p + p âˆ’mâˆ¥2
2 = âˆ¥b âˆ’pâˆ¥2
2 + âˆ¥p âˆ’mâˆ¥2
2 â‰¥âˆ¥p âˆ’mâˆ¥2
2 .

436
Chapter 5
Norms, Inner Products, and Orthogonality
In other words, minmâˆˆM âˆ¥b âˆ’mâˆ¥2 = âˆ¥b âˆ’pâˆ¥2 . Now argue that there is not
another point in M that is as close to b as p is. If
:m âˆˆM such that
âˆ¥b âˆ’:mâˆ¥2 = âˆ¥b âˆ’pâˆ¥2 , then by using the Pythagorean theorem again we see
âˆ¥b âˆ’:mâˆ¥2
2 = âˆ¥b âˆ’p + p âˆ’:mâˆ¥2
2 = âˆ¥b âˆ’pâˆ¥2
2 + âˆ¥p âˆ’:mâˆ¥2
2
=â‡’
âˆ¥p âˆ’:mâˆ¥2 = 0,
and thus :m = p.
Example 5.13.4
To illustrate some of the previous ideas, consider â„œnÃ—n with the inner product
âŸ¨A BâŸ©= trace

AT B

. If Sn is the subspace of n Ã— n real-symmetric matrices,
then each of the following statements is true.
â€¢
SâŠ¥
n = the subspace Kn of n Ã— n skew-symmetric matrices.
â–·Sn âŠ¥Kn because for all S âˆˆSn and K âˆˆKn,
âŸ¨S KâŸ©= trace

ST K

= âˆ’trace

SKT 
= âˆ’trace

SKT T
= âˆ’trace

KST 
= âˆ’trace

ST K

= âˆ’âŸ¨S KâŸ©
=â‡’
âŸ¨S KâŸ©= 0.
â–·â„œnÃ—n = Sn âŠ•Kn because every A âˆˆâ„œnÃ—n can be uniquely expressed
as the sum of a symmetric and a skew-symmetric matrix by writing
A = A + AT
2
+ A âˆ’AT
2
(recall (5.9.3) and Exercise 3.2.6).
â€¢
The orthogonal projection of A âˆˆâ„œnÃ—n onto Sn is P(A) = (A + AT )/2.
â€¢
The closest symmetric matrix to A âˆˆâ„œnÃ—n is P(A) = (A + AT )/2.
â€¢
The distance from A âˆˆâ„œnÃ—n to Sn (the deviation from symmetry) is
dist(A, Sn) = âˆ¥Aâˆ’P(A)âˆ¥F =
(Aâˆ’AT )/2

F =

trace (AT A)âˆ’trace (A2)
2
.
Example 5.13.5
Aï¬ƒne Projections.
If v Ì¸= 0 is a vector in a space V, and if M is a
subspace of V, then the set of points A = v + M is called an aï¬ƒne space in
V. Strictly speaking, A is not a subspace (e.g., it doesnâ€™t contain 0 ), but, as
depicted in Figure 5.13.5, A is the translate of a subspaceâ€”i.e., A is just a copy
of M that has been translated away from the origin through v. Consequently,
notions such as projection onto A and points closest to A are analogous to the
corresponding concepts for subspaces.

5.13 Orthogonal Projection
437
Problem: For b âˆˆV, determine the point p in A = v + M that is closest to
b. In other words, explain how to project b orthogonally onto A.
Solution: The trick is to subtract v from b as well as from everything in A
to put things back into the context of subspaces where we already know the
answers. As illustrated in Figure 5.13.5, this moves A back down to M, and it
translates v â†’0, b â†’(b âˆ’v), and p â†’(p âˆ’v).
0
p - v
b
v
p
A = v + M
b âˆ’v
p âˆ’v
M
0
Figure 5.13.5
If p is to be the orthogonal projection of b onto A, then p âˆ’v must be the
orthogonal projection of b âˆ’v onto M, so
p âˆ’v = PM(b âˆ’v)
=â‡’
p = v + PM(b âˆ’v),
(5.13.14)
and thus p is the point in A that is closest to b. Applications to the solution
of linear systems are developed in Exercises 5.13.17â€“5.13.22.
We are now in a position to replace the classical calculus-based theory of
least squares presented in Â§4.6 with a more modern vector space development.
In addition to being straightforward, the modern geometrical approach puts
the entire least squares picture in much sharper focus. Viewing concepts from
more than one perspective generally produces deeper understanding, and this is
particularly true for the theory of least squares.
Recall from p. 226 that for an inconsistent system AmÃ—nx = b, the object
of the least squares problem is to ï¬nd vectors x that minimize the quantity
(Ax âˆ’b)T (Ax âˆ’b) = âˆ¥Ax âˆ’bâˆ¥2
2 .
(5.13.15)
The classical development in Â§4.6 relies on calculus to argue that the set of vectors
x that minimize (5.13.15) is exactly the set that solves the (always consistent)
system of normal equations AT Ax = AT b. In the context of the closest point
theorem the least squares problem asks for vectors x such that Ax is as close

438
Chapter 5
Norms, Inner Products, and Orthogonality
to b as possible. But Ax is always a vector in R (A), and the closest point
theorem says that the vector in R (A) that is closest to b is PR(A)b, the
orthogonal projection of b onto R (A). Figure 5.13.6 illustrates the situation
in â„œ3.
b
0
R (A)
PR(A)b
PR(A)b
âˆ¥
âˆ’bâˆ¥2
=
min
xâˆˆâ„œn âˆ¥Ax âˆ’bâˆ¥2
Figure 5.13.6
So the least squares problem boils down to ï¬nding vectors x such that
Ax = PR(A)b.
But this system is equivalent to the system of normal equations because
Ax = PR(A)b â‡â‡’PR(A)Ax = PR(A)b
â‡â‡’PR(A)(Ax âˆ’b) = 0
â‡â‡’(Ax âˆ’b) âˆˆN

PR(A)

= R (A)âŠ¥= N

AT 
â‡â‡’AT (Ax âˆ’b) = 0
â‡â‡’AT Ax = AT b.
Characterizing the set of least squares solutions as the solutions to Ax = PR(A)b
makes it obvious that x = Aâ€ b is a particular least squares solution because
(5.13.12) insures AAâ€  = PR(A), and thus
A(Aâ€ b) = PR(A)b.
Furthermore, since Aâ€ b is a particular solution of Ax = PR(A)b, the general
solutionâ€”i.e., the set of all least squares solutionsâ€”must be the aï¬ƒne space
S = Aâ€ b + N (A). Finally, the fact that Aâ€ b is the least squares solution of
minimal norm follows from Example 5.13.5 together with
R

Aâ€ 
= R

AT 
= N (A)âŠ¥
(see part (g) of Exercise 5.12.16)
because (5.13.14) insures that the point in S that is closest to the origin is
p = Aâ€ b + PN(A)(0 âˆ’Aâ€ b) = Aâ€ b.
The classical development in Â§4.6 based on partial diï¬€erentiation is not easily
generalized to cover the case of complex matrices, but the vector space approach
given in this example trivially extends to complex matrices by simply replacing
(â‹†)T by (â‹†)âˆ—.
Below is a summary of some of the major points concerning the theory of
least squares.

5.13 Orthogonal Projection
439
Least Squares Solutions
Each of the following four statements is equivalent to saying that :x is a
least squares solution for a possibly inconsistent linear system Ax = b.
â€¢
âˆ¥A:x âˆ’bâˆ¥2 = min
xâˆˆâ„œn âˆ¥Ax âˆ’bâˆ¥2 .
(5.13.16)
â€¢
A:x = PR(A)b.
(5.13.17)
â€¢
AT A:x = AT b
( Aâˆ—A:x = Aâˆ—b when A âˆˆCmÃ—n ).
(5.13.18)
â€¢
:x âˆˆAâ€ b + N (A) ( Aâ€ b is the minimal 2-norm LSS).
(5.13.19)
Caution! These are valuable theoretical characterizations, but none is
recommended for ï¬‚oating-point computation. Directly solving (5.13.17)
or (5.13.18) or explicitly computing Aâ€  can be ineï¬ƒcient and numeri-
cally unstable. Computational issues are discussed in Example 4.5.1 on
p. 214; Example 5.5.3 on p. 313; and Example 5.7.3 on p. 346.
The least squares story will not be complete until the following fundamental
question is answered: â€œWhy is the method of least squares the best way to make
estimates of physical phenomena in the face of uncertainty?â€ This is the focal
point of the next section.
Exercises for section 5.13
5.13.1. Find the orthogonal projection of b onto M = span {u} , and then de-
termine the orthogonal projection of b onto MâŠ¥, where b = ( 4
8 )T
and u = ( 3
1 )T .
5.13.2. Let A =
ï£«
ï£­
1
2
0
2
4
1
1
2
0
ï£¶
ï£¸and b =
ï£«
ï£­
1
1
1
ï£¶
ï£¸.
(a)
Compute the orthogonal projectors onto each of the four funda-
mental subspaces associated with A.
(b)
Find the point in N (A)âŠ¥that is closest to b.
5.13.3. For an orthogonal projector P, prove that âˆ¥Pxâˆ¥2 = âˆ¥xâˆ¥2 if and only
if x âˆˆR (P).
5.13.4. Explain why AT PR(A) = AT for all A âˆˆâ„œmÃ—n.

440
Chapter 5
Norms, Inner Products, and Orthogonality
5.13.5. Explain why PM = r
i=1 uiuiT whenever B = {u1, u2, . . . , ur} is an
orthonormal basis for M âŠ†â„œnÃ—1.
5.13.6. Explain how to use orthogonal reduction techniques to compute the
orthogonal projectors onto each of the four fundamental subspaces of a
matrix A âˆˆâ„œmÃ—n.
5.13.7. (a)
Describe all 2 Ã— 2 orthogonal projectors in â„œ2Ã—2.
(b)
Describe all 2 Ã— 2 projectors in â„œ2Ã—2.
5.13.8. The line L in â„œn passing through two distinct points u and v is
L = u + span {u âˆ’v} . If u Ì¸= 0 and v Ì¸= Î±u, then L is a line not
passing through the originâ€”i.e., L is not a subspace. Sketch a picture
in â„œ2 or â„œ3 to visualize this, and then explain how to project a vector
b orthogonally onto L.
5.13.9. Explain why :x is a least squares solution for Ax = b if and only if
âˆ¥A:x âˆ’bâˆ¥2 =
PN(AT )b

2 .
5.13.10. Prove that if Îµ = A:x âˆ’b, where :x is a least squares solution for
Ax = b, then âˆ¥Îµâˆ¥2
2 = âˆ¥bâˆ¥2
2 âˆ’
PR(A)b
2
2 .
5.13.11. Let M be an r-dimensional subspace of â„œn. We know from (5.4.3)
that if B = {u1, u2, . . . , ur} is an orthonormal basis for M, and if
x âˆˆM, then x is equal to its Fourier expansion with respect to B.
That is, x = r
i=1(uiT x)ui. However, if x /âˆˆM, then equality is not
possible (why?), so the question that arises is, â€œWhat does the Fourier
expansion on the right-hand side of this expression represent?â€ Answer
this question by showing that the Fourier expansion r
i=1(uiT x)ui is
the point in M that is closest to x in the euclidean norm. In other
words, show that r
i=1(uiT x)ui = PMx.
5.13.12. Determine the orthogonal projection of b onto M, where
b =
ï£«
ï£¬
ï£­
5
2
5
3
ï£¶
ï£·
ï£¸
and
M = span
ï£±
ï£´
ï£²
ï£´
ï£³
ï£«
ï£¬
ï£­
âˆ’3/5
0
4/5
0
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
0
0
0
1
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
4/5
0
3/5
0
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
.
Hint: Is this spanning set in fact an orthonormal basis?

5.13 Orthogonal Projection
441
5.13.13. Let M and N be subspaces of a vector space V, and consider the
associated orthogonal projectors PM and PN .
(a)
Prove that PMPN = 0 if and only if M âŠ¥N.
(b)
Is it true that PMPN = 0 if and only if PN PM = 0? Why?
5.13.14. Let M and N be subspaces of the same vector space, and let PM
and PN be orthogonal projectors onto M and N, respectively.
(a)
Prove that R (PM + PN ) = R (PM) + R (PN ) = M + N.
Hint: Use Exercise 4.2.9 along with (4.5.5).
(b)
Explain why M âŠ¥N if and only if PMPN = 0.
(c)
Explain why PM + PN is an orthogonal projector if and only
if PMPN = 0, in which case R (PM + PN ) = M âŠ•N and
M âŠ¥N. Hint: Recall Exercise 5.9.17.
5.13.15. Andersonâ€“Duï¬ƒn Formula.
59 Prove that if M and N are subspaces
of the same vector space, then the orthogonal projector onto M âˆ©N
is given by PMâˆ©N = 2PM(PM + PN )â€ PN . Hint: Use (5.13.12) and
Exercise 5.13.14 to show PM(PM + PN )â€ PN = PN (PM + PN )â€ PM.
Argue that if Z = 2PM(PM+PN )â€ PM, then Z = PMâˆ©N Z = PMâˆ©N .
5.13.16. Given a square matrix X, the matrix exponential eX is deï¬ned as
eX = I + X + X2
2! + X3
3! + Â· Â· Â· =
âˆ

n=0
Xn
n! .
It can be shown that this series converges for all X, and it is legitimate
to diï¬€erentiate and integrate it term by term to produce the statements
deAt/dt = AeAt = eAtA and

eAtA dt = eAt.
(a)
Use the fact that limtâ†’âˆeâˆ’AT At = 0 for all A âˆˆâ„œmÃ—n to
show Aâ€  =
 âˆ
0
eâˆ’AT AtAT dt.
(b)
If limtâ†’âˆeâˆ’Ak+1t = 0, show AD =
 âˆ
0
eâˆ’Ak+1tAkdt, where
k = index(A).
60
(c)
For nonsingular matrices, show that if limtâ†’âˆeâˆ’At = 0, then
Aâˆ’1 =
 âˆ
0
eâˆ’Atdt.
59
W. N. Anderson, Jr., and R. J. Duï¬ƒn discovered this formula for the orthogonal projector onto
an intersection in 1969. They called PM(PM + PN )â€ PN the parallel sum of PM and PN
because it is the matrix generalization of the scalar function r1r2/(r1 + r2) = r1(r1 + r2)âˆ’1r2
that is the resistance of a circuit composed of two resistors r1 and r2 connected in parallel.
The simple elegance of the Andersonâ€“Duï¬ƒn formula makes it one of the innumerable little
sparkling facets in the jewel that is linear algebra.
60
A more useful integral representation for AD is given in Exercise 7.9.22 (p. 615).

442
Chapter 5
Norms, Inner Products, and Orthogonality
5.13.17. An aï¬ƒne space v + M âŠ†â„œn for which dim M = n âˆ’1 is called a
hyperplane. For example, a hyperplane in â„œ2 is a line (not necessarily
through the origin), and a hyperplane in â„œ3 is a plane (not necessarily
through the origin). The ith equation Aiâˆ—x = bi in a linear system
AmÃ—nx = b is a hyperplane in â„œn, so the solutions of Ax = b occur
at the intersection of the m hyperplanes deï¬ned by the rows of A.
(a)
Prove that for a given scalar Î² and a nonzero vector u âˆˆâ„œn,
the set H = {x | uT x = Î²} is a hyperplane in â„œn.
(b)
Explain why the orthogonal projection of b âˆˆâ„œn onto H is
p = b âˆ’

uT b âˆ’Î²/uT u

u.
5.13.18. For u, w âˆˆâ„œn such that uT w Ì¸= 0, let M = uâŠ¥and W = span {w} .
(a)
Explain why â„œn = M âŠ•W.
(b)
For b âˆˆâ„œnÃ—1, explain why the oblique projection of b onto
M along W is given by p = b âˆ’uT b/uT ww.
(c)
For a given scalar Î², let H be the hyperplane in â„œn deï¬ned
by H = {x | uT x = Î²}â€”see Exercise 5.13.17. Explain why the
oblique projection of b onto H along W should be given by
p = b âˆ’

uT b âˆ’Î²/uT w

w.
5.13.19. Kaczmarzâ€™s
61 Projection Method. The solution of a nonsingular
system

a11
a12
a21
a22
 
x1
x2

=

b1
b2

is the intersection of the two hyperplanes (lines in this case) deï¬ned by
H1={(x1, x2) | a11x1+a12x2 = b1} , H2={(x1, x2) | a21x1+a22x2 = b2}.
Itâ€™s visually evident that by starting with an arbitrary point p0 and
alternately projecting orthogonally onto H1 and H2 as depicted in
Figure 5.13.7, the resulting sequence of projections {p1, p2, p3, p4, . . . }
converges to H1 âˆ©H2, the solution of Ax = b.
61
Although this idea has probably occurred to many people down through the ages, credit is
usually given to Stefan Kaczmarz, who published his results in 1937. Kaczmarz was among a
school of bright young Polish mathematicians who were beginning to ï¬‚ower in the ï¬rst part
of the twentieth century. Tragically, this group was decimated by Hitlerâ€™s invasion of Poland,
and Kaczmarz himself was killed in military action while trying to defend his country.

5.13 Orthogonal Projection
443
Figure 5.13.7
This idea can be generalized by using Exercise 5.13.17. For a consis-
tent system AnÃ—rx = b with rank (A) = r, scale the rows so that
âˆ¥Aiâˆ—âˆ¥2 = 1 for each i, and let Hi = {x | Aiâˆ—x = bi} be the hyperplane
deï¬ned by the ith equation. Begin with an arbitrary vector p0 âˆˆâ„œrÃ—1,
and successively perform orthogonal projections onto each hyperplane
to generate the following sequence:
p1 = p0 âˆ’(A1âˆ—p0 âˆ’b1) (A1âˆ—)T
(project p0 onto H1 ),
p2 = p1 âˆ’(A2âˆ—p1 âˆ’b2) (A2âˆ—)T
(project p1 onto H2 ),
...
...
pn = pnâˆ’1 âˆ’(Anâˆ—pnâˆ’1 âˆ’bn) (Anâˆ—)T
(project pnâˆ’1 onto Hn ).
When all n hyperplanes have been used, continue by repeating the
process. For example, on the second pass project pn onto H1; then
project pn+1 onto H2, etc. For an arbitrary p0, the entire Kaczmarz
sequence is generated by executing the following double loop:
For k = 0, 1, 2, 3, . . .
For i = 1, 2, . . . , n
pkn+i = pkn+iâˆ’1 âˆ’(Aiâˆ—pkn+iâˆ’1 âˆ’bi) (Aiâˆ—)T
Prove that the Kaczmarz sequence converges to the solution of Ax = b
by showing âˆ¥pkn+i âˆ’xâˆ¥2
2 = âˆ¥pkn+iâˆ’1 âˆ’xâˆ¥2
2 âˆ’(Aiâˆ—pkn+iâˆ’1 âˆ’bi)2 .
5.13.20. Oblique Projection Method.
Assume that a nonsingular system
AnÃ—nx = b has been row scaled so that âˆ¥Aiâˆ—âˆ¥2 = 1 for each i, and let
Hi = {x | Aiâˆ—x = bi} be the hyperplane deï¬ned by the ith equationâ€”
see Exercise 5.13.17. In theory, the system can be solved by making nâˆ’1
oblique projections of the type described in Exercise 5.13.18 because if
an arbitrary point p1 in H1 is projected obliquely onto H2 along H1
to produce p2, then p2 is in H1âˆ©H2. If p2 is projected onto H3 along
H1 âˆ©H2 to produce p3, then p3 âˆˆH1 âˆ©H2 âˆ©H3, and so forth until
pn âˆˆâˆ©n
i=1Hi. This is similar to Kaczmarzâ€™s method given in Exercise
5.13.19, but here we are projecting obliquely instead of orthogonally.
However, projecting pk onto Hk+1 along âˆ©k
i=1Hi is diï¬ƒcult because

444
Chapter 5
Norms, Inner Products, and Orthogonality
âˆ©k
i=1Hi is generally unknown. This problem is overcome by modifying
the procedure as followsâ€”use Figure 5.13.8 with n = 3 as a guide.
Figure 5.13.8
Step 0.
Begin with any set

p(1)
1 , p(1)
2 , . . . , p(1)
n

âŠ‚H1 such that

p(1)
1
âˆ’p(1)
2

,

p(1)
1
âˆ’p(1)
3

, . . . ,

p(1)
1
âˆ’p(1)
n

is linearly independent
and A2âˆ—

p(1)
1
âˆ’p(1)
k

Ì¸= 0 for k = 2, 3, . . . , n.
Step 1.
In turn, project p(1)
1
onto H2 through p(1)
2 , p(1)
3 , . . . , p(1)
n
to
produce

p(2)
2 , p(2)
3 , . . . , p(2)
n

âŠ‚H1 âˆ©H2 (see Figure 5.13.8).
Step 2.
Project p(2)
2
onto H3 through p(2)
3 , p(2)
4 , . . . , p(2)
n
to produce

p(3)
3 , p(3)
4 , . . . , p(3)
n

âŠ‚H1 âˆ©H2 âˆ©H3. And so the process continues.
Step nâˆ’1.
Project p(nâˆ’1)
nâˆ’1
through p(nâˆ’1)
n
to produce p(n)
n
âˆˆâˆ©n
i=1Hi.
Of course, x = p(n)
n
is the solution of the system.
For any initial set {x1, x2, . . . , xn} âŠ‚H1 satisfying the properties
described in Step 0, explain why the following algorithm performs the
computations described in Steps 1, 2, . . . , n âˆ’1.
For i = 2 to n
For j = i to n
xj â†xj âˆ’(Aiâˆ—xiâˆ’1 âˆ’bi)
Aiâˆ—(xiâˆ’1 âˆ’xj)(xiâˆ’1 âˆ’xj)
x â†xn
(the solution of the system)
5.13.21. Let M be a subspace of â„œn, and let R = I âˆ’2PM. Prove that the
orthogonal distance between any point x âˆˆâ„œn and MâŠ¥is the same as
the orthogonal distance between Rx and MâŠ¥. In other words, prove
that R reï¬‚ects everything in â„œn about MâŠ¥. Naturally, R is called
the reï¬‚ector about MâŠ¥. The elementary reï¬‚ectors I âˆ’2uuT /uT u
discussed on p. 324 are special casesâ€”go back and look at Figure 5.6.2.

5.13 Orthogonal Projection
445
5.13.22. Cimminoâ€™s Reï¬‚ection Method. In 1938 the Italian mathematician
Gianfranco Cimmino used the following elementary observation to con-
struct an iterative algorithm for solving linear systems. For a 2 Ã— 2 sys-
tem Ax = b, let H1 and H2 be the two lines (hyperplanes) deï¬ned
by the two equations. For an arbitrary guess r0, let r1 be the reï¬‚ection
of r0 about the line H1, and let r2 be the reï¬‚ection of r0 about the
line H2. As illustrated in Figure 5.13.9, the three points r0, r1, and
r2 lie on a circle whose center is H1 âˆ©H2 (the solution of the system).
Figure 5.13.9
The mean value m = (r1 + r2)/2 is strictly inside the circle, so m is a
better approximation to the solution than r0. Itâ€™s visually evident that
iteration produces a sequence that converges to the solution of Ax = b.
Prove this in general by using the following blueprint.
(a)
For a scalar Î² and a vector u âˆˆâ„œn such that âˆ¥uâˆ¥2 = 1,
consider the hyperplane H = {x | uT x = Î²} (Exercise 5.13.17).
Use (5.6.8) to show that the reï¬‚ection of a vector b about H
is r = b âˆ’2(uT b âˆ’Î²)u.
(b)
For a system Ax = b in which the rows of A âˆˆâ„œnÃ—r have been
scaled so that âˆ¥Aiâˆ—âˆ¥2 = 1 for each i, let Hi = {x | Aiâˆ—x = bi}
be the hyperplane deï¬ned by the ith equation. If r0 âˆˆâ„œrÃ—1 is
an arbitrary vector, and if ri is the reï¬‚ection of r0 about Hi,
explain why the mean value of the reï¬‚ections {r1, r2, . . . , rn} is
m = r0 âˆ’(2/n)AT Îµ, where Îµ = Ar0 âˆ’b.
(c)
Iterating part (b) produces mk = mkâˆ’1 âˆ’(2/n)AT Îµkâˆ’1, where
Îµkâˆ’1 = Amkâˆ’1 âˆ’b. Show that if A is nonsingular, and if
x = Aâˆ’1b, then xâˆ’mk =

I âˆ’(2/n)AT A
k (xâˆ’m0). Note:
It can be proven that

I âˆ’(2/n)AT A
k â†’0 as k â†’âˆ, so
mk â†’x for all m0. In fact, mk converges even if A is rank
deï¬cientâ€”if consistent, it converges to a solution, and, if incon-
sistent, the limit is a least squares solution. Cimminoâ€™s method
also works with weighted means. If W = diag (w1, w2, . . . , wn),
where wi > 0 and  wi = 1, then mk = mkâˆ’1 âˆ’Ï‰AT WÎµkâˆ’1
is a convergent sequence in which 0 < Ï‰ < 2 is a â€œrelaxation
parameterâ€ that can be adjusted to alter the rate of convergence.

446
Chapter 5
Norms, Inner Products, and Orthogonality
5.14
WHY LEAST SQUARES?
Drawing inferences about natural phenomena based upon physical observations
and estimating characteristics of large populations by examining small samples
are fundamental concerns of applied science. Numerical characteristics of a phe-
nomenon or population are often called parameters, and the goal is to design
functions or rules called estimators that use observations or samples to estimate
parameters of interest. For example, the mean height h of all people is a pa-
rameter of the worldâ€™s population, and one way of estimating h is to observe
the mean height of a sample of k people. In other words, if hi is the height of
the ith person in a sample, the function Ë†h deï¬ned by
Ë†h(h1, h2, . . . , hk) = 1
k
 k

i=1
hi

is an estimator for h. Moreover, Ë†h is a linear estimator because Ë†h is a linear
function of the observations.
Good estimators should possess at least two propertiesâ€”they should be un-
biased and they should have minimal variance. For example, consider estimating
the center of a circle drawn on a wall by asking Larry, Moe, and Curly to each
throw one dart at the circle. To decide which estimator is best, we need to know
more about each throwerâ€™s style. While being able to throw a tight pattern, it is
known that Larry tends to have a left-hand bias in his style. Moe doesnâ€™t suï¬€er
from a bias, but he tends to throw a rather large pattern. However, Curly can
throw a tight pattern without a bias. Typical patterns are shown below.
Larry
Moe
Curly
Although Larry has a small variance, he is an unacceptable estimator be-
cause he is biased in the sense that his average is signiï¬cantly diï¬€erent than
the center. Moe and Curly are each unbiased estimators because they have an
average that is the center, but Curly is clearly the preferred estimator because
his variance is much smaller than Moeâ€™s. In other words, Curly is the unbiased
estimator of minimal variance.
To make these ideas more formal, letâ€™s adopt the following standard no-
tation and terminology from elementary probability theory concerning random
variables X and Y.

5.14 Why Least Squares?
447
â€¢
E[X] = ÂµX denotes the mean (or expected value) of X.
â€¢
Var[X] = E

(X âˆ’ÂµX)2
= E[X2] âˆ’Âµ2
X is the variance of X.
â€¢
Cov[X, Y ] = E[(X âˆ’ÂµX)(Y âˆ’ÂµY )] = E[XY ] âˆ’ÂµXÂµY is the covariance of
X and Y.
Minimum Variance Unbiased Estimators
An estimator Ë†Î¸ (consider as a random variable) for a parameter Î¸ is
said to be unbiased when E[Ë†Î¸] = Î¸, and Ë†Î¸ is called a minimum
variance unbiased estimator for Î¸ whenever Var[Ë†Î¸] â‰¤Var[Ë†Ï†] for
all unbiased estimators Ë†Ï† of Î¸.
These ideas make it possible to precisely articulate why the method of least
squares is the best way to ï¬t observed data. Let Y be a variable that is known
(or assumed) to be linearly related to other variables X1, X2, . . . , Xn according
to the equation
62
Y = Î²1X1 + Â· Â· Â· + Î²nXn,
(5.14.1),
where the Î²i â€™s are unknown constants (parameters). Suppose that the values
assumed by the Xi â€™s are not subject to error or variation and can be exactly
observed or speciï¬ed, but, due perhaps to measurement error, the values of Y
cannot be exactly observed. Instead, we observe
y = Y + Îµ = Î²1X1 + Â· Â· Â· + Î²nXn + Îµ,
(5.14.2)
where Îµ is a random variable accounting for the measurement error. For exam-
ple, consider the problem of determining the velocity v of a moving object by
measuring the distance D it has traveled at various points in time T by using
the linear relation D = vT. Time can be prescribed at exact values such as
T1 = 1 second, T2 = 2 seconds, etc., but observing the distance traveled at the
prescribed values of T will almost certainly involve small measurement errors so
that in reality the observed distances satisfy d = D + Îµ = vT + Îµ. Now consider
the general problem of determining the parameters Î²k in (5.14.1) by observing
(or measuring) values of Y at m diï¬€erent points Xiâˆ—= (xi1, xi2, . . . , xin) âˆˆâ„œn,
where xij is the value of Xj to be used when making the ith observation. If yi
denotes the random variable that represents the outcome of the ith observation
of Y, then according to (5.14.2),
yi = Î²1xi1 + Â· Â· Â· + Î²nxin + Îµi,
i = 1, 2, . . . , m,
(5.14.3)
62
Equation (5.14.1) is called a no-intercept model, whereas the slightly more general equation
Y = Î²0 + Î²1X1 + Â· Â· Â· + Î²nXn is known as an intercept model. Since the analysis for an
intercept model is not signiï¬cantly diï¬€erent from the analysis of the no-intercept case, we deal
only with the no-intercept case and leave the intercept model for the reader to develop.

448
Chapter 5
Norms, Inner Products, and Orthogonality
where Îµi is a random variable accounting for the ith observation (or mea-
surement) error.
63 It is generally valid to assume that observation errors are not
correlated with each other but have a common variance (not necessarily known)
and a zero mean. In other words, we assume that
E[Îµi] = 0 for each i
and
Cov[Îµi, Îµj] =

Ïƒ2
when i = j,
0
when i Ì¸= j.
If y =
ï£«
ï£¬
ï£¬
ï£­
y1
y2
...
ym
ï£¶
ï£·
ï£·
ï£¸, X =
ï£«
ï£¬
ï£¬
ï£­
x11
x12
Â· Â· Â·
x1n
x21
x22
Â· Â· Â·
x2n
...
...
...
...
xm1
xm2
Â· Â· Â·
xmn
ï£¶
ï£·
ï£·
ï£¸, Î² =
ï£«
ï£¬
ï£¬
ï£­
Î²1
Î²2
...
Î²n
ï£¶
ï£·
ï£·
ï£¸, Îµ =
ï£«
ï£¬
ï£¬
ï£­
Îµ1
Îµ2
...
Îµm
ï£¶
ï£·
ï£·
ï£¸,
then the equations in (5.14.3) can be written as y = XmÃ—nÎ² + Îµ. In practice,
the points Xiâˆ—at which observations yi are made can almost always be selected
to insure that rank (XmÃ—n) = n, so the complete statement of the standard
linear model is
y = XmÃ—nÎ² + Îµ
such that
ï£±
ï£´
ï£²
ï£´
ï£³
rank (X) = n,
E[Îµ] = 0,
Cov[Îµ] = Ïƒ2I,
(5.14.4)
where we have adopted the conventions
E[Îµ]=
ï£«
ï£¬
ï£¬
ï£­
E[Îµ1]
E[Îµ2]
...
E[Îµm]
ï£¶
ï£·
ï£·
ï£¸and Cov[Îµ]=
ï£«
ï£¬
ï£¬
ï£­
Cov[Îµ1, Îµ1]
Cov[Îµ1, Îµ2]
Â· Â· Â·
Cov[Îµ1, Îµm]
Cov[Îµ2, Îµ1]
Cov[Îµ2, Îµ2]
Â· Â· Â·
Cov[Îµ2, Îµm]
...
...
...
...
Cov[Îµm, Îµ1]
Cov[Îµm, Îµ2]
Â· Â· Â·
Cov[Îµm, Îµm]
ï£¶
ï£·
ï£·
ï£¸.
The problem is to determine the best (minimum variance) linear (linear function
of the yi â€™s) unbiased estimators for the components of Î². Gauss realized in 1821
that this is precisely what the least squares solution provides.
Gaussâ€“Markov Theorem
For the standard linear model (5.14.4), the minimum variance linear
unbiased estimator for Î²i is given by the ith component
Ë†Î²i in the
vector Ë†Î² =

XT X
âˆ’1XT y = Xâ€ y. In other words, the best linear
unbiased estimator for Î² is the least squares solution of XË†Î² = y.
63
In addition to observation and measurement errors, other errors such as modeling errors or
those induced by imposing simplifying assumptions produce the same kind of equationâ€”recall
the discussion of ice cream on p. 228.

5.14 Why Least Squares?
449
Proof.
It is clear that Ë†Î² = Xâ€ y is a linear estimator of Î² because each com-
ponent Ë†Î²i = 
k[Xâ€ ]ik yk is a linear function of the observations. The fact that
Ë†Î² is unbiased follows by using the linear nature of expected value to write
E[y] = E[XÎ² + Îµ] = E[XÎ²] + E[Îµ] = XÎ² + 0 = XÎ²,
so that
E
Ë†Î²

= E

Xâ€ y

= Xâ€ E[y] = Xâ€ XÎ² =

XT X
âˆ’1XT XÎ² = Î².
To argue that Ë†Î² = Xâ€ y has minimal variance among all linear unbiased estima-
tors for Î², let Î²âˆ—be an arbitrary linear unbiased estimator for Î². Linearity of
Î²âˆ—implies the existence of a matrix LnÃ—m such that Î²âˆ—= Ly, and unbiased-
ness insures Î² = E[Î²âˆ—] = E[Ly] = LE[y] = LXÎ². We want Î² = LXÎ² to hold
irrespective of the values of the components in Î², so it must be the case that
LX = In (recall Exercise 3.5.5). For i Ì¸= j we have
0 = Cov[Îµi, Îµj] = E[ÎµiÎµj] âˆ’ÂµÎµiÂµÎµj
=â‡’
E[ÎµiÎµj] = E[Îµi]E[Îµj] = 0,
so that
Cov[yi, yj] =

E[(yi âˆ’Âµyi)2] = E[Îµ2
i ] = Var[Îµi] = Ïƒ2
when i = j,
E[(yi âˆ’Âµyi)(yj âˆ’Âµyj)] = E[ÎµiÎµj] = 0
when i Ì¸= j.
(5.14.5)
This together with the fact that Var[aW +bZ] = a2Var[W]+b2Var[Z] whenever
Cov[W, Z] = 0 allows us to write
Var[Î²âˆ—
i ] = Var[Liâˆ—y] = Var
; m

k=1
likyk
<
= Ïƒ2
m

k=1
l2
ik = Ïƒ2 âˆ¥Liâˆ—âˆ¥2
2 .
Since LX = I, it follows that Var[Î²âˆ—
i ] is minimal if and only if Liâˆ—is the
minimum norm solution of the system zT X = eT
i . We know from (5.12.17) that
the (unique) minimum norm solution is given by zT = eT
i Xâ€  = Xâ€ 
iâˆ—, so Var[Î²âˆ—
i ]
is minimal if and only if Liâˆ—= Xâ€ 
iâˆ—. Since this holds for i = 1, 2, . . . , m, it follows
that L = Xâ€ . In other words, the components of Ë†Î² = Xâ€ y are the (unique)
minimal variance linear unbiased estimators for the parameters in Î².
Exercises for section 5.14
5.14.1. For a matrix ZmÃ—n = [zij], of random variables, E[Z] is deï¬ned to be
the m Ã— n matrix whose (i, j)-entry is E[zij]. Consider the standard
linear model described in (5.14.4), and let Ë†e denote the vector of random
variables deï¬ned by Ë†e = y âˆ’XË†Î² in which Ë†Î² =

XT X
âˆ’1XT y = Xâ€ y.
Demonstrate that
Ë†Ïƒ2 =
Ë†eT Ë†e
m âˆ’n
is an unbiased estimator for Ïƒ2. Hint: dT c = trace(cdT ) for column
vectors c and d, and, by virtue of Exercise 5.9.13,
trace

I âˆ’XXâ€ 
= m âˆ’trace

XXâ€ 
= m âˆ’rank

XXâ€ 
= m âˆ’n.

450
Chapter 5
Norms, Inner Products, and Orthogonality
5.15
ANGLES BETWEEN SUBSPACES
Consider the problem of somehow gauging the separation between a pair of
nontrivial but otherwise general subspaces M and N of â„œn. Perhaps the ï¬rst
thing that comes to mind is to measure the angle between them. But deï¬ning the
â€œangleâ€ between subspaces in â„œn is not as straightforward as the visual geometry
of â„œ2 or â„œ3 might suggest. There is just too much â€œwiggle roomâ€ in higher
dimensions to make any one deï¬nition completely satisfying, and the â€œcorrectâ€
deï¬nition usually varies with the speciï¬c application under consideration.
Before exploring general angles, recall what has already been said about
some special cases beginning with the angle between a pair of one-dimensional
subspaces. If M and N are spanned by vectors u and v, respectively, and if
âˆ¥uâˆ¥= 1 = âˆ¥vâˆ¥, then the angle between M and N is deï¬ned by the expression
cos Î¸ = vT u (p. 295). This idea was carried one step further on p. 389 to deï¬ne
the angle between two complementary subspaces, and an intuitive connection to
norms of projectors was presented. These intuitive ideas are now made rigorous.
Minimal Angle
The minimal angle between nonzero subspaces M, N âŠ†â„œn is deï¬ned
to be the number 0 â‰¤Î¸min â‰¤Ï€/2 for which
cos Î¸min =
max
uâˆˆM, vâˆˆN
âˆ¥uâˆ¥2=âˆ¥vâˆ¥2=1
vT u.
(5.15.1)
â€¢
If PM and PN are the orthogonal projectors onto M and N,
respectively, then
cos Î¸min = âˆ¥PN PMâˆ¥2 .
(5.15.2)
â€¢
If M and N are complementary subspaces, and if PMN is the
oblique projector onto M along N, then
sin Î¸min =
1
âˆ¥PMN âˆ¥2
.
(5.15.3)
â€¢
M and N are complementary subspaces if and only if PM âˆ’PN
is invertible, and in this case
sin Î¸min =
1
âˆ¥(PM âˆ’PN )âˆ’1âˆ¥2
.
(5.15.4)
Proof of (5.15.2).
If f : V â†’â„œis a function deï¬ned on a space V such that
f(Î±x) = Î±f(x) for all scalars Î± â‰¥0, then
max
âˆ¥xâˆ¥=1 f(x) = max
âˆ¥xâˆ¥â‰¤1 f(x)
(see Exercise 5.15.8).
(5.15.5)

5.15 Angles between Subspaces
451
This together with (5.2.9) and the fact that PMx âˆˆM and PN y âˆˆN means
cos Î¸min =
max
uâˆˆM, vâˆˆN
âˆ¥uâˆ¥2=âˆ¥vâˆ¥2=1
vT u =
max
uâˆˆM, vâˆˆN
âˆ¥uâˆ¥2â‰¤1, âˆ¥vâˆ¥2â‰¤1
vT u
=
max
âˆ¥xâˆ¥2â‰¤1, âˆ¥yâˆ¥2â‰¤1 yT PN PMx = âˆ¥PN PMâˆ¥2 .
Proof of (5.15.3).
Let U =

U1 | U2

and V =

V1 | V2

be orthogonal
matrices in which the columns of U1 and U2 constitute orthonormal bases for
M and MâŠ¥, respectively, and V1 and V2 are orthonormal bases for N âŠ¥
and N, respectively, so that UT
i Ui = I and VT
i Vi = I for i = 1, 2, and
PM = U1UT
1 , I âˆ’PM = U2UT
2 , PN = V2VT
2 , I âˆ’PN = V1VT
1 .
As discussed on p. 407, there is a nonsingular matrix C such that
PMN = U

C
0
0
0

VT = U1CVT
1 .
(5.15.6)
Notice that P2
MN = PMN
implies C = CVT
1 U1C, which in turn insures
Câˆ’1 = VT
1 U1. Recall that âˆ¥XAYâˆ¥2 = âˆ¥Aâˆ¥2 whenever X has orthonormal
columns and Y has orthonormal rows (Exercise 5.6.9). Consequently,
âˆ¥PMN âˆ¥2 = âˆ¥Câˆ¥2 =
1
min
âˆ¥xâˆ¥2=1
Câˆ’1x

2
=
1
min
âˆ¥xâˆ¥2=1
VT
1 U1x

2
(recall (5.2.6)).
Combining this with (5.15.2) produces (5.15.3) by writing
sin2 Î¸min = 1 âˆ’cos2 Î¸min = 1 âˆ’âˆ¥PN PMâˆ¥2
2 = 1 âˆ’
V2VT
2 U1UT
1
2
2
= 1 âˆ’
(I âˆ’V1VT
1 )U1
2
2 = 1 âˆ’max
âˆ¥xâˆ¥2=1
(I âˆ’V1VT
1 )U1x
2
2
= 1 âˆ’max
âˆ¥xâˆ¥2=1 xT UT
1 (I âˆ’V1VT
1 )U1x = 1 âˆ’max
âˆ¥xâˆ¥2=1

1 âˆ’
VT
1 U1x
2
2

= 1 âˆ’

1 âˆ’min
âˆ¥xâˆ¥2=1
VT
1 U1x
2
2

=
1
âˆ¥PMN âˆ¥2
2
.
Proof of (5.15.4).
Observe that
UT (PM âˆ’PN )V =
 UT
1
UT
2

(U1UT
1 âˆ’V2VT
2 )

V1 | V2

=

UT
1 V1
0
0
âˆ’UT
2 V2

,
(5.15.7)

452
Chapter 5
Norms, Inner Products, and Orthogonality
where UT
1 V1 = (Câˆ’1)T is nonsingular. To see that UT
2 V2 is also nonsingular,
suppose dim M = r so that dim N = n âˆ’r and UT
2 V2 is n âˆ’r Ã— n âˆ’r. Use
the formula for the rank of a product (4.5.1) to write
rank

UT
2 V2

= rank

UT
2

âˆ’dim N

UT
2

âˆ©R (V2) = nâˆ’râˆ’dim Mâˆ©N = nâˆ’r.
It now follows from (5.15.7) that PM âˆ’PN is nonsingular, and
VT (PM âˆ’PN )âˆ’1U =

(UT
1 V1)âˆ’1
0
0
âˆ’(UT
2 V2)âˆ’1

.
(Showing that PM âˆ’PN
is nonsingular implies M âŠ•N = â„œn is Exercise
5.15.6.) Formula (5.2.12) on p. 283 for the 2-norm of a block-diagonal matrix
can now be applied to yield
(PM âˆ’PN )âˆ’1
2 = max
5 (UT
1 V1)âˆ’1
2 ,
(UT
2 V2)âˆ’1
2
6
.
(5.15.8)
But
(UT
1 V1)âˆ’1
2 =
(UT
2 V2)âˆ’1
2 because we can again use (5.2.6) to write
1
(UT
1 V1)âˆ’12
2
= min
âˆ¥xâˆ¥2=1
UT
1 V1x
2
2 = min
âˆ¥xâˆ¥2=1 xT VT
1 U1UT
1 V1x
= min
âˆ¥xâˆ¥2=1 xT VT
1 (I âˆ’U2UT
2 )V1x
= min
âˆ¥xâˆ¥2=1(1 âˆ’xT VT
1 U2UT
2 V1x)
= 1 âˆ’max
âˆ¥xâˆ¥2=1
UT
2 V1x
2
2 = 1 âˆ’
UT
2 V1
2
2 .
By a similar argument, 1/
(UT
2 V2)âˆ’12
2 = 1âˆ’
UT
2 V1
2
2 (Exercise 5.15.11(a)).
Therefore,
(PM âˆ’PN )âˆ’1
2 =
(UT
1 V1)âˆ’1
2 =
CT 
2 = âˆ¥Câˆ¥2 = âˆ¥PMN âˆ¥2 .
While the minimal angle works ï¬ne for complementary spaces, it may not
convey much information about the separation between noncomplementary sub-
spaces. For example, Î¸min = 0 whenever M and N have a nontrivial inter-
section, but there nevertheless might be a nontrivial â€œgapâ€ between M and
N â€”look at Figure 5.15.1. Rather than thinking about angles to measure such a
gap, consider orthogonal distances as discussed in (5.13.13). Deï¬ne
Î´(M, N) = max
mâˆˆM
âˆ¥mâˆ¥2=1
dist (m, N) = max
mâˆˆM
âˆ¥mâˆ¥2=1
âˆ¥(I âˆ’PN )mâˆ¥2

5.15 Angles between Subspaces
453
to be the directed distance from M to N, and notice that Î´(M, N) â‰¤1
because (5.2.5) and (5.13.10) can be combined to produce
dist (m, N) = âˆ¥(I âˆ’PN )mâˆ¥2 = âˆ¥PN âŠ¥mâˆ¥2 â‰¤âˆ¥PN âŠ¥âˆ¥2 âˆ¥mâˆ¥2 = 1.
Figure 5.15.1 illustrates Î´(M, N) for two planes in â„œ3.
M
m
N
Î´(M, N) = max
mâˆˆM
âˆ¥mâˆ¥2=1
dist (m, N)
Figure 5.15.1
This picture is a bit misleading because Î´(M, N) = Î´(N, M) for this particular
situation. However, Î´(M, N) and Î´(N, M) need not always agreeâ€”thatâ€™s why
the phrase directed distance is used. For example, if M is the xy-plane in â„œ3
and N = span {(0, 1, 1)} , then Î´(N, M) = 1/
âˆš
2 while Î´(M, N) = 1. Con-
sequently, using orthogonal distance to gauge the degree of maximal separation
between an arbitrary pair of subspaces requires that both values of Î´ be taken
into account. Hence we make the following deï¬nition.
Gap Between Subspaces
The gap between subspaces M, N âŠ†â„œn is deï¬ned to be
gap (M, N) = max

Î´(M, N), Î´(N, M)

,
(5.15.9)
where Î´(M, N) = max
mâˆˆM
âˆ¥mâˆ¥2=1
dist (m, N).
Evaluating the gap between a given pair of subspaces requires knowing some
properties of directed distance. Observe that (5.15.5) together with the fact that
âˆ¥AT âˆ¥2 = âˆ¥Aâˆ¥2 can be used to write
Î´(M, N) = max
mâˆˆM
âˆ¥mâˆ¥2=1
dist (m, N) = max
mâˆˆM
âˆ¥mâˆ¥2=1
âˆ¥(I âˆ’PN )mâˆ¥2
= max
mâˆˆM
âˆ¥mâˆ¥2â‰¤1
âˆ¥(I âˆ’PN )mâˆ¥2 = max
âˆ¥xâˆ¥2=1 âˆ¥(I âˆ’PN )PMxâˆ¥2
= âˆ¥(I âˆ’PN )PMâˆ¥2 = âˆ¥PM(I âˆ’PN )âˆ¥2 .
(5.15.10)

454
Chapter 5
Norms, Inner Products, and Orthogonality
Similarly, Î´(N, M) = âˆ¥(I âˆ’PM)PN âˆ¥2 = âˆ¥PN (I âˆ’PM)âˆ¥2 . If U =

U1 | U2

and V =

V1 | V2

are the orthogonal matrices introduced on p. 451, then
Î´(M, N) = âˆ¥PM(I âˆ’PN )âˆ¥2 =
U1UT
1 V1VT
1

2 =
UT
1 V1

2
and
(5.15.11)
Î´(N, M) = âˆ¥(I âˆ’PM)PN âˆ¥2 =
U2UT
2 V2VT
2

2 =
UT
2 V2

2 .
Combining these observations with (5.15.7) leads us to conclude that
âˆ¥PM âˆ’PN âˆ¥2 = max
5 UT
1 V1

2 ,
UT
2 V2

2
6
= max

Î´(M, N), Î´(N, M)

= gap (M, N).
(5.15.12)
Below is a summary of these and other properties of the gap measure.
Gap Properties
The following statements are true for subspaces M, N âŠ†â„œn.
â€¢
gap (M, N) = âˆ¥PM âˆ’PN âˆ¥2 .
â€¢
gap (M, N) = max

âˆ¥(I âˆ’PN )PMâˆ¥2 , âˆ¥(I âˆ’PM)PN âˆ¥2

.
â€¢
gap (M, N) = 1 whenever dim M Ì¸= dim N.
(5.15.13)
â€¢
If dim M = dim N, then Î´(M, N) = Î´(N, M), and
â–·
gap (M, N) = 1 when MâŠ¥âˆ©N (or M âˆ©N âŠ¥) Ì¸= 0, (5.15.14)
â–·
gap (M, N) < 1 when MâŠ¥âˆ©N (or M âˆ©N âŠ¥) = 0. (5.15.15)
Proof of (5.15.13).
Suppose that dim M = r and dim N = k, where r < k.
Notice that this implies that MâŠ¥âˆ©N Ì¸= 0, for otherwise the formula for the
dimension of a sum (4.4.19) yields
n â‰¥dim(MâŠ¥+ N) = dim MâŠ¥+ dim N = n âˆ’r + k > n,
which is impossible. Thus there exists a nonzero vector x âˆˆMâŠ¥âˆ©N, and by
normalization we can take âˆ¥xâˆ¥2 = 1. Consequently, (Iâˆ’PM)x = x = PN x, so
âˆ¥(I âˆ’PM)PN xâˆ¥2 = 1. This insures that âˆ¥(I âˆ’PM)PN âˆ¥2 = 1, which implies
Î´(N, M) = 1.
Proof of (5.15.14).
Assume dim M = dim N = r, and use the formula for the
dimension of a sum along with (M âˆ©N âŠ¥)âŠ¥= MâŠ¥+ N (Exercise 5.11.5) to
conclude that
dim

MâŠ¥âˆ©N

= dim MâŠ¥+ dim N âˆ’dim

MâŠ¥+ N

= (n âˆ’r) + r âˆ’dim

M âˆ©N âŠ¥âŠ¥= dim

M âˆ©N âŠ¥
.

5.15 Angles between Subspaces
455
When dim

M âˆ©N âŠ¥
= dim

MâŠ¥âˆ©N

> 0, there are vectors x âˆˆMâŠ¥âˆ©N
and y âˆˆM âˆ©N âŠ¥such that âˆ¥xâˆ¥2 = 1 = âˆ¥yâˆ¥2 . Hence, âˆ¥(I âˆ’PM)PN xâˆ¥2 =
âˆ¥xâˆ¥2 = 1, and âˆ¥(I âˆ’PN )PMyâˆ¥2 = âˆ¥yâˆ¥2 = 1, so
Î´(N, M) = âˆ¥(I âˆ’PM)PN âˆ¥2 = 1 = âˆ¥(I âˆ’PN )PMâˆ¥2 = Î´(M, N).
Proof of (5.15.15).
If dim

M âˆ©N âŠ¥
= dim

MâŠ¥âˆ©N

= 0, then UT
2 V1 is
nonsingular because it is r Ã— r and has rank râ€”apply the formula (4.5.1) for
the rank of a product. From (5.15.11) we have
Î´2(M, N) =
UT
1 V1
2
2 =
U1UT
1 V1
2
2 =
(I âˆ’U2UT
2 )V1
2
2
= max
âˆ¥xâˆ¥2=1 xT VT
1 (I âˆ’U2UT
2 )V1x = max
âˆ¥xâˆ¥2=1

1 âˆ’
UT
2 V1x
2
2

= 1 âˆ’min
âˆ¥xâˆ¥2=1
UT
2 V1x
2
2 = 1 âˆ’
1
(UT
2 V1)âˆ’12
2
< 1 (recall (5.2.6)).
A similar argument shows Î´2(N, M) =
UT
2 V2
2
2 = 1 âˆ’1/
(UT
2 V1)âˆ’12
2 (Ex-
ercise 5.15.11(b)), so Î´(N, M) = Î´(M, N) < 1.
Because 0 â‰¤gap (M, N) â‰¤1, the gap measure deï¬nes another angle be-
tween M and N.
Maximal Angle
The maximal angle between subspaces M, N âŠ†â„œn is deï¬ned to be
the number 0 â‰¤Î¸max â‰¤Ï€/2 for which
sin Î¸max = gap (M, N) = âˆ¥PM âˆ’PN âˆ¥2 .
(5.15.16)
For applications requiring knowledge of the degree of separation between
a pair of nontrivial complementary subspaces, the minimal angle does the job.
Similarly, the maximal angle adequately handles the task for subspaces of equal
dimension. However, neither the minimal nor maximal angle may be of much
help for more general subspaces. For example, if M and N
are subspaces
of unequal dimension that have a nontrivial intersection, then Î¸min = 0 and
Î¸max = Ï€/2, but neither of these numbers might convey the desired information.
Consequently, it seems natural to try to formulate deï¬nitions of â€œintermediateâ€
angles between Î¸min and Î¸max. There are a host of such angles known as the
principal or canonical angles, and they are derived as follows.

456
Chapter 5
Norms, Inner Products, and Orthogonality
Let k = min{dim M, dim N}, and set M1 = M, N1 = N, and Î¸1 = Î¸min.
Let u1 and v1 be vectors of unit 2-norm such that the following maximum is
attained when u = u1 and v = v1 :
cos Î¸min =
max
uâˆˆM, vâˆˆN
âˆ¥uâˆ¥2=âˆ¥vâˆ¥2=1
vT u = vT
1 u1.
Set
M2 = uâŠ¥
1 âˆ©M1
and
N2 = vâŠ¥
1 âˆ©N1,
and deï¬ne the second principal angle Î¸2 to be the minimal angle between M2
and N2. Continue in this mannerâ€”e.g., if u2 and v2 are vectors such that
âˆ¥u2âˆ¥2 = 1 = âˆ¥v2âˆ¥2 and
cos Î¸2 =
max
uâˆˆM2, vâˆˆN2
âˆ¥uâˆ¥2=âˆ¥vâˆ¥2=1
vT u = vT
2 u2,
set
M3 = uâŠ¥
2 âˆ©M2
and
N3 = vâŠ¥
2 âˆ©N2,
and deï¬ne the third principal angle Î¸3 to be the minimal angle between M3
and N3. This process is repeated k times, at which point one of the subspaces
is zero. Below is a summary.
Principal Angles
For nonzero subspaces M, N âŠ†â„œn with k = min{dim M, dim N},
the principal angles between M = M1 and N = N1 are recursively
deï¬ned to be the numbers 0 â‰¤Î¸i â‰¤Ï€/2 such that
cos Î¸i =
max
uâˆˆMi, vâˆˆNi
âˆ¥uâˆ¥2=âˆ¥vâˆ¥2=1
vT u = vT
i ui,
i = 1, 2, . . . , k,
where âˆ¥uiâˆ¥2 = 1 = âˆ¥viâˆ¥2 , Mi = uâŠ¥
iâˆ’1âˆ©Miâˆ’1, and Ni = vâŠ¥
iâˆ’1âˆ©Niâˆ’1.
â€¢
Itâ€™s possible to prove that Î¸min = Î¸1 â‰¤Î¸2 â‰¤Â· Â· Â· â‰¤Î¸k â‰¤Î¸max, where
Î¸k = Î¸max when dim M = dim N.
â€¢
The vectors ui and vi are not uniquely deï¬ned, but the Î¸i â€™s are
unique. In fact, it can be proven that the sin Î¸i â€™s are singular values
(p. 412) for PM âˆ’PN . Furthermore, if dim M â‰¥dim N = k,
then the cos Î¸i â€™s are the singular values of VT
2 U1, and the sin Î¸i â€™s
are the singular values of VT
2 U2UT
2 , where U =

U1 | U2

and
V =

V1 | V2

are the orthogonal matrices from p. 451.

5.15 Angles between Subspaces
457
Exercises for section 5.15
5.15.1. Determine the angles Î¸min and Î¸max between the following subspaces
of â„œ3.
(a)
M = xy-plane,
N = span {(1, 0, 0), (0, 1, 1)} .
(b)
M = xy-plane,
N = span {(0, 1, 1)} .
5.15.2. Determine the principal angles between the following subspaces of â„œ3.
(a)
M = xy-plane,
N = span {(1, 0, 0), (0, 1, 1)} .
(b)
M = xy-plane,
N = span {(0, 1, 1)} .
5.15.3. Let Î¸min be the minimal angle between nonzero subspaces M, N âŠ†â„œn.
(a)
Explain why Î¸max = 0 if and only if M = N.
(b)
Explain why Î¸min = 0 if and only if M âˆ©N Ì¸= 0.
(c)
Explain why Î¸min = Ï€/2 if and only if M âŠ¥N.
5.15.4. Let Î¸min be the minimal angle between nonzero subspaces M, N âŠ‚â„œn,
and let Î¸âŠ¥
min denote the minimal angle between MâŠ¥and N âŠ¥. Prove
that if M âŠ•N = â„œn, then Î¸min = Î¸âŠ¥
min.
5.15.5. For nonzero subspaces M, N âŠ‚â„œn, let ËœÎ¸min denote the minimal angle
between M and N âŠ¥, and let Î¸max be the maximal angle between M
and N. Prove that if M âŠ•N âŠ¥= â„œn, then cos ËœÎ¸min = sin Î¸max.
5.15.6. For subspaces M, N âŠ†â„œn, prove that PM âˆ’PN is nonsingular if and
only if M and N are complementary.
5.15.7. For complementary spaces M, N âŠ‚â„œn, let P = PMN be the oblique
projector onto M along N, and let Q = PMâŠ¥N âŠ¥be the oblique
projector onto MâŠ¥along N âŠ¥.
(a)
Prove that (PM âˆ’PN )âˆ’1 = P âˆ’Q.
(b)
If Î¸min is the minimal angle between M and N, explain why
sin Î¸min =
1
âˆ¥P âˆ’Qâˆ¥2
.
(c)
Explain why âˆ¥P âˆ’Qâˆ¥2 = âˆ¥Pâˆ¥2 .

458
Chapter 5
Norms, Inner Products, and Orthogonality
5.15.8. Prove that if f : V â†’â„œis a function deï¬ned on a space V such that
f(Î±x) = Î±f(x) for scalars Î± â‰¥0, then
max
âˆ¥xâˆ¥=1 f(x) = max
âˆ¥xâˆ¥â‰¤1 f(x).
5.15.9. Let M and N be nonzero complementary subspaces of â„œn.
(a)
Explain why PMN =

(I âˆ’PN )PM
â€ , where PM and PN
are the orthogonal projectors onto M and N, respectively,
and PMN is the oblique projector onto M along N.
(b)
If Î¸min is the minimal angle between M and N, explain why
sin Î¸min =


(I âˆ’PN )PM
â€ 
âˆ’1
2
=


PM(I âˆ’PN )
â€ 
âˆ’1
2
=


(I âˆ’PM)PN
â€ 
âˆ’1
2
=


PN (I âˆ’PM)
â€ 
âˆ’1
2
.
5.15.10. For complementary subspaces M, N âŠ‚â„œn, let Î¸min be the minimal
angle between M and N, and let Â¯Î¸min denote the minimal angle be-
tween M and N âŠ¥.
(a)
If PMN is the oblique projector onto M along N, prove that
cos Â¯Î¸min =
Pâ€ 
MN

2 .
(b)
Explain why sin Î¸min â‰¤cos Â¯Î¸min.
5.15.11. Let U =

U1 | U2

and V =

V1 | V2

be the orthogonal matrices
deï¬ned on p. 451.
(a)
Prove that if UT
2 V2 is nonsingular, then
1
(UT
2 V2)âˆ’12
2
= 1 âˆ’
UT
2 V1
2
2 .
(b)
Prove that if UT
2 V1 is nonsingular, then
UT
2 V2
2
2 = 1 âˆ’
1
(UT
2 V1)âˆ’12
2
.

CHAPTER 6
Determinants
6.1
DETERMINANTS
At the beginning of this text, reference was made to the ancient Chinese counting
board on which colored bamboo rods were manipulated according to prescribed
â€œrules of thumbâ€ in order to solve a system of linear equations. The Chinese
counting board is believed to date back to at least 200 B.C., and it was used
more or less in the same way for a millennium. The counting board and the â€œrules
of thumbâ€ eventually found their way to Japan where Seki Kowa (1642â€“1708),
a great Japanese mathematician, synthesized the ancient Chinese ideas of array
manipulation. Kowa formulated the concept of what we now call the determinant
to facilitate solving linear systemsâ€”his deï¬nition is thought to have been made
some time before 1683.
About the same timeâ€”somewhere between 1678 and 1693â€”Gottfried W.
Leibniz (1646â€“1716), a German mathematician, was independently developing
his own concept of the determinant together with applications of array manipu-
lation to solve systems of linear equations. It appears that Leibnizâ€™s early work
dealt with only three equations in three unknowns, whereas Seki Kowa gave a
general treatment for n equations in n unknowns. It seems that Kowa and
Leibniz both developed what later became known as Cramerâ€™s rule (p. 476), but
not in the same form or notation. These men had something else in commonâ€”
their ideas concerning the solution of linear systems were never adopted by the
mathematical community of their time, and their discoveries quickly faded into
oblivion.
Eventually the determinant was rediscovered, and much was written on the
subject between 1750 and 1900. During this era, determinants became the ma-
jor tool used to analyze and solve linear systems, while the theory of matrices
remained relatively undeveloped. But mathematics, like a river, is everchanging

460
Chapter 6
Determinants
in its course, and major branches can dry up to become minor tributaries while
small trickling brooks can develop into raging torrents. This is precisely what
occurred with determinants and matrices. The study and use of determinants
eventually gave way to Cayleyâ€™s matrix algebra, and today matrix and linear
algebra are in the main stream of applied mathematics, while the role of deter-
minants has been relegated to a minor backwater position. Nevertheless, it is still
important to understand what a determinant is and to learn a few of its funda-
mental properties. Our goal is not to study determinants for their own sake, but
rather to explore those properties that are useful in the further development of
matrix theory and its applications. Accordingly, many secondary properties are
omitted or conï¬ned to the exercises, and the details in proofs will be kept to a
minimum.
Over the years there have evolved various â€œslickâ€ ways to deï¬ne the determi-
nant, but each of these â€œslickâ€ approaches seems to require at least one â€œstickyâ€
theorem in order to make the theory sound. We are going to opt for expedience
over elegance and proceed with the classical treatment.
A permutation p = (p1, p2, . . . , pn) of the numbers (1, 2, . . . , n) is simply
any rearrangement. For example, the set
{(1, 2, 3)
(1, 3, 2)
(2, 1, 3)
(2, 3, 1)
(3, 1, 2)
(3, 2, 1)}
contains the six distinct permutations of (1, 2, 3). In general, the sequence
(1, 2, . . . , n) has n! = n(n âˆ’1)(n âˆ’2) Â· Â· Â· 1 diï¬€erent permutations. Given a per-
mutation, consider the problem of restoring it to natural order by a sequence
of pairwise interchanges. For example, (1, 4, 3, 2) can be restored to natural or-
der with a single interchange of 2 and 4 or, as indicated in Figure 6.1.1, three
adjacent interchanges can be used.
( 1,  2,  3,  4 )
( 1,  4,  3  2)
( 1,  2,  3,  4 )
( 1,  4,  3,  2 )
( 1,  4,  2,  3 )
( 1,  2,  4,  3 )
Figure 6.1.1
The important thing here is that both 1 and 3 are odd. Try to restore
(1, 4, 3, 2) to natural order by using an even number of interchanges, and you
will discover that it is impossible. This is due to the following general rule that is
stated without proof. The parity of a permutation is uniqueâ€”i.e., if a permuta-
tion p can be restored to natural order by an even (odd) number of interchanges,
then every other sequence of interchanges that restores p to natural order must

6.1 Determinants
461
also be even (odd). Accordingly, the sign of a permutation p is deï¬ned to be
the number
Ïƒ(p) =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
+1
if p can be restored to natural order by an
even number of interchanges,
âˆ’1
if p can be restored to natural order by an
odd number of interchanges.
For example, if p = (1, 4, 3, 2), then Ïƒ(p) = âˆ’1, and if p = (4, 3, 2, 1), then
Ïƒ(p) = +1. The sign of the natural order p = (1, 2, 3, 4) is naturally Ïƒ(p) = +1.
The general deï¬nition of the determinant can now be given.
Deï¬nition of Determinant
For an n Ã— n matrix A = [aij], the determinant of A is deï¬ned to
be the scalar
det (A) =

p
Ïƒ(p)a1p1a2p2 Â· Â· Â· anpn,
(6.1.1)
where the sum is taken over the n! permutations p = (p1, p2, . . . , pn)
of (1, 2, . . . , n). Observe that each term a1p1a2p2 Â· Â· Â· anpn in (6.1.1) con-
tains exactly one entry from each row and each column of A. The de-
terminant of A can be denoted by det (A) or |A|, whichever is more
convenient.
Note: The determinant of a nonsquare matrix is not deï¬ned.
For example, when A is 2 Ã— 2 there are 2! = 2 permutations of (1,2),
namely, {(1, 2)
(2, 1)}, so det (A) contains the two terms
Ïƒ(1, 2)a11a22
and
Ïƒ(2, 1)a12a21.
Since Ïƒ(1, 2) = +1 and Ïƒ(2, 1) = âˆ’1, we obtain the familiar formula

a11
a12
a21
a22
 = a11a22 âˆ’a12a21.
(6.1.2)
Example 6.1.1
Problem: Use the deï¬nition to compute det (A), where A =
 1
2
3
4
5
6
7
8
9
	
.
Solution: The 3! = 6 permutations of (1, 2, 3) together with the terms in the
expansion of det (A) are shown in Table 6.1.1.

462
Chapter 6
Determinants
Table 6.1.1
p = (p1, p2, p3)
Ïƒ(p)
a1p1a2p2a3p3
(1, 2, 3)
+
1 Ã— 5 Ã— 9 = 45
(1, 3, 2)
âˆ’
1 Ã— 6 Ã— 8 = 48
(2, 1, 3)
âˆ’
2 Ã— 4 Ã— 9 = 72
(2, 3, 1)
+
2 Ã— 6 Ã— 7 = 84
(3, 1, 2)
+
3 Ã— 4 Ã— 8 = 96
(3, 2, 1)
âˆ’
3 Ã— 5 Ã— 7 = 105
Therefore,
det (A) =

p
Ïƒ(p)a1p1a2p2a3p3 = 45 âˆ’48 âˆ’72 + 84 + 96 âˆ’105 = 0.
Perhaps you have seen rules for computing 3 Ã— 3 determinants that involve
running up, down, and around various diagonal lines. These rules do not easily
generalize to matrices of order greater than three, and in case you have forgotten
(or never knew) them, do not worry about it. Remember the 2 Ã— 2 rule given
in (6.1.2) as well as the following statement concerning triangular matrices and
let it go at that.
Triangular Determinants
The determinant of a triangular matrix is the product of its diagonal
entries. In other words,

t11
t12
Â· Â· Â·
t1n
0
t22
Â· Â· Â·
t2n
...
...
...
...
0
0
Â· Â· Â·
tnn

= t11t22 Â· Â· Â· tnn.
(6.1.3)
Proof.
Recall from the deï¬nition (6.1.1) that each term t1p1t2p2 Â· Â· Â· tnpn con-
tains exactly one entry from each row and each column. This means that there
is only one term in the expansion of the determinant that does not contain an
entry below the diagonal, and this term is t11t22 Â· Â· Â· tnn.

6.1 Determinants
463
Transposition Doesnâ€™t Alter Determinants
â€¢
det

AT 
= det (A) for all n Ã— n matrices.
(6.1.4)
Proof.
As p = (p1, p2, . . . , pn) varies over all permutations of (1, 2, . . . , n), the
set of all products {Ïƒ(p)a1p1a2p2 Â· Â· Â· anpn} is the same as the set of all products
{Ïƒ(p)ap11ap22 Â· Â· Â· apnn} . Explicitly construct both of these sets for n = 3 to
convince yourself.
Equation (6.1.4) insures that itâ€™s not necessary to distinguish between rows
and columns when discussing properties of determinants, so theorems concern-
ing determinants that involve row manipulations will remain true when the word
â€œrowâ€ is replaced by â€œcolumn.â€ For example, itâ€™s essential to know how elemen-
tary row and column operations alter the determinant of a matrix, but, by virtue
of (6.1.4), it suï¬ƒces to limit the discussion to elementary row operations.
Effects of Row Operations
Let B be the matrix obtained from AnÃ—n by one of the three elemen-
tary row operations:
Type I:
Interchange rows i and j.
Type II:
Multiply row i by Î± Ì¸= 0.
Type III:
Add Î± times row i to row j.
The value of det (B) is as follows:
â€¢
det (B) = âˆ’det (A) for Type I operations.
(6.1.5)
â€¢
det (B) = Î± det (A) for Type II operations.
(6.1.6)
â€¢
det (B) = det (A) for Type III operations.
(6.1.7)
Proof of (6.1.5).
If B agrees with A except that Biâˆ—= Ajâˆ—and Bjâˆ—= Aiâˆ—,
then for each permutation p = (p1, p2, . . . , pn) of (1, 2, . . . , n),
b1p1 Â· Â· Â· bipi Â· Â· Â· bjpj Â· Â· Â· bnpn = a1p1 Â· Â· Â· ajpi Â· Â· Â· aipj Â· Â· Â· anpn
= a1p1 Â· Â· Â· aipj Â· Â· Â· ajpi Â· Â· Â· anpn.
Furthermore, Ïƒ(p1, . . . , pi, . . . , pj, . . . , pn) = âˆ’Ïƒ(p1, . . . , pj, . . . , pi, . . . , pn) be-
cause the two permutations diï¬€er only by one interchange. Consequently, deï¬ni-
tion (6.1.1) of the determinant guarantees that det (B) = âˆ’det (A).

464
Chapter 6
Determinants
Proof of (6.1.6).
If B agrees with A except that Biâˆ—= Î±Aiâˆ—, then for each
permutation p = (p1, p2, . . . , pn),
b1p1 Â· Â· Â· bipi Â· Â· Â· bnpn = a1p1 Â· Â· Â· Î±aipi Â· Â· Â· anpn = Î±(a1p1 Â· Â· Â· aipi Â· Â· Â· anpn),
and therefore the expansion (6.1.1) yields det (B) = Î± det (A).
Proof of (6.1.7).
If B agrees with A except that Bjâˆ—= Ajâˆ—+ Î±Aiâˆ—, then
for each permutation p = (p1, p2, . . . , pn),
b1p1 Â· Â· Â· bipi Â· Â· Â· bjpj Â· Â· Â· bnpn = a1p1 Â· Â· Â· aipi Â· Â· Â· (ajpj + Î±aipj) Â· Â· Â· anpn
= a1p1 Â· Â· Â· aipi Â· Â· Â· ajpj Â· Â· Â· anpn + Î±(a1p1 Â· Â· Â· aipi Â· Â· Â· aipj Â· Â· Â· anpn),
so that
det (B) =

p
Ïƒ(p)a1p1 Â· Â· Â· aipi Â· Â· Â· ajpj Â· Â· Â· anpn
+Î±

p
Ïƒ(p)a1p1 Â· Â· Â· aipi Â· Â· Â· aipj Â· Â· Â· anpn.
(6.1.8)
The ï¬rst sum on the right-hand side of (6.1.8) is det (A), while the second sum is
the expansion of the determinant of a matrix ËœA in which the ith and jth rows
are identical. For such a matrix, det( ËœA) = 0 because (6.1.5) says that the sign
of the determinant is reversed whenever the ith and jth rows are interchanged,
so det( ËœA) = âˆ’det( ËœA). Consequently, the second sum on the right-hand side of
(6.1.8) is zero, and thus det (B) = det (A).
It is now possible to evaluate the determinant of an elementary matrix as-
sociated with any of the three types of elementary operations. Let E, F, and
G be elementary matrices of Types I, II, and III, respectively, and recall from
the discussion in Â§3.9 that each of these elementary matrices can be obtained by
performing the associated row (or column) operation to an identity matrix of ap-
propriate size. The result concerning triangular determinants (6.1.3) guarantees
that det (I) = 1 regardless of the size of I, so if E is obtained by interchanging
any two rows (or columns) in I, then (6.1.5) insures that
det (E) = âˆ’det (I) = âˆ’1.
(6.1.9)
Similarly, if F is obtained by multiplying any row (or column) in I by Î± Ì¸= 0,
then (6.1.6) implies that
det (F) = Î± det (I) = Î±,
(6.1.10)
and if G is the result of adding a multiple of one row (or column) in I to
another row (or column) in I, then (6.1.7) guarantees that
det (G) = det (I) = 1.
(6.1.11)

6.1 Determinants
465
In particular, (6.1.9)â€“(6.1.11) guarantee that the determinants of elementary
matrices of Types I, II, and III are nonzero.
As discussed in Â§3.9, if P is an elementary matrix of Type I, II, or III,
and if A is any other matrix, then the product PA is the matrix obtained by
performing the elementary operation associated with P to the rows of A. This,
together with the observations (6.1.5)â€“(6.1.7) and (6.1.9)â€“(6.1.11), leads to the
conclusion that for every square matrix A,
det (EA)
= âˆ’det (A) = det (E)det (A),
det (FA)
= Î± det (A) = det (F)det (A),
det (GA) =
det (A)
= det (G)det (A).
In other words, det (PA) = det (P)det (A) whenever P is an elementary matrix
of Type I, II, or III. Itâ€™s easy to extend this observation to any number of these
elementary matrices, P1, P2, . . . , Pk, by writing
det (P1P2 Â· Â· Â· PkA) = det (P1)det (P2 Â· Â· Â· PkA)
= det (P1)det (P2)det (P3 Â· Â· Â· PkA)
...
= det (P1)det (P2) Â· Â· Â· det (Pk)det (A).
(6.1.12)
This leads to a characterization of invertibility in terms of determinants.
Invertibility and Determinants
â€¢
AnÃ—n is nonsingular if and only if det (A) Ì¸= 0
(6.1.13)
or, equivalently,
â€¢
AnÃ—n is singular if and only if det (A) = 0.
(6.1.14)
Proof.
Let P1, P2, . . . , Pk be a sequence of elementary matrices of Type I, II,
or III such that P1P2 Â· Â· Â· PkA = EA, and apply (6.1.12) to conclude
det (P1)det (P2) Â· Â· Â· det (Pk)det (A) = det (EA).
Since elementary matrices have nonzero determinants,
det (A) Ì¸= 0 â‡â‡’det (EA) Ì¸= 0 â‡â‡’there are no zero pivots
â‡â‡’every column in EA (and in A) is basic
â‡â‡’A is nonsingular.

466
Chapter 6
Determinants
Example 6.1.2
Caution! Small Determinants
/
â‡â‡’Near Singularity. Because of (6.1.13)
and (6.1.14), it might be easy to get the idea that det (A) is somehow a measure
of how close A is to being singular, but this is not necessarily the case. Nearly
singular matrices need not have determinants of small magnitude. For example,
An =
 n
0
0
1/n

is nearly singular when n is large, but det (An) = 1 for all
n. Furthermore, small determinants do not necessarily signal nearly singular
matrices. For example,
An =
ï£«
ï£¬
ï£¬
ï£­
.1
0
Â· Â· Â·
0
0
.1
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
.1
ï£¶
ï£·
ï£·
ï£¸
nÃ—n
is not close to any singular matrixâ€”see (5.12.10) on p. 417â€”but det (An) =
(.1)n is extremely small for large n.
A minor determinant (or simply a minor) of AmÃ—n is deï¬ned to be the
determinant of any k Ã— k submatrix of A. For example,

1
2
4
5
 = âˆ’3 and

2
3
8
9
 = âˆ’6 are 2 Ã— 2 minors of A =
ï£«
ï£­
1
2
3
4
5
6
7
8
9
ï£¶
ï£¸.
An individual entry of A can be regarded as a 1 Ã— 1 minor, and det (A) itself
is considered to be a 3 Ã— 3 minor of A.
We already know that the rank of any matrix A is the size of the largest
nonsingular submatrix in A (p. 215). But (6.1.13) guarantees that the nonsingu-
lar submatrices of A are simply those submatrices with nonzero determinants,
so we have the following characterization of rank.
Rank and Determinants
â€¢
rank (A) = the size of the largest nonzero minor of A.
Example 6.1.3
Problem: Use determinants to compute the rank of A =
 1
2
3
1
4
5
6
1
7
8
9
1
	
.
Solution: Clearly, there are 1 Ã— 1 and 2 Ã— 2 minors that are nonzero, so
rank (A) â‰¥2. In order to decide if the rank is three, we must see if there

6.1 Determinants
467
are any 3 Ã— 3 nonzero minors. There are exactly four 3 Ã— 3 minors, and they
are

1
2
3
4
5
6
7
8
9

= 0,

1
2
1
4
5
1
7
8
1

= 0,

1
3
1
4
6
1
7
9
1

= 0,

2
3
1
5
6
1
8
9
1

= 0.
Since all 3 Ã— 3 minors are 0, we conclude that rank (A) = 2. You should be
able to see from this example that using determinants is generally not a good
way to compute the rank of a matrix.
In (6.1.12) we observed that the determinant of a product of elementary
matrices is the product of their respective determinants. We are now in a position
to extend this observation.
Product Rules
â€¢
det (AB) = det (A)det (B) for all n Ã— n matrices.
(6.1.15)
â€¢
det

A
B
0
D
	
= det (A)det (D) if A and D are square.
(6.1.16)
Proof of (6.1.15).
If A is singular, then AB is also singular because (4.5.2)
says that rank (AB) â‰¤rank (A). Consequently, (6.1.14) implies that
det (AB) = 0 = det (A)det (B),
so (6.1.15) is trivially true when A is singular. If A is nonsingular, then A
can be written as a product of elementary matrices A = P1P2 Â· Â· Â· Pk that are
of Type I, II, or IIIâ€”recall (3.9.3). Therefore, (6.1.12) can be applied to produce
det (AB) = det (P1P2 Â· Â· Â· PkB) = det (P1)det (P2) Â· Â· Â· det (Pk)det (B)
= det (P1P2 Â· Â· Â· Pk) det (B) = det (A)det (B).
Proof of (6.1.16).
First consider the special case X =
 ArÃ—r
0
0
I

, and use the
deï¬nition to write det (X) = 
Ïƒ(p) x1j1x2j2 Â· Â· Â· xrjrxr+1,jr+1 Â· Â· Â· xn,jn. But
xrjrxr+1,jr+1 Â· Â· Â· xn,jn =
ï£±
ï£²
ï£³
1
when p =

1
Â· Â· Â·
r
r + 1
Â· Â· Â·
n
j1
Â· Â· Â·
jr
r + 1
Â· Â· Â·
n
	
,
0
for all other permutations,
so, if pr denotes permutations of only the ï¬rst r positive integers, then
det (X) =

Ïƒ(p)
x1j1x2j2 Â· Â· Â· xrjrxr+1,jr+1 Â· Â· Â· xn,jn =

Ïƒ(pr)
x1j1x2j2 Â· Â· Â· xrjr = det (A).

468
Chapter 6
Determinants
Thus
 A
0
0
I
 = det (A). Similarly,
 I
0
0
D
 = det (D), so, by (6.1.15),

A
0
0
D
 = det

A
0
0
I
	 
I
0
0
D
	
=

A
0
0
I


I
0
0
D
 = det (A)det (D).
If A = QARA and D = QDRD are the respective QR factorizations (p. 345) of
A and D, then
 A
B
0
D

=
 QA
0
0
QD
 RA
QT
AB
0
RD

is also a QR factorization.
By (6.1.3), the determinant of a triangular matrix is the product of its diagonal
entries, and this together with the previous results yield

A
B
0
D
 =

QA
0
0
QD


RA
QT
AB
0
RD
 = det (QA)det (QD)det (RA)det (RD)
= det (QARA)det (QDRD) = det (A)det (D).
Example 6.1.4
Volume and Determinants. The deï¬nition of a determinant is purely al-
gebraic, but there is a concrete geometrical interpretation. A solid in â„œm with
parallel opposing faces whose adjacent sides are deï¬ned by vectors from a linearly
independent set {x1, x2, . . . , xn} is called an n-dimensional parallelepiped. As
depicted in Figure 6.1.2, a two-dimensional parallelepiped is a parallelogram, and
a three-dimensional parallelepiped is a skewed rectangular box.
x1
x2
x1
x2
x3
Figure 6.1.2
Problem: When A âˆˆâ„œmÃ—n has linearly independent columns, explain why
the volume of the n-dimensional parallelepiped generated by the columns of A
is Vn =

det

AT A
1/2. In particular, if A is square, then Vn = |det (A)|.
Solution: Recall from Example 5.13.2 on p. 431 that if AmÃ—n = QmÃ—nRnÃ—n is
the (rectangular) QR factorization of A, then the volume of the n-dimensional
parallelepiped generated by the columns of A is Vn = Î½1Î½2 Â· Â· Â· Î½n = det (R),
where the Î½k â€™s are the diagonal elements of the upper-triangular matrix R. Use

6.1 Determinants
469
QT Q = I together with the product rule (6.1.15) and the fact that transposition
doesnâ€™t aï¬€ect determinants (6.1.4) to write
det

AT A

= det

RT QT QR

= det

RT R

= det

RT 
det (R)
= (det (R))2 = (Î½1Î½2 Â· Â· Â· Î½n)2 = V 2
n .
(6.1.17)
If A is square, det

AT A

= det

AT 
det (A) = (det (A))2, so Vn = |det (A)|.
Hadamardâ€™s Inequality: Recall from (5.13.7) that if
A =

x1 | x2 | Â· Â· Â· | xn

nÃ—n
and
Aj =

x1 | x2 | Â· Â· Â· | xj

nÃ—j,
then Î½1 = âˆ¥x1âˆ¥2 and Î½k = âˆ¥(I âˆ’Pk)xkâˆ¥2 (the projected height of xk ) for
k > 1, where Pk is the orthogonal projector onto R (Akâˆ’1). But
Î½2
k = âˆ¥(I âˆ’Pk)xkâˆ¥2
2 â‰¤âˆ¥(I âˆ’Pk)âˆ¥2
2 âˆ¥xkâˆ¥2
2 = âˆ¥xkâˆ¥2
2
(recall (5.13.10)),
so, by (6.1.17), det

AT A

â‰¤âˆ¥x1âˆ¥2
2 âˆ¥x2âˆ¥2
2 Â· Â· Â· âˆ¥xnâˆ¥2
2 or, equivalently,
|det (A)| â‰¤
n

k=1
âˆ¥xkâˆ¥2 =
n

j=1
 n

i=1
|aij|2
1/2
,
(6.1.18)
with equality holding if and only if the xk â€™s are mutually orthogonal. This
is Hadamardâ€™s inequality.
64 In light of the preceding discussion, it simply
asserts that the volume of the parallelepiped P generated by the columns of A
canâ€™t exceed the volume of a rectangular box whose sides have length âˆ¥xkâˆ¥2 , a
fact that is geometrically evident because P is a skewed rectangular box with
sides of length âˆ¥xkâˆ¥2 .
The product rule (6.1.15) provides a practical way to compute determinants.
Recall from Â§3.10 that for every nonsingular matrix A, there is a permutation
matrix P (which is a product of elementary interchange matrices) such that
PA = LU in which L is lower triangular with 1â€™s on its diagonal, and U is
upper triangular with the pivots on its diagonal. The product rule guarantees
64
Jacques Hadamard (1865â€“1963), a leading French mathematician of the ï¬rst half of the twenti-
eth century, discovered this inequality in 1893. Inï¬‚uenced in part by the tragic death of his sons
in World War I, Hadamard became a peace activist whose politics drifted far left to the extent
that the United States was reluctant to allow him to enter the country to attend the Interna-
tional Congress of Mathematicians held in Cambridge, Massachusetts, in 1950. Due to support
from inï¬‚uential mathematicians, Hadamard was made honorary president of the congress, and
the resulting visibility together with pressure from important U.S. scientists forced oï¬ƒcials to
allow him to attend.

470
Chapter 6
Determinants
that det (P)det (A) = det (L)det (U), and we know from (6.1.9) that if E is an
elementary interchange matrix, then det (E) = âˆ’1, so
det (P) =
 +1
if P is the product of an even number of interchanges,
âˆ’1
if P is the product of an odd number of interchanges.
The result concerning triangular determinants (6.1.3) shows that det (L) = 1
and det (U) = u11u22 Â· Â· Â· unn, where the uii â€™s are the pivots, so, putting these
observations together yields det (A) = Â±u11u22 Â· Â· Â· unn, where the sign depends
on the number of row interchanges used. Below is a summary.
Computing a Determinant
If PAnÃ—n = LU is an LU factorization obtained with row interchanges
(use partial pivoting for numerical stability), then
det (A) = Ïƒu11u22 Â· Â· Â· unn.
The uii â€™s are the pivots, and Ïƒ is the sign of the permutation. That is,
Ïƒ =
 +1
if an even number of row interchanges are used,
âˆ’1
if an odd number of row interchanges are used.
If a zero pivot emerges that cannot be removed (because all entries below
the pivot are zero), then A is singular and det (A) = 0. Exercise 6.2.18
discusses orthogonal reduction to compute det (A).
Example 6.1.5
Problem: Use partial pivoting to determine an LU decomposition PA = LU,
and then evaluate the determinant of A =
ï£«
ï£­
1
2
âˆ’3
4
4
8
12
âˆ’8
2
3
2
1
âˆ’3
âˆ’1
1
âˆ’4
ï£¶
ï£¸.
Solution: The LU factors of A were computed in Example 3.10.4 as follows.
L=
ï£«
ï£¬
ï£­
1
0
0
0
âˆ’3/4
1
0
0
1/4
0
1
0
1/2
âˆ’1/5
1/3
1
ï£¶
ï£·
ï£¸, U=
ï£«
ï£¬
ï£­
4
8
12
âˆ’8
0
5
10
âˆ’10
0
0
âˆ’6
6
0
0
0
1
ï£¶
ï£·
ï£¸, P=
ï£«
ï£¬
ï£­
0
1
0
0
0
0
0
1
1
0
0
0
0
0
1
0
ï£¶
ï£·
ï£¸.
The only modiï¬cation needed is to keep track of how many row interchanges are
used. Reviewing Example 3.10.4 reveals that the pivoting process required three
interchanges, so Ïƒ = âˆ’1, and hence det (A) = (âˆ’1)(4)(5)(âˆ’6)(1) = 120.
Itâ€™s sometimes necessary to compute the derivative of a determinant whose
entries are diï¬€erentiable functions. The following formula shows how this is done.

6.1 Determinants
471
Derivative of a Determinant
If the entries in AnÃ—n = [aij(t)] are diï¬€erentiable functions of t, then
d

det (A)

dt
= det (D1) + det (D2) + Â· Â· Â· + det (Dn),
(6.1.19)
where Di is identical to A except that the entries in the ith row are
replaced by their derivativesâ€”i.e., [Di]kâˆ—=
 Akâˆ—
if i Ì¸= k,
d Akâˆ—/dt
if i = k.
Proof.
This follows directly from the deï¬nition of a determinant by writing
d

det (A)

dt
= d
dt

p
Ïƒ(p)a1p1a2p2 Â· Â· Â· anpn =

p
Ïƒ(p)d

a1p1a2p2 Â· Â· Â· anpn

dt
=

p
Ïƒ(p)

aâ€²
1p1a2p2 Â· Â· Â· anpn + a1p1aâ€²
2p2 Â· Â· Â· anpn + Â· Â· Â· + a1p1a2p2 Â· Â· Â· aâ€²
npn

=

p
Ïƒ(p)aâ€²
1p1a2p2 Â· Â· Â· anpn +

p
Ïƒ(p)a1p1aâ€²
2p2 Â· Â· Â· anpn
+ Â· Â· Â· +

p
Ïƒ(p)a1p1a2p2 Â· Â· Â· aâ€²
npn
= det (D1) + det (D2) + Â· Â· Â· + det (Dn).
Example 6.1.6
Problem: Evaluate the derivative d

det (A)

/dt for A =
 et
eâˆ’t
cos t
sin t

.
Solution: Applying formula (6.1.19) yields
d

det (A)

dt
=

et
âˆ’eâˆ’t
cos t
sin t
 +

et
eâˆ’t
âˆ’sin t
cos t
 =

et + eâˆ’t
(cos t + sin t) .
Check this by ï¬rst expanding det (A) and then computing the derivative.

472
Chapter 6
Determinants
Exercises for section 6.1
6.1.1. Use the deï¬nition to evaluate det (A) for each of the following matrices.
(a)
A =
ï£«
ï£­
3
âˆ’2
1
âˆ’5
4
0
2
1
6
ï£¶
ï£¸.
(b)
A =
ï£«
ï£­
2
1
1
6
2
1
âˆ’2
2
1
ï£¶
ï£¸.
(c)
A =
ï£«
ï£­
0
0
Î±
0
Î²
0
Î³
0
0
ï£¶
ï£¸.
(d)
A =
ï£«
ï£­
a11
a12
a13
a21
a22
a23
a31
a32
a33
ï£¶
ï£¸.
6.1.2. What is the volume of the parallelepiped generated by the three vectors
x1 = (3, 0, âˆ’4, 0)T , x2 = (0, 2, 0, âˆ’2)T , and x3 = (0, 1, 0, 1)T ?
6.1.3. Using Gaussian elimination to reduce A to an upper-triangular matrix,
evaluate det (A) for each of the following matrices.
(a) A =
ï£«
ï£­
1
2
3
2
4
1
1
4
4
ï£¶
ï£¸.
(b) A =
ï£«
ï£­
1
3
5
âˆ’1
4
2
3
âˆ’2
4
ï£¶
ï£¸.
(c) A =
ï£«
ï£¬
ï£­
1
2
âˆ’3
4
4
8
12
âˆ’8
2
3
2
1
âˆ’3
âˆ’1
1
âˆ’4
ï£¶
ï£·
ï£¸.
(d) A =
ï£«
ï£¬
ï£­
0
0
âˆ’2
3
1
0
1
2
âˆ’1
1
2
1
0
2
âˆ’3
0
ï£¶
ï£·
ï£¸.
(e) A =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
2
âˆ’1
0
0
0
âˆ’1
2
âˆ’1
0
0
0
âˆ’1
2
âˆ’1
0
0
0
âˆ’1
2
âˆ’1
0
0
0
âˆ’1
1
ï£¶
ï£·
ï£·
ï£·
ï£¸. (f) A =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
1
1
Â· Â· Â·
1
1
2
1
Â· Â· Â·
1
1
1
3
Â· Â· Â·
1
...
...
...
...
...
1
1
1
Â· Â· Â·
n
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
6.1.4. Use determinants to compute the rank of A =
ï£«
ï£¬
ï£­
1
3
âˆ’2
0
1
2
âˆ’1
âˆ’1
6
2
5
âˆ’6
ï£¶
ï£·
ï£¸.
6.1.5. Use determinants to ï¬nd the values of Î± for which the following system
possesses a unique solution.
ï£«
ï£­
1
Î±
0
0
1
âˆ’1
Î±
0
1
ï£¶
ï£¸
ï£«
ï£­
x1
x2
x3
ï£¶
ï£¸=
ï£«
ï£­
âˆ’3
4
7
ï£¶
ï£¸.

6.1 Determinants
473
6.1.6. If A is nonsingular, explain why det

Aâˆ’1
= 1/det (A).
6.1.7. Explain why determinants are invariant under similarity transforma-
tions. That is, show det

Pâˆ’1AP

= det (A) for all nonsingular P.
6.1.8. Explain why det (Aâˆ—) = det (A).
6.1.9.
(a)
Explain why |det (Q)| = 1 when Q is unitary. In particular,
det (Q) = Â±1 if Q is an orthogonal matrix.
(b)
How are the singular values of A âˆˆCnÃ—n related to det (A)?
6.1.10. Prove that if A is m Ã— n, then det (Aâˆ—A) â‰¥0, and explain why
det (Aâˆ—A) > 0 if and only if rank (A) = n.
6.1.11. If A is n Ã— n, explain why det (Î±A) = Î±ndet (A) for all scalars Î±.
6.1.12. If A is an n Ã— n skew-symmetric matrix, prove that A is singular
whenever n is odd. Hint: Use Exercise 6.1.11.
6.1.13. How can you build random integer matrices with det (A) = 1?
6.1.14. If the kth row of AnÃ—n is written as a sum Akâˆ—= xT + yT + Â· Â· Â· + zT ,
where xT, yT, . . . , zT are row vectors, explain why
det (A) = det
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
A1âˆ—
...
xT
...
Anâˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
+ det
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
A1âˆ—
...
yT
...
Anâˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
+ Â· Â· Â· + det
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
A1âˆ—
...
zT
...
Anâˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
6.1.15. The CBS inequality (p. 272) says that |xâˆ—y| â‰¤âˆ¥xâˆ¥2
2 âˆ¥yâˆ¥2
2 for vectors
x, y âˆˆCnÃ—1. Use Exercise 6.1.10 to give an alternate proof of the CBS
inequality along with an alternate explanation of why equality holds if
and only if y is a scalar multiple of x.

474
Chapter 6
Determinants
6.1.16. Determinant Formula for Pivots. Let Ak be the k Ã— k leading
principal submatrix of AnÃ—n (p. 148). Prove that if A has an LU
factorization A = LU, then det (Ak) = u11u22 Â· Â· Â· ukk, and deduce
that the kth pivot is ukk =
 det (A1) = a11
for k = 1,
det (Ak)/det (Akâˆ’1)
for k = 2, 3, . . . , n.
6.1.17. Prove that if rank (AmÃ—n) = n, then AT A has an LU factorization
with positive pivotsâ€”i.e., AT A is positive deï¬nite (pp. 154 and 559).
6.1.18. Let A(x) =
ï£«
ï£­
2 âˆ’x
3
4
0
4 âˆ’x
âˆ’5
1
âˆ’1
3 âˆ’x
ï£¶
ï£¸.
(a)
First evaluate det (A), and then compute d

det (A)

/dx.
(b)
Use formula (6.1.19) to evaluate d

det (A)

/dx.
6.1.19. When the entries of A = [aij(x)] are diï¬€erentiable functions of x,
we deï¬ne d A/dx = [d aij/dx] (the matrix of derivatives). For square
matrices, is it always the case that d

det (A)

/dx = det (dA/dx)?
6.1.20. For a set of functions S = {f1(x), f2(x), . . . , fn(x)} that are nâˆ’1 times
diï¬€erentiable, the determinant
w(x) =

f1(x)
f2(x)
Â· Â· Â·
fn(x)
f â€²
1(x)
f â€²
2(x)
Â· Â· Â·
f â€²
n(x)
...
...
...
...
f (nâˆ’1)
1
(x)
f (nâˆ’1)
2
(x)
Â· Â· Â·
f (nâˆ’1)
n
(x)

is called the Wronskian of S. If S is a linearly dependent set, explain
why w(x) = 0 for every value of x. Hint: Recall Example 4.3.6 (p. 189).
6.1.21. Consider evaluating an n Ã— n determinant from the deï¬nition (6.1.1).
(a)
How many multiplications are required?
(b)
Assuming a computer will do 1,000,000 multiplications per sec-
ond, and neglecting all other operations, what is the largest
order determinant that can be evaluated in one hour?
(c)
Under the same conditions of part (b), how long will it take to
evaluate the determinant of a 100 Ã— 100 matrix?
Hint: 100! â‰ˆ9.33 Ã— 10157.
(d)
If all other operations are neglected, how many multiplications
per second must a computer perform if the task of evaluating
the determinant of a 100 Ã— 100 matrix is to be completed in
100 years?

6.2 Additional Properties of Determinants
475
6.2
ADDITIONAL PROPERTIES OF DETERMINANTS
The purpose of this section is to present some additional properties of determi-
nants that will be helpful in later developments.
Block Determinants
If A and D are square matrices, then
det

A
B
C
D
	
=

det (A)det

D âˆ’CAâˆ’1B

when Aâˆ’1 exists,
det (D)det

A âˆ’BDâˆ’1C

when Dâˆ’1 exists.
(6.2.1)
The matrices D âˆ’CAâˆ’1B and A âˆ’BDâˆ’1C are called the Schur
complements of A and D, respectivelyâ€”see Exercise 3.7.11 on p. 123.
Proof.
If Aâˆ’1 exists, then
 A
B
C
D

=

I
0
CAâˆ’1
I
  A
B
0
D âˆ’CAâˆ’1B

, and
the product rules (p. 467) produce the ï¬rst formula in (6.2.1). The second formula
follows by using a similar trick.
Since the determinant of a product is equal to the product of the deter-
minants, itâ€™s only natural to inquire if a similar result holds for sums. In other
words, is det (A + B) = det (A)+det (B)? Almost never! Try a couple of exam-
ples to convince yourself. Nevertheless, there are still some statements that can
be made regarding the determinant of certain types of sums. In a loose sense, the
result of Exercise
6.1.14 was a statement concerning determinants and sums,
but the following result is a little more satisfying.
Rank-One Updates
If AnÃ—n is nonsingular, and if c and d are n Ã— 1 columns, then
â€¢
det

I + cdT 
= 1 + dT c,
(6.2.2)
â€¢
det

A + cdT 
= det (A)

1 + dT Aâˆ’1c

.
(6.2.3)
Exercise 6.2.7 presents a generalized version of these formulas.
Proof.
The proof of (6.2.2) follows by applying the product rules (p. 467) to

I
0
dT
1
	 
I + cdT
c
0
1
	 
I
0
âˆ’dT
1
	
=

I
c
0
1 + dT c
	
.
To prove (6.2.3), write A + cdT = A

I + Aâˆ’1cdT 
, and apply the product
rule (6.1.15) along with (6.2.2).

476
Chapter 6
Determinants
Example 6.2.1
Problem: For A =
ï£«
ï£¬
ï£¬
ï£­
1 + Î»1
1
Â· Â· Â·
1
1
1 + Î»2
Â· Â· Â·
1
...
...
...
...
1
1
Â· Â· Â·
1 + Î»n
ï£¶
ï£·
ï£·
ï£¸, Î»i Ì¸= 0, ï¬nd det (A).
Solution: Express A as a rank-one updated matrix A = D + eeT , where
D = diag (Î»1, Î»2, . . . , Î»n) and eT = ( 1
1
Â· Â· Â·
1 ) . Apply (6.2.3) to produce
det (D + eeT ) = det (D)

1 + eT Dâˆ’1e

=
 n

i=1
Î»i
 
1 +
n

i=1
1
Î»i

.
The classical result known as Cramerâ€™s rule
65 is a corollary of the rank-one
update formula (6.2.3).
Cramerâ€™s Rule
In a nonsingular system AnÃ—nx = b, the ith unknown is
xi = det (Ai)
det (A) ,
where Ai =

Aâˆ—1
 Â· Â· Â·
 Aâˆ—iâˆ’1
 b
 Aâˆ—i+1
 Â· Â· Â·
 Aâˆ—n

. That is, Ai is
identical to A except that column Aâˆ—i has been replaced by b.
Proof.
Since Ai = A + (b âˆ’Aâˆ—i) eT
i , where ei is the ith unit vector, (6.2.3)
may be applied to yield
det (Ai) = det (A)

1 + eT
i Aâˆ’1 (b âˆ’Aâˆ—i)

= det (A)

1 + eT
i (x âˆ’ei)

= det (A) (1 + xi âˆ’1) = det (A) xi.
Thus xi = det (Ai)/det (A) because A being nonsingular insures det (A) Ì¸= 0
by (6.1.13).
65
Gabriel Cramer (1704â€“1752) was a mathematician from Geneva, Switzerland. As mentioned
in Â§6.1, Cramerâ€™s rule was apparently known to others long before Cramer rediscovered and
published it in 1750. Nevertheless, Cramerâ€™s recognition is not undeserved because his work
was responsible for a revived interest in determinants and systems of linear equations. After
Cramerâ€™s publication, Cramerâ€™s rule met with instant success, and it quickly found its way
into the textbooks and classrooms of Europe. It is reported that there was a time when stu-
dents passed or failed the exams in the schools of public service in France according to their
understanding of Cramerâ€™s rule.

6.2 Additional Properties of Determinants
477
Example 6.2.2
Problem: Determine the value of t for which x3(t) is minimized in
ï£«
ï£­
t
0
1/t
0
t
t2
1
t2
t3
ï£¶
ï£¸
ï£«
ï£­
x1(t)
x2(t)
x3(t)
ï£¶
ï£¸=
ï£«
ï£­
1
1/t
1/t2
ï£¶
ï£¸.
Solution: Only one component of the solution is required, so itâ€™s wasted eï¬€ort
to solve the entire system. Use Cramerâ€™s rule to obtain
x3(t) =

t
0
1
0
t
1/t
1
t2
1/t2


t
0
1/t
0
t
t2
1
t2
t3

= 1 âˆ’t âˆ’t2
âˆ’1
= t2 + t âˆ’1,
and set
d x3(t)
dt
= 0
to conclude that x3(t) is minimized at t = âˆ’1/2.
Recall that minor determinants of A are simply determinants of subma-
trices of A. We are now in a position to see that in an n Ã— n matrix the
n âˆ’1 Ã— n âˆ’1 minor determinants have a special signiï¬cance.
Cofactors
The cofactor of AnÃ—n associated with the (i, j)-position is deï¬ned as
ËšAij = (âˆ’1)i+jMij,
where Mij is the n âˆ’1 Ã— n âˆ’1 minor obtained by deleting the ith row
and jth column of A. The matrix of cofactors is denoted by Ëš
A.
Example 6.2.3
Problem: For A =

1
âˆ’1
2
2
0
6
âˆ’3
9
1
	
, determine the cofactors ËšA21 and ËšA13.
Solution:
ËšA21=(âˆ’1)2+1M21 = (âˆ’1)(âˆ’19)= 19
and
ËšA13=(âˆ’1)1+3M13=(+1)(18) = 18.
The entire matrix of cofactors is Ëš
A =
 âˆ’54
âˆ’20
18
19
7
âˆ’6
âˆ’6
âˆ’2
2
	
.

478
Chapter 6
Determinants
The cofactors of a square matrix A appear naturally in the expansion of
det (A). For example,

a11
a12
a13
a21
a22
a23
a31
a32
a33

= a11a22a33 + a12a23a31 + a13a21a32
âˆ’a11a23a32 âˆ’a12a21a33 âˆ’a13a22a31
= a11 (a22a33 âˆ’a23a32) + a12 (a23a31 âˆ’a21a33)
+ a13 (a21a32 âˆ’a22a31)
= a11ËšA11 + a12ËšA12 + a13ËšA13.
(6.2.4)
Because this expansion is in terms of the entries of the ï¬rst row and the corre-
sponding cofactors, (6.2.4) is called the cofactor expansion of det (A) in terms
of the ï¬rst row. It should be clear that there is nothing special about the ï¬rst
row of A. That is, itâ€™s just as easy to write an expression similar to (6.2.4) in
which entries from any other row or column appear. For example, the terms in
(6.2.4) can be rearranged to produce
det (A) = a12 (a23a31 âˆ’a21a33) + a22 (a11a33 âˆ’a13a31) + a32 (a13a21 âˆ’a11a23)
= a12ËšA12 + a22ËšA22 + a32ËšA32.
This is called the cofactor expansion for det (A) in terms of the second column.
The 3 Ã— 3 case is typical, and exactly the same reasoning can be applied to a
more general n Ã— n matrix in order to obtain the following statements.
Cofactor Expansions
â€¢
det (A) = ai1ËšAi1 + ai2ËšAi2 + Â· Â· Â· + ainËšAin (about row i).
(6.2.5)
â€¢
det (A) = a1jËšA1j +a2jËšA2j +Â· Â· Â·+anjËšAnj (about column j). (6.2.6)
Example 6.2.4
Problem: Use cofactor expansions to evaluate det (A) for
A =
ï£«
ï£¬
ï£­
0
0
0
2
7
1
6
5
3
7
2
0
0
3
âˆ’1
4
ï£¶
ï£·
ï£¸.
Solution: To minimize the eï¬€ort, expand det (A) in terms of the row or column
that contains a maximal number of zeros. For this example, the expansion in
terms of the ï¬rst row is most eï¬ƒcient because
det (A) = a11ËšA11 + a12ËšA12 + a13ËšA13 + a14ËšA14 = a14ËšA14 = (2)(âˆ’1)

7
1
6
3
7
2
0
3
âˆ’1

.

6.2 Additional Properties of Determinants
479
Now expand this remaining 3 Ã— 3 determinant either in terms of the ï¬rst column
or the third row. Using the ï¬rst column produces

7
1
6
3
7
2
0
3
âˆ’1

= (7)(+1)

7
2
3
âˆ’1
 + (3)(âˆ’1)

1
6
3
âˆ’1
 = âˆ’91 + 57 = âˆ’34,
so det (A) = (2)(âˆ’1)(âˆ’34) = 68. You may wish to try an expansion using
diï¬€erent rows or columns, and verify that the ï¬nal result is the same.
In the previous example, we were able to take advantage of the fact that
there were zeros in convenient positions. However, for a general matrix AnÃ—n
with no zero entries, itâ€™s not diï¬ƒcult to verify that successive application of
cofactor expansions requires n!

1 + 1
2! + 1
3! + Â· Â· Â· +
1
(nâˆ’1)!

multiplications to
evaluate det (A). Even for moderate values of n, this number is too large for
the cofactor expansion to be practical for computational purposes. Neverthe-
less, cofactors can be useful for theoretical developments such as the following
determinant formula for Aâˆ’1.
Determinant Formula for A
âˆ’1
The adjugate of AnÃ—n is deï¬ned to be adj (A) = Ëš
A
T , the transpose of
the matrix of cofactorsâ€”some older texts call this the adjoint matrix.
If A is nonsingular, then
Aâˆ’1 =
Ëš
A
T
det (A) = adj (A)
det (A).
(6.2.7)
Proof.

Aâˆ’1
ij is the ith component in the solution to Ax = ej, where ej
is the jth unit vector. By Cramerâ€™s rule, this is

Aâˆ’1
ij = xi = det (Ai)
det (A) ,
where Ai is identical to A except that the ith column has been replaced by
ej, and the cofactor expansion in terms of the ith column implies that
det (Ai) =
ith
â†“

a11
Â· Â· Â·
0
Â· Â· Â·
a1n
...
Â· Â· Â·
...
Â· Â· Â·
...
aj1
Â· Â· Â·
1
Â· Â· Â·
ajn
...
Â· Â· Â·
...
Â· Â· Â·
...
an1
Â· Â· Â·
0
Â· Â· Â·
ann

= ËšAji.

480
Chapter 6
Determinants
Example 6.2.5
Problem: Use determinants to compute

Aâˆ’1
12 and

Aâˆ’1
31 for the matrix
A =
ï£«
ï£­
1
âˆ’1
2
2
0
6
âˆ’3
9
1
ï£¶
ï£¸.
Solution: The cofactors ËšA21 and ËšA13 were determined in Example 6.2.3 to be
ËšA21 = 19 and ËšA13 = 18, and itâ€™s straightforward to compute det (A) = 2, so

Aâˆ’1
12 =
ËšA21
det (A) = 19
2
and

Aâˆ’1
31 =
ËšA13
det (A) = 18
2 = 9.
Using the matrix of cofactors Ëš
A computed in Example 6.2.3, we have that
Aâˆ’1 = adj (A)
det (A) =
Ëš
A
T
det (A) = 1
2
ï£«
ï£­
âˆ’54
19
âˆ’6
âˆ’20
7
âˆ’2
18
âˆ’6
2
ï£¶
ï£¸.
Example 6.2.6
Problem: For A =
 a
b
c
d

, determine a general formula for Aâˆ’1.
Solution: adj (A) = ËšAT =

d
âˆ’b
âˆ’c
a

, and det (A) = ad âˆ’bc, so
Aâˆ’1 = adj (A)
det (A) =
1
ad âˆ’bc

d
âˆ’b
âˆ’c
a
	
.
Example 6.2.7
Problem: Explain why the entries in Aâˆ’1 vary continuously with the entries in
A when A is nonsingular. This is in direct contrast with the lack of continuity
exhibited by pseudoinverses (p. 423).
Solution: Recall from elementary calculus that the sum, the product, and the
quotient of continuous functions are each continuous functions. In particular,
the sum and the product of any set of numbers varies continuously as the num-
bers vary, so det (A) is a continuous function of the aij â€™s. Since each entry in
adj (A) is a determinant, each quotient [Aâˆ’1]ij = [adj (A)]ij/det (A) must be
a continuous function of the aij â€™s.
The Moral: The formula Aâˆ’1 = adj (A) /det (A) is nearly worthless for actu-
ally computing the value of Aâˆ’1, but, as this example demonstrates, the formula
is nevertheless a useful mathematical tool. Itâ€™s not uncommon for applied ori-
ented students to fall into the trap of believing that the worth of a formula or
an idea is tied to its utility for computing something. This example makes the
point that things can have signiï¬cant mathematical value without being compu-
tationally important. In fact, most of this chapter is in this category.

6.2 Additional Properties of Determinants
481
Example 6.2.8
Problem: Explain why the inner product of one row (or column) in AnÃ—n with
the cofactors of a diï¬€erent row (or column) in A must always be zero.
Solution: Let ËœA be the result of replacing the jth column in A by the kth
column of A. Since ËœA has two identical columns, det ( ËœA) = 0. Furthermore, the
cofactor associated with the (i, j)-position in ËœA is ËšAij, the cofactor associated
with the (i, j) in A, so expansion of det ( ËœA) in terms of the jth column yields
0 = det ( ËœA) =
jth
â†“
kth
â†“

a11
Â· Â· Â·
a1k
Â· Â· Â·
a1k
Â· Â· Â·
a1n
...
...
...
ai1
Â· Â· Â·
aik
Â· Â· Â·
aik
Â· Â· Â·
ain
...
...
...
an1
Â· Â· Â·
ank
Â· Â· Â·
ank
Â· Â· Â·
ann

=
n

i=1
aikËšAij.
Thus the inner product of the kth column of AnÃ—n with the cofactors of the
jth column of A is zero. A similar result holds for rows. Combining these
observations with (6.2.5) and (6.2.6) produces
n

j=1
akjËšAij =

det (A)
if k = i,
0
if k Ì¸= i,
and
n

i=1
aikËšAij =

det (A)
if k = j,
0
if k Ì¸= j,
which is equivalent to saying that A[adj (A)] = [adj (A)]A = det (A) I.
Example 6.2.9
Diï¬€erential Equations and Determinants. A system of n homogeneous
ï¬rst-order linear diï¬€erential equations
d xi(t)
dt
= ai1(t)x1(t) + ai2(t)x2(t) + Â· Â· Â· + ain(t)xn(t),
i = 1, 2, . . . , n
can be expressed in matrix notation by writing
ï£«
ï£¬
ï£¬
ï£­
xâ€²
1(t)
xâ€²
2(t)
...
xâ€²
n(t)
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
a11(t)
a12(t)
Â· Â· Â·
a1n(t)
a21(t)
a22(t)
Â· Â· Â·
a2n(t)
...
...
...
...
an1(t)
an2(t)
Â· Â· Â·
ann(t)
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
x1(t)
x2(t)
...
xn(t)
ï£¶
ï£·
ï£·
ï£¸
or, equivalently, xâ€² = Ax. Let S = {w1(t), w2(t), . . . , wn(t)} be a set of n Ã— 1
vectors that are solutions to xâ€² = Ax, and place these solutions as columns in
a matrix W(t)nÃ—n = [w1(t) | w2(t) | Â· Â· Â· | wn(t)] so that Wâ€² = AW.
Problem: Prove that if w(t) = det (W), (called the Wronskian (p. 474)), then
w(t) = w(Î¾0) e
 t
Î¾0 trace A(Î¾) dÎ¾,
where Î¾0 is an arbitrary constant.
(6.2.8)

482
Chapter 6
Determinants
Solution: By (6.1.19), d w(t)/dt = n
i=1 det (Di), where
Di =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
w11
w12
Â· Â· Â·
w1n
...
...
Â· Â· Â·
...
wâ€²
i1
wâ€²
i2
Â· Â· Â·
wâ€²
in
...
...
Â· Â· Â·
...
wn1
wn2
Â· Â· Â·
wnn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
= W + eieT
i Wâ€² âˆ’eieT
i W.
Notice that

âˆ’eieT
i W

subtracts Wiâˆ—from the ith row while

+eieT
i Wâ€²
adds Wâ€²
iâˆ—to the ith row. Use the fact that Wâ€² = AW to write
Di = W+eieT
i Wâ€²âˆ’eieT
i W = W+eieT
i AWâˆ’eieT
i W =

I+ei

eT
i A âˆ’eT
i
 
W,
and apply formula (6.2.2) for the determinant of a rank-one updated matrix
together with the product rule (6.1.15) to produce
det (Di) =

1 + eT
i Aei âˆ’eT
i ei

det (W) = aii(t) w(t),
so
d w(t)
dt
=
n

i=1
det (Di) =
 n

i=1
aii(t)

w(t) = trace A(t) w(t).
In other words, w(t) satisï¬es the ï¬rst-order diï¬€erential equation wâ€² =Ï„ w, where
Ï„ = trace A(t), and the solution of this equation is w(t)=w(Î¾0) e
 t
Î¾0
Ï„(Î¾) dÎ¾.
Consequences: In addition to its aesthetic elegance, (6.2.8) is a useful result
because it is the basis for the following theorems.
â€¢
If xâ€² = Ax has a set of solutions S = {w1(t), w2(t), . . . , wn(t)} that is
linearly independent at some point Î¾0 âˆˆ(a, b), and if
 t
Î¾0 Ï„(Î¾) dÎ¾ is ï¬nite for
t âˆˆ(a, b), then S must be linearly independent at every point t âˆˆ(a, b).
â€¢
If A is a constant matrix, and if S is a set of n solutions that is linearly
independent at some value t = Î¾0, then S must be linearly independent for
all values of t.
Proof.
If S is linearly independent at Î¾0, then W(Î¾0) is nonsingular, so
w(Î¾0) Ì¸= 0. If
 t
Î¾0 Ï„(Î¾) dÎ¾ is ï¬nite when t âˆˆ(a, b), then e
 t
Î¾0
Ï„(Î¾) dÎ¾ is ï¬nite
and nonzero on (a, b), so, by (6.2.8), w(t) Ì¸= 0 on (a, b). Therefore, W(t) is
nonsingular for t âˆˆ(a, b), and thus S is linearly independent at each t âˆˆ(a, b).
Exercises for section 6.2
6.2.1. Use a cofactor expansion to evaluate each of the following determinants.
(a)

2
1
1
6
2
1
âˆ’2
2
1

,
(b)

0
0
âˆ’2
3
1
0
1
2
âˆ’1
1
2
1
0
2
âˆ’3
0

,
(c)

0
1
1
1
1
0
1
1
1
1
0
1
1
1
1
0

.

6.2 Additional Properties of Determinants
483
6.2.2. Use determinants to compute the following inverses.
(a)
ï£«
ï£­
2
1
1
6
2
1
âˆ’2
2
1
ï£¶
ï£¸
âˆ’1
.
(b)
ï£«
ï£¬
ï£­
0
0
âˆ’2
3
1
0
1
2
âˆ’1
1
2
1
0
2
âˆ’3
0
ï£¶
ï£·
ï£¸
âˆ’1
.
6.2.3.
(a)
Use Cramerâ€™s rule to solve
x1 + x2 + x3 = 1,
x1 + x2
= Î±,
x2 + x3 = Î².
(b)
Evaluate limtâ†’âˆx2(t), where x2(t) is deï¬ned by the system
x1 +
tx2 + t2x3 = t4,
t2x1 +
x2 +
tx3 = t3,
tx1 + t2x2 +
x3 = 0.
6.2.4. Is the following equation a valid derivation of Cramerâ€™s rule for solving
a nonsingular system Ax = b, where Ai is as described on p. 476?
det (Ai)
det (A) = det

Aâˆ’1Ai

= det

e1 Â· Â· Â· eiâˆ’1 x ei+1 Â· Â· Â· en

= xi.
6.2.5.
(a)
By example, show that det (A + B) Ì¸= det (A) + det (B).
(b)
Using square matrices, construct an example that shows that
det

A
B
C
D
	
Ì¸= det (A)det (D) âˆ’det (B)det (C).
6.2.6. Suppose rank (BmÃ—n) = n, and let Q be the orthogonal projector onto
N

BT 
. For A = [B | cnÃ—1] , prove cT Qc = det

AT A

/det

BT B

.
6.2.7. If AnÃ—n is a nonsingular matrix, and if D and C are n Ã— k matrices,
explain how to use (6.2.1) to derive the formula
det

A + CDT 
= det (A)det

Ik + DT Aâˆ’1C

.
Note: This is a generalization of (6.2.3) because if ci and di are the
ith columns of C and D, respectively, then
A + CDT = A + c1dT
1 + c2dT
2 + Â· Â· Â· + ckdT
k .

484
Chapter 6
Determinants
6.2.8. Explain why A is singular if and only if A[adj (A)] = 0.
6.2.9. For a nonsingular linear system Ax = b, explain why each component
of the solution must vary continuously with the entries of A.
6.2.10. For scalars Î±, explain why adj (Î±A) = Î±nâˆ’1adj (A) . Hint: Recall
Exercise 6.1.11.
6.2.11. For an n Ã— n matrix A, prove that the following statements are true.
(a)
If rank (A) < n âˆ’1, then adj (A) = 0.
(b)
If rank (A) = n âˆ’1, then rank (adj (A)) = 1.
(c)
If rank (A) = n, then rank (adj (A)) = n.
6.2.12. In 1812, Cauchy discovered the formula that says that if A is n Ã— n,
then det (adj (A)) = [det (A)]nâˆ’1. Establish Cauchyâ€™s formula.
6.2.13. For the following tridiagonal matrix, An, let Dn = det (An), and de-
rive the formula Dn = 2Dnâˆ’1 âˆ’Dnâˆ’2 to deduce that Dn = n + 1.
An =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
2
âˆ’1
0
Â· Â· Â·
0
âˆ’1
2
âˆ’1
Â· Â· Â·
0
...
...
...
0
Â· Â· Â·
âˆ’1
2
âˆ’1
0
Â· Â· Â·
0
âˆ’1
2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
nÃ—n
.
6.2.14. By considering rank-one updated matrices, derive the following formulas.
(a)

1+Î±1
Î±1
1
Â· Â· Â·
1
1
1+Î±2
Î±2
Â· Â· Â·
1
...
...
...
...
1
1
Â· Â· Â·
1+Î±n
Î±n

= 1 +  Î±i
 Î±i
.
(b)

Î±
Î²
Î²
Â· Â· Â·
Î²
Î²
Î±
Î²
Â· Â· Â·
Î²
Î²
Î²
Î±
Â· Â· Â·
Î²
...
...
...
...
...
Î²
Î²
Î²
Â· Â· Â·
Î±

nÃ—n
=

(Î± âˆ’Î²)n 
1 +
nÎ²
Î±âˆ’Î²

if Î± Ì¸= Î²,
0
if Î± = Î².
(c)

1 + Î±1
Î±2
Â· Â· Â·
Î±n
Î±1
1 + Î±2
Â· Â· Â·
Î±n
...
...
...
...
Î±1
Î±2
Â· Â· Â·
1 + Î±n

= 1 + Î±1 + Î±2 + Â· Â· Â· + Î±n.

6.2 Additional Properties of Determinants
485
6.2.15. A bordered matrix has the form B =
 A
x
yT
Î±

in which AnÃ—n is
nonsingular, x is a column, yT is a row, and Î± is a scalar. Explain
why the following statements must be true.
(a)

A
x
yT
âˆ’1
 = âˆ’det

A + xyT 
.
(b)

A
x
yT
0
 = âˆ’yT adj (A) x.
6.2.16. If B is m Ã— n and C is n Ã— m, explain why (6.2.1) guarantees that
Î»mdet (Î»In âˆ’CB) = Î»ndet (Î»Im âˆ’BC) is true for all scalars Î».
6.2.17. For a square matrix A and column vectors c and d, derive the fol-
lowing two extensions of formula (6.2.3).
(a)
If Ax = c, then det

A + cdT 
= det (A)

1 + dT x

.
(b)
If yT A = dT , then det

A + cdT 
= det (A)

1 + yT c

.
6.2.18. Describe the determinant of an elementary reï¬‚ector (p. 324) and a plane
rotation (p. 333), and then explain how to ï¬nd det (A) using House-
holder reduction (p. 341) and Givens reduction (Example 5.7.2).
6.2.19. Suppose that A is a nonsingular matrix whose entries are integers.
Prove that the entries in Aâˆ’1 are integers if and only if det (A) = Â±1.
6.2.20. Let A = I âˆ’2uvT be a matrix in which u and v are column vectors
with integer entries.
(a)
Prove that Aâˆ’1 has integer entries if and only if vT u = 0 or 1.
(b)
A matrix is said to be involutory whenever Aâˆ’1 = A. Explain
why A = I âˆ’2uvT is involutory when vT u = 1.
6.2.21. Use induction to argue that a cofactor expansion of det (AnÃ—n) requires
c(n) = n!

1 + 1
2! + 1
3! + Â· Â· Â· +
1
(n âˆ’1)!
	
multiplications for n â‰¥2. Assume a computer will do 1,000,000 multi-
plications per second, and neglect all other operations to estimate how
long it will take to evaluate the determinant of a 100 Ã— 100 matrix using
cofactor expansions. Hint: Recall the series expansion for ex, and use
100! â‰ˆ9.33 Ã— 10157.

486
Chapter 6
Determinants
6.2.22. Determine all values of Î» for which the matrix Aâˆ’Î»I is singular, where
A =
ï£«
ï£­
0
âˆ’3
âˆ’2
2
5
2
âˆ’2
âˆ’3
0
ï£¶
ï£¸.
Hint: If p(Î») = Î»n + Î±nâˆ’1Î»nâˆ’1 + Â· Â· Â· + Î±1Î» + Î±0 is a monic polynomial
with integer coeï¬ƒcients, then the integer roots of p(Î») are a subset of
the factors of Î±0.
6.2.23. Suppose that f1(t), f2(t), . . . , fn(t) are solutions of nth-order linear
diï¬€erential equation y(n) + p1(t)y(nâˆ’1) + Â· Â· Â· + pnâˆ’1(t)yâ€² + pn(t)y = 0,
and let w(t) be the Wronskian
w(t) =

f1(t)
f2(t)
Â· Â· Â·
fn(t)
f â€²
1(t)
f â€²
2(t)
Â· Â· Â·
f â€²
n(t)
...
...
...
...
f (nâˆ’1)
1
(t)
f (nâˆ’1)
2
(t)
Â· Â· Â·
f (nâˆ’1)
n
(t)

.
By converting the nth-order equation into a system of n ï¬rst-order
equations with the substitutions x1 = y, x2 = yâ€², . . . , xn = y(nâˆ’1),
show that w(t) = w(Î¾0) e
âˆ’ t
Î¾0
p1(Î¾) dÎ¾ for an arbitrary constant Î¾0.
6.2.24. Evaluate the Vandermonde determinant by showing

1
x1
x2
1
Â· Â· Â·
xnâˆ’1
1
1
x2
x2
2
Â· Â· Â·
xnâˆ’1
2
...
...
...
Â· Â· Â·
...
1
xn
x2
n
Â· Â· Â·
xnâˆ’1
n

=

j>i
(xj âˆ’xi).
When is this nonzero (compare with Example 4.3.4)? Hint: For the
polynomial p(Î») =

1
Î»
Î»2
Â· Â· Â·
Î»kâˆ’1
1
x2
x2
2
Â· Â· Â·
xkâˆ’1
2
...
...
...
Â· Â· Â·
...
1
xk
x2
k
Â· Â· Â·
xkâˆ’1
k

kÃ—k
, use induction to ï¬nd the
degree of p(Î»), the roots of p(Î»), and the coeï¬ƒcient of Î»kâˆ’1 in p(Î»).
6.2.25. Suppose that each entry in AnÃ—n = [aij(x)] is a diï¬€erentiable function
of a real variable x. Use formula (6.1.19) to derive the formula
d

det (A)

dx
=
n

j=1
n

i=1
d aij
dx
ËšAij.

6.2 Additional Properties of Determinants
487
6.2.26. Consider the entries of A to be independent variables, and use formula
(6.1.19) to derive the formula
âˆ‚det (A)
âˆ‚aij
= ËšAij.
6.2.27. Laplaceâ€™s Expansion. In 1772, the French mathematician Pierre-Simon
Laplace (1749â€“1827) presented the following generalized version of the
cofactor expansion. For an n Ã— n matrix A, let
A(i1i2 Â· Â· Â· ik | j1j2 Â· Â· Â· jk) = the k Ã— k submatrix of A that lies on
the intersection of rows i1, i2, . . . , ik
with columns j1, j2, . . . , jk,
and let
M(i1i2 Â· Â· Â· ik | j1j2 Â· Â· Â· jk) = the n âˆ’k Ã— n âˆ’k minor determinant
obtained by deleting rows i1, i2, . . . , ik
and columns j1, j2, . . . , jk from A.
The cofactor of A(i1 Â· Â· Â· ik | j1 Â· Â· Â· jk) is deï¬ned to be the signed minor
ËšA(i1 Â· Â· Â· ik | j1 Â· Â· Â· jk) = (âˆ’1)i1+Â·Â·Â·+ik+j1+Â·Â·Â·+jkM(i1 Â· Â· Â· ik | j1 Â· Â· Â· jk).
This is consistent with the deï¬nition of cofactor given earlier because if
A(i | j) = aij, then ËšA(i | j) = (âˆ’1)i+jM(i | j) = (âˆ’1)i+jMij = ËšAij. For
each ï¬xed set of row indices 1 â‰¤i1 < Â· Â· Â· < ik â‰¤n,
det (A) =

1â‰¤j1<Â·Â·Â·<jkâ‰¤n
det A(i1 Â· Â· Â· ik | j1 Â· Â· Â· jk)ËšA(i1 Â· Â· Â· ik | j1 Â· Â· Â· jk).
Similarly, for each ï¬xed set of column indices 1 â‰¤j1 < Â· Â· Â· < jk â‰¤n,
det (A) =

1â‰¤i1<Â·Â·Â·<ikâ‰¤n
det A(i1 Â· Â· Â· ik | j1 Â· Â· Â· jk)ËšA(i1 Â· Â· Â· ik | j1 Â· Â· Â· jk).
Each of these sums contains

n
k

terms. Use Laplaceâ€™s expansion to
evaluate the determinant of
A =
ï£«
ï£¬
ï£­
0
0
âˆ’2
3
1
0
1
2
âˆ’1
1
2
1
0
2
âˆ’3
0
ï£¶
ï£·
ï£¸
in terms of the ï¬rst and third rows.

CHAPTER 7
Eigenvalues
and
Eigenvectors
7.1
ELEMENTARY PROPERTIES OF EIGENSYSTEMS
Up to this point, almost everything was either motivated by or evolved from the
consideration of systems of linear algebraic equations. But we have come to a
turning point, and from now on the emphasis will be diï¬€erent. Rather than being
concerned with systems of algebraic equations, many topics will be motivated
or driven by applications involving systems of linear diï¬€erential equations and
their discrete counterparts, diï¬€erence equations.
For example, consider the problem of solving the system of two ï¬rst-order
linear diï¬€erential equations, du1/dt = 7u1 âˆ’4u2 and du2/dt = 5u1 âˆ’2u2. In
matrix notation, this system is

uâ€²
1
uâ€²
2

=

7
âˆ’4
5
âˆ’2
 
u1
u2

or, equivalently,
uâ€² = Au,
(7.1.1)
where uâ€² =
 uâ€²
1
uâ€²
2

, A =
 7
âˆ’4
5
âˆ’2

, and u =
 u1
u2

. Because solutions of a single
equation uâ€² = Î»u have the form u = Î±eÎ»t, we are motivated to seek solutions
of (7.1.1) that also have the form
u1 = Î±1eÎ»t
and
u2 = Î±2eÎ»t.
(7.1.2)
Diï¬€erentiating these two expressions and substituting the results in (7.1.1) yields
Î±1Î»eÎ»t = 7Î±1eÎ»t âˆ’4Î±2eÎ»t
Î±2Î»eÎ»t = 5Î±1eÎ»t âˆ’2Î±2eÎ»t â‡’
Î±1Î» = 7Î±1 âˆ’4Î±2
Î±2Î» = 5Î±1 âˆ’2Î±2
â‡’
 7
âˆ’4
5
âˆ’2
  Î±1
Î±2

=Î»
 Î±1
Î±2

.

490
Chapter 7
Eigenvalues and Eigenvectors
In other words, solutions of (7.1.1) having the form (7.1.2) can be constructed
provided solutions for Î» and x =
 Î±1
Î±2

in the matrix equation Ax = Î»x can
be found. Clearly, x = 0 trivially satisï¬es Ax = Î»x, but x = 0 provides no
useful information concerning the solution of (7.1.1). What we really need are
scalars Î» and nonzero vectors x that satisfy Ax = Î»x. Writing Ax = Î»x
as (A âˆ’Î»I) x = 0 shows that the vectors of interest are the nonzero vectors in
N (A âˆ’Î»I) . But N (A âˆ’Î»I) contains nonzero vectors if and only if A âˆ’Î»I
is singular. Therefore, the scalars of interest are precisely the values of Î» that
make A âˆ’Î»I singular or, equivalently, the Î» â€™s for which det (A âˆ’Î»I) = 0.
These observations motivate the deï¬nition of eigenvalues and eigenvectors.
66
Eigenvalues and Eigenvectors
For an n Ã— n matrix A, scalars Î» and vectors xnÃ—1 Ì¸= 0 satisfying
Ax = Î»x are called eigenvalues and eigenvectors of A, respectively,
and any such pair, (Î», x), is called an eigenpair for A. The set of
distinct eigenvalues, denoted by Ïƒ (A) , is called the spectrum of A.
â€¢
Î» âˆˆÏƒ (A) â‡â‡’A âˆ’Î»I is singular â‡â‡’det (A âˆ’Î»I) = 0.
(7.1.3)
â€¢

x Ì¸= 0
 x âˆˆN (A âˆ’Î»I)

is the set of all eigenvectors associated
with Î». From now on, N (A âˆ’Î»I) is called an eigenspace for A.
â€¢
Nonzero row vectors yâˆ—such that yâˆ—(A âˆ’Î»I) = 0 are called left-
hand eigenvectors for A (see Exercise 7.1.18 on p. 503).
Geometrically, Ax = Î»x says that under transformation by A, eigenvec-
tors experience only changes in magnitude or signâ€”the orientation of Ax in â„œn
is the same as that of x. The eigenvalue Î» is simply the amount of â€œstretchâ€
or â€œshrinkâ€ to which the eigenvector x is subjected when transformed by A.
Figure 7.1.1 depicts the situation in â„œ2.
Ax = Î»x
x
Figure 7.1.1
66
The words eigenvalue and eigenvector are derived from the German word eigen, which means
owned by or peculiar to. Eigenvalues and eigenvectors are sometimes called characteristic values
and characteristic vectors, proper values and proper vectors, or latent values and latent vectors.

7.1 Elementary Properties of Eigensystems
491
Letâ€™s now face the problem of ï¬nding the eigenvalues and eigenvectors of
the matrix A =
 7
âˆ’4
5
âˆ’2

appearing in (7.1.1). As noted in (7.1.3), the eigen-
values are the scalars Î» for which det (A âˆ’Î»I) = 0. Expansion of det (A âˆ’Î»I)
produces the second-degree polynomial
p(Î») = det (A âˆ’Î»I) =

7 âˆ’Î»
âˆ’4
5
âˆ’2 âˆ’Î»
 = Î»2 âˆ’5Î» + 6 = (Î» âˆ’2)(Î» âˆ’3),
which is called the characteristic polynomial for A. Consequently, the eigen-
values for A are the solutions of the characteristic equation p(Î») = 0 (i.e.,
the roots of the characteristic polynomial), and they are Î» = 2 and Î» = 3.
The eigenvectors associated with Î» = 2 and Î» = 3 are simply the nonzero
vectors in the eigenspaces N (A âˆ’2I) and N (A âˆ’3I), respectively. But deter-
mining these eigenspaces amounts to nothing more than solving the two homo-
geneous systems, (A âˆ’2I) x = 0 and (A âˆ’3I) x = 0.
For Î» = 2,
A âˆ’2I =

5
âˆ’4
5
âˆ’4

âˆ’â†’

1
âˆ’4/5
0
0

=â‡’
x1 = (4/5)x2
x2 is free
=â‡’
N (A âˆ’2I) =
	
x
 x = Î±

4/5
1

.
For Î» = 3,
A âˆ’3I =

4
âˆ’4
5
âˆ’5

âˆ’â†’

1
âˆ’1
0
0

=â‡’
x1 = x2
x2 is free
=â‡’
N (A âˆ’3I) =
	
x
 x = Î²

1
1

.
In other words, the eigenvectors of A associated with Î» = 2 are all nonzero
multiples of x = ( 4/5
1 )T , and the eigenvectors associated with Î» = 3 are
all nonzero multiples of y = ( 1
1 )T . Although there are an inï¬nite number of
eigenvectors associated with each eigenvalue, each eigenspace is one dimensional,
so, for this example, there is only one independent eigenvector associated with
each eigenvalue.
Letâ€™s complete the discussion concerning the system of diï¬€erential equations
uâ€² = Au in (7.1.1). Coupling (7.1.2) with the eigenpairs (Î»1, x) and (Î»2, y) of
A computed above produces two solutions of uâ€² = Au, namely,
u1 = eÎ»1tx = e2t

4/5
1

and
u2 = eÎ»2ty = e3t

1
1

.
It turns out that all other solutions are linear combinations of these two particular
solutionsâ€”more is said in Â§7.4 on p. 541.
Below is a summary of some general statements concerning features of the
characteristic polynomial and the characteristic equation.

492
Chapter 7
Eigenvalues and Eigenvectors
Characteristic Polynomial and Equation
â€¢
The characteristic polynomial of AnÃ—n is p(Î») = det (A âˆ’Î»I).
The degree of p(Î») is n, and the leading term in p(Î») is (âˆ’1)nÎ»n.
â€¢
The characteristic equation for A is p(Î») = 0.
â€¢
The eigenvalues of A are the solutions of the characteristic equation
or, equivalently, the roots of the characteristic polynomial.
â€¢
Altogether, A has n eigenvalues, but some may be complex num-
bers (even if the entries of A are real numbers), and some eigenval-
ues may be repeated.
â€¢
If A contains only real numbers, then its complex eigenvalues must
occur in conjugate pairsâ€”i.e., if Î» âˆˆÏƒ (A) , then Î» âˆˆÏƒ (A) .
Proof.
The fact that det (A âˆ’Î»I) is a polynomial of degree n whose leading
term is (âˆ’1)nÎ»n follows from the deï¬nition of determinant given in (6.1.1). If
Î´ij =
	
1
if i = j,
0
if i Ì¸= j,
then
det (A âˆ’Î»I) =

p
Ïƒ(p)(a1p1 âˆ’Î´1p1Î»)(a2p2 âˆ’Î´2p2Î») Â· Â· Â· (anpn âˆ’Î´npnÎ»)
is a polynomial in Î». The highest power of Î» is produced by the term
(a11 âˆ’Î»)(a22 âˆ’Î») Â· Â· Â· (ann âˆ’Î»),
so the degree is n, and the leading term is (âˆ’1)nÎ»n. The discussion given
earlier contained the proof that the eigenvalues are precisely the solutions of the
characteristic equation, but, for the sake of completeness, itâ€™s repeated below:
Î» âˆˆÏƒ (A) â‡â‡’Ax = Î»x for some x Ì¸= 0 â‡â‡’(A âˆ’Î»I) x = 0 for some x Ì¸= 0
â‡â‡’A âˆ’Î»I is singular â‡â‡’det (A âˆ’Î»I) = 0.
The fundamental theorem of algebra is a deep result that insures every poly-
nomial of degree n with real or complex coeï¬ƒcients has n roots, but some
roots may be complex numbers (even if all the coeï¬ƒcients are real), and some
roots may be repeated. Consequently, A has n eigenvalues, but some may be
complex, and some may be repeated. The fact that complex eigenvalues of real
matrices must occur in conjugate pairs is a consequence of the fact that the roots
of a polynomial with real coeï¬ƒcients occur in conjugate pairs.

7.1 Elementary Properties of Eigensystems
493
Example 7.1.1
Problem: Determine the eigenvalues and eigenvectors of A =
 1
âˆ’1
1
1

.
Solution: The characteristic polynomial is
det (A âˆ’Î»I) =

1 âˆ’Î»
âˆ’1
1
1 âˆ’Î»
 = (1 âˆ’Î»)2 + 1 = Î»2 âˆ’2Î» + 2,
so the characteristic equation is Î»2 âˆ’2Î» + 2 = 0. Application of the quadratic
formula yields
Î» = 2 Â± âˆšâˆ’4
2
= 2 Â± 2âˆšâˆ’1
2
= 1 Â± i,
so the spectrum of A is Ïƒ (A) = {1 + i, 1 âˆ’i}. Notice that the eigenvalues are
complex conjugates of each otherâ€”as they must be because complex eigenvalues
of real matrices must occur in conjugate pairs. Now ï¬nd the eigenspaces.
For Î» = 1 + i,
A âˆ’Î»I =

âˆ’i
âˆ’1
1
âˆ’i

âˆ’â†’

1
âˆ’i
0
0

=â‡’
N (A âˆ’Î»I) = span
	
i
1

.
For Î» = 1 âˆ’i,
A âˆ’Î»I =

i
âˆ’1
1
i

âˆ’â†’

1
i
0
0

=â‡’
N (A âˆ’Î»I) = span
	
âˆ’i
1

.
In other words, the eigenvectors associated with Î»1 = 1 + i are all nonzero
multiples of x1 = ( i
1 )T , and the eigenvectors associated with Î»2 = 1 âˆ’i
are all nonzero multiples of x2 = ( âˆ’i
1 )T . In previous sections, you could
be successful by thinking only in terms of real numbers and by dancing around
those statements and issues involving complex numbers. But this example makes
it clear that avoiding complex numbers, even when dealing with real matrices,
is no longer possibleâ€”very innocent looking matrices, such as the one in this
example, can possess complex eigenvalues and eigenvectors.
As we have seen, computing eigenvalues boils down to solving a polynomial
equation. But determining solutions to polynomial equations can be a formidable
task. It was proven in the nineteenth century that itâ€™s impossible to express
the roots of a general polynomial of degree ï¬ve or higher using radicals of the
coeï¬ƒcients. This means that there does not exist a generalized version of the
quadratic formula for polynomials of degree greater than four, and general poly-
nomial equations cannot be solved by a ï¬nite number of arithmetic operations
involving
+,âˆ’,Ã—,Ã·, nâˆš. Unlike solving Ax = b, the eigenvalue problem gener-
ally requires an inï¬nite algorithm, so all practical eigenvalue computations are
accomplished by iterative methodsâ€”some are discussed later.

494
Chapter 7
Eigenvalues and Eigenvectors
For theoretical work, and for textbook-type problems, itâ€™s helpful to express
the characteristic equation in terms of the principal minors. Recall that an r Ã— r
principal submatrix of AnÃ—n is a submatrix that lies on the same set of r
rows and columns, and an r Ã— r principal minor is the determinant of an r Ã— r
principal submatrix. In other words, r Ã— r principal minors are obtained by
deleting the same set of nâˆ’r rows and columns, and there are
n
r

= n!/r!(nâˆ’r)!
such minors. For example, the 1 Ã— 1 principal minors of
A =
ï£«
ï£­
âˆ’3
1
âˆ’3
20
3
10
2
âˆ’2
4
ï£¶
ï£¸
(7.1.4)
are the diagonal entries âˆ’3, 3, and 4. The 2 Ã— 2 principal minors are

âˆ’3
1
20
3
 = âˆ’29,

âˆ’3
âˆ’3
2
4
 = âˆ’6,
and

3
10
âˆ’2
4
 = 32,
and the only 3 Ã— 3 principal minor is det (A) = âˆ’18.
Related to the principal minors are the symmetric functions of the eigenval-
ues. The kth symmetric function of Î»1, Î»2, . . . , Î»n is deï¬ned to be the sum
of the product of the eigenvalues taken k at a time. That is,
sk =

1â‰¤i1<Â·Â·Â·<ikâ‰¤n
Î»i1 Â· Â· Â· Î»ik.
For example, when n = 4,
s1 = Î»1 + Î»2 + Î»3 + Î»4,
s2 = Î»1Î»2 + Î»1Î»3 + Î»1Î»4 + Î»2Î»3 + Î»2Î»4 + Î»3Î»4,
s3 = Î»1Î»2Î»3 + Î»1Î»2Î»4 + Î»1Î»3Î»4 + Î»2Î»3Î»4,
s4 = Î»1Î»2Î»3Î»4.
The connection between symmetric functions, principal minors, and the coeï¬ƒ-
cients in the characteristic polynomial is given in the following theorem.
Coefï¬cients in the Characteristic Equation
If Î»n + c1Î»nâˆ’1 + c2Î»nâˆ’2 + Â· Â· Â· + cnâˆ’1Î» + cn = 0 is the characteristic
equation for AnÃ—n, and if sk is the kth symmetric function of the
eigenvalues Î»1, Î»2, . . . , Î»n of A, then
â€¢
ck = (âˆ’1)k (all k Ã— k principal minors),
(7.1.5)
â€¢
sk = (all k Ã— k principal minors),
(7.1.6)
â€¢
trace (A) = Î»1 + Î»2 + Â· Â· Â· + Î»n = âˆ’c1,
(7.1.7)
â€¢
det (A) = Î»1Î»2 Â· Â· Â· Î»n = (âˆ’1)ncn.
(7.1.8)

7.1 Elementary Properties of Eigensystems
495
Proof.
At least two proofs of (7.1.5) are possible, and although they are concep-
tually straightforward, each is somewhat tedious. One approach is to successively
use the result of Exercise 6.1.14 to expand det (A âˆ’Î»I). Another proof rests on
the observation that if
p(Î») = det(A âˆ’Î»I) = (âˆ’1)nÎ»n + a1Î»nâˆ’1 + a2Î»nâˆ’2 + Â· Â· Â· + anâˆ’1Î» + an
is the characteristic polynomial for A, then the characteristic equation is
Î»n + c1Î»nâˆ’1 + c2Î»nâˆ’2 + Â· Â· Â· + cnâˆ’1Î» + cn = 0,
where
ci = (âˆ’1)nai.
Taking the rth derivative of p(Î») yields p(r)(0) = r!anâˆ’r, and hence
cnâˆ’r = (âˆ’1)n
r!
p(r)(0).
(7.1.9)
Itâ€™s now a matter of repeatedly applying the formula (6.1.19) for diï¬€erentiating
a determinant to p(Î») = det (A âˆ’Î»I). After r applications of (6.1.19),
p(r)(Î») =

ijÌ¸=ik
Di1Â·Â·Â·ir(Î»),
where Di1Â·Â·Â·ir(Î») is the determinant of the matrix identical to A âˆ’Î»I except
that rows i1, i2, . . . , ir have been replaced by âˆ’eT
i1, âˆ’eT
i2, . . . , âˆ’eT
ir, respectively.
It follows that Di1Â·Â·Â·ir(0) = (âˆ’1)rdet (Ai1Â·Â·Â·ir), where Ai1i2Â·Â·Â·ir is identical to
A except that rows i1, i2, . . . , ir have been replaced by eT
i1, eT
i2, . . . , eT
ir, re-
spectively, and det (Ai1Â·Â·Â·ir) is the n âˆ’r Ã— n âˆ’r principal minor obtained by
deleting rows and columns i1, i2, . . . , ir from A. Consequently,
p(r)(0) =

ijÌ¸=ik
Di1Â·Â·Â·ir(0) = (âˆ’1)r 
ijÌ¸=ik
det (Ai1Â·Â·Â·ir)
= r! Ã— (âˆ’1)r 
(all n âˆ’r Ã— n âˆ’r principal minors).
The factor r! appears because each of the r! permutations of the subscripts on
Ai1Â·Â·Â·ir describes the same matrix. Therefore, (7.1.9) says
cnâˆ’r = (âˆ’1)n
r!
p(r)(0) = (âˆ’1)nâˆ’r 
(all n âˆ’r Ã— n âˆ’r principal minors).
To prove (7.1.6), write the characteristic equation for A as
(Î» âˆ’Î»1)(Î» âˆ’Î»2) Â· Â· Â· (Î» âˆ’Î»n) = 0,
(7.1.10)
and expand the left-hand side to produce
Î»n âˆ’s1Î»nâˆ’1 + Â· Â· Â· + (âˆ’1)kskÎ»nâˆ’k + Â· Â· Â· + (âˆ’1)nsn = 0.
(7.1.11)
(Using n = 3 or n = 4 in (7.1.10) makes this clear.) Comparing (7.1.11)
with (7.1.5) produces the desired conclusion. Statements (7.1.7) and (7.1.8) are
obtained from (7.1.5) and (7.1.6) by setting k = 1 and k = n.

496
Chapter 7
Eigenvalues and Eigenvectors
Example 7.1.2
Problem: Determine the eigenvalues and eigenvectors of
A =
ï£«
ï£­
âˆ’3
1
âˆ’3
20
3
10
2
âˆ’2
4
ï£¶
ï£¸.
Solution: Use the principal minors computed in (7.1.4) along with (7.1.5) to
obtain the characteristic equation
Î»3 âˆ’4Î»2 âˆ’3Î» + 18 = 0.
A result from elementary algebra states that if the coeï¬ƒcients Î±i in
Î»n + Î±nâˆ’1Î»nâˆ’1 + Â· Â· Â· + Î±1Î» + Î±0 = 0
are integers, then every integer solution is a factor of Î±0. For our problem, this
means that if there exist integer eigenvalues, then they must be contained in the
set S = {Â±1, Â±2, Â±3, Â±6, Â±9, Â±18}. Evaluating p(Î») for each Î» âˆˆS reveals
that p(3) = 0 and p(âˆ’2) = 0, so Î» = 3 and Î» = âˆ’2 are eigenvalues for A.
To determine the other eigenvalue, deï¬‚ate the problem by dividing
Î»3 âˆ’4Î»2 âˆ’3Î» + 18
Î» âˆ’3
= Î»2 âˆ’Î» âˆ’6 = (Î» âˆ’3)(Î» + 2).
Thus the characteristic equation can be written in factored form as
(Î» âˆ’3)2(Î» + 2) = 0,
so the spectrum of A is Ïƒ (A) = {3, âˆ’2} in which Î» = 3 is repeatedâ€”we say
that the algebraic multiplicity of Î» = 3 is two. The eigenspaces are obtained
as follows.
For Î» = 3,
A âˆ’3I âˆ’â†’
ï£«
ï£­
1
0
1/2
0
1
0
0
0
0
ï£¶
ï£¸
=â‡’
N (A âˆ’3I) = span
ï£±
ï£²
ï£³
ï£«
ï£­
âˆ’1
0
2
ï£¶
ï£¸
ï£¼
ï£½
ï£¾.
For Î» = âˆ’2,
A + 2I âˆ’â†’
ï£«
ï£­
1
0
1
0
1
âˆ’2
0
0
0
ï£¶
ï£¸
=â‡’
N (A + 2I) = span
ï£±
ï£²
ï£³
ï£«
ï£­
âˆ’1
2
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾.
Notice that although the algebraic multiplicity of Î» = 3 is two, the dimen-
sion of the associated eigenspace is only oneâ€”we say that A is deï¬cient in
eigenvectors. As we will see later, deï¬cient matrices pose signiï¬cant diï¬ƒculties.

7.1 Elementary Properties of Eigensystems
497
Example 7.1.3
Continuity of Eigenvalues. A classical result (requiring complex analysis)
states that the roots of a polynomial vary continuously with the coeï¬ƒcients. Since
the coeï¬ƒcients of the characteristic polynomial p(Î») of A can be expressed
in terms of sums of principal minors, it follows that the coeï¬ƒcients of p(Î»)
vary continuously with the entries of A. Consequently, the eigenvalues of A
must vary continuously with the entries of A. Caution! Components of an
eigenvector need not vary continuously with the entries of A â€”e.g., consider
x = (Ïµâˆ’1, 1) as an eigenvector for A =
 0
1
0
Ïµ

, and let Ïµ â†’0.
Example 7.1.4
Spectral Radius. For square matrices A, the number
Ï(A) = max
Î»âˆˆÏƒ(A) |Î»|
is called the spectral radius of A. Itâ€™s not uncommon for applications to
require only a bound on the eigenvalues of A. That is, precise knowledge of
each eigenvalue may not called for, but rather just an upper bound on Ï(A)
is all thatâ€™s often needed. A rather crude (but cheap) upper bound on Ï(A)
is obtained by observing that Ï(A) â‰¤âˆ¥Aâˆ¥for every matrix norm. This is
true because if (Î», x) is any eigenpair, then X =

x | 0 | Â· Â· Â· | 0

nÃ—n Ì¸= 0, and
Î»X = AX implies |Î»| âˆ¥Xâˆ¥= âˆ¥Î»Xâˆ¥= âˆ¥AXâˆ¥â‰¤âˆ¥Aâˆ¥âˆ¥Xâˆ¥, so
|Î»| â‰¤âˆ¥Aâˆ¥
for all Î» âˆˆÏƒ (A) .
(7.1.12)
This result is a precursor to a stronger relationship between spectral radius
and norm that is hinted at in Exercise 7.3.12 and developed in Example 7.10.1
(p. 619).
The eigenvalue bound (7.1.12) given in Example 7.1.4 is cheap to compute,
especially if the 1-norm or âˆ-norm is used, but you often get what you pay
for. You get one big circle whose radius is usually much larger than the spectral
radius Ï(A). Itâ€™s possible to do better by using a set of Gerschgorin
67 circles as
described below.
67
S. A. Gerschgorin illustrated the use of Gerschgorin circles for estimating eigenvalues in 1931,
but the concept appears earlier in work by L. LÂ´evy in 1881, by H. Minkowski (p. 278) in 1900,
and by J. Hadamard (p. 469) in 1903. However, each time the idea surfaced, it gained little
attention and was quickly forgotten until Olga Taussky (1906â€“1995), the premier woman of
linear algebra, and her fellow German emigr`e Alfred Brauer (1894â€“1985) became captivated
by the result. Taussky (who became Olga Taussky-Todd after marrying the numerical analyst
John Todd) and Brauer devoted signiï¬cant eï¬€ort to strengthening, promoting, and popularizing
Gerschgorin-type eigenvalue bounds. Their work during the 1940s and 1950s ended the periodic
rediscoveries, and they made Gerschgorin (who might otherwise have been forgotten) famous.

498
Chapter 7
Eigenvalues and Eigenvectors
Gerschgorin Circles
â€¢
The eigenvalues of A âˆˆCnÃ—n are contained the union Gr of the n
Gerschgorin circles deï¬ned by
|z âˆ’aii| â‰¤ri,
where ri =
n

j=1
jÌ¸=i
|aij| for i = 1, 2, . . . , n.
(7.1.13)
In other words, the eigenvalues are trapped in the collection of circles
centered at aii with radii given by the sum of absolute values in Aiâˆ—
with aii deleted.
â€¢
Furthermore, if a union U of k Gerschgorin circles does not touch
any of the other n âˆ’k circles, then there are exactly k eigenvalues
(counting multiplicities) in the circles in U.
(7.1.14)
â€¢
Since Ïƒ(AT ) = Ïƒ (A) , the deleted absolute row sums in (7.1.13)
can be replaced by deleted absolute column sums, so the eigenvalues
of A are also contained in the union Gc of the circles deï¬ned by
|z âˆ’ajj| â‰¤cj,
where cj =
n

i=1
iÌ¸=j
|aij| for j = 1, 2, . . . , n.
(7.1.15)
â€¢
Combining (7.1.13) and (7.1.15) means that the eigenvalues of A
are contained in the intersection Gr âˆ©Gc.
(7.1.16)
Proof.
Let (Î», x) be an eigenpair for A, and assume x has been normalized
so that âˆ¥xâˆ¥âˆ= 1. If xi is a component of x such that |xi| = 1, then
Î»xi = [Î»x]i = [Ax]i =
n

j=1
aijxj
=â‡’
(Î» âˆ’aii)xi =
n

j=1
jÌ¸=i
aijxj,
and hence
|Î» âˆ’aii| =|Î» âˆ’aii| |xi| =


jÌ¸=i
aijxj
 â‰¤

jÌ¸=i
|aij| |xj| â‰¤

jÌ¸=i
|aij| = ri.
Thus Î» is in one of the Gerschgorin circles, so the union of all such circles
contains Ïƒ (A) . To establish (7.1.14), let D = diag (a11, a22, . . . , ann) and
B = Aâˆ’D, and set C(t) = D+tB for t âˆˆ[0, 1]. The ï¬rst part shows that the
eigenvalues of Î»i(t) of C(t) are contained in the union of the Gerschgorin circles
Ci(t) deï¬ned by |zâˆ’aii| â‰¤t ri. The circles Ci(t) grow continuously with t from
individual points aii when t = 0 to the Gerschgorin circles of A when t = 1,

7.1 Elementary Properties of Eigensystems
499
so, if the circles in the isolated union U are centered at ai1i1, ai2i2, . . . , aikik,
then for every t âˆˆ[0, 1] the union U(t) = Ci1(t) âˆªCi2(t) âˆªÂ· Â· Â· âˆªCik(t) is dis-
joint from the union U(t) of the other n âˆ’k Gerschgorin circles of C(t). Since
(as mentioned in Example 7.1.3) each eigenvalue Î»i(t) of C(t) also varies con-
tinuously with t, each Î»i(t) is on a continuous curve Î“i having one end at
Î»i(0) = aii and the other end at Î»i(1) âˆˆÏƒ (A) . But since U(t) âˆ©U(t) = Ï† for
all t âˆˆ[0, 1], the curves Î“i1, Î“i2, . . . , Î“ik are entirely contained in U, and hence
the end points Î»i1(1), Î»i2(1), . . . , Î»ik(1) are in U. Similarly, the other n âˆ’k
eigenvalues of A are in the union of the complementary set of circles.
Example 7.1.5
Problem: Estimate the eigenvalues of A =
 5
1
1
0
6
1
1
0
âˆ’5

.
â€¢
A crude estimate is derived from the bound given in Example 7.1.4 on p. 497.
Using the âˆ-norm, (7.1.12) says that |Î»| â‰¤âˆ¥Aâˆ¥âˆ= 7 for all Î» âˆˆÏƒ (A) .
â€¢
Better estimates are produced by the Gerschgorin circles in Figure 7.1.2 that
are derived from row sums. Statements (7.1.13) and (7.1.14) guarantee that
one eigenvalue is in (or on) the circle centered at âˆ’5, while the remaining
two eigenvalues are in (or on) the larger circle centered at +5.
1
2
3
4
5
6
7
-1
-2
-3
-4
-5
-6
-7
Figure 7.1.2.
Gerschgorin circles derived from row sums.
â€¢
The best estimate is obtained from (7.1.16) by considering Gr âˆ©Gc.
1
2
3
4
5
6
7
-1
-2
-3
-4
-5
-6
-7
Figure 7.1.3.
Gerschgorin circles derived from Gr âˆ©Gc.
In other words, one eigenvalue is in the circle centered at âˆ’5, while the other
two eigenvalues are in the union of the other two circles in Figure 7.1.3. This is
corroborated by computing Ïƒ (A)={5, (1Â±5
âˆš
5)/2} â‰ˆ{5, 6.0902, âˆ’5.0902}.
Example 7.1.6
Diagonally Dominant Matrices Revisited. Recall from Example 4.3.3 on
p. 184 that AnÃ—n is said to be diagonally dominant (some authors say strictly
diagonally dominant) whenever

500
Chapter 7
Eigenvalues and Eigenvectors
|aii| >
n

j=1
jÌ¸=i
|aij|
for each i = 1, 2, . . . , n.
Gerschgorinâ€™s theorem (7.1.13) guarantees that diagonally dominant matrices
cannot possess a zero eigenvalue. But 0 /âˆˆÏƒ (A) if and only if A is nonsingular
(Exercise 7.1.6), so Gerschgorinâ€™s theorem provides an alternative to the argu-
ment used in Example 4.3.3 to prove that all diagonally dominant matrices are
nonsingular.
68 For example, the 3 Ã— 3 matrix A in Example 7.1.5 is diagonally
dominant, and thus A is nonsingular. Even when a matrix is not diagonally
dominant, Gerschgorin estimates still may be useful in determining whether or
not the matrix is nonsingular simply by observing if zero is excluded from Ïƒ (A)
based on the conï¬guration of the Gerschgorin circles given in (7.1.16).
Exercises for section 7.1
7.1.1. Determine the eigenvalues and eigenvectors for the following matrices.
A =

âˆ’10
âˆ’7
14
11

.
B =
ï£«
ï£­
2
16
8
4
14
8
âˆ’8
âˆ’32
âˆ’18
ï£¶
ï£¸.
C =
ï£«
ï£­
3
âˆ’2
5
0
1
4
0
âˆ’1
5
ï£¶
ï£¸.
D =
ï£«
ï£­
0
6
3
âˆ’1
5
1
âˆ’1
2
4
ï£¶
ï£¸.
E =
ï£«
ï£­
3
0
0
0
3
0
0
0
3
ï£¶
ï£¸.
Which, if any, are deï¬cient in eigenvectors in the sense that there fails
to exist a complete linearly independent set?
7.1.2. Without doing an eigenvalueâ€“eigenvector computation, determine which
of the following are eigenvectors for
A =
ï£«
ï£¬
ï£­
âˆ’9
âˆ’6
âˆ’2
âˆ’4
âˆ’8
âˆ’6
âˆ’3
âˆ’1
20
15
8
5
32
21
7
12
ï£¶
ï£·
ï£¸,
and for those which are eigenvectors, identify the associated eigenvalue.
(a)
ï£«
ï£¬
ï£­
âˆ’1
1
0
1
ï£¶
ï£·
ï£¸.
(b)
ï£«
ï£¬
ï£­
1
0
âˆ’1
0
ï£¶
ï£·
ï£¸.
(c)
ï£«
ï£¬
ï£­
âˆ’1
0
2
2
ï£¶
ï£·
ï£¸.
(d)
ï£«
ï£¬
ï£­
0
1
âˆ’3
0
ï£¶
ï£·
ï£¸.
68
In fact, this result was the motivation behind the original development of Gerschgorinâ€™s circles.

7.1 Elementary Properties of Eigensystems
501
7.1.3. Explain why the eigenvalues of triangular and diagonal matrices
T =
ï£«
ï£¬
ï£¬
ï£­
t11
t12
Â· Â· Â·
t1n
0
t22
Â· Â· Â·
t2n
...
...
...
...
0
0
Â· Â· Â·
tnn
ï£¶
ï£·
ï£·
ï£¸
and
D =
ï£«
ï£¬
ï£¬
ï£­
Î»1
0
Â· Â· Â·
0
0
Î»2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
Î»n
ï£¶
ï£·
ï£·
ï£¸
are simply the diagonal entriesâ€”the tii â€™s and Î»i â€™s.
7.1.4. For T =
 A
B
0
C

, prove det (T âˆ’Î»I) = det (A âˆ’Î»I)det (C âˆ’Î»I) to
conclude that Ïƒ
 A
B
0
C

= Ïƒ (A) âˆªÏƒ (C) for square A and C.
7.1.5. Determine the eigenvectors of D = diag (Î»1, Î»2, . . . , Î»n) . In particular,
what is the eigenspace associated with Î»i?
7.1.6. Prove that 0 âˆˆÏƒ (A) if and only if A is a singular matrix.
7.1.7. Explain why itâ€™s apparent that AnÃ—n =
ï£«
ï£¬
ï£¬
ï£­
n
1
1
Â· Â· Â·
1
1
n
1
Â· Â· Â·
1
1
1
n
Â· Â· Â·
1
...
...
...
...
...
1
1
1
Â· Â· Â·
n
ï£¶
ï£·
ï£·
ï£¸doesnâ€™t
have a zero eigenvalue, and hence why A is nonsingular.
7.1.8. Explain why the eigenvalues of Aâˆ—A and AAâˆ—are real and nonneg-
ative for every A âˆˆCmÃ—n. Hint: Consider âˆ¥Axâˆ¥2
2 / âˆ¥xâˆ¥2
2 . When are
the eigenvalues of Aâˆ—A and AAâˆ—strictly positive?
7.1.9.
(a)
If A is nonsingular, and if (Î», x) is an eigenpair for A, show
that

Î»âˆ’1, x

is an eigenpair for Aâˆ’1.
(b)
For all Î± /âˆˆÏƒ(A), prove that x is an eigenvector of A if and
only if x is an eigenvector of (A âˆ’Î±I)âˆ’1.
7.1.10.
(a)
Show that if (Î», x) is an eigenpair for A, then (Î»k, x) is an
eigenpair for Ak for each positive integer k.
(b)
If p(x) = Î±0 + Î±1x + Î±2x2 + Â· Â· Â· + Î±kxk is any polynomial, then
we deï¬ne p(A) to be the matrix
p(A) = Î±0I + Î±1A + Î±2A2 + Â· Â· Â· + Î±kAk.
Show that if (Î», x) is an eigenpair for A, then (p(Î»), x) is an
eigenpair for p(A).

502
Chapter 7
Eigenvalues and Eigenvectors
7.1.11. Explain why (7.1.14) in Gerschgorinâ€™s theorem on p. 498 implies that
A =
ï£«
ï£­
1
0
âˆ’2
0
0
12
0
âˆ’4
1
0
âˆ’1
0
0
5
0
0
ï£¶
ï£¸must have at least two real eigenvalues. Cor-
roborate this fact by computing the eigenvalues of A.
7.1.12. If A is nilpotent ( Ak = 0 for some k ), explain why trace (A) = 0.
Hint: What is Ïƒ (A)?
7.1.13. If x1, x2, . . . , xk are eigenvectors of A associated with the same eigen-
value Î», explain why every nonzero linear combination
v = Î±1x1 + Î±2x2 + Â· Â· Â· + Î±nxn
is also an eigenvector for A associated with the eigenvalue Î».
7.1.14. Explain why an eigenvector for a square matrix A cannot be associated
with two distinct eigenvalues for A.
7.1.15. Suppose Ïƒ (AnÃ—n) = Ïƒ (BnÃ—n) . Does this guarantee that A and B
have the same characteristic polynomial?
7.1.16. Construct 2 Ã— 2 examples to prove the following statements.
(a)
Î» âˆˆÏƒ (A) and Âµ âˆˆÏƒ (B)
Ì¸=â‡’
Î» + Âµ âˆˆÏƒ (A + B) .
(b)
Î» âˆˆÏƒ (A) and Âµ âˆˆÏƒ (B)
Ì¸=â‡’
Î»Âµ âˆˆÏƒ (AB) .
7.1.17. Suppose that {Î»1, Î»2, . . . , Î»n} are the eigenvalues for AnÃ—n, and let
(Î»k, c) be a particular eigenpair.
(a)
For Î» /âˆˆÏƒ (A) , explain why (A âˆ’Î»I)âˆ’1c = c/(Î»k âˆ’Î»).
(b)
For an arbitrary vector dnÃ—1, prove that the eigenvalues of
A + cdT agree with those of A except that Î»k is replaced by
Î»k + dT c.
(c)
How can d be selected to guarantee that the eigenvalues of
A+cdT and A agree except that Î»k is replaced by a speciï¬ed
number Âµ?

7.1 Elementary Properties of Eigensystems
503
7.1.18. Suppose that A is a square matrix.
(a)
Explain why A and AT have the same eigenvalues.
(b)
Explain why Î» âˆˆÏƒ (A) â‡â‡’Î» âˆˆÏƒ (Aâˆ—) .
Hint: Recall Exercise 6.1.8.
(c)
Do these results imply that Î» âˆˆÏƒ (A) â‡â‡’Î» âˆˆÏƒ (A) when A
is a square matrix of real numbers?
(d)
A nonzero row vector yâˆ—is called a left-hand eigenvector for
A whenever there is a scalar Âµ âˆˆC such that yâˆ—(A âˆ’ÂµI) = 0.
Explain why Âµ must be an eigenvalue for A in the â€œright-handâ€
sense of the term when A is a square matrix of real numbers.
7.1.19. Consider matrices AmÃ—n and BnÃ—m.
(a)
Explain why AB and BA have the same characteristic poly-
nomial if m = n. Hint: Recall Exercise 6.2.16.
(b)
Explain why the characteristic polynomials for AB and BA
canâ€™t be the same when m Ì¸= n, and then explain why Ïƒ (AB)
and Ïƒ (BA) agree, with the possible exception of a zero eigen-
value.
7.1.20. If AB = BA, prove that A and B have a common eigenvector.
Hint: For Î» âˆˆÏƒ (A) , let the columns of X be a basis for N (A âˆ’Î»I)
so that (A âˆ’Î»I)BX = 0. Explain why there exists a matrix P such
that BX = XP, and then consider any eigenpair for P.
7.1.21. For ï¬xed matrices PmÃ—m and QnÃ—n, let T be the linear operator on
CmÃ—n deï¬ned by T(A) = PAQ.
(a)
Show that if x is a right-hand eigenvector for P and yâˆ—is a
left-hand eigenvector for Q, then xyâˆ—is an eigenvector for T.
(b)
Explain why trace (T) = trace (P) trace (Q).
7.1.22. Let D = diag (Î»1, Î»2, . . . , Î»n) be a diagonal real matrix such that
Î»1 < Î»2 < Â· Â· Â· < Î»n, and let vnÃ—1 be a column of real nonzero numbers.
(a)
Prove that if Î± is real and nonzero, then Î»i is not an eigenvalue
for D + Î±vvT . Show that the eigenvalues of D + Î±vvT are in
fact given by the solutions of the secular equation f(Î¾) = 0
deï¬ned by
f(Î¾) = 1 + Î±
n

i=1
v2
i
Î»i âˆ’Î¾ .
For n = 4 and Î± > 0, verify that the graph of f(Î¾) is as de-
picted in Figure 7.1.4, and thereby conclude that the eigenvalues
of D + Î±vvT interlace with those of D.

504
Chapter 7
Eigenvalues and Eigenvectors
Î¾1 Î¾2
Î¾3
Î¾4
Î»1
Î»2
Î»3
Î»4
Î»4 + Î±
1
1
Figure 7.1.4
(b)
Verify that (D âˆ’Î¾iI)âˆ’1v is an eigenvector for D + Î±vvT that
is associated with the eigenvalue Î¾i.
7.1.23. Newtonâ€™s Identities.
Let Î»1, . . . , Î»n be the roots of the polynomial
p(Î») = Î»n +c1Î»nâˆ’1 +c2Î»nâˆ’2 +Â· Â· Â·+cn, and let Ï„k = Î»k
1 +Î»k
2 +Â· Â· Â·+Î»k
n.
Newtonâ€™s identities say ck = âˆ’(Ï„1ckâˆ’1 + Ï„2ckâˆ’2 + Â· Â· Â· + Ï„kâˆ’1c1 + Ï„k)/k.
Derive these identities by executing the following steps:
(a)
Show pâ€²(Î») = p(Î») n
i=1(Î»âˆ’Î»i)âˆ’1 (logarithmic diï¬€erentiation).
(b)
Use the geometric series expansion for (Î» âˆ’Î»i)âˆ’1 to show that
for |Î»| > maxi|Î»i|,
n

i=1
1
(Î» âˆ’Î»i) = n
Î» + Ï„1
Î»2 + Ï„2
Î»3 + Â· Â· Â· .
(c)
Combine these two results, and equate like powers of Î».
7.1.24. Leverrierâ€“Souriauâ€“Frame Algorithm.
69 Let the characteristic equa-
tion for A be given by Î»n +c1Î»nâˆ’1 +c2Î»nâˆ’2 +Â· Â· Â·+cn = 0, and deï¬ne
a sequence by taking B0 = I and
Bk = âˆ’trace (ABkâˆ’1)
k
I + ABkâˆ’1
for
k = 1, 2, . . . , n.
Prove that for each k,
ck = âˆ’trace (ABkâˆ’1)
k
.
Hint: Use Newtonâ€™s identities, and recall Exercise 7.1.10(a).
69
This algorithm has been rediscovered and modiï¬ed several times. In 1840, the Frenchman U.
J. J. Leverrier provided the basic connection with Newtonâ€™s identities. J. M. Souriau, also from
France, and J. S. Frame, from Michigan State University, independently modiï¬ed the algo-
rithm to its present formâ€”Souriauâ€™s formulation was published in France in 1948, and Frameâ€™s
method appeared in the United States in 1949. Paul Horst (USA, 1935) along with Faddeev
and Sominskii (USSR, 1949) are also credited with rediscovering the technique. Although the
algorithm is intriguingly beautiful, it is not practical for ï¬‚oating-point computations.

7.2 Diagonalization by Similarity Transformations
505
7.2
DIAGONALIZATION BY SIMILARITY TRANSFORMATIONS
The correct choice of a coordinate system (or basis) often can simplify the form
of an equation or the analysis of a particular problem. For example, consider the
obliquely oriented ellipse in Figure 7.2.1 whose equation in the xy -coordinate
system is
13x2 + 10xy + 13y2 = 72.
By rotating the xy -coordinate system counterclockwise through an angle of 45â—¦
x
y
u
v
Figure 7.2.1
into a uv -coordinate system by means of (5.6.13) on p. 326, the cross-product
term is eliminated, and the equation of the ellipse simpliï¬es to become
u2
9 + v2
4 = 1.
Itâ€™s shown in Example 7.6.3 on p. 567 that we can do a similar thing for quadratic
equations in â„œn.
Choosing or changing to the most appropriate coordinate system (or basis)
is always desirable, but in linear algebra it is fundamental. For a linear operator
L on a ï¬nite-dimensional space V, the goal is to ï¬nd a basis B for V such
that the matrix representation of L with respect to B is as simple as possible.
Since diï¬€erent matrix representations A and B of L are related by a similarity
transformation Pâˆ’1AP = B (recall Â§4.8),
70 the fundamental problem for linear
operators is strictly a matrix issueâ€”i.e., ï¬nd a nonsingular matrix P such that
Pâˆ’1AP is as simple as possible. The concept of similarity was ï¬rst introduced
on p. 255, but in the interest of continuity it is reviewed below.
70
While it is helpful to have covered the topics in Â§Â§4.7â€“4.9, much of the subsequent development
is accessible without an understanding of this material.

506
Chapter 7
Eigenvalues and Eigenvectors
Similarity
â€¢
Two n Ã— n matrices A and B are said to be similar whenever
there exists a nonsingular matrix P such that Pâˆ’1AP = B. The
product Pâˆ’1AP is called a similarity transformation on A.
â€¢
A Fundamental Problem. Given a square matrix A, reduce it to
the simplest possible form by means of a similarity transformation.
Diagonal matrices have the simplest form, so we ï¬rst ask, â€œIs every square
matrix similar to a diagonal matrix?â€ Linear algebra and matrix theory would
be simpler subjects if this were true, but itâ€™s not. For example, consider
A =

0
1
0
0

,
(7.2.1)
and observe that A2 = 0 ( A is nilpotent). If there exists a nonsingular matrix
P such that Pâˆ’1AP = D, where D is diagonal, then
D2 = Pâˆ’1APPâˆ’1AP = Pâˆ’1A2P = 0
=â‡’
D = 0
=â‡’
A = 0,
which is false. Thus A, as well as any other nonzero nilpotent matrix, is not sim-
ilar to a diagonal matrix. Nonzero nilpotent matrices are not the only ones that
canâ€™t be diagonalized, but, as we will see, nilpotent matrices play a particularly
important role in nondiagonalizability.
So, if not all square matrices can be diagonalized by a similarity transforma-
tion, what are the characteristics of those that can? An answer is easily derived
by examining the equation
Pâˆ’1AnÃ—nP = D =
ï£«
ï£¬
ï£¬
ï£­
Î»1
0
Â· Â· Â·
0
0
Î»2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
Î»n
ï£¶
ï£·
ï£·
ï£¸,
which implies A [Pâˆ—1 | Â· Â· Â· | Pâˆ—n] = [Pâˆ—1 | Â· Â· Â· | Pâˆ—n]
ï£«
ï£­
Î»1
Â· Â· Â·
0
...
...
...
0
Â· Â· Â·
Î»n
ï£¶
ï£¸or, equiva-
lently, [APâˆ—1 | Â· Â· Â· | APâˆ—n] = [Î»1Pâˆ—1 | Â· Â· Â· | Î»nPâˆ—n] . Consequently, APâˆ—j = Î»jPâˆ—j
for each j, so each (Î»j, Pâˆ—j) is an eigenpair for A. In other words, Pâˆ’1AP = D
implies that P must be a matrix whose columns constitute n linearly indepen-
dent eigenvectors, and D is a diagonal matrix whose diagonal entries are the
corresponding eigenvalues. Itâ€™s straightforward to reverse the above argument to
prove the converseâ€”i.e., if there exists a linearly independent set of n eigenvec-
tors that are used as columns to build a nonsingular matrix P, and if D is the
diagonal matrix whose diagonal entries are the corresponding eigenvalues, then
Pâˆ’1AP = D. Below is a summary.

7.2 Diagonalization by Similarity Transformations
507
Diagonalizability
â€¢
A square matrix A is said to be diagonalizable whenever A is
similar to a diagonal matrix.
â€¢
A complete set of eigenvectors for AnÃ—n is any set of n lin-
early independent eigenvectors for A. Not all matrices have com-
plete sets of eigenvectorsâ€”e.g., consider (7.2.1) or Example 7.1.2.
Matrices that fail to possess complete sets of eigenvectors are some-
times called deï¬cient or defective matrices.
â€¢
AnÃ—n is diagonalizable if and only if A possesses a complete set of
eigenvectors. Moreover, Pâˆ’1AP = diag (Î»1, Î»2, . . . , Î»n) if and only
if the columns of P constitute a complete set of eigenvectors and
the Î»j â€™s are the associated eigenvaluesâ€”i.e., each (Î»j, Pâˆ—j) is an
eigenpair for A.
Example 7.2.1
Problem: If possible, diagonalize the following matrix with a similarity trans-
formation:
A =
ï£«
ï£­
1
âˆ’4
âˆ’4
8
âˆ’11
âˆ’8
âˆ’8
8
5
ï£¶
ï£¸.
Solution: Determine whether or not A has a complete set of three linearly
independent eigenvectors. The characteristic equationâ€”perhaps computed by
using (7.1.5)â€”is
Î»3 + 5Î»2 + 3Î» âˆ’9 = (Î» âˆ’1)(Î» + 3)2 = 0.
Therefore, Î» = 1 is a simple eigenvalue, and Î» = âˆ’3 is repeated twice (we
say its algebraic multiplicity is 2). Bases for the eigenspaces N (A âˆ’1I) and
N (A + 3I) are determined in the usual way to be
N (A âˆ’1I) = span
ï£±
ï£²
ï£³
ï£«
ï£­
1
2
âˆ’2
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
and
N (A + 3I) = span
ï£±
ï£²
ï£³
ï£«
ï£­
1
1
0
ï£¶
ï£¸,
ï£«
ï£­
1
0
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾,
and itâ€™s easy to check that when combined these three eigenvectors constitute a
linearly independent set. Consequently, A must be diagonalizable. To explicitly
exhibit the similarity transformation that diagonalizes A, set
P =
ï£«
ï£­
1
1
1
2
1
0
âˆ’2
0
1
ï£¶
ï£¸,
and verify
Pâˆ’1AP =
ï£«
ï£­
1
0
0
0
âˆ’3
0
0
0
âˆ’3
ï£¶
ï£¸= D.

508
Chapter 7
Eigenvalues and Eigenvectors
Since not all square matrices are diagonalizable, itâ€™s natural to inquire about
the next best thingâ€”i.e., can every square matrix be triangularized by similarity?
This time the answer is yes, but before explaining why, we need to make the
following observation.
Similarity Preserves Eigenvalues
Row reductions donâ€™t preserve eigenvalues (try a simple example). How-
ever, similar matrices have the same characteristic polynomial, so they
have the same eigenvalues with the same multiplicities. Caution! Sim-
ilar matrices need not have the same eigenvectorsâ€”see Exercise 7.2.3.
Proof.
Use the product rule for determinants in conjunction with the fact that
det

Pâˆ’1
= 1/det (P) (Exercise 6.1.6) to write
det (A âˆ’Î»I) = det

Pâˆ’1BP âˆ’Î»I

= det

Pâˆ’1(B âˆ’Î»I)P

= det

Pâˆ’1
det (B âˆ’Î»I)det (P) = det (B âˆ’Î»I).
In the context of linear operators, this means that the eigenvalues of a matrix
representation of an operator L are invariant under a change of basis. In other
words, the eigenvalues are intrinsic to L in the sense that they are independent
of any coordinate representation.
Now we can establish the fact that every square matrix can be triangularized
by a similarity transformation. In fact, as Issai Schur (p. 123) realized in 1909,
the similarity transformation always can be made to be unitary.
Schurâ€™s Triangularization Theorem
Every square matrix is unitarily similar to an upper-triangular matrix.
That is, for each AnÃ—n, there exists a unitary matrix U (not unique)
and an upper-triangular matrix T (not unique) such that Uâˆ—AU = T,
and the diagonal entries of T are the eigenvalues of A.
Proof.
Use induction on n, the size of the matrix. For n = 1, there is nothing
to prove. For n > 1, assume that all n âˆ’1 Ã— n âˆ’1 matrices are unitarily similar
to an upper-triangular matrix, and consider an n Ã— n matrix A. Suppose that
(Î», x) is an eigenpair for A, and suppose that x has been normalized so that
âˆ¥xâˆ¥2 = 1. As discussed on p. 325, we can construct an elementary reï¬‚ector
R = Râˆ—= Râˆ’1 with the property that Rx = e1 or, equivalently, x = Re1
(set R = I if x = e1). Thus x is the ï¬rst column in R, so R =

x | V

, and
RAR = RA

x | V

= R

Î»x | AV

=

Î»e1 | RAV

=

Î»
xâˆ—AV
0
Vâˆ—AV

.

7.2 Diagonalization by Similarity Transformations
509
Since Vâˆ—AV is n âˆ’1 Ã— n âˆ’1, the induction hypothesis insures that there exists
a unitary matrix Q such that Qâˆ—(Vâˆ—AV)Q = ËœT is upper triangular. If U =
R
 1
0
0
Q

, then U is unitary (because Uâˆ—= Uâˆ’1), and
Uâˆ—AU =

Î»
xâˆ—AVQ
0
Qâˆ—Vâˆ—AVQ

=

Î»
xâˆ—AVQ
0
ËœT

= T
is upper triangular. Since similar matrices have the same eigenvalues, and since
the eigenvalues of a triangular matrix are its diagonal entries (Exercise 7.1.3),
the diagonal entries of T must be the eigenvalues of A.
Example 7.2.2
The Cayleyâ€“Hamilton
71 theorem asserts that every square matrix satisï¬es
its own characteristic equation p(Î») = 0. That is, p(A) = 0.
Problem: Show how the Cayleyâ€“Hamilton theorem follows from Schurâ€™s trian-
gularization theorem.
Solution: Schurâ€™s theorem insures the existence of a unitary U such that
Uâˆ—AU = T is triangular, and the development allows for the eigenvalues A to
appear in any given order on the diagonal of T. So, if Ïƒ (A) = {Î»1, Î»2, . . . , Î»k}
with Î»i repeated ai times, then there is a unitary U such that
Uâˆ—AU=T=
ï£«
ï£¬
ï£¬
ï£­
T1
â‹†
Â· Â· Â·
â‹†
T2
Â· Â· Â·
â‹†
...
...
Tk
ï£¶
ï£·
ï£·
ï£¸,
where Ti =
ï£«
ï£¬
ï£¬
ï£­
Î»i
â‹†
Â· Â· Â·
â‹†
Î»i
Â· Â· Â·
â‹†
...
...
Î»i
ï£¶
ï£·
ï£·
ï£¸
aiÃ—ai
.
Consequently, (Ti âˆ’Î»iI)ai = 0, so (T âˆ’Î»iI)ai has the form
(T âˆ’Î»iI)ai =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
â‹†
Â· Â· Â·
â‹†
Â· Â· Â·
â‹†
...
...
...
0
Â· Â· Â·
â‹†
...
...
â‹†
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
â†âˆ’ith row of blocks.
71
William Rowan Hamilton (1805â€“1865), an Irish mathematical astronomer, established this
result in 1853 for his quaternions, matrices of the form

a + bi
c + di
âˆ’c + di
a âˆ’bi

that resulted
from his attempt to generalize complex numbers. In 1858 Arthur Cayley (p. 80) enunciated
the general result, but his argument was simply to make direct computations for 2 Ã— 2 and
3 Ã— 3 matrices. Cayley apparently didnâ€™t appreciate the subtleties of the result because he
stated that a formal proof â€œwas not necessary.â€ Hamiltonâ€™s quaternions took shape in his mind
while walking with his wife along the Royal Canal in Dublin, and he was so inspired that he
stopped to carve his idea in the stone of the Brougham Bridge. He believed quaternions would
revolutionize mathematical physics, and he spent the rest of his life working on them. But the
world did not agree. Hamilton became an unhappy man addicted to alcohol who is reported
to have died from a severe attack of gout.

510
Chapter 7
Eigenvalues and Eigenvectors
This form insures that (T âˆ’Î»1I)a1(T âˆ’Î»2I)a2 Â· Â· Â· (T âˆ’Î»kI)ak = 0. The charac-
teristic equation for A is p(Î») = (Î» âˆ’Î»1)a1(Î» âˆ’Î»2)a2 Â· Â· Â· (Î» âˆ’Î»k)ak = 0, so
Uâˆ—p(A)U = Uâˆ—(A âˆ’Î»1I)a1(A âˆ’Î»2I)a2 Â· Â· Â· (A âˆ’Î»kI)akU
= (T âˆ’Î»1I)a1(T âˆ’Î»2I)a2 Â· Â· Â· (T âˆ’Î»kI)ak = 0,
and thus p(A) = 0. Note: A completely diï¬€erent approach to the Cayleyâ€“
Hamilton theorem is discussed on p. 532.
Schurâ€™s theorem is not the complete story on triangularizing by similarity.
By allowing nonunitary similarity transformations, the structure of the upper-
triangular matrix T can be simpliï¬ed to contain zeros everywhere except on
the diagonal and the superdiagonal (the diagonal immediately above the main
diagonal). This is the Jordan form developed on p. 590, but some of the seeds
are sown here.
Multiplicities
For Î» âˆˆÏƒ (A) = {Î»1, Î»2, . . . , Î»s} , we adopt the following deï¬nitions.
â€¢
The algebraic multiplicity of Î» is the number of times it is re-
peated as a root of the characteristic polynomial. In other words,
alg multA (Î»i) = ai if and only if (x âˆ’Î»1)a1 Â· Â· Â· (x âˆ’Î»s)as = 0 is
the characteristic equation for A.
â€¢
When alg multA (Î») = 1, Î» is called a simple eigenvalue.
â€¢
The geometric multiplicity of Î» is dim N (A âˆ’Î»I). In other
words, geo multA (Î») is the maximal number of linearly independent
eigenvectors associated with Î».
â€¢
Eigenvalues such that alg multA (Î») = geo multA (Î») are called
semisimple eigenvalues of A. It follows from (7.2.2) on p. 511
that a simple eigenvalue is always semisimple, but not conversely.
Example 7.2.3
The algebraic and geometric multiplicity need not agree. For example, the nilpo-
tent matrix A =
 0
1
0
0

in (7.2.1) has only one distinct eigenvalue, Î» = 0,
that is repeated twice, so alg multA (0) = 2. But
dim N (A âˆ’0I) = dim N (A) = 1
=â‡’
geo multA (0) = 1.
In other words, there is only one linearly independent eigenvector associated with
Î» = 0 even though Î» = 0 is repeated twice as an eigenvalue.
Example 7.2.3 shows that geo multA (Î») < alg multA (Î») is possible. How-
ever, the inequality can never go in the reverse direction.

7.2 Diagonalization by Similarity Transformations
511
Multiplicity Inequality
For every A âˆˆCnÃ—n, and for each Î» âˆˆÏƒ(A),
geo multA (Î») â‰¤alg multA (Î») .
(7.2.2)
Proof.
Suppose alg multA (Î») = k. Schurâ€™s triangularization theorem (p. 508)
insures the existence of a unitary U such that Uâˆ—AnÃ—nU =
 T11
T12
0
T22

,
where T11 is a k Ã— k upper-triangular matrix whose diagonal entries are equal
to Î», and T22 is an n âˆ’k Ã— n âˆ’k upper-triangular matrix with Î» /âˆˆÏƒ (T22) .
Consequently, T22 âˆ’Î»I is nonsingular, so
rank (A âˆ’Î»I) = rank (Uâˆ—(A âˆ’Î»I)U) = rank

T11 âˆ’Î»I
T12
0
T22 âˆ’Î»I

â‰¥rank (T22 âˆ’Î»I) = n âˆ’k.
The inequality follows from the fact that the rank of a matrix is at least as great
as the rank of any submatrixâ€”recall the result on p. 215. Therefore,
alg multA (Î») = k â‰¥n âˆ’rank (A âˆ’Î»I) = dim N (A âˆ’Î»I) = geo multA (Î») .
Determining whether or not AnÃ—n is diagonalizable is equivalent to deter-
mining whether or not A has a complete linearly independent set of eigenvectors,
and this can be done if you are willing and able to compute all of the eigenvalues
and eigenvectors for A. But this brute force approach can be a monumental
task. Fortunately, there are some theoretical tools to help determine how many
linearly independent eigenvectors a given matrix possesses.
Independent Eigenvectors
Let {Î»1, Î»2, . . . , Î»k} be a set of distinct eigenvalues for A.
â€¢
If {(Î»1, x1), (Î»2, x2), . . . , (Î»k, xk)} is a set of eigenpairs for
A, then S = {x1, x2, . . . , xk} is a linearly independent set.
(7.2.3)
â€¢
If Bi is a basis for N (A âˆ’Î»iI), then B = B1âˆªB2âˆªÂ· Â· Â·âˆªBk
is a linearly independent set.
(7.2.4)

512
Chapter 7
Eigenvalues and Eigenvectors
Proof of (7.2.3).
Suppose S is a dependent set. If the vectors in S are arranged
so that M = {x1, x2, . . . , xr} is a maximal linearly independent subset, then
xr+1 =
r

i=1
Î±ixi,
and multiplication on the left by A âˆ’Î»r+1I produces
0 =
r

i=1
Î±i (Axi âˆ’Î»r+1xi) =
r

i=1
Î±i (Î»i âˆ’Î»r+1) xi.
Because M is linearly independent, Î±i (Î»i âˆ’Î»r+1) = 0 for each i. Conse-
quently, Î±i = 0 for each i (because the eigenvalues are distinct), and hence
xr+1 = 0. But this is impossible because eigenvectors are nonzero. Therefore,
the supposition that S is a dependent set must be false.
Proof of (7.2.4).
The result of Exercise 5.9.14 guarantees that B is linearly
independent if and only if
Mj = N (A âˆ’Î»jI) âˆ©

N (A âˆ’Î»1I) + N (A âˆ’Î»2I) + Â· Â· Â· + N (A âˆ’Î»jâˆ’1I)

= 0
for each j = 1, 2, . . . , k. Suppose we have 0 Ì¸= x âˆˆMj for some j. Then
Ax = Î»jx and x = v1 + v2 + Â· Â· Â· + vjâˆ’1 for vi âˆˆN (A âˆ’Î»iI), which implies
jâˆ’1

i=1
(Î»i âˆ’Î»j)vi =
jâˆ’1

i=1
Î»ivi âˆ’Î»j
jâˆ’1

i=1
vi = Ax âˆ’Î»jx = 0.
By (7.2.3), the vi â€™s are linearly independent, and hence Î»i âˆ’Î»j = 0 for each
i = 1, 2, . . . , j âˆ’1. But this is impossible because the eigenvalues are distinct.
Therefore, Mj = 0 for each j, and thus B is linearly independent.
These results lead to the following characterization of diagonalizability.
Diagonalizability and Multiplicities
A matrix AnÃ—n is diagonalizable if and only if
geo multA (Î») = alg multA (Î»)
(7.2.5)
for each Î» âˆˆÏƒ(A) â€”i.e., if and only if every eigenvalue is semisimple.

7.2 Diagonalization by Similarity Transformations
513
Proof.
Suppose geo multA (Î»i) = alg multA (Î»i) = ai for each eigenvalue Î»i.
If there are k distinct eigenvalues, and if Bi is a basis for N (A âˆ’Î»iI), then
B = B1 âˆªB2 âˆªÂ· Â· Â· âˆªBk contains k
i=1 ai = n vectors. We just proved in (7.2.4)
that B is a linearly independent set, so B represents a complete set of linearly
independent eigenvectors of A, and we know this insures that A must be
diagonalizable. Conversely, if A is diagonalizable, and if Î» is an eigenvalue for
A with alg multA (Î») = a, then there is a nonsingular matrix P such that
Pâˆ’1AP = D =

Î»IaÃ—a
0
0
B

,
where Î» /âˆˆÏƒ(B). Consequently,
rank (A âˆ’Î»I) = rank P

0
0
0
B âˆ’Î»I

Pâˆ’1 = rank (B âˆ’Î»I) = n âˆ’a,
and thus
geo multA (Î») = dim N (A âˆ’Î»I) = n âˆ’rank (A âˆ’Î»I) = a = alg multA (Î») .
Example 7.2.4
Problem: Determine if either of the following matrices is diagonalizable:
A =
ï£«
ï£­
âˆ’1
âˆ’1
âˆ’2
8
âˆ’11
âˆ’8
âˆ’10
11
7
ï£¶
ï£¸,
B =
ï£«
ï£­
1
âˆ’4
âˆ’4
8
âˆ’11
âˆ’8
âˆ’8
8
5
ï£¶
ï£¸.
Solution: Each matrix has exactly the same characteristic equation
Î»3 + 5Î»2 + 3Î» âˆ’9 = (Î» âˆ’1)(Î» + 3)2 = 0,
so Ïƒ (A) = {1, âˆ’3} = Ïƒ (B) , where Î» = 1 has algebraic multiplicity 1 and
Î» = âˆ’3 has algebraic multiplicity 2. Since
geo multA (âˆ’3) = dim N (A + 3I) = 1 < alg multA (âˆ’3) ,
A is not diagonalizable. On the other hand,
geo multB (âˆ’3) = dim N (B + 3I) = 2 = alg multB (âˆ’3) ,
and geo multB (1) = 1 = alg multB (1) , so B is diagonalizable.
If AnÃ—n happens to have n distinct eigenvalues, then each eigenvalue is
simple. This means that geo multA (Î») = alg multA (Î») = 1 for each Î», so
(7.2.5) produces the following corollary guaranteeing diagonalizability.

514
Chapter 7
Eigenvalues and Eigenvectors
Distinct Eigenvalues
If no eigenvalue of A is repeated, then A is diagonalizable.
(7.2.6)
Caution! The converse is not trueâ€”see Example 7.2.4.
Example 7.2.5
Toeplitz
72 matrices have constant entries on each diagonal parallel to the main
diagonal. For example, a 4 Ã— 4 Toeplitz matrix T along with a tridiagonal
Toeplitz matrix A are shown below:
T =
ï£«
ï£¬
ï£­
t0
t1
t2
t3
tâˆ’1
t0
t1
t2
tâˆ’2
tâˆ’1
t0
t1
tâˆ’3
tâˆ’2
tâˆ’1
t0
ï£¶
ï£·
ï£¸,
A =
ï£«
ï£¬
ï£­
t0
t1
0
0
tâˆ’1
t0
t1
0
0
tâˆ’1
t0
t1
0
0
tâˆ’1
t0
ï£¶
ï£·
ï£¸.
Toeplitz structures occur naturally in a variety of applications, and tridiago-
nal Toeplitz matrices are commonly the result of discretizing diï¬€erential equa-
tion problemsâ€”e.g., see Â§1.4 (p. 18) and Example 7.6.1 (p. 559). The Toeplitz
structure is rich in special properties, but tridiagonal Toeplitz matrices are par-
ticularly nice because they are among the few nontrivial structures that admit
formulas for their eigenvalues and eigenvectors.
Problem: Show that the eigenvalues and eigenvectors of
A =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
b
a
c
b
a
...
...
...
c
b
a
c
b
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
nÃ—n
with
a Ì¸= 0 Ì¸= c
are given by
Î»j = b + 2a

c/a cos
 jÏ€
n + 1

and
xj =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
(c/a)1/2 sin (1jÏ€/(n + 1))
(c/a)2/2 sin (2jÏ€/(n + 1))
(c/a)3/2 sin (3jÏ€/(n + 1))
...
(c/a)n/2 sin (njÏ€/(n + 1))
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
72
Otto Toeplitz (1881â€“1940) was a professor in Bonn, Germany, but because of his Jewish back-
ground he was dismissed from his chair by the Nazis in 1933. In addition to the matrix that
bears his name, Toeplitz is known for his general theory of inï¬nite-dimensional spaces devel-
oped in the 1930s.

7.2 Diagonalization by Similarity Transformations
515
for j = 1, 2, . . . , n, and conclude that A is diagonalizable.
Solution: For an eigenpair (Î», x), the components in (A âˆ’Î»I) x = 0 are
cxkâˆ’1+(bâˆ’Î»)xk+axk+1 = 0, k = 1, . . . , n with x0 = xn+1 = 0 or, equivalently,
xk+2+
b âˆ’Î»
a

xk+1+
 c
a

xk = 0
for k = 0, . . . , n âˆ’1 with x0 = xn+1 = 0.
These are second-order homogeneous diï¬€erence equations, and solving them is
similar to solving analogous diï¬€erential equations. The technique is to seek solu-
tions of the form xk = Î¾rk for constants Î¾ and r. This produces the quadratic
equation r2 + (b âˆ’Î»)r/a + c/a = 0 with roots r1 and r2, and it can be argued
that the general solution of xk+2 + ((b âˆ’Î»)/a)xk+1 + (c/a)xk = 0 is
xk =
	 Î±rk
1 + Î²rk
2
if r1 Ì¸= r2,
Î±Ïk + Î²kÏk
if r1 = r2 = Ï,
where Î± and Î² are arbitrary constants.
For the eigenvalue problem at hand, r1 and r2 must be distinctâ€”otherwise
xk = Î±Ïk + Î²kÏk, and x0 = xn+1 = 0 implies each xk = 0, which is impossible
because x is an eigenvector. Hence xk = Î±rk
1 + Î²rk
2, and x0 = xn+1 = 0 yields
	
0 = Î± + Î²
0 = Î±rn+1
1
+ Î²rn+1
2

=â‡’
r1
r2
n+1
= âˆ’Î²
Î± = 1
=â‡’
r1
r2
= ei2Ï€j/(n+1),
so r1 = r2ei2Ï€j/(n+1) for some 1 â‰¤j â‰¤n. Couple this with
r2 + (b âˆ’Î»)r
a
+ c
a = (r âˆ’r1)(r âˆ’r2)
=â‡’
	
r1r2 = c/a
r1 + r2 = âˆ’(b âˆ’Î»)/a
to conclude that r1 =

c/a eiÏ€j/(n+1), r2 =

c/a eâˆ’iÏ€j/(n+1), and
Î» = b + a

c/a

eiÏ€j/(n+1) + eâˆ’iÏ€j/(n+1)
= b + 2a

c/a cos
 jÏ€
n + 1

.
Therefore, the eigenvalues of A must be given by
Î»j = b + 2a

c/a cos
 jÏ€
n + 1

,
j = 1, 2, . . . , n.
Since these Î»j â€™s are all distinct (cos Î¸ is a strictly decreasing function of Î¸ on
(0, Ï€), and a Ì¸= 0 Ì¸= c), A must be diagonalizableâ€”recall (7.2.6). Finally, the
kth component of any eigenvector associated with Î»j satisï¬es xk = Î±rk
1 + Î²rk
2
with Î± + Î² = 0, so
xk = Î±
 c
a
k/2
eiÏ€jk/(n+1) âˆ’eâˆ’iÏ€jk/(n+1)
= 2iÎ±
 c
a
k/2
sin
 jkÏ€
n + 1

.

516
Chapter 7
Eigenvalues and Eigenvectors
Setting Î± = 1/2i yields a particular eigenvector associated with Î»j as
xj =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
(c/a)1/2 sin (1jÏ€/(n + 1))
(c/a)2/2 sin (2jÏ€/(n + 1))
(c/a)3/2 sin (3jÏ€/(n + 1))
...
(c/a)n/2 sin (njÏ€/(n + 1))
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Because the Î»j â€™s are distinct, {x1, x2, . . . , xn} is a complete linearly indepen-
dent setâ€”recall (7.2.3)â€”so P =

x1 | x2 | Â· Â· Â· | xn

diagonalizes A.
Itâ€™s often the case that a right-hand and left-hand eigenvector for some
eigenvalue is known. Rather than starting from scratch to ï¬nd additional eigen-
pairs, the known information can be used to reduce or â€œdeï¬‚ateâ€ the problem to
a smaller one as described in the following example.
Example 7.2.6
Deï¬‚ation. Suppose that right-hand and left-hand eigenvectors x and yâˆ—for an
eigenvalue Î» of A âˆˆâ„œnÃ—n are already known, so Ax = Î»x and yâˆ—A = Î»yâˆ—.
Furthermore, suppose yâˆ—x Ì¸= 0 â€”such eigenvectors are guaranteed to exist if Î»
is simple or if A is diagonalizable (Exercises 7.2.23 and 7.2.22).
Problem: Use x and yâˆ—to deï¬‚ate the size of the remaining eigenvalue problem.
Solution: Scale x and yâˆ—so that yâˆ—x = 1, and construct XnÃ—nâˆ’1 so that its
columns are an orthonormal basis for yâŠ¥. An easy way of doing this is to build
a reï¬‚ector R =
Ëœy | X

having Ëœy = y/ âˆ¥yâˆ¥2 as its ï¬rst column as described on
p. 325. If P =

x | X

, then straightforward multiplication shows that
Pâˆ’1 =

yâˆ—
Xâˆ—(I âˆ’xyâˆ—)

and
Pâˆ’1AP =

Î»
0
0
B

,
where B = Xâˆ—AX is n âˆ’1 Ã— n âˆ’1. The eigenvalues of B constitute the re-
maining eigenvalues of A (Exercise 7.1.4), and thus an n Ã— n eigenvalue prob-
lem is deï¬‚ated to become one of size n âˆ’1 Ã— n âˆ’1.
Note: When A is symmetric, we can take x = y to be an eigenvector with
âˆ¥xâˆ¥2 = 1, so P = R = Râˆ’1, and RAR =
 Î»
0
0
B

in which B = BT .
An elegant and more geometrical way of expressing diagonalizability is now
presented to help simplify subsequent analyses and pave the way for extensions.

7.2 Diagonalization by Similarity Transformations
517
Spectral Theorem for Diagonalizable Matrices
A matrix AnÃ—n with spectrum Ïƒ(A) = {Î»1, Î»2, . . . , Î»k} is diagonaliz-
able if and only if there exist matrices {G1, G2, . . . , Gk} such that
A = Î»1G1 + Î»2G2 + Â· Â· Â· + Î»kGk,
(7.2.7)
where the Gi â€™s have the following properties.
â€¢
Gi is the projector onto N (A âˆ’Î»iI) along R (A âˆ’Î»iI).
(7.2.8)
â€¢
GiGj = 0 whenever i Ì¸= j.
(7.2.9)
â€¢
G1 + G2 + Â· Â· Â· + Gk = I.
(7.2.10)
The expansion (7.2.7) is known as the spectral decomposition of A,
and the Gi â€™s are called the spectral projectors associated with A.
Proof.
If A is diagonalizable, and if Xi is a matrix whose columns form a
basis for N (A âˆ’Î»iI), then P =

X1 | X2 | Â· Â· Â· | Xk

is nonsingular. If Pâˆ’1 is
partitioned in a conformable manner, then we must have
A = PDPâˆ’1 =

X1 | X2 | Â· Â· Â· | Xk

ï£«
ï£¬
ï£¬
ï£­
Î»1I
0
Â· Â· Â·
0
0
Î»2I
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
Î»kI
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
YT
1
YT
2
...
YT
k
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
= Î»1X1YT
1 + Î»2X2YT
2 + Â· Â· Â· + Î»kXkYT
k
= Î»1G1 + Î»2G2 + Â· Â· Â· + Î»kGk.
(7.2.11)
For Gi = XiYT
i , the statement PPâˆ’1 = I translates to k
i=1 Gi = I, and
Pâˆ’1P = I
=â‡’
YT
i Xj =
	
I
when i = j,
0
when i Ì¸= j,
=â‡’
	
G2
i = Gi,
GiGj = 0
when i Ì¸= j.
To establish that R (Gi) = N (A âˆ’Î»iI), use R (AB) âŠ†R (A) (Exercise 4.2.12)
and YT
i Xi = I to write
R (Gi) = R(XiYT
i ) âŠ†R (Xi) = R(XiYT
i Xi) = R(GiXi) âŠ†R (Gi).
Thus R (Gi) = R (Xi) = N (A âˆ’Î»iI). To show N (Gi) = R (A âˆ’Î»iI), use
A = k
j=1 Î»jGj with the already established properties of the Gi â€™s to conclude
Gi(A âˆ’Î»iI) = Gi
ï£«
ï£­
k

j=1
Î»jGj âˆ’Î»i
k

j=1
Gj
ï£¶
ï£¸= 0
=â‡’
R (A âˆ’Î»iI) âŠ†N (Gi).

518
Chapter 7
Eigenvalues and Eigenvectors
But we already know that N (A âˆ’Î»iI) = R (Gi), so
dim R (A âˆ’Î»iI) = n âˆ’dim N (A âˆ’Î»iI) = n âˆ’dim R (Gi) = dim N (Gi),
and therefore, by (4.4.6), R (A âˆ’Î»iI) = N (Gi). Conversely, if there exist ma-
trices Gi satisfying (7.2.8)â€“(7.2.10), then A must be diagonalizable. To see
this, note that (7.2.8) insures dim R (Gi) = dim N (A âˆ’Î»iI) = geo multA (Î»i) ,
while (7.2.9) implies R (Gi) âˆ©R (Gj) = 0 and R
 k
i=1 Gi

= k
i=1 R (Gi)
(Exercise 5.9.17). Use these with (7.2.10) in the formula for the dimension of a
sum (4.4.19) to write
n = dim R (I) = dim R (G1 + G2 + Â· Â· Â· + Gk)
= dim [R (G1) + R (G2) + Â· Â· Â· + R (Gk)]
= dim R (G1) + dim R (G2) + Â· Â· Â· + dim R (Gk)
= geo multA (Î»1) + geo multA (Î»2) + Â· Â· Â· + geo multA (Î»k) .
Since geo multA (Î»i) â‰¤alg multA (Î»i) and k
i=1 alg multA (Î»i) = n, the above
equation insures that geo multA (Î»i) = alg multA (Î»i) for each i, and, by
(7.2.5), this means A is diagonalizable.
Simple Eigenvalues and Projectors
If x and yâˆ—are respective right-hand and left-hand eigenvectors asso-
ciated with a simple eigenvalue Î» âˆˆÏƒ (A) , then
G = xyâˆ—/yâˆ—x
(7.2.12)
is the projector onto N (A âˆ’Î»I) along R (A âˆ’Î»I). In the context
of the spectral theorem (p. 517), this means that G is the spectral
projector associated with Î».
Proof.
Itâ€™s not diï¬ƒcult to prove yâˆ—x Ì¸= 0 (Exercise 7.2.23), and itâ€™s clear that
G is a projector because G2 = x(yâˆ—x)yâˆ—/(yâˆ—x)2 = G. Now determine R (G).
The image of any z is Gz = Î±x with Î± = yâˆ—z/yâˆ—x, so
R (G) âŠ†span {x} = N (A âˆ’Î»I)
and
dim R (G) = 1 = dim N (A âˆ’Î»I).
Thus R (G) = N (A âˆ’Î»I). To ï¬nd N (G), recall N (G) = R (I âˆ’G) (see
(5.9.11), p. 386), and observe that yâˆ—(A âˆ’Î»I) = 0
=â‡’
yâˆ—(I âˆ’G) = 0, so
R (A âˆ’Î»I)âŠ¥âŠ†R (I âˆ’G)âŠ¥= N (G)âŠ¥=â‡’N (G) âŠ†R (A âˆ’Î»I) (Exercise 5.11.5).
But dim N (G) = nâˆ’dim R (G) =nâˆ’1 =nâˆ’dim N (A âˆ’Î»I) = dim R (A âˆ’Î»I),
so N (G) = R (A âˆ’Î»I).

7.2 Diagonalization by Similarity Transformations
519
Example 7.2.7
Problem: Determine the spectral projectors for A =

1
âˆ’4
âˆ’4
8
âˆ’11
âˆ’8
âˆ’8
8
5

.
Solution: This is the diagonalizable matrix from Example 7.2.1 (p. 507). Since
there are two distinct eigenvalues, Î»1 = 1 and Î»2 = âˆ’3, there are two spectral
projectors,
G1 = the projector onto N (A âˆ’1I) along R (A âˆ’1I),
G2 = the projector onto N (A + 3I) along R (A + 3I).
There are several diï¬€erent ways to ï¬nd these projectors.
1.
Compute bases for the necessary nullspaces and ranges, and use (5.9.12).
2.
Compute Gi = XiYT
i
as described in (7.2.11). The required computations
are essentially the same as those needed above. Since much of the work has
already been done in Example 7.2.1, letâ€™s complete the arithmetic. We have
P =
ï£«
ï£­
1
1
1
2
1
0
âˆ’2
0
1
ï£¶
ï£¸=

X1 | X2

,
Pâˆ’1 =
ï£«
ï£­
1
âˆ’1
âˆ’1
âˆ’2
3
2
2
âˆ’2
âˆ’1
ï£¶
ï£¸=
 YT
1
YT
2
!
,
so
G1 = X1YT
1 =
ï£«
ï£­
1
âˆ’1
âˆ’1
2
âˆ’2
âˆ’2
âˆ’2
2
2
ï£¶
ï£¸,
G2 = X2YT
2 =
ï£«
ï£­
0
1
1
âˆ’2
3
2
2
âˆ’2
âˆ’1
ï£¶
ï£¸.
Check that these are correct by conï¬rming the validity of (7.2.7)â€“(7.2.10).
3.
Since Î»1 = 1 is a simple eigenvalue, (7.2.12) may be used to compute G1
from any pair of associated right-hand and left-hand eigenvectors x and yT .
Of course, P and Pâˆ’1 are not needed to determine such a pair, but since P
and Pâˆ’1 have been computed above, we can use X1 and YT
1 to make the
point that any right-hand and left-hand eigenvectors associated with Î»1 = 1
will do the job because they are all of the form x = Î±X1 and yT = Î²YT
1
for Î± Ì¸= 0 Ì¸= Î². Consequently,
G1 = xyT
yT x =
Î±
ï£«
ï£­
1
2
âˆ’2
ï£¶
ï£¸Î² ( 1
âˆ’1
âˆ’1 )
Î±Î²
=
ï£«
ï£­
1
âˆ’1
âˆ’1
2
âˆ’2
âˆ’2
âˆ’2
2
2
ï£¶
ï£¸.
Invoking (7.2.10) yields the other spectral projector as G2 = I âˆ’G1.
4.
An even easier solution is obtained from the spectral theorem by writing
A âˆ’I = (1G1 âˆ’3G2) âˆ’(G1 + G2) = âˆ’4G2,
A + 3I = (1G1 âˆ’3G2) + 3 (G1 + G2) = 4G1,

520
Chapter 7
Eigenvalues and Eigenvectors
so that
G1 = (A + 3I)
4
and
G2 = âˆ’(A âˆ’I)
4
.
Can you see how to make this rather ad hoc technique work in more general
situations?
5.
In fact, the technique above is really a special case of a completely general
formula giving each Gi as a function A and Î»i as
Gi =
k"
j=1
jÌ¸=i
(A âˆ’Î»jI)
k"
j=1
jÌ¸=i
(Î»i âˆ’Î»j)
.
This â€œinterpolation formulaâ€ is developed on p. 529.
Below is a summary of the facts concerning diagonalizability.
Summary of Diagonalizability
For an n Ã— n matrix A with spectrum Ïƒ(A) = {Î»1, Î»2, . . . , Î»k} , the
following statements are equivalent.
â€¢
A is similar to a diagonal matrixâ€”i.e., Pâˆ’1AP = D.
â€¢
A has a complete linearly independent set of eigenvectors.
â€¢
Every Î»i is semisimpleâ€”i.e., geo multA (Î»i) = alg multA (Î»i) .
â€¢
A = Î»1G1 + Î»2G2 + Â· Â· Â· + Î»kGk, where
â–·
Gi is the projector onto N (A âˆ’Î»iI) along R (A âˆ’Î»iI),
â–·
GiGj = 0 whenever i Ì¸= j,
â–·
G1 + G2 + Â· Â· Â· + Gk = I,
â–·
Gi =
k"
j=1
jÌ¸=i
(A âˆ’Î»jI)
# k"
j=1
jÌ¸=i
(Î»i âˆ’Î»j)
(see (7.3.11) on p. 529).
â–·If Î»i is a simple eigenvalue associated with right-hand and left-
hand eigenvectors x and yâˆ—, respectively, then Gi = xyâˆ—/yâˆ—x.
Exercises for section 7.2
7.2.1. Diagonalize A =
 âˆ’8
âˆ’6
12
10

with a similarity transformation, or else
explain why A canâ€™t be diagonalized.

7.2 Diagonalization by Similarity Transformations
521
7.2.2. (a)
Verify that alg multA (Î») = geo multA (Î») for each eigenvalue of
A =
ï£«
ï£­
âˆ’4
âˆ’3
âˆ’3
0
âˆ’1
0
6
6
5
ï£¶
ï£¸.
(b)
Find a nonsingular P such that Pâˆ’1AP is a diagonal matrix.
7.2.3. Show that similar matrices need not have the same eigenvectors by
giving an example of two matrices that are similar but have diï¬€erent
eigenspaces.
7.2.4. Î» = 2 is an eigenvalue for A =

3
2
1
0
2
0
âˆ’2
âˆ’3
0

. Find alg multA (Î») as
well as geo multA (Î») . Can you conclude anything about the diagonal-
izability of A from these results?
7.2.5. If B = Pâˆ’1AP, explain why Bk = Pâˆ’1AkP.
7.2.6. Compute limnâ†’âˆAn for A =
 7/5
1/5
âˆ’1
1/2

.
7.2.7. Let {x1, x2, . . . , xt} be a set of linearly independent eigenvectors for
AnÃ—n associated with respective eigenvalues {Î»1, Î»2, . . . , Î»t} , and let
X be any n Ã— (n âˆ’t) matrix such that PnÃ—n =

x1 | Â· Â· Â· | xt | X

is
nonsingular. Prove that if Pâˆ’1 =
ï£«
ï£¬
ï£¬
ï£­
yâˆ—
1
...
yâˆ—
t
Yâˆ—
ï£¶
ï£·
ï£·
ï£¸, where the yâˆ—
i â€™s are rows
and Yâˆ—is (n âˆ’t) Ã— n, then {yâˆ—
1, yâˆ—
2, . . . , yâˆ—
t } is a set of linearly inde-
pendent left-hand eigenvectors associated with {Î»1, Î»2, . . . , Î»t} , respec-
tively (i.e., yâˆ—
i A = Î»iyâˆ—
i ).
7.2.8. Let A be a diagonalizable matrix, and let Ï(â‹†) denote the spectral
radius (recall Example 7.1.4 on p. 497). Prove that limkâ†’âˆAk = 0 if
and only if Ï(A) < 1. Note: It is demonstrated on p. 617 that this
result holds for nondiagonalizable matrices as well.
7.2.9. Apply the technique used to prove Schurâ€™s triangularization theorem
(p. 508) to construct an orthogonal matrix P such that PT AP is
upper triangular for A =
 13
âˆ’9
16
âˆ’11

.

522
Chapter 7
Eigenvalues and Eigenvectors
7.2.10. Verify the Cayleyâ€“Hamilton theorem for A =

1
âˆ’4
âˆ’4
8
âˆ’11
âˆ’8
âˆ’8
8
5

.
Hint: This is the matrix from Example 7.2.1 on p. 507.
7.2.11. Since each row sum in the following symmetric matrix A is 4, itâ€™s clear
that x = (1, 1, 1, 1)T
is both a right-hand and left-hand eigenvector
associated with Î» = 4 âˆˆÏƒ (A) . Use the deï¬‚ation technique of Example
7.2.6 (p. 516) to determine the remaining eigenvalues of
A =
ï£«
ï£¬
ï£­
1
0
2
1
0
2
1
1
2
1
1
0
1
1
0
2
ï£¶
ï£·
ï£¸.
7.2.12. Explain why AGi = GiA = Î»iGi for the spectral projector Gi asso-
ciated with the eigenvalue Î»i of a diagonalizable matrix A.
7.2.13. Prove that A = cnÃ—1dT
1Ã—n is diagonalizable if and only if dT c Ì¸= 0.
7.2.14. Prove that A =
 W
0
0
Z

is diagonalizable if and only if WsÃ—s and
ZtÃ—t are each diagonalizable.
7.2.15. Prove that if AB = BA, then A and B can be simultaneously tri-
angularized by a unitary similarity transformationâ€”i.e., Uâˆ—AU = T1
and Uâˆ—BU = T2 for some unitary matrix U. Hint: Recall Exercise
7.1.20 (p. 503) along with the development of Schurâ€™s triangularization
theorem (p. 508).
7.2.16. For diagonalizable matrices, prove that AB = BA if and only if A
and B can be simultaneously diagonalizedâ€”i.e., Pâˆ’1AP = D1 and
Pâˆ’1BP = D2 for some P. Hint: If A and B commute, then so do
Pâˆ’1AP =
 Î»1I
0
0
D

and Pâˆ’1BP =
 W
X
Y
Z

.
7.2.17. Explain why the following â€œproofâ€ of the Cayleyâ€“Hamilton theorem is
not valid. p(Î») = det (A âˆ’Î»I) =â‡’p(A) = det (A âˆ’AI) = det (0) = 0.
7.2.18. Show that the eigenvalues of the ï¬nite diï¬€erence matrix (p. 19)
A =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
2
âˆ’1
âˆ’1
2
âˆ’1
...
...
...
âˆ’1
2
âˆ’1
âˆ’1
2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
nÃ—n
are Î»j = 4 sin2
jÏ€
2(n + 1), 1 â‰¤j â‰¤n.

7.2 Diagonalization by Similarity Transformations
523
7.2.19. Let N =
ï£«
ï£¬
ï£¬
ï£­
0
1
...
...
...
1
0
ï£¶
ï£·
ï£·
ï£¸
nÃ—n
.
(a)
Show that Î» âˆˆÏƒ

N + NT 
if and only if iÎ» âˆˆÏƒ

N âˆ’NT 
.
(b)
Explain why N + NT is nonsingular if and only if n is even.
(c)
Evaluate det

N âˆ’NT 
/det

N + NT 
when n is even.
7.2.20. A Toeplitz matrix having the form
C =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
c0
cnâˆ’1
cnâˆ’2
Â· Â· Â·
c1
c1
c0
cnâˆ’1
Â· Â· Â·
c2
c2
c1
c0
Â· Â· Â·
c3
...
...
...
...
...
cnâˆ’1
cnâˆ’2
cnâˆ’3
Â· Â· Â·
c0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
nÃ—n
is called a circulant matrix. If p(x) = c0 + c1x + Â· Â· Â· + cnâˆ’1xnâˆ’1,
and if {1, Î¾, Î¾2, . . . , Î¾nâˆ’1} are the nth roots of unity, then the results
of Exercise 5.8.12 (p. 379) insure that
FnCFâˆ’1
n
=
ï£«
ï£¬
ï£¬
ï£­
p(1)
0
Â· Â· Â·
0
0
p(Î¾)
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
p(Î¾nâˆ’1)
ï£¶
ï£·
ï£·
ï£¸
in which Fn is the Fourier matrix of order n. Verify these facts for the
circulant below by computing its eigenvalues and eigenvectors directly:
C =
ï£«
ï£¬
ï£­
1
0
1
0
0
1
0
1
1
0
1
0
0
1
0
1
ï£¶
ï£·
ï£¸.
7.2.21. Suppose that (Î», x) and (Âµ, yâˆ—) are right-hand and left-hand eigen-
pairs for A âˆˆâ„œnÃ—n â€”i.e., Ax = Î»x and yâˆ—A = Âµyâˆ—. Explain why
yâˆ—x = 0 whenever Î» Ì¸= Âµ.
7.2.22. Consider A âˆˆâ„œnÃ—n.
(a)
Show that if A is diagonalizable, then there are right-hand and
left-hand eigenvectors x and yâˆ—associated with Î» âˆˆÏƒ (A)
such that yâˆ—x Ì¸= 0 so that we can make yâˆ—x = 1.
(b)
Show that not every right-hand and left-hand eigenvector x and
yâˆ—associated with Î» âˆˆÏƒ (A) must satisfy yâˆ—x Ì¸= 0.
(c)
Show that (a) need not be true when A is not diagonalizable.

524
Chapter 7
Eigenvalues and Eigenvectors
7.2.23. Consider A âˆˆâ„œnÃ—n with Î» âˆˆÏƒ (A) .
(a)
Prove that if Î» is simple, then yâˆ—x Ì¸= 0 for every pair of respec-
tive right-hand and left-hand eigenvectors x and yâˆ—associated
with Î» regardless of whether or not A is diagonalizable. Hint:
Use the core-nilpotent decomposition on p. 397.
(b)
Show that yâˆ—x = 0 is possible when Î» is not simple.
7.2.24. For A âˆˆâ„œnÃ—n with Ïƒ (A) = {Î»1, Î»2, . . . , Î»k} , show A is diagonaliz-
able if and only if â„œn = N (A âˆ’Î»1I)âŠ•N (A âˆ’Î»2I)âŠ•Â· Â· Â·âŠ•N (A âˆ’Î»kI).
Hint: Recall Exercise 5.9.14.
7.2.25. The Real Schur Form. Schurâ€™s triangularization theorem (p. 508)
insures that every square matrix A is unitarily similar to an upper-
triangular matrixâ€”say, Uâˆ—AU = T. But even when A is real, U
and T may have to be complex if A has some complex eigenvalues.
However, the matrices (and the arithmetic) can be constrained to be real
by settling for a block-triangular result with 2 Ã— 2 or scalar entries on
the diagonal. Prove that for each A âˆˆâ„œnÃ—n there exists an orthogonal
matrix P âˆˆâ„œnÃ—n and real matrices Bij such that
PT AP =
ï£«
ï£¬
ï£­
B11
B12
Â· Â· Â·
B1k
0
B22
Â· Â· Â·
B2k
...
...
...
...
0
0
Â· Â· Â·
Bkk
ï£¶
ï£·
ï£¸,
where Bjj is 1 Ã— 1 or 2 Ã— 2.
If Bjj = [Î»j] is 1 Ã— 1, then Î»j âˆˆÏƒ (A) , and if Bjj is 2 Ã— 2, then
Ïƒ (Bjj) = {Î»j, Î»j} âŠ†Ïƒ (A) .
7.2.26. When A âˆˆâ„œnÃ—n is diagonalizable by a similarity transformation S,
then S may have to be complex if A has some complex eigenvalues.
Analogous to Exercise 7.2.25, we can stay in the realm of real numbers
by settling for a block-diagonal result with 1 Ã— 1 or 2 Ã— 2 entries on the
diagonal. Prove that if A âˆˆâ„œnÃ—n is diagonalizable with real eigenvalues
{Ï1, . . . , Ïr} and complex eigenvalues {Î»1, Î»1, Î»2, Î»2, . . . , Î»t, Î»t} with
2t+r = n, then there exists a nonsingular P âˆˆâ„œnÃ—n and Bj â€™s âˆˆâ„œ2Ã—2
such that
Pâˆ’1AP =
ï£«
ï£¬
ï£­
D
0
Â· Â· Â·
0
0
B1
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
Bt
ï£¶
ï£·
ï£¸,
where
D =
ï£«
ï£¬
ï£­
Ï1
0
Â· Â· Â·
0
0
Ï2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
Ïr
ï£¶
ï£·
ï£¸,
and where Bj has eigenvalues Î»j and Î»j.
7.2.27. For A âˆˆCnÃ—n, prove that xâˆ—Ax = 0 for all x âˆˆCnÃ—1 â‡’A = 0. Show
that xT Ax = 0 for all x âˆˆâ„œnÃ—1 Ì¸â‡’A = 0, even if A is real.

7.3 Functions of Diagonalizable Matrices
525
7.3
FUNCTIONS OF DIAGONALIZABLE MATRICES
For square matrices A, what should it mean to write sin A, eA, ln A, etc.?
A naive approach might be to simply apply the given function to each entry of
A such as
sin

a11
a12
a21
a22

?=

sin a11
sin a12
sin a21
sin a22

.
(7.3.1)
But doing so results in matrix functions that fail to have the same properties as
their scalar counterparts. For example, since sin2 x + cos2 x = 1 for all scalars
x, we would like our deï¬nitions of sin A and cos A to result in the analogous
matrix identity sin2 A + cos2 A = I for all square matrices A. The entrywise
approach (7.3.1) clearly fails in this regard.
One way to deï¬ne matrix functions possessing properties consistent with
their scalar counterparts is to use inï¬nite series expansions. For example, consider
the exponential function
ez =
âˆ

k=0
zk
k! = 1 + z + z2
2! + z3
3! Â· Â· Â· .
(7.3.2)
Formally replacing the scalar argument z by a square matrix A ( z0 = 1 is
replaced with A0 = I ) results in the inï¬nite series of matrices
eA = I + A + A2
2! + A3
3! Â· Â· Â· ,
(7.3.3)
called the matrix exponential. While this results in a matrix that has properties
analogous to its scalar counterpart, it suï¬€ers from the fact that convergence must
be dealt with, and then there is the problem of describing the entries in the limit.
These issues are handled by deriving a closed form expression for (7.3.3).
If A is diagonalizable, then A = PDPâˆ’1 = P diag (Î»1, . . . , Î»n) Pâˆ’1, and
Ak = PDkPâˆ’1 = P diag

Î»k
1, . . . , Î»k
n

Pâˆ’1, so
eA =
âˆ

k=0
Ak
k! =
âˆ

k=0
PDkPâˆ’1
k!
= P
 âˆ

k=0
Dk
k!
!
Pâˆ’1 = P diag

eÎ»1, . . . , eÎ»n
Pâˆ’1.
In other words, we donâ€™t have to use the inï¬nite series (7.3.3) to deï¬ne eA.
Instead, deï¬ne eD = diag (eÎ»1, eÎ»2, . . . , eÎ»n), and set
eA = PeDPâˆ’1 = P diag (eÎ»1, eÎ»2, . . . , eÎ»n) Pâˆ’1.
This idea can be generalized to any function f(z) that is deï¬ned on the
eigenvalues Î»i of a diagonalizable matrix A = PDPâˆ’1 by deï¬ning f(D) to
be f(D) = diag (f(Î»1), f(Î»2), . . . , f(Î»n)) and by setting
f(A) = Pf(D)Pâˆ’1 = P diag (f(Î»1), f(Î»2), . . . , f(Î»n)) Pâˆ’1.
(7.3.4)

526
Chapter 7
Eigenvalues and Eigenvectors
At ï¬rst glance this deï¬nition seems to have an edge over the inï¬nite series ap-
proach because there are no convergence issues to deal with. But convergence
worries have been traded for uniqueness worries. Because P is not unique, itâ€™s
not apparent that (7.3.4) is well deï¬ned. The eigenvector matrix P you compute
for a given A need not be the same as the eigenvector matrix I compute, so what
insures that your f(A) will be the same as mine? The spectral theorem (p. 517)
does. Suppose there are k distinct eigenvalues that are grouped according to
repetition, and expand (7.3.4) just as (7.2.11) is expanded to produce
f(A) = PDPâˆ’1 =

X1|X2| Â· Â· Â· |Xk

ï£«
ï£¬
ï£¬
ï£­
f(Î»1)I
0
Â· Â· Â·
0
0
f(Î»2)I
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
f(Î»k)I
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
YT
1
YT
2
...
YT
k
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
k

i=1
f(Î»i)XiYT
i =
k

i=1
f(Î»i)Gi.
Since Gi is the projector onto N (A âˆ’Î»iI) along R (A âˆ’Î»iI), Gi is uniquely
determined by A. Therefore, (7.3.4) uniquely deï¬nes f(A) regardless of the
choice of P. We can now make a formal deï¬nition.
Functions of Diagonalizable Matrices
Let A = PDPâˆ’1 be a diagonalizable matrix where the eigenvalues in
D = diag (Î»1I, Î»2I, . . . , Î»kI) are grouped by repetition. For a function
f(z) that is deï¬ned at each Î»i âˆˆÏƒ (A) , deï¬ne
f(A) = Pf(D)Pâˆ’1 = P
ï£«
ï£¬
ï£¬
ï£­
f(Î»1)I
0
Â· Â· Â·
0
0
f(Î»2)I
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
f(Î»k)I
ï£¶
ï£·
ï£·
ï£¸Pâˆ’1 (7.3.5)
= f(Î»1)G1 + f(Î»2)G2 + Â· Â· Â· + f(Î»k)Gk,
(7.3.6)
where Gi is the ith spectral projector as described on pp. 517, 529.
The generalization to nondiagonalizable matrices is on p. 603.
The discussion of matrix functions was initiated by considering inï¬nite se-
ries, so, to complete the circle, a formal statement connecting inï¬nite series with
(7.3.5) and (7.3.6) is needed. By replacing A by PDPâˆ’1 in âˆ
n=0 cn(Aâˆ’z0I)n
and expanding the result, the following result is established.

7.3 Functions of Diagonalizable Matrices
527
Inï¬nite Series
If f(z) = âˆ
n=0 cn(z âˆ’z0)n converges when |z âˆ’z0| < r, and if
|Î»i âˆ’z0| < r for each eigenvalue Î»i of a diagonalizable matrix A, then
f(A) =
âˆ

n=0
cn(A âˆ’z0I)n.
(7.3.7)
It can be argued that the matrix series on the right-hand side of (7.3.7)
converges if and only if |Î»iâˆ’z0| < r for each Î»i, regardless of whether or
not A is diagonalizable. So (7.3.7) serves to deï¬ne f(A) for functions
with series expansions regardless of whether or not A is diagonalizable.
More is said in Example 7.9.3 (p. 605).
Example 7.3.1
Neumann Series Revisited. The function f(z) = (1âˆ’z)âˆ’1 has the geometric
series expansion (1âˆ’z)âˆ’1 = âˆ
k=1 zk that converges if and only if |z| < 1. This
means that the associated matrix function f(A) = (I âˆ’A)âˆ’1 is given by
(I âˆ’A)âˆ’1 =
âˆ

k=0
Ak
if and only if |Î»| < 1 for all Î» âˆˆÏƒ (A) .
(7.3.8)
This is the Neumann series discussed on p. 126, where it was argued that
if limnâ†’âˆAn = 0, then (I âˆ’A)âˆ’1 = âˆ
k=0 Ak. The two approaches are the
same because it turns out that limnâ†’âˆAn = 0 â‡â‡’|Î»| < 1 for all Î» âˆˆÏƒ (A) .
This is immediate for diagonalizable matrices, but the nondiagonalizable case is
a bit more involvedâ€”the complete statement is developed on p. 618. Because
maxi |Î»i| â‰¤âˆ¥Aâˆ¥for all matrix norms (Example 7.1.4, p. 497), a corollary of
(7.3.8) is that (I âˆ’A)âˆ’1 exists and
(I âˆ’A)âˆ’1 =
âˆ

k=0
Ak
when âˆ¥Aâˆ¥< 1 for any matrix norm.
(7.3.9)
Caution! (I âˆ’A)âˆ’1 can exist without the Neumann series expansion being
valid because all thatâ€™s needed for I âˆ’A to be nonsingular is 1 /âˆˆÏƒ (A) , while
convergence of the Neumann series requires each |Î»| < 1.

528
Chapter 7
Eigenvalues and Eigenvectors
Example 7.3.2
Eigenvalue Perturbations. Itâ€™s often important to understand how the eigen-
values of a matrix are aï¬€ected by perturbations. In general, this is a complicated
issue, but for diagonalizable matrices the problem is more tractable.
Problem: Suppose B = A+E, where A is diagonalizable, and let Î² âˆˆÏƒ (B) .
If Pâˆ’1AP = D = diag (Î»1, Î»2, . . . , Î»n) , explain why
min
Î»iâˆˆÏƒ(A) |Î² âˆ’Î»i| â‰¤Îº(P) âˆ¥Eâˆ¥,
where
Îº(P) = âˆ¥Pâˆ¥âˆ¥Pâˆ’1âˆ¥
(7.3.10)
for matrix norms satisfying âˆ¥Dâˆ¥= maxi |Î»i| (e.g., any standard induced norm).
Solution: Assume Î² Ì¸âˆˆÏƒ (A) â€”(7.3.10) is trivial if Î² âˆˆÏƒ (A) â€”and observe
that
(Î²I âˆ’A)âˆ’1(Î²I âˆ’B) = (Î²I âˆ’A)âˆ’1(Î²I âˆ’A âˆ’E) = I âˆ’(Î²I âˆ’A)âˆ’1E
implies that 1 â‰¤âˆ¥(Î²Iâˆ’A)âˆ’1Eâˆ¥â€”otherwise Iâˆ’(Î²Iâˆ’A)âˆ’1E is nonsingular by
(7.3.9), which is impossible because (Î²I âˆ’B) (and hence (Î²I âˆ’A)âˆ’1(Î²I âˆ’B)
is singular). Consequently,
1 â‰¤âˆ¥(Î²I âˆ’A)âˆ’1Eâˆ¥= âˆ¥P(Î²I âˆ’D)âˆ’1Pâˆ’1Eâˆ¥â‰¤âˆ¥Pâˆ¥âˆ¥(Î²I âˆ’D)âˆ’1âˆ¥âˆ¥Pâˆ’1âˆ¥âˆ¥Eâˆ¥
= Îº(P) âˆ¥Eâˆ¥max
i
|Î² âˆ’Î»i|âˆ’1 = Îº(P) âˆ¥Eâˆ¥
1
mini |Î² âˆ’Î»i|,
and this produces (7.3.10). Similar to the case of linear systems (Example 5.12.1,
p. 414), the expression Îº(P) is a condition number in the sense that if Îº(P) is
relatively small, then the Î»i â€™s are relatively insensitive, but if Îº(P) is relatively
large, we must be suspicious. Note: Because itâ€™s a corollary of their 1960 results,
the bound (7.3.10) is often referred to as the Bauerâ€“Fike bound.
Inï¬nite series representations can always be avoided because every func-
tion of AnÃ—n can be expressed as a polynomial in A. In other words, when
f(A) exists, there is a polynomial p(z) such that p(A) = f(A). This is
true for all matrices, but the development here is limited to diagonalizable
matricesâ€”nondiagonalizable matrices are treated in Exercise 7.3.7. In the di-
agonalizable case, f(A) exists if and only if f(Î»i) exists for each Î»i âˆˆÏƒ (A) =
{Î»1, Î»2, . . . , Î»k} , and, by (7.3.6), f(A) = k
i=1 f(Î»i)Gi, where Gi is the ith
spectral projector. Any polynomial p(z) agreeing with f(z) on Ïƒ (A) does the
job because if p(Î»i) = f(Î»i) for each Î»i âˆˆÏƒ (A) , then
p(A) =
k

i=1
p(Î»i)Gi =
k

i=1
f(Î»i)Gi = f(A).

7.3 Functions of Diagonalizable Matrices
529
But is there always a polynomial satisfying p(Î»i) = f(Î»i) for each Î»i âˆˆÏƒ (A)?
Sureâ€”thatâ€™s what the Lagrange interpolating polynomial from Example 4.3.5
(p. 186) does. Itâ€™s given by
p(z)=
k

i=1
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
f(Î»i)
k"
j=1
jÌ¸=i
(z âˆ’Î»j)
k"
j=1
jÌ¸=i
(Î»i âˆ’Î»j)
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
, so f(A)=p(A)=
k

i=1
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
f(Î»i)
k"
j=1
jÌ¸=i
(A âˆ’Î»jI)
k"
j=1
jÌ¸=i
(Î»i âˆ’Î»j)
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Using the function gi(z) =
	
1
if z = Î»i,
0
if z Ì¸= Î»i, with this representation as well as
that in (7.3.6) yields
k"
j=1
jÌ¸=i
(A âˆ’Î»jI)
# k"
j=1
jÌ¸=i
(Î»i âˆ’Î»j) = gi(A) = Gi. For example,
if Ïƒ (AnÃ—n) = {Î»1, Î»2, Î»3}, then f(A) = f(Î»1)G1 + f(Î»2)G2 + f(Î»3)G3 with
G1 = (Aâˆ’Î»2I)(Aâˆ’Î»3I)
(Î»1âˆ’Î»2)(Î»1âˆ’Î»3) ,
G2 = (Aâˆ’Î»1I)(Aâˆ’Î»3I)
(Î»2âˆ’Î»1)(Î»2âˆ’Î»3) ,
G3 = (Aâˆ’Î»1I)(Aâˆ’Î»2I)
(Î»3âˆ’Î»1)(Î»3âˆ’Î»2) .
Below is a summary of these observations.
Spectral Projectors
If A is diagonalizable with Ïƒ (A) = {Î»1, Î»2, . . . , Î»k} , then the spectral
projector onto N (A âˆ’Î»iI) along R (A âˆ’Î»iI) is given by
Gi =
k
$
j=1
jÌ¸=i
(A âˆ’Î»jI)
# k
$
j=1
jÌ¸=i
(Î»i âˆ’Î»j)
for i = 1, 2, . . . , k.
(7.3.11)
Consequently, if f(z) is deï¬ned on Ïƒ (A) , then f(A) = k
i=1 f(Î»i)Gi
is a polynomial in A of degree at most k âˆ’1.
Example 7.3.3
Problem: For a scalar t, determine the matrix exponential eAt, where
A =

âˆ’Î±
Î²
Î±
âˆ’Î²

with Î± + Î² Ì¸= 0.
Solution 1: The characteristic equation for A is Î»2 + (Î± + Î²)Î» = 0, so the
eigenvalues of A are Î»1 = 0 and Î»2 = âˆ’(Î±+Î²). Note that A is diagonalizable

530
Chapter 7
Eigenvalues and Eigenvectors
because no eigenvalue is repeatedâ€”recall (7.2.6). Using the function f(z) = ezt,
the spectral representation (7.3.6) says that
eAt = f(A) = f(Î»1)G1 + f(Î»2)G2 = eÎ»1tG1 + eÎ»2tG2.
The spectral projectors G1 and G2 are determined from (7.3.11) to be
G1 = A âˆ’Î»2I
âˆ’Î»2
=
1
Î± + Î²

Î²
Î²
Î±
Î±

and
G2 = A
Î»2
=
1
Î± + Î²

Î±
âˆ’Î²
âˆ’Î±
Î²

,
so
eAt = G1 + eâˆ’(Î±+Î²)tG2 =
1
Î± + Î²
%
Î²
Î²
Î±
Î±

+ eâˆ’(Î±+Î²)t

Î±
âˆ’Î²
âˆ’Î±
Î²
&
.
Solution 2: Compute eigenpairs (Î»1, x1) and (Î»2, x2), construct P =

x1 | x2

,
and compute
eAt = P

f(Î»1)
0
0
f(Î»2)

Pâˆ’1 = P

eÎ»1t
0
0
eÎ»2t

Pâˆ’1.
The computational details are called for in Exercise 7.3.2.
Example 7.3.4
Problem: For T =
 1/2
1/2
1/4
3/4

, evaluate limkâ†’âˆTk.
Solution 1: Compute two eigenpairs, Î»1 = 1, x1 = (1, 1)T , and Î»2 = 1/4,
x2 = (âˆ’2, 1)T . If P = [x1 | x2], then T = P
 1
0
0
1/4

Pâˆ’1, so
Tk = P

1k
0
0
1/4k

Pâˆ’1 â†’P

1
0
0
0

Pâˆ’1 = 1
3

1
2
1
2

.
(7.3.12)
Solution 2: We know from (7.3.6) that Tk = 1kG1 + (1/4)kG2 â†’G1. Since
Î»1 = 1 is a simple eigenvalue, formula (7.2.12) on p. 518 can be used to compute
G1 = x1yT
1 /yT
1 x1, where x1 and yT
1 are any right- and left-hand eigenvectors
associated with Î»1 = 1. A right-hand eigenvector x1 was computed above.
Computing a left-hand eigenvector yT
1 = (1, 2) yields
Tk â†’G1 = x1yT
1
yT
1 x1
= 1
3

1
2
1
2

.
(7.3.13)

7.3 Functions of Diagonalizable Matrices
531
Example 7.3.5
Population Migration. Suppose that the population migration between two
geographical regionsâ€”say, the North and the Southâ€”is as follows. Each year,
50% of the population in the North migrates to the South, while only 25% of
the population in the South moves to the North. This situation is depicted by
drawing a transition diagram as shown below in Figure 7.3.1.
N
S
.25
.5
.5
.75
Figure 7.3.1
Problem: If this migration pattern continues, will the population in the North
continually shrink until the entire population is eventually in the South, or will
the population distribution somehow stabilize before the North is completely
deserted?
Solution: Let nk and sk denote the respective proportions of the total popula-
tion living in the North and South at the end of year k, and assume nk+sk = 1.
The migration pattern dictates that the fractions of the population in each region
at the end of year k + 1 are
'
nk+1 = nk(.5) + sk(.25)
sk+1 = nk(.5) + sk(.75)
(
or, equivalently,
pT
k+1 = pT
k T,
(7.3.14)
where pT
k = ( nk
sk ) and pT
k+1 = ( nk+1
sk+1 ) are the respective population
distributions at the end of years k and k + 1, and where
T =
 N
S
N
.5
.5
S
.25
.75

is the associated transition matrix (recall Example 3.6.3). Inducting on
pT
1 = pT
0 T,
pT
2 = pT
1 T = pT
0 T2,
pT
3 = pT
2 T = pT
0 T3,
Â· Â· Â·
leads to pT
k = pT
0 Tk, which indicates that the powers of T determine how the
process evolves. Determining the long-run population distribution
73 is therefore
73
The long-run distribution goes by a lot of diï¬€erent names. Itâ€™s also called the limiting distri-
bution, the steady-state distribution, and the stationary distribution.

532
Chapter 7
Eigenvalues and Eigenvectors
accomplished by analyzing limkâ†’âˆTk. The results of Example 7.3.4 together
with n0 + s0 = 1 yield the long-run (or limiting) population distribution as
pT
âˆ= lim
kâ†’âˆpT
k = lim
kâ†’âˆpT
0 Tk = pT
0 lim
kâ†’âˆTk = ( n0
s0 )

1/3
2/3
1/3
2/3

=
 n0 + s0
3
2(n0 + s0)
3

=
 1
3
2
3

.
So if the migration pattern continues to hold, then the population distribution
will eventually stabilize with 1/3 of the population being in the North and 2/3 of
the population in the South. And this is independent of the initial distribution!
Observations: This is an example of a broader class of evolutionary processes
known as Markov chains (p. 687), and the following observations are typical.
â€¢
Itâ€™s clear from (7.3.12) or (7.3.13) that the rate at which the population
distribution stabilizes is governed by how fast (1/4)k â†’0. In other words,
the magnitude of the largest subdominant eigenvalue of T determines the
rate of evolution.
â€¢
For the dominant eigenvalue Î»1 = 1, the column, x1, of 1â€™s is a right-
hand eigenvector (because T has unit row sums). This forces the limiting
distribution pT
âˆto be a particular left-hand eigenvector associated with
Î»1 = 1 because for an arbitrary left-hand eigenvector yT
1
associated with
Î»1 = 1, equation (7.3.13) in Example 7.3.4 insures that
pT
âˆ= lim
kâ†’âˆpT
0 Tk = pT
0 lim
kâ†’âˆTk = pT
0 G1 = (pT
0 x1)yT
1
yT
1 x1
=
yT
1
yT
1 x1
.
(7.3.15)
The fact that pT
0 Tk converges to an eigenvector is a special case of the
power method discussed in Example 7.3.7.
â€¢
Equation (7.3.15) shows why the initial distribution pT
0 always drops away
in the limit. But pT
0
is not completely irrelevant because it always aï¬€ects
the transient behaviorâ€”i.e., the behavior of pT
k = pT
0 Tk for smaller k â€™s.
Example 7.3.6
Cayleyâ€“Hamilton Revisited. The Cayleyâ€“Hamilton theorem (p. 509) says
that if p(Î») = 0 is the characteristic equation for A, then p(A) = 0. This is
evident for diagonalizable A because p(Î»i) = 0 for each Î»i âˆˆÏƒ (A) , so, by
(7.3.6), p(A) = p(Î»1)G1 + p(Î»2)G2 + Â· Â· Â· + p(Î»k)Gk = 0.
Problem: Establish the Cayleyâ€“Hamilton theorem for nondiagonalizable matri-
ces by using the diagonalizable result together with a continuity argument.
Solution: Schurâ€™s triangularization theorem (p. 508) insures AnÃ—n = UTUâˆ—
for a unitary U and an upper triangular T having the eigenvalues of A on the

7.3 Functions of Diagonalizable Matrices
533
diagonal. For each Ïµ Ì¸= 0, itâ€™s possible to ï¬nd numbers Ïµi such that (Î»1 + Ïµ1),
(Î»2 + Ïµ2), . . . , (Î»n + Ïµn) are distinct and  Ïµ2
i = |Ïµ|. Set
D(Ïµ) = diag (Ïµ1, Ïµ2, . . . , Ïµn)
and
B(Ïµ) = U

T + D(Ïµ)

Uâˆ—= A + E(Ïµ),
where E(Ïµ) = UD(Ïµ)Uâˆ—. The (Î»i + Ïµi) â€™s are the eigenvalues of B(Ïµ) and
they are distinct, so B(Ïµ) is diagonalizableâ€”by (7.2.6). Consequently, B(Ïµ)
satisï¬es its own characteristic equation 0 = pÏµ(Î») = det (A + E(Ïµ) âˆ’Î»I) for
each Ïµ Ì¸= 0. The coeï¬ƒcients of pÏµ(Î») are continuous functions of the entries in
E(Ïµ) (recall (7.1.6)) and hence are continuous functions of the Ïµi â€™s. Combine
this with limÏµâ†’0 E(Ïµ) = 0 to obtain 0 = limÏµâ†’0 pÏµ(B(Ïµ)) = p(A).
Note:
Embedded in the above development is the fact that every square com-
plex matrix is arbitrarily close to some diagonalizable matrix because for each
Ïµ Ì¸= 0, we have âˆ¥A âˆ’B(Ïµ)âˆ¥F = âˆ¥E(Ïµ)âˆ¥F = Ïµ (recall Exercise 5.6.9).
Example 7.3.7
Power method
74 is an iterative technique for computing a dominant eigenpair
(Î»1, x) of a diagonalizable A âˆˆâ„œmÃ—m with eigenvalues
|Î»1| > |Î»2| â‰¥|Î»3| â‰¥Â· Â· Â· â‰¥|Î»k|.
Note that this implies Î»1 is realâ€”otherwise Î»1 is another eigenvalue with the
same magnitude as Î»1. Consider f(z) = (z/Î»1)n, and use the spectral repre-
sentation (7.3.6) along with |Î»i/Î»1| < 1 for i = 2, 3, . . . , k to conclude that
 A
Î»1
n
= f(A) = f(Î»1)G1 + f(Î»2)G2 + Â· Â· Â· + f(Î»k)Gk
= G1 +
Î»2
Î»1
n
G2 + Â· Â· Â· +
Î»k
Î»1
n
Gk â†’G1
(7.3.16)
as n â†’âˆ. Consequently, (Anx0/Î»n
1) â†’G1x0 âˆˆN (A âˆ’Î»1I) for all x0. So if
G1x0 Ì¸= 0 or, equivalently, x0 /âˆˆR (A âˆ’Î»1I), then Anx0/Î»n
1 converges to an
eigenvector associated with Î»1. This means that the direction of Anx0 tends
toward the direction of an eigenvector because Î»n
1 acts only as a scaling factor
to keep the length of Anx0 under control. Rather than using Î»n
1, we can scale
Anx0 with something more convenient. For example, âˆ¥Anx0âˆ¥(for any vector
norm) is a reasonable scaling factor, but there are even better choices. For vectors
v, let m(v) denote the component of maximal magnitude, and if there is more
74
While the development of the power method was considered to be a great achievement when
R. von Mises introduced it in 1929, later algorithms relegated its computational role to that of
a special purpose technique. Nevertheless, itâ€™s still an important idea because, in some way or
another, most practical algorithms for eigencomputations implicitly rely on the mathematical
essence of the power method.

534
Chapter 7
Eigenvalues and Eigenvectors
than one maximal component, let m(v) be the ï¬rst maximal componentâ€”e.g.,
m(1, 3, âˆ’2) = 3, and m(âˆ’3, 3, âˆ’2) = âˆ’3. Itâ€™s clear that m(Î±v) = Î±m(v) for
all scalars Î±. Suppose m(Anx0/Î»n
1) â†’Î³. Since (An/Î»n
1) â†’G1, we see that
lim
nâ†’âˆ
Anx0
m(Anx0) = lim
nâ†’âˆ
(An/Î»n
1)x0
m(Anx0/Î»n
1) = G1x0
Î³
= x
is an eigenvector associated with Î»1. But rather than successively powering
A, the sequence Anx0/m(Anx0) is more eï¬ƒciently generated by starting with
x0 /âˆˆR (A âˆ’Î»1I) and setting
yn = Axn,
Î½n = m(yn),
xn+1 = yn
Î½n
,
for n = 0, 1, 2, . . . .
(7.3.17)
Not only does xn â†’x, but as a bonus we get Î½n â†’Î»1 because for all n,
Axn+1 = A2xn/Î½n, so if Î½n â†’Î½ as n â†’âˆ, the limit on the left-hand side
is Ax = Î»1x, while the limit on the right-hand side is A2x/Î½ = Î»2
1x/Î½. Since
these two limits must agree, Î»1x = (Î»2
1/Î½)x, and this implies Î½ = Î»1.
Summary. The sequence (Î½n, xn) deï¬ned by (7.3.17) converges to an eigenpair
(Î»1, x) for A provided that G1x0 Ì¸= 0 or, equivalently, x0 /âˆˆR (A âˆ’Î»1I).
â–·
Advantages. Each iteration requires only one matrixâ€“vector product, and
this can be exploited to reduce the computational eï¬€ort when A is large
and sparseâ€”assuming that a dominant eigenpair is the only one of interest.
â–·
Disadvantages. Only a dominant eigenpair is determinedâ€”something else
must be done if others are desired. Furthermore, itâ€™s clear from (7.3.16) that
the rate at which (7.3.17) converges depends on how fast (Î»2/Î»1)n â†’0, so
convergence is slow when |Î»1| is close to |Î»2|.
Example 7.3.8
Inverse Power Method. Given a real approximation Î± /âˆˆÏƒ(A) to any real
Î» âˆˆÏƒ(A), this algorithm (also called the inverse iteration) determines an
eigenpair (Î», x) for a diagonalizable matrix A âˆˆâ„œmÃ—m by applying the power
method
75 to B = (A âˆ’Î±I)âˆ’1. Recall from Exercise 7.1.9 that
x is an eigenvector for A â‡â‡’x is an eigenvector for B,
Î» âˆˆÏƒ(A) â‡â‡’(Î» âˆ’Î±)âˆ’1 âˆˆÏƒ(B).
(7.3.18)
If |Î» âˆ’Î±| < |Î»i âˆ’Î±| for all other Î»i âˆˆÏƒ(A), then (Î» âˆ’Î±)âˆ’1 is the dominant
eigenvalue of B because |Î» âˆ’Î±|âˆ’1 > |Î»i âˆ’Î±|âˆ’1. Therefore, applying the power
75
The relation between the power method and inverse iteration is clear to us now, but it originally
took 15 years to make the connection. Inverse iteration was not introduced until 1944 by the
German mathematician Helmut Wielandt (1910â€“2001).

7.3 Functions of Diagonalizable Matrices
535
method to B produces an eigenpair

(Î» âˆ’Î±)âˆ’1, x

for B from which the
eigenpair (Î», x) for A is determined. That is, if x0 /âˆˆR (B âˆ’Î»I), and if
yn = Bxn = (A âˆ’Î±I)âˆ’1xn,
Î½n = m(yn),
xn+1 = yn
Î½n
for n = 0, 1, 2, . . . ,
then (Î½n, xn) â†’

(Î» âˆ’Î±)âˆ’1, x

, an eigenpair for B, so (7.3.18) guarantees that
(Î½âˆ’1
n +Î±, xn) â†’(Î», x), an eigenpair for A. Rather than using matrix inversion
to compute yn = (A âˆ’Î±I)âˆ’1xn, itâ€™s more eï¬ƒcient to solve the linear system
(A âˆ’Î±I)yn = xn for yn. Because this is a system in which the coeï¬ƒcient
matrix remains the same from step to step, the eï¬ƒciency is further enhanced by
computing an LU factorization of (A âˆ’Î±I) at the outset so that at each step
only one forward solve and one back solve (as described on pp. 146 and 153) are
needed to determine yn.
â–·
Advantages. Striking results are often obtained (particularly in the case of
symmetric matrices) with only one or two iterations, even when x0 is nearly
in R (B âˆ’Î»I) = R (A âˆ’Î»I). For Î± close to Î», computing an accurate
ï¬‚oating-point solution of (A âˆ’Î±I)yn = xn is diï¬ƒcult because A âˆ’Î±I is
nearly singular, and this almost surely guarantees that (Aâˆ’Î±I)yn = xn is an
ill-conditioned system. But only the direction of the solution is important,
and the direction of a computed solution is usually reasonable in spite of
conditioning problems. Finally, the algorithm can be adapted to compute
approximations of eigenvectors associated with complex eigenvalues.
â–·
Disadvantages. Only one eigenpair at a time is computed, and an approxi-
mate eigenvalue must be known in advance. Furthermore, the rate of conver-
gence depends on how fast [(Î» âˆ’Î±)/(Î»i âˆ’Î±)]n â†’0, and this can be slow
when there is another eigenvalue Î»i close to the desired Î». If Î»i is too
close to Î», roundoï¬€error can divert inverse iteration toward an eigenvector
associated with Î»i instead of Î» in spite of a theoretically correct Î±.
Note: In the standard version of inverse iteration a constant value of Î± is used at
each step to approximate an eigenvalue Î», but there is variation called Rayleigh
quotient iteration that uses the current iterate xn to improve the value of Î±
at each step by setting Î± = xT
nAxn/xT
nxn. The function R(x) = xT Ax/xT x is
called the Rayleigh quotient. It can be shown that if x is a good approximation to
an eigenvector, then R(x) is a good approximation of the associated eigenvalue.
More is said about this in Example 7.5.1 (p. 549).
Example 7.3.9
The QR Iteration algorithm for computing the eigenvalues of a general ma-
trix came from an elegantly simple idea that was proposed by Heinz Rutishauser
in 1958 and reï¬ned by J. F. G. Francis in 1961-1962. The underlying concept is
to alternate between computing QR factors (Rutishauser used LU factors) and

536
Chapter 7
Eigenvalues and Eigenvectors
reversing their order as shown below. Starting with A1 = A âˆˆâ„œnÃ—n,
Factor:
A1 = Q1R1,
Set:
A2 = R1Q1,
Factor:
A2 = Q2R2,
Set:
A3 = R2Q2,
...
In general, Ak+1 = RkQk, where Qk and Rk are the QR factors of Ak.
Notice that if Pk = Q1Q2 Â· Â· Â· Qk, then each Pk is an orthogonal matrix such
that
PT
1 AP1 = QT
1 Q1R1Q1 = A2,
PT
2 AP2 = QT
2 QT
1 AQ1Q2 = QT
2 A2Q2 = A3,
...
PT
k APk = Ak+1.
In other words, A2, A3, A4, . . . are each orthogonally similar to A, and hence
Ïƒ (Ak) = Ïƒ (A) for each k. But the process does more than just create a matrix
that is similar to A at each step. The magic lies in the fact that if the process
converges, then limkâ†’âˆAk = R is an upper-triangular matrix in which the
diagonal entries are the eigenvalues of A. Indeed, if Pk â†’P, then
Qk = PT
kâˆ’1Pk â†’PT P = I
and
Rk = Ak+1QT
k â†’R I = R,
so
lim
kâ†’âˆAk = lim
kâ†’âˆQkRk = R,
which is necessarily upper triangular having diagonal entries equal to the eigen-
values of A. However, as is often the case, there is a big gap between theory
and practice, and turning this clever idea into a practical algorithm requires sig-
niï¬cant eï¬€ort. For example, one obvious hurdle that needs to be overcome is the
fact that the R factor in a QR factorization has positive diagonal entries, so,
unless modiï¬cations are made, the â€œvanillaâ€ version of the QR iteration canâ€™t
converge for matrices with complex or nonpositive eigenvalues. Laying out all of
the details and analyzing the rigors that constitute the practical implementation
of the QR iteration is tedious and would take us too far astray, but the basic
principals are within our reach.
â€¢
Hessenberg Matrices.
A big step in turning the QR iteration into a prac-
tical method is to realize that everything can be done with upper-Hessenberg
matrices. As discussed in Example 5.7.4 (p. 350), Householder reduction
can be used to produce an orthogonal matrix P such that PT AP = H1,
and Example 5.7.5 (p. 352) shows that Givens reduction easily produces

7.3 Functions of Diagonalizable Matrices
537
the QR factors of any Hessenberg matrix. Givens reduction on H1 pro-
duces the Q factor of H1 as the transposed product of plane rotations
Q1 = PT
12PT
23 Â· Â· Â· PT
(nâˆ’1)n, and this is also upper Hessenberg (constructing a
4 Ã— 4 example will convince you). Since multiplication by an upper-triangular
matrix canâ€™t alter the upper-Hessenberg structure, the matrix R1Q1 = H2
at the second step of the QR iteration is again upper Hessenberg, and so
on for each successive step. Being able to iterate with Hessenberg matrices
results in a signiï¬cant reduction of arithmetic. Note that if A = AT , then
Hk = HT
k for each k, which means that each Hk is tridiagonal in structure.
â€¢
Convergence.
When the Hk â€™s converge, the entries at the bottom of the
ï¬rst subdiagonal tend to die ï¬rstâ€”i.e., a typical pattern might be
Hk =
ï£«
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
0
0
Ïµ
â‹†
ï£¶
ï£·
ï£¸.
When Ïµ is satisfactorily small, take â‹†(the (n, n)-entry) to be an eigenvalue,
and deï¬‚ate the problem. An even nicer state of aï¬€airs is to have a zero (or a
satisfactorily small) entry in row n âˆ’1 and column 2 (illustrated below for
n = 4)
Hk =
ï£«
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
Ïµ
â‹†
â‹†
0
0
â‹†
â‹†
ï£¶
ï£·
ï£¸
(7.3.19)
because the trailing 2 Ã— 2 block
 â‹†
â‹†
â‹†
â‹†

will yield two eigenvalues by the
quadratic formula, and thus complex eigenvalues can be revealed.
â€¢
Shifts.
Instead of factoring Hk at the kth step, factor a shifted matrix
Hk âˆ’Î±kI = QkRk, and set Hk+1 = RkQk + Î±kI, where Î±k is an ap-
proximate real eigenvalueâ€”a good candidate is Î±k = [Hk]nn. Notice that
Ïƒ (Hk+1) = Ïƒ (Hk) because Hk+1 = QT
k HkQk. The inverse power method
is now at work. To see how, drop the subscripts, and write H âˆ’Î±I = QR
as QT = R(H âˆ’Î±I)âˆ’1. If Î± â‰ˆÎ» âˆˆÏƒ (H) = Ïƒ (A) (say, |Î» âˆ’Î±| = Ïµ with
Î±, Î» âˆˆâ„œ), then the discussion concerning the inverse power method in Exam-
ple 7.3.8 insures that the rows in QT are close to being left-hand eigenvectors
of H associated with Î». In particular, if qT
n is the last row in QT , then
rnneT
n = eT
nR = qT
nQR = qT
n(H âˆ’Î±I) = qT
nH âˆ’Î±qT
n â‰ˆ(Î» âˆ’Î±)qT
n,
so rnn = |rnn| â‰ˆ
))(Î» âˆ’Î±)qT
n
))
2 = Ïµ and qT
n â‰ˆÂ±eT
n. The signiï¬cance of this

538
Chapter 7
Eigenvalues and Eigenvectors
is revealed by looking at a generic 4 Ã— 4 pattern for
Hk+1 = RQ + Î±I
â‰ˆ
ï£«
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
0
0
âˆ—
âˆ—
0
0
0
Ïµ
ï£¶
ï£¸
ï£«
ï£­
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
0
0
âˆ—
âˆ—
0
0
0
â‹†
Â±1
ï£¶
ï£¸+
ï£«
ï£­
Î±
Î±
Î±
Î±
ï£¶
ï£¸
=
ï£«
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
0
0
Ïµâ‹†
Î± Â± Ïµ
ï£¶
ï£¸â‰ˆ
ï£«
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
0
0
0
Î± Â± Ïµ
ï£¶
ï£¸.
The strength of the last approximation rests not only on the size of Ïµ, but
it is also reinforced by the fact that â‹†â‰ˆ0 because the 2-norm of the last
row of Q must be 1. This indicates why this technique (called the single
shifted QR iteration) can provide rapid convergence to a real eigenvalue. To
extract complex eigenvalues, a double shift strategy is employed in which the
eigenvalues Î±k and Î²k of the lower 2 Ã— 2 block of Hk are used as shifts
as indicated below:
Factor:
Hk âˆ’Î±kI = QkRk,
Set:
Hk+1 = RkQk + Î±kI
(so Hk+1 = QT
k HkQk),
Factor:
Hk+1 âˆ’Î²kI = Qk+1Rk+1,
Set:
Hk+2 = Rk+1Qk+1 + Î²kI
(so Hk+2 = QT
k+1QT
k HkQkQk+1),
...
The nice thing about the double shift strategy is that even when Î±k is
complex (so that Î²k = Î±k) the matrix QkQk+1 (and hence Hk+2) is
real, and there are eï¬ƒcient ways to form QkQk+1 by computing only the
ï¬rst column of the product. The double shift method typically requires very
few iterations (using only real arithmetic) to produce a small entry in the
(n âˆ’2, 2)-position as depicted in (7.3.19) for a generic 4 Ã— 4 pattern.
Exercises for section 7.3
7.3.1. Determine cos A for A =
 âˆ’Ï€/2
Ï€/2
Ï€/2
âˆ’Ï€/2

.
7.3.2. For the matrix A in Example 7.3.3, verify with direct computation that
eÎ»1tG1 + eÎ»2tG2 = P
 eÎ»1t
0
0
eÎ»2t

Pâˆ’1 = eAt.
7.3.3. Explain why sin2 A + cos2 A = I for a diagonalizable matrix A.

7.3 Functions of Diagonalizable Matrices
539
7.3.4. Explain e0 = I for every square zero matrix.
7.3.5. The spectral mapping property for diagonalizable matrices says that
if f(A) exists, and if {Î»1, Î»2, . . . , Î»n} are the eigenvalues of AnÃ—n
(including multiplicities), then {f(Î»1), . . . , f(Î»n)} are the eigenvalues
of f(A).
(a)
Establish this for diagonalizable matrices.
(b)
Establish this when an inï¬nite series f(z) = âˆ
n=0 cn(z âˆ’z0)n
deï¬nes f(A) = âˆ
n=0 cn(A âˆ’z0I)n as discussed in (7.3.7).
7.3.6. Explain why det

eA
= etrace(A).
7.3.7. Suppose that for nondiagonalizable matrices AmÃ—m an inï¬nite series
f(z) = âˆ
n=0 cn(z âˆ’z0)n is used to deï¬ne f(A) = âˆ
n=0 cn(A âˆ’z0I)n
as suggested in (7.3.7). Neglecting convergence issues, explain why there
is a polynomial p(z) of at most degree m âˆ’1 such that f(A) = p(A).
7.3.8. If f(A) exists for a diagonalizable A, explain why Af(A) = f(A)A.
What can you say when A is not diagonalizable?
7.3.9. Explain why eA+B = eAeB whenever AB = BA. Give an example
to show that eA+B, eAeB, and eBeA all can diï¬€er when AB Ì¸= BA.
Hint: Exercise 7.2.16 can be used for the diagonalizable case. For the
general case, consider F(t) = e(A+B)t âˆ’eAteBt and Fâ€²(t).
7.3.10. Show that eA is an orthogonal matrix whenever A is skew symmetric.
7.3.11. A particular electronic device consists of a collection of switching circuits
that can be either in an ON state or an OFF state. These electronic
switches are allowed to change state at regular time intervals called clock
cycles. Suppose that at the end of each clock cycle, 30% of the switches
currently in the OFF state change to ON, while 90% of those in the ON
state revert to the OFF state.
(a)
Show that the device approaches an equilibrium in the sense
that the proportion of switches in each state eventually becomes
constant, and determine these equilibrium proportions.
(b)
Independent of the initial proportions, about how many clock
cycles does it take for the device to become essentially stable?

540
Chapter 7
Eigenvalues and Eigenvectors
7.3.12. The spectral radius of A is Ï(A) = maxÎ»iâˆˆÏƒ(A) |Î»i| (p. 497). Prove
that if A is diagonalizable, then
Ï(A) = lim
nâ†’âˆâˆ¥Anâˆ¥1/n
for every matrix norm.
This result is true for nondiagonalizable matrices as well, but the proof
at this point in the game is more involved. The full development is given
in Example 7.10.1 (p. 619).
7.3.13. Find a dominant eigenpair for A=

7
2
3
0
2
0
âˆ’6
âˆ’2
âˆ’2

by the power method.
7.3.14. Apply the inverse power method (Example 7.3.8, p. 534) to ï¬nd an eigen-
vector for each of the eigenvalues of the matrix A in Exercise 7.3.13.
7.3.15. Explain why the function m(v) used in the development of the power
method in Example 7.3.7 is not a continuous function, so statements
like m(xn) â†’m(x) when xn â†’x are not valid. Nevertheless, if
limnâ†’âˆxn Ì¸= 0, then limnâ†’âˆm(xn) Ì¸= 0.
7.3.16. Let H =

1
0
0
âˆ’1
âˆ’2
âˆ’1
0
2
1

.
(a)
Apply the â€œvanillaâ€ QR iteration to H.
(b)
Apply the the single shift QR iteration on H.
7.3.17. Show that the QR iteration can fail to converge using H =
 0
0
1
1
0
0
0
1
0

.
(a)
First use the â€œvanillaâ€ QR iteration on H to see what happens.
(b)
Now try the single shift QR iteration on H.
(c)
Finally, execute the double shift QR iteration on H.

7.4 Systems of Diï¬€erential Equations
541
7.4
SYSTEMS OF DIFFERENTIAL EQUATIONS
Systems of ï¬rst-order linear diï¬€erential equations with constant coeï¬ƒcients were
used in Â§7.1 to motivate the introduction of eigenvalues and eigenvectors, but
now we can delve a little deeper. For constants aij, the goal is to solve the
following system for the unknown functions ui(t).
uâ€²
1 = a11u1 + a12u2 + Â· Â· Â· + a1nun,
uâ€²
2 = a21u1 + a22u2 + Â· Â· Â· + a2nun,
...
uâ€²
n = an1u1 + an2u2 + Â· Â· Â· + annun,
with
u1(0) = c1,
u2(0) = c2,
...
un(0) = cn.
(7.4.1)
Since the scalar exponential provides the unique solution to a single diï¬€erential
equation uâ€²(t) = Î±u(t) with u(0) = c as u(t) = eÎ±tc, itâ€™s only natural to try to
use the matrix exponential in an analogous way to solve a system of diï¬€erential
equations. Begin by writing (7.4.1) in matrix form as uâ€² = Au, u(0) = c, where
u =
ï£«
ï£¬
ï£¬
ï£­
u1(t)
u2(t)
...
un(t)
ï£¶
ï£·
ï£·
ï£¸,
A =
ï£«
ï£¬
ï£¬
ï£­
a11
a12
Â· Â· Â·
a1n
a21
a22
Â· Â· Â·
a2n
...
...
...
...
an1
an2
Â· Â· Â·
ann
ï£¶
ï£·
ï£·
ï£¸,
and
c =
ï£«
ï£¬
ï£¬
ï£­
c1
c2
...
cn
ï£¶
ï£·
ï£·
ï£¸.
If A is diagonalizable with Ïƒ(A) = {Î»1, Î»2, . . . , Î»k} , then (7.3.6) guarantees
eAt = eÎ»1tG1 + eÎ»2tG2 + Â· Â· Â· + eÎ»ktGk.
(7.4.2)
The following identities are derived from properties of the Gi â€™s given on p. 517.
â€¢
deAt/dt = k
i=1 Î»ieÎ»itGi =
k
i=1 Î»iGi
 k
i=1 eÎ»itGi

= AeAt.
(7.4.3)
â€¢
AeAt = eAtA
(by a similar argument).
(7.4.4)
â€¢
eâˆ’AteAt = eAteâˆ’At = I = e0
(by a similar argument).
(7.4.5)
Equation (7.4.3) insures that u = eAtc is one solution to uâ€² = Au, u(0) = c.
To see that u = eAtc is the only solution, suppose v(t) is another solution so
that vâ€² = Av with v(0) = c. Diï¬€erentiating eâˆ’Atv produces
d

eâˆ’Atv

dt
= eâˆ’Atvâ€² âˆ’eâˆ’AtAv = 0,
so
eâˆ’Atv is constant for all t.

542
Chapter 7
Eigenvalues and Eigenvectors
At t = 0 we have eâˆ’Atv

t=0 = e0v(0) = Ic = c, and hence eâˆ’Atv = c for
all t. Multiply both sides of this equation by eAt and use (7.4.5) to conclude
v = eAtc. Thus u = eAtc is the unique solution to uâ€² = Au with u(0) = c.
Finally, notice that vi = Gic âˆˆN (A âˆ’Î»iI) is an eigenvector associated
with Î»i, so that the solution to uâ€² = Au, u(0) = c, is
u = eÎ»1tv1 + eÎ»2tv2 + Â· Â· Â· + eÎ»ktvk,
(7.4.6)
and this solution is completely determined by the eigenpairs (Î»i, vi). It turns
out that u also can be expanded in terms of any complete set of independent
eigenvectorsâ€”see Exercise 7.4.1. Letâ€™s summarize whatâ€™s been said so far.
Differential Equations
If AnÃ—n is diagonalizable with Ïƒ (A) = {Î»1, Î»2, . . . , Î»k} , then the
unique solution of uâ€² = Au, u(0) = c, is given by
u = eAtc = eÎ»1tv1 + eÎ»2tv2 + Â· Â· Â· + eÎ»ktvk
(7.4.7)
in which vi is the eigenvector vi = Gic, where Gi is the ith spectral
projector. (See Exercise 7.4.1 for an alternate eigenexpansion.) Nonho-
mogeneous systems as well as the nondiagonalizable case are treated in
Example 7.9.6 (p. 608).
Example 7.4.1
An Application to Diï¬€usion.
Important issues in medicine and biology in-
volve the question of how drugs or chemical compounds move from one cell to
another by means of diï¬€usion through cell walls. Consider two cells, as depicted
in Figure 7.4.1, which are both devoid of a particular compound. A unit amount
of the compound is injected into the ï¬rst cell at time t = 0, and as time proceeds
the compound diï¬€uses according to the following assumption.
Cell  1
Cell  2
Î±
Î²
Figure 7.4.1

7.4 Systems of Diï¬€erential Equations
543
At each point in time the rate (amount per second) of diï¬€usion from one cell to
the other is proportional to the concentration (amount per unit volume) of the
compound in the cell giving up the compoundâ€”say the rate of diï¬€usion from
cell 1 to cell 2 is Î± times the concentration in cell 1, and the rate of diï¬€usion
from cell 2 to cell 1 is Î² times the concentration in cell 2. Assume Î±, Î² > 0.
Problem: Determine the concentration of the compound in each cell at any
given time t, and, in the long run, determine the steady-state concentrations.
Solution: If uk = uk(t) denotes the concentration of the compound in cell k at
time t, then the statements in the above assumption are translated as follows:
du1
dt = rate in âˆ’rate out = Î²u2 âˆ’Î±u1,
where
u1(0) = 1,
du2
dt = rate in âˆ’rate out = Î±u1 âˆ’Î²u2,
where
u2(0) = 0.
In matrix notation this system is uâ€² = Au, u(0) = c, where
A =

âˆ’Î±
Î²
Î±
âˆ’Î²

,
u =

u1
u2

,
and
c =

1
0

.
Since A is the matrix of Example 7.3.3 we can use the results from Example
7.3.3 to write the solution as
u(t) = eAtc =
1
Î± + Î²
%
Î²
Î²
Î±
Î±

+ eâˆ’(Î±+Î²)t

Î±
âˆ’Î²
âˆ’Î±
Î²
& 
1
0

,
so that
u1(t) =
Î²
Î± + Î² +
Î±
Î± + Î² eâˆ’(Î±+Î²)t
and
u2(t) =
Î±
Î± + Î²

1 âˆ’eâˆ’(Î±+Î²)t
.
In the long run, the concentrations in each cell stabilize in the sense that
lim
tâ†’âˆu1(t) =
Î²
Î± + Î²
and
lim
tâ†’âˆu2(t) =
Î±
Î± + Î² .
An innumerable variety of physical situations can be modeled by uâ€² = Au,
and the form of the solution (7.4.6) makes it clear that the eigenvalues and
eigenvectors of A are intrinsic to the underlying physical phenomenon being
investigated. We might say that the eigenvalues and eigenvectors of A act as its
genes and chromosomes because they are the basic components that either dic-
tate or govern all other characteristics of A along with the physics of associated
phenomena.

544
Chapter 7
Eigenvalues and Eigenvectors
For example, consider the long-run behavior of a physical system that can be
modeled by uâ€² = Au. We usually want to know whether the system will even-
tually blow up or will settle down to some sort of stable state. Might it neither
blow up nor settle down but rather oscillate indeï¬nitely? These are questions
concerning the nature of the limit
lim
tâ†’âˆu(t) = lim
tâ†’âˆeAtc = lim
tâ†’âˆ

eÎ»1tG1 + eÎ»2tG2 + Â· Â· Â· + eÎ»ktGk

c,
and the answers depend only on the eigenvalues. To see how, recall that for a
complex number Î» = x + iy and a real parameter t > 0,
eÎ»t = e(x+iy)t = exteiyt = ext (cos yt + i sin yt) .
(7.4.8)
The term eiyt = (cos yt + i sin yt) is a point on the unit circle that oscillates as a
function of t, so |eiyt| = |cos yt + i sin yt| = 1 and
eÎ»t = |exteiyt| = |ext| = ext.
This makes it clear that if Re (Î»i) < 0 for each i, then, as t â†’âˆ, eAt â†’0,
and u(t) â†’0 for every initial vector c. Thus the system eventually settles down
to zero, and we say the system is stable. On the other hand, if Re (Î»i) > 0 for
some i, then components of u(t) may become unbounded as t â†’âˆ, and
we say the system is unstable. Finally, if
Re (Î»i) â‰¤0 for each i, then the
components of u(t) remain ï¬nite for all t, but some may oscillate indeï¬nitely,
and this is called a semistable situation. Below is a summary of stability.
Stability
Let uâ€² = Au, u(0) = c, where A is diagonalizable with eigenvalues
Î»i.
â€¢
If Re (Î»i) < 0 for each i, then
lim
tâ†’âˆeAt = 0, and
lim
tâ†’âˆu(t) = 0
for every initial vector c. In this case uâ€² = Au is said to be a stable
system, and A is called a stable matrix.
â€¢
If Re (Î»i) > 0 for some i, then components of u(t) can become
unbounded as t â†’âˆ, in which case the system uâ€² = Au as well
as the underlying matrix A are said to be unstable.
â€¢
If Re (Î»i) â‰¤0 for each i, then the components of u(t) remain
ï¬nite for all t, but some can oscillate indeï¬nitely. This is called a
semistable situation.
Example 7.4.2
Predatorâ€“Prey Application.
Consider two species of which one is the preda-
tor and the other is the prey, and assume there are initially 100 in each popula-
tion. Let u1(t) and u2(t) denote the respective population of the predator and

7.4 Systems of Diï¬€erential Equations
545
prey species at time t, and suppose their growth rates are given by
uâ€²
1 = u1 + u2,
uâ€²
2 = âˆ’u1 + u2.
Problem: Determine the size of each population at all future times, and decide
if (and when) either population will eventually become extinct.
Solution: Write the system as uâ€² = Au, u(0) = c, where
A =

1
1
âˆ’1
1

,
u =

u1
u2

,
and
c =

100
100

.
The characteristic equation for A is p(Î») = Î»2 âˆ’2Î» + 2 = 0, so the eigenvalues
for A are Î»1 = 1 + i and Î»2 = 1 âˆ’i. We know from (7.4.7) that
u(t) = eÎ»1tv1 + eÎ»2tv2
(where vi = Gic )
(7.4.9)
is the solution to uâ€² = Au, u(0) = c. The spectral theorem on p. 517 implies
A âˆ’Î»2I = (Î»1 âˆ’Î»2)G1 and I = G1 + G2, so (A âˆ’Î»2I)c = (Î»1 âˆ’Î»2)v1 and
c = v1 + v2, and consequently
v1 = (A âˆ’Î»2I)c
(Î»1 âˆ’Î»2) = 50

Î»2
Î»1

and
v2 = c âˆ’v1 = 50

Î»1
Î»2

.
With the aid of (7.4.8) we obtain the solution components from (7.4.9) as
u1(t) = 50

Î»2eÎ»1t + Î»1eÎ»2t
= 100et(cos t + sin t)
and
u2(t) = 50

Î»1eÎ»1t + Î»2eÎ»2t
= 100et(cos t âˆ’sin t).
The system is unstable because Re (Î»i) > 0 for each eigenvalue. Indeed, u1(t)
and u2(t) both become unbounded as t â†’âˆ. However, a population cannot
become negativeâ€“once itâ€™s zero, itâ€™s extinct. Figure 7.4.2 shows that the graph
of u2(t) will cross the horizontal axis before that of u1(t).
u1(t)
u2(t)
   t
0.2
0.4
0.6
0.8
1
-200
-100
100
200
300
400
0
Figure 7.4.2

546
Chapter 7
Eigenvalues and Eigenvectors
Therefore, the prey species will become extinct at the value of t for which
u2(t) = 0 â€”i.e., when
100et(cos t âˆ’sin t) = 0
=â‡’
cos t = sin t
=â‡’
t = Ï€
4 .
Exercises for section 7.4
7.4.1. Suppose that AnÃ—n is diagonalizable, and let P = [x1 | x2 | Â· Â· Â· | xn]
be a matrix whose columns are a complete set of linearly independent
eigenvectors corresponding to eigenvalues Î»i. Show that the solution to
uâ€² = Au, u(0) = c, can be written as
u(t) = Î¾1eÎ»1tx1 + Î¾2eÎ»2tx2 + Â· Â· Â· + Î¾neÎ»ntxn
in which the coeï¬ƒcients Î¾i satisfy the algebraic system PÎ¾ = c.
7.4.2. Using only the eigenvalues, determine the long-run behavior of the so-
lution to uâ€² = Au, u(0) = c for each of the following matrices.
(a)
A =

âˆ’1
âˆ’2
0
âˆ’3

. (b) A =

1
âˆ’2
0
3

. (c) A =

1
âˆ’2
1
âˆ’1

.
7.4.3. Competing Species.
Consider two species that coexist in the same
environment but compete for the same resources. Suppose that the pop-
ulation of each species increases proportionally to the number of its
own kind but decreases proportionally to the number in the competing
speciesâ€”say that the population of each species increases at a rate equal
to twice its existing number but decreases at a rate equal to the number
in the other population. Suppose that there are initially 100 of species I
and 200 of species II.
(a)
Determine the number of each species at all future times.
(b)
Determine which species is destined to become extinct, and com-
pute the time to extinction.
7.4.4. Cooperating Species.
Consider two species that survive in a sym-
biotic relationship in the sense that the population of each species de-
creases at a rate equal to its existing number but increases at a rate
equal to the existing number in the other population.
(a)
If there are initially 200 of species I and 400 of species II, deter-
mine the number of each species at all future times.
(b)
Discuss the long-run behavior of each species.

7.5 Normal Matrices
547
7.5
NORMAL MATRICES
A matrix A is diagonalizable if and only if A possesses a complete independent
set of eigenvectors, and if such a complete set is used for columns of P, then
Pâˆ’1AP = D is diagonal (p. 507). But even when A possesses a complete in-
dependent set of eigenvectors, thereâ€™s no guarantee that a complete orthonormal
set of eigenvectors can be found. In other words, thereâ€™s no assurance that P
can be taken to be unitary (or orthogonal). And the Gramâ€“Schmidt procedure
(p. 309) doesnâ€™t helpâ€”Gramâ€“Schmidt can turn a basis of eigenvectors into an
orthonormal basis but not into an orthonormal basis of eigenvectors. So when (or
how) are complete orthonormal sets of eigenvectors produced? In other words,
when is A unitarily similar to a diagonal matrix?
Unitary Diagonalization
A âˆˆCnÃ—n is unitarily similar to a diagonal matrix (i.e., A has a com-
plete orthonormal set of eigenvectors) if and only if Aâˆ—A = AAâˆ—, in
which case A is said to be a normal matrix.
â€¢
Whenever Uâˆ—AU = D with U unitary and D diagonal, the
columns of U must be a complete orthonormal set of eigenvectors
for A, and the diagonal entries of D are the associated eigenvalues.
Proof.
If A is normal with Ïƒ (A) = {Î»1, Î»2, . . . , Î»k} , then Aâˆ’Î»kI is also nor-
mal. All normal matrices are RPN (range is perpendicular to nullspace, p. 409),
so there is a unitary matrix Uk such that
Uâˆ—
k(A âˆ’Î»kI)Uk =

Ck
0
0
0

(by (5.11.15) on p. 408)
or, equivalently
Uâˆ—
kAUk =

Ck+Î»kI
0
0
Î»kI

=

Akâˆ’1
0
0
Î»kI

,
where Ck is nonsingular and Akâˆ’1 = Ck+Î»kI. Note that Î»k /âˆˆÏƒ (Akâˆ’1) (oth-
erwise Akâˆ’1 âˆ’Î»kI = Ck would be singular), so Ïƒ (Akâˆ’1) = {Î»1, Î»2, . . . , Î»kâˆ’1}
(Exercise 7.1.4). Because Akâˆ’1 is also normal, the same argument can be re-
peated with Akâˆ’1 and Î»kâˆ’1 in place A and Î»k to insure the existence of a
unitary matrix Ukâˆ’1 such that
Uâˆ—
kâˆ’1Akâˆ’1Ukâˆ’1 =

Akâˆ’2
0
0
Î»kâˆ’1I

,

548
Chapter 7
Eigenvalues and Eigenvectors
where Akâˆ’2 is normal and Ïƒ (Akâˆ’2) = {Î»1, Î»2, . . . , Î»kâˆ’2} . After k such rep-
etitions, Uk
 Ukâˆ’1
0
0
I

Â· Â· Â·
 U1
0
0
I

= U is a unitary matrix such that
Uâˆ—AU =
ï£«
ï£¬
ï£¬
ï£­
Î»1Ia1
0
Â· Â· Â·
0
0
Î»2Ia2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
Î»kIak
ï£¶
ï£·
ï£·
ï£¸= D,
ai = alg multA (Î»i) .
(7.5.1)
Conversely, if there is a unitary matrix U such that Uâˆ—AU = D is diagonal,
then Aâˆ—A = UDâˆ—DUâˆ—= U = UDDâˆ—Uâˆ—= AAâˆ—, so A is normal.
Caution! While itâ€™s true that normal matrices possess a complete orthonormal
set of eigenvectors, not all complete independent sets of eigenvectors of a normal
A are orthonormal (or even orthogonal)â€”see Exercise 7.5.6. Below are some
things that are true.
Properties of Normal Matrices
If A is a normal matrix with Ïƒ (A) = {Î»1, Î»2, . . . , Î»k} , then
â€¢
A is RPNâ€”i.e., R (A) âŠ¥N (A) (see p. 408).
â€¢
Eigenvectors corresponding to distinct eigenvalues are orthogonal. In
other words,
N (A âˆ’Î»iI) âŠ¥N (A âˆ’Î»jI)
for Î»i Ì¸= Î»j.
(7.5.2)
â€¢
The spectral theorems (7.2.7) and (7.3.6) on pp. 517 and 526 hold,
but the spectral projectors Gi on p. 529 specialize to become orthog-
onal projectors because R (A âˆ’Î»iI) âŠ¥N (A âˆ’Î»iI) for each Î»i.
Proof of (7.5.2).
If A is normal, so is A âˆ’Î»jI, and hence A âˆ’Î»jI is RPN.
Consequently, N (A âˆ’Î»jI)âˆ—= N (A âˆ’Î»jI) â€”recall (5.11.14) from p. 408. If
(Î»i, xi) and (Î»j, xj) are distinct eigenpairs, then (A âˆ’Î»jI)âˆ—xj = 0, and 0 =
xâˆ—
j(A âˆ’Î»jI)xi = xâˆ—
jAxi âˆ’xâˆ—
jÎ»jxi = (Î»i âˆ’Î»j)xâˆ—
jxi implies 0 = xâˆ—
jxi.
Several common types of matrices are normal. For example, real-symmetric
and hermitian matrices are normal, real skew-symmetric and skew-hermitian
matrices are normal, and orthogonal and unitary matrices are normal. By virtue
of being normal, these kinds of matrices inherit all of the above properties, but
itâ€™s worth looking a bit closer at the real-symmetric and hermitian matrices
because they have some special eigenvalue properties.
If A is real symmetric or hermitian, and if (Î», x) is an eigenpair for A,
then xâˆ—x Ì¸= 0, and Î»x = Ax implies Î»xâˆ—= xâˆ—Aâˆ—, so
xâˆ—x(Î» âˆ’Î») = xâˆ—(Î» âˆ’Î»)x = xâˆ—Ax âˆ’xâˆ—Aâˆ—x = 0
=â‡’
Î» = Î».

7.5 Normal Matrices
549
In other words, eigenvalues of real-symmetric and hermitian matrices are real.
A similar argument (Exercise 7.5.4) shows that the eigenvalues of a real skew-
symmetric or skew-hermitian matrix are pure imaginary numbers.
Eigenvectors for a hermitian A âˆˆCnÃ—n may have to involve complex num-
bers, but a real-symmetric matrix possesses a complete orthonormal set of real
eigenvectors. Consequently, the real-symmetric case can be distinguished by ob-
serving that A is real symmetric if and only if A is orthogonally similar to a
real-diagonal matrix D. Below is a summary of these observations.
Symmetric and Hermitian Matrices
In addition to the properties inherent to all normal matrices,
â€¢
Real-symmetric and hermitian matrices have real eigenvalues. (7.5.3)
â€¢
A is real symmetric if and only if A is orthogonally similar to a
real-diagonal matrix D â€”i.e., PT AP = D for some orthogonal P.
â€¢
Real skew-symmetric and skew-hermitian matrices have pure imag-
inary eigenvalues.
Example 7.5.1
Largest and Smallest Eigenvalues. Since the eigenvalues of a hermitian ma-
trix AnÃ—n are real, they can be ordered as Î»1 â‰¥Î»2 â‰¥Â· Â· Â· â‰¥Î»n.
Problem: Explain why the largest and smallest eigenvalues can be described as
Î»1 = max
âˆ¥xâˆ¥2=1 xâˆ—Ax
and
Î»n = min
âˆ¥xâˆ¥2=1 xâˆ—Ax.
(7.5.4)
Solution: There is a unitary U such that Uâˆ—AU = D = diag (Î»1, Î»2, . . . , Î»n)
or, equivalently, A = UDUâˆ—. Since âˆ¥xâˆ¥2 = 1 â‡â‡’âˆ¥yâˆ¥2 = 1 for y = Uâˆ—x,
max
âˆ¥xâˆ¥2=1 xâˆ—Ax = max
âˆ¥yâˆ¥2=1 yâˆ—Dy = max
âˆ¥yâˆ¥2=1
n

i=1
Î»i|yi|2 â‰¤max
âˆ¥yâˆ¥2=1 Î»1
n

i=1
|yi|2 = Î»1
with equality being attained when x is an eigenvector of unit norm associated
with Î»1. The expression for the smallest eigenvalue Î»n is obtained by writing
min
âˆ¥xâˆ¥2=1 xâˆ—Ax =
min
âˆ¥yâˆ¥2=1 yâˆ—Dy =
min
âˆ¥yâˆ¥2=1
n

i=1
Î»i|yi|2 â‰¥
min
âˆ¥yâˆ¥2=1 Î»n
n

i=1
|yi|2 = Î»n,
where equality is attained at an eigenvector of unit norm associated with Î»n.
Note: The characterizations in (7.5.4) often appear in the equivalent forms
Î»1 = max
xÌ¸=0
xâˆ—Ax
xâˆ—x
and
Î»n = min
xÌ¸=0
xâˆ—Ax
xâˆ—x .

550
Chapter 7
Eigenvalues and Eigenvectors
Consequently, Î»1 â‰¥(xâˆ—Ax/xâˆ—x) â‰¥Î»n for all x Ì¸= 0. The term xâˆ—Ax/xâˆ—x
is referred to as a Rayleigh quotient in honor of the famous English physicist
John William Strutt (1842â€“1919) who became Baron Rayleigh in 1873.
Itâ€™s only natural to wonder if the intermediate eigenvalues of a hermitian
matrix have representations similar to those for the extreme eigenvalues as de-
scribed in (7.5.4). Ernst Fischer (1875â€“1954) gave the answer for matrices in 1905,
and Richard Courant (1888â€“1972) provided extensions for inï¬nite-dimensional
operators in 1920.
Courantâ€“Fischer Theorem
The eigenvalues Î»1 â‰¥Î»2 â‰¥Â· Â· Â· â‰¥Î»n of a hermitian matrix AnÃ—n are
Î»i = max
dim V=i min
xâˆˆV
âˆ¥xâˆ¥2=1
xâˆ—Ax
and
Î»i =
min
dim V=nâˆ’i+1 max
xâˆˆV
âˆ¥xâˆ¥2=1
xâˆ—Ax.
(7.5.5)
When i = 1 in the min-max formula and when i = n in the max-
min formula, V = Cn, so these cases reduce to the equations in (7.5.4).
Alternate max-min and min-max formulas are given in Exercise 7.5.12.
Proof.
Only the min-max characterization is provenâ€”the max-min proof is
analogous (Exercise 7.5.11). As shown in Example 7.5.1, a change of coordinates
y = Uâˆ—x with a unitary U such that Uâˆ—AU = D = diag (Î»1, Î»2, . . . , Î»n) has
the eï¬€ect of replacing A by D, so we need only establish that
Î»i =
min
dim V=nâˆ’i+1 max
yâˆˆV
âˆ¥yâˆ¥2=1
yâˆ—Dy.
For a subspace V of dimension n âˆ’i + 1, let SV = {y âˆˆV, âˆ¥yâˆ¥2 = 1}, and let
Sâ€²
V = {y âˆˆV âˆ©F, âˆ¥yâˆ¥2 = 1},
where
F = span {e1, e2, . . . , ei} .
Note that V âˆ©F Ì¸= 0, for otherwise dim(V + F) = dim V + dim F = n + 1,
which is impossible. In other words, Sâ€²
V contains those vectors of SV of the
form y = (y1, . . . , yi, 0, . . . , 0)T with i
j=1 |yj|2 = 1. So for each subspace V
with dim V = n âˆ’i + 1,
yâˆ—Dy =
i

j=1
Î»j|yj|2 â‰¥Î»i
i

j=1
|yj|2 = Î»i
for all y âˆˆSâ€²
V.
Since Sâ€²
V âŠ†SV, it follows that maxSV yâˆ—Dy â‰¥maxSâ€²
V yâˆ—Dy â‰¥Î»i, and hence
min
V
max
SV
yâˆ—Dy â‰¥Î»i.

7.5 Normal Matrices
551
But this inequality is reversible because if ËœV = {e1, e2, . . . , eiâˆ’1}âŠ¥, then every
y âˆˆËœV has the form y = (0, . . . , 0, yi, . . . , yn)T , and hence
yâˆ—Dy =
n

j=i
Î»j|yj|2 â‰¤Î»i
n

j=i
|yj|2 = Î»i
for all y âˆˆSËœV.
So min
V
max
SV
yâˆ—Dy â‰¤max
S Ëœ
V
yâˆ—Dy â‰¤Î»i, and thus min
V
max
SV
yâˆ—Dy = Î»i.
The value of the Courantâ€“Fischer theorem is its ability to produce inequal-
ities concerning eigenvalues of hermitian matrices without involving the associ-
ated eigenvectors. This is illustrated in the following two important examples.
Example 7.5.2
Eigenvalue Perturbations. Let Î»1 â‰¥Î»2 â‰¥Â· Â· Â· â‰¥Î»n be the eigenvalues of
a hermitian A âˆˆCnÃ—n, and suppose A is perturbed by a hermitian E with
eigenvalues Ïµ1 â‰¥Ïµ2 â‰¥Â· Â· Â· â‰¥Ïµn to produce B = A + E, which is also hermitian.
Problem: If Î²1 â‰¥Î²2 â‰¥Â· Â· Â· â‰¥Î²n are the eigenvalues of B, explain why
Î»i + Ïµ1 â‰¥Î²i â‰¥Î»i + Ïµn
for each i.
(7.5.6)
Solution: If U is a unitary matrix such that Uâˆ—AU = D = diag (Î»1, . . . , Î»n),
then ËœB = Uâˆ—BU and ËœE = Uâˆ—EU have the same eigenvalues as B and E,
respectively, and ËœB = D+ ËœE. For x âˆˆF = span {e1, e2, . . . , ei} with âˆ¥xâˆ¥2 = 1,
x = (x1, . . . , xi, 0, . . . , 0)T
and
xâˆ—Dx =
i

j=1
Î»j|xj|2 â‰¥Î»i
i

j=1
|xj|2 = Î»i,
so applying the max-min part of the Courantâ€“Fischer theorem to ËœB yields
Î²i = max
dim V=i min
xâˆˆV
âˆ¥xâˆ¥2=1
xâˆ—ËœBx â‰¥min
xâˆˆF
âˆ¥xâˆ¥2=1
xâˆ—ËœBx = min
xâˆˆF
âˆ¥xâˆ¥2=1

xâˆ—Dx + xâˆ—ËœEx

â‰¥min
xâˆˆF
âˆ¥xâˆ¥2=1
xâˆ—Dx + min
xâˆˆF
âˆ¥xâˆ¥2=1
xâˆ—ËœEx â‰¥Î»i + min
xâˆˆCn
âˆ¥xâˆ¥2=1
xâˆ—ËœEx = Î»i + Ïµn,
where the last equality is the result of the â€œminâ€ part of (7.5.4). Similarly, for
x âˆˆT = span {ei, . . . , en} with âˆ¥xâˆ¥2 = 1, we have xâˆ—Dx â‰¤Î»i, and
Î²i =
min
dim V=nâˆ’i+1 max
xâˆˆV
âˆ¥xâˆ¥2=1
xâˆ—ËœBx â‰¤max
xâˆˆT
âˆ¥xâˆ¥2=1
xâˆ—ËœBx = max
xâˆˆT
âˆ¥xâˆ¥2=1

xâˆ—Dx + xâˆ—ËœEx

â‰¤max
xâˆˆT
âˆ¥xâˆ¥2=1
xâˆ—Dx + max
xâˆˆT
âˆ¥xâˆ¥2=1
xâˆ—ËœEx â‰¤Î»i + max
xâˆˆCn
âˆ¥xâˆ¥2=1
xâˆ—ËœEx = Î»i + Ïµ1.

552
Chapter 7
Eigenvalues and Eigenvectors
Note: Because E often represents an error, only âˆ¥Eâˆ¥(or an estimate thereof)
is known. But for every matrix norm, |Ïµj| â‰¤âˆ¥Eâˆ¥for each j (Example 7.1.4,
p. 497). Since the Ïµj â€™s are real, âˆ’âˆ¥Eâˆ¥â‰¤Ïµj â‰¤âˆ¥Eâˆ¥, so (7.5.6) guarantees that
Î»i âˆ’âˆ¥Eâˆ¥â‰¤Î²i â‰¤Î»i + âˆ¥Eâˆ¥.
(7.5.7)
In other words,
â€¢
the eigenvalues of a hermitian matrix A are perfectly conditioned because a
hermitian perturbation E changes no eigenvalue of A by more than âˆ¥Eâˆ¥.
Itâ€™s interesting to compare (7.5.7) with the Bauerâ€“Fike bound of Example 7.3.2
(p. 528). When A is hermitian, (7.3.10) reduces to minÎ»iâˆˆÏƒ(A) |Î² âˆ’Î»i| â‰¤âˆ¥Eâˆ¥
because P can be made unitary, so, for induced matrix norms, Îº(P) = 1. The
two results diï¬€er in that Bauerâ€“Fike does not assume E and B are hermitian.
Example 7.5.3
Interlaced Eigenvalues. For a hermitian matrix A âˆˆCnÃ—n with eigenvalues
Î»1 â‰¥Î»2 â‰¥Â· Â· Â· â‰¥Î»n, and for c âˆˆCnÃ—1, let B be the bordered matrix
B =

A
c
câˆ—
Î±

n+1Ã—n+1
with eigenvalues
Î²1 â‰¥Î²2 â‰¥Â· Â· Â· â‰¥Î²n â‰¥Î²n+1.
Problem: Explain why the eigenvalues of A interlace with those of B in that
Î²1 â‰¥Î»1 â‰¥Î²2 â‰¥Î»2 â‰¥Â· Â· Â· â‰¥Î²n â‰¥Î»n â‰¥Î²n+1.
(7.5.8)
Solution: To see that Î²i â‰¥Î»i â‰¥Î²i+1 for 1 â‰¤i â‰¤n, let U be a unitary
matrix such that UT AU = D = diag (Î»1, Î»2, . . . , Î»n) . Since V =
 U
0
0
1

is
also unitary, the eigenvalues of B agree with those of
ËœB = Vâˆ—BV =

D
y
yâˆ—
Î±

,
where
y = Uâˆ—c.
For x âˆˆF = span {e1, e2, . . . , ei} âŠ‚Cn+1Ã—1 with âˆ¥xâˆ¥2 = 1,
x = (x1, . . . , xi, 0, . . . , 0)T
and
xâˆ—ËœBx =
n

j=1
Î»j|xj|2 â‰¥Î»i
n

j=1
|xj|2 = Î»i,
so applying the max-min part of the Courantâ€“Fisher theorem to ËœB yields
Î²i = max
dim V=i min
xâˆˆV
âˆ¥xâˆ¥2=1
xâˆ—ËœBx â‰¥min
xâˆˆF
âˆ¥xâˆ¥2=1
xâˆ—ËœBx â‰¥Î»i.

7.5 Normal Matrices
553
For x âˆˆT = span{eiâˆ’1, ei, . . . , en} âŠ‚Cn+1Ã—1 with âˆ¥xâˆ¥2 = 1,
x = (0, . . . , 0, xiâˆ’1, . . . , xn, 0)T and xâˆ—ËœBx=
n

j=iâˆ’1
Î»j|xj|2 â‰¤Î»iâˆ’1
n

j=i
|xj|2 = Î»iâˆ’1,
so the min-max part of the Courantâ€“Fisher theorem produces
Î²i =
min
dim V=nâˆ’i+2 max
xâˆˆV
âˆ¥xâˆ¥2=1
xâˆ—ËœBx â‰¤max
xâˆˆF
âˆ¥xâˆ¥2=1
xâˆ—ËœBx â‰¤Î»iâˆ’1.
Note: If A is any n Ã— n principal submatrix of B, then (7.5.8) still holds
because each principal submatrix can be brought to the upper-left-hand corner
by a similarity transformation PT BP, where P is a permutation matrix. In
other words,
â€¢
the eigenvalues of an n + 1 Ã— n + 1 hermitian matrix are interlaced with the
eigenvalues of each of its n Ã— n principal submatrices.
For A âˆˆCmÃ—n (or â„œmÃ—n), the products Aâˆ—A and AAâˆ—(or AT A and
AAT ) are hermitian (or real symmetric), so they are diagonalizable by a uni-
tary (or orthogonal) similarity transformation, and their eigenvalues are nec-
essarily real. But in addition to being real, the eigenvalues of these matrices
are always nonnegative. For example, if (Î», x) is an eigenpair of Aâˆ—A, then
Î» = xâˆ—Aâˆ—Ax/xâˆ—x = âˆ¥Axâˆ¥2
2 / âˆ¥xâˆ¥2
2 â‰¥0, and similarly for the other products. In
fact, these Î» â€™s are the squares of the singular values for A developed in Â§5.12
(p. 411) because if
A = U

DrÃ—r
0
0
0

mÃ—n
Vâˆ—
is a singular value decomposition, where D = diag (Ïƒ1, Ïƒ2, . . . , Ïƒr) contains the
nonzero singular values of A, then
Vâˆ—Aâˆ—AV =

D2
0
0
0

,
(7.5.9)
and this means that (Ïƒ2
i , vi) for i = 1, 2, . . . , r is an eigenpair for Aâˆ—A. In
other words, the nonzero singular values of A are precisely the positive square
roots of the nonzero eigenvalues of Aâˆ—A, and right-hand singular vectors vi of
A are particular eigenvectors of Aâˆ—A. Note that this establishes the uniqueness
of the Ïƒi â€™s (but not the vi â€™s), and pay attention to the fact that the number
of zero singular values of A need not agree with the number of zero eigenvalues
of Aâˆ—A â€”e.g., A1Ã—2 = (1, 1) has no zero singular values, but Aâˆ—A has one
zero eigenvalue. The same game can be played with AAâˆ—in place of Aâˆ—A to
argue that the nonzero singular values of A are the positive square roots of

554
Chapter 7
Eigenvalues and Eigenvectors
the nonzero eigenvalues of AAâˆ—, and left-hand singular vectors ui of A are
particular eigenvectors of AAâˆ—.
Caution! The statement that right-hand singular vectors vi of A are eigenvec-
tors of Aâˆ—A and left-hand singular vectors ui of A are eigenvectors of AAâˆ—
is a one-way streetâ€”it doesnâ€™t mean that just any orthonormal sets of eigen-
vectors for Aâˆ—A and AAâˆ—can be used as respective right-hand and left-hand
singular vectors for A. The columns vi of any unitary matrix V that diago-
nalizes Aâˆ—A as in (7.5.9) can serve as right-hand singular vectors for A, but
corresponding left-hand singular vectors ui are constrained by the relationships
Avi = Ïƒiui,
i = 1, 2, . . . , r
=â‡’
ui = Avi
Ïƒi
=
Avi
âˆ¥Aviâˆ¥2
,
i = 1, 2, . . . , r,
uâˆ—
i A = 0,
i = r + 1, . . . , m
=â‡’
span {ur+1, ur+2, . . . , um} = N (Aâˆ—).
In other words, the ï¬rst r left-hand singular vectors for A are uniquely deter-
mined by the ï¬rst r right-hand singular vectors, while the last m âˆ’r left-hand
singular vectors can be any orthonormal basis for N (Aâˆ—). If U is constructed
from V as described above, then U is guaranteed to be unitary because for
U=

u1 Â· Â· Â· ur|ur+1 Â· Â· Â· um

=

U1|U2

and V=

v1 Â· Â· Â· vr|vr+1 Â· Â· Â· vn

=

V1|V2

,
U1 and U2 each contain orthonormal columns, and, by using (7.5.9),
R (U1) = R

AV1Dâˆ’1
= R (AV1) = R (AV1D) = R

[AV1D][AV1D]âˆ—
= R (AAâˆ—AAâˆ—) = R (AAâˆ—) = R (A) = N (Aâˆ—)âŠ¥= R (U2)âŠ¥.
The matrix V is unitary to start with, but, in addition,
R (V1) = R (V1D) = R ([V1D][V1D]âˆ—) = R (Aâˆ—A) = R (Aâˆ—) and
R (V2) = R (Aâˆ—)âŠ¥= N (A).
These observations are consistent with those established on p. 407 for any
URV factorization. Otherwise something would be terribly wrong because an
SVD is just a special kind of a URV factorization. Finally, notice that there
is nothing special about starting with V to build a U â€”we can also take the
columns of any unitary U that diagonalizes AAâˆ—as left-hand singular vectors
for A and build corresponding right-hand singular vectors in a manner similar
to that described above. Below is a summary of the preceding developments
concerning singular values together with an additional observation connecting
singular values with eigenvalues.

7.5 Normal Matrices
555
Singular Values and Eigenvalues
For A âˆˆCmÃ—n with rank (A) = r, the following statements are valid.
â€¢
The nonzero eigenvalues of Aâˆ—A and AAâˆ—are equal and positive.
â€¢
The nonzero singular values of A are the positive square roots of
the nonzero eigenvalues of Aâˆ—A (and AAâˆ—).
â€¢
If A is normal with nonzero eigenvalues {Î»1, Î»2, . . . , Î»r} , then the
nonzero singular values of A are {|Î»1|, |Î»2|, . . . , |Î»r|}.
â€¢
Right-hand and left-hand singular vectors for A are special eigen-
vectors for Aâˆ—A and AAâˆ—, respectively.
â€¢
Any complete orthonormal set of eigenvectors vi for Aâˆ—A can serve
as a complete set of right-hand singular vectors for A, and a cor-
responding complete set of left-hand singular vectors is given by
ui = Avi/ âˆ¥Aviâˆ¥2, i = 1, 2, . . . , r, together with any orthonormal
basis {ur+1, ur+2, . . . , um} for N (Aâˆ—). Similarly, any complete or-
thonormal set of eigenvectors for AAâˆ—can be used as left-hand sin-
gular vectors for A, and corresponding right-hand singular vectors
can be built in an analogous way.
â€¢
The hermitian matrix B =
 0mÃ—m
A
Aâˆ—
0nÃ—n

of order m + n has
nonzero eigenvalues {Â±Ïƒ1, Â±Ïƒ2, . . . , Â±Ïƒr} in which {Ïƒ1, Ïƒ2, . . . , Ïƒr}
are the nonzero singular values of A.
Proof.
Only the last point requires proof, and this follows by observing that if
Î» is an eigenvalue of B, then

0
A
Aâˆ—
0
 
x1
x2

= Î»

x1
x2

=â‡’
	
Ax2 = Î»x1
Aâˆ—x1 = Î»x2

=â‡’
Aâˆ—Ax2 = Î»2x2,
so each eigenvalue of B is the square of a singular value of A. But B is
hermitian with rank (B) = 2r, so there are exactly 2r nonzero eigenvalues of
B. Therefore, each pair Â±Ïƒi, i = 1, 2, . . . , r, must be an eigenvalue for B.
Example 7.5.4
Min-Max Singular Values. Since the singular values of A are the positive
square roots of the eigenvalues of Aâˆ—A, and since âˆ¥Axâˆ¥2 = (xâˆ—Aâˆ—Ax)1/2, itâ€™s
a corollary of the Courantâ€“Fischer theorem (p. 550) that if Ïƒ1 â‰¥Ïƒ2 â‰¥Â· Â· Â· â‰¥Ïƒn
are the singular values for AmÃ—n (n â‰¤m), then
Ïƒi = max
dim V=i min
xâˆˆV
âˆ¥xâˆ¥2=1
âˆ¥Axâˆ¥2
and
Ïƒi =
min
dim V=nâˆ’i+1 max
xâˆˆV
âˆ¥xâˆ¥2=1
âˆ¥Axâˆ¥2 .

556
Chapter 7
Eigenvalues and Eigenvectors
These expressions provide intermediate values between the extremes
Ïƒ1 = max
âˆ¥xâˆ¥2=1 âˆ¥Axâˆ¥2
and
Ïƒn = min
âˆ¥xâˆ¥2=1 âˆ¥Axâˆ¥2
(see p. 414).
Exercises for section 7.5
7.5.1. Is A =
 5 + i
âˆ’2 i
2
4 + 2 i

a normal matrix?
7.5.2. Give examples of two distinct classes of normal matrices that are real
but not symmetric.
7.5.3. Show that A âˆˆâ„œnÃ—n is normal and has real eigenvalues if and only if
A is symmetric.
7.5.4. Prove that the eigenvalues of a real skew-symmetric or skew-hermitian
matrix must be pure imaginary numbers (i.e., multiples of i ).
7.5.5. When trying to decide whatâ€™s true about matrices and whatâ€™s not, it
helps to think in terms of the following associations.
Hermitian matrices
â†â†’
Real numbers (z = z).
Skew-hermitian matrices
â†â†’
Imaginary numbers (z = âˆ’z).
Unitary matrices
â†â†’
Points on the unit circle (z = eiÎ¸).
For example, the complex function f(z) = (1 âˆ’z)(1 + z)âˆ’1 maps the
imaginary axis in the complex plane to points on the unit circle because
|f(z)|2 = 1 whenever z = âˆ’z. Itâ€™s therefore reasonable to conjecture
(as Cayley did in 1846) that if A is skew hermitian (or real skew sym-
metric), then
f(A) = (I âˆ’A)(I + A)âˆ’1 = (I + A)âˆ’1(I âˆ’A)
(7.5.10)
is unitary (or orthogonal). Prove this is indeed correct. Note: Expression
(7.5.10) has come to be known as the Cayley transformation.
7.5.6. Show by example that a normal matrix can have a complete independent
set of eigenvectors that are not orthonormal, and then explain how every
complete independent set of eigenvectors for a normal matrix can be
transformed into a complete orthonormal set of eigenvectors.

7.5 Normal Matrices
557
7.5.7. Construct an example to show that the converse of (7.5.2) is false. In
other words, show that it is possible for N (A âˆ’Î»iI) âŠ¥N (A âˆ’Î»jI)
whenever i Ì¸= j without A being normal.
7.5.8. Explain why a triangular matrix is normal if and only if it is diagonal.
7.5.9. Use the result of Exercise 7.5.8 to give an alternate proof of the unitary
diagonalization theorem given on p. 547.
7.5.10. For a normal matrix A, explain why (Î», x) is an eigenpair for A if
and only if (Î», x) is an eigenpair for Aâˆ—.
7.5.11. To see if you understand the proof of the min-max part of the Courantâ€“
Fischer theorem (p. 550), construct an analogous proof for the max-min
part of (7.5.5).
7.5.12. The Courantâ€“Fischer theorem has the following alternate formulation.
Î»i =
max
v1,...,vnâˆ’iâˆˆCn
min
xâŠ¥v1,...,vnâˆ’i
âˆ¥xâˆ¥2=1
xâˆ—Ax
and
Î»i =
min
v1,...,viâˆ’1âˆˆCn
max
xâŠ¥v1,...,viâˆ’1
âˆ¥xâˆ¥2=1
xâˆ—Ax
for 1 < i < n. To see if you really understand the proof of the min-
max part of (7.5.5), adapt it to prove the alternate min-max formulation
given above.
7.5.13.
(a)
Explain why every unitary matrix is unitarily similar to a diag-
onal matrix of the form
D =
ï£«
ï£¬
ï£¬
ï£­
eiÎ¸1
0
Â· Â· Â·
0
0
eiÎ¸2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
eiÎ¸n
ï£¶
ï£·
ï£·
ï£¸.
(b)
Prove that every orthogonal matrix is orthogonally similar to a
real block-diagonal matrix of the form
B =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Â±1
...
Â±1
cos Î¸1 sin Î¸1
âˆ’sin Î¸1 cos Î¸1
...
cos Î¸t sin Î¸t
âˆ’sin Î¸t cos Î¸t
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.

558
Chapter 7
Eigenvalues and Eigenvectors
7.6
POSITIVE DEFINITE MATRICES
Since the symmetric structure of a matrix forces its eigenvalues to be real, what
additional property will force all eigenvalues to be positive (or perhaps just non-
negative)? To answer this, letâ€™s deal with real-symmetric matricesâ€”the hermi-
tian case follows along the same lines. If A âˆˆâ„œnÃ—n is symmetric, then, as
observed above, there is an orthogonal matrix P such that A = PDPT , where
D = diag (Î»1, Î»2, . . . , Î»n) is real. If Î»i â‰¥0 for each i, then D1/2 exists, so
A = PDPT = PD1/2D1/2PT = BT B
for
B = D1/2PT ,
and Î»i > 0 for each i if and only if B is nonsingular. Conversely, if A can be
factored as A = BT B, then all eigenvalues of A are nonnegative because for
any eigenpair (Î», x),
Î» = xT Ax
xT x
= xT BT Bx
xT x
= âˆ¥Bxâˆ¥2
2
âˆ¥xâˆ¥2
2
â‰¥0.
Moreover, if B is nonsingular, then N (B) = 0 =â‡’Bx Ì¸= 0, so Î» > 0. In other
words, a real-symmetric matrix A has nonnegative eigenvalues if and only if A
can be factored as A = BT B, and all eigenvalues are positive if and only if B
is nonsingular. A symmetric matrix A whose eigenvalues are positive is called
positive deï¬nite, and when the eigenvalues are just nonnegative, A is said to
be positive semideï¬nite.
The use of this terminology is consistent with that introduced in Exam-
ple 3.10.7 (p. 154), where the term â€œpositive deï¬niteâ€ was used to designate
symmetric matrices possessing an LU factorization with positive pivots. It was
demonstrated in Example 3.10.7 that possessing positive pivots is equivalent to
the existence of a Cholesky factorization A = RT R, where R is upper trian-
gular with positive diagonal entries. By the result of the previous paragraph this
means that all eigenvalues of a symmetric matrix A are positive if and only if
A has an LU factorization with positive pivots.
But the pivots are intimately related to the leading principal minor deter-
minants. Recall from Exercise 6.1.16 (p. 474) that if Ak is the kth leading
principal submatrix of AnÃ—n, then the kth pivot is given by
ukk =
	 det (A1) = a11
for k = 1,
det (Ak)/det (Akâˆ’1)
for k = 2, 3, . . . , n.
Consequently, a symmetric matrix is positive deï¬nite if and only if each of its
leading principal minors is positive. However, if each leading principal minor
is positive, then all principal minors must be positive because if Pk is any
principal submatrix of A, then there is a permutation matrix Q such that

7.6 Positive Deï¬nite Matrices
559
Pk is a leading principal submatrix in C = QT AQ =
 Pk
â‹†
â‹†
â‹†

, and, since
Ïƒ (A) = Ïƒ (C) , we have, with some obvious shorthand notation,
A â€™s leading pmâ€™s > 0 â‡’A pd â‡’C pd â‡’det (Pk) > 0 â‡’all of A â€™s pmâ€™s > 0.
Finally, observe that A is positive deï¬nite if and only if xT Ax > 0 for
every nonzero x âˆˆâ„œnÃ—1. If A is positive deï¬nite, then A = BT B for a
nonsingular B, so xT Ax = xT BT Bx = âˆ¥Bxâˆ¥2
2 â‰¥0 with equality if and only if
Bx = 0 or, equivalently, x = 0. Conversely, if xT Ax > 0 for all x Ì¸= 0, then
for every eigenpair (Î», x) we have Î» = (xT Ax/xT x) > 0.
Below is a formal summary of the results for positive deï¬nite matrices.
Positive Deï¬nite Matrices
For real-symmetric matrices A, the following statements are equivalent,
and any one can serve as the deï¬nition of a positive deï¬nite matrix.
â€¢
xT Ax > 0 for every nonzero x âˆˆâ„œnÃ—1 (most commonly used as
the deï¬nition).
â€¢
All eigenvalues of A are positive.
â€¢
A = BT B for some nonsingular B.
â–·While B is not unique, there is one and only one upper-triangular
matrix R with positive diagonals such that A = RT R. This is
the Cholesky factorization of A (Example 3.10.7, p. 154).
â€¢
A has an LU (or LDU) factorization with all pivots being positive.
â–·The LDU factorization is of the form A = LDLT = RT R, where
R = D1/2LT is the Cholesky factor of A (also see p. 154).
â€¢
The leading principal minors of A are positive.
â€¢
All principal minors of A are positive.
For hermitian matrices, replace (â‹†)T by (â‹†)âˆ—and â„œby C.
Example 7.6.1
Vibrating Beads on a String. Consider n small beads, each having mass
m, spaced at equal intervals of length L on a very tightly stretched string
or wire under a tension T as depicted in Figure 7.6.1. Each bead is initially
displaced from its equilibrium position by a small vertical distanceâ€”say bead k
is displaced by an amount ck at t = 0. The beads are then released so that
they can vibrate freely.

560
Chapter 7
Eigenvalues and Eigenvectors
m
m
L
Equilibrium Position
A Typical Initial Position
Figure 7.6.1
Problem: For small vibrations, determine the position of each bead at time
t > 0 for any given initial conï¬guration.
Solution: The small vibration hypothesis validates the following assumptions.
â€¢
The tension T remains constant for all time.
â€¢
There is only vertical motion (the horizontal forces cancel each other).
â€¢
Only small angles are involved, so the approximation sin Î¸ â‰ˆtan Î¸ is valid.
Let yk(t) = yk be the vertical distance of the kth bead from equilibrium at
time t, and set y0 = 0 = yn+1.
ykâ€“1
yk
yk+1
kâ€“1
k
k+1
Î¸kâ€“1
Î¸k+1
Î¸k
Figure 7.6.2
If Î¸k is the angle depicted in Figure 7.6.2, the diagram above, then the upward
force on the kth bead at time t is Fu = T sin Î¸k, while the downward force is
Fd = T sin Î¸kâˆ’1, so the total force on the kth bead at time t is
F = Fu âˆ’Fd = T(sin Î¸k âˆ’sin Î¸kâˆ’1) â‰ˆT(tan Î¸k âˆ’tan Î¸kâˆ’1)
= T
yk+1 âˆ’yk
L
âˆ’yk âˆ’ykâˆ’1
L

= T
L(ykâˆ’1 âˆ’2yk + yk+1).
Newtonâ€™s second law says force = mass Ã— acceleration, so we set
myâ€²â€²
k = T
L(ykâˆ’1 âˆ’2yk + yk+1)
=â‡’
yâ€²â€²
k + T
mL(âˆ’ykâˆ’1 + 2yk âˆ’yk+1) = 0 (7.6.1)
together with yk(0) = ck and yâ€²
k(0) = 0 to model the motion of the kth
bead. Altogether, equations (7.6.1) represent a system of n second-order linear
diï¬€erential equations, and each is coupled to its neighbors so that no single

7.6 Positive Deï¬nite Matrices
561
equation can be solved in isolation. To extract solutions, the equations must
somehow be uncoupled, and hereâ€™s where matrix diagonalization works its magic.
Write equations (7.6.1) in matrix form as
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
yâ€²â€²
1
yâ€²â€²
2
yâ€²â€²
3
...
yâ€²â€²
n
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
+ T
mL
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
2
âˆ’1
âˆ’1
2
âˆ’1
âˆ’1
2
...
...
...
âˆ’1
âˆ’1
2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
y1
y2
y3
...
yn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
0
0
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
,
or
yâ€²â€² + Ay = 0,
(7.6.2)
with y(0) = c = (c1c2 Â· Â· Â· cn)T and yâ€²(0) = 0. Since A is symmetric, there is
an orthogonal matrix P such that PT AP = D = diag (Î»1, Î»2, . . . , Î»n), where
the Î»i â€™s are the eigenvalues of A. By making the substitution y = Pz (or,
equivalently, by changing the coordinate system), (7.6.2) is transformed into
zâ€²â€² + Dz = 0,
z(0) = PT c = Ëœc,
zâ€²(0) = 0,
or
ï£«
ï£¬
ï£¬
ï£­
zâ€²â€²
1
zâ€²â€²
2
...
zâ€²â€²
n
ï£¶
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£­
Î»1
0
Â· Â· Â·
0
0
Î»2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
Î»n
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
z1
z2
...
zn
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
0
0
...
0
ï£¶
ï£·
ï£·
ï£¸.
In other words, by changing to a coordinate system deï¬ned by a complete set of
orthonormal eigenvectors for A, the original system (7.6.2) is completely uncou-
pled so that each equation zâ€²â€²
k +Î»kzk = 0 with zk(0) = Ëœck and zâ€²
k(0) = 0 can be
solved independently. This helps reveal why diagonalizability is a fundamentally
important concept. Recall from elementary diï¬€erential equations that
zâ€²â€²
k + Î»kzk = 0
=â‡’
zk(t) =
'
Î±ketâˆšâˆ’Î»k + Î²keâˆ’tâˆšâˆ’Î»k
when Î»k < 0,
Î±k cos

t
âˆš
Î»k

+ Î²k sin

t
âˆš
Î»k

when Î»k â‰¥0.
Vibrating beads suggest sinusoidal solutions, so we expect each Î»k > 0. In other
words, the mathematical model would be grossly inconsistent with reality if the
symmetric matrix A in (7.6.2) were not positive deï¬nite. It turns out that A
is positive deï¬nite because there is a Cholesky factorization A = RT R with
R =
*
T
mL
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
r1
âˆ’1/r1
r2
âˆ’1/r2
...
...
rnâˆ’1
âˆ’1/rnâˆ’1
rn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
with
rk =
*
2 âˆ’k âˆ’1
k
,
and thus we are insured that each Î»k > 0. In fact, since A is a tridiagonal
Toeplitz matrix, the results of Example 7.2.5 (p. 514) can be used to show that
Î»k = 2T
mL

1 âˆ’cos
kÏ€
n + 1

= 4T
mL sin2
kÏ€
2(n + 1)
(see Exercise 7.2.18).

562
Chapter 7
Eigenvalues and Eigenvectors
Therefore,
ï£±
ï£²
ï£³
zk
= Î±k cos

t
âˆš
Î»k

+ Î²k sin

t
âˆš
Î»k

zk(0) = Ëœck
zâ€²
k(0) = 0
ï£¼
ï£½
ï£¾
=â‡’
zk = Ëœck cos

t
âˆš
Î»k

, (7.6.3)
and for P =

x1 | x2 | Â· Â· Â· | xn

,
y = Pz = z1x1 + z2x2 + Â· Â· Â· + znxn =
n

j=1

Ëœcj cos

t
âˆš
Î»k

xj.
(7.6.4)
This means that every possible mode of vibration is a combination of modes
determined by the eigenvectors xj. To understand this more clearly, suppose
that the beads are initially positioned according to the components of xj â€”i.e.,
c = y(0) = xj. Then Ëœc = PT c = PT xj = ej, so (7.6.3) and (7.6.4) reduce to
zk =

cos

t
âˆš
Î»k

if k = j
0
if k Ì¸= j
=â‡’
y =

cos

t
âˆš
Î»k

xj.
(7.6.5)
In other words, when y(0) = xj, the jth eigenpair (Î»j, xj) completely deter-
mines the mode of vibration because the amplitudes are determined by xj, and
each bead vibrates with a common frequency f =

Î»j/2Ï€. This type of motion
(7.6.5) is called a normal mode of vibration. In these terms, equation (7.6.4)
translates to say that every possible mode of vibration is a combination of the
normal modes. For example, when n = 3, the matrix in (7.6.2) is
A =
T
mL
ï£«
ï£­
2
âˆ’1
0
âˆ’1
2
âˆ’1
0
âˆ’1
2
ï£¶
ï£¸
with
ï£±
ï£²
ï£³
Î»1 = (T/mL)(2)
Î»2 = (T/mL)(2 âˆ’
âˆš
2)
Î»3 = (T/mL)(2 +
âˆš
2)
ï£¼
ï£½
ï£¾,
and a complete orthonormal set of eigenvectors is
x1 =
1
âˆš
2
ï£«
ï£­
1
0
âˆ’1
ï£¶
ï£¸,
x2 = 1
2
ï£«
ï£­
1
âˆš
2
1
ï£¶
ï£¸,
x3 = 1
2
ï£«
ï£­
1
âˆ’
âˆš
2
1
ï£¶
ï£¸.
The three corresponding normal modes are shown in Figure 7.6.3.
Mode for (Î»1, x1)
Mode for (Î»2, x2)
Mode for (Î»3, x3)
Figure 7.6.3

7.6 Positive Deï¬nite Matrices
563
Example 7.6.2
Discrete Laplacian. According to the laws of physics, the temperature at time
t at a point (x, y, z) in a solid body is a function u(x, y, z, t) satisfying the
diï¬€usion equation
âˆ‚u
âˆ‚t = Kâˆ‡2u,
where
âˆ‡2u = âˆ‚2u
âˆ‚x2 + âˆ‚2u
âˆ‚y2 + âˆ‚2u
âˆ‚z2
is the Laplacian of u and K is a constant of thermal diï¬€usivity. At steady
state the temperature at each point does not vary with time, so âˆ‚u/âˆ‚t = 0 and
u = u(x, y, z) satisfy Laplaceâ€™s equation âˆ‡2u = 0. Solutions of this equation
are often called harmonic functions. The nonhomogeneous equation âˆ‡2u = f
(Poissonâ€™s equation) is addressed in Exercise 7.6.9. To keep things simple, letâ€™s
conï¬ne our attention to the following two-dimensional problem.
Problem: For a square plate as shown in Figure 7.6.4(a), explain how to nu-
merically determine the steady-state temperature at interior grid points when
the temperature around the boundary is prescribed to be u(x, y) = g(x, y) for
a given function g. In other words, explain how to extract a numerical solution
to âˆ‡2u = 0 in the interior of the square when u(x, y) = g(x, y) on the squareâ€™s
boundary. This is called a Dirichlet problem.
76
Solution: Discretize the problem by overlaying the plate with a square mesh
containing n2 interior points at equally spaced intervals of length h. As il-
lustrated in Figure 7.6.4(b) for n = 4, label the grid points using a rowwise
ordering schemeâ€”i.e., label them as you would label matrix entries.
âˆ‡2u = 0 in the interior
u(x, y) = g(x, y) on the boundary
u(x, y) = g(x, y) on the boundary
u(x, y) = g(x, y) on the boundary
u(x, y) = g(x, y) on the boundary
00
01
02
03
04
05
10
11
12
13
14
15
20
21
22
23
24
25
30
31
32
33
34
35
40
41
42
43
44
45
50
51
52
53
54
55
+
,-
.
+
,-
.
h
h
(a)
(b)
Figure 7.6.4
76
Johann Peter Gustav Lejeune Dirichlet (1805â€“1859) held the chair at GÂ¨ottingen previously
occupied by Gauss. Because of his work on the convergence of trigonometric series, Dirichlet
is generally considered to be the founder of the theory of Fourier series, but much of the
groundwork was laid by S. D. Poisson (p. 572) who was Dirichletâ€™s Ph.D. advisor.

564
Chapter 7
Eigenvalues and Eigenvectors
Approximate âˆ‚2u/âˆ‚x2 and âˆ‚2u/âˆ‚y2 at the interior grid points (xi, yj) by using
the second-order centered diï¬€erence formula (1.4.3) developed on p. 19 to write
âˆ‚2u
âˆ‚x2

(xi,yj) = u(xi âˆ’h, yj) âˆ’2u(xi, yj) + u(xi + h, yj)
h2
+ O(h2),
âˆ‚2u
âˆ‚y2

(xi,yj) = u(xi, yj âˆ’h) âˆ’2u(xi, yj) + u(xi, yj + h)
h2
+ O(h2).
(7.6.6)
Adopt the notation uij = u(xi, yj), and add the expressions in (7.6.6) using
âˆ‡2u|(xi,yj) = 0 for interior points (xi, yj) to produce
4uij = (uiâˆ’1,j + ui+1,j + ui,jâˆ’1 + ui,j+1) + O(h4)
for
i, j = 1, 2, . . . , n.
In other words, the steady-state temperature at an interior grid point is approxi-
mately the average of the steady-state temperatures at the four neighboring grid
points as illustrated in Figure 7.6.5.
ij
i âˆ’1, j
i + 1, j
i, j + 1
i, j âˆ’1
uij = uiâˆ’1,j + ui+1,j + ui,jâˆ’1 + ui,j+1
4
+ O(h4)
Figure 7.6.5
If the O(h4) terms are neglected, the resulting ï¬ve-point diï¬€erence equations,
4uij âˆ’(uiâˆ’1,j + ui+1,j + ui,jâˆ’1 + ui,j+1) = 0
for
i, j = 1, 2, . . . , n,
constitute an n2 Ã— n2 linear system Lu = g in which the unknowns are the
uijâ€™s, and the right-hand side contains boundary values. For example, a mesh
with nine interior points produces the 9 Ã— 9 system in Figure 7.6.6.
00
01
02
03
04
10
11
12
13
14
20
21
22
23
24
30
31
32
33
34
40
41
42
43
44
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
4
âˆ’1
0
âˆ’1
0
0
0
0
0
âˆ’1
4
âˆ’1
0
âˆ’1
0
0
0
0
0
âˆ’1
4
0
0
âˆ’1
0
0
0
âˆ’1
0
0
4
âˆ’1
0
âˆ’1
0
0
0
âˆ’1
0
âˆ’1
4
âˆ’1
0
âˆ’1
0
0
0
âˆ’1
0
âˆ’1
4
0
0
âˆ’1
0
0
0
âˆ’1
0
0
4
âˆ’1
0
0
0
0
0
âˆ’1
0
âˆ’1
4
âˆ’1
0
0
0
0
0
âˆ’1
0
âˆ’1
4
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
u11
u12
u13
u21
u22
u23
u31
u32
u33
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
g01 + g10
g02
g03 + g14
g20
0
g24
g30 + g41
g42
g43 + g34
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
Figure 7.6.6

7.6 Positive Deï¬nite Matrices
565
The coeï¬ƒcient matrix of this system is the discrete Laplacian, and in general
it has the symmetric block-tridiagonal form
L=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
T
âˆ’I
âˆ’I
T
âˆ’I
...
...
...
âˆ’I
T
âˆ’I
âˆ’I
T
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
n2Ã—n2
with
T=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
4
âˆ’1
âˆ’1
4
âˆ’1
...
...
...
âˆ’1
4
âˆ’1
âˆ’1
4
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
nÃ—n
.
In addition, L is positive deï¬nite. In fact, the discrete Laplacian is a primary
example of how positive deï¬nite matrices arise in practice. Note that L is the
two-dimensional version of the one-dimensional ï¬nite-diï¬€erence matrix in Exam-
ple 1.4.1 (p. 19).
Problem: Show L is positive deï¬nite by explicitly exhibiting its eigenvalues.
Solution: Example 7.2.5 (p. 514) insures that the n eigenvalues of T are
Î»i = 4 âˆ’2 cos

iÏ€
n + 1

,
i = 1, 2, . . . , n.
(7.6.7)
If U is an orthogonal matrix such that UT TU = D = diag (Î»1, Î»2, . . . , Î»n) ,
and if B is the n2 Ã— n2 block-diagonal orthogonal matrix
B =
ï£«
ï£¬
ï£¬
ï£­
U
0
Â· Â· Â·
0
0
U
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
U
ï£¶
ï£·
ï£·
ï£¸,
then
BT LB = ËœL =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
D
âˆ’I
âˆ’I
D
âˆ’I
...
...
...
âˆ’I
D
âˆ’I
âˆ’I
D
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Consider the permutation obtained by placing the numbers 1, 2, . . . , n2 rowwise
in a square matrix, and then reordering them by listing the entries columnwise.
For example, when n = 3 this permutation is generated as follows:
v = (1, 2, 3, 4, 5, 6, 7, 8, 9) â†’A =
ï£«
ï£­
1
2
3
4
5
6
7
8
9
ï£¶
ï£¸â†’(1, 4, 7, 2, 5, 8, 3, 6, 9) = Ëœv.
Equivalently, this can be described in terms of wrapping and unwrapping rows by
writing v
wrap
âˆ’âˆ’âˆ’â†’A âˆ’â†’AT unwrap
âˆ’âˆ’âˆ’âˆ’â†’Ëœv. If P is the associated n2 Ã— n2 permutation
matrix, then
PT ËœLP=
ï£«
ï£¬
ï£¬
ï£­
T1
0
Â· Â· Â·
0
0
T2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
Tn
ï£¶
ï£·
ï£·
ï£¸
with
Ti =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î»i
âˆ’1
âˆ’1
Î»i
âˆ’1
...
...
...
âˆ’1
Î»i
âˆ’1
âˆ’1
Î»i
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
nÃ—n
.

566
Chapter 7
Eigenvalues and Eigenvectors
If you try it on the 9 Ã— 9 case, you will see why it works. Now, Ti is another
tridiagonal Toeplitz matrix, so Example 7.2.5 (p. 514) again applies to yield
Ïƒ (Ti) = {Î»i âˆ’2 cos (jÏ€/n + 1) , j = 1, 2, . . . , n} . This together with (7.6.7) pro-
duces the n2 eigenvalues of L as
Î»ij = 4 âˆ’2
%
cos

iÏ€
n + 1

+ cos
 jÏ€
n + 1
&
,
i, j = 1, 2, . . . , n,
or, by using the identity 1 âˆ’cos Î¸ = 2 sin2(Î¸/2),
Î»ij = 4
%
sin2

iÏ€
2(n + 1)

+ sin2

jÏ€
2(n + 1)
&
,
i, j = 1, 2, . . . , n.
(7.6.8)
Since each Î»ij is positive, L must be positive deï¬nite. As a corollary, L is
nonsingular, and hence Lu = g yields a unique solution for the steady-state
temperatures on the square plate (otherwise something would be amiss).
At ï¬rst glance itâ€™s tempting to think that statements about positive deï¬nite
matrices translate to positive semideï¬nite matrices simply by replacing the word
â€œpositiveâ€ by â€œnonnegative,â€ but this is not always true. When A has zero
eigenvalues (i.e., when A is singular) there is no LU factorization, and, unlike the
positive deï¬nite case, having nonnegative leading principal minors doesnâ€™t insure
that A is positive semideï¬niteâ€”e.g., consider A =
 0
0
0
âˆ’1

. The positive
deï¬nite properties that have semideï¬nite analogues are listed below.
Positive Semideï¬nite Matrices
For real-symmetric matrices such that rank (AnÃ—n) = r, the following
statements are equivalent, so any one of them can serve as the deï¬nition
of a positive semideï¬nite matrix.
â€¢
xT Ax â‰¥0 for all x âˆˆâ„œnÃ—1 (the most common deï¬nition). (7.6.9)
â€¢
All eigenvalues of A are nonnegative.
(7.6.10)
â€¢
A = BT B for some B with rank (B) = r.
(7.6.11)
â€¢
All principal minors of A are nonnegative.
(7.6.12)
For hermitian matrices, replace (â‹†)T by (â‹†)âˆ—and â„œby C.
Proof of (7.6.9) =â‡’(7.6.10). The hypothesis insures xT Ax â‰¥0 for eigenvectors
of A. If (Î», x) is an eigenpair, then Î» = xT Ax/xT x = âˆ¥Bxâˆ¥2
2 / âˆ¥xâˆ¥2
2 â‰¥0.
Proof of (7.6.10) =â‡’(7.6.11). Similar to the positive deï¬nite case, if each Î»i â‰¥0,
write A = PD1/2D1/2PT = BT B, where B = D1/2PT has rank r.

7.6 Positive Deï¬nite Matrices
567
Proof of (7.6.11) =â‡’(7.6.12). If Pk is a principal submatrix of A, then

Pk
â‹†
â‹†
â‹†

= QT AQ = QT BT BQ =

FT
â‹†
 
F | â‹†

=â‡’
Pk = FT F
for a permutation matrix Q. Thus det (Pk) = det

FT F

â‰¥0 (Exercise 6.1.10).
Proof of (7.6.12) =â‡’(7.6.9). If Ak is the leading k Ã— k principal submatrix
of A, and if {Âµ1, Âµ2, . . . , Âµk} are the eigenvalues (including repetitions) of Ak,
then ÏµI + Ak has eigenvalues {Ïµ + Âµ1, Ïµ + Âµ2, . . . , Ïµ + Âµk}, so, for every Ïµ > 0,
det (ÏµI + Ak) = (Ïµ + Âµ1)(Ïµ + Âµ2) Â· Â· Â· (Ïµ + Âµk) = Ïµk + s1Ïµkâˆ’1 + Â· Â· Â· + Ïµskâˆ’1 + sk > 0
because sj is the jth symmetric function of the Âµi â€™s (p. 494), and, by (7.1.6),
sj is the sum of the j Ã— j principal minors of Ak, which are principal minors
of A. In other words, each leading principal minor of ÏµI + A is positive, so
ÏµI+A is positive deï¬nite by the results on p. 559. Consequently, for each nonzero
x âˆˆâ„œnÃ—1, we must have xT (ÏµI + A)x > 0 for every Ïµ > 0. Let Ïµ â†’0+ (i.e.,
through positive values) to conclude that xT Ax â‰¥0 for each x âˆˆâ„œnÃ—1.
Quadratic Forms
For a vector x âˆˆâ„œnÃ—1 and a matrix A âˆˆâ„œnÃ—n, the scalar function
deï¬ned by
f(x) = xT Ax =
n

i=1
n

j=1
aijxixj
(7.6.13)
is called a quadratic form. A quadratic form is said to be positive def-
inite whenever A is a positive deï¬nite matrix. In other words, (7.6.13)
is a positive deï¬nite form if and only if f(x) > 0 for all 0 Ì¸= x âˆˆâ„œnÃ—1.
Because xT Ax = xT 
(A+AT )/2

x, and because (A+AT )/2 is symmet-
ric, the matrix of a quadratic form can always be forced to be symmetric. For
this reason it is assumed that the matrix of every quadratic form is symmetric.
When x âˆˆCnÃ—1,
A âˆˆCnÃ—n, and A is hermitian, the expression xHAx is
known as a complex quadratic form.
Example 7.6.3
Diagonalization of a Quadratic Form. A quadratic form f(x) = xT Dx
is said to be a diagonal form whenever DnÃ—n is a diagonal matrix, in which
case xT Dx = n
i=1 diix2
i (there are no cross-product terms). Every quadratic
form xT Ax can be diagonalized by making a change of variables (coordinates)

568
Chapter 7
Eigenvalues and Eigenvectors
y = QT x. This follows because A is symmetric, so there is an orthogonal ma-
trix Q such that QT AQ = D = diag (Î»1, Î»2, . . . , Î»n) , where Î»i âˆˆÏƒ (A) , and
setting y = QT x (or, equivalently, x = Qy) gives
xT Ax = yT QT AQy = yT Dy =
n

i=1
Î»iy2
i .
(7.6.14)
This shows that the nature of the quadratic form is determined by the eigenvalues
of A (which are necessarily real). The eï¬€ect of diagonalizing a quadratic form in
this way is to rotate the standard coordinate system so that in the new coordinate
system the graph of xT Ax = Î± is in â€œstandard form.â€ If A is positive deï¬nite,
then all of its eigenvalues are positive (p. 559), so (7.6.14) makes it clear that the
graph of xT Ax = Î± for a constant Î± > 0 is an ellipsoid centered at the origin.
Go back and look at Figure 7.2.1 (p. 505), and see Exercise 7.6.4 (p. 571).
Example 7.6.4
Congruence. Itâ€™s not necessary to solve an eigenvalue problem to diagonalize
a quadratic form because a congruence transformation CT AC in which C
is nonsingular (but not necessarily orthogonal) can be found that will do the
job. A particularly convenient congruence transformation is produced by the
LDU factorization for A, which is A = LDLT because A is symmetricâ€”see
Exercise 3.10.9 (p. 157). This factorization is relatively cheap, and the diagonal
entries in D = diag (p1, p2, . . . , pn) are the pivots that emerge during Gaussian
elimination (p. 154). Setting y = LT x (or, equivalently, x = (LT )âˆ’1y) yields
xT Ax = yT Dy =
n

i=1
piy2
i .
The inertia of a real-symmetric matrix A is deï¬ned to be the triple (Ï, Î½, Î¶)
in which Ï,
Î½, and Î¶ are the respective number of positive, negative, and
zero eigenvalues, counting algebraic multiplicities. In 1852 J. J. Sylvester (p. 80)
discovered that the inertia of A is invariant under congruence transformations.
Sylvesterâ€™s Law of Inertia
Let A âˆ¼= B denote the fact that real-symmetric matrices A and B
are congruent (i.e., CT AC = B for some nonsingular C). Sylvesterâ€™s
law of inertia states that:
A âˆ¼= B
if and only if A and B have the same inertia.

7.6 Positive Deï¬nite Matrices
569
Proof.
77 Observe that if AnÃ—n is real and symmetric with inertia (p, j, s), then
A âˆ¼=
ï£«
ï£­
IpÃ—p
âˆ’IjÃ—j
0sÃ—s
ï£¶
ï£¸= E,
(7.6.15)
because if {Î»1, . . . , Î»p, âˆ’Î»p+1, . . . , âˆ’Î»p+j, 0, . . . , 0} are the eigenvalues of A
(counting multiplicities) with each Î»i > 0, there is an orthogonal matrix P
such that PT AP = diag (Î»1, . . . , Î»p, âˆ’Î»p+1, . . . , âˆ’Î»p+j, 0, . . . , 0) , so C = PD,
where D = diag

Î»âˆ’1/2
1
, . . . , Î»âˆ’1/2
p+j , 1, . . . , 1

, is nonsingular and CT AC = E.
Let B be a real-symmetric matrix with inertia (q, k, t) so that
B âˆ¼=
ï£«
ï£­
IqÃ—q
âˆ’IkÃ—k
0tÃ—t
ï£¶
ï£¸= F.
If B âˆ¼= A, then F âˆ¼= E (congruence is transitive), so rank (F) = rank (E), and
hence s = t. To show that p = q, assume to the contrary that p > q, and write
F = KT EK for some nonsingular K =

XnÃ—q | YnÃ—nâˆ’q

. If M = R (Y) âŠ†â„œn
and N = span {e1, . . . , ep} âŠ†â„œn, then using the formula (4.4.19) for the dimen-
sion of a sum (p. 205) yields
dim(Mâˆ©N) = dim M+dim N âˆ’dim(M+N) = (nâˆ’q)+pâˆ’dim(M+N) > 0.
Consequently, there exists a nonzero vector x âˆˆM âˆ©N. For such a vector,
x âˆˆM
=â‡’
x = Yy = K

0
y

=â‡’
xT Ex =

0T | yT 
F

0
y

â‰¤0,
and
x âˆˆN
=â‡’
x = (x1, . . . , xp, 0, . . . , 0)T
=â‡’
xT Ex > 0,
which is impossible. Therefore, we canâ€™t have p > q. A similar argument shows
that itâ€™s also impossible to have p < q, so p = q. Thus it is proved that if
A âˆ¼= B, then A and B have the same inertia. Conversely, if A and B have in-
ertia (p, j, s), then the argument that produced (7.6.15) yields A âˆ¼= E âˆ¼= B.
77
The fact that inertia is invariant under congruence is also a corollary of a deeper theo-
rem stating that the eigenvalues of A vary continuously with the entries. The argument
is as follows. Assume A is nonsingular (otherwise consider A + ÏµI for small Ïµ), and set
X(t) = tQ + (1 âˆ’t)QR for t âˆˆ[0, 1], where C = QR is the QR factorization. Both X(t)
and Y(t) = XT (t)AX(t) are nonsingular on [0, 1], so continuity of eigenvalues insures that
no eigenvalue Y(t) can cross the origin as t goes from 0 to 1. Hence Y(0) = CT AC has
the same number of positive (and negative) eigenvalues as Y(1) = QT AQ, which is similar
to A. Thus CT AC and A have the same inertia.

570
Chapter 7
Eigenvalues and Eigenvectors
Example 7.6.5
Taylorâ€™s theorem in â„œn says that if f is a smooth real-valued function deï¬ned
on â„œn, and if x0 âˆˆâ„œnÃ—1, then the value of f at x âˆˆâ„œnÃ—1 is given by
f(x) = f(x0) + (x âˆ’x0)T g(x0) + (x âˆ’x0)T H(x0)(x âˆ’x0) + O(âˆ¥x âˆ’x0âˆ¥3),
where g(x0) = âˆ‡f(x0) (the gradient of f evaluated at x0) has components
gi = âˆ‚f/âˆ‚xi

x0
, and where H(x0) is the Hessian matrix whose entries are
given by hij = âˆ‚2f/âˆ‚xiâˆ‚xj

x0
. Just as in the case of one variable, the vector
x0 is called a critical point when g(x0) = 0. If x0 is a critical point, then
Taylorâ€™s theorem shows that (x âˆ’x0)T H(x0)(x âˆ’x0) governs the behavior of
f at points x near to x0. This observation yields the following conclusions
regarding local maxima or minima.
â€¢
If x0 is a critical point such that H(x0) is positive deï¬nite, then f has a
local minimum at x0.
â€¢
If x0 is a critical point such that H(x0) is negative deï¬nite (i.e., zT Hz < 0
for all z Ì¸= 0 or, equivalently, âˆ’H is positive deï¬nite), then f has a local
maximum at x0.
Exercises for section 7.6
7.6.1. Which of the following matrices are positive deï¬nite?
A =
ï£«
ï£­
1
âˆ’1
âˆ’1
âˆ’1
5
1
âˆ’1
1
5
ï£¶
ï£¸.
B =
ï£«
ï£­
20
6
8
6
3
0
8
0
8
ï£¶
ï£¸.
C =
ï£«
ï£­
2
0
2
0
6
2
2
2
4
ï£¶
ï£¸.
7.6.2. Spring-Mass Vibrations.
Two masses m1 and m2 are suspended
between three identical springs (with spring constant k) as shown in
Figure 7.6.7. Each mass is initially displaced from its equilibrium posi-
tion by a horizontal distance and released to vibrate freely (assume there
is no vertical displacement).
m1
m2
m1
m2
x1
x2
Figure 7.6.7

7.6 Positive Deï¬nite Matrices
571
(a) If xi(t) denotes the horizontal displacement of mi from equilibrium at
time t, show that Mxâ€²â€² = Kx, where
M =

m1
0
0
m2

,
x =

x1(t)
x2(t)

,
and
K = k

2
âˆ’1
âˆ’1
2

.
(Consider a force directed to the left to be positive.) Notice that the
mass-stiï¬€ness equation Mxâ€²â€² = Kx is the matrix version of Hookeâ€™s
law F = kx, and K is positive deï¬nite.
(b) Look for a solution of the form x = eiÎ¸tv for a constant vector v, and
show that this reduces the problem to solving an algebraic equation of
the form Kv = Î»Mv (for Î» = âˆ’Î¸2). This is called a generalized
eigenvalue problem because when M = I we are back to the ordi-
nary eigenvalue problem. The generalized eigenvalues Î»1 and Î»2 are
the roots of the equation det (K âˆ’Î»M) = 0â€”ï¬nd them when k = 1,
m1 = 1, and m2 = 2, and describe the two modes of vibration.
(c) Take m1 = m2 = m, and apply the technique used in the vibrating
beads problem in Example 7.6.1 (p. 559) to determine the normal modes.
Compare the results with those of part (b).
7.6.3. Three masses m1, m2, and m3 are suspended on three identical springs
(with spring constant k) as shown below. Each mass is initially displaced
from its equilibrium position by a vertical distance and then released to
vibrate freely.
(a)
If yi(t) denotes the displacement of mi from equilibrium
at time t, show that the mass-stiï¬€ness equation is Myâ€²â€² = Ky,
where
M=
ï£«
ï£­
m1
0
0
0
m2
0
0
0
m3
ï£¶
ï£¸, y=
ï£«
ï£­
y1(t)
y2(t)
y3(t)
ï£¶
ï£¸, K=k
ï£«
ï£­
2
âˆ’1
0
âˆ’1
2
âˆ’1
0
âˆ’1
1
ï£¶
ï£¸
(k33 = 1 is not a mistake!).
(b)
Show that K is positive deï¬nite.
(c)
Find the normal modes when m1 = m2 = m3 = m.
7.6.4. By diagonalizing the quadratic form 13x2 + 10xy + 13y2, show that the
rotated graph of 13x2 +10xy +13y2 = 72 is an ellipse in standard form
as shown in Figure 7.2.1 on p. 505.
7.6.5. Suppose that A is a real-symmetric matrix. Explain why the signs of
the pivots in the LDU factorization for A reveal the inertia of A.

572
Chapter 7
Eigenvalues and Eigenvectors
7.6.6. Consider the quadratic form
f(x) = 1
9(âˆ’2x2
1 + 7x2
2 + 4x2
3 + 4x1x2 + 16x1x3 + 20x2x3).
(a)
Find a symmetric matrix A so that f(x) = xT Ax.
(b)
Diagonalize the quadratic form using the LDLT factorization
as described in Example 7.6.4, and determine the inertia of A.
(c)
Is this a positive deï¬nite form?
(d)
Verify the inertia obtained above is correct by computing the
eigenvalues of A.
(e)
Verify Sylvesterâ€™s law of inertia by making up a congruence
transformation C and then computing the inertia of CT AC.
7.6.7. Polar Factorization. Explain why each nonsingular A âˆˆCnÃ—n can be
uniquely factored as A = RU, where R is hermitian positive deï¬nite
and U is unitary. This is the matrix analog of the polar form of a
complex number z = reiÎ¸,
r > 0, because 1 Ã— 1 hermitian positive
deï¬nite matrices are positive real numbers, and 1 Ã— 1 unitary matrices
are points on the unit circle. Hint: First explain why R = (AAâˆ—)1/2.
7.6.8. Explain why trying to produce better approximations to the solution
of the Dirichlet problem in Example 7.6.2 by using ï¬ner meshes with
more grid points results in an increasingly ill-conditioned linear system
Lu = g.
7.6.9. For a given function f the equation âˆ‡2u = f is called Poissonâ€™s
78
equation. Consider Poissonâ€™s equation on a square in two dimensions
with Dirichlet boundary conditions. That is,
âˆ‚2u
âˆ‚x2 + âˆ‚2u
âˆ‚y2 = f(x, y)
with
u(x, y) = g(x, y)
on the boundary.
78
SimÂ´eon Denis Poisson (1781â€“1840) was a proliï¬c French scientist who was originally encouraged
to study medicine but was seduced by mathematics. While he was still a teenager, his work
attracted the attention of the reigning scientiï¬c elite of France such as Legendre, Laplace, and
Lagrange. The latter two were originally his teachers (Lagrange was his thesis director) at the
Â´Ecole Polytechnique, but they eventually became his friends and collaborators. It is estimated
that Poisson published about 400 scientiï¬c articles, and his 1811 book TraitÂ´e de mÂ´ecanique was
the standard reference for mechanics for many years. Poisson began his career as an astronomer,
but he is primarily remembered for his impact on applied areas such as mechanics, probability,
electricity and magnetism, and Fourier series. This seems ironic because he held the chair of
â€œpure mathematicsâ€ in the FacultÂ´e des Sciences. The next time you ï¬nd yourself on the streets
of Paris, take a stroll on the Rue Denis Poisson, or you can check out Poissonâ€™s plaque, along
with those of Lagrange, Laplace, and Legendre, on the ï¬rst stage of the Eiï¬€el Tower.

7.6 Positive Deï¬nite Matrices
573
Discretize the problem by overlaying the square with a regular mesh con-
taining n2 interior points at equally spaced intervals of length h as ex-
plained in Example 7.6.2 (p. 563). Let fij = f(xi, yj), and deï¬ne f to be
the vector f = (f11, f12, . . . , f1n|f21, f22 . . . , f2n| Â· Â· Â· |fn1, fn2, . . . , fnn)T .
Show that the discretization of Poissonâ€™s equation produces a system
of linear equations of the form Lu = g âˆ’h2f, where L is the discrete
Laplacian and where u and g are as described in Example 7.6.2.
7.6.10. As deï¬ned in Exercise 5.8.15 (p. 380) and discussed in Exercise 7.8.11
(p. 597) the Kronecker product (sometimes called tensor product, or
direct product) of matrices AmÃ—n and BpÃ—q is the mp Ã— nq matrix
A âŠ—B =
ï£«
ï£¬
ï£¬
ï£­
a11B
a12B
Â· Â· Â·
a1nB
a21B
a22B
Â· Â· Â·
a2nB
...
...
...
...
am1B
am2B
Â· Â· Â·
amnB
ï£¶
ï£·
ï£·
ï£¸.
Verify that if In is the n Ã— n identity matrix, and if
An =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
2
âˆ’1
âˆ’1
2
âˆ’1
...
...
...
âˆ’1
2
âˆ’1
âˆ’1
2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
nÃ—n
is the nth-order ï¬nite diï¬€erence matrix of Example 1.4.1 (p. 19), then
the discrete Laplacian is given by
Ln2Ã—n2 = (In âŠ—An) + (An âŠ—In).
Thus we have an elegant matrix connection between the ï¬nite diï¬€erence
approximations of the one-dimensional and two-dimensional Laplacians.
This formula leads to a simple alternate derivation of (7.6.8)â€”see Exer-
cise 7.8.12 (p. 598). As you might guess, the discrete three-dimensional
Laplacian is
Ln3Ã—n3 = (In âŠ—In âŠ—An) + (In âŠ—An âŠ—In) + (An âŠ—In âŠ—In).

574
Chapter 7
Eigenvalues and Eigenvectors
7.7
NILPOTENT MATRICES AND JORDAN STRUCTURE
While itâ€™s not always possible to diagonalize a matrix A âˆˆCmÃ—m with a similar-
ity transformation, Schurâ€™s theorem (p. 508) guarantees that every A âˆˆCmÃ—m
is unitarily similar to an upper-triangular matrixâ€”say Uâˆ—AU = T. But other
than the fact that the diagonal entries of T are the eigenvalues of A, there is
no pattern to the nonzero part of T. So to what extent can this be remedied by
giving up the unitary nature of U? In other words, is there a nonunitary P for
which Pâˆ’1AP has a simpler and more predictable pattern than that of T? We
have already made the ï¬rst step in answering this question. The core-nilpotent
decomposition (p. 397) says that for every singular matrix A of index k and
rank r, there is a nonsingular matrix Q such that
Qâˆ’1AQ =

CrÃ—r
0
0
L

, where rank (C) = r and L is nilpotent of index k.
Consequently, any further simpliï¬cation by means of similarity transformations
can revolve around C and L. Letâ€™s begin by examining the degree to which
nilpotent matrices can be reduced by similarity transformations.
In what follows, let LnÃ—n be a nilpotent matrix of index k so that Lk = 0
but Lkâˆ’1 Ì¸= 0. The ï¬rst question is, â€œCan L be diagonalized by a similarity
transformation?â€ To answer this, notice that Î» = 0 is the only eigenvalue of L
because
Lx = Î»x
=â‡’
Lkx = Î»kx
=â‡’
0 = Î»kx
=â‡’
Î» = 0
(since x Ì¸= 0 ).
So if L is to be diagonalized by a similarity transformation, it must be the case
that Pâˆ’1LP = D = 0 (diagonal entries of D must be eigenvalues of L ), and
this forces L = 0. In other words, the only nilpotent matrix that is similar to a
diagonal matrix is the zero matrix.
Assume L Ì¸= 0 from now on so that L is not diagonalizable. Since L
can always be triangularized (Schurâ€™s theorem again), our problem boils down
to ï¬nding a nonsingular P such that Pâˆ’1LP is an upper-triangular matrix
possessing a simple and predictable form. This turns out to be a fundamental
problem, and the rest of this section is devoted to its solution. But before diving
in, letâ€™s set the stage by thinking about some possibilities.
If Pâˆ’1LP = T is upper triangular, then the diagonal entries of T must
be the eigenvalues of L, so T must have the form
T =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
â‹†
Â· Â· Â·
â‹†
...
...
...
...
â‹†
0
ï£¶
ï£·
ï£·
ï£·
ï£¸.

7.7 Nilpotent Matrices and Jordan Structure
575
One way to simplify the form of T is to allow nonzero entries only on the
superdiagonal (the diagonal immediately above the main diagonal) of T, so we
might try to construct a nonsingular P such that T has the form
T =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
â‹†
...
...
...
â‹†
0
ï£¶
ï£·
ï£·
ï£·
ï£¸.
To gain some insight on how this might be accomplished, let L be a 3 Ã— 3
nilpotent matrix for which L3 = 0 and L2 Ì¸= 0, and search for a P such that
Pâˆ’1LP =
ï£«
ï£­
0
1
0
0
0
1
0
0
0
ï£¶
ï£¸â‡â‡’L[ Pâˆ—1 Pâˆ—2 Pâˆ—3 ] = [ Pâˆ—1 Pâˆ—2 Pâˆ—3 ]
ï£«
ï£­
0
1
0
0
0
1
0
0
0
ï£¶
ï£¸
â‡â‡’LPâˆ—1 = 0,
LPâˆ—2 = Pâˆ—1,
LPâˆ—3 = Pâˆ—2.
Since L3 = 0, we can set Pâˆ—1 = L2x for any x3Ã—1 such that L2x Ì¸= 0. This
in turn allows us to set Pâˆ—2 = Lx and Pâˆ—3 = x. Because J = {L2x, Lx, x}
is a linearly independent set (Exercise 5.10.8), P = [ L2x | Lx | x ] will do the
job. J is called a Jordan chain, and it is characterized by the fact that its
ï¬rst vector is a somewhat special eigenvector for L while the other vectors are
built (or â€œchainedâ€) on top of this eigenvector to form a special basis for C3.
There are a few more wrinkles in the development of a general theory for n Ã— n
nilpotent matrices, but the features illustrated here illuminate the path.
For a general nilpotent matrix LnÃ—n Ì¸= 0 of index k, we know that Î» = 0 is
the only eigenvalue, so the set of eigenvectors of L is N (L) (excluding the zero
vector of course). Realizing that L is not diagonalizable is equivalent to realizing
that L does not possess a complete linearly independent set of eigenvectors or,
equivalently, dim N (L) < n. As in the 3 Ã— 3 example above, the strategy for
building a similarity transformation P that reduces L to a simple triangular
form is as follows.
(1)
Construct a somewhat special basis B for N (L).
(2)
Extend B to a basis for Cn by building Jordan chains on top of the
eigenvectors in B.
To accomplish (1), consider the subspaces deï¬ned by
Mi = R

Li
âˆ©N (L)
for i = 0, 1, . . . , k,
(7.7.1)
and notice (Exercise 7.7.4) that these subspaces are nested as
0 = Mk âŠ†Mkâˆ’1 âŠ†Mkâˆ’2 âŠ†Â· Â· Â· âŠ†M1 âŠ†M0 = N (L).

576
Chapter 7
Eigenvalues and Eigenvectors
Use these nested spaces to construct a basis for N (L) = M0 by starting with
any basis Skâˆ’1 for Mkâˆ’1 and by sequentially extending Skâˆ’1 with addi-
tional sets Skâˆ’2, Skâˆ’3, . . . , S0 such that Skâˆ’1 âˆªSkâˆ’2 is a basis for Mkâˆ’2,
Skâˆ’1 âˆªSkâˆ’2 âˆªSkâˆ’3 is a basis for Mkâˆ’3, etc. In general, Si is a set of vectors
that extends Skâˆ’1 âˆªSkâˆ’2 âˆªÂ· Â· Â· âˆªSiâˆ’1 to a basis for Mi. Figure 7.7.1 is a
heuristic diagram depicting an example of k = 5 nested subspaces Mi along
with some typical extension sets Si that combine to form a basis for N (L).
Figure 7.7.1
Now extend the basis B = Skâˆ’1 âˆªSkâˆ’2 âˆªÂ· Â· Â· âˆªS0 = {b1, b2, . . . , bt} for
N (L) to a basis for Cn by building Jordan chains on top of each b âˆˆB. If
b âˆˆSi, then there exists a vector x such that Lix = b because each b âˆˆSi
belongs to Mi = R

Li
âˆ©N (L) âŠ†R

Li
. A Jordan chain is built on top of
each b âˆˆSi by solving the system Lix = b for x and by setting
Jb = {Lix, Liâˆ’1x, . . . , Lx, x}.
(7.7.2)
Notice that chains built on top of vectors from Si each have length i + 1. The
heuristic diagram in Figure 7.7.2 depicts Jordan chains built on top of the basis
vectors illustrated in Figure 7.7.1â€”the chain that is labeled is built on top of a
vector b âˆˆS3.
Figure 7.7.2

7.7 Nilpotent Matrices and Jordan Structure
577
The collection of vectors in all of these Jordan chains is a basis for Cn.
To demonstrate this, ï¬rst it must be argued that the total number of vectors
in all Jordan chains is n, and then it must be proven that this collection is a
linearly independent set. To count the number of vectors in all Jordan chains
Jb, ï¬rst recall from (4.5.1) that the rank of a product is given by the formula
rank (AB) = rank (B) âˆ’dim N (A) âˆ©R (B), and apply this to conclude that
dim Mi = dim R

Li
âˆ©N (L) = rank

Li
âˆ’rank

LLi
. In other words, if we
set di = dim Mi and ri = rank

Li
, then
di = dim Mi = rank

Li
âˆ’rank

Li+1
= ri âˆ’ri+1,
(7.7.3)
so the number of vectors in Si is
Î½i = di âˆ’di+1 = ri âˆ’2ri+1 + ri+2.
(7.7.4)
Since every chain emanating from a vector in Si contains i + 1 vectors, and
since dk = 0 = rk, the total number of vectors in all Jordan chains is
total =
kâˆ’1

i=0
(i + 1)Î½i =
kâˆ’1

i=0
(i + 1)(di âˆ’di+1)
= d0 âˆ’d1 + 2(d1 âˆ’d2) + 3(d2 âˆ’d3) + Â· Â· Â· + k(dkâˆ’1 âˆ’dk)
= d0 + d1 + Â· Â· Â· + dkâˆ’1
= (r0 âˆ’r1) + (r1 âˆ’r2) + (r2 âˆ’r3) + Â· Â· Â· + (rkâˆ’1 âˆ’rk)
= r0 = n.
To prove that the set of all vectors from all Jordan chains is linearly independent,
place these vectors as columns in a matrix QnÃ—n and show that N (Q) = 0.
The trick in doing so is to arrange the vectors from the Jb â€™s in just the right
order. Begin by placing the vectors at the top level in chains emanating from Si
as columns in a matrix Xi as depicted in the heuristic diagram in Figure 7.7.3.
Figure 7.7.3

578
Chapter 7
Eigenvalues and Eigenvectors
The matrix LXi contains all vectors at the second highest level of those chains
emanating from Si, while L2Xi contains all vectors at the third highest level
of those chains emanating from Si, and so on. In general, LjXi contains all
vectors at the (j+1)st highest level of those chains emanating from Si. Proceed
by ï¬lling in Q = [ Q0 | Q1 | Â· Â· Â· | Qkâˆ’1 ] from the bottom up by letting Qj be
the matrix whose columns are all vectors at the jth level from the bottom in all
chains. For the example illustrated in Figures 7.7.1â€“7.7.3 with k = 5,
Q0 = [ X0 | LX1 | L2X2 | L3X3 | L4X4 ] = vectors at level 0 = basis B for N (L),
Q1 = [ X1 | LX2 | L2X3 | L3X4 ] = vectors at level 1 (from the bottom),
Q2 = [ X2 | LX3 | L2X4 ] = vectors at level 2 (from the bottom),
Q3 = [ X3 | LX4 ] = vectors at level 3 (from the bottom),
Q4 = [ X4 ] = vectors at level 4 (from the bottom).
In general, Qj = [ Xj | LXj+1 | L2Xj+2 | Â· Â· Â· | Lkâˆ’1âˆ’jXkâˆ’1 ]. Since the columns
of LjXj are all on the bottom level (level 0), they are part of the basis B for
N (L). This means that the columns of LjQj are also part of the basis B for
N (L), so they are linearly independent, and thus N

LjQj

= 0. Furthermore,
since the columns of LjQj are in N (L), we have L

LjQj

= 0, and hence
Lj+hQj = 0 for all h â‰¥1. Now use these observations to prove N (Q) = 0. If
Qz = 0, then multiplication by Lkâˆ’1 yields
0 = Lkâˆ’1Qz = [ Lkâˆ’1Q0 | Lkâˆ’1Q1 | Â· Â· Â· | Lkâˆ’1Qkâˆ’1 ] z
= [ 0 | 0 | Â· Â· Â· | Lkâˆ’1Qkâˆ’1 ]
ï£«
ï£¬
ï£¬
ï£­
z0
z2
...
zkâˆ’1
ï£¶
ï£·
ï£·
ï£¸
=â‡’
zkâˆ’1 âˆˆN

Lkâˆ’1Qkâˆ’1

=â‡’
zkâˆ’1 = 0.
This conclusion with the same argument applied to 0 = Lkâˆ’2Qz produces
zkâˆ’2 = 0. Similar repetitions show that zi = 0 for each i, and thus N (Q) = 0.
It has now been proven that if B = Skâˆ’1 âˆªSkâˆ’2 âˆªÂ· Â· Â·âˆªS0 = {b1, b2, . . . , bt}
is the basis for N (L) derived from the nested subspaces Mi, then the set of
all Jordan chains J = Jb1 âˆªJb2 âˆªÂ· Â· Â· âˆªJbt is a basis for Cn. If the vectors
from J are placed as columns (in the order in which they appear in J ) in a
matrix PnÃ—n = [ J1 | J2 | Â· Â· Â· | Jt ], then P is nonsingular, and if bj âˆˆSi, then
Jj = [ Lix | Liâˆ’1x | Â· Â· Â· | Lx | x ] for some x such that Lix = bj so that
LJj = [ 0 | Lix | Â· Â· Â· | Lx ] = [ Lix | Â· Â· Â· | Lx | x ]
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
1
...
...
...
1
0
ï£¶
ï£·
ï£·
ï£·
ï£¸= JjNj,

7.7 Nilpotent Matrices and Jordan Structure
579
where Nj is an (i + 1) Ã— (i + 1) matrix whose entries are equal to 1 along the
superdiagonal and zero elsewhere. Therefore,
LP = [ LJ1 | LJ2 | Â· Â· Â· | LJt ] = [ J1 | J2 | Â· Â· Â· | Jt ]
ï£«
ï£¬
ï£¬
ï£­
N1
0
Â· Â· Â·
0
0
N2
Â· Â· Â·
0
...
...
...
0
0
Â· Â· Â·
Nt
ï£¶
ï£·
ï£·
ï£¸
or, equivalently,
Pâˆ’1LP = N =
ï£«
ï£¬
ï£¬
ï£­
N1 0 Â· Â· Â·
0
0
N2Â· Â· Â·
0
...
...
...
0
0 Â· Â· Â· Nt
ï£¶
ï£·
ï£·
ï£¸, where Nj =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
1
...
...
...
1
0
ï£¶
ï£·
ï£·
ï£·
ï£¸. (7.7.5)
Each Nj is a nilpotent matrix whose index is given by its size. The Nj â€™s are
called nilpotent Jordan blocks, and the block-diagonal matrix N is called the
Jordan form for L. Below is a summary.
Jordan Form for a Nilpotent Matrix
Every nilpotent matrix LnÃ—n of index k is similar to a block-diagonal
matrix
Pâˆ’1LP = N =
ï£«
ï£¬
ï£¬
ï£­
N1 0 Â· Â· Â·
0
0
N2Â· Â· Â·
0
...
...
...
0
0 Â· Â· Â· Nt
ï£¶
ï£·
ï£·
ï£¸
(7.7.6)
in which each Nj is a nilpotent matrix having ones on the superdiagonal
and zeros elsewhereâ€”see (7.7.5).
â€¢
The number of blocks in N is given by t = dim N (L).
â€¢
The size of the largest block in N is k Ã— k.
â€¢
The number of i Ã— i blocks in N is Î½i = riâˆ’1 âˆ’2ri + ri+1, where
ri = rank

Li
â€”this follows from (7.7.4).
â€¢
If B = Skâˆ’1 âˆªSkâˆ’2 âˆªÂ· Â· Â·âˆªS0 = {b1, b2, . . . , bt} is a basis for N (L)
derived from the nested subspaces Mi = R

Li
âˆ©N (L), then
â–·
the set of vectors J = Jb1 âˆªJb2 âˆªÂ· Â· Â· âˆªJbt from all Jordan
chains is a basis for Cn;
â–·
PnÃ—n = [ J1 | J2 | Â· Â· Â· | Jt ] is the nonsingular matrix containing
these Jordan chains in the order in which they appear in J .

580
Chapter 7
Eigenvalues and Eigenvectors
The following theorem demonstrates that the Jordan structure (the num-
ber and the size of the blocks in N ) is uniquely determined by L, but P is
not. In other words, the Jordan form is unique up to the arrangement of the
individual Jordan blocks.
Uniqueness of the Jordan Structure
The structure of the Jordan form for a nilpotent matrix LnÃ—n of index
k is uniquely determined by L in the sense that whenever L is similar
to a block-diagonal matrix B = diag (B1, B2, . . . , Bt) in which each
Bi has the form
Bi =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
Ïµi
0
Â· Â· Â·
0
0
0
Ïµi
0
...
...
...
...
0
0
Â· Â· Â·
0
Ïµi
0
0
Â· Â· Â·
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
niÃ—ni
for
Ïµi Ì¸= 0,
then it must be the case that t = dim N (L), and the number of
blocks having size i Ã— i must be given by riâˆ’1 âˆ’2ri + ri+1, where
ri = rank

Li
.
Proof.
Suppose that L is similar to both B and N, where B is as described
above and N is as described in (7.7.6). This implies that B and N are similar,
and hence rank

Bi
= rank

Li
= ri for every nonnegative integer i. In
particular, index (B) = index (L). Each time a block Bi is powered, the line of
Ïµi â€™s moves to the next higher diagonal level so that
rank (Bp
i ) =
	
ni âˆ’p
if p < ni,
0
if p â‰¥ni.
Since rp = rank (Bp) = t
i=1 rank (Bp
i ), it follows that if Ï‰i is the number of
i Ã— i blocks in B, then
rkâˆ’1 = Ï‰k,
rkâˆ’2 = Ï‰kâˆ’1 + 2Ï‰k,
rkâˆ’3 = Ï‰kâˆ’2 + 2Ï‰kâˆ’1 + 3Ï‰k,
...
and, in general, ri = Ï‰i+1 + 2Ï‰i+2 + Â· Â· Â· + (k âˆ’i)Ï‰k. Itâ€™s now straightforward
to verify that riâˆ’1 âˆ’2ri + ri+1 = Ï‰i. Finally, using this equation together with
(7.7.4) guarantees that the number of blocks in B must be
t =
k

i=1
Ï‰i =
k

i=1
(riâˆ’1 âˆ’2ri + ri+1) =
k

i=1
Î½i = dim N (L).

7.7 Nilpotent Matrices and Jordan Structure
581
The manner in which we developed the Jordan theory spawned 1â€™s on the su-
perdiagonals of the Jordan blocks Ni in (7.7.5). But it was not necessary to
do soâ€”it was simply a matter of convenience. In fact, any nonzero value can be
forced onto the superdiagonal of any Ni â€”see Exercise 7.7.9. In other words,
the fact that 1â€™s appear on the superdiagonals of the Ni â€™s is artiï¬cial and is not
important to the structure of the Jordan form for L. Whatâ€™s important, and
what constitutes the â€œJordan structure,â€ is the number and sizes of the Jordan
blocks (or chains) and not the values appearing on the superdiagonals of these
blocks.
Example 7.7.1
Problem: Determine the Jordan forms for 3 Ã— 3 nilpotent matrices L1, L2,
and L3 that have respective indices k = 1, 2, 3.
Solution: The size of the largest block must be k Ã— k, so
N1 =
ï£«
ï£­
0
0
0
0
0
0
0
0
0
ï£¶
ï£¸,
N2 =
ï£«
ï£­
0
1
0
0
0
0
0
0
0
ï£¶
ï£¸,
N3 =
ï£«
ï£­
0
1
0
0
0
1
0
0
0
ï£¶
ï£¸.
Example 7.7.2
For a nilpotent matrix L, the theoretical development relies on a complicated
basis for N (L) to derive the structure of the Jordan form N as well as the
Jordan chains that constitute a nonsingular matrix P such that Pâˆ’1LP = N.
But, after the dust settled, we saw that a basis for N (L) is not needed to
construct N because N is completely determined simply by ranks of powers of
L. A basis for N (L) is only required to construct the Jordan chains in P.
Question:
For the purpose of constructing Jordan chains in P, can we use an
arbitrary basis for N (L) instead of the complicated basis built from the Mi â€™s?
Answer:
No!
Consider the nilpotent matrix
L =
ï£«
ï£­
2
0
1
âˆ’4
0
âˆ’2
âˆ’4
0
âˆ’2
ï£¶
ï£¸
and its Jordan form
N =
ï£«
ï£­
0
1
0
0
0
0
0
0
0
ï£¶
ï£¸.
If Pâˆ’1LP = N, where P = [ x1 | x2 | x3 ], then LP = PN implies that
Lx1 = 0,
Lx2 = x1, and Lx3 = 0. In other words, B = {x1, x3} must
be a basis for N (L), and Jx1 = {x1, x2} must be a Jordan chain built on top
of x1. If we try to construct such vectors by starting with the naive basis
x1 =
ï£«
ï£­
1
0
âˆ’2
ï£¶
ï£¸
and
x3 =
ï£«
ï£­
0
1
0
ï£¶
ï£¸
(7.7.7)

582
Chapter 7
Eigenvalues and Eigenvectors
for N (L) obtained by solving Lx = 0 with straightforward Gaussian elimi-
nation, we immediately hit a brick wall because x1 Ì¸âˆˆR (L) means Lx2 = x1
is an inconsistent system, so x2 cannot be determined. Similarly, x3 Ì¸âˆˆR (L)
insures that the same diï¬ƒculty occurs if x3 is used in place of x1. In other
words, even though the vectors in (7.7.7) constitute an otherwise perfectly good
basis for N (L), they canâ€™t be used to build P.
Example 7.7.3
Problem: Let LnÃ—n be a nilpotent matrix of index k. Provide an algorithm
for constructing the Jordan chains that generate a nonsingular matrix P such
that Pâˆ’1LP = N is in Jordan form.
Solution:
1.
Start with the fact that Mkâˆ’1 = R

Lkâˆ’1
(Exercise 7.7.5), and deter-
mine a basis {y1, y2, . . . , yq} for R

Lkâˆ’1
.
2.
Extend {y1, y2, . . . , yq} to a basis for Mkâˆ’2 = R

Lkâˆ’2
âˆ©N (L) as
follows.
â–·
Find a basis {v1, v2, . . . , vs} for N (LB), where B is a matrix con-
taining a basis for R

Lkâˆ’2
â€”e.g., the basic columns of Lkâˆ’2. The
set {Bv1, Bv2, . . . , Bvs} is a basis for Mkâˆ’2 (see p. 211).
â–·
Find the basic columns in [ y1 | y2 | Â· Â· Â· | yq | Bv1 | Bv2 | Â· Â· Â· | Bvs ]. Say
they are {y1, . . . , yq, BvÎ²1, . . . , BvÎ²j} (all of the yj â€™s are basic be-
cause they are a leading linearly independent subset). This is a basis
for Mkâˆ’2 that contains a basis for Mkâˆ’1. In other words,
Skâˆ’1 = {y1, y2, . . . , yq}
and
Skâˆ’2 = {BvÎ²1, BvÎ²2, . . . , BvÎ²j}.
3.
Repeat the above procedure k âˆ’1 times to construct a basis for N (L)
that is of the form B = Skâˆ’1 âˆªSkâˆ’2 âˆªÂ· Â· Â· âˆªS0 = {b1, b2, . . . , bt}, where
Skâˆ’1 âˆªSkâˆ’2 âˆªÂ· Â· Â· âˆªSi is a basis for Mi for each i = k âˆ’1, k âˆ’2, . . . , 0.
4.
Build a Jordan chain on top of each bj âˆˆB. If bj âˆˆSi, then we solve
Lixj = bj and set Jj = [ Lixj | Liâˆ’1xj | Â· Â· Â· | Lxj | xj ]. The desired simi-
larity transformation is PnÃ—n = [ J1 | J2 | Â· Â· Â· | Jt ].
Example 7.7.4
Problem: Find P and N such that Pâˆ’1LP = N is in Jordan form, where
L =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
1
âˆ’2
0
1
âˆ’1
3
1
5
1
âˆ’1
3
âˆ’2
âˆ’1
0
0
âˆ’1
0
2
1
0
0
1
0
âˆ’5
âˆ’3
âˆ’1
âˆ’1
âˆ’1
âˆ’1
âˆ’3
âˆ’2
âˆ’1
âˆ’1
0
âˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.

7.7 Nilpotent Matrices and Jordan Structure
583
Solution: First determine the Jordan form for L. Computing ri = rank

Li
reveals that r1 = 3, r2 = 1, and r3 = 0, so the index of L is k = 3, and
the number of 3 Ã— 3 blocks = r2 âˆ’2r3 + r4 = 1,
the number of 2 Ã— 2 blocks = r1 âˆ’2r2 + r3 = 1,
the number of 1 Ã— 1 blocks = r0 âˆ’2r1 + r2 = 1.
Consequently, the Jordan form of L is
N =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Notice that three Jordan blocks were found, and this agrees with the fact that
dim N (L) = 6 âˆ’rank (L) = 3. Determine P by following the procedure de-
scribed in Example 7.7.3.
1.
Since rank

L2
= 1, any nonzero column from L2 will be a basis for
M2 = R

L2
, so set y1 = [L2]âˆ—1 = (6, âˆ’6, 0, 0, âˆ’6, âˆ’6)T .
2.
To extend y1 to a basis for M1 = R (L) âˆ©N (L), use
B = [Lâˆ—1 | Lâˆ—2 | Lâˆ—3 ] =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
1
âˆ’2
3
1
5
âˆ’2
âˆ’1
0
2
1
0
âˆ’5
âˆ’3
âˆ’1
âˆ’3
âˆ’2
âˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=â‡’LB =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
6
3
3
âˆ’6
âˆ’3
âˆ’3
0
0
0
0
0
0
âˆ’6
âˆ’3
âˆ’3
âˆ’6
âˆ’3
âˆ’3
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
and determine a basis for N (LB) to be
	
v1 =
 âˆ’1
2
0

, v2 =
 âˆ’1
0
2

.
Reducing [ y1 | Bv1 | Bv2 ] to echelon form shows that its basic columns
are in the ï¬rst and third positions, so {y1, Bv2} is a basis for M1 with
S2 =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
6
âˆ’6
0
0
âˆ’6
âˆ’6
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
= b1
ï£¼
ï£´
ï£´
ï£´
ï£´
ï£´
ï£½
ï£´
ï£´
ï£´
ï£´
ï£´
ï£¾
and
S1 =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ’5
7
2
âˆ’2
3
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
= b2
ï£¼
ï£´
ï£´
ï£´
ï£´
ï£´
ï£½
ï£´
ï£´
ï£´
ï£´
ï£´
ï£¾
.

584
Chapter 7
Eigenvalues and Eigenvectors
3.
Now extend S2 âˆªS1 = {b1, b2} to a basis for M0 = N (L). This time,
B = I, and a basis for N (LB) = N (L) can be computed to be
v1 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
2
âˆ’4
âˆ’1
3
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
v2 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ’4
5
2
0
3
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
and
v3 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
âˆ’2
âˆ’2
0
0
3
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
and {Bv1, Bv2, Bv3} = {v1, v2, v3}. Reducing [ b1 | b2 | v1 | v2 | v3 ] to
echelon form reveals that its basic columns are in positions one, two, and
three, so v1 is the needed extension vector. Therefore, the complete nested
basis for N (L) is
b1 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
6
âˆ’6
0
0
âˆ’6
âˆ’6
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
âˆˆS2,
b2 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ’5
7
2
âˆ’2
3
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
âˆˆS1,
and
b3 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
2
âˆ’4
âˆ’1
3
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
âˆˆS0.
4.
Complete the process by building a Jordan chain on top of each bj âˆˆSi
by solving Lixj = bj and by setting Jj = [Lixj | Â· Â· Â· | Lxj | xj ]. Since
x1 = e1 solves L2x1 = b1, we have J1 = [ L2e1 | Le1 | e1 ]. Solving
Lx2 = b2 yields x2 = (âˆ’1, 0, 2, 0, 0, 0)T , so J2 = [ Lx2 | x2 ]. Finally,
J3 = [ b3 ]. Putting these chains together produces
P = [ J1 | J2 | J3 ] =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
6
1
1
âˆ’5
âˆ’1
2
âˆ’6
3
0
7
0
âˆ’4
0
âˆ’2
0
2
2
âˆ’1
0
2
0
âˆ’2
0
3
âˆ’6
âˆ’5
0
3
0
0
âˆ’6
âˆ’3
0
1
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
It can be veriï¬ed by direct multiplication that Pâˆ’1LP = N.
Itâ€™s worthwhile to pay attention to how the results in this section translate into
the language of direct sum decompositions of invariant subspaces as discussed
in Â§4.9 (p. 259) and Â§5.9 (p. 383). For a linear nilpotent operator L of index
k deï¬ned on a ï¬nite-dimensional vector space V, statement (7.7.6) on p. 579
means that V can be decomposed as a direct sum V = V1 âŠ•V2 âŠ•Â· Â· Â·âŠ•Vt, where
Vj = span(Jbj) is the space spanned by a Jordan chain emanating from the
basis vector bj âˆˆN (L) and where t = dim N (L). Furthermore, each Vj is an

7.7 Nilpotent Matrices and Jordan Structure
585
invariant subspace for L, and the matrix representation of L with respect to
the basis J = Jb1 âˆªJb2 âˆªÂ· Â· Â· âˆªJbt is
[L]J =
ï£«
ï£¬
ï£¬
ï£­
N1
0
Â· Â· Â·
0
0
N2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
Nt
ï£¶
ï£·
ï£·
ï£¸
in which
Nj =

L/Vj

Jbj
.
(7.7.8)
Exercises for section 7.7
7.7.1. Can the index of an n Ã— n nilpotent matrix ever exceed n?
7.7.2. Determine all possible Jordan forms N for a 4 Ã— 4 nilpotent matrix.
7.7.3. Explain why the number of blocks of size i Ã— i or larger in the Jordan
form for a nilpotent matrix is given by rank

Liâˆ’1
âˆ’rank

Li
.
7.7.4. For a nilpotent matrix L of index k, let Mi = R

Li
âˆ©N (L). Prove
that Mi âŠ†Miâˆ’1 for each i = 0, 1, . . . , k.
7.7.5. Prove that R

Lkâˆ’1
âˆ©N (L) = R

Lkâˆ’1
for all nilpotent matrices L
of index k > 1. In other words, prove Mkâˆ’1 = R

Lkâˆ’1
.
7.7.6. Let L be a nilpotent matrix of index k > 1. Prove that if the columns
of B are a basis for R

Li
for i â‰¤k âˆ’1, and if {v1, v2, . . . , vs} is a
basis for N (LB), then {Bv1, Bv2, . . . , Bvs} is a basis for Mi.
7.7.7. Find P and N such that Pâˆ’1LP = N is in Jordan form, where
L =
ï£«
ï£¬
ï£­
3
3
2
1
âˆ’2
âˆ’1
âˆ’1
âˆ’1
1
âˆ’1
0
1
âˆ’5
âˆ’4
âˆ’3
âˆ’2
ï£¶
ï£·
ï£¸.
7.7.8. Determine the Jordan form for the following 8 Ã— 8 nilpotent matrix.
L =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
41
30
15
7
4
6
1
3
âˆ’54
âˆ’39
âˆ’19
âˆ’9
âˆ’6
âˆ’8
âˆ’2
âˆ’4
9
6
2
1
2
1
0
1
âˆ’6
âˆ’5
âˆ’3
âˆ’2
1
âˆ’1
0
0
âˆ’32
âˆ’24
âˆ’13
âˆ’6
âˆ’2
âˆ’5
âˆ’1
âˆ’2
âˆ’10
âˆ’7
âˆ’2
0
âˆ’3
0
3
âˆ’2
âˆ’4
âˆ’3
âˆ’2
âˆ’1
0
âˆ’1
âˆ’1
0
17
12
6
3
2
3
2
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.

586
Chapter 7
Eigenvalues and Eigenvectors
7.7.9. Prove that if N is the Jordan form for a nilpotent matrix L as described
in (7.7.5) and (7.7.6) on p. 579, then for any set of nonzero scalars
{Ïµ1, Ïµ2, . . . , Ïµt} , the matrix L is similar to a matrix ËœN of the form
ËœN =
ï£«
ï£¬
ï£¬
ï£­
Ïµ1N1
0
Â· Â· Â·
0
0
Ïµ2N2Â· Â· Â·
0
...
...
...
0
0
Â· Â· Â· ÏµtNt
ï£¶
ï£·
ï£·
ï£¸.
In other words, the 1â€™s on the superdiagonal of the Ni â€™s in (7.7.5) are
artiï¬cial because any nonzero value can be forced onto the superdiagonal
of any Ni. Whatâ€™s important in the â€œJordan structureâ€ of L is the
number and sizes of the nilpotent Jordan blocks (or chains) and not the
values appearing on the superdiagonals of these blocks.

7.8 Jordan Form
587
7.8
JORDAN FORM
The goal of this section is to do for general matrices A âˆˆCnÃ—n what was done for
nilpotent matrices in Â§7.7â€”reduce A by means of a similarity transformation
to a block-diagonal matrix in which each block has a simple triangular form.
The two major components for doing this are now in placeâ€”they are the core-
nilpotent decomposition (p. 397) and the Jordan form for nilpotent matrices. All
that remains is to connect these two ideas. To do so, it is convenient to adopt
the following terminology.
Index of an Eigenvalue
The index of an eigenvalue Î» for a matrix A âˆˆCnÃ—n is deï¬ned to
be the index of the matrix (A âˆ’Î»I) . In other words, from the charac-
terizations of index given on p. 395, index (Î») is the smallest positive
integer k such that any one of the following statements is true.
â€¢
rank

(A âˆ’Î»I)k
= rank

(A âˆ’Î»I)k+1
.
â€¢
R

(A âˆ’Î»I)k
= R

(A âˆ’Î»I)k+1
.
â€¢
N

(A âˆ’Î»I)k
= N

(A âˆ’Î»I)k+1
.
â€¢
R

(A âˆ’Î»I)k
âˆ©N

(A âˆ’Î»I)k
= 0.
â€¢
Cn = R

(A âˆ’Î»I)k
âŠ•N

(A âˆ’Î»I)k
.
It is understood that index (Âµ) = 0 if and only if Âµ Ì¸âˆˆÏƒ (A) .
The Jordan form for A âˆˆCnÃ—n is derived by digesting the distinct eigen-
values in Ïƒ (A) = {Î»1, Î»2, . . . , Î»s} one at a time with a core-nilpotent decom-
position as follows. If index (Î»1) = k1, then there is a nonsingular matrix X1
such that
Xâˆ’1
1 (A âˆ’Î»1I)X1 =

L1
0
0
C1

,
(7.8.1)
where L1 is nilpotent of index k1 and C1 is nonsingular (it doesnâ€™t matter
whether C1 or L1 is listed ï¬rst, so, for the sake of convenience, the nilpotent
block is listed ï¬rst). We know from the results on nilpotent matrices (p. 579)
that there is a nonsingular matrix Y1 such that
Yâˆ’1
1 L1Y1 = N(Î»1) =
ï£«
ï£¬
ï£¬
ï£­
N1(Î»1)
0
Â· Â· Â·
0
0
N2(Î»1)
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
Nt1(Î»1)
ï£¶
ï£·
ï£·
ï£¸
is a block-diagonal matrix that is characterized by the following features.

588
Chapter 7
Eigenvalues and Eigenvectors
â–·
Every block in N(Î»1) has the form Nâ‹†(Î»1) =
ï£«
ï£¬
ï£¬
ï£­
0
1
...
...
...
1
0
ï£¶
ï£·
ï£·
ï£¸.
â–·
There are t1 = dim N (L1) = dim N (A âˆ’Î»1I) such blocks in N(Î»1).
â–·
The number of i Ã— i blocks of the form Nâ‹†(Î»1) contained in N(Î»1) is
Î½i(Î»1) = rank

Liâˆ’1
1

âˆ’2 rank

Li
1

+ rank

Li+1
1

. But C1 in (7.8.1) is
nonsingular, so rank (Lp
1) = rank ((A âˆ’Î»1I)p) âˆ’rank (C1), and thus the
number of i Ã— i blocks Nâ‹†(Î»1) contained in N(Î»1) can be expressed as
Î½i(Î»1) = riâˆ’1(Î»1) âˆ’2ri(Î»1) + ri+1(Î»1),
where
ri(Î»1) = rank

(A âˆ’Î»1I)i
.
Now, Q1=X1
 Y1 0
0
I

is nonsingular, and Qâˆ’1
1 (A âˆ’Î»1I)Q1 =
 N(Î»1)
0
0
C1

or,
equivalently,
Qâˆ’1
1 AQ1 =

N(Î»1) + Î»1I
0
0
C1 + Î»1I

=

J(Î»1)
0
0
A1

.
(7.8.2)
The upper-left-hand segment J(Î»1) = N(Î»1) + Î»1I has the block-diagonal form
J(Î»1) =
ï£«
ï£¬
ï£¬
ï£­
J1(Î»1)
0
Â· Â· Â·
0
0
J2(Î»1) Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â· Jt1(Î»1)
ï£¶
ï£·
ï£·
ï£¸
with
Jâ‹†(Î»1) = Nâ‹†(Î»1) + Î»1I.
The matrix J(Î»1) is called the Jordan segment associated with the eigenvalue
Î»1, and the individual blocks Jâ‹†(Î»1) contained in J(Î»1) are called Jordan
blocks associated with the eigenvalue Î»1. The structure of the Jordan segment
J(Î»1) is inherited from Jordan structure of the associated nilpotent matrix L1.
â–·
Each Jordan block looks like Jâ‹†(Î»1) = Nâ‹†(Î»1) + Î»1I =
ï£«
ï£¬
ï£¬
ï£­
Î»1
1
...
...
...
1
Î»1
ï£¶
ï£·
ï£·
ï£¸.
â–·
There are t1 = dim N (A âˆ’Î»1I) such Jordan blocks in the segment J(Î»1).
â–·
The number of i Ã— i Jordan blocks Jâ‹†(Î»1) contained in J(Î»1) is
Î½i(Î»1) = riâˆ’1(Î»1) âˆ’2ri(Î»1) + ri+1(Î»1),
where
ri(Î»1) = rank

(A âˆ’Î»1I)i
.
Since the distinct eigenvalues of A are Ïƒ (A) = {Î»1, Î»2, . . . , Î»s} , the distinct
eigenvalues of A âˆ’Î»1I are
Ïƒ (A âˆ’Î»1I) = {0, (Î»2 âˆ’Î»1), (Î»3 âˆ’Î»1), . . . , (Î»s âˆ’Î»1)}.

7.8 Jordan Form
589
Couple this with the fact that the only eigenvalue for the nilpotent matrix L1
in (7.8.1) is zero to conclude that
Ïƒ (C1) = {(Î»2 âˆ’Î»1), (Î»3 âˆ’Î»1), . . . , (Î»s âˆ’Î»1)}.
Therefore, the spectrum of A1 = C1+Î»1I in (7.8.2) is Ïƒ (A1) = {Î»2, Î»3, . . . , Î»s}.
This means that the core-nilpotent decomposition process described above can
be repeated on A1 âˆ’Î»2I to produce a nonsingular matrix Q2 such that
Qâˆ’1
2 A1Q2 =

J(Î»2)
0
0
A2

,
where
Ïƒ (A2) = {Î»3, Î»4, . . . , Î»s},
(7.8.3)
and where J(Î»2) = diag (J1(Î»2), J2(Î»2), . . . , Jt2(Î»2)) is a Jordan segment com-
posed of Jordan blocks Jâ‹†(Î»2) with the following characteristics.
â–·
Each Jordan block in J(Î»2) has the form Jâ‹†(Î»2) =
ï£«
ï£¬
ï£¬
ï£­
Î»2
1
...
...
...
1
Î»2
ï£¶
ï£·
ï£·
ï£¸.
â–·
There are t2 = dim N (A âˆ’Î»2I) Jordan blocks in segment J(Î»2).
â–·
The number of i Ã— i Jordan blocks in segment J(Î»2) is
Î½i(Î»2) = riâˆ’1(Î»2) âˆ’2ri(Î»2) + ri+1(Î»2), where ri(Î»2) = rank

(A âˆ’Î»2I)i
.
If we set P2 = Q1
 I
0
0
Q2

, then P2 is a nonsingular matrix such that
Pâˆ’1
2 AP2 =
ï£«
ï£­
J(Î»1)
0
0
0
J(Î»2)
0
0
0
A2
ï£¶
ï£¸,
where
Ïƒ (A2) = {Î»3, Î»4, . . . , Î»s}.
Repeating this process until all eigenvalues have been depleted results in a
nonsingular matrix Ps such that Pâˆ’1
s APs = J = diag (J(Î»1), J(Î»2), . . . , J(Î»s))
in which each J(Î»j) is a Jordan segment containing tj = dim N (A âˆ’Î»jI) Jor-
dan blocks. The matrix J is called the Jordan form
79 for A (some texts refer
to J as the Jordan canonical form or the Jordan normal form). The Jordan
structure of A is deï¬ned to be the number of Jordan segments in J along
with the number and sizes of the Jordan blocks within each segment. The proof
of uniqueness of the Jordan form for a nilpotent matrix (p. 580) can be extended
to all A âˆˆCnÃ—n. In other words, the Jordan structure of a matrix is uniquely
determined by its entries. Below is a formal summary of these developments.
79
Marie Ennemond Camille Jordan (1838â€“1922) discussed this idea (not over the complex num-
bers but over a ï¬nite ï¬eld) in 1870 in TraitÂ´e des substitutions et des Â´equations algebraique
that earned him the Poncelet Prize of the AcadÂ´emie des Science. But Jordan may not have
been the ï¬rst to develop these concepts. It has been reported that the German mathematician
Karl Theodor Wilhelm Weierstrass (1815â€“1897) had previously formulated results along these
lines. However, Weierstrass did not publish his ideas because he was fanatical about rigor, and
he would not release his work until he was sure it was on a ï¬rm mathematical foundation.
Weierstrass once said that â€œa mathematician who is not also something of a poet will never be
a perfect mathematician.â€

590
Chapter 7
Eigenvalues and Eigenvectors
Jordan Form
For every A âˆˆCnÃ—n with distinct eigenvalues Ïƒ (A) = {Î»1, Î»2, . . . , Î»s} ,
there is a nonsingular matrix P such that
Pâˆ’1AP = J =
ï£«
ï£¬
ï£¬
ï£­
J(Î»1)
0
Â· Â· Â·
0
0
J(Î»2) Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â· J(Î»s)
ï£¶
ï£·
ï£·
ï£¸.
(7.8.4)
â€¢
J has one Jordan segment J(Î»j) for each eigenvalue Î»j âˆˆÏƒ (A) .
â€¢
Each segment J(Î»j) is made up of tj = dim N (A âˆ’Î»jI) Jordan
blocks Jâ‹†(Î»j) as described below.
J(Î»j)=
ï£«
ï£¬
ï£¬
ï£­
J1(Î»j)
0
Â· Â· Â·
0
0
J2(Î»j) Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â· Jtj(Î»j)
ï£¶
ï£·
ï£·
ï£¸with Jâ‹†(Î»j) =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
Î»j
1
...
...
...
1
Î»j
ï£¶
ï£·
ï£·
ï£·
ï£¸.
â€¢
The largest Jordan block in J(Î»j) is kj Ã— kj, where kj = index (Î»j).
â€¢
The number of i Ã— i Jordan blocks in J(Î»j) is given by
Î½i(Î»j)= riâˆ’1(Î»j) âˆ’2ri(Î»j) + ri+1(Î»j) with ri(Î»j)=rank

(A âˆ’Î»jI)i
.
â€¢
Matrix J in (7.8.4) is called the Jordan form for A. The structure
of this form is unique in the sense that the number of Jordan seg-
ments in J as well as the number and sizes of the Jordan blocks in
each segment is uniquely determined by the entries in A. Further-
more, every matrix similar to A has the same Jordan structureâ€”i.e.,
A, B âˆˆCnÃ—n are similar if and only if A and B have the same
Jordan structure. The matrix P is not uniqueâ€”see p. 594.
Example 7.8.1
Problem: Find the Jordan form for A =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
5
4
0
0
4
3
2
3
1
0
5
1
0
âˆ’1
2
0
2
0
âˆ’8
âˆ’8
âˆ’1
2
âˆ’12
âˆ’7
0
0
0
0
âˆ’1
0
âˆ’8
âˆ’8
âˆ’1
0
âˆ’9
âˆ’5
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.

7.8 Jordan Form
591
Solution: Computing the eigenvalues (which is the hardest part) reveals two
distinct eigenvalues Î»1 = 2 and Î»2 = âˆ’1, so there are two Jordan segments in
the Jordan form J =
 J(2)
0
0
J(âˆ’1)

. Computing ranks ri(2) = rank

(A âˆ’2I)i
and ri(âˆ’1) = rank

(A + I)i
until rk(â‹†) = rk+1(â‹†) yields
r1(2) = rank (A âˆ’2I)
= 4,
r1(âˆ’1) = rank (A + I)
= 4,
r2(2) = rank

(A âˆ’2I)2
= 3,
r2(âˆ’1) = rank

(A + I)2
= 4,
r3(2) = rank

(A âˆ’2I)3
= 2,
r4(2) = rank

(A âˆ’2I)4
= 2,
so k1 = index (Î»1) = 3 and k2 = index (Î»2) = 1. This tells us that the largest
Jordan block in J(2) is 3 Ã— 3, while the largest Jordan block in J(âˆ’1) is 1 Ã— 1
so that J(âˆ’1) is a diagonal matrix (the associated eigenvalue is semisimple
whenever this happens). Furthermore,
Î½3(2)
= r2(2) âˆ’2r3(2) + r4(2) = 1
=â‡’
one 3 Ã— 3 block in J(2),
Î½2(2)
= r1(2) âˆ’2r2(2) + r3(2) = 0
=â‡’
no 2 Ã— 2 blocks in J(2),
Î½1(2)
= r0(2) âˆ’2r1(2) + r2(2) = 1
=â‡’
one 1 Ã— 1 block in J(2),
Î½1(âˆ’1) = r0(âˆ’1) âˆ’2r1(âˆ’1) + r2(âˆ’1) = 2
=â‡’
two 1 Ã— 1 blocks in J(âˆ’1).
Therefore, J(2) =
ï£«
ï£¬
ï£­
2
1
0
0
0
2
1
0
0
0
2
0
0
0
0
2
ï£¶
ï£·
ï£¸and J(âˆ’1) =
 âˆ’1
0
0
âˆ’1

so that
J =

J(2)
0
0
J(âˆ’1)

=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
2
1
0
0
0
2
1
0
0
0
2
0
0
0
0
2
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
âˆ’1
0
0
âˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
The above example suggests that determining the Jordan form for AnÃ—n
is straightforward, and perhaps even easy. In theory, it isâ€”just ï¬nd Ïƒ (A) , and
calculate some ranks. But, in practice, both of these tasks can be diï¬ƒcult. To
begin with, the rank of a matrix is a discontinuous function of its entries, and rank
computed with ï¬‚oating-point arithmetic can vary with the algorithm used and is
often diï¬€erent than rank computed with exact arithmetic (recall Exercise 2.2.4).

592
Chapter 7
Eigenvalues and Eigenvectors
Furthermore, computing higher-index eigenvalues with ï¬‚oating-point arithmetic
is fraught with peril. To see why, consider the matrix
L(Ïµ) =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
1
...
...
...
1
Ïµ
0
ï£¶
ï£·
ï£·
ï£·
ï£¸
nÃ—n
whose characteristic equation is
Î»n âˆ’Ïµ = 0.
For Ïµ = 0, zero is the only eigenvalue (and it has index n ), but for all Ïµ > 0,
there are n distinct eigenvalues given by Ïµ1/ne2kÏ€i/n for k = 0, 1, . . . , nâˆ’1. For
example, if n = 32, and if Ïµ changes from 0 to 10âˆ’16, then the eigenvalues
of L(Ïµ) change in magnitude from 0 to 10âˆ’1/2 â‰ˆ.316, which is substantial for
such a small perturbation. Sensitivities of this kind present signiï¬cant problems
for ï¬‚oating-point algorithms. In addition to showing that high-index eigenvalues
are sensitive to small perturbations, this example also shows that the Jordan
structure is highly discontinuous. L(0) is in Jordan form, and there is just one
Jordan block of size n, but for all Ïµ Ì¸= 0, the Jordan form of L(Ïµ) is a diagonal
matrixâ€”i.e., there are n Jordan blocks of size 1 Ã— 1. Lest you think that this
example somehow is an isolated case, recall from Example 7.3.6 (p. 532) that
every matrix in CnÃ—n is arbitrarily close to a diagonalizable matrix.
All of the above observations make it clear that itâ€™s hard to have faith in
a Jordan form that has been computed with ï¬‚oating-point arithmetic. Conse-
quently, numerical computation of Jordan forms is generally avoided.
Example 7.8.2
The Jordan form of A conveys complete information about the eigenvalues of
A. For example, if the Jordan form for A is
J =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
4
1
0
4
1
4
4
1
0
4
3
1
0
3
2
2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
then we know that
â–·
A9Ã—9 has three distinct eigenvalues, namely Ïƒ (A) = {4, 3, 2};
â–·
alg mult (4) = 5, alg mult (3) = 2, and alg mult (2) = 2;
â–·
geo mult (4) = 2, geo mult (3) = 1, and geo mult (2) = 2;

7.8 Jordan Form
593
â–·
index (4) = 3, index (3) = 2, and index (2) = 1;
â–·
Î» = 2 is a semisimple eigenvalue, so, while A is not diagonalizable, part of
it is; i.e., the restriction A/N(Aâˆ’2I) is a diagonalizable linear operator.
Of course, if both P and J are known, then A can be completely reconstructed
from (7.8.4), but the point being made here is that only J is needed to reveal
the eigenstructure along with the other similarity invariants of A.
Now that the structure of the Jordan form J is known, the structure of the
similarity transformation P such that Pâˆ’1AP = J is easily revealed. Focus
on a single p Ã— p Jordan block Jâ‹†(Î») contained in the Jordan segment J(Î»)
associated with an eigenvalue Î», and let Pâ‹†= [ x1 x2 Â· Â· Â· xp ] be the portion of
P = [ Â· Â· Â· | Pâ‹†| Â· Â· Â·] that corresponds to the position of Jâ‹†(Î») in J. Notice that
AP = PJ implies APâ‹†= Pâ‹†Jâ‹†(Î») or, equivalently,
A[ x1 x2 Â· Â· Â· xp ] = [ x1 x2 Â· Â· Â· xp ]
ï£«
ï£¬
ï£¬
ï£¬
ï£­
Î»
1
...
...
...
1
Î»
ï£¶
ï£·
ï£·
ï£·
ï£¸
pÃ—p
,
so equating columns on both sides of this equation produces
Ax1 = Î»x1
=â‡’
x1 is an eigenvector
=â‡’
(A âˆ’Î»I) x1 = 0,
Ax2 = x1 + Î»x2
=â‡’
(A âˆ’Î»I) x2 = x1
=â‡’
(A âˆ’Î»I)2 x2 = 0,
Ax3 = x2 + Î»x3
=â‡’
(A âˆ’Î»I) x3 = x2
=â‡’
(A âˆ’Î»I)3 x3 = 0,
...
...
...
Axp = xpâˆ’1 + Î»xp
=â‡’
(A âˆ’Î»I) xp = xpâˆ’1
=â‡’
(A âˆ’Î»I)p xp = 0.
In other words, the ï¬rst column x1 in Pâ‹†is a eigenvector for A associated with
Î». We already knew there had to be exactly one independent eigenvector for each
Jordan block because there are t = dim N (A âˆ’Î»I) Jordan blocks Jâ‹†(Î»), but
now we know precisely where these eigenvectors are located in P.
Vectors x such that x âˆˆN

(Aâˆ’Î»I)g
but x Ì¸âˆˆN

(Aâˆ’Î»I)gâˆ’1
are called
generalized eigenvectors of order g associated with Î». So Pâ‹†consists of an
eigenvector followed by generalized eigenvectors of increasing order. Moreover,
the columns of Pâ‹†form a Jordan chain analogous to (7.7.2) on p. 576; i.e.,
xi = (A âˆ’Î»I)pâˆ’i xp implies Pâ‹†must have the form
Pâ‹†=

(A âˆ’Î»I)pâˆ’1 xp | (A âˆ’Î»I)pâˆ’2 xp | Â· Â· Â· | (A âˆ’Î»I) xp | xp

.
(7.8.5)
A complete set of Jordan chains associated with a given eigenvalue Î» is de-
termined in exactly the same way as Jordan chains for nilpotent matrices are

594
Chapter 7
Eigenvalues and Eigenvectors
determined except that the nested subspaces Mi deï¬ned in (7.7.1) on p. 575
are redeï¬ned to be
Mi = R

(A âˆ’Î»I)i
âˆ©N (A âˆ’Î»I)
for i = 0, 1, . . . , k,
(7.8.6)
where k = index (Î»). Just as in the case of nilpotent matrices, it follows that
0 = Mk âŠ†Mkâˆ’1 âŠ†Â· Â· Â· âŠ†M0 = N (A âˆ’Î»I) (see Exercise 7.8.8). Since
(A âˆ’Î»I)/N((Aâˆ’Î»I)k) is a nilpotent linear operator of index k (Example 5.10.4,
p. 399), it can be argued that the same process used to build Jordan chains for
nilpotent matrices can be used to build Jordan chains for a general eigenvalue
Î». Below is a summary of the process adapted to the general case.
Constructing Jordan Chains
For each Î» âˆˆÏƒ (AnÃ—n) , set Mi = R

(A âˆ’Î»I)i
âˆ©N (A âˆ’Î»I) for
i = k âˆ’1, k âˆ’2, . . . , 0, where k = index (Î»).
â€¢
Construct a basis B for N (A âˆ’Î»I).
â–·Starting with any basis Skâˆ’1 for Mkâˆ’1 (see p. 211), sequentially
extend Skâˆ’1 with sets Skâˆ’2, Skâˆ’3, . . . , S0 such that
Skâˆ’1
is a basis for
Mkâˆ’1,
Skâˆ’1 âˆªSkâˆ’2
is a basis for
Mkâˆ’2,
Skâˆ’1 âˆªSkâˆ’2 âˆªSkâˆ’3
is a basis for
Mkâˆ’3,
etc., until a basis B = Skâˆ’1 âˆªSkâˆ’2 âˆªÂ· Â· Â· âˆªS0 = {b1, b2, . . . , bt}
for M0 = N (A âˆ’Î»I) is obtained (see Example 7.7.3 on p. 582).
â€¢
Build a Jordan chain on top of each eigenvector bâ‹†âˆˆB.
â–·For each eigenvector bâ‹†âˆˆSi, solve (A âˆ’Î»I)i xâ‹†= bâ‹†(a neces-
sarily consistent system) for xâ‹†, and construct a Jordan chain on
top of bâ‹†by setting
Pâ‹†=

(A âˆ’Î»I)i xâ‹†
 (A âˆ’Î»I)iâˆ’1 xâ‹†
 Â· Â· Â·
 (A âˆ’Î»I) xâ‹†
 xâ‹†

(i+1)Ã—n.
â–·Each such Pâ‹†corresponds to one Jordan block Jâ‹†(Î») in the Jor-
dan segment J(Î») associated with Î».
â–·The ï¬rst column in Pâ‹†is an eigenvector, and subsequent columns
are generalized eigenvectors of increasing order.
â€¢
If all such Pâ‹†â€™s for a given Î»j âˆˆÏƒ (A) = {Î»1, Î»2, . . . , Î»s} are put in
a matrix Pj, and if P =

P1 | P2 | Â· Â· Â· | Ps

, then P is a nonsingu-
lar matrix such that Pâˆ’1AP = J = diag (J(Î»1), J(Î»2), . . . , J(Î»s))
is in Jordan form as described on p. 590.

7.8 Jordan Form
595
Example 7.8.3
Caution!
Not every basis for N(A âˆ’Î»I) can be used to build Jordan chains
associated with an eigenvalue Î» âˆˆÏƒ (A) . For example, the Jordan form of
A =
ï£«
ï£­
3
0
1
âˆ’4
1
âˆ’2
âˆ’4
0
âˆ’1
ï£¶
ï£¸
is
J =
ï£«
ï£­
1
1
0
0
1
0
0
0
1
ï£¶
ï£¸
because Ïƒ (A) = {1} and index (1) = 2. Consequently, if P = [ x1 | x2 | x3 ]
is a nonsingular matrix such that Pâˆ’1AP = J, then the derivation beginning
on p. 593 leading to (7.8.5) shows that {x1, x2} must be a Jordan chain such
that (A âˆ’I)x1 = 0 and (A âˆ’I)x2 = x1, while x3 is another eigenvector (not
dependent on x1 ). Suppose we try to build the Jordan chains in P by starting
with the eigenvectors
x1 =
ï£«
ï£­
1
0
âˆ’2
ï£¶
ï£¸
and
x3 =
ï£«
ï£­
0
1
0
ï£¶
ï£¸
(7.8.7)
obtained by solving (A âˆ’I)x = 0 with straightforward Gaussâ€“Jordan elimina-
tion. This naive approach fails because x1 Ì¸âˆˆR (A âˆ’I) means (A âˆ’I)x2 = x1 is
an inconsistent system, so x2 cannot be determined. Similarly, x3 Ì¸âˆˆR (A âˆ’I)
insures that the same diï¬ƒculty occurs if x3 is used in place of x1. In other
words, even though the vectors in (7.8.7) constitute an otherwise perfectly good
basis for N (A âˆ’I), they are not suitable for building Jordan chains. You are
asked in Exercise 7.8.2 to ï¬nd the correct basis for N (A âˆ’I) that will yield the
Jordan chains that constitute P.
Example 7.8.4
Problem: What do the results concerning the Jordan form for A âˆˆCnÃ—n say
about the decomposition of Cn into invariant subspaces?
Solution: Consider Pâˆ’1AP = J = diag (J(Î»1), J(Î»2), . . . , J(Î»s)) , where the
J(Î»j) â€™s are the Jordan segments and P =

P1 | P2 | Â· Â· Â· | Ps

is a matrix of
Jordan chains as described in (7.8.5) and on p. 594. If A is considered as a
linear operator on Cn, and if the set of columns in Pi is denoted by Ji, then
the results in Â§4.9 (p. 259) concerning invariant subspaces together with those
in Â§5.9 (p. 383) about direct sum decompositions guarantee that each R (Pi) is
an invariant subspace for A such that
Cn = R (P1) âŠ•R (P2) âŠ•Â· Â· Â· âŠ•R (Ps)
and
J(Î»i) =

A/R(Pi)

Ji
.
More can be said. If alg mult (Î»i) = mi and index (Î»i) = ki, then Ji is a
linearly independent set containing mi vectors, and the discussion surrounding

596
Chapter 7
Eigenvalues and Eigenvectors
(7.8.5) insures that each column in Ji belongs to N

(Aâˆ’Î»iI)ki
. This coupled
with the fact that dim N

(A âˆ’Î»iI)ki
) = mi (Exercise 7.8.7) implies that Ji
is a basis for
R (Pi) = N

(A âˆ’Î»iI)ki
.
Consequently, each N

(A âˆ’Î»iI)ki
is an invariant subspace for A such that
Cn = N

(A âˆ’Î»1I)k1
âŠ•N

(A âˆ’Î»2I)k2
âŠ•Â· Â· Â· âŠ•N

(A âˆ’Î»sI)ks
and
J(Î»i) =
%
A/N

(Aâˆ’Î»iI)ki
&
Ji
.
Of course, an even ï¬ner direct sum decomposition of Cn is possible because
each Jordan segment is itself a block-diagonal matrix containing the individual
Jordan blocksâ€”the details are left to the interested reader.
Exercises for section 7.8
7.8.1. Find the Jordan form of the following matrix whose distinct eigenvalues
are Ïƒ (A) = {0, âˆ’1, 1}. Donâ€™t be frightened by the size of A.
A =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ’4
âˆ’5
âˆ’3
1
âˆ’2
0
1
âˆ’2
4
7
3
âˆ’1
3
0
âˆ’1
2
0
âˆ’1
0
0
0
0
0
0
âˆ’1
1
2
âˆ’4
2
0
âˆ’3
1
âˆ’8
âˆ’14
âˆ’5
1
âˆ’6
0
1
âˆ’4
4
7
4
âˆ’3
3
âˆ’1
âˆ’3
4
2
âˆ’2
âˆ’2
5
âˆ’3
0
4
âˆ’1
6
7
3
0
2
0
0
3
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
7.8.2. For the matrix
A =
 
3
0
1
âˆ’4
1
âˆ’2
âˆ’4
0
âˆ’1
!
that was used in Example 7.8.3, use
the technique described on p. 594 to construct a nonsingular matrix P
such that Pâˆ’1AP = J is in Jordan form.
7.8.3. Explain why index (Î») â‰¤alg mult (Î») for each Î» âˆˆÏƒ (AnÃ—n) .
7.8.4. Explain why index (Î») = 1 if and only if Î» is a semisimple eigenvalue.
7.8.5. Prove that every square matrix is similar to its transpose. Hint: Con-
sider the â€œreversal matrixâ€ R =
ï£«
ï£¬
ï£­
1
1
...
1
ï£¶
ï£·
ï£¸obtained by reversing the
order of the rows (or the columns) of the identity matrix I.

7.8 Jordan Form
597
7.8.6. Cayleyâ€“Hamilton Revisited. Prove the the Cayleyâ€“Hamilton theo-
rem (pp. 509, 532) by means of the Jordan form; i.e., prove that every
A âˆˆCnÃ—n satisï¬es its own characteristic equation.
7.8.7. Prove that if Î» is an eigenvalue of A âˆˆCnÃ—n such that index (Î») = k
and alg multA (Î») = m, then dim N

(A âˆ’Î»I)k
= m. Is it also true
that dim N

(A âˆ’Î»I)m
= m?
7.8.8. Let Î»j be an eigenvalue of A with index (Î»j) = kj. Prove that if
Mi(Î»j) = R

(A âˆ’Î»jI)i
âˆ©N (A âˆ’Î»jI), then
0 = Mkj(Î»j) âŠ†Mkjâˆ’1(Î»j) âŠ†Â· Â· Â· âŠ†M0(Î»j) = N (A âˆ’Î»jI).
7.8.9. Explain why (Aâˆ’Î»jI)ix = b(Î»j) must be a consistent system whenever
Î»j âˆˆÏƒ (A) and b(Î»j) âˆˆSi(Î»j), where b(Î»j) and Si(Î»j) are as deï¬ned
on p. 594.
7.8.10. Does the result of Exercise 7.7.5 extend to nonnilpotent matrices? That
is, if Î» âˆˆÏƒ (A) with index (Î») = k > 1, is Mkâˆ’1 = R

(A âˆ’Î»I)kâˆ’1
?
7.8.11. As deï¬ned in Exercise 5.8.15 (p. 380) and mentioned in Exercise 7.6.10
(p. 573), the Kronecker
80 product (sometimes called tensor product,
80
Leopold Kronecker (1823â€“1891) was born in Liegnitz, Prussia (now Legnica, Poland), to a
wealthy business family that hired private tutors to educate him until he enrolled at Gymna-
sium at Liegnitz where his mathematical talents were recognized by Eduard Kummer (1810â€“
1893), who became his mentor and lifelong colleague. Kronecker went to Berlin University
in 1841 to earn his doctorate, writing on algebraic number theory, under the supervision of
Dirichlet (p. 563). Rather than pursuing a standard academic career, Kronecker returned to
Liegnitz to marry his cousin and become involved in his uncleâ€™s banking business. But he never
lost his enjoyment of mathematics. After estate and business interests were left to others in
1855, Kronecker joined Kummer in Berlin who had just arrived to occupy the position vacated
by Dirichletâ€™s move to GÂ¨ottingen. Kronecker didnâ€™t need a salary, so he didnâ€™t teach or hold a
university appointment, but his research activities led to his election to the Berlin Academy in
1860. He declined the oï¬€er of the mathematics chair in GÂ¨ottingen in 1868, but he eventually
accepted the chair in Berlin that was vacated upon Kummerâ€™s retirement in 1883. Kronecker
held the unconventional view that mathematics should be reduced to arguments that involve
only integers and a ï¬nite number of steps, and he questioned the validity of nonconstructive
existence proofs, so he didnâ€™t like the use of irrational or transcendental numbers. Kronecker be-
came famous for saying that â€œGod created the integers, all else is the work of man.â€ Kroneckerâ€™s
signiï¬cant inï¬‚uence led to animosity with people of diï¬€ering philosophies such as Georg Cantor
(1845â€“1918), whose publications Kronecker tried to block. Kroneckerâ€™s small physical size was
another sensitive issue. After Hermann Schwarz (p. 271), who was Kummerâ€™s son-in-law and
a student of Weierstrass (p. 589), tried to make a joke involving Weierstrassâ€™s large physique
by stating that â€œhe who does not honor the Smaller, is not worthy of the Greater,â€ Kronecker
had no further dealings with Schwarz.

598
Chapter 7
Eigenvalues and Eigenvectors
or direct product) of AmÃ—n and BpÃ—q is the mp Ã— nq matrix
A âŠ—B =
ï£«
ï£¬
ï£¬
ï£­
a11B
a12B
Â· Â· Â·
a1nB
a21B
a22B
Â· Â· Â·
a2nB
...
...
...
...
am1B
am2B
Â· Â· Â·
amnB
ï£¶
ï£·
ï£·
ï£¸.
(a)
Assuming conformability, establish the following properties.
â—¦
A âŠ—(B âŠ—C) = (A âŠ—B) âŠ—C.
â—¦
A âŠ—(B + C) = (A âŠ—B) + (A âŠ—C).
â—¦
(A + B) âŠ—C = (A âŠ—C) + (B âŠ—C).
â—¦
(A1 âŠ—B1)(A2 âŠ—B2) Â· Â· Â· (Ak âŠ—Bk) = (A1 Â· Â· Â· Ak) âŠ—(B1 Â· Â· Â· Bk).
â—¦
(A âŠ—B)âˆ—= Aâˆ—+ Bâˆ—.
â—¦
rank (A âŠ—B) = (rank (A))(rank (B)).
Assume A is m Ã— m and B is n Ã— n for the following.
â—¦
trace (A âŠ—B) = (trace (A))(trace (B)).
â—¦
(A âŠ—In)(Im âŠ—B) = A âŠ—B = (Im âŠ—B)(A âŠ—In).
â—¦
det (A âŠ—B) = (det (A))m(det (B))n.
â—¦
(A âŠ—B)âˆ’1 = Aâˆ’1 âŠ—Bâˆ’1.
(b)
Let the eigenvalues of AmÃ—m be denoted by Î»i and let the eigenvalues
of BnÃ—n be denoted by Âµj. Prove the following.
â—¦The eigenvalues of A âŠ—B are the mn numbers {Î»iÂµj} m
i=1
n
j=1.
â—¦The eigenvalues of (A âŠ—In) + (Im âŠ—B) are {Î»i + Âµj} m
i=1
n
j=1.
7.8.12. Use part (b) of Exercise 7.8.11 along with the result of Exercise 7.6.10
(p. 573) to construct an alternate derivation of (7.6.8) on p. 566. That
is, show that the n2 eigenvalues of the discrete Laplacian Ln2Ã—n2 de-
scribed in Example 7.6.2 (p. 563) are given by
Î»ij = 4
%
sin2

iÏ€
2(n + 1)

+ sin2

jÏ€
2(n + 1)
&
,
i, j = 1, 2, . . . , n.
Hint: Recall Exercise 7.2.18 (p. 522).
7.8.13. Determine the eigenvalues of the three-dimensional discrete Laplacian
by using the formula from Exercise 7.6.10 (p. 573) that states
Ln3Ã—n3 = (In âŠ—In âŠ—An) + (In âŠ—An âŠ—In) + (An âŠ—In âŠ—In).

7.9 Functions of Nondiagonalizable Matrices
599
7.9
FUNCTIONS OF NONDIAGONALIZABLE MATRICES
The development of functions of nondiagonalizable matrices parallels the devel-
opment for functions of diagonal matrices that was presented in Â§7.3 except that
the Jordan form is used in place of the diagonal matrix of eigenvalues. Recall
from the discussion surrounding (7.3.5) on p. 526 that if A âˆˆCnÃ—n is diago-
nalizable, say A = PDPâˆ’1, where D = diag (Î»1I, Î»2I, . . . , Î»sI) , and if f(Î»i)
exists for each Î»i, then f(A) is deï¬ned to be
f(A) = Pf(D)Pâˆ’1 = P
ï£«
ï£¬
ï£­
f(Î»1)I
0
Â· Â· Â·
0
0
f(Î»2)I
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
f(Î»s)I
ï£¶
ï£·
ï£¸Pâˆ’1.
The Jordan decomposition A = PJPâˆ’1 described on p. 590 easily provides a
generalization of this idea to nondiagonalizable matrices. If J is the Jordan form
for A, itâ€™s natural to deï¬ne f(A) by writing f(A) = Pf(J)Pâˆ’1. However,
there are a couple of wrinkles that need to be ironed out before this notion
actually makes sense. First, we have to specify what we mean by f(J)â€”this is
not as clear as f(D) is for diagonal matrices. And after this is taken care of
we need to make sure that Pf(J)Pâˆ’1 is a uniquely deï¬ned matrix. This also
is not clear because, as mentioned on p. 590, the transforming matrix P is not
uniqueâ€”it would not be good if for a given A you used one P, and I used
another, and this resulted in your f(A) being diï¬€erent than mine.
Letâ€™s ï¬rst make sense of f(J). Assume throughout that A = PJPâˆ’1 âˆˆCnÃ—n
with Ïƒ (A) = {Î»1, Î»2, . . . , Î»s} and where J = diag (J(Î»1), J(Î»2), . . . , J(Î»s)) is
the Jordan form for A in which each segment J(Î»j) is a block-diagonal matrix
containing one or more Jordan blocks. That is,
J(Î»j) =
ï£«
ï£¬
ï£­
J1(Î»j)
0
Â· Â· Â·
0
0
J2(Î»j)Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â· Jtj(Î»j)
ï£¶
ï£·
ï£¸
with
Jâ‹†(Î»j) =
ï£«
ï£¬
ï£¬
ï£­
Î»j
1
...
...
...
1
Î»j
ï£¶
ï£·
ï£·
ï£¸.
We want to deï¬ne f(J) to be
f(J) =
ï£«
ï£¬
ï£­
f
J(Î»1)
...
f
J(Î»s)
ï£¶
ï£·
ï£¸
with
f

J(Î»j)

=
ï£«
ï£¬
ï£­
...
f
Jâ‹†(Î»j)
...
ï£¶
ï£·
ï£¸,
but doing so requires that we give meaning to f

Jâ‹†(Î»j)

. To keep the notation
from getting out of hand, let Jâ‹†=
 Î»
1
...
...
Î»
!
denote a generic k Ã— k Jordan

600
Chapter 7
Eigenvalues and Eigenvectors
block, and letâ€™s develop a deï¬nition of f(Jâ‹†). Suppose for a moment that f(z)
is a function from C into C that has a Taylor series expansion about Î». That
is, for some r > 0,
f(z) = f(Î»)+f â€²(Î»)(zâˆ’Î»)+f â€²â€²(Î»)
2!
(zâˆ’Î»)2+f â€²â€²â€²(Î»)
3!
(zâˆ’Î»)3+ Â· Â· Â·
for
|zâˆ’Î»| < r.
The representation (7.3.7) on p. 527 suggests that f(Jâ‹†) should be deï¬ned as
f(Jâ‹†) = f(Î»)I + f â€²(Î»)(Jâ‹†âˆ’Î»I) + f â€²â€²(Î»)
2!
(Jâ‹†âˆ’Î»I)2 + f â€²â€²â€²(Î»)
3!
(Jâ‹†âˆ’Î»I)3 + Â· Â· Â· .
But since N = Jâ‹†âˆ’Î»I is nilpotent of index k, this series is just the ï¬nite sum
f(Jâ‹†) =
kâˆ’1

i=0
f (i)(Î»)
i!
Ni,
(7.9.1)
and this means that only f(Î»), f â€²(Î»), . . . , f (kâˆ’1)(Î») are required to exist. Also,
N=
ï£«
ï£¬
ï£­
0
1
...
...
...
1
0
ï£¶
ï£·
ï£¸,
N2=
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
0
1
...
...
...
...
...
1
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£¸, . . . , Nkâˆ’1=
ï£«
ï£¬
ï£­
0
0
Â· Â· Â·
1
...
...
...
0
0
ï£¶
ï£·
ï£¸,
so the representation of f(Jâ‹†) in (7.9.1) can be elegantly expressed as follows.
Functions of Jordan Blocks
For a k Ã— k Jordan block Jâ‹†with eigenvalue Î», and for a function
f(z) such that f(Î»), f â€²(Î»), . . . , f (kâˆ’1)(Î») exist, f(Jâ‹†) is deï¬ned to be
f(Jâ‹†) = f
ï£«
ï£¬
ï£­
Î»
1
...
...
...
1
Î»
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
f(Î»)
f â€²(Î»)
f â€²â€²(Î»)
2!
Â· Â· Â· f (kâˆ’1)(Î»)
(k âˆ’1)!
f(Î»)
f â€²(Î»)
...
...
...
...
f â€²â€²(Î»)
2!
f(Î»)
f â€²(Î»)
f(Î»)
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
(7.9.2)

7.9 Functions of Nondiagonalizable Matrices
601
Every Jordan form J =
 
... Jâ‹†...
!
is a block-diagonal matrix composed of
various Jordan blocks Jâ‹†, so (7.9.2) allows us to deï¬ne
f(J) =
 
...f(Jâ‹†)...
!
as
long as we pay attention to the fact that a suï¬ƒcient number of derivatives of f
are required to exist at the various eigenvalues. More precisely, if the size of the
largest Jordan block associated with an eigenvalue Î» is k (i.e., if index (Î») = k),
then f(Î»), f â€²(Î»), . . . , f (kâˆ’1)(Î») must exist in order for f(J) to make sense.
Matrix Functions
For A âˆˆCnÃ—n with Ïƒ (A) = {Î»1, Î»2, . . . , Î»s} , let ki = index (Î»i).
â€¢
A function f : C â†’C is said to be deï¬ned (or to exist) at A when
f(Î»i), f â€²(Î»i), . . . , f (kiâˆ’1)(Î»i) exist for each Î»i âˆˆÏƒ (A) .
â€¢
Suppose that A = PJPâˆ’1, where J =
 
... Jâ‹†...
!
is in Jordan form
with the Jâ‹†â€™s representing the various Jordan blocks described on
p. 590. If f exists at A, then the value of f at A is deï¬ned to be
f(A) = Pf(J)Pâˆ’1 = P
ï£«
ï£­
...
f(Jâ‹†)...
ï£¶
ï£¸Pâˆ’1,
(7.9.3)
where the f(Jâ‹†) â€™s are as deï¬ned in (7.9.2).
We still need to explain why (7.9.3) produces a uniquely deï¬ned matrix.
The following argument will not only accomplish this purpose, but it will also
establish an alternate expression for f(A) that involves neither the Jordan form
J nor the transforming matrix P. Begin by partitioning J into its s Jordan
segments as described on p. 590, and partition P and Pâˆ’1 conformably as
P =

P1 | Â· Â· Â· | Ps

,
J =
ï£«
ï£­
J(Î»1)
...
J(Î»s)
ï£¶
ï£¸,
and
Pâˆ’1 =
ï£«
ï£¬
ï£­
Q1...
Qs
ï£¶
ï£·
ï£¸.
Deï¬ne Gi = PiQi, and observe that if ki = index (Î»i), then Gi is the pro-
jector onto N

(A âˆ’Î»iI)ki
along R

(A âˆ’Î»iI)ki
. To see this, notice that
Li = J(Î»i) âˆ’Î»iI is nilpotent of index ki, but J(Î»j) âˆ’Î»iI is nonsingular when

602
Chapter 7
Eigenvalues and Eigenvectors
i Ì¸= j, so
(A âˆ’Î»iI) = P(J âˆ’Î»iI)Pâˆ’1 = P
ï£«
ï£¬
ï£¬
ï£¬
ï£­
J(Î»1) âˆ’Î»iI
...
Li
...
J(Î»s) âˆ’Î»iI
ï£¶
ï£·
ï£·
ï£·
ï£¸Pâˆ’1
(7.9.4)
is a core-nilpotent decomposition as described on p. 397 (reordering the eigenval-
ues can put the nilpotent block Li on the bottom to realize the form in (5.10.5)).
Consequently, the results in Example 5.10.3 (p. 398) insure that PiQi = Gi is
the projector onto N

(A âˆ’Î»iI)ki
along R

(A âˆ’Î»iI)ki
, and this is true for
all similarity transformations that reduce A to J. If A happens to be diago-
nalizable, then ki = 1 for each i, and the matrices Gi = PiQi are precisely
the spectral projectors deï¬ned on p. 517. For this reason, there is no ambigu-
ity in continuing to use the Gi notation, and we will continue to refer to the
Gi â€™s as spectral projectors. In the diagonalizable case, Gi projects onto the
eigenspace associated with Î»i, and in the nondiagonalizable case Gi projects
onto the generalized eigenspace associated with Î»i.
Now consider
f(A) = Pf(J)Pâˆ’1 = P
ï£«
ï£¬
ï£­
f
J(Î»1)
...
f
J(Î»s)
ï£¶
ï£·
ï£¸Pâˆ’1 =
s

i=1
Pif

J(Î»i)

Qi.
(7.9.5)
Since f

J(Î»i)

=
ï£«
ï£­
...
f

Jâ‹†(Î»i)

...
ï£¶
ï£¸, where the Jâ‹†(Î»i) â€™s are the Jordan blocks
associated with Î»i, (7.9.2) insures that if ki = index (Î»i), then
f

J(Î»i)

= f(Î»i)I + f â€²(Î»i)Li + f â€²â€²(Î»i)
2!
L2
i + Â· Â· Â· + f (kiâˆ’1)(Î»i)
(ki âˆ’1)! Lkiâˆ’1
i
,
where Li = J(Î»i) âˆ’Î»iI, and thus (7.9.5) becomes
f(A) =
s

i=1
Pif

J(Î»i)

Qi =
s

i=1
kiâˆ’1

j=0
f (j)(Î»i)
j!
PiLj
iQi.
(7.9.6)
The terms PiLj
iQi can be simpliï¬ed by noticing that
Pâˆ’1P = I =â‡’QiPj =
	
I
if i = j,
0
if i Ì¸= j, =â‡’Pâˆ’1Gi =
ï£«
ï£¬
ï£­
Q1
...
Qi...
Qs
ï£¶
ï£·
ï£¸PiQi =
ï£«
ï£¬
ï£­
0...
Qi...0
ï£¶
ï£·
ï£¸,

7.9 Functions of Nondiagonalizable Matrices
603
and by using this with (7.9.4) to conclude that
(A âˆ’Î»iI)jGi = P
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­

J(Î»1) âˆ’Î»iIj
...
Lj
i
...

J(Î»s) âˆ’Î»iIj
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
Pâˆ’1Gi = PiLj
iQi.
(7.9.7)
Thus (7.9.6) can be written as
f(A) =
s

i=1
kiâˆ’1

j=0
f (j)(Î»i)
j!
(A âˆ’Î»iI)jGi,
(7.9.8)
and this expression is independent of which similarity is used to reduce A to J.
Not only does (7.9.8) prove that f(A) is uniquely deï¬ned, but it also provides
a generalization of the spectral theorems for diagonalizable matrices given on
pp. 517 and 526 because if A is diagonalizable, then each ki = 1 so that (7.9.8)
reduces to (7.3.6) on p. 526. Below is a formal summary along with some related
properties.
Spectral Resolution of f(A)
For A âˆˆCnÃ—n with Ïƒ (A) = {Î»1, Î»2, . . . , Î»s} such that ki = index (Î»i),
and for a function f : C â†’C such that f(Î»i), f â€²(Î»i), . . . , f (kiâˆ’1)(Î»i)
exist for each Î»i âˆˆÏƒ (A) , the value of f(A) is
f(A) =
s

i=1
kiâˆ’1

j=0
f (j)(Î»i)
j!
(A âˆ’Î»iI)jGi,
(7.9.9)
where the spectral projectors Gi â€™s have the following properties.
â€¢
Gi is the projector onto the generalized eigenspace N

(A âˆ’Î»iI)ki
along R

(A âˆ’Î»iI)ki
.
â€¢
G1 + G2 + Â· Â· Â· + Gs = I.
(7.9.10)
â€¢
GiGj = 0 when i Ì¸= j.
(7.9.11)
â€¢
Ni = (A âˆ’Î»iI)Gi = Gi(A âˆ’Î»iI) is nilpotent of index ki. (7.9.12)
â€¢
If A is diagonalizable, then (7.9.9) reduces to (7.3.6) on p. 526, and
the spectral projectors reduce to those described on p. 517.

604
Chapter 7
Eigenvalues and Eigenvectors
Proof of (7.9.10)â€“(7.9.12).
Property (7.9.10) results from using (7.9.9) with the
function f(z) = 1, and property (7.9.11) is a consequence of
I = Pâˆ’1P
=â‡’
QiPj =
	
I
if i = j,
0
if i Ì¸= j.
(7.9.13)
To prove (7.9.12), establish that (A âˆ’Î»iI)Gi = Gi(A âˆ’Î»iI) by noting that
(7.9.13) implies Pâˆ’1Gi =

0 Â· Â· Â· Qi Â· Â· Â· 0
T and GiP =

0 Â· Â· Â· Pi Â· Â· Â· 0

. Use this
with (7.9.4) to observe that (A âˆ’Î»iI)Gi = PiLiQi = Gi(A âˆ’Î»iI). Now
Nj
i = (PiLiQi)j = PiLj
iQi
for j = 1, 2, 3, . . . ,
and thus Ni is nilpotent of index ki because Li is nilpotent of index ki.
Example 7.9.1
A coordinate-free version of the representation in (7.9.3) results by separating
the ï¬rst-order terms in (7.9.9) from the higher-order terms to write
f(A) =
s

i=1
ï£®
ï£°f(Î»i)Gi +
kiâˆ’1

j=1
f (j)(Î»i)
j!
Nj
i
ï£¹
ï£».
Using the identity function f(z) = z produces a coordinate-free version of the
Jordan decomposition of A in the form
A =
s

i=1

Î»iGi + Ni

,
and this is the extension of (7.2.7) on p. 517 to the nondiagonalizable case.
Another version of (7.9.9) results from lumping things into one matrix to write
f(A) =
s

i=1
kiâˆ’1

j=0
f (j)(Î»i)Zij,
where
Zij = (A âˆ’Î»iI)jGi
j!
.
(7.9.14)
The Zij â€™s are often called the component matrices or the constituent matrices.
Example 7.9.2
Problem: Describe f(A) for functions f deï¬ned at A =

6
2
8
âˆ’2
2
âˆ’2
0
0
2

.
Solution: A is block triangular, so itâ€™s easy to see that Î»1 = 2 and Î»2 = 4
are the two distinct eigenvalues with index (Î»1) = 1 and index (Î»2) = 2. Thus
f(A) exists for all functions such that f(2), f(4), and f â€²(4) exist, in which case
f(A) = f(2)G1 + f(4)G2 + f â€²(4)(A âˆ’4I)G2.
The spectral projectors could be computed directly, but things are easier if some
judicious choices of f are made. For example,
	
f(z) = 1
â‡’
I
= f(A) = G1 + G2
f(z) = (z âˆ’4)2 â‡’(A âˆ’4I)2 = f(A) = 4G1

=â‡’
G1 = (A âˆ’4I)2/4,
G2 = I âˆ’G1.

7.9 Functions of Nondiagonalizable Matrices
605
Now that the spectral projectors are known, any function deï¬ned at A can be
evaluated. For example, if f(z) = z1/2, then
f(A) =
âˆš
A =
âˆš
2G1 +
âˆš
4G2 + (1/2
âˆš
4)(A âˆ’4I)G2 = 1
2
ï£«
ï£­
5
1
7 âˆ’2
âˆš
2
âˆ’1
3
5 âˆ’4
âˆš
2
0
0
2
âˆš
2
ï£¶
ï£¸.
This technique illustrated above is rather ad hoc, but it always works if a suf-
ï¬cient number of appropriate functions are used. For example, using f(z) = zp
for p = 0, 1, 2, . . . will always produce a system of equations that will yield the
component matrices Zij given in (7.9.14) because
for f(z) = 1:
I =  Zi0,
for f(z) = z :
A =  Î»iZi0 +  Zi1,
for f(z) = z2 :
A2 =  Î»2
i Zi0 +  2Î»iZi1 +  2Zi2,
...
and this can be considered as a generalized Vandermonde linear system (p. 185)
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
Â· Â· Â·
1
Î»1
Â· Â· Â·
Î»s
1
Â· Â· Â·
1
Î»2
1
Â· Â· Â·
Î»2
s
2Î»1
Â· Â· Â·
2Î»s
2
Â· Â· Â·
2
...
...
...
...
...
...
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Z10
...
Zs0
Z11
...
Zs1
Z21
...
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
I
A
A2
A3
...
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
that can be solved for the Zij â€™s. Other sets of polynomials such as
{1, (z âˆ’Î»1)k1, (z âˆ’Î»1)k2(z âˆ’Î»2)k2, . . . (z âˆ’Î»1)k1 Â· Â· Â· (z âˆ’Î»s)ks}
will generate other linear systems that yield solutions containing the Zij â€™s.
Example 7.9.3
Series Representations. Suppose that âˆ
j=0 cj(z âˆ’z0)j converges to f(z) at
each point inside a circle |z âˆ’z0| = r, and suppose that A is a matrix such
that |Î»i âˆ’z0| < r for each eigenvalue Î»i âˆˆÏƒ (A) .
Problem: Explain why âˆ
j=0 cj(A âˆ’z0I)j converges to f(A).
Solution: If Pâˆ’1AP = J is in Jordan form as described on p. 601, then itâ€™s
not diï¬ƒcult to argue that âˆ
j=0 cj(A âˆ’z0I)j converges if and only if
Pâˆ’1
âˆ

j=0
cj(Aâˆ’z0I)j
P=
âˆ

j=0
cjPâˆ’1(Aâˆ’z0I)jP=
âˆ

j=0
cj(Jâˆ’z0I)j =
ï£«
ï£¬
ï£¬
ï£­
...
âˆ

j=0
cj(Jâ‹†âˆ’z0I)j
...
ï£¶
ï£·
ï£·
ï£¸

606
Chapter 7
Eigenvalues and Eigenvectors
converges. Consequently, it suï¬ƒces to prove that âˆ
j=0 cj(Jâ‹†âˆ’z0I)j converges
to f(Jâ‹†) for a generic k Ã— k Jordan block
Jâ‹†=
 Î»
1
...
...
Î»
!
= Î»I + N,
where
N =
 0
1
...
...
0
!
kÃ—k
.
A standard theorem from analysis states that if âˆ
j=0 cj(z âˆ’z0)j converges to
f(z) when |z âˆ’z0| < r, then the series may be diï¬€erentiated term by term
to yield series that converge to derivatives of f at points inside the circle of
convergence. Consequently, for each i = 0, 1, 2, . . . ,
f (i)(z)
i!
=
âˆ

j=0
cj
j
i

(z âˆ’z0)jâˆ’i
when
|z âˆ’z0| < r.
(7.9.15)
We know from (7.9.1) (with f(z) = zj) that
(Jâ‹†âˆ’z0I)j = (Î»âˆ’z0)jI+
j
1

(Î»âˆ’z0)jâˆ’1N+Â· Â· Â·+

j
k âˆ’1

(Î»âˆ’z0)jâˆ’(kâˆ’1)Nkâˆ’1,
so this together with (7.9.15) produces
âˆ

j=0
cj(Jâ‹†âˆ’z0I)j =
ï£«
ï£­
âˆ

j=0
cj(Î» âˆ’z0)j
ï£¶
ï£¸I +
ï£«
ï£­
âˆ

j=0
cj
j
1

(Î» âˆ’z0)jâˆ’1
ï£¶
ï£¸N
+ Â· Â· Â· +
ï£«
ï£­
âˆ

j=0
cj

j
k âˆ’1

(Î» âˆ’z0)jâˆ’(kâˆ’1)
ï£¶
ï£¸Nkâˆ’1
= f(Î»)I + f â€²(Î»)N + Â· Â· Â· + f (kâˆ’1)
(k âˆ’1)!(Î»)Nkâˆ’1 = f(Jâˆ—).
Note: The result of this example validates the statements made on p. 527.
Example 7.9.4
All Matrix Functions Are Polynomials.
It was pointed out on p. 528
that if A is diagonalizable, and if f(A) exists, then there is a polynomial
p(z) such that f(A) = p(A), and you were asked in Exercise
7.3.7 (p. 539)
to use the Cayleyâ€“Hamilton theorem (pp. 509, 532) to extend this property to
nondiagonalizable matrices for functions that have an inï¬nite series expansion.
We can now see why this is true in general.
Problem: For a function f deï¬ned at A âˆˆCnÃ—n, exhibit a polynomial p(z)
such that f(A) = p(A).

7.9 Functions of Nondiagonalizable Matrices
607
Solution: Suppose that Ïƒ (A) = {Î»1, Î»2, . . . , Î»s} with index (Î»i) = ki. The
trick is to ï¬nd a polynomial p(z) such that for each i = 1, 2, . . . , s,
p(Î»i) = f(Î»i),
pâ€²(Î»i) = f â€²(Î»i),
. . . ,
p(kiâˆ’1)(Î»i) = f (kiâˆ’1)(Î»i)
(7.9.16)
because if such a polynomial exists, then (7.9.9) guarantees that
p(A) =
s

i=1
kiâˆ’1

j=0
p(j)(Î»i)
j!
(A âˆ’Î»iI)jGi =
s

i=1
kiâˆ’1

j=0
f (j)(Î»i)
j!
(A âˆ’Î»iI)jGi = f(A).
Since there are k = s
i=1 ki equations in (7.9.16) to be satisï¬ed, letâ€™s look for
a polynomial of the form
p(z) = Î±0 + Î±1z + Î±2z2 + Â· Â· Â· + Î±kâˆ’1zkâˆ’1
by writing the equations in (7.9.16) as the following k Ã— k linear system Hx = f :
p(Î»1) = f(Î»1)
...
p(Î»s) = f(Î»s)
...
pâ€²(Î»i) = f â€²(Î»i)
...
...
pâ€²â€²(Î»i) = f â€²â€²(Î»i)
...
...
â‡’
â‡’
â‡’
â‡’
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
Î»1
Î»2
1
Î»3
1
Â· Â· Â·
Î»kâˆ’1
1
...
...
...
...
...
1
Î»s
Î»2
s
Î»3
s
Â· Â· Â·
Î»kâˆ’1
s
...
...
...
...
...
0
1
2Î»i
3Î»2
i
Â· Â· Â·
(k âˆ’1)Î»kâˆ’2
i
...
...
...
...
...
...
...
...
...
...
0
0
2
6Î»i
Â· Â· Â·
(k âˆ’1)(k âˆ’2)Î»(kâˆ’3)
i
...
...
...
...
...
...
...
...
...
...
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î±0
Î±1
Î±2
Î±3
...
...
...
Î±kâˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
f(Î»1)
...
f(Î»s)
...
f â€²(Î»i)
...
...
f â€²â€²(Î»i)
...
...
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
The coeï¬ƒcient matrix H can be proven to be nonsingular because the rows in
each segment of H are linearly independent. The rows in the top segment of
H are a subset of rows from a Vandermonde matrix (p. 185), while the nonzero
portion of each succeeding segment has the form VD, where the rows of V are
a subset of rows from a Vandermonde matrix and D is a nonsingular diagonal
matrix. Consequently, Hx = f has a unique solution, and thus there is a unique
polynomial p(z) = Î±0 + Î±1z + Î±2z2 + Â· Â· Â· + Î±kâˆ’1zkâˆ’1 that satisï¬es the condi-
tions in (7.9.16). This polynomial p(z) is called the Hermite interpolation
polynomial, and it has the property that f(A) = p(A).

608
Chapter 7
Eigenvalues and Eigenvectors
Example 7.9.5
Functional Identities.
Scalar functional identities generally extend to the
matrix case. For example, the scalar identity sin2 z + cos2 z = 1 extends to
matrices as sin2 Z + cos2 Z = I, and this is valid for all Z âˆˆCnÃ—n. While
itâ€™s possible to prove such identities on a case-by-case basis by using (7.9.3) or
(7.9.9), there is a more robust approach that is described below.
For two functions f1 and f2 from C into C and for a polynomial p(x, y) in
two variables, let h be the composition deï¬ned by h(z) = p

f1(z), f2(z)

. If
AnÃ—n has eigenvalues Ïƒ (A) = {Î»1, Î»2, . . . , Î»s} with index (Î»i) = ki, and if h
is deï¬ned at A, then we are allowed to assert that h(A) = p

f1(A), f2(A)

because Example 7.9.4 insures that there are polynomials g(z) and q(z) such
that h(A) = g(A) and p

f1(A), f2(A)

= q(A), where for each Î»i âˆˆÏƒ (A) ,
g(j)(Î»i) = h(j)(Î»i) = dj
p

f1(z), f2(z)

dzj

z=Î»i
= q(j)(Î»i)
for j = 0, 1, . . . , ki âˆ’1,
so g(A) = q(A), and thus h(A) = p

f1(A), f2(A)

. To build functional iden-
tities for A, choose f1 and f2 in h(z) = p

f1(z), f2(z)

that will make
h(Î»i) = hâ€²(Î»i) = hâ€²â€²(Î»i) = Â· Â· Â· = h(kiâˆ’1)(Î»i) = 0
for each
Î»i âˆˆÏƒ (A) ,
thereby insuring that 0 = h(A) = p

f1(A), f2(A)

. This technique produces a
plethora of functional identities. For example, using
ï£±
ï£²
ï£³
f1(z) = sin2 z
f2(z) = cos2 z
p(x, y) = x2 + y2 âˆ’1
ï£¼
ï£½
ï£¾produces h(z) = p

f1(z), f2(z)

= sin2 z + cos2 z âˆ’1.
Since h(z) = 0 for all z âˆˆC, it follows that h(Z) = 0 for all Z âˆˆCnÃ—n, and
thus sin2 Z+cos2 Z = I for all Z âˆˆCnÃ—n. Itâ€™s evident that this technique can be
extended to include any number of functions f1, f2, . . . , fm with a polynomial
p(x1, x2, . . . , xm) to produce even more complicated relationships.
Example 7.9.6
Systems of Diï¬€erential Equations Revisited.
The purpose here is to ex-
tend the discussion in Â§7.4 to cover the nondiagonalizable case. Write the system
of diï¬€erential equations in (7.4.1) on p. 541 in matrix form as
uâ€²(t) = AnÃ—nu(t)
with
u(0) = c,
(7.9.17)
but this time donâ€™t assume that AnÃ—n is diagonalizableâ€”suppose instead that
Ïƒ (A) = {Î»1, Î»2, . . . , Î»s} with index (Î»i) = ki. The development parallels that

7.9 Functions of Nondiagonalizable Matrices
609
for the diagonalizable case, but eAt is now a little more complicated than (7.4.2).
Using f(z) = ezt in (7.9.3) and (7.9.2) yields
eAt = P
 
...eJâ‹†t
...
!
Pâˆ’1 with eJâ‹†t =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
eÎ»t
teÎ»t
t2eÎ»t
2!
Â· Â· Â· tkâˆ’1eÎ»t
(k âˆ’1)!
eÎ»t
teÎ»t
...
...
...
...
t2eÎ»t
2!
eÎ»t
teÎ»t
eÎ»t
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
, (7.9.18)
while setting f(z) = ezt in (7.9.9) produces
eAt =
s

i=1
kiâˆ’1

j=0
tjeÎ»it
j!
(A âˆ’Î»iI)jGi.
(7.9.19)
Either of these can be used to show that the three properties (7.4.3)â€“(7.4.5)
on p. 541 still hold. In particular, d eAt/dt = AeAt = eAtA, so, just as in
the diagonalizable case, u(t) = eAtc is the unique solution of (7.9.17) (the
uniqueness argument given in Â§7.4 remains valid). In the diagonalizable case,
the solution of (7.9.17) involves only the eigenvalues and eigenvectors of A as
described in (7.4.7) on p. 542, but generalized eigenvectors are needed for the
nondiagonalizable case. Using (7.9.19) yields the solution to (7.9.17) as
u(t) = eAtc =
s

i=1
kiâˆ’1

j=0
tjeÎ»it
j!
vj(Î»i),
where vj(Î»i) = (A âˆ’Î»iI)jGic. (7.9.20)
Each vkiâˆ’1(Î»i) is an eigenvector associated with Î»i because (A âˆ’Î»iI)kiGi = 0,
and {vkiâˆ’2(Î»i), . . . , v1(Î»i), v0(Î»i)} is an associated chain of generalized eigen-
vectors. The behavior of the solution (7.9.20) as t â†’âˆis similar but not
identical to that discussed on p. 544 because for Î» = x + iy and t > 0,
tjeÎ»t = tjext (cos yt + i sin yt) â†’
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³
0
if x < 0,
unbounded
if x â‰¥0 and j > 0,
oscillates indeï¬nitely
if x = j = 0 and y Ì¸= 0,
1
if x = y = j = 0.
In particular, if Re (Î»i) < 0 for every Î»i âˆˆÏƒ (A) , then u(t) â†’0 for every
initial vector c, in which case the system is said to be stable.
â€¢
Nonhomogeneous Systems.
It can be veriï¬ed by direct manipulation
that the solution of uâ€²(t) = Au(t) + f(t) with u(t0) = c is given by
u(t) = eA(tâˆ’t0)c +
4 t
t0
eA(tâˆ’Ï„)f(Ï„)dÏ„.

610
Chapter 7
Eigenvalues and Eigenvectors
Example 7.9.7
Nondiagonalizable Mixing Problem.
To make the point that even simple
problems in nature can be nondiagonalizable, consider three V gallon tanks as
shown in Figure 7.9.1 that are initially full of polluted water in which the ith
tank contains ci lbs of a pollutant. In an attempt to ï¬‚ush the pollutant out, all
spigots are opened at once allowing fresh water at the rate of r gal/sec to ï¬‚ow
into the top of tank #3, while r gal/sec ï¬‚ow from its bottom into the top of
tank #2, and so on.
r  gal/sec
r  gal/sec
r  gal/sec
r  gal/sec
3
2
1
Fresh
Figure 7.9.1
Problem: How many pounds of the pollutant are in each tank at any ï¬nite time
t > 0 when instantaneous and continuous mixing occurs?
Solution: If ui(t) denotes the number of pounds of pollutant in tank i at time
t > 0, then the concentration of pollutant in tank i at time t is ui(t)/V lbs/gal,
so the model uâ€²
i(t) = (lbs/sec) coming inâˆ’(lbs/sec) going out produces the non-
diagonalizable system:
ï£«
ï£¬
ï£­
uâ€²
1(t)
uâ€²
2(t)
uâ€²
3(t)
ï£¶
ï£·
ï£¸= r
V
ï£«
ï£¬
ï£­
âˆ’1
1
0
0
âˆ’1
1
0
0
âˆ’1
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
u1(t)
u2(t)
u3(t)
ï£¶
ï£·
ï£¸, or uâ€² =Au with u(0)=c=
ï£«
ï£­
c1
c2
c3
ï£¶
ï£¸.
This setup is almost the same as that in Exercise 3.5.11 (p. 104). Notice that
A is simply a scalar multiple of a single Jordan block
Jâ‹†=
 âˆ’1
1
0
0
âˆ’1
1
0
0
âˆ’1
!
, so
eAt is easily determined by replacing t by rt/V and Î» by âˆ’1 in the second
equation of (7.9.18) to produce
eAt = e(rt/V )Jâ‹†= eâˆ’rt/V
ï£«
ï£¬
ï£­
1
rt/V
(rt/V )2 /2
0
1
rt/V
0
0
1
ï£¶
ï£·
ï£¸.

7.9 Functions of Nondiagonalizable Matrices
611
Therefore,
u(t) = eAtc = eâˆ’rt/V
ï£«
ï£­
c1 + c2(rt/V ) + c3 (rt/V )2 /2
c2 + c3(rt/V )
c3
ï£¶
ï£¸,
and, just as common sense dictates, the pollutant is never completely ï¬‚ushed
from the tanks in ï¬nite time. Only in the limit does each ui â†’0, and itâ€™s clear
that the rate at which u1 â†’0 is slower than the rate at which u2 â†’0, which
in turn is slower than the rate at which u3 â†’0.
Example 7.9.8
The Cauchy integral formula is an elegant result from complex analysis
stating that if f : C â†’C is analytic in and on a simple closed contour Î“ âŠ‚C
with positive (counterclockwise) orientation, and if Î¾0 is interior to Î“, then
f(Î¾0) =
1
2Ï€i
4
Î“
f(Î¾)
Î¾ âˆ’Î¾0
dÎ¾
and
f (j)(Î¾0) = j!
2Ï€i
4
Î“
f(Î¾)
(Î¾ âˆ’Î¾0)j+1 dÎ¾.
(7.9.21)
These formulas produce analogous representations of matrix functions. Suppose
that A âˆˆCnÃ—n with Ïƒ (A) = {Î»1, Î»2, . . . , Î»s} and index (Î»i) = ki. For a
complex variable Î¾, the resolvent of A âˆˆCnÃ—n is deï¬ned to be the matrix
R(Î¾) = (Î¾I âˆ’A)âˆ’1.
If Î¾ Ì¸âˆˆÏƒ (A) , then r(z) = (Î¾ âˆ’z)âˆ’1 is deï¬ned at A with r(A) = R(Î¾), so the
spectral resolution theorem (p. 603) can be used to write
R(Î¾) =
s

i=1
kiâˆ’1

j=0
r(j)(Î»i)
j!
(A âˆ’Î»iI)jGi =
s

i=1
kiâˆ’1

j=0
1
(Î¾ âˆ’Î»i)j+1 (A âˆ’Î»iI)jGi.
If Ïƒ (A) is in the interior of a simple closed contour Î“, and if the contour
integral of a matrix is deï¬ned by entrywise integration, then (7.9.21) produces
1
2Ï€i
4
Î“
f(Î¾)(Î¾I âˆ’A)âˆ’1dÎ¾ =
1
2Ï€i
4
Î“
f(Î¾)R(Î¾)dÎ¾
=
1
2Ï€i
4
Î“
s

i=1
kiâˆ’1

j=0
f(Î¾)
(Î¾ âˆ’Î»i)j+1 (A âˆ’Î»iI)jGidÎ¾
=
s

i=1
kiâˆ’1

j=0
% 1
2Ï€i
4
Î“
f(Î¾)
(Î¾ âˆ’Î»i)j+1 dÎ¾
&
(A âˆ’Î»iI)jGi
=
s

i=1
kiâˆ’1

j=0
f (j)(Î»i)
j!
(A âˆ’Î»iI)jGi = f(A).

612
Chapter 7
Eigenvalues and Eigenvectors
â€¢
In other words, if Î“ is a simple closed contour containing Ïƒ (A) in its
interior, then
f(A) =
1
2Ï€i
4
Î“
f(Î¾)(Î¾I âˆ’A)âˆ’1dÎ¾
(7.9.22)
whenever f is analytic in and on Î“. Since this formula makes sense for
general linear operators, it is often adopted as a deï¬nition for f(A) in more
general settings.
â€¢
Furthermore, if Î“i is a simple closed contour enclosing Î»i but excluding all
other eigenvalues of A, then the ith spectral projector is given by
Gi =
1
2Ï€i
4
Î“i
R(Î¾)dÎ¾ =
1
2Ï€i
4
Î“i
(Î¾I âˆ’A)âˆ’1dÎ¾
(Exercise 7.9.19).
Exercises for section 7.9
7.9.1. Lake #i in a closed system of three lakes of equal volume V initially
contains ci lbs of a pollutant. If the water in the system is circulated
at rates (gal/sec) as indicated in Figure 7.9.2, ï¬nd the amount of pollu-
tant in each lake at time t > 0 (assume continuous mixing), and then
determine the pollution in each lake in the long run.
4r
2r
3r
r
2r
#3
#2
#1
Figure 7.9.2
7.9.2. Suppose that A âˆˆCnÃ—n has eigenvalues Î»i with index (Î»i) = ki. Ex-
plain why the ith spectral projector is given by
Gi = fi(A),
where
fi(z) =
5 1
when z = Î»i,
0
otherwise.
7.9.3. Explain why each spectral projector Gi can be expressed as a polyno-
mial in A.
7.9.4. If Ïƒ (AnÃ—n) = {Î»1, Î»2, . . . , Î»s} with ki = index (Î»i), explain why
Ak =
s

i=1
kiâˆ’1

j=0
k
j

Î»kâˆ’j
i
(A âˆ’Î»iI)jGi.

7.9 Functions of Nondiagonalizable Matrices
613
7.9.5. With the convention that
k
j

= 0 for j > k, explain why
ï£«
ï£­
Î»
1
...
...
Î»
ï£¶
ï£¸
k
mÃ—m
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î»k
k
1

Î»kâˆ’1
k
2

Î»kâˆ’2 Â· Â· Â· 
k
mâˆ’1

Î»kâˆ’m+1
Î»k
k
1

Î»kâˆ’1 ...
...
...
...
k
2

Î»kâˆ’2
Î»k
k
1

Î»kâˆ’1
Î»k
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
7.9.6. Determine eA for A =

6
2
8
âˆ’2
2
âˆ’2
0
0
2

.
7.9.7. For f(z) = 4âˆšz âˆ’1, determine f(A) when A =
 âˆ’3
âˆ’8
âˆ’9
5
11
9
âˆ’1
âˆ’2
1

.
7.9.8.
(a)
Explain why every nonsingular A âˆˆCnÃ—n has a square root.
(b)
Give necessary and suï¬ƒcient conditions for the existence of
âˆš
A
when A is singular.
7.9.9. Spectral Mapping Property.
Prove that if (Î», x) is an eigenpair
for A, then (f(Î»), x) is an eigenpair for f(A) whenever f(A) exists.
Does it also follow that alg multA (Î») = alg multf(A) (f(Î»))?
7.9.10. Let f be deï¬ned at A, and let Î» âˆˆÏƒ (A) . Give an example or an
explanation of why the following statements are not necessarily true.
(a)
f(A) is similar to A.
(b)
geo multA (Î») = geo multf(A) (f(Î»)) .
(c)
indexA(Î») = indexf(A)(f(Î»)).
7.9.11. Explain why Af(A) = f(A)A whenever f(A) exists.
7.9.12. Explain why a function f is deï¬ned at A âˆˆCnÃ—n if and only if f
is deï¬ned at AT , and then prove that f(AT ) =

f(A)
T . Why canâ€™t
(â‹†)âˆ—be used in place of (â‹†)T ?

614
Chapter 7
Eigenvalues and Eigenvectors
7.9.13. Use the technique of Example 7.9.5 (p. 608) to establish the following
identities.
(a)
eAeâˆ’A = I for all A âˆˆCnÃ—n.
(b)
eÎ±A =

eAÎ± for all Î± âˆˆC and A âˆˆCnÃ—n.
(c)
eiA = cos A + i sin A for all A âˆˆCnÃ—n.
7.9.14. (a)
Show that if AB = BA, then eA+B = eAeB.
(b)
Give an example to show that eA+B Ì¸= eAeB in general.
7.9.15. Find the Hermite interpolation polynomial p(z) as described in Exam-
ple 7.9.4 such that p(A) = eA for A =

3
2
1
âˆ’3
âˆ’2
âˆ’1
âˆ’3
âˆ’2
âˆ’1

.
7.9.16. The Cayleyâ€“Hamilton theorem (pp. 509, 532) says that every A âˆˆCnÃ—n
satisï¬es its own characteristic equation, and this guarantees that An+j
(j = 0, 1, 2, . . .) can be expressed as a polynomial in A of at most
degree n âˆ’1. Since f(A) is always a polynomial in A, the Cayleyâ€“
Hamilton theorem insures that f(A) can be expressed as a polynomial
in A of at most degree n âˆ’1. Such a polynomial can be determined
whenever f (j)(Î»i), j = 0, 1, . . . , ai âˆ’1 exists for each Î»i âˆˆÏƒ (A) ,
where ai = alg mult (Î»i) . The strategy is the same as that in Example
7.9.4 except that ai is used in place of ki . If we can ï¬nd a polynomial
p(z) = Î±0 + Î±1z + Â· Â· Â· + Î±nâˆ’1znâˆ’1 such that for each Î»i âˆˆÏƒ (A) ,
p(Î»i) = f(Î»i),
pâ€²(Î»i) = f â€²(Î»i),
. . . ,
p(aiâˆ’1)(Î»i) = f (aiâˆ’1)(Î»i),
then p(A) = f(A). Why? These equations are an n Ã— n linear system
with the Î±i â€™s as the unknowns, and, for the same reason outlined in
Example 7.9.4, a solution is always possible.
(a)
What advantages and disadvantages does this approach have
with respect to the approach in Example 7.9.4?
(b)
Use this method to ï¬nd a polynomial p(z) such that p(A) = eA
for A =

3
2
1
âˆ’3
âˆ’2
âˆ’1
âˆ’3
âˆ’2
âˆ’1

. Compare with Exercise 7.9.15.
7.9.17. Show that if f is a function deï¬ned at
A =
ï£«
ï£­
Î±
Î²
Î³
0
Î±
Î²
0
0
Î±
ï£¶
ï£¸= Î±I + Î²N + Î³N2,
where
N =
ï£«
ï£­
0
1
0
0
0
1
0
0
0
ï£¶
ï£¸,
then f(A) = f(Î±)I + Î²f â€²(Î±)N +

Î³f â€²(Î±) + Î²2f â€²â€²(Î±)
2!

N2.

7.9 Functions of Nondiagonalizable Matrices
615
7.9.18. Composition of Matrix Functions. If h(z) = f(g(z)), where f
and g are functions such that g(A) and f

g(A)

each exist, then
h(A) = f

g(A)

. However, itâ€™s not legal to prove this simply by saying
â€œreplace z by A.â€ One way to prove that h(A) = f

g(A)

is to
demonstrate that h(Jâ‹†) = f

g(Jâ‹†)

for a generic Jordan block and then
invoke (7.9.3). Do this for a 3 Ã— 3 Jordan blockâ€”the generalization to
k Ã— k blocks is similar. That is, let h(z) = f(g(z)), and use Exercise
7.9.17 to prove that if g(Jâ‹†) and f

g(Jâ‹†)

each exist, then
h(Jâ‹†) = f

g(Jâ‹†)

for
Jâ‹†=
ï£«
ï£­
Î»
1
0
0
Î»
1
0
0
Î»
ï£¶
ï£¸.
7.9.19. Prove that if Î“i is a simple closed contour enclosing Î»i âˆˆÏƒ (A) but
excluding all other eigenvalues of A, then the ith spectral projector is
Gi =
1
2Ï€i
4
Î“i
(Î¾I âˆ’A)âˆ’1dÎ¾ =
1
2Ï€i
4
Î“i
R(Î¾)dÎ¾.
7.9.20. For f(z) = zâˆ’1, verify that f(A) = Aâˆ’1 for every nonsingular A.
7.9.21. If Î“ is a simple closed contour enclosing all eigenvalues of a nonsingular
matrix A, what is the value of
1
2Ï€i
4
Î“
Î¾âˆ’1(Î¾I âˆ’A)âˆ’1dÎ¾ ?
7.9.22. Generalized Inverses.
The inverse function f(z) = zâˆ’1 is not de-
ï¬ned at singular matrices, but the generalized inverse function
g(z) =
	
zâˆ’1
if z Ì¸= 0,
0
if z = 0,
is deï¬ned on all square matrices. Itâ€™s clear from Exercise 7.9.20 that
if A is nonsingular, then g(A) = Aâˆ’1, so g(A) is a natural way to
extend the concept of inversion to include singular matrices. Explain why
g(A) = AD is the Drazin inverse of Example 5.10.5 (p. 399) and not
necessarily the Mooreâ€“Penrose pseudoinverse Aâ€  described on p. 423.
7.9.23. Drazin Is â€œNatural.â€
Suppose that A is a singular matrix, and let
Î“ be a simple closed contour that contains all eigenvalues of A except
Î»1 = 0, which is neither in nor on Î“. Prove that
1
2Ï€i
4
Î“
Î¾âˆ’1(Î¾I âˆ’A)âˆ’1dÎ¾ = AD
is the Drazin inverse for A as deï¬ned in Example 5.10.5 (p. 399). Hint:
The Cauchyâ€“Goursat theorem states that if a function f is analytic at
all points inside and on a simple closed contour Î“, then
6
Î“ f(z)dz = 0.

616
Chapter 7
Eigenvalues and Eigenvectors
7.10
DIFFERENCE EQUATIONS, LIMITS, AND SUMMABILITY
A linear diï¬€erence equation of order m with constant coeï¬ƒcients has the form
y(k + 1) = Î±my(k) + Î±mâˆ’1y(k âˆ’1) Â· Â· Â· + Î±1y(k âˆ’m + 1) + Î±0
(7.10.1)
in which Î±0, Î±1, . . . , Î±m along with initial conditions y(0), y(1), . . . , y(m âˆ’1)
are known constants, and y(m), y(m + 1), y(m + 2) . . . are unknown. Diï¬€erence
equations are the discrete analogs of diï¬€erential equations, and, among other
ways, they arise by discretizing diï¬€erential equations. For example, discretizing
a second-order linear diï¬€erential equation results in a system of second-order
diï¬€erence equations as illustrated in Example 1.4.1, p 19. The theory of linear
diï¬€erence equations parallels the theory for linear diï¬€erential equations, and
a technique similar to the one used to solve linear diï¬€erential equations with
constant coeï¬ƒcients produces the solution of (7.10.1) as
y(k) =
Î±0
1 âˆ’Î±1 âˆ’Â· Â· Â· âˆ’Î±m
+
m

i=1
Î²iÎ»k
i ,
for k = 0, 1, . . .
(7.10.2)
in which the Î»i â€™s are the roots of Î»m âˆ’Î±mÎ»mâˆ’1 âˆ’Â· Â· Â· âˆ’Î±0 = 0, and the Î²i â€™s
are constants determined by the initial conditions y(0), y(1), . . . , y(m âˆ’1). The
ï¬rst term on the right-hand side of (7.10.2) is a particular solution of (7.10.1),
and the summation term in (7.10.2) is the general solution of the associated
homogeneous equation deï¬ned by setting Î±0 = 0.
This section focuses on systems of ï¬rst-order linear diï¬€erence equations with
constant coeï¬ƒcients, and such systems can be written in matrix form as
x(k + 1) = Ax(k)
(a homogeneous system)
or
x(k + 1) = Ax(k) + b(k)
(a nonhomogeneous system),
(7.10.3)
where matrix AnÃ—n, the initial vector x(0), and vectors b(k), k = 0, 1, . . . , are
known. The problem is to determine the unknown vectors x(k), k = 1, 2, . . . ,
along with an expression for the limiting vector limkâ†’âˆx(k). Such systems are
used to model linear discrete-time evolutionary processes, and the goal is usually
to predict how (or to where) the process eventually evolves given the initial state
of the process. For example, the population migration problem in Example 7.3.5
(p. 531) produces a 2 Ã— 2 system of homogeneous linear diï¬€erence equations
(7.3.14), and the long-run (or steady-state) population distribution is obtained
by ï¬nding the limiting solution. More sophisticated applications are given in
Example 7.10.8 (p. 635) and Example 8.3.7 (p. 683).

7.10 Diï¬€erence Equations, Limits, and Summability
617
Solving the equations in (7.10.3) is easy. Direct substitution veriï¬es that
x(k) = Akx(0)
for
k = 1, 2, 3, . . .
and
(7.10.4)
x(k) = Akx(0) +
kâˆ’1

j=0
Akâˆ’jâˆ’1b(j)
for
k = 1, 2, 3, . . .
are respective solutions to (7.10.3). So rather than ï¬nding x(k) for any ï¬-
nite k, the real problem is to understand the nature of the limiting solution
limkâ†’âˆx(k), and this boils down to analyzing limkâ†’âˆAk. We begin this anal-
ysis by establishing conditions under which Ak â†’0.
For scalars Î± we know that Î±k â†’0 if and only if |Î±| < 1, so itâ€™s natural
to ask if there is an analogous statement for matrices. The ï¬rst inclination is to
replace | â‹†| by a matrix norm âˆ¥â‹†âˆ¥, but this doesnâ€™t work for the standard
norms. For example, if A =
 0
2
0
0

, then Ak â†’0 but âˆ¥Aâˆ¥= 2 for all of the
standard matrix norms. Although itâ€™s possible to construct a rather goofy-looking
matrix norm âˆ¥â‹†âˆ¥g such that âˆ¥Aâˆ¥g < 1 when limkâ†’âˆAk = 0, the underlying
mechanisms governing convergence to zero are better understood and analyzed
by using eigenvalues and the Jordan form rather than norms. In particular, the
spectral radius of A deï¬ned as Ï(A) = maxÎ»âˆˆÏƒ(A) |Î»| (Example 7.1.4, p. 497)
plays a central role.
Convergence to Zero
For A âˆˆCnÃ—n,
lim
kâ†’âˆAk = 0
if and only if
Ï(A) < 1.
(7.10.5)
Proof.
If Pâˆ’1AP = J is the Jordan form for A, then
Ak = PJkPâˆ’1 = P
ï£«
ï£¬
ï£­
...
Jk
â‹†
...
ï£¶
ï£·
ï£¸Pâˆ’1,
where
Jâ‹†=
ï£«
ï£­
Î»
1
...
...
Î»
ï£¶
ï£¸(7.10.6)
denotes a generic Jordan block in J. Clearly, Ak â†’0 if and only if Jk
â‹†â†’0
for each Jordan block, so it suï¬ƒces to prove that Jk
â‹†â†’0 if and only if |Î»| <
1. Using the function f(z) = zn in formula (7.9.2) on p. 600 along with the
convention that
k
j

= 0 for j > k produces

618
Chapter 7
Eigenvalues and Eigenvectors
Jk
â‹†=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î»k
k
1

Î»kâˆ’1
k
2

Î»kâˆ’2 Â· Â· Â· 
k
mâˆ’1

Î»kâˆ’m+1
Î»k
k
1

Î»kâˆ’1 ...
...
...
...
k
2

Î»kâˆ’2
Î»k
k
1

Î»kâˆ’1
Î»k
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
mÃ—m
.
(7.10.7)
Itâ€™s clear from the diagonal entries that if Jk
â‹†â†’0, then Î»k â†’0, so |Î»| < 1.
Conversely, if |Î»| < 1 then limkâ†’âˆ
k
j

Î»kâˆ’j = 0 for each ï¬xed value of j
because
k
j

= k(k âˆ’1) Â· Â· Â· (k âˆ’j + 1)
j!
â‰¤kj
j!
=â‡’

k
j

Î»kâˆ’j
 â‰¤kj
j! |Î»|kâˆ’j â†’0.
You can see that the last term on the right-hand side goes to zero as k â†’âˆ
either by applying lâ€™Hopitalâ€™s rule or by realizing that kj goes to inï¬nity with
polynomial speed while |Î»|kâˆ’j is going to zero with exponential speed. There-
fore, if |Î»| < 1, then Jk
â‹†â†’0, and thus (7.10.5) is proven.
Intimately related to the question of convergence to zero is the convergence
of the Neumann series âˆ
k=0 Ak. It was demonstrated in (3.8.5) on p. 126
that if limnâ†’âˆAn = 0, then the Neumann series converges, and it was argued
in Example 7.3.1 (p. 527) that the converse holds for diagonalizable matrices.
Now we are in a position to prove that the converse is true for all square matrices
and thereby produce the following complete statement regarding the convergence
of the Neumann series.
Neumann Series
For A âˆˆCnÃ—n, the following statements are equivalent.
â€¢
The Neumann series I + A + A2 + Â· Â· Â· converges.
(7.10.8)
â€¢
Ï(A) < 1.
(7.10.9)
â€¢
lim
kâ†’âˆAk = 0.
(7.10.10)
In which case, (I âˆ’A)âˆ’1 exists and âˆ
k=0 Ak = (I âˆ’A)âˆ’1. (7.10.11)
Proof.
We know from (7.10.5) that (7.10.9) and (7.10.10) are equivalent, and it
was argued on p. 126 that (7.10.10) implies (7.10.8), so the theorem can be estab-
lished by proving that (7.10.8) implies (7.10.9). If âˆ
k=0 Ak converges, it follows
that âˆ
k=0 Jk
âˆ—must converge for each Jordan block Jâˆ—in the Jordan form for A.
This together with (7.10.7) implies that
âˆ
k=0 Jk
âˆ—

ii = âˆ
k=0 Î»k converges for

7.10 Diï¬€erence Equations, Limits, and Summability
619
each Î» âˆˆÏƒ (A) , and this scalar geometric series converges if and only if |Î»| < 1.
Thus the convergence of âˆ
k=0 Ak implies Ï(A) < 1. When it converges,
âˆ
k=0 Ak = (I âˆ’A)âˆ’1 because (I âˆ’A)(I + A + A2 + Â· Â· Â· + Akâˆ’1) = I âˆ’Ak â†’
I as k â†’âˆ.
The following examples illustrate the utility of the previous results for es-
tablishing some useful (and elegant) statements concerning spectral radius.
Example 7.10.1
Spectral Radius as a Limit. It was shown in Example 7.1.4 (p. 497) that
if A âˆˆCnÃ—n, then Ï (A) â‰¤âˆ¥Aâˆ¥for every matrix norm. But this was just the
precursor to the following elegant relationship between spectral radius and norm.
Problem: Prove that for every matrix norm,
Ï(A) = lim
kâ†’âˆ
))Ak))1/k .
(7.10.12)
Solution: First note that Ï (A)k = Ï

Ak
â‰¤
))Ak))
=â‡’
Ï (A) â‰¤
))Ak))1/k .
Next, observe that Ï

A/(Ï (A) + Ïµ)

< 1 for every Ïµ > 0, so, by (7.10.5),
lim
kâ†’âˆ

A
Ï (A) + Ïµ
k
= 0
=â‡’
lim
kâ†’âˆ
))Ak))
(Ï (A) + Ïµ)k = 0.
Consequently, there is a positive integer KÏµ such that
))Ak)) /(Ï (A) + Ïµ)k < 1
for all k â‰¥KÏµ, so
))Ak))1/k < Ï (A) + Ïµ for all k â‰¥KÏµ, and thus
Ï (A) â‰¤
))Ak))1/k < Ï (A) + Ïµ
for k â‰¥KÏµ.
Because this holds for each Ïµ > 0, it follows that limkâ†’âˆ
))Ak))1/k = Ï(A).
Example 7.10.2
For A âˆˆCnÃ—n let |A| denote the matrix having entries |aij|, and for matrices
B, C âˆˆâ„œnÃ—n deï¬ne B â‰¤C to mean bij â‰¤cij for each i and j.
Problem: Prove that if |A| â‰¤B, then
Ï (A) â‰¤Ï (|A|) â‰¤Ï (B) .
(7.10.13)
Solution: The triangle inequality yields |Ak| â‰¤|A|k for every positive integer
k. Furthermore, |A| â‰¤B implies that |A|k â‰¤Bk. This with (7.10.12) produces
))Ak))
âˆ=
)) |Ak|
))
âˆâ‰¤
)) |A|k ))
âˆâ‰¤
))Bk))
âˆ
=â‡’
))Ak))1/k
âˆâ‰¤
)) |A|k ))1/k
âˆâ‰¤
))Bk))1/k
âˆ
=â‡’
lim
kâ†’âˆ
))Ak))1/k
âˆâ‰¤lim
kâ†’âˆâ‰¤
)) |A|k ))1/k
âˆâ‰¤lim
kâ†’âˆâ‰¤
))Bk))1/k
âˆ
=â‡’
Ï (A) â‰¤Ï (|A|) â‰¤Ï (B) .

620
Chapter 7
Eigenvalues and Eigenvectors
Example 7.10.3
Problem: Prove that if 0 â‰¤BnÃ—n, then
Ï (B) < r if and only if (rI âˆ’B)âˆ’1 exists and (rI âˆ’B)âˆ’1 â‰¥0.
(7.10.14)
Solution: If Ï (B) < r, then Ï(B/r) < 1, so (7.10.8)â€“(7.10.11) imply that
rI âˆ’B = r

I âˆ’B
r

is nonsingular and (rI âˆ’B)âˆ’1 = 1
r
âˆ

k=0
B
r
k
â‰¥0.
To prove the converse, itâ€™s convenient to adopt the following notation. For any
P âˆˆâ„œmÃ—n, let |P| =

|pij|

denote the matrix of absolute values, and notice
that the triangle inequality insures that |PQ| â‰¤|P| |Q| for all conformable P
and Q. Now assume that rIâˆ’B is nonsingular and (rIâˆ’B)âˆ’1 â‰¥0, and prove
Ï (B) < r. Let (Î», x) be any eigenpair for B, and use B â‰¥0 together with
(rI âˆ’B)âˆ’1 â‰¥0 to write
Î»x = Bx
=â‡’
|Î»| |x| = |Î»x| = |Bx| â‰¤|B| |x| = B |x|
=â‡’
(rI âˆ’B)|x| â‰¤(r âˆ’|Î»|) |x|
=â‡’
0 â‰¤|x| â‰¤(r âˆ’|Î»|) (rI âˆ’B)âˆ’1|x|
(7.10.15)
=â‡’
r âˆ’|Î»| â‰¥0.
But |Î»| Ì¸= r; otherwise (7.10.15) would imply that |x| (and hence x) is zero,
which is impossible. Thus |Î»| < r for all Î» âˆˆÏƒ (B) , which means Ï (B) < r.
Iterative algorithms are often used in lieu of direct methods to solve large
sparse systems of linear equations, and some of the traditional iterative schemes
fall into the following class of nonhomogeneous linear diï¬€erence equations.
Linear Stationary Iterations
Let Ax = b be a linear system that is square but otherwise arbitrary.
â€¢
A splitting of A is a factorization A = Mâˆ’N, where Mâˆ’1 exists.
â€¢
Let H = Mâˆ’1N (called the iteration matrix), and set d = Mâˆ’1b.
â€¢
For an initial vector x(0)nÃ—1, a linear stationary iteration is
x(k) = Hx(k âˆ’1) + d,
k = 1, 2, 3, . . . .
(7.10.16)
â€¢
If Ï(H) < 1, then A is nonsingular and
lim
kâ†’âˆx(k) = x = Aâˆ’1b
for every initial vector x(0).
(7.10.17)

7.10 Diï¬€erence Equations, Limits, and Summability
621
Proof.
To prove (7.10.17), notice that if A = Mâˆ’N = M(Iâˆ’H) is a splitting
for which Ï(H) < 1, then (7.10.11) guarantees that (I âˆ’H)âˆ’1 exists, and thus
A is nonsingular. Successive substitution applied to (7.10.16) yields
x(k) = Hkx(0) + (I + H + H2 + Â· Â· Â· + Hkâˆ’1)d,
so if Ï(H) < 1, then (7.10.9)â€“(7.10.11) insures that for all x(0),
lim
kâ†’âˆx(k) = (I âˆ’H)âˆ’1d = (I âˆ’H)âˆ’1Mâˆ’1b = Aâˆ’1b = x.
(7.10.18)
Itâ€™s clear that the convergence rate of (7.10.16) is governed by the size
of Ï(H) along with the index of its associated eigenvalue (go back and look
at (7.10.7)). But what really is needed is an indication of how many digits of
accuracy can be expected to be gained per iteration. So as not to obscure the
simple underlying idea, assume that HnÃ—n is diagonalizable with
Ïƒ (H) = {Î»1, Î»2, . . . , Î»s} ,
where
1 > |Î»1| > |Î»2| â‰¥|Î»3| â‰¥Â· Â· Â· â‰¥|Î»s|
(which is frequently the case in applications), and let Ïµ(k) = x(k) âˆ’x denote
the error after the kth iteration. Subtracting x = Hx + d (a consequence of
(7.10.18)) from x(k) = Hx(k âˆ’1) + d produces (for large k)
Ïµ(k) = HÏµ(k âˆ’1) = HkÏµ(0) = (Î»k
1G1 + Î»k
2G2 + Â· Â· Â· + Î»k
sGs)Ïµ(0) â‰ˆÎ»k
1G1Ïµ(0),
where the Gi â€™s are the spectral projectors occurring in the spectral decomposi-
tion (pp. 517 and 520) of Hk. Similarly, Ïµ(k âˆ’1) â‰ˆÎ»kâˆ’1
1
G1Ïµ(0), so comparing
the ith components of Ïµ(k âˆ’1) and Ïµ(k) reveals that after several iterations,

Ïµi(k âˆ’1)
Ïµi(k)
 â‰ˆ
1
|Î»1| =
1
Ï (H)
for each
i = 1, 2, . . . , n.
To understand the signiï¬cance of this, suppose for example that
|Ïµi(k âˆ’1)| = 10âˆ’q
and
|Ïµi(k)| = 10âˆ’p
with
p â‰¥q > 0,
so that the error in each entry is reduced by p âˆ’q digits per iteration. Since
p âˆ’q = log10

Ïµi(k âˆ’1)
Ïµi(k)
 â‰ˆâˆ’log10 Ï (H) ,
we see that âˆ’log10 Ï (H) provides us with an indication of the number of digits
of accuracy that can be expected to be eventually gained on each iteration. For
this reason, the number R = âˆ’log10 Ï (H) (or, alternately, R = âˆ’ln Ï (H)) is
called the asymptotic rate of convergence, and this is the primary tool for
comparing diï¬€erent linear stationary iterative algorithms.
The trick is to ï¬nd splittings that guarantee rapid convergence while insuring
that H = Mâˆ’1N and d = Mâˆ’1b can be computed easily. The following three
examples present the classical splittings.

622
Chapter 7
Eigenvalues and Eigenvectors
Example 7.10.4
Jacobiâ€™s method
81 is produced by splitting A = D âˆ’N, where D is the
diagonal part of A (we assume each aii Ì¸= 0 ), and âˆ’N is the matrix containing
the oï¬€-diagonal entries of A. Clearly, both H = Dâˆ’1N and d = Dâˆ’1b can be
formed with little eï¬€ort. Notice that the ith component in the Jacobi iteration
x(k) = Dâˆ’1Nx(k âˆ’1) + Dâˆ’1b is given by
xi(k) =

bi âˆ’
jÌ¸=i aijxj(k âˆ’1)

/aii.
(7.10.19)
This shows that the order in which the equations are considered is irrelevant
and that the algorithm can process equations independently (or in parallel).
For this reason, Jacobiâ€™s method was referred to in the 1940s as the method of
simultaneous displacements.
Problem: Explain why Jacobiâ€™s method is guaranteed to converge for all initial
vectors x(0) and for all right-hand sides b when A is diagonally dominant as
deï¬ned and discussed in Examples 4.3.3 (p. 184) and 7.1.6 (p. 499).
Solution: According to (7.10.17), it suï¬ƒces to show that Ï(H) < 1. This follows
by combining |aii| > 
jÌ¸=i |aij| for each i with the fact that Ï(H) â‰¤âˆ¥Hâˆ¥âˆ
(Example 7.1.4, p. 497) to write
Ï(H) â‰¤âˆ¥Hâˆ¥âˆ= max
i

j
|aij|
|aii| = max
i

jÌ¸=i
|aij|
|aii| < 1.
Example 7.10.5
The Gaussâ€“Seidel method
82 is the result of splitting A = (Dâˆ’L)âˆ’U, where
D is the diagonal part of A (aii Ì¸= 0 is assumed) and where âˆ’L and âˆ’U
contain the entries occurring below and above the diagonal of A, respectively.
The iteration matrix is H = (Dâˆ’L)âˆ’1U, and d = (Dâˆ’L)âˆ’1b. The ith entry
in the Gaussâ€“Seidel iteration x(k) = (D âˆ’L)âˆ’1Ux(k âˆ’1) + (D âˆ’L)âˆ’1b is
xi(k) =

bi âˆ’
j<i aijxj(k) âˆ’
j>i aijxj(k âˆ’1)

/aii.
(7.10.20)
This shows that Gaussâ€“Seidel determines xi(k) by using the newest possible
informationâ€”namely, x1(k), x2(k), . . . , xiâˆ’1(k) in the current iterate in con-
junction with xi+1(k âˆ’1), xi+2(k âˆ’1), . . . , xn(k âˆ’1) from the previous iterate.
81
Karl Jacobi (p. 353) considered this method in 1845, but it seems to have been independently
discovered by others. In addition to being called the method of simultaneous displacements in
1945, Jacobiâ€™s method was referred to as the Richardson iterative method in 1958.
82
Ludwig Philipp von Seidel (1821â€“1896) studied with Dirichlet in Berlin in 1840 and with
Jacobi (and others) in KÂ¨onigsberg. Seidelâ€™s involvement in transforming Jacobiâ€™s method into
the Gaussâ€“Seidel scheme is natural, but the reason for attaching Gaussâ€™s name is unclear.
Seidel went on to earn his doctorate (1846) in Munich, where he stayed as a professor for the
rest of his life. In addition to mathematics, Seidel made notable contributions in the areas of
optics and astronomy, and in 1970 a lunar crater was named for Seidel.

7.10 Diï¬€erence Equations, Limits, and Summability
623
This diï¬€ers from Jacobiâ€™s method because Jacobi relies strictly on the old data
in x(k âˆ’1). The Gaussâ€“Seidel algorithm was known in the 1940s as the method
of successive displacements (as opposed to the method of simultaneous displace-
ments, which is Jacobiâ€™s method). Because Gaussâ€“Seidel computes xi(k) with
newer data than that used by Jacobi, it appears at ï¬rst glance that Gaussâ€“Seidel
should be the superior algorithm. While this is often the case, it is not universally
trueâ€”see Exercise 7.10.7.
Other Comparisons. Another major diï¬€erence between Gaussâ€“Seidel and Ja-
cobi is that the order in which the equations are processed is irrelevant for Ja-
cobiâ€™s method, but the value (not just the position) of the components xi(k) in
the Gaussâ€“Seidel iterate can change when the order of the equations is changed.
Since this ordering feature can aï¬€ect the performance of the algorithm, it was the
object of much study at one time. Furthermore, when core memory is a concern,
Gaussâ€“Seidel enjoys an advantage because as soon as a new component xi(k) is
computed, it can immediately replace the old value xi(k âˆ’1), whereas Jacobi
requires all old values in x(k âˆ’1) to be retained until all new values in x(k)
have been determined. Something that both algorithms have in common is that
diagonal dominance in A guarantees global convergence of each method.
Problem: Explain why diagonal dominance in A is suï¬ƒcient to guarantee
convergence of the Gaussâ€“Seidel method for all initial vectors x(0) and for all
right-hand sides b .
Solution: Show Ï (H) < 1. Let (Î», z) be any eigenpair for H, and suppose
that the component of maximal magnitude in z occurs in position m. Write
(D âˆ’L)âˆ’1Uz = Î»z as Î»(D âˆ’L)z = Uz, and write the mth row of this latter
equation as Î»(d âˆ’l) = u, where
d = ammzm,
l = âˆ’

j<m
amjzj,
and
u = âˆ’

j>m
amjzj.
Diagonal dominance |amm| > 
jÌ¸=m |amj| and |zj| â‰¤|zm| for all j yields
|u| + |l| =


j<m
amjzj
 +


j>m
amjzj
 â‰¤|zm|
 
j<m
|amj| +

j>m
|amj|

< |zm||amm| = |d|
=â‡’
|u| < |d| âˆ’|l|.
This together with Î»(d âˆ’l) = u and the backward triangle inequality (Example
5.1.1, p. 273) produces the conclusion that
|Î»| =
|u|
|d âˆ’l| â‰¤
|u|
|d| âˆ’|l| < 1,
and thus
Ï(H) < 1.
Note: Diagonal dominance in A guarantees convergence for both Jacobi and
Gaussâ€“Seidel, but diagonal dominance is a rather severe condition that is often

624
Chapter 7
Eigenvalues and Eigenvectors
not present in applications. For example the linear system in Example 7.6.2
(p. 563) that results from discretizing Laplaceâ€™s equation on a square is not
diagonally dominant (e.g., look at the ï¬fth row in the 9 Ã— 9 system on p. 564).
But such systems are always positive deï¬nite (Example 7.6.2), and there is a
classical theorem stating that if A is positive deï¬nite, then the Gaussâ€“Seidel
iteration converges to the solution of Ax = b for every initial vector x(0). The
same cannot be said for Jacobiâ€™s method, but there are matrices (the M-matrices
of Example 7.10.7, p. 626) having properties resembling positive deï¬niteness for
which Jacobiâ€™s method is guaranteed to convergeâ€”see (7.10.29).
Example 7.10.6
The successive overrelaxation (SOR) method improves on Gaussâ€“Seidel
by introducing a real number Ï‰ Ì¸= 0, called a relaxation parameter, to form
the splitting A = M âˆ’N, where M = Ï‰âˆ’1D âˆ’L and N = (Ï‰âˆ’1 âˆ’1)D + U.
As before, D is the diagonal part of A ( aii Ì¸= 0 is assumed) and âˆ’L and âˆ’U
contain the entries occurring below and above the diagonal of A, respectively.
Since Mâˆ’1 = Ï‰(D âˆ’Ï‰L)âˆ’1 = Ï‰(I âˆ’Ï‰Dâˆ’1L)âˆ’1, the SOR iteration matrix is
HÏ‰ =Mâˆ’1N=(Dâˆ’Ï‰L)âˆ’1
(1âˆ’Ï‰)D+Ï‰U

=(Iâˆ’Ï‰Dâˆ’1L)âˆ’1
(1âˆ’Ï‰)I+Ï‰Dâˆ’1U

,
and the kth SOR iterate emanating from (7.10.16) is
x(k) = HÏ‰x(k âˆ’1) + Ï‰(I âˆ’Ï‰Dâˆ’1L)âˆ’1Dâˆ’1b.
(7.10.21)
This is the Gaussâ€“Seidel iteration when Ï‰ = 1. Using Ï‰ > 1 is called overrelax-
ation, while taking Ï‰ < 1 is referred to as underrelaxation. Writing (7.10.21) in
the form (I âˆ’Ï‰Dâˆ’1L)x(k) =

(1 âˆ’Ï‰)I + Ï‰Dâˆ’1U

x(k âˆ’1) + Ï‰Dâˆ’1b and con-
sidering the ith component on both sides of this equality produces
xi(k) = (1 âˆ’Ï‰)xi(k âˆ’1) + Ï‰
aii

bi âˆ’

j<i
aijxj(k) âˆ’

j>i
aijxj(k âˆ’1)

. (7.10.22)
The matrix splitting approach is elegant and unifying, but it obscures the simple
idea behind SOR. To understand the original motivation, write the Gaussâ€“Seidel
iterate in (7.10.20) as 7xi(k) = 7xi(k âˆ’1) + ck, where ck is the â€œcorrection termâ€
ck = 1
aii

bi âˆ’

j<i
aij7xj(k) âˆ’
n

j=i
aij7xj(k âˆ’1)

.
This clearly suggests that the performance of the iteration can be aï¬€ected by
adjusting (or â€œrelaxingâ€) the correction termâ€”i.e., by replacing ck with Ï‰ck.
The resulting algorithm, 7xi(k) = 7xi(k âˆ’1) + Ï‰ck, is in fact (7.10.22), which
produces (7.10.21). Moreover, it was observed early on that Gaussâ€“Seidel applied
to ï¬nite diï¬€erence approximations for elliptic partial diï¬€erential equations, such

7.10 Diï¬€erence Equations, Limits, and Summability
625
as the one in Example 7.6.2 (p. 563), often produces successive corrections ck
that have the same sign, so it was reasoned that convergence might be accelerated
for these applications by increasing the magnitude of the correction factor at each
step (i.e., by setting Ï‰ > 1). Thus the technique became known as â€œsuccessive
overrelaxationâ€ rather than simply â€œsuccessive relaxation.â€ Itâ€™s not hard to see
that Ï (HÏ‰) < 1 only if 0 < Ï‰ < 2 (Exercise 7.10.9), and it can be proven
that positive deï¬niteness of A is suï¬ƒcient to guarantee Ï (HÏ‰) < 1 whenever
0 < Ï‰ < 2. But determining Ï‰ to minimize Ï (HÏ‰) is generally a diï¬ƒcult task.
Nevertheless, there is one famous special case
83 for which the optimal value
of Ï‰ can be explicitly given. If det (Î±D âˆ’L âˆ’U) = det

Î±D âˆ’Î²L âˆ’Î²âˆ’1U

for
all real Î± and Î² Ì¸= 0, and if the iteration matrix HJ for Jacobiâ€™s method has
real eigenvalues with Ï (HJ) < 1, then the eigenvalues Î»J for HJ are related
to the eigenvalues Î»Ï‰ of HÏ‰ by
(Î»Ï‰ + Ï‰ âˆ’1)2 = Ï‰2Î»2
JÎ»Ï‰.
(7.10.23)
From this it can be proven that the optimum value of Ï‰ for SOR is
Ï‰opt =
2
1 +

1 âˆ’Ï2(HJ)
and
Ï

HÏ‰opt

= Ï‰opt âˆ’1.
(7.10.24)
Furthermore, setting Ï‰ = 1 in (7.10.23) yields Ï (HGS) = Ï2(HJ), where HGS
is the Gaussâ€“Seidel iteration matrix. For example, the discrete Laplacian Ln2Ã—n2
in Example 7.6.2 (p. 563) satisï¬es the special case conditions, and the spectral
radii of the iteration matrices associated with L are
Jacobi:
Ï (HJ)
= cos Ï€h
â‰ˆ1 âˆ’(Ï€2h2/2)
(see Exercise 7.10.10),
Gaussâ€“Seidel:
Ï (HGS)
= cos2 Ï€h
â‰ˆ1 âˆ’Ï€2h2,
SOR:
Ï

HÏ‰opt

= 1 âˆ’sin Ï€h
1 + sin Ï€h â‰ˆ1 âˆ’2Ï€h,
where we have set h = 1/(n + 1). Examining asymptotic rates of convergence
reveals that Gaussâ€“Seidel is twice as fast as Jacobi on the discrete Laplacian
because RGS = âˆ’log10 cos2 Ï€h = âˆ’2 log10 cos Ï€h = 2RJ. However, optimal
SOR is much better because 1 âˆ’2Ï€h is signiï¬cantly smaller than 1 âˆ’Ï€2h2 for
even moderately small h. The point is driven home by looking at the asymptotic
rates of convergence for h = .02 (n = 49) as shown below:
Jacobi:
RJ â‰ˆ.000858,
Gaussâ€“Seidel:
RGS = 2RJ â‰ˆ.001716,
SOR:
Ropt â‰ˆ.054611 â‰ˆ32RGS = 64RJ.
83
This special case was developed by the contemporary numerical analyst David M. Young, Jr.,
who produced much of the SOR theory in his 1950 Ph.D. dissertation that was directed by
Garrett Birkhoï¬€at Harvard University. The development of SOR is considered to be one of the
major computational achievements of the ï¬rst half of the twentieth century, and it motivated
at least two decades of intense eï¬€ort in matrix computations.

626
Chapter 7
Eigenvalues and Eigenvectors
In other words, after things settle down, a single SOR step on L (for h = .02)
is equivalent to about 32 Gaussâ€“Seidel steps and 64 Jacobi steps!
Note: In spite of the preceding remarks, SOR has limitations. Special cases
for which the optimum Ï‰ can be explicitly determined are rare, so adaptive
computational procedures are generally necessary to approximate a good Ï‰, and
the results are often not satisfying. While SOR was a big step forward over the
algorithms of the nineteenth century, the second half of the twentieth century saw
the development of more robust methodsâ€”such as the preconditioned conjugate
gradient method (p. 657) and GMRES (p. 655)â€”that have relegated SOR to a
secondary role.
Example 7.10.7
M-matrices
84 are real nonsingular matrices AnÃ—n such that aij â‰¤0 for all
i Ì¸= j and Aâˆ’1 â‰¥0 (each entry of Aâˆ’1 is nonnegative). They arise naturally in
a broad variety of applications ranging from economics (Example 8.3.6, p. 681)
to hard-core engineering problems, and, as shown in (7.10.29), they are partic-
ularly relevant in formulating and analyzing iterative methods. Some important
properties of M-matrices are developed below.
â€¢
A is an M-matrix if and only if there exists a matrix B â‰¥0 and a real
number r > Ï(B) such that A = rI âˆ’B.
(7.10.25)
â€¢
If A is an M-matrix, then Re (Î») > 0 for all Î» âˆˆÏƒ (A) . Conversely, all
matrices with nonpositive oï¬€-diagonal entries whose spectrums are in the
right-hand halfplane are M-matrices.
(7.10.26)
â€¢
Principal submatrices of M-matrices are also M-matrices.
(7.10.27)
â€¢
If A is an M-matrix, then all principal minors in A are positive. Conversely,
all matrices with nonpositive oï¬€-diagonal entries whose principal minors are
positive are M-matrices.
(7.10.28)
â€¢
If A = M âˆ’N is a splitting of an M-matrix for which Mâˆ’1 â‰¥0, then the
linear stationary iteration (7.10.16) is convergent for all initial vectors x(0)
and for all right-hand sides b. In particular, Jacobiâ€™s method in Example
7.10.4 (p. 622) converges for all M-matrices.
(7.10.29)
Proof of (7.10.25).
Suppose that A is an M-matrix, and let r = maxi |aii| so
that B = rI âˆ’A â‰¥0. Since Aâˆ’1 = (rI âˆ’B)âˆ’1 â‰¥0, it follows from (7.10.14)
in Example 7.10.3 (p. 620) that r > Ï(B). Conversely, if A is any matrix of
84
This terminology was introduced in 1937 by the twentieth-century mathematician Alexan-
der Markowic Ostrowski, who made several contributions to the analysis of classical iterative
methods. The â€œMâ€ is short for â€œMinkowskiâ€ (p. 278).

7.10 Diï¬€erence Equations, Limits, and Summability
627
the form A = rI âˆ’B, where B â‰¥0 and r > Ï (B) , then (7.10.14) guarantees
that Aâˆ’1 exists and Aâˆ’1 â‰¥0, and itâ€™s clear that aij â‰¤0 for each i Ì¸= j, so
A must be an M-matrix.
Proof of (7.10.26).
If A is an M-matrix, then, by (7.10.25), A = rI âˆ’B,
where r > Ï (B) . This means that if Î»A âˆˆÏƒ (A) , then Î»A = r âˆ’Î»B for some
Î»B âˆˆÏƒ (B) . If Î»B = Î± + iÎ², then r > Ï (B) â‰¥|Î»B| =

Î±2 + Î²2 â‰¥|Î±| â‰¥Î±
implies that Re (Î»A) = râˆ’Î± â‰¥0. Now suppose that A is any matrix such that
aij â‰¤0 for all i Ì¸= j and Re (Î»A) > 0 for all Î»A âˆˆÏƒ (A) . This means that
there is a real number Î³ such that the circle centered at Î³ and having radius
equal to Î³ contains Ïƒ (A)â€”see Figure 7.10.1. Let r be any real number such
that r > max{2Î³, maxi |aii|}, and set B = rI âˆ’A. Itâ€™s apparent that B â‰¥0,
and, as can be seen from Figure 7.10.1, the distance |r âˆ’Î»A| between r and
every point in Ïƒ (A) is less than r.
Ïƒ(A)
r
Î³
x
iy
Figure 7.10.1
All eigenvalues of B look like Î»B = r âˆ’Î»A, and |Î»B| = |r âˆ’Î»A| < r, so
Ï (B) < r. Since A = rI âˆ’B is nonsingular (because 0 /âˆˆÏƒ (A) ) with B â‰¥0
and r > Ï (B) , it follows from (7.10.14) in Example 7.10.3 (p. 620) that
Aâˆ’1 â‰¥0, and thus A is an M-matrix.
Proof of (7.10.27).
If AkÃ—k is the principal submatrix lying on the intersection
of rows and columns i1, . . . , ik in an M-matrix A = rI âˆ’B, where B â‰¥0 and
r > Ï (B) , then A = rI âˆ’B, where B â‰¥0 is the corresponding principal
submatrix of B. Let P be a permutation matrix such that
PT BP =
 B
X
Y
Z

, or B = P
 B
X
Y
Z

PT , and let C = P
 B
0
0
0

PT .
Clearly, 0 â‰¤C â‰¤B, so, by (7.10.13) on p. 619, Ï(B) = Ï (C) â‰¤Ï (B) < r.
Consequently, (7.10.25) insures that A is an M-matrix.
Proof of (7.10.28).
If A is an M-matrix, then det (A) > 0 because the eigenval-
ues of a real matrix appear in complex conjugate pairs, so (7.10.26) and (7.1.8),

628
Chapter 7
Eigenvalues and Eigenvectors
p. 494, guarantee that det (A) = "n
i=1 Î»i > 0. It follows that each principal
minor is positive because each submatrix of an M-matrix is again an M-matrix.
Now prove that if AnÃ—n is a matrix such that aij â‰¤0 for i Ì¸= j and each prin-
cipal minor is positive, then A must be an M-matrix. Proceed by induction on
n. For n = 1, the assumption of positive principal minors implies that A = [Ï]
with Ï > 0, so Aâˆ’1 = 1/Ï > 0. Suppose the result is true for n = k, and
consider the LU factorization
A(k+1)Ã—(k+1) =
 7AkÃ—k
c
dT
Î±
!
=

I
0
dT 7Aâˆ’1
1
  7A
c
0
Î± âˆ’dT 7Aâˆ’1c
!
= LU.
We know that A is nonsingular (det (A) is a principal minor) and Î± > 0 (itâ€™s
a 1 Ã— 1 principal minor), and the induction hypothesis insures that 7Aâˆ’1 â‰¥0.
Combining these facts with c â‰¤0 and dT â‰¤0 produces
Aâˆ’1 = Uâˆ’1Lâˆ’1 =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
7Aâˆ’1
âˆ’7Aâˆ’1c
Î± âˆ’dT 7Aâˆ’1c
0
1
Î± âˆ’dT 7Aâˆ’1c
ï£¶
ï£·
ï£·
ï£·
ï£¸
 
I
0
âˆ’dT 7Aâˆ’1
1
!
â‰¥0,
and thus the induction argument is completed.
Proof of (7.10.29).
If A = Mâˆ’N is an M-matrix, and if Mâˆ’1 â‰¥0 and N â‰¥0,
then the iteration matrix H = Mâˆ’1N is clearly nonnegative. Furthermore,
(I âˆ’H)âˆ’1 âˆ’I = (I âˆ’H)âˆ’1H = Aâˆ’1N â‰¥0
=â‡’
(I âˆ’H)âˆ’1 â‰¥I â‰¥0,
so (7.10.14) in Example 7.10.3 (p. 620) insures that Ï (H) < 1. Convergence of
Jacobiâ€™s method is a special case because the Jacobi splitting is A = D âˆ’N,
where D = diag (a11, a22, . . . , ann) , and (7.10.28) implies that each aii > 0.
Note: Comparing properties of M-matrices with those of positive deï¬nite ma-
trices reveals many parallels, and, in a rough sense, an M-matrix often plays the
role of â€œa poor manâ€™s positive deï¬nite matrix.â€ Only a small sample of M-matrix
theory has been presented here, but there is in fact enough to ï¬ll a monograph
on the subject. For example, there are at least 50 known equivalent conditions
that can be imposed on a real matrix with nonpositive oï¬€-diagonal entries (often
called a Z-matrix) to guarantee that it is an M-matrixâ€”see Exercise 7.10.12 for
a sample of such conditions in addition to those listed above.

7.10 Diï¬€erence Equations, Limits, and Summability
629
We now focus on broader issues concerning when limkâ†’âˆAk exists but may
be nonzero. Start from the fact that limkâ†’âˆAk exists if and only if limkâ†’âˆJk
â‹†
exists for each Jordan block in (7.10.6). Itâ€™s clear from (7.10.7) that limkâ†’âˆJk
â‹†
cannot exist when |Î»| > 1, and we already know the story for |Î»| < 1, so we
only have to examine the case when |Î»| = 1. If |Î»| = 1 with Î» Ì¸= 1 (i.e., Î» = eiÎ¸
with 0 < Î¸ < 2Ï€ ), then the diagonal terms Î»k oscillate indeï¬nitely, and this
prevents Jk
â‹†(and Ak ) from having a limit. When Î» = 1,
Jk
â‹†=
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1
k
1

Â· Â· Â·

k
mâˆ’1

...
...
...
...
k
1

1
ï£¶
ï£·
ï£·
ï£·
ï£¸
mÃ—m
(7.10.30)
has a limiting value if and only if m = 1, which is equivalent to saying that
Î» = 1 is a semisimple eigenvalue. But Î» = 1 may be repeated p times so that
there are p Jordan blocks of the form Jâ‹†= [1]1Ã—1. Consequently, limkâ†’âˆAk
exists if and only if the Jordan form for A has the structure
J = Pâˆ’1AP =

IpÃ—p
0
0
K

, where p = alg mult (1) and Ï(K) < 1.
(7.10.31)
Now that we know when limkâ†’âˆAk exists, letâ€™s describe what limkâ†’âˆAk
looks like. We already know the answer when p = 0â€”itâ€™s 0 (because Ï (A) < 1).
But when p is nonzero, limkâ†’âˆAk Ì¸= 0, and it can be evaluated in a couple of
diï¬€erent ways. One way is to partition P =

P1 | P2

and Pâˆ’1 =
 Q1
Q2

, and
use (7.10.5) and (7.10.31) to write
lim
kâ†’âˆAk
nÃ—n = lim
kâ†’âˆP

IpÃ—p
0
0
Kk

Pâˆ’1 = P

IpÃ—p
0
0
0

Pâˆ’1
=

P1 | P2
 
IpÃ—p
0
0
0
 
Q1
Q2

= P1Q1 = G.
(7.10.32)
Another way is to use f(z) = zk in the spectral resolution theorem on p. 603. If
Ïƒ (A) = {Î»1, Î»2, . . . , Î»s} with 1 = Î»1 > |Î»2| â‰¥Â· Â· Â· â‰¥|Î»s|, and if index (Î»i) = ki,
where k1 = 1, then limkâ†’âˆ
k
j

Î»kâˆ’j
i
= 0 for i â‰¥2 (see p. 618), and
Ak =
s

i=1
kiâˆ’1

j=0
k
j

Î»kâˆ’j
i
(A âˆ’Î»iI)jGi
= G1 +
s

i=2
kiâˆ’1

j=0
k
j

Î»kâˆ’j
i
(A âˆ’Î»iI)jGi â†’G1
as
k â†’âˆ.

630
Chapter 7
Eigenvalues and Eigenvectors
In other words, limkâ†’âˆAk = G1 = G is the spectral projector associated
with Î»1 = 1. Since index (Î»1) = 1, we know from the discussion on p. 603 that
R (G) = N (I âˆ’A) and N (G) = R (I âˆ’A). Notice that if Ï(A) < 1, then
I âˆ’A is nonsingular, and N (I âˆ’A) = {0}. So regardless of whether the limit
is zero or nonzero, limkâ†’âˆAk is always the projector onto N (I âˆ’A) along
R (I âˆ’A). Below is a summary of the above observations.
Limits of Powers
For A âˆˆCnÃ—n, limkâ†’âˆAk exists if and only if
Ï(A) < 1
or else
(7.10.33)
Ï(A) = 1,
where Î» = 1 is the only eigenvalue on the
unit circle, and Î» = 1 is semisimple.
When it exists,
lim
kâ†’âˆAk = the projector onto N (I âˆ’A) along R (I âˆ’A).
(7.10.34)
With each scalar sequence {Î±1, Î±2, Î±3, . . .} there is an associated sequence
of averages {Âµ1, Âµ2, Âµ3, . . .} in which
Âµ1 = Î±1,
Âµ2 = Î±1 + Î±2
2
,
. . . ,
Âµn = Î±1 + Î±2 + Â· Â· Â· + Î±n
n
.
This sequence of averages is called the associated Ces`aro sequence,
85 and when
limnâ†’âˆÂµn = Î±, we say that {Î±n} is Ces`aro summable (or merely summable)
to Î±. It can be proven (Exercise 7.10.11) that if {Î±n} converges to Î±, then
{Âµn} converges to Î±, but not conversely. In other words, convergence implies
summability, but summability doesnâ€™t insure convergence. To see that a sequence
can be summable without being convergent, notice that the oscillatory sequence
{0, 1, 0, 1, . . .} doesnâ€™t converge, but it is Ces`aro summable to 1/2, which is the
mean value of {0, 1}. This is typical because averaging has a smoothing eï¬€ect
so that oscillations that prohibit convergence of the original sequence tend to be
smoothed away or averaged out in the Ces`aro sequence.
85
Ernesto Ces`aro (1859â€“1906) was an Italian mathematician who worked mainly in diï¬€erential
geometry but also contributed to number theory, divergent series, and mathematical physics.
After studying in Naples, Li`ege, and Paris, Ces`aro received his doctorate from the University
of Rome in 1887, and he went on to occupy the chair of mathematics at Palermo. Ces`aroâ€™s
most important contribution is considered to be his 1890 book Lezione di geometria intrinseca,
but, in large part, his name has been perpetuated because of its attachment to the concept of
Ces`aro summability.

7.10 Diï¬€erence Equations, Limits, and Summability
631
Similar statements hold for general sequences of vectors and matrices (Ex-
ercise 7.10.11), but Ces`aro summability is particularly interesting when it is
applied to the sequence P = {Ak}âˆ
k=0 of powers of a square matrix A. We
know from (7.10.33) and (7.10.34) under what conditions sequence P converges
as well as the nature of the limit, so letâ€™s now suppose that P doesnâ€™t converge,
and decide when P is summable, and what P is summable to.
From now on, we will say that AnÃ—n is a convergent matrix when
limkâ†’âˆAk exists, and we will say that A is a summable matrix when
limkâ†’âˆ(I+A+A2 +Â· Â· Â·+Akâˆ’1)/k exists. As in the scalar case, if A is conver-
gent to G, then A is summable to G, but not conversely (Exercise 7.10.11).
To analyze the summability of A in the absence of convergence, begin with the
observation that A is summable if and only if the Jordan form J = Pâˆ’1AP
for A is summable, which in turn is equivalent to saying that each Jordan
block Jâ‹†in J is summable. Consequently, A cannot be summable whenever
Ï(A) > 1 because if Jâ‹†=
 Î»
1
...
...
Î»
!
is a Jordan block in which |Î»| > 1, then
each diagonal entry of

I + Jâ‹†+ Â· Â· Â· + Jkâˆ’1
â‹†

/k is
Î´(Î», k) = 1 + Î» + Â· Â· Â· + Î»kâˆ’1
k
= 1
k
1 âˆ’Î»k
1 âˆ’Î»

=
1
1 âˆ’Î»
1
k âˆ’Î»k
k

,
(7.10.35)
and this becomes unbounded as k â†’âˆ. In other words, itâ€™s necessary that
Ï(A) â‰¤1 for A to be summable. Since we already know that A is convergent
(and hence summable) to 0 when Ï(A) < 1, we need only consider the case
when A has eigenvalues on the unit circle.
If Î» âˆˆÏƒ (A) such that |Î»| = 1, Î» Ì¸= 1, and if index (Î») > 1, then there
is an associated Jordan block Jâ‹†=
 Î»
1
...
...
Î»
!
that is larger than 1 Ã— 1. Each
entry on the ï¬rst superdiagonal of

I + Jâ‹†+ Â· Â· Â· + Jkâˆ’1
â‹†

/k is the derivative
âˆ‚Î´/âˆ‚Î» of the expression in (7.10.35), and itâ€™s not hard to see that âˆ‚Î´/âˆ‚Î» oscil-
lates indeï¬nitely as k â†’âˆ. In other words, A cannot be summable if there
are eigenvalues Î» Ì¸= 1 on the unit circle such that index (Î») > 1.
Similarly, if Î» = 1 is an eigenvalue of index greater than one, then A canâ€™t
be summable because each entry on the ï¬rst superdiagonal of
I + Jâ‹†+ Â· Â· Â· + Jkâˆ’1
â‹†
k
is
1 + 2 + Â· Â· Â· + (k âˆ’1)
k
= k(k âˆ’1)
2k
= k âˆ’1
2
â†’âˆ.
Therefore, if A is summable and has eigenvalues Î» such that |Î»| = 1, then itâ€™s
necessary that index (Î») = 1. The condition also is suï¬ƒcientâ€”i.e., if Ï(A) = 1
and each eigenvalue on the unit circle is semisimple, then A is summable. This
follows because each Jordan block associated with an eigenvalue Âµ such that
|Âµ| < 1 is convergent (and hence summable) to 0 by (7.10.5), and for semisimple

632
Chapter 7
Eigenvalues and Eigenvectors
eigenvalues Î» such that |Î»| = 1, the associated Jordan blocks are 1 Ã— 1 and
hence summable because (7.10.35) implies
1 + Î» + Â· Â· Â· + Î»kâˆ’1
k
=
ï£±
ï£´
ï£²
ï£´
ï£³
1
1 âˆ’Î»
1
k âˆ’Î»k
k

â†’0
for |Î»| = 1, Î» Ì¸= 1,
1
for Î» = 1.
In addition to providing a necessary and suï¬ƒcient condition for A to be
Ces`aro summable, the preceding analysis also reveals the nature of the Ces`aro
limit because if A is summable, then each Jordan block Jâ‹†=
 Î»
1
...
...
Î»
!
in
the Jordan form for A is summable, in which case we have established that
lim
kâ†’âˆ
I + Jâ‹†+ Â· Â· Â· + Jkâˆ’1
â‹†
k
=
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³

1

1Ã—1
if Î» = 1 and index (Î») = 1,

0

1Ã—1
if |Î»| = 1, Î» Ì¸= 1, and index (Î») = 1,
0
if |Î»| < 1.
Consequently, if A is summable, then the Jordan form for A must look like
J = Pâˆ’1AP =

IpÃ—p
0
0
C

,
where
p = alg multA (Î» = 1) ,
and the eigenvalues of C are such that |Î»| < 1 or else |Î»| = 1,
Î» Ì¸= 1,
index (Î») = 1. So C is summable to 0, J is summable to
 IpÃ—p
0
0
0

, and
I + A + Â· Â· Â· + Akâˆ’1
k
= P
I + J + Â· Â· Â· + Jkâˆ’1
k

Pâˆ’1 â†’P

IpÃ—p
0
0
0

Pâˆ’1 = G.
Comparing this expression with that in (7.10.32) reveals that the Ces`aro limit
is exactly the same as the ordinary limit, had it existed. In other words, if A is
summable, then regardless of whether or not A is convergent, A is summable
to the projector onto N (I âˆ’A) along R (I âˆ’A). Below is a formal summary
of our observations concerning Ces`aro summability.

7.10 Diï¬€erence Equations, Limits, and Summability
633
Ces`aro Summability
â€¢
A âˆˆCnÃ—n is Ces`aro summable if and only if Ï(A) < 1 or else
Ï(A) = 1 with each eigenvalue on the unit circle being semisimple.
â€¢
When it exists, the Ces`aro limit
lim
kâ†’âˆ
I + A + Â· Â· Â· + Akâˆ’1
k
= G
(7.10.36)
is the projector onto N (I âˆ’A) along R (I âˆ’A).
â€¢
G Ì¸= 0 if and only if 1 âˆˆÏƒ (A) , in which case G is the spectral
projector associated with Î» = 1.
â€¢
If A is convergent to G, then A is summable to G, but not
conversely.
Since the projector G onto N (I âˆ’A) along R (I âˆ’A) plays a prominent
role, letâ€™s consider how G might be computed. Of course, we could just iterate
on Ak or (I + A + Â· Â· Â· + Akâˆ’1)/k, but this is ineï¬ƒcient and, depending on the
proximity of the eigenvalues relative to the unit circle, convergence can be slowâ€”
averaging in particular can be extremely slow. The Jordan form is the basis for
the theoretical development, but using it to compute G would be silly (see
p. 592). The formula for a projector given in (5.9.12) on p. 386 is a possibility,
but using a full-rank factorization of I âˆ’A is an attractive alternative.
A full-rank factorization of a matrix MmÃ—n of rank r is a factorization
M = BmÃ—rCrÃ—n,
where
rank (B) = rank (C) = r = rank (M).
(7.10.37)
All of the standard reduction techniques produce full-rank factorizations. For
example, Gaussian elimination can be used because if B is the matrix of basic
columns of M, and if C is the matrix containing the nonzero rows in the
reduced row echelon form EM, then M = BC is a full-rank factorization
(Exercise 3.9.8, p. 140). If orthogonal reduction (p. 341) is used to produce a
unitary matrix P =
 P1
P2

and an upper-trapezoidal matrix T =
 T1
0

such
that PA = T, where P1 is r Ã— m and T1 contains the nonzero rows, then
M = Pâˆ—
1T1 is a full-rank factorization. If
M = U

D
0
0
0

Vâˆ—= (U1 | U2)

D
0
0
0
  Vâˆ—
1
Vâˆ—
2
!
= U1DVâˆ—
1
(7.10.38)

634
Chapter 7
Eigenvalues and Eigenvectors
is the singular value decomposition (5.12.2) on p. 412 (a URV factorization
(p. 407) could also be used), then M = U1(DVâˆ—
1) = (U1D)Vâˆ—
1 are full-rank
factorizations. Projectors, in general, and limiting projectors, in particular, are
nicely described in terms of full-rank factorizations.
Projectors
If MnÃ—n = BnÃ—rCrÃ—n is any full-rank factorization as described in
(7.10.37), and if R (M) and N (M) are complementary subspaces of
Cn, then the projector onto R (M) along N (M) is given by
P = B(CB)âˆ’1C
(7.10.39)
or
P = U1(Vâˆ—
1U1)âˆ’1Vâˆ—
1
when (7.10.38) is used.
(7.10.40)
If A is convergent or summable to G as described in (7.10.34) and
(7.10.36), and if I âˆ’A = BC is a full-rank factorization, then
G = I âˆ’B(CB)âˆ’1C
(7.10.41)
or
G = I âˆ’U1(Vâˆ—
1U1)âˆ’1Vâˆ—
1
when (7.10.38) is used.
(7.10.42)
Note: Formulas (7.10.39) and (7.10.40) are extensions of (5.13.3) on
p. 430.
Proof.
Itâ€™s always true (Exercise 4.5.12, p. 220) that
R (XmÃ—nYnÃ—p) = R (X)
when
rank (Y) = n,
N (XmÃ—nYnÃ—p) = N (Y)
when
rank (X) = n.
(7.10.43)
If MnÃ—n = BnÃ—rCrÃ—n is a full-rank factorization, and if R (M) and N (M)
are complementary subspaces of CN, then rank (M) = rank

M2
(Exercise
5.10.12, p. 402), so combining this with the ï¬rst part of (7.10.43) produces
r = rank (BC) = rank (BCBC) = rank (CB)rÃ—r
=â‡’
(CB)âˆ’1 exists.
P = B(CB)âˆ’1C is a projector because P2 = P (recall (5.9.8), p. 386), and
(7.10.43) insures that R (P) = R (B) = R (M) and N (P) = N (C) = N (M).
Thus (7.10.39) is proved. If (7.10.38) is used to produce a full-rank factorization
M = U1(DVâˆ—
1), then, because D is nonsingular,
P = (U1D)(Vâˆ—
1(U1D))âˆ’1Vâˆ—
1 = U1(Vâˆ—
1U1)âˆ’1Vâˆ—
1.
Equations (7.10.41) and (7.10.42) follow from (5.9.11), p. 386.
Formulas (7.10.40) and (7.10.42) are useful because all good matrix com-
putation packages contain numerically stable SVD implementations from which
U1 and Vâˆ—
1 can be obtained. But, of course, the singular values are not needed
in this application.

7.10 Diï¬€erence Equations, Limits, and Summability
635
Example 7.10.8
Shell Game. As depicted in Figure 7.10.2, a pea is placed under one of four
shells, and an agile manipulator quickly rearranges them by a sequence of discrete
moves. At the end of each move the shell containing the pea has been shifted
either to the left or right by only one position according to the following rules.
#1
#2
#3
#4
1
1/2
1/2
1
1/2
1/2
Figure 7.10.2
When the pea is under shell #1, it is moved to position #2, and if the pea is
under shell #4, it is moved to position #3. When the pea is under shell #2 or
#3, it is equally likely to be moved one position to the left or to the right.
Problem 1: Given that we know something about where the pea starts, what
is the probability of ï¬nding the pea in any given position after k moves?
Problem 2: In the long run, what proportion of time does the pea occupy each
of the four positions?
Solution to Problem 1: Let pj(k) denote the probability that the pea is in
position j after the kth move, and translate the given information into four
diï¬€erence equations by writing
p1(k) = p2(kâˆ’1)
2
p2(k) = p1(kâˆ’1) + p3(kâˆ’1)
2
p3(k) = p2(kâˆ’1)
2
+ p4(kâˆ’1)
p4(k) = p3(kâˆ’1)
2
or
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
p1(k)
p2(k)
p3(k)
p4(k)
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
1/2
0
0
1
0
1/2
0
0
1/2
0
1
0
0
1/2
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
p1(kâˆ’1)
p2(kâˆ’1)
p3(kâˆ’1)
p4(kâˆ’1)
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
The matrix equation on the right-hand side is a homogeneous diï¬€erence equation
p(k) = Ap(k âˆ’1) whose solution, from (7.10.4), is p(k) = Akp(0), and thus
Problem 1 is solved. For example, if you know that the pea is initially under
shell #2, then p(0) = e2, and after six moves the probability that the pea is
in the fourth position is p4(6) =

A6e2

4 = 21/64. If you donâ€™t know exactly
where the pea starts, but you assume that it is equally likely to start under any
one of the four shells, then p(0) = (1/4, 1/4, 1/4, 1/4)T , and the probabilities

636
Chapter 7
Eigenvalues and Eigenvectors
for occupying the four positions after six moves are given by p(6) = A6p(0), or
ï£«
ï£¬
ï£­
p1(6)
p2(6)
p3(6)
p4(6)
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
11/32
0
21/64
0
0
43/64
0
21/32
21/32
0
43/64
0
0
21/64
0
11/32
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
1/4
1/4
1/4
1/4
ï£¶
ï£·
ï£¸=
1
256
ï£«
ï£¬
ï£­
43
85
85
43
ï£¶
ï£·
ï£¸.
Solution to Problem 2: There is a straightforward solution when A is a con-
vergent matrix because if Ak â†’G as k â†’âˆ, then p(k) â†’Gp(0) = p, and
the components in this limiting (or steady-state) vector p provide the answer.
Intuitively, if p(k) â†’p, then after awhile p(k) is practically constant, so the
probability that the pea occupies a particular position remains essentially the
same move after move. Consequently, the components in limkâ†’âˆp(k) reveal
the proportion of time spent in each position over the long run. For example,
if limkâ†’âˆp(k) = (1/6, 1/3, 1/3, 1/6)T , then, as the game runs on indeï¬nitely,
the pea is expected to be under shell #1 for about 16.7% of the time, under shell
#2 for about 33.3% of the time, etc.
A Fly in the Ointment: Everything above rests on the assumption that A
is convergent. But A is not convergent for the shell game because a bit of
computation reveals that Ïƒ (A) = {Â±1, Â±(1/2)}. That is, there is an eigenvalue
other than 1 on the unit circle, so (7.10.33) guarantees that limkâ†’âˆAk does
not exist. Consequently, thereâ€™s no limiting solution p to the diï¬€erence equation
p(k) = Ap(k âˆ’1), and the intuitive analysis given above does not apply.
Ces`aro to the Rescue: However, A is summable because Ï(A) = 1, and
every eigenvalue on the unit circle is semisimpleâ€”these are the conditions in
(7.10.36). So as k â†’âˆ,
I + A + Â· Â· Â· + Akâˆ’1
k

p(0) â†’Gp(0) = p.
The job now is to interpret the meaning of this Ces`aro limit in the context of
the shell game. To do so, focus on a particular positionâ€”say the jth oneâ€”and
set up â€œcounting functionsâ€ (random variables) deï¬ned as
X(0) =
	
1
if the pea starts under shell j,
0
otherwise,
and
X(i) =
	
1
if the pea is under shell j after the ith move,
0
otherwise,
i = 1, 2, 3, . . . .
Notice that X(0) + X(1) + Â· Â· Â· + X(k âˆ’1) counts the number of times the pea
occupies position j before the kth move, so

X(0) + X(1) + Â· Â· Â· + X(k âˆ’1)

/k

7.10 Diï¬€erence Equations, Limits, and Summability
637
represents the fraction of times that the pea is under shell j before the kth
move. Since the expected (or mean) value of X(i) is, by deï¬nition,
E[X(i)] = 1 Ã— P

X(i) = 1

+ 0 Ã— P

X(i) = 0

= pj(i),
and since expectation is linear ( E[Î±X(i) + X(h)] = Î±E[X(i)] + E[X(h)] ), the
expected fraction of times that the pea occupies position j before move k is
E
%X(0) + X(1) + Â· Â· Â· + X(k âˆ’1)
k
&
= E[X(0)] + E[X(1)] + Â· Â· Â· + E[X(k âˆ’1)]
k
= pj(0) + pj(1) + Â· Â· Â· + pj(k âˆ’1)
k
=
%p(0) + p(1) + Â· Â· Â· + p(k âˆ’1)
k
&
j
=
%p(0) + Ap(0) + Â· Â· Â· + Akâˆ’1p(0)
k
&
j
=
%I + A + Â· Â· Â· + Akâˆ’1
k

p(0)
&
j
â†’[Gp(0)]j .
In other words, as the game progresses indeï¬nitely, the components of the Ces`aro
limit p = Gp(0) provide the expected proportion of times that the pea is under
each shell, and this is exactly what we wanted to know.
Computing the Limiting Vector. Of course, p can be determined by ï¬rst
computing G with a full-rank factorization of I âˆ’A as described in (7.10.41),
but there is some special structure in this problem that can be exploited to make
the task easier. Recall from (7.2.12) on p. 518 that if Î» is a simple eigenvalue for
A, and if x and yâˆ—are respective right-hand and left-hand eigenvectors associ-
ated with Î», then xyâˆ—/yâˆ—x is the projector onto N (Î»I âˆ’A) along R (Î»I âˆ’A).
We can use this because, for the shell game, Î» = 1 is a simple eigenvalue for
A. Furthermore, we get an associated left-hand eigenvector for freeâ€”namely,
eT = (1, 1, 1, 1) â€”because each column sum of A is one, so eT A = eT . Con-
sequently, if x is any right-hand eigenvector of A associated with Î» = 1, then
(by noting that eT p(0) = p1(0) + p2(0) + p3(0) + p4(0) = 1) the limiting vector
is given by
p = Gp(0) = xeT p(0)
eT x
=
x
eT x =
x
 xi
.
(7.10.44)
In other words, the limiting vector is obtained by normalizing any nonzero so-
lution of (I âˆ’A)x = 0 to make the components sum to one. Not only does
(7.10.44) show how to compute the limiting proportions, it also shows that the
limiting proportions are independent of the initial values in p(0). For example, a
simple calculation reveals that x = (1, 2, 2, 1)T is one solution of (Iâˆ’A)x = 0,
so the vector of limiting proportions is p = (1/6, 1/3, 1/3, 1/6)T . Therefore, if
many moves are made, then, regardless of where the pea starts, we expect the
pea to end up under shell #1 in about 16.7% of the moves, under #2 for about

638
Chapter 7
Eigenvalues and Eigenvectors
33.3% of the moves, under #3 for about 33.3% of the moves, and under shell #4
for about 16.7% of the moves.
Note: The shell game (and its analysis) is a typical example of a random walk
with reï¬‚ecting barriers, and these problems belong to a broader classiï¬cation
of stochastic processes known as irreducible, periodic Markov chains. (Markov
chains are discussed in detail in Â§8.4 on p. 687.) The shell game is irreducible
in the sense of Exercise 4.4.20 (p. 209), and it is periodic because the pea can
return to given position only at deï¬nite periods, as reï¬‚ected in the periodicity
of the powers of A. More details are given in Example 8.4.3 on p. 694.
Exercises for section 7.10
7.10.1. Which of the following are convergent, and which are summable?
A=
ï£«
ï£­
âˆ’1/2
3/2
âˆ’3/2
1
0
âˆ’1/2
1
âˆ’1
1/2
ï£¶
ï£¸. B=
ï£«
ï£­
0
1
0
0
0
1
1
0
0
ï£¶
ï£¸. C=
ï£«
ï£­
âˆ’1
âˆ’2
âˆ’3/2
1
2
1
1
1
3/2
ï£¶
ï£¸.
7.10.2. For the matrices in Exercise 7.10.1, evaluate the limit of each convergent
matrix, and evaluate the Ces`aro limit for each summable matrix.
7.10.3. Verify that the expressions in (7.10.4) are indeed the solutions to the
diï¬€erence equations in (7.10.3).
7.10.4. Determine the limiting vector for the shell game in Example 7.10.8 by
ï¬rst computing the Ces`aro limit G with a full-rank factorization.
7.10.5. Verify that the expressions in (7.10.4) are indeed the solutions to the
diï¬€erence equations in (7.10.3).
7.10.6. Prove that if there exists a matrix norm such that âˆ¥Aâˆ¥< 1, then
limkâ†’âˆAk = 0.
7.10.7. By examining the iteration matrix, compare the convergence of Jacobiâ€™s
method and the Gaussâ€“Seidel method for each of the following coeï¬ƒcient
matrices with an arbitrary right-hand side. Explain why this shows that
neither method can be universally favored over the other.
A1 =
ï£«
ï£­
1
2
âˆ’2
1
1
1
2
2
1
ï£¶
ï£¸.
A2 =
ï£«
ï£­
2
âˆ’1
1
2
2
2
âˆ’1
âˆ’1
2
ï£¶
ï£¸.

7.10 Diï¬€erence Equations, Limits, and Summability
639
7.10.8. Let A =

2
âˆ’1
0
âˆ’1
2
âˆ’1
0
âˆ’1
2

(the ï¬nite-diï¬€erence Example 1.4.1, p. 19).
(a)
Verify that A satisï¬es the special case conditions given in Ex-
ample 7.10.6 that guarantee the validity of (7.10.24).
(b)
Determine the optimum SOR relaxation parameter.
(c)
Find the asymptotic rates of convergence for Jacobi, Gaussâ€“
Seidel, and optimum SOR.
(d)
Use x(0) = (1, 1, 1)T and b = (2, 4, 6)T to run through sev-
eral steps of Jacobi, Gaussâ€“Seidel, and optimum SOR to solve
Ax = b until you can see a convergence pattern.
7.10.9. Prove that if Ï (HÏ‰) < 1, where HÏ‰ is the iteration matrix for the SOR
method, then 0 < Ï‰ < 2. Hint: Use det (HÏ‰) to show |Î»k| â‰¥|1 âˆ’Ï‰|
for some Î»k âˆˆÏƒ (HÏ‰) .
7.10.10. Show that the spectral radius of the Jacobi iteration matrix for the
discrete Laplacian Ln2Ã—n2 described in Example 7.6.2 (p. 563) is
Ï (HJ) = cos Ï€/(n + 1).
7.10.11. Consider a scalar sequence {Î±1, Î±2, Î±3, . . .} and the associated Ces`aro
sequence of averages {Âµ1, Âµ2, Âµ3, . . .}, where Âµn = (Î±1+Î±2+Â· Â· Â·+Î±n)/n.
Prove that if {Î±n} converges to Î±, then {Âµn} also converges to Î±.
Note: Like scalars, a vector sequence {vn} in a ï¬nite-dimensional space
converges to v if and only if for each Ïµ > 0 there is a natural number
N = N(Ïµ) such that âˆ¥vn âˆ’vâˆ¥< Ïµ for all n â‰¥N, and, by virtue of
Example 5.1.3 (p. 276), it doesnâ€™t matter which norm is used. Therefore,
your proof should also be valid for vectors (and matrices).
7.10.12. M-matrices Revisited. For matrices with nonpositive oï¬€-diagonal en-
tries (Z-matrices), prove that the following statements are equivalent.
(a)
A is an M-matrix.
(b)
All leading principal minors of A are positive.
(c)
A has an LU factorization, and both L and U are M-matrices.
(d)
There exists a vector x > 0 such that Ax > 0.
(e)
Each aii > 0 and AD is diagonally dominant for some diago-
nal matrix D with positive diagonal entries.
(f)
Ax â‰¥0 implies x â‰¥0.

640
Chapter 7
Eigenvalues and Eigenvectors
7.10.13. Index by Full-Rank Factorization.
Suppose that Î» âˆˆÏƒ (A) , and
let M1 = Aâˆ’Î»I. The following procedure yields the value of index (Î»).
Factor M1 = B1C1 as a full-rank factorization.
Set M2 = C1B1.
Factor M2 = B2C2 as a full-rank factorization.
Set M3 = C2B2.
...
In general, Mi = Ciâˆ’1Biâˆ’1, where Miâˆ’1 = Biâˆ’1Ciâˆ’1 is a full-rank
factorization.
(a)
Explain why this procedure must eventually produce a matrix
Mk that is either nonsingular or zero.
(b)
Prove that if k is the smallest positive integer such that Mâˆ’1
k
exists or Mk = 0, then
index (Î») =
	
k âˆ’1
if Mk is nonsingular,
k
if Mk = 0.
7.10.14. Use the procedure in Exercise 7.10.13 to ï¬nd the index of each eigenvalue
of A =
 âˆ’3
âˆ’8
âˆ’9
5
11
9
âˆ’1
âˆ’2
1

. Hint: Ïƒ (A) = {4, 1}.
7.10.15. Let A be the matrix given in Exercise 7.10.14.
(a)
Find the Jordan form for A.
(b)
For any function f deï¬ned at A, ï¬nd the Hermite interpolation
polynomial that is described in Example 7.9.4 (p. 606), and
describe f(A).
7.10.16. Limits and Group Inversion. Given a matrix BnÃ—n of rank r such
that index (B) â‰¤1 (i.e., index (Î» = 0) â‰¤1 ), the Jordan form for B
looks like
 0
0
0
CrÃ—r

= Pâˆ’1BP, so B = P
 0
0
0
C

Pâˆ’1, where C
is nonsingular. This implies that B belongs to an algebraic group G
with respect to matrix multiplication, and the inverse of B in G is
B# = P
 0
0
0
Câˆ’1

Pâˆ’1. Naturally, B# is called the group inverse of
B. The group inverse is a special case of the Drazin inverse discussed in
Example 5.10.5 on p. 399, and properties of group inversion are devel-
oped in Exercises 5.10.11â€“5.10.13 on p. 402. Prove that if limkâ†’âˆAk
exists, and if B = I âˆ’A, then
lim
kâ†’âˆAk = I âˆ’BB#.
In other words, the limiting matrix can be characterized as the diï¬€er-
ence of two identity elementsâ€” I is the identity in the multiplicative
group of nonsingular matrices, and BB# is the identity element in the
multiplicative group containing B.

7.10 Diï¬€erence Equations, Limits, and Summability
641
7.10.17. If MnÃ—n is a group matrix (i.e., if index (M) â‰¤1 ), then the group
inverse of M can be characterized as the unique solution M# of the
equations MM#M = M, M#MM# = M#, and MM# = M#M.
In fact, some authors use these equations to deï¬ne M#. Use this char-
acterization to show that if M = BC is any full-rank factorization of
M, then M# = B(CB)âˆ’2C. In particular, if M = U1DVâˆ—
1 is the
full-rank factorization derived from the singular value decomposition as
described in (7.10.38), then
M# = U1Dâˆ’1/2(Vâˆ—
1U1)âˆ’2Dâˆ’1/2Vâˆ—
1
= U1Dâˆ’1(Vâˆ—
1U1)âˆ’2Vâˆ—
1
= U1(Vâˆ—
1U1)âˆ’2Dâˆ’1Vâˆ—
1.

642
Chapter 7
Eigenvalues and Eigenvectors
7.11
MINIMUM POLYNOMIALS AND KRYLOV METHODS
The characteristic polynomial plays a central role in the theoretical development
of linear algebra and matrix analysis, but it is not alone in this respect. There
are other polynomials that occur naturally, and the purpose of this section is to
explore some of them.
In this section it is convenient to consider the characteristic polynomial of
A âˆˆCnÃ—n to be c(x) = det (xI âˆ’A). This diï¬€ers from the deï¬nition given on
p. 492 only in the sense that the coeï¬ƒcients of c(x) = det (xI âˆ’A) have diï¬€erent
signs than the coeï¬ƒcients of Ë†c(x) = det (A âˆ’xI). In particular, c(x) is a monic
polynomial (i.e., its leading coeï¬ƒcient is 1), whereas the leading coeï¬ƒcient of Ë†c(x)
is (âˆ’1)n. (Of course, the roots of c and Ë†c are identical.)
Monic polynomials p(x) such that p(A) = 0 are said to be annihilating
polynomials for A. For example, the Cayleyâ€“Hamilton theorem (pp. 509, 532)
guarantees that c(x) is an annihilating polynomial of degree n.
Minimum Polynomial for a Matrix
There is a unique annihilating polynomial for A of minimal degree, and
this polynomial, denoted by m(x), is called the minimum polynomial
for A. The Cayleyâ€“Hamilton theorem guarantees that deg[m(x)] â‰¤n.
Proof.
Only uniqueness needs to be proven. Let k be the smallest degree of
any annihilating polynomial for A. There is a unique annihilating polynomial
for A of degree k because if there were two diï¬€erent annihilating polynomials
p1(x) and p2(x) of degree k, then d(x) = p1(x) âˆ’p2(x) would be a nonzero
polynomial such that d(A) = 0 and deg[d(x)] < k. Dividing d(x) by its leading
coeï¬ƒcient would produce an annihilating polynomial of degree less than k, the
minimal degree, and this is impossible.
The ï¬rst problem is to describe what the minimum polynomial m(x) for
A âˆˆCnÃ—n looks like, and the second problem is to uncover the relationship
between m(x) and the characteristic polynomial c(x). The Jordan form for
A reveals everything. Suppose that A = PJPâˆ’1, where J is in Jordan form.
Since p(A) = 0 if and only if p(J) = 0 or, equivalently, p(Jâ‹†) = 0 for each
Jordan block Jâ‹†, itâ€™s clear that m(x) is the monic polynomial of smallest degree
that annihilates all Jordan blocks. If Jâ‹†is a k Ã— k Jordan block associated
with an eigenvalue Î», then (7.9.2) on p. 600 insures that p(Jâ‹†) = 0 if and
only if p(i)(Î») = 0 for i = 0, 2, . . . , k âˆ’1, and this happens if and only if
p(x) = (x âˆ’Î»)kq(x) for some polynomial q(x). Since this must be true for
all Jordan blocks associated with Î», it must be true for the largest Jordan
block associated with Î», and thus the minimum degree monic polynomial that

7.11 Minimum Polynomials and Krylov Methods
643
annihilates all Jordan blocks associated with Î» is
pÎ»(x) = (x âˆ’Î»)kÎ»,
where
kÎ» = index (Î»).
Since the minimum polynomial for A must annihilate the largest Jordan block
associated with each Î»j âˆˆÏƒ (A) , it follows that
m(x) = (x âˆ’Î»1)k1(x âˆ’Î»2)k2 Â· Â· Â· (x âˆ’Î»s)ks,
where
kj = index (Î»j) (7.11.1)
is the minimum polynomial for A.
Example 7.11.1
Minimum Polynomial, Gramâ€“Schmidt, and QR.
If you are willing to
compute the eigenvalues Î»j and their indicies kj for a given A âˆˆCnÃ—n, then,
as shown in (7.11.1), the minimum polynomial for A âˆˆCnÃ—n is obtained by
setting m(x) = (x âˆ’Î»1)k1(x âˆ’Î»2)k2 Â· Â· Â· (x âˆ’Î»s)ks. But ï¬nding the eigenvalues
and their indicies can be a substantial task, so letâ€™s consider how we might
construct m(x) without computing eigenvalues. An approach based on ï¬rst
principles is to determine the ï¬rst matrix Ak for which {I, A, A2, . . . , Ak} is
linearly dependent. In other words, if k is the smallest positive integer such that
Ak = kâˆ’1
j=0 Î±jAj, then the minimum polynomial for A is
m(x) = xk âˆ’
kâˆ’1

j=0
Î±jxj.
The Gramâ€“Schmidt orthogonalization procedure (p. 309) with the standard in-
ner product âŸ¨A BâŸ©= trace (Aâˆ—B) (p. 286) is the perfect theoretical tool for
determining k and the Î±j â€™s. Gramâ€“Schmidt applied to {I, A, A2, . . .} begins
by setting U0 = I/ âˆ¥Iâˆ¥F = I/âˆšn, and it proceeds by sequentially computing
Uj =
Aj âˆ’jâˆ’1
i=0
8
Ui Aj9
Ui
âˆ¥Aj âˆ’jâˆ’1
i=0 âŸ¨Ui AjâŸ©Uiâˆ¥F
for
j = 1, 2, . . .
(7.11.2)
until Ak âˆ’kâˆ’1
i=0
8
Ui Ak9
Ui = 0. The ï¬rst such k is the smallest positive in-
teger such that Ak âˆˆspan {U0, U1, . . . , Ukâˆ’1} = span

I, A, . . . , Akâˆ’1
. The
coeï¬ƒcients Î±j such that Ak = kâˆ’1
j=0 Î±jAj are easily determined from the
upper-triangular matrix R in the QR factorization produced by the Gramâ€“
Schmidt process. To see how, extend the notation in the discussion on p. 311 in
an obvious way to write (7.11.2) in matrix form as

I | A | Â· Â· Â· | Ak
=

U0 | U1 | Â· Â· Â· | Uk

ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î½0
r01
Â· Â· Â·
r0kâˆ’1
r0k
0
Î½1
Â· Â· Â·
r1kâˆ’1
r1k
...
...
...
...
...
0
0
Î½kâˆ’1
rkâˆ’1k
0
0
Â· Â· Â·
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
, (7.11.3)

644
Chapter 7
Eigenvalues and Eigenvectors
where Î½0 = âˆ¥Iâˆ¥F = âˆšn , Î½j =
)))Aj âˆ’jâˆ’1
i=0
8
Ui Aj9
Ui
)))
F, and rij =
8
Ui Aj9
.
If we set R =
ï£«
ï£­
Î½0
Â· Â· Â·
r0kâˆ’1
...
...
Î½kâˆ’1
ï£¶
ï£¸and c =
ï£«
ï£­
r0k
...
rkâˆ’1k
ï£¶
ï£¸, then (7.11.3) implies that
Ak =

U0| Â· Â· Â· |Ukâˆ’1

c =

I| Â· Â· Â· |Akâˆ’1
Râˆ’1c, so Râˆ’1c =
ï£«
ï£­
Î±0
...
Î±kâˆ’1
ï£¶
ï£¸contains
the coeï¬ƒcients such that Ak = kâˆ’1
j=0 Î±jAj, and thus the coeï¬ƒcients in the
minimum polynomial are determined.
Caution!
While Gramâ€“Schmidt works ï¬ne to produce m(x) in exact arith-
metic, things are not so nice in ï¬‚oating-point arithmetic. For example, if A
has a dominant eigenvalue, then, as explained in the power method (Example
7.3.7, p. 533), Ak asymptotically approaches the dominant spectral projector
G1, so, as k grows, Ak becomes increasingly close to span

I, A, . . . , Akâˆ’1
.
Consequently, ï¬nding the ï¬rst Ak that is truly in span

I, A, . . . , Akâˆ’1
is
an ill-conditioned problem, and Gramâ€“Schmidt may not work well in ï¬‚oating-
point arithmeticâ€”the modiï¬ed Gramâ€“Schmidt algorithm (p. 316), or a version
of Householder reduction (p. 341), or Arnoldiâ€™s method (p. 653) works better.
Fortunately, explicit knowledge of the minimum polynomial often is not needed
in applied work.
The relationship between the characteristic polynomial c(x) and the mini-
mum polynomial m(x) for A is now transparent. Since
c(x) = (x âˆ’Î»1)a1(x âˆ’Î»2)a2 Â· Â· Â· (x âˆ’Î»s)as,
where
aj = alg mult (Î»j),
and
m(x) = (x âˆ’Î»1)k1(x âˆ’Î»2)k2 Â· Â· Â· (x âˆ’Î»s)ks,
where
kj = index (Î»j),
itâ€™s clear that m(x) divides c(x). Furthermore, m(x) = c(x) if and only if
alg mult (Î»j) = index (Î»j) for each Î»j âˆˆÏƒ (A) . Matrices for which m(x) = c(x)
are said to be nonderogatory matrices, and they are precisely the ones for
which geo mult (Î»j) = 1 for each eigenvalue Î»j because
m(x) = c(x) â‡â‡’alg mult (Î»j) = index (Î»j) for each j
â‡â‡’there is only one Jordan block for each Î»j
â‡â‡’there is only one independent eigenvector for each Î»j
â‡â‡’geo mult (Î»j) = 1 for each Î»j.
In addition to dividing the characteristic polynomial c(x), the minimum
polynomial m(x) divides all other annihilating polynomials p(x) for A be-
cause deg[m(x)] â‰¤deg[p(x)] insures the existence of polynomials q(x) and
r(x) (quotient and remainder) such that
p(x) = m(x)q(x) + r(x),
where
deg[r(x)] < deg[m(x)].

7.11 Minimum Polynomials and Krylov Methods
645
Since
0 = p(A) = m(A)q(A) + r(A) = r(A),
it follows that r(x) = 0; otherwise r(x), when normalized to be monic, would
be an annihilating polynomial having degree smaller than the degree of the min-
imum polynomial.
The structure of the minimum polynomial for A is related to the diago-
nalizability of A. By combining the fact that kj = index (Î»j) is the size of
the largest Jordan block for Î»j with the fact that A is diagonalizable if and
only if all Jordan blocks are 1 Ã— 1, it follows that A is diagonalizable if and
only if kj = 1 for each j, which, by (7.11.1), is equivalent to saying that
m(x) = (x âˆ’Î»1)(x âˆ’Î»2) Â· Â· Â· (x âˆ’Î»s). In other words, A is diagonalizable if and
only if its minimum polynomial is the product of distinct linear factors.
Below is a summary of the preceding observations about properties of m(x).
Properties of the Minimum Polynomial
Let A âˆˆCnÃ—n with Ïƒ (A) = {Î»1, Î»2, . . . , Î»s} .
â€¢
The minimum polynomial of A is the unique monic polynomial
m(x) of minimal degree such that m(A) = 0.
â€¢
m(x) = (x âˆ’Î»1)k1(x âˆ’Î»2)k2 Â· Â· Â· (x âˆ’Î»s)ks, where kj = index (Î»j).
â€¢
m(x) divides every polynomial p(x) such that p(A) = 0. In par-
ticular, m(x) divides the characteristic polynomial c(x).
(7.11.4)
â€¢
m(x) = c(x) if and only if geo mult (Î»j) = 1 for each Î»j or,
equivalently, alg mult (Î»j) = index (Î»j) for each j, in which case
A is called a nonderogatory matrix.
â€¢
A is diagonalizable if and only if m(x) = (xâˆ’Î»1)(xâˆ’Î»2) Â· Â· Â· (xâˆ’Î»s)
(i.e., if and only if m(x) is a product of distinct linear factors).
The next immediate aim is to extend the concept of the minimum polyno-
mial for a matrix to formulate the notion of a minimum polynomial for a vector.
To do so, itâ€™s helpful to introduce Krylov
86 sequences, subspaces, and matrices.
86
Aleksei Nikolaevich Krylov (1863â€“1945) showed in 1931 how to use sequences of the form
{b, Ab, A2b, . . .} to construct the characteristic polynomial of a matrix (see Example 7.11.3
on p. 649). Krylov was a Russian applied mathematician whose scientiï¬c interests arose from
his early training in naval science that involved the theories of buoyancy, stability, rolling
and pitching, vibrations, and compass theories. Krylov served as the director of the Physicsâ€“
Mathematics Institute of the Soviet Academy of Sciences from 1927 until 1932, and in 1943
he was awarded a â€œstate prizeâ€ for his work on compass theory. Krylov was made a â€œhero of

646
Chapter 7
Eigenvalues and Eigenvectors
Krylov Sequences, Subspaces, and Matrices
For A âˆˆCnÃ—n and 0 Ì¸= b âˆˆCnÃ—1, we adopt the following terminology.
â€¢
{b, Ab, A2b, . . . , Ajâˆ’1b} is called a Krylov sequence.
â€¢
Kj = span

b, Ab, . . . , Ajâˆ’1b

is called a Krylov subspace.
â€¢
KnÃ—j =

b | Ab | Â· Â· Â· | Ajâˆ’1b

is called a Krylov matrix.
Since dim(Kj) â‰¤n (because Kj âŠ†CnÃ—1 ), there is a ï¬rst vector Akb in
the Krylov sequence that is a linear combination of preceding Krylov vectors. If
Akb =
kâˆ’1

j=0
Î±jAjb,
then we deï¬ne
v(x) = xk âˆ’
kâˆ’1

j=0
Î±jxj,
and we say that v(x) is an annihilating polynomial for b relative to A
because v(x) is a monic polynomial such that v(A)b = 0. The argument on
p. 642 that establishes uniqueness of the minimum polynomial for matrices can
be reapplied to prove that for each matrixâ€“vector pair (A, b) there is a unique
annihilating polynomial of b relative to A that has minimal degree. These
observations are formalized below.
Minimum Polynomial for a Vector
â€¢
The minimum polynomial for b âˆˆCnÃ—1 relative to A âˆˆCnÃ—n
is deï¬ned to be the monic polynomial v(x) of minimal degree such
that v(A)b = 0.
â€¢
If Akb is the ï¬rst vector in the Krylov sequence {b, Ab, A3b, . . .}
that is a linear combination of preceding Krylov vectors (say
Akb = kâˆ’1
j=0 Î±jAjb ), then v(x) = xk âˆ’kâˆ’1
j=0 Î±jxj (or v(x) = 1
when b = 0 ) is the minimum polynomial for b relative to A.
socialist labor,â€ and he is one of a few mathematicians to have a lunar feature named in his
honorâ€”on the moon there is the â€œCrater Krylov.â€

7.11 Minimum Polynomials and Krylov Methods
647
So is the minimum polynomial for a matrix related to minimum polynomials
for vectors? It seems intuitive that knowing the minimum polynomial of b rela-
tive to A for enough diï¬€erent vectors b should somehow lead to the minimum
polynomial for A. This is indeed the case, and here is how itâ€™s done. Recall that
the least common multiple (LCM) of polynomials v1(x), . . . , vn(x) is the unique
monic polynomial l(x) such that
(i)
each vi(x) divides l(x);
(ii)
if each vi(x) also divides q(x), then l(x) divides q(x).
Minimum Polynomial as LCM
Let A âˆˆCnÃ—n, and let B = {b1, b2, . . . , bn} be any basis for CnÃ—1.
If vi(x) is the minimum polynomial for bi relative to A, then the
minimum polynomial m(x) for A is the least common multiple of
v1(x), v2(x), . . . , vn(x).
(7.11.5)
Proof.
The strategy ï¬rst is to prove that if l(x) is the LCM of the vi(x) â€™s,
then m(x) divides l(x). Then prove the reverse by showing that l(x) also
divides m(x). Since each vi(x) divides l(x), it follows that l(A)bi = 0 for
each i. In other words, B âŠ‚N(l(A)), so dim N(l(A)) = n or, equivalently,
l(A) = 0. Therefore, by property (7.11.4) on p. 645, m(x) divides l(x). Now
show that l(x) divides m(x) . Since m(A)bi = 0 for every bi, it follows that
deg[vi(x)] < deg[m(x)] for each i, and hence there exist polynomials qi(x) and
ri(x) such that m(x) = qi(x)vi(x) + ri(x), where deg[ri(x)] < deg[vi(x)]. But
0 = m(A)bi = qi(A)vi(A)bi + ri(A)bi = ri(A)bi
insures ri(x) = 0, for otherwise ri(x) (when normalized to be monic) would be
an annihilating polynomial for bi of degree smaller than the minimum polyno-
mial for bi, which is impossible. In other words, each vi(x) divides m(x), and
this implies l(x) must also divide m(x). Therefore, since m(x) and l(x) are
divisors of each other, it must be the case that m(x) = l(x).
The utility of this result is illustrated in the following development. We
already know that associated with n Ã— n matrix A is an nth-degree monic
polynomialâ€”namely, the characteristic polynomial c(x) = det (xI âˆ’A). But
the reverse is also true. That is, every nth-degree monic polynomial is the char-
acteristic polynomial of some n Ã— n matrix.

648
Chapter 7
Eigenvalues and Eigenvectors
Companion Matrix of a Polynomial
For each monic polynomial p(x) = xn + Î±nâˆ’1xnâˆ’1 + Â· Â· Â· + Î±1x + Î±0,
the companion matrix of p(x) is deï¬ned (by G. Frobenius) to be
C =
ï£«
ï£¬
ï£¬
ï£­
0
0
Â· Â· Â·
0
âˆ’Î±0
1
0
Â· Â· Â·
0
âˆ’Î±1
...
...
...
...
0
Â· Â· Â·
1
0
âˆ’Î±nâˆ’2
0
0
Â· Â· Â·
1
âˆ’Î±nâˆ’1
ï£¶
ï£·
ï£·
ï£¸
nÃ—n
.
(7.11.6)
â€¢
The polynomial p(x) is both the characteristic and minimum poly-
nomial for C (i.e., C is nonderogatory).
Proof.
To prove that det (xI âˆ’C) = p(x), write C = N âˆ’ceT
n, where
N =
ï£«
ï£¬
ï£¬
ï£­
0
1
...
...
...
1
0
ï£¶
ï£·
ï£·
ï£¸
and
c =
ï£«
ï£¬
ï£­
Î±0
Î±1
...
Î±nâˆ’1
ï£¶
ï£·
ï£¸,
and use (6.2.3) on p. 475 to conclude that
det (xI âˆ’C) = det (xI âˆ’N)(1 + eT
ndet (xI âˆ’N)âˆ’1c)
= xn

1 + eT
n
 I
x + N
x2 + N2
x3 + Â· Â· Â· + Nnâˆ’1
xn

c

= xn + Î±nâˆ’1xnâˆ’1 + Î±nâˆ’2xnâˆ’2 + Â· Â· Â· + Î±0
= p(x).
The fact that p(x) is also the minimum polynomial for C is a consequence of
(7.11.5). Set B = {e1, e2, . . . , en} , and let vi(x) be the minimum polynomial
of ei with respect to C. Observe that v1(x) = p(x) because Cej = ej+1 for
j = 1, . . . , n âˆ’1, so
{e1, Ce1, C2e1, . . . , Cnâˆ’1e1} = {e1, e2, e3, . . . , en}
and
Cne1 = Cen = Câˆ—n = âˆ’
nâˆ’1

j=0
Î±jej+1 = âˆ’
nâˆ’1

j=0
Î±jCje1
=â‡’
v1(x) = p(x).
Since v1(x) divides the LCM of all vi(x) â€™s (which we know from (7.11.5) to be
the minimum polynomial m(x) for C ), we conclude that p(x) divides m(x).
But m(x) always divides p(x) â€”recall (7.11.4)â€”so m(x) = p(x).

7.11 Minimum Polynomials and Krylov Methods
649
Example 7.11.2
Poor Manâ€™s Root Finder. The companion matrix is the source of what is
often called the poor manâ€™s root ï¬nder because any general purpose algorithm
designed to compute eigenvalues (e.g., the QR iteration on p. 535) can be applied
to the companion matrix for a polynomial p(x) to compute the roots of p(x).
When used in conjunction with (7.1.12) on p. 497, the companion matrix is also
a poor manâ€™s root bounder. For example, it follows that if Î» is a root of p(x),
then
|Î»| â‰¤âˆ¥Câˆ¥âˆ= max{|Î±0|, 1 + |Î±1|, . . . , 1 + |Î±nâˆ’1|} â‰¤1 + max |Î±i|.
The results on p. 647 insure that the minimum polynomial v(x) for every
nonzero vector b relative to A âˆˆCnÃ—n divides the minimum polynomial m(x)
for A, which in turn divides the characteristic polynomial c(x) for A, so it
follows that every v(x) divides c(x). This suggests that it might be possible to
construct c(x) as a product of vi(x) â€™s. In fact, this is what Krylov did in 1931,
and the following example shows how he did it.
Example 7.11.3
Krylovâ€™s method for constructing the characteristic polynomial for A âˆˆCnÃ—n
as a product of minimum polynomials for vectors is as follows.
Starting with any nonzero vector bnÃ—1, let v1(x) = xkâˆ’kâˆ’1
j=0 Î±jxj be the min-
imum polynomial for b relative to A, and let K1 =

b | Ab | Â· Â· Â· | Akâˆ’1b

nÃ—k
be the associated Krylov matrix. Notice that rank (K1) = k (by deï¬nition of
the minimum polynomial for b ). If C1 is the k Ã— k companion matrix of v(x)
as described in (7.11.6), then direct multiplication shows that
K1C1 = AK1.
(7.11.7)
If k = n, then Kâˆ’1
1 AK1 = C1, so v1(x) must be the characteristic polynomial
for A, and there is nothing more to do. If k < n, then use any n Ã— (n âˆ’k)
matrix 7K1 such that K2 =

K1 | 7K1

nÃ—n is nonsingular, and use (7.11.7) to
write
AK2 =

AK1 | A 7K1

=

K1 | 7K1
 
C1
X
0
A2

,
where

X
A2

= Kâˆ’1
2 A 7K1.
Therefore, Kâˆ’1
2 AK2 =
 C1
X
0
A2

, and hence
c(x) = det (xI âˆ’A) = det (xI âˆ’C1)det (xI âˆ’A2) = v1(x) det (xI âˆ’A2).

650
Chapter 7
Eigenvalues and Eigenvectors
Repeat the process on A2. If the Krylov matrix on the second time around is
nonsingular, then c(x) = v1(x)v2(x); otherwise c(x) = v1(x)v2(x) det (xI âˆ’A3)
for some matrix A3. Continuing in this manner until a nonsingular Krylov
matrix is obtainedâ€”say at the mth stepâ€”produces a nonsingular matrix K
such that
Kâˆ’1AK=
ï£«
ï£­
C1
Â· Â· Â·
â‹†
...
...
Cm
ï£¶
ï£¸= H,
(7.11.8)
where the Cj â€™s are companion matrices, and thus c(x) = v1(x)v2(x) Â· Â· Â· vm(x).
Note: All companion matrices are upper-Hessenberg matrices as described in
Example 5.7.4 (p. 350)â€”e.g., a 5 Ã— 5 Hessenberg form is
H5 =
ï£«
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
âˆ—
0
0
âˆ—
âˆ—
âˆ—
0
0
0
âˆ—
âˆ—
ï£¶
ï£¸.
Since the matrix H in (7.11.8) is upper Hessenberg, we see that Krylovâ€™s method
boils down to a recipe for using Krylov sequences to build a similarity transfor-
mation that will reduce A to upper-Hessenberg form. In eï¬€ect, this means that
most information about A can be derived from Krylov sequences and the asso-
ciated Hessenberg form H. This is the real message of this example.
Deriving information about A by using a Hessenberg form and a Krylov
similarity transformation as shown in (7.11.8) has some theoretical appeal, but
itâ€™s not a practical idea as far as computation is concerned. Krylov sequences tend
to be nearly linearly dependent sets because, as the power method of Example
7.3.7 (p. 533) indicates, the directions of the vectors Akb want to converge to
the direction of an eigenvector for A, so, as k grows, the vectors in a Krylov
sequence become ever closer to being multiples of each other. This means that
Krylov matrices tend to be ill conditioned. Putting conditioning issues aside,
there is still a problem with computational eï¬ƒciency because K is usually a
dense matrix (one with a preponderance of nonzero entries) even when A is
sparse (which it often is in applied work), so the amount of arithmetic involved
in the reduction (7.11.8) is prohibitive.
However, these objections often can be overcome by replacing a Krylov
matrix K =

b | Ab | Â· Â· Â· | Akâˆ’1b

with its QR factorization K = QnÃ—kRkÃ—k.
Doing so in (7.11.7) (and dropping the subscript) produces
AK = KC
=â‡’
AQR = QRC
=â‡’
Qâˆ—AQ = RCRâˆ’1 = H.
(7.11.9)
While H = RCRâˆ’1 is no longer a companion matrix, itâ€™s still in upper-
Hessenberg form (convince yourself by writing out the pattern for the 4 Ã— 4
case). In other words, an orthonormal basis for a Krylov subspace can reduce a

7.11 Minimum Polynomials and Krylov Methods
651
matrix to upper-Hessenberg form. Since matrices with orthonormal columns are
perfectly conditioned, the ï¬rst objection raised above is overcome. The second
objection concerning computational eï¬ƒciency is dealt with in Examples 7.11.4
and 7.11.5.
If k < n, then Q is not square, and Qâˆ—AQ = H is not a similarity
transformation, so it would be wrong to conclude that A and H have the same
spectral properties. Nevertheless, itâ€™s often the case that the eigenvalues of H,
which are called the Ritz values for A, are remarkably good approximations to
the extreme eigenvalues of A, especially when A is hermitian. This is somewhat
intuitive because Qâˆ—AQ can be viewed as a generalization of (7.5.4) on p. 549
that says Î»max = maxâˆ¥xâˆ¥2=1 xâˆ—Ax and Î»min = minâˆ¥xâˆ¥2=1 xâˆ—Ax. The results
of Exercise 5.9.15 (p. 392) can be used to argue the point further.
Example 7.11.4
Lanczos
87 Tridiagonalization Algorithm. The fact that the matrix H in
(7.11.9) is upper Hessenberg is particularly nice when A is real and symmetric
because AT = A implies HT = (QT AQ)T = H, and symmetric Hessenberg
matrices are tridiagonal in structure. That is,
H =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î±1
Î²1
Î²1
Î±2
Î²2
Î²2
Î±3
...
...
...
Î²nâˆ’1
Î²nâˆ’1
Î±n
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
when A = AT .
(7.11.10)
This makes Q particularly easy to determine. While the matrix Q in (7.11.9)
was only n Ã— k, letâ€™s be greedy and look for an n Ã— n orthogonal matrix Q
such that AQ = QH, where H is tridiagonal as depicted in (7.11.10). If we
set Q =

q1 | q2 | Â· Â· Â· | qn

, and if we agree to let Î²0 = 0 and qn+1 = 0, then
87
Cornelius Lanczos (1893â€“1974) was born KornÂ´el LÂ¨owy in Budapest, Hungary, to Jewish par-
ents, but he changed his name to avoid trouble during the dangerous times preceding World
War II. After receiving his doctorate from the University of Budapest in 1921, Lanczos moved
to Germany where he became Einsteinâ€™s assistant in Berlin in 1928. After coming home to
Germany from a visit to Purdue University in Lafayette, Indiana, in 1931, Lanczos decided
that the political climate in Germany was unacceptable, and he returned to Purdue in 1932 to
continue his work in mathematical physics. The development of electronic computers stimu-
lated Lanczosâ€™s interest in numerical analysis, and this led to positions at the Boeing Company
in Seattle and at the Institute for Numerical Analysis of the National Bureau of Standards
in Los Angeles. When senator Joseph R. McCarthy led a crusade against communism in the
1950s, Lanczos again felt threatened, so he left the United States to accept an oï¬€er from the
famous Nobel physicist Erwin SchrÂ¨odinger (1887â€“1961) to head the Theoretical Physics De-
partment at the Dublin Institute for Advanced Study in Ireland where Lanczos returned to his
ï¬rst loveâ€”the theory of relativity. Lanczos was aware of the fast Fourier transform algorithm
(p. 373) 25 years before the heralded work of J. W. Cooley and J. W. Tukey (p. 368) in 1965,
but 1940 was too early for applications of the FFT to be realized. This is yet another instance
where credit and fame are accorded to those who ï¬rst make good use of an idea rather than
to those who ï¬rst conceive it.

652
Chapter 7
Eigenvalues and Eigenvectors
equating the jth column of AQ to the jth column of QH tells us that we
must have
Aqj = Î²jâˆ’1qjâˆ’1 + Î±jqj + Î²jqj+1
for j = 1, 2, . . . , n
or, equivalently,
Î²jqj+1 = vj,
where
vj = Aqj âˆ’Î±jqj âˆ’Î²jâˆ’1qjâˆ’1
for j = 1, 2, . . . , n.
By observing that Î±j = qT
j Aqj and Î²j = âˆ¥vjâˆ¥2 , we are led to Lanczosâ€™s
algorithm.
â€¢
Start with an arbitrary b Ì¸= 0, set Î²0 = 0, q0 = 0, q1 = b/ âˆ¥bâˆ¥2 , and
iterate as indicated below.
For j = 1 to n
v
â†
Aqj
Î±j
â†
qT
j v
v
â†
v âˆ’Î±jqj âˆ’Î²jâˆ’1qjâˆ’1
Î²j
â†
âˆ¥vâˆ¥2
If Î²j = 0, then quit
qj+1 â†
v/Î²j
End
After the kth step we have an n Ã— (k + 1) matrix Qk+1 =

q1 | q2 | Â· Â· Â· | qk+1

of orthonormal columns such that
AQk = Qk+1

Tk
Î²keT
k

, where Tk is the k Ã— k tridiagonal form (7.11.10).
If the iteration terminates prematurely because Î²j = 0 for j < n, then restart
the algorithm with a new initial vector b that is orthogonal to q1, q2, . . . , qj.
When a full orthonormal set {q1, q2, . . . , qn} has been computed and turned
into an orthogonal matrix Q, we will have
QT AQ =
ï£«
ï£¬
ï£­
T1
0
Â· Â· Â·
0
0
T2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
Tm
ï£¶
ï£·
ï£¸,
where each Ti is tridiagonal
(7.11.11)
with the splits occurring at rows where the Î²j â€™s are zero. Of course, having these
splits is generally a desirable state of aï¬€airs, especially when the objective is to
compute the eigenvalues of A.
Note: The Lanczos algorithm is computationally eï¬ƒcient because if each row of
A has Î½ nonzero entries, then each matrixâ€“vector product uses Î½n multiplica-
tions, so each step of the process uses only Î½n + 4n multiplications (and about

7.11 Minimum Polynomials and Krylov Methods
653
the same number of additions). This can be a tremendous savings over what
is required by Householder (or Givens) reduction as discussed in Example 5.7.4
(p. 350). Once the form (7.11.11) has been determined, spectral properties of A
usually can be extracted by a variety of standard methods such as the QR iter-
ation (p. 535). An alternative to computing the full tridiagonal decomposition
is to stop the Lanczos iteration before completion, accept the Ritz values (the
eigenvalues HkÃ—k = QT
kÃ—nAQnÃ—k) as approximations to a portion of Ïƒ (A) ,
deï¬‚ate the problem, and repeat the process on the smaller result.
Even when A is not symmetric, the same logic that produces the Lanc-
zos algorithm can be applied to obtain an orthogonal matrix Q such that
QT AQ = H is upper Hessenberg. But we canâ€™t expect to obtain the eï¬ƒciency
that Lanczos provides because the tridiagonal structure is lost. The more general
algorithm is called Arnoldiâ€™s
88 method, and itâ€™s presented below.
Example 7.11.5
Arnoldi Orthogonalization Algorithm. Given A âˆˆCnÃ—n, the goal is to
compute an orthogonal matrix Q =

q1 | q2 | Â· Â· Â· | qn

such that QT AQ = H
is upper Hessenberg. Proceed in the manner that produced the Lanczos algorithm
by equating the jth column of AQ to the jth column of QH to obtain
Aqj =
j+1

i=1
qihij
=â‡’
qT
k Aqj =
j+1

i=1
qT
k qihij = hkj
for each 1 â‰¤k â‰¤j
=â‡’
hj+1,jqj+1 = Aqj âˆ’
j

i=1
qihij.
By observing that hj+1,j = âˆ¥vjâˆ¥2 for vj = Aqj âˆ’j
i=1 qihij, we are led to
Arnoldiâ€™s algorithm.
â€¢
Start with an arbitrary b Ì¸= 0, set q1 = b/ âˆ¥bâˆ¥2 , and then iterate as
indicated below.
88
Walter Edwin Arnoldi (1917â€“1995) was an American engineer who published this technique in
1951, not far from the time that Lanczosâ€™s algorithm emerged. Arnoldi received his undergrad-
uate degree in mechanical engineering from Stevens Institute of Technology, Hoboken, New
Jersey, in 1937 and his MS degree at Harvard University in 1939. He spent his career working
as an engineer in the Hamilton Standard Division of the United Aircraft Corporation where he
eventually became the divisionâ€™s chief researcher. He retired in 1977. While his research con-
cerned mechanical and aerodynamic properties of aircraft and aerospace structures, Arnoldiâ€™s
name is kept alive by his orthogonalization procedure.

654
Chapter 7
Eigenvalues and Eigenvectors
For j = 1 to n
v
â†
Aqj
For i = 1 to j
hij â†
qT
i v
v
â†
v âˆ’hijqi
End For
hj+1,j â†
âˆ¥vâˆ¥2
If hj+1,j = 0, then quit
qj+1
â†
v/hj+1,j
End For
(7.11.12)
After the kth step we have an n Ã— (k + 1) matrix Qk+1 =

q1 | q2 | Â· Â· Â· | qk+1

of orthonormal columns such that
AQk = Qk+1

Hk
hk+1,keT
k

,
(7.11.13)
where Hk is a k Ã— k upper-Hessenberg matrix.
Note: Remarks similar to those made about the Lanczos algorithm also hold
for Arnoldiâ€™s algorithm, but the computational eï¬ƒciency of Arnoldi is not as
great as that of Lanczos. Close examination of Arnoldiâ€™s method reveals that it
amounts to a modiï¬ed Gramâ€“Schmidt process (p. 316).
Krylov methods are a natural way to solve systems of linear equations. To
see why, suppose that AnÃ—nx = b with b Ì¸= 0 is a nonsingular system, and let
v(x) = xk âˆ’kâˆ’1
j=0 Î±jxj be the minimum polynomial of b with respect to A.
Since Î±0 Ì¸= 0 (otherwise v(x)/x would be an annihilating polynomial for b of
degree less than deg v), we have
Akb âˆ’
kâˆ’1

j=0
Î±jAjb = 0
=â‡’
A
%Akâˆ’1b âˆ’Î±kâˆ’1Akâˆ’2b âˆ’Â· Â· Â· âˆ’Î±1b
Î±0
&
= b.
In other words, the solution of Ax = b is somewhere in the Krylov space Kk.
A technique for sorting through Kk to ï¬nd the solution (or at least an
acceptable approximate solution) of Ax = b is to sequentially consider the
subspaces A(K1), A(K2), . . . , A(Kk), where at the jth step of the process the
vector xj âˆˆA(Kj) that is closest to b is used as an approximation to x. If
Qj is an n Ã— j orthogonal matrix whose columns constitute a basis for Kj,
then R (AQj) = A(Kj), so the vector xj âˆˆA(Kj) that is closest to b is the
orthogonal projection of b onto R (AQj). This means that xj is the least
squares solution of AQjz = b (p. 439). If the solution of this least squares
problem yields a vector xj such that the residual rj = b âˆ’AQjxj is zero
(or satisfactorily small), then set x = Qjxj, and quit. Otherwise move up one

7.11 Minimum Polynomials and Krylov Methods
655
dimension, and compute the least squares solution xj+1 of AQj+1z = b. Since
x âˆˆKk, the process is guaranteed to terminate in k â‰¤n steps or less (when
exact arithmetic is used). When Arnoldiâ€™s method is used to implement this idea,
the resulting algorithm is known as GMRES (an acronym for the generalized
minimal residual algorithm that was formulated by Yousef Saad and Martin H.
Schultz in 1986).
Example 7.11.6
GMRES Algorithm. To implement the idea discussed above by employing
Arnoldiâ€™s algorithm, recall from (7.11.13) that after j steps of the Arnoldi pro-
cess we have matrices Qj and Qj+1 with orthonormal columns that span Kj
and Kj+1, respectively, along with a j Ã— j upper-Hessenberg matrix Hj such
that
AQj = Qj+1 7Hj,
where
7Hj =

Hj
hj+1,jeT
j

.
Consequently the least squares solution of AQjz = b is the same as the least
squares solution of Qj+1 7Hjz = b, which in turn is the same as the least squares
solution of 7Hjz = QT
j+1b. But QT
j+1b = âˆ¥bâˆ¥2 e1 (because the ï¬rst column in
Qj+1 is b/ âˆ¥bâˆ¥2), so the GMRES algorithm is as follows.
â€¢
To compute the solution to a nonsingular linear system AnÃ—nx = b Ì¸= 0,
start with q1 = b/ âˆ¥bâˆ¥2 , and iterate as indicated below.
For j = 1 to n
execute the jth Arnoldi step in (7.11.12)
compute the least squares solution of 7Hjz = âˆ¥bâˆ¥2 e1 by using a QR
factorization of 7Hj (see Note at the end of the example)
If âˆ¥b âˆ’AQjzâˆ¥2 = 0 (or is satisfactorily small)
set x = Qjz, and quit (see Note at the end of the example)
End If
End For
The structure of the 7Hj â€™s allows us to update the QR factors of 7Hj to produce
the QR factors of 7Hj+1 with a single plane rotation (p. 333). To see how this
is done, consider what happens when moving from the third step to the fourth
step of the process. Let U3 =
 QT
vT

be the 4 Ã— 4 orthogonal matrix that was
previously accumulated (as a product of plane rotations) to give U3 7H3 =
 R3
0

with R3 being upper triangular so that 7H3 = QR3. Since

656
Chapter 7
Eigenvalues and Eigenvectors

U3
0
0
1

7H4 =

U3
0
0
1

ï£«
ï£¬
ï£¬
ï£­
â‹†
â‹†
7H3
â‹†
â‹†
0
0
0
â‹†
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
â‹†
â‹†
U3 7H3
â‹†
â‹†
0
0
0
â‹†
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£­
â‹†
â‹†
â‹†
â‹†
0
â‹†
â‹†
â‹†
0
0
â‹†
â‹†
0
0
0
â‹†
0
0
0
â‹†
ï£¶
ï£·
ï£¸,
a plane rotation of the form P45 =
ï£«
ï£­
1
1
1
c
s
âˆ’s c
ï£¶
ï£¸will annihilate the entry in
the lower-right-hand corner of this last array. Consequently, U4 = P45
 U3
0
0
1

is an orthogonal matrix such that U4 7H4 =
 R4
0

, where R4 is upper triangu-
lar, and this produces the QR factors of 7H4.
Note: The value of the residual norm âˆ¥b âˆ’AQjzâˆ¥2 at each step of GMRES is
available at almost no cost. To see why, notice that the previous discussion shows
that at the jth step there is a (j + 1) Ã— (j + 1) orthogonal matrix U =
 QT
vT

(that exists as an accumulation of plane rotations) such that U 7Hj =
 R
0

,
and this produces 7Hj = QR. The least squares solution of 7Hjz = âˆ¥bâˆ¥2 e1 is
obtained by solving Rz = QT âˆ¥bâˆ¥2 e1 (p. 314), so
âˆ¥b âˆ’AQjzâˆ¥2 =
)))âˆ¥bâˆ¥2 e1 âˆ’7Hjz
)))
2 =
)))âˆ¥bâˆ¥2 Ue1 âˆ’
 R
0

z
)))
2
=
)))âˆ¥bâˆ¥2
 QT
vT

e1 âˆ’
 R
0

z
)))
2 =
)))

0
âˆ¥bâˆ¥2 vT e1
)))
2
= âˆ¥bâˆ¥2 |uj+1,1|.
Since uj+1,1 is just the last entry in the accumulation of the various plane
rotations applied to e1, the cost of producing these values as the algorithm
proceeds is small, so deciding on the acceptability of an approximate solution at
each step in the GMRES algorithm is cheap.
When solving nonsingular symmetric systems Ax = b, a strategy similar
to the one that produced the GMRES algorithm can be adopted except that
the Lanczos procedure (p. 651) is used in place of the Arnoldi process (p. 653).
When this is done, the resulting algorithm is called MINRES (an acronym for
minimal residual algorithm), and, as you might guess, there is an increase in
computational eï¬ƒciency when Lanczos replaces Arnoldi. Historically, MINRES
preceded GMRES.
Another Krylov method that deserves mention is the conjugate gradient
algorithm, presented by Magnus R. Hestenes and Eduard Stiefel in 1952, that
is used to solve positive deï¬nite systems.

7.11 Minimum Polynomials and Krylov Methods
657
Example 7.11.7
Conjugate Gradient Algorithm. Suppose that AnÃ—nx = b Ì¸= 0 is a (real)
positive deï¬nite system, and suppose that the minimum polynomial of b with
respect to A is v(x) = xk âˆ’kâˆ’1
j=0 Î±jxj so that the solution x is somewhere
in the Krylov space Kk (p. 654). The conjugate gradient algorithm emanated
from the observation that if A is positive deï¬nite, then the quadratic function
f(x) = xT Ax
2
âˆ’xT b
has as its gradient
âˆ‡f(x) = Ax âˆ’b,
and there is a unique minimizer for f that happens to be the solution of Ax = b.
Consequently, any technique that attempts to minimize f is a technique that
attempts to solve Ax = b. Since the x is somewhere in Kk, it makes sense
to try to minimize f over Kk. One approach for doing this is the method of
steepest descent in which a current approximation xj is updated by adding a
correction term directed along the negative gradient âˆ’âˆ‡f(xj) = b âˆ’Axj = rj
(the jth residual). In other words, let
xj+1 = xj + Î±jrj,
and set
Î±j =
rT
j rj
rT
j Arj
because this Î±j minimizes f(xj+1). In spite of the fact that successive residuals
are orthogonal (rT
j+1rj = 0), the rate of convergence can be slow because as the
ratio of eigenvalues Î»max(A)/Î»min(A) becomes larger, the surface deï¬ned by f
becomes more distorted, and a negative gradient rj need not point in a direction
aimed anywhere near the lowest point on the surface. An ingenious mechanism
for overcoming this diï¬ƒculty is to replace the search directions rj by directions
deï¬ned by vectors q1, q2, . . . that are conjugate to each other in the sense that
qT
i Aqj = 0 for all i Ì¸= j (some authors say â€œA-orthogonalâ€). Starting with
x0 = 0, the idea is to begin by moving in the direction of steepest descent with
x1 = Î±1q1,
where q1 = r0 = b and Î±1 =
rT
0 r0
rT
0 Ar0
,
but at the second step use a direction vector
q2 = r1 + Î²1q1,
where Î²1 is chosen to force qT
2 Aq1 = 0.
With a bit of eï¬€ort you can see that Î²1 = rT
1 r1/rT
0 r0 does the job. Then set
x2 = x1 + Î±2q2, and recycle the process. The formal algorithm is as follows.

658
Chapter 7
Eigenvalues and Eigenvectors
Formal Conjugate Gradient Algorithm. To compute the solution to a pos-
itive deï¬nite linear system AnÃ—nx = b, start with x0 = 0,
r0 = b, and
q1 = b, and iterate as indicated below.
For j = 1 to n
Î±j
â†
rT
jâˆ’1rjâˆ’1/qT
j Aqj
(step size)
xj
â†
xjâˆ’1 + Î±jqj
(approximate solution)
rj
â†
rjâˆ’1 âˆ’Î±jAqj
(residual)
If âˆ¥rjâˆ¥2 = 0 (or is satisfactorily small)
set x = xj, and quit
End If
Î²j
â†
rT
j rj/rT
jâˆ’1rjâˆ’1
(conjugation factor)
qj+1
â†
rj + Î²jqj
(search direction)
End For
It can be shown that vectors produced by this algorithm after j steps are such
that (in exact arithmetic)
span {x1, . . . , xj} = span {q1, . . . , qj} = span {r0, r1, . . . , rjâˆ’1} = Kj,
and, in addition to having qiAqj = 0 for i < j, the residuals are orthogonalâ€”
i.e., rT
i rj = 0 for i < j. Furthermore, the algorithm will ï¬nd the solution in
k â‰¤n steps.
As mentioned earlier, Krylov solvers such as GMRES and the conjugate
gradient algorithm produce the solution of Ax = b in k â‰¤n steps (in exact
arithmetic), so, at ï¬rst glance, this looks like good news. But in practice n can
be prohibitively large, and itâ€™s not rare to have k = n. Consequently, Krylov
algorithms are often viewed as iterative methods that are terminated long before
n steps have been completed. The challenge in applying Krylov solvers (as well as
iterative methods in general) revolves around the issue of how to replace Ax = b
with an equivalent preconditioned system Mâˆ’1Ax = Mâˆ’1b that requires
only a small number of iterations to deliver a reasonably accurate approximate
solution. Building eï¬€ective preconditioners Mâˆ’1 is part science and part art,
and the techniques vary from algorithm to algorithm.
Classical linear stationary iterative methods (p. 620) are formed by splitting
A = M âˆ’N and setting x(k) = Hx(k âˆ’1) + d, where H = Mâˆ’1N and
d = Mâˆ’1b. This is a preconditioning technique because the eï¬€ect is to replace
Ax = b by Mâˆ’1Ax = Mâˆ’1b, where Mâˆ’1A = I âˆ’H such that Ï (H) < 1.
The goal is to ï¬nd an easily inverted M (in the sense that Md = b is easily
solved) that drives the value of Ï (H) down far enough to insure a satisfactory
rate of convergence, and this is a delicate balancing act.

7.11 Minimum Polynomials and Krylov Methods
659
The goal in preconditioning Krylov solvers is somewhat diï¬€erent. For ex-
ample, if k = deg v(x) is the degree of the minimum polynomial of b with
respect to A, then GMRES sorts through Kk to ï¬nd the solution of Ax = b
in k steps. So the aim of preconditioning GMRES might be to manipulate the
interplay between Mâˆ’1b and Mâˆ’1A to insure that the degree of minimum
polynomial 7v(x) of Mâˆ’1b with respect to Mâˆ’1A is signiï¬cantly smaller than
k. Since this is diï¬ƒcult to do, an alternate goal is to try to reduce the degree
of the minimum polynomial 7m(x) for Mâˆ’1A because driving down deg 7m(x)
also drives down deg 7v(x)â€”remember, 7v(x) is a divisor of 7m(x) (p. 647). If a
preconditioner Mâˆ’1 can be found to force Mâˆ’1A to be diagonalizable with
only a few distinct eigenvalues (say j of them), then deg 7m(x) = j (p. 645),
and GMRES will ï¬nd the solution in no more than j steps. But this too is an
overly ambitious goal for practical problems. In reality this objective is compro-
mised by looking for a preconditioner such that Mâˆ’1A is diagonalizable whose
eigenvalues fall into a few small clustersâ€”say j of them. The hope is that if
Mâˆ’1A is diagonalizable, and if the diameters of the clusters are small enough,
then Mâˆ’1A will behave numerically like a diagonalizable matrix with j distinct
eigenvalues, so GMRES is inclined to produce reasonably accurate approxima-
tions in no more than j steps. While the intuition is simple, subtleties involving
the magnitudes of eigenvalues, separation of clusters, and the meaning of â€œsmall
diameterâ€ complicate the picture to make deï¬nitive statements and rigorous ar-
guments diï¬ƒcult to formulate. Constructing good preconditioners and proving
they actually work as advertised remains an active area of research in the ï¬eld
of numerical analysis.
Only the tip of the iceberg concerning practical applications of Krylov meth-
ods is revealed in this section. The analysis required to more fully understand the
numerical behavior of various Krylov methods can be found in several excellent
advanced texts specializing in matrix computations.
Exercises for section 7.11
7.11.1. Determine the minimum polynomial for A =

5
1
2
âˆ’4
0
âˆ’2
âˆ’4
âˆ’1
âˆ’1

.
7.11.2. Find the minimum polynomial of b = (âˆ’1, 1, 1)T with respect to the
matrix A given in Exercise 7.11.1.
7.11.3. Use Krylovâ€™s method to determine the characteristic polynomial for the
matrix A given in Exercise 7.11.1.
7.11.4. What is the Jordan form for a matrix whose minimum polynomial
is m(x) = (x âˆ’Î»)(x âˆ’Âµ)2 and whose characteristic polynomial is
c(x) = (x âˆ’Î»)2(x âˆ’Âµ)4?

660
Chapter 7
Eigenvalues and Eigenvectors
7.11.5. Use the technique described in Example 7.11.1 (p. 643) to determine the
minimum polynomial for A =
ï£«
ï£­
âˆ’7
âˆ’4
8
âˆ’8
âˆ’4
âˆ’1
4
âˆ’4
âˆ’16
âˆ’8
17
âˆ’16
âˆ’6
âˆ’3
6
âˆ’5
ï£¶
ï£¸.
7.11.6. Explain why similar matrices have the same minimum and characteristic
polynomials.
7.11.7. Show that two matrices can have the same minimum and characteristic
polynomials without being similar by considering A =
 N
0
0
N

and
B =
 N
0
0
0

, where N =
 0
1
0
0

.
7.11.8. Prove that if A and B are nonderogatory matrices that have the same
characteristic polynomial, then A is similar to B.
7.11.9. Use the Lanczos algorithm to ï¬nd an orthogonal matrix P such that
PT AP = T is tridiagonal, where A =
 2
1
1
1
2
1
1
1
2

.
7.11.10. Starting with x0 = 0, apply the conjugate gradient algorithm to solve
Ax = b, where A =
 2
1
1
1
2
1
1
1
2

and b =
 4
0
0

.
7.11.11. Use Arnoldiâ€™s algorithm to ï¬nd an orthogonal matrix Q such that
QT AQ = H is upper Hessenberg, where A =

5
1
2
âˆ’4
0
âˆ’2
âˆ’4
âˆ’1
âˆ’1

.
7.11.12. Use GMRES to solve Ax = b for A =

5
1
2
âˆ’4
0
âˆ’2
âˆ’4
âˆ’1
âˆ’1

and b =
 1
2
1

.

CHAPTER 8
Perronâ€“Frobenius
Theory of
Nonnegative Matrices
8.1
INTRODUCTION
A âˆˆâ„œmÃ—n is said to be a nonnegative matrix whenever each aij â‰¥0, and
this is denoted by writing A â‰¥0. In general, A â‰¥B means that each aij â‰¥bij.
Similarly, A is a positive matrix when each aij > 0, and this is denoted by
writing A > 0. More generally, A > B means that each aij > bij.
Applications abound with nonnegative and positive matrices. In fact, many
of the applications considered in this text involve nonnegative matrices. For
example, the connectivity matrix C in Example 3.5.2 (p. 100) is nonnegative.
The discrete Laplacian L from Example 7.6.2 (p. 563) leads to a nonnegative
matrix because (4I âˆ’L) â‰¥0. The matrix eAt that deï¬nes the solution of
the system of diï¬€erential equations in the mixing problem of Example 7.9.7
(p. 610) is nonnegative for all t â‰¥0. And the system of diï¬€erence equations
p(k) = Ap(k âˆ’1) resulting from the shell game of Example 7.10.8 (p. 635) has
a nonnegative coeï¬ƒcient matrix A.
Since nonnegative matrices are pervasive, itâ€™s natural to investigate their
properties, and thatâ€™s the purpose of this chapter. A primary issue concerns
the extent to which the properties A > 0 or A â‰¥0 translate to spectral
propertiesâ€”e.g., to what extent does A have positive (or nonnegative) eigen-
values and eigenvectors?
The topic is called the â€œPerronâ€“Frobenius theoryâ€ because it evolved from
the contributions of the German mathematicians Oskar (or Oscar) Perron
89 and
89
Oskar Perron (1880â€“1975) originally set out to fulï¬ll his fatherâ€™s wishes to be in the family busi-

662
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
Ferdinand Georg Frobenius.
90 Perron published his treatment of positive matri-
ces in 1907, and in 1912 Frobenius contributed substantial extensions of Perronâ€™s
results to cover the case of nonnegative matrices.
In addition to saying something useful, the Perronâ€“Frobenius theory is ele-
gant. It is a testament to the fact that beautiful mathematics eventually tends
to be useful, and useful mathematics eventually tends to be beautiful.
ness, so he only studied mathematics in his spare time. But he was eventually captured by the
subject, and, after studying at Berlin, TÂ¨ubingen, and GÂ¨ottingen, he completed his doctorate,
writing on geometry, at the University of Munich under the direction of Carl von Lindemann
(1852â€“1939) (who ï¬rst proved that Ï€ was transcendental). Upon graduation in 1906, Perron
held positions at Munich, TÂ¨ubingen, and Heidelberg. Perronâ€™s career was interrupted in 1915
by World War I in which he earned the Iron Cross. After the war he resumed work at Hei-
delberg, but in 1922 he returned to Munich to accept a chair in mathematics, a position he
occupied for the rest of his career. In addition to his contributions to matrix theory, Perronâ€™s
work covered a wide range of other topics in algebra, analysis, diï¬€erential equations, continued
fractions, geometry, and number theory. He was a man of extraordinary mental and physical
energy. In addition to being able to climb mountains until he was in his midseventies, Perron
continued to teach at Munich until he was 80 (although he formally retired at age 71), and he
maintained a remarkably energetic research program into his nineties. He published 18 of his
218 papers after he was 84.
90
Ferdinand Georg Frobenius (1849â€“1917) earned his doctorate under the supervision of Karl
Weierstrass (p. 589) at the University of Berlin in 1870. As mentioned earlier, Frobenius was
a mentor to and a collaborator with Issai Schur (p. 123), and, in addition to their joint work
in group theory, they were among the ï¬rst to study matrix theory as a discipline unto itself.
Frobenius in particular must be considered along with Cayley and Sylvester when thinking
of core developers of matrix theory. However, in the beginning, Frobeniusâ€™s motivation came
from Kronecker (p. 597) and Weierstrass, and he seemed oblivious to Cayleyâ€™s work (p. 80).
It was not until 1896 that Frobenius became aware of Cayleyâ€™s 1857 work, A Memoir on
the Theory of Matrices, and only then did the terminology â€œmatrixâ€ appear in Frobeniusâ€™s
work. Even though Frobenius was the ï¬rst to give a rigorous proof of the Cayleyâ€“Hamilton
theorem (p. 509), he generously attributed it to Cayley in spite of the fact that Cayley had
only discussed the result for 2 Ã— 2 and 3 Ã— 3 matrices. But credit in this regard is not overly
missed because Frobeniusâ€™s extension of Perronâ€™s results are more substantial, and they alone
may keep Frobeniusâ€™s name alive forever.

8.2 Positive Matrices
663
8.2
POSITIVE MATRICES
The purpose of this section is to focus on matrices AnÃ—n > 0 with positive en-
tries, and the aim is to investigate the extent to which this positivity is inherited
by the eigenvalues and eigenvectors of A.
There are a few elementary observations that will help along the way, so
letâ€™s begin with them. First, notice that
A > 0
=â‡’
Ï (A) > 0
(8.2.1)
because if Ïƒ (A) = {0}, then the Jordan form for A, and hence A itself, is
nilpotent, which is impossible when each aij > 0. This means that our discus-
sions can be limited to positive matrices having spectral radius 1 because A
can always be normalized by its spectral radiusâ€”i.e., A > 0 â‡â‡’A/Ï (A) > 0,
and Ï (A) = r â‡â‡’Ï(A/r) = 1. Other easily veriï¬ed observations are
P > 0, x â‰¥0, x Ì¸= 0
=â‡’Px > 0,
(8.2.2)
N â‰¥0, u â‰¥v â‰¥0
=â‡’Nu â‰¥Nv,
(8.2.3)
N â‰¥0, z > 0, Nz = 0
=â‡’N = 0,
(8.2.4)
N â‰¥0, N Ì¸= 0, u > v > 0 =â‡’Nu > Nv.
(8.2.5)
In all that follows, the bar notation | â‹†| is used to denote a matrix of
absolute valuesâ€”i.e., |M| is the matrix having entries |mij|. The bar notation
will never denote a determinant in the sequel. Finally, notice that as a simple
consequence of the triangle inequality, itâ€™s always true that |Ax| â‰¤|A| |x|.
Positive Eigenpair
If AnÃ—n > 0, then the following statements are true.
â€¢
Ï (A) âˆˆÏƒ (A) .
(8.2.6)
â€¢
If Ax = Ï (A) x, then A|x| = Ï (A) |x| and |x| > 0.
(8.2.7)
In other words, A has an eigenpair of the form (Ï (A) , v) with v > 0.
Proof.
As mentioned earlier, it can be assumed that Ï (A) = 1 without any
loss of generality. If (Î», x) is any eigenpair for A such that |Î»| = 1, then
|x| = |Î»| |x| = |Î»x| = |Ax| â‰¤|A| |x| = A |x|
=â‡’
|x| â‰¤A |x|.
(8.2.8)
The goal is to show that equality holds. For convenience, let z = A |x| and
y = z âˆ’|x|, and notice that (8.2.8) implies y â‰¥0. Suppose that y Ì¸= 0â€”i.e.,

664
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
suppose that some yi > 0. In this case, it follows from (8.2.2) that Ay > 0 and
z > 0, so there must exist a number Ïµ > 0 such that Ay > Ïµ z or, equivalently,
A
1 + Ïµz > z.
Writing this inequality as Bz > z, where B = A/(1 + Ïµ), and successively
multiplying both sides by B while using (8.2.5) produces
B2z > Bz > z,
B3z > B2z > z,
. . .
=â‡’
Bkz > z
for all k = 1, 2, . . . .
But limkâ†’âˆBk = 0 because Ï (B) = Ïƒ

A/(1 + Ïµ)

= 1/(1 + Ïµ) < 1 (recall
(7.10.5) on p. 617), so, in the limit, we have 0 > z, which contradicts the fact
that z > 0. Since the supposition that y Ì¸= 0 led to this contradiction, the
supposition must be false and, consequently, 0 = y = A |x| âˆ’|x|. Thus |x| is
an eigenvector for A associated with the eigenvalue 1 = Ï (A) . The proof is
completed by observing that |x| = A |x| = z > 0.
Now that itâ€™s been established that Ï (A) > 0 is in fact an eigenvalue for
A > 0, the next step is to investigate the index of this special eigenvalue.
Index of Ï (A)
If AnÃ—n > 0, then the following statements are true.
â€¢
Ï (A) is the only eigenvalue of A on the spectral circle.
â€¢
index (Ï (A)) = 1. In other words, Ï (A) is a semisimple eigenvalue.
Recall Exercise 7.8.4 (p. 596).
Proof.
Again, assume without loss of generality that Ï (A) = 1. We know from
(8.2.7) on p. 663 that if (Î», x) is an eigenpair for A such that |Î»| = 1, then
0 < |x| = A |x|, so 0 < |xk| =

A |x|

k = n
j=1 akj|xj|. But itâ€™s also true that
|xk| = |Î»| |xk| = |(Î»x)k| = |(Ax)k| =
 n
j=1 akjxj
,
and thus


j
akjxj
 =

j
akj|xj| =

j
|akjxj|.
(8.2.9)
For nonzero vectors {z1, . . . , zn} âŠ‚Cn, itâ€™s a fact that âˆ¥
j zjâˆ¥2 = 
j âˆ¥zjâˆ¥2
(equality in the triangle inequality) if and only if each zj = Î±jz1 for some
Î±j > 0 (Exercise 5.1.10, p. 277). In particular, this holds for scalars, so (8.2.9)
insures the existence of numbers Î±j > 0 such that
akjxj = Î±j(ak1x1)
or, equivalently,
xj = Ï€jx1
with Ï€j = Î±jak1
akj
> 0.

8.2 Positive Matrices
665
In other words, if |Î»| = 1, then x = x1p, where p = (1, Ï€2, . . . , Ï€n)T > 0, so
Î»x = Ax
=â‡’
Î»p = Ap = |Ap| = |Î»p| = |Î»|p = p
=â‡’
Î» = 1,
and thus 1 is the only eigenvalue of A on the spectral circle. Now suppose that
index (1) = m > 1. It follows that
Ak
âˆâ†’âˆas k â†’âˆbecause there is
an m Ã— m Jordan block Jâ‹†in the Jordan form J = Pâˆ’1AP that looks like
(7.10.30) on p. 629, so
Jk
â‹†

âˆâ†’âˆ, which in turn means that
Jk
âˆâ†’âˆ
and, consequently,
Jk
âˆ=
Pâˆ’1AkP

âˆâ‰¤
Pâˆ’1
âˆ
Ak
âˆâˆ¥Pâˆ¥âˆimplies
Ak
âˆâ‰¥
Jk
âˆ
âˆ¥Pâˆ’1âˆ¥âˆâˆ¥Pâˆ¥âˆ
â†’âˆ.
Let Ak =

a(k)
ij
	
, and let ik denote the row index for which
Ak
âˆ= 
j a(k)
ikj.
We know that there exists a vector p > 0 such that p = Ap, so for such an
eigenvector,
âˆ¥pâˆ¥âˆâ‰¥pik =

j
a(k)
ikjpj â‰¥

 
j
a(k)
ikj

(min
i
pi) =
Ak
âˆ(min
i
pi) â†’âˆ.
But this is impossible because p is a constant vector, so the supposition that
index (1) > 1 must be false, and thus index (1) = 1.
Establishing that Ï (A) is a semisimple eigenvalue of A > 0 was just a
steppingstone (but an important one) to get to the following theorem concerning
the multiplicities of Ï (A) .
Multiplicities of Ï (A)
If AnÃ—n > 0, then alg multA (Ï (A)) = 1. In other words, the spectral
radius of A is a simple eigenvalue of A.
So dim N (A âˆ’Ï (A) I) = geo multA (Ï (A)) = alg multA (Ï (A)) = 1.
Proof.
As before, assume without loss of generality that Ï (A) = 1, and sup-
pose that alg multA (Î» = 1) = m > 1. We already know that Î» = 1 is a
semisimple eigenvalue, which means that alg multA (1) = geo multA (1) (p. 510),
so there are m linearly independent eigenvectors associated with Î» = 1. If x
and y are a pair of independent eigenvectors associated with Î» = 1, then
x Ì¸= Î±y for all Î± âˆˆC. Select a nonzero component from y, say yi Ì¸= 0,
and set z = x âˆ’(xi/yi)y. Since Az = z, we know from (8.2.7) on p. 663
that A|z| = |z| > 0. But this contradicts the fact that zi = xi âˆ’(xi/yi)yi = 0.
Therefore, the supposition that m > 1 must be false, and thus m = 1.

666
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
Since N (A âˆ’Ï (A) I) is a one-dimensional space that can be spanned by
some v > 0, there is a unique eigenvector p âˆˆN (A âˆ’Ï (A) I) such that p > 0
and 
j pj = 1 (itâ€™s obtained by the normalization p = v/ âˆ¥vâˆ¥1â€”see Exercise
8.2.3). This special eigenvector p is called the Perron vector for A > 0, and
the associated eigenvalue r = Ï (A) is called the Perron root of A.
Since A > 0 â‡â‡’AT > 0, and since Ï(A) = Ï(AT ), itâ€™s clear that
if A > 0, then in addition to the Perron eigenpair (r, p) for A there is a
corresponding Perron eigenpair (r, q) for AT . Because qT A = rqT , the vector
qT > 0 is called the left-hand Perron vector for A.
While eigenvalues of A > 0 other than Ï (A) may or may not be positive,
it turns out that no eigenvectors other than positive multiples of the Perron
vector can be positiveâ€”or even nonnegative.
No Other Positive Eigenvectors
There are no nonnegative eigenvectors for AnÃ—n > 0 other than the
Perron vector p and its positive multiples.
(8.2.10)
Proof.
If (Î», y) is an eigenpair for A such that y â‰¥0, and if x > 0 is the
Perron vector for AT , then xT y > 0 by (8.2.2), so
Ï (A) xT = xT A
=â‡’
Ï (A) xT y = xT Ay = Î»xT y
=â‡’
Ï (A) = Î».
In 1942 the German mathematician Lothar Collatz (1910â€“1990) discovered
the following formula for the Perron root, and in 1950 Helmut Wielandt (p. 534)
used it to develop the Perronâ€“Frobenius theory.
Collatzâ€“Wielandt Formula
The Perron root of AnÃ—n > 0 is given by r = maxxâˆˆN f(x), where
f(x) =
min
1â‰¤iâ‰¤n
xiÌ¸=0
[Ax]i
xi
and
N = {x | x â‰¥0 with x Ì¸= 0}.
Proof.
If Î¾ = f(x) for x âˆˆN, then 0 â‰¤Î¾x â‰¤Ax. Let p and qT be the
respective the right-hand and left-hand Perron vectors for A associated with
the Perron root r, and use (8.2.3) along with qT x > 0 (by (8.2.2)) to write
Î¾x â‰¤Ax
=â‡’
Î¾qT x â‰¤qT Ax = rqT x
=â‡’
Î¾ â‰¤r
=â‡’
f(x) â‰¤r âˆ€x âˆˆN.
Since f(p) = r and p âˆˆN, it follows that r = maxxâˆˆN f(x).
Below is a summary of the results obtained in this section.

8.2 Positive Matrices
667
Perronâ€™s Theorem
If AnÃ—n > 0 with r = Ï (A) , then the following statements are true.
â€¢
r > 0.
(8.2.11)
â€¢
r âˆˆÏƒ (A)
(r is called the Perron root).
(8.2.12)
â€¢
alg multA (r) = 1.
(8.2.13)
â€¢
There exists an eigenvector x > 0 such that Ax = rx.
(8.2.14)
â€¢
The Perron vector is the unique vector deï¬ned by
Ap = rp,
p > 0,
and
âˆ¥pâˆ¥1 = 1,
and, except for positive multiples of p, there are no other nonneg-
ative eigenvectors for A, regardless of the eigenvalue.
â€¢
r is the only eigenvalue on the spectral circle of A.
(8.2.15)
â€¢
r = maxxâˆˆN f(x) (the Collatzâ€“Wielandt formula),
where f(x) =
min
1â‰¤iâ‰¤n
xiÌ¸=0
[Ax]i
xi
and N = {x | x â‰¥0 with x Ì¸= 0}.
Note: Our development is the reverse of that of Wielandt and others in the
sense that we ï¬rst proved the existence of the Perron eigenpair (r, p) without
reference to f(x) , and then we used the Perron eigenpair to established the
Collatz-Wielandt formula. Wielandtâ€™s approach is to do things the other way
aroundâ€”ï¬rst prove that f(x) attains a maximum value on N, and then es-
tablish existence of the Perron eigenpair by proving that maxxâˆˆN f(x) = Ï(A)
with the maximum value being attained at a positive eigenvector p.
Exercises for section 8.2
8.2.1. Verify Perronâ€™s theorem by by computing the eigenvalues and eigenvec-
tors for
A =
ï£«
ï£­
7
2
3
1
8
3
1
2
9
ï£¶
ï£¸.
Find the right-hand Perron vector p as well as the left-hand Perron
vector qT .

668
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
8.2.2. Convince yourself that (8.2.2)â€“(8.2.5) are indeed true.
8.2.3. Provide the details that explain why the Perron vector is uniquely de-
ï¬ned.
8.2.4. Find the Perron root and the Perron vector for
A =

1 âˆ’Î±
Î²
Î±
1 âˆ’Î²

,
where Î± + Î² = 1 with Î±, Î² > 0.
8.2.5. Suppose that AnÃ—n > 0 has Ï(A) = r.
(a)
Explain why limkâ†’âˆ(A/r)k exists.
(b)
Explain why limkâ†’âˆ(A/r)k = G > 0 is the projector onto
N(A âˆ’rI) along R(A âˆ’rI).
(c)
Explain why rank (G) = 1.
8.2.6. Prove that if every row (or column) sum of AnÃ—n > 0 is equal to Ï,
then Ï (A) = Ï.
8.2.7. Prove that if AnÃ—n > 0, then
min
i
n

j=1
aij â‰¤Ï (A) â‰¤max
i
n

j=1
aij.
Hint: Recall Example 7.10.2 (p. 619).
8.2.8. To show the extent to which the hypothesis of positivity cannot be re-
laxed in Perronâ€™s theorem, construct examples of square matrices A
such that A â‰¥0, but A Ì¸> 0 (i.e., A has at least one zero entry),
with r = Ï (A) âˆˆÏƒ (A) that demonstrate the validity of the following
statements. Diï¬€erent examples may be used for the diï¬€erent statements.
(a)
r can be 0.
(b)
alg multA (r) can be greater than 1.
(c)
index (r) can be greater than 1.
(d)
N(A âˆ’rI) need not contain a positive eigenvector.
(e)
r need not be the only eigenvalue on the spectral circle.

8.2 Positive Matrices
669
8.2.9. Establish the min-max version of the Collatzâ€“Wielandt formula that
says the Perron root for A > 0 is given by r = minxâˆˆP g(x), where
g(x) = max
1â‰¤iâ‰¤n
[Ax]i
xi
and
P = {x | x > 0}.
8.2.10. Notice that N = {x | x â‰¥0 with x Ì¸= 0} is used in the max-min version
of the Collatzâ€“Wielandt formula on p. 666, but P = {x | x > 0} is used
in the min-max version in Exercise 8.2.9. Give an example of a matrix
A > 0 that shows r Ì¸= minxâˆˆN g(x) when g(x) is deï¬ned as
g(x) = max
1â‰¤iâ‰¤n
xiÌ¸=0
[Ax]i
xi
.

670
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
8.3
NONNEGATIVE MATRICES
Now let zeros creep into the picture and investigate the extent to which Perronâ€™s
results generalize to nonnegative matrices containing at least one zero entry. The
ï¬rst result along these lines shows how to extend the statements on p. 663 to
nonnegative matrices by sacriï¬cing the existence of a positive eigenvector for a
nonnegative one.
Nonnegative Eigenpair
For AnÃ—n â‰¥0 with r = Ï (A) , the following statements are true.
â€¢
r âˆˆÏƒ (A) , (but r = 0 is possible).
(8.3.1)
â€¢
Az = rz for some z âˆˆN = {x | x â‰¥0 with x Ì¸= 0}.
(8.3.2)
â€¢
r = maxxâˆˆN f(x), where f(x) =
min
1â‰¤iâ‰¤n
xiÌ¸=0
[Ax]i
xi
(8.3.3)
(i.e., the Collatzâ€“Wielandt formula remains valid).
Proof.
Consider the sequence of positive matrices Ak = A + (1/k)E > 0,
where E is the matrix of all 1 â€™s, and let rk > 0 and pk > 0 denote the
Perron root and Perron vector for Ak, respectively. Observe that {pk}âˆ
k=1 is
a bounded set because itâ€™s contained in the unit 1-sphere in â„œn. The Bolzanoâ€“
Weierstrass theorem states that each bounded sequence in â„œn has a convergent
subsequence. Therefore, {pk}âˆ
k=1 has convergent subsequence
{pki}âˆ
i=1 â†’z, where z â‰¥0 with z Ì¸= 0 (because pki > 0 and âˆ¥pkiâˆ¥1 = 1).
Since A1 > A2 > Â· Â· Â· > A, the result in Example 7.10.2 (p. 619) guarantees
that r1 â‰¥r2 â‰¥Â· Â· Â· â‰¥r, so {rk}âˆ
k=1 is a monotonic sequence of positive numbers
that is bounded below by r. A standard result from analysis guarantees that
lim
kâ†’âˆrk = râ‹†exists, and râ‹†â‰¥r.
In particular,
lim
iâ†’âˆrki = râ‹†â‰¥r.
But limkâ†’âˆAk = A implies limiâ†’âˆAki â†’A, so, by using the easily estab-
lished fact that the limit of a product is the product of the limits (provided that
all limits exist), itâ€™s also true that
Az = lim
iâ†’âˆAkipki = lim
iâ†’âˆrkipki = râ‹†z
=â‡’
râ‹†âˆˆÏƒ (A)
=â‡’
râ‹†â‰¤r.
Consequently, râ‹†= r, and Az = rz with z â‰¥0 and z Ì¸= 0. Thus (8.3.1) and
(8.3.2) are proven. To prove (8.3.3), let qT
k > 0 be the left-hand Perron vector
of Ak. For every x âˆˆN and k > 0 we have qT
k x > 0 (by (8.2.2)), and
0 â‰¤f(x)x â‰¤Ax â‰¤Akx
=â‡’
f(x)qT
k x â‰¤qT
k Akx = rkqT
k x
=â‡’
f(x) â‰¤rk
=â‡’
f(x) â‰¤r
(because rk â†’râˆ—=r).
Since f(z) = r and z âˆˆN, it follows that maxxâˆˆN f(x) = r.

8.3 Nonnegative Matrices
671
This is as far as Perronâ€™s theorem can be generalized to nonnegative matrices
without additional hypothesis. For example, A =
 0
1
0
0

shows that properties
(8.2.11), (8.2.13), and (8.2.14) on p. 667 do not hold for general nonnegative ma-
trices, and A =
 0
1
1
0

shows that (8.2.15) is also lost. Rather than accepting
that the major issues concerning spectral properties of nonnegative matrices had
been settled, Frobenius had the insight to look below the surface and see that
the problem doesnâ€™t stem just from the existence of zero entries, but rather from
the positions of the zero entries. For example, (8.2.13) and (8.2.14) are false for
A =

1
0
1
1

, but they are true for A =

1
1
1
0

.
(8.3.4)
Frobeniusâ€™s genius was to see the diï¬€erence between A and A in terms of re-
ducibility and to relate these ideas to spectral properties of nonnegative matrices.
Reducibility and graphs were discussed in Example 4.4.6 (p. 202) and Exercise
4.4.20 (p. 209), but for the sake of continuity they are reviewed below.
Reducibility and Graphs
â€¢
AnÃ—n is said to be a reducible matrix when there exists a permu-
tation matrix P such that
PT AP =

X
Y
0
Z

,
where X and Z are both square.
Otherwise A is said to be an irreducible matrix.
â€¢
PT AP is called a symmetric permutation of A. The eï¬€ect is to
interchange rows in the same way as columns are interchanged.
â€¢
The graph G(A) of A is deï¬ned to be the directed graph on n
nodes {N1, N2, . . . , Nn} in which there is a directed edge leading
from Ni to Nj if and only if aij Ì¸= 0.
â€¢
G(PT AP) = G(A) whenever P is a permutation matrixâ€”the eï¬€ect
is simply a relabeling of nodes.
â€¢
G(A) is called strongly connected if for each pair of nodes (Ni, Nk)
there is a sequence of directed edges leading from Ni to Nk.
â€¢
A is an irreducible matrix if and only if G(A) is strongly connected
(see Exercise 4.4.20 on p. 209).

672
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
For example, the matrix A in (8.3.4) is reducible because
PT AP =

1
1
0
1

for
P =

0
1
1
0

,
and, as can be seen from Figure 8.3.1, G(A) is not strongly connected because
there is no sequence of paths leading from node 1 to node 2. On the other
hand, A is irreducible, and as shown in Figure 8.3.1, G( A) is strongly connected
because each node is accessible from the other.
1
2
1
2
G(A)
G( A)
Figure 8.3.1
This discussion suggests that some of Perronâ€™s properties given on p. 667
extend to nonnegative matrices when the zeros are in just the right positions to
insure irreducibility. To prove that this is in fact the case, the following lemma is
needed. It shows how to convert a nonnegative irreducible matrix into a positive
matrix in a useful fashion.
Converting Nonnegativity & Irreducibility to Positivity
If AnÃ—n â‰¥0 is irreducible, then (I + A)nâˆ’1 > 0.
(8.3.5)
Proof.
Let a(k)
ij
denote the (i, j)-entry in Ak, and observe that
a(k)
ij =

h1,...,hkâˆ’1
aih1ah1h2 Â· Â· Â· ahkâˆ’1j > 0
if and only if there exists a set of indicies h1, h2, . . . , hkâˆ’1 such that
aih1 > 0
and
ah1h2 > 0
and
Â· Â· Â·
and
ahkâˆ’1j > 0.
In other words, there is a sequence of k paths Ni â†’Nh1 â†’Nh2 â†’Â· Â· Â· â†’Nj
in G(A) that lead from node Ni to node Nj if and only if a(k)
ij
> 0. The
irreducibility of A insures that G(A) is strongly connected, so for any pair of
nodes (Ni, Nj) there is a sequence of k paths (with k < n) from Ni to Nj.
This means that for each position (i, j), there is some 0 â‰¤k â‰¤n âˆ’1 such that
a(k)
ij > 0, and this guarantees that for each i and j,

(I + A)nâˆ’1
ij =
nâˆ’1

k=0
n âˆ’1
k

Ak

ij
=
nâˆ’1

k=0
n âˆ’1
k

a(k)
ij > 0.

8.3 Nonnegative Matrices
673
With the exception of the Collatzâ€“Wielandt formula, we have seen that
Ï (A) âˆˆÏƒ (A) is the only property in the list of Perron properties on p. 667
that extends to nonnegative matrices without additional hypothesis. The next
theorem shows how adding irreducibility to nonnegativity recovers the Perron
properties (8.2.11), (8.2.13), and (8.2.14).
Perronâ€“Frobenius Theorem
If AnÃ—n â‰¥0 is irreducible, then each of the following is true.
â€¢
r = Ï (A) âˆˆÏƒ (A) and r > 0.
(8.3.6)
â€¢
alg multA (r) = 1.
(8.3.7)
â€¢
There exists an eigenvector x > 0 such that Ax = rx.
(8.3.8)
â€¢
The unique vector deï¬ned by
Ap = rp,
p > 0,
and
âˆ¥pâˆ¥1 = 1,
(8.3.9)
is called the Perron vector. There are no nonnegative eigenvectors
for A except for positive multiples of p, regardless of the eigenvalue.
â€¢
The Collatzâ€“Wielandt formula r = maxxâˆˆN f(x),
where f(x) =
min
1â‰¤iâ‰¤n
xiÌ¸=0
[Ax]i
xi
and N = {x | x â‰¥0 with x Ì¸= 0}
was established in (8.3.3) for all nonnegative matrices, but it is in-
cluded here for the sake of completeness.
Proof.
We already know from (8.3.2) that r = Ï (A) âˆˆÏƒ (A) . To prove that
alg multA (r) = 1, let B = (I + A)nâˆ’1 > 0 be the matrix in (8.3.5). It fol-
lows from (7.9.3) that Î» âˆˆÏƒ (A) if and only if (1 + Î»)nâˆ’1 âˆˆÏƒ (B) , and
alg multA (Î») = alg multB

(1 + Î»)nâˆ’1
. Consequently, if Âµ = Ï (B) , then
Âµ = max
Î»âˆˆÏƒ(A) |(1 + Î»)|nâˆ’1 =

max
Î»âˆˆÏƒ(A) |(1 + Î»)|
nâˆ’1
= (1 + r)nâˆ’1
because when a circular disk |z| â‰¤Ï is translated one unit to the right, the point
of maximum modulus in the resulting disk |z + 1| â‰¤Ï is z = 1 + Ï (itâ€™s clear if
you draw a picture). Therefore, alg multA (r) = 1; otherwise alg multB (Âµ) > 1,
which is impossible because B > 0. To see that A has a positive eigenvector

674
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
associated with r, recall from (8.3.2) that there exists a nonnegative eigenvector
x â‰¥0 associated with r. Itâ€™s a simple consequence of (7.9.9) that if (Î», x) is an
eigenpair for A, then (f(Î»), x) is an eigenpair for f(A) (Exercise 7.9.9, p. 613),
so (r, x) being an eigenpair for A implies that (Âµ, x) is an eigenpair for B.
Hence (8.2.10) insures that x must be a positive multiple of the Perron vector
of B, and thus x must in fact be positive. Now, r > 0; otherwise Ax = 0,
which is impossible because A â‰¥0 and x > 0 forces Ax > 0. The argument
used to prove (8.2.10) also proves (8.3.9).
Example 8.3.1
Problem: Suppose that AnÃ—n â‰¥0 is irreducible with r = Ï (A) , and suppose
that rz â‰¤Az for z â‰¥0. Explain why rz = Az, and z > 0.
Solution: If rz < Az, then by using the Perron vector q > 0 for AT we have
(A âˆ’rI)z â‰¥0
=â‡’
qT (A âˆ’rI)z > 0,
which is impossible since qT (A âˆ’rI) = 0. Thus rz = Az, and since z must
be a multiple of the Perron vector for A by (8.3.9), we also have that z > 0.
The only property in the list on p. 667 that irreducibility is not able to
salvage is (8.2.15), which states that there is only one eigenvalue on the spectral
circle. Indeed, A =
 0
1
1
0

is nonnegative and irreducible, but the eigenvalues
Â±1 are both on the unit circle. The property of having (or not having) only
one eigenvalue on the spectral circle divides the set of nonnegative irreducible
matrices into two important classes.
Primitive Matrices
â€¢
A nonnegative irreducible matrix A having only one eigenvalue,
r = Ï (A) , on its spectral circle is said to be a primitive matrix.
â€¢
A nonnegative irreducible matrix having h > 1 eigenvalues on its
spectral circle is called imprimitive, and h is referred to as index
of imprimitivity.
â€¢
A nonnegative irreducible matrix A with r = Ï (A) is primitive if
and only if limkâ†’âˆ(A/r)k exists, in which case
lim
kâ†’âˆ
A
r
k
= G = pqT
qT p > 0,
(8.3.10)
where p and q are the respective Perron vectors for A and AT .
G is the (spectral) projector onto N(A âˆ’rI) along R(A âˆ’rI).

8.3 Nonnegative Matrices
675
Proof of (8.3.10).
The Perronâ€“Frobenius theorem insures that 1 = Ï(A/r) is a
simple eigenvalue for A/r, and itâ€™s clear that A is primitive if and only if A/r
is primitive. In other words, A is primitive if and only if 1 = Ï(A/r) is the only
eigenvalue on the unit circle, which is equivalent to saying that limkâ†’âˆ(A/r)k
exists by the results on p. 630. The structure of the limit as described in (8.3.10)
is the result of (7.2.12) on p. 518.
The next two results, discovered by Helmut Wielandt (p. 534) in 1950,
establish the remarkable fact that the eigenvalues on the spectral circle of an
imprimitive matrix are in fact the hth roots of the spectral radius.
Wielandtâ€™s Theorem
If |B| â‰¤AnÃ—n, where A is irreducible, then Ï (B) â‰¤Ï (A) . If equality
holds (i.e., if Âµ = Ï (A) eiÏ† âˆˆÏƒ (B) for some Ï†), then
B = eiÏ†DADâˆ’1
for some
D =
ï£«
ï£¬
ï£¬
ï£­
eiÎ¸1
eiÎ¸2
...
eiÎ¸n
ï£¶
ï£·
ï£·
ï£¸,
(8.3.11)
and conversely.
Proof.
We already know that Ï (B) â‰¤Ï (A) by Example 7.10.2 (p. 619). If
Ï (B) = r = Ï (A) , and if (Âµ, x) is an eigenpair for B such that |Âµ| = r, then
r|x| = |Âµ| |x| = |Âµx| = |Bx| â‰¤|B| |x| â‰¤A|x|
=â‡’
|B| |x| = r|x|
because the result in Example 8.3.1 insures that A|x| = r|x|, and |x| > 0.
Consequently, (A âˆ’|B|)|x| = 0. But A âˆ’|B| â‰¥0, and |x| > 0, so A = |B|
by (8.2.4). Since xk/|xk| is on the unit circle, xk/|xk| = eiÎ¸k for some Î¸k. Set
D =
ï£«
ï£¬
ï£¬
ï£­
eiÎ¸1
eiÎ¸2
...
eiÎ¸n
ï£¶
ï£·
ï£·
ï£¸, and notice that x = D|x|.
Since |Âµ| = r, there is a Ï† âˆˆâ„œsuch that Âµ = reiÏ†, and hence
BD|x|=Bx=Âµx=reiÏ†x=reiÏ†D|x| â‡’eâˆ’iÏ†Dâˆ’1BD|x|=r|x|=A|x|. (8.3.12)
For convenience, let C = eâˆ’iÏ†Dâˆ’1BD, and note that |C| = |B| = A to write
(8.3.12) as 0 = (|C| âˆ’C)|x|. Considering only the real part of this equation

676
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
yields 0 =

|C| âˆ’Re (C)

|x|. But |C| â‰¥Re (C) , and |x| > 0, so it follows
from (8.2.4) that Re (C) = |C|, and hence
Re (cij) = |cij| =

Re (cij) 2 + Im (cij) 2
=â‡’Im (cij) = 0 =â‡’Im (C) = 0.
Therefore, C = Re (C) = |C| = A, which implies B = eiÏ†DADâˆ’1. Conversely,
if B = eiÏ†DADâˆ’1, then similarity insures that Ï (B) = Ï

eiÏ†A

= Ï (A) .
hth Roots of Ï (A) on Spectral Circle
If AnÃ—n â‰¥0 is irreducible and has h eigenvalues {Î»1, Î»2, . . . , Î»h} on
its spectral circle, then each of the following statements is true.
â€¢
alg multA (Î»k) = 1 for k = 1, 2, . . . , h.
(8.3.13)
â€¢
{Î»1, Î»2, . . . , Î»h} are the hth roots of r = Ï (A) given by
{r, rÏ‰, rÏ‰2, . . . , rÏ‰hâˆ’1},
where
Ï‰ = e2Ï€i/h.
(8.3.14)
Proof.
Let S = {r, reiÎ¸1, . . . , reiÎ¸hâˆ’1} denote the eigenvalues on the spectral
circle of A. Applying (8.3.11) with B = A and Âµ = reiÎ¸k insures the existence
of a diagonal matrix Dk such that A = eiÎ¸kDkADâˆ’1
k , thus showing that eiÎ¸kA
is similar to A. Since r is a simple eigenvalue of A (by the Perronâ€“Frobenius
theorem), reiÎ¸k must be a simple eigenvalue of eiÎ¸kA. But similarity transfor-
mations preserve eigenvalues and algebraic multiplicities (because the Jordan
structure is preserved), so reiÎ¸k must be a simple eigenvalue of A, thus estab-
lishing (8.3.13). To prove (8.3.14), consider another eigenvalue reiÎ¸s âˆˆS. Again,
we can write A = eiÎ¸sDsADâˆ’1
s
for some Ds, so
A = eiÎ¸kDkADâˆ’1
k
= eiÎ¸kDk(eiÎ¸sDsADâˆ’1
s )Dâˆ’1
k
= ei(Î¸k+Î¸r)(DkDs)A(DkDs)âˆ’1
and, consequently, rei(Î¸k+Î¸r) is also an eigenvalue on the spectral circle of A.
In other words, S = {r, reiÎ¸1, . . . , reiÎ¸hâˆ’1} is closed under multiplication. This
means that G = {1, eiÎ¸1, . . . , eiÎ¸hâˆ’1} is closed under multiplication, and it follows
that G is a ï¬nite commutative group of order h. A standard result from algebra
states that the hth power of every element in a ï¬nite group of order h must be
the identity element in the group. Therefore, (eiÎ¸k)h = 1 for each 0 â‰¤k â‰¤hâˆ’1,
so G is the set of the hth roots of unity e2Ï€ki/h ( 0 â‰¤k â‰¤h âˆ’1), and thus S
must be the hth roots of r.
Combining the preceding results reveals just how special the spectrum of an
imprimitive matrix is.

8.3 Nonnegative Matrices
677
Rotational Invariance
If A is imprimitive with h eigenvalues on its spectral circle, then Ïƒ (A)
is invariant under rotation about the origin through an angle 2Ï€/h. No
rotation less than 2Ï€/h can preserve Ïƒ (A) .
(8.3.15)
Proof.
Since Î» âˆˆÏƒ (A) â‡â‡’Î»e2Ï€i/h âˆˆÏƒ(e2Ï€i/hA), it follows that Ïƒ(e2Ï€i/hA)
is Ïƒ (A) rotated through 2Ï€/h. But (8.3.11) and (8.3.14) insure that A and
e2Ï€i/hA are similar and, consequently, Ïƒ (A) = Ïƒ(e2Ï€i/hA). No rotation less
than 2Ï€/h can keep Ïƒ (A) invariant because (8.3.14) makes it clear that the
eigenvalues on the spectral circle wonâ€™t go back into themselves for rotations less
than 2Ï€/h.
Example 8.3.2
The Spectral Projector Is Positive. We already know from (8.3.10) that
if A is a primitive matrix, and if G is the spectral projector associated with
r = Ï (A) , then G > 0.
Problem: Explain why this is also true for an imprimitive matrix. In other
words, establish the fact that if G is the spectral projector associated with
r = Ï (A) for any nonnegative irreducible matrix A, then G > 0.
Solution: Being imprimitive means that A is nonnegative and irreducible with
more than one eigenvalue on the spectral circle. However, (8.3.13) says that
each eigenvalue on the spectral circle is simple, so the results concerning Ces`aro
summability on p. 633 can be applied to A/r to conclude that
lim
kâ†’âˆ
I + (A/r) + Â· Â· Â· + (A/r)kâˆ’1
k
= G,
where G is the spectral projector onto N((A/r) âˆ’I) = N(A âˆ’rI) along
R((A/r) âˆ’I) = R(A âˆ’rI). Since r is a simple eigenvalue the same argument
used to establish (8.3.10) (namely, invoking (7.2.12) on p. 518) shows that
G = pqT
qT p > 0,
where p and q are the respective Perron vectors for A and AT .
Trying to determine if an irreducible matrix A â‰¥0 is primitive or imprim-
itive by ï¬nding the eigenvalues is generally a diï¬ƒcult task, so itâ€™s natural to ask
if thereâ€™s another way. It turns out that there is, and, as the following example
shows, determining primitivity can sometimes be trivial.

678
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
Example 8.3.3
Suï¬ƒcient Condition for Primitivity. If a nonnegative irreducible matrix A
has at least one positive diagonal element, then A is primitive.
Proof.
Suppose there are h > 1 eigenvalues on the spectral circle. We know
from (8.3.15) that if Î»0 âˆˆÏƒ (A) , then Î»k = Î»0e2Ï€ik/h âˆˆÏƒ (A) for k =
0, 1, . . . , h âˆ’1, so
hâˆ’1

k=0
Î»k = Î»0
hâˆ’1

k=0
e2Ï€ik/h = 0
(roots of unity sum to 1â€”see p. 357).
This implies that the sum of all of the eigenvalues is zero. In other words,
â€¢
if A is imprimitive, then trace (A) = 0.
(Recall (7.1.7) on p. 494.)
Therefore, if A has a positive diagonal entry, then A must be primitive.
Another of Frobeniusâ€™s contributions was to show how the powers of a non-
negative matrix determine whether or not the matrix is primitive. The exact
statement is as follows.
Frobeniusâ€™s Test for Primitivity
A â‰¥0 is primitive if and only if Am > 0 for some m > 0.
(8.3.16)
Proof.
First assume that Am > 0 for some m. This implies that A is irre-
ducible; otherwise there exists a permutation matrix such that
A = P

X
Y
0
Z

PT
=â‡’
Am = P

Xm
â‹†
0
Zm

PT
has zero entries.
Suppose that A has h eigenvalues {Î»1, Î»2, . . . , Î»h} on its spectral circle so
that r = Ï (A) = |Î»1| = Â· Â· Â· = |Î»h| > |Î»h+1| > Â· Â· Â· > |Î»n|. Since Î» âˆˆÏƒ (A)
implies Î»m âˆˆÏƒ(Am) with alg multA (Î») = alg multAm (Î»m) (consider the Jor-
dan formâ€”Exercise 7.9.9 on p. 613), it follows that Î»m
k (1 â‰¤k â‰¤h) is on the
spectral circle of Am with alg multA (Î»k) = alg multAm (Î»m
k ) . Perronâ€™s theo-
rem (p. 667) insures that Am has only one eigenvalue (which must be rm) on
its spectral circle, so rm = Î»m
1 = Î»m
2 = Â· Â· Â· = Î»m
h . But this means that
alg multA (r) = alg multAm (rm) = h,
and therefore h = 1 by (8.3.7). Conversely, if A is primitive with r = Ï (A) ,
then limkâ†’âˆ(A/r)k > 0 by (8.3.10). Hence there must be some m such that
(A/r)m > 0, and thus Am > 0.

8.3 Nonnegative Matrices
679
Example 8.3.4
Suppose that we wish to decide whether or not a nonnegative matrix A is
primitive by computing the sequence of powers A, A2, A3, . . . . Since this can be
a laborious task, it would be nice to know when we have computed enough powers
of A to render a judgement. Unfortunately there is nothing in the statement or
proof of Frobeniusâ€™s test to help us with this decision. But Wielandt provided
an answer by proving that a nonnegative matrix AnÃ—n is primitive if and only
if An2âˆ’2n+2 > 0. Furthermore, n2 âˆ’2n + 2 is the smallest such exponent
that works for the class of n Ã— n primitive matrices having all zeros on the
diagonalâ€”see Exercise 8.3.9.
Problem: Determine whether or not A =
 0
1
0
0
0
2
3
4
0

is primitive.
Solution: Since A has zeros on the diagonal, the result in Example 8.3.3 doesnâ€™t
apply, so we are forced into computing powers of A. This job is simpliï¬ed by
noticing that if B = Î²(A) is the Boolean matrix that results from setting
bij =
 1
if aij > 0,
0
if aij = 0,
then [Bk]ij > 0 if and only if [Ak]ij > 0 for every k > 0. This means that
instead of using A, A2, A3, . . . to decide on primitivity, we need only compute
B1 = Î²(A),
B2 = Î²(B1B1),
B3 = Î²(B1B2),
B4 = Î²(B1B3), . . . ,
going no further than Bn2âˆ’2n+2, and these computations require only Boolean
operations AND and OR. The matrix A in this example is primitive because
B1 =

 0
1
0
0
0
1
1
1
0

, B2 =

 0
0
1
1
1
0
0
1
1

, B3 =

 1
1
0
0
1
1
1
1
1

, B4 =

 0
1
1
1
1
1
1
1
1

, B5 =

 1
1
1
1
1
1
1
1
1

.
The powers of an irreducible matrix A â‰¥0 can tell us if A has more
than one eigenvalue on its spectral circle, but the powers of A provide no clue
to the number of such eigenvalues. The next theorem shows how the index of
imprimitivity can be determined without explicitly calculating the eigenvalues.
Index of Imprimitivity
If c(x) = xn + ck1xnâˆ’k1 + ck2xnâˆ’k2 + Â· Â· Â· + cksxnâˆ’ks = 0 is the char-
acteristic equation of an imprimitive matrix AnÃ—n in which only the
terms with nonzero coeï¬ƒcients are listed (i.e., each ckj Ì¸= 0, and
n > (n âˆ’k1) > Â· Â· Â· > (n âˆ’ks)), then the index of imprimitivity h
is the greatest common divisor of {k1, k2, . . . , ks}.

680
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
Proof.
We know from (8.3.15) that if {Î»1, Î»2, . . . , Î»n} are the eigenvalues of
A (including multiplicities), then {Ï‰Î»1, Ï‰Î»2, . . . , Ï‰Î»n} are also the eigenvalues
of A, where Ï‰ = e2Ï€i/h. It follows from the results on p. 494 that
ckj = (âˆ’1)kj 
1â‰¤i1<Â·Â·Â·<ikj â‰¤n
Î»i1 Â· Â· Â· Î»ikj = (âˆ’1)kj 
1â‰¤i1<Â·Â·Â·<ikj â‰¤n
Ï‰Î»i1 Â· Â· Â· Ï‰Î»ikj = Ï‰kjckj =â‡’Ï‰kj = 1.
Therefore, h must divide each kj. If d divides each kj with d > h, then
Î³âˆ’kj = 1 for Î³ = e2Ï€i/d. Hence Î³Î» âˆˆÏƒ (A) for each Î» âˆˆÏƒ (A) because
c(Î³Î») = 0. But this means that Ïƒ (A) is invariant under a rotation through
an angle (2Ï€/d) < (2Ï€/h), which, by (8.3.15), is impossible.
Example 8.3.5
Problem: Find the index of imprimitivity of A =
ï£«
ï£­
0
1
0
0
2
0
1
0
0
1
0
2
0
0
1
0
ï£¶
ï£¸.
Solution: Using the principal minors to compute the characteristic equation as
illustrated in Example 7.1.2 (p. 496) produces the characteristic equation
c(x) = x4 âˆ’5x2 + 4 = 0,
so that k1 = 2 and k2 = 4. Since gcd{2, 4} = 2, it follows that h = 2. The
characteristic equation is relatively simple in this example, so the eigenvalues
can be explicitly determined to be {Â±2, Â±1}. This corroborates the fact that
h = 2. Notice also that this illustrates the property that Ïƒ (A) is invariant
under rotation through an angle 2Ï€/h = Ï€.
More is known about nonnegative matrices than what has been presented
hereâ€”in fact, there are entire books on the subject. But before moving on to
applications, there is a result that Frobenius discovered in 1912 that is worth
mentioning because it completely reveals the structure of an imprimitive matrix.
Frobenius Form
For each imprimitive matrix A with index of imprimitivity h > 1,
there exists a permutation matrix P such that
PT AP=
ï£«
ï£¬
ï£¬
ï£­
0
A12
0
Â· Â· Â·
0
0
0
A23
Â· Â· Â·
0
...
...
...
...
...
0
0
Â· Â· Â·
0
Ahâˆ’1,h
Ah1
0
Â· Â· Â·
0
0
ï£¶
ï£·
ï£·
ï£¸,
where the zero blocks on the main diagonal are square.

8.3 Nonnegative Matrices
681
Example 8.3.6
Leontiefâ€™s
91 Inputâ€“Output Economic Model. Suppose that n major indus-
tries in a closed economic system each make one commodity, and let a J-unit be
what industry J produces that sells for $1. For example, the Boeing Company
makes airplanes, and the Champion Company makes rivets, so a BOEING-unit
is only a tiny fraction of an airplane, but a CHAMPION-unit might be several
rivets. If
0 â‰¤sj = # J-units produced by industry J each year, and if
0 â‰¤aij = # I-units needed to produce one J-unit,
then
aijsj = # I-units consumed by industry J each year, and
n

j=1
aijsj = # I-units consumed by all industries each year,
so
di = si âˆ’
n

j=1
aijsj = # I-units available to the public (nonindustry) each year.
Consider d = (d1, d2, . . . , dn)T to be the public demand vector, and think of
s = (s1, s2, . . . , sn)T as the industrial supply vector.
Problem: Determine the supply s â‰¥0 that is required to satisfy a given
demand d â‰¥0.
Solution: At ï¬rst glance the problem seems to be trivial because the equations
di = si âˆ’n
j=1 aijsj translate to (Iâˆ’A)s = d, so if Iâˆ’A is nonsingular, then
s = (Iâˆ’A)âˆ’1d. The catch is that this solution may have negative components in
spite of the fact that A â‰¥0. So something must be added. Itâ€™s not unreasonable
to assume that major industries are strongly connected in the sense that the
commodity of each industry is either directly or indirectly needed to produce
all commodities in the system. In other words, itâ€™s reasonable to assume that
91
Wassily Leontief (1906â€“1999) was the 1973 Nobel Laureate in Economics. He was born in St.
Petersburg (now Leningrad), where his father was a professor of economics. After receiving his
undergraduate degree in economics at the University of Leningrad in 1925, Leontief went to
the University of Berlin to earn a Ph.D. degree. He migrated to New York in 1931 and moved
to Harvard University in 1932, where he became Professor of Economics in 1946. Leontief spent
a signiï¬cant portion of his career developing and applying his inputâ€“output analysis, which
eventually led to the famous â€œLeontief paradox.â€ In the U.S. economy of the 1950s, labor was
considered to be scarce while capital was presumed to be abundant, so the prevailing thought
was that U.S. foreign trade was predicated on trading capital-intensive goods for labor-intensive
goods. But Leontiefâ€™s inputâ€“output tables revealed that just the opposite was true, and this
contributed to his fame. One of Leontiefâ€™s secret weapons was the computer. He made use
of large-scale computing techniques (relative to the technology of the 1940s and 1950s), and
he was among the ï¬rst to put the Mark I (one of the ï¬rst electronic computers) to work on
nonmilitary projects in 1943.

682
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
G(A) is a strongly connected graph so that in addition to being nonnegative,
A is an irreducible matrix. Furthermore, itâ€™s not unreasonable to assume that
Ï (A) < 1. To understand why, notice that the jth column sum of A is
cj =
n

i=1
aij = total number of all units required to make one J-unit
= total number of dollars spent by J to create $1 of revenue.
In a healthy economy all major industries should have cj â‰¤1, and there should
be at least one major industry such that cj < 1. This means that there exists a
matrix E â‰¥0, but E Ì¸= 0, such that each column sum of A + E is 1, so
eT (A + E) = eT ,
where
eT is the row of all 1 â€™s.
This forces Ï (A) < 1; otherwise the Perron vector p > 0 for A can be used
to write
1 = eT p = eT (A + E)p = 1 + eT Ep > 1
because
E â‰¥0, E Ì¸= 0, p > 0
=â‡’
Ep > 0.
(Conditions weaker than the column-sum condition can also force Ï (A) < 1â€”see
Example 7.10.3 on p. 620.) The assumption that A is a nonnegative irreducible
matrix whose spectral radius is Ï (A) < 1 combined with the Neumann series
(p. 618) provides the conclusion that
(I âˆ’A)âˆ’1 =
âˆ

k=0
Ak > 0.
Positivity is guaranteed by the irreducibility of A because the same argu-
ment given on p. 672 that is to prove (8.3.5) also applies here. Therefore, for
each demand vector d â‰¥0, there exists a unique supply vector given by
s = (I âˆ’A)âˆ’1d, which is necessarily positive. The fact that (I âˆ’A)âˆ’1 > 0
and s > 0 leads to the interesting conclusion that an increase in public demand
by just one unit from a single industry will force an increase in the output of all
industries.
Note: The matrix I âˆ’A is an M-matrix as deï¬ned and discussed in Example
7.10.7 (p. 626). The realization that M-matrices are naturally present in economic
models provided some of the motivation for studying M-matrices during the ï¬rst
half of the twentieth century. Some of the M-matrix properties listed on p. 626
were independently discovered and formulated in economic terms.

8.3 Nonnegative Matrices
683
Example 8.3.7
Leslie Population Age Distribution Model. Divide a population of females
into age groups G1, G2, . . . , Gn, where each group covers the same number of
years. For example,
G1 = all females under age 10,
G2 = all females from age 10 up to 20,
G1 = all females from age 20 up to 30,
...
Consider discrete points in time, say t = 0, 1, 2, . . . years, and let bk and sk
denote the birth rate and survival rate for females in Gk. That is, let
bk = Expected number of daughters produced by a female in Gk,
sk = Proportion of females in Gk at time t that are in Gk+1 at time t + 1.
If
fk(t) = Number of females in Gk at time t,
then it follows that
f1(t + 1) = f1(t)b1 + f2(t)b2 + Â· Â· Â· + fn(t)bn
and
(8.3.17)
fk(t + 1) = fkâˆ’1(t)skâˆ’1
for k = 2, 3, . . . , n.
Furthermore,
Fk(t) =
fk(t)
f1(t) + f2(t) + Â· Â· Â· + fn(t) = % of population in Gk at time t.
The vector F(t) = (F1(t), F2(t), . . . , Fn(t))T represents the population age dis-
tribution at time t, and, provided that it exists, Fâ‹†= limtâ†’âˆF(t) is the
long-run (or steady-state) age distribution.
Problem: Assuming that s1, . . . , sn and b2, . . . , bn are positive, explain why
the population age distribution approaches a steady state, and then describe it.
In other words, show that Fâ‹†= limtâ†’âˆF(t) exists, and determine its value.
Solution: The equations in (8.3.17) constitute a system of homogeneous diï¬€er-
ence equations that can be written in matrix form as
f(t + 1) = Lf(t),
where
L =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
b1
b2
Â· Â· Â·
bnâˆ’1
bn
s1
0
Â· Â· Â·
Â· Â· Â·
0
0
s2
0
0
...
...
...
...
0
0
Â· Â· Â·
sn
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
nÃ—n
.
(8.3.18)

684
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
The matrix L is called the Leslie matrix in honor of P. H. Leslie who used this
model in 1945. Notice that in addition to being nonnegative, L is also irreducible
when s1, . . . , sn and b2, . . . , bn are positive because the graph G(L) is strongly
connected. Moreover, L is primitive. This is obvious if in addition to s1, . . . , sn
and b2, . . . , bn being positive we have b1 > 0 (recall Example 8.3.3 on p. 678).
But even if b1 = 0, L is still primitive because Ln+2 > 0 (recall (8.3.16) on
p. 678). The technique on p. 679 also can be used to show primitivity (Exercise
8.3.11). Consequently, (8.3.10) on p. 674 guarantees that
lim
tâ†’âˆ
L
r
t
= G = pqT
qT p > 0,
where p > 0 and q > 0 are the respective Perron vectors for L and LT . If we
combine this with the fact that the solution to the system of diï¬€erence equations
in (8.3.18) is f(t) = Ltf(0) (p. 617), and if we assume that f(0) Ì¸= 0, then we
arrive at the conclusion that
lim
tâ†’âˆ
f(t)
rt
= Gf(0) = p

qT f(0)
qT p

and
lim
tâ†’âˆ

f(t)
rt

1
= qT f(0)
qT p
> 0
(8.3.19)
(because âˆ¥â‹†âˆ¥1 is a continuous functionâ€”Exercise 5.1.7 on p. 277). Now
Fk(t) =
fk(t)
âˆ¥f(t)âˆ¥1
= % of population that is in Gk at time t
is the quantity of interest, and (8.3.19) allows us to conclude that
Fâ‹†= lim
tâ†’âˆF(t) = lim
tâ†’âˆ
f(t)
âˆ¥f(t)âˆ¥1
= lim
tâ†’âˆ
f(t)/rt
âˆ¥f(t)âˆ¥1 /rt
=
limtâ†’âˆf(t)/rt
limtâ†’âˆâˆ¥f(t)âˆ¥1 /rt = p
(the Perron vector!).
In other words, while the numbers in the various age groups may increase or
decrease, depending on the value of r (Exercise 8.3.10), the proportion of in-
dividuals in each age group becomes stable as time increases. And because the
steady-state age distribution is given by the Perron vector of L, each age group
must eventually contain a positive fraction of the population.
Exercises for section 8.3
8.3.1. Let A =
 0
1
0
3
0
3
0
2
0

.
(a)
Show that A is irreducible.
(b)
Find the Perron root and Perron vector for A.
(c)
Find the number of eigenvalues on the spectral circle of A.

8.3 Nonnegative Matrices
685
8.3.2. Suppose that the index of imprimitivity of a 5 Ã— 5 nonnegative irre-
ducible matrix A is h = 3. Explain why A must be singular with
alg multA (0) = 2.
8.3.3. Suppose that A is a nonnegative matrix that possesses a positive spec-
tral radius and a corresponding positive eigenvector. Does this force A
to be irreducible?
8.3.4. Without computing the eigenvalues or the characteristic polynomial,
explain why Ïƒ (Pn) = {1, Ï‰, Ï‰2, . . . , Ï‰nâˆ’1}, where Ï‰ = e2Ï€i/n for
Pn=
ï£«
ï£¬
ï£¬
ï£­
0
1
0
Â· Â· Â·
0
0
0
1
Â· Â· Â·
0
...
...
...
...
...
0
0
Â· Â· Â·
0
1
1
0
0
Â· Â· Â·
0
ï£¶
ï£·
ï£·
ï£¸.
8.3.5. Determine whether A =
ï£«
ï£­
0
1
2
0
0
0
0
0
7
0
2
0
0
0
0
0
9
2
0
4
0
0
0
1
0
ï£¶
ï£¸is reducible or irreducible.
8.3.6. Determine whether the matrix A in Exercise 8.3.5 is primitive or im-
primitive.
8.3.7. A matrix SnÃ—n â‰¥0 having row sums less than or equal to 1 with at
least one row sum less than 1 is called a substochastic matrix.
(a)
Explain why Ï (S) â‰¤1 for every substochastic matrix.
(b)
Prove that Ï (S) < 1 for every irreducible substochastic matrix.
8.3.8. A nonnegative matrix for which each row sum is 1 is called a stochastic
matrix (some say row-stochastic). Prove that if AnÃ—n is nonnegative
and irreducible with r = Ï (A) , then A is similar to rP for some ir-
reducible stochastic matrix P. Hint: Consider D=
ï£«
ï£¬
ï£­
p1
0
Â· Â· Â·
0
0
p2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
pn
ï£¶
ï£·
ï£¸,
where the pk â€™s are the components of the Perron vector for A.
8.3.9. Wielandt constructed the matrix Wn=
ï£«
ï£¬
ï£¬
ï£­
0
1
0
Â· Â· Â·
0
0
0
1
Â· Â· Â·
0
...
...
...
...
...
0
0
Â· Â· Â·
0
1
1
1
0
Â· Â· Â·
0
ï£¶
ï£·
ï£·
ï£¸to show
that Wn2âˆ’2n+2 > 0, but [Wn2âˆ’2n+1]11 = 0. Verify that this is true
for n = 4.

686
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
8.3.10. In the Leslie population model on p. 683, explain what happens to the
vector f(t) as t â†’âˆdepending on whether r < 1, r = 1, or r > 1.
8.3.11. Use the characteristic equation as described on p. 679 to show that the
Leslie matrix in (8.3.18) is primitive even if b1 = 0 (assuming all other
bk â€™s and sk â€™s are positive).
8.3.12. A matrix A âˆˆâ„œnÃ—n is said to be essentially positive if A is irre-
ducible and aij â‰¥0 for every i Ì¸= j. Prove that each of the following
statements is equivalent to saying that A is essentially positive.
(a)
There exists some Î± âˆˆâ„œsuch that A + Î±I is primitive.
(b)
etA > 0 for all t > 0.
8.3.13. Let A be an essentially positive matrix as deï¬ned in Exercise 8.3.12.
Prove that each of the following statements is true.
(a)
A has an eigenpair (Î¾, x), where Î¾ is real and x > 0.
(b)
If Î» is any eigenvalue for A other than Î¾, then Re (Î») < Î¾.
(c)
Î¾ increases when any entry in A is increased.
8.3.14. Let A â‰¥0 be an irreducible matrix, and let a(k)
ij
denote entries in Ak.
Prove that A is primitive if and only if
Ï (A) = lim
kâ†’âˆ

a(k)
ij
1/k
.

8.4 Stochastic Matrices and Markov Chains
687
8.4
STOCHASTIC MATRICES AND MARKOV CHAINS
One of the most elegant applications of the Perronâ€“Frobenius theory is the al-
gebraic development of the theory of ï¬nite Markov chains. The purpose of this
section is to present some of the aspects of this development.
A stochastic matrix is a nonnegative matrix PnÃ—n in which each row
sum is equal to 1. Some authors say â€œrow-stochasticâ€ to distinguish this from
the case when each column sum is 1.
A Markov
92chain is a stochastic process (a set of random variables {Xt}âˆ
t=0
in which Xt has the same range {S1, S2, . . . , Sn}, called the state space) that
satisï¬es the Markov property
P(Xt+1 = Sj | Xt = Sit, Xtâˆ’1 = Sitâˆ’1, . . . , X0 = Si0) = P(Xt+1 = Sj | Xt = Sit)
for each t = 0, 1, 2, . . . . Think of a Markov chain as a random chain of events
that occur at discrete points t = 0, 1, 2, . . . in time, where Xt represents the
state of the event that occurs at time t. For example, if a mouse moves randomly
through a maze consisting of chambers S1, S2, . . . , Sn, then Xt might represent
the chamber occupied by the mouse at time t. The Markov property asserts that
the process is memoryless in the sense that the state of the chain at the next
time period depends only on the current state and not on the past history of the
chain. In other words, the mouse moving through the maze obeys the Markov
property if its next move doesnâ€™t depend on where in the maze it has been in
the pastâ€”i.e., the mouse is not using its memory (if it has one).
To emphasize that time is considered discretely rather than continuously the
phrase â€œdiscrete-time Markov chainâ€ is often used, and the phrase â€œï¬nite-state
Markov chainâ€ might be used to emphasize that the state space is ï¬nite rather
than inï¬nite.
92
Andrei Andreyevich Markov (1856â€“1922) was born in Ryazan, Russia, and he graduated from
Saint Petersburg University in 1878 where he later became a professor. Markovâ€™s early interest
was number theory because this was the area of his famous teacher Pafnuty Lvovich Chebyshev
(1821â€“1894). But when Markov discovered that he could apply his knowledge of continued frac-
tions to probability theory, he embarked on a new course that would make him famousâ€”enough
so that there was a lunar crater named in his honor in 1964. In addition to being involved with
liberal political movements (he once refused to be decorated by the Russian Czar), Markov
enjoyed poetry, and in his spare time he studied poetic style. Therefore, it was no accident
that led him to analyze the distribution of vowels and consonants in Pushkinâ€™s work, Eugene
Onegin, by constructing a simple model based on the assumption that the probability that a
consonant occurs at a given position in any word should depend only on whether the preceding
letter is a vowel or a consonant and not on any prior history. This was the birth of the â€œMarkov
chain.â€ Markov was wrong in one regardâ€”he apparently believed that the only real examples
of his chains were to be found in literary texts. But Markovâ€™s work in 1907 has grown to be-
come an indispensable tool of enormous power. It launched the theory of stochastic processes
that is now the foundation for understanding, explaining, and predicting phenomena in diverse
areas such as atomic physics, quantum theory, biology, genetics, social behavior, economics,
and ï¬nance. Markovâ€™s chains serve to underscore the point that the long-term applicability of
mathematical research is impossible to predict.

688
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
Every Markov chain deï¬nes a stochastic matrix, and conversely. Letâ€™s see
how this happens. The value pij(t) = P(Xt = Sj | Xtâˆ’1 = Si) is the probability
of being in state Sj at time t given that the chain is in state Si at time t âˆ’1,
so pij(t) is called the transition probability of moving from Si to Sj at
time t. The matrix of transition probabilities PnÃ—n(t) = [pij(t)] is clearly a
nonnegative matrix, and a little thought should convince you that each row sum
must be 1. Thus P(t) is a stochastic matrix. When the transition probabilities
donâ€™t vary with time (say pij(t) = pij for all t), the chain is said to be stationary
(or homogeneous), and the transition matrix is the constant stochastic matrix
P = [pij]. We will make the assumption of stationarity throughout. Conversely,
every stochastic matrix PnÃ—n deï¬nes an n -state Markov chain because the
entries pij deï¬ne a set of transition probabilities, which can be interpreted as a
stationary Markov chain on n states.
A probability distribution vector is deï¬ned to be a nonnegative vector
pT = (p1, p2, . . . , pn) such that 
k pk = 1. (Every row in a stochastic ma-
trix is such a vector.) For an n -state Markov chain, the kth step probability
distribution vector is deï¬ned to be
pT (k) =

p1(k), p2(k), . . . , pn(k)

,
k = 1, 2, . . . ,
where pj(k) = P(Xk = Sj).
In other words, pj(k) is the probability of being in the jth state after the kth
step, but before the (k + 1)st step. The initial distribution vector is
pT (0) =

p1(0), p2(0), . . . , pn(0)

,
where
pj(0) = P(X0 = Sj)
is the probability that the chain starts in Sj.
For example, consider the Markov chain deï¬ned by placing a mouse in the
3-chamber box with connecting doors as shown in Figure 8.4.1, and suppose that
the mouse moves from the chamber it occupies to another chamber by picking a
door at randomâ€”say that the doors open each minute, and when they do, the
mouse is forced to move by electrifying the ï¬‚oor of the occupied chamber.
#1
#2
#3
Figure 8.4.1
If the mouse is initially placed in chamber #2, then the initial distribution vector
is pT (0) = (0, 1, 0) = eT
2 . But if the process is started by tossing the mouse into
the air so that it randomly lands in one of the chambers, then a reasonable

8.4 Stochastic Matrices and Markov Chains
689
initial distribution is pT (0) = (.5, .25, .25) because the area of chamber #1 is
50% of the box, while chambers #2 and #3 each constitute 25% of the box. The
transition matrix for this Markov chain is the stochastic matrix
M =
ï£«
ï£­
0
1/2
1/2
1/3
0
2/3
1/3
2/3
0
ï£¶
ï£¸.
(8.4.1)
A standard eigenvalue calculation reveals that Ïƒ (M) = {1, âˆ’1/3, /, âˆ’2/3}, so
itâ€™s apparent that M is a nonnegative matrix having spectral radius Ï (M) = 1.
This is a feature that is shared by all stochastic matrices PnÃ—n because having
row sums equal to 1 means that âˆ¥Pâˆ¥âˆ= 1 or, equivalently, Pe = e, where e
is the column of all 1â€™s. Because (1, e) is an eigenpair for every stochastic matrix,
and because Ï (â‹†) â‰¤âˆ¥â‹†âˆ¥for every matrix norm (recall (7.1.12) on p. 497), it
follows that
1 â‰¤Ï (P) â‰¤âˆ¥Pâˆ¥âˆ= 1
=â‡’
Ï (P) = 1.
Furthermore, e is a positive eigenvector associated with Ï (P) = 1. But be
careful! This doesnâ€™t mean that you necessarily can call e the Perron vector for
P because P might not be irreducibleâ€”consider P =
 .5
.5
0
1

.
Two important issues that arise in Markovian analysis concern the transient
behavior of the chain as well as the limiting behavior. In other words, we want
to accomplish the following goals.
â€¢
Describe the kth step distribution pT (k) for any given initial distribution
vector pT (0).
â€¢
Determine whether or not limkâ†’âˆpT (k) exists, and if it exists, determine
the value of limkâ†’âˆpT (k).
â€¢
If there is no limiting distribution, then determine the possibility of having
a Ces`aro limit
lim
kâ†’âˆ
pT (0) + pT (1) + Â· Â· Â· + pT (k âˆ’1)
k

.
If such a limit exists, interpret its meaning, and determine its value.
The kth step distribution is easily described by using the laws of elementary
probabilityâ€”in particular, recall that P(E âˆ¨F) = P(E) + P(F) when E and
F are mutually exclusive events, and the conditional probability of E occurring
given that F occurs is P(E | F) = P(E âˆ§F)/P(F) (itâ€™s convenient to use âˆ§
and âˆ¨to denote AND and OR, respectively). To determine the jth component

690
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
pj(1) in pT (1) for a given pT (0), write
pj(1) = P(X1=Sj) = P

X1=Sj âˆ§(X0=S1 âˆ¨X0=S2 âˆ¨Â· Â· Â· âˆ¨X0=Sn)

= P

(X1=Sj âˆ§X0=S1) âˆ¨(X1=Sj âˆ§X0=S2) âˆ¨Â· Â· Â· âˆ¨(X1=Sj âˆ§X0=Sn)

=
n

i=1
P

X1=Sj âˆ§X0=Si

=
n

i=1
P

X0 = Si

P

X1 = Sj | X0 = Si

=
n

i=1
pi(0)pij
for
j = 1, 2, . . . , n.
Consequently, pT (1) = pT (0)P. This tells us what to expect after one step when
we start with pT (0). But the â€œno memoryâ€ Markov property tells us that the
state of aï¬€airs at the end of two steps is determined by where we are at the end of
the ï¬rst stepâ€”itâ€™s like starting over but with pT (1) as the initial distribution.
In other words, it follows that pT (2) = pT (1)P, and pT (3) = pT (2)P, etc.
Therefore, successive substitution yields
pT (k) = pT (k âˆ’1)P = pT (k âˆ’2)P2 = Â· Â· Â· = pT (0)Pk,
and thus the kth step distribution is determined from the initial distribution
and the transition matrix by the vectorâ€“matrix product
pT (k) = pT (0)Pk.
(8.4.2)
Notice that if we adopt the notation Pk =

p(k)
ij
	
, and if we set pT (0) = eT
i
in
(8.4.2), then we get pj(k) = p(k)
ij
for each i = 1, 2, . . . , n, and thus we arrive at
the following conclusion.
â€¢
The (i, j)-entry in Pk represents the probability of moving from Si to Sj
in exactly k steps. For this reason, Pk is often called the k-step transition
matrix.
Example 8.4.1
Letâ€™s go back to the mouse-in-the-box example, and, as suggested earlier, toss
the mouse into the air so that it randomly lands somewhere in the box in Fig-
ure 8.4.1â€”i.e., take the initial distribution to be pT (0) = (1/2, 1/4, 1/4). The
transition matrix is given by (8.4.1), so the probability of ï¬nding the mouse in
chamber #1 after three moves is
[pT (3)]1 = [pT (0)M3]1 = 13/54.
In fact, the entire third step distribution is pT (3) = ( 13/54, 41/108, 41/108 ) .

8.4 Stochastic Matrices and Markov Chains
691
To analyze limiting properties of Markov chains, divide the class of stochas-
tic matrices (and hence the class of stationary Markov chains) into four mutually
exclusive categories as described below.
(1)
Irreducible with limkâ†’âˆPk existing
(i.e., P is primitive).
(2)
Irreducible with limkâ†’âˆPk not existing
(i.e., P is imprimitive).
(3)
Reducible
with limkâ†’âˆPk existing.
(4)
Reducible
with limkâ†’âˆPk not existing.
In case (1), where P is primitive, we know exactly what limkâ†’âˆPk looks
like. The Perron vector for P is e/n (the uniform distribution vector), so if
Ï€ = (Ï€1, Ï€2, . . . , Ï€n)T is the Perron vector for PT , then
lim
kâ†’âˆPk = (e/n)Ï€T
Ï€T (e/n) = eÏ€T
Ï€T e = eÏ€T =
ï£«
ï£¬
ï£¬
ï£­
Ï€1
Ï€2
Â· Â· Â·
Ï€n
Ï€1
Ï€2
Â· Â· Â·
Ï€n
...
...
...
Ï€1
Ï€2
Â· Â· Â·
Ï€n
ï£¶
ï£·
ï£·
ï£¸> 0
(8.4.3)
by (8.3.10) on p. 674. Therefore, if P is primitive, then a limiting probability
distribution exists, and it is given by
lim
kâ†’âˆpT (k) = lim
kâ†’âˆpT (0)Pk = pT (0)eÏ€T = Ï€T .
(8.4.4)
Notice that because 
k pk(0) = 1, the term pT (0)e drops away, so we have the
conclusion that the value of the limit is independent of the value of the initial
distribution pT (0), which isnâ€™t too surprising.
Example 8.4.2
Going back to the mouse-in-the-box example, itâ€™s easy to conï¬rm that the transi-
tion matrix M in (8.4.1) is primitive, so limkâ†’âˆMk as well as limkâ†’âˆpT (0)
must exist, and their values are determined by the left-hand Perron vector of
M that can be found by calculating any nonzero vector v âˆˆN
	
I âˆ’MT 
and
normalizing it to produce Ï€T = vT / âˆ¥vâˆ¥1 . Routine computation reveals that
the one solution of the homogeneous equation (Iâˆ’MT )v = 0 is vT = (2, 3, 3),
so Ï€T = (1/8)(2, 3, 3), and thus
lim
kâ†’âˆMk = 1
8
ï£«
ï£­
2
3
3
2
3
3
2
3
3
ï£¶
ï£¸
and
lim
kâ†’âˆpT (k) = 1
8(2, 3, 3).
This limiting distribution can be interpreted as meaning that in the long run the
mouse will occupy chamber #1 one-fourth of the time, while 37.5% of the time itâ€™s
in chamber #2, and 37.5% of the time itâ€™s in chamber #3, and this is independent
of where (or how) the process started. The mathematical justiï¬cation for this
statement is on p. 693.

692
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
Now consider the imprimitive case. We know that if P is irreducible and has
h > 1 eigenvalues on the unit (spectral) circle, then limkâ†’âˆPk cannot exist
(p. 674), and hence limkâ†’âˆpT (k) cannot exist (otherwise taking pT (0) = eT
i
for each i would insure that Pk has a limit). However, each eigenvalue on the
unit circle is simple (p. 676), and this means that P is Ces`aro summable (p. 633).
Moreover, e/n is the Perron vector for P, and, as pointed out in Example 8.3.2
(p. 677), if Ï€T = (Ï€1, Ï€2, . . . , Ï€n) is the left-hand Perron vector, then
lim
kâ†’âˆ
I + P + Â· Â· Â· + Pkâˆ’1
k
= (e/n)Ï€T
Ï€T (e/n) = eÏ€T
Ï€T e = eÏ€T =
ï£«
ï£¬
ï£¬
ï£­
Ï€1
Ï€2
Â· Â· Â·
Ï€n
Ï€1
Ï€2
Â· Â· Â·
Ï€n
...
...
...
Ï€1
Ï€2
Â· Â· Â·
Ï€n
ï£¶
ï£·
ï£·
ï£¸,
which is exactly the same form as the limit (8.4.3) for the primitive case. Con-
sequently, the kth step distributions have a Ces`aro limit given by
lim
kâ†’âˆ
pT (0) + pT (1) + Â· Â· Â· + pT (k âˆ’1)
k

= lim
kâ†’âˆpT (0)
I + P + Â· Â· Â· + Pkâˆ’1
k

= pT (0)eÏ€T = Ï€T ,
and, just as in the primitive case (8.4.4), this Ces`aro limit is independent of the
initial distribution.
Letâ€™s interpret the meaning of this Ces`aro limit. The analysis is essentially
the same as the description outlined in the shell game in Example 7.10.8 (p. 635),
but for the sake of completeness we will duplicate some of the logic here. The
trick is to focus on one state, say Sj, and deï¬ne a sequence of random variables
{Zk}âˆ
k=0 that count the number of visits to Sj. Let
Z0 =

1
if the chain starts in Sj,
0
otherwise,
and for i > 1,
(8.4.5)
Zi =

1
if the chain is in Sj after the ith move,
0
otherwise.
Notice that Z0 + Z1 + Â· Â· Â· + Zkâˆ’1 counts the number of visits to Sj before the
kth move, so (Z0 + Z1 + Â· Â· Â· + Zkâˆ’1)/k represents the fraction of times that Sj
is hit before the kth move. The expected (or mean) value of each Zi is
E[Zi] = 1 Â· P(Zi=1) + 0 Â· P(Zi=0) = P(Zi=1) = pj(i),
and, since expectation is linear, the expected fraction of times that Sj is hit
before move k is
E
Z0 + Z1 + Â· Â· Â· + Zkâˆ’1
k

= E[Z0] + E[Z1] + Â· Â· Â· + E[Zkâˆ’1]
k
= pj(0) + pj(1) + Â· Â· Â· + pj(k âˆ’1)
k
=
pT (0) + pT (1) + Â· Â· Â· + pT (k âˆ’1)
k

j
â†’Ï€j.

8.4 Stochastic Matrices and Markov Chains
693
In other words, the long-run fraction of time that the chain spends in Sj is
Ï€j, which is the jth component of the Ces`aro limit or, equivalently, the jth
component of the left-hand Perron vector for P.
When limkâ†’âˆpT (k) exists, it must be the case that
lim
kâ†’âˆpT (k) = lim
kâ†’âˆ
pT (0)+pT (1)+Â· Â· Â·+pT (kâˆ’1)
k

(Exercise 7.10.11, p. 639),
and therefore the interpretation of the limiting distribution limkâ†’âˆpT (k) for
the primitive case is exactly the same as the interpretation of the Ces`aro limit
in the imprimitive case.
Below is a summary of our ï¬ndings for irreducible chains.
Irreducible Markov Chains
Let P be the transition probability matrix for an irreducible Markov
chain on states
{S1, S2, . . . , Sn}
(i.e.,
P
is an
n Ã— n
irreducible
stochastic matrix), and let Ï€T denote the left-hand Perron vector for P.
The following statements are true for every initial distribution pT (0).
â€¢
The kth step transition matrix is Pk because the (i, j) -entry in
Pk is the probability of moving from Si to Sj in exactly k steps.
â€¢
The kth step distribution vector is given by pT (k) = pT (0)Pk.
â€¢
If P is primitive, and if e denotes the column of all 1â€™s, then
lim
kâ†’âˆPk = eÏ€T
and
lim
kâ†’âˆpT (k) = Ï€T .
â€¢
If P is imprimitive, then
lim
kâ†’âˆ
I + P + Â· Â· Â· + Pkâˆ’1
k
= eÏ€T
and
lim
kâ†’âˆ
pT (0)+pT (1)+Â· Â· Â·+pT (kâˆ’1)
k

= Ï€T .
â€¢
Regardless of whether P is primitive or imprimitive, the jth com-
ponent Ï€j of Ï€T represents the long-run fraction of time that the
chain is in Sj.
â€¢
Ï€T is often called the stationary distribution vector for the chain
because it is the unique distribution vector satisfying Ï€T P = Ï€T .

694
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
Example 8.4.3
Periodic Chains. Consider an electronic switch that can be in one of three
states {S1, S2, S3}, and suppose that the switch changes states on regular clock
cycles. If the switch is in either S1 or S3, then it must change to S2 on the
next clock cycle, but if the switch is in S2, then there is an equal likelihood that
it changes to S1 or S3 on the next clock cycle. The transition matrix is
P =
ï£«
ï£­
0
1
0
.5
0
.5
0
1
0
ï£¶
ï£¸,
and itâ€™s not diï¬ƒcult to see that P is irreducible (because G(P) is strongly con-
nected) and imprimitive (because Ïƒ (P) = {Â±1, 0}). Since the left-hand Perron
vector is Ï€T = (.25, .5, .25), the long-run expectation is that the switch should
be in S1 25% of the time, in S2 50% of the time, and in S3 25% of the time,
and this agrees with what common sense tells us. Furthermore, notice that the
switch cannot be in just any position at any given clock cycle because if the
chain starts in either S1 or S3, then it must be in S2 on every odd-numbered
cycle, and it can occupy S1 or S3 only on even-numbered cycles. The situation
is similar, but with reversed parity, when the chain starts in S2. In other words,
the chain is periodic in the sense that the states can be occupied only at peri-
odic points in time. In this example the period of the chain is 2, and this is the
same as the index of imprimitivity. This is no accident. The Frobenius form for
imprimitive matrices on p. 680 can be used to prove that this is true in general.
Consequently, an irreducible Markov chain is said to be a periodic chain when
its transition matrix P is imprimitive (with the period of the chain being the
index of imprimitivity for P), and an irreducible Markov chain for which P
is primitive is called an aperiodic chain. The shell game in Example 7.10.8
(p. 635) is a periodic Markov chain that is similar to the one in this example.
Because the Perronâ€“Frobenius theorem is not directly applicable to reducible
chains (chains for which P is a reducible matrix), the strategy for analyzing
reducible chains is to deï¬‚ate the situation, as much as possible, back to the
irreducible case as described below.
If P is reducible, then, by deï¬nition, there exists a permutation matrix Q
and square matrices X and Z such that
QT PQ =
 X
Y
0
Z

. For convenience, denote this by writing P âˆ¼
 X
Y
0
Z

.
If X or Z is reducible, then another symmetric permutation can be performed
to produce
 X
Y
0
Z

âˆ¼
 R
S
T
0
U
V
0
0
W

,
where R, U, and W are square.

8.4 Stochastic Matrices and Markov Chains
695
Repeating this process eventually yields
P âˆ¼
ï£«
ï£¬
ï£­
X11
X12
Â· Â· Â·
X1k
0
X22
Â· Â· Â·
X2k
...
...
...
0
0
Â· Â· Â·
Xkk
ï£¶
ï£·
ï£¸,
where each Xii is irreducible or Xii = [0]1Ã—1.
Finally, if there exist rows having nonzero entries only in diagonal blocks, then
symmetrically permute all such rows to the bottom to produce
P âˆ¼
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
P11
P12
Â· Â· Â·
Prr
P1,r+1
P1,r+2
Â· Â· Â·
P1m
0
P22
Â· Â· Â·
P2r
P2,r+1
P2,r+2
Â· Â· Â·
P2m
...
...
...
...
...
Â· Â· Â·
...
0
0
Â· Â· Â·
Prr
Pr,r+1
Pr,r+2
Â· Â· Â·
Prm
0
0
Â· Â· Â·
0
Pr+1,r+1
0
Â· Â· Â·
0
0
0
Â· Â· Â·
0
0
Pr+2,r+2
Â· Â· Â·
0
...
...
Â· Â· Â·
...
...
...
...
...
0
0
Â· Â· Â·
0
0
0
Â· Â· Â·
Pmm
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
(8.4.6)
where each P11, . . . , Prr is either irreducible or [0]1Ã—1, and Pr+1,r+1, . . . , Pmm
are irreducible (they canâ€™t be zero because each has row sums equal to 1). As
mentioned on p. 671, the eï¬€ect of a symmetric permutation is simply to relabel
nodes in G(P) or, equivalently, to reorder the states in the chain. When the
states of a chain have been reordered so that P assumes the form on the right-
hand side of (8.4.6), we say that P is in the canonical form for reducible
matrices. When P is in canonical form, the subset of states corresponding
to Pkk for 1 â‰¤k â‰¤r is called the kth transient class (because once left,
a transient class canâ€™t be reentered), and the subset of states corresponding to
Pr+j,r+j for j â‰¥1 is called the jth ergodic class. Each ergodic class is an
irreducible Markov chain unto itself that is imbedded in the larger reducible
chain. From now on, we will assume that the states in our reducible chains have
been ordered so that P is in canonical form.
The results on p. 676 guarantee that if an irreducible stochastic matrix P
has h eigenvalues on the unit circle, then these h eigenvalues are the hth roots
of unity, and each is a simple eigenvalue for P. The same canâ€™t be said for
reducible stochastic matrices, but the canonical form (8.4.6) allows us to prove
the next best thing as discussed below.

696
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
Unit Eigenvalues
The unit eigenvalues for a stochastic matrix are deï¬ned to be those
eigenvalues that are on the unit circle. For every stochastic matrix PnÃ—n,
the following statements are true.
â€¢
Every unit eigenvalue of P is semisimple.
â€¢
Every unit eigenvalue has form Î» = e2kÏ€i/h for some k < h â‰¤n.
â€¢
In particular, Ï (P) = 1 is always a semisimple eigenvalue of P.
Proof.
If P is irreducible, then there is nothing to prove because, as proved on
p. 676, the unit eigenvalues are roots of unity, and each unit eigenvalue is simple.
If P is reducible, suppose that a symmetric permutation has been performed so
that P is in the canonical form (8.4.6), and observe that
Ï (Pkk) < 1
for each k = 1, 2, . . . , r.
(8.4.7)
This is certainly true when Pkk = [0]1Ã—1, so suppose that Pkk (1 â‰¤k â‰¤r)
is irreducible. Because there must be blocks Pkj,
j Ì¸= k, that have nonzero
entries, it follows that
Pkke â‰¤e
and
Pkke Ì¸= e,
where e is the column of all 1â€™s.
If Ï (Pkk) = 1, then the observation in Example 8.3.1 (p. 674) forces Pkke = e,
which is impossible, and thus Ï (Pkk) < 1. Consequently, the unit eigenval-
ues for P are the collection of the unit eigenvalues of the irreducible matrices
Pr+1,r+1, . . . , Pmm. But each unit eigenvalue of Pr+i,r+i is simple and is a
root of unity. Consequently, if Î» is a unit eigenvalue for P, then it must be
some root of unity, and although it might be repeated because it appears in
the spectrum of more than one Pr+i,r+i, it must nevertheless be the case that
alg multP (Î») = geo multP (Î») , so Î» is a semisimple eigenvalue of P.
We know from the discussion on p. 633 that a matrix A âˆˆCnÃ—n is Ces`aro
summable if and only if Ï(A) < 1 or Ï(A) = 1 with each eigenvalue on the unit
circle being semisimple. We just proved that the latter holds for all stochastic
matrices P, so we have in fact established the following powerful statement
concerning all stochastic matrices.

8.4 Stochastic Matrices and Markov Chains
697
All Stochastic Matrices Are Summable
Every stochastic matrix P is Ces`aro summable. That is,
lim
kâ†’âˆ
I + P + Â· Â· Â· + Pkâˆ’1
k
exists for all stochastic matrices P,
and, as discussed on p. 633, the value of the limit is the (spectral) pro-
jector G onto N (I âˆ’P) along R (I âˆ’P).
Since we already know the structure and interpretation of the Ces`aro limit
when P is an irreducible stochastic matrix (p. 693), all that remains in order to
complete the picture is to analyze the nature of limkâ†’âˆ(I + P + Â· Â· Â· + Pkâˆ’1)/k
for the reducible case.
Suppose that P =
 T11
T12
0
T22

is a reducible stochastic matrix that is in
the canonical form (8.4.6), where
T11 =
ï£«
ï£­
P11
Â· Â· Â·
Prr
...
...
Prr
ï£¶
ï£¸,
T12 =
ï£«
ï£­
P1,r+1 Â· Â· Â· P1m
...
...
Pr,r+1 Â· Â· Â· Prm
ï£¶
ï£¸,
and
T22 =
ï£«
ï£­
Pr+1,r+1
...
Pmm
ï£¶
ï£¸.
(8.4.8)
We know from (8.4.7) that Ï (Pkk) < 1 for each k = 1, 2, . . . , r, so it follows
that Ï (T11) < 1, and hence
lim
kâ†’âˆ
I + T11 + Â· Â· Â· + Tkâˆ’1
11
k
= lim
kâ†’âˆTk
11 = 0
(recall Exercise 7.10.11 on p. 639).
Furthermore, Pr+1,r+1, . . . , Pmm are each irreducible stochastic matrices, so if
Ï€T
j
is the left-hand Perron vector for Pjj, r + 1 â‰¤j â‰¤m, then our previous
results (p. 693) tell us that
lim
kâ†’âˆ
I + T22 + Â· Â· Â· + Tkâˆ’1
22
k
=
ï£«
ï£¬
ï£­
eÏ€T
r+1
...
eÏ€T
m
ï£¶
ï£·
ï£¸= E.
(8.4.9)
Furthermore, itâ€™s clear from the results on p. 674 that limkâ†’âˆTk
22 exists if and
only if Pr+1,r+1, . . . , Pmm are each primitive, in which case limkâ†’âˆTk
22 = E.

698
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
Therefore, the limits, be they Ces`aro or ordinary (if it exists), all have the form
lim
kâ†’âˆ
I + P + Â· Â· Â· + Pkâˆ’1
k
=

0
Z
0
E

= G = lim
kâ†’âˆPk (when it exists).
To determine the precise nature of Z, use the fact that R (G) = N (I âˆ’P)
(because G is the projector onto N (I âˆ’P) along R (I âˆ’P)) to write
(Iâˆ’P)G = 0
=â‡’
 I âˆ’T11
âˆ’T12
0
I âˆ’T22
  0
Z
0
E

= 0
=â‡’
(Iâˆ’T11)Z = T12E.
Since I âˆ’T11 is nonsingular (because Ï (T11) < 1 by (8.4.7)), it follows that
Z = (I âˆ’T11)âˆ’1T12E,
and thus the following results concerning limits of reducible chains are produced.
Reducible Markov Chains
If the states in a reducible Markov chain have been ordered to make the
transition matrix assume the canonical form
P =

T11
T12
0
T22

that is described in (8.4.6) and (8.4.8), and if Ï€T
j is the left-hand Perron
vector for Pjj
(r + 1 â‰¤j â‰¤m), then I âˆ’T11 is nonsingular, and
lim
kâ†’âˆ
I + P + Â· Â· Â· + Pkâˆ’1
k
=
 0
(I âˆ’T11)âˆ’1T12E
0
E

,
where
E =
ï£«
ï£­
eÏ€T
r+1
...
eÏ€T
m
ï£¶
ï£¸.
Furthermore, limkâ†’âˆPk exists if and only if the stochastic matrices
Pr+1,r+1, . . . , Pmm in (8.4.6) are each primitive, in which case
lim
kâ†’âˆPk =
 0
(I âˆ’T11)âˆ’1T12E
0
E

.
(8.4.10)

8.4 Stochastic Matrices and Markov Chains
699
The preceding analysis shows that every reducible chain eventually gets
absorbed (trapped) into one of the ergodic classesâ€”i.e., into a subchain deï¬ned
by Pr+j,r+j for some j â‰¥1. If Pr+j,r+j is primitive, then the chain settles
down to a steady-state deï¬ned by the left-hand Perron vector of Pr+j,r+j, but
if Pr+j,r+j is imprimitive, then the process will oscillate in the jth ergodic class
forever. There is not much more that can be said about the limit, but there are
still important questions concerning which ergodic class the chain will end up in
and how long it takes to get there. This time the answer depends on where the
chain startsâ€”i.e., on the initial distribution.
For convenience, let Ti denote the ith transient class, and let Ej be the jth
ergodic class. Suppose that the chain starts in a particular transient stateâ€”say
we start in the pth state of Ti. Since the question at hand concerns only which
ergodic class is hit but not what happens after itâ€™s entered, we might as well
convert every state in each ergodic class into a trap by setting Pr+j,r+j = I
for each j â‰¥1 in (8.4.6). The transition matrix for this modiï¬ed chain is
P =
 T11
T12
0
I

, and it follows from (8.4.10) that limkâ†’âˆPk exists and has
the form
lim
kâ†’âˆ
Pk =
 0
(I âˆ’T11)âˆ’1T12
0
I

=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
0
Â· Â· Â·
0
L1,1
L1,2
Â· Â· Â·
L1s
0
0
Â· Â· Â·
0
L2,1
L2,2
Â· Â· Â·
L2s
...
...
...
...
...
Â· Â· Â·
...
0
0
Â· Â· Â·
0
Lr,1
Lr,2
Â· Â· Â·
Lrs
0
0
Â· Â· Â·
0
I
0
Â· Â· Â·
0
0
0
Â· Â· Â·
0
0
I
Â· Â· Â·
0
...
...
Â· Â· Â·
...
...
...
...
...
0
0
Â· Â· Â·
0
0
0
Â· Â· Â·
I
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Consequently, the (p, q)-entry in block Lij represents the probability of even-
tually hitting the qth state in Ej given that we start from the pth state in
Ti. Therefore, if e is the vector of all 1 â€™s, then the probability of eventually
entering somewhere in Ej is given by
â€¢
P(absorption into Ej | start in pth state of Ti) = 
k

Lij
	
pk =

Lije
	
p.
If pT
i (0) is an initial distribution for starting in the various states of Ti, then
â€¢
P

absorption into Ej | pT
i (0)

= pT
i (0)Lije.
To determine the expected number of steps required to ï¬rst hit an ergodic
state, proceed as follows. Count the number of times the chain is in transient
state Sj given that it starts in transient state Si by reapplying the argument
given in (8.4.5) on p. 692. That is, given that the chain starts in Si, let
Z0 =

1
if Si = Sj,
0
otherwise,
and
Zk =
 1
if the chain is in Sj after step k,
0
otherwise.

700
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
Since
E[Zk] = 1 Â· P(Zk=1) + 0 Â· P(Zk=0) = P(Zk=1) =

Tk
11
	
ij,
and since âˆ
k=0 Zk is the total number of times the chain is in Sj, we have
E[# times in Sj| start in Si] = E
 âˆ

k=0
Zk

=
âˆ

k=0
E [Zk] =
âˆ

k=0

Tk
11
	
ij
=

(I âˆ’T11)âˆ’1	
ij
(because Ï (T11) < 1).
Summing this over all transient states produces the expected number of times
the chain is in some transient state, which is the same as the expected number
of times before ï¬rst hitting an ergodic state. In other words,
â€¢
E[# steps until absorption | start in ith transient state] =

(I âˆ’T11)âˆ’1e
	
i.
Example 8.4.4
Absorbing Markov Chains. Itâ€™s often the case in practical applications that
there is only one transient class, and the ergodic classes are just single absorbing
states (states such that once they are entered, they are never left). If the single
transient class contains r states, and if there are s absorbing states, then the
canonical form for the transition matrix is
P =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
p11
Â· Â· Â·
p1r
p1,r+1
Â· Â· Â·
p1s
...
...
...
...
pr1
Â· Â· Â·
prr
pr,r+1
Â· Â· Â·
prs
0
Â· Â· Â·
0
1
Â· Â· Â·
0
...
...
...
...
...
0
Â· Â· Â·
0
0
Â· Â· Â·
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
and
Lij =

(I âˆ’T11)âˆ’1T12
	
ij.
The preceding analysis specializes to say that every absorbing chain must even-
tually reach one of its absorbing states. The probability of being absorbed into
the jth absorbing state (which is state Sr+j) given that the chain starts in the
ith transient state (which is Si) is
P(absorption into Sr+j| start in Si for 1 â‰¤i â‰¤r) =

(I âˆ’T11)âˆ’1T12
	
ij,
while the expected time until absorption is
E[# steps until absorption | start in Si] =

(I âˆ’T11)âˆ’1e
	
i ,
and the amount of time spent in Sj is
E[# times in Sj| start in Si] =

(I âˆ’T11)âˆ’1	
ij .

8.4 Stochastic Matrices and Markov Chains
701
Example 8.4.5
Fail-Safe System. Consider a system that has two independent controls, A and
B, that can prevent the system from being destroyed. The system is activated at
discrete points in time t1, t2, t3, . . . , and the system is considered to be â€œunder
controlâ€ if either control A or B holds at the time of activation. The system is
destroyed if A and B fail simultaneously.
â–·
For example, an automobile has two independent braking systemsâ€”one is
operated by a foot pedal, whereas the â€œemergency brakeâ€ is operated by a
hand lever. The automobile is â€œunder controlâ€ if at least one braking system
is operative when you try to stop, but a crash occurs if both braking systems
fail simultaneously.
If one of the controls fails at some activation point but the other control holds,
then the defective control is repaired before the next activation. If a control holds
at time t = tk, then it is considered to be 90% reliable at t = tk+1, but if a
control fails at time t = tk, then its untested replacement is considered to be
only 60% reliable at t = tk+1.
Problem: Can the system be expected to run indeï¬nitely without every being
destroyed? If not, how long is the system expected to run before destruction
occurs?
Solution: This is a four-state Markov chain with the states being the controls
that hold at any particular time of activation. In other words the state space is
the set of pairs (a, b) in which
a =

1
if A holds,
0
if A fails,
and
b =
 1
if B holds,
0
if B fails.
State (0, 0) is absorbing, and the transition matrix (in canonical form) is
P =
ï£«
ï£¬
ï£¬
ï£­
(1, 1)
(1, 0)
(0, 1)
(0, 0)
(1, 1)
.81
.09
.09
.01
(1, 0)
.54
.36
.06
.04
(0, 1)
.54
.06
.36
.04
(0, 0)
0
0
0
1
ï£¶
ï£·
ï£·
ï£¸
with
T11 =
ï£«
ï£­
.81
.09
.09
.54
.36
.06
.54
.06
.36
ï£¶
ï£¸
and
T12 =
ï£«
ï£­
.01
.04
.04
ï£¶
ï£¸.
The fact that limkâ†’âˆPk exists and is given by
lim
kâ†’âˆPk =
 0
(I âˆ’T11)âˆ’1T12
0
1


702
Chapter 8
Perronâ€“Frobenius Theory of Nonnegative Matrices
makes it clear that the absorbing state must eventually be reached. In other
words, this proves the validity of the popular belief that â€œif something can go
wrong, then it eventually will.â€ Rounding to three signiï¬cant ï¬gures produces
(I âˆ’T11)âˆ’1 =
ï£«
ï£­
44.6
6.92
6.92
41.5
8.02
6.59
41.5
6.59
8.02
ï£¶
ï£¸
and
(I âˆ’T11)âˆ’1e =
ï£«
ï£­
58.4
56.1
56.1
ï£¶
ï£¸,
so the mean time to failure starting with two proven controls is slightly more
than 58 steps, while the mean time to failure starting with one untested control
and one proven control is just over 56 steps. The diï¬€erence here doesnâ€™t seem
signiï¬cant, but consider what happens when only one control is used in the
system. In this case, there are only two states in the chain, 1 (meaning that the
control holds) and 0 (meaning that it doesnâ€™t). The transition matrix is
P =
 1
0
1
.9
.1
0
0
1

,
so now the mean time to failure is only (I âˆ’T11)âˆ’1e = 10 steps. Itâ€™s interesting
to consider what happens when three independent control are used. How much
more security does your intuition tell you that you should have? See Exercise
8.4.8.
Exercises for section 8.4
8.4.1. Find the stationary distribution for P =
ï£«
ï£­
1/4
0
0
3/4
3/8
1/4
3/8
0
1/3
1/6
1/6
1/3
0
0
1/2
1/2
ï£¶
ï£¸. Does
this stationary distribution represent a limiting distribution in the reg-
ular sense or only in the Ces`aro sense?
8.4.2. A doubly-stochastic matrix is a nonnegative matrix PnÃ—n having
all row sums as well as all column sums equal to 1. For an irreducible
n -state Markov chain whose transition matrix is doubly stochastic,
what is the long-run proportion of time spent in each state? What form
do limkâ†’âˆ(I + P + Â· Â· Â· + Pkâˆ’1)/k and limkâ†’âˆPk (if it exists) have?
Note: The purpose of this exercise is to show that doubly-stochastic
matrices are not very interesting from a Markov-chain point of view.
However, there is an interesting theoretical result (due to G. Birkhoï¬€
in 1946) that says the set of n Ã— n doubly-stochastic matrices forms
a convex polyhedron in â„œnÃ—n with the permutation matrices as the
vertices.

8.4 Stochastic Matrices and Markov Chains
703
8.4.3. Explain why rank (I âˆ’P) = n âˆ’1 for every irreducible stochastic ma-
trix Pnn. Give an example to show that this need not be the case for
reducible stochastic matrices.
8.4.4. Prove that the left-hand Perron vector for an irreducible stochastic ma-
trix PnÃ—n (n > 1) is given by
Ï€T =
1
n
i=1 Pi

P1, P2, . . . , Pn

,
where Pi is the ith principal minor determinant of order nâˆ’1 in Iâˆ’P.
Hint: What is [adj (A)]A if A is singular?
8.4.5. Let PnÃ—n be an irreducible stochastic matrix, and let QkÃ—k be a prin-
cipal submatrix of I âˆ’P, where 1 â‰¤k < n. Prove that Ï (Q) < 1.
8.4.6. Let PnÃ—n be an irreducible stochastic matrix, and let QkÃ—k be a prin-
cipal submatrix of I âˆ’P, where 1 â‰¤k < n. Explain why Q is an
M-matrix as deï¬ned and discussed on p. 626.
8.4.7. Let PnÃ—n (n > 1) be an irreducible stochastic matrix. Explain why all
principal minors of order 1 â‰¤k < n in I âˆ’P are positive.
8.4.8. Use the same assumptions that are used for the fail-safe system described
in Example 8.4.5, but use three controls, A, B, and C, instead of two.
Determine the mean time to failure starting with three proven controls,
two proven but one untested control, and three untested controls.
8.4.9. A mouse is placed in one chamber of the box shown in Figure 8.4.1 on
p. 688, and a cat is placed in another chamber. Each minute the doors to
the chambers are opened just long enough to allow movement from one
chamber to an adjacent chamber. Half of the time when the doors are
opened, the cat doesnâ€™t leave the chamber it occupies. The same is true
for the mouse. When either the cat or mouse moves, a door is chosen at
random to pass through.
(a)
Explain why the cat and mouse must eventually end up in the
same chamber, and determine the expected number of steps for
this to occur.
(b)
Determine the probability that the cat will catch the mouse in
chamber #j for each j = 1, 2, 3.

Solutions for Chapter 1
Solutions for exercises in section 1. 2
1.2.1. (1, 0, 0)
1.2.2. (1, 2, 3)
1.2.3. (1, 0, âˆ’1)
1.2.4. (âˆ’1/2, 1/2, 0, 1)
1.2.5.
ï£«
ï£­
2
âˆ’4
3
4
âˆ’7
4
5
âˆ’8
4
ï£¶
ï£¸
1.2.6. Every row operation is reversible. In particular the â€œinverseâ€ of any row operation
is again a row operation of the same type.
1.2.7.
Ï€
2 , Ï€, 0
1.2.8. The third equation in the triangularized form is 0x3 = 1, which is impossible
to solve.
1.2.9. The third equation in the triangularized form is 0x3 = 0, and all numbers are
solutions. This means that you can start the back substitution with any value
whatsoever and consequently produce inï¬nitely many solutions for the system.
1.2.10. Î± = âˆ’3, Î² = 11
2 , and Î³ = âˆ’3
2
1.2.11. (a) If xi = the number initially in chamber #i, then
.4x1 + 0x2 + 0x3 + .2x4 = 12
0x1 + .4x2 + .3x3 + .2x4 = 25
0x1 + .3x2 + .4x3 + .2x4 = 26
.6x1 + .3x2 + .3x3 + .4x4 = 37
and the solution is x1 = 10, x2 = 20, x3 = 30, and x4 = 40.
(b)
16, 22, 22, 40
1.2.12. To interchange rows i and j, perform the following sequence of Type II and
Type III operations.
Rj â†Rj + Ri
(replace row j by the sum of row j and i)
Ri â†Ri âˆ’Rj
(replace row i by the diï¬€erence of row i and j)
Rj â†Rj + Ri
(replace row j by the sum of row j and i)
Ri â†âˆ’Ri
(replace row i by its negative)
1.2.13. (a)
This has the eï¬€ect of interchanging the order of the unknownsâ€” xj and
xk are permuted.
(b)
The solution to the new system is the same as the

2
Solutions
solution to the old system except that the solution for the jth unknown of the
new system is Ë†xj = 1
Î±xj. This has the eï¬€ect of â€œchanging the unitsâ€ of the jth
unknown.
(c) The solution to the new system is the same as the solution for
the old system except that the solution for the kth unknown in the new system
is Ë†xk = xk âˆ’Î±xj.
1.2.14. hij =
1
i+jâˆ’1
1.2.16. If x =
ï£«
ï£¬
ï£¬
ï£­
x1
x2
...
xm
ï£¶
ï£·
ï£·
ï£¸
and
y =
ï£«
ï£¬
ï£¬
ï£­
y1
y2
...
ym
ï£¶
ï£·
ï£·
ï£¸are two diï¬€erent solutions, then
z = x + y
2
=
ï£«
ï£¬
ï£¬
ï£¬
ï£­
x1+y1
2
x2+y2
2
...
xm+ym
2
ï£¶
ï£·
ï£·
ï£·
ï£¸
is a third solution diï¬€erent from both x and y.
Solutions for exercises in section 1. 3
1.3.1. (1, 0, âˆ’1)
1.3.2. (2, âˆ’1, 0, 0)
1.3.3.
ï£«
ï£­
1
1
1
1
2
2
1
2
3
ï£¶
ï£¸
Solutions for exercises in section 1. 4
1.4.2. Use yâ€²(tk) = yâ€²
k â‰ˆyk+1 âˆ’ykâˆ’1
2h
and yâ€²â€²(tk) = yâ€²â€²
k â‰ˆykâˆ’1 âˆ’2yk + yk+1
h2
to write
f(tk) = fk = yâ€²â€²
k âˆ’yâ€²
k â‰ˆ2ykâˆ’1 âˆ’4yk + 2yk+1
2h2
âˆ’hyk+1 âˆ’hykâˆ’1
2h2
,
k = 1, 2, . . . , n,
with y0 = yn+1 = 0. These discrete approximations form the tridiagonal system
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ’4
2 âˆ’h
2 + h
âˆ’4
2 âˆ’h
...
...
...
2 + h
âˆ’4
2 âˆ’h
2 + h
âˆ’4
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
y1
y2
...
ynâˆ’1
yn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
= 2h2
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
f1
f2
...
fnâˆ’1
fn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.

Solutions
3
Solutions for exercises in section 1. 5
1.5.1. (a)
(0, âˆ’1)
(c)
(1, âˆ’1)
(e)

1
1.001,
âˆ’1
1.001
	
1.5.2. (a)
(0, 1)
(b)
(2, 1)
(c)
(2, 1)
(d)

2
1.0001, 1.0003
1.0001
	
1.5.3. Without PP: (1.01, 1.03)
With PP: (1, 1)
Exact: (1, 1)
1.5.4. (a)
ï£«
ï£­
1
.500
.333
.333
.500
.333
.250
.333
.333
.250
.200
.200
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
1
.500
.333
.333
0
.083
.083
.166
0
.083
.089
.089
ï£¶
ï£¸
âˆ’â†’
ï£«
ï£­
1
.500
.333
.333
0
.083
.083
.166
0
0
.006
âˆ’.077
ï£¶
ï£¸
z = âˆ’.077/.006 = âˆ’12.8,
y = (.166 âˆ’.083z)/.083 = 14.8,
x = .333 âˆ’(.5y + .333z) = âˆ’2.81
(b)
ï£«
ï£­
1
.500
.333
.333
.500
.333
.250
.333
.333
.250
.200
.200
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
1
.500
.333
.333
1
.666
.500
.666
1
.751
.601
.601
ï£¶
ï£¸
âˆ’â†’
ï£«
ï£­
1
.500
.333
.333
0
.166
.167
.333
0
.251
.268
.268
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
1
.500
.333
.333
0
.251
.268
.268
0
.166
.167
.333
ï£¶
ï£¸
âˆ’â†’
ï£«
ï£­
1
.500
.333
.333
0
.251
.268
.268
0
0
âˆ’.01
.156
ï£¶
ï£¸
z = âˆ’.156/.01 = âˆ’15.6,
y = (.268 âˆ’.268z)/.251 = 17.7,
x = .333 âˆ’(.5y + .333z) = âˆ’3.33
(c)
ï£«
ï£­
1
.500
.333
.333
.500
.333
.250
.333
.333
.250
.200
.200
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
1
.500
.333
.333
1
.666
.500
.666
1
.751
.601
.601
ï£¶
ï£¸
âˆ’â†’
ï£«
ï£­
1
.500
.333
.333
0
.166
.167
.333
0
.251
.268
.268
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
1
.500
.333
.333
0
.994
1
1.99
0
.937
1
1
ï£¶
ï£¸
âˆ’â†’
ï£«
ï£­
1
.500
.333
.333
0
.994
1
1.99
0
0
.057
âˆ’.880
ï£¶
ï£¸
z = âˆ’.88/.057 = âˆ’15.4,
y = (1.99 âˆ’z)/.994 = 17.5,
x = .333 âˆ’(.5y + .333z) = âˆ’3.29
(d)
x = âˆ’3,
y = 16,
z = âˆ’14
1.5.5. (a)
.0055x + .095y + 960z = 5000
.0011x + . 01y + 112z = 600
.0093x + .025y + 560z = 3000

4
Solutions
(b)
3-digit solution = (55, 900 lbs. silica, 8, 600 lbs. iron, 4.04 lbs. gold).
Exact solution (to 10 digits) = (56, 753.68899, 8, 626.560726, 4.029511918). The
relative error (rounded to 3 digits) is er = 1.49 Ã— 10âˆ’2.
(c)
Let u = x/2000, v = y/1000, and w = 12z to obtain the system
11u + 95v +
80w = 5000
2.2u + 10v + 9.33w = 600
18.6u + 25v + 46.7w = 3000.
(d)
3-digit solution = (28.5 tons silica, 8.85 half-tons iron, 48.1 troy oz. gold).
Exact solution (to 10 digits) = (28.82648317, 8.859282804, 48.01596023). The
relative error (rounded to 3 digits) is er = 5.95 Ã— 10âˆ’3. So, partial pivoting
applied to the column-scaled system yields higher relative accuracy than partial
pivoting applied to the unscaled system.
1.5.6. (a)
(âˆ’8.1, âˆ’6.09) = 3-digit solution with partial pivoting but no scaling.
(b) No! Scaled partial pivoting produces the exact solutionâ€”the same as with
complete pivoting.
1.5.7. (a)
2nâˆ’1
(b)
2
(c)
This is a famous example that shows that there are indeed cases where par-
tial pivoting will fail due to the large growth of some elements during elimination,
but complete pivoting will be successful because all elements remain relatively
small and of the same order of magnitude.
1.5.8. Use the fact that with partial pivoting no multiplier can exceed 1 together with
the triangle inequality |Î± + Î²| â‰¤|Î±| + |Î²| and proceed inductively.
Solutions for exercises in section 1. 6
1.6.1. (a) There are no 5-digit solutions. (b) This doesnâ€™t helpâ€”there are now inï¬nitely
many 5-digit solutions. (c) 6-digit solution = (1.23964, âˆ’1.3) and exact solution
= (1, âˆ’1) (d) r1 = r2 = 0 (e) r1 = âˆ’10âˆ’6 and r2 = 10âˆ’7 (f) Even if computed
residuals are 0, you canâ€™t be sure you have the exact solution.
1.6.2. (a)
(1, âˆ’1.0015) (b) Ill-conditioning guarantees that the solution will be very
sensitive to some small perturbation but not necessarily to every small perturba-
tion. It is usually diï¬ƒcult to determine beforehand those perturbations for which
an ill-conditioned system will not be sensitive, so one is forced to be pessimistic
whenever ill-conditioning is suspected.
1.6.3. (a)
m1(5) = m2(5) = âˆ’1.2519,
m1(6) = âˆ’1.25187, and m2(6) = âˆ’1.25188
(c)
An optimally well-conditioned system represents orthogonal (i.e., perpen-
dicular) lines, planes, etc.
1.6.4. They rank as (b) = Almost optimally well-conditioned. (a) = Moderately well-
conditioned. (c) = Badly ill-conditioned.
1.6.5. Original solution = (1, 1, 1). Perturbed solution = (âˆ’238, 490, âˆ’266). System
is ill-conditioned.

Solutions for Chapter 2
Solutions for exercises in section 2. 1
2.1.1. (a)
ï£«
ï£­
1
2
3
3
0
2
1
0
0
0
0
3
ï£¶
ï£¸is one possible answer. Rank = 3 and the basic columns
are {Aâˆ—1, Aâˆ—2, Aâˆ—4}. (b)
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1
2
3
0
2
2
0
0
âˆ’8
0
0
0
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£¸is one possible answer. Rank = 3 and
every column in A is basic.
(c)
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
2
1
1
3
0
4
1
0
0
2
âˆ’2
1
âˆ’3
3
0
0
0
0
âˆ’1
3
âˆ’1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
is one possible answer. The rank is 3, and
the basic columns are {Aâˆ—1, Aâˆ—3, Aâˆ—5}.
2.1.2. (c) and (d) are in row echelon form.
2.1.3. (a) Since any row or column can contain at most one pivot, the number of pivots
cannot exceed the number of rows nor the number of columns. (b) A zero row
cannot contain a pivot. (c) If one row is a multiple of another, then one of
them can be annihilated by the other to produce a zero row. Now the result
of the previous part applies. (d) One row can be annihilated by the associated
combination of row operations. (e) If a column is zero, then there are fewer than
n basic columns because each basic column must contain a pivot.
2.1.4. (a) rank (A) = 3 (b) 3-digit rank (A) = 2 (c) With PP, 3-digit rank (A) = 3
2.1.5. 15
2.1.6. (a) No, consider the form
ï£«
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
0
0
0
0
0
0
0
âˆ—
ï£¶
ï£¸(b) Yesâ€”in fact, E is a row
echelon form obtainable from A .
Solutions for exercises in section 2. 2
2.2.1. (a)
ï£«
ï£­
1
0
2
0
0
1
1
2
0
0
0
0
1
ï£¶
ï£¸
and
Aâˆ—3 = 2Aâˆ—1 + 1
2Aâˆ—2

6
Solutions
(b)
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
1
2
0
2
0
2
0
0
0
1
âˆ’1
0
0
1
0
0
0
0
1
âˆ’3
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
and
Aâˆ—2 = 1
2Aâˆ—1,
Aâˆ—4 = 2Aâˆ—1âˆ’Aâˆ—3,
Aâˆ—6 = 2Aâˆ—1âˆ’3Aâˆ—5,
Aâˆ—7 = Aâˆ—3+Aâˆ—5
2.2.2. No.
2.2.3. The same would have to hold in EA, and there you can see that this means not
all columns can be basic. Remember, rank (A) = number of basic columns.
2.2.4. (a)
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸
(b)
ï£«
ï£­
1
0
âˆ’1
0
1
2
0
0
0
ï£¶
ï£¸Aâˆ—3 is almost a combination of Aâˆ—1
and Aâˆ—2. In particular, Aâˆ—3 â‰ˆâˆ’Aâˆ—1 + 2Aâˆ—2.
2.2.5. Eâˆ—1 = 2Eâˆ—2 âˆ’Eâˆ—3 and Eâˆ—2 = 1
2Eâˆ—1 + 1
2Eâˆ—3
Solutions for exercises in section 2. 3
2.3.1. (a), (b)â€”There is no need to do any arithmetic for this one because the right-
hand side is entirely zero so that you know (0,0,0) is automatically one solution.
(d), (f)
2.3.3. It is always true that rank (A) â‰¤rank[A|b] â‰¤m. Since rank (A) = m, it
follows that rank[A|b] = rank (A).
2.3.4. Yesâ€”Consistency implies that b and c are each combinations of the basic
columns in A . If b = 
 Î²iAâˆ—bi and c = 
 Î³iAâˆ—bi where the Aâˆ—bi â€™s are the
basic columns, then b + c = 
(Î²i + Î³i)Aâˆ—bi = 
 Î¾iAâˆ—bi, where Î¾i = Î²i + Î³i
so that b + c is also a combination of the basic columns in A .
2.3.5. Yesâ€”because the 4 Ã— 3 system Î± + Î²xi + Î³x2
i = yi obtained by using the four
given points (xi, yi) is consistent.
2.3.6. The system is inconsistent using 5-digits but consistent when 6-digits are used.
2.3.7. If x, y, and z denote the number of pounds of the respective brands applied,
then the following constraints must be met.
total # units of phosphorous = 2x + y + z = 10
total # units of potassium = 3x + 3y
= 9
total # units of nitrogen = 5x + 4y + z = 19
Since this is a consistent system, the recommendation can be satisï¬ed exactly.
Of course, the solution tells how much of each brand to apply.
2.3.8. Noâ€”if one or more such rows were ever present, how could you possibly eliminate
all of them with row operations? You could eliminate all but one, but then there
is no way to eliminate the last remaining one, and hence it would have to appear
in the ï¬nal form.

Solutions
7
Solutions for exercises in section 2. 4
2.4.1. (a)
x2
ï£«
ï£¬
ï£­
âˆ’2
1
0
0
ï£¶
ï£·
ï£¸+ x4
ï£«
ï£¬
ï£­
âˆ’1
0
âˆ’1
1
ï£¶
ï£·
ï£¸
(b)
y
ï£«
ï£­
âˆ’1
2
1
0
ï£¶
ï£¸
(c)
x3
ï£«
ï£¬
ï£­
âˆ’1
âˆ’1
1
0
ï£¶
ï£·
ï£¸+ x4
ï£«
ï£¬
ï£­
âˆ’1
1
0
1
ï£¶
ï£·
ï£¸
(d) The trivial solution is the only solution.
2.4.2.
ï£«
ï£­
0
0
0
ï£¶
ï£¸
and
ï£«
ï£­
1
âˆ’1
2
0
ï£¶
ï£¸
2.4.3. x2
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’2
1
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£¸+ x4
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’2
0
âˆ’1
1
0
ï£¶
ï£·
ï£·
ï£·
ï£¸
2.4.4. rank (A) = 3
2.4.5. (a) 2â€”because the maximum rank is 4.
(b) 5â€”because the minimum rank is
1.
2.4.6. Because r = rank (A) â‰¤m < n
=â‡’
n âˆ’r > 0.
2.4.7. There are many diï¬€erent correct answers. One approach is to answer the question
â€œWhat must EA look like?â€ The form of the general solution tells you that
rank (A) = 2 and that the ï¬rst and third columns are basic. Consequently,
EA =
ï£«
ï£­
1
Î±
0
Î²
0
0
1
Î³
0
0
0
0
ï£¶
ï£¸so that x1 = âˆ’Î±x2 âˆ’Î²x4 and x3 = âˆ’Î³x4 gives rise
to the general solution x2
ï£«
ï£¬
ï£­
âˆ’Î±
1
0
0
ï£¶
ï£·
ï£¸+ x4
ï£«
ï£¬
ï£­
âˆ’Î²
0
âˆ’Î³
1
ï£¶
ï£·
ï£¸. Therefore, Î± = 2,
Î² = 3,
and Î³ = âˆ’2. Any matrix A obtained by performing row operations to EA
will be the coeï¬ƒcient matrix for a homogeneous system with the desired general
solution.
2.4.8. If 
i xfihi is the general solution, then there must exist scalars Î±i and Î²i such
that c1 = 
i Î±ihi and c2 = 
i Î²ihi. Therefore, c1 + c2 = 
i(Î±i + Î²i)hi,
and this shows that c1 + c2 is the solution obtained when the free variables xfi
assume the values xfi = Î±i + Î²i.
Solutions for exercises in section 2. 5
2.5.1. (a)
ï£«
ï£¬
ï£­
1
0
2
0
ï£¶
ï£·
ï£¸+ x2
ï£«
ï£¬
ï£­
âˆ’2
1
0
0
ï£¶
ï£·
ï£¸+ x4
ï£«
ï£¬
ï£­
âˆ’1
0
âˆ’1
1
ï£¶
ï£·
ï£¸
(b)
ï£«
ï£­
1
0
2
ï£¶
ï£¸+ y
ï£«
ï£­
âˆ’1
2
1
0
ï£¶
ï£¸

8
Solutions
(c)
ï£«
ï£¬
ï£­
2
âˆ’1
0
0
ï£¶
ï£·
ï£¸+ x3
ï£«
ï£¬
ï£­
âˆ’1
âˆ’1
1
0
ï£¶
ï£·
ï£¸+ x4
ï£«
ï£¬
ï£­
âˆ’1
1
0
1
ï£¶
ï£·
ï£¸
(d)
ï£«
ï£­
3
âˆ’3
âˆ’1
ï£¶
ï£¸
2.5.2. From Example 2.5.1, the solutions of the linear equations are:
x1 = 1 âˆ’x3 âˆ’2x4
x2 = 1 âˆ’x3
x3 is free
x4 is free
x5 = âˆ’1
Substitute these into the two constraints to get x3 = Â±1 and x4 = Â±1. Thus
there are exactly four solutions:
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’2
0
1
1
âˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£¸,
ï£«
ï£¬
ï£¬
ï£¬
ï£­
2
0
1
âˆ’1
âˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£¸,
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
2
âˆ’1
1
âˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£¸,
ï£«
ï£¬
ï£¬
ï£¬
ï£­
4
2
âˆ’1
âˆ’1
âˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£¸
ï£¼
ï£´
ï£´
ï£´
ï£½
ï£´
ï£´
ï£´
ï£¾
2.5.3. (a) {(3, 0, 4), (2, 1, 5), (1, 2, 6), (0, 3, 7)} See the solution to Exercise 2.3.7 for
the underlying system.
(b)
(3, 0, 4) costs $15 and is least expensive.
2.5.4. (a) Consistent for all Î±.
(b)
Î± Ì¸= 3, in which case the solution is (1, âˆ’1, 0).
(c)
Î± = 3, in which case the general solution is
ï£«
ï£­
1
âˆ’1
0
ï£¶
ï£¸+ z
ï£«
ï£­
0
âˆ’3
2
1
ï£¶
ï£¸.
2.5.5. No
2.5.6.
EA =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
Â· Â· Â·
0
0
1
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
1
0
0
Â· Â· Â·
0
...
...
Â· Â· Â·
...
0
0
Â· Â· Â·
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
mÃ—n
2.5.7. See the solution to Exercise 2.4.7.
2.5.8. (a)
ï£«
ï£­
âˆ’.3976
0
1
ï£¶
ï£¸+ y
ï£«
ï£­
âˆ’.7988
1
0
ï£¶
ï£¸
(b)
There are no solutions in this case.
(c)
ï£«
ï£­
1.43964
âˆ’2.3
1
ï£¶
ï£¸

Solutions
9
Solutions for exercises in section 2. 6
2.6.1. (a)
(1/575)(383, 533, 261, 644, âˆ’150, âˆ’111)
2.6.2. (1/211)(179, 452, 36)
2.6.3. (18, 10)
2.6.4. (a)
4
(b)
6
(c)
7 loops but only 3 simple loops.
(d)
Show that
rank ([A|b]) = 3
(g)
5/6

10
Solutions
I fear explanations explanatory of things explained.
â€” Abraham Lincoln (1809â€“1865)

Solutions for Chapter 3
Solutions for exercises in section 3. 2
3.2.1. (a)
X =

0
1
2
3

(b)
x = âˆ’1
2, y = âˆ’6, and z = 0
3.2.2. (a) Neither
(b) Skew symmetric
(c) Symmetric
(d) Neither
3.2.3. The 3 Ã— 3 zero matrix trivially satisï¬es all conditions, and it is the only pos-
sible answer for part (a). The only possible answers for (b) are real symmetric
matrices. There are many nontrivial possibilities for (c).
3.2.4. A = AT and B = BT
=â‡’
(A + B)T = AT + BT = A + B. Yesâ€”the
skew-symmetric matrices are also closed under matrix addition.
3.2.5. (a)
A = âˆ’AT
=â‡’
aij = âˆ’aji. If i = j, then ajj = âˆ’ajj
=â‡’
ajj = 0.
(b) A = âˆ’Aâˆ—
=â‡’
aij = âˆ’aji. If i = j, then ajj = âˆ’ajj. Write ajj = x+iy
to see that ajj = âˆ’ajj
=â‡’
x + iy = âˆ’x + iy
=â‡’
x = 0
=â‡’
ajj is pure
imaginary.
(c)
Bâˆ—= (iA)âˆ—= âˆ’iAâˆ—= âˆ’iA
T = âˆ’iAT = âˆ’iA = âˆ’B.
3.2.6. (a) Let S = A+AT and K = Aâˆ’AT . Then ST = AT +AT T = AT +A = S.
Likewise, KT = AT âˆ’AT T = AT âˆ’A = âˆ’K.
(b) A = S
2 + K
2 is one such decomposition. To see it is unique, suppose A = X+
Y, where X = XT and Y = âˆ’YT . Thus, AT = XT +YT = X âˆ’Y =â‡’A+
AT = 2X, so that X =
A+AT
2
=
S
2 . A similar argument shows that Y =
Aâˆ’AT
2
= K
2 .
3.2.7. (a)
[(A + B)âˆ—]ij = [A + B]ji = [A + B]ji = [A]ji + [B]ji = [Aâˆ—]ij + [Bâˆ—]ij =
[Aâˆ—+ Bâˆ—]ij
(b)
[(Î±A)âˆ—]ij = [Î±A]ji = [Â¯Î±A]ji = Â¯Î±[A]ji = Â¯Î±[Aâˆ—]ij
3.2.8. k
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
âˆ’1
0
Â· Â· Â·
0
0
âˆ’1
2
âˆ’1
Â· Â· Â·
0
0
0
âˆ’1
2
Â· Â· Â·
0
0
...
...
...
...
...
...
0
0
0
Â· Â· Â·
2
âˆ’1
0
0
0
Â· Â· Â·
âˆ’1
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
Solutions for exercises in section 3. 3
3.3.1. Functions (b) and (f) are linear. For example, to check if (b) is linear, let
A =

a1
a2

and B =

b1
b2

, and check if f(A + B) = f(A) + f(B) and

12
Solutions
f(Î±A) = Î±f(A). Do so by writing
f(A + B) = f

a1 + b1
a2 + b2

=

a2 + b2
a1 + b1

=

a2
a1

+

b2
b1

= f(A) + f(B),
f(Î±A) = f

Î±a1
Î±a2

=

Î±a2
Î±a1

= Î±

a2
a1

= Î±f(A).
3.3.2. Write f(x) = 
n
i=1 Î¾ixi. For all points x =
ï£«
ï£¬
ï£¬
ï£­
x1
x2
...
xn
ï£¶
ï£·
ï£·
ï£¸and y =
ï£«
ï£¬
ï£¬
ï£­
y1
y2
...
yn
ï£¶
ï£·
ï£·
ï£¸, and for
all scalars Î±, it is true that
f(Î±x + y) =
n

i=1
Î¾i(Î±xi + yi) =
n

i=1
Î¾iÎ±xi +
n

i=1
Î¾iyi
= Î±
n

i=1
Î¾ixi +
n

i=1
Î¾iyi = Î±f(x) + f(y).
3.3.3. There are many possibilities. Two of the simplest and most common are Hookeâ€™s
law for springs that says that F = kx (see Example 3.2.1) and Newtonâ€™s second
law that says that F = ma (i.e., force = mass Ã— acceleration).
3.3.4. They are all linear. To see that rotation is linear, use trigonometry to deduce
that if p =

x1
x2

, then f(p) = u =

u1
u2

, where
u1 = (cos Î¸)x1 âˆ’(sin Î¸)x2
u2 = (sin Î¸)x1 + (cos Î¸)x2.
f is linear because this is a special case of Example 3.3.2. To see that reï¬‚ection
is linear, write p =

x1
x2

and f(p) =

x1
âˆ’x2

. Veriï¬cation of linearity is
straightforward. For the projection function, use the Pythagorean theorem to
conclude that if p =

x1
x2

, then f(p) = x1+x2
2

1
1

. Linearity is now easily
veriï¬ed.

Solutions
13
Solutions for exercises in section 3. 4
3.4.1. Refer to the solution for Exercise 3.3.4. If Q, R, and P denote the matrices
associated with the rotation, reï¬‚ection, and projection, respectively, then
Q =

cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸

,
R =

1
0
0
âˆ’1

,
and
P =
 1
2
1
2
1
2
1
2

.
3.4.2. Refer to the solution for Exercise 3.4.1 and write
RQ =

1
0
0
âˆ’1
 
cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸

=

cos Î¸
âˆ’sin Î¸
âˆ’sin Î¸
âˆ’cos Î¸

.
If Q(x) is the rotation function and R(x) is the reï¬‚ection function, then the
composition is
R

Q(x)

=

(cos Î¸)x1 âˆ’(sin Î¸)x2
âˆ’(sin Î¸)x1 âˆ’(cos Î¸)x2

.
3.4.3. Refer to the solution for Exercise 3.4.1 and write
PQR =

a11x1 + a12x2
a21x1 + a22x2
 
cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸
 
1
0
0
âˆ’1

= 1
2

cos Î¸ + sin Î¸
sin Î¸ âˆ’cos Î¸
cos Î¸ + sin Î¸
sin Î¸ âˆ’cos Î¸

.
Therefore, the composition of the three functions in the order asked for is
P

Q

R(x)

= 1
2

(cos Î¸ + sin Î¸)x1 + (sin Î¸ âˆ’cos Î¸)x2
(cos Î¸ + sin Î¸)x1 + (sin Î¸ âˆ’cos Î¸)x2

.
Solutions for exercises in section 3. 5
3.5.1. (a) AB =
ï£«
ï£­
10
15
12
8
28
52
ï£¶
ï£¸
(b) BA does not exist
(c) CB does not exist
(d) CT B = ( 10
31 )
(e) A2 =
ï£«
ï£­
13
âˆ’1
19
16
13
12
36
âˆ’17
64
ï£¶
ï£¸
(f) B2 does not exist
(g) CT C = 14
(h) CCT =
ï£«
ï£­
1
2
3
2
4
6
3
6
9
ï£¶
ï£¸
(i) BBT =
ï£«
ï£­
5
8
17
8
16
28
17
28
58
ï£¶
ï£¸
(j) BT B =

10
23
23
69

(k) CT AC = 76

14
Solutions
3.5.2. (a)
A =
ï£«
ï£­
2
1
1
4
0
2
2
2
0
ï£¶
ï£¸, x =
ï£«
ï£­
x1
x2
x3
ï£¶
ï£¸, b =
ï£«
ï£­
3
10
âˆ’2
ï£¶
ï£¸
(b)
s =
ï£«
ï£­
1
âˆ’2
3
ï£¶
ï£¸
(c)
b = Aâˆ—1 âˆ’2Aâˆ—2 + 3Aâˆ—3
3.5.3. (a)
EA =
ï£«
ï£­
A1âˆ—
A2âˆ—
3A1âˆ—+ A3âˆ—
ï£¶
ï£¸
(b)
AE = ( Aâˆ—1 + 3Aâˆ—3
Aâˆ—2
Aâˆ—3 )
3.5.4. (a)
Aâˆ—j
(b)
Aiâˆ—
(c)
aij
3.5.5. Ax = Bx âˆ€x
=â‡’
Aej = Bej âˆ€ej
=â‡’
Aâˆ—j = Bâˆ—j âˆ€j
=â‡’
A = B.
(The symbol âˆ€is mathematical shorthand for the phrase â€œfor all.â€)
3.5.6. The limit is the zero matrix.
3.5.7. If A is m Ã— p and B is p Ã— n, write the product as
AB = ( Aâˆ—1
Aâˆ—2
Â· Â· Â·
Aâˆ—p )
ï£«
ï£¬
ï£¬
ï£­
B1âˆ—
B2âˆ—
...
Bpâˆ—
ï£¶
ï£·
ï£·
ï£¸= Aâˆ—1B1âˆ—+ Aâˆ—2B2âˆ—+ Â· Â· Â· + Aâˆ—pBpâˆ—
=
p

k=1
Aâˆ—kBkâˆ—.
3.5.8. (a)
[AB]ij = Aiâˆ—Bâˆ—j = ( 0
Â· Â· Â·
0
aii
Â· Â· Â·
ain )
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
b1j
...
bjj
0
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
is 0 when i > j.
(b)
When i = j, the only nonzero term in the product Aiâˆ—Bâˆ—i is aiibii.
(c) Yes.
3.5.9. Use [AB]ij = 
k aikbkj along with the rules of diï¬€erentiation to write
d[AB]ij
dt
= d (
k aikbkj)
dt
=

k
d(aikbkj)
dt
=

k
daik
dt bkj + aik
dbkj
dt

=

k
daik
dt bkj +

k
aik
dbkj
dt
=
dA
dt B

ij
+

AdB
dt

ij
=
dA
dt B + AdB
dt

ij
.
3.5.10. (a)
[Ce]i = the total number of paths leaving node i.
(b)
[eT C]i = the total number of paths entering node i.

Solutions
15
3.5.11. At time t, the concentration of salt in tank i is
xi(t)
V
lbs/gal. For tank 1,
dx1
dt = lbs
sec coming in âˆ’lbs
sec going out = 0lbs
sec âˆ’

r gal
sec Ã— x1(t)
V
lbs
gal

= âˆ’r
V x1(t)lbs
sec.
For tank 2,
dx2
dt = lbs
sec coming in âˆ’lbs
sec going out = r
V x1(t)lbs
sec âˆ’

r gal
sec Ã— x2(t)
V
lbs
gal

= r
V x1(t)lbs
sec âˆ’r
V x2(t)lbs
sec = r
V

x1(t) âˆ’x2(t)

,
and for tank 3,
dx3
dt = lbs
sec coming in âˆ’lbs
sec going out = r
V x2(t)lbs
sec âˆ’

r gal
sec Ã— x3(t)
V
lbs
gal

= r
V x2(t)lbs
sec âˆ’r
V x3(t)lbs
sec = r
V

x2(t) âˆ’x3(t)

.
This is a system of three linear ï¬rst-order diï¬€erential equations
dx1
dt
= r
V

âˆ’x1(t)

dx2
dt
= r
V

x1(t) âˆ’x2(t)

dx3
dt
= r
V

x2(t) âˆ’x3(t)

that can be written as a single matrix diï¬€erential equation
ï£«
ï£¬
ï£­
dx1/dt
dx2/dt
dx3/dt
ï£¶
ï£·
ï£¸= r
V
ï£«
ï£¬
ï£­
âˆ’1
0
0
1
âˆ’1
0
0
1
âˆ’1
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
x1(t)
x2(t)
x3(t)
ï£¶
ï£·
ï£¸.

16
Solutions
Solutions for exercises in section 3. 6
3.6.1.
AB =

A11
A12
A13
A21
A22
A23
 ï£«
ï£­
B1
B2
B3
ï£¶
ï£¸=

A11B1 + A12B2 + A13B3
A21B1 + A22B2 + A23B3

=
ï£«
ï£­
âˆ’10
âˆ’19
âˆ’10
âˆ’19
âˆ’1
âˆ’1
ï£¶
ï£¸
3.6.2. Use block multiplication to verify L2 = I â€”be careful not to commute any of
the terms when forming the various products.
3.6.3. Partition the matrix as A =

I
C
0
C

, where C = 1
3
ï£«
ï£­
1
1
1
1
1
1
1
1
1
ï£¶
ï£¸and observe
that C2 = C. Use this together with block multiplication to conclude that
Ak =

I
C + C2 + C3 + Â· Â· Â· + Ck
0
Ck

=

I
kC
0
C

.
Therefore, A300 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
0
100
100
100
0
1
0
100
100
100
0
0
1
100
100
100
0
0
0
1/3
1/3
1/3
0
0
0
1/3
1/3
1/3
0
0
0
1/3
1/3
1/3
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
3.6.4. (Aâˆ—A)âˆ—= Aâˆ—Aâˆ—âˆ—= Aâˆ—A and (AAâˆ—)âˆ—= Aâˆ—âˆ—Aâˆ—= AAâˆ—.
3.6.5. (AB)T = BT AT = BA = AB. It is easy to construct a 2 Ã— 2 example to show
that this need not be true when AB Ì¸= BA.
3.6.6.
[(D + E)F]ij = (D + E)iâˆ—Fâˆ—j =

k
[D + E]ik[F]kj =

k
([D]ik + [E]ik) [F]kj
=

k
([D]ik[F]kj + [E]ik[F]kj) =

k
[D]ik[F]kj +

k
[E]ik[F]kj
= Diâˆ—Fâˆ—j + Eiâˆ—Fâˆ—j = [DF]ij + [EF]ij
= [DF + EF]ij.
3.6.7. If a matrix X did indeed exist, then
I = AX âˆ’XA
=â‡’
trace (I) = trace (AX âˆ’XA)
=â‡’
n = trace (AX) âˆ’trace (XA) = 0,

Solutions
17
which is impossible.
3.6.8. (a)
yT A = bT
=â‡’
(yT A)
T = bT T
=â‡’
AT y = b. This is an n Ã— m
system of equations whose coeï¬ƒcient matrix is AT .
(b) They are the same.
3.6.9. Draw a transition diagram similar to that in Example 3.6.3 with North and South
replaced by ON and OFF, respectively. Let xk be the proportion of switches in
the ON state, and let yk be the proportion of switches in the OFF state after
k clock cycles have elapsed. According to the given information,
xk = xkâˆ’1(.1) + ykâˆ’1(.3)
yk = xkâˆ’1(.9) + ykâˆ’1(.7)
so that pk = pkâˆ’1P, where
pk = ( xk
yk )
and
P =

.1
.9
.3
.7

.
Just as in Example 3.6.3, pk = p0Pk. Compute a few powers of P to ï¬nd
P2 =

.280
.720
.240
.760

,
P3 =

.244
.756
.252
.748

P4 =

.251
.749
.250
.750

,
P5 =

.250
.750
.250
.750

and deduce that Pâˆ= limkâ†’âˆPk =

1/4
3/4
1/4
3/4

. Thus
pk â†’p0Pâˆ= ( 1
4(x0 + y0)
3
4(x0 + y0) ) = ( 1
4
3
4 ) .
For practical purposes, the device can be considered to be in equilibrium after
about 5 clock cyclesâ€”regardless of the initial proportions.
3.6.10. ( âˆ’4
1
âˆ’6
5 )
3.6.11. (a)
trace (ABC) = trace (A{BC}) = trace ({BC}A) = trace (BCA). The
other equality is similar.
(b) Use almost any set of 2 Ã— 2 matrices to con-
struct an example that shows equality need not hold.
(c) Use the fact that
trace

CT 	
= trace (C) for all square matrices to conclude that
trace

AT B
	
=trace

(AT B)
T 
= trace

BT AT T 
=trace

BT A
	
= trace

ABT 	
.
3.6.12. (a)
xT x = 0 â‡â‡’
n
k=1 x2
i = 0 â‡â‡’xi = 0 for each i â‡â‡’x = 0.
(b) trace

AT A
	
= 0 â‡â‡’

i
[AT A]ii = 0 â‡â‡’

i
(AT )iâˆ—Aâˆ—i = 0
â‡â‡’

i

k
[AT ]ik[A]ki = 0 â‡â‡’

i

k
[A]ki[A]ki = 0
â‡â‡’

i

k
[A]2
ki = 0
â‡â‡’[A]ki = 0 for each k and i â‡â‡’A = 0

18
Solutions
Solutions for exercises in section 3. 7
3.7.1. (a)

3
âˆ’2
âˆ’1
1

(b) Singular
(c)
ï£«
ï£­
2
âˆ’4
3
4
âˆ’7
4
5
âˆ’8
4
ï£¶
ï£¸
(d) Singular
(e)
ï£«
ï£¬
ï£­
2
âˆ’1
0
0
âˆ’1
2
âˆ’1
0
0
âˆ’1
2
âˆ’1
0
0
âˆ’1
1
ï£¶
ï£·
ï£¸
3.7.2. Write the equation as (I âˆ’A)X = B and compute
X = (I âˆ’A)âˆ’1B =
ï£«
ï£­
1
âˆ’1
1
0
1
âˆ’1
0
0
1
ï£¶
ï£¸
ï£«
ï£­
1
2
2
1
3
3
ï£¶
ï£¸=
ï£«
ï£­
2
4
âˆ’1
âˆ’2
3
3
ï£¶
ï£¸.
3.7.3. In each case, the given information implies that rank (A) < n â€”see the solution
for Exercise 2.1.3.
3.7.4. (a) If D is diagonal, then Dâˆ’1 exists if and only if each dii Ì¸= 0, in which case
ï£«
ï£¬
ï£¬
ï£­
d11
0
Â· Â· Â·
0
0
d22
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
dnn
ï£¶
ï£·
ï£·
ï£¸
âˆ’1
=
ï£«
ï£¬
ï£¬
ï£­
1/d11
0
Â· Â· Â·
0
0
1/d22
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
1/dnn
ï£¶
ï£·
ï£·
ï£¸.
(b)
If T is triangular, then Tâˆ’1 exists if and only if each tii Ì¸= 0. If T
is upper (lower) triangular, then Tâˆ’1 is also upper (lower) triangular with
[Tâˆ’1]ii = 1/tii.
3.7.5.

Aâˆ’1	T =

AT 	âˆ’1 = Aâˆ’1.
3.7.6. Start with A(I âˆ’A) = (I âˆ’A)A and apply (I âˆ’A)âˆ’1 to both sides, ï¬rst on
one side and then on the other.
3.7.7. Use the result of Example 3.6.5 that says that trace (AB) = trace (BA) to
write
m = trace (Im) = trace (AB) = trace (BA) = trace (In) = n.
3.7.8. Use the reverse order law for inversion to write

A(A + B)âˆ’1B
âˆ’1 = Bâˆ’1(A + B)Aâˆ’1 = Bâˆ’1 + Aâˆ’1
and

B(A + B)âˆ’1A
âˆ’1 = Aâˆ’1(A + B)Bâˆ’1 = Bâˆ’1 + Aâˆ’1.
3.7.9. (a)
(I âˆ’S)x = 0
=â‡’
xT (I âˆ’S)x = 0
=â‡’
xT x = xT Sx. Taking trans-
poses on both sides yields xT x = âˆ’xT Sx, so that xT x = 0, and thus x = 0

Solutions
19
(recall Exercise 3.6.12). The conclusion follows from property (3.7.8).
(b)
First notice that Exercise 3.7.6 implies that A = (I + S)(I âˆ’S)âˆ’1 =
(I âˆ’S)âˆ’1(I + S). By using the reverse order laws, transposing both sides yields
exactly the same thing as inverting both sides.
3.7.10. Use block multiplication to verify that the product of the matrix with its inverse
is the identity matrix.
3.7.11. Use block multiplication to verify that the product of the matrix with its inverse
is the identity matrix.
3.7.12. Let M =

A
B
C
D

and X =

DT
âˆ’BT
âˆ’CT
AT

. The hypothesis implies that
MX = I, and hence (from the discussion in Example 3.7.2) it must also be
true that XM = I, from which the conclusion follows. Note: This problem
appeared on a past Putnam Examâ€”a national mathematics competition for
undergraduate students that is considered to be quite challenging. This means
that you can be proud of yourself if you solved it before looking at this solution.
Solutions for exercises in section 3. 8
3.8.1. (a)
Bâˆ’1 =
ï£«
ï£­
1
2
âˆ’1
0
âˆ’1
1
1
4
âˆ’2
ï£¶
ï£¸
(b) Let c =
ï£«
ï£­
0
0
1
ï£¶
ï£¸and dT = ( 0
2
1 ) to obtain Câˆ’1 =
ï£«
ï£­
0
âˆ’2
1
1
3
âˆ’1
âˆ’1
âˆ’4
2
ï£¶
ï£¸
3.8.2. Aâˆ—j needs to be removed, and b needs to be inserted in its place. This is
accomplished by writing B = A+(bâˆ’Aâˆ—j)eT
j . Applying the Shermanâ€“Morrison
formula with c = b âˆ’Aâˆ—j and dT = eT
j
yields
Bâˆ’1 = Aâˆ’1 âˆ’Aâˆ’1(b âˆ’Aâˆ—j)eT
j Aâˆ’1
1 + eT
j Aâˆ’1(b âˆ’Aâˆ—j) = Aâˆ’1 âˆ’Aâˆ’1beT
j Aâˆ’1 âˆ’ejeT
j Aâˆ’1
1 + eT
j Aâˆ’1b âˆ’eT
j ej
= Aâˆ’1 âˆ’Aâˆ’1b[Aâˆ’1]jâˆ—âˆ’ej[Aâˆ’1]jâˆ—
[Aâˆ’1]jâˆ—b
= Aâˆ’1 âˆ’

Aâˆ’1b âˆ’ej
	
[Aâˆ’1]jâˆ—
[Aâˆ’1]jâˆ—b
.
3.8.3. Use the Shermanâ€“Morrison formula to write
z = (A + cdT )âˆ’1b =

Aâˆ’1 âˆ’Aâˆ’1cdT Aâˆ’1
1 + dT Aâˆ’1c

b = Aâˆ’1b âˆ’Aâˆ’1cdT Aâˆ’1b
1 + dT Aâˆ’1c
= x âˆ’
ydT x
1 + dT y.
3.8.4. (a)
For a nonsingular matrix A, the Shermanâ€“Morrison formula guarantees
that A + Î±eieT
j
is also nonsingular when 1 + Î±

Aâˆ’1
ji Ì¸= 0, and this certainly
will be true if Î± is suï¬ƒciently small.

20
Solutions
(b)
Write EmÃ—m = [Ïµij] = 
m
i,j=1 ÏµijeieT
j
and successively apply part (a) to
I + E =

I + Ïµ11e1eT
1
	
+ Ïµ12e1eT
2

+ Â· Â· Â· + ÏµmmemeT
m

to conclude that when the Ïµij â€™s are suï¬ƒciently small,
I + Ïµ11e1eT
1 ,

I + Ïµ11e1eT
1
	
+ Ïµ12e1eT
2

,
. . . ,
I + E
are each nonsingular.
3.8.5. Write A + ÏµB = A(I + Aâˆ’1ÏµB). You can either use the Neumann series result
(3.8.5) or Exercise 3.8.4 to conclude that (I + Aâˆ’1ÏµB) is nonsingular whenever
the entries of Aâˆ’1ÏµB are suï¬ƒciently small in magnitude, and this can be insured
by restricting Ïµ to a small enough interval about the origin. Since the product
of two nonsingular matrices is again nonsingularâ€”see (3.7.14)â€”it follows that
A + ÏµB = A(I + Aâˆ’1ÏµB) must be nonsingular.
3.8.6. Since

I
C
0
I
 
A
C
DT
âˆ’I
 
I
0
DT
I

=

A + CDT
0
0
âˆ’I

,
we can use R = DT and B = âˆ’I in part (a) of Exercise 3.7.11 to obtain

I
0
âˆ’DT
I
 
Aâˆ’1 + Aâˆ’1CSâˆ’1DT Aâˆ’1
âˆ’Aâˆ’1CSâˆ’1
âˆ’Sâˆ’1DT Aâˆ’1
Sâˆ’1
 
I
âˆ’C
0
I

=
 
A + CDT 	âˆ’1
0
0
âˆ’I

,
where S = âˆ’

I + DT Aâˆ’1C
	
. Comparing the upper-left-hand blocks produces

A + CDT 	âˆ’1 = Aâˆ’1 âˆ’Aâˆ’1C

I + DT Aâˆ’1C
	âˆ’1 DT Aâˆ’1.
3.8.7. The ranking from best to worst condition is A, B, C, because
Aâˆ’1 =
1
100
ï£«
ï£­
2
1
1
1
2
1
1
1
1
ï£¶
ï£¸
=â‡’
Îº(A) = 20 = 2 Ã— 101
Bâˆ’1 =
ï£«
ï£­
âˆ’1465
âˆ’161
17
173
19
âˆ’2
âˆ’82
âˆ’9
1
ï£¶
ï£¸
=â‡’
Îº(B) = 149, 513 â‰ˆ1.5 Ã— 105
Câˆ’1 =
ï£«
ï£­
âˆ’42659
39794
âˆ’948
2025
âˆ’1889
45
45
âˆ’42
1
ï£¶
ï£¸
=â‡’
Îº(C) = 82, 900, 594 â‰ˆ8.2 Ã— 107.
3.8.8. (a)
Diï¬€erentiate A(t)A(t)âˆ’1 = I with the product rule for diï¬€erentiation (re-
call Exercise 3.5.9).
(b)
Use the product rule for diï¬€erentiation together with part (a) to diï¬€eren-
tiate A(t)x(t) = b(t).

Solutions
21
Solutions for exercises in section 3. 9
3.9.1. (a) If G1, G2, . . . , Gk is the sequence of elementary matrices that corresponds
to the elementary row operations used in the reduction [A|I] âˆ’â†’[B|P], then
Gk Â· Â· Â· G2G1[A|I] = [B|P]
=â‡’
[Gk Â· Â· Â· G2G1A | Gk Â· Â· Â· G2G1I] = [B|P]
=â‡’
Gk Â· Â· Â· G2G1A = B
and
Gk Â· Â· Â· G2G1 = P.
(b) Use the same argument given above, but apply it on the right-hand side.
(c)
[A|I]
Gaussâ€“Jordan
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’[EA|P] yields
ï£«
ï£­
1
2
3
4
1
0
0
2
4
6
7
0
1
0
1
2
3
6
0
0
1
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
1
2
3
0
âˆ’7
4
0
0
0
0
1
2
âˆ’1
0
0
0
0
0
âˆ’5
2
1
ï£¶
ï£¸.
Thus P =
ï£«
ï£­
âˆ’7
4
0
2
âˆ’1
0
âˆ’5
2
1
ï£¶
ï£¸
is the product of the elementary matrices corre-
sponding to the operations used in the reduction, and PA = EA.
(d) You already have P such that PA = EA. Now ï¬nd Q such that EAQ =
Nr by column reducing EA. Proceed using part (b) to accumulate Q.
EA
I4

âˆ’â†’
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
2
3
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
âˆ’â†’
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
2
3
0
1
0
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
1
0
1
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
âˆ’â†’
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
0
0
0
1
0
0
0
0
0
0
1
0
âˆ’2
âˆ’3
0
0
1
0
0
0
0
1
0
1
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
3.9.2. (a)
Yesâ€”because rank (A) = rank (B).
(b)
Yesâ€”because EA = EB.
(c) Noâ€”because EAT Ì¸= EBT .
3.9.3. The positions of the basic columns in A correspond to those in EA. Because
A
row
âˆ¼B â‡â‡’EA = EB, it follows that the basic columns in A and B must
be in the same positions.
3.9.4. An elementary interchange matrix (a Type I matrix) has the form E = I âˆ’uuT ,
where u = ei âˆ’ej, and it follows from (3.9.1) that E = ET = Eâˆ’1. If
P = E1E2 Â· Â· Â· Ek is a product of elementary interchange matrices, then the re-
verse order laws yield
Pâˆ’1 = (E1E2 Â· Â· Â· Ek)âˆ’1 = Eâˆ’1
k
Â· Â· Â· Eâˆ’1
2 Eâˆ’1
1
= ET
k Â· Â· Â· ET
2 ET
1 = (E1E2 Â· Â· Â· Ek)T = PT .

22
Solutions
3.9.5. They are all true! A âˆ¼I âˆ¼Aâˆ’1 because rank (A) = n = rank

Aâˆ’1	
,
A
row
âˆ¼
Aâˆ’1 because PA = Aâˆ’1 with P =

Aâˆ’1	2 = Aâˆ’2,
and A
col
âˆ¼Aâˆ’1 because
AQ = Aâˆ’1 with Q = Aâˆ’2. The fact that A
row
âˆ¼I and A
col
âˆ¼I follows since
Aâˆ’1A = AAâˆ’1 = I.
3.9.6. (a), (c), (d), and (e) are true.
3.9.7. Rows i and j can be interchanged with the following sequence of Type II and
Type III operationsâ€”this is Exercise 1.2.12 on p. 14.
Rj â†Rj + Ri
(replace row j by the sum of row j and i)
Ri â†Ri âˆ’Rj
(replace row i by the diï¬€erence of row i and j)
Rj â†Rj + Ri
(replace row j by the sum of row j and i)
Ri â†âˆ’Ri
(replace row i by its negative)
Translating these to elementary matrices (remembering to build from the right
to the left) produces
(I âˆ’2eieT
i )(I + ejeT
i )(I âˆ’eieT
j )(I + ejeT
i ) = I âˆ’uuT ,
where
u = ei âˆ’ej.
3.9.8. Let BmÃ—r = [Aâˆ—b1Aâˆ—b2 Â· Â· Â· Aâˆ—br] contain the basic columns of A, and let CrÃ—n
contain the nonzero rows of EA. If Aâˆ—k is basicâ€”say Aâˆ—k = Aâˆ—bj â€”then
Câˆ—k = ej, and
(BC)âˆ—k = BCâˆ—k = Bej = Bâˆ—j = Aâˆ—bj = Aâˆ—k.
If Aâˆ—k is nonbasic, then Câˆ—k is nonbasic and has the form
Câˆ—k =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Âµ1
Âµ2
...
Âµj
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
= Âµ1
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
...
0
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
+ Âµ2
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
1
...
0
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
+ Â· Â· Â· + Âµj
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
0
...
1
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
= Âµ1e1 + Âµ2e2 + Â· Â· Â· + Âµjej,
where the ei â€™s are the basic columns to the left of Câˆ—k. Because A
row
âˆ¼EA,
the relationships that exist among the columns of A are exactly the same as
the relationships that exist among the columns of EA. In particular,
Aâˆ—k = Âµ1Aâˆ—b1 + Âµ2Aâˆ—b2 + Â· Â· Â· + ÂµjAâˆ—bj,
where the Aâˆ—bi â€™s are the basic columns to the left of Aâˆ—k. Therefore,
(BC)âˆ—k = BCâˆ—k = B (Âµ1e1 + Âµ2e2 + Â· Â· Â· + Âµjej)
= Âµ1Bâˆ—1 + Âµ2Bâˆ—2 + Â· Â· Â· + ÂµjBâˆ—j
= Âµ1Aâˆ—b1 + Âµ2Aâˆ—b2 + Â· Â· Â· + ÂµjAâˆ—bj
= Aâˆ—k.

Solutions
23
3.9.9. If A = uvT , where umÃ—1 and vnÃ—1 are nonzero columns, then
u
row
âˆ¼e1
and
vT col
âˆ¼eT
1
=â‡’
A = uvT âˆ¼e1eT
1 = N1
=â‡’
rank (A) = 1.
Conversely, if rank (A) = 1, then the existence of u and v follows from Exer-
cise 3.9.8. If you do not wish to rely on Exercise 3.9.8, write PAQ = N1 = e1eT
1 ,
where e1 is m Ã— 1 and eT
1 is 1 Ã— n so that
A = Pâˆ’1e1eT
1 Qâˆ’1 =

Pâˆ’1	
âˆ—1

Qâˆ’1	
1âˆ—= uvT .
3.9.10. Use Exercise 3.9.9 and write
A = uvT
=â‡’
A2 =

uvT 	 
uvT 	
= u

vT u
	
vT = Ï„uvT = Ï„A,
where Ï„ = vT u. Recall from Example 3.6.5 that trace (AB) = trace (BA),
and write
Ï„ = trace(Ï„) = trace

vT u
	
= trace

uvT 	
= trace (A).
Solutions for exercises in section 3. 10
3.10.1. (a)
L =
ï£«
ï£­
1
0
0
4
1
0
3
2
1
ï£¶
ï£¸and U =
ï£«
ï£­
1
4
5
0
2
6
0
0
3
ï£¶
ï£¸
(b)
x1 =
ï£«
ï£­
110
âˆ’36
8
ï£¶
ï£¸and
x2 =
ï£«
ï£­
112
âˆ’39
10
ï£¶
ï£¸
(c)
Aâˆ’1 = 1
6
ï£«
ï£­
124
âˆ’40
14
âˆ’42
15
âˆ’6
10
âˆ’4
2
ï£¶
ï£¸
3.10.2. (a)
The second pivot is zero.
(b)
P is the permutation matrix associated
with the permutation p = ( 2
4
1
3 ) . P is constructed by permuting the
rows of I in this manner.
L =
ï£«
ï£¬
ï£­
1
0
0
0
0
1
0
0
1/3
0
1
0
2/3
âˆ’1/2
1/2
1
ï£¶
ï£·
ï£¸
and
U =
ï£«
ï£¬
ï£­
3
6
âˆ’12
3
0
2
âˆ’2
6
0
0
8
16
0
0
0
âˆ’5
ï£¶
ï£·
ï£¸
(c)
x =
ï£«
ï£¬
ï£­
2
âˆ’1
0
1
ï£¶
ï£·
ï£¸

24
Solutions
3.10.3. Î¾ = 0, Â±
âˆš
2, Â±
âˆš
3
3.10.4. A possesses an LU factorization if and only if all leading principal submatrices
are nonsingular. The argument associated with equation (3.10.13) proves that

Lk
0
cT Uâˆ’1
k
1
 
Uk
Lâˆ’1
k b
0
ak+1,k+1 âˆ’cT Aâˆ’1
k b

= Lk+1Uk+1
is the LU factorization for Ak+1. The desired conclusion follows from the fact
that the k + 1th pivot is the (k + 1, k + 1) -entry in Uk+1. This pivot must be
nonzero because Uk+1 is nonsingular.
3.10.5. If L and U are both triangular with 1â€™s on the diagonal, then Lâˆ’1 and Uâˆ’1
contain only integer entries, and consequently Aâˆ’1 = Uâˆ’1Lâˆ’1 is an integer
matrix.
3.10.6. (b)
L =
ï£«
ï£¬
ï£­
1
0
0
0
âˆ’1/2
1
0
0
0
âˆ’2/3
1
0
0
0
âˆ’3/4
1
ï£¶
ï£·
ï£¸and U =
ï£«
ï£¬
ï£­
2
âˆ’1
0
0
0
3/2
âˆ’1
0
0
0
4/3
âˆ’1
0
0
0
1/4
ï£¶
ï£·
ï£¸
3.10.7. Observe how the LU factors evolve from Gaussian elimination. Following the
procedure described in Example 3.10.1 where multipliers â„“ij are stored in the
positions they annihilate (i.e., in the (i, j) -position), and where â‹†â€™s are put
in positions that can be nonzero, the reduction of a 5 Ã— 5 band matrix with
bandwidth w = 2 proceeds as shown below.
ï£«
ï£¬
ï£¬
ï£¬
ï£­
â‹†
â‹†
â‹†
0
0
â‹†
â‹†
â‹†
â‹†
0
â‹†
â‹†
â‹†
â‹†
â‹†
0
â‹†
â‹†
â‹†
â‹†
0
0
â‹†
â‹†
â‹†
ï£¶
ï£·
ï£·
ï£·
ï£¸âˆ’â†’
ï£«
ï£¬
ï£¬
ï£¬
ï£­
â‹†
â‹†
â‹†
0
0
l21
â‹†
â‹†
â‹†
0
l31
â‹†
â‹†
â‹†
â‹†
0
â‹†
â‹†
â‹†
â‹†
0
0
â‹†
â‹†
â‹†
ï£¶
ï£·
ï£·
ï£·
ï£¸âˆ’â†’
ï£«
ï£¬
ï£¬
ï£¬
ï£­
â‹†
â‹†
â‹†
0
0
l21
â‹†
â‹†
â‹†
0
l31
l32
â‹†
â‹†
â‹†
0
l42
â‹†
â‹†
â‹†
0
0
â‹†
â‹†
â‹†
ï£¶
ï£·
ï£·
ï£·
ï£¸
âˆ’â†’
ï£«
ï£¬
ï£¬
ï£¬
ï£­
â‹†
â‹†
â‹†
0
0
l21
â‹†
â‹†
â‹†
0
l31
l32
â‹†
â‹†
â‹†
0
l42
l43
â‹†
â‹†
0
0
l53
â‹†
â‹†
ï£¶
ï£·
ï£·
ï£·
ï£¸âˆ’â†’
ï£«
ï£¬
ï£¬
ï£¬
ï£­
â‹†
â‹†
â‹†
0
0
l21
â‹†
â‹†
â‹†
0
l31
l32
â‹†
â‹†
â‹†
0
l42
l43
â‹†
â‹†
0
0
l53
l54
â‹†
ï£¶
ï£·
ï£·
ï£·
ï£¸
Thus L =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1
0
0
0
0
l21
1
0
0
0
l31
l32
1
0
0
0
l42
l43
1
0
0
0
l53
l54
1
ï£¶
ï£·
ï£·
ï£·
ï£¸and U =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
â‹†
â‹†
â‹†
0
0
0
â‹†
â‹†
â‹†
0
0
0
â‹†
â‹†
â‹†
0
0
0
â‹†
â‹†
0
0
0
0
â‹†
ï£¶
ï£·
ï£·
ï£·
ï£¸.
3.10.8. (a)
A =

0
1
1
0

(b)
A =

1
0
0
âˆ’1

3.10.9. (a)
L =
ï£«
ï£­
1
0
0
4
1
0
3
2
1
ï£¶
ï£¸,
D =
ï£«
ï£­
1
0
0
0
2
0
0
0
3
ï£¶
ï£¸, and U =
ï£«
ï£­
1
4
5
0
1
3
0
0
1
ï£¶
ï£¸

Solutions
25
(b)
Use the same argument given for the uniqueness of the LU factorization
with minor modiï¬cations.
(c)
A = AT
=â‡’
LDU = UT DT LT = UT DLT . These are each LDU
factorizations for A, and consequently the uniqueness of the LDU factorization
means that U = LT .
3.10.10. A is symmetric with pivots 1, 4, 9. The Cholesky factor is R =
ï£«
ï£­
1
0
0
2
2
0
3
3
3
ï£¶
ï£¸.

26
Solutions
It is unworthy of excellent men to lose hours
like slaves in the labor of calculations.
â€” Baron Gottfried Wilhelm von Leibnitz (1646â€“1716)

Solutions for Chapter 4
Solutions for exercises in section 4. 1
4.1.1. Only (b) and (d) are subspaces.
4.1.2. (a), (b), (f), (g), and (i) are subspaces.
4.1.3. All of â„œ3.
4.1.4. If v âˆˆV is a nonzero vector in a space V, then all scalar multiples Î±v must
also be in V.
4.1.5. (a) A line.
(b) The (x,y)-plane.
(c)
â„œ3
4.1.6. Only (c) and (e) span â„œ3. To see that (d) does not span â„œ3, ask whether
or not every vector (x, y, z) âˆˆâ„œ3 can be written as a linear combination of
the vectors in (d). Itâ€™s convenient to think in terms columns, so rephrase the
question by asking if every b =
ï£«
ï£­
x
y
z
ï£¶
ï£¸can be written as a linear combination
of
ï£±
ï£²
ï£³v1 =
ï£«
ï£­
1
2
1
ï£¶
ï£¸, v2 =
ï£«
ï£­
2
0
âˆ’1
ï£¶
ï£¸, v3 =
ï£«
ï£­
4
4
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾. That is, for each b âˆˆâ„œ3, are
there scalars Î±1, Î±2, Î±3 such that Î±1v1 + Î±2v2 + Î±3v3 = b or, equivalently, is
ï£«
ï£­
1
2
4
2
0
4
1
âˆ’1
1
ï£¶
ï£¸
ï£«
ï£­
Î±1
Î±2
Î±3
ï£¶
ï£¸=
ï£«
ï£­
x
y
z
ï£¶
ï£¸
consistent for all
ï£«
ï£­
x
y
z
ï£¶
ï£¸?
This is a system of the form Ax = b, and it is consistent for all b if and only
if rank ([A|b]) = rank (A) for all b. Since
ï£«
ï£­
1
2
4
x
2
0
4
y
1
âˆ’1
1
z
ï£¶
ï£¸â†’
ï£«
ï£­
1
2
4
x
0
âˆ’4
âˆ’4
y âˆ’2x
0
âˆ’3
âˆ’3
z âˆ’x
ï£¶
ï£¸
â†’
ï£«
ï£­
1
2
4
x
0
âˆ’4
âˆ’4
y âˆ’2x
0
0
0
(x/2) âˆ’(3y/4) + z
ï£¶
ï£¸,
itâ€™s clear that there exist b â€™s (e.g., b = (1, 0, 0)T ) for which Ax = b is not
consistent, and hence not all b â€™s are a combination of the vi â€™s. Therefore, the
vi â€™s donâ€™t span â„œ3.
4.1.7. This follows from (4.1.2).

28
Solutions
4.1.8. (a)
u, v âˆˆX âˆ©Y
=â‡’
u, v âˆˆX and u, v âˆˆY. Because X and Y are
closed with respect to addition, it follows that u + v âˆˆX and u + v âˆˆY,
and therefore u + v âˆˆX âˆ©Y. Because X and Y are both closed with respect
to scalar multiplication, we have that Î±u âˆˆX and Î±u âˆˆY for all Î±, and
consequently Î±u âˆˆX âˆ©Y for all Î±.
(b) The union of two diï¬€erent lines through the origin in â„œ2 is not a subspace.
4.1.9. (a)
(A1) holds because x1, x2 âˆˆA(S)
=â‡’
x1 = As1 and x2 = As2 for
some s1, s2 âˆˆS
=â‡’
x1 + x2 = A(s1 + s2). Since S is a subspace, it is
closed under vector addition, so s1 + s2 âˆˆS. Therefore, x1 + x2 is the image of
something in S â€”namely, s1+s2 â€”and this means that x1+x2 âˆˆA(S). To see
that (M1) holds, consider Î±x, where Î± is an arbitrary scalar and x âˆˆA(S).
Now, x âˆˆA(S)
=â‡’
x = As for some s âˆˆS
=â‡’
Î±x = Î±As = A(Î±s).
Since S is a subspace, we are guaranteed that Î±s âˆˆS, and therefore Î±x is the
image of something in S. This is what it means to say Î±x âˆˆA(S).
(b)
Prove equality by demonstrating that span {As1, As2, . . . , Ask} âŠ†A(S)
and A(S) âŠ†span {As1, As2, . . . , Ask} . To show span {As1, As2, . . . , Ask} âŠ†
A(S), write
x âˆˆspan {As1, As2, . . . , Ask}
=â‡’
x =
k

i=1
Î±i(Asi) = A
 k

i=1
Î±isi

âˆˆA(S).
Inclusion in the reverse direction is established by saying
x âˆˆA(S)
=â‡’
x = As for some s âˆˆS
=â‡’
s =
k

i=1
Î²isi
=â‡’x = A
 k

i=1
Î²isi

=
k

i=1
Î²i(Asi) âˆˆspan {As1, As2, . . . , Ask} .
4.1.10. (a) Yes, all of the deï¬ning properties are satisï¬ed.
(b) Yes, this is essentially â„œ2.
(c) No, it is not closed with respect to scalar multiplication.
4.1.11. If span (M) = span (N) , then every vector in N must be a linear combination
of vectors from M. In particular, v must be a linear combination of the mi â€™s,
and hence v âˆˆspan (M) . To prove the converse, ï¬rst notice that span (M) âŠ†
span (N) . The desired conclusion will follow if it can be demonstrated that
span (M) âŠ‡span (N) . The hypothesis that v âˆˆspan (M) guarantees that
v = 
r
i=1 Î²imi. If z âˆˆspan (N) , then
z =
r

i=1
Î±imi + Î±r+1v =
r

i=1
Î±imi + Î±r+1
r

i=1
Î²imi
=
r

i=1

Î±i + Î±r+1Î²i

mi,

Solutions
29
which shows z âˆˆspan (M) , and therefore span (M) âŠ‡span (N) .
4.1.12. To show span (S) âŠ†M, observe that x âˆˆspan (S)
=â‡’
x = 
i Î±ivi.
If V is any subspace containing S, then 
i Î±ivi âˆˆV because V is closed
under addition and scalar multiplication, and therefore x âˆˆM. The fact that
M âŠ†span (S) follows because if x âˆˆM, then x âˆˆspan (S) because span (S)
is one particular subspace that contains S.
Solutions for exercises in section 4. 2
4.2.1. R (A) = span
ï£±
ï£²
ï£³
ï£«
ï£­
1
âˆ’2
1
ï£¶
ï£¸,
ï£«
ï£­
1
0
2
ï£¶
ï£¸
ï£¼
ï£½
ï£¾,
N

AT 	
= span
ï£±
ï£²
ï£³
ï£«
ï£­
4
1
âˆ’2
ï£¶
ï£¸
ï£¼
ï£½
ï£¾,
N (A) = span
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’2
1
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£¸,
ï£«
ï£¬
ï£¬
ï£¬
ï£­
2
0
âˆ’3
1
0
ï£¶
ï£·
ï£·
ï£·
ï£¸,
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’1
0
âˆ’4
0
1
ï£¶
ï£·
ï£·
ï£·
ï£¸
ï£¼
ï£´
ï£´
ï£´
ï£½
ï£´
ï£´
ï£´
ï£¾
,
R

AT 	
= span
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1
2
0
âˆ’2
1
ï£¶
ï£·
ï£·
ï£·
ï£¸,
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
0
1
3
4
ï£¶
ï£·
ï£·
ï£·
ï£¸
ï£¼
ï£´
ï£´
ï£´
ï£½
ï£´
ï£´
ï£´
ï£¾
.
4.2.2. (a) This is simply a restatement of equation (4.2.3).
(b)
Ax = b has a unique solution if and only if rank (A) = n (i.e., there are
no free variablesâ€”see Â§2.5), and (4.2.10) says rank (A) = n â‡â‡’N (A) = {0}.
4.2.3. (a)
It is consistent because b âˆˆR (A).
(b)
It is nonunique because N (A) Ì¸= {0} â€”see Exercise 4.2.2.
4.2.4. Yes, because rank[A|b] = rank (A) = 3
=â‡’
âˆƒx such that Ax = b â€”i.e.,
Ax = b is consistent.
4.2.5. (a)
If R (A) = â„œn, then
R (A) = R (In)
=â‡’
A
col
âˆ¼In
=â‡’
rank (A) = rank (In) = n.
(b)
R (A) = R

AT 	
= â„œn and N (A) = N

AT 	
= {0}.
4.2.6. EA Ì¸= EB means that R

AT 	
Ì¸= R

BT 	
and N (A) Ì¸= N (B). However,
EAT = EBT implies that R (A) = R (B) and N

AT 	
= N

BT 	
.
4.2.7. Demonstrate that rank (AnÃ—n) = n by using (4.2.10). If x âˆˆN (A), then
Ax = 0
=â‡’
A1x = 0
and
A2x = 0
=â‡’
x âˆˆN (A1) = R

AT
2
	
=â‡’
âˆƒyT such that xT = yT A2
=â‡’
xT x = yT A2x = 0
=â‡’

i
x2
i = 0
=â‡’
x = 0.

30
Solutions
4.2.8. yT b = 0 âˆ€y âˆˆN

AT 	
= R

PT
2
	
=â‡’
P2b = 0
=â‡’
b âˆˆN (P2) = R (A)
4.2.9. x âˆˆR

A | B
	
â‡â‡’âˆƒy such that x =

A | B
	
y =

A | B
	 
y1
y2

= Ay1 +
By2 â‡â‡’x âˆˆR (A) + R (B)
4.2.10. (a) p+N (A) is the set of all possible solutions to Ax = b. Recall from (2.5.7)
that the general solution of a nonhomogeneous equation is a particular solution
plus the general solution of the homogeneous equation Ax = 0. The general
solution of the homogeneous equation is simply a way of describing all possible
solutions of Ax = 0, which is N (A).
(b)
rank (A3Ã—3) = 1 means that N (A) is spanned by two vectors, and hence
N (A) is a plane through the origin. From the parallelogram law, p + N (A) is
a plane parallel to N (A) passing through the point deï¬ned by p.
(c) This time N (A) is spanned by a single vector, and p + N (A) is a line
parallel to N (A) passing through the point deï¬ned by p.
4.2.11. a âˆˆR

AT 	
â‡â‡’âˆƒy such that aT = yT A. If Ax = b, then
aT x = yT Ax = yT b,
which is independent of x.
4.2.12. (a)
b âˆˆR (AB)
=â‡’
âˆƒx such that b = ABx = A(Bx)
=â‡’
b âˆˆR (A)
because b is the image of Bx.
(b)
x âˆˆN (B)
=â‡’
Bx = 0
=â‡’
ABx = 0
=â‡’
x âˆˆN (AB).
4.2.13. Given any z âˆˆR (AB), the object is to show that z can be written as some
linear combination of the Abi â€™s. Argue as follows. z âˆˆR (AB)
=â‡’
z = ABy
for some y. But it is always true that By âˆˆR (B), so
By = Î±1b1 + Î±2b2 + Â· Â· Â· + Î±nbn,
and therefore z = ABy = Î±1Ab1 + Î±2Ab2 + Â· Â· Â· + Î±nAbn.
Solutions for exercises in section 4. 3
4.3.1. (a) and (b) are linearly dependentâ€”all others are linearly independent. To write
one vector as a combination of others in a dependent set, place the vectors as
columns in A and ï¬nd EA. This reveals the dependence relationships among
columns of A.
4.3.2. (a) According to (4.3.12), the basic columns in A always constitute one maximal
linearly independent subset.
(b) Tenâ€”5 sets using two vectors, 4 sets using one vector, and the empty set.
4.3.3. rank (H) â‰¤3, and according to (4.3.11), rank (H) is the maximal number of
independent rows in H.

Solutions
31
4.3.4. The question is really whether or not the columns in
Ë†A =
ï£«
ï£¬
ï£¬
ï£­
S
L
F
#1
1
1
1
10
#2
1
2
1
12
#3
1
2
2
15
#4
1
3
2
17
ï£¶
ï£·
ï£·
ï£¸
are linearly independent. Reducing Ë†A to E Ë†
A shows that 5 + 2S + 3L âˆ’F = 0.
4.3.5. (a) This follows directly from the deï¬nition of linear dependence because there
are nonzero values of Î± such that Î±0 = 0.
(b) This is a consequence of (4.3.13).
4.3.6. If each tii Ì¸= 0, then T is nonsingular, and the result follows from (4.3.6) and
(4.3.7).
4.3.7. It is linearly independent because
Î±1

1
0
0
0

+ Î±2

1
1
0
0

+ Î±3

1
1
1
0

+ Î±4

1
1
1
1

=

0
0
0
0

â‡â‡’Î±1
ï£«
ï£¬
ï£­
1
0
0
0
ï£¶
ï£·
ï£¸+ Î±2
ï£«
ï£¬
ï£­
1
1
0
0
ï£¶
ï£·
ï£¸+ Î±3
ï£«
ï£¬
ï£­
1
1
1
0
ï£¶
ï£·
ï£¸+ Î±4
ï£«
ï£¬
ï£­
1
1
1
1
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
0
0
0
0
ï£¶
ï£·
ï£¸
â‡â‡’
ï£«
ï£¬
ï£­
1
1
1
1
0
1
1
1
0
0
1
1
0
0
0
1
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
Î±1
Î±2
Î±3
Î±4
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
0
0
0
0
ï£¶
ï£·
ï£¸â‡â‡’
ï£«
ï£¬
ï£­
Î±1
Î±2
Î±3
Î±4
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
0
0
0
0
ï£¶
ï£·
ï£¸.
4.3.8. A is nonsingular because it is diagonally dominant.
4.3.9. S is linearly independent using exact arithmetic, but using 3-digit arithmetic
yields the conclusion that S is dependent.
4.3.10. If e is the column vector of all 1â€™s, then Ae = 0, so that N (A) Ì¸= {0}.
4.3.11. (Solution 1.)

i Î±iPui = 0
=â‡’
P 
i Î±iui = 0
=â‡’

i Î±iui =
0
=â‡’
each Î±i = 0 because the ui â€™s are linearly independent.
(Solution 2.)
If AmÃ—n is the matrix containing the ui â€™s as columns, then
PA = B is the matrix containing the vectors in P(S) as its columns. Now,
A
row
âˆ¼B
=â‡’
rank (B) = rank (A) = n,
and hence (4.3.3) insures that the columns of B are linearly independent. The
result need not be true if P is singularâ€”take P = 0 for example.
4.3.12. If AmÃ—n is the matrix containing the ui â€™s as columns, and if
QnÃ—n =
ï£«
ï£¬
ï£¬
ï£­
1
1
Â· Â· Â·
1
0
1
Â· Â· Â·
1
...
...
...
...
0
0
Â· Â· Â·
1
ï£¶
ï£·
ï£·
ï£¸,

32
Solutions
then the columns of B = AQ are the vectors in Sâ€². Clearly, Q is nonsingular
so that A
col
âˆ¼B, and thus rank (A) = rank (B). The desired result now follows
from (4.3.3).
4.3.13. (a) and (b) are linearly independent because the Wronski matrix W(0) is non-
singular in each case. (c) is dependent because sin2 x âˆ’cos2 x + cos 2x = 0.
4.3.14. If S were dependent, then there would exist a constant Î± such that x3 = Î±|x|3
for all values of x. But this would mean that
Î± = x3
|x|3 =

1
if x > 0,
âˆ’1
if x < 0,
which is clearly impossible since Î± must be constant. The associated Wronski
matrix is
W(x) =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³

x3
x3
3x2
3x2

when x â‰¥0,

x3
âˆ’x3
3x2
âˆ’3x2

when x < 0,
which is singular for all values of x.
4.3.15. Start with the fact that
AT diag. dom.
=â‡’
|bii| > |di| +

jÌ¸=i
|bji|
and
|Î±| >

j
|cj|
=â‡’

jÌ¸=i
|bji| < |bii| âˆ’|di|
and
1
|Î±|

jÌ¸=i
|cj| < 1 âˆ’|ci|
|Î±| ,
and then use the forward and backward triangle inequality to write

jÌ¸=i
|xij| =

jÌ¸=i
bji âˆ’dicj
Î±
 â‰¤

jÌ¸=i
|bji| + |di|
|Î±|

jÌ¸=i
|cj|
<

|bii| âˆ’|di|
	
+ |di|

1 âˆ’|ci|
|Î±|

= |bii| âˆ’|di| |ci|
|Î±|
â‰¤
bii âˆ’dici
Î±
 = |xii|.
Now, diagonal dominance of AT insures that Î± is the entry of largest magni-
tude in the ï¬rst column of A, so no row interchange is needed at the ï¬rst step
of Gaussian elimination. After one step, the diagonal dominance of X guar-
antees that the magnitude of the second pivot is maximal with respect to row
interchanges. Proceeding by induction establishes that no step requires partial
pivoting.

Solutions
33
Solutions for exercises in section 4. 4
4.4.1. dim R (A) = dim R

AT 	
= rank (A) = 2,
dim N (A) = n âˆ’r = 4 âˆ’2 = 2,
and dim N

AT 	
= m âˆ’r = 3 âˆ’2 = 1.
4.4.2. BR(A) =
ï£±
ï£²
ï£³
ï£«
ï£­
1
3
2
ï£¶
ï£¸,
ï£«
ï£­
0
1
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾,
BN(AT ) =
ï£±
ï£²
ï£³
ï£«
ï£­
1
âˆ’1
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
BR(AT ) =
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1
2
0
2
1
ï£¶
ï£·
ï£·
ï£·
ï£¸,
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
0
1
3
3
ï£¶
ï£·
ï£·
ï£·
ï£¸
ï£¼
ï£´
ï£´
ï£´
ï£½
ï£´
ï£´
ï£´
ï£¾
,
BN(A) =
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’2
1
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£¸,
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’2
0
âˆ’3
1
0
ï£¶
ï£·
ï£·
ï£·
ï£¸,
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’1
0
âˆ’3
0
1
ï£¶
ï£·
ï£·
ï£·
ï£¸
ï£¼
ï£´
ï£´
ï£´
ï£½
ï£´
ï£´
ï£´
ï£¾
4.4.3. dim

span (S)
	
= 3
4.4.4. (a)
n + 1 (See Example 4.4.1)
(b)
mn
(c)
n2+n
2
4.4.5. Use the technique of Example 4.4.5. Find EA to determine
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³
h1 =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’2
1
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£¸, h2 =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’2
0
1
1
0
ï£¶
ï£·
ï£·
ï£·
ï£¸, h3 =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’1
0
âˆ’2
0
1
ï£¶
ï£·
ï£·
ï£·
ï£¸
ï£¼
ï£´
ï£´
ï£´
ï£½
ï£´
ï£´
ï£´
ï£¾
is a basis for N (A). Reducing the matrix

v, h1, h2, h3

to row echelon
form reveals that its ï¬rst, second, and fourth columns are basic, and hence
{v, h1, h3} is a basis for N (A) that contains v.
4.4.6. Placing the vectors from A and B as rows in matrices and reducing
A =
ï£«
ï£­
1
2
3
5
8
7
3
4
1
ï£¶
ï£¸âˆ’â†’EA =
ï£«
ï£­
1
0
âˆ’5
0
1
4
0
0
0
ï£¶
ï£¸
and
B =

2
3
2
1
1
âˆ’1

âˆ’â†’EB =

1
0
âˆ’5
0
1
4

shows A and B have the same row space (recall Example 4.2.2), and hence A
and B span the same space. Because B is linearly independent, it follows that
B is a basis for span (A) .
4.4.7. 3 = dim N (A) = n âˆ’r = 4 âˆ’rank (A)
=â‡’
rank (A) = 1. Therefore, any
rank-one matrix with no zero entries will do the job.
4.4.8. If v = Î±1b1 + Î±2b2 + Â· Â· Â· + Î±nbn and v = Î²1b1 + Î²2b2 + Â· Â· Â· + Î²nbn, then
subtraction produces
0 = (Î±1 âˆ’Î²1)b1 + (Î±2 âˆ’Î²2)b2 + Â· Â· Â· + (Î±n âˆ’Î²n)bn.

34
Solutions
But B is a linearly independent set, so this equality can hold only if (Î±iâˆ’Î²i) = 0
for each i = 1, 2, . . . , n, and hence the Î±i â€™s are unique.
4.4.9. Prove that if {s1, s2, . . . , sk} is a basis for S, then {As1, As2, . . . , Ask} is a
basis for A(S). The result of Exercise 4.1.9 insures that
span {As1, As2, . . . , Ask} = A(S),
so we need only establish the independence of {As1, As2, . . . , Ask}. To do this,
write
k

i=1
Î±i (Asi) = 0
=â‡’
A
 k

i=1
Î±isi

= 0
=â‡’
k

i=1
Î±isi âˆˆN (A)
=â‡’
k

i=1
Î±isi = 0
because
S âˆ©N (A) = 0
=â‡’
Î±1 = Î±2 = Â· Â· Â· = Î±k = 0
because {s1, s2, . . . , sk} is linearly independent. Since {As1, As2, . . . , Ask} is
a basis for A(S), it follows that dim A(S) = k = dim(S).
4.4.10. rank (A) = rank (A âˆ’B + B) â‰¤rank (A âˆ’B) + rank (B) implies that
rank (A) âˆ’rank (B) â‰¤rank (A âˆ’B).
Furthermore, rank (B) = rank (B âˆ’A + A) â‰¤rank (B âˆ’A) + rank (A) =
rank (A âˆ’B) + rank (A) implies that
âˆ’

rank (A) âˆ’rank (B)

â‰¤rank (A âˆ’B).
4.4.11. Example 4.4.8 guarantees that rank (A + E) â‰¤rank (A) + rank (E) = r + k.
Use Exercise 4.4.10 to write
rank (A + E) = rank (A âˆ’(âˆ’E)) â‰¥rank (A) âˆ’rank (âˆ’E) = r âˆ’k.
4.4.12. Let v1 âˆˆV such that v1 Ì¸= 0. If span {v1} = V, then S1 = {v1} is an
independent spanning set for V, and we are ï¬nished. If span {v1} Ì¸= V, then
there is a vector
v2 âˆˆV such that v2 /âˆˆspan {v1} , and hence the extension set
S2 = {v1, v2} is independent. If span (S2) = V, then we are ï¬nished. Otherwise,
we can proceed as described in Example 4.4.5 and continue to build independent
extension sets S3, S4, . . . . Statement (4.3.16) guarantees that the process must
eventually yield a linearly independent spanning set Sk with k â‰¤n.
4.4.13. Since 0 = eT E = E1âˆ—+E2âˆ—+Â· Â· Â·+Emâˆ—, any row can be written as a combination
of the other m âˆ’1 rows, so any set of m âˆ’1 rows from E spans N

ET 	
.
Furthermore, rank (E) = m âˆ’1 insures that no fewer than m âˆ’1 vectors

Solutions
35
can span N

ET 	
, and therefore any set of m âˆ’1 rows from E is a minimal
spanning set, and hence a basis.
4.4.14.

EET 
ij = Eiâˆ—

ET 	
âˆ—j = Eiâˆ—(Ejâˆ—)T = 
k eikejk. Observe that edge Ek
touches node Ni if and only if eik = Â±1 or, equivalently, e2
ik = 1. Thus

EET 
ii = 
k e2
ik = the number of edges touching Ni. If i Ì¸= j, then
eikejk =
 âˆ’1
if Ek is between Ni and Nj
0
if Ek is not between Ni and Nj
so that

EET 
ij = 
k eikejk = âˆ’(the number of edges between Ni and Nj ).
4.4.15. Apply (4.4.19) to span (M âˆªN) = span (M) + span (N) (see Exercise 4.1.7).
4.4.16. (a) Exercise 4.2.9 says R (A | B) = R (A) + R (B). Since rank is the same as
dimension of the range, (4.4.19) yields
rank (A | B) = dim R (A | B) = dim

R (A) + R (B)

= dim R (A) + dim R (B) âˆ’dim

R (A) âˆ©R (B)

= rank (A) + rank (B) âˆ’dim

R (A) âˆ©R (B)

.
(b) Use the results of part (a) to write
dim N (A | B) = n + k âˆ’rank (A | B)
=

n âˆ’rank (A)

+

k âˆ’rank (B)

+ dim

R (A) âˆ©R (B)

= dim N (A) + dim N (B) + dim

R (A) âˆ©R (B)

.
(c) Let A =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’1
1
âˆ’2
âˆ’1
0
âˆ’4
âˆ’1
0
âˆ’5
âˆ’1
0
âˆ’6
âˆ’1
0
âˆ’6
ï£¶
ï£·
ï£·
ï£·
ï£¸and B =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
3
âˆ’2
2
âˆ’1
1
0
0
1
0
1
ï£¶
ï£·
ï£·
ï£·
ï£¸contain bases for R (C)
and N (C), respectively, so that R (A) = R (C) and R (B) = N (C). Use
either part (a) or part (b) to obtain
dim

R (C) âˆ©N (C)

= dim

R (A) âˆ©R (B)

= 2.
Using R (A | B) = R (A) + R (B) produces
dim

R (C) + N (C)

= dim

R (A) + R (B)

= rank (A | B) = 3.
4.4.17. Suppose A is m Ã— n. Existence of a solution for every b implies R (A) = â„œm.
Recall from Â§2.5 that uniqueness of the solution implies rank (A) = n. Thus
m = dim R (A) = rank (A) = n so that A is m Ã— m of rank m.

36
Solutions
4.4.18. (a) x âˆˆS
=â‡’x âˆˆspan (Smax) â€”otherwise, the extension set E = Smaxâˆª{x}
would be linearly independentâ€”which is impossible because E would contain
more independent solutions than Smax. Now show span (Smax) âŠ†span {p} +
N (A). Since S = p + N (A) (see Exercise 4.2.10), si âˆˆS means there must
exist a corresponding vector ni âˆˆN (A) such that si = p + ni, and hence
x âˆˆspan (Smax)
=â‡’
x =
t

i=1
Î±isi =
t

i=1
Î±i (p + ni) =
t

i=1
Î±ip +
t

i=1
Î±ini
=â‡’
x âˆˆspan {p} + N (A)
=â‡’
span (Smax) âŠ†span {p} + N (A).
To prove the reverse inclusion, observe that if x âˆˆspan {p}+N (A), then there
exists a scalar Î± and a vector n âˆˆN (A) such that
x = Î±p + n = (Î± âˆ’1)p + (p + n).
Because p and (p + n) are both solutions, S âŠ†span(Smax) guarantees that
p and (p + n) each belong to span (Smax) , and the closure properties of a
subspace insure that x âˆˆspan (Smax) . Thus span {p}+N (A) âŠ†span (Smax) .
(b) The problem is really to determine the value of t in Smax. The fact that
Smax is a basis for span (Smax) together with (4.4.19) produces
t = dim

span (Smax)
	
= dim

span {p} + N (A)

= dim

span {p}
	
+ dim N (A) âˆ’dim

span {p} âˆ©N (A)

= 1 + (n âˆ’r) âˆ’0.
4.4.19. To show Smax is linearly independent, suppose
0 = Î±0p +
nâˆ’r

i=1
Î±i (p + hi) =
nâˆ’r

i=0
Î±i

p +
nâˆ’r

i=1
Î±ihi.
Multiplication by A yields 0 =

nâˆ’r
i=0 Î±i

b, which implies 
nâˆ’r
i=0 Î±i = 0,
and hence 
nâˆ’r
i=1 Î±ihi = 0. Because H is independent, we may conclude that
Î±1 = Î±2 = Â· Â· Â· = Î±nâˆ’r = 0. Consequently, Î±0p = 0, and therefore Î±0 = 0
(because p Ì¸= 0 ), so that Smax is an independent set. By Exercise 4.4.18, it
must also be maximal because it contains n âˆ’r + 1 vectors.
4.4.20. The proof depends on the observation that if B = PT AP, where P is a per-
mutation matrix, then the graph G(B) is the same as G(A) except that the
nodes in G(B) have been renumbered according to the permutation deï¬ning
P. This follows because PT = Pâˆ’1 implies A = PBPT , so if the rows (and

Solutions
37
columns) in P are the unit vectors that appear according to the permutation
Ï€ =

1
2
Â· Â· Â·
n
Ï€1
Ï€2
Â· Â· Â·
Ï€n

, then
aij =

PBPT 
ij =
ï£®
ï£¯ï£°
ï£«
ï£¬
ï£­
eT
Ï€1
...
eT
Ï€n
ï£¶
ï£·
ï£¸B ( eÏ€1
Â· Â· Â·
eÏ€n )
ï£¹
ï£ºï£»
ij
= eT
Ï€iBeÏ€j = bÏ€iÏ€j.
Consequently, aij Ì¸= 0 if and only if bÏ€iÏ€j Ì¸= 0, and thus G(A) and G(B)
are the same except for the fact that node Nk in G(A) is node NÏ€k in G(B)
for each k = 1, 2, . . . , n. Now we can prove G(A) is not strongly connected
â‡â‡’A is reducible. If A is reducible, then there is a permutation matrix such
that PT AP = B =

X
Y
0
Z

, where X is r Ã— r and Z is n âˆ’r Ã— n âˆ’r.
The zero pattern in B indicates that the nodes {N1, N2, . . . , Nr} in G(B) are
inaccessible from nodes {Nr+1, Nr+2, . . . , Nn} , and hence G(B) is not strongly
connectedâ€”e.g., there is no sequence of edges leading from Nr+1 to N1. Since
G(B) is the same as G(A) except that the nodes have diï¬€erent numbers, we
may conclude that G(A) is also not strongly connected. Conversely, if G(A)
is not strongly connected, then there are two nodes in G(A) such that one
is inaccessible from the other by any sequence of directed edges. Relabel the
nodes in G(A) so that this pair is N1 and Nn, where N1 is inaccessible
from Nn. If there are additional nodesâ€”excluding Nn itselfâ€”which are also
inaccessible from Nn, label them N2, N3, . . . , Nr so that the set of all nodes that
are inaccessible from Nn â€”with the possible exception of Nn itselfâ€”is Nn =
{N1, N2, . . . , Nr} (inaccessible nodes). Label the remaining nodesâ€”which are
all accessible from Nn â€”as Nn = {Nr+1, Nr+2, . . . , Nnâˆ’1} (accessible nodes).
It follows that no node in Nn can be accessible from any node in Nn, for
otherwise nodes in Nn would be accessible from Nn through nodes in Nn.
In other words, if Nr+k âˆˆNn and Nr+k â†’Ni âˆˆNn, then Nn â†’Nr+k â†’
Ni, which is impossible. This means that if Ï€ =

1
2
Â· Â· Â·
n
Ï€1
Ï€2
Â· Â· Â·
Ï€n

is the
permutation generated by the relabeling process, then aÏ€iÏ€j = 0 for each i =
r+1, r+2, . . . , nâˆ’1 and j = 1, 2, . . . , r. Therefore, if B = PT AP, where P is
the permutation matrix corresponding to the permutation Ï€, then bij = aÏ€iÏ€j,
so PT AP = B =

X
Y
0
Z

, where X is r Ã— r and Z is n âˆ’r Ã— n âˆ’r, and
thus A is reducible.
Solutions for exercises in section 4. 5
4.5.1. rank

AT A
	
= rank (A) = rank

AAT 	
= 2
4.5.2. dim N (A) âˆ©R (B) = rank (B) âˆ’rank (AB) = 2 âˆ’1 = 1.

38
Solutions
4.5.3. Gaussian elimination yields X =

1
1
âˆ’1
1
2
2

,
V =
 1
1

, and XV =
 2
0
4

.
4.5.4. Statement (4.5.2) says that the rank of a product cannot exceed the rank of any
factor.
4.5.5. rank (A) = rank

AT A
	
= 0
=â‡’
A = 0.
4.5.6. rank (A) = 2, and there are six 2 Ã— 2 nonsingular submatrices in A.
4.5.7. Yes. A =

1
1
1
1

and B =

1
1
âˆ’1
âˆ’1

is one of many examples.
4.5.8. Noâ€”it is not diï¬ƒcult to construct a counterexample using two singular matrices.
If either matrix is nonsingular, then the statement is true.
4.5.9. Transposition does not alter rank, so (4.5.1) says
rank (AB) = rank(AB)T = rank

BT AT 	
= rank

AT 	
âˆ’dim N

BT 	
âˆ©R

AT 	
= rank (A) âˆ’dim N

BT 	
âˆ©R

AT 	
.
4.5.10. This follows immediately from (4.5.1) because dim N (AB) = p âˆ’rank (AB)
and dim N (B) = p âˆ’rank (B).
4.5.11. (a)
First notice that N (B) âŠ†N (AB) (Exercise 4.2.12) for all conformable A
and B, so, by (4.4.5), dim N (B) â‰¤dim N (AB), or Î½(B) â‰¤Î½(AB), is always
trueâ€”this also answers the second half of part (b). If A and B are both n Ã— n,
then the rank-plus-nullity theorem together with (4.5.2) produces
Î½(A) = dim N (A) = n âˆ’rank (A) â‰¤n âˆ’rank (AB) = dim N (AB) = Î½(AB),
so, together with the ï¬rst observation, we have max {Î½(A), Î½(B)} â‰¤Î½(AB).
The rank-plus-nullity theorem applied to (4.5.3) yields Î½(AB) â‰¤Î½(A) + Î½(B).
(b)
To see that Î½(A) > Î½(AB) is possible for rectangular matrices, consider
A = ( 1
1 ) and B =

1
1

.
4.5.12. (a)
rank (BnÃ—p) = n
=â‡’
R (B) = â„œn
=â‡’
N (A)âˆ©R (B) = N (A)
=â‡’
rank (AB) = rank (B) âˆ’dim N (A) âˆ©R (B) = n âˆ’dim N (A)
= n âˆ’(n âˆ’rank (A)) = rank (A)
Itâ€™s always true that R (AB) âŠ†R (A). When dim R (AB) = dim R (A) (i.e.,
when rank (AB) = rank (A) ), (4.4.6) implies R (AB) = R (A).
(b)
rank (AmÃ—n) = n =â‡’N (A) = {0} =â‡’N (A) âˆ©R (B) = {0} =â‡’
rank (AB) = rank (B) âˆ’dim N (A) âˆ©R (B) = rank (B)
Assuming the product exists, it is always the case that N (B) âŠ†N (AB). Use
rank (B) = rank (AB) =â‡’pâˆ’rank (B) = pâˆ’rank (AB) =â‡’dim N (B) =
dim N (AB) together with (4.4.6) to conclude that N (B) = N (AB).

Solutions
39
4.5.13. (a)
rank (A) = 2, and the unique exact solution is (âˆ’1, 1).
(b)
Same as part (a).
(c)
The 3-digit rank is 2, and the unique 3-digit solution is (âˆ’1, 1).
(d)
The 3-digit normal equations

6
12
12
24
 
x1
x2

=

6.01
12

have inï¬nitely
many 3-digit solutions.
4.5.14. Use an indirect argument. Suppose x âˆˆN (I + F) in which xi Ì¸= 0 is a compo-
nent of maximal magnitude. Use the triangle inequality together with x = âˆ’Fx
to conclude
|xi| =

r

j=1
fijxj
 â‰¤
r

j=1
|fijxj| =
r

j=1
|fij| |xj| â‰¤

r

j=1
|fij|

|xi| < |xi|,
which is impossible. Therefore, N (I + F) = 0, and hence I + F is nonsingular.
4.5.15. Follow the approach used in (4.5.8) to write
A âˆ¼

W
0
0
S

,
where
S = Z âˆ’YWâˆ’1X.
rank (A) = rank (W)
=â‡’
rank (S) = 0
=â‡’
S = 0, so Z = YWâˆ’1X.
The desired conclusion now follows by taking B = YWâˆ’1 and C = Wâˆ’1X.
4.5.16. (a)
Suppose that A is nonsingular, and let Ek = Akâˆ’A so that lim
kâ†’âˆEk = 0.
This together with (4.5.9) implies there exists a suï¬ƒciently large value of k such
that
rank (Ak) = rank (A + Ek) â‰¥rank (A) = n,
which is impossible because each Ak is singular. Therefore, the supposition that
A is nonsingular must be false.
(b)
No!â€”consider
 1
k

1Ã—1 â†’[0].
4.5.17. M âŠ†N
because R (BC) âŠ†R (B), and therefore dim M â‰¤dim N. For-
mula (4.5.1) guarantees dim M = rank (BC) âˆ’rank (ABC) and dim N =
rank (B) âˆ’rank (AB), so the desired conclusion now follows.
4.5.18. N (A) âŠ†N

A2	
and R

A2	
âŠ†R (A) always hold, so (4.4.6) insures
N (A) = N

A2	
â‡â‡’dim N (A) = dim N

A2	
â‡â‡’n âˆ’rank (A) = n âˆ’rank

A2	
â‡â‡’rank (A) = rank

A2	
â‡â‡’R (A) = R

A2	
.
Formula (4.5.1) says rank

A2	
= rank (A) âˆ’dim R (A) âˆ©N (A), so
R

A2	
= R (A) â‡â‡’rank

A2	
= rank (A) â‡â‡’dim R (A) âˆ©N (A) = 0.

40
Solutions
4.5.19. (a)
Since

A
B

(A + B)(A | B) =

A
B

(A | B) =

A
0
0
B

,
the result of Example 3.9.3 together with (4.5.2) insures
rank (A) + rank (B) â‰¤rank (A + B).
Couple this with the fact that rank (A + B) â‰¤rank (A) + rank (B) (see Ex-
ample 4.4.8) to conclude rank (A + B) = rank (A) + rank (B).
(b)
Verify that if B = I âˆ’A, then B2 = B and AB = BA = 0, and apply
the result of part (a).
4.5.20. (a)
BT ACT = BT BCCT . The products BT B and CCT are each nonsin-
gular because they are r Ã— r with
rank

BT B
	
= rank (B) = r
and
rank

CCT 	
= rank (C) = r.
(b)
Notice that Aâ€  = CT 
BT BCCT 	âˆ’1BT = CT 
CCT 	âˆ’1
BT B
	âˆ’1BT , so
AT AAâ€ b = CT BT BCCT 
CCT 	âˆ’1
BT B
	âˆ’1BT b = CT BT b = AT b.
If Ax = b is consistent, then its solution set agrees with the solution set for the
normal equations.
(c)
AAâ€ A = BCCT 
CCT 	âˆ’1
BT B
	âˆ’1BT BC = BC = A. Now,
x âˆˆR

I âˆ’Aâ€ A
	
=â‡’
x =

I âˆ’Aâ€ A
	
y
for some y
=â‡’
Ax =

A âˆ’AAâ€ A
	
y = 0
=â‡’
x âˆˆN (A).
Conversely,
x âˆˆN (A)
=â‡’
Ax = 0
=â‡’
x =

I âˆ’Aâ€ A
	
x
=â‡’
x âˆˆR

I âˆ’Aâ€ A
	
,
so R

I âˆ’Aâ€ A
	
= N (A). As h ranges over all of â„œnÃ—1, the expression

I âˆ’Aâ€ A
	
h generates R

I âˆ’Aâ€ A
	
= N (A). Since Aâ€ b is a particular solu-
tion of AT Ax = AT b, the general solution is
x = Aâ€ b + N (A) = Aâ€ b +

I âˆ’Aâ€ A
	
h.
(d)
If r = n, then B = A and C = In.
(e)
If A is nonsingular, then so is AT , and
Aâ€  =

AT A
	âˆ’1AT = Aâˆ’1
AT 	âˆ’1AT = Aâˆ’1.
(f)
Follow along the same line as indicated in the solution to part (c) for the
case AAâ€ A = A.

Solutions
41
Solutions for exercises in section 4. 6
4.6.1. A =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
5
7
8
10
12
ï£¶
ï£·
ï£·
ï£·
ï£¸and b =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
11.1
15.4
17.5
22.0
26.3
ï£¶
ï£·
ï£·
ï£·
ï£¸, so AT A = 382 and AT b = 838.9. Thus the
least squares estimate for k is 838.9/382 = 2.196.
4.6.2. This is essentially the same problem as Exercise 4.6.1. Because it must pass
through the origin, the equation of the least squares line is y = mx, and hence
A =
ï£«
ï£¬
ï£¬
ï£­
x1
x2
...
xn
ï£¶
ï£·
ï£·
ï£¸and b =
ï£«
ï£¬
ï£¬
ï£­
y1
y2
...
yn
ï£¶
ï£·
ï£·
ï£¸, so AT A = 
i x2
i and AT b = 
i xiyi.
4.6.3. Look for the line p = Î± + Î²t that comes closest to the data in the least squares
sense. That is, ï¬nd the least squares solution for the system Ax = b, where
A =
ï£«
ï£­
1
1
1
2
1
3
ï£¶
ï£¸,
x =

Î±
Î²

,
and
b =
ï£«
ï£­
7
4
3
ï£¶
ï£¸.
Set up normal equations AT Ax = AT b to get

3
6
6
14
 
Î±
Î²

=

14
24

=â‡’

Î±
Î²

=

26/3
âˆ’2

=â‡’
p = (26/3) âˆ’2t.
Setting p = 0 gives t = 13/3. In other words, we expect the company to begin
losing money on May 1 of year ï¬ve.
4.6.4. The associated linear system Ax = b is
Year 1:
Î± + Î² = 1
Year 2:
2Î± = 1
Year 3:
âˆ’Î² = 1
or
ï£«
ï£­
1
1
2
0
0
âˆ’1
ï£¶
ï£¸

Î±
Î²

=
ï£«
ï£­
1
1
1
ï£¶
ï£¸.
The least squares solution to this inconsistent system is obtained from the system
of normal equations AT Ax = AT b that is

5
1
1
2
 
Î±
Î²

=

3
0

. The unique
solution is

Î±
Î²

=

2/3
âˆ’1/3

, so the least squares estimate for the increase in
bread prices is
B = 2
3W âˆ’1
3M.
When W = âˆ’1 and M = âˆ’1, we estimate that B = âˆ’1/3.
4.6.5. (a)
Î±0 = .02 and Î±1 = .0983.
(b)
1.986 grams.

42
Solutions
4.6.6. Use ln y = ln Î±0 + Î±1t to obtain the least squares estimates Î±0 = 9.73 and
Î±1 = .507.
4.6.7. The least squares line is y = 9.64 + .182x and for Îµi = 9.64 + .182xi âˆ’yi, the
sum of the squares of these errors is 
i Îµ2
i = 162.9. The least squares quadratic
is y = 13.97 + .1818x âˆ’.4336x2, and the corresponding sum of squares of the
errors is 
 Îµ2
i = 1.622. Therefore, we conclude that the quadratic provides a
much better ï¬t.
4.6.8. 230.7 min. (Î±0 = 492.04, Î±1 = âˆ’23.435, Î±2 = âˆ’.076134, Î±3 = 1.8624)
4.6.9. x2 is a least squares solution
=â‡’
AT Ax2 = AT b
=â‡’
0 = AT (b âˆ’Ax2).
If we set x1 = b âˆ’Ax2, then
 ImÃ—m
A
AT
0nÃ—n
  x1
x2

=
 ImÃ—m
A
AT
0nÃ—n
  b âˆ’Ax2
x2

=
 b
0

.
The converse is true because
 ImÃ—m
A
AT
0nÃ—n
  x1
x2

=
 b
0

=â‡’
Ax2 = b âˆ’x1
and
AT x1 = 0
=â‡’
AT Ax2 = AT b âˆ’AT x1 = AT b.
4.6.10. t âˆˆR

AT 	
= R

AT A
	
=â‡’tT = zT AT A for some z. For each x satisfying
AT Ax = AT b, write
Ë†y = tT x = zT AT Ax = zT AT b,
and notice that zT AT b is independent of x.
Solutions for exercises in section 4. 7
4.7.1. (b) and (f)
4.7.2. (a), (c), and (d)
4.7.3. Use any x to write T(0) = T(x âˆ’x) = T(x) âˆ’T(x) = 0.
4.7.4. (a)
4.7.5. (a) No
(b) Yes
4.7.6. T(u1) = (2, 2) = 2u1 + 0u2 and T(u2) = (3, 6) = 0u1 + 3u2 so that [T]B =

2
0
0
3

.
4.7.7. (a) [T]SSâ€² =
ï£«
ï£­
1
3
0
0
2
âˆ’4
ï£¶
ï£¸
(b) [T]SSâ€²â€² =
ï£«
ï£­
2
âˆ’4
0
0
1
3
ï£¶
ï£¸
4.7.8. [T]B =
ï£«
ï£­
1
âˆ’3/2
1/2
âˆ’1
1/2
1/2
0
1/2
âˆ’1/2
ï£¶
ï£¸and [v]B =
ï£«
ï£­
1
1
0
ï£¶
ï£¸.

Solutions
43
4.7.9. According to (4.7.4), the jth column of [T]S is
[T(ej)]S = [Aej]S = [Aâˆ—j]S = Aâˆ—j.
4.7.10. [Tk]B = [TT Â· Â· Â· T]B = [T]B[T]B Â· Â· Â· [T]B = [T]k
B
4.7.11. (a)
Sketch a picture to observe that P(e1) =

x
x

= P(e2) and that the
vectors e1,
P(e1), and 0 are vertices of a 45â—¦right triangle (as are e2,
P(v2), and 0 ). So, if âˆ¥â‹†âˆ¥denotes length, the Pythagorean theorem may be
applied to yield 1 = 2 âˆ¥P(e1)âˆ¥2 = 4x2 and 1 = 2 âˆ¥P(e2)âˆ¥2 = 4x2. Thus
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
P(e1) =

1/2
1/2

= (1/2)e1 + (1/2)e2
P(e2) =

1/2
1/2

= (1/2)e1 + (1/2)e2
ï£¼
ï£´
ï£´
ï£½
ï£´
ï£´
ï£¾
=â‡’
[P]S =

1/2
1/2
1/2
1/2

.
(b)
P(v) =
 Î±+Î²
2
Î±+Î²
2

4.7.12. (a)
If U1 =

1
0
0
0

,
U2 =

0
1
0
0

,
U3 =

0
0
1
0

,
U4 =

0
0
0
1

,
then
T(U1) = U1 + 0U2 + 0U3 + 0U4,
T(U2) = 1
2

0
1
1
0

= 0U1 + 1/2U2 + 1/2U3 + 0U4,
T(U3) = 1
2

0
1
1
0

= 0U1 + 1/2U2 + 1/2U3 + 0U4,
T(U4) = 0U1 + 0U2 + 0U3 + U4,
so [T]S =
ï£«
ï£¬
ï£­
1
0
0
0
0
1/2
1/2
0
0
1/2
1/2
0
0
0
0
1
ï£¶
ï£·
ï£¸. To verify [T(U)]S = [T]S[U]S, observe that
T(U) =

a
(b + c)/2
(b + c)/2
d

, [T(U)]S =
ï£«
ï£¬
ï£­
a
(b + c)/2
(b + c)/2
d
ï£¶
ï£·
ï£¸, [U]S =
ï£«
ï£¬
ï£­
a
b
c
d
ï£¶
ï£·
ï£¸.
(b)
For U1, U2, U3, and U4 as deï¬ned above,

44
Solutions
T(U1) =

0
âˆ’1
âˆ’1
0

= 0U1 âˆ’U2 âˆ’U3 + 0U4,
T(U2) =

1
2
0
âˆ’1

= U1 + 2U2 + 0U3 âˆ’U4,
T(U3) =

1
0
âˆ’2
âˆ’1

= U1 + 0U2 âˆ’2U3 âˆ’1U4,
T(U4) =

0
1
1
0

= 0U1 + U2 + U3 + 0U4,
so [T]S =
ï£«
ï£¬
ï£­
0
1
1
0
âˆ’1
2
0
1
âˆ’1
0
âˆ’2
1
0
âˆ’1
âˆ’1
0
ï£¶
ï£·
ï£¸. To verify [T(U)]S = [T]S[U]S, observe that
T(U) =

c + b
âˆ’a + 2b + d
âˆ’a âˆ’2c + d
âˆ’b âˆ’c

and [T(U)]S =
ï£«
ï£¬
ï£­
c + b
âˆ’a + 2b + d
âˆ’a âˆ’2c + d
âˆ’b âˆ’c
ï£¶
ï£·
ï£¸.
4.7.13. [S]BBâ€² =
ï£«
ï£¬
ï£­
0
0
0
1
0
0
0
1/2
0
0
0
1/3
ï£¶
ï£·
ï£¸
4.7.14. (a) [RQ]S = [R]S[Q]S =

1
0
0
âˆ’1
 
cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸

=

cos Î¸
âˆ’sin Î¸
âˆ’sin Î¸
âˆ’cos Î¸

(b)
[QQ]S = [Q]S[Q]S =

cos2 Î¸ âˆ’sin2 Î¸
âˆ’2 cos Î¸ sin Î¸
2 cos Î¸ sin Î¸
cos2 Î¸ âˆ’sin2 Î¸

=

cos 2Î¸
âˆ’sin 2Î¸
sin 2Î¸
cos 2Î¸

4.7.15. (a)
Let B = {ui}n
i=1,
Bâ€² = {vi}m
i=1. If [P]BBâ€² = [Î±ij] and [Q]BBâ€² = [Î²ij],
then P(uj) = 
i Î±ijvi and Q(uj) = 
i Î²ijvi. Thus (P+Q)(uj) = 
i(Î±ij +
Î²ij)vi and hence [P + Q]BBâ€² = [Î±ij +Î²ij] = [Î±ij]+[Î²ij] = [P]BBâ€² +[Q]BBâ€². The
proof of part (b) is similar.
4.7.16. (a)
If B = {xi}n
i=1 is a basis, then I(xj) = 0x1 + 0x2 + Â· Â· Â· + 1xj + Â· Â· Â· + 0xn
so that the jth column in [I]B is just the jth unit column.
(b)
Suppose xj = 
i Î²ijyi so that [xj]Bâ€² =
ï£«
ï£¬
ï£­
Î²1j
...
Î²nj
ï£¶
ï£·
ï£¸. Then
I(xj) = xj =

i
Î²ijyi
=â‡’
[I]BBâ€² = [Î²ij] =

[x1]Bâ€²
 [x2]Bâ€²
 Â· Â· Â·
 [xn]Bâ€²

.
Furthermore, T(yj) = xj = 
i Î²ijyi
=â‡’
[T]Bâ€² = [Î²ij], and
T(xj) = T
 
i
Î²ijyi

=

i
Î²ijT(yi) =

i
Î²ijxi
=â‡’
[T]B = [Î²ij].

Solutions
45
(c)
ï£«
ï£­
1
âˆ’1
0
0
1
âˆ’1
0
0
1
ï£¶
ï£¸
4.7.17. (a)
Tâˆ’1(x, y, z) = (x + y + z, x + 2y + 2z, x + 2y + 3z)
(b)
[Tâˆ’1]S =
ï£«
ï£­
1
1
1
1
2
2
1
2
3
ï£¶
ï£¸= [T]âˆ’1
S
4.7.18. (1) =â‡’(2) :
T(x) = T(y)
=â‡’
T(xâˆ’y) = 0
=â‡’
(yâˆ’x) = Tâˆ’1(0) = 0.
(2)
=â‡’
(3) :
T(x) = 0 and T(0) = 0
=â‡’
x = 0.
(3)
=â‡’
(4) :
If {ui}n
i=1 is a basis for V, show that N (T) = {0} implies
{T(ui)}n
i=1 is also a basis. Consequently, for each v âˆˆV there are coordinates
Î¾i such that
v =

i
Î¾iT(ui) = T
 
i
Î¾iui

.
(4)
=â‡’
(2) :
For each basis vector ui, there is a vi such that T(vi) = ui.
Show that {vi}n
i=1 is also a basis. If T(x) = T(y), then T(x âˆ’y) = 0.
Let x âˆ’y = 
i Î¾ivi so that 0 = T(x âˆ’y) = T
 
i Î¾ivi

= 
i Î¾iT(vi) =

i Î¾iui
=â‡’
each Î¾i = 0
=â‡’
x âˆ’y = 0
=â‡’
x = y.
(4) and (2)
=â‡’
(1) :
For each y âˆˆV, show there is a unique x such
that T(x) = y. Let Ë†T be the function deï¬ned by the rule Ë†T(y) = x. Clearly,
TË†T = Ë†TT = I. To show that Ë†T is a linear function, consider Î±y1 +y2, and let
x1 and x2 be such that T(x1) = y1, T(x2) = y2. Now, T(Î±x1+x2) = Î±y1+y2
so that Ë†T(Î±y1 + y2) = Î±x1 + x2. However, x1 = Ë†T(y1), x2 = Ë†T(y2) so that
Î± Ë†T(y1) + Ë†T(y2) = Î±x1 + x2 = Ë†T(Î±y1 + y2). Therefore Ë†T = Tâˆ’1.
4.7.19. (a)
0 = 
i Î±ixi â‡â‡’
ï£«
ï£­
0
...
0
ï£¶
ï£¸= [0]B =
% 
i Î±ixi
&
B = 
i[Î±ixi]B = 
i Î±i[xi]B
(b)
G =
'
T(u1), T(u2), . . . , T(un)
(
spans R (T). From part (a), the set
'
T(ub1), T(ub2), . . . , T(ubr)
(
is a maximal independent subset of G if and only if the set
'
[T(ub1)]B, [T(ub2)]B, . . . , [T(ubr)]B
(
is a maximal linearly independent subset of
)
[T(u1)]B, [T(u2)]B, . . . , [T(un)]B
*
,
which are the columns of [T]B.

46
Solutions
Solutions for exercises in section 4. 8
4.8.1. Multiplication by nonsingular matrices does not change rank.
4.8.2. A = Qâˆ’1BQ
and
B = Pâˆ’1CP
=â‡’
A = (PQ)âˆ’1C(PQ).
4.8.3. (a) [A]S =
ï£«
ï£­
1
2
âˆ’1
0
âˆ’1
0
1
0
7
ï£¶
ï£¸
(b) [A]Sâ€² =
ï£«
ï£­
1
4
3
âˆ’1
âˆ’2
âˆ’9
1
1
8
ï£¶
ï£¸and Q =
ï£«
ï£­
1
1
1
0
1
1
0
0
1
ï£¶
ï£¸
4.8.4. Put the vectors from B into a matrix Q and compute
[A]B = Qâˆ’1AQ =
ï£«
ï£­
âˆ’2
âˆ’3
âˆ’7
7
9
12
âˆ’2
âˆ’1
0
ï£¶
ï£¸.
4.8.5. [B]S = B and [B]Sâ€² = C. Therefore, C = Qâˆ’1BQ, where Q =

2
âˆ’3
âˆ’1
2

is the change of basis matrix from Sâ€² to S.
4.8.6. If B = {u, v} is such a basis, then T(u) = 2u and T(v) = 3v. For u =
(u1, u2), T(u) = 2u implies
âˆ’7u1 âˆ’15u2 = 2u1
6u1 + 12u2 = 2u2,
or
âˆ’9u1 âˆ’15u2 = 0
6u1 + 10u2 = 0,
so u1 = (âˆ’5/3)u2 with u2 being free. Letting u2 = âˆ’3 produces u = (5, âˆ’3).
Similarly, a solution to T(v) = 3v is v = (âˆ’3, 2). [T]S =

âˆ’7
âˆ’15
6
12

and
[T]B =

2
0
0
3

. For Q =

5
âˆ’3
âˆ’3
2

, [T]B = Qâˆ’1[T]SQ.
4.8.7. If sin Î¸ = 0, the result is trivial. Assume sin Î¸ Ì¸= 0. Notice that with respect
to the standard basis S, [P]S = R. This means that if R and D are to be
similar, then there must exist a basis B = {u, v} such that [P]B = D, which
implies that P(u) = eiÎ¸u and P(v) = eâˆ’iÎ¸v. For u = (u1, u2), P(u) = eiÎ¸u
implies
u1 cos Î¸ âˆ’u2 sin Î¸ = eiÎ¸u1 = u1 cos Î¸ + iu1 sin Î¸
u1 sin Î¸ + u2 cos Î¸ = eiÎ¸u2 = u2 cos Î¸ + iu2 sin Î¸,
or
iu1 + u2 = 0
u1 âˆ’iu2 = 0,

Solutions
47
so u1 = iu2 with u2 being free. Letting u2 = 1 produces u = (i, 1). Similarly,
a solution to P(v) = eâˆ’iÎ¸v is v = (1, i). Now, [P]S = R and [P]B = D so
that R and D must be similar. The coordinate change matrix from B to S
is Q =

i
1
1
i

, and therefore D = Qâˆ’1RQ.
4.8.8. (a) B = Qâˆ’1CQ
=â‡’
(B âˆ’Î»I) = Qâˆ’1CQâˆ’Î»Qâˆ’1Q = Qâˆ’1 (C âˆ’Î»I) Q. The
result follows because multiplication by nonsingular matrices does not change
rank.
(b) B = Pâˆ’1DP
=â‡’
B âˆ’Î»iI = Pâˆ’1(D âˆ’Î»iI)P and (D âˆ’Î»iI) is singular
for each Î»i. Now use part (a).
4.8.9. B = Pâˆ’1AP
=â‡’
Bk = Pâˆ’1APPâˆ’1AP Â· Â· Â· Pâˆ’1AP = Pâˆ’1AA Â· Â· Â· AP =
Pâˆ’1AkP
4.8.10. (a) YT Y is nonsingular because rank

YT Y
	
nÃ—n = rank (Y) = n. If
[v]B =
ï£«
ï£­
Î±1
...
Î±n
ï£¶
ï£¸
and
[v]Bâ€² =
ï£«
ï£¬
ï£­
Î²1
...
Î²n
ï£¶
ï£·
ï£¸,
then
v =

i
Î±ixi = X[v]B
and
v =

i
Î²iyi = Y[v]Bâ€²
=â‡’
X[v]B = Y[v]Bâ€²
=â‡’
YT X[v]B = YT Y[v]Bâ€²
=â‡’
(YT Y)âˆ’1YT X[v]B = [v]Bâ€².
(b) When m = n, Y is square and (YT Y)âˆ’1YT = Yâˆ’1 so that P = Yâˆ’1X.
4.8.11. (a) Because B contains n vectors, you need only show that B is linearly in-
dependent. To do this, suppose 
nâˆ’1
i=0 Î±iNi(y) = 0 and apply Nnâˆ’1 to both
sides to get Î±0Nnâˆ’1(y) = 0
=â‡’
Î±0 = 0. Now 
nâˆ’1
i=1 Î±iNi(y) = 0. Apply
Nnâˆ’2 to both sides of this to conclude that Î±1 = 0. Continue this process until
you have Î±0 = Î±1 = Â· Â· Â· = Î±nâˆ’1 = 0.
(b) Any n Ã— n nilpotent matrix of index n can be viewed as a nilpotent operator
of index n on â„œn. Furthermore, A = [A]S and B = [B]S, where S is the
standard basis. According to part (a), there are bases B and Bâ€² such that
[A]B = J and [B]Bâ€² = J. Since [A]S â‰ƒ[A]B, it follows that A â‰ƒJ. Similarly
B â‰ƒJ, and hence A â‰ƒB by Exercise 4.8.2.
(c) Trace and rank are similarity invariants, and part (a) implies that every
n Ã— n nilpotent matrix of index n is similar to J, and trace (J) = 0 and
rank (J) = n âˆ’1.
4.8.12. (a) xi âˆˆR (E)
=â‡’
xi = E(vi) for some vi
=â‡’
E(xi) = E2(vi) =
E(vi) = xi. Since B contains n vectors, you need only show that B is linearly
independent. 0 = 
i Î±ixi + Î²iyi
=â‡’
0 = E(0) = 
i Î±iE(xi) + Î²iE(yi) =

i Î±ixi
=â‡’
Î±i â€™s = 0
=â‡’

i Î²iyi = 0
=â‡’
Î²i â€™s = 0.

48
Solutions
(b) Let B = X âˆªY = {b1, b2, . . . , bn}. For j = 1, 2, . . . , r, the jth column
of [E]B is [E(bj)]B = [E(xj)]B = ej. For j = r + 1, r + 2, . . . , n, [E(bj)]B =
[E(yjâˆ’r)]B = [0]B = 0.
(c) Suppose that B and C are two idempotent matrices of rank r. If you
regard them as linear operators on â„œn, then, with respect to the standard basis,
[B]S = B and [C]S = C. You know from part (b) that there are bases U and
V such that [B]U = [C]V =

Ir
0
0
0

= P. This implies that B â‰ƒP, and
P â‰ƒC. From Exercise 4.8.2, it follows that B â‰ƒC.
(d) It follows from part (c) that F â‰ƒP =

Ir
0
0
0

. Since trace and rank are
similarity invariants, trace (F) = trace (P) = r = rank (P) = rank (F).
Solutions for exercises in section 4. 9
4.9.1. (a)
Yes, because T(0) = 0.
(b)
Yes, because x âˆˆV
=â‡’
T(x) âˆˆV.
4.9.2. Every subspace of V is invariant under I.
4.9.3. (a)
X is invariant because x âˆˆX â‡â‡’x = (Î±, Î², 0, 0) for Î±, Î² âˆˆâ„œ, so
T(x) = T(Î±, Î², 0, 0) = (Î± + Î², Î², 0, 0) âˆˆX.
(b)
%
T/X
&
{e1,e2} =

1
1
0
1

(c)
[T]B =
ï£«
ï£¬
ï£¬
ï£­
1
1
âˆ—
âˆ—
0
1
âˆ—
âˆ—
0
0
âˆ—
âˆ—
0
0
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£¸
4.9.4. (a)
Q is nonsingular.
(b)
X is invariant because
T(Î±1Qâˆ—1 + Î±2Qâˆ—2) = Î±1
ï£«
ï£¬
ï£­
1
1
âˆ’2
3
ï£¶
ï£·
ï£¸+ Î±2
ï£«
ï£¬
ï£­
1
2
âˆ’2
2
ï£¶
ï£·
ï£¸= Î±1Qâˆ—1 + Î±2(Qâˆ—1 + Qâˆ—2)
= (Î±1 + Î±2)Qâˆ—1 + Î±2Qâˆ—2 âˆˆspan {Qâˆ—1, Qâˆ—2} .
Y is invariant because
T(Î±3Qâˆ—3 + Î±4Qâˆ—4) = Î±3
ï£«
ï£¬
ï£­
0
0
0
0
ï£¶
ï£·
ï£¸+ Î±4
ï£«
ï£¬
ï£­
0
3
1
âˆ’4
ï£¶
ï£·
ï£¸= Î±4Qâˆ—3 âˆˆspan {Qâˆ—3, Qâˆ—4} .
(c)
According to (4.9.10), Qâˆ’1TQ should be block diagonal.

Solutions
49
(d)
Qâˆ’1TQ =
ï£«
ï£¬
ï£¬
ï£­
1
1
0
0
0
1
0
0
0
0
0
1
0
0
0
0
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£­
%
T/X
&
{Qâˆ—1,Qâˆ—2}
0
0
%
T/Y
&
{Qâˆ—3,Qâˆ—4}
ï£¶
ï£¸
4.9.5. If A = [Î±ij] and C = [Î³ij], then
T(uj) =
r

i=1
Î±ijui âˆˆU
and
T(wj) =
q

i=1
Î³ijwi âˆˆW.
4.9.6. If S is the standard basis for â„œnÃ—1, and if B is the basis consisting of the
columns of P, then
[T]B = Pâˆ’1[T]SP = Pâˆ’1TP =

A
0
0
C

.
(Recall Example 4.8.3.) The desired conclusion now follows from the result of
Exercise 4.9.5.
4.9.7. x âˆˆN (A âˆ’Î»I)
=â‡’
(A âˆ’Î»I) x = 0
=â‡’
Ax = Î»x âˆˆN (A âˆ’Î»I)
4.9.8. (a)
(A âˆ’Î»I) is singular when Î» = âˆ’1 and Î» = 3.
(b)
There are four invariant subspacesâ€”the trivial space {0}, the entire space
â„œ2, and the two one-dimensional spaces
N (A + I) = span

1
2
+
and
N (A âˆ’3I) = span

1
3
+
.
(c)
Q =

1
1
2
3


50
Solutions
Clearly spoken, Mr. Fogg; you explain English by Greek.
â€” Benjamin Franklin (1706â€“1790)

Solutions for Chapter 5
Solutions for exercises in section 5. 1
5.1.1. (a)
âˆ¥xâˆ¥1 = 9,
âˆ¥xâˆ¥2 = 5,
âˆ¥xâˆ¥âˆ= 4
(b)
âˆ¥xâˆ¥1 = 5 + 2
âˆš
2,
âˆ¥xâˆ¥2 =
âˆš
21,
âˆ¥xâˆ¥âˆ= 4
5.1.2. (a)
âˆ¥u âˆ’vâˆ¥=
âˆš
31
(b)
âˆ¥u + vâˆ¥=
âˆš
27 â‰¤7 = âˆ¥uâˆ¥+ âˆ¥vâˆ¥
(c)
|uT v| = 1 â‰¤10 = âˆ¥uâˆ¥âˆ¥vâˆ¥
5.1.3. Use the CBS inequality with x =
ï£«
ï£¬
ï£¬
ï£­
Î±1
Î±2
...
Î±n
ï£¶
ï£·
ï£·
ï£¸and y =
ï£«
ï£¬
ï£¬
ï£­
1
1
...
1
ï£¶
ï£·
ï£·
ï£¸.
5.1.4. (a)
)
x âˆˆâ„œn  âˆ¥xâˆ¥2 â‰¤1
*
(b)
)
x âˆˆâ„œn  âˆ¥x âˆ’câˆ¥2 â‰¤Ï
*
5.1.5. âˆ¥x âˆ’yâˆ¥2 = âˆ¥x + yâˆ¥2
=â‡’
âˆ’2xT y = 2xT y
=â‡’
xT y = 0.
5.1.6. âˆ¥x âˆ’yâˆ¥= âˆ¥(âˆ’1)(y âˆ’x)âˆ¥= |(âˆ’1)| âˆ¥y âˆ’xâˆ¥= âˆ¥y âˆ’xâˆ¥
5.1.7. xâˆ’y = 
n
i=1(xi âˆ’yi)ei
=â‡’
âˆ¥x âˆ’yâˆ¥â‰¤
n
i=1 |xi âˆ’yi| âˆ¥eiâˆ¥â‰¤Î½ 
n
i=1 |xi âˆ’yi|,
where Î½ = maxi âˆ¥eiâˆ¥. For each Ïµ > 0, set Î´ = Ïµ/nÎ½. If |xi âˆ’yi| < Î´ for each
i, then, using (5.1.6),
 âˆ¥xâˆ¥âˆ’âˆ¥yâˆ¥
 â‰¤âˆ¥x âˆ’yâˆ¥< Î½nÎ´ = Ïµ.
5.1.8. To show that âˆ¥xâˆ¥1 â‰¤âˆšn âˆ¥xâˆ¥2 , apply the CBS inequality to the standard inner
product of a vector of all 1â€™s with a vector whose components are the |xi| â€™s.
5.1.9. If y = Î±x, then |xâˆ—y| = |Î±| âˆ¥xâˆ¥2 = âˆ¥xâˆ¥âˆ¥yâˆ¥. Conversely, if |xâˆ—y| = âˆ¥xâˆ¥âˆ¥yâˆ¥,
then (5.1.4) implies that âˆ¥Î±x âˆ’yâˆ¥= 0, and hence Î±x âˆ’y = 0 â€”recall (5.1.1).
5.1.10. If y = Î±x for Î± > 0, then âˆ¥x + yâˆ¥= âˆ¥(1 + Î±)xâˆ¥= (1 + Î±) âˆ¥xâˆ¥= âˆ¥xâˆ¥+ âˆ¥yâˆ¥.
Conversely, âˆ¥x + yâˆ¥= âˆ¥xâˆ¥+ âˆ¥yâˆ¥
=â‡’
(âˆ¥xâˆ¥+ âˆ¥yâˆ¥)2 = âˆ¥x + yâˆ¥2
=â‡’
âˆ¥xâˆ¥2 + 2 âˆ¥xâˆ¥âˆ¥yâˆ¥+ âˆ¥yâˆ¥2 = (xâˆ—+ yâˆ—) (x + y)
= xâˆ—x + xâˆ—y + yâˆ—x + yâˆ—y
= âˆ¥xâˆ¥2 + 2 Re(xâˆ—y) + âˆ¥yâˆ¥2 ,
and hence âˆ¥xâˆ¥âˆ¥yâˆ¥= Re (xâˆ—y) . But itâ€™s always true that Re (xâˆ—y) â‰¤
xâˆ—y
,
so the CBS inequality yields
âˆ¥xâˆ¥âˆ¥yâˆ¥= Re (xâˆ—y) â‰¤
xâˆ—y
 â‰¤âˆ¥xâˆ¥âˆ¥yâˆ¥.
In other words,
xâˆ—y
 = âˆ¥xâˆ¥âˆ¥yâˆ¥. We know from Exercise 5.1.9 that equality
in the CBS inequality implies y = Î±x, where Î± = xâˆ—y/xâˆ—x. We now need to
show that this Î± is real and positive. Using y = Î±x in the equality âˆ¥x + yâˆ¥=

52
Solutions
âˆ¥xâˆ¥+ âˆ¥yâˆ¥produces |1 + Î±| = 1 + |Î±|, or |1 + Î±|2 = (1 + |Î±|)2 . Expanding this
yields
(1 + Â¯Î±)(1 + Î±) = 1 + 2|Î±| + |Î±|2
=â‡’
1 + 2 Re(Î±) + Â¯Î±Î± = 1 + 2|Î±| + Â¯Î±Î±
=â‡’
Re(Î±) = |Î±|,
which implies that Î± must be real. Furthermore, Î± = Re (Î±) = |Î±| â‰¥0. Since
y = Î±x and y Ì¸= 0, it follows that Î± Ì¸= 0, and therefore Î± > 0.
5.1.11. This is a consequence of HÂ¨olderâ€™s inequality because
|xT y| = |xT (y âˆ’Î±e)| â‰¤âˆ¥xâˆ¥1 âˆ¥y âˆ’Î±eâˆ¥âˆ
for all Î±, and minÎ± âˆ¥y âˆ’Î±eâˆ¥âˆ= (ymax âˆ’ymin)/2 (with the minimum being
attained at Î± = (ymax + ymin)/2 ).
5.1.12. (a)
Itâ€™s not diï¬ƒcult to see that f â€²(t) < 0 for t < 1, and f â€²(t) > 0 for t > 1,
so we can conclude that f(t) > f(1) = 0 for t Ì¸= 1. The desired inequality
follows by setting t = Î±/Î².
(b)
This inequality follows from the inequality of part (a) by setting
Î± = |Ë†xi|p,
Î² = |Ë†yi|q,
Î» = 1/p,
and
(1 âˆ’Î») = 1/q.
(c)
HÂ¨olderâ€™s inequality results from part (b) by setting Ë†xi = xi/ âˆ¥xâˆ¥p and
Ë†yi = yi/ âˆ¥yâˆ¥q . To obtain the â€œvector formâ€ of the inequality, use the triangle
inequality for complex numbers to write
|xâˆ—y| =

n

i=1
xiyi
 â‰¤
n

i=1
|xi| |yi| =
n

i=1
|xiyi| â‰¤
 n

i=1
|xi|p
1/p  n

i=1
|yi|q
1/q
= âˆ¥xâˆ¥p âˆ¥yâˆ¥q .
5.1.13. For p = 1, Minkowskiâ€™s inequality is a consequence of the triangle inequality
for scalars. The inequality in the hint follows from the fact that p = 1 + p/q
together with the scalar triangle inequality, and it implies that
n

i=1
|xi +yi|p =
n

i=1
|xi +yi| |xi +yi|p/q â‰¤
n

i=1
|xi| |xi +yi|p/q +
n

i=1
|yi| |xi +yi|p/q.
Application of HÂ¨olderâ€™s inequality produces
n

i=1
|xi| |xi + yi|p/q â‰¤
 n

i=1
|xi|p
1/p  n

i=1
|xi + yi|p
1/q
=
 n

i=1
|xi|p
1/p  n

i=1
|xi + yi|p
(pâˆ’1)/p
= âˆ¥xâˆ¥p âˆ¥x + yâˆ¥pâˆ’1
p
.

Solutions
53
Similarly,
n

i=1
|yi| |xi + yi|p/q â‰¤âˆ¥yâˆ¥p âˆ¥x + yâˆ¥pâˆ’1
p
, and therefore
âˆ¥x + yâˆ¥p
p â‰¤

âˆ¥xâˆ¥p + âˆ¥yâˆ¥p

âˆ¥x + yâˆ¥pâˆ’1
p
=â‡’
âˆ¥x + yâˆ¥p â‰¤âˆ¥xâˆ¥p + âˆ¥yâˆ¥p .
Solutions for exercises in section 5. 2
5.2.1. âˆ¥Aâˆ¥F =
%
i,j |aij|2&1/2
= [trace (Aâˆ—A)]1/2 =
âˆš
10,
âˆ¥Bâˆ¥F =
âˆš
3, and âˆ¥Câˆ¥F =
âˆš
9.
5.2.2. (a)
âˆ¥Aâˆ¥1 = max absolute column sum = 4, and âˆ¥Aâˆ¥âˆ= max absolute
row sum = 3. âˆ¥Aâˆ¥2 = âˆšÎ»max, where Î»max is the largest value of Î» for which
AT A âˆ’Î»I is singular. Determine these Î» â€™s by row reduction.
AT A âˆ’Î»I =

2 âˆ’âˆ’Î»
âˆ’4
âˆ’4
8 âˆ’Î»

âˆ’â†’

âˆ’4
8 âˆ’Î»
2 âˆ’Î»
âˆ’4

âˆ’â†’

âˆ’4
8 âˆ’Î»
0
âˆ’4 + 2âˆ’Î»
4 (8 âˆ’Î»)

This matrix is singular if and only if the second pivot is zero, so we must have
(2 âˆ’Î»)(8 âˆ’Î») âˆ’16 = 0
=â‡’
Î»2 âˆ’10Î» = 0
=â‡’
Î» = 0, Î» = 10, and therefore
âˆ¥Aâˆ¥2 =
âˆš
10.
(b)
Use the same technique to get âˆ¥Bâˆ¥1 = âˆ¥Bâˆ¥2 = âˆ¥Bâˆ¥âˆ= 1, and
(c)
âˆ¥Câˆ¥1 = âˆ¥Câˆ¥âˆ= 10 and âˆ¥Câˆ¥2 = 9.
5.2.3. (a)
âˆ¥Iâˆ¥= maxâˆ¥xâˆ¥=1 âˆ¥Ixâˆ¥= maxâˆ¥xâˆ¥=1 âˆ¥xâˆ¥= 1.
(b)
âˆ¥InÃ—nâˆ¥F =

trace

IT I
	1/2 = âˆšn.
5.2.4. Use the fact that trace (AB) = trace (BA) (recall Example 3.6.5) to write
âˆ¥Aâˆ¥2
F = trace (Aâˆ—A) = trace (AAâˆ—) = âˆ¥Aâˆ—âˆ¥2
F .
5.2.5. (a)
For x = 0, the statement is trivial. For x Ì¸= 0, we have âˆ¥(x/ âˆ¥xâˆ¥)âˆ¥= 1,
so for any particular x0 Ì¸= 0,
âˆ¥Aâˆ¥= max
âˆ¥xâˆ¥=1 âˆ¥Axâˆ¥= max
xÌ¸=0
,,,,A x
âˆ¥xâˆ¥
,,,, â‰¥âˆ¥Ax0âˆ¥
âˆ¥x0âˆ¥
=â‡’
âˆ¥Ax0âˆ¥â‰¤âˆ¥Aâˆ¥âˆ¥x0âˆ¥.
(b)
Let x0 be a vector such that âˆ¥x0âˆ¥= 1 and
âˆ¥ABx0âˆ¥= max
âˆ¥xâˆ¥=1 âˆ¥ABxâˆ¥= âˆ¥ABâˆ¥.
Make use of the result of part (a) to write
âˆ¥ABâˆ¥= âˆ¥ABx0âˆ¥â‰¤âˆ¥Aâˆ¥âˆ¥Bx0âˆ¥â‰¤âˆ¥Aâˆ¥âˆ¥Bâˆ¥âˆ¥x0âˆ¥= âˆ¥Aâˆ¥âˆ¥Bâˆ¥.

54
Solutions
(c)
âˆ¥Aâˆ¥= max
âˆ¥xâˆ¥=1 âˆ¥Axâˆ¥â‰¤max
âˆ¥xâˆ¥â‰¤1 âˆ¥Axâˆ¥because {x | âˆ¥xâˆ¥= 1} âŠ‚{x | âˆ¥xâˆ¥â‰¤1} .
If there would exist a vector x0 such that âˆ¥x0âˆ¥< 1 and âˆ¥Aâˆ¥< âˆ¥Ax0âˆ¥,
then part (a) would insure that âˆ¥Aâˆ¥< âˆ¥Ax0âˆ¥â‰¤âˆ¥Aâˆ¥âˆ¥x0âˆ¥< âˆ¥Aâˆ¥, which is
impossible.
5.2.6. (a)
Applying the CBS inequality yields
|yâˆ—Ax| â‰¤âˆ¥yâˆ¥2 âˆ¥Axâˆ¥2
=â‡’
max
âˆ¥xâˆ¥2=1
âˆ¥yâˆ¥2=1
|yâˆ—Ax| â‰¤max
âˆ¥xâˆ¥2=1 âˆ¥Axâˆ¥2 = âˆ¥Aâˆ¥2 .
Now show that equality is actually attained for some pair x and y on the unit
2-sphere. To do so, notice that if x0 is a vector of unit length such that
âˆ¥Ax0âˆ¥2 = max
âˆ¥xâˆ¥2=1 âˆ¥Axâˆ¥2 = âˆ¥Aâˆ¥2 ,
and if
y0 =
Ax0
âˆ¥Ax0âˆ¥2
= Ax0
âˆ¥Aâˆ¥2
,
then
yâˆ—
0Ax0 = xâˆ—
0Aâˆ—Ax0
âˆ¥Aâˆ¥2
= âˆ¥Ax0âˆ¥2
2
âˆ¥Aâˆ¥2
= âˆ¥Aâˆ¥2
2
âˆ¥Aâˆ¥2
= âˆ¥Aâˆ¥2 .
(b)
This follows directly from the result of part (a) because
âˆ¥Aâˆ¥2 = max
âˆ¥xâˆ¥2=1
âˆ¥yâˆ¥2=1
|yâˆ—Ax| = max
âˆ¥xâˆ¥2=1
âˆ¥yâˆ¥2=1
|(yâˆ—Ax)âˆ—| = max
âˆ¥xâˆ¥2=1
âˆ¥yâˆ¥2=1
|xâˆ—Aâˆ—y| = âˆ¥Aâˆ—âˆ¥2 .
(c)
Use part (a) with the CBS inequality to write
âˆ¥Aâˆ—Aâˆ¥2 = max
âˆ¥xâˆ¥2=1
âˆ¥yâˆ¥2=1
|yâˆ—Aâˆ—Ax| â‰¤max
âˆ¥xâˆ¥2=1
âˆ¥yâˆ¥2=1
âˆ¥Ayâˆ¥2 âˆ¥Axâˆ¥2 = âˆ¥Aâˆ¥2
2 .
To see that equality is attained, let x = y = x0, where x0 is a vector of unit
length such that âˆ¥Ax0âˆ¥2 = maxâˆ¥xâˆ¥2=1 âˆ¥Axâˆ¥2 = âˆ¥Aâˆ¥2 , and observe
|xâˆ—
0Aâˆ—Ax0| = xâˆ—
0Aâˆ—Ax0 = âˆ¥Ax0âˆ¥2
2 = âˆ¥Aâˆ¥2
2 .
(d)
Let D =

A
0
0
B

. We know from (5.2.7) that âˆ¥Dâˆ¥2
2 is the largest value
Î» such that DT D âˆ’Î»I is singular. But DT D âˆ’Î»I is singular if and only if
AT A âˆ’Î»I or BT B âˆ’Î»I is singular, so Î»max(D) = max {Î»max(A), Î»max(B)} .
(e)
If UUâˆ—= I, then âˆ¥Uâˆ—Axâˆ¥2
2 = xâˆ—Aâˆ—UUâˆ—Ax = xâˆ—Aâˆ—Ax = âˆ¥Axâˆ¥2
2, so
âˆ¥Uâˆ—Aâˆ¥2 = maxâˆ¥xâˆ¥2=1 âˆ¥Uâˆ—Axâˆ¥2 = maxâˆ¥xâˆ¥2=1 âˆ¥Axâˆ¥2 = âˆ¥Aâˆ¥2. Now, if Vâˆ—V =
I, use what was just established with part (b) to write
âˆ¥AVâˆ¥2 = âˆ¥(AV)âˆ—âˆ¥2 = âˆ¥Vâˆ—Aâˆ—âˆ¥2 = âˆ¥Aâˆ—âˆ¥2 = âˆ¥Aâˆ¥2
=â‡’
âˆ¥Uâˆ—AVâˆ¥2 = âˆ¥Aâˆ¥2.

Solutions
55
5.2.7. Proceed as follows.
1
min
âˆ¥xâˆ¥=1
,,Aâˆ’1x
,, = max
âˆ¥xâˆ¥=1

1
âˆ¥Aâˆ’1xâˆ¥
+
= max
yÌ¸=0
ï£±
ï£²
ï£³
1
,,,Aâˆ’1 (Ay)
âˆ¥Ayâˆ¥
,,,
ï£¼
ï£½
ï£¾
= max
yÌ¸=0
âˆ¥Ayâˆ¥
âˆ¥Aâˆ’1(Ay)âˆ¥= max
yÌ¸=0
âˆ¥Ayâˆ¥
âˆ¥yâˆ¥
= max
yÌ¸=0
,,,,A
 y
âˆ¥yâˆ¥
,,,,
= max
âˆ¥xâˆ¥=1 âˆ¥Axâˆ¥= âˆ¥Aâˆ¥
5.2.8. Use (5.2.6) on p. 280 to write âˆ¥(zIâˆ’A)âˆ’1âˆ¥= (1/ minâˆ¥xâˆ¥=1 âˆ¥(zI âˆ’A)xâˆ¥), and let
w be a vector for which âˆ¥wâˆ¥= 1 and âˆ¥(zI âˆ’A)wâˆ¥= minâˆ¥xâˆ¥=1 âˆ¥(zI âˆ’A)xâˆ¥.
Use âˆ¥Awâˆ¥â‰¤âˆ¥Aâˆ¥< |z| together with the â€œbackward triangle inequalityâ€ from
Example 5.1.1 (p. 273) to write
âˆ¥(zI âˆ’A)wâˆ¥= âˆ¥zw âˆ’Awâˆ¥â‰¥
âˆ¥zwâˆ¥âˆ’âˆ¥Awâˆ¥
 =
|z| âˆ’âˆ¥Awâˆ¥

= |z| âˆ’âˆ¥Awâˆ¥â‰¥|z| âˆ’âˆ¥Aâˆ¥.
Consequently, minâˆ¥xâˆ¥=1 âˆ¥(zI âˆ’A)xâˆ¥= âˆ¥(zI âˆ’A)wâˆ¥â‰¥|z| âˆ’âˆ¥Aâˆ¥implies that
âˆ¥(zI âˆ’A)âˆ’1âˆ¥=
1
min
âˆ¥xâˆ¥=1 âˆ¥(zI âˆ’A)xâˆ¥â‰¤
1
|z| âˆ’âˆ¥Aâˆ¥.
Solutions for exercises in section 5. 3
5.3.1. Only (c) is an inner product. The expressions in (a) and (b) each fail the ï¬rst
condition of the deï¬nition (5.3.1), and (d) fails the second.
5.3.2. (a)
âŸ¨x yâŸ©= 0
âˆ€x âˆˆV
=â‡’
âŸ¨y yâŸ©= 0
=â‡’
y = 0.
(b)
âŸ¨Î±x yâŸ©= âŸ¨y Î±xâŸ©= Î± âŸ¨y xâŸ©= Î±âŸ¨y xâŸ©= Î± âŸ¨x yâŸ©
(c)
âŸ¨x + y zâŸ©= âŸ¨z x + yâŸ©= âŸ¨z xâŸ©+ âŸ¨z yâŸ©= âŸ¨z xâŸ©+ âŸ¨z yâŸ©= âŸ¨x zâŸ©+ âŸ¨y zâŸ©
5.3.3. The ï¬rst property in (5.2.3) holds because âŸ¨x xâŸ©â‰¥0 for all x âˆˆV implies
âˆ¥xâˆ¥=
-
âŸ¨x xâŸ©â‰¥0, and âˆ¥xâˆ¥= 0 â‡â‡’âŸ¨x xâŸ©= 0 â‡â‡’x = 0. The second
property in (5.2.3) holds because
âˆ¥Î±xâˆ¥2 = âŸ¨Î±x Î±xâŸ©= Î± âŸ¨Î±x xâŸ©= Î±âŸ¨x Î±xâŸ©= Î±Î±âŸ¨x xâŸ©= |Î±|2 âŸ¨x xâŸ©= |Î±|2 âˆ¥xâˆ¥2 .
5.3.4. 0 â‰¤âˆ¥x âˆ’yâˆ¥2 = âŸ¨x âˆ’y x âˆ’yâŸ©= âŸ¨x xâŸ©âˆ’2 âŸ¨x yâŸ©+âŸ¨y yâŸ©= âˆ¥xâˆ¥2âˆ’2 âŸ¨x yâŸ©+âˆ¥yâˆ¥2
5.3.5. (a)
Use the CBS inequality with the Frobenius matrix norm and the standard
inner product as illustrated in Example 5.3.3, and set A = I.
(b)
Proceed as in part (a), but this time set A = BT (recall from Example
3.6.5 that trace

BT B
	
= trace

BBT 	
).

56
Solutions
(c) Use the result of Exercise 5.3.4 with the Frobenius matrix norm and the inner
product for matrices.
5.3.6. Suppose that parallelogram identity holds, and verify that (5.3.10) satisï¬es the
four conditions in (5.3.1). The ï¬rst condition follows because âŸ¨x xâŸ©r = âˆ¥xâˆ¥2 and
âŸ¨ix xâŸ©r = 0 combine to yield âŸ¨x xâŸ©= âˆ¥xâˆ¥2 . The second condition (for real Î± )
and third condition hold by virtue of the argument for (5.3.7). We will prove the
fourth condition and then return to show that the second holds for complex Î±.
By observing that âŸ¨x yâŸ©r = âŸ¨y xâŸ©r and âŸ¨ix iyâŸ©r = âŸ¨x yâŸ©r , we have
âŸ¨iy xâŸ©r =
.
iy âˆ’i2x
/
r = âŸ¨y âˆ’ixâŸ©r = âˆ’âŸ¨y ixâŸ©r = âˆ’âŸ¨ix yâŸ©r ,
and hence
âŸ¨y xâŸ©= âŸ¨y xâŸ©r + i âŸ¨iy xâŸ©r = âŸ¨y xâŸ©r âˆ’i âŸ¨ix yâŸ©r = âŸ¨x yâŸ©r âˆ’i âŸ¨ix yâŸ©r = âŸ¨x yâŸ©.
Now prove that âŸ¨x Î±yâŸ©= Î± âŸ¨x yâŸ©for all complex Î±. Begin by showing it is
true for Î± = i.
âŸ¨x iyâŸ©= âŸ¨x iyâŸ©r + i âŸ¨ix iyâŸ©r = âŸ¨x iyâŸ©r + i âŸ¨x yâŸ©r = âŸ¨iy xâŸ©r + i âŸ¨x yâŸ©r
= âˆ’âŸ¨ix yâŸ©r + i âŸ¨x yâŸ©r = i (âŸ¨x yâŸ©r + i âŸ¨ix yâŸ©r)
= i âŸ¨x yâŸ©
For Î± = Î¾ + iÎ·,
âŸ¨x Î±yâŸ©= âŸ¨x Î¾y + iÎ·yâŸ©= âŸ¨x Î¾yâŸ©+ âŸ¨x iÎ·yâŸ©= Î¾ âŸ¨x yâŸ©+ iÎ· âŸ¨x yâŸ©= Î± âŸ¨x yâŸ©.
Conversely, if âŸ¨â‹†â‹†âŸ©is any inner product on V, then with âˆ¥â‹†âˆ¥2 = âŸ¨â‹†â‹†âŸ©we have
âˆ¥x + yâˆ¥2 + âˆ¥x âˆ’yâˆ¥2 = âŸ¨x + y x + yâŸ©+ âŸ¨x âˆ’y x âˆ’yâŸ©
= âˆ¥xâˆ¥2 + 2Re âŸ¨x yâŸ©+ âˆ¥yâˆ¥2 + âˆ¥xâˆ¥2 âˆ’2Re âŸ¨x yâŸ©+ âˆ¥yâˆ¥2
= 2

âˆ¥xâˆ¥2 + âˆ¥yâˆ¥2
.
5.3.7. The parallelogram identity (5.3.7) fails to hold for all x, y âˆˆCn. For example,
if x = e1 and y = e2, then
âˆ¥e1 + e2âˆ¥2
âˆ+ âˆ¥e1 âˆ’e2âˆ¥2
âˆ= 2,
but
2

âˆ¥e1âˆ¥2
âˆ+ âˆ¥e2âˆ¥2
âˆ
	
= 4.
5.3.8. (a) As shown in Example 5.3.2, the Frobenius matrix norm CnÃ—n is generated
by the standard matrix inner product (5.3.2), so the result on p. 290 guarantees
that âˆ¥â‹†âˆ¥F satisï¬es the parallelogram identity.
5.3.9. No, because the parallelogram inequality (5.3.7) doesnâ€™t hold. To see that
âˆ¥X + Yâˆ¥2 + âˆ¥X âˆ’Yâˆ¥2 = 2

âˆ¥Xâˆ¥2 + âˆ¥Yâˆ¥2 	
is not valid for all X, Y âˆˆCnÃ—n,
let X = diag (1, 0, . . . , 0) and Y = diag (0, 1, . . . , 0) . For â‹†= 1, 2, or âˆ,
âˆ¥X + Yâˆ¥2
â‹†+ âˆ¥X âˆ’Yâˆ¥2
â‹†= 1 + 1 = 2,
but
2

âˆ¥Xâˆ¥2
â‹†+ âˆ¥Yâˆ¥2
â‹†
	
= 4.

Solutions
57
Solutions for exercises in section 5. 4
5.4.1. (a), (b), and (e) are orthogonal pairs.
5.4.2. First ï¬nd v =

Î±1
Î±2

such that 3Î±1 âˆ’2Î±2 = 0, and then normalize v. The
second must be the negative of v.
5.4.3. (a)
Simply verify that xT
i xj = 0 for i Ì¸= j.
(b)
Let xT
4 = ( Î±1
Î±2
Î±3
Î±4 ) , and notice that xT
i x4 = 0 for i = 1, 2, 3
is three homogeneous equations in four unknowns
ï£«
ï£­
1
âˆ’1
0
2
1
1
1
0
âˆ’1
âˆ’1
2
0
ï£¶
ï£¸
ï£«
ï£¬
ï£­
Î±1
Î±2
Î±3
Î±4
ï£¶
ï£·
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸
=â‡’
ï£«
ï£¬
ï£­
Î±1
Î±2
Î±3
Î±4
ï£¶
ï£·
ï£¸= Î²
ï£«
ï£¬
ï£­
âˆ’1
1
0
1
ï£¶
ï£·
ï£¸.
(c)
Simply normalize the set by dividing each vector by its norm.
5.4.4. The Fourier coeï¬ƒcients are
Î¾1 = âŸ¨u1 xâŸ©=
1
âˆš
2,
Î¾2 = âŸ¨u2 xâŸ©= âˆ’1
âˆš
3,
Î¾3 = âŸ¨u3 xâŸ©= âˆ’5
âˆš
6,
so
x = Î¾1u1 + Î¾2u2 + Î¾3u3 = 1
2
ï£«
ï£­
1
âˆ’1
0
ï£¶
ï£¸âˆ’1
3
ï£«
ï£­
1
1
1
ï£¶
ï£¸âˆ’5
6
ï£«
ï£­
âˆ’1
âˆ’1
2
ï£¶
ï£¸.
5.4.5. If U1, U2, U3, and U4 denote the elements of B, verify they constitute an
orthonormal set by showing that
âŸ¨Ui UjâŸ©= trace(UT
i Uj) = 0 for i Ì¸= j
and
âˆ¥Uiâˆ¥=
0
trace(UT
i Ui) = 1.
Consequently, B is linearly independentâ€”recall (5.4.2)â€”and therefore B is a
basis because it is a maximal independent setâ€”part (b) of Exercise 4.4.4 insures
dim â„œ2Ã—2 = 4. The Fourier coeï¬ƒcients âŸ¨Ui AâŸ©= trace(UT
i A) are
âŸ¨U1 AâŸ©=
2
âˆš
2,
âŸ¨U2 AâŸ©= 0,
âŸ¨U3 AâŸ©= 1,
âŸ¨U4 AâŸ©= 1,
so the Fourier expansion of A is A = (2/
âˆš
2)U1 + U3 + U4.
5.4.6. cos Î¸ = xT y/ âˆ¥xâˆ¥âˆ¥yâˆ¥= 1/2, so Î¸ = Ï€/3.
5.4.7. This follows because each vector has a unique representation in terms of a basisâ€”
see Exercise 4.4.8 or the discussion of coordinates in Â§4.7.
5.4.8. If the columns of U = [u1 | u2 | Â· Â· Â· | un] are an orthonormal basis for Cn, then
[Uâˆ—U]ij = uâˆ—
i uj =

1
when i = j,
0
when i Ì¸= j,
(â€¡)

58
Solutions
and, therefore, Uâˆ—U = I. Conversely, if Uâˆ—U = I, then ( â€¡ ) holds, so the
columns of U are orthonormalâ€”they are a basis for Cn because orthonormal
sets are always linearly independent.
5.4.9. Equations (4.5.5) and (4.5.6) guarantee that
R (A) = R (AAâˆ—)
and
N (A) = N (Aâˆ—A),
and consequently r âˆˆR (A) = R (AAâˆ—)
=â‡’
r = AAâˆ—x for some x, and
n âˆˆN (A) = N (Aâˆ—A)
=â‡’
Aâˆ—An = 0. Therefore,
âŸ¨r nâŸ©= râˆ—n = xâˆ—AAâˆ—n = xâˆ—Aâˆ—An = 0.
5.4.10. (a) Ï€/4
(b) Ï€/2
5.4.11. The number xT y or xâˆ—y will in general be complex. In order to guarantee that
we end up with a real number, we should take
cos Î¸ = |Re (xâˆ—y) |
âˆ¥xâˆ¥âˆ¥yâˆ¥.
5.4.12. Use the Fourier expansion y = 
i âŸ¨ui yâŸ©ui together with the various properties
of an inner product to write
âŸ¨x yâŸ©=
1
x

i
âŸ¨ui yâŸ©ui
2
=

i
âŸ¨x âŸ¨ui yâŸ©uiâŸ©=

i
âŸ¨ui yâŸ©âŸ¨x uiâŸ©.
5.4.13. In a real space, âŸ¨x yâŸ©= âŸ¨y xâŸ©, so the third condition in the deï¬nition (5.3.1)
of an inner product and Exercise 5.3.2(c) produce
âŸ¨x + y x âˆ’yâŸ©= âŸ¨x + y xâŸ©âˆ’âŸ¨x + y yâŸ©
= âŸ¨x xâŸ©+ âŸ¨y xâŸ©âˆ’âŸ¨x yâŸ©âˆ’âŸ¨y yâŸ©
= âˆ¥xâˆ¥2 âˆ’âˆ¥yâˆ¥2 = 0.
5.4.14. (a)
In a real space, âŸ¨x yâŸ©= âŸ¨y xâŸ©, so the third condition in the deï¬nition
(5.3.1) of an inner product and Exercise 5.3.2(c) produce
âˆ¥x + yâˆ¥2 = âŸ¨x + y x + yâŸ©= âŸ¨x + y xâŸ©+ âŸ¨x + y yâŸ©
= âŸ¨x xâŸ©+ âŸ¨y xâŸ©+ âŸ¨x yâŸ©+ âŸ¨y yâŸ©
= âˆ¥xâˆ¥2 + 2 âŸ¨x yâŸ©+ âˆ¥yâˆ¥2 ,
and hence âŸ¨x yâŸ©= 0 if and only if âˆ¥x + yâˆ¥2 = âˆ¥xâˆ¥2 + âˆ¥yâˆ¥2 .
(b)
In a complex space, x âŠ¥y
=â‡’
âˆ¥x + yâˆ¥2 = âˆ¥xâˆ¥2 + âˆ¥yâˆ¥2 , but the
converse is not validâ€”e.g., consider C2 with the standard inner product, and
let x =

âˆ’i
1

and y =

1
i

.

Solutions
59
(c)
Again, using the properties of a general inner product, derive the expansion
âˆ¥Î±x + Î²yâˆ¥2 = âŸ¨Î±x + Î²y Î±x + Î²yâŸ©
= âŸ¨Î±x Î±xâŸ©+ âŸ¨Î±x Î²yâŸ©+ âŸ¨Î²y Î±xâŸ©+ âŸ¨Î²y Î²yâŸ©
= âˆ¥Î±xâˆ¥2 + Î±Î² âŸ¨x yâŸ©+ Î²Î± âŸ¨y xâŸ©+ âˆ¥Î²yâˆ¥2 .
Clearly, x âŠ¥y
=â‡’
âˆ¥Î±x + Î²yâˆ¥2 = âˆ¥Î±xâˆ¥2 + âˆ¥Î²yâˆ¥2 âˆ€Î±, Î². Conversely, if
âˆ¥Î±x + Î²yâˆ¥2 = âˆ¥Î±xâˆ¥2 + âˆ¥Î²yâˆ¥2 âˆ€Î±, Î², then Î±Î² âŸ¨x yâŸ©+ Î²Î± âŸ¨y xâŸ©= 0 âˆ€Î±, Î².
Letting Î± = âŸ¨x yâŸ©and Î² = 1 produces the conclusion that 2| âŸ¨x yâŸ©|2 = 0,
and thus âŸ¨x yâŸ©= 0.
5.4.15. (a)
cos Î¸i = âŸ¨ui xâŸ©/ âˆ¥uiâˆ¥âˆ¥xâˆ¥= âŸ¨ui xâŸ©/ âˆ¥xâˆ¥= Î¾i/ âˆ¥xâˆ¥
(b)
Use the Pythagorean theorem (Exercise 5.4.14) to write
âˆ¥xâˆ¥2 = âˆ¥Î¾1u1 + Î¾2u2 + Â· Â· Â· + Î¾nunâˆ¥2
= âˆ¥Î¾1u1âˆ¥2 + âˆ¥Î¾2u2âˆ¥2 + Â· Â· Â· + âˆ¥Î¾nunâˆ¥2
= |Î¾1|2 + |Î¾2|2 + Â· Â· Â· + |Î¾n|2.
5.4.16. Use the properties of an inner product to write
,,,,,x âˆ’
k

i=1
Î¾iui
,,,,,
2
=
1
x âˆ’
k

i=1
Î¾iui x âˆ’
k

i=1
Î¾iui
2
= âŸ¨x xâŸ©âˆ’2

i
|Î¾i|2 +
1 k

i=1
Î¾iui
k

i=1
Î¾iui
2
= âˆ¥xâˆ¥2 âˆ’2

i
|Î¾i|2 +
,,,,,
k

i=1
Î¾iui
,,,,,
2
,
and then invoke the Pythagorean theorem (Exercise 5.4.14) to conclude
,,,,,
k

i=1
Î¾iui
,,,,,
2
=

i
âˆ¥Î¾iuiâˆ¥2 =

i
|Î¾i|2.
Consequently,
0 â‰¤
,,,,,x âˆ’
k

i=1
Î¾iui
,,,,,
2
= âˆ¥xâˆ¥2 âˆ’

i
|Î¾i|2
=â‡’
k

i=1
|Î¾i|2 â‰¤âˆ¥xâˆ¥2 .
(â€¡)
If x âˆˆspan {u1, u2, . . . , uk} , then the Fourier expansion of x with respect
to the ui â€™s is x = 
k
i=1 Î¾iui, and hence equality holds in (â€¡). Conversely, if
equality holds in (â€¡), then x âˆ’
k
i=1 Î¾iui = 0.

60
Solutions
5.4.17. Choose any unit vector ei for y. The angle between e and ei approaches Ï€/2
as n â†’âˆ, but eT ei = 1 for all n.
5.4.18. If y is negatively correlated to x, then zx = âˆ’zy, but âˆ¥zx âˆ’zyâˆ¥2 = 2âˆšn
gives no indication of the fact that zx and zy are on the same line. Continuity
therefore dictates that when y â‰ˆÎ²0e + Î²1x with Î²1 < 0, then zx â‰ˆâˆ’zy, but
âˆ¥zx âˆ’zyâˆ¥2 â‰ˆ2âˆšn gives no hint that zx and zy are almost on the same line.
If we want to use norms to gauge linear correlation, we should use
min
)
âˆ¥zx âˆ’zyâˆ¥2 , âˆ¥zx + zyâˆ¥2
*
.
5.4.19. (a) cos Î¸ = 1
=â‡’
âŸ¨x yâŸ©= âˆ¥xâˆ¥âˆ¥yâˆ¥> 0, and the straightforward extension of
Exercise 5.1.9 guarantees that
y = âŸ¨x yâŸ©
âˆ¥xâˆ¥2 x,
and clearly
âŸ¨x yâŸ©
âˆ¥xâˆ¥2 > 0.
Conversely, if y = Î±x for Î± > 0, then âŸ¨x yâŸ©= Î± âˆ¥xâˆ¥2
=â‡’
cos Î¸ = 1.
(b)
cos Î¸ = âˆ’1
=â‡’
âŸ¨x yâŸ©= âˆ’âˆ¥xâˆ¥âˆ¥yâˆ¥< 0, so the generalized version of
Exercise 5.1.9 guarantees that
y = âŸ¨x yâŸ©
âˆ¥xâˆ¥2 x,
and in this case
âŸ¨x yâŸ©
âˆ¥xâˆ¥2 < 0.
Conversely, if y = Î±x for Î± < 0, then âŸ¨x yâŸ©= Î± âˆ¥xâˆ¥2 , so
cos Î¸ = Î± âˆ¥xâˆ¥2
|Î±| âˆ¥xâˆ¥2 = âˆ’1.
5.4.20. F(t) = 
âˆ
n (âˆ’1)n 2
n sin nt.
Solutions for exercises in section 5. 5
5.5.1. (a)
u1 = 1
2
ï£«
ï£¬
ï£­
1
1
1
âˆ’1
ï£¶
ï£·
ï£¸,
u2 =
1
2
âˆš
3
ï£«
ï£¬
ï£­
3
âˆ’1
âˆ’1
1
ï£¶
ï£·
ï£¸,
u3 =
1
âˆš
6
ï£«
ï£¬
ï£­
0
1
1
2
ï£¶
ï£·
ï£¸
(b)
First verify this is an orthonormal set by showing uT
i uj =

1
when i = j,
0
when i Ì¸= j.
To show that the xi â€™s and the ui â€™s span the same space, place the xi â€™s as rows
in a matrix A, and place the ui â€™s as rows in a matrix B, and then verify that
EA = EBâ€”recall Example 4.2.2.
(c)
The result should be the same as in part (a).

Solutions
61
5.5.2. First reduce A to EA to determine a â€œregularâ€ basis for each space.
R (A) = span
ï£±
ï£²
ï£³
ï£«
ï£­
1
2
3
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
N

AT 	
= span
ï£±
ï£²
ï£³
ï£«
ï£­
âˆ’2
1
0
ï£¶
ï£¸,
ï£«
ï£­
âˆ’3
0
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
R

AT 	
= span
ï£±
ï£´
ï£²
ï£´
ï£³
ï£«
ï£¬
ï£­
1
âˆ’2
3
âˆ’1
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
N (A) = span
ï£±
ï£´
ï£²
ï£´
ï£³
ï£«
ï£¬
ï£­
2
1
0
0
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
âˆ’3
0
1
0
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
1
0
0
1
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
Now apply Gramâ€“Schmidt to each of these.
R (A) = span
ï£±
ï£²
ï£³
1
âˆš
14
ï£«
ï£­
1
2
3
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
N

AT 	
= span
ï£±
ï£²
ï£³
1
âˆš
5
ï£«
ï£­
âˆ’2
1
0
ï£¶
ï£¸,
1
âˆš
70
ï£«
ï£­
âˆ’3
âˆ’6
5
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
R

AT 	
= span
ï£±
ï£´
ï£²
ï£´
ï£³
1
âˆš
15
ï£«
ï£¬
ï£­
1
âˆ’2
3
âˆ’1
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
N (A) = span
ï£±
ï£´
ï£²
ï£´
ï£³
1
âˆš
5
ï£«
ï£¬
ï£­
2
1
0
0
ï£¶
ï£·
ï£¸,
1
âˆš
70
ï£«
ï£¬
ï£­
âˆ’3
6
5
0
ï£¶
ï£·
ï£¸,
1
âˆš
210
ï£«
ï£¬
ï£­
1
âˆ’2
3
14
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
5.5.3.
u1 =
1
âˆš
3
ï£«
ï£­
i
i
i
ï£¶
ï£¸,
u2 =
1
âˆš
6
ï£«
ï£­
âˆ’2i
i
i
ï£¶
ï£¸,
u3 =
1
âˆš
2
ï£«
ï£­
0
âˆ’i
i
ï£¶
ï£¸
5.5.4. Nothing! The resulting orthonormal set is the same as the original.
5.5.5. It breaks down at the ï¬rst vector such that xk âˆˆspan {x1, x2, . . . , xkâˆ’1} because
if
xk âˆˆspan {x1, x2, . . . , xkâˆ’1} = span {u1, u2, . . . , ukâˆ’1} ,
then the Fourier expansion of xk with respect to span {u1, u2, . . . , ukâˆ’1} is
xk =
kâˆ’1

i=1
âŸ¨ui xkâŸ©ui,
and therefore
uk =

xk âˆ’
kâˆ’1
i=1 âŸ¨ui xkâŸ©ui

,,,

xk âˆ’
kâˆ’1
i=1 âŸ¨ui xkâŸ©ui
,,,
=
0
âˆ¥0âˆ¥

62
Solutions
is not deï¬ned.
5.5.6. (a)
The rectangular QR factors are
Q =
ï£«
ï£¬
ï£­
1/
âˆš
3
âˆ’1/
âˆš
3
1/
âˆš
6
1/
âˆš
3
1/
âˆš
3
1/
âˆš
6
1/
âˆš
3
0
âˆ’2/
âˆš
6
0
1/
âˆš
3
0
ï£¶
ï£·
ï£¸
and
R =
ï£«
ï£­
âˆš
3
âˆš
3
âˆ’
âˆš
3
0
âˆš
3
âˆš
3
0
0
âˆš
6
ï£¶
ï£¸.
(b)
Following Example 5.5.3, solve Rx = QT b to get x =
ï£«
ï£­
2/3
1/3
0
ï£¶
ï£¸.
5.5.7. For k = 1, there is nothing to prove. For k > 1, assume that Ok is an
orthonormal basis for Sk. First establish that Ok+1 must be an orthonormal
set. Orthogonality follows because for each j < k + 1,
âŸ¨uj uk+1âŸ©=
1
uj
1
Î½k+1

xk+1 âˆ’
k

i=1
âŸ¨ui xk+1âŸ©ui
2
=
1
Î½k+1

âŸ¨uj xk+1âŸ©âˆ’
1
uj
k

i=1
âŸ¨ui xk+1âŸ©ui
2
=
1
Î½k+1

âŸ¨uj xk+1âŸ©âˆ’
k

i=1
âŸ¨ui xk+1âŸ©âŸ¨uj uiâŸ©

=
1
Î½k+1
(âŸ¨uj xk+1âŸ©âˆ’âŸ¨uj xk+1âŸ©) = 0.
This together with the fact that each ui has unit norm means that Ok+1 is an
orthonormal set. Now assume Ok is a basis for Sk, and prove that Ok+1 is a
basis for Sk+1. If x âˆˆSk+1, then x can be written as a combination
x =
k+1

i=1
Î±ixi =
 k

i=1
Î±ixi

+ Î±k+1xk+1,
where 
k
i=1 Î±ixi âˆˆSk = span (Ok) âŠ‚span (Ok+1) . Couple this together with
the fact that
xk+1 = Î½k+1uk+1 +
k

i=1
âŸ¨ui xk+1âŸ©ui âˆˆspan (Ok+1)
to conclude that x âˆˆspan (Ok+1) . Consequently, Ok+1 spans Sk+1, and there-
fore Ok+1 is a basis for Sk+1 because orthonormal sets are always linearly
independent.

Solutions
63
5.5.8. If A = Q1R1 = Q2R2 are two rectangular QR factorizations, then (5.5.6)
implies AT A = RT
1 R1 = RT
2 R2. It follows from Example 3.10.7 that AT A is
positive deï¬nite, and R1 = R2 because the Cholesky factorization of a positive
deï¬nite matrix is unique. Therefore, Q1 = ARâˆ’1
1
= ARâˆ’1
2
= Q2.
5.5.9. (a)
Step 1:
fl âˆ¥x1âˆ¥= 1, so u1 â†x1.
Step 2:
uT
1 x2 = 1, so
u2 â†x2 âˆ’

uT
1 x2
	
u1 =
ï£«
ï£­
0
0
âˆ’10âˆ’3
ï£¶
ï£¸
and
u2 â†
u2
âˆ¥u2âˆ¥=
ï£«
ï£­
0
0
âˆ’1
ï£¶
ï£¸.
Step 3:
uT
1 x3 = 1 and uT
2 x3 = 0, so
u3 â†x3 âˆ’

uT
1 x3
	
u1 âˆ’

uT
2 x3
	
u2 =
ï£«
ï£­
0
10âˆ’3
âˆ’10âˆ’3
ï£¶
ï£¸and u3 â†
u3
âˆ¥u3âˆ¥=
ï£«
ï£­
0
.709
âˆ’.709
ï£¶
ï£¸.
Therefore, the result of the classical Gramâ€“Schmidt algorithm using 3-digit arith-
metic is
u1 =
ï£«
ï£­
1
0
10âˆ’3
ï£¶
ï£¸,
u2 =
ï£«
ï£­
0
0
âˆ’1
ï£¶
ï£¸,
u3 =
ï£«
ï£­
0
.709
âˆ’.709
ï£¶
ï£¸,
which is not very good because u2 and u3 are not even close to being orthog-
onal.
(b)
Step 1:
fl âˆ¥x1âˆ¥= 1, so
{u1, u2, u3} â†{x1, x2, x3} .
Step 2:
uT
1 u2 = 1 and uT
1 u3 = 1, so
u2 â†u2 âˆ’

uT
1 u2
	
u1 =
ï£«
ï£­
0
0
âˆ’10âˆ’3
ï£¶
ï£¸,
u3 â†u3 âˆ’

uT
1 u3
	
u1 =
ï£«
ï£­
0
10âˆ’3
âˆ’10âˆ’3
ï£¶
ï£¸,
and then
u2 â†
u2
âˆ¥u2âˆ¥=
ï£«
ï£­
0
0
âˆ’1
ï£¶
ï£¸.
Step 3:
uT
2 u3 = 10âˆ’3, so
u3 â†u3 âˆ’

uT
2 u3
	
u2 =
ï£«
ï£­
0
10âˆ’3
0
ï£¶
ï£¸
and
u3 â†
u3
âˆ¥u3âˆ¥=
ï£«
ï£­
0
1
0
ï£¶
ï£¸.

64
Solutions
Thus the modiï¬ed Gramâ€“Schmidt algorithm produces
u1 =
ï£«
ï£­
1
0
10âˆ’3
ï£¶
ï£¸,
u2 =
ï£«
ï£­
0
0
âˆ’1
ï£¶
ï£¸,
u3 =
ï£«
ï£­
0
1
0
ï£¶
ï£¸,
which is as close to being an orthonormal set as one could reasonably hope to
obtain by using 3-digit arithmetic.
5.5.10. Yes. In both cases rij is the (i, j)-entry in the upper-triangular matrix R in the
QR factorization.
5.5.11. p0(x) = 1/
âˆš
2,
p1(x) =
-
3/2 x,
p2(x) =
-
5/8 (3x2 âˆ’1)
Solutions for exercises in section 5. 6
5.6.1. (a), (c), and (d).
5.6.2. Yes, because Uâˆ—U =

1
0
0
1

.
5.6.3. (a)
Eight: D =
ï£«
ï£­
Â±1
0
0
0
Â±1
0
0
0
Â±1
ï£¶
ï£¸
(b)
2n : D =
ï£«
ï£¬
ï£¬
ï£­
Â±1
0
Â· Â· Â·
0
0
Â±1
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
Â±1
ï£¶
ï£·
ï£·
ï£¸
(c) There are inï¬nitely many because each diagonal entry can be any point on the
unit circle in the complex planeâ€”these matrices have the form given in part (d)
of Exercise 5.6.1.
5.6.4. (a)
When Î±2 + Î²2 = 1/2.
(b)
When Î±2 + Î²2 = 1.
5.6.5. (a)
(UV)âˆ—(UV) = Vâˆ—Uâˆ—UV = Vâˆ—V = I.
(b)
Consider I + (âˆ’I) = 0.
(c)

U
0
0
V
âˆ—
U
0
0
V

=

Uâˆ—
0
0
Vâˆ—
 
U
0
0
V

=

Uâˆ—U
0
0
Vâˆ—V

=

I
0
0
I

.
5.6.6. Recall from (3.7.8) or (4.2.10) that (I+A)âˆ’1 exists if and only if N (I + A) = 0,
and write x âˆˆN (I + A)
=â‡’
x = âˆ’Ax
=â‡’
xâˆ—x = âˆ’xâˆ—Ax. But
taking the conjugate transpose of both sides yields xâˆ—x = âˆ’xâˆ—Aâˆ—x = xâˆ—Ax,
so xâˆ—x = 0, and thus x = 0. Replacing A by âˆ’A in Exercise 3.7.6 gives
A(I + A)âˆ’1 = (I + A)âˆ’1A, so
(I âˆ’A)(I + A)âˆ’1 = (I + A)âˆ’1 âˆ’A(I + A)âˆ’1
= (I + A)âˆ’1 âˆ’(I + A)âˆ’1A = (I + A)âˆ’1(I âˆ’A).

Solutions
65
These results together with the fact that A is skew hermitian produce
Uâˆ—U = (I + A)âˆ’1âˆ—(I âˆ’A)âˆ—(I âˆ’A)(I + A)âˆ’1
= (I + A)âˆ—âˆ’1(I âˆ’A)âˆ—(I âˆ’A)(I + A)âˆ’1
= (I âˆ’A)âˆ’1(I + A)(I âˆ’A)(I + A)âˆ’1 = I.
5.6.7. (a)
Yesâ€”because if R = I âˆ’2uuâˆ—, where âˆ¥uâˆ¥= 1, then

I
0
0
R

= I âˆ’2

0
u

( 0
uâˆ—)
and
,,,,

0
u
,,,, = 1.
(b)
Noâ€”Suppose R = I âˆ’2uuâˆ—and S = I âˆ’2vvâˆ—, where âˆ¥uâˆ¥= 1 and
âˆ¥vâˆ¥= 1 so that

R
0
0
S

= I âˆ’2

uuâˆ—
0
0
vvâˆ—

.
If we could ï¬nd a vector w such that âˆ¥wâˆ¥= 1 and

R
0
0
S

= I âˆ’2wwâˆ—,
then
wwâˆ—=

uuâˆ—
0
0
vvâˆ—

.
But this is impossible because (recall Example 3.9.3)
rank (wwâˆ—) = 1
and
rank

uuâˆ—
0
0
vvâˆ—

= 2.
5.6.8. (a)
uâˆ—v = (Ux)âˆ—Uy = xâˆ—Uâˆ—Uy = xâˆ—y
(b)
The fact that P is an isometry means âˆ¥uâˆ¥= âˆ¥xâˆ¥and âˆ¥vâˆ¥= âˆ¥yâˆ¥. Use
this together with part (a) and the deï¬nition of cosine given in (5.4.1) to obtain
cos Î¸u,v =
uT v
âˆ¥uâˆ¥âˆ¥vâˆ¥=
xT y
âˆ¥xâˆ¥âˆ¥yâˆ¥= cos Î¸x,y.
5.6.9. (a) Since UmÃ—r has orthonormal columns, we have Uâˆ—U = Ir so that
âˆ¥Uâˆ¥2
2 = max
âˆ¥xâˆ¥2=1 xâˆ—Uâˆ—Ux = max
âˆ¥xâˆ¥2=1 xâˆ—x = 1.
This together with âˆ¥Aâˆ¥2 = âˆ¥Aâˆ—âˆ¥2â€”recall (5.2.10)â€”implies âˆ¥Vâˆ¥2 = 1. For the
Frobenius norm we have
âˆ¥Uâˆ¥F = [trace (Uâˆ—U)]1/2 = [trace (I)]1/2 = âˆšr.
trace (AB) = trace (BA) (Example 3.6.5) and VVâˆ—= Ik
=â‡’
âˆ¥Vâˆ¥F =
âˆš
k.

66
Solutions
(b)
First show that âˆ¥UAâˆ¥2 = âˆ¥Aâˆ¥2 by writing
âˆ¥UAâˆ¥2
2 = max
âˆ¥xâˆ¥2=1 âˆ¥UAxâˆ¥2
2 = max
âˆ¥xâˆ¥2=1 xâˆ—Aâˆ—Uâˆ—UAx = max
âˆ¥xâˆ¥2=1 xâˆ—Aâˆ—Ax
= max
âˆ¥xâˆ¥2=1 âˆ¥Axâˆ¥2
2 = âˆ¥Aâˆ¥2
2 .
Now use this together with âˆ¥Aâˆ¥2 = âˆ¥Aâˆ—âˆ¥2 to observe that
âˆ¥AVâˆ¥2 = âˆ¥Vâˆ—Aâˆ—âˆ¥2 = âˆ¥Aâˆ—âˆ¥2 = âˆ¥Aâˆ¥2 .
Therefore, âˆ¥UAVâˆ¥2 = âˆ¥U(AV)âˆ¥2 = âˆ¥AVâˆ¥2 = âˆ¥Aâˆ¥2 .
(c)
Use trace (AB) = trace (BA) with Uâˆ—U = Ir and VVâˆ—= Ik to write
âˆ¥UAVâˆ¥2
F = trace

(UAV)âˆ—UAV
	
= trace (Vâˆ—Aâˆ—Uâˆ—UAV)
= trace (Vâˆ—Aâˆ—AV) = trace (Aâˆ—AVVâˆ—)
= trace (Aâˆ—A) = âˆ¥Aâˆ¥2
F .
5.6.10. Use (5.6.6) to compute the following quantities.
(a)
vvT
vT vu =
vT u
vT v

v = 1
6v = 1
6
ï£«
ï£¬
ï£­
1
4
0
âˆ’1
ï£¶
ï£·
ï£¸
(b)
uuT
uT uv =
uT v
uT u

u = 1
5u = 1
5
ï£«
ï£¬
ï£­
âˆ’2
1
3
âˆ’1
ï£¶
ï£·
ï£¸
(c)

I âˆ’vvT
vT v

u = u âˆ’
vT u
vT v

v = u âˆ’1
6v = 1
6
ï£«
ï£¬
ï£­
âˆ’13
2
18
âˆ’5
ï£¶
ï£·
ï£¸
(d)

I âˆ’uuT
uT u

v = v âˆ’
uT v
uT u

u = v âˆ’1
5u = 1
5
ï£«
ï£¬
ï£­
7
19
âˆ’3
âˆ’4
ï£¶
ï£·
ï£¸
5.6.11. (a)
N (Q) Ì¸= {0} because Qu = 0 and âˆ¥uâˆ¥= 1
=â‡’
u Ì¸= 0, so Q must
be singular by (4.2.10).
(b)
The result of Exercise 4.4.10 insures that nâˆ’1 â‰¤rank (Q), and the result
of part (a) says rank (Q) â‰¤n âˆ’1, and therefore rank (Q) = n âˆ’1.
5.6.12. Use (5.6.5) in conjunction with the CBS inequality given in (5.1.3) to write
âˆ¥pâˆ¥= |uâˆ—x| â‰¤âˆ¥uâˆ¥âˆ¥xâˆ¥= âˆ¥xâˆ¥.

Solutions
67
The fact that equality holds if and only if x is a scalar multiple of u follows
from the result of Exercise 5.1.9.
5.6.13. (a)
Set u = x âˆ’âˆ¥xâˆ¥e1 = âˆ’2/3
ï£«
ï£­
1
1
1
ï£¶
ï£¸, and compute
R = I âˆ’2uuT
uT u = 1
3
ï£«
ï£­
1
âˆ’2
âˆ’2
âˆ’2
1
âˆ’2
âˆ’2
âˆ’2
1
ï£¶
ï£¸.
(You could also use u = x + âˆ¥xâˆ¥e1. )
(b)
Verify that R = RT , RT R = I, and R2 = I.
(c)
The columns of the reï¬‚ector R computed in part (a) do the job.
5.6.14. Rx = x
=â‡’
2uuâˆ—x = 0
=â‡’
uâˆ—x = 0 because u Ì¸= 0.
5.6.15. If Rx = y in Figure 5.6.2, then the line segment between x âˆ’y is parallel to
the line determined by u, so x âˆ’y itself must be a scalar multiple of u. If
x âˆ’y = Î±u, then
u = x âˆ’y
Î±
=
x âˆ’y
âˆ¥x âˆ’yâˆ¥.
It is straightforward to verify that this choice of u produces the desired reï¬‚ector.
5.6.16. You can verify by direct multiplication that PT P = I and Uâˆ—U = I, but you
can also recognize that P and U are elementary reï¬‚ectors that come from
Example 5.6.3 in the sense that
P = I âˆ’2uuT
uT u,
where
u = x âˆ’e1 =

x1 âˆ’1
Ëœx

and
U = Âµ

I âˆ’2uuâˆ—
uâˆ—u

,
where
u = x âˆ’Âµe1 =

x1 âˆ’Âµ
Ëœx

.
5.6.17. The ï¬nal result is
v3 =
ï£«
ï£­
âˆ’
âˆš
2/2
âˆš
6/2
1
ï£¶
ï£¸
and
Q = Pz(Ï€/6)Py(âˆ’Ï€/2)Px(Ï€/4) = 1
4
ï£«
ï£­
0
âˆ’
âˆš
6 âˆ’
âˆš
2
âˆ’
âˆš
6 +
âˆš
2
0
âˆš
6 âˆ’
âˆš
2
âˆ’
âˆš
6 âˆ’
âˆš
2
4
0
0
ï£¶
ï£¸.
5.6.18. It matters because the rotation matrices given on p. 328 generally do not com-
mute with each other (this is easily veriï¬ed by direct multiplication). For exam-
ple, this means that it is generally the case that
Py(Ï†)Px(Î¸)v Ì¸= Px(Î¸)Py(Ï†)v.

68
Solutions
5.6.19. As pointed out in Example 5.6.2, uâŠ¥= (u/ âˆ¥uâˆ¥)âŠ¥, so we can assume without
any loss of generality that u has unit norm. We also know that any vector of
unit norm can be extended to an orthonormal basis for Cnâ€”Examples 5.6.3 and
5.6.6 provide two possible ways to accomplish this. Let {u, v1, v2, . . . , vnâˆ’1}
be such an orthonormal basis for Cn.
Claim: span {v1, v2, . . . , vnâˆ’1} = uâŠ¥.
Proof. x âˆˆspan {v1, v2, . . . , vnâˆ’1}
=â‡’
x = 
i Î±ivi
=â‡’
uâˆ—x =

i Î±iuâˆ—vi = 0
=â‡’
x âˆˆuâŠ¥, and thus span {v1, v2, . . . , vnâˆ’1} âŠ†uâŠ¥.
To establish the reverse inclusion, write x = Î±0u + 
i Î±ivi, and then note
that x âŠ¥u
=â‡’
0 = uâˆ—x = Î±0
=â‡’
x âˆˆspan {v1, v2, . . . , vnâˆ’1} , and
hence
=â‡’
uâŠ¥âŠ†span {v1, v2, . . . , vnâˆ’1} .
Consequently, {v1, v2, . . . , vnâˆ’1} is a basis for uâŠ¥because it is a spanning set
that is linearly independentâ€”recall (4.3.14)â€”and thus dim uâŠ¥= n âˆ’1.
5.6.20. The relationship between the matrices in (5.6.6) and (5.6.7) on p. 324 suggests
that if P is a projector, then A = I âˆ’2P is an involutionâ€”and indeed this
is true because A2 = (I âˆ’2P)2 = I âˆ’4P + 4P2 = I. Similarly, if A is an
involution, then P = (I âˆ’A)/2 is easily veriï¬ed to be a projector. Thus each
projector uniquely deï¬nes an involution, and vice versa.
5.6.21. The outside of the face is visible from the perspective indicated in Figure 5.6.6
if and only if the angle Î¸ between n and the positive x-axis is between âˆ’90â—¦
and +90â—¦. This is equivalent to saying that the cosine between n and e1 is
positive, so the desired conclusion follows from the fact that
cos Î¸ > 0 â‡â‡’
nT e1
âˆ¥nâˆ¥âˆ¥e1âˆ¥> 0 â‡â‡’nT e1 > 0 â‡â‡’n1 > 0.
Solutions for exercises in section 5. 7
5.7.1. (a)
Householder reduction produces
R2R1A =
ï£«
ï£­
1
0
0
0
âˆ’3/5
4/5
0
4/5
3/5
ï£¶
ï£¸
ï£«
ï£­
1/3
âˆ’2/3
2/3
âˆ’2/3
1/3
2/3
2/3
2/3
1/3
ï£¶
ï£¸
ï£«
ï£­
1
19
âˆ’34
âˆ’2
âˆ’5
20
2
8
37
ï£¶
ï£¸
=
ï£«
ï£­
3
15
0
0
15
âˆ’30
0
0
45
ï£¶
ï£¸= R,
so
Q = (R2R1)T =
ï£«
ï£­
1/3
14/15
âˆ’2/15
âˆ’2/3
1/3
2/3
2/3
âˆ’2/15
11/15
ï£¶
ï£¸.

Solutions
69
(b)
Givens reduction produces P23P13P12A = R, where
P12 =
ï£«
ï£­
1/
âˆš
5
âˆ’2/
âˆš
5
0
2/
âˆš
5
1/
âˆš
5
0
0
0
1
ï£¶
ï£¸
P13 =
ï£«
ï£­
âˆš
5/3
0
2/3
0
1
0
âˆ’2/3
0
âˆš
5/3
ï£¶
ï£¸
P23 =
ï£«
ï£­
1
0
0
0
11/5
âˆš
5
âˆ’2/5
âˆš
5
0
2/5
âˆš
5
11/5
âˆš
5
ï£¶
ï£¸
5.7.2. Since P is an orthogonal matrix, so is PT , and hence the columns of X are
an orthonormal set. By writing
A = PT T = [X | Y]

R
0

= XR,
and by using the fact that rank (A) = n
=â‡’
rank (R) = n, it follows that
R (A) = R (XR) = R (X)â€”recall Exercise 4.5.12. Since every orthonormal set
is linearly independent, the columns of X are a linearly independent spanning
set for R (A), and thus the columns of X are an orthonormal basis for R (A).
Notice that when the diagonal entries of R are positive, A = XR is the
â€œrectangularâ€ QR factorization for A introduced on p. 311, and the columns of
X are the same columns as those produced by the Gramâ€“Schmidt procedure.
5.7.3. According to (5.7.1), set u = Aâˆ—1 âˆ’âˆ¥Aâˆ—1âˆ¥e1 =
ï£«
ï£¬
ï£­
âˆ’1
2
âˆ’2
1
ï£¶
ï£·
ï£¸, so
R1 = Iâˆ’2uuâˆ—
uâˆ—u = 1
5
ï£«
ï£¬
ï£­
4
2
âˆ’2
1
2
1
4
âˆ’2
âˆ’2
4
1
2
1
âˆ’2
2
4
ï£¶
ï£·
ï£¸
and
R1A =
ï£«
ï£¬
ï£­
5
âˆ’15
5
0
10
âˆ’5
0
âˆ’10
2
0
5
14
ï£¶
ï£·
ï£¸.
Next use u =
ï£«
ï£­
10
âˆ’10
5
ï£¶
ï£¸âˆ’
ï£«
ï£­
15
0
0
ï£¶
ï£¸=
ï£«
ï£­
âˆ’5
âˆ’10
5
ï£¶
ï£¸to build
Ë†R2 = I âˆ’2uuâˆ—
uâˆ—u = 1
3
ï£«
ï£­
2
âˆ’2
1
âˆ’2
âˆ’1
2
1
2
2
ï£¶
ï£¸
and
R2 = 1
3
ï£«
ï£¬
ï£­
3
0
0
0
0
2
âˆ’2
1
0
âˆ’2
âˆ’1
2
0
1
2
2
ï£¶
ï£·
ï£¸,
so
R2R1A =
ï£«
ï£¬
ï£­
5
âˆ’15
5
0
15
0
0
0
12
0
0
9
ï£¶
ï£·
ï£¸.

70
Solutions
Finally, with u =

12
9

âˆ’

15
0

=

âˆ’3
9

, build
Ë†R3 = 1
5

4
3
3
âˆ’4

and
R3 = 1
5
ï£«
ï£¬
ï£­
5
0
0
0
0
5
0
0
0
0
4
3
0
0
3
âˆ’4
ï£¶
ï£·
ï£¸,
so that
R3R2R1A =
ï£«
ï£¬
ï£­
5
âˆ’15
5
0
15
0
0
0
15
0
0
0
ï£¶
ï£·
ï£¸.
Therefore, PA = T =

R
0

, where
P = R3R2R1 = 1
15
ï£«
ï£¬
ï£­
12
6
âˆ’6
3
9
âˆ’8
8
âˆ’4
0
âˆ’5
2
14
0
âˆ’10
âˆ’11
âˆ’2
ï£¶
ï£·
ï£¸
and
R =
ï£«
ï£­
5
âˆ’15
5
0
15
0
0
0
15
ï£¶
ï£¸.
The result of Exercise 5.7.2 insures that the ï¬rst three columns in
PT = R1R2R3 = 1
15
ï£«
ï£¬
ï£­
12
9
0
0
6
âˆ’8
âˆ’5
âˆ’10
âˆ’6
8
2
âˆ’11
3
âˆ’4
14
âˆ’2
ï£¶
ï£·
ï£¸
are an orthonormal basis for R (A). Since the diagonal entries of R are positive,
1
15
ï£«
ï£¬
ï£­
12
9
0
6
âˆ’8
âˆ’5
âˆ’6
8
2
3
âˆ’4
14
ï£¶
ï£·
ï£¸
ï£«
ï£­
5
âˆ’15
5
0
15
0
0
0
15
ï£¶
ï£¸= A
is the â€œrectangularâ€ QR factorization for A discussed on p. 311.
5.7.4. If A has full column rank, and if P is an orthogonal matrix such that
PA = T =

R
0

and
Pb =

c
d

,
where R is an upper-triangular matrix, then the results of Example 5.7.3 insure
that the least squares solution of Ax = b can be obtained by solving the
triangular system Rx = c. The matrices P and R were computed in Exercise
5.7.3, so the least squares solution of Ax = b is the solution to
ï£«
ï£­
5
âˆ’15
5
0
15
0
0
0
15
ï£¶
ï£¸
ï£«
ï£­
x1
x2
x3
ï£¶
ï£¸=
ï£«
ï£­
4
3
33
ï£¶
ï£¸
=â‡’
x = 1
5
ï£«
ï£­
âˆ’4
1
11
ï£¶
ï£¸.

Solutions
71
5.7.5. âˆ¥Aâˆ¥F = âˆ¥QRâˆ¥F = âˆ¥Râˆ¥F
because orthogonal matrices are norm preserving
transformationsâ€”recall Exercise 5.6.9.
5.7.6. Follow the procedure outlined in Example 5.7.4 to compute the reï¬‚ector
Ë†R =

âˆ’3/5
4/5
4/5
3/5

,
and then set
R =
ï£«
ï£­
1
0
0
0
âˆ’3/5
4/5
0
4/5
3/5
ï£¶
ï£¸.
Since A is 3 Ã— 3, there is only one step, so P = R and
PT AP = H =
ï£«
ï£­
âˆ’2
âˆ’5
0
âˆ’5
âˆ’41
38
0
38
41
ï£¶
ï£¸.
5.7.7. First argue that the product of an upper-Hessenberg matrix with an upper-
triangular matrix must be upper Hessenbergâ€”regardless of which side the tri-
angular factor appears. This implies that Q is upper Hessenberg because Q =
HRâˆ’1 and Râˆ’1 is upper triangularâ€”recall Exercise 3.7.4. This in turn means
that RQ must be upper Hessenberg.
5.7.8. From the structure of the matrices in Example 5.7.5, it can be seen that P12
requires 4n multiplications, P23 requires 4(nâˆ’1) multiplications, etc. Use the
formula 1 + 2 + Â· Â· Â· + n = n(n + 1)/2 to obtain the total as
4[n + (n âˆ’1) + (n âˆ’2) + Â· Â· Â· + 2] = 4
n2 + n
2
âˆ’1

â‰ˆ2n2.
Solutions for exercises in section 5. 8
5.8.1. (a)
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
4
13
28
27
18
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
(b)
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ’1
0
2
0
âˆ’1
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
(c)
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î±0
Î±0 + Î±1
Î±0 + Î±1 + Î±2
Î±1 + Î±2
Î±2
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
5.8.2. The answer to both parts is
ï£«
ï£¬
ï£­
0
0
0
4
ï£¶
ï£·
ï£¸.
5.8.3. F2 =

1
1
1
âˆ’1

, D2 =

1
0
0
âˆ’i

, and

72
Solutions
F4PT
4 =
ï£«
ï£¬
ï£­
1
1
1
1
1
âˆ’i
âˆ’1
i
1
âˆ’1
1
âˆ’1
1
i
âˆ’1
âˆ’i
ï£¶
ï£·
ï£¸PT
4 =
ï£«
ï£¬
ï£­
1
1
1
1
1
âˆ’1
âˆ’i
i
1
1
âˆ’1
âˆ’1
1
âˆ’1
i
âˆ’i
ï£¶
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1
1
1
âˆ’1

1
0
0
âˆ’i
 
1
1
1
âˆ’1

1
1
1
âˆ’1
âˆ’

1
0
0
âˆ’i
 
1
1
1
âˆ’1

ï£¶
ï£·
ï£·
ï£·
ï£¸=
 F2
D2F2
F2
âˆ’D2F2

.
5.8.4. (a)
a âŠ™b =
ï£«
ï£¬
ï£­
Î±0Î²0
Î±0Î²1 + Î±1Î²0
Î±1Î²1
0
ï£¶
ï£·
ï£¸
F4(a âŠ™b) =
ï£«
ï£¬
ï£­
Î±0Î²0 + Î±0Î²1 + Î±1Î²0 + Î±1Î²1
Î±0Î²0 âˆ’iÎ±0Î²1 âˆ’iÎ±1Î²0 âˆ’Î±1Î²1
Î±0Î²0 âˆ’Î±0Î²1 âˆ’Î±1Î²0 + Î±1Î²1
Î±0Î²0 + iÎ±0Î²1 + iÎ±1Î²0 âˆ’Î±1Î²1
ï£¶
ï£·
ï£¸= (F4Ë†a) Ã— (F4Ë†b)
(b)
Fâˆ’1
4

(F4Ë†a) Ã— (F4Ë†b)

= a âŠ™b
5.8.5. p(x)q(x) = Î³0 + Î³1x + Î³2x2 + Î³3x3, where
ï£«
ï£¬
ï£­
Î³0
Î³1
Î³2
Î³3
ï£¶
ï£·
ï£¸= Fâˆ’1
4

(F4Ë†a) Ã— (F4Ë†b)

= Fâˆ’1
4
ï£®
ï£¯ï£°
ï£«
ï£¬
ï£­
1
1
1
1
1
âˆ’i
âˆ’1
i
1
âˆ’1
1
âˆ’1
1
i
âˆ’1
âˆ’i
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
âˆ’3
2
0
0
ï£¶
ï£·
ï£¸Ã—
ï£«
ï£¬
ï£­
1
1
1
1
1
âˆ’i
âˆ’1
i
1
âˆ’1
1
âˆ’1
1
i
âˆ’1
âˆ’i
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
âˆ’4
3
0
0
ï£¶
ï£·
ï£¸
ï£¹
ï£ºï£»
= Fâˆ’1
4
ï£®
ï£¯ï£°
ï£«
ï£¬
ï£­
âˆ’1
âˆ’3 âˆ’2i
âˆ’5
âˆ’3 + 2i
ï£¶
ï£·
ï£¸Ã—
ï£«
ï£¬
ï£­
âˆ’1
âˆ’4 âˆ’3i
âˆ’7
âˆ’4 + 3i
ï£¶
ï£·
ï£¸
ï£¹
ï£ºï£»
= 1
4
ï£«
ï£¬
ï£­
1
1
1
1
1
i
âˆ’1
âˆ’i
1
âˆ’1
1
âˆ’1
1
âˆ’i
âˆ’1
i
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
1
6 + 17i
35
6 âˆ’17i
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
12
âˆ’17
6
0
ï£¶
ï£·
ï£¸.
5.8.6. (a)

3
4

âŠ™

1
2

=
ï£«
ï£¬
ï£­
3
10
8
0
ï£¶
ï£·
ï£¸, so
4310 Ã— 2110 = (8 Ã— 102) + (10 Ã— 101) + (3 Ã— 100)
= (9 Ã— 102) + (0 Ã— 101) + (3 Ã— 100) = 903.

Solutions
73
(b)
ï£«
ï£­
1
2
3
ï£¶
ï£¸âŠ™
ï£«
ï£­
6
0
1
ï£¶
ï£¸=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
3
2
19
12
6
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
, so
1238 Ã— 6018 = (6 Ã— 84) + (12 Ã— 83) + (19 Ã— 82) + (2 Ã— 81) + (3 Ã— 80).
Since
12 = 8 + 4
=â‡’
12 Ã— 83 = (8 + 4) Ã— 83 = 84 + (4 Ã— 83)
19 = (2 Ã— 8) + 3
=â‡’
19 Ã— 82 = (2 Ã— 83) + (3 Ã— 82),
we have that
1238 Ã— 6018 = (7 Ã— 84) + (6 Ã— 83) + (3 Ã— 82) + (2 Ã— 81) + (3 Ã— 80) = 763238.
(c)
ï£«
ï£¬
ï£­
1
0
1
0
ï£¶
ï£·
ï£¸âŠ™
ï£«
ï£¬
ï£­
1
1
0
1
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
1
0
2
1
1
1
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
, so
10102Ã—11012 = (1Ã—26)+(1Ã—25)+(1Ã—24)+(2Ã—23)+(0Ã—22)+(1Ã—21)+(0Ã—20).
Substituting 2 Ã— 23 = 1 Ã— 24 in this expression and simplifying yields
10102 Ã— 11012 = (1 Ã— 27) + (0 Ã— 26) + (0 Ã— 25) + (0 Ã— 24)
+ (0 Ã— 23) + (0 Ã— 22) + (1 Ã— 21) + (0 Ã— 20)
= 100000102.
5.8.7. (a)
The number of multiplications required by the deï¬nition is
1 + 2 + Â· Â· Â· + (n âˆ’1) + n + (n âˆ’1) + Â· Â· Â· + 2 + 1
= 2

1 + 2 + Â· Â· Â· + (n âˆ’1)

+ n
= (n âˆ’1)n + n = n2.
(b)
In the formula anÃ—1 âŠ™bnÃ—1 = Fâˆ’1
2n

(F2nË†a) Ã— (F2nË†b)

, using the FFT to
compute F2nË†a and F2nË†b requires (2n/2) log2 2n = n(1 + log2 n) multiplica-
tions for each term, and an additional 2n multiplications are needed to form
the product (F2nË†a)Ã—(F2nË†b). Using the FFT in conjunction with the procedure

74
Solutions
described in Example 5.8.2 to apply Fâˆ’1 to (F2nË†a) Ã— (F2nË†b) requires another
(2n/2) log2 2n = n(1 + log2 n) multiplications to compute F2nx followed by
2n more multiplications to produce (1/2n)F2nx = Fâˆ’1
2n x . Therefore, the total
count is 3n(1 + log2 n) + 4n = 3n log2 n + 7n.
5.8.8. Recognize that y is of the form
y = 1(e2 + e6) + 4(e3 + e5) + 5i(âˆ’e1 + e7) + 3i(âˆ’e2 + e6).
The real part says that there are two cosinesâ€”one with amplitude 1 and fre-
quency 2, and the other with amplitude 4 and frequency 3. The imaginary
part says there are two sinesâ€”one with amplitude 5 and frequency 1, and the
other with amplitude 3 and frequency 2. Therefore,
x(Ï„) = cos 4Ï€Ï„ + 4 cos 6Ï€Ï„ + 5 sin 2Ï€Ï„ + 3 sin 4Ï€Ï„.
5.8.9. Use (5.8.12) to write a âŠ™b = Fâˆ’1
(FË†a) Ã— (FË†b)

= Fâˆ’1
(FË†b) Ã— (FË†a)

= a âŠ™b.
5.8.10. This is a special case of the result given in Example 4.3.5. The Fourier matrix
Fn is a special case of the Vandermonde matrixâ€”simply let xk â€™s that deï¬ne
the Vandermonde matrix be the nth roots of unity.
5.8.11. The result of Exercise 5.8.10 implies that if
Ë†a =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î±0
...
Î±nâˆ’1
0
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
2nÃ—1
and
Ë†b =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î²0
...
Î²nâˆ’1
0
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
2nÃ—1
,
then F2nË†a = p and F2nË†b = q, and we know from (5.8.11) that the Î³k â€™s are
given by Î³k = [a âŠ™b]k. Therefore, the convolution theorem guarantees
ï£«
ï£¬
ï£¬
ï£­
Î³0
Î³1
Î³2
...
ï£¶
ï£·
ï£·
ï£¸= a âŠ™b = Fâˆ’1
2n

(F2nË†a) Ã— (F2nË†b)

= Fâˆ’1
2n

p Ã— q

= Fâˆ’1
2n
ï£«
ï£¬
ï£¬
ï£­
p(1)q(1)
p(Î¾)q(Î¾)
p(Î¾2)q(Î¾2)
...
ï£¶
ï£·
ï£·
ï£¸.
5.8.12. (a)
This follows from the observation that Qk has 1â€™s on the kth subdiagonal
and 1â€™s on the (n âˆ’k)th superdiagonal. For example, if n = 8, then
Q3 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.

Solutions
75
(b)
If the rows of F are indexed from 0 to n âˆ’1, then they satisfy the
relationships Fkâˆ—Q = Î¾kFkâˆ—for each k (verifying this for n = 4 will indicate
why it is true in general). This means that FQ = DF, which in turn implies
FQFâˆ’1 = D.
(c)
Couple parts (a) and (b) with FQkFâˆ’1 = (FQFâˆ’1)k = Dk to write
FCFâˆ’1 = Fp(Q)Fâˆ’1
= F(c0I + c1Q + Â· Â· Â· + cnâˆ’1Qnâˆ’1)Fâˆ’1
= c0I + c1FQFâˆ’1 + Â· Â· Â· + cnâˆ’1FQnâˆ’1Fâˆ’1
= c0I + c1D + Â· Â· Â· + cnâˆ’1Dnâˆ’1
=
ï£«
ï£¬
ï£¬
ï£­
p(1)
0
Â· Â· Â·
0
0
p(Î¾)
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
p(Î¾nâˆ’1)
ï£¶
ï£·
ï£·
ï£¸.
(d)
FC1Fâˆ’1 = D1 and FC2Fâˆ’1 = D2, where D1 and D2 are diagonal
matrices, and therefore
C1C2 = Fâˆ’1D1FFâˆ’1D2F = Fâˆ’1D1D2F = Fâˆ’1D2D1F = Fâˆ’1D2FFâˆ’1D1F
= C2C1.
5.8.13. (a)
According to Exercise 5.8.12,
C=
ï£«
ï£¬
ï£¬
ï£­
Ïƒ0
Ïƒnâˆ’1
Â· Â· Â·
Ïƒ1
Ïƒ1
Ïƒ0
Â· Â· Â·
Ïƒ2
...
...
...
...
Ïƒnâˆ’1
Ïƒnâˆ’2
Â· Â· Â·
Ïƒ0
ï£¶
ï£·
ï£·
ï£¸=Fâˆ’1
ï£«
ï£¬
ï£¬
ï£­
p(1)
0
Â· Â· Â·
0
0
p(Î¾)
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
p(Î¾nâˆ’1)
ï£¶
ï£·
ï£·
ï£¸F=Fâˆ’1DF
in which p(x) = Ïƒ0+Ïƒ1x+Â· Â· Â·+Ïƒnâˆ’1xnâˆ’1. Therefore, x = Câˆ’1b = Fâˆ’1Dâˆ’1Fb,
so we can execute the following computations.
(i)
ï£«
ï£¬
ï£¬
ï£­
p(0)
p(Î¾)
...
p(Î¾nâˆ’1)
ï£¶
ï£·
ï£·
ï£¸â†âˆ’F
ï£«
ï£¬
ï£¬
ï£­
Ïƒ0
Ïƒ1
...
Ïƒnâˆ’1
ï£¶
ï£·
ï£·
ï£¸
using the FFT
(ii)
x â†âˆ’Fb
using the FFT
(iii)
xk â†âˆ’xk/p(Î¾k)
for
k = 0, 1, . . . , n âˆ’1
(iv)
x â†âˆ’Fâˆ’1x
using the FFT as described in Example 5.8.2

76
Solutions
(b)
Use the same techniques described in part (a) to compute the kth column
of Câˆ’1 from the formula
[Câˆ’1]âˆ—k = Câˆ’1ek = Fâˆ’1Dâˆ’1Fek
= Fâˆ’1 
Dâˆ’1[F]âˆ—k
	
= Fâˆ’1
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1/p(1)
Î¾k/p(Î¾)
Î¾2k/p(Î¾2)
...
Î¾nâˆ’k/p(Î¾nâˆ’1)
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
(c)
The kth column of P = C1C2 is given by
Pâˆ—k = Pek = Fâˆ’1D1FFâˆ’1D2Fek = Fâˆ’1
D1D2[F]âˆ—k

.
If ( Ïƒ0
Ïƒ1
Â· Â· Â·
Ïƒnâˆ’1 ) and ( Î·0
Î·1
Â· Â· Â·
Î·nâˆ’1 ) are the ï¬rst rows in C1 and
C2, respectively, and if p(x) = 
nâˆ’1
k=0 Ïƒkxk and q(x) = 
nâˆ’1
k=0 Î·kxk , then ï¬rst
compute
p =
ï£«
ï£¬
ï£¬
ï£­
p(0)
p(Î¾)
...
p(Î¾nâˆ’1)
ï£¶
ï£·
ï£·
ï£¸â†âˆ’F
ï£«
ï£¬
ï£¬
ï£­
Ïƒ0
Ïƒ1
...
Ïƒnâˆ’1
ï£¶
ï£·
ï£·
ï£¸
and
q =
ï£«
ï£¬
ï£¬
ï£­
q(0)
q(Î¾)
...
q(Î¾nâˆ’1)
ï£¶
ï£·
ï£·
ï£¸â†âˆ’F
ï£«
ï£¬
ï£¬
ï£­
Î·0
Î·1
...
Î·nâˆ’1
ï£¶
ï£·
ï£·
ï£¸.
The kth column of the product can now be obtained from
Pâˆ—k â†âˆ’Fâˆ’1
p Ã— q Ã— Fâˆ—k

for
k = 0, 1, . . . , n âˆ’1.
5.8.14. (a)
For n = 3 we have
CË†b =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î±0
0
0
0
Î±2
Î±1
Î±1
Î±0
0
0
0
Î±2
Î±2
Î±1
Î±0
0
0
0
0
Î±2
Î±1
Î±0
0
0
0
0
Î±2
Î±1
Î±0
0
0
0
0
Î±2
Î±1
Î±0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î²0
Î²1
Î²2
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î±0Î²0
Î±1Î²0 + Î±0Î²1
Î±2Î²0 + Î±1Î²1 + Î±0Î²2
Î±2Î²1 + Î±1Î²2
Î±2Î²2
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Use this as a model to write the expression for CË†b, where n is arbitrary.
(b)
We know from part (c) of Exercise 5.8.12 that if F is the Fourier matrix
of order 2n, then FCFâˆ’1 = D, where
D =
ï£«
ï£¬
ï£¬
ï£­
p(1)
0
Â· Â· Â·
0
0
p(Î¾)
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
p(Î¾2nâˆ’1)
ï£¶
ï£·
ï£·
ï£¸
(the Î¾k â€™s are the 2nth roots of unity)

Solutions
77
in which p(x) = Î±0 + Î±1x + Â· Â· Â· + Î±nâˆ’1xnâˆ’1. Therefore, from part (a),
F(a âŠ™b) = FCË†b = FCFâˆ’1FË†b = DFË†b.
According to Exercise 5.8.10, we also know that
FË†a =
ï£«
ï£¬
ï£¬
ï£­
p(1)
p(Î¾)
...
p(Î¾2nâˆ’1)
ï£¶
ï£·
ï£·
ï£¸,
and hence
F(a âŠ™b) = DFË†b = (FË†a) Ã— (FË†b).
5.8.15. (a) Pnx performs an evenâ€“odd permutation to all components of x. The matrix
(I2 âŠ—Pn/2) =

Pn/2
0
0
Pn/2

x
performs an evenâ€“odd permutation to the top half of x and then does the same
to the bottom half of x. The matrix
(I4 âŠ—Pn/4) =
ï£«
ï£¬
ï£­
Pn/4
0
0
0
0
Pn/4
0
0
0
0
Pn/4
0
0
0
0
Pn/4
ï£¶
ï£·
ï£¸x
performs an evenâ€“odd permutation to each individual quarter of x. As this
pattern is continued, the product
Rn = (I2râˆ’1 âŠ—P21)(I2râˆ’2 âŠ—P22) Â· Â· Â· (I21 âŠ—P2râˆ’1)(I20 âŠ—P2r)x
produces the bit-reversing permutation. For example, when n = 8,

78
Solutions
R8x = (I4 âŠ—P2)(I2 âŠ—P4)(I1 âŠ—P8)x
=
ï£«
ï£¬
ï£­
P2
0
0
0
0
P2
0
0
0
0
P2
0
0
0
0
P2
ï£¶
ï£·
ï£¸

P4
0
0
P4

P8
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
x0
x1
x2
x3
x4
x5
x6
x7
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£­
P2
0
0
0
0
P2
0
0
0
0
P2
0
0
0
0
P2
ï£¶
ï£·
ï£¸

P4
0
0
P4

ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
x0
x2
x4
x6
x1
x3
x5
x7
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£­
P2
0
0
0
0
P2
0
0
0
0
P2
0
0
0
0
P2
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
x0
x4
x2
x6
x1
x5
x3
x7
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
x0
x4
x2
x6
x1
x5
x3
x7
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
because
P2 =

1
0
0
1

.
(b)
To prove that I2râˆ’k âŠ—F2k = L2kR2k using induction, note ï¬rst that for
k = 1 we have
L2 = (I2râˆ’1 âŠ—B2)1 = I2râˆ’1 âŠ—F2
and
R2 = In(I2râˆ’1 âŠ—P2) = InIn = In,
so L2R2 = I2râˆ’1 âŠ—F2. Now assume that the result holds for k = jâ€”i.e., assume
I2râˆ’j âŠ—F2j = L2jR2j.
Prove that the result is true for k = j + 1â€”i.e., prove
I2râˆ’(j+1) âŠ—F2j+1 = L2j+1R2j+1.
Use the fact that F2j+1 = B2j+1(I2 âŠ—Fj)P2j+1 along with the two basic prop-

Solutions
79
erties of the tensor product given in the introduction of this exercise to write
I2râˆ’(j+1) âŠ—F2j+1 = I2râˆ’(j+1) âŠ—B2j+1(I2 âŠ—F2j)P2j+1
=

I2râˆ’(j+1) âŠ—B2j+1(I2 âŠ—F2j)

I2râˆ’(j+1) âŠ—P2j+1

= (I2râˆ’(j+1) âŠ—B2j+1)(I2râˆ’(j+1) âŠ—I2 âŠ—F2j)(I2râˆ’(j+1) âŠ—P2j+1)
= (I2râˆ’(j+1) âŠ—B2j+1)(I2râˆ’j âŠ—F2j)(I2râˆ’(j+1) âŠ—P2j+1)
= (I2râˆ’(j+1) âŠ—B2j+1)L2jR2j(I2râˆ’(j+1) âŠ—P2j+1)
= L2j+1R2j+1.
Therefore, I2râˆ’k âŠ—F2k = L2kR2k for k = 1, 2, . . . , r, and when k = r we have
that Fn = LnRn.
5.8.16. According to Exercise 5.8.10,
Fna = b,
where
a =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î±0
Î±1
Î±2
...
Î±nâˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
and
b =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
p(1)
p(Î¾)
p(Î¾2)
...
p(Î¾nâˆ’1)
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
By making use of the fact that (1/âˆšn)Fn is unitary we can write
nâˆ’1

k=0
p(Î¾k)
2 = bâˆ—b = (Fna)âˆ—(Fna) = aâˆ—Fâˆ—
nFna = aâˆ—(nI)a = n
nâˆ’1

k=0
|Î±k|2 .
5.8.17. Let y = (2/n)Fx, and use the result in (5.8.7) to write
âˆ¥yâˆ¥2 =
,,,,,

k

(Î±k âˆ’iÎ²k)efk + (Î±k + iÎ²k)enâˆ’fk
,,,,,
=

k

|Î±k âˆ’iÎ²k|2 + |Î±k + iÎ²k|2
= 2

k

Î±2
k + Î²2
k

.
But because Fâˆ—F = nI, it follows that
âˆ¥yâˆ¥2 =
,,,,
2
nFx
,,,,
2
= 4
n2 xâˆ—Fâˆ—Fx = 4
n âˆ¥xâˆ¥2 ,
so combining these two statements produces the desired conclusion.

80
Solutions
5.8.18. We know from (5.8.11) that if p(x) = 
nâˆ’1
k=0 Î±kxk, then
p2(x) =
2nâˆ’2

k=0
[a âŠ™a]kxk.
The last component of a âŠ™a is zero, so we can write
cT (a âŠ™a) =
2nâˆ’2

k=0
[a âŠ™a]kÎ·k = p2(Î·) =
nâˆ’1

k=0
Î±kÎ·k
2
=

cT Ë†a
	2 .
5.8.19. Start with X â†âˆ’rev(x) = (x0 x4 x2 x6 x1 x5 x3 x7).
For j = 0 :
D â†âˆ’(1)
X(0) â†âˆ’( x0
x2
x1
x3 )
X(1) â†âˆ’( x4
x6
x5
x7 )
X â†âˆ’
 X(0) + D Ã— X(1)
X(0) âˆ’D Ã— X(1)

=
 x0 + x4
x2 + x6
x1 + x5
x3 + x7
x0 âˆ’x4
x2 âˆ’x6
x1 âˆ’x5
x3 âˆ’x7

2Ã—8
For j = 1 :
D â†âˆ’

1
eâˆ’Ï€i/2

=

1
Î¾2

X(0) â†âˆ’

x0 + x4
x1 + x5
x0 âˆ’x4
x1 âˆ’x5

X(1) â†âˆ’

x2 + x6
x3 + x7
x2 âˆ’x6
x3 âˆ’x7

X â†âˆ’
 X(0) + D Ã— X(1)
X(0) âˆ’D Ã— X(1)

=
ï£«
ï£¬
ï£­
x0 + x4 +
x2 +
x6
x1 + x5 +
x3 +
x7
x0 âˆ’x4 + Î¾2x2 âˆ’Î¾2x6
x1 âˆ’x5 + Î¾2x3 âˆ’Î¾2x7
x0 + x4 âˆ’
x2 âˆ’
x6
x1 + x5 âˆ’
x3 âˆ’
x7
x0 âˆ’x4 âˆ’Î¾2x2 + Î¾2x6
x1 âˆ’x5 âˆ’Î¾2x3 + Î¾2x7
ï£¶
ï£·
ï£¸
4Ã—2
For j = 2 :
D â†âˆ’
ï£«
ï£¬
ï£­
1
eâˆ’Ï€i/4
eâˆ’2Ï€i/4
eâˆ’3Ï€i/4
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
1
Î¾
Î¾2
Î¾3
ï£¶
ï£·
ï£¸

Solutions
81
X(0) â†âˆ’
ï£«
ï£¬
ï£­
x0 + x4 +
x2 +
x6
x0 âˆ’x4 + Î¾2x2 âˆ’Î¾2x6
x0 + x4 âˆ’
x2 âˆ’
x6
x0 âˆ’x4 âˆ’Î¾2x2 + Î¾2x6
ï£¶
ï£·
ï£¸
X(1) â†âˆ’
ï£«
ï£¬
ï£­
x1 + x5 +
x3 +
x7
x1 âˆ’x5 + Î¾2x3 âˆ’Î¾2x7
x1 + x5 âˆ’
x3 âˆ’
x7
x1 âˆ’x5 âˆ’Î¾2x3 + Î¾2x7
ï£¶
ï£·
ï£¸
X â†âˆ’
 X(0) + D Ã— X(1)
X(0) âˆ’D Ã— X(1)

=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
x0 + x4 +
x2 +
x6 +
x1 +
x5 +
x3 +
x7
x0 âˆ’x4 + Î¾2x2 âˆ’Î¾2x6 + Î¾ x1 âˆ’Î¾x5 + Î¾3x3 âˆ’Î¾3x7
x0 + x4 âˆ’
x2 âˆ’
x6 + Î¾2x1 + Î¾2x5 âˆ’Î¾2x3 âˆ’Î¾2x7
x0 âˆ’x4 âˆ’Î¾2x2 + Î¾2x6 + Î¾3x1 âˆ’Î¾3x5 âˆ’Î¾5x3 + Î¾5x7
x0 + x4 +
x2 +
x6 âˆ’
x1 âˆ’
x5 âˆ’
x3 âˆ’
x7
x0 âˆ’x4 + Î¾2x2 âˆ’Î¾2x6 âˆ’Î¾ x1 + Î¾ x5 âˆ’Î¾3x3 + Î¾3x7
x0 + x4 âˆ’
x2 âˆ’
x6 âˆ’Î¾2x1 âˆ’Î¾2x5 + Î¾2x3 + Î¾2x7
x0 âˆ’x4 âˆ’Î¾2x2 + Î¾2x6 âˆ’Î¾3x1 + Î¾3x5 + Î¾5x3 âˆ’Î¾5x7
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
8Ã—1
To verify that this is the same as F8x8, use the fact that Î¾ = âˆ’Î¾5, Î¾2 = âˆ’Î¾6,
Î¾3 = âˆ’Î¾7, and Î¾4 = âˆ’1.
Solutions for exercises in section 5. 9
5.9.1. (a)
The fact that
rank (B) = rank

X | Y

= rank
ï£«
ï£­
1
1
1
1
2
2
1
2
3
ï£¶
ï£¸= 3
implies BX âˆªBY is a basis for â„œ3, so (5.9.4) guarantees that X and Y are
complementary.
(b)
According to (5.9.12), the projector onto X along Y is
P =

X | 0

X | Y
âˆ’1 =
ï£«
ï£­
1
1
0
1
2
0
1
2
0
ï£¶
ï£¸
ï£«
ï£­
1
1
1
1
2
2
1
2
3
ï£¶
ï£¸
âˆ’1
=
ï£«
ï£­
1
1
0
1
2
0
1
2
0
ï£¶
ï£¸
ï£«
ï£­
2
âˆ’1
0
âˆ’1
2
âˆ’1
0
âˆ’1
1
ï£¶
ï£¸=
ï£«
ï£­
1
1
âˆ’1
0
3
âˆ’2
0
3
âˆ’2
ï£¶
ï£¸,
and (5.9.9) insures that the complementary projector onto Y along X is
Q = I âˆ’P =
ï£«
ï£­
0
âˆ’1
1
0
âˆ’2
2
0
âˆ’3
3
ï£¶
ï£¸.

82
Solutions
(c)
Qv =
ï£«
ï£­
2
4
6
ï£¶
ï£¸
(d)
Direct multiplication shows P2 = P and Q2 = Q.
(e)
To verify that R (P) = X = N (Q), you can use the technique of Example
4.2.2 to show that the basic columns of P (or the columns in a basis for N (Q) )
span the same space generated by BX . To verify that N (P) = Y, note that
P
ï£«
ï£­
1
2
3
ï£¶
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸together with the fact that dim N (P) = 3 âˆ’rank (P) = 1.
5.9.2. There are many ways to do this. One way is to write down any basis for â„œ5â€”say
B = {x1, x2, x3, x4, x5}â€”and set
X = span {x1, x2}
and
Y = span {x3, x4, x5} .
Property (5.9.4) guarantees that X and Y are complementary.
5.9.3. Let X = {(Î±, Î±) | Î± âˆˆâ„œ} and Y = â„œ2 so that â„œ2 = X + Y, but X âˆ©Y Ì¸= 0.
For each vector in â„œ2 we can write
(x, y) = (x, x) + (0, y âˆ’x)
and
(x, y) = (y, y) + (x âˆ’y, 0).
5.9.4. Exercise 3.2.6 says that each A âˆˆâ„œnÃ—n can be uniquely written as the sum of
a symmetric matrix and a skew-symmetric matrix according to the formula
A = A + AT
2
+ A âˆ’AT
2
,
so (5.9.3) guarantees that â„œnÃ—n = S âŠ•K. By deï¬nition, the projection of A
onto S along K is the S -component of A â€”namely (A + AT )/2. For the
given matrix, this is
A + AT
2
=
ï£«
ï£­
1
3
5
3
5
7
5
7
9
ï£¶
ï£¸.
5.9.5. (a)
Assume that X âˆ©Y = 0. To prove BX âˆªBY is linearly independent, write
m

i=1
Î±ixi +
n

j=1
Î²jyj = 0
=â‡’
m

i=1
Î±ixi = âˆ’
n

j=1
Î²jyj
=â‡’
m

i=1
Î±ixi âˆˆX âˆ©Y = 0
=â‡’
m

i=1
Î±ixi = 0
and
n

j=1
Î²jyj = 0
=â‡’
Î±1 = Â· Â· Â· = Î±m = Î²1 = Â· Â· Â· = Î²n = 0
(because BX and BY are both independent).

Solutions
83
Conversely, if BX âˆªBY is linearly independent, then
v âˆˆX âˆ©Y
=â‡’
v =
m

i=1
Î±ixi
and
v =
n

j=1
Î²jyj
=â‡’
m

i=1
Î±ixi âˆ’
n

j=1
Î²jyj = 0
=â‡’
Î±1 = Â· Â· Â· = Î±m = Î²1 = Â· Â· Â· = Î²n = 0
(because BX âˆªBY is independent)
=â‡’
v = 0.
(b)
No. Take X to be the xy-plane and Y to be the yz-plane in â„œ3 with
BX = {e1, e2} and BY = {e2, e3}. We have BX âˆªBY = {e1, e2, e3}, but
X âˆ©Y Ì¸= 0.
(c)
No, the fact that BX âˆªBY is linearly independent is no guarantee that
X + Y is the entire spaceâ€”e.g., consider two distinct lines in â„œ3.
5.9.6. If x is a ï¬xed point for P, then Px = x implies x âˆˆR (P). Conversely, if
x âˆˆR (P), then x = Py for some y âˆˆV
=â‡’
Px = P2y = Py = x.
5.9.7. Use (5.9.10) (which you just validated in Exercise 5.9.6) in conjunction with the
deï¬nition of a projector onto X to realize that
x âˆˆX â‡â‡’Px = x â‡â‡’x âˆˆR (P),
and
x âˆˆR (P) â‡â‡’Px = x â‡â‡’(I âˆ’P)x = 0 â‡â‡’x âˆˆN (I âˆ’P).
The statements concerning the complementary projector I âˆ’P are proven in a
similar manner.
5.9.8. If Î¸ is the angle between R (P) and N (P), it follows from (5.9.18) that âˆ¥Pâˆ¥2 =
(1/ sin Î¸) â‰¥1. Furthermore, âˆ¥Pâˆ¥2 = 1 if and only if sin Î¸ = 1 (i.e., Î¸ = Ï€/2 ),
which is equivalent to saying R (P) âŠ¥N (P).
5.9.9. Let Î¸ be the angle between R (P) and N (P). We know from (5.9.11) that
R (I âˆ’P) = N (P) and N (I âˆ’P) = R (P), so Î¸ is also the angle between
R (I âˆ’P) and N (I âˆ’P). Consequently, (5.9.18) says that
âˆ¥I âˆ’Pâˆ¥2 =
1
sin Î¸ = âˆ¥Pâˆ¥2 .
5.9.10. The trick is to observe that P = uvT is a projector because vT u = 1 implies
P2 = uvT uvT = uvT = P, so the result of Exercise 5.9.9 insures that
,,I âˆ’uvT ,,
2 =
,,uvT ,,
2 .

84
Solutions
To prove that
,,uvT ,,
2 = âˆ¥uâˆ¥2 âˆ¥vâˆ¥2 , start with the deï¬nition of an induced
matrix given in (5.2.4) on p. 280, and write
,,uvT ,,
2 = maxâˆ¥xâˆ¥2=1
,,uvT x
,,
2 . If
the maximum occurs at x = x0 with âˆ¥x0âˆ¥2 = 1, then
,,uvT ,,
2 =
,,uvT x0
,,
2 = âˆ¥uâˆ¥2 |vT x0|
â‰¤âˆ¥uâˆ¥2 âˆ¥vâˆ¥2 âˆ¥x0âˆ¥2 by CBS inequality
= âˆ¥uâˆ¥2 âˆ¥vâˆ¥2 .
But we can also write
âˆ¥uâˆ¥2 âˆ¥vâˆ¥2 = âˆ¥uâˆ¥2
âˆ¥vâˆ¥2
2
âˆ¥vâˆ¥2
= âˆ¥uâˆ¥2
(vT v)
âˆ¥vâˆ¥2
=
,,uvT v
,,
2
âˆ¥vâˆ¥2
=
,,,,uvT

v
âˆ¥vâˆ¥2
,,,,
2
â‰¤max
âˆ¥xâˆ¥2=1
,,uvT x
,,
2
=
,,uvT ,,
2 ,
so
,,uvT ,,
2 = âˆ¥uâˆ¥2 âˆ¥vâˆ¥2 . Finally, if P = uvT , use Example 3.6.5 to write
âˆ¥Pâˆ¥2
F = trace

PT P
	
= trace(vuT uvT ) = trace(uT uvT v) = âˆ¥uâˆ¥2
2 âˆ¥vâˆ¥2
2 .
5.9.11. p = Pv = [X | 0][X | Y]âˆ’1v = [X | 0]z = Xz1
5.9.12. (a)
Use (5.9.10) to conclude that
R (P) = R (Q)
=â‡’
PQâˆ—j = Qâˆ—j
and
QPâˆ—j = Pâˆ—j
âˆ€j
=â‡’
PQ = Q
and
QP = P.
Conversely, use Exercise 4.2.12 to write
 PQ = Q
=â‡’
R (Q) âŠ†R (P)
QP = P
=â‡’
R (P) âŠ†R (Q)
+
=â‡’
R (P) = R (Q).
(b)
Use N (P) = N (Q) â‡â‡’R (I âˆ’P) = R (I âˆ’Q) together with part (a).
(c)
From part (a), EiEj = Ej so that
 
j
Î±jEj
2
=

i

j
Î±iÎ±jEiEj =

i

j
Î±iÎ±jEj
=
 
i
Î±i
 
j
Î±jEj

=

j
Î±jEj.
5.9.13. According to (5.9.12), the projector onto X along Y is P = B

Ir
0
0
0

Bâˆ’1,
where B = [X | Y] in which the columns of X and Y form bases for X

Solutions
85
and Y, respectively. Since multiplication by nonsingular matrices does not alter
the rank, it follows that rank (P) = rank

Ir
0
0
0

= r. Using the result of
Example 3.6.5 produces
trace (P) = trace

B

Ir
0
0
0

Bâˆ’1

= trace

Ir
0
0
0

Bâˆ’1B

= trace

Ir
0
0
0

= r = rank (P).
5.9.14. (i) =â‡’
(ii) : If v = x1 + Â· Â· Â· + xk and v = y1 + Â· Â· Â· + yk, where xi, yi âˆˆXi,
then
k

i=1
(xi âˆ’yi) = 0
=â‡’
(xk âˆ’yk) = âˆ’
kâˆ’1

i=1
(xi âˆ’yi)
=â‡’
(xk âˆ’yk) âˆˆXk âˆ©(X1 + Â· Â· Â· + Xkâˆ’1) = 0
=â‡’
xk = yk
and
kâˆ’1

i=1
(xi âˆ’yi) = 0.
Now repeat the argumentâ€”to be formal, use induction.
(ii)
=â‡’
(iii) : The proof is essentially the same argument as that used to
establish (5.9.3) =â‡’(5.9.4).
(iii) =â‡’(i) : B always spans X1 + X2 + Â· Â· Â· + Xk, and since the hypothesis is
that B is a basis for V, it follows that B is a basis for both V and X1+Â· Â· Â·+Xk.
Consequently V = X1 + X2 + Â· Â· Â· + Xk. Furthermore, the set B1 âˆªÂ· Â· Â· âˆªBkâˆ’1 is
linearly independent (each subset of an independent set is independent), and it
spans Vkâˆ’1 = X1+Â· Â· Â·+Xkâˆ’1, so B1âˆªÂ· Â· Â·âˆªBkâˆ’1 must be a basis for Vkâˆ’1. Now,
since (B1âˆªÂ· Â· Â·âˆªBkâˆ’1)âˆªBk is a basis for V = (X1+Â· Â· Â·+Xkâˆ’1)+Xk, it follows from
(5.9.2)â€“(5.9.4) that V = (X1 + Â· Â· Â· + Xkâˆ’1) âŠ•Xk, so Xk âˆ©(X1 + Â· Â· Â· + Xkâˆ’1) = 0.
The same argument can now be repeated on Vkâˆ’1â€”to be formal, use induction.
5.9.15. We know from (5.9.12) that P = Q

I
0
0
0

Qâˆ’1 and Iâˆ’P = Q

0
0
0
I

Qâˆ’1,
so
PAP = Q

I
0
0
0

Qâˆ’1Q

A11
A12
A21
A22

Qâˆ’1Q

I
0
0
0

Qâˆ’1
= Q

A11
0
0
0

Qâˆ’1.
The other three statements are derived in an analogous fashion.
5.9.16. According to (5.9.12), the projector onto X along Y is P =

X | 0

X | Y
âˆ’1,
where the columns of X and Y are bases for X
and Y, respectively. If

86
Solutions

XnÃ—r | Y
âˆ’1 =

ArÃ—n
C

, then
P =

XnÃ—r | 0
 
ArÃ—n
C

= XnÃ—rArÃ—n.
The nonsingularity of

X | Y

and

A
C

insures that X has full column rank
and A has full row rank. The fact that AX = Ir is a consequence of

Ir
0
0
I

=

X | Y
âˆ’1
X | Y

=

ArÃ—n
C
 
XnÃ—r | Y

=

AX
AY
CX
CY

.
5.9.17. (a)
Use the fact that a linear operator P is a projector if and only if P is
idempotent. If EF = FE = 0, then (E + F)2 = E + F. Conversely, if E + F is
a projector, then
(E + F)2 = E + F
=â‡’
EF + FE = 0
=â‡’
E(EF + FE) = 0
and
(EF + FE)E = 0
=â‡’
EF = FE
=â‡’
EF = 0 = FE
(because EF + FE = 0).
Thus P = E + F is a projector if and only if EF = FE = 0. Now prove that
under this condition R (P) = X1âŠ•X2. Start with the fact that z âˆˆR (P) if and
only if Pz = z, and write each such vector z as z = x1 + y1 and z = x2 + y2,
where xi âˆˆXi and yi âˆˆYi so that Ex1 = x1,
Ey1 = 0,
Fx2 = x2, and
Fy2 = 0. To prove that R (P) = X1 + X2, write
z âˆˆR (P)
=â‡’
Pz = z
=â‡’
(E + F)z = z
=â‡’
(E + F)(x2 + y2) = (x2 + y2)
=â‡’
Ez = y2
=â‡’
x1 = y2
=â‡’
z = x1 + x2 âˆˆX1 + X2
=â‡’
R (P) âŠ†X1 + X2.
Conversely, X1 + X2 âŠ†R (P) because
z âˆˆX1 + X2
=â‡’
z = x1 + x2,
where
x1 âˆˆX1 and x2 âˆˆX2
=â‡’
x1 = Ex1
and
x2 = Fx2
=â‡’
Fx1 = FEx1 = 0 and Ex2 = EFx2 = 0
=â‡’
Pz = (E + F)(x1 + x2) = x1 + x2 = z
=â‡’
z âˆˆR (P).
The fact that X1 and X2 are disjoint follows by writing
z âˆˆX1 âˆ©X2
=â‡’
Ez = z = Fz
=â‡’
z = EFz = 0,

Solutions
87
and thus R (P) = X1 âŠ•X2 is established. To prove that N (P) = Y1 âˆ©Y2, write
Pz = 0
=â‡’
(E + F)z = 0
=â‡’
Ez = âˆ’Fz
=â‡’
Ez = âˆ’EFz
and
FEz = âˆ’Fz
=â‡’
Ez = 0
and
0 = Fz
=â‡’
z âˆˆY1 âˆ©Y2.
5.9.18. Use the hint together with the result of Exercise 5.9.17 to write
E âˆ’F is a projector â‡â‡’I âˆ’(E âˆ’F) is a projector
â‡â‡’(I âˆ’E) + F is a projector
â‡â‡’(I âˆ’E)F = 0 = F(I âˆ’E)
â‡â‡’EF = F = FE.
Under this condition, Exercise 5.9.17 says that
R (I âˆ’E + F) = R (I âˆ’E) âŠ•R (F) and N (I âˆ’E + F) = N (I âˆ’E) âˆ©N (F),
so (5.9.11) guarantees
R (E âˆ’F) = N (I âˆ’E + F) = N (I âˆ’E) âˆ©N (F) = R (E) âˆ©N (F) = X1 âˆ©Y2
N (E âˆ’F) = R (I âˆ’E + F) = R (I âˆ’E) âŠ•R (F) = N (E) âŠ•R (F) = Y1 âŠ•X2.
5.9.19. If EF = P = FE, then P is idempotent, and hence P is a projector. To prove
that R (P) = X1 âˆ©X2, write
z âˆˆR (P)
=â‡’
Pz = z
=â‡’
E(Fz) = z
and
F(Ez) = z
=â‡’
z âˆˆR (E) âˆ©R (F) = X1 âˆ©X2
=â‡’
R (P) âŠ†X1 âˆ©X2.
Conversely,
z âˆˆX1 âˆ©X2
=â‡’
Ez = z = Fz
=â‡’
Pz = z
=â‡’
X1 âˆ©X2 âŠ†R (P),
and hence R (P) = X1 âˆ©X2. To prove that N (P) = Y1 + Y2, ï¬rst notice that
z âˆˆN (P)
=â‡’
FEz = 0
=â‡’
Ez âˆˆN (F).
This together with the fact that (I âˆ’E)z âˆˆN (E) allows us to conclude that
z = (I âˆ’E)z + Ez âˆˆN (E) + N (F) = Y1 + Y2
=â‡’
N (P) âŠ†Y1 + Y2.

88
Solutions
Conversely,
z âˆˆY1 + Y2
=â‡’
z = y1 + y2, where yi âˆˆYi for i = 1, 2
=â‡’
Ey1 = 0
and
Fy2 = 0
=â‡’
Pz = 0
=â‡’
Y1 + Y2 âŠ†N (P).
Thus N (P) = Y1 + Y2.
5.9.20. (a)
For every inner pseudoinverse, AAâˆ’is a projector onto R (A), and I âˆ’
Aâˆ’A is a projector onto N (A). The system being consistent means that
b âˆˆR (A) = R

AAâˆ’	
=â‡’
AAâˆ’b = b,
so Aâˆ’b is a particular solution. Therefore, the general solution of the system is
Aâˆ’b + N (A) = Aâˆ’b + R

I âˆ’Aâˆ’A
	
.
(b)
Aâˆ’A is a projector along N (A), so Exercise 5.9.12 insures Q(Aâˆ’A) = Q
and (Aâˆ’A)Q = (Aâˆ’A). This together with the fact that PA = A allows us
to write
AXA = AQAâˆ’PA = AQAâˆ’A = AQ = AAâˆ’AQ = AAâˆ’A = A.
Similarly,
XAX = (QAâˆ’P)A(QAâˆ’P) = QAâˆ’(PA)QAâˆ’P = QAâˆ’AQAâˆ’P
= Q(Aâˆ’AQ)Aâˆ’P = Q(Aâˆ’A)Aâˆ’P = QAâˆ’P = X,
so X is a reï¬‚exive pseudoinverse for A. To show X has the prescribed range
and nullspace, use the fact that XA is a projector onto R (X) and AX is a
projector along N (X) to write
R (X) = R (XA) = R

QAâˆ’PA
	
= R

QAâˆ’A
	
= R (Q) = L
and
N (X) = N (AX) = N

AQAâˆ’P
	
= N

AAâˆ’AQAâˆ’P
	
= N

AAâˆ’AAâˆ’P
	
= N

AAâˆ’P
	
= N (P) = M.
To prove uniqueness, suppose that X1 and X2 both satisfy the speciï¬ed con-
ditions. Then
N (X2) = M = R (I âˆ’AX1)
=â‡’
X2(I âˆ’AX1) = 0
=â‡’
X2 = X2AX1
and
R (X2A) = R (X2) = L = R (X1)
=â‡’
X2AX1 = X1,
so X2 = X1.

Solutions
89
Solutions for exercises in section 5. 10
5.10.1. Since index(A) = k, we must have that
rank

Akâˆ’1	
> rank

Ak	
= rank

Ak+1	
= Â· Â· Â· = rank

A2k	
= Â· Â· Â· ,
so rank

Ak	
= rank(Ak)2, and hence index(Ak) â‰¤1. But Ak is singular
(because A is singular) so that index(Ak) > 0. Consequently, index(Ak) = 1.
5.10.2. In this case, R

Ak	
= 0 and N

Ak	
= â„œn. The nonsingular component C
in (5.10.5) is missing, and you can take Q = I, thereby making A its own
core-nilpotent decomposition.
5.10.3. If A is nonsingular, then index(A) = 0, regardless of whether or not A is
symmetric. If A is singular and symmetric, we want to prove index(A) = 1.
The strategy is to show that R (A) âˆ©N (A) = 0 because this implies that
R (A) âŠ•N (A) = â„œn. To do so, start with
x âˆˆR (A) âˆ©N (A)
=â‡’
Ax = 0
and
x = Ay
for some
y.
Now combine this with the symmetry of A to obtain
xT = yT AT = yT A
=â‡’
xT x = yT Ax = 0
=â‡’
âˆ¥xâˆ¥2
2 = 0
=â‡’
x = 0.
5.10.4. index(A) = 0 when A is nonsingular. If A is singular and normal we want
to prove index(A) = 1. The strategy is to show that R (A) âˆ©N (A) = 0
because this implies that R (A)âŠ•N (A) = Cn. Recall from (4.5.6) that N (A) =
N (Aâˆ—A) and N (Aâˆ—) = N (AAâˆ—), so N (A) = N (Aâˆ—). Start with
x âˆˆR (A) âˆ©N (A)
=â‡’
Ax = 0
and
x = Ay
for some
y,
and combine this with N (A) = N (Aâˆ—) to obtain
Aâˆ—x = 0 and x = Ay
=â‡’
xâˆ—x = yâˆ—Aâˆ—x = 0
=â‡’
âˆ¥xâˆ¥2
2 = 0
=â‡’
x = 0.
5.10.5. Compute rank

A0	
= 3, rank (A) = 2, rank

A2	
= 1, and rank

A3	
= 1,
to see that k = 2 is the smallest integer such that rank

Ak	
= rank

Ak+1	
,
so index(A) = 2. The matrix Q = [X | Y] is a matrix in which the columns of
X are a basis for R

A2	
, and the columns of Y are a basis for N

A2	
. Since
EA2 =
ï£«
ï£­
1
1
0
0
0
0
0
0
0
ï£¶
ï£¸,
we have
X =
ï£«
ï£­
âˆ’8
12
8
ï£¶
ï£¸
and
Y =
ï£«
ï£­
âˆ’1
0
1
0
0
1
ï£¶
ï£¸,
so
Q =
ï£«
ï£­
âˆ’8
âˆ’1
0
12
1
0
8
0
1
ï£¶
ï£¸.

90
Solutions
It can now be veriï¬ed that
Qâˆ’1AQ =
ï£«
ï£­
1/4
1/4
0
âˆ’3
âˆ’2
0
âˆ’2
âˆ’2
1
ï£¶
ï£¸
ï£«
ï£­
âˆ’2
0
âˆ’4
4
2
4
3
2
2
ï£¶
ï£¸
ï£«
ï£­
âˆ’8
âˆ’1
0
12
1
0
8
0
1
ï£¶
ï£¸=
ï£«
ï£­
2
0
0
0
âˆ’2
4
0
âˆ’1
2
ï£¶
ï£¸,
where
C = [2]
and
N =

âˆ’2
4
âˆ’1
2

,
and N2 = 0. Finally, AD = Q

Câˆ’1
0
0
0

Qâˆ’1 =
ï£«
ï£­
âˆ’1
âˆ’1
0
3/2
3/2
0
1
1
0
ï£¶
ï£¸.
5.10.6. (a)
Because
J âˆ’Î»I =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1 âˆ’Î»
0
0
0
0
0
1 âˆ’Î»
0
0
0
0
0
1 âˆ’Î»
0
0
0
0
0
2 âˆ’Î»
0
0
0
0
0
2 âˆ’Î»
ï£¶
ï£·
ï£·
ï£·
ï£¸,
and because a diagonal matrix is singular if and only if it has a zero-diagonal
entry, it follows that Jâˆ’Î»I is singular if and only if Î» = 1 or Î» = 2, so Î»1 = 1
and Î»2 = 2 are the two eigenvalues of J. To ï¬nd the index of Î»1, use block
multiplication to observe that
J âˆ’I =

0
0
0
I2Ã—2

=â‡’
rank (J âˆ’I) = 2 = rank (J âˆ’I)2.
Therefore, index(Î»1) = 1. Similarly,
J âˆ’2I =

âˆ’I3Ã—3
0
0
0

and
rank (J âˆ’2I) = 3 = rank (J âˆ’2I)2,
so index(Î»2) = 1.
(b)
Since
J âˆ’Î»I =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1 âˆ’Î»
1
0
0
0
0
1 âˆ’Î»
1
0
0
0
0
1 âˆ’Î»
0
0
0
0
0
2 âˆ’Î»
1
0
0
0
0
2 âˆ’Î»
ï£¶
ï£·
ï£·
ï£·
ï£¸,
and since a triangular matrix is singular if and only if there exists a zero-diagonal
entry (i.e., a zero pivot), it follows that J âˆ’Î»I is singular if and only if Î» = 1

Solutions
91
or Î» = 2, so Î»1 = 1 and Î»2 = 2 are the two eigenvalues of J. To ï¬nd the
index of Î»1, use block multiplication to compute
J âˆ’I =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
1
ï£¶
ï£·
ï£·
ï£·
ï£¸,
(J âˆ’I)2 =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
2
0
0
0
0
1
ï£¶
ï£·
ï£·
ï£·
ï£¸,
(J âˆ’I)3 =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
3
0
0
0
0
1
ï£¶
ï£·
ï£·
ï£·
ï£¸,
(J âˆ’I)4 =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
4
0
0
0
0
1
ï£¶
ï£·
ï£·
ï£·
ï£¸.
Since
rank (J âˆ’I) > rank (J âˆ’I)2 > rank (J âˆ’I)3 = rank (J âˆ’I)4,
it follows that index(Î»1) = 3. A similar computation using Î»2 shows that
rank (J âˆ’2I) > rank (J âˆ’2I)2 = rank (J âˆ’2I)3,
so index(Î»2) = 2. The fact that eigenvalues associated with diagonal matrices
have index 1 while eigenvalues associated with triangular matrices can have
higher indices is no accident. This will be discussed in detail in Â§7.8 (p. 587).
5.10.7. (a)
If P is a projector, then, by (5.9.13), P = P2, so rank (P) = rank

P2	
,
and hence index(P) â‰¤1. If P Ì¸= I, then P is singular, and thus index(P) = 1.
If P = I, then index(P) = 0. An alternate argument could be given on the
basis of the observation that â„œn = R (P) âŠ•N (P).
(b)
Recall from (5.9.12) that if the columns of X and Y constitute bases for
R (P) and N (P), respectively, then for Q =

X | Y

,
Qâˆ’1PQ =

I
0
0
0

,
and it follows that

I
0
0
0

is the core-nilpotent decomposition for P.
5.10.8. Suppose that 
kâˆ’1
i=0 Î±iNix = 0, and multiply both sides by Nkâˆ’1 to ob-
tain Î±0Nkâˆ’1x = 0. By assumption, Nkâˆ’1x Ì¸= 0, so Î±0 = 0, and hence

kâˆ’1
i=1 Î±iNix = 0. Now multiply both sides of this equation by Nkâˆ’2 to pro-
duce Î±1Nkâˆ’1x = 0, and conclude that Î±1 = 0. Continuing in this manner (or
by making a formal induction argument) gives Î±0 = Î±1 = Î±2 = Â· Â· Â· = Î±kâˆ’1 = 0.
5.10.9. (a)
b âˆˆR

Ak	
âŠ†R (A)
=â‡’
b âˆˆR (A)
=â‡’
Ax = b is consistent.

92
Solutions
(b)
We saw in Example 5.10.5 that when considered as linear operators re-
stricted to R

Ak	
, both A and AD are invertible, and in fact they are
true inverses of each other. Consequently, A and AD are one-to-one map-
pings on R

Ak	
(recall Exercise 4.7.18), so for each b âˆˆR

Ak	
there is
a unique x âˆˆR

Ak	
such that Ax = b, and this unique x is given by
x =

A/R(Ak)
âˆ’1
b = ADb.
(c)
Part (b) shows that ADb is a particular solution. The desired result follows
because the general solution is any particular solution plus the general solution
of the associated homogeneous equation.
5.10.10. Notice that AAD = Q

I
0
0
0

Qâˆ’1, and use the results from Example 5.10.3
(p. 398). I âˆ’AAD is the complementary projector, so it projects onto N

Ak	
along R

Ak	
.
5.10.11. In each case verify that the axioms (A1), (A2), (A4), and (A5) in the deï¬nition
of a vector space given on p. 160 hold for matrix multiplication (rather than
+). In parts (a) and (b) the identity element is the ordinary identity matrix,
and the inverse of each member is the ordinary inverse. In part (c), the identity
element is E =

1/2
1/2
1/2
1/2

because AE = A = EA for each A âˆˆG, and

Î±
Î±
Î±
Î±
#
= 1
4Î±

1
1
1
1

because AA# = E = A#A.
5.10.12. (a)
=â‡’
(b) : If A belongs to a matrix group G in which the identity element
is E, and if A# is the inverse of A in G, then A#A2 = EA = A, so
x âˆˆR (A) âˆ©N (A)
=â‡’
x = Ay for some y and Ax = 0
=â‡’
Ay = A#A2y = A#Ax = 0
=â‡’
x = 0.
(b)
=â‡’
(c) : Suppose A is n Ã— n, and let BR and BN be bases for R (A)
and N (A), respectively. Verify that B = R (A) âˆ©N (A) = 0 implies BR âˆ©BN
is a linearly independent set, and use the fact that there are n vectors in B to
conclude that B is a basis for â„œn. Statement (c) now follows from (5.9.4).
(c)
=â‡’
(d) : Use the fact that R

Ak	
âˆ©N

Ak	
= 0.
(d)
=â‡’
(e) : Use the result of Example 5.10.5 together with the fact that
the only nilpotent matrix of index 1 is the zero matrix.
(e)
=â‡’
(a) : It is straightforward to verify that the set
G =

Q

XrÃ—r
0
0
0

Qâˆ’1  X is nonsingular
+
is a matrix group, and itâ€™s clear that A âˆˆG.

Solutions
93
5.10.13. (a)
Use part (e) of Exercise 5.10.12 to write A = Q

CrÃ—r
0
0
0

Qâˆ’1. For the
given E, verify that EA = AE = A for all A âˆˆG. The fact that E is the
desired projector follows from (5.9.12).
(b)
Simply verify that AA# = A#A = E. Notice that the group inverse
agrees with the Drazin inverse of A described in Example 5.10.5. However, the
Drazin inverse exists for all square matrices, but the concept of a group inverse
makes sense only for group matricesâ€”i.e., when index(A) = 1.
Solutions for exercises in section 5. 11
5.11.1. Proceed as described on p. 199 to determine the following bases for each of the
four fundamental subspaces.
R (A) = span
ï£±
ï£²
ï£³
ï£«
ï£­
2
âˆ’1
âˆ’2
ï£¶
ï£¸,
ï£«
ï£­
1
âˆ’1
âˆ’1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
N

AT 	
= span
ï£±
ï£²
ï£³
ï£«
ï£­
1
0
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
N (A) = span
ï£±
ï£²
ï£³
ï£«
ï£­
âˆ’1
1
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
R

AT 	
= span
ï£±
ï£²
ï£³
ï£«
ï£­
1
0
1
ï£¶
ï£¸,
ï£«
ï£­
0
1
âˆ’1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
Since each vector in a basis for R (A) is orthogonal to each vector in a basis
for N

AT 	
, it follows that R (A) âŠ¥N

AT 	
. The same logic also explains
why N (A) âŠ¥R

AT 	
. Notice that R (A) is a plane through the origin in â„œ3,
and N

AT 	
is the line through the origin perpendicular to this plane, so it
is evident from the parallelogram law that R (A) âŠ•N

AT 	
= â„œ3. Similarly,
N (A) is the line through the origin normal to the plane deï¬ned by R

AT 	
, so
N (A) âŠ•R

AT 	
= â„œ3.
5.11.2. VâŠ¥= 0, and 0âŠ¥= V.
5.11.3. If A =
ï£«
ï£¬
ï£­
1
2
2
4
0
1
3
6
ï£¶
ï£·
ï£¸, then R (A) = M, so (5.11.5) insures MâŠ¥= N

AT 	
. Using
row operations, a basis for N

AT 	
is computed to be
ï£±
ï£´
ï£²
ï£´
ï£³
ï£«
ï£¬
ï£­
âˆ’2
1
0
0
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
âˆ’3
0
0
1
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
.
5.11.4. Verify that MâŠ¥is closed with respect to vector addition and scalar multipli-
cation. If x, y âˆˆMâŠ¥, then âŸ¨m xâŸ©= 0 = âŸ¨m yâŸ©for each m âˆˆM so that
âŸ¨m x + yâŸ©= 0 for each m âˆˆM, and thus x + y âˆˆMâŠ¥. Similarly, for every
scalar Î± we have âŸ¨m Î±xâŸ©= Î± âŸ¨m xâŸ©= 0 for each m âˆˆM, so Î±x âˆˆMâŠ¥.
5.11.5. (a)
x âˆˆN âŠ¥=â‡’x âŠ¥N âŠ‡M =â‡’x âŠ¥M =â‡’x âˆˆMâŠ¥.

94
Solutions
(b)
Simply observe that
x âˆˆ(M + N)âŠ¥â‡â‡’x âŠ¥(M + N)
â‡â‡’x âŠ¥M and x âŠ¥N
â‡â‡’x âˆˆ

MâŠ¥âˆ©N âŠ¥	
.
(c)
Use part (b) together with (5.11.4) to write

MâŠ¥+ N âŠ¥	âŠ¥= MâŠ¥âŠ¥âˆ©N âŠ¥âŠ¥= M âˆ©N,
and then perp both sides.
5.11.6. Use the fact that dim R

AT 	
= rank

AT 	
= rank (A) = dim R (A) together
with (5.11.7) to conclude that
n = dim N (A) + dim R

AT 	
= dim N (A) + dim R (A).
5.11.7. U is a unitary matrix in which the columns of U1 are an orthonormal basis for
R (A) and the columns of U2 are an orthonormal basis for N

AT 	
, so setting
X = U1,
Y = U2, and

X | Y
âˆ’1 = UT in (5.9.12) produces P = U1UT
1 .
According to (5.9.9), the projector onto N

AT 	
along R (A) is I âˆ’P = I âˆ’
U1UT
1 = U2UT
2 .
5.11.8. Start with the ï¬rst column of A, and set u = Aâˆ—1 + 6e1 = ( 2
2
âˆ’4 )T to
obtain
R1 = Iâˆ’2uuT
uT u = 1
3
ï£«
ï£­
2
âˆ’1
2
âˆ’1
2
2
2
2
âˆ’1
ï£¶
ï£¸
and
R1A =
ï£«
ï£­
âˆ’6
0
âˆ’6
âˆ’3
0
0
0
0
0
âˆ’3
0
0
ï£¶
ï£¸.
Now set u =

0
âˆ’3

+ 3e1 =

3
âˆ’3

to get
Ë†R2 = I âˆ’2uuT
uT u =

0
1
1
0

and
R2 =

1
0
0
Ë†R2

=
ï£«
ï£­
1
0
0
0
0
1
0
1
0
ï£¶
ï£¸,
so
P = R2R1 = 1
3
ï£«
ï£­
2
âˆ’1
2
2
2
âˆ’1
âˆ’1
2
2
ï£¶
ï£¸and PA =
ï£«
ï£­
âˆ’6
0
âˆ’6
âˆ’3
0
âˆ’3
0
0
0
0
0
0
ï£¶
ï£¸=

B
0

.

Solutions
95
Therefore, rank (A) = 2, and orthonormal bases for R (A) and N

AT 	
are
extracted from the columns of U = PT as shown below.
R (A) = span
ï£±
ï£²
ï£³
ï£«
ï£­
2/3
âˆ’1/3
2/3
ï£¶
ï£¸,
ï£«
ï£­
2/3
2/3
âˆ’1/3
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
and
N

AT 	
= span
ï£±
ï£²
ï£³
ï£«
ï£­
âˆ’1/3
2/3
2/3
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
Now work with BT , and set u = (B1âˆ—)T + 9e1 = ( 3
0
âˆ’6
âˆ’3 )T to get
Q = Iâˆ’2uuT
uT u = 1
3
ï£«
ï£¬
ï£­
2
0
2
1
0
3
0
0
2
0
âˆ’1
âˆ’2
1
0
âˆ’2
2
ï£¶
ï£·
ï£¸
and
QBT =
ï£«
ï£¬
ï£­
âˆ’9
0
0
âˆ’3
0
0
0
0
ï£¶
ï£·
ï£¸=

T
0

.
Orthonormal bases for R

AT 	
and N (A) are extracted from the columns of
V = QT = Q as shown below.
R

AT 	
=span
ï£±
ï£´
ï£²
ï£´
ï£³
ï£«
ï£¬
ï£­
2/3
0
2/3
1/3
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
0
1
0
0
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
and N (A)=span
ï£±
ï£´
ï£²
ï£´
ï£³
ï£«
ï£¬
ï£­
2/3
0
âˆ’1/3
âˆ’2/3
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
1/3
0
âˆ’2/3
2/3
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
A URV factorization is obtained by setting U = PT , V = QT , and
R =

TT
0
0
0

=
ï£«
ï£­
âˆ’9
0
0
0
0
âˆ’3
0
0
0
0
0
0
ï£¶
ï£¸.
5.11.9. Using EA =
ï£«
ï£­
1
0
1
1/2
0
1
0
0
0
0
0
0
ï£¶
ï£¸along with the standard methods of Chapter 4,
we have
R (A) = span
ï£±
ï£²
ï£³
ï£«
ï£­
âˆ’4
2
âˆ’4
ï£¶
ï£¸,
ï£«
ï£­
âˆ’2
âˆ’2
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
and
N

AT 	
= span
ï£±
ï£²
ï£³
ï£«
ï£­
âˆ’1
2
2
ï£¶
ï£¸
ï£¼
ï£½
ï£¾,
R

AT 	
= span
ï£±
ï£´
ï£²
ï£´
ï£³
ï£«
ï£¬
ï£­
1
0
1
1/2
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
0
1
0
0
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
and N (A) = span
ï£±
ï£´
ï£²
ï£´
ï£³
ï£«
ï£¬
ï£­
âˆ’1
0
1
0
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
âˆ’1/2
0
0
1
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
.
Applying the Gramâ€“Schmidt procedure to each of these sets produces the fol-
lowing orthonormal bases for the four fundamental subspaces.
BR(A) =
ï£±
ï£²
ï£³
1
3
ï£«
ï£­
âˆ’2
1
âˆ’2
ï£¶
ï£¸, 1
3
ï£«
ï£­
âˆ’2
âˆ’2
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
BN(AT) =
ï£±
ï£²
ï£³
1
3
ï£«
ï£­
âˆ’1
2
2
ï£¶
ï£¸
ï£¼
ï£½
ï£¾

96
Solutions
BR(AT) =
ï£±
ï£´
ï£²
ï£´
ï£³
1
3
ï£«
ï£¬
ï£­
2
0
2
1
ï£¶
ï£·
ï£¸,
ï£«
ï£¬
ï£­
0
1
0
0
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
BN(A) =
ï£±
ï£´
ï£²
ï£´
ï£³
1
âˆš
2
ï£«
ï£¬
ï£­
âˆ’1
0
1
0
ï£¶
ï£·
ï£¸,
1
3
âˆš
2
ï£«
ï£¬
ï£­
âˆ’1
0
âˆ’1
4
ï£¶
ï£·
ï£¸
ï£¼
ï£´
ï£½
ï£´
ï£¾
The matrices U and V were deï¬ned in (5.11.8) to be
U =

BR(A) âˆªBN(AT)

= 1
3
ï£«
ï£­
âˆ’2
âˆ’2
âˆ’1
1
âˆ’2
2
âˆ’2
1
2
ï£¶
ï£¸
and
V =

BR(AT) âˆªBN(A)

= 1
3
ï£«
ï£¬
ï£­
2
0
âˆ’3/
âˆš
2
âˆ’1/
âˆš
2
0
3
0
0
2
0
3/
âˆš
2
âˆ’1/
âˆš
2
1
0
0
4/
âˆš
2
ï£¶
ï£·
ï£¸.
Direct multiplication now produces
R = UT AV =
ï£«
ï£­
9
0
0
0
0
3
0
0
0
0
0
0
ï£¶
ï£¸.
5.11.10. According to the discussion of projectors on p. 386, the unique vectors satisfying
v = x+y, x âˆˆR (A), and y âˆˆN

AT 	
are given by x = Pv and y = (Iâˆ’P)v,
where P is the projector onto R (A) along N

AT 	
. Use the results of Exercise
5.11.7 and Exercise 5.11.8 to compute
P = U1UT
1 = 1
9
ï£«
ï£­
8
2
2
2
5
âˆ’4
2
âˆ’4
5
ï£¶
ï£¸, x = Pv =
ï£«
ï£­
4
1
1
ï£¶
ï£¸, y = (I âˆ’P)v =
ï£«
ï£­
âˆ’1
2
2
ï£¶
ï£¸.
5.11.11. Observe that
R (A) âˆ©N (A) = 0
=â‡’
index(A) â‰¤1,
R (A) Ì¸âŠ¥N (A)
=â‡’
A is singular,
R (A) Ì¸âŠ¥N (A)
=â‡’
R

AT 	
Ì¸= R (A).
It is now trial and error to build a matrix that satisï¬es the three conditions on
the right-hand side. One such matrix is A =

1
2
1
2

.
5.11.12. R (A) âŠ¥N (A) =â‡’R (A)âˆ©N (A) = 0 =â‡’index(A) = 1 by using (5.10.4).
The example in the solution to Exercise 5.11.11 shows that the converse is false.
5.11.13. The facts that real symmetric
=â‡’
hermitian
=â‡’
normal are direct conse-
quences of the deï¬nitions. To show that normal =â‡’RPN, use (4.5.5) to write
R (A) = R (AAâˆ—) = R (Aâˆ—A) = R (Aâˆ—). The matrix

1
i
âˆ’i
2

is hermitian
but not symmetric. To construct a matrix that is normal but not hermitian or

Solutions
97
real symmetric, try to ï¬nd an example with real numbers. If A =

a
b
c
d

,
then
AAT =

a2 + b2
ac + bd
ac + bd
c2 + d2

and
AT A =

a2 + c2
ab + cd
ab + cd
b2 + d2

,
so we need to have b2 = c2. One such matrix is A =

1
âˆ’1
1
1

. To construct
a singular matrix that is RPN but not normal, try again to ï¬nd an example with
real numbers. For any orthogonal matrix P and nonsingular matrix C, the
matrix A = P

C
0
0
0

PT is RPN. To prevent A from being normal, simply
choose C to be nonnormal. For example, let C =

1
2
3
4

and P = I.
5.11.14. (a)
Aâˆ—A = AAâˆ—
=â‡’
(A âˆ’Î»I)âˆ—(A âˆ’Î»I) = (A âˆ’Î»I) (A âˆ’Î»I)âˆ—
=â‡’
(A âˆ’Î»I) is normal
=â‡’
(A âˆ’Î»I) is RPN
=â‡’
R (A âˆ’Î»I) âŠ¥N (A âˆ’Î»I) .
(b)
Suppose x âˆˆN (A âˆ’Î»I) and y âˆˆN (A âˆ’ÂµI), and use the fact that
N (A âˆ’Î»I) = N (A âˆ’Î»I)âˆ—to write
(A âˆ’Î»I) x = 0
=â‡’
0 = xâˆ—(A âˆ’Î»I)
=â‡’
0 = xâˆ—(A âˆ’Î»I) y
= xâˆ—(Âµy âˆ’Î»y) = xâˆ—y(Âµ âˆ’Î»)
=â‡’
xâˆ—y = 0.
Solutions for exercises in section 5. 12
5.12.1. Since CT C =

25
0
0
100

, Ïƒ2
1 = 100, and itâ€™s clear that x = e2 is a vector
such that (CT C âˆ’100I)x = 0 and âˆ¥xâˆ¥2 = 1. Let y = Cx/Ïƒ1 =

âˆ’3/5
âˆ’4/5

.
Following the procedure in Example 5.6.3, set ux = x âˆ’e1 and uy = y âˆ’e1,
and construct
Rx = I âˆ’2uxuT
x
uTx ux
=

0
1
1
0

and
Ry = I âˆ’2uyuT
y
uTy uy
=

âˆ’3/5
âˆ’4/5
âˆ’4/5
3/5

.
Since RyCRx =

10
0
0
5

= D, it follows that C = RyDRx is a singular
value decomposition of C.
5.12.2. Î½2
1(A) = Ïƒ2
1 = âˆ¥Aâˆ¥2
2 needs no proofâ€”itâ€™s just a restatement of (5.12.4). The
fact that Î½2
r(A) = âˆ¥Aâˆ¥2
F amounts to observing that
âˆ¥Aâˆ¥2
F = trace

AT A
	
= traceV

D2
0
0
0

VT = trace

D2	
= Ïƒ2
1 + Â· Â· Â· + Ïƒ2
r.

98
Solutions
5.12.3. If Ïƒ1 â‰¥Â· Â· Â· â‰¥Ïƒr are the nonzero singular values for A, then it follows from
Exercise 5.12.2 that âˆ¥Aâˆ¥2
2 = Ïƒ2
1 â‰¤Ïƒ2
1 + Ïƒ2
2 + Â· Â· Â· + Ïƒ2
r = âˆ¥Aâˆ¥2
F â‰¤nÏƒ2
1 = nâˆ¥Aâˆ¥2
2.
5.12.4. If rank (A + E) = k < r, then (5.12.10) implies that
âˆ¥Eâˆ¥2 = âˆ¥A âˆ’(A + E)âˆ¥2 â‰¥
min
rank(B)=k âˆ¥A âˆ’Bâˆ¥2 = Ïƒk+1 â‰¥Ïƒr,
which is impossible. Hence rank (A + E) â‰¥r = rank (A).
5.12.5. The argument is almost identical to that given for the nonsingular case except
that Aâ€  replaces Aâˆ’1. Start with SVDs
A = U

D
0
0
0

VT
and
Aâ€  = V

Dâˆ’1
0
0
0

UT ,
where D = diag (Ïƒ1, Ïƒ2, . . . , Ïƒr) , and note that
,,Aâ€ Ax
,,
2 â‰¤
,,Aâ€ A
,,
2 âˆ¥xâˆ¥2 = 1
with equality holding when Aâ€ A = I (i.e., when r = n ). For each y âˆˆA(S2)
there is an x âˆˆS2 such that y = Ax, so, with w = UT y,
1 â‰¥
,,Aâ€ Ax
,,2
2 =
,,Aâ€ y
,,2
2 =
,,VDâˆ’1UT y
,,2
2 =
,,Dâˆ’1UT y
,,2
2
=
,,Dâˆ’1w
,,2
2 = w2
1
Ïƒ2
1
+ w2
2
Ïƒ2
2
+ Â· Â· Â· + w2
r
Ïƒ2r
with equality holding when r = n. In other words, the set UT A(S2) is an
ellipsoid (degenerate if r < n ) whose kth semiaxis has length Ïƒk. To resolve
the inequality with what it means for points to be on an ellipsoid, realize that the
surface of a degenerate ellipsoid (one having some semiaxes with zero length) is
actually the set of all points in and on a smaller dimension ellipsoid. For example,
visualize an ellipsoid in â„œ3, and consider what happens as one of its semiaxes
shrinks to zero. The skin of the three-dimensional ellipsoid degenerates to a solid
planar ellipse. In other words, all points on a degenerate ellipsoid with semiaxes
of length Ïƒ1 Ì¸= 0, Ïƒ2 Ì¸= 0, Ïƒ3 = 0 are actually points on and inside a planar
ellipse with semiaxes of length Ïƒ1 and Ïƒ2. Arguing that the kth semiaxis of
A(S2) is ÏƒkUâˆ—k = AVâˆ—k is the same as the nonsingular case given in the text.
5.12.6. If A = U

D
0
0
0

VT and Aâ€ 
nÃ—m = V

Dâˆ’1
0
0
0

UT are SVDs in which
V =

V1 | V2
	
, then the columns of V1 are an orthonormal basis for R

AT 	
,
so x âˆˆR

AT 	
and âˆ¥xâˆ¥2 = 1 if and only if x = V1y with âˆ¥yâˆ¥2 = 1. Since
the 2-norm is unitarily invariant (Exercise 5.6.9),
min
âˆ¥xâˆ¥2=1
xâˆˆR(AT )
âˆ¥Axâˆ¥2 =
min
âˆ¥yâˆ¥2=1 âˆ¥AV1yâˆ¥2 =
min
âˆ¥yâˆ¥2=1 âˆ¥Dyâˆ¥2 =
1
âˆ¥Dâˆ’1âˆ¥2
= Ïƒr =
1
âˆ¥Aâ€ âˆ¥2
.

Solutions
99
5.12.7. x = Aâ€ b and Ëœx = Aâ€ (b âˆ’e) are the respective solutions of minimal 2-norm of
Ax = b and AËœx = Ëœb = b âˆ’e. The development of the more general bound is
the same as for (5.12.8).
âˆ¥x âˆ’Ëœxâˆ¥= âˆ¥Aâ€ (b âˆ’Ëœb)âˆ¥â‰¤âˆ¥Aâ€ âˆ¥âˆ¥b âˆ’Ëœbâˆ¥,
b = Ax
=â‡’
âˆ¥bâˆ¥â‰¤âˆ¥Aâˆ¥âˆ¥xâˆ¥
=â‡’
1/âˆ¥xâˆ¥â‰¤âˆ¥Aâˆ¥/âˆ¥bâˆ¥,
so
âˆ¥x âˆ’Ëœxâˆ¥
âˆ¥xâˆ¥
â‰¤

âˆ¥Aâ€ âˆ¥âˆ¥b âˆ’Ëœbâˆ¥
 âˆ¥Aâˆ¥
âˆ¥bâˆ¥= Îº âˆ¥eâˆ¥
âˆ¥bâˆ¥.
Similarly,
âˆ¥b âˆ’Ëœbâˆ¥= âˆ¥A(x âˆ’Ëœx)âˆ¥â‰¤âˆ¥Aâˆ¥âˆ¥x âˆ’Ëœxâˆ¥,
x = Aâ€ b
=â‡’
âˆ¥xâˆ¥â‰¤âˆ¥Aâ€ âˆ¥âˆ¥bâˆ¥
=â‡’
1/âˆ¥bâˆ¥â‰¤âˆ¥Aâ€ âˆ¥/âˆ¥xâˆ¥,
so
âˆ¥b âˆ’Ëœbâˆ¥
âˆ¥bâˆ¥
â‰¤(âˆ¥Aâˆ¥âˆ¥x âˆ’Ëœxâˆ¥) âˆ¥Aâ€ âˆ¥
âˆ¥xâˆ¥= Îºâˆ¥x âˆ’Ëœxâˆ¥
âˆ¥xâˆ¥
.
Equality was attained in Example 5.12.1 by choosing b and e to point in
special directions. But for these choices, Ax = b and AËœx = Ëœb = b âˆ’e cannot
be guaranteed to be consistent for all singular or rectangular matrices A, so
the answer to the second part is â€œno.â€ However, the argument of Example 5.12.1
proves equality for all A such that AAâ€  = I (i.e., when rank (AmÃ—n) = m ).
5.12.8. If A = U

D
0
0
0

VT is an SVD, then AT A+ÏµI = U

D2 + ÏµI
0
0
ÏµI

VT is
an SVD with no zero singular values, so itâ€™s nonsingular. Furthermore,
(AT A + ÏµI)âˆ’1AT = U

(D2 + ÏµI)âˆ’1D
0
0
0

VT â†’U

Dâˆ’1
0
0
0

VT = Aâ€ .
5.12.9. Since Aâˆ’1 =

âˆ’266000
667000
333000
âˆ’835000

,
Îºâˆ= âˆ¥Aâˆ¥âˆ
,,Aâˆ’1,,
âˆ= 1, 754, 336.
Similar to the 2-norm situation discussed in Example 5.12.1, the worst case is
realized when b is in the direction of a maximal vector in A(Sâˆ) while e is
in the direction of a minimal vector in A(Sâˆ). Sketch A(Sâˆ) as shown below
to see that v = ( 1.502
.599 )T is a maximal vector in A(Sâˆ).

100
Solutions
(.168, .067)
(-.168, -.067)
(-1.502, -.599)
(1.502, .599)
(1, -1)
(-1, -1)
(-1, 1)
(1, 1)
A
Itâ€™s not clear which vector is minimalâ€”donâ€™t assume ( .168
.067 )T is. A min-
imal vector y in A(Sâˆ) satisï¬es âˆ¥yâˆ¥âˆ= minâˆ¥xâˆ¥âˆ=1 âˆ¥Axâˆ¥âˆ= 1/
,,Aâˆ’1,,
âˆ
(see (5.2.6) on p. 280), so, for y = Ax0 with âˆ¥x0âˆ¥âˆ= 1,
,,,,Aâˆ’1
 y
âˆ¥yâˆ¥âˆ
,,,,
âˆ
= âˆ¥x0âˆ¥âˆ
âˆ¥yâˆ¥âˆ
=
1
âˆ¥yâˆ¥âˆ
=
,,Aâˆ’1,,
âˆ=
max
âˆ¥zâˆ¥âˆ=1
,,Aâˆ’1z
,,
âˆ.
In other words, Ë†y = y/ âˆ¥yâˆ¥âˆmust be a vector in Sâˆthat receives maximal
stretch under Aâˆ’1. You donâ€™t have to look very hard to ï¬nd such a vector
because its components are Â±1â€”recall the proof of (5.2.15) on p. 283. Notice
that Ë†y = ( 1
âˆ’1 )T âˆˆSâˆ, and Ë†y receives maximal stretch under Aâˆ’1 because
,,Aâˆ’1y
,,
âˆ= 1, 168, 000 =
,,Aâˆ’1,,
âˆ, so setting
b = Î±v = Î±

1.502
.599

and
e = Î²Ë†y = Î²

1
âˆ’1

produces equality in (5.12.8), regardless of Î± and Î². You may wish to compu-
tationally verify that this is indeed the case.
5.12.10. (a)
Consider A =

Ïµ
âˆ’1
1
0

or A =

Ïµ
Ïµn
0
Ïµ

for small Ïµ Ì¸= 0.
(b)
For Î± > 1, consider
A =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
âˆ’Î±
0
Â· Â· Â·
0
0
1
âˆ’Î±
Â· Â· Â·
0
...
...
...
...
...
0
0
Â· Â· Â·
1
âˆ’Î±
0
0
Â· Â· Â·
0
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
nÃ—n
and Aâˆ’1 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
Î±
Â· Â· Â·
Î±nâˆ’2
Î±nâˆ’1
0
1
Â· Â· Â·
Î±nâˆ’3
Î±nâˆ’2
...
...
...
...
...
0
0
Â· Â· Â·
1
Î±
0
0
Â· Â· Â·
0
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Regardless of which norm is used, âˆ¥Aâˆ¥> Î± and
,,Aâˆ’1,, > Î±nâˆ’1, so Îº > Î±n
exhibits exponential growth. Even for moderate values of n and Î± > 1, Îº can
be quite large.

Solutions
101
5.12.11. For B = Aâˆ’1E, write (A âˆ’E) = A(I âˆ’B), and use the Neumann series
expansion to obtain
Ëœx = (Aâˆ’E)âˆ’1b = (Iâˆ’B)âˆ’1Aâˆ’1b = (I+B+B2+Â· Â· Â·)x = x+B(I+B+B2+Â· Â· Â·)x.
Therefore, âˆ¥x âˆ’Ëœxâˆ¥â‰¤âˆ¥Bâˆ¥
âˆ
n=0 âˆ¥Bâˆ¥n âˆ¥xâˆ¥â‰¤
,,Aâˆ’1,, âˆ¥Eâˆ¥âˆ¥xâˆ¥
âˆ
n=0 Î±n, so
âˆ¥x âˆ’Ëœxâˆ¥
âˆ¥xâˆ¥
â‰¤
,,Aâˆ’1,, âˆ¥Eâˆ¥
1
1 âˆ’Î± = âˆ¥Aâˆ¥
,,Aâˆ’1,, âˆ¥Eâˆ¥
âˆ¥Aâˆ¥
1
1 âˆ’Î± =
Îº
1 âˆ’Î±
âˆ¥Eâˆ¥
âˆ¥Aâˆ¥.
5.12.12. Begin with
x âˆ’Ëœx = x âˆ’(I âˆ’B)âˆ’1Aâˆ’1(b âˆ’e) =

I âˆ’(I âˆ’B)âˆ’1	
x + (I âˆ’B)âˆ’1Aâˆ’1e.
Use the triangle inequality with b = Ax â‡’1/ âˆ¥xâˆ¥â‰¤âˆ¥Aâˆ¥/ âˆ¥bâˆ¥to obtain
âˆ¥x âˆ’Ëœxâˆ¥
âˆ¥xâˆ¥
â‰¤
,,I âˆ’(I âˆ’B)âˆ’1,, +
,,(I âˆ’B)âˆ’1,, Îº âˆ¥eâˆ¥
âˆ¥bâˆ¥.
Write (Iâˆ’B)âˆ’1 = 
âˆ
i=0 Bi, and use the identity Iâˆ’(I âˆ’B)âˆ’1 = âˆ’B(I âˆ’B)âˆ’1
to produce
,,(I âˆ’B)âˆ’1,, â‰¤
âˆ

i=0
âˆ¥Bâˆ¥i =
1
1 âˆ’âˆ¥Bâˆ¥
and
,,I âˆ’(I âˆ’B)âˆ’1,, â‰¤
âˆ¥Bâˆ¥
1 âˆ’âˆ¥Bâˆ¥.
Now combine everything above with âˆ¥Bâˆ¥â‰¤
,,Aâˆ’1,, âˆ¥Eâˆ¥= Îº âˆ¥Eâˆ¥/ âˆ¥Aâˆ¥.
5.12.13. Even though the URV factors are not unique, Aâ€  is, so in each case you should
arrive at the same matrix
Aâ€  = VRâ€ UT = 1
81
ï£«
ï£¬
ï£­
âˆ’4
2
âˆ’4
âˆ’18
âˆ’18
9
âˆ’4
2
âˆ’4
âˆ’2
1
âˆ’2
ï£¶
ï£·
ï£¸.
5.12.14. By (5.12.17), the minimum norm solution is Aâ€ b = (1/9) ( 10
9
10
5 )T .
5.12.15. U is a unitary matrix in which the columns of U1 are an orthonormal basis for
R (A) and the columns of U2 are an orthonormal basis for N

AT 	
, so setting
X = U1,
Y = U2, and

X | Y
âˆ’1 = UT in (5.9.12) produces P = U1UT
1 .
Furthermore,
AAâ€  = U

C
0
0
0

VT V

Câˆ’1
0
0
0

UT = U

I
0
0
0

UT = U1UT
1 .
According to (5.9.9), the projector onto N

AT 	
along R (A) is I âˆ’P = I âˆ’
U1UT
1 = U2UT
2 = I âˆ’AAâ€ .

102
Solutions
5.12.16. (a)
When A is nonsingular, U = V = I and R = A, so Aâ€  = Aâˆ’1.
(b)
If A = URVT is as given in (5.12.16), where R =

C
0
0
0

, it is clear
that (Râ€ )
â€  = R, and hence (Aâ€ )
â€  = (VRâ€ UT )â€  = U(Râ€ )
â€  VT = URVT =
A.
(c)
For R as above, it is easy to see that (Râ€ )
T = (RT )
â€  , so an argument
similar to that used in part (b) leads to (Aâ€ )
T = (AT )
â€  .
(d)
When rank (AmÃ—n) = n, an SVD must have the form
A = UmÃ—m

DnÃ—n
0mâˆ’nÃ—n

InÃ—n,
so
Aâ€  = I ( Dâˆ’1
0 ) UT .
Furthermore, AT A = D2, and (AT A)âˆ’1AT = I ( Dâˆ’1
0 ) UT = Aâ€ . The
other part is similar.
(e)
AT AAâ€  = V

CT
0
0
0

UT U

CrÃ—r
0
0
0

VT V

Câˆ’1
0
0
0

UT = AT .
The other part is similar.
(f)
Use an SVD to write
AT (AAT )â€  = V

DT
0
0
0

UT U

Dâˆ’2
0
0
0

UT = V

Dâˆ’1
0
0
0

UT = Aâ€ .
The other part is similar.
(g)
The URV factorization insures that rank

Aâ€ 	
= rank (A) = rank

AT 	
,
and part (f) implies R

Aâ€ 	
âŠ†R

AT 	
, so R

Aâ€ 	
= R

AT 	
. Argue that
R

AT 	
= R

Aâ€ A
	
by using Exercise 5.12.15. The other parts are similar.
(h)
If A = URVT is a URV factorization for A, then (PU)R(QT V)T is a
URV factorization for B = PAQ. So, by (5.12.16), we have
Bâ€  = QT V

Câˆ’1
0
0
0

UT PT = QT Aâ€ PT .
Almost any two singular or rectangular matrices can be used to build a coun-
terexample to show that (AB)â€  is not always the same as Bâ€ Aâ€ .
(i)
If A = URVT , then (AT A)â€  = (VRT UT URV)â€  = VT (RT R)â€ VT . Sim-
ilarly, Aâ€ (AT )â€  = VRâ€ UT URT â€ VT = VRâ€ RT â€ VT = VT (RT R)â€ VT . The
other part is argued in the same way.
5.12.17. If A is RPN, then index(A) = 1, and the URV decomposition (5.11.15) is a
similarity transformation of the kind (5.10.5). That is, N = 0 and Q = U, so
AD as deï¬ned in (5.10.6) is the same as Aâ€  as deï¬ned by (5.12.16). Conversely,
if Aâ€  = AD, then
AAD = ADA
=â‡’
Aâ€ A = AAâ€ 
=â‡’
R (A) = R

AT 	
.

Solutions
103
5.12.18. (a)
Recall that âˆ¥Bâˆ¥2
F = trace

BT B
	
, and use the fact that R (X) âŠ¥R (Y)
implies XT Y = 0 = YT X to write
âˆ¥X + Yâˆ¥2
F = trace

(X + Y)T (X + Y)

= trace

XT X + XT Y + YT X + YT Y
	
= trace

XT X
	
+ trace

YT Y
	
= âˆ¥Xâˆ¥2
F + âˆ¥Yâˆ¥2
F .
(b)
Consider X =

2
0
0
0

and Y =

0
0
0
3

.
(c)
Use the result of part (a) to write
âˆ¥I âˆ’AXâˆ¥2
F =
,,I âˆ’AAâ€  + AAâ€  âˆ’AX
,,2
F
=
,,I âˆ’AAâ€ ,,2
F +
,,AAâ€  âˆ’AX
,,2
F
â‰¥
,,I âˆ’AAâ€ ,,2
F ,
with equality holding if and only if AX = AAâ€ â€”i.e., if and only if X = Aâ€ +Z,
where R (Z) âŠ†N (A) âŠ¥R

AT 	
= R

Aâ€ 	
. Moreover, for any such X,
âˆ¥Xâˆ¥2
F =
,,Aâ€  + Z
,,2
F =
,,Aâ€ ,,2
F + âˆ¥Zâˆ¥2
F â‰¥
,,Aâ€ ,,2
F
with equality holding if and only if Z = 0.
Solutions for exercises in section 5. 13
5.13.1. PM = uuT /(uT u) = (1/10)
 9
3
3
1

, and PMâŠ¥= I âˆ’PM = (1/10)

1
âˆ’3
âˆ’3
9

,
so PMb =

6
2

, and PMâŠ¥b =

âˆ’2
6

.
5.13.2. (a)
Use any of the techniques described in Example 5.13.3 to obtain the fol-
lowing.
PR(A) =
ï£«
ï£­
.5
0
.5
0
1
0
.5
0
.5
ï£¶
ï£¸
PN(A) =
ï£«
ï£­
.8
âˆ’.4
0
âˆ’.4
.2
0
0
0
0
ï£¶
ï£¸
PR(AT ) =
ï£«
ï£­
.2
.4
0
.4
.8
0
0
0
1
ï£¶
ï£¸
PN(AT ) =
ï£«
ï£­
.5
0
âˆ’.5
0
0
0
âˆ’.5
0
.5
ï£¶
ï£¸
(b)
The point in N (A)âŠ¥that is closest to b is
PN(A)âŠ¥b = PR(AT )b =
ï£«
ï£­
.6
1.2
1
ï£¶
ï£¸.

104
Solutions
5.13.3. If x âˆˆR (P), then Px = xâ€”recall (5.9.10)â€”so âˆ¥Pxâˆ¥2 = âˆ¥xâˆ¥2 . Conversely,
suppose âˆ¥Pxâˆ¥2 = âˆ¥xâˆ¥2 , and let x = m+n, where m âˆˆR (P) and n âˆˆN (P)
so that m âŠ¥n. The Pythagorean theorem (Exercise 5.4.14) guarantees that
âˆ¥xâˆ¥2
2 = âˆ¥m + nâˆ¥2
2 = âˆ¥mâˆ¥2
2 + âˆ¥nâˆ¥2
2 . But we also have
âˆ¥xâˆ¥2
2 = âˆ¥Pxâˆ¥2
2 = âˆ¥P(m + n)âˆ¥2
2 = âˆ¥Pmâˆ¥2
2 = âˆ¥mâˆ¥2
2 .
Therefore, n = 0, and thus x = m âˆˆR (P).
5.13.4. (AT PR(A))T = PT
R(A)A = PR(A)A = A.
5.13.5. Equation (5.13.4) says that PM = UUT = 
r
i=1 uiuiT , where U contains the
ui â€™s as columns.
5.13.6. The Householder (or Givens) reduction technique can be employed as described
in Example 5.11.2 on p. 407 to compute orthogonal matrices U =

U1 | U2
	
and V =

V1 | V2
	
, which are factors in a URV factorization of A. Equation
(5.13.12) insures that
PR(A) = U1UT
1 ,
PN(AT ) = PR(A)âŠ¥= I âˆ’U1UT
1 = U2UT
2 ,
PR(AT ) = V1VT
1 ,
PN(A) = PR(AT )âŠ¥= I âˆ’V1VT
1 = V2VT
2 .
5.13.7. (a)
The only nonsingular orthogonal projector (i.e., the only nonsingular sym-
metric idempotent matrix) is the identity matrix. Consequently, for all other or-
thogonal projectors P, we must have rank (P) = 0 or rank (P) = 1, so P = 0
or, by Example 5.13.1, P = (uuT )/uT u. In other words, the 2 Ã— 2 orthogonal
projectors are P = I, P = 0, and, for a nonzero vector uT = ( Î±
Î² ) ,
P = uuT
uT u =
1
Î±2 + Î²2

Î±2
Î±Î²
Î±Î²
Î²2

.
(b)
P = I, P = 0, and, for nonzero vectors u, v âˆˆâ„œ2Ã—1, P = (uvT )/uT v.
5.13.8. If either u or v is the zero vector, then L is a one-dimensional subspace, and
the solution is given in Example 5.13.1. Suppose that neither u nor v is the
zero vector, and let p be the orthogonal projection of b onto L. Since L is the
translate of the subspace span {u âˆ’v} , subtracting u from everything moves
the situation back to the originâ€”the following picture illustrates this in â„œ2.

Solutions
105
b
p
u
v
L
L âˆ’u
b âˆ’u
u âˆ’v
p âˆ’u
In other words, L is translated back down to span {u âˆ’v} , b â†’b âˆ’u, and
p â†’p âˆ’u, so that p âˆ’u must be the orthogonal projection of b âˆ’u onto
span {u âˆ’v} . Example 5.13.1 says that
p âˆ’u = Pspan{uâˆ’v}(b âˆ’u) = (u âˆ’v)(u âˆ’v)T
(u âˆ’v)T (u âˆ’v)(b âˆ’u),
and thus
p = u +
(u âˆ’v)T (b âˆ’u)
(u âˆ’v)T (u âˆ’v)

(u âˆ’v).
5.13.9. âˆ¥A3x âˆ’bâˆ¥2 =
,,PR(A)b âˆ’b
,,
2 =
,,(I âˆ’PR(A))b
,,
2 =
,,PN(AT )b
,,
2
5.13.10. Use (5.13.17) with PR(A) = PT
R(A) = P2
R(A), to write
âˆ¥Îµâˆ¥2
2 = (b âˆ’PR(A)b)T (b âˆ’PR(A)b)
= bT b âˆ’bT PT
R(A)b âˆ’bT PR(A)b + bT PT
R(A)PR(A)b
= bT b âˆ’bT PR(A)b = âˆ¥bâˆ¥2
2 âˆ’
,,PR(A)b
,,2
2 .
5.13.11. According to (5.13.13) we must show that 
r
i=1(uiT x)ui = PMx. It follows
from (5.13.4) that if UnÃ—r is the matrix containing the vectors in B as columns,
then
PM = UUT =
r

i=1
uiui
T
=â‡’
PMx =
r

i=1
uiui
T x =
r

i=1
(ui
T x)ui.
5.13.12. Yes, the given spanning set {u1, u2, u3} is an orthonormal basis for M, so, by
Exercise 5.13.11,
PMb =
3

i=1
(ui
T b)ui = u1 + 3u2 + 7u3 =
ï£«
ï£¬
ï£­
5
0
5
3
ï£¶
ï£·
ï£¸.

106
Solutions
5.13.13. (a)
Combine the fact that PMPN = 0 if and only if R (PN ) âŠ†N (PM) with
the facts R (PN ) = N and N (PM) = MâŠ¥to write
PMPN = 0 â‡â‡’N âŠ†MâŠ¥â‡â‡’N âŠ¥M.
(b)
Yesâ€”this is a direct consequence of part (a). Alternately, you could say
0 = PMPN â‡â‡’0 = (PMPN )T = PT
N PT
M = PN PM.
5.13.14. (a)
Use Exercise 4.2.9 along with (4.5.5) to write
R (PM) + R (PN ) = R (PM | PN ) = R
ï£«
ï£­(PM | PN )
 PM
PN
T ï£¶
ï£¸
= R

PMPM
T + PN PN
T 
= R

P2
M + P2
N
	
= R (PM + PN ).
(b)
PMPN = 0 â‡â‡’R (PN ) âŠ†N (PM) â‡â‡’N âŠ†MâŠ¥â‡â‡’M âŠ¥N.
(c)
Exercise 5.9.17 says PM + PN is idempotent if and only if PMPN = 0 =
PN PM. Because PM and PN are symmetric, PMPN = 0 if and only if
PMPN = PN PM = 0 (via the reverse order law for transposition). The fact
that R (PM + PN ) = R (PM) âŠ•R (PN ) = M âŠ•N was established in Exercise
5.9.17, and M âŠ¥N follows from part (b).
5.13.15. First notice that PM +PN is symmetric, so (5.13.12) and the result of Exercise
5.13.14, part (a), can be combined to conclude that
(PM + PN )(PM + PN )â€  = (PM + PN )â€ (PM + PN ) = PR(PM+PN ) = PM+N .
Now, M âŠ†M + N implies PM+N PM = PM, and the reverse order law for
transposition yields PMPM+N = PM so that PM+N PM = PMPM+N . In
other words, (PM + PN )(PM + PN )â€ PM = PM(PM + PN )â€ (PM + PN ), or
PM(PM + PN )â€ PM + PN (PM + PN )â€ PM
= PM(PM + PN )â€ PM + PM(PM + PN )â€ PN .
Subtracting PM(PM + PN )â€ PM from both sides of this equation produces
PM(PM + PN )â€ PN = PN (PM + PN )â€ PM.
Let Z = 2PM(PM+PN )â€ PN = 2PN (PM+PN )â€ PM, and notice that R (Z) âŠ†
R (PM) = M and R (Z) âŠ†R (PN ) = N implies R (Z) âŠ†Mâˆ©N. Furthermore,
PMPMâˆ©N = PMâˆ©N = PN PMâˆ©N , and PM+N PMâˆ©N = PMâˆ©N , so, by the

Solutions
107
reverse order law for transposition, PMâˆ©N PM = PMâˆ©N = PMâˆ©N PN
and
PMâˆ©N PM+N = PMâˆ©N . Consequently,
Z = PMâˆ©N Z = PMâˆ©N

PM(PM + PN )â€ PN + PN (PM + PN )â€ PM

= PMâˆ©N (PM + PN )â€ (PM + PN ) = PMâˆ©N PM+N = PMâˆ©N .
5.13.16. (a)
Use the fact that AT = AT PR(A) = AT AAâ€  (see Exercise 5.13.4) to write
4 âˆ
0
eâˆ’AT AtAT dt =
4 âˆ
0
eâˆ’AT AtAT AAâ€ dt =
4 âˆ
0
eâˆ’AT AtAT Adt

Aâ€ 
=
%
âˆ’eâˆ’AT At&âˆ
0 Aâ€  = [0 âˆ’(âˆ’I)]Aâ€  = Aâ€ .
(b)
Recall from Example 5.10.5 that Ak = Ak+1AD = AkAAD, and write
4 âˆ
0
eâˆ’Ak+1tAkdt =
4 âˆ
0
eâˆ’Ak+1tAkAADdt =
4 âˆ
0
eâˆ’Ak+1tAk+1Adt

AD
=
%
âˆ’eâˆ’Ak+1t&âˆ
0 AD = [0 âˆ’(âˆ’I)]AD = AD.
(c)
This is just a special case of the formula in part (b) with k = 0. However,
it is easy to derive the formula directly by writing
4 âˆ
0
eâˆ’Atdt =
4 âˆ
0
eâˆ’AtAAâˆ’1dt =
4 âˆ
0
eâˆ’AtAdt

Aâˆ’1
=
%
eâˆ’At&âˆ
0 Aâˆ’1 = [0 âˆ’(âˆ’I)]Aâˆ’1 = Aâˆ’1.
5.13.17. (a)
The points in H are just solutions to a linear system uT x = Î². Using the
fact that the general solution of any linear system is a particular solution plus
the general solution of the associated homogeneous equation produces
H = Î²u
uT u + N(uT ) = Î²u
uT u + [R(u)]âŠ¥= Î²u
uT u + uâŠ¥,
where uâŠ¥denotes the orthogonal complement of the one-dimensional space
spanned by the vector u. Thus H = v+M, where v = Î²u/uT u and M = uâŠ¥.
The fact that dim (uâŠ¥) = n âˆ’1 follows directly from (5.11.3).
(b)
Use (5.13.14) with part (a) and the fact that PuâŠ¥= I âˆ’uuT /uT u to write
p = Î²u
uT u +

I âˆ’uuT
uT u
 
b âˆ’Î²u
uT u

= Î²u
uT u + b âˆ’uuT b
uT u = b âˆ’
uT b âˆ’Î²
uT u

u.

108
Solutions
5.13.18. (a)
uT w Ì¸= 0 implies M âˆ©W = 0 so that
dim (M + W) = dim M + dim W = (n âˆ’1) + 1 = n.
Therefore, M+W = â„œn. This together with Mâˆ©W = 0 means â„œn = MâŠ•W.
(b)
Write
b = b âˆ’uT b
uT ww + uT b
uT ww = p + uT b
uT ww,
and observe that p âˆˆM (because uT p = 0 ) and (uT b/uT w)w âˆˆW. By
deï¬nition, p is the projection of b onto M along W.
(c)
We know from Exercise 5.13.17, part (a), that H = v + M, where v =
Î²u/uT u and M = uâŠ¥, so subtracting v = Î²u/uT u from everything in H
as well as from b translates the situation back to the origin. Sketch a picture
similar to that of Figure 5.13.5 to see that this moves H back to M, it translates
b to bâˆ’v, and it translates p to pâˆ’v. Now, pâˆ’v should be the projection
of b âˆ’v onto M along W, so by the result of part (b),
pâˆ’v = bâˆ’vâˆ’uT (b âˆ’v)
uT w
w
=â‡’
p = bâˆ’uT (b âˆ’v)
uT w
w = bâˆ’
uT b âˆ’Î²
uT w

w.
5.13.19. For convenience, set Î² = Aiâˆ—pkn+iâˆ’1 âˆ’bi so that pkn+i = pkn+iâˆ’1 âˆ’Î²(Aiâˆ—)T .
Use the fact that
Aiâˆ—(pkn+iâˆ’1 âˆ’x) = Aiâˆ—pkn+iâˆ’1 âˆ’bi = Î²
together with âˆ¥Aiâˆ—âˆ¥2 = 1 to write
âˆ¥pkn+i âˆ’xâˆ¥2
2 =
,,,pkn+iâˆ’1 âˆ’Î²(Aiâˆ—)T âˆ’x
,,,
2
2
=
,,,(pkn+iâˆ’1 âˆ’x) âˆ’Î²(Aiâˆ—)T ,,,
2
2
= (pkn+iâˆ’1 âˆ’x)T (pkn+iâˆ’1 âˆ’x)
âˆ’2Î²Aiâˆ—(pkn+iâˆ’1 âˆ’x) + Î²2Aiâˆ—(Aiâˆ—)T
= âˆ¥pkn+iâˆ’1 âˆ’xâˆ¥2
2 âˆ’Î²2.
Consequently, âˆ¥pkn+i âˆ’xâˆ¥2 â‰¤âˆ¥pkn+iâˆ’1 âˆ’xâˆ¥2 , with equality holding if and
only if Î² = 0 or, equivalently, if and only if pkn+iâˆ’1 âˆˆHiâˆ’1 âˆ©Hi. Therefore,
the sequence of norms âˆ¥pkn+i âˆ’xâˆ¥2 is monotonically decreasing, and hence it
must have a limiting value. This implies that the sequence of the Î² â€™s deï¬ned
above must approach 0, and thus the sequence of the pkn+i â€™s converges to x.
5.13.20. Refer to Figure 5.13.8, and notice that the line passing from p(1)
1
to p(1)
2
is
parallel to V = span

p(1)
1
âˆ’p(1)
2

, so projecting p(1)
1
through p(1)
2
onto H2

Solutions
109
is exactly the same as projecting p(1)
1
onto H2 along (i.e., parallel to) V.
According to part (c) of Exercise 5.13.18, this projection is given by
p(2)
2
= p(1)
1
âˆ’
A2âˆ—

p(1)
1
âˆ’b1AT
2âˆ—

A2âˆ—

p(1)
1
âˆ’p(1)
2


p(1)
1
âˆ’p(1)
2

= p(1)
1
âˆ’

A2âˆ—p(1)
1
âˆ’b1

A2âˆ—

p(1)
1
âˆ’p(1)
2


p(1)
1
âˆ’p(1)
2

.
All other projections are similarly derived. It is now straightforward to verify
that the points created by the algorithm are exactly the same points described
in Steps 1, 2, . . . , n âˆ’1.
Note:
The condition that
'
p(1)
1
âˆ’p(1)
2

,

p(1)
1
âˆ’p(1)
3

, . . . ,

p(1)
1
âˆ’p(1)
n
(
is independent insures that
'
p(2)
2
âˆ’p(2)
3

,

p(2)
2
âˆ’p(2)
4

, . . . ,

p(2)
2
âˆ’p(2)
n
(
is also independent. The same holds at each subsequent step. Furthermore,
A2âˆ—

p(1)
1
âˆ’p(1)
k

Ì¸= 0 for k > 1 implies that Vk = span

p(1)
1
âˆ’p(1)
k

is
not parallel to H2, so all projections onto H2 along Vk are well deï¬ned. It can
be argued that the analogous situation holds at each step of the processâ€”i.e.,
the initial conditions insure Ai+1âˆ—

p(i)
i
âˆ’p(i)
k

Ì¸= 0 for k > i.
5.13.21. Equation (5.13.13) says that the orthogonal distance between x and MâŠ¥is
dist (x, MâŠ¥) = âˆ¥x âˆ’PMâŠ¥xâˆ¥2 = âˆ¥(I âˆ’PMâŠ¥)xâˆ¥2 = âˆ¥PMxâˆ¥2 .
Similarly,
dist (Rx, MâŠ¥) = âˆ¥PMRxâˆ¥2 = âˆ¥âˆ’PMxâˆ¥2 = âˆ¥PMxâˆ¥2 .
5.13.22. (a)
We know from Exercise 5.13.17 that H = v + uâŠ¥, where v = Î²u, so
subtracting v from everything in H as well as from b translates the situation
back to the origin. As depicted in the diagram below, this moves H down to
uâŠ¥, and it translates b to b âˆ’v and r to r âˆ’v.

110
Solutions
v
u
uâŠ¥
0
b
p
b - v
p - v
H
Now, we know from (5.6.8) that the reï¬‚ection of b âˆ’v about uâŠ¥is
r âˆ’v = R(b âˆ’v) = (I âˆ’2uuT )(b âˆ’v) = b + (Î² âˆ’2uT b)u,
and therefore the reï¬‚ection of b about H is
r = R(b âˆ’v) + v = b âˆ’2(uT b âˆ’Î²)u.
(b)
From part (a), the reï¬‚ection of r0 about Hi is
ri = r0 âˆ’2(Aiâˆ—r0 âˆ’bi) (Aiâˆ—)T ,
and therefore the mean value of all of the reï¬‚ections {r1, r2, . . . , rn} is
m = 1
n
n

i=1
ri = 1
n
n

i=1

r0 âˆ’2(Aiâˆ—r0 âˆ’bi) (Aiâˆ—)T 
= r0 âˆ’2
n
n

i=1
(Aiâˆ—r0 âˆ’bi)(Aiâˆ—)T
= r0 âˆ’2
nAT (Ar0 âˆ’b) = r0 âˆ’2
nAT Îµ.
Note: If weights wi > 0 such that 
 wi = 1 are used, then the weighted mean
is
m =
n

i=1
wiri =
n

i=1
wi

r0 âˆ’2(Aiâˆ—r0 âˆ’bi) (Aiâˆ—)T 
= r0 âˆ’2
n

i=1
wi(Aiâˆ—r0 âˆ’bi)(Aiâˆ—)T
= r0 âˆ’2
nAT W (Ar0 âˆ’b) = r0 âˆ’2
nAT WÎµ,

Solutions
111
where W = diag {w1, w2, . . . , wn} .
(c)
First observe that
x âˆ’mk = x âˆ’mkâˆ’1 + 2
nAT Îµkâˆ’1
= x âˆ’mkâˆ’1 + 2
nAT (Amkâˆ’1 âˆ’b)
= x âˆ’mkâˆ’1 + 2
nAT (Amkâˆ’1 âˆ’Ax)
= x âˆ’mkâˆ’1 + 2
nAT A(mkâˆ’1 âˆ’x)
=

I âˆ’2
nAT A

(x âˆ’mkâˆ’1),
and then use successive substitution to conclude that
x âˆ’mk =

I âˆ’2
nAT A
k
(x âˆ’m0).
Solutions for exercises in section 5. 14
5.14.1. Use (5.14.5) to observe that
E[yiyj] = Cov[yi, yj] + ÂµyiÂµyj =

Ïƒ2 + (Xiâˆ—Î²)2
if i = j,
(Xiâˆ—Î²)(Xjâˆ—Î²)
if i Ì¸= j,
so that
E[yyT ] = Ïƒ2I + (XÎ²)(XÎ²)T = Ïƒ2I + XÎ²Î²T XT .
Write Ë†e = y âˆ’XË†Î² = (Iâˆ’XXâ€ )y, and use the fact that Iâˆ’XXâ€  is idempotent
to obtain
Ë†eT Ë†e = yT (I âˆ’XXâ€ )y = trace

(I âˆ’XXâ€ )yyT 	
.
Now use the linearity of trace and expectation together with the result of Exercise
5.9.13 and the fact that (I âˆ’XXâ€ )X = 0 to write
E[Ë†eT Ë†e] = E

trace

(I âˆ’XXâ€ )yyT 	
= trace

E[(I âˆ’XXâ€ )yyT ]
	
= trace

(I âˆ’XXâ€ )E[yyT ]
	
= trace

(I âˆ’XXâ€ )(Ïƒ2I + XÎ²Î²T XT )

= Ïƒ2trace

I âˆ’XXâ€ 	
= Ïƒ2 
m âˆ’trace

XXâ€ 		
= Ïƒ2 
m âˆ’rank

XXâ€ 		
= Ïƒ2(m âˆ’n).

112
Solutions
Solutions for exercises in section 5. 15
5.15.1. (a)
Î¸min = 0, and Î¸max = Î¸ = Ï† = Ï€/4.
(b)
Î¸min = Î¸ = Ï† = Ï€/4, and Î¸max = 1.
5.15.2. (a)
The ï¬rst principal angle is Î¸1 = Î¸min = 0, and we can take u1 = v1 = e1.
This means that
M2 = uâŠ¥
1 âˆ©M = span {e2}
and
N2 = vâŠ¥
1 âˆ©N = span {(0, 1, 1)} .
The second principal angle is the minimal angle between M2 and N2, and this
is just the angle between e2 and (0, 1, 1), so Î¸2 = Ï€/4.
(b)
This time the ï¬rst principal angle is Î¸1 = Î¸min = Ï€/4, and we can take
u1 = e1 and v1 = (0, 1/
âˆš
2, 1/
âˆš
2). There are no more principal angles because
N2 = vâŠ¥
1 âˆ©N = 0.
5.15.3. (a)
This follows from (5.15.16) because PM = PN if and only if M = N.
(b)
If 0 Ì¸= x âˆˆM âˆ©N, then (5.15.1) evaluates to 1 with the maximum being
attained at u = v = x/ âˆ¥xâˆ¥2 . Conversely, cos Î¸min = 1 =â‡’vT u = 1 for some
u âˆˆM and v âˆˆN such that âˆ¥uâˆ¥2 = 1 = âˆ¥vâˆ¥2 . But vT u = 1 = âˆ¥uâˆ¥2 âˆ¥vâˆ¥2
represents equality in the CBS inequality (5.1.3), and we know this occurs if and
only if v = Î±u for Î± = vT u/uâˆ—u = 1/1 = 1. Thus u = v âˆˆM âˆ©N.
(c)
max
uâˆˆM, vâˆˆN
âˆ¥uâˆ¥2=âˆ¥vâˆ¥2=1
vT u = 0 â‡â‡’vT u = 0 âˆ€u âˆˆM, v âˆˆV â‡â‡’M âŠ¥N.
5.15.4. You can use either (5.15.3) or (5.15.4) to arrive at the result. The latter is used
by observing
,,(PMâŠ¥âˆ’PN âŠ¥)âˆ’1,,
2 =
,,,

(I âˆ’PM) âˆ’(I âˆ’PN )
	âˆ’1,,,
2
=
,,(PN âˆ’PM)âˆ’1,,
2 =
,,(PM âˆ’PN )âˆ’1,,
2 .
5.15.5. M âŠ•N âŠ¥= â„œn
=â‡’
dim M = dim N
=â‡’
sin Î¸max = Î´(M, N) = Î´(N, M),
so cos ËœÎ¸min = âˆ¥PMPN âŠ¥âˆ¥2 = âˆ¥PM(I âˆ’PN )âˆ¥2 = Î´(M, N) = sin Î¸max.
5.15.6. It was argued in the proof of (5.15.4) that PM âˆ’PN is nonsingular whenever
M and N are complementary, so we need only prove the converse. Suppose
dim M = r > 0 and dim N = k > 0 (the problem is trivial if r = 0 or
k = 0 ) so that UT
1 V1 is r Ã— n âˆ’k and UT
2 V2 is n âˆ’r Ã— k. If PM âˆ’PN is
nonsingular, then (5.15.7) insures that the rows as well as the columns in each of
these products must be linearly independent. That is, UT
1 V1 and UT
2 V2 must
both be square and nonsingular, so r + k = n. Combine this with the formula
for the rank of a product (4.5.1) to conclude
k = rank

UT
2 V2
	
= rank

UT
2
	
âˆ’dim N

UT
2
	
âˆ©R (V2)
= n âˆ’r âˆ’dim M âˆ©N = k âˆ’dim M âˆ©N.
It follows that M âˆ©N = 0, and hence M âŠ•N = â„œn.

Solutions
113
5.15.7. (a)
This can be derived from (5.15.7), or it can be veriï¬ed by direct multipli-
cation by using PN (I âˆ’P) = I âˆ’P
=â‡’
P âˆ’PN P = I âˆ’PN to write
(PM âˆ’PN )(P âˆ’Q) = PMP âˆ’PMQ âˆ’PN P + PN Q
= P âˆ’0 âˆ’PN P + PN Q = I âˆ’PN + PN Q
= I âˆ’PN (I âˆ’Q) = I.
(b) and (c) follow from (a) in conjunction with (5.15.3) and (5.15.4).
5.15.8. Since we are maximizing over a larger set, maxâˆ¥xâˆ¥=1 f(x) â‰¤maxâˆ¥xâˆ¥â‰¤1 f(x). A
strict inequality here implies the existence of a nonzero vector x0 such that
âˆ¥x0âˆ¥< 1 and f(x) < f(x0) for all vectors such that âˆ¥xâˆ¥= 1. But then
f(x0) > f(x0/ âˆ¥x0âˆ¥) = f(x0)/ âˆ¥x0âˆ¥
=â‡’
âˆ¥x0âˆ¥f(x0) > f(x0),
which is impossible because âˆ¥x0âˆ¥< 1.
5.15.9. (a)
We know from equation (5.15.6) that PMN = U

C
0
0
0

VT in which
C is nonsingular and Câˆ’1 = VT
1 U1. Consequently,
Pâ€ 
MN = V

Câˆ’1
0
0
0

UT = V1Câˆ’1UT
1 = V1VT
1 U1UT
1 = PN âŠ¥PM.
(b)
Use the fact
,,(UT
1 V1)âˆ’1,,
2 =
,,(VT
1 U1)âˆ’1,,
2 =
,,U1(VT
1 U1)âˆ’1VT
1
,,
2
=
,,(V1VT
1 U1UT
1 )â€ ,,
2 =
,,,

(I âˆ’PN )PM
â€ ,,,
2
(and similarly for the other term) to show that
,,,

(I âˆ’PN )PM
â€ ,,,
2 =
,,(UT
1 V1)âˆ’1,,
2 =
,,,

PM(I âˆ’PN )
â€ ,,,
2 ,
and
,,,

(I âˆ’PM)PN
â€ ,,,
2 =
,,(UT
2 V2)âˆ’1,,
2 =
,,,

PN (I âˆ’PM)
â€ ,,,
2 .
It was established in the proof of (5.15.4) that
,,(UT
1 V1)âˆ’1,,
2 =
,,(UT
2 V2)âˆ’1,,
2 ,
so combining this with the result of part (a) and (5.15.3) produces the desired
conclusion.
5.15.10. (a)
We know from (5.15.2) that cos Â¯Î¸min = âˆ¥PN âŠ¥PMâˆ¥2 = âˆ¥(I âˆ’PN )PMâˆ¥2 ,
and we know from Exercise 5.15.9 that PMN =

(I âˆ’PN )PM
â€ , so taking the
pseudoinverse of both sides of this yields the desired result.

114
Solutions
(b)
Use (5.15.3) together with part (a), (5.13.10), and (5.13.12) to write
1 =
,,,PMN Pâ€ 
MN
,,,
2 â‰¤
,,,PMN
,,,
2
,,,Pâ€ 
MN
,,,
2 = cos Â¯Î¸min
sin Î¸min
.
5.15.11. (a)
Use the facts that âˆ¥Aâˆ¥2 = âˆ¥AT âˆ¥2 and (AT )âˆ’1 = (Aâˆ’1)T to write
1
,,(UT
2 V2)âˆ’1,,2
2
=
1
,,(VT
2 U2)âˆ’1,,2
2
= min
âˆ¥xâˆ¥2=1
,,VT
2 U2x
,,2
2
= min
âˆ¥xâˆ¥2=1 xT UT
2 V2VT
2 U2x
= min
âˆ¥xâˆ¥2=1 xT UT
2 (I âˆ’V1VT
1 )U2x = min
âˆ¥xâˆ¥2=1

1 âˆ’
,,VT
1 U2x
,,2
2

= 1 âˆ’max
âˆ¥xâˆ¥2=1
,,VT
1 U2x
,,2
2 = 1 âˆ’
,,VT
1 U2
,,2
2 = 1 âˆ’
,,UT
2 V1
,,2
2 .
(b)
Use a similar technique to write
,,UT
2 V2
,,2
2 =
,,UT
2 V2VT
2
,,2
2 =
,,UT
2 (I âˆ’V1VT
1 )
,,2
2
=
,,(I âˆ’V1VT
1 )U2
,,2
2 = max
âˆ¥xâˆ¥2=1 xT UT
2 (I âˆ’V1VT
1 )U2x
= 1 âˆ’min
âˆ¥xâˆ¥2=1
,,VT
1 U2x
,,2
2 = 1 âˆ’
1
,,(VT
1 U2)âˆ’1,,2
2
= 1 âˆ’
1
,,(UT
2 V1)âˆ’1,,2
2
.

Solutions for Chapter 6
Solutions for exercises in section 6. 1
6.1.1. (a)
âˆ’1
(b)
8
(c)
âˆ’Î±Î²Î³
(d)
a11a22a33 + a12a23a31 + a13a21a32 âˆ’(a11a23a32 + a12a21a33 + a13a22a31)
(This is where the â€œdiagonal ruleâ€ you learned in high school comes from.)
6.1.2. If A = [x1 | x2 | x3], then V3 =

det

AT A
	1/2 = 20 (recall Example 6.1.4).
But you could also realize that the xi â€™s are mutually orthogonal to conclude
that V3 = âˆ¥x1âˆ¥2 âˆ¥x2âˆ¥2 âˆ¥x3âˆ¥2 = 20.
6.1.3. (a)
10
(b)
0
(c)
120
(d)
39
(e)
1
(f)
(n âˆ’1)!
6.1.4. rank (A) = 2
6.1.5. A square system has a unique solution if and only if its coeï¬ƒcient matrix is
nonsingularâ€”recall the discussion in Â§2.5. Consequently, (6.1.13) guarantees that
a square system has a unique solution if and only if the determinant of the
coeï¬ƒcient matrix is nonzero. Since

1
Î±
0
0
1
âˆ’1
Î±
0
1

= 1 âˆ’Î±2,
it follows that there is a unique solution if and only if Î± Ì¸= Â±1.
6.1.6. I = Aâˆ’1A
=â‡’
det (I) = det

Aâˆ’1A
	
= det

Aâˆ’1	
det (A)
=â‡’
1 = det

Aâˆ’1	
det (A)
=â‡’
det

Aâˆ’1	
= 1/det (A).
6.1.7. Use the product rule (6.1.15) to write
det

Pâˆ’1AP
	
= det

Pâˆ’1	
det (A)det (P) = det

Pâˆ’1	
det (P)det (A)
= det

Pâˆ’1P
	
det (A) = det (I)det (A) = det (A).
6.1.8. Use (6.1.4) together with the fact that z1z2 = Â¯z1Â¯z2 and z1 + z2 = Â¯z1 + Â¯z2 for
all complex numbers to write
det (Aâˆ—) = det
Â¯AT 	
= det
Â¯A
	
=

p
Ïƒ(p)a1p1 Â· Â· Â· anpn
=

p
Ïƒ(p)a1p1 Â· Â· Â· anpn =

p
Ïƒ(p)a1p1 Â· Â· Â· anpn = det (A).
6.1.9. (a)
I = Qâˆ—Q
=â‡’
1 = det (Qâˆ—Q) = det (Qâˆ—)det (Q) = [det (Q)]2 by
Exercise 6.1.8.

116
Solutions
(b)
If A = UDVâˆ—is an SVD, then, by part (a),
|det (A)| = |det (UDVâˆ—)| = |det (U)| |det (D)| |det (Vâˆ—)|
= det (D) = Ïƒ1Ïƒ2 Â· Â· Â· Ïƒn.
6.1.10. Let r = rank (A), and let Ïƒ1 â‰¥Â· Â· Â· â‰¥Ïƒr be the nonzero singular values of A.
If A = UmÃ—m

DrÃ—r
0
0
0

mÃ—n
(Vâˆ—)nÃ—n is an SVD, then, by Exercises 6.1.9 and
6.1.8, det (V)det (Vâˆ—) = |det (V)|2 = 1, so
det (Aâˆ—A) = det (VDâˆ—DVâˆ—) = det (V)

(Dâˆ—D)rÃ—r
0
0
0

nÃ—n
det (Vâˆ—)
= Ïƒ2
1Ïƒ2
2 Â· Â· Â· Ïƒ2
r 0 Â· Â· Â· 0
5 67 8,
nâˆ’r
and this is
' = 0
when r < n,
> 0
when r = n.
Note: You canâ€™t say det (Aâˆ—A) = det (A)det (A) = |det (A)|2 â‰¥0 because A
need not be square.
6.1.11. Î±A = (Î±I)A
=â‡’
det (Î±A) = det (Î±I)det (A) = Î±ndet (A).
6.1.12. A = âˆ’AT
=â‡’
det (A) = det

âˆ’AT 	
= det (âˆ’A) = (âˆ’1)ndet (A) (by
Exercise 6.1.11)
=â‡’
det (A) = âˆ’det (A) when n is odd
=â‡’
det (A) = 0.
6.1.13. If A = LU, where L is lower triangular and U is upper triangular where each
has 1â€™s on its diagonal and random integers in the remaining nonzero positions,
then det (A) = det (L)det (U) = 1 Ã— 1 = 1, and the entries of A are rather
random integers.
6.1.14. According to the deï¬nition,
det (A) =

p
Ïƒ(p)a1p1 Â· Â· Â· akpk Â· Â· Â· anpn
=

p
Ïƒ(p)a1p1 Â· Â· Â· (xpk + ypk + Â· Â· Â· + zpk) Â· Â· Â· anpn
=

p
Ïƒ(p)a1p1 Â· Â· Â· xpk Â· Â· Â· anpn +

p
Ïƒ(p)a1p1 Â· Â· Â· ypk Â· Â· Â· anpn
+ Â· Â· Â· +

p
Ïƒ(p)a1p1 Â· Â· Â· zpk Â· Â· Â· anpn
= det
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
A1âˆ—
...
xT
...
Anâˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
+ det
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
A1âˆ—
...
yT
...
Anâˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
+ Â· Â· Â· + det
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
A1âˆ—
...
zT
...
Anâˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.

Solutions
117
6.1.15. If AnÃ—2 = [x | y] , then the result of Exercise 6.1.10 implies
0 â‰¤det (Aâˆ—A) =

xâˆ—x
xâˆ—y
yâˆ—x
yâˆ—y
 = (xâˆ—x) (yâˆ—y) âˆ’(xâˆ—y) (yâˆ—x)
= âˆ¥xâˆ¥2
2 âˆ¥yâˆ¥2
2 âˆ’(xâˆ—y) (xâˆ—y)
= âˆ¥xâˆ¥2
2 âˆ¥yâˆ¥2
2 âˆ’|xâˆ—y|2,
with equality holding if and only if rank (A) < 2 â€”i.e., if and only if y is a
scalar multiple of x.
6.1.16. Partition A as
A = LU =

Lk
0
L21
L22
 
Uk
U12
0
U22

=

LkUk
âˆ—
âˆ—
âˆ—

to deduce that Ak can be written in the form
Ak = LkUk =

Lkâˆ’1
0
dT
1
 
Ukâˆ’1
c
0
ukk

and
Akâˆ’1 = Lkâˆ’1Ukâˆ’1.
The product rule (6.1.15) shows that
det (Ak) = det (Ukâˆ’1) Ã— ukk = det (Akâˆ’1) Ã— ukk,
and the desired conclusion follows.
6.1.17. According to (3.10.12), a matrix has an LU factorization if and only if each
leading principal submatrix is nonsingular. The leading k Ã— k principal subma-
trix of AT A is given by Pk = AT
k Ak, where Ak = [Aâˆ—1 | Aâˆ—2 | Â· Â· Â· | Aâˆ—k] . If
A has full column rank, then any nonempty subset of columns is linearly in-
dependent, so rank (Ak) = k. Therefore, the results of Exercise 6.1.10 insure
that det (Pk) = det

AT
k Ak
	
> 0 for each k, and hence AT A has an LU
factorization. The fact that each pivot is positive follows from Exercise 6.1.16.
6.1.18. (a)
To evaluate det (A), use Gaussian elimination as shown below.
ï£«
ï£­
2 âˆ’x
3
4
0
4 âˆ’x
âˆ’5
1
âˆ’1
3 âˆ’x
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
1
âˆ’1
3 âˆ’x
0
4 âˆ’x
âˆ’5
2 âˆ’x
3
4
ï£¶
ï£¸
âˆ’â†’
ï£«
ï£­
1
âˆ’1
3 âˆ’x
0
4 âˆ’x
âˆ’5
0
5 âˆ’x
âˆ’x2 + 5x âˆ’2
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
1
âˆ’1
3 âˆ’x
0
4 âˆ’x
âˆ’5
0
0
x3âˆ’9x2+17x+17
4âˆ’x
ï£¶
ï£¸= U.
Since one interchange was used, det (A) is (âˆ’1) times the product of the diag-
onal entries of U, so
det (A) = âˆ’x3 + 9x2 âˆ’17x âˆ’17
and
d

det (A)

dx
= âˆ’3x2 + 18x âˆ’17.

118
Solutions
(b)
Using formula (6.1.19) produces
d

det (A)

dx
=

âˆ’1
0
0
0
4 âˆ’x
âˆ’5
1
âˆ’1
3 âˆ’x

+

2 âˆ’x
3
4
0
âˆ’1
0
1
âˆ’1
3 âˆ’x

+

2 âˆ’x
3
4
0
4 âˆ’x
âˆ’5
0
0
âˆ’1

= (âˆ’x2 + 7x âˆ’7) + (âˆ’x2 + 5x âˆ’2) + (âˆ’x2 + 6x âˆ’8)
= âˆ’3x2 + 18x âˆ’17.
6.1.19. Noâ€”almost any 2 Ã— 2 example will show that this cannot hold in general.
6.1.20. It was argued in Example 4.3.6 that if there is at least one value of x for which
the Wronski matrix
W(x) =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
f1(x)
f2(x)
Â· Â· Â·
fn(x)
f â€²
1(x)
f â€²
2(x)
Â· Â· Â·
f â€²
n(x)
...
...
...
...
f (nâˆ’1)
1
(x)
f (nâˆ’1)
2
(x)
Â· Â· Â·
f (nâˆ’1)
n
(x)
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
is nonsingular, then S is a linearly independent set. This is equivalent to saying
that if S is a linearly dependent set, then the Wronski matrix W(x) is singular
for all values of x. But (6.1.14) insures that a matrix is singular if and only
if its determinant is zero, so, if S is linearly dependent, then the Wronskian
w(x) must vanish for every value of x. The converse of this statement is false
(Exercise 4.3.14).
6.1.21. (a)
(n!)(nâˆ’1)
(b)
11 Ã— 11
(c)
About 9.24Ã—10153 sec â‰ˆ3Ã—10146
years
(d)
About 3 Ã— 10150 mult/sec. (Now this would truly be a â€œsuper
computer.â€)
Solutions for exercises in section 6. 2
6.2.1. (a)
8
(b)
39
(c)
âˆ’3
6.2.2. (a)
Aâˆ’1 = adj (A)
det (A) = 1
8
ï£«
ï£­
0
1
âˆ’1
âˆ’8
4
4
16
âˆ’6
âˆ’2
ï£¶
ï£¸
(b)
Aâˆ’1 = adj (A)
det (A) = 1
39
ï£«
ï£¬
ï£­
âˆ’12
25
âˆ’14
7
âˆ’9
9
9
15
âˆ’6
6
6
âˆ’3
9
4
4
âˆ’2
ï£¶
ï£·
ï£¸
6.2.3. (a)
x1 = 1 âˆ’Î²,
x2 = Î± + Î² âˆ’1,
x3 = 1 âˆ’Î±

Solutions
119
(b)
Cramerâ€™s rule yields
x2(t) =

1
t4
t2
t2
t3
t
t
0
1


1
t
t2
t2
1
t
t
t2
1

=
t

t4
t2
t3
t
 +

1
t4
t2
t3


1
t
t2
1
 âˆ’t

t2
t
t
1
 + t2

t2
1
t
t2

=
t3 âˆ’t6
(t3 âˆ’1)(t3 âˆ’1) =
âˆ’t3
(t3 âˆ’1),
and hence
lim
tâ†’âˆx2(t) = lim
tâ†’âˆ
âˆ’1
1 âˆ’1/t3 = âˆ’1.
6.2.4. Yes.
6.2.5. (a)
Almost any two matrices will do the job. One example is A = I and
B = âˆ’I.
(b)
Again, almost anything you write down will serve the purpose. One example
is A = D = 02Ã—2, B = C = I2Ã—2.
6.2.6. Recall from Example 5.13.3 that Q = I âˆ’BBT Bâˆ’1BT . According to (6.2.1),
det

AT A
	
= det

BT B
BT c
cT B
cT c

= det

BT B
	 
cT Qc
	
.
Since det

BT B
	
> 0 (by Exercise 6.1.10), cT Qc = det

AT A
	
/det

BT B
	
.
6.2.7. Expand

A
âˆ’C
DT
Ik
 both of the ways indicated in (6.2.1).
6.2.8. The result follows from Example 6.2.8, which says A[adj (A)] = det (A) I, to-
gether with the fact that A is singular if and only if det (A) = 0.
6.2.9. The solution is x = Aâˆ’1b, and Example 6.2.7 says that the entries in Aâˆ’1 are
continuous functions of the entries in A. Since xi = 
k[Aâˆ’1]ikbk, and since
the sum of continuous functions is again continuous, it follows that each xi is a
continuous function of the aij â€™s.
6.2.10. If B = Î±A, then Exercise 6.1.11 implies BËšij = Î±nâˆ’1ËšAij, so B
Ëš = Î±nâˆ’1Ëš
A, and
hence adj (B) = Î±nâˆ’1adj (A) .
6.2.11. (a)
We saw in Â§6.1 that rank (A) is the order of the largest nonzero minor of
A. If rank (A) < n âˆ’1, then every minor of order n âˆ’1 (as well as det (A)
itself) must be zero. Consequently, Ëš
A = 0, and thus adj (A) = Ëš
A
T = 0.
(b)
rank (A) = n âˆ’1
=â‡’
at least one minor of order n âˆ’1 is nonzero
=â‡’
some ËšAij Ì¸= 0
=â‡’
adj (A) Ì¸= 0
=â‡’
rank (adj (A)) â‰¥1.

120
Solutions
Also,
rank (A) = n âˆ’1
=â‡’
det (A) = 0
=â‡’
A[adj (A)] = 0
(by Exercise 6.2.8)
=â‡’
R (adj (A)) âŠ†N (A)
=â‡’
dim R (adj (A)) â‰¤dim N (A)
=â‡’
rank (adj (A)) â‰¤n âˆ’rank (A) = 1.
(c)
rank (A) = n
=â‡’
det (A) Ì¸= 0
=â‡’
adj (A) = det (A) Aâˆ’1
=â‡’
rank (adj (A)) = n
6.2.12. If det (A) = 0, then Exercise 6.2.11 insures that rank (adj (A)) â‰¤1. Conse-
quently, det (adj (A)) = 0, and the result is trivially true because both sides
are zero. If det (A) Ì¸= 0, apply the product rule (6.1.15) to A[adj (A)] =
det (A) I (from Example 6.2.8) to obtain det (A)det (adj (A)) = [det (A)]n ,
so that det (adj (A)) = [det (A)]nâˆ’1 .
6.2.13. Expanding in terms of cofactors of the ï¬rst row produces Dn = 2ËšA11 âˆ’ËšA12. But
ËšA11 = Dnâˆ’1 and expansion using the ï¬rst column yields
ËšA12 = (âˆ’1)

âˆ’1
âˆ’1
0
Â· Â· Â·
0
0
2
âˆ’1
Â· Â· Â·
0
0
âˆ’1
2
Â· Â· Â·
0
...
...
...
...
...
0
0
0
Â· Â· Â·
2

= (âˆ’1)(âˆ’1)Dnâˆ’2,
so Dn = 2Dnâˆ’1 âˆ’Dnâˆ’2. By recursion (or by direct substitution), it is easy to
see that the solution of this equation is Dn = n + 1.
6.2.14. (a)
Use the results of Example 6.2.1 with Î»i = 1/Î±i.
(b)
Recognize that the matrix A is a rank-one updated matrix in the sense
that
A = (Î± âˆ’Î²)I + Î²eeT ,
where
e =
ï£«
ï£­
1
...
1
ï£¶
ï£¸.
If Î± = Î², then A is singular, so det (A) = 0. If Î± Ì¸= Î², then (6.2.3) may be
applied to obtain
det (A) = det

(Î± âˆ’Î²)I
 
1 + Î²eT e
Î± âˆ’Î²

= (Î± âˆ’Î²)n

1 +
nÎ²
Î± âˆ’Î²

.
(c)
Recognize that the matrix is I + edT , where
e =
ï£«
ï£¬
ï£¬
ï£­
1
1
...
1
ï£¶
ï£·
ï£·
ï£¸
and
d =
ï£«
ï£¬
ï£¬
ï£­
Î±1
Î±2
...
Î±n
ï£¶
ï£·
ï£·
ï£¸.

Solutions
121
Apply (6.2.2) to produce the desired formula.
6.2.15. (a)
Use the second formula in (6.2.1).
(b)
Apply the ï¬rst formula in (6.2.1) along with (6.2.7).
6.2.16. If Î» = 0, then the result is trivially true because both sides are zero. If Î» Ì¸= 0,
then expand

Î»Im
Î»B
C
Î»In
 both of the ways indicated in (6.2.1).
6.2.17. (a)
Use the product rule (6.1.15) together with (6.2.2) to write
A + cdT = A + AxdT = A

I + xdT 	
.
(b)
Apply the same technique used in part (a) to obtain
A + cdT = A + cyT A =

I + cyT 	
A.
6.2.18. For an elementary reï¬‚ector R = I âˆ’2uuT /uT u, (6.2.2) insures det (R) = âˆ’1.
If AnÃ—n is reduced to upper-triangular form (say PA = T ) by Householder
reduction as explained on p. 341, then det (P)det (A) = det (T) = t11 Â· Â· Â· tnn.
Since P is the product of elementary reï¬‚ectors, det (A) = (âˆ’1)kt11 Â· Â· Â· tnn,
where k is the number of reï¬‚ections used in the reduction process. In general,
one reï¬‚ection is required to annihilate entries below a diagonal position, so, if
no reduction steps can be skipped, then det (A) = (âˆ’1)nâˆ’1t11 Â· Â· Â· tnn. If Pij is
a plane rotation, then there is a permutation matrix (a product of interchange
matrices) B such that Pij = BT

Q
0
0
I

B, where Q =

c
s
âˆ’s
c

with
c2 + s2 = 1. Consequently, det (Pij) = det

BT 	 
Q
0
0
I
 det (B) = det (Q) = 1
because det (B)det

BT 	
= det (B)2 = 1 by (6.1.9). Since Givens reduction
produces PA = T, where P is a product of plane rotations and T is upper
triangular, the product rule (6.1.15) insures det (P) = 1, so det (A) = det (T) =
t11 Â· Â· Â· tnn.
6.2.19. If det (A) = Â±1, then (6.2.7) implies Aâˆ’1 = Â±adj (A) , and thus Aâˆ’1 is
an integer matrix because the cofactors are integers. Conversely, if Aâˆ’1 is an
integer matrix, then det

Aâˆ’1	
and det (A) are both integers. Since
AAâˆ’1 = I
=â‡’
det (A)det

Aâˆ’1	
= 1,
it follows that det (A) = Â±1.
6.2.20. (a)
Exercise 6.2.19 guarantees that Aâˆ’1 has integer entries if and only if
det (A) = Â±1, and (6.2.2) says that det (A) = 1 âˆ’2vT u, so Aâˆ’1 has inte-
ger entries if and only if vT u is either 0 or 1.
(b)
According to (3.9.1),
Aâˆ’1 =

I âˆ’2uvT 	âˆ’1 = I âˆ’
2uvT
2vT u âˆ’1,

122
Solutions
and thus Aâˆ’1 = A when vT u = 1.
6.2.21. For n = 2, two multiplications are required, and c(2) = 2. Assume c(k) mul-
tiplications are required to evaluate any k Ã— k determinant by cofactors. For a
k + 1 Ã— k + 1 matrix, the cofactor expansion in terms of the ith row is
det (A) = ai1ËšAi1 + Â· Â· Â· + aikËšAik + aik+1ËšAik+1.
Each ËšAij requires c(k) multiplications, so the above expansion contains
(k + 1) + (k + 1)c(k) = (k + 1) + (k + 1)k!

1 + 1
2! + 1
3! + Â· Â· Â· +
1
(k âˆ’1)!

= (k + 1)!
 1
k! +

1 + 1
2! + 1
3! + Â· Â· Â· +
1
(k âˆ’1)!

= c(k + 1)
multiplications. Remember that ex = 1+x+x2/2!+x3/3!+Â· Â· Â· , so for n = 100,
1 + 1
2! + 1
3! + Â· Â· Â· + 1
99! â‰ˆe âˆ’1,
and c(100) â‰ˆ100!(eâˆ’1). Consequently, approximately 1.6Ã—10152 seconds (i.e.,
5.1 Ã— 10144 years) are required.
6.2.22. A âˆ’Î»I is singular if and only if det (A âˆ’Î»I) = 0. The cofactor expansion in
terms of the ï¬rst row yields
det (A âˆ’Î»I) = âˆ’Î»

5 âˆ’Î»
2
âˆ’3
âˆ’Î»
 + 3

2
2
âˆ’2
âˆ’Î»
 âˆ’2

2
5 âˆ’Î»
âˆ’2
âˆ’3

= âˆ’Î»3 + 5Î»2 âˆ’8Î» + 4,
so A âˆ’Î»I is singular if and only if Î»3 âˆ’5Î»2 + 8Î» âˆ’4 = 0. According to the
hint, the integer roots of p(Î») = Î»3 âˆ’5Î»2 +8Î»âˆ’4 are a subset of {Â±4, Â±2, Â±1}.
Evaluating p(Î») at these points reveals that Î» = 2 is a root, and either ordinary
or synthetic division produces
p(Î»)
Î» âˆ’2 = Î»2 âˆ’3Î» + 2 = (Î» âˆ’2)(Î» âˆ’1).
Therefore, p(Î») = (Î» âˆ’2)2(Î» âˆ’1), so Î» = 2 and Î» = 1 are the roots of p(Î»),
and these are the values for which A âˆ’Î»I is singular.
6.2.23. The indicated substitutions produce the system
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
xâ€²
1
xâ€²
2
...
xâ€²
nâˆ’1
xâ€²
n
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
1
0
Â· Â· Â·
0
0
0
1
Â· Â· Â·
0
...
...
...
...
...
0
0
0
Â· Â· Â·
1
âˆ’pn
âˆ’pnâˆ’1
âˆ’pnâˆ’2
Â· Â· Â·
âˆ’p1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
x1
x2
...
xnâˆ’1
xn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.

Solutions
123
Each of the n vectors wi =

fi(t)
f â€²
i(t)
Â· Â· Â·
f (nâˆ’1)
i
	T for i = 1, 2, . . . , n
satisï¬es this system, so (6.2.8) may be applied to produce the desired conclusion.
6.2.24. The result is clearly true for n = 2. Assume the formula holds for n = k âˆ’1,
and prove that it must also hold for n = k. According to the cofactor expansion
in terms of the ï¬rst row, deg p(Î») = k âˆ’1, and itâ€™s clear that
p(x2) = p(x3) = Â· Â· Â· = p(xk) = 0,
so x2, x3, . . . , xk are the k âˆ’1 roots of p(Î»). Consequently,
p(Î») = Î±(Î» âˆ’x2)(Î» âˆ’x3) Â· Â· Â· (Î» âˆ’xk),
where Î± is the coeï¬ƒcient of Î»kâˆ’1. But the coeï¬ƒcient of Î»kâˆ’1 is the cofactor
associated with the (1, k) -entry, so the induction hypothesis yields
Î± = (âˆ’1)kâˆ’1

1
x2
x2
2
Â· Â· Â·
xkâˆ’2
2
1
x3
x2
3
Â· Â· Â·
xkâˆ’2
3
...
...
...
Â· Â· Â·
...
1
xk
x2
k
Â· Â· Â·
xkâˆ’2
k

kâˆ’1Ã—kâˆ’1
= (âˆ’1)kâˆ’1 9
j>iâ‰¥2
(xj âˆ’xi).
Therefore,
det (Vk) = p(x1) = (x1 âˆ’x2)(x1 âˆ’x3) Â· Â· Â· (x1 âˆ’xk)Î±
= (x1 âˆ’x2)(x1 âˆ’x3) Â· Â· Â· (x1 âˆ’xk)

(âˆ’1)kâˆ’1 9
j>iâ‰¥2
(xj âˆ’xi)
	
= (x2 âˆ’x1)(x3 âˆ’x1) Â· Â· Â· (xk âˆ’x1)
9
j>iâ‰¥2
(xj âˆ’xi)
=
9
j>i
(xj âˆ’xi),
and the formula is proven. The determinant is nonzero if and only if the xi â€™s
are distinct numbers, and this agrees with the conclusion in Example 4.3.4.
6.2.25. According to (6.1.19),
d

det (A)

dx
= det (D1) + det (D2) + Â· Â· Â· + det (Dn),
where Di is the matrix
Di =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a11
a12
Â· Â· Â·
a1n
...
...
Â· Â· Â·
...
aâ€²
i1
aâ€²
i2
Â· Â· Â·
aâ€²
in
...
...
Â· Â· Â·
...
an1
an2
Â· Â· Â·
ann
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.

124
Solutions
Expanding det (Di) in terms of cofactors of the ith row yields
det (Ai) = aâ€²
i1ËšAi1 + aâ€²
i2ËšAi2 + Â· Â· Â· + aâ€²
inËšAin,
so the desired conclusion is obtained.
6.2.26. According to (6.1.19),
âˆ‚det (A)
âˆ‚aij
= det (Di) =

a11
Â· Â· Â·
a1j
Â· Â· Â·
a1n
...
Â· Â· Â·
...
Â· Â· Â·
...
0
Â· Â· Â·
1
Â· Â· Â·
0
...
Â· Â· Â·
...
Â· Â· Â·
...
an1
Â· Â· Â·
anj
Â· Â· Â·
ann

â†row i = ËšAij.
6.2.27. The
4
2
	
= 6 ways to choose pairs of column indices are
(1, 2)
(1, 3)
(1, 4)
(2, 3)
(2, 4)
(3, 4)
so that the Laplace expansion using i1 = 1 and i2 = 3 is
det (A) = det A(1, 3 | 1, 2) ËšA(1, 3 | 1, 2) + det A(1, 3 | 1, 3) ËšA(1, 3 | 1, 3)
+ det A(1, 3 | 1, 4) ËšA(1, 3 | 1, 4) + det A(1, 3 | 2, 3) ËšA(1, 3 | 2, 3)
+ det A(1, 3 | 2, 4) ËšA(1, 3 | 2, 4) + det A(1, 3 | 3, 4) ËšA(1, 3 | 3, 4)
= 0 + (âˆ’2)(âˆ’4) + (âˆ’1)(3)(âˆ’2) + 0 + (âˆ’3)(âˆ’3) + (âˆ’1)(âˆ’8)(2)
= 39.

Solutions for Chapter 7
Solutions for exercises in section 7. 1
7.1.1. Ïƒ (A) = {âˆ’3, 4}
N (A + 3I) = span

âˆ’1
1
+
and
N (A âˆ’4I) = span

âˆ’1/2
1
+
Ïƒ (B) = {âˆ’2, 2} in which the algebraic multiplicity of Î» = âˆ’2 is two.
N (B + 2I) = span
ï£±
ï£²
ï£³
ï£«
ï£­
âˆ’4
1
0
ï£¶
ï£¸,
ï£«
ï£­
âˆ’2
0
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾and N (B âˆ’2I) = span
ï£±
ï£²
ï£³
ï£«
ï£­
âˆ’1/2
âˆ’1/2
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
Ïƒ (C) = {3} in which the algebraic multiplicity of Î» = 3 is three.
N (C âˆ’3I) = span
ï£±
ï£²
ï£³
ï£«
ï£­
1
0
0
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
Ïƒ (D) = {3} in which the algebraic multiplicity of Î» = 3 is three.
N (D âˆ’3I) = span
ï£±
ï£²
ï£³
ï£«
ï£­
2
1
0
ï£¶
ï£¸,
ï£«
ï£­
1
0
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
Ïƒ (E) = {3} in which the algebraic multiplicity of Î» = 3 is three.
N (E âˆ’3I) = span
ï£±
ï£²
ï£³
ï£«
ï£­
1
0
0
ï£¶
ï£¸,
ï£«
ï£­
0
1
0
ï£¶
ï£¸,
ï£«
ï£­
0
0
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
Matrices C and D are deï¬cient in eigenvectors.
7.1.2. Form the product Ax, and answer the question, â€œIs Ax some multiple of x ?â€
When the answer is yes, then x is an eigenvector for A, and the multiplier
is the associated eigenvalue. For this matrix, (a), (c), and (d) are eigenvectors
associated with eigenvalues 1, 3, and 3, respectively.

126
Solutions
7.1.3. The characteristic polynomial for T is
det (T âˆ’Î»I) = (t11 âˆ’Î») (t22 âˆ’Î») Â· Â· Â· (tnn âˆ’Î») ,
so the roots are the tii â€™s.
7.1.4. This follows directly from (6.1.16) because
det (T âˆ’Î»I) =

A âˆ’Î»I
B
0
C âˆ’Î»I
 = det (A âˆ’Î»I)det (C âˆ’Î»I).
7.1.5. If Î»i is not repeated, then N (A âˆ’Î»iI) = span {ei} . If the algebraic multiplic-
ity of Î»i is k, and if Î»i occupies positions i1, i2, . . . , ik in D, then
N (A âˆ’Î»iI) = span {ei1, ei2, . . . , eik} .
7.1.6. A singular â‡â‡’det (A) = 0 â‡â‡’0 solves det (A âˆ’Î»I) = 0 â‡â‡’0 âˆˆÏƒ (A) .
7.1.7. Zero is not in or on any Gerschgorin circle. You could also say that A is non-
singular because it is diagonally dominantâ€”see Example 7.1.6 on p. 499.
7.1.8. If (Î», x) is an eigenpair for Aâˆ—A, then âˆ¥Axâˆ¥2
2 / âˆ¥xâˆ¥2
2 = xâˆ—Aâˆ—Ax/xâˆ—x = Î» is
real and nonnegative. Furthermore, Î» > 0 if and only if Aâˆ—A is nonsingular or,
equivalently, n = rank (Aâˆ—A) = rank (A). Similar arguments apply to AAâˆ—.
7.1.9. (a)
Ax = Î»x
=â‡’
x = Î»Aâˆ’1x
=â‡’
(1/Î»)x = Aâˆ’1x.
(b)
Ax = Î»x â‡â‡’(A âˆ’Î±I)x = (Î» âˆ’Î±)x â‡â‡’(Î» âˆ’Î±)âˆ’1x = (A âˆ’Î±I)âˆ’1x.
7.1.10. (a)
Successively use A as a left-hand multiplier to produce
Ax = Î»x
=â‡’A2x = Î»Ax = Î»2x
=â‡’A3x = Î»2Ax = Î»3x
=â‡’A4x = Î»3Ax = Î»4x
etc.
(b)
Use part (a) to write
p(A)x =

i
Î±iAi

x =

i
Î±iAix =

i
Î±iÎ»ix =

i
Î±iÎ»i

x = p(Î»)x.
7.1.11. Since one Geschgorin circle (derived from row sums and shown below) is isolated
2
4
6
8
10
12
14
16
-2
-4
-6

Solutions
127
from the union of the other three circles, statement (7.1.14) on p. 498 insures
that there is one eigenvalue in the isolated circle and three eigenvalues in the
union of the other three. But, as discussed on p. 492, the eigenvalues of real
matrices occur in conjugate pairs. So, the root in the isolated circle must be real
and there must be at least one real root in the union of the other three circles.
Computation reveals that Ïƒ (A) = {Â±i, 2, 10}.
7.1.12. Use Exercise 7.1.10 to deduce that
Î» âˆˆÏƒ (A)
=â‡’
Î»k âˆˆÏƒ

Ak	
=â‡’
Î»k = 0
=â‡’
Î» = 0.
Therefore, (7.1.7) insures that trace (A) = 
i Î»i = 0.
7.1.13. This is true because N (A âˆ’Î»I) is a subspaceâ€”recall that subspaces are closed
under vector addition and scalar multiplication.
7.1.14. If there exists a nonzero vector x that satisï¬es Ax = Î»1x and Ax = Î»2x,
where Î»1 Ì¸= Î»2, then
0 = Ax âˆ’Ax = Î»1x âˆ’Î»2x = (Î»1 âˆ’Î»2)x.
But this implies x = 0, which is impossible. Consequently, no such x can exist.
7.1.15. Noâ€”consider A =
ï£«
ï£­
1
0
0
0
1
0
0
0
2
ï£¶
ï£¸and B =
ï£«
ï£­
1
0
0
0
2
0
0
0
2
ï£¶
ï£¸.
7.1.16. Almost any example with rather random entries will do the job, but avoid diag-
onal or triangular matricesâ€”they are too special.
7.1.17. (a)
c = (A âˆ’Î»I)âˆ’1(Aâˆ’Î»I)c = (A âˆ’Î»I)âˆ’1(Acâˆ’Î»c) = (A âˆ’Î»I)âˆ’1(Î»k âˆ’Î»)c.
(b)
Use (6.2.3) to compute the characteristic polynomial for A + cdT to be
det

A + cdT âˆ’Î»I
	
= det

A âˆ’Î»I + cdT 	
= det (A âˆ’Î»I)

1 + dT (A âˆ’Î»I)âˆ’1c
	
=

Â±
n
9
i=1
(Î»j âˆ’Î»)
 
1 +
dT c
Î»k âˆ’Î»

=
ï£«
ï£­Â±
9
jÌ¸=k
(Î»j âˆ’Î»)
ï£¶
ï£¸
Î»k + dT c âˆ’Î»
	
.
The roots of this polynomial are Î»1, . . . , Î»kâˆ’1, Î»k + dT c, Î»k+1, . . . , Î»n.
(c)
d = (Âµ âˆ’Î»k)c
cT c
will do the job.
7.1.18. (a)
The transpose does not alter the determinantâ€”recall (6.1.4)â€”so that
det (A âˆ’Î»I) = det

AT âˆ’Î»I
	
.

128
Solutions
(b)
We know from Exercise 6.1.8 that det (A) = det (Aâˆ—), so
Î» âˆˆÏƒ (A) â‡â‡’0 = det (A âˆ’Î»I)
â‡â‡’0 = det (A âˆ’Î»I) = det ((A âˆ’Î»I)âˆ—) = det

Aâˆ—âˆ’Î»I
	
â‡â‡’Î» âˆˆÏƒ (Aâˆ—) .
(c)
Yes.
(d)
Apply the reverse order law for conjugate transposes to obtain
yâˆ—A = Âµyâˆ—
=â‡’
Aâˆ—y = Âµy
=â‡’
AT y = Âµy
=â‡’
Âµ âˆˆÏƒ

AT 	
= Ïƒ (A) ,
and use the conclusion of part (c) insuring that the eigenvalues of real matrices
must occur in conjugate pairs.
7.1.19. (a)
When m = n, Exercise 6.2.16 insures that
Î»ndet (AB âˆ’Î»I) = Î»ndet (BA âˆ’Î»I)
for all Î»,
so det (AB âˆ’Î»I) = det (BA âˆ’Î»I).
(b)
If m Ì¸= n, then the characteristic polynomials of AB and BA are of
degrees m and n, respectively, so they must be diï¬€erent. When m and n are
diï¬€erentâ€”say m > n â€”Exercise 6.2.16 implies that
det (AB âˆ’Î»I) = (âˆ’Î»)mâˆ’ndet (BA âˆ’Î»I).
Consequently, AB has m âˆ’n more zero eigenvalues than BA.
7.1.20. Suppose that A and B are n Ã— n, and suppose X is n Ã— g. The equation
(A âˆ’Î»I)BX = 0 says that the columns of BX are in N (A âˆ’Î»I), and hence
they are linear combinations of the basis vectors in X. Thus
[BX]âˆ—j =

i
pijXâˆ—j
=â‡’
BX = XP,
where PgÃ—g = [pij] .
If (Âµ, z) is any eigenpair for P, then
B(Xz) = XPz = Âµ(Xz)
and
AX = Î»X
=â‡’
A(Xz) = Î»(Xz),
so Xz is a common eigenvector.
7.1.21. (a)
If Px = Î»x and yâˆ—Q = Âµyâˆ—, then T(xyâˆ—) = Pxyâˆ—Q = Î»Âµxyâˆ—.
(b)
Since dim CmÃ—n = mn, the operator T (as well as any coordinate ma-
trix representation of T ) must have exactly mn eigenvalues (counting mul-
tiplicities), and since there are exactly mn products Î»Âµ, where Î» âˆˆÏƒ (P) ,
Âµ âˆˆÏƒ (Q) , it follows that Ïƒ (T) = {Î»Âµ | Î» âˆˆÏƒ (P) , Âµ âˆˆÏƒ (Q)}. Use the fact

Solutions
129
that the trace is the sum of the eigenvalues (recall (7.1.7)) to conclude that
trace (T) = 
i,j Î»iÂµj = 
i Î»i

j Âµj = trace (P) trace (Q).
7.1.22. (a)
Use (6.2.3) to compute the characteristic polynomial for D + Î±vvT to be
p(Î») = det

D + Î±vvT âˆ’Î»I

= det

D âˆ’Î»I + Î±vvT 
= det (D âˆ’Î»I)

1 + Î±vT (D âˆ’Î»I)âˆ’1v

(â€¡)
=
ï£«
ï£­
n

j=1
(Î» âˆ’Î»j)
ï£¶
ï£¸

1 + Î±
n

i=1
v2
i
Î»i âˆ’Î»

=
n

j=1
(Î» âˆ’Î»j) + Î±
n

i=1
ï£«
ï£­vi

jÌ¸=i
(Î» âˆ’Î»j)
ï£¶
ï£¸.
For each Î»k, it is true that
p(Î»k) = Î±vk

jÌ¸=k
(Î»k âˆ’Î»j) Ì¸= 0,
and hence no Î»k can be an eigenvalue for D + Î±vvT . Consequently, if Î¾ is an
eigenvalue for D + Î±vvT , then det (D âˆ’Î¾I) Ì¸= 0, so p(Î¾) = 0 and (â€¡) imply
that
0 = 1 + Î±vT (D âˆ’Î¾I)âˆ’1v = 1 + Î±
n

i=1
v2
i
Î»i âˆ’Î¾ = f(Î¾).
(b)
Use the fact that f(Î¾i) = 1 + Î±vT (D âˆ’Î¾iI)âˆ’1v = 0 to write

D + Î±vvT 
(D âˆ’Î¾iI)âˆ’1v = D(D âˆ’Î¾iI)âˆ’1v + v

Î±vT (D âˆ’Î¾iI)âˆ’1v

= D(D âˆ’Î¾iI)âˆ’1v âˆ’v
=

D âˆ’(D âˆ’Î¾iI)

(D âˆ’Î¾iI)âˆ’1v
= Î¾i(D âˆ’Î¾iI)âˆ’1v.
7.1.23. (a)
If p(Î») = (Î» âˆ’Î»1) (Î» âˆ’Î»2) Â· Â· Â· (Î» âˆ’Î»n) , then
ln p(Î») =
n

i=1
ln (Î» âˆ’Î»i)
=â‡’
pâ€²(Î»)
p(Î») =
n

i=1
1
(Î» âˆ’Î»i).
(b)
If |Î»i/Î»| < 1, then we can write
(Î» âˆ’Î»i)âˆ’1 =

Î»

1 âˆ’Î»i
Î»
âˆ’1
= 1
Î»

1 âˆ’Î»i
Î»
âˆ’1
= 1
Î»

1 + Î»i
Î» + Î»2
i
Î»2 + Â· Â· Â·

.

130
Solutions
Consequently,
n

i=1
1
(Î» âˆ’Î»i) =
n

i=1
 1
Î» + Î»i
Î»2 + Î»2
i
Î»3 + Â· Â· Â·

= n
Î» + Ï„1
Î»2 + Ï„2
Î»3 + Â· Â· Â· .
(c) Combining these two results yields
nÎ»nâˆ’1 + (n âˆ’1)c1Î»nâˆ’2 + (n âˆ’2)c2Î»nâˆ’3 + Â· Â· Â· + cnâˆ’1
=

Î»n + c1Î»nâˆ’1 + c2Î»nâˆ’2 + Â· Â· Â· + cn
	 n
Î» + Ï„1
Î»2 + Ï„2
Î»3 + Â· Â· Â·

= nÎ»nâˆ’1 + (nc1 + Ï„1) Î»nâˆ’2 + (nc2 + Ï„1c1 + Ï„2) Î»nâˆ’3
+ Â· Â· Â· + (ncnâˆ’1 + Ï„1cnâˆ’2 + Ï„2cnâˆ’3 + Â· Â· Â· + Ï„nâˆ’1)
+ (ncn + Ï„1cnâˆ’1 + Ï„2cnâˆ’2 Â· Â· Â· + Ï„n) 1
Î» + Â· Â· Â· ,
and equating like powers of Î» produces the desired conclusion.
7.1.24. We know from Exercise 7.1.10 that Î» âˆˆÏƒ (A)
=â‡’
Î»k âˆˆÏƒ

Ak	
, so (7.1.7)
guarantees that trace

Ak	
= 
i Î»k
i = Ï„k. Proceed by induction. The result is
true for k = 1 because (7.1.7) says that c1 = âˆ’trace (A). Assume that
ci = âˆ’trace (ABiâˆ’1)
i
for
i = 1, 2, . . . , k âˆ’1,
and prove the result holds for i = k. Recursive application of the induction
hypothesis produces
B1 = c1I + A
B2 = c2I + c1A + A2
...
Bkâˆ’1 = ckâˆ’1I + ckâˆ’2A + Â· Â· Â· + c1Akâˆ’2 + Akâˆ’1,
and therefore we can use Newtonâ€™s identities given in Exercise 7.1.23 to obtain
trace (ABkâˆ’1) = trace

ckâˆ’1A + ckâˆ’2A2 + Â· Â· Â· + c1Akâˆ’1 + Ak	
= ckâˆ’1Ï„1 + ckâˆ’2Ï„2 + Â· Â· Â· + c1Ï„kâˆ’1 + Ï„k
= âˆ’kck.

Solutions
131
Solutions for exercises in section 7. 2
7.2.1. The characteristic equation is Î»2âˆ’2Î»âˆ’8 = (Î»+2)(Î»âˆ’4) = 0, so the eigenvalues
are Î»1 = âˆ’2 and Î»2 = 4. Since no eigenvalue is repeated, (7.2.6) insures A
must be diagonalizable. A similarity transformation P that diagonalizes A is
constructed from a complete set of independent eigenvectors. Compute a pair of
eigenvectors associated with Î»1 and Î»2 to be
x1 =

âˆ’1
1

, x2 =

âˆ’1
2

,
and set
P =

âˆ’1
âˆ’1
1
2

.
Now verify that
Pâˆ’1AP =

âˆ’2
âˆ’1
1
1
 
âˆ’8
âˆ’6
12
10
 
âˆ’1
âˆ’1
1
2

=

âˆ’2
0
0
4

= D.
7.2.2. (a)
The characteristic equation is Î»3 âˆ’3Î» âˆ’2 = (Î» âˆ’2)(Î» + 1)2 = 0, so the
eigenvalues are Î» = 2 and Î» = âˆ’1. By reducing A âˆ’2I and A + I to echelon
form, compute bases for N (A âˆ’2I) and N (A + I). One set of bases is
N (A âˆ’2I) = span
ï£±
ï£²
ï£³
ï£«
ï£­
âˆ’1
0
2
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
and
N (A + I) = span
ï£±
ï£²
ï£³
ï£«
ï£­
âˆ’1
1
0
ï£¶
ï£¸,
ï£«
ï£­
âˆ’1
0
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾.
Therefore,
geo multA (2)
= dim N (A âˆ’2I) = 1 = alg multA (2) ,
geo multA (âˆ’1) = dim N (A + I)
= 2 = alg multA (âˆ’1) .
In other words, Î» = 2 is a simple eigenvalue, and Î» = âˆ’1 is a semisimple
eigenvalue.
(b)
A similarity transformation P that diagonalizes A is constructed from a
complete set of independent eigenvectors, and these are obtained from the above
bases. Set P =
ï£«
ï£­
âˆ’1
âˆ’1
âˆ’1
0
1
0
2
0
1
ï£¶
ï£¸, and compute Pâˆ’1 =
ï£«
ï£­
1
1
1
0
1
0
âˆ’2
âˆ’2
âˆ’1
ï£¶
ï£¸and
verify that Pâˆ’1AP =
ï£«
ï£­
2
0
0
0
âˆ’1
0
0
0
âˆ’1
ï£¶
ï£¸.
7.2.3. Consider the matrix A of Exercise 7.2.1. We know from its solution that A
is similar to D =

âˆ’2
0
0
4

, but the two eigenspaces for A are spanned by

âˆ’1
1

and

âˆ’1
2

, whereas the eigenspaces for D are spanned by the unit
vectors e1 and e2.

132
Solutions
7.2.4. The characteristic equation of A is p(Î») = (Î»âˆ’1)(Î»âˆ’2)2, so alg multA (2) = 2.
To ï¬nd geo multA (2) , reduce A âˆ’2I to echelon form to ï¬nd that
N (A âˆ’2I) = span
ï£±
ï£²
ï£³
ï£«
ï£­
âˆ’1
0
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾,
so geo multA (2) = dim N (A âˆ’2I) = 1. Since there exists at least one eigen-
value such that geo multA (Î») < alg multA (Î») , it follows (7.2.5) on p. 512 that
A cannot be diagonalized by a similarity transformation.
7.2.5. A formal induction argument can be given, but it suï¬ƒces to â€œdo it with dotsâ€
by writing
Bk = (Pâˆ’1AP)(Pâˆ’1AP) Â· Â· Â· (Pâˆ’1AP)
= Pâˆ’1A(PPâˆ’1)A(PPâˆ’1) Â· Â· Â· (PPâˆ’1)AP = Pâˆ’1AA Â· Â· Â· AP = Pâˆ’1AkP.
7.2.6. limnâ†’âˆAn =

5
2
âˆ’10
âˆ’4

. Of course, you could compute A, A2, A3, . . . in
hopes of seeing a pattern, but this clumsy approach is not deï¬nitive. A better
technique is to diagonalize A with a similarity transformation, and then use the
result of Exercise 7.2.5. The characteristic equation is 0 = Î»2âˆ’(19/10)Î»+(1/2) =
(Î»âˆ’1)(Î»âˆ’(9/10)), so the eigenvalues are Î» = 1 and Î» = .9. By reducing Aâˆ’I
and A âˆ’.9I to echelon form, we see that
N (A âˆ’I) = span

âˆ’1
2
+
and
N (A âˆ’.9I) = span

âˆ’2
5
+
,
so A is indeed diagonalizable, and P =

âˆ’1
âˆ’2
2
5

is a matrix such that
Pâˆ’1AP =

1
0
0
.9

= D or, equivalently, A = PDPâˆ’1. The result of Exer-
cise 7.2.5 says that An = PDnPâˆ’1 = P

1
0
0
.9n

Pâˆ’1, so
lim
nâ†’âˆAn =P

1
0
0
0

Pâˆ’1 =

âˆ’1
âˆ’2
2
5
 
1
0
0
0
 
âˆ’5
âˆ’2
2
1

=

5
2
âˆ’10
âˆ’4

.
7.2.7. It follows from Pâˆ’1P = I that yâˆ—
i xj =

1
if i = j,
0
if i Ì¸= j, as well as yâˆ—
i X = 0 and
Yâˆ—xi = 0 for each i = 1, . . . , t, so
Pâˆ’1AP =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
yâˆ—
1
...
yâˆ—
t
Yâˆ—
ï£¶
ï£·
ï£·
ï£·
ï£¸A

x1 | Â· Â· Â· | xt | X

=
ï£«
ï£¬
ï£¬
ï£­
Î»1
Â· Â· Â·
0
0
...
...
...
...
0
Â· Â· Â·
Î»t
0
0
Â· Â· Â·
0
Yâˆ—AX
ï£¶
ï£·
ï£·
ï£¸= B.

Solutions
133
Therefore, examining the ï¬rst t rows on both sides of Pâˆ’1A = BPâˆ’1 yields
yâˆ—
i A = Î»iyâˆ—
i for i = 1, . . . , t.
7.2.8. If Pâˆ’1AP = diag (Î»1, Î»2, . . . , Î»n) , then Pâˆ’1AkP = diag

Î»k
1, Î»k
2, . . . , Î»k
n
	
for
k = 0, 1, 2, . . . or, equivalently, Ak = P diag

Î»k
1, Î»k
2, . . . , Î»k
n
	
Pâˆ’1. Therefore,
Ak â†’0 if and only if each Î»k
i â†’0, which is equivalent to saying that |Î»i| < 1
for each i. Since Ï(A) = maxÎ»iâˆˆÏƒ(A) |Î»i| (recall Example 7.1.4 on p. 497), it
follows that Ak â†’0 if and only if Ï(A) < 1.
7.2.9. The characteristic equation for A is Î»2 âˆ’2Î» + 1, so Î» = 1 is the only distinct
eigenvalue. By reducing A âˆ’I to echelon form, we see that

3
4

is a basis for
N (A âˆ’I), so x = (1/5)

3
4

is an eigenvector of unit length. Following the
procedure on p. 325, we ï¬nd that R =

3/5
4/5
4/5
âˆ’3/5

is an elementary reï¬‚ector
having x as its ï¬rst column, and RT AR = RAR =

1
25
0
1

.
7.2.10. From Example 7.2.1 on p. 507 we see that the characteristic equation for A is
p(Î») = Î»3 + 5Î»2 + 3Î» âˆ’9 = (Î» âˆ’1)(Î» + 3)2 = 0. Straightforward computation
shows that
p(A) = (A âˆ’I)(A + 3I)2 =
ï£«
ï£­
0
âˆ’4
âˆ’4
8
âˆ’12
âˆ’8
âˆ’8
8
4
ï£¶
ï£¸
ï£«
ï£­
16
âˆ’16
âˆ’16
32
âˆ’32
âˆ’32
âˆ’32
32
32
ï£¶
ï£¸= 0.
7.2.11. Rescale the observed eigenvector as x = (1/2)(1, 1, 1, 1)T = y so that xT x = 1.
Follow the procedure described in Example 5.6.3 (p. 325), and set u = x âˆ’e1
to construct
R = I âˆ’2uuT
uT u = 1
2
ï£«
ï£¬
ï£­
1
1
1
1
1
1
âˆ’1
âˆ’1
1
âˆ’1
1
âˆ’1
1
âˆ’1
âˆ’1
1
ï£¶
ï£·
ï£¸= P =

x | X

(since x = y ).
Consequently, B = XT AX =
ï£«
ï£­
âˆ’1
0
âˆ’1
0
2
0
âˆ’1
0
1
ï£¶
ï£¸, and Ïƒ (B) = {2,
âˆš
2, âˆ’
âˆš
2}.
7.2.12. Use the spectral theorem with properties GiGj = 0 for i Ì¸= j and G2
i = Gi
to write AGi = (Î»1G1 + Î»2G2 + Â· Â· Â· + Î»kGk)Gi = Î»iG2
i = Î»iGi. A similar
argument shows GiA = Î»iGi.
7.2.13. Use (6.2.3) to show that Î»nâˆ’1(Î»âˆ’dT c) = 0 is the characteristic equation for A.
Thus Î» = 0 and Î» = dT c are the eigenvalues of A. We know from (7.2.5) that
A is diagonalizable if and only if the algebraic and geometric multiplicities agree
for each eigenvalue. Since geo multA (0) = dim N (A) = n âˆ’rank (A) = n âˆ’1,
and since
alg multA (0) =

n âˆ’1
if
dT c Ì¸= 0,
n
if
dT c = 0,

134
Solutions
it follows that A is diagonalizable if and only if dT c Ì¸= 0.
7.2.14. If W and Z are diagonalizableâ€”say Pâˆ’1WP and Qâˆ’1ZQ are diagonalâ€”
then

P
0
0
Q

diagonalizes A. Use an indirect argument for the converse.
Suppose A is diagonalizable but W (or Z ) is not. Then there is an eigenvalue
Î» âˆˆÏƒ (W) with geo multW (Î») < alg multW (Î») . Since Ïƒ (A) = Ïƒ (W) âˆªÏƒ (Z)
(Exercise 7.1.4), this would mean that
geo multA (Î») = dim N (A âˆ’Î»I) = (s + t) âˆ’rank (A âˆ’Î»I)
= (s âˆ’rank (W âˆ’Î»I)) + (t âˆ’rank (Z âˆ’Î»I))
= dim N (W âˆ’Î»I) + dim N (Z âˆ’Î»I)
= geo multW (Î») + geo multZ (Î»)
< alg multW (Î») + alg multZ (Î»)
< alg multA (Î») ,
which contradicts the fact that A is diagonalizable.
7.2.15. If AB = BA, then, by Exercise 7.1.20 (p. 503), A and B have a common
eigenvectorâ€”say Ax = Î»x and Bx = Âµx, where x has been scaled so that
âˆ¥xâˆ¥2 = 1. If R =

x | X

is a unitary matrix having x as its ï¬rst column
(Example 5.6.3, p. 325), then
Râˆ—AR =

Î»
xâˆ—AX
0
Xâˆ—AX

and
Râˆ—BR =

Âµ
xâˆ—BX
0
Xâˆ—BX

.
Since A and B commute, so do Râˆ—AR and Râˆ—BR, which in turn implies
A2 = Xâˆ—AX and B2 = Xâˆ—BX commute. Thus the problem is deï¬‚ated, so the
same argument can be applied inductively in a manner similar to the development
of Schurâ€™s triangularization theorem (p. 508).
7.2.16. If Pâˆ’1AP = D1 and Pâˆ’1BP = D2 are both diagonal, then D1D2 = D2D1
implies that AB = BA. Conversely, suppose AB = BA. Let Î» âˆˆÏƒ (A) with
alg multA (Î») = a, and let P be such that Pâˆ’1AP =

Î»Ia
0
0
D

, where D
is a diagonal matrix with Î» Ì¸âˆˆÏƒ (D) . Since A and B commute, so do Pâˆ’1AP
and Pâˆ’1BP. Consequently, if Pâˆ’1BP =

W
X
Y
Z

, then

Î»Ia
0
0
D
 
W
X
Y
Z

=

W
X
Y
Z
 
Î»Ia
0
0
D

=â‡’

Î»X = XD,
DY = Î»Y,
so (Dâˆ’Î»I)X = 0 and (Dâˆ’Î»I)Y = 0. But (Dâˆ’Î»I) is nonsingular, so X = 0
and Y = 0, and thus Pâˆ’1BP =

W
0
0
Z

. Since B is diagonalizable, so is

Solutions
135
Pâˆ’1BP, and hence so are W and Z (Exercise 7.2.14). If Q =

Qw
0
0
Qz

,
where Qw and Qz are such that Qâˆ’1
w WQw = Dw and Qâˆ’1
z ZQz = Dz are
each diagonal, then
(PQ)âˆ’1A(PQ) =

Î»Ia
0
0
Qâˆ’1
z DQz

and
(PQ)âˆ’1B(PQ) =

Dw
0
0
Dz

.
Thus the problem is deï¬‚ated because A2 = Qâˆ’1
z DQz and B2 = Dz commute
and are diagonalizable, so the same argument can be applied to them. If A has k
distinct eigenvalues, then the desired conclusion is attained after k repetitions.
7.2.17. Itâ€™s not legitimate to equate p(A) with det (A âˆ’AI) because the former is a
matrix while the latter is a scalar.
7.2.18. This follows from the eigenvalue formula developed in Example 7.2.5 (p. 514) by
using the identity 1 âˆ’cos Î¸ = 2 sin2(Î¸/2).
7.2.19. (a)
The result in Example 7.2.5 (p. 514) shows that the eigenvalues of N+NT
and Nâˆ’NT are Î»j = 2 cos (jÏ€/n + 1) and Î»j = 2i cos (jÏ€/n + 1) , respectively.
(b)
Since N âˆ’NT is skew symmetric, it follows from Exercise 6.1.12 (p. 473)
that Nâˆ’NT is nonsingular if and only if n is even, which is equivalent to saying
N âˆ’NT has no zero eigenvalues (recall Exercise 7.1.6, p. 501), and hence, by
part (a), the same is true for N + NT .
(b: Alternate)
Since the eigenvalues of N+NT are Î»j = 2 cos (jÏ€/n + 1) you
can argue that N+NT has a zero eigenvalue (and hence is singular) if and only
if n is odd by showing that there exists an integer Î± such that jÏ€/n+1 = Î±Ï€/2
for some 1 â‰¤j â‰¤n if and only if n is odd.
(c)
Since a determinant is the product of eigenvalues (recall (7.1.8), p. 494),
det

N âˆ’NT 	
/det

N + NT 	
= (iÎ»1 Â· Â· Â· iÎ»n)/(Î»1 Â· Â· Â· Î»n) = in = (âˆ’1)n/2.
7.2.20. The eigenvalues are {2, 0, 2, 0}. The columns of F4 =
ï£«
ï£¬
ï£­
1
1
1
1
1
âˆ’i
âˆ’1
i
1
âˆ’1
1
âˆ’1
1
i
âˆ’1
âˆ’i
ï£¶
ï£·
ï£¸are
corresponding eigenvectors.
7.2.21. Ax = Î»x
=â‡’
yâˆ—Ax = Î»yâˆ—x and yâˆ—A = Âµyâˆ—
=â‡’
yâˆ—Ax = Âµyâˆ—x.
Therefore, Î»yâˆ—x = Âµyâˆ—x
=â‡’
(Î» âˆ’Âµ)yâˆ—x = 0
=â‡’
yâˆ—x = 0 when Î» Ì¸= Âµ.
7.2.22. (a)
Suppose P is a nonsingular matrix such that Pâˆ’1AP = D is diagonal,
and suppose that Î» is the kth diagonal entry in D. If x and yâˆ—are the kth
column and kth row in P and Pâˆ’1, respectively, then x and yâˆ—must be
right-hand and left-hand eigenvectors associated with Î» such that yâˆ—x = 1.
(b)
Consider A = I with x = ei and y = ej for i Ì¸= j.
(c)
Consider A =

0
1
0
0

.
7.2.23. (a)
Suppose notâ€”i.e., suppose yâˆ—x = 0. Then
x âŠ¥span (y) = N (A âˆ’Î»I)âˆ—
=â‡’
x âˆˆN (A âˆ’Î»I)âˆ—âŠ¥= R (A âˆ’Î»I).

136
Solutions
Also, x âˆˆN (A âˆ’Î»I), so x âˆˆR (A âˆ’Î»I)âˆ©N (A âˆ’Î»I). However, because Î» is
a simple eigenvalue, the the core-nilpotent decomposition on p. 397 insures that
A âˆ’Î»I is similar to a matrix of the form

C
0
0
01Ã—1

, and this implies that
R (A âˆ’Î»I)âˆ©N (A âˆ’Î»I) = 0 (Exercise 5.10.12, p. 402), which is a contradiction.
Thus yâˆ—x Ì¸= 0.
(b)
Consider A = I with x = ei and y = ej for i Ì¸= j.
7.2.24. Let Bi be a basis for N (A âˆ’Î»iI), and suppose A is diagonalizable. Since
geo multA (Î»i) = alg multA (Î»i) for each i, (7.2.4) implies B = B1 âˆªB2 âˆªÂ· Â· Â·âˆªBk
is a set of n independent vectorsâ€”i.e., B is a basis for â„œn. Exercise 5.9.14
now guarantees that â„œn = N (A âˆ’Î»1I) âŠ•N (A âˆ’Î»2I) âŠ•Â· Â· Â· âŠ•N (A âˆ’Î»kI).
Conversely, if this equation holds, then Exercise 5.9.14 says B = B1âˆªB2âˆªÂ· Â· Â·âˆªBk
is a basis for â„œn, and hence A is diagonalizable because B is a complete
independent set of eigenvectors.
7.2.25. Proceed inductively just as in the development of Schurâ€™s triangularization the-
orem. If the ï¬rst eigenvalue Î» is real, the reduction is exactly the same as
described on p. 508 (with everything being real). If Î» is complex, then (Î», x)
and (Î», x) are both eigenpairs for A, and, by (7.2.3), {x, x} is linearly indepen-
dent. Consequently, if x = u + iv, with u, v âˆˆâ„œnÃ—1, then {u, v} is linearly
independentâ€”otherwise, u = Î¾v implies x = (1 + iÎ¾)u and x = (1 âˆ’iÎ¾)u,
which is impossible. Let Î» = Î± + iÎ²,
Î±, Î² âˆˆâ„œ, and observe that Ax = Î»x
implies Au = Î±u âˆ’Î²v and Av = Î²u + Î±v, so AW = W

Î±
Î²
âˆ’Î²
Î±

, where
W =

u | v

. Let W = QnÃ—2R2Ã—2 be a rectangular QR factorization (p. 311),
and let B = R

Î±
Î²
âˆ’Î²
Î±

Râˆ’1 so that Ïƒ (B) = Ïƒ

Î±
Î²
âˆ’Î²
Î±

= {Î», Î»}, and
AW = AQR = QR

Î±
Î²
âˆ’Î²
Î±

=â‡’
QT AQ = R

Î±
Î²
âˆ’Î²
Î±

Râˆ’1 = B.
If XnÃ—nâˆ’2 is chosen so that P =

Q | X

is an orthogonal matrix (i.e., the
columns of X complete the two columns of Q to an orthonormal basis for
â„œn ), then XT AQ = XT QB = 0, and
PT AP =

QT AQ
QT AX
XT AQ
XT AX

=

B
QT AX
0
XT AX

.
Now repeat the argument on the n âˆ’2 Ã— n âˆ’2 matrix XT AX. Continuing in
this manner produces the desired conclusion.
7.2.26. Let the columns RnÃ—r be linearly independent eigenvectors corresponding to
the real eigenvalues Ïj, and let {x1, x1, x2, x2, . . . , xt, xt} be a set of linearly
independent eigenvectors associated with {Î»1, Î»1, Î»2, Î»2, . . . , Î»t, Î»t} so that the
matrix Q =

R | x1 | x1 | Â· Â· Â· | xt | xt

is nonsingular. Write xj = uj + ivj for

Solutions
137
uj, vj âˆˆâ„œnÃ—1 and Î»j = Î±j + iÎ²j for Î±, Î² âˆˆâ„œ, and let P be the real matrix
P =

R | u1 | v1 | u2 | v2 | Â· Â· Â· | ut | vt

. This matrix is nonsingular because Exer-
cise 6.1.14 can be used to show that det (P) = 2t(âˆ’i)t det (Q). For example, if
t = 1, then P =

R | u1 | v1

and
det (Q) = det

R | x1 | x1] = det

R | u1 + iv1 | u1 âˆ’iv1

= det

R | u1 | u1

+ det

R | u1 | âˆ’iv1

+ det

R | iv1 | u1

+ det

R | iv1 | iv1

= âˆ’i det

R | u1 |v1

+ i det

R | v1 | u1

= âˆ’i det

R | u1 |v1

âˆ’i det

R | u1 |v1

= 2(âˆ’i) det (P).
Induction can now be used. The equations A(uj + ivj) = (Î±j + iÎ²j)(uj + ivj)
yield Auj = Î±juj âˆ’Î²jvj and Avj = Î²juj + Î±jvj. Couple these with the fact
that AR = RD to conclude that
AP =

RD | Â· Â· Â· | Î±juj âˆ’Î²jvj | Î²juj + Î±jvj | Â· Â· Â·

= P
ï£«
ï£¬
ï£¬
ï£­
D
0
Â· Â· Â·
0
0
B1
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
Bt
ï£¶
ï£·
ï£·
ï£¸,
where
D =
ï£«
ï£¬
ï£¬
ï£­
Ï1
0
Â· Â· Â·
0
0
Ï2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
Ïr
ï£¶
ï£·
ï£·
ï£¸
and
Bj =

Î±j
Î²j
âˆ’Î²j
Î±j

.
7.2.27. Schurâ€™s triangularization theorem says Uâˆ—AU = T where U is unitary and T
is upper triangular. Setting x = Uei in xâˆ—Ax = 0 yields that tii = 0 for each
i, so tij = 0 for all i â‰¥j. Now set x = U(ei +ej) with i < j in xâˆ—Ax = 0 to
conclude that tij = 0 whenever i < j. Consequently, T = 0, and thus A = 0.
To see that xT Ax = 0 âˆ€x âˆˆâ„œnÃ—1 Ì¸â‡’A = 0, consider A =
 0
âˆ’1
1
0

.
Solutions for exercises in section 7. 3
7.3.1. cos A =

0
1
1
0

. The characteristic equation for A is Î»2 + Ï€Î» = 0, so the
eigenvalues of A are Î»1 = 0 and Î»2 = âˆ’Ï€. Note that A is diagonalizable
because no eigenvalue is repeated. Associated eigenvectors are computed in the
usual way to be
x1 =

1
1

and
x2 =

âˆ’1
1

,
so
P =

1
âˆ’1
1
1

and
Pâˆ’1 = 1
2

1
1
âˆ’1
1

.

138
Solutions
Thus
cos A = P

cos (0)
0
0
cos (âˆ’Ï€)

Pâˆ’1 = 1
2

1
âˆ’1
1
1
 
1
0
0
âˆ’1
 
1
1
âˆ’1
1

=

0
1
1
0

.
7.3.2. From Example 7.3.3, the eigenvalues are Î»1 = 0 and Î»2 = âˆ’(Î± + Î²), and
associated eigenvectors are computed in the usual way to be
x1 =

Î²/Î±
1

and
x2 =

âˆ’1
1

,
so
P =

Î²/Î±
âˆ’1
1
1

and
Pâˆ’1 =
1
1 + Î²/Î±

1
1
âˆ’1
Î²/Î±

.
Thus
P

eÎ»1t
0
0
eÎ»2t

Pâˆ’1 =
Î±
Î± + Î²

Î²/Î±
âˆ’1
1
1
 
1
0
0
eâˆ’(Î±+Î²)t
 
1
1
âˆ’1
Î²/Î±

=
1
Î± + Î²

Î²
Î²
Î±
Î±

+ eâˆ’(Î±+Î²)t

Î±
âˆ’Î²
âˆ’Î±
Î²

= eÎ»1tG1 + eÎ»2tG2.
7.3.3. Solution 1: If A = PDPâˆ’1, where D = diag (Î»1, Î»2, . . . , Î»n) , then
sin2 A = P

sin2 D
	
Pâˆ’1 = P
ï£«
ï£¬
ï£¬
ï£­
sin2 Î»1
0
Â· Â· Â·
0
0
sin2 Î»2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
sin2 Î»n
ï£¶
ï£·
ï£·
ï£¸Pâˆ’1.
Similarly for cos2A, so sin2A + cos2A = P

sin2D + cos2D
	
Pâˆ’1=PIPâˆ’1 = I.
Solution 2: If Ïƒ (A) = {Î»1, Î»2, . . . , Î»k} , use the spectral representation (7.3.6)
to write sin2 A = 
k
i=1(sin2 Î»i)Gi and cos2 A = 
k
i=1(cos2 Î»i)Gi, so that
sin2 A + cos2 A = 
k
i=1(sin2 Î»i + cos2 Î»i)Gi = 
k
i=1 Gi = I.
7.3.4. The inï¬nite series representation of eA readily yields this.
7.3.5. (a)
Eigenvalues are invariant under a similarity transformation, so the eigen-
values of f(A) = Pf(D)Pâˆ’1 are the eigenvalues of f(D), which are given by
{f(Î»1), f(Î»2), . . . , f(Î»n)}.
(b)
If (Î», x) is an eigenpair for A, then (A âˆ’z0I)nx = (Î» âˆ’z0)nx implies
that (f(Î»), x) is an eigenpair for f(A).

Solutions
139
7.3.6. If {Î»1, Î»2, . . . , Î»n} are the eigenvalues of AnÃ—n, then {eÎ»1, eÎ»2, . . . , eÎ»n} are the
eigenvalues of eA by the spectral mapping property from Exercise 7.3.5. The
trace is the sum of the eigenvalues, and the determinant is the product of the
eigenvalues (p. 494), so det

eA	
= eÎ»1eÎ»2 Â· Â· Â· eÎ»n = eÎ»1+Î»2+Â·Â·Â·+Î»n = etrace(A).
7.3.7. The Cayleyâ€“Hamilton theorem says that each AmÃ—m satisï¬es its own charac-
teristic equation, 0 = det (A âˆ’Î»I) = Î»m +c1Î»mâˆ’1 +c2Î»mâˆ’2 +Â· Â· Â·+cmâˆ’1Î»+cm,
so Am = âˆ’c1Amâˆ’1 âˆ’Â· Â· Â· âˆ’cmâˆ’1A âˆ’cmI. Consequently, Am and every higher
power of A is a polynomial in A of degree at most mâˆ’1, and thus any expres-
sion involving powers of A can always be reduced to an expression involving at
most I, A, . . . , Amâˆ’1.
7.3.8. When A is diagonalizable, (7.3.11) insures f(A) = p(A) is a polynomial in
A, and Ap(A) = p(A)A. If f(A) is deï¬ned by the series (7.3.7) in the non-
diagonalizable case, then, by Exercise 7.3.7, itâ€™s still true that f(A) = p(A) is
a polynomial in A, and thus Af(A) = f(A)A holds in the nondiagonalizable
case also.
7.3.9. If A and B are diagonalizable with AB = BA, Exercise 7.2.16 insures A and
B can be simultaneously diagonalized. If Pâˆ’1AP = DA = diag (Î»1, Î»2, . . . , Î»n)
and Pâˆ’1BP=DB =diag (Âµ1, Âµ2, . . . , Âµn) , then A + B = P(DA + DB)Pâˆ’1, so
eA+B = P

eDA+DB	
Pâˆ’1 = P
ï£«
ï£¬
ï£¬
ï£­
eÎ»1+Âµ1
0
Â· Â· Â·
0
0
eÎ»2+Âµ2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
eÎ»n+Âµn
ï£¶
ï£·
ï£·
ï£¸Pâˆ’1
= P
ï£«
ï£¬
ï£¬
ï£­
eÎ»1
0
Â· Â· Â·
0
0
eÎ»2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
eÎ»n
ï£¶
ï£·
ï£·
ï£¸Pâˆ’1P
ï£«
ï£¬
ï£¬
ï£­
eÂµ1
0
Â· Â· Â·
0
0
eÂµ2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
eÂµn
ï£¶
ï£·
ï£·
ï£¸Pâˆ’1
= eAeB.
In general, the same brute force multiplication of scalar series that yields
ex+y =
âˆ

n=0
(x + y)n
n!
=
 âˆ

n=0
xn
n!
  âˆ

n=0
yn
n!

= exey
holds for matrix series when AB = BA, but this is quite messy. A more elegant
approach is to set F(t) = eAt+Bt âˆ’eAteBt and note that Fâ€²(t) = 0 for all t
when AB = BA, so F(t) must be a constant matrix for all t. Since F(0) = 0,
it follows that e(A+B)t = eAteBt for all t. To see that eA+B, eAeB, and eBeA
can be diï¬€erent when AB Ì¸= BA, consider A =

1
0
0
0

and B =

0
1
1
0

.
7.3.10. The inï¬nite series representation of eA shows that if A is skew symmetric,
then

eA	T = eAT = eâˆ’A, and hence eA 
eA	T = eAâˆ’A = e0 = I.

140
Solutions
7.3.11. (a)
Draw a transition diagram similar to that in Figure 7.3.1 with North and
South replaced by ON and OFF, respectively. Let xk be the fraction of switches
in the ON state and let yk be the fraction of switches in the OFF state after k
clock cycles have elapsed. According to the given information,
xk = xkâˆ’1(.1) + ykâˆ’1(.3)
yk = xkâˆ’1(.9) + ykâˆ’1(.7)
so that pT
k+1 = pT
k T, where pT
k = ( xk
yk ) and T =

.1
.9
.3
.7

. Compute
Ïƒ(T) = {1, âˆ’1/5}, and use the methods of Example 7.3.4 to determine the
steady-state (or limiting) distribution as
pT
âˆ= lim
kâ†’âˆpT
k = lim
kâ†’âˆpT
0 Tk = pT
0 lim
kâ†’âˆTk = ( x0
y0 )

1/4
3/4
1/4
3/4

=
 x0 + y0
4
3(x0 + y0)
4

= ( 1/4
3/4 ) .
Alternately, (7.3.15) can be used with x1 =

1
1

and y1 = ( 1
3 ) to obtain
pT
âˆ= pT
0 lim
kâ†’âˆTk = pT
0 lim
kâ†’âˆG1 = (pT
0 x1)yT
1
yT
1 x1
=
yT
1
yT
1 x1
= ( 1/4
3/4 ) .
(b)
Computing a few powers of T reveals that
T2 =

.280
.720
.240
.760

,
T3 =

.244
.756
.252
.748

,
T4 =

.251
.749
.250
.750

,
T5 =

.250
.750
.250
.750

,
so, for practical purposes, the device can be considered to be in equilibrium after
about 5 clock cycles, regardless of the initial conï¬guration.
7.3.12. Let Ïƒ (A) = {Î»1, Î»2, . . . , Î»k} with |Î»1| â‰¥|Î»2| â‰¥Â· Â· Â· â‰¥|Î»k|, and assume Î»1 Ì¸= 0;
otherwise A = 0 and there is nothing to prove. Set
Î½n = âˆ¥Anâˆ¥
|Î»n
1| = âˆ¥Î»n
1G1 + Î»n
2G2 + Â· Â· Â· + Î»n
kGkâˆ¥
|Î»n
1|
=
,,,,
Î»n
1G1 + Î»n
2G2 + Â· Â· Â· + Î»n
kGk
Î»n
1
,,,,
=
,,,,G1 +
Î»2
Î»1
n
G2 + Â· Â· Â· +
Î»k
Î»1
n
Gk
,,,,
and let
Î½ =
k

i=1
âˆ¥Giâˆ¥.

Solutions
141
Observe that 1 â‰¤Î½n â‰¤Î½ for every positive integer n â€”the ï¬rst inequality
follows because Î»n
1 âˆˆÏƒ (An) implies |Î»n
1| â‰¤âˆ¥Anâˆ¥by (7.1.12) on p. 497, and
the second is the result of the triangle inequality. Consequently,
11/n â‰¤Î½1/n
n
â‰¤Î½1/n =â‡’1 â‰¤lim
nâ†’âˆÎ½1/n
n
â‰¤1 =â‡’1 = lim
nâ†’âˆÎ½1/n
n
= lim
nâ†’âˆ
âˆ¥Anâˆ¥1/n
|Î»1|
.
7.3.13. The dominant eigenvalue is Î»1 = 4, and all corresponding eigenvectors are
multiples of (âˆ’1, 0, 1)T .
7.3.15. Consider
xn =

1 âˆ’1/n
âˆ’1

â†’x =

1
âˆ’1

,
but m(xn) = âˆ’1 for all n = 1, 2, . . . , and m(x) = 1, so m(xn) Ì¸â†’m(x).
Nevertheless, if limnâ†’âˆxn Ì¸= 0, then limnâ†’âˆm(xn) Ì¸= 0 because the function
Ëœm(v) = |m(v)| = âˆ¥vâˆ¥âˆis continuous.
7.3.16. (a)
The â€œvanillaâ€ QR iteration fails to converge.
(b)
H âˆ’I = QR =

0
0
1
âˆ’1
0
0
0
1
0
  1
3
1
0
2
0
0
0
0

and RQ + I =
 âˆ’2
1
1
âˆ’2
1
0
0
0
1

.
Solutions for exercises in section 7. 4
7.4.1. The unique solution to uâ€² = Au, u(0) = c, is
u = eAtc = P
ï£«
ï£¬
ï£¬
ï£­
eÎ»1t
0
Â· Â· Â·
0
0
eÎ»2t
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
eÎ»nt
ï£¶
ï£·
ï£·
ï£¸Pâˆ’1c
= [x1 | x2 | Â· Â· Â· | xn]
ï£«
ï£¬
ï£¬
ï£­
eÎ»1t
0
Â· Â· Â·
0
0
eÎ»2t
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
eÎ»nt
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
Î¾1
Î¾2
...
Î¾n
ï£¶
ï£·
ï£·
ï£¸
= Î¾1eÎ»1tx1 + Î¾2eÎ»2tx2 + Â· Â· Â· + Î¾neÎ»ntxn.
7.4.2. (a)
All eigenvalues in Ïƒ (A) = {âˆ’1, âˆ’3} are negative, so the system is stable.
(b)
All eigenvalues in Ïƒ (A) = {1, 3} are positive, so the system is unstable.
(c)
Ïƒ (A) = {Â±i}, so the system is semistable. If c Ì¸= 0, then the components
in u(t) will oscillate indeï¬nitely.
7.4.3. (a)
If uk(t) denotes the number in population k at time t, then
uâ€²
1 = 2u1 âˆ’u2,
uâ€²
2 = âˆ’u1 + 2u2,
u1(0) = 100,
u2(0) = 200,

142
Solutions
or uâ€² = Au,
u(0) = c, where A =

2
âˆ’1
âˆ’1
2

and c =

100
200

. The
characteristic equation for A is p(Î») = Î»2 âˆ’4Î» + 3 = (Î» âˆ’1)(Î» âˆ’3) = 0, so
the eigenvalues for A are Î»1 = 1 and Î»2 = 3. We know from (7.4.7) that
u(t) = eÎ»1tv1 + eÎ»2tv2
(where vi = Gic )
is the solution to uâ€² = Au, u(0) = c. The spectral theorem on p. 517 implies
A âˆ’Î»2I = (Î»1 âˆ’Î»2)G1 and I = G1 + G2, so (A âˆ’Î»2I)c = (Î»1 âˆ’Î»2)v1 and
c = v1 + v2, and consequently
v1 = (A âˆ’Î»2I)c
(Î»1 âˆ’Î»2) =

150
150

and
v2 = c âˆ’v1 =

âˆ’50
50

,
so
u1(t) = 150et âˆ’50e3t
and
u2(t) = 150et + 50e3t.
(b)
As t â†’âˆ,
u1(t) â†’âˆ’âˆand u2(t) â†’+âˆ. But a population canâ€™t
become negative, so species I is destined to become extinct, and this occurs at
the value of t for which u1(t) = 0 â€”i.e., when
et 
e2t âˆ’3
	
= 0
=â‡’
e2t = 3
=â‡’
t = ln 3
2 .
7.4.4. If uk(t) denotes the number in population k at time t, then the hypothesis
says
uâ€²
1 = âˆ’u1 + u2,
uâ€²
2 = u1 âˆ’2u2,
u1(0) = 200,
u2(0) = 400,
or uâ€² = Au,
u(0) = c, where A =

âˆ’1
1
1
âˆ’1

and c =

200
400

. The
characteristic equation for A is p(Î») = Î»2+2Î» = Î»(Î»+2) = 0, so the eigenvalues
for A are Î»1 = 0 and Î»2 = âˆ’2. We know from (7.4.7) that
u(t) = eÎ»1tv1 + eÎ»2tv2
(where vi = Gic )
is the solution to uâ€² = Au, u(0) = c. The spectral theorem on p. 517 implies
A âˆ’Î»2I = (Î»1 âˆ’Î»2)G1 and I = G1 + G2, so (A âˆ’Î»2I)c = (Î»1 âˆ’Î»2)v1 and
c = v1 + v2, and consequently
v1 = (A âˆ’Î»2I)c
(Î»1 âˆ’Î»2) =

300
300

and
v2 = c âˆ’v1 =

âˆ’100
100

,
so
u1(t) = 300 âˆ’100eâˆ’2t
and
u2(t) = 300 + 100eâˆ’2t.
As t â†’âˆ, u1(t) â†’300 and u2(t) â†’300, so both populations will stabilize
at 300.

Solutions
143
Solutions for exercises in section 7. 5
7.5.1. Yes, because Aâˆ—A = AAâˆ—=

30
6 âˆ’6 i
6 + 6 i
24

.
7.5.2. Real skew-symmetric and orthogonal matrices are examples.
7.5.3. We already know from (7.5.3) that real-symmetric matrices are normal and have
real eigenvalues, so only the converse needs to be proven. If A is real and
normal with real eigenvalues, then there is a complete orthonormal set of real
eigenvectors, so using them as columns in P âˆˆâ„œnÃ—n results in an orthogonal
matrix such that PT AP = D is diagonal or, equivalently, A = PDPT , and
thus A = AT .
7.5.4. If (Î», x) is an eigenpair for A = âˆ’Aâˆ—then xâˆ—x Ì¸= 0, and Î»x = Ax implies
Î»xâˆ—= xâˆ—Aâˆ—, so
xâˆ—x(Î» + Î») = xâˆ—(Î» + Î»)x = xâˆ—Ax + xâˆ—Aâˆ—x = 0 =â‡’Î» = âˆ’Î» =â‡’â„œe(Î») = 0.
7.5.5. If A is skew hermitian (real skew symmetric), then A is normal, and hence
A is unitarily (orthogonally) similar to a diagonal matrixâ€”say A = UDUâˆ—.
Moreover, the eigenvalues Î»j in D = diag (Î»1, Î»2, . . . , Î»n) are pure imaginary
numbers (Exercise 7.5.4). Since f(z) = (1 âˆ’z)(1 + z)âˆ’1 maps the imaginary
axis in the complex plane to points on the unit circle, each f(Î»j) is on the unit
circle, so there is some Î¸j such that f(Î»j) = eiÎ¸j = cos Î¸j+i sin Î¸j. Consequently,
f(A) = U
ï£«
ï£¬
ï£¬
ï£­
f(Î»1)
0
Â· Â· Â·
0
0
f(Î»2)
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
f(Î»n)
ï£¶
ï£·
ï£·
ï£¸Uâˆ—= U
ï£«
ï£¬
ï£¬
ï£­
eiÎ¸1
0
Â· Â· Â·
0
0
eiÎ¸2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
eiÎ¸n
ï£¶
ï£·
ï£·
ï£¸Uâˆ—
together with eiÎ¸jeiÎ¸j = eiÎ¸jeâˆ’iÎ¸j = 1 yields f(A)âˆ—f(A) = I. Note: The fact
that (I âˆ’A)(I + A)âˆ’1 = (I + A)âˆ’1(I âˆ’A) follows from Exercise 7.3.8. See the
solution to Exercise 5.6.6 for an alternate approach.
7.5.6. Consider the identity matrixâ€”every nonzero vector is an eigenvector, so not ev-
ery complete independent set of eigenvectors needs to be orthonormal. Given
a complete independent set of eigenvectors for a normal A with Ïƒ (A) =
{Î»1, Î»2, . . . , Î»k} , use the Gramâ€“Schmidt procedure to form an orthonormal basis
for N (A âˆ’Î»iI) for each i. Since N (A âˆ’Î»iI) âŠ¥N (A âˆ’Î»jI) for Î»i Ì¸= Î»j (by
(7.5.2)), the union of these orthonormal bases will be a complete orthonormal
set of eigenvectors for A.
7.5.7. Consider A =
ï£«
ï£­
0
1
0
0
0
0
0
0
1
ï£¶
ï£¸.
7.5.8. Suppose TnÃ—n is an upper-triangular matrix such that Tâˆ—T = TTâˆ—. The (1,1)-
entry of Tâˆ—T is |t11|2, and the (1,1)-entry of TTâˆ—is 
n
k=1 |t1k|2. Equating

144
Solutions
these implies t12 = t13 = Â· Â· Â· = t1n = 0. Now use this and compare the (2,2)-
entries to get t23 = t24 = Â· Â· Â· = t2n = 0. Repeating this argument for each row
produces the conclusion that T must be diagonal. Conversely, if T is diagonal,
then T is normal because Tâˆ—T = diag (|t11|2 Â· Â· Â· |tnn|2) = TTâˆ—.
7.5.9. Schurâ€™s triangularization theorem on p. 508 says every square matrix is unitarily
similar to an upper-triangular matrixâ€”say Uâˆ—AU = T. If A is normal, then
so is T. Exercise 7.5.8 therefore insures that T must be diagonal. Conversely,
if T is diagonal, then it is normal, and thus so is A.
7.5.10. If A is normal, so is A âˆ’Î»I. Consequently, A âˆ’Î»I is RPN, and hence
N (A âˆ’Î»I) = N (A âˆ’Î»I)âˆ—(p. 408), so (A âˆ’Î»I) x = 0 â‡â‡’(Aâˆ—âˆ’Î»I)x = 0.
7.5.11. Just as in the proof of the min-max part, it suï¬ƒces to prove
Î»i = max
dim V=i min
yâˆˆV
âˆ¥yâˆ¥2=1
yâˆ—Dy.
For each subspace V of dimension i, let SV = {y âˆˆV, âˆ¥yâˆ¥2 = 1}, and let
Sâ€²
V = {y âˆˆV âˆ©FâŠ¥, âˆ¥yâˆ¥2 = 1},
where
F = {e1, e2, . . . , eiâˆ’1} .
( V âˆ©FâŠ¥Ì¸= 0 â€”otherwise dim(V + FâŠ¥) = dim V + dim FâŠ¥= n + 1, which is
impossible.) So Sâ€²
V contains vectors of SV of the form y = (0, . . . , 0, yi, . . . , yn)T
with 
n
j=i |yj|2 = 1, and for each subspace V with dim V = i,
yâˆ—Dy =
n

j=i
Î»j|yj|2 â‰¤Î»i
n

j=i
|yj|2 = Î»i
for all y âˆˆSâ€²
V.
Since Sâ€²
V âŠ†SV, it follows that min
SV yâˆ—Dy â‰¤min
Sâ€²
V
yâˆ—Dy â‰¤Î»i, and hence
max
V
min
SV yâˆ—Dy â‰¤Î»i.
To reverse this inequality, let ËœV = span {e1, e2, . . . , ei} , and observe that
yâˆ—Dy =
i

j=1
Î»j|yj|2 â‰¥Î»i
i

j=1
|yj|2 = Î»i
for all y âˆˆSËœV,
so max
V
min
SV yâˆ—Dy â‰¥max
S Ëœ
V
yâˆ—Dy â‰¥Î»i.
7.5.12. Just as before, it suï¬ƒces to prove Î»i =
min
v1,...,viâˆ’1âˆˆCn
max
yâŠ¥v1,...,viâˆ’1
âˆ¥yâˆ¥2=1
yâˆ—Dy. For each set
V = {v1, v2, . . . , viâˆ’1} , let SV = {y âˆˆVâŠ¥, âˆ¥yâˆ¥2 = 1}, and let
Sâ€²
V = {y âˆˆVâŠ¥âˆ©T âŠ¥, âˆ¥yâˆ¥2 = 1},
where
T = {ei+1, . . . , en}

Solutions
145
( VâŠ¥âˆ©T âŠ¥Ì¸= 0 â€”otherwise dim(VâŠ¥+T âŠ¥) = dim VâŠ¥+dim T âŠ¥= n+1, which is
impossible.) So Sâ€²
V contains vectors of SV of the form y= (y1, . . . , yi, 0, . . . , 0)T
with 
i
j=1 |yj|2 = 1, and for each V = {v1, . . . , viâˆ’1},
yâˆ—Dy =
i

j=1
Î»j|yj|2 â‰¥Î»i
i

j=1
|yj|2 = Î»i
for all y âˆˆSâ€²
V.
Since Sâ€²
V âŠ†SV, it follows that max
SV
yâˆ—Dy â‰¥max
Sâ€²
V
yâˆ—Dy â‰¥Î»i, and hence
min
V max
SV
yâˆ—Dy â‰¥Î»i.
This inequality is reversible because if ËœV = {e1, e2, . . . , eiâˆ’1} , then every y âˆˆËœV
has the form y = (0, . . . , 0, yi, . . . , yn)T , so
yâˆ—Dy =
n

j=i
Î»j|yj|2 â‰¤Î»i
n

j=i
|yj|2 = Î»i
for all y âˆˆSËœV,
and thus min
V max
SV
yâˆ—Dy â‰¤max
S Ëœ
V
yâˆ—Dy â‰¤Î»i. The solution for Exercise 7.5.11
can be adapted in a similar fashion to prove the alternate max-min expression.
7.5.13. (a)
Unitary matrices are unitarily diagonalizable because they are normal. Fur-
thermore, if (Î», x) is an eigenpair for a unitary U, then
âˆ¥xâˆ¥2
2 = âˆ¥Uxâˆ¥2
2 = âˆ¥Î»xâˆ¥2
2 = |Î»|2 âˆ¥xâˆ¥2
2
=â‡’
|Î»| = 1
=â‡’
Î» = cos Î¸+i sin Î¸ = eiÎ¸.
(b)
This is a special case of Exercise 7.2.26 whose solution is easily adapted to
provide the solution for the case at hand.
Solutions for exercises in section 7. 6
7.6.1. Check the pivots in the LDLT factorization to see that A and C are positive
deï¬nite. B is positive semideï¬nite.
7.6.2. (a)
Examining Figure 7.6.7 shows that the force on m1 to the left, by Hookeâ€™s
law, is F (l)
1 =kx1, and the force to the right is F (r)
1
=k(x2 âˆ’x1), so the total
force on m1 is F1 = F (l)
1
âˆ’F (r)
1
= k(2x1 âˆ’x2). Similarly, the total force on
m2 is F2 = k(âˆ’x1 + 2x2). Using Newtonâ€™s laws F1 = m1a1 = m1xâ€²â€²
1 and
F2 = m2a2 = m2xâ€²â€²
2 yields the two second-order diï¬€erential equations
m1xâ€²â€²
1(t) = k(2x1 âˆ’x2)
m2xâ€²â€²
2(t) = k(âˆ’x1 + 2x2)
=â‡’
Mxâ€²â€² = Kx,
where M =

m1
0
0
m2

, and K = k

âˆ’2
1
1
âˆ’2

.

146
Solutions
(b)
Î» = (3Â±
âˆš
3)/2, and the normal modes are determined by the corresponding
eigenvectors, which are found in the usual way by solving
(K âˆ’Î»M)v = 0.
They are
v1 =

âˆ’1 âˆ’
âˆš
3
1

and
v2 =

âˆ’1 +
âˆš
3
1

(c)
This part is identical to that in Example 7.6.1 (p. 559) except a 2 Ã— 2
matrix is used in place of a 3 Ã— 3 matrix.
7.6.3. Each mass â€œfeelsâ€ only the spring above and below it, so
m1yâ€²â€²
1 = Force up âˆ’Force down = ky1 âˆ’k(y2 âˆ’y1) = k(2y1 âˆ’y2)
m2yâ€²â€²
2 = Force up âˆ’Force down = k(y2 âˆ’y1) âˆ’k(y3 âˆ’y2) = k(âˆ’y1 + 2y2 âˆ’y3)
m3yâ€²â€²
3 = Force up âˆ’Force down = k(y3 âˆ’y2)
(b)
Gerschgorinâ€™s theorem (p. 498) shows that the eigenvalues are nonnegative,
as since det (K) Ì¸= 0, it follows that K is positive deï¬nite.
(c)
The same technique used in the vibrating beads problem in Example 7.6.1
(p. 559) shows that modes are determined by the eigenvectors. Some computation
is required to produce Î»1 â‰ˆ.198, Î»2 â‰ˆ1.55, and Î»3 â‰ˆ3.25. The modes are
deï¬ned by the associated eigenvectors
x1 =
ï£«
ï£­
Î³
Î±
Î²
ï£¶
ï£¸â‰ˆ
ï£«
ï£­
.328
.591
.737
ï£¶
ï£¸,
x2 =
ï£«
ï£­
âˆ’Î²
âˆ’Î³
Î±
ï£¶
ï£¸,
and
x3 =
ï£«
ï£­
âˆ’Î±
Î²
âˆ’Î³
ï£¶
ï£¸.
7.6.4. Write the quadratic form as 13x2+10xy+13y2 = ( x
y )
 13
5
5
13
	  x
y
	
= zT Az.
We know from Example 7.6.3 on p. 567 that if Q is an orthogonal matrix such
that QT AQ = D =
 Î»1
0
0
Î»2
	
, and if w = QT z =
 u
v
	
, then
13x2 + 10xy + 13y2 = zT Az = wT Dw = Î»1u2 + Î»2v2.
Computation reveals that Î»1 = 8,
Î»2 = 18, and Q =
1
âˆš
2

1
1
âˆ’1
1
	
, so the
graph of 13x2 + 10xy + 13y2 = 72 is the same as that for 18u2 + 8v2 = 72
or, equivalently, u2/9 + v2/4 = 1. It follows from (5.6.13) on p. 326 that the
uv-coordinate system results from rotating the standard xy-coordinate system
counterclockwise by 45â—¦.
7.6.5. Since A is symmetric, the LDU factorization is really A = LDLT (see Exercise
3.10.9 on p. 157). In other words, A âˆ¼= D, so Sylvesterâ€™s law of inertia guarantees
that the inertia of A is the same as the inertia of D.

Solutions
147
7.6.6. (a)
Notice that, in general, when xT Ax is expanded, the coeï¬ƒcient of xixj
is given by (aij + aji)/2. Therefore, for the problem at hand, we can take
A = 1
9
ï£«
ï£­
âˆ’2
2
8
2
7
10
8
10
4
ï£¶
ï£¸.
(b)
Gaussian elimination provides A = LDLT , where
L =
ï£«
ï£­
1
0
0
âˆ’1
1
0
âˆ’4
2
1
ï£¶
ï£¸
and
D =
ï£«
ï£­
âˆ’2/9
0
0
0
1
0
0
0
0
ï£¶
ï£¸,
so the inertia of A is (1, 1, 1). Setting y = LT x (or, x = (LT )âˆ’1y) yields
xT Ax = yT Dy = âˆ’2
9y2
1 + y2
2.
(c)
No, the form is indeï¬nite.
(d)
The eigenvalues of A are {2, âˆ’1, 0}, and hence the inertia is (1, 1, 1).
7.6.7. AAâˆ—is positive deï¬nite (because A is nonsingular), so its eigenvalues Î»i are
real and positive. Consequently, the spectral decomposition (p. 517) allows us to
write AAâˆ—= 
k
i=1 Î»iGi. Use the results on (p. 526), and set
R = (AAâˆ—)1/2 =
k

i=1
Î»1/2
i
Gi,
and
Râˆ’1 = (AAâˆ—)âˆ’1/2 =
k

i=1
Î»âˆ’1/2
i
Gi.
It now follows that R is positive deï¬nite, and A = R(Râˆ’1A) = RU, where
U = Râˆ’1A. Finally, U is unitary because
UUâˆ—= (AAâˆ—)âˆ’1/2AAâˆ—(AAâˆ—)âˆ’1/2 = I.
If R1U1 = A = R2U2, then Râˆ’1
2 R1 = U2Uâˆ—
1, which is unitary, so
Râˆ’1
2 R1R1Râˆ’1
2
= I
=â‡’
R2
1 = R2
2
=â‡’
R1 = R2 (because the Ri â€™s are pd).
7.6.8. The 2-norm condition number is the ratio of the largest to smallest singular
values. Since L is symmetric and positive deï¬nite, the singular values are the
eigenvalues, and, by (7.6.8), max Î»ij â†’8 and min Î»ij â†’0 as n â†’âˆ.
7.6.9. The procedure is essentially identical to that in Example 7.6.2. The only diï¬€er-
ence is that when (7.6.6) is applied, the result is
âˆ’4uij + (uiâˆ’1,j + ui+1,j + ui,jâˆ’1 + ui,j+1)
h2
+ O(h2) = fij

148
Solutions
or, equivalently,
4uij âˆ’(uiâˆ’1,j +ui+1,j +ui,jâˆ’1+ui,j+1)+O(h4) = âˆ’h2fij
for
i, j = 1, 2, . . . , n.
If the O(h4) terms are neglected, and if the boundary values gij are taken to
the right-hand side, then, with the same ordering as indicated in Example 7.6.2,
the system Lu = g âˆ’h2f is produced.
7.6.10. InâŠ—An =
ï£«
ï£¬
ï£¬
ï£­
An
0
Â· Â· Â·
0
0
An
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
An
ï£¶
ï£·
ï£·
ï£¸, AnâŠ—In =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
2In
âˆ’In
âˆ’In
2In
âˆ’In
...
...
...
âˆ’In
2In
âˆ’In
âˆ’In
2In
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
,
An + 2In = Tn =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
4
âˆ’1
âˆ’1
4
âˆ’1
...
...
...
âˆ’1
4
âˆ’1
âˆ’1
4
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
nÃ—n
, so
(In âŠ—An) + (An âŠ—In) =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Tn
âˆ’In
âˆ’In
Tn
âˆ’In
...
...
...
âˆ’In
Tn
âˆ’In
âˆ’In
Tn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
= Ln2Ã—n2.
Solutions for exercises in section 7. 7
7.7.1. No. This can be deduced directly from the deï¬nition of index given on p. 395,
or it can be seen by looking at the Jordan form (7.7.6) on p. 579.
7.7.2. Since the index k of a 4 Ã— 4 nilpotent matrix cannot exceed 4, consider the
diï¬€erent possibilities for k = 1, 2, 3, 4. For k = 1,
N = 04Ã—4 is the only
possibility. If k = 2, the largest Jordan block in N is 2 Ã— 2, so
N =
ï£«
ï£¬
ï£¬
ï£­
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
ï£¶
ï£·
ï£·
ï£¸
and
N =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£¸
are the only possibilities. If k = 3 or k = 4, then the largest Jordan block is
3 Ã— 3 or 4 Ã— 4, so
N =
ï£«
ï£¬
ï£¬
ï£­
0
1
0
0
0
0
1
0
0
0
0
0
0
0
0
0
ï£¶
ï£·
ï£·
ï£¸
and
N =
ï£«
ï£¬
ï£­
0
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
ï£¶
ï£·
ï£¸

Solutions
149
are the only respective possibilities.
7.7.3. Let k = index (L), and let Î¶i denote the number of blocks of size i Ã— i or
larger. This number is determined by the number of chains of length i or larger,
and such chains emanate from the vectors in Skâˆ’1 âˆªSkâˆ’2 âˆªÂ· Â· Â· âˆªSiâˆ’1 = Biâˆ’1.
Since Biâˆ’1 is a basis for Miâˆ’1, it follows that Î¶i = dim Miâˆ’1 = riâˆ’1 âˆ’ri,
where ri = rank

Li	
â€”recall (7.7.3).
7.7.4. x âˆˆMi = R

Li	
âˆ©N (L)
=â‡’
x = Liy for some y and Lx = 0
=â‡’
x =
Liâˆ’1(Ly)y and Lx = 0
=â‡’
x âˆˆR

Liâˆ’1	
âˆ©N (L) = Miâˆ’1.
7.7.5. It suï¬ƒces to prove that R

Lkâˆ’1	
âŠ†N (L), and this is accomplished by writing
x âˆˆR

Lkâˆ’1	
=â‡’x = Lkâˆ’1y for some y
=â‡’Lx = Lky = 0 =â‡’x âˆˆN (L).
7.7.6. This follows from the result on p. 211.
7.7.7. L2 = 0 means that L is nilpotent of index k = 2. Consequently, the size of
the largest Jordan block in N is 2 Ã— 2. Since r1 = 2 and ri = 0 for i â‰¥2,
the number of 2 Ã— 2 blocks is r1 âˆ’2r2 + r3 = 2, so the Jordan form is
N =
ï£«
ï£¬
ï£¬
ï£­
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
ï£¶
ï£·
ï£·
ï£¸.
In this case, M0 = N (L) = R (L) = M1 because L2 = 0
=â‡’
R (L) âŠ†N (L)
and dim R (L) = 2 = dim R (L). Now, S1 = {Lâˆ—1, Lâˆ—2} (the basic columns in
L ) is a basis for M1 = R (L), and S0 = Ï†. Since x1 = e1 and x2 = e2 are
solutions for Lx1 = Lâˆ—1 and Lx2 = Lâˆ—1, respectively, there are two Jordan
chains, namely {Lx1, x1} = {Lâˆ—1, e1} and {Lx2, x2} = {Lâˆ—2, e2}, so
P = [ Lâˆ—1 | e1 | Lâˆ—2 | e2 ] =
ï£«
ï£¬
ï£­
3
1
3
0
âˆ’2
0
âˆ’1
1
1
0
âˆ’1
0
âˆ’5
0
âˆ’4
0
ï£¶
ï£·
ï£¸.
Use direct computation to verify that Pâˆ’1LP = N.
7.7.8. Computing ri = rank

Li	
reveals that r1 = 4, r2 = 1, and r3 = 0, so the
index of L is k = 3, and
the number of 3 Ã— 3 blocks = r2 âˆ’2r3 + r4 = 1,
the number of 2 Ã— 2 blocks = r1 âˆ’2r2 + r3 = 2,
the number of 1 Ã— 1 blocks = r0 âˆ’2r1 + r2 = 1.

150
Solutions
Consequently, the Jordan form of L is
N =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Notice that four Jordan blocks were found, and this agrees with the fact that
dim N (L) = 8 âˆ’rank (L) = 4.
7.7.9. If Ni is an ni Ã— ni, nilpotent block as described in (7.7.5), and if Di is the
diagonal matrix Di = diag

1, Ïµi, . . . , Ïµniâˆ’1	
, then Dâˆ’1
i NiDi = ÏµiNi. There-
fore, if Pâˆ’1LP = N is in Jordan form, and if Q = PD, where D is the
block-diagonal matrix D = diag (D1, D2, . . . , Dt) , then Qâˆ’1LQ = ËœN.
Solutions for exercises in section 7. 8
7.8.1. Since rank (A) = 7, rank

A2	
= 6, and rank

A3	
= 5 = rank

A3+i	
, there
is one 3 Ã— 3 Jordan block associates with Î» = 0. Since rank (A + I) = 6 and
rank

(A + I)2	
= 5 = rank

(A + I)2+i	
, there is one 1 Ã— 1 and one 2 Ã— 2
Jordan block associated with Î» = âˆ’1. Finally, rank (Aâˆ’I) = rank

(Aâˆ’I)1+i	
implies there are two 1 Ã— 1 blocks associated with Î» = 1 â€”i.e., Î» = 1 is a
semisimple eigenvalue. Therefore, the Jordan form for A is
J =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
1
0
0
1
0
âˆ’1
1
0
âˆ’1
âˆ’1
1
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
7.8.2. As noted in Example 7.8.3, Ïƒ (A) = {1} and k = index (1) = 2. Use the pro-
cedure on p. 211 to determine a basis for Mkâˆ’1 = M1 = R(A âˆ’I) âˆ©N(A âˆ’I)
to be S1 =

1
âˆ’2
âˆ’2

= b1
+
. (You might also determine this just by inspection.)
A basis for N (A âˆ’I) is easily found to be
 0
1
0

,

1
0
âˆ’2
+
, so examining

Solutions
151
the basic columns of

1
0
1
âˆ’2
1
0
âˆ’2
0
âˆ’2

yields the extension set S0 =
 0
1
0

= b2
+
.
Solving (A âˆ’I)x = b1 produces x = e3, so the associated Jordan chain is
{b1, e3}, and thus P =

b1 | e3 | b2
	
=

1
0
0
âˆ’2
0
1
âˆ’2
1
0

. Itâ€™s easy to check that
Pâˆ’1AP =
 1
1
0
0
1
0
0
0
1

is indeed the Jordan form for A.
7.8.3. If k = index (Î»), then the size of the largest Jordan block associated with Î»
is k Ã— k. This insures that Î» must be repeated at least k times, and thus
index (Î») â‰¤alg mult (Î») .
7.8.4. index (Î») = 1 if and only if every Jordan block is 1 Ã— 1, which happens if
and only if the number of eigenvectors associated with Î» in P such that
Pâˆ’1AP = J is the same as the number of Jordan blocks, and this is just an-
other way to say that alg multA (Î») = geo multA (Î») , which is the deï¬nition of
Î» being a semisimple eigenvalue (p. 510).
7.8.5. Notice that R2 = I, so Râˆ’1 = R = RT , and if Jâ‹†=
 Î»
1
...
...
Î»

is a generic
Jordan block, then Râˆ’1Jâ‹†R = RJâ‹†R = JT
â‹†. Thus every Jordan block is similar
to its transpose. Given any Jordan form, reversal matrices of appropriate sizes
can be incorporated into a block-diagonal matrix :R such that :Râˆ’1J :R = JT
showing that J is similar to JT . Consequently, if A = PJPâˆ’1, then
AT = Pâˆ’1T JT PT = Pâˆ’1T :Râˆ’1J :RPT = Pâˆ’1T :Râˆ’1Pâˆ’1AP :RPT = Qâˆ’1AQ,
where Q = P :RPT .
7.8.6. If Ïƒ (A) = {Î»1, Î»2, . . . , Î»s} , where alg mult (Î»i) = ai, then the characteristic
equation for A is 0 = (x âˆ’Î»1)a1(x âˆ’Î»2)a2 Â· Â· Â· (x âˆ’Î»s)as = c(x). If
J =
ï£«
ï£­
...
Jâ‹†...
ï£¶
ï£¸= Pâˆ’1AP is in Jordan form with Jâ‹†=
 Î»i
1
...
...
Î»i

representing a generic Jordan block, then
c(A) = c(PJPâˆ’1) = Pc(J)Pâˆ’1 = P
ï£«
ï£­
...
c(Jâ‹†)...
ï£¶
ï£¸Pâˆ’1.
Notice that if Jâ‹†is r Ã— r , then (Jâ‹†âˆ’Î»iI)r =
 0
1
...
...
0
r
= 0. Since the size of
the largest Jordan block associated with Î»i is ki Ã— ki, where ki = index (Î»i) â‰¤
alg mult (Î»i) = ai, it follows that (Jâ‹†âˆ’Î»iI)ai = 0. Consequently c(Jâ‹†) = 0 for
every Jordan block, and thus c(A) = 0.

152
Solutions
7.8.7. By using the Jordan form for A, one can ï¬nd a similarity transformation P
such that Pâˆ’1 (A âˆ’Î»I) P =

LmÃ—m
0
0
C

with Lk = 0 and C nonsingular.
Therefore, Pâˆ’1 (A âˆ’Î»I)k P =

0mÃ—m
0
0
Ck

, and thus
dim N

(A âˆ’Î»I)k	
= n âˆ’rank

(A âˆ’Î»I)k 	
= n âˆ’rank

Ck	
= m.
It is also true that dim N

(A âˆ’Î»I)m	
= m because the nullspace remains the
same for all powers beyond the index (p. 395).
7.8.8. To prove Mkj(Î»j) = 0, suppose x âˆˆMkj(Î»j) so that (A âˆ’Î»jI)x = 0 and
x = (A âˆ’Î»jI)kjz for some z. Combine these with the properties of index (Î»j)
(p. 587) to obtain
(A âˆ’Î»jI)kj+1z = 0
=â‡’
(A âˆ’Î»jI)kjz = 0
=â‡’
x = 0.
The fact that the subspaces are nested follows from the observation that if x âˆˆ
Mi+1(Î»j), then x = (A âˆ’Î»jI)i+1z and (A âˆ’Î»jI)x = 0 implies x = (A âˆ’
Î»jI)i
(A âˆ’Î»jI)z
	
and (A âˆ’Î»jI)x = 0, so Mi+1(Î»j) âŠ†Mi(Î»j).
7.8.9. b(Î»j) âˆˆSi(Î»j) âŠ†Mi(Î»j) = R

(A âˆ’Î»jI)i	
âˆ©N (A âˆ’Î»jI) âŠ†R

(A âˆ’Î»jI)i	
.
7.8.10. Noâ€”consider A =
ï£«
ï£­
1
1
0
0
1
0
0
0
2
ï£¶
ï£¸and Î» = 1.
7.8.11. (a)
All of these facts are established by straightforward arguments using ele-
mentary properties of matrix algebra, so the details are omitted here.
(b)
To show that the eigenvalues of AâŠ—B are {Î»iÂµj} m
i=1
n
j=1, let JA = Pâˆ’1AP
and JB = Qâˆ’1BQ be the respective Jordan forms for A and B, and use
properties from (a) to establish that A âŠ—B is similar to JA âŠ—JB by writing
JA âŠ—JB = (Pâˆ’1AP) âŠ—(Qâˆ’1BQ) = (Pâˆ’1 âŠ—Qâˆ’1)(A âŠ—B)(P âŠ—Q)
= (P âŠ—Q)âˆ’1(A âŠ—B)(P âŠ—Q)
Thus the eigenvalues of A âŠ—B are the same as those of JA âŠ—JB, and because
JA and JB are upper triangular with the Î»i â€™s and Âµi â€™s on the diagonal, itâ€™s
clear that JA âŠ—JB is also upper triangular with diagonal entries being Î»iÂµj.
To show that the eigenvalues of (AâŠ—In)+(Im âŠ—B) are {Î»i +Âµj} m
i=1
n
j=1, show
that (A âŠ—In) + (Im âŠ—B) is similar to (JA âŠ—In) + (Im âŠ—JB) by writing
(JA âŠ—In) + (Im âŠ—JB) = (Pâˆ’1AP) âŠ—(Qâˆ’1IQ) + (Pâˆ’1IP) âŠ—(Qâˆ’1BQ)
= (Pâˆ’1 âŠ—Qâˆ’1)(A âŠ—I)(P âŠ—Q)
+ (Pâˆ’1 âŠ—Qâˆ’1)(I âŠ—B)(P âŠ—Q)
= (Pâˆ’1 âŠ—Qâˆ’1)

(A âŠ—I) + (I âŠ—B)

(P âŠ—Q)
= (P âŠ—Q)âˆ’1
(A âŠ—I) + (I âŠ—B)

(P âŠ—Q).

Solutions
153
Thus (AâŠ—In)+(ImâŠ—B) and (JA âŠ—In) + (Im âŠ—JB) have the same eigenvalues,
and the latter matrix is easily seen to be an upper-triangular matrix whose
diagonal entries are {Î»i + Âµj} m
i=1
n
j=1.
7.8.12. It was established in Exercise 7.6.10 (p. 573) that Ln2Ã—n2 = (InâŠ—An)+(AnâŠ—In),
where
An =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
2
âˆ’1
âˆ’1
2
âˆ’1
...
...
...
âˆ’1
2
âˆ’1
âˆ’1
2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
nÃ—n
is the ï¬nite diï¬€erence matrix of Example 1.4.1 (p. 19). The eigenvalues of An
were determined in Exercise 7.2.18 (p. 522) to be Âµj = 4 sin2[jÏ€/2(n + 1)] for
j = 1, 2, . . . , n, so it follows from the last property in Exercise 7.8.11 that the
n2 eigenvalues of Ln2Ã—n2 = (In âŠ—An) + (An âŠ—In) are
Î»ij = Âµi + Âµj = 4

sin2

iÏ€
2(n + 1)

+ sin2

jÏ€
2(n + 1)

,
i, j = 1, 2, . . . , n.
7.8.13. The same argument given in the solution of the last part of Exercise 7.8.11
applies to show that if J is the Jordan form for A, then L is similar to
(I âŠ—I âŠ—J) + (I âŠ—J âŠ—I) + (J âŠ—I âŠ—I), and since J is upper triangular with the
eigenvalues Âµj = 4 sin2[jÏ€/2(n + 1)] of A (recall Exercise 7.2.18 (p. 522)) on
the diagonal of J, it follows that the eigenvalues of Ln3Ã—n3 are the n3 numbers
Î»ijk = Âµi +Âµj +Âµk = 4

sin2

iÏ€
2(n + 1)

+ sin2

jÏ€
2(n + 1)

+ sin2

kÏ€
2(n + 1)

for i, j, k = 1, 2, . . . , n.
Solutions for exercises in section 7. 9
7.9.1. If ui(t) denotes the number of pounds of pollutant in lake i at time t > 0,
then the concentration of pollutant in lake i at time t is ui(t)/V lbs/gal, so
the model uâ€²
i(t) = (lbs/sec) coming inâˆ’(lbs/sec) going out produces the system
uâ€²
1 = 4r
V u2 âˆ’4r
V u1
uâ€²
2 = 2r
V u1 + 3r
V u3 âˆ’5r
V u2
uâ€²
3 = 2r
V u1 + r
V u2 âˆ’3r
V u3
or
ï£«
ï£¬
ï£­
uâ€²
1
uâ€²
2
uâ€²
3
ï£¶
ï£·
ï£¸= r
V
ï£«
ï£¬
ï£­
âˆ’4
4
0
2
âˆ’5
3
2
1
âˆ’3
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
u1(t)
u2(t)
u3(t)
ï£¶
ï£·
ï£¸.
The solution of uâ€² = Au with u(0) = c is u = eAtc. The eigenvalues of A
are Î»1 = 0 with alg mult (Î»1) = 1 and Î»2 = âˆ’6r/V with index (Î»2) = 2, so
u = eAtc = eÎ»1tG1c + eÎ»2tG2c + teÎ»2t(A âˆ’Î»2I)G2c.

154
Solutions
Since Î»1 = 0 is a simple eigenvalue, it follows from (7.2.12) on p. 518 that
G1 = xyT /yT x, where x and yT are any pair of respective right-hand and
left-hand eigenvectors associated with Î»1 = 0. By observing that Ae = 0 and
eT A = 0T for e = (1, 1, 1)T (this is a result of being a closed system), and by
using G1 + G2 = I, we have (by using x = y = e)
G1 = eeT
3 ,
G2 = I âˆ’eeT
3 ,
and
(A âˆ’Î»2I)G2 = A âˆ’Î»2I + Î»2
3 eeT .
If Î± = (c1 + c2 + c3)/3 = eT c/3 denotes the average of the initial values, then
G1c = Î±e and G2c = c âˆ’Î±e, so
u(t) = Î±e + eÎ»2t(c âˆ’Î±e) + teÎ»2t
Ac âˆ’Î»2(c âˆ’Î±e)

for
Î»2 = âˆ’6r/V.
Since Î»2 < 0, it follows that u(t) â†’Î±e as t â†’âˆ. In other words, the long-run
amount of pollution in each lake is the sameâ€”namely Î± lbsâ€”and this is what
common sense would dictate.
7.9.2. It follows from (7.9.9) that fi(A) = Gi.
7.9.3. We know from Exercise 7.9.2 that Gi = fi(A) for fi(z) =

1
when z = Î»i,
0
otherwise,
and from Example 7.9.4 (p. 606) we know that every function of A is a poly-
nomial in A.
7.9.4. Using f(z) = zk in (7.9.9) on p. 603 produces the desired result.
7.9.5. Using f(z) = zn in (7.9.2) on p. 600 produces the desired result.
7.9.6. A is the matrix in Example 7.9.2, so the results derived there imply that
eA = e2G1 + e4G2 + e4(A âˆ’4I)G2 =
ï£«
ï£­
3e4
2e4
e4 âˆ’e2
âˆ’2e4
âˆ’e4
âˆ’4e4 âˆ’2e2
0
0
e2
ï£¶
ï£¸.
7.9.7. The eigenvalues of A are Î»1 = 1 and Î»2 = 4 with alg mult (1) = 1 and
index (4) = 2, so
f(A) = f(1)G1 + f(4)G2 + f â€²(4)(A âˆ’4I)G2
Since Î»1 = 1 is a simple eigenvalue, it follows from formula (7.2.12) on p. 518
that G1 = xyT /yT x, where x and yT are any pair of respective right-hand
and left-hand eigenvectors associated with Î»1 = 1. Using x = (âˆ’2, 1, 0)T and
y = (1, 1, 1)T produces
G1 =
ï£«
ï£­
2
2
2
âˆ’1
âˆ’1
âˆ’1
0
0
0
ï£¶
ï£¸
and
G2 = I âˆ’G1 =
ï£«
ï£­
âˆ’1
âˆ’2
âˆ’2
1
2
1
0
0
1
ï£¶
ï£¸

Solutions
155
Therefore,
f(A) = 4
âˆš
A âˆ’I = 3G1 + 7G2 + (A âˆ’4I)G2 =
ï£«
ï£­
âˆ’2
âˆ’10
âˆ’11
6
15
10
âˆ’1
âˆ’2
4
ï£¶
ï£¸.
7.9.8. (a)
The only point at which derivatives of f(z) = z1/2 fail to exist are at
Î» = 0, so as long as A is nonsingular, f(A) =
âˆš
A is deï¬ned.
(b)
If A is singular so that 0 âˆˆÏƒ (A) itâ€™s clear from (7.9.9) that A1/2 exists
if and only if derivatives of f(z) = z1/2 need not be evaluated at Î» = 0, and
this is the case if and only if index (0) = 1.
7.9.9. If 0 Ì¸= xh âˆˆN(A âˆ’Î»hI), then (7.9.11) guarantees that
Gixh =

0
if h Ì¸= i,
xh
if h = i,
so (7.9.9) can be used to conclude that f(A)xh = f(Î»h)xh. Itâ€™s an immediate
consequence of (7.9.3) that alg multA (Î») = alg multf(A) (f(Î»)) .
7.9.10. (a)
If AkÃ—k (with k > 1) is a Jordan block associated with Î» = 0, and if
f(z) = zk, then f(A) = 0 is not similar to A Ì¸= 0.
(b)
Also, geo multA (0) = 1 but geo multf(A) (f(0)) = geo mult0 (0) = k.
(c)
And indexA(0) = k while indexf(A)(f(Î»)) = index0(0) = 1.
7.9.11. This follows because, as explained in Example 7.9.4 (p. 606), there is always a
polynomial p(z) such that f(A) = p(A), and A commutes with p(A).
7.9.12. Because every square matrix is similar to its transpose (recall Exercise 7.8.5 on
p. 596), and because similar matrices have the same Jordan structure, transpo-
sition doesnâ€™t change the eigenvalues or their indicies. So f(A) exists if and
only if f(AT ) exists. As proven in Example 7.9.4 (p. 606), there is a polyno-
mial p(z) such that f(A) = p(A), so

f(A)
T =

p(A)
T = p(AT ) = f(AT ).
While transposition doesnâ€™t change eigenvalues, conjugate transposition doesâ€”it
conjugates themâ€”so itâ€™s possible that f can exist at A but not at Aâˆ—. Fur-
thermore, you canâ€™t replace (â‹†)T by (â‹†)âˆ—in the above argument because if p(z)
has some complex coeï¬ƒcients, then

p(A)
âˆ—Ì¸= p(Aâˆ—).
7.9.13. (a)
If f1(z) = ez, f2(z) = eâˆ’z, and p(x, y) = xy âˆ’1, then
h(z) = p

f1(z), f2(z)
	
= ezeâˆ’z âˆ’1 = 0
for all
z âˆˆC,
so h(A) = p

f1(A), f2(A)
	
= 0 for all A âˆˆCnÃ—n, and thus eAeâˆ’A âˆ’I = 0.
(b)
Use f1(z) = eÎ±z, f2(z) =

ez	Î±, and p(x, y) = x âˆ’y. Since
h(z) = p

f1(z), f2(z)
	
= eÎ±z âˆ’

ez	Î± = 0
for all z âˆˆC,
h(A) = p

f1(A), f2(A)
	
= 0 for all A âˆˆCnÃ—n, and thus eÎ±A =

eA	Î±.

156
Solutions
(c)
Using f1(z) = eiz, f2(z) = cos z + i sin z, and p(x, y) = x âˆ’y produces
h(z) = p

f1(z), f2(z)
	
, which is zero for all z, so h(A) = 0 for all A âˆˆCnÃ—n.
7.9.14. (a)
The representation ez = 
âˆ
n=0 zn/n! together with AB = BA yields
eA+B =
âˆ

n=0
(A + B)n
n!
=
âˆ

n=0
1
n!
n

j=0
n
j

AjBnâˆ’j =
âˆ

n=0
n

j=0
AjBnâˆ’j
j!(n âˆ’j)!
=
âˆ

r=0
âˆ

s=0
1
r!
1
s!ArBs =
 âˆ

r=0
Ar
r!
  âˆ

s=0
Bs
s!

= eAeB.
(b)
If A =

0
1
0
0

and B =

0
0
1
0

, then
eAeB =

1
1
0
1
 
1
0
1
1

=

2
1
1
1

,
but
eA+B = 1
2

e + eâˆ’1
e âˆ’eâˆ’1
e âˆ’eâˆ’1
e + eâˆ’1

.
7.9.15. The characteristic equation for A is Î»3 = 0, and the number of 2 Ã— 2 Jordan
blocks associated with Î» = 0 is Î½2 = rank (A) âˆ’2 rank

A2	
+ rank

A3	
= 1
(from the formula on p. 590), so index (Î» = 0) = 2. Therefore, for f(z) = ez we
are looking for a polynomial p(z) = Î±0 + Î±1z such that p(0) = f(0) = 1 and
pâ€²(0) = f â€²(0) = 1. This yields the Hermite interpolation polynomial as
p(z) = 1 + z,
so
eA = p(A) = I + A.
Note: Since A2 = 0, this agrees with the inï¬nite series representation for eA.
7.9.16. (a)
The advantage is that the only the algebraic multiplicity and not the index
of each eigenvalue is requiredâ€”determining index generally requires more eï¬€ort.
The disadvantage is that a higher-degree polynomial might be required, so a
larger system might have to be solved. Another disadvantage is the fact that f
may not have enough derivatives deï¬ned at some eigenvalue for this method to
work in spite of the fact that f(A) exists.
(b)
The characteristic equation for A is Î»3 = 0, so, for f(z) = ez, we are
looking for a polynomial p(z) = Î±0 + Î±1z + Î±2z2 such that p(0) = f(0) = 1,
pâ€²(0) = f â€²(0) = 1, and pâ€²â€²(0) = f â€²â€²(0) = 1. This yields
p(z) = 1 + z + z2
2 ,
so
eA = p(A) = I + A + A2
2 .
Note: Since A2 = 0, this agrees with the result in Exercise 7.9.15.
7.9.17. Since Ïƒ (A) = {Î±} with index (Î±) = 3, it follows from (7.9.9) that
f(A) = f(Î±)G1 + f â€²(Î±)(A âˆ’Î±I)G1 + f â€²â€²(Î±)
2!
(A âˆ’Î±I)2G1.

Solutions
157
The desired result is produced by using G1 = I (because of (7.9.10)), and
A âˆ’Î±I = Î²N + Î³N2, and N3 = 0.
7.9.18. Since
g(Jâ‹†) =
ï£«
ï£­
g(Î»)
gâ€²(Î»)
gâ€²â€²(Î»)/2!
0
g(Î»)
gâ€²(Î»)
0
0
g(Î»)
ï£¶
ï£¸,
using Exercise 7.9.17 with Î± = g(Î»), Î² = gâ€²(Î»), and Î³ = gâ€²â€²(Î»)/2! yields
f

g(Jâ‹†)
	
= f(g(Î»))I+gâ€²(Î»)f â€²(g(Î»))N+
;
gâ€²â€²(Î»)f â€²(g(Î»))
2!
+

gâ€²(Î»)
2f â€²â€²(g(Î»))
2!
<
N2.
Observing that
h(Jâ‹†) =
ï£«
ï£­
h(Î»)
hâ€²(Î»)
hâ€²â€²(Î»)/2!
0
h(Î»)
hâ€²(Î»)
0
0
h(Î»)
ï£¶
ï£¸= h(Î»)I + hâ€²(Î»)N + hâ€²â€²(Î»)
2!
N2,
where h(Î») = f(g(Î»)), hâ€²(Î») = f â€²(g(Î»))gâ€²(Î»), and
hâ€²â€²(Î») = gâ€²â€²(Î»)f â€²(g(Î»)) + f â€²â€²(g(Î»))

gâ€²(Î»)
2
proves that h(Jâ‹†) = f

g(Jâ‹†)
	
.
7.9.19. For the function
fi(z) =

1
in a small circle about Î»i that is interior to Î“i,
0
elsewhere,
it follows, just as in Exercise 7.9.2, that fi(A) = Gi. But using fi in (7.9.22)
produces fi(A) =
1
2Ï€i
=
Î“i(Î¾I âˆ’A)âˆ’1dÎ¾, and thus the result is proven.
7.9.20. For a k Ã— k Jordan block Jâ‹†=
ï£«
ï£¬
ï£­
Î»
1
...
...
Î»
ï£¶
ï£·
ï£¸, itâ€™s straightforward to verify that
Jâˆ’1
â‹†
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î»âˆ’1
âˆ’Î»âˆ’2
Î»âˆ’3
Â· Â· Â· âˆ’1(kâˆ’1)Î»âˆ’k
Î»âˆ’1
âˆ’Î»âˆ’2 ...
...
...
...
Î»âˆ’3
Î»âˆ’1
âˆ’Î»âˆ’2
Î»âˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
f(Î»)
f â€²(Î»)
f â€²â€²(Î»)
2!
Â· Â· Â· f (kâˆ’1)(Î»)
(k âˆ’1)!
f(Î»)
f â€²(Î»)
...
...
...
...
f â€²â€²(Î»)
2!
f(Î»)
f â€²(Î»)
f(Î»)
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
for f(z) = zâˆ’1, and thus if J =

... Jâ‹†...

is the Jordan form for A = PJPâˆ’1,
then the representation of Aâˆ’1 as Aâˆ’1 = PJâˆ’1Pâˆ’1 agrees with the expression
for f(A) = Pf(J)Pâˆ’1 given in (7.9.3) when f(z) = zâˆ’1.

158
Solutions
7.9.21.
1
2Ï€i
4
Î“
Î¾âˆ’1(Î¾I âˆ’A)âˆ’1dÎ¾ = Aâˆ’1.
7.9.22. Partition the Jordan form for A as J =

C
0
0
N

in which C contains all
Jordan segments associated with nonzero eigenvalues and N contains all Jordan
segments associated with the zero eigenvalue (if one exists). Observe that N is
nilpotent, so g(N) = 0, and consequently
A = P

C
0
0
N

Pâˆ’1 â‡’g(A) = P

g(C)
0
0 g(N)

Pâˆ’1 = P

Câˆ’1
0
0
0

Pâˆ’1 = AD.
It follows from Exercise 5.12.17 (p. 428) that g(A) is the Mooreâ€“Penrose pseu-
doinverse Aâ€  if and only if A is an RPN matrix.
7.9.23. Use the Cauchyâ€“Goursat theorem to observe that
=
Î“ Î¾âˆ’jdÎ¾ = 0 for j = 2, 3, . . . ,
and follow the argument given in Example 7.9.8 (p. 611) with Î»1 = 0 along with
the result of Exercise 7.9.22 to write
1
2Ï€i
4
Î“
Î¾âˆ’1(Î¾I âˆ’A)âˆ’1dÎ¾ =
1
2Ï€i
4
Î“
Î¾âˆ’1R(Î¾)dÎ¾
=
1
2Ï€i
4
Î“
s

i=1
kiâˆ’1

j=0
Î¾âˆ’1
(Î¾ âˆ’Î»i)j+1 (A âˆ’Î»iI)jGidÎ¾
=
s

i=1
kiâˆ’1

j=0
 1
2Ï€i
4
Î“
Î¾âˆ’1
(Î¾ âˆ’Î»i)j+1 dÎ¾

(A âˆ’Î»iI)jGi
=
s

i=1
kiâˆ’1

j=0
g(j)(Î»i)
j!
(A âˆ’Î»iI)jGi = g(A) = AD.
Solutions for exercises in section 7. 10
7.10.1. The characteristic equation for A is 0 = x3âˆ’(3/4)xâˆ’(1/4) = (xâˆ’1)(xâˆ’1/2)2,
so (7.10.33) guarantees that A is convergent (and hence also summable).
The characteristic equation for B is x3 âˆ’1 = 0, so the eigenvalues are the three
cube roots of unity, and thus (7.10.33) insures B is not convergent, but B is
summable because Ï(B) = 1 and each eigenvalue on the unit circle is semisimple
(in fact, each eigenvalue is simple).
The characteristic equation for C is
0 = x3 âˆ’(5/2)x2 + 2x âˆ’(1/2) = (x âˆ’1)2(x âˆ’1/2),
but index (Î» = 1) = 2 because rank (C âˆ’I) = 2 while 1 = rank (C âˆ’I)2 =
rank (C âˆ’I)3 = Â· Â· Â· , so C is neither convergent nor summable.

Solutions
159
7.10.2. Since A is convergent, (7.10.41) says that a full-rank factorization I âˆ’A = BC
can be used to compute limkâ†’âˆAk = G = I âˆ’B(CB)âˆ’1C. One full-rank
factorization is obtained by placing the basic columns of I âˆ’A in B and the
nonzero rows of E(Iâˆ’A) in C. This yields
B =
ï£«
ï£­
âˆ’3/2
âˆ’3/2
1
âˆ’1/2
1
âˆ’1/2
ï£¶
ï£¸,
C =

1
âˆ’1
0
0
0
1

,
and
G =
ï£«
ï£­
0
1
âˆ’1
0
1
âˆ’1
0
0
0
ï£¶
ï£¸.
Alternately, since Î» = 1 is a simple eigenvalue, the limit G can also be de-
termined by computing right- and left-hand eigenvectors, x = (1, 1, 0)T
and
yT = (0, âˆ’1, 1), associated with Î» = 1 and setting G = xyT /(yT x) as de-
scribed in (7.2.12) on p. 518. The matrix B is not convergent but it is summable,
and since the unit eigenvalue is simple, the Ces`aro limit G can be determined
as described in (7.2.12) on p. 518 by computing right- and left-hand eigenvec-
tors, x = (1, 1, 1)T
and yT = (1, 1, 1), associated with Î» = 1 and setting
G = xyT /(yT x) = 1
3
ï£«
ï£­
1
1
1
1
1
1
1
1
1
ï£¶
ï£¸.
7.10.3. To see that x(k) = Akx(0) solves x(k+1) = Ax(k), use successive substitution
to write x(1) = Ax(0),
x(2) = Ax(1) = A2x(0),
x(3) = Ax(2) = A3x(0),
etc. Of course you could build a formal induction argument, but itâ€™s not necessary.
To see that x(k) = Akx(0) + 
kâˆ’1
j=0 Akâˆ’jâˆ’1b(j) solves the nonhomogeneous
equation x(k + 1) = Ax(k) + b(k), use successive substitution to write
x(1) = Ax(0) + b(0),
x(2) = Ax(1) + b(1) = A2x(0) + Ab(0) + b(0),
x(3) = Ax(2) + b(2) = A3x(0) + A2b(0) + Ab(0) + b(0),
etc.
7.10.4. Put the basic columns of I âˆ’A in B and the nonzero rows of the reduced row
echelon form E(Iâˆ’A) in C to build a full-rank factorization of I âˆ’A = BC
(Exercise 3.9.8, p. 140), and use (7.10.41).
p=Gp(0)=(I âˆ’B(CB)âˆ’1C)p(0)=
ï£«
ï£¬
ï£­
1/6
1/6
1/6
1/6
1/3
1/3
1/3
1/3
1/3
1/3
1/3
1/3
1/6
1/6
1/6
1/6
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
p1(0)
p2(0)
p3(0)
p4(0)
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
1/6
1/3
1/3
1/6
ï£¶
ï£·
ï£¸.
7.10.5. To see that x(k) = Akx(0) solves x(k+1) = Ax(k), use successive substitution
to write x(1) = Ax(0),
x(2) = Ax(1) = A2x(0),
x(3) = Ax(2) = A3x(0),
etc. Of course you could build a formal induction argument, but itâ€™s not necessary.

160
Solutions
To see that x(k) = Akx(0) + 
kâˆ’1
j=0 Akâˆ’jâˆ’1b(j) solves the nonhomogeneous
equation x(k + 1) = Ax(k) + b(k), use successive substitution to write
x(1) = Ax(0) + b(0),
x(2) = Ax(1) + b(1) = A2x(0) + Ab(0) + b(0),
x(3) = Ax(2) + b(2) = A3x(0) + A2b(0) + Ab(0) + b(0),
etc.
7.10.6. Use (7.1.12) on p. 497 along with (7.10.5) on p. 617.
7.10.7. For A1, the respective iteration matrices for Jacobi and Gaussâ€“Seidel are
HJ =
ï£«
ï£­
0
âˆ’2
2
âˆ’1
0
âˆ’1
âˆ’2
âˆ’2
0
ï£¶
ï£¸
and
HGS =
ï£«
ï£­
0
âˆ’2
2
0
2
âˆ’3
0
0
2
ï£¶
ï£¸.
HJ is nilpotent of index three, so Ïƒ (HJ) = {0} , and hence Ï(HJ) = 0 < 1.
Clearly, HGS is triangular, so Ï(HGS) = 2. > 1 Therefore, for arbitrary right-
hand sides, Jacobiâ€™s method converges after two steps, whereas the Gaussâ€“Seidel
method diverges. On the other hand, for A2,
HJ = 1
2
ï£«
ï£­
0
1
âˆ’1
âˆ’2
0
âˆ’2
1
1
0
ï£¶
ï£¸
and
HGS = 1
2
ï£«
ï£­
0
1
âˆ’1
0
âˆ’1
âˆ’1
0
0
âˆ’1
ï£¶
ï£¸,
and a little computation reveals that Ïƒ (HJ) =
)
Â±i
âˆš
5/2
*
, so Ï(HJ) > 1, while
Ï(HGS) = 1/2 < 1. These examples show that there is no universal superiority
enjoyed by either method because there is no universal domination of Ï(HJ) by
Ï(HGS), or vise versa.
7.10.8. (a)
det (Î±D âˆ’L âˆ’U) = det

Î±D âˆ’Î²L âˆ’Î²âˆ’1U
	
= 8Î±3 âˆ’4Î± for all real Î±
and Î² Ì¸= 0. Furthermore, the Jacobi iteration matrix is
HJ =
ï£«
ï£­
0
1/2
0
1/2
0
1/2
0
1/2
0
ï£¶
ï£¸,
and Example 7.2.5, p. 514, shows Ïƒ (HJ) = {cos(Ï€/4), cos(2Ï€/4), cos(3Ï€/4)}.
Clearly, these eigenvalues are real and Ï (HJ) = (1/
âˆš
2) â‰ˆ.707 < 1.
(b)
According to (7.10.24),
Ï‰opt =
2
1 +
-
1 âˆ’Ï2(HJ)
â‰ˆ1.172,
and
Ï

HÏ‰opt
	
= Ï‰opt âˆ’1 â‰ˆ.172.
(c)
RJ = âˆ’log10 Ï (HJ) = log10(
âˆš
2) â‰ˆ.1505,
RGS = 2RJ â‰ˆ.301, and
Ropt = âˆ’log10 Ï (Hopt) â‰ˆ.766.

Solutions
161
(d)
I used standard IEEE 64-bit ï¬‚oating-point arithmetic (i.e., about 16 deci-
mal digits of precision) for all computations, but I rounded the results to 3 places
to report the answers given below. Depending on your own implementation, your
answers may vary slightly.
Jacobi with 21 iterations:
1
1.5
2.5
3.25
3.75
4.12
4.37
4.56
4.69
4.78
4.84
4.89
4.92
4.95
4.96
4.97
4.98
4.99
4.99
4.99
5
5
1
3
4.5
5.5
6.25
6.75
7.12
7.37
7.56
7.69
7.78
7.84
7.89
7.92
7.95
7.96
7.97
7.98
7.99
7.99
7.99
8
1
3.5
4.5
5.25
5.75
6.12
6.37
6.56
6.69
6.78
6.84
6.89
6.92
6.95
6.96
6.97
6.98
6.99
6.99
6.99
7
7
Gaussâ€“Seidel with 11 iterations:
1
1.5
2.62
3.81
4.41
4.7
4.85
4.93
4.96
4.98
4.99
5
1
3.25
5.62
6.81
7.41
7.7
7.85
7.93
7.96
7.98
7.99
8
1
4.62
5.81
6.41
6.7
6.85
6.93
6.96
6.98
6.99
7
7
SOR (optimum) with 6 iterations:
1
1.59
3.06
4.59
4.89
4.98
5
1
3.69
6.73
7.69
7.93
7.99
8
1
5.5
6.51
6.9
6.98
7
7
7.10.9. The product rule for determinants produces
det (HÏ‰)=det

(Dâˆ’Ï‰L)âˆ’1
det

(1âˆ’Ï‰)D+Ï‰U

=
n
9
i=1
aâˆ’1
ii
n
9
i=1
(1âˆ’Ï‰)aii =(1âˆ’Ï‰)n.
But itâ€™s also true that det (HÏ‰) = >n
i=1 Î»i, where the Î»i â€™s are the eigenvalues
of HÏ‰. Consequently, |Î»k| â‰¥|1 âˆ’Ï‰| for some k because if |Î»i| < |1 âˆ’Ï‰| for
all i, then |1 âˆ’Ï‰|n = |det (HÏ‰)| = >
i |Î»i| < |1 + Ï‰|n, which is impossible.
Therefore, |1 âˆ’Ï‰| â‰¤|Î»k| â‰¤Ï (HÏ‰) < 1 implies 0 < Ï‰ < 2.
7.10.10. Observe that HJ is the block-triangular matrix
HJ = 1
4
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
K
I
I
K
I
...
...
...
I
K
I
I
K
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
n2Ã—n2
with
K =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
1
1
0
1
...
...
...
1
0
1
1
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
nÃ—n
.
Proceed along the same lines as in Example 7.6.2, to argue that HJ is similar
to the block-diagonal matrix
ï£«
ï£¬
ï£¬
ï£­
T1
0
Â· Â· Â·
0
0
T2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
Tn
ï£¶
ï£·
ï£·
ï£¸,
where
Ti =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Îºi
1
1
Îºi
1
...
...
...
1
Îºi
1
1
Îºi
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
nÃ—n

162
Solutions
in which Îºi âˆˆÏƒ (K) . Use the result of Example 7.2.5 (p. 514) to infer that the
eigenvalues of Ti are Îºi + 2 cos jÏ€/(n + 1) for j = 1, 2, . . . , n and, similarly,
the eigenvalues of K are Îºi = 2 cos iÏ€/(n+1) for i = 1, 2, . . . , n. Consequently
the n2 eigenvalues of HJ are Î»ij = (1/4)

2 cos iÏ€/(n + 1) + 2 cos jÏ€/(n + 1)

,
so Ï (HJ) = maxi,j Î»ij = cos Ï€/(n + 1).
7.10.11. If limnâ†’âˆÎ±n = Î±, then for each Ïµ > 0 there is a natural number N = N(Ïµ)
such that |Î±n âˆ’Î±| < Ïµ/2 for all n â‰¥N. Furthermore, there exists a real number
Î² such that |Î±n âˆ’Î±| < Î² for all n. Consequently, for all n â‰¥N,
|Âµn âˆ’Î±| =

Î±1 + Î±2 + Â· Â· Â· + Î±n
n
âˆ’Î±
 = 1
n

N

k=1
(Î±k âˆ’Î±) +
n

k=N+1
(Î±k âˆ’Î±)

â‰¤1
n
N

k=1
|Î±k âˆ’Î±| + 1
n
n

k=N+1
|Î±k âˆ’Î±| < NÎ²
n
+ n âˆ’N
n
Ïµ
2 â‰¤NÎ²
n
+ Ïµ
2.
When n is suï¬ƒciently large, NÎ²/n â‰¤Ïµ/2 so that |Âµn âˆ’Î±| < Ïµ, and therefore,
limnâ†’âˆÂµn = Î±. Note: The same proof works for vectors and matrices by
replacing | â‹†| with a vector or matrix norm.
7.10.12. Prove that (a) â‡’(b) â‡’(c) â‡’(d) â‡’(e) â‡’(f) â‡’(a).
(a) â‡’(b):
This is a consequence of (7.10.28).
(b) â‡’(c):
Use induction on the size of AnÃ—n. For n = 1, the result is trivial.
Suppose the result holds for n = k â€”i.e., suppose positive leading minors insures
the existence of LU factors which are M-matrices when n = k. For n = k + 1,
use the induction hypothesis to write
A(k+1)Ã—(k+1) =
 :A
c
dT
Î±

=
 :L :U
c
dT
Î±

=

:L
0
dT :Uâˆ’1
1
 :U
:Lâˆ’1c
0
Ïƒ

= LU,
where :L and
:U are M-matrices. Notice that Ïƒ > 0 because det( :U) > 0
and 0 < det (A) = Ïƒ det(:L) det( :U). Consequently, L and U are M-matrices
because
Lâˆ’1 =

:Lâˆ’1
0
âˆ’dT :Uâˆ’1:Lâˆ’1
1

â‰¥0
and
Uâˆ’1 =
 :Uâˆ’1
âˆ’Ïƒâˆ’1 :Uâˆ’1:Lâˆ’1c
0
Ïƒâˆ’1

â‰¥0.
(c) â‡’(d):
A = LU with L and U M-matrices implies Aâˆ’1 = Uâˆ’1Lâˆ’1 â‰¥0,
so if x = Aâˆ’1e, where e = (1, 1, . . . , 1)T , then x > 0 (otherwise Aâˆ’1 would
have a zero row, which would force A to be singular), and Ax = e > 0.
(d) â‡’(e):
If x > 0 is such that Ax > 0, deï¬ne D = diag (x1, x2, . . . , xn)
and set B = AD, which is clearly another Z-matrix. For e = (1, 1, . . . , 1)T ,
notice that Be = ADe = Ax > 0 says each row sum of B = AD is positive.
In other words, for each i = 1, 2, . . . , n,
0 <

j
bij =

jÌ¸=i
bij +bii â‡’bii >

jÌ¸=i
âˆ’bij =

jÌ¸=i
|bij|
for each i = 1, 2, . . . , n.

Solutions
163
(e) â‡’(f):
Suppose that AD is diagonally dominant for a diagonal matrix D
with positive entries, and suppose each aii > 0. If E = diag (a11, a22, . . . , ann)
and âˆ’N is the matrix containing the oï¬€-diagonal entries of A, then A = Eâˆ’N
is the Jacobi splitting for A as described in Example 7.10.4 on p. 622, and
AD = ED + ND is the Jacobi splitting for AD with the iteration matrix
H = Dâˆ’1Eâˆ’1ND. It was shown in Example 7.10.4 that diagonal dominance
insures convergence of Jacobiâ€™s method (i.e., Ï (H) < 1 ), so, by (7.10.14), p. 620,
A = ED(I âˆ’H)Dâˆ’1
=â‡’
Aâˆ’1 = D(I âˆ’H)âˆ’1Dâˆ’1Eâˆ’1 â‰¥0,
and this guarantees that if Ax â‰¥0, then x â‰¥0.
(f) â‡’(a):
Let r â‰¥max |aii| so that B = rI âˆ’A â‰¥0, and ï¬rst show that the
condition (Ax â‰¥0 â‡’x â‰¥0) insures the existence of Aâˆ’1. For any x âˆˆN (A),
(rIâˆ’B)x = 0 â‡’rx = Bx â‡’r|x| â‰¤|B|x| â‡’A(âˆ’|x|) â‰¥0 â‡’âˆ’|x| â‰¥0 â‡’x = 0,
so N (A) = 0. Now, A[Aâˆ’1]âˆ—i = ei â‰¥0 â‡’[Aâˆ’1]âˆ—i â‰¥0, and thus Aâˆ’1 â‰¥0.
7.10.13. (a)
If Mi is ni Ã— ni with rank (Mi) = ri, then Bi is ni Ã— ri and Ci is
ri Ã— ni with rank (Bi) = rank (Ci) = ri. This means that Mi+1 = CiBi is
ri Ã— ri, so if ri < ni, then Mi+1 has smaller size than Mi. Since this canâ€™t
happen indeï¬nitely, there must be a point in the process at which rk = nk or
rk = 0 and thus some Mk is either nonsingular or zero.
(b)
Let M = M1 = A âˆ’Î»I, and notice that
M2 = B1C1B1C1 = B1M2C1,
M3 = B1C1B1C1B1C1 = B1(B2C2)(B2C2)C1 = B1B2M3C2C1,
...
Mi = B1B2 Â· Â· Â· Biâˆ’1MiCiâˆ’1 Â· Â· Â· C2C1.
In general, itâ€™s true that rank (XYZ) = rank (Y) whenever X has full col-
umn rank and Z has full row rank (Exercise 4.5.12, p. 220), so applying this
yields rank

Mi	
= rank (Mi) for each i = 1, 2, . . . . Suppose that some
Mi = Ciâˆ’1Biâˆ’1 is ni Ã— ni and nonsingular. For this to happen, we must have
Miâˆ’1 = Biâˆ’1Ciâˆ’1, where Biâˆ’1 is niâˆ’1 Ã— ni, Ciâˆ’1 is ni Ã— niâˆ’1, and
rank (Miâˆ’1) = rank (Biâˆ’1) = rank (Ciâˆ’1) = ni = rank (Mi).
Therefore, if k is the smallest positive integer such that Mâˆ’1
k
exists, then k
is the smallest positive integer such that rank (Mkâˆ’1) = rank (Mk), and thus
k is the smallest positive integer such that rank

Mkâˆ’1	
= rank

Mk	
, which
means that index (M) = k âˆ’1 or, equivalently, index (Î») = k âˆ’1. On the other
hand, if some Mi = 0, then rank

Mi	
= rank (Mi) insures that Mi = 0.

164
Solutions
Consequently, if k is the smallest positive integer such that Mk = 0, then k
is the smallest positive integer such that Mk = 0. Therefore, M is nilpotent of
index k, and this implies that index (Î») = k.
7.10.14. M = A âˆ’4I =
ï£«
ï£­
âˆ’7
âˆ’8
âˆ’9
5
7
9
âˆ’1
âˆ’2
âˆ’3
ï£¶
ï£¸âˆ’â†’
ï£«
ï£­
1
0
âˆ’1
0
1
2
0
0
0
ï£¶
ï£¸â‡’B1 =
ï£«
ï£­
âˆ’7
âˆ’8
5
7
âˆ’1
âˆ’2
ï£¶
ï£¸
and C1 =

1
0
âˆ’1
0
1
2

, so M2 = C1B1 =

âˆ’6
âˆ’6
3
3

âˆ’â†’

1
1
0
0

â‡’
B2 =

âˆ’6
3

and C2 =

1
1
	
, so M3 = C2B2 = âˆ’3. Since M3 is the ï¬rst
Mi to be nonsingular, index (4) = 3 âˆ’1 = 2. Now, index (1) if forced to be 1
because 1 = alg mult (1) â‰¥index (1) â‰¥1.
7.10.15. (a)
Since Ïƒ (A) = {1, 4} with index (1) = 1 and index (4) = 2, the Jordan
form for A is J =
ï£«
ï£­
1
0
0
0
4
1
0
0
4
ï£¶
ï£¸.
(b)
The Hermite interpolation polynomial p(z) = Î±0+Î±1z+Î±2z2 is determined
by solving p(1) = f(1), p(4) = f(4), and pâ€²(4) = f â€²(4) for Î±i â€™s. So
ï£«
ï£­
1
1
1
1
4
16
0
1
8
ï£¶
ï£¸
ï£«
ï£­
Î±0
Î±1
Î±2
ï£¶
ï£¸=
ï£«
ï£­
f(1)
f(4)
f â€²(4)
ï£¶
ï£¸
=â‡’
ï£«
ï£­
Î±0
Î±1
Î±2
ï£¶
ï£¸=
ï£«
ï£­
1
1
1
1
4
16
0
1
8
ï£¶
ï£¸
âˆ’1 ï£«
ï£­
f(1)
f(4)
f â€²(4)
ï£¶
ï£¸
= âˆ’1
9
ï£«
ï£­
âˆ’16
7
âˆ’12
8
âˆ’8
15
âˆ’1
1
âˆ’3
ï£¶
ï£¸
ï£«
ï£­
f(1)
f(4)
f â€²(4)
ï£¶
ï£¸
= âˆ’1
9
ï£«
ï£­
âˆ’16f(1) + 7f(4) âˆ’12f â€²(4)
8f(1) âˆ’8f(4) + 15f â€²(4)
âˆ’f(1) + f(4) âˆ’3f â€²(4)
ï£¶
ï£¸.
Writing f(A) = p(A) produces
f(A) =
âˆ’16I + 8A âˆ’A2
âˆ’9

f(1) +
7I âˆ’8A + A2
âˆ’9

f(4)
+
âˆ’12I + 15A âˆ’3A2
âˆ’9

f â€²(4).
7.10.16. Suppose that limkâ†’âˆAk exists and is nonzero. It follows from (7.10.33) that
Î» = 1 is a semisimple eigenvalue of A, so the Jordan form for B looks like
B = I âˆ’A = P

0
0
0
I âˆ’K

Pâˆ’1, where I âˆ’K is nonsingular. Therefore, B
belongs to a matrix group and
B# = P

0
0
0
(I âˆ’K)âˆ’1

Pâˆ’1
=â‡’
I âˆ’BB# = P

I
0
0
0

Pâˆ’1.

Solutions
165
Comparing I âˆ’BB# with (7.10.32) shows that limkâ†’âˆAk = I âˆ’BB#. If
limkâ†’âˆAk = 0, then Ï(A) < 1, and hence B is nonsingular, so B# = Bâˆ’1
and I âˆ’BB# = 0. In other words, itâ€™s still true that limkâ†’âˆAk = I âˆ’BB#.
7.10.17. We already know from the development of (7.10.41) that if rank (M) = r, then
CB and Vâˆ—
1U1 are r Ã— r nonsingular matrices. Itâ€™s a matter of simple algebra
to verify that MM#M = M, M#MM# = M#, and MM# = M#M.
Solutions for exercises in section 7. 11
7.11.1. m(x) = x2 âˆ’3x + 2
7.11.2. v(x) = x âˆ’2
7.11.3. c(x) = (x âˆ’1)(x âˆ’2)2
7.11.4. J =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
Î»
Î»
Î»
Âµ
Âµ
Âµ
1
Âµ
ï£¶
ï£·
ï£·
ï£·
ï£¸
7.11.5. Set Î½0 = âˆ¥Iâˆ¥F = 2, U0 = I/2, and generate the sequence (7.11.2).
r01 = âŸ¨U0 AâŸ©= 2,
Î½1 = âˆ¥A âˆ’r01U0âˆ¥F =
âˆš
1209,
U1 = A âˆ’r01U0
Î½1
= A âˆ’I
âˆš
1209,
r02 =
.
U0 A2/
= 2,
r12 =
.
U1 A2/
= 2
âˆš
1209,
Î½2 = âˆ¥A2 âˆ’r02U0 âˆ’r12U1âˆ¥F = 0,
so that
R =
 2
2
0
âˆš
1209

,
c =

2
2
âˆš
1209

, and Râˆ’1c =
 âˆ’1
2

=
 Î±0
Î±1

.
Consequently, the minimum polynomial is m(x) = x2 âˆ’2x + 1 = (x âˆ’1)2. As a
by-product, we see that Î» = 1 is the only eigenvalue of A, and index (Î») = 2,
so the Jordan form for A must be J =
ï£«
ï£¬
ï£­
1
1
0
0
0
1
0
0
0
0
1
0
0
0
0
1
ï£¶
ï£·
ï£¸.
7.11.6. Similar matrices have the same minimum polynomial because similar matrices
have the same Jordan form, and hence they have the same eigenvalues with the
same indicies.
7.11.10. x = (3, âˆ’1, âˆ’1)T
7.11.12. x = (âˆ’3, 6, 5)T

Solutions for Chapter 8
Solutions for exercises in section 8. 2
8.2.1. The eigenvalues are Ïƒ (A) = {12, 6} with alg multA (6) = 2, and itâ€™s clear that
12 = Ï(A) âˆˆÏƒ (A) . The eigenspace N(Aâˆ’12I) is spanned by e = (1, 1, 1)T , so
the Perron vector is p = (1/3)(1, 1, 1)T . The left-hand eigenspace N(AT âˆ’12I)
is spanned by (1, 2, 3)T , so the left-hand Perron vector is qT = (1/6)(1, 2, 3).
8.2.3. If p1 and p2 are two vectors satisfying Ap = Ï (A) p, p > 0, and âˆ¥pâˆ¥1 = 1,
then dim N (A âˆ’Ï (A) I) = 1 implies that p1 = Î±p2 for some Î± < 0. But
âˆ¥p1âˆ¥1 = âˆ¥p2âˆ¥1 = 1 insures that Î± = 1.
8.2.4. Ïƒ (A) = {0, 1}, so Ï (A) = 1 is the Perron root, and the Perron vector is
p = (Î± + Î²)âˆ’1(Î², Î±).
8.2.5. (a)
Ï(A/r) = 1 is a simple eigenvalue of A/r, and itâ€™s the only eigenvalue on
the spectral circle of A/r, so (7.10.33) on p. 630 guarantees that limkâ†’âˆ(A/r)k
exists.
(b)
This follows from (7.10.34) on p. 630.
(c)
G is the spectral projector associated with the simple eigenvalue Î» = r,
so formula (7.2.12) on p. 518 applies.
8.2.6. If e is the column of all 1 â€™s, then Ae = Ïe. Since e > 0, it must be a positive
multiple of the Perron vector p, and hence p = nâˆ’1e. Therefore, Ap = Ïp
implies that Ï = Ï (A) . The result for column sums follows by considering AT .
8.2.7. Since Ï = maxi

j aij is the largest row sum of A, there must exist a matrix
E â‰¥0 such that every row sum of B = A + E is Ï. Use Example 7.10.2
(p. 619) together with Exercise 8.2.7 to obtain Ï (A) â‰¤Ï (B) = Ï. The lower
bound follows from the Collatzâ€“Wielandt formula. If e is the column of ones,
then e âˆˆN, so
Ï (A) = max
xâˆˆN f(x) â‰¥f(e) = min
1â‰¤iâ‰¤n
[Ae]i
ei
= min
i
n

j=1
aij.
8.2.8. (a), (b), (c), and (d) are illustrated by using the nilpotent matrix A =

0
1
0
0

.
(e)
A =

0
1
1
0

has eigenvalues Â±1.
8.2.9. If Î¾ = g(x) for x âˆˆP, then Î¾x â‰¥Ax > 0. Let p and qT be the respective
the right-hand and left-hand Perron vectors for A associated with the Perron
root r, and use (8.2.3) along with qT x > 0 to write
Î¾x â‰¥Ax > 0
=â‡’
Î¾qT x â‰¥qT Ax = rqT x
=â‡’
Î¾ â‰¥r,

168
Solutions
so g(x) â‰¥r for all x âˆˆP. Since g(p) = r and p âˆˆP, it follows that
r = minxâˆˆP g(x).
8.2.10. A =
 1
2
2
4

=â‡’
Ï(A) = 5, but g(e1) = 1
=â‡’
minxâˆˆN g(x) < Ï(A).
Solutions for exercises in section 8. 3
8.3.1. (a)
The graph is strongly connected.
(b)
Ï (A) = 3, and p = (1/6, 1/2, 1/3)T .
(c)
h = 2 because A is imprimitive and singular.
8.3.2. If A is nonsingular then there are either one or two distinct nonzero eigenvalues
inside the spectral circle. But this is impossible because Ïƒ (A) has to be invariant
under rotations of 120â—¦by the result on p. 677. Similarly, if A is singular with
alg multA (0) = 1, then there is a single nonzero eigenvalue inside the spectral
circle, which is impossible.
8.3.3. No! The matrix A =

1
1
0
2

has Ï (A) = 2 with a corresponding eigenvector
e = (1, 1)T , but A is reducible.
8.3.4. Pn is nonnegative and irreducible (its graph is strongly connected), and Pn
is imprimitive because Pn
n = I insures that every power has zero entries. Fur-
thermore, if Î» âˆˆÏƒ (Pn) , then Î»n âˆˆÏƒ(Pn
n) = {1}, so all eigenvalues of Pn
are roots of unity. Since all eigenvalues on the spectral circle are simple (re-
call (8.3.13) on p. 676) and uniformly distributed, it must be the case that
Ïƒ (Pn) = {1, Ï‰, Ï‰2, . . . , Ï‰nâˆ’1}.
8.3.5. A is irreducible because the graph G(A) is strongly connectedâ€”every node is
accessible by some sequence of paths from every other node.
8.3.6. A is imprimitive. This is easily seen by observing that each A2n for n > 1 has
the same zero pattern (and each A2n+1 for n > 0 has the same zero pattern),
so every power of A has zero entries.
8.3.7. (a)
Having row sums less than or equal to 1 means that âˆ¥Pâˆ¥âˆâ‰¤1. Because
Ï (â‹†) â‰¤âˆ¥â‹†âˆ¥for every matrix norm (recall (7.1.12) on p. 497), it follows that
Ï (S) â‰¤âˆ¥Sâˆ¥1 â‰¤1.
(b)
If e denotes the column of all 1â€™s, then the hypothesis insures that Se â‰¤e,
and Se Ì¸= e. Since S is irreducible, the result in Example 8.3.1 (p. 674) implies
that itâ€™s impossible to have Ï (S) = 1 (otherwise Se = e), and therefore Ï (S) <
1 by part (a).
8.3.8. If p is the Perron vector for A, and if e is the column of 1 â€™s, then
Dâˆ’1ADe = Dâˆ’1Ap = rDâˆ’1p = re
shows that every row sum of Dâˆ’1AD is r, so we can take P = râˆ’1Dâˆ’1AD
because the Perronâ€“Frobenius theorem guarantees that r > 0.
8.3.9. Construct the Boolean matrices as described in Example 8.3.5 (p. 680), and show
that B9 has a zero in the (1, 1) position, but B10 > 0.

Solutions
169
8.3.10. According to the discussion on p. 630, f(t) â†’0 if r < 1. If r = 1, then
f(t) â†’Gf(0) = p

qT f(0)/qT p

> 0, and if r > 1, the results of the Leslie
analysis imply that fk(t) â†’âˆfor each k.
8.3.11. The only nonzero coeï¬ƒcient in the characteristic equation for L is c1, so
gcd{2, 3, . . . , n} = 1.
8.3.12. (a)
Suppose that A is essentially positive. Since we can always ï¬nd a Î² > 0
such that Î²I + diag (a11, a22, . . . , ann) â‰¥0, and since aij â‰¥0 for i Ì¸= j, it
follows that A + Î²I is a nonnegative irreducible matrix, so (8.3.5) on p. 672
can be applied to conclude that (A + (1 + Î²)I)nâˆ’1 > 0, and thus A + Î±I is
primitive with Î± = Î² +1. Conversely, if A+Î±I is primitive, then A+Î±I must
be nonnegative and irreducible, and hence aij â‰¥0 for every i Ì¸= j, and A must
be irreducible (diagonal entries donâ€™t aï¬€ect the reducibility or irreducibility).
(b)
If A is essentially positive, then A + Î±I is primitive for some Î± (by the
ï¬rst part), so (A + Î±I)k > 0 for some k. Consequently, for all t > 0,
0 <
âˆ

k=0
tk(A + Î±I)k
k!
= et(A+Î±I) = etAetÎ±I = B
=â‡’
0 < eâˆ’Î±tB = etA.
Conversely, if 0 < etA = âˆ
k=0 tkAk/k! for all t > 0, then aij â‰¥0 for every
i Ì¸= j, for if aij < 0 for some i Ì¸= j, then there exists a suï¬ƒciently small t > 0
such that [I + tA + t2A2/2 + Â· Â· Â·]ij < 0, which is impossible. Furthermore, A
must be irreducible; otherwise
A âˆ¼

X
Y
0
Z

=â‡’
etA =
âˆ

k=0
tkAk/k! âˆ¼

â‹†
â‹†
0
â‹†

,
which is impossible.
8.3.13. (a)
Being essentially positive implies that there exists some Î± âˆˆâ„œsuch that
A+Î±I is nonnegative and irreducible (by Exercise 8.3.12). If (r, x) is the Perron
eigenpair for A + Î±I, then for Î¾ = r âˆ’Î±, (Î¾, x) is an eigenpair for A.
(b)
Every eigenvalue of A + Î±I has the form z = Î» + Î±, where Î» âˆˆÏƒ (A) ,
so if r is the Perron root of A + Î±I, then for z Ì¸= r,
|z| < r
=â‡’
Re (z) < r
=â‡’
Re (Î» + Î±) < r
=â‡’
Re (Î») < r âˆ’Î± = Î¾.
(c)
If A â‰¤B, then A + Î±I â‰¤B + Î±I, so Wielandtâ€™s theorem (p. 675) insures
that rA = Ï (A + Î±I) â‰¤Ï (B + Î±I) = rB, and hence Î¾A = rA âˆ’Î± â‰¤rB âˆ’Î± = Î¾B.
8.3.14. If A is primitive with r = Ï (A) , then, by (8.3.10) on p. 674,
A
r
k
â†’G > 0
=â‡’
âˆƒk0 such that
A
r
m
> 0
âˆ€m â‰¥k0
=â‡’
a(m)
ij
rm > 0
âˆ€m â‰¥k0
=â‡’
lim
mâ†’âˆ

a(m)
ij
rm
	1/m
â†’1
=â‡’
lim
mâ†’âˆ

a(m)
ij
1/m
= r.

170
Solutions
Conversely, we know from the Perronâ€“Frobenius theorem that r > 0, so if
limkâ†’âˆ

a(k)
ij
1/k
= r, then âˆƒk0 such that âˆ€m â‰¥k0,

a(m)
ij
1/m
> 0, which
implies that Am > 0, and thus A is primitive by Frobeniusâ€™s test (p. 678).
Solutions for exercises in section 8. 4
8.4.1. The left-hand Perron vector for P is Ï€T = (10/59, 4/59, 18/59, 27/59). Itâ€™s
the limiting distribution in the regular sense because P is primitive (it has a
positive diagonal entryâ€”recall Example 8.3.3 (p. 678)).
8.4.2. The left-hand Perron vector is Ï€T = (1/n)(1, 1, . . . , 1). Thus the limiting dis-
tribution is the uniform distribution, and in the long run, each state is occupied
an equal proportion of the time. The limiting matrix is G = (1/n)eeT .
8.4.3. If P is irreducible, then Ï (P) = 1 is a simple eigenvalue for P, so
rank (I âˆ’P) = nâˆ’dim N (I âˆ’P) = nâˆ’geo multP (1) = nâˆ’alg multP (1) = nâˆ’1.
8.4.4. Let A = Iâˆ’P, and recall that rank (A) = nâˆ’1 (Exercise 8.4.3). Consequently,
A singular =â‡’A[adj (A)] = 0 = [adj (A)]A
(Exercise 6.2.8, p. 484),
and
rank (A) = n âˆ’1 =â‡’rank (adj (A)) = 1
(Exercises 6.2.11).
It follows from A[adj (A)] = 0 and the Perronâ€“Frobenius theorem that each col-
umn of [adj (A)] must be a multiple of e (the column of 1 â€™s or, equivalently,
the right-hand Perron vector for P), so [adj (A)] = evT for some vector v.
But [adj (A)]ii = Pi forces vT = (P1, P2, . . . , Pn). Similarly, [adj (A)]A = 0
insures that each row in [adj (A)] is a multiple of Ï€T (the left-hand Perron vec-
tor of P), and hence vT = Î±Ï€T for some Î±. This scalar Î± canâ€™t be zero; other-
wise [adj (A)] = 0, which is impossible because rank (adj (A)) = 1. Therefore,
vT e = Î± Ì¸= 0, and vT /(vT e) = vT /Î± = Ï€T .
8.4.5. If QkÃ—k (1 â‰¤k < n) is a principal submatrix of P, then there is a permutation
matrix H such that HT PH =

Q
X
Y
Z

= P. If B =

Q
0
0
0

, then
B â‰¤P, and we know from Wielandtâ€™s theorem (p. 675) that Ï (B) â‰¤Ï

P

= 1,
and if Ï (B) = Ï

P

= 1, then there is a number Ï† and a nonsingular diagonal
matrix D such that B = eiÏ†DPDâˆ’1 or, equivalently, P = eâˆ’iÏ†DBDâˆ’1. But
this implies that X = 0, Y = 0, and Z = 0, which is impossible because P
is irreducible. Therefore, Ï (B) < 1, and thus Ï (Q) < 1.
8.4.6. In order for I âˆ’Q to be an M-matrix, it must be the case that [I âˆ’Q]ij â‰¤0
for i Ì¸= j, and I âˆ’Q must be nonsingular with (I âˆ’Q)âˆ’1 â‰¥0. Itâ€™s clear that
[I âˆ’Q]ij â‰¤0 because 0 â‰¤qij â‰¤1. Exercise 8.4.5 says that Ï (Q) < 1, so

Solutions
171
the Neumann series expansion (p. 618) insures that I âˆ’Q is nonsingular and
(I âˆ’Q)âˆ’1 = âˆ
j=1 Qj â‰¥0. Thus I âˆ’Q is an M-matrix.
8.4.7. We know from Exercise 8.4.6 that every principal submatrix of order 1 â‰¤k <
n is an M-matrix, and M-matrices have positive determinants by (7.10.28) on
p. 626.
8.4.8. You can consider an absorbing chain with eight states
{(1, 1, 1), (1, 1, 0), (1, 0, 1), (0, 1, 1), (1, 0, 0), (0, 1, 0), (0, 0, 1), (0, 0, 0)}
similar to what was described in Example 8.4.5, or you can use a four-state
chain in which the states are deï¬ned to be the number of controls that hold at
each activation of the system. Using the eight-state chain yields the following
mean-time-to-failure vector.
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
(1, 1, 1)
368.4
(1, 1, 0)
366.6
(1, 0, 1)
366.6
(0, 1, 1)
366.6
(1, 0, 0)
361.3
(0, 1, 0)
361.3
(0, 0, 1)
361.3
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
= (I âˆ’T11)âˆ’1e.
8.4.9. This is a Markov chain with nine states (c, m) in which c is the chamber
occupied by the cat, and m is the chamber occupied by the mouse. There are
three absorbing statesâ€”namely (1, 1), (2, 2), (3, 3). The transition matrix is
P = 1
72
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
(1, 2)
(1, 3)
(2, 1)
(2, 3)
(3, 1)
(3, 2)
(1, 1)
(2, 2)
(3, 3)
(1, 2)
18
12
3
6
3
9
6
9
6
(1, 3)
12
18
3
9
3
6
6
6
9
(2, 1)
3
3
18
9
12
6
6
9
6
(2, 3)
4
6
6
18
4
8
2
12
12
(3, 1)
3
3
12
6
18
9
6
6
9
(3, 2)
6
4
4
8
6
18
2
12
12
(1, 1)
0
0
0
0
0
0
72
0
0
(2, 2)
0
0
0
0
0
0
0
72
0
(3, 3)
0
0
0
0
0
0
0
0
72
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
The expected number of steps until absorption and absorption probabilities are
(I âˆ’T11)âˆ’1e=
ï£«
ï£¬
ï£¬
ï£¬
ï£­
(1, 2)
3.24
(1, 3)
3.24
(2, 1)
3.24
(2, 3)
2.97
(3, 1)
3.24
(3, 2)
2.97
ï£¶
ï£·
ï£·
ï£·
ï£¸
and
(I âˆ’T11)âˆ’1T12=
ï£«
ï£¬
ï£¬
ï£¬
ï£­
(1, 1)
(2, 2)
(3, 3)
0.226
0.41
0.364
0.226
0.364
0.41
0.226
0.41
0.364
0.142
0.429
0.429
0.226
0.364
0.41
0.142
0.429
0.429
ï£¶
ï£·
ï£·
ï£·
ï£¸

Index
A
absolute uncertainty or error,
414
absorbing Markov chains,
700
absorbing states,
700
addition, properties of,
82
additive identity,
82
additive inverse,
82
adjacency matrix,
100
adjoint,
84, 479
adjugate,
479
aï¬ƒne functions,
89
aï¬ƒne space,
436
algebraic group,
402
algebraic multiplicity,
496, 510
amplitude,
362
Anderson-Duï¬ƒn formula,
441
Anderson, Jean,
xii
Anderson, W. N., Jr.,
441
angle,
295
canonical,
455
between complementary spaces,
389, 450
maximal,
455
principal,
456
between subspaces,
450
annihilating polynomials,
642
aperiodic Markov chain,
694
Arnoldiâ€™s algorithm,
653
Arnoldi, Walter Edwin,
653
associative property
of addition,
82
of matrix multiplication,
105
of scalar multiplication,
83
asymptotic rate of convergence,
621
augmented matrix,
7
Autonne, L.,
411
B
back substitution,
6, 9
backward error analysis,
26
backward triangle inequality,
273
band matrix,
157
Bartlett, M. S.,
124
base-b,
375
basic columns,
45, 61, 178, 218
combinations of,
54
basic variables,
58, 61
in nonhomogeneous systems,
70
basis,
194, 196
change of,
253
characterizations,
195
orthonormal,
355
basis for
direct sum,
383
intersection of spaces,
211
space of linear transformations,
241
Bauerâ€“Fike bound,
528
beads on a string,
559
Bellman, Richard,
xii
Beltrami, Eugenio,
411
Benzi, Michele,
xii
Bernoulli, Daniel,
299
Bessel, Friedrich W.,
305
Besselâ€™s inequality,
305
best approximate inverse,
428
biased estimator,
446
binary representation,
372
Birkhoï¬€, Garrett,
625
Birkhoï¬€â€™s theorem,
702
bit reversal,
372
bit reversing permutation matrix,
381
block diagonal,
261â€“263
rank of,
137
using real arithmetic,
524
block matrices,
111
determinant of,
475
linear operators,
392
block matrix multiplication,
111
block triangular,
112, 261â€“263
determinants,
467
eigenvalues of,
501
Bolzanoâ€“Weierstrass theorem,
670
Boolean matrix,
679
bordered matrix,
485
eigenvalues of,
552
branch,
73, 204
Brauer, Alfred,
497
Bunyakovskii, Victor,
271
C
cancellation law,
97
canonical angles,
455
canonical form, reducible matrix,
695
Cantor, Georg,
597
Cauchy, Augustin-Louis,
271
determinant formula,
484
integral formula,
611
Cauchyâ€“Bunyakovskiiâ€“Schwarz inequality,
272
Cauchyâ€“Goursat theorem,
615
Cauchyâ€“Schwarz inequality,
287
Cayley, Arthur,
80, 93, 158, 460
Cayleyâ€“Hamilton theorem,
509, 532, 597
to determine f(A) ,
614
Cayley transformation,
336, 556
CBS inequality,
272, 277, 473
general form,
287
centered diï¬€erence approximations,
19
Ces`aro, Ernesto,
630
Ces`aro sequence,
630
Ces`aro summability,
630, 633, 677
for stochastic matrix,
697
chain, Jordan,
575
change of basis,
251, 253, 258

706
Index
change of coordinates,
252
characteristic equation,
491, 492
coeï¬ƒcients,
494, 504
characteristic polynomial,
491, 492
of a product,
503
characteristic values and vectors,
490
Chebyshev, Pafnuty Lvovich,
40, 687
checking an answer,
35, 416
Cholesky, Andre-Louis,
154
Cholesky factorization,
154, 314, 558, 559
Cimmino, Gianfranco,
445
Cimminoâ€™s reï¬‚ection method,
445
circuit,
204
circulant matrix,
379
with convolution,
380
eigenvalues, eigenvectors,
523
classical Gramâ€“Schmidt algorithm,
309
classical least squares,
226
clock cycles,
539, 694
closest point
to an aï¬ƒne space,
436
with Fourier expansion,
440
theorem,
435
closure property
of addition,
82, 160
of multiplication, 83, 160
coeï¬ƒcient matrix,
7, 42
coeï¬ƒcient of linear correlation,
297
cofactor,
477, 487
expansion,
478, 481
Collatz, Lothar,
666
Collatzâ€“Wielandt formula,
666, 673, 686
column,
7
equivalence,
134
and nullspace,
177
operations,
14, 134
rank,
198
relationships,
50, 136
scaling,
27
space,
170, 171, 178
spanning set for,
172
vector,
8, 81
Comdico, David,
xii
commutative law,
97
commutative property of addition,
82
commuting matrices, eigenvectors,
503, 522
companion matrix,
648
compatibility of norms,
285
compatible norms,
279, 280
competing species model,
546
complementary projector,
386
complementary subspaces,
383, 403
angle between,
389, 450
complete pivoting,
28
numerical stability,
349
complete set of eigenvectors,
507
complex conjugate,
83
complex exponential,
362, 544
complex numbers, the set of,
81
component matrices,
604
component vectors,
384
composition
of linear functions,
93
of linear transformations,
245, 246
of matrix functions,
608, 615
computer graphics,
328, 330
condition number
for eigenvalues,
528
generalized,
426
for matrices, 127, 128, 414, 415
condition of
eigenvalues, hermitian matrices,
552
linear system,
128
conditioning and pivots,
426
conformable,
96
conformably partitioned,
111
congruence transformation,
568
conjugate, complex,
83
conjugate gradient algorithm,
657
conjugate matrix,
84
conjugate transpose,
84
reverse order law,
109
conjugate vectors,
657
connected graph,
202
connectivity and linear dependence,
208
connectivity matrix,
100
consistent system,
53, 54
constituent matrices,
604
continuity
of eigenvalues,
497
of inversion,
480
of norms,
277
continuous Fourier transform,
357
continuous functions, max and min,
276
convergence,
276, 277
convergent matrix,
631
converse of a statement,
54
convolution
with circulants,
380
deï¬nition,
366
operation count,
377
theorem,
367, 368, 377
Cooley, J. W.,
368, 375, 651
cooperating species model,
546
coordinate matrix,
242
coordinates,
207, 240, 299
change of,
252
of a vector,
240
coordinate spaces,
161
core-nilpotent decomposition,
397
correlation,
296
correlation coeï¬ƒcient,
297
cosine
of angle,
295
minimal angle,
450
discrete,
361

Index
707
Courantâ€“Fischer theorem,
550
alternate,
557
for singular values,
555
Courant, Richard,
550
covariance,
447
Cramer, Gabriel,
476
Cramerâ€™s rule,
459, 476
critical point,
570
cross product,
332, 339
Cuomo, Kelly,
xii
curve ï¬tting,
186, 229
D
defective,
507
deï¬cient,
496, 507
deï¬nite matrices,
559
deï¬‚ation, eigenvalue problems,
516
dense matrix,
350
dependent set,
181
derivative
of a determinant,
471, 474, 486
of a linear system,
130
of a matrix,
103, 226
operator,
245
determinant,
461
computing,
470
of a product,
467
as product of eigenvalues,
494
of a sum,
485
and volume,
468
deviation from symmetry,
436
diagonal dominance,
639
diagonal matrix,
85
eigenvalues of,
501
inverse of,
122
diagonalizability,
507
being arbitrarily close to,
533
in terms of minimum polynomial,
645
in terms of multiplicities,
512
summary,
520
diagonalization
of circulants,
379
Jacobiâ€™s method,
353
of normal matrices,
547
simultaneous,
522
diagonally dominant,
184, 499, 622, 623, 639
systems,
193
diï¬€erence equations,
515, 616
diï¬€erence of matrices,
82
diï¬€erence of projectors,
393
diï¬€erential equations,
489, 541, 542
independent solutions,
481
nonhomogeneous,
609
solution of,
546
stability,
544, 609
systems,
608
uncoupling,
559
diï¬€usion equation,
563
diï¬€usion model,
542
dimension,
196
of direct sum,
383
of fundamental subspaces,
199
of left-hand nullspace,
218
of nullspace,
218
of orthogonal complement,
339
of range,
218
of row space,
218
of space of linear transformations,
241
of subspace,
198
of sum,
205
direct product,
380, 597
direct sum,
383
of linear operators,
399
of several subspaces,
392
of symmetric and skew-symmetric matrices,
391
directed distance between subspaces,
453
directed graph,
202
Dirichlet, Johann P. G. L,
563, 597
Dirichlet problem,
563
discrete Fourier transform,
356, 358
discrete Laplacian,
563
eigenvalues of,
598
discrete sine, cosine, and exponential,
361
disjoint subspaces,
383
distance,
271
to lower-rank matrices,
417
between subspaces,
450
to symmetric matrices,
436
between a vector and a subspace,
435
distinct eigenvalues,
514
distributions,
532
distributive property
of matrix multiplication,
105
of scalar multiplication,
83
domain,
89
doubly stochastic,
702
Drazin generalized inverse,
399, 401, 422, 640
Cauchy formula,
615
integral representation,
441
Drazin, M. P.,
399
Duï¬ƒn, R. J.,
441
Duncan, W. J.,
124
E
Eckart, C.,
411
economic inputâ€“output model,
681
edge matrix,
331
edges,
202
eigenpair,
490
eigenspace,
490

708
Index
eigenvalues,
266, 410, 490
bounds for,
498
continuity of,
497
determinant and trace,
494
distinct,
514
generalized,
571
index of,
401, 587, 596
perturbations and condition of,
528, 551
semisimple,
596
sensitivity, hermitian matrices,
552
unit,
696
eigenvalues of
bordered matrices,
552
discrete Laplacian,
566, 598
triangular and diagonal matrices,
501
tridiagonal Toeplitz matrices,
514
eigenvectors,
266, 490
of commuting matrices,
503
generalized,
593, 594
independent,
511
of tridiagonal Toeplitz matrices,
514
electrical circuits,
73, 204
elementary matrices,
131â€“133
interchange matrices,
135, 140
elementary orthogonal projector,
322, 431
rank of,
337
elementary reï¬‚ector,
324, 444
determinant of,
485
elementary row and column operations,
4, 8
and determinants,
463
elementary triangular matrix,
142
ellipsoid,
414
degenerate,
425
elliptical inner product,
286
elliptical norm,
288
EP matrices,
408
equal matrices,
81
equivalence, row and column,
134
testing for,
137
equivalent norms
matrices,
425
vectors,
276
equivalent statements,
54
equivalent systems,
3
ergodic class,
695
error, absolute and relative,
414
essentially positive matrix,
686
estimators,
446
euclidean norm,
270
unitary invariance of,
321
evolutionary processes,
616
exponential
complex,
544
discrete,
361
matrix,
441, 525
inverse of,
614
products of,
539
sums of,
614
extending to a basis,
201
extending to an orthonormal basis,
325, 335, 338, 404
extension set,
188
F
Faddeev and Sominskii,
504
fail-safe system,
701
fast Fourier transform (FFT),
368
FFT algorithm,
368, 370, 373, 381, 651
FFT operation count,
377
fast integer multiplication,
375
ï¬ltering random noise,
418
ï¬nite diï¬€erence matrix,
522, 639
ï¬nite-dimensional spaces,
195
ï¬nite group,
676
Fischer, Ernst,
550
ï¬ve-point diï¬€erence equations,
564
ï¬xed points,
386, 391
of a reï¬‚ector,
338
ï¬‚atness,
164
ï¬‚oating-point number,
21
forward substitution,
145
four fundamental subspaces,
169
summary,
178
Fourier coeï¬ƒcients,
299
Fourier expansion,
299
and projection,
440
Fourier, Jean Baptiste Joseph,
299
Fourier matrix,
357
Fourier series,
299, 300
Fourier transform
continuous,
357
discrete,
356, 358
Frame, J. S.,
504
Francis, J. F. G.,
535
FrÂ´echet, Maurice, R.,
289
free variables,
58, 61
in nonhomogeneous systems,
70
frequency,
362
frequency domain,
363
Frobenius, Ferdinand Georg,
44, 123, 215, 662
Frobenius form,
680
Frobenius inequality,
221
Frobenius matrix norm,
279, 425, 428
and inner product,
288
of rank-one matrices,
391
unitary invariance of,
337
Frobenius test for primitivity,
678
full-rank factorization,
140, 221, 633
for determining index,
640
of a projector,
393
function
aï¬ƒne,
89
composition of,
93, 615, 608
domain of,
89
linear,
89, 238
norm of,
288
range of,
89

Index
709
functional matrix identities,
608
functions of
diagonalizable matrices,
526
of Jordan blocks,
600
matrices,
601
using Cauchy integral formula,
611
using Cayleyâ€“Hamilton theorem,
614
nondiagonalizable matrices,
603
fundamental (normal) mode of vibration,
562
fundamental problem of matrix theory,
506
fundamental subspaces,
169
dimension of,
199
orthonormal bases for,
407
projector onto,
434
fundamental theorem of algebra,
185, 492
fundamental theorem of linear algebra,
405
G
gap,
453, 454
Gauss, Carl F.,
ix, 2, 93, 234, 488
as a teacher,
353
Gaussian elimination,
2, 3
and LU factorization,
141
eï¬€ects of roundoï¬€,
129
modiï¬ed,
43
numerical stability,
348
operation counts,
10
Gaussian transformation,
341
Gaussâ€“Jordan method,
15, 47, 48
for computing a matrix inverse,
118
operation counts,
16
Gaussâ€“Markov theorem,
229, 448
Gaussâ€“Seidel method,
622
general solution
algebraic equations
homogeneous systems,
59, 61,
nonhomogeneous systems,
64, 66, 70, 180, 221
diï¬€erence equations,
616
diï¬€erential equations,
541, 609
generalized condition number,
426
generalized eigenvalue problem,
571
generalized eigenvectors,
593, 594
generalized inverse,
221, 393, 422, 615
Drazin,
399
group,
402
and orthogonal projectors,
434
generalized minimal residual (GMRES),
655
genes and chromosomes,
543
geometric multiplicity,
510
geometric series,
126, 527, 618
Gerschgorin circles,
498
Gerschgorin, S. A.,
497
Givens reduction,
344
and determinants,
485
numerical stability,
349
Givens rotations,
333
Givens, Wallace,
333
GMRES,
655
Golub, Gene H.,
xii
gradient,
570
Gram, Jorgen P.,
307
Gram matrix,
307
Gramâ€“Schmidt algorithm
classical version,
309
implementations of,
319
and minimum polynomial,
643
modiï¬ed version,
316
numerical stability of,
349
and volume,
431
Gramâ€“Schmidt process,
345
Gramâ€“Schmidt sequence,
308, 309
graph,
202
of a matrix,
209, 671
graphics, 3-D rotations,
328, 330
Grassmann, Hermann G.,
160
Graybill, Franklin A.,
xii
grid norm,
274
grid points,
18
group, ï¬nite,
676
group inverse,
402, 640, 641
growth in Gaussian elimination,
26
Guttman, L.,
124
H
Hadamard, Jacques,
469, 497
Hadamardâ€™s inequality,
469
Halmos, Paul,
xii, 268
Hamilton, William R.,
509
harmonic functions,
563
Haynsworth, Emilie V.,
123
heat equation,
563
Helfrich, Laura,
xii
Hermite, Charles,
48
Hermite interpolation polynomial,
607
Hermite normal form,
48
Hermite polynomial,
231
hermitian matrix,
85, 409, 410
condition of eigenvalues,
552
eigen components of,
549
Hessenberg matrices
350
QR factorization of,
352
Hessian matrix,
570
Hestenes, Magnus R.,
656
hidden surfaces,
332, 339
Hilbert, David,
307
Hilbert matrix,
14, 31, 39
Hilbertâ€“Schmidt norm,
279
Hohn, Franz,
xii
HÂ¨older, Ludwig O.,
278
HÂ¨olderâ€™s inequality,
274, 277, 278
homogeneous systems,
57, 61
Hooke, Robert,
86
Hookeâ€™s law,
86
Horn, Roger,
xii

710
Index
Horst, Paul,
504
Householder, Alston S.,
324
Householder reduction,
341, 342
and determinants,
485
and fundamental subspaces,
407
numerical stability,
349
Householder transformations,
324
hyperplane,
442
I
idempotent,
113, 258, 339, 386
and projectors,
387
identity matrix,
106
identity operator,
238
ill-conditioned matrix,
127, 128, 415
ill-conditioned system,
33, 535
normal equations,
214
image and image space,
168, 170
dimension of,
208
image of unit sphere,
417
imaginary, pure,
556
imprimitive matrices,
674
maximal root of,
676
spectrum of,
677
test for,
678
imprimitivity, index of,
679, 680
incidence matrix,
202
inconsistent system,
53
independent columns,
218
independent eigenvectors,
511
independent rows,
218
independent set,
181
basic facts,
188
maximal,
186
independent solutions
for algebraic equations,
209
for diï¬€erential equations,
481
index
of an eigenvalue,
401, 587, 596
of imprimitivity,
674, 679, 680
of nilpotency,
396
of a square matrix,
394, 395
by full-rank factorization,
640
induced matrix norm,
280, 389
of Aâˆ’1,
285
elementary properties,
285
of rank-one matrices,
391
unitary invariance of,
337
inertia,
568
inï¬nite-dimensional spaces,
195
inï¬nite series and matrix functions,
527
inï¬nite series of matrices,
605
information retrieval,
419
inner product,
286
geometric interpretation,
431
inputâ€“output economic model,
681
integer matrices,
156, 473, 485
integer multiplication,
375
integral formula
for generalized inverses,
441
for matrix functions,
611
intercept model,
447
interchange matrices,
135, 140
interlacing of eigenvalues,
552
interpolation
formula for f(A),
529
Hermite polynomial,
607
Lagrange polynomial,
186
intersection of subspaces
basis for,
211
projection onto,
441
invariant subspace,
259, 262, 263
inverse Fourier transform,
358
inverse iteration,
534
inverse matrix,
115
best approximation to,
428
Cauchy formula for,
615
computation of,
118
operation counts,
119
continuity of,
480
determinants,
479
eigenvalues of,
501
existence of,
116
generalized,
615
integral representation of,
441
norm of,
285
properties of,
120
of a sum,
220
invertible operators,
246, 250
invertible part of an operator,
399
involutory,
113, 325, 339, 485
irreducible Markov chain, limits,
693
irreducible matrix,
209, 671
isometry,
321
iteration matrix,
620
iterative methods,
620
J
Jacobiâ€™s diagonalization method,
353
Jacobiâ€™s iterative method,
622, 626
Jacobi, Karl G. J.,
353
Johnson, Charlie,
xii
Jordan blocks,
588, 590
functions of,
600
nilpotent,
579
Jordan chains,
210, 401, 575, 576, 593
construction of,
594
Jordan form,
397, 408, 589, 590
for nilpotent matrices,
579
preliminary version,
397
Jordan, Marie Ennemond Camille,
15, 411, 589
Jordan segment,
588, 590
Jordan structure of matrices,
580, 581, 586, 589
uniqueness of,
580

Index
711
Jordan, Wilhelm,
15
K
Kaczmarzâ€™s projection method,
442, 443
Kaczmarz, Stefan,
442
Kaplansky, Irving,
268
Kearn, Vickie,
xi, 12
kernel,
173
Kirchhoï¬€â€™s rules,
73
loop rule,
204
Kline, Morris,
80
Kowa, Seki,
459
Kronecker, Leopold,
597
Kronecker product,
380, 597
and the Laplacian,
573
Krylov, Aleksei Nikolaevich,
645
Krylov
method,
649
sequence,
401
subspaces, sequences, matrices,
646
Kummer, Ernst Eduard,
597
L
Lagrange interpolating polynomial,
186, 230, 233, 529
Lagrange, Joseph-Louis,
186, 572
Lagrange multipliers,
282
Lancaster, Peter,
xii
Lanczos algorithm,
651
Lanczos, Cornelius,
651
Laplaceâ€™s determinant expansion,
487
Laplaceâ€™s equation,
624
Laplace, Pierre-Simon,
81, 307, 487, 572
Laplacian,
563
latent semantic indexing,
419
latent values and vectors,
490
law of cosines,
295
LDU factorization,
154
leading principal minor,
558
leading principal submatrices,
148, 156
least common multiple,
647
least squares,
226, 439
and Gramâ€“Schmidt,
313
and orthogonal projection,
437
and polynomial ï¬tting,
230
and pseudoinverse,
438
and QR factorization,
346
total least squares,
223
why least squares?,
446
LeBlanc, Kathleen,
xii
left-hand eigenvectors,
490, 503, 516, 523, 524
in inverses,
521
and projectors,
518
left-hand nullspace,
174, 178, 199
spanning set for,
176
Legendre, Adrienâ€“Marie,
319, 572
Legendre polynomials,
319
Legendreâ€™s diï¬€erential equation,
319
Leibniz, Gottfried W.,
459
length of a projection,
323
Leontiefâ€™s inputâ€“output model,
681
Leontief, Wassily,
681
Leslie, P. H.,
684
Leslie population model,
683
Leverrierâ€“Souriauâ€“Frame Algorithm,
504
Leverrier, U. J. J.,
504
LÂ´evy, L.,
497
limiting distribution,
531, 636
limits
and group inversion,
640
in Markov chains
irreducible Markov chains,
693
reducible Markov chains,
698
of powers of matrices,
630
and spectral radius,
617
of vector sequences,
639
in vector spaces,
276, 277
Lindemann, Carl Louis Ferdinand von,
662
linear
algebra,
238
combination,
91
correlation,
296, 306
dependence and connectivity,
208
estimation,
446
functions,
89, 238
deï¬ned by matrix multiplication,
106
deï¬ned by systems of equations,
99
models,
448
operators,
238
and block matrices,
392
regression,
227, 446
spaces,
169
stationary iterations,
620
transformation,
238
linearly independent and dependent sets,
181
basic facts,
188
maximal,
186
and rank,
183
linearly independent eigenvectors,
511
lines in â„œn not through the origin,
440
lines, projection onto,
440
long-run distribution,
531
loop,
73
equations,
204
rule,
74
simple,
75
lower triangular,
103
LU factorization,
141, 144
existence of,
149
with interchanges,
148
operation counts,
146
summary,
153

712
Index
M
main diagonal,
41, 85
Markov, Andrei Andreyevich,
687
Markov chains,
532, 638, 687
absorbing,
700
periodic,
694
mass-stiï¬€ness equation,
571
matrices, the set of,
81
matrix,
7
diagonal
85
exponential,
441, 525, 529
and diï¬€erential equations,
541, 546, 608
inverse of,
614
products,
539
sums,
614
functions,
526, 601
as inï¬nite series,
527
as polynomials,
606
group,
402
multiplication,
96
by blocks,
111
as a linear function,
106
properties of,
105
relation to linear transformations,
244
norms,
280
1-norm,
283
2-norm,
281, 425
âˆ-norm,
127, 283
Frobenius norm,
425
induced norm,
285
polynomials,
501
product,
96
representation of a projector,
387
representations,
262
triangular
41
maximal angle,
455
maximal independent set,
218
maximal linearly independent subset,
186, 196
maximum and minimum of continuous functions,
276
McCarthy, Joseph R.,
651
mean,
296, 447
Meyer
Bethany B.,
xii
Carl, Sr.,
xii
Holly F.,
xii
Louise,
xii
Margaret E.,
xii
Martin D.,
xii
min-max theorem,
550
alternate formulation,
557
for singular values,
555
minimal angle,
450
minimal spanning set,
196, 197
minimum norm least squares solution,
438
minimum norm solution,
426
minimum polynomial,
642
determination of,
643
of a vector,
646
minimum variance estimator,
446
Minkowski, Hermann,
184, 278, 497, 626
Minkowski inequality,
278
minor determinant, principal,
559, 466
MINRES algorithm,
656
Mirsky, Leonid,
xii
M-matrix,
626, 639, 682, 703
modern least squares,
437
modiï¬ed gaussian elimination,
43
modiï¬ed Gramâ€“Schmidt algorithm,
316
monic polynomial,
642
Montgomery, Michelle,
xii
Moore, E. H.,
221
Mooreâ€“Penrose generalized inverse,
221, 422, 400
best approximate inverse,
428
integral representation,
441
and orthogonal projectors,
434
Morrison, W. J.,
124
multiplication
of integers,
375
of matrices,
96
of polynomials,
367
multiplicities,
510
and diagonalizability,
512
multiplier,
22, 25
in partial pivoting,
26
N
negative deï¬nite,
570
Neumann series,
126, 527, 618
Newton,
86
Newtonâ€™s identities,
504
Newtonâ€™s second law,
560
nilpotent,
258, 396, 502, 510
Jordan blocks,
579
part of an operator,
399
Noble, Ben,
xii
node,
18, 73, 202, 204
rule,
74, 204
no-intercept model,
447
noise removal with SVD,
418
nonbasic columns,
50, 61
nonderogatory matrices,
644, 648
nondiagonalizable, spectral resolution,
603
nonhomogeneous diï¬€erential equations,
609
nonhomogeneous systems,
57, 64
general solution,
64, 66, 70
summary,
70
nonnegative matrices,
661, 670
nonsingular matrices,
115
and determinants,
465
and elementary matrices,
133
products of,
121
sequences of,
220

Index
713
norm,
269
compatibility,
279, 280, 285
elliptical,
288
equivalent,
276, 425
of a function,
288
on a grid,
274
of an inverse,
285
for matrices,
280
1-, 2-, and âˆ-norms,
281, 283
Frobenius,
279, 337
induced,
280, 285, 337
of a projection,
323
for vectors,
275
1-, 2-, and âˆ-norms,
274
p-norms,
274
of a waveform,
382
normal equations,
213, 214, 221, 226, 313, 437
normal modes of vibration,
562, 571
normalized vector,
270
normal matrix,
304, 400, 409, 547
nullity,
200, 220
nullspace,
173, 174, 178, 199
equality,
177
of an orthogonal projector,
434
of a partitioned matrix,
208
of a product,
180, 220
spanning set for,
175
and transpose,
177
number of pivots,
218
numerical stability,
347
O
oblique projection,
385
method for linear systems,
443
oblique projectors from SVD,
634
Ohmâ€™s law,
73
Oh notation O(hp) ,
18
one-to-one mapping,
250
onto mapping,
250
operation counts
for convolution,
377
for Gaussian elimination,
10
for Gaussâ€“Jordan method,
16
for LU factorization,
146
for matrix inversion,
119
operator, linear,
238
operator norm,
280
Ortega, James,
xii
orthogonal complement,
322, 403
dimension of,
339
involving range and nullspace,
405
orthogonal decomposition theorem,
405, 407
orthogonal diagonalization,
549
orthogonal distance,
435
orthogonal matrix,
320
determinant of,
473
orthogonal projection,
239, 243, 248, 299, 305, 385, 429
and 3-D graphics,
330
onto an aï¬ƒne space,
436
and least squares,
437
orthogonal projectors,
322, 410, 427, 429
elementary,
431
formulas for,
430
onto an intersection,
441
and pseudoinverses,
434
sums of,
441
orthogonal reduction,
341
to determine full-rank factorization,
633
to determine fundamental subspaces,
407
orthogonal triangularization,
342
orthogonal vectors,
294
orthonormal basis,
298
extending to,
325, 335, 38
for fundamental subspaces,
407
by means of orthogonal reduction,
355
orthonormal set,
298
Ostrowski, Alexander,
626
outer product,
103
overrelaxation,
624
P
Painter, Richard J.,
xii
parallelepiped,
431, 468
parallelogram identity,
290, 291
parallelogram law,
162
parallel sum,
441
parity of a permutation,
460
Parseval des ChË†enes, M.,
305
Parsevalâ€™s identity,
305
partial pivoting,
24
and diagonal dominance,
193
and LU factorization,
148
and numerical stability,
349
particular solution,
58, 65â€“68, 70, 180, 213
partitioned matrix,
111
and linear operators,
392
rank and nullity of,
208
Peano, Giuseppe,
160
Penrose equations,
422
Penrose, Roger,
221
perfect shuï¬„e,
372, 381
period of trigonometric functions,
362
periodic extension,
302
periodic function,
301
periodic Markov chain,
694
permutation,
460
symmetric,
671
permutation counter,
151
permutation matrix,
135, 140, 151
perpendicular,
294
perp, properties of,
404, 409
Perronâ€“Frobenius theory,
661, 673
Perron, Oskar,
661
Perron root,
666, 668

714
Index
Perron vector,
665, 668, 673
perturbations
aï¬€ecting rank,
216
eigenvalues,
528
hermitian eigenvalues,
551
in inverses,
128
in linear systems,
33, 128, 217
rank-one update,
208
singular values,
421
Piazzi, Giuseppe,
233
pivot
conditioning,
426
determinant formula for,
474, 558
elements and equations,
5
positions,
5, 58, 61
in partial pivoting,
24
uniqueness,
44
pivoting
complete,
28
partial,
24
plane rotation,
333
determinant of,
485
p-norm,
274
Poissonâ€™s equation,
563, 572
Poisson, SimÂ´eon D.,
78, 572
polar factorization,
572
polarization identity,
293
polynomial
equations,
493
in a matrix,
501
and matrix functions,
606
minimum,
642
multiplication and convolution,
367
polytope,
330, 339
ponderal index,
236
poor manâ€™s root ï¬nder,
649
population distribution,
532
population migration,
531
population model, Leslie,
683
positive deï¬nite form,
567
positive deï¬nite matrix,
154, 474, 558, 559
positive matrix,
661, 663
positive semideï¬nite matrix,
558, 566
Poulson, Deborah ,
xii
power method,
532, 533
powers of a matrix,
107
limiting values,
530
powers of linear transformations,
248
precision,
21
preconditioned system,
658
predatorâ€“prey model,
544
primitive matrices,
674
test for,
678
principal angles,
456
principal minors,
494, 558
in an M-matrix,
626, 639
nonnegative,
566
positive,
559
principal submatrix,
494, 558
and interlaced eigenvalues,
553
of an M-matrix,
626
of a stochastic,
703
products
of matrices,
96
of nonsingular matrices,
121
of orthogonal projectors,
441
of projectors,
393
product rule for determinants,
467
projection,
92, 94, 322, 385, 429
and Fourier expansion,
440
method for solving linear systems,
442, 443
onto
aï¬ƒne spaces,
436
fundamental subspaces,
434
hyperplanes,
442
lines,
440, 431
oblique subspaces,
385
orthogonal subspaces,
429
symmetric matrices,
436
projectors,
239, 243, 339, 385, 386
complementary,
386
from core-nilpotent decomposition,
398
diï¬€erence of,
393
from full-rank factorization,
633, 634
as idempotents,
387
induced norm of,
389
matrix representation of,
387
oblique,
386
orthogonal,
429
product of,
393
spectral,
517, 603
sum of,
393
proper values and vectors,
490
pseudoinverse,
221, 422, 615
as best approximate inverse,
428
Drazin,
399
group,
402
inner, outer, reï¬‚exive,
393
integral representation of,
441, 615
and least squares,
438
Mooreâ€“Penrose,
422
and orthogonal projectors,
434
pure imaginary,
556
Pythagorean theorem,
294, 305, 423
and closest point theorem,
435
for matrices with Frobenius norm,
428
Q
QR factorization,
345, 535
and Hessenberg matrices,
352
and least squares,
346
and minimum polynomial,
643
rectangular version of,
311
and volume,
431
quadratic form,
567

Index
715
quaternions,
509
R
random integer matrices,
156
random walk,
638
range
of a function,
89, 169
of a matrix,
170, 171, 178, 199
of an operator,
250
of an orthogonal projector,
434
of a partitioned matrix,
179
of a product,
180, 220
of a projector,
391
of a sum,
206
range-nullspace decomposition,
394, 407
range-symmetric matrices,
408
rank,
45, 139
of a block diagonal matrix,
137
and consistency,
54
and determinants,
466
of a diï¬€erence,
208
of an elementary projector,
337
and free variables,
61
of an incidence matrix,
203
and independent sets,
183
and matrix inverses,
116
and nonhomogeneous systems,
70
and nonsingular submatrices,
218
numerical determination,
421
of a partitioned matrix,
208
of a perturbed matrix,
216
of a product,
210, 211, 219
of a projector,
392
and submatrices,
215
of a sum,
206, 221
summary,
218
and trivial nullspaces,
175
rank normal form,
136
rank-one matrices
characterization of,
140
diagonalizability of,
522
perturbations of,
208
rank-one updates
determinants of,
475
eigenvalues of,
503
rank plus nullity theorem,
199, 410
Rayleigh, Lord,
550
Rayleigh quotient,
550
iteration,
535
real numbers, the set of,
81
real Schur form,
524
real-symmetric matrix,
409, 410
rectangular matrix,
8
rectangular QR factorization,
311
rectangular systems,
41
reduced row echelon form,
48
reducible Markov chain,
698
reducible matrices,
209, 671
canonical form for,
695
in linear systems,
112
reï¬‚ection,
92, 94
about a hyperplane,
445
method for solving linear systems,
445
reï¬‚ector,
239, 324, 444
determinant of,
485
reï¬‚exive pseudoinverse,
393
regression,
227, 446
relative uncertainty or error,
414
relaxation parameter,
445, 624
residual,
36, 416
resolvent,
285, 611
restricted operators,
259, 393, 399
restricted transformations,
424
reversal matrix,
596
reverse order law
for inversion,
120, 121
for transpose and conjugate transpose,
109
reversing binary bits,
372
Richardson iterative method,
622
right angle,
294
right-hand rule,
340
right-hand side,
3
Ritz values,
651
roots of unity,
356
and imprimitive matrices,
676
Rose, Nick,
xii
rotation,
92, 94
determinant of,
485
plane (Givens rotations),
333
in â„œ2,
326
in â„œ3,
328
in â„œn,
334
rotator,
239, 326
rounding convention,
21
roundoï¬€error,
21, 129, 347
row,
7
echelon form,
44
reduced,
48
equivalence,
134, 218
and nullspace,
177
operations,
134
rank,
198
relationships,
136
scaling,
27
space,
170, 171, 178, 199
spanning set for,
172
vector,
8, 81
RPN matrices,
408
Rutishauser, Heinz,
535
S
Saad, Yousef,
655
saw-toothed function,
306
scalar,
7, 81

716
Index
scalar multiplication,
82, 83
scale,
27
scaling a linear system,
27, 28
scaling in 3-D graphics,
332
Schmidt, Erhard,
307
SchrÂ¨odinger, Erwin,
651
Schultz, Martin H.,
655
Schur complements,
123, 475
Schur form for real matrices,
524
Schur, Issai,
123, 508, 662
Schur norm,
279
Schur triangularization theorem,
508
Schwarz, Hermann A.,
271, 307
search engine,
418, 419
sectionally continuous,
301
secular equation,
503
Seidel, Ludwig,
622
Sellers, Lois,
xii
semiaxes,
414
semideï¬nite,
566
semisimple eigenvalue,
510, 591, 593, 596
semistable,
544
sensitivity,
128
minimum norm solution,
426
sequence
limit of,
639
of matrices,
220
series for f(A) ,
605
shape,
8
shell game,
635
Sherman, J.,
124
Shermanâ€“Morrison formula,
124, 130
SIAM,
324, 333
signal processing,
359
signal-to-noise ratio,
418
sign of a permutation,
461
similar matrices,
255, 473, 506
similarity,
505
and block-diagonal matrices,
263
and block-triangular matrices,
263
and eigenvalues,
508
invariant,
256
and orthogonal matrices,
549
transformation,
255, 408, 506
and transpose,
596
unitary,
547
simple eigenvalue,
510
simple loops,
75
simultaneous diagonalization, triangularization,
522
simultaneous displacements,
622
sine, discrete,
361
singular matrix,
115
eigenvalues of,
501
sequences of,
220
singular systems, practical solution of,
217
singular values,
553
Courantâ€“Fischer theorem,
555
and determinants,
473
as eigenvalues,
555
and the SVD,
412
size,
8
skew-hermitian matrices,
85, 88
skew-symmetric matrices,
85, 88, 391, 473
eigenvalues of,
549, 556
as exponentials,
539
vector space of,
436
SOR method,
624
Souriau, J. M.,
504
spanning sets,
165
for column space,
172
for four fundamental subspaces,
178
for left-hand nullspace,
176
minimal,
197
for nullspace,
175
for row space,
172
test for,
172
sparse least squares,
237
sparse matrix,
350
spectral circle, imprimitive matrices,
676
spectral mapping property,
539, 613
spectral projectors,
517, 602, 603
commuting property,
522
interpolation formula for,
529
positivity of,
677
in terms of eigenvectors,
518
spectral radius,
497, 521, 540
Collatzâ€“Wielandt formula,
666, 673, 686
as a limit,
619
and limits,
617
spectral representation of matrix functions,
526
spectral resolution of f(A) ,
603
spectral theorem for diagonalizable matrices,
517
spectrum,
490
of imprimitive matrix,
677
spheres,
275
splitting,
620
spring-mass vibrations,
570
springs,
86
square
matrix,
8
system,
5
wave function,
301
stable,
544
algorithm,
217, 317, 347, 422
matrix,
544
system,
544, 609
standard
basis,
194, 240, 299
coordinates,
240
deviation,
296
inner product,
95, 271
scores,
296
standardization of data,
296
stationary distribution,
531, 693

Index
717
steady-state distribution,
531, 636
steepest descent,
657
step size,
19
Stewart, G. W.,
xii
Stiefel, Eduard,
656
stiï¬€ness
constant,
86
matrix,
87
stochastic matrix,
685, 687
doubly,
702
summability of,
697
unit eigenvalues of,
696
Strang, Gilbert,
xii
strongly connected graph,
209, 671
Strutt, John W.,
550
stuï¬€in a vector space,
197, 200
subgroup,
402
submatrix,
7
as a block matrix,
111
and rank,
215
subscripts,
7
subset,
162
subspace,
162
angles or gaps between,
450
dimension of,
198
directed distance between,
453
four fundamental,
169
invariant,
259, 262, 263
maximal angle between,
455
sum of,
205
substochastic matrix,
685
successive displacements,
623
successive overrelaxation method,
624
sum
of matrices,
81
of orthogonal projectors,
441
of projectors,
393
of vector spaces,
166, 383
dimension of,
205
summable matrix and summability,
631, 633, 677
stochastic matrices,
697
superdiagonal,
575
SVD,
412
and full-rank factorization,
634
and oblique projectors,
634
switching circuits,
539
Sylvester, James J.,
44, 80, 411
Sylvesterâ€™s law of inertia,
568
Sylvesterâ€™s law of nullity,
220
symmetric
functions,
494
matrices,
85
diagonalization and eigen components of,
549
reduction to tridiagonal form,
352
space of,
436
permutation,
671
T
Taussky-Todd, Olga,
497
Taylor series,
18, 570, 600
t-digit arithmetic,
21
tensor product,
380, 597
and the Laplacian,
573
term-by-document matrix,
419
text mining,
419
three-dimensional rotations,
328, 330
time domain,
363
Todd, John,
497
Toeplitz matrices,
514
Toeplitz, Otto,
514
total least squares,
223
trace,
90
and characteristic equation,
504
of imprimitive matrices,
678
inequalities,
293
of a linear operator,
256
of a product,
110, 114
of a projector,
392
as sum of eigenvalues,
494
transformation, linear,
238
transient behavior,
532
transient class,
695
transition diagram,
108, 531
transition matrix,
108, 531, 688
transitive operations,
257
translation, in 3-D graphics,
332
transpose,
83
and determinants,
463
nullspace,
177
properties of,
84
reverse order law for,
109
and similarity,
596
trapezoidal form,
342
trend of observations,
231
triangle inequality,
220, 273, 277
backward version,
273
triangular matrices,
41, 103
block versions,
112
determinant of,
462
eigenvalues of,
501
elementary,
142
inverses of,
122
triangularization, simultaneous,
522
triangularization using elementary reï¬‚ectors,
342
triangular system,
6
tridiagonal matrix,
20, 156, 352
Toeplitz matrices,
514
trivial
nullspaces,
175
solution,
57, 60, 69
and nonhomogeneous systems,
70
and nonsingular matrices,
116
subspace,
162, 197
Tukey, J. W.,
368, 375, 651

718
Index
two-point boundary value problem,
18
U
unbiased estimator for variance,
449, 446
uncertainties in linear systems,
414
underrelaxation,
624
unique solution
for diï¬€erential equations,
541
and free variables,
61
for homogeneous systems,
61
for nonhomogeneous systems,
70
unitarily invariant norm,
425, 337
unitary diagonalization,
547
unitary matrices,
304, 320
determinant of,
473
unit columns,
102, 107
unit eigenvalues of stochastic matrices,
696
units,
27
unit sphere,
275
image of,
414, 425
unstable,
544
upper-trapezoidal form,
342, 344
upper triangular,
103
URV factorization,
406, 407
and full-rank factorization,
634
V
Vandermonde, Alexandre-Theophile,
185
Vandermonde determinant,
486
Vandermonde matrices,
185, 230, 357
Van Loan, Charlie,
xii
variance,
447
vector,
159
norms,
274
spaces,
160
vertex matrix,
330
vibrations, small,
559
volume
by determinants,
468
by Gramâ€“Schmidt, and QR,
431
von Mises, R.,
533
von Neumann, John,
289
W
Weierstrass, Karl Theodor Wilhelm,
589, 662
well conditioned,
33, 127, 415
Weyl, Hermann,
160
why least squares?,
446
Wielandt, Helmut,
534, 666, 675, 679
Wielandtâ€™s matrix,
685
Wielandtâ€™s theorem,
675
Will, Marianne,
xii
wire frame ï¬gure,
330
Woodbury, M.,
124
Wronskian,
474, 481, 486
Wronski, Jozef M.,
189
Wronski matrix,
189, 190
X, Y, Z
Young, David M.,
625
Young, G.,
411
Zeeman, E. Christopher,
704
zero nullspace,
175
zero transformation,
238
Z-matrix,
628, 639, 296
z-scores,
296


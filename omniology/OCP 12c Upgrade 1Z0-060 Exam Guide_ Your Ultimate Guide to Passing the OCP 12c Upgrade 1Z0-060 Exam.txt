
OCP 12c Upgrade 1Z0-060 Exam Guide

Table of Contents
OCP 12c Upgrade 1Z0-060 Exam Guide
Credits
About the Authors
About the Reviewer
www.PacktPub.com
Why subscribe?
Preface
What this book covers
What you need for this book
Who this book is for
Conventions
Reader feedback
Customer support
Downloading the example code
Downloading the color images of this book
Errata
Piracy
Questions
1. Getting Started with Oracle 12c
Installation and configuration using OUI and DBCA
Managing PDB databases using DBCA
Using EM Database Express
Configuring EM Express
Access control for EM Express
Exploring Oracle database EM Express
Emergency monitoring and Real-Time ADDM
Slow system
Emergency monitoring
Real-Time ADDM
Working of Real-Time ADDM
Automatic triggering of Real-Time ADDM
Generating Real-Time ADDM report
Emergency monitoring versus Real-Time ADDM
Generating ADDM compare period
How does the ADDM compare report help?
Workload commonality
Generating ADDM compare period
Using EM Cloud Control 12c
Using DBMS_ADDM package
Diagnosing performance issues using ASH enhancements
Limitations on top activity page in previous release

New ASH analysis page
Summary
2. Multitenant Container Database Architecture
Challenges with non-container databases
Benefits of multitenant architecture
Root and multitenant architecture
Separation of metadata
Shared and non-shared objects
Managing a CDB
Organization of CDBs and PDBs
Common entities
Exclusive entities
Data dictionary views
Local users and common users
Local users
Common users
Local and common roles and privileges
Local roles
Common roles
Example of granting a common role commonly
Example of granting a common role locally
Local privileges and common privileges
Creating and configuring a CDB
Post-CDB creation
What's in your CDB?
Creating PDBs
Creating a PDB from seed
Cloning a PDB from an existing PDB
Plugging an unplugged PDB into a CDB
Unplugging PDB2 from ORCL
Plugging PDB2 into a CDB
Migrating a non-CDB to a PDB
Plugging a non-CDB into a CDB using datapump TTS
Using the Golden Gate replication to plug a non-CDB into a PDB
Migrating a non-CDB to a PDB using the DBMS_PDB package
Summary
3. Managing CDBs and PDBs
Establishing connection to CDB/PDBs
Services
Creating services
Connecting to CDBs
Locally using user ID and password but without specifying a service name
Locally using OS authentication
Using a net service name/connect string/alias configured in tnsnames.ora

Remotely using EasyConnect
Connecting to PDBs
Switching connections
Using reconnect
Using alter session
Starting up and shutting down a CDB/PDB
Starting up CDB
Opening PDBs
Shutting down of CDBs and PDBs
Closing PDBs
Shutting down CDBs
Changing instance parameters for a CDB/PDB
Modifying PDB settings
Object link and metadata link
The metadata link
Object links
Container data objects
Enabling a common user to access data in a specific PDB
Managing tablespaces in a CDB/PDB
Default tablespaces for containers
Permanent tablespaces in CDB and PDB
Temporary tablespaces in a CDB and PDB
Managing users and privileges in CDB/PDB
Local users and common users
Local users
Common users
Viewing local and common users
Local privileges and common privileges
Managing privileges
Granting privileges
Revoking privileges
Local roles and common roles
How common roles work
Rules for creating common roles
Rules for creating local roles
Creating roles
Granting roles
Revoking roles
Summary
4. Information Lifecycle Management and Storage Enhancements
Information lifecycle management
Typical lifecycle of data
How automatic data optimization works
Enabling heat maps

Enabling and disabling heat maps
Checking heat map details
V$HEAT_MAP_SEGMENT view
SYS.HEAT_MAP_STAT$ and DBA_HEAT_MAP_SEG_HISTOGRAM
DBA_HEATMAP_TOP_TABLESPACES
DBMS_HEAT_MAP package
Enabling ADO policies
ADO policies for compression action
Tablespace level compression policy
Group-level compression policy
Segment level compression policy
Row level compression policy
ADO policies for data movement action
CUSTOM_ILM_RULES
Implementing multiple policies on a segment
Policy priority and policy inheritance
Checking ILM policy details
DBA_ILMPOLICIES
DBA_ILMDATAMOVEMENTPOLICIES
Evaluating and executing ADO policies
Scheduled in maintenance window
Manually executing ADO policies
Checking execution details of ADO policies
DBA_ILMTASKS
DBA_ILMEVALUATIONDETAILS
DBA_ILMRESULTS
DBMS_ILM_ADMIN package
Enabling and disabling ADO policies
Enabling/disabling ADO policies
Deleting ADO policies
Enabling/disabling ILM
Moving data files online
Moving partitions online
In-database archiving and temporal validity
Oracle Hybrid Columnar Compression
How does HCC compression work?
In-database archiving
Temporal validity and temporal history
Temporal history
Defining temporal validity
PERIOD FOR clause
DBMS_FLASHBACK_ARCHIVE
Summary
5. Auditing, Privilege Analysis and Data Redaction

Unified auditing
Advantages of unified auditing
Mixed auditing mode and unified auditing mode
Enabling unified auditing mode
Extended audit information
Enabling extended auditing
Creating and enabling unified audit policy
Audit policies in a multitenant database
Common audit policy
Local audit policy
Creating audit policies
Creating a system-wide audit policy
Creating an object-specific audit policy
Using conditions and evaluation
Modifying audit policies
Enabling audit policies
Altering audit policies
Disabling audit policies
Dropping audit policies
Viewing audit policies
Using predefined audit policies
Capturing application context
Cleaning up audit data
Using administrative privileges
New operating system group
New password file
Privilege analysis
Creating a privilege analysis policy
Enabling privilege analysis
Disabling the privilege analysis
Generating a privilege analysis report
Viewing the report
Dropping the privilege analysis policy
New privilege checking during PL/SQL calls
New inherit privilege
New privilege checking with bequeath views
Oracle Data Redaction
Activities exempt from data redaction
Types of data redaction
Full data redaction
Partial data redaction
Random data redaction
Regular expression redaction
Creating a redaction policy

Adding a redaction
Adding a redaction policy to a table or view
Modifying a redaction policy
How Oracle data redaction affects tables and views
Altering the default full data redaction value
Restrictions on data redaction
Enabling and disabling data redaction policies
Exempting users from data redaction
Viewing data redaction policies details
REDACTION_POLICIES
REDACTION_COLUMNS
Summary
6. Database Tuning and Troubleshooting
Real-time database operation monitoring
Database operation
Identifying database operations
Enabling monitoring of database operations
Monitoring progress of database operations
Monitoring operations using Oracle EM Database Express
Monitoring operations using views
Monitoring operations using reports
SQL tuning
Adaptive execution plans - dynamic plans
What happens in adaptive plan execution?
Enabling adaptive execution plans
Adaptive execution plans and join methods
Adaptive plans and parallel distribution methods
Automatic re-optimization
Statistics feedback
Performance feedback
Adaptive statistics
Dynamic statistics
SQL plan directives
DBMS_SPD
Enhanced features of statistics gathering
Online statistics gathering for bulk-load
Concurrent statistics gathering
Enabling concurrent statistics collection
Parallel execution of stats gathering
Statistics gathering for a global temporary table
Histogram enhancements
Top frequency histogram
Hybrid histogram
Extended statistics enhancements

Adaptive SQL plan management
Challenges in previous releases
SQL plan management - previous releases
Limitations of outlines and profiles
Finally, how are baselines different than outline and profile, then?
SQL plan baseline parameters
Adaptive SPM - Oracle 12c
New task-based function
Configuring the SPM evolve advisor task
Summary
7. Backup and Flashback Enhancements
Performing a backup of CDB and PDB
Performing complete CDB backup
Performing partial CDB backup
Performing complete PDB backup
Performing partial PDB backup
Performing PDB hot backup
Archive log backup
Performing recovery of CDB and PDB
Performing instance recovery
Media failure - CDB temporary file recovery
Media failure - PDB temporary file recovery
Media failure - control file loss
Media failure - Redo log file loss
Media failure - root SYSTEM or UNDO data file loss
Media failure - root SYSAUX data file loss
Media failure - PDB SYSTEM data file loss
Media failure - PDB non-SYSTEM data file loss
Performing PDB PITR (Point-in-Time Recovery)
PDB tablespace PITR
Performing a flashback of CDB
Performing a flashback of a database
Flashing back CDB after a PDB PITR
Using RMAN enhancements
Separation of DBA duty - New SYSBACKUP privileges
Using SQL in RMAN
Multi-section image copies and Multi-section incremental backup
RMAN Active duplication enhancements
Default use of a backup set in active database duplication
Introducing choice of compression, encryption, and section size
Option to complete duplication with the target database in mounted mode
Duplicating container databases with PDBs
Duplicating CDB
Duplicating PDBs

Recovering a database with a third party snapshot
Transporting data across a platform using backup sets
Transporting databases
Transporting tablespaces
Automatic table recovery using RMAN
Recovering a table
Recovery point-in-time options
Steps for performing table recovery
Implementing new features of Flashback Data Archive
Database Hardening - enabling Flashback Data Archive for Security-Related
Application Tables
Flashback Data Archive improvements
Optimization for Flashback Data Archive History Tables
Summary
8. Database Administration Enhancements
Resource manager
Managing resources between PDBs
Creating and enabling a CDB resource plan
Viewing CDB resource plan directives
Managing CDB resource plans
PDB resource plans
Enhancements in creating resource manager plan directives
Multi-process, multi-threaded Oracle architecture
Enabling multi-threaded architecture
Connecting to a database with multi-threaded architecture
Smart flash cache enhancements
Using multiple flash cache devices
Index and table enhancements
Invisible columns
Usage and limitations of invisible columns
Multiple indexes on the same columns
Online redefinition - tables with VPD
Online redefinition - dml_lock_timeout
Advanced row compression and compression advisor
LOB compression
Using compression advisor
Enhanced online DDL capabilities
Drop index/constraints
Index unusable
Setting unused columns
ADR and network enhancements
ADR enhancements
DDL logging
Debug log

Network-related enhancements
Network compression
Enabling network compression
Session Data Unit
Summary
9. Miscellaneous New Features
Oracle data pump enhancements
Full transportable export and import of data
Restrictions in a full transportable export
Disabling logging for data pump import
Exporting views as a table
Creating a SecureFile LOB during import
Specifying an encryption password
SQL Loader enhancements
SQL* Loader express mode
Support for loading of identity columns
SQL* Loader syntax enhancements and external tables
Partitioning enhancements
Online partition operations
Reference partitioning enhancements
Interval reference partitioning
CASCADE option for truncate partition
Multi-partition maintenance operations
Adding multiple partitions
Truncating multiple partitions
Merging multiple partitions
Splitting multiple partitions
Dropping multiple partitions
Index enhancements for partitioned tables
Partial index for partitioned tables
Specifying default indexing property for table/partitions
Creating partial local and global indexes
Modifying the indexing property of a partition/table
Effect of partial indexes on SQL plans
Data dictionary changes
Asynchronous global index maintenance
The DBMS_PART package
Oracle Database Migration Assistant for Unicode
SecureFile LOB enhancements
The row limiting clause
Extended data types
Enabling extended data types
Summary
10. Core Administration

Fundamentals of DB architecture
Oracle database files
Oracle instances
Memory architecture of the Oracle database
System Global Area
Program Global Area
User Global Area
Oracle database process architecture
Client process
Server process
Background processes
Shared server architecture and dedicated server architecture
Dedicated server processes
Shared server processes
Installing and configuring a database
Configuring the database
Data file location or OMF
Enabling archiving
Configuring the listener and service names
Static registration
Dynamic service registration
Monitoring database errors and alerts
Monitoring errors with trace files and alert log files
Monitoring database operations with server generated alerts
Perform daily administration tasks
Moving tables online with the least downtime
Recompiling invalid objects
Using various advisors to tune various components of the database
Segment advisor
SQL tuning advisor
Enabling and disabling automatic SQL tuning
Configuring automatic SQL tuning
Managing long idle sessions by creating appropriate user profiles
UNLIMITED resources
Apply and review patches
Using Enterprise Manager cloud control
Using the OPatch utility
Backup and recover the database
Classification of backups
Backup sets vs image copies
RMAN backup vs user-managed backup
Hot backup versus cold backup
Full backup versus incremental backup
Incremental backup and block change tracking file

Validating backup
Physical and logical block corruption
Intrablock corruption
Interblock corruption
Data recovery advisor
Detecting failures
Failure status
Failure priority
Manual actions and automatic repair options
Repair scripts
Implement flashback technology
Enabling Oracle flashback
Flashback database and restore points
Limitations of flashback database
Prerequisites for flashback database
Performing a flashback database operation
Enabling flashback data archive
DBMS_FLASHBACK package
Relocate SYSAUX occupants
Create a default permanent tablespace
Use secure file LOBs
BasicFile LOB vs SecureFile LOB
Use Direct NFS
Enabling Direct NFS
Summary
11. Performance Management
Design the database layout for optimal performance
Designing for performance and scalability
Collecting application requirement
Designing the schema and data model
Using bind variables
Using automatic segment space management
Locally managed tablespaces
Dictionary managed tablespaces
Segment space management in locally managed tablespaces
Using database resource manager
Designing for availability
Configuring FAST_START_MTTR_TARGET parameter
Protection against corruption
Configure automatic shared memory management
Setting up the maximum SGA size
Setting SGA target size
Monitoring performance
Proactive database monitoring

Manually monitoring database performance
Diagnosing performance problems
Performance degradation over time
Managing baselines
Comparing period AWR
Using advisors to optimize database performance
Managing memory
Automatic memory management
Automatic shared memory management
PGA memory management
Setting minimums for automatically sized SGA components
Configuring manually sized memory components
The keep and recycle buffer caches
Nonstandard block size buffer caches
Analysing and identifying performance issues
Automatic Workload Repository (AWR)
End to end application tracing
Tracing for client identifier
Tracing for service, module, and action
Tracing for session
Tracing for the entire instance or database
Using the trcsess and tkprof utility
Clustering factor
SQL performance analyzer
Running the SQL performance analyzer
Comparing SQL performance
Cardinality feedback
Adaptive cursor sharing
The purpose of adaptive cursor sharing
Bind-sensitive cursors
Bind-aware cursors
Adaptive cursor sharing views
Implement application tuning
Summary
12. Storage
Managing database structures
Physical storage structure
Logical storage structure
Undo size advisor
The undo advisor a PL/SQL interface
Administering ASM
Administering different releases
Initialization parameters for an ASM instance
Authentication for an ASM instance

About the SYSASM Privilege
Local connection using operating system authentication
Oracle ASM filesystem
Filenames
Directories
Aliases
ASM metadata backup and restore
Managing ASM disks and disk groups
Disk group attributes
Creating disk groups
Altering disk groups
Oracle ASM disk discovery
Disk group redundancy
Oracle ASM failure groups
Managing an ASM instance
Starting up an ASM instance
Restricted mode
Shutting down an ASM instance
ASM background processes
Managing VLDB
High availability
Hardware-based mirroring
Mirroring using ASM
Performance and manageability
Hardware-based stripping
Stripping using ASM
Partitioning
Implementing space management
Logical space management
Locally managed tablespaces (default)
Automatic segment-space management
Manual Segment Space Management
Dictionary-managed tablespaces
Space management in a data block
Summary
13. Security
Develop and implement a security policy
User security
Enforcing password complexity verification
Assigning a default tablespace to the user
Tablespace quota for the user
Temporary tablespace for the user
Specifying a profile for the user
Default role for the user

Configuring authentication
Configuring password protection
Password management policy
Database authentication
Operating system authentication
Configuring authorization
Privileges and roles
Managing System Privileges
Allowing Access to Objects in the SYS Schema
ANY privileges and PUBLIC role
Managing user roles
Restricting SQL*Plus users from using database roles
Limiting roles through the PRODUCT_USER_PROFILE table
Managing object privileges
Application security
Associating privileges with user database roles
Configuring the maximum number of authentication attempts
Configure and manage auditing
Purpose of auditing
Mandatory auditing
Auditing types
Auditing general activities with standard auditing
AUDIT and NOAUDIT SQL statements
Auditing SQL statements
Auditing privileges
Auditing schema objects
Auditing network activity
Auditing statement executions - successful, unsuccessful, or both
Fine-grained auditing
Fine-grained auditing records
Using the DBMS_FGA package to manage fine-grained audit policies
Creating a fine-grained audit policy
Auditing specific columns and rows
Disabling and enabling a fine-grained audit policy
Dropping a fine-grained audit policy
Create the password file
Sharing and disabling the password file
Keeping administrator passwords synchronized with the data dictionary
Adding users to a password file
Viewing password file members
Implement column and tablespace encryption
Benefits of using transparent data encryption
Types of transparent data encryption
Using transparent data encryption

Specifying a wallet location for transparent data encryption
Using wallets with automatic login enabled
Setting the master encryption key
Opening and closing the encrypted wallet
Creating tables with encrypted columns
Encrypting entire tablespaces
Setting the tablespace master encryption key
Opening the Oracle wallet
Creating an encrypted tablespace
Restrictions on using TDE tablespace encryption
Summary

OCP 12c Upgrade 1Z0-060 Exam Guide

OCP 12c Upgrade 1Z0-060 Exam Guide
Copyright © 2016 Packt Publishing
All rights reserved. No part of this book may be reproduced, stored in a retrieval system, or
transmitted in any form or by any means, without the prior written permission of the
publisher, except in the case of brief quotations embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the
information presented. However, the information contained in this book is sold without
warranty, either express or implied. Neither the authors, nor Packt Publishing, and its dealers
and distributors will be held liable for any damages caused or alleged to be caused directly or
indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the
companies and products mentioned in this book by the appropriate use of capitals. However,
Packt Publishing cannot guarantee the accuracy of this information.
First published: November 2016
Production reference: 1241116
Published by Packt Publishing Ltd.
Livery Place
35 Livery Street
Birmingham 
B3 2PB, UK.
ISBN 978-1-78712-660-2
www.packtpub.com

Credits
Authors
Advait Deo
Indira Karnati
Copy Editor
Safis Editing
Reviewer
Osama Mustafa
Project Coordinator
Shweta H Birwatkar 
Commissioning Editor
Veena Pagare
Proofreader
Safis Editing
Acquisition Editor
Divya Poojari 
Indexer
Mariammal Chettiyar
Content Development Editor
Amrita Noronha
Graphics
Disha Haria
Technical Editor
Deepti Tuscano
Production Coordinator
Arvindkumar Gupta

About the Authors
Advait Deo has 12 years of experience in the database domain starting from Oracle Version
9i until Oracle 12c. By nature, a mechanical engineer, Advait transformed his career around
Software and databases right from the beginning. He joined a consultancy company Tata
Consultancy Services and 2004 and learned about database technology. In 2006, he moved to
Oracle India Pvt. Ltd. where he was mentored by Indira Karnati. He later moved to
Amazon.com as database administrator in 2010. He is currently working at Amazon as a
senior database engineer.
Advait has worked in various aspects of Oracle databases and administered huge fleet of
complex production databases. He is an expert in performance tuning and troubleshooting. He
also focuses on schema design, automation using shell and Python, and architecting solutions
to complex problems.
Currently, he is working on various products in Amazon Web Services (AWS) technology
and providing database and storage solutions in AWS to various clients.
Advait is an Oracle Certified Professional in Oracle 9i, 10g, 11g, and 12c databases and
Oracle Certified Expert in 11g RAC. His certification credentials are available
at https://www.youracclaim.com/users/advait-deo. He also publishes some of his work and
learnings on his website http://avdeo.com.
You can find him on LinkedIn at https://www.linkedin.com/in/advaitdeo
Prior to writing this book, Advait reviewed Oracle Database 11g R2 Performance Tuning
Cookbook and Learning Linux Shell Scripting.
I would like to thank my wife for putting up with my busy schedule and supporting me all the
way. This book would not have happened without her support. I would also like to thank my
parents and my mentors who helped me over the years and who believed in me. Without them, I
would not have reached to the place where I am now.
Without someone passing the knowledge to us, it makes it is more challenging to learn and
understand the concepts and I feel everyone is obligated to pass on the knowledge that they
gain, back to the community. This book, along with my blog, is a medium by which I would like
to pass on the knowledge to the community, and I hope this will prove to be helpful to
everyone.
Indira Karnati is an engineering graduate in electronics and communications and did her
Executive General Management Program at IIM Bangalore, India.
Indira is an expert Oracle DBA and Apps DBA, having worked in various capacities for the
past 19 years in IT, of which 15 years has been with Oracle Corporation, India Development

Centre, Hyderabad. She has extensive hands on experience of installations, configurations of
Oracle databases (single instance and RAC), Oracle Identity Management (OID/OAM/OIM),
Fusion Middleware products, Oracle EBS, Oracle Fusion applications, Oracle Demantra, and
Oracle iLearning.
Indira has rich experience in turning around the complex Oracle installations. Her expertise
includes installation, configuration, cloning, refreshing, patching, upgrading, debugging
many Oracle Products, the primary being Oracle Database, identity management, and
enterprise manager. Her work experience covers high availability and disaster recovery
solutions.
She is an Oracle Certified Professional in 12c and 11g in the Database Administration track,
Oracle Certified Expert in 11g RAC, and Exadata Ceritified Implementation Specialist. Her
Certification credentials are available at https://www.youracclaim.com/users/indira-karnati
Find her on LinkedIn at https://www.linkedin.com/in/indirakaranati
"Praise God, Seek God, Worship God, Trust God, Thank God" is one of the most important
things that my father taught me in life. So, I thank God for making me achieve this milestone
from the bottom of my heart. I would like to thank my parents for their blessings and teachings,
which have made my life more meaningful. Without their inspiration, drive, and support I
would not be the person I am today. I would like to thank my husband Srinivas, and my son
Kartheek; I am so fortunate to have them in my life, but for their love and continued support in
whatever I do, I would not have achieved this. And special thanks to my wonderful brothers
Prasanna, Srinivas and Mohan Krishna for being there during all the difficult times to
support. Also I would like to thank my ex-colleague, friend and co-author Advait for his
invaluable effort in transforming this dream into reality, it's so wonderful knowing him and
having worked closely with him.

About the Reviewer
Osama Mustafa (Oracle ACE) has progressive experience in Oracle Products, community.
He recently served as Oracle Database Administrator, Osama Mustafa holds more than 25
Oracle certification in different products.
Provide Database implementation solutions, high availability solution, infrastructure and
storage planning, install, configure, implement, and manage Oracle E-Business suite
environments. Architect, build and support highly-available Oracle EBS, database and fusion
middleware specialist, exalytics and exalogic expert.
Included to this Osama is volunteer in Oracle user group, international speaker, and author
for Oracle penetration testing book, reviewer for Oracle book, organizing RAC attack
around the world, board member in Oracle RAC SIG, volunteer in IOUG and ODTUG,
publish online articles on his blog https://osamamustafa.blogspot.com and his articles
published in Oracle magazine and OTech magazine.
Some of the book he reviews:-
Oracle Data Guard 11gR2 Administration Beginner's Guide
Oracle Database 12c Backup and Recovery Survival Guide
Oracle 11g Anti-hacker's Cookbook
Oracle 12c Security Cookbook.
Finally, I would like to take this opportunity to thank my family for supporting me specially
my mother during this career and tolerate me during my life for the long working hours and
traveling most of time.

www.PacktPub.com
For support files and downloads related to your book, please visit www.PacktPub.com.
Did you know that Packt offers eBook versions of every book published, with PDF and ePub
files available? You can upgrade to the eBook version at www.PacktPub.com and as a print
book customer, you are entitled to a discount on the eBook copy. Get in touch with us
at service@packtpub.com for more details.
At www.PacktPub.com, you can also read a collection of free technical articles, sign up for a
range of free newsletters and receive exclusive discounts and offers on Packt books and
eBooks.
https://www.packtpub.com/mapt
Get the most in-demand software skills with Mapt. Mapt gives you full access to all Packt
books and video courses, as well as industry-leading tools to help you plan your personal
development and advance your career.

Why subscribe?
Fully searchable across every book published by Packt
Copy and paste, print, and bookmark content
On demand and accessible via a web browser

Preface
This book provides complete details about the new features introduced in Oracle 12c release
1 (12.1.0.1). This book will get Oracle admins up to date with the latest developments in
Oracle 12c. It includes all the necessary information so you understand and learn how to
implement the latest features of Oracle 12c in your existing systems.
In this book, all the information is organized so you can pass your Oracle 12c upgrade exam
with much ease. Every chapter in this book is set in line with the objectives of the exam. With
the help of real-world examples, you will learn concepts of new features such as multi-tenant
container database architecture, managing container, pluggable databases, database
administration enhancements, database auditing, tuning, backup and flashback enhancements
etc.

What this book covers
Chapter 1 , Getting Started with Oracle 12c, this chapter introduces some of the new features
related to installation and configuration of Oracle 12c. It also introduces a new XML based
web tool in Oracle 12c – Oracle EM Database Express. We will also see other features that are
introduced in Oracle 12c like emergency monitoring, real-time ADDM and compare period
ADDM. Finally, it introduces us to the new ASH analytics page in Oracle EM cloud control
12c.
Chapter 2, Multitenant Container Database Architecture, this chapter introduces you to Oracle
12c multitenant architecture. We are going to look into the benefits of using multitenant
architecture, different components of multitenant architecture, how to create and configure
multitenant database and different ways to create pluggable database PDB and finally how to
migrate a non-CDB database as pluggable database into a container database.
Chapter 3, Managing CDBs and PDBs, in this chapter we are going to see how to establish
connection to container database and pluggable database. Performing general administration
tasks such as startup and shutdown of CDB and PDBs. Managing tablespace, users, roles and
privileges in CDB and PDBs etc.
Chapter 4, Information Life Cycle Management and Storage Enhancements, in this chapter we
are going to introduce you to information life cycle management in Oracle 12c and how this
has been remarkably automated in Oracle 12c. We will also show you have to move the
datafiles online and other archiving solutions introduced in Oracle 12c like in-database
archiving and valid-time temporal.
Chapter 5, Auditing, Privilege Analysis and, Data Redaction, this chapters introduces you to
the new security features of Oracle 12c. We are going to look into the new auditing
introduced in Oracle 12c called unified auditing. We will learn to create new auditing policy
to meet our audit requirement and how we can move our database to use unified auditing. We
will also introduce you to the new tool in Oracle 12c which allows you to perform privilege
analysis. Finally, we will also see another new feature called data redaction and how it works
in Oracle 12c.
Chapter 6, Database Tuning and Troubleshooting, in this chapter we will learn about new
features related to performance tuning. Oracle 12c introduces real-time operations
monitoring which is an extension of real-time SQL monitoring. We will also learn about new
enhancements to SQL tuning like adaptive SQL plans, Dynamic statistics and SQL plan
directives. This is one of the most important chapter for learning new features of SQL tuning
in Oracle 12c. Finally, we will see some improvements related to statistics gathering followed
by enhancements to histogram. We will conclude this chapter with enhancements related to
SQL plan management (SPM).
Chapter 7, Backup and Flashback, in this chapter we are going to look into backup and

recovery scenarios and how multitenant architecture affect these scenarios. We are also going
to look into some of the RMAN enhancements in Oracle 12c followed by table restore and
recovery using simple RMAN command. We will conclude this chapter by mentioning new
enhancement related to flashback data archive.
Chapter 8, Database Administration Enhancements, in this chapter we are going to look into
resource manager and how it will work with Oracle 12c multitenant database. We are also
going to introduce some enhancement related to indexes and tables and at the end, look into
ADR and network enhancements.
Chapter 9, Miscellaneous New Features, this chapter bundles the other new features
introduced in Oracle 12c including enhancement related to Oracle data pump, SQL* Loader,
online operations etc. It also provides new partitioning enhancements introduced in Oracle
12c and new top n SQL clauses in Oracle 12c.
Chapter 10, Core Administration, this chapter is more of a general administration and we are
going to explain the fundamentals of database administration. We are briefly cover
installation and configuration of database, setting up client and server connections,
monitoring database alerts, different tasks performed by DBAs on daily basis, patching,
backup and recovery, troubleshooting etc. We also cover data recovery advisor and how to
use the same to recover the data. Later we cover implementing flashback technology and
techniques for loading and unloading of data.
Chapter 11, Performance Management, in this chapter we discussed about designing the
database for optimal performance, monitoring the performance of database to improve the
same, analyze and identify performance issues and preforming real application testing to
analyze the performance. We will also see how to use resource manager and tune our
applications so that they perform the best.
Chapter 12, Storage, in this chapter we are going to introduce the logical and physical
structure of database and how they are different. We will also look into Oracle ASM and how
that can be used to manage database storage. We will talk about ASM disks and disk groups,
managing ASM instances and automatic space management in Oracle.
Chapter 13, Security, in this chapter we will discuss about developing and implementing
robust security policy for Oracle database. Auditing the actions in Oracle database, creating
password file and authenticating privileged users remotely using password file. Finally, we
will talk about encryption and how to use the same to encrypt Oracle data.

What you need for this book
This book needs an Oracle database 12c release 1 (12.1.0.1) software to be installed and a
container database to be created. You can use any flavor of operating system that are certified
with Oracle 12c release 12.1.0.1.
In terms of resources required, you should have at least 2GB of physical memory and around
10G of disk space.
For practicing ASM section, you need 4 raw devices added to the system for creating ASM
disks and disk groups.

Who this book is for
This book is for Oracle Admins who have working knowledge of Oracle administration and
now want to upgrade their knowledge to the latest version (Oracle 12c). This book is perfect
for those who wish to pass the OCP upgrade 1Z0-060 exam.

Conventions
In this book, you will find a number of text styles that distinguish between different kinds of
information. Here are some examples of these styles and an explanation of their meaning.
Code words in text, database table names, folder names, filenames, file extensions, pathnames,
dummy URLs, user input, and Twitter handles are shown as follows: "You can use the
DBMS_ADDM package's COMPARE_INSTANCES function to compare two periods within the same
instance."
A block of code is set as follows:
SQL> exec dbms_xdb_config.sethttpport(5500); 
 
PL/SQL procedure successfully completed.
New terms and important words are shown in bold. Words that you see on the screen, for
example, in menus or dialog boxes, appear in the text like this: "You can access the new ASH
analytics page using Performance | ASH Analytics as shown in the following screenshot."
Note
Warnings or important notes appear in a box like this.
Tip
Tips and tricks appear like this.

Reader feedback
Feedback from our readers is always welcome. Let us know what you think about this book-
what you liked or disliked. Reader feedback is important for us as it helps us develop titles
that you will really get the most out of. To send us general feedback, simply e-
mail feedback@packtpub.com, and mention the book's title in the subject of your message. If
there is a topic that you have expertise in and you are interested in either writing or
contributing to a book, see our author guide at www.packtpub.com/authors.

Customer support
Now that you are the proud owner of a Packt book, we have a number of things to help you to
get the most from your purchase.

Downloading the example code
You can download the example code files for this book from your account at
http://www.packtpub.com. If you purchased this book elsewhere, you can visit
http://www.packtpub.com/support and register to have the files e-mailed directly to you.
You can download the code files by following these steps:
1. Log in or register to our website using your e-mail address and password.
2. Hover the mouse pointer on the SUPPORT tab at the top.
3. Click on Code Downloads & Errata.
4. Enter the name of the book in the Search box.
5. Select the book for which you're looking to download the code files.
6. Choose from the drop-down menu where you purchased this book from.
7. Click on Code Download.
Once the file is downloaded, please make sure that you unzip or extract the folder using the
latest version of:
WinRAR / 7-Zip for Windows
Zipeg / iZip / UnRarX for Mac
7-Zip / PeaZip for Linux
The code bundle for the book is also hosted on GitHub at
https://github.com/PacktPublishing/OCP-12c-Upgrade-1Z0-060-Exam-Guide. We also have
other code bundles from our rich catalog of books and videos available at
https://github.com/PacktPublishing/. Check them out!

Downloading the color images of this book
We also provide you with a PDF file that has color images of the screenshots/diagrams used
in this book. The color images will help you better understand the changes in the output. You
can download this file from
https://www.packtpub.com/sites/default/files/downloads/OCP12cUpgrade1Z0060Examguide_C

Errata
Although we have taken every care to ensure the accuracy of our content, mistakes do happen.
If you find a mistake in one of our books-maybe a mistake in the text or the code-we would be
grateful if you could report this to us. By doing so, you can save other readers from
frustration and help us improve subsequent versions of this book. If you find any errata,
please report them by visiting http://www.packtpub.com/submit-errata, selecting your book,
clicking on the Errata Submission Form link, and entering the details of your errata. Once
your errata are verified, your submission will be accepted and the errata will be uploaded to
our website or added to any list of existing errata under the Errata section of that title.
To view the previously submitted errata, go to
https://www.packtpub.com/books/content/support and enter the name of the book in the search
field. The required information will appear under the Errata section.

Piracy
Piracy of copyrighted material on the Internet is an ongoing problem across all media. At
Packt, we take the protection of our copyright and licenses very seriously. If you come across
any illegal copies of our works in any form on the Internet, please provide us with the
location address or website name immediately so that we can pursue a remedy.
Please contact us at copyright@packtpub.com with a link to the suspected pirated material.
We appreciate your help in protecting our authors and our ability to bring you valuable
content.

Questions
If you have a problem with any aspect of this book, you can contact us
at questions@packtpub.com, and we will do our best to address the problem.

Chapter 1. Getting Started with Oracle 12c
Oracle released a new version of Oracle 12c in June 2013. This is by far the most advanced
version of the Oracle database containing some of the best features and improvements that
have been released so far. We are going to start looking into all the new features of Oracle
12c (12.1.0.1) in each of the coming chapters. Let's begin our journey with the installation of
Oracle 12c and what new things we see during installation.
In this chapter, we are going to cover the following topics:
Installation and configuration using OUI and DBCA
Using EM Database Express
Emergency monitoring, Real-Time ADDM, compare period ADDM, and
ActiveSessionHistory (ASH) analytics
Performing emergency monitoring and Real-Time ADDM
Generating ADDM compare period
Diagnose performance issues using ASH enhancements

Installation and configuration using OUI and
DBCA
Oracle 12c has lots of new features introduced and it all begins right from the creation of the
database. To begin with, when we create the database at the time of installation using Oracle
Universal Installer (OUI) or separately after installation using Database Creation
Assistance (DBCA), we can see new options have been introduced in the Oracle Call
Interface (OCI) and Database Configuration Assistant (DBCA). Oracle database 12c can be
created as a multitenant container database containing multiple pluggable databases. So when
we create the Oracle 12c database, we have an option to create the database as a multitenant
database with zero or more pluggable databases. A pluggable database provides an option to
consolidate many databases as pluggable databases into a single container database making it
easier to administer and maintain them together along with many more benefits.
DBCA in Oracle 12c lets you create three types of databases:
Normal databases (no multitenant container option): These are the same types of
databases that were created in the previous version.
Multitenant container databases: These are new types of databases introduced in
Oracle 12c, where we have multiple databases plugged into a single container database.
They mostly contain metadata information.
Pluggable databases: These are the user databases which are plugged into a multitenant
container database.
The following screenshot shows new options while creating the Oracle 12c database:

As you can see from the preceding screenshot, we can create the new database as a multitenant
container database (by checking the checkbox) or a normal non-container database as it used
to be until Oracle 11gR2 version. Also, if we select the option of creating a multitenant
container database, we can either create the database as empty with no pluggable databases or
a number of pluggable databases.

Managing PDB databases using DBCA
Before we check the details of configuring pluggable database using DBCA, I would like to
give a simple definition of pluggable database in order to understand the configuration
details. A pluggable database is a user created database containing application data. It has all
schema and non-schema objects and appears to end user as normal database. But pluggable
database needs to be part of main container database (CDB). Pluggable database cannot be
created without a container database.
You can manage and configure a pluggable database using DBCA. When you launch DBCA,
you get an option to manage pluggable database as shown in the following screenshot:
When you click on Manage Pluggable Databases, you get multiple management options for
pluggable databases as listed in the following screenshot:
The first option is to create a pluggable database. We can create a pluggable database

either manually or using DBCA.
The second option is to unplug a pluggable database from the container database. Again,
this can be done manually as well but DBCA gives us an option.
The third option is to delete a pluggable database.
The fourth option to configure a pluggable database is used for configuring a database
vault and/or label security for the pluggable database.

Using EM Database Express
Until the previous release of the Oracle database, we used to have built-in Oracle EM database
control which was a GUI tool to manage the Oracle database. Oracle EM database control
used to have complete access for an individual database and was even capable of doing most
of the administration tasks and intrusive changes to databases including shutdown/startup and
configuration changes.
Starting with Oracle 12c, Oracle EM database control has been deprecated and replaced by
Oracle EM Database Express. It is a lightweight monitoring tool providing a web-based
interface for performance monitoring, configuration management, diagnostics, and tuning.
The following figure shows how EM Express works:
EM Express is available out of the box after creation of the database, so every database has a
separate EM Express URL. It runs inside a database with minimal CPU and memory resources
because the database only runs SQL calls and UI rendering is actually done by web browser
on the client site. EM Express uses a web-based console that communicates with a built-in web
server available in XML DB. Once a request is made on the web console, an EM servlet
handles the request, including authentication, session management, compression, and so on.
The servlet processes requests for reports by running the required SQL in the database and
returns XML pages containing the output data. These XML pages are then rendered by the web
browser on the client site. Often a single request is made per page to reduce round-trips to the
database.
The following are the operational features of Oracle EM Express:

When the state of the database is changed, Oracle EM Express cannot perform any
operation
When the database is closed, we cannot connect Oracle EM Express
We cannot access EM Express if the database is in a mounted state.
Note
EM Express cannot be used to shut down or start up a database. We can however use it to
change any database parameter, as it is very good in diagnosing and analyzing
bottlenecks in a database.

Configuring EM Express
The Oracle database EM express configuration is very simple. If you are installing the Oracle
database using DBCA, it provides an option to configure EM Express as shown in the
following screenshot:
You only need to provide the EM Express port number. This port will be used in the URL to
access EM Express. If you have multiple databases on the same server, you need to make sure
to have different ports assigned to EM Express in each database. We cannot have one Oracle
database EM Express to monitor all databases in a host.
If you have manually created the database, you can configure the port as either SSL or non-
SSL:
dbms_xdb_config.sethttpport procedure for non-SSL connection using the command:
SQL> exec dbms_xdb_config.sethttpport(5500); 

PL/SQL procedure successfully completed. 
dbms_xdb_config.sethttpsport for SSL connections using the command
SQL> exec dbms_xdb_config.sethttpsport(5500); 
PL/SQL procedure successfully completed. 
You can also check the port number that is configured for EM Express using
dbms_xdb_config.gethttpport for non-SSL connections or dbms_xdb_config.gethttpsport
for SSL connections as shown in the following code:
SQL> select dbms_xdb_config.gethttpsport() from dual; 
DBMS_XDB_CONFIG.GETHTTPSPORT() 
------------------------------ 
                 5500 
Once you configure the port, EM Express can be accessed using the following URL:
https://<hostname>:<port>/em
In our case it would be https://192.168.56.20:5500/em. You can use system user to access
EM Express.

Access control for EM Express
You can use a system user and password to access EM Express. A system user has access to all
functionality of EM Express. Oracle also provides EM_EXPRESS_BASIC and EM_EXPRESS_ALL
roles which can be assigned to different users to control access to functionality in EM
Express.
EM_EXPRESS_BASIC, which includes SELECT_CATALOG_ROLE, gives read-only permissions to
users, whereas EM_EXPRESS_ALL gives all administrative privileges.

Exploring Oracle database EM Express
Oracle EM Express is a lightweight web-based tool for managing the Oracle database 12c.
This tool is built inside the Oracle database and does not have any moving parts outside. As
explained previously, we need to configure a port on which we can communicate with an
XMLDB component inside the database and EM Express starts working. We will be looking
into the different sections of EM Express and how this tool will help DBAs in managing the
Oracle database 12c.
Oracle database EM Express has four major sections:
Configuration
Storage
Security
Performance
Each section provides key information in the database in respect to namespaces. Before
checking details about each section, let's quickly go over the home page of EM Express.
The home page provides a quick overview status of the database, the uptime, and other
database-related information in the Status section. It also has a Performance section, which
provides information about database performance in terms of IO, waits, CPU, and so on.
The home page provides resource utilization in terms of host CPU, memory, and active
sessions. We can also see currently running jobs in the Running Jobs section as well as top
bad performing SQL in the SQL monitor - Last Hour section. The SQL monitor section
provides performance metrics for top bad performing SQLs in the database.
The following shows the screenshot for the home page of EM Express:

At the top of the home page, we can see four different menus which can take us to different
sections in EM Express. Let's go through the sections one by one:
Configuration: This includes initialization, parameters, memory, database feature usage,
and current database properties. Clicking each of these areas will take us to the respective
page and provide detailed information about each of these sections.
For example, if we click on Initialization Parameter section, it will take us to the
initialization parameters page, where we can view or modify initiation parameters.
On many pages, when we take some action (for example, changing the parameters and so
on), we have a Show SQL button which provides us the SQL that will be run in order to
take the required action. DBA can copy the SQL and run it in SQL prompt to make same
changes.
Storage: This includes table space, undo management, redo log groups, archive logs,
and control files.
Security: This includes users, roles, and profiles.
Performance: This menu includes the performance hub and SQL tuning advisor.

Emergency monitoring and Real-Time ADDM
Until now it has been very difficult to understand the root cause of the issue causing database
slowness especially in situations where the database has become completely inaccessible.
Getting the database back to normal usually required bouncing the database.
But what if we have some mechanism to get inside the database and query some statistical data
during the time when it has become completely slow and is inaccessible to the normal user?
This will help a lot in understanding the root cause and in many cases even fixing the root
cause, thus preventing the bounce of the database. This is exactly what emergency monitoring
and Real-Time ADDM does.

Slow system
We have observed many times that a database becomes slow. When this happens, we see that
either the connection to the database is slow, or queries run slow, or even EM page refreshes
run slow. Also, many times users complain that applications are running slow.
With DBA, how do you try to find out the reason for the slowness? It could be a session
holding too many locks, or some DML which is causing slowness or any other issue. DBA
usually start the performance analysis using a regular ADDM report. But we can see
following limitations in generating an ADDM report:
Since the system is slow already, running ADDM is not advised as it takes up resources
in the database. Also, ADDM report generation may not finish as well because the
database is starving for resources.
Sometimes it's not possible to even connect to the database as it becomes slow and hangs.
As a final resort, DBA can go for bouncing the database. But after the database is bounced,
you might be losing the analysis data from the memory and it would be really difficult to
identify the root cause of the problem that caused the database to slow down. If we don't know
the root cause, we cannot apply the fix and if we don't fix the issue, it can cause database
slowdown again and will incur another downtime.
So to identify the root cause before we bounce the database, it's important for us to have some
kind of a lightweight tool which can get the required analysis data from the memory buffer
without taking much resources and provide us the root cause of the problem. Oracle 12c has
introduced a new tool, Emergency Monitoring, for doing analysis in such a situation.

Emergency monitoring
In the previous release, we had the memory access mode, which could be enabled and
disabled explicitly using enterprise manager. Starting the memory access mode in enterprise
manager starts a collector process, which was used to collect data from memory for further
analysis.
In Oracle 12c, we don't have the memory access mode in enterprise manager. Instead we have
Emergency Monitoring. We don't have to switch between modes in case of emergency
monitoring. Emergency monitoring can be enabled by accessing the Emergency Monitoring
page from Oracle database cloud control as shown in the following screenshot:
Before shutting down the database, you can launch emergency monitoring, which allows you
to perform a quick performance analysis of a database instance even in a situation where we
cannot connect to the database. This is possible because emergency monitoring connects to
database SGA memory in a diagnostic mode bypassing all other IO and global resources.
As soon as you access the emergency monitoring page in Cloud Control, an agent connects
directly to SGA data, bypassing the SQL layer to get performance statistics. Data collection
stops when you navigate away from the emergency monitoring page to regular performance
monitoring.
Emergency monitoring uses data from ASH buffers in memory to populate a performance
page. ASH buffers generally contains the data for last 60 minutes and these buffers are rolling
buffers so older data gets overwritten. It may not have enough history in following cases:

If there is too much activity in the database, ASH buffers may get filled up quickly
If there is huge resource contention, the process responsible for writing data to ASH
buffers may get stuck and so will not able to write data to ASH buffers
When you enable emergency monitoring, you can view following information:
Emergency performance page refreshed with real-time performance data
ASH data and hang analysis table that shows blocking and blocking sessions
The hang analysis page helps you to identify the exact session or, in some situations, multiple
sessions which are blocking all other sessions. You can kill the main session which is
blocking all other sessions and causing the database to hang. This should fix the hang issue
right away in most situations. In a certain situation, if it doesn't help or if there are no
blocking sessions, then you have to go for database reboot. The following shows emergency
monitoring page in EM Cloud Control 12c:

Real-Time ADDM
Emergency monitoring is good for identifying the top blocking sessions or checking ASH
real-time data to understand the problem with the database. But sometimes the root cause is
complex and emergency monitoring is not helpful in situations where we are not able to see
any top blocking sessions or ASH data does not show any concrete issue with the database.
Real-Time ADD analysis provides a complementary analysis to emergency monitoring. It
provides a deeper root cause analysis that emergency monitoring does not provide.
Real-Time ADDM analysis is available through Oracle Enterprise Manager 12c and provides
the detailed analysis of the issue along with the root cause of the issue. It helps DBA to resolve
the critical issues such as database hang without the need to restart the database. This feature is
different to emergency monitoring in that you log into the database during the database hang
to get the real-time statistical data to fix the issue. Emergency monitoring does not provide a
root cause analysis.

Working of Real-Time ADDM
Real-Time ADDM works in similar way to normal ADDM to analyze performance. Normal
ADDM uses an AWR snapshot to analyze and provide the findings and recommendations to
let you know what needs to be changed to improve the performance.
Real-Time ADDM uses the data from ASH buffers in SGA memory. Real-Time ADD does not
use an AWR snapshot to perform the analysis. With Real-Time ADDM, you can connect to the
database in either mode depending on the state of the instance:
Normal mode: If you are able to connect, Real-Time ADDM always tries to connect
using the normal mode as SYSDBA.
Diagnostic mode: If you are not able to connect using the normal mode, Real-Time
ADDM uses the diagnostic mode to connect to the database. So if the database is hung
completely and you are not able to make any connection to the database, Real-Time
ADDM can still collect the performance data from the database by connecting directly to
the SGA memory.
For example, you can encounter such a situation when you have exhausted all the
connections to the database and you cannot make another connection. Similarly, you have
filled up all the space where audit trace files are stored and when you try to make a new
connection, Oracle wants to create a new audit trace file and there is no space left to do
that. In that case, you cannot log into the database and so Real-Time ADDM will help you
a lot.

Automatic triggering of Real-Time ADDM
Oracle also enables triggers, which automatically start collecting Real-Time ADDM analysis
data. This is required so that DBA does not have to use Real-Time ADDM and collect analysis
data manually whenever there is an issue. So Oracle makes sure that if a certain threshold is
crossed, it should automatically start collecting Real-Time ADDM analysis data that would be
useful later to perform the analysis.
To automatically collect Real-Time ADDM analysis data, Oracle relies on the Manageability
Monitor (MMON) background process and runs every 3 seconds using in-memory data
without the need for any latches or locks. This helps in collecting Real-Time data without
using any resources. The following are the triggering thresholds used by MMON to start
collecting Real-Time ADDM data and store in the Automatic Workload Repository (AWR):
High load: If Average Active Sessions (AAS) are greater than three times the number of
CPU cores or I/O active sessions impacted by I/O performance on the single block read
time
CPU bound: Active sessions greater than 10 percent of total load and CPU usage greater
than 50 percent
Hung sessions: Hung sessions are more than 10 percent of the total number of sessions
Data collected for Real-Time ADDM analysis is stored in the Automatic Workload
Repository (AWR) view in DBA_HIST_REPORTS and DBA_HIST_REPORTS_DETAILS.
To make sure that automatic triggering does not happen too often, Oracle has implemented
checks and fulfilling those checks only will trigger an automatic collection of Real-Time
ADDM data. To avoid any performance impact caused by collecting the data, new reports
won't be generated if a Real-Time ADDM report was generated in the past 5 minutes by an
automatic trigger. Also, to avoid multiple triggers pointing towards a performance problem
with same severity, a new trigger will come into effect if it has an impact of 100% or higher
when compared to previous impact within the past 45 minutes. This applies to reports
triggered by the same triggering issue.

Generating Real-Time ADDM report
We can use the following function to generate a Real-Time ADDM report:
Select dbms_addm.real_time_addm_report() from dual; 
This query, which uses the dbms_addm function real_time_addm_report, will get you a Real-
Time ADDM report for the last 5 minutes.
Alternatively, you can get a Real-Time ADDM report from EM Database Express by doing the
following:
1. Click the Performance tab on the home page of EM Database Express.
2. Click Performance Hub.
3. Click Current ADDM Findings.
You can also use Real-Time ADDM from EM Cloud Control 12c. The following
screenshot shows the Real-Time ADDM menu from the Performance tab dropdown in
EM Cloud Control 12c:

Once you click on Real-Time ADDM, it uses the data from ASH buffers in the SGA
memory to perform the analysis. In the following screenshot of the ADDM page in EM
Cloud Control, you can see the findings tab, hang data tab, and a few other tabs along
with the statistics tab:

Emergency monitoring versus Real-Time ADDM
If you are confused about when to use emergency monitoring and when to use Real-Time
ADDM, you can follow these guidelines.
Whenever a system becomes slow or hung, always start with emergency monitoring.
Emergency monitoring is useful for finding any issue that stands out above other small
issues.
For example, if we have a blocking session which is using lot of resources and causing the
database to hang, emergency monitoring will highlight such sessions immediately. We also
have ASH real-time data for doing the analysis of what is going on during the time of issue.
If nothing obvious stands out and you want to go for bouncing the database, you can give it a
shot and run Real-Time ADDM analysis.
Real-Time ADDM does a much deeper analysis to find the root cause of the issue. It uses hang
analysis and other key metrics such as IO to understand the analysis data. You can check hang
analysis data as well as top activity data in Oracle Cloud Control 12c to understand the
analysis and root cause. Real-Time ADDM also provides you with findings and
recommendations just like normal ADDM that you can implement to resolve the issue.

Generating ADDM compare period
Imagine that your database performance is degrading over time and you need to understand
what changed over the period of time which is causing performance to degrade. To check this
out manually, you need to have an AWR snapshot when the database is running good (or a
preserved AWR baseline) and take an AWR snapshot when database is running slow. You can
then compare the two AWR reports and analyze what has changed which could have resulted
in the degraded performance.
Comparing two AWR reports is both time consuming and error prone. That's why in Oracle
11g, Oracle has introduced an AWR comparison period report.
It takes two different snapshots as input and performs comparison of statistical values of two
different time periods (a good one and a bad one), providing a single report that highlights
the difference in performance between the two periods.
In the case of the AWR compare report, you can select two sets of snapshots from the existing
preserved snapshots. So the first set of snapshots (two snapshots) will be from the previous
timeline and another set of snapshots will be from the time where we encountered the issue.
You can generate an AWR compare period report either in text format or HTML format and
the report provides the difference values in critical areas such as wait events, response times
of top queries, OS statistics, and instance activity.
In addition to comparing the snapshot at two different time period, an AWR compare report
can also be generated for two different DB replays. We can set up different environments and
run replay multiple times comparing the AWR statistics for two different snapshots or
replays.
Regardless of the nature of the comparison-two different time periods or two different DB
replays-you still need to analyze the huge volumes of performance metrics summarized in the
report and identify the change that has happened between old time and current time to identify
the root causes of the performance difference between them. The reports don't map the root
causes to performance changes. So you have to guess what change could have caused the
performance degradation.

How does the ADDM compare report help?
The ADDM compare report performs cause-to-effect analysis:
1. It first identifies the system changes that may have caused the performance degradation.
For example, it identifies a configuration change such as a DB version change or
workload change because of new SQLs. These causes might lead to performance
degradation.
2. It then identifies the effect of these changes. For that it runs an ADDM analysis for the
base period and an ADDM report for the current period.
3. Finally, it maps the effect to the cause by using certain pre-defined rules.
Unlike the AWR compare periods report, which tells you what exactly is the difference in
performance between two periods, the compare period ADDM report tells you what you can
attribute the changes to.

Workload commonality
Workload commonality defines how close the two workloads that we are comparing are. Are
we comparing similar things? Are we comparing similar SQL statements in two periods? Is it
the same application running during the two time periods?
It's good to compare two workloads by first understanding if the two workloads are similar.
The compare period ADDM report shows workload commonality and if the report shows
workload commonality close to 100%, it means that we are comparing a similar workload.
But if workload commonality is less than 70%, it means that we are comparing different
workloads. Less workload commonality means that the baseline AWR snapshot was having a
different load than the one we have when the database became slow.
Usually AWR compare periods for database replay shows very high workload commonality
as we are running the same workload again. If you are comparing two different time periods,
make sure that workload commonality is reported high in the ADDM compare report.

Generating ADDM compare period
We can generate a ADDM compare period using EM Cloud Control 12c or using the
DBMS_ADDM package. Let's look into the details of each method.
Using EM Cloud Control 12c
The easiest way to generate the ADDM compare period report is to do it using EM Cloud
Control 12c. To do this click on the Performance tab and go to the Compare Period ADDM
page under AWR as shown in the following screenshot:
Here, you can perform the following steps:
1. Select the time period when the performance was bad.
2. We need to compare the time period in step 1 with the base period when performance
was good and acceptable. You have the following three options to define the base period
as shown in the following screenshot:

The first option allows you to select an offset of one snapshot. So if the comparison
period is set for snapshot x, the base period will be for snapshot (x-1). In the same
option, you can select the offset of a day, so the base period will be same time as the
comparison period but it will be of the previous day. Similarly, you have a third
choice of selecting a base period of a week ago. It means the base period time will
be same as the comparison period time but it would be a week older.
The second option lets you select the baseline, either an out-of-the box system
moving window baseline or any other baseline that represents a normal working
performance.
The third option let you select a customized time for your base period. Oracle will
automatically convert these customized times into snapshots.
3. If you are selecting a time range, it will automatically adjust the base period to the range
of AWR snapshots that are closer to the selected time.
4. Run the report and it will provide the details of the difference between two periods as
shown in the following screenshot:

The report shows various details:
The overview at the top of the screenshot shows two time periods. Between the two time
periods, the port shows that they are 100% identical (SQL Commonality = 100%). It
means that the load is similar between the two time periods. But even if the commonality
is less, it's still worth taking a look at the findings.
In the bottom-half of the page (in the following Overview section), we have three tabs -
Configuration, Finding, and Resource as shown in the following screenshot:
The Configuration tab shows changes and differences that happened between the
two time periods. If there is a change in the instance parameter, you should see that
change under this tab. This tab only provides you information about configuration
changes but it does not tell you why performance has degraded and if any of the
changes have caused performance degradation.
The Finding tab is important. It explains what caused the performance to degrade.
You can choose to view only the area where performance has degraded or
performance has improved or both. The Findings tab attributes the performance
degrade to the change that happened between the two time periods. So this tab tells
us the root cause of the performance degrade.
The Resource tab provides the resource consumption of two time periods. It tells us
how much CPU, memory, and IO was consumed in the base period and the
comparison period.

Using DBMS_ADDM package
You can also use new functions in the DBMS_ADDM package to generate the ADDM
compare period report. This package has many functions which helps to create the report for
different situations.
You can use the DBMS_ADDM package's COMPARE_INSTANCES function to compare two
periods within the same instance. Here's an example:
select DBMS_ADDM.COMPARE_INSTANCES ( 
        base_dbid               => 1465910504, 
       base_instance_id         => 1, 
       base_begin_snap_id       => 152, 
       base_end_snap_id         => 153, 
 
        comp_dbid               => 1465910504, 
       comp_instance_id         => 1, 
       comp_begin_snap_id       => 232, 
       comp_end_snap_id         => 233, 
        report_type             => 'XML') 
from dual; 
In the preceding function, lines in the first block represents the details of the baseline period
and lines in the second block represent the comparison period (degraded performance). The
following are the parameters to be supplied to the preceding DBMS_ADDM.COMPARE_INSTNACES
function:
BASE_DBID: DB ID of the base period
BASE_INSTANCE_ID: Instance ID of the base period
BASE_BEGIN_SNAP_ID: Snapshot ID of the beginning of the base period
BASE_END_SNAP_ID: Snapshot ID of the end of the base period
COMP_DBID: DB ID of the comparison period
COMP_INSTANCE_ID: Instance ID of the comparison period
COMP_BEGIN_SNAP_ID: Snapshot ID of the beginning of the comparison period
COMP_END_SNAP_ID: Snapshot ID of the end of the comparison period
REPORT_TYPE: Output type for the report-XML or HTML (the default is HTML)
In addition to the COMPARE_INSTANCES function, the DBMS_ADDM package offers the following

functions:
COMPARE_DATBASES: This procedure generates the ADDM compare report for the entire
database. You can compare the same database at two different time periods (two
snapshots set) or you can compare two different databases over two time periods.
COMPARE_CAPTURE_REPLAY_REPORT: This generates the ADDM compare report between
workload capture and workload replay.
COMPARE_REPLAY_REPLAY_REPORT: This generates the ADDM compare report between
two different replays. This is helpful in understanding how our changes are having effect
on the database.

Diagnosing performance issues using ASH
enhancements
One of the limitations of AWR is that it's not very useful for analyzing the issue that is
currently happening in the database. AWR snapshots are taken at an interval of 1 hour by
default. We can change the snapshot interval to a lower value. But we cannot take a snapshot
every second. This is where ASH comes into the picture.
The Active Session History (ASH) contains the history of recent session activity. Because
recording session activity is expensive, ASH samples V$SESSION every second and records the
events that the sessions are waiting for. Inactive sessions are not sampled. The sampling
facility is very efficient because it directly accesses the internal database structures. ASH is a
rolling buffer in memory and earlier information will be overwritten by the new information.
How much data an ASH can store in the database depends on the activity of the database.
AWR data is generated from ASH data in memory. MMON process filters one out of every 10
samples from ASH and stores the same in AWR. So ASH data is more accurate as we take
samples every second, whereas AWR data is not accurate as we are getting one out of 10
samples from ASH.
ASH has huge amount of data and it would be a complete waste if we were not able to utilize
that data for our analysis.
In the previous Oracle database version, the top activity page on the performance tab of the
enterprise manager had very limited options and we were not able to perform detailed
analysis. Oracle 12c has enhanced the op activity page and provided several options to
perform analysis.

Limitations on top activity page in previous release
The following are some of the limitations of the ASH top activity page in the previous
release. We will also see how Oracle 12c has overcome these limitations:
You cannot switch dimensions on the area chart on the ASH top activity page on Oracle
11g.
The Top-SQL table below the Top Activity area chart is fixed to displaying Top SQLs
only, while the Top Sessions table has only a few dimensions that it can display, such as
Top Sessions or Top Modules.
The information does not harness the full value of ASH data as some key dimensions that
are actually captured with the ASH data are not displayed at all.
The slider used to select the time period for the detailed section is a fixed width (5
minutes real-time, 30 minutes historical).
The visualization is restricted to a stacked area chart by wait classes so you always see
the wait classes stacked over one another.
The data cannot be displayed as an active report, that is, it cannot be sent offline to other
users for review.
There are limited drilldown capabilities that simply send you to other pages.

New ASH analysis page
The limitations described in the previous section are addressed in the new ASH analysis page.
You can access the new ASH analytics page using Performance | ASH Analytics as shown in
the following screenshot:
This page provides capabilities to filter dimensions based on different filters such as wait
events, instance, service, module, and so on, as shown in the following screenshot:

Based on the dimension that we are selecting, the underlying graph changes. As you can see,
the following graph has changed from the wait class to SQL ID:
Clicking any of the SQLs will set a filter for that SQL and show resources consumed for that
SQL as shown in the following screenshot:
You can also select the dimensions for the left and right-hand tables in the bottom-left and
right parts of the page as shown in the following screenshot:

If you change the dimensions in one of the preceding tables to, let's say, SQL, we can see the
top SQLs as shown in the following screenshot:
So, as you can see, the ASH Analytics page is created keeping in mind all the potential that
ASH data can provide to make the most use of that data. We have been provided with huge
flexibility to make sure that ASH data can tune our databases.

Summary
In this chapter, we saw new features related to the installation and configuration of Oracle 12c.
We saw Oracle EM Express, a new web-based tool introduced in Oracle 12c which serves as a
replacement for the Oracle database control GUI tool. We also saw how emergency
monitoring and Real-Time ADDM can be used during the time of a database hang situation to
investigate and fix the root cause.
This chapter also introduced you to the ADDM compare report, which is a new feature in
Oracle 12c. We compared the ADDM compare report with the AWR compare report and saw
how the ADDM compare report is more helpful in identifying the root cause. Finally, we saw
some good enhancements done to the ASH Top Activity page and how this new GUI page
helps in overcoming the shortfalls of the ASH Top Activity page in older releases.
In the next chapter, we are going to introduce you to the Oracle multitenant architecture its
components. We will also create and configure a container database (CDB) and see various
methods of creating a pluggable database (PDB). We will also see how we can migrate a
non-CDB database as PDB into a CDB.
Stay tuned and move on to the next chapter.

Chapter 2. Multitenant Container Database
Architecture
Multitenant Container Database (MCD) architecture is the most important and popular
feature of Oracle 12c. Oracle has introduced multitenancy to consolidate many databases into
one. But the advantages of multitenant architecture go much beyond this consolidation. In this
chapter, we are going to introduce you to the concepts of multitenant architecture, describing
what it contains and how it works. We are going to look into creating and configuring the
Oracle container database (CDB) using the create database command.
We are also going to provide details about various options for creating a pluggable database
(PDB) and how to plug a PDB into a CDB. We will conclude this chapter by checking various
ways of migrating a non-CDB database into a CDB database as PDB.
The topics that we will be covering in the chapter are as follows:
Identifying the benefits of multitenant architecture
Explaining root and multitenant architecture
Creating and configuring CDBs and PDBs
Creating and configuring CDBs
Create and configuring PDBs
Migrating non-CDBs to PDBs

Challenges with non-container databases
Oracle customers are facing challenges with respect to the ever-increasing number of
databases. There are many times when each business unit and business subunits need separate
databases for managing transactions for their departments. Because of this, DBAs end up
creating many databases belonging to business units and subunits. These databases usually
have dedicated hosts and need teams of full-time DBAs to maintain so many databases.
These are individually created databases for each business unit or subunit and following are
the challenges associated with those databases:
They do not use complete resources available at host level and quite often resources are
underutilized.
The company infrastructure costs increase as they have to maintain hosts for so many
databases. This may also include corresponding standby hosts if they are configured for
availability.
Not all databases are tier 1, so do not need 100% attention by DBAs.
They need significant time and effort in order to maintain, tune, back up, and patch these
databases.
Considering these challenges, Oracle has taken a step ahead to re-architect traditional
databases into container databases. The Oracle 12c database comes with container database
options which can be used to plug multiple Oracle databases as pluggable databases into a
single container database (hereafter referred to as a CDB).
Consolidating many non-CDB databases into a single platform reduces instance overheads,
avoids redundant copies of data dictionaries, and consequently storage allocation, and
benefits from fast provisioning.
Each pluggable database (hereafter referred to as a PDB) will be seen as a separate database
by the application as it was in the previous release. Application developers can connect to a
PDB in the same way they were connecting to databases in the previous release.
Let's check out some of the benefits of using multitenant architecture.

Benefits of multitenant architecture
The following are the most significant benefits of multitenant architecture:
Lower costs: You will need fewer hosts and storage.
Easy and fast provisioning: You can create new PDBs from seed or from an existing
PDB. You can also plug and unplug databases very quickly without any changes on the
application site.
Faster upgrade and patching: Instead of upgrading individual databases, you just need
to upgrade your CDB, and all PDBs which are plugged into that CDB will get upgraded
automatically. So all databases are patched together. You can also unplug a PDB from a
lower version CDB database and plug it in to a higher version CDB to upgrade an
individual PDB.
Reduced administration efforts: As a DBA, you need to take care of only one database
with one set of background processes/instances, and one set of data files instead of
managing many databases.
Separation of duties: DBAs can be restricted to connect only to specific PDBs. Only
main DBAs can have privileges to connect to a CDB and manage PDBs. Other DBAs can
provide access to only the PDBs that they are supposed to manage.
Having spoken about the benefits of using multitenant architecture, it's important to mention
that multitenant architecture is an option and if we don't use this option while creating a
database, we will end up creating normal non-container databases that we saw in previous
versions. So the following configurations are possible in Oracle 12c:
Multitenant configuration (CDB database): A container database consisting of zero,
one, or more PDBs
Non-CDB configuration: This is the same as the pre-Oracle 12c database architecture

Root and multitenant architecture
Multitenant architecture contains a CDB, a seed database, and one or more PDBs. A pluggable
database is very similar to normal databases and contains application schemas, tablespaces,
and tables. PDB databases appear to application end users as separate databases but they are
actually part of the main CDB databases.
Administration-wise, PDB databases are different form normal non-CDB databases in the
following ways:
PDB databases do not have their own background process or memory allocation. They
share background processes and memory of CDB databases.
Not all parameters are modifiable in PDB databases. There are certain parameters that
can be modified at the PDB level. You can check the ISPDB_MODIFIABLE parameter in the
V$SYSTEM_PARAMETER view to check what parameters can be modified at the PDB level.
If the CDB instance goes down, all PDBs go down. But we have the flexibility to start and
stop one or more PDBs if the CDB instance is in an OPEN state.
Until the previous release, all the databases were standard Oracle databases and not container
databases. A container database is one which can contain multiple standard databases plugged
into it. So the standard databases in the previous release can now be plugged into a single
container database and they are now called pluggable databases in Oracle 12c.
We can also create a standard non-container database and it looks exactly like we have in the
previous release.
Oracle multi-tenant container database contains the following databases:
One root container database:
A root container is just like standard Oracle database containing Oracle-supplied metadata in
the form of data dictionary tables and views. When we create a container database, this root
container gets created first by default and has a name CDB$ROOT. This is the main data
containing all Oracle supplied code and is required for the working of other pluggable
databases. PDB databases cannot work without a root container database.
The root container database is the one which has an instance associated with it. It also includes
controlfile, online redo log files, SYSTEM and SYSAUX tablespaces, UNDO tablespaces,
pfile, spfile and so on. SGA and PGA memory are attached to this main root container. Only
when we shut down the root container is memory released.
One seed database:
A seed database gets created along with the main container database CDB$ROOT and name of the
seed container database is PDB$SEED. As seen in Chapter 1, Getting Started with Oracle 12c,

we have an option to create the Oracle 12c database as a container database and choosing this
option will create a seed database. The seed database contains SYSTEM, SYSAUX, and TEMP
tablespaces by default. The seed container is used for creating new pluggable databases.
Zero or more user-created PDBs:
These are pluggable databases (PDBs) that are created for storing user data. These are actual
user databases containing application data. We can also consolidate multiple independent
databases as PDBs inside a single root container database. These PDBs cannot work without a
root container and have to be associated with one and only one root container.
We can create multiple PDBs in single container databases. There are different methods of
creating PDBs. Using the seed database to create a PDB is just one method. We can also clone
existing PDBs from the same container database or different container databases. We can also
make other non-CDB Oracle 12c databases as pluggable databases in our container database.
Each of these databases, the root, the seed, and the PDB, are called a container and have a
unique container ID (CON_ID) and container name (CON_NAME). Each PDB also has a
global unique identifier (GUID) to identify it.

In the preceding figure, we can see that the container database is the same as the normal
database, having all the required components as the normal database. The pluggable database
has its own SYSTEM, SYSAUX, and DATA tablespaces. All other components such as the
UNDO tablespace, online redo logs, control file, archive logs and so on, are common for the
container database and pluggable database.
When compared to the Real Application Cluster (RAC), the architecture is exactly opposite.
In RAC we have multiple instances and one database. In a multitenant architecture we have a
single instance and multiple databases.
We can query the V$CONTAINERS view for information about all containers within a CDB:
SQL> select con_id, dbid, name, open_mode from v$containers; 
   CON_ID       DBID NAME                           OPEN_MODE 
---------- ---------- ------------------------------ ---------- 
        1 1441716604 CDB$ROOT                       READ WRITE 
        2 3759262022 PDB$SEED                       READ ONLY 
        3 2994499415 PDB1                           READ WRITE 
        4 1789448398 PDB2                           READ WRITE 
        5 2325602183 PDB3                           READ WRITE 
SQL> 
We can use v$containers view in mount mode as well to check all the containers that are
attached to the instance.

Separation of metadata
The key idea behind creating the container database is to separate Oracle-related metadata and
user-related metadata by placing these two in separate containers. This can be thought of as
horizontal partitioning. The root container contains Oracle metadata. It has all Oracle
supplied DBMS packages and metadata tables (X_$*, COL$, TAB$, SOURCE$ tables, and so on).
Initially, when the database is created with the root container, all Oracle-supplied packages
and metadata are created in the root container in SYSTEM and SYSAUX tablespaces.
Later, when a PDB is created, it has its own system and SYSAUX tablespaces but the Oracle
metadata is not duplicated in the PDB database. Instead, Oracle provides pointers (metadata-
links and object-links) from PDB databases to Oracle-supplied objects to allow these
system objects to be accessed without duplicating them in the PDB. That way the PDB has all
the required pieces (in the form of links to Oracle-supplied objects in the root container) to
complete the environment for the database. That's why there is no need to upgrade individual
PDBs and upgrading a CDB will upgrade all PDBs in that CDB.
The following figure provides a better view of the separation of metadata between the root
container and the user container:
In the preceding example, let's say we have EMP and DEPT tables in one of the schema in the
PDB database. The metadata about the EMP and DEPT tables will be physically stored in PDB
metadata tables such as OBJ$, TAB$, and so on, but the definitions of these metadata objects and

Oracle-supplied metadata in these tables are coming via pointers to objects in the root
container.

Shared and non-shared objects
Shared objects are the ones that are visible and accessible to all PDBs and non-shared objects
are the ones that are specific to a container. All Oracle-supplied metadata are shared objects
and are accessible to all PDBs. These includes DBMS_* packages and metadata tables.
An Oracle supplied shared object is either metadata-linked or object-linked:
Metadata-linked objects store metadata about dictionary objects only in the root
container. Each PDB has a private data copy of an object pointing to a metadata link
stored in the root. For example, each PDB has its own metadata describing its own data.
But the metadata for the dictionary itself must be shared. All the dbms_xxx packages are
stored only in the CDB$ROOT container and the PDB has metadata links for them.
An object-linked object and its data resides in the root only and is shared by all PDBs.
For example, some dictionary data must be shared as well, such as some reference tables
(AUDIT_ACTIONS) or common repositories (AWR data exposed as DBA_HIST_). They are
stored only in the CDB$ROOT container and each PDB defines a view with just an object
link for them.

Managing a CDB
In a container database, we have a concept of local users and common users. We will see this
in a forthcoming topic. But for now we need to understand that a SYS user is an Oracle-
supplied common user in the container database which has all permissions to manage CDBs
and all PDBs. SYS is a container database administrator (CDBA) which can perform all
admin tasks on the container database including the unplugging, plugging, and dropping of
PDB databases.
Whenever we create a pluggable database, we have to create a PDB administrator (PDBA)
user. This PDB admin user is assigned a PDB_DBA empty role and is responsible for managing
that specific PDB. It is a local user in a PDB and does not exist in a CDB or other PDBs which
are attached to that CDB. The PDBA has privileges to administer PDB databases including
tablespace management, local user management in PDBs, and other schema-related changes.

Organization of CDBs and PDBs
A CDB has a background process and memory allocation (SGA and PGA) whereas a PDB
does not have any of these. A PDB is attached to an existing CDB and uses the same
background process and memory of a CDB. There are certain files which are common
between CDBs and PDBs and some files which are exclusive for CDBs and PDBs. Let's check
out common entities and exclusive entities between CDBs and PDBs.

Common entities
The following are the common entities that we see between a CDB and PDBs that are plugged
into that CDB:
Background processes: There's a single set of background processes for a CDB. PDBs
don't have any background processes attached to them.
Redo log files: These files are common for an entire CDB, with Oracle annotating redo
log entries that identify the container to which the redo entry is associated. Redo log
groups are created at CDB level only.
Memory allocation: This happens at the CDB level only. When the CDB is started, an
instance is started. No instance is attached to PDBs. An instance is attached to a CDB.
Control files: We have a single set of control files for a CDB. It has information about
PDBs.
Oracle metadata: As explained before, Oracle metadata resides in a CDB and SYSAUX
tablespaces. We have link objects called metadata-linked objects and object-linked objects
to access this metadata in PDB databases.
Undo tablespace: We have only one undo tablespace created at the CDB level which is
also used by a PDB. We cannot have a separate undo tablespace for a PDB. Strangely
though, if we try to create the undo tablespace in the PDB database, the command to
create the undo tablespace succeeds, but neither the undo tablespace nor the data file gets
created in the PDB.
Temporary tablespace: By default, we have a single temporary tablespace or tablespace
group at the CDB level and this serves as a default temporary tablespace for the CDB as
well as the PDB. However, if the PDB admin exclusively creates a temporary tablespace
in a PDB and assigns that as the default temporary tablespace for the PDB, then that
temporary tablespace will be used as the default temporary tablespace.

Exclusive entities
The following are the exclusive entities between the CDB and PDBs. These are the entities
which are not commonly shared and exist separately for the CDB and PDBs that are plugged
into that CDB:
User tablespaces: A PDB has its own user tablespaces and these are not shared by the
CDB or other PDBs. Tablespaces in the CDB belong to only that CDB and are not visible
in any of the PDBs.
Temporary tablespace: As mentioned, if the PDB admin exclusively creates a temporary
tablespace and makes it the default temporary tablespace for the PDB, then it becomes the
exclusive temporary tablespace for that PDB. In that case, the temporary tablespace of the
CDB will not be shared by the PDB.
Local users and local roles: Users and roles that are created in a PDB will remain in that
PDB only. These users and roles will not be visible in the CDB or other PDBs.
Local metadata: Schema objects created in a PDB will have their metadata stored
exclusively in that PDB. This metadata is not shared with other PDBs.

Data dictionary views
Oracle 12c has introduced a new set of data dictionary views called CDB_XXX views. It's
important to understand what data each of these views can display in different containers and
to different users with different privileges:
USER_XXX: These views show all objects which are owned by the current user. This
behavior is the same in the container database (CDB) as in a PDB. So if a user connects
to the CDB and queries USER_OBJECTS, that user will see a list of all objects owned by
him and which exist in the CDB only. The user won't be able to see any objects created by
him in other PDBs. When that user connects to a PDB and queries USER_OBJECTS, he will
see objects that are owned by him and the ones that exist in that PDB.
ALL_XXX: These views list all objects in the container for which a user has privileges.
If a user queries this view in the CDB, he will see objects only in that CDB on which he
has privileges. Similar is the scope for a PDB.
DBA_XXX: These views list all objects in the container. The behavior is the same as the
previous release except the scope of these views has changed from database to container.
So in a multitenant container database, DBA_XXX will list objects in a container rather
than the entire database. If a user queries DBA_OBJECTS in the CDB, he will see objects
existing in that CDB only. He will not see any objects in other PDB databases. Similar is
the scope in PDB containers.
CDB_XXX: These are new views introduced in Oracle 12c and their scope depends on
the container in which they are being queried. If CDB_XXX views are queried in PDBs
then their output is similar to DBA_XXX views and shows data belonging to PDBs only.
For example, a CDB_OBJECTS view when queried in a PDB, will list all objects in that
PDB. It won't list any objects in other PDBs or the CDB. However, if a DBA queries
CDB_XXX views in the CDB, he will see data for the entire database including all PDBs.
CDB_XXX views have all the same columns as DBA_XXX views with an addition on
one column called CON_ID, which provides a container ID to which the record of data

belongs. For example, if we query CDB_OBJECTS in the CDB, it will show all objects in the
entire database and the CON_ID column will have the ID of the container where the
respective objects belong. Also, if one of the PDBs is down or closed when you query
the CDB_XXX view, the data for that PDB will not be displayed in the output.
All of the V$ views have also been modified to include the CON_ID column so that we can
query V$ views from the CDB and can identify information coming from different containers.
The root container (CDB$ROOT) has CON_ID as 1 and the seed container (PDB$SEED) has CON_ID
as 2. User-created PDBs start from CON_ID 3 and so on. We can have up to 252 user-created
PDBs in a CDB. This excludes PDB$SEED.

Local users and common users
Unlike in the previous release, Oracle 12c offers two types of users - local users and common
users. Local users are the ones that can be created only in PDBs and these are the users that the
application uses to connect to the database for getting application data from PDBs. Common
users can only be created in a CDB and they are the administrative users who have complete
access to the CDB and PDBs.

Local users
These are the normal users that we have seen upto now from the previous release. So all users
in non-CDB databases are basically local users. Local users can be defined only in PDBs and
not a CDB. All naming restrictions that apply to normal users in the previous release, apply to
local users. Local users are created only in a specific PDB and they can connect to that PDB
only when it's defined. Based on the privileges assigned to the user, it can work on application
schema and objects. These users cannot access the root container or other PDBs unless we
have a DB link created between the PDBs.

Common users
Common users are new in Oracle 12c and can be created only in the root container
(CDB$ROOT). They cannot be created in a PDB, but when they are created in the root, their
presence is available in all PDBs. A common user created in the root container can connect to
the root as well as all PDBs (provided it has privileges to connect to different PDBs). A
common user is like a super user who can perform all administrative tasks such as plugging
and unplugging PDBs, starting up a CDB, and opening up PDBs when granted proper
privileges.
As far as  the naming convention goes, a common user's name should start with C##. So
C##ADMIN_USER is a valid common user name. We can control this using the parameter
common_user_prefix, which is by default set to C##.
SQL> show parameters common 
NAME                                 TYPE        VALUE 
------------------------------------ ----------- ------------------- 
--
common_user_prefix                   string      C## 
When a common user is created in the root container, it's also created in all the PDBs that are
open. PDBs which are closed will get this new common user created when they open and get
synched automatically with the root container. Even though a common user exists in all
containers, schemas of common user can differ in each container. This user can have different
objects created in each container.

Local and common roles and privileges
Just like local user and common users, we also have local roles and common roles and also
local privileges and common privileges. But there is a difference in the definition of user
versus roles and privileges. Unlike local users and common users whose scope depends upon
the existence, scopes or roles and privileges do not depend on the existence, rather they
depend on the way roles and privileges are assigned to the user. Let's try to understand this
deeper.

Local roles
Roles behave similarly to users and have similar rules. Local roles can be created in a PDB
and exist only in that PDB. Local roles exist in a PDB where they are created and they don't
exist in other PDBs or a CDB. Local roles can be assigned locally to a local user or global
user in that PDB. All the roles created in prior releases are considered as local roles in Oracle
12c. We cannot have local roles created in a CDB.

Common roles
Common roles are similar to common users. They should be created in a CDB only. We
cannot create common roles in PDBs. But once created in a CDB, common roles are available
in all PDBs. This makes it a little easier for DBAs to create one role and it's available in all
containers. Common roles should be prefixed with C## as per the parameter
common_user_prefix, which also applied to roles.
SQL> show con_name; 
CON_NAME 
------------------------------ 
CDB$ROOT 
SQL> create role c##admin_role; 
Role created. 
SQL> select role, con_id from cdb_roles where role = 'C##ADMIN_ROLE'; 
ROLE                               CON_ID 
------------------------------ ---------- 
C##ADMIN_ROLE                           1 
C##ADMIN_ROLE                           5 
C##ADMIN_ROLE                           4 
C##ADMIN_ROLE                           3 
Apart from the definition of local and common roles, we can grant these roles either locally
(using the CONTAINER=CURRENT clause of the grant statement) or commonly (using
CONTAINER=ALL). Common roles can be created either locally in a specific container or
commonly across all containers. Local roles can be granted only locally in the specific
container in which they belong.
Example of granting a common role commonly
In this example, we have a common role, C##ADMIN_ROLE, that we are granting commonly
using the CONTAINER=ALL to Sys user:
SQL> show con_name 
CON_NAME 
------------------------------ 
CDB$ROOT 
SQL> grant C##ADMIN_ROLE to Sys container = ALL; 
Grant succeeded. 
SQL> 
Example of granting a common role locally
In this example, we have a common role, C##ADMIN_ROLE_2, that we are granting locally in the

PDB1 container using the CONTAINER=CURRENT to sys user:
SQL> create role C##ADMIN_ROLE_2;  
Role created. 
SQL> alter session set container = PDB1; 
Session altered. 
SQL> grant C##ADMIN_ROLE_2 to sys; 
Grant succeeded. 
SQL> 
One final thing to note about local and common roles is that local roles contain only locally
granted privileges, that is, privileges that apply only to an individual PDB. Local roles do not
have common privileges. Common roles, on the other hand, contain local as well as common
privileges granted across all containers.

Local privileges and common privileges
Understanding local and global privileges is a bit tricky because they are referred to as local
and common privileges but, to be more precise, privileges are granted locally or commonly.
By default, privileges are not local or common; they are simply privileges. It's the act of
granting privileges which can be either local or global. We can grant privileges locally using
the CONTAINER=CURRENT clause to be used in the grant statement or we can grant privileges
commonly by using the CONTAINER=ALL clause in the grant statement. Let's check out this
example objective of Creating  and  configuring  CDBs  and  PDBs. We have three sub-
objectives under this:
Create and configure a CDB
Create and configure a PDB
Migrate a non-CDB to a PDB
Let's go through each one of them.

Creating and configuring a CDB
There are multiple ways of creating a CDB:
Using SQL*Plus
Using DBCA
Using EM Cloud control
You cannot use the SQL Developer tool to create a CDB. The SQL Developer can be used
only to create PDBs.
We have already seen a few screens of the DBCA in Chapter 1, Getting Started with Oracle,
which shows what changes are made to the DBCA in Oracle 12c. Apart from these changes,
the overall procedure of creating the database using the DBCA is more or less the same as the
previous release. So we are not going to cover the DBCA in this section. The only advantage
of using the DBCA for creating a CDB is that you can also create PDBs along with the CDB at
the same time.
Let's create a CDB manually using SQL*Plus. I am assuming that we have Oracle 12c 12.1.0.1
software installation already done on the host where we are going to create the database.
Creating a CDB is very similar to creating a normal database. The following are the steps to
be executed:
1. Create the initialization parameter file:
We will create a small basic init.ora file for our database. I don't want to include too
many parameters; otherwise, we will lose focus.
Let's say we want to create a database named ORCL. Create the following initorcl.ora:
  sga_target=768M
  pga_aggregate_target=256M
  audit_file_dest='/u01/app/oracle/admin/orcl/adump'
  audit_trail='db'
  compatible='12.1.0.2.0'
        
control_files='/u01/app/oracle/oradata/orcl/orcl/control01.ctl','/u01/a  
pp/oracle/fast_recovery_area/orcl/control02.ctl'
  db_block_size=8192
  db_domain='example.com'
  db_name='orcl'
  db_recovery_file_dest='/u01/app/oracle/fast_recovery_area'
  db_recovery_file_dest_size=4560m
  diagnostic_dest='/u01/app/oracle'
  dispatchers='(PROTOCOL=TCP) (SERVICE=orclXDB)'
  open_cursors=300
  processes=300
  remote_login_passwordfile='EXCLUSIVE'
  undo_tablespace='UNDOTBS1'

  enable_pluggable_database=true
This init file is similar to the one we created for creating the database in the previous
release, except for one initialization parameter--enable_pluggable_database, which is
set to true. This is a new parameter introduced in Oracle 12c which enables us to create
the multitenant container database. If we don't include this parameter in the initialization
parameter file, it takes a default value of false and the database will be created as a non-
CDB, just like we used to have in previous releases.
A non-CDB database cannot have pluggable databases plugged into it. So based on your
requirement you can have this parameter set to true or false (default).
Create a spfile from preceding pfile using:
SQL>Create spfile from pfile;
2. Start the instance in the nomount mode:
      SQL> startup nomount
      ORACLE instance started.
      Total System Global Area  805306368 bytes 
      Fixed Size                  2929552 bytes 
      Variable Size             318770288 bytes 
      Database Buffers          478150656 bytes 
      Redo Buffers                5455872 bytes 
      SQL> 
Make sure that the instance is getting started. You can create the required huge pages if
you are using Linux so that System Global Area (SGA) fits in the huge pages.
3. Create the database:
Use the create database command to create the required database. The big difference in
using the create database command in Oracle 12c is that if you want to create the
multitenant database, you need to use an additional new clause--enable pluggable
database.
Please note that we have the ENABLE_PLUGGABLE_DATABASE init parameter as well as the
enable pluggable database clause in the create database statement. Both of these have
to be present if we want to create a CDB. If we set the ENABLE_PLUGGABLE_DATABASE
parameter to false, using the enable pluggable database clause in the create
database statement will give an error.
Make sure you create all the directories that are mentioned in the database creation script
as well as in the init.ora file:
   CREATE DATABASE orcl
   USER SYS IDENTIFIED BY oracle
   USER SYSTEM IDENTIFIED by oracle
   LOGFILE
   GROUP 1

   ('/u01/app/oracle/oradata/orcl/orcl/redologs/redo-01-a.log',
   '/u01/app/oracle/oradata/orcl/orcl/redologs/redo-01-b.log')
   size 100M,
   GROUP 2
   ('/u01/app/oracle/oradata/orcl/orcl/redologs/redo-02-a.log',
   '/u01/app/oracle/oradata/orcl/orcl/redologs/redo-02-b.log')
   size 100M
   CHARACTER SET utf8 NATIONAL CHARACTER SET utf8
   EXTENT MANAGEMENT LOCAL
   DATAFILE
   '/u01/app/oracle/oradata/orcl/orcl/datafiles/system-01.dbf'
   size 1000M
   SYSAUX DATAFILE
   '/u01/app/oracle/oradata/orcl/orcl/datafiles/sysaux-01.dbf'
   size 1000M
   DEFAULT TEMPORARY TABLESPACE TEMP TEMPFILE
   '/u01/app/oracle/oradata/orcl/orcl/datafiles/temp-01.dbf'
   size 1000M
   UNDO TABLESPACE UNDOTBS1 DATAFILE
   '/u01/app/oracle/oradata/orcl/orcl/datafiles/undotbs1-01.dbf'
   size 1000M
   ENABLE PLUGGABLE DATABASE
   SEED FILE_NAME_CONVERT =
   ('/u01/app/oracle/oradata/orcl/orcl/datafiles/',
   '/u01/app/oracle/oradata/orcl/seed/datafiles/')
Apart from the ENABLE PLUGGABLE DATABASE clause, we used another clause, SEED
FILE_NAME_CONVERT. This clause is optional and it decides where to place the seed container
data files. If you remember, when we create a container database (CDB), it also creates a seed
container (PDB$SEED), which can be used to create pluggable databases. So for this seed
container, it creates data files for the SYSTEM, SYSAUX, and TEMPORARY tablespaces.
There are multiple ways to provide the location for creating data files for the seed container:
SEED FILE_NAME_CONVERT: This is one way of specifying the location of data files for the
seed container. We use data file names the same as for a CDB and we only provide the
location for seed data files using the following format:
    SEED FILE_NAME_CONVERT =    
('/u01/app/oracle/oradata/orcl/orcl/datafiles/','/u01/app/oracle/oradat    
a/orcl/seed/datafiles/') 
Using this format, we tell Oracle to copy the data files for the seed container from the
root container's location /u01/app/oracle/oradata/orcl/orcl/datafiles to seed
container location /u01/app/oracle/oradata/orcl/seed/datafiles
DB_CREATE_FILE_DEST: If you set this parameter, data files will be created as OMF-
managed data files in the specified location. For example: DB_CREATE_FILE_DEST =
'/u01/app/oracle/oradata/orcl/datafile
PDB_FILE_NAME_CONVERT: This is the new initialization parameter introduced in Oracle
12c. You need to mention this parameter in the init.ora file and the format of this
parameter is the same as the SEED FILE_NAME_COVERT clause in the create database

command. For example: PDB_FILE_NAME_CONVERT=
('/u01/app/oracle/oradata/orcl/orcl/datafiles/','/u01/app/oracle/oradata/orcl
The purpose of this parameter is to provide the destination location of the PDB data files at
which data files from the seed database will be copied.
If you specify more than one of these three parameters, the following order of precedence is
used:
SEED FILE_NAME_CONVERT
DB_CREATE_FILE_DEST
PDB_FILE_NAME_CONVERT
Make sure all the directories are mentioned in these parameters and the create database
statement exists in the filesystem, otherwise the create database statement will fail.
In our example, the SYSTEM and SYSAUX data files of the seed container use the same physical
attributes of data files in the root container. You can actually specify different physical
attributes for the SYSTEM and SYSAUX data files in the seed container as shown in the following
code:
SEED FILE_NAME_CONVERT =('/u01/app/oracle/oradata/orcl/orcl/datafiles/',
'/u01/app/oracle/oradata/orcl/seed/datafiles/')
SYSTEM DATAFILE SIZE 50M autoextend on next 10M max size
unlimited
SYSAUX DATAFILE SIZE 50M
/
If you use Oracle Managed Files (OMF) and specify the DB_CREATE_FILE_DEST initialization
parameter, our create database statement reduces to the bare minimal as shown in the
following code:
CREATE DATABASE orcl
USER SYS IDENTIFIED BY oracle
USER SYSTEM IDENTIFIED by oracle
DEFAULT TEMPORARY TABLESPACE TEMP
UNDO TABLESPACE UNDOTBS1
ENABLE PLUGGABLE DATABASE
/
For our example, let's use SEED FILE_NAME_CONVERT and create the required CDB:
1. Run catcdb.sql, which installs all the required components.
In the Oracle 12c release, there is no need to run catalog.sql and catproc.sql. Instead
we need to run only catcdb.sql and it takes care of running all the required scripts
including utlrp at the end. catcdb.sql script creates views on top of data dictionary
tables and installs standard PL/SQL packages in the root container.
You might hit the following error while running catcdb.sql:

 Can't locate Term/ReadKey.pm in @INC (@INC contains: 
/u01/app/oracle/product/12.1.0.2/dbhome_1/rdbms/admin 
/usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl 
/usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 . 
/u01/app/oracle/product/12.1.0.2/dbhome_1/rdbms/admin/) at 
/u01/app/oracle/product/12.1.0.2/dbhome_1/rdbms/admin/catcon.pm line 189. 
BEGIN failed--compilation aborted at 
/u01/app/oracle/product/12.1.0.2/dbhome_1/rdbms/admin/catcon.pm line 189. 
Compilation failed in require at 
/u01/app/oracle/product/12.1.0.2/dbhome_1/rdbms/admin/catcon.pl line 94. 
BEGIN failed--compilation aborted at 
/u01/app/oracle/product/12.1.0.2/dbhome_1/rdbms/admin/catcon.pl line 94. 
This could be because the script is not able to find the required perl modules. The
solution is to include ORACLE_HOME/perl/bin in the PATH variable as shown here:
export PATH=$ORACLE_HOME/bin:$ORACLE_HOME/perl/bin/:$PATH 
catcdb.sql will ask for three inputs--sys password, system password, and temp
tablespace name.

Post-CDB creation
Once the database is created and all the required metadata is populated by catcdb.sql, we
need to configure a few more things:
Default permanent tablespace for CDB: Create a PERMANENT tablespace in the CDB
and make it a default tablespace.
Default TEMPORARY tablespace for CDB: You have already created a TEMPORARY
tablespace as part of the create database statement and made it a TEMPORARY
tablespace. If you want to create another TEMPORARY tablespace or TEMPORARY
tablespace group and make it the default, you can do so.
Listener configuration: You can either use a netca utility to create a tns alias and
listener configuration or manually create these files. Using netca automatically creates a
listening protocol address and service information in Oracle database. In my example, I
manually created the following files and started the listener:
    $ORACLE_HOME/network/admin/listener.ora
     LISTENER =
     (DESCRIPTION_LIST =
     (DESCRIPTION =
     (ADDRESS = (PROTOCOL = TCP)(HOST = deo.example.com)(PORT = 1521))
     (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1521))
     )
     )
    $ORACLE_HOME/network/admin/tnsnames.ora
    ORCL =
   (DESCRIPTION =
   (ADDRESS = (PROTOCOL = TCP)(HOST = deo.example.com)(PORT = 1521))
   (CONNECT_DATA =
   (SERVER = DEDICATED)
   (SERVICE_NAME = orcl.example.com)
   )
   )
Start the listener using lsnrctl start.
Trigger to start existing PDBs after the CDB starts: You can create the required start
up trigger which will open PDB databases as soon as the CDB is opened. Opening a PDB
is not automated in 12.1.0.1, so the DBA needs to have start up triggers in place to open
PDBs if this task has to be automatic.
Backup and recovery strategies/procedure for CDB: Decide on the backup strategies
for your databases. You need to modify your backup scripts to make sure that all PDBs
are getting backed up along with the root container.
Creating PDBs: Once the CDB is ready, we can create one or more PDBs which can
store business data and will be used by the application. We can also plug in existing
Oracle 12c non-CDBs as PDBs in this CDB.
The next section demonstrates creating pluggable databases using various methods.

What's in your CDB?
Your new CDB has some different characteristics compared to a non-CDB. Your CDB will
have the following after its creation:
Two containers
Root container  CDB$ROOT
Seed container  PDB$SEED
You can check this using the v$container view of the CDB_PDB view:
SQL> select name, open_mode, con_id from v$containers; 
NAME                           OPEN_MODE      CON_ID 
------------------------------ ---------- ---------- 
CDB$ROOT                       READ WRITE          1 
PDB$SEED                       READ ONLY           2 
Several services get created - one service per container:
Every container has one service created and registered with the listener
The CDB has multiple services corresponding to XML DB and default services for USER
and BACKGROUND processes
The following query from the V$SERVICES view shows services corresponding to XML DB and
the default service for USER and BACKGROUND processes:
SQL> select name, pdb, con_id from v$services; 
NAME                           PDB                                CON_ID 
------------------------------ ------------------------------ ---------- 
orclXDB                        CDB$ROOT                                1 
orcl.example.com               CDB$ROOT                                1 
SYS$BACKGROUND                 CDB$ROOT                                1 
SYS$USERS                      CDB$ROOT                                1 
When a CDB is created:
common users and common privileges are granted to common users
SYS and SYSTEM users are created in the root and seed containers and common privileges
are granted to all users in all containers
Pre-defined common roles are also created in all containers
Tablespaces--SYSTEM, SYSAUX, and, TEMP are created in each container
You can configure EM Express for the root container and each of the PDB containers. Note
that we need to have a separate EM Express for the root and PDBs on different ports. EM
Express does not understand PDBs. For EM Express, it's just a database.

Creating PDBs
There are multiple ways of creating PDBs and we need to choose which method to use based
on the situation. The following are different ways of creating PDBs:
Create a PDB from seed
Clone a PDB from an existing PDB in the same or a different CDB
Plug an unplugged PDB into a CDB
Plug a non-CDB into a CDB as a PDB
Let's discuss each of these methods in detail. While there are multiple tools available to create
PDBs such as EM Cloud Control, SQL*Plus, SQL Developer, DBCA, and so on, we will be
focusing on creating a PDB using SQL*Plus

Creating a PDB from seed
Once of the simplest and fastest methods to create a PDB is to clone it from seed. As we
learned previously, when we create a CDB, we also create a seed database. This seed can be
used to clone a new PDB in a CDB. Creating a PDB using seed involves copying data files
from the seed database location to the location of the new database. We have the same rules
for specifying the location of a new data file as we do for specifying for a seed database
during the create database statement while creating a CDB.
We have the following three ways of specifying the location of data files for a new database in
order of precedence:
FILE_NAME_CONVERT clause: We can use this clause while creating a PDB. This provides
the location of seed data files (source) from where to copy and the location of new
databases (destination) where seed data files should be copied for cloning a PDB. For
example: FILE_NAME_CONVERT=('/u01/app/oracle/oradata/orcl/seed/datafiles','
/u01/app/oracle/oradata/orcl/pdb1/datafiles')
DB_CREATE_FILE_DEST: This is an OMF parameter that can be set in the init.ora file.
Once we set this parameter to some location, Oracle automatically determines the name
for data files and creates data files in the location specified by this parameter. For
example: DB_CREATE_FILE_DEST = '/u01/app/oracle/oradata/orcl/pdb1/datafiles
PDB_FILE_NAME_CONVERT: This is a new init.ora parameter introduced in Oracle 12c.
This provides the location of the source (seed database data files) and destination (new
PDB data files). For example: PDB_FILE_NAME_CONVERT =
('/u01/app/oracle/oradata/orcl/seed/datafiles','
/u01/app/oracle/oradata/orcl/pdb1/datafiles')
Note
Note that all the directories related to a new PDB should pre-exist before creating the
PDB.
Let's create a PDB using the first clause _FILE_NAME_CONVERT. The following command
shows how to create a PDB:
SQL> Create pluggable database pdb1
admin user pdb1_admin identified by oracle
file_name_convert=(
'/u01/app/oracle/oradata/orcl/seed/datafiles',
'/u01/app/oracle/oradata/orcl/pdb1/datafiles');
Pluggable database created.
This is just the basic command for creating a PDB. You can also use roles=(dba) to assign
the DBA or any other roles to the PDB1_ADMIN user while creating the PDB. You can always
refer to the Oracle documentation for different clauses that you can provide for creating a
PDB.

What does the aforementioned create pluggable database statement do?
It creates the pluggable database PDB1 by copying data files from the seed location to the
new location provided
It creates SYSTEM and SYSAUX tablespaces
It creates default users and schemas--sys, system, and so on
It creates a new user PDB1_ADMIN and assigns the role PDB_DBA. This role does not have
any privileges as of now.
The PDB sys user must grant privileges to PDB_DBA locally in the PDB1 database
It creates a new service pdb.example.com for this PDB.
The new PDB that is created will have its new CON_ID and GUID assigned to it. You can check
the details of the new PDB in the V$CONTAINERS view:
SQL> select con_id, GUID, name, open_mode from v$containers; 
   CON_ID GUID                             NAME       OPEN_MODE 
---------- -------------------------------- ---------- ---------- 
        1 3513A5A6C1451099E0531438A8C0896C CDB$ROOT   READ WRITE 
        2 3513A5A6C1441099E0531438A8C0896C PDB$SEED   READ ONLY 
        3 352245E6E3D8249EE0531438A8C042FA PDB1       MOUNTED 
The status of the PDB is mounted after creation. We need to manually open this PDB so that it
will sync its metadata with the CDB metadata. This is a very important step. Until this is done,
we cannot use this PDB.
SQL> alter pluggable database PDB1 open; 
Pluggable database altered. 
After opening the PDB, you can check the status for this in the CDB_PDBS view:
SQL> select PDB_ID, PDB_NAME, STATUS, con_id from cdb_pdbs; 
   PDB_ID PDB_NAME   STATUS         CON_ID 
---------- ---------- --------- ---------- 
      2 PDB$SEED   NORMAL               2 
      3 PDB1       NORMAL               3 

Cloning a PDB from an existing PDB
You can clone a PDB from another PDB in the same CDB or in a different CDB on the same
or a different host. The cloning process will copy the data files from the source PDB into a
new PDB location that we are cloning. Cloning the existing PDB into the new PDB is ideally
suitable for a situation where we want to test new code and want to replicate the production
database environment.
Again the same rules apply to specifying the location for new PDB data files. You can either
specify FILE_NAME_CONVERT in the create pluggable database statement or use the
DB_CREATE_FILE_DEST parameter in init.ora or use the PDB_FILE_NAME_CONVERT parameter
in init.ora.
Before we clone a new PDB from an existing PDB, we need to make sure to open the existing
PDB in read-only mode. This is required, otherwise the new PDB will not be consistent. So
let's re-open the existing PDB in read-only mode:
SQL> alter pluggable database pdb1 close; 
Pluggable database altered. 
SQL> alter pluggable database pdb1 open read only; 
Pluggable database altered. 
The following command creates a new pluggable database, PDB2, from the existing
PDB1 database. Note that the directories required for storing files for the new PDB should pre-
exist at the host level:
SQL> create pluggable database pdb2 from pdb1 file_name_convert=
('/u01/app/oracle/oradata/orcl/pdb1/datafiles','/u01/app/oracle/oradata/orcl/pdb
2/datafiles'); 
Pluggable database created. 
SQL> alter pluggable database pdb2 open; 
Pluggable database altered. 
While creating a PDB from another PDB, you cannot specify the admin user. The admin user
of the source PDB will be available in the new cloned PDB. You can create a different admin
user after opening the PDB. If your source PDB is in a different CDB, you need to first create
a database link to that PDB and use this to clone the PDB as shown in the following code:
create pluggable database pdb2 from pdb1@pdb1_dblink; 
You can also specify a snapshot copy clause if you want to perform cloning using a storage
snapshot. However, for this to work, the underlying filesystem should be supporting the
storage snapshot.

Once PDB2 is created, you can open PDB1 again in read-write mode.

Plugging an unplugged PDB into a CDB
We can also unplug a PDB from a CDB and plug in this in a different PDB. This is required
for a migration, an upgrade, and so on. When you unplug a PDB, it will create an XML file.
This file is a metadata which has all the information about all the data files of the PDB. This
XML file along with all the data files of the PDBs are required for plugging a PDB into a new
CDB. Let's check out the process of unplugging a PDB and plugging it into a new CDB.
I have two CDBs running on my host: ORCL and DEO.
In ORCL CDB, I have two PDBs: PDB1 and PDB2.
In DEO CDB, I don't have PDBs.
Let's try to unplug PDB2 from ORCL and plug it into DEO CDB.

Unplugging PDB2 from ORCL
The following is the current status of the PDB:
SQL> select con_id, name, open_mode from v$containers; 
   CON_ID NAME                           OPEN_MODE 
---------- ------------------------------ ---------- 
        1 CDB$ROOT                       READ WRITE 
        2 PDB$SEED                       READ ONLY 
        3 PDB1                           READ WRITE 
        4 PDB2                           READ WRITE 
Before we unplug the PDB, we need to make sure that the PDB is closed. Use the following
commands to close and unplug the PDB:
SQL> alter pluggable database pdb2 close immediate; 
Pluggable database altered. 
SQL> alter pluggable database pdb2 unplug into '/home/oracle/pdb2.xml'; 
Pluggable database altered. 
When unplugging, we need to specify the path and name of the non-existing XML file. This
file will be created when you unplug a PDB and will contain the metadata for the PDB. This
file is required while cloning a new PDB from this PDB.
If we check the status of the PDB in cdb_pdbs, we see the status as UNPLUGGED:
SQL> select PDB_ID, PDB_NAME, STATUS, con_id from cdb_pdbs; 
   PDB_ID PDB_NAME             STATUS        CON_ID 
---------- -------------------- --------- ---------- 
        2 PDB$SEED             NORMAL             2 
        3 PDB1                 NORMAL             3 
        4 PDB2                 UNPLUGGED          4 
Once unplugged, we also need to drop the PDB as we cannot plug in an unplugged PDB again
unless we drop it.
SQL> drop pluggable database pdb2; 
Pluggable database dropped. 
You can also include a clause, including datafiles, in the preceding drop command if you
want to remove the data files physically from the OS location. But make sure you have copied
those data files to a new location where you are plugging this PDB. Alternatively, we have the
keep datafiles clause which will keep data files at the OS level even after dropping the
pluggable database. Since this is the default behavior, there is no need to mention keep
datafiles.

Plugging PDB2 into a CDB
Before plugging PDB2 into DEO CDB, we need to make sure that the PDB is compatible and
we can plug in this PDB into DEO CDB. We can check this by executing the
DBMS_PDB.CHECk_PLUG_COMPATIBILITY function and providing the previously created XML
metadata file (/u01/app/oracle/pdb2.xml) as input. This will check the version compatibility
and other things and provide us with violations if any. Violations will be recorded in the
PDB_PLUG_IN_VIOLATIONS view.
As shown in the following code, we are not seeing any violations and the PDB is compatible:
SET SERVEROUTPUT ON 
DECLARE 
 l_result BOOLEAN; 
BEGIN 
 l_result := DBMS_PDB.check_plug_compatibility( 
               pdb_descr_file => '/u01/app/oracle/pdb2.xml', 
               pdb_name       => 'pdb2'); 
 IF l_result THEN 
   DBMS_OUTPUT.PUT_LINE('PDB is Compatible'); 
 ELSE 
   DBMS_OUTPUT.PUT_LINE('PDB is InCompatible'); 
 END IF; 
END; 
/ 
PDB is Compatible 
PL/SQL procedure successfully completed. 
Let's plug in this PDB to a new CDB. Before that I would like to copy the data files related to
the PDB to the desired location:
Current location of data files: /u01/app/oracle/oradata/orcl/pdb2/datafiles
New location of data files: /u01/app/oracle/oradata/deo/pdb2/datafiles
Once the data files are copied, we can use the following command to plug PDB2 in DEO CDB:
SQL> create pluggable database pdb2 using '/u01/app/oracle/pdb2.xml' nocopy; 
Pluggable database created. 
SQL> alter pluggable database pdb2 open; 
Pluggable database altered. 
SQL> select PDB_ID, PDB_NAME, STATUS, con_id from cdb_pdbs; 
   PDB_ID PDB_NAME                       STATUS        CON_ID 
---------- ------------------------------ --------- ---------- 
        2 PDB$SEED                       NORMAL             2 

        3 PDB2                           NORMAL             3 
SQL> select con_id, name, open_mode from v$containers; 
   CON_ID NAME                           OPEN_MODE 
---------- ------------------------------ ---------- 
        1 CDB$ROOT                       READ WRITE 
        2 PDB$SEED                       READ ONLY 
        3 PDB2                           READ WRITE 
If we look carefully at the preceding plug-in command, we used a nocopy clause. This is
because we already copied the data files to the desired location. But if we don't copy the data
files to the correct location, we should use the FILE_NAME_CONVERT clause which specifies the
new location of the data files. In that case, the plugin command will copy the data files from
the old location to the new location.
If you are plugging a PDB into a CDB that contains one or more PDBs that were created by
plugging in the same PDB, then you need to use the as clone clause in the plug-in command.

Migrating a non-CDB to a PDB
One of the important tasks that can be performed is to migrate a non-CDB database as a PDB
in a CDB. This is a situation where we want to consolidate many databases into a single CDB
containing many PDBs. Your non-CDB can be an Oracle 12c database or a prior version.
Depending on the situation and version of the database, we can use one of the following
methods for plugging a non-CDB as a PDB:
Exporting/importing data into an empty PDB using transportable tablespace.
Using Golden Gate replication to replicate data from a non-CDB database to a PDB. This
works when the database is big and running on a prior version.
Migrating a non-CDB using the DBMS_PDB package. This is the fastest method but works
when the non-CDB is on the Oracle 12c version.
We will discuss briefly the first two methods and then look at the third option-migrating a
non-CDB using the DBMS_PDB package.
Plugging a non-CDB into a CDB using datapump TTS
If your database is on a previous release and you don't want to upgrade this database before
plugging into a CDB as a PDB, you can go with the TRANSPORTABLE tablespace. This
method is the fastest of all the methods and is much less complicated in Oracle 12c.
The following are the brief steps you need to follow:
1. Perform the transportable tablespace export DUMP. This will create a metadata dump file
and a log file, which will have a list of data files that need to be migrated and plugged
into the CDB.
2. Create a new pluggable database in the CDB. This will be an empty PDB database and we
are going to transport our tablespaces from the non-CDB database to this new PDB
database.
3. Connect to the new PDB and perform an impdb transportable tablespace import. Make
sure you use the connect parameters to the impdp to attach the tablespaces to the new
PDB.
If your database is relatively small, you can also use traditional export/import techniques
instead of transportable tablespaces.
Using the Golden Gate replication to plug a non-CDB into a PDB
Instead of using the export/import and transportable tablespace, you can also go with Golden
Gate replication. This method is used when you have significantly less downtime from
business. We can use the following steps to migrate a non-CDB into a PDB:
1. Create a new empty PDB in a CDB.
2. Open the PDB in read-write mode.
3. Configure unidirectional replication to replicate data from the source non-CDB to the

PDB. This involves doing an initial load of data and creating all the required schemas
and tablespaces.
4. Keep the databases in sync using Golden Gate.
5. During downtime, failover to the new PDB.
Migrating a non-CDB to a PDB using the DBMS_PDB package
We have the DBMS_PDB.DESCRIBE procedure which can be run in an existing non-CDB Oracle
12c database and which creates an XML metadata file similar to the one that gets created when
we unplug a PDB, as seen previously. This XML file can be used to plug in a non-CDB as a
PDB into an existing CDB. But this method can be used only if a non-CDB is an Oracle 12c
database.
Let's say I have a DEO 12c database which was created as a non-CDB database (without the
enable pluggable database clause). We can plug this non-CDB as a PDB into an existing
CDB - ORCL.
The following are the steps to do this:
1. Open the non-CDB database (DEO) in read-only mode:
    SQL> select name, database_role, open_mode from v$database; 
 
    NAME      DATABASE_ROLE    OPEN_MODE                      
    --------- ---------------- -------------------- 
    DEO       PRIMARY          READ ONLY 
2. Connect to the non-CDB and execute DBMS_PDB.DESCRIBE to generate the metadata XML
file:
   SQL> exec DBMS_PDB.DESCRIBE(PDB_DESCR_FILE=>'/u01/app/oracle/deo.xml'); 
 
    PL/SQL procedure successfully completed. 
 
   SQL> !ls -lrt /u01/app/oracle/deo.xml 
   -rw-r--r-- 1 oracle oinstall 5106 Jun 13 18:30  /u01/app/oracle/deo.xml 
3. Check the compatibility by running DBMS_PDB. CHECK_PLUG_COMPATIBILITY in the CDB
using this XML file:
   SET SERVEROUTPUT ON 
   DECLARE 
     l_result BOOLEAN; 
   BEGIN 
     l_result := DBMS_PDB.check_plug_compatibility( 
                   pdb_descr_file => '/u01/app/oracle/deo.xml', 
                   pdb_name       => 'deo'); 
 
     IF l_result THEN 
       DBMS_OUTPUT.PUT_LINE('PDB is Compatible'); 
     ELSE 
       DBMS_OUTPUT.PUT_LINE('PDB is InCompatible'); 

     END IF; 
   END; 
   / 
 
   PDB is Compatible 
 
   PL/SQL procedure successfully completed. 
4. Shut down the non-CDB DEO database.
5. Create the pluggable database using the XML metadata file:
   SQL> create pluggable database deo using '/u01/app/oracle/deo.xml'    
file_name_convert=   
('/u01/app/oracle/oradata/deo/deo/datafiles/','/u01/app/oracle/oradata/ 
orcl/deo/datafiles/'); 
 
   Pluggable database created. 
6. Execute the script $ORACLE_HOME/rdbms/admin/noncdb_to_pdb.sql. This script removes
the superfluous metadata from the new PDB dictionary and recompiles the objects:
    SQL> alter session set container = deo; 
 
    Session altered. 
 
    SQL> @?/rdbms/admin/noncdb_to_pdb.sql 
7. While you are running the noncdb_to_pdb.sql script, the open_mode of PDB remains as
MIGRATE:
    SQL> select con_id, name, open_mode from v$containers; 
  
        CON_ID NAME       OPEN_MODE 
    ---------- ---------- ---------- 
             1 CDB$ROOT   READ WRITE 
             2 PDB$SEED   READ ONLY 
             3 PDB1       MOUNTED 
             4 DEO        MIGRATE 
8. Once the script completes, you can open the PDB and check the status of:
    SQL> select con_id, name, open_mode from v$containers; 
 
        CON_ID NAME                           OPEN_MODE 
    ---------- ------------------------------ ---------- 
             1 CDB$ROOT                       READ WRITE 
             2 PDB$SEED                       READ ONLY 
             3 PDB1                           READ WRITE 
             4 DEO                            READ WRITE 
That's how you can plug in a non-CDB running on the Oracle 12c version into a CDB.

Summary
We started this chapter with the challenges involved in non-container databases and explained
how the container database benefits us. We then dived into the architecture of the container
database and the different components of the container database. We went deeper into the
concepts of multitenancy and saw how metadata is stored in container databases and how that
metadata is made available to PDBs via a metadata link and object link. We also saw new data
dictionary views called CDB_* views that are introduced in Oracle 12c and outlined the scope
of each type of view.
We then introduced you to the concept of local and global users, roles and privileges, and
how they differ. We then created and configured a CDB database using command lines and
introduced various methods of creating PDB databases. We also showed you how to unplug
and plug a PDB into a CDB. Finally, we saw multiple ways of plugging a non-CDB database
as a PDB into a CDB.
In the next chapter, we are going introduce you to administering a container database. We will
see how we can connect to CDBs and PDBs, how to start up and shut down CDBs and PDBs,
and the effects of changing database parameters at the CDB level and the PDB level. We will
also go deeper into local and common users, roles, and privileges. At the end we will see how
we can manage tablespaces in CDBs and PDBs.
Stay tuned and move on to the next chapter.

Chapter 3. Managing CDBs and PDBs
Multitenancy plays a key role in Oracle 12c and is one of its biggest features. Consolidating
multiple databases makes administration of the database easy and reduces the downtime for
the database. Before we plan to use container databases in production we should have a very
good understanding of administrating container databases. In this chapter, we are going to see
how we can manage multitenant container databases.
We are going to cover the following topics in this chapter:
Establishing connection to CDB/PDBs
Starting up and shutting down CDB/PDBs
Changing instance parameters for CDB/PDBs
Managing tablespaces, common and local users, privileges, and roles
Managing tablespaces in CDB/PDBs
Managing users and privileges in a CDB/PDB

Establishing connection to CDB/PDBs
Connecting to a PDB or CDB is not much different from connecting to a non-CDB database.
Whenever we create a PDB, Oracle automatically creates a service name for that PDB and
registers the service name with the listener.

Services
Services are ways to connect to the databases. We use service names in our connect string and
on the database side; we configure those services inside the database. So, the client connects to
the database using services that are configured.
Every container in a CDB owns a service name.
The root container service name is the name given at the time of the CDB creation
suffixed with the domain name.
While creating a PDB in a CDB, a service name is created and started. The service has
the same name as the PDB suffixed with a domain name. The container service name has
to be unique within a CDB and across CDBs if they are registered with the same listener.
You can check the details of service names using v$services or the cdb_services view:
SQL> select service_id, name, PDB from v$services; 
SERVICE_ID NAME                 PDB 
---------- -------------------- -------------------- 
        0 pdb2.example.com     PDB2 
        0 pdb1.example.com     PDB1 
        5 orclXDB              CDB$ROOT 
        6 orcl.example.com     CDB$ROOT 
        1 SYS$BACKGROUND       CDB$ROOT 
        2 SYS$USERS            CDB$ROOT 
6 rows selected. 
V$SERVICES view contains information about all the services that are configured and
registered in a database. By default, we have SYS$BACKGROUND and SYS$USER service created.
We also have a service named after the database name. In Oracle 12c, this view provides
information about the services configured in a specific container only. CDB_SERVICE view
contains the information about all the services configured across all containers (CDB and all
PDBs).
Each service has an optional PDB property which is shown by the PDB column in the preceding
output. This property is optional and represents the current container that clients will be
connecting to when they use this service. For example, in the preceding output we have the
pdb1.example.com service and it has a PDB property set to PDB1. So if a client tries to connect
to pdb1.example.com service, they will be connected to the PDB1 container.
An important point to make note of is that PDB$SEED is not listed in order to prevent users
from connecting to this container as there is no job to be performed. It just serves as a
template to create PDBs. And also notice in the following code that the PDB$SEED is in read
only mode:
SQL> SELECT CON_ID, NAME, OPEN_MODE from v$pdbs; 
   CON_ID NAME                           OPEN_MODE 

---------- ------------------------------ ---------- 
        2 PDB$SEED                       READ ONLY 
        3 PDB1                           READ WRITE 
        4 PDB2                           READ WRITE 
You can also check that these services are registered with a listener:
[oracle@advait ~]$ lsnrctl status 
LSNRCTL for Linux: Version 12.1.0.2.0 - Production on 28-JUL-2016 17:36:19 
Copyright (c) 1991, 2014, Oracle.  All rights reserved. 
Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=advait.example.com)
(PORT=1521))) 
STATUS of the LISTENER 
------------------------ 
Alias                     LISTENER 
Version                   TNSLSNR for Linux: Version 12.1.0.2.0 - Production 
Start Date                28-JUL-2016 10:29:41 
Uptime                    0 days 7 hr. 6 min. 38 sec 
Trace Level               off 
Security                  ON: Local OS Authentication 
SNMP                      OFF 
Listener Parameter File   
/u01/app/oracle/product/12.1.0.2/dbhome_1/network/admin/listener.ora 
Listener Log File         
/u01/app/oracle/diag/tnslsnr/advait/listener/alert/log.xml 
Listening Endpoints Summary... 
 (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=advait.example.com)(PORT=1521))) 
 (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC1521))) 
 (DESCRIPTION=(ADDRESS=(PROTOCOL=tcps)(HOST=advait.example.com)(PORT=5500))
(Security=
(my_wallet_directory=/u01/app/oracle/product/12.1.0.2/dbhome_1/admin/orcl/xdb_w
allet))(Presentation=HTTP)(Session=RAW)) 
Services Summary... 
Service "orcl.example.com" has 1 instance(s). 
 Instance "orcl", status READY, has 1 handler(s) for this service... 
Service "orclXDB.example.com" has 1 instance(s). 
 Instance "orcl", status READY, has 1 handler(s) for this service... 
Service "pdb1.example.com" has 1 instance(s). 
 Instance "orcl", status READY, has 1 handler(s) for this service... 
Service "pdb2.example.com" has 1 instance(s). 
 Instance "orcl", status READY, has 1 handler(s) for this service... 
The command completed successfully 
As you can see in the preceding code, we have PDB1 and PDB2 services registered with a
listener.

Creating services
Oracle recommends not to use the default service configured when we create PDBs. Instead,
Oracle recommends that we create our own services for applications to connect to the
database.
If the database is managed by Oracle Restart or Oracle Clusterware, we can use the srvctl
utility to create the services. The srvctl utility allows us to specify the PDB property of the
service which determines the container for which the service is created.
This is an example of adding a service to a database using srvctl:
srvctl add service -db pdb1 -service dss_service -pdb pdb1 
In this example, we created a service named dss_service and we have set the PDB property of
that service to PDB1. So any client using this service will connect to PDB1.
If your database is not being managed by Oracle Restart or Oracle Clusterware, you can
create or modify services for each PDB by using the DBMS_SERVICE package. In this case, the
PDB property is set to the current PDB where the operation is performed.
For example:
SQL> CONNECT sys@pdb2 as sysdba 
SQL> EXEC DBMS_SERVICE.CREATE_SERVICE('oltp_service','oltp_service')  
SQL> EXEC DBMS_SERVICE.START_SERVICE('oltp_service') 
The oltp_service service is created and started by connecting to PDB2 so the PDB property of
this service will be set to PDB2 and clients using this service will connect to PDB2.

Connecting to CDBs
For connecting to a CDB, we can use OS authentication or a user ID and password.
Connection can be made in one of the following ways:
Locally using a userID and password but without specifying the service name
Locally using OS authentication
Remotely using a net service name/connect string/alias configured in tnsnames.ora
Remotely using EasyConnect

Locally using user ID and password but without specifying a
service name
If you already have the environment sourced on the database host, we can connect locally to a
CDB without even specifying DB_NAME or SERVICE_NAME
If you don't specify any service name, the username is resolved in the context of the root
container. The following example shows this:
bash-3.2$ sqlplus deo_user/welcome 
SQL*Plus: Release 12.1.0.1.0 Production on Mon Jul 18 08:57:56 2016 
Copyright (c) 1982, 2014, Oracle.  All rights reserved. 
Connected to: 
Oracle Database 12c Enterprise Edition Release 12.1.0.2.0 - 64bit Production 
With the Partitioning, OLAP, Advanced Analytics and Real Application Testing 
options 
SQL> 
In the preceding example, we provided only the username/password without providing any
database name or service name. This is because the environment was sourced so there was no
need to provide the ORACLE_SID.
Locally using OS authentication
We can also use OS level authentication to connect to the database without a password. In the
following example, we are not providing a database name or user password for connecting.
This is because the environment is set to ORACLE_SID and they are not required, and since we
are using OS authentication, a password is not required.
bash-3.2$ sqlplus '/ as sysdba' 
SQL*Plus: Release 12.1.0.1.0 Production on Mon Jul 18 08:57:56 2016 
Copyright (c) 1982, 2014, Oracle.  All rights reserved. 
Connected to: 
Oracle Database 12c Enterprise Edition Release 12.1.0.2.0 - 64bit Production 
With the Partitioning, OLAP, Advanced Analytics and Real Application Testing 
options
SQL> 
Check whether the database is a multitenant container database:
SQL> select name, cdb, con_id from v$database; 
NAME      CDB     CON_ID 
--------- --- ---------- 

CDB2      YES          0 
Using a net service name/connect string/alias configured in tnsnames.ora
If you have configured tnsnames for your service, you can easily connect to the database
using the TNS alias as shown here:
SQL> connect system@cdb2        
Enter password:  
Connected. 
SQL> select name, cdb, con_id from v$database; 
NAME      CDB     CON_ID 
--------- --- ---------- 
CDB2      YES          0 
Remotely using EasyConnect
EasyConnect, as the name implies, is a simple connection method. With EasyConnect, you
don't need to have TNS alias or a long description used in TNS alias. EasyConnect uses out-
of-the-box TCP/IP connectivity to databases and simply uses a host, port, and database service
name to connect to the database. You don't need any directory system or tnsnames.ora file for
connecting to the database using EasyConnect:
SQL> CONNECT username@hostname:portnumber/service_name 
SQL> connect sys\@localhost:1522\/deo.example.com as sysdba 
Enter password:  
Connected. 
SQL> select name, cdb, con_id from v$database; 
NAME                      CDB CON_ID 
------------------------- --- ------ 
DEO                       YES      0 

Connecting to PDBs
To connect to a PDB, a user should be:
A common user with CREATE SESSION privileges granted commonly or locally in that
PDB
A local user in the PDB with CREATE SESSION privileges
You can connect to a PDB using EasyConnect or using the Oracle net service name.
Let's say we have a local user - local_pdb2 created in PDB2. We will first try to connect to
PDB1 using the common user sys and then try to connect to PDB2 using local_pdb2 which was
given CREATE SESSION privileges in the PDB2 database.
Connecting to PDB1 using common user sys:
SQL> connect sys@PDB1 as sysdba 
Enter password:  
Connected. 
SQL> select con_id, name, open_mode from v$pdbs; 
   CON_ID NAME                           OPEN_MODE 
---------- ------------------------------ ---------- 
        3 PDB1                           READ WRITE 
Connecting to PDB2 using local user local_pdb2:
SQL> connect local_pdb2@pdb2 
Enter password:  
Connected. 
SQL> select con_id, name, open_mode from v$pdbs; 
   CON_ID NAME                           OPEN_MODE 
---------- ------------------------------ ---------- 
        4 PDB2                           READ WRITE 
V$PDBS view provides details about all the PDBs that are attached to the current instance.

Switching connections
A CDB administrator can switch to any PDB in that CDB. There are two ways to switch
connection between the containers within a CDB:
Using reconnect
RECONNECT allows connection under a common or local user. In this case you have to use
connect <user ID>/<password>@<container> to connect to the required container:
bash-3.2$ sqlplus / as sysdba 
SQL*Plus: Release 12.1.0.2.0 Production on Tue Jul 19 01:03:43 2016 
Copyright (c) 1982, 2014, Oracle.  All rights reserved. 
Connected to: 
Oracle Database 12c Enterprise Edition Release 12.1.0.2.0 - 64bit Production 
With the Partitioning, OLAP, Advanced Analytics and Real Application Testing 
options 
SQL> connect PDBADMIN@PDB2 
Enter password:  
Connected. 
SQL> show con_name 
CON_NAME 
------------------------------ 
PDB2 
SQL> 
Using alter session
You can use the ALTER SESSION SET CONTAINER statement to change the container. This
allows connection under a common user only and that common user should have a new
system privilege SET CONTAINER:
bash-3.2$ sqlplus / as sysdba 
SQL*Plus: Release 12.1.0.2.0 Production on Tue Jul 19 01:06:31 2016 
Copyright (c) 1982, 2014, Oracle.  All rights reserved. 
Connected to: 
Oracle Database 12c Enterprise Edition Release 12.1.0.2.0 - 64bit Production 
With the partitioning, OLAP, Advanced Analytics and Real Application Testing 
options 
SQL> connect sys@PDB1 as sysdba
Enter password:  
Connected. 

SQL> show con_name 
CON_NAME 
------------------------------ 
PDB1 
SQL> alter session set container=pdb2; 
Session altered. 
SQL> show con_name 
CON_NAME 
------------------------------ 
PDB2 
SQL> alter session set container=CDB$ROOT; 
Session altered. 
SQL> show con_name 
CON_NAME 
------------------------------ 
CDB$ROOT 
There are a few things to note while switching the container:
Using the connect command allows connections under common or local users
Using alter session set container allows connections under a common user only,
granted the new system privilege set container
Transactions that are not committed nor rolled back in the original container are still in a
pending state when switching to another container and when switching back to the
original container

Starting up and shutting down a CDB/PDB
In the 12c multitenant container database CDB, the root and pluggable databases share a
common single instance. If the container database CDB is a Real Application Cluster (RAC)
database, then the root and the pluggable databases share all the instances of the CDB.

Starting up CDB
A common user having privileges to start and stop the database and with root as the current
container can start up a CDB. The command to start up a CDB is similar to the command we
use for non-CDB. Issue startup to start the instance, and mount and open the database at
single shot. Alternatively, you can issue startup nomount followed by alter database mount
and alter database open. In both cases, the start-up process remains the same.
Let's take a close look at the start-up process of the CDB at every mode:
NOMOUNT mode: When this is issued, the instance is started. The databases are still
down as usual. Only memory allocation takes place at the NOMOUNT stage. The
following command shows startup nomount and you can see that only memory has been
allocated:
    SQL> startup nomount 
    ORACLE instance started. 
 
    Total System Global Area 4882169856 bytes 
    Fixed Size                  2934312 bytes 
    Variable Size            1023412696 bytes 
    Database Buffers         3841982464 bytes 
    Redo Buffers               13840384 bytes 
In MOUNT mode, look at the following process:
CDB control file is opened and read
Root container is mounted
PDBs are mounted as well
The following code shows mounting the database and we can see that all PDBs are
mounted as well:
SQL> alter database mount; 
 
Database altered. 
 
SQL> select name, open_mode from v$pdbs; 
 
NAME                           OPEN_MODE 
------------------------------ ---------- 
PDB$SEED                       MOUNTED 
PDB1                           MOUNTED 
PDB2                           MOUNTED 
OPEN mode: The root container is opened. The redo log files and the root's data files
are opened. And the SEED container PDB$SEED is opened in READ ONLY mode while the
other PDBs are in the mount state.
The following commands open the database and we can see that all PDBs are still
mounted and the SEED database is opened in READ ONLY:

SQL> alter database open; 
 
Database altered. 
 
SQL> select name, open_mode from v$pdbs; 
 
NAME                           OPEN_MODE 
------------------------------ ---------- 
PDB$SEED                       READ ONLY 
PDB1                           MOUNTED 
PDB2                           MOUNTED 

Opening PDBs
As you have seen, by default when a CDB is opened, the root container is opened, but all
PDBs are in mount state. You need to explicitly start PDBs using the alter command.
You can use the following alter pluggable database to change the open mode of PDBs as
shown here:
SQL> alter pluggable database pdb1 open;      
Pluggable database altered. 
This command is the same as opening a pluggable database in read write mode as shown in
the following code:
SQL> alter pluggable database pdb1 open read write;
The pluggable database is altered.
We can open all PDBs at once using the following statement:
SQL> alter pluggable database all open;      
Pluggable database altered. 
SQL> select name, open_mode from v$pdbs; 
NAME                           OPEN_MODE 
------------------------------ ---------- 
PDB$SEED                       READ ONLY 
PDB1                           READ WRITE 
PDB2                           READ WRITE 
You can also use the startup pluggable database command instead of the alter pluggable
database command and it does the same function:
SQL> startup pluggable database all open;      
Pluggable database altered. 
As seen in the preceding code, PDBs inside a CDB will remain in the mount state even after
opening the CDB. You need to manually open the PDB using the alter pluggable database
statement. You can also create a startup trigger to automatically open all PDBs after the CDB
is started.
You can open a PDB in read only mode using the following command:
SQL> alter pluggable database pdb1 open read only;      
Pluggable database altered. 

Shutting down of CDBs and PDBs
When you shut down a CDB, the data files of all the PDBs and the root are closed, then the
control files are closed, and finally the instance is shut down. When you shut down a PDB
only the data files of the PDB are closed.
You won't see any messages that indicate any of these operations--you'll see the same server
messages as when you shut down a non-CDB database. The shutdown command, when the
current container is the root, shuts down the CDB instance.
Closing PDBs
We can close the PDB either by connecting to the root container and issuing alter pluggable
database ... close or by connecting to the specific PDB and doing shutdown immediate.
You can also specify alter pluggable database ... close immediate and this has the same
effect as shutdown immediate. With the immediate clause, the transactions in the PDBs are
rolled back and the sessions are terminated. If this clause is not used, the statement waits until
all the sessions are disconnected:
SQL> connect / as sysdba 
Connected. 
SQL> alter pluggable database pdb1 close immediate; 
Pluggable database altered. 
SQL> select con_id, name, open_mode from v$pdbs; 
   CON_ID NAME                           OPEN_MODE 
---------- ------------------------------ ---------- 
        2 PDB$SEED                       READ ONLY 
        3 PDB1                           MOUNTED 
        4 PDB2                           READ WRITE 
You can also shut down a PDB by changing the container in the session and issuing shutdown
immediate as shown in the following code:
SQL> alter session set container = pdb1; 
Session altered. 
SQL> shutdown immediate
Pluggable Database closed. 
While the PDB is in a mounted state, the availability of that PDB to the users is declined as
shown in the following code:
SQL> connect system@PDB2 
Enter password:  
ERROR: 

ORA-01033: ORACLE initialization or shutdown in progress 
Process ID: 0 
Session ID: 0 Serial number: 0 
Warning: You are no longer connected to ORACLE. 
SQL> 
Shutting down CDBs
Shutting down a CDB is the same as shutting down a database prior to 12c except at the
backend, where Oracle takes care of shutting down all PDBs first, followed by closing the
CDB, dismounting the CDB, and shutting down the instance. We will see the same shutdown
message as we can see in the previous release:
SQL> show con_name 
CON_NAME 
------------------------------ 
CDB$ROOT 
SQL> shutdown immediate 
Database closed. 
Database dismounted. 
ORACLE instance shut down. 
SQL> 

Changing instance parameters for a CDB/PDB
We have a single spfile or pfile for a CDB. Individual PDBs attached to a CDB do not have
a separate init file or spfile. Parameters configured in the spfile or init.ora file are
applicable to the root container and serve as default values for all PDB containers.
You can set different values in PDBs for parameters where the column ispdb_modifiable is
set to YES in the v$parameter view. These parameters have the scope of the PDB and they are
preserved across the bounce of the database. Once these parameters are explicitly set in PDBs,
they are persisted in a data dictionary and so they carry the value across the bounce of the
database.
The following example shows changes to the value of one of the parameters at the PDB level.
This parameter is modifiable at the PDB level:
SQL> alter system set resumable_timeout=20; 
System altered. 
SQL> select name, value, ispdb_modifiable from v$parameter where name  = 
'resumable_timeout'; 
NAME                 VALUE      ISPDB 
-------------------- ---------- ----- 
resumable_timeout    20         TRUE 
Connect to another PDB and change the value of resumable_timeout to 40. This is to illustrate
that the different PDBs can have different values for the parameter ispdb_modifiable is TRUE:
SQL> connect sys@PDB2_2 as sysdba 
Enter password:  
Connected. 
SQL> show con_name 
CON_NAME 
------------------------------ 
PDB2_2 
SQL> alter system set resumable_timeout=40; 
System altered. 
SQL> show parameter resumable_timeout 
NAME                                 TYPE        VALUE 
------------------------------------ ----------- ------- 
resumable_timeout                    integer     40 
SQL> connect sys@CDB2 as sysdba 
Enter password:  
Connected. 
SQL> show con_name 
CON_NAME 

------------------------------ 
CDB$ROOT 
SQL> select name, value, ispdb_modifiable, con_id from v$system_parameter where 
name  = 'resumable_timeout'; 
NAME                 VALUE      ISPDB     CON_ID 
-------------------- ---------- ----- ---------- 
resumable_timeout    0          TRUE           0 
resumable_timeout    20         TRUE           3 
resumable_timeout    40         TRUE           4 

Modifying PDB settings
We can modify several settings in a PDB without changing the mode of the PDB. But to make
these changes, we need to be connected to the PDB where we are making the changes. These
changes include changing the status of the data file of the PDB, changing the storage attribute
of the PDB, renaming the global name of the PDB, and so on .
Let's take few examples:
1. To take datafile offline in the PDB, please follow these steps:
    [oracle@advait ~]$ sqlplus sys@pdb1 as sysdba 
 
    SQL*Plus: Release 12.1.0.2.0 Production on Fri Jul 29 08:10:32 2016 
 
    Copyright (c) 1982, 2014, Oracle.  All rights reserved. 
 
    Enter password:  
 
    Connected to: 
    Oracle Database 12c Enterprise Edition Release 12.1.0.2.0 - 64bit     
Production 
    With the Partitioning, OLAP, Advanced Analytics and Real   Application 
Testing options 
 
    SQL> alter pluggable database datafile 
'/u01/app/oracle/oradata/deo/pdb1/datafiles/users01.dbf' offline; 
    Pluggable database altered. 
2. To change the storage limit for the PDB, please write the following query:
    SQL> alter pluggable database pdb1 storage(maxsize 10G); 
 
    Pluggable database altered. 
3. To change global name of PDB, please write the following query:
    SQL> alter pluggable database rename global_name to pdb_test; 
 
    Pluggable database altered. 
 
    SQL> select con_id, name, open_mode, restricted from v$pdbs; 
 
        CON_ID NAME                 OPEN_MODE  RES 
    ---------- -------------------- ---------- --- 
             3 PDB_TEST             READ WRITE YES 

Object link and metadata link
When we talk about metadata, as we have seen, Oracle metadata resides in CDB whereas PDB
contains only user metadata (metadata related to user created objects). But Oracle metadata is
very much required for running the Oracle database. So how is the pluggable database able to
access Oracle metadata?
The answer to this question is the metadata link and object link. Let's understand what these
mean.

The metadata link
Oracle 12c stores the metadata of Oracle only in the root container and the pluggable database
contains only the user-related metadata. For example, Oracle stores the OBJ$ table (which is an
underlying table of the DBA_OBJECTS view) in the root container only. Pluggable databases are
able to access the definition of the OBJ$ table in the root container using the metadata link.
Whenever a user creates an object, its metadata is stored in that pluggable database only, but it
refers to the definition of OBJ$ from the root container using the metadata link.
In the following figure, we can see that the definition of the OBJ$ table has been made
available to PDBs via the metadata link:
Metadata of the objects created in the respective PDBs will be stored in those PDBs only. For
example, if you create a table T1 in PDB1, Oracle will add a record of this object in the OBJ$
metadata table in PDB1 only and not in the root container. However, the definition of OBJ$ will
be accessed by PDB1 from the root container. If we query OBJ$ or a DBA_OBJECTS view (which
is created from the OBJ$ table) from the PDB1 database, we will be able to see the record of
table T1 existing in DBA_OBEJCTS. But if we run the same query in the root container, we will
not be able to see the record of the T1 table in DBA_OBJECTS. Thus, querying metadata tables
from different containers shows different results.

Object links
An object link is an internal entity in the container database which is used by the pluggable
database to read the data related to that PDB. In certain cases, Oracle stores the data only in the
root container and other pluggable databases read the data related to that pluggable database
from the root container over the object link. For example, the AWS snapshot data for all
pluggable databases is stored only in the root container. Each PDB uses an object link to
access AWR data related to that PDB from the root container. So views such
as DBA_HIST_ACTIVE_SESS_HISTORY and DBA_HIST_BASELINE show different data in different
container but all the data is stored in the root containers. If we query these AWR tables from
the root container, we will see only the root container-related AWR data.

Container data objects
Container data objects are the metadata views which contain metadata of all the containers
plugged into the CDB. Examples of container data objects are Oracle supplied views whose
names begin with V$, CDB_, X$ tables, and so on. Container data objects also have a mechanism
to restrict the access to the data pertaining to the current container when they are queried from
user PDBs (other than the root container). Only when we query these container data objects
from the root container will we be able to see the data for all the containers (the entire CDB as
a whole). This is done in others to limit exposing sensitive data to user containers (PDBs). It
achieves the security of one user container not being able to view the data of other user
containers, whereas the root container with a common user can view the data of all the
containers.
A container data object is a table or view that shows data relating to the following: one or
more PDBs, or the CDB as a whole.
You can find out if a specific table or view is a container data object by querying the
TABLE_NAME, VIEW_NAME, and CONTAINER_DATA columns of the USER_ | DBA_| ALL_VIEWS |
TABLES dictionary views.
A common user can view the data dictionary for the root and for the PDBS by querying
container data objects. The common user's current container must be the root to do this.
All container data objects have a CON_ID column. The following table shows the meaning of
the values for this column:
Container ID Rows pertain to
0
Whole CDB, or non-CDB
1
CDB$ROOT
2
PDB$SEED
All other IDs User-created PDBs
In a CDB, for every DBA_ view, a corresponding CDB_ view exists. The owner of a CDB_ view
is the owner of the corresponding DBA_ view.

Enabling a common user to access data in a
specific PDB
We can control access to data in various PDBs by using the container clause of the alter
user statement. When we create a common user, that user is available in all containers (the
root as well as PDBs). By default, a common user does not have privileges and we need to
assign the privileges. When we assign privileges to the common user (let's say select
privilege), he can access data in the container data objects specific to that container only. So if
a common user is connected to the root container, he can see users in the root container only
in the cdb_users view. If he connects to one of the PDBs, he can see the users only in that PDB
when he queries cdb_users.
Let's take a simple demo.
I am creating a common user C##DEO in the root container and granting select on CDB_USERS.
Note that I have used container = ALL so select privileges are granted across all containers:
SQL> create user c##deo identified by oracle;          
 
User created. 
 
SQL> grant select on cdb_users to c##deo container = ALL; 
 
Grant succeeded. 
But the important thing to note is that this common user will not be able to view data in all the
containers. This is the property of the container data object. It restricts access to the data of all
the containers. The purpose behind doing this is to limit the exposure of sensitive information
about other PDBs when a common user is querying a specific PDB.
Let's see what data this common user can see. I connect as C##DEO and try to check what
CON_ID I can see in CDB_USERS:
[oracle@advait ~]$ sqlplus c##deo/oracle 
 
SQL> select distinct con_id from cdb_users; 
 
    CON_ID 
---------- 
    1 
I can see only CON_ID = 1. So when I connect to the root container, I see data of the only root
container.
I can connect to the PDB to see what data can be viewed by this common user:
SQL> alter session set container = pdb1; 
 

Session altered. 
SQL> select distinct con_id from cdb_users; 
   CON_ID 
---------- 
   3 
So you can view data of only that PDB.
How do we enable this user to view data of all the containers when he is connected to the root
container? For that we need to change the container_data attribute of this user. We can do so
by using the alter user statement as shown in the following code:
SQL> alter user c##deo set container_data=(CDB$ROOT,PDB1) for cdb_users 
container = current; 
User altered. 
[oracle@advait ~]$ sqlplus c##deo/oracle 
SQL> select distinct con_id from cdb_users; 
   CON_ID 
---------- 
   3 
   1 
As shown in the preceding code, we have set the container_data attribute to (CDB$ROOT, PDB1).
This allows the user to view the data of PDB1 as well when he was querying cdb_users in the
root container. Similarly, we can enable the common user to view data in PDB2 using the
following alter user statement:
SQL> alter user c##deo set container_data=(CDB$ROOT,PDB1,PDB2) for cdb_users 
container = current; 
User altered. 
[oracle@advait ~]$ sqlplus c##deo/oracle 
SQL> select distinct con_id from cdb_users; 
   CON_ID 
---------- 
   1 
   4 
   3 
To find information about the default (user-level) and object-specific CONTAINER_DATA
attributes, query the CDB_CONTAINER_DATA data dictionary view:
SQL> SELECT USERNAME, DEFAULT_ATTR, OBJECT_NAME, ALL_CONTAINERS, CONTAINER_NAME 
FROM CDB_CONTAINER_DATA ORDER BY OBJECT_NAME;

USERNAME           DEFAULT OBJECT_NAME            ALL CONTAINER_
------------------ ------- ---------------------- --- ----------
GSMADMIN_INTERNAL  N       CDB_SERVICES           Y            
C##DEO             N       CDB_USERS              N   CDB$ROOT 
C##DEO             N       CDB_USERS              N   PDB1     
C##DEO             N       CDB_USERS              N   PDB2     
GSMADMIN_INTERNAL  N       GV_$ACTIVE_SERVICES    Y            
APPQOSSYS          N       V_$WLM_PCMETRIC        Y             
SYSTEM             Y                              Y            
SYSBACKUP          Y                              Y            
SYS                Y                              Y            
DBSNMP             Y                              Y             
10 rows selected.

Managing tablespaces in a CDB/PDB
In a non-CDB database, all tablespaces belong to the same database. This includes SYSTEM,
SYSAUX, UNDO, and user tablespaces. In the case of multitenant architecture, we have
separate tablespaces belonging to the root container and each PDB, which includes SYSTEM,
SYSAUX, and TEMP. Only the UNDO tablespace is common for the entire CDB and all other
tablespaces are separate for the root and PDBs.
We have SYSTEM and SYSAUX tablespaces for the root container holding common objects,
which are accessible in PDBs via links. The links enable object sharing across the CDB. Each
PDB will have its own SYSTEM and SYSAUX tablespaces, which store the PDB metadata.
Every PDB also has tablespaces for storing user application data.
Oracle 12c introduced USER_TABLESPACES as a new clause in the create pluggable database
command, specifying which tablespaces will be available in the new PDB being created. This
feature will be handy when you move a non-CDB to a PDB within a CDB, and you can specify
one or more user tablespaces to move.
Application data should not be saved in the root container; instead it should be saved in PDBs.
It is a recommendation not to store any application data in the root.

Default tablespaces for containers
You can assign default permanent tablespaces and default temporary tablespaces to each
container. The default tablespace for a database is a database property. To change the default
tablespace for a CDB root container, you must connect to the root container as a user with the
proper privileges, and issue the alter database command. This operation does not change
the default permanent tablespace of PDBs.
To change the default tablespace for a PDB, you must connect to the PDB as a user with the
proper permissions and issue the alter pluggable database command. 
You can assign the default permanent tablespace and default temporary tablespace to
containers using the following commands:
Default permanent tablespace:
SQL> alter pluggable database pdb1 default tablespace users; 
 
Pluggable database altered. 
Default temporary tablespace:
SQL> alter pluggable database pdb1 default temporary tablespace temp; 
 
Pluggable database altered. 
You can view the default tablespace properties using the database_properties view:
SQL> select property_name, property_value from database_properties  
 2  where property_name like '%DEFAULT%TABLE%'; 
PROPERTY_NAME                  PROPERTY_VALUE 
------------------------------ -------------------- 
DEFAULT_TEMP_TABLESPACE        TEMP 
DEFAULT_PERMANENT_TABLESPACE   USERS 
You can similarly assign the default tablespace to the root container as well. Notice that the
dba_tablespaces lists include fewer tablespaces than the cdb_tablespaces view:
SQL> connect sys@CDB as sysdba 
Enter password:  
Connected.  
SQL> show con_name 
CON_NAME 
------------------------------ 
CDB$ROOT 
SQL> select tablespace_name from dba_tablespaces; 
TABLESPACE_NAME 
------------------------------ 

SYSTEM 
SYSAUX 
UNDOTBS1 
TEMP 
USERS 
DBA_TABLESPACES shows the ones belonging to the root container, whereas
CDB_TABLESPACES shows the tablespaces belonging to all the PDBs and SEED:
SQL> select tablespace_name, con_id from cdb_tablespaces; 
TABLESPACE_NAME                    CON_ID 
------------------------------ ---------- 
SYSTEM                                  3 
SYSAUX                                  3 
TEMP                                    3 
USERS                                   3 
SYSTEM                                  1 
SYSAUX                                  1 
UNDOTBS1                                1 
TEMP                                    1 
USERS                                   1 
SYSTEM                                  4 
SYSAUX                                  4 
TEMP                                    4 
USERS                                   4 
13 rows selected. 
Note that there is only one UNDO tablespace at the CDB level:
SQL> select tablespace_name, con_id from cdb_tablespaces 
 2  where tablespace_name like '%UNDO%'; 
TABLESPACE_NAME                    CON_ID 
------------------------------ ---------- 
UNDOTBS1                                 
Observe that there is a temporary tablespace in the PDBs apart from the root:
SQL> select tablespace_name, con_id from cdb_tablespaces 
 2  where tablespace_name like '%TEMP%'; 
TABLESPACE_NAME                    CON_ID 
------------------------------ ---------- 
TEMP                                    1 
TEMP                                    3 
TEMP                                    4 
SQL> 
Similarly, for data files, DBA_DATAFILES shows the data files of the root container
and CDB_DATAFILES shows the data files of all the PDBs and SEED:
SQL> select file_name from dba_data_files; 

FILE_NAME 
--------------------------------------------------------------------- 
/u01/app/oracle/oradata/deo/deo/datafiles/system01.dbf 
/u01/app/oracle/oradata/deo/deo/datafiles/sysaux01.dbf 
/u01/app/oracle/oradata/deo/deo/datafiles/undotbs01.dbf 
/u01/app/oracle/oradata/deo/deo/datafiles/users01.dbf 
SQL> select file_name, con_id from cdb_data_files; 
FILE_NAME                                                        CON_ID 
------------------------------------------------------------ ---------- 
/u01/app/oracle/oradata/deo/deo/datafiles/system01.dbf                1 
/u01/app/oracle/oradata/deo/deo/datafiles/sysaux02.dbf                1 
/u01/app/oracle/oradata/deo/deo/datafiles/sysaux01.dbf                1 
/u01/app/oracle/oradata/deo/deo/datafiles/undotbs01.dbf               1 
/u01/app/oracle/oradata/deo/deo/datafiles/users01.dbf                 1 
/u01/app/oracle/oradata/deo/pdb1/system01.dbf                         3 
/u01/app/oracle/oradata/deo/pdb1/sysaux01.dbf                         3 
/u01/app/oracle/oradata/deo/pdb1/datafiles/users01.db                 3 
/u01/app/oracle/oradata/deo/pdb2/system01.dbf                         4 
/u01/app/oracle/oradata/deo/pdb2/sysaux01.dbf                         4 
10 rows selected. 

Permanent tablespaces in CDB and PDB
The create tablespace command is the same as it was in the previous release. The only
change in behavior is that a tablespace is created in the container in which this command is
run. You should be careful to separate the data files of the root containers and each PDB into
separate directories:
Create a permanent tablespace in the root container:
    SQL> connect system@deo 
    Enter password:  
    Connected. 
 
    SQL> create tablespace CDB_users datafile 
'/u01/app/oracle/oradata/deo/deo/datafiles/cdb_users01.dbf' size 100M; 
 
    Tablespace created. 
Create a permanent tablespace in the PDB:
    SQL> connect system@PDB1 
    Enter password:  
    Connected. 
    SQL> create tablespace PDB_users datafile 
'/u01/app/oracle/oradata/deo/pdb1/datafiles/PDB_users01.dbf' size 100M; 
 
    Tablespace created. 

Temporary tablespaces in a CDB and PDB
A CDB can have multiple temporary tablespaces but only one temporary tablespace or
tablespace group can be a default temporary tablespace.
Similarly, a PDB can have multiple temporary tablespace but only one tablespace can be the
default temporary tablespace. The temporary tablespace at the PDB level will be transported
along with a PDB when it is unplugged from a CDB
A user, when created in a PDB, can be assigned a default temporary tablespace. This
tablespace will be the default temporary tablespace for that user. If you don't assign any
temporary tablespace to the user, then the default temporary tablespace of the PDB will be
used by the user. If a PDB is not assigned any default temporary tablespace, then the default
temporary tablespace of CDB will be used.
The following example shows how to create a temporary tablespace in the root container and
PDB.
Create a temporary tablespace in the root container:
    password:  
    Connected. 
    SQL> create temporary tablespace CDB_temp tempfile 
'/u01/app/oracle/oradata/deo/deo/datafiles/CDB_temp01.dbf' size 100M; 
 
    Tablespace created. 
Create a temporary tablespace in a PDB:
    SQL> connect system@PDB1 
    Enter password:  
    Connected. 
    SQL> 
 
    SQL> create temporary tablespace PDB_temp tempfile 
'/u01/app/oracle/oradata/deo/pdb1/datafiles/PDB_temp01.dbf' size 100M; 
 
    Tablespace created. 

Managing users and privileges in CDB/PDB
Managing users and privileges in the database is one of the key tasks that DBA has to perform
and DBA should assign the minimum required privileges to users. Since Oracle 12c has
introduced a concept of local and common users, privilege, and roles, assigning privileges to
users has become a little tricky. Let's try to understand this concept a bit deeper starting with
managing users.

Local users and common users
Until the previous release, we only saw one type of user, but in Oracle 12c we have two
different types of users: local users and common users. Let's understand each of these two
types of users and how to manage them.
Local users
In the previous release, a user created in the database can perform operations which are
authorized by system privileges or object privileges. These privileges apply to the database in
which the user is connected. In the previous release, the context of a "single database" is
implied since there the previous release is only one database per instance. But things are
different in Oracle 12c and so are users, roles and privileges. Whatever users, roles, and
privileges are present in the previous release now correspond to local users, local roles, and
local privileges.
Local users exist only in one PDB. Even if the local user with the same name and credential
exists in multiple PDBs, each user is distinct and specific to the PDB and can exercise the
privileges they have in the context of the single PDB. These local users can perform the DDLs
authorized by the system privileges and DMLs permitted by the object privileges only in the
local PDB.
A privileged local user can create another local user and can have other administrative
privileges, but only in the specific PDB where the user is created and present.
An example of creating a local user is shown here:
sqlplus sys/oracle@pdb1 as sysdba 
SQL> create user advait_local identified by oracle; 
User created. 
In this example, we connected to the PDB as a sys user and created a local user advait_local.
Remember that we cannot create a local user in the root container.
Common users
A common user is one which exists in all containers of the database. We can create a common
user only in the root container (CDB$ROOT), but that user gets created in all containers. We can
use the same credential for that common user to connect to any of the containers. So the
identity of the common user is the same across all containers.
A common user is created by another common user with appropriate privileges using the
CONTAINER=ALL clause (default). This implies that the common user which is creating another
common user should have create user and set container privileges in all PDBs.

The type of user created is determined by the CONTAINER clause:
To create a common user, we should be connected to the root container and we should
use the CONTAINER = ALL clause.
For example:
    create user c##deo identified by oracle CONTAINER = ALL; 
To create a local user, we should be connected to the PDB in which we want to create the
local user and we should use the CONTAINER = CURRENT clause.
For example:
    create user advait_local identified by oracle CONTAINER = CURRENT; 
If the container clause is omitted, the default depends on the context. If the common user is
connected to the root container, the default is CONTAINER=ALL. If the common user is
connected to a PDB, the default is CONTAINER=CURRENT.
When a common user is created, if any PDB is not OPEN READ WRITE or OPEN RESTRICTED, then
an attempt will be made to synchronize the common user information later on. If in this
process a new common username conflicts with a local user in the PDB, an error will be
reported. A privileged common user can create a local user by connecting to the PDB with the
CONTAINER=CURRENT clause. When creating a common user, any tablespace, tablespace group,
or profile specified in the CREATE command must exist in every PDB. If none of these are
specified, the default TABLESPACE, TEMPORARY TABLESPACE, and PROFILE for the PDB will be
used.
Any amendment or deletion of a common user triggers modification or removal of the
description of that user in the root and every PDB that is OPEN READ WRITE or RESTRICTED
implies that it is replaying the statement in every container.
A common user will have the same identity across all containers in the database but each
container can have different schema for the common user, meaning that the common user can
own different sets of objects in each container.
The following figure gives some idea about what local and common users looks like in the
container database:

As shown in the preceding figure, we have multiple containers in the container database. We
have the root container (CDB$ROOT) and the pluggable containers PDB1 and PDB2. A
common user is shown as C##COMM_USER. This user is present in every container. So
when we create a common user in the root container, this gets created in every container. It
has the same identity in every container. But every container can have different objects under
this user as shown in the preceding figure. So C##COMM_USER has objects such as TAB1,
TAB2 and so on in the root container, whereas the same user has objects such as, PROC1,
TAB3 in PDB1 and yet different objects in PDB2 represented by FUNC1 and TAB4.
A pluggable database can have a local user as well. In the preceding figure, we are seeing a
local user LOCAL_USER1 as present in PDB1. Since this is a local user, its identity is
specific to only PDB1. We cannot see this user in any other PDB. We can create the same user
in another PDB, but that would be a completely different user and would have to be created
manually. Similarly, we have LOCAL_USER2 created in PDB2. Each of these local users has
its own objects in each of the containers.

Viewing local and common users
The CDB_USERS view, when queried in the root container by the sys user shows all common
and local users. If we query this view by connecting as any other created common user, we
will be able to see only users in that container. This is because of the restrictions that are
applied to container objects.
The DBA_USERS view always shows users in that container only.
The following query shows a few local and common users in the database. The common
column in the CDB_USER view tells us if a user is a local user or a common user:
SQL> select username, common, con_id from cdb_users order by 2;     
USERNAME                  COM     CON_ID 
------------------------- --- ---------- 
LOCAL_PDB2                NO           4 
PDB1_USER                 NO           3 
PDB2_ADMIN                NO           4 
ADVAIT_LOCAL              NO           3 
PDB1_ADMIN                NO           3 
ORACLE_OCM                YES          1 
OJVMSYS                   YES          1 
SYSKM                     YES          1 
XS$NULL                   YES          1 
C##DEO2                   YES          1 
C##DEO1                   YES          1 
An example of creating a common user is shown here:
SQL> create user C##_U01 identified by oracle container=all;               
User created. 
Note that this user is available in all containers:
SQL> select username, con_id from cdb_users where username = 'C##_U01'; 
USERNAME                       CON_ID 
------------------------------ ------ 
C##_U01                             1 
C##_U01                             4 
C##_U01                             3 
SQL> grant create session to c##_u01 container=all; 
Grant succeeded. 
Verify that the user can connect to all the containers since it has the privilege to do so:
SQL> connect c##_u01/oracle@pdb1 
Connected. 
SQL> connect c##_u01/oracle@pdb2 

Connected. 
SQL> connect c##_u01/oracle@cdb 
Connected. 
SQL> 

Local privileges and common privileges
A privilege is called local or common based on the way the privilege is granted. By default, a
privilege does not have a property of being local or common. A privilege granted across all
containers is called a common privilege and a privilege granted in a specific PDB is called a
local privilege.
A common user can be granted both local and common privileges. This means that the
privileges granted to a common user can be different in every PDB. Common and local users
can exercise common and local privileges that have been granted in the context of a PDB that
they are connected to. A common user with common privileges can perform cross-container
tasks while connected to the root container.
To grant a privilege in a multitenant environment, you must include the CONTAINER clause in
the GRANT or REVOKE statement. If you do not include the CONTAINER clause in the grant or
revoke statement, the default value of CONTAINER is CURRENT irrespective of which container
you are connected to; as per the Oracle documentation.
Things to note for commonly granted privileges include:
A commonly granted privilege is applicable to all the existing and future containers.
Only common users can grant privileges commonly and only to other common user.
Only a common user can grant privileges commonly to another common user. We
cannot grant privileges commonly to local user or local user cannot grant privileges
commonly to another local or common user.
A common user must be connected to a root container in order to grant privileges
commonly to another common user. The grant statement should also include
CONTAINER=ALL at the end of the statement.
We can grant both system as well as object privileges commonly. Object privileges
granted commonly are applicable wherever an object exists.
When a common user connects to other containers, he has both commonly granted
privileges and locally granted privileges in that container.
The same common user can have different locally granted privileges in different
containers.
Do not grant privileges to PUBLIC commonly.
Things to note for locally granted privileges include:
A local privilege can be exercised only in the container where it's granted
We can grant local privileges to both a local user and common user
Both a common user and local user can grant local privileges to other local or common
users, provided they have the privileges to grant the privileges
A user granting the privilege should be connected to the container in which he wants to
grant the privilege and the grantor should use CONTAINER=CURRENT when granting the
privileges

Commonly granted system privileges works as follows:
A commonly granted system privilege applies to the root as well as to all existing
containers. It also applies to any container that will be created in the future.
System privileges can be created by a common user to other common users or a
common role or to PUBLIC and they can be granted only when connected to the root
container.
A common user who is granting a common privilege should have been granted that
privilege with ADMIN OPTION. Otherwise, he won't be able to grant the privilege.
The GRANT statement must contain the CONTAINER=ALL at the end.
The following example shows how to grant a common system privilege to another common
user c##hr_admin:
GRANT CREATE ANY TABLE TO c##hr_admin CONTAINER=ALL; 
Commonly granted object privileges work as follows:
An object privilege on a common object grants permission on that object to a common
user from all PDBs. So a common object privilege applies to the object as well as the
object link and metadata link associated with that object.
An object privilege can be granted by a common user to another common user, common
role, or to PUBLIC only.
A common user granting the privilege should have GRANT OPTION on the object to grant
the privilege to another common user, role, or to PUBLIC.
The GRANT statement should contain the CONTAINER=ALL clause at the end.
The following example shows how to grant an object privilege to the common user
c##hr_admin on the table USER_DATE:
    GRANT READ ON user_data TO c##hr_admin CONTAINER=CURRENT; 
Managing privileges
The syntax and behavior of GRANT and REVOKE are mostly unchanged. The syntax has been
extended with the CONTAINER clause so that users with the proper privileges can grant
privileges commonly or locally.
When a user grants a privilege with the CONTAINER=ALL clause, the privilege becomes a
common privilege. The user must have the SET CONTAINER privilege for all the PDBs because
he grants the privilege to the same common user in each container.
When a user grants a privilege with the CONTAINER=CURRENT clause, the privilege becomes a
local privilege.
Let's take some examples.
Granting privileges

The following examples illustrate the concept of granting local and common privileges. All
examples are based on the common user c##_u01 and local user l_u01, which is created in
PDB1 PDB.
We can check CDB_SYS_PRIVS and CDB_TAB_PRIVS to validate if the privileges granted are
local or common.
Example 1:
Grant the CREATE SESSION privilege commonly to the C##U01 user.
We will be connecting to the root and granting the create session privilege to C##U01 using
container=all (commonly):
SQL> grant create session to c##u01 container = all; 
Grant succeeded.
SQL> select grantee, privilege, common, con_id from cdb_sys_privs where grantee 
= 'C##U01';
GRANTEE              PRIVILEGE             COM     CON_ID
-------------------- --------------------- --- ----------
C##U01               CREATE SESSION        YES          1
C##U01               CREATE SESSION        YES          4
C##U01               CREATE SESSION        YES          3
Example 2:
Grant the same create sequence to a common user c##_u01 only in the context of pdb1.
We will first change the container to pdb1 and then grant the required privilege:
SQL> alter session set container = pdb1; 
Session altered. 
SQL> grant create sequence to c##u01 container = current; 
Grant succeeded. 
SQL> alter session set container = cdb$root; 
Session altered.
SQL> select grantee, privilege, common, con_id from cdb_sys_privs where grantee 
= 'C##U01';
GRANTEE              PRIVILEGE          COM     CON_ID
-------------------- ------------------ --- ----------
C##U01               CREATE SEQUENCE    NO           3
C##U01               CREATE SESSION     YES          3
C##U01               CREATE SESSION     YES          1
C##U01               CREATE SESSION     YES          4

As seen in the preceding code, C##U01 is granted create sequence only for pdb1 and the
common column is showing NO. So the privilege is not a common privilege. This is correct
because we used CONTAINER = CURRENT. Only when we use CONTAINER = ALL does the
privilege become a common privilege. Also, we cannot use CONTAINER = ALL when
connected to PDB. We should be connected to the root container when using CONTAINER =
ALL.
Revoking privileges
Revoking privileges is the same as granting privileges. You should be very careful in
revoking privileges because the extended clause CONTAINER plays a similar role in revoking
privileges as it plays in granting privileges.
If you are trying to revoke a common privilege from a common user, you should use the
CONTAINER=ALL clause. Otherwise, Oracle will give an error.
Let's take an example:
SQL> alter session set container = cdb$root; 
Session altered. 
SQL> revoke create session from c##u01; 
revoke create session from c##u01 
* 
ERROR at line 1: 
ORA-65092: system privilege granted with a different scope to 'C##U01' 
We should use CONTAINER = ALL for the common privilege which was assigned using
CONTAINER = ALL:
SQL> alter session set container = cdb$root; 
Session altered. 
SQL> revoke create session from c##u01 container = ALL; 
Revoke succeeded.
SQL> select grantee, privilege, common, con_id from cdb_sys_privs where grantee 
in ('C##U01','L_U01');
GRANTEE              PRIVILEGE             COM     CON_ID
-------------------- --------------------- --- ----------
L_U01                CREATE SESSION        NO           3
L_U01                CREATE ANY TABLE      NO           3
C##U01               CREATE SEQUENCE       NO           3
C##U01               CREATE ANY TABLE      NO           3

Local roles and common roles
Common roles are like common users. They can be created only in root containers and are
available in all existing as well as in any future container that you will create. All Oracle pre-
defined roles are common roles.
A local role exists only in a specific PDB in which you are creating the role. You cannot
create a local role in the root container. Also, a local role cannot have commonly granted
privileges.
The following points should be noted:
A common user can grant common roles to other common users as well as local users.
A common role can be granted to a common user either commonly (in the root container
with the CONTAINERS=ALL clause) or locally (in a specific PDB with the
CONTAINERS=CURRENT clause).
If you grant a common role to a local user in a PDB, privileges associated with that role
are applicable to only that PDB.
Local users cannot create common roles, but they can grant common roles to other
common users or local users within that PDB. These granted roles are locally granted
and not commonly granted.
How common roles work
Common roles can contain commonly granted privileges as well as locally granted
privileges. Common roles apply to the root as well as all the pluggable databases existing and
any pluggable databases that will be created in the future. Locally granted privileges in
common roles are applicable in only those PDBs in which local privileges were granted to
that role.
A common role can be created by a common user in the root container only. It can be granted
by a common user to other common users only. Common users granting a common role to
other common users should have an ADMIN OPTION privilege. While granting a common role
to other common users or other common roles, we should have the CONTAINER=ALL clause at
the end of the statement.
For example, if we grant a DBA role to C##COMM_USER, that user can use privileges assigned to
the DBA role in the root container as well as all other PDBs. However, if this common user
C##COMM_USER is granted the privilege locally in only one PDB, that user can exercise the
privileges of the DBA role only in that PDB.
Rules for creating common roles
While creating common roles, you should be connected to the root container as you cannot
create common roles in PDBs:
Common role names should start with C##. This is governed by the parameter

COMMON_USER_PREFIX. This is applicable to user created common roles and not to Oracle
supplied common roles such as DBA or RESOURCE.
Use CONTAINER=ALL while creating a common role. But even if this clause is omitted, the
role is created as a common role by default.
Rules for creating local roles
While creating local roles, you should be connected to one of the PDB containers as you
cannot create local roles in the root container:
The local role name should not start with C##.
You can use the CONTAINER=CURRENT clause at the end of the CREATE ROLE statement. But
omitting this clause will create a local role by default.
You can use the same name for local roles in different PDBs.
You cannot use common privileges in local roles.
Creating roles
Let's take a look at some simple examples of creating common roles and local roles. An
example of creating a common role follows:
SQL> connect / as sysdba 
Connected. 
SQL> create role c##_r01 container=all; 
Role created. 
An example of creating a local role follows:
SQL> connect system@PDB1          
Enter password:  
Connected. 
SQL> create role l_r01 container=current; 
Role created. 
Granting roles
For granting a role, the grantee user should have the GRANT ANY ROLE privilege or the user
should have been granted the WITH ADMIN OPTION.
To grant a common role to a common user locally in the root:
SQL> connect / as sysdba 
Connected. 
SQL> grant c##_r01 to c##_u01; 
Grant succeeded. 
SQL> select grantee, granted_role, common, con_id from  cdb_role_privs where 

grantee='C##_U01'; 
GRANTEE              GRANTED_ROLE         COM     CON_ID 
-------------------- -------------------- --- ---------- 
C##_U01              C##_R01              NO           1 
Since we skipped the CONTAINER = ALL clause, by default it will grant the role only in the root
container. This behavior is the same as what we have seen in the privileges.
To grant a common role to a common user commonly across all containers:
SQL> grant c##_r01 to c##_u01 container=all; 
Grant succeeded. 
SQL> select grantee, granted_role, common, con_id from  cdb_role_privs where 
grantee='C##_U01'; 
GRANTEE              GRANTED_ROLE         COM     CON_ID 
-------------------- -------------------- --- ---------- 
C##_U01              C##_R01              NO           1 
C##_U01              C##_R01              YES          1 
C##_U01              C##_R01              YES          4 
C##_U01              C##_R01              YES          3 
Now that we specified CONTAINER = ALL, the role is assigned as a common role across all
containers.
Revoking roles
Revoke a common role commonly/locally by using CONTAINER=ALL or CONTAINER=CURRENT.
Revoke a common role from a common user in the root:
SQL> revoke C##_R01 from C##_U01 container=current; 
Revoke succeeded 
Revoke a common role from a common user from the root container across all the
containers:
SQL> revoke C##_R01 from C##_U01 container=all; 
Revoke succeeded. 
SQL> 

Summary
In this chapter, we talked about CDBs and PDBs, how to connect to them, and how to start up
and shut down a CDB and PDB. We also learned about managing the parameters for CDBs
and PDBs and how the same parameter can have different values in different PDBs. Later we
talked about object links and metadata links, which are very important in order to understand
the functioning of PDBs under a CDB. We also learned about container data objects and how
we can use those views to fetch different levels of information from different containers. We
then talked about managing tablespaces, users, and privileges in CDBs and PDBs.
We deep dived into local users and common users, locally granted privileges, and commonly
granted privileges, as well as local roles and common roles. We saw different rules to
understand how these entities work in container databases. We hope all these discussions will
help you in getting a good understanding of users, privileges, and roles in the container
database.
In the next chapter, we are going to talk about ILM and how we can use ILM to manage our
archived data. We are also going to see moving data files online and in-database archiving of
data. At the end, we will talk about temporal validity and temporal history, which are new and
enhanced features of flashback technology.

Chapter 4. Information Lifecycle Management
and Storage Enhancements
Archiving of older data is one of the most painful tasks that DBA has to perform. Not only
does the data need to be preserved safely on a durable device, we also have to make sure that
it is accessible whenever required and in the shortest possible time. We also have to ensure
performance while fetching archived data so as to generate business intelligence reports and
balance the cost of storing such big data.
Oracle 12c introduced automatic lifecycle management of data, which takes care of all the
following three factors effectively: cost of storing archive data is reduced by automatically
moving the data on cheaper storage as it gets older, making the data readily accessible
anytime, and making sure that the performance of fetching the data is good.
We are going to look into following topics in this chapter:
Using ILM features
Performing tracking and automated data placement
Moving a data file online
In-database archiving and valid-time temporal

Information lifecycle management
Information lifecycle management (ILM) is a strategy for managing business data over its
life time, right from creation until deprecation, to reduce the cost of storage, improve data
access within a database, and adapt to regulatory requirements.
All companies have to follow regulatory requirements of maintaining business data for a
period of time. It could be few years to a couple of decades worth of data. It's important that
business maintains this data but at the same time reduces the cost of storage for this data.
Traditionally, old data used to get archived over tapes and those tapes were maintained in
storage facilities in case someone wanted to access that data again. But accessibility was not
easy as the tapes had to be loaded again and read into a temporary storage/database to
generate a required report.
ILM deals with applying such regulatory policies to business data and maintaining the data in
the same database, but within a cost-effective IT infrastructure, and at the same time keeping
that data available and accessible to the business. ILM is a completely automated solution
provided by Oracle which takes advantages of compression and storage tiering to move data
which is out of life into a compressed, high density cost-effective storage tier
Oracle provides two strategies to manage your data as it goes through life-cycle challenges:
Automatic data optimization
In-database archiving
We are going to look into these ILM strategies in more details. We are also going to look into
temporal validity, which provides the capability to create valid-time dimensions for a table's
rows, to indicate its operational relevance and temporal history feature, which is a
reintroduction of an old flashback feature available in Oracle 11g with some enhancements.

Typical lifecycle of data
The life cycle of data starts with insert statements. New data is inserted into the table. At this
stage, data is either kept in a non-compressed form or is compressed by DBA. But
compression for such active data is optimized for DMLs (an example is OLTP table
compression). Compression has a minimal impact on DMLs.
After a month, activity subsides, although significant OLTP transactions are still carried out.
At this stage, data in the former most active partition stays in the OLTP table compressed
format and a new partition is automatically created.
Two months down the line, data is rarely modified and accessed. At this stage, the partition
can be moved to a low-cost storage tier and at higher compression level such as hybrid
columnar compression (HCC).
After one year, data is considered dormant and is no longer accessed or updated. At this stage,
data can be moved to a low cost storage tier with highest level of HCC compression and may
also be marked as read-only.

How automatic data optimization works
Automatic data optimization (ADO) is an ILM strategy that uses activity tracking by using a
heat map to determine if the data is still getting accessed or not.
Heat maps provide the ability to track and mark data as it goes through life-cycle changes.
They can track data at following levels:
Data accesses at segment level
Data modification at segment level and block level
These collected heat map statistics are stored in a SYSAUX tablespace. These statistics
provide a heat map of hot and cold data based on how frequent the data is accessed or
modified.
ADO allows you to create policies that use heat map statistics to compress and move data
when necessary. ADO automatically evaluates and executes policies that perform compression
and storage tiering actions.
The following are the outline steps that can be used to implement and use ADO policies:
1. Enable a heat map at the database level. The heat map tracks data access at segment level
and data modification at segment level and row level.
2. Create ADO policies.
You can create ADO policies at different scopes/levels:
Tablespace level
Group level
Segment level
Row level
You can also create ADO policies based on different operations to track:
Creation
Access
Modification
You can also provide conditions when this policy takes effect:
Based on time period. For example: after 3 days, after 1 week, after 1 year.
Tablespace usage above threshold.
You can also specify different kinds of action for ADO policy:
Compression
Moving to different storage tier
Both compression and moving
3. Evaluate and execute ADO policies. By default, segment level ADO policies are

evaluated every day during the maintenance window. DBA can also create custom
schedules for evaluating ADO policies. Row level policies are evaluated by MMON
every 15 minutes.
4. Verify the ADO execution details by using DBA_ILMEVALUATIONDETAILS and
DBA_ILMRESULTS view.
5. Verify if a segment is compressed or moved based on the action provided in the executed
policy.
We will go into the details of all these steps and see how we can enable ADO polices as per
our business requirements.

Enabling heat maps
A heat map is the basic starting point of implementing ILM. Heat maps provide the
information about access frequency at the segment level. Segments which are accessed
frequently are considered hot by heat map statistics. Let's look into the implementation details.

Enabling and disabling heat maps
The very first step before you can start using ADO is to enable activity tracking, which you
can enable by setting the new initialization parameter heat_map:
SQL> alter system set heat_map = on;
System altered. 
This parameter will enable activity tracking for an entire instance. Enabling a heat map will
enable tracking both DML and access at the segment level and store these details in relevant
tables in SYSAUX tablespace. This will not track any access details of segments in SYSTEM
and SYSAUX tablespace.
Note
An important thing to note here is that a heat map does not work with CDB databases. It works
only for non-CDB databases. So you cannot use this feature with CDB databases.
You can also enable a heat map at the session level using following command:
SQL> alter session set heat_map = on; 
Session altered. 
You can turn off the heat map by setting the parameter to off as shown here:
SQL> alter system set heat_map = off; 
System altered. 

Checking heat map details
Once you enable a heap map, you can check various details of activity tracking provided by
the heat map. You can check details about when segments were accessed recently, whether it
was a row ID access or full scan access, or when the recent DML was performed on the
segment.
V$HEAT_MAP_SEGMENT view
Tracking details of segments are available in real time in the v$heap_map_segment view. As
soon as a session accesses some objects in the database, its heat map information will be
available immediately in this view. In other words the real-time segment access statistics are
stored in the V$HEAT_MAP_SEGMENT view.
For example, I just accessed one of the demo table (SALES) in my test database after enabling
the heat map and I can see the following info:
SQL> select object_name, SUBOBJECT_NAME, TRACK_TIME, SEGMENT_WRITE, 
SEGMENT_READ, FULL_SCAN, LOOKUP_SCAN, con_id from v$heat_map_segment order by 2; 
OBJECT_NAME          SUBOBJECT_ TRACK_TIM SEG SEG FUL LOO     CON_ID 
-------------------- ---------- --------- --- --- --- --- ---------- 
SALES                P10        14-JUN-16 NO  NO  YES NO           0 
SALES                P11        14-JUN-16 NO  NO  YES NO           0 
SALES                P12        14-JUN-16 YES NO  YES NO           0 
SALES                P2         14-JUN-16 NO  NO  YES NO           0 
SALES                P3         14-JUN-16 NO  NO  YES NO           0 
SALES                P4         14-JUN-16 NO  NO  YES NO           0 
SALES                P5         14-JUN-16 NO  NO  YES NO           0 
SALES                P6         14-JUN-16 NO  NO  YES NO           0 
SALES                P7         14-JUN-16 NO  NO  YES NO           0 
SALES                P8         14-JUN-16 NO  NO  YES NO           0 
SALES                P9         14-JUN-16 NO  NO  YES NO           0 
SALES                PM         14-JUN-16 NO  NO  YES NO           0 
I_SALES_ITEM_YEAR               14-JUN-16 NO  NO  NO  YES          0 
SALES_ITEM                      14-JUN-16 NO  NO  YES NO           0 
14 rows selected. 
The following are the key observations in the preceding output:
If you see that the column FULL_SCAN is YES, it means the access was done using a full
scan.
If you see that the column SEGMENT_WRITE is YES, it means a modification was done to the
table
TRACK_TIME is the timestamp when access or modification was done.
You can also see that the LOOKUP_SCAN column is YES for one row where object_name is
the name of the index. So if the index is getting used in your select plan, it will show
LOOKUP_SCAN as YES for such access.
SYS.HEAT_MAP_STAT$ and DBA_HEAT_MAP_SEG_HISTOGRAM

Data from v$heap_map_segment is persisted into the SYS.HEAT_MAP_STAT$ table in SYSAUX
tablespace by the DBMS_SCHEDULER job at regular time periods. This data is then available via
DBA_HEAT_MAP_SEG_HISTOGRAM view. The following shows similar data when we query
DBA_HEAT_MAP_SEG_HISTOGRAM:
SQL> select object_name, subobject_name, track_time, SEGMENT_WRITE, full_scan, 
lookup_scan from DBA_HEAT_MAP_SEG_HISTOGRAM; 
OBJECT_NAME          SUBOBJECT_ TRACK_TIM SEG FUL LOO 
-------------------- ---------- --------- --- --- --- 
SALES                P2         14-JUN-16 NO  YES NO 
SALES                P3         14-JUN-16 NO  YES NO 
SALES                P4         14-JUN-16 NO  YES NO 
SALES                P5         14-JUN-16 NO  YES NO 
SALES                P6         14-JUN-16 NO  YES NO 
SALES                P7         14-JUN-16 NO  YES NO 
SALES                P8         14-JUN-16 NO  YES NO 
SALES                P9         14-JUN-16 NO  YES NO 
SALES                P10        14-JUN-16 NO  YES NO 
SALES                P11        14-JUN-16 NO  YES NO 
SALES                P12        14-JUN-16 YES YES NO 
SALES                PM         14-JUN-16 NO  YES NO 
SALES_ITEM                      14-JUN-16 NO  YES NO 
I_SALES_ITEM_YEAR               14-JUN-16 NO  NO  YES 
14 rows selected. 
DBA_HEATMAP_TOP_TABLESPACES
This view shows heat map information for the top 100 tablespaces. It gives reports at
tablespace level instead of segment level. You can find MIN, MAX and AVG read time, write time,
full table access time, and lookup access time for each tablespace.
DBMS_HEAT_MAP package
The DBMS_HEAT_MAP package can be used to get heat map statistics at an even deeper level. The
default access is only going to let you know heat map statistics at the segment level. But if you
want to go deeper, for example to check which blocks were accessed and at what time, you
can use subprograms in the DBMS_HEAT_MAP package.
The following is a summary of the procedures and functions in the DBMS_HEAT_MAP package:
BLOCK_HEAT_MAP function returns the last modification time for each block in a table
segment
EXTENT_HEAT_MAP function returns the extent-level heat map statistics for a table segment
OBJECT_HEAT_MAP procedure shows the minimum, maximum, and average access times
for all segments belonging to an object
SEGMENT_HEAT_MAP procedure shows the heat map attributes for a segment
TABLESPACE_HEAT_MAP procedure shows minimum, maximum, and average access times
for all segments in a tablespace

For example, we can use DBMS_HEAT_MAP.BLOCK_HEAT_MAP to get heat map statistics of a block
of a segment.
Similarly, the DBMS_HEAT_MAP.EXTENT_HEAT_MAP function provides heat map statistics
information at an extent level.

Enabling ADO policies
Let's move to the next step of implementing ILM. Now that we have enabled a heat map, we
need to create policies at various levels to manage our data. The policy we create will use the
heat map statistical data and take the required action that we define in the policy. There are
four sections in creating an ADO policy.
The level at which you can create ADO policies are as follows:
Tablespace level: A default ADO policy can be defined at the tablespace level and will be
applicable to all the segments existing in that tablespace or new segments getting created
in that tablespace.
Group level: When we define a group level ADO policy, if a table is eligible for a
specific ADO action defined in the policy, the same action would be performed on all
dependent objects. If the action is to compress and the table has LOB columns, all
secureFile LOBs will be compressed as well.
Segment level: Applies to tables and table partitions. If no policy is defined at the
segment level and we have a default policy at the tablespace level, the tablespace level
policy applies to the segment. But the segment level policy overrides the tablespace level
policy.
Row level: Can be created only for tracking modifications. We cannot have a row level
policy for creation and access operations.
Operations to track are:
Creation: Action will be taken on a specified time condition after creation of the data
Access: Action will be taken on a specified time condition after access to the data
Modification: Action will be taken on a specified time condition after modification of
the data
Conditions when this policy comes into effect are:
Based on time period: For example, after 3 days, after 1 week, after 1 year. We can
provide this input to the policy where we want the segment to be compressed after 1
month of no access. So this 1 month is the time period and Oracle will monitor if the
table is not accessed for a month. After that it will compress the table.
Tablespace usage above threshold: We can set a threshold value for tablespace usage
and action will be taken if the space used in the tablespace is above the threshold. The
default value for this is 85% and after crossing the tablespace usage beyond this value,
automatic action (either compressed or moved) will take place.
We can define actions that ADO policy can take. We can have the following actions:
Compression: If we use compression as an action in ADO policy, an object will be
compressed as per the compression level set in the policy.
Data movement: Moving to different storage tier. We can also move our objects to

cheaper storage after a certain period of time.
Both compression and movement: We can also use the combination of the two.
You can create ILM policies during the CREATE TABLE statement or you can add ILM policies
on existing tables using ALTER TABLE statements. A segment can have multiple ILM policies
defined on it.
Before we look into different examples of creating ILM policies let's take a look at various
compression levels available. Data can be compressed while it's being inserted, updated, or
loaded into a table via bulk load.
We have the following four compression levels available:
ROW STORE COMPRESS BASIC: This is basic level of compression and is used while
inserting data into a table without using direct-path insert, using the advanced
compression option (ACO).
ROW STORE COMPRESS ADVANCED: This is a renamed syntax for the previous OLTP table
compression features that were part of ACO. ROW STORE COMPRESS ADVANCED on a heat
table maps to LOW compression for SecureFile LOB segments when GROUP level ADO
policy is implemented.
COLUMN STORE COMPRESS FOR QUERY LOW or HIGH: This corresponds to HCC and provides
higher level compression than ROW STORE COMPRESS. This can be used when we are going
to load data once a day and query several times--it's a kind of bulk load. We should not
use this for situations where we are doing frequent DMLs on the table as performance
will be bad. COLUMN STORE COMPRESS FOR QUERY LOW or HIGH maps to MEDIUM
compression for SecureFile LOB segments when GROUP level ADO policy is
implemented.
COLUMN STORE COMPRESS FOR ARCHIVE LOW or HIGH: This is the highest level of HCC and
it compresses data from 10 times to 50 times. This kind of compression is used when you
are going to access data very infrequently and also when no DMLs are performed.
Because of heavy compression, it saves lot of space and is used for storing archived data.
COLUMN STORE COMPRESS FOR ARCHIVE LOW or HIGH maps to MEDIUM compression for
SecureFile LOB segments when GROUP level ADO policy is implemented.
Let's look at creating ADO polices for compression action and data movement action.

ADO policies for compression action
We can create ADO polices where ACTION for those policies is compression. So when the
timeline for the policy is met, compression action will be taken. These policies are also called
compression policies.
The following are different examples of creating ADO compression policies.
Tablespace level compression policy
The following shows an example of creating a tablespace level compression policy and we
are using ROW STORE compression:
ALTER TABLESPACE USERS DEFAULT ILM ADD POLICY 
ROW STORE COMPRESS ADVANCED 
SEGMENT AFTER 30 DAYS OF NO ACCESS 
In the preceding command, we can see various sections required in the policy
implementation:
TABLESPACE: This tells us that this is the default policy at the tablespace level and
applicable to segments in that tablespace as represented by the SEGMENT keyword.
ILM ADD POLICY: This is required to add a policy to tablespace or segment
ROW STORE COMPRESS ADVANCED: This is the compression action level we are using, as
this is a compression based on ILM policy
SEGMENT: This indicates that this policy is applicable to all segments in the USERS
tablespace.
AFTER 30 DAYS OF: This represents the condition when this policy will be eligible to take
required action of compression.
NO ACCESS: This represents an operation to track. We have three different types of
operations that we define--access, modification, creation
So if we alter the tablespace USERS using the preceding command and add ILM policy, if
any segment in the tablespace, (which has no other ILM policy set) is not accessed for
more than 30 days then that segment will be compressed to ROW STORE COMPRESS
ADVANCED.
Group-level compression policy
The following shows an example of creating a group level compression policy and we are
using ROW STORE compression. This compression policy applies to the main table as well as
dependent objects such as LOB:
ALTER TABLE SALES ILM ADD POLICY 
ROW STORE COMPRESS ADVANCED 
GROUP AFTER 60 DAYS OF NO MODIFICATION 
In the preceding example, we are implementing group level ILM policy. This policy will
automatically compress the SALES table if no modification is done for 60 days. Since we are

using GROUP level policy, dependent LOBs are compressed with LOW compression AFTER 60
DAYS OF NO MODIFICATION.
ALTER TABLE SALES MODIFY PARTITION P1 ILM ADD POLICY 
COLUMN STORE COMPRESS FOR ARCHIVE HIGH 
GROUP AFTER 3 MONTHS OF CREATION 
In this example, we are using a GROUP level compression policy on a partition of a table. This
policy will automatically compress PARTITION P1 after 3 months of creation and
corresponding LOBs will be compressed with MEDIUM compression. This does the highest
level of compression using HCC. Global indexes are maintained.
Segment level compression policy
The following shows an example of creating a segment level compression policy and we are
using COLUMN STORE compression:
ALTER TABLE SALES ILM ADD POLICY 
COLUMN STORE COMPRESS FOR QUERY HIGH 
SEGMENT AFTER 90 DAYS OF NO MODIFICATION 
In this example, we are creating a segment level compression policy on the SALES table. This
policy is going to automatically compress the SALES table with COLUMN STORE COMPRESS FOR
QUERY HIGH if it's not modified for 90 days.
ALTER TABLE SALES ILM ADD POLICY 
ROW STORE COMPRESS 
SEGMENT AFTER 1 YEAR OF CREATION 
This example implements the basic level of compression (ROW STORE COMPRESS [BASIC]) on
the SALES table after 1 year of its creation. The SALES table will be automatically compressed
after 1 year of its creation.
Row level compression policy
The following shows an example of creating a row level compression policy and we are
using ROW STORE compression:
ALTER TABLE SALES ILM ADD POLICY 
ROW STORE COMPRESS ADVANCED 
ROW AFTER 30 DAYS OF NO MODIFICATION 
The preceding policy will compress any block of the SALES table whose rows are not
modified from the last 30 days. Even if one row in a block gets modified, it does not qualify
for compression.
The important thing to note here is that row level compression polices can be created based
on modification time only. They cannot be created based on creation time or access time.
Also, only the compression type available for row level compression policy is ROW STORE
COMPRESS ADVANCED/BASIC. We cannot use columnar compression for row level policy.

ADO policies for data movement action
This is another type of ADO policy that we can set at segment level only. We cannot set this at
tablespace level or group level or any other level. Also, data movement happens at tablespace
level. Meaning that we can move the table or partition to another tablespace based on low cost
storage.
Data movement policy takes the following format:
ALTER TABLE <TABLE_NAME> ILM ADD POLICY 
TIER TO <TABLESPACE_NAME> 
[ CUSTOM ILM RULE | SEGMENT <CONDITION> <OPERATION> ] 
TIER TO is a new clause and we can specify which tablespace the segment should move if it
satisfies the condition set. We can also provide custom ILM rules in the last section of the
command instead of providing a condition (for example: AFTER 30 DAYS, AFTER 1 YEAR,
and so on) and an operation (for example: CREATION, NO MODIFICATION, NO ACCESS, and so
on).
Let's take an example:
ALTER TABLE SALES ILM ADD POLICY 
TIER TO low_cost_storage 
SEGMENT AFTER 60 DAYS OF CREATION 
In this example, we replaced the compression clause with the "TIER TO" clause.
The TIER TO clause provides the tablespace name where the segment should move after it
satisfies the condition. In this example, ILM policy will automatically move the SALES table to
the low_cost_storage tablespace after 60 days of creation.
If we don't provide the last part of the condition and operation, then the policy will be created
and data movement action will take place when the tablespace is running out of space. The
space threshold for the tablespace is already set and we can see those values in the
DBA_ILMPARAMETERS table.
The following are tiering fullness thresholds for a tablespace:
TBS PERCENT USED (default 85): Objects with a tiering policy will be moved out of
the tablespace if its TBS PERCENT USED full (default 85% full)
TBS PERCENT FREE (default 25): Objects with tiering policy will continue to move
out until the source tablespace has TBS PERCENT FREE space (default 25% free)
You can see the current values set by querying the DBA_ILMPARAMETERS view. You can
customize the following parameters using the DBMS_ILM_ADMIN package:
SQL> select * from dba_ilmparameters; 

NAME                                VALUE 
------------------------------ ---------- 
ENABLED                                 1 
RETENTION TIME                         30 
JOB LIMIT                               2 
EXECUTION MODE                          2 
EXECUTION INTERVAL                     15 
TBS PERCENT USED                       85 
TBS PERCENT FREE                       25 
POLICY TIME                             0 
8 rows selected 
For example, if we want to change TBS PERCENT USER to 90% from the existing 85%, we can
use following procedure:
SQL> exec dbms_ilm_admin.customize_ilm(dbms_ilm_admin.tbs_percent_used,90); 
PL/SQL procedure successfully completed 
We will check out other procedures and functions in the DBMS_ILM_ADMIN package in a later
section in this chapter.
So, if we set the following policy, then the SALES table will be moved out of the current
tablespace to the low_cost_storage tablespace when the SALES tablespace reaches 90% of
fullness:
ALTER TABLE SALES ILM ADD POLICY 
TIER TO low_cost_storage 
Also, if we set this policy on many tables in the same tablespace and if the tablespace reaches
the fullness threshold, then tables will be moved to a different tablespace in the order of the
oldest accessed tables first.
The last thing to know about data movement policies is that they are executed only once in a
life time. Since they apply only at the segment level, once the policy is executed on the
segment and the required movement takes place, the policy is disabled for that segment. This
rule applies only for data movement policies and not for compression policies.

CUSTOM_ILM_RULES
Instead of having conditions and operations in the simplified syntax provided by Oracle, you
can have more complex algorithms about when the desired action for ILM policy should take
place by specifying the custom ILM rule. The syntax for the custom ILM rule looks like this:
ALTER TABLE SALES ILM ADD POLICY 
<Compression clause> | <Data movement clause> 
ON <CUSTOM_ILM_RULE> 
A custom ILM rule should be a function which returns a Boolean. You can implement a
complex logic inside the function and return a Boolean after evaluating that complex logic. If
the return value is true, the ILM policy takes action. If the return value is false, the ILM policy
does not take action. This custom ILM rule can be used for both compression action as well as
data movement action policies.

Implementing multiple policies on a segment
A segment can have multiple ADO policies implemented on this. This logically makes sense
as we want to have different actions taken on data as it ages more and more. For example, we
might go for the basic level of compression after 1 week of data insertion, more advanced
compression after a month of no creation, or no modification and the highest level of
compression of moving to a low cost storage after a month of no access or after a year of
creation of data. So to satisfy all these timelines and criteria it's not unusual to have multiple
ILM policies on a segment. But there are rules to follow:
1. The policy should be applied on the same operation or statistics (creation, modification,
or access). If you create a segment level policy on a table based on ACCESS, make sure
that other policies are also created based on the ACCESS operation only. For example, I
create a policy on the SALES table based on ACCESS:
   SQL> alter table demo_user.sales ilm add policy row store compress    
advanced segment after 2 days of no access; 
 
   Table altered. 
Then I create another policy on the same table based on ACCESS:
  SQL> alter table demo_user.sales ilm add policy column store compress for 
query high segment after 30 days of no access; 
 
  Table altered. 
Now, if I tried to create a third policy based on creation or modification, I will get
conflict, because we created the first two policies based on "access" and now we are
trying to create a policy based on "creation", which is not allowed:
  SQL> alter table demo_user.sales ilm add policy column store compress   
for archive high segment after 60 days of creation; 
  alter table demo_user.sales ilm add policy column store compress for   
archive high segment after 60 days of creation 
  * 
  ERROR at line 1: 
  ORA-38323: policy conflicts with policy 21 
2. The compression level should increase with time. You should not reduce the
compression level as the timeline condition increases. In the following example, I tried
to set the column level compression after 7 days of no access and row store compression
(which is lower level compression then column level) after 30 days of no access. This
causes conflicts.
3. There should be only one row level policy allowed per segment. You can have only one
row level policy on a table. In the following example, I tried to create two row level
policies on the same table and it didn't allow the same because it violates the rule of
having only one row level policy on a table.
  SQL> alter table demo_user.sales ilm add policy row store compress   

advanced row after 7 days of no modification; 
 
  Table altered. 
 
  SQL> alter table demo_user.sales ilm add policy row store compress   
advanced row after 30 days of no modification; 
  alter table demo_user.sales ilm add policy row store compress   advanced 
row after 30 days of no modification 
  * 
  ERROR at line 1: 
  ORA-38325: policy conflicts with policy 24 
Also remember that you can only create a no modification operation policy at the row
level.
4. The segment level policy should not overwrite the row level policy. Continuing with the
given example, after creating a row level policy, we can still create a segment level
policy:
  SQL> alter table demo_user.sales ilm add policy column store compress   
for query high segment after 30 days of no modification;
  Table altered.

Policy priority and policy inheritance
As we discussed before, we can create a policy at the tablespace level, which serves as a
default policy for every segment created in that tablespace with no policy or segment level,
and even at partition level. The question remains as to which policy will actually take affect.
Again, this follows rules. The following are the simple rules to understand this:
The child level policy always overrides the parent level policy for the same action. By
this logic, policies set at the table partition will override the one set at the table level if
both have the same actions and policies at the table level will override the policy at the
tablespace level if both have the same actions.
If no policies are defined at the child level, inheritance takes place. For example, if no
ILM policies are defined at the partition level, table level policies are applied at the
partition level. Also, if no ILM policies are defined at the table level, tablespace level
policies are inherited at the table level.
Inheritance is additive if policy actions are different. For example, if you have a
compression policy at the tablespace level and a condition and compression policy at the
segment level after some specified condition then the total effect on the segment is the
sum of the effect of both policies, that is compression plus data movement.
SQL> select policy_name, object_name, subobject_name, object_type, 
inherited_from from dba_ilmobjects;
POLICY_NAM OBJECT_NAME  SUBOBJECT_ OBJECT_TYPE      INHERITED_FROM
---------- ------------ ---------- ---------------- ---------
P25        SALES        P1         TABLE PARTITION  TABLE
P25        SALES        P10        TABLE PARTITION  TABLE
P25        SALES        P11        TABLE PARTITION  TABLE
P25        SALES        P12        TABLE PARTITION  TABLE
P25        SALES        P2         TABLE PARTITION  TABLE
P25        SALES        P3         TABLE PARTITION  TABLE
P25        SALES        P4         TABLE PARTITION  TABLE
P25        SALES        P5         TABLE PARTITION  TABLE
P25        SALES        P6         TABLE PARTITION  TABLE
P25        SALES        P7         TABLE PARTITION  TABLE
P25        SALES        P8         TABLE PARTITION  TABLE
P25        SALES        P9         TABLE PARTITION  TABLE
P25        SALES        PM         TABLE PARTITION  TABLE
P25        SALES                   TABLE            POLICY NOT
INHERITED
P41        SALES_ITEM              TABLE            TABLESPACE
15 rows selected.
We can check different policies implemented on different objects using the DBA_ILMOBJECTS
view.
In the preceding example, policy P41 is applied on the SALES_ITEM table as it exists in the
USERS tablespace and has no ILM policy defined on it. But since we have the default ILM

policy at the tablespace level USERS, this table SALES_ITEM has inherited this policy from the
tablespace. In the column INHERITED_FROM, you can see the value as TABLESPACE.
Similarly, policy P25 is applied to the table and all its partition. But we created the policy at
the table level only and not at the partition level so all partitions of the table inherit the table
level policy.
For the object type TABLE and policy P25, we see POLICY NO INHERITED in the
INHERITED_FROM column. This is because we have created a new policy at the table level and
it's not getting inherited from any level.

Checking ILM policy details
We have a couple of views to provide details about ILM policies that exists at various levels in
the database.
DBA_ILMPOLICIES
The DBA_ILMPOLICIES view contains the information of ADO policies in the database. You
can view all the policies configured in the database in this view:
SQL> select * from dba_ilmpolicies; 
POLICY_NAM POLICY_TYPE   TABLESPACE ENABLE DELETED 
---------- ------------- ---------- ------ ------- 
P24        DATA MOVEMENT            YES    NO 
P25        DATA MOVEMENT            YES    NO 
P41        DATA MOVEMENT USERS      YES    NO 
Even compression policies are represented as data movement policies in this table. This is
because during compression data movement happens.
DBA_ILMDATAMOVEMENTPOLICIES
This view contains all the data movement related attributes of an ADO policy. The following
query shows all the attributes for the data movement:
SQL> select policy_name, compression_level, condition_type, condition_days from 
DBA_ILMDATAMOVEMENTPOLICIES;
POLICY_NAM COMPRESSION_LEV CONDITION_TYPE         CONDITION_DAYS
---------- --------------- ---------------------- --------------
P41        ADVANCED        LAST MODIFICATION TIME             60
P24        ADVANCED        LAST MODIFICATION TIME              7
P25        QUERY HIGH      LAST MODIFICATION TIME             30

Evaluating and executing ADO policies
We have two ways of evaluating and executing ADO policies scheduled in the maintenance
window, or evaluating and executing it manually. Let's look at both approaches

Scheduled in maintenance window
Row level policies are evaluated and executed every 15 minutes by the MMON background
process. Segment level policies are evaluated and executed daily once during the maintenance
window using a scheduler job. This scheduler job uses the DBMS_ILM.EXECUTE_ILM_TASK
procedure to evaluate and execute ADO policies.
Note
Note that the name of the package is DBMS_ILM as against DBMS_ILM_ADMIN, which is used for
customizing ILM parameters and other admin related tasks. We will look at the
DBMS_ILM_ADMIN package in a short while.
In the DBMS_ILM.EXECUTE_ILM_TASK procedure, TASK_ID is an input parameter which is
automatically generated during the evaluation process, which also happens during the
scheduled maintenance window.
You can change the interval of evaluation and execution using the
DBMS_ILM_ADMIN.CUSTOMIZE_ILM procedure. We have seen this procedure previously when we
were changing the TBS PERCENT FREE and TBS PERCENT USED parameters. We can use same
procedure to customize execution of the interval of ILM tasks:
SQL> exec dbms_ilm_admin.customize_ilm(dbms_ilm_admin.execution_interval, 1); 
PL/SQL procedure successfully completed. 

Manually executing ADO policies
For manually executing ADO policies, we have two options:
Directly execute ADO policies without doing any customization
Preview the evaluation of ADO policies, make any desired changes to the task, and then
execute the task
You can directly execute ADO policies without doing any customization using the
DBMS_ILM.EXECUTE_ILM procedure. Note that this procedure is different than the one that runs
in the scheduled maintenance window (DBMS_ILM.EXECUTE_ILM_TASK). There are multiple
definitions of this procedure and in a simple definition, you can just pass the owner and table
name and all the policies in that table will be executed.
You can also preview the evaluation result before you execute the task. Perform the following
steps to customize ILM tasks:
1. Evaluate the set of ADO policies and preview the result. Check which tables/policies will
be executed.
2. Add/remove objects/subobjects from the ILM tasks evaluation result.
3. Execute the final customized evaluated ILM task.
You can use the following procedure in the same order to perform the preceding steps of
customizing ILM tasks:
1. Execute the DBMS_ILM.PREVIEW_ILM procedure to preview the ILM tasks for the database
or specific schema.
2. Execute DBMS_ILM.ADD_TO_ILM or DBMS_ILM.REMOVE_FROM_ILM to add or remove
objects/subobjects from the evaluated task.
3. Execute the DBMS_ILM.EXECUTE_ILM_TASK to execute the aforementioned customized ILM
task.
For example:
var v_task_id number; 
exec DBMS_ILM.PREVIEW_ILM(TASK_ID=> :v_task_id, 
ILM_SCOPE=>DBMS_ILM.SCOPE_SCHEMA); 
Let's say the preceding command generated the task_id as 1234:
exec DBMS_ILM.ADD_TO_ILM(TASK_ID=>1234, OWN=>DEMO_USER', 
OBJ_NAME=>'NEW_TABLE'); 
exec DBMS_ILM.EXECUTE_ILM_TASK(TASK_ID=>1234); 
You can also pass the following two parameters to DBMS_ILM.EXECUTE_ILM_TASK:
EXECUTION_MODE: ILM_EXECUTION_ONLINE value executes the task online and

ILM_EXECUTION_OFFLINE executes the task offline.
EXECUTION_SCHEDULE: The only possible value for this parameter is
DBMS_ILM.SCHEDULE_IMMEDIATE

Checking execution details of ADO policies
You can check the details of ADO policies evaluation using following three views.

DBA_ILMTASKS
Every task that is executed manually or automatically as scheduled will be recorded in the
DBA_ILMTASKS view. It stores the creation time, start time, and completion time of the task and
if the task is completed, failed, or active:
SQL> select TASK_ID, STATE, START_TIME, COMPLETION_TIME from DBA_ILMTASKS order 
by task_id;
TASK_ID STATE     START_TIME              COMPLETION_TIME
------- --------- ----------------------  ----------------------
     1 COMPLETED 15-JUN-16 10.19.24 PM   15-JUN-16 10.19.24 PM
     2 COMPLETED 15-JUN-16 10.34.27 PM   15-JUN-16 10.34.27 PM
     3 COMPLETED 15-JUN-16 10.49.30 PM   15-JUN-16 10.49.30 PM
     4 COMPLETED 15-JUN-16 11.04.32 PM   15-JUN-16 11.04.32 PM
     5 COMPLETED 15-JUN-16 11.19.35 PM   15-JUN-16 11.19.35 PM

DBA_ILMEVALUATIONDETAILS
This view can be used to see details on the task execution after the evaluation of the ADO
policies is completed. If the SELECTED_FOR_EXECUTION column in this view has the value
selected for execution, then the policy has been selected for execution indeed and an ADO
job will be executed to satisfy the ILM policy. However, a job may not execute if there's a
value other than selected for execution under the column SELECTED_FOR_EXECUTION.
The following values all mean that the ADO policy hasn't successfully passed the evaluation:
Precondition not satisfied
Policy disabled
Policy overruled
Inherited policy overruled
Job already exists
No operation since the last ILM action
Target compression not higher than current
Statistics not available

DBA_ILMRESULTS
This provides details about completed ILM tasks. It has a STATISTICS column which is a CLOB
and provides job-specific statistics, such as space saved via compression, and so on.

DBMS_ILM_ADMIN package
We have many functions in the DBMS_ILM_ADMIN package which are useful for managing the
information life cycle at the database level and segment level. The following are some of the
important functions:
CLEAR_HEAT_MAP_ALL and CLEAR_HEAT_MAP_TABLE procedures are used to clear all heat map
information for all table or specific segments in a database.
The CUSTOMIZE_ILM procedure is used to customize parameters for tablespace full and
tablespace free. These are tiering fullness thresholds for the tablespace:
TBS PERCENT USED (default 85): Objects with a tiering policy will be moved out of
the tablespace if its TBS PERCENT USED is full (default 85% full)
TBS PERCENT FREE (default 25): Objects with a tiering policy will continue to move
out until the source tablespace has TBS PERCENT FREE space (default 25% free).
You can set many other properties using the CUSTOMIZE_ILM procedure such as the execution
interval of the scheduled ILM execution task, and so on.
ENABLE_ILM and DISABLE_IML procedures are used to enable and disable ADO without
deleting any of the policies. We can keep all the policies created at the tablespace level, group
level, segment, and row level intact and just execute DISABLE_IML and ADO will be disabled.
No policies will be executed and no changes will be done for old data.

Enabling and disabling ADO policies
You can enable and or disable ADO policies at the individual table level or at the database
level completely.

Enabling/disabling ADO policies
You can enable/disable individual ADO policy on a table. The following example shows how
to disable and enable a specific policy:
SQL> alter table demo_user.sales ilm disable policy P25; 
Table altered. 
SQL> alter table demo_user.sales ilm enable policy P25; 
Table altered. 
You can also enable/disable all policies on a table:
SQL> alter table demo_user.sales ilm disable_all; 
Table altered. 
SQL> alter table demo_user.sales ilm enable_all; 
Table altered. 

Deleting ADO policies
You can delete an individual policy on a table using the policy name. The following example
shows deleting a policy:
SQL> alter table demo_user.sales ilm delete policy P24; 
Table altered. 
If you want to delete all policies on a table, you can use delete_all:
SQL> alter table demo_user.sales ilm delete_all; 
Table altered. 

Enabling/disabling ILM
You can disable ILM completely in a database using the DBMS_ILM_ADMIN.DISABLE_ILM
procedure. This disables ILM, but keeps all policies intact. Those policies will not get
executed until we enable ILM:
SQL> exec DBMS_ILM_ADMIN.DISABLE_ILM; 
PL/SQL procedure successfully completed. 
SQL> exec DBMS_ILM_ADMIN.ENABLE_ILM; 
PL/SQL procedure successfully completed. 

Moving data files online
One of the most useful feature of Oracle 12c is moving data files online while we have DMLs
and DDLs running in a database and accessing data from the data file being moved. This
feature is useful in many ways. One important advantage of this is that we can move data files
from the normal unix ext3/ext4 filesystem to ASM without taking any downtime.
The ADO feature that we discussed previously takes advantage of this functionality. Actions
such as data movement to a different storage tier can be performed online. This means that a
maintenance operation can move data files and tablespaces from a high cost to a low cost
storage system while letting ADO data movement actions execute ADO compression actions,
thus taking advantage of the online move data file capability.
The following command gives an example of moving data files online:
SQL> alter database move datafile 
'/u01/app/oracle/oradata/deo/deo/datafiles/system-01.dbf' to 
'/u02/app/oracle/oradata/deo/deo/datafiles/system-01.dbf' ; 
 
Database altered. 
You can also move data files to ASM by giving the disk group name and path. You can also
use the REUSE clause in a data file move, which will overwrite the data file at the destination if
the data file with the same name exists. This is especially useful if a previous data file move
command has failed. By default, REUSE is not enabled.
You can also use the KEEP clause if you want to keep the data file at the source location even
after the move. By default, the data file at the source location will be deleted after the move.
Moving data files takes time as the entire data file is copied to a new location. You can
monitor the progress of the data file move in the V$SESSION_LONGOPS view. If you have
multiple data files being moved, you can see multiple records in this view, one for each data
file. V$SESSION_LONGOPS shows the state and process of the data file being moved.
Progress is shown by the number of blocks moved so far. The state goes through the
following cycle:
NORMAL => COPYING => SUCCESS => NORMAL
An online data file move will fail for the following conditions:
Data file offline
"Flashback Database" operation is running
Concurrent media recovery is running
Data file resize is happening (only if it's shrinking)
An online data file move will succeed for following conditions:

Block media recovery happening on the data file being moved
Tablespace containing the data file being moved is made read-only or read-write
Data file resize is happening (extending the size)
Online backup of data file is happening
If you move a data file to a new location and at a later point (a few hours later after the data
file move), you flash back the database to a point prior to the data file move, then the data file
will not be moved back to previous location where it was. The data file will continue to
remain at the new location only after the database flashback to the time prior to the data file
move.

Moving partitions online
Oracle 12c allows partitions to be moved online. It means that we can perform online moving
of partitions while there are DMLs running on that partition. Until the previous release, the
online partition move was not supported and all DMLs used to get blocked until the partition
move was done. This is no longer the case in Oracle 12c.
The online partition move has a great advantage during maintenance operation and especially
in implementing ADO policies. ADO policies require partitions to be moved to low-cost
storage or they need to be compressed. ADO takes advantage of the online partition move to
complete these activities in the background while continuing to run DMLs for users.
Note that the online move of the partition cannot be performed if we have a concurrent DDL
running on the partition.
The following command will move partition p1 of the SALES table to a different tablespace:
SQL> alter table demo_user.sales move partition p1 tablespace low_cost_storage 
update indexes online; 
 
Table altered. 
When you run the partition move command, it waits for all existing DMLs to complete. This
includes all DMLs that started before the online move command was executed.
The UPDATE INDEXES clause maintains global indexes automatically. There is no need to
perform index rebuilds after the move.
The ONLINE clause is important as it does not block any DMLs on the table/partition.
A move is also used for the compression of partitions and is either done manually or as part
of ADO policies. For example:
SQL> alter table demo_user.sales move partition p2 row store compress advanced 
update indexes online; 
 
Table altered. 

In-database archiving and temporal validity
One of the major challenges faced by an Oracle DBA is: how to effectively deal with
historical data. Today, if we consider database tables for an enterprise, data in the table goes
back several years and most of the data in the table is inactive. The challenge remains as to
how to archive this data and make our query run efficiently.
If we choose to archive old data outside of a database on a tape, the cost of storing data
reduces but older data is not available for one-off reports. Also regulatory compliance
suggests that older historical data should be accessible all the time.
If we choose to maintain old data inside the database, the size of the database grows, and so
the size of backup and storage costs increase. Not only that, queries on bigger tables do not
run efficiently, so there are performance costs associated with queries, DMLs, and DDLs (for
example, index creation and rebuild).
In 11g, older archived data is handled using following two solutions:
Partitioning: Distinguish the data as old or new based on the date of creation or some
date attribute in the table and partition the table according to that column. DBA can
manually move older partitions to low-cost storage.
Flash back data archive (total recall feature in 11g): This feature automatically track
changes to the data over a period of time and stores the changes that have happened over
time in another tablespace. The amount of change tracking that will be stored in another
tablespace depends on the RETENTION parameter that we configure when we create the
flashback data archive.
When we want to query the historical data, Oracle uses the changes tracked in another
tablespace to regenerate the data that exists at a time in the past. We have to use the AS OF
clause and provide a timestamp or SCN to view the data in the past. We also have RETENTION
parameter to automatically purge the data that has passed beyond the retention time.
Oracle 12c provides additional solutions to the aforementioned challenges. We have two ways
to handle the situation for archive data in Oracle 12c:
In-database archiving: In-database archiving deals with archiving at the row level. We can
distinguish between the rows which are active and those that need to be archived. Oracle
provides a new hidden column; ORA_ARCHIVE_STATE, which when set manually by users will
make that row eligible for archiving.
The following is an example describing how in-database archiving works:
SQL> show user
USER is "L_U01"
SQL> show con_name
CON_NAME

------------------------------
PDB2_1
Create a table with clause row archival:
SQL> create table ilm_table1 (eno number, ename varchar2(20), sal number) row 
archival; 
Table created.
Insert rows into table:
SQL> insert into ilm_table1 values (1, 'avdeo', 10000);
1 row created. 
SQL> insert into ilm_table1 values (2, 'imorishe', 20000); 
1 row created. 
SQL> insert into ilm_table1 values (3, 'avdeo2', 30000); 
1 row created. 
SQL> insert into ilm_table1 values (4, 'imorishe2', 40000); 
1 row created. 
SQL> commit; Commit complete.
RA_ARHIVE_STATE column is automatically created when row archival clause is used.
But when you describe the table this column is not visible and it is not visible in ILM_TABLE1
output as well as shown in the following code:
SQL> select * from ilm_table1;
      ENO ENAME                       SAL
---------- -------------------- ----------
        1 avdeo                     10000
        2 imorishe                  20000
        3 avdeo2                    30000
        4 imorishe2                 40000
SQL> desc ilm_table1
Name              Null?    Type
----------------- -------- ---------------
ENO                        NUMBER
ENAME                      VARCHAR2(20)
SAL                        NUMBER
When you explicitly include the column name, then it is visible and it has value 0 by default
which means that it is active:

SQL> select eno, ename, sal, ora_archive_state from ilm_table1;
 ENO ENAME               SAL ORA_ARCHIV
----- ------------ ---------- ----------
   1 avdeo             10000 0
   2 imorishe          20000 0
   3 avdeo2            30000 0
   4 imorishe2         40000 0
Since it is Oracle managed column, it cannot be altered to visible column:
SQL> alter table ilm_table1 modify (ora_archive_state visible);
alter table ilm_table1 modify (ora_archive_state visible)
*
ERROR at line 1:
ORA-38398: DDL not allowed on the system ILM column
The following shows that the column is hidden and it is not user generated.
SQL> select table_name, column_name, hidden_column, user_generated from 
user_tab_cols  where table_name = 'ILM_TABLE1'
TABLE_NAME           COLUMN_NAME          HID USER_GENERATED
-------------------- -------------------- --- --------------
ILM_TABLE1           SAL                  NO  YES
ILM_TABLE1           ENAME                NO  YES
ILM_TABLE1           ENO                  NO  YES
ILM_TABLE1           ORA_ARCHIVE_STATE    YES NO
You can make a few rows in a table inactive by setting ORA_ARCHIVE_STATE to non-zero:
SQL> update ilm_table1 set ora_archive_state = 2 where eno=1;
1 row updated.
SQL> update ilm_table1 set ora_archive_state=
dbms_ilm.archivestatename(2) where eno=2;
1 row updated.
Only active rows are visible as shown below.
SQL> select eno, ename, sal, ora_archive_state from ilm_table1;
 ENO ENAME                SAL ORA_ARCHIV
----- ------------- ---------- ----------
   3 avdeo2             30000 0
   4 imorishe2          40000 0
You can see inactive rows by setting row archival visibility to all as follows:
SQL> alter session set row archival visibility = all;
Session altered.

SQL> select eno, ename, sal, ora_archive_state from ilm_table1;
      ENO ENAME               SAL ORA_ARCHIV
---------- ------------ ---------- ----------
        1 avdeo             10000 2
        2 imorishe          20000 1
        3 avdeo2            30000 0
        4 imorishe2         40000 0
Note that the function DBMS_ILM.ARCHIVESTATENAME(N) returns 0 for n=0 and 1 for n <> 0
Temporal validity: In the case of temporal validity, Oracle adds two hidden columns of date
data type to determine the start date and the end date of the records. Temporal validity can also
be used with temporal history to determine which records were valid in the past.
The following is an example describing how temporal validity works:
SQL> create table orders as select * from OE.orders;
Table created.
SQL> select count(*) from orders;
COUNT(*)
----------
1234
SQL> alter table orders add period for track_time;
Table altered.
There are three hidden columns created by Oracle when the preceding alter is issued, they are
TRACK_TIME, TRACK_TIME_START and TRACK_TIME_END:
SQL> desc orders
Name                    Null?    Type
----------------------- -------- ----------------------------
PROD_ID                 NOT NULL NUMBER
CUST_ID                 NOT NULL NUMBER
TIME_ID                 NOT NULL DATE
ORDER_TIME              NOT NULL DATE
ORDER_QTY               NOT NULL NUMBER(10,2)
AMOUNT_SOLD             NOT NULL NUMBER(10,2)
SQL> select column_name, hidden_column from user_tab_cols where
table_name='ORDERS';
COLUMN_NAME            HIDDEN_COLUMN
---------------------- --------------
TRACK_TIME             YES
TRACK_TIME_END         YES
TRACK_TIME_START       YES
PROD_ID                No
CUST_ID                No
TIME_ID                No
ORDER_TIME             No
ORDER_QTY              No
AMOUNT_SOLD            No

Use DBMS_FLASHBACK_ARCHIVE to set the valid time visibility as of the given time (or) to set
the visibility of temporal data to the currently valid data within the valid time period at the
session level (or) to set the visibility of temporal data to the full table, which is the default
temporal table visibility.
The visibility is applicable to all DMLs and selects:
SQL> select count(*) from orders;
COUNT(*)
----------
1234
Update the table so that the rows with timestamp > 31-MAR-2010 are set to be active or valid:
SQL> update orders set track_time_start=sysdate;
1234 rows updated.
SQL> update orders set track_time_end=sysdate where time_id < '31-MAR
2010';
321 rows updated.
SQL> execute DBMS_FLASHBACK_ARCHIVE.enable_at_valid_time('CURRENT');
PL/SQL procedure successfully completed.
The count(*) from the orders statement returns the rows which are active or current:
SQL> select count(*) from orders;
COUNT(*)
----------
913
Now if you move the data from 31-MAR-2010 to 31-MAR-2014:
SQL> update orders set track_time_end=sysdate where time_id < '31-MAR-2014';
1040 rows updated.
Now the count(*) returns less rows:
SQL> execute DBMS_FLASHBACK_ARCHIVE.enable_at_valid_time('CURRENT');
PL/SQL procedure successfully completed.
SQL> select count(*) from orders;
COUNT(*)
----------
194
SQL> execute DBMS_FLASHBACK_ARCHIVE.enable_at_valid_time('ALL');
PL/SQL procedure successfully completed.
SQL> select count(*) from orders;
COUNT(*)
----------
1234
Before we check these new features let's check out the new compression utility which was
specifically designed in the database archiving feature.

Oracle Hybrid Columnar Compression
Oracle has introduced a deeper table compression feature, called Hybrid Columnar
Compression (HCC). HCC is designed mainly for in-database archiving and provides from
15X to 50X compression. The goal is to keep as much data as possible online in the database.
But this type of compression requires Exadata, ZFS or Pillar Axiom 600 storage. There are
two levels of compression in HCC and we can choose the level of compression depending on
our requirements as stated here:
Warehouse compression: This is idle and is for compressing data that is used by queries in
the data warehouse. Specifically, if you have a big fact table that you use in your query. This
type of compression provides a compression ratio of 10X-15X depending on the data. This
does not cause too much overhead for queries to endure at the same time as having 10X-15X
compression reduces the amount of IO to be done from the disk (the number of block reads).
Archive compression: This is best used for in-database archiving where we are trying to
compact and keep as much data as possible. Access to this data is less frequent so having a
higher compression ratio is beneficial in terms of storage savings. This compression ratio is
around 50X depending on the data. The data remains online and is accessible to queries but
the overhead of decompressing the data during query access is high. This is specifically
useful for archive data which is not accessed frequently.
How does HCC compression work?
Normal database compression compresses repeated values found in the row of a table. So if a
row of a table has multiple columns, it takes into account all the columns of the row and tries
to compress repeated values. This type of compression is not very efficient because we have
very different data in each column of the table and even different data types. We might be
having varchar2 data followed by date or integer. In that case, fewer repeated values are
found.
HCC stores data in a different format. Instead of storing in a row format, HCC stores data in a
column format. So data in all rows of a column will be stored together. Now, since we have
data for one column stored together, Oracle will find lots of repeated values because the
nature of the data is the same and having the same type of data (for example, date) will have
lots of repeated values. That's how HCC is able to achieve a 50X compression ratio.

In-database archiving
Until now, it was very difficult to identify active data from in-active data at the row level. In
Oracle 12c, we can identify active data versus inactive data at the row level. This is possible
by attaching a property to every row of a table to mark it as inactive. If the row is not marked
inactive, it's an active row. Setting such a property at row level helps separate active and non-
active data in a table and lets you archive rows in a table by marking them inactive. The key
thing to understand here is that the data remains in the table and you can compress it, but to the
applications, this part of the data (inactive) remains invisible. We also have a session level
parameter which enables us to see active as well as inactive data (if required).
In-database archiving is enabled at the table level, either during the creation of the table or
later by altering the table. The following is an example of creating a table with in-database
archiving:
SQL> create table sales_archive 
  2  (year number(4), 
  3  product varchar2(10), 
  4  amt number(10,2)) 
  5  row archival; 
 
Table created. 
When you create a table with the ROW ARCHIVAL clause (line 5 in the preceding example), it
automatically adds an extra column ORA_ARCHIVE_STATE to the table. But this column is added
as hidden column. If you try to describe the table, you won't see this column:
SQL> desc sales_archive 
 Name                    Null?    Type 
 ----------------------- -------- ---------------- 
 YEAR                             NUMBER(4) 
 PRODUCT                          VARCHAR2(10) 
 AMT                              NUMBER(10,2) 
But you can check if the column is added or not using the dba_tab_columns view:
SQL> select table_name, column_name, column_id, HIDDEN_COLUMN from dba_tab_cols 
where table_name = 'SALES_ARCHIVE'; 
 
TABLE_NAME           COLUMN_NAME           COLUMN_ID HID 
-------------------- -------------------- ---------- --- 
SALES_ARCHIVE        ORA_ARCHIVE_STATE               YES 
SALES_ARCHIVE        YEAR                          1 NO 
SALES_ARCHIVE        PRODUCT                       2 NO 
SALES_ARCHIVE        AMT                           3 NO 
Similarly, you can enable an existing table for a row archival using the alter table as shown in
the
SQL> alter table sales row archival

Table altered. 
SQL> select table_name, column_name, column_id, HIDDEN_COLUMN from dba_tab_cols 
where table_name = 'SALES'; 
TABLE_NAME           COLUMN_NAME           COLUMN_ID HID 
-------------------- -------------------- ---------- --- 
SALES                YEAR                          1 NO 
SALES                PRODUCT                       2 NO 
SALES                AMT                           3 NO 
SALES                ORA_ARCHIVE_STATE               YES 
The ORA_ARCHIVE_STATE column is a very important column for distinguishing active data
versus inactive data. The column ORA_ARCHIVE_STATE can take two values-0 and 1. By default,
a newly inserted row is active and is denoted by the value 0 for the ORA_ARCHIVE_STATE
column. When the rows start being rarely accessed and not updated any longer, they're
considered to be in the non-active state and are denoted by the value 1 (or any value other than
zero) for the ORA_ARCHIVE_STATE column.
If you want to check the value of ORA_ARCHIVE_STATE column, you have to select the column
explicitly in your select statement.
SQL> select year, product, amt, ORA_ARCHIVE_STATE from sales_archive; 
     YEAR PRODUCT           AMT ORA_ARCHIV 
---------- ---------- ---------- ---------- 
     2001 A                 100 0 
     2002 B                 200 0 
     2003 C                 300 0 
     2004 D                 400 0 
     2005 E                 500 0 
     2006 F                 600 0 
     2007 G                 700 0 
7 rows selected. 
As you can see, we have a value of 0 in all the selected rows, which means all this data is
active.
Note that if rows are rarely accessed, the column ORA_ARCHIVE_STATE will not be set to a
value of 1 automatically. A DBA or owner of the data has to decide which rows are not
accessed and are inactive and they have to manually update the value of the
ORA_ARCHIVE_STATE column.
The following DML shows how to mark the data inactive:
SQL> update sales_archive set ORA_ARCHIVE_STATE = DBMS_ILM.ARCHIVESTATENAME(1) 
where year = 2001; 
1 row updated. 

SQL> commit; 
Commit complete. 
In the preceding DML, we used the DBMS_ILM.ARCHIVESTATENAME function. This function is
officially used for updating a value of the ORA_ARCHIVE_STATE column. You can also directly
update this column to any non-zero value and the result will be the same. In the preceding
DML, we updated one record of year 2001 and marked it as inactive (or archived).
If you query the table again, you won't see the record that we have updated.
SQL> select year, product, amt, ORA_ARCHIVE_STATE from sales_archive; 
     YEAR PRODUCT           AMT ORA_ARCHIV 
---------- ---------- ---------- ---------- 
     2002 B                 200 0 
     2003 C                 300 0 
     2004 D                 400 0 
     2005 E                 500 0 
     2006 F                 600 0 
     2007 G                 700 0 
6 rows selected. 
This is because, by default Oracle only shows active records. Any records marked inactive
will not be shown. This is done by Oracle by adding a simple filter to your queries. We can
see the same in the following explain plan:
SQL> explain plan for 
 2  select year, product, amt, ORA_ARCHIVE_STATE from sales_archive; 
Explained.
SQL> select * from table(dbms_xplan.display);
PLAN_TABLE_OUTPUT
-------------------------------
Plan hash value: 4043476784
-----------------------------------------------------------
| Id  | Operation         | Name          | Rows  | Bytes |
-----------------------------------------------------------
|   0 | SELECT STATEMENT  |               |     6 | 12210 |
|*  1 |  TABLE ACCESS FULL| SALES_ARCHIVE |     6 | 12210 |
-----------------------------------------------------------
Predicate Information (identified by operation id):
---------------------------------------------------
  1 - filter("SALES_ARCHIVE"."ORA_ARCHIVE_STATE"='0')
Note
-----
  - dynamic statistics used: dynamic sampling (level=2)

17 rows selected.
You can view archived records again by setting row archival visibility parameter to ALL at the
session level. By default, the value of this parameter is ACTIVE; that's why we are seeing only
active records. the following shows the same:
SQL> alter session set row archival visibility = ALL; 
Session altered. 
SQL> select year, product, amt, ORA_ARCHIVE_STATE from sales_archive; 
     YEAR PRODUCT           AMT ORA_ARCHIV 
---------- ---------- ---------- ---------- 
     2001 A                 100 1 
     2002 B                 200 0 
     2003 C                 300 0 
     2004 D                 400 0 
     2005 E                 500 0 
     2006 F                 600 0 
     2007 G                 700 0 
7 rows selected. 
After setting the row archival visibility to ALL, we are able to see archive values. This is one
great advantage of in-database archiving. If some ad hoc report needs to be run, which needs
access to archived records, we can just set the row archival visibility to ALL at the beginning
of the report and we should be able to get the desired result. You can set visibility back to
ACTIVE again:
SQL> alter session set row archival visibility = ACTIVE; 
Session altered. 
You can also restore archived data as active data by running the update statement on archived
data and using the DBMS_ILM.ARCHIVESTATENAME function. But in this case you have to pass 0
as value to this function. Also, you need to set the ROW ARCHIVAL VISIBILITY clause to ALL
before you run update or else archived data won't be visible.
You can disable the row archival for a table by using ALTER TABLE ... NO ROW ARCHIVAL. If
you disable row archival for a table, the ORA_ARCHIVE_STATE column will get dropped
immediately and all the data will be visible in your select statement:
SQL> alter table sales no row archival; 
Table altered. 
Note that if you use CREATE TABLE AS SELECT (CTAS) to create a new table from an
existing table, then the new table will have active as well as inactive data. In-database
archiving is only applicable to selects and DMLs but not to DDLs.

Temporal validity and temporal history
With in-database archiving, we can distinguish active rows from non-active rows using the
row archival column (ORA_ARCHIVE_STATE). You can achieve the same functionality using
temporal validity. Many of the applications today have their data dependent on various dates
that are relevant to the underlying business. For example, an insurance application will have
basic dates recorded for the insurance start date and insurance end date. This represents valid
time when the insurance policy will be active for a customer. The dates attribute which
denotes valid time for a record is called temporal validity. Temporal validity lets you keep
active and inactive data together in the same table, while providing all the benefits of
archiving non-active data. Temporal validity support helps in cases where it's critical to know
when certain data becomes valid and when it's invalid.
Similar to business-relevant date attributes used in a table by application, Oracle provides
temporal validity by creating its own date attributes (start date and end date) to represent valid
time dimensions when the record will be active. The application user or DBA can decide
which records are active versus inactive by updating the start date and the end date attribute of
each record. Records for which end date has passed are inactive records.
By using the valid time temporal implicit filter on a valid-time dimension, queries can show
rows that are currently valid or that will be valid in the future. Queries can hide rows whose
facts are currently not valid.

Temporal history
Before we go further, it's better to understand about temporal history and how temporal
validity is different to temporal history. Temporal history was called the Flashback Data
Archive (FDA) in the previous release. To understand this, let's consider the following.
Temporal validity dates and times are different than the dates and times when the record is
created. The date and time when the record was created in the database is called temporal
history. For example, if we have to create a new customer whose insurance policy starts from
today, it's possible that we might create a record for that application tomorrow or after a week
but put the insurance start date of today. In this case, the temporal validity date is of today but
since the record is created 1 week later, the temporal history date will be 1 week later.
We can use temporal history to get the past data along with temporal validity to get rows
which were valid in the past.

Defining temporal validity
You define a valid-time dimension at table creation time, or by altering a table. In order to
create the valid-time dimension, you specify the PERIOD FOR clause in the table creation
statement.
The following example shows how to explicitly define two date-time columns:
USER_TIME_START and USER_TIME_END:
The following example creates a table with valid-time dimension, explicitly specifying valid-
time dimension columns:
SQL> create table policy 
 2  (cust_id number, 
 3  policy_no number, 
 4  policy_type varchar2(20), 
 5  user_time_start date,    
 6  user_time_end date, 
 7  period for user_time (user_time_start, user_time_end)); 
Table created. 
In the preceding example, we explicitly specified valid-time dimension columns in the create
table statement. But even if we don't specify the column names, the valid-time temporal creates
the desired columns:
SQL> create table policy 
 2  (cust_id number, 
 3  policy_no number, 
 4  policy_type varchar2(20), 
 5  period for user_time); 
Table created. 
A valid-time dimension represented by the new PERIOD FOR clause consists of two date-time
columns. If you don't specify the columns explicitly, as in second example, Oracle
automatically creates two hidden columns whose names starts with a prefix provided in the
PERIOD FOR clause and ends with start and end. So in second example, the two hidden
columns created would be user_time_start and user_time_end (the same as in the first
example).
The only difference between the first example and the second one is the visibility of the valid-
time dimension columns. In the first table, the columns are visible as we explicitly created
them, whereas in second example, the columns are hidden.
If the columns are created hidden, you need to explicitly specify them in select, updates, and
inserts. The following example shows an insert statement into the policy table where we
created implicit valid-time dimension columns:

SQL> insert into policy (cust_id, policy_no, policy_type, user_time_start, 
user_time_end) values (123, 3424, 'HEALTH',sysdate, null); 
1 row created. 
SQL> select * from policy; 
  CUST_ID  POLICY_NO POLICY_TYPE 
---------- ---------- -------------------- 
      123      3424       HEALTH
SQL> select cust_id, policy_no, policy_type, user_time_start from policy;
CUST_ID POLICY_NO  POLICY_TYPE  USER_TIME_START
------- ---------- ------------ -----------------------------
   123   3424       HEALTH      20-JUN-16 12.33.53 AM +05:30
As you can see, using select * won't show hidden valid-time dimension columns. In the
second statement, we explicitly select them and see the values.
Now, we have seen how to create valid-time columns in a table to separate active data from
inactive data. Let's check how to filter these valid-time columns and select only valid data.
There are two methods of selecting active data by filtering on valid-time columns:
PERIOD FOR clause
Using DBMS_FLASHBACK_ARCHIVE procedure
PERIOD FOR clause
To filter valid data, you can use the SELECT statement with the new PERIOD FOR clause. We
have a set of data which is valid based on its start date and end date of a valid-time temporal
and we have another set of data in the same table which is invalid as it falls outside the start
time and end time of the valid-time temporal. Both sets of rows reside in the same table.
However, by controlling the visibility of data to valid rows, you can limit what queries and
DMLs to see only active data.
For each record that you insert into a table, you specify a valid-time start date and end date.
These dates represent the activeness of data. These dates are entered manually by end users.
The date when the record is actually inserted into the table is the transaction time and is called
temporal history in Oracle 12c.
You can use either the AS OF PERIOD FOR clause or the VERSIONS PERIOD FOR clause to
display valid data. The AS OF PERIOD FOR clause is used when we want to see valid data on a
specific date, whereas the VERSIONS PERIOD FOR clause is used when we want to see valid data
between two dates (range).
The following example displays all policies that were active as on 01-JAN-2016:
select * from policy AS OF PERIOD FOR user_time to_date('01-JAN-2016','DD-MON-
YYYY'); 

Here is another example which shows valid data between 01-JAN-2016 and 01-FEB-2016:
select * from policy VERSIONS PERIOD FOR user_time between to_date('01-JAN-
2016','DD-MON-YYYY') and to_date('01-FEB-2016','DD-MON-YYYY'); 
DBMS_FLASHBACK_ARCHIVE
You can also use the DBMS_FLASHBACK_ARCHIVE procedure to show valid data. This procedure
is used to set the visibility of data at the session level. Once visibility is set, the select statement
on the valid-time temporal table shows data that was valid data at the set time.
In the following example, we are setting the visibility of data as of a given time 01-JAN-2016.
This means any select statement or DMLs in the same session will see valid data as of 01-
JAN-2016:
SQL> exec DBMS_FLASHBACK_ARCHIVE.ENABLE_AT_VALID_TIME('ASOF',to_date('01-JAN-
2016','DD-MON-YYYY')); 
PL/SQL procedure successfully completed. 
You can set the visibility to current time. This will show data which is currently valid:
SQL> exec DBMS_FLASHBACK_ARCHIVE.ENABLE_AT_VALID_TIME('CURRENT')); 
PL/SQL procedure successfully completed. 
Note that this visibility setting affects only selects and DMLs. It doesn't affect DDL. DDL sees
complete data (valid as well as invalid). Also, in-database archiving and temporal validity is
NOT supported in the CDB database. They are only supported in the non-CDB database.

Summary
In this chapter, we saw how information ILM works. We looked into the concept of heat maps
and how a heat map is important for the working of ADO policies. We saw details about
creating different types of ADO policies and how we can create custom ILM rules for
implementing ADO policies. We then saw the evaluation and execution of ADO policies. We
checked out different views in a database to see the details of the policies. We also played
around to enable and disable one or all of the ADO policies.
We saw new features related to online data file moves and how we can move the data file
without impacting any of the ongoing activity in a database. We then saw two new feature for
data archiving: in-database archiving and temporal validity. We saw the details of these
features and how they can be used to implement row level archiving of data. We concluded the
chapter with temporal history, which is an advancement of previous flashback technology.
In the next chapter, we are going to talk about new features of auditing and how we can create
audit policies to audit different entities in a database. We are also going to look into privilege
management and different new privileges that are introduced in Oracle 12c. We will also
check new privilege analysis tools which will help us in removing unnecessary privileges.
Lastly, we will learn about a new data redaction feature which helps us in protecting our data
from end users who are not authorized to view sensitive data.

Chapter 5. Auditing, Privilege Analysis and
Data Redaction
Security is one of the most important parts of the Oracle database, and as an administrator we
have the responsibility to make sure that our database is secure from outside as well as inside
attacks. It's often found that a database is more vulnerable to internal attacks rather than from
the outside. One of the main reasons for this is incorrect privileges or high level privileges
being given to the lower level users and misuse of such privileges often does more damage.
Oracle 12c came up with new features of auditing the data to provide a unified overview of
who is accessing what in the database. This also includes access via tools such as
the expdp/impdp (export/import), utility, and so on. The following are the broad topics that we
are going to cover in this chapter:
Auditing
Privileges
Oracle Data Redaction

Unified auditing
Auditing gives you the ability to track and record activities in the database. Auditing provides
you with information about "who is changing what" in the database. This is essential to keep
accountability in the database and helps in analyzing suspicious activities. In today's Internet
world, auditing has become mandatory for businesses to protect the personal data of
customers, employees, and stakeholders. Auditing is necessary to safeguard corporate data
from both external hackers and internal users who are privileged.
Oracle offers multiple types of auditing in order to fulfill different scenarios. Auditing
required users and actions is essential in order to generate minimal audit data and to minimize
the impact of auditing on the performance of the database. The focus should be to narrow
auditing for only the necessary actions and users as required by the business.
Before we talk about new auditing features in Oracle 12c, I would like to brief you about
auditing options in the previous release of Oracle.
In the Oracle database 11g, Oracle offered three types of auditing:
Standard auditing
Audit privileged user activity
Fine-grained auditing
The initialization parameter, audit_trail, defines where audit information is written based
on its value DB/DB,EXTENDED/OS/XML/XML,EXTENDED. By default it is DB. These values mean
the following:
DB: Database audit information is written to the database audit trail, the sys.aud$ table
DB, EXTENDED: Apart from the preceding point, it also audits SQL executed along
with bind variables
XML: Database audit information is written in XML format
XML, EXTENDED: Same as DB, EXTENDED
OS: The audit information is written to audit files in location defined by
audit_file_dest
In the previous releases of databases, though there were strong auditing features available,
there was an absence of consolidating auditing information. The three different levels of
auditing existed, as mentioned above, and they all were used to generate audit information at
different places and in different formats. This made it difficult to efficiently consolidate and
archive the auditing data for future reference.
In Oracle 12c, a new enhanced auditing called unified auditing makes it much simpler to
configure condition-based auditing and it consolidates multiple audit trails into single unified
audit trail tables. New unified auditing captures the precise auditing information, which
reduces the amount of auditing information to be logged. It also reduces the overhead

associated with auditing by having the audit information in memory and writing it
periodically to the audit disk. All the audit records are formatted in the same way with the
same fields, making each record consistent for analysis. Also, all the auditing records are
saved in one place in a single table and that makes running the analysis on audit records very
easy. You don't have to collect the audit records from different places manually to run the
analysis.
Audit records are collected and stored from the following sources:
Standard audit records as part of audit policies are created inside a database. This also
includes audit records created from the sys user account.
Fine grained auditing records are created by the DBMS_FGA PL/SQL package.
Oracle Real Application Security audits records.
Audit records are created by the Oracle RMAN tool.
The Oracle database, Vault, audits records.
Oracle label security audit records.
Audit records from Oracle data mining.
Export and import utility (expdp/impdp).
SQL*Loader audit records.
Audit records for all the preceding sources are stored in a single place in a table called
UNIFIED_AUDIT_TRAIL in the AUDSYS schema in the SYSAUX tablespace. All the records in
this table have a uniform format for our analysis.

Advantages of unified auditing
The following are some of the advantages of unified auditing:
Consolidation: Audit records generated from various sources are all stored in a single
UNIFIED_AUDIT_TRAIL table in the AUDSYS schema in the SYSAUX tablespace. So DBA
does not have to look at different tables or locations to get all the audit records. Also, the
format of the record is uniform and that makes it really easy for DBAs to perform
analysis.
Simplicity: Audit options can be grouped into simple audit policies in which you can
audit the supported components and sys user. You can use conditions and have exclusions
in your policy.
Security: The audit trail table is read-only. All audit configurations are audited. sys user
actions are audited. Unified audit trails improve the security without having to protect
multiple audit trails. They have separated audit admin duties from audit admin roles via
AUDIT_ADMIN and AUDIT_VIEWER. Users with the AUDIT_ADMIN role can perform auditing,
and the user with the AUDIT_VIEWER role can view the audit data.
Performance: In the default auditing mode is the Queued Write mode; the audit
information is batched, queued, and persisted in the System Global Area (SGA) in a
periodic way. Because the audit information is written to the SGA, the performance
improvement is very significant.

Mixed auditing mode and unified auditing mode
When you create a new database, by default you will get, mixed auditing mode. With the mixed
auditing mode, you have all your old auditing functionality working just as it used to work in
older releases, but you can also create unified audit policies as introduced in Oracle 12c.
Oracle will honor both auditing types and it will create audit records for both types of
auditing when you are running in the mixed auditing mode.
You can change the database auditing to pure unified auditing, but by doing so, Oracle will
discard all your previous auditing settings and only new unified auditing policies will be in
effect. So you have to make sure you migrate all your old audit settings into the
corresponding new unified auditing policies before you convert the DB into pure unified
auditing.
The mixed mode is intended to introduce unified auditing, so that you can have a feel of how
it works while your existing audit policies continues to behave and work the same way they
used to work in previous releases. It helps you to ease into the unified auditing mode. Once
you have decided to use pure unified auditing, you need to relink the Oracle binary with
the uniaud_on option, as shown in the next section.
In the previous release, audit functionality was mainly driven by the AUDIT_TRAIL
initialization parameter, as we have just discussed and based on the setting of this parameter,
audit records used to get generated in the desired format and location. With mixed auditing,
this parameter continue to exists and if valid will generate audit records similar to the
previous release.
When you upgrade your database from the previous releases to Oracle 12c, you will still see
that the traditional auditing works and it continues to write the auditing records in
the traditional audit trail. Once you migrate to unified auditing, traditional auditing will stop
and only the unified auditing policy will work. However, you can still access the records of
traditional audit trails as they are not deleted. Also, when you move over to unified auditing,
all the traditional audit related parameters that were set in previous releases will become
obsolete. Unified auditing is initially enabled in the mixed auditing mode and you can use it
along with traditional auditing.
Unified auditing works in the form of audit policies. For any action or entity that you want to
track, you need to create and enable an audit policy. Once the audit policy is enabled, audit
information will be generated and written in memory in SGA queues and are persisted
periodically on disk in the AUDSYS table under the SYSAUX schema. The GEN0 background
process pushes the content from memory to the tables on the disk periodically. Each client
process has two SGA queues; while one queue is being written to the table, the other queue is
used by the client for writing. Audit trail records are stored in the SYS.UNIFIED_AUDIT_TRAIL
dictionary view, which is a view on the read-only audit table owned by the AUDSYS schema.
Even though there is a clear performance benefit in writing audit information to SGA queues

in memory and then periodically dumping the information to disk, you must understand that
this can cause loss of audit data if the database crashes, which wipes out all information from
memory. Usually for auditing purpose, performance is more of a matter of concern than data
loss, and that's why unified auditing runs in queued-write mode by default. This means
auditing writes information in SGA queues, which is later dumped into the disk. The queue
size is by default 1 MB and it can be increased to 30 MB by setting the
UNIFIED_AUDIT_SGA_QUEUE_SIZE parameter in the database.
We can, however, change the auditing mode of unified auditing to write to the disk
immediately for every audit record that is being generated. This will take some resources and
performance will not be as good as when using queued-write, but you will not lose any audit
data. You can change the mode from queued-write to immediate-write by changing the
property of unified auditing using the DBMS_AUDIT_MGMT package, as shown in the following
code:
SQL> exec dbms_audit_mgmt.set_audit_trail_property( - 
> dbms_audit_mgmt.audit_trail_unified, - 
> dbms_audit_mgmt.audit_trail_write_mode, - 
> dbms_audit_mgmt.audit_trail_immediate_write); 
PL/SQL procedure successfully completed. 
Note
You can again change back to queue-write mode by setting AUDIT_TRAIL_WRITE_MODE to
AUDIT_TRAIL_QUEUED_WRITE.
The SYS.DBMS_AUDIT_MGMT.FLUSH_UNIFIED_AUDIT_TRAIL procedure is available if we want to
manually flush the audit information from the memory disk in the AUDSYS table:
SQL> exec sys.dbms_audit_mgmt.flush_unified_audit_trail;
PL/SQL procedure successfully completed. 

Enabling unified auditing mode
Once you are comfortable with unified auditing and you know how to create unified auditing
policies, you can move from the default mixed configuration (traditional auditing + unified
auditing) mode to the pure unified auditing mode.
For enabling database to use pure unified auditing instead of mixed auditing, you must shut
down the database and relink the Oracle executable with uniaud_on. The audit trail table
resides in the SYSAUX tablespace. If you do not want the audit information to be written to
SYSAUX, create a new tablespace and make it the default tablespace for the AUDSYS schema.
Once unified auditing is enabled, none of the old audit parameters can be used. To perform
the unified auditing, the user must have the roles AUDIT_ADMIN and AUDIT_VIEWER role
privileges.
Make sure you shut down the DB and all the processes from the ORACLE_HOME in which you are
enabling auditing, and follow these steps to enable unified auditing:
bash-3.2$ cd $ORACLE_HOME/rdbms/lib 
bash-3.2$ make -f ins_rdbms.mk uniaud_on ioracle ORACLE_HOME=$ORACLE_HOME 
bash-3.2$ sqlplus '/ as sysdba' 
 
SQL*Plus: Release 12.1.0.2.0 Production on Thu Aug 4 07:41:40 2016 
 
Copyright (c) 1982, 2014, Oracle.  All rights reserved. 
 
Connected to an idle instance. 
SQL> startup 
ORACLE instance started. 
 
Total System Global Area 4882169856 bytes 
Fixed Size                  2934312 bytes 
Variable Size            1023412696 bytes 
Database Buffers         3841982464 bytes 
Redo Buffers               13840384 bytes 
Database mounted. 
Database opened. 
 
SQL> SELECT VALUE FROM V$OPTION WHERE PARAMETER = 'Unified Auditing'; 
 
PARAMETER         VALUE 
----------------  ---------- 
Unified Auditing  TRUE 
Under the unified auditing framework, as part of security, the sys user activity is
automatically audited. When the sys user is connected as SYSDBA, SYSOPER, SYSASM, SYSDG, and
SYSBACKUP, the top level activity such as startup, shutdown, alter database, and alter system are
audited. Any system wide audit policies that are enabled for the sys user are audited.
The SYS audit records when the database is not in an open state will be written to

$ORACLE_BASE/audit/<dbname>. After the database is open, the audit records can be written to
the database table using the DBMS_AUDIT_MGMT.LOAD_UNIFIED_AUDIT_FILES procedure. Once
the records are loaded, the files at OS level will be deleted by the procedure.
The audit trail table is in the AUDSYS schema and it is a read-only table, this is to secure the
audit data. The table becomes updatable by SYSDBA during recovery and upgrade operations.
Any activities related to the policy such as create, enable, and delete policy are audited by
Oracle by default. Users with the AUDIT_ADMIN role can configure and manage the audit trail.
Users with AUDIT_VIEWER can view the audit data for analysis purposes.

Extended audit information
In previous releases, every type of auditing was written to its own audit information system.
For example, general auditing might be written to the DBA_AUDIT_TRAIL table and fine grain
auditing might be written to its own location. Auditors and DBA's had a hard time keeping
track of all the audit information. With unified auditing in Oracle 12c, all the audit
information is consolidated in a single place - the AUDSYS table or the
SYS.UNIFIED_AUDIT_TRAIL view on top of this table.
SYS.UNIFIED_AUDIT_TRAIL captures all the basic auditing, which includes audit information
related to database operations, SQL statements, sys user auditing, and so on. This is called
basic auditing information (BAI). In addition to basic information, unified auditing also
captures component-based auditing information.
For example, if we want to use the audit datapump operation or RMAN operation, we can
capture this audit information for these components and that information will again be stored
in the same SYS.UNIFIED_AUDIT_TRAIL view. This additional information that you can track
with component specific columns and store in the SYS.UNIFIED_AUDIT_TRAIL view is called
extended audit information (EAI).
Until previous releases, it was not possible to store the audit information of these components
into the same audit table (AUD$). But SYS.UNIFIED_AUDIT_TRAIL has an extensible audit
framework and it contains columns for each of these extended audits for components and
actions. The following are the additional columns available in SYS.UNIFIED_AUDIT_TRAIL to
store audit information for various components:
FGA events: These are the auditing events for fine grained auditing. Fine grained
auditing policy names get stored in the FGA_POLICY_NAME column of
SYS.UNIFIED_AUDIT_TRAIL table. The FGA_POLICY_NAME column can be joined with
columns in fine grained auditing tables. But having a reference in the main audit table
helps in consolidation.
Datapump export/import: The specific data pump information is stored in two new
columns:
DP_TEXT_PARAMETERS1
DP_BOOLEAN_PARAMETERS1
RMAN backup/recovery: RMAN specific audit information is stored in several columns
specific to RMAN, such as:
RMAN_OPERATION
RMAN_OBJECT_TYPE
RMAN_DEVICE_TYPE
Label security: When a label security policy is applied on a table, the extended audit
information is stored in a column name OLS_POLICY_NAME and other columns which
starts with OLS_.
Database vault: Audit information related to database vaults is stored in the DV_XXX
columns.

Real application security: Specific RAS information is stored in the XS_XXX columns.

Enabling extended auditing
Extended auditing is enabled by default for RMAN activities. All the RMAN related activities
are audited by default and information gets stored in the SYS.UNIFIED_AUDIT_TRAIL view. We
don't have to explicitly enable auditing for RMAN. However, we can enable unified auditing
for other components such as data pump export/import. We need to create and enable an audit
policy for data pump export and import to capture auditing information for export/import
tasks.
For example, here is step-by-step process of how you can create and enable the audit policy
for Oracle data pump (export/import/both):
1. Create the audit policy to capture the audit records for export or import or both:
  SQL> create audit policy dp_pol_both actions component=datapump ALL; 
  Audit policy created. 
2. Enable the audit policy created previously:
  SQL> audit policy db_pol_both; 
  Audit succeeded. 
3. Execute a data pump operation:
  $ expdp hr/hr dumpfile=HR_tables tables=emp,dept 
  directory=<data_pump_dir> 
4. Push the audit information to the disk:
  SQL> sys.dbms_audit_mgmt.flush_unified)audit_trail _trail
 
  PL/SQL procedure successfully completed. 
After the preceding steps are performed, then you can view the audit data related to the
data pump in the unified audit trail, that is SYS.UNIFIED_AUDIT_TRAIL.

Creating and enabling unified audit policy
Audit policy is a named set of activities that will let you audit a user's activity or database or a
component by using system, object privileges, and roles. The policies are managed using the
statements create audit policy, alter audit policy, and drop audit policy. This
requires the audit_admin role.
Once the policy has been created it has to be activated for it to take effect. The policy can be
applied in a multitenant database either in the root or in a pluggable database.
In a database where unified auditing is enabled, the following actions are audited by default
and are mandatory to audit. They are listed as follows:
CREATE AUDIT POLICY: This action is used for creating a unified audit policy. This is
audited by default.
ALTER AUDIT POLICY: This action is for altering an existing unified policy.
DROP AUDIT POLICY: This action is for dropping an existing unified auditing policy.
EXECUTE DBMS_FGA: This package is used for creating and managing fine grained
auditing. Any execute on this package is audited by default.
EXECUTE DBMS_AUDIT_MGMT: This is a new package for managing unified audit records.
Any execute on this package is audited by default.

Audit policies in a multitenant database
Audit policies can be applied to an individual PDB or CDB based on the policy type. Root and
every PDB has its own unified audit trail. You can create policies that can be applied to both
root and individual PDBs, and policies that can be applied to individual PDBs.
Common audit policy
Common policies are available to all PDBs. Only common users with the AUDIT_ADMIN role
can create and maintain common audit policies. You must create the common audit policies in
the root. You can enable them only for common users. This policy can contain object audit
options of the common objects and can be enabled for common users only.
Local audit policy
Local policies can be created in root or the PDB. If the policy is created in root, it can contain
the object audit options for both local and common objects. The local and common users
having the AUDIT_ADMIN role can enable local policies for local users from their PDBs and
common users from the root or the PDB to which they have privileges.

Creating audit policies
Here is the syntax with which you can create an audit policy:
CREATE AUDIT POLICY policy_name 
   { {privilege_audit_clause [action_audit_clause ] [role_audit_clause ] } 
       | { action_audit_clause  [role_audit_clause ] }  
       | { role_audit_clause } 
    }         
   [WHEN audit_condition EVALUATE PER {STATEMENT|SESSION|INSTANCE}]  
   [CONTAINER = {CURRENT | ALL}]; 
Here is the meaning of the preceding clauses:
privilege_audit_clause: Describes privilege-related audit options. This clause can be
used for auditing specific privileges. For example, the CREATE ANY TABLE privilege.
action_audit_clause: This clause can be used to audit any action:
The action could be a standard action such as EXECUTE on a procedure, or DELETE
from a table, and so on.
The action could be component-based actions, such as export/import using data
pump.
role_audit_clause: This clause is used to audit the use of privilege granted directly to
the specified role.
audit_condition: Audit policies can evaluate the condition once per statement or once
per session or once per instance. You must include the EVALUATE PER clause with the
WHEN condition.
You can also use the CONTAINER clause (as used in a multitenant environment) while creating
an audit policy to create the audit policy as either local (specific to a PDB or a container) or
common (applicable to all containers). You should define an auditing policy as either system-
wide audit options or object-specific audit options. System-wide auditing includes auditing
for all the objects across the entire system. The object-specific audit option is specific of an
object (for example, a table or a procedure).
You can check all auditable system-wide options in the SYS.AUDIABLE_SYSTEM_ACTIONS table.
Similarly, you can check all auditable object-wide options in the
SYS.AUDITABLE_OBJECT_ACTIONS table.
In the following section, we are going to look into the examples of creating audit policies.
Creating a system-wide audit policy
We can audit privileges, actions, or roles using an audit policy. With a system-wide audit
policy, we can audit system-wide privileges such as CREATE ANY TABLE or system-wide
actions such as ALTER TABLE (not a specific table, this applies to all tables across the system),
or audit use of privileges directly assigned to a role.

You can include all the three audit options: privilege, action, and role in a policy. The
SYS.AUDITABLE_SYSTEM_ACTIONS table has all the system-wide auditable options.
Here are the audit policy examples using privileges, actions, and roles clauses--each one
separately and all the clauses together:
SQL> create audit policy audit_syspriv_pol 
 2  privileges create any table; 
Audit policy created. 
The preceding audit policy will audit all users who use the CREATE ANY TABLE system
privilege.
SQL> create audit policy audit_act_pol 
 2  actions alter any table; 
Audit policy created. 
The preceding audit policy will audit all users who use the ALTER ANY TABLE system
privilege.
SQL> create audit policy audit_rol_pol 
 2  roles emp_role; 
Audit policy created. 
The preceding audit policy will audit all users who use the EMP_ROLE role.
SQL> create audit policy audit_mix_pol1 
 2  privileges create any table  
 3  actions alter any table 
 4 roles emp_role; 
Audit policy created. 
The preceding audit policy will audit all users who use the CREATE ANY TABLE system
privilege or ALTER ANY TABLE system privilege or EMP_ROLE role:
SQL> create audit policy audit_mix_pol2 
 2  privileges create any table  
 3  actions alter any table 
 4  roles emp_role 
 5  container=all; 
Audit policy created.
Above audit policy is a common audit policy which will audit all users and uses the CREATE
ANY TABLE system privilege or ALTER ANY TABLE system privilege or EMP_ROLE role on all
containers.
Creating an object-specific audit policy

This audit policy is created using the object-specific action. This is to audit a specified action
on the specified object. The object level audit options take effect dynamically. The changes
will be applied to current and future user sessions. The SYS.AUDITABLE_OBJECT_ACTIONS table
has all the object-specific auditable options:
SQL>  create audit policy audit_updt_pol 
 2  actions update on hr.emp; 
Audit policy created. 
Above audit policy audits all updates on HR.EMP table. 
SQL> create audit policy audit_exec_pol 
 2  actions execute on hr.adj_sal_proc; 
                      
Audit policy created. 
SQL> 
The preceding audit policy audits all sessions doing execute on HR.ADJ_SAL_PROC procedure.
Using conditions and evaluation
Audit policies have the capability to evaluate the statements once per statement, once per
session, and once per instance.
The following example creates an audit policy based on the condition user HR, which is
evaluated once per session on actions as alter on t1 table and update on t2 table of the OE
schema:
SQL> create audit policy audit_cond_pol 
 2  actions alter on oe.t1, update on oe.t2 
 3  when 'sys_context ('userenv'',''session_user'')=' 'HR''' 
 4  evaluate per session; 
Audit policy created. 
The following example creates an audit policy based on the condition user OE, which is
evaluated once per statement, and the actions being performed are insert on T1 table and
update on T2 table owned by the OE schema:
SQL> create audit policy test_policy_t1_t2 
 2  actions insert on oe.t1, update on oe.t2  
 3  when 'SYS_CONTEXT(''USERENV'', ''SESSION_USER'') = ''OE'''  
 4  evaluate per statement; 
Audit policy created. 
You can also configure a policy that can be evaluated once per instance:
SQL>  create audit policy aud_policy_t1 
 2  actions delete on oe.t1 

 3  when 'SYS_CONTEXT(''USERENV'', ''INSATNCE_NAME'') = ''deo''' 
 4  evaluate per instance; 
Audit policy created. 
All the audit policies configured in a database are available in the AUDIT_UNIFIED_POLICIES
table (note, this table is different from the main table, which has auditing data
—UNIFIED_AUDIT_TRAIL).

Modifying audit policies
Once we create an audit policy we can modify the audit policy to change, add, or remove
privileges to the auditing or alter any other component of the policy. We can also enable,
disable, or drop the policy as required. Let's look into each of these.
Enabling audit policies
After creating the audit policy, it has to be enabled by using the audit keyword. All users are
audited by default unless the user is specified. The audit policy can be applied to one or more
users using the by clause, and excluding some users by using the except clause. The audit
administrator audits the sys user related actions. The by and except clauses cannot be used
together.
By default, audit records are generated regardless of whether the actions succeeded or failed.
If you use the whenever not successful clause, then only failed actions will be audited. If
you use whenever successful, then only successful actions will be audited. If the WHENEVER
clause is skipped, then both successful and failed actions are audited. The return_code
column determines whether it is successful or not.
All unified audit policies are listed in audit_unified_policies and enabled policies are
listed in audit_unified_enabled_policies:
SQL> audit policy audit_syspriv_pol; 
Audit succeeded. 
SQL> audit policy audit_act_pol by sys;    
Audit succeeded. 
SQL> audit policy audit_rol_pol except scott; 
Audit succeeded. 
SQL> audit policy audit_mix_pol1 whenever successful; 
Audit succeeded. 
SQL> audit policy audit_mix_pol2 whenever not successful; 
Audit succeeded. 
SQL> 
Altering audit policies
Any property of an audit policy can be modified except the CONTAINER. This implies that you
cannot convert a common policy to a local policy. Altering an audit policy will preserve its
enabled status; hence it does not need to be re-enabled.

Here's the syntax of the ALTER AUDIT POLICY statement. Each clause here is self-explanatory
as we have seen the description of every clause in detail in the preceding CREATE AUDIT
POLICY syntax:
ALTER AUDIT POLICY  policy_name 
[ADD [privilege_audit_clause][action_audit_clause] 
 [role_audit_clause]] 
[DROP [privilege_audit_clause][action_audit_clause] 
   [role_audit_clause]]  
[CONDITION {DROP | audit_condition EVALUATE PER {STATEMENT|SESSION|INSTANCE}}]  
The following examples will show how to alter an audit policy:
SQL> alter audit policy audit_syspriv_pol 
 2  add privileges drop any table; 
Audit policy altered. 
SQL> alter audit policy orders_mixcond_pol1 
 2  condition drop; 
Audit policy altered. 
Disabling audit policies
Use the NOAUDIT command to disable the audit policy. The syntax is covered in the following
code:
NOAUDIT POLICY {policy_auditing | existing_audit_options}; 
For example:
SQL> noaudit policy audit_syspriv_pol; 
Noaudit succeeded. 
SQL> noaudit policy audit_act_pol by sys; 
Noaudit succeeded. 
Dropping audit policies
To drop a unified audit policy that was enabled earlier, it should be disabled first, and then run
the DROP AUDIT POLICY command to drop it. The syntax is as follows:
DROP AUDIT POLICY policy_name; 
For example:
SQL> drop audit policy audit_syspriv_pol; 
Audit Policy dropped. 
SQL> drop audit policy audit_actions_pol; 

Audit Policy dropped. 
Viewing audit policies
The AUDIT_UNIFIED_POLICIES view has the list of currently configured audit policies. The
condition_eval_opt column shows when the condition is evaluated.
The audit_unified_enabled_policies view has the list of enabled audit policies. The
ENABLED_OPT column has the by value for the list of users enabled and the EXCEPT value for
the exempted users. The audited or excluded users are listed in the USER_NAME column.
If the audit policy generates audit information or not, it is determined by the success and
failure columns based on the values YES or NO:
SQL> select policy_name, audit_option, condition_eval_opt 
 2  from audit_unified_policies; 
POLICY_NAME          AUDIT_OPTION         CONDITION 
-------------------- -------------------- --------- 
AUDIT_MIX_POL        ALTER TABLE          NONE 
AUDIT_MIX_POL02      ALTER TABLE          NONE 
AUDIT_OBJPRIV_POL    UPDATE               NONE 
.. 
.. 
.. 
.. 
1869 rows selected. 
SQL> select policy_name, enabled_opt, user_name, success, failure 
 1  from audit_unified_enabled_policies 
POLICY_NAME          ENABLED_ USER_NAME            SUC FAI 
-------------------- -------- -------------------- --- --- 
ORA_SECURECONFIG     BY       ALL USERS            YES YES 
ORA_LOGON_FAILURES   BY       ALL USERS            NO  YES 
AUDIT_MIX_POL        BY       ALL USERS            YES NO 
AUDIT_MIX_POL02      BY       ALL USERS            NO  YES 

Using predefined audit policies
The Oracle Database 12c has three predefined audit policies. Oracle recommends using these
policies under audit best practices. These three policies are created by the
$ORACLE_HOME/rdbms/admin/secconf.sql script and ORA_SECURECONFIG is alone enabled. The
following are the three policies that will be enabled by the secconf.sql script:
The audit policy ORA_ACCOUNT_MGMT audits user account and privilege management events
The audit policy ORA_DATABASE_PARAMETER audits database parameters setting changes
The audit policy ORA_SECURECONFIG audits all the Oracle Database 11g default audit
configurations
Once you create the predefined policy under the secconf.sql script, we can check the policy
and audit options that it creates using the following query:
SQL> select policy_name, audit_option 
 2  from audit_unified_policies 
 3  where policy_name in ('ORA_ACCOUNT_MGMT', 'ORA_DATABASE_PARAMETER', 
'ORA_SECURECONFIG'); 
POLICY_NAME                    AUDIT_OPTION 
------------------------------ ---------------------------------------- 
ORA_ACCOUNT_MGMT               ALTER USER 
ORA_ACCOUNT_MGMT               CREATE USER 
.. 
.. 
.. 
ORA_DATABASE_PARAMETER         ALTER DATABASE 
ORA_DATABASE_PARAMETER         ALTER SYSTEM 
.. 
.. 
.. 
ORA_SECURECONFIG               LOGMINING 
ORA_SECURECONFIG               TRANSLATE ANY SQL 
.. 
.. 
.. 
58 rows selected. 

Capturing application context
Applications set various contexts before executing the actions in the database. They are
basically the key-value pairs stored for every session and provide more details about the
session environment. Application context has a label called namespace to which all the
environment related key-value pairs are associated. The default namespace for Oracle
sessions is USERENV but we can set any different namespace. We can also have multiple
namespaces for different applications and key-values specific to that application can reside in
that namespace. These key-values include client_identifier, module name, and so on, and
they can be viewed using namespace (for example, USERENV) variables, which are stored as
name value pairs in a namespace. Unified auditing can be used to capture application contexts
using the AUDIT CONTEXT NAMESPACE ATTRIBUTES command. You can list all the contexts in
the database using the DBA_CONTEXT view.
For example, the following command can be used to audit a few attributes in MY_CONTEXT:
AUDIT CONTEXT NAMESPACE MY_CONTEXT ATTRIBUTES MY_SESSION, MY_LOCATION by SCOTT; 
You can view all the application contexts that are getting audited using the following SQL:
select application_contexts from unified_audit_trail where application_contexts 
is not null; 

Cleaning up audit data
You should periodically archive and then delete (purge) audit trail records. You can use a
variety of ways to purge audit trail records. You can purge in automated by scheduling a
purge job and by running a package manually.
You can use the DBMS_AUDIT_MGMT PL/SQL package to create and schedule the purge job:
SQL> BEGIN 
 DBMS_AUDIT_MGMT.CREATE_PURGE_JOB ( 
  AUDIT_TRAIL_TYPE            => DBMS_AUDIT_MGMT.AUDIT_TRAIL_UNIFIED,  
  AUDIT_TRAIL_PURGE_INTERVAL  => 12, 
  AUDIT_TRAIL_PURGE_NAME      => 'Audit_Trail_PJ', 
  USE_LAST_ARCH_TIMESTAMP     => TRUE, 
  CONTAINER                   => DBMS_AUDIT_MGMT.CONTAINER_CURRENT); 
END; 
/  
PL/SQL procedure successfully completed. 
In the preceding procedure:
AUDIT_TRAIL_TYPE defines the audit type for which purge job is being created.
AUDIT_TRAIL_PURGE_INTERVAL defines an interval in hours at which the cleanup
procedure is called.
AUDIT_TRAIL_PURGE_NAME is the unique name of the purge job.
USE_LAST_ARCH_TIMESTAMP specifies whether last archived timestamp should be used for
deciding the records that should be deleted. A value of TRUE means only the audit
records created before the last archived timestamp will be deleted.
CONTAINER defines whether this job is applicable to all the containers or the current
container.
Purge the audit trail records by running the DBMS_AUDIT_MGMT.CLEAN_AUDIT_TRAIL PL/SQL
procedure manually:
SQL> BEGIN 
 DBMS_AUDIT_MGMT.CLEAN_AUDIT_TRAIL( 
  AUDIT_TRAIL_TYPE           =>  DBMS_AUDIT_MGMT.AUDIT_TRAIL_UNIFIED, 
  USE_LAST_ARCH_TIMESTAMP    =>  TRUE, 
  CONTAINER                  =>  DBMS_AUDIT_MGMT.CONTAINER_CURRENT ); 
END; 
/  
PL/SQL procedure successfully completed. 

Using administrative privileges
Until now, users were using SYSDBA privileges for almost all the administrative tasks.
SYSASM privileges were introduced in Oracle 11g and were used for performing ASM
related tasks. But other than that, any DBA who wants to perform any administrative task (no
matter what it is or how small it is), will have to be given SYSDBA privileges. SYSDBA is a
very broad privilege containing lots of administrative permissions. Giving such a broad
privilege to every other DBA is risky. Rather, privileges should be given based on the tasks
and duties that DBA is supposed to perform.
Security compliance dictates the need for the least privilege principle. In the least privilege
principle, a user should be granted only the required privileges and nothing beyond.
To achieve separation of duties and the least privileges principle, Oracle 12c has introduced
new tasks and specific administrative system privileges for performing Recovery Manager
(RMAN), transparent data encryption, and Oracle data guard related tasks.
In older releases, you were given the same SYSDBA privileges for performing each of the
previously mentioned tasks even though you had separate DBAs and users to perform each of
these tasks. The SYSDBA privilege continues to exist in Oracle 12c and it performs the same
operations as before, but it's really only there for backward-compatibility purposes. Oracle
12c introduces new administrate privileges to support specific tasks. These are the new
SYSBACKUP, SYSDG, and SYSKM privileges.
These new privileges are actually predefined Oracle users containing the respective
privileges. When you connect with any of these privileges, you're actually connected as the
corresponding predefined Oracle user whose name is the same as the privilege name. This is
applicable only for OS authenticated users. If you assign this privilege to a regular user of a
database, and when you connect as that user, the connected user will be shown as that user and
not SYSDBA.
The following is an example of this:
SQL> connect /as sysbackup 
Connected. 
SQL> show user 
USER is "SYSBACKUP" 
SQL> 
Let's check out each of these new administrative privileges:
SYSBACKUP privilege: The SYSBACKUP administrative privilege allows you to
perform Oracle RMAN backup and recovery operations from Oracle RMAN or through
SQL, STARTUP, SHUTDOWN, and other operations. The following operations are supported
by SYSBACKUP privileges:
Create and drop a database

Create a control file
Change the archive log mode
Flash back the database
Create SPFILE/PFILE manage restore points
Start up and shut down the database
We have previously seen how to connect as SYSBACKUP.
SYSDG privilege: This privilege is for performing dataguard administrative tasks using
dataguard broker or using the DGMGRL command-line tool. When you connect to a
database using OS authentication as SYSDG, you will be actually connected to the SYSDG
user, as shown in the following code:
SQL> connect /as sysdg 
Connected. 
SQL> show user 
USER is "SYSDG" 
The following are the privileges available under the SYSDG administrative privilege:
Start up and shut down the database
Manage restore points
Flash back the database
Run DGMGRL
Start the observer
SYSKM privilege: The SYSKM administrative privilege is for managing transparent
data encryption (TDE) key store operations. This privilege is tied to the Oracle user
SYSKM. When you connect to the database using OS authentication as SYSKM, you actually
connect to the SYSKM user:
       SQL> connect /as syskm
       Connected.
       SQL> show user
       USER is "SYSKM"
The following are the privileges associated with SYSKM:
Create, open, and close key stores
Create and change the master key for encryption
Manage encryption keys for column and tablespace encryption

New operating system group
With new database privileges, Oracle 12c has also introduced the ability to assign different
operating system groups for different administrative privileges. Until now, we used to have
different Operating System and Database Administration (OSDBA) and Operating System
and Operations (OSOPER) Oracle groups, and we used the map operating system group for
these Oracle groups so that OS authentication can work. Oracle 12c has introduced three
more groups and we can map OS group to these groups. The following are the three groups:
OSBACKUPDBA
OSDGDBA
OSKMDBA
You can create separate OS groups and assign OS users to these OS groups. Later when you
create the database, you will see the following screenshot where you can map OS groups with
Oracle groups. So any users who belong to an OS group which is mapped to say OSBACKUPDBA
can use OS authentication to connect as SYSBACKUP automatically.

In the preceding screenshot, I used the same OS group (dba) for all the Oracle groups, but we
can actually separate the responsibilities by creating separate groups and assigning those
groups to Oracle groups.
Membership in a UNIX or Windows group affects your connection to the database as follows:
If you are a member of the OSBACKUP group, and you specify AS SYSBACKUP when you
connect to the database, you connect to the database with the SYSBACKUP administrative
privilege under the SYSBACKUP user
If you are a member of the OSDG group, and you specify AS SYSDG when you connect to
the database, you connect to the database with the SYSDG administrative privilege under
the SYSDG user
If you are a member of the OSKM group, and you specify AS SYSKM when you connect to
the database, you connect to the database with the SYSKM administrative privilege under
the SYSKM user
Since we specified the dba OS group for all the oracle groups and we have the oracle OS
user, which belongs to the OS group dba, we can use any of the administrative privileges to
connect using OS authentication whenever we have logged into the server using the oracle OS
user.
Without being a member of these OS groups, users will not be able to connect as
administrative users using the OS authentication. That is, CONNECT / AS SYSDBA will fail.
However, the users can still connect using other authentication mechanisms such as remote
connection or local connection using password or directory-based authentication.
You can still change the OS groups mapped to the Oracle group even after installation of
oracle software. To do that, you need to provide the mapping of the OS group with the Oracle
group in the $ORACLE_HOME/rdbms/lib/config.[cs] file and relink the oracle executable.

New password file
With new administrative privileges, Oracle 12c has also introduced new passwords. In
previous releases, password files used to contain users with SYSDBA or SYSASM privileges
only. But now we have three more additional privileges. Password files in Oracle 12c can
contain users with any of the five administrative privileges (SYSDBA, SYSASM, SYSDG, SYSBACKUP,
and SYSKM).
The password authentication still requires the REMOTE_LOGIN_PASSWORDFILE instance
parameter set to EXCLUSIVE and a password file. You can use the ORAPWD utility to create the
password file:
[oracle@advait ~]$ orapwd -h
Usage: orapwd file=<fname> entries=<users> force=<y/n> asm=<y/n>       
dbuniquename=<dbname> format=<legacy/12> sysbackup=<y/n> sysdg=<y/n>       
syskm=<y/n> delete=<y/n> input_file=<input-fname>
The following are the new parameters introduced in the ORAPWD utility:
FORMAT: We can pass either a value 12 or a legacy value . If you pass value 12, it will
create a password file in the version 12.x format. However, if you pass the legacy value ,
it will create the password file in a legacy format. The password file of version 12.x
supports SYSBACKUP, SYSDG, and SYSKM administrative privileges, whereas the
legacy format file supports SYSDBA and SYSASM privileges only.
SYSBACKUP/SYSDG/SYSKM: If these parameters are set to Y, it creates a
corresponding entry of these privileges in the password file and you are prompted for
the password of the user to whom you are assigning these privileges.
INPUT_FILE: The name of the input password file. ORAPWD migrates the entries in
the input file to a new password file. This argument can convert a password file from a
legacy format to a 12 format.
It is recommended not to use the SYSBACKUP user to connect as SYSBACKUP. The DBAs
that are responsible for backup/recovery would share the same password. This is undesirable
in terms of accountability. Also, it would be impossible to take away this privilege from
particular DBAs later (unless changing the password). It is better to create a database account
designated for each DBA and grant the SYSBACKUP privilege if necessary. In this way, no
password is shared, and SYSBACKUP can be revoked from any DBA without affecting other
DBAs.
You can also migrate a password file from the legacy format (prior version) to the new
Oracle Database 12c format during the upgrade process by using the following command:
orapwd FILE='/u01/app/oracle/passwd_files/orapworcl' FORMAT=12 
INPUT_FILE='/u01/app/oracle/passwd_files/orapworcl' FORCE=Y
FORCE=Y will replace the existing file with a new format file.

To use password file authentication for the SYSBACKUP user, you have to either grant
SYSBACKUP administrative privilege to the required user or you have to manually make an
entry in a password file with SYSBACKUP=Y. The following examples describe both methods:
Granting administrative privilege:
Let's say we have a DEO user, we can grant SYSBACKUP administrative privilege to the
DEO user and its entry will be added to a password file automatically:
SQL> grant sysbackup to deo; 
 
Grant succeeded. 
 
SQL> select * from v$pwfile_users where username = 'DEO'; 
 
USERNAME           SYSDB SYSOP SYSAS SYSBA SYSDG SYSKM     CON_ID 
------------------------ ----- ----- ----- ----- ----- ----- ---------- 
DEO             FALSE FALSE FALSE TRUE  FALSE FALSE      0 
Making an entry in the password file:
You can use the ORAPWD utility to make an entry in the password file, as shown in the
following code:
[oracle@advait dbs]$ orapwd file=orapwdeo sysbackup=y 
Enter password for SYS:  
Enter password for SYSBACKUP:  
Once you make an entry in the password file, you should be able to connect as SYSBACKUP
privilege using correct password. OS authentication works anyway without using a password
file because of Oracle group to OS group mapping.
You can check what users are present in the password file by querying the V$PWFILE_USER
view:
SQL> select * from v$pwfile_users;
USERNAME       SYSDB SYSOP SYSAS SYSBA SYSDG SYSKM     CON_ID
-------------- ----- ----- ----- ----- ----- ----- ----------
SYS            TRUE  TRUE  FALSE FALSE FALSE FALSE      0
SYSDG          FALSE FALSE FALSE FALSE TRUE  FALSE      0
SYSBACKUP      FALSE FALSE FALSE TRUE  FALSE FALSE      0
SYSKM          FALSE FALSE FALSE FALSE FALSE TRUE       0
DEO            FALSE FALSE FALSE TRUE  FALSE FALSE      0

Privilege analysis
One of the major challenges that DBA's are facing today is having excess privileges assigned
to the users, thereby violating the critical principles of least privilege assignments. These are
the privileges that are granted, but never used by users and are granted excessively as part of
some role. Because it's difficult to identify if we have such privileges assigned to various
users, these kinds of things are never looked into and they are never worked upon. Ideally, a
DBA should be analyzing the privileges assigned to the users and revoking the excessive or
unwanted privileges from the users.
Oracle 12c changes all of that by introducing the privilege analysis tool. You can use this tool
to analyze what privileges are being used by different users. This way we can clearly identify
what privileges are assigned extra to the user and take action to remove those unrequired
privileges.
A privilege analysis mechanism captures privilege usage according to a specific condition.
For example, you can track the privileges a user exercises during a database session. You can
use the results of the privilege analysis to either remove unnecessary grants or reconfigure
the privilege grants.
Oracle 12c has introduced a DBMS_PRIVILEGE_CAPTURE package to capture the usage of
privileges by users. The following are the steps to perform privilege analysis and take
required actions:
1. Create a privilege analysis policy based on certain types and conditions.
2. Enable the policy and start analyzing the privileges.
3. Disable the policy after a time interval. This is the time interval during which users are
performing day to day tasks that they are required to do. This is our observation period.
4. Generate reports of both USER and UNUSED privileges.
5. Viewing the report generated by the privilege analysis and taking further action.
6. Drop the privilege analysis policy and report if required.
Let's take a detailed look at these steps.

Creating a privilege analysis policy
Before we create a privilege analysis policy, we need to decide on the type of privilege
analysis and accordingly define the condition for performing privilege analysis. This
basically decides which type of objects we want to target for privilege analysis.
There are four types of privilege analysis:
Database: Using the database level privilege analysis feature analyzes all used privileges,
across the entire database. This type of analysis does not need any conditions in privilege
analysis policy.
Role: Using the role level privilege analysis feature analyzes the privileges exercised
through any given role. For example, if you create a privilege analysis policy to analyze
on PUBLIC, the privileges that are directly and indirectly granted to PUBLIC are
analyzed when they are used.
Context: This privilege analysis feature analyzes privileges assigned through an
application module or a specified context. For example, the context may be a condition
such as USERS=OE, HR, or MODULES=INVENTORY. Different conditions can be combined with
AND and/or OR Boolean operators.
Role and Context: We can combine the conditions of roles and context to perform the
combined analysis. For example, if we want to see privilege analysis of a specific role
used by a specific set of users, we can go for this combination.
You can create a privilege analysis policy by executing the CREATE_CAPTURE procedure from
the DBMS_PRIVILEGE_CAPTURE package. The following is the description of the procedure:
DBMS_PRIVILEGE_CAPTURE.CREATE_CAPTURE (   
 name            IN  VARCHAR2,   
 description     IN  VARCHAR2 DEFAULT NULL,    
 type            IN  NUMBER DEFAULT G_DATABASE,   
 roles           IN  ROLE_NAME_LIST DEFAULT ROLE_NAME_LIST(),                 
 condition       IN  VARCHAR2 DEFAULT NULL);
The following are the three important parameters of this procedure:
Type: We can define the type of privilege analysis we need to perform. We have already
seen the preceding type and it could be either Database, Role, Context, or Role and
Context. You need to pass a respective value to the type parameter to perform any of the
preceding analysis. The database type of analysis is the default analysis if we don't
provide any value for the type parameter:
Database         - TYPE => DBMS_PRIVILEGE_CAPTURE.G_DATABASE. 
Role             - TYPE => DBMS_PRIVILEGE_CAPTURE.G_ROLE  
Context          - TYPE => DBMS_PRIVILEGE_CAPTURE.G_CONTEXT  
Role and context - TYPE => DBMS_PRIVILEGE_CAPTURE.G_ROLE_AND_CONTEXT  
Roles: Denotes the roles whose used privileges will be analyzed. You must specify this
argument if you specify DBMS_PRIVILEGE_CAPTURE.G_ROLE or
DBMS_PRIVILEGE_CAPTURE.G_ROLE_AND_CONTEXT for the type argument. You should

specify roles as comma separated values in ROLE_NAME_LIST list objects, as follows:
ROLES => ROLE_NAME_LIST('role1','role2') 
Condition: Let's specify a Boolean expression with up to 4,000 characters. You must
specify this argument if you specify DBMS_PRIVILEGE_CAPTURE.G_CONTEXT or
DBMS_PRIVILEGE_CAPTURE.G_ROLE_ AND_CONTEXT for the type argument. Only
SYS_CONTEXT expressions with relational operators (==, >, >=, <, <=, <>, BETWEEN, and IN)
are permitted in this Boolean expression:
CONDITION => 'SYS_CONTEXT(''USERENV'',''SESSION_USER'') = ''HR''' 
Let's take an example of each type. We are now going to create a database type privilege
analysis policy:
Database type privilege analysis policy:
SQL> exec DBMS_PRIVILEGE_CAPTURE.CREATE_CAPTURE (    - 
   name        => 'DB_LEVEL_PRIVS',            - 
   description => 'All privileges',            - 
   type        => DBMS_PRIVILEGE_CAPTURE.G_DATABASE);  
PL/SQL procedure successfully completed. 
This will let you analyze all privileges in the database.
Role type privilege analysis policy:
Instead of analyzing at complete database level, you can analyze permissions granted via
roles. If you want to know what all privileges in a role are actually being used by users, you
should go for analyzing the roles. Following example creates a role type privilege analysis
policy.
SQL> exec DBMS_PRIVILEGE_CAPTURE.CREATE_CAPTURE (    - 
   name        => 'ROLE_PRIVS',                     - 
   description => 'Pivileges in roles',             - 
   type        => DBMS_PRIVILEGE_CAPTURE.G_ROLE,    -  
   roles       => ROLE_NAME_LIST('role1','role2'));  
PL/SQL procedure successfully completed. 
In the preceding example, we used DBMS_PRIVILEGE_CAPTURE.G_ROLE for the TYPE parameter
and we are analyzing two roles-role1 and role2.
Context type privilege analysis policy:
Context type privilege analysis policy can be used to analyze privileges based on certain
conditions. For example, if want to analyze privileges granted to a user, we can use
SYS_CONTEXT in the condition and analyze the privileges that are used by that user. Later we
can get unused privileges from the report. The following example shows creating context

type privileges:
SQL> exec DBMS_PRIVILEGE_CAPTURE.CREATE_CAPTURE (    - 
   name        => 'CONTEXT_PRIVS',                  - 
   description => 'Privileges for user',            - 
   type        => DBMS_PRIVILEGE_CAPTURE.G_CONTEXT, -  
   condition   => 'SYS_CONTEXT(''USERENV'',''SESSION_USER'') = ''HR''');  
PL/SQL procedure successfully completed. 
Role and context type privilege analysis policy:
You can also create a privilege analysis policy by combining the roles and context
parameters. This will be helpful for us in a situation where we want to analyze what privileges
in a role/roles are being used by specific users or a set of users. The following example can
analyze public privileges used when you run the order_entry module:
SQL> exec DBMS_PRIVILEGE_CAPTURE.CREATE_CAPTURE (                 - 
   name        => 'ROLE_CONTEXT_PRIVS',                          - 
   description => 'public Pivileges used by module order_entry', - 
   type        => DBMS_PRIVILEGE_CAPTURE.G_ROLE_AND_CONTEXT,     -  
   roles       => ROLE_NAME_LIST('PUBLIC'),                      - 
   condition   => 'SYS_CONTEXT(''USERENV'',''MODULE'') = ''order_entry''');  
PL/SQL procedure successfully completed. 

Enabling privilege analysis
Once we are done with creating required policies, we can start the privilege analysis. You can
only start one privilege policy at a time with an exception of a DATABASE type privilege policy,
which can be started along with any other type of policy.
For example, let's start a database type privilege policy along with a role type privilege
analysis policy:
SQL> exec DBMS_PRIVILEGE_CAPTURE.ENABLE_CAPTURE(name => 'DB_LEVEL_PRIVS'); 
PL/SQL procedure successfully completed. 
SQL> exec DBMS_PRIVILEGE_CAPTURE.ENABLE_CAPTURE(name => 'ROLE_PRIVS'); 
PL/SQL procedure successfully completed. 
If you try to start any other type of privilege policy, you will get following 
error: 
SQL> exec DBMS_PRIVILEGE_CAPTURE.ENABLE_CAPTURE(name => 'CONTEXT_PRIVS'); 
BEGIN DBMS_PRIVILEGE_CAPTURE.ENABLE_CAPTURE(name => 'CONTEXT_PRIVS'); END; 
* 
ERROR at line 1: 
ORA-47934: Two privilege captures are already enabled. 
ORA-06512: at "SYS.DBMS_PRIVILEGE_CAPTURE", line 28 
ORA-06512: at line 1 
Privilege policy remains enabled even after database restart. You have to manually disable the
privilege policy.

Disabling the privilege analysis
Once the user activities are run or you have enabled the privilege analysis for a sufficient
amount of time, you can disable it using the DBMS_PRIVILEGE_CAPTURE.DISABLE_CAPTURE
procedure:
SQL> exec DBMS_PRIVILEGE_CAPTURE.DISABLE_CAPTURE(name => 'DB_LEVEL_PRIVS'); 
PL/SQL procedure successfully completed. 
SQL> exec DBMS_PRIVILEGE_CAPTURE.DISABLE_CAPTURE(name => 'ROLE_PRIVS'); 
PL/SQL procedure successfully completed. 

Generating a privilege analysis report
You need to generate a separate report for each analysis that was enabled. You can generate
the report using the DBMS_PRIVILEGE_CAPTURE.GENERATE_RESULT procedure, as shown in the
following code:
SQL> exec DBMS_PRIVILEGE_CAPTURE.GENERATE_RESULT(name => 'DB_LEVEL_PRIVS'); 
PL/SQL procedure successfully completed. 
SQL> exec DBMS_PRIVILEGE_CAPTURE.GENERATE_RESULT(name => 'ROLE_PRIVS'); 
PL/SQL procedure successfully completed. 

Viewing the report
We have several tables that store the information about used and unused privileges as
generated by the privilege analysis report. The following are the tables available for viewing
different information about the privilege analysis report.
Used privileges tables:
The following tables show what object privileges or system privileges are actually used
during the analysis time:
DBA_USED_PRIVS: This table lists all the privileges that have been used by users as
reported by privilege analysis policies
DBA_USED_OBJPRIVS: This table lists the object privileges that have been used by users as
reported by privilege analysis policies
DBA_USED_SYSPRIVS: This table lists the system privileges that have been used by users as
reported by privilege analysis policies
DBA_USED_PUBPRIVS: This table lists all the privileges of the PUBLIC role that have been
used by users as reported by privilege analysis policies
DBA_USED_OBJPRIVS_PATH: This table provides the object privileges that are used by
users and also the source from where those privileges are granted to the user (either
directly or via a role)
DBA_USED_SYSPRIVS_PATH: This table provides the system privileges that are used by
users and also the source from where those privileges are granted to the user (either
directly or via a role)
Unused privileges:
The following tables show what object privileges or system privileges are unused during the
analysis time:
DBA_UNUSED_PRIVS: This table lists all the privileges that have not been used by users as
reported by privilege analysis policies
DBA_UNUSED_OBJPRIVS: This table lists the object privileges that have not been used by
users as reported by privilege analysis policies
DBA_UNUSED_SYSPRIVS: This table lists the system privileges that have not been used by
users as reported by privilege analysis policies
DBA_UNUSED_OBJPRIVS_PATH: This table provides the object privileges that are not used
by users and also the source from where those privileges are granted to the user (either
directly or via a role)
DBA_UNUSED_SYSPRIVS_PATH: This table provides the system privileges that are not used
by users and also the source from where those privileges are granted to the user (either
directly or via a role)
For example, the following code shows what privileges are not used by the user DEO and the
path for those privileges. The PATH column refers to how those privileges are granted to the

user:
SQL> select CAPTURE, SYS_PRIV, PATH from DBA_UNUSED_PRIVS where USERNAME = 
'DEO';
CAPTURE          SYS_PRIV             PATH
---------------- -------------------- --------------------------
DB_LEVEL_PRIVS   DELETE ANY TABLE    GRANT_PATH('DEO', 'ROLE1')
DB_LEVEL_PRIVS   UPDATE ANY TABLE    GRANT_PATH('DEO', 'ROLE1')
DB_LEVEL_PRIVS   INSERT ANY TABLE    GRANT_PATH('DEO', 'ROLE1')
DB_LEVEL_PRIVS   CREATE ANY PROCEDURE GRANT_PATH('DEO', 'ROLE2')
Similarly, if you want to check what privileges are actually used by the user, you can check
the DBA_USED_PRIVS view:
SQL> select CAPTURE, SYS_PRIV, PATH from DBA_USED_PRIVS where username = 'DEO' 
order by 1, 2;
CAPTURE          SYS_PRIV               PATH
---------------- ---------------------- --------------------------
DB_LEVEL_PRIVS   CREATE SESSION         GRANT_PATH('DEO')
DB_LEVEL_PRIVS   EXECUTE ANY PROCEDURE  GRANT_PATH('DEO', 'ROLE2')
DB_LEVEL_PRIVS   SELECT ANY TABLE       GRANT_PATH('DEO', 'ROLE1')
DB_LEVEL_PRIVS   SELECT ANY TABLE       GRANT_PATH('DEO')
DB_LEVEL_PRIVS                          GRANT_PATH('PUBLIC')
DB_LEVEL_PRIVS                          GRANT_PATH('PUBLIC')
DB_LEVEL_PRIVS                          GRANT_PATH('PUBLIC'
DB_LEVEL_PRIVS                          GRANT_PATH('PUBLIC')
DB_LEVEL_PRIVS                          GRANT_PATH('PUBLIC')
DB_LEVEL_PRIVS                          GRANT_PATH('PUBLIC')
DB_LEVEL_PRIVS                          GRANT_PATH('PUBLIC')
ROLE_PRIVS       EXECUTE ANY PROCEDURE  GRANT_PATH('DEO', 'ROLE2')
ROLE_PRIVS       SELECT ANY TABLE       GRANT_PATH('DEO', 'ROLE1')
13 rows selected.
You can check all the privilege policies created in the database in the DBA_PRIV_CAPTURES
view:
SQL> select type, enabled, roles, context from dba_priv_captures;
TYPE             ROLES            CONTEXT
---------------- ---------------- ---------------------------
DATABASE       
ROLE             (111, 112)
CONTEXT                           SYS_CONTEXT('USERENV',
                                   'SESSION_USER') = 'HR'
ROLE_AND_CONTEXT ROLE_ID_LIST(1)  SYS_CONTEXT('USERENV',
                                   'MODULE') = 'order_entry'

Dropping the privilege analysis policy
Once you are done with analyzing the report and revoking extra privileges from the users,
you can drop the privilege analysis policy using the DBMS_PRIVILEGE_CAPTURE.DROP_CAPTURE
procedure, as shown in the following code:
SQL> exec DBMS_PRIVILEGE_CAPTURE.DROP_CAPTURE(name => 'CONTEXT_PRIVS');
PL/SQL procedure successfully completed.
Note that if you drop the policy, you also remove all the unused privilege records associated
with the policy.
Also, you have to first disable the capture before you can drop it, or else you will get the
following error:
SQL> exec DBMS_PRIVILEGE_CAPTURE.DROP_CAPTURE(name => 'ROLE_PRIVS');
BEGIN DBMS_PRIVILEGE_CAPTURE.DROP_CAPTURE(name => 'ROLE_PRIVS'); END;
ERROR at line 1:
ORA-47932: Privilege capture ROLE_PRIVS is still enabled.
ORA-06512: at "SYS.DBMS_PRIVILEGE_CAPTURE", line 48
ORA-06512: at line 1

New privilege checking during PL/SQL calls
Until previous releases, we were able to create procedures with the definer's right or with the
invoker's right. The AUTHID property of a stored PL/SQL unit defines if the procedure is
created with the definer's right or with the invoker's right. When we create a procedure using
the definer's right, the procedure runs using the privileges of the owner (definer) who created
the procedure. So if this procedure does a DML into a table, another user only needs to have
execute permissions on this procedure and they will be able to perform DML on that table
using the procedure even though they don't have direct DML permissions on the table. The
following figure clarifies this point:
In the above figure, user U2 executes permission on the U1.PROC procedure. This procedure
does select from U1.T1 and it deletes from U1.T2. Even though user U2 does not have
permissions on the T1 and T2 tables, they will be able to perform these DML's by just having
execute permissions on the U1.PROC1 procedure.
On the other hand, if we create a procedure with the invoker's right, then the user executing
the procedure should have permissions on all the objects on which the procedure is acting.
So, if the procedure is doing DML on a table, the user executing the procedure should have
DML privilege on the table on which the procedure is performing DML. The following figure
will clarify this point:

In the above figure, procedure U1.PROC1 is created with the invoker's right (AUTH_ID
CURRENT_USER). In this case, if another user U2 wants to execute this procedure, they need to
have execute permission on this procedure, plus they should also have select and DML
privileges on the underlying tables accessed by this procedure. In the case of procedure with
the invoker's right, privilege checking happens during runtime.
The preceding behavior continues in Oracle 12c as well, but Oracle 12c has introduced a new
privilege called INHERIT PRIVILEGE. If you see the invoker's right procedure, we have a
security loop hole. If a low privilege user creates an invoker's right procedure, it could
potentially perform unintended or malicious actions if executed by a high privileged user.
For example, if a low privilege user creates a simple invoker's right procedure to perform
some DML's on a table and also puts a simple grant statement to grant a DBA role to themself,
then if a high privilege user having a DBA role with a grant option runs this procedure, the
DBA role will be granted to the low privilege user automatically. Because the high privilege
user has a DBA role with a grant option, when the invoker's right procedure was executed it
uses the privileges of the invoker. This is a big security loop hole.
Let's take a simple example to understand this better. Let's say we have a low_privs user, who
does not have much privileges. This user has created the following procedure, which is an
invoker's right procedure:
create or replace procedure test_proc authid current_user as   
2  begin 
3  insert into low_privs.T1 values (100,'TEST');  
4  commit;  
5  execute immediate 'grant dba to low_privs';  
6  end;  
7  
/Procedure created.
Note that we have an execute immediate statement in the procedure, which grants a DBA role
to the low_privs user.
Next, we create a high_privs user and grant a DBA role to that user with the admin option:
SQL> create user high_privs identified by oracle; 
User created. 
SQL> grant dba to high_privs with admin option; 
Grant succeeded. 
So, currently, the low_privs user does not have a DBA role assigned to it:
SQL>select * from dba_role_privs where grantee = 'LOW_PRIV'; 

no rows selected 
Next, we will execute the procedure using HIGH_PRIVS user and see what happens:
SQL> connect high_privs/oracle Connected.
SQL> exec low_privs.test_proc; 
PL/SQL procedure successfully completed.
We can see it has inserted the record in the T1 table, as shown in the following code:
SQL> select * from T1; 
     COL1 COL2 
---------- ---------- 
        1 A 
        2 B 
        3 C 
      100 TEST 
But if we check the roles assigned to the LOW_PRIVS user, we can see that a DBA role is
granted:
SQL>select * from dba_role_privs where grantee = 'LOW_PRIVS'; 
GRANTEE                        GRANTED_ROLE                   ADM DEF 
------------------------------ ------------------------------ --- --- 
LOW_PRIVS                      DBA                            NO  YES 
This happened because the invoker's right procedure had to execute an immediate statement to
grant a DBA role to LOW_PRIVS. So any low privilege user can play such mischief to gain
access.

New inherit privilege
Oracle 12c has introduced a new restriction for fixing this security loop hole. When a user
runs an invoker's right procedure, Oracle now checks the procedure owner's privileges
before running the procedure. The owner of the procedure should have INHERIT PRIVILEGES
on the invoking user or INHERIT ANY PRIVILEGE. If this is not the case, a runtime error is
raised.
Internally, the session is temporarily switched to the definer's right and checks if the owner of
the procedure has an INHERIT PRIVILEGES object privilege on the user who is executing the
procedure or an INHERIT ANY PRIVILEGES system privilege. The session then reverts back to
the prior environment of the invoker's right.
So in Oracle 12c, if the HIGH_PRIVS user executes the invoker's right procedure in the
LOW_PRIVS schema, they will get the following error:
SQL> exec low_privs.test_proc;
BEGIN low_privs.test_proc; END;
*
ERROR at line 1:
ORA-06598: insufficient INHERIT PRIVILEGES privilege
ORA-06512: at "LOW_PRIVS.TEST_PROC", line 1
ORA-06512: at line 1
This is because we have not granted the new INHERIT PRIVILEGE privilege to the LOW_PRIVS
user on the HIGH_PRIVS user. We can do so using the following grant statement:
SQL> grant inherit privileges on user high_privs to low_privs; 
Grant succeeded. 
After granting this privilege, the preceding procedure can be executed by the HIGH_PRIVS
user.
The benefit of this privilege is that it gives invoking users control over who can access their
privileges when they run an invokers' rights procedure. This way we can grant the INHERIT
PRIVILEGES privilege only on the trusted users who are allowed to execute the invoker's right
privilege.
Oracle has also introduced a higher level INHERIT ANY PRIVILEGES system privilege. Also, a
user who has INHERIT ANY PRIVILEGES will allow any other user to execute the invoker's
right procedure. So basically, behavior goes back to previous releases.

New privilege checking with bequeath views
Bequeath CURRENT_USER views are new in Oracle 12c and have similar security features to that
of PL/SQL code with the invoker's right. They behave like the invoker's right rather than the
owner's right. When you call the AUTHID CURRENT_USER function (also called invoker's right),
the current schema and currently enabled roles and privileges are inherited from the call
user's environment. We can now use the AUTHID CURRENT user while creating the view and it
behaves in the same way as the invoker's right PL/SQL objects.
A user selecting from the view should have privileges on underlying tables. Only then will
they be able to see the data from the view. But there is more to it. Oracle 12c also applies a
new privilege checking for bequeath views. The INHERIT PRIVLEGE checking that happens in
PL/SQL objects also applies for bequeath views. The owner of the view should have the
INHERIT PRIVILEGES privilege on the list of trusted users or the INHERIT ANY PRIVILEGES
system privilege. Only then can those users query the bequeath views.
The bequeath view types are displayed in a new BEQUEATH column in DBA_VIEWS.

Oracle Data Redaction
Many of the day to day applications have a need to redact customer sensitive data. Redacting
data means hiding parts of the data and showing only partial data. A simple example of data
redaction includes credit card numbers or back account numbers. Whenever we get an e-mail
or a message from banks, we get to see only the last four digits of a bank account number or
card number. Other valid digits of an account number or card number are released by the
character X. For example, an account number will be shown in the message as XXXXXXXX5171.
This is a very classical example of data redaction.
Until now, this was being done at application tier. Once an application receives the data,
developers used to encode the logic of redacting the data based on the user that has been
logged into the application. A super user should have privileges to view all the data, whereas
other employees should not see sensitive data.
Starting with Oracle 12c, Oracle has moved this functionality of data redaction inside the
database. This approach has several benefits of over redacting at the application tier:
Data redaction inside the database is a transparent, flexible, and simple solution.
No additional coding is required at the application layer.
Data redaction inside the database modifies the data right before the results are sent to the
clients. The overhead involved is negligible. It does not modify any data block in disk or
in memory.
Columns are redacted based on flexible policies.The DBA developer has complete
control over creating and altering the policy as per the requirements.
This feature come built-in inside the database and no separate installation is required.
The same features can be applied to a variety of applications to redact application
screens, reports, dashboards, and so on. Redaction depends on data and not the end
application.
Whereas a product such as Oracle Data Masking actually transforms data by applying
masking formats, Oracle Data Redaction doesn't modify the data stored on disk. You must
consider Oracle Data Redaction not as a replacement, but as a complement to other Oracle
Database Security solutions such as Oracle Database Vault and Oracle Audit Vault.

Activities exempt from data redaction
When we implement data redaction, operational activities are not subject to redaction.
Operational activities include:
Backup and restore of the database
Export and import of data
Patching and upgrades
Replication
If a user is performing any of the preceding activities, the data will not be redacted. For
example, if a user is doing export/import for transferring the data, redaction will not be
applied to the data even if we have a redaction policy created on the table. Similarly,
replication happens for the actual data and no redaction policy is applicable for replication.
Other than these operational activities, users such as SYS and SYSTEM are also exempt from
redaction policies. So these users will be able to see sensitive data without applying any
redaction policy. This is possible because of the EXEMPT REDACTION POLICY system privilege.
The SYS and SYSTEM have this new privilege assigned to them. Whichever user has this
privilege will be exempt from data redaction.

Types of data redaction
Oracle 12c offers four types of data redaction:
Full
Partial
Random
Regular expression
Other than these four, we also have a data redaction type of NONE. This is provided to evaluate
the impact of implementing redaction policy on performance. Also this redaction policy is
useful in testing where a user wants to see complete data for testing purposes.
You can create redaction policy on columns in a table using the DBMS_REDACT package. This
package has several procedures to add, modify, and delete policies on the table/columns. We
will look into the details of creating policies at a later point.
Let's take a look at each type of data redaction.

Full data redaction
Full data redaction redacts the entire content of the column. For example, if a column holds
the account numbers, all the characters of the account number will be redacted. The format of
the output displayed depends on the data type of the column:
Character data type: The output text is a single blank space. The value of the column
will be replaced by a single blank space.
Number data type: The output text is a single 0. The value of the column will be
replaced by a single 0.
Date data type: The output is a fixed value of 1st Jan 2001. The value of the column
will be replaced by 01-Jan-2001.
You can create a redaction policy using the DBMS_REDACT.ADD_POLICY procedure. One of the
parameters to this procedure is FUNCTION_TYPE. This parameter value decides the type of
redaction. To create a full redaction policy, you should pass the function type as
DBMS_REDACT.FULL, as shown in the following code:
FUNCTION_TYPE => DBMS_REDACT.FULL
Full redaction is the default type of redaction if we do not specify FUNCTION_TYPE in
DBMS_REDACT.ADD_POLICY, it will by default create a full redaction policy.
The following is the kind of output you can see for full data redaction:
Data type Data in the table column Output data
Number
302974230482
0
Characters This is test
(blank space)
Date
24-Jun-1983
01-Jan-2001

Partial data redaction
In case of partial data redaction, you redact only part of the data. So, partial data is visible to
the end user. For example, credit card and bank account numbers are usually partially
redacted, where we can see the last four characters of the data.
You must set the function_type parameter in the DBMS_REDACT.ADD_POLICY procedure to
DBMS_REDACT.PARTIAL and also specify the function_parameters parameter to define partial
data redaction:
FUNCTION_TYPE => DBMS_REDACT.PARTIAL
The function_parameters parameter includes the starting position for redaction, number of
characters to redact, and the redaction characters to use as replacement. Without setting the
function_parameter argument, partial redaction will not work because partial redaction does
not have any default values like we have for full redaction.
The following are the examples of how partial data redaction will work on different data
types:
Character data type: For example, if you have a bank account number in the following
format: 123-32423-5171 and you want to redact the first eight characters only, you can
use the following function_parameters to redact the first eight characters:
Function_parameters => 'VVVFVVVVVFVVVV,*,1,8' 
This will replace the first eight numbers with an astrix (*), leaving behind the hyphens (-).
The output will look as follows: ***-*****-5171
Number data type: For example, if you have a bank account in a simple format:
123324235171, and you want to redact the first eight characters, you can do so using the
following function_parameter argument:
Function_parameters => 'X,1,8' 
This will replace the first eight characters by value X. The output will look as follows:
XXXXXXXX5171

Random data redaction
Random data redaction works on complete values of the column such as full data redaction.
But the difference between full data redaction and random data redaction is that random data
redaction replaces the data with random values as against default values replaced in case of
full data redaction. Every time we select a column which has random data redaction policy set,
we will see different values for that column. You should specify the FUNCTION_TYPE parameter
to DBMS_REDACT.RANDOM to create a random data redaction policy:
FUNCTION_TYPE => DBMS_REDACT.RANDOM 
The output that is displayed for a random redaction policy depends on the data type:
Character data type: The output is a string of random characters. The length of the
string depends on the size of the column. If the column length is 20 (varchar2(20)),
then the output will be the string of 20 random characters.
Number data type: The output is random numbers. The length of the output is less than
or equal to the length of the original data. For example, if we had a number
12408897425, using random redaction will create the output number as 52921622641.
Date data type: The output is a random valid date.

Regular expression redaction
Regular expression redaction offers high flexibility in choosing how the data should be
redacted. Regular expressions are very powerful when it comes to selecting partial data. We
can use different patterns of regular expressions to match and replace data in the column. For
using regular expression redaction we should set the FUNCTION_TYPE parameter of the
DBMS_REDACT.ADD_POLICY procedure to DBMS_REDACT.REGEXP, as shown in the following
code:
FUNCTION_TYPE => DBMS_REDACT.REGEXP 
Apart from the FUNCTION_TYPE parameter, you should also use the following parameters to
search and replace using regular expressions:
REGEXP_PATTERN: This takes the input pattern to search the value in the column. The
selected string will be replaced with redacted values.
For example: REGEXP_PATTERN => '(.+)@(.+\.[A-Za-z]{2,4})'
REGEXP_REPLACE_STRING: This parameter takes the input string that will be used to
replace the string found by the preceding REGEXP_PATTERN.
REGEXP_BEGINNING: This parameter takes the starting position for searching the pattern.
An example value could be DBMS_REDACT.RE_BEGINNING.
REGEXP_OCCURRENCE: This takes input regarding replacing the first occurrence or all
occurrences of the selected pattern. An example value could be DBMS_REDACT.RE_ALL.
REGEXP_MATCH_PARAMETER: This parameter decides the case sensitivity of the regular
expression search pattern. You can mention the search as case insensitive using
REGEXP_MATCH_PARAMETER => 'i'.
For example, the following REGEXP_SEARCH looks for e-mail address patterns:
REGEXP_PATTERN => '(.+)@(.+\.[A-Za-z]{2,4})' 
The preceding expression will look for:
One or more characters
Followed by the @ character
Followed by one or more characters
Followed by the . character
Followed by characters in A-Z or a-z and only 2-4 characters (for example, in, com, org,
and so on)

Creating a redaction policy
When you create a redaction policy, you decide the following attributes about the data:
What to redact: You can decide what to redact by specifying the SCHEMA_NAME,
OBJECT_NAME, and COLUMN_NAME parameter of the DBMS_REDACT.ADD_POLICY procedure.
This will apply a redaction policy at the column level. If you don't specify the column, no
columns are redacted by the policy. This enables you to create your policies so that they
are in place, and then later on, you can add the column specification when you are ready.
How to redact: This is the redaction type that you want to specify. We have already seen
four different redaction types. You can specify the redaction type by setting the
FUNCTION_TYPE parameter in the DBMS_REDACT.ADD_POLICY procedure. If you do not
specify the FUNCTION_TYPE parameter, the default policy taken will be FULL redaction.
When to redact: You can specify when the redaction should be applicable. You can
specify this by setting the EXPRESSION parameter of the DBMS_REDACT.ADD_POLICY
procedure. You can specify different types of expression. For example, the following
expression enabled the redaction policy for APP_USER:
expression => 'SYS_CONTEXT(''USERENV'',''SESSION_USER'') = ''APP_USER''' 
Remember that you can apply only one redaction policy to a table or view. You can have
multiple columns added in the same policy and have different types of redaction applied to
different columns in the same policy.
To redact data in a table or view, all you need is the EXECUTE privilege on this package—you
don't need privileges on the tables and views. Therefore, it's critical that you strictly control
the granting of the EXECUTE privilege on the DBMS_REDACT package:

Adding a redaction
The following is the syntax for DBMS_REDACT.ADD_POLICY:
DBMS_REDACT.ADD_POLICY ( 
object_schema               IN VARCHAR2 := NULL, 
object_name                 IN VARCHAR2 := NULL, 
policy_name                 IN VARCHAR2,  
policy_description          IN VARCHAR2 := NULL, 
column_name                 IN VARCHAR2 := NULL, 
column_description          IN VARCHAR2 := NULL, 
function_type               IN BINARY_INTEGER := DBMS_REDACT.FULL, 
function_parameters         IN VARCHAR2 := NULL, 
expression                  IN VARCHAR2, 
enable                      IN BOOLEAN := TRUE, 
regexp_pattern              IN VARCHAR2 := NULL, 
regexp_replace_string       IN VARCHAR2 := NULL, 
regexp_position             IN BINARY_INTEGER :=1, 
regexp_occurrence           IN BINARY_INTEGER :=0, 
regexp_match_parameter      IN VARCHAR2 := NULL); 
The following list explains these parameters:
object_schema: Specifies the schema of the object on which the data redaction policy will
be applied.
object_name: Specifies the name of the table or view to which the data redaction policy
applies.
policy_name: Specifies the name of the policy to be created.
policy_description: Specifies a brief description of the purpose of the policy.
column_name: Specifies the column whose data you want to redact.
column_description: Specifies a brief description of the column that you are redacting.
function_type: Specifies a function that sets the type of redaction.
function_parameters: Specifies how the column redaction should appear for partial
redaction.
expression: Specifies a Boolean SQL expression to determine how the policy is applied.
enable: When set to TRUE, it enables the policy upon creation.
regexp_pattern, regexp_replace_string, regexp_position, regexp_position,
regexp_occurrence, regexp_match_parameter: Enables you to use regular expressions to
redact data, either fully or partially. If the regexp_pattern does not match anything in the
actual data, then full redaction will take place, so be careful when specifying the
regexp_pattern.

Adding a redaction policy to a table or view
I have a table t1 in the advaitd schema, as shown in the following code:
SQL> select * from advaitd.t1; 
        COL1 COL2                 COL3 
------------- -------------------- --------- 
320934301232 advaitdeo@yahoo.com  31-JUL-16 
We can create a full redaction policy on COL1, which is a number data type. By creating a full
redaction policy, the value in the column will be shown as 0 if the user is not exempted by full
redaction privileges and if the EXPRESSION parameter fulfills the criteria. For our example, we
will provide an EXPRESSION parameter such that the policy is applicable to all users and every
connection.
The following procedure will create a FULL redaction policy on COL1 of table T1:
SQL> exec dbms_redact.ADD_POLICY(           - 
       OBJECT_SCHEMA   => 'ADVAITD',      - 
       OBJECT_NAME     => 'T1',        - 
       COLUMN_NAME     => 'COL1',        - 
       FUNCTION_TYPE   => DBMS_REDACT.FULL, - 
       POLICY_NAME     => 'TEST_POLICY',    - 
       EXPRESSION      => '1=1'); 
PL/SQL procedure successfully completed. 
SQL> select * from advaitd.t1; 
     COL1 COL2                 COL3 
---------- -------------------- --------- 
        0 advaitdeo@yahoo.com  31-JUL-16 
As you can see after adding the full redaction policy, COL1 is now showing a value of 0.

Modifying a redaction policy
You can also modify the existing policy to add another column in the same policy and apply a
different type of redaction to the second column.
For example, let's add col2 in the preceding policy and apply a regular expression redaction
to col2.
We can use the DBMS_REDACT.ALTER_POLICY procedure to modify the existing policy.
If we are adding a column to the existing policy, the ACTION parameter in
DBMS_REDACT.ALTER_POLICY should have a value of DBMS_REDACT.ADD_COLUMN:
exec DBMS_REDACT.ALTER_POLICY( - 
       OBJECT_SCHEMA           =>'ADVAITD',                    - 
       OBJECT_NAME             =>'T1',                         -  
       COLUMN_NAME             =>'COL2',                       -  
       POLICY_NAME             =>'TEST_POLICY',                -  
       ACTION                  => DBMS_REDACT.ADD_COLUMN,      - 
       FUNCTION_TYPE           => DBMS_REDACT.REGEXP,          - 
       EXPRESSION              => '1=1',                       - 
       REGEXP_PATTERN          => '(.+)@(.+\.[A-Za-z]{2,4})',  - 
       REGEXP_REPLACE_STRING   => '[hidden]\2',                - 
       REGEXP_POSITION         => DBMS_REDACT.RE_BEGINNING,    - 
       REGEXP_OCCURRENCE       => DBMS_REDACT.RE_ALL,          - 
       REGEXP_MATCH_PARAMETER  => 'i'); 
PL/SQL procedure successfully completed. 
After modifying the policy, we can select the data from the same table and you can see the
following output:
SQL> select * from advaitd.t1; 
     COL1 COL2                 COL3 
---------- -------------------- --------- 
        0 [hidden]yahoo.com    31-JUL-16 
Note that the value in col2 is now changed and replaced by a regular expression redaction
policy.
I was selecting the data from the preceding table using the username, DEO. I can exempt the DEO
user from this policy by using the EXPRESSION parameter. So, in the EXPRESSION parameter,
we can specify if the username != DEO. In that case, the policy is applicable to all users except
the DEO user.
The following is an example of this.
To modify the policy to change the EXPRESSION parameter, the ACTION parameter should be
DBMS_REDACT.MODIFY_EXPRESSION:

exec DBMS_REDACT.ALTER_POLICY( - 
       OBJECT_SCHEMA           =>'ADVAITD',                      - 
       OBJECT_NAME             =>'T1',                           -  
       POLICY_NAME             =>'TEST_POLICY',                  -  
       ACTION                  => DBMS_REDACT.MODIFY_EXPRESSION, - 
       EXPRESSION              => 'SYS_CONTEXT(''USERENV'',''SESSION_USER'') = 
''APP_USER'''); 
Now, if the DEO user tries to select from the table, he should be able to see the complete data:
SQL> select * from advaitd.t1; 
          COL1 COL2                 COL3 
--------------- -------------------- --------- 
  320934301232 advaitdeo@yahoo.com  31-JUL-16 
Exempting a user from the policy, exempts the user from all columns.
The action column can have the following values depending on the action you want to
perform on the exiting policy:
Add or modify a column: ACTION => DBMS_REDACT.MODIFY_COLUMN
Change the policy expression: ACTION=>DBMS_REDACT.MODIFY_EXPRESSION
Change the description of the policy: ACTION =>
DBMS_REDACT.SET_POLICY_DESCRIPTION
Change the description of the column: ACTION =>
DBMS_REDACT.SET_COLUMN_DESCRIPTION
Drop a column: ACTION => DBMS_REDACT.DROP_COLUMN

How Oracle data redaction affects tables and
views
Oracle data redaction policy applies to the tables and views that are created on the table. It is
also inherited by the entire chain of views that are created from the table. However, if one of
the views in the chain has a different policy further views down the chain will inherit the
policy that it see on the immediate parent.
For example, if we create a chain of three views on a table (t1) so that view V1 is on table T1,
view V2 is on V1, and view V3 is on V2 and we have a redaction policy on table t1 (col1,
col2), this redaction policy is applicable to all the views, as shown in the following code:
V3  ---------------> V2  ---------------> V1 ---------------> T1  
                                                           (col1, col2) 
-----------------------------------------------------------------------
T1(col1, col2)      T1(col1, col2)       T1(col1, col2)       T1(col1, col2) 
But if we create another policy on view V1 (col2), then view V2 and V3 will have a redaction
policy for col1 coming from table t1, but the redaction policy for column col2 will come
from view V1 and not from table T1:
 V3  ---------------> V2  ---------------> V1 ---------------> T1  
                                           (col2)     (col1, col2) 
 ----------------------------------------------------------------------
 T1(col1) V1(col2)    T1(col1) V1(col2)   T1(col1) V1(col2)      T1(col1, 
col2) 

Altering the default full data redaction value
As we have seen in the preceding section, the default value for FULL redaction depends on the
data type of the column. For a character data type, the value is a blank space, for a number it's
0, and for data it's 01-Jan-2001. We can change these values using the
UPDATE_FULL_REDACTION_VALUES procedure.
We can set default values for various data types, as shown in the definition of the procedure:
PROCEDURE UPDATE_FULL_REDACTION_VALUES 
 Argument Name                  Type                    In/Out Default? 
 ------------------------------ ----------------------- ------ -------- 
 NUMBER_VAL                     NUMBER                  IN     DEFAULT 
 BINFLOAT_VAL                   BINARY_FLOAT            IN     DEFAULT 
 BINDOUBLE_VAL                  BINARY_DOUBLE           IN     DEFAULT 
 CHAR_VAL                       CHAR                    IN     DEFAULT 
 VARCHAR_VAL                    VARCHAR2                IN     DEFAULT 
 NCHAR_VAL                      NCHAR                   IN     DEFAULT 
 NVARCHAR_VAL                   NVARCHAR2               IN     DEFAULT 
 DATE_VAL                       DATE                    IN     DEFAULT 
 TS_VAL                         TIMESTAMP               IN     DEFAULT 
 TSWTZ_VAL                      TIMESTAMP WITH TIME ZONE IN     DEFAULT 
 BLOB_VAL                       BLOB                    IN     DEFAULT 
 CLOB_VAL                       CLOB                    IN     DEFAULT 
 NCLOB_VAL                      NCLOB                   IN     DEFAULT 
For example, to set the default value for the number data type to a -1, you can execute the
procedure as follows:
SQL> exec DBMS_REDACT.UPDATE_FULL_REDACTION_VALUES(NUMBER_VAL => -1); 
 
PL/SQL procedure successfully completed. 

Restrictions on data redaction
The following are some of the restrictions on the data you can redact:
You can't redact any objects owned by the users SYS and SYSTEM
You can't redact columns of specific data types
You can't redact virtual columns
You can apply the same data redaction policy to multiple columns with the help of the
DBMS_REDACT.ALTER_POLICY procedure (you must specify the ACTION parameter to
action => DBMS_REDACT.ADD_COLUMN)
You can define only one policy on a table or view

Enabling and disabling data redaction policies
You can use the DBMS_REDACT.ENABLE_POLICY procedure to enable a specific policy:
PROCEDURE ENABLE_POLICY 
 Argument Name                  Type                    In/Out Default? 
 ------------------------------ ----------------------- ------ -------- 
 OBJECT_SCHEMA                  VARCHAR2                IN     DEFAULT 
 OBJECT_NAME                    VARCHAR2                IN 
 POLICY_NAME                    VARCHAR2                IN 
For example:
SQL> exec dbms_redact.enable_policy(OBJECT_SCHEMA => 'ADVAITD', OBJECT_NAME => 
'T1',POLICY_NAME => 'TEST_POLICY'); 
 
PL/SQL procedure successfully completed. 
Similarly, you can disable the policy using the DBMS_REDACT.DISABLE_POLICY procedure with
the same input arguments:
SQL> exec dbms_redact.disable_policy(OBJECT_SCHEMA => 'ADVAITD', OBJECT_NAME => 
'T1',POLICY_NAME => 'TEST_POLICY'); 
 
PL/SQL procedure successfully completed. 

Exempting users from data redaction
The best way to exempt a user from a redaction policy is to define the EXPRESSION parameter
for that policy so that the user will be exempted. SYS and SYSTEM users are by default
exempted from all policies. Any user who has DBA privileges is also exempted from
redaction policies.
Oracle 12c has also introduced a new system privilege, EXEMPT REDACTION POLICY.
Whichever user gets this system privilege gets exempted from all redaction policies.
Note that the EXPORT_FULL_DATABASE role has the EXEMPT REDACTION POLICY system privilege
included. So any user that has this role will be exempt from all redaction policies.
To prevent unauthorized access to sensitive data, a DBA should restrict the following
Granting execute privileges to the DBMS_REDACT package. Remember that a user having
execute privilege on the DBMS_REDACT package can alter any policy irrespective of
whether he has a privilege on the underlying table. So a user with an execute privilege on
the DBMS_REDACT package can exempt himself from all redaction policies.
Granting an EXEMPT REDACTION POLICY system privilege. Any user who has this
privilege will be exempted from all redaction policies that are not desirable for a
database having sensitive data.

Viewing data redaction policies details
We have two main views that provide all the information about redaction policies and
columns on which policies are created.

REDACTION_POLICIES
You can view all the data redaction policies in one place, and whether they're enabled, by
querying the REDACTION_POLICIES view:
SQL> desc REDACTION_POLICIES 
Name                                      Null?    Type 
----------------------------------------- -------- -------------------------
--- 
OBJECT_OWNER                              NOT NULL VARCHAR2(128) 
OBJECT_NAME                               NOT NULL VARCHAR2(128) 
POLICY_NAME                               NOT NULL VARCHAR2(128) 
EXPRESSION                                NOT NULL VARCHAR2(4000) 
ENABLE                                             VARCHAR2(7) 
POLICY_DESCRIPTION                                 VARCHAR2(4000) 
SQL> select * from REDACTION_POLICIES;
SQL> select object_name, policy_name, expression from REDACTION_POLICIES;
OBJECT_NAM POLICY_NAME   EXPRESSION                   
---------- ------------- ------------------------------
T1         TEST_POLICY   SYS_CONTEXT('USERENV',       
                        'SESSION_USER') =  'APP_USER'

REDACTION_COLUMNS
You can view all the redacted columns in a database and the type of redaction function by
querying the new view, REDACTION_COLUMNS:
SQL> desc REDACTION_COLUMNS 
Name                                      Null?    Type 
----------------------------------------- -------- -------------------------
--- 
OBJECT_OWNER                              NOT NULL VARCHAR2(128) 
OBJECT_NAME                               NOT NULL VARCHAR2(128) 
COLUMN_NAME                               NOT NULL VARCHAR2(128) 
FUNCTION_TYPE                                      VARCHAR2(27) 
FUNCTION_PARAMETERS                                VARCHAR2(1000) 
REGEXP_PATTERN                                     VARCHAR2(512) 
REGEXP_REPLACE_STRING                              VARCHAR2(4000) 
REGEXP_POSITION                                    NUMBER 
REGEXP_OCCURRENCE                                  NUMBER 
REGEXP_MATCH_PARAMETER                             VARCHAR2(10) 
COLUMN_DESCRIPTION                                 VARCHAR2(4000) 
Since we have created a policy on two columns, we should see two rows for table T1:
SQL> select OBJECT_OWNER, OBJECT_NAME, COLUMN_NAME, FUNCTION_TYPE, 
REGEXP_PATTERN, REGEXP_REPLACE_STRING, REGEXP_POSITION, REGEXP_OCCURRENCE from 
REDACTION_COLUMNS;
SQL> select OBJECT_NAME, COLUMN_NAME, FUNCTION_TYPE, REGEXP_PATTERN, 
REGEXP_REPLACE_STRING from REDACTION_COLUMNS;
COLUMN_NAM FUNCTION_TYPE      REGEXP_PATTERN       REGEXP_REPLA
---------- ------------------ -------------------- ------------
COL1       FULL REDACTION                                             
COL2       REGEXP REDACTION   (.+)@(.+\.[A-Za-z]{2 [hidden]\2         

Summary
In this chapter, we learned about new unified auditing and how it's different to traditional
auditing. We have learned how we can implement unified auditing, create auditing policies,
and view auditing data. We also looked at altering auditing policies after we create them.
We then saw new privileges introduced in Oracle 12c—SYSBACKUP, SYSDG, and SYSKM. We also
saw how we can create a new password file and include these privileges in the password file.
Next, we saw a new privilege analysis feature, which helps us in analyzing used and unused
privileges assigned to the users. We then saw a new type of privilege called INHERIT
PRIVILEGE that is used in the PL/SQL procedure.
We then learned of another new feature of Oracle 12c called data redaction. We saw different
types of data redaction policies. We have learned how we can create and modify a data
redaction policy. We also saw altering default values of the FULL data redaction policy. Finally,
we saw a few restrictions on data redaction and how we can exempt users from data redaction.
We concluded this chapter by looking into metadata views, which provide us with information
about data redaction policies.
In the next chapter, we are going to explore the new features of performance tuning in Oracle
12c.

Chapter 6. Database Tuning and
Troubleshooting
Performance tuning is a continuous process, and as we move towards new releases, the
complexity will increase. To counter this, Oracle has provided lots of new enhancements and
features that will help us make our databases run better.
In this chapter, we are going to cover the following topics:
Real-time database operation monitoring
SQL tuning
Enhanced features of statics gathering
Adaptive SQL plan management

Real-time database operation monitoring
Oracle 11g introduced a new feature called real-time SQL monitoring. This feature enabled us
to monitor the execution of SQL at runtime. This was a very useful feature, and it enabled
DBA to monitor currently running SQL and understand where exactly the SQL is spending
time. It gave an accurate breakdown of time and resources spent at each step of the SQL.
Using this feature, it was easy to pinpoint any steps in the SQL execution that are expensive
and take the most time/resources so that DBA can modify the plan to exclude or simplify such
inefficient steps.
While real-time SQL monitoring was a great feature, Oracle 12c takes this feature further by
introducing real-time operation monitoring.
An operation is one or more SQL or PL/SQL statements that are running together in a single
session or multiple concurrent sessions.
An operation is a superset of SQL in that many SQLs that are combined together become an
operation. The real-time SQL monitoring feature of Oracle 11g is a subset of the operations
monitoring feature in Oracle 12c.
For example, say that we have a load job where we are trying to load the data into multiple
tables. This will need multiple load/SQL statements combined into a single load operation.
Then say that, suddenly, this job starts taking more time to complete. Real-time operation
monitoring will monitor the entire operation rather than individual load statements. We can
easily identify which parts of the entire operation started taking more time.
Another example is a page load that is taking time, with many sections on that page executing
many SQLs. You can monitor the operation of the entire page and identify which parts of the
whole operation are taking the most time to complete.

Database operation
A database operation is a combination of one or more SQL and PL/SQL statements that are
running in single or multiple sessions. We have two types of operations - simple and
composite.
In the case of simple operations, we have a single SQL or single PL/SQL statement running in
a single session. This is very similar to real-time SQL monitoring where we have only single
SQLs running in single sessions.
Composite operations consist of multiple SQL and PL/SQL statements running at any given
time. We can further classify them based on the number of sessions they are using at any point
in time as follows:
Single session: In this case, all SQL and PL/SQL statements that belong to an operation
execute in a single session. The SESSION_ID remains the same from the beginning till the
end.
Multiple sessions: In this case, SQL and PL/SQL statements are executed by multiple
sessions, but only one session will be active at any one time. This happens when each
sub-operation in an operation makes a new connection to the database.
Concurrent sessions: This is usually used in the case of an ETL load job where we are
trying to load the data in parallel using concurrent sessions.

Identifying database operations
You identify a database operation with the help of two pieces of information:
Database operation name: This is a unique name that the user has to provide.
Database operation execution ID: This is a unique execution ID, which can be provided
by the user. The database can also generate a unique ID if one is not provided.
In the case of real-time SQL monitoring, we can identify a particular execution of an SQL ID
based on SQL_ID and SQL_EXEC_ID, which are unique for every SQL. We can also use
SQL_EXEC_START, which shows the start time of the SQL.
Similarly, we have the operation name (DBOP_NAME) and the operation execution ID
(DBOP_EID), which uniquely identify the operation in the database. However, in this case, the
operation name is provided by the user executing the operation. The user should also provide
the beginning and end of the operation. This is called the bracketing of operation. Usually,
the end of the operation marks the beginning of a new operation unless specified explicitly.
There are two ways to name a database operation:
Use the DBMS_SQL_MONITOR.BEGIN_OPERATION function at the beginning of the operation
and the DBMS_SQL_MONITOR.END_OPERATION procedure at the end of the operation.
Set a tag (used for Java and OCI (Oracle Call Interface) clients). Tags can be set in OCI
clients using OCI call OCIAttrSet and OCIAppCtxSet. In java applications, we can use java
call SetClientInfo to set tags, or set the OS environment variable ORA_DBOP.
So the DB operation should be run in the following way:
Using DBMS_SQL_MONITOR provides an explicit beginning and end of the database operation,
but in the case of OCI calls and java calls, there is not an explicit way to end the operation. In
that case, the user should set the tag to NULL to represent the end of the operation.
The following figure shows how database operations look in multiple sessions:

So, in the preceding figure we can see that Session 1 started with Operation A, and, since this
was the first session to initiate the operation named A, its exec ID is 1. After some time,
Session 2 started another operation B, and, since it is the first session to start this operation,
its exec ID is 1. After some time, Session 4 started Operation B, and, since it is the second
execution, the exec ID for this operation is 2. Later, Session 3 started Operation A, and, since
this is its second execution, its exec ID is 2. Session 1, after finishing Operation A, started
Operation B, and, since it is the third execution of Operation B, its exec ID is 3.

Enabling monitoring of database operations
The following are the various levels where you can enable database monitoring:
System level: Real-time operation monitoring is enabled at system level by default,
provided the following conditions are met:
STATISTICS_LEVEL is set to TYPICAL or ALL. If STATISTICS_LEVEL is set to BASIC,
monitoring will not be enabled.
CONTROL_MANAGEMENT_PACK_ACCESS is set to DIAGNOSTIC+TUNING. This is because
SQL monitoring is a feature of tuning. By default, an SQL will be monitored if the
SQL is run in parallel, or if the SQL consumes more than five seconds of CPU time
or IO time in a single execution.
Statement level: You can force the monitoring of an SQL statement by using MONITOR
hint in SQL. An SQL might not get monitored automatically if it consumes less than five
seconds of CPU or IO time. If you want to continue to monitor the SQL statement, you
can do so using the MONITOR hint in the SQL as follows:
SELECT /*+ MONITOR */ .... 
Database operation level: You can enable database operation monitoring by using the
FORCED_TRACKING parameter in the DBMS_SQL_MONITOR.BEGIN_OPERATION function. We
have multiple parameters defined in this function, and one of the parameters is
FORCED_TRACKING. When we set YES to this parameter, the database operation will be
tracked by the database. Otherwise, if this parameter has a default value of NO, the
database operation will be tracked if it consumes enough resources in the database, or if
it's running in parallel.
DBMS_SQL_MONITOR.BEGIN_OPERATION ( 
   dbop_name       IN VARCHAR2, 
   dbop_eid        IN NUMBER   := NULL, 
   forced_tracking IN VARCHAR2 := NO_FORCE_TRACKING, 
   attribute_list  IN VARCHAR2 := NULL) 
  RETURN NUMBER; 
Let us look at an example to see how we can use BEING and END database operations.
For every operation that we execute, we need to assign an operation name and execution ID to
that operation. If you do not provide an execution ID value, Oracle will auto-generate for you.
These two values will uniquely identify database operations.
We can use operation names to monitor the operations, however, if we have multiple sessions
spawned by multiple users running the same operation, we can distinguish each operation
using an execution ID. One more thing to note is that that, even though the execution ID is
unique for each operation among multiple sessions spawned by different users, this execution
ID could be the same if the same operation spawns multiple sessions (in the case of parallel
operations).
Also, it is possible that different executions of the same operation can result in different SQL

or PL/SQL codes being executed. This could happen because of the internal code logic, which
spawns different SQLs based on different conditions.
For setting the operation name, Oracle recommends that you follow the format of
<component>.<subcomponent>.<operation_name> when naming database operations. You do
this in order to maintain standards and to avoid collision with similarly named operations,
because all the operation names you assign share the same namespace. For all operations that
happen inside the database, we can use ORA as a component.
For example:
SQL> var dbop_eid number; 
SQL> exec :dbop_eid := 
dbms_sql_monitor.begin_operation('ORA.DEMO_USER.SELECT1',forced_tracking=>'Y'); 
PL/SQL procedure successfully completed. 
SQL> select * from demo_user.sales where year = 2001; 
SQL> insert into demo_user.accounts select * from ... 
368640 rows created. 
SQL> select * from demo_user.products where year = 
SQL> select * from demo_user.sales where product = 
SQL> exec dbms_sql_monitor.end_operation('ORA.MV.REFRESH',:dbop_eid); 
PL/SQL procedure successfully completed. 
As you can see in the preceding example, we are starting a database operation called
ORA.DEMO_USER.SELECT1. We have a bunch of selects and DMLs to be performed as part of this
operation and at the end we end the operation.

Monitoring progress of database operations
You have multiple ways to monitor database operations, including the following techniques:
Oracle EM Cloud Control or Oracle EM Database Express
Use V$ Views in SQL*Plus
Use functions in DBMS_SQL_MONITOR to generate reports
Let us look at each of these options in more detail.
Monitoring operations using Oracle EM Database Express
As we have already gone through EM express in Chapter 1, Getting Started with Oracle 12c
we can see how EM express can be used for monitoring operations. On Oracle 12c EM
Database Express, you can go into the Performance tab and click on Performance Hub. After
clicking Performance Hub, you can move the slider to the area where you faced a
performance issue. In the following tab, in the Real Time - Last Hour area, click on the
Monitored SQL tab as shown in the following screenshot.
In the Type dropdown, you can select Database operation and the following table will
refresh and provide details about database operations in the database. This type dropdown is
available to filter the type of operations that we want to see (SQL, PL/SQL, Database
Operation).
A database operation is represented by an asterisk (*) and SQL statements are represented by
different icons, as shown in the following screenshot:

You can also see database time spend, start time and end time for complete database
operations as well as for individual SQL as shown below:
Monitoring operations using views
You can also use V$ views to monitor database operations and V$SQL_MONITOR and
V$ACTIVE_SESSION_HISTORY to monitor database operations. Oracle 12c has added two new
columns to these two views: DBOP_NAME and DBOP_EXEC_ID.
Let us go through each one of them:
V$SQL_MONITOR: The following SQL provides all the database operations in the database,
completed as well as executing
SQL> select dbop_name, dbop_exec_id, status from v$sql_monitor where 
dbop_name is not null; 
 
DBOP_NAME                      DBOP_EXEC_ID STATUS 
------------------------------ ------------ ------------------- 
ORA.DEMO_USER.SELECT1                     2 EXECUTING 
ORA.MV.REFRESH                            1 DONE 
The preceding SQL provides high-level information about top SQL statements in the database
operation. Each monitored SQL has an entry in this view. For a database operation and
DBOP_EXEC_ID, we might see multiple records, each one belonging to different SQL_ID. The

primary key for V$SQL_MONITOR is a combination of DBOP_NAME, DBOP_EXEC_ID, and SQL_ID.
V$ACTIVE_SESSION_HISTORY: You can also check session level details in the
V$ACTIVE_SESSION_HISTORY view. We will see many records in this view, as we have
samples taken every second.
SQL> select sample_time,  session_id, dbop_name, dbop_exec_id from 
v$active_session_history where dbop_name is not null order by 1;
SAMPLE_TIME             SESSION_ID DBOP_NAME         DBOP_
----------------------- ---------- ----------------- -----
21-JUN-16 11.29.10 AM       20 ORA.MV.REFRESH         1
21-JUN-16 01.21.12 PM      266 ORA.DEMO_USER.SELECT1  2
21-JUN-16 01.21.13 PM      266 ORA.DEMO_USER.SELECT1  2
21-JUN-16 01.21.14 PM      266 ORA.DEMO_USER.SELECT1  2
21-JUN-16 01.21.15 PM      266 ORA.DEMO_USER.SELECT1  2
21-JUN-16 01.21.16 PM      266 ORA.DEMO_USER.SELECT1  2
21-JUN-16 01.21.29 PM      266 ORA.DEMO_USER.SELECT1  2
21-JUN-16 01.21.42 PM      266 ORA.DEMO_USER.SELECT1  2
You can also query DBA_HIST_ACTIVE_SESS_HISTORY to see whether the information in
V$ACTIVE_SESSION_HISTORY is lost. But only one of every ten samples goes into the
DBA_HIST_ACTIVE_SESS_HISTORY view, so the data is less accurate. We don't have any
DBA_HIST counterpart for V$SQL_MONITOR.
V$SQL_MONITOR_SESSTAT: If you want to measure the statistics for an SQL, you can check
out this view. This view has session-level statistics so you can check the required
statistics for all sessions involved in the database operation.
V$SQL_PLAN_MONITOR: While V$SQL_MONITOR shows one record for every SQL_ID being
run by a session, this view shows every step in an SQL plan and complete statistics for
just about every step in the plan. In fact, the reports generated for real-time SQL
monitoring use this view to report statistics at each step.
Monitoring operations using reports
We can also use functions in the DBMS_SQL_MONITOR package to generate a report of the
database operations being monitored. We can generate the report in either XML or CLOB
format. CLOB is a more readable option. The following are the different functions available
to generate the report:
REPORT_SQL_MONITOR_LIST_XML: Generates a list-like report for database operations in
XML format
REPORT_SQL_MONITOR_XML: Generates a detailed report for database operations in XML
format
REPORT SQL_MONITOR_LIST: Generates a list-like report for database operations in CLOB
format
REPORT_SQL_MONITOR: Generates a detailed report for database operations in CLOB
format
Each of these functions need various optional input parameters, such as SQL_ID, DBOP_NAME,

and so on.
For example:
Select DBMS_SQL_MONITOR.REPORT_SQL_MONITOR_LIST('ORA.MV.REFRESH') from dual; 
All these generated reports are stored and persisted in AWR tables. The following tables store
each of the generated reports:
DBA_HIST_REPORT
DBA_HIST_REPOTR_DETAILS
DBA_HIST_ACTIVE_SESS_HISTORY
You can refer to the previous tables in the future to get the details of the report instead of
generating the report again.

SQL tuning
Oracle 12c has some exciting features regarding SQL tuning. It gives us new enhancements to
its optimizers concerning plan execution, SQL plan management, and dynamic sampling. We
will look into these features in detail. Later, we will also look into the new enhanced statistics
gathering feature.
The following are the new SQL tuning features introduced in Oracle 12c:
Adaptive query optimization: This feature has the following technique:
Adaptive execution plans
Adaptive statistics: This feature has the following two techniques:
Dynamic statistics
SQL plan directives
Before we go into the details of each of these new features, let us rewind and check what we
used to have in Oracle 11g.
Behavior in Oracle 11g R1:
Whenever an SQL is executed for the first time, an optimizer will generate an execution plan
for the SQL based on the statistics available for the different objects used in the plan. If
statistics are not available, or if the optimizer thinks that the existing statistics are of low
quality, or if we have complex predicates used in the SQL for which the optimizer cannot
estimate the cardinality, the optimizer may choose to use dynamic sampling for those tables.
So, based on the statistics' values, the optimizer generates the plan and executes the SQL. But,
there are two problems with this approach:
Statistics generated by dynamic sampling may not be of good quality as they are
generated in a limited time and are based on a limited sample size. A trade-off is made to
minimize the impact and try to approach a higher level of accuracy.
A plan generated using this approach may not be accurate, as the estimated cardinality
may differ a lot from the actual cardinality. The next time the query executes, it goes for
soft parsing and picks the same plan.
Behavior in Oracle 11g R2:
To overcome these drawbacks, Oracle enhanced the dynamic sampling feature further in
Oracle 11g Release 2. In the 11.2 release, Oracle will automatically enable dynamic sample
when the query is run if statistics are missing, or if the optimizer thinks that current statistics
are not up to the mark. The optimizer also decides the level of the dynamic sample, provided
the user does not set the non-default value of the OPTIMIZER_DYNAMIC_SAMPLING
parameter (default value is 2). So, if this parameter has a default value in Oracle 11g R2, the
optimizer will decide when to spawn dynamic sampling in a query and at what level to spawn
the dynamic sample.

Oracle also introduced a new feature in Oracle 11g R2 called cardinality feedback. This was
in order to further improve the performance of SQLs, which are executed repeatedly and for
which the optimizer does not have the correct cardinality, perhaps because of missing
statistics, or complex predicate conditions, or because of some other reason. In such cases,
cardinality feedback is very useful.
The way cardinality feedback works is, during the first execution, the plan for the SQL is
generated using the traditional method without using cardinality feedback. However, during
the optimization stage of the first execution, the optimizer notes down all the estimates that are
of low quality (due to missing statistics, complex predicates, or some other reason) and
monitoring is enabled for the cursor that is created. If this monitoring is enabled during the
optimization stage, then, at the end of the first execution, some cardinality estimates in the
plan are compared with the actual estimates to understand how significant the variation is.
If the estimates vary significantly, then the actual estimates for such predicates are stored
together with the cursor, and these estimates are used directly for the next execution instead of
being discarded and calculated again. So when the query executes the next time, it will be
optimized again (hard parse will happen), but this time it will use the actual statistics or
predicates that were saved in the first execution, and the optimizer will come up with a better
plan.
But even with these improvements, there are drawbacks:
With cardinality feedback, any missing cardinality or correct estimates are available for
the next execution only and not for the first execution. So the first execution may be bad.
The dynamic sample improvements (that is, the optimizer deciding whether dynamic
sampling should be used and the level of the dynamic sampling) are only applicable to
parallel queries. It is not applicable to queries that aren't running in parallel.
Dynamic sampling does not include joins and groups by columns.
Oracle 12c has provided new improvements, which eliminate the drawbacks of Oracle 11g
R2.

Adaptive execution plans - dynamic plans
The Oracle optimizer chooses the best execution plan for a query based on all the information
available to it. Sometimes, the optimizer may not have sufficient statistics or good quality
statistics available to it, making it difficult to generate optimal plans. In Oracle 12c, the
optimizer has been enhanced to adapt a poorly performing execution plan at run time and
prevent a poor plan from being chosen on subsequent executions. An adaptive plan can
change the execution plan in the current run when the optimizer estimates prove to be wrong.
This is made possible by collecting the statistics at critical places in a plan when the query
starts executing. A query is internally split into multiple steps, and the optimizer generates
multiple sub-plans for every step. Based on the statistics collected at critical points, the
optimizer compares the collected statistics with estimated cardinality. If the optimizer finds a
deviation in statistics beyond the set threshold, it picks a different sub-plan for those steps.
This improves the ability of the query-processing engine to generate better execution plans.
What happens in adaptive plan execution?
In Oracle 12c, the optimizer generates dynamic plans. A dynamic plan is an execution plan
that has many built-in sub-plans. A sub-plan is a portion of a plan that the optimizer can switch
to as an alternative at run time. When the first execution starts, the optimizer observes
statistics at various critical stages in the plan. An optimizer makes a final decision about the
sub-plan based on observations made during the execution up to this point.
Going deeper into the logic for the dynamic plan, the optimizer actually places the statistics
collected at various critical stages in the plan. These critical stages are the places in the plan
where the optimizer has to join two tables or where the optimizer has to decide upon the
optimal degree of parallelism. During the execution of the plan, the statistics collector buffers
a portion of the rows. The portion of the plan preceding the statistics collector can have
alternative sub-plans, each of which is valid for the subset of possible values returned by the
collector. This means that each of the sub-plans has a different threshold value. Based on the
data returned by the statistics collector, a sub-plan is chosen which falls within the required
threshold.
For example, an optimizer can insert code to collect statistics before joining two tables,
during the query plan building phase. It can have multiple sub-plans based on the type of join
it can perform between two tables. If the number of rows returned by the statistics collector on
the first table is less than the threshold value, then the optimizer might go with the sub-plan
containing the nested loop join. But if the number of rows returned by the statistics collector
is above the threshold values, then the optimizer might choose the second sub-plan to go with
the hash join.
After the optimizer chooses a sub-plan, buffering is disabled and the statistics collector stops
collecting rows and passes them through instead. On subsequent executions of the same SQL,
the optimizer stops buffering and chooses the same plan instead.

With dynamic plans, the optimizer adapts to poor plan choices and correct decisions are made
at various steps during runtime. Instead of using predetermined execution plans, adaptive
plans enable the optimizer to postpone the final plan decision until statement execution time.
Consider the following simple query:
SELECT  a.sales_rep, b.product, sum(a.amt) 
FROM    sales a, product b 
WHERE   a.product_id = b.product_id 
GROUP BY a.sales_rep, b.product 
When the query plan was built initially, the optimizer will put the statistics collector before
making the join. So it will scan the first table (SALES) and, based on the number of rows
returned, it might make a decision to select the correct type of join.
The following figure shows the statistics collector being put in at various stages:

Enabling adaptive execution plans
To enable adaptive execution plans, you need to fulfill the following conditions:
optimizer_features_enable should be set to the minimum of 12.1.0.1
optimizer_adapive_reporting_only should be set to FALSE (default)
If you set the OPTIMIZER_ADAPTIVE_REPORTING_ONLY parameter to TRUE, the adaptive
execution plan feature runs in the reporting-only mode - it collects the information for
adaptive optimization, but doesn't actually use this information to change the execution plans.
You can find out if the final plan chosen was the default plan by looking at the column
IS_RESOLVED_ADAPTIVE_PLAN in the view V$SQL. Join methods and parallel distribution
methods are two areas where adaptive plans have been implemented by Oracle 12c.
Adaptive execution plans and join methods
Here is an example that shows how the adaptive execution plan will look. Instead of
simulating a new query in the database and checking if the adaptive plan has worked, I used
one of the queries in the database that is already using the adaptive plan.
You can get many such queries if you check V$SQL with is_resolved_adaptive_plan = 'Y'.
The following queries will list all SQLs that are going for adaptive plans.
Select sql_id from v$sql where is_resolved_adaptive_plan = 'Y'; 
While evaluating the plan, the optimizer uses the cardinality of the join to select the superior
join method. The statistics collector starts buffering the rows from the first table, and if the
number of rows exceeds the threshold value, the optimizer chooses to go for a hash join. But
if the rows are less than the threshold value, the optimizer goes for a nested loop join. The
following is the resulting plan:
SQL> SELECT * FROM 
TABLE(DBMS_XPLAN.display_cursor(sql_id=>'dhpn35zupm8ck',cursor_child_no=>0; 
Plan hash value: 3790265618
------------------------------------------------------------------
| Id  | Operation                      | Name    | Rows  | Bytes |
------------------------------------------------------------------
|   0 | SELECT STATEMENT               |         |       |       |
|   1 |  SORT ORDER BY                 |         |     1 |    73 |
|   2 |   NESTED LOOPS                 |         |     1 |    73 |
|   3 |    NESTED LOOPS                |         |   151 |    73 |
|*  4 |     TABLE ACCESS BY INDEX ROWID| OBJ$    |   151 |  7701 |
|*  5 |      INDEX FULL SCAN           | I_OBJ3  |     1 |       |
|*  6 |     INDEX UNIQUE SCAN          | I_TYPE2 |     1 |       |
|*  7 |    TABLE ACCESS BY INDEX ROWID | TYPE$   |     1 |    22 |
------------------------------------------------------------------
Predicate Information (identified by operation id): 

--------------------------------------------------- 
  4 - filter(SYSDATE@!-"O"."CTIME">.0007) 
  5 - filter("O"."OID$" IS NOT NULL) 
  6 - access("O"."OID$"="T"."TVOID") 
  7 - filter(BITAND("T"."PROPERTIES",8388608)=8388608) 
Note 
----- 
  - this is an adaptive plan 
If we check this plan, we can see the notes section, and it tells us that this is an adaptive plan. It
tells us that the optimizer must have started with some default plan based on the statistics in the
tables and indexes, and during run time execution it changed the join method for a sub-plan.
You can actually check which step optimizer has changed and at what point it has collected the
statistics. You can display this using the new format of DBMS_XPLAN.DISPLAY_CURSOR -
format => 'adaptive', resulting in the following:
DEO>SELECT * FROM 
TABLE(DBMS_XPLAN.display_cursor(sql_id=>'dhpn35zupm8ck',cursor_child_no=>0,form
at=>'adaptive')); 
Plan hash value: 3790265618
----------------------------------------------------------------------
|   Id  | Operation                        | Name    | Rows  | Bytes |
----------------------------------------------------------------------
|     0 | SELECT STATEMENT                 |         |       |       |
|     1 |  SORT ORDER BY                   |         |     1 |    73 |
|- *  2 |   HASH JOIN                      |         |     1 |    73 |
|     3 |    NESTED LOOPS                  |         |     1 |    73 |
|     4 |     NESTED LOOPS                 |         |   151 |    73 |
|-    5 |      STATISTICS COLLECTOR        |         |       |       |
|  *  6 |       TABLE ACCESS BY INDEX ROWID| OBJ$    |   151 |  7701 |
|  *  7 |        INDEX FULL SCAN           | I_OBJ3  |     1 |       |
|  *  8 |      INDEX UNIQUE SCAN           | I_TYPE2 |     1 |       |
|  *  9 |     TABLE ACCESS BY INDEX ROWID  | TYPE$   |     1 |    22 |
|- * 10 |    TABLE ACCESS FULL             | TYPE$   |     1 |    22 |
----------------------------------------------------------------------
Predicate Information (identified by operation id): 
--------------------------------------------------- 
  2 - access("O"."OID$"="T"."TVOID") 
  6 - filter(SYSDATE@!-"O"."CTIME">.0007) 
  7 - filter("O"."OID$" IS NOT NULL) 
  8 - access("O"."OID$"="T"."TVOID") 
  9 - filter(BITAND("T"."PROPERTIES",8388608)=8388608) 
 10 - filter(BITAND("T"."PROPERTIES",8388608)=8388608) 
Note 
----- 
  - this is an adaptive plan (rows marked '-' are inactive) 
In this output, you can see that it has given three extra steps: Steps 2, 5, and 10. But these steps

were present in the original plan when the query started. Initially, the optimizer generated a
plan with a hash join on the outer tables. During runtime, the optimizer started collecting rows
returned from the OBJ$ table (Step 6), as we can see the STATISTICS COLLECTOR at step 5.
Once the rows are buffered, the optimizer came to know that the number of rows returned by
the OBJ$ table are less than the threshold and so it can go for a nested loop join instead of a
hash join. The rows indicated by - in the beginning belong to the original plan, and they are
removed from the final plan. Instead of those records, we have three new steps added: Steps 3,
8, and 9. Step 10 of the full table scan on the TYPE$ table is changed to an index unique scan
of I_TYPE2, followed by the "table access by index rowid" at Step 9.
Adaptive plans and parallel distribution methods
Adaptive plans are also useful in adapting from bad distributing methods when running the
SQL in parallel. Parallel execution distributes the data to perform parallel sorts, joins and
aggregates. Oracle optimizer has multiple ways to distribute the data. The number of rows to
be distributed determines the data distribution method, along with the number of parallel
server processes.
If many parallel server processes distribute only a few rows, the database chooses a broadcast
distribution method and sends the entire result set to all the parallel server processes. On the
other hand, if a few processes distribute many rows, the database distributes the rows equally
among the parallel server processes by choosing a hash distribution method.
In adaptive plans, the optimizer does not commit to a specific broadcast method. Instead, the
optimizer starts with an adaptive parallel data distribution technique called hybrid data
distribution. It places a statistics collector to buffer rows returned by the table. Based on the
number of rows returned, the optimizer decides the distribution method. If the rows returned
by the result are less than the threshold, the data distribution method switches to broadcast
distribution. If the rows returned by the table are more than the threshold, the data distribution
method switches to hash distribution.

Automatic re-optimization
Adaptive plans are not always feasible. In cases where a query has an inefficient join order,
adaptive plans do not support modifying the join order in the middle of the execution. Apart
from join orders, there could be many more complex situations where the Oracle optimizer
cannot make decisions during runtime. In such cases, the optimizer considers automatic re-
optimization, where the optimizer changes a plan only for subsequent executions, but not the
current execution.
There are two techniques for automatic re-optimization:
Statistics feedback
Performance feedback
Statistics feedback
In Oracle 11g R2, during query execution, the optimizer compares the estimated statistics with
the actual statistics, and if there is a significant difference between the actual and estimated
statistics, it will provide the actual cardinality as feedback to the query plan and, in subsequent
executions, the optimizer will make use of this feedback to evolve a more accurate plan. So
when the query executes for the second time, it goes for a hard parse, and a new plan will be
generated based on the cardinality feedback from a previous run. This functionality was made
available in 11g R2, and is called cardinality feedback. You can see in the notes section that
cardinality feedback is used for the statement.
In Oracle 12c, this functionality is enhanced further and introduced as statistics feedback.
Oracle 12c introduces a statistics collector at critical points in the plan. It also includes the
join statistics feature, which was not present in previous releases. Statements are continuously
monitored to verify whether the statistics fluctuate over different executions. It also works
with the adaptive cursor, sharing information in order to create different plans based on
different feedback for different sets of bind values. This improves the ability of the query
processing engine to generate better plans.
Once the statistics collector gathers statistics at critical points in the plans, the optimizer
evaluates whether there is a need for re-optimization. If the actual statistics differ from the
estimated statistics beyond the threshold value set, then the optimizer marks the plan for re-
optimization and submits all collected statistics to the optimizer. We have a new column
(IS_REOPTIMIZABLE) in V$SQL which tells us that re-optimization will trigger if SQL runs
again.
The following are the basic steps about how statistics feedback works:
The optimizer enables statistics feedback under the following conditions:
Statistics are missing
Complex filter predicates
Predicates with complex operators where it's difficult to estimate the cardinality

After the first execution, the optimizer compares the actual cardinality with the estimated
cardinality, and if the difference in cardinality is more than the threshold value, it stores
the cardinality to be used for the next execution.
These cardinality estimates are stored in the SQL plan directives, which have another
new feature in Oracle 12c. We will check what the SQL plan directives are in the next
section. For now, we can understand that actual cardinality is persisted in the SQL plan
directives and can be used by any SQL using the same predicates.
In subsequent executions, the optimizer takes into account the cardinality estimates stored
from the first execution and compiles better plans based on stored statistics values.
The following example shows a re-optimizable plan. In the notes section, we can see that
statistics feedback is used for the SQL. To simulate the statistics feedback condition, I have
changed the statistics of the tables and indexes to some false values using the following
procedures:
For tables:
exec DBMS_STATS.SET_TABLE_STATS(OWNNAME => 
'OE',TABNAME=>'ORDERS',NUMROWS=>10000,NUMBLKS=>10000); 
exec DBMS_STATS.SET_TABLE_STATS(OWNNAME => 
'OE',TABNAME=>'ORDER_ITEMS',NUMROWS=>100,NUMBLKS=>10); 
exec DBMS_STATS.SET_TABLE_STATS(OWNNAME => 
'OE',TABNAME=>'INVENTORIES',NUMROWS=>5,NUMBLKS=>1); 
exec DBMS_STATS.SET_TABLE_STATS(OWNNAME => 
'OE',TABNAME=>'PRODUCT_INFORMATION',NUMROWS=>40,NUMBLKS=>2); 
For indexes:
exec DBMS_STATS.SET_INDEX_STATS(OWNNAME => 
'OE',INDNAME=>'ORDER_PK',NUMROWS=>10,NUMLBLKS=>10); 
exec DBMS_STATS.SET_INDEX_STATS(OWNNAME => 
'OE',INDNAME=>'ORD_CUSTOMER_IX',NUMROWS=>100,NUMLBLKS=>10); 
Now, when we execute the query, we can see that the estimates are all wrong. To get the actual
cardinality, use the GATHER_PLAN_STATISTICS hint in the query and display_cursor with
format => 'ALLSTATS LAST'.
After executing, we can use the following procedure to display the plan:
select * from table(dbms_xplan.display_cursor('&SQL_ID',0,format=>'ALLSTATS 
LAST')); 
In the following plan, we can see that the cardinality estimates are all wrong and the statistics
feedback is used as shown in the Note section:
Plan hash value: 35479787
------------------------------------------------------------------
| Id  | Operation              | Name          | E-Rows | A-Rows |
------------------------------------------------------------------
|   0 | SELECT STATEMENT       |               |        |    269 |

|   1 |  NESTED LOOPS          |               |      1 |    269 |
|*  2 |   HASH JOIN            |               |      1 |    269 |
|*  3 |    TABLE ACCESS FULL   | PRODUCT_INFORM|      1 |     87 |
|   4 |    INDEX FAST FULL SCAN| ORDER_ITEMS_UK|      1 |    665 |
|*  5 |   INDEX UNIQUE SCAN    | ORDER_PK      |     95 |    269 |
------------------------------------------------------------------
Predicate Information (identified by operation id): 
--------------------------------------------------- 
  2 - access("P"."PRODUCT_ID"="OI"."PRODUCT_ID") 
  3 - filter(("MIN_PRICE"<40 AND "LIST_PRICE"<50)) 
  5 - access("O"."ORDER_ID"="OI"."ORDER_ID") 
Note 
----- 
  - this is an adaptive plan 
From v$sql, we can check whether this query is re-optimizable or not:
DEO>select sql_id, is_reoptimizable from v$sql where sql_id = 'gc0sy3nccmkmm'; 
SQL_ID        I 
------------- - 
gc0sy3nccmkmm Y 
So it looks like this query is re-optimizable. Let us run the same query again and display the
new plan. Note that for the second execution, Oracle will hard parse the query to it, and will
generate a new cursor. We need to check the plan for CHILD_NUMBER = 1.
select * from table(dbms_xplan.display_cursor('&SQL_ID',1,format=>'ALLSTATS 
LAST')); 
Note, that in the preceding command, I used child number 1 instead of 0.
The following is the new plan generated. You can see that the A-rows (actual rows) are very
close to the E-rows (estimated rows) now, and the note states statistics feedback used for
this statement:
Plan hash value: 35479787
------------------------------------------------------------------
| Id  | Operation              | Name          | E-Rows | A-Rows |
------------------------------------------------------------------
|   0 | SELECT STATEMENT       |               |        |    269 |
|   1 |  NESTED LOOPS          |               |    269 |    269 |
|*  2 |   HASH JOIN            |               |    269 |    269 |
|*  3 |    TABLE ACCESS FULL   | PRODUCT_INFORM|     87 |     87 |
|   4 |    INDEX FAST FULL SCAN| ORDER_ITEMS_UK|    665 |    665 |\
|*  5 |   INDEX UNIQUE SCAN    | ORDER_PK      |      1 |    269 |
------------------------------------------------------------------
Predicate Information (identified by operation id): 
--------------------------------------------------- 

  2 - access("P"."PRODUCT_ID"="OI"."PRODUCT_ID") 
  3 - filter(("MIN_PRICE"<40 AND "LIST_PRICE"<50)) 
  5 - access("O"."ORDER_ID"="OI"."ORDER_ID") 
Note 
----- 
  - statistics feedback used for this statement 
Performance feedback
Performance feedback is another type of re-optimization technique introduced in Oracle 12c
for tuning the degree of parallelism used in the query. Before we understand how
performance feedback works, we need to understand the parameter PARALLEL_DEGREE_POLICY.
We have multiple values for this parameter, and also a new value has been introduced in
Oracle 12c. The following are the different values for this parameter:
Manual: This is the default value of this parameter, and it disables the automatic degree
of parallelism, statement queuing, and in-memory parallel execution.
Limited: This enables the automatic degree of parallelism for only those table and
indexes whose parallelism property is set to DEFAULT. It disables statement queuing and
in-memory parallel execution.
AUTO: Setting this value enables the automatic degree of parallelism, statement queuing,
and in-memory parallel execution.
Adaptive: This value is introduced in Oracle 12c, and it's the same as AUTO, except that
additional performance feedback is enabled. If you want to use performance feedback,
you need to set PARALLEL_DEGREE_POLICY to ADAPTIVE.
Now, let's dig into performance feedback.
Performance feedback helps the optimizer in correcting the parallelism to be used for the
query. During the first execution, the degree of parallelism chosen by the optimizer will be
compared with the actual degree of parallelism used by the query. If there is a significant
variation in the degree of parallelism chosen by the optimizer, then the statement is marked
for re-parse, and the initial performance statistics are provided as input to the subsequent
execution of the statement. During the second execution, the query is hard parsed again, and
the optimizer uses the execution statistics from the first execution to come up with more
accurate degree of parallelism.

Adaptive statistics
Adaptive statistics uses the following two techniques:
Dynamic statistics
SQL plan directives
Dynamic statistics
In Oracle 11g, whenever there was a missing statistic on a table or index, or if they were low
quality statistics, the optimizer would automatically gather statistics at run time and provide
those additional statistics during the plan evaluation. This feature was named dynamic
sampling. Often, the collecting of statistics dynamically by the optimizer at query run time
proved to be better than running the query with missing statistics on key tables.
In Oracle 12c, there is a new name for dynamic sampling-dynamic statistics, and this comes
with some enhancements, as follows:
The optimizer determines when to gather dynamic statistics and at what level. In previous
releases, dynamic statistics were gathered only when missing, but in Oracle 12c, if the
optimizer thinks that statistics are stale or even of low quality, it will gather dynamic
statistics.
In earlier releases, the results of dynamic sampling were held in the memory and never
persisted. So each query was doing dynamic sampling during runtime, causing extra
processing. The results of dynamic statistics are now persisted in the SQL plan directive.
Subsequent queries can use the persistent results of dynamic statistics.
Dynamic statistics also include joins, group by, and non-parallel statements. In order,
release only tables with completely missing statistics and queries accessing table in
parallel are eligible for dynamic sampling.
Until the previous release, we had different values of dynamic sampling from 1 to 10
assigned to the OPTIMIZER_DYNAMIC_SAMPLING parameter. The default value was 2 and the
optimizer used to gather statistics whenever the stats were missing on the object. In Oracle
12c, a new value of 11 was introduced, which, when assigned, enabled dynamic statistics. The
default value of the Optimizer_Dynamic_Sampling is still 2, but if you want to enable dynamic
statistics, you can change the value of the parameter to 11.
If you set the value of the parameter to 11, the optimizer will collect the dynamic statistics
under the following conditions:
Statistics are missing: The objects may be new and might, therefore, be missing statistics.
Statistics are stale: The optimizer considers the statistics as stale if more than ten percent
of rows in a table have been modified since the last statistics-gathering operation.
Statistics are insufficient: In the case of some complex predicate or query using multiple
columns for which the optimizer is not able to estimate the cardinality accurately, the
optimizer considers those statistics as insufficient.
Sometimes complex predicate expressions can use dynamic statistics if there are no

extended statistics available.
You can disable dynamic statistics / dynamic sampling by setting the
Optimizer_Dynamic_Sampling to 0 (zero). One thing to note here is that dynamic statistics
incur a performance cost during compile time, so if you want your queries to compile
quickly, as in the case of the OLTP system, then you should not use dynamic statistics.
SQL plan directives
SQL plan directives are a new feature introduced in Oracle 12c, which help the optimizer
generate better execution plans. In basic terms, the SQL plan directives are just additional
instructions or information provided to the optimizer to help generate better execution plans.
SQL plan directives provide hints to the optimizer to gather supplemental data, such as
missing object statistics, dynamic sampling, create column groups, and so on, to generate a
better execution plan. Whenever the optimizer goes for adaptive plans or dynamic statistics, it
stores additional information in the SQL plan directive, which is useful for other SQLs using
the same predicates.
Directives are created based on query predicates or expressions, and do not associate with a
specific SQL. The same SQL plan directive can be used on multiple SQL statements using
similar predicates or expressions.
For example, let us say we have an SQL that has complex predicates and there is a relationship
that exists between these predicates. When the query is executed for the first time, the Oracle
optimizer will try to use adaptive plans or dynamic statistics to understand that there is a
relationship between the predicates, and it will also gather additional
data/statistics/information about these relationships. This valuable data will be stored as an
SQL plan directive for the related predicates. This directive will tell us that whenever these
predicates are used together, the optimizer will check for extended statistics in the column
group containing those two columns. So whenever any other SQL that uses the same
predicates runs, the optimizer will use that directive and check what additional information it
has. If the directive suggests checking for extended statistics for those predicates, the
optimizer will check whether any extended statistics exit. If they exist, the optimizer will try to
use them in generating a query plan. This way, the optimizer will be able to derive accurate
plans using SQL plan directives.
SQL plan directives are created initially in a shared pool. These directives are persisted on
disk in SYSAUX tablespace every 15 minutes. Unused directives are automatically purged
after a year of not being accessed. The following are the views that can be used to check SQL
plan directives:
DBA_SQL_PLAN_DIRECTIVE: This view displays SQL plan directives created in the
database, their status, and the reason for creating the directives.
DBA_SQL_PLAN_DIR_OBJECTS: This view displays objects and sub-objects on which the
directives are created. It also has a NOTES column which shows, in XML format, when this
directive will be used. The following code indicates that the directive should be used in

equality predicates only and on simple column predicates:
<obj_note> 
  <equality_predicates_only>YES</equality_predicates_only> 
  <simple_column_predicates_only>YES</simple_column_predicates_only> 
  <index_access_by_join_predicates>NO</index_access_by_join_predicates> 
  <filter_on_joining_object>NO</filter_on_joining_object> 
</obj_note> 
The following example shows how SQL plan directives are used:
Let us say a user submits the following SQL against order_items and product_information:
select  /*+ gather_plan_statistics */ oi.order_id, p.product_name 
from    order_items oi, product_information p 
where   p.product_id = oi.product_id 
and     list_price < 20 
and     min_price < 10 
We use a gather_plan_statistics hint to gather plan information. We want to see the
cardinality estimates and actual rows when the plan is executed.
After running the query, we can check the plan and cardinality estimates with actual rows. We
can use the following select statement with dbms_xplan.display_cursor:
select * from table(dbms_xplan.display_cursor('&SQL_ID',0,format=>'ALLSTATS 
LAST')); 
ALLSTATS LAST, shows the values of E-rows (estimated rows) and A-rows (actual rows). The
following is the output of the previous command:
Plan hash value: 3092865807
-----------------------------------------------------------------
| Id  | Operation             | Name          | E-Rows | A-Rows |
-----------------------------------------------------------------
|   0 | SELECT STATEMENT      |               |        |     63 |
|*  1 |  HASH JOIN            |               |     16 |     63 |
|*  2 |   TABLE ACCESS FULL   | PRODUCT_INFORM|     10 |     30 |
|   3 |   INDEX FAST FULL SCAN| ORDER_ITEMS_UK|    100 |    665 |
-----------------------------------------------------------------
Predicate Information (identified by operation id): 
--------------------------------------------------- 
  1 - access("P"."PRODUCT_ID"="OI"."PRODUCT_ID") 
  2 - filter(("MIN_PRICE"<10 AND "LIST_PRICE"<20)) 
We can see that the cardinality estimates are way off the line. We see a huge difference
between E-rows and A-rows.
Since we ran the query for the first time and the cardinality estimates were wrong, let's see if
the optimizer has marked this query as re-optimizable:

DEO>select sql_id, child_number, is_reoptimizable from v$sql where sql_id = 
'9trfmyxfy697a'; 
SQL_ID        CHILD_NUMBER I 
------------- ------------ - 
9trfmyxfy697a            0 Y 
And indeed, the query is marked for re-optimization. So when the query ran for the first time,
the optimizer must have checked the estimated statistics against the actual rows returned by the
statistics collector, and, since the difference is above the threshold, it marked this query for
automatic re-optimization. The database stores the information about the re-optimization of
the query as an SQL plan directive. Let us check if we have any SQL plan directive stored for
predicates used in this query:
SQL> select a.directive_id, b.object_name, b.subobject_name, b.object_type, 
a.REASON from dba_sql_plan_dir_objects b, dba_sql_plan_directives a where 
a.directive_id = b.directive_id and b.owner = 'OE' and object_name = 
'PRODUCT_INFORMATION' ;     
            DIRECTIVE_ID OBJECT_NAME                    SUBOBJECT_NAME       
OBJECT REASON 
------------------------- ------------------------------ -------------------- 
------ ------------------------------------ 
     8466635027916212629 PRODUCT_INFORMATION            LIST_PRICE           
COLUMN SINGLE TABLE CARDINALITY MISESTIMATE 
     8466635027916212629 PRODUCT_INFORMATION            MIN_PRICE            
COLUMN SINGLE TABLE CARDINALITY MISESTIMATE 
     8466635027916212629 PRODUCT_INFORMATION                                 
TABLE  SINGLE TABLE CARDINALITY MISESTIMATE 
Sometimes, you may not be able to see SQL plan directives in DBA_* tables. It is possible that
the directive is created recently in memory and has not yet flushed to the disk in the SYSAUX
table. You can manually flush the directives created to disk using the following procedure:
DEO>exec dbms_spd.flush_sql_plan_directive; 
PL/SQL procedure successfully completed. 
From the preceding output, we have one directive defined in PRODUCT_INFORMATION. This
directive is in the table and two of its columns-LIST_PRICE and MIN_PRICE. So whenever a
query uses these columns from PRODUCT_INFORMATION table, this directive will be used for that
query.
So during the first execution of the SQL, the optimizer collects additional statistics using a
statistics collector, compares the estimated statistics with actual rows returned by the collector,
and, if it exceeds the threshold values, marks the SQL for automatic re-optimization and
stores additional information as an SQL plan directive.
Now, if you resubmit the query again, it will go for a hard parse, and the optimizer will use
the directive that was created when we executed the query the first time.

After resubmitting the query, let us check the plan for the query again.
As you can see in the following code, the query created another child cursor as it went for
hard parsing:
SQL> select sql_id, child_number, is_reoptimizable from v$sql where sql_id = 
'gc0sy3nccmkmm'; 
SQL_ID        CHILD_NUMBER I 
------------- ------------ - 
9trfmyxfy697a            0 Y 
9trfmyxfy697a            1 N 
Let us check the plan for child_cursor 1:
Plan hash value: 3092865807
-----------------------------------------------------------------
| Id  | Operation             | Name          | E-Rows | A-Rows |
-----------------------------------------------------------------
|   0 | SELECT STATEMENT      |               |        |     63 |
|*  1 |  HASH JOIN            |               |     63 |     63 |
|*  2 |   TABLE ACCESS FULL   | PRODUCT_INFORM|     30 |     30 |
|   3 |   INDEX FAST FULL SCAN| ORDER_ITEMS_UK|    665 |    665 |
-----------------------------------------------------------------
Predicate Information (identified by operation id): 
--------------------------------------------------- 
  1 - access("P"."PRODUCT_ID"="OI"."PRODUCT_ID") 
  2 - filter(("MIN_PRICE"<10 AND "LIST_PRICE"<20)) 
Note 
----- 
  - dynamic statistics used: dynamic sampling (level=2) 
  - statistics feedback used for this statement 
  - this is an adaptive plan 
  - 1 Sql Plan Directive used for this statement 
From the preceding plan table output, A-rows and E-rows are very much the same now and in
the notes section, as we can see from the notes - "1 Sql Plan Directive used for this
statement".
We can manage the SQL plan directive using the DBMS_SDP package. The following shows the
various procedures in this package and what they do.
DBMS_SPD
Oracle 12c provides the DBMS_SPD package to manage SQL plan directives. There are multiple
procedures and functions available in this package to manage SQL plan directives. We have
already seen one procedure, used to flush SQL plan directives from the memory into the

SYSAUX tablespace. The following are a few more important procedures.
PACK_STGTAB_DIRECTIVE and UNPACK_STGTAB_DIRECTIVE: Just as we can transfer SQL
plan baselines from one database to another, so we can also transfer directives from one
database to another. These procedures are for packing and unpacking SQL plan
directives into a staging table. We can pack a directive into a staging table (just like SQL
plan baselines), export and import tables into different databases, and unpack the
directive.
SET_PREFS: This procedure is used to set retention for SQL plan directives. The
parameter name to be used is SPD_RETENTION_WEEKS and the value can be any number.
The default is 52 weeks.
DROP_SQL_PLAN_DIRECTIVE: This procedure is used to manually drop a directive. The
input for this procedure is directive ID.

Enhanced features of statistics gathering
Oracle 12c has implemented many new features with respect to object level statistics
gathering. Let us check out these features:
Online statistics gathering for bulk-load
Concurrent statistics gathering
Statistics gathering for global temp tables
Histogram enhancements
Top frequency histograms
Hybrid histograms
Extended statistics enhancements

Online statistics gathering for bulk-load
Until the previous release, whenever we used to bulk load into a table using a CTAS or CREATE
TABLE AS SELECT ... statement or using INSERT INTO ... SELECT that uses direct path load,
Oracle never gathered statistics automatically during the bulk load operation. A DBA had to
manually gather statistics after the load is complete. This used to take a long time depending
on the size of the table and it would duplicate the task by reading the same data that had just
been loaded.
In Oracle12, statistics for the bulk load operation are gathered automatically. This increases
the bulk load using CTAS or INSERT INTO ... SELECT that uses direct path inserts only.
Parallel inserts are direct path inserts, so the stats will be gathered automatically. We can also
force a direct path insert using /*+ APPEND */ hint.
There are a few limitations to note here:
Statistics are gathered automatically on empty tables only. If the table already has data,
bulk load data using a direct path insert will not gather statistics automatically.
Only the statistics in the table are gathered. You need to manually gather statistics on
indexes and histograms. To do this, you can run DBMS_STATS.GATHER_TABLE_STATS with
the OPTIONS parameter set to GATHER AUTO. This way, when you run statistics gathering
on the table, it will gather only missing statistics, which in this case is of index and
histogram tables. Alternatively, you can also set the table preference option to GATHER
AUTO so that you don't have to use the OPTIONS parameter in DBMS_STATS procedures.
In the case of partition tables, if the entire partition table is empty and you bulk load the
partition table, only global statistics for the partition table will be gathered automatically
during the load. Individual partition statistics are not gathered.
If you load data into individual empty partitions, then only partition level statistics are
gathered automatically, and global statistics are not gathered automatically. Also, if
incremental statistics are enabled for partitioned tables and extended syntax is not used
for bulk load, automatic statistics gathering will not happen.
Oracle will not collect statistics during bulk load under the following conditions:
Table is an IOT or an external table
Table has a virtual column
Table is owed by SYS
Statistics preference PUBLISH is set to FALSE
Table is a global temporary table with ON COMMIT DELETE

Concurrent statistics gathering
In previous releases, statistics gathering was a single process job, and it used to take lot of
time to gather statistics for large tables. Also, statistics gathering was serialized, one after the
other. In Oracle 12c, we can gather statistics concurrently using multiple processes. We can
configure Oracle to run stats gathering on tables and indexes concurrently. We can also
enable stats gathering to gather stats for multiple tables concurrently or for multiple partitions
of the same table concurrently.
The Oracle database uses the following three components to enable concurrent statistics
gathering:
Oracle scheduler
Oracle advanced queuing (AQ)
Oracle resource manager
The gather stats job is scheduled in the Oracle scheduler. The maximum number of job
processes a database can run depends on the parameter job_queue_processes. Oracle can use
all the available job queue processes to run the stats gathering job in parallel (concurrently).
However, if the number of concurrent processes are more than the job_queue_processes, the
remaining processes will be queued using Oracle's advanced queuing. Oracle scheduler will
add jobs from the queue into the list of running jobs until the database completes the
gathering of statistics for all objects.
However, the Oracle scheduler cannot control the amount of resources used by the gather
stats job when concurrent statistics gathering is enabled. So, in order to control the resource
utilization during the gather stats job, enabling the Oracle resource manager is required. You
can either use Oracle's supplied DEFAULT PLAN or you can manually create your own plan for
the ORA$AUTOTASK consumer group to prevent all resources being consumed by the stats
gather job.
Enabling concurrent statistics collection
To enable the concurrent statistics collection, you need to:
Set the job_queue_processes parameter to at least 4
Enable resource manager: If you haven't created your owner plan, you can use Oracle's
supplied DEFAULT PLAN
Use the DBMS_STATS.SET_GLOBAL_PREFS procedure to enable concurrent statistics
collection
By default, concurrent stats collection is disabled for the manual and automatic stats
collection process. You can set the following values for CONCURRENT parameters in the
DBMS_STATS.SET_GLOBAL_PREFS procedure.
MANUAL: Concurrency is enabled only for manual stats gathering processes

AUTOMATIC: Concurrency is enabled only for automatic stats gathering processes
ALL: Concurrency is enabled for both MANUAL and AUTOMATIC stats gathering processes
Take the following steps to action:
1. Set job_queue_processes to at least 4.
 
SQL> alter system set job_queue_processes=16; 
 
System altered. 
2. Enable the resource manager in the database. You can create your own resource plan or
use Oracle's supplied DEFAULT_PLAN.
SQL> alter system set RESOURCE_MANAGER_PLAN = 'DEFAULT_PLAN'; 
 
System altered. 
3. Use the DBMS_STATS.SET_GLOBAL_PREFS procedure to enable concurrent statistics
collection.
SQL> exec dbms_stats.set_global_prefs('CONCURRENT','ALL'); 
 
PL/SQL procedure successfully completed. 
You can confirm whether concurrency is enabled by using the DBMS_STATS.GET_GLOBAL_STATS
procedure. You can further monitor the progress of the automatic stats gathering job using the
DBA_OPTSTAT_OPERATION_TASKS view.
You can also disable the concurrent stats gathering by setting CONCURRENT parameter to OFF, as
shown in the following:
SQL> exec dbms_stats.set_global_prefs('CONCURRENT','OFF'); 
PL/SQL procedure successfully completed. 

Parallel execution of stats gathering
Apart from running stats gathering concurrently, we can also run the job in parallel. With
concurrent statistics gathering, we can have statistics gathering run in parallel. This can be
used when the size of the object on which we are gathering the stats is large.
To enable parallel execution of the stats gathering job, we need to disable the
PARALLEL_ADAPTIVE_MULTI_USER parameter. This parameter is mainly used in multi-user
environments which use parallel processing. If this parameter is enabled (default), it
automatically reduces the requested degree of parallelism based on the system load at query
startup. The effective degree of parallelism requested (either by hint or by setting the default
parallelism of the table) will be divided by a reduction factor because of the system load.
We also should enable parallel statement queuing. To do this, we need to enable resource
manager and create a temporary resource plan for our consumer group called OTHER_GROUPS.
By default, the resource manager is activated during the maintenance window, but for this, we
need to enable the resource manager manually. The following script illustrates one way of
creating a temporary resource plan (parallel_stats_gather), and enabling the resource
manager with this plan:
-- connect as a user with dba privileges 
begin 
 dbms_resource_manager.create_pending_area(); 
 dbms_resource_manager.create_plan('parallel_stats_gather', 
'parallel_stats_gather'); 
 dbms_resource_manager.create_plan_directive( 
       ' parallel_stats_gather ', 
       'OTHER_GROUPS', 
       'OTHER_GROUPS directive for parallel stats gather', 
       parallel_target_percentage => 75, 
       max_utilization_limit => 90); 
 dbms_resource_manager.submit_pending_area(); 
end; 
/ 
ALTER SYSTEM SET RESOURCE_MANAGER_PLAN = 'parallel_stats_gather'; 

Statistics gathering for a global temporary table
Until the previous release (Oracle 11g R2), the global temporary table used to have single
statistics on the entire table shared by all sessions that were using global temporary table.
Using statistics on the global temporary table plays a critical role in deciding the optimum
plan for the SQL. But since every session had private data in the global temporary table, and a
session could not view the data of other sessions, the statistics of the entire table seemed
inaccurate for the optimizer, so there was a need to have private session statistics on the
global temporary table instead of having same statistics for the entire table shared among all
sessions.
Oracle 12c introduced a new feature to create the session statistics allowing sessions to create
and use their own set of statistics for each global temporary table.
To enable session private statistics for all global temporary tables created in a database, we
need to set the preference GLOBAL_TEMP_TABLE_STATS to SESSION using the
DBMS_STATS.SET_GLOBAL_STATS procedure. Session level private statistics are the default
setting in Oracle 12c meaning it will by default have session level private stats for each
session and not the shared statistics as was the code in previous release.
Let us check out the default value for GLOBAL_TEMP_TABLE_STATS:
SQL> select dbms_stats.GET_PREFS('GLOBAL_TEMP_TABLE_STATS') from dual; 
DBMS_STATS.GET_PREFS('GLOBAL_TEMP_TABLE_STATS') 
------------------------------------------------------------------------ 
SESSION 
The following example shows you how to enable SHARED stats at the database level. This is the
same arrangement that we had in previous releases:
SQL> exec dbms_stats.set_global_prefs('GLOBAL_TEMP_TABLE_STATS','SHARED'); 
PL/SQL procedure successfully completed. 
SQL> select dbms_stats.GET_PREFS('GLOBAL_TEMP_TABLE_STATS') from dual; 
DBMS_STATS.GET_PREFS('GLOBAL_TEMP_TABLE_STATS') 
------------------------------------------------------------------------ 
SHARED 
We can also enable SHARED stats or private SESSION stats for individual global temporary
tables using the DBMS_STATS.SET_TABLE_PREFS procedure. This provides control at the
individual GTT level. The following example sets private session stats for the individual GTT
using the SET_TABLE_PREFS procedure:
SQL> exec 

dbms_stats.set_table_prefs('SYS','T1','GLOBAL_TEMP_TABLE_STATS','SESSION'); 
PL/SQL procedure successfully completed. 
SQL> select dbms_stats.get_prefs('GLOBAL_TEMP_TABLE_STATS','SYS','T1') from dual; 
DBMS_STATS.GET_PREFS('GLOBAL_TEMP_TABLE_STATS','SYS','T1') 
---------------------------------------------------------- 
SESSION 
The shared stats on the GTT are visible to all sessions, whereas the session private stats are
visible only to that session. You can choose to use session private statistics for a few sessions
by gathering the statistics on GTT in the session, whereas other sessions can still continue to
have shared statistics available to them. In the following example, I created a GTT and
inserted some data and gathered statistics on the table. This was done when
GLOBAL_TEMP_TABLE_STATS was set to SHARED at database level. So these SHARED statistics will
be available to all sessions. After that, I changed GLOBAL_TEMP_TABLE_STATS to SESSION. So
whenever a new session is created, SHARED stats are available to that session, but if that session
inserts data into GTT and collects session level stats, those will be private SESSION stats
available only to that session.
Let us check out the example.
Setting GLOBAL_TEMP_TABLE_STATS to SHARED as sys user:
SQL> exec dbms_stats.set_global_prefs('GLOBAL_TEMP_TABLE_STATS','SHARED'); 
PL/SQL procedure successfully completed. 
Let us create a GTT, insert some data and gather stats. This will be shared stats available to all
the sessions.
SQL> create global temporary table T1 (col1 number, col2 varchar2(10)) on commit 
preserve rows; 
Table created. 
<insert data> 
SQL> exec dbms_stats.gather_table_stats(user,'T1'); 
PL/SQL procedure successfully completed. 
SQL> select table_name, num_rows, scope from user_tab_statistics where table_name 
= 'T1'; 
TABLE_NAME                       NUM_ROWS SCOPE 
------------------------------ ---------- ------- 
T1                                     10 SHARED 

Note on commit preserve rows clause in the command. This makes this GTT a session
specific. If you do not specify this clause, then on commit delete rows is the default. This will
make GTT a transaction specific table. DBMS_STATS commit changes only to session specific
GTT and not transaction specific GTT.
I have inserted 10 rows and gather stats on the table. This is now a SHARED stats and is
available to all other sessions.
Let us change the GLOBAL_TEMP_TABLE_STATS to SESSION now and insert more records in
same GTT.
SQL> exec dbms_stats.set_global_prefs('GLOBAL_TEMP_TABLE_STATS', 'SESSION'); 
PL/SQL procedure successfully completed. 
After changing GLOBAL_TEMP_TABLE_STATS, I inserted ten more records in same session as the
previous and gathered stats on the table. Now you can see two records in the
USER_TAB_STATISTICS table: One for SHARED and one for SESSION.:
SQL> select table_name, num_rows, scope from user_tab_statistics where table_name 
= 'T1'; 
TABLE_NAME                       NUM_ROWS SCOPE 
------------------------------ ---------- ------- 
T1                                     10 SHARED 
T1                                     20 SESSION 
SESSION stats are private stats and are not available to other sessions, whereas SHARED stats are
available to other sessions. You can verify them by creating a new session and checking
records from USER_TAB_STATISTICS, you will see only SHARED stats:
SQL> select table_name, num_rows, scope from user_tab_statistics where table_name 
= 'T1'; 
TABLE_NAME                       NUM_ROWS SCOPE 
------------------------------ ---------- ------- 
T1                                     10 SHARED 
All the tables which store the statistics such as DBA_TAB_STATISITCS, DBA_IND_STATISTICS,
DBA_TAB_HISTOGRAM and so on,have a new column called SCOPE, which tells whether the
statistics are SESSION level or SHARED statistics. Also, session private statistics are never
persisted on disk. They are deleted immediately after the session is closed.

Histogram enhancements
Histogram helps optimizer to improve the cardinality estimates of columns having skewed
data. When we gather statistics on tables, we can specify in METHOD_OPT to gather statistics on
all columns, or indexed columns or only for skewed columns. Based on the values of data in
columns, Oracle decides to go for different types of histogram. Optimizer can evaluate a
more accurate plan if predicates of the query have histogram statistics available. Until
previous release, Oracle used to the generate following two types of histogram based on the
number of distinct values of the column:
Frequency balanced histogram: This was created when the number of distinct values
were less than number of buckets, which is 254 by default. Each bucket used to represent
the distinct value and the frequency with which that value was repeated in the column.
Height balanced histogram: This was created when the number of distinct values are
more than number of buckets. Each bucket woul have range of values and
ENDPOINT_VALUE represented the highest distinct column value in the bucket and
ENDPOINT_NUMBER represented cumulative frequency.
Oracle 12c provides two new types in histograms:
Top-frequency histogram
Hybrid histogram
Let's look into each of these histograms.
Top frequency histogram
In previous releases, Oracle used to create a frequency histogram if the number of distinct
values were less than number of buckets, which is usually 254, but if the number of distinct
values were more than the number of buckets, Oracle used to create height balanced
histogram, which was less accurate. This is because, in the case of a height balanced
histogram, there were not enough buckets to represent each value so each bucket represented
a range of values, which should contain approximately the same frequency of rows. The
ENDPOINT_VALUE represent the highest value in the bucket and the ENDPOINT_NUMBER
represented the cumulative frequency. With a height balanced histogram, it was difficult to
identify the most popular values. Sometimes, a value that occurred as an endpoint value of
only one bucket but actually occupied almost two buckets wasn't deemed popular.
To give more accurate statistics of data, Oracle 12c introduced top-frequency histogram. This
histogram is useful to represent accurate statistics of the most popular values in the column.
Top-frequency histogram is best used where a small number of distinct values are occupying
99% of the table column. Oracle 12c creates top-frequency histograms in the following
condition:
Sampling percentage is set to AUTO
Distinct values in a column is more than the number of buckets

Percentage of top distinct values are above a certain threshold (99%)
Columns should contain more than 254 distinct values
Let the number of top distinct values be t, then threshold p can be calculated as p = (1 -
(1/t))*100.
If the number of top distinct values are 254, then threshold p = (1 - (1/254))*100 = 99.6%.
If the number of top distinct values are 10, then threshold p = (1 - (1/10))*100 = 90%. So top
10 distinct values should cover 10% of total column values.
Instead of using statistically insignificant unpopular values, the top-frequency histogram
produces better histograms by focusing only on popular values and showing the data
distribution for only popular values. This helps in getting accurate estimates for most of the
queries because most of the queries will be using the most popular values in the query bind
variables and we have more accurate distribution of data for the most popular values.
Let's take an example: We will create a table and insert data which is skewed. 10 distinct values
will be repeated in 90% of the table column. The remaining 10% of the table column will all
have distinct values. Later we will collect statistics such that the number of buckets are less
than the number of distinct values and we will see Oracle automatically creates a top-
frequency histogram.
Let's start:
1. Creating the table and inserting the records:
SQL> create table T1 (col1 number, col2 number, col3 varchar2(10)); 
 
Table created. 
 
SQL> insert into T1 select level, case when level <= 9000 then 
trunc(DBMS_RANDOM.value(1,10)) else level end, 'row '||level from dual 
connect by level <= 10000; 
 
10000 rows created. 
 
SQL> commit; 
 
Commit complete. 
2. Creating an index and gathering statistics:
SQL> create index I_T1_COL2 on T1(col2) ; 
 
Index created. 
 
SQL> exec dbms_stats.gather_table_stats(user,'T1',method_opt => 'for columns 
col2 size 10'); 
 
PL/SQL procedure successfully completed. 

 
3. Note that we are using method_opt => 'for columns col2 size 10'. So this is going to
create only 10 buckets.
SQL> SELECT column_name, histogram FROM user_tab_columns WHERE  table_name = 
'T1'; 
 
COLUMN_NAME          HISTOGRAM 
-------------------- --------------- 
COL3                 NONE 
COL2                 TOP-FREQUENCY 
COL1                 NONE 
As you can see in the preceding code, Oracle has created a TOP-FREQUENCY histogram. If we
check ENDPOINT_VALUE and ENDPOINT_NUMBER, it will show as below. Note that
ENDPOINT_VALUE represents the highest value in the bucket and ENDPOINT_NUMBER represents
the cumulative frequency.
In the following output, we can see values 1-9 are endpoint values of each bucket, meaning
they are the highest values of the bucket, whereas the last bucket has the highest value of
10,000. Also, the frequency represented by ENDPOINT_NUMBER is cumulative, meaning its
adding to the previous number.
SQL> select endpoint_value, endpoint_number from user_tab_histograms where 
table_name = 'T1' and column_name = 'COL2'; 
ENDPOINT_VALUE ENDPOINT_NUMBER 
-------------- --------------- 
            1             989 
            2            1974 
            3            2969 
            4            3926 
            5            4927 
            6            5963 
            7            6985 
            8            8009 
            9            9000 
        10000            9001 
10 rows selected. 
We can get the frequency of each ENDPOINT_VALUE separately using the following query.
Below, the FREQUENCY column shows the individual ENDPOINT_VALUE frequency and not the
cumulative frequency:
SQL> select endpoint_value, endpoint_number, endpoint_number - 
(lag(endpoint_number,1) over (order by endpoint_value)) "frequency" from 
user_tab_histograms where table_name = 'T1' and column_name = 'COL2'; 
ENDPOINT_VALUE ENDPOINT_NUMBER  FREQUENCY 
-------------- --------------- ---------- 

            1             989 
            2            1974        985 
            3            2969        995 
            4            3926        957 
            5            4927       1001 
            6            5963       1036 
            7            6985       1022 
            8            8009       1024 
            9            9000        991 
        10000            9001          1 
10 rows selected. 
As you can see, each of the popular values has a frequency close to 1,000, whereas a non-
popular value has frequency of only 1. This helps optimizer to better evaluate the plan for the
query.
Hybrid histogram
Hybrid histograms, so called because they represent a combination of the features present in a
height-balanced histogram and a frequency balanced histogram, are an attempt to provide
more accurate estimates. Hybrid histograms store additional information compared to other
histograms. This additional information is the endpoint repeat count value, which is the
number of times the endpoint is repeated for each endpoint in histogram. Also, unlike a in
height balanced histogram, a single endpoint cannot span multiple buckets in a hybrid
histogram.
Based on the information about endpoint cumulative frequency, endpoint_vale and endpoint
repeat count, optimizer will have much better estimates of popular values in the column.
Oracle will create hybrid histogram in the following conditions:
Sampling percentage is set to AUTO.
Condition for top-frequency histogram is not met, meaning that 90% table column, do
not have any of the top popular values. This means that data is distributed but we do have
popular values which are repeated more than non-popular values but number of popular
values are much higher than the number of buckets.
Number of distinct values are higher than the user specified number of buckets.
Oracle 12c has introduced a new column ENDPOINT_REPEAT_COUNT which notes the number of
times endpoint value has repeated in the bucket.
For example, I created a table with 10,000 records and 50% of the values in the columns are
popular. Meaning that we are repeating a few values in the table which are occupying 50% of
the table. Let us say we have 100 distinct values in the table and each value is repeated 50
times. The remaining 50% of the table has unique values.
When we gather stats on the table with buckets as 254, we will get a hybrid histogram because
the number of distinct values (5000 + 100) are more than the number of buckets. Also,

condition for a top-frequency histogram has not been met as 90% of the table column does
not have top popular values. Only 50% of the table has top popular values:
SQL> SELECT column_name, histogram FROM user_tab_columns WHERE  table_name = 
'T1'; 
COLUMN_NAME          HISTOGRAM 
-------------------- --------------- 
COL3                 NONE 
COL2                 HYBRID 
COL1                 NONE 
We can also check endpoint_repeat_count and see how it differentiates clearly between
popular values and non-popular values:
SQL> select endpoint_value, endpoint_number, endpoint_repeat_count from 
user_tab_histograms where table_name = 'T1' and column_name = 'COL2'; 
ENDPOINT_VALUE ENDPOINT_NUMBER ENDPOINT_REPEAT_COUNT 
-------------- --------------- --------------------- 
            1              58                    58 
            2             105                    47 
            3             154                    49 
            4             209                    55 
            5             250                    41 
            6             298                    48 
            7             356                    58 
            8             404                    48 
            9             448                    44 
           10             502                    54 
           11             553                    51 
.. 
.. 
.. 
Less popular values will have ENDPOINT_REPEAT_COUNT as 1 as shown below 
ENDPOINT_VALUE ENDPOINT_NUMBER ENDPOINT_REPEAT_COUNT 
-------------- --------------- --------------------- 
         7871            8936                     1 
         7935            8968                     1 
         7997            8999                     1 
         8061            9031                     1 
         8125            9063                     1 
         8189            9095                     1 
         8253            9127                     1 
         8317            9159                     1 
         8379            9190                     1 
         8443            9222                     1 
         8507            9254                     1 

Extended statistics enhancements
Oracle Database 12c introduced the concept of extended statistics, which are statistics that help
the optimizer estimate cardinality more accurately when multiple predicates exist on different
columns of a table. The key idea is to collect statistics on a group of columns within a table,
rather than collecting statistics independently for each column, which leads to inefficient
estimates when the columns are related.
We have two types of extended statistics:
Column group statistics: When a group of columns are involved
Expression statistics: When using an expression on a column
Although extended stats are maintained automatically, it is important to find the column group
and expressions used in various queries in a database and create stats for those. Oracle 12c
makes this easier by automatically collecting the column group and expression information.
All we have to do is to enable workload monitoring for a period of time when we are running
queries and Oracle will gather all the required information related to columns and
expressions used in various queries. This information will be used to create extended
statistics.
The following are the steps:
1. Enable workload monitoring: You can use a new procedure
DBMS_STATS.SEED_COL_USAGE to enable workload monitoring. This procedure goes
through all SQL statements that are executed in the instance during the next five minutes
(300 seconds), compiling the statements and storing the column usage information
(seeding) in the data dictionary (sys.col_usage$ and sys.col_group_usage$) for all the
columns that appear in the SQL statements. You have three inputs to this procedure:
Name of SQL tuning set
Owner of SQL tuning set
Time limit in secs
If you haven't created an SQL tuning set, you can pass NULL and it will gather
information about all SQLs currently running.
SQL> exec dbms_stats.seed_col_usage(null, null, 300); 
PL/SQL procedure successfully completed.
2. Running the workload: You can let the application run the desired workload or if you
have a set of queries that you want to gather extended stats on, you can run the same. In
fact, you don't have to run the SQLs. Just running explain plan for those SQLs will gather
the required information.
3. Reviewing the column group usage: You can get reports on column usage by different
queries during the time of workload monitoring. You can get this using the
DBMS_STATS.REPORT_COL_USAGE function as follows:

SQL> set long 999999
SQL> set line 9999
SQL> set pagesize 9999
SQL> select dbms_stats.report_col_usage('OE','ORDER_ITEMS') from dual;
DBMS_STATS.REPORT_COL_USAGE('OE','ORDER_ITEMS')
--------------------------------------------------------------
LEGEND:
.......
EQ         : Used in single table EQuality predicate
RANGE      : Used in single table RANGE predicate
LIKE       : Used in single table LIKE predicate
NULL       : Used in single table is (not) NULL predicate
EQ_JOIN    : Used in EQuality JOIN predicate
NONEQ_JOIN : Used in NON EQuality JOIN predicate
FILTER     : Used in single table FILTER predicate
JOIN       : Used in JOIN predicate
GROUP_BY   : Used in GROUP BY expression
...............................................................
###############################################################
COLUMN USAGE REPORT FOR OE.ORDER_ITEMS
...................................
ORDER_ID : EQ EQ_JOIN
PRODUCT_ID : EQ_JOIN
QUANTITY : NONEQ_JOIN
(ORDER_ID, PRODUCT_ID) : JOIN
###############################################################
Note that, REPORT_COL_USAGE is for per table, so you need to run it multiple times,
passing the relevant table name to get extended stats to be created on the required tables.
4. Creating extended stats: Finally, once we review the column usage report in step 3, we
can proceed to create extended stats using the DBMS_STATS.CREATE_EXTENDED_STATS
function:
 
SQL> select dbms_stats.create_extended_stats('OE','ORDER_ITEMS') from dual; 
 
DBMS_STATS.CREATE_EXTENDED_STATS('OE','ORDER_ITEMS') 
-------------------------------------------------------------------------
------- 
#########################################################################
###### 
 
EXTENSIONS FOR OE.ORDER_ITEMS 
............................. 
 
1. (ORDER_ID, PRODUCT_ID)              : SYS_STUOXJE7O6WNTT9ER8PSQ$EAIZ 
created 
#########################################################################

###### 
From the previous output we can see that, Oracle has created extended stats for
(ORDER_ID, PRODUCT_ID) column group. These column groups are used in JOIN as we
can see from Step 3.

Adaptive SQL plan management
SQL plan management was introduced in Oracle 11g to avoid plan regressions. Let us review
what we had in 11g about SQL plan management and then we will look at the new feature
introduced in Oracle 12c.

Challenges in previous releases
Before we dive into adaptive SPM in Oracle 12c, let us look into the features and limitations
of previous releases.
SQL plan management - previous releases
SQL plan management was introduced in Oracle 11g and is mainly used for plan stability.
This feature was one of the most important feature for avoiding regression of SQL. SQL plan
management (SPM) stores the baseline (plans + hints) inside the database and only good
plans are enabled for this. Whenever an SQL without a baseline is executed for the first time,
optimizer might derive multiple plans for that SQL and the best plan (as per optimizer) is used
for that SQL. If we run the same SQL multiple times and if the same plan gets used, Oracle
automatically creates a baseline for such SQL and store it inside the database. Eventually, as
more and more new plans get discovered by optimizer because of changes in the environment
or changes in data inside table, optimizer will create new baselines for those new plans but
will not use the new plans unless we enable those new plans. So only the plan that was enabled
before will be used.
This provides plan stability in our environment and any change in our environment including
upgrades of the database and does not cause plans to change.
Limitations of outlines and profiles
Outline is basically a way to tell optimizer what should be done for a SQL. We are directly
providing optimizer the plan to be executed in the form of hints. This will not work in all
cases because outlines basically fixes the execution plan of the SQL. So in future, if there is a
change in the data or environment, the existing outlined plan might become bad and in that
case optimizer does not have a way to move towards a better plan. For example, let's say, you
have 100 entries for month JUNE in a table with a total of 150 entries. In this case, optimizer
might decide to go for a full table scan whenever a user uses JUNE as the month in the
predicate. But over time, if number of entries in table is skewed such that entry for JUNE
remains less than one percent of the total rows, doing full table scan is a bad idea, but since
you used outline, you are forcing optimizer to go for a FTS.
Profile is another feature introduced in 10g that can be implemented at SQL level. Profiles are
better than outlines in that they are not fixing a plan throughout the life of SQL. With any
change in the environment plans can change even if the profile is used. This is because the
profile does not tell optimizer what needs to be done. Profile only assists optimizer in getting
the correct statistics.
Finally, how are baselines different than outline and profile, then?
Baselines takes the best of outline and profile. So baseline tells optimizer what needs to be
done in the form of plan hints at the same time if there is a change in environment, Oracle
optimizer will generate a new plan and a corresponding baseline will get created. Oracle can

also automatically evolve that baseline depending on whether you have Auto Tuning task
enabled in the database. As a DBA, you can disable the task to prevent any plan change, but if
you enable Auto Tuning task, Oracle will evaluate the plan with respect to current plan before
it enables the same. The new plan will be enabled only if it is found to be better than the
current plan. So in a way SQL baselines are a combination of outlines and profiles. They give
the stability of a plan similar to outlines and it also allows for capturing better plans if the
environment changes.
This way DBAs have better control on SQL plans using the baselines.
SQL plan baseline parameters
The following two parameters are related to baselines:
optimizer_capture_sql_plan_baselines: The default value of this parameter is false.
This parameter enables automatic capturing of baselines for SQL. If a SQL runs multiple
times and this parameter is set to TRUE, Oracle will automatically capture the baseline.
The first baseline that is captured for a SQL will be enabled and accepted by default.
Further baselines are only enabled but they are not accepted. If we set this parameter to
FALSE, we can still create baselines manually using the following two procedures:
Using DBMS_SPM.LOAD_PLANS_FROM_SQLSET: We can use this if we have SQLs stored in
SQL tuning set and we want to create baselines for those SQLs.
Using DBMS_SPM.LOAD_PLANS_FROM_CURSOR_CACHE: We can use this if we want to fix one
SQL which is currently present in cache.
optimizer_use_sql_plan_baselines: The default value of this parameter is TRUE. If this
parameter is true, it allows optimizer to use whatever baselines are present in the
database. If this parameter is FALSE, the use of baselines are not allowed by optimizer.

Adaptive SPM - Oracle 12c
Oracle 12c has introduced adaptive SQL plan management. In this feature, the original
autotask client which used to run the DBMS_SPM.EVOLVE_SQL_PLAN_BASELINE function has been
removed. There are few changes made to the evolution process of SQL plans.
Oracle 12c has introduced a new advisor framework for SQL plan management called SPM
evolve advisor. The evolve function in Oracle 11g has not became an evolve advisor. This
SPM evolve advisor is scheduled to run during the nightly maintenance window and is part of
advisor task. When SPM evolve advisor is enabled, it runs a verification or evolve process
for all SQL statements that have non-accepted plans during the nightly maintenance window.
All non-accepted plans are ranked and the evolve process is run for all those SQLs. The
newly formed plans are listed at the top and are ranked highest.
If a new plan performs better than other accepted plans for the SQL, then the new plan gets
accepted and is available for use. After verification is complete a report is generated for each
run and for each SQL detailing how a non-accepted plan performed and why it was accepted.
These reports are persisted in SYSAUX tablespace and can be viewed at a later point.
Oracle 12c has also introduced a new task called SYS_AUTO_SPM_EVOLVE_TASK. This is the
advisory task for SPM evolve advisor and is run during the nightly maintenance window.
As you can see in the following, we have the autotask of sql tuning advisor configured and
it runs a task name called AUTO_SQL_TUNING_PROG:
DEO>select client_name, task_name from dba_autotask_task where CLIENT_NAME = 
'sql tuning advisor'; 
CLIENT_NAME                    TASK_NAME 
------------------------------ ------------------------------ 
sql tuning advisor             AUTO_SQL_TUNING_PROG 
You can run the following SQL against DBA_SCHEDULER_PROGRAMS and see that internally
AUTO_SQL_TUNING_PROG is calling SYS_AUTO_SPM_EVOLVE_TASK.
New task-based function
With the introduction of SPM evolve advisor, Oracle has introduced new functions in the
DBMS_SPM package. These functions are used for creating the task, executing the task,
generating the report and finally implementing the required baselines which have passed
verification.
The following are the new functions added to the DBMS_SPM package:
DBMS_SPM.CREATE_EVOLVE_TASK: This function will create a new evolve task to prepare a
plan evolution of one of more SQLs.
DBMS_SPM.EXECUTE_EVOLVE_TASK: This function will execute the plan verification steps

for all the non-accepted plans available for SQLs added in the task.
DBMS_SPM.REPORT_EVOLVE_TASK: This function will report the plan verification step and
what plans are accepted with all the stats.
DBMS_SPM.IMPLEMENT_EVOLVE_TASK: This function will implement the result of the task.
All the plans which are verified to be accepted will be evolved and accepted by this
function.
Configuring the SPM evolve advisor task
You can configure various parameters of the SPM evolve advisor task using
DBMS_SPM.SET_EVOLVE_TASK_PARAMETER procedure. The following are the various parameters
that you can configure:
DEO>select parameter_name, parameter_value from dba_advisor_parameters where 
task_name = 'SYS_AUTO_SPM_EVOLVE_TASK' and parameter_value != 'UNUSED'; 
PARAMETER_NAME                 PARAMETER_VALUE 
------------------------------ ------------------------------ 
ACCEPT_PLANS                   TRUE 
_SPM_VERIFY                    TRUE 
DAYS_TO_EXPIRE                 UNLIMITED 
JOURNALING                     INFORMATION 
MODE                           COMPREHENSIVE 
TARGET_OBJECTS                 1 
TIME_LIMIT                     3600 
DEFAULT_EXECUTION_TYPE         SPM EVOLVE 
EXECUTION_DAYS_TO_EXPIRE       30 
The , ACCEPT_PLANS parameter is by default TRUE. This means that, when sql tuning advisor
runs in the nightly maintenance window, it is going to accept all the recommended plans
without DBA intervention. You can change that and set ACCEPT_PLANS to false using
DBMS_SPM.SET_EVOLVE_TASK_PARAMETER as shown in the following:
DEO>exec 
dbms_spm.set_evolve_task_parameter('SYS_AUTO_SPM_EVOLVE_TASK','ACCEPT_PLANS','FAL
SE'); 
PL/SQL procedure successfully completed. 
Furthermore, in previous releases, DBMS_XPLAN.DISPLAY_SQL_PLAN_BASELINE used to display
the execution plan of a baseline by compiling the SQL against the baseline, mainly by using
the outline hints stored in the baseline. SQL plan management never stored new plans added to
history until they were accepted.
Starting from Oracle 12c, SQL management base stores all the plans (all rows of plans)
whenever a new plan gets added to the plan history. DBMS_XPLAN.DISPLAY_SQL_PLAN_BASELINE
function fetches plan rows from baselines and displays the same.
For databases upgraded to Oracle 12c, plan rows are populated in baselines whenever a plan
is executed.

Summary
In this chapter we discussed real-time database operation monitoring and how to enable it the
same and monitor progress. We also talked about identifying the operation based on
operation name and execution ID. We then dived deep into SQL tuning and discussed the
behavior in Oracle 11g R1 and how the limitations are worked up in Oracle 11g R2 by
improving dynamic sampling and implementing cardinality feedback. We then discussed the
limitations in Oracle 11g R2 and how Oracle 12c overcomes the same by using dynamic
plans. We also saw how dynamic plans work using an example of join methods. We then
discussed automatic optimization and how it is different from dynamic plans and its benefits.
We also saw that automatic optimization has two techniques: Statistics feedback and
performance feedback. We saw a couple of examples of statistics feedback and how it helps in
improving the performance of SQL in subsequent execution. Later we moved on to adaptive
statistics and two techniques used in adaptive statistics: Dynamic statistics and a SQL plan
directive. We discussed each of them and also saw example of using SQL plan directives. We
also saw how we can manage SQL plan directives using the DBMS_SDP package and various
procedures inside this package.
We then discussed the enhancements of statistics gathering and how Concurrent statistics
gathering can benefit us when we are gathering statistics for bigger tables. We also discussed
about parallel execution of statistics gathering and statistics gathering for a global temporary
table. We then saw the enhancements to histograms, which includes two new histogram types:
Top frequency histograms and hybrid histograms. We  dived deep into each of these
histograms with an example to understand the difference and where they can be used. We also
checked out enhancements to two types of extended statistics: Column group statistics and
expression statistics and how we can enable monitoring to check column usage so that
automatic extended statistics can be created.
Finally, we saw the new feature called Adaptive SQL plan management and how it is different
from SQL plan management in previous releases.
In the next chapter we are going to look into backup and flashback enhancements. We are
going to go through various scenarios of backup and recovery to demonstrate how they work
in a multi-tenant environment. We will check out flashback enhancements and how flashback
works in a multi-tenant environment. Finally, we will see one of the cool features of restoring
a dropped table using a simple RMAN command.

Chapter 7. Backup and Flashback
Enhancements
Database backup and recovery is one of the most important things that a DBA should be
familiar with. Every production DBA will encounter a situation where they have to perform a
point-in-time recovery of a database at least once. Restoring and recovering the entire
database will be done many times while working as a DBA. This chapter is all about learning
the new features of RMAN and understanding how RMAN will work in a multitenant
environment. We will also look into flashback features and how flashback works in a
multitenant environment. This chapter will cover the following topics:
Performing CDB and PDB backups
Performing CDB and PDB recovery
Performing CDB flashbacks
Using RMAN enhancements
Automatic Table recovery using RMAN
Implementing new features of Flashback Data Archive
With the changes in the Oracle 12c architecture (the introduction of a multitenant
architecture), Oracle has had to make corresponding changes in RMAN to support the backup
and recovery of multitenant databases. We can now use RMAN to perform the backup and
recovery of non-CDB, CDB, and PDB databases. Let's look into the new enhancements of
RMAN in order to back up and recover multitenant containers and pluggable databases.

Performing a backup of CDB and PDB
You can back up and recover an entire CDB, including its PDB, one or more PDBs, or just the
root container with the new RMAN capabilities. You can also recover an individual tablespace
or datafile that belongs to either the CDB or PDB. Most of the backup and recovery
commands are more or less the same, with a new clause added to take care of backing up and
recovering pluggable databases. Before we deep dive into backup and recovery using RMAN,
we should note down a few changes that have been made in the RMAN command syntax.
A few things have changed in RMAN; they are as follows: Due to multitenant architectures, a
few changes have been made in the RMAN syntax in Oracle 12c, which are mentioned as
follows:
When we connect to the root container and perform BACKUP DATABASE, the DATABASE
refers to the entire database--root container, seed database, and all pluggable databases.
If we connect to an individual pluggable database and perform BACKUP DATABASE, the
database refers to that specific pluggable database. In this case, it will only back up that
pluggable database.
There are a few new clauses that have been introduced in RMAN PLUGGABLE DATABASE that
can be used to back up the pluggable database by connecting to the ROOT container. For
example, you can connect to the root container and run BACKUP PLUGGABLE DATABASE
PDB1 to back up just the PDB1 pluggable database. You cannot use this clause when
connected to an individual pluggable database, otherwise you will receive the following
error:
rman target=sys/oracle@pdb1
RMAN> BACKUP PLUGGABLE DATABASE PDB1;
RMAN-07538: Pluggable Database qualifier not allowed when connected to a 
Pluggable Database 
You can back up the tablespace in PDB while connected to the root container. This is
possible by qualifying the tablespace of PDB with the PDB name, as shown in the
following code:
     BACKUP TABLESPACE PDB1:SYSTEM; 
Let's look into using RMAN to back up CDB and PDBs. So, we have two pluggable databases,
PDB1 and PDB2, in our container database. The code is as follows:
SQL> select name, open_mode from v$containers; 
NAME                           OPEN_MODE 
------------------------------ ---------- 
CDB$ROOT                       READ WRITE 
PDB$SEED                       READ ONLY 
PDB1                           READ WRITE 
PDB2                           READ WRITE 
All the following examples are based on this container database, ORCL.


Performing complete CDB backup
You can back up a complete container database (CDB), including all PDB's, as backup sets
or image copies. A complete CDB backup includes a backup of all datafiles (CDB, PDBs, and
seed PDB), archive log files, spfiles, and control files.
The command for backing up the entire CDB is the same as taking a complete backup of a
non-CDB database. A complete CDB backup is the same as backing up a root container plus
backing up each pluggable database individually.
Let's try to perform a complete backup. Note that you have to be connected to the root
container to perform a complete PDB backup.
When we perform a CDB backup containing multiple PDB's, RMAN first performs the root
container backup, followed by the SEED container backup, and after that it backs up the
individual PDB's. At the end, it performs an auto-backup of SPFILE and CONTROLFILE.
We can see this behavior in the RMAN backup output, as shown in the following code:
connected to target database: ORCL (DBID=1445436483) 
RMAN> backup as compressed backupset database; 
Starting backup at 20-JUL-16 
using target database control file instead of recovery catalog 
allocated channel: ORA_DISK_1 
channel ORA_DISK_1: SID=114 device type=DISK 
channel ORA_DISK_1: starting compressed full datafile backup set 
channel ORA_DISK_1: specifying datafile(s) in backup set 
input datafile file number=00001 
name=/u01/app/oracle/oradata/deo/deo/datafiles/system01.dbf 
input datafile file number=00003 
name=/u01/app/oracle/oradata/deo/deo/datafiles/sysaux01.dbf 
input datafile file number=00004 
name=/u01/app/oracle/oradata/deo/deo/datafiles/undotbs01.dbf 
input datafile file number=00006 
name=/u01/app/oracle/oradata/deo/deo/datafiles/users01.dbf 
... 
... 
channel ORA_DISK_1: starting compressed full datafile backup set 
channel ORA_DISK_1: specifying datafile(s) in backup set 
input datafile file number=00007 
name=/u01/app/oracle/oradata/deo/seed/sysaux01.dbf 
input datafile file number=00005 
name=/u01/app/oracle/oradata/deo/seed/system01.dbf 
... 
... 
channel ORA_DISK_1: starting compressed full datafile backup set 
channel ORA_DISK_1: specifying datafile(s) in backup set 
input datafile file number=00011 
name=/u01/app/oracle/oradata/deo/pdb1/sysaux01.dbf 
input datafile file number=00010 

name=/u01/app/oracle/oradata/deo/pdb1/system01.dbf 
... 
... 
channel ORA_DISK_1: starting compressed full datafile backup set 
channel ORA_DISK_1: specifying datafile(s) in backup set 
input datafile file number=00013 
name=/u01/app/oracle/oradata/deo/pdb2/sysaux01.dbf 
input datafile file number=00012 
name=/u01/app/oracle/oradata/deo/pdb2/system01.dbf 
... 
... 
Starting Control File and SPFILE Autobackup at 20-JUL-16 
piece 
handle=/u01/app/oracle/fast_recovery_area/ORCL/autobackup/2016_07_20/o1_mf_s_917
699257_cry87289_.bkp comment=NONE 
Note that in the preceding output, we can see that it is performing a backup of CDB datafiles,
followed by seed, and after that a backup of PDBs. Additionally, in the previous command, a
backup of archive logs wasn't performed, as we didn't mention it. To also enable the
performing of archive log backups, you need to use the plus archivelog clause.

Performing partial CDB backup
A partial CDB backup refers to the backing up of individual containers as required. We can
back up the root container and each pluggable database individually. The following command
shows the backing up of individual containers.
To back up the ROOT container (CDB$ROOT):
BACKUP AS COMPRESSED BACKUPSET DATABASE 'CDB$ROOT'; 
Or we can also use the PLUGGABLE keyword to back up only the root container, as shown in the
following code:
BACKUP AS COMPRESSED BACKUPSET PLUGGABLE DATABASE 'CDB$ROOT'; 
Or we can also use the ROOT keyword to back up only the root container:
BACKUP AS COMPRESSED BACKUPSET DATABASE ROOT; 
To backup individual PDBs, you can either connect to the root container and issue the BACKUP
PLUGGABLE DATABASE command or you can connect to individual PDBs and issue the BACKUP
DATABASE command. An advantage of connecting to root and backing up individual PDBs is
that you can back up multiple PDBs in a single command, as shown in the following code:
BACKUP AS COMPRESSED BACKUPSET PLUGGABLE DATABASE 'CDB$ROOT','PDB2'; 

Performing complete PDB backup
You can perform a complete backup of PDB by connecting to root or an individual PDB. An
advantage of connecting to root and performing a backup is that you can also use a "plus
archivelog" clause to back up archive logs as well. You can back up archive logs when
connected to PDB, as archive logs are generated by CDB. Another advantage of connecting to
root is, when backing up PDBs, you can backup multiple PDBs with a single command.
To connect to root and back up PDB1, use the following command:
BACKUP AS COMPRESSED BACKUPSET PLUGGABLE DATABASE PDB1; 
To connect to PDB and perform a backup, use the following command:
rman target=sys/oracle@pdb1 
BACKUP AS COMPRESSED BACKUPSET DATABASE; 

Performing partial PDB backup
You can also perform a partial backup of PDB, for example, one or more tablespaces of PDB.
This is similar to performing a tablespace backup in non-CDB. You can either connect to the
root container or the respective PDB to perform an individual tablespace backup. If you
connect to the root container, you need to qualify the tablespace name of PDB with the PDB
name. The following are some examples of this.
To connect to the root container to perform a tablespace backup of PDB:
The following command will perform a backup of the SYSTEM tablespace of the PDB1 database:
BACKUP AS COMPRESSED BACKUPSET TABLESPACE PDB1:SYSTEM; 
An advantage of connecting to root is that you can perform back ups of multiple tablespaces
that belong to different PDBs with the same command, as shown in the following code:
RMAN> backup as compressed backupset tablespace PDB1:SYSTEM, PDB2:SYSTEM; 
... 
... 
channel ORA_DISK_1: starting compressed full datafile backup set 
channel ORA_DISK_1: specifying datafile(s) in backup set 
input datafile file number=00010 
name=/u01/app/oracle/oradata/deo/pdb1/system01.dbf 
... 
... 
channel ORA_DISK_1: starting compressed full datafile backup set 
channel ORA_DISK_1: specifying datafile(s) in backup set 
input datafile file number=00012 
name=/u01/app/oracle/oradata/deo/pdb2/system01.dbf 
... 
... 
Finished backup at 20-JUL-16 
Starting Control File and SPFILE Autobackup at 20-JUL-16 
piece 
handle=/u01/app/oracle/fast_recovery_area/ORCL/autobackup/2016_07_20/o1_mf_s_917
702683_crycl41l_.bkp comment=NONE 
Finished Control File and SPFILE Autobackup at 20-JUL-16 
In the preceding command, I backed up the SYSTEM tablespace in PDB1 and PDB2. As you can
see from the partial output, the datafiles of the SYSTEM tablespace in PDB1 and PDB2 are being
backed up.
You can also connect to the respective PDB and perform the required backup of a tablespace,
just like we do with non-CDB databases:
BACKUP TABLESPACE SYSTEM; 

Performing PDB hot backup
You can also perform user-managed hot backups of a pluggable database. This will involve
putting the pluggable database in the BEGIN BACKUP mode, copying all datafiles manually and
performing an END BACKUP. You can put an entire pluggable database in BEGIN BACKUP
mode, shown as follows.
Note that you should be connected to the respective pluggable database in order to enable
BEGIN BACKUP mode. You cannot enable begin backup mode for pluggable databases by
connecting to the root container; otherwise, you will receive the following error:
ORA-65046: operation not allowed from outside a pluggable database 
The following command shows connecting to PDB and placing the pluggable database in
begin backup mode:
sqlplus pdb1_admin/oracle@pdb1 
ALTER PLUGGABLE DATABASE PDB1 BEGIN BACKUP; 
Copy all the datafiles of pluggable database, PDB1:
ALTER PLUGGABLE DATABASE END BACKUP; 
If you connect to the root container and enable the BEGIN BACKUP mode, then all pluggable
databases will be in the BEGIN BACKUP mode, as shown in the following code.
The ACTIVE status means that these files are in the BEGIN BACKUP mode and are ready for a hot
backup:
SQL> select * from v$backup order by status, con_id; 
    FILE# STATUS                CHANGE# TIME          CON_ID 
---------- ------------------ ---------- --------- ---------- 
        1 ACTIVE                1928187 20-JUL-16          1 
        3 ACTIVE                1928187 20-JUL-16          1 
        4 ACTIVE                1928187 20-JUL-16          1 
        6 ACTIVE                1928187 20-JUL-16          1 
       10 ACTIVE                1928187 20-JUL-16          3 
       11 ACTIVE                1928187 20-JUL-16          3 
       13 ACTIVE                1928187 20-JUL-16          4 
       12 ACTIVE                1928187 20-JUL-16          4 
        7 NOT ACTIVE                  0                    2 
        5 NOT ACTIVE                  0                    2 

Archive log backup
You can back up and delete archived redo logs, but only after connecting to the root container.
You can't back up or delete archive logs after connecting to PDBs because archive logs are
only created by the root container. Individual PDBs don't have their own redo logs. We have a
single redo stream that belongs to the root container. The following command shows you
how to back up all archive logs and delete them from the source once the backup is complete:
rman target / 
BACKUP ARCHIVELOG ALL DELETE INPUT; 
If you try to back up archive logs after connecting to PDB, you will not get any errors, but
nothing will be backed up:
rman target=sys/oracle@pdb1 
RMAN> backup archivelog all; 
Starting backup at 20-JUL-16 
using target database control file instead of recovery catalog 
allocated channel: ORA_DISK_1 
channel ORA_DISK_1: SID=121 device type=DISK 
specification does not match any archived log in the repository 
backup cancelled because there are no files to backup 
Finished backup at 20-JUL-16 

Performing recovery of CDB and PDB
Before we go into the details of performing recovery on multitenant databases, we should
first understand how recovery is affected by a multitenant architecture. In the case of CDB, we
have a single instance started by CDB, which manages PDBs as well. So, if an instance
crashes, we need to perform crash recovery at the CDB level only. We cannot perform crash
recovery at the PDB level. However, the granularity of media recovery is very flexible in a
multitenant architecture, and we can perform media recovery for entire CDBs, for a PDB,
tablespace, datafiles, or even a block. However, there is an exception for undo, redo, control
files, and the root container SYSTEM tablespace. These files exist at CDB level only, and so
media recovery can only be performed at CDB level. For other files that belong to PDBs, we
can perform media recovery at PDB level.
We can also perform incomplete or point-in-time recoveries (PITR) for PDB, which is
similar to the tablespace point-in-time recovery (TSPITR) in non-CDB databases.

Performing instance recovery
Instance recovery happens at the CDB level only as we have a single instance started by CDB
that supports all PDBs. Instance recovery is performed by redo logs, which are available at
CDB level. We have a single redo stream that has redo changes for the CDB root container, as
well as for all PDBs.
For a database to open after an instance crash, Oracle needs to check the SCN number of the
datafile header with the SCN number of the control file. If the SCN number matches, the
database will be opened. If not, instance recovery needs to be performed.
The following steps must be completed during instance recovery:
During instance recovery, Oracle will apply all redo changes to the datafiles. This is
called a roll-forward phase. This includes applying changes to the datafiles of the CDB,
as well as to all PDBs. This will change the database to the state it was in during the time
of failure. This also includes transactions that were not committed. This step also
generates all the undo blocks whose redo was present in redo logs.
When we open the root container (CDB$ROOT), Oracle will rollback all the transactions
that were not committed when the database crashed. At this stage, all PDBs will be in a
mount state.
As we open each PDB, Oracle will rollback all the uncommitted transactions of that PDB,
making it consistent.
To perform media recovery, you have to perform the STARTUP at CDB root level, and once the
root container is up, open all PDBs using ALTER PLUGGABLE DATABASE ALL OPEN.

Media failure - CDB temporary file recovery
Whenever we create a database, we also create a temp tablespace. A temp tablespace is
essential for sorting operations for queries in a database. We can also set one of the temp
tablespaces as a default temporary tablespace. Any user who is not specifically assigned a
temporary tablespace during user creation can make use of this default temporary tablespace.
If we don't specify any default temporary tablespace and we also forgot to assign a temporary
tablespace to a user during its creation, then SYSTEM tablespace will be used as a temporary
tablespace for sorting. This can cause fragmentation in the SYSTEM tablespace and degrade
the performance of databases.
In the case of container databases, we can define a default temporary tablespace for the root
container, as well as for each PDB. If we don't specify a default temporary tablespace for
PDB, it uses the default temporary tablespace of the root container.
If a temp file belonging to the CDB temporary tablespace is lost, and if a user is using that
tablespace for sorting in their query, that query will fail with the following error:
ORA-01565: error in identifying file 
'/u01/app/oracle/oradata/deo/deo/temp01.dbf' 
ORA-27037: unable to obtain file status 
Linux Error: 2: No such file or directory 
Additional information: 3 
A CDB instance can start up with a missing tempfile. If any of the temp files do not exist
when CDB starts, CDB will automatically create those temp files when it opens.
You can see the following messages in the alert log file:
Errors in file /u01/app/oracle/diag/rdbms/orcl/orcl/trace/orcl_dbw0_6354.trc: 
ORA-01186: file 201 failed verification tests 
ORA-01157: cannot identify/lock data file 201 - see DBWR trace file 
ORA-01110: data file 201: '/u01/app/oracle/oradata/deo/deo/temp01.dbf' 
Wed Jul 20 20:55:20 2016 
File 201 not verified due to error ORA-01157 
Starting background process SMCO 
Wed Jul 20 20:55:20 2016 
Re-creating tempfile /u01/app/oracle/oradata/deo/deo/temp01.dbf 
After recreating the required tempfiles, the CDB database opens.
Alternatively, you can manually add another temp file, as shown in the following code, and
drop the old tempfile:
SQL> alter tablespace temp add tempfile 
'/u01/app/oracle/oradata/deo/deo/temp02.dbf' size 50M; 
Tablespace altered. 
SQL> alter database tempfile '/u01/app/oracle/oradata/deo/deo/temp01.dbf' drop; 

Database altered. 
You might get the following error while dropping the old tempfile:
SQL> alter database tempfile '/u01/app/oracle/oradata/deo/deo/temp01.dbf' drop; 
alter database tempfile '/u01/app/oracle/oradata/deo/deo/temp01.dbf' drop 
* 
ERROR at line 1: 
ORA-25152: TEMPFILE cannot be dropped at this time 
This could be because a session is still stuck using that temp file. Either kill the session if
possible or bounce the DB at a later point.

Media failure - PDB temporary file recovery
If a temp file belonging to the temp tablespace in PDB is accidentally dropped or missing, and
if a session tries to use that temp file, you will get the following error:
ORA-01565: error in identifying file 
'/u01/app/oracle/oradata/deo/pdb1/pdbseed_temp012016-07-20_12-23-06-AM.dbf' 
ORA-27037: unable to obtain file status 
Linux Error: 2: No such file or directory 
Additional information: 3 
This error is encountered when a session needs to perform a sort and the available memory
for sort is not enough.
The missing temp file will be created automatically when you open PDB. You will see the
following messages in the alert log file:
Thu Jul 21 10:26:16 2016 
alter pluggable database open 
... 
... 
ORA-01186: file 203 failed verification tests 
ORA-01157: cannot identify/lock data file 203 - see DBWR trace file 
ORA-01110: data file 203: '/u01/app/oracle/oradata/deo/pdb1/pdbseed_temp012016-
07-20_12-23-06-AM.dbf' 
Thu Jul 21 10:26:16 2016 
File 203 not verified due to error ORA-01157 
Thu Jul 21 10:26:16 2016 
Re-creating tempfile /u01/app/oracle/oradata/deo/pdb1/pdbseed_temp012016-07-
20_12-23-06-AM.dbf 
Alternatively, you can manually create a new temp file for the tablespace and drop the old
temp file:
SQL> alter tablespace temp add tempfile 
'/u01/app/oracle/oradata/deo/pdb1/pdb1_temp.dbf' size 100M; 
Tablespace altered. 
SQL> alter tablespace temp drop tempfile 
'/u01/app/oracle/oradata/deo/pdb1/pdbseed_temp012016-07-20_12-23-06-AM.dbf'; 
Tablespace altered. 

Media failure - control file loss
A control file is created at CDB level only and not for individual PDBs. A control file holds
information for the entire database (the CDB as well as PDBs). If a control file goes missing,
the database will crash and we will have to perform media recovery at CDB level.
Let's say we start a database and encounter the following error as the control file is missing:
SQL> startup 
ORACLE instance started. 
Total System Global Area  805306368 bytes 
Fixed Size                  2929552 bytes 
Variable Size             260050032 bytes 
Database Buffers          536870912 bytes 
Redo Buffers                5455872 bytes 
ORA-00205: error in identifying control file, check alert log for more info 
We can follow the following steps to perform media recovery at CDB level:
1. Restore the control file from auto-backup:
 [oracle@advait ~]$ rman target / 
 
Recovery Manager: Release 12.1.0.2.0 - Production on Thu Jul 21 11:04:17 
2016 
 
Copyright (c) 1982, 2014, Oracle and/or its affiliates.  All rights 
reserved. 
 
connected to target database: ORCL (not mounted) 
 
RMAN> restore controlfile from autobackup; 
 
Starting restore at 21-JUL-16 
using target database control file instead of recovery catalog 
allocated channel: ORA_DISK_1 
channel ORA_DISK_1: SID=12 device type=DISK 
 
recovery area destination: /u01/app/oracle/fast_recovery_area 
database name (or database unique name) used for search: ORCL 
channel ORA_DISK_1: AUTOBACKUP 
/u01/app/oracle/fast_recovery_area/ORCL/autobackup/2016_07_21/o1_mf_s_91777
9152_cs0p7rkh_.bkp found in the recovery area 
AUTOBACKUP search with format "%F" not attempted because DBID was not set 
channel ORA_DISK_1: restoring control file from AUTOBACKUP 
/u01/app/oracle/fast_recovery_area/ORCL/autobackup/2016_07_21/o1_mf_s_91777
9152_cs0p7rkh_.bkp 
channel ORA_DISK_1: control file restore from AUTOBACKUP complete 
output file name=/u01/app/oracle/oradata/deo/deo/control01.ctl 
output file name=/u01/app/oracle/fast_recovery_area/deo/deo/control02.ctl 
Finished restore at 21-JUL-16 
 
RMAN>  

2. Mount and recover the database:
RMAN> alter database mount; 
 
Statement processed 
released channel: ORA_DISK_1 
 
RMAN> recover database; 
 
Starting recover at 21-JUL-16 
Starting implicit crosscheck backup at 21-JUL-16 
allocated channel: ORA_DISK_1 
... 
... 
 
starting media recovery 
 
archived log for thread 1 with sequence 34 is already on disk as file 
/u01/app/oracle/fast_recovery_area/ORCL/archivelog/2016_07_21/o1_mf_1_34_cs
0pf836_.arc 
archived log for thread 1 with sequence 35 is already on disk as file 
/u01/app/oracle/fast_recovery_area/ORCL/archivelog/2016_07_21/o1_mf_1_35_cs
0pgr4m_.arc 
archived log for thread 1 with sequence 36 is already on disk as file 
/u01/app/oracle/fast_recovery_area/ORCL/archivelog/2016_07_21/o1_mf_1_36_cs
0phw6t_.arc 
... 
... 
3. Open the database in resetlogs and open the pluggable databases:
RMAN> alter database open resetlogs; 
 
Statement processed 
 
RMAN> alter pluggable database all open; 
 
Statement processed 
 
RMAN>  

Media failure - Redo log file loss
The redo log stream belongs to CDB and NOT PDBs. So, when we lose a redo log file, we
need to perform recovery at CDB level. Depending on the status of the online redo log file -
ACTIVE, INACTIVE, or CURRENT - we have to take the necessary procedure. Note that if you
lose a CURRENT online redo log and you don't have a multiplexed redo copy of that file, you
will end up performing an incomplete recovery and there will be data loss.
The procedure for performing recovery after losing a redo log file is the same as for the
CDB database as it was in previous releases.

Media failure - root SYSTEM or UNDO data file loss
If you lose a root container SYSTEM tablespace datafile or UNDO datafile, you need to
perform recovery at CDB level. We have only one UNDO tablespace, and that belongs to
CDB. All PDBs use the same UNDO tablespace present in CDB. The recovery procedure is the
same as with a non-CDB database. All PDBs and CDB will have to be shut down and
recovered in the mount phase.
You will see the following error when the file is missing:
ERROR at line 1: 
ORA-01157: cannot identify/lock data file 4 - see DBWR trace file 
ORA-01110: data file 4: 
'/u01/app/oracle/oradata/deo/deo/datafiles/undotbs01.dbf' 
The following are the steps required to recover your files:
1. Restore the missing datafile:
 [oracle@advait ~]$ rman target / 
 
Recovery Manager: Release 12.1.0.2.0 - Production on Fri Jul 22 01:49:29 
2016 
 
Copyright (c) 1982, 2014, Oracle and/or its affiliates.  All rights 
reserved. 
 
connected to target database: ORCL (DBID=1445436483, not open) 
 
RMAN> restore datafile 
'/u01/app/oracle/oradata/deo/deo/datafiles/undotbs01.dbf'; 
 
Starting restore at 22-JUL-16 
using target database control file instead of recovery catalog 
allocated channel: ORA_DISK_1 
channel ORA_DISK_1: SID=12 device type=DISK 
... 
... 
2. Recover the datafile:
RMAN> recover datafile 
'/u01/app/oracle/oradata/deo/deo/datafiles/undotbs01.dbf'; 
 
Starting recover at 22-JUL-16 
using channel ORA_DISK_1 
 
starting media recovery 
 
archived log for thread 1 with sequence 28 is already on disk as file 
/u01/app/oracle/fast_recovery_area/ORCL/archivelog/2016_07_20/o1_mf_1_28_cr
z5m4n3_.arc 
archived log for thread 1 with sequence 29 is already on disk as file 
/u01/app/oracle/fast_recovery_area/ORCL/archivelog/2016_07_20/o1_mf_1_29_cr

z5z0g6_.arc 
archived log for thread 1 with sequence 30 is already on disk as file 
/u01/app/oracle/fast_recovery_area/ORCL/archivelog/2016_07_20/o1_mf_1_30_cr
z75jdo_.arc 
... 
... 
3. Open CDB and all PDBs:
RMAN> alter database open; 
 
Statement processed 
 
RMAN> alter pluggable database all open; 
 
Statement processed 

Media failure - root SYSAUX data file loss
The SYSAUX tablespace is available in CDB, as well as in PDB, and since this is a non-
system tablespace, the database can continue to work while we take the datafile offline and
recover it. While we recover the SYSAUX tablespace, only the functionalities related to the
SYSAUX tablespace will be unavailable in CDB. All PDBs continue to work with all available
functionalities, as SYSAUX for PDBs are available.
You will get the following error:
ORA-01157: cannot identify/lock data file 3 - see DBWR trace file 
ORA-01110: data file 3: 
'/u01/app/oracle/oradata/deo/deo/datafiles/sysaux01.dbf' 
Following are the steps required to recover the SYSAUX tablespace data file:
1. Take the datafile offline and open the database:
SQL> alter database datafile 
'/u01/app/oracle/oradata/deo/deo/datafiles/sysaux01.dbf' offline; 
 
Database altered. 
 
SQL> alter database open; 
 
Database altered. 
Since losing one datafile can make SYSAUX functionality inaccessible, it's always good
to take the entire tablespace offline by:
     alter tablespace SYSAUX offline; 
2. Restore the SYSAUX datafile from backup:
RMAN> restore datafile 
'/u01/app/oracle/oradata/deo/deo/datafiles/sysaux01.dbf'; 
 
Starting restore at 22-JUL-16 
using channel ORA_DISK_1 
 
channel ORA_DISK_1: starting datafile backup set restore 
channel ORA_DISK_1: specifying datafile(s) to restore from backup set 
channel ORA_DISK_1: restoring datafile 00003 to 
/u01/app/oracle/oradata/deo/deo/datafiles/sysaux01.dbf 
... 
... 
3. Recover the datafile:
RMAN> recover datafile 
'/u01/app/oracle/oradata/deo/deo/datafiles/sysaux01.dbf'; 
 
Starting recover at 22-JUL-16 
using channel ORA_DISK_1 

 
starting media recovery 
 
archived log for thread 1 with sequence 28 is already on disk as file 
/u01/app/oracle/fast_recovery_area/ORCL/archivelog/2016_07_20/o1_mf_1_28_cr
z5m4n3_.arc 
archived log for thread 1 with sequence 29 is already on disk as file 
/u01/app/oracle/fast_recovery_area/ORCL/archivelog/2016_07_20/o1_mf_1_29_cr
z5z0g6_.arc 
archived log for thread 1 with sequence 30 is already on disk as file 
/u01/app/oracle/fast_recovery_area/ORCL/archivelog/2016_07_20/o1_mf_1_30_cr
z75jdo_.arc 
... 
... 
4. Make the SYSAUX data file/tablespace online:
RMAN> alter database datafile 
'/u01/app/oracle/oradata/deo/deo/datafiles/sysaux01.dbf' online; 
 
Statement processed 

Media failure - PDB SYSTEM data file loss
If we have multiple PDBs that are all up and running and lose the SYSTEM datafile for one of
the PDBs, then the entire CDB will crash.
I have two PDBs in my CDB. The following command from the V$CONTAINERS view shows the
same:
SQL> select name, open_mode from v$containers; 
NAME                           OPEN_MODE 
------------------------------ ---------- 
CDB$ROOT                       READ WRITE 
PDB$SEED                       READ ONLY 
PDB1                           MOUNTED 
PDB2                           READ WRITE 
I have removed the SYSTEM datafile of PDB1. I connected to PDB1 and tried performing a
checkpoint. I got an end-of-communication error:
SQL> alter system checkpoint; 
alter system checkpoint 
* 
ERROR at line 1: 
ORA-03113: end-of-file on communication channel 
Process ID: 1141 
Session ID: 110 Serial number: 37143 
I saw the following error in the alert log file:
Mon Jul 25 12:06:42 2016 
Errors in file /u01/app/oracle/diag/rdbms/orcl/orcl/trace/orcl_ckpt_827.trc: 
ORA-01243: system tablespace file suffered media failure 
ORA-01116: error in opening database file 10 
ORA-01110: data file 10: '/u01/app/oracle/oradata/deo/pdb1/system01.dbf' 
ORA-27041: unable to open file 
Linux-x86_64 Error: 2: No such file or directory 
Additional information: 3 
Immediately after that, the instance was also terminated, as seen in the following error from
the alert log:
Mon Jul 25 12:06:42 2016 
USER (ospid: 827): terminating the instance due to error 1243 
Mon Jul 25 12:06:42 2016 
System state dump requested by (instance=1, osid=827 (CKPT)), summary=[abnormal 
instance termination]. 
System State dumped to trace file 
/u01/app/oracle/diag/rdbms/orcl/orcl/trace/orcl_diag_815_20160725120642.trc 
Mon Jul 25 12:06:43 2016 
Dumping diagnostic data in directory=[cdmp_20160725120642], requested by 
(instance=1, osid=827 (CKPT)), summary=[abnormal instance termination]. 
Mon Jul 25 12:06:43 2016 

Instance terminated by USER, pid = 827 
To recover from such a situation, complete the following steps:
1. Start the CDB database:
SQL> startup 
 
ORACLE instance started. 
 
Total System Global Area  805306368 bytes 
Fixed Size                  2929552 bytes 
Variable Size             260050032 bytes 
Database Buffers          536870912 bytes 
Redo Buffers                5455872 bytes 
Database mounted. 
Database opened. 
We should not have any issues starting up the CDB, as all its files are intact.
2. Open all pluggable databases, except the one that lost the SYSTEM datafile. You can use
either one of the following statements:
alter pluggable database all open; 
or 
alter pluggable database pdb2 open; 
3. If you run all open, it will open all PDBs except the one that has the missing datafile,
and you can see that error in the command and alert logs:
SQL> select name, open_mode from v$containers; 
 
NAME                           OPEN_MODE 
------------------------------ ---------- 
CDB$ROOT                       READ WRITE 
PDB$SEED                       READ ONLY 
PDB1                           MOUNTED 
PDB2                           READ WRITE 
4. Restore and recover the SYSTEM datafile of PDB1:
RMAN> restore datafile '/u01/app/oracle/oradata/deo/pdb1/system01.dbf'; 
 
Starting restore at 25-JUL-16 
using target database control file instead of recovery catalog 
allocated channel: ORA_DISK_1 
channel ORA_DISK_1: SID=108 device type=DISK 
... 
... 
 
RMAN> recover datafile '/u01/app/oracle/oradata/deo/pdb1/system01.dbf'; 
 
Starting recover at 25-JUL-16 
using channel ORA_DISK_1 
 
starting media recovery 
 

archived log for thread 1 with sequence 28 is already on disk as file 
/u01/app/oracle/fast_recovery_area/ORCL/archivelog/2016_07_20/o1_mf_1_28_cr
z5m4n3_.arc 
archived log for thread 1 with sequence 29 is already on disk as file 
/u01/app/oracle/fast_recovery_area/ORCL/archivelog/2016_07_20/o1_mf_1_29_cr
z5z0g6_.arc 
... 
... 
5. Open the pluggable databasem, PDB1:
RMAN> alter pluggable database pdb1 open; 
 
Statement processed 
If PDB1 is down, and all the other PDBs are up and running, and we lose the SYSTEM datafile
for PDB1 (which is down), it will not impact any other PDB or CDB. They will continue to
function normally, and we can restore and recover the SYSTEM datafile of PDB1 without causing
any downtime to CDB or PDBs.

Media failure - PDB non-SYSTEM data file loss
This is similar to restoring and recovering tablespaces from a non-CDB database. You must
first connect to the PDB and take the affected tablespace offline. You can then restore and
recover the tablespace while users in other PDBs continue to work in those PDBs. The
affected PDB itself can also be open, as you don't need to close a database during tablespace
recovery unless you're recovering the SYSTEM tablespace. Only the affected tablespace will
be offline. Other tablespaces in that PDB can be open for read-write.

Performing PDB PITR (Point-in-Time Recovery)
A point-in-time recovery can be done on a database to take the database back in time. This is
done by restoring the database from back prior to the target time (to which you want to take
the database) and then applying either incremental backup or archives to recover the database
to the target time.
An easier way to take PDB back in time is by using flashback. But if you want to take back a
PDB that is beyond flashback retention, then you need to perform a PDB PITR (point-in-time
recovery). When you perform a point-in-time recovery of a PDB, it does not affect other
PDBs and CDB. They continue to remain open and perform normal functions for users. Once
we perform PDB PITR, we need to open PDB in resetlog. Opening PDB in resetlog changes
the incarnation of PDB, but it does not affect the incarnation of other PDBs or CDB.
When we perform resetlogs for PDBs:
A PDB record in the controlfile will be updated
Each redo record also carries a PDB ID in the redo header; this is how recovery will
know which redo entry belongs to which PDB
Conceptually, a PDB resetlog is similar to a database resetlog. An old backup of PDB will
remain valid and can be used for media failure.
A PDB incarnation is a sub-incarnation of CDB. For example, if CDB incarnation is 5 and a
PDB incarnation is 3, then we can specify the full incarnation of PDB as (5, 3). A PDB
incarnation starts with 0. We can check the incarnation details in the V$PDB_INCARNATION view:
SQL> select DB_INCARNATION#, con_id, PDB_INCARNATION#, PRIOR_PDB_INCARNATION#, 
status from v$pdb_incarnation order by 1,2; 
DB_INCARNATION#     CON_ID PDB_INCARNATION# PRIOR_PDB_ STATUS 
--------------- ---------- ---------------- ---------- ------- 
             1          1                0            PARENT 
             2          1                0 0          PARENT 
             2          2                0            PARENT 
             2          3                0            PARENT 
             2          4                0            PARENT 
             3          1                0 0          CURRENT 
             3          2                0 0          CURRENT 
             3          3                0 0          CURRENT 
             3          4                0 0          CURRENT 
The following steps are performed in a PDB PITR when we run the following commands:
run{ 
   set until time '24-JUL-2016'; 
   restore pluggable database pdb1; 
   recover pluggable database pdb1 auxiliary 
destination='/u01/app/oracle/oradata/aux'; 
   alter pluggable database PDB1 open resetlogs; 

} 
When we try to perform a PDB PITR, RMAN performs the following steps:
Restores the pluggable database PDB1 from a backup taken before the timestamp we set
using until time; this restore is done at the PDB datafile's location and not on an
auxiliary location.
Creates a new instance automatically with a system-generated name for SID - this creates
an automatic instance with SID='mhqk'.
Restores the control file for the auxiliary instance.
Mounts a clone database.
Restores the SYSTEM, UNDO, and USERS tablespaces from CDB. This is because the
UNDO tablespace is shared by all PDBs and it is not possible to recover the UNDO
tablespace for a specific PDB alone. So, RMAN restores a copy of the SYSTEM, UNDO,
and USERS tablespaces for the auxiliary instance into an auxiliary destination. Oracle
determines that these datafiles will be required for doing PDB PITR.
Switches to clone datafiles for recovery.
Recovers PDB datafiles using CDB clone datafiles until the timestamp, or SCN, is set as
per the UNTIL TIME clause.
After recovery, drops the auxiliary instance.
Note
When specifying the AUXILIARY DESTINATION, we need to make sure that we have
enough space to hold the cloned database. If we have an FRA (Fast Recovery Area)
configured, we don't have to use an auxiliary destination.
The following RMAN code shows how to perform a point-in-time recovery of a pluggable
database:
[oracle@advait ~]$ rman target / 
Recovery Manager: Release 12.1.0.2.0 - Production on Mon Jul 25 12:57:10 2016 
Copyright (c) 1982, 2014, Oracle and/or its affiliates.  All rights reserved. 
connected to target database: ORCL (DBID=1445436483) 
RMAN> run{ 
2> set until time '24-JUL-2016'; 
3> restore pluggable database pdb1; 
4> recover pluggable database pdb1 auxiliary 
destination='/u01/app/oracle/oradata/aux'; 
5> alter pluggable database PDB1 open resetlogs; 
6> } 
executing command: SET until clause 
Starting restore at 25-JUL-16 
using target database control file instead of recovery catalog 

allocated channel: ORA_DISK_1 
channel ORA_DISK_1: SID=38 device type=DISK 
channel ORA_DISK_1: starting datafile backup set restore 
channel ORA_DISK_1: specifying datafile(s) to restore from backup set 
channel ORA_DISK_1: restoring datafile 00010 to 
/u01/app/oracle/oradata/deo/pdb1/system01.dbf 
channel ORA_DISK_1: restoring datafile 00011 to 
/u01/app/oracle/oradata/deo/pdb1/sysaux01.dbf 
... 
... 
As you can see in the following code, PDB_INCARNATION has changed after opening PDB in
resetlogs. The following command shows PDB incarnation, and we can see that PDB
incarnation changed after we opened the database with resetlogs:
SQL> select DB_INCARNATION#, con_id, PDB_INCARNATION#, PRIOR_PDB_INCARNATION#, 
status from v$pdb_incarnation order by 1,2; 
DB_INCARNATION#     CON_ID PDB_INCARNATION# PRIOR_PDB_ STATUS 
--------------- ---------- ---------------- ---------- ------- 
             1          1                0            PARENT 
             2          1                0 0          PARENT 
             2          2                0            PARENT 
             2          4                0            PARENT 
             3          1                0 0          CURRENT 
             3          2                0 0          CURRENT 
             3          3                1 0          CURRENT 
             3          3                0 0          PARENT 
             3          3                0            PARENT 
             3          4                0 0          CURRENT 

PDB tablespace PITR
You can also perform a tablespace point-in-time recovery similar to the way we did in non-
CDB. In the case of CDB, when you perform a tablespace PITR, it doesn't affect CDB or other
PDBs. They continue to remain available to users. In addition, all other tablespaces in PDB in
which you are performing TS PITR remain available to users. The following command can
perform tablespace PITR for the USERS tablespace in PDB1:
recover tablespace PDB1:USERS until SCN 1435353241123 auxiliary destination 
'/u01/app/oracle/oradata/aux';

Performing a flashback of CDB
You can flashback an entire CDB to a previous point in time. This includes flashing back the
root container and all PDB databases. You cannot flashback a CDB beyond the flashback
retention period. So, if your flashback retention period is one day, you can only take your
CDB back by a maximum of one day.
The following points must be noted before performing a flashback of a database:
You cannot flashback only the root container or a single PDB. Flashback has to be done
on the entire CDB only.
You cannot flashback to a point earlier than the time when you last performed a PDB
point-in-time recovery (unless you use special steps, which we will mention later).
For a flashback to work, we need to:
Enable the archive log
Enable flashback for the database
I am assuming that archive log is enabled for the database. To enable flashback and set
retention, use the following commands. Check the flashback retention target. If the parameter
db_flashback_retention_target is not set, you can set it using the following command. The
value is in mins:
SQL> show parameters flashback 
 
NAME                                 TYPE        VALUE 
------------------------------------ ----------- ------------------------- 
db_flashback_retention_target        integer     1440 
 
SQL> alter system set db_flashback_retention_target = 1440; 
You can enable the flashback using the following command:
SQL> alter database flashback on; 
 
Database altered. 
You can check if flashback is on using the following command:
SQL> select flashback_on from v$database; 
 
FLASHBACK_ON 
------------------ 
YES 
Flashback logs are stored in the fast recovery area, so it's important to configure the FRA
when using flashback.

Performing a flashback of a database
You need to perform a flashback of a database in MOUNT mode only. After performing the
flashback of a database, you can open the database in read-only just to check that you have
flashbacked the database to the correct level. If not, you can, again, shut down and mount the
database and flashback to the correct point. If you have flashbacked the database to a point
earlier than required, you can recover the database to the correct time by applying archives
and performing a point-in-time recovery.
After flashback is complete, you have to open the CDB database in RESETLOGS. Let's drop a
user and try to perform a flashback using the following steps:
1. Create a user and note down the System Change Number (SCN):
sqlplus '/as sysdba' 
 
SQL> create user pdb1_user identified by oracle; 
 
User created. 
 
SQL> grant connect, resource to pdb1_user; 
 
Grant succeeded. 
 
SQL> select current_scn from v$database; 
 
    CURRENT_SCN 
--------------- 
        3129049 
2. Drop the user so that we can flashback and recover:
sqlplus sys/oracle@pdb1 as sysdba 
 
SQL> drop user pdb1_user; 
 
User dropped. 
3. Shut down and mount the DB:
shutdown the DB instance and mount the same 
 
SQL> shut immediate 
Database closed. 
Database dismounted. 
ORACLE instance shut down. 
SQL> startup mount 
ORACLE instance started. 
 
Total System Global Area  805306368 bytes 
Fixed Size                  2929552 bytes 
Variable Size             260050032 bytes 
Database Buffers          536870912 bytes 
Redo Buffers                5455872 bytes 

Database mounted. 
4. Flashback the database to the noted SCN:
SQL> flashback database to scn 3129049; 
 
Flashback complete. 
 
open in read only to verify if you got the required user. 
 
SQL> alter database open read only; 
 
Database altered. 
 
SQL> alter pluggable database all open read only; 
 
Pluggable database altered. 
 
SQL> alter session set container = pdb1; 
 
Session altered. 
 
SQL> select username from dba_users where username = 'PDB1_USER'; 
 
USERNAME 
------------------ 
PDB1_USER 
5. Shut down again and open resetlogs:
SQL> alter session set container = cdb$root; 
 
Session altered. 
 
SQL> shut immediate 
Database closed. 
Database dismounted. 
ORACLE instance shut down. 
SQL> startup mount 
ORACLE instance started. 
 
Total System Global Area  805306368 bytes 
Fixed Size                  2929552 bytes 
Variable Size             260050032 bytes 
Database Buffers          536870912 bytes 
Redo Buffers                5455872 bytes 
Database mounted. 
SQL> alter database open resetlogs; 
 
Database altered. 
 
SQL> alter pluggable database all open; 
 
Pluggable database altered. 
 
SQL>  

Flashing back CDB after a PDB PITR
If you have finished a PDB point in time recovery, you cannot flashback a CDB beyond that
point. For example, if you have performed a PDB PITR at SCN 100, you cannot flashback the
CDB below SCN 100. However, there is a way to rewind the database beyond this point. This
requires taking the datafiles offline from the PDB database on which you have performed
PDB PITR. This way, when we perform the flashback of the database, the offline datafiles will
be ignored and we can flashback the database beyond the point when PDB PITR was last
performed.
The following are the steps required to do so:
1. Identify the SCN or time at which you want to flashback the database.
2. For the PDB on which you have performed PDB PITR, you should take all the datafiles
of that PDB offline using the ALTER DATABASE DATAFILE ... OFFLINE command.
3. Shut down and mount the CDB.
4. Flashback the CDB to the required SCN or timestamp. Since the datafiles of PDB on
which we performed PDB PITR are offline, they remain unaffected.
5. Open the database in resetlogs.
6. Open all the pluggable databases, except the one on which you performed PDB PITR.
7. Restore the PDB on which you previously performed PDB PITR.
8. Recover the PDB to the correct point.
9. Open PDB with resetlogs.

Using RMAN enhancements
RMAN has been enhanced significantly over the years and Oracle 12c has added further
enhancements to the RMAN capabilities. The following are the top RMAN enhancements in
Oracle 12c:
The separation of DBA Duty - new SYSBACKUP privileges
SQL in RMAN
Multi-section image copies and multi-section incremental backup
RMAN active duplication enhancements
The duplication of container database with PDBs
Recovery of databases with a third-party snapshot
The transportation of data across platforms using backup sets:
Transporting databases
Transporting tablespaces
Automatic table recovery using RMAN
Let's take a look at each of these enhancements, starting with the SYSBACKUP privilege.

Separation of DBA duty - New SYSBACKUP privileges
Oracle 12c separates DBA duties from backup and recovery duties with the introduction of
new SYSBACKUP privileges. Until now, you needed SYSDBA privileges to perform RMAN
backup and recovery operations. But in Oracle 12c, a new privilege has been introduced
called SYSBACKUP. This privilege can be assigned to a DBA to perform backup, restore, and
recovery operations without having to assign the super user privilege - SYSDBA.
SYSBACKUP privilege can only be used to backup, restore, and recover a database. It also
has the privilege to shut down and start up databases. SYSBACKUP does not have the
privilege to access data from tables and objects, so it does not have the SELECT ANY TABLE
privilege.
You can further limit the privilege assignment to the user by assigning this privilege to the
local user of the PDB. That way, a user will only be able to back up, restore, and recover a
specific PDB. Additionally, only the local user of that PDB will be able to shut down and start
up the PDB.
Literally speaking, SYSBACKUP is a user in a database. When you connect to a database using
/as sysbackup, it actually connects as a SYSBACKUP user. You can check the following:
[oracle@advait dbs]$ sqlplus "/as sysbackup" 
SQL*Plus: Release 12.1.0.2.0 Production on Mon Jul 25 19:40:22 2016 
Copyright (c) 1982, 2014, Oracle.  All rights reserved. 
Connected to: 
Oracle Database 12c Enterprise Edition Release 12.1.0.2.0 - 64bit Production 
With the Partitioning, OLAP, Advanced Analytics and Real Application Testing 
options 
SQL> show user; 
USER is "SYSBACKUP" 
SQL> 
We can check in the DBA_USERS view if SYSBACKUP is a user:
SQL> select username from dba_users where username = 'SYSBACKUP'; 
USERNAME 
---------------------- 
SYSBACKUP 

Using SQL in RMAN
In the previous release, you were able to run SQL statements from the RMAN prompt, but this
required enclosing the command in quotes and prefixing it with the SQL keyword.
In this release, you can run an SQL statement without enclosing it in quotes and without
prefixing it with the SQL keyword. This even includes the describe command to check the table
structure.
For example:
RMAN> alter system switch logfile; 
Statement processed 
RMAN> desc dba_sys_privs 
Name                                      Null?    Type 
----------------------------------------- -------- ------------------ 
GRANTEE                                            VARCHAR2(128)             
PRIVILEGE                                          VARCHAR2(40)              
ADMIN_OPTION                                       VARCHAR2(3)               
COMMON                                             VARCHAR2(3)               
You can also execute statements such as ALTER TABLESPACE, CREATE DIRECTORY, and so on.
SQL commands that you issue from within RMAN have the same functionality as those that
you issue from the SQL*Plus interface. The way you have to specify SQL statements within
quotes is called SQL(Quoted), and this is still available in Oracle Database 12c for
compatibility reasons.
In order to distinguish RMAN commands from regular SQL commands, you must specify the
keyword SQL before executing the following RMAN commands:
DELETE executes the SQL DELETE command
DROP DATABASE executes the SQL DROP DATABASE command
FLASHBACK executes the SQL FLASHBACK command

Multi-section image copies and Multi-section incremental
backup
Until its previous release, RMAN was able to use multi-section functionality to break up large
files into sections and use multiple channels to back up these sections in parallel to create
backup sets. You can initiate multi-section backup by using the SECTION SIZE keyword. Multi-
section backup worked well while performing a full backup of a database or while
performing a back up of a big datafile. However, multi-section backups were limited to
creating backup sets only.
In Oracle 12c, RMAN has been enhanced to also create multi-section image copies. This
feature is powerful and comes in handy in a non- backup situation, such as when you're
transporting a large tablespace or creating a database with the active database duplication
feature.
Multi-section backups use multiple channels to back up large data files in parallel. While a
multi-section image backup is being performed, multiple channels write a file's sections, but
at the end of the backup there's only one copy for each data file being backed up.
To create multi-section incremental image copies, you should:
Set the COMPATIBLE parameter to 12.0.0 or higher
Use the backup command with the SECTION SIZE and AS COPY clauses
For example:
BACKUP AS COPY SECTION SIZE 1G DATABASE; 
RMAN has also been enhanced to utilize multi-section backups for taking incremental
backups. Until its previous release, RMAN multi-section backup only used to work for FULL
backups and not incremental ones. Now with Oracle 12c, we can use the SECTION SIZE clause
while performing incremental level 1 and above backups.
A few restrictions are required while performing a multi-section backup:
You can only create a multi-section incremental backup for datafiles, not for other
database files, such as a control file or spfile.
You should not apply a large value of parallelism when backing up a large file that
resides on a small number of disks. This is because it would defeat the purpose of a
parallel operation and the same disk would be competing to serve the request of multiple
parallel threads.

RMAN Active duplication enhancements
The RMAN active database duplication feature was introduced in Oracle 11g. In this release,
we have the following new enhancements to the RMAN duplicate feature:
The default use of backups set in active database duplication
The choice of compression, encryption, and section size
The option to complete duplication with the target database in mounted mode
Let's check out these enhancements in detail.
Default use of a backup set in active database duplication
With active database duplication, you don't need to back up a database to create a duplicate
database. Active database duplication directly accesses the active database, creates an image
copy on the fly, and transmits those image copies through Oracle Net to create a duplicate
database.
The following figure shows how active duplication works:
The following steps are performed by RMAN during active database duplication:
RMAN connects to the source database as TARGET
RMAN connects to the target database as AUXILIARY
The required files are backed up as image copies and are copied over from the source to
the auxiliary instance
RMAN then uses a memory script to recover and open the auxiliary database
This method of active database duplication is called the PUSH method as backups are
being pushed from the source to an auxiliary instance

In Oracle 12c, active duplication has been enhanced to use backup sets instead of image
copies. Backup sets provide a smaller size of backups, and so the time required to transfer the
backup is reduced significantly. This is now the default setting for doing active duplication. If
you want to use image copies, you need to specify them explicitly.
Also, the PUSH method that was used with image copies now uses the PULL method with backup
sets. In the case of the PULL method, the auxiliary instance retrieves the required database files
from the source database as backup sets. A restore operation is performed on an auxiliary
instance. Therefore, fewer resources are used on the source database.
Based on an active duplication clause, RMAN dynamically determines if it should use the PULL
method or PUSH method. The following are the clauses that tell RMAN to use the PULL method:
If you specify the USING BACKUPSET clause, RMAN will use the PULL method
If you specify the COMPRESSION clause, such as USING COMPRESSED BACKUPSET, RMAN
will use the PULL method
If you specify the SET ENCRYPTION clause before the DUPLICATE database command,
RMAN will use the PULL method
If you use multi-section during active duplication using the SECTION SIZE clause, RMAN
will use the PULL method
In the following command, the clauses in bold tell RMAN to use the PULL method:
RMAN> SET ENCRYPTION ...; 
RMAN> DUPLICATE TARGET DATABASE TO deo FROM ACTIVE DATABASE 
              [ USING BACKUPSET ] 
              [ SECTION SIZE ] 
              [ USING COMPRESSED BACKUPSET ] 
Introducing choice of compression, encryption, and section size
As mentioned previously, RMAN is enhanced for compression, encryption, and section size
parameters during active database duplication:
Compression is one of the new features of Oracle 12c in active database duplication.
During active database duplication, Oracle compresses the backup sets in order to reduce
the size of backup sets so that the transferal of files is faster and uses less network
bandwidth.
Using the SECTION SIZE clause in backup will divide the bigger datafiles into sections,
and we can use multiple RMAN threads to back up the section of datafiles making
backups quicker. We can use multi-section backup sets in active duplication to make our
backups run faster.
Using encryption secures the data that is being transferred over the network. This
provides extra security during active database duplication.
You can encrypt the backup sets you transfer over the network to the auxiliary database during
active duplication. You specify the encryption algorithm as part of the DUPLICATE command.
Before you can specify encryption during database duplication, you must do the following:

Share the Oracle key store between the target and the auxiliary database
Specify the encryption password with the following command.
Option to complete duplication with the target database in mounted mode
In its previous release, whenever we used to duplicate a database from the active database,
Oracle used to open the new target database using RESETLOGS and used to create online redo
logs. This functionality continues in Oracle 12c as well. However, in Oracle 12c, we have a
new option called NOOPEN, which finishes the active duplication process with the target
database kept in the mount stage only.
This is very useful in situations where we don't want our database to be opened with
RESETLOGS (for example, for upgrade purposes or if you want to move the database to ASM).
The NOOPEN option of active database duplication will allow you to create a new database and
will leave the database in the mounted state.
For example:
duplicate target database to orcl_dup from active database NOOPEN; 

Duplicating container databases with PDBs
You can duplicate a multi-tenant container database using RMAN. You can duplicate an entire
CDB, a single PDB, or a set of PDBs.
Duplicating CDB
Duplicating a whole CDB is similar to duplicating a non-CDB database, but with minor
changes. You need to perform the following steps to duplicate an entire CDB database:
1. Back up the target database.
2. Create a password file for the duplicate instance.
3. Add the required TNS names so that we can connect to the auxiliary instance.
4. Create an init.ora file for the auxiliary database. This will be our target database.
5. Start the auxiliary instance in the nomount mode. Make sure that the auxiliary instance is
a CDB database. You have to ensure that the ENABLE_PLUGGABLE_DATABASE parameter is
set to TRUE in ini.ora.
6. Connect to the target and auxiliary instances using RMAN. Make sure that you connect to
the root container of the target and auxiliary databases.
7. Run the duplicate command. This will duplicate the entire CDB database, including all
PDBs:
DUPLICATE TARGET DATABASE TO ORCL_DUP SPFILE NOFILENAMECHECK; 
Duplicating PDBs
You can duplicate one or more PDBs with the DUPLICATE command. By default, RMAN will
duplicate the root (CDB$ROOT) and the seed database (PDB$SEED) of the CDB that contains the
pluggable database when you duplicate a PDB. Thus, the duplicate database is actually a CDB
that contains the root, the seed database, and the duplicate PDB.
To duplicate a few PDB databases, you can execute the DUPLICATE command with either the
PLUGGABLE DATABASE clause, and provide a list of PDBs that you want to duplicate, or use the
SKIP PLUGGABLE DATABASE clause to duplicate all PDB databases except the one you mention
in the SKIP PLUGGABLE DATABASE clause.
For example:
To duplicate the single PDB database-PDB1 to a new container database, CDB_NEW:
DUPLICATE DATABASE TO CDB_NEW PLUGGABLE DATABASE PDB1; 
To duplicate multiple pluggable databases to a new container database, CDB_NEW:
DUPLICATE DATABASE TO CDB_NEW PLUGGABLE DATABASE PDB1, PDB3; 
To duplicate all pluggable databases except one:
DUPLICATE DATABASE TO CDB_NEW SKIP PLUGGABLE DATABASE PDB2; 

When duplicating the database, you must ensure that you set the ENABLE_PLUGGABLE_DATABASE
parameter to TRUE on the auxiliary instance.
You can also duplicate a tablespace from a PDB to a new auxiliary instance. In this case, the
RMAN process will also duplicate the ROOT container and SEED container, create the required
PDB, and copy only the mentioned tablespace.
For example, to duplicate only the USERS tablespace:
DUPLICATE DATABASE TO CDB_NEW TABLESPACE PDB1:USERS; 

Recovering a database with a third party snapshot
You can use snapshot technology to create OS level snapshots of database files as a means of
backing up a database. This snapshot technology creates an empty snapshot, which is nothing
but sets of pointers pointing to each block of datafiles. When the block is modified, the
original block is copied over to the snapshot area, so with time, the snapshot size grows.
Oracle 12c has incorporated using snapshot technology while recovering the database. RMAN
commands now understand third party snapshots and can be used for recovering a database.
But for snapshots to be valid and usable for recovery, snapshots should conform to the
following requirements:
A database should be crash-consistent during the snapshot
A snapshot should reserve the write order for each file
The snapshot time is the time at which snapshot has completed
Apart from the preceding requirements, we should also make sure not to make any structural
changes while a snapshot is in progress, otherwise that snapshot is not usable for recovery.
Operations on datafiles and tablespaces, such as OFFLINE, READONLY, ONLINE, DROP, RENAME,
and so on, make structural changes and can render a snapshot unusable.
The following are some examples of using SNAPSHOT to recover a database.
Example: Recovering a database using a particular snapshot:
RECOVER DATABASE UNTIL TIME '07/15/2016 10:00:00' snapshot time '07/15/2016 
08:00:00' 
In the previous example, we are using a snapshot taken at 07/15/2016 08:00 AM to recover a
database. The UNTIL TIME clause can specify any time after snapshot time.
The following example performs a partial recovery using archive logs file from a snapshot
taken at 07/18/2016 12:00:00:
RECOVER DATABASE UNTIL CANCEL snapshot time '07/18/2016 12:00:00' 

Transporting data across a platform using backup sets
In its previous release, Oracle enabled you to transport datafiles, tablespaces and databases
from one platform to another using image copies. If we have a platform with a different
endian format, RMAN allows us to change the endian format of the data to the target platform
endian format. In its previous release, Oracle allowed creating image copies only while
perforing the database or tablespace transport.
In Oracle 12c, you can now transport both tablespaces and entire databases with backup sets
without having to rely on image copies. Using backup sets rather than image copies for
transporting data across platforms provides several advantages: you can choose to compress
the data and also take advantage of the multi-section option, so as to speed up the data
transport. You can also perform inconsistent tablespace transport, meaning that you don't have
to put a tablespace in read-only mode in order to transport it to a different platform. This is
allowed using a new clause, ALLOW INCONSISTENT.
Transporting databases
The following are important things to note when performing cross platform database
transports using backup sets. We will also look into the detailed steps:
The COMPATIBLE parameter should be set to 12.0.0.
To transport an entire database, you have to put the database in the READ ONLY mode as
the SYSTEM and SYSAUX tablespaces are also involved in transport.
To transport only tablespaces, use the DATAPUMP clause in the RMAN command. The
database must be opened in read write mode.
The steps for transporting a database are as follows:
1. Verify the prerequisite:
COMPATIBLE must be set to 12.0.0 or above
The database should be in read only mode
2. Query the exact name of the destination platform using the V$TRANSPORTABLE_PLATFORM
view:
SQL> select * from v$transportable_platform order by 1;
PLAT PLATFORM_NAME                  ENDIAN_F CON_ID
---- ------------------------------ -------- ------
   1 Solaris[tm] OE (32-bit)        Big           0
   2 Solaris[tm] OE (64-bit)        Big           0
   3 HP-UX (64-bit)                 Big           0
   4 HP-UX IA (64-bit)              Big           0
   5 HP Tru64 UNIX                  Little        0
   6 AIX-Based Systems (64-bit)     Big           0
   7 Microsoft Windows IA (32-bit)  Little        0
   8 Microsoft Windows IA (64-bit)  Little        0
   9 IBM zSeries Based Linux        Big           0
…

3. Back up the database using the BACKUP command. If you are transporting the database to
a different platform, you should use the CONVERT clause in the BACKUP command to
convert the data to the required platform. We have two options for converting the data:
Convert at source host:
You can use the BACKUP TO PLATFORM '<destination platform>' .. command to
convert the data to the destination platform. This conversion happens while doing the
backup on the source database host.
For example:
BACKUP TO PLATFORM 'Solaris Operating System (x86-64)' FORMAT 
/u01/backups/%U' DATABASE
Convert at the destination host:
If you want to perform conversion on the destination host, you can perform a backup
using the FOR TRANSPORT clause. This will not perform the conversion on the source
host, but the conversion will happen during restore on the destination host:
Example 
BACKUP FOR TRANSPORT FORMAT /u01/backups/%U' DATABASE 
4. Connect to the destination host as target and perform the complete restore of the
database. Again, the restore command for restoring the database on the destination host
depends on whether we have performed the conversion of data on the source host or not.
The following are two options for restoring the data:
5. Once backup is complete, move the backup sets to the destination host for restore.
Conversion happened on the source host:
In this case, you don't have to perform any conversion of data on the destination host. The
following command will restore the already converted backup sets to the destination host:
RESTORE FROM PLATFORM 'Linux x86 64-bit' ALL FOREIGN DATAFILES FORMAT 
'/u01/app/oracle/oradata/orcl/%U' FROM BACKUPSET '/u01/backups/%U'; 
If the conversion of data has to be done on the destination host:
In this case, you need to convert the data to the format of the destination platform. You can use
the FOREIGN DATABASE clause to perform the conversion:
RESTORE FOREIGN DATABASE TO NEW FORMAT '/u01/app/oracle/oradata/orcl/%U' FROM 
BACKUPSET '/u01/backups/%U'; 
Transporting tablespaces
The steps for transporting tablespaces are as follows:

1. Verify the prerequisites:
COMPATIBLE must be 12.0.0 or more.
The database must be in READ WRITE mode.
The tablespace must be in read only or we should be using the ALLOW INCONSISTENT
clause.
2. Query the exact name of the destination platform using the V$TRANSPORTABLE_PLATFORM
view.
3. If the tablespace is not in read only, you should convert the tablespace to read only.
4. Back up the required tablespaces using either the TO PLATFORM or FOR TRANSPORT clauses
depending on whether you want to convert data on the source or destination host. Use the
DATAPUMP clause to indicate that an export dump file for the tablespace metadata must be
created. This dump file holds the metadata of the tablespace.
Note
A new backup clause, ALLOW INCONSISTENT, enables you to back up tablespaces that are
not in read only mode. Although the backup is created, we cannot plugin these databases
directly to the destination database because they are inconsistent. We should make them
consistent by taking another incremental backup of a tablespace in read only mode and
applying the incremental backup to previous inconsistent backups. Incremental backups
can be taken at a later point and takes less time.
5. Once backup is complete, move the backup sets to the destination host for restore.
6. Connect to the destination host as the target and perform the restore of the tablespace
backup and data pump export. The restore command depends on whether data conversion
happened on the source host during backup or if conversion should be done on the
destination host during report.
If conversion happened on the source host:
You can simply restore the tablespace using the FOREIGN TABLESPACE clause, as follows:
RESTORE FOREIGN TABLESPACE USERS FORMAT '/u01/app/oracle/oradata/orcl/' FROM
BACKUPSET '/u01/backups/%U' DUMP FILE FROM BACKUPSET 
'/u01/backups/users.dmp'; 
If conversion has to be done on the destination host:
You have to use the TO NEW clause of the restore command to automatically convert the
data format from the source platform to the destination platform. The following is the
command:
RESTORE FOREIGN TABLESPACE USERS TO NEW FORMAT 
'/u01/app/oracle/oradata/orcl/' FROM BACKUPSET '/u01/backups/%U' DUMP FILE 
FROM BACKUPSET '/u01/backups/users.dmp'; 
In the context of backup set-based RMAN duplication, a foreign data file or a foreign database

indicates the source database data files and database. These are the data files and tablespaces
that are part of the transportation process and are eventually plugged into the destination
database. The DATAPUMP clause refers to the location you specify for storing data pump files,
including the export dump file and log files.
If you skip the FORMAT clause in the BACKUP command, you must ensure that you set the
DB_FILE_CREATE_DEST initialization parameter in the parameter file for the target database.
The data files that are transported from the source database to the target database are called
foreign data files and the tablespace that contains them is referred to as a foreign tablespace.
In the RESTORE FOREIGN TABLESPACE command, the  BACKUPSET clause refers to the RMAN
backup sets created earlier in the source database. RMAN will restore the data from these
backup sets.

Automatic table recovery using RMAN
Table recovery was possible in earlier releases as well. Until previous releases, if we wanted
to recover a table, we had the following options:
Database point in time recovery (DBPITR)
Tablespace point in time recovery (TSPITR)
Flashback technology
In Oracle 12c, RMAN has been enhanced to perform the recovery of a table. We have a new
command in RMAN that automates the complete process of recovering the table. The new
process does not affect the existing objects in the database and reduces the time and space
required for performing table recovery.

Recovering a table
The following are the various ways for recovering a table:
RMAN uses the database backups taken previously to the recovery table or table
partitions to a specified time. You need to provide the following inputs to RMAN:
The table name or partition names to be recovered
The point in time until which the recovery should be done
Whether the recovered tables should be imported back to the original database
RMAN determines the correct backup based on your inputs
RMAN creates an auxiliary instance
RMAN restores the controlfile
RMAN restores the necessary files required for obtaining the older image of the table;
this might include SYSTEM, SYSAUX, UNDO tablespaces, and the tablespace
that contains the required table
RMAN recovers the table or table partitions in the auxiliary instance until the specified
time
RMAN creates the data pump export dump file that contains the recovered objects
RMAN imports the recovered objects into the original database

Recovery point-in-time options
You can recover a table or table partition to a past point in time by specifying one of the
following three clauses:
UNTIL SCN: The system change number (SCN)
UNTIL TIME: Time in the NLS_DATE_FORMAT environment variable
UNTIL SEQUENCE: The log sequence number and thread number

Steps for performing table recovery
Let's take an example. I have a table, T1, created in an ORCL non-CDB database, which has
three records. I have performed a complete backup of the database regularly and the database
is running in the archive log mode. Note that table recovery does not work in a CDB database.
I have tried performing a table recovery of a table in the PDB database and it did not work.
I got the following error:
RMAN-00571: =========================================================== 
RMAN-00569: =============== ERROR MESSAGE STACK FOLLOWS =============== 
RMAN-00571: =========================================================== 
RMAN-03002: failure of recover command at 07/26/2016 09:28:49 
RMAN-05063: Cannot recover specified tables 
RMAN-05057: Table PDB1_ADMIN.T1 not found 
I have a table, T1, in a non-CDB database and I will drop this table, T1, and recover it using
table recovery:
[oracle@advait ~]$ sqlplus advaitd/oracle@deo 
SQL> select * from T1; 
     COL1 
---------- 
   1 
   2 
   3 
SQL> select current_scn from v$database; 
CURRENT_SCN 
----------- 
    722729 
In the previous output, we saw that the T1 table has three records and we noted down the SCN
number. In the following command, we are going to drop this T1 table:
SQL> drop table T1; 
Table dropped. 
Use the following command to recover this table. Note that the SCN number we are giving is
the SCN number we noted down when the table was existing:
RMAN> recover table advaitd.t1 until scn 722729 auxiliary destination 
'/u01/app/oracle/oradata/aux'; 
Let's see what this command does exactly:
1. This command creates and mounts a new adhoc dummy instance, as shown in the
following code:

RMAN> recover table advaitd.t1 until scn 722729 auxiliary destination 
'/u01/app/oracle/oradata/aux'; 
 
Starting recover at 26-JUL-16 
using target database control file instead of recovery catalog 
allocated channel: ORA_DISK_1 
channel ORA_DISK_1: SID=107 device type=DISK 
RMAN-05026: WARNING: presuming following set of tablespaces applies to 
specified Point-in-Time 
 
List of tablespaces expected to have UNDO segments 
Tablespace SYSTEM 
Tablespace UNDOTBS1 
 
Creating automatic instance, with SID='jCvj' 
It uses the controlfile autobackup to restore the controlfile to mount the dummy instance.
2. It then restores the controlfile for the auxiliary instance.
The following memory script shows the commands for restoring the controlfile:
contents of Memory Script: 
{ 
# set requested point in time 
set until  scn 1243251; 
# restore the controlfile 
restore clone controlfile; 
  
# mount the controlfile 
sql clone 'alter database mount clone database'; 
  
# archive current online log  
sql 'alter system archive log current'; 
} 
3. Next, it restores the tablespaces required to perform tablespace PITR.
The following memory script shows that it's going to restore the required datafiles (until
SCN 722729) into its auxiliary destination and switch the datafiles to copy.
When we use UNTIL SCN, RMAN restores the datafiles that are a little older than this
SCN:
contents of Memory Script: 
{ 
# set requested point in time 
set until  scn 722729; 
# set destinations for recovery set and auxiliary set datafiles 
set newname for clone datafile  1 to new; 
set newname for clone datafile  3 to new; 
set newname for clone datafile  2 to new; 
set newname for clone tempfile  1 to new; 
# switch all tempfiles 
switch clone tempfile all; 

# restore the tablespaces in the recovery set and the auxiliary set 
restore clone datafile  1, 3, 2; 
  
switch clone datafile all; 
} 
4. Recover the datafiles until the required SCN.
The following memory script shows that RMAN tried to recover these datafiles until the
specified SCN. Once recovered, it opened the database in read only mode:
contents of Memory Script: 
{ 
# set requested point in time 
set until  scn 722729; 
# online the datafiles restored or switched 
sql clone "alter database datafile  1 online"; 
sql clone "alter database datafile  3 online"; 
sql clone "alter database datafile  2 online"; 
# recover and open database read only 
recover clone database tablespace  "SYSTEM", "UNDOTBS1", "SYSAUX"; 
sql clone 'alter database open read only'; 
} 
5. Create the SPFILE for the auxiliary instance and mount the DB again.
The following memory script tell us that RMAN has created SPFILE for the auxiliary
instance and included the CONTROL_FILES parameter in SPFILE and mounted the auxiliary
instance again:
contents of Memory Script: 
{ 
   sql clone "create spfile from memory"; 
   shutdown clone immediate; 
   startup clone nomount; 
   sql clone "alter system set  control_files =  
  ''/u01/app/oracle/oradata/aux/DEO/controlfile/o1_mf_csgwkf6f_.ctl'' 
comment= 
 ''RMAN set'' scope=spfile"; 
   shutdown clone immediate; 
   startup clone nomount; 
# mount database 
sql clone 'alter database mount clone database'; 
} 
6. Restore the USERS tablespace where the table belongs.
The following memory script shows that RMAN is trying to restore the USERS
tablespace (datafile 0004) and is switching to the copy:
contents of Memory Script: 
{ 
# set requested point in time 
set until  scn 722729; 
# set destinations for recovery set and auxiliary set datafiles 

set newname for datafile  4 to new; 
# restore the tablespaces in the recovery set and the auxiliary set 
restore clone datafile  4; 
  
switch clone datafile all; 
} 
7. Recover the auxiliary database until the required SCN.
The following memory script shows that RMAN is trying to recover the USERS tablespace
and other tablespaces that it has restored previously to the required SCN. It also opened
the database in resetlogs:
contents of Memory Script: 
{ 
# set requested point in time 
set until  scn 722729; 
# online the datafiles restored or switched 
sql clone "alter database datafile  4 online"; 
# recover and open resetlogs 
recover clone database tablespace  "USERS", "SYSTEM", "UNDOTBS1", "SYSAUX" 
delete archivelog; 
alter clone database open resetlogs; 
} 
8. Create the directory for the datapump export and export the required table.
The following memory script shows that RMAN is creating a directory for exporting the
table and it has also exported the table:
contents of Memory Script: 
{ 
# create directory for datapump import 
sql "create or replace directory TSPITR_DIROBJ_DPDIR as '' 
/u01/app/oracle/oradata/aux''"; 
# create directory for datapump export 
sql clone "create or replace directory TSPITR_DIROBJ_DPDIR as '' 
/u01/app/oracle/oradata/aux''"; 
} 
executing Memory Script 
 
sql statement: create or replace directory TSPITR_DIROBJ_DPDIR as 
''/u01/app/oracle/oradata/aux'' 
 
sql statement: create or replace directory TSPITR_DIROBJ_DPDIR as 
''/u01/app/oracle/oradata/aux'' 
 
Performing export of tables... 
   EXPDP> Starting "SYS"."TSPITR_EXP_jCvj_rjBm":   
   EXPDP> Estimate in progress using BLOCKS method... 
   EXPDP> Processing object type TABLE_EXPORT/TABLE/TABLE_DATA 
   EXPDP> Total estimation using BLOCKS method: 64 KB 
   EXPDP> Processing object type TABLE_EXPORT/TABLE/TABLE 
   EXPDP> Processing object type 
TABLE_EXPORT/TABLE/STATISTICS/TABLE_STATISTICS 

   EXPDP> Processing object type TABLE_EXPORT/TABLE/STATISTICS/MARKER 
   EXPDP> . . exported "ADVAITD"."T1"                              5.062 
KB       3 rows 
   EXPDP> Master table "SYS"."TSPITR_EXP_jCvj_rjBm" successfully 
loaded/unloaded 
   EXPDP> 
*************************************************************************
***** 
   EXPDP> Dump file set for SYS.TSPITR_EXP_jCvj_rjBm is: 
   EXPDP>   /u01/app/oracle/oradata/aux/tspitr_jCvj_52000.dmp 
   EXPDP> Job "SYS"."TSPITR_EXP_jCvj_rjBm" successfully completed at Tue Jul 
26 19:55:13 2016 elapsed 0 00:00:21 
Export completed 
9. Import the table to the original database.
The following memory script shows that RMAN is trying to import the table to the
original database. You can provide additional options to prevent RMAN from importing
the table or you can provide import options to remap the table to a different name if
required, or append the content to an existing table:
contents of Memory Script: 
{ 
# shutdown clone before import 
shutdown clone abort 
} 
executing Memory Script 
 
Oracle instance shut down 
 
Performing import of tables... 
   IMPDP> Master table "SYS"."TSPITR_IMP_jCvj_tCzd" successfully 
loaded/unloaded 
   IMPDP> Starting "SYS"."TSPITR_IMP_jCvj_tCzd":   
   IMPDP> Processing object type TABLE_EXPORT/TABLE/TABLE 
   IMPDP> Processing object type TABLE_EXPORT/TABLE/TABLE_DATA 
   IMPDP> . . imported "ADVAITD"."T1"                              5.062 
KB       3 rows 
   IMPDP> Processing object type 
TABLE_EXPORT/TABLE/STATISTICS/TABLE_STATISTICS 
   IMPDP> Processing object type TABLE_EXPORT/TABLE/STATISTICS/MARKER 
   IMPDP> Job "SYS"."TSPITR_IMP_jCvj_tCzd" successfully completed at Tue Jul 
26 19:55:24 2016 elapsed 0 00:00:04 
Import completed 
10. Finally, RMAN will clean up and remove the auxiliary instance:
Removing automatic instance 
Automatic instance removed 
auxiliary instance file 
/u01/app/oracle/oradata/aux/DEO/datafile/o1_mf_temp_csgwmp8s_.tmp deleted 
auxiliary instance file 
/u01/app/oracle/oradata/aux/JCVJ_PITR_DEO/onlinelog/o1_mf_2_csgwo8fy_.log 
deleted 
auxiliary instance file 
/u01/app/oracle/oradata/aux/JCVJ_PITR_DEO/onlinelog/o1_mf_1_csgwo7ht_.log 

deleted 
auxiliary instance file 
/u01/app/oracle/oradata/aux/JCVJ_PITR_DEO/datafile/o1_mf_users_csgwo53j_.db
f deleted 
auxiliary instance file 
/u01/app/oracle/oradata/aux/DEO/datafile/o1_mf_sysaux_csgwkm4b_.dbf deleted 
auxiliary instance file 
/u01/app/oracle/oradata/aux/DEO/datafile/o1_mf_undotbs1_csgwkm4f_.dbf 
deleted 
auxiliary instance file 
/u01/app/oracle/oradata/aux/DEO/datafile/o1_mf_system_csgwkm2x_.dbf deleted 
auxiliary instance file 
/u01/app/oracle/oradata/aux/DEO/controlfile/o1_mf_csgwkf6f_.ctl deleted 
auxiliary instance file tspitr_jCvj_52000.dmp deleted 
Finished recover at 26-JUL-16 
 
RMAN>  
11. You can verify if you got the required table or not by using the following command:
SQL> select * from T1; 
 
      COL1 
---------- 
    1 
    2 
    3 

Implementing new features of Flashback Data
Archive
With flashback, Oracle 12c has introduced the following new features.

Database Hardening - enabling Flashback Data Archive for
Security-Related Application Tables
This features extends the Flashback Data Archive (FDA) feature to providea full history of
security sensitive tables of an application. This feature provides a single command to enable
FDA on all the designated tables for an application and addresses the need for strong auditing
for these tables. This feature also allows the administrator to make all security tables in an
application read-only with a single command.
Database hardening makes it easy to track the history for all security-related tables in an
application and to make those tables read-only as needed, without the need for writing scripts
to loop through all the tables or other manual operations. Extending Flashback Data Archive
support for tables grouped together by an application makes it easier to track all the changes
made to those tables and to access the history using Oracle Flashback Query.

Flashback Data Archive improvements
Several improvements have been made to the FDA:
User-context tracking: The metadata information for tracking transactions, including
the user context, is now tracked, making it easier to determine which user made what
changes to a table.
Hybrid Columnar Compression (HCC): FDA can now be fully utilized on HCC
compressed tables on Exadata and other Oracle storage platforms.
Import and export of history: Support for importing user-generated history into FDA
tables has been added. Customers who have been maintaining history using some other
mechanism, such as triggers, can now import that history into FDA.

Optimization for Flashback Data Archive History Tables
With Oracle 12c, you can now optimize the history tables created with flashback data archives.
These history tables are used for querying point in time data, and having optimized history
tables improves the performance of flashback queries. Oracle 12c added an "OPTIMIZE
DATA" clause when creating the flashback data archive, which will enable optimization of
history tables.

Summary
We started off this chapter with RMAN and how RMAN works in a multitenant environment.
We took a very practical approach and showed you various scenarios of backing up and
recovering container databases and pluggable databases. We then looked into the flashback of
databases and how flashback works in a multitenant environment. We also took practical
examples of performing a flashback of a database, and we performed a point in time recovery
of a database using flashback. We also saw new RMAN enhancements, including new
SYSBACKUP privileges, multi-section image copies, enhancements in active database
duplication and so on. We also deep dived into how to transport data across platforms and
showed you detailed steps on how it works. We then saw one of the cool features of Oracle
12c--how to recover a table using a simple RMAN command, as well as the various steps that
RMAN completes in the backend in order to restore and recover a table. Finally, we saw a few
new improvements that have been made to the flashback data archive.
In the next chapter, we are going to look into resource manager enhancements in Oracle 12c
and enhancements related to indexes and tables. We will also look into the new features of
ADR (Automatic Diagnostic Repository) and a few network enhancements.

Chapter 8. Database Administration
Enhancements
Tables are the logical structures that store the entire application data inside a database. They
logically organize the content and form relationships with other tables. Indexes assist
applications in faster searching of content in the table. Tables and indexes are the most
important entities in the Oracle database. Oracle 12c has introduced many enhancements
related to table and indexes. We also have a few enhancements related to resource
management as Oracle 12c has introduced a major architecture change in the form of
multitenant. In this chapter, we are going to cover the following major topics:
Resource manager and other performance enhancements
Index and table enhancements
ADR and network enhancements

Resource manager
Oracle database resource manager enables you to manage allocating of resources to different
sessions in databases that are fighting for common resources such as CPU. In today's
applications, where we have several application users spawning multiple sessions to a
database, it's very essential to manage the user of resources to the application sessions.
Operating systems allocate equal resources to every process that is running on the host. These
processes could be an Oracle database background process or they could be a critical
frontend application session or a background ETL job in the database. But allocating equal
resources to every process is not an efficient way to manage applications and databases.
There are many scenarios where certain groups of sessions or processes spawned by specific
application logic should be given more priority over others.
Oracle database resource manager helps DBAs in allocating resources to different groups of
application processes. It also prevents overuse of resources by certain processes that run
resource intensive queries. resource manager allows you to classify your sessions into
groups and allocate different resources to each group. So you can classify certain parts of
your application or calls to certain APIs, which are critical into a single group and allocate
more resources. Similarly, you can classify your batch jobs into another group and allocate
less resources to that group. resource manager provides control over how hardware
resources are allocated.
Resource manager is an old feature of Oracle database, but Oracle 12c comes with new
enhancements related to CDB and PDB. Since Oracle 12c is a container database, resource
manager has to manage resource allocations at CDB level to allocate resources to different
PDBs that are plugged into CDB and also allocate resources inside a PDB.
In a CDB, resource manager can manage resources at two basic levels:
CDB level: At CDB level, resource manager works at a high level and manages the
allocation of resources to different PDBs that are running in CDB. You can decide the
amount of resources that are allocated to each PDB. You can also set a cap (higher limit)
to the amount of resources a PDB can utilize.
PDB level: Resource manager has to manage resources inside a PDB. This is very
similar to resource manager in previous releases, with some new limitations. We will
discuss those limitations in this section.
In order to deal with the inter and intra-PDB resource allocation needs, Oracle resource
manager allocates a portion of a server's resources to each PDB and also takes care of the
resource allocation to sessions connected to each PDB.

Managing resources between PDBs
In previous releases of Oracle, resource manager used to manage resources within the
database instance. In Oracle 12c, we have a container database (CDB) and pluggable
database (PDB). So resource manager has to manage resources between various PDBs and
within PDB.
Oracle 12c uses the concept of shares to allocate resource between PDBs. The concept of a
share helps you prioritize the resources used by the PDBs. A higher share value for a PDB
means more guaranteed resources for that PDB. Different PDBs can be allocated different
number of shares of the system resources, thus letting the resource manager allocate more
resources to the more important PDBs. So if certain PDBs are critical and important, you can
allocate more shares to that PDB. Shares control the following resources:
CPU
Exadata I/O
Parallel servers
Shares are allocated to PDB by creating plan directives for each PDB using the
CREATE_CDB_PLAN_DIRECTIVE procedure of the DBMS_RESOURCE_MANAGER package. Each
directive can refer to only one PDB. We cannot have multiple PDBs listed in a directive.
In addition, limits can be used to restrict the max resources used by a PDB. While shares
decide the minimum amount of resources a PDB can get, utilization limits put a cap on the
maximum amount of resources a PDB can use. You can limit max CPU utilization, exadata
I/O, and max parallel server processes that a PDB can utilize using the following two
parameters:
utilization_limit parameter in the CREATE_CDB_PLAN_DIRECTIVE procedure is
expressed as a percentage of system resources that the PDB can use. This parameter
limits the amount of CPU and Exadata I/O that a PDB can use. The resource manager
throttles the PDB sessions to ensure that PDB's CPU utilization does not exceed the
utilization limit specified by this parameter for that PDB.
The parallel_server_limit parameter in the CREATE_CDB_PLAN_DIRECTIVE procedure
is expressed as a percentage of parallel_server_target. This parameter provides the
maximum number of parallel servers a PDB can be expressed as a percentage of
parallel_server_target:
For example, if parallel_server_target is 64 and you define
parallel_server_limit as 50% for a PDB, that PDB can use (50% of 64) 32
parallel servers. If the number of parallel execution servers used by the PDB crosses
the limit specified by the PARALLEL_SERVERS_TARGET parameter value times the
value of the PARALLEL_SERVER_LIMIT parameter, the resource manager will queue
parallel queries.
If you do not define or create a directive for a PDB, the PDB uses default directive. A default

directive is the one which is always present in CDB and is assigned to PDB, which does not
have any directive explicitly assigned to it.
The default directive has one share allocated with utilization_limit of 100% and
parallel_server_limit of 100%. We can also change the allocation limits for default
directives using the DBMS_RESOURCE_MANAGER.UPDATE_CDB_DEFAULT_DIRECTIVE procedure.
Let's check out a few examples. Consider example one, where we have a CDB and we have
three PDBs plugged into CDB.
In this example, we assign one share to each PDB. So each PDB gets an equal amount of
resources in terms of CPU, Exadata I/O, and parallel server processes.
When we add a new PDB, we don't have to change any directive or recompute the resources.
Oracle automatically recomputes and assigns the resources. So if we add a fourth PDB with
one share, each PDB will now get 25% resources instead of 33%.
In each of the preceding PDBs, we haven't specified any utilization_limit or
parallel_server_limit. So each PDB can get up to 100% of system resources.
If you have a PDB that is critical and you want to assign more resources, you can create a plan
directive to assign more shares to critical PDB. In the preceding example, if PDB4 is critical,
we can assign two shares to PDB4. In that case, each PDB - PDB1, PDB2, and PDB3 will have
one share each and will get 20% of system resources guaranteed, whereas PDB4 being
assigned two shares, will get 40% of system resources, as shown in the following figure:

All this computation of resource percentage is done by Oracle. We just have to assign the
shares. We can also put an upper limit on the amount of system resources a PDB is allowed to
use. We can do that by using the following parameters in the CREATE_CDB_PLAN_DIRECTIVE
procedure, as discussed previously:
utilization_limit: This parameter puts an upper limit on absolute CPU utilization for a
consumer group
parallel_server_limit: This parameter puts an upper limit on the percentage of
parallel execution servers pool that a consumer group can use
These parameters are always expressed in percentage. Default values of these parameters are
100%.
Note
Note that if we don't use these parameters, PDB is allowed to use 100% of system resources.
We can update the default limit setting by using the
DBMS_RESOURCE_MANAGER.UPDATE_CDB_DEFAULT_DIRECTIVE procedure.

Creating and enabling a CDB resource plan
Let's take a full example of creating a CDB resource plan. We will understand what resource
plan directives we need to create and we will also show the commands to do so:
PDB/Directive
Shares Utilization limit Parallel server limit
(Default allocation) 1
100%
100%
PDB1
1
50%
50%
PDB2
1
50%
50%
PDB3
2
100%
100%
In this example, we have three PDBs plugged into CDB-PDB1, PDB2, and PDB3. Share
allocations, utilization limits, and parallel server limits are shown in the preceding table. We
are not changing any attributes of the default allocation. The following are the steps to create
and activate the required resource manager plan:
1. Create a pending area: A pending area is a staging area to create new resource plans,
update existing resource plans, or delete a plan. When you create a pending area, the
database initializes it and creates a new plan, which you must validate and submit by
validating and submitting the pending area:
SQL> exec dbms_resource_manager.create_pending_area(); 
 
PL/SQL procedure successfully completed. 
2. Create a CDB resource plan: We will first create a resource plan and then we will create
directives for this plan:
SQL> BEGIN 
  2  dbms_resource_manager.create_cdb_plan( 
  3  plan      => 'example_plan', 
  4  comment   => 'This is an example plan for ORCL CDB' 
  5  ); 
  6  END; 
  7  / 
 
PL/SQL procedure successfully completed. 
3. Create a CDB resource plan directive for each PDB, as per the preceding table:
For PDB1:

SQL> BEGIN 
  2  dbms_resource_manager.create_cdb_plan_directive( 
  3  plan                    => 'example_plan', 
  4  pluggable_database      => 'pdb1', 
  5  shares                  => 1, 
  6  utilization_limit       => 50, 
  7  parallel_server_limit   => 50); 
  8  END; 
  9  / 
 
PL/SQL procedure successfully completed. 
For PDB2:
SQL> BEGIN 
  2  dbms_resource_manager.create_cdb_plan_directive( 
  3  plan                    => 'example_plan', 
  4  pluggable_database      => 'pdb2', 
  5  shares                  => 1, 
  6  utilization_limit       => 50, 
  7  parallel_server_limit   => 50); 
  8  END; 
  9  / 
 
PL/SQL procedure successfully completed. 
For PDB3:
For PDB3, we have to set utilization_limit and parallel_server_limit to 100%. We
can do this by setting a null value to these parameters in create_cdb_plan_directive. It
will take the default value of 100% for these parameters:
SQL> BEGIN 
  2  dbms_resource_manager.create_cdb_plan_directive( 
  3  plan                    => 'example_plan', 
  4  pluggable_database      => 'pdb3', 
  5  shares                  => 2, 
  6  utilization_limit       => NULL, 
  7  parallel_server_limit   => NULL); 
  8  END; 
  9  / 
 
PL/SQL procedure successfully completed. 
4. Validate the pending area: Once you complete making changes, you should validate the
pending area:
SQL> exec dbms_resource_manager.validate_pending_area(); 
 
PL/SQL procedure successfully completed. 
5. Submit pending area: The final step in creating a resource manager plan is to submit the
pending area:
SQL> exec dbms_resource_manager.submit_pending_area(); 

 
PL/SQL procedure successfully completed. 
You can enable the new resource manager plan by setting the resource_manager_plan
parameter, as shown in the following code:
SQL> alter system set resource_manager_plan = 'example_plan'; 
System altered. 
If you want to disable the resource manager plan, you can set the preceding parameter to null
string, as shown in the following code:
SQL> alter system set resource_manager_plan = ''; 
System altered. 

Viewing CDB resource plan directives
You can view the information about resource plans and resource plan directives using DBA_*
views. The following views are helpful in viewing different information:
DBA_CDB_RSRC_PLANS: You can view various resource manager plans created in the
database in this view. Each line corresponds to the plan that you have created:
SQL> select plan, comments, mandatory from DBA_CDB_RSRC_PLANS; 
 
PLAN                           COMMENTS                                 
MAN 
------------------------------ ---------------------------------------- -
-- 
DEFAULT_CDB_PLAN               Default CDB plan                         
YES 
DEFAULT_MAINTENANCE_PLAN       Default CDB maintenance plan             YES 
EXAMPLE_PLAN                   This is an example plan for ORCL CDB     NO 
ORA$QOS_CDB_PLAN               QOS CDB plan                             
YES 
ORA$INTERNAL_CDB_PLAN          Internal CDB plan                        
YES 
DBA_CDB_RSRC_PLAN_DIRECTIVES: You can check various directives assigned to a plan
using this view. For example, if you want to see the directives related to example_plan
that we created previously, we can use the following SQL:
select plan, PLUGGABLE_DATABASE, SHARES, UTILIZATION_LIMIT, 
PARALLEL_SERVER_LIMIT
from DBA_CDB_RSRC_PLAN_DIRECTIVES where plan = 'EXAMPLE_PLAN';
PLAN           PLUGGABLE_DATABASE         SHARES UTIL PARA
-------------  -------------------------  ------ ---- ----
EXAMPLE_PLAN   ORA$DEFAULT_PDB_DIRECTIVE       1  100  100
EXAMPLE_PLAN   ORA$AUTOTASK                        90  100
EXAMPLE_PLAN   PDB1                            1   50   5
EXAMPLE_PLAN   PDB2                            1   50   50
EXAMPLE_PLAN   PDB3   
As you can see, every plan includes ORA$DEFAULT_PDB_DIRECTIVE. This directive is used by
PDB when we don't explicitly specify any directive for PDB. Also, we have the ORA$AUTOTASK
directive, which is used by all background auto tasks.

Managing CDB resource plans
Once you create a CDB resource plan, you can update or delete the plan and associated
directives with the help of the following procedures from the DBMS_RESOURCE_MANAGER
package:
CREATE_CDB_PLAN_DIRECTIVE: This procedure creates a CDB resource plan directive for
a PDB in order to specify how resources are allocated to a PDB
UPDATE_CDB_PLAN: This procedure updates a CDB resource plan to change its comment
UPDATE_CDB_PLAN_DIRECTIVE: This procedure updates the resource plan directive that
specifies how resources are allocated to the PDB
UPDATE_CDB_DEFAULT_DIRECTIVE: This procedure helps you change the default directive
attribute values for a PDB
DELETE_CDB_PLAN: This procedure deletes the CDB resource plan
DELETE_CDB_PLAN_DIRECTIVE: This procedure deletes the CDB resource plan directive
for a CDB
For example, you can use UPDATE_CDB_DEFAULT_DIRECTIVE to update attributes of the
default directive:
SQL> BEGIN 
  2  dbms_resource_manager.update_cdb_default_directive( 
  3  plan                            => 'example_plan', 
  4  new_shares                      => 1, 
  5  new_utilization_limit           => 50, 
  6  new_parallel_server_limit       => 50); 
  7  END; 
  8  / 
 
PL/SQL procedure successfully completed. 
Note that for updating the default CDB plan, you need to provide the plan name, because
default plans exist for all the CDB plans that we create. Also, you need to create the pending
area, validate, and submit the pending area if you are making this change.
Finally, when you unplug a PDB from a CDB, the CDB will retain the dropped PDBs
directives. If you happen to plug the PDB back into the CDB, the CDB will use the directives it
has, unless you've manually deleted them already.

PDB resource plans
PDB resource plans are created to manage resources within PDB. These are similar to what
we used to see in non-CDB databases in previous releases. A PDB resource plan determines
the allocation of resources to consumer groups within PDB. But the amount of resources that
are available to PDB is based on the CDB plan directive. So if we have four PDBs on a system
and each has one share, then the PDB will get a guaranteed 25% of total system CPU. So the
PDB resource plan will allocate the CPU to different consumer groups from this 25% of
system CPU that it got.
There are certain limitations/differences between non-CDB resource manager plans and PDB
resource manager plans. They are as follows:
We cannot use multilevel resource plans in PDB. You are limited to single level resource
plans. This was not the case until previous releases, where we could create multilevel
plans for databases. We can still create multilevel plans for non-CDB databases in Oracle
12c.
We can only have a maximum of eight consumer groups. In previous releases, we were
allowed to create up to 32 consumer groups. We can create 32 consumer groups in
Oracle 12c in non-CDB databases only and not in PDB.
We cannot have sub-plans in PDB resource plans. We can have sub-plans in non-CDB
databases.
You can use identical procedures for creating and managing resource plans in PDBs as you
do in a non-multitenant container database (pre-12c) CDB. In the CREATE_PLAN_DIRECTIVE
procedure, there's a new SHARE argument now. In addition, you must replace
MAX_UTILIZATION_LIMIT with the new UTILIZATION_LIMIT argument, and the
PARALLEL_TARGET_PERCENTAGE argument with the PARALLEL_SERVER_LIMIT argument.
To put it together, let's continue with the previous example. We have three PDBs, as shown in
the following table:
PDB/Directive
Shares Utilization limit Parallel server limit
(Default allocation) 1
100%
100%
PDB1
1
50%
50%
PDB2
1
50%
50%
PDB3
2
100%
100%

Let's say in PDB3 we have four consumer groups-OLTP, DSS, OLAP, and ADHOC, and we
allocated resources to these consumer groups as per the following table:
Consumer group Shares Utilization limit
OLTP
1
50%
DSS
2
100%
ETL
1
50%
ADHOC
1
25%
In this case, PDB3 will get 50% of total system resources as we have 2/4 shares allocated to
PDB3. This 50% of system resources will be distributed among four consumer groups in
PDB3 as per the shares allocation. So OLTP, ETL, and ADHOC consumer groups got 1 share
each out of 5 so they will get 20% resources each, of given 50%. DSS will get 40% resources
of given 50% to PDB3. Similarly, utilization_limit provides a cap for resource utilization
and its definition is the same as what we saw in CDB plan directives.
Setting a PDB resource plan is optional. If we don't set any PDB resource plan, then every
session in PDB is treated equally. If a non-CDB has a resource plan and it is plugged into CDB
as PDB, resource plans behave the same if:
Consumer groups are less than or equal to 8
There are no sub-plans in non-CDB
Allocations are at level one only
However, if it violates any of the preceding restrictions, the plan is converted into something
equivalent and the original plan is stored in a dictionary. The converted plan is enabled and in
use. If the converted plan doesn't work for you, you can change the plan as per your
requirement.

Enhancements in creating resource manager
plan directives
In order to track adhoc SQL queries, the resource manager in Oracle Database 12c has been
enhanced with the addition of two new parameters that you can specify for the
DBMS_RESOURCE_MANAGER.CREATE_PLAN_DIRECTIVE procedure:
SWITCH_IO_LOGICAL: Sets the number of logical I/O's that will trigger the action specified
by the switch_group parameter
SWITCH_ELAPSED_TIME: The elapsed time that triggers the actions specified by the
switch_group parameter
There's also a new meta-consumer group named LOG_ONLY, which you can specify as an
argument for the switch_group parameter, if you merely want to log the adhoc query, but
don't want to change its consumer group or perform any other action.
Resource manager integrates adhoc queries with SQL monitors. To retain important results
for the resource manager, it pins up to five SQL's per consumer group. SQL monitor does not
purge those SQL executions until they are unpinned.
The following new columns are added to V$SQL_MONITOR to track adhoc SQL information:
RM_LAST_ACTION: More recent action is taken on this SQL operation by the resource
manager. It can have one of the following values: CANCEL_SQL, KILL_SESSION, LOG_ONLY,
SWITCH TO <consumer_group name>.
RM_LAST_ACTION_REASON: The reason for most recent action taken on this SQL operation
by resource manager. Values could be: SWITCH_CPU_TIME, SWITCH_IO_REQS,
SWITCH_IO_MBS, SWITCH_ELAPSED_TIME, SWITCH_IO_LOGICAL.
RM_LAST_ACTION_TIME: Time when the most recent action was taken.
RM_CONSUMER_GROUP: Current consumer group of the SQL operation.

Multi-process, multi-threaded Oracle
architecture
The Oracle process architecture is different for different operating systems. For example,
Oracle on Windows uses threaded architecture where the background process is a thread of
execution within the operating system process. On Linux and Unix, Oracle background
processes run as OS processes.
Oracle 12c has introduced multi-threaded process architecture for Linux and Unix OS. The
multi-threaded model enables certain Oracle background processes to run as thread, whereas
remaining background processes run as processes. The OS process to Oracle process
allocation is random. In Oracle 12.1, the following Oracle background processes will run as
processes just like in previous releases:
PMON
VKTM
DBWr
PSP (process spawner)
All other Oracle background processes will run as thread. Each OS process will also run a
special thread called SCMN, which is basically an internal listener thread. All thread creation
is routed through the following thread:

Enabling multi-threaded architecture
By default, multi-threaded process architecture is disabled. You can enable multi-threaded
process architecture by setting the THREADED_EXECUTION parameter to TRUE in spfile and
bouncing the database. The THREADED_EXECUTION parameter specifies whether multithreaded
Oracle architecture is enabled or not. When set to TRUE, multithreaded Oracle architecture is
enabled. In multithreaded architecture, Oracle processes on UNIX and Linux runs as operating
system threads in separate address spaces.
You have to bounce the DB to enable this. The default value of the threaded_execution
parameter is false. When you set it to true, you can't use OS authentication to connect to the
database. There's no workaround to this you must use a password to connect to the DB. Also,
make sure that you have the password file created for the database, or else you won't be able
to even start the instance after enabling the threaded_execution parameter.
Let's try to enable multi-threaded architecture:
SQL> alter system set threaded_execution = true scope = spfile; 
System altered. 
SQL> shut immediate 
Database closed. 
Database dismounted. 
ORACLE instance shut down. 
SQL> startup 
ERROR: 
ORA-01017: invalid username/password; logon denied 
ORA-01017: invalid username/password; logon denied 
SQL> 
As you see, we got an error for an invalid username/password because we were connected
initially using OS authentication. Since threaded_execution is set to true, we need to login
using username/password and start the instance:
[oracle@deo dbs]$ sqlplus sys as sysdba 
SQL*Plus: Release 12.1.0.2.0 Production on Mon Jun 27 03:11:54 2016 
Copyright (c) 1982, 2014, Oracle.  All rights reserved. 
Enter password:  
Connected to an idle instance. 
SQL> startup 
ORACLE instance started. 
Total System Global Area  805306368 bytes 
Fixed Size                  2929552 bytes 

Variable Size             318770288 bytes 
Database Buffers          478150656 bytes 
Redo Buffers                5455872 bytes 
Database mounted. 
Database opened. 
SQL>  
Let's check out the background processes and threads at OS level. We can only see the
following background process:
[oracle@deo dbs]$ ps -eaf | grep ora_ 
oracle   30899     1  0 03:11 ?        00:00:00 ora_pmon_deo 
oracle   30901     1  0 03:11 ?        00:00:00 ora_psp0_deo 
oracle   30903     1  2 03:11 ?        00:00:07 ora_vktm_deo 
oracle   30907     1  0 03:11 ?        00:00:00 ora_u004_deo 
oracle   30913     1  3 03:11 ?        00:00:13 ora_u005_deo 
oracle   30920     1  0 03:11 ?        00:00:00 ora_dbw0_deo 
As you can see, we are seeing only four background processes that we listed in the " Multi-
process, multi-threaded Oracle architecture" section as being the one spawned as OS
processes and not threads. All other background processes are converted into threads and they
are spawned under two OS processes, u004 and u005. We can check all background threads
using the following command at the Unix shell:
[oracle@deo dbs]$ ps -L -eo "pid tid comm args" | grep ora_ 
30899 30899 ora_pmon_deo    ora_pmon_deo 
30901 30901 ora_psp0_deo    ora_psp0_deo 
30903 30903 ora_vktm_deo    ora_vktm_deo 
30907 30907 ora_scmn_deo    ora_u004_deo 
30907 30908 oracle          ora_u004_deo 
30907 30909 ora_gen0_deo    ora_u004_deo 
30907 30910 ora_mman_deo    ora_u004_deo 
30907 30916 ora_dbrm_deo    ora_u004_deo 
30907 30921 ora_lgwr_deo    ora_u004_deo 
30907 30922 ora_ckpt_deo    ora_u004_deo 
30907 30923 ora_lg00_deo    ora_u004_deo 
30907 30924 ora_smon_deo    ora_u004_deo 
30907 30925 ora_lg01_deo    ora_u004_deo 
30907 30927 ora_lreg_deo    ora_u004_deo 
30913 30913 ora_scmn_deo    ora_u005_deo 
30913 30914 oracle          ora_u005_deo 
30913 30915 ora_diag_deo    ora_u005_deo 
30913 30917 ora_vkrm_deo    ora_u005_deo 
30913 30918 ora_dia0_deo    ora_u005_deo 
30913 30926 ora_reco_deo    ora_u005_deo 
30913 30928 ora_pxmn_deo    ora_u005_deo 
30913 30929 ora_mmon_deo    ora_u005_deo 
30913 30930 ora_mmnl_deo    ora_u005_deo 
30913 30931 ora_d000_deo    ora_u005_deo 
30913 30932 ora_s000_deo    ora_u005_deo 
30913 30933 ora_n000_deo    ora_u005_deo 
30913 30940 ora_tmon_deo    ora_u005_deo 
30913 30941 ora_tt00_deo    ora_u005_deo 
30913 30942 ora_smco_deo    ora_u005_deo 

30913 30943 ora_w000_deo    ora_u005_deo 
30913 30944 ora_w001_deo    ora_u005_deo 
30913 30945 ora_aqpc_deo    ora_u005_deo 
30913 30947 ora_p000_deo    ora_u005_deo 
30913 30948 ora_p001_deo    ora_u005_deo 
30913 30949 ora_p002_deo    ora_u005_deo 
30913 30950 ora_cjq0_deo    ora_u005_deo 
30913 30951 ora_p003_deo    ora_u005_deo 
30913 30952 ora_p004_deo    ora_u005_deo 
30913 30953 ora_p005_deo    ora_u005_deo 
30913 30954 ora_p006_deo    ora_u005_deo 
30913 30955 ora_p007_deo    ora_u005_deo 
30913 31071 ora_qm02_deo    ora_u005_deo 
30913 31072 ora_q001_deo    ora_u005_deo 
30913 31074 ora_q003_deo    ora_u005_deo 
30920 30920 ora_dbw0_deo    ora_dbw0_deo 
As you can see, we have 30907 and 30913 as the OS process IDs. These OS processes contain
multiple threads and each thread has a different thread ID, as shown in the second column in
the preceding output. You can also see the SCMN thread (underlined) for each OS background
process. These are listener threads. You can see similar information from the V$PROCESS view.
Columns SPID and STID show the relationship between the process ID and thread ID, as
follows:
SQL> select spid, stid, program from v$process order by 1,2; 
SPID       STID       PROGRAM 
---------- ---------- ---------------------------------------- 
30899      30899      oracle@deo.example.com (PMON) 
30901      30901      oracle@deo.example.com (PSP0) 
30903      30903      oracle@deo.example.com (VKTM) 
30907      30907      oracle@deo.example.com (SCMN) 
30907      30909      oracle@deo.example.com (GEN0) 
30907      30910      oracle@deo.example.com (MMAN) 
30907      30916      oracle@deo.example.com (DBRM) 
30907      30921      oracle@deo.example.com (LGWR) 
30907      30922      oracle@deo.example.com (CKPT) 
30907      30923      oracle@deo.example.com (LG00) 
30907      30924      oracle@deo.example.com (SMON) 
30907      30925      oracle@deo.example.com (LG01) 
30907      30927      oracle@deo.example.com (LREG) 
30913      30913      oracle@deo.example.com (SCMN) 
30913      30915      oracle@deo.example.com (DIAG) 
30913      30917      oracle@deo.example.com (VKRM) 
30913      30918      oracle@deo.example.com (DIA0) 
30913      30926      oracle@deo.example.com (RECO) 
30913      30928      oracle@deo.example.com (PXMN) 
30913      30929      oracle@deo.example.com (MMON) 
30913      30930      oracle@deo.example.com (MMNL) 
30913      30931      oracle@deo.example.com (D000) 

Connecting to a database with multi-threaded architecture
By default, when a connection request uses a TNS name, Oracle starts an OS process to run
the server process as a thread within that process. But in a multi-threaded architecture, the
server process runs as a thread rather than an OS process. Oracle recommends that you set the
following two parameters to ensure that the database connection spawns a thread (and not an
OS process):
LOCAL_LISTENER - <tns name of instance>
DEDICATED_THROUGH_BROKER_<listener_name> = ON
Following this, when the listener receives a connection request, it passes that request to the
database so it can spawn a database thread to handle the connection. The listener won't spawn
an OS process in this case, but it passes along the request to the database. Each OS process
runs an internal listener thread called SCMN. All thread creation is routed through this thread.
Here are a few more considerations to remember when using threaded architecture:
An Oracle background or server process running as a thread still using PGA for private
memory and SGA for inter-process communication.
As seen previously, the listener process does not spawn thread. Instead the connection
request is handed out to the spawning thread SCMN for the given instance.
Multi-threaded architectures require password files for SYSDBA authentication. OS
authentication does not work in a multi-threaded architecture.
A threaded execution does not run over a static service definition. Connections should be
routed over a dynamically registered service.

Smart flash cache enhancements
Smart flash cache was introduced in Oracle 11gR2 as an extension to buffer cache. Smart
flash cache works only for Solaris and Oracle Enterprise Linux (OEL). Usually, when data
is required for the first time it is fetched from the disk, which is a slow process as reading
from the disk is a slow process. Once data is in the memory cache, data access is much faster.
However, after sometime when the data is not required, it will be aged out of the memory
cache to make room for new data. If this aged data is required again, it has to be fetched from
the disk. But if you have smart flash cache configured, data that ages out from memory goes
to smart flash cache, which is much faster than the disk. So if that data is needed again, it will
be read from smart flash cache instead of the disk, thus reducing the latency. Oracle
recommends using this feature when either the AWR report or statspack report shows a
recommendation to increase buffer_cache or when the db file sequential read wait event
is among the top wait event in the database. As a general rule, the size of smart flash cache
should be between two to ten times the size of the buffer cache if you want to see the benefits.
In Oracle Database 11g, Oracle limited the use of Database Smart Flash Cache to one device.
When a system required a larger flash cache than could be supported by a single flash device
(ASM disk group or disk manager), you were required to present the multiple devices as a
single device (or file).
In Oracle Database 12c, the limitation of one device goes away and you can now specify up to
16 devices dynamically as part of the flash cache. Also in the new release, Oracle has added a
new object-based algorithm to existing standard LRU algorithms of ageing out old
infrequently accessed blocks out of the buffer cache. The object-based algorithm segregates
the buffer cache, which includes the flash cache, and replaces a portion of the buffer with the
specialized object-based algorithm. The other portion of the buffer cache is replaced by the
traditional LRU algorithm (which continues to include the original smart flash cache
algorithm). The new object-based algorithm allows the use of the in-memory parallel query
(PQ) algorithm.
Let's check out these two features.

Using multiple flash cache devices
You can specify up to 16 separate flash cache devices. You specify the files for the flash cache
devices and the flash cache size with the following initialization parameters in your spfile or
the init.ora file:
db_flash_cache_file = /dev/raw/sda1, /dev/raw/sdb1 
db_flash_cache_size = 100G, 50G 
A restart of the database is required when you set these parameters. The
DB_FLASH_CACHE_FILE parameter specifies a list of paths and filenames for the files that are
contained in the Oracle Database Smart Flash Cache, on the disk, or in an Oracle ASM disk
group. It's important to understand that the DB_FLASH_CACHE_SIZE parameter specifies the size
of each file in the cache and not the total size of the cache you're allocating. Once the instance
starts with a set of flash cache devices, you can dynamically disable a flash device by setting
the DB_FLASH_CACHE_SIZE parameter to zero.
For example, if we want to disable the first cache file, we can set the size of that file to 0. Each
size value in the DB_FLASH_CACHE_SIZE parameter corresponds to the filename in the
DB_FLASH_CACHE_FILE parameter in the same sequence.
To disable the first flash cache file, use the following code:
Alter system set DB_FLASH_CACHE_SIZE = 0,1G; 
Of course, if you have only one flash cache device, setting its size to zero disables flash cache
completely in the database. You can re-enable a flash device back to the same size as before.
You cannot change the size of flash cache once it's set. You can enable the flash cache file by
specifying the same size only.
For the in-memory parallel query algorithm, in order to allow in-memory parallel queries to
take advantage of the flash cache, Oracle has now added a new object-based algorithm to the
standard LRU algorithm to manage the buffer cache in conjunction with the flash cache. The
new object-based algorithm is used along with the standard LRU algorithm, which replaces
the least recently used buffer blocks from the buffer cache.
The new object-based algorithm partitions the buffer cache and uses a portion of the buffer
cache for the object-based algorithm, and the remaining portion is available for the standard
LRU algorithm. The new object-based algorithm is a temperature-based, object-level
replacement algorithm. This algorithm is used with big table caching (by enabling the
parameters DB_BIG_TABLE_CACHE_PERCENT_TARGET and PARALLEL_DEGREE_POLICY) enabled.
In an RAC environment, an object is partitioned and fit into the combined buffer cache and the
flash cache. To do this, the object's size must be smaller than the total size of the buffer cache,
plus the Smart Flash Cache. Doing this can eliminate disk reads for in-memory parallel
queries and allow parallel reads from the disk and the Smart Flash Cache in order to increase

the bandwidth of read operations.
The object-based replacement algorithm does not trash buffers from the buffer cache.
Trashing refers to throwing out blocks of table data (partial table) out of memory to
accommodate other data in memory. When a table does not fit into the memory, Oracle stores
only part of the table into the memory. For example, if only 70 percent of the table blocks can
be accommodated in the cache, the remaining 30 percent of the table blocks will not be read
from the disk into the memory. But those 30 percent of blocks will be read when required.
During that time a few blocks from this 70 percent will be thrown out of memory. This is
called trashing. In case of the object-based algorithm, trashing does not happen as Oracle
either stores the complete table in cache or it does not store the table at all. The database
considers more popular tables as hot table and less popular tables as cold tables. Only hot
tables are cached in memory, whereas cold tables are not cached in memory. The
DB_BIG_TABLE_CACHE_PERCENT_TARGET parameter sets the percentage of the buffer cache that
uses this algorithm.
The following statistics measure the use of the object replacement algorithm:
Data warehousing scanned blocks: Number of 1MB chunks scanned using the in-
memory parallel query data warehousing scan
Data warehousing scanned disk: Disk number of chunks that have been scanned where
the request was satisfied by direct disk reads
Data warehousing scanned offload: Number of chunks that have been scanned by
exadata offloading
Data warehousing scanned memory: Number of chunks that have been scanned where
the request was satisfied by buffers in memory
Data warehousing scanned objects: Number of objects that were scanned using the in-
memory parallel query data warehousing scan

Index and table enhancements
Oracle 12c has introduced very useful table and index enhancements to improve productivity
and manageability. Let's check out some of the important features related to the table and
index.

Invisible columns
Oracle has introduced invisible columns in tables. Invisible columns are the one that are
available only if they are accessed explicitly. Invisible columns provide benefits to
developers, where they can create the new columns as hidden for the upcoming development
cycle. This way, users can access the production application without any impact while
developers can change the schema to meet new business requirement. During deployment,
developers can expose the newly added columns by making them visible.
Usage and limitations of invisible columns
You can use invisible columns as partition keys of tables and also as virtual columns. You
need to explicitly select the invisible columns to view data. Invisible columns will not show up
in the SELECT * statement.
While inserting data into the table, you need to explicitly specify the invisible column name in
the column list of the insert statement. Specifying only the value of the invisible column in
the VALUE clause will cause the insert statement to fail.
You cannot create invisible columns in the following types of tables:
External table
Cluster table
Temporary table
You cannot make system generated hidden columns visible. All other user created invisible
columns can be made visible using the ALTER TABLE statement. Let's check out an example.
Creating a table with invisible columns:
SQL> create table T1 (col1 number, col2 varchar2(10) invisible); 
Table created. 
SQL> desc T1 
Name                                      Null?    Type 
-------------------------------------- -------- ---------------------------- 
COL1                                            NUMBER 
As you can see, we cannot see invisible columns when we describe the table. To be able to
check invisible columns when we describe the table, we need to use SET INVISIBLECOL ON at
the SQL prompt, as shown in the following code:
SQL> set colinvisible on  
SQL> desc T1 
Name                                      Null?    Type 
-------------------------------------- -------- ---------------------------- 
COL1                                            NUMBER 
COL2 (INVISIBLE)                                VARCHAR2(10) 

If you try inserting invisible columns into the table without explicitly specifying the column
name, you will get the following error:
SQL> insert into T1 values (1, 'A'); 
insert into T1 values (1, 'A') 
           * 
ERROR at line 1: 
ORA-00913: too many values 
SQL> insert into T1 (col1, col2) values (1, 'A'); 
1 row created. 
If we use SELECT *, it will show only one column. We have to explicitly select the required
invisible column to see the value in that column, as shown in the following code:
SQL> select * from T1; 
     COL1 
---------- 
        1 
SQL> select col1, col2 from T1; 
     COL1 COL2 
---------- ---------- 
        1 A 
You can make an invisible column visible and vice-versa using the ALTER TABLE statement, as
shown in the following code:
SQL> alter table T1 modify (col2 visible); 
Table altered. 
SQL> desc T1  
Name                                      Null?    Type 
-------------------------------------- -------- ---------------------------- 
COL1                                            NUMBER 
COL2                                            VARCHAR2(10) 
You can query the HIDDEN_COLUMN column of the ALL_, DBA_, and USER_TAB_COLS data
dictionary views to find out if a column is a hidden column. Similarly, the USER_GENERATED
column in this view shows if a column is user generated or system generated. Remember that
you cannot make a system generated invisible column visible.

Multiple indexes on the same columns
Oracle Database 12c allows you to create multiple indexes on the same set of columns. In
order to do this, you must ensure that at least one of the following conditions is true:
Indexes should be of different types: This includes an exception of a B-tree index and a
B-tree cluster index. Even though the B-tree index and B-tree cluster indexes are
different, you cannot have both on the same column of the table at the same time. Another
exception is that of a B-tree index and an index-organized table on the same set of
columns.
The indexes must use different types of partitioning: You can have non-partitioned
and partitioned indexes together or indexes that have different partitioning schemes or
indexes that are locally partitioned together with indexes that are globally partitioned.
Indexes with different uniqueness properties: A unique and a non-unique index-are
treated as different indexes, even though you create them on the same set of columns.
The important thing to understand is that only one index will be visible at any given time. So
even if you create different type of indexes, only one type of index should be visible at any
given time. But this gives a benefit in terms of checking the performance of different indexes.
You don't have to create different types of index every time to check performance. You can
simply create multiple invisible indexes anytime and change the visibility of indexes to
analyze the performance.
Even though you can create multiple indexes on the same set of columns, the optimizer can't
choose among the multiple indexes and pick the best index for a query. The reason is that at
any given time, only one of the indexes can be visible. Thus, the optimizer ends up using (or
not using) the only visible index, just as in prior database releases.
If you set the OPTIMIZER_USE_INVISIBLE_INDEXES parameter to TRUE, the optimizer can
consider and use the invisible index.
Let's now look at an example.
Create a unique and non-unique index on the same column of table T1:
SQL> create table T1 as select * from order_items; 
Table created. 
SQL> create unique index T1_UK on T1(ORDER_ID, PRODUCT_ID) tablespace users; 
Index created. 
SQL> create index T1_NON_UK on T1(ORDER_ID, PRODUCT_ID) tablespace users 
invisible; 
Index created. 

Note that the second, non-unique index that I created is invisible. We can only have one index
visible at any time.

Online redefinition - tables with VPD
Virtual Private Database (VPD) policies let you restrict the data that's shown to or
modifiable by users. In Oracle Database 12c, you can perform online redefinition of a table
with VPD polices without changing the properties of any of the table's columns.
The new parameter COPY_VPD_OPT in the DBMS_REDEFINITION.START_REDEF_TABLE procedure,
allows you to handle any VPD polices defined on the table, during the online redefinition of
the table. This parameter has the following values:
DBMS_REDEFINITION.NONE: This is the default value for the COPY_VPD_OPT parameter, and
it is meant for cases where there aren't any VPD policies in the table being redefined. If
there are actually VPD policies on the table, you'll receive an error.
DBMS_REDEFINITION.CONS_VPD_AUTO: Setting this value for the COPY_VPD_OPT parameter
copies the current VPD policies on the table to the new table during the online
redefinition.
DBMS_REDEFINITION.CONS_VPD_MANUAL: Setting this value means that you're going to
manually copy the VPD policies during the online redefinition of the table. You must
specify the CONS_VPD_MANUAL option when one of the following conditions is true:
The original table has VPD policies specified for it and there are column mappings
between it and the interim table
You want to add new VPD policies or modify existing policies during the online
redefinition
You must specify the VPD policies for the interim table before executing the
START_REDEF_TABLE procedure. Once the redefinition is completed, the newly redefined table
will contain the modified VPD policies. The following example shows the usage of this
parameter:
exec dbms_redefinition.start_redef_table( 
UNAME           => 'OE', 
ORIG_TABLE      => 'ORDER_ITEMS', 
INT_TABLE       => 'ORDER_ITEMS_INTRIM', 
OPTION_FLAG     => dbms_redefinition.cons_use_pk, 
COPY_VPD_OPT    => dbms_redefinition.cons_vpd_auto); 
/ 

Online redefinition - dml_lock_timeout
While finishing the online redefinition, the original table is briefly locked in exclusive mode.
This is required for changing of metadata in the background and to have a clean cutoff. We
can finish the redefinition process using the FINISH_REDEF_TABLE procedure. Before it can
complete the redefinition process, the procedure will wait for all pending DML statements to
commit. Oracle Database 12c lets you specify how long the FINISH_REDEF_TABLE procedure
can wait for pending DML statements to complete. You do this by setting the
DML_LOCK_TIMEOUT parameter. The following is the example:
exec dbms_redefinition.finish_redef_table( 
UNAME                => 'OE', 
ORIG_TABLE           => 'ORDER_ITEMS', 
INT_TABLE            => 'ORDER_ITEMS_INTRIM', 
DML_LOCK_TIMEOUT     => 100); 
/ 
In this example, the DML_LOCK_TIMEOUT parameter is set to 100 seconds, meaning that the
database will wait for 100 seconds for all pending DML statements to complete before
gracefully terminating the FINISH_REDEF_TABLE procedure. The default value of this
parameter is zero (if we don't specify) and it means it won't wait at all. The maximum value of
1,000,000 seconds means that the DML statement will wait indefinitely to acquire a DML lock.
If we specify the timeout value and the procedure has timed out waiting for the lock, we can
restart the procedure and it will resume from the point at which it timed out. If you set the
parameter to NULL, there will be no automatic timing out of the FINISH_REDEF_TABLE
procedure. In that case, you can stop the procedure manually by executing the
ABORT_REDEF_TABLE procedure. Unfortunately, this means that you have to start the
FINISH_REDEF_TABLE procedure from the beginning to complete the redefinition process.

Advanced row compression and compression
advisor
Advanced row compression was introduced in Oracle 11g and supported all DML statements
(insert, update, delete). This compression was available to use with OLTP as well as data
warehouse. In Oracle 12c, the feature is enhanced incrementally and renamed. In Oracle 12c,
advanced row compression provides a better compression ratio.
In Oracle 11g, compression was specified using the BASIC and FOR OLTP clause. In Oracle 12c,
we have a different clause for providing compression. We have ROW STORE and COLUMN STORE,
which are attributes of the table, like the organization clause. ADVANCED and BASIC are types
of compression. The following are the examples:
Oracle 11g:
Alter table T1 compress basic
Alter table T1 compress for OLTP
Oracle 12c:
Alter table T1 row store compress basic
Alter table T1 row store compress advanced
Another problem with OLTP and basic compression in 11g is that when a row is updated, all
columns of the row are uncompressed up to the highest referenced column for the update.
Now in Oracle 12c, only referenced columns are uncompressed. Also, Oracle 11g did not
support more than 255 columns, whereas Oracle 12c supports more than 255 columns in a
row for compression.

LOB compression
Oracle 12c has also renamed the Advanced Compression Option related to SecureFile LOB:
SecureFiles Compression is now changed to Advanced LOB Compression
SecureFiles Deduplication is now changed to Advanced LOB Deduplication

Using compression advisor
Oracle 12c has enhanced compression advisor, which helps determine an optimal
compression ratio for tables. This advisor now displays new advanced compression types.
The following is an example:
SQL> DECLARE 
 2  blkcnt_cmp      integer; 
 3  blkcnt_uncmp    integer; 
 4  row_cmp         integer; 
 5  row_uncmp       integer; 
 6  cmp_ratio       integer; 
 7  comptype_str    varchar2(100); 
 8  BEGIN 
 9  dbms_compression.get_compression_ratio( 
10  SCRATCHTBSNAME  => 'USERS', 
11  OWNNAME         => 'OE', 
12  OBJNAME         => 'ORDER_ITEMS', 
13  SUBOBJNAME      => null, 
14  COMPTYPE        => dbms_compression.comp_advanced, 
15  BLKCNT_CMP      => blkcnt_cmp, 
16  BLKCNT_UNCMP    => blkcnt_uncmp, 
17  ROW_CMP         => row_cmp, 
18  ROW_UNCMP       => row_uncmp, 
19  CMP_RATIO       => cmp_ratio, 
20  COMPTYPE_STR    => comptype_str, 
21  SUBSET_NUMROWS  => 1000, 
22  OBJTYPE         => 1); 
23  dbms_output.put_line('Block count compressed = '||blkcnt_cmp); 
24  dbms_output.put_line('Block count uncompressed = '||blkcnt_uncmp); 
25  dbms_output.put_line('Compression type = '||comptype_str); 
26  dbms_output.put_line('Compression ratio = '||cmp_ratio); 
27  END; 
28  / 
Block count compressed = 2 
Block count uncompressed = 3 
Compression type = "Compress Advanced" 
Compression ratio = 2 
PL/SQL procedure successfully completed. 

Enhanced online DDL capabilities
You can now specify the ONLINE keyword in executing DDL statements. This allows DMLs to
continue while DDL is being executed. The ONLINE keyword can be specified for the
following types of DDL statements:
Drop index
Drop constraint
Alter index unusable
Set column unused
Having online capability for these statements causes no impact to DMLs, and DDLs can be
performed even when user activities are running on tables.

Drop index/constraints
When we provide online keywords for drop index/constraint statements, DML operations on
tables or partitions are allowed while dropping index/constraints. Drop index online is
supported for partitioned as well as non-partitioned tables. Drop constraints online has the
following restrictions:
Cannot drop constraints with CASCADE
Cannot drop referencing constraints that hold references for other dependencies
For example:
SQL> drop index oe.ORDER_ITEMS_UK online; 
 
Index dropped. 
 
SQL> alter table order_items drop constraint ORDER_ITEMS_ORDER_ID_FK online; 
 
Table altered. 

Index unusable
You can mark an index unusable using the ONLINE keyword even when DML's are running on
the table. The space allocated to the index is released immediately. If you want to make an
index usable, you need to rebuild or drop and recreate the index. You cannot specify UNUSABLE
for indexes on a temporary table:
SQL> alter index oe.ITEM_ORDER_IX unusable online; 
Index altered. 

Setting unused columns
You can set a column as unused if you want to drop that column. Setting a column as unused is
the first step in dropping the column. The ONLINE clause allows DML operations to continue
while marking the column as unused. After marking the column as unused, you cannot access
the column.
SELECT * will not list the column, neither can you convert the column to usable again. The
only action possible after marking the column as unused is to drop the column. You can
specify DROP USUSED COLUMNS to drop unused columns of the table. Also, you cannot specify
the ONLINE clause for columns with DEFERRABLE constraints.

ADR and network enhancements
ADR stands for Automatic Diagnostics Repository, which is a file-based repository for
storing all log files, trace files, dump files, health monitor reports, and core files generated
by a running Oracle instance. It has a unified directory structure and is meant for storing the
diagnostic files for databases as well as all other Oracle products. Each product/component
has its own ADR home and files related to each component, and they are stored in its
respective ADR home.
For example, an RAC database with ASM will have a separate ADR home for each instance of
RAC and also separate ADR homes for ASM instances. Having such organized diagnostic
locations helps Oracle support and customers to get only the required diagnostics files related
to the incident making debugging and analysis easier.
Oracle has also introduced network compression as part of advanced compression options.
Let's take a quick look at ADR enhancements and network enhancements.

ADR enhancements
Oracle 12c has introduced two new diagnostics files:
DDL file: Used to store all DDL commands and audit information of all DDL statements
that are run on the database
Debug file: Used to store debug information to be used by Oracle support
These files are available in their respective directories-ddl and debug. These directories are
added as sub-directories under the log directory:
[oracle@deo log]$ pwd 
/u01/app/oracle/diag/rdbms/deo/deo/log 
[oracle@deo log]$ ls 
ddl  debug  imdb  test 
DDL logging
The DDL directory has a log file that stores all the DDL's that are run in the database. In Oracle
11g, DDL statements were logged in an alert log file. Setting ENABLE_DDL_LOGGING to TRUE
enables logging of DDL statements in the alert log.
Starting with Oracle 12c, enable_ddl_logging is disabled by default. You can enable it by
setting the parameter to true. Also, logging of the DDL statement is moved from the alert log
file to a new DDL log file created in the ddl directory under the log directory, as shown
previously. DDL statements are logged in an XML log file under the ddl directory along with
a timestamp when the DDL was executed.
For example:
SQL> alter system set enable_ddl_logging = true; 
System altered. 
SQL> alter index oe.ITEM_ORDER_IX rebuild online; 
Index altered. 
[oracle@deo ddl]$ pwd 
/u01/app/oracle/diag/rdbms/deo/deo/log/ddl 
[oracle@deo ddl]$ cat log.xml  
<msg time='2016-06-27T14:56:55.450+05:30' org_id='oracle' comp_id='rdbms' 
msg_id='opiexe:4409:2946163730' type='UNKNOWN' group='diag_adl' 
level='16' host_id='deo.example.com' host_addr='192.168.56.20' 
version='1'> 
<txt>alter index oe.ITEM_ORDER_IX rebuild online 
</txt> 
</msg> 
As you can see, once we enable DDL logging, we can see that DDL statements are getting
logged into the ddl log file in an XML format. If the XML format is less convenient to read,

we have a new command introduced in ADRCI, which shows the content in a normal text
format:
[oracle@deo ~]$ adrci 
ADRCI: Release 12.1.0.2.0 - Production on Mon Jun 27 14:59:41 2016 
Copyright (c) 1982, 2014, Oracle and/or its affiliates.  All rights reserved. 
ADR base = "/u01/app/oracle" 
adrci> show log; 
Choose the home from which to view the diagnostic logs:
1: diag/rdbms/orcl/orcl 
2: diag/rdbms/deo/deo 
3: diag/clients/user_oracle/host_1860839767_82 
4: diag/tnslsnr/deo/listener 
Q: to quit 
Please select option: 2 
Output the results to file: /tmp/utsout_3552_139758_1.ado 
When you issue this command, the instance will open the log.xml file in an editor (such as vi
in Unix/Linux and Notepad in Windows) and show the contents, which are in the following
format:
2016-06-27 14:56:55.450000 +05:30 
alter index oe.ITEM_ORDER_IX rebuild online 
Debug log
The debug log contains information about errors that are not very critical to log in the alert
log file, but at the same time are not very usual that they can be ignored. That's why Oracle
12c provides a new type of log file called debug log. This file contains no critical errors that
the database is facing. This log file is not meant for DBAs, but it's mainly used by Oracle
support for diagnosis purposes. Entries in this file have the same format as a regular alert log
file, and they contain information about the potential issue happening in the database. This file
is created so that we don't miss non-critical errors and at the same time avoid flooding of
alert log files with non-critical errors.

Network-related enhancements
Oracle 12c introduces the following network related enhancements:
Network compression
SDU (session data unit) size
Network compression
Network performance is greatly dependent on network bandwidth and the volume of data to
be passed. In simple terms, network bandwidth can be considered as max capacity to push data
at any given point. The larger the capacity, more data can be sent at any instant. The volume of
data refers to the amount of data that needs to be transmitted from one location to another. To
improve the network performance, we have to either increase the bandwidth of the network or
reduce the data volume so that data can be transmitted in the least possible time.
Many times increasing the bandwidth is not possible because of infrastructure limitation,
investment cost, and so on. In such cases, if we can reduce the amount of data, it provides
benefits in improving the network performance.
Oracle has introduced network compression as part of Advanced Compression Option
(ADO). Compressing the data reduces the amount of bits to be transmitted, which reduces the
bandwidth used, allowing more data to be transmitted at any instant. Network compression is
transparent to application and no changes need to be made at application level.
Enabling network compression
You can enable network compression by setting the following parameters in an sqlnet.ora
file and bouncing the listeners for these parameters to take effect:
SQLNET.COMPRESSION: This is the main parameter that can enable or disable network level
compression. The default value is OFF. If we set this value to ON, network compression is
enabled.
SQLNET.COMPRESSION_LEVEL: This parameter determines the level of compression at both
ends of the network. This parameter accepts the value of LOW or HIGH. When set to LOW,
data is less compressed, but CPU usage is also less. If we set this value to HIGH, data is
tightly compressed, but CPU usage also goes high.
SQLNET.COMPRESSION_THRESHOLD: Determines the minimum data size for compression to
kick in. The default value is 1KB (1024 bytes).
Session Data Unit
Session Data Unit (SDU) refers to the size of packets that are being transmitted over the
network. Oracle converts the data blocks to be sent into network packets and these packages
are called SDU. Oracle 12c has increased the maximum size of SDU to 2MB (this was 64KB
in previous releases). The value of SDU ranges from 512 bytes to 2MB. A bigger size of SDU
can improve the network performance as more data can be sent at once and fewer round trips

can be done for sending the same data. But a bigger SDU size increases the bandwidth
requirement of the network. The default size for the client and dedicated server is 8KB. The
default SDU size for a shared server is 64KB.
The actual SDU size is negotiated between a client and server when connections are made and
the smaller of two values are used.
SDU size should be 70 bytes larger than the message size to be transmitted. 70 bytes are
reserved to hold metadata information. If the size of the message to be transmitted plus 70
bytes is more than the SDU size, then the message is split into multiple packets called
fragments. In such cases, SDU size should be adjusted in such a way that the message size is
divided into the smallest number of equal parts where each part of 70 bytes is less than the
SDU size (for holding metadata).
One has to analyze the requirements before increasing the SDU size. Having a smaller SDU
size will cause many fragments and multiple trips to transmit the data causing performance
degradation. But having a higher SDU size will create bigger packets and if the network is
nearing its capacity, this will cause collisions, thus reducing performance. Efforts should be
made to reduce data to be transmitted by designing the application efficiently and/or using
network compression.
You can set the default SDU size in an sqlnet.ora file using the DEFAULT_SDU_SIZE parameter.
For example, DEFAULT_SDU_SIZE = 8192.

Summary
In this chapter, we talked about new features of resource manager and how resource manager
will work in a multitenant environment. We also saw examples of creating and enabling
resource manager plans and checking information about resource manager from backend
tables. We then talked about multi-process and multi-threaded architectures, how to enable
them, and what are the benefits of using them. We then discussed smart flash cache and how
it's enhanced from previous releases of Oracle. Next we saw enhancements related to index
and tables. We mainly talked about creating multiple indexes on the same columns. We talked
about limitations of this and how to use this for checking the performance of different types
of indexes. We also discussed invisible columns and how they work. We also saw
enhancements related to online redefinition, advanced row compression, and compression
advisor. We also went through some of the new enhancements related to online DDL
capabilities. Finally, we concluded this chapter with ADR and network enhancements.
In the next chapter, we are going to talk about other miscellaneous enhancements such as new
features of SQL*Loader, data pump, and online operations. We will also talk about partition
enhancements in Oracle 12c and finally a few SQL enhancements.

Chapter 9. Miscellaneous New Features
Use of data pump and SQL*Loader to extract and load data is a very common practice. DBAs
and data engineers use these tools extensively for the loading and unloading of data. Oracle
12c has introduced a few very useful enhancements related to these tools that will help us in
using these tools in production. Partitioning is also one of the major functionalities in Oracle
and we use partition tables extensively. Oracle 12c has introduced new features related to the
partitioning of tables and we will cover these enhancements in this chapter. The following
topics are covered in this chapter:
Oracle data pump, SQL*Loader, external tables, and online operations enhancements
Partitioning enhancements
SQL enhancements

Oracle data pump enhancements
Oracle 12c has introduced many enhancements to data pump, including the following:
Full transportable export and import of data
Disabling logging for data pump import
Exporting views as table
Compressing data during import
Creating SecureFile LOB during import
Specifying encryption password
Let's take a look at each of these enhancements in detail.

Full transportable export and import of data
Oracle 12c has introduced full database export and import of data. In previous releases, we
have seen transportable tablespace where we can export the metadata of tablespaces to be
transported to different platforms/databases and import just the metadata after coping the
required data files. This was a much faster approach and preferred approach in situations such
as upgrading a database, merging two or more databases, or moving a database to a different
operating system. Oracle 12c goes a step further and allows a full database as transportable
(instead of just tablespace).
A full transportable export will export all the data necessary to create a complete copy of a
database. This method uses multiple data movement approaches:
Objects residing in a transportable tablespace that are not exported. Only the metadata of
those tablespaces are exported. Data gets copied when we copy the data files of these
tablespaces to a new database location.
Objects that are residing in non-transportable tablespaces will have their data exported to
export dump files along with metadata for those objects. These dump files can later be
imported to get the data into a new database. Copying of the data file is not required in
this case.
A full transportable export is allowed from version 11.2.0.3 onwards, but a full transportable
import is allowed from Oracle 12c R1 onwards only. A full transportable export/import can
be useful in the following situations:
Upgrading a database: If we want to upgrade 11.2.0.3 database to Oracle 12c, we can use
this approach
Transporting non-CDB 11g database to Oracle 12c PDB: You can create an empty
PDB and transport non-CDB 11g databases into PDB
Moving the database to different OS: You can enable full transportable export/import
by setting the following two parameters in the expdp/impdp command: 
TRANSPORTABLE = ALWAYS: determines if the transportable option is being used. So
if you want to use transportable tablespace or database, you should specify this
parameter
FULL = Y: parameter specifies that entire database is being transported in FULL mode.
Restrictions in a full transportable export
The following restrictions apply to a full transportable export:
If you are exporting an encrypted tablespace or a table which has encrypted columns, you
should use the ENCRYPTION_PASSWORD parameter in order to supply a password to decrypt
the encrypted entity.
If you have encrypted a tablespace or tables with encrypted columns, you cannot
transport that data on a system with different endianness. However, transport of such data
will work on a system with the same endianness. But if you don't have any encrypted

entity (either tablespace or tables with encrypted columns), you can transport the database
to a system with different endianness by converting the endianness to a target platform
using the DBMS_FILE_TRANSFER package or by using the RMAN CONVERT command.
If you have a full transportable job running and if that job fails in between, you cannot
restart the job from that point. The job gets restarted from the beginning.
Objects being exported should have their storage segment either entirely in a non-
transportable tablespace (SYSTEM/SYSAUX) or entirely in a transportable tablespace.
Segments of an object should not overlap between two types of tablespaces.
If you have a LONG, LONG RAW column in a table and you are transporting the database
over a network using NETWORK_LINK, make sure that those tables should not reside in a
non-transportable tablespace (SYSTEM/SYSAUX).
When transporting tablespace over a network, auditing should not be enabled for objects
in a non-transportable tablespace (SYSTEM/SYSAUX) if an audit trial table is present in
a transportable tablespace.
If both the source and target database are on Oracle 12c version, then either the
COMPATIBLE parameter should be set to 12.0 or the VERSION parameter should be used in
export and should be set to 12.0.
The following restrictions apply to a full transportable import. Some of the limitations are
already covered in export limitations so the same thing is not being repeated:
A full transportable import is supported on Oracle 12c databases only. The source
database should be 11.2.0.3 or a higher version.
If the source and target databases are on different endianness, you should convert data
being transported to target system endianness using DBMS_FILE_TRANSFER or using RMAN
CONVERT.
You cannot transport encrypted tablespaces to a platform with different endianness.
Steps for exporting/importing a full database:
The following are the brief steps for carrying out a full transportable export and import:
1. Create an empty target database:
Before you begin doing the export, we need to create an empty target database where we
will be importing the source database. So you need to create an empty Oracle 12c
database.
2. Make user defined tablespaces read-only:
You need to make user defined tablespaces read-only so that no changes are made to
datafiles. This is required for transporting datafiles from the source to the target.
3. Perform a full database export:
Invoke expdp as a user with the DATAPUMP_EXP_FULL_DATABASE role to perform a full
database export. You need to use TRANSPORTABLE = ALWAYS, FULL = Y, and VERSION=12
arguments.

VERSION=12 parameter is required if source database is 11gR2 and above but not Oracle
12c. This parameter will create the export dump in Oracle 12c format. If source database
is already Oracle 12c, this parameter is not required.
Your export might fail if objects are not self-contained in the tablespace. Make sure that
objects are self-contained. The following is the example (full content is not shown):
[oracle@deo ~]$ expdp system/oracle full=y transportable=always       
version=12.0 dumpfile=expfull.dat directory=DUMP_DIR logfile=expfull.log
 Export: Release 12.1.0.2.0 - Production on Tue Jun 28 04:33:40 2016
 Copyright (c) 1982, 2014, Oracle and/or its affiliates. All rights 
reserved.
Connected to: Oracle Database 12c Enterprise Edition Release 12.1.0.2.0 - 
64bit Production
 With the Partitioning, OLAP, Advanced Analytics and Real Application 
Testing options
 Starting "SYSTEM"."SYS_EXPORT_FULL_01": system/******** full=y 
transportable=always version=12.0 dumpfile=expfull.dat directory=DUMP_DIR 
logfile=expfull.log
 Estimate in progress using BLOCKS method...
 Processing object type DATABASE_EXPORT/PLUGTS_FULL/FULL/PLUGTS_TABLESPACE
 Processing object type DATABASE_EXPORT/PLUGTS_FULL/PLUGTS_BLK
 Processing object type 
DATABASE_EXPORT/EARLY_OPTIONS/VIEWS_AS_TABLES/TABLE_DATA
 ...
 ...
You can import 11.2.0.3 or higher databases, but you can import in only 12.1 databases.
You must specify the VERSION parameter if you are exporting from a 11.2.0.3 database.
In the preceding example, the VERSION parameter was not required as a source database
is already on the 12.1 version. It was shown just for demo reasons.
The log file of the export will contain the list of datafiles that should be copied to the
target server.
4. Transport dumpfiles and datafiles to the target location:
Once the full database export is complete, you should transport the dump files and
datafiles to the target server. Note that if the target server has different endianness, you
should convert the endianness of the datafiles to the target server endianness using
DBMS_FILE_TRANSFER or RMAN CONVERT.
The following example shows how to use DBMS_FILE_TRANSFER package to change the
endianness of datafiles from source system to target system using either PUT_FILE or
GET_FILE procedure.
BEGIN
DBMS_FILE_TRANSFER.PUT_FILE(
source_directory_object => 'SRC_DUMP_DIR',
source_file_name => 'data.dbf',
destination_directory_object => 'DEST_DUMP_DIR',
destination_file_name => 'data.dbf',

destination_database => 'targetdb');
END;
/
Also, if the source database has an encrypted tablespace or table with an encrypted
column, the full database transportable export/import is not supported.
Also, you need to copy the datafiles while the tablespace is in read only. If you shut down
the DB and then copy the datafiles, it will have more changes than desired as the
checkpoint will happen. So it is better to copy the datafile once the full export is complete
and when the tablespaces are in read only. You can get the following error if you shut
down and copy the datafiles:
ORA-39123: Data Pump transportable tablespace job aborted
 ORA-19722: datafile 
/u01/app/oracle/oradata/orcl/pdb4/datafiles/low_cost_storage-01.dbf is an 
incorrect version
 Job "SYSTEM"."SYS_IMPORT_TRANSPORTABLE_01" stopped due to fatal error at 
Tue Jun 28 05:04:02 2016 elapsed 0 00:00:07
In my example, I have copied the datafiles to the required location and conversion is not
required.
5. Perform a full database transportable import:
Perform a full database transportable import into the target database. If the target
database is CDB, you can import the database as PDB. For that, you first need to create an
empty PDB.
In my example, I have already created an empty PDB called PDB4 and I am going to
perform a full transportable import. Use the following command:
[oracle@deo ~]$ impdp system/oracle@pdb4 dumpfile=expfull.dat 
directory=DUMP_DIR 
transport_datafiles='/u01/app/oracle/oradata/orcl/pdb4/datafiles/users-
01.dbf','/u01/app/oracle/oradata/orcl/pdb4/datafiles/low_cost_storage-
01.dbf' logfile=import.log 
 
Import: Release 12.1.0.2.0 - Production on Tue Jun 28 05:41:07 2016 
 
Copyright (c) 1982, 2014, Oracle and/or its affiliates.  All rights 
reserved. 
 
Connected to: Oracle Database 12c Enterprise Edition Release 12.1.0.2.0 - 
64bit Production 
With the Partitioning, OLAP, Advanced Analytics and Real Application 
Testing options 
Master table "SYSTEM"."SYS_IMPORT_TRANSPORTABLE_01" successfully 
loaded/unloaded 
Starting "SYSTEM"."SYS_IMPORT_TRANSPORTABLE_01":  system/********@pdb4 
dumpfile=expfull.dat directory=DUMP_DIR 
transport_datafiles=/u01/app/oracle/oradata/orcl/pdb4/datafiles/users-
01.dbf,/u01/app/oracle/oradata/orcl/pdb4/datafiles/low_cost_storage-01.dbf 

logfile=import.log  
Processing object type DATABASE_EXPORT/PRE_SYSTEM_IMPCALLOUT/MARKER 
Processing object type DATABASE_EXPORT/PRE_INSTANCE_IMPCALLOUT/MARKER 
Processing object type DATABASE_EXPORT/PLUGTS_FULL/PLUGTS_BLK 
Processing object type DATABASE_EXPORT/TABLESPACE 
ORA-31684: Object type TABLESPACE:"TEMP" already exists 
Processing object type DATABASE_EXPORT/PROFILE 
Processing object type DATABASE_EXPORT/SYS_USER/USER 
Processing object type DATABASE_EXPORT/SCHEMA/USER 
... 
... 
If there are any encrypted tablespaces or columns in the source database, you must also
specify the ENCRYPTION_PASSWORD parameter. The full transportable export will export all
the objects and data you need in order to create a complete copy of the source database.
Only the metadata of all the objects in the transportable tablespaces are unloaded into the
dump file set. The data itself is exported by copying the data files to the target host. The
LOGFILE parameter in the export command specifies the log file that contains the list of
the data files you must copy. If you want to perform a network_link transport, you can
do so using the following command:
impdp system/oracle@pdb4 network_link=deo 
transport_datafiles='/u01/app/oracle/oradata/orcl/pdb4/datafiles/users-
01.dbf','/u01/app/oracle/oradata/orcl/pdb4/datafiles/low_cost_storage-
01.dbf' logfile=import.log 
We have replaced the DUMPFILE and DIRECTORY parameters with a NETWORK_LINK
parameter. Also, all other steps remain the same even if you are doing a NETWORK_LINK
transport.
6. Make source database tablespace read write.
You can perform this step before step 5 as well.

Disabling logging for data pump import
When you're performing large imports, the duration of the import process is often a major
concern. Oracle Database 12c helps you make faster data loads by allowing you to disable
redo log writes during database import. This makes import operation very fast. The new
option DISABLE_ARCHIVE_LOGGING of the TRANSFORM parameter (which can be specified during
import) can disable archive logging. This parameter is especially recommended for full
transportable database import.
Certain Oracle data pump commands such as DDL and DCL (create and alter) will still
generate redo logs, but main data load operations will not generate any redo logs. Because
you aren't logging the changes, if the import fails in the middle, you must restart it. Also, if
force logging is enabled at database level, this parameter has no effect and changes will
generate redo logs.
Here's the general syntax of the TRANSFORM parameter:
TRANSFORM = transform_name:value[:object_type] 
There are multiple transform_name that are supported in the TRANSFORM parameter. The newly
introduced option in Oracle 12c is DISABLE_ARCHIVE_LOGGING. The following are examples of
using this parameter.
Example 1: Disable archive logging for tables as well as indexes:
impdp system/oracle@pdb4 dumpfile=expfull.dat directory=DUMP_DIR 
TRANSFORM=DISABLE_ARCHIVE_LOGGING:Y ... 
If we don't specify the optional object_type in the TRANSFORM argument, then it will disable
redo logging for both tables as well as indexes. We can choose to disable logging for only
table or indexes or both.
Example 2: Disable archive logging for only indexes:
impdp system/oracle@pdb4 dumpfile=expfull.dat directory=DUMP_DIR 
TRANSFORM=DISABLE_ARCHIVE_LOGGING:Y:INDEX ... 
We can also specify the TRANSFORM parameter multiple times, as shown in the following
example.
Example 3: Disable archive logging for only tables:
impdp system/oracle@pdb4 dumpfile=expfull.dat directory=DUMP_DIR 
TRANSFORM=DISABLE_ARCHIVE_LOGGING:Y:TABLE 
TRANSFORM=DISABLE_ARCHIVE_LOGGING:N:INDEX ... 

Exporting views as a table
Oracle 12c allows you to export a view as a table. You are, however, limited to just those
views that contain scalar columns and not views that reference object types or functions.
Instead of just exporting view definition SQL, Oracle data pump writes into dump files a
corresponding table definition and all the data that is visible in view. At import time, Oracle
data pump will create the table that has same definition as view and import the data. All the
dependent objects such as grants are also exported and those are imported on the
corresponding table. Only dependent objects which are applicable to the table are imported.
Data pump export/import has a new parameter VIEW_AS_TABLES, which allows any views to be
exported and imported as a table. You can specify one or more views that you want to export
and import.
This feature is useful in situations where you want to de-normalize tables. If you have a view
spanning multiple tables and you want to make that a de-normalize table, you can use
VIEW_AS_TABLES in export/import. Exporting the view as a table is limited only to views with
scalar columns. More complex views, including views containing object types or functions
are not supported. The following are the examples:
expdp system/oracle directory=DUMP_DIR dumpfile=exp.dat 
view_as_tables=sales_view tables=sales,sales_items 
In the preceding example, we exported SALES_VIEW which is a view. In the import command
we imported this view as table and renamed the table as SALES_SUMMARY using the REMAP_TABLE
parameter, as shown in the following code:
impdp system/oracle directory=DUMP_DIR dumpfile=exp.dat 
view_as_tables=sales_view remap_table=sales_view:sales_summary 
tables=sales,sales_items 
Here is another example:
impdp system/oracle directory=DUMP_DIR dumpfile=exp.dat 
view_as_tables=sales_view remap_table=sales_view:sales_items tables=sales 
table_exists_action=append 
In this example, we are importing SALES_VIEW into the SALES_ITEMS table and if the table
already exists, we want the data to be appended. For this we used the TABLE_EXISTS_ACTION
parameter with a value of append.
Compressing data during import
Oracle 12c allows you to compress or uncompress or change the compression algorithm
while importing data. This is possible by using the new option TABLE_COMPRESSION_CLAUSE in
the TRANSFORM parameter.

You can set the table compression clause as shown in the following code:
TRANSFORM = TABLE_COMPRESSION_CLAUSE:<compression_clause> 
You can specify the following values for the compression clause:
NONE: The original compression clause from the source table is omitted and the table
being imported will inherit the default compression setting of its tablespace in the target
database. This can be used when you want to apply a uniform compression rule for all
tables in a tablespace.
COMPRESS: This is equivalent to ROW STORE COMPRESS BASIC.
ROW STORE COMPRESS BASIC: Same as COMPRESS.
ROW STORE COMPRESS ADVANCED: Enables advanced row compression, which compress
data during all DML operations.
If the compression clause is a string of more than one word, you should enclose it in a
quotation. For example:
impdp system/oracle directory=DUMP_DIR dumpfile=exp.dat tables=sales 
TRANSFORM=TABLE_COMPRESSION_CLAUSE:"ROW STORE COMPRESS ADVANCED" 

Creating a SecureFile LOB during import
Oracle 12c allows you to convert all your LOBs to SecureFile LOB while importing. This is
made possible by using a new option, LOB_STORAGE of the TRANSFORM parameter. The
following is the syntax using the TRANSFORM parameter with the new LOB_STORAGE option:
TRANSFORM = LOB_STORAGE:<lob_storage_clause> 
The following values can be specified for lob_storage_clause:
DEFAULT: Using this value, the LOB storage clause is omitted and the imported table gets
default settings associated with session or instance. So if you have SecureFile LOB
enabled at the instance level, having the DEFAULT value will convert all LOB to
SecureFile LOBs.
NO_CHANGE: Using this value, LOB storage will remain the same as it is in the source
database table irrespective of LOB storage settings in the target database.
BASICFILE: Changes all LOBs to BasicFile LOB.
SECUREFILE: Changes all LOBs to SecureFile.

Specifying an encryption password
Oracle data pump allows encrypting data being written to export dump file. You can use
Oracle wallet for encryption, and even password-based encryption is also available. To
provide a password for encryption, data pump provides the ENCRYPTION_PASSWORD parameter,
which provides a password for encryption. The issue with this parameter is that the password
can easily be viewed using the ps command at the UNIX command prompt. There is no other
way to provide a password at the command line. Also, storing passwords in scripts is
considered as poor practice from a security point of view.
Oracle 12c introduced a new parameter ENCRYPTION_PWD_PROMPT, which when set to Y,
prompts the user for an encryption password. The password is read from standard input
streams and the terminal echo is suppressed while the password is being entered. Having this
parameter makes the password invisible to other users and the password is not displayed in
the ps command output.

SQL Loader enhancements
SQL Loader is an Oracle utility that is used to load data from an external text file to Oracle
tables. It has a very powerful parsing engine capable of reading the external text file of any
format and uploading the correct data to Oracle tables.
The following three major enhancements are introduced in SQL* Loader:
SQL* Loader express mode
SQL* Loader support for direct path loading of identity columns
SQL* Loader syntax enhancements and external table
Let's check out each of the enhancements in detail.

SQL* Loader express mode
Oracle 12c has introduced SQL Loader express mode. This can be used when you want to
quickly load a table without specifying any parameters. Only two parameters need to be
provided while running SQL Loader in express mode: username and table_name. All other
parameters assume default values. You can still specify some of the other parameters if you
want to. Note that not all parameters can be specified in SQL Loader express mode. Only
some parameters are applicable.
For using SQL Loader express mode, table columns should contain only scalar data types and
input datafile should contain only delimited character data. SQL*Loader doesn't use its usual
control file when you specify the express mode. Instead, it uses the table column definitions to
find the input field order and data types. When you run SQL*Loader in Express Mode, it
generates two files for use by you later on, in case you want to load data using regular
SQL*Loader or external tables. The first file is an SQL script, which contains DDL for
creating external tables and loading data using the INSERT INTO AS SELECT statement. This
script is provided only for convenience, in case we want to use them to perform load
operations using a traditional SQL Loader. The second file is a log file, which is similar to
what a traditional SQL Loader will create and it contains load operation results.
Let's look at an example: we have an ORDER_ITEMS_COPY table created and a spool file that has
comma separated values. The following code shows running an SQL Loader in express mode
(only username and table_name is provided):
[oracle@deo ~]$ sqlldr oe/oracle table=order_items_copy 
SQL*Loader: Release 12.1.0.2.0 - Production on Tue Jun 28 09:46:27 2016 
Copyright (c) 1982, 2014, Oracle and/or its affiliates.  All rights reserved. 
Express Mode Load, Table: ORDER_ITEMS_COPY 
Path used:      External Table, DEGREE_OF_PARALLELISM=AUTO 
Table ORDER_ITEMS_COPY: 
 665 Rows successfully loaded. 
Check the log files: 
 order_items_copy.log 
 order_items_copy_%p.log_xt 
for more information about the load. 
This has created two files: order_items_copy_%p.log_xt and order_items_copy.log. The %p
symbol stands for process ID.
The following are the various default values assumed by the SQL Loader when it is run in
express mode:
Datafile name is the same as table_name.dat, to be present in the same directory from
where you are running SQL Loader.

Load method is the default load method and it is an external table load method. If there
are certain errors, the load method is switched to direct path load.
DEGREE_OF_PRALLELISM is assumed as AUTO.
If the table has data, new data will be appended.
The log filename will be table_name.log.
The bad filename will be table_name_%p.bad, where %p is process ID.
The log filename is table_name_%p.log_xt, where %p is process ID.

Support for loading of identity columns
Oracle Database 12c introduces a new type of column called an identity column, which is
assigned a sequentially increasing or decreasing integer value from a sequence generator for
each insert into the table that contains the column. You can create a table with identity columns
or add an identity column to an existing table by specifying the GENERATED AS IDENTITY
clause.
SQL Loader supports loading data into tables with identity columns using direct path load.
The following are different identity columns with their meaning and the action that SQL
Loader will perform against these column types:
NUMBER GENERATED ALWAYS AS IDENTITY: You cannot load these types of identity
columns explicitly. The values for these columns are provided implicitly by the sequence
generator. ALWAYS in the preceding clause means that values are always provided by the
sequence generator. If you try to assign the values manually and explicitly, you will
receive an error. If null values are explicitly inserted, the sequence generator will
provide the value.
For example: We will create the table with an identity column and will try to insert
records into the table:
SQL> create table t1 ( 
  2  col1 number generated always as identity, 
  3  col2 varchar2(10)); 
 
Table created. 
 
SQL> insert into T1 (col2) values ('A'); 
 
1 row created. 
 
SQL> select * from T1; 
 
      COL1 COL2 
---------- ---------- 
         1 A 
 
SQL> insert into T1 values (null, 'B'); 
insert into T1 values (null, 'B') 
* 
ERROR at line 1: 
ORA-32795: cannot insert into a generated always identity column 
 
 
SQL> insert into T1 values (2,'B'); 
insert into T1 values (2,'B') 
* 
ERROR at line 1: 
ORA-32795: cannot insert into a generated always identity column 
NUMBER GENERATED BY DEFAULT AS IDENTITY: In this case, you are allowed to explicitly

insert values into identity columns, but you will get an error when you try to insert a NULL
value into the column. Also, if you explicitly load values into this column, the values
inserted by the sequence generator might collide with the explicitly inserted values. The
following is the example:
SQL> create table t1 ( 
  2  col1 number generated by default as identity, 
  3  col2 varchar2(10)); 
 
Table created. 
 
SQL> insert into T1 (col2) values ('A'); 
 
1 row created. 
 
SQL> select * from T1; 
 
      COL1 COL2 
---------- ---------- 
         1 A 
 
SQL> insert into T1 values (null, 'B'); 
insert into T1 values (null, 'B') 
                       * 
ERROR at line 1: 
ORA-01400: cannot insert NULL into ("SYS"."T1"."COL1") 
 
 
SQL> insert into T1 values (2,'B'); 
 
1 row created. 
 
SQL> select * from T1; 
 
      COL1 COL2 
---------- ---------- 
         1 A 
         2 B 
 
SQL> insert into T1 (col2) values ('C'); 
 
1 row created. 
 
SQL> select * from T1; 
 
      COL1 COL2 
---------- ---------- 
         1 A 
         2 B 
         2 C         <= Implicit value by sequence generator  
                        might collide with manually inserted value 
NUMBER GENERATED BY DEFAULT ON NULL AS IDENTITY: This type of identity column
can be loaded explicitly. The sequence generator will provide column values if the
column isn't loaded explicitly. That is, if explicit NULLs are inserted into the identity

column, by default the sequence generator is used. Any explicitly loaded NULL values
are replaced by sequence values. The following is the example:
SQL> create table T2( 
  2  col1 number generated by default on null as identity, 
  3  col2 varchar2(10)); 
 
Table created. 
 
SQL> insert into T2 (col2) values ('A'); 
 
1 row created. 
 
SQL> select * from T2; 
 
      COL1 COL2 
---------- ---------- 
         1 A 
 
SQL> insert into T2 values (null, 'B'); 
 
1 row created. 
 
SQL> insert into T2 values (3, 'C'); 
 
1 row created. 
 
SQL> select * from T2; 
 
      COL1 COL2 
---------- ---------- 
         1 A 
         2 B 
         3 C 
 
SQL> insert into T2 values (null, 'D'); 
 
1 row created. 
 
SQL> select * from T2; 
 
      COL1 COL2 
---------- ---------- 
         1 A 
         2 B 
         3 C 
         3 D        <= Implicit value by sequence generator  
                        might collide with manually inserted value 

SQL* Loader syntax enhancements and external tables
The following are the most important enhancements that pertain to SQL*Loader and external
table operations:
We can specify wild cards when specifying datafile names in controlfile. For example:
INFILE (sales*.dat) or (sales??.dat)
CSV files with embedded newlines are supported. For example: FIELDS CSV WITH
EMBEDDED
Table level NULLIF and date formats can be specified once for all character and date
fields.
For example:
FIELDS DATE FORMAT "DD-MM-YYYY"
For example:
FIELDS TERMINIATED BY "," NULLIF = "NA"
In this case, null values will be replaced by "NA"
The FIELD NAMES clause is added which tells us that the first record in the data files
contains the names and order of fields.
For example:
FIELD NAMES FIRST FILE: contains the field names in the first record of the first file
For example:
FIELD NAMES ALL FILES: contains the field names in the first record of all files
ALL FIELDS OVERRIDE: This is specific to external tables and it tells us that all fields are
present and that they are in the same order as columns in the external table. You only
need to specify fields that have a special definition.

Partitioning enhancements
The following are the partitioning related enhancements introduced in Oracle 12c:
Online partition operations
Reference partitioning enhancements
Multi-partition maintenance operations
Adding multiple partitions
Truncating multiple partitions
Merging multiple partitions
Splitting into multiple partitions
Dropping multiple partitions
Let's check out each of these enhancements in detail.

Online partition operations
Oracle 12c lets you move the table partition online. This operation does not affect currently
executing DML statements. The ability to move partitions online is a very important feature as
it helps in implementing ILM policies by moving partitions to low cost storage tablespaces.
You can also use this feature to compress the older partitions and save on space.
Online partition moves wait for existing DML statements (the ones which started before
executing the move statement) to complete before it initiates the move. Also, we don't have to
manually rebuild any indexes after the partition move. We can use the "update indexes" clause
to update all global indexes on the table automatically. The following are some of the
examples of the online partition move.
Example 1: Moving the partition to a low cost storage ONLINE:
SQL> alter table sales move partition SALES_1996 tablespace low_cost_storage 
update indexes online; 
Table altered. 
SQL> select index_name, status from dba_indexes where index_name = 
'I_SALES_PROD_CUST_ID'; 
INDEX_NAME                     STATUS 
------------------------------ -------- 
I_SALES_PROD_CUST_ID           VALID 
As you can see, the UPDATE INDEXES clause was used and the global index remains valid after
the online move of partition.
Example 2: Compressing the partition row level basic level:
SQL> alter table sales move partition SALES_1996 row store compress basic update 
indexes online; 
Table altered. 
Example 3: Compressing the partition row level advanced level:
SQL> alter table sales move partition SALES_1996 row store compress advanced 
update indexes online; 
Table altered. 

Reference partitioning enhancements
Reference partitioning was introduced in Oracle 11g. This feature provides the ability to
equipartition two tables related by referential integrity constraint. When you partition the table
by reference, the partition maintenance operation on the parent table automatically cascades to
the child table as well.
Oracle 12c introduces the following three new features in reference partitioning:
Interval reference partitioning: Allows interval partitioning for the parent table involved
in the reference partitioning
The CASCADE option for truncate partition operations, which cascades the operation to
the reference-partitioned child table
The CASCADE option for exchange partition operation, which cascades the operation to
the reference-partitioned child table
Let's check out each enhancement in detail.
Interval reference partitioning
In Oracle Database 12c, you can now use interval-partitioned tables as parent tables for
reference partitioning. When data is inserted into an interval partitioned parent table, a
partition will be created automatically when the first row is inserted for a new partition. And
since we have a reference partitioned child table, a corresponding partition will also be
created in the child table as well. The partition name in the child table is inherited from the
parent table whenever an interval partition is created in a child table. The new interval
partition will inherit the tablespace from the parent table partition, if the child table doesn't
have its own table-level default tablespace. Any operations that transform interval partitions to
normal partitions in the parent table, such as the ALTER TABLE SPLIT PARTITION operation on
an interval partition, reproduce the appropriate transformation in the child table and create the
necessary partitions in the child table.
For example, let's say we have a parent table (PARENT) and a child table (CHILD). Let's create
these tables, insert some values, and see how partitions are created automatically in both the
parent and child tables.
Let's create an interval partition parent table and reference partition child table:
SQL> create table parent( 
 2  col1 number primary key, 
 3  col2 number,       
 4  col3 varchar2(10)) 
 5  partition by range(col2) interval(100) 
 6  (partition P1 values less than (100)); 
Table created. 
SQL> create table child( 

 2  col1 number primary key, 
 3  col2 number not null, 
 4  col3 varchar2(10), 
 5  constraint child_fk foreign key(col2) references parent(col1) on delete 
cascade) 
 6  partition by reference(child_fk); 
Table created. 
Let's check the partitions, add data in the parent table, and check the partitions again:
SQL> select table_name, partition_name, high_value, interval from 
user_tab_partitions where table_name in ('PARENT','CHILD') order by 1, 2; 
TABLE_NAME           PARTITION_ HIGH_VALUE INT 
-------------------- ---------- ---------- --- 
CHILD                P1                    NO 
PARENT               P1         100        NO 
SQL> insert into parent values (10,10,'A'); 
1 row created. 
SQL> insert into parent values (110,110,'B'); 
1 row created. 
SQL> insert into parent values (210,210,'C'); 
1 row created. 
SQL> select table_name, partition_name, high_value, interval from 
user_tab_partitions where table_name in ('PARENT','CHILD') order by 1, 2; 
TABLE_NAME           PARTITION_ HIGH_VALUE INT 
-------------------- ---------- ---------- --- 
CHILD                P1                    NO 
PARENT               P1         100        NO 
PARENT               SYS_P791   200        YES 
PARENT               SYS_P792   300        YES 
As you can see, once we insert the data into the parent table, we can see additional partitions
with a system generated name. We have one P1 partition in the parent and child table, which is
not an interval partition. This partition was created when we created the tables.
Let's add some records in the child table too, and see if partitions are created automatically
and if the name of the partitions are inherited from the parent table:
SQL> insert into child values (1,110,'A'); 
1 row created. 
SQL> insert into child values (2,210,'B'); 
1 row created. 

SQL> commit; 
Commit complete. 
SQL> select table_name, partition_name, high_value, interval from 
user_tab_partitions where table_name in ('PARENT','CHILD') order by 1, 2; 
TABLE_NAME           PARTITION_ HIGH_VALUE INT 
-------------------- ---------- ---------- --- 
CHILD                P1                    NO 
CHILD                SYS_P791              YES 
CHILD                SYS_P792              YES 
PARENT               P1         100        NO 
PARENT               SYS_P791   200        YES 
PARENT               SYS_P792   300        YES 
6 rows selected. 
As you can see, once we added rows to the child table, partitions were created in the child
table and they inherited the name of partitions from the parent table.
If you split the partitions in the parent table, some interval partitions will get converted into
normal partitions. Correspondingly, you can see those partitions get converted to normal
partitions in the child table as well.
Let's try splitting the interval partitions, as shown in the following code:
SQL> alter table parent split partition for (150) at (150) into (partition P2, 
partition P3); 
Table altered. 
After splitting the partition on the parent table, you can see the corresponding partition split in
the child table and that the partition names are inherited from the parent table:
SQL> select table_name, partition_name, high_value, interval from 
user_tab_partitions where table_name in ('PARENT','CHILD') order by 1, 2; 
TABLE_NAME           PARTITION_ HIGH_VALUE INT 
-------------------- ---------- ---------- --- 
CHILD                P1                    NO 
CHILD                P2                    NO 
CHILD                P3                    NO 
CHILD                SYS_P792              YES 
PARENT               P1         100        NO 
PARENT               P2         150        NO 
PARENT               P3         200        NO 
PARENT               SYS_P792   300        YES
CASCADE option for truncate partition
If you specify the CASCADE option for a truncate table operation in Oracle Database 12c, the

database recursively truncates all the child tables that reference the table with an enabled ON
DELETE CASCADE referential constraint. This applies to all truncate operations, such as
TRUNCATE TABLE, ALTER TABLE TRUNCATE PARTITION, and ALTER TABLE TRUNCATE
SUBPARTITION. Because it's a recursive operation, it applies to all child tables and their child
tables, and so on.
In our example, we created a child table with a referential integrity constraint and added the ON
DELETE CASCADE clause. If you don't add this clause, then the cascade clause will not work on
the parent table and you will get the following error:
SQL> alter table parent truncate partition SYS_P811 cascade; 
alter table parent truncate partition SYS_P811 cascade 
           * 
ERROR at line 1: 
ORA-14705: unique or primary keys referenced by enabled foreign keys in table 
"OE"."CHILD" 
If there are multiple referential constraints between a parent and a child table, at least one of
those constraints needs to have ON DELETE CASCADE enabled for the TRUNCATE TABLE CASCADE
operation to succeed.
Since we have ON DELETE CASCADE enabled for our referential integrity constraint, we can
perform the CASCADE operation on a partition. The following is the example:
SQL> select * from child; 
     COL1       COL2 COL3 
---------- ---------- ---------- 
        1        110 A 
        2        210 B 
        3        310 C 
        4        410 D 
SQL> alter table parent truncate partition SYS_P811 cascade; 
Table truncated. 
SQL> select * from child; 
     COL1       COL2 COL3 
---------- ---------- ---------- 
        1        110 A 
        2        210 B 
        4        410 D
As you can see, we had four records in the child table and we did truncate one partition of the
parent table with the CASCADE option, and we saw the corresponding partition in the child table
get truncated. So the one record in that partition, which got truncated, is gone. The CASCADE
option is off by default.
Truncate can be targeted at any level in a reference-partitioned hierarchy and cascades to

child tables starting from a targeted table. Privileges are not required on a child table. The
CASCADE option is ignored if you don't have a reference partitioned child table. Any other
options specified for the operations such as DROP STORAGE or UPDATE INDEXES applies to all
tables affected by the operation.

Multi-partition maintenance operations
We can perform following multi-partition operations. In previous releases, Oracle allowed
splitting into and merging from only two partitions at a time. As a result, to split partitions
into an N partition or to merge N partitions into one, N - 1 DDL's need to be executed.
For example, if you want to rollup monthly data for an entire year into one partition, you
need to execute 11 DDL statements. An alternative approach is to create a new non-partitioned
table with data from these 12 partitions (for an entire year), drop the first 11 partitions (keep
the last partition for partition exchange) from the source table, and exchange the 12th partition
with the new table created.
Oracle 12c has made these maintenance operations much easier. In Oracle 12c, we can have
the following operations done on multiple partitions:
Adding multiple partitions
Truncating multiple partitions
Merging multiple partitions
Splitting into multiple partitions
Dropping multiple partitions
Let's look at an example for each of these operations to understand them better.
Adding multiple partitions
You can now use the ADD PARTITION clause of an ALTER TABLE statement to add multiple new
partitions at the same time. When you're adding multiple range partitions you should list the
partitions in ascending order of their upper bound value to the high end of a range-partitioned
or composite range-partitioned table, as long as you haven't defined a MAXVALUE partition. For
a list-partitioned table, you can add new list partitions using new sets of partition values,
provided that the default partition doesn't exist.
You can add the following partition types to a table in a single ALTER TABLE statement:
One or more range, list, or system partitions
One or more range or list sub-partitions to a partition
When you add multiple new partitions, local and global indexes behave the same as when
adding single partitions. The following is an example of this:
alter table sales add 
partition sales_q1_2004 values less than (to_date('01-APR-2004','DD-MON-YYYY')), 
partition sales_q2_2004 values less than (to_date('01-JUL-2004','DD-MON-YYYY')), 
partition sales_q3_2004 values less than (to_date('01-OCT-2004','DD-MON-YYYY')), 
partition sales_q4_2004 values less than (to_date('01-JAN-2005','DD-MON-YYYY')); 
In this example, we added four partitions to the table. Note that if you have a partition with
MAXVALUE, then you cannot add a new partition to the table. You will get the following error:

ERROR at line 2: 
ORA-14074: partition bound must collate higher than that of the last partition 
Truncating multiple partitions
You can truncate multiple partitions in the same command. Using the update indexes clause
will also update global indexes. If you don't specify the update indexes clause, then you need
to manually rebuild global indexes. Local indexes are automatically truncated:
SQL> alter table sales truncate partition sales_q1_2004, sales_q2_2004, 
sales_q3_2004, sales_q4_2004; 
Table truncated. 
Merging multiple partitions
In Oracle 11g, you can merge the content of two partitions only in single DDL. However, in
Oracle 12c, you can merge multiple partitions into one in a single DDL statement. After
merging the content of multiple partitions into a single partition, you can drop the original
partitions that are merged.
One condition while merging range partitions is that partitions must be adjacent and specified
in ascending order of their partition bound values. The new partition will inherit the highest
partition upper bound of the original partition.
For example:
SQL> alter table sales merge partitions sales_q1_2004, sales_q2_2004, 
sales_q3_2004, sales_q4_2004 into partition sales_2004; 
Table altered. 
Note that we need to use the PARTITIONS keyword, as shown in bold. You can also use a range
of partitions instead of listing down all partitions. You can specify the first partition to last
partition to merge all partitions in-between, including the first and last. The following is an
example which has the same result as the preceding example:
SQL> alter table sales merge partitions sales_q1_2004 to sales_q4_2004 into 
partition sales_2004; 
Table altered. 
If you merge a partition with MAXVALUE with another partition, the resulting upper bound value
of the merged partition will be MAXVALUE only.
When merging list partitions, a resulting partition value list will be the union of all list
partition values to be merged. A partition with a DEFAULT list value, when merged with other
partitions will result in a DEFAULT partition only.
Splitting multiple partitions

You can split a partition into multiple partitions. In previous releases, you were able to split
partitions into two partitions only in a single DDL. But in Oracle 12c, you can now split a
partition into multiple partitions. When splitting a partition into multiple partitions, the
segment associated with the current partition is preserved and new segments are created. Each
new partition obtains a new segment and inherits all of the unspecified physical attributes
from a current source partition. You can use an extended syntax in the partition split command
to provide the physical attributes for a new partition, just like when you create a new partition
table. Also, the range or list value clause for the last new partition in the partition split is
derived based on the high bound of the source partition:
If you want to split range partition into N partitions, you must specify N - 1 values of the
partitioning key column within a range of values of the source partition.
If you want to split the list partition into N partitions, you must specify N - 1 literal
values.
If you are splitting the DEFAULT list partition or the MAXVALUE range partition into N
partitions, Oracle will create N - 1 new partitions with a literal value for list partition or
higher bound values you specified for range partition. The last Nth partition will contain
the DEFAULT value for the list partition and the MAXVALUE for the range partition.
The following are some examples.
Example 1: Splitting the range partition
In this example, we have a SALES_2000 partition in the SALES table. We want to split this
partition into four partitions, one for each quarter. The following is the command to do so:
alter table sales split partition sales_2000 into ( 
partition sales_q1_2000 values less than (to_date('01-APR-2000','DD-MON-YYYY')), 
partition sales_q2_2000 values less than (to_date('01-JUL-2000','DD-MON-YYYY')), 
partition sales_q3_2000 values less than (to_date('01-OCT-2000','DD-MON-YYYY')), 
partition sales_q4_2000); 
Note that you should not have a partition bound for the last partition, or else you will get the
following error:
ORA-14805: last resulting partition cannot contain bounds 
You can also see that data got split automatically into new partitions:
SQL> select count(1) from sales partition (sales_q1_2000); 
 COUNT(1) 
---------- 
    62197 
SQL> select count(1) from sales partition (sales_q2_2000); 
 COUNT(1) 
---------- 
    55515 

Example 2: Splitting the list partition:
alter table list_part split partition INDIAN into( 
partition north_indian values ('DELHI','CHANDIGARH'), 
partition south_indian values ('TAMILNAIDU','KERALA'), 
partition east_indian values ('ASSAM','MEGHALAYA'), 
partition west_indian values ('MAHARASHTRA','GOA')); 
Example 3: Splitting the MAXVALUE partition
We currently have a PMAX partition, which is holding the higher bounds as max values. We can
split this partition and create four new partitions and also specify PMAX again in the command
so that PMAX retains the MAXVALUE. Or else the last partition we specify will have a higher
bound of MAXVALUE:
alter table sales split partition pmax into ( 
partition sales_q1_2004 values less than (to_date('01-APR-2004','DD-MON-YYYY')), 
partition sales_q2_2004 values less than (to_date('01-JUL-2004','DD-MON-YYYY')), 
partition sales_q3_2004 values less than (to_date('01-OCT-2004','DD-MON-YYYY')), 
partition sales_q4_2004 values less than (to_date('01-JAN-2005','DD-MON-YYYY')), 
partition pmax); 
Dropping multiple partitions
We can drop multiple partitions or sub-partitions from a range or a list-based partition table.
The local and global index operations remain the same when the single partition is dropped.
The following is an example of this:
SQL> alter table sales drop partition sales_q1_2004, sales_q2_2004, 
sales_q3_2004, sales_q4_2004; 
Table altered. 

Index enhancements for partitioned tables
The following are the key enhancements introduced in Oracle 12c, related to indexes on a
partitioned table:
Partial index for partitioned tables
Asynchronous global index maintenance
The DBMS_PART package
Let's take a look at each of them in detail.

Partial index for partitioned tables
Before we go into the details of this feature, let's briefly review index partition concepts.
Partition index is similar to partition tables, where we have multiple partitions created for an
index. We have two types of partitioned indexes - global and local.
A global partition index can be created on a non-partitioned table or a partitioned table and it
has its own partitioning key. Global index can also be created as a non-partitioned index.
Local index, on the other hand, are always partitioned indexes and they follow the same
partition key as that of a partitioned table. Local indexes cannot be created on a non-
partitioned table. Number of partitions in a local index corresponds to the number of
partitions in a table.
In Oracle 12c, you can now create global and local partitioned indexes on a subset of table
partitions, enabling more flexibility during index creation. This is called partial index for a
partitioned table. This feature is supported by attaching new indexing property to a table
partition. Now in Oracle 12c, you can provide indexing property to table partitions,
specifying if a specific partition should be included in index creation. Oracle 12c now
supports two types of indexes:
FULL index: This is same as what we had in previous releases. An index will be created
on all partitions irrespective of the indexing property of the partition.
PARTIAL index: This is a newly introduced index type. An index will be created based
on the indexing property of partitions.
During partial index creation, Oracle refers to the indexing property of a partition and
decides if a partition should be part of the index. If you are creating global index, partial
global index will include only those partitions in which the indexing property is ON. If you are
creating local index, partial local index will be USABLE for only those partitions in which the
indexing property is ON. In other partitions, local index partition will be UNUSABLE.
FULL index creation is the default for creating indexes if we don't specifically say PARTIAL.

Specifying default indexing property for table/partitions
You can specify the default index property for a table or a table partition. Oracle Database 12c
extends the CREATE TABLE syntax by adding the new clause INDEXING to help you specify the
default indexing attributes of a table. You can set the INDEXING clause to ON or OFF.
If you specify the INDEXING property for a table and do not specify any indexing property for
partitions, then all the partitions will inherit the indexing property of the table. You can use
alter the table statement to change the indexing property of individual partitions at a later
point.
The following example shows the defining indexing property for tables and partitions:
create table T1 ( 
col1    number, 
col2    date, 
col3    varchar2(10)) 
indexing off 
partition by range(col2)( 
partition p1 values less than (to_date('01-JAN-2001','DD-MON-YYYY')), 
partition p2 values less than (to_date('01-JAN-2002','DD-MON-YYYY')) indexing 
off, 
partition p3 values less than (to_date('01-JAN-2003','DD-MON-YYYY')) indexing 
on, 
partition p4 values less than (to_date('01-JAN-2004','DD-MON-YYYY')) indexing 
on); 
I have highlighted the important clauses in the preceding command. When we are creating the
partition table T1, we have set the indexing property at table level (just after defining the
columns) and that is the default indexing property of the table. In our case, we are by default
setting indexing to OFF. It means that any new partition that is added to this table without
specifying the indexing property will have indexing set to OFF by default.
We have also set indexing property at partition level. Here are some interesting things to note:
Partition P1 does not have indexing property defined. So it will inherit the default
indexing property at table level which is OFF. So partition P1 will not be included when
we create a global partial index, and the local partial index partition for P1 will be
UNUSABLE.
For partition P2, we have defined the indexing property and its value is the same as P1.
So partition P2 will not be considered in partial indexing.
For partition P3 and P4, we have set the indexing property to ON, which overrides the
table level default indexing property. These partitions will be considered in global and
local partial indexes.
Also, note that if we are creating a FULL index (and not the PARTIAL index), all partitions will
be included in index creation irrespective of the indexing property set.

Creating partial local and global indexes
As we discussed before, we have two types of global and local partitioned indexes in Oracle
12c:
FULL index: Which includes all partitions of tables irrespective of the indexing property
defined at table level and partition level.
PARTIAL index: Which includes only partitions on which the indexing property is set to
ON.
FULL index creation does not need any additional clause or change. There has been no change
in syntax for creating a FULL index. Also, FULL index creation is the default in Oracle 12c.
For creating PARTIAL indexes, you need to use the INDEXING PARTIAL clause while creating
the index. The following is an example of this:
SQL> create index I_T1_COL1 on T1(col1) global indexing partial; 
Index created. 
In the preceding example, I created a partial global index. Although the global keyword is not
required and is default if you don't specify LOCAL, I just provided that for readability. When we
specify INDEXING PARTIAL, this global index includes only those partitions of the T1 table,
which has the indexing property ON (Partition P3 and P4).
Let's try to create a local index using the partition key COL2:
SQL> create index I_T1_COL1_COL2 on T1(col2, col1) local indexing partial; 
Index created. 
As with the local index, we are expecting only partition P3 and P4 will be usable as they had
indexing property set to ON. Partition P1 and P2 had the indexing property turned off. We can
check this using the DBA_IND_PARTITIONS view, as shown in the following code:
SQL> select index_name, partition_name, STATUS from dba_ind_partitions where 
index_name = 'I_T1_COL1_COL2'; 
INDEX_NAME           PARTITION_ STATUS 
-------------------- ---------- -------- 
I_T1_COL1_COL2       P4         USABLE 
I_T1_COL1_COL2       P3         USABLE 
I_T1_COL1_COL2       P2         UNUSABLE 
I_T1_COL1_COL2       P1         UNUSABLE 
As you can see, partitions P1 and P2 are shown as UNUSABLE, whereas partition P3 and P4 are
shown as USABLE.
You can override the indexing properties by specifying INDEXING FULL or eliminating the
INDEXING PARTIAL clause while creating the index. This will include all the partitions of the

table.
Let's try to create a FULL local index, as shown in the following code:
SQL> drop index I_T1_COL1_COL2; 
Index dropped. 
SQL> create index I_T1_COL1_COL2 on T1(col2, col1) local indexing full; 
Index created. 
SQL> select index_name, partition_name, STATUS from dba_ind_partitions where 
index_name = 'I_T1_COL1_COL2'; 
INDEX_NAME           PARTITION_ STATUS 
-------------------- ---------- -------- 
I_T1_COL1_COL2       P1         USABLE 
I_T1_COL1_COL2       P2         USABLE 
I_T1_COL1_COL2       P3         USABLE 
I_T1_COL1_COL2       P4         USABLE 
Modifying the indexing property of a partition/table
You can modify the indexing property of an individual partition using the ALTER TABLE
MODIFY PARTITION statement.
To modify the indexing property of an individual partition, use the following code:
SQL> alter table sales modify partition SALES_Q4_2003 indexing off; 
Table altered. 
Effect of partial indexes on SQL plans
So what will happen to SQL plans when data that we are fetching spans partitions having
indexing properties ON and OFF. If we are selecting data from multiple partitions and the
optimizer decides to go with the index scan, it can use the index scan for only those partitions
where indexing is ON. Other partitions where indexing property is OFF, the optimizer goes for
FULL PARTITION SCAN. So the partial plan will be the index scan and the rest of the plan will
be the partition FULL scan.
We are now going to look at an example. Let's consider partitions for year 2000 and 2001. I
have made indexing OFF for year 2001 and indexing is ON for year 2000. We have a partial
local index on the TIME_ID column. We can see from the following code that partitions
corresponding to year 2000 are USABLE, whereas partitions corresponding to 2001 are
UNUSABLE:
SQL> select index_name, partition_name, STATUS from dba_ind_partitions where 
index_name = 'T_TIME_ID' and (partition_name like '%2001' or partition_name like
'%2000') order by 3;  

INDEX_NAME           PARTITION_NAME                 STATUS 
-------------------- ------------------------------ -------- 
T_TIME_ID            SALES_Q1_2001                  UNUSABLE 
T_TIME_ID            SALES_Q3_2001                  UNUSABLE 
T_TIME_ID            SALES_Q2_2001                  UNUSABLE 
T_TIME_ID            SALES_Q4_2001                  UNUSABLE 
T_TIME_ID            SALES_Q2_2000                  USABLE 
T_TIME_ID            SALES_Q1_2000                  USABLE 
T_TIME_ID            SALES_Q4_2000                  USABLE 
T_TIME_ID            SALES_Q3_2000                  USABLE 
Let's check the explain plan for the following SQL. The predicates in the following SQL is
spawning from 2000 and 2001. So we should see a plan with an index scan for year 2000
partitions, and a FULL scan for partitions on year 2001.
The following plan shows that INDEX RANGE SCAN is used for partition 16 and TABLE ACCESS
FULL is used for partition 17:
-----------------------------------------------------------------
| Operation                 | Name      | Rows  | Pstart| Pstop |
-----------------------------------------------------------------
| SELECT STATEMENT          |           |     1 |       |       |
|  SORT AGGREGATE           |           |     1 |       |       |
|   VIEW                    | VW_TE_2   | 16494 |       |       |
|    UNION-ALL              |           |       |       |       |
|     PARTITION RANGE SINGLE|           | 11124 |    16 |    16 |
|      INDEX RANGE SCAN     | T_TIME_ID | 11124 |    16 |    16 |
|     PARTITION RANGE SINGLE|           |  5370 |    17 |    17 |
|      TABLE ACCESS FULL    | SALES     |  5370 |    17 |    17 |
-----------------------------------------------------------------
Predicate Information (identified by operation id):
---------------------------------------------------
5 - access("SALES"."TIME_ID">=TO_DATE(' 2000-12-10 00:00:00',
'syyyy-mm-dd hh24:mi:ss') AND
"SALES"."TIME_ID"<TO_DATE(' 2001-01-01 00:00:00', 'syyyy-mmdd
hh24:mi:ss'))
7 - filter("SALES"."TIME_ID"<=TO_DATE(' 2001-01-10 00:00:00', 'syyyy-mmdd
hh24:mi:ss'))
Data dictionary changes
Data dictionary tables holds metadata information about user tables and indexes. With changes
to index partitioning types, certain changes are also made to data dictionary tables.
The following tables are modified in Oracle 12c to provide information about indexing
properties of table/partitions and also to identify if the index is FULL or PARTIAL:
DBA|ALL|USER_INDEXES: Has a new column, INDEXING, with possible values
PARTIAL/FULL
DBA|ALL|USER_IND_PARTITIONS: The column STATUS shows USABLE if the partial local
index partition is usable, and UNUSABLE otherwise
DBA|ALL|USER_PART_TABLES: Has a new column, DEF_INDEXING, with possible values

ON/OFF, indicating whether indexing is turned on at the table level
DBA|ALL|USER_TAB_PARTITIONS: Has a new column, DEF_INDEXING, with possible values
ON/OFF, indicating whether indexing is turned on at the partition level
DBA|ALL|USER_TAB_SUBPARTITIONS: Has a new column, DEF_INDEXING, with possible
values ON/OFF, indicating whether indexing is turned on at the sub-partition level

Asynchronous global index maintenance
Global index maintenance needs to be performed whenever a partition is dropped or truncated
from a partition table. During global index maintenance, all the records corresponding to
those partitions are removed from the global index. Until the previous release, global indexes
were maintained during dropping or truncating of table partition using the UPDATE INDEXES
clause. If this clause if not used, all global indexes on the table will be marked as UNUSABLE
and rebuild of the index is required to make it usable. Oracle 12c has introduced an
enhancement where we can defer global index maintenance to some other time. So when we
drop or truncate a partition and do not use the UPDATE INDEXES clause, global indexes will not
be marked as UNUSABLE. The index entries from the dropped or truncated index partition
remain, but are invalid, and the database simply ignores them. The maintenance of the index
can be done asynchronously and can be put off to a later point in time. Asynchronous global
index maintenance is the default option now. So if we skip using the UPDATE INDEXES clause
in the drop or truncate partition command, it will keep the global indexes as USABLE and use
asynchronous global index maintenance.
The following are the limitations that do not support asynchronous global index maintenance:
Tables with object types
Tables with domain indexes
Tables owned by SYS
So when we partition drop or truncate and defer global index maintenance, how are global
indexes maintained? Oracle 12c has introduced a new scheduler job to automatically cleanup
all global indexes - SYS.PMO_DEFERRED_GIDX_MAINT_JOB. This job is scheduled to run at 2:00
AM every day. You can also run this job manually if you want to proactively cleanup stale
records from the global index.
You can find out if a global index or index partition contains stale entries due to deferred
index maintenance during a DROP/TRUNCATE PARTITION operation, by viewing the new
column, ORPHANED_ENTRIES in the DBA_INDEXES or DBA_IND_PARTITIONS view. The column
can have one of the following three values:
YES: The index (partition) contains orphaned entries
NO: The index (partition) doesn't contain any orphaned entries
N/A: The index is a local index or a global index, or an index on a non-partitioned table,
and hence the property is not applicable

The DBMS_PART package
Oracle Database 12c provides a new package named DBMS_PART to help you maintain and
manage your partitioned tables and indexes. When you perform partition maintenance with
asynchronous global index maintenance, global indexes may be left with stale index rows.
Although these extra rows don't affect the correctness of queries that use the indexes, they
may have an adverse impact on storage and performance.
The DBMS_PART package contains a procedure named CLEANUP_GIDX to identify and clean up
the stale index row from a global index. This procedure finds all global indexes that require a
cleanup and restores them to a clean state. This is the same procedure that is being used in the
DBMS scheduler job, SYS.PMO_DEFERRED_GIDX_MAINT_JOB, to automatically clean stale
entries from global indexes:
PROCEDURE CLEANUP_GIDX 
Argument Name                  Type                    In/Out Default? 
------------------------------ ----------------------- ------ -------- 
SCHEMA_NAME_IN                 VARCHAR2                IN     DEFAULT 
TABLE_NAME_IN                  VARCHAR2                IN     DEFAULT 
In addition to executing the DBMS_PART.CLEANUP_GIDX procedure, you can also execute the
well-known SQL statement, ALTER INDEX REBUILD <PARTITION_NAME>, to clean up the stale
index entries, and the SQL statement, ALTER INDEX <PARTITION_NAME> COALESCE CLEANUP, to
clean up any orphaned stale entries in the index blocks.

Oracle Database Migration Assistant for
Unicode
Database Migration Assistant for Unicode (DMU) is a new utility introduced in Oracle 12c.
This is a next generation tool that provides end-to-end solution for migrating standard
database characters set to Unicode. Previous utilities such as CSSCAN and CSALTER have
been deprecated in new releases, and are no longer supported. DMU supports all data types
and deals with all types of database objects such as Mviews, indexes, triggers, constraints,
package, and so on, which are affected by conversion of tables so they are property synched
up after migration. DMU is fast and provides guided GUI interface. This makes migration
much easier.
You can launch DMU from the $ORACLE_HOME/dmu/dmu/bin directory. The following
screenshot shows the GUI front screen for DMU:

DMU provides advanced data analysis and cleansing tools. It also has a validation mode to
check data integrity and compliance with Unicode standard. DMU is robust in error handling
and failure recovery and it prevents data loss.

SecureFile LOB enhancements
SecureFile LOB comes up with many advantages in terms of performance of unstructured
data. Oracle is encouraging customers to move to SecureFile LOB. One of the enhancements
in this regard is the change in the default value of the DB_SECUREFILE initialization parameter.
In previous releases, the default value of DB_SECUREFILE was PERMITTED. It means that
SecureFile LOB will be created when we explicitly specify it during creation.
In Oracle 12c, the default value of the initialization parameter DB_SECUREFILE is changed to
PREFERRED. The default value of this parameter is set to PREFERRED if the COMPATIBLE
parameter is 12.0.0.0.0 or above. The setting PREFERRED for the DB_SECUREFILE initialization
parameter means that all LOBs are created as SecureFiles unless you specify BASICFILE in the
LOB storage clause. When you set PREFERRED as the value, cases where BASICFILE would
otherwise be inherited from the partition or column level LOB storage are ignored, and the
LOBs will be created as SecureFiles instead.
The idea behind making SecureFiles the default for unstructured data is to improve
performance when handling those types of data.

The row limiting clause
Oracle 12c has introduced new syntax for the row limiting clause. Queries that order data and
limit row output are called Top-N queries and are widely used in business intelligence. In
previous version, we can write our own Top-N query by using ordering a clause in an inner
query and limiting the number of rows in an outer query using ROWNUM.
Oracle 12c provides a more sophisticated way of writing a Top-N query. You can specify the
number of rows or percentage of rows to return with the FETCH FIRST/NEXT keywords. You
can also specify the OFFSET keyword to exclude the leading rows of the result set.
For example, specifying an offset helps you answer questions such as who are the second top
ten students in an institution-this query will return the students ranked 11th through 20th in an
organization.
The following is the syntax for row_limiting_clause:
 [ OFFSET offset { ROW | ROWS } ] 
[ FETCH { FIRST | NEXT } [ { rowcount | percent PERCENT } ] 
    { ROW | ROWS } { ONLY | WITH TIES } ] 
The following is the meaning of each keyword used in the preceding syntax:
OFFSET: This parameter specifies the number of rows to skip before giving the output. So
if we specify OFFSET as 10, the output will be given starting from the 11th row.
ROW|ROWS: These keywords are for semantic clarity only, and they do not alter the output.
FETCH: This parameter is used to specify how many rows to be fetched in the output. We
can also specify the rows in percentage. For example, we can specify 100 rows to be
fetched or 10 percent of total rows to be fetched. If we don't specify anything for FETCH,
it will fetch all records from OFFSET+1.
rowcount|percent: As previously mentioned, we can specify either rowcount (default) or
percent.
FIRST | NEXT: These keywords can be used interchangeably and are provided for
semantic clarity. They do not alter the output.
ONLY | WITH TIES: If you specify ONLY, then the exact number of rows or percentage of
rows will be returned. But if you specify WITH TIES, it will return all the rows that match
the sort key of the last row fetched.
Let's look at a few examples to understand the usage of clauses.
If we have a CUSTOMERS table and I want to check the top 10 customers that have the lowest
income level, I can write the following query in previous versions of Oracle:
select * from (select CUST_ID, CUST_CITY, CUST_INCOME_LEVEL from customers order 
by CUST_INCOME_LEVEL) where rownum <= 10; 

We have an inner sub-query that list the customers in order of CUST_INCOME_LEVEL, and an
outer query that restricts the number of rows to 10, giving the top 10 customers with the
lowest income level.
If we want to get next set of 10 customers having lowest income level, things can get a little
complicated. Also, if we want to list all customers that match the last income level of the
lowest top 10 income level, it becomes even more complex. But this can be written very easily
using new syntax in Oracle 12c.
The following query produces same output as the previous query:
SQL> select CUST_ID, CUST_CITY, CUST_INCOME_LEVEL from customers order by 
CUST_INCOME_LEVEL fetch first 10 rows only; 
  CUST_ID CUST_CITY                      CUST_INCOME_LEVEL 
---------- ------------------------------ ------------------------------ 
    36148 Ingolstadt                     A: Below 30,000 
    14815 Dolores                        A: Below 30,000 
    11259 Leeds                          A: Below 30,000 
     7704 San Mateo                      A: Below 30,000 
     4148 Norwich                        A: Below 30,000 
    50592 King's Lynn                    A: Below 30,000 
    47037 San Mateo                      A: Below 30,000 
    25781 Greenwich                      A: Below 30,000 
    43259 Asten                          A: Below 30,000 
    39703 Belfast City                   A: Below 30,000 
10 rows selected. 
Note that we ordered based on CUST_INCOME_LEVEL and used the FETCH FIRST 10 ROWS ONLY
clause. This will fetch the top 10 customers that have the lowest income level.
If you want to get the next 10 customers that have the lowest income level, you can use the
OFFSET keyword and offset the result by 10. So Oracle will exclude the first 10 records and
provide the output from the 11th to 20th record, as shown in the following code:
SQL> select CUST_ID, CUST_CITY, CUST_INCOME_LEVEL from customers order by 
CUST_INCOME_LEVEL offset 10 rows fetch next 10 rows only; 
  CUST_ID CUST_CITY                      CUST_INCOME_LEVEL 
---------- ------------------------------ ------------------------------ 
    18370 Stuttgart                      A: Below 30,000 
    14815 Dolores                        A: Below 30,000 
    11259 Leeds                          A: Below 30,000 
     7704 San Mateo                      A: Below 30,000 
     4148 Norwich                        A: Below 30,000 
    36115 Waddinxveen                    B: 30,000 - 49,999 
    14782 Arnemuiden                     B: 30,000 - 49,999 
    11226 Delhi                          B: 30,000 - 49,999 
     7671 Hyderabad                      B: 30,000 - 49,999 
     4115 Velp                           B: 30,000 - 49,999 
10 rows selected. 

As you can see, we have two top 10 rows now.
Note
Note that FETCH NEXT and FETCH FIRST have the same meaning.
When we use offset, it's logical to say FETCH NEXT, so Oracle has introduced this additional
syntax. But even if we use FETCH FIRST with OFFSET, it will give the same result.
If you specify PERCENT, you will get the rows output as the percentage of total rows in the
table. So if you specify FETCH FIRST 10 PERCENT ROWS ONLY and the table has 1000 rows,
Oracle will fetch the top 100 rows.
Instead of ONLY at the end of the clause, you can also use WITH TIES. When you use WITH
TIES, the value in the last row of the output is compared with further rows and all rows
matching that value will be displayed.
We will now look at an example of this. As you can see in the following output, we asked for
10 ROWS WITH TIES. In this case, it compared the value of column CUST_INCOME_LEVEL (since
that's the one we are ordering by) in the last rows with further rows down the line and
displayed all rows that match the value. The value in the last row (10th row) was A: Below
30,000, which was compared with other rows down in the table and all records matching this
value were displayed. We had 16 customers having this income level. If we use the 10 ROWS
ONLY clause, then it will only display 10 rows:
SQL> select CUST_ID, CUST_CITY, CUST_INCOME_LEVEL from customers order by 
CUST_INCOME_LEVEL fetch first 10 rows with ties; 
  CUST_ID CUST_CITY                      CUST_INCOME_LEVEL 
---------- ------------------------------ ------------------------------ 
    36148 Ingolstadt                     A: Below 30,000 
   103935 Arbuckle                       A: Below 30,000 
   103545 Aladdin                        A: Below 30,000 
   100708 Arbuckle                       A: Below 30,000 
   101355 Elba                           A: Below 30,000 
   100875 Mackville                      A: Below 30,000 
   103884 Blountstown                    A: Below 30,000 
   104234 Chesterfield                   A: Below 30,000 
   103430 Puako                          A: Below 30,000 
   100880 New Auburn                     A: Below 30,000 
   100504 Montara                        A: Below 30,000 
    18370 Stuttgart                      A: Below 30,000 
    14815 Dolores                        A: Below 30,000 
    11259 Leeds                          A: Below 30,000 
     7704 San Mateo                      A: Below 30,000 
     4148 Norwich                        A: Below 30,000 
16 rows selected. 

Extended data types
Oracle 12c has enhanced VARCHAR2, NVARCHAR2, and RAW data types. You can now specify
extended sizes for these data types, VARCHAR2 and NVARCHAR2, which are used to store 4,000
bytes max and can now store up to 32,767 bytes. Similarly, the RAW data type which used to
store 2000 bytes max can now store up to 32,767 bytes. This enables users to store longer
strings in a database. However, depending on the declared size of the column, Oracle can
store the column values inline or out of line.
You can create a new table with extended data types or add a new column with extended data
types to an existing table. In this case, if the declared length of data exceeds 4000 bytes for
VARCHAR2 or NVARCHAR2 then the data will be stored out of line. But if the declared length is
less than 4000 bytes, then the data for the column will be stored in line. Similarly, for RAW data
types, if the declared length of data is less than 2,000 bytes, then data is stored inline. But if the
declared length of data is more than 2,000 bytes, then the data will be stored out of line. Out of
line storage of data is similar to storing CLOB or BLOB data.
If you extend existing VARCHAR2 or NVARCHAR2 columns above 4,000 bytes, in this case Oracle
performs in-place length extension and does not migrate the inline storage to external LOB
storage. Similarly, if the raw data type is extended beyond 2,000 bytes, Oracle performs in-
place length extension.
Oracle recommends not to extend data types excessively as it can cause the following
performance issues:
Row chaining may occur.
If the column data type is extended, entire data will be stored inline and every time that
data will be read, even if we are selecting other columns in the table. Extended data type
columns stored inline can negatively affect performance.
To migrate to out of line data storage for extended data types, you need to recreate the
table or use ALTER TABLE MOVE.

Enabling extended data types
To enable extended data types, you need to set the initialization parameter MAX_STRING_SIZE
to EXTENDED. This parameter can have the following two values:
MAX_STRING_SIZE=EXTENDED: The size limit is 32,767 bytes for the VARCHAR2,
NVARCHAR2, and RAW data types
MAX_STRING_SIZE=STANDARD: This is the default value, and when you use this setting, the
size limits for releases prior to Oracle Database 12c apply: 4,000 bytes for the VARCHAR2
and NVARCHAR2 data types, and 2,000 bytes for the RAW data type
The following are the steps to enable extended data types:
Clean shutdown the database
Start the database in UPGRADE mode (startup upgrade)
Change the initialization parameter MAX_STRING_SIZE to EXTENDED
Run $ORACLE_HOME/rdbms/admin/utl32k.sql as sysdba
Restart the database
Note that once you enable the extended data type you cannot go back to the standard data type
by setting MAX_STRING_SIZE to STANDARD again.
You can create a table with an extended data type column, add a new column with an extended
data type to an existing table, or modify an existing column to an extended data type. The
following are the examples:
create table T1 (col1 number, col2 varchar2(32767)); 
alter table T1 add (col2 varchar2(32767)); 
alter table T1 modify (col2 varchar2(32767)); 
If you have an index on an existing column and you want to change the column to extended
data type, the existing index on the column will prevent data type extension. You must first
drop the index on the column, then modify the column to the new extended data type, and then
recreate the index.

Summary
In this chapter, we saw new enhancements related to data pump. We saw how we can use data
pump to perform full transportable database export. We also saw several other new features
of data pump such as disabling logging for data pump, exporting views as tables,
compressing data during import, creating SecureFile LOB during import, and specifying
encryption passwords. We then saw the new features related to SQL* Loader. Specifically, we
saw how we can use SQL* Loader express mode to quickly load the data, how we can use
SQL* Loader to load identity columns, and a few syntax enhancements. We then saw several
new enhancements related to partitioning. We used several examples to explain these new
features. We learned new features related to online partition operations and how they are
enhanced to wait for existing DML changes to complete. We also saw referenced partitioning
enhancements and various multi-partition maintenance operations such as adding, removing,
truncating partitions, and so on. We also saw index enhancements related to partition tables
and how we can create partial indexes on partition tables using examples and how that benefits
us in production. Finally, we saw enhancements related to SQL and how the row limiting
clause can be used to get the top N query data. We saw various parameters of the row limiting
clause and the significance of each of those parameters. At the end, we had a brief discussion
about extended data types in Oracle 12c and how we can enable them.
This completes section 1 of the book and in the next chapter we are going to start section 2.
Before we begin, we would like to mention the scope and expectations from section 2 of the
book. Section 2 covers the database administration in general and does not include any Oracle
12c new features. Topics in section 2 are aligned as per the topics in section 2 of 1Z0-060
OCP 12c upgrade exam topics. All the Oracle database features mentioned in that section are
limited to cover only the exam related scope and does not explain the feature end to end. We
tried to provide completeness and examples for most of the feature in that section, which will
help the readers to understand the topics. We hope that section 2 will definitely add values
while preparing for the OCP upgrade exam as well as in day to day work.

Chapter 10. Core Administration
Welcome to section 2 of the book. This section deals with the general administration and
features that a DBA uses daily in managing production databases. Section 1 was focused
mostly on the new features of Oracle 12c, whereas section 2 is more general. We are going to
cover all the exam topics that are listed in section 2 of the 1Z0-060 exam. In this chapter, we
are going to cover the core administration topics, which center around installing and
configuring the database, monitoring the database for issues, performing backup and
recovery, detecting data failures and using advisor to recover from such issues, and using
flashback technology. You must have already worked on these topics, but a reminder of these
topics will definitely help you in section 2 of the exam.
In this chapter, we are going to cover the following topics:
Explain the fundamentals of DB architecture
Install and configure a database
Configure the server and client network for a database
Monitor database alerts
Perform daily administration tasks
Apply and review patches
Backup and recover the database
Troubleshoot network and database issues
Detect and repair data failures with data recovery advisor
Implement flashback technology
Load and unload data
Miscellaneous

Fundamentals of DB architecture
Oracle database architecture includes physical components, memory components, processes,
and logical structures.
Let's look into each of the components in detail, and try to understand the architecture. An
Oracle database consists of an Oracle instance and Oracle database files.

Oracle database files
An Oracle database contains the following different types of files:
Datafiles, which basically store the application data as well as the Oracle database
metadata.
The redo log files are used to recover the database in the event of application program
failures, instance failures, and other minor failures. These files contain the sequentially
written change vectors that can be used to regenerate the required data. These files are
limited in number, and so, once all files are filled, the first file will get reused.
The archived redo log files are copies of the online redo log files before they get
overwritten.
The parameter file, which is a required file for starting the database. This file contains
different parameters that we want to set for the database. There are certain mandatory
parameters that need to be set and certain optional parameters, which, if not set, assume
default values.
The password file, which is an optional file and is used for remote authentication of
privileged users, including database administrators.
The alert and trace log file, which contains errors and debugging information, including
the timestamp, which helps DBAs and the Oracle support team to understand and debug
the issues in the database.

Oracle instances
An Oracle instance is made up of two components:
Background processes, such as SMON, PMON, DBW0, LGWR, and so on. Each of these
background processes is a complex computer program that performs its own task. For
example, one of the tasks performed by the SMON background process is recovery in
case of crash. These background processes are the core engine of the Oracle database,
and they provide performance, durability, and availability for the Oracle database.
The user process and server process. When a user connects to the database, a user
process is spawned on the client side and is connects to the server process on the Oracle
database server. The user request is handed over to the server process for processing. In
the case of dedicated process architecture, a new server process is spawned on the Oracle
database server for each user process. It manages memory and variables for single user
processes, whereas, in the case of shared server process architecture, we have a pool of
server processes on the Oracle database server, and these processes serve the requests
for many user processes.
The second component set includes the memory structures that comprise the Oracle
instance:
When an instance starts up, a memory structure called the System Global Area
(SGA) is allocated.
At this point the background processes also start.
An Oracle instance provides access to one and only one Oracle database. Things are
little different in container databases, as we saw in Chapter 3, Managing CDBs and
PDBs.

Memory architecture of the Oracle database
The memory structures include three areas of memory:
System Global Area (SGA): This is allocated when an Oracle instance starts up
Program Global Area (PGA): This is allocated when a server process starts up
User Global Area (UGA): This is allocated when a user connects to create a session

System Global Area
The System Global Area covers the majority of the memory area allocated to the database. It's
a read/write memory space that is shared by all background processes of the Oracle database.
Whenever a user connects to the database and asks for data, the data is fetched from the disk
into the SGA memory area, specifically in the buffer cache of the SGA. We can control the
size of the SGA by using a parameter SGA_MAX_SIZE in the parameter file. The SGA memory
is allocated when we start the instance (no mount mode). The size of SGA cannot be changed
dynamically, and it needs a database bounce.
Let's look into the memory structure of the SGA. I will provide a very brief explanation of
each of these structures for the sake of completeness. The SGA has the following mandatory
memory structures:
Database buffer cache: The database buffer cache is the major part of the SGA, and is
mainly used to store the data blocks that are retrieved by queries. This memory structure
optimizes the I/O required to obtain the necessary data requested by user queries. Every
time a user executes an SQL, Oracle does not have to read the data from the disk. Instead,
Oracle users buffer the recently read data from cache to cache. The data is cached based
on least recently used algorithm. Also, all the changes to data blocks happen inside the
buffer cache. Data is stored in the buffer cache in the form of the database block, which is
defined by the DB_BLOCK_SIZE parameter. The allowable values are 2 KB, 4 KB, 8 KB, 16
KB, and 32 KB.
Redo log buffer: The redo log buffer contains all the changes made to the database
blocks. This memory structure contains only the small information called change vector
which specifies what change has been made. This information is enough to use for any
changes (if required during recovery). A background process LGWR writes the change
vector from the redo log buffer to online redo logs based on the following conditions:
Whenever the redo log buffer is one-third full
Whenever 1 MB of data change happens
Every 3 seconds
Whenever a checkpoint happens
Redo log buffer is a circular buffer that is reused over and over.
Java pool: The Java pool is an optional memory object, but is required if the database
has Oracle Java installed and in use for Oracle Java Virtual Machine (JVM). The Java
pool is used for memory allocation to parse Java commands and to store data associated
with Java commands. We can set the size of Java pool using JAVA_POOL_SIZE parameter.
Streams pool: This pool stores data and control structures to support the Oracle Streams
feature of the Oracle Enterprise Edition. We can set the size of the stream pool using the
STREAMS_POOL_SIZE parameter. If STREAMS_POOL_SIZE is not set or is zero, the size of the
pool grows dynamically.
Shared pool: This includes two components:
Library cache
Data dictionary cache

Apart from these distinct memory pools, Oracle also has many more internal
structures, such as lock and latch management, statistical data, and so on.
Additional optional memory structures in the SGA include:
Large pool: The large pool is an optional memory component. When set, the large pool
is used for the following tasks:
Allocating space for session memory requirements from the User Global Area
where a shared server is in use.
Transactions that interact with more than one database, for example, a distributed
database scenario.
Backup and restoration operations by the Recovery Manager (RMAN) process. If
the large pool is too small, the memory allocation for backup will fail and the
memory will be allocated from the shared pool.
Parallel execution message buffers for parallel server operations. The
PARALLEL_AUTOMATIC_TUNING = TRUE parameter must be set.
It also provides cursor area for storing runtime information about cursors in case
of a shared server configuration.
The large pool size is set with the LARGE_POOL_SIZE parameter.
Keep pool and recycle pool: The keep pool contains the frequently used blocks
those have to be retained for a longer time than the buffer cache. An example can be
a small lookup table that is accessed frequently. If such a table is placed in the keep
pool, it does not get flushed out of the SGA, even with space pressure. The recycle
pool is mainly used for blocks that should not stay in the SGA for a longer time.
These data blocks are quickly removed from the memory. An example could be a
big query that is going to run only one time for generating a report.
The show sga SQL command will show you the SGA memory allocations:
SQL> show sga
Total System Global Area 805306368 bytes
Fixed Size 2929552 bytes
Variable Size 260050032 bytes
Database Buffers 536870912 bytes
Redo Buffers 5455872 bytes
Program Global Area
A Program Global Area (PGA) is:
A non-shared memory area that is private to the Oracle process. It contains the
information specific to the Oracle process, such as data and control information required
by the process.
The PGA memory area is a private area that is used by processes (both the user process
and the background processes of Oracle).
The PGA stores data and control information for a process and this is not shared by any
other process.
Memory is allocated from the PGA whenever a process is created and memory is

returned back to the OS when the process terminates.
The total PGA size is a sum of the PGA memory allocated to every process.
The database initialization PGA_AGGREGATE_TARGET parameter sets the size of the instance
PGA, not individual process PGAs.
The Program Global Area is also termed the Process Global Area (PGA) and is a part of the
memory allocated outside of the Oracle instance.
The content of the PGA varies, but, as shown in the preceding figure, generally includes the
following:
The private SQL area stores the information about parsed SQL statements. It mainly
stores the bind values used by bind variables in the SQL and runtime memory
allocations. If multiple users execute the same SQL, a private SQL area may get
associated with the shared SQL area. This is usually the case in an OLTP system where
multiple users are executing the same SQL. The location of the private SQL area depends
on the kind of architecture we are using:
In the case of a dedicated server architecture, the private SQL area is located in the PGA
In the case of a shared server architecture, the private SQL area is located in the shared
pool in SGA
Session memory is another memory area inside the PGA which holds session variables
and other session information.
The SQL work area is a memory area in the PGA that is used for sorting, doing hash-
joins, bitmap merges and bitmap creation of types of operations. Since the work area
size is limited, if the amount of data to be processed is bigger than the work area size,

Oracle divides the total work into chunks and process one chunk at a time. Remaining of
the data is stored in temp files.
From Oracle 9i onwards, the work area can be managed automatically by setting the
WORKAREA_SIZE_POLICY = AUTO parameter. Oracle will automatically allocate memory
for sorting, joins, and so on. We can also set PGA_AGGREGATE_TARGET = n (where n is an
amount of memory in MG or GB) for automatic management of PGA memory.

User Global Area
The User Global Area (UGA) is session memory, and contains user session information.
For example, a session executes a PLSQL procedure and this procedure gets loaded into the
memory. All the variables used in this procedure will have their states stored in the UGA. It's
called the state of procedure. As the procedure executes further, these variable values can
change and so does the state of procedure. By default, these variables are private and unique,
and persist for the duration of the session.
The UGA also holds the OLAP page pool, which stores the OLAP data pages. An OLAP's data
pages are similar to data blocks, and hold data. Whenever a user queries a dimensional object,
such as CUBE, an OLAP session opens and uses a memory in the UGA to store the OLAP data
pages.
Note
Oracle OLAP is a multidimensional analytic engine embedded in Oracle Database 11g Oracle
OLAP cubes deliver sophisticated calculations using simple SQL queries, producing results
with speed-of-thought response times.
The UGA is very specific to the session and should be available to the session until the session
is alive. For this reason, the UGA cannot be stored in the PGA when using a shared server
connection. This is because, in the case of a shared server connection, the PGA will be
specific to a small set of server processes that are serving many database sessions. Therefore,
whenever we use a shared server architecture, the UGA is part of the shared pool and is
shared by the multiple shared server processes accessing it. But in the case of a dedicated
server architecture, the UGA is part of the PGA and is not shared among the server processes.

Oracle database process architecture
You need to understand three different types of processes:
Client process: Starts when a database user requests a connection to an Oracle Server.
This process is created on the client computer.
Server process: Establishes the connection to an Oracle Instance when a client process
requests a connection. This process is created on the server side.
Background process: These are the processes that are started when an Oracle Instance is
started up.

Client process
The client process is the one which is initiated by the end user. This could be a simple
SQL*Plus connection or a web page accessing the database or Oracle forms or any other
application program. The client process runs on the client side, and is spawned by one of the
aforementioned client programs.

Server process
A server process is spawned on the Oracle server, and it takes the client request to the Oracle
instance. We can have either a dedicated server process, where we have 1-1 mapping between
the client process and the server process, or we can have a shared server process, where we
use a small pool of shared server processes which serve a high number of client processes.
Dedicated server environment: There is a single server process to serve each client
process
Shared server environment: A server process can serve several client processes,
although with some performance reduction

Background processes
We have many mandatory and optional background processes started when we start the
database. We will look into some of the important background processes.
The following are mandatory background processes:
Process Monitor Process (PMON): The PMON background process is one of the most
important background processes of the database. If you kill the PMON process, the
database will crash. This process monitors other background processes. It also cleans up
the memory for failed processes that have occurred because of abnormal termination.
System Monitor Process (SMON): The SMON is mainly for recovering the database in
the background. When the instance crashes and restarts, SMON will perform an instance
recovery by applying archives and online redo logs. It also coalesces free space and
deallocates temporary segments.
Database Writer Process (DBWn): The Database Writer process writes modified
blocks from the database buffer cache to the datafiles. We can configure multiple DBWn
processes (up to 20) using the DB_WRITER_PROCESSES parameter if the load on the
database is high and the number of blocks to be written is high.
Note
The purpose of DBWn is to improve system performance by doing infrequent writes to
datafiles. Oracle tries to perform most of the reads and writes into the memory, and
periodically performs database file writes using the DBWn process. Whenever data is
modified, changes happen in the database buffer cache. The changed blocks are called
dirty blocks. The DBWn process keeps a list of dirty blocks as these blocks have to be
written periodically to disk in datafiles. The DBWn process comes into action during the
checkpoint process or during the log switch or whenever there is a space pressure in the
buffer cache.
Log Writer Process (LGWR): The Log Writer (LGWR) writes contents from the
Redo Log Buffer to the Redo Log File that is in use. Whenever a change is made to the
block, a small change vector gets written to the redo log buffer. This change vector is
later copied to online redo logs. Whenever a transaction commits, LGWR has to write
the leftover content of that transaction to the online redo log file and also has to write the
commit marker. Only then is the transaction considered committed. The LGWR writes
according to the following events:
At commit
When the log buffer is one-third full
When 1 MB of redo is generated
Every 3 seconds
When DBWR writes
Checkpoint Process (CKPT): This background process comes into action during the
checkpoint and is responsible for updating the control file and data file headers with
checkpoint information (the SCN and location in online redo log file). It also signals the

DBWn process to write dirty blocks to the disk.
Manageability Monitor Processes (MMON and MMNL): These background processes
perform all the tasks related to the automatic workload repository (AWR). Various
performance metrics are periodically written by these background processes to the
automatic workload repository in the form of snapshots.
Recover Process (RECO): This is mainly used in distributed environments for
resolving the failures in distributed transactions.
The following are optional background processes:
Archiver Process (ARCn)
Coordinator Job Queue (CJQ0)
Dispatcher (number "nnn") (Dnnn)

Shared server architecture and dedicated server
architecture
The Oracle database creates server processes to serve the client processes with the requests.
The Oracle architecture supports two types of server process:
A dedicated server process, which services only one user process
A shared server process, which can service multiple user processes

Dedicated server processes
In dedicated server processes, each client connection spawns a new process on the server side.
So the number of server processes spawned in the database server will be equal to the number
of client processes.
In the following diagram, two user processes are connected to the database through dedicated
server processes.
A user can request a dedicated connection even when the process architecture is configured
for a shared server. This is possible by using the SERVER=DEDICATED clause in the connect
descriptor of the TNS alias. So if we just create a TNS alias, a user named SERVER=DEDICATED
, and use that TNS alias to connect to a shared server database, we can still get a dedicated
connection:

Shared server processes
A shared server process contains a small number of pools of server processes running in the
database. Oracle does not spawn a new server process each time a new client connection
comes to the database. Instead, one of the existing shared server processes is used for
processing the request from the client.
The shared server architecture consists of a listener process, a dispatcher process, a request
queue, the shared server process, and a response queue.
The listener process listens on the configured port for client connections. Whenever a client
tries to connect, the listener will forward that request to the dispatcher, which then places the
connection request on the request queue. The request queue is part of the SGA memory and
is shared by all the configured dispatchers. The request queue is polled by the shared server
process, and one of the free shared server processes picks the request from the request queue
and starts the connection. After processing the request, the response is placed in the response
queue by the shared server process. The dispatcher owns the response queue as well, and it's
part of the SGA. The dispatcher returns the response to the user process.

Installing and configuring a database
Installation of Oracle database software is very straightforward and does not need much help
so I am not going to go over the screenshot for installing the Oracle database. But let's discuss
some key points regarding the installation and configuration of the database.
Starting from Oracle 11g R2, we need to first install Oracle Grid infrastructure, followed by
the Oracle database installation (although it isn't mandatory, it is recommended). Oracle Grid
Infrastructure for a standalone server is the Oracle software that provides system support for
an Oracle database, including volume management, a file system, and automatic restart
capabilities. If you plan to use Oracle Restart or Oracle Automatic Storage Management
(Oracle ASM), then you must install Oracle Grid Infrastructure before you install and create
the database. Oracle grid infrastructure includes Oracle restart and Oracle ASM. These two
infrastructure products are bundled into a single Oracle software package, called Oracle Grid
Infrastructure Home. Oracle Restart improves the availability compared to a single instance
database by providing the following:
In case the database crashes, Oracle Restart automatically restarts the database. The same
is applicable to other database components, such as listener, ASM, and other database
services.
Oracle Restart knows the startup sequence so that it starts the components in the correct
sequence.
Oracle Restart continuously monitors the components, taking immediate action and
sending notifications should anything go wrong.
During the installation of the Oracle software and database, you will be asked many questions
by the installer. The following are some of the critical inputs you need to provide (which can
cause errors or majorly alter the installation process):
Install software only or create and configure a database: You may wish to just install
the software or even create a database during the installation. In Oracle 12c, if you are
creating the database, it will provide an option to create a CDB or non-CDB database.
For CDB database creation, it allows you to also create PDBs.
ORACLE_BASE and software location: You need to provide a location for
ORACLE_BASE and the location where the software will be installed. You have to make
sure that the Oracle user has permission to write to these locations.
If you are installing Oracle software for the first time, you also need to set up the location for
the Oracle inventory. You should make sure that the OS group you specified for installation
has the permission to write to the specified inventory location.

Configuring the database
Configuring the database involves the following:
Setting memory parameters (and/or enabling ASMM)
Data file locations (or enabling OMF)
Enabling archive logs
Configuring listeners
Configuring service names and the TNS alias
We already discussed the setting of memory parameters, so we will move on to datafiles and
OMF.

Data file location or OMF
While creating datafiles, we can create the data file by specifying the complete path and name
of the datafiles. This provides us flexibility in terms of organizing the datafiles and their
name pattern. However, we can reduce the administrative overhead of providing the complete
name and path of the data file by letting Oracle do that. This auto-management of the data file
name and path is called Oracle Managed Files (OMF).
Using OMF simplifies the administration of an Oracle Database. OMF eliminates the need for
you, the DBA, to directly manage the operating system files that comprise an Oracle database.
With OMF, you need to specify just the location where the datafiles needs to be created and
Oracle will auto-generate the names of datafiles and create them at the specified location. This
makes it very easy for DBA during tablespace creation or while adding datafiles to the
tablespace. But this makes the names of the datafiles very unfriendly.
The following are some of the benefits of OMF:
OMF makes the administration of the database easier as datafiles are managed
automatically.
It saves disk space as it automatically cleans up obsolete files.
We don't need to know the underlying file system mount point every time we create a
datafile. We can just set the appropriate parameters once.
It makes the creation of the test and dev environment fast.
You can enable OMF using one of the following parameters:
DB_CREATE_FILE_DEST: This init.ora parameter specifies the location where datafiles
will be created when using OMF. In other words, setting this parameter enables OMF.
You can specify the file system directory or ASM disk group to this parameter and OMF
will automatically create the datafiles in this location. This parameter is also used as the
default location for online redo logs when parameter DB_CREATE_ONLINE_LOG_DEST_n is
not specified.
DB_CREATE_ONLINE_LOG_DEST_n: This parameter determines the location of online redo
logs and the control file. By replacing n at the end of this parameter with a number, you
can specify multiple locations for storing online redo logs and control files. For
example, we can define the DB_CREATE_ONLINE_LOG_DEST_1 parameter as our location.
Similarly, DB_CREATE_ONLINE_LOG_DEST_2 will be our second location. This is helpful in
multiplexing. The location can be a file system directory or an ASM disk group.
DB_RECOVERY_FILE_DEST: This defines the location where all recovery-related files are
kept. This should contain all your archive logs, flashback logs, one copy of the database
backup, and so on. We can configure this either as a file system directory on UNIX or as
a location in the ASM disk group. This can also contain one multiplex copy of the online
redo log files. You have to make sure that the directory specified by this parameter
already exists. Oracle does not create the directory.

Enabling archiving
Once you create the database, it's important to enable flashback and archive. When you enable
archives, it creates a copy of the online redo log files when they are full and before they get
overwritten. These files are copied and saved as the archive log file. We have various
parameters that we can configure to enable archive logging. Having archive logs helps us
perform the complete recovery of the database up to the latest time and ensures no data loss.
The following is the code for enabling archive logging:
SQL> archive log list 
Database log mode              No Archive Mode 
Automatic archival             Disabled 
Archive destination            USE_DB_RECOVERY_FILE_DEST 
Oldest online log sequence     69 
Current log sequence           70 
As you can see, we are in no archive log mode. To enable archiving, follow these two steps:
Shutdown the database
Mount the database and enable archiving
The following command shows how to mount the database using the startup mount
command:
SQL> startup mount 
ORACLE instance started. 
Total System Global Area 1073741824 bytes 
Fixed Size                  2932632 bytes 
Variable Size             587202664 bytes 
Database Buffers          478150656 bytes 
Redo Buffers                5455872 bytes 
Database mounted. 
SQL> alter database archivelog; 
Database altered. 
You can set the following parameters for configuring archive logs. This is required if you
don't want to setup archive log location to recovery file destination.
log_archive_dest_1='location=/u01/app/oracle/oradata/deo/deo/archivelogs'; 
log_archive_format=' arch-%t_%s_%r.dbf' 
Once you set these parameters, start the instance. If you want the archive logs to be configured
in the recovery file destination, then you can set LOG_ARCHIVE_DEST_1 to the
USE_DB_RECOVERY_FILE_DEST value. This will automatically start creating archives in the
DB_RECOVERY_FILE_DEST location. Just setting the recovery file destination will not enable
archives to be created in that location. Archives will be created in the location set by the
LOG_ARCIHVE_DEST_n parameter:

SQL> archive log list; 
Database log mode              Archive Mode 
Automatic archival             Enabled 
Archive destination            /u01/app/oracle/oradata/deo/deo/archivelogs 
Oldest online log sequence     69 
Next log sequence to archive   70 
Current log sequence           70 

Configuring the listener and service names
Oracle Net Listener is a separate component of the Oracle database, and is required for
accepting remote connections to the database. A listener is configured with one or more
listening ports, a protocol, information about supported services, and a few parameters that
control the runtime behavior. Listener configuration is stored in a separate file called
listener.ora under the ORACLE_HOME/network/admin folder. Almost all the configuration
parameters have default values, including the port number and protocol, so we can just start
the listener off using the lsnrctl start command without creating any configuration file.
The listener that is started with all default values is called the default listener, and it listens on
port 1521 and on the TCP/IP protocol.
A client connects to the database using the service name. These services names are configured
and registered to the listener, and that's how the listener, which is running on specific port, can
pass on the connection to the appropriate service when it gets the client request.
A typical TNS alias looks like the following:
Attempting to contact (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = 
deo.example.com)(PORT = 1521)) (CONNECT_DATA = (SERVER = DEDICATED) 
(SERVICE_NAME = deo.example.com))) 
In the preceding TNS alias, we see at the end, SERVICE_NAME = deo.example.com. This is the
same as the DB name, but we can have different service names configured for different types
of application requirements. By default, a service name which is the same as the database
name is always created by Oracle. This service is also automatically registered with the
listener. If you describe the listener, it will show that the service is registered and has one
handler, as shown in the following code:
lsnrctl status 
LSNRCTL for Linux: Version 11.2.0.4.0 - Production on 17-NOV-2016 10:52:42 
Copyright (c) 1991, 2013, Oracle.  All rights reserved. 
Connecting to (DESCRIPTION=(address=(protocol=ipc)(key=listener))) 
STATUS of the LISTENER 
------------------------ 
Alias                     LISTENER 
Version                   TNSLSNR for Linux: Version 11.2.0.4.0 - Production 
Start Date                10-SEP-2016 00:17:28 
Uptime                    68 days 11 hr. 35 min. 14 sec 
Trace Level               off 
Security                  ON: Local OS Authentication 
SNMP                      OFF 
Listener Parameter File   /oracle/network/admin/listener.ora 
Listener Log File         /oracle/network/log/alert/log.xml 
Listening Endpoints Summary... 
 (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=listener))) 
 (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=desktop.example.com)(PORT=1521))) 
Services Summary... 

Service "orcl.example" has 1 instance(s). 
 Instance "orcl", status READY, has 1 handler(s) for this service... 
The command completed successfully 
There are two ways if registering services with listener
Static registration
Dynamic registration
Static registration
The service's name can be registered with the listener using static registration. This is done by
including a SID_LIST section in the listener.ora configuration file.
For example, a listener.ora file looks like the following:
l_deo_001 = 
   (description = 
       (address_list = 
           (address=(protocol=tcp)(host=deo-server.example.com)(port=1529)))) 
sid_list_l_deo_001 = 
   (sid_list = 
       (sid_desc = 
           (sid_name = deo) 
           (oracle_home = /u01/app/oracle/product/12.1.0.2/dbhome_1)) 
   (sid_desc = 
           (GLOBAL_DBNAME = deo.example.com) 
           (sid_name = deo) 
           (oracle_home = /u01/app/oracle/product/12.1.0.2/dbhome_1)) 
   ) 
The highlighted part in the preceding code shows a SID_DESC with GLOBAL_DBNAME =
deo.example.com. This performs the static registration of the service with the listener.
Remember that with static registration you need to have the instance name in the SID_LIST
section in listener.ora as specified by (sid_name = deo). If you have some service_name
configured in the TNS alias, you need to make sure that, in the case of static registration,
those service_names are part of the listener.ora file as specified by GLOBAL_DBNAME =
deo.example.com.
Now, if I remove the SID_LIST section from listener.ora, the listener status will show this
service as UNKNOWN, and if the client tries to connect to that service, it will hit the following
error:
ORA-12514: TNS:listener does not currently know of service requested in connect 
Descriptor 
Dynamic service registration
In the case of dynamic registration of services with the listener, we don't need the SID_LIST

section in the listener.ora file as we saw above with static registration. So how does the
instance get registered with the listener?
To answer that, I have to explain the concept of the local_listener parameter.
If you are using the default port (1521) for the listener, then the database will automatically
register the instance with the listener. The supported service, that is, the service to which the
listener forwards the client requests, can be dynamically registered with the listener. This
dynamic registration is performed by the Oracle background process PMON, which has the
necessary configuration details.
For dynamic registration to work, PMON needs to be aware of the services that need to be
registered and the listener details with which the services will be registered. These two pieces
of information are obtained by two parameters:
service_names: This parameter provides a comma-separated list of parameters that
should be registered with the listener
local_listener: This parameter provides details about the listener configuration, its
protocol, endpoint, and port number.
These two pieces of information are required for dynamic registration only if the default port
is not being used by the listener. If we are using the default port, namely 1521, PMON
automatically registers the service names running on port 1521 with the listener. Alternatively,
you can manually trigger dynamic registration by executing the ALTER SYSTEM REGISTER
command. Services are also dynamically registered when we bounce the database with the
listener running.

Monitoring database errors and alerts
Typically, monitoring database errors and alerts can be done in the following two ways:
Monitoring errors with trace files and alert log files
Monitoring database operations with server generated alerts

Monitoring errors with trace files and alert log files
Oracle writes all the information to log files and trace files. Whenever an internal error
occurs, Oracle dumps the information about the error and related trace data to trace the file.
Oracle also creates an incident file for that error. These trace files and incident files are very
useful for Oracle support in debugging and providing the fix for the issue. Many of these
trace files are also useful for DBAs in getting additional information about the error. DBAs
can use the information from the trace files to tune applications and instances.
The alert log file is the most important log file for databases. It contains all the messages and
errors, along with the timestamp of when those events or errors happened in the database.
This helps DBAs out in debugging the issue. The following information is logged in the alert
log file:
All internal errors (ORA-600), block corruption errors (ORA-1578), and deadlock
errors (ORA-60) that occur
DDLs such as CREATE, ALTER, and DROP statements
STARTUP and SHUTDOWN of database is logged in the alert log
Messages and errors relating to the functions of the shared server and dispatcher
processes
Any errors because of an ORA-01555 - snapshot too old error
Several other errors are also logged in alert log file
A list of all the parameters that are set in init.ora or spfile at the time of the instance
startup
Instead of logging all this information on the console screen, Oracle logs everything in the
alert log so that we have it saved and can use it for debugging. We can also control the size of
the trace files that get generated for some critical errors by setting the parameter
MAX_DUMP_FILE_SIZE.

Monitoring database operations with server generated alerts
We can configure the server alert for our database. These alerts are triggered automatically
whenever a threshold is crossed. The alert contains the information about which value was
breached and possibly the way to fix the issue. Server generated alerts could be based on the
threshold, or could be due to an event in the database. Threshold-based alerts can be triggered
at both threshold warning and critical levels. Threshold values for warning and critical levels
can be adjusted by DBAs as per the need.
For example, we can create server alerts for the following statistics:
Physical reads per second
User commits per second
SQL service response time
We have many more statistics on which we can generate the alert.
The following example shows how to set thresholds with the SET_THRESHOLD procedure for
CPU time for each user call for an instance:
DBMS_SERVER_ALERT.SET_THRESHOLD( 
DBMS_SERVER_ALERT.CPU_TIME_PER_CALL,  
DBMS_SERVER_ALERT.OPERATOR_GE, '8000',  
DBMS_SERVER_ALERT.OPERATOR_GE, '10000', 1, 2, 'deo', 
DBMS_SERVER_ALERT.OBJECT_TYPE_SERVICE, 'deo.example.com'); 
In this example, a warning alert is issued when the CPU time exceeds 8000 microseconds for
each user call, and a critical alert is issued when the CPU time exceeds 10,000 microseconds
for each user call.
The following are the arguments for the SET_THRESHOLD procedure:
CPU_TIME_PER_CALL: This is the name of the statistics on which we are creating an alert.
DBMS_SERVER_ALERT.OPERATOR_GE, '8000': This defines a warning threshold. If the
CPU time for a call goes above 8000, it will issue a warning alert.
DBMS_SERVER_ALERT.OPERATOR_GE, '10000': This defines the critical threshold. If the
CPU time for a call goes above 10000, it will issue a critical alert.
1, 2: This states that the observation period is 1 minute and the occurrence is set to 2.
So, if the CPU time for a call is high for 1 minute and if it happens two times
consecutively, an alert will be issued.
DBMS_ALERT.OBJECT_TYPE_SERVICE: This tell us that the object type in this case is a
service and the service name is deo.example.com.

Perform daily administration tasks
The following are some of the typical day-to-day activities performed by the DBA:
Moving tables online with the least downtime
Recompiling invalid objects
Using various advisors to tune various components of the database
Segment advisors
SQL tuning advisor
Managing long idle sessions by creating appropriate user profiles

Moving tables online with the least downtime
One of the challenges that DBA faces every day is to make modifications to the table structure.
Because of agile business development, it has become essential to deploy new changes
quickly without impacting any of the existing business. As the application changes, the
database structure will also undergo changes. But because many of the tables in the database
are hot, and having downtime on such a table is a challenge, Oracle provides another way to
make changes to the structure of such busy tables. This mechanism is called online
redefinition. Redefining tables online provides a substantial increase in availability compared
to traditional methods of redefining tables.
During the redefinition process, Oracle creates a new table with a changed structure-for
example, it could be partitioning a non-partitioned table. Using Oracle for online redefinition,
the initial load of data is performed on the new table, and data from the existing table is
synced periodically to the new table. Once the data in the new table is in sync with the existing
table, we need to take a short downtime of a few minutes and complete the redefinition
process. This downtime is required to complete the redefinition process, where it takes the
final, exclusive look on the table and completely syncs the data, renaming the new table as the
original and giving the original table a new name. This technique is best used for moving
tables and indexes online while maintaining the highest level of availability.
Online redefinition requires the same amount of space used by the object that we are
redefining. Again, space usage can be less if the existing object is highly bloated or more if
we are adding extra columns to the new object.
You can perform online redefinition using either the enterprise manager console or by using
the DBMS_REDEFINITON package.
The following are the brief steps that need to be followed if you are doing online redefinition
using the DBMS_REDEFINITION package
You can choose the redefinition method as either by key or by ROWID. The differences between
the methods are outlined as follows:
By key: In this method, a row is identified by using a primary key. For redefinition, to
apply the changes happening in the original table to the new table, we should be able to
identify a row using the primary key. We can also use pseudo-primary keys, which
represent a row uniquely. All the columns in pseudo-primary keys should be not null.
This is the most simple and preferred method of doing table redefinition.
By row ID: This method is usually used when we don't have primary keys defined on the
table. We can identify every row uniquely using a row ID. When we use this method, the
table redefinition process creates a hidden column M_ROW$$ to the post-redefined version
of the table. This table is automatically dropped later when we complete the redefinition
process. You cannot use this method on index-organized tables.

The following are the steps of the redefinition process:
1. Check if the table can be redefined using the CAN_REDEF_TABLE procedure. In certain
cases, a table cannot be redefined if it has BFILE, LONG or LONG RAW columns, or tables
belonging to SYS or SYSTEM, and so on.
2. Create an empty interim table in an appropriate schema with the required change
structure. For example, if you want to partition an original non-partition table, you
should create the required partitions.
3. You can also create all the indexes, constraints, grants, triggers, and so on, but it's not
required, and, in fact, it's better to create these at the end for performance reason.
Alternatively, you can also copy all these dependent objects from the original table as
part of the redefinition process at a later point.
4. Start the redefinition process by calling the START_REDEF_TABLE.
5. You can copy the dependent objects, such as indexes, constraints, grants, and so on, from
the original table to the interim table using one of the following two methods:
Automatic copy: You can use the COPY_TABLE_DEPENDENTS procedure to
automatically copy the dependent objects. This procedure also registers the new
dependent object names and enables these object names to be swapped when we
complete the redefinition process. That way we will have same names of the
dependent objects in the new table as we had in the original table.
Manual creation: You can also manually create all the dependent objects in the
interim table at any time.
6. Once the initial load is done and you are able to keep the data in sync between the
original table and the interim table, we can do a final copy of the leftover data and
complete the redefinition process by using the FINISH_REDEF_TABLE procedure. During
this procedure, the original table is locked in exclusive mode for a very short time, and
segment names are swapped at metadata level. The interim object takes the name of the
original object and we can see the new structure for our original table.
Note that this final step takes the exclusive lock for a very short time and should be performed
when you don't have any ongoing DMLs on the original table. If there are ongoing DMLs on
original table, the redefinition process will wait for all those DMLs to complete before taking
the lock.

Recompiling invalid objects
Many of the database activities, such as patching upgrade and DDL changes, can invalidate the
objects in the database. Accessing an invalid object will make the object revalidated
automatically on-demand using automatic recompilation, provided the object does not trigger
compilation failure. But this can take unacceptable amount of time, and we have to wait until
all the invalid objects get accessed. For this reason, it makes sense to recompile invalid
objects in advance of user calls.
You can identify the invalid object using DBA_INVALID_OBJECTS:
SQL> select owner, object_name, object_type from DBA_INVALID_OBJECTS; 
OWNER                OBJECT_NAME                    OBJECT_TYPE 
-------------------- ------------------------------ ----------------------- 
OE                   OC_CUSTOMERS                   VIEW 
OE                   OC_CORPORATE_CUSTOMERS         VIEW 
OE                   OC_ORDERS                      VIEW 
OE                   ORDERS_TRG                     TRIGGER 
OE                   ORDERS_ITEMS_TRG               TRIGGER 
There are multiple ways to recompile invalid objects:
Using a manual approach: You can manually compile packages, procedures and views
using the ALTER statement. This takes time as each object has to be compiled manually.
For example
    SQL> alter trigger OE.ORDERS_TRG compile; 
 
    Trigger altered. 
To speed up the process, you can write a dynamic SQL or custom program to create ALTER
statements and run them automatically.
DBMS_UTILITY.compile_schema: The COMPILE_SCHEMA procedure in the DBMS_UTILITY
package compiles all procedures, functions, packages, and triggers in the specified
schema. For example:
    EXEC DBMS_UTILITY.compile_schema(schema => 'OE'); 
UTL_RECOMP: The UTL_RECOMP package contains two procedures used to recompile invalid
objects. The RECOMP_SERIAL procedure recompiles all the invalid objects one at a time,
while the RECOMP_PARALLEL procedure performs the same task in parallel using the
specified number of threads.
There are a number of restrictions associated with the use of this package:
Parallel execution is performed by job queue processes. All existing jobs are marked as
disabled until this operation is complete.
You should run this package with a privileged user only (SYSDBA privs).

This package internally uses STANDARD, DBMS_STANDARD, DBMS_JOB, and DBMS_RANDOM, so
these packages should be valid.
Do not run any DDL operations while this package is running as it will cause deadlocks.
utlrp.sql and utlprp.sql scripts are provided by Oracle to recompile all invalid
objects in the database. If you are doing any major upgrades that cause most of the
objects to be invalid, you can always use these scripts to revalidate them. Scripts are
located in the $ORACLE_HOME/rdbms/admin directory, and provide a wrapper on the
UTL_RECOMP package:
0: The level of parallelism is based on the CPU_COUNT parameter.
1: The recompilation is run serially, one object at a time.
N: The recompilation is run in parallel with N number of threads.
This utility can do parallel recompilation of stored PL/SQL code objects, as well as Java code
objects.

Using various advisors to tune various components of the
database
We can use the following utilities for tuning the various components of the database:
Segment advisors
SQL tuning advisor
Segment advisor
The segment advisor is one of the most useful utilities in identifying the unused space. This
can help us reclaim the unused space, as well as understand how the space is being utilized in
the database. The segment advisor uses Automatic Workload Repository (AWR) data to
analyze the grown statistics of the segments. It's configured to run automatically during the
maintenance window, but the DBA can run this advisor manually as required.
The segment advisor generates the following types of advice:
If a segment has lot of free space available and is bloated, the segment advisor
recommends online segment shrink
If a segment (table) does not use automatic segment space management and has potential
free space, the segment advisor recommends online table redefinition
If a table has lot of row chaining beyond a certain threshold, the segment advisor records
those facts
If you have bloated segments that are using a high amount of space in the database, you can
use the segment space advisor to understand what action to take in order to reclaim the space.
The automatic segment space advisor does not provide advice on every object in the database.
The advisor makes the decision based on the statistics available and then doing some
sampling on the object. The following are usually the target for the segment advisor:
Tablespace that is running out of space, and for which warning or critical threshold
values have been reached
Hot tables on which lots of DMLs are happening
Segments that are growing very fast
You can run the segment advisor at the following three levels:
Segment level: Advice is generated for a specific segment, such as a table or partition or
sub-partition.
Object level: Advice is generated for an entire object, such as a table or index. This
includes all the partitions of the table.
Tablespace level: All the objects in the tablespace are considered and advice is generated
for every segment in the tablespace.
You can run the segment advisor from the enterprise manager console or using the

DBMS_ADVISOR package. You can use package procedures to create a segment advisor task, set
task arguments, and then execute the task. You must have the ADVISOR privilege.
SQL tuning advisor
As the trend of the Oracle database was moving towards the self-managing database setup, the
development of the SQL tuning advisor was not a big surprise. The SQL tuning advisor is one
of the most useful tools for improving the plans of SQL queries. This advisor takes one or
more SQL statements as an input and invokes the automatic tuning optimizer to perform SQL
tuning on the statements. It recommends feasible solutions to improve the performance of the
SQL along with the expected benefits. Its recommendation might be the collection of statistics
on objects, the creation of new indexes, the restructuring of the SQL statement, or the creation
of a SQL profile. You can choose whether to accept the recommendation to complete the
tuning of the SQL statements.
The SQL tuning advisor is scheduled to run during the maintenance window, and it selects the
top bad performance SQLs from AWR metadata. We can choose to automatically implement
the recommendation as well.
Once automatic SQL tuning begins, which by default runs for at most one hour during a
maintenance window, the following steps are performed:
1. Identify bad SQLs in the AWR for tuning: Oracle identifies bad SQLs in the database
as potential candidates for tuning. It generates a list of SQLs that have the most impact on
database performance, based on the CPU time and IO wait time encountered by SQL.
These performance statistics are read from the AWR data for each SQL. The SQL tuning
advisor only selects those SQLs that can obtain significant benefits when tuned.
Recursive SQLs and SQLs that have been recently tuned are not selected again by the
tuning advisor. Also, DMLs, SQLs with concurrency issues, parallel queries, and so on
are not selected by the advisor. Selected SQL statements are analyzed based on
performance impact and the SQL which will benefit the most and have the most impact is
selected for tuning.
2. Tune each SQL individually: Oracle SQL tuning advisor goes into each SQL
individually and provides recommendations. Recommendations can include creating an
index, gathering stats, creating a profile, or other such advice. Profile creation can be
implemented automatically by the tuning advisor.
3. Test the SQL profiles by executing the SQL statement: If an SQL profile is
recommended, the advisor will test the SQL profile by executing the SQL statement with
and without the SQL profile. If performance seems good with the SQL profile, the
profile will be accepted automatically if ACCEPT_SQL_PROFILES argument is set to
true when we are running sql tuning advisor. Otherwise, it only recommends creating the
SQL profile, which we can create manually.
4. Optionally implement the SQL profiles, provided they meet the criteria of threefold
performance improvement

As mentioned above, if the tuning advisor recommends implementing a profile, we can accept
the recommendation and the advisor will create the required profile that should fix the plan. If
SQL plan management (from Oracle 11g onwards) is enabled, a new profile will also create
and accept a new baseline for the SQL.
Enabling and disabling automatic SQL tuning
To enable automatic SQL tuning, use the ENABLE procedure in the DBMS_AUTO_TASK_ADMIN
package:
BEGIN 
 DBMS_AUTO_TASK_ADMIN.ENABLE( 
   client_name => 'sql tuning advisor',  
   operation => NULL,  
   window_name => NULL); 
END; 
/ 
To disable automatic SQL tuning, use the DISABLE procedure in the DBMS_AUTO_TASK_ADMIN
package:
BEGIN 
 DBMS_AUTO_TASK_ADMIN.DISABLE( 
   client_name => 'sql tuning advisor',  
   operation => NULL,  
   window_name => NULL); 
END; 
/ 
Configuring automatic SQL tuning
We can configure some of the parameters of the SQL tuning advisor using the DBMS_SQLTUNE
package. To use the APIs, the user needs at least the ADVISOR privilege. Parameters for the
SQL tuning advisor can be set using the DBMS_SQLTUNE.SET_TUNING_TASK_PARAMETER
procedure.
The following parameters can be configured using the SET_TUNING_TASK_PARAMETER
procedure:
ACCEPT_SQL_PROFILE: This parameter, when set TRUE, will accept the recommended
SQL profile automatically. When set FALSE, we have to manually create the SQL profile.
MAX_SQL_PROFILES_PER_EXEC: Specifies the maximum number of SQL profiles that can
be accepted automatically in one run of the SQL tuning advisor. Since we don't want to
make too many changes at one time, we can set the upper limit.
MAX_AUTO_SQL_PROFILES: Specifies the limit of SQL profiles that are accepted in total.
EXECUTION_DAYS_TO_EXPIRE: Specifies the number of days until the SQL tuning advisor
task history can be saved. By default, the history for the advisor task is saved for 30 days.
For example:
BEGIN 

  DBMS_SQLTUNE.SET_TUNING_TASK_PARAMETER( 
    task_name => 'SYS_AUTO_SQL_TUNING_TASK', 
    parameter => 'ACCEPT_SQL_PROFILES', value => 'TRUE'); 
END; 
/ 

Managing long idle sessions by creating appropriate user
profiles
A profile is a named set of resource limits and password parameters that restrict database
usage and instance resources for a user. The Oracle database has a set of default profiles
already available. We can make use of the default profiles and assign them to the users, or we
can create a new profile as per our requirements, however, a user can be assigned only one
profile at a time. If we assign a second profile, it will override the first one.
Using profiles, we can enforce certain limitations to the users. These limitations include
limiting the IDLE_TIME a user session can have, the number of sessions a user can spawn, the
amount of CPU time used per call by the user, and others. Such limitations can be enabled
even when the database is up and running. This does not need a bounce.
Any authorized user with CREATE PROFILE system privileges can assign profiles to other
users. A profile can also be altered or dropped at any time. Changing or assigning the profile
to the user does not affect existing sessions of that user; it only affects the subsequent
sessions. Using profiles, you can enforce certain constraints on the user. These constraints are
the resource limits that are applicable to the user for example, the number of sessions a user
can spawn, the maximum elapsed idle time limit for a user's session, and so on.
To apply these constraints or resource limits to the user, the following are required:
Enable the resource limit using ALTER SYSTEM SET RESOURCE_LIMIT = TRUE or by
setting the RESOURCE_LIMIT parameter
Create a profile and define the various limits that you want to enforce
Assign the profile to the user
UNLIMITED resources
The Oracle database provides an option to specify the UNLIMITED value for a specific resource
in a profile. An UNLIMITED value indicates that user can use that resource for any amount and
the database will not take any action for the same amount. UNLIMITED indicates that no limit
has been set for the parameter.
DEFAULT profile
The Oracle database also enables you to provide a value called DEFAULT for any resources in
a profile. Oracle has a DEFAULT profile which has default values for all resources. If the
profile that is explicitly assigned to a user omits limits for some resources or specifies
DEFAULT for some limits, then the user is subject to the limits on those resources defined by
the DEFAULT profile. So, specifying a DEFAULT value for any resources in a user-generated
profile will use the value of that resource from the DEFAULT profile. We can also change the
default values in the DEFAULT profile using the ALTER PROFILE statement.

The following are important resources that we can limit for a user by creating a profile:
SESSIONS_PER_USER: This specifies the maximum number of sessions a user can spawn
concurrently.
CPU_PER_SESSION: Specifies the amount of CPU time (in centisecs) a session can
consume.
CPU_PER_CALL: Specifies amount of CPU time (in centisecs) a call can consume. A
session can have multiple calls (for example, parse, fetch, execute).
CONNECT_TIME: Specifies the total elapsed time (in minutes) a session can have.
IDLE_TIME: Specifies the total idle time (in minutes) a session can have.
LOGICAL_READS_PER_SESSION: Specifies the amount of data blocks that can be read by a
session in memory and from the disk.
LOGICAL_READS_PER_CALL: Specifies the amount of data blocks that can be read by a call
in a session.
The following are the restrictions that can be applied on the password of user:
FAILED_LOGIN_ATTEMPTS: Specifies the number of failed attempts that are allowed before
the account gets locked.
PASSWORD_LIFE_TIME: Specifies the number of days a password can be used. After this,
you need to change the password, otherwise the account will expire.
PASSWORD_REUSE_TIME and PASSWORD_REUSE_MAX: PASSWORD_REUSE_TIME defines the
number of days before which you cannot use the same password. So if you have set a
password today with PASSWORD_LIFE_TIME of 30 days and PASSWORD_REUSE_TIME of 90
days, you cannot use the same password until you set a minimum of three different
passwords each time they expire. Similarly, PASSWORD_REUSE_MAX specifies the number of
password changes required before the current password can be reused.
PASSWORD_LOCK_TIME: Specifies the number of days an account will get locked after
failed login attempts lock the password. The default is 1 day.
PASSWORD_GRACE_TIME: Specifies the number of days that the password expire warning
will be issued before the password expires. The default is 7 days.
PASSWORD_VERIFY_FUNCTION: You can make password rules more complex by specifying
the password verify function. You can create your own function or use the Oracle default
function.
For example:
CREATE PROFILE app_user LIMIT  
   SESSIONS_PER_USER          UNLIMITED  
   CPU_PER_SESSION            UNLIMITED  
   CPU_PER_CALL               3000  
   CONNECT_TIME               45  
   LOGICAL_READS_PER_SESSION  DEFAULT  
   LOGICAL_READS_PER_CALL     1000 ; 
 
CREATE PROFILE app_user2 LIMIT 
   FAILED_LOGIN_ATTEMPTS 5 
   PASSWORD_LIFE_TIME 60 

   PASSWORD_REUSE_TIME 60 
   PASSWORD_REUSE_MAX 5 
   PASSWORD_VERIFY_FUNCTION verify_function 
   PASSWORD_LOCK_TIME 1/24 
   PASSWORD_GRACE_TIME 10; 

Apply and review patches
The Oracle Corporation releases patches as part of bug fixes and enhancements. Patches are
an important part of the database life cycle and prevent major outages. It's very important that
we apply the latest patches to the database and keep the DB up to date.
Applying a patch involves downloading the patches, checking for pre-requisites, applying the
patches and verifying that the patch is applied successfully, and that it has fixed the desired
issue. Patching also means migrating from one minor version to another, such as migrating
from 11.2.0.3 to 11.2.0.4. These are patch set upgrades, and they are basically bundles of
patches tailored to fix various issues. These minor upgrades are different than major
upgrades where we move from one release of software to a newer release of software.
There are multiple ways to apply the patch:
Using Enterprise Manager cloud control
Using the OPatch utility

Using Enterprise Manager cloud control
You can use Enterprise Manager cloud control to provision and patch one or more databases.
You can use this for patching a single instance or Oracle RAC, as well as your grid
infrastructure home. Before we use Oracle EM cloud control for patching, we need to
perform the following tasks:
Install EM agent on all the hosts on which Oracle home is installed and needs to be
patched
Configure named and preferred credentials in Enterprise Manager
Configure a library to store downloaded patch files

Using the OPatch utility
You can obtain patches and patch sets from My Oracle Support, which is the Oracle Support
Services website, at https://support.oracle.com. Patches are available under the Patches &
Updates tab. If you know the patch number, you can directly download the patch. If you don't
know the patch number, you can search for it based on the product family, product, software
release, and platform.
Once you download the patch, you can unzip the same and go through the README file of
the patch. The README file has all the instructions to apply the patch either online or offline.
Some patches should be applied only offline. Those instructions are mentioned clearly in the
patch.
Preparing for patching
Before you apply the patch, you need to perform a few checks and fix certain issues, if you
have any. This is especially important in an RAC environment. The following checks should
be performed before you go for patch application:
Check whether the ORACLE_HOME environment variable is set correctly.
Perform a backup before applying the patch. This is for safety, in case of any issues, we
can always roll back. Alternatively, the patch application utility OPatch also makes a
backup of the binaries and libraries that it's going to change. We can use the same utility
to roll back the patch.
Stage the patch on each node if you are applying the patch to an RAC environment.
Update the PATH environment variable to include the OPatch directory path.
Check SSH user equivalence among the nodes of the RAC.
Applying the patch
When it comes to patching, we have to use a different method for patching the RAC databases
than we did for patching single instance databases. The Opatch utility of Oracle automatically
detects the cluster environment when we are patching the RAC environment. It also knows the
list of nodes to apply the patch from the software inventory.
Before you install a patch, you must stop all the applications running from the software
directory that is being patched.
In a cluster environment, we need to bring down an instance on one node at a time. Other
nodes can be up and serving the user request. Patching in a cluster environment can happen
one node at a time so there is no 100% downtime involved, whereas, in the case of a single
instance environment, we have to take complete downtime. Some of the patches can also be
applied online while the database is running. Such patches do not need downtime.
Depending on whether you are using RAC or single instance, you need to stop the following

services:
Oracle RAC database: Oracle RAC database, Enterprise Manager Agent, listener, and
any other applications that run from the Oracle RAC home directory.
Oracle Grid Infrastructure: Oracle RAC database and all applications running from the
Oracle RAC home directory, any single-instance databases that use the same Oracle ASM
instance as the cluster database, the Oracle ASM instance, all node applications, Oracle
Clusterware, and any other applications that run from the Grid home directory.
To apply the patch, you can perform the following steps:
1. Change the directory to the location where the patch was unzipped. If you have unzipped
a patch, it will automatically create a directory named after the patch number. You should
CD to that directory.
2. Export the PATH environment variable to include the OPatch directory:
export PATH=$PATH:$ORACLE_HOME/OPatch 
3. Check whether this patch is applied already. Following the opatch command will check
the inventory to see whether the patch is already applied:
opatch lsinventory | grep <patch_number> 
4. Check if all pre-requisites are met by the patch before it's applied. You can use the
following command to validate whether the patch meets all pre-requisites:
opatch prereq CheckConflictAgainstOHWithDetail -ph ./ 
5. Apply the patch. The following commands apply the patch. You have to make sure you
are in the patch directory after extracting the patch:
Note
Make sure that you read the README file that comes with patch. README file has all
the required instructions for applying and rolling back the patch
       opatch apply

Backup and recover the database
A backup is a copy of the database that can be used either to create a new copy of the database
or to recover the same database in case of disaster (data loss). As a backup administrator,
your primary job is making and monitoring backups for data protection. You should device
your backup and recovery plan in order to be able to recover your database in any situation.
You should be sure to take a complete backup of your database, including all the datafiles,
controlfile, spfile and all archivelog files that are generated between the two backups.
Backups can be either physical or logical:
Physical backups are the backups of datafiles taken using the RMAN utility. These files
are used for creating and recovering Oracle databases. These backups are taken directly
from the blocks of datafiles.
Logical backups are the backups of data and not the datafiles or blocks. We can use the
expdp utility to take the logical back of the table or a schema or an entire database.
In case of issues that need media recovery, such as user errors or application errors, DBA
intervention is required for database recovery. Recovery can be complete or incomplete. An
incomplete recovery means data loss, so a DBA should design its backup and recovery
strategy in order to be able to support complete recovery. Data loss is very bad for business.

Classification of backups
We have different classifications of backup. The following are some of the different types of
backups we can take.
Backup sets vs image copies
Backup sets are the backup copies of datafiles stored in an RMAN-specific format. We cannot
use backup sets directly in the database. We have to restore datafiles from backup sets so that
those datafiles can be used in the database. A backup set contains the data from one or more
datafiles, archived redo logs, or control files or SPFILE. A single backup set can contain
backups of multiple datafiles, but we cannot mix datafile and archive log backups in same
backup sets. A backup set is made of multiple backup pieces, which are RMAN-specific
formatted files. By default, we have one backup piece in one backup set.
An image copy is an absolute copy of a datafile. It's just a bit-by-bit duplicate copy of a
datafile. The size of an image copy is the same as the size of the datafile. RMAN creates
image copies when we use the clause AS COPY in our backup command.
RMAN backup vs user-managed backup
We can create backup schedules to take backups automatically. Recovery Manager is fully
integrated with the Oracle database to perform a range of backup and recovery activities,
including maintaining an RMAN repository of historical data about backups. You can access
RMAN through the command line or through Enterprise Manager.
In a user-managed backup, the DBA is responsible for taking a manual backup of the database.
The DBA decides the complete strategy of retention and deletion of backups manually.
Hot backup versus cold backup
In the case of a cold backup, the database must be down for taking the backup. Because the
database is down and no activity is happening, cold backups are consistent backup. For taking
code backup, you should shut down the DB in NORMAL mode. If the database is shut down with
the IMMEDIATE or ABORT option, it should be restarted in RESTRICT mode and then shut down
with the NORMAL option. You can use OS utilities, such as cpio, tar, dd, or some third-party
utility, to back up the datafiles.
In case you cannot get downtime for the database because of the kind of business it is
supporting, you can also go for a hot backup. In the case of a hot backup, you don't have to
shut down the database. Instead, you can put the tablespace in begin backup mode and copy all
the datafiles to the backup location, even when they are getting accessed. This is possible
because, when we put tablespace in "begin backup" mode, it will freeze the header of the
datafiles of that tablespace and, even though changes are being written to the datafile, the
entire changed block also gets copied to online redo logs. This is required to overcome the
fractured block.

Note
You can read more about fractured blocks in my blog article
https://avdeo.com/2008/05/27/excess-redo-log-generation-during-hot-backup-oracle-9i/.
Note
Also, you can read more about hot backup at my blog article
https://avdeo.com/2008/01/02/oracle-hot-backup-under-microscope/
Full backup versus incremental backup
A full backup takes a backup of every allocated block in a datafile. A full backup can be a
backup set or an image copy. If we take an image copy of a datafile, even unallocated blocks
are backed up. In the case of the backup set, only allocated blocks are backed up.
An incremental backup stores only blocks changed since the last backup. So, if we take a full
backup on Sunday and take an incremental backup on Wednesday, only the blocks that are
changed between Sunday and Wednesday will be backed up. We cannot have image copies for
incremental backup. Incremental backups are small in size and require much less time for
completing the backup if we enable block change tracking.

Incremental backup and block change tracking file
RMAN incremental backup helps in reducing the backup time of database and also make
backup size small. Taking complete backup of database is painful especially when the size of
database is huge (in terabytes). Also, its unnecessary to copy all the blocks when only small
number of blocks are changed since last backup. Incremental backup takes backup of only
changed block. In older releases, RMAN used to scan all the blocks of datafiles to check the
last change SCN of the block and it used to compare that number with last backup SCN. But
from Oracle 10g onwards, Oracle introduced block change tracking file. Using change
tracking file, Oracle records all the blocks that are changed since last backup. So when we
take incremental backup, RMAN has to scan only the block change tracking file to get the list
of blocks that are changed and copy only those blocks in the incremental backup. This avoids
the need to scan every block of datafiles.
After enabling change tracking, the first level 0 incremental backup still has to scan the entire
datafile, as the change tracking file does not yet reflect the status of the blocks. Subsequent
incremental backup that uses this level 0 as parent will take advantage of the change tracking
file. Change tracking is disabled by default, as it incurs a small amount of performance
overhead to keep track of blocks that are changing. However, this overhead is far less than the
saving that we can gain by perfoming incremental backup of datafiles. One change tracking
file is created for the whole database. By default, the change tracking file is created as an
Oracle managed file in DB_CREATE_FILE_DEST. You can also specify the name of the block
change tracking file, placing it in any location you choose.
You can enable block change tracking using following command:
SQL> ALTER DATABASE ENABLE BLOCK CHANGE TRACKING using file 
'/u01/app/oracle/fast_recovery_area/deo/rman_change_tracking.dbf'; 
Database altered. 
You can check if block change tracking is enabled or not and change tracking file using
following command:
SQL> select * from V$BLOCK_CHANGE_TRACKING; 
STATUS     FILENAME                                      BYTES     CON_ID 
---------- ---------------------------------------- ---------- ---------- 
ENABLED    /u01/app/oracle/fast_recovery_area/deo/r   11599872          0 
          man_change_tracking.dbf 

Validating backup
The main purpose of RMAN validation is to check for corrupt blocks and missing files. You
can also use RMAN to determine whether backups is good or not and can be used for restore.
You can use the following RMAN commands to perform validation:
VALIDATE
BACKUP ... VALIDATE
RESTORE ... VALIDATE
Physical and logical block corruption
A physical corruption is the one that is caused by underlying media(disk). When physical
corruption happens, Oracle will not be able to read the block as the data in the block gets
corrupted. Format of data is different than Oracle format or the entire block contains only
zeros. It can also cause mismatch of header and footer. Physical corruptions are listed in
V$DATABASE_BLOCK_CORRUPTION view as well as in alert log file.
In case of logical corruption, usually a row piece of information gets corrupted. So you
might have header and footer of the block intact, but some problem with one of the record in
the block. Usually logical corruptions are detected when the block is accessed by any process.
RMAN does not check for logical corruption by default. But you can use RMAN to detect
logical corruption by using CHECK LOGICAL clause of validate datafile. RMAN logs logical
corruption in alert log file and also a trace file is created for the same.
Oracle Database supports different techniques for detecting, repairing, and monitoring block
corruption. The technique depends on whether the corruption is interblock corruption or
intrablock corruption.
In intrablock corruption, the corruption occurs within the block itself. This corruption can be
either physical or logical and can be fixed by RMAN.
In an interblock corruption, the corruption occurs between blocks and can only be logical.
This kind of corruption cannot be fixed by RMAN.
For example, the V$DATABASE_BLOCK_CORRUPTION view records intrablock corruptions, while
the Automatic Diagnostic Repository (ADR) tracks all types of corruptions. The following
explains how Intrablock Corruption and Interblock Corruption can be detected, tracked and
fixed.
Intrablock corruption
You need to consider following points about detection, tracking and fixing of intrablock
corruption:
Detection: You can detect intrablock corruption using RMAN and also use dbv -

dbverify utility. If a database process can encounter the ORA-1578 error, then it can
detect the corruption and monitor it. Corruption information also gets written to alert log
file.
Tracking: As mentioned above, we can check alert log file for tracking corruption. We
can also check v$database_block_corruption view to check blocks that are corrupt or
we can use dbv utility to validate the datafiles. Finally RMAN can also report corruption
of datafiles while taking backups or during validation
Fixing: Several techniques are available for fixing corruption. The best and most simple
way to fix corruption is to go with block repair. We can do block repair using RMAN
utility. During block repair, RMAN will try to recover the block from flashback logs if
flashback is enabled. If not, it will try to repair the block from standby automatically if
standby is configured and if you are using Oracle 12c version. If not, Oracle will try to
repair the block from backups of datafiles taken previously using RMAN.
Interblock corruption
You need to consider the following points about detection, tracking and fixing of interblock
corruption:
Detection: Only DBVERIFY and the ANALYZE statement detect interblock corruption.
Tracking: The database monitors this type of block corruption in ADR.
V$DATABASE_BLOCK_CORRUPTION does not report interblock corruption. You can also run
RMAN ... VALIDATE CHECK LOGICAL command to check interblock corruption.
Fixing: You must fix interblock corruption with manual techniques such as dropping an
object, rebuilding an index, and so on.
You can use the VALIDATE command to manually check for physical and logical corruptions
in database files. This command performs the same types of checks as BACKUP VALIDATE, but
VALIDATE can check a larger selection of objects. RMAN can check and detect corruption for
following types of files:
Datafiles
Backup sets
Control files
Image copies of datafiles
Copy of control file
Recovery area files
Recovery files (includes incremental backup sets, control file auto backups, archived log
files, datafiles copy)
spfile
The following types of files are not validated by RMAN:
PFILE
Block change tracking file
Password file
Data pump export dump files

Data recovery advisor
Data recovery advisor is a tool provided by Oracle to help DBA's in recovering the databases.
It automatically diagnoses data failures and provides appropriate repair options based on
situation. It can also automatically repair the data based on DBA's request. A database failure
happens because of corruption or loss of data. If we have a backup strategy in place and if we
have taken complete backup of database and archive logs, it's easy to restore and recover the
database. These steps of restoring and recovering of data is provided and if required, done
automatically by data recovery advisor.
In the RMAN command-line interface, the Data Recovery Advisor commands are LIST
FAILURE, ADVISE FAILURE, REPAIR FAILURE, and CHANGE FAILURE.
A failure is detected either automatically by the database or through a manual check such as
the VALIDATE command. You can use LIST FAILURE command to list all the failures detected
by Oracle or RMAN. Each failure has unique failure ID. You can also use ADVICE FAILURE
command to know about the possible solutions to overcome the corruption. Data recovery
advisor automatically provides the required commands for recovering the corruption or lost
data. REPAIR FAILURE command can actually run those commands on behalf of DBA (if DBA
wants to repair failures automatically) and fix all repairs automatically.
Detecting failures
Failure can be detected by multiple means. Whenever a database access a corrupt block, it
reports the same in alert log or in V$DATABASE_BLOCK_CORRUPTION view. You can run health
monitor to perform all data integrity checks, which provides information about all failures.
After a problem is diagnosed by the database as a failure, you can obtain information about
the failure and potentially repair it by means of Data Recovery Advisor.
Data Recovery Advisor can diagnose following failures:
Failure to read datafile or control files because they do not exists, do not have correct
access permissions or have been taken offline and so on.
Physical corruption of data blocks such as checksum failure or bad header
Inconsistencies such as datafile needs a recovery as its older than other datafiles
I/O failures caused by hardware issues or OS driver issues or OS parameters like
open_files reaching max values etc.
Failure status
Every failure has a failure status: OPEN or CLOSED. Whenever a new failure is created, its status
is OPEN. After the failure is repaired, it's status changes to CLOSED. You can use LIST FAILURE
to list all the open failures. Database recovery advisor revalidates the open failures, every
time you run LIST FAILURE command. That way, you have the latest and valid list of failures.
So if you fix some failures either manually or using data recovery advisor, LIST FAILURE
will automatically mark them closed and will not show them again.

Failure priority
Every failure has a property assigned: CRITICAL, HIGH or LOW. Usually whenever data
recovery advisor detects a failure, it assigns the priority as either CRITICAL or HIGH. Critical
priority failures require immediate attention as that can cause unavailability to database. For
example, system data file corruption can have a bigger effect on availability. Failures with
high availability can make database partly unavailable. For example, a normal data file
corruption or a block corruption can cause partial database unavailable.
Manual actions and automatic repair options
Once the failure is detected, you can either fix the failure automatically using data recovery
advisor or you can fix them manually. In some cases, automatic fix is not an option as it needs
additional steps to be done by DBA. For example if a controlfile is lost and we don't have
controlfile backup, in that case you need to manually create controlfile and RMAN cannot
recover automatically. If RMAN can perform automatic fix of the problem, we can simple use
REPAIR FAILURE command and RMAN will automatically take care of repairing the failure.
You can also manually repaire any failure for which automatic repair option is available. Data
recovery advisor also provides a script, which has all the commands to fix the failures. You
can also manually run those commands.
Repair scripts
Whenever Data Recovery Advisor generates an automated repair option, it creates a script
that explains which commands RMAN intends to use to repair the failure. Data Recovery
Advisor prints the location of this script, which is a text file residing on the operating system

Implement flashback technology
Oracle flashback technology is a group of Oracle database features that help you rewind the
database to past time to see the database back in time. You can use flashback technology to
take the entire database back in time or take a tablespace or even a table back in time.

Enabling Oracle flashback
You can enable flashback at database level and it will start creating flashback log just like
archive log. Before we enable the flashback let's check what flashback can be used for.
You can achieve following using flashback technology:
View the data of the past from a table
Perform query that shows detailed history of changes to the database or a table
Recover a table to point in time
Automatically track and archive transactional data changes
The following command will enable flashback. Note that its mandatory to enable archiving
before we enable flashback, or flashback will not be enabled and you will receive an error:
SQL> alter database flashback on; 
Database altered. 

Flashback database and restore points
Oracle flashback technology is used mainly for data protection. For example, if someone
makes a mistake while updating the data or while doing any DML, you can use flashback
technology to go back to the time before the user has performed the update or any DML and
get the original data back. This is a better option than doing point in time recovery using
RMAN as it does not need backups to be restored. You can also create a safe marker called
restore point before making major changes to database (like big deployment or an upgrade).
Oracle keep track of very change that is happening in database from this restore point. If
something bad happens, you can simply rewind your database to the restore point that you
have created using one simple command. This functionality can be used in test and dev to
validate new changes and rewind back database to original to try something new or retry the
same changes. Flashback database is also helpful in reinstating new standby after failover and
recreation can be avoided.
For making use of flashback, you need to enable the same. You can enable flashback at
database level using ALTER DATBASE FLASHBACK ON statement. Enabling the flashback will
generate flashback logs, which are used for rewinding the database back in time. So before
you enable flashback, you need to set db_recovery_file_dest parameter, which determines
the location where flashback logs will be stored. You also need to set
db_recovery_file_dest_size, which tells the size of location to be used. Once flashback logs
occupy the space set by this parameter, it will automatically delete the old logs. Lastly, you can
set db_flashback_retention_target parameter to determine how far back you can flashback
the database.
When you do flashback database to rewind database to past time, the command actually scans
all the blocks that are changed from the past target time and restore the version of those
blocks from the flashback logs. The version restored in just before the target time. Oracle
then uses archive logs to perform the recovery to the exact target time. This is required
because flashback logs can take back the database little further than it is supposed to. So
Oracle uses archive log to bring it forward to the exact target time that we have provided. So
it's very important that we retain all the archive logs for the entire time period spanned by the
flashback logs. In practice, redo logs are typically needed much longer than the flashback
retention target to support point-in-time recovery.
Limitations of flashback database
Because flashback database works by undoing changes to the data files that exist at the
moment when you run the command, it has the following limitations:
You cannot use flashback technology to restore or recover an accidentally deleted or
dropped datafile. You should use RMAN for that. If you flashback database to the time
when the file existed, only the entry of the file will take place in the metadata but
physically file will not be present.
You cannot use flashback to undo shrink datafile operation

If you restore controlfile from backup or create a new controlfile, all your flashback
metadata will be gone and you cannot flashback the database.
If you use flashback to go back in past at which a NOLOGGING operation was in progress,
your objects and datafiles can get corrupted..
Prerequisites for flashback database
To ensure successful operation of flashback database, you must first set several key database
options:
You should run your database in archive log mode as archive logs are used to recover
the database after flashback.
You should enable fast recovery area using DB_RECOVERY_FILE_DEST parameter.
Flashback logs are stored only in flash recovery area
You cannot use flashback to repair media failures by going back in past before media
failure happened. You have to use RMAN for repairing such failures.
Performing a flashback database operation
We have several flashback technologies such as Oracle Flashback Query, DBMS_FLASHBACK
package and flashback data archive.
Let's look at enabling flashback data archive.
Enabling flashback data archive
Once you enable flashback, all the above flashback related features will be available to you,
except Flashback Data Archive (Oracle total recall). To enable flashback data archive, you
need to create a tablespace which will hold flashback data archive, set retention time which
defines number of days' flashback data archive will retain the data and assign flashback data
archive to table to start storing its previous image.
Here are the simple steps:
1. Create flashback data archive tablespace:
  SQL> create tablespace fda1 datafile     
'/u01/app/oracle/oradata/deo/deo/datafiles/fda-01.dbf' size 500M; 
 
 Tablespace created. 
2. Create flashback data archive and use above tablespace for storage. You can also specify
quota on the tablespace. If not, default is unlimited quota:
    SQL> create flashback archive FDA1 tablespace fda1 quota 200M    
retention 1 year; 
 
    Flashback archive created. 
3. Enabling flashback data archive: By default, flashback archiving is disabled for any table.
To enable flashback archiving for a table, include the FLASHBACK ARCHIVE clause in

either the CREATE TABLE or ALTER TABLE statement. In the FLASHBACK ARCHIVE clause,
you can specify the Flashback Data Archive where the historical data for the table will be
stored. You can specify flashback archive during the creation of table or using ALTER
TABLE command to enable flashback archive for existing tables:
   SQL> create table T1( 
      2  col1 number, 
      3  col2 varchar2(10)) FLASHBACK ARCHIVE FDA1; 
 
     Table created. 
You can use alter table to enable flashback archive on existing table:
SQL> alter table T1 flashback archive fda1; 
Table altered. 
You can disable flashback archive using NO FLASHBACK ARCHIVE clause.
Tip
DDL statements not allowed on tables enabled for flashback data archive
Using any of the following DDL statements on a table enabled for Flashback Data Archive
causes error ORA-55610: Invalid DDL statement on history-tracked table
ALTER TABLE statement that does any of the following:
Drops, renames, or modifies a column
Performs partition or sub partition operations
Converts a LONG column to a LOB column
Includes an UPGRADE TABLE clause, with or without an INCLUDING DATA clause:
DROP TABLE statement
RENAME TABLE statement
TRUNCATE TABLE statement
DBMS_FLASHBACK package
The DBMS_FLASHBACK package provides the same functionality as Oracle Flashback Query, but
Oracle Flashback Query is sometimes more convenient as this package can be used
programmatically.
You can use DBMS_FLASHBACK to set the time in a session to some past time and run some
queries. Those queries will be run as of old time and return the older data. You can then return
back to present time. Because you can use the DBMS_FLASHBACK package to perform queries on
past data without special clauses such as AS OF or VERSIONS BETWEEN, you can reuse existing
PL/SQL code to query the database at earlier times.

You must have the EXECUTE privilege on the DBMS_FLASHBACK package. To use the
DBMS_FLASHBACK package in your PL/SQL code:
1. Specify a past time by invoking either DBMS_FLASHBACK.ENABLE_AT_TIME or
DBMS_FLASHBACK.ENABLE_AT_SYSTEM_CHANGE_NUMBER.
2. Perform regular queries (that is, queries without special flashback-feature syntax such as
AS OF). Do not perform DDL or DML operations. The database is queried at the specified
past time.
3. Return to the present by invoking DBMS_FLASHBACK.DISABLE. You must invoke
DBMS_FLASHBACK.DISABLE before invoking DBMS_FLASHBACK.ENABLE_AT_TIME or
DBMS_FLASHBACK.ENABLE_AT_SYSTEM_CHANGE_NUMBER again. You cannot nest
enable/disable pairs.

Relocate SYSAUX occupants
SYSAUX tablespace is a mandatory tablespace that gets created when we create a database.
This tablespace holds all the auxiliary components of an Oracle database. This arrangement
was done in order to consolidate all the auxiliary components into a single place and to avoid
using SYSTEM tablespace for the same. We can still install other components in a different
tablespace if required during the time of installation. Also, it's possible to move a component
into or out of SYSAUX tablespace after installation. We can achieve this by using a procedure
in V$SYSAUX_CCUPANTS view. These respective procedures make the move of component to the
required location.
For example, if you have installed Oracle Ultra Search component in default SYSAUX
tablespace and you now want to move this component into its dedicated tablespace. You can do
that by checking the procedure name in V$SYSAUX_OCCUPANTS view and running that procedure.
For example, let's say we want to relocate one of the SYSAUX component ORDIM as shown:
SQL> column OCCUPANT_NAME format a10
SQL> column OCCUPANT_DESC format a40
SQL> column SCHEMA_NAME format a10
SQL> column MOVE_PROCEDURE format a10
SQL> select OCCUPANT_NAME, OCCUPANT_DESC, SCHEMA_NAME, MOVE_PROCEDURE,
SPACE_USAGE_KBYTES from v$sysaux_occupants where OCCUPANT_NAME = 'ORDIM'; 2 
OCCUPANT_N OCCUPANT_DESC SCHEMA_NAM MOVE_PROCE SPACE_USAGE_KBYTES
---------- ---------------------------------------- ---------- ---------- ---
---------------
ORDIM Oracle interMedia ORDSYS Components ORDSYS 512
SQL> 
As you can see from the preceding output, we have a MOVE PROCEDURE to move this
component out of SYSAUX tablespace. The same procedure can be used to move this
component from other tablespace to SYSAUX tablespace.
Executing the following procedure will move this component out of SYSAUX tablespace.
Let's move this component to USERS tablespace.
SQL> desc ordsys.ord_admin      
PROCEDURE MOVE_ORDIM_TBLSPC 
 Argument Name                  Type                    In/Out Default? 
 ------------------------------ ----------------------- ------ -------- 
 P_NEW_TABLESPACE               VARCHAR2                IN 
 
SQL> exec ordsys.ord_admin.move_ordim_tblspc('USERS'); 
 
PL/SQL procedure successfully completed. 

Create a default permanent tablespace
Prior to Oracle 9i release, If the user did not specify a tablespace explicitly while creating a
segment, it was created in SYSTEM tablespace, provided the user has quota allocated in
SYSTEM tablespace either granted explicitly or though the system privilege UNLIMITED
TABLESPACE.
Oracle9i alleviated this problem by allowing the DBA to specify a default temporary
tablespace for all users created without an explicit temporary tablespace clause. But this was
only for temporary tablespace.
In Oracle Database 10g onwards, you can similarly specify a default tablespace for users.
During database creation, the CREATE DATABASE command can contain the clause DEFAULT
TABLESPACE <tsname>. After creation, you can make a tablespace the default by issuing:
ALTER DATABASE DEFAULT TABLESPACE <tsname>; 
All users created without the DEFAULT TABLESPACEclause will have<tsname> as their default.
You can change the default tablespace at any time through this ALTER command, which allows
you to specify different tablespaces as default at different points.
If the default tablespace is not specified during the database creation, it defaults to SYSTEM. But
how do you know which tablespace is default for the existing database? Issue the following
query:
SQL> alter database default tablespace users; 
 
Database altered. 
 
SQL> SELECT PROPERTY_VALUE FROM DATABASE_PROPERTIES WHERE PROPERTY_NAME =  
'DEFAULT_PERMANENT_TABLESPACE'; 
 
PROPERTY_VALUE 
-------------------- 
USERS 
The DATABASE_PROPERTIES view shows some very important information, in addition to the
default tablespace-such as the default temporary tablespace, global database name, time zone,
and much more.
Also, you cannot drop a default permanent tablespace even if it's empty. You will receive the
following error:
SQL> drop tablespace test_tbs including contents and datafiles; 
drop tablespace test_tbs including contents and datafiles 
* 
ERROR at line 1: 
ORA-12919: Can not drop the default permanent tablespace 

Use secure file LOBs
Earlier release had only one type of LOB storage. In Oracle 11g, Oracle introduced a new
type of LOB storage called SecureFile LOB and it named the earlier LOB as BasicFile LOB.
Even though SecureFile LOB was introduced in Oracle 11g, it was not the default for LOB
storage. BasicFile LOB was the default for LOB storage. Let's checkout both SecureFile and
BasicFile LOB.

BasicFile LOB vs SecureFile LOB
With Oracle 12c, Oracle made SecureFile LOB as the default for LOB storage. So if we create
a table with LOB columns without specifying any LOB types, by default SecureFile LOB will
get created. One of the essential condition for using SecureFile LOB is that the tablespace in
which you are going to store SecureFile LOB should have Automatic Segment Space
Management (ASSM) enabled. SecureFile LOB is designed to provide much better
performance compared to BasicFile LOB.
SecureFile LOG supports three new features that BasicFile LOB did not support:
Compression: It uses advanced compression to transparently compress SecureFile LOB
to space disk space
Deduplication: It automatically detects duplicate LOBs in a column of a table and stores
only 1 LOB even though multiple column uses same LOB
Encryption: Oracle encrypts SecureFile LOB data using transparent data encryption
(TDE), which allows the data to be stored securely.
Oracle recommends that you enable these feature at the time of creation of table. If you want
to enable these feature at later point (after the table has data already), it reads, modifies and
writes the converted LOB again. This is potentially lot of work and can lock the table for
longer time.
When you create a table having LOB column, you can use the STORE AS SECUREFILE clause to
create the LOB as SecureFile LOB.
For example:
SQL> CREATE TABLE t1 (a CLOB) 
   LOB(a) STORE AS SECUREFILE( 
   COMPRESS LOW 
   DEDUPLICATE 
   CACHE 
 ) tablespace users;  
Table created. 
As mentioned in the preceding code, you can also use ALTER TABLE statement to alter the LOB
parameters as required.
For example:
SQL> ALTER TABLE t1 MODIFY 
  LOB(a) ( 
          COMPRESS LOW 
  );  
Table altered. 

SQL> ALTER TABLE t1 MODIFY  
    LOB(a) ( 
        KEEP_DUPLICATES  
   );  
Table altered. 
Using the preceding features for SecureFile LOB will require compatibility of 11.2.0.0.0 or
higher. We cannot reduce compatibility once 11.2.0.0.0 is set.
If you want to upgrade BasicFiles LOBs to SecureFile LOBs, you can do so using CTAS,
online redefinition, export/import and so on; but these solutions required twice the space as
original table. Another way to do this is to partition the original table and take one partition at
a time to convert from BasicFile LOB to SecureFile LOB.
Oracle database has an initialization parameter DB_SECUREFILE. The valid values for
DB_SECUREFILE are:
NEVER: Specifying NEVER value for DB_SECUREFILE parameter will prevent SecureFile
LOB to be created. Even when you specify SecureFile for an LOB, they will still be
created as BasicFile. If no storage option is specified BasicFile is default.
IGNORE: Disallows creating SecrueFile LOB and ignore any error if any SecureFile
options are used with BasicFile LOB.
PERMITTED: BasicFile LOB will remain as default by SecureFile LOB can be created if
specified explicitly as storage option
PERFERRED: Default will be SecureFile LOB unless BasicFile LOB is specified explicitly.
This is the default value of DB_SECUREFILE parameter in Oracle 12c.
ALWAYS: Always creates SecureFile LOB unless the tablespace does not have ASSM
enabled.
FORCE: attempts to create all LOBs as SecureFiles LOBs even if users specify BASICFILE

Use Direct NFS
Oracle Direct NFS (dNFS) is an NFS client from Oracle that provides faster and more
scalable access to NFS storage. Oracle recommends using direct NFS to connect to NAS
storage as it provide better scalability and performance. To start communication with NAS
filer, Oracle uses oradism that needs root level access to obtain root file handle for exported
volume and NFS server port and mount port to initiate communication with NAS filer. Once
root file handle and ports are obtained, further communication can be done by normal Oracle
user process using normal privileges
Direct NFS is a new feature of Oracle 11g and Oracle recommends using this instead of using
OS kernel NFS client. Data is cached once in user space, which saves memory. Performance is
further improved by load balancing across multiple network interfaces.

Enabling Direct NFS
The following are the steps to enable or disable direct NFS direct NFS:
1. Execute these commands to set libraries for direct NFS:
    cd $ORACLE_HOME/lib 
    mv libodm11.so libodm11.so_stub 
    ln -s libnfsodm11.so libodm11.so 
2. To enable direct NFS:
    cd $ORACLE_HOME/rdbms/lib 
    make -f ins_rdbms.mk dnfs_on 
3. To disable direct NFS:
    cd $ORACLE_HOME/rdbms/lib 
    make -f ins_rdbms.mk dnfs_off 

Summary
In this chapter we have seen several topics related to general administration of database. We
started the chapter with memory architecture and process architecture of Oracle database. We
then saw the difference between shared server and dedicated server process. We also saw
installing and configuring Oracle database, configuring listener and service names and how
we can monitor database errors by generating alerts. We then saw daily administration tasks
that a DBA has to perform specifically we talked about online table movement, recompiling
invalid objects, segment advisor to reclaim space and SQL tuning advisor to tune SQL
statements. We then learned how we can manage resources for users by creating and
configuring profiles for them.
Next we talked about backup and recovery and how we use that to recover from corruption
automatically using data recovery advisor. We then talked about flashback technology and
how we can use this technology in various situation. We also learned about SYSAUX
occupants and how we can move them inside or out of SYSAUX tablespace. Finally, we
discussed about SecureFile LOB and how it's better than BasicFile LOB. We also took an
example to create SecureFile LOB column. We concluded the chapter with using Direct NFS
for NAS storage.
In the next chapter we are going to look into the topics of performance management.
Specifically, we will discuss about monitoring and identifying performance issues,
performing real application testing and using resource manager to manage resources within
database.

Chapter 11. Performance Management
Performance is one of the key things that any DBA should understand. It's one of the most
essential activities that can keep a database going for years. Over time, we often see an
increase in our database's workload because of increased business; we see more modules and
connections working on our database, as well as the general growth of data , making the
database bigger and the queries more expensive. To tackle such issues, it's essential that we
understand how we should design our database and manage different performance policies so
that we keep our database performing well for as long as possible.
This is the second chapter of section 2 of this book and in this chapter we are going to cover
performance management of databases in general. Before we begin, it is important to
understand the scope of this chapter and to set right expectations. In this chapter we are going
to cover the topics which are mentioned in the Performance Management topic under section
2 of 1Z0-060 exam course. We are not covering performance management end-to-end as that
will make this chapter very big and topics will not be aligned as per the exam course. In order
to keep this chapter in line with the exam course, we are covering only the required topics of
performance management. The following topics are covered in this chapter:
Design the database layout for optimal performance
Monitor performance
Manage memory
Analyze and identify performance issues
Perform real application testing
Use resource manager to manage resources
Implement application tuning

Design the database layout for optimal
performance
This is one of most important activities for an Oracle DBA. As a DBA, we should understand
the implications of designing Oracle databases right from the beginning. We see the impact of
the decisions we make in the early design phase later on in the life of database. Attaining
optimal database performance throughout the life of the database can be achieved by
understanding the best practices for database configuration. We need to consider best
practices for the following areas:
Designing for performance
Designing for availability
Designing for scalability

Designing for performance and scalability
The key to database and application performance is design, not tuning. While tuning is quite
valuable, it cannot make up for poor design. Your design must start with an efficient data
model, well-defined performance goals and metrics, and a sensible benchmarking strategy.
Otherwise, you will encounter problems during implementation, when no amount of tuning
will produce the results that you could have obtained with good design.
The following are some of the key points to be considered while designing a database for
performance and scalability.
Collecting application requirements
Designing the schema and data model
Partitioning and data retention/purging
Deciding indexes based on access patterns
Deciding correct data types for columns
Constraints for data integrity
Normalizing tables
Using bind variables
Using automatic segment space management
Using database resource manager

Collecting application requirement
Analyze the data requirements of your application by following this basic procedure:
Collecting requirements:The first and most important step for having an effective
database design is to collect the requirements. You should liaise with those who have
commissioned the database and have a clear idea about its business requirements. You
should speak with them about what type of applications will be accessing the database,
what the nature of the data will be, what the expectations of the performance and response
time are, and so on. Try to check the application and what type of information it is going
to fetch from the database, what data model is required, and so on.
Analyzing the requirements: The next step after collecting the requirements is to
analyze the requirements to understand what type of data modeling is required, how
tables have to be normalized, draw some E-R diagram to understand how the data will be
stored and what the relationship will be between them.
Functional analysis of data: After analyzing the requirements, we can use that data to
understand how the data flow will happen. We can construct the building blocks after
various stages to understand the data flow pattern.
Designing the schema and data model
Designing the schema starts with logical design. Logical design is the diagrammatic
representation of the database, and depicts the relationship among the database objects, such
as tables. It takes into consideration the requirements of various business modules and how
they will impact the data modification and storage. An effective design also considers the life
of the data, and how the data will move to different storage layers before it's purged from the
database.
To show relationships among the database objects you should do the following:
Convert the data requirements into data items, namely columns
Categorize the related columns into tables
Draw relationships among the columns and tables, determining the primary and foreign
key for each table
Normalize the tables to reduce redundancy and dependency to the minimum
Physical design is the actual design of the database, and is the implementation of the logical
design on the physical database.
Once we have our logical design ready, we can create scripts that use DDL to create the
required schema, define the database objects in their required sequence, define the storage
requirements of the specification, and so on.
While doing the schema design, the following points should be considered:
How much data you are going to store?

Based on the amount of data that you want to store, you need to decide the space requirement,
as well as the purging policy.
How to purge older data?
Purging can be done either by application modules using normal delete statements or DBA
can take care of purging. The best way to purge old data is to create a table as a partition table
based on the date column. This date column can be used to decide the age of data, and older
partitions can be simply dropped. This makes it very efficient to maintain only the required
data. With Oracle 12c, you can easily implement ADO policies over partitioned tables.
Deciding on indexes:
Careful consideration must be put in place to create the optimal number of indexes. Indexes
are decided based on the queries that the application is going to run on the database schema.
Based on the access pattern of the queries, appropriate indexes should be created. We should
also decide among unique indexes and non-unique indexes. If we have partition tables, efforts
should be made to create local indexes as far as possible as this makes maintaining the
indexes easier. If you want to create unique indexes without using a partition key in the index,
you have to go with the global index option.
Normalizing tables:
Efforts should be made in designing normalized tables in the schema. We should avoid having
the same repeated columns in different tables. Instead we should normalize the tables and
define the relationships between the tables.
Using bind variables
A bind variable placeholder in an SQL statement or PL/SQL block indicates where data must
be supplied at runtime.
Suppose that you want your application to insert data into the table created with this statement:
CREATE TABLE T1 (col1 VARCHAR2(30), col2 VARCHAR2(30)); 
Because the data is not known until runtime, you must use dynamic SQL. The following
statement inserts a row into table T1, concatenating string literals for columns col1 and col2:
INSERT INTO T1 (col1,col2) VALUES ( ''' || REPLACE (col1, '''', '''''') || ''', 
                                   '''  || REPLACE (col2, '''', '''''') || 
'''); 
The following statement inserts a row into table T1 using bind variables :b1 and :b2 for
columns col1 and col2:
INSERT INTO T1 (col1,col2) VALUES (:b1, :b2); 
Coding is easier if the statement uses bind variable placeholders. Take the example of a bulk

load operation inserting 1,000 rows into table T1 using each of the preceding methods.
The method that concatenates string literals uses 1,000 INSERT statements. Each statement is
hard parsed, qualified, checked for security, optimized, and compiled. Every statement's hard
parsing can result in an increased number of latches.
The method that makes use of bind variables uses only one INSERT statement. The statement is
hard parsed only once when it is executed for the first time and later it is soft parsed for other
999 executions. During hard parsing the statement is compiled, optimized and a plan is
generated for this. This plan along with other information is stored in a shared pool. During
the soft parse, Oracle optimizer directly uses this information instead of generating it again.
The caching of the statement is an important benefit of the bind variables usage.
An application using bind variable placeholders is more scalable, demands less resources,
and runs faster than an application using string concatenation, and is less prone to SQL
injection attacks.
Using automatic segment space management
Before we understand automatic segment space management and its benefits, let's look at
locally managed tablespaces and dictionary managed tablespaces:
Locally managed tablespaces: Extent management by the bitmaps
Dictionary managed tablespaces: Extent management by the data dictionary
When you create a tablespace, you choose one of these methods of space management. Later,
you can change the management method with the DBMS_SPACE_ADMIN PL/SQL package.
Locally managed tablespaces
A locally managed tablespace maintains its own extent bitmap in the header of the datafile to
keep track of free and used space in the datafile. Bitmap marks the free and used extent using
bits like 0 and 1. Each bit in the bitmap corresponds to a block or a group of blocks.
Whenever an extent is allocated or used by  a server process, Oracle optimizer updates the
bitmap values in the datafile header to show the new status of blocks.
For example, you can create locally managed tablespace using following command:
CREATE TABLESPACE user_tbs DATAFILE '/u01/oradata/user_tbs_01.dbf' SIZE 100M 
EXTENT MANAGEMENT LOCAL UNIFORM SIZE 10K;
Dictionary managed tablespaces
In older releases, before local tablespace management, dictionary managed tablespace was the
default option. In the case of dictionary managed tablespace, Oracle uses the data dictionary to
manage its extents. Whenever a table is added to an extent, Oracle updates the appropriate
tables in the data dictionary. Similarly, whenever an extent is freed, it updates the data
dictionary tables marking that extent as free. Since we need to update the data dictionary for

each and every table and extent, it was not an efficient method, and used to cause more
locking and blocking of sessions. So Oracle migrated to locally managed tablespaces.
For example, you can create dictionary managed tablespace using the following command:
CREATE TABLESPACE user_tbs DATAFILE '/u01/oradata/user_tbs_01.dbf' SIZE 100M 
EXTENT MANAGEMENT DICTIONARY DEFAULT STORAGE ( INITIAL 10K NEXT 10K MINEXTENTS 
2 MAXEXTENTS 50 PCTINCREASE 0);
Segment space management in locally managed tablespaces
Automatic free space management is available only in locally managed tablespaces. When a
locally managed tablespace is created using the SEGMENT SPACE MANAGEMENT clause, it allows
you to specify how the free space within the segment has to be managed. There are two
methods with which Oracle database manages the segment space:
Auto: Automatic segment space management uses bitmaps to show the space usage of
each block within a segment. The bitmap is stored in a separate block called a bitmapped
block, resulting in less contention on the header block. The following example shows the
use of automatic segment space management
CREATE TABLESPACE user_tbs DATAFILE '/u01/oradata/user_tbs_01.dbf' SIZE 
100M EXTENT MANAGEMENT LOCAL SEGMENT SPACE MANAGEMENT AUTO;
Manual: Manual segment space management makes the Oracle database use free lists to
manage the space within segments. Free lists will make the list of blocks have free space
to accommodate new rows. The following example shows the use of manual segment
space management
CREATE TABLESPACE user_tbs DATAFILE '/u01/oradata/user_tbs_01.dbf' SIZE 
100M EXTENT MANAGEMENT LOCAL SEGMENT SPACE MANAGEMENT MANUAL;
Using database resource manager
When you are running your database on a host, resources are evenly allocated to the database
by the OS. So if a user executes a query that is taking up most of the CPU, the OS will try to
allocate as much CPU as is required by that process. Other processes will start to see the
impact of that and will get delayed. If all processes are executing with the same priority, then
the OS will perform a lot of context switches to divide the CPU time between each of those
processes. The following are some of the problems that can happen if you don't control the
resource allocation on your host:
Excessive context switches might happen if the same priority processes are running and
everyone demands the same CPU time. This usually happens when the number of
processes is very high.
Inefficient scheduling: The operating system de-schedules database servers while they
hold latches, which is inefficient.
Inappropriate allocation of resources: The operating system distributes resources
equally among all active processes and cannot prioritize one task over another.

Inability to manage database-specific resources, such as parallel execution servers and
active sessions.
The resource manager helps to overcome these problems by allowing the database more
control over how hardware resources are allocated. You can classify your sessions into
groups based on the criticality of those sessions, and then allocate resources to that group.
For example, you can create a group and add all your sessions that are going to run
background jobs during the night, and then you can assign resources to that group so that they
will use only a set percentage of the CPU.
This way, you have control over the distribution of resources on the host to optimize your
hardware utilization. With the resource manager, you can:
Make sure that you allocate every session a certain minimum amount of CPU regardless
of the load on the system.
Distribute the CPU to different groups based on the percentage set by the resource
manager. For example, we can create a group of sessions that are running data
warehouse queries and assign a resource manager plan to those sessions allocating more
CPU.
Limit the degree of parallelism that can be used by sessions that are assigned to a
consumer group.
Maintain the control over the order of the statements in the parallel statement queue.
Parallel statements of a high priority application are enqueued before the parallel
statements of a low priority application.
Restrict the number of parallel servers used by a session. This is a way to make sure
parallel servers are not allocated only to one group of users.
Manage resources consumed by ad hoc sessions by setting a higher limit to the CPU
consumption for that group of sessions.
Limit the resource consumed by a session by setting the threshold on the amount of CPU
or IO, and then automatically terminate the session or switch the session to another
consumer group that is assigned less CPU and resources. In this way it can help mitigate
the impact of sessions consuming high resources.
Prevent the start of execution of certain operations; this will optimize operations that will
take longer than the specified time.
Limit the idle time of sessions. Once the session reaches the idle time, it will be
disconnected. This can be further defined to mean only sessions that are blocking other
sessions.
Dynamically change the resource plan of a consumer group based on the time of day.
For example, during the night, you can have batch jobs running in a consumer group
which is assigned high resources. During the day, you can reduce the resources assigned
to the batch job consumer group and assign more resources to the OLTP consumer
group.
Let's take an example of creating a resource manager group. In this example we are going to

assign 60% of the CPU resources to user_group, and 20% to batch_job_group and remaining
20% to maintenance_group each:
1. Create a pending area for plan, consumer group and directives:
BEGIN
DBMS_RESOURCE_MANAGER.CREATE_PENDING_AREA();
END;
/
PL/SQL procedure successfully completed.
2. Create resource plans:
BEGIN
DBMS_RESOURCE_MANAGER.CREATE_PLAN( PLAN => 'MY_RSRC_PLAN', COMMENT => 
'resource plan for my database');
END;
/
PL/SQL procedure successfully completed.
3. Create resource consumer groups:
BEGIN
DBMS_RESOURCE_MANAGER.CREATE_CONSUMER_GROUP( CONSUMER_GROUP => 'USER_GROUP',
COMMENT => 'resource consumer group for users sessions');
DBMS_RESOURCE_MANAGER.CREATE_CONSUMER_GROUP(
CONSUMER_GROUP => 'BATCH_JOB_GROUP',
COMMENT => 'resource consumer group for running batch jobs');
DBMS_RESOURCE_MANAGER.CREATE_CONSUMER_GROUP(
CONSUMER_GROUP => 'MAINTENANCE_GROUP',
COMMENT => 'resource consumer group for maintenance tasks in database');
END;
/
PL/SQL procedure successfully completed.
4. Create resource plan directives:
BEGIN
DBMS_RESOURCE_MANAGER.CREATE_PLAN_DIRECTIVE(
PLAN => 'MY_RSRC_PLAN',
GROUP_OR_SUBPLAN => 'USER_GROUP',
COMMENT => 'user sessions in database',
CPU_P1 => 60,
PARALLEL_DEGREE_LIMIT_P1 => 0);
DBMS_RESOURCE_MANAGER.CREATE_PLAN_DIRECTIVE(
PLAN => 'MY_RSRC_PLAN',
GROUP_OR_SUBPLAN => 'BATCH_JOB_GROUP',
COMMENT => 'batch jobs in database',
CPU_P1 => 20,
PARALLEL_DEGREE_LIMIT_P1 => 8);
DBMS_RESOURCE_MANAGER.CREATE_PLAN_DIRECTIVE(
PLAN => 'MY_RSRC_PLAN',
GROUP_OR_SUBPLAN => 'MAINTENANCE_GROUP',
COMMENT => 'maintenance tasks in database',
CPU_P1 => 20,
PARALLEL_DEGREE_LIMIT_P1 => 2);
DBMS_RESOURCE_MANAGER.CREATE_PLAN_DIRECTIVE(

PLAN => 'MY_RSRC_PLAN',
GROUP_OR_SUBPLAN => 'OTHER_GROUPS',
COMMENT => 'other_groups is mandatory',
CPU_P1 => 0,
PARALLEL_DEGREE_LIMIT_P1 => 0);
END;
/
PL/SQL procedure successfully completed.
5. Validate a pending area for the plan:
BEGIN
DBMS_RESOURCE_MANAGER.VALIDATE_PENDING_AREA();
END;
/
PL/SQL procedure successfully completed.
6. Submit the pending area for the plan:
BEGIN
DBMS_RESOURCE_MANAGER.SUBMIT_PENDING_AREA();
END;
/
PL/SQL procedure successfully completed.
7. Assign users to resource consumer groups: For assigning the user to the resource
consumer group, we need to again create a pending area:
BEGIN
DBMS_RESOURCE_MANAGER.CREATE_PENDING_AREA();
END;
/
PL/SQL procedure successfully completed.
8. Grant permissions to a user to switch to the respective consumer groups. We are
assuming here that we have test_user1, test_user2 and test_user3 created in the
database:
BEGIN
DBMS_RESOURCE_MANAGER_PRIVS.GRANT_SWITCH_CONSUMER_GROUP(
GRANTEE_NAME => 'TEST_USER1',
CONSUMER_GROUP => 'USER_GROUP',
GRANT_OPTION => FALSE);
DBMS_RESOURCE_MANAGER_PRIVS.GRANT_SWITCH_CONSUMER_GROUP(
GRANTEE_NAME => 'TEST_USER2',
CONSUMER_GROUP => 'BATCH_JOB_GROUP',
GRANT_OPTION => FALSE);
DBMS_RESOURCE_MANAGER_PRIVS.GRANT_SWITCH_CONSUMER_GROUP(
GRANTEE_NAME => 'TEST_USER3',
CONSUMER_GROUP => 'MAINTENANCE_GROUP',
GRANT_OPTION => FALSE);
END;
/
PL/SQL procedure successfully completed.
9. Assign consumer groups to the respective users:

BEGIN
DBMS_RESOURCE_MANAGER.SET_INITIAL_CONSUMER_GROUP(
USER => 'TEST_USER1',
CONSUMER_GROUP => 'USER_GROUP');
DBMS_RESOURCE_MANAGER.SET_INITIAL_CONSUMER_GROUP(
USER => 'TEST_USER2',
CONSUMER_GROUP => 'BATCH_JOB_GROUP');
DBMS_RESOURCE_MANAGER.SET_INITIAL_CONSUMER_GROUP(
USER => 'TEST_USER3',
CONSUMER_GROUP => 'MAINTENANCE_GROUP');
END;
/
PL/SQL procedure successfully completed.
10. Validate the pending area:
BEGIN
DBMS_RESOURCE_MANAGER.VALIDATE_PENDING_AREA();
END;
/
PL/SQL procedure successfully completed.
11. Submit the pending area:
BEGIN
DBMS_RESOURCE_MANAGER.SUBMIT_PENDING_AREA();
END;
/
PL/SQL procedure successfully completed.
12. Enable the resource manager plan to be used by the database instance:
ALTER SYSTEM SET RESOURCE_MANAGER_PLAN = MY_RSRC_PLAN;

Designing for availability
The following are some of the key points to be considered while designing a database for
availability:
Enable archive log mode and force logging mode:
Having a database in ARCHIVELOG mode is really essential for a production database.
Without archive log mode enabled, it makes it impossible to perform a complete
database recovery. Similarly, FORCE LOGGING is essential for generating redo for
every operation happening in database. Without enabling FORCE LOGGING, users can
use NOLOGGING feature to skip generating redo logs in order to gain performance.
Features such as Oracle Data Guard and Flashback Database require the database to
be in ARCHIVELOG mode.
You can however, isolate critical tables in specific tablespaces that needs to be
recovered and you can use tablespace level FORCE LOGGING instead of using
database level FORCELOGGING.
Configure redo log groups appropriately:
Create multiple redo log members in each group for multiplexing. It is
recommended to keep one set in the data area and another in the fast recovery area
(this recommendation is not applicable in cases where the redo logs are maintained
in an Oracle ASM high redundancy disk group). Organizing redo logs by
multiplexing it helps in recovering in case one of the copies is lost or corrupted due
to I/O failure or gets deleted by mistake. If we have at least one member of redo log
group available and working fine, the instance continues to run. We should always
maintain the multiplex copies of the redo logs in separate disks and if possible on
separate controllers. Also, the disk that is used for redo log groups should be fast in
order to gain performance benefits.
Size redo log files and groups: A few recommendations are given as follows:
Create a minimum of three redo log groups. This is to prevent the log writer
process (LGWR) from waiting for a redo log group to be available after the log
switch.
Create all redo logs and standby redo logs with equal size.
Create a redo log size of 2 GB or more or redo log size >= peak redo rate x 20
minutes. In a one-hour time window, there should not be more than four or five log
switches.
Allocate redo log files to high performance disks.
If using ASM, either keep the redo log files on a high redundancy disk group, or
multiplex those across the normal redundancy disk groups.
Configure fast recovery area:
The fast recovery area is an Oracle managed file system or Automatic Storage
Management (ASM) disk group meant for backup and recovery operations of the
database. It is the area where archive logs and flash back logs are created. Recovery
manager creates backup sets and image copies in this area.
DB_RECOVERY_FILE_DEST initialization parameter designates the location of fast

recovery areas.
DB_RECOVERY_FILE_DEST_SIZE initialization parameter states the disk limit in bytes.
Specifying DB_RECOVERY_FILE_DEST without DB_RECOVERY_FILE_DEST_SIZE is not
allowed.
The least recommended disk limit is the aggregate size of the database, incremental
backups, all archive logs that are not copied to tape yet, and flashback logs.
Check out Chapter 10, Core Administration for details about flashback technology and
implementation.

Configuring FAST_START_MTTR_TARGET parameter
The FAST_START_MTTR_TARGET parameter controls how much recovery is required if the
database crashes at any point. The value of this parameter is specified in seconds, and, based
on this value, the Oracle database decides how frequently it has to perform checkpoint in
order to keep the data files updated. For example, if we set the value of this parameter to 120
seconds, then Oracle will keep running the checkpoint as per some internal calculation, and
this frequency is different o 120 seconds. If the database crashes at any time, it would need
approximately 120 seconds to recover the database, so this variable is kind of a simplified
abstract setting, which only takes our expectations in terms of recovery time as input. If we set
this to a low value so as to have a fast recovery during a crash, then the database will keep
writing data aggressively and can cause high IO. If we set this to a high value, the IO will be
less, but the recovery time will be more after the crash. So we need to balance the value as per
our requirement.
Initially, set the FAST_START_MTTR_TARGET initialization parameter to 300 (seconds), or to the
value required for your expected recovery time objective (RTO).
Outage testing for cases such as for node or instance failures during peak loads is
recommended.

Protection against corruption
A block is set to be corrupted if Oracle is unable to recognize the block, or if its contents are
not internally consistent. A block can be physically corrupted, where Oracle is not able to read
the data from the block, or the block could be logically corrupted, where data is intact but the
header and footer of the block is bad and so its metadata is lost.
A corrupted block is reported in the alert log file or in the v$database_block_corruption
view. We have multiple solutions available to fix the corrupted block-data guard, block media
recovery, and data file media recovery. Database level logical corruptions can be fixed using
Oracle Flashback. There are multiple utilities available for checking logical data structures
proactively. The SQL*Plus ANALYZE TABLE statement detects inter-block corruptions. The
RMAN VALIDATE command detects physical and logical corruption.
To achieve the most comprehensive data corruption prevention and detection, do the
following:
Use Oracle Data Guard with physical standby databases to prevent widespread block
corruption
Set the Oracle Database block-corruption initialization parameters on the Data Guard
primary and standby databases
On the primary database set:
DB_BLOCK_CHECKSUM=FULL
DB_LOST_WRITE_PROTECT=TYPICAL
DB_BLOCK_CHECKING=FULL
On the standby database set:
DB_BLOCK_CHECKSUM=FULL
DB_LOST_WRITE_PROTECT=TYPICAL
DB_BLOCK_CHECKING=FULL
Performance overhead is incurred on every block change; therefore, performance testing is
of particular importance when setting the DB_BLOCK_CHECKING parameter. Oracle highly
recommends the minimum setting of DB_BLOCK_CHECKING=MEDIUM (block checks on data
blocks but not index blocks) on either the primary or standby database. If the performance
overhead of setting DB_BLOCK_CHECKING to MEDIUM or FULL is unacceptable on your primary
database, then set DB_BLOCK_CHECKING to MEDIUM or FULL for your standby databases.
Use the HIGH REDUNDANCY option available with Oracle Automatic Storage Management
for disk mirroring to protect against disk/media failures
The Oracle Active Data Guard option is useful for automatic block repair
Having Data Recovery Advisor configured helps in the automatic detection of data
failures
Configure flashback to perform fast point-in-time recovery to fix logical corruption
often caused by human error, and also to reinstate the primary database after the failover
Use Recovery Manager for backup and recovery, and periodically perform RMAN BACKUP

VALIDATE CHECK LOGICAL to detect corruptions

Configure automatic shared memory management
You can configure automatic shared memory management using the SGA_TARGET parameter.
Setting this parameter to a non-zero value enables automatic shared memory management.
Once we enable automatic shared memory management, it takes care of auto tuning
parameters (memory pools) to which it will assign the memory automatically. The following
are the memory pools, which are taken care of by automatic shared memory management:
DB_CACHE_SIZE
SHARED_POOL
LARGE_POOL
JAVA_POOL
STREAMS_POOL
Use automatic shared memory management so that the different SGA components get adjusted
dynamically according to the workloads being processed without any intervention. If a
specific component needs more memory, Oracle will deallocate some memory from other
pools that are not in use and allocate that memory to the pool where it's currently required.
You can also manually set the value for the above auto tuning parameters. That set value will
be considered as a minimum value for that parameter and that memory pool size will not
reduce below that value.
If the database uses the server parameter file SPFILE, then the database will be aware of the
sizes of the auto tuned SGA components across the instance startups. The next time the
instance will start with the information from the previous instance, and then will evaluate the
workload it left earlier during the last shutdown.

Setting up the maximum SGA size
Initialization parameter SGA_MAX_SIZE is an optional parameter, and denotes the maximum
size of the SGA for the instance lifetime. The size is mentioned in MB or GB. If you do not
specify SGA_MAX_SIZE, then a default value will be generated that is the sum of all components
given to this parameter. Ideally, the value of this parameter should be greater than or equal to
the sum of all components specified, or defaulted at the time of initialization. The
SGA_MAX_SIZE value sets up a hard limit for the SGA_TARGET value.

Setting SGA target size
Automatic shared memory management is enabled by setting up a SGA_TARGET parameter in
the initialization file or server parameter file to a non-zero value. This parameter indicates the
total size of the SGA. This eliminates the need for setting up a specific set of individual
memory component parameters. Automatic memory management dynamically tunes them
according to the workload requirements.
The following SGA components are automatically sized when SGA_TARGET is set:
The shared pool
The large pool
The Java pool
The buffer cache
The streams pool
If the automatically-sized SGA component is set to a non-zero value, then it is the minimum
value that the component is assigned during SGA tuning. This will be explained in detail later
in the section.
We have covered these topics in Chapter 10, Core Administration under the Memory
architecture section.

Monitoring performance
Monitoring the health of a database and ensuring that it performs optimally is an important
task for a database administrator.
The following are the different forms of monitoring Oracle database:
Proactive database monitoring
Diagnosing performance problems
Performance degradation over time
Using advisors to optimize database performance

Proactive database monitoring
Proactive database monitoring involves having a lot of monitoring scripts and alerts to make
sure that an issue is detected well before it causes any impact to the end user or service.
Proactive database monitoring checks for any vital signs of issues related to database health,
analyzes the workload running against the database, and automatically identifies critical issues
that might require your attention. Proactive database monitoring reports the issues either in
the alert log or in enterprise manager, or via email.
The following are the different methods of doing proactive database monitoring:
Alerts: Alerts keeps you updated with the latest events happening in the database. The
Oracle database has some default alerts configured in the database that reports the issue
either in the alert log file or in enterprise manager. You can also receive email alerts for
such issues. For example, Oracle by default has tablespace alerts that send a warning
message whenever the tablespace is 85% full and a critical message if the tablespace is
97% full. Similarly, Oracle has alerts for critical errors and errors such as ORA-01555.
You can also create your own customized alerts to indicate when the event has happened.
You can also customize the way in which you want to receive the alerts, such as via
email, EM, alert log, or any other way.
For example, we can set an alert for CPU time per call as we have seen in Chapter 10 ,
Core Administration:
DBMS_SERVER_ALERT.SET_THRESHOLD( 
DBMS_SERVER_ALERT.CPU_TIME_PER_CALL,   DBMS_SERVER_ALERT.OPERATOR_GE, 
'8000',  
 DBMS_SERVER_ALERT.OPERATOR_GE, '10000', 1, 2, 'deo', 
 DBMS_SERVER_ALERT.OBJECT_TYPE_SERVICE, 'deo.example.com'); 
Self-diagnostics: Automatic Database Diagnostics Monitor (ADDM) is a performance
analysis tool that helps DBAs in analyzing the current performance of the database, and
provides a root cause of the problem. It also provides solutions for fixing the issue.
ADDM is an engine that makes it possible for the database to tune its own performance.
ADDM works on top of Automatic Workload Repository (AWR) reports. AWR reports
are the snapshot of database metadata taken at a configured interval of time. An AWR
snapshot periodically collects snapshots of the database state and workload, and stores
them in AWR tables in SYSAUX tablespace. These snapshots have a default retention
period of 30 days, after which the snapshots are purged automatically. ADDM uses these
snapshots for analyzing the performance of the database and tries to identify the
performance issues and probable root cause, and suggests solutions for fixing those
issues.
ADDM helps DBAs in analyzing the major bottlenecks in the database, recommends
solutions, and quantifies expected benefits. The ADDM report is one of the first few places
that the DBA should be checking whenever analyzing the bad performance.

The following example shows running ADDM report using the script provided by Oracle
$ORACLE_HOME/rdbms/admin/addmrpt.sql
SQL>@?/rdbms/admin/addmrpt
Current Instance
~~~~~~~~~~~~~~~~
DB Id DB Name Inst Num Instance
----------- ------------ -------- ------------
1085061196 ORCL 1 orcl
...
...
Specify the Begin and End Snapshot Ids
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Enter value for begin_snap: 185832
Begin Snapshot Id specified: 185832
Enter value for end_snap: 185836
End Snapshot Id specified: 185836
...
...
Generating the ADDM report for this analysis ...
ADDM Report for Task 'TASK_166233'
----------------------------------
...
...
Activity During the Analysis Period
-----------------------------------
Total database time was 16384 seconds.
The average number of active sessions was 4.54.
Summary of Findings
-------------------
Description Active Sessions Recommendations
Percent of Activity
---------------------------------------- ------------------- ---------------
1 Top SQL Statements 3 | 65.92 3
2 Top Segments by "User I/O" and "Cluster" 1.57 | 34.47 4
3 Undersized PGA 1.51 | 33.17 0
4 PL/SQL Execution .12 | 2.62 3
...
...
The ADDM report is quite big so it's not possible to provide the entire report here, but it
mainly contains the summary or finding and top findings and recommendations. It analysis
the top activity that is consuming the resources and provides recommendations to fix the
same. You can also run the ADDM report using Oracle database control (EM Express in
Oracle 12c) or Oracle cloud control.
Manually monitoring database performance
Every DBA has its own set of scripts to report violation of policies, bad SQLs, incorrect
configuration, and so on. These tools range from a very raw format of providing general
formatted output to a very sophisticated GUI report to identify anomalies in the database.
These are commonly used for identifying issues with the database proactively.

Diagnosing performance problems
Sometimes the issues are identified by DBAs or users. These issues require the immediate
attention of DBAs to diagnose the issue and to identify and fix the root cause. Slow
performance could be related to the overall database being slow, or a specific query or set of
queries being slow.
Sometimes the problems are flagged by the Automatic Database Diagnostics Monitor
(ADDM), which does a top-down system analysis every hour by default and reports its
findings on the Oracle Enterprise Manager Home page. ADDM runs every hour based on
AWR snapshots and uses these snapshots to analyze the performance of the database. ADDM
is a complex engine with many threshold values set, and if any of these threshold values are
breached ADDM flags that as a problem. It also recommends the solution for fixing that
problem.

Performance degradation over time
Sometimes, we witness a situation wherein database performance does not degrade suddenly,
but over a period of time, for example, over a few months. It's difficult to identify the
performance difference if we compare the performance daily with the previous day. But if
you compare the performance of the database today against two months back, you can see the
performance has degraded.
This is where the AWR Compare Report comes in. The AWR report shows data between two
snapshots, whereas the AWR Compare Report compares data between two AWR reports.
Using AWR Compare Report, you can identify detailed performance attributes and
configuration changes between the two time periods. You can also compare two AWR reports
that are generated with a different time duration.
For example, you can compare two hours of good time in the past with six hours of bad time
recently. The report normalizes the statistics by the amount of time spend on the database for
each time period and provides a report based on the configuration that has the highest
difference between the periods.
If you have a batch job that used to complete in two hours and now it's taking four hours, you
can compare the AWR report of that two hour period in the past and a four hour period in the
present. Comparing such AWR reports will provide us with detailed configuration changes,
difference between statistics, and so on. This makes it easier for us to understand the change
and identify the root cause.
Managing baselines
Baselines are the way to preserve your snapshots so that you can use them any time in the
future for comparison. For example, you can take snapshots of the database when the DB is
performing well with a full load. You can convert those snapshots into baselines and preserve
them. Baselines are not subject to purging like normal snapshots, so the baselines will stay for
years, and when the performance degrades in the future, you can use those baselines to
compare against the current AWR reports to understand why the database is performing badly.
Comparing period AWR
When performance degradation occurs over time, you can run the AWR compare periods
report to compare the degraded performance of current time snapshots with an existing
baseline taken previously that represents good performance of the database.
Unlike a plain AWR report that compares the database performance between two points in
time, the compare period AWR compares the database performance between two distinct
periods, for example, a compare period AWR report between 10 am and 11 am last Monday
and this Monday. The report describes the performance changes and the root cause of the
changes. You can create a compare period AWR report either from the EM or using compare

period AWR script directly running in the database.
The following are the sections in the report that provide different details. The report has four
sections:
Overview: This section provides the overview of the compare AWR report. It shows the
SQL commonality, which represents how comparable the load is between two periods.
So if you are running a similar workload with the same SQLs, you will see that the SQL
commonality is high (close to 100%), but as the workload differs and we have different
SQLs running in the current workload as against the baseline or snapshot from the
previous time period that we are comparing, the SQL commonality will be low (close to
0%). You should see high commonality, and then you are only comparing similar
workload. You should not get a commonality less than 70-80%, otherwise the compare
period AWR report is not very useful.
Configuration: The report consists of the information about the base period and
comparison period values for various parameters as per instance, host, and database
categories.
Findings: Findings are the performance changes, and pin point the system changes that
caused the major performance differences.
Resources: Resources such as CPU, memory, and I/O usage are detailed in the report and
summary of the division of the database time.
We can generate an AWR compare period using the script provided by Oracle or by using EM
Cloud control. We can see the AWR compare period option in one of the images showing
under Generating ADDM compare period topic in Chapter 1, Getting Started with Oracle 12c.
Here we are going to see the generat on of an AWR compare period using
script: $ORACLE_HOME/rdbms/admin/ awrddrpt.sql
When you run an AWR Compare Report, you need to provide two sets of begin snap ID and
end snap ID. The first pair of begin snap ID and end snap ID will generate an AWR report and
it will be compared with a second AWR report generated by second set of begin snap ID and
end snap ID.
The report looks like an AWR report, but all sections are compared side by side between two
AWR reports.
SQL>@$ORACLE_HOME/rdbms/admin/awrddrpt.sql
...
...
Specify the First Pair of Begin and End Snapshot Ids
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Enter value for begin_snap: 185800
First Begin Snapshot Id specified: 185800
Enter value for end_snap: 185804
First End Snapshot Id specified: 185804
...
...
Specify the Second Pair of Begin and End Snapshot Ids

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Enter value for begin_snap2: 185816
Second Begin Snapshot Id specified: 185816
Enter value for end_snap2: 185820
Second End Snapshot Id specified: 185820
...
...
Workload Comparison
~~~~~~~~~~~~~~~~~~~        1st Per Sec  2nd Per Sec  %Diff  1st/Txn 
2nd/Txn   %Diff
                        -------------  ----------- ------  -------  -
-----  ------
               DB time:           8.2        3.0    -62.9     
0.0      0.0   -50.0
              CPU time:           2.3        0.7    -70.0     
0.0      0.0  -100.0
     Redo size (bytes):   1,743,007.9  287,553.0    -83.5  5,233.5   
790.7   -84.9
 Logical read (blocks):     125,738.3   22,663.1    -82.0    377.5    
62.3   -83.5
         Block changes:       4,132.2    1,194.5    -71.1    
12.4      3.3   -73.6
Physical read (blocks):       7,968.7    6,954.9    -12.7     23.9    
19.1   -20.1
Physical write (blocks):       2,253.9       75.1    -96.7     
6.8      0.2   -96.9
      Read IO requests:       1,136.6      309.6    -72.8     
3.4      0.9   -75.1
     Write IO requests:         888.6       26.0    -97.1     
2.7      0.1   -97.4
          Read IO (MB):          62.3       54.3    -12.7     
0.2      0.1   -21.1
         Write IO (MB):          17.6        0.6    -96.6     
0.1      0.0  -100.0
            User calls:       5,063.4    3,905.7    -22.9     15.2    
10.7   -29.3
          Parses (SQL):       1,770.9    1,276.5    -27.9     
5.3      3.5   -34.0
     Hard parses (SQL):           0.3        0.1    -46.4     
0.0      0.0     0.0
    SQL Work Area (MB):           8.5        2.0    -76.5     
0.0      0.0   -76.5
                Logons:           1.9        1.7     -9.7     
0.0      0.0  -100.0
        Executes (SQL):       2,539.9    1,711.9    -32.6     
7.6      4.7   -38.3
          Transactions:         333.0      363.7      9.2
                            First   Second    Diff
                           ------   ------  ------
% Blocks changed per Read:    3.3      5.3     2.0
         Recursive Call %:   24.3     15.1    -9.2
Rollback per transaction %:   88.6     91.9     3.4
            Rows per Sort:  637.2     87.9  -549.3
Avg DB time per Call (sec):    0.0      0.0    -0.0

Using advisors to optimize database performance
Advisors are powerful tools for database management, and are essential for DBAs. They
provide advice on specific issues in the database and cover a wide range of issues. You can
save a lot of time and effort in identifying the root cause of and solution to a problem by just
using the correct advisor.
The following performance advisors are provided by Oracle:
Automatic Database Diagnostic Monitor (ADDM): ADDM makes it possible for the
Oracle database to diagnose its own performance and determine how any identified
problems can be resolved.
SQL tuning advisor: The SQL tuning advisor is one of the helpful advisors in tuning the
SQL. You can provide one or more SQL statements as input to the SQL tuning advisor
and it provides the recommendations to improve the performance of SQLs. It can
provide multiple recommendations, along with the rationale and expected benefit for
each recommendation. The recommendations might be something like collecting
statistics on the table because the optimizer thinks that the stats are stale, or it might
advise you to create a new index or rebuild an existing index, and so on. You can choose
to accept and implement the recommendations automatically, or to manually implement
the recommendation.
SQL access advisor: SQL access advisor provides recommendations about the best
access method for fetching the required data. This includes advice about creating new
indexes, materialized views of materialized view logs that can improve the performance
of query. Note that new index advice is also provided by the SQL tuning advisor, but
adding a new index causes degradation of DMLs. The SQL tuning advisor does not take
that into consideration, and only focuses on the current SQL being tuned. However, the
SQL access advisor takes into consideration the impact of creating a new index when it is
advising you to do so.
Memory advisor: The SGA and PGA target parameters define the physical memory
available for the database. Sufficient physical memory allocation has a significant
performance impact on the database. Using memory advisor, you can run a what-if
analysis to evaluate the following without actually having to change the memory:
The performance benefit gained by adding more physical memory to the database. The
performance impact on reducing the physical memory already available to the database.
The following memory advisors exist:
Shared Pool Advisor (SGA)
Buffer Cache Advisor (SGA)
PGA Advisor

Managing memory
With the changing demands of the database, the various memory structures have to be
maintained at optimal sizes for the database instance. System Global Area (SGA) and
Program Global Area (PGA) have to be managed accordingly.
We can manage memory in the following ways:
Automatic memory management: Starting with the Oracle 10g database, Oracle
implemented automatic shared memory management. This means that Oracle was
managing SGA memory automatically between its components. From Oracle 11g
onwards, Oracle could manage both SGA and PGA memory. You can specify the total
memory to be allocated to the database and Oracle will distribute the memory chucks to
different components in the SGA and PGA. This capability is called automatic memory
management. This makes it very easy for DBAs to let Oracle decide which component
needs which quantity of memory.
Manual memory management: If you wish not to use automatic shared memory
management, so that you can exercise more control over the size of the memory
components, then disable automatic memory management and set up the database for
manual memory management.
Let's go a little deeper into these memory pool management methods.

Automatic memory management
Automatic memory management is the simplest way to manage memory for a database. You
only need to specify the amount of memory that will be allocated to the database by setting the
MEMORY_TARGET parameter. Once you set this parameter, Oracle will automatically manage the
memory between the SGA and PGA. You can also optionally set the parameter SGA_TARGET,
which enables automatic shared memory management and manages the memory to be
allocated to the SGA components.
The MEMORY_TARGET parameter is dynamic, and you can decrease or increase the amount of
memory allocated to the MEMORY_TARGET, provided that you are below the limit set by the
MEMORY_MAX_TARGET parameter. The MEMORY_MAX_TARGET parameter describes the maximum
amount of memory that will be assigned to the database, and your MEMORY_TARGET value
should be less than the MEMORY_MAX_TARGET value.
As memory requirements change, the instance dynamically redistributes memory between the
SGA and instance PGA. If you set any value for memory components, that value will serve as
the minimum value for that memory component. In the case of redistribution of memory
between components, Oracle will not reduce the memory component below the specified
value of that component.
The dynamic performance view V$MEMORY_DYNAMIC_COMPONENTS shows the current sizes of all
dynamically tuned memory components, including the total sizes of the SGA and instance
PGA. The view V$MEMORY_TARGET_ADVICE provides tuning advice for the MEMORY_TARGET
initialization parameter.
You can disable automatic memory management by setting MEMORY_TARGET to 0. If you disable
automatic memory management, you can either go with automatic shared memory
management to automatically manage the SGA memory, or you can do it manually.

Automatic shared memory management
Automatic shared memory management relieves you from the SGA memory management and
makes SGA management simpler and easier. You just mention the SGA_TARGET initialization
parameter and the database engine automatically allocates the memory for various SGA
components.
Use automatic shared memory management so that the different SGA components get adjusted
dynamically according to the workloads being processed without manual intervention.
The memory for dynamic components in the SGA is allocated in the form of granules. The
granule size is determined by the amount of SGA memory requested when the instance starts.
Specifically, the granule size is based on the value of the SGA_MAX_SIZE initialization
parameter. We've already covered this topic above, so we are not going to go over it again.

PGA memory management
The Oracle database manages the total memory dedicated to the PGA automatically. The
initialization parameter PGA_AGGREGATE_TARGET specifies the memory allocated for the PGA.
The database distributes the memory across both server and background processes, and
ensures the value is never exceeded. Automatic PGA allocations control the following
memory areas:
SORT_AREA_SIZE
HASH_AREA_SIZE
BITMAP_MERGE_AREA_SIZE
CREATE_BITMAP_AREA_SIZE
You need to set WORKAREA_SIZE_POLICY to AUTO in order for PGA to manage different work
areas automatically.
If you want to go for manually managing PGA memory, you will need to manually take care
of setting the following work areas: SORT_AREA_SIZE, HASH_AREA_SIZE,
BITMAP_MERGE_AREA_SIZE, and CREATE_BITMAP_AREA_SIZE.
Setting these parameters is difficult, because the maximum work area size is ideally selected
from the data input size and the total number of work areas active in the system. These two
factors vary greatly from one work area to another and from one time to another. For this
reason, Oracle strongly recommends that you leave automatic PGA memory management
enabled.

Setting minimums for automatically sized SGA components
You can control the minimum size of auto-tuned memory components in the SGA, namely the
following: DB_CACHE_SIZE, SHARED_POOL_SIZE, STREAM_POOL_SIZE, JAVA_POOL_SIZE, and
LARGE_POOL_SIZE. So even if you set SGA_TARGET to automatically manage these five memory
components, you can set a minimum value for these components so that, during memory
redistribution (in case of variable load), Oracle will not reduce the size of these components
following the size set by you. But the downside to this is that it reduces the total amount of
memory available for dynamic adjustment.

Configuring manually sized memory components
Even though we might use automatic memory management or automatic shared memory
management, there are a few memory components that should be sized manually.
The following memory components are still manually sized:
The log buffer LOG_BUFFER
The keep and recycle buffer caches DB_KEEP_CACHE_SIZE, DB_RECYCLE_CACHE_SIZE
Nonstandard block size buffer caches DB_nK_CACHE_SIZE
The keep and recycle buffer caches
You can configure separate buffer pools in the buffer cache to keep the data in the buffer
cache, or to make the buffers available for new data. Specific schema objects, such as tables,
table partitions, and indexes, can be assigned to the correct buffer pool; this is to control the
way their data blocks age out of the cache.
The KEEP pool is used to maintain frequently accessed objects in the memory.
The RECYCLE buffer pool is used to maintain the infrequently accessed objects and it is
also used for an application randomly accessing the blocks of a very large object.
The initialization parameters DB_KEEP_CACHE_SIZE and DB_RECYCLE_CACHE_SIZE are
used to configure the KEEP and RECYCLE buffer pools.
Nonstandard block size buffer caches
If you intend to use multiple block sizes in your database, you must have the DB_CACHE_SIZE
and at least one DB_nK_CACHE_SIZE parameter set.
The Oracle database assigns an appropriate default value to the DB_CACHE_SIZE parameter, but
the DB_nK_CACHE_SIZE parameters default to 0, and no additional block size caches are
configured.
The sizes and numbers of nonstandard block size buffers are specified by the following
parameters:
DB_2K_CACHE_SIZE
DB_4K_CACHE_SIZE
DB_8K_CACHE_SIZE
DB_16K_CACHE_SIZE
DB_32K_CACHE_SIZE
Each parameter specifies the size of the cache for the corresponding block size.

Analysing and identifying performance issues
Whenever the database encounters issues, we should analyze the situation in detail before
making any changes. Often, DBAs are provoked to make quick changes to fix an existing
problem based purely on an initial analysis; however, having a detailed analysis to fix the root
cause of problem helps in long term.
For example, if a query is performing badly, a DBA might try to generate a better plan and fix
the query. But if we do detailed analysis to identify the root cause of why that query has picked
a bad plan in the first place, we may reveal the exact root cause, and fixing that root cause will
fix many queries at once.
We will look into the following key areas that help us in analyzing and identifying
performance issues:
Automatic Workload Repository 
Tracing
Clustering factor
SQL performance analyzer
Cardinality feedback
Adaptive cursor sharing
Let's start with AWR, which is the key to gathering performance statistics

Automatic Workload Repository (AWR)
The AWR gathers the performance metrics and processes and maintains them in order to
detect problems and aid in self-tuning. You can view AWR data using the Enterprise Manager,
AWR reports, and the DBA_HIST views.
Snapshots: Snapshots are the point-in-time values of all the statistics at a specific time.
By default, the Oracle database automatically generates snapshots of the performance
data once every hour, and retains the statistics in AWR for eight days. Statistics values in
snapshots are used by ADDM to create diagnoses of the issues and help in providing an
idea of the root cause and recommendations. You can also manually generate a snapshot,
as well as change the frequency and retention period of snapshots.
Baseline: A baseline is a pair of snapshots usually taken over a period where the database
performed well at peak workload. You can compare the metrics gathered during a bad
performance period to the baseline to figure out the actual cause of the problem. The
snapshots of the baseline are not purged and are retained indefinitely.
Enabling the AWR: Use the initialization parameters STATISTICS_LEVEL= TYPICAL or
ALL to enable statistics collection by AWR. The default value for the preceding setting is
TYPICAL, so the gathering of database statistics by AWR is enabled by default.
Generating automatic workload repository reports: To generate an AWR report, we
need to provide two snapshot numbers as input. Those two snapshots need not be
continuous. The AWR report provides various statistics based on the difference in values
of two snapshots. It provides good information on activities that happened between the
time periods. It has a section for top wait events, top SQLs based on different criteria,
segment level statistics, and so on.
Every snapshot that happens at a configured interval captures lots of information, such as top
SQLs, segment level statistics, timing statistics, and so on. This information is stored in AWR
tables in SYSAUX tablespace. The difference between the values of these two snapshots
provides the basis for the AWR report.
For example, if a SQL ID is executing very quickly, and we take one snapshot now and
another after 15 mins, the AWR report will calculate the difference of the number of
executions of that SQL, and all statistics of that SQL can be calculated based on per execution
basis. This AWR data is very important for higher level, self-tuning components, such as
ADDM, SQL tuning advisor, SQL performance analyzer, and so on. AWR also computes time
model statistics based on the timing of different activities. This timing information is used in
the V$SYS_TIME_MODEL and V$SESS_TIME_MODEL views to compute timing information for
statistics such as DB TIME, CPU TIME, and so on. AWR works kind of like the eyes and ears of
the database performance diagnostics. Without AWR we are totally blind.
You can generate an AWR report using oracle supplied script
$ORACLE_HOME/rdbms/admin/awrrpt.sql or by using Oracle Cloud Control. The following is
an example of generating an AWR report using the script:

SQL>@?/rdbms/admin/awrrpt
...
...
Specify the Begin and End Snapshot Ids
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Enter value for begin_snap: 185800
Begin Snapshot Id specified: 185800
Enter value for end_snap: 185804
End Snapshot Id specified: 185804
...
...
             Snap Id      Snap Time      Sessions Curs/Sess
           --------- ------------------- -------- ---------
Begin Snap:    185800 13-Nov-16 00:00:32     2,913       4.9
 End Snap:    185804 13-Nov-16 01:00:58     2,805       5.0
  Elapsed:               60.43 (mins)
  DB Time:              496.70 (mins)
Load Profile              Per Second  Per Txn  Per Exec  Per Call
~~~~~~~~~~~~~~~          -----------  ------- --------- --------
            DB Time(s):         8.2      0.0      0.00     0.00
             DB CPU(s):         2.3      0.0      0.00     0.00
     Redo size (bytes): 1,743,007.9  5,233.5
 Logical read (blocks):   125,738.3    377.5
         Block changes:     4,132.2     12.4
Physical read (blocks):     7,968.7     23.9
Physical write (blocks):     2,253.9      6.8
      Read IO requests:     1,136.6      3.4
     Write IO requests:       888.6      2.7
          Read IO (MB):        62.3      0.2
         Write IO (MB):        17.6      0.1
            User calls:     5,063.4     15.2
          Parses (SQL):     1,770.9      5.3
     Hard parses (SQL):         0.3      0.0
    SQL Work Area (MB):         8.5      0.0
                Logons:         1.9      0.0
        Executes (SQL):     2,539.9      7.6
             Rollbacks:       295.0      0.9
          Transactions:       333.1
...
...
Top 10 Foreground Events by Total Wait Time
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                                     Tota    Wait   % DB
Event                           Waits Time Avg(ms)   time Wait Class
------------------------ ------------ ---- ------- ------ ----------
db file sequential read     3,210,021 14.8       5   49.7 User I/O
DB CPU                                8228           27.6
direct path read temp         795,320 4451       6   14.9 User I/O
control file sequential     1,105,052 716.       1    2.4 System I/O
direct path read               60,538 538.       9    1.8 User I/O
log file sync                 140,381  365       3    1.2 Commit
direct path write temp         52,690 337.       6    1.1 User I/O
Disk file operations I/O      179,049 295.       2    1.0 User I/O
recovery area: computing           32 161.    5056     .5 Other

PX Deq: Table Q Get Keys           72  156    2166     .5 Other
...
...
Tracing
Tracing is another way of monitoring and fixing performance issues. Oracle provides several
tracing tools to help you diagnose the issue. We can also implement end to end application
tracing to identify the source of the excessive workload and identify the exact part of the
application that needs tuning. We can identify the clients that are causing excessive load on the
system by using session ID, client identifier, service, module, action, and so on.
Oracle provides the trcsess command-line utility that consolidates the tracing information
based on specific criteria. Oracle also provides the tkprof utility to format the trace file to a
readable format for further analysis.
Let's look into the application level tracing first to understand how to generate the trace files.
We will get into the details of trcsess and tkprof after that.
End to end application tracing
In a multi-tier environment, a client's requests get routed through various layers and
connection pooling to finally connect to the database in order to process the request. This
makes it difficult to identify who is causing the problem. End to end application tracing uses a
client identifier to uniquely trace a specific end-client through all tiers to the database server.
This feature can easily identify the source of excessive workload, and allow you to contact the
user that is causing the problems.
Enabling and disabling statistic gathering for end to end tracing.
Application tracing lets you track and manage the application workloads. You can drill
down the workload problem using end to end application tracing for the client identifier,
service, module, action, session, and instance.
We have different procedures for enabling statistics gathering:
Based on CLIENT_ID
Based on service, module, and action
Based on session
Based on instance
Tracing for client identifier
The CLIENT_ID_TRACE_ENABLE procedure enables tracing globally for the database of a given
client identifier. For example:
EXECUTE DBMS_MONITOR.CLIENT_ID_TRACE_ENABLE(client_id => 'OE.OE',
       waits => TRUE, binds => FALSE); 
The CLIENT_ID_TRACE_DISABLE procedure disables tracing globally for the database of a
given client identifier. To disable the tracing from the previous example, do the following:

EXECUTE DBMS_MONITOR.CLIENT_ID_TRACE_DISABLE(client_id => 'OE.OE');
Tracing for service, module, and action
The SERV_MOD_ACT_TRACE_ENABLE procedure enables global SQL tracing for a given
combination of service name, module, and action for a database, unless an instance name is
specified in the procedure:
EXECUTE DBMS_MONITOR.SERV_MOD_ACT_TRACE_ENABLE(service_name => 'ACCTG',  
       waits => TRUE, binds => FALSE, instance_name => 'inst1'); 
To enable tracing for all actions for a given combination of service and module, do the
following:
EXECUTE DBMS_MONITOR.SERV_MOD_ACT_TRACE_ENABLE(service_name => 'ACCTG',  
       module_name => 'PAYROLL', waits => TRUE,  binds => FALSE,  
       instance_name => 'inst1'); 
The SERV_MOD_ACT_TRACE_DISABLE procedure disables the trace globally at all enabled
instances for a given combination of service name, module, and action name. For example,
the following disables tracing for the first example in this section:
EXECUTE DBMS_MONITOR.SERV_MOD_ACT_TRACE_DISABLE(service_name => 'ACCTG', 
       instance_name => 'inst1'); 
This example disables tracing for the second example in this section:
EXECUTE DBMS_MONITOR.SERV_MOD_ACT_TRACE_DISABLE(service_name => 'ACCTG',  
       module_name => 'PAYROLL', instance_name => 'inst1'); 
Tracing for session
The SESSION_TRACE_ENABLE procedure enables the trace for a given database session
identifier (SID) on the local instance.
For example:
EXECUTE DBMS_MONITOR.SESSION_TRACE_ENABLE(session_id => 27, serial_num => 60, 
       waits => TRUE, binds => FALSE); 
The SESSION_TRACE_DISABLE procedure disables the trace for a given database session
identifier (SID) and serial number. For example:
EXECUTE DBMS_MONITOR.SESSION_TRACE_DISABLE(session_id => 27, serial_num => 60); 
Tracing for the entire instance or database
The DATABASE_TRACE_ENABLE procedure enables SQL tracing for a given instance or an entire
database. For example:
EXECUTE DBMS_MONITOR.DATABASE_TRACE_ENABLE (waits => TRUE, binds => FALSE, 
instance_name => 'inst1'); 

The DATABASE_TRACE_DISABLE procedure disables the tracing for an entire instance or
database. For example:
EXECUTE DBMS_MONITOR.DATABASE_TRACE_DISABLE(instance_name => 'inst1'); 
Using the trcsess and tkprof utility
The trcsess utility consolidates the trace output from selected trace files based on several
criteria:
Session ID
Client ID
Service name
Action name
Module name
After trcsess merges the trace information into a single output file, the output file can be
processed by TKPROF.
trcsess has the following format and can be invoked with any of the following options to
consolidate the trace files:
trcsess [output=output_file_name]
[session=session_id]
[clientid=client_id]
[service=service_name]
[action=action_name]
[module=module_name]
[trace_files]
For example, to consolidate all the trace files of a specific session we can use following.
Session is a combination of SID and SERIAL#:
trcsess session=2324.68784 output=session_2324.trc
TKPROF has the following format and is used to format the trace file:
tkprof filename1 filename2 [waits=yes|no] [sort=option] [print=n]
[aggregate=yes|no] [insert=filename3] [sys=yes|no] [table=schema.table]
[explain=user/password] [record=filename4] [width=n]
Oracle writes the trace files in its own format which is difficult to read. You can run the
TKPROF program to format the contents of the trace file and place the output into a readable
output file. TKPROF can also do the following:
Create a SQL script that stores the statistics in the database
Determine the execution plans of SQL statements
TKPROF reports each statement, along with much more detailed information about the number
of times the SQL is executed during the time of monitoring and how many resources that SQL

has consumed. It also provides details about how many times the query is parsed and executed.
Based on this information, it's easy to identify the top query in the traced module and take
further action to fix it.
TKPROF lists the statistics for an SQL statement returned by the SQL trace facility in rows and
columns:
PARSE: Oracle parses the SQL statement to prepare the execution plan. It checks for
security and authorization. It also verifies whether the table, columns, and other
referenced objects are present.
EXECUTE: The execution plan prepared during the parse phase is executed. DML
statements such as INSERT, DELETE, and UPDATE modify the data and SELECT identifies the
data for the SELECT.
FETCH: Fetches the rows selected by a DML.
The following example uses TKPROF to convert a trace file into text file. You can use multiple
options with TKPROF.
tkprof session_2324.trc session_2324.txt

Clustering factor
The clustering factor of an index measures the row order in the index in relation to the row
order in the table. If rows are stored nearly in the same order as the index and table, the
clustering factor is good. If not, then the clustering factor is bad.
The clustering factor impacts the performance of the SQL when it comes to reading the data
from the index and then from the table. So, if we use a where clause containing the column
(with range predicates) that has an index on it, the optimizer will run a range scan of that
index.
In this case, Oracle will read that column and table row ID value from an index (essentially a
block) and then read the row ID (as returned by the index) from the table. If we have a bad
clustering factor, then reading the next row from the index block will require us to fetch
another block of the table from the disk. This is because the order of rows in the index and
table is different, and so all rows in the index block might be residing in different blocks of
the table.
The Oracle database does more I/O during the last index range scan if the clustering factor is
high. This is because the index entries point to random data blocks.
The Oracle database does less I/O during the last index range scan if the clustering factor is
low. This is because the index entries point to the same data blocks.
The clustering factor is relevant for index scans because it can show the following:
Whether the database will use an index for large range scans
The degree of table organization in relation to the index key
Whether you should consider using an index-organized table, partitioning, or table
cluster if the rows must be ordered by the index key
So basically, clustering factors gives us the order of rows in a table with respect to the order
of rows in an index. The way that Oracle calculates the clustering factor is based on whether it
has to access separate blocks for retrieving records that belong to the same block in the index.
The following is a simple explanation of calculating the clustering factor:
Oracle reads the first record from the index block and the corresponding record from
the table (based on row ID).
Oracle then reads the second record from the same index block and checks the
corresponding record in the table (based on row ID). If the record belongs to a different
block than the previously accessed block, the clustering factor will be incremented by
one, otherwise the clustering factor is not incremented.
The same process is followed for each record. So, if the clustering factor is close to the
number of blocks in the table, then the index is good. But if the clustering factor is close
to the number of rows, then the order or rows in the table is very different from the
order of rows in the index. This means that, for each record, Oracle has to read a

different block, rendering the process inefficient.
The only way to improve the clustering factor is to move the table and rebuild the
indexes. Only rebuilding the indexes will not help because, in doing so, we are not
changing the row organization in the table.

SQL performance analyzer
The SQL performance analyzer enables you to assess the performance impact of any system
change resulting in changes to the SQL execution plans and performance characteristics.
Examples of common system changes for which you can use SQL Performance Analyzer
include the following:
Database upgrades
Configuration changes to the operating system, hardware, or database
Database initialization parameter changes
Schema change, for example, adding new indexes or materialized views
Gathering optimizer statistics
Validating SQL tuning actions-for example, creating SQL profiles or implementing
partitioning
You can run SQL performance analysis on a production system or a test system. However,
since we need to see the impact of the changes, it's not recommended to make those changes
to a production system as it will impact performance.
Instead, we should either test the changes on a test system using similar data or create a clone
of the production system to test the changes and see the effect of those changes on the cloned
database using the SQL performance analyzer.
The following are the steps involved in analyzing the effects of SQL changes using SQL
performance analyzer:
1. Store the SQL workload to be analyzed in an SQL tuning set.
2. Create a test system from a copy of the production environment and ship the SQL tuning
set to the test database.
3. Create an SQL performance analyzer task.
4. Execute the SQL tuning set on the test system as it is without any changes (pre-trial).
5. Make the intended/planned changes on the test system.
6. Execute the SQL tuning set again on the test system (post-trial).
7. Compare the pre-trial and post-trial performance data. This will help you identify the
SQL statements that performed better, those that remained the same, and those that
regressed after the system change.
8. Find a way to tune the SQLs that regressed.
9. Repeat steps from 6 to 8 until the performance goal is met.

Running the SQL performance analyzer
After building the SQL tuning set on the system, you can use it as an input source to run the
SQL performance analyzer.
To run the SQL performance analyzer, go through the following steps:
1. Creating a SQL performance analyzer task: Create this using the
DBMS_SQLPA.CREATE_ANALYSIS_TASK function. Before creating the task, ensure that the
SQL workload that you are using for the performance analysis is available in the form of
an SQL tuning set on the system. For example:
VARIABLE t_name VARCHAR2(100); 
EXEC :t_name := DBMS_SQLPA.CREATE_ANALYSIS_TASK(sqlset_name => 
'before_change', - 
       task_name => 'my_spa_task'); 
2. Building the pre-upgrade SQL trial: After the SQL performance analyzer task is
created on the system, you need to call the EXECUTE_ANALYSIS_TASK procedure to take the
execution plans and runtime statistics in the SQL tuning set and use them to build a pre-
upgrade SQL trial. For example:
EXEC DBMS_SQLPA.EXECUTE_ANALYSIS_TASK(task_name => 'my_spa_task', - 
       execution_type => 'CONVERT SQLSET', -  
       execution_name => 'before_change'); 
3. Building the post-upgrade SQL trial: Once the pre-upgrade SQL trial is built, you can
make changes to the test systems (maybe an upgrade), and then you need to run an SQL
performance analyzer task to perform a test execute or explain plan of SQL statements in
the SQL tuning set on the changed system.
EXEC DBMS_SQLPA.EXECUTE_ANALYSIS_TASK(task_name => 'my_spa_task', - 
       execution_type => 'CONVERT SQLSET', -  
       execution_name => 'after_change'); 

Comparing SQL performance
After the pre-upgrade and post-upgrade SQL trials are built, you can compare the pre-
upgrade version of performance data (from the production system) to the post-upgrade
version (from the test system) by calling the DBMS_SQLPA.EXECUTE_ANALYSIS_TASK procedure
or function to run a comparison analysis. Afterwards, the SQL performance analyzer can
generate a report that shows the results of the comparison and then interpret the results.
To compare the pre-change and post-change SQL performance data, you can use the
EXECUTE_ANALYSIS_TASK procedure, as shown in the following example:
EXEC DBMS_SQLPA.EXECUTE_ANALYSIS_TASK(task_name => 'my_spa_task', -  
      execution_type => 'COMPARE PERFORMANCE', - 
      execution_name => 'my_exec_compare', - 
      execution_params => dbms_advisor.arglist(- 
                          'comparison_metric', 'buffer_gets', 
                          'execution_name1', 'before_change', 
                          'execution_name2', 'after_change')) 
You can call the DBMS_SQLPA.REPORT_ANALYSIS_TASK function to generate a report as shown
in the following example:
VAR rep   CLOB; 
EXEC :rep := DBMS_SQLPA.REPORT_ANALYSIS_TASK('my_spa_task', - 
               'text', 'typical', 'summary', NULL, 100, 'my_exec_compare'); 
SET LONG 100000 LONGCHUNKSIZE 100000 LINESIZE 130 
PRINT :rep 

Cardinality feedback
Oracle introduced cardinality feedback as a new feature in Oracle 11g release 2. Cardinality
feedback helps the optimizer to generate a better plan in cases where the optimizer mis-
estimates the cardinality of predicates. This can happen whenever we have missing or stale
statistics, or if we are using complex predicates in a query, or where getting correct estimates
is not possible.
During the first execution of an SQL statement, the optimizer generates cardinality estimates
for various steps and also checks the quality of the estimates. If the quality of the cardinality
estimates is low because of bad statistics or complex predicates, it notes down such cardinality
estimates and starts cardinality monitoring for the cursor.
If cardinality monitoring is enabled for the cursor then, at the end of execution, the estimated
cardinality is compared against the actual cardinalities, and, if the difference is high, the
optimizer notes down the actual cardinality seen during the execution.
During the next execution, the optimizer will parse the statement again, but this time, instead
of carrying out the estimates again, the optimizer will use the previously stored actual
cardinality values. Since these values are accurate, the optimizer will be able to generate a
better plan for the second cursor.
Whenever a cardinality feedback is used by the SQL, we can see in the notes section -
cardinality feedback used for this statement. You can also determine this by checking the
USE_FEEDBACK_STATS column in V$SQL_SHARED_CURSOR.
Once cardinality estimates are stored during first-time parsing, they are not generated again.
Cardinality feedback from the first parsed cursor is used for subsequent executions. This is
because the objective of cardinality feedback is to provide plan stability, and, once it provides
the feedback and generates a better plan, it continues to use the same plan. There is no benefit
to continuous monitoring.
It's possible that after the first executions, when cardinality feedback is used to generate a
better plan for the second execution, there could still be more cardinality estimates that are
wrong. In that case, the optimizer may choose to get the actual cardinality from the second or
third execution and try to create better plans. But this situation is rare and depends on the
complexity of the plan. Eventually the plans gets settled down in a few executions.
Cardinality feedback monitoring is enabled for the following cases:
Object with missing or no statistics
Complex predicates, where it's difficult to correctly estimates the statistics
Multiple conjunctive or disjunctive filter predicates
The optimizer has other ways to get better cardinality estimates, such as by using dynamic

sampling or using extended statistics on multiple columns. In such cases, the optimizer does
not use cardinality feedback.
The improved estimates used by cardinality feedback are not persisted. For this reason, it's
always preferable to use other techniques, such as extended statistics, dynamic sampling, or
SQL profiles, to get cardinality estimates right the first time every time.

Adaptive cursor sharing
Adaptive cursor sharing makes a statement with bind variables use as many execution plans as
possible. For each execution or bind variable value, the cursor adapts its behavior so the
database does not always use the same plan, hence the name adaptive cursor sharing.

The purpose of adaptive cursor sharing
The problem with bind peeking is that the optimizer peeks at a bind value only once when it
does the parsing of an SQL statement during the first execution. During bind peeking, the
optimizer will check the actual values that are used by bind variables, and since it knows the
literal values being used, it will be able to generate a better plan. However, say that we have
skewed data for a column where a bind variable is being used. If bind peeking is enabled, the
optimizer will peek at bind variables to find a literal value, and since this value is skewed, the
optimizer will generate a plan based on that value. Next time the query runs and we pass
values for bind variables, the optimizer will not parse the statement and instead will go for a
soft parse. So no peeking will happen, and instead it will use the same plan that it generated in
the first parsing. Now, this plan could be bad because the value we pass the second time may
be different, and, since the data is skewed for the column, the cardinality for that value will be
different and the plan may turn out to be bad for a different set of bind values.
Adaptive cursor sharing tries to answer this problem. In adaptive cursor sharing, the database
monitors the data accessed over time for different bind values, ensuring the optimal choice of
cursor for a specific bind value. If different values of bind variables are causing the plans to
be sub-optimal, the optimizer marks the cursor as bind sensitive, meaning that with changing
values of bind variables, the plan for the query needs to change.

Bind-sensitive cursors
In a bind-sensitive cursor, the optimal execution plan depends on the value of the bind
variable. The database examines the value the bind variable gets while computing cardinality
and considers the query sensitive to plan changes based on different bind values. The database
watches the behavior of the bind-sensitive cursors to assess whether a different plan is
beneficial for different bind values.
The optimizer records the execution statistics for the new value of each execution of the query
and then compares these with the execution statistics for the previous value. If the statistics
differ greatly, then the database marks the cursor bind-aware.

Bind-aware cursors
A bind-aware cursor is a bind-sensitive cursor, and the optimizer considers a different
execution plan for different bind values. Once the cursor is bind-aware, the plans for future
executions will be based on the bind values and cardinality calculations. Therefore, with bind-
aware cursors, the best plan for the current bind value is chosen.
To summarize, the optimizer uses an internal algorithm when an SQL with a bind-sensitive
cursor is executed to check whether to mark the cursor bind-aware. The decision is based on
the results. These results might differ significantly for the different bind values, which can
result in unwanted performance costs. Once the cursor is marked as bind-aware, the optimizer
will do the following in the next execution:
Generates a new plan based on the bind value used.
Mark the original cursor as non-sharable (V$SQL.IS_SHAREABLE is N). This is because the
performance of the query changes with bind values, and the query is bind-sensitive and bind-
aware, whereas the original cursor was not marked by either of those. So Oracle prevents
anyone from using that cursor by marking it as non-sharable, and, eventually, the cursor will
age out of the database.
When the same SQL runs repeatedly with different values for bind variables, the optimizer
categorizes bind values in multiple buckets, with each bucket having a range. The optimizer
does not create a separate cursor and plan for each bind value. Rather, these bind values are
categorized into buckets depending on whether those bind values will give sufficiently
different data volumes and whether or not the existing plan is sufficient. So practically, we can
see three or four cursors with different plans, depending on the different combination of bind
values, and each combination of values will generate a different volume of data output.

Adaptive cursor sharing views
You can use the following views of adaptive cursor sharing to get more information about the
cursors and bind variable buckets and the different plans being used:
V$SQL: This is a very general view used for checking all the SQLs in the database. We can
check the columns is_bind_sensitive and is_bind_aware.
V$SQL_CS_HISTOGRAM: Shows the distribution of execution across the small, medium, and
large values buckets. The SQL execution is distributed in these buckets based on the value
of the rows_processed column.
V$SQL_CS_SELECTIVITY: This view shows the selectivity range of bind variables. Each
predicate's selectivity must fall within a low or high value in order for the cursor to be
sharable.
V$SQL_CS_STATISTICS: This view contains the statistics used by the adaptive cursor
sharing monitoring function. These statistics are used by the optimizer to determine
whether the cursor should be marked as bind-sensitive or bind-aware. For a few
executions, the optimizer will track the rows processed, CPU time, and so on.

Implement application tuning
This section provides some of the best practices to be followed for applications running on
the Oracle database. For best performance, applications running on an Oracle database should
follow some standards to reduce processing on the database and return the results efficiently.
The following are some tips that you might find helpful:
Limit the number of connections to the database: Establishing a database connection
is an expensive operation. For every connection, the Oracle database has to spawn a new
process in the OS and link all libraries. Applications should avoid continually creating
and releasing database connections. Applications should instead maintain a connection
pool, which is a set of persistent connections maintained with the database at all times.
Whenever the application gets a request from a client, it should use one of the existing
persistent connections to process the request on the database. Upon completion of the
request, the application should not disconnect from the database, instead maintaining an
idle connection. The minimum and maximum number of connections to maintain the
connection pool should be configured correctly using C3P0 parameters.
Reducing hard parsing: Whenever a new SQL is run in a database, the SQL goes for
hard parsing, whereby Oracle does a syntax check, semantic analysis, and optimization
(query transformation and permutation) of plans, and a row source is generated. These
steps are very expensive, so if every SQL has to be hard parsed before execution, then
the performance of the database will degrade. The code for the application should be
written so that it minimizes the parsing effort. A repeatedly executed SQL statement has
to be given a persistent handle (cursor) in order to ensure that it is parsed only once. The
use of bind variables is recommended over the literals. When rewriting the application to
use bind variables is impractical, use the CURSOR_SHARING initialization parameter to
eliminate some parsing overhead. The usage of stored procedures as they are stored in
parsed form can reduce the runtime parsing.
Whenever possible, process multiple rows at a time: An SQL statement that processes
multiple rows and performs multiple operations offers a better performance. Thus,
fetching, processing, and writing rows in bulk is faster than doing it row by row. Set-
oriented processing is highly optimized and parallelized in the database server, and it
requires only one network round trip.
Making sure correct indexes are getting used: Using an index to select, update, or
delete a few rows in a big table is orders of magnitude faster than using a table scan.
Making sure we have the right indexes on the table is very important. To analyze which
indexes are required, the DBA can use an SQL access advisor to analyze the workload
and Oracle will automatically suggest correct indexes for the queries. The DBA can also
analyze the performance of indexes on existing queries by making indexes initially
invisible and using the OPTIMIZER_USE_INVISIBLE_INDEXES parameter to test index
performance in a session. Invisible indexes are maintained by Oracle just like normal
indexes. Whenever DML happens on a table, the corresponding indexes (including
invisible indexes) are also updated. Once we are satisfied with the index performance, the

DBA can set the index as VISIBLE using the ALTER INDEX statement.
Test application before production implementation: Test your application completely
on a test environment before going in production. Benchmark your application
performance at periodic intervals, and at specific milestones such as database upgrades,
major version release uptakes, and so on. First perform single-user testing, when you are
satisfied of single-user testing, then move to multi-user testing, examine results with the
reference data.
Monitor SQL statements by querying V$SQL_MONITOR and
V$SQL_PLAN_MONITOR views: Examine AWR and ADDM reports for the
performance metrics.
Tune SQLs with high execution: Sometimes we can get huge performance benefit by
tuning small SQLs that have very low buffer_gets/executions or
disk_reads/executions, but a very high execution count per sec. In such SQLs, having
the benefit of fewer buffer_gets/exec or fewer disk_reads/exec will also prove to be a
huge performance benefit over all. Care should also be taken while tuning such SQLs: If
the wrong plan is selected, then the performance implication will be huge and can make
the entire database unusable.
Use correct data types: Use correct data types to facilitate better optimizer
performance. Do not use strings to store dates or numbers to avoid unnecessary database
conversions. A type of conversion function on the indexed column can cause the
optimizer to not use the index. A better way is to use the type conversion functions on
values that are compared to a column.
Define constraints on columns: Having constraints on columns improves the
performance of SQLs as data in the column is trusted and the optimizer will uses these
constraints to build a better query plan. For example, having NOT NULL improves the
cardinality numbers for queries.
Use static SQLs whenever possible: There are many advantages of using static over
dynamic SQLs in PL/SQL. Specifically, SQL statements that are fixed at compile time are
usually executed faster, and the principle parse once, execute many is automatically
applied to them. There are certain tools, such as Hyperion, that generate dynamic SQL as
the user chooses the different options on the screen. These tools, while very flexible and
easy to implement, create a bad impact on the database. Care should be taken not to have
too many SQLs generated in order to avoid causing database performance issues.

Summary
In this chapter, we saw how we can design the database for optimal performance; specifically,
we covered designing the database for performance and scalability. We also saw how we can
design a scalable database with good performance by taking into account the various good
practices, such as using bind variables in SQL, designing a good data model, using resource
manager to manage resources in the database, and other techniques. We also saw various
points that we should consider when designing the database for availability. We then looked
into the ways of monitoring the performance of the database and what tools we should be
using to diagnose the performance issues. We also briefly covered managing the SGA and
PGA memory. Finally, we talked about adaptive cursor sharing and how that benefits us in
resolving some of the practical problems of skewed data. We concluded the chapter by
providing a few tips for application tuning.
In the next chapter, we are going to cover storage, paying particular attention to administering
Oracle ASM. We are also going to look into the physical and logical structure of the database,
how we can manage it, and the requirements of managing a very large database (VLDB).

Chapter 12. Storage
Storage forms the most important layer for a database system, and it's equally import, for a
database administrator to understand how the database stores its files and data. In this chapter,
we are going to understand how to manage database structures, and to know how storage
works specifically, we are going to learn more about Automatic Storage Management
(ASM).
This is the third chapter of Section 2 of this book and in this chapter, we are going to cover
database storage in general. Before we begin, it is important to understand the scope of this
chapter and to set right expectations. In this chapter, we are going to cover the topics which
are mentioned in Storage topic under section 2 of, 1Z0-060 exam course. We are NOT
covering database storage end-to-end as that will make this chapter very big and topics will
not be aligned as per exam course. In order to keep this chapter in line with exam course, we
are covering only the required topics of database storage. The following topics are covered
in this chapter:
Managing database structures
Administering ASM
Managing ASM disks and disk groups
Managing ASM instance
Managing VLDB
Implementing space management

Managing database structures
An Oracle database is made up of physical and logical structures. A physical structure consists
of entities that are present at the filesystem level such as data files, redo log file, and so on.
Logical structures are created inside the database and are recognized by the database. These
structures are not known to filesystems, for example, segments, tablespaces, database blocks,
and so on. Only the database can access the segment separately from a data file.
The following figure shows the relationships between logical and physical structures. S1, S2,
S3, S4, and so on, represent the segments within a tablespace:
Let's look into the details of the following topics:
Physical storage structure
Logical storage structure
Undo size advisor
Segment space advisor

Physical storage structure
The following files constitute the physical storage structure:
Control files
Online redo log files
Archived redo log files
Data files
Undo files
Tablespaces
Other storage structure
Most of these files are very basic and form the core part of the database. DBAs are all aware
of these files, so I am going to provide only a brief introduction of these files:
Control file: This is the main file that has all the information about the structure of the
database. It contains the information about all datafiles, their checkpoint SCN numbers
and how much recovery is required, online redo log sequence number, and so on. If this
file fails, the database will crash. You should always multiplex the control file as it holds
very important metadata information.
Online redo log files: Online redo logs store the information related to the changes that
are being made in the database. This information is called change vector. Change vector
is a small amount of information that gets generated for every block that is being
changed in the database. It contains the value being changed. If a failure requires a data
file to be restored from backup, then the recent data changes that are missing from the
restored data file can be generated from online redo logs. Online redo logs are written in
sequential order. New change details are just appended to the online redo log files. The
database has multiple online redo log files and each file should be multiplexed in order
to protect the file from losing data. Each file and its multiplex copy together form a
group. The Oracle database has multiple groups configured. The LGWR background
process is responsible for writing the change vector from the memory buffer to online
redo logs. Whenever an online redo log file from a group is full, Oracle starts writing
the change vectors to the second group.
Archived redo log files: You can enable the archive log for the database and if you do
so, the online redo log files are copied as an archived log. So, archived logs are
basically just copy of online redo logs. As we know we have a limited number of online
redo logs, the database will overwrite online redo logs once it completes the writing to
all. In order to preserve the changes to make the database recoverable at any point, we
can configure archive logging for the database. This way, the Oracle database maintains
the copy of online redo logs as archived logs and we can recover the database until the
current time.
Data files: Data files are created at OS mount points and they store the user data. Data is
written to these files in Oracle proprietary format, which can only be ready by Oracle
background processes. Apart from this storage structure, we have a few other files such
as:

Password file: The password file stores the password for privileged users having
SYSDBA or SYSOPER privileges. When these privileged users connect remotely,
Oracle verifies the password using the password file. You also need to set the
REMOTE_LOGIN_PASSWORDFILE parameter to exclusive.
Initialization parameter file: This file contains various parameters that we want to
set to configure our database. We have already seen many memory parameters in
Chapter 10, Core Administration. We have a dynamic parameter file called spfile,
which allows you to change some parameters (not all) using the ALTER SYSTEM
statement without bouncing the database.

Logical storage structure
The Oracle database allocates logical space for all the data in the database. The logical units
of database space allocation are data blocks, extents, segments, and tablespaces:
Data block: At the finest level of granularity, the Oracle Database stores data in data
blocks. One logical data block corresponds to a specific number of bytes of physical
disk space. In Oracle, we can configure the database block size to 2KB, 4KB, 8KB, 16KB,
and 32KB.
Extent: An extent is a continuous set of data blocks. The size of the extent is given in KB
or MB, and the number of data blocks is decided to round off the size to the nearest
value.
Let's check the storage parameters for extents:
Storage parameters for extents: A segment has various storage parameters that can be
assigned while creating a segment. Almost all of these storage parameters have default
values, if you don't specify them.
If you don't specify the storage parameters at, segment level, Oracle will use the storage
parameters at, tablespace level and if the storage parameters are not specified at,
tablespace level, Oracle will use the default storage parameter at database level. A locally
managed tablespace can have uniform extent allocation or variable extent allocation.
Uniform extent allocation will assign all extents of the same size. You can specify the

uniform size of the extent when you create a segment or you can use the default of 1 MB.
The following example shows uniform extent allocation:
      CREATE TABLESPACE user_tbs DATAFILE 
      /u01/oradata/user_tbs_01.dbf' SIZE 100M EXTENT MANAGEMENT LOCAL
      UNIFORM SIZE 1M;
With variable extent allocation, you can specify initial extent size, next extent size that
will be allocated, and percentage increase in extent size with every extent allocation. So
as more extents get allocated, extent size becomes bigger and bigger. For automatically
allocated extents, Oracle database determines the optimal size of additional extents.
Following example shows variable size extent allocation:
      CREATE TABLESPACE user_tbs DATAFILE
      /u01/oradata/user_tbs_01.dbf' SIZE 100M
      DEFAULT STORAGE (INITIAL 10M NEXT 1M PCTINCREASE 10);
We have covered locally managed tablespace and dictionary managed tablespace at the
end of the chapter.
Segment: Multiple extents form a segment. Extents do not have to be contiguous to form
a segment. Extent can be spread across different data files within a tablespace. If all
extents of segment are full, a new extent is allocated. The following are the different
types of segments:
Permanent segment: These are used for storing the user data. Data in permanent
tablespace persists unless the user wants to delete the data manually. Permanent segments
can be created only in permanent tablespaces and they consist of a set of extents. For
example, when we create a segment, initially, a few extents are allocated to the segment.
Once all the extents are full, Oracle assigns more extents to the segment. There are
different types of permanent segments such as - table, indexes, tablespace partitions,
LOB, and so on.
Temporary segments: These are mainly used for performing sorts or joins or merging
bitmaps. These segments are used to store intermittent results if the process runs out of
memory. Temporary segments are also used during index creation and rebuild. Oracle
stores temporary intermittent index segments into temp segments while preforming the
creation or rebuild. This happens if the index is big enough and allocated memory is not
sufficient. Also, temporary segments have to be created in only temporary tablespace. We
cannot create temporary segments in permanent tablespace.
Undo segments: These are used to store the before image of data during the transaction.
Undo segments are created in the undo tablespace. From Oracle 9i onwards, we have
automatic UNDO management, where Oracle will automatically create undo segments in
undo tablespace. Whenever a user makes changes to the data, Oracle stores the previous
values of the data into undo tablespace. This is required in case users want to perform the

rollback or if the database crashes before the user commits the data. Undo data is also
used for giving read consistency to other users. For example, if a user has made the
changes but has not committed the change, other users should not see the changed
database. In such a situation, Oracle uses undo data to generate original data to show it to
other users. Oracle Database stores undo data inside the database rather than in external
logs. Oracle creates undo segments inside undo tablespace and assigns those segments
for transactions. These undo segments are similar to other segments in the database.
Changes to blocks of undo segments generates redo data and it is stored in online redo
logs. In this way, Oracle Database can efficiently access undo data without needing to
read external logs.
Tablespaces: A database is logically divided into tablespaces. A tablespace logically
consists of segments and physically it's made of datafiles. Whenever we create a
segment, we can assign a tablespace to that segment. If we don't specify the tablespace
name, the default tablespace assigned to the user who is creating the segment will be
used. Physically, a tablespace contains multiple datafiles and a segment that we create in
the tablespace can have multiple extents coming from multiple datafiles. So a segment is
spread across multiple datafiles.
By default, when we create a database, it creates the following mandatory tablespaces -
SYSTEM, SYSAUX, UNDO, and so on.
Tablespaces have different properties:
Space management: We can have a locally managed tablespace or a dictionary managed
tablespace. In case of a locally managed tablespace, Oracle keeps track of space
allocation to the tablespace locally within the tablespace header, whereas in case of a
dictionary managed tablespace, the same information is kept in the data dictionary table.
Locally managed tablespaces work better because Oracle finds the information about
used and unused space of tablespaces within the tablespace header as a bitmap structure.
So it's easy to just read the header and allocate the space within the tablespace. With a
dictionary managed tablespace, Oracle has to check with the data dictionary tables and
with many datafiles in the database; this can cause contention.
Permanent tablespace versus temporary tablespace: All the user data is stored in a
permanent tablespace and the data is preserved permanently unless the user wants to
delete the data. Temporary tablespace is used for sorting activity and for storing temp
data (for example, during index rebuild). Segments in a temporary tablespace are
released once the process is complete.
UNDO tablespace: Undo holds the previous image of the changes being done. This is
required for rollback and recovery. In case of automatic undo management, Oracle uses
an undo tablespace to create undo segments and assign them to different sessions doing
the transactions. Undo tablespaces cannot contain any persistent user data. Once the
transaction is complete, undo data gets discarded once it reaches the retention period.

Autoextend tablespace: You can make a tablespace extend automatically once the space
becomes full. You can provide the steps in which it should autoextend, and the max size
until which it continues to extend. If you do not enable autoextend and the tablespace
becomes full, you have to manually add space to the tablespace to further insert data to
existing segments or to create a new segment.

Undo size advisor
Undo advisor is very useful in deciding the size of the undo tablespace required, which will
have configured retention of undo data. For example, if you set undo retention to four hours
and you need to decide the size of the undo that will allow four hours' retention of undo data,
you can use undo advisor. You can access undo advisor either using enterprise manager or
using a PLSQL interface.
Undo advisor relies on the past data in Automatic Workload Repository (AWR). It is
therefore important that AWR has good workload statistics for a database.
Undo advisor provides its analysis based on the following two values:
Time for your longest running query: This is the longest query that you want to support
in a database. So if a query takes one hour to execute, that is your longest query time.
Longest interval required for Oracle flashback operation: If your requirement is able
to flash back 24 hours, your longest interval required for flashback is 24 hours.
Undo advisor takes the max of these two values and uses that as input for analyzing the undo
tablespace size.
As a best practice, Oracle advises the use of fixed sized data files for your undo tablespace.
The undo advisor a PL/SQL interface
You can activate the Undo advisor by creating an undo advisor task through the advisor
framework. The following example creates an undo advisor task to evaluate the UNDO
tablespace:
DECLARE 
  tid    NUMBER; 
  tname  VARCHAR2(30); 
  oid    NUMBER; 
BEGIN 
  DBMS_ADVISOR.CREATE_TASK('Undo Advisor', tid, tname, 'Undo Advisor Task'); 
  DBMS_ADVISOR.CREATE_OBJECT(tname, 'UNDO_TBS', null, null, null, 'null', 
oid); 
  DBMS_ADVISOR.SET_TASK_PARAMETER(tname, 'TARGET_OBJECTS', oid); 
  DBMS_ADVISOR.SET_TASK_PARAMETER(tname, 'START_SNAPSHOT', 183382); 
  DBMS_ADVISOR.SET_TASK_PARAMETER(tname, 'END_SNAPSHOT', 183476); 
  DBMS_ADVISOR.SET_TASK_PARAMETER(tname, 'INSTANCE', 1); 
  DBMS_ADVISOR.execute_task(tname); 
  DBMS_OUTPUT.PUT_LINE(tid); 
  DBMS_OUTPUT.PUT_LINE(tname); 
END; 
/ 
183382 and 183476 are the snap ID and end snap ID. UNDO_TBS is the object type.
You can view the output and recommendations in the Automatic Database Diagnostic Monitor

in Enterprise Manager or in the DBA_ADVISOR_* data dictionary views
(DBA_ADVISOR_TASKS, DBA_ADVISOR_OBJECTS, DBA_ADVISOR_FINDINGS,
DBA_ADVISOR_RECOMMENDATIONS, and so on).

Administering ASM
Administering an ASM instance is similar to administering an Oracle Database instance, but
the process requires fewer procedures.

Administering different releases
An ASM instance in Oracle 11g supports current, as well as older, versions. This is possible
by creating a separate disk group which is specific to the version of databases that you want to
use it for. Oracle 11g ASM disk groups have two compatibility parameters
(compatible.rdbms and compatible.asm). These parameters decide the minimum supported
version for ASM. By setting the correct values of these parameters, Oracle ASM can support
combination of 10.1, 10.2 and 11.1 releases of ASM and the database instance. For
compatibility between Oracle Clusterware and ASM, the Oracle Clusterware release must be
greater than or equal to the ASM release.
When using different software versions, the database instance supports ASM functionality of
the earliest release in use. For example:
A 10.1 database instance operating with a 11.1 ASM instance supports only ASM 10.1
features
A 11.1 database instance operating with a 10.1 ASM instance supports only ASM 10.1
features
The V$ASM_CLIENT view contains the SOFTWARE_VERSION and COMPATIBLE_VERSION columns,
with information about the software version number and instance compatibility level.

Initialization parameters for an ASM instance
An ASM instance comes with very few initialization parameters. Most of the parameters in
ASM have a default value, and it is good in most situations.
When installing ASM for a single-instance Oracle Database, DBCA creates a separate server
parameter file (spfile) and password file for the ASM instance. In case of a clustered
environment, where ASM home can be shared, Oracle creates an spfile for ASM. With Oracle
11g R2, we have a grid infrastructure home and the ASM home is part of the grid
infrastructure home and spfile can be put in ASM disk groups.
Until Oracle 11g R1, if you are using shared ASM home, you can put an spfile in a shared
location or if ASM home is not shared, we need to maintain a separate spfile for each ASM
instance.
There is a limited number of initialization parameters for an ASM instance, but the
INSTANCE_TYPE initialization parameter is the only required parameter in the ASM instance
parameter file. ASM comes with automatic memory management, so memory required for
SGA and PGA is managed automatically. You can disable automatic memory management by
setting MEMORY_TARGET=0. Doing so, ASM will continue to use automatic shared memory
management and automatic PGA management. You can disable that too, if required and set
each memory component manually.
The minimum MEMORY_TARGET for ASM is 256 MB. If you set MEMORY_TARGET to 100 MB, then
Oracle increases the value for MEMORY_TARGET to 256 MB automatically. ASM does not need
more memory. So even if we set MEMORY_TARGET to a nominal 300MB, the ASM instance
works fine.
The following are some of the ASM parameters that we can set:
ASM_DISKGROUPS: Specifies a list of the disk group names that an ASM instance should
mount at startup. This parameter is dynamic. You don't need to set this parameter as
Oracle ASM will automatically add disk group to this parameter once we mount the disk
group for the first time.
ASM_DISKSTRING: Specifies a comma separated list of paths that Oracle ASM will check
to discover ASM disks. The value of the path can contain wildcards. Only the matching
disks are discovered and added to an ASM instance. You need to make sure that the same
disk does not exist at different paths. This could happen in case of multi pathing. In such
cases, make sure you provide only one path for disk discovery. If you do not specify any
value for this parameter, Oracle ASM will automatically check the default paths where it
has read-write access. These paths are OS specific.
ASM_POWER_LIMIT: Specifies the power of the disk rebalancing operation. The default
value is 1 and we can specify the value up to 11. A value of 0 will disable the disk
rebalancing operation. A higher value spawns more worker process and disk balancing
completes quickly.

ASM_PREFERRED_READ_FAILURE_GROUPS: This parameter is generally used for clustered
environments and it has different values in different nodes. If you have multiple instances
in a cluster running in different regions and you want our clients to read the data from a
local region, then you can set this parameter to use the failure group of a local region
node. This parameter is valid only if you are using clustered ASM instances.

Authentication for an ASM instance
An ASM instance does not have a data dictionary, so the only way to connect to an ASM
instance is by using one of three system privileges, SYSASM, SYSDBA, or SYSOPER. There are
three modes of connecting to ASM instances:
Local connection using operating system authentication
Local connection using password authentication
Remote connection by way of Oracle Net Services using password authentication
About the SYSASM Privilege
SYSASM privilege was introduced in Oracle 11g R1 as a way to manage Oracle ASM instances.
SYSASM is a system privilege just like SYSDBA, but it has privileges for administering ASM
instances only. Just like SYSDBA and SYSOPER, access to the SYSASM privilege is granted by
membership to an operating system group that is designated as the OSASM group.
When you log in as SYS as SYSASM using OS authentication, you can create a user and grant
SYSASM privilege to that user. These commands update the password file for the local ASM
instance.
Local connection using operating system authentication
Users who are part of OS groups are given privileges in a database. For ASM, we have a new
group called OSASM and any user who belongs to this OS group, will be granted SYSASM
privilege. When you install the database, Oracle asks for the names of different groups that
need to be used for OS authentication. If you have separate groups created at OS level for
different database responsibility, you need to provide those groups while installing the
database. Users belonging to the OSASM group have complete access to the ASM instance
and can perform all administrative tasks.

Oracle ASM filesystem
Let's look into the different structure of ASM filesystems including filenames, directories,
and aliases.
Filenames
Every file created in Oracle ASM gets a system-generated file name, otherwise known as a
fully qualified file name. This is similar to a complete path name in a local filesystem.
Oracle ASM generates file names according to the following scheme:
+diskGroupName/databaseName/fileType/fileTypeTag.fileNumber.incarnation 
An example of a fully qualified file name is as follows:
+data/orcl/CONTROLFILE/Current.256.541956473 
In the previous fully qualified file name, data is the disk group name, orcl is the database
name, CONTROLFILE is the file type, and so on.
Only the slash (/) is supported by ASMCMD. Filenames are not case sensitive, but are case
retentive. If you type a path name as lowercase, ASMCMD retains the lowercase.
Directories
Just like OS file system directories, Oracle has filesystem directories for ASM as well. An
ASM directory is just a container of files or other directories. We can create a hierarchy of
directories just like we see in an OS filesystem. A directory can be inside a parent directory
and it can have multiple child directories inside it. A fully qualified file name contains a
hierarchy of directories. A + sign at the beginning of a fully qualified file name represents the
root directory. These directories in ASM are created by ASM based on the template that is
used by default for each type of file. You can create your own directory structure in ASM and
copy the files to your own directory.
ASMCMD provides the interface to navigate in ASM storage. It provides many commands,
which enable a DBA to list, copy, remove, navigate, and perform many tasks. For example,
the ls command enables us to list the content of the current directory, whereas the pwd
command prints the name and complete path of the current directory.
For example:
ASMCMD> cd +data/orcl/CONTROLFILE 
ASMCMD> ls 
Current.256.541956473 
Current.257.541956475 
Aliases

Aliases are the symbolic links to the system generated file names. By default, we don't have
any aliases created in ASM, but we can create an alias to an existing system generated file.
Aliases can be created with user friendly names as opposed to system generated datafiles. This
helps with the easy referencing of files. You can create aliases using the mkalias command in
the ASMCMD interface. You can also use the ALTER DISKGROUP command to create aliases.
An alias should contain at least the disk group name as part of its path. The following are
examples of aliases:
+data/ctl1.f 
+data/orcl/ctl1.f 
+data/mydir/ctl1.f 
If you run the ASMCMD ls (list directory) with the -l flag, each alias is listed with the system-
generated file to which the alias refers:
ctl1.f => +data/orcl/CONTROLFILE/Current.256.541956473 

ASM metadata backup and restore
ASM does not have any of its own storage like database SYSTEM tablespace. But ASM does
have its metadata and it stores that metadata in the header of disk.
Suppose all the disks crash and the header information disappears. In that case it would be
difficult for us to restore the metadata. Even though we have taken the backup of the database
using RMAN and we can completely restore and recover the database, we need to first get
back our disks and disk groups. You can restore and recover the database only after we
restore and recover the disks and disk groups.
From Oracle 11g onward, we can take a backup of ASM metadata using the MD_BACKUP
command. The MD_BACKUP records all the diskgroups, disks, directories, the disk attributes,
and so on. By default, this file records all the diskgroups. If you want to backup only a
specific diskgroup, you can use the -g option. In addition, you can use the -b option to create
a specific named file:
md_backup -g dg1 -b dg1.backup 
This backup file stores all the metadata information of disk groups including, disk group
name, disk names, attributes, compatibility, directory structure, aliases, and so on.
In case a disk group is lost, you can restore the disk group using MD_RESTORE. MD_RESTORE
creates the diskgroup as well as the templates and the directories as they were when backup
was taken. You can specify the -G option with MD_RESTORE to restore the metadata of a specific
diskgroup. If there is any data in the disk group, it will be lost. This is because md_backup
backs up only the metadata and when we restore, only metadata is restored. You have to make
sure to backup the data using RMAN. Once the disk group is created along with all the
directory structure from the metadata backup, we can use RMAN to restore the datafiles:
md_restore --full -G dg1 dg1.backup 
You should take regular backup of your metadata or at least whenever any change is made at
disk group level or at disk level including creating a new directory:

Managing ASM disks and disk groups
Let's now look into the details of ASM disks and disk groups and how we can manage them.
Before we go any further, it's important to understand the various attributes that we can set for
a disk group.

Disk group attributes
Disk group attributes are parameters that are bound to a disk group, rather than an Oracle
ASM instance. Disk group attributes can be set when a disk group is created or altered. The
following are the disk group attributes:
AU_SIZE: AU_SIZE represents the allocation unit for a disk group. A disk in a disk group
is made up of extents and extents are made up of allocation units. An allocation unit is the
fundamental unit of allocation within a disk group. You can set an allocation unit only
while creating a disk group. The value for this parameter can be 1, 2, 4, 8, 16, 32, and 64
MB depending on the disk group compatibility level. Setting a larger value for an
allocation unit works better in a data warehouse environment, where you are doing large
sequential reads. The default size for this parameter is 1 MB.
COMPATIBLE.ASM: This attribute defines the minimum software of an Oracle ASM instance
that can be used with disk groups. This value determines the format of data structure for
Oracle ASM metadata on the disk. The format of file content is determined by Oracle
ASM Dynamic Volume Manager (Oracle ADVM) and the database instance.
For Oracle ASM in Oracle Database 11g, 10.1 is the default setting for the
COMPATIBLE.ASM attribute. When creating a disk group with ASMCA, the default
setting is 11.2. For example, if you create a disk group with COMPATIBLE.ASM set to 11.1,
you need to have an ASM software version of minimum 11.1 to mount that disk group.
COMPATIBLE.RDBMS: This parameter determines the minimum value of the Oracle
database version (COMPATIBLE parameter of the database) that is allowed to use the disk
group. For example, if you set the value of this parameter to 11.1, any database instance
whose version is less than 11.1 will not be able to connect to this ASM disk group. Also,
once the parameter is set, we cannot lower the value, it can only be advanced. Before
setting this value, make sure that you set the value to the lowest database version that is
allowed to use this disk group.
COMPATIBLE.ADVM: This is a new parameter introduced in Oracle 11.2 and it is related to
ASM dynamic volume manager. This parameter is required to be set before adding a
volume to the ASM disk group. The value of this parameter starts from 11.2 and goes
higher. Before setting this attribute, the COMPATIBLE.ASM value must be 11.2 or higher.
Also, the Oracle ADVM volume drivers must be loaded in the supported environment.
By default, the value of the COMPATIBLE.ADVM attribute is empty until set.
DISK_REPAIR_TIME: This attribute of ASM disk group is used to set the amount of time
the disk in that disk group should not be dropped after taking the disk offline. In old
releases, if your disk failed and you had to take the disk offline for repair, once you put
that disk back again, you had to rebuild the entire disk to restore the redundancy. This is
very time consuming, especially if you have to rebuild the entire failed group. Oracle
introduced ASM fast mirror resync in Oracle 11g to reduce the time to resynchronize a
failed disk. With fast mirror resync, when a disk fails, Oracle starts a timer and it waits
until the DISK_REPAIR_TIME before it drops the disk. During this time, Oracle ASM
tracks the changes that are supposed to happen on the offline disk. Once the disk is

inserted back, Oracle ASM will sync only the changed data during the time when the disk
was offline. This avoids doing a complete rebuilding of the disk and the process is very
fast. By default, DISK_REPAIR_TIME is set to 3.6 hours, but it can be altered anytime. The
time can be specified in units of minutes (m or M) or hours (h or H). If you omit the unit,
then the default unit is hours.
STORAGE.TYPE: STORAGE.TYPE attributes specify the type of disks in the disk group.
Allowed values are AXIOM, ZFSSA, and OTHER. If you set the STORAGE.TYPE attribute to
AXIOM or ZFSSA, you can enable hybrid columnar compression (HCC) for Pillar AXIOM
or for ZFS storage. To set this parameter, make sure that COMPATIBLE.ASM and
COMPATIBLE.RDBMS disk group attributes are set to 11.2.0.3 or higher. This attribute can be
altered after creating a disk group, as well as making sure that no clients are connected to
the disk group.

Creating disk groups
The CREATE DISKGROUP SQL statement is used to create disk groups. When creating a disk
group, you:
Assign a unique name to the disk group.
Specify the redundancy level of the disk group.
You can specify redundancy level as NORMAL REDUNDANCY (2-way mirroring) or HIGH
REDENDENCY (three-way mirroring). You can also specify EXTERNAL REDUNDENCY if you
do not want ASM to mirror the disk group contents. EXTERNAL REDENDENCY is useful if
you are using the RAID file system, and the OS file system does the work of stripping and
mirroring. Also, you cannot change the redundancy after you create a disk group.
Specify the disks that should be formatted and included in this disk group. ASM has a
parameter called ASM_DISKSTRING, where we can provide an operating system dependent
wildcard character that is the path for disk storage and ASM automatically searches for
disks in that path and adds them to the metadata. Disks that are new and available have the
status of CANDIDATE.
You can also specify a failure group when you are creating the disk group. You can
divide the disks into multiple failure groups in order to provide required redundancy. If
you do not specify a failure group, every disk is created in its own failure group.
You can also specify other optional attributes such as allocation unit, compatibility, and
so on.
Oracle ASM will automatically take the size of the disk same as the size of the device that it
sees at OS level. You can limit the size of the disk by specifying the SIZE clause for each disk.
This way you can limit the amount of disk that is used by ASM. Oracle ASM also assigns disk
names to the disks automatically, but you can specify your own name to the disk while
creating it using the NAME clause. You can see these disk names in the V$ASM_DISK view.
For example, if we have two sets of disks for controller 1 and controller 2, as shown in the
following code:
   Controller 1:
   
   /devices/diska1
   /devices/diska2
   /devices/diska3
   /devices/diska4
   
   Controller 2:
   
   /devices/diskb1
   /devices/diskb2
   /devices/diskb3
   /devices/diskb4
We can create a disk group using the following command:

   CREATE DISKGROUP data NORMAL REDUNDANCY
     FAILGROUP controller1 DISK
       '/devices/diska1' NAME diska1,
       '/devices/diska2' NAME diska2,
       '/devices/diska3' NAME diska3,
       '/devices/diska4' NAME diska4
     FAILGROUP controller2 DISK
       '/devices/diskb1' NAME diskb1,
       '/devices/diskb2' NAME diskb2,
       '/devices/diskb3' NAME diskb3,
       '/devices/diskb4' NAME diskb4
     ATTRIBUTE 'au_size'='4M',
       'compatible.asm' = '11.2', 
       'compatible.rdbms' = '11.2',
       'compatible.advm' = '11.2';
Specifying AU_SIZE
An allocation unit is similar to block size. Oracle recommends using an allocation unit size of
4MB for better performance. With a 4 MB allocation unit size, you can experience better
performance in terms of IO, reduced SGA size to manage extent map of the data files, faster
initialization as the extent map is reduced, and intern in reducing database startup time. You
also get increased file-size limit.
Once you specify an allocation unit size for the disk group, you cannot change it after the disk
group is created.

Altering disk groups
You can use the ALTER DISKGROUP statement to alter the properties of the disk group. You can
also use this to add, drop, or resize the disks in the disk group. All this can be done while the
disks are being accessed. Whenever you use the ALTER DISKGROUP statement, Oracle ASM
does rebalancing of the disks in the disk group. This is required to balance and spread the data
in different disks. That's why, whenever possible, you should combine the actions in the ALTER
DISKGROUP statement so that rebalancing runs only once as it is an expensive operation.
When you run ALTER DISKGROUP, the command completes quickly and gives you back to the
prompt, but rebalancing happens in the backend. You can check the V$ASM_OPERATION view to
check the status of the rebalancing operation. If you don't want rebalancing to run in the
backend, you can use REBALNCE WAIT in your ALTER DISKGROUP statement and that statement
will not return the prompt until rebalancing is complete.
You can also control the speed of rebalancing by setting the ASM_POWER_LIMIT parameter of
the ASM instance. If you want to increase the power only for a specific operation, you can use
the REBALANCE POWER clause in statements that add, drop, or resize disks.
For example, you can use ALTER DISKGROUP to add disks to the disk group:
   ALTER DISKGROUP data1 ADD DISK
        '/devices/diska5' NAME diska5,
        '/devices/diska6' NAME diska6,
        '/devices/diska7' NAME diska7,
        '/devices/diska8' NAME diska8;
Similarly, you can use ALTER DISKGROUP to:
Perform manual rebalancing
Resizing disks in the diskgroup
Adding/dropping disks in the diskgroup
Managing volumes in the diskgroup

Oracle ASM disk discovery
Disk discovery is a mechanism by which Oracle ASM searches for disk devices to be added to
ASM. You can set the ASM_DISKSTRING parameter to set the locations where Oracle ASM
should check for disks. The value of this parameter accepts wildcards. If you do not set this
parameter, Oracle ASM does disk discovery at default paths specific to different operating
systems.
Disk discovery also occurs when you:
Mount a disk group with the ALTER DISKGROUP ... MOUNT command
Bring a disk online using the ALTER DISKGROUP ... ONLINE DISK command
Add a disk to the disk group or resize a disk in the disk group
Perform a query on V$ASM_DISKGROUP or the V$ASM_DISK view
Once Oracle ASM successfully discovers a disk, it appears in the V$ASM_DISK view. If that disk
already belongs to a disk group, its status will be shown as MEMBER. If it's a new disk and it has
never been used before, its status will be either CANDIDATE or PROVISIONED. If a disk was used
previously with any other disk group and was later dropped cleanly from this group, its status
will be shown as FORMER.
Oracle ASM identifies the following configuration errors during discovery:
Multiple paths to the same disk.
In case of multi-pathing where we have redundant components, it's possible that different
paths can lead to the same disk. If Oracle ASM discovers the disk twice, it automatically
recognizes this and gives an error. You should make sure that your multi-pathing
configuration does not make ASM discover the same disk twice. You can do that by
adjusting the ASM_DISKSTRING parameter as well.
Multiple Oracle ASM disks with the same disk header.
If you copy one disk into another header, it also gets copied and ASM will give an error
if multiple disks have the same header.

Disk group redundancy
If you specify redundancy during the creation of the disk group, then Oracle ASM stores a
copy of extent into another failure group in the same disk group. Failure groups are created
for NORMAL and HIGH redundancy disk groups. A disk group with EXTERNAL redundancy should
use an external file system to manage mirroring (using RAID or any other means).
There are three types of disk groups based on the Oracle ASM redundancy level. The
redundancy levels are:
External redundancy: In case of external redundancy, Oracle ASM does not take care of
mirroring your data. It's assumed that the underlying file system has the necessary
capability to mirror the blocks (for example, using RAID configuration or by any other
means). But in case of external redundancy, any write error causes a forced dismount of
the disk group.
Normal redundancy: In case of NORMAL REDUNDENCY, Oracle ASM mirrors the extents in
the file. So we have two copies of every extent in a file. A loss of one ASM disk does not
cause data loss because we have a mirror copy still available. Space requirement in the
case of NORMAL REDUNDENCY is double that of the external redundancy, so we have to
make sure that we have double the space required by the database.
High redundancy: In case of HIGH redundancy, Oracle ASM makes two copies of the
original data, so we have three copies in total. Because of this, loss of disk in two
different failure groups is tolerated. If we don't have enough online failure groups to
satisfy the disk group mirroring, Oracle ASM allocates as many mirrors as possible and
it allocates the remaining required mirror once the sufficient number of failure groups
are available. This redundancy needs thrice the amount of space than data.

Oracle ASM failure groups
A failure group logically organizes the disk in the disk group as the mirror copies. For
example, if you have eight disks in the disk group with NORMAL REDUNDENCY, you can have
four disks in one failure group and other four in another failure group. Data in each failure
group will be a copy in that case. When Oracle ASM writes data in the disk group, it writes
data in the primary disk failure group and also in the mirror disk failure group. Multiple
copies of allocation units is stored in each failure group so that failure of all disks in a failure
group does not result in data loss. A failure group is basically a group of disks in a disk
group. We should group the disk logically in different failure groups based on whether or not
they are dependent on the same hardware. For example, all the disks that belong to one
controller should be on one failure group so that if the entire controller fails, we will not lose
the data. You should not share hardware components between two failure groups, or else disks
in both failure groups will fail, leading to loss of data.
You can create a failure group when you create a disk group. Manually creating a failure
group in the disk group allows you to arrange your disks as per your configuration. If you
don't specify the failure group during disk group creation, every disk is created in a different
failure group. But redundancy maintained by Oracle ASM is as per configuration only (2-way
mirroring for normal redundancy and 3-way mirroring for high redundancy). Oracle
recommends using several failure groups. We should not use a small number of failure
groups or failure groups with an uneven number of disks as this can cause allocation
problems or we will not be able to use all the storage capacity. Make sure that each disk in the
disk group belongs to only one failure group and that all failure groups are of the same size.
For normal redundancy, we should create at least two failure groups, and for high redundancy
we should have three failure groups.

Managing an ASM instance
Managing an ASM instance involves starting and stopping the instance, performing any
maintenance tasks, and making sure that all the disk groups are available without any issues.
Let's get into the details of starting and stopping an ASM instance.

Starting up an ASM instance
You can start an ASM instance just like you start a database instance. However, there are a few
distinctions as follows:
You should set the z environment variable to ASM SID, which is +asm for a single
instance database and +asm<instance_number> for cluster ASM instances.
You should set the INSTANCE_TYPE=ASM initialization parameter in ASM spfile to tell
Oracle that this is an ASM instance and not a DB instance.
When you use the STARTUP command to start an ASM instance, it will start the instance by
allocating the memory and trying to mount the disk groups. If you have not set the
ASM_DISKGROUP parameter, it will give an error and it will not mount the disk group. You
can manually mount the disk groups using the ALTER DISKGROUP ... MOUNT command.
ASM also provides an option to mount the disk group configured with NORMAL or HIGH
redundancy in case some of the disks are not available to fulfill the redundancy
requirement. You can use the FORCE keyword to mount the disk group if a few disks are
offline and not available. However, you can use the FORCE option only when mounting
without the FORCE option has failed. This is to prevent unnecessary use of the FORCE
option.
Also, the disk group gets mounted with the FORCE option in the case of a few missing
disks, provided ASM is able to find at least one copy of all extents for all data files in the
disk group.
The following are the options of the STARTUP command to start an ASM instance:
FORCE: If you use FORCE with STARTUP, it will shut down and abort if an existing
instance is already running and it will restart the instance.
MOUNT or OPEN: Mounts the disk group mentioned in the ASM_DISKGROUP parameter. This is
the default behavior and it is not required to pass.
NOMOUNT: This only allocates the instance memory and no disk groups are mounted.
RESTRICT: This starts the instance in restricted mode and only users with RESTRICTED
SESSION privileges are allowed to connect.

Restricted mode
You can start an ASM instance in restricted mode for doing maintenance activity. While in
restricted mode, all disk groups are in restricted mode and the database instance cannot
connect to an ASM instance. In addition, the restricted clause of the ALTER SYSTEM statement is
disabled for the ASM instance. The ALTER DISKGROUP <diskgroup_name> MOUNT statement is
extended to enable ASM to mount a disk group in restricted mode. It means that you can run
the ASM instance in normal mode and have one or few ASM disk groups opened in
RESTRICTED mode. Doing so will prevent clients of only those disk groups from connecting.
In case of an ASM cluster environment, you can mount the disk group in restricted mode only
on one instance. Other instances cannot mount that disk group. RESTRICTED mode prevents
other clients from connecting to an ASM instance. This improves the performance of
rebalancing operations when it's done in RESTRICTED mode. Because no client is accessing
ASM, the rebalancing operation is fast as it eliminates the lock and unlock extent map
messaging between the ASM instance in the Oracle RAC environment. But once the
rebalancing operation is complete, you have to unmount the disk groups and mount them back
in normal mode.

Shutting down an ASM instance
The following list describes the SHUTDOWN modes and it describes the behavior of the ASM
instance in each mode:
NORMAL clause: In NORMAL shutdown mode, Oracle ASM waits for all the transactions
to complete and all the users to disconnect from the ASM instance. NORMAL shutdown
happens in the most orderly sequence. Oracle first dismounts all the disk groups and then
shuts down the ASM instance. If any database instances are connected to the ASM
instance, then the SHUTDOWN command returns an error and leaves the ASM instance
running. NORMAL is the default shutdown mode.
IMMEDIATE or TRANSACTIONAL clause: In case of an immediate or transactional
clause, the ASM instance waits for any ongoing transaction to complete before it shuts
down the instance. If any database is connected to ASM, this clause will give an error.
Shutdown happens in an orderly sequence and no recovery is required after startup.
ABORT: ASM immediately shuts down the instance without caring much about ongoing
transactions. When an ASM instance starts, it has to perform the recovery. Also, if any
database instance is connected to an ASM instance while we do ABORT, the Oracle
database also aborts.

ASM background processes
The following are some of the important background processes of ASM:
ARBn: These are the worker processes that do the actual work of extent copying during
the rebalancing operation. These background processes run in the ASM instance.
ASMB: This background process runs in the Oracle database instance that connects to the
ASM instance. This background process provides all the statistics required by the Oracle
database instance. It also runs an ASM instance whenever the ASMCMD cp command is
executed or when the database instance has to start and spfile is stored in ASM.
GMON: This background process maintains disk membership in the ASM disk group.
MARK: MARK stands for Mark Allocation Unit Resynchronization Coordinator. It
marks the ASM allocation unit as stale following a missed write to an offline disk. So
whenever a disk is taken offline for repair, during the repair time, this process keeps
track of all the allocation units that need to be resynchronized.
RBAL: This background process runs both in the Oracle database which is connecting to
ASM and an ASM instance. While running in the Oracle database, it does a global open
of ASM disks, whereas in the ASM instance, this background process coordinates the
rebalancing operation.
The preceding processes are ASM-specific only. There are other additional background
processes that are the same as that of the RDBMS instance, such as SMON, PMON, DBWR,
and so on.
Also, there are ASM slave processes that run periodically to perform a specific task. For
example, the Snnn transient slave process is responsible for performing the resync of extents
at the time that the disk is brought online.

Managing VLDB
Management storage for very large database (VLDB) includes managing the following
aspects at the storage level:
High availability
Performance and manageability

High availability
High availability can be achieved by implementing storage redundancy. In storage terms,
these are mirroring techniques. There are two options for mirroring in a database
environment:
Hardware-based mirroring
Using ASM for mirroring
Hardware-based mirroring
Most external storage devices provide support for different Redundant Array of
Inexpensive Disks (RAID) levels. The most commonly used high-availability hardware RAID
levels in VLDB environments are RAID 1, RAID 5, and RAID 10:
RAID 1: RAID 1 provides a basic mirroring of data. It needs at least two disks, and one
disk will be used as a mirror for another disk. So, the storage space available in RAID 1
mirroring is reduced to half. In case of any issues with one disk, the same data is always
available in another disk. This way RAID 1 provides fault tolerance.
RAID 5: RAID 5 does not mirror the data, but it strips the data into multiple disks. It also
calculates the parity of the stripped data and stores that parity on separate disks. That's
why RAID 5 needs at least three disks. Whenever data is read, parity is calculated and
compared with parity bits. If a disk is lost, content of the disk is created dynamically by
reading the parity from the parity disk.
RAID 10: This provides mirrors and striping. The minimum number of disks required
for RAID 10 is four. The data is mirrored and then stripped across to achieve redundancy
and performance. It's actually a combination of RAID 1 (mirroring) and RAID 0
(stripping). This also reduces the storage capacity to 50%, as we need mirroring of data.
Mirroring using ASM
ASM provides software-based mirroring in that the mirror logic is present inside the ASM
binaries. Oracle ASM provides a NORMAL redundancy mirroring option in which it
provides two-way mirroring (two copies of data) and a HIGH redundancy mirroring option
in which it provides three-way mirroring (three copies of data). If you don't want to use ASM
mirroring, Oracle ASM also offers EXTERNAL redundancy in which mirroring is handled
externally by an OS file system. We can use RAID arrays in case of EXTERNAL redundancy.
ASM mirroring requires high IO because mirroring is done based on an allocation unit and
the minimum value of an allocation unit is 1 MB. So if an allocation unit is changed, it has to
mirror the entire allocation unit.
In ASM, failure groups enable redundancy. Whenever an extent is written to one failure
group, it's copied over to another failure group until it matches the redundancy set for the
disk group. A failure group should be strategically created such that any failure of the
underlying hardware should not affect the data. For example, if we have a few disks on
controller 1 and a few disks on controller 2, we should make a different failure group for

disks on controller 1 and controller 2 so that even if a controller goes bad, we have a
complete copy, on disks, of another controller. Hardware RAID configurations typically do
not support this kind of fault tolerance.
Another advantage of ASM over RAID in terms of mirroring is that in case of ASM we have
many disk partners for a single disk. Meaning, a content of a disk is spread across multiple
disks in a failure group. So failure of one disk on mirror site does not cause reduce
redundancy for the entire content of one disk on primary. Whereas in case of RAID, we have
1-1 mapping. So if a disk fails in RAID, we lose the mirror copy of an entire one disk and a
disk for which the mirror disk has failed already, we have a chance of losing the data. This
does not happen in the case of ASM.

Performance and manageability
In order to achieve optimum performance and to improve throughput from storage devices,
multiple disks have to work in parallel to fetch the data. This can be achieved using a
technique called stripping. Stripping is slicing the data and storing each slice on a separate
disk. So when that data is requested, multiple disks work in parallel and fetch each slice to
fetch the required data. This provides good throughput and performance.
But there is a trade-off and we need to decide between seek time and accessing consecutive
blocks. Sometimes if the seek time is more, it's better not to strip the data and have it stored as
continuous blocks instead. In VLDB, usually, 1 MB strip size provides a perfect balance of IO
throughput and performance, both for OLTP and data warehouse systems. There are three
options for striping in a database environment:
Hardware-based stripping
Software-based striping using ASM
Partitioning
Hardware-based stripping
Most external storage devices provide striping capabilities. The most commonly used striping
techniques to improve storage performance are RAID 0 and RAID 5, as we have seen
previously.
Stripping using ASM
Oracle ASM strips the data across all the disks in disk group. Stripping in ASM is done based
on an allocation unit whose default value 1 MB. An allocation unit is the fundamental unit of
allocation within a disk group. A file extent consists of one or more allocation units. An
Oracle ASM file consists of one or more file extents.
You can set different values of disk group allocation using the AU_SIZE parameter when you
create a disk group. Larger AU sizes typically provide performance advantages for data
warehouse applications that use large sequential reads.
Extents consist of one or more allocation units (AU). To accommodate increasingly larger
files, Oracle ASM uses variable size extents. This is a new feature introduced in Oracle
11gR2.
In case of very large files for very large databases (VLDB), an ASM data file will have a
huge number of extents and so the amount of SGA memory required in the database for
loading that extent map will be high. Also, maintaining such a big extent map is a
performance overhead. In order to fix these issues, Oracle introduced variable-size extents. In
case of variable size extents, the size of extent grow, as your datafiles grows and so the total
number of extents in a large data file will be less. This reduces the amount of shared pool
required to store the extent mapping for files and improves performance of files created and

open operation. Initially when the file is created, the extent size used for the file is the same as
the allocation unit. When the file grows, extent size automatically increases after the
predefined size of the file. The following are the extent sizes used in case of variable size
extents:
For the first 20,000 extents used by the datafile, extent size is the same as the allocation
unit size
For the next 20,000 extents (from 20,000-39,999), the size of extents is 4*AU size
For extents above 40,000, the extent size becomes 16*AU size
Partitioning
Partitioning is one of the ways to distribute your data. You can create partitions for tables as
well as indexes. Partitioning makes segments smaller as each partition is a segment, which
makes them manageable. No changes are required on the application side after you partition
your tables and indexes, and it's completely transparent to the application.
We see improved performance of SQL queries and DMLs as they can access only the required
partitions to get the required data. We see improved performance especially in data warehouse
applications where the amount of data fetched is high. Each partition of a table or index must
have the same logical attributes, such as column names, data types, and constraints, but each
partition can have separate physical attributes such as pctfree, pctused, and tablespaces.
You can use hash partitioning or hash sub-partitioning on a column with high cardinality and
with a number of partitions equal to the power of 2. This will result in an even distribution of
data across all partitions and better IO throughput for queries.

Implementing space management
Oracle databases allocate space from tablespaces. A tablespace is a logical structure in the
database and a tablespace is made up of one or more physical datafiles. So when a segment is
stored in a tablespace, actually that segment is stored physically in one or more datafiles.
At the finest level of granularity, an Oracle database stores data in a database blocks. The size
of the block ranges from 2 KB to incrementing with power of 2. These blocks map to
multiple OS blocks when written to the datafiles physically. Oracle data blocks are the
smallest unit of storage in a database.
An extent consists of multiple continuous data blocks allocated for storing user data. Multiple
extents form a segment. But those multiple extents do not need to be continuous. Every
segment is created as a different segment and it occupies its own space. For example, the data
for the employees table is stored in its own data segment, whereas each index for employees
is stored in its own index segment.
Each segment belongs to one and only one tablespace, so all extents of segments are stored in
only one tablespace. However, they can have extents that are spread across multiple datafiles.

Logical space management
An Oracle database must have logical space management in place to track the logical space
being used by segments. Only knowing the logical space allocation, Oracle will be able to
allocate one or more extents to segments when needed. Similarly, when a segment does not
need a space, Oracle should be able to free up the extent not required by the segment.
An Oracle Database manages space within a tablespace based on the type of tablespace that
you created
In the preceding figure, we can see that a tablespace can be locally managed or dictionary
managed, and only a locally managed tablespace can have automatic segment-space
management enabled. Let's look into each of these two types of tablespaces.

Locally managed tablespaces (default)
Oracle uses bitmap in the tablespace datafile header to manage the extents for that datafile. So
in case of a locally managed tablespace, we have some space in the datafile header set aside
for the bitmap structure to manage the space. Each bit in the header corresponds to a group of
blocks that form the extent. When a space is allocated or freed in datafiles, Oracle changes the
corresponding bits in a data file header. Since every datafile has its own bitmap structure,
contention is avoided.
The following are some of the advantages of a locally managed tablespace:
Prevents recursive SQLs from updating data dictionary tables every time a space is used
or freed. This improves a lot of performance for database and prevents contention.
Tracks free space adjacent to every extent. This eliminates the need to coalesce free
extents.
Determines the size of locally managed extents automatically. Alternatively, all extents
can have the same size in a locally managed tablespace.
Automatic segment-space management
ASSM uses bitmap structure in a data file header to manage the space in a data file. ASSM
offers the following advantages:
Simplified administration: DBAs don't have to decide correct settings for storage
parameters, ASSM does it for you
Increased concurrency: Multiple transactions can search for free blocks using the
bitmap structure, thereby reducing the contention and waits
Dynamic affinity of space to instances in an Oracle Real Application Clusters (Oracle
RAC) environment
ASSM is more efficient and it is the default for permanent, locally managed tablespaces.
Manual Segment Space Management
Manual Segment Space Management (MSSM) existed before ASSM and we had to manage
the space ourselves. MSSM uses a linked list kind of structure called free list, which keeps
track of free blocks, below high water mark (HWM). High water mark is the dividing link
below which all blocks are formatted and above which all blocks are new and never been
assigned before. As the blocks get used, the database puts the block on a free list if we are
deleting the data from the block, or takes the block off free list if we are writing the data to the
block.
We also have to define parameters such as PCTFREE, PCTUSED, FREELISTS, and FREELIST GROUP
to use MSSM. The PCTUSED parameter sets the percentage of used space in the block below
which we can consider that block as a free block and add it to FREELIST. For example, if you
set PCTUSED to 40% for a segment, then the block is considered free and available for
inserting rows if the amount of used space drops below 40%. Similarly, PCTFREE is the

percentage of free space that should be left in the block for future updates. So if you set
PCTFREE to 20%, that block will be considered full and taken out of FREELISTS after it reaches
80% of its capacity.
When you insert a row into a table, the database checks for the free list of the table for the
first available block. If the free space in the block is less than PCTFREE, the block is marked as
full and is taken out of the free list. The database then checks for the next free block until it
finds one. Similarly, if you delete a row from the block and the used space in the block drops
below PCTUSED, the block is marked as free and put on the free list.
An object may have multiple free lists. In this way, multiple sessions performing DML on a
table can use different lists, which can reduce contention. Each database session uses only one
free list for the duration of its session.
As you can see, managing space manually can get complex with so many parameters to
configure. You should also avoid row chaining and migration and avoid waste of space.
Oracle recommends using ASSM all the time.
Dictionary-managed tablespaces
A dictionary-managed tablespace uses data dictionary tables to manage the space in a
tablespace. Every time an extent needs to be allocated, Oracle will insert a record in the data
dictionary table to mark the extent as allocated. Similarly, if an extent is to be marked as free,
Oracle will update the data dictionary tables to reflect this. This causes a lot of contention as
we have many segments in the database and some or the other segment always needs an extent
or free up an extent. Updating the data dictionary table for each of these extents is a recursive
job that executes in the background. Frequent use of such a recursive SQL can have a negative
impact on performance because updates to the data dictionary is serialized. Locally managed
tablespaces, which are the default, avoid this performance problem.

Space management in a data block
Let's look at how space is managed in the data block starting with data block format:
Data block format: Database blocks have an internal structure that contains a lot of
information. This helps the database to track changes to data and free space in the block.
Data block format is the same for all types of data blocks such as table, index, and so on.
The following figure shows the internal structure of a data block.
Data block overhead: This is the header of the block that contains the metadata to
manage the block. This space in the block is not available for storing any user data.
Block overhead contains the following parts:
Block header: Contains general information about the block including its physical
address, segment type, and so on. It also contains Interested Transaction List
(ITL), which is a slot used by the active session, which is carrying out the
transaction on that block. Every transaction needs to acquire ITL entry before
carrying out the transaction.
Table directory: This area contains the metadata about the tables whose rows are
present in this block. A single block can have rows from multiple tables.
Row directory: This area contains the location of rows in the data portion of the
block. Deleting the row from the block does not reclaim this space. So if the block
was initially full and "Row Directory" has used 200 bytes of space to store the row
information, even when the block becomes empty later, "Row Directory" continues
to use 200 bytes. When a new row is inserted in the block, this space gets reused.
As data gets loaded into the block, Oracle starts filling up the block from bottom to top. The
top portion of the block contains all metadata information and user data starts filling up the
space from the other end of the block. Space between metadata and user data is free and gets
used as more data gets added.

Oracle uses a storage parameter called PCTFREE. Oracle tracks the free space between user
data and metadata and makes sure that the block has at least PCTFREE percentage of space free
for future updates. In future whenever the columns of rows are updated to larger values or
when null values are changed to not-null values, this free space (equivalent to PCTFREE) gets
used. Thus, PCTFREE helps to prevent row migration by providing the previously reserved
space to the additional data coming in.
For example, let's say you have a table in which you will have occasional updates and mostly
inserts. The following figure shows one of the blocks of the table and you can see that we
have a header at the top and data gets filled up from bottom to top. We are setting PCTFREE
as 20 percent. Now as the data gets inserted into the table, causing the row data to grow
upwards toward the block header, the block header itself is expanding downwards by filling
up more metadata. Because we have set PCTFREE to 20 percent, it will not allow the entire
block to be used. When the used size of the block (user data + header metadata) reaches 80
percent of the total block size, this block is considered full and will not allow any more
inserts. The remaining 20 percent of the space is left for the data to grow as part of the update
statements (in order to prevent row migration):

Summary
In this chapter, we talked about managing physical database structures, which includes
different types of files for database and logical database structures which includes tablespace,
segment, extents, and so on. We went into the details of administering ASM instance and saw
various details of ASM. We saw different parameters for ASM instances, SYSASM privilege,
ASM files and directories, and how to use ASMCMD to navigate and perform other tasks in ASM
datafiles. We also saw how we can back up and restore of ASM metadata using the md_backup
and md_restore commands. We then looked into the details of ASM disk groups, various
attributes of disk groups, failure groups, redundancy, and so on. We also learned how to
manage ASM instances and saw various background processes for ASM instances. We then
talked about high availability and we also talked about mirroring using hardware and ASM.
We also talked about stripping using hardware RAID as well as user ASM.
Finally, we talked about space management and how dictionary-managed tablespaces and
locally managed tablespaces work, and how Automatic Segment Space Management
(ASSM) is of benefit in the case of locally managed tablespaces. We also went into the block
structure and how space is managed inside a block.
In the next chapter, we are going to look into the security of databases. Typically, we are
going to talk about authentication and authorization, privileges and roles, encryption, and
application level security.

Chapter 13. Security
Security is one of the most important areas for an Oracle DBA. Every DBA has to make sure
that databases are kept secure and all the loop holes are taken care of before opening the
database to users. Oracle database provides lots of security-related features that can be
implanted to make the database strong enough to prevent any external hacks into the database.
In this chapter, we are going to learn how we can make our database and application secure.
This is the final chapter of section 2 of this book and in this chapter we are going to cover
database security in general. Before we begin, it is important to understand the scope of this
chapter and to set right expectations. In this chapter we are going to cover the topics which are
mentioned in Security topic under section 2 of 1Z0-060 exam course. We are NOT covering
database security end-to-end as that will make this chapter very big and topics will not be
aligned as per exam course. In order to keep this chapter in line with exam course, we are
covering only the required topics of database security. Following topics are covered in this
chapter:
Develop and implement a security policy
Configure and manage auditing
Create the password file
Implement column and tablespace encryption

Develop and implement a security policy
Let's start the discussion about security in general and what it takes to make databases secure.
When it comes to Oracle databases, what are the options we have to strengthen the security of
our database? In this topic, we will go through following:
User security
Authentication and authorization
Privileges and roles
Application security
Encryption
Auditing

User security
Accessing an Oracle database needs a valid user ID and password created in the database.
Oracle allows you to secure these user IDs in a variety of ways. For each user that you create
in the database, you can assign various limitations on that user. These limitations might
include how frequently the password should be changed, how many failed login attempts are
allowed for the user, and so on. All these limitations are part of a profile, and you can assign
the profile to users in the database. These limitations have a technical term in the database
called attributes. We can modify these attributes for a profile and all the users who are
assigned this attribute will see the change. We can also create a new profile, and by default it
will have all the attributes assigned with their default values. We can change whatever
attributes we want as per our requirement and keep the default value for other attributes.
When we create a user, we need to assign a password to that user using the IDENTIFIED BY
clause, as shown in the following. This password will be used by the user when he logs in to
the database:
CREATE USER jward 
IDENTIFIED BY password 
DEFAULT TABLESPACE data_ts 
QUOTA 100M ON test_ts 
QUOTA 500K ON data_ts 
TEMPORARY TABLESPACE temp_ts 
PROFILE clerk; 
Enforcing password complexity verification
Complexity verification checks make sure that the password set for the user is complex
enough to provide reasonable protection against intruders. We can set as many password
complexity rules as per our need. This is possible by creating a function in the database which
takes the user password (during the time of creating the user) and tell us if the password
passes the complexity test or not. The Oracle database provides a sample password
verification function in the PL/SQL script UTLPWDMG.SQL. This script creates the required
password complexity function in the database. Once the function is created, we can assign that
function to one of the attributes of the user profile, so whichever user is going to have that
attribute assigned should have the password set that will pass the password complexity
function.
Assigning a default tablespace to the user
Every user should be assigned a default tablespace. This helps in organizing the schema
inside the database. If we don't specify the default tablespace, the user-created objects get
created at the database-level default tablespace. We have learned in previous chapters that we
can have a default tablespace set at the database level as well, and we can see the same in the
DATABASE_PROPERTIES view. However, it's always good practice to assign a default tablespace
to each user.

Also, if the default tablespace is not specified at database level, the SYSTEM tablespace becomes
the default tablespace. In that case, the user objects get created in the SYSTEM tablespace, which
is undesirable. For the user database, we should never have a user SYSTEM tablespace.
Tablespace quota for the user
For each user that you create, you can assign the quota to the default tablespace that is
assigned to the user. The quota sets the limit on the amount of tablespace space that a user can
use from that tablespace. So if a user is creating objects and inserting huge amounts of data,
we can restrict the amount of space that will be used by him. After exhausting the quota from
the tablespace, the user will not be able to use any further space from that tablespace even
though the tablespace will have additional space.
By default, a user has no quota on any tablespace in the database. If the user has the privilege
to create a schema object, then you must assign a quota to allow the user to create objects. You
can assign a quota to a user on multiple tablespaces apart from the default tablespace.
SQL>alter user test_user quota 100m on test_ts; 
User altered. 
In the preceding example, we are assigning a 100 MB quota to the test_ts tablespace to
test_user.
You can also grant unlimited quotas to the user on a tablespace as shown in the following:
SQL>alter user test_user quota unlimited on test_ts; 
User altered. 
Another way to grant unlimited quotas to all tablespaces is to assign the user UNLIMITED
TABLESPACE system privilege. This overrides all explicit tablespace quotas for the user. If you
later revoke the privilege, then explicit quotas will again take effect. You can grant this
privilege only to users, not to roles.
Temporary tablespace for the user
Just like default tablespace, you should also assign default temporary tablespace to the user.
Temporary tablespace is used whenever a user is running an SQL that is sorting and the PGA
memory is not sufficient to perform all the sorting in memory. In that case, Oracle uses
temporary tables to perform the sorting.
If you do not assign a temporary tablespace to the user, all the sorting happens in default
temporary tablespace at the database level, which you can see in the DATABASE_PROPERTIES
view. If you don't have default temporary tablespace set at database level, or if there is no
temporary tablespace, then the default is the SYSTEM tablespace or another permanent default
established by the system administrator. It's a bad thing to have sorting done in the SYSTEM
tablespace, so always have temporary tablespace created and assigned as default at database

level and at user level.
Specifying a profile for the user
A profile is a set of limits on database resources and password access to the database. If you
do not specify a profile, then the Oracle database assigns the user a default profile. We have
already seen the discussion of profiles in Chapter 10, Core Administration under the
Managing long idle sessions by creating appropriate user profiles section, but I am reiterating
the points that I made.
We can restrict (put a limit to) the following resources for users using profiles:
SESSIONS_PER_USER: Specifies the maximum number of sessions a user can spawn
concurrently.
CPU_PER_SESSION: Specifies the amount of CPU time (in centi sec) a session can
consume.
CPU_PER_CALL: Specifies the amount of CPU time (in centi sec) a call can consume. A
session can have multiple calls (for example, parse, fetch, execute).
CONNECT_TIME: Specifies the total elapsed time (in minutes) a session can have.
IDLE_TIME: Specifies the total idle time (in minutes) a session can have.
LOGICAL_READS_PER_SESSION: Amount of data blocks that can be read by a session in
memory and from disk.
LOGICAL_READS_PER_CALL: Amount of data blocks that can be read by a call in a session.
The following are the restrictions that can be applied to the password of the user:
FAILED_LOGIN_ATTEMPTS: Specifies the number of failed attempts that are allowed before
the account gets locked
PASSWORD_LIFE_TIME: Specifies the number of days a password can be used. After this,
you need to change the password, otherwise the account will expire.
PASSWORD_REUSE_TIME and PASSWORD_REUSE_MAX: PASSWORD_REUSE_TIME: Defines the
number of days before which you cannot use the same password. So, if you have set a
password today with the PASSWORD_LIFE_TIME of 30 days and the PASSWORD_REUSE_TIME
of 90 days, you cannot use the same password until you set a minimum of three different
passwords each time they expire. Similarly, PASSWORD_REUSE_MAX specifies the number of
password changes required before the current password can be reused.
PASSWORD_LOCK_TIME: Specifies the number of days an account will be locked after failed
login attempts lock the password. The default is one day.
PASSWORD_GRACE_TIME: Specifies the number of days that the password expires warning
will be issued before the password expires. The default is seven days.
PASSWORD_VERIFY_FUNCTION: You can make password rules more complex by specifying
the password verify function. You can create your own function or use Oracle's default
function.
For example:
CREATE PROFILE app_user LIMIT  

   SESSIONS_PER_USER          UNLIMITED  
   CPU_PER_SESSION            UNLIMITED  
   CPU_PER_CALL               3000  
   CONNECT_TIME               45  
   LOGICAL_READS_PER_SESSION  DEFAULT  
   LOGICAL_READS_PER_CALL     1000  
   PRIVATE_SGA                15k; 
 
CREATE PROFILE app_user2 LIMIT 
   FAILED_LOGIN_ATTEMPTS 5 
   PASSWORD_LIFE_TIME 60 
   PASSWORD_REUSE_TIME 60 
   PASSWORD_REUSE_MAX 5 
   PASSWORD_VERIFY_FUNCTION verify_function 
   PASSWORD_LOCK_TIME 1/24 
   PASSWORD_GRACE_TIME 10; 
Default role for the user
A role is nothing but a group of privileges. Instead of granting the same bunch of privileges
one by one to different users, we can grant those privileges once to a role and then grant that
role to multiple users. It takes less administrative effort to manage the privileges using a role
instead of assigning so many privileges directly. A user can also be set a default role. When a
user is created, its default role setting is ALL, meaning that all the roles assigned to the user
apply to the session that the user creates. You cannot assign a default role at the time of the
creation of a user, but you can alter the user and assign a default role. For example:
GRANT USER deo manager_role; 
ALTER USER advait DEFAULT ROLE manager; 
Before a role can be made the default role for a user, that user must have been already granted
the role.

Configuring authentication
Authentication means verifying the identification of a user. If a user wants to connect to the
database, then the authentication steps allow the password verification of the user in order to
validate whether a user is allowed to connect to the database. Authentication also makes this
action accountable, so if a user is authenticated, and if he performs certain actions, we will
know who has performed those actions.
Configuring password protection
The Oracle database has an inbuilt password protection functionality that prevents any other
user from decrypting the password. The following are a few important protections that Oracle
has implemented:
Password encryption: Oracle automatically and transparently encrypts the password
using an Advanced Encryption Standard (AES) before sending it across the network.
Password complexity: Oracle has an inbuilt password complexity check function that
has many complex rules. Oracle also allows you to create your own password
complexity function and assign it to user profiles.
Password resilience: If a user enters the incorrect password for set number of times (as
per user profile), the user account will be locked. Also, for each attempt to input the
password, Oracle delays the password prompt. This prevents any un-manned program
from trying to hack the password. Oracle gradually increases the time before the user
can try another password, up to a maximum of about ten seconds.
Case sensitive passwords: Oracle has introduced the parameter
sec_case_sensitive_logon, which, when set to TRUE, will make passwords case sensitive.
This is another way of protecting the password and making it more complex.
Password management policy
As seen previously, we have several password-related limits that can be assigned to a profile,
and we can assign that profile to several users. You should always set password-related limits
in the profile in order to protect your password. These password- related limits are basically
password management policies.
When you create a database using Oracle database, most of the default accounts are locked
with the password expired. If you have upgraded your database, you might have default
passwords for most of the default accounts. You should always set different passwords for
default accounts in order to make them less vulnerable.
Password policies should be set to change the passwords periodically and password
complexity rules should be implemented as part of the password management policy to make
the passwords more complex.
Database authentication

Oracle authenticates the user based on the password stored in the database. We can explicitly
limit the clients who can connect to the database by using an authentication protocol. Oracle
has the parameter SQLNET.ALLOWED_LOGON_VERSION in the sqlnet.ora file that controls the
version of the authentication protocol that is allowed for a client to connect to the database.
The default value of this is 8 and the values increase by one each time, from 8 to 9, 10,
11, and so on. These values correspond to the Oracle version, but they are not the version
number of the Oracle database-they are the version of the protocol. If you set this value to 11,
then any client using a pre-11g client version will not able to access the database.
We have two different types of authentication:
Database authentication using a password
OS authentication at group level
We are not covering database authentication using password as we have seen this in Chapter 3,
Managing CDBs and PDBs, under Connecting to CDB and Connecting to PDB section.
Operating system authentication
In the case of operating system authentication, oracle has already given the SYSDBA
privileges to users that belong to certain groups at OS level. So we create users and assign
them groups at OS level.
For example, in the case of Linux, it's customary to create an oracle user and assign it to the
"dba" group. When we install the database software, it will ask the name of the group that
should be given as SYSDBA OS authentication; at this point, we provide the dba OS group.
So, when we create the database under Oracle home, we will be able to connect to that
database using sqlplus /as sysdba without needing any password. This is possible because
of OS-level authentication; we already provided the password for connecting to the OS
account, and oracle considered that as authentication.
SQLPLUS /  
Within SQL*Plus, you enter the following:
CONNECT /  
You can also connect directly from the OS, as shown in the following:
sqlplus / as sysdba 
Because of this, there is a centralized authentication via the operating system for the Oracle
user as well as for the database user, and the Oracle database doesn't need to store any
passwords. However, usernames still have to be maintained in the database.
Auditing at OS level and database level uses the same usernames.
You can authenticate both operating system and non-operating system users in the same

system. For example:
Oracle can create users in the database using the IDENTIFIED EXTERNALLY clause of the
CREATE USER statement, and then we can set the OS_AUTHENT_PREFIX initialization
parameter. We have to make sure that we create a user that is a combination of the
OS_AUTHENT_PREFIX + OS user name, so that when you connect to the operating system
using the user, you can directly connect to the database without a password.
Oracle can authenticate non-operating system users. These are users who are assigned
passwords and authenticated by the database.
Authenticate Oracle database enterprise user security users. These users can be created
with the IDENTIFIED EXTERNALLY clause, and they can be authenticated using the Oracle
Internet Directory (OID), and so on.
However, you should be aware of the following drawbacks to using the operating system to
authenticate users:
You need to have corresponding operating system users created, which will be
authenticated externally.
Anyone who has access to a user account at OS level can get access to the database.
When we use OS-authenticated users, the distributed database environment will need
special attention as we are not going to store the password inside the database. Therefore,
we will have to make sure that we have the same users and groups at OS level on the
other hosts as well. Similarly, the OS authenticated database link can be a security issue,
and Oracle recommends that you do not use such database links.

Configuring authorization
Authorization validates the permissions once the user is authenticated. So a user may be
allowed to connect to the database, but may not be allowed to insert any data. This check is
done during the authorization step. An authorization processes can allow or limit the levels of
access and action permitted to that user.
A user privilege is a right to perform a certain task in a database. This privelege could be
running some query on a table or modifying the data of a table or executing a procedure.
The Oracle database has different types of privilege, and they are divided into two main
categories: Object privileges and system privileges.
Roles are created by users (usually administrators) to group together privileges or other
roles. Roles are mainly for administrative benefits and help in assigning privileges.
You grant roles and privileges to the user so that they can perform the desired tasks and also
prevent them from doing any undesired tasks. The excessive granting of unnecessary
privileges can compromise security. You should be very careful about granting SYSDBA,
SYSOPER or any administrative privileges.
We have talked about roles before as well, and we can assign the privileges to users directly
or we can assign the privileges to the roles and assign those roles to the users. Roles make it
easier for DBAs to manage the privileges.

Privileges and roles
As we discussed before, roles are nothing but a set of privileges. We can assign SYSTEM or
OBJECT privileges to the role and then assign that role to the user. Lets try to understand the
details of managing privileges and roles.
Managing System Privileges
System privilege is the right to perform the action on any schema object or particular type.
For example, if we grant the SELECT ANY TABLE system privilege to the user, that user will be
able to select from any table in any schema. There are over 100 distinct system privileges.
Each system privilege allows a user to perform a particular database operation or class of
database operations.
Be careful when granting system privileges as they are very powerful, and users can misuse
system privileges.
Allowing Access to Objects in the SYS Schema
Any user with SYSDBA privilege can access any table or view in a SYS schema, but we don't
necessarily have to assign a SYSDBA role to the user just to allow them access to SYS
schema objects. Oracle provides the following predefined roles that allow users to access
SYS schema objects:
SELECT_CATALOG_ROLE: Assigning this role to the user allows them to SELECT privileges
on SYS-owned objects.
EXECUTE_CATALOG_ROLE: This role grants EXECUTE privileges on packages and procedures
in the data dictionary.
DELETE_CATALOG_ROLE: This allows users to delete records from objects on which it is
allowed, for example, the audit tables like SYS.AUD$ or SYS.FGA_LOG$.
ANY privileges and PUBLIC role
As mentioned before, system privileges that use ANY keywords are very powerful privileges
as they allow a user to have privilege on an entire category of objects in the database.
For example, if a user is granted the CREATE ANY PROCEDURE privilege, that user can create a
procedure anywhere and is not restricted by a schema, so if that user creates a procedure in
one of the DBA accounts and has a few undesired statements in that procedure, then whenever
that procedure runs, it will run as that DBA user (since it's owned by that DBA account),
potentially causing undesired events.
For this reason, we should be very careful when assigning system privileges to the user.
The PUBLIC role is a special role that every database user account automatically has when the
account is created. We cannot drop this role, and we cannot see this role in the DBA_ROLES and
SESSION_ROLES views. However, if certain privileges are granted to PUBLIC, then those

privileges get granted to all the users in the database, so we should be careful when assigning
privileges to the PUBLIC role. We should avoid granting any privilege to the PUBLIC role.
Managing user roles
Managing privileges through a role is much easier than assigning direct privileges. We can
create multiple roles in a database, but each role should have a unique name, just like the user.
Also, we can assign same role to multiple users. We can assign privileges to the role at any
time, and any user granted that role will automatically get a new privilege.
Oracle database by default provides a few roles already created, but it is recommended that
you create your own role and assign that to the user. This is because the default roles provided
by Oracle can change with the release; creating our own role would reduce the impact that
this would have. DBAs usually create roles for some application, often creating multiple roles
in order to fulfill different kinds of users. We can also assign a password to the role so that
whenever that role needs to be set, password authentication will be required. This makes for
better security, as users who log in directly will not be able to use the role directly. However,
an application can be designed to automatically switch to that role when it starts. As a result,
an application user does not need to know the password for an application role.
Restricting SQL*Plus users from using database roles
Usually, applications that use Oracle database control the user actions inside the database. So
any data modification happens when the user uses the application. However, the database user
that the application uses to perform data modification can also be used to connect to the
database directly using SQL*Plus and perform any kind of modification to the table. This
could have undesired effects.
A user can use the privileges he/she got for use in the application and can issue destructive
SQL statements against the database tables.
For example, if we have a sales application which has an order_entry_mgr role, then this role
has privileges to perform data modifications to inventory tables.
If a database user who has been granted this role by an application connects to the database
directly using SQL*Plus, then that user can perform any data modification to the table.
Because SQL*Plus is an ad hoc query tool, the user is not restricted to a set of predefined
actions as they were on the application page. The user can query or modify data in the
inventory tables as he or she chooses.
Limiting roles through the PRODUCT_USER_PROFILE table
Oracle provides a table in the SYSTEM schema called PRODUCT_USER_PROFILE. We can use this
table to disable certain commands in the SQL*Plus utility for each user. This way we can
restrict the user, disabling the availability of certain commands, such as GRANT, REVOKE, SET
ROLE, and so on. This prevents users from changing database privileges.

We can also list any roles that we want prohibited to users in the PRODUCT_USER_PROFILE table.
For example, you could create an entry in the PRODUCT_USER_PROFILE table to do either of the
following:
Disallow the use of the clerk and manager roles with SQL*Plus
Disallow the use of SET ROLE with SQL*Plus
But the PRODUCT_USER_PROFILE table does not completely guarantee security. If a user has any
other privileges granted directly (without a role) then that user can exercise those using
SQL*Plus.
Managing object privileges
Object privileges act at the object level. You need to grant different privileges on each object
separately.
For example, if you have three tables and you want to grant SELECT privilege on those three
tables, you have to explicitly grant privileges on those three tables. You can specify the ALL
keyword if you want to grant all available privileges on an object.
ALL is not a privilege; rather, it's a shortcut that tells Oracle to grant all privileges on that
object to the user. This includes all privileges that are applicable to that object.
For example, if it's a table, it will include SELECT, DELETE, INSERT, and UPDATE. However, if its
procedure, it will also include EXECUTE.
You can similarly revoke object privilege on individual objects. You can also use the ALL
keyword to revoke all privilege on an object.
Some schema objects, such as clusters, indexes, triggers, and database links, do not have
associated object privileges. Their use is controlled with system privileges.

Application security
Creating an application security policy is the key thing in securing applications and databases.
An application security policy is a list of policies that should be implemented in applications
and databases in order to secure the access. Care should be taken not to have any open ends or
loopholes while drafting the policy. A strong policy grants minimum privileges to the users
and role and prevents users from directly connecting to the database.
Make sure that you have good application security enabled. The application should be able to
map database roles to the application users logging in so that different levels of users will
have different access to the application.
The following are some of the guidelines for application security:
The database user connecting from an application to the database should be different than
the schema user in which all objects reside. The schema user password should be locked
and the CREATE SESSION privileges should be removed.
It's always beneficial to the user database to connect users as application users so that you
can leverage the database permissions and security mechanism.
While writing command line programs, do not ask the user for password input. It is
better to check the user who is running the command line and make a decision in the
program whether to allow that user to run the program or not. If the user is allowed to
run, your program should be able to supply the database username and password
automatically (maybe from a wallet).
Authentication should be based on some form of wallet, central directory, or SSO rather
than a username/password combination.
Protect your application from SQL injection attacks. This happens when the user appends
an SQL statement along with other inputs and these SQL statements can perform
unintentional tasks.
You should also separate the working user from the schema user. The schema user should
contain all the objects and data, and working users should be the ones who are going to use
these objects and data. Each working user can have privileges on a different set of objects and
data. That way, a single user will not have access to all the tables and data.
Associating privileges with user database roles
Use the SET ROLE statement in your application to set the correct role in the session if a user
has been assigned multiple roles containing different privileges.
For example:
The ORDER role (for an application called order) contains the UPDATE privilege for the
INVENTORY table
The INVENTORY role (for an application called Inventory) contains the SELECT privilege
for the INVENTORY table

Several order entry clerks were granted both the ORDER and INVENTORY roles.
In this case, an order entry person who has been granted both privileges can log in to the
INVENTORY application to update inventory tables. This should not be allowed as inventory
tables should be updated by the inventory application. So when a user logs in to the order
application, we should use the SET ROLE statement to set the correct role in the database
session.
Configuring the maximum number of authentication attempts
You can set a limit to the number of failed authentication attempts that can be allowed by a
user by setting the SEC_MAX_FAILED_LOGIN_ATTEMPTS initialization parameter. After a defined
number of authentication attempts, the database drops the connection and the user has to make
a new connection again. The default value of this parameter is set to 10. This parameter is
meant for intruders who are trying to authenticate themselves by making multiple attempts.
We have similar parameters, INBOUND_CONNECT_TIMEOUT and FAILED_LOGIN_ATTEMPTS, in the
sqlnet.ora file, but these are meant for valid database users.

Configure and manage auditing
Auditing is a way to monitor database activities. If you want to keep a track of who is doing
what, auditing is the way to do it. You can audit every activity inside the database. Oracle, by
default, enables auditing or SYS user activity by setting the parameter audit_sys_operations
to TRUE. Setting this parameter to TRUE audits all admin-related activities, including the
database startup and shutdown and ALTER commands.
Another type of auditing is fine-grained auditing, which has granular settings. If we are
interested in knowing if a user has selected specific rows in table, we can enable auditing for
that as well. For example, you might want to know who is accessing records in table T1,
where col1 value > 1000.
Here too we can enable auditing. You can also use fine-grained auditing to audit activities
based on access to or changes in a column.

Purpose of auditing
Auditing is very important for understanding who is doing what. The following are some of
the advantages of auditing:
Accountability for action: We will know who has performed what action. Unless we
know this, we won't be able to root cause the issue. Deter users from inappropriate
actions based on that accountability.
Investigate suspicious activity: If some suspicious activity has happened inside the
database, we will come to know who is behind it.
Investigate correct privileges for the user: Auditing helps you in identifying
unnecessary privileges assigned to users. You can revoke the unnecessary privileges
from the user.
Address auditing requirements for compliance: Many companies need to have auditing
enabled for compliance purposes, and submit the audit report to auditing companies.
Regulations, such as the Sarbanes-Oxley Act (SOX), the Health Insurance Portability
and Accountability Act (HIPAA), and so on, have common auditing-related
requirements.
Monitor and gather data about suspicious database activities: We can always keep an
eye on suspicious activities in the database, and auditing helps us to catch those.

Mandatory auditing
Oracle database by default does some mandatory auditing and writes the audit logs to
operating system audit files. This auditing includes an auditing login for any user who uses
SYSDBA or SYSOPER privileges. Even if we set the audit_trail to DB (meaning Oracle will
write auditing records to the AUD$ table), mandatory auditing writes audit records to operating
system files.
The following are the mandatory auditing enabled by default in Oracle:
Database startup and shutdown: Every time we start up and shut down the database, an
audit record is generated and an entry is made in the audit trial file at OS level. The audit
record includes the terminal identifier alone with a timestamp. This audit record is saved
in the OS file only, irrespective of the audit_trail parameter.
SYSDBA and SYSOPER logins: All the logins done with SYSDBA and SYSOPER
privileges are auditing, and are stored in the audit trail file.

Auditing types
Following are the different types of auditing in database:
General Database auditing: This auditing can be used to audit SQL statements,
privileges, schema objects, functions, procedures, packages, triggers, and network
activity. For example, you can audit each time a particular user performs an UPDATE or a
DELETE SQL statement.
Default, security-relevant auditing: This auditing is the default auditing provided by
Oracle. If you configure the database using DBCA, this default auditing is enabled and
DB_AUDIT_TRAIL is set to DB. If you manually create a database, then you should run the
secconf.sql script to apply the default audit settings to your database. Default, security-
relevant auditing audits CREATE ANY PROCEDURE, CREATE ANY TABLE, DROP ANY USER,
and so on.
Fine-grained auditing: You can use fine-grained auditing for auditing very specific
action. For example, you can audit whether someone is querying a table with a condition
value of > 10. You can also audit based on the IP address of the client.
Auditing of SYS user: As mentioned previously, all the activities with SYSDBA
privilege, including startup and shutdown of the database, are audited.

Auditing general activities with standard auditing
Standard auditing is used for the auditing of user activities on database objects. You can
enable auditing at object level by using the AUDIT statement. Similarly, you can disable
auditing at object level by setting the NOAUDIT statement. Audit records will be written to the
database table or operating system audit files. Each audit record contains the following
information:
The user performing the operation
The type of operation
The object involved in the operation
The date and time of the operation
When we use standard database auditing, the audit records go to DBA_AUDIT_TRAIL (the
SYS.AUD$ table) if we want to store audit records in the database.
You can configure where the audit record should be stored by setting the AUDIT_TRAIL
parameter. The following are the various values for this parameter:
DB: Setting the AUDIT_TRAIL parameter to DB will write the audit record inside the
database in the AUD$ table, apart from the SYSDBA auditing records which are always
written to OS files.
OS: Audit records will be written to the OS audit files. You can set the parameter
AUDIT_FILE_DEST to specify the location where the audit files will be written.
NONE: Disables auditing.
DB, EXTENDED: This is an extended version of auditing. If we set the value to DB only,
it does not capture the SQL statements and bind values. Setting AUDIT_TRAIL to DB,
EXTENDED will capture the SQL statements and bind values as well. Audit records will
be stored in the database in AUD$ table.
XML: Writes audit records in XML file format in OS files.
XML, EXTENDED: Writes the audit records in XML format and also contains extended
information, such as SQL text and bind values.
You don't have to bounce the database if you change the auditing of an object, so the AUDIT
and NOAUDIT commands do not need the database to be bounced. However, if you want to
completely turn off auditing or change the destination of the audit record storage by changing
the AUDIT_TRAIL parameter, you need to bounce the database.
For fine-grained auditing, you don't have to enable the AUDIT_TRAIL; you just need to create
auditing policies in the database and assign those policies to the objects that you want to audit.
AUDIT and NOAUDIT SQL statements
To configure the standard auditing option, use the AUDIT SQL statement. AUDIT statement can
be used to enable auditing for the following categories.

Auditing SQL statements
The SQL statements that you can audit can be the following:
DDL statements: For example, enabling the auditing of tables (AUDIT TABLE) audits
all CREATE and DROP TABLE statements.
DML statements: For example, enabling the auditing of SELECT TABLE audits all SELECT
... FROM TABLE/VIEW statements, regardless of the table or view.
Statement auditing can be broad or focused, for example, by auditing the activities of all
database users or of only a select list of users.
Auditing privileges
Privilege auditing is a way to audit privileges. This way we will know everyone who is using
that privilege.
For example, we can audit the SELECT ANY TABLE privilege and whoever uses that privilege
will come into the audit records. This is another way to know which users are not using this
privilege so that we can revoke them. As with SQL statement auditing, you use the AUDIT and
NOAUDIT statements to enable and disable privilege auditing. In addition, you must have the
AUDIT SYSTEM privilege before you can enable auditing.
Auditing schema objects
Schema object auditing can audit all SELECT and DML statements permitted by object privileges,
such as SELECT or DELETE statements, on a particular table. The GRANT and REVOKE statements
that control those privileges are also audited.
Auditing network activity
Auditing can also be used to audit network-related issues. It helps to uncover unexpected
errors in the network protocol or internal errors in the network layer. We can hit several
network issues such as conflicting configuration settings for encryption or internal events and
so on. Auditing really helps you to understand what is causing these issues while keeping a
track of all the external entities that are connecting to our system.
Auditing statement executions - successful, unsuccessful, or both
We can audit only successful events or only unsuccessful events or we can audit both. This
decision is completely based on the situation that we want to audit for.
For example, if you are looking for users who are not able to perform the actions
successfully, you can audit only unsuccessful events. That way you can provide grants to such
users. Sometimes, you want to prevent too many audit records when auditing some hot tables.
In such cases, you can go for auditing only unsuccessful events. You can control auditing of
successful or unsuccessful records by providing that option when you are executing the AUDIT
statement.

The options are as follows:
WHENEVER SUCCESSFUL clause: This clause audits only successful executions of the
audited statement
WHENEVER NOT SUCCESSFUL clause: This clause audits only unsuccessful executions of the
audited statement
Auditing on unsuccessful events will provide us details of only valid SQL statements, but
which failed to execute because of permission issues or because they referenced a non-
existing schema object and so on. If an SQL is not a valid SQL, it fails to execute, but it will
not be recorded in the audit.
If you omit this clause (WHENEVER SUCCESSFUL or WHENEVER NOT SUCCESSFUL), Oracle will
audit both.
For example:
AUDIT DELETE ANY TABLE BY ACCESS WHENEVER NOT SUCCESSFUL; 
Benefits of using the BY ACCESS clause
You can specify another option with the AUDIT statement and that option is whether to audit the
object BY SESSION or BY ACCESS. You can provide one of the two options, the default being BY
ACCESS.
If you specify BY SESSION, then Oracle will make a single entry for all SQL statements of the
same type or operations of the same time executed on objects with the same schema.
But if you specify BY ACCESS or don't specify anything, Oracle will make an entry for each
audited statement and operation. The BY ACCESS audit records have separate LOGON and LOGOFF
entries, each with fine-grained timestamps.
For example:
AUDIT SELECT TABLE BY ACCESS; 
In this scenario, the user A connects to the database and issues two SELECT statements against
the table named DEPT and then disconnects from the database. User B connects to the database
and issues three SELECT statements against the DEPT table and then disconnects from the
database.
The audit trail contains five records, one recorded for each SELECT statement.
Auditing actions performed by specific users
We can audit the actions of specific users using the BY clause. This way we can focus on things
that we actually want to audit and also reduce the audit records.

For example, the following statement will audit select and update actions by deo_user and
advait users only.
AUDIT SELECT TABLE, UPDATE TABLE BY deo_user, advait BY ACCESS; 
Removing the audit option with the NOAUDIT SQL statement
We can use the NOAUDIT statement to turn off auditing. NOAUDIT statement also includes various
clauses to limit the scope of NOAUDIT. You can include the BY user clause to specify the list of
users for which this is applicable. You can also use WHENEVER SUCCESSFULL or WHENEVER NOT
SUCCESSFUL to turn off specific types of audit statements. However, the NOAUDIT statement does
not support the BY ACCESS clause.
For example:
NOAUDIT SELECT TABLE, INSERT TABLE, DELETE TABLE, EXECUTE PROCEDURE 
NOAUDIT DELETE ANY TABLE BY deo_user; 

Fine-grained auditing
Fine-grained auditing allows you to audit your tables at more granular level then standard
auditing. It allows you to audit based on the content of the table.
For example, in the case of the ordering application, you may have the need to audit users
whose orders are reaching beyond a certain value. In this case, it's not a good idea to audit all
the users by access; instead we can have a fine-grained auditing where we will audit only
those users whose orders are above a certain threshold value.
Fine-grained auditing policies are created with where predicates on table to audit statements
which fulfills that condition. Whenever a statement is executed and if fine-grained auditing is
enabled on the table, Oracle will write audit record if the where clause of the statement fulfills
the where clause of audit policy.
Fine-grained auditing records
Fine-grained audit records are stored in the SYS.FGA_LOG$ table internally. Oracle provides a
view DBA_FGA_AUDIT_TRAIL on top of this table for easy reference. You can also refer to
DBA_FGA_AUDIT_TRAIL which has audit records for standard auditing as well as fine-grained
auditing.
Oracle also audits all DML statements on the SYS.FGA_LOG$ table into the SYS.AUD$ table,
which is a table for standard auditing.
Using the DBMS_FGA package to manage fine-grained audit policies
You can use the DBMS_FGA package to manage fine-grained auditing. You need to have execute
privilege on this package for creating and managing fine-grained policies. You can use this
package to audit all types of statements, such as SELECT, UPDATE, DELETE, and INSERT. You can
also audit the MERGE statement indirectly by auditing the INSERT and DELETE statements. A
single audit policy can be created to audit all types of statements. and that policy is attached to
the object that you are auditing. If the audit condition matches the queries that are run by users,
those statements will be recorded in audit trail. The fine-grained auditing policy does not
include any information that is reported by regular auditing. Only one record will be inserted
into the audit trail for every fine-grained audit policy that evaluates to true.
Creating a fine-grained audit policy
You can create a fine-grained auditing policy on a table using the DBMS_FGA.ADD_POLICY
procedure. This procedure takes the auditing predicates as input, and whenever the statement
executes and fulfills the predicate, an audit record will be created for that statement. Oracle
stores all the policies in the SYS.FGA$ data dictionary table.
You cannot modify a fine-grained auditing policy once you create it. You need to drop and
recreate the policy if you want to make changes. Also, beware of a few restrictions that come

with fine-grained auditing, such as the inability to encrypt or decrypt the column of a table
that has fine-grained auditing enabled. If you want to update such columns, you have to
disable the policy first and then perform the updates, which will decrypt the column during the
update. Once all updates are done, you can re-enable the policy.
The following example shows how to audit statements INSERT, UPDATE, DELETE, and SELECT
on table HR.EMPLOYEES.
BEGIN 
 DBMS_FGA.ADD_POLICY( 
  object_schema      => 'HR', 
  object_name        => 'DEPT', 
  policy_name        => 'check_dept_values', 
  enable             =>  TRUE, 
  statement_types    => 'INSERT, UPDATE, SELECT, DELETE', 
  audit_trail        =>  DBMS_FGA.DB); 
END; 
Auditing specific columns and rows
You can fine-tune fine-grained audit performance by auditing only the relevant column of
interest. You can do so by using the audit_column parameter of the DBMS_FGA.ADD_POLICY
procedure. You can also further fine grain the auditing by auditing data in specific rows by
using audit_condition parameter to define a Boolean condition.
The following example performs an audit if anyone in Department 50 tries to access the
SALARY and COMMISSION_PCT columns:
audit_condition    => 'DEPARTMENT_ID = 50',  
audit_column       => 'SALARY,COMMISSION_PCT,' 
This feature is very helpful as it reduces the data set that Oracle has to audit, thus improving
the performance. It also provides protection for sensitive columns like social security
number, salary, and so on.
If the audit_column lists more than one column, you can use the audit_column_opts
parameter to specify whether a statement is audited when the query references any column
specified in the audit_column parameter, or only when all columns are referenced. For
example:
audit_column_opts   => DBMS_FGA.ANY_COLUMNS, 
audit_column_opts   => DBMS_FGA.ALL_COLUMNS, 
If you do not specify a relevant column, then auditing applies to all columns.
Disabling and enabling a fine-grained audit policy
You can disable a fine-grained audit policy by using the DBMS_FGA.DISABLE_POLICY
procedure. The syntax for DISABLE_POLICY is:

Disabling a fine-grained audit policy:
DBMS_FGA.DISABLE_POLICY( 
  object_schema        => 'HR', 
  object_name          => 'EMPLOYEES', 
  policy_name          => 'chk_hr_employees'); 
Enabling a fine-grained audit policy:
DBMS_FGA.ENABLE_POLICY( 
  object_schema        => 'HR', 
  object_name          => 'EMPLOYEES', 
  policy_name          => 'chk_hr_employees', 
  enable               => TRUE); 
Dropping a fine-grained audit policy
Oracle database automatically drops the audit policy if you remove the object specified in the
object_name parameter of the DBMS_FGA.ADD_POLICY procedure, or if you drop the user who
created the audit policy. The syntax is as follows:
DBMS_FGA.DROP_POLICY( 
 object_schema      => 'HR', 
 object_name        => 'EMPLOYEES', 
 policy_name        => 'chk_hr_employees'); 

Create the password file
Creating a password file with ORAPWD:
ORAPWD FILE=filename [ENTRIES=numusers] [FORCE={Y|N}] [IGNORECASE={Y|N}], 
The explanation of parameters are as follows:
FILE: Name of the password file. You can either pass the complete path of the file or you
can just pass the file name and it will be created in the current directory.
ENTRIES: (Optional) Maximum number of entries that are permitted in the file. This
corresponds to the number of users who are allowed to connect to the database remotely
as SYSDBA or SYSOPER.
FORCE: (Optional) If Y, overwrites the existing password file.
IGNORECASE - (Optional) If Y, passwords are treated as case-insensitive
For example:
The following command creates a password file named orapworcl that allows up to 30
privileged users with different passwords.
orapwd FILE=orapworcl ENTRIES=30 

Sharing and disabling the password file
You can control the sharing of password files among multiple databases on that host by using
the REMOTE_LOGIN_PASSWORDFILE parameter. This parameter also controls enabling or
disabling or password file authentication. The following are the valid values for this
parameter:
NONE: This parameter disables the use of the password file. It means that remote
authentication such as SYSDBA or SYSOPER will not work.
EXCLUSIVE: This is the default value and it means that password file will not be shared
among databases, so every database has to have an exclusive password file of its own.
This is a better setting from a security stand point. Only an EXCLUSIVE file can be
modified
SHARED: Multiple databases running on the same server can share the same password file.
You cannot modify this password file, meaning that you cannot add or delete the user
from this password file. and neither can you change the password for a SYS user. You
have to make the parameter exclusive to make those changes.

Keeping administrator passwords synchronized with the data
dictionary
If you recreate the password file or you want to change the password for users, you should
change the password in database using ALTER USER command. This command will
synchronize the password file as well if REMOTE_LOGIN_PASSWORDFILE is set to EXCLUSIVE. For
SYS user, you can simply change the password and automatically it will be synchronized. For
non-SYS user who as SYSDBA privileges, you must revoke and re-grant the privilege to the
user.
Adding users to a password file
Whenever you grant a SYSDBA or SYSOPER privilege to the user, that user gets added into the
password file. If you revoke these privileges, Oracle removes the user from password file.
Also, you should make sure to set REMOTE_LOGIN_PASSWORD to EXCLUSIVE else Oracle will
issue an error if you try to grant these privileges to the users.
Viewing password file members
Use the V$PWFILE_USERS view to see the users who have been granted the SYSDBA, SYSOPER, or
SYSASM system privileges
SQL>select * from V$PWFILE_USERS; 
USERNAME                       SYSDB SYSOP SYSAS 
------------------------------ ----- ----- ----- 
SYS                            TRUE  TRUE  FALSE 

Implement column and tablespace encryption
Oracle provide TDE or transparent data encryption to encrypt sensitive data inside database.
Encryption of data is transparent and application does not have to make any changes.
Whenever application access the data, it will be decrypted automatically.
Oracle database uses authentication, authorization and auditing mechanism to protect data
inside memory, but to protect data in OS data files, Oracle uses TDE. Also, to prevent
unauthorized decryption of sensitive encrypted data, Oracle stores encryption keys in security
module external to database. Application users or database users does not have to manage the
decryption key as this is handled automatically by database inside a wallet. An application that
processes sensitive data can use TDE to provide strong data encryption with little or no
change to the application.

Benefits of using transparent data encryption
Following are the advantages of transparent data encryption:
Transparent data encryption makes it easy to comply with your security policies without
making much changes on database side and application side. You only have to enable
encryption for column/tablespace.
You are sure that your sensitive data is secure and no one can steal any sensitive data
from your database.
No need of additional code to encrypt and decrypt the data as it's taking care internally by
database.
Application does not need any changes. In fact, application users are not even aware of
encryption on database side.
Encryption key is managed automatically by database inside a wallet and we don't have to
worry about that.

Types of transparent data encryption
Following types are explained:
TDE column encryption: TDE column encryption applies to the column level
encryption inside the table. This is used to protect sensitive columns of the tables.
Example, columns related to storage of social security number or credit card
information can be encrypted using this encryption. Oracle uses two-tired, key-based
architecture to encrypt the data in a column. It stores the encryption key in external
security module like wallet or hardware security module (HSM). This key is used for
both encryption as well as decryption.
TDE Tablespace encryption: This encryption can be used when we want to secure all
the tables inside a tablespace. We can go for encrypting entire tablespace and all the
tables and columns inside those tables will be encrypted automatically. This is better
instead of going for granular level as you don't have to determine which columns to
encrypt and no chances of missing any sensitive column. Also, performance is better as
encryption happens in bulk.

Using transparent data encryption
There are multiple ways to use encryption in database. We can either user wallet to store the
encryption key or use automatic login or set a master encryption key. Let's look into each of
these methods.
Specifying a wallet location for transparent data encryption
You can use wallet for storing encryption key and use that wallet every time for decrypting
the encrypted information. To use wallet, you need to set ENCRYPTION_WALLET_LOCATION
parameter in sqlnet.ora file. Oracle recommends using this parameter for specifying wallet
location.
Using wallets with automatic login enabled
Auto login feature automatically opens the wallet whenever database is open. You don't have
to explicitly open the wallet whenever you want to use it. This feature is useful in applications
which does not need extra security and for which we can keep the wallet open all the time.
Setting the master encryption key
We can also create master encryption key to encrypt the data in columns or tablespace. Same
encryption key can be used for both. Also decrypting the data needs same key to be set.
Following command shows setting master encryption key.
To set the master encryption key, use the following command:
SQL> ALTER SYSTEM SET ENCRYPTION KEY ["certificate_ID"] IDENTIFIED BY "password" 
Where:
certificate_ID is an optional string which is a unique identifier for searching
certificate inside a wallet. This parameter has no default setting and you should enclose
certificate_ID in double quotes.
We use password at the end of the statement. This password will be stored inside a wallet
and will be used for encrypting and decrypting the data in columns. Password is
mandatory parameter as that is the key for encrypting and decrypting data. Password is
case-sensitive in this case and you should enclose the password in double quotes.
You can check your certificate_ID by quering V$WALLET view whenever the wallet is
opened. Only certificates that can be used as master encryption keys by TDE are shown.
Opening and closing the encrypted wallet
You need to open the wallet before you can access encrypted data or before you can encrypt
any data. Following command shows how to open a wallet:
SQL> ALTER SYSTEM SET ENCRYPTION WALLET OPEN IDENTIFIED BY "password" 

Where password is the password to open the wallet.
Wallet remains open until you explicitly close the wallet or database has been shutdown.
Following commands shows how to close the wallet:
SQL> ALTER SYSTEM SET ENCRYPTION WALLET CLOSE IDENTIFIED BY "password" 
Once you close the wallet you can encrypt or decrypt the data in column or tablespace. If you
attempt to do so, you will get following error:
ORA-28365: wallet is not open 
So every time you restart database, you need to manually open the wallet explicitly.
Note
Note: Auto login wallets are opened automatically and do not need to be opened explicitly.
If the user does not have the ALTER SYSTEM privilege, or the wallet is unavailable, or an
incorrect password is given, then the command returns an error and exits. If the wallet is
already open, the command returns an error and takes no action

Creating tables with encrypted columns
By default, TDE uses ASE encryption with 192-bit key length(AES192) so if you do not
specify any encryption algorithm, it uses AES192. Transparent Data Encryption also adds
SALT to the clear text. SALE is just a random string added to main string before doing
encryption. This is done to provide better security. With SALT in place, it would be really
difficult for a hacker to get original data even after decrypting.
Creating a new table with an encrypted column using the default algorithm (AES192)
CREATE TABLE employee ( 
     first_name VARCHAR2(128), 
     last_name VARCHAR2(128), 
     empID NUMBER, 
     salary NUMBER(6) ENCRYPT 
); 
Creating a new table with an encrypted column Using 3DES168 and NO SALT
CREATE TABLE employee ( 
     first_name VARCHAR2(128), 
     last_name VARCHAR2(128), 
     empID NUMBER ENCRYPT NO SALT, 
     salary NUMBER(6) ENCRYPT USING '3DES168' 
); 
You can also encrypt an existing non-encrypted column
Encrypting an unencrypted column
SQL> ALTER TABLE employee MODIFY (first_name ENCRYPT); 

Encrypting entire tablespaces
Oracle provides an option to encrypt the entire tablespace so that all the segments belonging
to that tablespace will be encrypted automatically. This encryption is provided by Oracle TDE
(Transparent Data Encryption), which is available under "Oracle Advanced Security" option
of Oracle database.
Encryption requires two things: an encryption key and an algorithm. TDE uses encryption key
to encrypt data in column and tablespace and stores that encryption key within database itself.
But that encryption key is again encrypted using another master key, which is stored outside
of database. oracle can store master key outside database in a special container called an
external security module, which can be something as easy to set up as an Oracle wallet or as
sophisticated as a hardware security module device.
Setting the tablespace master encryption key
Before you can encrypt or decrypt tablespaces, you must generate or set a master encryption
key like we did for column level encryption. The tablespace master encryption key is stored
in an external security module and is used to encrypt the TDE tablespace encryption keys.
Check to ensure that the ENCRYPTION_WALLET_LOCATION (or WALLET_LOCATION) parameter in
the sqlnet.ora file points to the correct software wallet location. For example:
ENCRYPTION_WALLET_LOCATION= 
 (SOURCE=(METHOD=FILE)(METHOD_DATA= 
  (DIRECTORY=/app/wallet))) 
Oracle database 11g Release 2 (11.2) uses the same master encryption key for both TDE
column encryption and TDE tablespace encryption. When you issue the ALTER SYSTEM SET
ENCRYPTION KEY command, a unified master encryption key is created for both TDE column
encryption and TDE tablespace encryption.
Opening the Oracle wallet
We need to open the wallet before we create an encrypted tablespace. This is because
tablespace creation command needs access to wallet to read the encryption key to create
encrypted wallet. Wallet should also be open for any access that is required for encrypted
columns or tablespaces. So if application wants to access encrypted tablespace, we need to
have wallet open for them. Similarly, recovery of database needs wallet to be open for
recovering encrypted tablespace.
Following commands shows opening of wallet:
SQL> STARTUP MOUNT;
SQL> ALTER SYSTEM SET ENCRYPTION WALLET OPEN IDENTIFIED BY "password";
SQL> ALTER DATABASE OPEN;
Creating an encrypted tablespace

Following example shows creating encrypted tablespace:
CREATE TABLESPACE securespace 
DATAFILE '/home/user/oradata/secure01.dbf' 
SIZE 150M 
ENCRYPTION USING '3DES168' 
DEFAULT STORAGE(ENCRYPT); 
You can encrypt an existing tablespace. If you want to encrypt set of tables, you can create a
new encrypted empty tablespace and then move required tables to the encrypted tablespace.
You can either use data pump utility or you can use ALTER TABLE ... MOVE command.

Restrictions on using TDE tablespace encryption
TDE tablespace encryption/decryption happens during read/write operations, whereas TDE
column encryption/decryption happens at SQL layer. So most of the restrictions like data type
and index type restrictions that applies to column level encryption does not apply to
tablespace level encryption.
Following restrictions applies to tablespace level encryption:
External large options like BFILE and so on. It cannot be encrypted as they are stored
outside tablespace.
We cannot use original exp/imp utility to export and import data from encrypted
tablespace. We need to use data pump utility.

Summary
In this chapter we talked about developing a good security policy for our database and
application. We talked about various user security rules like enforcing password complexity,
added default tablespace for user and limiting the quota, creating profile for user to provide
limit for some of the database resources and so on. We also saw how we can configure
authentication and authorization for users. Later we talked about application security and
associating privileges to the database roles and use those roles to assign privileges to the
users. We then learned about auditing, different types of auditing and how auditing can benefit
us in terms of security. We saw different types of auditing including fine-grained auditing and
how it's different than normal auditing. We also saw the role of password file in enforcing
security and how users are stored in password file. Finally, we talked about transparent data
encryption and how we can implement the same to secure our data.
This concludes the entire course for Oracle 12c OCP upgrade exam. We hope this book
served the purpose of good study material for 1Z0-060 exam as well as helped DBA's to
understand and implement Oracle 12c new features in production databases. We tried our
level best to back every feature with an example so that readers have clear understanding
about the feature and what it can and cannot do. Because of the limitation of the content, it's
not possible to cover every possible variation of the new feature, but you can always visit our
blog http://avdeo.com to provide your feedback and suggestions and we will try to cover all
your doubts in the form of articles. You can consider this blog as an extension to this book to
further clarify and answer all your questions. Thank You !!


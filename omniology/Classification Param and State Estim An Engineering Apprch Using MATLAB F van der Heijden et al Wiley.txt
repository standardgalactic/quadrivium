
Classification, Parameter
Estimation and
State Estimation
An Engineering Approach using MATLAB
F. van der Heijden
Faculty of Electrical Engineering, Mathematics and Computer Science
University of Twente
The Netherlands
R.P.W. Duin
Faculty of Electrical Engineering, Mathematics and Computer Science
Delft University of Technology
The Netherlands
D. de Ridder
Faculty of Electrical Engineering, Mathematics and Computer Science
Delft University of Technology
The Netherlands
D.M.J. Tax
Faculty of Electrical Engineering, Mathematics and Computer Science
Delft University of Technology
The Netherlands

Classification, Parameter Estimation and
State Estimation

Classification, Parameter
Estimation and
State Estimation
An Engineering Approach using MATLAB
F. van der Heijden
Faculty of Electrical Engineering, Mathematics and Computer Science
University of Twente
The Netherlands
R.P.W. Duin
Faculty of Electrical Engineering, Mathematics and Computer Science
Delft University of Technology
The Netherlands
D. de Ridder
Faculty of Electrical Engineering, Mathematics and Computer Science
Delft University of Technology
The Netherlands
D.M.J. Tax
Faculty of Electrical Engineering, Mathematics and Computer Science
Delft University of Technology
The Netherlands

Copyright  2004
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester,
West Sussex PO19 8SQ, England
Telephone
(þ44) 1243 779777
Email (for orders and customer service enquiries): cs-books@wiley.co.uk
Visit our Home Page on www.wileyeurope.com or www.wiley.com
All Rights Reserved. No part of this publication may be reproduced, stored in a retrieval
system or transmitted in any form or by any means, electronic, mechanical, photocopying,
recording, scanning or otherwise, except under the terms of the Copyright, Designs and
Patents Act 1988 or under the terms of a licence issued by the Copyright Licensing Agency Ltd,
90 Tottenham Court Road, London W1T 4LP, UK, without the permission in writing
of the Publisher. Requests to the Publisher should be addressed to the Permissions Department,
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex PO19 8SQ,
England, or emailed to permreq@wiley.co.uk, or faxed to (þ44) 1243 770620.
Designations used by companies to distinguish their products are often claimed as trademarks.
All brand names and product names used in this book are trade names, service marks,
trademarks or registered trademarks of their respective owners. The Publisher is not
associated with any product or vendor mentioned in this book.
This publication is designed to provide accurate and authoritative information in regard to the
subject matter covered. It is sold on the understanding that the Publisher is not engaged in
rendering professional services. If professional advice or other expert assistance is
required, the services of a competent professional should be sought.
Other Wiley Editorial Offices
John Wiley & Sons Inc., 111 River Street, Hoboken, NJ 07030, USA
Jossey-Bass, 989 Market Street, San Francisco, CA 94103-1741, USA
Wiley-VCH Verlag GmbH, Boschstr. 12, D-69469 Weinheim, Germany
John Wiley & Sons Australia Ltd, 33 Park Road, Milton, Queensland 4064, Australia
John Wiley & Sons (Asia) Pte Ltd, 2 Clementi Loop #02-01, Jin Xing Distripark, Singapore
129809
John Wiley & Sons Canada Ltd, 22 Worcester Road, Etobicoke, Ontario, Canada M9W 1L1
Wiley also publishes its books in a variety of electronic formats. Some content that
appears in print may not be available in electronic books.
Library of Congress Cataloging in Publication Data
Classification, parameter estimation and state estimation : an engineering approach using
MATLAB / F. van der Heijden . . . [et al.].
p.
cm.
Includes bibliographical references and index.
ISBN 0-470-09013-8 (cloth : alk. paper)
1. Engineering mathematics—Data processing.
2. MATLAB.
3. Mensuration—Data
processing.
4. Estimation theory—Data processing.
I. Heijden, Ferdinand van der.
TA331.C53 2004
6810.2—dc22
2004011561
British Library Cataloguing in Publication Data
A catalogue record for this book is available from the British Library
ISBN 0-470-09013-8
Typeset in 10.5/13pt Sabon by Integra Software Services Pvt. Ltd, Pondicherry, India
Printed and bound in Great Britain by TJ International Ltd, Padstow, Cornwall
This book is printed on acid-free paper responsibly manufactured from sustainable
forestry in which at least two trees are planted for each one used for paper production.

Contents
Preface
xi
Foreword
xv
1
Introduction
1
1.1
The scope of the book
2
1.1.1
Classification
3
1.1.2
Parameter estimation
4
1.1.3
State estimation
5
1.1.4
Relations between the subjects
6
1.2
Engineering
9
1.3
The organization of the book
11
1.4
References
12
2
Detection and Classification
13
2.1
Bayesian classification
16
2.1.1
Uniform cost function and minimum error rate
23
2.1.2
Normal distributed measurements; linear
and quadratic classifiers
25
2.2
Rejection
32
2.2.1
Minimum error rate classification with
reject option
33
2.3
Detection: the two-class case
35
2.4
Selected bibliography
43
2.5
Exercises
43
3
Parameter Estimation
45
3.1
Bayesian estimation
47
3.1.1
MMSE estimation
54

3.1.2
MAP estimation
55
3.1.3
The Gaussian case with linear sensors
56
3.1.4
Maximum likelihood estimation
57
3.1.5
Unbiased linear MMSE estimation
59
3.2
Performance of estimators
62
3.2.1
Bias and covariance
63
3.2.2
The error covariance of the unbiased linear
MMSE estimator
67
3.3
Data fitting
68
3.3.1
Least squares fitting
68
3.3.2
Fitting using a robust error norm
72
3.3.3
Regression
74
3.4
Overview of the family of estimators
77
3.5
Selected bibliography
79
3.6
Exercises
79
4
State Estimation
81
4.1
A general framework for online estimation
82
4.1.1
Models
83
4.1.2
Optimal online estimation
86
4.2
Continuous state variables
88
4.2.1
Optimal online estimation in linear-Gaussian
systems
89
4.2.2
Suboptimal solutions for nonlinear
systems
100
4.2.3
Other filters for nonlinear systems
112
4.3
Discrete state variables
113
4.3.1
Hidden Markov models
113
4.3.2
Online state estimation
117
4.3.3
Offline state estimation
120
4.4
Mixed states and the particle filter
128
4.4.1
Importance sampling
128
4.4.2
Resampling by selection
130
4.4.3
The condensation algorithm
131
4.5
Selected bibliography
135
4.6
Exercises
136
5
Supervised Learning
139
5.1
Training sets
140
5.2
Parametric learning
142
5.2.1
Gaussian distribution, mean unknown
143
vi
CONTENTS

5.2.2
Gaussian distribution, covariance matrix
unknown
144
5.2.3
Gaussian distribution, mean and covariance
matrix both unknown
145
5.2.4
Estimation of the prior probabilities
147
5.2.5
Binary measurements
148
5.3
Nonparametric learning
149
5.3.1
Parzen estimation and histogramming
150
5.3.2
Nearest neighbour classification
155
5.3.3
Linear discriminant functions
162
5.3.4
The support vector classifier
168
5.3.5
The feed-forward neural network
173
5.4
Empirical evaluation
177
5.5
References
181
5.6
Exercises
181
6
Feature Extraction and Selection
183
6.1
Criteria for selection and extraction
185
6.1.1
Inter/intra class distance
186
6.1.2
Chernoff–Bhattacharyya distance
191
6.1.3
Other criteria
194
6.2
Feature selection
195
6.2.1
Branch-and-bound
197
6.2.2
Suboptimal search
199
6.2.3
Implementation issues
201
6.3
Linear feature extraction
202
6.3.1
Feature extraction based on the
Bhattacharyya distance with Gaussian
distributions
204
6.3.2
Feature extraction based on inter/intra
class distance
209
6.4
References
213
6.5
Exercises
214
7
Unsupervised Learning
215
7.1
Feature reduction
216
7.1.1
Principal component analysis
216
7.1.2
Multi-dimensional scaling
220
7.2
Clustering
226
7.2.1
Hierarchical clustering
228
7.2.2
K-means clustering
232
CONTENTS
vii

7.2.3
Mixture of Gaussians
234
7.2.4
Mixture of probabilistic PCA
240
7.2.5
Self-organizing maps
241
7.2.6
Generative topographic mapping
246
7.3
References
250
7.4
Exercises
250
8
State Estimation in Practice
253
8.1
System identification
256
8.1.1
Structuring
256
8.1.2
Experiment design
258
8.1.3
Parameter estimation
259
8.1.4
Evaluation and model selection
263
8.1.5
Identification of linear systems with
a random input
264
8.2
Observability, controllability and stability
266
8.2.1
Observability
266
8.2.2
Controllability
269
8.2.3
Dynamic stability and steady state solutions
270
8.3
Computational issues
276
8.3.1
The linear-Gaussian MMSE form
280
8.3.2
Sequential processing of the measurements
282
8.3.3
The information filter
283
8.3.4
Square root filtering
287
8.3.5
Comparison
291
8.4
Consistency checks
292
8.4.1
Orthogonality properties
293
8.4.2
Normalized errors
294
8.4.3
Consistency checks
296
8.4.4
Fudging
299
8.5
Extensions of the Kalman filter
300
8.5.1
Autocorrelated noise
300
8.5.2
Cross-correlated noise
303
8.5.3
Smoothing
303
8.6
References
306
8.7
Exercises
307
9
Worked Out Examples
309
9.1
Boston Housing classification problem
309
9.1.1
Data set description
309
9.1.2
Simple classification methods
311
viii
CONTENTS

9.1.3
Feature extraction
312
9.1.4
Feature selection
314
9.1.5
Complex classifiers
316
9.1.6
Conclusions
319
9.2
Time-of-flight estimation of an acoustic tone burst
319
9.2.1
Models of the observed waveform
321
9.2.2
Heuristic methods for determining the ToF
323
9.2.3
Curve fitting
324
9.2.4
Matched filtering
326
9.2.5
ML estimation using covariance models
for the reflections
327
9.2.6
Optimization and evaluation
332
9.3
Online level estimation in an hydraulic system
339
9.3.1
Linearized Kalman filtering
341
9.3.2
Extended Kalman filtering
343
9.3.3
Particle filtering
344
9.3.4
Discussion
350
9.4
References
352
Appendix A
Topics Selected from Functional Analysis
353
A.1
Linear spaces
353
A.1.1
Normed linear spaces
355
A.1.2
Euclidean spaces or inner product spaces
357
A.2
Metric spaces
358
A.3
Orthonormal systems and Fourier series
360
A.4
Linear operators
362
A.5
References
366
Appendix B
Topics Selected from Linear Algebra
and Matrix Theory
367
B.1
Vectors and matrices
367
B.2
Convolution
370
B.3
Trace and determinant
372
B.4
Differentiation of vector and matrix functions
373
B.5
Diagonalization of self-adjoint matrices
375
B.6
Singular value decomposition (SVD)
378
B.7
References
381
Appendix C
Probability Theory
383
C.1
Probability theory and random variables
383
C.1.1
Moments
386
CONTENTS
ix

C.1.2
Poisson distribution
387
C.1.3
Binomial distribution
387
C.1.4
Normal distribution
388
C.1.5
The Chi-square distribution
389
C.2
Bivariate random variables
390
C.3
Random vectors
395
C.3.1
Linear operations on Gaussian random
vectors
396
C.3.2
Decorrelation
397
C.4
Reference
398
Appendix D
Discrete-time Dynamic Systems
399
D.1
Discrete-time dynamic systems
399
D.2
Linear systems
400
D.3
Linear time invariant systems
401
D.3.1
Diagonalization of a system
401
D.3.2
Stability
402
D.4
References
403
Appendix E
Introduction to PRTools
405
E.1
Motivation
405
E.2
Essential concepts in PRTools
406
E.3
Implementation
407
E.4
Some details
410
E.4.1
Data sets
410
E.4.2
Classifiers and mappings
411
E.5
How to write your own mapping
414
Appendix F
MATLAB Toolboxes Used
417
Index
419
x
CONTENTS

1
Introduction
Engineering disciplines are those fields of research and development that
attempt to create products and systems operating in, and dealing with,
the real world. The number of disciplines is large, as is the range of scales
that they typically operate in: from the very small scale of nanotechnol-
ogy up to very large scales that span whole regions, e.g. water manage-
ment systems, electric power distribution systems, or even global systems
(e.g. the global positioning system, GPS). The level of advancement in
the fields also varies wildly, from emerging techniques (again, nanotech-
nology) to trusted techniques that have been applied for centuries (archi-
tecture, hydraulic works). Nonetheless, the disciplines share one
important aspect: engineering aims at designing and manufacturing
systems that interface with the world around them.
Systems designed by engineers are often meant to influence their
environment: to manipulate it, to move it, to stabilize it, to please it,
and so on. To enable such actuation, these systems need information,
e.g. values of physical quantities describing their environments and
possibly also describing themselves. Two types of information sources
are available: prior knowledge and empirical knowledge. The latter is
knowledge obtained by sensorial observation. Prior knowledge is the
knowledge that was already there before a given observation became
available (this does not imply that prior knowledge is obtained without
any observation). The combination of prior knowledge and empirical
knowledge leads to posterior knowledge.
Classification, Parameter Estimation and State Estimation: An Engineering Approach using MATLAB
F. van der Heijden, R.P.W. Duin, D. de Ridder and D.M.J. Tax
 2004 John Wiley & Sons, Ltd
ISBN: 0-470-09013-8

The sensory subsystem of a system produces measurement signals.
These signals carry the empirical knowledge. Often, the direct usage
of these signals is not possible, or inefficient. This can have several
causes:
. The information in the signals is not represented in an explicit way.
It is often hidden and only available in an indirect, encoded form.
. Measurement signals always come with noise and other hard-
to-predict disturbances.
. The information brought forth by posterior knowledge is more
accurate and more complete than information brought forth by
empirical knowledge alone. Hence, measurement signals should
be used in combination with prior knowledge.
Measurement signals need processing in order to suppress the noise and
to disclose the information required for the task at hand.
1.1
THE SCOPE OF THE BOOK
In a sense, classification and estimation deal with the same pro-
blem: given the measurement signals from the environment, how
can the information that is needed for a system to operate in the
real world be inferred? In other words, how should the measure-
ments from a sensory system be processed in order to bring max-
imal information in an explicit and usable form? This is the main
topic of this book.
Good processing of the measurement signals is possible only if
some knowledge and understanding of the environment and the
sensory system is present. Modelling certain aspects of that environ-
ment – like objects, physical processes or events – is a necessary task
for the engineer. However, straightforward modelling is not always
possible. Although the physical sciences provide ever deeper insight
into nature, some systems are still only partially understood; just
think of the weather. But even if systems are well understood,
modelling them exhaustively may be beyond our current capabilities
(i.e. computer power) or beyond the scope of the application. In such
cases, approximate general models, but adapted to the system at
hand, can be applied. The development of such models is also a
topic of this book.
2
INTRODUCTION

1.1.1
Classification
The title of the book already indicates the three main subtopics it will cover:
classification, parameter estimation and state estimation. In classification,
one tries to assign a class label to an object, a physical process, or an event.
Figure 1.1 illustrates the concept. In a speeding detector, the sensors are
a radar speed detector and a high-resolution camera, placed in a box beside
a road. When the radar detects a car approaching at too high a velocity
(a parameter estimation problem), the camera is signalled to acquire an
image of the car. The system should then recognize the license plate, so that
the driver of the car can be fined for the speeding violation. The system
should be robust to differences in car model, illumination, weather circum-
stances etc., so some pre-processing is necessary: locating the license plate in
the image, segmenting the individual characters and converting it into a
binary image. The problem then breaks down to a number of individual
classification problems. For each of the locations on the license plate, the
input consists of a binary image of a character, normalized for size, skew/
rotation and intensity. The desired output is the label of the true character,
i.e. one of ‘A’, ‘B’,.. ., ‘Z’, ‘0’,.. ., ‘9’.
Detection is a special case of classification. Here, only two class labels
are available, e.g. ‘yes’ and ‘no’. An example is a quality control system
that approves the products of a manufacturer, or refuses them. A second
problem closely related to classification is identification: the act of
proving that an object-under-test and a second object that is previously
seen, are the same. Usually, there is a large database of previously seen
objects to choose from. An example is biometric identification, e.g.
Figure 1.1
License plate recognition: a classification problem with noisy measurements
THE SCOPE OF THE BOOK
3

fingerprint recognition or face recognition. A third problem that can be
solved by classification-like techniques is retrieval from a database, e.g.
finding an image in an image database by specifying image features.
1.1.2
Parameter estimation
In parameter estimation, one tries to derive a parametric description for
an object, a physical process, or an event. For example, in a beacon-
based position measurement system (Figure 1.2), the goal is to find the
position of an object, e.g. a ship or a mobile robot. In the two-
dimensional case, two beacons with known reference positions suffice.
The sensory system provides two measurements: the distances from the
beacons to the object, r1 and r2. Since the position of the object involves
two parameters, the estimation seems to boil down to solving two
equations with two unknowns. However, the situation is more complex
because measurements always come with uncertainties. Usually, the
application not only requires an estimate of the parameters, but also
an assessment of the uncertainty of that estimate. The situation is even
more complicated because some prior knowledge about the position
must be used to resolve the ambiguity of the solution. The prior know-
ledge can also be used to reduce the uncertainty of the final estimate.
In order to improve the accuracy of the estimate the engineer can
increase the number of (independent) measurements to obtain an over-
determined system of equations. In order to reduce the cost of the
sensory system, the engineer can also decrease the number of measure-
ments leaving us with fewer measurements than parameters. The system
beacon 1
beacon 2
r1r
r2r
object
prior
knowledge
Figure 1.2
Position measurement: a parameter estimation problem handling uncer-
tainties
4
INTRODUCTION

of equations is underdetermined then, but estimation is still possible if
enough prior knowledge exists, or if the parameters are related to each
other (possibly in a statistical sense). In either case, the engineer is
interested in the uncertainty of the estimate.
1.1.3
State estimation
In state estimation, one tries to do either of the following – either
assigning a class label, or deriving a parametric (real-valued) description –
but for processes which vary in time or space. There is a fundamental
difference between the problems of classification and parameter estima-
tion on the one hand, and state estimation on the other hand. This is the
ordering in time (or space) in state estimation, which is absent from
classification and parameter estimation. When no ordering in the data is
assumed, the data can be processed in any order. In time series, ordering
in time is essential for the process. This results in a fundamental differ-
ence in the treatment of the data.
In the discrete case, the states have discrete values (classes or labels)
that are usually drawn from a finite set. An example of such a set is the
alarm stages in a safety system (e.g. ‘safe’, ‘pre-alarm’, ‘red alert’, etc.).
Other examples of discrete state estimation are speech recognition,
printed or handwritten text recognition and the recognition of the
operating modes of a machine.
An example of real-valued state estimation is the water management
system of a region. Using a few level sensors, and an adequate dynamical
model of the water system, a state estimator is able to assess the water
levels even at locations without level sensors. Short-term prediction of
the levels is also possible. Figure 1.3 gives a view of a simple water
management system of a single canal consisting of three linearly con-
nected compartments. The compartments are filled by the precipitation
in the surroundings of the canal. This occurs randomly but with a
seasonal influence. The canal drains its water into a river. The measure-
ment of the level in one compartment enables the estimation of the levels
in all three compartments. For that, a dynamic model is used that
describes the relations between flows and levels. Figure 1.3 shows an
estimate of the level of the third compartment using measurements of the
level in the first compartment. Prediction of the level in the third com-
partment is possible due to the causality of the process and the delay
between the levels in the compartments.
THE SCOPE OF THE BOOK
5

1.1.4
Relations between the subjects
The reader who is familiar with one or more of the three subjects might
wonder why they are treated in one book. The three subjects share the
following factors:
. In all cases, the engineer designs an instrument, i.e. a system whose
task is to extract information about a real-world object, a physical
process or an event.
. For that purpose, the instrument will be provided with a sensory sub-
system that produces measurement signals. In all cases, these signals are
represented by vectors (with fixed dimension) or sequences of vectors.
. The measurement vectors must be processed to reveal the informa-
tion that is required for the task at hand.
. Allthreesubjectsrelyontheavailabilityofmodelsdescribingtheobject/
physical process/event, and of models describing the sensory system.
. Modelling is an important part of the design stage. The suitability
of the applied model is directly related to the performance of the
resulting classifier/estimator.
0
1
2
3
4
5
6
5
5.2
5.4
5.6
5.8
level (cm)
measured,
canal 1
estimated, canal 3
time (hr)
canal 1
level sensor
canal 2
canal 3
drain
Figure 1.3
Assessment of water levels in a water management system: a state
estimation problem (the data is obtained from a scale model)
6
INTRODUCTION

Since the nature of the questions raised in the three subjects is similar, the
analysis of all three cases can be done using the same framework. This allows
an economical treatment of the subjects. The framework that will be used is
a probabilistic one. In all three cases, the strategy will be to formulate the
posterior knowledge in terms of a conditional probability (density) function:
Pðquantities of interestjmeasurements availableÞ
This so-called posterior probability combines the prior knowledge with
the empirical knowledge by using Bayes’ theorem for conditional prob-
abilities. As discussed above, the framework is generic for all three cases.
Of course, the elaboration of this principle for the three cases leads to
different solutions, because the natures of the ‘quantities of interest’
differ.
The second similarity between the topics is their reliance on models.
It is assumed that the constitution of the object/physical process/event
(including the sensory system) can be captured by a mathematical model.
Unfortunately, the physical structures responsible for generating the
objects/process/events are often unknown, or at least partly unknown. Con-
sequently, the model is also, at least partly, unknown. Sometimes, some
functional form of the model is assumed, but the free parameters still
have to be determined. In any case, empirical data is needed in order to
establish the model, to tune the classifier/estimator-under-development,
and also to evaluate the design. Obviously, the training/evaluation data
should be obtained from the process we are interested in.
In fact, all three subjects share the same key issue related to modelling,
namely the selection of the appropriate generalization level. The empirical
data is only an example of a set of possible measurements. If too much
weight is given to the data at hand, the risk of overfitting occurs. The
resulting model will depend too much on the accidental peculiarities (or
noise) of the data. On the other hand, if too little weight is given, nothing will
be learned and the model completely relies on the prior knowledge. The right
balance between these opposite sides depends on the statistical significance
of the data. Obviously, the size of the data is an important factor. However,
the statistical significance also holds a relation with dimensionality.
Many of the mathematical techniques for modelling, tuning, training
and evaluation can be shared between the three subjects. Estimation
procedures used in classification can also be used in parameter estima-
tion or state estimation with just minor modifications. For instance,
probability density estimation can be used for classification purposes,
and also for estimation. Data-fitting techniques are applied in both
THE SCOPE OF THE BOOK
7

classification and estimation problems. Techniques for statistical infer-
ence can also be shared. Of course, there are also differences between the
three subjects. For instance, the modelling of dynamic systems, usually
called system identification, involves aspects that are typical for dynamic
systems (i.e. determination of the order of the system, finding an appro-
priate functional structure of the model). However, when it finally
comes to finding the right parameters of the dynamic model, the tech-
niques from parameter estimation apply again.
Figure 1.4 shows an overview of the relations between the topics.
Classification and parameter estimation share a common foundation
indicated by ‘Bayes’. In combination with models for dynamic systems
(with random inputs), the techniques for classification and parameter
estimation find their application in processes that proceed in time, i.e.
state estimation. All this is built on a mathematical basis with selected
topics from mathematical analysis (dealing with abstract vector spaces,
metric spaces and operators), linear algebra and probability theory.
As such, classification and estimation are not tied to a specific application.
The engineer, who is involved in a specific application, should add the
individual characteristics of that application by means of the models and
prior knowledge. Thus, apart from the ability to handle empirical data,
the engineer must also have some knowledge of the physical background
related to the application at hand and to the sensor technology being used.
dynamic systems
with random
inputs
linear algebra
and matrix
theory
mathematical
analysis
probability
theory
dynamic
systems
mathematical basis
classification
parameter
estimation
physical background
sensor
technology
physical
processes
system
identification
learning from
examples
statistical
inference
modelling
data fitting &
regression
Bayes
state estimation
Figure 1.4
Relations between the subjects
8
INTRODUCTION

All three subjects are mature research areas, and many overview
books have been written. Naturally, by combining the three subjects
into one book, it cannot be avoided that some details are left out.
However, the discussion above shows that the three subjects are close
enough to justify one integrated book, covering these areas.
The combination of the three topics into one book also introduces
some additional challenges if only because of the differences in termin-
ology used in the three fields. This is, for instance, reflected in the
difference in the term used for ‘measurements’. In classification theory,
the term ‘features’ is frequently used as a replacement for ‘measure-
ments’. The number of measurements is called the ‘dimension’, but in
classification theory the term ‘dimensionality’ is often used.1 The same
remark holds true for notations. For instance, in classification theory the
measurements are often denoted by x. In state estimation, two notations
are in vogue: either y or z (MATLAB uses y, but we chose z). In all cases
we tried to be as consistent as possible.
1.2
ENGINEERING
The top-down design of an instrument always starts with some primary
need. Before starting with the design, the engineer has only a global view of
the system of interest. The actual need is known only at a high and abstract
level. The design process then proceeds through a number of stages during
which progressively more detailed knowledge becomes available, and the
system parts of the instrument are described at lower and more concrete
levels. At each stage, the engineer has to make design decisions. Such
decisions must be based on explicitly defined evaluation criteria. The
procedure, the elementary design step, is shown in Figure 1.5. It is used
iteratively at the different levels and for the different system parts.
An elementary design step typically consists of collecting and organiz-
ing knowledge about the design issue of that stage, followed by an
explicit formulation of the involved task. The next step is to associate
1 Our definition complies with the mathematical definition of ‘dimension’, i.e. the maximal
number of independent vectors in a vector space. In MATLAB the term ‘dimension’ refers to an
index of a multidimensional array as in phrases like: ‘the first dimension of a matrix is the row
index’, and ‘the number of dimensions of a matrix is two’. The number of elements along a row
is the ‘row dimension’ or ‘row length’. In MATLAB the term ‘dimensionality’ is the same as the
‘number of dimensions’.
ENGINEERING
9

the design issue with an evaluation criterion. The criterion expresses the
suitability of a design concept related to the given task, but also other
aspects can be involved, such as cost of manufacturing, computational
cost or throughput. Usually, there is a number of possible design con-
cepts to select from. Each concept is subjected to an analysis and an
evaluation, possibly based on some experimentation. Next, the engineer
decides which design concept is most appropriate. If none of the possible
concepts are acceptable, the designer steps back to an earlier stage to
alter the selections that have been made there.
One of the first tasks of the engineer is to identify the actual need that
the instrument must fulfil. The outcome of this design step is a descrip-
tion of the functionality, e.g. a list of preliminary specifications, operat-
ing characteristics, environmental conditions, wishes with respect to user
interface and exterior design. The next steps deal with the principles and
methods that are appropriate to fulfil the needs, i.e. the internal func-
tional structure of the instrument. At this level, the system under design
is broken down into a number of functional components. Each com-
ponent is considered as a subsystem whose input/output relations are
mathematically defined. Questions related to the actual construction,
realization of the functions, housing, etc., are later concerns.
The functional structure of an instrument can be divided roughly into
sensing, processing and outputting (displaying, recording). This book
focuses entirely on the design steps related to processing. It provides:
task definition
design concept generation
analysis / evaluation
decision
from preceding stage of the design process
to next stage of the design process
Figure 1.5
An elementary step in the design process (Finkelstein and Finkelstein,
1994)
10
INTRODUCTION

. Knowledge about various methods to fulfil the processing tasks of
the instrument. This is needed in order to generate a number of
different design concepts.
. Knowledge about how to evaluate the various methods. This is
needed in order to select the best design concept.
. A tool for the experimental evaluation of the design concepts.
The book does not address the topic ‘sensor technology’. For this, many
good textbooks already exist, for instance see Regtien et al. (2004) and
Brignell and White (1996). Nevertheless, the sensory system does have a
large impact on the required processing. For our purpose, it suffices to
consider the sensory subsystem at an abstract functional level such that it
can be described by a mathematical model.
1.3
THE ORGANIZATION OF THE BOOK
The first part of the book, containing Chapters 2, 3 and 4, considers each of
the three topics – classification, parameter estimation and state estimation –
at a theoretical level. Assuming that appropriate models of the objects,
physical process or events, and of the sensory system are available, these
three tasks are well defined and can be discussed rigorously. This facilitates
the development of a mathematical theory for these topics.
The second part of the book, Chapters 5 to 8, discusses all kinds of
issues related to the deployment of the theory. As mentioned in Section
1.1, a key issue is modelling. Empirical data should be combined with
prior knowledge about the physical process underlying the problem at
hand, and about the sensory system used. For classification problems,
the empirical data is often represented by labelled training and evalua-
tion sets, i.e. sets consisting of measurement vectors of objects together
with the true classes to which these objects belong. Chapters 5 and 6
discuss several methods to deal with these sets. Some of these techni-
ques – probability density estimation, statistical inference, data fitting –
are also applicable to modelling in parameter estimation. Chapter 7 is
devoted to unlabelled training sets. The purpose is to find structures
underlying these sets that explain the data in a statistical sense. This is
useful for both classification and parameter estimation problems. The
practical aspects related to state estimation are considered in Chapter 8.
In the last chapter all the topics are applied in some fully worked out
examples. Four appendices are added in order to refresh the required
mathematical background knowledge.
THE ORGANIZATION OF THE BOOK
11

The subtitle of the book, ‘An Engineering Approach using MATLAB’, indi-
cates that its focus is not just on the formal description of classification,
parameter estimation and state estimation methods. It also aims to
provide practical implementations of the given algorithms. These imple-
mentations are given in MATLAB. MATLAB is a commercial software
package for matrix manipulation. Over the past decade it has become
the de facto standard for development and research in data-processing
applications. MATLAB combines an easy-to-learn user interface with a
simple, yet powerful language syntax, and a wealth of functions orga-
nized in toolboxes. We use MATLAB as a vehicle for experimentation,
the purpose of which is to find out which method is the most appro-
priate for a given task. The final construction of the instrument can also
be implemented by means of MATLAB, but this is not strictly necessary.
In the end, when it comes to realization, the engineer may decide to
transform his design of the functional structure from MATLAB to other
platforms
using,
for
instance,
dedicated
hardware,
software
in
embedded systems or virtual instrumentation such as LabView.
For classification we will make use of PRTools (described in Appendix E),
a pattern recognition toolbox for MATLAB freely available for non-com-
mercial use. MATLAB itself has many standard functions that are useful for
parameter estimation and state estimation problems. These functions are
scattered over a number of toolboxes. Appendix F gives a short overview of
these toolboxes. The toolboxes are accompanied with a clear and crisp
documentation, and for details of the functions we refer to that.
Each chapter is followed by a few exercises on the theory provided.
However, we believe that only working with the actual algorithms will
provide the reader with the necessary insight to fully understand the
matter. Therefore, a large number of small code examples are provided
throughout the text. Furthermore, a number of data sets to experiment
with are made available through the accompanying website.
1.4
REFERENCES
Brignell, J. and White, N., Intelligent Sensor Systems, Revised edition, IOP Publishing,
London, UK, 1996.
Finkelstein, L. and Finkelstein A.C.W., Design Principles for Instrument Systems in
Measurement and Instrumentation (eds. L. Finkelstein and K.T.V. Grattan), Pergamon
Press, Oxford, UK, 1994.
Regtien, P.P.L., van der Heijden, F., Korsten, M.J. and Olthuis, W., Measurement Science
for Engineers, Kogan Page Science, London, UK, 2004.
12
INTRODUCTION

2
Detection and Classification
Pattern classification is the act of assigning a class label to an object, a
physical process or an event. The assignment is always based on meas-
urements that are obtained from that object (or process, or event). The
measurements are made available by a sensory system. See Figure 2.1.
Table 2.1 provides some examples of application fields in which classi-
fication is the essential task.
The definition of the set of relevant classes in a given application is in
some cases given by the nature of the application, but in other cases the
definition is not trivial. In the application ‘character reading for license
plate recognition’, the choice of the classes does not need much discus-
sion. However, in the application ‘sorting tomatoes into ‘‘class A’’, ‘‘class
B’’, and ‘‘class C’’’ the definition of the classes is open for discussion. In
such cases, the classes are defined by a generally agreed convention that
measurements
sensory
system
pattern
classification
object,
physical process
or event
class assigned to
object, process or
event
measurement system
Figure 2.1
Pattern classification
Classification, Parameter Estimation and State Estimation: An Engineering Approach using MATLAB
F. van der Heijden, R.P.W. Duin, D. de Ridder and D.M.J. Tax
 2004 John Wiley & Sons, Ltd
ISBN: 0-470-09013-8

the object is qualified according to the values of some attributes of the
object, e.g. its size, shape and colour.
The sensory system measures some physical properties of the object
that, hopefully, are relevant for classification. This chapter is confined
to the simple case where the measurements are static, i.e. time inde-
pendent. Furthermore, we assume that for each object the number of
measurements is fixed. Hence, per object the outcomes of the measure-
ments can be stacked to form a single vector, the so-called measurement
vector. The dimension of the vector equals the number of meas-
urements. The union of all possible values of the measurement vector
is the measurement space. For some authors the word ‘feature’ is very
close to ‘measurement’, but we will reserve that word for later use in
Chapter 6.
The sensory system must be designed so that the measurement vector
conveys the information needed to classify all objects correctly. If this is
the case, the measurement vectors from all objects behave according to
some pattern. Ideally, the physical properties are chosen such that all
objects from one class form a cluster in the measurement space without
overlapping the clusters formed by other classes.
Table 2.1
Some application fields of pattern classification
Application field
Possible measurements
Possible classes
Object classification
Sorting electronic
parts
Shape, colour
‘resistor’, ‘capacitor’,
‘transistor’, ‘IC’
Sorting mechanical
parts
Shape
‘ring’, ‘nut’, ‘bolt’
Reading characters
Shape
‘A’, ‘B’, ‘C’,
Mode estimation in a physical process
Classifying
manoeuvres of a
vehicle
Tracked point features
in an image sequence
‘straight on’, ‘turning’
Fault diagnosis in a
combustion engine
Cylinder pressures,
temperature, vibrations,
acoustic emissions, crank
angle resolver,
‘normal operation’, ‘defect
fuel injector’, ‘defect air
inlet valve’, ‘leaking
exhaust valve’,
Event detection
Burglar alarm
Infrared
‘alarm’, ‘no alarm’
Food inspection
Shape, colour, temperature,
mass, volume
‘OK’, ‘NOT OK’
14
DETECTION AND CLASSIFICATION

Example 2.1
Classification of small mechanical parts
Many workrooms have a spare part box where small, obsolete
mechanical parts such as bolts, rings, nuts and screws are kept. Often,
it is difficult to find a particular part. We would like to have the parts
sorted out. For automated sorting we have to classify the objects by
measuring some properties of each individual object. Then, based on
the measurements we decide to what class that object belongs.
As an example, Figure 2.2(a) shows an image with rings, nuts, bolts
and remaining parts, called scrap. These four types of objects will be
classified by means of two types of shape measurements. The first
type expresses to what extent the object is six-fold rotational sym-
metric. The second type of measurement is the eccentricity of the
object. The image-processing technique that is needed to obtain these
measurements is a topic that is outside the scope of this book.
The 2D measurement vector of an object can be depicted as a point
in the 2D measurement space. Figure 2.2(b) shows the graph of the
points of all objects. Since the objects in Figure 2.2(a) are already
sorted manually, it is easy here to mark each point with a symbol that
indicates the true class of the corresponding object. Such a graph is
called a scatter diagram of the data set.
The measure for six-fold rotational symmetry is suitable to discrim-
inate between rings and nuts since rings and nuts have a similar shape
except for the six-fold rotational symmetry of a nut. The measure for
eccentricity is suitable to discriminate bolts from the nuts and the rings.
bolts
nuts
rings
scrap
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
(b)
(a)
Figure 2.2
Classification of mechanical parts. (a) Image of various objects,
(b) Scatter diagram
DETECTION AND CLASSIFICATION
15

The shapes of scrap objects are difficult to predict. Therefore, their
measurements are scattered all over the space.
In this example the measurements are more or less clustered accord-
ing to their true class. Therefore, a new object is likely to have
measurements that are close to the cluster of the class to which the
object belongs. Hence, the assignment of a class boils down to decid-
ing to which cluster the measurements of the object belong. This can
be done by dividing the 2D measurement space into four different
partitions; one for each class. A new object is classified according to
the partitioning to which its measurement vector points.
Unfortunately, some clusters are in each other’s vicinity, or even
overlapping. In these regions the choice of the partitioning is critical.
This chapter addresses the problem of how to design a pattern classifier.
This is done within a Bayesian-theoretic framework. Section 2.1
discusses the general case. In Sections 2.1.1 and 2.1.2 two particular
cases are dealt with. The so-called ‘reject option’ is introduced in Section
2.2. Finally, the two-class case, often called ‘detection’, is covered by
Section 2.3.
2.1
BAYESIAN CLASSIFICATION
Probability theory is a solid base for pattern classification design. In this
approach the pattern-generating mechanism is represented within a
probabilistic framework. Figure 2.3 shows such a framework. The start-
ing point is a stochastic experiment (Appendix C.1) defined by a set
O ¼ f!1, . . . ,!Kg of K classes. We assume that the classes are mutually
exclusive. The probability P(!k) of having a class !k is called the prior
experiment
object
pattern
classification
class
measurement
vector
assigned
class
Ω = {ω1,...,ωk}
P (ωk)
z
measurement data not available (prior)
measurement data available (posterior)
ω ∈Ω
y(z)
sensory
system
Figure 2.3
Statistical pattern classification
16
DETECTION AND CLASSIFICATION

probability. It represents the knowledge that we have about the class of
an object before the measurements of that object are available. Since the
number of possible classes is K, we have:
X
K
k¼1
Pð!kÞ ¼ 1
ð2:1Þ
The sensory system produces a measurement vector z with dimension N.
Objects from different classes should have different measurement vec-
tors. Unfortunately, the measurement vectors from objects within the
same class also vary. For instance, the eccentricities of bolts in Figure 2.2
are not fixed since the shape of bolts is not fixed. In addition, all
measurements are subject to some degree of randomness due to all kinds
of unpredictable phenomena in the sensory system, e.g. quantum noise,
thermal noise, quantization noise. The variations and randomness are
taken into account by the probability density function of z.
The conditional probability density function of the measurement vec-
tor z is denoted by p(zj!k). It is the density of z coming from an object
with known class !k. If z comes from an object with unknown class, its
density is indicated by p(z). This density is the unconditional density of z.
Since classes are supposed to be mutually exclusive, the unconditional
density can be derived from the conditional densities by weighting these
densities by the prior probabilities:
pðzÞ ¼
X
K
k¼1
pðzj!kÞPð!kÞ
ð2:2Þ
The pattern classifier casts the measurement vector in the class that will
be assigned to the object. This is accomplished by the so-called decision
function ^!(:) that maps the measurement space onto the set of possible
classes. Since z is an N-dimensional vector, the function maps RN onto O.
That is: ^!(:): RN ! O.
Example 2.2
Probability densities of the ‘mechanical parts’ data
Figure 2.4 is a graphical representation of the probability densities of
the measurement data from Example 2.1. The unconditional density
p(z) is derived from (2.2) by assuming that the prior probabilities
P(!k) are reflected in the frequencies of occurrence of each type of
object in Figure 2.2. In that figure, there are 94 objects with frequen-
cies bolt:nut:ring:scrap ¼ 20:28:27:19. Hence the corresponding prior
BAYESIAN CLASSIFICATION
17

probabilities are assumed to be 20/94, 28/94, 27/94 and 19/94,
respectively.
The probabilities densities shown in Figure 2.4 are in fact not the
real densities, but they are estimates obtained from the samples. The
topic of density estimation will be dealt with in Chapter 5. PRTools
code to plot 2D-contours and 3D-meshes of a density is given in
Listing 2.1.
Listing 2.1
PRTools code for creating density plots.
load nutsbolts;
% Load the dataset; see listing 5.1
w ¼ gaussm(z,1);
% Estimate a mixture of Gaussians
figure(1); scatterd (z); hold on;
plotm(w,6,[0.1 0.5 1.0]);
% Plot in 3D
figure(2); scatterd (z); hold on;
for c ¼ 1: 4
w ¼ gaussm(seldat(z,c),1);
% Estimate a Gaussian per class
plotm(w,2,[0.1 0.5 1.0]);
% Plot in 2D
end;
In some cases, the measurement vectors coming from objects with differ-
ent classes show some overlap in the measurement space. Therefore, it
cannot always be guaranteed that the classification is free from mistakes.
0
0.5
0
0.5
1
six-fold rotational
symmetry
eccentricity
(a)
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
measure of six-fold rotational symmetry
measure of eccentricity
(b)
Figure 2.4
Probability densities of the measurements shown in Figure 2.2. (a) The
3D plot of the unconditional density together with a 2D contour plot of this density
on the ground plane. (b) 2D contour plots of the conditional probability densities
18
DETECTION AND CLASSIFICATION

An erroneous assignment of a class to an object causes some damage, or
some loss of value of the misclassified object, or an impairment of its
usefulness. All this depends on the application at hand. For instance, in the
application ‘sorting tomatoes into classes A, B or C’, having a class B
tomato being misclassified as ‘class C’ causes a loss of value because a
‘class B’ tomato yields more profit than a ‘class C’ tomato. On the other
hand, if a class C tomato is misclassified as a ‘class B’ tomato, the damage
is much more since such a situation may lead to malcontent customers.
A Bayes classifier is a pattern classifier that is based on the following
two prerequisites:
. The damage, or loss of value, involved when an object is erro-
neously classified can be quantified as a cost.
. The expectation of the cost is acceptable as an optimization criterion.
If the application at hand meets these two conditions, then the develop-
ment of an optimal pattern classification is theoretically straightforward.
However, the Bayes classifier needs good estimates of the densities of the
classes. These estimates can be problematic to obtain in practice.
The damage, or loss of value, is quantified by a cost function (or loss
function) C(^!j!k). The function C(:j:): O  O ! R expresses the cost that is
involved when the class assigned to an object is ^!, while the true class of that
object is !k. Since there are K classes, the function C(^!j!k) is fully specified
by a K  K matrix. Therefore, sometimes the cost function is called a cost
matrix. In some applications, the cost function might be negative, expressing
the fact that the assignment of that class pays off (negative cost ¼ profit).
Example 2.3
Cost function of the mechanical parts application
In fact, automated sorting of the parts in a ‘bolts-and-nuts’ box is an
example of a recycling application. If we are not collecting the
mechanical parts for reuse, these parts would be disposed of. There-
fore, a correct classification of a part saves the cost of a new part, and
thus the cost of such a classification is negative. However, we have to
take into account that:
. The effort of classifying and sorting a part also has to be paid. This
cost is the same for all parts regardless of its class and whether it has
been classified correctly or not.
. A bolt that has been erroneously classified as a nut or a ring causes
more trouble than a bolt that has been erroneously misclassified as
scrap. Likewise arguments hold for a nut and a ring.
BAYESIAN CLASSIFICATION
19

Table 2.2 is an example of a cost function that might be appropriate
for this application.
The concepts introduced above, i.e. prior probabilities, conditional
densities and cost function, are sufficient to design optimal classifiers.
However, first another probability has to be derived: the posterior
probability P(!kjz). It is the probability that an object belongs to
class !k given that the measurement vector associated with that object
is z. According to Bayes’ theorem for conditional probabilities
(Appendix C.2) we have:
Pð!kjzÞ ¼ pðzj!kÞPð!kÞ
pðzÞ
ð2:3Þ
If an arbitrary classifier assigns a class ^!i to a measurement vector z
coming from an object with true class !k, then a cost C(^!ij!k) is
involved. The posterior probability of having such an object is P(!kjz).
Therefore, the expectation of the cost is:
Rð^!ijzÞ ¼ E½Cð^!ij!kÞjz ¼
X
K
k¼1
Cð^!ij!kÞPð!kjzÞ
ð2:4Þ
This quantity is called the conditional risk. It expresses the expected cost
of the assignment ^!i to an object whose measurement vector is z.
From (2.4) it follows that the conditional risk of a decision function
^!(z) is R(^!(z)jz). The overall risk can be found by averaging the condi-
tional risk over all possible measurement vectors:
R ¼ E½Rð^!ðzÞjzÞ ¼
Z
z
Rð^!ðzÞjzÞpðzÞdz
ð2:5Þ
Table 2.2
Cost function of the ‘sorting mechanical part’ application
C( ^wi
wijwk) in $
True class
!1 ¼ bolt
!2 ¼ nut
!3 ¼ ring
!4 ¼ scrap
^! ¼ bolt
0.20
0.07
0.07
0.07
^! ¼ nut
0.07
0.15
0.07
0.07
^! ¼ ring
0.07
0.07
0.05
0.07
Assigned
class
^! ¼ scrap
0.03
0.03
0.03
0.03
20
DETECTION AND CLASSIFICATION

The integral extends over the entire measurement space. The quantity R
is the overall risk (average risk, or briefly, risk) associated with the
decision function ^!(z). The overall risk is important for cost price
calculations of a product.
The second prerequisite mentioned above states that the optimal
classifier is the one with minimal risk R. The decision function that
minimizes the (overall) risk is the same as the one that minimizes the
conditional risk. Therefore, the Bayes classifier takes the form:
^!BAYESðzÞ ¼ ^!i
such that: Rð^!ijzÞ  Rð^!jjzÞ
i; j ¼ 1; . . . ; K
ð2:6Þ
This can be expressed more briefly by:
^!BAYESðxÞ ¼ argmin
!2O
fRð!jzÞg
ð2:7Þ
The expression argminfg gives the element from O that minimizes
R(!jz). Substitution of (2.3) and (2.4) yields:
^!BAYESðzÞ ¼ argmin
!2O
X
K
k¼1
Cð!j!kÞPð!kjzÞ
(
)
¼ argmin
!2O
X
K
k¼1
Cð!j!kÞ pðzj!kÞPð!kÞ
pðzÞ
(
)
¼ argmin
!2O
X
K
k¼1
Cð!j!kÞpðzj!kÞPð!kÞ
(
)
ð2:8Þ
Pattern classification according to (2.8) is called Bayesian classification
or minimum risk classification.
Example 2.4
Bayes classifier for the mechanical parts application
Figure 2.5(a) shows the decision boundary of the Bayes classifier
for the application discussed in the previous examples. Figure
2.5(b) shows the decision boundary that is obtained if the prior
probability of scrap is increased to 0.50 with an evenly decrease of
the prior probabilities of the other classes. Comparing the results
it can be seen that such an increase introduces an enlargement of
the compartment for the scrap at the expense of the other com-
partments.
BAYESIAN CLASSIFICATION
21

The overall risk associated with the decision function in Figure 2.5(a)
appears to be $0.092; the one in Figure 2.5(b) is $0.036. The
increase of cost (¼ decrease of profit) is due to the fact that scrap is
unprofitable. Hence, if the majority of a bunch of objects consists of
worthless scrap, recycling pays off less.
The total cost of all classified objects as given in Figure 2.5(a)
appears to be $8.98. Since the figure shows 94 objects, the average
cost is $8.98/94 ¼ $0.096. As expected, this comes close to the
overall risk.
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
(a)
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
0
0.2
0.4
0.6
0.8
1
(b)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
(c)
Figure 2.5
Bayes classification. (a) With prior probabilities: P(bolt) ¼ 0:21,
P(nut) ¼ 0:30, P(ring) ¼ 0:29, and P(scrap) ¼ 0:20. (b) With increased prior prob-
ability for scrap: P(scrap) ¼ 0:50. (c) With uniform cost function
22
DETECTION AND CLASSIFICATION

Listing 2.2
PRTools code for estimating decision boundaries taking account of the
cost.
load nutsbolts;
cost ¼ [ 0.20
0.07
0.07
0.07 ; . . .
0.07
0.15
0.07
0.07 ; . . .
0.07
0.07
0.05
0.07 ; . . .
0.03
0.03
0.03
0.03];
w1 ¼ qdc(z);
% Estimate a single Gaussian per class
% Change output according to cost
w2 ¼ w1*classc*costm([],cost);
scatterd(z);
plotc(w1);
% Plot without using cost
plotc(w2);
% Plot using cost
2.1.1
Uniform cost function and minimum error rate
A uniform cost function is obtained if a unit cost is assumed when an
object is misclassified, and zero cost when the classification is correct.
This can be written as:
Cð^!ij!kÞ ¼ 1  ði; kÞ
with: ði; kÞ ¼
1
if i ¼ k
0
elsewhere

ð2:9Þ
(i,k) is the Kronecker delta function. With this cost function the condi-
tional risk given in (2.4) simplifies to:
Rð^!ijzÞ ¼
X
K
k¼1;k6¼i
Pð!kjzÞ ¼ 1  Pð^!ijzÞ
ð2:10Þ
Minimization of this risk is equivalent to maximization of the posterior
probability P(^!ijz). Therefore, with a uniform cost function, the Bayes
decision function (2.8) becomes the maximum a posteriori probability
classifier (MAP classifier):
^!MAPðzÞ ¼ argmax
!2O
fPð!jzÞg
ð2:11Þ
Application of Bayes’ theorem for conditional probabilities and cancel-
lation of irrelevant terms yield a classification, equivalent to a MAP
BAYESIAN CLASSIFICATION
23

classification, but fully in terms of the prior probabilities and the condi-
tional probability densities:
^!MAPðzÞ ¼ argmax
!2O
fpðzj!ÞPð!Þg
ð2:12Þ
The functional structure of this decision function is given in Figure 2.6.
Suppose that a class ^!i is assigned to an object with measurement
vector z. The probability of having a correct classification is P(^!ijz).
Consequently, the probability of having a classification error is
1  P(^!ijz). For an arbitrary decision function ^!(z), the conditional error
probability is:
eðzÞ ¼ 1  Pð^!ðzÞjzÞ
ð2:13Þ
It is the probability of an erroneous classification of an object whose
measurement is z. The error probability averaged over all objects can be
found by averaging e(z) over all the possible measurement vectors:
E ¼ E½eðzÞ ¼
Z
z
eðzÞpðzÞdz
ð2:14Þ
The integral extends over the entire measurement space. E is called the
error rate, and is often used as a performance measure of a classifier.
The classifier that yields the minimum error rate among all other
classifiers is called the minimum error rate classifier. With a uniform
cost function, the risk and the error rate are equal. Therefore, the
minimum error rate classifier is a Bayes classifier with uniform cost
P(ω1)
P(ω2)
P(ωk)
z
maximum
selector
p(z | ω1)
p(z | ω2)
p(z | ωk)
ω (z)
∧
Figure 2.6
Bayes decision function with uniform cost function (MAP classification)
24
DETECTION AND CLASSIFICATION

function. With our earlier definition of MAP classification we come to
the following conclusion:
Minimum error rate
classification

Bayes classification
with unit cost function 
MAP
classification
The conditional error probability of a MAP classifier is found by sub-
stitution of (2.11) in (2.13):
eminðzÞ ¼ 1  max
!2O fPð!jzÞg
ð2:15Þ
The minimum error rate Emin follows from (2.14):
Emin ¼
Z
z
eminðzÞpðzÞdz
ð2:16Þ
Of course, phrases like ‘minimum’ and ‘optimal’ are strictly tied to the
given sensory system. The performance of an optimal classification with
a given sensory system may be less than the performance of a non-
optimal classification with another sensory system.
Example 2.5
MAP classifier for the mechanical parts application
Figure 2.5(c) shows the decision function of the MAP classifier. The
error rate for this classifier is 4.8%, whereas the one of the Bayes
classifier in Figure 2.5(a) is 5.3%. In Figure 2.5(c) four objects are
misclassified. In Figure 2.5(a) that number is five. Thus, with respect
to error rate, the MAP classifier is more effective compared with the
Bayes classifier of Figure 2.5(a). On the other hand, the overall risk of
the classifier shown in Figure 2.5(c) and with the cost function given
in Table 2.2 is $0.084 which is a slight impairment compared with
the $0.092 of Figure 2.5(a).
2.1.2
Normal distributed measurements; linear and quadratic
classifiers
A further development of Bayes classification with uniform cost function
requires the specification of the conditional probability densities. This
BAYESIAN CLASSIFICATION
25

section discusses the case in which these densities are modelled as
normal. Suppose that the measurement vectors coming from an object
with class !k are normally distributed with expectation vector mk and
covariance matrix Ck (see Appendix C.3):
pðzj!kÞ ¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð2ÞNjCkj
q
exp ðz  mkÞTC1
k ðz  mkÞ
2
 
!
ð2:17Þ
where N is the dimension of the measurement vector.
Substitution of (2.17) in (2.12) gives the following minimum error rate
classification:
^!ðzÞ ¼ !i
with
i ¼ argmax
k¼1;...;K
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð2ÞNjCkj
q
exp ðz  mkÞTC1
k ðz  mkÞ
2
 
!
Pð!kÞ
8
>
<
>
:
9
>
=
>
;
ð2:18Þ
We can take the logarithm of the function between braces without
changing the result of the argmaxf g function. Furthermore, all terms
not containing k are irrelevant. Therefore (2.18) is equivalent to
^!ðzÞ ¼!i
with
i ¼ argmax
k¼1;...;K
 1
2 ln jCkj þ ln Pð!kÞ  1
2 ðz  mkÞTC1
k ðz  mkÞ


¼ argmax
k¼1;...;K
ln jCkj þ 2 ln Pð!kÞ  mT
k C1
k mk þ 2zTC1
k mk  zTC1
k z


ð2:19Þ
Hence, the expression of a minimum error rate classification with nor-
mally distributed measurement vectors takes the form of:
^!ðzÞ ¼ !i
with
i ¼ argmax
k¼1;...;K
fwk þ zTwk þ zTWkzg
ð2:20Þ
26
DETECTION AND CLASSIFICATION

with:
wk ¼  ln jCkj þ 2 ln Pð!kÞ  mT
k C1
k mk
wk ¼ 2C1
k mk
Wk ¼ C1
k
ð2:21Þ
A classifier according to (2.20) is called a quadratic classifier and the
decision function is a quadratic decision function. The boundaries
between the compartments of such a decision function are pieces of
quadratic hypersurfaces in the N-dimensional space. To see this, it
suffices to examine the boundary between the compartments of two
different classes, e.g. !i and !j. According to (2.20) the boundary
between the compartments of these two classes must satisfy the follow-
ing equation:
wi þ zTwi þ zTWiz ¼ wj þ zTwj þ zTWjz
ð2:22Þ
or:
wi  wj þ zTðwi  wjÞ þ zTðWi  WjÞz ¼ 0
ð2:23Þ
Equation (2.23) is quadratic in z. In the case that the sensory system has
only two sensors, i.e. N ¼ 2, then the solution of (2.23) is a quadratic
curve in the measurement space (an ellipse, a parabola, an hyperbola, or
a degenerated case: a circle, a straight line, or a pair of lines). Examples
will follow in subsequent sections. If we have three sensors, N ¼ 3, then
the solution of (2.23) is a quadratic surface (ellipsoid, paraboloid,
hyperboloid, etc.). If N > 3, the solutions are hyperquadrics (hyperellip-
soids, etc.).
If the number of classes is more than two, K > 2, then (2.23) is a
necessary condition for the boundaries between compartments, but not
a sufficient one. This is because the boundary between two classes may be
intersected by a compartment of a third class. Thus, only pieces of the
surfaces found by (2.23) are part of the boundary. The pieces of the sur-
face that are part of the boundary are called decision boundaries. The
assignment of a class to a vector exactly on the decision boundary is
ambiguous. The class assigned to such a vector can be arbitrarily selected
from the classes involved.
BAYESIAN CLASSIFICATION
27

As an example we consider the classifications shown in Figure 2.5.
In fact, the probability densities shown in Figure 2.4(b) are normal.
Therefore, the decision boundaries shown in Figure 2.5 must be quad-
ratic curves.
Class-independent covariance matrices
In this subsection, we discuss the case in which the covariance matrices
do not depend on the classes, i.e. Ck ¼ C for all !k 2 O. This situation
occurs when the measurement vector of an object equals the (class-
dependent) expectation vector corrupted by sensor noise, that is
z ¼ mk þ n. The noise n is assumed to be class-independent with covari-
ance matrix C. Hence, the class information is brought forth by the
expectation vectors only.
The quadratic decision function of (2.19) degenerates into:
^!ðxÞ ¼ !i
with
i ¼ argmax
k¼1;...;K
f2 ln Pð!kÞ  ðz  mkÞTC1ðz  mkÞg
¼ argmin
k¼1;...;K
f2 ln Pð!kÞ þ ðz  mkÞTC1ðz  mkÞg
ð2:24Þ
Since the covariance matrix C is self-adjoint and positive definite
(Appendix B.5) the quantity (z  mk)TC1(z  mk) can be regarded as a
distance measure between the vector z and the expectation vector mk.
The measure is called the squared Mahalanobis distance. The function of
(2.24) decides for the class whose expectation vector is nearest to the
observed measurement vector (with a correction factor 2 ln P(!k) to
account for prior knowledge). Hence, the name minimum Mahalonobis
distance classifier.
The decision boundaries between compartments in the measurement
space are linear (hyper)planes. This follows from (2.20) and (2.21):
^!ðzÞ ¼ !i
with
i ¼ argmax
k¼1;...;K
fwk þ zTwkg
ð2:25Þ
28
DETECTION AND CLASSIFICATION

where:
wk ¼ 2 ln Pð!kÞ  mT
k C1mk
wk ¼ 2C1mk
ð2:26Þ
A decision function which has the form of (2.25) is linear. The corre-
sponding classifier is called a linear classifier.
The equations of the
decision boundaries are wi  wj þ zT(wi  wj) ¼ 0.
Figure 2.7 gives an example of a four-class problem (K ¼ 4) in a two-
dimensional measurement space (N ¼ 2). A scatter diagram with the
contour plots of the conditional probability densities are given (Figure
2.7(a)), together with the compartments of the minimum Mahalanobis
distance classifier (Figure 2.7(b)). These figures were generated by the
code in Listing 2.3.
Listing 2.3
PRTools code for minimum Mahalanobis distance classification
mus ¼ [0.2 0.3; 0.35 0.75; 0.65 0.55; 0.8 0.25];
C ¼ [0.018 0.007; 0.007 0.011]; z ¼ gauss(200,mus,C);
w ¼ ldc(z);
% Normal densities, identical covariances
figure(1); scatterd(z); hold on; plotm(w);
figure(2); scatterd(z); hold on; plotc(w);
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measurement 1 
measurement 2 
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measurement1
measurement 2 
(a)
(b)
Figure 2.7
Minimum Mahalanobis distance classification. (a) Scatter diagram with
contour plot of the conditional probability densities. (b) Decision boundaries
BAYESIAN CLASSIFICATION
29

Minimum distance classification
A further simplification is possible when the measurement vector equals
the class-dependent vector mk corrupted by class-independent white
noise with covariance matrix C ¼ 2I.
^!ðzÞ ¼ !i
with
i ¼ argmin
k¼1;...;K
2 ln Pð!kÞ þ kz  mkk2
2
(
)
ð2:27Þ
The quantity k(z  mk)k is the normal (Euclidean) distance between z
and mk. The classifier corresponding to (2.27) decides for the class whose
expectation vector is nearest to the observed measurement vector (with a
correction factor 22 log P(!k) to account for the prior knowledge).
Hence, the name minimum distance classifier. As with the minimum
Mahalanobis distance classifier, the decision boundaries between com-
partments are linear (hyper)planes. The plane separating the compart-
ments of two classes !i and !j is given by:
2 log Pð!iÞ
Pð!jÞ þ 1
2 ðkmjk2  kmik2Þ þ zTðmi  mjÞ ¼ 0
ð2:28Þ
The solution of this equation is a plane perpendicular to the line segment
connecting mi and mj. The location of the hyperplane depends on the
factor 2 log (P(!i)/P(!j)). If P(!i) ¼ P(!j), the hyperplane is the perpen-
dicular bisector of the line segment (see Figure 2.8).
Figure 2.9 gives an example of the decision function of the minimum
distance classification. PRTools code to generate these figures is given in
Listing 2.4.
decision
boundary
z0
z1
µj
µi
Figure 2.8
Decision boundary of a minimum distance classifier
30
DETECTION AND CLASSIFICATION

Listing 2.4
PRTools code for minimum distance classification
mus ¼ [0.2 0.3; 0.35 0.75; 0.65 0.55; 0.8 0.25];
C ¼ 0.01*eye(2); z ¼ gauss(200,mus,C);
% Normal densities, uncorrelated noise with equal variances
w ¼ nmsc(z);
figure (1); scatterd (z); hold on; plotm (w);
figure (2); scatterd (z); hold on; plotc (w);
Class-independent expectation vectors
Another interesting situation is when the class information is solely
brought forth by the differences between covariance matrices. In that
case, the expectation vectors do not depend on the class: mk ¼ m for all
k. Hence, the central parts of the conditional probability densities overlap.
In the vicinity of the expectation vector, the probability of making a
wrong decision is always largest. The decision function takes the form of:
^!ðxÞ ¼ !i
with
i ¼ argmax
k¼1;...;K
 ln jCkj þ 2 ln Pð!kÞ  ðz  mÞTC1
k ðz  mÞ
n
o
ð2:29Þ
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measurement 1
measurement 2 
(a)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measurement 1
measurement 2 
(b)
Figure 2.9
Minimum distance classification. (a) Scatter diagram with contour plot
of the conditional probability densities. (b) Decision boundaries
BAYESIAN CLASSIFICATION
31

If the covariance matrices are of the type 2
kI, the decision boundaries are
concentric circles or (hyper)spheres. Figure 2.10(a) gives an example of
such a situation. If the covariance matrices are rotated versions of one
prototype, the decision boundaries are hyperbolae. If the prior probabil-
ities are equal, these hyperbolae degenerate into a number of linear
planes (or, if N ¼ 2, linear lines). An example is given in Figure 2.10(b).
2.2
REJECTION
Sometimes, it is advantageous to provide the classification with a
so-called reject option. In some applications, an erroneous decision may
lead to very high cost, or even to a hazardous situation. Suppose that the
measurement vector of a given object is in the vicinity of the decision
boundary. The measurement vector does not provide much class infor-
mation then. Therefore, it might be beneficial to postpone the classifica-
tion of that particular object. Instead of classifying the object, we reject
the classification. On rejection of a classification we have to classify the
object either manually, or by means of more involved techniques (for
instance, by bringing in more sensors or more advanced classifiers).
We may take the reject option into account by extending the range of
the decision function by a new element: the rejection class !0. The range
of the decision function becomes: Oþ ¼ f!0, !1, . . . , !Kg. The decision
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measurement 1 
measurement 2 
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measurement 1 
measurement 2 
(a)
(b)
Figure 2.10
Classification of objects with equal expectation vectors. (a) Rotational
symmetric conditional probability densities. (b) Conditional probability densities
with different orientations; see text
32
DETECTION AND CLASSIFICATION

function itself is a mapping ^!(z): RN ! Oþ. In order to develop a Bayes
classifier, the definition of the cost function must also be extended:
C(^!j!): Oþ  O ! R. The definition is extended so as to express the
cost of rejection. C(!0j!k) is the cost of rejection while the true class of
the object is !k.
With these extensions, the decision function of a Bayes classifier
becomes (2.8):
^!BAYESðzÞ ¼ argmin
!2Oþ
X
K
k¼1
Cð!j!kÞPð!kjzÞ
(
)
ð2:30Þ
The further development of the classifier follows the same course as in (2.8).
2.2.1
Minimum error rate classification with reject option
The minimum error rate classifier can also be extended with a reject
option. Suppose that the cost of a rejection is Crej regardless of the true
class of the object. All other costs are uniform and defined by (2.9).
We first note that if the reject option is chosen, the risk is Crej. If it is
not, the minimal conditional risk is the emin(z) given by (2.15). Minimiza-
tion of Crej and emin(z) yields the following optimal decision function:
^!ðzÞ ¼
!0
if Crej < eminðzÞ
^!ðzÞ ¼ argmax
!2O
fPð!jzÞg
otherwise
(
ð2:31Þ
The maximum posterior probability maxfP(!jz)g is always greater than
or equal to 1/K. Therefore, the minimal conditional error probability is
bounded by (1  1/K). Consequently, in (2.31) the reject option never
wins if Crej  1  1/K.
The overall probability of having a rejection is called the reject rate.
It is found by calculating the fraction of measurements that fall inside
the reject region:
Rej-Rate ¼
Z
fzjCrej<eminðzÞg
pðzÞdz
ð2:32Þ
The integral extends over those regions in the measurement space for
which Crej < e(z). The error rate is found by averaging the conditional
REJECTION
33

error over all measurements except those that fall inside the reject
region:
Emin ¼
Z
fzjCrejeminðzÞg
eminðzÞpðzÞdz
ð2:33Þ
Comparison of (2.33) with (2.16) shows that the error rate of a classi-
fication with reject option is bounded by the error rate of a classification
without reject option.
Example 2.6
The reject option in the mechanical parts application
In the classification of bolts, nuts, rings and so on, discussed in the
previous examples, it might be advantageous to manually inspect
those parts whose automatic classification is likely to fail. We
assume that the cost of manual inspection is about $0.04. Table
2.3 tabulates the cost function with the reject option included (com-
pare with Table 2.2).
The corresponding classification map is shown in Figure 2.11. In
this example, the reject option is advantageous only between the
regions of the rings and the nuts. The overall risk decreases from
$0.092 per classification to $0.093 per classification. The benefit
of the reject option is only marginal because the scrap is an expensive
item when offered to manual inspection. In fact, the assignment of an
object to the scrap class is a good alternative for the reject option.
Listing 2.5 shows the actual implementation in MATLAB. Clearly it is very
similar to the implementation for the classification including the costs. To
incorporate the reject option, not only the cost matrix has to be extended,
but clabels has to be redefined as well. When these labels are not
supplied explicitly, they are copied from the data set. In the reject case, an
extra class is introduced, so the definition of the labels cannot be avoided.
Table 2.3
Cost function of the mechanical part application with the reject option
included
C( ^wi
wijwk) in $
True class
!1 ¼ bolt
!2 ¼ nut
!3 ¼ ring
!4 ¼ scrap
^! ¼ bolt
0.20
0.07
0.07
0.07
^! ¼ ring
0.07
0.15
0.07
0.07
^! ¼ nut
0.07
0.07
0.05
0.07
^! ¼ scrap
0.03
0.03
0.03
0.03
Assigned class
^! ¼ !0 ¼ rejection
0.16
0.11
0.01
0.07
34
DETECTION AND CLASSIFICATION

Listing 2.5
PRTools code for minimum risk classification including a reject option
load nutsbolts;
cost ¼ [ 0.20
0.07
0.07
0.07 ; . . .
0.07
0.15
0.07
0.07 ; . . .
0.07
0.07
0.05
0.07 ; . . .
0.03
0.03
0.03
0.03 ; . . .
0.16
0.11
0.01
0.07 ];
clabels ¼ str2mat(getlablist(z),‘reject’);
w1 ¼ qdc(z);
% Estimate a single Gaussian per class
scatterd(z);
% Change output according to cost
w2 ¼ w1*classc*costm([],cost’,clabels);
plotc(w1);
% Plot without using cost
plotc(w2);
% Plot using cost
2.3
DETECTION: THE TWO-CLASS CASE
The detection problem is a classification problem with two possible
classes: K ¼ 2. In this special case, the Bayes decision rule can be
moulded into a simple form. Assuming a uniform cost function the
MAP classifier, expressed in (2.12), reduces to the following test:
pðzj!1ÞPð!1Þ > pðzj!2ÞPð!2Þ
ð2:34Þ
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of 6–fold rotational symmetry
measure of eccentricity
Figure 2.11
Bayes classification with the reject option included
DETECTION: THE TWO-CLASS CASE
35

If the test fails, it is decided for !2, otherwise for !1. We write symbolically:
!1
>
pðzj!1ÞPð!1Þ
pðzj!2ÞPð!2Þ
<
!2
ð2:35Þ
Rearrangement gives:
!1
pðzj!1Þ
pðzj!2Þ
>
<
Pð!2Þ
Pð!1Þ
!2
ð2:36Þ
Regarded as a function of !k the conditional probability density p(zj!k)
is called the likelihood function of !k. Therefore, the ratio:
LðzÞ ¼ pðzj!1Þ
pðzj!2Þ
ð2:37Þ
is called the likelihood ratio. With this definition the classification
becomes a simple likelihood ratio test:
!1
LðzÞ >
<
Pð!2Þ
Pð!1Þ
!2
ð2:38Þ
The test is equivalent to a threshold operation applied to L(z) with
threshold P(!2)/P(!1).
Even if the cost function is not uniform, the Bayes detector retains the
structure of (2.38), only the threshold should be adapted so as to reflect
the change of cost. The proof of this is left as an exercise for the reader.
In case of measurement vectors with normal distributions, it is con-
venient to replace the likelihood ratio test with a so-called log-likelihood
ratio test:
!1
>
LðzÞ
T
<
!2
with LðzÞ ¼ ln LðzÞ and T ¼ ln Pð!2Þ
Pð!1Þ


ð2:39Þ
36
DETECTION AND CLASSIFICATION

For vectors drawn from normal distributions, the log-likelihood ratio is:
LðzÞ ¼  1
2 ln jC1j  ln jC2j þ ðz  m1ÞTC1
1 ðz  m1Þ

ðz  m2ÞTC1
2 ðz  m2Þ
	
ð2:40Þ
which is much easier than the likelihood ratio. When the covariance
matrices of both classes are equal (C1 ¼ C2 ¼ C) the log-likelihood ratio
simplifies to:
LðzÞ ¼
z  1
2 m1 þ m2
ð
Þ

T
C1 m1  m2
ð
Þ
ð2:41Þ
Two types of errors are involved in a detection system. Suppose that ^!(z)
is the result of a decision based on the measurement z. The true (but
unknown) class ! of an object is either !1 or !2. Then the following four
states may occur:
! ¼ !1
! ¼ !2
^!(z) ¼ !1
correct decision I
type II error
^!(z) ¼ !2
type I error
correct decision II
Often, a detector is associated with a device that decides whether an
object is present (! ¼ !2) or not (! ¼ !1), or that an event occurs or not.
These types of problems arise, for instance, in radar systems, medical
diagnostic systems, burglar alarms, etc. Usually, the nomenclature for
the four states is as follows then:
! ¼ !1
! ¼ !2
^!(z) ¼ !1
true negative
missed event
or false negative
^!(z) ¼ !2
false alarm
or false positive
detection (or hit)
or true positive
Sometimes, the true negative is called ‘rejection’. However, we have
reserved this term for Section 2.2, where it has a different denotation.
The probabilities of the two types of errors, i.e. the false alarm and the
missed event, are performance measures of the detector. Usually these
DETECTION: THE TWO-CLASS CASE
37

probabilities are given conditionally with respect to the true classes, i.e.
Pmiss¼
defP(^!1j!2) and Pfa¼
defP(^!2j!1). In addition, we may define the prob-
ability of a detection Pdet¼
defP(^!2j!2).
The overall probability of a false alarm can be derived from the prior
probability using Bayes’ theorem, e.g. P(^!2, !1) ¼ P(^!2j!1)P(!1) ¼
PfaP(!1). The probabilities Pmiss and Pfa, as a function of the threshold
T, follow from (2.39):
PfaðTÞ ¼ PðLðzÞ < Tj!1Þ ¼
Z T
1
pðLj!1ÞdL
PmissðTÞ ¼ PðLðzÞ > Tj!2Þ ¼
Z 1
T
pðLj!2ÞdL
PdetðTÞ ¼ 1  PmissðTÞ
ð2:42Þ
In general, it is difficult to find analytical expressions for Pmiss(T) and
Pfa(T). In the case of Gaussian distributed measurement vectors, with
C1 ¼ C2 ¼ C, expression (2.42) can be further developed. Equation
(2.41) shows that L(z) is linear in z. Since z has a normal distribution,
so has L(z); see Appendix C.3.1. The posterior distribution of L(z) is
fully specified by its conditional expectation and its variance. As L(z) is
linear in z, these parameters are obtained as:
E LðzÞj!1
½
 ¼
E½zj!1  1
2 ðm1 þ m2Þ

T
C1ðm1  m2Þ
¼
m1  1
2 ðm1 þ m2Þ

T
C1ðm1  m2Þ
¼ 1
2 ðm1  m2ÞTC1ðm1  m2Þ
ð2:43Þ
Likewise:
E½LðzÞj!2 ¼  1
2 ðm1  m2ÞTC1ðm1  m2Þ
ð2:44Þ
and:
Var LðzÞj!1
½
 ¼ ðm1  m2ÞTC1ðm1  m2Þ ¼ Var LðzÞj!2
½

ð2:45Þ
With that, the signal-to-noise ratio is:
SNR ¼ ðE½Lj!2  E½Lj!1Þ2
Var½Lj!2
¼ ðm1  m2ÞTC1ðm1  m2Þ
ð2:46Þ
38
DETECTION AND CLASSIFICATION

The quantity (m1  m2)TC1(m1  m2) is the squared Mahalanobis dis-
tance between m1 and m2 with respect to C. The square root, d ¼
def
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
SNR
p
is called the discriminability of the detector. It is the signal-to-noise ratio
expressed as an amplitude ratio.
The conditional probability densities of L are shown in Figure 2.12.
The two overlapping areas in this figure are the probabilities of false
alarm and missed event. Clearly, these areas decrease as d increases.
Therefore, d is a good indicator of the performance of the detector.
Knowing that the conditional probabilities are Gaussian, it is possible
to evaluate the expressions for Pmiss(T) and Pfa(T) in (2.42) analytically.
The distribution function of a Gaussian random variable is given in
terms of the error function erf():
PfaðTÞ ¼ 1
2 þ 1
2 erf
T  1
2 d2
d
ﬃﬃﬃ
2
p
0
B
@
1
C
A
PmissðTÞ ¼ 1
2  1
2 erf
T þ 1
2 d2
d
ﬃﬃﬃ
2
p
0
B
@
1
C
A
ð2:47Þ
Figure 2.13(a) shows a graph of Pmiss, Pfa, and Pdet ¼ 1  Pmiss when the
threshold T varies. It can be seen that the requirements for T are contra-
dictory. The probability of a false alarm (type I error) is small if the
d
Pfa
Pmiss
d
d2
Λ
p(Λω1)
p(Λω2)
T
Figure 2.12
The conditional probability densities of the log-likelihood ratio in the
Gaussian case with C1 ¼ C2 ¼ C
DETECTION: THE TWO-CLASS CASE
39

threshold is chosen small. However, the probability of a missed event
(type II error) is small if the threshold is chosen large. A trade-off must
be found between both types of errors.
The trade-off can be made explicitly visible by means of a parametric
plot of Pdet versus Pfa with varying T. Such a curve is called a receiver
operating characteristic curve (ROC curve). Ideally, Pfa ¼ 0 and
Pdet ¼ 1, but the figure shows that no threshold exists for which this
occurs. Figure 2.13(b) shows the ROC curve for a Gaussian case with
equal covariance matrices. Here, the ROC curve can be obtained analyt-
ically, but in most other cases the ROC curve of a given detector must
be obtained either empirically or by numerical integration of (2.42).
In Listing 2.6 the MATLAB implementation for the computation of the
ROC curve is shown. To avoid confusion about the roles of the different
classes (which class should be considered positive and which negative) in
PRTools the ROC curve shows the fraction false positive and false
negative. This means that the resulting curve is a vertically mirrored
version of Figure 2.13(b). Note also that in the listing the training set is
used to both train a classifier and to generate the curve. To have a
reliable estimate, an independent data set should be used for the estima-
tion of the ROC curve.
Listing 2.6
PRTools code for estimation of a ROC curve
z ¼ gendats(100,1,2);
% Generate a 1D dataset
w ¼ qdc(z);
% Train a classifier
T
Pdet(T)
Pdet
Pmiss(T)
Pfa(T)
0
1
(a)
(b)
–10
10
0
T
Pfa
0
1
1
0
d =√8
d =√8
Figure 2.13
Performance of a detector in the Gaussian case with equal covariance
matrices. (a) Pmiss, Pdet and Pfa versus the threshold T. (b) Pdet versus Pfa as a
parametric plot of T
40
DETECTION AND CLASSIFICATION

r ¼ roc(z*w);
% Compute the ROC curve
plotr(r);
% Plot it
The merit of a ROC curve is that it specifies the intrinsic ability of the
detector to discriminate between the two classes. In other words,
the ROC curve of a detector depends neither on the cost function of
the application nor on the prior probabilities.
Since a false alarm and a missed event are mutually exclusive, the error
rate of the classification is the sum of both probabilities:
E ¼ Pð^!2; !1Þ þ Pð^!1; !2Þ
¼ Pð^!2j!1ÞPð!1Þ þ Pð^!1j!2ÞPð!2Þ
¼ PfaPð!1Þ þ PmissPð!2Þ
ð2:48Þ
In the example of Figure 2.13, the discriminability d equals
ﬃﬃﬃ
8
p
. If this
indicator becomes larger, Pmiss and Pfa become smaller. Hence, the error
rate E is a monotonically decreasing function of d.
Example 2.7
Quality inspection of empty bottles
In the bottling industry, the detection of defects of bottles (to be
recycled) is relevant in order to assure the quality of the product. A
variety of flaws can occur: cracks, dirty bottoms, fragments of glass,
labels, etc. In this example, the problem of detecting defects of the
mouth of an empty bottle is addressed. This is important, especially in
the case of bottles with crown caps. Small damages of the mouth may
cause a non-airtight enclosure of the product which subsequently
causes an untimely decay.
The detection of defects at the mouth is a difficult task. Some
irregularities at the mouth seem to be perturbing, but in fact are
harmless. Other irregularities (e.g. small intrusions at the surface of
the mouth) are quite harmful. The inspection system (Figure 2.14)
that performs the task consists of a stroboscopic, specular ‘light field’
illuminator, a digital camera, a detector, an actuator and a sorting
mechanism. The illumination is such that in the absence of irregular-
ities at the mouth, the bottle is seen as a bright ring (with fixed size
and position) on a dark background. Irregularities at the mouth give
rise to disturbances of the ring. See Figure 2.15.
The decision of the inspection system is based on a measurement
vector that is extracted from the acquired image. For this purpose the
area of the ring is divided into 256 equally sized sectors. Within each
DETECTION: THE TWO-CLASS CASE
41

sector the average of the grey levels of the observed ring is estimated.
These averages (as a function of running arc length along the ring) are
made rotational invariant by a translation invariant transformation,
e.g. the amplitude spectrum of the discrete Fourier transform.
The transformed averages form the measurement vector z. The next
step is the construction of a log-likelihood ratio L(z) according to
(2.40). Comparing the likelihood ratio against a suitable threshold
value gives the final decision.
Such a detector should be trained with a set of bottles that are
manually inspected so as to determine the parameters m1, m2 etc. (see
Chapter 5). Another set of manually inspected bottles is used for evalua-
tion. The result for a particular application is shown in Figure 2.16.
refused
valve
accepted
conveyor
acquisition
detector
actuator
Figure 2.14
Quality inspection system for the recycling of bottles
(b)
(a)
Figure 2.15
Acquired images of two different bottles. (a) Image of the mouth of a
new bottle. (b) Image of the mouth of an older bottle with clearly visible intrusions
42
DETECTION AND CLASSIFICATION

It seems that the Gaussian assumption with equal covariance matrices is
appropriate here. The discriminability appears to be d ¼ 4:8.
2.4
SELECTED BIBLIOGRAPHY
Many good textbooks on pattern classification have been written. These
books go into more detail than is possible here and approach the subject
from different points of view. The following list is a selection.
Duda, R.O., Hart, P.E. and Stork, D.G., Pattern Classification, Wiley, London, UK,
2001.
Fukanaga, K., Statistical Pattern Recognition, Academic Press, New York, NY, 1990.
Ripley, B.D., Pattern Recognition and Neural Networks, Cambridge University Press,
Cambridge, UK, 1996.
Webb, A.R., Statistical Pattern Recognition, 2nd edition, Wiley, London, UK, 2002.
2.5
EXERCISES
1. Give at least two more examples of classification systems. Also define possible meas-
urements and the relevant classes. (0)
2. Give a classification problem where the class definitions are subjective. (0)
3. Assume we have three classes of tomato with decreasing quality, class ‘A’, class ‘B’ and
class ‘C’. Assume further that the cost of misclassifying a tomato to a higher quality is
twice as expensive as vice versa. Give the cost matrix. What extra information do you
need in order to fully determine the matrix? (0)
4. Assume that the number of scrap objects in Figure 2.2 is actually twice as large. How
should the cost matrix, given in Table 2.2, be changed, such that the decision function
remains the same? (0)
0
0.05
Pdet
(b)
(a)
Pfa
0.1
p(Λ ω2)
p(Λ ω1)
–25
–15
–5
5
15
25
0.8
0.9
1
0
0.1
0.2
Λ
Figure 2.16
Estimated performance of the bottle inspector. (a) The conditional
probability densities of the log-likelihood ratio. (b) The ROC curve
EXERCISES
43

5. What quantities do you need to compute the Bayes classifier? How would you obtain
these quantities? (0)
6. Derive a decision function assuming that objects come from normally distributed
classes as in Section 2.1.2, but now with an arbitrary cost function ().
7. Can you think of a physical measurement system in which it can be expected that the
class distributions are Gaussian and where the covariance matrices are independent
of the class? (0)
8. Construct the ROC curve for the case that the classes have no overlap, and classes
which are completely overlapping. (0)
9. Derive how the ROC curve changes when the class prior probabilities are changed. (0)
10. Reconstruct the class conditional probabilities for the case that the ROC curve is not
symmetric around the axis which runs from (1,0) to (0,1). (0)
44
DETECTION AND CLASSIFICATION

3
Parameter Estimation
Parameter estimation is the process of attributing a parametric descrip-
tion to an object, a physical process or an event based on measurements
that are obtained from that object (or process, or event). The measure-
ments are made available by a sensory system. Figure 3.1 gives an over-
view. Parameter estimation and pattern classification are similar
processes because they both aim to describe an object using measure-
ments. However, in parameter estimation the description is in terms of a
real-valued scalar or vector, whereas in classification the description is in
terms of just one class selected from a finite number of classes.
Example 3.1
Estimation of the backscattering coefficient from
SAR images
In earth observation based on airborne SAR (synthetic aperture radar)
imaging, the physical parameter of interest is the backscattering coef-
ficient. This parameter provides information about the condition of
the surface of the earth, e.g. soil type, moisture content, crop type,
growth of the crop.
The mean backscattered energy of a radar signal in a direction is
proportional to this backscattering coefficient. In order to reduce
so-called speckle noise the given direction is probed a number of
times. The results are averaged to yield the final measurement. Figure 3.2
shows a large number of realizations of the true backscattering
Classification, Parameter Estimation and State Estimation: An Engineering Approach using MATLAB
F. van der Heijden, R.P.W. Duin, D. de Ridder and D.M.J. Tax
 2004 John Wiley & Sons, Ltd
ISBN: 0-470-09013-8

measurements
sensory
system
parameter
estimation
object,
physical process,
or event
described by
parameters
estimated
parameters
measurement system
Figure 3.1
Parameter estimation
0
0.5
1
1.5
0
0.5
1
1.5
backscattering coefficient
measurement
x
z
Figure 3.2
Different realizations of the backscattering coefficient and its corres-
ponding measurement
46
PARAMETER ESTIMATION

coefficient and its corresponding measurement.1 In this example, the
number of probes per measurement is eight. It can be seen that, even
after averaging, the measurement is still inaccurate. Moreover,
although the true backscattering coefficient is always between 0 and 1,
the measurements can easily violate this constraint (some measure-
ments are greater than 1).
The task of a parameter estimator here is to map each measurement
to an estimate of the corresponding backscattering coefficient.
This chapter addresses the problem of how to design a parameter
estimator. For that, two approaches exist: Bayesian estimation (Section
3.1) and data-fitting techniques (Section 3.3). The Bayesian-theoretic
framework for parameter estimation follows the same line of reasoning
as the one for classification (as discussed in Chapter 2). It is a probabil-
istic approach. The second approach, data fitting, does not have such a
probabilistic context. The various criteria for the evaluation of an esti-
mator are discussed in Section 3.2.
3.1
BAYESIAN ESTIMATION
Figure 3.3 gives a framework in which parameter estimation can be
defined. The starting point is a probabilistic experiment where the out-
come is a random vector x defined in RM, and with probability density
experiment
object
parameter
estimation
parameter
vector
measurement
vector
estimate
prior
probability
density
p(x)
ˆx(z) ∈RM
z ∈RN
x ∈RM
measurement data not available (prior)
measurement data available (posterior)
sensory
system
Figure 3.3
Parameter estimation
1 The data shown in Figure 3.2 is the result of a simulation. Therefore, in this case, the true
backscattering coefficients are known. Of course, in practice, the true parameter of interest is
always unknown. Only the measurements are available.
BAYESIAN ESTIMATION
47

p(x). Associated with x is a physical object, process or event (in short:
‘physical object’), of which x is a property. x is called a parameter
vector, and its density p(x) is called the prior probability density.
The object is sensed by a sensory system which produces an N-dimen-
sional measurement vector z. The task of the parameter estimator is to
recover the original parameter vector x given the measurement vector z.
This is done by means of the estimation function ^x(z): RN ! RM. The
conditional probability density p(zjx) gives the connection between the
parameter vector and measurements. With fixed x, the randomness of
the measurement vector z is due to physical noise sources in the sensor
system and other unpredictable phenomena. The randomness is charac-
terized by p(zjx). The overall probability density of z is found by averaging
the conditional density over the complete parameter space:
pðzÞ ¼
Z
x
pðzjxÞpðxÞdx
ð3:1Þ
The integral extends over the entire M-dimensional space RM.
Finally, Bayes’ theorem for conditional probabilities gives us the
posterior probability density p(xjz):
pðxjzÞ ¼ pðzjxÞpðxÞ
pðzÞ
ð3:2Þ
This density is most useful since z, being the output of the sensory
system, is at our disposal and thus fully known. Thus, p(xjz) represents
exactly the knowledge that we have on x after having observed z.
Example 3.2
Estimation of the backscattering coefficient
The backscattering coefficient x from Example 3.1 is within the
interval [0,1]. In most applications, however, lower values of the
coefficient occur more frequently than higher ones. Such a preference
can be taken into account by means of the prior probability density
p(x). We will assume that for a certain application x has a beta
distribution:
pðxÞ ¼ ða þ b þ 1Þ!
a!b!
xað1  xÞb
for
0  x  1
ð3:3Þ
The parameters a and b are the shape parameters of the distribution.
In Figure 3.4(a) these parameters are set to a ¼ 1 and b ¼ 4. These
48
PARAMETER ESTIMATION

values will be used throughout the examples in this chapter. Note that
there is no physical evidence for the beta distribution of x. The
assumption is a subjective result of our state of knowledge concerning
the occurrence of x. If no such knowledge is available, a uniform
distribution between 0 and 1 (i.e. all x are equally likely) would be
more reasonable.
The measurement is denoted by z. The mathematical model for
SAR measurements is that, with fixed x, the variable Nprobesz/x has
a gamma distribution with parameter Nprobes (the number of probes
per measurement). The probability density associated with a gamma
distribution is:
gamma pdfðu; Þ ¼ UðuÞ
ðÞ u 1 expðuÞ
ð3:4Þ
where u is the independent variable, () is the gamma function, a is
the parameter of the distribution and U(u) is the unit step function
which returns 0 if u is negative and 1 otherwise. Since z can be
regarded as a gamma-distributed random variable scaled by a factor
x/Nprobes, the conditional density of z becomes:
0
0.5
1
1.5
2
2.5
3
0
0.5
1
1.5
z/x
x p(z|x)
Nprobes=8
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
backscattering coefficient
p(x)
(a)
(b)
x
Figure 3.4
Probability densities for the backscattering coefficient. (a) Prior density
p(x). (b) Conditional density p(zjx) with Nprobes ¼ 8. The two axes have been scaled
with x and 1/x, respectively, to obtain invariance with respect to x
BAYESIAN ESTIMATION
49

pðzjxÞ ¼ UðzÞ Nprobes
x
gamma pdf
Nprobesz
x
; Nprobes


ð3:5Þ
Figure 3.4(b) shows the conditional density.
Cost functions
The optimization criterion of Bayes, minimum risk, applies to statistical
parameter estimation provided that two conditions are met. First, it
must be possible to quantify the cost involved when the estimates differ
from the true parameters. Second, the expectation of the cost, the risk,
should be acceptable as an optimization criterion.
Suppose
that
the
damage
is
quantified
by
a
cost
function
C(^xjx): RM  RM ! R. Ideally, this function represents the true cost.
In most applications, however, it is difficult to quantify the cost accur-
ately. Therefore, it is common practice to choose a cost function whose
mathematical treatment is not too complex. Often, the assumption is
that the cost function only depends on the difference between estimated
and true parameters: the estimation error e ¼ ^x  x. With this assump-
tion, the following cost functions are well known (see Table 3.1):
. quadratic cost function:
Cð^xjxÞ ¼
^x  x
k
k2
2 ¼
X
M1
m¼0
^xm  xm
ð
Þ2
ð3:6Þ
. absolute value cost function:
Cð^xjxÞ ¼
^x  x
k
k1¼
X
M1
m¼0
^xm  xm
j
j
ð3:7Þ
Table 3.1
Three different Bayes estimators worked out for the scalar case
MMSE estimation
MMAE estimation
MAP estimation
Quadratic cost function
Absolute cost function
Uniform cost function
ˆx–x
ˆ|x–x|
ˆ
C(x|x)
ˆx–x
∆
ˆx–x
ˆ
(x–x)2
^xMMSE(z) ¼ E[xjz]
^xMMAE(z) ¼ ^x
^xMAP(z) ¼ argmax
x
fp(xjz)g
¼
R
x xp(xjz)dx
with
R ^x
1 p(xjz)dx ¼ 1
2
50
PARAMETER ESTIMATION

. uniform cost function:
Cð^xjxÞ ¼
1
if
^x  x
k
k1> 
0
if
^x  x
k
k1 

with:  ! 0
ð3:8Þ
The first two cost functions are instances of the Minkowski distance
measures (see Appendix A.2). The third cost function is an approxima-
tion of the distance measure mentioned in (a.22).
Risk minimization
With an arbitrarily selected estimate ^x and a given measurement vector
z, the conditional risk of ^x is defined as the expectation of the cost
function:
Rð^xjzÞ ¼ E½Cð^xjxÞjz ¼
Z
x
Cð^xjxÞpðxjzÞdx
ð3:9Þ
In Bayes estimation (or minimum risk estimation) the estimate is the
parameter vector that minimizes the risk:
^xðzÞ ¼ argmin
x
RðxjzÞ
f
g
ð3:10Þ
The minimization extends over the entire parameter space.
The overall risk (also called average risk) of an estimator ^x(z) is the
expected cost seen over the full set of possible measurements:
R ¼ E½Rð^xðzÞjzÞ ¼
Z
z
Rð^xðzÞjzÞpðzÞdz
ð3:11Þ
Minimization of the integral is accomplished by minimization of the
integrand. However, since p(z) is positive, it suffices to minimize
R(^x(z)jz). Therefore, the Bayes estimator not only minimizes the condi-
tional risk, but also the overall risk.
The Bayes solution is obtained by substituting the selected cost func-
tion in (3.9) and (3.10). Differentiating and equating it to zero yields for
the three cost functions given in (3.6), (3.7) and (3.8):
. MMSE estimation (MMSE ¼ minimum mean square error).
. MMAE estimation (MMAE ¼ minimum mean absolute error).
. MAP estimation (MAP ¼ maximum a posterior).
BAYESIAN ESTIMATION
51

Table 3.1 gives the solutions that are obtained if x is a scalar. The
MMSE and the MAP estimators will be worked out for the vectorial
case in the next sections. But first, the scalar case will be illustrated by
means of an example.
Example 3.3
Estimation of the backscattering coefficient
The estimators for the backscattering coefficient (see previous example)
take the form as depicted in Figure 3.5. These estimators are found by
substitution of (3.3) and (3.5) in the expressions in Table 3.1.
In this example, the three estimators do not differ much. Never-
theless their own typical behaviours manifest themselves clearly if we
evaluate their results empirically. This can be done by means of the
population of the Npop ¼ 500 realizations that are shown in the figure.
Foreachsample zi wecalculatetheaveragecost1=Npop
PNpop
i¼1 C(^x(zi)jxi),
0
0.5
1
1.5
0
0.5
1
1.5
backscattering coefficient
measurement
x
z
realizations
MAP estimator
MMSE estimator
MMAE estimator
Figure 3.5
Three different Bayesian estimators
52
PARAMETER ESTIMATION

where xi is the true value of the i-th sample and ^x(:) is the estimator under
test. Table 3.2 gives the results of that evaluation for the three different
estimators and the three different cost functions.
Not surprisingly, in Table 3.2 the MMSE, the MMAE and the
MAP estimators are optimal with respect to their own criterion, i.e.
the quadratic, the absolute value and the uniform cost criterion,
respectively. It appears that the MMSE estimator is preferable if the
cost of a large error is considerably higher than the one of a small
error. The MAP estimator does not discriminate between small or
large errors. The MMAE estimator takes its position in between.
MATLAB code for generating Figure 3.5 is given in Listing 3.1. It
uses the Statistics toolbox for calculating the various probability
density functions. Although the MAP solution can be found analyt-
ically, here we approximate all three solutions numerically. To avoid
confusion, it is easy to create functions that calculate the various
probabilities needed. Note how p(z) ¼
R
p(zjx)p(x)dx is approxi-
mated by a sum over a range of values of x, whereas p(xjz) is found
by Bayes’ rule.
Listing 3.1
MATLAB code for MMSE, MMSA and MAP estimation in the scalar
case.
function estimates
global N Np a b xrange;
N ¼ 500;
% Number of samples
Np ¼ 8;
% Number of looks
a ¼ 2; b ¼ 5;
% Beta distribution parameters
x ¼ 0.005:0.005:1;
% Interesting range of x
z ¼ 0.005:0.005:1.5;
% Interesting range of z
load scatter;
% Load set (for plotting only)
xrange ¼ x;
Table 3.2
Empirical evaluation of the three different Bayes estimators in Figure 3.5
Type of estimation
MMSE
estimation
MMAE
estimation
MAP
estimation
Quadratic cost function
0.0067
0.0069
0.0081
Absolute cost function
0.062
0.060
0.063
Evaluation
criterion
Uniform cost function
(evaluated with  ¼ 0:05)
0.26
0.19
0.10
BAYESIAN ESTIMATION
53

for i ¼ 1:length(z)
[dummy,ind] ¼ max(px_z(x,z(i))); x_map(i) ¼ x(ind);
x_mse(i) ¼ sum(pz_x(z(i),x).*px(x).*x)
./.sum(pz_x(z(i),x).*px(x));
ind ¼ find((cumsum(px_z(x,z(i))) ./ .sum(px_z(x,z(i))))>0.5);
x_mae(i) ¼ x(ind(1));
end
figure; clf; plot(zset,xset,‘.’); hold on;
plot(z,x_map,‘k-.’); plot(z,x_mse,‘k--’);
plot(z,x_mae,‘k-’);
legend(‘realizations’,‘MAP’,‘MSE’,‘MAE’);
return
function ret ¼ px(x)
global a b; ret ¼ betapdf(x,a,b);
return
function ret ¼ pz_x(z,x)
global Np; ret ¼ (z>0).*(Np./x).*gampdf(Np*z./x,Np,1);
return
function ret ¼ pz(z)
global xrange; ret ¼ sum(px(xrange).*pz_x(z,xrange));
return
function ret ¼ px_z(x,z)
ret ¼ pz_x(z,x).*px(x)./pz(z);
return
3.1.1
MMSE estimation
The solution based on the quadratic cost function (3.6) is called the
minimum mean square error estimator, also called the minimum vari-
ance estimator for reasons that will become clear in a moment. Sub-
stitution of (3.6) and (3.9) in (3.10) gives:
^xMMSEðzÞ ¼ argmin
^x
Z
x
ð^x  xÞTð^x  xÞpðxjzÞdx


ð3:12Þ
Differentiating the function between braces with respect to ^x (see Appen-
dix B.4), and equating this result to zero yields a system of M linear
equations, the solution of which is:
^xMMSEðzÞ ¼
Z
x
xpðxjzÞdx ¼ E½xjz
ð3:13Þ
54
PARAMETER ESTIMATION

The conditional risk of this solution is the sum of the variances of the
estimated parameters:
Rð^xMMSEðzÞjzÞ ¼
Z
x
ð^xMMSEðzÞ  xÞTð^xMMSEðzÞ  xÞpðxjzÞdx
¼
Z
x
ðE½xjz  xÞTðE½xjz  xÞpðxjzÞdx
¼
X
M1
m¼0
Var½xmjz
ð3:14Þ
Hence the name ‘minimum variance estimator’.
3.1.2
MAP estimation
If the uniform cost function is chosen, the conditional risk (3.9)
becomes:
Rð^xjzÞ ¼
Z
x
pðxjzÞdx  pð^xjzÞ
¼ 1  pð^xjzÞ
ð3:15Þ
The estimate which now minimizes the risk is called the maximum a
posterior (MAP) estimate:
^xMAPðzÞ ¼ argmax
x
fpðxjzÞg
ð3:16Þ
This solution equals the mode (maximum) of the posterior probability. It
can also be written entirely in terms of the prior probability densities and
the conditional probabilities:
^xMAPðzÞ ¼ argmax
x
pðzjxÞpðxÞ
pðzÞ


¼ argmax
x
fpðzjxÞpðxÞg
ð3:17Þ
This expression is similar to the one of MAP classification; see (2.12).
BAYESIAN ESTIMATION
55

3.1.3
The Gaussian case with linear sensors
Suppose that the parameter vector x has a normal distribution with
expectation vector E[x] ¼ mx and covariance matrix Cx. In addition,
suppose that the measurement vector can be expressed as a linear
combination of the parameter vector corrupted by additive Gaussian
noise:
z ¼ Hx þ v
ð3:18Þ
where v is an N-dimensional random vector with zero expectation and
covariance matrix Cv. x and v are uncorrelated. H is an N  M matrix.
The assumption that both x and v are normal implies that the condi-
tional probability density of z is also normal. The conditional expect-
ation vector of z equals:
E½zjx ¼ mzjx ¼ Hmx
ð3:19Þ
The conditional covariance matrix of z is Czjx ¼ Cv.
Under these assumptions the posterior distribution of x is normal as
well. Application of (3.2) yields the following expressions for the MMSE
estimate and the corresponding covariance matrix:
^xMMSEðzÞ ¼ mxjz ¼ E½xjz ¼ HTC1
v H þ C1
x

1 HTC1
v z þ C1
x mx


Cxjz ¼ HTC1
v H þ C1
x

1
ð3:20Þ
The proof is left as an exercise for the reader. See exercise 3. Note that
mxjz, being the posterior expectation, is the MMSE estimate ^xMMSE(z).
The posterior expectation, E[xjz], consists of two terms. The first term
is linear in z. It represents the information coming from the measure-
ment. The second term is linear in mx, representing the prior knowledge.
To show that this interpretation is correct it is instructive to see what
happens at the extreme ends: either no information from the measure-
ment, or no prior knowledge:
. The measurements are useless if the matrix H is virtually zero, or if
the noise is too large, i.e. C1
v
is too small. In both cases, the second
term in (3.20) dominates. In the limit, the estimate becomes
^xMMSE(z) ¼ mx with covariance matrix Cx, i.e. the estimate is purely
based on prior knowledge.
56
PARAMETER ESTIMATION

. On the other hand, if the prior knowledge is weak, i.e. if the
variances of the parameters are very large, the inverse covariance
matrix C1
x
tends to zero. In the limit, the estimate becomes:
^xMMSEðzÞ ¼ HTC1
v H

1HTC1
v z
ð3:21Þ
In this solution, the prior knowledge, i.e. mx, is completely ruled out.
Note that the mode of a normal distribution coincides with the expec-
tation. Therefore, in the linear-Gaussian case, MAP estimation and
MMSE estimation coincide: ^xMMSE(z) ¼ ^xMAP(z).
3.1.4
Maximum likelihood estimation
In many practical situations the prior knowledge needed in MAP estima-
tion is not available. In these cases, an estimator which does not depend
on prior knowledge is desirable. One attempt in that direction is the
method referred to as maximum likelihood estimation (ML estimation).
The method is based on the observation that in MAP estimation, (3.17),
the peak of the first factor p(zjx) is often in an area of x in which the
second factor p(x) is almost constant. This holds true especially if
little prior knowledge is available. In these cases, the prior density p(x)
does not affect the position of the maximum very much. Discarding
the factor, and maximizing the function p(zjx) solely, gives the ML
estimate:
^xMLðzÞ ¼ argmax
x
fpðzjxÞg
ð3:22Þ
Regarded as a function of x the conditional probability density is called the
likelihood function. Hence the name ‘maximum likelihood estimation’.
Another motivation for the ML estimator is when we change our
viewpoint with respect to the nature of the parameter vector x. In the
Bayesian approach x is a random vector, statistically defined by means
of probability densities. In contrast, we may also regard x as a non-
random vector whose value is simply unknown. This is the so-called
Fisher approach. In this view, there are no probability densities asso-
ciated with x. The only density in which x appears is p(zjx), but here x
must be regarded as a parameter of the density of z. From all estimators
discussed so far, the only estimator that can handle this deterministic
point of view on x is the ML estimator.
BAYESIAN ESTIMATION
57

Example 3.4
Maximum likelihood estimation of the backscattering
coefficient
The maximum likelihood estimator for the backscattering coefficient
(see previous examples) is found by maximizing (3.5):
dpðzjxÞ
dx
¼ 0
)
^xMLðzÞ ¼ z
ð3:23Þ
The estimator is depicted in Figure 3.6 together with the MAP esti-
mator. The figure confirms the statement above that in areas of flat
prior probability density the MAP estimator and the ML estimator
coincide. However, the figure also reveals that the ML estimator can
produce an estimate of the backscattering coefficient that is larger
than one; a physical impossibility. This is the price that we have to
pay for not using prior information about the physical process.
0
0.5
1
1.5
0
0.5
1
1.5
backscattering coefficient
measurement
x
z
realizations
MAP estimator
ML  estimator
ulMMSE estimator
Figure 3.6
MAP estimation, ML estimation and linear MMSE estimation
58
PARAMETER ESTIMATION

If the measurement vector z is linear in x and corrupted by additive
Gaussian noise, as given in equation (3.18), the likelihood of x is given in
(3.21). Thus, in that case:
^xMLðzÞ ¼ HTC1
v H

1HTC1
v z
ð3:24Þ
A further simplification is obtained if we assume that the noise is white,
i.e. Cv  I:
^xMLðzÞ ¼ ðHTHÞ1HTz
ð3:25Þ
The operation (HTH)1HT is the pseudo inverse of H. Of course, its
validity depends on the existence of the inverse of HTH. Usually, such is
the case if the number of measurements exceeds the number of para-
meters, i.e. N > M. That is, if the system is overdetermined.
3.1.5
Unbiased linear MMSE estimation
The estimators discussed in the previous sections exploit full statistical
knowledge of the problem. Designing such an estimator is often difficult.
The first problem that arises is the adequate modelling of the probability
densities. Such modelling requires detailed knowledge of the physical
process and the sensors. Once we have the probability densities, the
second problem is how to minimize the conditional risk. Analytic
solutions are often hard to obtain. Numerical solutions are often
burdensome.
If we constrain the expression for the estimator to some mathematical
form, the problem of designing the estimator boils down to finding the
suitable parameters of that form. An example is the unbiased linear
MMSE estimator with the following form:2
^xulMMSEðzÞ ¼ Kz þ a
ð3:26Þ
The matrix K and the vector a must be optimized during the design
phase so as to match the behaviour of the estimator to the problem at
hand. The estimator has the same optimization criterion as the MMSE
2 The connotation of the term ‘unbiased’ becomes clear in Section 3.2.1. The linear MMSE
(without the adjective ‘unbiased’) also exists. It has the form ^xlMMSE(z) ¼ Kz. See exercise 1.
BAYESIAN ESTIMATION
59

estimator, i.e. a quadratic cost function. The constraint results in an
estimator that is not as good as the (unconstrained) MMSE estimator,
but it requires only knowledge of moments up to the order two, i.e.
expectation vectors and covariance matrices.
The starting point is the overall risk expressed in (3.11). Together with
the quadratic cost function we have:
R ¼
Z
z
Z
x
ðKz þ a  xÞTðKz þ a  xÞpðxjzÞpðzÞdxdz
ð3:27Þ
The optimal unbiased linear MMSE estimator is found by minimizing
R with respect to K and a. Hence we differentiate R with respect to a and
equate the result to zero:
dR
da ¼
Z
z
Z
x
ð2a þ 2Kz  2xÞpðxjzÞpðzÞdxdz
¼ 2a þ 2Kmz  2mx ¼ 0
yielding:
a ¼ mx  K mz
ð3:28Þ
with mx and mz the expectations of x and z.
Substitution of a back into (3.27), differentiation with respect to K,
and equating the result to zero (see also Appendix B.4):
R ¼
Z
z
Z
x
ðKðz  mzÞ  ðx  mxÞÞTðKðz  mzÞ  ðx  mxÞÞpðxjzÞpðzÞdxdz
¼ trace KCzKT þ Cx  2KCzx


dR
dK ¼ 2KCz  2Cxz ¼ 0
yields:
K ¼ CxzC1
z
ð3:29Þ
Cz is the covariance matrix of z, and Cxz ¼ E[(x  mx)(z  mz)T] the
cross-covariance matrix between x and z.
60
PARAMETER ESTIMATION

Example 3.5
Unbiased linear MMSE estimation of the
backscattering coefficient
In the scalar case, the linear MMSE estimator takes the form:
^xulMMSEðzÞ ¼ cov½x; z
Var½z ðz  E½zÞ þ E½x
ð3:30Þ
where cov[x,z] is the covariance of x and z. In the backscattering
problem, the required moments are difficult to obtain analytically.
However, they are easily estimated from the population of the 500
realizations shown in Figure 3.6 using techniques from Chapter 5.
The resulting estimator is shown in Figure 3.6. MATLAB code to plot
the ML and unbiased linear MMSE estimates of the backscattering
coefficient on a data set is given in Listing 3.2.
Listing 3.2
MATLAB code for unbiased linear MMSE estimation.
load scatter;
% Load dataset (zset,xset)
z ¼ 0.005:0.005:1.5;
% Interesting range of z
x_ml ¼ z;
% Maximum likelihood
mu_x ¼ mean(xset); mu_z ¼ mean(zset);
K ¼ ((xset-mu_x)’*(zset-mu_z))*inv((zset-mu_z)’*(zset-mu_z));
a ¼ mu_x  Kmu_z;
x_ulmse ¼ Kz þ a;
% Unbiased linear MMSE
figure; clf; plot(zset,xset,’.’); hold on;
plot(z,x_ml,’k-’); plot(z,x_ulmse,’k--’);
Linear sensors
The linear MMSE estimator takes a particular form if the sensory system
is linear and the sensor noise is additive:
z ¼ Hx þ v
ð3:31Þ
This case is of special interest because of its crucial role in the Kalman filter
(to be discussed in Chapter 4). Suppose that the noise has zero mean with
covariance matrix Cv. In addition, suppose that x and v are uncorrelated,
i.e. Cxv ¼ 0. Under these assumptions the moments of z are as follows:
mz ¼ H mx
Cz ¼ HCxHT þ Cv
Cxz ¼ CxHT
Czx ¼ HCx
ð3:32Þ
The proof is left as an exercise for the reader.
BAYESIAN ESTIMATION
61

Substitution of (3.32), (3.28) and (3.29) in (3.26) gives rise to the
following estimator:
^xulMMSEðzÞ ¼ mx þ Kðz  HmxÞ
with
K ¼ CxHT HCxHT þ Cv

1
ð3:33Þ
This version of the unbiased linear MMSE estimator is the so-called
Kalman form of the estimator.
Examination of (3.20) reveals that the MMSE estimator in the Gaussian
case with linear sensors is also expressed as a linear combination of mx and
z. Thus, in this special case (that is, Gaussian densities þ linear sensors)
^xMMSE(z) is a linear estimator. Since ^xulMMSE(z) and ^xMMSE(z) are based on
the same optimization criterion, the two solutions must be identical here:
^xulMMSE(z) ¼ ^xMMSE(z). We conclude that (3.20) is an alternative form of
(3.33). The forms are mathematically equivalent. See exercise 5.
The interpretation of ^xulMMSE(z) is as follows. The term mx represents
the prior knowledge. The term Hmx is the prior knowledge that we have
about the measurements. Therefore, the factor z  Hmx is the informa-
tive part of the measurements (called the innovation). The so-called
Kalman gain matrix K transforms the innovation into a correction term
K(z  Hmx) that represents the knowledge that we have gained from the
measurements.
3.2
PERFORMANCE OF ESTIMATORS
No matter which precautions are taken, there will always be a difference
between the estimate of a parameter and its true (but unknown) value.
The difference is the estimation error. An estimate is useless without an
indication of the magnitude of that error. Usually, such an indication is
quantified in terms of the so-called bias of the estimator, and the vari-
ance. The main purpose of this section is to introduce these concepts.
Suppose that the true, but unknown value of a parameter is x. An
estimator ^x(:) provides us with an estimate ^x ¼ ^x(z) based on measure-
ments z. The estimation error e is the difference between the estimate
and the true value:
e ¼ ^x  x
ð3:34Þ
Since x is unknown, e is unknown as well.
62
PARAMETER ESTIMATION

3.2.1
Bias and covariance
The error e is composed of two parts. One part is the one that does not
change value if we repeat the experiment over and over again. It is the
expectation of the error, called the bias. The other part is the random
part and is due to sensor noise and other random phenomena in the
sensory system. Hence, we have:
error ¼ bias þ random part
If x is a scalar, the variance of an estimator is the variance of e. As such
the variance quantifies the magnitude of the random part. If x is a vector,
each element of e has its own variance. These variances are captured in
the covariance matrix of e, which provides an economical and also a
more complete way to represent the magnitude of the random error.
The application of the expectation and variance operators to e needs
some discussion. Two cases must be distinguished. If x is regarded as a
non-random, unknown parameter, then x is not associated with any
probability density. The only randomness that enters the equations is
due to the measurements z with density p(zjx). However, if x is regarded
as random, it does have a probability density. We have two sources of
randomness then, x and z.
We start with the first case which applies to, for instance, the max-
imum likelihood estimator. Here, the bias b(x) is given by:
bðxÞ¼
defE½^x  xjx
¼
Z
ð^xðzÞ  xÞpðzjxÞdz
ð3:35Þ
The integral extends over the full space of z. In general, the bias depends
on x. The bias of an estimator can be small or even zero in one area of x,
whereas in another area the bias of that same estimator might be large.
In the second case, both x and z are random. Therefore, we define an
overall bias b by taking the expectation operator now with respect to
both x and z:
b¼
defE½^x  x
¼
Z Z
ð^xðzÞ  xÞpðx; zÞdzdx
ð3:36Þ
The integrals extend over the full space of x and z.
PERFORMANCE OF ESTIMATORS
63

The overall bias must be considered as an average taken over the full
range of x. To see this, rewrite p(x,z) ¼ p(zjx)p(x) to yield:
b ¼
Z
bðxÞpðxÞdx
ð3:37Þ
where b(x) is given in (3.35).
If the overall bias of an estimator is zero, then the estimator is said to
be unbiased. Suppose that in two different areas of x the biases of an
estimator have opposite sign, then these two opposite biases may cancel
out. We conclude that, even if an estimator is unbiased (i.e. its overall
bias is zero), then this does not imply that the bias for a specific value of
x is zero. Estimators that are unbiased for every x are called absolutely
unbiased.
The variance of the error, which serves to quantify the random fluc-
tuations, follows the same line of reasoning as the one of the bias. First
we determine the covariance matrix of the error with non-random x:
CeðxÞ¼
defE ðe  E½eÞðe  E½eÞTjx
h
i
¼
Z
ð^xðzÞ  x  bðxÞÞð^xðzÞ  x  bðxÞÞTpðzjxÞdz
ð3:38Þ
As before, the integral extends over the full space of z. The variances of
the elements of e are at the diagonal of Ce(x).
The magnitude of the full error (bias þ random part) is quantified by
the so-called mean square error (the second order moment matrix of the
error):
MeðxÞ¼
defE eeTjx


¼
Z
ð^xðzÞ  xÞð^xðzÞ  xÞTpðzjxÞdz
ð3:39Þ
It is straightforward to prove that:
MeðxÞ ¼ bðxÞbTðxÞ þ CeðxÞ
ð3:40Þ
This expression underlines the fact that the error is composed of a bias
and a random part.
The overall mean square error Me is found by averaging Me(x) over all
possible values of x:
64
PARAMETER ESTIMATION

Me ¼
defE eeT


¼
Z
MeðxÞpðxÞdx
ð3:41Þ
Finally, the overall covariance matrix of the estimation error is found as:
Ce ¼ E ðe  bÞðe  bÞT
h
i
¼ Me  bbT
ð3:42Þ
The diagonal elements of this matrix are the overall variances of the
estimation errors.
The MMSE estimator and the unbiased linear MMSE estimator are
always unbiased. To see this, rewrite (3.36) as follows:
b ¼
Z Z
ð^xMMSEðzÞ  xÞpðxjzÞpðzÞdxdz
¼
Z Z
ðE½xjz  xÞpðxjzÞpðzÞdxdz
ð3:43Þ
The inner integral is identical to zero, and thus b must be zero. The proof
of the unbiasedness of the unbiased linear MMSE estimator is left as an
exercise.
Other properties related to the quality of an estimator are stability and
robustness. In this context, stability refers to the property of being
insensitive to small random errors in the measurement vector. Robust-
ness is the property of being insensitive to large errors in a few elements
of the measurements (outliers); see Section 3.3.2. Often, the enlargement
of prior knowledge increases both the stability and the robustness.
Example 3.6
Bias and variance in the backscattering application
Figure 3.7 shows the bias and variance of the various estimators
discussed in the previous examples. To enable a fair comparison
between bias and variance in comparable units, the square root of
the latter, i.e. the standard deviation, has been plotted. Numerical
evaluation of (3.37), (3.41) and (3.42) yields:3
3 In this example, the vector b(x) and the matrix C(x) turn into scalars because here x is a scalar.
PERFORMANCE OF ESTIMATORS
65

bMMSE ¼ 0
bulMMSE ¼ 0
bML ¼ 0
bMAP ¼ 0:036
MMSE ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
CMMSE
p
¼ 0:086
ulMMSE ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
CulMMSE
p
¼ 0:094
ML ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
CML
p
¼ 0:116
MAP ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
CMAP
p
¼ 0:087
From this, and from Figure 3.7, we observe that:
. The overall bias of the ML estimator appears to be zero. So, in this
example, the ML estimator is unbiased (together with the two
MMSE estimators which are intrinsically unbiased). The MAP
estimator is biased.
. Figure 3.7 shows that for some ranges of x the bias of the MMSE
estimator is larger than its standard deviation. Nevertheless, the
MMSE estimator outperforms all other estimators with respect to
overall bias and variance. Hence, although a small bias is a desir-
able property, sometimes the overall performance of an estimator
can be improved by allowing a larger bias.
. The ML estimator appears to be linear here. As such, it is comparable
with the unbiased linear MMSE estimator. Of these two linear esti-
mators, the unbiased linear MMSE estimator outperforms the ML
estimator. The reason is that – unlike the ML estimator – the ulMMSE
0
0.2
0.4
0.6
0.8
1
–0.4
–0.2
0
0.2
x
bias(x)
backscattering coefficient
MAP
MMSE
ML
ulMMSE
(a)
0
0.2
0.4
0.6
0.8
1
MAP
MMSE
ML
ulMMSE
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
x
σ(x)
backscattering coefficient
(b)
Figure 3.7
The bias and the variance of the various estimators in the backscattering
problem
66
PARAMETER ESTIMATION

estimator exploits prior knowledge about the parameter. In addition,
the ulMMSE estimator is more apt to the evaluation criterion.
. Of the two nonlinear estimators, the MMSE estimator outperforms
the MAP estimator. The obvious reason is that the cost function of
the MMSE estimator matches the evaluation criterion.
. Of the two MMSE estimators, the nonlinear MMSE estimator outper-
forms the linear one. Both estimators have the same optimization
criterion, but the constraint of the ulMMSE degrades its performance.
3.2.2
The error covariance of the unbiased linear MMSE
estimator
We now return to the case of having linear sensors, z ¼ Hx þ v, as
discussed in Section 3.1.5. The unbiased linear MMSE estimator
appeared to be (see eq. (3.33)):
^xulMMSEðzÞ ¼ mx þ Kðz  HmxÞ
with
K ¼ CxHT HCxHT þ Cv

1
where Cv and Cx are the covariance matrices of v and x. mx is the (prior)
expectation vector of x. As said before, the ^xulMMSE(:) is unbiased.
Due to the unbiasedness of ^xulMMSE(:), the mean of the estimation
error e ¼ ^xulMMSE(:)  x is zero. The error covariance matrix, Ce, of e
expresses the uncertainty that remains after having processed the meas-
urements. Therefore, Ce is identical to the covariance matrix associated
with the posterior probability density. It is given by (3.20):
Ce ¼ Cxjz ¼ C1
x
þ HTC1
v H

1
ð3:44Þ
The inverse of a covariance matrix is called an information matrix. For
instance, C1
e
is a measure of the information provided by the estimate
^xulMMSE(:). If the norm of C1
e
is large, then the norm of Ce must be small
implying that the uncertainty in ^xulMMSE(:) is small as well. Equation
(3.44) shows that C1
e
is made up of two terms. The term C1
x
represents
the prior information provided by mx. The matrix C1
v
represents the
information that is given by z about the vector Hx. Therefore, the matrix
HTC1
v H represents the information about x provided by z. The two
sources of information add up. So, the information about x provided by
^xulMMSE(:) is C1
e
¼ C1
x
þ HTC1
v H.
PERFORMANCE OF ESTIMATORS
67

Using the matrix inversion lemma (b.10) the expression for the error
covariance matrix can be given in an alternative form:
Ce ¼ Cx  KSKT
with: S ¼ HCxHT þ Cn
ð3:45Þ
The matrix S is called the innovation matrix because it is the covariance
matrix of the innovations z  Hmx. The factor K(z  Hmx) is a correction
term for the prior expectation vector mx. Equation (3.45) shows that the
prior covariance matrix Cx is reduced by the covariance matrix KSKT of
the correction term.
3.3
DATA FITTING
In data-fitting techniques, the measurement process is modelled as:
z ¼ hðxÞ þ v
ð3:46Þ
where h(:) is the measurement function that models the sensory system,
and v are disturbing factors, such as sensor noise and modelling errors.
The purpose of fitting is to find the parameter vector x which ‘best’ fits
the measurements z.
Suppose that ^x is an estimate of x. Such an estimate is able to ‘predict’
the modelled part of z, but it cannot predict the disturbing factors. Note
that v represents both the noise and the unknown modelling errors. The
prediction of the estimate ^x is given by h(^x). The residuals e are defined
as the difference between observed and predicted measurements:
" ¼ z  hð^xÞ
ð3:47Þ
Data fitting is the process of finding the estimate ^x that minimizes some
error norm
e
k k of the residuals. Different error norms (see Appendix
A.1.1) lead to different data fits. We will shortly discuss two error norms.
3.3.1
Least squares fitting
The most common error norm is the squared Euclidean norm, also called
the sum of squared differences (SSD), or simply the LS norm (least
squared error norm):
e
k k2
2¼
X
N1
n¼0
"2
n ¼
X
N1
n¼0
ðzn  hnð^xÞÞ2 ¼ ðz  hð^xÞÞTðz  hð^xÞÞ
ð3:48Þ
68
PARAMETER ESTIMATION

The least squares fit, or least squares estimate (LS) is the parameter
vector which minimizes this norm:
^xLSðzÞ ¼ argmin
x
ðz  hðxÞÞTðz  hðxÞÞ
n
o
ð3:49Þ
If v is random with a normal distribution, zero mean and covariance
matrix Cv ¼ 2
vI, the LS estimate is identical to the ML estimate:
^xLS  ^xML. To see this, it suffices to note that in the Gaussian case the
likelihood takes the form
pðzjxÞ ¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð2ÞN2N
v
q
exp  ðz  hðxÞÞTðz  hðxÞÞ
22
v
 
!
ð3:50Þ
The minimization of (3.48) is equivalent to the minimization of (3.50).
If the measurement function is linear, that is z ¼ Hx þ v, and H is an
N  M matrix having a rank M with M < N, then according to (3.25):
^xLSðzÞ ¼ ^xMLðzÞ ¼ ðHTHÞ1HTz
ð3:51Þ
Example 3.7
Repeated measurements
Suppose that a scalar parameter x is N times repeatedly measured
using a calibrated measurement device: zn ¼ x þ vn. These repeated
measurements can be represented by a vector z ¼ [z1 . . . zN]T. The
corresponding
measurement
matrix
is
H ¼ [ 1 . . . 1 ]T.
Since
(HTH)1 ¼ 1/N, the resulting least squares fit is:
^xLS ¼ 1
N HTz ¼ 1
N
X
N
n¼1
zn
In other words, the best fit is found by averaging the measurements.
Nonlinear sensors
If h(:) is nonlinear, an analytic solution of (3.49) is often difficult. One is
compelled to use a numerical solution. For that, several algorithms exist,
such as ‘Gauss–Newton’, ‘Newton–Raphson’, ‘steepest descent’ and
many others. Many of these algorithms are implemented within
MATLAB’s optimization toolbox. The ‘Gauss–Newton’ method will be
explained shortly.
DATA FITTING
69

Assuming that some initial estimate xref is available we expand (3.46)
in a Taylor series and neglect all terms of order higher than two:
z ¼ hðxÞ þ v
 hðxrefÞ þ Href x  xref


þ v
with:
Href ¼ qhðxÞ
qx




x¼xref
ð3:52Þ
where Href is the Jacobian matrix of h(:) evaluated at xref, see Appendix
B.4. With such a linearization, (3.51) applies. Therefore, the following
approximate value of the LS estimate is obtained:
^xLS  xref þ
HT
refHref

1
Href z  hðxrefÞ


ð3:53Þ
A refinement of the estimate could be achieved by repeating the proce-
dure with the approximate value as reference. This suggests an iterative
approach. Starting with some initial guess ^x(0), the procedure becomes
as follows:
^xði þ 1Þ ¼ ^xðiÞ þ HTðiÞHðiÞ

1HðiÞ z  hð^xðiÞÞ
ð
Þ
with:
HðiÞ ¼ qhðxÞ
qx




x¼^xðiÞ
ð3:54Þ
In each iteration, the variable i is incremented. The iterative process
stops if the difference between x(i þ 1) and x(i) is smaller than some
predefined threshold. The success of the method depends on whether the
first initial guess is already close enough to the global minimum. If not,
the process will either diverge, or get stuck in a local minimum.
Example 3.8
Estimation of the diameter of a blood vessel
In vascular X-ray imaging, one of the interesting parameters is the
diameter of blood vessels. This parameter provides information about
a possible constriction. As such, it is an important aspect in cardiolo-
gic diagnosis.
Figure 3.8(a) is a (simulated) X-ray image of a blood vessel of the
coronary circulation. The image quality depends on many factors.
Most important are the low-pass filtered noise (called quantum mot-
tle) and the image blurring due to the image intensifier.
Figure 3.8(b) shows the one-dimensional, vertical cross-section of
the image at a location as indicated by the two black arrows in
Figure 3.8(a). Suppose that our task is to estimate the diameter of the
imaged blood vessel from the given cross-section. Hence, we define
70
PARAMETER ESTIMATION

a measurement vector z whose elements consist of the pixel grey values
along the cross-section.
The parameter of interest is the diameter D. However, other para-
meters might be unknown as well, e.g. the position and orientation of
the blood vessel, the attenuation coefficient or the intensity of the
X-ray source. This example will be confined to the case where the only
unknown parameters are the diameter D and the position y0 of the
image blood vessel in the cross-section. Thus, the parameter vector is
4
4.5
5
5.5
6
0.2
0.3
0.4
0.5
0.6
20
40
60
80
100
D (mm)
y0 (mm)
(c)
(b)
0
2
4
6
8
10
295
300
305
310
315
320
y (mm)
z
observed
fitted
(a)
|| ||2
ε
Figure 3.8
LS estimation of the diameter D and the position y0 of a blood vessel. (a)
X-ray image of the blood vessel. (b) Cross-section of the image together with fitted
profile. (c) The sum of least squared errors as a function of the diameter and the
position
DATA FITTING
71

two-dimensional: xT ¼ [ D
y0 ]. Since all other parameters are
assumed to be known, a radiometric model can be worked out to a
measurement function h(x) which quantitatively predicts the cross-
section, and thus also the measurement vector z for any value of the
parameter vector x.
With this measurement function it is straightforward to calculate the
LS norm
e
k k2
2 for a couple of values of x. Figure 3.8(c) is a graphical
representation of that. It appears that the minimum of e
k k2
2 is obtained
if ^DLS ¼ 0:42 mm. The true diameter is D ¼ 0:40 mm. The thus
obtained fitted cross-section is also shown in Figure 3.8(b).
Note that the LS norm in Figure 3.8(c) is a smooth function of x.
Hence, the convergence region of a numerical optimizer will be large.
3.3.2
Fitting using a robust error norm
Suppose that the measurement vector in an LS estimator has a few
number of elements with large measurement errors, the so-called out-
liers. The influence of an outlier is much larger than the one of the others
because the LS estimator weights the errors quadraticly. Consequently,
the robustness of LS estimation is poor.
Much can be improved if the influence is bounded in one way or
another. This is exactly the general idea of applying a robust error norm.
Instead of using the sum of squared differences, the objective function of
(3.48) becomes:
e
k krobust¼
X
N1
n¼0
ð"nÞ ¼
X
N1
n¼0
ðzn  hnð^xÞÞ
ð3:55Þ
(:) measures the size of each individual residual zn  hn(^x). This meas-
ure should be selected such that above a given level of "n its influence is
ruled out. In addition, one would like to have (:) being smooth so that
numerical optimization of e
k krobust is not too difficult. A suitable choice
(among others) is the so-called Geman–McClure error norm:
ð"Þ ¼
"2
"2 þ 2
ð3:56Þ
A graphical representation of this function and its derivative is shown in
Figure 3.9. The parameter  is a soft threshold value. For values of e
smaller than about , the function follows the LS norm. For values larger
than , the function gets saturated. Consequently, for small values of e the
72
PARAMETER ESTIMATION

derivative e(e) ¼ q e
k krobust=qx of (:) is nearly a constant. But for large
values of e, i.e. for outliers, it becomes nearly zero. Therefore, in a
Gauss–Newton style of optimization, the Jacobian matrix is virtually
zero for outliers. Only residuals that are about as large as  or smaller
than that play a role.
Example 3.9
Robust estimation of the diameter of a blood vessel
If in example 3.8 the diameter must be estimated near the bifurcation
(as indicated in Figure 3.8(a) by the white arrows) a large modelling
error occurs because of the branching vessel. See the cross-section in
Figure 3.10(a). These modelling errors are large compared to the
noise and they should be considered as outliers. However, Figure
3.10(b) shows that the error landscape
e
k k2
2 has its minimum at
^DLS ¼ 0:50 mm. The true value is D ¼ 0:40 mm. Furthermore, the
minimum is less pronounced than the one in Figure 3.8(c), and there-
fore also less stable.
Note also that in Figure 3.10(a) the position found by the LS estimator
is in the middle between the two true positions of the two vessels.
Figure 3.11 shows the improvements that are obtained by applying
a robust error norm. The threshold  is selected just above the noise
level. For this setting, the error landscape clearly shows two pro-
nounced minima corresponding to the two blood vessels. The global
minimum is reached at ^Drobust ¼ 0:44 mm. The estimated position
now clearly corresponds to one of the two blood vessels as shown in
Figure 3.11(a).
ε
ε2
ρ(ε)
+σ
–σ
ε
2ε
ρε(ε)
+σ
–σ
Figure 3.9
A robust error norm and its derivative
DATA FITTING
73

3.3.3
Regression
Regression is the act of deriving an empirical function from a set of
experimental data. Regression analysis considers the situation involving
pairs of measurements (t, z). The variable t is regarded as a measurement
without any appreciable error. t is called the independent variable.
0
(a)
2
4
6
8
10
295
300
305
310
315
320
y (mm)
z
observed
fitted
4
(b)
4.5
5
5.5
6
0.2
0.3
0.4
0.5
0.6
40
60
80
100
D (mm)
||ε||2
y0(mm)
Figure 3.10
LS estimation of the diameter D and the position y0. (a) Cross-section
of the image together with a profile fitted with the LS norm. (b) The LS norm as a
function of the diameter and the position
0
2
4
6
8
10
295
300
305
310
315
320
y (mm)
z
observed
fitted
4
(b)
(a)
4.5
5
5.5
6
0.2
0.3
0.4
0.5
0.6
800
1000
1200
1400
1600
D (mm)
||ε||robust
y0 (mm)
Figure 3.11
Robust estimation of the diameter D and the position y0. (a) Cross-
section of the image together with a profile fitted with a robust error norm. (b) The
robust error norm as a function of the diameter and the position
74
PARAMETER ESTIMATION

We assume that some empirical function f(:) is chosen that (hopefully)
can predict z from the independent variable t. Furthermore, a parameter
vector x can be used to control the behaviour of f(:). Hence, the model is:
z ¼ fðt; xÞ þ "
ð3:57Þ
f(:,:) is the regression curve, and " represents the residual, i.e. the part of
z that cannot be predicted by f(:,:). Such a residual can originate from
sensor noise (or other sources of randomness) which makes the predic-
tion uncertain, but it can also be caused by an inadequate choice of the
regression curve.
The goal of regression is to determine an estimate ^x of the parameter
vector x based on N observations (tn, zn), n ¼ 0, . . . ,N  1 such that the
residuals "n are as small as possible. We can stack the observations zn in
a vector z. Using (3.57), the problem of finding ^x can be transformed to
the standard form of (3.47):
z ¼ hð^xÞ þ e
with:
z¼
def
z0
..
.
zN1
2
64
3
75
hðxÞ¼
def
fðt0; xÞ
...
fðtN1; xÞ
2
64
3
75
e ¼
def
"0
..
.
"N1
2
64
3
75
ð3:58Þ
where e is the vector that embodies the residuals "n.
Since the model is in the standard form, x can be estimated with a least
squares approach as in Section 3.3.1. Alternatively, we use a robust error
norm as defined in Section 3.3.2. The minimization of such a norm is
called robust regression analysis.
In the simplest case, the regression curve f(t, x) is linear in x. With that,
the model becomes of the form z ¼ H^x þ e, and thus, the solution of
(3.51) applies. As an example, we consider polynomial regression for
which the regression curve is a polynomial of order M  1:
fðt; xÞ ¼ x0 þ x1t þ 	 	 	 þ xM1tM1
ð3:59Þ
If, for instance, M ¼ 3, then the regression curve is a parabola described
by three parameters. These parameters can be found by least squares
estimation using the following model:
z ¼
1
t0
t2
0
1
t1
t2
1
..
.
..
.
..
.
1
tN1
t2
N1
2
6664
3
7775^x þ e
ð3:60Þ
DATA FITTING
75

Example 3.10
The calibration curve of a level sensor
In this example, the goal is to determine a calibration curve of a level
sensor to be used in a water tank. For that purpose, a second meas-
urement system is available with a much higher precision than the
‘sensor under test’. The measurement results of the second system
serve as a reference. Figure 3.12 shows the observed errors of the
sensor versus the reference values. Here, 46 pairs of measurements
are shown. A zero order (fit with a constant), a first order (linear
fit) and a tenth order polynomial are fitted to the data. As can be
seen, the constant fit appears to be inadequate for describing the
data (the model is too simple). The first order polynomial describes
the data reasonably well, and is also suitable for extrapolation. The
tenth order polynomial follows the measurement points better, but
also the noise. It cannot be used for extrapolation because it is an
example of overfitting the data. This occurs whenever the model
has too many degrees of freedom compared to the number of data
samples.
Listing 3.3 illustrates how to fit and evaluate polynomials using
MATLAB’s polyfit() and polyval() routines.
0
5
10
15
20
25
30
– 0.25
– 0.2
– 0.15
– 0.1
– 0.05
0
0.05
0.1
0.15
0.2
error (cm)
reference level (cm)
observed errors
M=1
M=2
M=10
Figure 3.12
Determination of a calibration curve by means of polynomial
regression
76
PARAMETER ESTIMATION

Listing 3.3
MATLAB code for polynomial regression.
load levelsensor;
% Load dataset (t,z)
figure; clf; plot(t,z,‘k.’); hold on;
% Plot it
y ¼ 0:0.2:30; M ¼ [ 1 2 10 ]; plotstring ¼ {‘k--’,‘k-’,‘k:’};
for m ¼ 1:3
p ¼ polyfit(t,z,M(m)-1);
% Fit polynomial
z_hat ¼ polyval(p,y);
% Calculate plot points
plot(y,z_hat,plotstring{m});
% and plot them
end;
axis([0 30 0.3 0.2]);
3.4
OVERVIEW OF THE FAMILY OF ESTIMATORS
The chapter concludes with the overview shown in Figure 3.13. Two
main approaches have been discussed, the Bayes estimators and the
fitting techniques. Both approaches are based on the minimization of
an objective function. The difference is that with Bayes, the objective
function is defined in the parameter domain, whereas with fitting tech-
niques, the objective function is defined in the measurement domain.
Another difference is that the Bayes approach has a probabilistic con-
text, whereas the approach of fitting lacks such a context.
Within the family of Bayes estimators we have discussed two estima-
tors derived from two cost functions. The quadratic cost function leads
to MMSE (minimum variance) estimation. The cost function is such that
small errors are regarded as unimportant, while larger errors are con-
sidered more and more serious. The solution is found as the conditional
mean, i.e. the expectation of the posterior probability density. The
estimator is unbiased.
If the MMSE estimator is constrained to be linear, the solution can be
expressed entirely in terms of first and second order moments. If, in
addition, the sensory system is linear with additive, uncorrelated noise, a
simple form of the estimator appears that is used in Kalman filtering.
This form is sometimes referred to as the Kalman form.
The other Bayes estimator is based on the uniform cost function. This
cost function is such that the damage of small and large errors are
equally weighted. It leads to MAP estimation. The solution appears to
be the mode of the posterior probability density. The estimator is not
guaranteed to be unbiased.
OVERVIEW OF THE FAMILY OF ESTIMATORS
77

It is remarkable that although the quadratic cost function and the unit
cost function differ a lot, the solutions are identical provided that the
posterior density is uni-modal and symmetric. An example of this occurs
when the prior probability density and conditional probability density
are both Gaussian. In that case the posterior probability density is
Gaussian too.
If no prior knowledge about the parameter is available, one can use
the ML estimator. Another possibility is to resort to fitting techniques, of
which the LS estimator is most popular. The ML estimator is essentially
a MAP estimator with uniform prior probability. Under the assumptions
of normal distributed sensor noise the ML solution and the LS solution
are identical. If, in addition, the sensors are linear, the ML and LS
estimator become the pseudo inverse.
Bayes estimation
MMSE
estimation
=
minimum variance
estimation
MAP
estimation
fitting
techniques
unbiased linear
MMSE
estimation
LS
estimation
robust
estimation
ML
estimation
regression
analysis
pseudo
inverse
MMSE = minimum mean square error
MAP 
= maximum a posteriori
ML  
= maximum likelihood
LS 
= least squares
Kalman
form
robust
regression
analysis
quadratic
cost function
uniform
cost function
prior knowledge
not available
linearity
constraints
Gaussian
densities
linear
sensors
+
additive
noise
sum of
squared
difference
norm
robust
error norm
function
fitting
function
fitting
linear
sensors
Gaussian
white noise
Gaussian
white noise +
linear sensors
Figure 3.13
A family tree of estimators
78
PARAMETER ESTIMATION

A robust estimator is one which can cope with outliers in the measure-
ments. Such an estimator can be achieved by application of a robust
error norm.
3.5
SELECTED BIBLIOGRAPHY
Kay, S.M., Fundamentals of Statistical Signal Processing – Estimation Theory, Prentice
Hall, New Jersey, 1993.
Papoulis, A., Probability, Random Variables and Stochastic Processes, Third Edition,
McGraw-Hill, New York, 1991.
Sorenson, H.W., Parameter Estimation: Principles and Problems, Marcel Dekker, New
York, 1980.
Tarantola, A., Inverse Problem Theory: Methods for Data Fitting and Model Parameter
Estimation, Elsevier, New York, 1987.
Van Trees, H.L., Detection, Estimation and Modulation Theory, Vol. 1, Wiley, New
York, 1968.
3.6
EXERCISES
1. Prove that the linear MMSE estimator, whose form is ^xlMMSE(z) ¼ Kz, is found as:
K ¼ MxzM1
z
with
Mxz ¼
defE xzT


and
Mz ¼
defE zzT


ðÞ
2. In the Gaussian case, in Section 3.1.3, we silently assumed that the covariance matrices
Cx and Cv are invertible. What can be said about the elements of x if Cx is singular?
And what about the elements of v if Cv is singular? What must be done to avoid such
a situation? ()
3. Prove that, in Section 3.1.3, the posterior density is Gaussian, and prove equation
(3.20). () Hint: use equation (3.2), and expand the argument of the exponential.
4. Prove equation (3.32). (0)
5. Use the matrix inversion lemma (b.10) to prove that the form given in (3.20):
^xMMSEðzÞ ¼ Ce C1
x mx þ HTC1
v z


with
Ce ¼ HTC1
v H þ C1
x

1
is equivalent to the Kalman form given in (3.33). ()
6. Explain why (3.42) cannot be replaced by Ce ¼
R
Ce(x)p(x)dx. ()
7. Prove that the unbiased linear MMSE estimator is indeed unbiased. (0)
8. Given that the random variable z is binominally distributed (Appendix C.1.3) with
parameters (x,M). x is the probability of success of a single trial. M is the number of
independent trials. z is the number of successes in M trials. The parameter x must be
estimated from an observed value of z.
. Develop the ML estimator for x. (0)
. What is the bias and the variance of this estimator? (0)
EXERCISES
79

9. If, without having observed z, the parameter x in exercise 8 is uniformly distributed
between 0 and 1, what will be the posterior density p(xjz) of x? Develop the MMSE
estimator and the MAP estimator for this case. What will be the bias and the variance
of these estimators? ()
10. A Geiger counter is an instrument that measures radioactivity. Essentially, it counts
the number of events (arrival of nuclear particles) within a given period of time.
These numbers are Poisson distributed with expectation , i.e. the mean number of
events within the period. z is the counted number of events within a period. We
assume that  is uniform distributed between 0 and L.
. Develop the ML estimator for . (0)
. Develop the MAP estimator for . What is the bias and the variance of the ML
estimator? ()
. Show that the ML estimator is absolutely unbiased, and that the MAP estimator is
biased. ()
. Give an expression for the (overall) variance of the ML estimator. (0)
80
PARAMETER ESTIMATION

4
State Estimation
The theme of the previous two chapters will now be extended to the case
in which the variables of interest change over time. These variables can
be either real-valued vectors (as in Chapter 3), or discrete class variables
that only cover a finite number of symbols (as in Chapter 2). In both
cases, the variables of interest are called state variables.
The design of a state estimator is based on a state space model that
describes the underlying physical process of the application. For
instance, in a tracking application, the variables of interest are the
position and velocity of a moving object. The state space model gives
the connection between the velocity and the position (which, in this case,
is a kinematical relation). Variables, like position and velocity, are real
numbers. Such variables are called continuous states.
The design of a state estimator is also based on a measurement model
that describes how the data of a sensory system depend on the state
variables. For instance, in a radar tracking system, the measurements are
the azimuth and range of the object. Here, the measurements are directly
related to the two-dimensional position of the object if represented in
polar coordinates.
The estimation of a dynamic class variable, i.e. a discrete state variable
is sometimes called mode estimation or labelling. An example is in
speech recognition where – for the recognition of a word – a sequence
of phonetic classes must be estimated from a sequence of acoustic
features. Here too, the analysis is based on a state space model and a
Classification, Parameter Estimation and State Estimation: An Engineering Approach using MATLAB
F. van der Heijden, R.P.W. Duin, D. de Ridder and D.M.J. Tax
 2004 John Wiley & Sons, Ltd
ISBN: 0-470-09013-8

measurement model (in fact, each possible word has its own state space
model).
The outline of the chapter is as follows. Section 4.1 gives a framework
for estimation in dynamic systems. It introduces the various concepts,
notations and mathematical models. Next, it presents a general scheme
to obtain the optimal solution. In practice, however, such a general
scheme is of less value because of the computational complexity involved
when trying to implement the solution directly. Therefore, the general
approach needs to be worked out for different cases. Section 4.2 is
devoted to the case of continuous state variables. Practical solutions
are feasible if the models are linear-Gaussian (Section 4.2.1). If the
model is not linear, one can resort to suboptimal methods (Section 4.2.2).
Section 4.3 deals with the discrete state case. The chapter finalizes
with Section 4.4 which contains an introduction to particle filtering.
This
technique
can
handle
nonlinear
and
non-Gaussian
models
covering the continuous and the discrete case, and even mixed cases
(i.e. combinations of continuous and discrete states).
The chapter confines itself to the theoretical aspects of state estima-
tion. Practical issues, like implementations, deployment, consistency
checks are dealt with in Chapter 8. The use of MATLAB is also deferred
to that chapter.
4.1
A GENERAL FRAMEWORK FOR ONLINE
ESTIMATION
Usually, the estimation problem is divided into three paradigms:
. online estimation (optimal filtering)
. prediction
. retrodiction (smoothing, offline estimation).
Online estimation is the estimation of the present state using all the
measurements that are available, i.e. all measurements up to the present
time. Prediction is the estimation of future states. Retrodiction is the
estimation of past states.
This section sets up a framework for the online estimation of the states
of time-discrete processes. Of course, most physical processes evolve in
the continuous time. Nevertheless, we will assume that these systems
can be described adequately by a model where the continuous time is
reduced to a sequence of specific times. Methods for the conversion from
82
STATE ESTIMATION

time-continuous to time-discrete models are described in many text-
books, for instance, on control engineering.
4.1.1
Models
We assume that the continuous time t is equidistantly sampled with
period . The discrete time index is denoted by an integer variable i.
Hence, the moments of sampling are ti ¼ i. Furthermore, we assume
that the estimation problem starts at t ¼ 0. Thus, i is a non-negative
integer denoting the discrete time.
The state space model
The state at time i is denoted by x(i) 2 X where X is the state space. For
discrete states, X ¼  ¼ f!1, . . . , !Kg where !k is the k-th symbol (label,
or class) out of K possible classes. For real-valued vectors with dimen-
sion M, we have X ¼ RM.
Suppose for a moment that we have observed the state of a process
during its whole history, i.e. from the beginning of time up to the
present. In other words, the sequence of states x(0), x(1), . . . , x(i) are
observed and as such fully known. i denotes the present time. In add-
ition, suppose that – using this sequence – we want to estimate (predict)
the next state x(i þ 1). Assuming that the states can be modelled as
random variables, we need to evaluate the conditional probability dens-
ity1 p(x(i þ 1)jx(0), x(1), . . . , x(i)). Once this probability density is
known, the application of the theory in Chapters 2 and 3 will provide
the optimal estimate of x(i þ 1). For instance, if X is a real-valued vector
space, the Bayes estimator from Chapter 3 provides the best prediction
of the next state (the density p(x(i þ 1)jx(0), x(1), . . . , x(i)) must be used
instead of the posterior density).
Unfortunately, the evaluation of p(x(i þ 1)jx(0), x(1), . . . , x(i)) is a
nasty task because it is not clear how to establish such a density in
real-world problems. Things become much easier if we succeed to define
the state such that the so-called Markov condition applies:
pðxði þ 1Þjxð0Þ; xð1Þ; . . . ; xðiÞÞ ¼ pðxði þ 1ÞjxðiÞÞ
ð4:1Þ
1 For the finite-state case, the probability densities transform into probabilities, and appropriate
summations replace the integrals.
A GENERAL FRAMEWORK FOR ONLINE ESTIMATION
83

The probability of x(i þ 1) depends solely on x(i) and not on the past
states. In order to predict x(i þ 1), the knowledge of the full history is
not needed. It suffices to know the present state. If the Markov condition
applies, the state of a physical process is a summary of the history of the
process.
Example 4.1
The density of a substance mixed with a liquid
Mixing and diluting are tasks frequently encountered in the food
industries, paper industry, cement industry, and so. One of the param-
eters of interest during the production process is the density D(t) of
some substance. It is defined as the fraction of the volume of the mix
that is made up by the substance.
Accurate models for these production processes soon involve a
large number of state variables. Figure 4.1 is a simplified view of the
process. It is made up by two real-valued state variables and one
discrete state. The volume V(t) of the liquid in the barrel is regulated
by an on/off feedback control of the input flow f1(t) of the liquid:
f1(t) ¼ f0x(t). The on/off switch is represented by the discrete state
variable x(t) 2 f0,1g. A hysteresis mechanism using a level detector
(LT) prevents jitter of the switch. x(t) switches to the ‘on’ state (¼1) if
V(t) < Vlow, and switches back to the ‘off’ state (¼0) if V(t) > Vhigh.
The rate of change of the volume is
_V(t) ¼ f1(t) þ f2(t)  f3(t)
with f2(t) the volume flow of the substance, and f3(t) the output
volume flow of the mix. We assume that the output flow is gov-
erned by Torricelli’s law: f3(t) ¼ c
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
V(t)/Vref
p
. The density is defined
as D(t) ¼ VS(t)/V(t) where VS(t) is the volume of the substance. The
rate of change of VS(t) is:
_VS(t) ¼ f2(t)  D(t)f3(t). After some
LT
x(t)
f1(t)
f3(t)
f2(t)
DT
V(t),D(t)
liquid
substance
3990
4000
4010
4020
4030 volume (litre)
0
2000
400
0.08
0.09
0.1
0.11 density
i∆ (s)
0
5
10
15 flow of substance (litre/s)
0
2000
4000
–0.5
0
0.5
1
1.5 liquid input flow (litre/s)
i∆ (s)
Figure 4.1
A density control system for the process industry
84
STATE ESTIMATION

manipulations
the
following
system
of
differential
equations
appears:
_VðtÞ ¼ f1ðtÞ þ f2ðtÞ  c
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
VðtÞ=Vref
q
_DðtÞ ¼ f2ðtÞð1  DðtÞÞ  f1ðtÞDðtÞ
VðtÞ
ð4:2Þ
A discrete time approximation2 (notation: V(i)  V(i), D(i)  D(i),
and so on) is:
Vði þ 1Þ ’ VðiÞ þ  f0xðiÞ þ f2ðiÞ  c
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
VðiÞ=Vref
q


Dði þ 1Þ ’ DðiÞ   f0xðiÞDðiÞ  f2ðiÞð1  DðiÞÞ
VðiÞ
xði þ 1Þ ¼ xðiÞ ^ :ðVðiÞ > VhighÞ


_ ðVðiÞ < VlowÞ
ð4:3Þ
This
equation
is
of
the
type
x(i þ 1) ¼ f(x(i), u(i), w(i))
with
x(i) ¼ [ V(i) D(i) x(i) ]T. The elements of the vector u(i) are the
known input variables, i.e. the non-random part of f2(i). The vector
w(i) contains the random input, i.e. the random part of f2(i). The
probability density of x(i þ 1) depends on the present state x(i), but
not on the past states.
Figure 4.1 shows a realization of the process. Here, the substance is
added to the volume in chunks with an average volume of 10 litre and
at random points in time.
If the transition probability density p(x(i þ 1)jx(i)) is known together
with the initial probability density p(x(0)), then the probability density
at an arbitrary time can be determined recursively:
pðxði þ 1ÞÞ ¼
Z
xðiÞ2X
pðxði þ 1ÞjxðiÞÞpðxðiÞÞdx for i ¼ 0; 1; . . .
ð4:4Þ
2 The approximation that is used here is _V(t) ’ V((i þ 1))  V(i). The approximation is
only close if  is sufficiently small. Other approximations may be more accurate, but this
subject is outside the scope of the book.
A GENERAL FRAMEWORK FOR ONLINE ESTIMATION
85

The joint probability density of the sequence x(0), x(1), . . . , x(i) follows
readily
pðxð0Þ; . . . ; xðiÞÞ ¼ pðxðiÞjxði  1ÞjÞpðxð0Þ; . . . ; xði  1ÞÞ
¼ pðxð0ÞÞ
Y
i
j¼1
pðxðjÞjxðj  1ÞÞ
ð4:5Þ
The measurement model
In addition to the state space model, we also need a measurement model
that describes the data from the sensor in relation with the state. Suppose
that at moment i the measurement data is z(i) 2 Z where Z is the
measurement space. For the real-valued state variables, the measurement
space is often a real-valued vector space, i.e. Z ¼ RN. For the discrete
case, one often assumes that the measurement space is also finite, i.e.
Z ¼ f#1, . . . , #Ng.
The probabilistic model of the sensory system is fully defined by the
conditional probability density p(z(i)jx(0), . . . , x(i), z(0), . . . , z(i  1)).
We assume that the sequence of measurements starts at time i ¼ 0.
In order to shorten the notation, the sequence of all measurements up to
the present will be denoted by:
ZðiÞ ¼ fzð0Þ; . . . ; zðiÞg
ð4:6Þ
We restrict ourselves to memoryless sensory systems, i.e. systems where
z(i) depends on the value of x(i), but not on previous states nor on
previous measurements. In other words:
pðzðiÞjxð0Þ; . . . ; xðiÞ; Zði  1ÞÞ ¼ pðzðiÞjxðiÞÞ
ð4:7Þ
4.1.2
Optimal online estimation
Figure 4.2 presents an overview of the scheme for the online estimation
of the state. The connotation of the phrase online is that for each time
index i an estimate ^x(i) of x(i) is produced based on Z(i), i.e. based on
all measurements that are available at that time. The crux of optimal
online estimation is to maintain the posterior density p(x(i)jZ(i)) for
86
STATE ESTIMATION

running values of i. This density captures all the available information of
the current state x(i) after having observed the current measurement and
all previous ones. With the availability of the posterior density, the
methods discussed in Chapters 2 and 3 become applicable. The only
work to be done, then, is to adopt an optimality criterion and to work
this out using the posterior density to get the optimal estimate of the
current state.
The maintenance of the posterior density is done efficiently by means
of a recursion. From the posterior density p(x(i)jZ(i)), valid for the
current period i, the density p

x(i þ 1)jZ(i þ 1)

, valid for the next
period i þ 1, is derived. The first step of the recursion cycle is a predic-
tion step. The knowledge about x(i) is extrapolated to knowledge about
x(i þ 1). Using Bayes’ theorem for conditional probabilities in combina-
tion with the Markov condition (4.1), we have:
pðxði þ 1ÞjZðiÞÞ ¼
Z
xðiÞ2X
p

xði þ 1Þ; xðiÞjZðiÞ

dxðiÞ
¼
Z
xðiÞ2X
p

xði þ 1ÞjxðiÞ; ZðiÞ

pðxðiÞjZðiÞÞdxðiÞ
¼
Z
xðiÞ2X
p

xði þ 1ÞjxðiÞ

pðxðiÞjZðiÞÞdxðiÞ
ð4:8Þ
At this point, we increment the counter i, so that p(x(i þ 1)jZ(i)) now
becomes p

x(i)jZ(i  1)

. The increment can be done anywhere in the
state prediction
update
estimation
i: = i+1
criterion
measurements
p(x (i+1)|Z(i))
p(x(i)|Z(i))
z(i)
p(x(0))
i = 0
p(x(i)|Z(i–1))
ˆx(i)
Figure 4.2
An overview of online estimation
A GENERAL FRAMEWORK FOR ONLINE ESTIMATION
87

loop, but the choice to do it at this point leads to a shorter notation of
the second step.
The second step is an update step. The knowledge gained from observ-
ing the measurement z(i) is used to refine the density. Using – once
again – the theorem of Bayes, now in combination with the conditional
density for memoryless sensory systems (4.7):
pðxðiÞjZðiÞÞ ¼ pðxðiÞjZði  1Þ; zðiÞÞ
¼ 1
c pðzðiÞjxðiÞ; Zði  1ÞÞpðxðiÞjZði  1ÞÞ
¼ 1
c pðzðiÞjxðiÞÞpðxðiÞjZði  1ÞÞ
ð4:9Þ
where c is a normalization constant:
c ¼
Z
xðiÞ2X
pðzðiÞjxðiÞÞpðxðiÞjZði  1ÞÞdxðiÞ
ð4:10Þ
The recursion starts with the processing of the first measurement z(0).
The posterior density p(x(0)jZ(0)) is obtained using p(x(0)) as the prior.
The outline for optimal estimation, expressed in (4.8), (4.9) and (4.10),
is useful in the discrete case (where integrals turn into summations). For
continuous states, a direct implementation is difficult for two reasons:
. It requires efficient representations for the N- and M-dimensional
density functions.
. It requires efficient algorithms for the integrations over an
M-dimensional space.
Both requirements are hard to fulfil, especially if M is large. Nonetheless,
many researchers have tried to implement the general scheme. One of the
most successful endeavours has resulted in what is called particle filtering
(see Section 4.4). But first, the discussion will be focused on special cases.
4.2
CONTINUOUS STATE VARIABLES
This session addresses the case when both the state and the measure-
ments are real-valued vectors with dimensions of M and N, respectively.
The starting point is the general scheme for online estimation discussed
in the previous section, and illustrated in Figure 4.2. As said before, in
88
STATE ESTIMATION

general, a direct implementation of the scheme is difficult. Fortunately,
there are circumstances which allow a fast implementation. For instance,
in the special case, where the models are linear and the disturbances have
a normal distribution, an implementation based on an ‘expectation and
covariance matrix’ representation of the probability densities is feasible
(Section 4.2.1). If the models are nonlinear, but the nonlinearity is
smooth, linearization techniques can be applied (Section 4.2.2). If the
models are highly nonlinear, but the dimensions N and M are not too
large, numerical methods are possible (Section 4.2.3 and 4.4).
4.2.1
Optimal online estimation in linear-Gaussian systems
Most literature in optimal estimation in dynamic systems deals with the
particular case in which both the state model and the measurement
model are linear, and the disturbances are Gaussian (the linear-Gaussian
systems). Perhaps the main reason for the popularity is the mathematical
tractability of this case.
Linear-Gaussian state space models
The state model is said to be linear if the transition from one state to the
next can be expressed by a so-called linear system equation (or: linear
state equation, linear plant equation, linear dynamic equation):
xði þ 1Þ ¼ FðiÞxðiÞ þ LðiÞuðiÞ þ wðiÞ
ð4:11Þ
F(i) is the system matrix. It is an M  M matrix where M is the dimen-
sion of the state vector. M is called the order of the system. The vector
u(i) is the control vector (input vector) of dimension L. Usually, the
vector is generated by a controller according to some control law. As
such the input vector is a deterministic signal that is fully known, at least
up to the present. L(i) is the gain matrix of dimension M  L. Sometimes
the matrix is called the distribution matrix as it distributes the control
vector across the elements of the state vector.
w(i) is the process noise (system noise, plant noise). It is a sequence of
random vectors of dimension3 M. The process noise represents the
3 Sometimes the process noise is represented by G(i)w(i) where G(i) is the noise gain matrix.
With that, w(i) is not restricted to have dimension M. Of course, the dimension of G(i) must be
appropriate.
CONTINUOUS STATE VARIABLES
89

unknown influences on the system, for instance, formed by disturbances
from the environment. The process noise can also represent an unknown
input/control signal. Sometimes process noise is also used to take care of
modelling errors. The general assumption is that the process noise is a
white random sequence with normal distribution. The term ‘white’ is
used here to indicate that the expectation is zero and the autocorrelation
is governed by the Kronecker delta function:
E wðiÞ
½
 ¼ 0
E wðiÞwTð jÞ


¼ CwðiÞdði; jÞ
ð4:12Þ
Cw(i) is the covariance matrix of w(i). Since w(i) is supposed to have
a normal distribution with zero mean, Cw(i) defines the density of w(i)
in full.
The initial condition of the state model is given in terms of the
expectation E[x(0)] and the covariance matrix Cx(0). In order to find
out how these parameters of the process propagate to an arbitrary time i,
the state equation (4.11) must be used recursively:
E xði þ 1Þ
½
 ¼ FðiÞE xðiÞ
½
 þ LðiÞuðiÞ
Cxði þ 1Þ ¼ FðiÞCxðiÞFTðiÞ þ CwðiÞ
ð4:13Þ
The first equation follows from E[w(i)] ¼ 0. The second equation uses
the fact that the process noise is white, i.e. E[w(i)wT(j)] ¼ 0 for i 6¼ j. See
(4.12).
If E[x(0)] and Cx(0) are known, then equation (4.13) can be used to
calculate E[x(1)] and Cx(1). From that, by reapplying the (4.13), the next
values, E[x(2)] and Cx(2), can be found, and so on. Thus, the iterative
use of equation (4.13) gives us E[x(i)] and Cx(i) for arbitrary i > 0.
In the special case, where neither F(i) nor Cw(i) depend on i, the state
space model is time invariant. The notation can be shortened then by
dropping the index, i.e. F and Cw. If, in addition, F is stable (the
magnitudes of the eigenvalues of F are all less than one; Appendix
D.3.2), the sequence Cx(i), i ¼ 0, 1, . . . converges to a constant matrix.
The balance in (4.13) is reached when the decrease of Cx(i) due to F
compensates the increase due to Cw. If such is the case, then:
Cx ¼ FCxFT þ Cw
ð4:14Þ
This is the discrete Lyapunov equation.
90
STATE ESTIMATION

Some special state space models
In this section, we introduce some elementary random processes. They
are presented here not only to illustrate the properties of state models
with random inputs, but also because they are often used as building
blocks for models of complicated physical processes.
Random constants
Sometimes it is useful to model static parameters
as states in a dynamic system. In that case, the states do not change in
time:
xði þ 1Þ ¼ xðiÞ
ð4:15Þ
Such a model is useful when the sequential measurements z(i) of x are
processed online so that the estimate of x becomes increasingly accurate
as time proceeds.
First order autoregressive models
A first order autoregressive (AR)
model is of the type
xði þ 1Þ ¼ xðiÞ þ wðiÞ
ð4:16Þ
where w(i) is a white, zero mean, normally distributed sequence
with variance 2
w. In this particular example, 2
x(i)  Cx(i) since x(i)
is a scalar. 2
x(i) can be expressed in closed form: 2
x(1) ¼ 2i2
x(0)þ
(1  2iþ2)2
w=(1  2). The equation holds if  6¼ 1. The system is
stable provided that jj < 1. In that case, the term 2i2
x(0) exponentially
fades out. The second term asymptotically reaches the steady state, i.e.
the solution of the Lyapunov equation:
2
xð1Þ ¼
1
1  2 2
w
ð4:17Þ
If jj > 0, the system is not stable, and both terms grow exponentially.
First order AR models are used to describe slowly fluctuating phe-
nomena. Physically, such phenomena occur when broadband noise is
dampened by a first order system, e.g. mechanical shocks damped by a
mass/dampener system. Processes that involve exponential growth are
also modelled by first AR models. Figure 4.3 shows a realization of a
stable and an unstable AR process.
CONTINUOUS STATE VARIABLES
91

Random walk
Consider the process:
xði þ 1Þ ¼ xðiÞ þ wðiÞ
with
xð0Þ ¼ 0
ð4:18Þ
w(i) is a random sequence of independent increments, þd, and decre-
ments, d; each occurs with probability 1/2. Suppose that after i time
steps, the number of increments is n(i), then the number of decrements is
i  n(i). Thus, x(i)/d ¼ 2n(i)  i. The variable n(i) has a binomial dis-
tribution (Appendix C.1.3) with parameters (i,1/2). Its mean value is 1/2i;
hence E[x(i)] ¼ 0. The variance of n(i) is 1/4i. Therefore, 2
x(i) ¼ id2.
Clearly, 2
x(i) is not limited, and the solution of the Lyapunov equation
does not exist.
According to the central limit theorem (Appendix C.1.4), after about
20 time steps the distribution of x(i) is reasonably well approximated by
a normal distribution. Figure 4.4 shows a realization of a random walk
process.
Random
walk
processes
find
application
in
navigation
problems.
Second order autoregressive models
Second order autoregressive
models are of the type:
xði þ 1Þ ¼ xðiÞ þ xði  1Þ þ wðiÞ
ð4:19Þ
+
w(i )
x(i + 1)
buffer
x(i )
α
0
50
100
–5
0
5
+1σ boundary
–1σ boundary
α = 0.95; σ w = 1
0
50
100
–40
–20
0
20
40
i
+1σ boundary
–1σ boundary
α = 1.02; σ w = 1
a)
c)
b)
Figure 4.3
First order autoregressive models. (a) Schematic diagram of the model.
(b) A stable realization. (c) An unstable realization
92
STATE ESTIMATION

The model can be cast into a state space model by defining
x(i) ¼
def[x(i) x(i  1)]T:
xði þ 1Þ ¼
xði þ 1Þ
xðiÞ

	
¼


1
0

	
xðiÞ þ
wðiÞ
0

	
ð4:20Þ
The eigenvalues of this system are 1/21/2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 þ 4
p
. If 2 > 4, the
system can be regarded as a cascade of two first order AR processes with
two real eigenvalues. However, if 2 < 4, the eigenvalues become
complex and can be written as de2jf with j ¼
ﬃﬃﬃﬃﬃﬃﬃ
1
p
. The magnitude of
the eigenvalues, i.e. the damping, is d ¼
ﬃﬃﬃﬃﬃﬃﬃ

p
. The frequency f is found
by the relation cos 2f ¼ 
j j/(2d). The solution of the Lyapunov equa-
tion is obtained by multiplying (4.19) on both sides by x(i þ 1), x(i) and
x(i  1), and taking the expectation:
E½xði þ 1Þxði þ 1Þ ¼ E½xðiÞxði þ 1Þ þ xði  1Þxði þ 1Þ þ wðiÞxði þ 1Þ
E½xði þ 1ÞxðiÞ ¼ E½xðiÞxðiÞ þ xði  1ÞxðiÞ þ wðiÞxðiÞ
E½xði þ 1Þxði  1Þ ¼ E½xðiÞxði  1Þ þ xði  1Þxði  1Þ þ wðiÞxði  1Þ
+
2
x ¼ 2
xr1 þ 2
xr2 þ 2
w
ð4:21Þ
2
xr1 ¼ 2
x þ 2
xr1
2
xr2 ¼ 2
xr1 þ 2
x
+
r1 ¼

1  
r2 ¼ 2 þ   2
1  
2
x ¼
2
w
1  ar1  r2
0
50
100
–15
–10
–5
0
5
10
15
+1σ boundary
–1σ boundary
i
Figure 4.4
Random walk
CONTINUOUS STATE VARIABLES
93

The equations are valid if the system is in the steady state, i.e.
when 2
x(i) ¼ 2
x(i þ 1) and E[x(i þ 1)x(i)] ¼ E[x(i)x(i  1)]. For this
situation the abbreviated notation 2
x  2
x(1) is used. Furthermore,
rk denotes the autocorrelation between x(i) and x(i þ k). That is,
E[x(i)x(i þ k)] ¼ Cov[x(i)x(i þ k)] ¼ 2
xrk (only valid in the steady state).
See also Section 8.1.5 and Appendix C.2.
Second order AR models are the time-discrete counterparts of second
order differential equations describing physical processes that behave
like a damped oscillator, e.g. a mass/spring/dampener system, a swinging
pendulum, an electrical LCR-circuit, and so on. Figure 4.5 shows a
realization of a second order AR process.
Prediction
Equation (4.13) is the basis for prediction. Suppose that at time i an
unbiased estimate ^x(i) is known together with the associated error
covariance Ce(i). The best predicted value (MMSE) of the state for ‘
samples ahead of i is obtained by the recursive application of (4.13).
The
recursion
starts
with
E[x(i)] ¼ ^x(i)
and
terminates
when
E[x(i þ ‘)] is obtained. The covariance matrix Ce(i) is a measure of
the magnitudes of the random fluctuations of x(i) around ^x(i). As such
it is also a measure of uncertainty. Therefore, the recursive usage of
(4.13) applied to Ce(i) gives Ce(i þ ‘), i.e. the uncertainty of the
prediction. With that, the recursive equations for the prediction
become:
^xði þ ‘ þ 1Þ ¼ Fði þ ‘Þ^xði þ ‘Þ þ Lði þ ‘Þuði þ ‘Þ
Ceði þ ‘ þ 1Þ ¼ Fði þ ‘ÞCeði þ ‘ÞFTði þ ‘Þ þ Cwði þ ‘Þ
ð4:22Þ
0
100
200
–20
–10
0
10
20
+1σ boundary
–1σ boundary
i
d = 0.995
f = 0.1
w
σ 2  = 1
xσ 2
 (∞) = 145.8 
β = – 0.99
α = 1.61
Figure 4.5
Second order autoregressive process
94
STATE ESTIMATION

Example 4.2
Prediction of a swinging pendulum
The mechanical system shown in Figure 4.6 is a pendulum whose pos-
ition is described by the angle (t) and the position of the hinge. The
length R of the arm is constant. The mass m is concentrated at the end.
The hinge moves randomly in the horizontal direction with an acceler-
ation given by a(t). Newton’s law, applied to the geometrical set up, gives:
maðtÞ cos ðtÞ þ mR€ðtÞ ¼ mg sin ðtÞ  mk
R
_ðtÞ
ð4:23Þ
k is a viscous friction constant; g is the gravitation constant. If the
sampling period , and max (jyj) is sufficiently small, the equation can
be transformed to a second order AR process. The following state model,
with x1(i) ¼ (i) and x2(i) ¼ _(i), is equivalent to that AR process:
x1ði þ 1Þ ¼ x1ðiÞ þ x2ðiÞ
x2ði þ 1Þ ¼ x2ðiÞ  
R
gx1ðiÞ þ k
R x2ðiÞ þ aðiÞ


ð4:24Þ
Figure 4.7(a) shows the result of a so-called fixed interval prediction.
The prediction is performed from a fixed point in time (i is fixed), and
with a running lead, that is ‘ ¼ 1, 2, 3, . . . . In Figure 4.7(a), the fixed
point is i  10(s). Assuming that for that i the state is fully known,
^x(i) ¼ x(i) and Ce(i) ¼ 0, predictions for the next states are calculated
and plotted. It can be seen that the prediction error increases with the
lead. For larger leads, the prediction covariance matrix approaches
the state covariance matrix, i.e. Ce(1) ¼ Cx(1).
Figure 4.7(b) shows the results from fixed lead prediction. Here, the
recursions are reinitiated for each i. The lead is fixed and chosen such
that the relative prediction error is 36%.
a (t)
m
θ(t)
R
R = 1.5 (m)
g = 9.8 (m/s2)
k = 0.2 (m2/s)
∆ = 0.01(s)
σa = 1 (m/s2)
Figure 4.6
A swinging pendulum
CONTINUOUS STATE VARIABLES
95

Linear-Gaussian measurement models
A linear measurement model takes the following form:
zðiÞ ¼ HðiÞxðiÞ þ vðiÞ
ð4:25Þ
H(i) is the so-called measurement matrix. It is an N  M matrix. v(i) is
the measurement noise. It is a sequence of random vectors of dimension
N. Obviously, the measurement noise represents the noise sources in the
sensory system. Examples are thermal noise in a sensor and the quant-
ization errors of an AD converter.
The general assumption is that the measurement noise is a zero mean,
white random sequence with normal distribution. In addition, the
sequence is supposed to have no correlation between the measurement
noise and the process noise:
E½vðiÞ ¼ 0
E vðiÞvTð jÞ


¼ CvðiÞdði; jÞ
E vðiÞwTð jÞ


¼ 0
ð4:26Þ
Cv(i) is the covariance matrix of v(i). Cv(i) specifies the density of v(i)
in full.
The sensory system is time invariant if neither H nor Cv depends on i.
(a)
(b)
0
10
20
30
40
50
–0.2
–0.1
0
0.1
0.2
real state and prediction (rad)
i∆ (s)
–10
0
10
20
30
40
–0.2
–0.1
0
0.1
0.2
prediction error (rad)
lead •∆ (s)
0
1
2
3
4
5
0%
50%
100%
σpred /σx
↓
lead •∆ = 2 (s)
lead •∆ (s)
0
10
20
30
40
50
–0.2
–0.1
0
0.1
0.2
real state and prediction (rad)
i∆ (s)
Figure 4.7
Prediction. (a) Fixed interval prediction. (b) Fixed lead prediction
96
STATE ESTIMATION

The discrete Kalman filter
The concepts developed in the previous section are sufficient to trans-
form the general scheme presented in Section 4.1 into a practical solu-
tion. In order to develop the estimator, first the initial condition valid for
i ¼ 0 must be established. In the general case, this condition is defined in
terms of the probability density p(x(0)) for x(0). Assuming a normal
distribution for x(0) it suffices to specify only the expectation E[x(0)]
and the covariance matrix Cx(0). Hence, the assumption is that these
parameters are available. If not, we can set E[x(0)] ¼ 0 and let Cx(0)
approach to infinity, i.e. Cx(0) ! 1I. Such a large covariance matrix
represents the lack of prior knowledge.
The next step is to establish the posterior density p(x(0)jz(0)) from
which the optimal estimate for x(0) follows. At this point, we enter the
loop of Figure 4.2. Hence, we calculate the density p(x(1)jz(0)) of the
next state, and process the measurement z(1) resulting in the updated
density p(x(1)jz(0), z(1)) ¼ p(x(1)jZ(1)). From that, the optimal estimate
for x(1) follows. This procedure has to be iterated for all the next time
cycles.
The representation of all the densities that are involved can be given
in terms of expectations and covariances. The reason is that any linear
combination of Gaussian random vectors yields a vector that is also
Gaussian. Therefore, both p(x(i þ 1)jZ(i)) and p((x(i)jZ(i)) are fully
represented by their expectations and covariances. In order to discrim-
inate between the two situations a new notation is needed. From
now on, the conditional expectation E[x(i)jZ(j)] will be denoted by
x(ijj). It is the expectation associated with the conditional density
p(x(i)jZ(j)). The covariance matrix associated with this density is
denoted by C(ijj).
The update, i.e. the determination of p((x(i)jZ(i)) given p(x(i)j
Z(i  1)), follows from Section 3.1.5 where it has been shown that the
unbiased linear MMSE estimate in the linear-Gaussian case equals the
MMSE estimate, and that this estimate is the conditional expectation.
Application of (3.33) and (3.45) to (4.25) and (4.26) gives:
^zðiÞ ¼ HðiÞxðiji  1Þ
SðiÞ ¼ HðiÞCðiji  1ÞHTðiÞ þ CvðiÞ
KðiÞ ¼ Cðiji  1ÞHTðiÞS1ðiÞ
xði ij Þ ¼ xðiji  1Þ þ KðiÞ zðiÞ  ^zðiÞ
ð
Þ
Cði ij Þ ¼ Cðiji  1Þ  KðiÞSðiÞKTðiÞ
ð4:27Þ
CONTINUOUS STATE VARIABLES
97

The interpretation is as follows. ^z(i) is the predicted measurement. It is an
unbiased estimate of z(i) using all information from the past. The
so-called innovation matrix S(i) represents the uncertainty of the predicted
measurement. The uncertainty is due to two factors: the uncertainty of
x(i) as expressed by C(iji  1), and the uncertainty due to the measure-
ment noise v(i) as expressed by Cv(i). The matrix K(i) is the Kalman gain
matrix. This matrix has large, when S(i) is small and C(iji  1)HT(i) is
large, that is, when the measurements are relatively accurate. When this is
the case, the values in the error covariance matrix C(iji) will be much
smaller than C(iji  1).
The prediction, i.e. the determination of p(x(i þ 1)jZ(i)) given
p((x(i)jZ(i)), boils down to finding out how the expectation x(iji) and
the covariance matrix C(iji) propagate to the next state. Using (4.11) and
(4.13) we have:
xði þ 1jiÞ ¼ FðiÞxðijiÞ þ LðiÞuðiÞ
Cði þ 1jiÞ ¼ FðiÞCðijiÞFTðiÞ þ CwðiÞ
ð4:28Þ
At this point, we increment the counter, and x(i þ 1ji) and C(i þ 1ji)
become x(iji  1) and C(iji  1). These recursive equations are generally
referred to as the discrete Kalman filter (DKF).
In the Gaussian case, it does not matter much which optimality
criterion we select. MMSE estimation, MMAE estimation and MAP
estimation yield the same result, i.e. the conditional mean. Hence, the
final estimate is found as ^x(i) ¼ x(iji). It is an absolute unbiased estimate,
and its covariance matrix is C(iji). Therefore, this matrix is often called
the error covariance matrix.
In the time invariant case, and assuming that the Kalman filter is
stable, the error covariance matrix converges to a constant matrix. In
that case, the innovation matrix and the Kalman gain matrix become
constant as well. The filter is said to be in the steady state. The steady
state condition simply implies that C(i þ 1ji) ¼ C(iji  1). If the notation
for this matrix is shortened to P, then (4.28) and (4.27) lead to the
following equation:
P ¼ FPFT þ Cw  FPHT HPHT þ Cv

1HPFT
ð4:29Þ
The equation is known as the discrete algebraic Ricatti equation.
Usually, it is solved numerically. Its solution implicitly defines the steady
state solution for S, K and C.
98
STATE ESTIMATION

Example 4.3
Application to the swinging pendulum
In this example we reconsider the mechanical system shown in
Figure 4.6, and described in Example 4.2. Suppose a gyroscope meas-
ures the angular speed _(t) at regular intervals of 0:4 s. The discrete
model in Example 4.2 uses a sampling period of  ¼ 0:01 s. We could
increase the sampling period to 0:4 s in order to match it with the
sampling period of the measurements, but then the applied discrete
approximation would be poor. Instead, we model the measurements
with a time variant model: z(i) ¼ H(i)x(i) þ v(i) where both H(i) and
Cv(i) are always zero except for those i that are multiples of 40:
HðiÞ ¼
½0 1
if modði; 40Þ ¼ 0
0
elsewhere

ð4:30Þ
The effect of such a measurement matrix is that during 39 consecutive
cycles of the loop only predictions take place. During these cycles
H(i) ¼ 0, and consequently the Kalman gains are zero. The corres-
ponding updates would have no effect, and can be skipped. Only
during each 40th cycle H(i) 6¼ 0, and a useful update takes place.
Figure4.8showsmeasurementsobtainedfromtheswingingpendulum.
The variance of the measurement noise is 2
v  Cv(i) ¼ 0:12 (rad2/s2).
The filter is initiated with x(0) ¼ 0 and with Cx(0) ! 1. The figure also
shows the second element of the Kalman gain matrix, e.g. K2,1(i) (the first
0
10
20
30
40
50
– 0.2
– 0.1
0
0.1
0.2
real state and estimate (rad)
0
0.5
1
K2,1(i)
0
10
20
30
40
50
– 0.2
0
0.2
estimation error (rad)
i∆ (s)
0
10
20
30
40
50
– 0.2
– 0.1
0
0.1
0.2
real state (rad)
0
10
20
30
40
50
– 0.4
– 0.2
0
0.2
0.4
measurements (rad/s)
i∆ (s)
Figure 4.8
Discrete Kalman filtering
CONTINUOUS STATE VARIABLES
99

one is much smaller, but follows the same pattern). It can be seen that
after about 3 (s) the Kalman gain (and the error covariance matrix)
remain constant. The filter has reached its steady state.
4.2.2
Suboptimal solutions for nonlinear systems
This section extends the discussion on state estimation to the more
general case of nonlinear systems and nonlinear measurement functions:
xði þ 1Þ ¼ f xðiÞ; uðiÞ; i
ð
Þ þ wðiÞ
zðiÞ ¼ h xðiÞ; i
ð
Þ þ vðiÞ
ð4:31Þ
The vector f(, , ) is a nonlinear, time variant function of the state x(i)
and the control vector u(i). The control vector is a deterministic signal.
Since u(i) is fully known, it only causes an explicit time dependency in
f(, , ). Without loss of generality, the notation can be shortened to
f(x(i),i), because such an explicit time dependency is already implied in
that shorter notation. If no confusion can occur, the abbreviation f(x(i))
will be used instead of f(x(i), i) even if the system does depend on time.
As before, w(i) is the process noise. It is modelled as zero mean, Gaussian
white noise with covariance matrix Cw(i). The vector h( ,) is a non-
linear measurement function. Here too, if no confusion is possible, the
abbreviation h(x(i)) will be used covering both the time variant and the
time invariant case. v(i) represents the measurement noise, modelled as
zero mean, Gaussian white noise with covariance matrix Cv(i).
Any Gaussian random vector that undergoes a linear operation retains its
Gaussian distribution. A linear operator only affects the expectation and the
covariance matrix of that vector. This property is the basis of the Kalman
filter. It is applicable to linear-Gaussian systems, and it permits a solution
that is entirely expressed in terms of expectations and covariance matrices.
However, the property does not hold for nonlinear operations. In nonlinear
systems, the state vectors and the measurement vectors are not Gaussian
distributed, even though the process noise and the measurement noise might
be. Consequently, the expectation and the covariance matrix do not fully
specify the probability density of the state vector. The question is then how
to determine this non-Gaussian density, and how to represent it in an
economical way. Unfortunately, no general answer exists to this question.
This section seeks the answer by assuming that the nonlinearities of the
system are smooth enough to allow linear or quadratic approximations.
100
STATE ESTIMATION

Using these approximations, Kalman-like filters become within reach.
These solutions are suboptimal since there is no guarantee that the
approximations are close.
An obvious way to get the approximations is by application of a
Taylor series expansion of the functions. Ignorance of the higher order
terms of the expansion gives the desired approximation. The Taylor
series exists by virtue of the assumed smoothness of the nonlinear func-
tion; it does not work out if the nonlinearity is a discontinuity, i.e.
saturation, dead zone, hysteresis, and so on. The Taylor series expan-
sions of the system equations are as follows:
fðx þ eÞ ¼ fðxÞ þ FðxÞe þ 1
2
X
M1
m¼0
emeTFm
xxðxÞe þ HOT
hðx þ eÞ ¼ hðxÞ þ HðxÞe þ 1
2
X
N1
n¼0
eneTHn
xxðxÞe þ HOT
ð4:32Þ
em is the Cartesian basis vector with appropriate dimension. The m-th
element of em is one; the other are zeros. em can be used to select the m-th
element of a vector: eT
mx ¼ xm. F(x) and H(x) are Jacobian matrices. Fm
xx(x)
and Hn
xx(x) are Hessian matrices. These matrices are defined in Appendix
B.4. HOT are the higher order terms. The quadratic approximation arises
if the higher order terms are ignored. If, in addition, the quadratic term is
ignored, the approximation becomes linear, e.g. f(x þ e) ﬃf(x) þ F(x)e.
The linearized Kalman filter
The simplest approximation occurs when the system equations are lin-
earized around some fixed value x
¼ of x(i). This approximation is useful if
the system is time invariant and stable, and if the states swing around an
equilibrium state. Such a state is the solution of:
x
¼ ¼ fðx
¼Þ
ð4:33Þ
Defining
e(i) ¼ x(i)  x
¼,
the
linear
approximation
of
the
state
equation (4.31) becomes x(i þ 1) ﬃf(x
¼) þ F(x
¼)e(i) þ w(i). After some
manipulations:
xði þ 1Þ ﬃFðx
¼ÞxðiÞ þ
I  Fðx
¼Þ


x
¼ þ wðiÞ
zðiÞ ﬃhðx
¼Þ þ Hðx
¼Þ xðiÞ  x
¼


þ vðiÞ
ð4:34Þ
CONTINUOUS STATE VARIABLES
101

By interpreting the term (I  F(x
¼))x
¼ as a constant control input, and
by compensating the offset term h(x
¼)  H(x
¼)x
¼ in the measurement
vector, these equations become equivalent to (4.11) and (4.25). This
allows the direct application of the DKF as given in (4.28) and (4.27).
Many practical implementations of the discrete Kalman filter are
inherently linearized Kalman filters because physical processes are sel-
dom exactly linear, and often a linear model is only an approximation of
the real process.
Example 4.4
The swinging pendulum
The swinging pendulum from Example 4.2 is described by (4.23):
maðtÞ cos ðtÞ þ mR€ðtÞ ¼ mg sin ðtÞ  mk
R
_ðtÞ
This equation is transformed in the linear model given in (4.24).
In fact, this is a linearized model derived from:
x1ði þ 1Þ ¼ x1ðiÞ þ x2ðiÞ
x2ði þ 1Þ ¼ x2ðiÞ  
R
g sin x1ðiÞ þ k
R x2ðiÞ þ aðiÞ cos x1ðiÞ


ð4:35Þ
The equilibrium for a(i) ¼ 0 is x
¼ ¼ 0. The linearized model is
obtained by equating sin x1(i) ﬃx1(i) and cos x1(i) ﬃ1.
Example 4.5
A linearized model for volume density estimation
In Section 4.1.1 we introduced the nonlinear, non-Gaussian problem
of the volume density estimation of a mix in the process industry
(Example 4.1). The model included a discrete state variable to
describe the on/off regulation of the input flow. We will now replace
this model by a linear feedback mechanism:
Vði þ 1Þ ’ VðiÞ þ  ðV0  VðiÞÞ þ w1ðiÞ þ f 2

þw2ðiÞ  c
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
VðiÞ=Vref
q

Dði þ 1Þ ’ DðiÞ
 
ðV0  VðiÞÞ þ w1ðiÞ
ð
ÞDðiÞ  f 2 þ w2ðiÞ


1  DðiÞ
ð
Þ
VðiÞ
ð4:36Þ
102
STATE ESTIMATION

The liquid input flow has now been modelled by f1(i) ¼ (V0  V(i))þ
w1(i).
The
constant
V0 ¼ Vref þ (c  f 2)=
(with
Vref ¼ 1/2
(Vhigh þ Vlow)) realizes the correct mean value of V(i). The random part
w1(i) of f1(i) establishes a first order AR model of V(i) which is used as a
rough approximation of the randomness of f1(i). The substance input
flow f2(i), which in Example 4.1 appears as chunks at some discrete points
of time, is now modelled by f 2 þ w2(i), i.e. a continuous flow with some
randomness.
The equilibrium is found as the solution of V(i þ 1) ¼ V(i) and
D(i þ 1) ¼ D(i). The results are:
x
¼ ¼
V
¼
D
¼

	
¼
Vref
f 2

ðf 2 þ ðV0  VrefÞÞ

	
ð4:37Þ
The expressions for the Jacobian matrices are:
FðxÞ ¼
1    þ
c
2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
VVref
p
 
!
0
 D
V  f 2ð1  DÞ  ðV0  VÞD
V2
 
!
1  f 2 þ ðV0  VÞ
V
2
666664
3
777775
ð4:38Þ
GðxÞ ¼ qfðx;wÞ
qw
¼


D
V
1  D
V
2
4
3
5
ð4:39Þ
The considered measurement system consists of two sensors:
. A level sensor that measures the volume V(i) of the barrel.
. A radiation sensor that measures the density D(i) of the output flow.
The latter uses the radiation of some source, e.g. X-rays, that
is absorbed by the fluid. According to Beer–Lambert’s law, Pout ¼
Pin exp (D(i)) where  is a constant depending on the path length
of the ray and on the material. Using an optical detector the
measurement function becomes z ¼ U exp ( D) þ v with U a
constant voltage. With that, the model of the two sensors becomes:
zðiÞ ¼ h xðiÞ
ð
Þ þ vðiÞ ¼
VðiÞ þ v1ðiÞ
U exp DðiÞ
ð
Þ þ v2ðiÞ
"
#
ð4:40Þ
CONTINUOUS STATE VARIABLES
103

with the Jacobian matrix:
HðxÞ ¼
1
0
0
U expðDÞ

	
ð4:41Þ
The best fitted parameters of this model are as follows:
Volume control
Substance flow
Output flow
Measurement system
 ¼ 1 (s)
f 2 ¼ 0:1 (lit/s)
Vref ¼ 4000 (lit)
v1 ¼ 16 (lit)
V0 ¼ 4001 (lit)
w2 ¼ 0:9 (lit/s)
c ¼ 1 (lit/s)
U ¼ 1000 (V)
 ¼ 0:95 (1/s)
 ¼ 100
w1 ¼ 0:1225 (lit/s)
v2 ¼ 0:02 (V)
Figure 4.9 shows the real states (obtained from a simulation using the
model from Example 4.1), observed measurements, estimated states
and estimation errors. It can be seen that:
. The density can only be estimated if the real density is close to the
equilibrium. In every other region, the linearization of the measure-
ment is not accurate enough.
. The estimator is able to estimate the mean volume, but cannot keep
track of the fluctuations. The estimation error of the volume is
much larger than indicated by the 1 boundaries (obtained from
the error covariance matrix). The reason for the inconsistent beha-
viour is that the linear-Gaussian AR model does not fit well enough.
4000
4020
volume (litre)
0.08
0.09
0.1
density
3950
4000
4050
volume measurements (litre)
0
0.2
0.4
density measurements (V)
i∆ (s)
4000
4020
real (thin) and estimated volume (litre)
–20
0
20
volume error (litre)
0.05
0.1
real (thin) and estimated density
0
2000
4000
0
0.02
0.04
density error
i∆ (s)
Figure 4.9
Linearized Kalman filtering applied to the volume density estimation
problem
104
STATE ESTIMATION

The extended Kalman filter
A straightforward generalization of the linearized Kalman filter occurs
when the equilibrium point x
¼ is replaced with a nominal trajectory
x
¼(i), recursively defined as:
x
¼ði þ 1Þ ¼ fðx
¼ðiÞÞ with x
¼ð0Þ ¼ E½xð0Þ
ð4:42Þ
Although the approach is suitable for time variant systems, it is not often
used. There is another approach with almost the same computational
complexity, but with better performance. That approach is the extended
Kalman filter (EKF).
Again, the intention is to keep track of the conditional expectation
x(iji) and the covariance matrix C(iji). In the linear-Gaussian case, where
all distributions are Gaussian, the conditional mean is identical to both
the MMSE estimate (¼ minimum variance estimate), the MMAE esti-
mate, and the MAP estimate; see Section 3.1.3. In the present case, the
distributions are not necessarily Gaussian, and the solutions of the three
estimators do not coincide. The extended Kalman filter provides only an
approximation of the MMSE estimate.
Each cycle of the extended Kalman filter consists of a ‘one step ahead’
prediction and an update, as before. However, the tasks are much more
difficult now, because the calculation of, for instance, the ‘one step
ahead’ expectation:
xðiþ1jiÞ¼E xðiþ1ÞjZðiÞ
½
¼
Z
xðiþ1Þp xðiþ1ÞjZðiÞ
ð
Þdxðiþ1Þ ð4:43Þ
requires the probability density p(x(i þ 1)jZ(i)); see (4.8). But, as said
before, it is not clear how to represent this density. The solution of the
EKF is to apply a linear approximation of the system function. With
that, the ‘one step ahead’ expectation can be expressed entirely in terms
of the moments of p(x(i)jZ(i)).
The EKF uses linear approximations of the system functions using the
first two terms of the Taylor series expansions.4 Suppose that at time i we
have the updated estimate ^x(i) ﬃx(iji) and the associated approximate
4 With more terms in the Taylor series expansion, the approximation becomes more accurate.
For instance, the second order extended Kalman filter uses quadratic approximations based on
the first three terms of the Taylor series expansions. The discussion on the extensions of this
type is beyond the scope of this book. See Bar-Shalom (1993)
CONTINUOUS STATE VARIABLES
105

error covariance matrix C(iji). The word ‘approximate’ expresses the fact
that our estimates are not guaranteed to be unbiased due to the linear
approximations. However, we do assume that the influence of the errors
induced by the linear approximations is small. If the estimation error is
denoted by e(i), then:
xðiÞ ¼ xðijiÞ  eðiÞ
xði þ 1Þ ¼ f xðiÞ
ð
Þ þ wðiÞ
¼ f xðijiÞ  eðiÞ
ð
Þ þ wðiÞ
ﬃf xðijiÞ
ð
Þ  FðxðijiÞÞeðiÞ þ wðiÞ
ð4:44Þ
In these expressions, x(iji) is our estimate.5 It is available at time i, and
as such, it is deterministic. Only e(i) and w(i) are random. Taking the
expectation on both sides of (4.44), we obtain approximate values for
the ‘one step ahead’ prediction:
xði þ 1jiÞ ﬃf xðijiÞ
ð
Þ
Cði þ 1jiÞ ﬃF xðijiÞ
ð
ÞCðijiÞFT xðijiÞ
ð
Þ þ CwðiÞ
ð4:45Þ
We have approximations instead of equalities for two reasons. First,
we neglect the possible bias of x(iji), i.e. a nonzero mean of e(i).
Second, we ignore the higher order terms of the Taylor series
expansion.
Upon incrementing the counter, x(i þ 1ji) becomes x(iji  1), and
we now have to update the prediction x(iji  1) by using a new
measurement z(i) in order to get an approximation of the conditional
mean x(iji). First we calculate the predicted measurement ^z(i) based
on x(iji  1) using a linear approximation of the measurement
function, that is h(^x  e) ﬃh(^x)  H(^x)e. Next, we calculate the
innovation matrix S(i) using that same approximation. Then, we
apply the update according to the same equations as in the linear-
Gaussian case.
5 Up to this point x(iji) has been the exact expectation of x(i) given all measurements up to z(i).
From now on, in this section, x(iji) will denote an approximation of that.
106
STATE ESTIMATION

The predicted measurements are:
xðiÞ ¼ xðiji  1Þ  eðiji  1Þ
^zðiÞ ¼ E zðiÞjxðiji  1Þ
½

¼ E h xðiÞ
ð
Þ þ vðiÞjxðiji  1Þ
½

ﬃE h xðiji  1Þ
ð
Þ  H xðiji  1Þ
ð
Þeðiji  1Þ þ vðiÞ
½

ﬃh xðiji  1Þ
ð
Þ
ð4:46Þ
The approximation is based on the assumption that E[e(iji  1)] ﬃ0,
and on the Taylor series expansion of h(). The innovation matrix
becomes:
SðiÞ ¼ H xðiji  1Þ
ð
ÞCðiji  1ÞHT xðiji  1Þ
ð
Þ þ CvðiÞ
ð4:47Þ
From this point on, the update continues as in the linear-Gaussian case;
see (4.27):
KðiÞ ¼ Cðiji  1ÞHT xðiji  1Þ
ð
ÞS1ðiÞ
xðijiÞ ¼ xðiji  1Þ þ KðiÞ zðiÞ  ^zðiÞ
ð
Þ
CðijiÞ ¼ Cðiji  1Þ  KðiÞSðiÞKTðiÞ
ð4:48Þ
Despite the similarity of the last equation with respect to the linear case,
there is an important difference. In the linear case, the Kalman gain K(i)
depends solely on deterministic parameters: H(i), F(i), Cw(i), Cv(i) and
Cx(0). It does not depend on the data. Therefore, K(i) is fully determin-
istic. It could be calculated in advance instead of online. In the EKF, the
gains depend upon the estimated states x(iji  1) through H(x(iji  1)),
and thus also upon the measurements z(i). As such, the Kalman gains are
random matrices. Two runs of the extended Kalman filter in two
repeated experiments lead to two different sequences of the Kalman
gains. In fact, this randomness of K(i) can cause instable behaviour.
Example 4.6
The extended Kalman filter for volume density
estimation
Application of the EKF to the density estimation problem introduced
in Example 4.1 and represented by a linear-Gaussian model in
Example 4.5 gives the results as shown in Figure 4.10. Compared
with the results of the linearized KF (Figure 4.9) the density errors are
now much better consistent with the 1 boundaries obtained from the
CONTINUOUS STATE VARIABLES
107

error covariance matrix. However, the EKF is still not able to cope
with the non-Gaussian disturbances of the volume.
Note also that the 1 boundaries do not reach a steady state. The
filter remains time variant, even in the long term.
The iterated extended Kalman filter
A further improvement of the update step in the extended Kalman filter
is within reach if the current estimate x(iji) is used to get an improved
linear approximation of the measurement function yielding an improved
predicted measurement ^z(i). In turn, such an improved predicted meas-
urement can improve the current estimate. This suggests an iterative
approach.
Let ^z‘(i) be the predicted measurement in the ‘-th iteration, and let
x‘(i) be the ‘-th improvement of x(iji). The iteration is initiated with
x0(i) ¼ x(iji  1). A naive approach for the calculation of x‘þ1(i) simply
uses a relinearization of h() based on x‘(i).
H‘þ1 ¼ H x‘ðiÞ
ð
Þ
S‘þ1 ¼ H‘þ1Cðiji  1ÞHT
‘þ1 þ CvðiÞ
K‘þ1 ¼ Cðiji  1ÞHT
‘þ1S1
‘þ1
x‘þ1ðiÞ ¼ xðiji  1Þ þ K‘þ1 zðiÞ  h xðiji  1Þ
ð
Þ
ð
Þ
ð4:49Þ
Hopefully, the sequence x‘(i), with ‘ ¼ 0, 1, 2, . . . , converges to a final
solution.
4000
4020
volume (litre)
0.08
0.09
0.1
density
3950
4000
4050
volume measurements (litre)
0
0.2
0.4
density measurements (V)
i∆ (s)
4000
4020
real (thin) and estimated volume (litre)
– 20
0
20
volume error (litre)
0.08
0.09
0.1
real (thick) and estimated density
0
2000
4000
– 0.005
0.005
– 0.005
density error
i∆ (s)
Figure 4.10
Extended Kalman filtering for the volume density estimation problem
108
STATE ESTIMATION

A better approach is the so-called iterated extended Kalman filter
(IEKF). Here, the approximation is made that both the predicted state
and the measurement noise are normally distributed. With that, the
posterior probability density (4.9)
pðxðiÞjZðiÞÞ ¼ 1
c pðxðiÞjZði  1ÞÞpðzðiÞjxðiÞÞ
ð4:50Þ
being the product of two Gaussians, is also a Gaussian. Thus, the MMSE
estimate coincides with the MAP estimate, and the task is now to find
the maximum of p(x(i)jZ(i)). Equivalently, we maximize its logarithm.
After the elimination of the irrelevant constants and factors, it all boils
down to minimizing the following function w.r.t. x:
fðxÞ ¼ 1
2 ðx  xpÞTC1
p ðx  xpÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
comes from pðxðiÞjZði1ÞÞ
þ 1
2 ðz  hðxÞÞTC1
v ðz  hðxÞÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
comes from pðzðiÞjxðiÞÞ
ð4:51Þ
For brevity, the following notation has been used:
xp ¼ xðiji  1Þ
Cp ¼ Cðiji  1Þ
z ¼ zðiÞ
Cv ¼ CvðiÞ
The strategy to find the minimum is to use Newton–Raphson iteration
starting from x0 ¼ x(iji  1). In the ‘-th iteration step, we have already
an estimate x‘1 obtained from the previous step. We expand f(x) in a
second order Taylor series approximation:
fðxÞ ﬃfðx‘1Þ þ ðx  x‘1ÞT qfðx‘1Þ
qx
þ 1
2 ðx  x‘1ÞT q2fðx‘1Þ
qx2
ðx  x‘1Þ
ð4:52Þ
where qf/qx is the gradient and q2f/qx2 is the Hessian of f(x). See
Appendix B.4. The estimate x‘ is the minimum of the approximation.
CONTINUOUS STATE VARIABLES
109

It is found by equating the gradient of the approximation to zero.
Differentiation of (4.52) w.r.t. x gives:
qfðx‘1Þ
qx
þ q2fðx‘1Þ
qx2
ðx  x‘1Þ ¼ 0
+
x‘ ¼ x‘1 
q2fðx‘1Þ
qx2
 
!1qfðx‘1Þ
qx
ð4:53Þ
The Jacobian and Hessian of (4.51), in explicit form, are:
qfðx‘1Þ
qx
¼ C1
p ðx‘1  xpÞ  HT
‘ C1
v ðz  hðx‘1ÞÞ
q2fðx‘1Þ
qx2
¼ C1
p
þ HT
‘ C1
v H‘
ð4:54Þ
where H‘ ¼ H(x‘1) is the Jacobian matrix of h(x) evaluated at x‘1.
Substitution of (4.54) in (4.53) yields the following iteration scheme:
x‘ ¼ x‘1 
C1
p
þ HT
‘ C1
v H‘

1
C1
p
x‘1  xp


h
 HT
‘ C1
v ðz  hðx‘1ÞÞ
i
ð4:55Þ
The result after one iteration, i.e. x1(i), is identical to the ordinary
extended Kalman filter. The required number of further iterations
depends on how fast x‘(i) converges. Convergence is not guaranteed,
but if the algorithm converges, usually a small number of iterations
suffices. Therefore, it is common practice to fix the number of iterations
to some practical number L. The final result is set to the last iteration,
i.e. x(iji) ¼ xL.
Equation (3.44) shows that the factor (C1
p
þ HT
‘ C1
v H‘)1 is the error
covariance matrix associated with x(iji):
CðijiÞ ﬃ
C1
p
þ HT
Lþ1C1
v HLþ1

1
ð4:56Þ
This insight gives another connotation to the last term in (4.55) because,
in fact, the term C(iji)HT
‘ C1
v
can be regarded as the Kalman gain matrix
K‘ during the ‘-th iteration; see (3.20).
110
STATE ESTIMATION

Example 4.7
The iterated EKF for volume density estimation
In the previous example, the EKF was applied to the density estima-
tion problem introduced in Example 4.1. The filter was initiated with
the
equilibrium
state
as
prior
knowledge,
i.e.
E[x(0)] ¼ x
¼ ¼
[4000 0:1]T. Figure 4.11(b) shows the transient which occurs if the
EKF is initiated with E[x(0)] ¼ [2000 0]T. It takes about 40 (s) before
the estimated density reaches the true densities. This slow transient is
due to the fact that in the beginning the linearization is poor. The
iterated EKF is of much help here. Figure 4.11(c) shows the results.
From the first measurement on the estimated density is close to the
real density. There is no transient.
The extended Kalman filter is widely used because for a long period of
time no viable alternative solution existed. Nevertheless, it has numerous
disadvantages:
. It only works well if the various random vectors are approximately
Gaussian distributed. For complicated densities, the expectation-
covariance representation does not suffice.
. It only works well if the nonlinearities of the system are not too
severe because otherwise the Taylor series approximations fail.
Discontinuities are deadly for the EKF’s proper functioning.
. Recalculating the Jacobian matrices at every time step is computa-
tionally expensive.
3950
4000
4050
volume measurements (litre)
0
100
200
– 0.05
0
0.05
0.1
density measurements (V)
i∆ (s)
3980
4000
4020
4040
4060
real (thick) and estimated
volume (litre)
0
100
200
0
0.05
0.1
real (thick) and estimated density
i∆ (s)
3980
3990
4000
4010
4020
real (thick) and estimated
volume (litre)
0
100
200
0.09
0.095
0.1
0.105
0.11
real (thick) and estimated density
i∆ (s)
(a)
(b)
(c)
Figure 4.11
Iterated extended Kalman filtering for the volume density estimation
problem. (a) Measurements (b) Results from the EKF (c) Results from the iterated
EKF (no. of iterations ¼ 20)
CONTINUOUS STATE VARIABLES
111

. In some applications, it is too difficult to find the Jacobian matrix
analytically. In these cases, numerical approximations of the
Jacobian matrix are needed. However, this introduces other types
of problems because now the influence of having approximations
rather than the true values comes in.
. In the EKF, the Kalman gain matrix depends on the data. With that,
the stability of the filter is not assured anymore. Moreover, it is very
hard to analyse the behaviour of the filter.
. The EKF does not guarantee unbiased estimates. In addition, the
calculated error covariance matrices do not necessarily represent
the true error covariances. The analysis of these effects is also hard.
4.2.3
Other filters for nonlinear systems
Besides the extended Kalman filter there are many more types of esti-
mators for nonlinear systems. Particle filtering is a relatively new
approach for the implementation of the scheme depicted in Figure 4.2.
The discussion about particle filtering will be deferred to Section 4.4
because it not only applies to continuous states. Particle filtering is
generally applicable; it covers the nonlinear, non-Gaussian continuous
systems, but also discrete systems and mixed systems.
Statistical linearization is a method comparable with the extended
Kalman filter. But, instead of using a truncated Taylor series approxi-
mation for the nonlinear system functions, a linear approximation
f(x þ e) ﬃf(xÞ þ Fe is used such that the deviation f(x þ e)  f(x)  Fe
is minimized according to a statistical criterion. For instance, one could
try to determine F such that E

kf(x þ e)  f(x)  Fek2
is minimal.
Another method is the unscented Kalman filter. This is a filter midway
between the extended Kalman filter and the particle filter. Assuming
Gaussian densities for x (as in the Kalman filter), the expectation and the
error covariance matrix is represented by means of a number of samples
x(k), that are used to calculate the effects of a nonlinear system function
on the expectation and the error covariance matrix. Unlike the particle
filter, these samples are not randomly selected. Instead the filter uses a
small amount of samples that are carefully selected and that uniquely
represent the covariance matrix. The transformed points, i.e. f(x(k)) are
used to reconstruct the covariance matrix of f(x). Such a reconstruction
is much more accurate than the approximation that is obtained by
means of the truncated Taylor series expansion.
112
STATE ESTIMATION

4.3
DISCRETE STATE VARIABLES
We consider physical processes that are described at any time as being in
one of a finite number of states. Examples of such processes are:
. The sequence of strokes of a tennis player during a game, e.g.
service, backhand-volley, smash, etc.
. The sequence of actions that a tennis player performs during a
particular stroke.
. The different types of manoeuvres of an airplane, e.g. a linear flight,
a turn, a nose dive, etc.
. The sequence of characters in a word, and the sequence of words in
a sentence.
. The sequence of tones of a melody as part of a musical piece.
. The emotional modes of a person: angry, happy, astonished, etc.
These situations are described by a state variable x(i) that can only take a
value from a finite set of states  ¼ f!1, . . . , !Kg.
The task is to determine the sequence of states that a particular process
goes through (or has gone through). For that purpose, at any time meas-
urements z(i) are available. Often, the output of the sensors is real-
valued. But nevertheless we will assume that the measurements take their
values from a finite set. Thus, some sort of discretization must take place
that maps the range of the sensor data onto a finite set Z ¼ f#1, . . . , #Ng.
This section first introduces a state space model that is often used for
discrete state variables, i.e. the hidden Markov model. This model will be
used in the next subsections for online and offline estimation of the states.
4.3.1
Hidden Markov models
A hidden Markov model (HMM) is an instance of the state space model
discussed in Section 4.1.1. It describes a sequence x(i) of discrete states
starting at time i ¼ 0. The sequence is observed by means of measure-
ments z(i) that can only take values from a finite set. The model consists
of the following ingredients:
. The set  containing the K states !k that x(i) can take.
. The set Z containing the N symbols #n that z(i) can take.
. The initial state probability P0(x(0)).
DISCRETE STATE VARIABLES
113

. The state transition probability Pt(x(i)jx(i  1)).
. The observation probability Pz(z(i)jx(i)).
The expression P0(x(0)) with x(0) 2 f1, . . . , Kg denotes the probability
that the random state variable x(0) takes the value !x(0). Thus
P0(k) ¼
defP0(x(0) ¼ !k). Similar conventions hold for other expressions,
like Pt(x(i)jx(i  1)) and Pz(z(i)jx(i)).
The
Markov
condition
of
an
HMM
states
that
P(x(i)jx(0), . . . , x(i  1)), i.e. the probability of x(i) under the condition
of all previous states, equals the transition probability. The assumption
of the validity of the Markov condition leads to a simple, yet powerful
model. Another assumption of the HMM is that the measurements are
memoryless. In other words, z(i) only depends on x(i) and not on the
states at other time points: P(z(j)jx(0), . . . , x(i)) ¼ P(z(j)jx(j)).
An ergodic Markov model is one for which the observation of a single
sequence x(0), x(1), .. . , x(1) suffices to determine all the state transition
probabilities. A suitable technique for that is histogramming, i.e. the deter-
mination of the relative frequency with which a transition occurs; see
Section 5.2.5. A sufficient condition for ergodicity is that all state prob-
abilities are nonzero. In that case, all states are reachable from everywhere
within one time step. Figure 4.12 is an illustration of an ergodic model.
Another type is the so-called left–right model. See Figure 4.13. This
model has the property that the state index k of a sequence is non-
decreasing as time proceeds. Such is the case when Pt(kj‘) ¼ 0 for all
k < ‘. In addition, the sequence always starts with !1 and terminates
Pt (1|1)
Pt (2 | 2)
Pt (1| 2)
Pt (1| 3)
Pt (2 |1)
Pt (2 | 3)
Pt (3| 2)
Pt (3|1)
Pt (3| 3)
ω1
ω2
ω3 
Figure 4.12
A three-state ergodic Markov model
114
STATE ESTIMATION

with !K. Thus, P0(k) ¼ d(k,1) and Pt(kjK) ¼ d(k,K). Sometimes, an
additional constraint is that large jumps are forbidden. Such a constraint
is enforced by letting Pt(kj‘) ¼ 0 for all k > ‘ þ . Left–right models
find applications in processes where the sequence of states must obey
some ordering over time. An example is the stroke of a tennis player. For
instance, the service of the player follows a sequence like: ‘take position
behind the base line’, ‘bring racket over the shoulder behind the back’,
‘bring up the ball with the left arm’, etc.
In a hidden Markov model the state variable x(i) is observable only
through its measurements z(i). Now, suppose that a sequence Z(i) ¼
fz(0), z(1), . . . , z(i)g of measurements has been observed. Some applications
require the numerical evaluation of the probability P(Z(i)) of particular
sequence. An example is the recognition of a stroke of a tennis player.
We can model each type of stroke by an HMM that is specific for that type,
thus having as many HMMs as there are types of strokes. In order to
recognize the stroke, we calculate for each type of stroke the probability
P(Z(i)jtype of stroke) and select the one with maximum probability.
For a given HMM, and a fixed sequence Z(i) of acquired measurements,
P(Z(i)) can be calculated by using the joint probability of having the meas-
urements Z(i) together with a specific sequence of state variables, i.e. X(i) ¼
fx(0), x(1), ... , x(i)g. First we calculate the joint probability P(X(i), Z(i)):
PðXðiÞ; ZðiÞÞ ¼ PðZðiÞjXðiÞÞPðXðiÞÞ
¼
Y
i
j¼0
PzðzðjÞjxðjÞÞ
 
!
P0ðxð0ÞÞ
Y
i
j¼1
PtðxðjÞjxðj  1ÞÞ
 
!
¼ P0ðxð0ÞÞPzðzð0Þjxð0ÞÞ
Y
i
j¼1
ðPzðzðjÞjxðjÞÞPtðxðjÞjxðj  1ÞÞÞ
ð4:57Þ
ω1
ω2
ω3
Pt (1|1)
Pt (2 | 2)
Pt (4 | 3)
Pt (2 |1)
Pt (4 | 2)
Pt (3| 2)
Pt (3|1)
Pt (3| 3)
ω4
Pt (4 | 4)
Figure 4.13
A four-state left–right model
DISCRETE STATE VARIABLES
115

Here, use has been made of the assumption that each measurement z(i)
only depends on x(i). P(Z(i)) follows from summation over all possible
state sequences:
PðZðiÞÞ ¼
X
all XðiÞ
PððXðiÞ; ZðiÞÞ
ð4:58Þ
Since there are Ki different state sequences, the direct implementation of
(4.57) and (4.58) requires on the order of (i þ 1)Kiþ1 operations. Even
for modest values of i, the number of operations is already impractical.
A more economical approach is to calculate P(Z(i)) by means of a
recursion. For that, consider the probability P(Z(i), x(i)). This probabil-
ity can be calculated from the previous time step i  1 using the follow-
ing expression:
PðZðiÞ;xðiÞÞ ¼
X
K
xði1Þ¼1
PðZðiÞ;xðiÞ;xði  1ÞÞ
¼
X
K
xði1Þ¼1
PðzðiÞ;xðiÞjZði  1Þ;xði  1ÞÞPðZði  1Þ;xði  1ÞÞ
¼
X
K
xði1Þ¼1
PðzðiÞ;xðiÞjxði  1ÞÞPðZði  1Þ;xði  1ÞÞ
¼ PðzðiÞjxðiÞÞ
X
K
xði1Þ¼1
PtðxðiÞjxði  1ÞÞPðZði  1Þ;xði  1ÞÞ
ð4:59Þ
The recursion must be initiated with P(z(0), x(0)) ¼ P0(x(0))Pz(z(0)jx(0)).
The probability P(Z(i)) can be retrieved from P(Z(i), x(i)) by:
PðZðiÞÞ ¼
X
K
xðiÞ¼1
PðZðiÞ; xðiÞÞ
ð4:60Þ
The so-called forward algorithm6 uses the array F(i, x(i)) ¼ P(Z(i), x(i))
to implement the recursion:
6 The adjective ‘forward’ refers to the fact that the algorithm proceeds forwards in time. Section
4.3.3 introduces the backward algorithm.
116
STATE ESTIMATION

Algorithm 4.1: The forward algorithm
1. Initialization:
Fð0; xð0Þ ¼ P0 xð0Þ
ð
ÞPz zð0Þjxð0Þ
ð
Þ
for
xð0Þ ¼ 1; . . . ; K
2. Recursion:
for i ¼ 1, 2, 3, 4, . . .
. for x(i) ¼ 1, . . . , K
F i;xðiÞ
ð
Þ ¼ Pz zðiÞjxðiÞ
ð
Þ
X
K
xði1Þ¼1
Fði  1;xði  1ÞÞPtðxðiÞjxði  1ÞÞ
. P(Z(i)) ¼ P
K
x(i)¼1
F(i, x(i))
In each recursion step, the sum consists of K terms, and the number of
possible values of x(i) is also K. Therefore, such a step requires on the
order of K2 calculations. The computational complexity for i time steps
is on the order of (i þ 1)K2.
4.3.2
Online state estimation
We now focus our attention on the situation of having a single HMM,
where the sequence of measurements is processed online so as to obtain
real-time estimates ^x(iji) of the states. This problem completely fits within
the framework of Section 4.1. As such, the solution provided by (4.8) and
(4.9) is valid, albeit that the integrals must be replaced by summations.
However, in line with the previous section, an alternative solution will
be presented that is equivalent to the one of Section 4.1. The alternative
solution is obtained by deduction of the posterior probability:
PðxðiÞjZðiÞÞ ¼ PðZðiÞ; xðiÞÞ
PðZðiÞÞ
ð4:61Þ
In view of the fact that Z(i) are the acquired measurements (and as such
known and fixed) the maximization of P(x(i)jZ(i)) is equivalent to the
maximization of P(Z(i), x(i)). Therefore, the MAP estimate is found as:
^xMAPðijiÞ ¼ arg max
k
fPðZðiÞ; kÞÞg
ð4:62Þ
DISCRETE STATE VARIABLES
117

The probability P(Z(i), x(i)) follows from the forward algorithm.
Example 4.8
Online license plate detection in videos
This example demonstrates the ability of HMMs to find the license plate
of a vehicle in a video. Figure 4.14 is a typical example of one frame of
such a video. The task is to find all the pixels that correspond to the license
plate. Such a task is the first step in a license plate recognition system.
A major characteristic of video is that a frame is scanned line-
by-line, and that each video line is acquired from left to right. The
real-time processing of each line individually is preferable because
the throughput requirement of the application is demanding. Therefore,
each line is individually modelled as an HMM. The hidden state of a
pixel is determined by whether the pixel corresponds to a license plate
or not.
The measurements are embedded in the video line. See Figure 4.15.
However, the video signal needs to be processed in order to map it
onto a finite measurement space. Simply quantizing the signal to a
finite number of levels does not suffice because the amplitudes of the
signal alone are not very informative. The main characteristic of a
license plate in a video line is a typical pattern of dark-bright and
bright-dark transitions due to the dark characters against a bright
background, or vice versa. The image acquisition is such that the
camera–object distance is about constant for all vehicles. Therefore,
the statistical properties of the succession of transitions are typical for
the imaged license plate regardless of the type of vehicle.
Figure 4.14
License plate detection
118
STATE ESTIMATION

One possibility to decode the succession of transitions is to apply
a filter bank and to threshold the output of each filter, thus yielding a
set of binary signals. In Figure 4.15, three high-pass filters have been
applied with three different cut-off frequencies. Using high-pass filters
has the advantage that the thresholds can be zero. As such, the results
do not depend on the contrast and brightness of the image. The three
binary signals define a measurement signal z(i) consisting of N ¼ 8
symbols. Figure 4.16 shows these symbols for one video line. Here,
the symbols are encoded as integers from 1 up to 8.
Due to the spatial context of the three binary signals we cannot
model the measurements as memoryless symbols. The trick to avoid
video line:
high–pass filtered:
very high–pass filtered:
ultra high–pass filtered:
thresholded:
thresholded:
thresholded:
true license
plate pixels:
Figure 4.15
Definitions of the measurements associated with a video line
true license
plate pixels:
true states:
measurements:
Figure 4.16
States and measurements of a video line
DISCRETE STATE VARIABLES
119

this problem is to embed the measurement z(i) in the state variable
x(i). This can be done by encoding the state variable as integers from 1
up to 16. If i is not a license plate pixel, we define the state as
x(i) ¼ z(i). If i is a license plate pixel, we define x(i) ¼ z(i) þ 8. With
that, K ¼ 16. Figure 4.16 shows these states for one video line.
The embedding of the measurements in the state variables is a form
of state augmentation. Originally, the number of states was 2, but after
this particular state augmentation, the number becomes 16. The advan-
tage of the augmentation is that the dependence, which does exist
between any pair z(i), z(j) of measurements, is now properly modelled
by means of the transition probability of the states. Yet, the model still
meets all the requirements of an HMM. However, due to our definition
of the state, the relation between state and measurement becomes
deterministic. The observation probability degenerates into:
PzðnjkÞ ¼
1
if n ¼ k and k 	 8
1
if n ¼ k  8 and k > 8
0
elsewhere
(
In order to define the HMM, the probabilities P0(k) and Pt(kj‘) must
be specified. We used a supervised learning procedure to estimate
P0(k) and Pt(kj‘). For that purpose, 30 images of 30 different vehicles,
similar to the one in Figure 4.14, were used. For each image, the
license plate area was manually indexed. Histogramming was used to
estimate the probabilities.
Application of the online estimation to the video line shown in
Figures 4.15 and 4.16 yields results like those shown in Figure 4.17.
The figure shows the posterior probability for having a license plate.
According to our definition of the state, the posterior probability of
having a license plate pixel is P(x(i) > 8jZ(i)). Since by definition
online estimation is causal, the rise and decay of this probability
shows a delay. Consequently, the estimated position of the license
plate is biased towards the right. Figure 4.18 shows the detected
license plate pixels.
4.3.3
Offline state estimation
In non-real-time applications the sequence of measurements can be
buffered before the state estimation takes place. The advantage is that
not only ‘past and present’ measurements can be used, but also ‘future’
120
STATE ESTIMATION

measurements. Exactly these measurements can prevent the delay that
inherently occurs in online estimation.
The problem is formulated as follows. Given a sequence Z(I) ¼
fz(0), . . . , z(I)g of I þ 1 measurements of a given HMM, determine the
optimal estimate of the sequence x(0), . . . , x(I) of the underlying states.
Up to now, the adjective ‘optimal’ meant that we determined the
individual posterior probability P(x(i)jmeasurements) for each time
point individually, and that some cost function was applied to determine
the estimate with the minimal risk. For instance, the adoption of a
uniform cost function for each state leads to an estimate that maximizes
the individual posterior probability. Such an estimate minimizes the
probability of having an erroneous decision for such a state.
true license
plate pixels:
0
0.5
1
posterior probability
online estimated
states:
detected pixels:
Figure 4.17
Online state estimation
Figure 4.18
Detected license plate pixels using online estimation
DISCRETE STATE VARIABLES
121

Minimizing the error probabilities of all individually estimated states
does not imply that the sequence of states is estimated with minimal
error probability. It might even occur that a sequence of ‘individually
best’ estimated states contains a forbidden transition, i.e. a transition for
which Pt(x(i)jx(i  1)) ¼ 0. In order to circumvent this, we need a criter-
ion that involves all states jointly.
This section discusses two offline estimation methods. One is an
‘individually best’ solution. The other is an ‘overall best’ solution.
Individually most likely states
Here, the strategy is to determine the posterior probability P(x(i)jZ(I)), and
then to determine the MAP estimate: ^x(ijI) ¼ arg max P(x(i)jZ(I)). As said
before, this method minimizes the error probabilities of the individual states.
As such, it maximizes the expected number of correctly estimated states.
Section 4.3.1 discussed the forward algorithm, a recursive algorithm
for the calculation of the probability P(x(i), Z(i)). We now introduce the
backward algorithm which calculates the probability P(z(i þ 1), . . . ,
z(I)jx(i)). During each recursion step of the algorithm, the probability
P(z(j), . . . , z(I)jx(j  1)) is derived from P(z(j þ 1), . . . , Z(I)jx(j)). The
recursion proceeds as follows:
PðzðjÞ; . . . ; zðIÞjxðj  1ÞÞ
¼
X
K
xðjÞ¼1
PtðxðjÞjxðj  1ÞÞPzðzðjÞjxðjÞÞPðzðj þ 1Þ; . . . ; ZðIÞjxðjÞÞ
ð4:63Þ
The algorithm starts with j ¼ I, and proceeds backwards in time, i.e.
I, I  1, I  2, . . . until finally j ¼ i þ 1. In the first step, the expression
P(z(I þ 1)jx(I)) appears. Since that probability does not exist (because
z(I þ 1) is not available), it should be replaced by 1 to have the proper
initialization.
The availability of the forward and backward probabilities suffices for
the calculation of the posterior probability:
PðxðiÞjZðIÞÞ ¼ PðxðiÞ; ZðIÞÞ
PðZðIÞÞ
¼ Pðzði þ 1Þ; . . . ; zðIÞjxðiÞ; ZðiÞÞPðxðiÞ; ZðiÞÞ
PðZðIÞÞ
¼ Pðzði þ 1Þ; . . . ; zðIÞjxðiÞÞPðxðiÞ; ZðiÞÞ
PðZðIÞÞ
ð4:64Þ
122
STATE ESTIMATION

As said before, the individually most likely state is the one which maxi-
mizes P(x(i)jZ(I)). The denominator of (4.64) is not relevant for this
maximization since it does not depend on x(i).
The complete forward–backward algorithm is as follows:
Algorithm 4.2: The forward–backward algorithm
1. Perform the forward algorithm as given in Section 4.3.1, resulting in
the array F(i, k) with i ¼ 0,    , I and k ¼ 1,    , K
2. Backward algorithm:
. Initialization:
BðI; kÞ ¼ 1
for
k ¼ 1;    ; K
. Recursion:
for i ¼ I  1, I  2,    , 0
and
x(i) ¼ 1,    , K
Fði; xðiÞÞ ¼
X
K
xðiþ1Þ¼1
PtðxðiÞjxði þ 1ÞÞPzðzði þ 1Þjxði þ 1ÞÞ
Fði þ 1; xði þ 1ÞÞ
3. MAP estimation of the states:
^xMAPðijIÞ ¼ arg max
k¼1;...; K
fBði; kÞFði; kÞg
The forward–backward algorithm has a computational complexity that
is on the order of (I þ 1)K2. The algorithm is therefore feasible.
The most likely state sequence
A criterion that involves the whole sequence of states is the overall uni-
form cost function. The function is zero when the whole sequence is
estimated without any error. It is unit if one or more states are estimated
erroneously. Application of this cost function within a Bayesian frame-
work leads to a solution that maximizes the overall posterior probability:
^xð0Þ; . . . ; ^xðIÞ ¼ arg max
xð0Þ;...; xðIÞ
fPðxð0Þ; . . . xðIÞjZðIÞÞg
ð4:65Þ
DISCRETE STATE VARIABLES
123

The computation of this most likely state sequence is done efficiently by
means of a recursion that proceeds forwards in time. The goal of this
recursion is to keep track of the following subsequences:
^xð0Þ; . . . ; ^xði  1Þ ¼ arg max
xð0Þ;...; xði1Þ
fPðxð0Þ; . . . xði  1Þ; xðiÞjZðiÞÞg ð4:66Þ
For each value of x(i), this formulation defines a particular partial
sequence. Such a sequence is the most likely partial sequence from time
zero and ending at a particular value x(i) at time i given the measure-
ments z(0), . . . , z(i). Since x(i) can have K different values, there are
K partial sequences for each value of i. Instead of using (4.66), we can
equivalently use
^xð0Þ; . . . ; ^xði  1Þ ¼ arg max
xð0Þ;...; xði1Þ
fPðxð0Þ; . . . xði  1Þ; xðiÞ; ZðiÞÞg ð4:67Þ
because P(X(i)jZ(i)) ¼ P(X(i), Z(i))P(Z(i)) and Z(i) is fixed.
In each recursion step the maximal probability of the path ending in
x(i) given Z(i) is transformed into the maximal probability of the path
ending in x(i þ 1) given Z(i þ 1). For that purpose, we use the following
equality:
Pðxð0Þ; . . . ; xðiÞ; xði þ 1Þ; Zði þ 1ÞÞ
¼ Pðxði þ 1Þ; zði þ 1Þjxð0Þ; . . . ; xðiÞ; ZðiÞÞPðxð0Þ; . . . ; xðiÞ; ZðiÞÞ
¼ Pzðzði þ 1Þjxði þ 1ÞÞPtðxði þ 1ÞjxðiÞÞPðxð0Þ; . . . ; xðiÞ; ZðiÞÞ
ð4:68Þ
Here, the Markov condition has been used together with the assumption
that the measurements are memoryless.
The maximization of the probability proceeds as follows:
max
xð0Þ;;xðiÞfPðxð0Þ;;xðiÞ;xði þ 1Þ;Zði þ 1ÞÞg
¼
max
xð0Þ;;xðiÞfPzðzði þ 1Þjxði þ 1ÞÞPtðxði þ 1ÞjxðiÞÞPðxð0Þ;;xðiÞ;ZðiÞÞg
¼ Pzðzði þ 1Þjxði þ 1ÞÞmax
xðiÞ

Ptðxði þ 1ÞjxðiÞÞ:
max
xð0Þ;...;xði1ÞfPðxð0Þ;;xði  1Þ;xðiÞ;ZðiÞÞg

ð4:69Þ
124
STATE ESTIMATION

The value of x(i) that maximizes P(x(0), . . . , x(i), x(i þ 1), Z(i þ 1)) is a
function of x(i þ 1):
^xðijxði þ 1ÞÞ ¼ arg max
xðiÞ

Ptðxði þ 1ÞjxðiÞÞ
max
xð0Þ;; xði1ÞfPðxð0Þ;    ; :xði  1Þ; xðiÞ; ZðiÞÞg

ð4:70Þ
The so-called Viterbi algorithm uses the recursive equation in (4.69) and
the corresponding optimal state dependency expressed in (4.70) to find
the
optimal
path.
For
that,
we
define
the
array
Q(i, x(i)) ¼
maxx(0),..., x(i1)fP(x(0), . . . , x(i  1), x(i), Z(i))g.
Algorithm 4.3: The Viterbi algorithm
1. Initialization:
for xð0Þ ¼ 1;    ; K

 Qð0; xð0ÞÞ ¼ P0ðxð0ÞÞPz zð0Þjxð0Þ
ð
Þ

 Rð0; xð0ÞÞ ¼ 0
2. Recursion:
for i ¼ 2;    ; I
and
xðiÞ ¼ 1;    ; K

 Q i; xðiÞ
ð
Þ ¼ max
xði1Þ Q i  1; xði  1ÞPt xðiÞjxði  1Þ
ð
Þ
ð
Þ
f
gPz zðiÞjxðiÞ
ð
Þ

 R i; xðiÞ
ð
Þ ¼ max
xði1Þ Q i  1; xði  1ÞPt xðiÞjxði  1Þ
ð
Þ
ð
Þ
f
g
3. Termination:

 P ¼ max
xðIÞ
QðI; xðIÞÞ
f
g

 ^xðI
IÞ ¼ arg max
xðIÞ
QðI; xðIÞÞ
f
g
4. Backtracking:
for i ¼ I  1; I  2;    ; 0

 ^xðijIÞ ¼ R i þ 1; ^xði þ 1; IÞ
ð
Þ
DISCRETE STATE VARIABLES
125

The computational structure of the Viterbi algorithm is comparable to
that of the forward algorithm. The computational complexity is also on
the order of (i þ 1)K2.
Example 4.9
Offline license plate detection in videos
Figure 4.19 shows the results of the two offline state estimators
applied to the video line shown in Figure 4.15. Figure 4.20 provides
the results of the whole image shown in Figure 4.14.
Both methods are able to prevent the delay that is inherent in online
estimation. Nevertheless, both methods show some falsely detected
license plate pixels on the right side of the plate. These errors are
caused by a sticker containing some text. Apparently, the statistical
properties of the image of this sticker are similar to the one of a license
plate.
A comparison between the individually estimated states and the
jointly estimated states shows that the latter are more coherent, and
that the former are more fragmented. Clearly, such a fragmentation
increases the probability of having erroneous transitions of estimated
states. However, usual the resulting erroneous regions are small. The
jointly estimated states do not show many of these unwanted transi-
tions, but if they occur, then they are more serious because they result
in a larger erroneous region.
true license
plate pixels:
offline, jointly
estimated states:
detected pixels:
offline individually
estimated states:
detected pixels:
Figure 4.19
Offline state estimation
126
STATE ESTIMATION

MATLAB functions for HMM
The MATLAB functions for the analysis of hidden Markov models are
found in the Statistics toolbox. There are five functions:
hmmgenerate:
Given Pt(j) and Pz(j), generate a sequence of states
and observations.
hmmdecode:
Given Pt(j), Pz(j) and a sequence of observations,
calculate the posterior probabilities of the states.
hmmestimate:
Given a sequence of states and observations, estimate
Pt(j) and Pz(j).
(a)
(b)
Figure 4.20
Detected license plate pixels using offline estimation. (a) Individually
estimated states. (b) Jointly estimated states
DISCRETE STATE VARIABLES
127

hmmtrain:
Given a sequence of observations, estimate Pt(j)
and Pz(j).
hmmviterbi:
Given Pt(j), Pz(j) and a sequence of observa-
tions, calculate the most likely state sequence.
The function hmmtrain() implements the so-called Baum–Welch
algorithm.
4.4
MIXED STATES AND THE PARTICLE FILTER
Sections 4.2 and 4.3 focused on special cases of the general online estimation
problem. The topic of Section 4.2 was continuous state estimation, and in
particular the linear-Gaussian case or approximations of that. Section 4.3
discussed discrete state estimation. We now return to the general scheme of
Figure 4.2. The current section introduces the family of particle filters (PF). It
is a group of estimators that try to implement the general case. These
estimators use random samples to represent the probability densities, just
as in the case of Parzen estimation; see Section 5.3.1. As such, particle filters
are able to handle nonlinear, non-Gaussian systems, continuous states, dis-
crete states and even combinations. In the sequel we use probability densities
(which in the discrete case must be replaced by probability functions).
4.4.1
Importance sampling
A Monte Carlo simulation uses a set of random samples generated from
a known distribution to estimate the expectation of any function of that
distribution. More specifically, let x(k), k ¼ 1, . . . , K be samples drawn
from a conditional probability density p(xjz). Then, the expectation of
any function g(x) can be estimated by:
E½gðxÞjz ﬃ1
K
X
K
k¼1
g xðkÞ


ð4:71Þ
Under mild conditions, the right-hand side asymptotically approximates
the expectation as K increases. For instance, the conditional expectation
and covariance matrix are found by substitution of g(x) ¼ x and
g(x) ¼ (x  ^x)(x  ^x)T, respectively.
In the particle filter, the set x(k) depends on the time index i. It
represents the posterior density p(x(i)jZ(i)). The samples are called the
128
STATE ESTIMATION

particles. The density can be estimated from the particles by some
kernel-based method, for instance, the Parzen estimator to be discussed
in Section 5.3.1.
A problem in the particle filter is that we do not know the posterior
density beforehand. The solution for that is to get the samples from some
other density, say q(x), called the proposal density. The various members
of the PF family differ (among other things) in their choice of this
density. The expectation of g(x) w.r.t. p(xjz) becomes:
E½gðxÞjz ¼
Z
gðxÞpðxjzÞdx
¼
Z
gðxÞ pðxjzÞ
qðxÞ qðxÞdx
¼
Z
gðxÞ pðzjxÞpðxÞ
pðzÞqðxÞ qðxÞdx
¼
Z
gðxÞ wðxÞ
pðzÞ qðxÞdx
with: wðxÞ ¼ pðzjxÞpðxÞ
qðxÞ
¼
1
pðzÞ
Z
gðxÞwðxÞqðxÞdx
ð4:72Þ
The factor 1/p(z) is a normalizing constant. It can be eliminated as
follows:
pðzÞ ¼
Z
pðzjxÞpðxÞdx
¼
Z
pðzjxÞpðxÞ qðxÞ
qðxÞ dx
¼
Z
wðxÞqðxÞdx
ð4:73Þ
Using (4.72) and (4.73) we can estimate E[g(x)jz] by means of a set of
samples drawn from q(x):
E½gðxÞjz ﬃ
P
K
k¼1
wðxðkÞÞgðxðkÞÞ
P
K
k¼1
wðxðkÞÞ
ð4:74Þ
Being the ratio of two estimates, E[g(x)jz] is a biased estimate. However,
under mild conditions, E[g(x)jz] is asymptotically unbiased and consistent
MIXED STATES AND THE PARTICLE FILTER
129

as K increases. One of the requirements is that q(x) overlaps the support
of p(x).
Usually, the shorter notation for the unnormalized importance
weights w(k) ¼ w(x(k)) is used. The so-called normalized importance
weights are w(k)
norm ¼ w(k) P w(k):

. With that, expression (4.74) simpli-
fies to:
E½gðxÞjz ﬃ
X
K
k¼1
wðkÞ
normg xðkÞ


ð4:75Þ
4.4.2
Resampling by selection
Importance sampling provides us with samples x(k) and weights w(k)
norm.
Taken together, they represent the density p(xjz). However, we can trans-
form this representation to a new set of samples with equal weights. The
procedure to do that is selection. The purpose is to delete samples with low
weights, and to retain multiple copies of samples with high weights. The
number of samples does not change by this; K is kept constant. The various
members from the PF family may differ in the way they select the samples.
However, an often used method is to draw the samples with replacement
according to a multinomial distribution with probabilities w(k)
norm.
Such a procedure is easily accomplished by calculation of the cumu-
lative weights:
wðkÞ
cum ¼
X
k
j¼1
wð jÞ
norm
ð4:76Þ
We generate K random numbers r(k) with k ¼ 1, . . . , K. These numbers
must be uniformly distributed between 0 and 1. Then, the k-th sample
x(k)
selected in the new set is a copy of the j-th sample x(j) where j is the
smallest integer for which w(j)
cum  r(k).
Figure 4.21 is an illustration. The figure shows a density p(x) and a
proposal density q(x). Samples x(k) from q(x) can represent p(x) if they
are provided with weights w(k)
norm / p(x(k)) q(x(k))

. These weights are
visualized in Figure 4.21(d) by the radii of the circles. Resampling by
selection gives an unweighted representation of p(x). In Figure 4.21(e),
multiple copies of one sample are depicted as a pile. The height of the
pile stands for the multiplicity of the copy.
130
STATE ESTIMATION

4.4.3
The condensation algorithm
One of the simplest applications of importance sampling combined
with resampling by selection is in the so-called condensation algorithm
(‘conditional density optimization’). The algorithm follows the general
scheme of Figure 4.2. The prediction density p(x(i)jZ(i  1)) is used as
the proposal density q(x). So, at time i, we assume that a set x(k) is
available which is an unweighted representation of p(x(i)jZ(i  1)). We
use importance sampling to find the posterior density p(x(i)jZ(i)). For
that purpose we make the following substitutions in (4.72):
pðxÞ ! pðxðiÞjZði  1ÞÞ
pðxjzÞ ! pðxðiÞjzðiÞ; Zði  1ÞÞ ¼ pðxðiÞjZðiÞÞ
qðxÞ ! pðxðiÞjZði  1ÞÞ
pðzjxÞ ! pðzðiÞjxðiÞ; Zði  1ÞÞ ¼ pðzðiÞjxðiÞÞ
The weights w(k)
norm that define the representation of pðxðiÞjzðiÞÞ is
obtained from:
wðkÞ ¼ pðzðiÞjxðkÞÞ
ð4:77Þ
Next, resampling by selection provides an unweighted representation
x(k)
selected. The last step is the prediction. Using x(k)
selected as a representation
0
2
4
(a)
p(x)
0
0.2
0.4
0.6
0.8
1
0
1
2
(b)
q(x)
x
(c)
(d)
(e)
Figure 4.21
Representation of a probability density. (a) A density p(x). (b) The
proposal density q(x). (c) 40 samples of q(x). (d) Importance sampling of p(x) using
the 40 samples from q(x). (e) Selected samples from (d) as an equally weighted
sample representation of p(x)
MIXED STATES AND THE PARTICLE FILTER
131

for p(x(i)jZ(i)), the representation of p(x(i þ 1)jZ(i)) is found by
generating
one
new
sample
x(k)
for
each
sample
x(k)
selected
using
p(x(i þ 1)jx(k)
selected) as the density to draw from. The algorithm is as
follows:
Algorithm 4.4: The condensation algorithm
1. Initialization
. Set i ¼ 0
. Draw K samples x(k), k ¼ 1, . . . , K, from the prior probability
density p(x(0))
2. Update using importance sampling:
. Set the importance weights equal to: w(k) ¼ p(z(i)jx(k))
. Calculate
the
normalized
importance
weights:
w(k)
norm ¼
w(k)= P w(k)
3. Resample by selection:
. Calculate the cumulative weights w(k)
cum ¼ P
k
j¼1
w(j)
norm
. for k ¼ 1, . . . , K:
. Generate a random number r uniformly distributed in [0, 1]
. Find the smallest j such that w(j)
cum  r(k)
. Set x(k)
selected ¼ x( j)
4. Predict:
. Set i ¼ i þ 1
. for k ¼ 1, . . . , K:
. Draw sample x(k), from the density p(x(i)jx(i  1) ¼ x(k)
selected)
5. Go to 2
After step 2, the posterior density is available in terms of the samples x(k)
and the weights w(k)
norm. The MMSE estimate and the associated error
covariance matrix can be obtained from (4.75). For insance, the MMSE
is obtained by substitution of g(x) ¼ x. Since we have a representation of
the posterior density, estimates associated with other criteria can be
obtained as well.
The calculation of the importance weights in step 2 involves the
conditional density p(z(i)jx(k)). In the case of the nonlinear measurement
functions of the type z(i) ¼ h(x(i)) þ v(i), it all boils down to calculating
the density of the measurement noise for v(i) ¼ z(i)  h(x(k)). For
132
STATE ESTIMATION

instance, if v(i) is zero mean, Gaussian with covariance matrix Cv, the
weights are calculated as:
wðkÞ ¼ constant  exp  1
2 ðzðiÞ  hðxðkÞÞÞTC1
v ðzðiÞ  hðxðkÞÞÞ


The actual value of constant is irrelevant because of the normalization.
The drawing of new samples in the prediction step involves the state
equation. If x(i þ 1) ¼ f(x(i),u(i)) þ w(i), then the drawing is governed
by the density of w(i).
The advantages of the particle filtering are obvious. Nonlinearities
of both the state equation and the measurement function are handled
smoothly without the necessity to calculate Jacobian matrices. How-
ever, the method works well only if enough particles are used. Espe-
cially for large dimensions of the state vector the required number
becomes large. If the number is not sufficient, then the particles are
not able to represent the density p(x(i)jZ(i  1)). Particularly, if for
some values of x(i) the likelihood p(z(i)jx(i)) is very large, while on
these locations p(x(i)jZ(i  1)) is small, the particle filtering may not
converge. It occurs frequently then that all weights become zero
except one which becomes unit. The particle filter is said to be
degenerated.
Example 4.10
Particle filtering applied to volume density estimation
The problem of estimating the volume density of a substance mixed
with a liquid is introduced in Example 4.1. The model, expressed in
equation (4.3), is nonlinear and non-Gaussian. The state vector
consists of two continuous variables (volume and density), and one
discrete variable (the state of the on/off controller). The measure-
ment system, expressed in equation (4.40), is nonlinear with additive
Gaussian noise. Example 4.6 has shown that the EKF is able to estimate
the density, but only by using an approximate model of the process
in which the discrete state is removed. The price to pay for such a
rough approximation is that the estimation error of the density and
(particularly) the volume has a large magnitude.
The particle filter does not need such a rough approximation
because it can handle the discrete state variable. In addition, the
particle filter can cope with discontinuities. These discontinuities
appear here because of the discrete on/off control, but also because
the input flow of the substance occurs in chunks at some random
points in time.
MIXED STATES AND THE PARTICLE FILTER
133

The particle filter implemented in this example uses the process
model given in (4.3), and the measurement model of (4.40). The
parameters used are tabulated in Example 4.5. Other parameters
are: Vlow ¼ 3990 (litre) and Vhigh ¼ 4010 (litre). The random points
of the substance are modelled as a Poisson process with mean time
between two points  ¼ 100 ¼ 100 (s). The chunks have an uni-
form distribution between 7 and 13 (litre). Results of the particle filter
using 10000 particles are shown in Figure 4.22. The figure shows an
example of a cloud of particles. Clearly, such a cloud is not
represented by a Gaussian distribution. In fact, the distribution is
3990
4000
4010
4020
4030
real (thick) and estimated volume (litre)
0.08
0.09
0.1
0.11
real (thick) and estimated density
real on/off control
0
2000
4000
estimated on/off control
i∆(s)
(c)
4000
4020
volume (litre)
0.08
0.09
0.1
density
3950
4000
4050
volume measurements (litre)
0
0
0.2
0.4
density measurements (V)
i∆(s)
(a)
3990
3995
4000
4005
volume (litre)
density
(b)
2000
4000
0.094
0.096
0.098
Figure 4.22
Application of particle filtering to the density estimation problem.
(a) Real states and measurements. (b) The particles obtained at i ¼ 511. (c) Results
134
STATE ESTIMATION

multi-modal due to the uncertainty of the moment at which the on/off
control switches its state.
In contrast with the Kalman filter, the particle filter is able to
estimate the fluctuations of the volume. In addition, the estimation
of the density is much more accurate. The price to pay is the compu-
tational cost.
MATLAB functions for particle filtering
Many MATLAB users have already implemented particle filters, but no
formal toolbox yet exists. Section 9.3 contains a listing of MATLAB code
that implements the condensation algorithm. Details of the implementa-
tion are also given.
4.5
SELECTED BIBLIOGRAPHY
Some introductory books on Kalman filtering and its applications are
Anderson and Moore (1979), Bar-Shalom and Li (1993), Gelb et al.
(1974), Grewal and Andrews (2001). Hidden Markov models are
described in Rabiner (1989). Tutorials on particle filtering are found in
Arulampalam et al. (2002) and Merwe et al. (2000). These tutorials also
describe some shortcomings of the particle filter, and possible remedies.
Seminal papers for Kalman filtering, particle filtering and unscented
Kalman filtering are Kalman (1960), Gordon et al. (1993) and Julier
and Uhlmann (1997), respectively. Linear systems with random inputs,
among which the AR models, are studied in Box and Jenkins (1976). The
topic of statistical linearization is treated in Gelb et al. (1974). The
condensation algorithm is due to Isard and Blake (1996). The Baum-
Welch algorithm is described in Rabiner (1986).
Anderson, B.D. and Moore, J.B., Optimal Filtering, Prentice Hall, Englewood Cliffs, NJ,
1979.
Arulampalam, M.S., Maskell, S., Gordon, N. and Clapp, T., A tutorial on particle filters
for online nonlinear/non-Gaussian Bayesian tracking, IEEE Transactions on Signal
Processing, 50(2), 174–88, February 2002.
Bar-Shalom, Y. and Li, X.R., Estimation and Tracking – Principles, Techniques, and
Software, Artech House, Boston, 1993.
Box, G.E.P. and Jenkins, G.M., Time Series Analysis: Forecasting and Control, Holden-
Day, San Francisco, 1976.
Gelb, A., Kasper, J.F., Nash, R.A., Price, C.F. and Sutherland, A.A., Applied Optimal
Estimation, MIT Press, Cambridge, MA, 1974.
Gordon, N.J., Salmond, D.J. and Smith, A.F.M., Novel approach to nonlinear/nonGaussian
Bayesian state estimation, IEE Proceedings-F, 140(2), 107–13, 1993.
SELECTED BIBLIOGRAPHY
135

Grewal, M.S. and Andrews, A.P., Kalman Filtering – Theory and Practice Using
MATLAB, Second edition, Wiley, New York, 2001.
Isard, M. and Blake, A. Contour tracking by stochastic propagation of conditional
density, European Conference on Computer Vision, pp. 343–56, Cambridge, UK,
1996.
Julier, S.J. and Uhlmann, J.K., A new extension of the Kalman filter to nonlinear systems,
Proc. of AeroSense: the 11th International Symposium on Aerospace/Defence Sensing,
Simulation and Controls, Vol. Multi Sensor, Tracking and Resource Management II,
Orlando, Florida, 1997.
Kalman, R.E., A new approach to linear filtering and prediction problems, ASME
Journal of Basic Engineering, 82, 34–45, 1960.
Merwe, R. van der, Doucet, A., Freitas N. de and Wan, E. The unscented particle filter,
Technical report CUED/F-INFENG/TR 380, Cambridge University Engineering
Department, 2000.
Rabiner, L.R., A tutorial on hidden Markov models and selected applications in speech
recognition, Proceedings of the IEEE, 77(2), 257–85, February 1989.
4.6
EXERCISES
1. Consider the following model for a random constant:
xði þ 1Þ ¼ xðiÞ
zðiÞ ¼ xðiÞ þ vðiÞ
vðiÞ is white noise with variance 2
v
The prior knowledge is E[x(0)] ¼ 0 and 2
x(0) ¼ 1. Give the expression for the solu-
tion of the discrete Lyapunov equation. (0).
2. For the random constant model given in exercise 1, give expressions for the innovation
matrix S(i), the Kalman gain matrix K(i), the error covariance matrix C(iji) and the
prediction matrix C(iji  1) for the first few time steps. That is, for i ¼ 0, 1, 2 and 3.
Explain the results. (0).
3. In exercise 2, can you find the expressions for arbitrary i. Can you also prove that these
expressions are correct? Hint: use induction. Explain the results. (*).
4. Consider the following time-invariant scalar linear-Gaussian system
xði þ 1Þ ¼ xðiÞ þ wðiÞ
wðiÞ is white noise with variance 2
w
zðiÞ ¼ xðiÞ þ vðiÞ
vðiÞ is white noise with variance 2
v
The prior knowledge is Ex0 ¼ 0 and 2
x0 ¼ 1. What is the condition for the
existence of the solution of the discrete Lyapunov equation? If this condition is
met, give an expression for that solution. (0).
5. For the system described in exercise 4, give the steady state solution. That is, give
expressions for S(i), K(i), C(iji) and C(iji  1) if i ! 1. (0).
6. For the system given in exercise 4, give expressions for S(i), K(i), C(iji) and C(iji  1)
for the first few time steps, that is, i ¼ 0, 1, 2 and 3. Explain the results. (0).
136
STATE ESTIMATION

7. In exercise 4, can you find the expressions for S(i), K(i), C(iji) and C(iji  1) for
arbitrary i? (*).
8. Autoregressive models and MATLAB’s Control System Toolbox: consider the follow-
ing second order autoregressive (AR) model:
xði þ 1Þ ¼ 1
2 xðiÞ þ 1
4 xði  1Þ þ wðiÞ
zðiÞ ¼ xðiÞ þ vðiÞ
Using the functions tf() and ss() from the Control Toolbox, convert this AR model
into an equivalent state space model, that is, x(i þ 1) ¼ Fx(i) þ Gw(i) and
z(i) ¼ Hx(i) þ v(i). (Use help tf and help ss to find out how these functions
should be used.) Assuming that w(i) and v(i) are white noise sequences with variances
2
w ¼ 1 and 2
v ¼ 1, use the function dlyap() to find the solution of the discrete
Lyapunov equation, and the function kalman() (or dlqe()) to find the solution for
the steady state Kalman gain and corresponding error covariance matrices. Hint: the
output variable of the command ss() is a ‘struct’ whose fields are printed by typing
struct(ss). (*).
9. Moving average models: repeat exercise 8, but now considering the so-called first
order moving average (MA) model:
xði þ 1Þ ¼ 1
2 ðwðiÞ þ wði  1ÞÞ 2
w ¼ 1
zðiÞ ¼ xðiÞ þ vðiÞ
2
v ¼ 1
ðÞ
10. Autoregressive, moving average models: repeat exercise 8, but now considering the
so-called ARMA(2, 1) model:
xði þ 1Þ ¼ 1
2 xðiÞ þ 1
2 xði  1Þ þ 1
2 ðwðiÞ þ wði  1ÞÞ 2
w ¼ 1
zðiÞ ¼ xðiÞ þ vðiÞ
2
v ¼ 1
ðÞ
11. Simulate the processes mentioned in exercise 1, 8, 9 and 10, using MATLAB, and
apply the Kalman filters.
EXERCISES
137


5
Supervised Learning
One method for the development of a classifier or an estimator is the
so-called model-based approach. Here, the required availability of the
conditional probability densities and the prior probabilities are obtained
by means of general knowledge of the physical process and the sensory
system in terms of mathematical models. The development of the esti-
mators for the backscattering coefficient, discussed in Chapter 3, follows
such an approach.
In many other applications, modelling the process is very difficult if
not impossible. For instance, in the mechanical parts application,
discussed in Chapter 2, the visual appearance of the objects depends
on many factors that are difficult to model. The alternative to the
model-based approach is the learning from examples paradigm. Here,
it is assumed that in a given application a population of objects is
available. From this population, some objects are selected. These
selected objects are called the samples. Each sample is presented to
the sensory system which returns the measurement vector associated
with that sample. The purpose of learning (or training) is to use these
measurement vectors of the samples to build a classifier or an estima-
tor.
The problem of learning has two versions: supervised and unsuper-
vised, that is, with or without knowing the true class/parameter of the
sample. See Figure 5.1. This chapter addresses the first version. Chapter 7
deals with unsupervised learning.
Classification, Parameter Estimation and State Estimation: An Engineering Approach using MATLAB
F. van der Heijden, R.P.W. Duin, D. de Ridder and D.M.J. Tax
 2004 John Wiley & Sons, Ltd
ISBN: 0-470-09013-8

The chapter starts with a section on the representation of training sets.
In Sections 5.2 and 5.3 two approaches to supervised learning are dis-
cussed: parametric and nonparametric learning. Section 5.4 addresses
the problem of how to evaluate a classifier empirically. The discussion
here is restricted to classification problems only. However, many tech-
niques that are useful for classification problems are also useful for
estimation problems. Especially Section 5.2 (parametric learning) is
useful for estimation problems too.
5.1
TRAINING SETS
The set of samples is usually called the training set (or: learning data or
design set). The selection of samples should occur randomly from the
population. In almost all cases it is assumed that the samples are i.i.d.,
independent and identically distributed. This means that all samples are
selected from the same population of objects (in the simplest case, with
equal probability). Furthermore, the probability of one member of the
population being selected is not allowed to depend on the selection of
other members of the population.
Figure 5.1 shows scatter diagrams of the mechanical parts application
of Chapter 2. In Figure 5.1(a) the samples are provided with a label
carrying the information of the true class of the corresponding object.
There are several methods to find the true class of a sample, e.g. manual
inspection, additional measurements, destructive analysis, etc. Often,
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
bolts
nuts
rings
scrap
(a)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
(b)
Figure 5.1
Training sets. (a) Labelled. (b) Unlabelled
140
SUPERVISED LEARNING

these methods are expensive and therefore allowed only if the number of
samples is not too large.
The number of samples in the training set is denoted by NS. The
samples are enumerated by the symbol n ¼ 1, . . . , NS. Object n has a
measurement vector zn. The true class of the n-th object is denoted by
n 2 O. Then, a labelled training set TS contains samples (zn, n) each
one consisting of a measurement vector and its true class:
TS ¼ fðzn; nÞg
with
n ¼ 1; . . . ; NS
ð5:1Þ
Another representation of the data set is obtained if we split the training
set according to their true classes:
TS ¼ fzk;ng
with
k ¼ 1; . . . ; K
and
n ¼ 1; . . . ; Nk
ð5:2Þ
where Nk is the number of samples with class !k and K ¼ jOj is the
number of classes. A representation equivalent to (5.2) is to introduce a
set Tk for each class.
Tk ¼ fðzn; nÞjn ¼ !kg
with
k ¼ 1; . . . ; K
and
n ¼ 1; . . . ; Nk
ð5:3Þ
It is understood that the numberings of samples used in these three
representations do not coincide. Since the representations are equivalent,
we have:
NS ¼
X
K
k¼1
Nk
ð5:4Þ
In PRTools, data sets are always represented as in (5.1). In order to
obtain representations as in (5.3), separate data sets for each of the
classes will have to be constructed. In Listing 5.1 these two ways are
shown. It is assumed that dat is an N  d matrix containing the meas-
urements, and lab an N  1 matrix containing the class labels.
Listing 5.1
Two methods of representing data sets in PRTools. The first method is
used almost exclusively.
% Create a standard MATLAB dataset from data and labels.
% Method (5.1):
TRAINING SETS
141

dat ¼ [ 0.1 0.9 ; 0.3 0.95 ; 0.2 0.7 ];
lab ¼ { ‘class 1’, ‘class 2’, ‘class 3’ };
z ¼ dataset(dat,lab);
% Method (5.3):
[nlab,lablist] ¼ getnlab(z);
% Extract the numeric labels
[m,k,c] ¼ getsize(z);
% Extract number of classes
for i ¼ 1:c
T{i} ¼ seldat(z,i);
end;
5.2
PARAMETRIC LEARNING
The basic assumption in parametric learning is that the only unknown
factors are parameters of the probability densities involved. Thus,
learning from samples boils down to finding the suitable values of these
parameters. The process is analogous to parameter estimation discussed
in Chapter 3. The difference is that the parameters in Chapter 3
describe a physical process whereas the parameters discussed here are
parameters of the probability densities of the measurements of the
objects. Moreover, in parametric learning a set of many measurement
vectors is available rather than just a single vector. Despite these two
differences, the concepts from Chapter 3 are fully applicable to the
current chapter.
Suppose that zn are the samples coming from a same class !k. These
samples are repeated realizations of a single random vector z. An alter-
native view is to associate the samples with single realizations coming
from a set of random vectors with identical probability densities. Thus, a
training set Tk consists of Nk mutually independent, random vectors zn.
The joint probability density of these vectors is
pðz1; z2; . . . ; zNkj!k; akÞ ¼
Y
Nk
n¼1
pðznj!k; akÞ
ð5:5Þ
ak is the unknown parameter vector of the conditional probability
density p(zj!k, ak). Since in parametric learning we assume that the form
of p(zj!k, ak) is known (only the parameter vector ak is unknown),
the complete machinery of Bayesian estimation (minimum risk, MMSE
estimation, MAP estimation, ML estimation) becomes available to find
estimators for the parameter vector ak: see Section 3.1. Known concepts
to evaluate these estimators (bias and variance) also apply. The next
subsections discuss some special cases for the probability densities.
142
SUPERVISED LEARNING

5.2.1
Gaussian distribution, mean unknown
Let us assume that under class !k the measurement vector z is a Gaussian
random vector with known covariance matrix Ck and unknown expect-
ation vector mk. No prior knowledge is available concerning this
unknown vector. The purpose is to find an estimator for mk.
Since no prior knowledge about mk is assumed, a maximum likelihood
estimator seems appropriate (Section 3.1.4). Substitution of (5.5) in (3.22)
gives the following general expression of a maximum likelihood estimator:
^mk ¼ argmax
m
Y
Nk
n¼1
pðznj!k; mÞ
(
)
¼ argmax
m
X
Nk
n¼1
lnðpðznj!k; mÞÞ
(
)
ð5:6Þ
The logarithms introduced in the last line transform the product into
a summation. This is only a technical matter which facilitates the
maximization.
Knowing that z is Gaussian, the likelihood of mk from a single observ-
ation zn is:
pðznj!k; mkÞ ¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð2ÞNjCkj
q
exp  1
2 ðzn  mkÞTC1
k ðzn  mkÞ


ð5:7Þ
Upon substitution of (5.7) in (5.6), rearrangement of terms and elimin-
ation of irrelevant terms, we have:
^mk ¼ argmin
m
X
Nk
n¼1
ðzn  mÞTC1
k ðzn  mÞ
(
)
¼ argmin
m
X
Nk
n¼1
zT
n C1
k zn þ
X
Nk
n¼1
mTC1
k m  2
X
Nk
n¼1
zt
nC1
k m
(
)
ð5:8Þ
Differentiating the expression between braces with respect to m (Appen-
dix B.4) and equating the result to zero yields the average or sample
mean calculated over the training set:
^mk ¼ 1
Nk
X
Nk
n¼1
zn
ð5:9Þ
PARAMETRIC LEARNING
143

Being a sum of Gaussian random variables, this estimate has a Gaussian
distribution too. The expectation of the estimate is:
E½^mk ¼ 1
Nk
X
Nk
n¼1
E½zn ¼ 1
Nk
X
Nk
n¼1
mk ¼ mk
ð5:10Þ
where mk is the true expectation of z. Hence, the estimation is unbiased.
The covariance matrix of the estimation error is found as:
C^mk ¼ E ð^mk  mkÞð^mk  mkÞT
h
i
¼ 1
Nk
Ck
ð5:11Þ
The proof is left as an exercise for the reader.
5.2.2
Gaussian distribution, covariance matrix unknown
Next, we consider the case where under class !k the measurement vector
z is a Gaussian random vector with unknown covariance matrix Ck. For
the moment we assume that the expectation vector mk is known. No
prior knowledge is available. The purpose is to find an estimator for Ck.
The maximum likelihood estimate follows from (5.5) and (5.7):
^Ck ¼ argmax
C
X
Nk
n¼1
lnðpðznj!k; CÞÞ
(
)
¼ 1
Nk
X
Nk
n¼1
ðzn  mkÞðzn  mkÞT
ð5:12Þ
The last step in (5.12) is non-trivial. The proof is rather technical and
will be omitted. However, the result is plausible since the estimate is the
average of the Nk matrices (zn  mk)(zn  mk)T whereas the true covari-
ance matrix is the expectation of (z  mk)(z  mk)T.
The probability distribution of the random variables in ^Ck is a Wishart
distribution. The estimator is unbiased. The variances of the elements of
^Ck are:
Var½^Cki; j ¼ 1
Nk
Cki; iCkj; j þ C2
ki; j


ð5:13Þ
144
SUPERVISED LEARNING

5.2.3
Gaussian distribution, mean and covariance matrix
both unknown
If both the expectation vector and the covariance matrix are unknown,
the estimation problem becomes more complicated because then we
have to estimate the expectation vector and covariance matrix simultan-
eously. It can be deduced that the following estimators for Ck and mk are
unbiased:
^mk ¼ 1
Nk
X
Nk
n¼1
zn
^Ck ¼
1
Nk  1
X
Nk
n¼1
ðzn  ^mkÞðzn  ^mkÞT
ð5:14Þ
^Ck is called the sample covariance. Comparing (5.12) with (5.14) we
note two differences. In the latter expression the unknown expectation
has been replaced with the sample mean. Furthermore, the divisor Nk
has been replaced with Nk1. Apparently, the lack of knowledge of mk
in (5.14) makes it necessary to sacrifice one degree of freedom in the
averaging operator. For large NK, the difference between (5.12) and
(5.14) vanishes.
In classification problems, often the inverse C1
k
is needed, for
instance, in quantities like: zTC1
k m, zTC1
k z, etc. Often, ^C1
k
is used as
an estimate of C1
k . To determine the number of samples required such
that ^C1
k
becomes an accurate estimate of C1
k , the variance of ^Ck, given
in (5.13), is not very helpful. To see this it is instructive to rewrite the
inverse as (see Appendix B.5 and C.3.2):
C1
k
¼ Vk1
k VT
k
ð5:15Þ
where k is a diagonal matrix containing the eigenvalues of Ck. Clearly,
the behaviour of C1
k
is strongly affected by small eigenvalues in k.
In fact, the number of nonzero eigenvalues in the estimate ^Ck given in
(5.14) cannot exceed Nk  1. If Nk  1 is smaller than the dimension
N of the measurement vector, the estimate ^Ck is not invertible. There-
fore, we must require that Nk is (much) larger than N. As a rule of
thumb, the number of samples must be chosen such that at least
Nk > 5N.
PARAMETRIC LEARNING
145

In order to reduce the sensitivity to statistical errors, we might also
want to regularize the inverse operation. Suppose that ^ is the diagonal
matrix containing the eigenvalues of the estimated covariance matrix ^C
(we conveniently drop the index k for a moment). ^V is the matrix
containing the eigenvectors. Then, we can define a regularized inverse
operation as follows:
^C1
regularized ¼ ^V ð1  Þ^ þ  traceð^Þ
N
I
 
!1
^VT
0    1
ð5:16Þ
where trace(^)/N is the average of the eigenvalues of ^C.  is a regular-
ization parameter. The effect is that the influence of the smallest eigen-
values is tamed. A simpler implementation of (5.16) is (see exercise 2):
^C1
regularized ¼
ð1  Þ^C þ  traceð^CÞ
N
I
 
!1
ð5:17Þ
Another method to regularize a covariance matrix estimate is by sup-
pressing all off-diagonal elements to some extent. This is achieved by
multiplying these elements by a factor that is selected between 0 and 1.
Example 5.1
Classification of mechanical parts, Gaussian
assumption
We now return to the example in Chapter 2 where mechanical parts
like nuts and bolts, etc. must be classified in order to sort them. See
Figure 2.2. In Listing 5.2 the PRTools procedure for training and
visualizing the classifiers is given. Two classifiers are trained: a linear
classifier (ldc) and a quadratic classifier (qdc). The trained classifiers
are stored in w_l and w_q, respectively. Using the plotc function,
the decision boundaries can be plotted. In principle this visualization
is only possible in 2D. For data with more than two measurements,
the classifiers cannot be visualized.
Listing 5.2
PRTools code for training and plotting linear and quadratic discrimin-
ants under assumption of normal distributions of the conditional prob-
ability densities.
load nutsbolts;
% Load the mechanical parts dataset
w_l ¼ ldc(z,0,0.7);
% Train a linear classifier on z
146
SUPERVISED LEARNING

w_q ¼ qdc(z,0,0.5);
% Train a quadratic classifier on z
figure; scatterd(z);
% Show scatter diagram of z
plotc(w_l);
% Plot the first classifier
plotc(w_q,‘:’);
% Plot the second classifier
[0.4 0.2]w_llabeld
% Classify a new object with z ¼ [0.4 0.2]
Figure 5.2 shows the decision boundaries obtained from the data shown
in Figure 5.1(a) assuming Gaussian distributions for each class. The
discriminant in Figure 5.2(a) assumes that the covariance matrices for
different classes are the same. This assumption yields a Mahalanobis
distance classifier. The effect of the regularization is that the classifier
tends to approach the Euclidean distance classifier. Figure 5.2(b)
assumes unequal covariance matrices. The effect of the regularization
here is that the decision boundaries tend to approach circle segments.
5.2.4
Estimation of the prior probabilities
The prior probability of a class is denoted by P(!k). There are exactly
K classes. Having a labelled training set with NS samples (randomly
selected from a population), the number Nk of samples with class !k
has a so-called multinomial distribution. If K ¼ 2 the distribution is
binomial. See Appendix C.1.3.
The multinomial distribution is fully defined by K parameters. In
addition to the K  1 parameters that are necessary to define the prior
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
γ = 0.7
γ = 0
(a)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
γ = 0.5
γ = 0
(b)
Figure 5.2
Classification assuming Gaussian distributions. (a) Linear decision
boundaries. (b) Quadratic decision boundaries
PARAMETRIC LEARNING
147

probabilities, we need to specify one extra parameter, NS, which is the
number of samples.
Intuitively, the following estimator is appropriate:
^Pð!kÞ ¼ Nk
NS
ð5:18Þ
The expectation of Nk equals NSP(!k). Therefore, ^P(!k) is an unbiased
estimate of P(!k). The variance of a multinomial distributed variable is
NSP(!k)(1  P(!k)). Consequently, the variance of the estimate is:
Var½^Pð!kÞ ¼ Pð!kÞð1  Pð!kÞÞ
NS
ð5:19Þ
This shows that the estimator is consistent. That is if NS ! 1, then
Var[^P(!k)] ! 0. The required number of samples follows from the
constraint that
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Var[^P(!k)]
q
<< P(!k). For instance, if for some class
we anticipate that P(!k) ¼ 0:01, and the permitted relative error is 20%,
i.e.
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Var[^P(!k)]
q
¼ 0:2P(!k), then NS must be about 2500 in order to
obtain the required precision.
5.2.5
Binary measurements
Another example of a multinomial distribution occurs when the meas-
urement vector z can only take a finite number of states. For instance,
if the sensory system is such that each element in the measurement
vector is binary, i.e. either ‘1’ or ‘0’, then the number of states the
vector can take is at most 2N. Such a binary vector can be replaced
with an equivalent scalar z that only takes integer values from 1 up to
2N. The conditional probability density p(zj!k) turns into a probability
function P(zj!k). Let Nk(z) be the number of samples in the training
set with measurement z and class !k. Nk(z) has a multinomial
distribution.
At first sight, one would think that estimating P(zj!k) is the same type
of problem as estimating the prior probabilities such as discussed in the
previous section:
148
SUPERVISED LEARNING

^Pðzj!kÞ ¼ NkðzÞ
Nk
ð5:20Þ
Var½^Pðzj!kÞ ¼ Pðzj!kÞ 1  Pðzj!kÞ
ð
Þ
Nk
ð5:21Þ
For small N and a large training set, this estimator indeed suffices.
However, if N is too large, the estimator fails. A small example demon-
strates this. Suppose the dimension of the vector is N ¼ 10. Then the
total number of states is 210  103. Therefore, some states will have a
probability of less than 103. The uncertainty of the estimated probabil-
ities must be a fraction of that, say 104. The number of samples, Nk,
needed to guarantee such a precision is on the order of 105 or more.
Needless to say that in many applications 105 samples is much too
expensive. Moreover, with even a slight increase of N the required
number of samples becomes much larger.
One way to avoid a large variance is to incorporate more prior know-
ledge. For instance, without the availability of a training set, it is known
beforehand that all parameters are bounded by 0  P(zj!k)  1. If noth-
ing further is known, we could first ‘guess’ that all states are equally
likely: P(zj!k) ¼ 2N. Based on this guess, the estimator takes the form:
^Pðzj!kÞ ¼ NkðzÞ þ 1
Nk þ 2N
ð5:22Þ
The variance of the estimate is:
Var½^Pðzj!kÞ ¼ NkPðzj!kÞ 1  Pðzj!kÞ
ð
Þ
Nk þ 2N
ð
Þ2
ð5:23Þ
Comparing (5.22) and (5.23) with (5.20) and (5.21) the conclusion is
that the variance of the estimate is reduced at the cost of a small bias. See
also exercise 4.
5.3
NONPARAMETRIC LEARNING
Nonparametric methods are learning methods for which prior knowledge
about the functional form of the conditional probability distributions is
NONPARAMETRIC LEARNING
149

not available or is not used explicitly. The name may suggest that no
parameters are involved. However, in fact these methods often require
more parameters than parametric methods. The difference is that in
nonparametric methods the parameters are not the parameters of the
conditional distributions.
At first sight, nonparametric learning seems to be more difficult than
parametric learning because nonparametric methods exploit less know-
ledge. For some nonparametric methods this is indeed the case. In
principle, these types of nonparametric methods can handle arbitrary
types of conditional distributions. Their generality is high. The downside
of this advantage is that large to very large training sets are needed to
compensate the lacking knowledge about the densities.
Other nonparametric learning methods cannot handle arbitrary types
of conditional distributions. The classifiers being trained are constrained
to some preset computational structure of their decision function. By
this, the corresponding decision boundaries are also constrained. An
example is the linear classifier already mentioned in Section 2.1.2. Here,
the decision boundaries are linear (hyper)planes. The advantage of
incorporating constraints is that fewer samples in the training set are
needed. The stronger the constraints are, the fewer samples are needed.
However, good classifiers can only be obtained if the constraints that are
used match the type of the underlying problem-specific distributions.
Hence, in constraining the computational structure of the classifier
implicit knowledge of the distribution is needed.
5.3.1
Parzen estimation and histogramming
The objective of Parzen estimation and histogramming is to obtain
estimates of the conditional probability densities. This is done without
much prior knowledge of these densities. As before, the estimation is
based on a labelled training set TS. We use the representation according
to (5.3), i.e. we split the training set into K subsets Tk, each having Nk
samples all belonging to class !k. The goal is to estimate the conditional
density p(zj!k) for arbitrary z.
A simple way to reach the goal is to partition the measurement space
into a finite number of disjoint regions Ri, called bins, and to count the
number of samples that falls in each of these bins. The estimated prob-
ability density within a bin is proportional to that count. This technique
is called histogramming. Suppose that Nk,i is the number of samples
150
SUPERVISED LEARNING

with class !k that fall within the i-th bin. Then the probability density
within the i-th bin is estimated as:
^pðzj!kÞ ¼
Nk;i
VolumeðRiÞ  Nk
with
z 2 Ri
ð5:24Þ
For each class, the number Nk,i has a multinomial distribution with
parameters
Pk;i ¼
Z
z2Ri
pðzj!kÞdz
with
i ¼ 1; . . . ; Nbin
where Nbin is the number of bins. The statistical properties of ^p(zj!k)
follows from arguments that are identical to those used in Section 5.2.5.
In fact, if we quantize the measurement vector to, for instance, the
nearest centre of gravity of the bins, we end up in a situation similar to
the one of Section 5.2.5. The conclusion is that histogramming works
fine if the number of samples within each bin is sufficiently large. With a
given size of the training set, the size of the bins must be large enough to
assure a minimum number of samples per bin. Hence, with a small
training set, or a large dimension of the measurement space, the resolu-
tion of the estimation will be very poor.
Parzen estimation can be considered as a refinement of histogram-
ming. The first step in the development of the estimator is to consider
only one sample from the training set. Suppose that zj 2 Tk. Then, we
are certain that at this position in the measurement space the density is
nonzero, i.e. p(zjj!k) 6¼ 0. Under the assumption that p(zj!k) is contin-
uous over the entire measurement space it follows that in a small
neighbourhood of zj the density is likely to be nonzero too. However,
the further we move away from zj, the less we can say about p(zj!k). The
basic idea behind Parzen estimation is that the knowledge gained by the
observation of zj is represented by a function positioned at zj and with an
influence restricted to a small vicinity of zj. Such a function is called the
kernel of the estimator. It represents the contribution of zj to the esti-
mate. Summing together the contributions of all vectors in the training
set yields the final estimate.
Let (z, zj) be a distance measure (Appendix A.2) defined in the meas-
urement space. The knowledge gained by the observation zj 2 Tk is
represented by the kernel h((z, zj)) where h(  ) is a function Rþ ! Rþ
such that h((z, zj)) has its maximum at z ¼ zj, i.e. at (z, zj) ¼ 0. Further-
more, h((  ,  )) must be monotonically decreasing as (  ,  ) increases,
NONPARAMETRIC LEARNING
151

and h((  ,  )) must be normalized to one, i.e.
R
h((z, zj))dz ¼ 1 where
the integration extends over the entire measurement space.
The contribution of a single observation zj is h((z, zj)). The contribu-
tions of all observations are summed to yield the final Parzen estimate:
^pðzj!kÞ ¼ 1
Nk
X
zj2Tk
h ðz; zjÞ


ð5:25Þ
The kernel h((  ,  )) can be regarded as an interpolation function that
interpolates between the samples of the training set.
Figure 5.3 gives an example of Parzen estimation in a one-dimensional
measurement space. The plot is generated by the code in Listing 5.3. The
true distribution is zero for negative z and has a peak value near z ¼ 1
after which it slowly decays to zero. Fifty samples are available (shown
at the bottom of the figure). The interpolation function chosen is a
Gaussian function with width h. The distance measure is Euclidean.
Figure 5.3(a) and Figure 5.3(b) show the estimations using h ¼ 1 and
h ¼ 0:2, respectively. These graphs illustrate a phenomenon related to
the choice of the interpolation function. If the interpolation function is
peaked, the influence of a sample is very local, and the variance of the
estimator is large. But if the interpolation is smooth, the variance
–2
0
2
4
6
8
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
z
Parzen estimate
p(z|ωk)
(b)
–2
0
2
4
6
8
10
0
0.05
0.1
0.15
0.2
0.25
z
Parzen estimate
p(z|ωk)
(a)
Figure 5.3
Parzen estimation of a density function using 50 samples. (a) h ¼ 1.
(b) h ¼ 0:2
152
SUPERVISED LEARNING

decreases, but at the same time the estimate becomes a smoothed version
of the true density. That is, the estimate becomes biased. By changing the
width of the interpolation function one can balance between the bias and
the variance. Of course, both types of errors can be reduced by enlarge-
ment of the training set.
Listing 5.3
The following PRTools listing generates plots similar to those in Figure
5.3. It generates 50 samples from a (a, b) distribution with a ¼ 2 and
b ¼ 1:5, and then estimates the density using the Parzen method with
two different kernel widths.
n ¼ 50; a ¼ 2; b ¼ 1.5;
x ¼ (2:0.01:10)’; y ¼ gampdf(x,a,b);
% Generate function
z ¼ dataset(gamrnd(a,b,n,1),genlab(n));
% Generate dataset
w ¼ parzenm(z,1);
% Parzen, sigma ¼ 1
figure; scatterd(z); axis([2 10 0 0.3]);
plotm(w,1); hold on; plot(x,y,’:’);
w ¼ parzenm(z,0.2);
% Parzen, sigma ¼ 0.2
figure; scatterd(z); axis([2 10 0 0.3]);
plotm(w,1); hold on; plot(x,y,’:’);
In the N-dimensional case, various interpolation functions are useful.
A popular one is the Gaussian function:
ðz; zjÞ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðz  zjÞTC1ðz  zjÞ
q
hðÞ ¼
1
N
h
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð2ÞNjCj
q
exp  2
22
h
 
!
ð5:26Þ
The matrix C must be symmetric and positive definite (Appendix B.5).
The metric is according to the Mahalanobis distance. The constant h
controls the size of the influence zone of h(). It can be chosen smaller as
the number of samples in the training set increases. If the training set is
very large, the actual choice of C is less important. If the set is not very
large, a suitable choice is the covariance matrix ^Ck determined according
to (5.14).
The following algorithm implements a classification based on Parzen
estimation:
NONPARAMETRIC LEARNING
153

Algorithm 5.1 Parzen classification
Input: a labelled training set TS, an unlabelled test set T.
1. Determination of h: maximize the log-likelihood of the training set
TS by varying h using leave-one-out estimation (see Section 5.4).
In other words, select h such that
X
K
k¼1
X
Nk
j¼1
ln ^pðzk;jj!kÞ


is maximized. Here, zk,j is the j-th sample from the k-th class, which is
left out during the estimation of ^p(zk,jj!k).
2. Density estimation: compute for each sample z in the test set the
density for each class:
^pðzj!kÞ ¼ 1
Nk
X
zj2Tk
1
N
h
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð2ÞN
q
exp  jjz  zjjj2
22
h
 
!
3. Classification: assign the samples in T to the class with the maximal
posterior probability:
^! ¼ !k
with
k ¼ argmax
i¼1;;K
^pðzj!iÞ^Pð!iÞ
n
o
Output: the labels ^! of T.
Example 5.2
Classification of mechanical parts, Parzen estimation
We return to Example 2.2 in Chapter 2, where mechanical parts like
nuts and bolts, etc. must be classified in order to sort them. Applica-
tion of Algorithm 5.1 with Gaussians as the kernels and estimated
covariance matrices as the weighting matrices yields h ¼ 0:0485 as
the optimal sigma. Figure 5.4(a) presents the estimated overall den-
sity. The corresponding decision boundaries are shown in Figure
5.4(b). To show that the choice of h significantly influences the
decision boundaries, in Figure 5.4(c) a similar density plot is shown,
for which h was set to 0.0175. The density estimate is more peaked,
and the decision boundaries (Figure 5.4(d)) are less smooth.
Figures 5.4(a–d) were generated using MATLAB code similar to that
given in Listing 5.3.
154
SUPERVISED LEARNING

5.3.2
Nearest neighbour classification
In Parzen estimation, each sample in the training set contributes in a like
manner to the estimate. The estimation process is space-invariant.
Consequently, the trade-off which exists between resolution and vari-
ance is a global one. A refinement would be to have an estimator with
high resolution in regions where the training set is dense, and with low
resolution in other regions. The advantage is that the balance between
resolution and variance can be adjusted locally.
0
0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
(a)
0
0.2
0.4
0.6
0.8
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
(b)
0
0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
(c)
0
0.2
0.4
0.6
0.8
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
(d)
Figure 5.4
Probability densities of the measurements shown in Figure 5.1. (a) The
3D plot of the Parzen estimate of the unconditional density together with a 2D
contour plot of this density on the ground plane. The parameter h was set to
0.0485. (b) The resulting decision boundaries. (c) Same as (a), but with h set to
0.0175. (d) Same as (b), for the density estimate shown in (c)
NONPARAMETRIC LEARNING
155

Nearest neighbour estimation is a method that implements such a
refinement. The method is based on the following observation. Let
R(z)  RN be a hypersphere with volume V. The centre of R(z) is z. If
the number of samples in the training set Tk is Nk, then the probability of
having exactly n samples within R(z) has a binomial distribution with
expectation:
E½n ¼ Nk
Z
y2RðzÞ
pðyj!kÞdy  NkVpðzj!kÞ
ð5:27Þ
Suppose that the radius of the sphere around z is selected such that this
sphere contains exactly  samples. It is obvious that this radius depends
on the position z in the measurement space. Therefore, the volume will
depend on z. We have to write V(z) instead of V. With that, an estimate
of the density is:
^pðzj!kÞ ¼

NkVðzÞ
ð5:28Þ
The expression shows that in regions where p(zj!k) is large, the volume
is expected to be small. This is similar to having a small interpolation
zone. If, on the other hand, p(zj!k) is small, the sphere needs to grow in
order to collect the required  samples.
The parameter  controls the balance between the bias and variance.
This is like the parameter h in Parzen estimation. The choice of  should
be such that:
 ! 1
as Nk ! 1
in order to obtain a low variance
=Nk ! 0 as Nk ! 1
in order to obtain a low bias
ð5:29Þ
A suitable choice is to make  proportional to
ﬃﬃﬃﬃﬃﬃﬃ
Nk
p
.
Nearest neighbour estimation is of practical interest because it paves
the way to a classification technique that directly uses the training set,
i.e. without explicitly estimating probability densities. The develop-
ment of this technique is as follows. We consider the entire training
set and use the representation TS as in (5.1). The total number of
samples is NS. Estimates of the prior probabilities follow from (5.18):
^P(!k) ¼ Nk/NS.
As before, let R(z)  RN be a hypersphere with volume V(z). In order
to classify a vector z we select the radius of the sphere around z such that
this sphere contains exactly  samples taken from TS. These samples are
156
SUPERVISED LEARNING

called the -nearest neighbours1 of z. Let k denote the number of
samples found with class !k. An estimate of the conditional density is
(see (5.28)):
^pðzj!kÞ 
k
NkVðzÞ
ð5:30Þ
Combination of (5.18) and (5.30) in the Bayes classification with
uniform cost function (2.12) produces the following suboptimal classi-
fication:
^!ðzÞ ¼ !k
with
k ¼ argmax
i¼1;...;K
^pðzj!iÞ^Pð!iÞ
n
o
¼ argmax
i¼1;...;K
i
NiVðzÞ
Ni
NS

	
¼ argmax
i¼1;...;K
fig
ð5:31Þ
The interpretation of this classification is simple. The class assigned to a
vector z is the class with the maximum number of votes coming from 
samples nearest to z. In literature, this classification method is known as
k-nearest neighbour rule classification (k-NNR, but in our nomenclature
-NNR). The special case in which  ¼ 1 is simply referred to as nearest
neighbour rule classification (NNR or 1-NNR).
Example 5.3
Classification of mechanical parts, NNR classification
PRTools can be used to perform -nearest neighbour classification.
Listing 5.4 shows how a -nearest neighbour classifier is trained on
the mechanical parts data set of Example 2.2. If  is not specified, it
will be found by minimizing the leave-one-out error. For this data set,
the optimal  is 7. The resulting decision boundaries are shown in
Figure 5.5(a). If we set  to 1, the classifier will classify all samples in
the training set correctly (see Figure 5.5(b)), but its performance on a
test set will be worse.
1 The literature about nearest neighbour classification often uses the symbol k to denote the
number of samples in a volume. However, in order to avoid confusion with symbols like !k, Tk,
etc. we prefer to use .
NONPARAMETRIC LEARNING
157

Listing 5.4
PRTools code for finding and plotting an optimal -nearest neighbour
classifier and a one-nearest neighbour classifier.
load nutsbolts;
% Load the dataset
[w,k] ¼ knnc(z);
% Train a k-NNR
disp(k);
% Show the optimal k found
figure; scatterd(z)
% Plot the dataset
plotc(w);
% Plot the decision boundaries
w ¼ knnc(z,1);
% Train a 1-NNR
figure; scatterd(z);
% Plot the dataset
plotc(w);
% Plot the decision boundaries
The analysis of the performance of -nearest neighbour classification is
difficult. This holds true especially if the number of samples in the
training set is finite. In the limiting case, when the number of samples
grows to infinity, some bounds on the error rate can be given. Let the
minimum error rate, i.e. the error rate of a Bayes classifier with uniform
cost function, be denoted by Emin. See Section 2.1.1. Since Emin is the
minimum error rate among all classifiers, the error rate of a -NNR,
denoted E, is bounded by:
Emin  E
ð5:32Þ
It can be shown that for the 1-NNR the following upper bound holds:
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
(b)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
(a)
Figure 5.5
Application of -NNR classification. (a)  ¼ 7. (b)  ¼ 1
158
SUPERVISED LEARNING

E1  Emin 2 
K
K  1 Emin


 2Emin
ð5:33Þ
Apparently, replacing the true probability densities with estimations
based on the first nearest neighbour gives an error rate that is at most
twice the minimum. Thus, at least half of the classification information
in a dense training set is contained in the first nearest neighbour.
In the two-class problem (K ¼ 2) the following bound can be proven:
E  Emin þ
E1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:5ð  1Þ
p
if  is odd
E ¼ E1
if  is even
ð5:34Þ
The importance of (5.34) is that it shows that the performance of the
-NNR approximates the optimum as  increases. This asymptotic
optimality holds true only if the training set is dense (NS ! 1). Never-
theless, even in the small sized training set given in Figure 5.5 it can be
seen that the 7-NNR is superior to the 1-NNR. The topology of the
compartments in Figure 5.5(b) is very specific for the given training set.
This is in contrast with the topology of the 7-NNR in Figure 5.5(a). The
7-NNR generalizes better than the 1-NNR.
Unfortunately, -NNR classifiers also have some serious disadvantages:
. A distance measure is needed in order to decide which sample in the
training set is nearest. Usually the Euclidean distance measure is
chosen, but this choice needs not to be optimal. A systematic
method to determine the optimal measure is hard to find.
. The optimality is reached only when  ! 1. But since at the same
time it is required that /NS ! 0, the demand on the size of the
training set is very high. If the size is not large enough, -NNR
classification may be far from optimal.
. If the training set is large, the computational complexity of -NNR
classification becomes a serious burden.
Many attempts have been made to remedy the last drawback. One method
is to design fast algorithms with suitably chosen data structures (e.g.
hierarchical data structures combined with a pre-ordered training set).
Another method is to preprocess the training set so as to speed up the
search for nearest neighbours. There are two principles on which this
reduction can be based: editing and condensing. The first principle is to
NONPARAMETRIC LEARNING
159

edit the training set such that the (expensive) -NNR can be replaced
with the 1-NNR. The strategy is to remove those samples from the
training set that when used in the 1-NNR would cause erroneous results.
An algorithm that accomplishes this is the so-called multi-edit algorithm.
The following algorithm is from Devijver and Kittler.
Algorithm 5.2 Multi-edit
Input: a labelled training set TS.
1. Diffusion: Partition the training set TS randomly into L disjunct
subsets: T0
1, T0
2, . . . , T0
L with TS ¼ [lT0
l, and L 	 3.
2. Classification: classify the samples in T0‘ using 1-NNR classification
with T0
(‘þ1)mod L as training set.
3. Editing: discard all the samples that were misclassified at step 2.
4. Confusion: pool all the remaining samples to constitute a new train-
ing set TS.
5. Termination: if the last I iterations produced no editing, then exit
with the final training set, else go to step 1.
Output: a subset of TS.
The subsets created in step 1 are regarded as independent random
selections. A minimum of three subsets is required in order to avoid a
two-way interaction between two subsets. Because in the first step the
subsets are randomized, it cannot be guaranteed that, if during one
iteration no changes in the training set occurred, changes in further
iterations are ruled out. Therefore, the algorithm does not stop immedi-
ately after an iteration with no changes has occurred.
The effect of the algorithm is that ambiguous samples in the training
set are removed. This eliminates the need to use the -NNR. The 1-NNR
can be used instead.
The second principle, called condensing, aims to remove samples that
do not affect the classification in any way. This is helpful to reduce the
computational cost. The algorithm – also from Devijver and Kittler – is
used to eliminate all samples in the training set that are irrelevant.
Algorithm 5.3 Condensing
Input: a labeled training set TS.
1. Initiation: set up two new training sets TSTORE and TGRABBAG; place
the first sample of TS in TSTORE, all other samples in TGRABBAG.
160
SUPERVISED LEARNING

2. Condensing: use 1-NNR classification with the current TSTORE to
classify a sample in TGRABBAG; if classified correctly, the sample is
retained in TGRABBAG, otherwise it is moved from TGRABBAG to
TSTORE; repeat this operation for all other samples in TGRABBAG.
3. Termination: if one complete pass is made through step 2 with no
transfer from TGRABBAG to TSTORE, or if TGRABBAG is empty, then
terminate; else go to step 2.
Output: a subset of TS.
The effect of this algorithm is that in regions where the training set is
overcrowded with samples of the same class most of these samples will
be removed. The remaining set will, hopefully, contain samples close to
the Bayes decision boundaries.
Example 5.4
Classification of mechanical parts, editing and
condensation
An example of a multi-edited training set is given in Figure 5.6(a). The
decision boundaries of the 1-NNR classifier are also shown. It can be
seen that the topology of the resulting decision function is in accord-
ance with the one of the 7-NNR given in Figure 5.5(a). Hence, multi-
editing improves the generalization property.
Figure 5.6(b) shows that condensing can be successful when applied
to a multi-edited training set. The decision boundaries in Figure 5.6(b)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of 6–fold rotational symmetry
measure of eccentricity
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of 6–fold rotational symmetry
measure of eccentricity
Figure 5.6
Application of editing and condensing. (a) Edited training set. (b) Edited
and condensed training set
NONPARAMETRIC LEARNING
161

are close to those in Figure 5.6(a), especially in the more important
areas of the measurement space.
The basic PRTools code used to generate Figure 5.6 is given in
Listing 5.5.
Listing 5.5
PRTools code for finding and plotting one-nearest neighbour classifiers
on both an edited and a condensed data set. The function edicon takes
a distance matrix as input. In PRTools, calculating a distance matrix is
implemented as a mapping proxm, so z  proxm(z) is the distance
matrix between all samples in z. See Section 7.2.
load nutsbolts;
% Load the dataset z
J ¼ edicon(zproxm(z),3,5,[]);
% Edit z
w ¼ knnc(z(J,:),1);
% Train a 1-NNR
figure; scatterd(z(J,:)); plotc(w);
J ¼ edicon(zproxm(z),3,5,10);
% Edit and condense z
w ¼ knnc(z(J,:),1);
% Train a 1-NNR
figure; scatterd(z(J,:)); plotc(w);
If a non-edited training set is fed into the condensing algorithm, it may
result in erroneous decision boundaries, especially in areas of the meas-
urement space where the training set is ambiguous.
5.3.3
Linear discriminant functions
Discriminant functions are functions gk(z),
k ¼ 1, . . . , K that are used
in a decision function as follows:
^!ðzÞ ¼ !n
with: n ¼ argmax
k¼1;...;K
fgkðzÞg
ð5:35Þ
Clearly, if gk(z) are the posterior probabilities P(!kjz), the decision
function becomes a Bayes decision function with a uniform cost func-
tion. Since the posterior probabilities are not known, the strategy is to
replace the probabilities with some predefined functions gk(z) whose
parameters should be learned from a labelled training set.
An assumption often made is that the samples in the training set can
be classified correctly with linear decision boundaries. In that case, the
discriminant functions take the form of:
gkðzÞ ¼ wT
k z þ wk
ð5:36Þ
162
SUPERVISED LEARNING

Functions of this type are called linear discriminant functions. In fact,
these functions implement a linear machine. See also Section 2.1.2.
The notation can be simplified by the introduction of an augmented
measurement vector y, defined as:
y ¼
z
1


ð5:37Þ
With that, the discriminant functions become:
gkðyÞ ¼ wT
k y
ð5:38Þ
where the scalar wk in (5.36) has been embedded in the vector wk by
augmenting the latter with the extra element wk.
The augmentation can also be used for a generalization that allows for
nonlinear machines. For instance, a quadratic machine is obtained with:
yðzÞ ¼
z
1
z2
0
z2
1
. ..
z2
N1
z0z1
z0z2
. ..
zN1zN

T
ð5:39Þ
The corresponding functions gk(y) ¼ wT
k y(z) are called generalized linear
discriminant functions.
Discriminant functions depend on a set of parameters. In (5.38) these
parameters are the vectors wk. In essence, the learning process boils down
to a search for parameters such that with these parameters the decision
function in (5.35) correctly classifies all samples in the training set.
The basic approach to find the parameters is to define a performance
measure that depends on both the training set and the set of parameters.
Adjustment of the parameters such that the performance measure is
maximized gives the optimal decision function; see Figure 5.7.
decision
function
performance
measure
parameter
adjustment
Training set
Parameters
Performance
Figure 5.7
Training by means of performance optimization
NONPARAMETRIC LEARNING
163

Strategies to adjust the parameters may be further categorized into ‘itera-
tive’ and ‘non-iterative’. Non-iterative schemes are found when the perfor-
mance measure allows for an analytic solution of the optimization. For
instance, suppose that the set of parameters is denoted by w and that the
performance measure is a continuous function J(w) of w. The optimal solu-
tion is one which maximizes J(w). Hence, the solution must satisfy qJ(w)
qw ¼ 0.
In iterative strategies the procedure to find a solution is numerical.
Samples from the training set are fed into the decision function. The
classes found are compared with the true classes. The result controls the
adjustment of the parameters. The adjustment is in a direction which
improves the performance. By repeating this procedure it is hoped that
the parameters iterate towards the optimal solution.
The most popular search strategy is the gradient ascent method (also
called steepest ascent)1. Suppose that the performance measure J(w) is a
continuous function of the parameters contained in w. Furthermore,
suppose that rJ(w) ¼ qJ(w)
qw
is the gradient vector. Then the gradient
ascent method updates the parameters according to:
wði þ 1Þ ¼ wðiÞ þ ðiÞrJ wðiÞ
ð
Þ
ð5:40Þ
where w(i) is the parameter obtained in the i-th iteration. (i) is the
so-called learning rate. If (i) is selected too small, the process converges
very slowly, but if it is too large, the process may overshoot the maximum,
or oscillate near the maximum. Hence, a compromise must be found.
Different choices of the performance measures and different search
strategies lead to a multitude of different learning methods. This section
confines itself to two-class problems. From the many iterative, gradient-
based methods we only discuss ‘perceptron learning’ and the ‘least
squared error learning’. Perhaps the practical significance of these two
methods is not very large, but they are introductory to the more involved
techniques of succeeding sections.
Perceptron learning
In a two-class problem, the decision function expressed in (5.35) is
equivalent to a test g1(y)  g2(y) > 0. If the test fails, it is decided for
!2, otherwise for !1. The test can be accomplished equally well with a
single linear function:
1 Equivalently, we define J(w) as an error measure. A gradient descent method should be
applied to minimize it.
164
SUPERVISED LEARNING

gðyÞ ¼ wTy
ð5:41Þ
defined as g(y) ¼ g1(y)  g2(y). The so-called perceptron, graphically
represented in Figure 5.8, is a computational structure that implements
g(y). The two possible classes are encoded in the output as ‘1’ and ‘1’.
A simple performance measure of a classifier is obtained by applying the
training set to the classifier, and to count the samples that are erroneously
classified. Obviously, such a performance measure – actually an error mea-
sure–shouldbeminimized.Thedisadvantageofthismeasureisthatitisnota
continuous function of y. Therefore, the gradient is not well defined.
The performance measure of the perceptron is based on the following
observation. Suppose that a sample yn is misclassified. Thus, if the true
class of the sample is !1, then g(yn) ¼ wTyn is negative, and if the true
class is !2, then g(yn) ¼ wTyn is positive. In the former case we would
like to correct wTyn with a positive constant, in the latter case with a
negative constant. We define Y1(w) as the set containing all !1 samples
in the training set that are misclassified, and Y2(w) as the set of all
misclassified !2 samples. Then:
JperceptronðwÞ ¼ 
X
y2Y1
wTy þ
X
y2Y2
wTy
ð5:42Þ
This measure is continuous in w and its gradient is:
rJperceptronðwÞ ¼ 
X
y2Y1
y þ
X
y2Y2
y
ð5:43Þ
Application of the gradient descent, see (5.40), gives the following
learning rule:
wði þ 1Þ ¼ wðiÞ  

X
y2Y1
y þ
X
y2Y2
y
 
!
ð5:44Þ
z0
z1
zN–1
w0
–1
w1
wN–1
wN
1
Σ
Figure 5.8
The perceptron
NONPARAMETRIC LEARNING
165

where i is the iteration count. The iteration procedure stops when
w(i þ 1) ¼ w(i), i.e. when all samples in the training set are classified
correctly. If such a solution exists, that is if the training set is linearly
separable, the perceptron learning rule will find it.
Instead of processing the full training set in one update step (so-called
batch processing) we can also cycle through the training set and update
the weight vector whenever a misclassified sample has been encountered
(single sample processing). If yn is a misclassified sample, then the
learning rule becomes:
wði þ 1Þ ¼ wðiÞ þ cnyn
ð5:45Þ
The variable cn is þ1 if yn is a misclassified !1 sample. If yn is a
misclassified !2 sample, then cn ¼ 1.
The error correction procedure of (5.45) can be extended to the multi-
class problem as follows. Let gk(y) ¼ wT
k y as before. We cycle through
the training set and update the weight vectors wk and wj whenever a !k
sample is classified as class !j:
wk ! wk þ yn
wj ! wj  yn
ð5:46Þ
The procedure will converge in a finite number of iterations provided
that the training set is linearly separable. Perceptron training is illus-
trated in Example 5.5.
Least squared error learning
A disadvantage of the perceptron learning rule is that it only works well
in separable cases. If the training set is not separable, the iterative
procedure often tends to fluctuate around some value. The procedure
is terminated at some arbitrary point, but it becomes questionable then
whether the corresponding solution is useful.
Non-separable cases can be handled if we change the performance
measure such that its maximization boils down to solving a set of linear
equations. Such a situation is created if we introduce a set of so-called
target vectors. A target vector tn is a K-dimensional vector associated
with the (augmented) sample yn. Its value reflects the desired response of
the discriminant function to yn. The simplest one is place coding:
tn;k ¼
1
if n ¼ !k
0
otherwise

ð5:47Þ
166
SUPERVISED LEARNING

(We recall that n is the class label of sample yn.) This target function
aims at a classification with minimum error rate.
We now can apply a least squares criterion to find the weight vectors:
JLS ¼
X
NS
n¼1
X
K
k¼1
wT
k yn  tn;k

2
ð5:48Þ
The values of wk that minimize JLS are the weight vectors of the least
squared error criterion.
The solution can be found by rephrasing the problem in a different nota-
tion. Let Y ¼ [ y1 . . . yNS ]T be a NS  (N þ 1) matrix, W ¼ [ w1 . . . wK ] a
(N þ 1)  K matrix, and T ¼ [ t1 . . . tNS ]T a NS  K matrix. Then:
JLS ¼ YW  T
k
k2
ð5:49Þ
where
k k2 is the Euclidean matrix norm, i.e. the sum of squared
elements. The value of W that minimizes JLS is the LS solution to the
problem:
WLS ¼ YTY

1YTT
ð5:50Þ
Of course, the solution is only valid if (YTY)1 exists. The matrix
(YTY)1YT is the pseudo inverse of Y. See (3.25).
An interesting target function is:
tn;k ¼ Cð!kjnÞ
ð5:51Þ
Here, tn embeds the cost that is involved if the assigned class is !k
whereas the true class is n. This target function aims at a classification
with minimal risk and the discriminant function gk(y) attempts to
approximate the risk PK
i¼1 C(!kj!i)P(!ijy) by linear LS fitting. The
decision function in (5.35) should now involve a minimization rather
than a maximization.
Example 5.5 illustrates how the least squared error classifier can be
found in PRTools.
Example 5.5
Classification of mechanical parts, perceptron and
least squared error classifier
Decision boundaries for the mechanical parts example are shown in
Figure 5.9(a) (perceptron) and Figure 5.9(b) (least squared error
NONPARAMETRIC LEARNING
167

classifier). These plots were generated by the code shown in Listing 5.6.
In PRTools, the linear perceptron classifier is implemented as perlc;
the least squared error classifier is called fisherc. For perlc to find
a good perceptron, the learning rate  had to be set to 0.01. Training
was stopped after 1000 iterations. Interestingly, the least squared error
classifier is not able to separate the data successfully, because the
‘scrap’ class is not linearly separable from the other classes.
Listing 5.6
PRTools code for finding and plotting a linear perceptron and least
squared error classifier on the mechanical parts data set.
load nutsbolts;
% Load the dataset
w ¼ perlc(z,1000,0.01);
% Train a linear perceptron
figure; scatterd(z); plotc(w);
w ¼ fisherc(z);
% Train a LS error classifier
figure; scatterd(z); plotc(w);
5.3.4
The support vector classifier
The basic support vector classifier is very similar to the perceptron. Both
are linear classifiers, assuming separable data. In perceptron learning,
the iterative procedure is stopped when all samples in the training set are
classified correctly. For linearly separable data, this means that the found
perceptron is one solution arbitrarily selected from an (in principle)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
Figure 5.9
Application of two linear classifiers. (a) Linear perceptron. (b) Least
squared error classifier
168
SUPERVISED LEARNING

infinite set of solutions. In contrast, the support vector classifier chooses
one particular solution: the classifier which separates the classes with
maximal margin. The margin is defined as the width of the largest ‘tube’
not containing samples that can be drawn around the decision boundary;
see Figure 5.10. It can be proven that this particular solution has the
highest generalization ability.
Mathematically, this can be expressed as follows. Assume we have
training samples zn, n ¼ 1,::, NS (not augmented with an extra element)
and for each sample a label cn 2 f1, 1g, indicating to which of the two
classes the sample belongs. Then a linear classifier g(z) ¼ wTz þ b is
sought, such that:
wTzn þ b 	 1
if
cn ¼ þ1
wTzn þ b  1
if
cn ¼ 1
for all n
ð5:52Þ
These two constraints can be rewritten into one inequality:
cnðwTzn þ bÞ 	 1
ð5:53Þ
The gradient vector of g(z) is w. Therefore, the square of the margin is
inversely proportional to
w
k k2¼ wTw. To maximize the margin, we
have to minimize w
k k2. Using Lagrange multipliers, we can incorporate
the constraints (5.53) into the minimization:
L ¼ 1
2 w
k k2þ
X
NS
n¼1
n cn wTzn þ b


 1


;
n 	 0
ð5:54Þ
cn = 1
cn = –1
support vectors
margin
wTz+b = –1
wTz+b = 0
wTz+b = +1
Figure 5.10
The linear support vector classifier
NONPARAMETRIC LEARNING
169

L should be minimized with respect to w and b, and maximized with
respect to the Lagrange multipliers n. Setting the partial derivates of L
w.r.t. w and b to zero results in the constraints:
w ¼
X
NS
n¼1
ncnzn
X
NS
n¼1
cnn ¼ 0
ð5:55Þ
Resubstituting this into (5.54) gives the so-called dual form:
L ¼
X
NS
n¼1
n  1
2
X
NS
n¼1
X
NS
m¼1
cncmnmzT
n zm;
n 	 0
ð5:56Þ
L should be maximized with respect to the n. This is a quadratic
optimization problem, for which standard software packages are avail-
able. After optimization, the n are used in (5.55) to find w. In typical
problems, the solution is sparse, meaning that many of the n become 0.
Samples zn for which n ¼ 0 are not required in the computation of w.
The remaining samples zn (for which n > 0) are called support vectors.
This formulation of the support vector classifier is of limited use: it
only covers a linear classifier for separable data. To construct nonlinear
boundaries, discriminant functions, introduced in (5.39), can be applied.
The data is transformed from the measurement space to a new feature
space. This can be done efficiently in this case because in formulation
(5.56) all samples are coupled to other samples by an inner product. For
instance, when all polynomial terms up to degree 2 are used (as in
(5.39)), we can write:
yðznÞTyðzmÞ ¼ ðzT
n zm þ 1Þ2 ¼ Kðzn; zmÞ
ð5:57Þ
This can be generalized further: instead of (zT
n zm þ 1)2 any integer degree
(zT
n zm þ 1)d with d > 1 can be used. Due to the fact that only the inner
products between the samples are considered, the very expensive explicit
expansion is avoided. The resulting decision boundary is a d-th degree
polynomial in the measurement space. The classifier w cannot easily
be expressed explicitly (as in (5.55)). However, we are only interested in the
classification result. And this is in terms of the inner product between the
object z to be classified and the classifier (compare also with (5.36)):
170
SUPERVISED LEARNING

gðzÞ ¼ wTyðzÞ ¼
X
NS
n¼1
Kðz; znÞ
ð5:58Þ
Replacing the inner product by a more general kernel function is called
the kernel trick. Besides polynomial kernels, other kernels have been
proposed. The Gaussian kernel with 2I as weighting matrix (the radial
basis function kernel, RBF kernel) is frequently used in practice:
Kðzn; zmÞ ¼ exp  kzn  zmk2
2
 
!
ð5:59Þ
For very small values of , this kernel gives very detailed boundaries,
while for high values very smooth boundaries are obtained.
In order to cope with overlapping classes, the support vector classifier
can be extended to have some samples erroneously classified. For that,
the hard constraints (5.53) are replaced by soft constraints:
wTzn þ b 	 1  	n
if
cn ¼ 1
wTzn þ b  1 þ 	n
if
cn ¼ 1
ð5:60Þ
Here so-called slack variables 	n 	 0 are introduced. These should be
minimized in combination with the w2. The optimization problem is
thus changed into:
L ¼ 1
2 w2 þ C
X
NS
n¼1
	n þ
X
NS
n¼1
nðcn wTzn þ b


 1 þ 	nÞ
þ
X
NS
n¼1
n	n;
n; n 	 0
ð5:61Þ
The second term expresses our desire to have the slack variables as small
as possible. C is a trade-off parameter that determines the balance
between having a large overall margin at the cost of more erroneously
classified samples, or having a small margin with less erroneously clas-
sified samples. The last term holds the Lagrange multipliers that are
needed to assure that 	n 	 0.
The dual formulation of this problem is the same as (5.56). Its deriv-
ation is left as an exercise for the reader; see exercise 6. The only
difference is that an extra upper bound on n is introduced: n  C.
NONPARAMETRIC LEARNING
171

It basically means that the influence of a single object on the description
of the classifier is limited. This upper bound avoids that noisy objects
with a very large weight completely determine the weight vector and
thus the classifier. The parameter C has a large influence on the final
solution, in particular when the classification problem contains over-
lapping class distributions. It should be set carefully. Unfortunately, it is
not clear beforehand what a suitable value for C will be. It depends on
both the data and the type of kernel function which is used. No generally
applicable number can be given. The only option in a practical applica-
tion is to run cross-validation (Section 5.4) to optimize C.
The support vector classifier has many advantages. A unique global
optimum for its parameters can be found using standard optimization
software. Nonlinear boundaries can be used without much extra com-
putational effort. Moreover, its performance is very competitive with
other methods. A drawback is that the problem complexity is not of the
order of the dimension of the samples, but of the order of the number of
samples. For large sample sizes (NS > 1000) general quadratic program-
ming software will often fail and special-purpose optimizers using
problem-specific speedups have to be used to solve the optimization.
A second drawback is that, like the perceptron, the classifier is basic-
ally a two-class classifier. The simplest solution for obtaining a classifier
with more than two classes is to train K classifiers to distinguish one
class from the rest (similar to the place coding mentioned above). The
classifier with the highest output wT
k z þ b then determines the class label.
Although the solution is simple to implement, and works reasonable
well, it can lead to problems because the output value of the support
vector classifier is only determined by the margin between the classes it is
trained for, and is not optimized to be used for a confidence estimation.
Other methods train K classifiers simultaneously, incorporating the one-
class-against-the-rest labelling directly into the constraints of the optim-
ization. This gives again a quadratic optimization problem, but the
number of constraints increases significantly which complicates the
optimization.
Example 5.6
Classification of mechanical parts, support vector
classifiers
Decision boundaries found by support vector classifiers for the
mechanical parts example are shown in Figure 5.11. These plots were
generated by the code shown in Listing 5.7. In Figure 5.11(a), the
kernel used was a polynomial one with degree d ¼ 2 (a quadratic
kernel); in Figure 5.11(b), it was a Gaussian kernel with a width
172
SUPERVISED LEARNING

 ¼ 0:1. In both cases, the trade-off parameter C was set to 100; if it
was set smaller, especially the support vector classifier with the poly-
nomial kernel did not find good results. Note in Figure 5.11(b) how
the decision boundary is built up by Gaussians around the support
vectors, and so forms a closed boundary around the classes.
Listing 5.7
PRTools code for finding and plotting two different support vector
classifiers.
load nutsbolts;
% Load the dataset
w ¼ svc(z,‘p’,2,100);
% Train a quadratic kernel svc
figure; scatterd(z); plotc(w);
w ¼ svc(z,‘r’,0.1,100);
% Train a Gaussian kernel svc
figure; scatterd(z); plotc(w);
5.3.5
The feed-forward neural network
A neural network extends the perceptron in another way: it combines
the output of several perceptrons by another perceptron. A single per-
ceptron is called a neuron in neural network terminology. Like a percep-
tron, a neuron computes the weighted sum of the inputs. However,
instead of a sign function, a more general transfer function is applied.
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
(a)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
(b)
Figure 5.11
Application of two support vector classifiers. (a) Polynomial kernel,
d ¼ 2, C ¼ 100. (b) Gaussian kernel,  ¼ 0:1, C ¼ 100
NONPARAMETRIC LEARNING
173

A transfer function used often is the sigmoid function, a continuous
version of the sign function:
gðyÞ ¼ fðwTyÞ ¼
1
1 þ expðwTyÞ
ð5:62Þ
where y is the vector z augmented with a constant value 1. The vector
w is called the weight vector, and the specific weight corresponding to
the constant value 1 in z is called the bias weight.
In principle, several layers of different numbers of neurons can be
constructed. For an example, see Figure 5.12. Neurons which are not
directly connected to the input or output are called hidden neurons. The
hidden neurons are organized in (hidden) layers. If all neurons in the
network compute their output based only on the output of neurons in
previous layers, the network is called a feed-forward neural network. In
a feed-forward neural network, no loops are allowed (neurons cannot
get their input from next layers).
Assume that we have only one hidden layer with H hidden neurons.
The output of the total neural network is:
gkðyÞ ¼ f
X
H
h¼1
vk;hfðwT
h yÞ þ vk;Hþ1
 
!
ð5:63Þ
Here, wh is the weight vector of the inputs to hidden neuron h, and vk is
the weight vector of the inputs to output neuron k. Analogous to least
input
layer
output
layer
hidden
layers
Figure 5.12
A two-layer feed-forward neural network with two input dimensions
and one output (for presentation purposes, not all connections have been drawn)
174
SUPERVISED LEARNING

squared error fitting, we define a sum of squared errors between the
output of the neural network and the target vector:
JSE ¼ 1
2
X
NS
n¼1
X
K
k¼1
gkðynÞ  tn;k

2
ð5:64Þ
The target vector is usually created by place coding: tn,k ¼ 1 if the label
of sample yn is !k, otherwise it is 0. However, as the sigmoid function
lies in the range <0, 1>, the values 0 and 1 are hard to reach, and as a
result the weights will grow very large. To prevent this, often targets are
chosen that are easier to reach, e.g. 0.8 and 0.2.
Because all neurons have continuous transfer functions, it is possible
to compute the derivative of this error JSE with respect to the weights.
The weights can then be updated using gradient descent. Using the chain
rule, the updates of vk,h are easy to compute:
vk;h ¼  qJSE
qvk;h
¼
X
NS
n¼1
gkðynÞ  tn;k

_f
X
H
h¼1
vk;hfðwT
h yÞ þ vk;Hþ1
 
!
fðwT
h yÞ
ð5:65Þ
The derivation of the gradient with respect to wh,i is more complicated:
wh;i ¼  qJSE
qwh;i
¼
X
K
k¼1
X
NS
n¼1
gkðynÞ  tn;k


vk;h_fðwT
h yÞyi_f
X
H
h¼1
vk;hfðwT
h yÞ þ vk;Hþ1
 
!
ð5:66Þ
For the computation of equation (5.66) many elements of equation
(5.65) can be reused. This also holds when the network contains more
than one hidden layer. When the updates for vk,h are computed first, and
those for wh,i are computed from that, we effectively distribute the error
between the output and the target value over all weights in the network.
We back-propagate the error. The procedure is called back-propagation
training.
The number of hidden neurons and hidden layers in a neural network
controls how nonlinear the decision boundary can be. Unfortunately, it
is hard to predict which number of hidden neurons is suited for the task
NONPARAMETRIC LEARNING
175

at hand. In practice, we often train a number of neural networks of
varying complexity and compare their performance on an independent
validation set. The danger of finding a too nonlinear decision boundary
is illustrated in Example 5.7.
Example 5.7
Classification of mechanical parts, neural networks
Figure 5.13 shows the decision boundaries found by training two
neural networks. The first network, whose decision boundaries is
shown in Figure 5.13(a), contains one hidden layer of five units. This
gives a reasonably smooth decision boundary. For the decision func-
tion shown in Figure 5.13(c), a network was used with two hidden
layers of 100 units each. This network has clearly found a highly
nonlinear solution, which does not generalize as well as the first
network. For example, the ‘’ region (nuts) contains one outlying ‘x’
sample (scrap). The decision boundary bends heavily to include the
single ‘x’ sample within the scrap region. Although such a crumpled
curve decreases the squared error, it is undesirable because the outlying
‘x’ is not likely to occur again at that same location in other realizations.
Note also the spurious region in the right bottom of the plot, in
which samples are classified as ‘bolt’ (denoted by þ in the scatterplot).
Here too, the network generalizes poorly as it has not seen any
examples in this region.
Figures 5.13(b) and (d) show the learn curves that were derived
during training the network. One epoch is a training period in which
the algorithm has cycled through all training samples. The figures
show ‘error’, which is the fraction of the training samples that are
erroneously classified, and ‘mse’, which is 2JSE/(KNS). The larger
the network is, the more epochs are needed before the minimum will
be reached. However, sometimes it is better to stop training before
actually reaching the minimum because the generalization ability can
degenerate in the vicinity of the minimum.
The figures were generated by the code shown in Listing 5.8.
Listing 5.8
PRTools code for training and plotting two neural network classifiers.
load nutsbolts;
% Load the dataset
[w,R] ¼ bpxnc(z,5,500);
% Train a small
network
figure; scatterd(z); plotc(w);
% Plot the
classifier
176
SUPERVISED LEARNING

figure; plotyy(R(:,1),R(:,2),R(:,1),R(:,4));
% Plot the learn
curves
[w,R] ¼ bpxnc(z,[100 100],1000);
% Train a larger
network
figure; scatterd(z); plotc(w);
% Plot the
classifier
figure; plotyy(R(:,1),R(:,2),R(:,1),R(:,4));
% Plot the learn
curves
5.4
EMPIRICAL EVALUATION
In the preceding sections various methods for training a classifier have
been discussed. These methods have led to different types of classifiers
and different types of learning rules. However, none of these methods
can claim overall superiority above the other because their applicability
and effectiveness is largely determined by the specific nature of the
problem at hand. Therefore, rather than relying on just one method that
has been selected at the beginning of the design process, the designer
often examines various methods and selects the one that appears most
suitable. For that purpose, each classifier has to be evaluated.
Another reason for performance evaluation stems from the fact that
many classifiers have their own parameters that need to be tuned. The
optimization of a design criterion using only training data holds the risk
of overfitting the design, leading to an inadequate ability to generalize.
The behaviour of the classifier becomes too specific for the training data
at hand, and is less appropriate for future measurement vectors coming
from the same application. Particularly, if there are many parameters
relative to the size of the training set and the dimension of the measure-
ment vector, the risk of overfitting becomes large (see also Figure 5.13
and Chapter 6). Performance evaluation based on a validation set (test
set, evaluation set), independent from the training set, can be used as a
stopping criterion for the parameter tuning process.
A third motivation for performance evaluation is that we would like
to have reliable specifications of the design anyhow.
There are many criteria for the performance of a classifier. The prob-
ability of misclassification, i.e. the error rate, is the most popular one. The
analytical expression for the error rate as given in (2.16) is not very useful
because, in practice, the conditional probability densities are unknown.
However, we can easily obtain an estimate of the error rate by subjecting
the classifier to a validation set. The estimated error rate is the fraction of
misclassified samples with respect to the size of the validation set.
EMPIRICAL EVALUATION
177

Where the classification uses the reject option we need to consider two
measures: the error rate and the reject rate. These two measures are not
fully independent because lowering the reject rate will increase the error
rate. A plot of the error rate versus the reject rate visualizes this dependency.
The error rate itself is somewhat crude as a performance measure.
In fact, it merely suffices in the case of a uniform cost function. A more
profound insight of the behaviour of the classifier is revealed by the
so-called confusion matrix. The i, j-th element of this matrix is the count
of !i samples in the validation set to which class !j is assigned. The
corresponding PRTools function is confmat().
Another design criterion might be the computational complexity
of a classifier. From an engineering point of view both the processor
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of six-fold rotational symmetry
measure of eccentricity
(a)
0
100
200
300
400
500
0
0.5
1
epoch
10–3
10–2
10–1
100
mse
error
(b)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of eccentricity
(c)
0
200
400
600
800
1000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
epoch
10–3
10–2
10–1
100
mse
error
(d)
Figure 5.13
Application of two neural networks. (a) One hidden layer of five units
(b) Learn curves of (a). (c) Two hidden layers of 100 units each. (d) Learn curves of (c)
178
SUPERVISED LEARNING

capability and the storage capability are subjected to constraints. At the
same time, the application may limit the computational time needed for
the classification. Often, a trade-off must be found between computa-
tional complexity and performance. In order to do so, we have to assess
the number of operations per classification and the storage require-
ments. These aspects scale with the number of elements N in the
measurement vector, the number of classes K, and for some classifiers
the size NS of the training set. For instance, the number of operations of
a quadratic machine is of the order of KN2, and so is its memory
requirement. For binary measurements, the storage requirement is of
the order of K2N (which becomes prohibitive even for moderate N).
Often, the acquisition of labelled samples is laborious and costly.
Therefore, it is tempting to use the training set also for evaluation
purposes. However, the strategy of ‘testing from training data’ disguises
the phenomenon of overfitting. The estimated error rate is likely to be
strongly overoptimistic. To prevent this, the validation set should be
independent of the training set.
A straightforward way to accomplish this is the so-called holdout
method. The available samples are randomly partitioned into a training
set and a validation set. Because the validation set is used to estimate
only one parameter, i.e. the error rate, and the training set is used to
tune all other parameters of the classifier (which may be quite numer-
ous), the training set must be larger than the validation set. As a rule of
thumb, the validation set contains about 20% of the data, and the
training set the remaining 80%.
Suppose that a validation set consisting of, say, NTest labelled samples
is available. Application of the ‘classifier-under-test’ to this validation
set results in nerror misclassified samples. If the true error rate of the
classifier is denoted by E, then the estimated error rate is:
^E ¼ nerror
NTest
ð5:67Þ
The random variable nerror has a binomial distribution with parameters
E and NTest. Therefore, the expectation of ^E and its standard deviation is
found as:
E ^E
h i
¼ E½nerror
NTest
¼ ENTest
NTest
¼ E
^E ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Var½nerror
p
NTest
¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
NTestð1  EÞE
p
NTest
¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð1  EÞE
NTest
s
ð5:68Þ
Hence, ^E is an unbiased, consistent estimate of E.
EMPIRICAL EVALUATION
179

Usually, the required number of samples NTest is defined such that ^E is
within some specified margin around the true E with a prescribed prob-
ability. It can be calculated from the posterior probability density
p(Ejnerror, NTest). See exercise 5. However, we take our ease by simply
requiring that the relative uncertainty ^E=E is equal to some fixed
fraction . Substitution in (5.68) and solving for NTest we obtain:
NTest ¼ 1  E
2E
ð5:69Þ
Figure 5.14 shows the required number of samples for different values of
E such that the relative uncertainty is 10%. The figure shows that with
E ¼ 0:01 the number of samples must be about 10 000.
The holdout method is not economic in handling the available data
because only part of that data is used for training. A classifier trained
with a reduced training set is expected to be inferior to a classifier
trained with all available data. Particularly if the acquisition of labelled
data is expensive, we prefer methods that use the data as much as
possible for training and yet give unbiased estimates for the error rate.
Examples of methods in this category are the cross-validation method
and the leave-one-out method. These methods are computationally
expensive, because they require the design of many classifiers.
The cross-validation method randomly partitions the available data
into L equally sized subsets T(‘), ‘ ¼ 1, . . . , L. First, the subset T(1) is
0
0.1
0.2
0.3
0.4
0.5
102
103
104
E
NTest
γ = 10%
Figure 5.14
Required number of test samples
180
SUPERVISED LEARNING

withheld and a classifier is trained using all the data from the remaining
L  1 subsets. This classifier is tested using T(1) as validation data,
yielding an estimate ^E(1). This procedure is repeated for all other sub-
sets, thus resulting in L estimates ^E(‘). The last step is the training of the
final classifier using all available data. Its error rate is estimated as the
average over ^E(‘). This estimate is a little pessimistic especially if L is small.
The leave-one-out method is essentially the same as the cross-
validation method except that now L is as large as possible, i.e. equal
to NS (the number of available samples). The bias of the leave-one-out
method is negligible, especially if the training set is large. However, it
requires the training of NS þ 1 classifiers and it is computationally very
expensive if NS is large.
5.5
REFERENCES
Bishop, C.M., Neural Networks for Pattern Recognition, Oxford University Press,
Oxford, UK, 1995.
Duda, R.O., Hart, P.E. and Stork, D.G., Pattern Classification, Wiley, London, UK, 2001.
Devijver, P.A. and Kittler, J., Pattern Recognition, a Statistical Approach, Prentice-Hall,
London, UK, 1982.
Haykin, S., Neural Networks – a Comprehensive Foundation, 2nd ed., Prentice-Hall,
Upper Saddle River, NJ, 1998.
Hertz, J., Krogh, A. and Palmer, R.G., Introduction to the Theory of Neural Computa-
tion, Addison-Wesley, Reading, MA, 1991.
Ripley, B.D., Pattern Recognition and Neural Networks, Cambridge University Press,
Cambridge, UK, 1996.
Vapnik, V.N., Statistical Learning Theory, Wiley, New York, 1998.
5.6
EXERCISES
1. Prove that if C is the covariance matrix of a random vector z, then
1
N C
is the covariance matrix of the average of N realizations of z. (0)
2. Show that (5.16) and (5.17) are equivalent. ()
3. Prove that, for the two-class case, (5.50) is equivalent to the Fisher linear discriminant
(6.52). ()
4. Investigate the behaviour (bias and variance) of the estimators for the conditional
probabilities of binary measurements, i.e. (5.20) and (5.22), at the extreme ends. That
is, if Nk << 2N and Nk >> 2N. ()
EXERCISES
181

5. Assuming that the prior probability density of the error rate of a classifier is uniform
between 0 and 1=K, give an expression of the posterior density p(Ejnerror, NTest)
where NTest is the size of an independent validation set and nerror is the number of
misclassifications of the classifier. ()
6. Derive the dual formulation of the support vector classifier from the primal formula-
tion. Do this by setting the partial derivatives of L to zero, and substituting the results
in the primal function. ()
7. Show that the support vector classifiers with slack variables gives almost the same
dual formulation as the one without slack variables (5.56). ()
8. Derive the neural network weight update rules (5.65) and (5.66). ()
9. Neural network weights are often initialized to random values in a small range, e.g.
<0:01, 0:01>. As training progresses, the weight values quickly increase. How-
ever, the support vector classifier tells us that solutions with small norms of the
weight vector have high generalization capability. What would be a simple way to
assure that the network does not become too nonlinear? ()
10. Given the answer to exercise 9, what will be the effect of using better optimization
techniques (such as second-order algorithms) in neural network training? Validate
this experimentally using PRTools lmnc function. ()
182
SUPERVISED LEARNING

6
Feature Extraction and
Selection
In some cases, the dimension N of a measurement vector z, i.e. the
number of sensors, can be very high. In image processing, when raw
image data is used directly as the input for a classification, the dimen-
sion can easily attain values of 104 (a 100  100 image) or more. Many
elements of z can be redundant or even irrelevant with respect to the
classification process.
For two reasons, the dimension of the measurement vector cannot be
taken arbitrarily large. The first reason is that the computational com-
plexity becomes too large. A linear classification machine requires in the
order of KN operations (K is the number of classes; see Chapter 2).
A quadratic machine needs about KN2 operations. For a machine acting
on binary measurements the memory requirement is on the order of
K2N. This, together with the required throughput (number of classifica-
tions per second), the state of the art in computer technology and the
available budget define an upper bound to N.
A second reason is that an increase of the dimension ultimately causes
a decrease of performance. Figure 6.1 illustrates this. Here, we have a
measurement space with the dimension N varying between 1 and 13.
There are two classes (K ¼ 2) with equal prior probabilities. The (true)
minimum error rate Emin is the one which would be obtained if all class
densities of the problem were fully known. Clearly, the minimum error
Classification, Parameter Estimation and State Estimation: An Engineering Approach using MATLAB
F. van der Heijden, R.P.W. Duin, D. de Ridder and D.M.J. Tax
 2004 John Wiley & Sons, Ltd
ISBN: 0-470-09013-8

rate is a non-increasing function of the number of sensors. Once an
element has been added with discriminatory information, the addition
of another element cannot destroy this information. Therefore, with
growing dimension, class information accumulates.
However, in practice the densities are seldom completely known.
Often, the classifiers have to be designed using a (finite) training set instead
of using knowledge about the densities. In the example of Figure 6.1
the measurement data is binary. The number of states a vector can take
is 2N. If there are no constraints on the conditional probabilities, then
the number of parameters to estimate is in the order of 2N. The number
of samples in the training set must be much larger than this. If not,
overfitting occurs and the trained classifier will become too much
adapted to the noise in the training data. Figure 6.1 shows that if the
size of the training set is NS ¼ 20, the optimal dimension of the mea-
surement vector is about N ¼ 4; that is where the error rate E is lowest.
Increasing the sample size permits an increase of the dimension. With
NS ¼ 80 the optimal dimension is about N ¼ 6.
One strategy to prevent overfitting, or at least to reduce its effect, has
already been discussed in Chapter 5: incorporating more prior know-
ledge by restricting the structure of the classifier (for instance, by an
appropriate choice of the discriminant function). In the current chapter,
2
4
6
8
10
12
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
dimension of measurement space
N
error rate
E
NS = 20
NS = 80
Emin (NS = ∞)
Figure 6.1
Error rates versus dimension of measurement space
184
FEATURE EXTRACTION AND SELECTION

an alternative strategy will be discussed: the reduction of the dimension
of the measurement vector. An additional advantage of this strategy is
that it automatically reduces the computational complexity.
For the reduction of the measurement space, two different approaches
exist. One is to discard certain elements of the vector and to select the
ones that remain. This type of reduction is feature selection. It is dis-
cussed in Section 6.2. The other approach is feature extraction. Here, the
selection of elements takes place in a transformed measurement space.
Section 6.3 addresses the problem of how to find suitable transforms.
Both methods rely on the availability of optimization criteria. These are
discussed in Section 6.1.
6.1
CRITERIA FOR SELECTION AND EXTRACTION
The first step in the design of optimal feature selectors and feature
extractors is to define a quantitative criterion that expresses how well
such a selector or extractor performs. The second step is to do the actual
optimization, i.e. to use that criterion to find the selector/extractor that
performs best. Such an optimization can be performed either analytically
or numerically.
Within a Bayesian framework ‘best’ means the one with minimal risk.
Often, the cost of misclassification is difficult to assess, or even fully
unknown. Therefore, as an optimization criterion the risk is often
replaced by the error rate E. Techniques to assess the error rate empiric-
ally by means of a validation set are discussed in Section 5.4. However,
in this section we need to be able to manipulate the criterion mathemat-
ically. Unfortunately, the mathematical structure of the error rate is
complex. The current section introduces some alternative, approximate
criteria that are simple enough for a mathematical treatment.
In feature selection and feature extraction, these simple criteria are
used as alternative performance measures. Preferably, such performance
measures have the following properties:
. The measure increases as the average distance between the expecta-
tion vectors of different classes increases. This property is based
on the assumption that the class information of a measurement
vector is mainly in the differences between the class-dependent
expectations.
. The measure decreases with increasing noise scattering. This prop-
erty is based on the assumption that the noise on a measurement
CRITERIA FOR SELECTION AND EXTRACTION
185

vector does not contain class information, and that the class dis-
tinction is obfuscated by this noise.
. The measure is invariant to reversible linear transforms. Suppose
that the measurement space is transformed to a feature space, i.e.
y ¼ Az with A an invertible matrix, then the measure expressed
in the y space should be exactly the same as the one expressed in
the z space. This property is based on the fact that both spaces carry
the same class information.
. The measure is simple to manipulate mathematically. Preferably,
the derivatives of the criteria are obtained easily as it is used as an
optimization criterion.
From the various measures known in literature (Devijver and Kittler, 1982),
two will be discussed. One of them – the interclass/intraclass distance
(Section 6.1.1) – applies to the multi-class case. It is useful if class informa-
tion is mainly found in the differences between expectation vectors in the
measurement space, while at the same time the scattering of the measure-
ment vectors (due to noise) is class-independent. The second measure – the
Chernoff distance (Section 6.1.2) – is particularly useful in the two-class
case because it can then be used to express bounds on the error rate.
Section 6.1.3 concludes with an overview of some other performance
measures.
6.1.1
Inter/intra class distance
The inter/intra distance measure is based on the Euclidean distance
between pairs of samples in the training set. We assume that the class-
dependent distributions are such that the expectation vectors of the
different classes are discriminating. If fluctuations of the measurement
vectors around these expectations are due to noise, then these fluctu-
ations will not carry any class information. Therefore, our goal is to
arrive at a measure that is a monotonically increasing function of the
distance between expectation vectors, and a monotonically decreasing
function of the scattering around the expectations.
As in Chapter 5, TS is a (labelled) training set with NS samples. The
classes !k are represented by subsets Tk  TS, each class having Nk
samples (Nk ¼ NS). Measurement vectors in TS – without reference
to their class – are denoted by zn. Measurement vectors in Tk (i.e. vectors
coming from class !k) are denoted by zk, n.
186
FEATURE EXTRACTION AND SELECTION

The development starts with the following definition of the average
squared distance of pairs of samples in the training set:
 2 ¼ 1
N2
S
X
NS
n¼1
X
NS
m¼1
ðzn  zmÞTðzn  zmÞ
ð6:1Þ
The summand (zn  zm)T(zn  zm) is the squared Euclidean distance
between a pair of samples. The sum involves all pairs of samples in the
training set. The divisor N2
S accounts for the number of terms.
The distance  2 is useless as a performance measure because none of
the desired properties mentioned above are met. Moreover, 2 is defined
without any reference to the labels of the samples. Thus, it does not give
any clue about how well the classes in the training set can be discrimin-
ated. To correct this,  2 must be divided into a part describing the
average distance between expectation vectors and a part describing
distances due to noise scattering. For that purpose, estimations of the
conditional expectations (mk ¼ E[zj!k]) of the measurement vectors are
used,
along
with
an
estimate
of
the
unconditional
expectation
(m ¼ E[z]). The sample mean of class !k is:
^mk ¼ 1
Nk
X
Nk
n¼1
zk;n
ð6:2Þ
The sample mean of the entire training set is:
^m ¼ 1
NS
X
NS
n¼1
zn
ð6:3Þ
With these definitions, it can be shown (see exercise 1) that the average
squared distance is:
2 ¼ 2
NS
X
K
k¼1
X
Nk
n¼1
ðzk;n  ^mkÞTðzk;n  ^mkÞ þ ð^m  ^mkÞTð^m  ^mkÞ


"
#
ð6:4Þ
The first term represents the average squared distance due to the scatter-
ing of samples around their class-dependent expectation. The second
term corresponds to the average squared distance between class-
dependent expectations and the unconditional expectation.
CRITERIA FOR SELECTION AND EXTRACTION
187

An alternative way to represent these distances is by means of scatter
matrices. A scatter matrix gives some information about the dispersion
of a population of samples around their mean. For instance, the matrix
that describes the scattering of vectors from class !k is:
Sk ¼ 1
Nk
X
Nk
n¼1
ðzk;n  ^mkÞðzk;n  ^mkÞT
ð6:5Þ
Comparison with equation (5.14) shows that Sk is close to an unbiased
estimate of the class-dependent covariance matrix. In fact, Sk is the
maximum likelihood estimate of Ck. With that, Sk does not only supply
information about the average distance of the scattering, it also supplies
information about the eccentricity and orientation of this scattering.
This is analogous to the properties of a covariance matrix.
Averaged over all classes the scatter matrix describing the noise is:
Sw ¼ 1
NS
X
K
k¼1
NkSk ¼ 1
NS
X
K
k¼1
X
Nk
n¼1
ðzk;n  ^mkÞðzk;n  ^mkÞT
ð6:6Þ
This matrix is the within-scatter matrix as it describes the average
scattering within classes. Complementary to this is the between-scatter
matrix Sb that describes the scattering of the class-dependent sample
means around the overall average:
Sb ¼ 1
NS
X
K
k¼1
Nkð^mk  ^mÞð^mk  ^mÞT
ð6:7Þ
Figure 6.2 illustrates the concepts of within-scatter matrices and
between-scatter matrices. The figure shows a scatter diagram of a train-
ing set consisting of four classes. A scatter matrix S corresponds to an
ellipse, zS1zT ¼ 1, that can be thought of as a contour roughly sur-
rounding the associated population of samples. Of course, strictly speak-
ing the correspondence holds true only if the underlying probability
density is Gaussian-like. But even if the densities are not Gaussian, the
ellipses give an impression of how the population is scattered. In the
scatter diagram in Figure 6.2 the within-scatter Sw is represented by four
similar ellipses positioned at the four conditional sample means. The
between-scatter Sb is depicted by an ellipse centred at the mixture sample
mean.
188
FEATURE EXTRACTION AND SELECTION

With the definitions in (6.5), (6.6) and (6.7) the average squared dis-
tance in (6.4) is proportional to the trace of the matrix Sw þ Sb; see (b.22):
2 ¼ 2traceðSw þ SbÞ ¼ 2traceðSwÞ þ 2traceðSbÞ
ð6:8Þ
Indeed, this expression shows that the average distance is composed of a
contribution due to differences in expectation and a contribution due to
noise. The term JINTRA ¼ trace(Sw) is called the intraclass distance. The
term JINTER ¼ trace(Sb) is the interclass distance. Equation (6.8) also
shows that the average distance is not appropriate as a performance
measure, since a large value of 2 does not imply that the classes are well
separated, and vice versa.
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
scatter diagram
sb
–0.5
0
0.5
–0.5
0
0.5
within-class and between-class scatter matrices
sb
sw
–0.5
0
0.5
–0.5
0
0.5
after decorrelation
sb
sw
–1 0 1
–1
0
1
after whitening and decorrelation
sb
sw
Figure 6.2
The inter/intra class distance
CRITERIA FOR SELECTION AND EXTRACTION
189

A performance measure more suited to express the separability of
classes is the ratio between interclass and intraclass distance:
JINTER
JINTRA
¼ traceðSbÞ
traceðSwÞ
ð6:9Þ
This measure possesses some of the desired properties of a performance
measure. In Figure 6.2, the numerator, trace(Sb), measures the area of
the ellipse associated with Sb. As such, it measures the fluctuations of the
conditional expectations around the overall expectation, i.e. the fluctu-
ations of the ‘signal’. The denominator, trace(Sw), measures the area of
the ellipse associated with Sw. As such, it measures the fluctuations due
to noise. Therefore, trace(Sb)/trace(Sw) can be regarded as a ‘signal-
to-noise ratio’.
Unfortunately, the measure of (6.9) oversees the fact that the ellipse
associated with the noise can be quite large, but without having a large
intersection with the ellipse associated with the signal. A large Sw can be
quite harmless for the separability of the training set. One way to correct
this defect is to transform the measurement space such that the within-
scattering becomes white, i.e. Sw ¼ I. For that purpose, we apply a linear
operation to all measurement vectors yielding feature vectors yn ¼ Azn.
In the transformed space the within- and between-scatter matrices
become: ASwAT and ASbAT, respectively. The matrix A is chosen such
that ASwAT ¼ I.
The matrix A can be found by factorization: Sw ¼ VVT where L is a
diagonal matrix containing the eigenvalues of Sw, and V a unitary matrix
containing the corresponding eigenvectors; see appendix B.5. With this
factorization it follows that A ¼ 1/2VT. An illustration of the process
is depicted in Figure 6.2. The operation VT performs a rotation that
aligns the axes of Sw. It decorrelates the noise. The operation 1/2 scales
the axes. The normalized within-scatter matrix corresponds with a circle
with unit radius. In this transformed space, the area of the ellipse
associated with the between-scatter is a useable performance measure:
JINTER=INTRA ¼ traceð1
2VTSbV1
2Þ
¼ traceðV1VTSbÞ
¼ traceðS1
w SbÞ
ð6:10Þ
This performance measure is called the inter/intra distance. It meets all
of our requirements stated above.
190
FEATURE EXTRACTION AND SELECTION

Example 6.1
Interclass and intraclass distance
Numerical calculations of the example in Figure 6.2 show that before
normalization JINTRA ¼ trace(Sw) ¼ 0:016 and JINTER ¼ trace(Sb) ¼
0:54. Hence, the ratio between these two is JINTER=JINTRA ¼ 33:8.
After normalization, JINTRA ¼ N ¼ 2, JINTER ¼ JINTER=INTRA ¼ 59:1,
and JINTER=JINTRA ¼ trace(Sb)=trace(Sw) ¼ 29:6. In this example,
before normalization, the JINTER=JINTRA measure is too optimistic.
The normalization accounts for this phenomenon.
6.1.2
Chernoff–Bhattacharyya distance
The interclass and intraclass distances are based on the Euclidean
metric defined in the measurement space. Another possibility is to use
a metric based on probability densities. Examples in this category are the
Chernoff distance (Chernoff, 1952) and the Bhattacharyya distance
(Bhattacharyya, 1943). These distances are especially useful in the two-
class case.
The merit of the Bhattacharyya and Chernoff distance is that an
inequality exists with which the distances bound the minimum error
rate Emin. The inequality is based on the following relationship:
minfa; bg 
ﬃﬃﬃﬃﬃﬃ
ab
p
ð6:11Þ
The inequality holds true for any positive quantities a and b. We will use
it in the expression of the minimum error rate. Substitution of (2.15) in
(2.16) yields:
Emin ¼
Z
z
½1  maxfPð!1jzÞ; Pð!2jzÞgpðzÞdz
¼
Z
z
minfpðzj!1ÞPð!1Þ; pðzj!2ÞPð!2Þgdz
ð6:12Þ
Together with (6.11) we have the following inequality:
Emin 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Pð!1ÞPð!2Þ
p
Z
z
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pðzj!1Þpðzj!2Þ
p
dz
ð6:13Þ
CRITERIA FOR SELECTION AND EXTRACTION
191

The inequality is called the Bhattacharyya upper bound. A more com-
pact notation of it is achieved with the so-called Bhattacharyya distance.
This performance measure is defined as:
JBHAT ¼ ln
Z
z
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pðzj!1Þpðzj!2Þ
p
dz


ð6:14Þ
With that, the Bhattacharyya upper bound simplifies to:
Emin 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Pð!1ÞPð!2Þ
p
expðJBHATÞ
ð6:15Þ
The bound can be made more tight if inequality (6.11) is replaced with
the more general inequality minfa, bg  asb1s. This last inequality
holds true for any s, a and b in the interval [0, 1]. The inequality leads
to the Chernoff distance, defined as:
JCðsÞ ¼ ln
Z
z
psðzj!1Þp1sðzj!2Þdz


with: 0  s  1
ð6:16Þ
Application of the Chernoff distance in a derivation similar to (6.12)
yields:
Emin  Pð!1ÞsPð!2Þ1s expðJCðsÞÞ
for any
s 2 ½0; 1
ð6:17Þ
The so-called Chernoff bound encompasses the Bhattacharyya upper
bound. In fact, for s ¼ 0:5 the Chernoff distance and the Bhattacharyya
distance are equal: JBHAT ¼ JC(0:5).
There also exists a lower bound based on the Bhattacharyya distance.
This bound is expressed as:
1
2 1 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  4Pð!1ÞPð!2Þ expð2JBHATÞ
p
h
i
 Emin
ð6:18Þ
A further simplification occurs when we specify the conditional
probability densities. An important application of the Chernoff and
Bhattacharyya distance is the Gaussian case. Suppose that these densities
have class-dependent expectation vectors mk and covariance matrices Ck,
192
FEATURE EXTRACTION AND SELECTION

respectively. Then, it can be shown that the Chernoff distance
transforms into:
JCðsÞ ¼ 1
2 sð1  sÞðm2  m1ÞT½ð1  sÞC1 þ sC21ðm2  m1Þ
þ 1
2 ln jð1  sÞC1 þ sC2j
jC1j1sjC2js
"
#
ð6:19Þ
It can be seen that if the covariance matrices are independent of the
classes, e.g. C1 ¼ C2, the second term vanishes, and the Chernoff and the
Bhattacharyya distances become proportional to the Mahalanobis distance
SNR given in (2.46): JBHAT ¼ SNR/8. Figure 6.3(a) shows the corresponding
Chernoff and Bhattacharyya upper bounds. In this particular case, the
relation between SNR and the minimum error rate is easily obtained using
expression (2.47). Figure 6.3(a) also shows the Bhattacharyya lower bound.
The dependency of the Chernoff bound on s is depicted in Figure 6.3(b).
If C1 ¼ C2, the Chernoff distance is symmetric in s, and the minimum
bound is located at s ¼ 0:5 (i.e. the Bhattacharyya upper bound). If the
covariance matrices are not equal, the Chernoff distance is not symmetric,
and the minimum bound is not guaranteed to be located midway. A
numerical optimization procedure can be applied to find the tightest bound.
If in the Gaussian case, the expectation vectors are equal (m1 ¼ m2),
the first term of (6.19) vanishes, and all class information is represented
by the second term. This term corresponds to class information carried
by differences in covariance matrices.
(a)
(b)
0
0.25
0.5
SNR
E
0
0.25
0.5
0
0.5
1
SNR = 10
S
E
Bhattacharyya lower bound
Chernoff bound (s = 0.5)
= Bhattacharyya upper bound
Chernoff bound (s = 0.8)
Emin
0
10
20
30
40
Figure 6.3
Error bounds and the true minimum error for the Gaussian case
(C1 ¼ C2). (a) The minimum error rate with some bounds given by the Chernoff
distance. In this example the bound with s ¼ 0:5 (Bhattacharyya upper bound) is the
most tight. The figure also shows the Bhattacharyya lower bound. (b) The Chernoff
bound with dependence on s
CRITERIA FOR SELECTION AND EXTRACTION
193

6.1.3
Other criteria
The criteria discussed above are certainly not the only ones used in
feature selection and extraction. In fact, large families of performance
measures exist; see Devijver and Kittler (1982) for an extensive over-
view. One family is the so-called probabilistic distance measures. These
measures are based on the observation that large differences between the
conditional densities p(zj!k) result in small error rates. Let us assume a
two-class problem with conditional densities p(zj!1) and p(zj!2). Then,
a probabilistic distance takes the following form:
J ¼
Z 1
1
gðpðzj!1Þ; pðzj!2ÞÞdz
ð6:20Þ
The function g( , ) must be such that J is zero when p(zj!1) ¼ p(zj!2),
8z, and non-negative otherwise. In addition, we require that J attains its
maximum whenever the two densities are completely non-overlapping.
Obviously, the Bhattacharyya distance (6.14) and the Chernoff distance
(6.16) are examples of performance measures based on a probabilistic
distance. Other examples are the Matusita measure and the divergence
measures:
JMATUSITA ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Z
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pðzj!1Þ
p

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pðzj!2Þ
p

2
dz
s
ð6:21Þ
JDIVERGENCE ¼
Z
ðpðzj!1Þ  pðzj!2ÞÞ ln pðzj!1Þ
pðzj!2Þ dz
ð6:22Þ
These measures are useful for two-class problems. For more classes, a
measure is obtained by taking the average of pairs. That is:
J ¼
X
K
k¼1
X
K
l¼0
Pð!kÞPð!lÞJk;l
ð6:23Þ
where Jk,l is the measure found between the classes !k and !l.
Another family is the one using the probabilistic dependence of the
measurement vector z on the class !k. Suppose that the conditional
density of z is not affected by !k, i.e. p(zj!k) ¼ p(z), 8z, then an observa-
tion of z does not increase our knowledge on !k. Therefore, the ability of
194
FEATURE EXTRACTION AND SELECTION

z to discriminate class !k from the rest can be expressed in a measure of
the probabilistic dependence:
Z 1
1
gðpðzj!kÞ; pðzÞÞdz
ð6:24Þ
where the function g( , ) must have likewise properties as in (6.20). In
order to incorporate all classes, a weighted sum of (6.24) is formed to get
the final performance measure. As an example, the Chernoff measure
now becomes:
JCdepðsÞ ¼ 
X
K
k¼1
Pð!kÞ
Z
psðzj!kÞp1sðzÞdz
ð6:25Þ
Other dependence measures can be derived from the probabilistic dis-
tance measures in likewise manner.
A third family is founded on information theory and involves the
posterior probabilities P(!kjz). An example is Shannon’s entropy meas-
ure. For a given z, the information of the true class associated with z is
quantified by Shannon by entropy:
HðzÞ ¼ 
X
K
k¼1
Pð!kjzÞ log2 Pð!kjzÞ
ð6:26Þ
Its expectation
JSHANNON ¼ E½HðzÞ ¼
Z
HðzÞpðzÞdz
ð6:27Þ
is a performance measure suitable for feature selection and extraction.
6.2
FEATURE SELECTION
This section introduces the problem of selecting a subset from the N-
dimensional measurement vector such that this subset is most suitable
for classification. Such a subset is called a feature set and its elements
are features. The problem is formalized as follows. Let F(N) ¼
fznjn ¼ 0, . . . , N  1g be the set with elements from the measurement
vector z. Furthermore, let Fj(D) ¼ fydjd ¼ 0, . . . , D  1g be a subset of
FEATURE SELECTION
195

F(N) consisting of D < N elements taken from z. For each element yd
there exists an element zn such that yd ¼ zn. The number of distinguish-
able subsets for a given D is:
qðDÞ ¼
N
D


¼
N!
ðN  DÞ!D!
ð6:28Þ
This quantity expresses the number of different combinations that can
be made from N elements, each combination containing D elements and
with no two combinations containing exactly the same D elements. We
will adopt an enumeration of the index j according to: j ¼ 1, . . . , q(D).
In Section 6.1 the concept of a performance measure has been intro-
duced. These performance measures evaluate the appropriateness of a
measurement vector for the classification task. Let J(Fj(D)) be a perform-
ance measure related to the subset Fj(D). The particular choice of
J(:) depends on the problem at hand. For instance, in a multi-class
problem, the interclass/intraclass distance JINTER=INTRA(:) could be useful;
see (6.10).
The goal of feature selection is to find a subset ^F(D) with dimension D
such that this subset outperforms all other subsets with dimension D:
^FðDÞ ¼ FiðDÞ
with: JðFiðDÞÞ  JðFjðDÞÞ
for all
j 2 f1; . . . ; qðDÞg
ð6:29Þ
An exhaustive search for this particular subset would solve the problem of
feature selection. However, there is one practical problem. How to accom-
plish the search process? An exhaustive search requires q(D) evaluations of
J(Fj(D)). Even in a simple classification problem, this number is enormous.
For instance, let us assume that N ¼ 20 and D ¼ 10. This gives about
2  105 different subsets. A doubling of the dimension to N ¼ 40 requires
about 109 evaluations of the criterion. Needless to say that even in problems
with moderate complexity, an exhaustive search is out of the question.
Obviously, a more efficient search strategy is needed. For this, many
options exist, but most of them are suboptimal (in the sense that they
cannot guarantee that the subset with the best performance will be
found). In the remaining part of this section, we will first consider a
search strategy called ’branch-and-bound’. This strategy is one of the
few that guarantees optimality (under some assumptions). Next, we
continue with some of the much faster, but suboptimal strategies.
196
FEATURE EXTRACTION AND SELECTION

6.2.1
Branch-and-bound
The search process is accomplished systematically by means of a tree
structure. See Figure 6.4. The tree consists of N  D þ 1 levels. Each
level is enumerated by a variable n with n varying from D up to N.
A level consists of a number of nodes. A node i at level n corresponds to
a subset Fi(n). At the highest level, n ¼ N, there is only one node
corresponding to the full set F(N). At the lowest level n ¼ D there are
q(D) nodes corresponding to the q(D) subsets among which the solution
must be found. Levels in between have a number of nodes that is less
than or equal to q(n). A node at level n is connected to one node at level
n þ 1 (except the node F(N) at level N). In addition, each node at level n
is connected to one or more nodes at level n  1 (except the nodes at
level D). In the example of Figure 6.4, N ¼ 6 and D ¼ 2.
A prerequisite of the branch-and-bound strategy is that the perform-
ance measure is a monotonically increasing function of the dimension D.
The assumption behind it is that if we remove one element from a
measurement vector, the performance can only become worse. In prac-
tice, this requirement is not always satisfied. If the training set is finite,
problems in estimating large numbers of parameters may result in an
actual performance increase when fewer measurements are used (see
Figure 6.1).
The search process takes place from the highest level (n ¼ N) by
systematically traversing all levels until finally the lowest level (n ¼ D)
is reached. Suppose that one branch of the tree has been explored up
to the lowest level, and suppose that the best performance measure
found so far at level n ¼ D is ^J. Consider a node Fi(l) (at a level
n > D) which has not been explored as yet. Then, if J(Fi(n))  ^J, it is
n = N = 5
0
1
2
3
4
n = 4
  1
2
3
4
2
 3  4
 3
 4
4
n = 3
z2z3
z1z2z3z4
z0z1z2z3z4
z0z2z3z4
z0z1z3z4
z0z1z2z4
z0z1z2z3
z3z4
z1z3z4
z1z3
z1z4
z1z2z4
z2z4
z1z2z3
z1z2
z0z3z4
z0z3
z0z4
z0z2z4 z0z2z3
z0z1z3 z0z1z2
z0z1
z0z1z4
z0z2
 2
3
4
3
4
4
3
4
4
4
n = D = 2
z2z3z4
Figure 6.4
A top-down tree structure in behalf of feature selection
FEATURE SELECTION
197

unnecessary to consider the nodes below Fi(n). The reason is that the
performance measure for these nodes can only become less than J(Fi(n))
and thus less than ^J.
The following algorithm traverses the tree structure according to a
depth first search with a backtrack mechanism. In this algorithm the
best performance measure found so far at level n ¼ D is stored in a
variable ^J. Branches whose performance measure is bounded by ^J are
skipped. Therefore, relative to exhaustive search the algorithm is much
more computationally efficient. The variable S denotes a subset. Initially
this subset is empty, and ^J is set to 0. As soon as a combination of D
elements has been found that exceeds the current performance, this
combination is stored in S and the corresponding measure in ^J.
Algorithm 6.1: Branch-and-bound search
Input: a labelled training set on which a performance measure J() is
defined.
1. Initiate: ^J ¼ 0 and S ¼ ;
2. Explore-node(F1(N));
Output: The maximum performance measure stored in ^J with the asso-
ciated subset of F1(N) stored in S.
Procedure: Explore-node(Fi(n))
1. If (J(Fi(n))  ^J) then return;
2. If (n ¼ D) then
2.1. If ( J(Fi(n)) > ^J) then
2.1.1. ^J ¼ J(Fi(n));
2.1.2. S ¼ Fi(n);
2.1.3. return;
3. For all (Fj(n  1)  Fi(n)) do Explore-node(Fj(n  1));
4. return;
The algorithm is recursive. The procedure ‘Explore-node()’ explores the
node given in its argument list. If the node is not a leaf, all its children
nodes are explored by calling the procedure recursively. The first call is
with the full set F(N) as argument.
The algorithm listed above does not specify the exact structure of the
tree. The structure follows from the specific implementation of the loop
198
FEATURE EXTRACTION AND SELECTION

in step 3. This loop also controls the order in which branches are
explored. Both aspects influence the computational efficiency of the
algorithm. In Figure 6.4, the list of indices of elements that are deleted
from a node to form the child nodes follows a specific pattern (see
exercise 2). The indices of these elements are shown in the figure. Note
that this tree is not minimal because some twigs have their leaves at a
level higher than D. Of course, pruning these useless twigs in advance is
computationally more efficient.
6.2.2
Suboptimal search
Although the branch-and-bound algorithm can save many calculations
relative to an exhaustive search (especially for large values of q(D)), it
may still require too much computational effort.
Another possible defect of branch-and-bound is the top-down search
order. It starts with the full set of measurements and successively deletes
elements. However, the assumption that the performance becomes worse
as the number of features decreases holds true only in theory; see Figure
6.1. In practice, the finite training set may give rise to an overoptimistic
view if the number of measurements is too large. Therefore, a bottom-up
search order is preferable. The tree in Figure 6.5 is an example.
Among the many suboptimal methods, we mention a few. A simple
method is sequential forward selection (SFS). The method starts at the
bottom (the root of the tree) with an empty set and proceeds its way to
the top (a leaf) without backtracking. At each level of the tree, SFS adds
one feature to the current feature set by selecting from the remaining
available measurements the element that yields a maximal increase of
the performance. A disadvantage of the SFS is that once a feature is
nl = D = 2
 z1
 z3z4
 z2z4
 z2z3
z1z4
z1z3
z1z2
z0z4
z0z3
z0z2
z0z1
z4
 z2
z3
 z0
1
2
3
4
2
3
4
3
4
4
n = 1
0
1
2
  3
4
n = 0
Figure 6.5
A bottom-up tree structure for feature selection
FEATURE SELECTION
199

included in the selected sets, it cannot be removed, even if at a higher
level in the tree, when more features are added, this feature would be less
useful.
The SFS adds one feature at a time to the current set. An improvement
is to add more than one, say l, features at a time. Suppose that at a given
stage of the selection process we have a set Fj(n) consisting of n features.
Then, the next step is to expand this set with l features taken from the
remaining
N  n
measurements.
For
that
we
have
(N  n)!=
((N  n  l)! l!) combinations which all must be checked to see which
one is most profitable. This strategy is called the generalized sequential
forward selection (GSFS(l)).
Both the SFS and the GSFS(l) lack a backtracking mechanism. Once a
feature has been added, it cannot be undone. Therefore, a further
improvement is to add l features at a time, and after that to dispose of
some of the features from the set obtained so far. Hence, starting with a
set of Fj(n) features we select the combination of l remaining measure-
ments that when added to Fj(n) yields the best performance. From this
expanded set, say Fi(n þ l), we select a subset of r features and remove
it from Fi(n þ l) to obtain a set, say Fk(n þ l  r). The subset of r features
is selected such that Fk(n þ l  r) has the best performance. This strategy
is called ‘Plus l – take away r selection’.
Example 6.2
Character classification for license plate recognition
In traffic management, automated license plate recognition is useful
for a number of tasks, e.g. speed checking and automated toll collec-
tion. The functions in a system for license plate recognition include
image acquisition, vehicle detection, license plate localization, etc.
Once the license plate has been localized in the image, it is partitioned
such that each part of the image – a bitmap – holds exactly one
character. The classes include all alphanumerical values. Therefore,
the number of classes is 36. Figure 6.6(a) provides some examples of
bitmaps obtained in this way.
The size of the bitmaps ranges from 15  6 up to 30  15. In order
to fix the number of pixels, and also to normalize the position, scale
and contrasts of the character within the bitmap, all characters are
normalized to 15  11.
Using the raw image data as measurements, the number of mea-
surements per object is 15  11 ¼ 165. A labelled training set con-
sisting of about 70 000 samples, i.e. about 2000 samples/class, is
available. Comparing the number of measurements against the num-
ber of samples/class we conclude that overfitting is likely to occur.
200
FEATURE EXTRACTION AND SELECTION

A feature selection procedure, based on the ‘plus l-take away r’
method (l ¼ 3, r ¼ 2) and the inter/intra distance (Section 6.1.1) gives
feature sets as depicted in Figure 6.6(b). Using a validation set con-
sisting of about 50 000 samples, it was established that 50 features
gives the minimal error rate. A number of features above 50 intro-
duces overfitting. The pattern of 18 selected features, as shown in
Figure 6.6(b), is one of the intermediate results that were obtained to
get the optimal set with 50 features. It indicates which part of a
bitmap is most important to recognize the character.
6.2.3
Implementation issues
PRTools offers a large range of feature selection methods. The
evaluation criteria are implemented in the function feateval, and
are basically all inter/intra cluster criteria. Additionally, a -nearest
neighbour classification error is defined as a criterion. This will give
a reliable estimate of the classification complexity of the reduced
data set, but can be very computationally intensive. For larger data
sets it is therefore recommended to use the simpler inter-intra-cluster
measures.
PRTools also offers several search strategies, i.e. the branch-and-
bound algorithm, plus-l-takeaway-r, forward selection and backward
selection. Feature selection mappings can be found using the function
featselm. The following listing is an example.
(a)
(b)
Figure 6.6
Character classification for license plate recognition. (a) Character sets
from license plates, before and after normalization. (b) Selected features. The num-
ber of features is 18 and 50 respectively
FEATURE SELECTION
201

Listing 6.1
PRTools code for performing feature selection.
% Create a labeled dataset with 8 features, of which only 2
% are useful, and apply various feature selection methods
z ¼ gendatd(200,8,3,3);
w ¼ featselm(z, ‘maha-s’, ‘forward’,2);
% Forward selection
figure; clf; scatterd(z*w);
title([‘forward: ’ num2str(þwf2g)]);
w ¼ featselm(z,‘maha-s’, ‘backward’,2);
% Backward selection
figure; clf; scatterd(z*w);
title([‘backward: ’ num2str(þwf2g)]);
w ¼ featselm(z, ‘maha-s’, ‘b&b’,2);
% B&B selection
figure; clf; scatterd(z*w);
title([‘b&b: ’ num2str(þwf2g)]);
The function gendatd creates a data set in which just the first two
measurements are informative while all other measurements only contain
noise (there the classes completely overlap). The listing shows three pos-
sible feature selection methods. All of them are able to retrieve the correct
two features. The main difference is in the required computing time:
finding two features out of eight is approached most efficiently by the
forward selection method, while backward selection is the most inefficient.
6.3
LINEAR FEATURE EXTRACTION
Another approach to reduce the dimension of the measurement vector is
to use a transformed space instead of the original measurement space.
Suppose that W(.) is a transformation that maps the measurement space
RN onto a reduced space RD, D  N. Application of the transformation
to a measurement vector yields a feature vector y 2 RD:
y ¼ WðzÞ
ð6:30Þ
Classification is based on the feature vector rather than on the measure-
ment vector; see Figure 6.7.
The advantage of feature extraction above feature selection is that no
information from any of the elements of the measurement vector needs
to be wasted. Furthermore, in some situations feature extraction is easier
than feature selection. A disadvantage of feature extraction is that it
requires the determination of some suitable transformation W(). If the
202
FEATURE EXTRACTION AND SELECTION

transform chosen is too complex, the ability to generalize from a small
data set will be poor. On the other hand, if the transform chosen is too
simple, it may constrain the decision boundaries to a form which is
inappropriate to discriminate between classes. Another disadvantage is
that all measurements will be used, even if some of them are useless. This
might be unnecessarily expensive.
This section discusses the design of linear feature extractors. The
transformation W() is restricted to the class of linear operations. Such
operations can be written as a matrix–vector product:
y ¼ Wz
ð6:31Þ
where W is a D  N matrix. The reason for the restriction is threefold.
First, a linear feature extraction is computationally efficient. Second, in
many classification problems – though not all – linear features are
appropriate. Third, a restriction to linear operations facilitates the math-
ematical handling of the problem.
An illustration of the computational efficiency of linear feature extraction
is the Gaussian case. If covariance matrices are unequal, the number of
calculations is on the order of KN2; see equation (2.20). Classification based
on linear features requires about DN þ KD2 calculations. If D is very small
compared with N, the extraction saves a large number of calculations.
The example of Gaussian densities is also well suited to illustrate the
appropriateness of linear features. Clearly, if the covariance matrices
are equal, then (2.25) shows that linear features are optimal. On the
other hand, if the expectation vectors are equal and the discriminatory
information is in the differences between the covariance matrices,
linear feature extraction may still be appropriate. This is shown in
the example of Figure 2.10(b) where the covariance matrices are
eccentric, differing only in their orientations. However, in the example
shown in Figure 2.10(a) (concentric circles) linear features seem to be
inappropriate. In practical situations, the covariance matrices will
often differ in both shape and orientations. Linear feature extraction
is likely to lead to a reduction of the dimension, but this reduction may
be less than what is feasible with nonlinear feature extraction.
feature extraction
W( )
measurement
vector
feature
vector
assigned
class
z ∈
y ∈
pattern classification
ω ( )
ˆ
ˆω ∈Ω
N
D
Figure 6.7
Feature extraction
LINEAR FEATURE EXTRACTION
203

Linear feature extraction may also improve the ability to generalize. If, in
the Gaussian case with unequal covariance matrices, the number of sam-
ples in the training set is in the same order as that of the number
of parameters, KN2, overfitting is likely to occur. But linear feature extrac-
tion – provided that D << N – helps to improve the generalization ability.
We assume the availability of a training set and a suitable performance
measure J(). The design of a feature extraction method boils down to
finding the matrix W that – for the given training set – optimizes the
performance measure.
The performance measure of a feature vector y ¼ Wz is denoted by
J(y) or J(Wz). With this notation, the optimal feature extraction is:
W ¼ argmax
W
fJðWzÞg
ð6:32Þ
Under the condition that J(Wz) is continuously differentiable in W, the
solution of (6.32) must satisfy:
qJðWzÞ
qW
¼ 0
ð6:33Þ
Finding a solution of either (6.32) or (6.33) gives us the optimal linear
feature extraction. The search can be accomplished numerically using
the training set. Alternatively, the search can also be done analytically
assuming parameterized conditional densities. Substitution of estimated
parameters (using the training set) gives the matrix W.
In the remaining part of this section, the last approach is worked out
for two particular cases: feature extraction for two-class problems with
Gaussian densities and feature extraction for multi-class problems based
on the inter/intra distance measure. The former case will be based on the
Bhattacharyya distance.
6.3.1
Feature extraction based on the Bhattacharyya distance
with Gaussian distributions
In the two-class case with Gaussian conditional densities a suitable
performance measure is the Bhattacharyya distance. In equation (6.19)
JBHAT implicitly gives the Bhattacharyya distance as a function of the
parameters of the Gaussian densities of the measurement vector z. These
parameters are the conditional expectations mk and covariance matrices
Ck. Substitution of y ¼ Wz gives the expectation vectors and covariance
204
FEATURE EXTRACTION AND SELECTION

matrices of the feature vector, i.e. Wmk and WCkWT, respectively. For
the sake of brevity, let m be the difference between expectations of z:
m ¼ m1  m2
ð6:34Þ
Then, substitution of m, Wmk and WCkWT in (6.19) gives the Bhatta-
charyya distance of the feature vector:
JBHATðWzÞ ¼ 1
4 ðWmÞT WC1WT þ WC2WT

	1Wm
þ 1
2 ln
jWC1WT þ WC2WTj
2D
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
jWC1WTjjWC2WTj
q
2
64
3
75
ð6:35Þ
The first term corresponds to the discriminatory information of the
expectation vectors; the second term to the discriminatory information
of covariance matrices.
Equation (6.35) is in such a form that an analytic solution of (6.32) is
not feasible. However, if one of the two terms in (6.35) is dominant, a
solution close to the optimal one is attainable. We consider the two
extreme situations first.
Equal covariance matrices
In the case where the conditional covariance matrices are equal, i.e.
C ¼ C1 ¼ C2, we have already seen that classification based on the
Mahalanobis distance (see (2.41)) is optimal. In fact, this classification
uses a 1  N dimensional feature extraction matrix given by:
W ¼ mTC1
ð6:36Þ
To prove that this equation also maximizes the Bhattacharyya distance is
left as an exercise for the reader.
Equal expectation vectors
If the expectation vectors are equal, m ¼ 0, the first term in (6.35)
vanishes and the second term simplifies to:
JBHATðWzÞ ¼ 1
2 ln
jWC1WT þ WC2WTj
2D
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
jWC1WTjjWC2WTj
q
2
64
3
75
ð6:37Þ
LINEAR FEATURE EXTRACTION
205

The D  N matrix W that maximizes the Bhattacharyya distance can be
derived as follows.
The first step is to apply a whitening operation (Appendix C.3.1) on z
with respect to class !1. This is accomplished by the linear operation
1/2VTz. The matrices V and  follow from factorization of the covari-
ance matrix: C1 ¼ VVT. V is an orthogonal matrix consisting of the
eigenvectors of C1.  is the diagonal matrix containing the correspond-
ing eigenvalues. The process is illustrated in Figure 6.8. The figure shows
–1
–0.5
0
0.5
1
–1
–0.5
0
0.5
1
(a)
scatter diagram
–1
0
1
–1
0
1
(c)
after decorrelation of the second class
–1
–0.5
0
0.5
1
–1
–0.8
–0.6
–0.4
–0.2
0
0.2
0.4
0.6
0.8
1
classification with 1 linear feature
–1 0
1
–1
0
1
(b)
(d)
after whitening of the first class
Figure 6.8
Linear feature extraction with equal expectation vectors. (a) Covariance
matrices with decision function. (b) Whitening of !1 samples. (c) Decorrelation of !2
samples. (d) Decision function based on one linear feature
206
FEATURE EXTRACTION AND SELECTION

a two-dimensional measurement space with samples from two classes.
The covariance matrices of both classes are depicted as ellipses.
Figure 6.8(b) shows the result of the operation 1/2VT. The operation
VT corresponds to a rotation of the coordinate system such that the
ellipse of class !1 lines up with the axes. The operation 1/2 corres-
ponds to a scaling of the axes such that the ellipse of !1 degenerates into
a circle. The figure also shows the resulting covariance matrix belonging
to class !2.
The result of the operation 1/2VT on z is that the covariance matrix
associated with !1 becomes I and the covariance matrix associated with
!2 becomes 1/2VTC2V1/2. The Bhattacharyya distance in the trans-
formed domain is:
JBHATð1
2VTzÞ ¼ 1
2 ln
jI þ 1
2VTC2V1
2j
2N
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
j1
2VTC2V1
2j
q
2
64
3
75
ð6:38Þ
The second step consists of decorrelation with respect to !2. Suppose
that U and  are matrices containing the eigenvectors and eigenvalues of
the
covariance
matrix
1/2VTC2V1/2.
Then,
the
operation
UT1/2VT decorrelates the covariance matrix with respect to class !2.
The covariance matrices belonging to the classes !1 and !2 transform
into UTIU ¼ I and , respectively. Figure 6.8(c) illustrates the decorrel-
ation. Note that the covariance matrix of !1 (being white) is not affected
by the orthonormal operation UT.
The matrix  is a diagonal matrix. The diagonal elements are denoted
i ¼ i, i. In the transformed domain UT1/2VTz, the Bhattacharyya
distance is:
JBHATðUT1
2VTzÞ ¼ 1
2 ln
jI þ j
2N
ﬃﬃﬃﬃﬃﬃ
jj
p
"
#
¼ 1
2
X
N1
i¼0
ln 1
2
ﬃﬃﬃﬃi
p
þ 1ﬃﬃﬃﬃi
p


ð6:39Þ
The expression shows that in the transformed domain the contribution
to the Bhattacharyya distance of any element is independent. The con-
tribution of the i-th element is:
1
2 ln 1
2
ﬃﬃﬃﬃi
p
þ 1ﬃﬃﬃﬃi
p


ð6:40Þ
LINEAR FEATURE EXTRACTION
207

Therefore, if in the transformed domain D elements have to be selected,
the selection with maximum Bhattacharyya distance is found as the set
with the largest contributions. If the elements are sorted according to:
ﬃﬃﬃﬃﬃ
0
p
þ
1ﬃﬃﬃﬃﬃ
0
p

ﬃﬃﬃﬃﬃ
1
p
þ
1ﬃﬃﬃﬃﬃ
1
p
    
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
N1
p
þ
1ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
N1
p
ð6:41Þ
then the first D elements are the ones with optimal Bhattacharyya
distance. Let UD be an N  D submatrix of U containing the D corres-
ponding eigenvectors of 1/2VTC2V1/2. The optimal linear feature
extractor is:
W ¼ UT
D1
2VT
ð6:42Þ
and the corresponding Bhattacharyya distance is:
JBHATðWzÞ ¼ 1
2
X
D1
i¼0
ln 1
2
ﬃﬃﬃﬃi
p
þ 1ﬃﬃﬃﬃi
p


ð6:43Þ
Figure 6.8(d) shows the decision function following from linear feature
extraction backprojected in the two-dimensional measurement space.
Here, the linear feature extraction reduces the measurement space to a
one-dimensional feature space. Application of Bayes classification in this
space is equivalent to a decision function in the measurement space defined
by two linear, parallel decision boundaries. In fact, the feature extraction is
a projection onto a line orthogonal to these decision boundaries.
The general Gaussian case
If both the expectation vectors and the covariance matrices depend on
the classes, an analytic solution of the optimal linear extraction problem
is not easy. A suboptimal method, i.e. a method that hopefully yields
a reasonable solution without the guarantee that the optimal solution
will be found, is the one that seeks features in the subspace defined by
the differences in covariance matrices. For that, we use the same simul-
taneous decorrelation technique as in the previous section. The Bhatta-
charyya distance in the transformed domain is:
JBHATðUT1
2VTzÞ ¼ 1
4
X
N1
i¼0
d2
i
1 þ i
þ 1
2
X
N1
i¼0
ln 1
2
ﬃﬃﬃﬃi
p
þ 1ﬃﬃﬃﬃi
p


ð6:44Þ
208
FEATURE EXTRACTION AND SELECTION

where di are the elements of the transformed difference of expectation,
i.e. d ¼ UT1/2VTm.
Equation (6.44) shows that in the transformed space the optimal
features are the ones with the largest contributions, i.e. with the largest
1
4
d2
i
1 þ i
þ 1
2 ln 1
2
ﬃﬃﬃﬃi
p
þ 1ﬃﬃﬃﬃi
p


ð6:45Þ
The extraction method is useful especially when most class information
is contained in the covariance matrices. If this is not the case, then the
results must be considered cautiously. Features that are appropriate for
differences in covariance matrices are not necessarily also appropriate
for differences in expectation vectors.
Listing 6.2
PRTools code for calculating a Bhattacharryya distance feature extractor.
z ¼ gendatl([200 200],0.2);
% Generate a dataset
J ¼ bhatm(z,0);
% Calculate criterion values
figure; clf; plot(J, ‘r.-’);
% and plot them
w ¼ bhatm(z,1);
% Extract one feature
figure; clf; scatterd(z);
% Plot original data
figure; clf; scatterd(z*w);
% Plot mapped data
6.3.2
Feature extraction based on inter/intra class distance
The inter/intra class distance, as discussed in Section 6.1.1, is another
performance measure that may yield suitable feature extractors. The
starting point is the performance measure given in the space defined by
y ¼ 1/2VTz. Here,  is a diagonal matrix containing the eigenvalues of
Sw, and V a unitary matrix containing the corresponding eigenvectors. In
the transformed domain the performance measure is expressed as (6.10):
JINTER=INTRA ¼ traceð1
2VTSbV1
2Þ
ð6:46Þ
A further simplification occurs when a second unitary transform is
applied. The purpose of this transform is to decorrelate the between-
scatter matrix. Suppose that  is a diagonal matrix whose diagonal
elements i ¼ i, i are the eigenvalues of the transformed between-scatter
LINEAR FEATURE EXTRACTION
209

matrix 1/2VTSbV1/2. Let U be a unitary matrix containing the
eigenvectors corresponding to . Then, in the transformed domain
defined by:
y ¼ UT1
2VTz
ð6:47Þ
the performance measure becomes:
JINTER=INTRA ¼ traceðÞ ¼
X
N1
i¼0
i
ð6:48Þ
The operation UT corresponds to a rotation of the coordinate system
such that the between-scatter matrix lines up with the axes. Figure 6.9
illustrates this.
The merit of (6.48) is that the contributions of the elements add up
independently. Therefore, in the space defined by y ¼ UT1/2VTz it is
easy to select the best combination of D elements. It suffices to determine
the D elements from y whose eigenvalues i are largest. Suppose that the
eigenvalues are sorted according to i  iþ1, and that the eigenvectors
–1 0 1
–1
0
1
(a)
after simultaneous decorrelation
sb
sw
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
(b)
projections→
classification with 1 linear feature
Figure 6.9
Feature extraction based on the interclass/intraclass distance (see
Figure 6.2). (a) The within and between scatters after simultaneous decorrelation.
(b) Linear feature extraction
210
FEATURE EXTRACTION AND SELECTION

corresponding to the D largest eigenvalues are collected in UD, being an
N  D submatrix of U. Then, the linear feature extraction becomes:
W ¼ UT
D1=2VT
ð6:49Þ
The feature space defined by y ¼ Wz can be thought of as a linear
subspace of the measurement space. This subspace is spanned by the D
row vectors in W. The performance measure associated with this feature
space is:
JINTER=INTRAðWzÞ ¼
X
D1
i¼0
i
ð6:50Þ
Example 6.3
Feature extraction based on inter/intra distance
Figure 6.9(a) shows the within-scattering and between-scattering of
Example 6.1 after simultaneous decorrelation. The within-scattering
has been whitened. After that, the between-scattering is rotated such
that its ellipse is aligned with the axes. In this figure, it is easy to see
which axis is the most important. The eigenvalues of the between-
scatter matrix are 0 ¼ 56:3 and 1 ¼ 2:8, respectively. Hence, omit-
ting the second feature does not deteriorate the performance much.
The feature extraction itself can be regarded as an orthogonal
projection of samples on this subspace. Therefore, decision bound-
aries defined in the feature space correspond to hyperplanes orthog-
onal to the linear subspace, i.e. planes satisfying equations of the type
Wz ¼ constant.
A characteristic of linear feature extraction based on JINTER/INTRA is that
the dimension of the feature space found will not exceed K  1, where K
is the number of classes. This follows from expression (6.7), which
shows that Sb is the sum of K outer products of vectors (of which one
vector linearly depends on the others). Therefore, the rank of Sb cannot
exceed K  1. Consequently, the number of nonzero eigenvalues of Sb
cannot exceed K  1 either. Another way to put this into words is that
the K conditional means mk span a (K  1) dimensional linear subspace
in RN. Since the basic assumption of the inter/intra distance is that
within-scattering does not convey any class information, any feature
extractor based on that distance can only find class information within
that subspace.
LINEAR FEATURE EXTRACTION
211

Example 6.4
License plate recognition (continued)
In the license plate application, discussed in Example 6.2, the
measurement space (consisting of 15  11 bitmaps) is too large with
respect to the size of the training set. Linear feature extraction based
on maximization of the inter/intra distance reduces this space to at
most Dmax ¼ K  1 ¼ 35 features. Figure 6.10(a) shows how the
inter/intra distance depends on D. It can be seen that at about
D ¼ 24 the distance has almost reached its maximum. Therefore, a
reduction to 24 features is possible without losing much information.
Figure 6.10(b) is a graphical representation of the transformation
matrix W. The matrix is 24  165. Each row of the matrix serves as a
vector on which the measurement vector is projected. Therefore, each
row can be depicted as a 15  11 image. The figure is obtained by
means of MATLAB code that is similar to Listing 6.3.
Listing 6.3
PRTools code for creating a linear feature extractor based on maximiza-
tion of the inter/intra distance. The function for calculating the mapping
is fisherm. The result is an affine mapping, i.e. a mapping of the
type Wz þ b. The additive term b shifts the overall mean of the features
to the origin. In this example, the measurement vectors come directly
from bitmaps. Therefore, the mapping can be visualized by images. The
listing also shows how fisherm can be used to get a cumulative plot of
JINTER/INTRA, as depicted in Figure 6.10(a). The precise call to fisherm
is discussed in more detail in Exercise 5.
(b)
0
50
100
150
200
250
0
10
20
30
40
50
D
0
10
20
30
40
50
JINTER/INTRA
γD
(a)
Figure 6.10
Feature extraction in the license plate application. (a) The inter/intra
distance as a function of D. (b) First 24 eigenvectors in W depicted as 15  11 pixel
images
212
FEATURE EXTRACTION AND SELECTION

load license_plates.mat
% Load dataset
figure; clf; show(z);
% Display it
J ¼ fisherm(z,0);
% Calculate criterion values
figure; clf; plot(J, ‘r.-’);
% and plot them
w ¼ fisherm(z,24,0.9);
% Calculate the feature extractor
figure; clf; show(w);
% Show the mappings as images
The two-class case, Fisher’s linear discrimant
Sb ¼ 1
NS
N1ð^m1  ^mÞð^m1  ^mÞT þ N2ð^m2  ^mÞð^m2  ^mÞT


¼ ð^m1  ^m2Þð^m1  ^m2ÞT
ð6:51Þ
where  is a constant that depends on N1 and N2. In the transformed space,
Sb becomes 1/2VT(^m1  ^m2)(^m1  ^m2)TV1/2. This matrix has only one
eigenvector with a nonzero eigenvalue 0 ¼ (^m1  ^m2)TS1
b
(^m1  ^m2).
The corresponding eigenvector is u ¼ 1/2VT(^m1  ^m2). With that, the
feature extractor evolves into (see (6.49)):
W ¼ uT1
2VT
¼
1
2VTð^m1  ^m2Þ

T
1
2VT
¼ ð^m1  ^m2ÞTS1
w
ð6:52Þ
This solution – known as Fisher’s linear discriminant (Fisher, 1936) – is
similar to the Bayes classifier for Gaussian random vectors with class-
independent covariance matrices; i.e. the Mahalanobis distance clas-
sifier. The difference with expression (2.41) is that the true covariance
matrix and the expectation vectors have been replaced with estimates
from the training set. The multi-class solution (6.49) can be regarded as
a generalization of Fisher’s linear discriminant.
6.4
REFERENCES
Bhattacharyya, A., On a measure of divergence between two statistical populations
defined by their probability distributions. Bulletin of the Calcutta Mathematical
Society, 35, 99–110, 1943.
Chernoff, H., A measure of asymptotic efficiency for tests of a hypothesis based on the
sum of observations. Annals of Mathematical Statistics, 23, 493–507, 1952.
6.4
REFERENCES
213

Devijver, P.A. and Kittler, J., Pattern Recognition, a Statistical Approach. Prentice-Hall,
London, UK, 1982.
Fisher, R.A., The use of multiple measurements in taxonomic problems. Annals of
Eugenics, 7, 179–88, 1936.
6.5
EXERCISES
1. Prove equation (6.4). (	)
Hint: use zk,n  zl,m ¼ (zk,n  ^mk) þ (^mk  ^m) þ (^m  ^ml) þ (^ml  zl,m).
2. Develop an algorithm that creates a tree structure like in Figure 6.4. Can you adapt
that algorithm such that the tree becomes minimal (thus, without the superfluous
twigs)? (0)
3. Under what circumstances would it be advisable to use forward selection, or plus-
l-takeaway-r selection with l > r? And backward selection, or plus-l-takeaway-r selec-
tion with l < r? (0)
4. Prove that W ¼ mTC1 is the feature extractor that maximizes the Bhattacharyyaa
distance in the two-class Gaussian case with equal covariance matrices. (		)
5. In Listing 6.3, fisherm is called with 0:9 as its third argument. Why do you think this
is used? Try the same routine, but leave out the third argument (i.e. use
w ¼ fisherm(z, 24)). Can you explain what you see now? (	)
6. Find an alternative method of preventing the singularities you saw in Exercise 6.5. Will
the results be the same as those found using the original Listing 6.3? (		)
7. What is the danger of optimizing the parameters of the feature extraction or selection
stage, such as the number of features to retain, on the training set? How could you
circumvent this? (0)
214
FEATURE EXTRACTION AND SELECTION

7
Unsupervised Learning
In the previous chapter we discussed methods for reducing the dimen-
sion of the measurement space in order to decrease the cost of classifica-
tion and to improve the ability to generalize. In these procedures it was
assumed that for all training objects, class labels were available. In many
practical applications, however, the training objects are not labelled, or
only a small fraction of them are labelled. In these cases it can be worth
while to let the data speak for itself. The structure in the data will have to
be discovered without the help of additional labels.
An example is colour-based pixel classification. In video-based surveil-
lance and safety applications, for instance, one of the tasks is to track the
foreground pixels. Foreground pixels are pixels belonging to the objects of
interest, e.g. cars on a parking place. The RGB representation of a pixel can
be used to decide whether a pixel belongs to the foreground or not. How-
ever, the colours of neither the foreground nor the background are known
in advance. Unsupervised training methods can help to decide which pixels
of the image belong to the background, and which to the foreground.
Another example is an insurance company which might want to know
if typical groups of customers exist, such that it can offer suitable
insurance packages to each of these groups. The information provided
by an insurance expert may introduce a significant bias. Unsupervised
methods can then help to discover additional structures in the data.
In unsupervised methods, we wish to transform and reduce the data
such that a specific characteristic in the data is highlighted. In this
Classification, Parameter Estimation and State Estimation: An Engineering Approach using MATLAB
F. van der Heijden, R.P.W. Duin, D. de Ridder and D.M.J. Tax
 2004 John Wiley & Sons, Ltd
ISBN: 0-470-09013-8

chapter we will discuss two main characteristics which can be explored:
the subspace structure of data and its clustering characteristics. The first
tries to summarize the objects using a smaller number of features than
the original number of measurements; the second tries to summarize the
data set using a smaller number of objects than the original number.
Subspace structure is often interesting for visualization purposes. The
human visual system is highly capable of finding and interpreting struc-
ture in 2D and 3D graphical representations of data. When higher
dimensional data is available, a transformation to 2D or 3D might
facilitate its interpretation by humans. Clustering serves a similar pur-
pose, interpretation, but also data reduction. When very large amounts
of data are available, it is often more efficient to work with cluster
representatives instead of the whole data set. In Section 7.1 we will treat
feature reduction, in Section 7.2 we discuss clustering.
7.1
FEATURE REDUCTION
The most popular unsupervised feature reduction method is principal
component analysis (Jolliffe, 1986). This will be discussed in Section
7.1.1. One of the drawbacks of this method is that it is a linear method,
so nonlinear structures in the data cannot be modelled. In Section 7.1.2
multi-dimensional scaling is introduced, which is a nonlinear feature
reduction method.
7.1.1
Principal component analysis
The purpose of principal component analysis (PCA) is to transform a
high dimensional measurement vector z to a much lower dimensional
feature vector y by means of an operation:
y ¼ WDðz  zÞ
ð7:1Þ
such that z can be reconstructed accurately from y.
z is the expectation of the random vector z. It is constant for all realiza-
tions of z. Without loss of generality, we can assume that z ¼ 0 because we
can always introduce a new measurement vector, ~z ¼ z  z, and apply the
analysis to this vector. Hence, under that assumption, we have:
y ¼ WDz
ð7:2Þ
216
UNSUPERVISED LEARNING

The D  N matrix WD transforms the N-dimensional measurement
space to a D-dimensional feature space. Ideally, the transform is such
that y is a good representation of z despite of the lower dimension of y.
This objective is strived for by selecting WD such that an (unbiased)
linear MMSE estimate1 ^zlMMSE for z based on y yields a minimum mean
square error (see Section 3.1.5):
WD ¼ arg min
W
E
^zlMMSEðyÞ  z
k
k2
h
i
n
o
with
y ¼ Wz
ð7:3Þ
It is easy to see that this objective function does not provide a unique
solution for WD. If a minimum is reached for some WD, then any matrix
AWD is another solution with the same minimum (provided that A is
invertible) as the transformation A will be inverted by the linear MMSE
procedure. For uniqueness, we add two requirements. First, we require that
the information carried in the individual elements of y add up individually.
With that we mean that if y is the optimal D dimensional representation of
z, then the optimal D  1 dimensional representation is obtained from y,
simply by deleting its least informative element. Usually, the elements of y
are sorted in decreasing order of importance, so that the least informative
element is always the last element. With this convention, the matrix WD1
is obtained from WD simply by deleting the last row of WD.
The requirement leads to the conclusion that the elements of y must be
uncorrelated. If not, then the least informative element would still carry
predictive information about the other elements of y which conflicts
with our requirement. Hence, the covariance matrix Cy of y must be a
diagonal matrix, say . If Cz is the covariance matrix of z, then:
Cy ¼ WDCzWT
D ¼ D
ð7:4Þ
For D ¼ N it follows that CzWT
N ¼ WT
NN because WN is an invertible
matrix (in fact, an orthogonal matrix) and WT
NWN must be a diagonal
matrix (see Appendix B.5 and C.3.2). As N is a diagonal matrix, the
columns of WT
N must be eigenvectors of Cz. The diagonal elements of N
are the corresponding eigenvalues.
The solution is still not unique because each element of y can be scaled
individually without changing the minimum. Therefore, the second
requirement is that each column of WT
N has unit length. Since the
1 Since z and y are zero mean, the unbiased linear MMSE estimator coincides with the linear
MMSE estimator.
FEATURE REDUCTION
217

eigenvectors are orthogonal, this requirement is fulfilled by WNWT
N ¼ I
with I the N  N unit matrix. With that, WN establishes a rotation on z.
The rows of the matrix WN, i.e. the eigenvectors, must be sorted such
that the eigenvalues form a non-ascending sequence. For arbitrary D, the
matrix WD is constructed from WN by deleting the last N  D rows.
The interpretation of this is as follows (see Figure 7.1). The operator
WN performs a rotation on z such that its orthonormal basis aligns with
the principal axes of the ellipsoid associated with the covariance matrix
of z. The coefficients of this new representation of z are called the
principal components. The axes of the ellipsoid point in the principal
directions. The MMSE approximation of z using only D coefficients
is obtained by nullifying the principal components with least variances.
Hence, if the principal components are ordered according to their
variances, the elements of y are formed by the first D principal compon-
ents. The linear MMSE estimate is:
^zlMMSEðyÞ ¼ WT
Dy ¼ WT
DWDz:
PCA can be used as a first step to reduce the dimension of the measure-
ment space. In practice, the covariance matrix is often replaced by the
sample covariance estimated from a training set. See Section 5.2.3.
Unfortunately, PCA can be counter-productive for classification and
estimation problems. The PCA criterion selects a subspace of the feature
space, such that the variance of z is conserved as much as possible.
However, this is done regardless of the classes. A subspace with large
variance is not necessarily one in which classes are well separated.
z0
z1
y0
y1
z
: reconstructed from y0
: original vector
√λ 1
√λ 0
Figure 7.1
Principal component analysis
218
UNSUPERVISED LEARNING

A second drawback of PCA is that the results are not invariant to the
particular choice of the physical units of the measurements. Each elem-
ent of z is individually scaled according to its unit in which it is
expressed. Changing a unit from, for instance, m (meter) to mm (micro-
meter) may result in dramatic changes of the principal directions.
Usually this phenomenon is circumvented by scaling the vector z such
that the numerical values of the elements all have unit variance.
Example 7.1
Image compression
Since PCA aims at the reduction of a measurement vector in such a way
that it can be reconstructed accurately, the technique is suitable for
image compression. Figure 7.2 shows the original image. The image
plane is partitioned into 32  32 regions each having 8  8 pixels.
original image
reconstruction
0 eigenvector
1st eigenvector
2nd eigenvector
3rd eigenvector
4th eigenvector
5th eigenvector
6th eigenvector
7th eigenvector
0
20
40
60
0.8
0.85
0.9
0.95
1
D
fraction of cumulative eigenvalues
Figure 7.2
Application of PCA to image compression
FEATURE REDUCTION
219

The grey levels of the pixels are stacked in 64 dimensional vectors zn
from which the sample mean and the sample covariance matrix have
been estimated. The figure also shows the fraction of the cumulative
eigenvalues, i.e. PD1
i¼0 i

trace() where i is the i-th diagonal element
of . The first eight eigenvectors are depicted as 8  8 images. The
reconstruction based on these eight eigenvectors is also shown. The
compression is about 96%. PRTools code for this PCA compression
algorithm is given in Listing 7.1.
Listing 7.1
PRTools code for finding a set of PCA basis vectors for image compres-
sion and producing output similar to Figure 7.2.
im ¼ double(imread(‘car.tif’));
% Load image
figure; clf; imshow(im, [0 255]);
% Display image
x ¼ im2col(im,[8 8],‘distinct’);
% Extract 8x8 windows
z ¼ dataset(x’);
% Create dataset
z.featsize ¼ [8 8];
% Indicate window size
% Plot fraction of cumulative eigenvalues
v ¼ pca(z,0); figure; clf; plot(v);
% Find 8D PCA mapping and show basis vectors
w ¼ pca(z,8); figure; clf; show(w)
% Reconstruct image and display it
z_hat ¼ z*w*w’;
im_hat ¼ col2im (+z_hat’,[8 8], size(im),‘distinct’);
figure; clf; imshow (im_hat, [0 255]);
The original image blocks are converted into vectors, a data set is created
and a PCA base is found. This base is then shown and used to recon-
struct the image. Note that this listing also uses the MATLAB Image
Processing toolbox, specifically the functions im2col and co12im.
The function pca is used for the actual analysis.
7.1.2
Multi-dimensional scaling
Principal component analysis is limited to finding linear combinations of
features to map the original data to. This is sufficient for many applica-
tions, such as discarding low-variance directions in which only noise is
suspected to be present. However, if the goal of a mapping is to inspect data
in a two or three dimensional projection, PCA might discard too much
220
UNSUPERVISED LEARNING

information. For example, it can project two distinct groups in the data on
top of each other; or it might not show whether data is distributed non-
linearly in the original high dimensional measurement space.
To retain such information, a nonlinear mapping method is needed.
As there are many possible criteria to fit a mapping, there are many
different methods available. Here we will discuss just one: multi-
dimensional scaling (MDS) (Kruskal and Wish, 1977) or Sammon map-
ping (Sammon Jr, 1969). The self-organizing map, discussed in Section
7.2.5, can be considered as another nonlinear projection method.
MDS is based on the idea that the projection should preserve the
distances between objects as well as possible. Given a data set containing
N dimensional measurement vectors (or feature vectors) zi i ¼ 1, . . . ,NS,
we try to find new D dimensional vectors yi i ¼ 1, . . . NS according to this
criterion. Usually, D  N; if the goal is to visualize the data, we choose
D ¼ 2 or D ¼ 3. If ij denotes the known distance between objects zi and
zj, and dij denotes the distance between projected objects yi and yj, then
distances can be preserved as well as possible by placing the yi such that the
stress measure
ES ¼
1
P
NS
i¼1
P
NS
j¼iþ1
2
ij
X
NS
i¼1
X
NS
j¼iþ1
ij  dij

2
ð7:5Þ
is minimized. To do so, we take the derivative of (7.5) with respect to
the objects y. It is not hard to derive that, when Euclidean distances are
used, then
qES
qyi
¼ 
2
P
NS
j¼1
P
NS
k¼jþ1
2
jk
X
NS
j¼1
j6¼i
ij  dij
dij
ðyi  yjÞ
ð7:6Þ
There is no closed-form solution setting this derivative to zero, but a
gradient descent algorithm can be applied to minimize (7.5). The MDS
algorithm then becomes:
Algorithm 7.1: Multi-dimensional scaling
1. Initialization: Randomly choose an initial configuration of the
projected objects y(0). Alternatively, initialize y(0) by projecting the
original data set on its first D principal components. Set t ¼ 0.
FEATURE REDUCTION
221

2. Gradient descent
. For each object y(t)
i , calculate the gradient according to (7.6).
. Update: y(tþ1)
i
¼ y(t)
i  qES=qy(t)
i , where  is a learning rate.
. As long as ES significantly decreases, set t ¼ t þ 1 and go to step 2.
Figures 7.3(a) to (c) show examples of two-dimensional MDS mapping.
The data set, given in Table 7.1, consists of the geodesic distances of 13
world cities. These distances can only be fully brought in accordance
with the true three-dimensional geographical positions of the cities if the
spherical surface of the earth is accounted for. Nevertheless, MDS
has found two-dimensional mappings that resemble the usual Mercator
projection of the earth surface on the tangent plane at the North Pole.
Since distances are invariant to translation, rotation and mirroring,
MDS can result in arbitrarily shifted, rotated and mirrored mappings.
–8000 –4000
0
4000
8000
–8000
–4000
0
4000
8000
Bangkok
Beijing
Cairo
Capetown
Honolulu
London
Los Angeles
Melbourne
Moscow
New York
Rio
Santiago
Tokyo
q = – 2
–8000 –4000
0
4000
8000
–8000
–4000
0
4000
8000
Bangkok
Beijing
Cairo
Capetown
Honolulu
London
Los Angeles
Melbourne
Moscow
New York
Rio
Santiago
Tokyo
q = 0
–8000 –4000
0
4000
8000
–8000
–4000
0
4000
8000
Bangkok
Beijing
Cairo
Capetown
Honolulu
London
Los Angeles
Melbourne
Moscow
New York
Rio
Santiago
Tokyo
q = 2
Tokyo
Beijing
Bangkok
Melbourne
Honolulu
D = 3; q = 0
Los Angeles
Capetown
New York
Santiago
Rio
Moscow
Moscow
Cairo
Cairo
London
London
London
Moscow
Cairo
Figure 7.3
MDS applied to a matrix of geodesic distances of world cities
222
UNSUPERVISED LEARNING

The mapped objects in Figures 7.3 have been rotated such that New
York and Beijing lie on a horizontal line with New York on the left.
Furthermore, the vertical axis is mirrored such that London is situated
below the line New York–Beijing.
The reason for the preferred projection on the tangent plane near the
North Pole is that most cities in the list are situated in the Northern
hemisphere. The exceptions are Santiago, Rio, Capetown and Melbourne.
The distances for just these cities are least preserved. For instance, the true
geodesic distance between Capetown and Melbourne is about 10 000 (km),
but they are mapped opposite to each other with a distance of about
18 000 (km).
For specific applications, it might be fruitful to focus more on local
structure or on global structure. A way of doing so is by using the more
general stress measure, where an additional free parameter q is introduced:
Eq
S ¼
1
P
NS
i¼1
P
NS
j¼iþ1
ðqþ2Þ
ij
X
NS
i¼1
X
NS
j¼iþ1
q
ijðij  dijÞ2
ð7:7Þ
For q ¼ 0, (7.7) is equal to (7.5). However, as q is decreased, the (ij  dij)2
term remains constant but the q
ij term will weigh small distances heavier
than large ones. In other words, local distance preservation is emphasized,
whereas global distance preservation is less important. This is demon-
strated in Figure 7.3(a) to (c), where q ¼ 2, q ¼ 0 and q ¼ þ2 have been
used, respectively. Conversely, if q is increased, the resulting stress measure
will emphasize preserving global distances over local ones. When the stress
Table 7.1
Distance matrix of 13 cities in (km)
0 3290 7280 10100 10600
9540 13300
7350
7060 13900 16100 17700
4640 Bangkok
0 7460 12900
8150
8090 10100
9190
5790 11000 17300 19000
2130 Beijing
0
7390 14000
3380 12100 14000
2810
8960
9950 12800
9500 Cairo
0 18500
9670 16100 10300 10100 12600
6080
7970 14800 Capetown
0 11600
4120
8880 11200
8260 13300 11000
6220 Honolulu
0
8760 16900
2450
5550
9290 11700
9560 London
0 12700
9740
3930 10100
9010
8830 Los Angeles
0 14400 16600 13200 11200
8200 Melbourne
0
7510 11500 14100
7470 Moscow
0
7770
8260 10800 New York
0
2730 18600 Rio
0 17200 Santiago
0 Tokyo
FEATURE REDUCTION
223

measure is used with q ¼ 1, the resulting mapping is also called a Sammon
mapping.
The effects of using different values of q are demonstrated in Table 7.2.
Here, for each city in the list, the nearest and the furthest cities are given.
For each pair, the true geodesic distance from Table 7.1 is shown
together with the corresponding distances in the MDS mappings.
Clearly, for short distances, q ¼ 2 is more accurate. For long distances,
q ¼ þ2 is favourable. In Figure 7.3(a) to (c) the nearest cities (according
to the geodesic distances) are indicated by solid thin lines. In addition,
the nearest cities according to the MDS maps are indicated by dotted
thick lines, Clearly, if q ¼ 2, the nearest neighbours are best preserved.
This is expected because the relations between nearest neighbours are
best preserved if the local structure is best preserved.
Figure 7.3 also shows a three-dimensional MDS mapping of the world
cities. For clarity, a sphere is also included. MDS is able to find a 2D to
3D mapping such that the resulting map resembles the true geographic
position on the earth surface. The positions are not exactly on the sur-
face of a sphere because the input of the mapping is a set of geodesic
distances, while the output map measures distances according to a three-
dimensional Euclidean metric.
The MDS algorithm has the same problems any gradient-based
method has: it is slow and it cannot be guaranteed to converge to a
global optimum. Another problem the MDS algorithm in particular
suffers from is the fact that the number of distances in a data set grows
quadratically with NS. If the data sets are very large, then we need to
select a subset of the points to map in order to make the application
tractable. Finally, a major problem specific to MDS is that there is no
clear way of finding the projection ynew of a new, previously unseen
point znew. The only way of making sure that the new configuration
fyi, i ¼ 1, . . . NS; ynewg minimizes (7.5) is to recalculate the entire MDS
mapping. In practice, this is infeasible. A crude approximation is to use
triangulation: to map down to D dimensions, search for the D þ 1
nearest neighbours of znew among the training points zi. A mapping
ynew for znew can then be found by preserving the distances to the
projections of these neighbours exactly. This method is fast, but inaccur-
ate: because it uses nearest neighbours, it will not give a smooth inter-
polation of originally mapped points. A different, but more complicated
solution is to use an MDS mapping to train a neural network to predict
the yi for given zi. This will give smoother mappings, but brings with it
all the problems inherent in neural network training. An example of how
to use MDS in PRTools is given in Listing 7.2.
224
UNSUPERVISED LEARNING

Table 7.2
Results of MDS mapping
Nearest cities
Furthest cities
City
Geodesic
distance
Differences between geodesic and
MDS distance
City
Geodesic
distance
Differences between geodesic and
MDS distance
q ¼ 2
q ¼ 0
q ¼ þ2
q ¼ 2
q ¼ 0
q ¼ þ2
Bangkok
Beijing
3290
213
1156
1928
Santiago
17700
146
52
18
Beijing
Tokyo
2130
8
37
168
Santiago
19000
1797
2307
1468
Cairo
Moscow
2810
104
626
983
Melbourne
14000
6268
19
237
Capetown
Rio
6080
1066
1164
5750
Honolulu
18500
1310
2387
864
Honolulu
Los Angeles
4120
225
183
62
Capetown
18500
1310
2387
864
London
Moscow
2450
39
221
1277
Melbourne
16900
11528
3988
2315
Los Angeles
New York
3930
49
124
1924
Capetown
16100
1509
2605
670
Melbourne
Bangkok
7350
75
2196
2327
London
16900
11528
3988
2315
Moscow
London
2450
39
221
1277
Melbourne
14400
9320
2563
1952
New York
Los Angeles
3930
49
124
1924
Melbourne
16600
10794
4878
2559
Rio
Santiago
2730
72
701
1658
Tokyo
18600
1518
2296
1643
Santiago
Rio
2730
72
701
1658
Beijing
19000
1797
2307
1468
Tokyo
Beijing
2130
8
37
168
Rio
18600
1518
2296
1643
FEATURE REDUCTION
225

Listing 7.2
PRTools code for performing an MDS mapping.
load worldcities;
% Load dataset D
options.q ¼ 2;
w ¼ mds(D,2,options);
% Map to 2D with q ¼ 2
figure; clf; scatterd(D*w,‘both’);
% Plot projections
7.2
CLUSTERING
Instead of reducing the number of features, we now focus on reducing
the number of objects in the data set. The aim is to detect ‘natural’
clusters in the data, i.e. clusters which agree with our human interpret-
ation of the data. Unfortunately, it is very hard to define what a natural
cluster is. In most cases, a cluster is defined as a subset of objects for
which the resemblance between the objects within the subset is larger
than the resemblance with other objects in other subsets (clusters).
This immediately introduces the next problem: how is the resemblance
between objects defined? The most important cue for the resemblance of
two objects is the distance between the objects, i.e. their dissimilarity. In
most cases the Euclidean distance between objects is used as a dissimi-
larity measure, but there are many other possibilities. The Lp norm is
well known (see Appendix A.1.1 and A.2):
dpðzi; zjÞ ¼
X
N
n¼1
ðzi;n  zj;nÞp
 
!1
p
ð7:8Þ
The cosine distance uses the angle between two vectors as a dissimilarity
measure. It is often used in the automatic clustering of text documents:
dðzi; zjÞ ¼ 1 
zT
i zj
jjzijj2jjzjjj2
ð7:9Þ
In PRTools the basic distance computation is implemented in the
function proxm. Several methods for computing distances (and similar-
ities) are defined. Next to the two basic distances mentioned above, also
some similarity measures are defined, like the inner product between
vectors, and the Gaussian kernel. The function is implemented as a
mapping. When a mapping w is trained on some data z, the application
226
UNSUPERVISED LEARNING

yw then computes the distance between any vector in z and any vector
in y. For the squared Euclidean distances a short-cut function distm is
defined.
Listing 7.3
PRTools code for defining and applying a proximity mapping.
z ¼ gendatb(3);
% Create some train data
y ¼ gendats(5);
% and some test data
w ¼ proxm(z,‘d’,2);
% Squared Euclidean distance to z
D ¼ y*w;
% 5  3 distance matrix
D ¼ distm(y,z);
% The same 5  3 distance matrix
w ¼ proxm(z,‘o’);
% Cosine distance to z
D ¼ y*w;
% New 5  3 distance matrix
The distance between objects should reflect the important structures in
the data set. It is assumed in all clustering algorithms that distances
between objects are informative. This means that when objects are close
in the feature space, they should also resemble each other in the real
world. When the distances are not defined sensibly, and remote objects
in the feature space correspond to similar real-world objects, no clus-
tering algorithm will be able to give acceptable results without extra
information from the user. In these sections it will therefore be assumed
that the features are scaled such that the distances between objects are
informative. Note that a cluster does not necessarily correspond directly
to a class. A class can consist of multiple clusters, or multiple classes may
form a single cluster (and will therefore probably be hard to discriminate
between).
By the fact that clustering is unsupervised, it is very hard to evaluate a
clustering result. Different clustering methods will yield a different set of
clusters, and the user has to decide which clustering is to be preferred.
A quantitative measure of the quality of the clustering is the average
distance of the objects to their respective cluster centre. Assume that the
objects zi (i ¼ 1, . . . , NS) are clustered in K clusters, Ck (k ¼ 1, . . . , K)
with cluster centre mk and to each of the clusters Nk objects are assigned:
J ¼ 1
NS
X
K
k¼1
1
Nk
X
z2Ck
ðz  mkÞ2
ð7:10Þ
Other criteria, such as the ones defined in Chapter 5 for supervised
learning, can also be applied.
CLUSTERING
227

With these error criteria, different clustering results can be compared
provided that K is kept constant. Clustering results found for varying K
cannot be compared. The pitfall of using criteria like (7.10) is that the
optimal number of clusters is K ¼ NS, i.e. the case in which each object
belongs to its own cluster. In the average-distance criterion it will result
in the trivial solution: J ¼ 0.
The choice of the number of clusters K is a very fundamental problem.
In some applications, an expected number of clusters is known beforehand.
Using this number for the clustering does not always yield optimal results
for all types of clustering methods: it might happen that, due to noise, a
local minimum is reached. Using a slightly larger number of clusters is
therefore sometimes preferred. In most cases the number of clusters to look
for is one of the research questions in the investigation of a data set. Often,
the data is repeatedly clustered using a range of values of K, and the
clustering criterion values are compared. When a significant decrease in a
criterion value appears, a ‘natural’ clustering is probably found. Unfortu-
nately, in practice, it is very hard to objectively say when a significant drop
occurs. True automatic optimization of the number of clusters is possible
in just a very few situations, when the cluster shapes are given.
In the coming sections we will discuss four methods for performing
a clustering: hierarchical clustering, K-means clustering, mixtures of
Gaussians and finally self-organizing maps.
7.2.1
Hierarchical clustering
The basic idea of hierarchical clustering (Johnson, 1967) is to collect
objects into clusters by combining the closest objects and clusters to
larger clusters until all objects are in one cluster. An important advan-
tage is that the objects are not just placed in K distinct groups, but are
placed in a hierarchy of clusters. This gives more information about the
structure in the data set, and shows which clusters are similar or dissimi-
lar. This makes it possible to detect subclusters in large clusters, or to
detect outlier clusters.
Figure 7.4 provides an example. The distance matrix of the 13 world
cities given in Table 7.1 are used to find a hierarchical cluster structure.
The figure shows that at the highest level the cities in the southern
hemisphere are separated from the ones in the northern part. At a
distance level of about 5000 km we see the following clusters: ‘East
Asiatic cities’, ‘European/Arabic cities’, ‘North American cities’, ‘African
city’, ‘South American cities’, ‘Australian city’.
228
UNSUPERVISED LEARNING

Given a set of NS objects zi to be clustered, and an NS  NS distance
matrix between these objects, the hierarchical clustering involves the
following steps:
Algorithm 7.2: Hierarchical clustering
1. Assign each object to its own cluster, resulting in NS clusters, each
containing just one object. The initial distances between all clusters
are therefore just the distances between all objects.
2. Find the closest pair of clusters and merge them into a single cluster,
so that the number of clusters reduces by one.
3. Compute the distance between the new cluster and each of the old
clusters, where the distance between two clusters can be defined in a
number of ways (see below).
4. Repeat steps 2 and 3 until all items are clustered into a single cluster
of size NS, or until a predefined number of clusters K is achieved.
In step 3 it is possible to compute the distances between clusters in
several different ways. We can distinguish single-link clustering, aver-
age-link clustering and complete-link clustering. In single-link clus-
tering, the distance between two clusters Ci and Cj is defined as the
0
2000
4000
6000
8000
10000
12000
14000
Bangkok
Beijing
Tokyo
Cairo
London
Moscow
Honolulu
Los Angeles
New York
Capetown
Rio
Santiago
Melbourne
Figure 7.4
Hierarchical clustering of the data in Table 7.1
CLUSTERING
229

shortest distance from any object in one cluster to any object in the
other cluster:
dslðCi; CjÞ ¼
min
x2Ci;y2Cj kx  yk2
ð7:11Þ
For average-link clustering, the minimum operator is replaced by the
average distance, and for the complete-link clustering it is replaced by
the maximum operator.
In Figure 7.5 the difference between single link and complete link is
shown for a very small toy data set (NS ¼ 6). At the start of the clus-
tering, both single-link (left) and complete-link clustering (right) combine
the same objects to clusters. When larger clusters appear, in the lower
row, different objects are combined. The different definitions for the
inter-cluster distances result in different characteristic cluster shapes. For
single-link clustering, the clusters tend to become long and spidery, while
for complete-link clustering the clusters become very compact.
The user now has to decide on what the most suitable number of
clusters is. This can be based on a dendrogram. The dendrogram shows
at which distances the objects or clusters are grouped together. Examples
–1
0
1
2
–1
–1
–1
–1
–1
–1
–1
–1
–1
–1
–1
–1
–1
–1
–1
–1
–1
0
1
2
1
2
3
4
5
6
0
1
2
0
1
2
1
2
3
4
5
6
0
1
2
0
1
2
1
2
3
4
5
6
0
1
2
0
1
2
1
2
3
4
5
6
0
1
2
0
1
2
1
2
3
4
5
6
0
1
2
0
1
2
1
2
3
4
5
6
(a)
0
1
2
0
1
2
1
2
3
4
5
6
–1
0
1
2
1
0
1
2
1
2
3
4
5
6
0
1
2
0
1
2
1
2
3
4
5
6
–1
0
1
2
1
0
1
2
1
2
3
4
5
6
0
1
2
0
1
2
1
2
3
4
5
6
–1
0
1
2
1
0
1
2
1
2
3
4
5
6
(b)
Figure 7.5
The development from K ¼ NS clusters to K ¼ 1 cluster. (a) Single-link
clustering. (b) Complete-link clustering
230
UNSUPERVISED LEARNING

for single- and complete-link clustering are shown in Figure 7.6. At
smaller distances, pairs of single objects are combined, at higher dis-
tances complete clusters. When there is a large gap in the distances, as
can be seen in the single-link dendrogram, it is an indication that the two
clusters are far apart. Cutting the dendrogram at height 1.0 will then
result in a ‘natural’ clustering, consisting of two clusters. In many
practical cases, the cut is not obvious to define, and the user has to guess
an appropriate number of clusters.
Note that this clustering is obtained using a fixed data set. When new
objects become available, there is no straightforward way to include it in
an existing clustering. In these cases, the clustering will have to be
constructed from the beginning using the complete data set.
In PRTools, it is simple to construct a hierarchical clustering; Listing
7.4 shows an example. Note that the clustering operates on a distance
matrix rather than the data set. A distance matrix can be obtained with
the function distm.
Listing 7.4
PRTools code for obtaining a hierarchical clustering.
z ¼ gendats(5);
% Generate some data
figure; clf; scatterd(z);
% and plot it
dendr ¼ hclust(distm(z),‘s’);
% Single link clustering
figure; clf; plotdg(dendr);
% Plot the dendrogram
1
5
2
4
3
6
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
distance
object
(a)
1
5
2
4
3
6
0
1
2
3
4
5
6
distance
object
(b)
Figure 7.6
Hierarchical clustering with two different clustering types. (a) Single-
link clustering. (b) Complete-link clustering
CLUSTERING
231

7.2.2
K-means clustering
K-means clustering (Bishop, 1995) differs in two important aspects from
hierarchical clustering. First, K-means clustering requires the number of
clusters K beforehand. Second, it is not hierarchical, instead it partitions
the data set into K disjoint subsets. Again the clustering is basically
determined by the distances between objects. The K-means algorithm
has the following structure:
Algorithm 7.3: K-means clustering
1. Assign each object randomly to one of the clusters k ¼ 1, . . . K.
2. Compute the means of each of the clusters:
mk ¼ 1
Nk
X
zi2Ck
zi
ð7:12Þ
3. Reassign each object zi to the cluster with the closest mean mk.
4. Return to step 2 until the means of the clusters do not change any-
more.
The initialization step can be adapted to speed up the convergence.
Instead of randomly labelling the data, K randomly chosen objects are
taken as cluster means. Then the procedure enters the loop in step 3.
Note again that the procedure depends on distances, in this case between
the objects zi and the means mk. Scaling the feature space will here also
change the final clustering result. An advantage of K-means clustering is
that it is very easy to implement. On the other hand, it is unstable:
running the procedure several times will give several different results.
Depending on the random initialization, the algorithm will converge to
different (local) minima. In particular when a high number of clusters is
requested, it often happens that some clusters do not gain sufficient
support and are ignored. The effective number of clusters then becomes
much less than K.
In Figure 7.7 the result of a K-means clustering is shown for a simple
2D data set. The means are indicated by the circles. At the start of the
optimization, i.e. at the start of the trajectory, each mean coincides with
a data object. After 10 iteration steps, the solution converged. The result
of the last iteration is indicated by ‘’. In this case, the number of
clusters in the data and the predefined K ¼ 3 match. A fairly stable
solution is found.
232
UNSUPERVISED LEARNING

Example 7.2
Classification of mechanical parts, K-means clustering
Two results of the K-means algorithm applied to the unlabelled data
set of Figure 5.1(b) are shown in Figure 7.8. The algorithm is called
with K ¼ 4. The differences between the two results are solely caused
by the different realizations of the random initialization of the algo-
rithm. The first result, Figure 7.8(a), is more or less correct (compare
with the correct labelling as given in Figure 5.1(a). Unfortunately, the
result in Figure 7.8(b) indicates that this success is not reproducible.
–2
–1
0
1
2
3
4
–1
0
1
2
3
4
5
6
Figure 7.7
The development of the cluster means during 10 update steps of the
K-means algorithm
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
(a)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
(b)
Figure 7.8
Two results of K-means clustering applied to the ‘mechanical parts’ data set
CLUSTERING
233

The algorithm is implemented in PRTools with the function kmeans.
See Listing 7.5.
Listing 7.5
PRTools code for fitting and plotting a K-means clustering, with K ¼ 4.
load nutsbolts_unlabeled;
% Load the data set z
lab ¼ kmeans(z,4);
% Perform k-means clustering
y ¼ dataset(z,lab);
% Label by cluster assignment
figure; clf; scatterd(y);
% and plot it
7.2.3
Mixture of Gaussians
In the K-means clustering algorithm, spherical clusters were assumed by
the fact that the Euclidean distance to the cluster centres mk is computed.
All objects on a circle or hypersphere around the cluster centre will have
the same resemblance to that cluster. However, in many cases clusters
have more structure than that. In the mixture of Gaussians model
(Dempster et al., 1977; Bishop, 1995), it is assumed that the objects in
each of the K clusters are distributed according to a Gaussian distribu-
tion. That means that each cluster is not only characterized by a mean mk
but also by a covariance matrix Ck. In effect, a complete density estimate
of the data is performed, where the density is modelled by:
pðzÞ ¼
X
K
k¼1
kN zjmk; Ck
ð
Þ
ð7:13Þ
N(zjmk,Ck) stands for the multivariate Gaussian distribution. k are the
mixing parameters (for which PK
k¼1 k ¼ 1 and k  0). The mixing
parameter k can be regarded as the probability that z is produced by
a random number generator with probability density N(zjmk, Ck).
The parameters to be estimated are: the number K of mixing compon-
ents, the mixing parameters 1, . . . , K, the mean vectors mk and the
covariance matrices Ck. We will denote this set of free parameters by
C ¼ fk, mk, Ckjk ¼ 1, . . . , Kg. This increase in the number of free
parameters, compared to the K-means algorithm, leads to the need for
a higher number of training objects in order to reliably estimate these
cluster parameters. On the other hand, because more flexible cluster
shapes are applied, fewer clusters might have to be used to approximate
the structure in the data.
234
UNSUPERVISED LEARNING

One method for fitting the parameters of (7.13) to the data is to use
maximum likelihood estimation (see Section 3.1.4), i.e. to optimize the
log-likelihood:
L ZjC
ð
Þ¼
def ln pðz1; . . . ; zNSjCÞ ¼ ln
Y
NS
n¼1
p znjC
ð
Þ
ð7:14Þ
with Z ¼ fz1, . . . , zNSg. The optimization of L(ZjC) w.r.t. C is compli-
cated and cannot be done directly with this equation. Fortunately, there
exists a standard ‘hill-climbing’ algorithm to optimize the parameters in
this equation. The expectation-maximization (EM) algorithm is a gen-
eral procedure which assumes the following problem definition. We
have two data spaces: the observed data Z and the so-called missing
data (hidden data) X ¼ fx1, . . . , xNSg. Let Y be the complete data in
which each vector yn consists of a known part zn and a missing part xn.
Thus, the vectors yn are defined by yT
n ¼ [ zT
n
xT
n ]. Only zn is available
in the training set, xn is missing, and as such unknown. The EM algo-
rithm uses this model of ‘incomplete data’ to iteratively estimate the
parameters C of the distribution of zn. It tries to maximize the log-
likelihood L(ZjC). In fact, if the result of the i-th iteration is denoted
by C(i), then the EM algorithm assures that L(ZjC(iþ1))  L(ZjC(i)).
The mathematics behind the EM algorithm is rather technical and will
be omitted. However, the intuitive idea behind the EM algorithm is as
follows. The basic assumption is that with all data available, that is if we
had observed the complete data Y, the maximization of L(YjC) would
be simple, and the problem could be solved easily. Unfortunately, only Z
is available and X is missing. Therefore, we maximize the expectation
E[L(YjC)jZ] instead of L(YjC). The expectation is taken over the com-
plete data Y, but under the condition of the observed data Z. The
estimate from the i-th iteration follows from:
E½LðYjCÞjZ ¼
Z
ðln pðYjCÞÞpðYjZ; CðiÞÞdY
Cðiþ1Þ ¼ arg max
C
fE½LðYjCÞjZg
ð7:15Þ
The integral extends over the entire Y space. The first step is the E step;
the last one is the M step.
In the application of EM optimization to a mixture of Gaussians (with
predefined K; see also the discussion on page 228) the missing part xn,
associated with zn, is a K dimensional vector. xn indicates which one of
CLUSTERING
235

the K Gaussians generated the corresponding object zn. The xn are called
indicator variables. They use position coding to indicate the Gaussian
associated with zn. In other words, if zn is generated by the k-th Gaus-
sian, then xn,k ¼ 1 and all other elements in xn are zero. With that, the
prior probability that xn,k ¼ 1 is k.
In this case, the complete log-likelihood of C can be written as:
LðYjCÞ ¼ ln
Y
NS
n¼1
pðzn; xnjCÞ ¼ ln
Y
NS
n¼1
pðznjxn; CÞpðxnjCÞ
¼ ln
Y
NS
n¼1
Y
K
k¼1
½Nðznjmk; CkÞkxn;k
¼
X
K
k¼1
X
NS
n¼1
xn;k ln Nðznjmk; CkÞ þ xn;k ln k
ð7:16Þ
Under the condition of a given Z, the probability density p(YjZ, C) ¼
p(X, ZjZ, C) can be replaced with the marginal probability P(XjZ, C).
Therefore in (7.15), we have:
E½LðYjCÞjZ ¼
Z
ln pðYjCÞ
ð
ÞpðYjZ; CðiÞÞdY
¼
X
X
LðYjCÞPðXjZ; CðiÞÞ
ð7:17Þ
But since in (7.16) L(YjC) is linear in X, we conclude that
E½LðYjCÞjZ ¼ LðX; ZjCÞ
ð7:18Þ
where X is the expectation of the missing data under the condition of Z
and C(i):
xn;k ¼ E½xn;kjzn; CðiÞ ¼ Pðxn;kjzn; CðiÞÞ
¼ pðznjxn;k; CðiÞÞPðxn;kjCðiÞÞ
pðznjCðiÞÞ
¼
N znjmðiÞ
k ; CðiÞ
k


ðiÞ
k
P
K
j¼1
N znjmðiÞ
j ; CðiÞ
j


ðiÞ
j
ð7:19Þ
The variable xn,k is called the ownership because it indicates to what
degree sample zn is attributed to the k-th component.
236
UNSUPERVISED LEARNING

For the M step, we have to optimize the expectation of the log-
likelihood, that is E[L(YjC)jZ] ¼ L(X, ZjC). We do this by substitut-
ing xn,k for xn,k into equation (7.16), taking the derivative with respect
to C and setting the result to zero. Solving the equations will yield
expressions for the parameters C ¼ fk, mk, Ckg in terms of the data zn
and xn,k.
Taking the derivative of L(X, ZjC) with respect to mk gives:
X
NS
n¼1
xn;kðzn  mkÞTC1
k
¼ 0
ð7:20Þ
Rewriting this, gives the update rule for mk:
^mk ¼
P
NS
n¼1
xn;kzn
P
NS
n¼1
xn;k
ð7:21Þ
The estimation of Ck is somewhat more complicated. With the help of
(b.39), we can derive:
q
qC 1
k
ln Nðznj^mk; CkÞ ¼ 1
2 Ck  1
2 ðzn  ^mkÞðzn  ^mkÞT
ð7:22Þ
This results in the following update rule for Ck:
^Ck ¼
P
NS
n¼1
xn;kðzn  ^mkÞðzn  ^mkÞT
P
NS
n¼1
xn;k
ð7:23Þ
Finally, the parameters k cannot be optimized directly because of the
extra constraint, namely that PK
k¼1 k ¼ 1. This constraint can be
enforced by introducing a Lagrange multiplier  and extending the log-
likelihood (7.16) by:
L0ðYjCÞ ¼
X
NS
n¼1
X
K
k¼1
xn;k ln Nðznjmk; CkÞ þ xn;k ln k  
X
K
k¼1
k  1
 
!
ð7:24Þ
CLUSTERING
237

Substituting xn;k for xn;k and setting the derivative with respect to k to
zero, yields:
X
NS
n¼1
xn;k ¼ k
ð7:25Þ
Summing equation (7.25) over all clusters, we get:
X
K
k¼1
X
NS
n¼1
xn;k ¼ 
X
K
k¼1
k ¼ 
ð7:26Þ
Further note, that summing PK
k¼1
PNS
n¼1 xn,k over all clusters gives the
total number of objects NS, thus it follows that  ¼ NS. By substituting
this result back into (7.25), the update rule for k becomes:
^k ¼ 1
NS
X
NS
n¼1
xn;k
ð7:27Þ
Note that with (7.27), the determination of mk and Ck in (7.21) and
(7.23) can be simplified to:
^mk ¼
1
NS^k
X
NS
n¼1
xn;kzn
^Ck ¼
1
NS^k
X
NS
n¼1
xn;kðzn  ^mkÞðzn  ^mkÞT
ð7:28Þ
The complete EM algorithm for fitting a mixture of Gaussian model to a
data set is as follows.
Algorithm 7.4: EM algorithm for estimating a mixture of Gaussians
Input: The number K of mixing components and the data zn.
1. Initialization: Select randomly (or heuristically) C(0). Set i ¼ 0.
2. Expectation step (E step): Using the observed data set zn and the
estimated parameters C(i), calculate the expectations xn of the miss-
ing values xn using (7.19).
238
UNSUPERVISED LEARNING

3. Maximization step (M step): Optimize the model parameters
C(iþ1) ¼ f^k, ^mk, ^Ckg by maximum likelihood estimation using the
expectation xn calculated in the previous step. See (7.27) and (7.28).
4. Stop if the likelihood did not change significantly in step 3. Else,
increment i and go to step 2.
Clustering by EM results in more flexible cluster shapes than K-means
clustering. It is also guaranteed to converge to (at least) a local optimum.
However, it still has some of the same problems of K-means: the choice
of the appropriate number of clusters, the dependence on initial condi-
tions and the danger of convergence to local optima.
Example 7.3
Classification of mechanical parts, EM algorithm for
mixture of Gaussians
Two results of the EM algorithm applied to the unlabelled data set of
Figure 5.1(b) are shown in Figure 7.9. The algorithm is called with
K ¼ 4, which is the correct number of classes. Figure 7.9(a) is a
correct result. The position and size of each component is appropri-
ate. With random initializations of the algorithm, this result is repro-
duced with a probability of about 30%. Unfortunately, if the
initialization is unlucky, the algorithm can also get stuck in a local
minimum, as shown in Figure 7.9(b). Here, the components are at the
wrong position.
The EM algorithm is implemented in PRTools by the function
emclust. Listing 7.6 illustrates its use.
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
(a)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
measure of six-fold rotational symmetry
measure of eccentricity
(b)
Figure 7.9
Two results of the EM algorithm for mixture of Gaussians estimation
CLUSTERING
239

Listing 7.6
MATLAB code for calling the EM algorithm for mixture of Gaussians
estimation.
load nutsbolts_unlabeled;
% Load the data set z
z ¼ setlabtype(z,‘soft’);
% Set probabilistic labels
[lab,w] ¼ emclust(z,qdc,4);
% Cluster using EM
figure; clf; scatterd(z);
plotm(w,[],0.2:0.2:1);
% Plot results
7.2.4
Mixture of probabilistic PCA
An interesting variant of the mixture of Gaussians is the mixture of
probabilistic principal component analyzers (Tipping and Bishop,
1999). Each single model is still a Gaussian like in (7.13), but its
covariance matrix is constrained:
C 0
k ¼ WT
k Wk þ 2
kI
ð7:29Þ
where the D  N matrix Wk has the D eigenvectors corresponding to the
largest eigenvalues of Ck as its columns, and the noise level outside the
subspace spanned by Wk is estimated using the remaining eigenvalues:
2
k ¼
1
N  D
X
N
m¼Dþ1
m
ð7:30Þ
The EM algorithm to fit a mixture of probabilistic principal component
analyzers proceeds just as for a mixture of Gaussians, using C0
k instead of
Ck. At the end of the M step, the parameters Wk and 2
k are re-estimated
for each cluster k by applying normal PCA to Ck and (7.30), respectively.
The mixture of probabilistic principal component analyzers intro-
duces a new parameter, the subspace dimension D. Nevertheless, it uses
far fewer parameters (when D  N) than the standard mixture of
Gaussians. Still it is possible to model nonlinear data, which cannot be
done using normal PCA. Finally, an advantage over normal PCA is that
it is a full probabilistic model, i.e. it can be used directly as a density
estimate.
In PRTools, probabilistic PCA is implemented in qdc: an additional
parameter specifies the number of subspace dimensions to use. To train a
mixture, one can use emclust, as is illustrated in Listing 7.7.
240
UNSUPERVISED LEARNING

Listing 7.7
MATLAB code for calling the EM algorithm for mixture of probabilistic
principal component analyzers estimation.
load nutsbolts_unlabeled;
% Load the data set z
z ¼ setlabtype(z, ‘soft’);
% Set probabilistic labels
[lab,w] ¼ emclust(z,qdc([],[],[],1),4); % Cluster 1D PCAs using EM
figure; clf; scatterd(z);
plotm(w, [],0.2:0.2:1);
% Plot results
7.2.5
Self-organizing maps
The self-organizing map (SOM; also known as self-organizing feature
map, Kohonen map) is an unsupervised clustering and feature extraction
method in which the cluster centres are constrained in their placing
(Kohonen, 1995). The construction of the SOM is such that all objects
in the input space retain as much as possible their distance and neigh-
bourhood relations in the mapped space. In other words, the topology is
preserved in the mapped space. The method is therefore strongly related
to multi-dimensional scaling.
The mapping is performed by a specific type of neural network, equipped
with a special learning rule. Assume that we want to map an
N-dimensional measurement space to a D-dimensional feature space,
where D  N. In fact, often D ¼ 1 or D ¼ 2. In the feature space, we
define a finite orthogonal grid with M1  M2      MD grid points. At
each grid point we place a neuron. Each neuron stores an N-dimensional
vector that serves as a cluster centre. By defining a grid for the neurons, each
neuron does not only have a neighbouring neuron in the measurement
space, it also has a neighbouring neuron in the grid. During the learning
phase, neighbouring neurons in the grid are enforced to also be neighbours
in the measurement space. By doing so, the local topology will be preserved.
The SOM is updated using an iterative update rule, which honours the
topological constraints on the neurons. The iteration runs over all training
objects zn, n ¼ 1, ... , NS and at each iteration, the following steps are taken:
Algorithm 7.5: Training a self-organizing map:
1. Initialization: Choose the grid size, M1, . . . , MD, and initialize the
weight vectors w(0)
j
(where j ¼ 1, . . . , M1     MD) of each neuron,
for instance, by assignment of the values of M1     MD different
samples from the data set and that are randomly selected. Set i ¼ 0.
CLUSTERING
241

2. Iterate:
2.1 Find, for each object zn in the training set, the most similar
neuron w(i)
k .
kðznÞ ¼ arg min
j
kzn  wðiÞ
j k
ð7:31Þ
This is called the best-matching or winning neuron for this input
vector.
2.2 Update the winning neuron and its neighbours using the update
rule:
wðiþ1Þ
j
¼ wðiÞ
j
þ ðiÞhðiÞðjkðznÞ  jjÞðzn  wðiÞ
j Þ
ð7:32Þ
2.3 Repeat 2.1 and 2.2 for all samples zn in the data set.
2.4 If the weights in the previous steps did not change significantly,
then stop. Else, increment i and go to step 2.1.
Here (i) is the learning rate and h(i)(jk(zn)  jj) is a weighting function.
Both can depend on the iteration number i. This weighting function
weighs how much a particular neuron in the grid is updated. The term
jk(zn)  jj indicates the distance between the winning neuron k(zn) and
neuron j, measured over the grid. The winning neuron (for which
j ¼ k(zn)) will get the maximal weight, because h(i)() is chosen such that:
hðiÞðÞ  1 and hðiÞð0Þ ¼ 1
ð7:33Þ
Thus, the winning neuron will get the largest update. This update moves
the neuron in the direction of zn by the term (zn  wj).
The other neurons in the grid will receive smaller updates. Since we
want to preserve the neighbourhood relations only locally, the further
the neuron is from the winning neuron, the smaller the update.
A commonly used weighting function which satisfies these requirements
is the Gaussian function:
hðiÞðxÞ ¼ exp  x2
2
ðiÞ
 
!
ð7:34Þ
For this function a suitable scale (i) over the map should be defined.
This weighting function can be used for a grid of any dimension (not just
one-dimensional), when we realize that jk(zn)  jj means in general the
distance between the winning neuron k(zn) and neuron j over the grid.
242
UNSUPERVISED LEARNING

Let us clarify this with an example. Assume we start with data zn
uniformly distributed in a square in a two-dimensional measurement
space, and we want to map this data into a one-dimensional space.
Therefore, K ¼ 15 neurons are defined. These neurons are ordered, such
that neuron j  1 is the left neighbour of neuron j and neuron j þ 1 is the
right neighbour. In the weighting function  ¼ 1 is used, in the update
rule  ¼ 0:01; these are not changed during the iterations. The neurons
have to be placed as objects in the feature space such that they represent
the data as best as possible. Listing 7.8 shows an implementation for
training a one-dimensional map in PRTools.
Listing 7.8
PRTools code for training and plotting a self-organizing map.
z ¼ rand(100,2);
% Generate the data set z
w ¼ som(z,15);
% Train a 1D SOM and show it
figure; clf; scatterd(z); plotsom(w);
In Figure 7.10 four scatter plots of this data set with the SOM (K ¼ 15)
are shown. In the left subplot, the SOM is randomly initialized by
picking K objects from the data set. The lines between the neurons
indicate the neighbouring relationships between the neurons. Clearly,
neighbouring neurons in feature space are not neighbouring in the grid.
In the fourth subplot it is visible that after 100 iterations over the data
set, the one-dimensional grid has organized itself over the square. This
solution does not change in the next 500 iterations.
With one exception, the neighbouring neurons in the measurement
space are also neighbouring neurons in the grid. Only where the one-
dimensional string crosses, neurons far apart in the grid become close
neighbours in feature space. This local optimum, where the map did
not unfold completely in the measurement space, is often encountered in
SOMs. It is very hard to get out of this local optimum. The solution
would be to restart the training with another random initialization.
Many of the unfolding problems, and the speed of convergence, can be
solved by adjusting the learning parameter (i) and the characteristic
width in the weighting function h(i)(jk(zn)  jj) during the iterations.
Often, the following functional forms are used:
ðiÞ ¼ ð0Þ expði=1Þ
ðiÞ ¼ ð0Þ expði=2Þ
ð7:35Þ
CLUSTERING
243

This introduces two additional scale parameters which have to be set by
the user.
Although the SOM offers a very flexible and powerful tool for map-
ping a data set to one or two dimensions, the user is required to make
many important parameter choices: the dimension of the grid, the num-
ber of neurons in the grid, the shape and initial width of the neighbour-
hood function, the initial learning rate and the iteration dependencies of
the neighbourhood function and learning rate. In many cases a two-
dimensional grid is chosen for visualization purposes, but it might not fit
No of iterations = 0
(a)
No of iterations = 10
(b)
No of iterations = 25
(c)
No of iterations = 100
(d)
Figure 7.10
The development of a one-dimensional self-organizing map, trained on
a two-dimensional uniform distribution: (a) initialization; (b)–(d) after 10, 25 and
100 iterations, respectively
244
UNSUPERVISED LEARNING

the data well. The retrieved data reduction will therefore not reflect the
true structure in the data and visual inspection only reveals an artificially
induced structure. Training several SOMs with different settings might
provide some stable solution.
Example 7.4
SOM of the RGB samples of an illuminated surface
Figure 7.11(a) shows the RGB components of a colour image. The
imaged object has a spherical surface that is illuminated by a spot
illuminator. The light–material interaction involves two distinct
red
green
blue
(a)
0 50 100 150 200 250
0
50
100
150
200
250
0
50
100
150
200
green
No of iterations = 100
blue
red
(b)
Figure 7.11
A SOM that visualizes the effects of a highlight. (a) RGB image of an
illuminated surface with a highlight (¼glossy spot). (b) Scatter diagram of RGB
samples together with a one-dimensional SOM
CLUSTERING
245

physical mechanisms: diffuse reflection and specular (mirror-like)
reflection. The RGB values that result from diffuse reflection are
invariant to the geometry. Specular reflection only occurs at specific
surface orientations determined by the position of the illuminator and
the camera. Therefore, specular reflection is seen in the image as a
glossy spot, a so-called highlight. The colour of the surface is deter-
mined by its spectral properties of the diffuse reflection component.
Usually, specular reflection does not depend on the wavelength of the
light so that the colour of the highlight is solely determined by the
illuminator (usually white light).
Since light is additive, a RGB value z is observed as a linear
combination of the diffuse component and the off-specular compon-
ent: z ¼ zdiff þ 	zspec. The variables  and 	 depend on the geom-
etry. The estimation of zdiff and zspec from a set of samples of z is an
interesting problem. Knowledge of zdiff and zspec would, for instance,
open the door to ‘highlight removal’.
Here, a one-dimensional SOM of the data is helpful to visualize the
data. Figure 7.11(b) shows such a map. The figure suggests that the
data forms a one-dimensional manifold, i.e. a curve in the three-
dimensional RGB space. The manifold has the shape of an elbow. In
fact, the orientation of the lower part of the elbow corresponds to
data from an area with only diffuse reflection, i.e. to zdiff. The upper
part corresponds to data from an area with both diffuse and specular
reflection, i.e. to zdiff þ 	zspec.
7.2.6
Generative topographic mapping
We conclude this chapter with a probabilistic version of the self-
organizing map, the generative topographic mapping (or GTM) (Bishop
et al., 1998). The goal of the GTM is the same as that of the SOM: to
model the data by clusters with the constraint that neighbouring clusters
in the original space are also neighbouring clusters in the mapped space.
Contrary to the SOM, a GTM is fully probabilistic and it can be trained
using the EM algorithm.
The GTM starts from the idea that the density p(z) can be represented
in terms of a D-dimensional latent variable q, where in general D  N.
For this, a function z ¼ f(q; W) with weights W has to be found. The
functions maps a point q into a corresponding object z ¼ f(q; W).
Because in reality the data will not be mapped perfectly, we assume
246
UNSUPERVISED LEARNING

some Gaussian noise with variance 2 in all directions. The full model
for the probability of observing a vector z is:
pðzjq; W; 2Þ ¼
1
2
D
2D exp  fðq; WÞ  z
k
k2
2
 
!
ð7:36Þ
In general, the distribution of z in the high dimensional space can be
found by integration over the latent variable q:
pðzjW; 2Þ ¼
Z
pðzjq; W; 2ÞpðqÞdq
ð7:37Þ
In order to allow an analytical solution of this integral, a simple grid-like
probability model is chosen for p(q), just like in the SOM:
pðqÞ ¼ 1
K
X
K
k¼1
ðq  qkÞ
ð7:38Þ
i.e. a set of Dirac functions centred on grid nodes qk. The log-likelihood
of the complete model can then be written as:
ln LðW; 2Þ ¼
X
NS
n¼1
ln
1
K
X
K
k¼1
pðznjqk; W; 2Þ
 
!
ð7:39Þ
Still, the functional form for the mapping function f(q; W) has to be
defined. This function maps the low dimensional grid to a manifold in
the high dimensional space. Therefore, its form controls how nonlinear
the manifold can become. In the GTM, a regression on a set of fixed
basis functions is used:
fðq; WÞ ¼ Wg ðqÞ
ð7:40Þ
g (q) is a vector containing the output of M basis functions, which are
usually chosen to be Gaussian with means on the grid points and a fixed
width ’. W is a N  M weight matrix.
Given settings for K, M and ’, the EM algorithm can be used to
estimate W and 2. Let the complete data be yT
n ¼ [zT
n
xT
n ], with xn the
hidden variables. xn is a K-dimensional vector. The element xn,k codes
CLUSTERING
247

the membership of zn with respect to the k-th cluster. After proper
initialization, the EM algorithm proceeds as follows:
Algorithm 7.6: EM algorithm for training a GTM:
1. E step: estimate the missing data:
xn;k ¼ pðqkjzn; ^W; ^2Þ ¼
pðznjqk; ^W; ^2Þ
P
K
k¼1
pðznjqk; ^W; ^2Þ
ð7:41Þ
using Bayes’ theorem.
2. M step: re-estimate the parameters:
^WT ¼ ðTG þ IÞ1TX
TZ
ð7:42Þ
^2 ¼
1
NSN
X
NS
n¼1
X
K
k¼1
xn;kk ^Wg ðqkÞ  znk2
ð7:43Þ
3. Repeat 1 and 2 until no significant changes occur.
G is a K  K diagonal matrix with Gkk ¼ PNS
n¼1 zn,k as elements.  is a
K  M matrix containing the basis functions: k,m ¼ m(qk). X is a
NS  K matrix with the xn,k as elements. Z is a NS  N matrix. It
contains the data vectors zT
n as its rows. Finally,  is a regularization
parameter, which is needed in cases where TG becomes singular and
the inverse cannot be computed.
The GTM can be initialized by performing PCA on the data set,
projecting the data to D dimensions and finding W such that the result-
ing latent variables qn approximate as well as possible the projected
data. The noise variance 2 can be found as the average of the N  D
smallest eigenvalues, as in probabilistic PCA.
Training the GTM is usually quick and converges well. However, it
suffers from the standard problem of EM algorithms in that it may
converge to a local optimum. Furthermore, its success depends highly
on the choices for K (the number of grid points), M (the number of basis
functions for the mapping), ’ (the width of these basis functions) and
248
UNSUPERVISED LEARNING

 (the regularization term). For inadequate choices, the GTM may
reflect the structure in the data very poorly. The SOM has the same
problem, but needs to estimate somewhat less parameters.
An advantage of the GTM over the SOM is that the parameters that
need to be set by the user have a clear interpretation, unlike in the SOM
where unintuitive parameters as learning parameters and time para-
meters have to be defined. Furthermore, the end result is a probabilistic
model, which can easily be compared to other models (e.g. in terms of
likelihood) or combined with them.
Figure 7.12 shows some examples of GTMs (D ¼ 1) trained on uni-
formly distributed data. Figure 7.12(a) clearly shows how the GTM can
be overtrained if too many basis functions – here, 10 – are used. In
Figure 7.12(b), less basis functions are used and the manifold found is
much smoother. Another option is to use regularization, which also
gives a more smooth result as shown in Figure 7.12(c) but cannot
completely prevent extreme nonlinearities.
Listing 7.9 shows how a GTM can be trained and displayed in
PRTools.
Listing 7.9
PRTools code for training and plotting generative topographic mapping.
z ¼ rand(100,2);
% Generate the data set z
w ¼ gtm(z,15);
% Train a 1D GTM and show it
figure; clf; scatterd(z); plotgtm(w);
(a)
(b)
(c)
Figure 7.12
Trained generative topographic mappings. (a) K ¼ 14, M ¼ 10, ’ ¼ 0:2
and  ¼ 0. (b) K ¼ 14, M ¼ 5, ’ ¼ 0:2 and  ¼ 0. (c) K ¼ 14, M ¼ 10, ’ ¼ 0:2 and
 ¼ 0:01
CLUSTERING
249

7.3
REFERENCES
Bishop, C.M., Neural Networks for Pattern Recognition, Oxford University Press,
Oxford, UK, 1995.
Bishop, C.M., Svense´n, M. and Williams, C.K.I., GTM: the generative topographic
mapping. Neural Computation, 10(1), 215–34, 1998.
Dempster, N.M., Laird, A.P. and Rubin, D.B., Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Statististical Society B, 39, 185–197,
1977.
Johnson, S.C., Hierarchical clustering schemes. Psychometrika, 2, 241–54, 1967.
Jolliffe, I.T., Principal Component Analysis, Springer-Verlag, New York, 1986.
Kohonen, T., Self-organizing Maps, Springer-Verlag, Heidelberg, Germany, 1995.
Kruskal, J.B. and Wish, M., Multidimensional scaling, Sage Publications, Beverly Hills,
CA, 1977.
Sammon, Jr, J.W., A nonlinear mapping for data structure analysis. IEEE Transactions
on Computers, C-18, 401–9, 1969.
Tipping, M.E. and Bishop, C.M., Mixtures of probabilistic principal component ana-
lyzers. Neural Computation, 11(2), 443–82, 1999.
7.4
EXERCISES
1. Generate a two-dimensional data set z uniformly distributed between 0 and 1. Create a
second data set y uniformly distributed between 1 and 2. Compute the (Euclidean)
distances between z and y, and find the objects in y which have distance smaller than 1
to an object in z. Make a scatter plot of these objects. Using a large number of objects
in z, what should be the shape of the area of objects with distance smaller than 1?
What would happen if you change the distance definition to the city-block distance
(Minkowski metric with q ¼ 1)? And what would happen if the cosine distance is
used? (0)
2. Create a data set z ¼ gendatd(50, 50, 4,2); . Make a scatter plot of the data. Is the
data separable? Predict what would happen if the data is mapped to one dimension.
Check your prediction by mapping the data using pca(z,1), and training a simple
classifier on the mapped data (such as ldc). (0)
3. Load the worldcities data set and experiment with using different values for q in
the MDS criterion function. What is the effect? Can you think of another way of
treating close sample pairs different from far-away sample pairs? (0)
4. Derive equations (7.21), (7.23) and (7.27). ()
5. Discuss in which data sets it can be expected that the data is distributed in a subspace,
or in clusters. In which cases will it not be useful to apply clustering or subspace
methods? ()
6. What is a desirable property of a clustering when the same algorithm is run multiple
times on the same data set? Develop an algorithm that uses this notion to estimate the
number of clusters present in the data. ()
7. In terms of scatter matrices (see the previous chapter), what does the K-means algo-
rithm minimize? (0)
250
UNSUPERVISED LEARNING

8. Under which set of assumptions does the EM algorithm degenerate to the K-means
algorithm? ()
9. What would be a simple way to lower the risk of ending up in a poor local minimum
with the K-means and EM algorithms? (0)
10. Which parameter(s) control(s) the generalization ability of the self-organizing map
(for example, its ability to predict the locations of previously unseen samples)? And
that of the generative topographic mapping? ()
EXERCISES
251


8
State Estimation in Practice
Chapter 4 discussed the theory needed for the design of a state estimator.
The current chapter addresses the practical issues related to the design.
Usually, the engineer cycles through a number of design stages of which
some are depicted in Figure 8.1.
One of the first steps in the design process is system identification. The
purpose is to formulate a mathematical model of the system of interest.
As stated in Chapter 4, the model is composed of two parts: the state
space model of the physical process and the measurement model of the
sensory system. Using these models, the theory from Chapter 4 provides
us with the mathematical expressions of the optimal estimator.
The next questions in the design process are the issues of observa-
bility (can all states of the process be estimated from the given set of
measurements?) and stability. If the system is not observable or not
stable, either the model must be revised or the sensory system must be
redesigned.
If the design passes the observability and the stability tests, the
attention is focussed at the computational issues. Due to finite arith-
metic precision, there might be some pitfalls. Since in state estimation
the measurements are processed sequentially, the effects of round-off
errors may accumulate and may cause inaccurate results. The estima-
tor may even completely fail to work due to numerical instabilities.
Although the optimal solution of an estimation problem is often
unique, there are a number of different implementations which are
Classification, Parameter Estimation and State Estimation: An Engineering Approach using MATLAB
F. van der Heijden, R.P.W. Duin, D. de Ridder and D.M.J. Tax
 2004 John Wiley & Sons, Ltd
ISBN: 0-470-09013-8

all mathematically equivalent, and thus all representing the same
solution, but with different sensitivities to round-off errors. Thus, in
this stage of the design process the appropriate implementation must
be selected.
As soon as the estimator has been realized, consistency checks must be
performed to see whether the estimator behaves in accordance with the
expectations. If these checks fail, it is necessary to return to an earlier
stage, i.e. refinements of the models, selection of another implementa-
tion, etc.
Section 8.1 presents a short introduction to system identification.
The topic is a discipline on its own and will certainly not be covered
here in its full length. For a full treatment we refer to the pertinent
literature (Box and Jenkins, 1976; Eykhoff, 1974, Ljung and Glad,
1994; Ljung, 1999; So¨derstro¨m and Stoica, 1989). Section 8.2 dis-
cusses the observability and the dynamic stability of an estimator.
Section 8.3 deals with the computational issues. Here, several imple-
mentations are given each with its own sensitivities to numerical
instabilities. Section 8.4 shows how consistency checks can be accom-
plished. Finally, Section 8.5 deals with extensions of the discrete
Kalman filter. These extensions make the estimator applicable to a wider
class of problems, i.e. non-white/cross-correlated noise sequences and
offline estimation.
Some aspects of state estimator design are not discussed in this book;
for instance sensitivity analysis and error budgets (Gelb et al., 1974).
These techniques are systemic methods for the identification of the most
vulnerable parts of the design.
Most topics in this chapter concern Kalman filtering as introduced in
Section 4.2.1, though some are also of relevance for extended Kalman
mathematical
model
system
identification
theoretical
estimator
design
mathematical
formulation of
state
estimator
implementation
 analysis
+ empirical
evaluation
observability
and stability
analysis
realization
consistency checks
computational
aspects
Figure 8.1
Design stages for state estimators
254
STATE ESTIMATION IN PRACTICE

filtering. For the sake of convenience, the equations are repeated here.
The point of departure in Kalman filtering is a linear-Gaussian model of
the physical process:
xði þ 1Þ ¼ FðiÞxðiÞ þ LðiÞuðiÞ þ wðiÞ
i ¼ 0;1;...
zðiÞ ¼ HðiÞxðiÞ þ vðiÞ
ðstate equationÞ
ðmeasurement modelÞ
ð8:1Þ
x(i) is the state vector with dimension M. z(i) is the measurement vector
with dimension N. The process noise w(i) and measurement noise v(i)
are white Gaussian noise sequences, zero mean, and with covariance
matrix Cw(i) and Cv(i), respectively. Process noise and measurement
noise are uncorrelated: Cwv(i) ¼ 0. The prior knowledge is that x(0)
has a Gaussian distribution with expectation E[x(0)] and covariance
matrix Cx(0).
The MMSE solution to the online estimation problem is developed in
Section 4.2.1, and is known as the discrete Kalman filter. The solution is
an iterative scheme. Each iteration cycles through (4.27) and (4.28),
which are repeated here for convenience:
update :
^zðiÞ ¼ HðiÞxðiji1Þ
SðiÞ ¼ HðiÞCðiji1ÞHTðiÞþCvðiÞ
KðiÞ ¼ Cðiji1ÞHTðiÞS1ðiÞ
xðijiÞ ¼ xðiji1ÞþKðiÞ zðiÞ^zðiÞ
ð
Þ
CðijiÞ ¼ Cðiji1ÞKðiÞSðiÞKTðiÞ
prediction :
xðiþ1jiÞ ¼ FðiÞxðijiÞþLðiÞuðiÞ
Cðiþ1jiÞ ¼ FðiÞCðijiÞFTðiÞþCwðiÞ
ðpredicted measurementÞ
ðinnovation matrixÞ
ðKalman gain matrixÞ
ðupdated estimateÞ
ðerror covariance matrixÞ
ðpredictionÞ
ðpredicted state covarianceÞ
ð8:2Þ
The iterative procedure is initiated with the prediction for i ¼ 0 set equal
to the prior:
xð0j1Þ¼
defE½xð0Þ and Cð0j1Þ¼
defCxð0Þ:
STATE ESTIMATION IN PRACTICE
255

8.1
SYSTEM IDENTIFICATION
System identification is the act of formulating a mathematical model of a
given dynamic system based on input and output measurements of that
system, and on general knowledge of the physical process at hand. The
discipline of system identification not only finds application in estimator
design, but also monitoring, fault detection and diagnosis (e.g. for
maintenance of machines) and design of control systems.
A dichotomy of models exists between parametric models and non-
parametric models. The nonparametric models describe the system by
means of tabulated data of, for instance, the Fourier transfer function(s)
or the edge response(s). Various types of parametric models exist, e.g.
state space models, poles-zeros models and so on. In our case, state space
models are the most useful, but other parametric models can also be used
since most of these models can be converted to a state space.
The identification process can roughly be broken down into four parts:
structuring, experiment design, estimation, evaluation and selection.
8.1.1
Structuring
The first activity is structuring. The structure of the model is settled by
addressing the following questions. What is considered part of the system
and what is environment? What are the (controllable) input variables? What
are the possible disturbances (process noise)? What are the state variables?
What are the output variables? What are the physical laws that relate the
physical variables? Which parameters of these laws are known, and which
are unknown? Which of these parameters can be measured directly?
Usually, there is not a unique answer to all these questions. In fact, the
result of structuring is a set of candidate models.
Example 8.1
Candidate models describing a simple hydraulic system
The hydraulic system depicted in Figure 8.2 consists of two identical
tanks connected by a pipeline with flow q1(t). The input flow q0(t) is
acting on the first tank. q2(t) is the output flow from the second tank.
The relation between the level and the net input flow of a tank is
Cqh ¼ qqt. C is the capacity of the tank. If the horizontal cross-sections
of the tanks are constant, the capacity does not depend on h. In the
present example, the capacity of both tanks is C ¼ 420 (cm2). The order
of the system is at least two; the two states being the levels h1(t) and h2(t).
256
STATE ESTIMATION IN PRACTICE

For the model of the flow through the pipelines we consider three
possibilities, each leading to a different structure of the model.
Candidate model I: Frictionless liquids; Torricelli’s law
For a frictionless liquid, Torricelli’s law states that when a tank leaks,
the sum of potential and kinetic energy is constant: q2 ¼ 2A2gh. A is
the area of the hole. Application of this law gives rise to the following
second order, nonlinear model (g is the gravitational constant):
_h1 ¼  1
C
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2A2
1gðh1  h2Þ
q
þ 1
C q0
_h2 ¼ þ 1
C
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2A2
1gðh1  h2Þ
q
 1
C
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2A2
2gh2
q
ð8:3Þ
Candidate model II: Linear friction
Here, we assume that the difference of pressure on both sides of a
pipeline holds a linear relation with the flow: p ¼ Rq. The para-
meter R is the resistance. Since p ¼ gh ( is the mass density), the
assumption brings the following linear, second order model:
_h1 ¼ g
R1C ðh2  h1Þ þ 1
C q0
_h2 ¼ g
R1C ðh1  h2Þ þ g
R2C h2
ð8:4Þ
Candidate model III: Linear friction and hydraulic inertness
A liquid within a pipeline with a length ‘ and cross-section A experi-
ences a force ‘_q (second law of Newton: F ¼ ma). This force is
induced by the difference of pressure F ¼ Ap ¼ Agh. Thus,
z1
z2
q1
q2
q0
h1
h2
tank 1
input flow
tank 2
drain
∆h = h1 – h2
Figure 8.2
A simple hydraulic system consisting of two connected tanks
SYSTEM IDENTIFICATION
257

‘_q ¼ Agh. With friction, the equation is ARq þ ‘_q ¼ Agh. With
that, we arrive at the following linear, fourth order model:
€h1
€h2


¼ 1
C
m1
m1
m1
m1  m2


h1
h2


þ
r1
0
r1  r2
r2


_h1_h2


þ 1
C
r1
1
r2  r1
0


q0
_q0


ð8:5Þ
where
m1 ¼ gA1/‘1, m2 ¼ gA2/‘2, r1 ¼ R1A1/(‘1) and r2 ¼ R2A2/(‘2).
8.1.2
Experiment design
The purpose of the experimentation is to enable the determination of the
unknown parameters and the evaluation and selection of the candidate
models. The experiment comes down to the enforcement of some input
test signals to the system at hand and the acquisition of measured data.
The design aspects of the experiments are the choice of input signals, the
choice of the sensors and the sensor locations and the preprocessing of
the data.
The input signals should be such that the relevant aspects of the
system at hand are sufficiently covered. The bandwidth of the input sig-
nal must match the bandwidth of interest. The signal magnitude must be
in a range as required by the application. Furthermore, the signal energy
should be sufficiently large so as to achieve suitable signal-to-noise
ratios at the output of the sensors. Here, a trade-off exists between the
bandwidth, the registration interval and the signal magnitude.
For instance, a very short input pulse covers a large bandwidth and
permits a low registration interval, but requires a tremendous (perhaps
excessive) magnitude in order to have sufficient signal-to-noise ratios. At
the other side of the extremes, a single sinusoidal burst with a long
duration covers only a very narrow bandwidth, and the signal magni-
tude can be kept low yet offering enough signal energy. A signal, often
used, is the pseudorandom binary signal.
The choice of sensors is another issue. First, the set of identifiable,
unknown parameters of the model must be determined. For instance, if
in equation (8.4)  and R1 are unknown, then these parameters are not
258
STATE ESTIMATION IN PRACTICE

separatelyidentifiablefromthelevelmeasurementsbecausetheseparameters
always occur in the combination /R1. Thus, in this case we can treat /R1 as
one identifiable parameter. Sometimes, it might be necessary to temporarily
use additional (often expensive) sensors so as to enable the estimation of all
relevant parameters. As soon as the system identification is satisfactorily
accomplished, these additional sensors can be removed from the system.
Often, the acquired data need preprocessing before the parameter esti-
mation and evaluation take place. Reasons for doing so are, for instance:
. If the bandwidth of the noise is larger than the bandwidth of
interest, filtering can be applied to suppress the noise in the unim-
portant frequency ranges.
. If a linearized model is strived for, the unimportant offsets should
be removed (offset correction, baseline removal). Often, this is done
by subtraction of the average from the signal.
. Sudden peaks (spikes) in the data are probably caused by disturb-
ances such as mechanical shocks and electrical inferences due to
insufficient shielding. These peaks should be removed.
In order to prevent overfitting, it might be useful to split the data
according to two time intervals. The data in the first interval is used
for parameter estimation. The second interval is used for model evalu-
ation. Cross-evaluation might also be useful.
Example 8.2
Experimental data from the hydraulic system
Figure 8.3 shows data obtained from the hydraulic system depicted in
Figure 8.2. The data is obtained using two level sensors that measure
the levels h1 and h2. The sample period is  ¼ 5 (s). The standard
deviation of the sensor noise is about v ¼ 0:04 (cm).
The measured levels in Figure 8.3 correspond to the free response of
the system obtained with zero input and with an initial condition in
which both tanks are completely filled, i.e. h1(0) ¼ h2(0) ¼ 25 (cm).
Such an experiment is useful if it is envisaged that in the application
this kind of level swings can occur.
8.1.3
Parameter estimation
Suppose that all unknown parameters are gathered in one parameter
vector a. The discrete system equation is denoted by f(x, w, a). The
SYSTEM IDENTIFICATION
259

measurement system is modelled, as before, by z ¼ h(x,v). We then have
the sequence of measurements according to the following recursions:
zðiÞ ¼ h xðiÞ; vðiÞ
ð
Þ
xði þ 1Þ ¼ f xðiÞ; wðiÞ; a
ð
Þ
)
for
i ¼ 0; 1; . . . ; I  1
ð8:6Þ
I is the length of the sequence. x(0) is the initial condition (which may be
known or unknown). v(i) and w(i) are the measurement noise and the
process noise, respectively.
One possibility for estimating a is to process the sequence z(i) in batches.
For that purpose, we stack all measurement vectors to one I  N dimen-
sional vector, say Z. Equation (8.6) defines the conditional probability
density p(Zja). The stochastic nature of Z is due to the randomness of
w(i), v(i) and possibly x(0). Equation (8.6) shows how this randomness
propagates to Z. Once the conditional density p(Zja) has been settled, the
complete estimation machinery from Chapter 3 applies, thus providing the
optimal solution of a. Especially, maximum likelihood estimation is pop-
ular since (8.6) can be used to calculate the (log-)likelihood of a.
A numerical optimization procedure must provide the solution.
Working in batches soon becomes complicated due to the (often)
nonlinear nature of the relations involved in (8.6). Many alternative
0
500
1000
1500
0
5
10
15
20
25
(cm)
t(s)
h1
h2
measured levels
Figure 8.3
Experimental data obtained from the hydraulic system
260
STATE ESTIMATION IN PRACTICE

techniques have been developed. For linear systems, processing in the
frequency domain may be advantageous. Another possibility is to pro-
cess the measurements sequentially. The trick is to regard the parameters
as state vectors a(i). Static parameters do not change in time. So, the
corresponding state equation is a(i þ 1) ¼ a(i). Sometimes, it is useful to
allow slow variations in a(i). This is helpful in order to model drift
phenomena, but also to improve the convergence properties of the
procedure. A simple model would be a process that is similar to random
walk (Section 4.2.1): a(i þ 1) ¼ a(i) þ w(i). The white noise sequence
w(i) is the driving force for the changes. Its covariance matrix Cw should
be small in order to prevent a too wild behaviour of a(i).
Using this model, equation (8.6) transforms into:
xði þ 1Þ
aði þ 1Þ
"
#
¼
fðxðiÞ; wðiÞ; aðiÞÞ
aðiÞ þ wðiÞ
"
#
(state equation)
zðiÞ ¼ hðxðiÞ; vðiÞÞ
(measurement equation)
ð8:7Þ
The original state vector x(i) has been augmented with a(i). The new
state equation can be written as x(i þ 1) ¼ f(x(i),w(i), w(i)) with
x(i) ¼
def[ xT(i) aT(i) ]T. This opens the door to simultaneous online
estimation of both x(i) and a(i) using the techniques discussed in
Chapter 4. However, the new state function f() is nonlinear and for
online estimation we must resort to estimators that can handle these
nonlinearities, e.g. extended Kalman filtering (Section 4.2.2), or particle
filtering (Section 4.4).
Note that if w(i)  0, then a(i) is a random constant and (hopefully) its
estimate ^a(i) converges to a constant. If we allow w(i) to deviate from zero
by setting Cw to some (small) nonzero diagonal matrix, the estimator
becomes adaptive. It has the potential to keep track of parameters that drift.
Example 8.3
Parameter estimation for the hydraulic system
In order to estimate the parameters of the three models of the
hydraulic system using the data from the previous example the
following procedure was applied. First, a particle filter was executed
in order to get a rough indication of the magnitudes of the parameters.
Figure 8.4(a) shows the results of the filter applied to the Torricelli
model. In this model, there are two parameters A1 and A2 which were
both initiated with a uniform distribution between 0 and 2(cm2). The
parameters were modelled with A(i þ 1) ¼ A(i) þ !(i) where !(i) is
white noise with a standard deviation of 0:004(cm2).
SYSTEM IDENTIFICATION
261

The second stage of the estimation procedure is a refinement of the
parameters based on maximum likelihood estimation. Equation (8.6)
was used to numerically evaluate the log-likelihood as a function of
the parameters. A normal distribution of v(i) was assumed. Therefore,
instead of the log-likelihood we can equivalently well calculate the
sum of squared Mahalanobis distances:
Jð^xð0Þ; aÞ ¼
X
I
i¼0
ðzðiÞ  ^xðiÞÞTC1
v ðzðiÞ  ^xðiÞÞ
with: ^xði þ 1Þ ¼ fð^xðiÞ; aÞ for i ¼ 0; 1; . . . ; I  1
ð8:8Þ
0
500
1000
1500
0
5
10
15
20
25
(cm)
t (s)
levels according to
Torricelli’s model
0
500
1000
1500
0
5
10
15
20
25
(cm)
t (s)
levels according to
4th order linear model
0
500
1000
1500
0
5
10
15
20
25
(cm)
t (s)
levels according to
2nd order linear model
0
500
1000
1500
0
0.3
0.6
(cm2)
t (s)
Estimated areas in
Torricelli’s model
A1
A2
(b)
(a)
(c)
(d)
Figure 8.4
Results of the estimation of the parameters of the hydraulic models. The
dotted lines are the measurements. The solid lines are results from the model
262
STATE ESTIMATION IN PRACTICE

The
minimalization
of
J(^x(0), a)
using
the
MATLAB
function
fminsearch from the Optimization Toolbox gives the final result.
The numerical optimization is initiated with the parameters obtained
from particle filtering.
Figures 8.4(b), (c) and (d) show the estimated levels ^x(i) obtained
with minimal J() for the three considered models.
8.1.4
Evaluation and model selection
The last step is the evaluation of the candidate models and the final
selection. For that purpose, we select a quality measure and evaluate and
compare the various models. Popular quality measures are the log-like-
lihood and the sum of squares of the residuals (the difference between
measurements and model-predicted measurements).
Models with a low order are preferable because the risk of over-
fitting is minimized and the estimators are less sensitive to estima-
tion errors of the parameters. Therefore, it might be beneficial to
consider not only the model with the best quality measure, but also
the models which score slightly less, but with a lower order. In order
to evaluate these models, other tests are useful. Ideally, the cross
correlation between the residuals and the input signal is zero. If not,
there is still part of the behaviour of the system that is not explained
by the model.
Example 8.4
Model selection for the hydraulic system
Qualitative evaluation of the three models using the log-likelihood as
a quality measure (or equivalently, the sum of squared Mahalanobis
distances) yields the following results:
Torricelli’s model:
J ¼ 4875
Second order linear model:
J ¼ 281140
Fourth order linear model:
J ¼ 62595
Here, there is wide gap between the nonlinear Torricelli’s model and
the two linear models. Yet, the quality measure J ¼ 4875 is still too
large to ascribe it fully to the measurement noise. Without the model-
ling errors, the mean value of the sum of squared Mahalanobis dis-
tances (I ¼ 300, N ¼ 2) is 600. Inspection of Figure 8.4(b) reveals
that the feet of the curves cause the discrepancy. Perhaps the Torricelli
model should be extended with a small linear friction term.
SYSTEM IDENTIFICATION
263

8.1.5
Identification of linear systems with a random input
There is a rich literature devoted to the problem of explaining a random
sequence x(i) by means of a linear system driven by white noise (Box,
1976). An example of such a model is the autoregressive model intro-
duced in Section 4.2.1. The Mth order AR model is:
xðiÞ ¼
X
M
n¼1
nxði  nÞ þ wðiÞ
ð8:9Þ
This type of model is easily cast into a state space model. As such it can
be used to describe non-white process noise. More general schemes are
the autoregressive moving average (ARMA) models and the autoregres-
sive integrating moving average (ARIMA) models. The discussion here is
only introductory and is restricted to AR models. For a full treatment we
refer to the pertinent literature.
The identification of an AR model from an observed sequence x(i)
boils down to the determination of the order M, and the estimation of
the parameters n and 2
w. Assuming that the system is in the steady
state, the estimation can be done by solving the Yule–Walker equations.
These
equations
arise
if
we
multiply
(8.9)
on
both
sides
by
x(i  1), . . . , x(i  M), and take expectations:
E xðiÞxði  kÞ
½
 ¼
X
M
n¼1
nE xði  nÞxði  kÞ
½

þ E½wðiÞxði  kÞ
for
k ¼ 1; . . . ; M
ð8:10Þ
Since E[x(i  k)w(i)] ¼ 0 and rk¼
defE[x(i)(i  k)]/2
x, equation (8.10)
defines the following systems of linear relations (see also equation
(4.21)):
r1
r2
r3
..
.
rM
2
66666664
3
77777775
¼
1
r1
r2
  
  
rM1
r1
1
r1
r2
  
rM2
r2
r1
1
r1
  
rM3
..
.
..
.
..
.
..
.
rM1
rM2
  
  
1
2
66666664
3
77777775
1
2
3
M
2
6666664
3
7777775
ð8:11Þ
264
STATE ESTIMATION IN PRACTICE

The parameters n are found by estimating the correlation coefficients rk
and solving (8.11).
The parameter 2
w is obtained by multiplying (8.9) by x(i) and taking
expectations:
2
x ¼
X
M
n¼1
an2
xrk þ 2
w
ð8:12Þ
Estimation of 2
x and solving (8.12) gives us the estimate of 2
w.
The order of the system can be retrieved by a concept called the
partial autocorrelation function. Suppose that an AR sequence x(i) has
been observed with unknown order M. The procedure for the identifi-
cation of this sequence is to first estimate the correlation coefficients rk
yielding estimates ^rk. Then, for a number of hypothesized orders
^M ¼ 1, 2, 3, . . . we estimate the AR coefficients ^k, ^M for k ¼ 1, . . . , ^M
(the subscript ^M has been added to discriminate between coefficients of
different orders). From these coefficients, the last one of each sequence,
i.e. ^ ^M, ^M, is called the partial autocorrelation function. It can be
proven that:
 ^M; ^M ¼ 0
for
^M > M
ð8:13Þ
Thus, the order M is determined by checking where ^ ^M, ^M drops down to
near zero.
Example 8.5
AR model of a pseudorandom binary sequence
Figure 8.5(a) shows a realization of a zero mean, pseudorandom
binary sequence. A discrete Markov model, given in terms of transi-
tion probabilities (see Section 4.3.1), would be an appropriate model
for this type of signal. However, sometimes it is useful to describe the
sequence with a linear, AR model. This occurs, for instance, when the
sequence is an observation of process noise in an (otherwise) linear
plant. The application of a Kalman filter requires the availability of a
linear model, and thus the process noise must be modelled as an AR
process.
Figure 8.5(b) shows the partial autocorrelation function obtained
from a registration of x(i) consisting of 4000 samples (Figure 8.5(a)
only shows the first 500 samples). The plot has been made using
MATLAB’s function aryule from the Signal Processing Toolbox.
Clearly, the partial autocorrelation function drops down at ^M ¼ 2,
SYSTEM IDENTIFICATION
265

so the estimated order is ^M ¼ 1, i.e. the best AR model is of first
order. Figure 8.5(c) shows a realization of such a process.
8.2
OBSERVABILITY, CONTROLLABILITY AND
STABILITY
8.2.1
Observability
We consider a deterministic linear system:
xði þ 1Þ ¼ FðiÞxðiÞ þ LðiÞuðiÞ
zðiÞ ¼ HðiÞxðiÞ
ð8:14Þ
The system is called observable if with known F(i), L(i)u(i) and H(i) the
state x(i) (with fixed i) can be solved from a sequence z(i), z(i þ 1), . . . of
measurements. The system is called completely observable if it is observ-
able for any i. In the following, we assume L(i)u(i) ¼ 0. This is without
any loss of generality since the influence to z(i), z(i þ 1), . . . of a L(i)u(i)
not being zero can be neutralized easily. Hence, the observability of a
system solely depends on F(i) and H(i).
0
100
200
300
400
500
–1
–0.5
0
0.5
1
observed signal
i
0
1
2
3
4
5
–0.2
0
0.2
0.4
0.6
0.8
1
1.2
M
partial autocorrelation function
0
100
200
300
400
500
–1
–0.5
0
0.5
1 a realization of the corresponding AR process
i
(a)
(c)
(b)
Figure 8.5
Modelling a pseudorandom binary signal by an AR process
266
STATE ESTIMATION IN PRACTICE

An approach to find out whether the system is observable is to con-
struct the observability Gramian (Bar-Shalom and Li, 1993). From (8.14):
zðiÞ
zði þ 1Þ
zði þ 2Þ
...
2
66664
3
77775
¼
HðiÞxðiÞ
Hði þ 1Þxði þ 1Þ
Hði þ 2Þxði þ 2Þ
...
2
66664
3
77775
¼
HðiÞ
Hði þ 1ÞFðiÞ
Hði þ 2ÞFðiÞFði þ 1Þ
...
2
66664
3
77775
xðiÞ ð8:15Þ
Equation (8.15) is of the type z ¼ Hx. The least squares estimate is
^x ¼ (HTH)1HTz. See Section 3.3.1. The solution exists if and only if
the inverse of HTH exists, or in other words, if the rank of HTH is equal
to the dimension M of the state vector. Equivalent conditions are that
HTH is positive definite (i.e. yTHTHy > 0 for every y 6¼ 0), or that the
eigenvalues of HTH are all positive. See Appendix B.5.
Translated to the present case, the requirement is that for at least one
n  0 the observability Gramian G, defined by:
G ¼ HTðiÞHðiÞ þ
X
n
j¼1
Hði þ jÞ
Y
j1
k¼0
Fði þ kÞ
 
!T
Hði þ jÞ
Y
j1
k¼0
Fði þ kÞ
 
!
ð8:16Þ
has rank equal to M. Equivalently we check whether the Gramian is
positive definite. For time-invariant systems, F and H do not depend on
time, and the Gramian simplifies to:
G ¼
X
n
j¼0
HFj

T HFj


ð8:17Þ
If the system F is stable (the magnitude of all eigenvalues of F are less
than one), we can set n ! 1 to check whether the system is observable.
A second approach to determine the observability of a time-invariant,
deterministic system is to construct the observability matrix:
M ¼
H
HF
HF2
...
HFM1
2
666664
3
777775
ð8:18Þ
OBSERVABILITY, CONTROLLABILITY AND STABILITY
267

According
to
(8.15),
x(i)
can
be
retrieved
from
a
sequence
z(i), . . . , z(i þ M  1) if M is invertible; that is, if the rank of M
equals M.
The advantage of using the observability Gramian instead of the
observability matrix is that the former is more stable. Modelling errors
and round-off errors in the coefficients in both F and H could make the
difference between an invertible G or M and a noninvertible one. How-
ever, G is less prone to small errors than M is.
A more quantitative measure of the observability is obtained by using
the eigenvalues of the Gramian G. A suitable measure is the ratio
between the smallest eigenvalue and the largest eigenvalue. The system
is less observable as this ratio tends to zero. A likewise result can be
obtained by using the singular values of the matrix M (see singular value
decomposition in Appendix B.6).
Example 8.6
Observability of a second order system
Consider the system (F, H) given by:
F ¼
0:66
0:12
0:32
0:74
"
#
H ¼
1
3
1
4


The rank of both the Gramian G and the observability matrix M
appear to be one, indicating that the system is not observable.
However, if the coefficients of F and H are represented in single
precision IEEE floating point format, the relative round-off error is
in the order of 108. These round-off errors cause the ratio of
eigenvalues of G to be in the order of 1016 instead of zero. The
comparable ratio of singular values of M is in the order of 109.
Clearly, both ratios indicate that the observability is poor. However,
the one of M is overoptimistic. In fact, if MATLAB’s function rank()
is applied to G and M, the former returns one, while the latter
(erroneously) yields two. The corresponding MATLAB code, given in
Listing 8.1, uses functions from the Control System Toolbox. Espe-
cially, the function ss() is of interest. It creates a state space
model, i.e. a special structure array containing all the matrices of
a linear time-invariant system.
268
STATE ESTIMATION IN PRACTICE

Listing 8.1
Two methods of obtaining an observability measure of a linear time-
invariant system.
F ¼ [0.66 0.12; 0.32 0.74];
% Define the system
H ¼ [1/3 1/4];
Fs ¼ double(single(F));
% Round-off to 32 bits
Hs ¼ double(single(H));
B ¼ [1; 0]; D ¼ 0;
sys ¼ ss(Fs,B,Hs,D,1);
% Create state-space model
M ¼ obsv(Fs,Hs);
% Get observability matrix
G ¼ gram(sys,’o’);
% and Gramian
eigG ¼ eig(G);
% Calculate eigenvalues
svdM ¼ svd(M);
% and singular values
disp(‘ratio of eigenvalues of Gramian:’);
min(eigG)/max(eigG)
disp(‘ratio of singular values of observability matrix:’);
min(svdM)/max(svdM)
The concept of observability can also be extended such that the influence
of measurement noise is incorporated. The stochastic observability has a
strong connection with a particular implementation of the Kalman filter,
known as the information filter. The details of this extension will follow
in Section 8.3.3.
8.2.2
Controllability
In control theory, the concept of controllability usually refers to the
ability that for any state x(i) at a given time i a finite input sequence
u(i), u(i þ 1), . . . , u(i þ n  1) exists that can drive the system to an arbi-
trary final state x(i þ n). If this is possible for any time, the system is
called completely controllable.1 As with observability, the controllability
of a system can be revealed by checking the rank of a Gramian. For
time-invariant systems, the controllability can also be analysed by means
of the controllability matrix. This matrix arises from the following
equation:
1 Some authors use the word ‘reachability’ instead, and reserve the word ‘controllability’ for a
system that can be driven to zero (but not necessarily to an arbitrary state). See A˚ mstro¨m and
Wittenmark (1990).
OBSERVABILITY, CONTROLLABILITY AND STABILITY
269

xði þ 1Þ ¼ FxðiÞ þ LuðiÞ
xði þ 2Þ ¼ F2xðiÞ þ FLuðiÞ þ Luði þ 1Þ
xði þ 3Þ ¼ F3xðiÞ þ F2LuðiÞ þ FLuði þ 1Þ þ Luði þ 2Þ
..
.
xði þ nÞ ¼ F nxðiÞ þ
X
n1
j¼0
F jLuði þ jÞ
ð8:19Þ
or:
L
FL
. . .
Fn1L


uðiÞ
uði þ 1Þ
...
uði þ n  1Þ
2
6664
3
7775 ¼ xði þ nÞ  FnxðiÞ
ð8:20Þ
The minimum number of steps, n, is at most equal to M, the dimension
of the state vector. Therefore, in order to test the controllability of the
system (F, L) it suffices to check whether the controllability matrix
[ L
FL
. . .
FM1L ] has rank M.
The MATLAB functions for creating the controllability matrix and
Gramian are ctrb() and gram(), respectively.
8.2.3
Dynamic stability and steady state solutions
The term stability refers to the ability of a system to resist to and recover
from disturbances acting on this system. A state estimator has to face
three different causes of instabilities: sensor instability, numerical
instability and dynamic instability.
Apart from the usual sensor noise and sensor linearity errors, a sensor
may produce unusual glitches and other errors caused by hard to predict
phenomena, such as radio interference, magnetic interference, thermal
drift, mechanical shocks and so on. This kind of behaviour is sometimes
denoted by sensor instability. Its early detection can be done using
consistency checks (to be discussed in Section 8.4).
Numerical instabilities originate from round-off errors. Particularly,
the inversion of a near singular matrix may cause large errors due to its
270
STATE ESTIMATION IN PRACTICE

sensitivity to round-off errors. A careful implementation of the design
must prevent these errors. See Section 8.3.
The third cause for instability lies in the dynamics of the state
estimator itself. In order to study the dynamic stability of the state
estimator it is necessary to consider the estimator as a dynamic system
with as inputs the measurements z(i) and the control vectors u(i). See
Appendix D. The output consists of the estimates x(iji). In linear
systems, the stability does not depend on the input sequences. For the
stability analysis it suffices to assume zero z(i) and u(i). The equations
of interest are derived from (8.2):
xði þ 1 i þ 1
j
Þ ¼ I  KðiÞHðiÞ
ð
ÞFði  1ÞxðijiÞ
ð8:21Þ
with:
KðiÞ ¼ PðiÞHTðiÞ HðiÞPðiÞHTðiÞ þ CvðiÞ

1
P(i) is the covariance matrix C(i þ 1ji) of the predicted state x(i þ 1ji).
P(i) is recursively defined by the discrete Ricatti equation:
Pði þ 1Þ ¼ FðiÞPðiÞFTðiÞ þ CwðiÞ
 FðiÞPðiÞHTðiÞ HðiÞPðiÞHTðiÞ

þ CvðiÞÞ1HðiÞPTðiÞFTðiÞ
ð8:22Þ
The recursion starts with P(0) ¼
defCx(0). The first term in (8.22) repre-
sents the absorption of uncertainty due to the dynamics of the system
during each time step (provided that F is stable; otherwise it represents
the growth of uncertainty). The second term represents the additional
uncertainty at each time step due to the process noise. The last term
represents the reduction of uncertainty thanks to the measurements.
For the stability analysis of a Kalman filter it is of interest to know
whether a sequence of process noise, w(i), can influence each element of
the state vector independently. The answer to this question is found by
writing
the
covariance
matrix
Cw(i)
of
the
process
noise
as
Cw(i) ¼ G(i)GT(i). Here, G(i) can be obtained by an eigenvalue/
eigenvector diagonalization of Cw(i). That is Cw(i) ¼ Vw(i)w(i)VT
w(i)
and G(i) ¼ Vw(i)1/2
w (i). See Appendix B.5 and C.3. w(i) is a K  K
diagonal matrix where K is the number of nonzero eigenvalues of
OBSERVABILITY, CONTROLLABILITY AND STABILITY
271

Cw(i). The matrices G(i) and Vw(i) are M  K matrices. The introduction
of G(i) allows us to write the system equation as:
xði þ 1Þ ¼ FðiÞxðiÞ þ LðiÞuðiÞ þ GðiÞ~wðiÞ
ð8:23Þ
where ~w(i) is a K dimensional Gaussian white noise vector with covari-
ance matrix I. The noise sequence w(i) influences each element of x(i)
independently if the system (F(i), G(i)) is controllable by the sequence
~w(i). We then have the following theorem (Bar-Shalom, 1993), which
provides a sufficient (but not a necessary) condition for stability:
If a time invariant system (F, H) is completely observable and the
system (F, G) is completely controllable, then for any initial condition
P(0) ¼ Cx(0) the solution of the Ricatti equation converges to a
unique, finite, invertible steady state covariance matrix P(1).
The observability of the system assures that the sequence of measure-
ments contains information about the complete state vector. Therefore,
the observability guarantees that the prediction covariance matrix P(1)
is bounded from above. Consequently, the steady state Kalman filter is
BIBO stable. See Appendix D.3.2. If one or more state elements are not
observable, the Kalman gains for these elements will be zero, and the
estimation of these elements is purely prediction driven (model based
without using measurements). If the system F is unstable for these elem-
ents, then so is the Kalman filter.
The controllability with respect ~w(i) assures that P(1) is unique ( does
not depend on Cx(0)), and that its inverse exists. If the system is not
controllable, then P(1) might depend on the specific choice of Cx(0). If
for the non-controllable state elements the system F is stable, then the
corresponding eigenvalues of P(1) are zero. Consequently, the Kalman
gains for these elements are zero, and the estimation of these elements is
again purely prediction driven.
The discrete algebraic Ricatti equation, P(i þ 1) ¼ P(i), mentioned in
Chapter 4, provides the steady state solution for the Ricatti equation:
P ¼ FPFT þ Cw  FPHT HPHT þ Cv

1HPTFT
ð8:24Þ
The steady state is reached due to the balance between the growth of
uncertainty (the second term and possibly the first term) and the reduc-
tion of uncertainty (the third term and possibly the first term).
272
STATE ESTIMATION IN PRACTICE

A steady state solution of the Ricatti equation gives rise to a constant
Kalman gain. The corresponding Kalman filter, derived from (8.2) and (8.21):
xði þ 1ji þ 1Þ ¼ I  KH
ð
ÞF xðijiÞ þ I  KH
ð
ÞLuðiÞ þ Kzði þ 1Þ
ð8:25Þ
with:
KðiÞ ¼ PHT HPHT þ Cv

1
becomes time invariant.
Example 8.7
Stability of a system that is not observable
Consider the system (F, H, Cw, Cv) given by:
F ¼
0:66
0:32
0:12
0:74


H ¼ 1:2
2:4
½

Cw ¼
1
0
0
1


Cv ¼ ½1
This system is controllable, but not observable. Yet, the Ricatti equa-
tion is asymptotically stable with steady state solution:
P ¼
1:3110
0:0822
0:0822
1:1293


and eigenvalues 1.0976 and 1.3427
The eigenvalues of the corresponding steady state Kalman filter, i.e.
the eigenvalues of (I  KH)F, are 0.5 and 0.101. Thus the Kalman
filter is stable.
Listing 8.2 provides the MATLAB code for this example. The func-
tion dlqe() returns the Kalman gain matrix together with the predic-
tion covariance, the error covariance and the eigenvalues of the
Kalman filter. Alternatively, we use the function kalman() that cre-
ates the estimator as a state space model. Internally, it uses the
function dare() which solves the discrete algebraic Ricatti equation.
The functions are from the Control System Toolbox.
The explanation for the stability in the last example is as follows.
Suppose that the measurements are switched off, that is, K ¼ 0. In that
case,
the
estimates
just
follow
the
dynamics
of
the
system:
x(i þ 1ji þ 1) ¼ F x(iji) þ Lu(i). Since F is stable, the initial uncertainty
Cx(0) will be absorbed. In the steady state, the final uncertainty is given
by the balance:
Cxð1Þ ¼ FCxð1ÞFT þ Cw
ð8:26Þ
OBSERVABILITY, CONTROLLABILITY AND STABILITY
273

(the discrete Lyapunov equation). Since the Kalman filter is optimal, the
steady state solution of the Ricatti equation is bounded from above by
Cx(1), and thus asymptotical stable. The MATLAB function dlyap()
returns the solution of the discrete Lyapunov equation.
Example 8.8
Stability of a system that is not observable (continued)
A further inspection of the situation confirms the statement made
above. The solution of the discrete Lyapunov equation is:
Cxð1Þ ¼
4:0893
2:3094
2:3094
3:2472


with eigenvalues 1.32 and 6.02
Indeed, P  Cx(1) (meaning that the difference Cx(1)  P is positive
semidefinite; i.e. possesses only non-negative eigenvalues). The eigen-
values of F are 0.5 and 0.9. The eigenvalue of 0.5 corresponds to the
state in the diagonalized system (Appendix D.3.1) that is not observed
by the measurements. However, this state is stable. The Kalman gain
for this state is zero, and thus the steady state Kalman filter copies this
eigenvalue.
If the second eigenvalue of F is increased from 0.9 to, say, 1.5, the
system is not stable anymore, nevertheless the steady state solution of
the Ricatti equation exists. The corresponding Kalman filter is stable.
However, if the first eigenvalue of F is increased from 0.5 to 1.5, the
system is again not stable. But this time, the corresponding Kalman
filter isn’t stable either. The Ricatti equation is not stable anymore.
Example 8.9
Stability of a system that is not controllable
Consider the system (F, H, Cw, Cv) given by:
F ¼ 0:66
0:32
0:12
0:74


Cw ¼
0:1111
0:0833
0:0833
0:0625


H ¼ ½ 1
1 
Cv ¼ ½1
This system is observable. The covariance matrix of the process noise
can be written as: Cw ¼ GGT with GT ¼ [ 0:333
0:25 ]. The system
(F, G) is not controllable as a simple test can show. The steady state
solution of the Ricatti equation is:
274
STATE ESTIMATION IN PRACTICE

P ¼
0:2166
0:1624
0:1624
0:1218


with eigenvalues 0 and 0:338
P is not invertible. The eigenvalues of F are 0.9 and 0.5. The Kalman
filter, with eigenvalues 0.5411 and 0.5, is stable.
The explanation of this behaviour is as follows. The diagonalized
system has one controllable state (corresponding to an eigenvalue of
0.9). For this state, the Kalman filter behaves regularly. The second
state (with eigenvalue 0.5) is not controllable. This state is not
affected by the process noise. It is a stable state, and thus, the initial
uncertainty fades out. The zero variance of this state causes a zero
eigenvalue in Cx(1): With that, the Kalman gain for that state also
becomes zero because without uncertainty there is no need for
measurements. Consequently, the eigenvalue of the system repeats
itself in the Kalman filter. The zero eigenvalue of Cx(1) causes a
corresponding zero eigenvalue in P. Thus, this matrix is not invertible.
If the second eigenvalue of F is increased from 0.5 to 1.5, the initial
condition Cx(0) influences the long term behaviour of Cx(i). If
Cx(0) ¼ 0, then Cx(i) converges to a constant. But this solution is
not stable. A small perturbation of Cx(0) causes Cx(i) to diverge to
infinity. Small perturbations of Cx(0) trigger P(i) to follow quite
different trajectories, but they finally converge to a nonzero steady
state for which the Kalman filter is stable.
The last example shows that if a system (F, G) is not controllable,
some eigenvalues of the prediction covariance matrix may become zero.
The matrix P is positive semidefinite. Such a situation does not contrib-
ute to the numerical stability.
Listing 8.2
Steady state solution of a system that is not observable.
lambda ¼ diag([0.9 0.5]);
% Define a system with
V ¼ [1/3 1; 1/4 1/2];
% eigenvalues 0.9 and 0.5
F ¼ V*lambda*inv(V);
H ¼ inv(V); H(2,:) ¼ [];
% Define a measurement matrix
% that only observes one state
Cv ¼ eye(1); Cw ¼ eye(2);
% Define covariance matrices
% Discrete steady state Kalman filter:
[M,P,Z,E] ¼ dlqe(F,eye(2),H,Cw,Cv);
Cx_inf ¼ dlyap(F,Cw)
% Solution of discrete Lyapunov equation
OBSERVABILITY, CONTROLLABILITY AND STABILITY
275

disp(‘Kalman gain matrix’);
disp(M);
disp(‘Eigenval. of Kalman filter’);
disp(E);
disp(‘Error covariance’);
disp(Z);
disp(‘Prediction covariance’);
disp(P);
disp(‘Eigenval. of prediction covariance’);
disp(eig(P));
disp(‘Solution of discrete Lyapunov equation’);
disp(Cx_inf);
disp(‘Eigenval. of sol.
of discrete Lyapunov eq.’);
disp(eig
(Cx_inf));
8.3
COMPUTATIONAL ISSUES
A straightforward implementation of the time-variant Kalman filter may
result in too large estimation errors. The magnitudes of these errors are
not compatible with the error covariance matrices. The filter may even
completely diverge even though theoretically the filter should be stable.
This anomalous behaviour is due to a number representation with
limited precisions. In order to find where round-off errors have the
largest impact it is instructive to rephrase the Kalman equations in
(8.2) as follows:
Ricatti loop:
CðijiÞ ¼ Cðiji  1Þ  Cðiji  1ÞHTðHCðiji  1ÞHT þCvÞ1 HCðiji  1Þ
Cði þ 1jiÞ ¼ FCðijiÞFT þ Cw
#
KðiÞ ¼ Cðiji  1ÞHTS1ðiÞ
ð8:27Þ
SðiÞ ¼ HCðiji  1ÞHT þ Cv
#
estimation loop:
xðijiÞ ¼ xðiji  1Þ þ KðiÞðzðiÞ  Hxðiji  1ÞÞ
xði þ 1jiÞ ¼ F xðijiÞ þ LuðiÞ
For simplicity, the system (F, L, H) is written without the time index.
276
STATE ESTIMATION IN PRACTICE

As can be seen, the filter consists of two loops. If the Kalman filter is
stable, the second loop (the estimation loop) is usually not that sensitive
to round-off errors. Possible induced errors are filtered in much the same
way as the measurement noise. However, the loop depends on the
Kalman gains K(i). Large errors in K(i) may cause the filter to become
unstable. These gains come from the first loop.
In the first loop (the Ricatti loop) the prediction covariance matrix
P(i) ¼ C(i þ 1ji) is recursively calculated. As can be seen, the recursion
involves nonlinear matrix operations including a matrix inversion. Espe-
cially, the representation of these matrices (and through this the Kalman
gain) may be sensitive to the effects of round-off errors.
The sensitivity to round-off errors becomes apparent if an eigenvalue–
eigenvector decomposition (Appendix B.5 and C.3.2) is applied to the
covariance matrix P:
P ¼
X
M
m¼1
mvmvT
m
ð8:28Þ
m are the eigenvalues of P and vm the corresponding eigenvectors. The
eigenvalues of a properly behaving covariance matrix are all positive (the
matrix is positive definite and non-singular). However, the range of the
eigenvalues may be very large. This finds expression in the condition
number
max/min
of
the
matrix.
Here,
max ¼ max (m)
and
min ¼ min (m). A large condition number indicates that if the matrix
is inverted, the propagation of round-off errors will be large:
P1 ¼
X
M
m¼1
1
m
vmvT
m
ð8:29Þ
In a floating point representation of P, the exponents are largely deter-
mined by max. Therefore, the round-off error in min is proportional to
max, and may be severe. It will result in large errors in 1/min.
Another operation with a large sensitivity to round-off errors is the
subtraction of two similar matrices.
These errors can result in a loss of symmetry in the covariance
matrices and a loss of positive definiteness. In some cases, the eigenval-
ues of the covariance matrices can even become negative. If this occurs,
the errors may accumulate during each recursion, and the process may
diverge.
COMPUTATIONAL ISSUES
277

As an example, consider the following Ricatti loop:
SðiÞ ¼ HCðiji  1ÞHT þ Cv
KðiÞ ¼ ðHCðiji  1ÞÞTS1ðiÞ
CðijiÞ ¼ Cðiji  1Þ  KðiÞHCðiji  1Þ
Cði þ 1jiÞ ¼ FCðijiÞFT þ Cw
ð8:30Þ
This loop is mathematically equivalent to (8.2), but is computationally
less expensive because the factor HC(iji  1) appears at several places
and can be reused. However, the form is prone to round-off errors:
. The part K(i)H can easily introduce asymmetries in C(iji).
. The subtraction I  K(i)H can introduce negative eigenvalues.
The implementation should be used cautiously. The preferred implemen-
tation is (8.2):
CðijiÞ ¼ Cðiji  1Þ  KðiÞSðiÞKTðiÞ
ð8:31Þ
It requires more operations than expression (8.30), but it is more
balanced. Therefore, the risk of introducing asymmetries is much lower.
Still, this implementation does not guarantee non-negative eigenvalues.
Listing 8.3 implements the Kalman filter using (8.31). The functions
acquire_measurement_vector() and get_control_vector()
are placeholders for the interfacing to the measurement system and a
possible control system. These functions should also take care of the
timing of the estimation process. The required number of operations
(using MATLAB’s standard matrix arithmetic) of the update step is about
2M2N þ 3MN2 þ N3. The number of the operations of the prediction
step is about 2M3.
Listing 8.3
Conventional time variant Kalman filter for a time invariant system.
load linsys;
% Load a system: F,H,Cw,Cv,Cx0,x0,L
C ¼ Cx0;
% Initialize prior uncertainty
xprediction ¼ x0;
% and prior mean
while (1)
% Endless loop
% Update:
S ¼ H*C*H’ þ Cv;
% Innovation matrix
K ¼ C*H’*S^1;
% Kalman gain matrix
278
STATE ESTIMATION IN PRACTICE

z ¼ acquire_measurement_vector();
innovation ¼ z - H*xprediction;
xestimation ¼ xprediction þ K*innovation;
C ¼ C - K*S*K’;
% Prediction:
u ¼ get_control_vector();
xprediction ¼ F*xestimation þ L*u;
C ¼ F*C*F’ þ Cw;
end
Example 8.10
Numerical instability of the conventional Kalman filter
Consider the system (F, H, Cw, Cv, Cx(0)) given by:
F ¼
0:9698
0:1434
0:1101
0:1245
0:9547
0:2557
0:1312
0:2455
0:9557
2
64
3
75
Cxð0Þ ¼
100
0
0
0
100
0
0
0
100
2
64
3
75
Cw ¼ AAT with A¼
8:1174
3:9092
4:3388
2
64
3
75
H ¼
0:0812
0:0391
0:0434
0:4339
0:9010
0
0:3909 0:1883 0:9010
2
64
3
75
Cv ¼
106
0
0
0
106
0
0
0
106
2
64
3
75
The eigenvalues of F are 0:999 exp (  0:1j), 0:999 exp (0:1j) and 0.98
where j ¼
ﬃﬃﬃﬃﬃﬃﬃ
1
p
. The magnitude of the first two eigenvalues are close to unit,
andtherefore, the systemis just stable. Adiagonalizationofthesystem would
reveal that the process noise only affects the state that corresponds with the
eigenvalue 0.98. Hence, the system is not controllable by the process noise.
The measurement matrix is such that it measures the diagonalized states.
An implementation of (8.30) for this system yields results as shown in
Figure 8.6. The results are obtained by means of a simulation of the
system and a 32-bit IEEE floating point implementation of the Kalman
filter (including the Ricatti loop). The relative round-off errors are of the
order 108. The Kalman filter appears to be unstable due to the negative
eigenvalues of the covariance matrix. The same implementation using
64-bit double precision (relative error is around 1016) yields stable results.
Application of the code in Listing 8.3, but performed with 32 bit precision
gives the results shown in Figure 8.7. Although the filter is stable now, it
still doesn’t work properly.
COMPUTATIONAL ISSUES
279

The symmetry of a matrix P can be enforced by the assignment:
P : ¼ (P þ PT)/2. The addition of this type of statement at some critical
locations in the code helps a little, but often not enough. The true
remedy is to use a proper implementation combined with sufficient
number precision. The remaining part of this section discusses a number
of different implementations.
8.3.1
The linear-Gaussian MMSE form
In Section 3.1.3 we derived the MMSE estimator for static variables in
the linear-Gaussian case. In section 3.1.5, the unbiased linear MMSE
estimator was derived. Since the MMSE solution, expressed in equation
(3.20), appeared to be linear and unbiased, the conclusion was drawn
that this solution is identical to the unbiased linear MMSE solution
(given in (3.33) and (3.45)). However, the two solutions have different
forms. We denote the first solution by the (linear-Gaussian) MMSE
form. The second solution is the Kalman form. The equivalence between
the two solutions can be shown by using the matrix inversion lemma,
(b.10).
0
5
10
15
20
25
30
35
–10
–5
0
5 measurements
0
5
10
15
20
25
30
35
–2000
0
2000
4000 estimation errors
i
0
5
10
15
20
25
30
35
–3
–2
–1
0
1
2
3
x 10–6 minimum eigenvalue of C(i|i)
i
Figure 8.6
Results of a computationally efficient implementation of the conven-
tional Kalman filter. The filter is unstable due to an eigenvalue of P that remains
negative
280
STATE ESTIMATION IN PRACTICE

As a result of all this, the Kalman filter can also be expressed in two
forms. The following implementation is based on the MMSE form, i.e.
equation (3.20):
update :
CðijiÞ ¼ C1ðiji  1Þ þ HTC1
v H

1
(error covariance
matrixÞ
xðijiÞ ¼ CðijiÞ C1ðiji  1Þxðiji  1Þ þ HTC1
v zðiÞ


(updated estimate)
ð8:32Þ
The Kalman form, given in (8.2), requires the inversion of S(i), an N  N
matrix. The MMSE form requires the inversion of C(iji  1) and
C1(i  1ji) þ HTC 1
v H; both are M  M matrices. It also requires the
inversion of Cv, but this can be done outside the Ricatti loop. Besides, Cv
is often a diagonal matrix (uncorrelated measurement noise), whose
inversion is without problems. The situation where Cv is not invertible
is a degenerated case. One or more measurements are completely correl-
ated with other measurements. Such a measurement can be removed
without loss of information.
In the time-invariant case, the calculation of the term HTC1
v H,
appearing in (8.32), can be kept outside the loop. If so, the number of
operations is 2M3 þ M2 þ M. Thus, if there are many measurements and
0
50
100
150
200
–10
–5
0
5
10
15
measurements
0
50
100
150
200
–4
–2
0
2
4
estimation errors
i
0
50
100
150
200
8
–6
–4
2
0
2 x 10–4 minimum eigenvalue of C(i|i)
i
Figure 8.7
Results of a balanced implementation of the conventional Kalman filter.
The filter is stable, but still an eigenvalue of C(iji) is sometimes negative
COMPUTATIONAL ISSUES
281

only a few states, i.e. N  M, the a priori form might be favourable. But
in other cases, the Kalman form is often preferred.
Example 8.11
The linear-Gaussian MMSE form
Application of the MMSE form to the same system and data as in
Example 8.10 yields the results as shown in Figure 8.8 (using 32-bit
floating point number representations). At first sight, there seems
nothing wrong with these results. However, the sudden change of
the smallest eigenvalue at i ¼ 2, from which point on it remains
constant, is reason to become suspicious.
8.3.2
Sequential processing of the measurements
The conventional Kalman filter processes the measurement data in blocks.
The data zn(i) of the sensors available at time i are collected in the
measurement vector z(i), and processed as one unit. Another possibility
is to process the individual measurements sequentially. A requirement is
that the measurement noise is uncorrelated. Cv must be a diagonal matrix.
If not, the measurement vector must be decorrelated first using techniques
as described in Appendix C.3.1 (Kaminski et al., 1971). In the following
algorithm, the diagonal elements of Cv are denoted by 2
n. The row vector
hn stands for the n-th row of H. Thus, HT ¼ [ hT
0
. . .
hT
N1 ].
0
50
100
150
200
–10
–5
0
5
10
15
measurements
0
50
100
150
200
–0.04
–0.02
0
0.02
0.04
estimation errors
i
0
50
100
150
200
6
6.5
7
7.5
8
8.5
9 x 10–7 minimum eigenvalue of C(i|i)
i
Figure 8.8
Results of the Kalman filter implemented in the MMSE form
282
STATE ESTIMATION IN PRACTICE

Algorithm 8.1: Sequential update
1. Initialization:
.
xði; 0Þ ¼
def xðiji  1Þ
.
Cði; 0Þ ¼
def Cðiji  1Þ
2. Sequential update:
For n ¼ 0, 1, 2, . . . , N  1:
.
sði; nÞ ¼ hnCði; nÞhT
n þ 2
n
(innovation variance)
.
kn ¼ Cði; nÞhT
n
sði; nÞ
(Kalman gain vector)
.
xði;nþ1Þ ¼ xði;nÞþknðznðiÞhnxði;nÞÞ(update of the estimate)
.
Cði;nþ1Þ ¼ Cði;nÞCði;nÞhT
n hnCði;nÞ
sði;nÞ
(update of the covariance)
3. Closure:
.
xðijiÞ ¼ xði; nÞ
.
CðijiÞ ¼ Cði; nÞ
The number of required operations is about 2M2N þ 2MN. It outper-
forms the conventional Kalman filter. However, the subtraction that is
needed to get C(i, n þ 1) can introduce negative eigenvalues. Conse-
quently, the algorithm is still sensitive to round-off errors.
Example 8.12
Sequential processing
The results obtained by the application of sequential processing of the
measurements to the problem from Example 8.10 is shown in
Figure 8.9. Negative eigenvalues are not prevented, and the filter does
not behave correctly.
8.3.3
The information filter
In the conventional Kalman filter it is difficult to represent a situation in
which no knowledge is available about (a subspace of) the state. It would
COMPUTATIONAL ISSUES
283

require that some eigenvalues of the corresponding error covariance
matrix would be infinity. The concept of an information matrix circum-
vents this problem.
An information matrix is the inverse of a covariance matrix. If one or
more eigenvalues of an information matrix are small, then a subspace
exists in which the uncertainty of the random vector is large. In fact, if
one or more eigenvalues are zero, then no knowledge exists about the
random vector in the subspace spanned by the corresponding eigenvec-
tors. In this situation, the covariance matrix does not exist because the
information matrix is not invertible.
The information filter is an implementation of the Kalman filter in
which the Ricatti loop is entirely expressed in terms of information
matrices (Grewal and Andrews, 2001). Let Y(ijj) be the information
matrix corresponding to C(ijj). Thus:
YðijjÞ¼
defC1ðijjÞ
ð8:33Þ
Using (8.32), the update in the Kalman filter is rewritten as:
update:
YðijiÞ ¼ Yðiji1ÞþHTC1
v H
(information matrix)
xðijiÞ ¼ Y1ðijiÞðYðiji1Þxðiji1ÞþHTC1
v zðiÞÞ
(updated estimate)
ð8:34Þ
0
50
100
150
200
–10
–5
0
5
10
15
measurements
0
50
100
150
200
–0.2
–0.1
0
0.1
0.2
estimation errors
i
0
50
100
150
200
–12
–10
–8
–6
–4
–2
0
2
x 10–6 minimum eigenvalue of C(i|i)
i
Figure 8.9
Sequential processing of the measurements
284
STATE ESTIMATION IN PRACTICE

The computational cost of the update of the information matrix is only
determined by the inversion of Y(iji) (in the time-invariant case) because
the term HTC1
v H is constant and can be kept outside the loop. The
number of required operations is about M3 þ 1
2 M2 þ 1
2 M.
In order to develop the expression of the predicted state information
matrix, the covariance of the process noise is factored as follows
Cw ¼ GGT. As mentioned in Section 8.2.3, such a factorization is
obtained by an eigenvector–eigenvalue decomposition Cw ¼ VwwVT
w.
The diagonal elements of w contain the eigenvalues of Cw. If some of
the eigenvalues are zero, we remove the rows and columns in which
these zero eigenvalues appear. Also, the corresponding columns in Vw
are removed. Suppose that the number of nonzero eigenvalues is K,
then w becomes a K  K matrix and Vw an M  K matrix. Conse-
quently, G ¼ Vw1/2
w is also an M  K matrix.
Furthermore, we define the matrix A(i) as:
AðiÞ¼
def F1

TYðijiÞF1
ð8:35Þ
In other words, A1(i) ¼ FC(iji)FT is the predicted covariance matrix in
the absence of process noise. Here, we have silently assumed that the
matrix F is invertible (which is the case if the time-discrete system is an
approximation of a time-continuous system).
The information matrix of the predicted state follows from (8.2):
Yði þ 1jiÞ ¼ ðFCðijiÞFT þ GGTÞ1
¼ ðA1ðiÞ þ GGTÞ1
(predicted state information)
ð8:36Þ
Using the matrix inversion lemma, this expression can be moulded in the
easier to implement form:
Yði þ 1jiÞ ¼ AðiÞ  AðiÞGðGTAðiÞG þ IÞ1GTAðiÞ
ð8:37Þ
This completes the Ricatti loop. The number of required operations of
(8.36) and (8.37) is 2M3 þ 2M2K þ 2MK2 þ K3.
As mentioned above, the information filter can represent the situ-
ation where no information about some states is available. Typically,
this occurs at the initialization of the filter, if no prior knowledge is
COMPUTATIONAL ISSUES
285

available. However, the information matrix cannot represent a situa-
tion where states are known precisely, i.e. without uncertainty.
Typically, such a situation occurs when the system (F, G) is not
controllable.
The information filter also offers the possibility to define a stochastic
observability criterion. For that purpose, consider the system without
process noise. In that case, Y(i þ 1ji) ¼ (F1)TY(iji)F1. Starting with
complete uncertainty, i.e. Y(0j  1) ¼ 0, the prediction information at
time i is found by iterative application of (8.34):
Yðiji  1Þ ¼
X
i
j¼0
ðF1ÞT

	j
HTC1
v H F1

j
ð8:38Þ
Note the similarity of this expression with the observability Grammian
given in (8.17). The only difference is the information matrix C1
v
which
weighs the importance of the measurements. Clearly, if for some i all
eigenvalues of Y(iji  1) are positive, then the measurements have pro-
vided information to all states.
Example 8.13
The information filter
The results obtained by the information filter are shown in Figure
8.10. The negative eigenvalues of C(iji) ¼ Y1(iji) indicate that the
filter is not robust with respect to round-off errors.
0
50
100
150
200
–10
–5
0
5
10
15
measurements
0
50
100
150
200
–0.04
–0.02
0
0.02
0.04
estimation errors
i
0
50
100
150
200
–500
–400
–300
–200
–100
0
100
minimum eigenvalue of C(i |i )
i
Figure 8.10
Results from the information filter
286
STATE ESTIMATION IN PRACTICE

8.3.4
Square root filtering
The square root of a square matrix P is a matrix A such that P ¼ AA.
Sometimes the matrix B that satisfies P ¼ BTB is also called a square
root, but strictly speaking such a B is a Cholesky factor, and not a square
root. Anyway, square roots, Cholesky factors and other factorizations
are useful matrix decomposition methods that enable stable implemen-
tations of the Kalman filter. The principal idea in square root filtering is
to decompose a covariance matrix P as P ¼ BTB (or likewise), and to use
B as a representation of P. This effectively doubles the precision of the
number representation. The various factorization methods lead to
various forms of square root filtering.
This section describes one particular implementation of a square root
filter, the Potter implementation (Potter and Stern, 1963). It uses:
. Triangular Cholesky factorization of the error covariance matrices.
. Sequentially processing of the measurements using symmetric ele-
mentary matrices.
. QR factorization for the prediction.
The update in Potter’s square root filter
We will represent the error covariance matrix C(ijj) by an upper triangu-
lar matrix B(ijj) where:
CðijjÞ ¼ BTðijjÞBðijjÞ
ð8:39Þ
An upper triangular matrix is a matrix with all elements below the
diagonal equal to zero, e.g.
B ¼
b00
b01
b02
  
0
b11
b12
0
0
b22
0
0
0
..
.
2
6664
3
7775
ð8:40Þ
The update formula, as expressed in (8.27):
CðijiÞ ¼ Cðiji  1Þ  Cðiji  1ÞHT HCðiji  1ÞHT þ Cv

1HCðiji  1Þ
COMPUTATIONAL ISSUES
287

turns into:
BðijiÞTBðijiÞ ¼ BTðiji  1ÞBðiji  1Þ  BTðijÞBðiji  1ÞHT
HBTðiji  1ÞBðiji  1ÞHT

þCvÞ1HBTðiji  1ÞBðiji  1Þ
¼ BTðiji  1ÞBðiji  1Þ  BTðiji  1ÞM
MTM þ Cv

1MTBðiji  1Þ
¼ BTðiji  1Þ I  M MTM þ Cv

1MT

	
Bðiji  1Þ
ð8:41Þ
where M ¼
defB(iji  1)HT is an M  N matrix.
If we would succeed in finding a Cholesky factor of:
I  M MTM þ Cv

1MT
ð8:42Þ
then we would have an update formula entirely expressed in Cholesky
factors. Unfortunately, in the general case it is difficult to find such a
factor.
For the special case of only one measurement, N ¼ 1, a factorization
is within reach. If N ¼ 1, then H becomes a row vector h. The matrix M
becomes an M dimensional (column) vector m ¼ B(iji  1)hT. Substitu-
tion in (8.42) yields:
I  m mTm þ 2
v

1mT
ð8:43Þ
2
v is the variance of the measurement noise.
Expression (8.43) is of the form I  mmT with  ¼ 1/(kmk2 þ 2
v).
Such a form is called a symmetric elementary matrix. The form can be
easily factored by:
I  mmT ¼ I  mmT

T I  mmT


with  ¼
1 	
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1   m
k
k2
q
m
k
k2
¼
1
m
k
k2
1 	
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2
v
m
k
k2þ2
v
s
 
!
ð8:44Þ
Substitution of (8.44) in (8.41) yields:
BðijiÞTBðijiÞ ¼ BTðiji  1Þ I  mmT

T I  mmT


Bðiji  1Þ
ð8:45Þ
288
STATE ESTIMATION IN PRACTICE

This gives us finally the update formula in terms of Cholesky factors:
square root filtering update :
BðijiÞ ¼ I  mmT


Bðiji  1Þ
with
 ¼
1
m
k
k2
1 þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2
v
m
k
k2þ2
v
s
 
!
ð8:46Þ
The general solution, when more measurements are available, is
obtained by sequentially processing them in exactly the same way as
discussed in Section 8.3.2.
The prediction in Potter’s square root filter
The prediction step C(i þ 1ji) ¼ FC(iji)FT þ Cw can be written as a
product of Cholesky factors:
Cði þ 1jiÞ ¼ FCðijiÞFT þ Cw ¼
C
1
2w

	T
jFBTðijiÞ


C
1
2w
BðijiÞFT
"
#
ð8:47Þ
where C
1
2w is the square root of Cw. Thus, if we define the 2M  M matrix
A ¼
def[(C
1
2w)TjFBT(iji)]T, then the prediction covariance matrix is found as
C(i þ 1ji) ¼ ATA.
A QR factorization of a matrix A (not necessarily square) produces an
orthonormal matrix Q and an upper triangular matrix R such that A ¼ QR.
The matrices Q and R have compatible dimensions. Such a factorization is
what we are looking for because, since Q is orthonormal (QTQ ¼ I), we
have: ATA ¼ RTR. The procedure to get B(iji þ 1) simply boils down to
constructing the matrix A, and then performing a QR factorization.
An implementation of Potter’s square root filter is given in Listing 8.4.
MATLAB provides two functions for a triangular Cholesky factorization.
We need such a function only at the initialization of the filter to get a
factorization of Cx(0) and Cw. The function chol() is applicable to
positive definite matrices. In our case, both matrices can have zero
eigenvalues. The cholinc() can also handle positive semidefinite
matrices, but is only applicable to sparse matrices. The functions
sparse() and full() take care of the conversions. Note that cho-
linc() returns a matrix whose number of rows, in principle, equals the
number of nonzero eigenvalues. Thus, possibly, the matrix should be
filled up with zeros to obtain an M  M matrix.
COMPUTATIONAL ISSUES
289

The QR factorization in the prediction step is taken care of by qr().
The size of the resulting matrix is (M þ K)  M where K is the number of
rows in SqCw. Only the first M rows carry the needed information and
the remaining rows are deleted.
The update of the algorithm requires N(M3 þ 3M2 þ M) operations.
The number of operations of the prediction is determined partly by qr ().
This function requires about 1
2 M3 þ M2K operations. In full, the predic-
tion requires 3
2 M3 þ M2K operations.
Listing 8.4
Potter’s implementation in MATLAB.
load linsys
% Load a system:
[B,p] ¼ cholinc(sparse(Cx0),‘0’);
% Initialize squared
B ¼ full(B); B(p:M,:) ¼ 0;
% prior uncertainty
x_est ¼ x0;
% and prior mean
[SqCw,p] ¼ cholinc(sparse(Cw),‘0’);
% Squared Cw
SqCw ¼ full(SqCw);
while (1)
% Endless loop
z ¼ acquire_measurement_vector();
% 1. Sequential update:
for n ¼ 1:N
% For all measurements . . .
m ¼ B*H(n,:)’;
% get row vector from H
norm_m ¼ m’*m;
S ¼ norm_m þ Cv(n,n);
% innovation variance
K ¼ B’*m/S;
% Kalman gain vector
inno ¼ z(n)  H(n,:)*x_est;
x_est ¼ x_est þ K*inno;
% update estimate
beta ¼ (1 þ sqrt(Cv(n,n)/S))/norm_m;
B ¼ (eye(M)-beta*m*m’)*B;
% covariance update
end
% 2. Prediction:
u ¼ get_control_vector();
x_est ¼ F*x_est þ L*u;
% Predict the state
A ¼ [SqCw; B*F’];
% Create block matrix
[q,B] ¼ qr(A);
% QR factorization
B ¼ B(1:M,1:M);
% Delete irrelevant part
end
Example 8.14
Square root filtering
The results obtained by Potter’s square root filter are shown in
Figure 8.11. The double logarithmic plot of the minimum eigen-
value of P ¼ C(iji) shows that the eigenvalue is of the order O(i1).
This is exactly according to the expectations since this eigenvalue
relates to a state without process noise. Thus, the variance of the
290
STATE ESTIMATION IN PRACTICE

estimation error should be inversely proportional to the number of
observations.
8.3.5
Comparison
In the preceding sections, five different implementations are discussed
which are all mathematically equivalent. However, different implemen-
tations have different sensitivities to round-off errors and different com-
putational cost.
Table 8.1 provides an overview of the cost expressed in the number
of operations required for a single iteration. The table assumes a time
invariant system so that terms like HTC1
v H can be reused. Furthermore,
the numbers are based on a straightforward MATLAB implementation with-
out optimization with respect to computational cost. Special code that
exploits the symmetry of covariance matrices can lower the number of
operations a little. The computational efficiency of the square root filter
can be improved by consistently maintaining the triangular structure of the
matrices. (In the current implementation, the triangular structure is lost
during the update, but is regained by the QR factorization.)
A quantitative, general analysis of the sensitivities of the various
implementations to round-off errors is difficult. However, Table 8.1
gives an indication. The table shows the results of an experiment that
relates to the system described in Example 8.10. For each implementation,
Table 8.1
Comparison of different implementation
Computational
cost
update
Computational
cost
prediction
Required
no. of
digits
Conventional
Kalman filter
2M2N þ 3MN2 þ N3
2M3
12
MMSE form
2M3 þ M2 þ M
2M3
13
Sequential
processing
of measurements
2M2N þ 2MN
2M3
12
Information filter
M3 þ 1
2 M2 þ 1
2 M
2M3 þ 2M2K þ 2MK2 þ K3
11
Potter’s square
root filter
N(M3 þ 3M2 þ M)
3
2 M3 þ M2K
5
M ¼ number of states.
N ¼ number of measurements.
K ¼ effective dimension of process noise vector.
COMPUTATIONAL ISSUES
291

the number of digits needed for the number representations of the vari-
ables (including intermediate results) was established, in order to have a
stable and consistent result.
As expected, the square root filter is the most numerical stable
method, but it is also the most expensive one. Square root filtering
should be considered:
. If other implementations result in covariance matrices with nega-
tive eigenvalues.
. If other implementations involve matrix inversions where the
inverse condition number, i.e. min/max, of the matrix is in the
same magnitude of the round-off errors.
The MMSE form is inexpensive if the number of measurements is large
relative to the number of states, i.e. if N >> M. The sequentially
processing method is inexpensive, especially when both N and M are
large.
8.4
CONSISTENCY CHECKS
The purpose of this section is to provide some tools that enable the
designer to check whether his design behaves consistently (Bar-Shalom
and Birmiwal, 1983). As discussed in the previous sections, the two main
0
50
100
150
200
–10
–5
0
5
10
15 measurements
0
50
100
150
200
–0.04
–0.02
0
0.02
0.04 estimation errors
i
100
101
102
10–9
10–8
10–7
10–6
minimum eigenvalue of C(i |i )
i
Figure 8.11
Results from Potter’s square root filter
292
STATE ESTIMATION IN PRACTICE

reasons why a realized filter does not behave correctly are modelling
errors and numerical instabilities of the filter.
Estimators are related with three types of error variances:
. The minimal variances that would be obtained with the most
appropriate model.
. The actual variances of the estimation errors of some given estima-
tor.
. The variances indicated by the calculated error covariance matrix
of a given estimator.
Of course, the purpose is to find an estimator whose error variances
equal the first one. Since we do not know whether our model approaches
the most appropriate one, we do not have the guarantee that our design
approaches the minimal attainable variance. However, if we have
reached the optimal solution, then the actual variances of the estimation
errors must coincide with the calculated variances. Such a correspondence
between actual and calculated variances is a necessary condition for an
optimal filter, but not a sufficient one.
Unfortunately, we need to know the real estimation errors in order to
check whether the two variances coincide. This is only possible if the
physical process is (temporarily) provided with additional instruments that
give us reference values of the states. Usually such provisions are costly. This
section discusses checks that can be accomplished without reference values.
8.4.1
Orthogonality properties
We recall that the update step of the Kalman filter is formed by the
following operations (8.2):
^zðiÞ ¼ HðiÞxðiji  1Þ
ðpredicted measurementÞ
~zðiÞ ¼ zðiÞ  ^zðiÞ
ðinnovationsÞ
xði ij Þ ¼ xðiji  1Þ þ KðiÞ~zðiÞ
ðupdated estimateÞ
ð8:48Þ
The vectors ~z(i) are the innovations (residuals). In linear-Gaussian sys-
tems, these vectors are zero mean with the innovation matrix as covari-
ance matrix:
SðiÞ ¼ HðiÞCðiji  1ÞHTðiÞ þ CvðiÞ
ð8:49Þ
CONSISTENCY CHECKS
293

Furthermore, let e(ijj) ¼
defx(i)  x(ijj) be the estimation error of the esti-
mate x(ijj). We then have the following properties:
E½eðijjÞzTðmÞ ¼ 0
m  j
E½eðijjÞxTðnjmÞ ¼ 0
m  j
E½~zðiÞ~zTðjÞ ¼ ði; jÞSðiÞ
i:e: ~zðiÞ is white
ð8:50Þ
These properties follow from the principle of orthogonality. In the static
case, any unbiased linear MMSE satisfies:
E½eðz  zÞT ¼ E½ðx  Kz  ðx  KzÞÞðz  zÞT ¼ Cxz  KCz ¼ 0
ð8:51Þ
The last step follows from K ¼ CxzC1
z . See (3.29). The principle of
orthogonality, E[eT(z  z)] ¼ 0, simply states that the covariance
between any component of the error and any measurement is zero.
Adopting an inner product definition for two random variables em and
zn as (em, zm) ¼
defCov[em, zn], the principle can be expressed as e?z.
Since x(ijj) is an unbiased linear MMSE estimate, it is a linear function
of the set fx(0), Z(j)g ¼ fx(0), z(0), z(1), . . . , z(j)g. According to the prin-
ciple of orthogonality, we have e(ijj)?Z(j). Therefore, e(ijj) must also be
orthogonal to any z(m) m  j, because z(m) is a subspace of Z(j). This
proves the first statement in (8.50).
In addition, x(njm), m  j, is a linear combination of Z(m). Therefore,
it is also orthogonal to e(ijj), which proves the second statement.
The whiteness property of the innovations follows from the following
argument.
Suppose
j < i.
We
may
write:
E[~z(i)~zT(j)] ¼ E[E[~z(i)
~zT(j)jZ(j)]]. In the inner expectation, the measurements Z(j) are known.
Since ~z(j) is a linear combination of Z(j),~z(j) is non-random. It can be
taken outside the inner expectation: E[~z(i)~zT(j)] ¼ E[E[~z(i)jZ(j)]~zT(j)].
However, E[~z(i)jZ(j)] must be zero because the predicted measurements
are unbiased estimates of the true measurements. If E[~z(i)jZ(j)] ¼ 0, then
E[~z(i)~zT(j)] ¼ 0 (unless i ¼ j).
8.4.2
Normalized errors
The NEES (normalized estimation error squared) is a test signal defined as:
NeesðiÞ ¼ eTðijiÞC1ðijiÞeðiÞ
ð8:52Þ
294
STATE ESTIMATION IN PRACTICE

In the linear-Gaussian case, the NEES has a 2
M distribution (chi-square
with M degrees of freedom; see Appendix C.1.5).
The 2
M distribution of the NEES follows from the following
argument. Since the state estimator is unbiased, E[e(iji)] ¼ 0. The
covariance matrix of e(iji) is C(iji). Suppose A(i) is a symmetric
matrix such that A(i)AT(i) ¼ C1(iji). Application of A(i) to e(i) will
give a random vector y(i) ¼ A(i)e(i). The covariance matrix of y(i) is:
A(i)C(iji)AT(i) ¼ A(i)(A(i)AT(i))1AT(i) ¼ I. Thus, the components of
y(i) are uncorrelated, and have unit variance. If both the process
noise and the measurement noise are normally distributed, then so
is y(i). Hence, the inner product yT(i)y(i) ¼ eT(i)C1(iji)e(i) is the sum
of
M
squared,
independent
random
variables,
each
normally
distributed with zero mean and unit variance. Such a sum has a 2
M
distribution.
The NIS (normalized innovation squared) is a test signal defined as:
NisðiÞ ¼ ~zTðiÞS1ðiÞ~zðiÞ
ð8:53Þ
In the linear-Gaussian case, the NIS has a 2
N distribution. This follows
readily from the same argument as used above.
Example 8.15
Normalized errors of second order system
Consider the following system:
F ¼
0:999 cosð0:1Þ
0:999 sinð0:1Þ
0:999 sinð0:1Þ
0:999 cosð0:1Þ


H ¼ 0
0:5
½

Cw ¼
1
0
0
0


Cv ¼ ½1
Cxð0Þ ¼
0:01
0
0
0:01


E½xð0Þ ¼
0
0


ð8:54Þ
Figure 8.12 shows the results of a MATLAB realization of this system
consisting of states and measurements. Application of the discrete
Kalman filter yields estimated states and innovations. From that, the
NEES and the NIS are calculated.
In this case, M ¼ 2 and N ¼ 1. Thus, the NEES and the NIS should
obey the statistics of a 2
2 and a 2
1 distribution. The 95% percentiles
of these distributions are 5.99 and 3.84, respectively. Thus, about
95% of the samples should be below these percentiles, and about 5%
above. Figure 8.12 affirms this.
CONSISTENCY CHECKS
295

8.4.3
Consistency checks
From equation (8.50) and the properties of the NEES and the NIS, the
following statement holds true:
If a state estimator for a linear-Gaussian system is optimal, then:
. The sequence Nees(i) must be 2
M distributed.
. The sequence Nis(i) must be 2
N distributed.
. The sequence ~z(i) (innovations) must be white.
Consistency checks of a ’state estimator-under-test’ can be performed by
collecting the three sequences and by applying statistical tests to see
whether the three conditions are fulfilled. If one or more of these condi-
tions are not satisfied, then we may conclude that the estimator is not
optimal.
In a real design, the NEES test is only possible if the true states are
known. As mentioned above, such is the case when the system is
0
50
100
–10
0
10
first state
0
50
100
–10
0
10
second state
0
50
100
–10
–5
0
5
10
i
measurements
true
estimate
0
50
100
–5
0
5
error first state
0
50
100
–5
0
5
error second state
0
50
100
–4
–2
0
2
4
innovations
i
0
50
100
0
2
4
6
8
nis
i
0
50
100
0
5
10
nees
0
50
100
0
5
10
periodogram
k
Figure 8.12
Innovations and normalized errors of a state estimator for a second
order system
296
STATE ESTIMATION IN PRACTICE

(temporarily) provided with extra measurement equipment that meas-
ures the states directly and with sufficient accuracy so that their outputs
can be used as a reference. Usually, such measurement devices are too
expensive and the designer has to rely on the other two tests. However,
the NEES test is applicable in simulations of the physical process. It can
be used to see if the design is very sensitive to changes in the parameters
of the model. The other two tests use the innovations. Since the innova-
tions are always available, even in a real design, these are of more
practical significance.
In order to check the hypothesis that a data set obeys a given distribu-
tion,
we
have
to
apply
a
distribution
test.
Examples
are
the
Kolmogorov–Smirnov test and the Chi-square test (Section 9.3.3). Often,
instead of these rigorous tests a quick procedure suffices to give an
indication. We simply determine an interval in which, say, 95% of the
samples
must
be.
Such
an
interval
[A,B]
is
defined
by
F(B)  F(A) ¼ 0:95. Here, F() is the probability distribution of the
random variable under test. If an appreciable part of the data set is
outside this interval, then the hypothesis is rejected. For chi-square
distributions there is the one-sided 95% acceptance boundary with
A ¼ 0, and B such that F2
Dof (B) ¼ 0:95. Sometimes the two-sided 95%
acceptance boundaries are used, defined by F2
Dof (A) ¼ 0:025 and
F2
Dof (B) ¼ 0:975. Table 8.2 gives values of A and B for various degrees
of freedom (Dof).
For state estimators that have reached the steady state, S(i) is constant,
and the whiteness property of the innovations implies that the power
spectrum of any element of ~z(i) must be flat.2 Suppose that ~zn(i) is an
element of ~z(i). The variance of ~zn(i), denoted by 2
n, is the n-th diagonal
element in S. The discrete Fourier transform of ~zn(i), calculated over
i ¼ 0, . . . , I  1, is:
~ZnðkÞ ¼
X
I1
i¼0
~znðiÞej2ki=I
k ¼ 0; . . . ; I  1
j ¼
ﬃﬃﬃﬃﬃﬃﬃ
1
p
ð8:55Þ
The periodogram of ~zn(i), defined here as Pn(k) ¼ j~Zn(k)j2/I, is an esti-
mate of the power spectrum (Blackman and Tukey, 1958). If the power
spectrum is flat, then E[Pn(k)] ¼ 2
n. It can be proven that in that case the
2 For estimators that are not in the steady state, the innovations have to be premultiplied by
S1/2(i) so that the covariance matrix of the resulting sequence becomes constant.
CONSISTENCY CHECKS
297

variables 2Pn(k)/2
n is 2
2 distributed for all k (except for k ¼ 0). Hence,
the whiteness of ~zn(i) is tested by checking whether 2Pn(k)/2
n is 2
2
distributed.
Example 8.16
Consistency checks applied to a second order system
The results of the estimator discussed in Example 8.15 and presented
in Figure 8.12 pass the consistency checks successfully. Both the
NEES and the NIS are about 95% of the time below the one-sided
acceptance boundaries, i.e. below 5.99 and 3.84. The figure also
shows the normalized periodogram calculated as 2Pn(k)/^ 2
1 with ^ 2
1
the estimated variance of the innovation. The normalized periodo-
gram shown seems to comply with the theoretical 2
2 distribution.
Example 8.17
Consistency checks applied to a slightly mismatched
filter
Figure 8.13 shows the results of a state estimator that is applied to the
same data as used in Example 8.15. However, the model the estimator
uses differs slightly. The real system matrix F of the generating pro-
cess and the system matrix Ffilter on which the design of the state
estimator is based are as follows:
F ¼
0:999 cosð0:1Þ
0:999 sinð0:1Þ
0:999 sinð0:1Þ
0:999 cosð0:1Þ


Ffilter ¼
0:999 cosð0:116Þ
0:999 sinð0:116Þ
0:999 sinð0:116Þ
0:999 cosð0:116Þ


Apart from that, the model used by the state estimator exactly
matches the real system.
In this example, the design does not pass the whiteness test of
the innovations. The peak of the periodogram at k ¼ 6 is above 20.
Table 8.2
95% Acceptance boundaries for 2
Dof distributions
One-sided
Two-sided
Dof
A
B
A
B
1
0
3.841
0.0010
5.024
2
0
5.991
0.0506
7.378
3
0
7.815
0.2158
9.348
4
0
9.488
0.4844
11.14
5
0
11.07
0.8312
12.83
298
STATE ESTIMATION IN PRACTICE

For a 2
2 distribution such a high value is unlikely to occur. (In fact,
the chance is smaller than 1 to 20 000.)
8.4.4
Fudging
If one or more of the consistency checks fail, then somewhere a serious
modelling error has occurred. The designer has to step back to an earlier
stage of the design in order to identify the fault. The problem can be
caused anywhere, from inaccurate modelling during the system identifi-
cation to improper implementations. If the system is nonlinear and the
extended Kalman filter is applied, problems may arise due to the neglect
of higher order terms of the Taylor series expansion.
A heuristic method to catch the errors that arise due to approxima-
tions of the model is to deliberately increase the modelled process noise
(Bar-Shalom and Li, 1993). One way to do so is by increasing the
0
50
100
–10
0
10
first state
0
50
100
–10
0
10
second state
0
50
100
–10
–5
0
5
10
i
measurements
true
estimate
0
50
100
–5
0
5
error first state
0
50
100
–5
0
5
error second state
0
50
100
–4
–2
0
2
4 innovations
i
0
50
100
0
5
10
nis
i
0
50
100
0
5
10
15
nees
0
50
100
0
10
20
30
periodogram
k
Figure 8.13
Innovations and normalized errors of a state estimator based on a
slightly mismatched model
CONSISTENCY CHECKS
299

covariance matrix Cw of the process noise by means of a fudge factor 	.
For instance, we simply replace Cw by Cw þ 	I. Other methods to
regulate a covariance matrix are discussed in Section 5.2.3. Instead of
adapting Cw we can also increase the diagonal of the prediction covari-
ance C(i þ 1ji) by some factor. The fudge factor should be selected such
that the consistency checks now pass as successful as possible.
Fudging effectuates a decreaseoffaith in the modelofthe process, andthus
causes a larger impact of the measurements. However, modelling errors give
rise to deviations that are autocorrelated and not independent from the
states. Thus, these deviations are not accurately modelled by white noise.
The designer should maintain a critical attitude with respect to fudging.
8.5
EXTENSIONS OF THE KALMAN FILTER
The extensions considered in this section make the Kalman filter applic-
able to a wider class of problems. In particular, we discuss extensions to
cover non-white and cross-correlated noise sequences. Also, the topic of
offline estimation will be introduced.
8.5.1
Autocorrelated noise
The Kalman filter considered so far assumes white uncorrelated random
sequences w(i) and v(i) for the process and measurement noise. What do
we do if these assumptions do not hold in practice?
The case of autocorrelated noise is usually tackled by assuming a state
space model for the noise. For instance,3 autocorrelated process noise is
represented by w(i þ 1) ¼ Fww(i) þ ~w(i) where ~w(i) is a white noise
sequence with covariance matrix C~w. State augmentation reduces the
problem to a standard form. For that, the state vector is extended by w(i):
xði þ 1Þ
wði þ 1Þ
"
#
¼
F
I
0
Fw
"
#
xðiÞ
wðiÞ
"
#
þ
0
I
"
#
~wðiÞ
zðiÞ ¼ H
0
½

xðiÞ
wðiÞ
"
#
þ vðiÞ
ð8:56Þ
3 For convenience of notation the time index of matrices (for time variant systems) is omitted in
this section.
300
STATE ESTIMATION IN PRACTICE

The new, augmented state vector can be estimated using the standard
Kalman filter.
The case of autocorrelated measurement noise is handled in the same
way although some complications may occur (Bryson and Hendrikson,
1965).
Suppose
that
the
measurement
noise
is
modelled
by
v(i þ 1) ¼ Fvv(i) þ ~v(i) with ~v(i) a white noise sequence with covariance
C~v. State augmentation brings the following model:
xði þ 1Þ
vði þ 1Þ


¼
F
0
0
Fv

 xðiÞ
vðiÞ


þ
I
0


wðiÞ þ
0
I


~vðiÞ
zðiÞ ¼ H
I
½
 xðiÞ
vðiÞ


ð8:57Þ
The process noise of the new system consists of two terms. Taken
together, the corresponding covariance matrix is [ I
0 ]TCw[ I
0 ]þ
[ 0
I ]TC~v[ 0
I ].
In the new, augmented system there is no measurement noise. The
corresponding covariance matrix is zero. This is not necessarily a pro-
blem because the standard Kalman filter does not use the inverse of the
measurement covariance matrix. However, the computation of the Kal-
man gain matrix does require the inverse of HC(iji  1)HT þ Cv. If in
this expression Cv ¼ 0, the feasibility of calculating K depends on the
invertibility of HC(iji  1)HT. The statement Cv ¼ 0 implies that some
linear combinations of the state vector is known without any uncer-
tainty. If in this subspace of the state space no process noise is active,
ultimately HC(iji  1)HT becomes near singular, and the filter becomes
unstable. The solution for this potential problem is to apply differencing.
Instead of using z(i) directly, we use the differenced measurements
y(i) ¼
defz(i)  Fvz(i  1) ¼ Hx(i)  FvHx(i  1) þ ~v(i  1).
Example 8.18
Suppression of 50 Hz emf interference
Consider a signal x(t) that is disturbed by a 50 (Hz) emf interference
caused, for instance, by an inductive crosstalk of an electric power
line. The bandwidth of the signal is B ¼ 10 (rad/s), and the sampling
period is  ¼ 1 (ms). We model the signal by a first order system
_x ¼ Bx þ w which in discrete time becomes x(i þ 1) ¼ (1  B)
x(i) þ w(i).
An inductive coupling with a power line induces an interfering
periodic waveform with a ground harmonic of f0 ¼ 50 (Hz). Usually,
such a waveform also contains a component with double frequency
EXTENSIONS OF THE KALMAN FILTER
301

(and possibly higher harmonics, but these will be neglected here). The
disturbance can be modelled by two second order equations, that is:
vði þ 1Þ ¼ d
cosð2f0Þ
sinð2f0Þ
sinð2f0Þ
cosð2f0Þ
0
0
cosð4f0Þ
sinð4f0Þ
sinð4f0Þ
cosð4f0Þ
2
6664
3
7775vðiÞ þ ~vðiÞ
ð8:58Þ
The factor d is selected close to one, modelling the fact that the
magnitudes of each component vary in time only slowly.
Application of the augmented state estimator to a simulation of
the process shows results as depicted in Figure 8.14. The Bode
diagram clearly shows that the state estimator acts as a double-notch
filter. The width of the notch depends on the choice of d. The Bode
diagram, valid for the steady state Kalman filter, is obtained with the
101
102
103
104
–100
–80
–60
–40
–20
0
Magnitude (dB)
0
0.1
0.2
0.3
–40
–20
0
20
40
state (thick) and measurement
t (sec)
0
0.1
0.2
0.3
–40
–20
0
20
40
estimate (thick) and measurement
t (sec)
0
0.1
0.2
0.3
–40
–20
0
20
40
estimation error
t (sec)
Bode diagram
frequency  (rad/sec)
Figure 8.14
Suppression of 50 Hz emf interference based on Kalman filtering
302
STATE ESTIMATION IN PRACTICE

following MATLAB fragment (making use of the Control System
Toolbox):
. . .
sys ¼ ss(Fn,eye(5),H,0,Ts);
% Create a state space model
[Kest,L,P,M,Z] ¼ kalman(sys,Wn,0);
% Find the steady state KF
bodemag(Kest(2,:),‘k’);
% Plot the Bode diagram
8.5.2
Cross-correlated noise
Another situation occurs when the process and measurement noise are
cross-correlated, that is, Cwv ¼ E[w(i)vT(i)] 6¼ 0. Such might happen if
both the physical process and measurement system is affected by the
same source of disturbance. Examples are changes of temperature and
electrical inference due to induction.
The strategy to bring this situation to the standard estimation problem
is to introduce a modified state equation by including a term T(z(i)
Hx(i)  v(i))  0:
xði þ 1Þ ¼ FxðiÞ þ wðiÞ
¼ FxðiÞ þ wðiÞ þ T zðiÞ  HxðiÞ  vðiÞ
ð
Þ
¼ F  TH
ð
ÞxðiÞ þ wðiÞ  TvðiÞ þ TzðiÞ
ð8:59Þ
The factor F  TH is the modified transition matrix. The terms
w(i)  Tv(i) are regarded as process noise. The term Tz(i) is known
and can be regarded as a control input.
Note that T can be selected arbitrarily because T(z(i)  Hx(i) v(i))  0.
Therefore, we can select T such that the new process noise becomes
uncorrelated
with
respect
to
the
measurement
noise.
That
is,
E[(w(i)  Tv(i))vT(i)] ¼ 0.
Or:
Cwv  TCv ¼ 0.
In
other
words,
if
T ¼ CwvC1
v , the modified state equation is in the standard form without
cross-correlation.
8.5.3
Smoothing
Up to now only online estimation of continuous states has been con-
sidered. The topic of prediction has been touched on in Section 4.2.1. Here,
we introduce the subject of offline estimation, generally referred to as
smoothing. Assuming a linear-Gaussian model of the process and
EXTENSIONS OF THE KALMAN FILTER
303

measurements, we have recorded a set of measurements ZK ¼ fz(0),
z(1), . . . , z(K)g. Using the prior knowledge E[x(0)] and Cx(0) we want
estimates for some points in time 0  k  K. We emphasize once again
that this section is only introductory. For details we refer to the pertinent
literature (Gelb et al., 1974).
The problem is often divided into three types of problems:
. Fixed interval smoothing: K is fixed. k is variable between 0 and K.
. Fixed point smoothing: k is fixed. K increases with time, K ¼ i.
. Fixed lag smoothing: both k and K increase with time, but with
K  k
fixed
to
the
so-called
lag¼
defK  k.
Thus,
K ¼ i
and
k ¼ i  lag.
Fixed interval smoothing is needed most often. The problem occurs
when an experiment has been done, the data has been acquired and
stored, and the data is analysed afterwards.
The general approach to smoothing is the same as for discrete states.
See Section 4.3.3. The estimation occurs in two passes. In the first pass,
the data is processed forward in time to yield estimates ^xf(k) ¼ x(kjk). In
the second pass, the data is processed backward in time. Starting with
k ¼ K we recursively estimate the previous states using only data from
the ‘future’. Thus, the backward estimate ^xb(k) only uses information
from ZK  Zkþ1 ¼ fz(k þ 1), . . . , z(K)g. For that, a ‘reversed time’
Kalman filter can be used. The final estimate x(kjK) is obtained by
optimally combining ^xf(k) and ^xb(k).
Although this approach yields the desired optimally smoothed states, it is
not computationally efficient. For each of the three differenttypes ofsmooth-
ing problems more efficient algorithms have been proposed. One of them is
the well-known Rauch–Tung–Striebel smoother (Rauch etal., 1965). The
algorithm implements a fixed interval smoother. It does not explicitly use
^xb(k). Instead, it uses recursively x(kjK). The algorithm is as follows:
Algorithm 8.2: Rauch–Tung–Striebel smoother
1. Apply the standard discrete Kalman filter to find the offline estimates
and store the results, that is, the estimates x(kjk), C(kjk), along with
the one-step-ahead predictions x(k þ 1jk), C(k þ 1jk).
2. For k ¼ K  1 looping back to k ¼ 0 with step ¼ 1:
2.1. A ¼ C(kjk)FTC1(k þ 1jk)
2.2. x(kjK) ¼ x(kjk) þ A(x(k þ 1jK)  x(k þ 1jk))
2.3. C(kjK) ¼ C(kjk) þ A(C(k þ 1jK)  C(k þ 1jk))AT
304
STATE ESTIMATION IN PRACTICE

Step 2.3 is only required if we are interested in the smoothing covariance
matrix.
Example 8.19
Estimation of a transient of an electrical RC circuit
Figure 8.15 shows an electric circuit consisting of a capacitor con-
nected by means of a switch to a resistor. The resistor represents the
input impedance of an AD converter that measures the voltage z. The
voltage across the capacitor is x. At t ¼ 0 the switch closes, giving rise
to a measured voltage z ¼ x þ v (v is regarded as sensor noise). The system
obeys the following state equation _x ¼ x/(RC) with RC ¼ 10 (ms).
R
C
v(t)
x(t)
z(t)
t = 0
Figure 8.15
The measurement of a transient in an electrical RC network
0
5
10
–20
–10
0
10
20
state (thick) and measurements
t (µs)
0
5
10
–20
–10
0
10
20
filtered state (thick) and measurements
0
5
10
–20
–10
0
10
20
smoothed state (thick) and measurements
0
5
10
–5
0
5
smooth error (thick) and filter error
t (µs)
t (µs)
t (µs)
Figure 8.16
Estimation of a transient by means of filtering and by smoothing
EXTENSIONS OF THE KALMAN FILTER
305

In
discrete
time this
becomes
x(i þ 1) ¼ (1  /(RC))x(i).
The
sampling period is  ¼ 0:1 (ms). K ¼ 100. No prior knowledge about
x(0) is available.
Figure 8.16 shows observed measurements along with the corres-
ponding estimated states and the result from the Rauch–Tung–Striebel
smoother. Clearly, the uncertainty of the offline obtained estimate
is much smaller than the uncertainty of the Kalman filtered result.
This holds true especially in the beginning where the online filter has
only a few measurements at its disposal. The offline estimator can
take advantage of all measurements.
8.6
REFERENCES
A˚ mstro¨m, K.J. and Wittenmark, B., Computer-Controlled Systems – Theory and Design,
second edition, Prentice Hall, Englewood Cliffs, NJ, 1990.
Bar-Shalom, Y. and Birmiwal, K., Consistency and robustness of PDAF for target track-
ing in cluttered environments, Automatica, 19, 431–7, July 1983.
Bar-Shalom, Y. and Li, X.R., Estimation and Tracking – Principles, Techniques, and
Software, Artech House, Boston, MA, 1993.
Blackman, R.B. and Tukey, J.W., The Measurement of Power Spectra, Dover, New York,
1958.
Box, G.E.P. and Jenkins, G.M., Time Series Analysis: Forecasting and Control, Holden-
Day, San Francisco, 1976.
Bryson, A.E. and Hendrikson, L.J., Estimation using sampled data containing sequen-
tially correlated noise, Journal of Spacecraft and Rockets, 5(6), 662–5, June 1968.
Eykhoff, P., System Identification, Parameter and State Estimation, Wiley, London,
1974.
Gelb, A., Kasper, J.F., Nash, R.A., Price, C.F. and Sutherland, A.A., Applied Optimal
Estimation, MIT Press, Cambridge, MA, 1974.
Grewal, M.S. and Andrews, A.P., Kalman Filtering – Theory and Practice Using MATLAB,
second edition, Wiley, New York, 2001.
Kaminski, P.G., Bryson, A.E. and Schmidt, J.F., Discrete square root filtering: a survey
of current techniques, IEEE Transactions on Automatic Control, 16(6), 727–36,
December 1971.
Ljung, L., System Identification – Theory for the User, 2nd edition, Prentice Hall, Upper
Saddle River, NJ, 1999.
Ljung, L. and Glad, T., Modeling of Dynamic Systems, Prentice Hall, Englewood Cliffs,
NJ, 1994.
Potter, J.E. and Stern, R.G., Statistical filtering of space navigation measurements,
Proceedings of AIAA Guidance and Control Conference, AIAA, New York, 1963.
Rauch, H.E., Tung, F. and Striebel, C.T., Maximum likelihood estimates of linear
dynamic systems, AIAA Journal, 3(8), 1445–50, August 1965.
So¨derstro¨m, T. and Stoica, P., System Identification, Prentice Hall, International,
London, 1989.
306
STATE ESTIMATION IN PRACTICE

8.7
EXERCISES
1. The observed sequence of data shown in Figure 8.17 is available in the data file
C8exercise1:mat. Use MATLAB to determine the smallest order of an autoregressive
modelthatis stillabletodescribethedatawell.Determinetheparameters ofthatmodel.(*).
2. Given the system:
F ¼
0:32857
0:085714
1:1429
1:0714
"
#
H ¼ 10
5
½

Determine the observability Grammian and the observability matrix. What are the
eigenvalues of these matrices? What can be said about the observability? (0)
3. Given the system x(i þ 1) ¼ Fx(i) þ Gw(i) and z(i) ¼ Hx(i) þ v(i) with
F ¼
0:65
0:06
0:375
0:65
"
#
G ¼
2
5


H ¼ 1
1
½

w(i) and v(i) are white noise sequences with unit variances. Examine the observability
and the controllability of this system. Does the steady state Kalman filter exist? If so,
determine the Kalman gain, the innovation matrix, the prediction covariance matrix
and the error covariance matrix. (*)
4. Repeat exercise 3, but this time with:
F ¼
0:85
0:14
0:875
0:85
"
#
ð*Þ
5. Repeat exercise 3, but now with:
F ¼
0:75
0:1
0:625
0:75
"
#
ð*Þ
0
10
20
30
40
50
60
70
80
90
100
–10
–5
0
5
10
x(i)
i
Figure 8.17
Observed random sequence in exercise 1
EXERCISES
307

6. Explain the different results obtained in exercises 3, 4 and 5, by examining the
eigenvalues of F in the different cases. (**)
7. Determine the computational complexity of the information filter. (*)
8. Drift in the measurements.
We consider a physical quantity x(i) that is sensed by a drifting sensor whose output is
modelled by z(i) ¼ x(i) þ v(i) and v(i þ 1) ¼ v(i) þ ~v(i) with  ¼ 0:999. ~v(i) is a white
noise sequence with zero mean and variance 2
~v ¼ 0:002. The physical quantity has a
limited bandwidth modelled by x(i þ 1) ¼ x(i) þ w(i) with  ¼ 0:95. The process
noise is white and has a variance 2
w ¼ 0:0975. A record of the measurements is shown
in Figure 8.18. The data is available in the file C8exercise8:mat.
. Give a state space model of this system. (0)
. Examine the observability and controllability of this system. (0)
. Give the solution of the discrete Lyapunov equation (0)
. Realize the discrete Kalman filter. Calculate and plot the estimates, its  boundary,
and the innovations and the periodogram of the innovations. (*)
. Compare the signal-to-noise ratios before and after filtering. (0)
. Perform the consistency checks. (*)
0
100
200
300
400
500
600
700
800
900
1000
–5
–4
–3
–2
–1
0
1
2
3
4
5
z(i)
i
Figure 8.18
Observed measurements from a drifting sensor
308
STATE ESTIMATION IN PRACTICE

9
Worked Out Examples
In this final chapter, three worked out examples will be given of the
topics discussed in this book: classification, parameter estimation and
state estimation. They will take the form of a step-by-step analysis of
data sets obtained from real-world applications. The examples demon-
strate the techniques treated in the previous chapters. Furthermore, they
are meant to illustrate the standard approach to solving these types of
problems. Obviously, the MATLAB and PRTools algorithms as they were
presented in the previous chapters will be used. The data sets used here
are available through the website accompanying this book.
9.1
BOSTON HOUSING CLASSIFICATION PROBLEM
9.1.1
Data set description
The Boston Housing data set is often used to benchmark data analysis
tools. It was first described in Harrison and Rubinfield (1978). This paper
investigates which features are linked to the air pollution in several areas in
Boston. The data set can be downloaded from the UCI Machine Learning
repository at http://www.ics.uci.edu/~mlearn/MLRepository.html.
Each feature vector from the set contains 13 elements. Each feature
element provides specific information on an aspect of an area of a
suburb. Table 9.1 gives a short description of each feature element.
Classification, Parameter Estimation and State Estimation: An Engineering Approach using MATLAB
F. van der Heijden, R.P.W. Duin, D. de Ridder and D.M.J. Tax
 2004 John Wiley & Sons, Ltd
ISBN: 0-470-09013-8

The goal we set for this section is to predict whether the median price
of a house in each area is larger than or smaller than $20 000. That is, we
formulate a two-class classification problem. In total, the data set con-
sists of 506 areas, where for 215 areas the price is lower than $20 000
and for 291 areas it is higher. This means that when a new object has to
be classified using only the class prior probabilities, assuming the data
set is a fair representation of the true classification problem, it can be
expected that in ð215/506Þ  100% ¼ 42:5% of the cases we will mis-
classify the area.
After the very first inspection of the data, by just looking what values
the different features might take, it appears that the individual features
differ significantly in range. For instance, the values for the feature TAX
varies between 187 and 711 (a range of more than 500), while the feature
values of NOX are between 0.38 and 0.87 (a range of less than 0.5). This
suggests that some scaling might be necessary. A second important obser-
vation is that some of the features can only have discrete values, like
RIVER and HIGHWAY. How to combine real valued features with
discrete features is already a problem in itself. In this analysis we will
Table 9.1
The features of the Boston Housing data set
Feature name
Description
Feature range and type
1
CRIME
Crime rate per capita
0–89, real valued
2
LARGE
Proportion of area
dedicated to lots larger than
25 000 square feet
0–100, real valued,
but many have value 0
3
INDUSTRY
Proportion of area dedicated
to industry
0.46–27.7, real valued
4
RIVER
1 ¼ borders the Charles
river, 0 ¼ does not border
the Charles river
0–1, nominal values
5
NOX
Nitric oxides concentration
(in parts per 10 million)
0.38–0.87, real valued
6
ROOMS
Average number of rooms
per house
3.5–8.8, real valued
7
AGE
Proportion of houses built
before 1940
2.9–100, real valued
8
WORK
Average distance to employment
centres in Boston
1.1–12.1, real valued
9
HIGHWAY
Highway accessibility index
1–24, discrete values
10
TAX
Property tax rate (per $10 000)
187–711, real valued
11
EDUCATION
Pupil–teacher ratio
12.6–22.0, real valued
12
AA
1000ðA  0:63Þ2 where A is the
proportion of African-Americans
0.32–396.9, real valued
13
STATUS
Percentage of the population
which has lower status
1.7–38.0, real valued
310
WORKED OUT EXAMPLES

ignore this problem, and we will assume that all features are real valued.
(Obviously, we will lose some performance using this assumption.)
9.1.2
Simple classification methods
Given the varying nature of the different features, and the fact that further
expert knowledge is not given, it will be difficult to construct a good model
for this data. The scatter diagram of Figure 9.1 shows that an assumption
of Gaussian distributed data is clearly wrong (if only by the presence of the
discrete features), but when just classification performance is considered,
the decision boundary might still be good enough. Perhaps more flexible
methods such as the Parzen density or the -nearest neighbour method will
perform better; after a suitable feature selection and feature scaling.
Let us start with some baseline methods and train a linear and quad-
ratic Bayes classifier, ldc and qdc:
Listing 9.1
% Load the housing dataset, and set the baseline performance
load housing.mat;
z
% Show what dataset we have
w ¼ ldc;
% Define an untrained linear
classifier
err_ldc_baseline ¼ crossval(z,w,5)
% Perform 5-fold
cross-validation
err_qdc_baseline ¼ crossval(z,qdc,5)
% idem for the quadratic
classifier
0
5
10
15
20
25
30
35
0
5
10
15
20
25
–6
–4
–2
0
2
4
6
–4
–3
–2
–1
0
1
2
3
4
5
Figure 9.1
Scatter plots of the Boston Housing data set. The left subplot shows
features STATUS and INDUSTRY, where the discrete nature of INDUSTRY can be
spotted. In the right subplot, the data set is first scaled to unit variance, after which it
is projected onto its first two principal components
BOSTON HOUSING CLASSIFICATION PROBLEM
311

The five-fold cross-validation errors of ldc and qdc are 13.0% and
17.4%, respectively. Note that by using the function crossval, we
avoided having to use the training set for testing. If we had done that,
the errors would be 11.9% and 16.2%, but this estimate would be
biased and too optimistic. The results also depend on how the data is
randomly split into batches. When this experiment is repeated, you
will probably find slightly different numbers. Therefore, it is advisable
to repeat the entire experiment a number of times (say, 5 or 10) to get
an idea of the variation in the cross-validation results. However, for
many experiments (such as the feature selection and neural network
training below), this may lead to unacceptable training times; there-
fore, the code given does not contain any repetitions. For all the
results given, the standard deviation is about 0.005, indicating that
the difference between ldc and qdc is indeed significant. Notice that
we use the word ‘significant’ here in a slightly loose sense. From a
statistical perspective it would mean that for the comparison of the
two methods, we should state a null-hypothesis that both methods
perform the same, and define a statistical test (for instance a t-test) to
decide if this hypothesis holds or not. In this discussion we use the
simple approach in which we just look at the standard deviation of
the classifier performances, and call the performance difference sig-
nificant when the averages differ by more than two times their stand-
ard deviations.
Even with these simple models on the raw, not preprocessed data, a
relative good test error of 13.0% can be achieved (with respect to the
simplest approach by looking at the class probabilities). Note that qdc,
which is a more powerful classifier, gives the worst performance. This is
due to the relatively low sample size: two covariance matrices, with
1
2 13(13 þ 1) ¼ 91 free parameters each, have to be estimated on 80%
of the data available for the classes (i.e. 172 and 233 samples, respect-
ively). Clearly, this leads to poor estimates.
9.1.3
Feature extraction
It might be expected that more flexible methods, like the -nearest neigh-
bour classifier (knnc, with  optimized using the leave-one-out method)
or the Parzen density estimator (parzenc) give better results. Surpris-
ingly, a quick check shows that then the errors become 19.6% and 21.9%
(with a standard deviation of about 1.1% and 0.6%, respectively), clearly
312
WORKED OUT EXAMPLES

indicating that some overtraining occurs and/or that some feature selection
and feature scaling might be required. The next step, therefore, is to rescale
the data to have zero mean and unit variance in all feature directions. This
can be performed using the PRTools function scalem([],‘variance’):
Listing 9.2
load housing.mat;
% Define an untrained linear classifier w/scaled input data
w_sc ¼ scalem([],‘variance’);
w ¼ w_sc*ldc;
% Perform 5-fold cross-validation
err_ldc_sc ¼ crossval(z,w,5)
% Do the same for some other classifiers
err_qdc_sc ¼ crossval(z,w_sc*qdc,5)
err_knnc_sc ¼ crossval(z,w_sc*knnc,5)
err_parzenc_sc ¼ crossval(z,w_sc*parzenc,5)
First note, that when we introduce a preprocessing step, this step should
be defined inside the mapping w. The obvious approach, to map the
whole data set z_sc ¼ z*scalem(a,‘variance’) and then to apply
the cross-validation to estimate the classifier performance, is incorrect.
In that case, some of the testing data is already used in the scaling of the
data, resulting in an overtrained classifier, and thus in an unfair estimate
of the error. To avoid this, the mapping should be extended from ldc to
w_sc*ldc. The routine crossval then takes care of fitting both the
scaling and the classifier.
By scaling the features, the performance of the first two classifiers,
ldc and qdc, should not change. The normal density based classifiers
are insensitive to the scaling of the individual features, because they
already use their variance estimation. The performance of knnc and
parzenc on the other hand improve significantly, to 14.1% and
13.1%, respectively (with a standard deviation of about 0.4%).
Although parzenc approaches the performance of the linear classifier,
it is still slightly worse. Perhaps feature extraction or feature selection
will improve the results.
As discussed in Chapter 7, principal component analysis (PCA) is one
of the most often used feature extraction methods. It focuses on the high-
variance directions in the data, and removes low-variance directions.
In this data set we have seen that the feature values have very different
scales. Applying PCA directly to this data will put high emphasis on the
feature TAX and will probably ignore the feature NOX. Indeed, when
BOSTON HOUSING CLASSIFICATION PROBLEM
313

PCA preprocessing is applied such that 90% of the variance is retained,
the performance of all the methods significantly decreases. To avoid this
clearly undesired effect, we will first rescale the data to have unit
variance and apply PCA on the resulting data. The basic training proced-
ure now becomes:
Listing 9.3
load housing.mat;
% Define a preprocessing
w_pca ¼ scalem([],‘variance’)*pca([],0.9);
% Define the classifier
w ¼ w_sc*ldc;
% Perform 5-fold cross-validation
err_ldc_pca ¼ crossval(z,w,5)
It appears that, compared with normal scaling, the application of
pca([],0:9) does not significantly improve the performances. For some
methods, the performance increases slightly (16.6% (0:6%) error for
qdc, 13.6% (0:9%) for knnc), but for other methods, it decreases.
This indicates that the high-variance features are not much more informa-
tive than the low-variance directions.
9.1.4
Feature selection
The use of a simple supervised feature extraction method, such as the
Bhattacharrya mapping (implemented by replacing the call to pca by
bhatm([])), also decreases the performance. We will therefore have to
use better feature selection methods to reduce the influence of noisy
features and to gain some performance.
We will first try branch-and-bound feature selection to find five
features, with the simple inter–intra class distance measure as a criterion,
finding the optimal number of features. Admittedly, the number of
features selected, five, is arbitrary, but the branch-and-bound method
does not allow for finding the optimal subset size.
Listing 9.4
% Load the housing dataset
load housing.mat;
% Construct scaling and feature selection mapping
w_fsf ¼ featselo([],‘in-in’,5)*scalem([],‘variance’);
314
WORKED OUT EXAMPLES

% Calculate cross-validation error for classifiers
% trained on the optimal 5-feature set
err_ldc_fsf ¼ crossval(z,w_fsf*ldc,5)
err_qdc_fsf ¼ crossval(z,w_fsf*qdc,5)
err_knnc_fsf ¼ crossval(z,w_fsf*knnc,5)
err_parzenc_fsf ¼ crossval(z,w_fsf*parzenc,5)
The feature selection routine often selects features STATUS, AGE and
WORK, plus some others. The results are not very good: the perform-
ance decreases for all classifiers. Perhaps we can do better if we take the
performance of the actual classifier to be used as a criterion, rather than
the general inter–intra class distance. To this end, we can just pass the
classifier to the feature selection algorithm. Furthermore, we can also let
the algorithm find the optimal number of features by itself. This means
that branch-and-bound feature selection can now no longer be used, as
the criterion is not monotonically increasing. Therefore, we will use
forward feature selection, featself.
Listing 9.5
% Load the housing dataset
load housing.mat;
% Optimize feature set for ldc
w_fsf ¼ featself([],ldc,0)*scalem([],‘variance’);
err_ldc_fsf ¼ crossval(z,w_fsf*ldc,5)
% Optimize feature set for qdc
w_fsf ¼ featself([],qdc,0)*scalem([],‘variance’);
err_qdc_fsf ¼ crossval(z,w_fsf*qdc,5)
% Optimize feature set for knnc
w_fsf ¼ featself([],knnc,0)*scalem([],‘variance’);
err_knnc_fsf ¼ crossval(z,w_fsf*knnc,5)
% Optimize feature set for parzenc
w_fsf ¼ featself([],parzenc,0)*scalem([],‘variance’);
err_parzenc_fsf ¼ crossval(z,w_fsf*parzenc,5)
This type of feature selection turns out to be useful only for ldc and
qdc, whose performances improve to 12.8% (0:6%) and 14.9%
(0:5%), respectively. knnc and parzenc, on the other hand, give
15.9% (1:0%) and 13.9% (1:7%), respectively. These results do
not differ significantly from the previous ones. The featself routine
often selects the same rather large set of features (from most to least
significant):
STATUS,
AGE,
WORK,
INDUSTRY,
AA,
CRIME,
LARGE, HIGHWAY, TAX. But these features are highly correlated,
and the set used can be reduced to the first three with just a small
BOSTON HOUSING CLASSIFICATION PROBLEM
315

increase in error of about 0.005. Nevertheless, feature selection in gen-
eral does not seem to help much for this data set.
9.1.5
Complex classifiers
The fact that ldc already performs so well on the original data indicates
that the data is almost linearly separable. A visual inspection of the
scatter plots in Figure 9.1 seems to strengthen this hypothesis. It becomes
even more apparent after the training of a linear support vector classifier
(svc([],‘p’,1)) and a Fisher classifier (fisherc), both with a cross-
validation error of 13.0%, the same as for ldc.
Given that ldc and parzenc thus far performed best, we might try
to train a number of classifiers based on these concepts, for which we are
able to tune classifier complexity. Two obvious choices for this are
neural networks and support vector classifiers (SVCs). Starting with
the latter, we can train SVCs with polynomial kernels of degree close
to 1, and with radial basis kernels of radius close to 1. By varying the
degree or radius, we can vary the resulting classifier’s nonlinearity:
Listing 9.6
load housing.mat;
% Load the housing dataset
w_pre ¼ scalem([], ‘variance’);
% Scaling mapping
degree ¼ 1:3;
% Set range of parameters
radius ¼ 1:0.25:3;
for i ¼ 1:length(degree)
err_svc_p(i) ¼ . . .
% Train polynomial SVC
crossval(z,w_pre*svc([],‘p’,degree(i)),5);
end;
for i ¼ 1:length(radius)
err_svc_r(i) ¼ . . .
% Train radial basis SVC
crossval(z,w_pre*svc([], ‘r’,radius(i)),5);
end;
figure; clf; plot(degree,err_svc_p);
figure; clf; plot(radius,err_svc_r);
The results of a single repetition are shown in Figure 9.2: the optimal
polynomial kernel SVC is a quadratic one (a degree of 2), with an average
error of 12.5%, and the optimal radial basis kernel SVC (a radius of
around 2) is slightly better, with an average error of 11.9%. Again, note
that we should really repeat the experiment a number of times, to get an
impression of the variance in the results. The standard deviation is
316
WORKED OUT EXAMPLES

between 0.2% to 1.0%. This means that the minimum in the right
subfigure is indeed significant, and that the radius parameter should
indeed be around 2.0. On the other hand, in the left subplot the graph
is basically flat and a linear SVC is therefore probably to be preferred.
For the sake of completeness, we can also train feed-forward neural
networks with varying numbers of hidden layers and units. In PRTools,
there are three routines for training feed-forward neural networks. The
bpxnc function trains a network using the back-propagation algorithm,
which is slow but does not often overtrain it. The lmnc function uses a
second-order
optimization
routine,
Levenberg–Marquardt,
which
speeds up training significantly but often results in overtrained net-
works. Finally, the neurc routine attempts to counteract the overtrain-
ing problem of lmnc by creating an artificial tuning set of 1000 samples
by perturbing the training samples (see gendatk) and stops training
when the error on this tuning set increases. It applies lmnc three times
and returns the neural network giving the best result on the tuning set.
Here, we apply both bpxnc and neurc:
Listing 9.7
load housing.mat;
% Load the housing dataset
w_pre ¼ scalem([], ‘variance’);
% Scaling mapping
networks ¼ {bpxnc, neurc};
% Set range of parameters
nlayers ¼ 1:2;
nunits ¼ [4 8 12 16 20 30 40];
for i ¼ 1:length(networks)
1
2
3
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
0
1
2
3
4
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
degree
kernel width
error
error
Figure 9.2
Performance of a polynomial kernel SVC (left, as a function of the
degree of the polynomial) and a radial basis function kernel SVC (right, as a function
of the basis function width)
BOSTON HOUSING CLASSIFICATION PROBLEM
317

for j ¼ 1:length(nlayers)
for k ¼ 1:length(nunits)
% Train a neural network with nlayers(j) hidden layers
% of nunits(k) units each, using algorithm network{i}
err_nn(i,j,k) ¼ crossval(z, . . .
w_pre*networks{i}([],ones(1,nlayers(j))*nunits(k)),5);
end;
end;
figure; clear all;
% Plot the errors
plot(nunits,err_nn(i,1,:), ‘-’); hold on;
plot(nunits,err_nn(i,2,:), ‘--’);
legend(‘1 hidden layer’, ‘2 hidden layers’);
end;
Training neural networks is a computationally intensive process; and
here they are trained for a large range of parameters, using cross-validation.
The algorithm above takes more than a day to finish on a modern
workstation, although per setting just a single neural network is trained.
The results, shown in Figure 9.3, seem to be quite noisy. After repeat-
ing the algorithm several times, it appears that the standard deviation is
in the order of 1%. Ideally, we would expect the error as a function of
the number of hidden layers and units per hidden layer to have a clear
global optimum. For bpxnc, this is roughly the case, with a minimal
cross-validation error of 10.5% for a network with one hidden layer of
30 units, and 10.7% for a network with two hidden layers of 16 units.
Normally, we would prefer to choose the network with the lowest
0
10
20
30
40
0
0.05
0.1
0.15
0.2
1 hidden layer
2 hidden layers
1 hidden layer
2 hidden layers
0
10
20
30
40
0
0.05
0.1
0.15
0.2
units/hidden layer
units/hidden layer
error
error
Figure 9.3
Performance of neural networks with one or two hidden layers as a
function of the number of units per hidden layer, trained using bpxnc (left) and neurc
(right)
318
WORKED OUT EXAMPLES

complexity as the variance in the error estimate would be lowest. How-
ever, the two optimal networks here have roughly the same number of
parameters. So, there is no clear best choice between the two.
The cross-validation errors of networks trained with neurc show
more variation (Figure 9.3). The minimal cross-validation error is again
10.5% for a network with a single hidden layer of 30 units. Given that
the graph for bpxnc is much smoother than that of neurc, we would
prefer to use a bpxnc-trained network.
9.1.6
Conclusions
The best overall result on the housing data set was obtained using a
bpxnc-trained neural network (10.5% cross-validation error), slightly
better than the best SVC (11.9%) or a simple linear classifier (13.0%).
However, remember that neural network training is a more noisy pro-
cess than training an SVC or linear classifier: the latter two will find the
same solution when run twice on the same data set, whereas a neural
network may give different results due to the random initialization.
Therefore, using an SVC in the end may be preferable.
Of course, the analysis above is not exhaustive. We could still have tried
more exotic classifiers, performed feature selection using different criteria
and search methods, searched through a wider range of parameters for the
SVCs and neural networks, investigated the influence of possible outlier
objects and so on. However, this will take a lot of computation, and for
this application there seems to be no reason to believe we might obtain
a significantly better classifier than those found above.
9.2
TIME-OF-FLIGHT ESTIMATION OF AN
ACOUSTIC TONE BURST
The determination of the time of flight (ToF) of a tone burst is a key
issue in acoustic position and distance measurement systems. The length
of the acoustic path between a transmitter and receiver is proportional to
the ToF, that is, the time evolved between the departure of the waveform
from the transmitter and the arrival at the receiver (Figure 9.4).
The position of an object is obtained, for instance, by measuring the
distances from the object to a number of acoustic beacons. See also
Figure 1.2.
TIME-OF-FLIGHT ESTIMATION OF AN ACOUSTIC TONE BURST
319

The quality of an acoustic distance measurement is directly related to
the quality of the ToF determination. Electronic noise, acoustic noise,
atmospheric turbulence and temperature variations are all factors that
influence the quality of the ToF measurement. In indoor situations,
objects in the environment (wall, floor, furniture, etc.) may cause echoes
that disturb the nominal response. These unwanted echoes can cause
hard-to-predict waveforms, thus making the measurement of the ToF
a difficult task.
The transmitted waveform can take various forms, for instance,
a frequency modulated (chirped) continuous waveform (CWFM), a
frequency or phase shift-keyed signal or a tone burst. The latter is a
pulse consisting of a number of periods of a sine wave. An advantage of
a tone burst is that the bandwidth can be kept moderate by adapting the
length of the burst. Therefore, this type of signal is suitable for use in
combination with piezoelectric transducers, which are cheap and robust,
but have a narrow bandwidth.
In this section, we design an estimator for the determination of the
ToFs of tone bursts that are acquired in indoor situations using a set-up
as shown in Figure 9.4. The purpose is to determine the time delay
between sending and receiving a tone burst. A learning and evaluation
data set is available that contains 150 records of waveforms acquired in
different rooms, different locations in the rooms, different distances and
different heights above the floor. Figure 9.4 shows an example of one of
the waveforms. Each record is accompanied by a reference ToF indica-
ting the true value of the ToF. The standard deviation of the reference
ToF is estimated at 10 (ms). The applied sampling period is  ¼ 2 (ms).
waveform
generator
vτ
w(t)
τ = ToF
0
0.005
0.01
0.015
0.02
t(s)
observed waveform w(t)
↑
ToF
8.5
9
9.5
10
t(ms)
zoomed
↑
ToF
Figure 9.4
Set-up of a sensory system for acoustic distance measurements
320
WORKED OUT EXAMPLES

The literature roughly mentions three concepts to determine the ToF,
i.e. thresholding, data fitting (regression) and ML (maximum likelihood)
estimation (Heijden van der et al., 2003). Many variants of these con-
cepts have been proposed. This section only considers the main repre-
sentatives of each concept:
. Comparing the envelope of the wave against a threshold that is
proportional to the magnitude of the waveform.
. Fitting a one-sided parabola to the foot of the envelope of the
waveform.
. Conventional matched filtering.
. Extended matched filtering based on a covariance model of the signal.
The first one is a heuristic method that does not optimize any criterion
function. The second one is a regression method, and as such a repre-
sentative of data fitting. The last two methods are ML estimators. The
difference between these is that the latter uses an explicit model of
multiple echoes. In the former case, such a model is missing.
The section first describes the methods. Next, the optimization of the
design parameters using the data set is explained, and the evaluation is
reported. The data set and the MATLAB listings of the various methods
can be found on the accompanying website.
9.2.1
Models of the observed waveform
The moment of time at which a transmission begins is well defined since
it is triggered under full control of the sensory system. The measurement
of the moment of arrival is much more involved. Due to the narrow
bandwidth of the transducers the received waveform starts slowly. A low
SNR makes the moment of arrival indeterminate. Therefore, the design
of a ToF estimator requires the availability of a model describing the
arriving waveform. This waveform w(t) consists of three parts: the
nominal response ah(t  t) to the transmitted wave; the interfering
echoes ar(t  t); and the noise v(t):
wðtÞ ¼ ahðt  tÞ þ arðt  tÞ þ vðtÞ
ð9:1Þ
We assume that the waveform is transmitted at time t ¼ 0, so that t is
the ToF. (9.1) simply states that the observed waveform equals the
TIME-OF-FLIGHT ESTIMATION OF AN ACOUSTIC TONE BURST
321

nominal response h(t), but now time-shifted by t and attenuated by a.
Such an assumption is correct for a medium like air because, within the
bandwidth of interest, the propagation of a waveform through air does
not show a significant dispersion. The attenuation coefficient a depends
on many factors, but also on the distance, and thus also on t. However,
for the moment we will ignore this fact. The possible echoes are
represented by ar(t  t). They share the same time shift t because no
echo can occur before the arrival of the nominal response. The addi-
tional time delays of the echoes are implicitly modelled within r(t). The
echoes and the nominal response also share a common attenuation
factor. The noise v(t) is considered white.
The actual shape of the nominal response h(t) depends on the choice
of the tone burst and on the dynamic properties of the transducers.
Sometimes, a parametric empirical model is used, for instance:
hðtÞ ¼ tm expðt=TÞ cosð2ft þ ’Þ
t  0
ð9:2Þ
f is the frequency of the tone burst; cos (2ft þ ’) is the carrier; and
tm exp (t/T) is the envelope. The factor tm describes the rise of the
waveform (m is empirically determined; usually between 1 and 3). The
factor exp (t/T) describes the decay. Another possibility is to model h(t)
non-parametrically. In that case, a sampled version of h(t), obtained in
an anechoic room where echoes and noise are negligible, is recorded.
The data set contains such a record. See Figure 9.5.
Often, the existence of echoes is simply ignored, r(t) ¼ 0. Sometimes,
a single echo is modelled r(t) ¼ d1  h(t  t1) where t1 is the delay of the
echo with respect to t ¼ t. The most extensive model is when multiple
echoes are considered r(t) ¼ P
k dkh(t  tk). The sequences dk and tk are
hardly predictable and therefore regarded as random. In that case, r(t)
0
0.2
0.4
0.6
0.8
1
1.2
t(ms)
nominal response h(t)
Figure 9.5
A record of the nominal response h(t)
322
WORKED OUT EXAMPLES

becomes random too, and the echoes are seen as disturbing noise with
non-stationary properties.
The observed waveform z ¼ [ z0 . . . zN1 ]T is a sampled version
of w(t):
zn ¼ wðnÞ
ð9:3Þ
 is the sampling period. N is the number of samples. Hence, N is the
registration period. With that, the noise v(t) manifests itself as a random
vector v, with zero mean and covariance matrix Cv ¼ 2
vI.
9.2.2
Heuristic methods for determining the ToF
Some applications require cheap solutions that are suitable for direct
implementation using dedicated hardware, such as instrumental elec-
tronics. For that reason, a popular method to determine the ToF is
simply thresholding the observed waveform at a level T. The estimated
ToF ^tthres is the moment at which the waveform crosses a threshold
level T.
Due to the slow rising of the nominal response, the moment ^tthres of
level crossing appears just after the true t, thus causing a bias. Such a
bias can be compensated afterwards. The threshold level T should be
chosen above the noise level. The threshold operation is simple to
realize, but a disadvantage is that the bias depends on the magnitude
of the waveform. Therefore, an improvement is to define the threshold
level relative to the maximum of the waveform, that is T ¼  max (w(t)).
 is a constant set to, for instance, 30%.
The observed waveform can be written as g(t) cos (2ft þ ’) þ n(t)
where g(t) is the envelope. The carrier cos (2ft þ ’) of the waveform
causes a resolution error equal to 1/f. Therefore, rather than applying
the threshold operation directly, it is better to apply it to the envelope.
A simple, but inaccurate method to get the envelope is to rectify the
waveform and to apply a low-pass filter to the result. The optimal method,
however, is much more involved and uses quadrature filtering. A simpler
approximation is as follows. First, the waveform is band-filtered to
reduce the noise. Next, the filtered signal is phase-shifted over 90 to
obtain
the
quadrature
component
q(t) ¼ g(t) sin (2ft þ ’) þ nq(t).
Finally, the envelope is estimated using ^g(t) ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
w2
band-filtered(t) þ q2(t)
q
.
Figure 9.6 provides an example.
TIME-OF-FLIGHT ESTIMATION OF AN ACOUSTIC TONE BURST
323

The design parameters are the relative threshold , the bias compen-
sation b, and the cut-off frequencies of the band-filter.
9.2.3
Curve fitting
In the curve-fitting approach, a functional form is used to model the
envelope. The model is fully known except for some parameters, one of
which is the ToF. As such, the method is based on the regression
techniques introduced in Section 3.3.3. On adoption of an error criterion
between the observed waveform and the model, the problem boils down
to finding the parameters that minimize the criterion. We will use the
SSD criterion discussed in Section 3.3.1. The particular function that will
be fitted is the one-sided parabola defined by:
fðt; xÞ ¼
x0 þ x1ðt  x2Þ2
if t > x2
x0
elsewhere

ð9:4Þ
The final estimate of the ToF is ^tcurve ¼ x2.
The function must be fitted to the foot of the envelope. Therefore, an
important task is to determine the interval tb < t < te that makes up the
foot. The choice of tb and te is critical. If the interval is short, then the
8.5
9
9.5
10
t (ms)
thresholding the waveform
max(w)
T
τthres
8.5
9
9.5
10
t (ms)
thresholding the envelope
max(w)
τthres
T
Figure 9.6
ToF measurements based on thresholding operations
324
WORKED OUT EXAMPLES

noise sensitivity is large. If the interval is too large, modelling errors
become too influential. The strategy is to find two anchor points t1 and
t2 that are stable enough under the various conditions.
The first anchor point t1 is obtained by thresholding a low-pass
filtered version gfiltered(t) of the envelope just above the noise level.
If v is the standard deviation of the noise in w(t), then gfiltered(t) is
thresholded at a level 3 1
2 v, thus yielding t1. The standard deviation v
is estimated during a period preceding the arrival of the tone. A second
suitable anchor point t2 is the first location just after t1 where gfiltered(t) is
maximal, i.e. the location just after t1 where dgfiltered(t)/dt ¼ 0. te is
defined midway between t1 and t2 by thresholding gfiltered(t) at a level
3 1
2 v þ (gfiltered(t2)  3 1
2 v). Finally, tb is calculated as tb ¼ t1  
(te  t1). Figure 9.7 provides an example.
Once the interval of the foot has been established, the curve can be
fitted to the data (which in this case is the original envelope ^g(t)). Since
the curve is nonlinear in x, an analytical solution, such as in the poly-
nomial regression in Section 3.3.3, cannot be applied. Instead a numer-
ical procedure, for instance using MATLAB’s fminsearch, should be
applied.
The design parameters are ,  and the cut-off frequency of the low-
pass filter.
8.5
9
9.5
10
t (ms)
one-sided parabola fitted to the envelope
τcurve
tb
te
t1
t2
Figure 9.7
ToF estimation by fitting a one-sided parabola to the foot of the
envelope
TIME-OF-FLIGHT ESTIMATION OF AN ACOUSTIC TONE BURST
325

9.2.4
Matched filtering
This conventional solution is achieved by neglecting the reflections. The
measurements are modelled by a vector z with N elements:
zn ¼ ahðn  tÞ þ vðnÞ
ð9:5Þ
The noise is represented by a random vector v with zero mean and
covariance matrix Cv ¼ 2
vI. Upon introduction of a vector h(t) with
elements h(n  t) the conditional probability density of z is:
pðzjtÞ ¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð22
nÞN
q
exp  1
22
n
ðz  ahðtÞÞTðz  ahðtÞÞ


ð9:6Þ
Maximization of this expression yields the maximum likelihood estimate
for t. In order to do so, we only need to minimize the L2 norm of z  ah(t):
ðz  ahðtÞÞTðz  ahðtÞÞ ¼ zTz þ a2hðtÞThðtÞ  2azThðtÞ
ð9:7Þ
The term zTz does not depend on t and can be ignored. The second term
is the signal energy of the direct response. A change of t only causes a
shift of it. But, if the registration period is long enough, the signal energy
is not affected by such a shift. Thus, the second term can be ignored as
well. The maximum likelihood estimate boils down to finding the t that
maximizes azTh(t). A further simplification occurs if the extent of h(t) is
limited to, say, K with K << N. In that case azTh(t) is obtained by
cross-correlating zn by ah(n þ t):
yðtÞ ¼ a
X
K1
k¼0
hðk  tÞzk
ð9:8Þ
The value of t which maximizes y(t) is the best estimate. The operator
expressed by (9.8) is called a matched filter or a correlator. Figure 9.8
shows a result of the matched filter. Note that apart from its sign, the
amplitude a does not affect the outcome of the estimate. Hence, the fact
that a is usually unknown doesn’t matter much. Actually, if the nominal
response is given in a non-parametric way, the matched filter doesn’t
have any design parameters.
326
WORKED OUT EXAMPLES

9.2.5
ML estimation using covariance models for the
reflections
The matched filtered is not designed to cope with interfering reflections.
Especially, if an echo partly overlaps the nominal response, the results
are inaccurate. In order to encompass situations with complex interfer-
ence patterns the matched filter must be extended. A possibility is to
model the echoes explicitly. A tractable model arises if the echoes are
described by a non-stationary autocovariance function.
Covariance models
The echoes are given by r(t) ¼ P
k dkh(t  tk). The points in time, tk, are
a random sequence. Furthermore we have tk > 0 since all echoes appear
after the arrival of the direct response. The attenuation factors dk have a
range of values. We will model them as independent Gaussian random
variables with zero mean and variance 2
d. Negative values of dk are
allowed because of the possible phase reversal of an echo. We limit the
occurrence of an echo to an interval 0 < tk < T, and assume a uniform
distribution. Then the autocovariance function of a single echo is:
Ckðt1; t2Þ ¼ E d2
khðt1  tkÞhðt2  tkÞ


¼ 2
d
T
Z T
tk¼0
hðt1  tkÞhðt2  tkÞdtk
ð9:9Þ
8
8.5
9
9.5
10
t (ms)
observed waveform
8
8.5
9
9.5
10
t (ms)
matched filtering
τmatch
Figure 9.8
Matched filtering
TIME-OF-FLIGHT ESTIMATION OF AN ACOUSTIC TONE BURST
327

If there are K echoes, the autocovariance function of r(t) is
Crðt1; t2Þ ¼ KCkðt1; t2Þ ¼ K2
d
T
Z T
tk¼0
hðt1  tkÞhðt2  tkÞdtk
ð9:10Þ
because the factors dk and random points tk are independent.
For arbitrary t, the reflections are shifted accordingly. The sampled ver-
sion of the reflections is r(n  t) which can be brought in a vector r(t).
The elements Crjt(n,m) of the covariance matrix Crjt of r(t), conditioned
on t, become:
Crjtðn; mÞ ¼ Crðn  t; m  tÞ
ð9:11Þ
If the registration period is sufficiently large, the determinant jCrjtj does
not depend on t.
The observed waveform w(t) ¼ a(h(t  t) þ r(t  t)) þ v(t) involves
two unknown factors, the amplitude a and the ToF t. The prior prob-
ability density of the latter is not important because the maximum like-
lihood estimator that we will apply does not require it. However, the
first factor a is a nuisance parameter. We deal with it by regarding a as a
random variable with its own density p(a). The influence of a is inte-
grated in the likelihood function by means of Bayes’ theorem for condi-
tional probabilities, i.e. p(zjt) ¼
R
p(zjt,a)p(a)da.
Preferably, the density p(a) reflects our state of knowledge that we
have about a. Unfortunately, taking this path is not easy, for two
reasons. It would be difficult to assess this state of knowledge quantita-
tively. Moreover, the result will not be very tractable. A more practical
choice is to assume a zero mean Gaussian density for a. With that,
conditioned on t, the vector ah(t) with elements ah(n  t) becomes
zero mean and Gaussian with covariance matrix:
Chjt ¼ 2
ahðtÞhTðtÞ
ð9:12Þ
where 2
a is the variance of the amplitude a.
At first sight it seems counterintuitive to model a as a zero mean
random variable since small and negative values of a are not very likely.
The only reason for doing so is that it paves the way to a mathematically
tractable model. In Section 9.2.4 we noticed already that the actual value
of a does not influence the solution. We simply hope that in the extended
matched filter a does not have any influence either. The advantage is that
328
WORKED OUT EXAMPLES

the dependence of t on z is now captured in a concise model, i.e. a single
covariance matrix:
Czjt ¼ 2
aðhðtÞhTðtÞ þ CrjtÞ þ 2
vI
ð9:13Þ
This matrix completes the covariance model of the measurements. In the
sequel, we assume a Gaussian conditional density for z. Strictly speak-
ing, this holds true only if sufficient echoes are present since in that case
the central limit theorem applies.
Maximum likelihood estimation of the time-of-flight
With the measurements modelled as a zero mean, Gaussian random
vector with the covariance matrix given in (9.13), the likelihood function
for t becomes:
pðzjtÞ ¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð2ÞKjCzjtj
q
exp  1
2 zTC1
zjt z


ð9:14Þ
The maximization of this probability with respect to t yields the max-
imum likelihood estimate; see Section 3.1.4. Unfortunately, this solution
is not practical because it involves the inversion of the matrix Czjt. The
size of Czjt is N  N where N is the number of samples of the registration
(which can easily be in the order of 104).
Principal component analysis
Economical solutions are attainable by using PCA techniques (Section
7.1.1). If the registration period is sufficiently large, the determinant
jCzjtj will not depend on t. With that, we can safely ignore this factor.
What remains is the maximization of the argument of the exponential:
ðzjtÞ¼
def zTC1
zjt z
ð9:15Þ
The functional (zjt) is a scaled version of the log-likelihood function.
The first computational savings can be achieved if we apply a principal
component analysis to Czjt. This matrix can be decomposed as follows:
Czjt ¼
X
N1
n¼0
nðtÞunðtÞuT
n ðtÞ
ð9:16Þ
TIME-OF-FLIGHT ESTIMATION OF AN ACOUSTIC TONE BURST
329

n(t) and un(t) are eigenvalues and eigenvectors of Czjt. Using (9.16) the
expression for (zjt) can be moulded into the following equivalent form:
ðzjtÞ ¼ zT
X
N1
n¼0
unðtÞuT
n ðtÞ
nðtÞ
 
!
z ¼ 
X
N1
n¼0
ðzTunðtÞÞ2
lnðtÞ
ð9:17Þ
The computational savings are obtained by discarding all terms in (9.17) that
do not capture much information about the true value of t. Suppose that n
and un are arranged according to their importance with respect to the
estimation, and that above some value of n, say J, the importance is negli-
gible. With that, the number of terms in (9.17) reduces from N to J. Experi-
ments show that J is in the order of 10. A speed up by a factor of 1000 is
feasible.
Selection of good components
The problem addressed now is how to order the eigenvectors in (9.17) such
that the most useful components come first, and thus will be selected. The
eigenvectors un(t) are orthonormal and span the whole space. Therefore:
X
N1
n¼0
ðzTunðtÞÞ2 ¼ kzk2
ð9:18Þ
Substitution in (9.17) yields:
ðzjtÞ ¼ 
X
N1
n¼0
ðzTunðtÞÞ2
nðtÞ
þ
X
N1
n¼0
ðzTunðtÞÞ2
2
v
 kzk2
2
v
¼
X
N1
n¼0
nðtÞ  2
v
nðtÞ2
v
ðzTunðtÞÞ2  kzk2
2
v
ð9:19Þ
The term containing kzk does not depend on t and can be omitted. The
maximum likelihood estimate for t appears to be equivalent to the one
that maximizes:
X
N1
n¼0
nðtÞðzTunðtÞÞ2
with
nðtÞ ¼ nðtÞ  2
v
nðtÞ2
v
ð9:20Þ
The weight n(t) is a good criterion to measure the importance of an
eigenvector. Hence, a plot of the n versus n is helpful to find a reasonable
330
WORKED OUT EXAMPLES

value of J such that J << N. Hopefully, n is large for the first few n, and
then drops down rapidly to zero.
The computational structure of the estimator based on a covariance model
A straightforward implementation of (9.20) is not very practical. The
expression must be evaluated for varying values of t. Since the dimen-
sion of Czjt is large, this is not computationally feasible.
The problem will be tackled as follows. First, we define a moving
window for the measurements zn. The window starts at n ¼ i and ends at
n ¼ i þ I  1. Thus, it comprises I samples. We stack these samples into
a vector x(i) with elements xn(i) ¼ znþi. Each value of i corresponds to a
hypothesized value t ¼ i. Thus, under this hypothesis, the vector x(i)
contains
the
direct
response
with
t ¼ 0,
i.e.
xn(i) ¼ ah(n)þ
ar(n) þ v(n). Instead of applying operation (9.20) for varying t, t is
fixed to zero and z is replaced by the moving window x(i):
yðiÞ ¼
X
J1
n¼0
nð0ÞðxðiÞTunð0ÞÞ2
ð9:21Þ
If ^i is the index that maximizes y(i), then the estimate for t is found as
^tcvm ¼ ^i.
The computational structure of the estimator is shown in Figure 9.9.
It consists of a parallel bank of J filters/correlators, one for each eigen-
vector un(0). The results of that are squared, multiplied by weight factors
n(0) and then accumulated to yield the signal y(i). It can be proven that
if we set 2
d ¼ 0, i.e. a model without reflection, the estimator degener-
ates to the classical matched filter.
γ0
correlator
u0
γ1
correlator
u1
γJ–1
correlator
uJ–1
+
z(i)
y(i )
( )2
( )2
( )2
Figure 9.9
ML estimation based on covariance models
TIME-OF-FLIGHT ESTIMATION OF AN ACOUSTIC TONE BURST
331

The design parameters of the estimators are the SNR
def
¼ 2
a/2
v, the
duration of echo generation T, the echo strength Sr
def
¼ K2
d, the number
of correlators J and the window size I.
Example
In the following example, the selected design parameters are SNR ¼
100, T ¼ 0:8 (ms), Sr ¼ 0:2 and I ¼ 1000. Using (9.11) and (9.13), we
calculate Czjt, and from that the eigenvectors and corresponding eigen-
values and weights are obtained. Figure 9.10 shows the result. As
expected the response of the first filter/correlator is similar to the direct
response, and this part just implements the conventional matched filter.
From the seventh filter on, the weights decrease, and from the fifteenth
filter on the weights are near zero. Thus, the useful number of filters is
between 7 and 15. Figure 9.11 shows the results of application of the
filters to an observed waveform.
9.2.6
Optimization and evaluation
In order to find the estimator with the best performance the best par-
ameters of the estimators must be determined. The next step then is to
assess their performances.
Cross-validation
In order to prevent overfitting we apply a three-fold cross-validation
procedure to the data set consisting of 150 records of waveforms. The
corresponding MATLAB code is given in Listing 9.8. Here, it is assumed
that the estimator under test is realized in a MATLAB function called
ToF_estimator().
The optimization of the operator using a training set occurs according
to the procedure depicted in Figure 9.12. In Listing 9.8 this is imple-
mented by calling the function opt_ToF_estimator(). The actual
code for the optimization, given in Listing 9.9, uses the MATLAB function
fminsearch().
We assume here that the operator has a bias that can be compensated
for. The value of the compensation is just another parameter of the
estimator. However, for its optimization the use of the function
fminsearch() is not needed. This would unnecessarily increase the
332
WORKED OUT EXAMPLES

search space of the parameters. Instead, we simply use the (estimated)
variance as the criterion to optimize, thereby ignoring a possible bias for
a moment. As soon as the optimal set of parameters has been found, the
corresponding bias is estimated afterwards by applying the optimized
estimator once again to the learning set.
Note, however, that the uncertainty in the estimated bias causes a
residual bias in the compensated ToF estimate. Thus, the compensation
of the bias does not imply that the estimator is necessarily unbiased.
0
5
10
15
20
25
30
10–5
10–4
10–3
10–2
10–1
100
101
n
λ (n )
0
5
10
15
20
25
30
0
2000
4000
6000
8000
10000
12000
n
γ (n )
filter responses
n = 1
i
n = 2
n = 3
i
i
n = 4
i
n = 5
i
n = 6
i
n = 7
i
n = 8
i
Figure 9.10
Eigenvalues, weights and filter responses of the covariance model
based estimator
TIME-OF-FLIGHT ESTIMATION OF AN ACOUSTIC TONE BURST
333

Therefore, the evaluation of the estimator should not be restricted to
assessment of the variance alone.
Listing 9.8
MATLAB listing for cross-validation.
load tofdata.mat;
% Load tof dataset containing 150 waveforms
Npart ¼ 3;
% Number of partitions
Nchunk ¼ 50;
% Number of waveforms in one partition
% Create 3 random partitions of the data set
observed waveform
0
response of filter: 1
0
log-likelihood after filter: 1
0
response of filter: 2
0
log-likelihood after filter: 2
0
response of filter: 3
0
log-likelihood after filter: 3
0
response of filter: 4
0
log-likelihood after filter: 4
0
response of filter: 5
0
log-likelihood after filter: 5
0
response of filter: 6
0
log-likelihood after filter: 6
0
response of filter: 7
0
log-likelihood after filter: 7
6.7
8.7
10.7
0
response of filter: 8
t(ms)
6.7
8.7
10.7
0
log-likelihood after filter: 8
t(ms)
τ cvm
Figure 9.11
Results of the estimator based on covariance models
334
WORKED OUT EXAMPLES

p ¼ randperm(Npart*Nchunk); % Find random permutation of 1:150
for n ¼ 1:Npart
for i ¼ 1:Nchunk
Zp{n,i} ¼ Zraw{p((n  1)*Nchunk þ i)};
Tp(n,i) ¼ TOFindex(p((n  1)*Nchunk þ i));
end
end
% Cross-validation
for n ¼ 1:Npart
% Create a learn set and an evaluation set
Zlearn ¼ Zp;
Tlearn ¼ Tp;
for i ¼ 1:Npart
if (i ¼ ¼ n)
Zlearn(i,:) ¼ []; Tlearn(i,:) ¼ [];
Zeval ¼ Zp(i,:); Teval ¼ Tp(i,:);
end
end
Zlearn ¼ reshape(Zlearn,(Npart-1)*Nchunk,1);
Tlearn ¼ reshape(Tlearn,1,(Npart-1)*Nchunk);
Zeval ¼ reshape(Zeval ,1, Nchunk);
% Optimize a ToF estimator
[parm,learn_variance,learn_bias] ¼ . . .
opt_ToF_estimator(Zlearn,Tlearn);
% Evaluate the estimator
for i ¼ 1:Nchunk
index(i) ¼ ToF_estimator(Zeval{i},parm);
end
variance(n) ¼ var(Teval-index);
bias(n) ¼ mean(Teval-index-learn_bias);
end
performance
evaluator
parameter
optimizer
waveforms
estimated
ToFs
performance
design
parameters
estimator
under
training
reference ToFs
training
set
Figure 9.12
Training the estimators
TIME-OF-FLIGHT ESTIMATION OF AN ACOUSTIC TONE BURST
335

Listing 9.9
MATLAB listing for the optimization of a ToF estimator.
function [parm,variance,bias] ¼ . . .
opt_ToF_estimator(Zlearn,Tlearn)
% Optimize the parameters of a ToF estimator
parm ¼ [0.15, 543, 1032]; % Initial parameters
parm ¼ fminsearch(@objective,parm,[],Zlearn,Tlearn);
[variance,bias] ¼ objective(parm,Zlearn,Tlearn);
return;
% Objective function:
% estimates the variance (and bias) of the estimator
function [variance,bias] ¼ objective(parm,Z,TOFindex)
for i ¼ 1:length(Z)
index(i) ¼ ToF_estimator(Z{i},parm);
end
variance ¼ var(TOFindex - index);
bias ¼ mean(TOFindex - index);
return
Results
Table 9.2 shows the results obtained from the cross-validation. The first
row of the table gives the variances directly obtained from the training
data during optimization. They are obtained by averaging over the three
variances of the three partitions. The second row tabulates the variances
obtained from the evaluation data (also obtained by averaging over the
three partitions). Inspection of these results reveals that the threshold
method is overfitted. The reason for this is that some of the records have
a very low signal-to-noise ratio. If – by chance – these records do not
Table 9.2
Results of three-fold cross-validation of the four ToF estimators
Envelope
thresholding
Curve
fitting
Matched
filtering
CVM based
estimator
Variance
(learn data) ðms2Þ
356
378
14339
572
Variance
(test data) ðms2Þ
181010
384
14379
591
corrected
variance ðms2Þ
180910
284
14279
491
Bias ðmsÞ
35
2
10
2
RMS ðmsÞ
427  25
17  1:2
119  7
22  1:4
336
WORKED OUT EXAMPLES

occur in the training set, then the threshold level will be set too low for
these noisy waveforms.
The reference values of the ToFs in the data set have an uncertainty of
10 (ms). Therefore, the variance estimated from the evaluation sets have
a bias of 100 (ms2). The third row in the table shows the variances after
correction for this effect.
Another error source to account for is the residual bias. This error can
be assessed as follows. The statistical fluctuations due to the finite data
set cause uncertainty in the estimated bias. Suppose that 2 is the
variance of a ToF estimator. Then, according to (5.11), the bias esti-
mated from a training set of NS samples has a variance of 2/NS. The
residual bias has an order of magnitude of /
ﬃﬃﬃﬃﬃﬃ
NS
p
. In the present case, 2
includes the variance due to the uncertainty in the reference values of the
ToFs. The calculated residual biases are given in the fourth row in Table
9.2.
The final evaluation criterion must include both the variance and the
bias. A suitable criterion is the mean square error, or equivalently the
root mean square error (RMS) defined as RMS ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
variance þ bias2
p
.
The calculated RMSs are given in the fifth row.
Discussion
Examination of Table 9.2 reveals that the four methods are ranked
according to: curve fitting, CVM1 based, matched filtering and finally
thresholding. The performance of the threshold method is far behind the
other methods. The reason of the failure of the threshold method is a
lack of robustness. The method fails for a few samples in the training set.
The robustness is simply improved by increasing the relative threshold,
but at the cost of a larger variance. For instance, if the threshold is raised
to 50%, the variance becomes around 900 (ms2).
The poor performance of the matched filter is due to the fact that it
is not able to cope with the echoes. The covariance model clearly
helps to overcome this problem. The performance of the CVM
method is just below that of curve fitting. Apparently, the covariance
model is an important improvement over the simple white noise model
of the matched filter, but still too inaccurate to beat the curve-fitting
method.
1 CVM ¼ covariance model
TIME-OF-FLIGHT ESTIMATION OF AN ACOUSTIC TONE BURST
337

One might argue that the difference between the performances of
curve fitting and CVM is not a statistically significant one. The dom-
inant factor in the uncertainty of the RMSs is brought forth by the
estimated variance. Assuming Gaussian distributions for the random-
ness, the variance of the estimation error of the variance is given by
(5.13):
Var½ b2
2 ¼ 2
NS
4
ð9:22Þ
where NS ¼ 150 is the number of samples in the data set. This error
propagates into the RMS with a standard deviation of /
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2NS
p
. The
corresponding margins are shown in Table 9.2. It can be seen that the
difference between the performances is larger than the margins, thus
invalidating the argument.
Another aspect of the design is the computational complexity of the
methods. The threshold method and the curve-fitting method both
depend on the availability of the envelope of the waveform. The quad-
rature filtering that is applied to calculate the envelope requires the
application of the Fourier transform. The waveforms in the data set are
windowed to N ¼ 8192 samples. Since N is a power of two, MATLAB’s
implementations of the fast Fourier transforms, fft() and ifft(),
have a complexity of 2N2 log2 N. The threshold method does not need
further substantial computational effort. The curve-fitting method
needs an additional numerical optimization, but since the number of
points of the curve is not large, such an optimization is not very
expensive.
A Fourier-based implementation of a correlator also has a complex-
ity of 2N2 log2 N. Thus, the computational cost of the matched filter
is in the same order as the envelope detector. The CVM based method
uses J correlators. Its complexity is ( J þ 1)N2 log2 N. Since typically
J ¼ 10, the CVM method is about 10 times more expensive than the
other methods.
In conclusion, the most accurate method for ToF estimation appears
to be the curve-fitting method with a computational complexity that is in
the order of 2N2 log2 N. With this method, ToF estimation with an
uncertainty of about 17 (ms) is feasible. The maximum likelihood esti-
mator based on a covariance model follows the curve-fitting method
closely with an uncertainty of 22 (ms). Additional modelling of the
occurrence of echoes is needed to improve its performance.
338
WORKED OUT EXAMPLES

9.3
ONLINE LEVEL ESTIMATION IN AN
HYDRAULIC SYSTEM
This third example considers the hydraulic system already introduced in
Section 8.1, where it illustrated some techniques for system identifica-
tion. Figure 8.2 shows an overview of the system. The goal in the present
section is to design an online state estimator for the two levels h1(t) and
h2(t), and for the input flow q0.
The model that appeared to be best is Torricelli’s model. In discrete
time, with x1(i)
def
¼ h1(i) and x2(i)
def
¼ h2(i), the model of (8.3)
becomes:
x1ði þ 1Þ
x2ði þ 1Þ
"
#
¼
x1ðiÞ
x2ðiÞ
"
#
þ 
C

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
R1ðx1ðiÞ  x2ðiÞÞ
p
þ q0ðiÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
R1ðx1ðiÞ  x2ðiÞÞ
p

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
R2x2ðiÞ
p
"
#
ð9:23Þ
R1 and R2 are two constants that depend on the areas of the cross-
sections of the pipelines of the two tanks. C is the capacity of the two
tanks. These parameters are determined during the system identification.
 ¼ 5 (s) is the sampling period.
The system in (9.23) is observable with only one level sensor. However,
in order to allow consistency checks (which are needed to optimize and to
evaluate the design) the system is provided with two level sensors. The
redundancy of sensors is only needed during the design phase, because once
a consistently working estimator has been found, one level sensor suffices.
The experimental data that is available for the design is a record of the
measured levels as shown in Figure 9.13. The levels are obtained by
means of pressure measurements using the Motorola MPX2010 pressure
sensor. Some of the specifications of this sensor are given in Table 9.3.
The full-scale span VFSS
corresponds to Pmax ¼ 10 kPa. In turn,
this maximum pressure corresponds to a level of hmax ¼ Pmax/g
 1000 (cm). Therefore, the linearity is specified between 10 (cm)
and þ10 (cm). This specification is for the full range. In our case, the
swing of the levels is limited to 20 (cm), and the measurement system
was calibrated at 0 (cm) and 25 (cm). Therefore, the linearity will be
much less. The pressure hysteresis is an error that depends on whether
the pressure is increasing or decreasing. The pressure hysteresis can induce
a maximal level error between 1 (cm) and þ1 (cm). Besides these
sensor errors, the measurements are also contaminated by electronic noise
ONLINE LEVEL ESTIMATION IN AN HYDRAULIC SYSTEM
339

modelled
by
white
noise
v(i)
with
a
standard
deviation
of
v ¼ 0:04 (cm). With that, the model of the sensory system becomes:
z1ðiÞ
z2ðiÞ


¼
x1ðiÞ
x2ðiÞ


þ
e1ðiÞ
e2ðiÞ


þ
v1ðiÞ
v2ðiÞ


ð9:24Þ
or more concisely: z(i) ¼ Hx(i) þ e(i) þ v(i) with H ¼ I. The error e(i)
represents the linearity and hysteresis error.
In the next sections, the linearized Kalman filter, the extended Kalman
filter and the particle filter will be examined. In all three cases a model is
needed for the input flow q0(i). The linearized Kalman filter can only
handle linear models. The extended Kalman filter can handle nonlinear
models, but only if the nonlinearities are smooth. Particle filtering offers
the largest freedom of modelling. All three models need parameter
estimation in order to adapt the model to the data. Consistency checks
must indicate which estimator is most suitable.
Table 9.3
Specifications of the MPX2010 pressure sensor
Characteristic
Symbol
Min
Typ
Max
Unit
Pressure range
P
0
—
10
kPa
Full-scale span
VFSS
24
25
26
mV
Sensitivity
V=P
—
2.5
—
mV/kPa
Linearity error
—
1:0
—
1.0
%VFSS
Pressure hysteresis
—
—
0:1
—
%VFSS
0
500
1000
1500
2000
2500
3000
3500
0
2
4
6
8
10
12
14
16
18
20
measured levels (cm)
t (s)
Figure 9.13
Measured levels of two interconnected tanks
340
WORKED OUT EXAMPLES

The prior knowledge that we assume is that the tanks at i ¼ 0 are
empty.
9.3.1
Linearized Kalman filtering
The simplest dynamic model for q0(i) is the first order AR model:
q0ði þ 1Þ ¼ q0 þ  q0ðiÞ  q0
	

þ wðiÞ
ð9:25Þ
q0 is a constant input flow that maintains the equilibrium in the tanks.
The Gaussian process noise w(i) causes random perturbations around
this equilibrium. The factor  regulates the bandwidth of the fluctu-
ations of the input flow. The design parameters are q0, w and .
The augmented state vectors are x(i) ¼ [ x1(i) x2(i) q0(i) ]. Equations
(9.23)
and
(9.25)
make
up
the
augmented
state
equation
x(i þ 1) ¼ f(x(i), w(i)), which is clearly nonlinear. The equilibrium fol-
lows from equating x ¼ f x, 0
	

:
x1
x2
"
#
¼
x1
x2
"
#
þ 
C

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
R1 x1  x2
	

q
þ q0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
R1 x1  x2
	

q

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
R2x2
p
2
64
3
75
)
x2 ¼ q
2
0=R2
x1 ¼ R1 þ R2
R1
x2
ð9:26Þ
The next step is to calculate the Jacobian matrix of f():
FðxÞ ¼ qfð Þ
qx ¼
1 
R1
2C
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
R1 x1x2
ð
Þ
p
R1
2C
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
R1ðx1x2Þ
p

C
R1
2C
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
R1ðx1x2Þ
p
1 
R1
2C
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
R1ðx1x2Þ
p

R2
2C
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
R2x2Þ
p
0
0
0

2
6664
3
7775
ð9:27Þ
The linearized model arises by application of a truncated Taylor series
expansion around the equilibrium
xði þ 1Þ ¼ x þ FðxÞ xðiÞ  x
	

þ GwðiÞ
ð9:28Þ
with G ¼ [ 0
0
1 ]T.
ONLINE LEVEL ESTIMATION IN AN HYDRAULIC SYSTEM
341

The three unknown parameters q0,  and w must be estimated
from the data. Various criteria can be used to find these parameters,
but in most of these criteria the innovations play an important role.
For a proper design the innovations are a white Gaussian sequence of
random vectors with zero mean and known covariance matrix, i.e. the
innovation matrix. See Section 8.4. In the sequel we will use the NIS
(normalized innovations squared; Section 8.4.2). For a consistent
design, the NIS must have a 	2
N distribution. In the present case
N ¼ 2. The expectation of a 	2
N-distributed variable is N. Thus, here,
ideally the mean of the NIS is 2. A simple criterion is one which
drives the average of the calculated NIS to 2. A possibility is:
Jðq0; ; wÞ ¼ 1
I
X
I1
i¼0
NisðiÞ  2


ð9:29Þ
The values of q0,  and w that minimize J are considered optimal.
The strategy is to realize a MATLAB function J ¼ flinDKF(y) that
implements the linearized Kalman filter, applies it to the data, calculates
Nis(i) and returns J according to eq. (9.29). The input argument y is an
array containing the variables q0,  and w. Their optimal values are
found by one of MATLAB’s optimization functions, e.g. [parm, J] ¼
fminsearch(@flinDKF, [20 0:95 5]).
Application of this strategy revealed an unexpected phenomenon. The
solution of (q0, , w) that minimizes J is not unique. The values of  and
w do not influence J as long as they are both sufficiently large. For
instance, Figure 9.14 shows the results for  ¼ 0:98 and w ¼ 1000
(cm3/s). But any value of  and w above this limit gives virtually the
same results and the same minimum. The minimum obtained is J ¼ 25:6.
This is far too large for a consistent solution. Also the NIS, shown in
Figure 9.14, does not obey the statistics of a 	2
2 distribution. During the
transient, in the first 200 seconds, the NIS reaches extreme levels indi-
cating that some unmodelled phenomena occur there. But also during
the remaining part of the process the NIS shows some unwanted high
peaks. Clearly the estimator is not consistent.
Three modelling errors might explain the anomalous behaviour of the
linearized Kalman filter. First, the linearization of the system equation
might fail. Second, the AR model of the input flow might be inappropri-
ate. Third, the ignorance of possible linearization errors of the sensors
might not be allowed. In the next two sections, the first two possible
explanations will be examined.
342
WORKED OUT EXAMPLES

9.3.2
Extended Kalman filtering
Linearization errors of the system equation are expected to be influential
when the levels deviate largely from the equilibrium state. The abnor-
mality during the transient in Figure 9.14 might be caused by this kind of
error because there the nonlinearity is strongest. The extended Kalman
filter is able to cope with smooth linearity errors. Therefore, this method
might give an improvement.
In order to examine this option, a MATLAB function J ¼ fextDKF(y)
was realized that implements the extended Kalman filter. Again, y is an
array containing the variables q0,  and w. Application to the data,
calculation of J and minimization with fminsearch() yields estimates
as shown in Figure 9.15.
The NIS of the extended Kalman filter, compared with that of the
linearized Kalman filter, is much better now. The optimization criter-
ion J attains a minimal value of 3.6 instead of 25.6; a significant
improvement has been reached. However, the NIS still doesn’t obey a
	2
2 distribution. Also, the optimization of the parameters q0,  and w
is not without troubles. Again, the minimum is not unique. Any solu-
tion of  and w satisfies as long as both parameters are sufficiently
0
500
1000
1500
2000
2500
3000
3500
0
5
10
15
20
estimated levels (solid) and measurements (dotted) (cm)
0
500
1000
1500
2000
2500
3000
3500
0
50
100
150
estimated input flow (cm3/s)
0
500
1000
1500
2000
2500
3000
3500
0
100
200
300
nis
t (s)
Figure 9.14
Results from the linearized Kalman filter
ONLINE LEVEL ESTIMATION IN AN HYDRAULIC SYSTEM
343

large. Moreover, it now appears that the choice of q0 does not have any
influence at all.
The explanation of this behaviour is as follows. First of all, we
observe that in the prediction step of the Kalman filter a large value of
w induces a large uncertainty in the predicted input flow. As a result,
the prediction of the level in the first tank is also very uncertain.
In the next update step, the corresponding Kalman gain will be close
to one, and the estimated level in the first tank will closely follow the
measurement, x1(iji)  z1(i). If  is sufficiently large, so that the
autocorrelation in the sequence q0(i) is large, the estimate q0(iji) is
derived
from
the
differences
of
succeeding
samples
x1(iji)
and
x1(i  1ji  1). Since x1(iji)  z1(i) and x1(i  1ji  1)  z1(i  1), the
estimate q0(iji) only depends on measurements. The value of q0 does
not influence that. The conclusion is that the AR model does not
provide useful prior knowledge for the input flow.
9.3.3
Particle filtering
A visual inspection of the estimated input flow in Figure 9.15 indeed
reveals that an AR model does not fit well. The characteristic of the
0
5
10
15
20
estimated levels (solid) and measurements (dotted) in (cm)
0
500
1000
1500
2000
2500
3000
3500
0
50
100
estimated input flow (cm3/s)
0
500
1000
1500
2000
2500
3000
3500
0
50
100
nis
t (s)
0
500
1000
1500
2000
2500
3000
3500
Figure 9.15
Results from the extended Kalman filter
344
WORKED OUT EXAMPLES

input flow is more like that of a random binary signal modelled by
two
discrete
states
fqmin, qmaxg,
and
a
transition
probability
Pt(q0(i)jq0(i  1)); see Section 4.3.1. The estimated flow in Figure 9.15
also suggests that qmin ¼ 0. This actually models an on/off control
mechanism of the input flow. With this assumption, the transition
probability is fully defined by two probabilities:
Pup ¼ Ptðq0ðiÞ ¼ qmaxjq0ði  1Þ ¼ 0Þ
Pdown ¼ Ptðq0ðiÞ ¼ 0jq0ði  1Þ ¼ qmaxÞ
ð9:30Þ
The unknown parameters of the new flow model are qmax, Pup and
Pdown. These parameters must be retrieved from the data by optimizing
some consistency criterion of an estimator. Kalman filters cannot cope
with binary signals. Therefore, we now focus on particle filtering,
because this method is able to handle discrete variables (see Section 4.4).
Again, the strategy is to realize a MATLAB function J ¼ fpf(y) that
implements a particle filter, applies it to the data and returns a criterion J.
The input argument y contains the design parameters qmax, Pup and
Pdown. The output J must express how well the result of the particle filter
is consistent. Minimization of this criterion gives the best attainable result.
An important issue is how to define the criterion J. The NIS, which
was previously used, exists within the framework of Kalman filters, i.e.
for linear-Gaussian systems. It is not trivial to find a concept within the
framework of particle filters that is equivalent to the NIS. The general
idea is as follows. Suppose that, using all previous measurement Z(i  1)
up
to
time
i  1,
the
probability
density
of
the
state
x(i)
is
p(x(i)jZ(i  1)). Then, the probability of z(i) is:
pðzðiÞjZði  1ÞÞ ¼
Z
x
pðzðiÞ; xðiÞjZði  1ÞÞdx
¼
Z
x
pðzðiÞjxðiÞÞpðxðiÞjZði  1ÞÞdx
ð9:31Þ
The density p(z(i)jx(i)) is simply the model of the sensory system and as
such known. The probability p(x(i)jZ(i  1)) is represented by the pre-
dicted samples. Therefore, using (9.31) the probability p(z(i)jZ(i  1))
can be calculated. The filter is consistent only if the sequence of observed
measurement z(i) obeys the statistics prescribed by the sequence of
densities p(z(i)jZ(i  1)).
A test of whether all z(i) comply with p(z(i)jZ(i  1)) is not easy, because
p(z(i)jZ(i  1)) depends on i. The problem will be tackled by treating each
ONLINE LEVEL ESTIMATION IN AN HYDRAULIC SYSTEM
345

scalar measurement separately. We consider the n-th element zn(i) of the
measurement vector, and assume that pn(z, i) is its hypothesized marginal
probability density. Suppose that the cumulative distribution of zn(i) is
Fn(z, i) ¼
R z
1 pn(
, i)d
. Then the random variable un(i) ¼ Fn(zn(i), i) has
a uniform distribution between 0 and 1. The consistency check boils down
to testing whether the set fun(i)ji ¼ 0, . . . ,I  1 and n ¼ 1, . . . ,Ng indeed
has such a uniform distribution.
In the literature, the statistical test of whether a set of variables has a
given distribution function is called a goodness-of-fit test. There are
various methods for performing such a test. A particular one is the chi-
square test and is as follows (Kreyszig, 1970):
Algorithm 9.1: Goodness of fit (Chi-square test for the uniform
distribution)
Input: a set of variables that are within the interval [0, 1]. The size of the
set is B.
1. Divide the interval [0, 1] into a number of L equally spaced contain-
ers and count the number of times that the random variable falls in
the ‘-th container. Denote this count by b‘. Thus, PL
‘¼1 b‘ ¼ B.
(Note: L must be such that b‘ > 5 for each ‘.)
2. Set e ¼ B/L. This is the expected number of variables in one container.
3. Calculate the test variable J ¼ PL
‘¼1
(b‘e)2
e
.
4. If the set is uniformly distributed, then J has a 	2
L1 distribution.
Thus, if J is too large to be compatible with the 	2
L1 distribution, the
hypothesis of a uniform distribution must be rejected. For instance, if
L ¼ 20, the probability that J > 43:8 equals 0.001.
The particular particle filter that was implemented is based on the
condensation algorithm described in Section 4.4.3. The MATLAB code
is given in Listing 9.10. Some details of the implementation follow next.
See Algorithm 4.4.
. The prior knowledge that we assume is that at i ¼ 0 both tanks are
empty, that is x1(0) ¼ x2(0) ¼ 0. The probability that the input
flow is ‘on’ or ‘off’ is 50/50.
. During the update, the importance weights should be set to
w(k) ¼ p(z(i)jx(k)). Assuming a Gaussian distribution of the meas-
urement errors, the weights are calculated as w(k) ¼ exp ( 1
2 (z(i)
x(k))TC1
v (z(i)  x(k))). The normalizing constant of the Gaussian
346
WORKED OUT EXAMPLES

can be ignored because the weights are going to be normalized
anyway.
. ‘Finding the smallest j such that w(j)
cum  r(k)’ is implemented by the
bisection method of finding a root using the golden rule.
. In the prediction step, ‘finding the samples x(k) drawn from the
density p(x(i)jx(i  1) ¼ x(k)
selected)’ is done by generating random
input flow transitions from ‘on’ to ‘off’ and from ‘off’ to ‘on’
according to the transition probabilities Pdown and Pup, and to
apply
these
to
the
state
equation
(implemented
by
f(Ys, R1,R2)). However, the time-discrete model only allows
the transitions to occur at exactly the sampling periods ti ¼ i.
In the real time-continuous world, the transitions can take place
anywhere in the time interval between two sampling periods. In
order to account for this effect, the level of the first tank is
randomized with a correction term randomly selected between 0
and qmax/C. Such a correction is only needed for samples where
a transition takes place.
. The conditional mean, approximated by ^x(i) ¼ P
k
w(k)x(k)= P
k
w(k),
is used as the final estimate.
. The probability density p(z(i)jZ(i  1)) is represented by samples
z(k), which are derived from the predicted samples x(k) according to
the model z(k) ¼ Hx(k) þ v(k) where v(k) is Gaussian noise with
covariance matrix Cv. The marginal probabilities pn(z, i) are then
represented by the scalar samples z(k)
n . The test variables un(i) are
simply obtained by counting the number of samples for which
z(k)
n < zn(i) and dividing it by K, the total number of samples.
Listing 9.10
MATLAB listing of a function implementing the condensation algorithm.
function J ¼ fpf (y)
load hyddata.mat;
% Load the dataset (measurements in Z)
I ¼ length (Z);
% Length of the sequence
R1 ¼ 105.78;
% Friction constant (cm^5/s^2)
R2 ¼ 84.532;
% Friction constant (cm^5/s^2)
qmin ¼ 0;
% Minimal input flow
delta ¼ 5;
% Sampling period (s)
C ¼ 420;
% Capacity of tank (cm^2)
sigma_v ¼ 0.04;
% Standard deviation sensor noise (cm)
Ncond ¼ 1000;
% Number of particles
M ¼ 3;
% Dimension of state vector
N ¼ 2;
% Dimension of measurement vector
ONLINE LEVEL ESTIMATION IN AN HYDRAULIC SYSTEM
347

% Set the design parameters
qmax ¼ y(1);
% Maximum input flow
Pup ¼ y(2);
% Transition probability up
Pdn ¼ y(3);
% Transition probability down
% Initialisation
hmax ¼ 0; hmin ¼ 0;
% Margins of levels at i ¼ 0
H ¼ eye(N,M);
% Measurement matrix
invCv ¼ inv(sigma_v^2 * eye(N));
% Inv. cov. of sensor noise
% Generate the samples
Xs(1,:) ¼ hmin þ (hmaxhmin)*rand(1,Ncond);
Xs(2,:) ¼ hmin þ (hmaxhmin)*rand(1,Ncond);
Xs(3,:) ¼ qmin þ (qmaxqmin)*(rand(1,Ncond)> 0:5);
for i ¼ 1:I
% Generate predicted meas. representing p(z(i)|Z(i-1))
Zs ¼ H*Xs þ sigma_v*randn(2,Ncond);
% Get uniform distributed rv
u(1,i) ¼ sum((Zs(1,:) < Z(1,i)))/Ncond;
u(2,i) ¼ sum((Zs(2,:) < Z(2,i)))/Ncond;
% Update
res ¼ H*Xs  Z(:,i)*ones(1,Ncond);
% Residuals
W ¼ exp(0:5*sum(res.*(invCv*res)))’;
% Weights
if (sum(W) ¼¼ 0), error(’process did not converge’); end
W ¼ W/sum(W); CumW ¼ cumsum(W);
xest(:,i) ¼ Xs(:,:)*W;
% Sample mean
% Find an index permutation using golden rule root finding
for j ¼ 1:Ncond
R ¼ rand; ja ¼ 1; jb ¼ Ncond;
while (ja < jb1)
jx ¼ floor(jb0:382*(jbja));
fa ¼ R  CumW(ja); fb ¼ R  CumW(jb); fxx ¼ R  CumW(jx);
if (fb*fxx < 0), ja ¼ jx; else, jb ¼ jx; end
end
ind(j) ¼ jb;
end
% Resample
for j ¼ 1:Ncond, Ys(:,j) ¼ Xs(:,ind(j)); end
% Predict
Tdn ¼ (rand(1,Ncond)<Pdn);
% Random transitions
348
WORKED OUT EXAMPLES

Tup ¼ (rand(1,Ncond)<Pup);
% idem
kdn ¼ find((Ys(3,:) ¼¼ qmax) & Tdn);
% Samples going down
kup ¼ find((Ys(3,:) ¼¼ qmin) & Tup);
% Samples going up
Ys(3,kdn) ¼ qmin;
% Turn input flow off
Ys(1,kdn) ¼ Ys(1,kdn) þ . . .
% Randomize level 1
(qmax  qmin)*delta*rand(1,length(kdn))/C;
Ys(3,kup) ¼ qmax;
% Turn input flow on
Ys(1,kup) ¼ Ys(1,kup)  . . .
% Randomize level 1
(qmaxqmin)*delta*rand(1,length(kup))/C;
Xs ¼ f(Ys,R1,R2);
% Update samples
end
e ¼ I/10;
% Expected number of rv in
one bin
% Get histograms (10 bins) and calculate test variables
for i ¼ 1:2, b ¼ hist(u(i,:)); c(i) ¼ sum((be).^2/e); end
J ¼ sum(c);
% Full test (chi-square
with 19 DoF)
return
Figure 9.16 shows the results obtained using particle filtering. With the
number of samples set to 1000, minimization of J with respect to
qmax, Pup and Pdown yielded a value of about J ¼ 1400; a clear indication
0
500
1000
1500
2000
2500
3000
3500
0
5
10
15
20
estimated levels (solid) and measurements (dotted) (cm)
0
500
1000
1500
2000
2500
3000
3500
0
50
100
estimated input flow (cm3/s)
0
500
1000
1500
2000
2500
3000
3500
0
0.5
1
test variables u1(i) and u2(i)
t(s)
Figure 9.16
Results from particle filtering
ONLINE LEVEL ESTIMATION IN AN HYDRAULIC SYSTEM
349

that the test variables u1ðiÞ and u2ðiÞ are not uniformly distributed
because J should have a 	2
19-distribution. The graph in Figure 9.16,
showing these test variables, confirms this statement. The values of Pup
and Pdown that provided the minimum were about 0.3. This minimum
was very flat though.
The large transition probabilities that are needed to minimize the criter-
ion indicate that we still have not found an appropriate model for the input
flow. After all, large transition probabilities mean that the particle filter
has much freedom of selecting an input flow that fits with the data.
9.3.4
Discussion
Up to now we have not succeeded in constructing a consistent estimator.
The linearized Kalman filter failed because of the nonlinearity of the
system, which is simply too severe. The extended Kalman filter was an
improvement, but its behaviour was still not regular. The filter could
only be put to action if the model for the input flow was unrestrictive,
but the NIS of the filter was still too large. The particle filter that we
tested used quite another model for the input flow than the extended
Kalman filter, but yet this filter also needed an unrestrictive model for
the input flow.
At this point, we must face another possibility. We have adopted a
linear model for the measurements, but the specifications of the sensors
mention linearity and hysteresis errors eðiÞ which can – when measured
over the full-scale span – become quite large. Until now we have ignored
the error eðiÞ in (9.24), but without really demonstrating that doing so is
allowed. If it is not, that would explain why both the extended Kalman
filter and the particle filter only work out with unrestrictive models for
the input signal. In trying to get estimates that fit the data from both
sensors the estimators cannot handle any further restrictions on the input
signal.
The best way to cope with the existence of linearity and hysteresis errors
is to model them properly. A linearity error – if reproducible – can be
compensated by a calibration curve. A hysteresis error is more difficult to
compensate because it depends on the dynamics of the states. In the
present case, a calibration curve can be deduced for the second sensor
using the previous results. In the particle filter of Section 9.3.3 the estimate
of the first level is obtained by closely following the measurements in the
first tank. The estimates of the second level are obtained merely by using
350
WORKED OUT EXAMPLES

the model of the hydraulic system. Therefore, these estimates can be used
as a reference for the measurements in the second tank.
Figure 9.17 shows a scatter diagram of the errors versus estimated
levels of the second tank. Of course, the data is contaminated by mea-
surement noise, but nevertheless the trend is appreciable. Using
MATLAB’s polyfit() and polyval() polynomial regression has
been applied to find a curve that fits the data. The resulting polynomial
model is used to compensate the linearity errors of the second sensor.
The results, illustrated in Figure 9.18, show test variables that are much
more uniformly distributed than the same variables in Figure 9.16. The
chi-square test gives a value of J ¼ 120. This is a significant improve-
ment, but nevertheless still too much for a consistent filter. However,
Figure 9.18 only goes to show that the compensation of the linearity
errors of the sensors do have a large impact.
For the final design, the errors of the first sensor should also be
compensated. Additional measurements are needed to obtain the cali-
bration curve for that sensor. Once this curve is available, the design
procedure as described above starts over again, but this time with
compensation of linearity errors included. There is no need to reconsider
the linearized Kalman filter because we have already seen that its linear-
ization errors are too severe.
0
1
2
3
4
5
6
7
–0.4
–0.3
–0.2
–0.1
0
0.1
0.2
0.3
0.4
x2
e2 (cm)
(cm)
∧
Figure 9.17
Calibration curve for the second sensor
ONLINE LEVEL ESTIMATION IN AN HYDRAULIC SYSTEM
351

9.4
REFERENCES
Harrison, D. and Rubinfeld, D.L., Hedonic prices and the demand for clean air, Journal
of Environmental Economics and Management, 5, 81–102, 1978.
Heijden, F. van der, Tu´querres, G. and Regtien, P.P.L., Time-of-flight estimation based
on covariance models, Measurement Science and Technology, 14, 1295–304, 2003.
Kreyszig, E., Introductory Mathematical Statistics – Principles and Methods, Wiley,
New York, 1970.
0
500
1000
1500
2000
2500
3000
3500
0
5
10
15
20
estimated levels (solid) and measurements (dotted) (cm)
0
500
1000
1500
2000
2500
3000
3500
0
50
100
estimated input flow (cm3/s)
0
500
1000
1500
2000
2500
3000
3500
0
0.5
1
test variables u1(i) and u2(i)
t(s)
Figure 9.18
Results from particle filtering after applying a linearity correction
352
WORKED OUT EXAMPLES

Appendix A
Topics Selected from
Functional Analysis
This appendix summarizes some concepts from functional analysis. The
concepts are part of the mathematical background required for under-
standing this book. Mathematical peculiarities not relevant in this
context are omitted. Instead, at the end of the appendix references to
more detailed treatments are given.
A.1
LINEAR SPACES
A linear space (or vector space) over a field F is a set R with elements
(vectors) f, g, h, . . . equipped with two operations:
. addition (f þ g):
R  R ! R
. scalar multiplication (f with  2 F): F  R ! R
Usually, the field F is the set R of real numbers, or the set C of complex
numbers. The addition and the multiplication operation must satisfy the
following axioms:
(a) f þ g ¼ g þ f
(b) (f þ g) þ h ¼ f þ (g þ h)
Classification, Parameter Estimation and State Estimation: An Engineering Approach using MATLAB
F. van der Heijden, R.P.W. Duin, D. de Ridder and D.M.J. Tax
 2004 John Wiley & Sons, Ltd
ISBN: 0-470-09013-8

(c) a so-called zero element 0 2 R exists such that f þ 0 ¼ f
(d) a negative element f exists for each f such that f þ (  f) ¼ 0
(e) (f þ g) ¼ f þ g
(f) ( þ )f ¼ f þ f
(g) ()f ¼ (f)
(h) 1f ¼ f
A linear subspace S of a linear space R is a subset of R which itself is
linear. A condition sufficient and necessary for a subset S  R to be
linear is that f þ g 2 S for all f, g 2 S and for all ,  2 F.
Examples
C[a,b] is the set of all complex functions f(x) continuous in the
interval [a,b]. With the usual definition of addition and scalar multi-
plication this set is a linear space.1 The set of polynomials of degree N:
fðxÞ ¼ c0 þ c1x þ c2x2 þ    þ cNxN
with
cn 2 C
is a linear subspace of C[a,b].
The set R1 consisting of an infinite, countable series of real num-
bers f ¼ (f0, f1, . . . ) is a linear space provided that the addition and
multiplication takes place element by element.1 The subset of R1 that
satisfies the convergence criterion:
X
1
n¼0
jfnj2 < 1
is a linear subspace of R1.
The set RN consisting of N real numbers f ¼ (f0, f1, . . . fN1) is a
linear space provided that the addition and multiplication takes place
element by element. Any linear hyperplane containing the null vector
(zero element) is a linear subspace.
1 Throughout this appendix the examples relate to vectors which are either real or complex.
However, these examples can be converted easily from real to complex or vice versa. The set of
all real functions continuous in the interval [a,b] is denoted by R[a,b]. The set of infinite and
finite countable complex numbers is denoted by C1 and CN, respectively.
354
APPENDIX A

Any vector that can be written as:
f ¼
X
n1
i¼0
ifi
i 2 F
ða:1Þ
is called a linear combination of f0, f1, . . . , fn1. The vectors f0, f1, . . . , fm1
are linear dependent if a set of numbers i exists, not all zero, for which
the following equation holds:
X
m1
i¼0
ifi ¼ 0
ða:2Þ
If no such set exists, the vectors f0, f1, . . . , fm1 are said to be linear
independent.
The dimension of a linear space R is defined as the non-negative integer
number N for which N independent linear vectors in R exist, while any set
of N þ 1 vectors in R is linear dependent. If for each number N there exist
N linear independent vectors in R, the dimension of R is 1.
Example
C[a,b] and R1 have dimension 1. RN has dimension N.
A.1.1
Normed linear spaces
A norm kfk of a linear space is a mapping R ! R (i.e. a real function or a
functional) such that:
(a) kfk  0, where kfk ¼ 0 if and only if f ¼ 0
(b) kfk ¼ jjkfk
(c) kf þ gk  kfk þ kgk
A linear space equipped with a norm is called a normed linear space.
Examples
The following real functions satisfy the axioms of a norm:
In C[a,b]:
kfðxÞkp ¼
Z b
x¼a
jfðxÞjpdx
 
!1
p
with: p  1
ða:3Þ
LINEAR SPACES
355

In R1:
kfkp ¼
X
1
n¼0
jfnjp
 
!1
p
with: p  1
ða:4Þ
In RN:
kfkp ¼
X
N1
n¼0
jfnjp
 
!1
p
with: p  1
ða:5Þ
These norms are called the Lp norm (continuous case, e.g. C[a,b]) and the
lp norm (discrete case, e.g. C1). Graphical representations of the norm are
depicted in Figure A.1. Special cases occur for particular choices of the
parameter p. If p ¼ 1, the norm is the sum of the absolute magnitudes, e.g.
kfðxÞk1 ¼
Z b
x¼a
jfðxÞjdx
and
kfk1 ¼
X
n
jfnj
ða:6Þ
If p ¼ 2, we have the Euclidean norm:
kfðxÞk2 ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Z b
x¼a
jfðxÞj2dx
s
and
kfk2 ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
X
n
jfnj2
r
ða:7Þ
In R3 and R2 the norm kfk2 is the length of the vector as defined in
geometry. If p ! 1, the Lp and lp norms are the maxima of the absolute
magnitudes, e.g.
kfðxÞk1 ¼ max
x2½a;b jfðxÞj
and
kfk1 ¼ max
n
jfnj
ða:8Þ
p = 1
p = 1.2
p = 2
p = 10
Figure A.1
‘Circles’ in R2 equipped with the lp norm
356
APPENDIX A

A.1.2
Euclidean spaces or inner product spaces
A Euclidean space (also called inner product space) R is a linear space
for which the inner product is defined. The inner product (f, g) between
two vectors f, g 2 R over a field F is a mapping R  R ! F that satisfies
the following axioms:
(a) (f þ g,h) ¼ (f,h) þ (g,h)
(b) (f,g) ¼ (f,g)
(c) (g,f) ¼ (f,g)
(d) (f,f)  0, real
(e) (f,f) ¼ 0 ) f ¼ 0
In (c) the number (f, g) is the complex conjugated of (f, g). Of course, this
makes sense only if F is complex. If F is the set of real numbers, then
(g, f) ¼ (f, g).
Examples
C[a,b] is a Euclidean space if the inner product is defined as:
ðfðxÞ; gðxÞÞ ¼
Z b
x¼a
fðxÞgðxÞdx
R1 is a Euclidean space if with f ¼ (f0, f1, . . . ) and g ¼ (g0, g1, . . . ):
ðf; gÞ ¼
X
1
n¼0
figi
RN
is
a
Euclidean
space
if
with
f ¼ (f0, f1, . . . , fN1)
and
g ¼ (g0, g1, . . . , gN1):
ðf; gÞ ¼
X
N1
n¼0
figi
In accordance with (a.7), any Euclidean space becomes a normed linear
space as soon as it is equipped with the Euclidean norm:
kfk ¼ þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðf; fÞ
q
ða:9Þ
LINEAR SPACES
357

Given this norm, any two vectors f and g satisfy the Schwarz inequality:
jðf; gÞj  kfkkgk
ða:10Þ
Two vectors f and g are said to be orthogonal (notation: f?g) whenever
(f,g) ¼ 0. If two vectors f and g are orthogonal then (Pythagoras):
kf þ gk2 ¼ kfk2 þ kgk2
ða:11Þ
Given a vector g 6¼ 0, an arbitrary vector f can be decomposed into a
component that coincides with g and an orthogonal component:
f ¼ g þ h with g?h. The scalar  follows from:
 ¼ ðf; gÞ
ðg; gÞ ¼ ðf; gÞ
kgk2
ða:12Þ
The term g is called the projection of f on g. The angle ’ between two
vectors f and g is defined such that it satisfies:
cosð’Þ ¼ ðf; gÞ
kfkkgk
ða:13Þ
A.2
METRIC SPACES
A metric space R is a set equipped with a distance measure (f, g)
that maps any couple of elements f,g 2 R into a non-negative real
number: R  R ! Rþ. The distance measure must satisfy the following
axioms:
(a) (f,g) ¼ 0 if and only if f ¼ g
(b) (f,g) ¼ (g,f)
(c) (f,h)  (f,g) þ (g,h)
All normed linear spaces become a metric space, if we set:
ðf; gÞ ¼ kf  gk
ða:14Þ
358
APPENDIX A

Consequently, the following mappings satisfy the axioms of a distance
measure. In C[a,b]:
ðfðxÞ; gðxÞÞ ¼
Z b
x¼a
jfðxÞ  gðxÞjpdx
 
!1
p
with:
p  1
ða:15Þ
In R1:
ðf; gÞ ¼
X
1
n¼0
jfn  gnjp
 
!1
p
with:
p  1
ða:16Þ
In RN:
ðf; gÞ ¼
X
N1
n¼0
jfn  gnjp
 
!1
p
with:
p  1
ða:17Þ
These are the Minkowski distances. A theorem related to these measures
is Minkowski’s inequality. In RN and C1 this equality states that:2
X
n
jfn þ gnjp
 
!1
p

X
n
jfnjp
 
!1
p
þ
X
n
jgnjp
 
!1
p
ða:18Þ
Special cases occur for particular choices of the parameter p. If p ¼ 1,
the distance measure equals the sum of absolute differences between the
various coefficients, e.g.
ðfðxÞ; gðxÞÞ ¼
Z b
x¼a
jfðxÞ  gðxÞjdx
and
ðf; gÞ ¼
X
n
jfn  gnj
ða:19Þ
This measure is called the city-block distance (also known as Manhattan,
magnitude, box-car or absolute value distance). If p ¼ 2, we have the
ordinary Euclidean distance measure. If p ! 1 the maximum of the
absolute differences between the various coefficients is determined, e.g.
2 In C[a,b] the inequality is similar.
METRIC SPACES
359

ðfðxÞ; gðxÞÞ ¼ max
x2½a;b jfðxÞ  gðxÞj
and
ðf; gÞ ¼ max
n
jfn  gnj
ða:20Þ
This measure is the chessboard distance (also known as Chebyshev or
maximum value distance).
The quadratic measure is defined as:
ðf; gÞ ¼ þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðf  g; Af  AgÞ
q
ða:21Þ
where A is a self-adjoint operator with non-negative eigenvalues. This
topic will be discussed in Section A.4 and Section B.5. This measure finds
its application in, for instance, pattern classification (where it is called
the Mahalanobis distance).
Another distance measure is:
ðf; gÞ ¼
1
if f ¼ g
0
if f 6¼ g

ða:22Þ
An application of this measure is in Bayesian estimation and classifica-
tion theory where it is used to express a particular cost function. Note
that in contrast with the preceding examples this measure cannot be
derived from a norm. For every metric derived from a norm we have
(f,0) ¼ jj(f,0). However, this equality does not hold for (a.22).
A.3
ORTHONORMAL SYSTEMS AND FOURIER
SERIES
In a Euclidean space R with the norm given by (a.9), a subset S  R is an
orthogonal system if every couple bi,bj in S is orthogonal; i.e. (bi,bj) ¼ 0
whenever i 6¼ j. If in addition each vector in S has unit length, i.e.
kbik ¼ 1, then S is called an orthonormal system.
Examples
In C[a,b] the following harmonic functions form an orthonormal
system:
wnðxÞ ¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
b  a
p
exp 2jnx
b  a


with: j ¼
ﬃﬃﬃﬃﬃﬃﬃ
1
p
ða:23Þ
360
APPENDIX A

In CN the following vectors are an orthonormal system:
wn ¼
1ﬃﬃﬃﬃﬃ
N
p
1ﬃﬃﬃﬃﬃ
N
p
exp 2jn
N


   1ﬃﬃﬃﬃﬃ
N
p
exp 2jðN  1Þn
N




ða:24Þ
Let S ¼ f w0
w1 . . . wN1 g be an orthonormal system in a Euclidean
space R, and let f be an arbitrary vector in R. Then the Fourier coeffi-
cients of f with respect to S are the inner products:
k ¼ ðf; wkÞ
k ¼ 0; 1; . . . ; N  1
ða:25Þ
Furthermore, the series:
X
N1
k¼0
kwk
ða:26Þ
is called the Fourier series of f with respect to the system S. Suppose we
wish to approximate f by a suitably chosen linear combination of the
system S. The best approximation (according to the norm in R) is given
by (a.26). This follows from the following inequality:
f 
X
N1
k¼0
kwk

 
f 
X
N1
k¼0
kwk


for arbitrary k
ða:27Þ
The approximation improves as the number N of vectors increases. This
follows readily from Bessel’s inequality:
X
N1
k¼0
j’kj2  kfk2
ða:28Þ
Let S ¼ f w0 w1 . . . g be an orthonormal system in a Euclidean space R.
The number of vectors in S may be infinite. Suppose that no vector ~w
exists for which the system S augmented by ~w, i.e. ~S ¼ f ~w w0 w1
. . . g is
also an orthonormal system. Then, S is called an orthonormal basis. In
that case, the smallest linear subspace containing S is the whole space R.
The number of vectors in an orthonormal basis S may be finite (as in
RN and CN), countable infinite (as in R1, C1, R[a, b], and C[a, b] with
1 < a < b < 1), or uncountable infinite (as in R[1,1], and
C[1, 1]).
ORTHONORMAL SYSTEMS AND FOURIER SERIES
361

Examples
In C[a,b] with 1 < a < b < 1 an orthonormal basis is given by the
harmonic functions in (a.23). The number of such functions is count-
able infinite. The Fourier series defined in (a.26) essentially corres-
ponds to the Fourier series expansion of a periodic signal. In
C[1,1] this expansion evolves into the Fourier integral.3
In CN an orthonormal basis is given by the vectors in (a.24). The
Fourier series in (a.26) is equivalent to the discrete Fourier transform.
The examples given above are certainly not the only orthonormal
bases. In fact, even in RN (with N > 1) infinitely many orthonormal
bases exist.
If S is an orthonormal basis, and f an arbitrary vector with Fourier
coefficients k with respect to S, then the following theorems hold:
f ¼
X
k
kwk
ða:29Þ
ðf; gÞ ¼
X
k
ðf; wkÞðg; wkÞ
ðParsevalÞ
ða:30Þ
kfk2 ¼
X
k
jkj2
(Parseval/Pythagoras)
ða:31Þ
Equation (a.29) corresponds to the inverse transform in Fourier analysis.
Equation (a.31) follows directly from (a.30), since kwkk ¼ 1, 8k. The
equation shows that in the case of an orthonormal basis Bessel’s inequal-
ity transforms to an equality.
A.4
LINEAR OPERATORS
Given two normed linear spaces R1 and R2, a mapping A of a subset of
R1 into R2 is called an operator from R1 to R2. The subset of R1
(possibly R1 itself) for which the operator A is defined is called the
domain DA of A. The range RA is the set fgjg ¼ Af, f 2 DAg. In the
sequel we will assume that DA is a linear (sub)space. An operator is
linear if for all vectors f,g 2 DA and for all  and :
3 In fact, C[1,1] must satisfy some conditions in order to assure the existence of the Fourier
expansion.
362
APPENDIX A

Aðf þ gÞ ¼ Af þ Ag
ða:32Þ
Examples
Any orthogonal transform is a linear operation. In RN and CN
any matrix–vector multiplication is a linear operation. In R1 and
C1 left-sided shifts, right-sided shifts and any linear combination of
them (i.e. discrete convolution) are linear operations. In R[a,b]
convolution integrals and differential operators are linear operators.
Some special linear operators are:
. The null operator 0 assigns the null vector to each vector: 0f ¼ 0.
. The identity operator I carries each vector into itself: If ¼ f.
An operator A is invertible if for each g 2 RA the equation g ¼ Af has a
unique solution f 2 DA. The operator A1 that uniquely assigns this
solution f to g is called the inverse operator of A:
g ¼ Af , f ¼ A1g
ða:33Þ
The following properties are shown easily:
. A1A ¼ I
. AA1 ¼ I
. The inverse of a linear operator – if it exists – is linear.
Suppose
that
in
a
linear
space
R
two
orthonormal
bases
Sa ¼ f a0
a1
   g and Sb ¼ f b0
b1
   g are defined. According to
(a.29) each vector f 2 R has two representations:
f ¼
X
k
kak
with: k ¼ ðf; akÞ
f ¼
X
k
kbk
with: k ¼ ðf; bkÞ
Since both Fourier series represent the same vector we conclude that:
f ¼
X
k
kak ¼
X
k
kbk
LINEAR OPERATORS
363

The relationship between the Fourier coefficients k and k can be made
explicit by the calculation of the inner product:
ðf; bnÞ ¼
X
k
kðak; bnÞ ¼
X
k
kðbk; bnÞ ¼ n
ða:34Þ
The Fourier coefficients k and k can be arranged as vectors
a ¼ (0, 1,    ) and b ¼ (0, 1,    ) in RN or CN (if the dimension of
R is finite), or in R1 and C1 (if the dimension of R is infinite). In one of
these spaces equation (a.34) defines a linear operator U:
b ¼ Ua
ða:35Þ
The inner product in (a.34) could equally well be accomplished with
respect to a vector an. This reveals that an operator U exists for which:
a ¼ Ub
ða:36Þ
Clearly, from (a.33):
U ¼ U1
ða:37Þ
Suppose we have two vectors f1 and f2 represented in Sa by a1 and a2,
and in Sb by b1, and b2. Since the inner product (f1,f2) must be independ-
ent of the representation, we conclude that (f1,f2) ¼ (a1, a2) ¼ (b1, b2).
Therefore:
ða1; U1b2Þ ¼ ðUa1; b2Þ
ða:38Þ
Each operator that satisfies (a.38) is called a unitary operator. A corollary
of (a.38) is that any unitary operator preserves the Euclidean norm.
The adjoint A of an operator A is an operator that satisfies:
ðAf; gÞ ¼ ðf; AgÞ
ða:39Þ
From this definition, and from (a.38), it follows that an operator U for
which its adjoint U equals its inverse U1 is a unitary operator. This is
in accordance with the notation used in (a.37). An operator A is called
self-adjoint, if A ¼ A.
Suppose that A is a linear operator in a space R. A vector ek that
satisfies:
364
APPENDIX A

Aek ¼ kek
ek 6¼ 0
ða:40Þ
with k a real or complex number is called an eigenvector of A. The
number k is the eigenvalue. The eigenvectors and eigenvalues of an
operator are found by solving the equation (A  kI)ek ¼ 0. Note that if
ek is a solution of this equation, then so is ek with  any real or complex
number. If a unique solution is required, we should constrain the length
of the eigenvector to unit, i.e. vk ¼ ek=kekk, yielding the so-called nor-
malized eigenvector. However, since þek=kekk and ek=kekk are both
valid eigenvectors, we still have to select one out of the two possible
solutions. From now on, the phrase ‘the normalized eigenvector’ will
denote both solutions.
Operators that are self-adjoint have – under mild conditions – some
nice properties related to their eigenvectors and eigenvalues. The proper-
ties relevant in our case are:
1. All eigenvalues are real.
2. With each eigenvalue at least one normalized eigenvector is asso-
ciated. However, an eigenvalue can also have multiple normalized
eigenvectors. These eigenvectors span a linear subspace.
3. There is an orthonormal basis V ¼ f v0
v1
   g formed by the
normalized eigenvectors. Due to possible multiplicities of normal-
ized eigenvalues (see above) this basis may not be unique.
A corollary of the properties is that any vector f 2 R can be represented
by a Fourier series with respect to V, and that in this representation the
operation becomes simply a linear combination, that is:
f ¼
X
k
kvk
with: k ¼ ðf; vkÞ
ða:41Þ
Af ¼
X
k
kkvk
ða:42Þ
The connotation of this decomposition of the operation is depicted in
Figure A.2. The set of eigenvalues is called the spectrum of the operator.
calculation of
Fourier
coefficients
Fourier series 
expansion
f
φk
Af
λ k
λ kφk
Figure A.2
Eigenvalue decomposition of a self-adjoint operator
LINEAR OPERATORS
365

A.5
REFERENCES
Kolmogorov, A.N. and Fomin, S.V., Introductory Real Analysis, Dover Publications,
New York, 1970.
Pugachev, V.S. and Sinitsyn, I.N., Lectures on Functional Analysis and Applications,
World Scientific, 1999.
366
APPENDIX A

Appendix B
Topics Selected from Linear
Algebra and Matrix Theory
Whereas Appendix A deals with general linear spaces and linear oper-
ators, the current appendix restricts the attention to linear spaces with
finite dimension, i.e. RN and CN. With that, all that has been said in
Appendix A also holds true for the topics of this appendix.
B.1
VECTORS AND MATRICES
Vectors in RN and CN are denoted by bold-faced letters, e.g. f, g. The
elements in a vector are arranged either vertically (a column vector) or
horizontally (a row vector). For example:
f ¼
f0
f1
..
.
fN1
2
6664
3
7775
or:
fT ¼ f0
f1
  
fN1
½
ðb:1Þ
The superscript T is used to convert column vectors to row vectors.
Vector addition and scalar multiplication are defined as in Section A.1.
A matrix H with dimension N  M is an arrangement of NM numbers
hn,m (the elements) on an orthogonal grid of N rows and M columns:
Classification, Parameter Estimation and State Estimation: An Engineering Approach using MATLAB
F. van der Heijden, R.P.W. Duin, D. de Ridder and D.M.J. Tax
 2004 John Wiley & Sons, Ltd
ISBN: 0-470-09013-8

H ¼
h0;0
h0;1
  
h0;M1
h1;0
h1;1
  
h1;M1
h2;0
...
...
...
...
...
hN1;0
  
  
hN1;M1
2
666664
3
777775
ðb:2Þ
The elements are real or complex. Vectors can be regarded as N  1
matrices (column vectors) or 1  M matrices (row vectors). A matrix can
be regarded as an horizontal arrangement of M column vectors with
dimension N, for example:
H ¼ h0
h1
  
hM1
½

ðb:3Þ
Of course, a matrix can also be regarded as a vertical arrangement of N
row vectors.
The scalar–matrix multiplication H replaces each element in H with
hn,m. The matrix–addition H ¼ A þ B is only defined if the two
matrices A and B have equal size N  M. The result H is an N  M
matrix with elements hn,m ¼ an,m þ bn,m. These two operations satisfy
the axioms of a linear space (Section A.1). Therefore, the set of all
N  M matrices is another example of a linear space.
The matrix–matrix product H ¼ AB is defined only when the number
of columns of A equals the number of rows of B. Suppose that A is an
N  P matrix, and that B is a P  M matrix, then the product H ¼ AB is
an N  M matrix with elements:
hn;m ¼
X
P1
p¼0
an;pbp;m
ðb:4Þ
Since a vector can be regarded as an N  1 matrix, this also defines the
matrix–vector product g ¼ Hf with f an M-dimensional column vector,
H an N  M matrix and g an N-dimensional column vector. In accord-
ance with these definitions, the inner product between two real
N-dimensional vectors introduced in Section A.1.2 can be written as:
ðf; gÞ ¼
X
N1
n¼0
fngn ¼ fTg
ðb:5Þ
368
APPENDIX B

It is easy to show that a matrix–vector product g ¼ Hf defines a linear
operator from RM into RN and CM into CN. Therefore, all definitions
and properties related to linear operators (Section A.4) also apply to
matrices.
Some special matrices are:
. The null matrix O. This is a matrix fully filled with zero. It corres-
ponds to the null operator: Of ¼ 0.
. The unit matrix I. This matrix is square (N ¼ M), fully filled with
zero, except for the diagonal elements which are unit:
I ¼
1
0
..
.
0
1
2
4
3
5
This matrix corresponds to the unit operator: If ¼ f.
. A diagonal matrix  is a square matrix, fully filled with zero, except
for its diagonal elements n,n:
 ¼
0;0
0
..
.
0
N1;N1
2
64
3
75
Often, diagonal matrices are denoted by upper case Greek symbols.
. The transposed matrix HT of an N  M matrix H is an M  N
matrix, its elements are given by hT
m,n ¼ hn,m.
. A symmetric matrix is a square matrix for which HT ¼ H.
. The conjugated of a matrix H is a matrix H the elements of which
are the complex conjugated of the one of H.
. The adjoint of a matrix H is a matrix H which is the conjugated
and the transposed of H, that is: H ¼ H
T. A matrix H is self-
adjoint or Hermitian if H ¼ H. This is the case only if H is square
and hn,m ¼ hm,n.
. The inverse of a square matrix H is the matrix H1 that satisfies
H1H ¼ I. If it exists, it is unique. In that case the matrix H is called
regular. If H1 doesn’t exist, H is called singular.
. A unitary matrix U is a square matrix that satisfies U1 ¼ U.
A real unitary matrix is called orthonormal. These matrices satisfy
U1 ¼ UT.
VECTORS AND MATRICES
369

. A square matrix H is Toeplitz if its elements satisfy hn,m ¼ g(nm) in
which gn is a sequence of 2N  1 numbers.
. A
square
matrix
H
is
circulant
if
its
elements
satisfy
hn,m ¼ g(nm)%N. Here, (n  m)%N is the remainder of (n  m)/N.
. A matrix H is separable if it can be written as the product of two
vectors: H ¼ fgT.
Some properties with respect to the matrices mentioned above:
ðHÞ ¼ H
ðb:6Þ
ðABÞ ¼ BA
ðb:7Þ
ðH1Þ ¼ ðHÞ1
ðb:8Þ
ðABÞ1 ¼ B1A1
ðb:9Þ
ðA1 þ HTB1HÞ1 ¼ A  AHT HAHT þ B

1
HA
ðb:10Þ
The relations hold if the size of the matrices are compatible and the
inverses exist. Property (b.10) is known as the matrix inversion lemma.
B.2
CONVOLUTION
Defined in a finite interval, the discrete convolution between a sequence
fk and gk:
gn ¼
X
N1
k¼0
hnkfk
with:
n ¼ 0; 1; . . . ; N  1
ðb:11Þ
can be written economically as a matrix–vector product g ¼ Hf. The
matrix H is a Toeplitz matrix:
H ¼
h0
h1
h2
  
h1N
h1
h0
h1
h2N
h2
h1
..
.
..
.
..
.
...
..
.
..
.
h1
hN1
hN2
  
h1
h0
2
666664
3
777775
ðb:12Þ
370
APPENDIX B

If this ‘finite interval’ convolution is replaced with a circulant (wrap-
around) discrete convolution, then (b.11) becomes:
gn ¼
X
N1
k¼0
hðnkÞ%Nfk
with:
n ¼ 0; 1; . . . ; N  1
ðb:13Þ
In that case, the matrix–vector relation g ¼ Hf still holds. However, the
matrix H is now a circulant matrix:
H ¼
h0
hN1
hN2
  
h1
h1
h0
hN1
h2
h2
h1
..
.
..
.
...
..
.
..
.
..
.
hN1
hN1
hN2
  
h1
h0
2
666664
3
777775
ðb:14Þ
The Fourier matrix W is a unitary N  N matrix with elements given by:
wn;m ¼
1ﬃﬃﬃﬃﬃ
N
p
exp 2jnm
N


with:
j ¼
ﬃﬃﬃﬃﬃﬃﬃ
1
p
ðb:15Þ
The (row) vectors in this matrix are the complex conjugated of the
basisvectors given in (a.24). Note that W ¼ W1 because W is unitary.
It can be shown that the circulant convolution in (b.13) can be trans-
formed into an element-by-element multiplication provided that the
vector g is represented by the orthonormal basis of (a.24). In this
representation the circulant convolution g ¼ Hf becomes; see (a.36):
Wg ¼ WHf
ðb:16Þ
Writing W ¼ [ w0
w1
. . .
wN1 ] and carefully examining Hwk
reveals that the basisvectors wk are the eigenvectors of the circulant
matrix
H.
Therefore,
we
may
write
Hwk ¼ kwk
with
k ¼ 0, 1, . . . , N  1. The numbers k are the eigenvalues of H. If these
eigenvalues are arranged in a diagonal matrix:
 ¼
0
0
..
.
0
N1
2
64
3
75
ðb:17Þ
CONVOLUTION
371

the
N
equations
Hwk ¼ kwk
can
be
written
economically
as:
HW ¼ W. Right-sided multiplication of this equation by W1 yields:
H ¼ WW1. Substitution in (b.16) gives:
Wg ¼ WWLW1f ¼ LW1f
ðb:18Þ
Note that the multiplication L with the vector W1f is an element-by-
element multiplication because the matrix L is diagonal. The final result
is obtained if we perform a left-sided multiplication in (b.18) by W:
g ¼ WLW1f
ðb:19Þ
The interpretation of this result is depicted in Figure B.1.
B.3
TRACE AND DETERMINANT
The trace trace(H) of a square matrix H is the sum of its diagonal elements:
traceðHÞ ¼
X
N1
n¼0
hn;n
ðb:20Þ
Properties related to the trace are (A and B are N  N matrices, f and g
are N-dimensional vectors):
traceðABÞ ¼ traceðBAÞ
ðb:21Þ
ðf; gÞ ¼ fg ¼ traceðfgÞ
ðb:22Þ
The determinant jHj of a square matrix H is recursively defined with its
co-matrices. The co-matrix Hn, m is an (N  1)  (N  1) matrix that is
derived from H by exclusion of the n-th row and the m-th column. The
following equations define the determinant:
If
N ¼ 1:
jHj ¼ h0; 0
If
N > 1:
jHj ¼
X
N1
m¼0
ð1Þmh0; mjH0; mj
ðb:23Þ
discrete Fourier
transform
inverse discrete
Fourier transform
f
W–1f
ΛW–1f
WΛW–1f = Hf
Λ
Figure B.1
Discrete circulant convolution accomplished in the Fourier domain
372
APPENDIX B

Some properties related to the determinant:
jABj ¼ jAjjBj
ðb:24Þ
jA1j ¼ 1
jAj
ðb:25Þ
jATj ¼ jAj
ðb:26Þ
U is unitary: )
jUj ¼ 1
ðb:27Þ
 is diagonal: )
jj ¼
Y
N1
n¼0
n;n
ðb:28Þ
If n are the eigenvalues of a square matrix A, then:
traceðAÞ ¼
X
N1
n¼0
n
and
jAj ¼
Y
N1
n¼0
n
ðb:29Þ
The rank of a matrix is the maximum number of column vectors (or row
vectors) that are linearly independent. The rank of a regular N  N
matrix is always N. In that case, the determinant is always non-zero.
The reverse holds true too. The rank of a singular N  N matrix is
always less than N, and the determinant is zero.
B.4
DIFFERENTIATION OF VECTOR AND MATRIX
FUNCTIONS
Suppose f(x) is a real or complex function of the real N-dimensional
vector x. Then, the first derivative of f(x) with respect to x is an
N-dimensional vector function (the gradient):
qfðxÞ
qx
with elements:
qfðxÞ
qxn
ðb:30Þ
If f(x) ¼ aTx (i.e. the inner product between x and a real vector a), then:
q½aTx
qx
¼ a
ðb:31Þ
DIFFERENTIATION OF VECTOR AND MATRIX FUNCTIONS
373

Likewise, if f(x) ¼ xTHx (i.e. a quadratic form defined by the matrix H),
then:
q½xTHx
qx
¼ 2Hx
ðb:32Þ
The second derivative of f(x) with respect to x is an N  N matrix called
the Hessian matrix:
HðxÞ ¼ q2fðxÞ
qx2
with elements:
hn;mðxÞ ¼ q2fðxÞ
qxnqxm
ðb:33Þ
The determinant of this matrix is called the Hessian.
The Jacobian matrix of an N-dimensional vector function f():
RM ! RN is defined as an N  M matrix:
HðxÞ ¼ qfðxÞ
qx
with elements:
hn;mðxÞ ¼ qfnðxÞ
qxm
ðb:34Þ
Its determinant (only defined if the Jacobian matrix is square) is called
the Jacobian.
The differentiation of a function of a matrix, e.g. f(H), with respect to this
matrixisdefinedsimilartothedifferentiationin(b.30).Theresultisamatrix:
qfðHÞ
qH
with elements:
qfðHÞ
qhn;m
ðb:35Þ
Suppose that we have a square, invertible R  R matrix function F(H) of
an N  M matrix H, that is F(): RN  RM ! RR  RR, then the deriva-
tive of F(H) with respect to one of the elements of H is:
q
qhn;m
FðHÞ ¼
q
qhn;m
f0;0ðHÞ
  
q
qhn;m
f0;R1ðHÞ
...
...
q
qhn;m
fR1;0ðHÞ
  
q
qhn;m
fR1;R1ðHÞ
2
666664
3
777775
ðb:36Þ
374
APPENDIX B

From this the following rules can be derived:
q
qhn;m
½FðHÞ þ GðHÞ ¼
q
qhn;m
FðHÞ þ
q
qhn;m
GðHÞ
ðb:37Þ
q
qhn;m
½FðHÞGðHÞ ¼
q
qhn;m
FðHÞ


GðHÞ þ
q
qhn;m
GðHÞ


FðHÞ
ðb:38Þ
q
qhn;m
F1ðHÞ ¼ F1ðHÞ
q
qhn;m
FðHÞ


F1ðHÞ
ðb:39Þ
Suppose that A, B and C are square matrices of equal size. Then,
some properties related to the derivatives of the trace and the
determinant are:
q
qA traceðAÞ ¼ I
ðb:40Þ
q
qA traceðBACÞ ¼ BTCT
ðb:41Þ
q
qA traceðABATÞ ¼ AðB þ BTÞ
ðb:42Þ
q
qA jBACj ¼ jBACjðA1ÞT
ðb:43Þ
q
qan;m
jAj ¼ jAj½A1m;n
ðb:44Þ
In (b.44), [A1]m, n is the m, n-th element of A1.
B.5
DIAGONALIZATION OF SELF-ADJOINT
MATRICES
Recall from Section B.1 that a N  N matrix H is called self-adjoint
or
Hermitian
if
H ¼ H.
From
the
discussion
on
self-adjoint
operators in Section A.4 it is clear that associated with H, there
exists an orthonormal basis V ¼ f v0
v1
. . .
vN1 g which we
now arrange in a unitary matrix V ¼ [ v0
v1
. . .
vN1 ]. Each
vector vk is an eigenvector with corresponding (real) eigenvalue k.
These eigenvalues are now arranged as the diagonal elements in a
diagonal matrix .
DIAGONALIZATION OF SELF-ADJOINT MATRICES
375

The operation Hf can be written as; see (a.42) and (b.5):
Hf ¼
X
N1
n¼0
nðvn; fÞvn ¼
X
N1
n¼0
nvnfvn ¼
X
N1
n¼0
nvnv
nf
ðb:45Þ
Suppose that the rank of H equals R. Then there are exactly R non-zero
eigenvalues. Consequently, the number of terms in (b.45) can be
replaced with R. From this, it follows that H is a composition of its
eigenvectors according to:
H ¼
X
R1
k¼0
kvkv
k
ðb:46Þ
The summation on the right-hand side can be written more economically
as: VLV. Therefore:
H ¼ VLV
ðb:47Þ
The unitary matrix V transforms the domain of H such that H becomes
the diagonal matrix  in this new domain. The matrix V accomplishes
the inverse transform. In fact, (b.47) is the matrix version of the decom-
position shown in Figure A.2.
If the rank R equals N, there are exactly N non-zero eigenvalues. In
that case, the matrix  is invertible, and so is H:
H1 ¼ V1V ¼
X
N1
n¼0
vnv
n
n
ðb:48Þ
It can be seen that the inverse H1 is also self-adjoint.
A self-adjoint matrix H is positive definite if its eigenvalues are all
positive. In that case the expression (f,g) ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
(f  g)H(f  g)
p
satisfies
the conditions of a distance measure (Section A.2). To show this it
suffices to prove that
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
fHf
p
satisfies the conditions of a norm; see
Section A.2. These conditions are given in Section A.1. We use the
diagonal form of H; see (b.47):
fHf ¼ fVVf
ðb:49Þ
Since V is a unitary matrix, the vector Vf can be regarded as the
representation of f with respect to the orthonormal basis defined by
376
APPENDIX B

the vectors in V. Let this representation be denoted by: f ¼ Vf. The
expression fHf equals:
fHf ¼ f ¼
X
N1
n¼0
njnj2
ðb:50Þ
Written in this form, it is easy to show that all conditions of a norm are met.
With the norm
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
fHf
p
the sets of points equidistant to the origin, i.e.
the vectors that satisfy fHf ¼ constant, are ellipsoids. See Figure B.2.
This follows from (b.50):
fHf ¼ constant
,
X
N1
n¼0
njnj2 ¼ constant
Hence, if we introduce a new vector u with elements defined as:
un ¼ n=
ﬃﬃﬃﬃﬃ
n
p
, we must require that:
X
N1
n¼0
junj2 ¼ constant
We conclude that in the u domain the ordinary Euclidean norm applies.
Therefore, the solution space in this domain is a sphere (Figure B.2(a)). The
operation un ¼ n=
ﬃﬃﬃﬃﬃ
n
p
is merely a scaling of the axes by factors
ﬃﬃﬃﬃﬃ
n
p
. This
transforms the sphere into an ellipsoid. The principal axes of this ellipsoid
line up with the basisvectors u (and f), see Figure B.2(b). Finally, the
unitary transform f ¼ Vf rotates the principal axes, but without affecting
the shape of the ellipsoid (Figure B.2(c)). Hence, the orientation of these
axes in the f domain is along the directions of the eigenvectors vn of H.
The metric expressed by
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
(f  g)H(f  g)
p
is called is a quadratic
distance. In pattern classification theory it is usually called the Mahala-
nobis distance.
(c) f domain
v0
v1
(a) u domain
(b)  φ domain
Figure B.2
‘Circles’ in R2 equipped with the Mahalanobis distance
DIAGONALIZATION OF SELF-ADJOINT MATRICES
377

B.6
SINGULAR VALUE DECOMPOSITION (SVD)
The singular value decomposition theorem states that an arbitrary
N  M matrix H can be decomposed into the product:
H ¼ UVT
ðb:51Þ
where U is an orthonormal N  R matrix,  is a diagonal R  R matrix,
and V is an orthonormal M  R matrix. R is the rank of the matrix H.
Therefore, R  min (N,M).
The proof of the theorem is beyond the scope of this book. However, its
connotation can be clarified as follows. Suppose for a moment, that
R ¼ N ¼ M. We consider all pairs of vectors of the form y ¼ Hx where
x is on the surface of the unit sphere in RM, i.e. kxk ¼ 1. The corres-
ponding y must be on the surface in RN defined by the equation
yTy ¼ xTHTHx. The matrix HTH is a symmetric M  M matrix. By
virtue of (b.47) there must be an orthonormal matrix V and a diagonal
matrix S such that:1
HTH ¼ VSVT
ðb:52Þ
The matrix V ¼ [ v0
  
vM1 ] contains the (unit) eigenvectors vi of
HTH. The corresponding eigenvalues are all on the diagonal of the
matrix S. Without loss of generality we may assume that they are sorted
in descending order. Thus, Si, i  Siþ1,iþ1.
With x def
¼ VTx, the solutions of the equation yTy ¼ xTHTHx with
kxk ¼ 1 is the same set as the solutions of yTy ¼ xTSx. Clearly, if x is
a unit vector, then so is x because V is orthonormal. Therefore, the
solutions of yTy ¼ xTSx is the set:
fyg
with
y ¼ x
and
kxk ¼ 1
where:
 ¼ S
1
2
ðb:53Þ
 is the matrix that is obtained by taking the square roots of all
(diagonal) elements in S. The diagonal elements of  are usually denoted
1 We silently assume that H is a real matrix so that V is also real, and V ¼ VT.
378
APPENDIX B

by the singular values i ¼ i,i ¼ þ
ﬃﬃﬃﬃﬃﬃ
Si,i
p
. With that, the solutions
become a set:
fyg
with
y ¼
X
N1
i¼0
iivi
and
X
N1
i¼0
2
i ¼ 1
ðb:54Þ
Equation (b.54) reveals that the solutions of yTy ¼ xTHTHx with
kxk ¼ 1 are formed by a scaled version of the sphere kxk ¼ 1. Scaling
takes place with a factor i in the direction of vi. In other words, the
solutions form an ellipsoid.
Each eigenvector vi gives rise to a principal axis of the ellipsoid. The
direction of such an axis is the one of the vector Hvi. We form the matrix
HV ¼ [ Hv0
. . .
HvM1 ] which contains the vectors that point in the
directions of all principal axes. Using (b.52):
HTHV ¼ VS
)
HHTHV ¼ HVS
Consequently, the column vectors in the matrix U that fulfils
HHTU ¼ US
ðb:55Þ
successively point in the directions of the principal axes. Since S is a diagonal
matrix, we conclude from (b.47) that U must contain the eigenvectors ui of
the matrix HHT, i.e. U ¼ [ u0
 
uM1 ]. The diagonal elements of S are
the corresponding eigenvalues. U is an orthonormal matrix.
The operator H maps the vector vi to a vector Hvi whose direction is
given by the unit vector ui, and whose length is given by i. Therefore,
Hvi ¼ iui. Representing an arbitrary vector x as a linear combination of
vi, i.e. x ¼ VTx gives us finally the SVD theorem stated in (b.51).
In the general case, N can differ from M and R  min (N, M). How-
ever, (b.51), (b.52), (b.54) and (b.55) are still valid, except that the
dimensions of the matrices must be adapted. If R < M, then the singular
values from R up to M1 are zero. These singular values and the
associated eigenvectors are discarded then. The ellipsoid mentioned
above becomes an R-dimensional ellipsoid that is embedded in RN.
The process is depicted graphically in Figure B.3. The operator
x ¼ VTx aligns the operand to the suitable basis fvig by means of a
rotation. This operator also discards all components vR up to vM1 if
R < M. The operator m ¼ x stretches the axes formed by the fvig basis.
SINGULAR VALUE DECOMPOSITION (SVD)
379

Finally, the operator y ¼ Um hangs out each vi component in RM as a ui
component in RN.
An important application of the SVD is matrix approximation. For
that purpose, (b.51) is written in an equivalent form:
H ¼
X
R1
i¼0
iuivT
i
ðb:56Þ
We recall that the sequence 0, 1,    is in descending order. Also,
i > 0. Therefore, if the matrix must be approximated by less then R
terms, then the best strategy is to nullify the last terms. For instance, if
the matrix H must be approximated by a vector product, i.e. H ¼ fgT,
then the best approximation is obtained by nullifying all singular values
except 0. That is H 	 0u0vT
0 .
The SVD theorem is useful to find the pseudo inverse of a matrix:
Hþ ¼ V1UT ¼
X
R1
i¼0
1
i
viuT
i
ðb:57Þ
The pseudo inverse gives the least squares solution to the equation
y ¼ Hx. In other words, the solution x^¼ Hþy is the one that minimizes
v1
v1
v2
v2
x
σ1v1
σ2u2
σ1u1
σ2v2
µ
y
H
VT(aligner)
Σ (stretcher)
U (hanger)
ξ
Figure B.3
Singular value decomposition of a matrix H
380
APPENDIX B

kHx  yk2
2. If the system is underdetermined, R < M, it provides a
minimum norm solution. That is, x^ is the smallest solution of y ¼ Hx.
The SVD is also a good departure point for a stability analysis of the
pseudo inverse, for instance by studying the ratio between the largest
and smallest singular values. This ratio is a measure of the sensitivity of
the inverse to noise in the data. If this ratio is too high, we may consider
regularizing the pseudo inverse by discarding all terms in (b.57) whose
singular values are too small.
Besides matrix approximation and pseudo inverse calculation the SVD
finds application in image restoration, image compression and image
modelling.
B.7
REFERENCES
Bellman, R.E., Introduction to Matrix Analysis, McGraw-Hill, New York, 1970.
Strang, G., Linear Algebra and its Applications, Harcourt Brace Jovanovich, San Diego,
CA, 3rd edition, 1989.
REFERENCES
381


Appendix C
Probability Theory
This appendix summarizes concepts from probability theory. This sum-
mary only concerns those concepts that are part of the mathematical
background required for understanding this book. Mathematical pecu-
liarities which are not relevant here are omitted. At the end of the
appendix references to detailed treatments are given.
C.1
PROBABILITY THEORY AND RANDOM
VARIABLES
The axiomatic development of probability involves the definitions of
three concepts. Taken together these concepts are called an experiment.
The three concepts are:
(a) A set  consisting of outcomes !i. A trial is the act of randomly
drawing a single outcome. Hence, each trial produces one ! 2 .
(b) A is a set of certain1 subsets of .
Each subset  2 A is called an event. The event f!ig, which
consists of a single outcome, is called an elementary event. The
1 This set of subsets must comply with some rules. In fact, A must be a so-called Borel set. But
this topic is beyond the scope of this book.
Classification, Parameter Estimation and State Estimation: An Engineering Approach using MATLAB
F. van der Heijden, R.P.W. Duin, D. de Ridder and D.M.J. Tax
 2004 John Wiley & Sons, Ltd
ISBN: 0-470-09013-8

set  is called the certain event. The empty subset ; is called the
impossible event. We say that an event  occurred if the outcome
! of a trial is contained in , i.e. if ! 2 .
(c) A real function P() is defined on A. This function, called prob-
ability, satisfies the following axioms:
I: P()  0
II: P() ¼ 1
III: If ,  2 A and  \  ¼ ; then P( [ ) ¼ P() þ P()
Example
The space of outcomes corresponding to the colours of a traffic-light
is:  ¼ fred, green, yellowg. The set A may consist of subsets like:
;, red, green, yellow, red [ green, red \ green, red [ green [ yellow, . . ..
With that, P(green) is the probability that the light will be green.
P(green [ yellow) is the probability that the light will be green or
yellow or both. P(green \ yellow) is the probability that at the same
time the light is green and yellow.
A random variable x(!) is a mapping of  onto a set of numbers,
for instance: integer numbers, real numbers, complex numbers, etc.
The distribution function Fx(x) is the probability of the event that
corresponds to x  x:
FxðxÞ ¼ Pðx  xÞ
ðc:1Þ
A note on the notation
In the notation Fx(x) the variable x is the random variable of interest.
The variable x is the independent variable. It can be replaced by other
independent variables or constants. So, Fx(s), Fx(x2) and Fx(0) are all
valid notations. However, to avoid lengthy notations the abbreviation
F(x) will often be used if it is clear from the context that Fx(x) is meant.
Also, the underscore notation for a random variable will be omitted
frequently.
The random variable x is said to be discrete if a finite number (or infinite
countable number) of events x1, x2, . . . exists for which:
Pðx ¼ xiÞ > 0
and
X
all i
Pðx ¼ xiÞ ¼ 1
ðc:2Þ
384
APPENDIX C

Notation
If the context is clear, the notation P(xi) will be used to denote P(x ¼ xi).
The random variable x is continuous if a function px(x) exists for which:
FxðxÞ ¼
Z x
¼1
pxðÞd
ðc:3Þ
The function px(x) is called the probability density of x. The discrete case
can be included in the continuous case by permitting px(x) to contain
Dirac functions of the type Pi(x  xi).
Notation
If the context is clear, the notation p(x) will be used to denote px(x).
Examples
We consider the experiment consisting of tossing a (fair) coin. The
possible outcomes are fhead, tailg. The random variable x is defined
according to:
head ! x ¼ 0
tail ! x ¼ 1
This random variable is discrete. Its distribution function and prob-
abilities are depicted in Figure C.1(a).
x
x
p(x)
P(xi)
F(x)
F(x)
x
x
0.5
–0.5
0.5
1
1
1
0
0.5
1
0.5
(a)
(b)
Figure C.1
(a) Discrete distribution function with probabilities. (b) Continuous
distribution function with a uniform probability density
PROBABILITY THEORY AND RANDOM VARIABLES
385

An example of a continuous random variable is the round-off error
which occurs when a random real number is replaced with its nearest
integer number. This error is uniformly distributed between 0:5 and
0.5. The distribution function and associated probability density are
shown in Figure C.1(b).
Since F(x) is a non-decreasing function of x, and F(1) ¼ 1, the density
must be a non-negative function with
R
p(x)dx ¼ 1. Here, the integral
extends over the real numbers from 1 to 1.
C.1.1
Moments
The moment of order n of a random variable is defined as:
E½xn ¼
Z 1
x¼1
xnpðxÞdx
ðc:4Þ
Formally, the notation should be E[xn], but as said before we omit
the underscore if there is no fear of ambiguity. Another notation of
E[xn] is xn.
The first order moment is called the expectation. This quantity is often
denoted by x or (if confusion is not to be expected) briefly . The
central moments of order n are defined as:
E½ðx  Þn ¼
Z 1
x¼1
ðx  ÞnpðxÞdx
ðc:5Þ
The first central moment is always zero. The second central moment is
called variance:
Var½x ¼ E½ðx  Þ2 ¼ E½x2  2
ðc:6Þ
The (standard) deviation of x denoted by x, or briefly , is the square
root of the variance:
x ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Var½x
p
ðc:7Þ
386
APPENDIX C

C.1.2
Poisson distribution
The act of counting the number of times that a certain random
event takes place during a given time interval often involves a Poisson
distribution. A discrete random variable n which obeys the Poisson
distribution has the probability function:
Pðn ¼ nÞ ¼ n expðÞ
n!
ðc:8Þ
 is a parameter of the distribution. It is the expected number of events.
Thus, E[n] ¼ . A special feature of the Poisson distribution is that the
variance and the expectation are equal: Var[n] ¼ E[n] ¼ . Examples of
this distribution are shown in Figure C.2.
Example
Radiant energy is carried by a discrete number of photons. In
daylight situations, the average number of photons per unit area
is in the order of 1012 (1/(s  mm2)). However, in fact the real
number is Poisson distributed. Therefore, the relative deviation
n/E[n] is 1/
ﬃﬃﬃ

p
. An image sensor with an integrating area of
100 (mm2), an integration time of 25 (ms) and an illuminance of
250 (lx) receives about  ¼ 106 photons. Hence, the relative devia-
tion is about 0.1%. In most imaging sensors this deviation is almost
negligible compared to other noise sources.
C.1.3
Binomial distribution
Suppose that we have an experiment with only two outcomes, !1 and !2.
The probability of the elementary event f!1g is denoted by P. Conse-
quently, the probability of f!2g is 1  P. We repeat the experiment
0.2
0
10
20
0
0.1
0.2
0
0.1
0.2
0
0.1
n
0
10
20
p(n)
p(n)
p(n)
n
0 
10
20
n
λ = 10
λ = 7
λ = 4
Figure C.2
Poisson distributions
PROBABILITY THEORY AND RANDOM VARIABLES
387

N times, and form a random variable n by counting the number of times
that f!1g occurred. This random variable is said to have a binomial
distribution with parameters N and P.
The probability function of n is:
Pðn ¼ nÞ ¼
N!
n!ðN  nÞ! Pnð1  PÞNn
ðc:9Þ
The expectation of n appears to be E[n] ¼ NP and its variance is
Var[n] ¼ NP(1  P).
Example
The error rate of a classifier is E. The classifier is applied to N objects.
The number of misclassified objects, nerror, has a binomial distribution
with parameters N and E.
C.1.4
Normal distribution
A well-known example of a continuous random variable is the one with
a Gaussian (or normal) probability density:
pðxÞ ¼
1

ﬃﬃﬃﬃﬃﬃ
2
p
exp ðx  Þ2
22
 
!
ðc:10Þ
The parameters  and 2 are the expectation and the variance, respect-
ively. Two examples of the probability density with different  and 2
are shown in Figure C.3.
Gaussian random variables occur whenever the underlying process is
caused by the outcomes of many independent experiments and the
associated random variables add up linearly (the central limit theorem).
An example is thermal noise in an electrical current. The current is
proportional to the sum of the velocities of the individual electrons.
Another example is the Poisson distributed random variable mentioned
above. The envelope of the Poisson distribution approximates the Gauss-
ian distribution as  tends to infinity. As illustrated in Figure C.2, the
approximation looks quite reasonable already when  > 10.
Also, the binomial distributed random variable is the result of an
addition of many independent outcomes. Therefore, the envelope of
388
APPENDIX C

the binomial distribution also approximates the normal distribution as
NP(1  P) tends to infinity. In practice, the approximation is already
reasonably good if NP(1  P) > 5.
C.1.5
The Chi-square distribution
Another example of a continuous distribution is the 	2
n distribution.
A random variable y is said to be 	2
n distributed (Chi-square distributed
with n degrees of freedom) if its density function equals:
pðyÞ ¼
1
2
n
2 n
2
  y
n
21ey
2
y  0
0
elsewhere
8
>
<
>
:
ðc:11Þ
with () the so-called gamma function. The expectation and variance of
a Chi-square distributed random variable appear to be:
E½y ¼ n
Var½y ¼ 2n
ðc:12Þ
Figure C.4 shows some examples of the density and distribution.
The 	2
n distribution arises if we construct the sum of the square of
normally distributed random variables. Suppose that xj with j ¼ 1, . . . ,n
are n Gaussian random variables with E[xj] ¼ 0 and Var[xj] ¼ 1.
p(x)
x
µ2
µ1
σ1
σ2
Figure C.3
Gaussian probability densities
PROBABILITY THEORY AND RANDOM VARIABLES
389

In addition, assume that these random variables are mutually independ-
ent,2 then the random variable:
y ¼
X
n
j¼1
x2
j
ðc:13Þ
is 	2
n distributed.
C.2
BIVARIATE RANDOM VARIABLES
In this section we consider an experiment in which two random variables
x and y are associated. The joint distribution function is the probability
that x  x and y  y, i.e.
Fðx; yÞ ¼ Pðx  x; y  yÞ
ðc:14Þ
The function p(x,y) for which:
Fðx; yÞ ¼
Z x
¼1
Z y

¼1
pð; 
Þd
d
ðc:15Þ
0
5
10
15
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
y
p(y)
F(y)
n = 5
n = 5
n = 1
n = 1
0
5
10
15
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
y
Figure C.4
Chi-square densities and (cumulative) distributions. The degrees of
freedom, n, varies from 1 up to 5
2 For the definition of ‘independent’: see Section C.2.
390
APPENDIX C

is called the joint probability density. Strictly speaking, definition (c.15)
holds true only when F(x,y) is continuous. However, by permitting
p(x,y) to contain Dirac functions, the definition also applies to the
discrete case.
From definitions (c.14) and (c.15) it is clear that the marginal distri-
bution F(x) and the marginal density p(x) are given by:
FðxÞ ¼ Fðx; 1Þ
ðc:16Þ
pðxÞ ¼
Z 1
y¼1
pðx; yÞdy
ðc:17Þ
Two random variables x and y are independent if:
Fx;yðx; yÞ ¼ FxðxÞFyðyÞ
ðc:18Þ
This is equivalent to:
px;yðx; yÞ ¼ pxðxÞpyðyÞ
ðc:19Þ
Suppose that h( ,  ) is a function R  R ! R. Then h(x,y) is a random
variable. The expectation of h(x,y) equals:
E½hðx; yÞ ¼
Z 1
x¼1
Z 1
y¼1
hðx; yÞpðx; yÞdydx
ðc:20Þ
The joint moments mij of two random variables x and y are defined as
the expectations of the functions xiyj:
mij ¼ E½xiyj
ðc:21Þ
The quantity i þ j is called the order of mij. It can easily be verified that:
m00 ¼ 1, m10 ¼ E[x] ¼ x and m01 ¼ E[y] ¼ y.
The joint central moments ij of order i þ j are defined as:
ij ¼ E ðx  xÞiðy  yÞj
h
i
ðc:22Þ
BIVARIATE RANDOM VARIABLES
391

Clearly, 20 ¼ Var[x] and 02 ¼ Var[y]. Furthermore, the parameter 11
is called the covariance (sometimes denoted by Cov[x,y]). This para-
meter can be written as: 11 ¼ m11  m10m01. Two random variables are
called uncorrelated if their covariance is zero. Two independent random
variables are always uncorrelated. The reverse is not necessarily true.
Two random variables x and y are Gaussian if their joint probability
density is:
pðx; yÞ ¼
1
2xy
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  r2
p
exp
1
2ð1  r2Þ
ðx  xÞ2
2
x
 
 
 2rðx  xÞðy  yÞ
xy
þ ðy  yÞ2
2
y
!!
ðc:23Þ
The parameters x and y are the expectations of x and y, respectively.
The parameters x and y are the standard deviations. The parameter r is
called the correlation coefficient, defined as:
r ¼
11
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2002
p
¼ Cov½x; y
xy
ðc:24Þ
Figure C.5 shows a scatter diagram with 121 realizations of two Gauss-
ian random variables. In this figure, a geometrical interpretation of
(c.23) is also given. The set of points (x,y) which satisfy:
pðx; yÞ ¼ pðx; yÞ exp  1
2


(i.e. the 1-level contour) turns out to form an ellipse. The centre of
this ellipse coincides with the expectation. The eccentricity, size and
orientation of the 1 contour describe the scattering of the samples
around this centre.
ﬃﬃﬃﬃﬃ
0
p
and
ﬃﬃﬃﬃﬃ
1
p
are the standard deviations of the
samples projected on the principal axes of the ellipse. The angle
between the principal axis associated with 0 and the x axis is .
With these conventions, the variances of x and y, and the correlation
coefficient r, are:
392
APPENDIX C

2
x ¼ 0 cos2  þ 1 sin2 
2
y ¼ 1 cos2  þ 0 sin2 
r ¼ ð0  1Þ sin  cos 
xy
ðc:25Þ
Consequently, r ¼ 0 whenever 0 ¼ 1 (the ellipse degenerates into a
circle), or whenever  is a multiple of /2 (the ellipse lines up with the
axes). In both cases the random variables are independent. The conclusion
is that two Gaussian random variables which are uncorrelated are also
independent.
The situation r ¼ 1 or r ¼ 1 occurs only if 0 ¼ 0 or 1 ¼ 0. The
ellipse degenerates into a straight line. Therefore, if two Gaussian ran-
dom variables are completely correlated, then this implies that two
constants a and b can be found for which y ¼ ax þ b.
The expectation and variance of the random variable defined by
z ¼ ax þ by are:
x
y
µ x = m10
µ y = m01
σy =
µ 02
σx =
µ 02
λ 0
λ1
θ
3 σ contour
2 σ contour
1 σ contour
Figure C.5
Scatter diagram of two Gaussian random variables
BIVARIATE RANDOM VARIABLES
393

E½z ¼ aE½x þ bE½y
ðc:26Þ
2
z ¼ a22
x þ b22
y þ 2ab Cov½x; y
ðc:27Þ
The expectation appears to be a linear operation, regardless of the joint
distribution function. The variance is not a linear operation. However,
(c.27) shows that if two random variables are uncorrelated, then
2
xþy ¼ 2
x þ 2
y.
Another important aspect of two random variables is the concept of
conditional probabilities and moments. The conditional distribution
Fxjy(xjy) (or in shorthand notation F(xjy)) is the probability that x  x
given that y  y. Written symbolically:
FxjyðxjyÞ ¼ FðxjyÞ ¼ Pðx  xjy  yÞ
ðc:28Þ
The conditional probability density associated with Fxjy(xjy) is denoted
by pxjy(xjy) or p(xjy). Its definition is similar to (c.3). An important
property of conditional probability densities is Bayes’ theorem for con-
ditional probabilities:
pxjyðxjyÞpyðyÞ ¼ px;yðx; yÞ ¼ pyjxðyjxÞpxðxÞ
ðc:29Þ
or in shorthand notation:
pðxjyÞpðyÞ ¼ pðx; yÞ ¼ pðyjxÞpðxÞ
Bayes’ theorem is very important for the development of a classifier or
an estimator; see Chapter 2 and 3.
The conditional moments are defined as:
E½xnjy ¼ y ¼
Z 1
x¼1
xnpðxjyÞdx
ðc:30Þ
The
shorthand
notation
is
E[xnjy].
The
conditional
expectation
and conditional variance are sometimes denoted by xjy and 2
xjy,
respectively.
394
APPENDIX C

C.3
RANDOM VECTORS
In this section, we discuss a finite sequence of N random variables:
x0, x1, . . . , xN1. We assume that these variables are arranged in an
N-dimensional random vector x. The joint distribution function F(x)
is defined as:
FðxÞ ¼ Pðx0  x0; x1  x1; . . . ; xN1  xN1Þ
ðc:31Þ
The probability density p(x) of the vector x is the function that satisfies:
FðxÞ ¼
Z x
x¼1
pðxÞdx
ðc:32Þ
with:
Z x
x¼1
pðxÞdx ¼
Z x0
0¼1
Z x1
1¼1
  
Z xN1
N1¼1
pðxÞdN1    d1d0
The expectation of a function g(x): RN ! R is:
E½gðxÞ ¼
Z 1
x¼1
gðxÞpðxÞdx
ðc:33Þ
Similar definitions apply to vector-to-vector mappings (RN ! RN) and
vector-to-matrix mappings (RN ! RN  RN). Particularly, the expect-
ation vector mx ¼ E[x] and the covariance matrix Cx ¼ E[(x  mx)
(x  mx)T] are frequently used.
A Gaussian random vector has a probability density given by:
pðxÞ ¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2
ð
ÞN Cx
j
j
q
exp  x  mx
ð
ÞTC1
x
x  mx
ð
Þ
2
 
!
ðc:34Þ
The parameters mx (expectation vector) and Cx (covariance matrix) fully
define the probability density.
A random vector is called uncorrelated if its covariance matrix is a
diagonal matrix. If the elements of a random vector x are independent,
the probability density of x is the product of the probability densities of
the elements:
RANDOM VECTORS
395

pðxÞ ¼
Y
N1
n¼0
pðxnÞ
ðc:35Þ
Such a random vector is uncorrelated. The reverse holds true in some
specific cases, e.g. for all Gaussian random vectors.
The conditional probability density p(xjy) of two random vectors x
and y is the probability density of x if y is known. Bayes’ theorem for
conditional probability densities becomes:
pðxjyÞpðyÞ ¼ pðx; yÞ ¼ pðyjxÞpðxÞ
ðc:36Þ
The definitions of the conditional expectation vector and the conditional
covariance matrix are similar.
C.3.1
Linear operations on Gaussian random vectors
Suppose the random vector y results from a linear (matrix) operation
y ¼ Ax. The input vector of this operator x has expectation vector mx
and covariance matrix Cx, respectively. Then, the expectation of the
output vector and its covariance matrix are:
my ¼ Amx
Cy ¼ ACxAT
ðc:37Þ
These relations hold true regardless of the type of distribution functions.
In general, the distribution function of y may be of a type different
from the one of x. For instance, if the elements from x are uniformly
distributed, then the elements of y will not be uniform except for trivial
cases (e.g. when A ¼ I). However, if x has a Gaussian distribution, then
so has y.
Example
Figure C.6 shows the scatter diagram of a Gaussian random vector x,
the covariance matrix of which is the identity matrix I. Such a vector
is called white. Multiplication of x by a diagonal matrix 1/2 yields a
vector y with covariance matrix . This vector is still uncorrelated.
Application of a unitary matrix V to z yields the random vector z,
which is correlated.
396
APPENDIX C

C.3.2
Decorrelation
Suppose a random vector z with covariance matrix Cz is given. Decor-
relation is a linear operation A which, when applied to z, will give a white
random vector x. (The random vector x is called white if its covariance
matrix Cx ¼ T.) The operation A can be found by diagonalization of the
matrix Cz. To see this, it suffices to recognize that the matrix Cz is self-
adjoint, i.e. Cz ¼ CT
z . According to Section B.5 a unitary matrix V and a
(real) diagonal matrix  must exist such that Cz ¼ VVT. The matrix V
consists of the normalized eigenvectors vn of Cz, i.e. V ¼ [v0    vN1].
The matrix  contains the eigenvalues n at the diagonal. Therefore,
application of the unitary transform VT yields a random vector y ¼ VTz
the covariance matrix of which is . Furthermore, the operation 1/2
applied to y gives the white vector x ¼ 1/2y. Hence, the decorrelation/
whitening operation A equals 1/2VT. Note that the operation 1/2VT
is the inverse of the operations shown in Figure C.6.
= I
x
Cx
µ x
Cy = Λ
Cz = VΛVT
scaling
Λ1/2
unitary transform
V
z = Vy = VΛ   x
1
2
y = Λ   x
1
2
µ y = Λ   µ x
1
2
µ z = VΛ   µ x
1
2
Figure C.6
Scatter diagrams of Gaussian random vectors applied to two linear
operators
RANDOM VECTORS
397

We define the 1-level contour of a Gaussian distribution as the
solution of:
ðz  mzÞTC1
z ðz  mzÞ ¼ 1
ðc:38Þ
The level contour is an ellipse (N ¼ 2), an ellipsoid (N ¼ 3) or a
hyperellipsoid (N > 3). A corollary of the above is that the principal
axes of these ellipsoids point in the direction of the eigenvectors of Cz,
and that the ellipsoids intersect these axes at a distance from mz equal to
the square root of the associated eigenvalue. See Figure C.6.
C.4
REFERENCE
Papoulis, A., Probability, Random Variables and Stochastic Processes, McGraw-Hill,
New York, 1965 (third edition: 1991).
398
APPENDIX C

Appendix D
Discrete-time Dynamic
Systems
This brief review is meant as a refresher for readers who are familiar
with the topic. It summarizes those concepts that are used within the
textbook. It also introduces the notations adopted in this book.
D.1
DISCRETE-TIME DYNAMIC SYSTEMS
A dynamic system is a system whose variables proceed in time. A concise
representation of such a system is the state space model. The model
consists of a state vector x(i) where i is an integer variable representing
the discrete time. The dimension of x(i), called the order of the system,
is M. We assume that the state vector is real-valued. The finite-state case
is introduced in Chapter 4.
The process can be influenced by a control vector (input vector) u(i)
with dimension L. The output of the system is given by the measurement
vector (observation vector) z(i) with dimension N. The output is mod-
elled as a memoryless vector that depends on the current values of the
state vector and the control vector.
By definition, the state vector holds the minimum number of variables
which completely summarize the past of the system. Therefore, the state
Classification, Parameter Estimation and State Estimation: An Engineering Approach using MATLAB
F. van der Heijden, R.P.W. Duin, D. de Ridder and D.M.J. Tax
 2004 John Wiley & Sons, Ltd
ISBN: 0-470-09013-8

vector at time i þ 1 is derived from the state vector and the control
vector, both valid at time i:
xði þ 1Þ ¼ fðxðiÞ; uðiÞ; iÞ
zðiÞ ¼ hðxðiÞ; uðiÞ; iÞ
ðd:1Þ
f(:) is a possible nonlinear vector function, the system function, that may
depend explicitly on time. h(:) is the measurement function.
Note that if the time series starts at i ¼ i0, and the initial condition x(i0)
is given, along with the input sequence u(i0), u(i0 þ 1), . . ., then (d.1) can
be used to solve x(i) for every i > i0. Such a solution is called a trajectory.
D.2
LINEAR SYSTEMS
If the system is linear, then the equations evolve into matrix–vector
equations according to:
xði þ 1Þ ¼ FðiÞxðiÞ þ LðiÞuðiÞ
zðiÞ ¼ HðiÞxðiÞ þ DðiÞuðiÞ
ðd:2Þ
The matrices F(i), L(i), H(i) and D(i) must have the appropriate dimen-
sions. They have the following names:
F(i)
system matrix
L(i)
distribution matrix (gain matrix)
H(i)
measurement matrix
D(i)
feedforward gain
Often, the feedforward gain is zero.
The solution of the linear system is found by introduction of the
transition matrix (i, i0), recursively defined by:
ði0; i0Þ ¼ I
ði þ 1; i0Þ ¼ FðiÞði; i0Þ
for
i ¼ i0; i0 þ 1; . . .
ðd:3Þ
Given the initial condition x(i0) at i ¼ i0, and assuming that the feed-
forward gain is zero, the solution is found as:
xðiÞ ¼ ði; i0Þxði0Þ þ
X
i1
j¼i0
ði; j þ 1ÞLðjÞuðjÞ
ðd:4Þ
400
APPENDIX D

D.3
LINEAR TIME INVARIANT SYSTEMS
In the linear time invariant case, the matrices become constants:
xði þ 1Þ ¼ FxðiÞ þ LuðiÞ
zðiÞ ¼ HxðiÞ þ DuðiÞ
ðd:5Þ
and the transition matrix simplifies to:
ði; i0Þ ¼ ði  i0Þ ¼ Fii0
ðd:6Þ
With that, the input/output relation of the system is:
zðiÞ ¼ HFii0xði0Þ þ
X
i1
j¼i0
HFij1LuðjÞ þ DuðiÞ
ðd:7Þ
The first term at the r.h.s. is the free response; the second term is the
particular response; the third term is the feedforward response.
D.3.1
Diagonalization of a system
We assume that the system matrix F has M distinct eigenvectors vk with
corresponding (distinct) eigenvalues k. Thus, by definition Fvk ¼ kvk.
We define the eigenvalue matrix  as the M  M diagonal matrix con-
taining the eigenvalues at the diagonal, i.e. k,k ¼ k, and the eigenvec-
tor matrix V as the matrix containing the eigenvectors as its column
vectors, i.e. V ¼ [ v0    vM1 ]. Consequently:
FV ¼ V
)
F ¼ VV1
ðd:8Þ
A corollary of (d.8) is that the power of F is calculated as follows:
Fk ¼ ðVV1Þk ¼ Vk V1
ðd:9Þ
Equation (d.9) is useful to decouple the system into a number of (scalar)
first order systems, i.e. to diagonalize the system. For that purpose,
define the vector
yðiÞ ¼ V1xðiÞ
ðd:10Þ
LINEAR TIME INVARIANT SYSTEMS
401

Then, the state equation transforms into:
yði þ 1Þ ¼ V1xði þ 1Þ
¼ V1FxðiÞ þ V1LuðiÞ
¼ V1FVyðiÞ þ V1LuðiÞ
¼ yðiÞ þ V1LuðiÞ
ðd:11Þ
and the solution is found as:
yðiÞ ¼ ii0yði0Þ þ
X
i1
j¼i0
ij1V1LuðiÞ
ðd:12Þ
The k-th component of the vector y(i) is:
ykðiÞ ¼ ii0
k
ykði0Þ þ
X
i1
j¼i0
ij1
k
ðV1Luð i ÞÞk
ðd:13Þ
The measurement vector is obtained from y(i) by substitution of
x(i) ¼ Vy(i) in equation (d.5).
If the system matrix does not have distinct eigenvalues, the situation is
a little more involved. In that case, the matrix  gets the Jordan form
with eigenvalues at the diagonal, but with ones at the superdiagonal. The
free response becomes combinations of terms pk(i  i0)ii0
k
where pk(i)
are polynomials in i with order one less than the multiplicity of k.
D.3.2
Stability
In dynamic systems there are various definitions of the term stability. We
return to the general case (d.1) first, and then check to see how the
definition applies to the linear time invariant case. Let xa(i) and xb(i) be
the solutions of (d.1) with a given input sequence and with initial
conditions xa(i0) and xb(i0), respectively.
The solution xa(i) is stable if for every " > 0 we can find a  > 0 such
that
for
every
xb(i0)
with
kxb(i0)  xa(i0)k < 
is
such
that
kxb(i)  xa(i)k < " for all i  i0. Loosely speaking, the system is stable
if small changes in the initial condition of stable solution cannot lead to
very large changes of the trajectory.
402
APPENDIX D

The solution is xa(i) is asymptotically stable, if it is stable, and if
kxb(i)  xa(i)k ! 0 as i ! 1 provided that kxb(i0)  xa(i0)k is not too
large. Loosely speaking, the additional requirement for a system to be
asymptotically stable is that the initial condition does not influence the
solution in the long range.
For linear systems, the stability of one solution assures the stability of
all other solutions. Thus, stability is a property of the system then; not
just of one of its solutions. From (d.13) it can be seen that a linear time
invariant system is asymptotically stable if and only if the magnitude of
the eigenvalues are less than one. That is, the eigenvalues must all be
within the (complex) unit circle.
A linear time invariant system has BIBO stability (bounded input,
bounded output) if a bounded input sequence always gives rise to a
bounded output sequence. Note that asymptotical stability implies BIBO
stability, but the reverse is not true. A system can be BIBO stable, while
it is not asymptotical stable.
D.4
REFERENCES
A˚ stro¨m, K.J. and Wittenmark, B., Computer-Controlled Systems – Theory and Design,
Second Edition, Prentice Hall International, Englewood Cliffs, NJ, 1990.
Luenberger, D.G., Introduction to Dynamic Systems, Theory, Models and Applications,
Wiley, Toronto, 1979.
REFERENCES
403


Appendix E
Introduction to PRTools
E.1
MOTIVATION
In statistical pattern recognition we study techniques for the general-
ization of decision rules to be used for the recognition of patterns in
experimental data sets. This area of research has a strong computational
character, demanding a flexible use of numerical programs for data
analysis as well as for the evaluation of the procedures. As new methods
keep being proposed in the literature, a programming platform is needed
that enables a fast and flexible implementation of such algorithms.
Because of its widespread availability, its simple syntax and general
nature, MATLAB is a good choice for such a platform.
The pattern recognition routines and support functions offered by
PRTools represent a basic set covering largely the area of statistical
pattern recognition. Many methods and proposals, however, are not yet
implemented. Neural networks are only implemented partially, as
MATLAB already includes a very good toolbox in that area. PRTools has
a few limitations. Due to the heavy memory demands of MATLAB, very
large problems with learning sets of tens of thousands of objects cannot be
handled on moderate machines. Moreover, some algorithms are slow as it
can be difficult to avoid nested loops. In the present version, the handling
of missing data has been prepared, but no routines are implemented yet.
Classification, Parameter Estimation and State Estimation: An Engineering Approach using MATLAB
F. van der Heijden, R.P.W. Duin, D. de Ridder and D.M.J. Tax
 2004 John Wiley & Sons, Ltd
ISBN: 0-470-09013-8

The use of fuzzy or symbolic data is not supported, except for soft (and
thereby also fuzzy) labels which are used by just a few routines. Multi-
dimensional target fields are allowed, but at this moment no procedure
makes use of this possibility.
The notation used in the PRTools documentation and code differs
slightly from that used in the code throughout this book. In this appendix
we try to follow the notation in the book. In Table E.1 notation differ-
ences between this book and the PRTools documentation are given.
E.2
ESSENTIAL CONCEPTS IN PRTOOLS
For recognizing the classes of objects they are first scanned by sensors, then
represented, e.g. in a feature space, and after some possible feature reduc-
tion steps they are finally mapped by a classifier to the set of class labels.
Between the initial representation in the feature space and this final map-
ping to the set of class labels the representation may be changed several
times: simplified feature spaces (feature selection), normalization of features
(e.g. by scaling), linear or nonlinear mappings (feature extraction), classifi-
cation by a possible set of classifiers, combining classifiers and the final
labelling. In each of these steps the data is transformed by some mapping.
Based on this observation. PRTools defines the following two basic
concepts:
. Data sets: matrices in which the rows represent the objects and the
columns the features, class memberships or other fixed sets of
properties (e.g. distances to a fixed set of other objects).
. Mappings: transformations operating on data sets.
As pattern recognition has two stages, training and execution, map-
pings also have two main types:
Table E.1
Notation differences between this book and the PRTools
documentation
Mathematical
notation
Notation in
pseudo-code
PRTools
notation
Meaning
T
x, z
a, b
data set
n
n
m
number of objects
N, D
N, D
k, n
number of features, dimensions
K
K
C
number of classes
406
APPENDIX E

. An untrained mapping refers to just the concept of a method, e.g.
forward feature selection, PCA or Fisher’s linear discriminant. It may
have some parameters that are needed for training, e.g. the desired
number of features or some regularization parameters. If an untrained
mapping is applied to a data set it will be trained by it (training).
. A trained mapping is specific for its training set. This data set
thereby determines the input dimensionality (e.g. the number of
input features) as well as the output dimensionality (e.g. the num-
ber of output features or the number of classes). If a trained map-
ping is applied to a data set, it will transform the data set according
to its definition (execution).
In addition, fixed mappings are used. They are almost identical to
trained mappings, except that they do not result from a training step, but
are directly defined by the user: e.g. the transformation of distances by a
sigmoid function to the [0, 1] interval.
PRTools deals with sets of labelled or unlabelled objects and offers
routines for the generalization of such sets into functions for mapping
and classification. A classifier is a special case of a mapping, as it maps
objects on class labels or on [0, 1] intervals that may be interpreted as
class memberships, soft labels or posterior probabilities. An object is a
N-dimensional vector of feature values, distances, (dis)similarities or
class memberships. Within PRTools they are usually just called features.
It is assumed that for all objects in a problem all values of the same set of
features are given. The space defined by the actual set of features is
called the feature space. Objects are represented as points or vectors in
this space. New objects in a feature space are usually gradually con-
verted to labels by a series of mappings followed by a final classifier.
E.3
IMPLEMENTATION
PRTools uses the object-oriented features of the MATLAB programming
language. Two object classes (not to be confused with the objects and
classes in pattern recognition) have been defined: dataset and
mapping. A large number of operators (like *, ½  etc.) and MATLAB
commands have been overloaded and have a special meaning when
applied to a dataset and/or a mapping.
The central data structure of PRTools is the dataset. It primarily
consists of a set of objects represented by a matrix of feature vectors.
Attached to this matrix is a set of labels, one for each object and a set of
IMPLEMENTATION
407

feature names, also called feature labels. Labels can be integer numbers
or character strings. Moreover, a set of prior probabilities, one for each
class, is stored. In most help files of PRTools, a dataset is denoted by a.
We will use z in this text to stay consistent with the rest of the book. In
almost any routine this is one of the inputs. Almost all routines can
handle multi-class object sets. It is possible that for some objects no
label is specified (a NaN is used, or an empty string). Such objects are,
unless otherwise mentioned, skipped during training.
Data structures of the object class mapping store data transforma-
tions (‘mappings’), classifiers, feature extraction results, data scaling
definitions, nonlinear projections, etc. They are usually denoted by w.
The easiest way of applying a mapping w to a data set z is by z*w. The
matrix multiplication symbol * is overloaded for this purpose. This
operation may also be written as map(z, w). Like everywhere else in
MATLAB, longer series of operations are possible, e.g. z*w1*w2*w3, and
are executed from left to right.
Listing E.1
z ¼ gendath([50 50]);
% generate data, 50 objects/class
[y,x] ¼ gendat(z, [20 20]);
% split into training and test set
w1 ¼ ldc(y);
% linear classifier
w2 ¼ qdc(y);
% quadratic
w3 ¼ parzenc(y);
% Parzen density-based
w4 ¼ bpxnc(y,3);
% neural net with 3 hidden units
disp([testc(x*w1),testc(x*w2),testc(x*w3),testc(x*w4)]);
% compute and display classification errors
scatterd(z);
% scatter plot of data
plotc({w1,w2,w3,w4});
% plot decision boundaries
A typical example is given in Listing E.1. This command file first
generates two sets of labelled objects using gendath, both containing
50 two-dimensional object vectors, and stores them, their labels and
prior probabilities in the dataset z. The distribution follows the so-called
‘Higleyman classes’. The next call to gendat takes this dataset and
splits it randomly into a dataset y, further on used for training, and a
dataset x, used for testing. This training set y contains 20 objects
from each class. The remaining 2  30 objects are collected in x.
In the next lines four classification functions (discriminants) are com-
puted, called w1, w2, w3 and w4. The first three are in fact density
estimators based on various assumptions (class priors stored in y are
taken into account). Programmatically, they are just mappings, as
xx ¼ x*w1 computes the class densities for the objects stored in d.
408
APPENDIX E

xx has as many columns as there are classes in the training set for w1 (here
two). The test routine testc assigns objects (represented by the rows in xx)
to the class corresponding to the highest density (times prior probability),
the mappings w1, .. . ,w4 can be used as classifiers. The linear classifier w1
(ldc) and quadratic classifier w2 (qdc) are both based on the assumption of
normally distributed classes. The first assumes equal class covariance
matrices. The Parzen classifier estimates the class densities by the Parzen
density estimation and has a built-in optimization of the kernel width. The
fourth classifier uses a feed forward neural network with three hidden units.
It is trained by the back propagation rule using a varying step size.
The results are then displayed and plotted. The test data set x is used
in a routine testc (test classifier) on each of the four discriminants.
They are combined in a cell array, but individual calls are possible as
well. The estimated probabilities of error are displayed in the MATLAB
command window and may look like (note that they may differ due to
different random seeds used in the generation of the data):
0:1500
0:0333
0:1333
0:0833
Finally the classes are plotted in a scatter diagram (scatterd)
together with the discriminants (plotc); see Figure E.1
–1
0
1
2
3
–4
–3
–2
–1
0
1
2
3
Feature 1
Feature 2
Higleyman data set
Bayes–Normal–1
Bayes–Normal–2
Parzen Classifier
BP Neural Classf
Figure E.1
Example output of Listing E.1
IMPLEMENTATION
409

For more advanced examples, see the Examples section in help
prtools.
E.4
SOME DETAILS
The command help files and the examples should give sufficient infor-
mation to use the toolbox with a few exceptions. These are discussed in
the following sections. They deal with the ways classifiers and mappings
are represented. As these are the constituting elements of a pattern
recognition analysis, it is important that the user understands these
issues.
E.4.1
Data sets
A dataset consists of a set of n objects, each given by D features.
In PRTools such a data set is represented by an n by D matrix: n rows,
each containing an object vector of D features. Usually a data set is
labelled. An example of a definition is:
 z ¼ dataset([1 2 3; 2 3 4; 3 4 5; 4 5 6],[3; 3; 5; 5])
4 by 3 dataset with 2 classes: [2 2]
The 4  3 data matrix (four objects given by three features) is accom-
panied by a label list of four labels, connecting each of the objects to
one of the two classes, labelled 3 and 5. Class labels can be numbers
or strings and should always be given as rows in the label list. If
the label list is not given all objects are given the default label 1.
In addition it is possible to assign labels to the columns (features) of a
data set:
 z ¼ dataset(rand(100,3),genlab([50 50],[3; 5]));
 z ¼ setfeatlab(z,[’r1’;’r2’;’r3’])
100 by 3 dataset with 2 classes: [50 50]
The routine genlab generates 50 labels with value 3, followed by 50
labels with value 5. Using setfeatlab the labels (0r10, 0r20, 0r30) for
the three features are set. Various other fields can be set as well. One of
the ways to see these fields is by converting the data set to a structure,
410
APPENDIX E

using struct(x). Fields can be inspected individually by the :- exten-
sion, also defined for data sets:
 x.lablist
ans ¼
3
5
The possibility to set prior probabilities for each of the classes by
setprior(x, prob, lablist) is important. The prior values in
prob should sum to one. If prob is empty or if it is not supplied the
prior probabilities are computed from the data set label frequencies. If
prob equals zero then equal class probabilities are assumed.
Various items stored in a data set can be retrieved by commands like
getdata, getlablist and getnlab. The last one retrieves the
numeric labels for the objects (1, 2, . . . ) referring to the true labels stored
in the rows of lablist. The size of the data set can be found by:
[n,D] ¼ size(x); or [n,D,K] ¼ getsize(xa);
in which n is the number of objects, D the number of features and K the
number of classes (equal to max(nlab)). Data sets can be combined by
[x; y] if x and y have equal numbers of features and by [x y] if they
have equal numbers of objects. Creating subsets of data sets can be done
by z(I,J) in which I is a set of indices defining the desired objects and J
is a set of indices defining the desired features.
The original data matrix can be retrieved by double(z) or by þz. The
labels in the objects of x can be retrieved by labels ¼ getlabels(z),
which is equivalent to
[nlab,lablist] ¼ get (z,’nlab’,’lablist’);
labels ¼ lablist(nlab,:);
Be aware that the order of classes returned by getprob and getlablist
is the standard order used in PRTools and may differ from the one used in the
definition of z. For more information, type help datasets.
E.4.2
Classifiers and mappings
There are many commands to train and use mappings between spaces of
different (or equal) dimensionalities. In general, the following applies:
SOME DETAILS
411

if
z
is an n by D data set (n objects in a D-dimensional
space)
and w
is a D by K mapping (map from D to K dimensions)
then z*w
is an n by K data set (n objects in an K-dimensional
space)
Mappings can be linear or affine (e.g. a rotation and a shift) as well as
nonlinear (e.g. a neural network). Typically they can be used as classi-
fiers. In that case a D by K mapping maps a D-feature data vector on the
output space of an K-class classifier (exception: two-class classifiers like
discriminant functions may be implemented by a mapping to a one-
dimensional space like the distance to the discriminant, K ¼ 1).
Mappings are of the data type ‘mapping’ (class(w) is ‘mapping’),
have a size of ½D, K if they map from D to K dimensions. Mappings can
be instructed to assign labels to the output columns, e.g. the class names.
These labels can be retrieved by
labels ¼ getlabels(w); %before the mapping, or
labels ¼ getlabels (z*w); %after the data set z is mapped by w.
Mappings can be learned from examples, (labelled) objects stored in a
data set z, for instance by training a classifier:
w1 ¼ ldc(z);
%the normal densities based linear classifier
w2 ¼ knnc(z,3);
%the 3-nearest neighbour rule
w3 ¼ svc(z,’p’,2);
%the support vector classifier based on a 2nd
order polynomial kernel
Untrained or empty mappings are supported. They may be very useful.
In this case the data set is replaced by an empty set or entirely skipped:
v1 ¼ ldc; v2 ¼ knnc([],3); v3 ¼ svc([],’p’,2);
Such mappings can be trained later by
w1 ¼ z*v1; w2 ¼ z*v2; w3 ¼ z*v3;
(which is equivalent to the statements a few lines above) or by using cell
arrays
v ¼ {ldc, knnc ([], 3), svc([],’p’,2)}; w ¼ z*v;
412
APPENDIX E

The mapping of a test set y by y*w1 is now equivalent to y*(z*v1).
Note that expressions are evaluated from left to right, so y*z*v1 will
result in an error as the multiplication of the two data sets (y*x) is
executed first.
Some trainable mappings do not depend on class labels and can be
interpreted as finding a feature space that approximates as good as
possible the original data set given some conditions and measures.
Examples are the Karhunen–Loe`ve mapping (klm), principal component
analysis (pca) and kernel mapping (kernelm) by which nonlinear,
kernel PCA mappings can be computed.
In addition to trainable mappings, there are fixed mappings, the
parameters of which cannot be trained. A number of them can be set
by cmapm; others are sigm and invsigm.
The result x of mapping a test set on a trained classifier, x ¼ y*w1, is
again a data set, storing for each object in y the output values of the
classifier. For discriminants they are sigmoids of distances, mapped on
the [0, 1] interval, for neural networks their unnormalized outputs and
for density based classifiers the densities. For all of them the following
holds: the larger, the more similar with the corresponding class. The
values in a single row (object) don’t necessarily sum to one. This can be
achieved by the fixed mapping classc:
x ¼ y*w1*classc
The values in x can be interpreted as posterior probability estimates or
classification confidences. Such a classification data set has column
labels (feature labels) for the classes and row labels for the objects.
The class labels of the maximum values in each object row can be
retrieved by
labels ¼ x*labeld; or labels ¼ labeld(x);
A global classification error follows from
e ¼ x*testc; or e ¼ testc(x);
Mappings can be inspected by entering w by itself, or using
display(w). This lists the size and type of a mapping or classifier as
well as the routine used for computing a mapping z*w. The data stored
in the mapping might be retieved using þw. Note that the type of data
stored in each mapping depends on the type of mapping.
SOME DETAILS
413

Affine mappings (e.g. constructed by klm) may be transposed. This is
useful for back projection of data into the original space. For instance:
w ¼ klm(x,3);
% compute 3-dimensional KL transform
y ¼ x*w;
% map x using w, resulting in y
z ¼ y*w’;
% back-projection to the original space
A mapping may be given an output selection by w ¼ w(: ,J), in which J is a
set of indices pointing to the desired classes; y ¼ z*w(: ,J); is equivalent
to y ¼ z*w; y ¼ y(: ,J); . Input selection is not possible for a mapping.
For more information, type help mappings.
E.5
HOW TO WRITE YOUR OWN MAPPING
Users can add new mappings or classifiers by a single routine that should
support the following type of calls:
. w ¼ newmapm([], par1, . . . );
Defines the untrained, empty mapping.
. w ¼ newmapm(z, par1, . . . );
Defines the map based on the training data set z.
. y ¼ newmapm(z, w);
Defines the mapping of data set z using w, resulting in a data set y.
For an example, list the routine subsc (using typesubsc). This classifier
approximates each class by a linear subspace and assigns new objects to
the class of the closest subspace found in the training set. The dimension-
alities of the subspaces can be directly set by w ¼ subsc(z, n), in which
the integer n determines the dimensionality of all class subspaces, or by
w ¼ subsc(z, alf), in which alf is the desired fraction of variance to
retain, e.g. alf ¼ 0:95. In both cases the class subspaces v (see the listing)
are determined by a principal component analysis of the single-class
data sets.
The three possible types of calls, listed above, are handled in the three
main
parts
of
the
routine.
If
no
input
parameters
are
given
(nargin <1) or no input data set is found (z is empty) an untrained
classifier is returned. This is useful for calls like w ¼ subsc([], n), defin-
ing an untrained classifier to be used in routines like cleval(z, w, . . . )
that operate on arbitrary untrained classifiers, but also to facilitate
training by constructions as w ¼ z*subsc or w ¼ z*subsc([],n).
414
APPENDIX E

The training section of the routine is accessed if z is not empty and n is
either not supplied or set by the user as a double (i.e. the subspace
dimensionality or the fraction of the retained variance). PRTools
takes
care
that
calls
like
w ¼ z*subsc([], n)
are
executed
as
w ¼ subsc(z, n). The first parameter in the mapping definitions
w ¼ mapping(mfilename, . . . is substituted by MATLAB as 0subsc0
(mfilename is a MATLAB function that returns the name of the calling
file). This string is stored by PRTools in the mapping_file field of the
mapping w and used to call subsc whenever it has to be applied to a
data set. For some special mappings, like ldc, another file might be used
(in the case of ldc it is normal_map).
The trained mapping w can be applied to a test data set y by x ¼
y*w or by z ¼ map(y, w). Such a call is converted by PRTools to
x ¼ subsc(y, w). Consequently, the second parameter of subsc(), n,
is now substituted by the mapping w. This is executed in the final part of
the routine. Here, the data stored in the data field of w during training is
retrieved (class mean, rotation matrix and mean square distances of
the training objects) and used to find normalized distances of the test
objects to the various subspaces. Finally they are converted to a density,
assuming a normal distribution of distances. These values are returned in
a data set using the setdata routine. This data set is thereby similar to
the input data set: same object labels, object identifiers, etc. Just the data
matrix itself is changed and the columns now refer to classes instead of
features. The definition of the mappings and data sets form the core of
the PRTools toolbox. There are numerous supporting algorithms for
inspecting and visualizing classifiers and classfication results. Also the
possibilities for combining classifiers is not discussed. In the file
Contents:m (which can be inspected in MATLAB by help prtools)
the most important commands are listed. A final possibility is to inspect
all *:m. files and use help.
HOW TO WRITE YOUR OWN MAPPING
415


Appendix F
MATLAB Toolboxes Used
Apart from the PRTools toolbox,1 this book has used the following
standard MATLAB toolboxes:
. Control System Toolbox
This toolbox contains many functions and data structures for the
modelling and design of control systems. However, the linear time
invariant cases are emphasized. As such, the toolbox contains a num-
ber of functions that are of interest for state estimation. But, unfortu-
nately, the scope of these functions is limited to the steady state case.
. Signal Processing Toolbox
This toolbox is a collection of processing operations. Much atten-
tion is paid to the realization aspect of filters, i.e. how to build a
filter with given properties, e.g. a given cut-off frequencies, con-
strained phase characteristics and so on. This aspect is of less
importance for the design of state estimators. Nevertheless, the
toolbox contains a number of functions that are of interest with
respect to the scope of this book:
. – correlation and covariance estimation
– spectral analysis, e.g. the periodogram
1 And with PRTools, implicitly the Neural Network Toolbox.
Classification, Parameter Estimation and State Estimation: An Engineering Approach using MATLAB
F. van der Heijden, R.P.W. Duin, D. de Ridder and D.M.J. Tax
 2004 John Wiley & Sons, Ltd
ISBN: 0-470-09013-8

– estimation of parameters of random processes, e.g. the AR coef-
ficients based on, for instance, the Yule–Walker equations
– some various tools, such as waveform generation and resampling
functions.
. Optimization Toolbox
This toolbox contains many functions for optimization problems
including constrained or unconstrained, linear or nonlinear mini-
mization. The toolbox also contains functions for curve fitting.
. Statistics Toolbox
This toolbox includes functions for the following topics (among
others):
* many distribution and density functions of random variables and
corresponding random number generators
* parameter estimation for some standard distributions
* linear and nonlinear regression methods
* methods for principal components analysis
* functions for the analysis of hidden Markov models.
. System Identification Toolbox
This toolbox contains various methods for the identification of
dynamic systems. Many models are available, but nevertheless the
emphasis is on linear time invariant systems.
MATLAB offers many more interesting toolboxes, e.g. for the analysis of
financial time series (Financial Time Series Toolbox; Garch Toolbox),
and for calibration and curve fitting (Curve Fitting Toolbox, Model-
based Calibration Toolbox), but these toolboxes are not used in this
book.
418
APPENDIX F

Index
Acceptance boundary, 297
Algorithm
backward, 122
condensation, 131
forward, 116
forward–backward, 123
Viterbi, 125
ARIMA, 264
ARMA, 264
Autoregressive model, 264
first order, 91, 341
second order, 92, 137
Autoregressive, moving average
models, 137
Back-propagation training, 175
Baseline removal, 259
Batch processing, 166
Bayes estimation, 142
Bayes’ theorem, 7, 20, 48
Bayesian classification, 16, 21
Bhattacharyya upper bound, 192
Bias, 63, 142, 332
Binary measurements, 148
Branch-and-bound, 197
Chernoff bound, 192
Chi-square test, 346
Classifier
Bayes, 8, 33
Euclidean distance, 147
feed-forward neural network, 173, 317
least squared error, 166
linear, 29, 311
linear discriminant function, 162
Mahalanobis distance, 147
maximum a posteriori (MAP), 14, 35
minimum distance, 30
minimum error rate, 24, 33
nearest neighbour, 155, 312
Parzen density-based, 150, 312
perceptron, 164
quadratic, 27, 311
support vector, 168, 316
Clustering, 226
average-link, 229
characteristics, 216
complete-link, 229
hierarchical, 228
K-means, 228
quality, 227
single-link, 229
Completely
controllable, 269
observable, 272
Computational complexity, 178
Computational issues, 253
Condensing, 159, 160
Confusion matrix, 178
Consistency checks, 292,
296, 342
Control vector, 89
Controllability matrix, 269
Cost
absolute value, 50
function, 19, 33, 50
matrix, 19
quadratic, 50
uniform, 35, 50
Covariance, 63
Covariance model (CVM) based
estimator, 331
Covariance models, 327
Cross-validation, 180, 312, 332
Classification, Parameter Estimation and State Estimation: An Engineering Approach using MATLAB
F. van der Heijden, R.P.W. Duin, D. de Ridder and D.M.J. Tax
 2004 John Wiley & Sons, Ltd
ISBN: 0-470-09013-8

Curve
calibration, 350
fitting, 324
Decision boundaries, 27, 150
Decision function, 17
Dendrogram, 230
Depth-first search, 198
Design set, 140
Detection, 35
Differencing, 301
Discrete
algebraic Ricatti equation,
98
Lyapunov equation, 90, 274
Ricatti equation, 271
Discriminability, 39, 41
Discriminant function, 162
generalized linear, 163
linear, 162
Dissimilarity, 226
Distance
Bhattacharyya, 191, 204
Chernoff, 186, 191
cosine, 226
Euclidean, 226
inter/intraclass, 186, 209
interclass, 189
intraclass, 189
Mahalanobis, 205
Matusita, 194
probabilistic, 194
Distribution
Gamma, 49
matrix, 89
test, 297
Drift, 261
Dynamic stability, 270
Editing, 159
Entropy, 195
Envelope detection, 323
Ergodic Markov model, 114
Error correction, 166
Error covariance matrix, 98
Error function, 39
Error rate, 24, 33
Estimation
maximum a posteriori
(MAP), 50, 51
maximum likelihood, 57
minimum mean absolute error
(MMAE), 50, 51
minimum mean squared error
(MMSE), 50, 51
minimum variance, 55
Estimation loop, 277
Evaluation, 263
Evaluation set, 177
Expectation-maximization, 235
Experiment design, 258
Feature, 14
Feature extraction, 185
Feature reduction, 216
Feature selection, 185
generalized sequential forward,
200
Plusl – take away r, 200
selection of good components, 330
sequential forward, 200
Feed-forward neural network, 173
Fisher approach, 57
Fisher’s linear discriminant, 213
Fudge factor, 300
Gain matrix, 89
Generative topographic mapping,
246
Goodness of fit, 346
Gradient ascent, 164
Hidden data, 235
Hidden Markov model (HMM), 113
Hidden neurons, 174
Hill climbing algorithm, 235
Histogramming, 150
Holdout method, 179
i.i.d., 140
Identification of linear systems, 264
Image compression, 219
Importance sampling, 128, 131
Incomplete data, 235
Indicator variables, 236
Information
filter, 269, 283
matrix, 67, 284
Innovation(s), 62, 293
matrix, 98
Input vector, 89
Kalman
filtering, 254
form, 62, 280
linear-Gaussian MMSE form, 280
420
INDEX

Kalman filter, 97
discrete, 97, 98
extended, 105, 343
iterated extended, 108
linearized, 101, 341
unscented, 112
Kalman gain matrix, 62, 98
Kernel, 151
Gaussian, 171
polynomial, 171
radial basis function (RBF),
171
trick, 171
K-nearest neighbour rule,
157
Kohonen map, 241
Lagrange multipliers, 170
Latent variable, 246
Learning, 139
least squared error, 166
nonparametric, 149–50
parametric, 142
perceptron, 164
supervised, 139
unsupervised, 139
Learning data, 140
Learning rate, 164
Least squared error (LSE), 68
Leave-one-out method, 180
Left–right model, 114
Level estimation, 339
Likelihood
function, 36, 57
ratio, 36
Linear
dynamic equation, 89
plant equation, 89
state equation, 89
system equation, 89
Linear feature extraction,
202
Linear feedback, 102
Linear-Gaussian system, 89
Log-likelihood, 262
Loss function, 19
Mahalanobis distance, 28
Mahalanobis distance
classifier, 213
Margin, 169
Markov condition, 83, 114
Matched filtering, 326
MATLAB functions
HMM analysis, 127
particle filtering, 112
steady state Kalman filtering,
137, 268, 270, 273
Maximum likelihood, 326, 327
Mean square error, 64
Measure
divergence, 194
Matusita, 194
Shannon’s entropy, 195
Measurement
matrix, 96
model, 86
noise, 96
space, 14, 86
vector, 14, 163
Minimum error rate, 158
Minimum mean squared
error, 217
Minimum risk classification,
21
Missing data, 235
Mixture
of Gaussians, 228
of probabilistic PCA, 240
Model selection, 263
Models, 83
Monte Carlo simulation, 128
Moving average models
first order, 137
Multi-dimensional scaling,
220
Multi-edit algorithm, 160
Nearest neighbour rule, 157
Neuron, 173
Noise, 48, 75
autocorrelated, 300
cross-correlated, 303
measurement, 96
plant, 89
process, 89
quantization, 17
quantum, 17
system, 89
thermal, 17, 96
Nominal trajectory, 105
Nonlinear mapping, 221
Normalized
estimation error squared, 294
importance weights, 130
innovation squared, 295, 342
INDEX
421

Observability, 253, 266
complete, 266
Gramian, 267
matrix, 267
stochastic, 269
Offset correction, 259
Online estimation, 82
Optimal filtering, 82
Optimization criterion, 185
Outlier clusters, 228
Outliers, 72
Overfitting, 76, 177, 184, 263
Parameter vector, 48
Partial autocorrelation
function, 265
Particle filter, 112, 128, 344
consistency criterion, 345
implementation, 347
Particles, 128–9
Parzen estimation, 150
Perceptron, 165
Periodogram, 297
Place coding, 169
Posterior, 48
prior, 48
Potter’s square root filter, 287
Predicted measurement, 98
Prediction, 82, 94
fixed interval, 95
fixed lead, 95
Principal
component analysis, 216, 313, 329
components, 218
directions, 218
Principle of orthogonality, 294
Probabilistic dependence, 194
Probability
posterior, 20
prior, 16–17
Probability density
conditional, 17, 36, 48, 83, 86
posterior, 86
unconditional, 17
Proposal density, 129
Quadratic decision function, 27
Quantization errors, 96
Random walk, 92
Rauch–Tung–Striebel smoother, 304
Regression, 74, 321, 351
curve, 75
Regularization, 146
parameter, 146
Reject rate, 33
Rejection, 32
class, 32
Resampling by selection, 130
Residual(s), 68, 75, 293
Retrodiction, 82
Ricatti loop, 277
Risk, 21
average, 21, 51
conditional, 20, 23, 51
Robust error norm, 72
Robustness, 65
ROC-curve, 40
Root mean square (RMS), 337
Sammon mapping, 224
Sample
covariance, 145
mean, 143
Scatter diagram, 15
Scatter matrix, 188
between-scatter matrix, 188
within-scatter matrix, 188
Self-organizing map, 241
Sensor, 258
location, 258
Sensory system, 13
Sequential update, 283
Signal-to-noise ratio, 38, 190
Single sample processing, 166
Smoothing, 303
Square root filtering, 287
Stability, 65, 253, 270
State space model, 81, 83
State
augmentation, 120
estimation
offline, 120
online, 117
mixed, 128
variable, 81
continuous, 81, 88
discrete, 81, 113
Statistical linearization, 112
Steady state, 98, 270
Steepest ascent, 164
Stochastic observability, 269
Stress measure, 221
Subspace
dimension, 240
structure, 216
422
INDEX

Sum of squared differences
(SSD), 68, 324
Support vector, 170
System identification,
253, 254, 256
Target vector, 166, 175
Test set, 177
Time-of-flight estimation, 319
Topology, 241
Training, 139
Training set, 140
Transfer function, 173
True class, 140
Unbiased, 64
absolutely, 64
Unit cost, 23
Validation set, 177
Variance, 142
Winning neuron, 242
Wishart distribution, 144
Yule–Walker equations, 264
INDEX
423


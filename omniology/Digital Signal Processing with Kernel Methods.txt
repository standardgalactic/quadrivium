Digital Signal Processing with Kernel Methods

Digital Signal Processing with Kernel Methods
José Luis Rojo-Álvarez
Department of Signal Theory and Communications
University Rey Juan Carlos
Fuenlabrada (Madrid)
and
Center for Computational Simulation
Universidad Politécnica de Madrid, Spain
Manel Martínez-Ramón
Department of Electrical and Computer Engineering
The University of New Mexico
Albuquerque, New Mexico
USA
Jordi Muñoz-Marí
Department of Electronics Engineering
Universitat de València
Paterna (València), Spain
Gustau Camps-Valls
Department of Electronics Engineering
Universitat de València
Paterna (València), Spain

This edition ﬁrst published 
© John Wiley & Sons Ltd
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or
transmitted, in any form or by any means, electronic, mechanical, photocopying, recording or otherwise,
except as permitted by law. Advice on how to obtain permission to reuse material from this title is available
at http://www.wiley.com/go/permissions.
The right of José Luis Rojo-Álvarez, Manel Martínez-Ramón, Jordi Muñoz-Marí, Gustau Camps-Valls to be
identiﬁed as the authors of the editorial material in this work has been asserted in accordance with law.
Registered Oﬃces
John Wiley & Sons, Inc., River Street, Hoboken, NJ , USA
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex, POSQ, UK
Editorial Oﬃce
The Atrium, Southern Gate, Chichester, West Sussex, POSQ, UK
For details of our global editorial oﬃces, customer services, and more information about Wiley products
visit us at www.wiley.com.
Wiley also publishes its books in a variety of electronic formats and by print-on-demand. Some content that
appears in standard print versions of this book may not be available in other formats.
Limit of Liability/Disclaimer of Warranty
MATLABⓇand Simulink is a trademark of The MathWorks, Inc. and is used with permission. The
MathWorks does not warrant the accuracy of the text or exercises in this book. This work’s use or discussion
of MATLABⓇsoftware or related products does not constitute endorsement or sponsorship by The
MathWorks of a particular pedagogical approach or particular use of the MATLABⓇsoftware.
While the publisher and authors have used their best eﬀorts in preparing this work, they make no
representations or warranties with respect to the accuracy or completeness of the contents of this work and
speciﬁcally disclaim all warranties, including without limitation any implied warranties of merchantability
or ﬁtness for a particular purpose. No warranty may be created or extended by sales representatives, written
sales materials or promotional statements for this work. The fact that an organization, website, or product is
referred to in this work as a citation and/or potential source of further information does not mean that the
publisher and authors endorse the information or services the organization, website, or product may
provide or recommendations it may make. This work is sold with the understanding that the publisher is not
engaged in rendering professional services. The advice and strategies contained herein may not be suitable
for your situation. You should consult with a specialist where appropriate. Further, readers should be aware
that websites listed in this work may have changed or disappeared between when this work was written and
when it is read. Neither the publisher nor authors shall be liable for any loss of proﬁt or any other
commercial damages, including but not limited to special, incidental, consequential, or other damages.
Library of Congress Cataloging-in-Publication data
Names: Rojo-Álvarez, José Luis, – author. | Martínez-Ramón, Manel, – author. |
Muñoz-Marí, Jordi, author. | Camps-Valls, Gustau, – author.
Title: Digital signal processing with kernel methods / by Dr. José Luis Rojo-Álvarez,
Dr. Manel Martínez-Ramón, Dr. Jordi Muñoz-Marí, Dr. Gustau Camps-Valls.
Description: First edition. | Hoboken, NJ : John Wiley & Sons, . | Includes bibliographical
references and index. |
Identiﬁers: LCCN (print) | LCCN (ebook) | ISBN (pdf) |
ISBN (epub) | ISBN (cloth)
Subjects: LCSH: Signal processing–Digital techniques.
Classiﬁcation: LCC TK.(ebook) | LCC TK..R(print) | DDC ./–dc
LC record available at https://lccn.loc.gov/
Cover design by Wiley
Cover image: ©AF-studio/Gettyimages
Set in /pt Warnock by SPi Global, Pondicherry, India











v
Contents
About the Authors
xiii
Preface
xvii
Acknowledgements
xxi
List of Abbreviations
xxiii
Part I
Fundamentals and Basic Elements

1
From Signal Processing to Machine Learning

.
A New Science is Born: Signal Processing

..
Signal Processing Before Being Coined

..
: Birth of the Information Age

..
s: Audio Engineering Catalyzes Signal Processing

.
From Analog to Digital Signal Processing

..
s: Digital Signal Processing Begins

..
s: Digital Signal Processing Becomes Popular

..
s: Silicon Meets Digital Signal Processing

.
Digital Signal Processing Meets Machine Learning

..
s: New Application Areas

..
s: Neural Networks, Fuzzy Logic, and Genetic Optimization

.
Recent Machine Learning in Digital Signal Processing

..
Traditional Signal Assumptions Are No Longer Valid

..
Encoding Prior Knowledge

..
Learning and Knowledge from Data

..
From Machine Learning to Digital Signal Processing

..
From Digital Signal Processing to Machine Learning

2
Introduction to Digital Signal Processing

.
Outline of the Signal Processing Field

..
Fundamentals on Signals and Systems

..
Digital Filtering

..
Spectral Analysis

..
Deconvolution

..
Interpolation


vi
Contents
..
System Identiﬁcation

..
Blind Source Separation

.
From Time–Frequency to Compressed Sensing

..
Time–Frequency Distributions

..
Wavelet Transforms

..
Sparsity, Compressed Sensing, and Dictionary Learning

.
Multidimensional Signals and Systems

..
Multidimensional Signals

..
Multidimensional Systems

.
Spectral Analysis on Manifolds

..
Theoretical Fundamentals

..
Laplacian Matrices

.
Tutorials and Application Examples

..
Real and Complex Signal Processing and Representations

..
Convolution, Fourier Transform, and Spectrum

..
Continuous-Time Signals and Systems

..
Filtering Cardiac Signals

..
Nonparametric Spectrum Estimation

..
Parametric Spectrum Estimation

..
Source Separation

..
Time–Frequency Representations and Wavelets

..
Examples for Spectral Analysis on Manifolds

.
Questions and Problems

3
Signal Processing Models

.
Introduction

.
Vector Spaces, Basis, and Signal Models

..
Basic Operations for Vectors

..
Vector Spaces

..
Hilbert Spaces

..
Signal Models

..
Complex Signal Models

..
Standard Noise Models in DSP

..
The Role of the Cost Function

..
The Role of the Regularizer

.
Digital Signal Processing Models

..
Sinusoidal Signal Models

..
System Identiﬁcation Signal Models

..
Sinc Interpolation Models

..
Sparse Deconvolution

..
Array Processing

.
Tutorials and Application Examples

..
Examples of Noise Models

..
Autoregressive Exogenous System Identiﬁcation Models

..
Nonlinear System Identiﬁcation Using Volterra Models

..
Sinusoidal Signal Models


Contents
vii
..
Sinc-based Interpolation

..
Sparse Deconvolution

..
Array Processing

.
Questions and Problems

.A
MATLAB simpleInterp Toolbox Structure

4
Kernel Functions and Reproducing Kernel Hilbert Spaces

.
Introduction

.
Kernel Functions and Mappings

..
Measuring Similarity with Kernels

..
Positive-Deﬁnite Kernels

..
Reproducing Kernel in Hilbert Space and Reproducing Property

..
Mercer’s Theorem

.
Kernel Properties

..
Tikhonov’s Regularization

..
Representer Theorem and Regularization Properties

..
Basic Operations with Kernels

.
Constructing Kernel Functions

..
Standard Kernels

..
Properties of Kernels

..
Engineering Signal Processing Kernels

.
Complex Reproducing Kernel in Hilbert Spaces

.
Support Vector Machine Elements for Regression and Estimation

..
Support Vector Regression Signal Model and Cost Function

..
Minimizing Functional

.
Tutorials and Application Examples

..
Kernel Calculations and Kernel Matrices

..
Basic Operations with Kernels

..
Constructing Kernels

..
Complex Kernels

..
Application Example for Support Vector Regression Elements

.
Concluding Remarks

.
Questions and Problems

Part II
Function Approximation and Adaptive Filtering

5
A Support Vector Machine Signal Estimation Framework

.
Introduction

.
A Framework for Support Vector Machine Signal Estimation

.
Primal Signal Models for Support Vector Machine Signal Processing

..
Nonparametric Spectrum and System Identiﬁcation

..
Orthogonal Frequency Division Multiplexing Digital Communications

..
Convolutional Signal Models

..
Array Processing

.
Tutorials and Application Examples


viii
Contents
..
Nonparametric Spectral Analysis with Primal Signal Models

..
System Identiﬁcation with Primal Signal Model γ-ﬁlter

..
Parametric Spectral Density Estimation with Primal Signal Models

..
Temporal Reference Array Processing with Primal Signal Models

..
Sinc Interpolation with Primal Signal Models

..
Orthogonal Frequency Division Multiplexing with Primal Signal Models

.
Questions and Problems

6
Reproducing Kernel Hilbert Space Models for Signal Processing

.
Introduction

.
Reproducing Kernel Hilbert Space Signal Models

..
Kernel Autoregressive Exogenous Identiﬁcation

..
Kernel Finite Impulse Response and the γ-ﬁlter

..
Kernel Array Processing with Spatial Reference

..
Kernel Semiparametric Regression

.
Tutorials and Application Examples

..
Nonlinear System Identiﬁcation with Support Vector
Machine–Autoregressive and Moving Average

..
Nonlinear System Identiﬁcation with the γ-ﬁlter

..
Electric Network Modeling with Semiparametric Regression

..
Promotional Data

..
Spatial and Temporal Antenna Array Kernel Processing

.
Questions and Problems

7
Dual Signal Models for Signal Processing

.
Introduction

.
Dual Signal Model Elements

.
Dual Signal Model Instantiations

..
Dual Signal Model for Nonuniform Signal Interpolation

..
Dual Signal Model for Sparse Signal Deconvolution

..
Spectrally Adapted Mercer Kernels

.
Tutorials and Application Examples

..
Nonuniform Interpolation with the Dual Signal Model

..
Sparse Deconvolution with the Dual Signal Model

..
Doppler Ultrasound Processing for Fault Detection

..
Spectrally Adapted Mercer Kernels

..
Interpolation of Heart Rate Variability Signals

..
Denoising in Cardiac Motion-Mode Doppler Ultrasound Images

..
Indoor Location from Mobile Devices Measurements

..
Electroanatomical Maps in Cardiac Navigation Systems

.
Questions and Problems

8
Advances in Kernel Regression and Function Approximation

.
Introduction

.
Kernel-Based Regression Methods

..
Advances in Support Vector Regression

..
Multi-output Support Vector Regression


Contents
ix
..
Kernel Ridge Regression

..
Kernel Signal-to-Noise Regression

..
Semi-supervised Support Vector Regression

..
Model Selection in Kernel Regression Methods

.
Bayesian Nonparametric Kernel Regression Models

..
Gaussian Process Regression

..
Relevance Vector Machines

.
Tutorials and Application Examples

..
Comparing Support Vector Regression, Relevance Vector Machines, and
Gaussian Process Regression

..
Proﬁle-Dependent Support Vector Regression

..
Multi-output Support Vector Regression

..
Kernel Signal-to-Noise Ratio Regression

..
Semi-supervised Support Vector Regression

..
Bayesian Nonparametric Model

..
Gaussian Process Regression

..
Relevance Vector Machines

.
Concluding Remarks

.
Questions and Problems

9
Adaptive Kernel Learning for Signal Processing

.
Introduction

.
Linear Adaptive Filtering

..
Least Mean Squares Algorithm

..
Recursive Least-Squares Algorithm

.
Kernel Adaptive Filtering

.
Kernel Least Mean Squares

..
Derivation of Kernel Least Mean Squares

..
Implementation Challenges and Dual Formulation

..
Example on Prediction of the Mackey–Glass Time Series

..
Practical Kernel Least Mean Squares Algorithms

.
Kernel Recursive Least Squares

..
Kernel Ridge Regression

..
Derivation of Kernel Recursive Least Squares

..
Prediction of the Mackey–Glass Time Series with Kernel Recursive Least
Squares

..
Beyond the Stationary Model

..
Example on Nonlinear Channel Identiﬁcation and Reconvergence

.
Explicit Recursivity for Adaptive Kernel Models

..
Recursivity in Hilbert Spaces

..
Recursive Filters in Reproducing Kernel Hilbert Spaces

.
Online Sparsiﬁcation with Kernels

..
Sparsity by Construction

..
Sparsity by Pruning

.
Probabilistic Approaches to Kernel Adaptive Filtering

..
Gaussian Processes and Kernel Ridge Regression

..
Online Recursive Solution for Gaussian Processes Regression


x
Contents
..
Kernel Recursive Least Squares Tracker

..
Probabilistic Kernel Least Mean Squares

.
Further Reading

..
Selection of Kernel Parameters

..
Multi-Kernel Adaptive Filtering

..
Recursive Filtering in Kernel Hilbert Spaces

.
Tutorials and Application Examples

..
Kernel Adaptive Filtering Toolbox

..
Prediction of a Respiratory Motion Time Series

..
Online Regression on the KINK Dataset

..
The Mackey–Glass Time Series

..
Explicit Recursivity on Reproducing Kernel in Hilbert Space
and Electroencephalogram Prediction

..
Adaptive Antenna Array Processing

.
Questions and Problems

Part III
Classification, Detection, and Feature Extraction

10
Support Vector Machine and Kernel Classification Algorithms

.
Introduction

.
Support Vector Machine and Kernel Classiﬁers

..
Support Vector Machines

..
Multiclass and Multilabel Support Vector Machines

..
Least-Squares Support Vector Machine

..
Kernel Fisher’s Discriminant Analysis

.
Advances in Kernel-Based Classiﬁcation

..
Large Margin Filtering

..
Semi-supervised Learning

..
Multiple Kernel Learning

..
Structured-Output Learning

..
Active Learning

.
Large-Scale Support Vector Machines

..
Large-Scale Support Vector Machine Implementations

..
Random Fourier Features

..
Parallel Support Vector Machine

..
Outlook

.
Tutorials and Application Examples

..
Examples of Support Vector Machine Classiﬁcation

..
Example of Least-Squares Support Vector Machine

..
Kernel-Filtering Support Vector Machine for Brain–Computer Interface
Signal Classiﬁcation

..
Example of Laplacian Support Vector Machine

..
Example of Graph-Based Label Propagation

..
Examples of Multiple Kernel Learning

.
Concluding Remarks

.
Questions and Problems


Contents
xi
11
Clustering and Anomaly Detection with Kernels

.
Introduction

.
Kernel Clustering

..
Kernelization of the Metric

..
Clustering in Feature Spaces

.
Domain Description Via Support Vectors

..
Support Vector Domain Description

..
One-Class Support Vector Machine

..
Relationship Between Support Vector Domain Description
and Density Estimation

..
Semi-supervised One-Class Classiﬁcation

.
Kernel Matched Subspace Detectors

..
Kernel Orthogonal Subspace Projection

..
Kernel Spectral Angle Mapper

.
Kernel Anomaly Change Detection

..
Linear Anomaly Change Detection Algorithms

..
Kernel Anomaly Change Detection Algorithms

.
Hypothesis Testing with Kernels

..
Distribution Embeddings

..
Maximum Mean Discrepancy

..
One-Class Support Measure Machine

.
Tutorials and Application Examples

..
Example on Kernelization of the Metric

..
Example on Kernel k-Means

..
Domain Description Examples

..
Kernel Spectral Angle Mapper and Kernel Orthogonal
Subspace Projection Examples

..
Example of Kernel Anomaly Change Detection Algorithms

..
Example on Distribution Embeddings and Maximum Mean Discrepancy

.
Concluding Remarks

.
Questions and Problems

12
Kernel Feature Extraction in Signal Processing

.
Introduction

.
Multivariate Analysis in Reproducing Kernel Hilbert Spaces

..
Problem Statement and Notation

..
Linear Multivariate Analysis

..
Kernel Multivariate Analysis

..
Multivariate Analysis Experiments

.
Feature Extraction with Kernel Dependence Estimates

..
Feature Extraction Using Hilbert–Schmidt Independence Criterion

..
Blind Source Separation Using Kernels

.
Extensions for Large-Scale and Semi-supervised Problems

..
Eﬃciency with the Incomplete Cholesky Decomposition

..
Eﬃciency with Random Fourier Features

..
Sparse Kernel Feature Extraction

..
Semi-supervised Kernel Feature Extraction


xii
Contents
.
Domain Adaptation with Kernels

..
Kernel Mean Matching

..
Transfer Component Analysis

..
Kernel Manifold Alignment

..
Relations between Domain Adaptation Methods

..
Experimental Comparison between Domain Adaptation Methods

.
Concluding Remarks

.
Questions and Problems

References

Index


xiii
About the Authors
José Luis Rojo-Álvarez received the Telecommunication Engineering degree in 
from University of Vigo, Spain, and a PhD in Telecommunication Engineering in 
from the Polytechnic University of Madrid, Spain. Since , he has been a full
Professor in the Department of Signal Theory and Communications, University Rey
Juan Carlos, Madrid, Spain. He has published more than papers in indexed journals
and more than international conference communications. He has participated in
more than projects (with public and private fundings), and directed more than 
of them, including several actions in the National Plan for Research and Fundamental
Science. He was a senior researcher at the Prometeo program in Ecuador (Army
University, to ) and research advisor at the Telecommunication Ministry. In
he received the Rey Juan Carlos University Prize for Talented Researcher.
His main current research interests include statistical learning theory, digital signal
processing, and complex system modeling, with applications to cardiac signals and
image processing. Speciﬁcally, he is committed to the development of new electrocar-
diographic imaging systems, long-term cardiac monitoring intelligent systems, and big
data for electronic recording and hospital information analysis at large scales.
He joined Persei vivarium, an eHealth company, as Chief Scientiﬁc Oﬃcer in
. Currently, he is running a pioneer degree program on Biomedical Engineering,
involving hospitals and companies in the electro-medicine and eHealth ﬁelds. In ,
he also joined the Center for Computational Simulation (Universidad Politécnica
de Madrid) for promoting eHealth technology transfer based on multivariate data
processing.
Manel Martínez-Ramón received an MsD in Telecommunications Engineering from
Universitat Politècnica de Catalunya in , and a PhD in Communications Technolo-
gies from Universidad Carlos III de Madrid (Spain) in . In he spent a -
month postdoctoral period at the MIND Research Network (New Mexico, USA). He
was an Associate Professor at Universidad Carlos III de Madrid until . There, he
held various positions from Associate Dean of the School of Engineering to Associate
Vice-Chancellor for Infrastructures. He has taught more than diﬀerent undergradu-
ate and graduate classes in diﬀerent universities.
Since August he has been a full professor with the Department of Electrical and
Computer Engineering at the University of New Mexico, where he was permanently
appointed Prince of Asturias Endowed Chair of the University of New Mexico, later
renamed to King Felipe VI Endowed Chair, which is sponsored by the Household of

xiv
About the Authors
the King of Spain. He is head of the machine learning track of this department and
he is the Associate Director of the Center of Emerging Energy Technologies of this
university. He is currently a principal investigator of several projects funded by the
National Science Foundation and other agencies.
He has co-authored more than journal papers and about conference papers, and
several books and book chapters. His research interests are in applications of machine
learning to cyberphysical systems, including ﬁrst-responders systems, smart grids, and
cognitive radio.
Jordi Muñoz-Marí was born in València, Spain, in , and received a BSc degree
in Physics (), a BSc degree in Electronics Engineering (), and a PhD degree
in Electronics Engineering () from the Universitat de València. He is currently
an associate professor in the Electronics Engineering Department at the Universitat
de València, where he teaches electronic circuits, digital signal processing, and data
science. He is a research member of the Image and Signal Processing (ISP) group. His
research activity is tied to the study and development of machine-learning algorithms
for signal and image processing.
Gustau Camps-Valls received BSc degrees in Physics () and in Electronics
Engineering () and a PhD degree in Physics (), all from the Universitat
de València. He is currently an Associate Professor (hab. Full Professor) in the
Department of Electronics Engineering. He is a research coordinator in the Image
and Signal Processing (ISP) group. He is interested in the development of machine-
learning algorithms for geoscience and remote-sensing data analysis. He is an author
of journal papers, more than conference papers, international book
chapters, and editor of the books Kernel Methods in Bioengineering, Signal and Image
Processing (IGI, ), Kernel Methods for Remote Sensing Data Analysis" (John
Wiley & Sons, ), and Remote Sensing Image Processing (MC, ). He holds
a Hirsch’s index h = , entered the ISI list of Highly Cited Researchers in ,
and Thomson Reuters ScienceWatch identiﬁed one of his papers on kernel-based
analysis of hyperspectral images as a Fast Moving Front research. In , he obtained
the prestigious European Research Council (ERC) consolidator grant on Statistical
Learning for Earth Observation Data Analysis. Since he has been a member of the
Data Fusion Technical Committee of the IEEE GRSS, and since of the Machine
Learning for Signal Processing Technical Committee of the IEEE SPS. He is a member
of the MTG-IRS Science Team (MIST) of EUMETSAT. He is Associate Editor of the
IEEE Transactions on Signal Processing, IEEE Signal Processing Letters, IEEE Geoscience
and Remote Sensing Letters, and invited guest editor for IEEE Journal of Selected Topics
in Signal Processing () and IEEE Geoscience and Remote Sensing Magazine ().
Valero Laparra Pérez-Muelas received a BSc degree in Telecommunications Engi-
neering (), a BSc degree in Electronics Engineering (), a BSc degree in
Mathematics (), and a PhD degree in Computer Science and Mathematics ().
Currently, he has a postdoctoral position in the Image Processing Laboratory (IPL) and

About the Authors
xv
an Assistant Professor position in the Department of Electronics Engineering at the
Universitat de València.
Luca Martino obtained his PhD in Statistical Signal Processing from Universidad
Carlos III de Madrid, Spain, in . He has been an Assistant Professor in the
Department of Signal Theory and Communications at Universidad Carlos III de
Madrid since then. In August he joined the Department of Mathematics and
Statistics at the University of Helsinki. In March , he joined the Universidade
de Sao Paulo (USP). Currently, he is a postdoctoral researcher at the Universitat de
València. His research interests include Bayesian inference, Monte Carlo methods, and
nonparametric regression techniques.
Sergio Muñoz-Romero earned his PhD in Machine Learning at Universidad Carlos
III de Madrid, where he also received the Telecommunication Engineering degree. He
has led pioneering projects where machine-learning knowledge was successfully used
to solve real big-data problems. Currently, he is a researcher at Universidad Rey Juan
Carlos. Since , he has worked at Persei vivarium as Head of Data Science and Big
Data. His present research interests are centered around machine-learning algorithms
and statistical learning theory, mainly in dimensionality reduction and feature selection
methods, and their applications to bioengineering and big data.
Adrián Pérez-Suay obtained his BSc degree in Mathematics (), a Master’s degree
in Advanced Computing and Intelligent Systems (), and a PhD degree in Com-
putational Mathematics and Computer Science () about distance metric learning,
all from the Universitat de València. He is currently a postdoctoral researcher at the
Image Processing Laboratory (IPL) working on feature extraction and classiﬁcation
problems in remote-sensing data analysis, and has worked as assistant professor in the
Department of Mathematics at the Universitat de València.
Margarita Sanromán-Junquera received the Technical Telecommunication Engineer-
ing degree from Universidad Carlos III de Madrid, Spain, in , the Telecommunica-
tion Engineering degree from Universidad Rey Juan Carlos, Spain, in , an MSc in
Biomedical Engineering and Telemedicine from the Universidad Politécnica de Madrid,
Spain, in , and a PhD in Multimedia and Communication from Universidad Rey
Juan Carlos and Universidad Carlos III de Madrid, in . She is currently an Assistant
Professor in the Department of Signal Theory and Communications, Telematics, and
Computing at Universidad Rey Juan Carlos. Her research interests include statistical
learning theory, digital processing of images and signals, and their applications to
bioengineering.
Cristina Soguero-Ruiz received the Telecommunication Engineering degree and a
BSc degree in Business Administration and Management in , and an MSc degree in
Biomedical Engineering from the University Rey Juan Carlos, Madrid, Spain, in .
She obtained her PhD degree in Machine Learning with Applications in Healthcare in
in the Joint Doctoral Program in Multimedia and Communications in conjunction
with University Rey Juan Carlos and University Carlos III. She was supported by FPU
Spanish Research and Teaching Fellowship (granted in , third place in TEC area).

xvi
About the Authors
She won the Orange Foundation Best PhD Thesis Award by the Spanish Oﬃcial College
of Telecommunication Engineering.
Steven Van-Vaerenbergh received his MSc degree in Electrical Engineering from
Ghent University, Belgium, in , and a PhD degree from the University of Cantabria,
Santander, Spain, in . He was a visiting researcher with the Computational Neu-
roengineering Laboratory, University of Florida, Gainesville, in . Currently, he
is a postdoctoral associate with the Department of Telecommunication Engineering,
University of Cantabria, Spain, where he is the principal researcher for a project on
pattern recognition in time series. His current research interests include machine
learning, Bayesian statistics, and signal processing.

xvii
Preface
Why Did We Write This Book?
In we were ﬁnishing or had just ﬁnished our PhD theses in electronics and
signal processing departments in Spain. Each of us worked with complicated and
diverse datasets, ranging from the analysis of signals from patients in cooperation with
hospitals, to satellite data imagery and antenna signals. All of us had grown in an
academic environment where neural networks were at the core of machine learning,
and our theses also dealt with them. However, support vector machines (SVMs) had just
arrived, and we enthusiastically adopted them. We were probably the Spanish pioneers
using them for signal processing. It took a bit to understand the fundamentals, but
then everything became crystal clear. It was a clean notation, a neat methodology,
often involved straightforward implementations, and admitted many alternatives and
modiﬁcations. After understanding the SVM classiﬁcation and regression algorithms
(the two ﬁrst ones that the kernel community delivered), we saw the enormous potential
for writing other problems than maximum margin classiﬁers, and to accommodate the
particularities of signal and image features and models.
First, we started to write down some support vector algorithms for problems using
standard signal models, the ones that we liked most, such as spectral analysis, decon-
volution, system identiﬁcation, or signal interpolation. Some concepts from both the
signal and the kernel worlds seemed to be naturally connected, including the concept
of signal autocorrelation, being closely related to the solid theory of reproducing kernel
functions. Then, we started to send our brand-new algorithms to good machine-
learning journals. Quite often, reviewers criticized that the approaches were trivial,
and suggested resubmission to a signal-processing journal. And then, signal-processing
reviewers apparently found no novelty whatsoever in redeﬁning old concepts in kernel
terms. It seemed that the clarity of the kernel methods methodology was playing
against us, and everything was apparently obvious. Nevertheless, we were (and still
are) convinced that signal processing is much more than just ﬁltering signals, and that
kernel methods are much more than maximum margin classiﬁers as well. Our vision
was that kernel methods should respect signal features and signal models as the only
way to ensure model–data integration.
For years we worked in growing and designing kernel algorithms guided by the
robustness requirements for the systems in our application ﬁelds. We studied other

xviii
Preface
works around these ﬁelds, and some of them were really inspiring and useful in our
signal-processing problems. We even wrote some tutorials and reviews along these
lines, aiming to put together the common elements of the kernel methods design under
signal-processing perspectives. However, we were not satisﬁed with the theoretical
tutorials, because our algorithms were designed according to our applications, and
the richness of the landscape given by the data was not reﬂected in these theoretical
tutorials, or even not fully conveyed by more application-oriented surveys. This is why
we decided to write a book that integrated the theoretical fundamentals, put together
representative application examples, and, if possible, to include code snippets and links
to relevant, useful toolboxes and packages. We felt that this could be a good way to help
the reader work on theoretical fundamentals, while being inspired by real problems.
This is, in some sense, the book we would have liked in the s for ourselves. This
book is not intended to be a set of tutorials, nor a set of application papers, and not just a
bunch of toolboxes. Rather, the book is intended to be a learning tour for those who like
and need to write their kernel algorithms, who need these algorithms for their signal-
processing applications in real data, and who can be inspired by simple yet illustrative
code tutorials.
Needless to say, completing a book like this in the intersection of signal processing and
kernel methods has been an enormous challenge. The literature of kernel methods in
signal processing is vast, so we could not include all the excellent contributions working
in this cross-ﬁeld during recent years. We tried our best in all chapters, though, by
revising the literature of what we feel are the main pillars and recent trends. The book
only reﬂects our personal view and experience, though.
Structure and Contents
This book is divided into three parts: one for fundamentals, one focused on signal
models for signal estimation and adaptive ﬁltering, and another for classiﬁcation,
detection, and feature extraction. They are summarized next.
Part One: Fundamentals and Basic Elements
This consists of an introductory set of chapters and presents the necessary set of basic
ideas from both digital signal processing (DSP) and reproducing kernel Hilbert spaces
(RKHSs). After an introductory road map (Chapter ), a basic overview of the ﬁeld of
DSP is presented (Chapter ). Then, data models for signal processing are presented
(Chapter ), on the one hand including well-known signal models (such as sinusoid-
based spectral estimation, system identiﬁcation, or deconvolution), and on the other
hand summarizing a set of additional fundamental concepts (such as adaptive ﬁltering,
noise, or complex signal models). Chapter consists of an introduction to kernel
functions and Hilbert spaces, including the necessary concepts on RKHS and their
properties for being used throughout the rest of the book. This chapter includes the
elements of the SVM algorithm for regression, as an instantiation of a kernel algorithm,
which in turn will be formally used as a founding optimization algorithm for the
algorithms in the next parts.

Preface
xix
Part Two: Function Approximation and Adaptive Filtering
This presents a set of diﬀerent SVM algorithms organized from the point of view of
the signal model being used from Chapter and its role in the structured function
estimation. The key in this part is that SVM for estimation problems (not to be con-
founded with the standard SVM for classiﬁcation) raises a well-structured and founded
approach to develop new general-purpose signal processing methods. Chapter starts
with a simple and structured explanation of the three diﬀerent sets of algorithms to
be addressed, which are primal signal models (linear kernel and signal model stated in
the primal problem, in the remainder of Chapter ), RKHS signal models (signal model
in the RKHS, conventional in kernel literature, to which Chapter is devoted), and
dual signal models (signal model in the dual solution, closely related to function basis
expansion in signal processing, to which Chapter is devoted). These three chapters
represent the main axis along which the kernel trick is used to adapt the richness of
signal processing model data, with emphasis on the idea that, far from using black-
box nonlinear regression/classiﬁcation model with an ad-hoc signal embedding model,
one can actually adapt the SVM equations to the signal model from digital signal
processing considerations on the structure of our data. Ending this part, Chapter 
provides an overview on the wide variety of kernel methods for signal estimation
which can beneﬁt from these proposed concepts for SVM regression, which include
such widespread techniques as least-squares SVM, kernel signal-to-noise regression,
or Bayesian approaches. A signal processing text for kernel methods in DSP must cover
the adaptive processing ﬁeld, which after some initial basic proposals, seems to be
reaching today a state of maturity in terms of theoretical fundamentals; all of them are
summarized in Chapter .
Part Three: Classification, Detection, and Feature Extraction
This presents a compendium of selected SVM algorithms for DSP which are not
included in the preceding framework. Starting from the state of the art in SVM
algorithms for classiﬁcation and detection problems in the context of signal processing,
the rationale for this set of existing contributions is quite diﬀerent from Part Two, given
that likely the most fundamental concept of SVM classiﬁers, namely the maximum
margin, holds in SVM classiﬁcation approaches for signal processing. Chapter revises
the conventional SVM classiﬁer and its variants, introduces other kernel classiﬁers
beyond SVMs, and discusses particular advanced formulations to treat with semi-
supervised, active, structured-output, and large-scale learning. Then, Chapter is
devoted speciﬁcally to clustering, anomaly detection, and one-class kernel classiﬁers,
with emphasis in signal- and image-processing applications. Finally, Chapter is fully
devoted to the rich literature and theoretical developments on kernel feature extrac-
tion, revisiting the classical taxonomy (unsupervised, supervised, semi-supervised, and
domain adaptation) from a DSP point of view.
Theory, Applications, and Examples
References on theoretical works and additional applications are stated and brieﬂy
commented in each chapter. The book can be considered as self-contained, but still it

xx
Preface
assumes some necessary previous and very basic knowledge on DSP. In the application
examples, references are given for the interested reader to be able to update or refresh
those concepts which are to be dealt with in each chapter.
Supporting material is also included in two forms. On the one hand, simple examples
for fundamental concepts are delivered, so that the reader gains conﬁdence and gets
familiar with the basic concepts, some readers may ﬁnd them trivial for some chapters,
on the other hand, real and more advanced application examples are provided in several
chapters. Scripts, code, and pointers to toolboxes are mostly in MATLAB™. The source
code and examples can be downloaded from GitHub at the following link:
http://github.com/DSPKM/
In this dedicated repository, many links are maintained to other widely used software
toolboxes for machine learning and signal processing in kernel methods. This repository
is periodically updated with the latest contributions, and can be helpful for the data
analysis practitioner. The reader can use the code provided with this book for their own
research and analysis. We only ask that, in this case, the book is properly cited:
Digital Signal Processing with Kernel Methods
José Luis Rojo-Álvarez, Manel Martínez-Ramón, Jordi Muñoz-Marí, and
Gustau Camps-Valls
John Wiley & Sons, .
When the code for the examples is simple and didactic, it is included in the text, so
that it can be examined, copied, and pasted. Those scripts and functions with increased
complexity are further delivered in the book repository. Owing to the multidisciplinary
nature of the examples, they can be of diﬀerent diﬃculty for each reader according to
the speciﬁc background, so that examples which seem extremely easy for some will be
harder to work out for others. The reader is encouraged to spend some time with the
code with which they are more unfamiliar and to skip the examples which are already
well known.

xxi
Acknowledgements
We would like to acknowledge the help of all involved in the collation and review process
of the book, without whose support the project could not have been satisfactorily
completed. A further special note of thanks goes also to all the staﬀat John Wiley & Sons,
Inc., whose contributions throughout the whole process, from inception of the initial
idea to ﬁnal publication, have been valuable. Special thanks also go to the publishing
team at John Wiley & Sons, Ltd., who continuously prodded via e-mail, keeping the
project on schedule. It has been really a pleasure to work with such a professional staﬀ.
Our special thanks goes to the coauthors of several of the chapters, who made it
possible to cover material that has largely enriched those chapters, and who are listed
in Table . We would like to express our deepest gratitude to them.
We also wish to thank all of the coauthors of the papers during these years. Without
them this work would not have been possible at all: J.C. Antoranz, J. Arenas-García,
A. Artés-Rodríguez, T. Bandos, O. Barquero-Pérez, J. Bermejo, K. Borgwardt, L.
Bruzzone, A.J. Caamaño-Fernández, S. Canú, C. Christodoulou, J. Cid-Sueiro, P. Conde-
Pardo, E. Everss, J.R. Feijoo, M.J. Fernández-Getino, A.R. Figueiras-Vidal, C. Figuera,
R. Flamary, A. García-Alberola, A. García-Armada, V. Gil-Jiménez, F.J. Gimeno-Blanes,
L. Gómez-Chova, V. Gómez-Verdejo, A.E. Gonnouni, R. Goya-Esteban, A. Guerrero-
Curieses, E. Izquierdo-Verdiguier, L.K. Hansen, R. Jenssen, M. Lázaro-Gredilla,
N. Longbotham, J. Malo, J.D. Martín-Guerrro, M.P. Martínez-Ruiz, F. Melgani, I. Mora-
Jiménez, N.M. Nasrabadi, A. Navia-Vázquez, A.A. Nielsen, F. Pérez-Cruz, K.B. Petersen,
M. de Prado-Cumplido, A. Rakotomamonjy, I. Santamaría-Caballero, B. Schölkopf,
E. Soria-Olivas, D. Tuia, J. Verrelst, J. Weston, R. Yotti, and D. Zhou.
This book was produced without any dedicated funding, but our research was partially
supported by research projects that made it possible. We want to thank all agencies
and organizations for supporting our research in general, and this book indirectly.
José Luis Rojo-Álvarez acknowledges support from the Spanish Ministry of Economy
and Competitiveness, under projects PRINCIPIAS (TEC--C--R), FINALE
(TEC--C--R), and KERMES (TEC--REDT), and from Comu-
nidad de Madrid under project PRICAM (S/ICE-). Manel Martínez-Ramón
acknowledges support from the Spanish Ministry of Economy and Competitiveness
under project TEC--R, from Comunidad de Madrid under project PRICAM
(S/ICE-), and from the National Science Foundation under projects S&CC
#and SBIR Phase II #. Jordi Muñoz-Marí acknowledges support from
the Spanish Ministry of Economy and Competitiveness and the European Regional
Development Fund, under project TIN--R, Gustau Camps-Valls and Luca

xxii
Acknowledgements
Table 1 List of coauthors in specific chapters.
Margarita Sanromán-Junquera
Universidad Rey Juan Carlos
Chapters and 
Sergio Muñoz Romero
Universidad Rey Juan Carlos
Chapters , , and 
Cristina Soguero-Ruiz
Universidad Rey Juan Carlos
Chapters and 
Luca Martino
Universitat de València
Chapter 
Steven Van Vaerenbergh
Universidad de Cantabria
Chapter 
Adrián Pérez-Suay
Universitat de València
Chapter 
Valero Laparra
Universitat de València
Chapter 
Martino acknowledge support by the European Research Council (ERC) under the
ERC-CoG-project . Steven Van Vaerenbergh is supported by the Spanish
Ministry of Economy and Competitiveness, under PRISMA project (TEC--
JIN).
José Luis Rojo-Álvarez, Manel Martínez-Ramón,
Jordi Muñoz-Marí, and Gustau Camps-Valls
Leganés, Albuquerque, and València, December 

xxiii
List of Abbreviations
ACD
anomalous change detection
AL
active learning
ALD
approximate linear dependency
AP
access point
AR
autoregressive
ARCH
autoregressive conditional heteroscedasticity
ARMA
autoregressive and moving average
ARX
autoregressive exogenous
AUC
area under the (ROC) curve
AVIRIS
Airborne Visible Infrared Imaging Spectrometer
BCI
brain–computer interface
BER
bit error rate
BG
Bernouilli–Gaussian
BRT
bootstrap resampling techniques
BSS
blind source separation
BT
breaking ties
CCA
canonical correlation analysis
CDMM
color Doppler M mode
CESNI
continuous-time equivalent system for nonuniform interpolation
CG
conjugate gradient
CI
conﬁdence interval
CNS
cardiac navigation system
COCO
constrained covariance
CS
compressive sensing
CV
cross-validation
DCT
discrete cosine transform
DFT
discrete Fourier transform
DMGF
double modulated Gaussian function
DOA
direction of arrival
DSM
dual signal model
DSP
digital signal processing
DWT
discrete wavelet transform
EAM
electroanatomical map
EC
elliptically contoured

xxiv
List of Abbreviations
ECG
electrocardiogram
EEC
error correction code
EEG
electroencephalogram
EM
expectation–maximization
ESD
energy spectral density
FB-KRLS ﬁxed budget kernel recursive least squares
FFT
fast Fourier transform
FIR
ﬁnite impulse response
FT
Fourier transform
GM
Gaussian mixture
GMM
Gaussian mixture model
GP
Gaussian process
GPR
Gaussian process regression
GRNN
generalized regression neural network
HRCN
high reliability communications network
HRV
heart rate variability
HSCA
Hilbert–Schmidt component analysis
HSIC
Hilbert–Schmidt independence criterion
i.i.d.
independent and identically distributed
ICA
independent component analysis
ICF
incomplete Cholesky factorization
IIR
inﬁnite impulse response
IMSE
integrated mean square error
IPM
interior point method
IRWLS
integrated reweighted least squares
KACD
kernel anomaly change detection
KAF
Kalman adaptive ﬁltering
KDE
kernel density estimation
KDR
kernel dimensionality reduction
KECA
kernel entropy component analysis
KEMA
kernel manifold alignment
KF
Kalman ﬁlter
KFD
kernel Fisher discriminant
KGV
kernel generalized variance
KICA
kernel independent component analysis
KKT
Karush–Kuhn–Tucker
KL
Kullback–Leibler
KLMS
kernel least mean squares
kMI
kernel mutual information
KMM
kernel mean matching
KNLMS
kernel normalized least mean squares
KOSP
kernel orthogonal subspace projection
KPCA
kernel principal component analysis
KRLS
kernel recursive least squares
KRLS-T
kernel recursive least square tracker
KRR
kernel ridge regression
KSAM
kernel spectral angle mapper

List of Abbreviations
xxv
KSNR
kernel signal-to-noise regression/ratio
KTA
kernel–target alignment
LapSVM
Laplacian support vector machine
LASSO
least absolute shrinkage and selection operator
LDA
linear discriminant analysis
LFD
linear Fisher discriminant
LI
linear interpolation
LMF
large margin ﬁltering
LMS
least mean squares
LOO
leave-one-out
LS
least squares
LS-SVM
least-squares support vector machine
LTI
linear time invariant
LUT
look-up table
MA
moving average
MAE
mean absolute error
MAO
most ambiguous and orthogonal
MAP
maximum a posteriori
MCLU
multiclass level uncertainty
MCMC
Markov chain–Monte Carlo
MERIS
medium resolution imaging spectrometer
MIMO
multiple input–multiple output
MKL
multiple kernel learning
ML
maximum likelihood
MLP
multilayer perceptron
MMD
maximum mean discrepancy
MMDE
maximum mean discrepancy embedding
MMSE
minimum mean square error
MNF
minimum noise fraction
MPDR
minimum power distortionless response
MRI
magnetic resonance imaging
MS
margin sampling
MSE
mean square error
MSSF
modulated squared sinc function
MSVR
multioutput support vector regression
MUSIC
multiple signal classiﬁcation
MVA
multivariate analysis
MVDR
minimum variance distortionless response
NN
neural network
NORMA
naive online regularized risk minimization algorithm
NW
Nadayara–Watson
OA
overall accuracy
OAA
one against all
OAO
one against one
OC-SVM
one class support vector machine
OFDM
orthogonal frequency division multiplexing
OKECA
optimized kernel entropy component analysis

xxvi
List of Abbreviations
OSP
orthogonal subspace projection
PCA
principal component analysis
PCK
probabilistic cluster kernel
pdf
probability density function
PLS
partial least squares
PSD
power spectral density
PSM
primal signal model
PSVM
parallel support vector machine
QAM
quadrature amplitude modulation
QKLMS
quantiﬁed kernel least mean squares
QP
quadratic programming
QPSK
quadrature-phase shift keying
RBF
radial basis function
RHSIC
randomized Hilbert–Schmidt independence criterion
RKHS
reproducing kernel in Hilbert space
RKS
random kitchen sink
RLS
recursive least squares
RMSE
root mean square error
ROC
receiver operating characteristic
RSM
reproducing kernel in Hilbert space signal model
RSS
received signal strength
RV
relevance vector
RVM
relevance vector machine
S/E
signal to error
SAM
spectral angle mapper
SDP
semi-deﬁnite program
SE
squared exponential
SMO
sequential minimal optimization
SNR
signal-to-noise ratio
SOGP
sparse online Gaussian process
SOM
self-organizing map
SR
semiparametric regression
SRM
structural risk minimization
SSL
semisupervised learning
SSMA
semisupervised manifold alignment
STFT
short-time Fourier transform
SVC
support vector classiﬁcation
SVD
singular value decomposition
SVDD
support vector domain description
SVM
support vector machine
SVR
support vector regression
SW-KRLS sliding window kernel recursive least squares
TCA
transfer component analysis
TFD
time–frequency distribution
TSVM
transductive support vector machine
WGP
warped Gaussian process
WGPR
warped Gaussian process regression

1
Part I
Fundamentals and Basic Elements

3
1
From Signal Processing to Machine Learning
Signal processing is a ﬁeld at the intersection of systems engineering, electrical
engineering, and applied mathematics. The ﬁeld analyzes both analog and digitized
signals that represent physical quantities. Signals include sound, electromagnetic
radiation, images and videos, electrical signals acquired by a diversity of sensors, or
waveforms generated by biological, control, or telecommunication systems, just to
name a few. It is, nevertheless, the subject of this book to focus on digital signal
processing (DSP), which deals with the analysis of digitized and discrete sampled
signals. The word “digital” derives from the Latin word digitus for “ﬁnger,” hence
indicating everything ultimately related to a representation as integer countable
numbers. DSP technologies are today pervasive in many ﬁelds of science and
engineering, including communications, control, computing and economics, biology,
or instrumentation. After all, signals are everywhere and can be processed in many
ways: ﬁltering, coding, estimation, detection, recognition, synthesis, or transmission,
are some of the main tasks in DSP.
In the following sections we review the main landmarks of signal processing in
the th century from the perspective of algorithmic developments. We will also pay
attention to the cross-fertilization with the ﬁeld of statistical (machine) learning in the
last decades. In the st century, model and data assumptions as well as algorithmic
constraints are no longer valid, and the ﬁeld of machine-learning signal processing has
erupted, with many successful stories to tell.
1.1
A New Science is Born: Signal Processing
1.1.1
Signal Processing Before Being Coined
One might argue that processing signals is as old as human perception of the nature, and
you would probably be right. Processing signals is actually a fundamental problem in
science. In ancient Egypt, the Greek civilization and the Roman Empire, the “men who
knew” (nowadays called scientists and philosophers) measured and quantiﬁed river
ﬂoods, sunny days, and exchange rates digitally. They also tried to predict them and
model them empirically with simple “algorithms.” One might say that system modeling,
causal inference, and world phenomena prediction were matters that already existed at
that time, yet were treated at a philosophical level only. Both the mathematical tools
and the intense data exploitation came later. The principles of what we actually call
Digital Signal Processing with Kernel Methods, First Edition. José Luis Rojo-Álvarez, Manel Martínez-Ramón,
Jordi Muñoz-Marí, and Gustau Camps-Valls.
© John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.

4
Digital Signal Processing with Kernel Methods
signal processing date back to the advances in classical numerical analysis techniques
of the th and th centuries. Big names of European scientists, like Newton, Euler,
Kirchhoﬀ, Gauss, Cauchy, and Fourier, set up the basis for the latter development of
sciences and engineering, and DSP is just the most obvious and particular case out of
them. The roots of DSP can be found later in the digital control systems of the s
and s, while their noticeable development and adoption by society took place later,
in the s and s.
1.1.2
1948: Birth of the Information Age
The year may be regarded as the birth of modern signal processing. Shannon
published his famous paper “A mathematical theory of communication” that estab-
lished bounds for the capacity of a band-limited channel and created the discipline
of information theory (Shannon, ). Hartley and Wiener fed Shannon’s mind with
their statistical viewpoint of communication; and others, like Gabor, developed the ﬁeld
enormously. In that year, Shannon also motivated the use of pulse code modulation in
another paper. This year was also the year when modern digital methods were intro-
duced: Bartlett and Tukey developed methods for spectrum estimation, while Hamming
introduced error correcting codes for eﬃcient signal transmission and recovery. These
advances were most of the time motivated by particular applications: audio engineering
promoted spectral estimation for signal analysis, and radar/sonar technologies dealt
with discrete data during World War II that needed to be analyzed in the spectral
domain. Another landmark in was the invention of the transistor at Bell Labs,
which was still limited for commercial applications. The take-oﬀof the signal processing
ﬁeld took place also because Shannon, Bode, and others discussed the possibility of
using digital circuits to implement ﬁlters, but no appropriate hardware was available at
that time.
1.1.3
1950s: Audio Engineering Catalyzes Signal Processing
DSP as we know it nowadays was still not possible at that time. Mathematical tools (e.g.,
the z-transform) were already available thanks to established disciplines like control
theory, but technology was ready only to deal with low-frequency signal processing
problems. Surprisingly, the ﬁeld of audio engineering (boosted by the fever of rock ’n’
roll in radio stations!) was the catalyst for new technological developments: automobile
phonographs, radio transistors, magnetic recording, high-quality low-distortion loud-
speakers and microphone design were important achievements.
The other important industry was telephony and the need for eﬃcient repeaters
(ampliﬁers, transistors): the transatlantic phone cable formed a huge low-pass ﬁlter
introducing delays and intersymbol interferences in the communications, and time-
assignment speech interpolators appeared as eﬃcient techniques to exploit the pauses
in speech during a phone conversation. Eﬃciency in communications took advantage of
Shannon’s theory of channel capacity, and introduced frequency-division multiplexing.
Transmission capacity was alternatively improved by the invention of the coaxial cable
and the vocoder in the s by Dudley.
This decade is also memorable because of work on the theory of wave ﬁlters,
mostly developed by Wagner, Campbell, Cauer, and Darlington. A new audio signal
representation called sound spectrograms, which essentially shows the frequency
content of speech as it varies through time, was introduced by Potter, Wigner, Ville,

From Signal Processing to Machine Learning
5
and other researchers. This time–frequency signal representation became widely used
in signal processing some time later. Communications during World War II were
quite noisy; hence, there was a notable eﬀort in constructing a mathematical theory
of signal and noise, notably by Wiener and Rice. The ﬁeld of seismic data processing
witnessed an important development in the early s, when Robinson showed how
to derive the desired reﬂection signals from seismic data carrying out one-dimensional
deconvolution.
1.2
From Analog to Digital Signal Processing
1.2.1
1960s: Digital Signal Processing Begins
In the late s, the introduction of the integrated circuit containing transistors
revolutionized electrical engineering technology. The s made technology ready for
DSP. Silicon integrated circuits were ready, but still quite expensive compared with their
analogical counterparts. The most remarkable contributions were the implementation
of a digital ﬁlter using the bilinear transform by Kaiser, and the work of Cooley and
Tukey in to compute the discrete Fourier transform eﬃciently, which is nowadays
well known as the fast Fourier transform (FFT). DSP also witnessed the introduction
of the Viterbi algorithm in (used especially in speech recognition), the chirp
z-transform algorithm in (which widened the application range for the FFT), the
maximum likelihood (ML) principle also in (for sensor-array signal processing),
and adaptive delta modulation in (for speech encoding).
New and cheaper hardware made digital ﬁlters a reality. It was possible to eﬃciently
implement long ﬁnite-impulse response (FIR) ﬁlters that were able to compete with
analog inﬁnite-impulse response (IIR) ﬁlters, oﬀering better band-pass properties.
But perhaps more crucial was that the s were a time for numerical simulation.
For instance, Tukey developed the concept of the cepstrum (the Fourier transform of the
logarithm of the amplitude spectrum) for pitch extraction in a vocoder. In early ,
Kaiser and Golden worked intensively to transfer ﬁlters from the analog to the digital
domain. Digital ﬁlters also oﬀered the possibility to synthesize time-varying, adaptive
and nonlinear ﬁlters, something that was not possible with analog ﬁlters. Kalman ﬁlters
(KFs) took advantage of the statistical properties of the signals for ﬁltering, while
Widrow invented the least mean squares (LMS) algorithm for adaptive ﬁltering, which
is the basis for neural networks (NNs) training. Bell Labs also developed adaptive
equalizers and echo cancellers. Schroeder introduced the adaptive predictive coding
algorithm for speech transmission of fair quality, while Atal invented linear predictive
coding, which was so useful for speech compression, recognition, and synthesis.
In the s, image processing stepped in the ﬁeld of DSP through applications
in space sciences. The topics of image coding, transmission, and reconstruction were
in their infancy. In , Anderson and Huang developed eﬃcient coders and later
introduced the famous discrete cosine transform (DCT) for image coding. Other two-
dimensional (D) and three-dimensional (D) signals entered the arena: computer-
ized tomography scanning, interferometry for high-resolution astronomy and geodesy,
and radar imaging contributed with improvements in multidimensional digital ﬁlters
for image restoration, compression, and analysis. The wide range of applications,
usually involving multidimensional and nonstationary data, exploited and adapted

6
Digital Signal Processing with Kernel Methods
previous DSP techniques: Wiener ﬁltering for radar/sonar tracking, Kalman ﬁltering
for control and signal-detection systems, and recursive time-variant implementation of
closed-form ﬁlter solutions, just to name a few.
1.2.2
1970s: Digital Signal Processing Becomes Popular
The s was the time when video games and word processors appeared. DSP started
to be everywhere. The speech processing community introduced adaptive diﬀerential
pulse code modulation to achieve moderate savings in coding. Subband coding divided
the signal spectrum into bands and adaptively quantized each independently. The
technique was also used for image compression, with important implications in storage
and processing.
Filter design continued introducing improvements, such as McClellan’s and Parks’s
design of equiripple FIR ﬁlters, the analog-to-digital design procedures introduced by
Burrus and Parks, Galand’s quadrature mirror ﬁlters, and Darlington’s multirate ﬁlters.
Huang pioneered in developing ﬁlters for image processing. State-space methods and
related mathematical techniques were developed, which were later introduced into
ﬁelds such as ﬁlter design, array processing, image processing, and adaptive ﬁltering.
FFT theory was extended to ﬁnite ﬁelds and used in areas such as coding theory.
1.2.3
1980s: Silicon Meets Digital Signal Processing
The s will be remembered as the decade in which personal computers (PCs) became
ubiquitous. IBM introduced its PC in and standardized the disk operating system.
PC clones appeared soon after, together with the ﬁrst Apple computers and the new
IBM PC-XT, the ﬁrst PC equipped with a hard-disk drive.
The most important fact in the s for the signal processing point of view was
the design and production of single-chip DSP. Compared with using general-purpose
processors, speciﬁcally designed chips for signal processing made operations much
faster, allowing parallel and real-time signal-processing systems. An important DSP
achievement of the s was JPEG, which essentially relies on the DCT, and still is
the international standard for still pictures compression. The success of JPEG inspired
eﬀorts to reach standards for moving images, which was achieved in the s in
the form of MPEGand MPEG. Automated image-recognition found its way into
both military and civil applications, as well as for Earth observation and monitoring.
Nowadays, these applications involve petabyte and multimillion ventures.
The introduction of NNs played a decisive role in many applications. In the s,
Frank Rosenblatt introduced the perceptron (a simple linear classiﬁer), and Bernard
Widrow the adaline (adaptive linear ﬁlter). Nevertheless, neural nets were not exten-
sively used until the s: new and more eﬃcient architectures and training algo-
rithms, capability to implement networks in very-large-scale integrated circuits, and
the belief that massive parallelism was needed for speech and image recognition.
Besides the famous multilayer perceptron (MLP), other important developments are
Interestingly, this is in current times a revived idea because of the concepts of “big data” and “deep
learning.”

From Signal Processing to Machine Learning
7
worth mentioning: Hopﬁeld’s recurrent networks, radial basis function (RBF) networks,
and Jordan’s and Elman’s dynamic and recurrent networks. NNs were implemented for
automatic speech recognition, automatic target detection, biomedical engineering, and
robotics.
1.3
Digital Signal Processing Meets Machine Learning
1.3.1
1990s: New Application Areas
The s changed the rules with the eﬀect of the Internet and PCs. More data to be
transmitted, analyzed, and understood were present in our lives. On top of this, the
continuing growth of the consumer electronics market impulsed DSP. New standards
like MPEGand MPEGmade eﬃcient coding of audio signals widely used. Actually,
the new era is “visual”: image processing and digital photography became even more
prominent branches of signal processing. New techniques for ﬁltering and image
enhancement and sharpening found application in many (in principle orthogonal) ﬁelds
of science, like astronomy, ecology, or meteorology. As in the previous decade, space
science introduced a challenging problem: the availability of multi- and hyperspectral
images impulsed new algorithms for image compression, restoration, fusion, and object
recognition with unprecedented accuracy and wide applicability.
New applications were now possible because of the new computer platforms and
interfaces, the possibility to eﬃciently simulate systems, and the well-established math-
ematical and physical theories of the previous decades and centuries. People started to
adopt DSP unconsciously when using voice-recognition software packages, accessing
the Internet securely, compressing family photographs in JPEG, or trading in the stock
markets using moving-average ﬁlters.
1.3.2
1990s: Neural Networks, Fuzzy Logic, and Genetic Optimization
NNs were originally developed for aircraft and automobile-engine control. They were
also used in image restoration and, given its parallel nature, eﬃciently implemented
on very-large-scale integrated architectures. Also exciting was the development and
application of fractals, chaos, and wavelets. Fractal coding was extensively applied in
image compression. Chaotic systems have been used to analyze and model complex
systems in astronomy, biology, chemistry, and other sciences. Wavelets are a mathemat-
ical decomposition technique that can be cast as an extension of Fourier analysis, and
intimately related to Gabor ﬁlters and time–frequency representations. Wavelets were
considerably advanced in the mid-s and extensively developed by Daubechies and
Mallat in the s.
Other new departures for signal processing in the s were related to fuzzy logic
and genetic optimization. The so-called fuzzy algorithms use fuzzy logic, primarily
developed by Zadeh, and they gave rise to a vast number of real-life applications.
Genetic algorithms, based on laws of genetics and natural selection, also emerged.
Since then they have been applied in diﬀerent signal-processing areas. These are mere
examples of the tight relation between DSP and computer science.

8
Digital Signal Processing with Kernel Methods
1.4
Recent Machine Learning in Digital Signal Processing
We are facing a totally new era in DSP. In this new scenario, the particular characteristics
of signals and data are challenging the traditional signal-processing technologies. Signal
and data streams are now massive, unreliable, unstructured, and barely ﬁt standard
statistical assumptions about the underlying system. The recent advances in interdisci-
plinary research are of paramount importance to develop new technologies able to deal
with the new scenario. Powerful approaches have been designed for advanced signal
processing, which can be implemented thanks to continuous advances in fast comput-
ing (which is becoming increasingly inexpensive) and algorithmic developments.
1.4.1
Traditional Signal Assumptions Are No Longer Valid
Standard signal-processing models have traditionally relied on the rather simplifying
and strong assumptions of linearity, Gaussianity, stationarity, circularity, causality, and
uniform sampling. These models provide mathematical tractability and simple and
fast algorithms, but they also constrain their performance and applicability. Current
approaches try to get rid of these approximations in a number of ways: by widely using
models that are intrinsically nonlinear and nonparametric; by encoding the relations
between the signal and noise (which are often modeled and no longer considered Gaus-
sian independent and identically distributed (i.i.d.) noise); by using new approaches to
treat the noncircularity and nonstationarity properties of signals; by learning in anti-
causal systems, which is an important topic of control theory; and, in some situations,
since the acquired signals and data streams are fundamentally unstructured, by not
assuming uniform sampling of the representation domain.
It is also important to take into account the increasing diversity of data. For example,
large and unstructured text and multimedia datasets stored in the Internet and the
increasing use of social network media produce masses of unstructured heterogeneous
data streams. Techniques for document classiﬁcation, part-of-speech tagging, multime-
dia tagging or classiﬁcation, together with massive data-processing techniques (known
as “big data” techniques) relying on machine-learning theory try to get rid of unjustiﬁed
assumptions about the data-generation mechanisms.
1.4.2
Encoding Prior Knowledge
Methods and algorithms are designed to be speciﬁc to the target application, and
most of the times they incorporate accurate prior and physical knowledge about the
processes generating the data. The issue is two-sided. Nowadays, there is a strong need
of constraining model capacity with proper priors. Inclusion of prior knowledge in the
machines for signal processing is strongly related to the issue of encoding invariances,
and this often requires the design of speciﬁc regularizers that constrain the space of
possible solutions to be conﬁned in a plausible space. Experts in the application ﬁeld
provide such knowledge (e.g., in the form of physical, biological, or psychophysical
models), while engineers and computer scientists design the algorithm in order to fulﬁll
the speciﬁcations. For instance, current advances in graphical models allow us to learn
more about structure from data (i.e., the dynamics and relationships of each variable
and their interactions), and multitask learning permits the design of models that tackle
the problem of learning a task as a composition of modular subtask problems.

From Signal Processing to Machine Learning
9
1.4.3
Learning and Knowledge from Data
Machine learning is a powerful framework to reach the goal of processing a signal or
data, turning it into information, and then trying to extract knowledge out of either new
data or the learning machine itself. Understanding is much more important and diﬃcult
than ﬁtting, and in machine learning we aim for this from empirical data. A naive exam-
ple can be found in biological signal processing, where a learning machine can be trained
from patients and control records, such as electrocardiograms (ECGs) and magnetic
resonance imaging (MRI) spatio-temporal signals. The huge amount of data coming
from the medical scanner needs to be processed to get rid of those features that are not
likely to contain information. Knowledge is certainly acquired from the detection of a
condition in a new patient, but there is also potentially important clinical knowledge in
the analysis of the learning-machine parameters in order to unveil which characteristics
or factors are actually relevant in order to detect a disease. For this fundamental goal,
learning with hierarchical deep NNs has permitted increasingly complex and more
abstract data representations. Similarly, cognitive information processing has allowed
moving from low-level feature analysis to higher order data understanding. Finally, the
ﬁeld of causal inference and learning has irrupted with new refreshing algorithms to
learn causal relations between variables.
The new era of DSP has an important constraint: the urgent need to deal with
massive data streams. From images and videos to speech and text, new methods
need to be designed. Halevy et al. () raised the debate about how it becomes
increasingly evident that machine learning achieves the most competitive results when
confronted with massive datasets. Learning semantic representations of the data in such
environments becomes a blessing rather than a curse. However, in order to deal with
huge datasets, eﬃcient automatic machines must be devised. Learning from massive
data also poses strong concerns, as the space is never ﬁlled in and distributions reveal
skewed and heavy tails.
1.4.4
From Machine Learning to Digital Signal Processing
Machine learning is a branch of computer science and artiﬁcial intelligence that enables
computers to learn from data. Machine learning is intended to capture the necessary
patterns in the observed data, such as accurately predicting the future or estimating
hidden variables of new, previously unseen data. This property is known in general as
generalization. Machine learning adequately ﬁts the constraints and solution require-
ments posed by DSP problems: from computational eﬃciency, online adaptation, and
learning with limited supervision, to their ability to combine heterogeneous informa-
tion, to incorporate prior knowledge about the problem, or to interact with the user
to achieve improved performance. Machine learning has been recognized as a very
suitable technology in signal processing since the introduction of NNs. Since the s,
this particular model has been successfully exploited in many DSP applications, such as
antennas, radar, sonar and speech processing, system identiﬁcation and control, and
time-series prediction (Camps-Valls and Bruzzone, ; Christodoulou and Geor-
giopoulos, ; Deng and Li, ; Vepa, ; Zhao and Principe, ).
The ﬁeld of DSP revitalized in the s with the advent of support vector machines
(SVMs) in particular and of kernel methods in general (Schölkopf and Smola, ;
Shawe-Taylor and Cristianini, ; Vapnik, ). The framework of kernel machines

10
Digital Signal Processing with Kernel Methods
allowed the robust formulation of nonlinear versions of linear algorithms in a very
simple way, such as the classical LMS (Liu et al., ) or recursive least squares (Engel
et al., ) algorithms for adaptive ﬁltering, Fisher’s discriminants for signal classiﬁ-
cation and recognition, and kernel-based autoregressive and moving average (ARMA)
models for system identiﬁcation and time series prediction (Martínez-Ramón et al.,
). In the last decade, the ﬁelds of graphical models (Koller and Friedman, ),
kernel methods (Shawe-Taylor and Cristianini, ), and Bayesian nonparametric
inference (Lid Hjort et al., ) have played an important role in modern signal pro-
cessing. Not only have many signal-processing problems been tackled from a canonical
machine-learning perspective, but the opposite direction has also been fruitful.
1.4.5
From Digital Signal Processing to Machine Learning
Machine learning is in constant cross-fertilization with signal processing, and thus the
converse situation has also been satisﬁed in the last decade. New machine-learning
developments have relied on achievements from the signal-processing community.
Advances in signal processing and information processing have given rise to new
machine-learning frameworks:
Sparsity-aware learning. This ﬁeld of signal processing takes advantage of the property
of sparseness or compressibility observed in many natural signals. This allows
one to determine the entire signal from relatively few scarce measurements.
Interestingly, this topic originated from the image and signal processing ﬁelds, and
rapidly extended to other problems, such as mobile communications and seismic
signal forecasting. The ﬁeld of sparse-aware models has recently inﬂuenced other
ﬁelds in machine learning, such as target detection in strong noise regimes, image
coding and restoration, and optimization, to name just a few.
Information-theoretic learning. The ﬁeld exploits fundamental concepts from informa-
tion theory (e.g., entropy and divergences) estimated directly from the data to
substitute the conventional statistical descriptors of variance and covariance. The
ﬁeld has encountered many applications in the adaptation of linear or nonlinear
ﬁlters and also in unsupervised and supervised machine-learning applications. In
the most recent years the framework has been interestingly related to the ﬁeld
of dependence estimation with kernels, and shown successful performance in
kernel-based adaptive ﬁlter and feature extraction.
Adaptive ﬁltering. The urgent need for nonlinear adaptive algorithms in particu-
lar communications applications and web recommendation tools to stream
databases has boosted the interest in a number of areas, including sequential
and active learning. In this ﬁeld, the introduction of online kernel adaptive
ﬁlters is remarkable. Sequential and adaptive online learning algorithms are a
fundamental tool in signal processing, and intelligent learning systems, mainly
since they entail constraints such as accuracy, algorithmic simplicity, robustness,
low latency, and fast implementation. In addition, by deﬁning an instantaneous
information measure on observations, kernel adaptive ﬁlters are able to actively
select training data in online learning scenarios. This active learning mechanism
provides a principled framework for knowledge discovery, redundancy removal,
and anomaly detection.

From Signal Processing to Machine Learning
11
Machine-learning methods in general and kernel methods in particular provide an
excellent framework to deal with the jungle of algorithms and applications. Kernel
methods are not only attractive for many of the traditional DSP applications, such as
pattern recognition, speech, audio, and video processing. Nowadays, as will be treated
in this book, kernel methods are also one of the primary candidates for emerging
applications such as brain–computer interfacing, satellite image processing, modeling
markets, antenna and communication network design, multimodal data fusion and pro-
cessing, behavior and emotion recognition from speech and videos, control, forecasting,
spectrum analysis, and learning in complex environments such as social networks.

13
2
Introduction to Digital Signal Processing
Signal processing deals with the representation, transformation, and manipulation of
signals and the information they contain. Typical examples include extracting the pure
signals from a mixture observation (a ﬁeld commonly known as deconvolution) or
particular signal (frequency) components from noisy observations (generally known
as ﬁltering). Before the s, the technology only permitted processing signals
analogically and in continuous time. The rapid development of computers and digital
processors, plus important theoretical advances such as the FFT, caused an important
growth of DSP techniques. This chapter ﬁrst outlines the basics of signal processing
and then introduces the more advanced concepts of time–frequency and time–scale
representations, as well as emerging ﬁelds of compressed sensing and multidimensional
signal processing.
2.1
Outline of the Signal Processing Field
A crucial point of DSP is that signals are processed in samples, either in batch or online
modes. In DSP, signals are represented as sequences of ﬁnite precision numbers and
the processing is carried out using digital computing techniques. Classic problems in
DSP involve processing an input signal to obtain another signal. Another important
part consists on interpreting or extracting information from an input signal, where
one is interested in characterizing it. Systems of this class typically involve several
steps, starting from a digital procedure, like ﬁltering or spectral density estimation,
followed by a pattern recognition system producing a symbolic representation of the
data. This symbolic representation may be the input to an expert system based on
rules that gives the ﬁnal interpretation of the original input signal. In machine learning
we know this as a classiﬁer or automatic detection algorithm. Classic problems also
deal with processing of symbolic expressions. In these cases, signals and systems
are represented as abstract objects and analyzed using basic theoretical tools such
as convolution, ﬁlter structures, FFT, decimation, or interpolation. Classic problems
found in the literature – and being present in most DSP courses (Oppenheim and
Schafer, ; Proakis and Manolakis, ) – include (but are not limited to): () ﬁlter
design using FIR, IIR, and adaptive ﬁlters; () autoregressive (AR) and moving-average
(MA) time-invariant systems; () spectral analysis, mainly using the discrete Fourier
Digital Signal Processing with Kernel Methods, First Edition. José Luis Rojo-Álvarez, Manel Martínez-Ramón,
Jordi Muñoz-Marí, and Gustau Camps-Valls.
© John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.

14
Digital Signal Processing with Kernel Methods
transform (DFT); () time–frequency analysis, using the spectrogram, time–frequency
distributions (TFDs), and wavelets; () decimation and interpolation techniques; and
() deconvolution-based signal models.
2.1.1
Fundamentals on Signals and Systems
Basic Signals in Continuous and Discrete Time
In a ﬁrst approach, a signal is generally (and very roughly) deﬁned as a physical mag-
nitude that changes with time, and it is traditional to distinguish between continuous-
time signals (e.g., denoted x(t)) and discrete-time signals (e.g., denoted x[n]). Several
speciﬁc signals are specially relevant because they can be used as building blocks of a
vast majority of observations, in the sense that they form the basis of a signal space. In
this setting, unit impulse functions such as the Dirac delta 𝛿(t) and the Kronecker delta
𝛿[n] are the most widely spread ones, and they are closely related to unit steps, either
in continuous u(t) or discrete cases u[n], by means of the integral or the cumulative
sum, respectively, of their unit impulse counterparts. Any signal fulﬁlling some integral
convergence requirements can be expressed in terms of impulse signals:
x(t) = ∫
+∞
−∞
x(𝜏)𝛿(t −𝜏) d𝜏
(.)
x[n] =
+∞
∑
k=−∞
x[k]𝛿[n −k]
(.)
for continuous- and discrete-time signals respectively. Note that this is essentially a
time-domain representation, as far as impulse signals are perfectly well located in time.
Another fundamental basis of signal spaces, which are intrinsic to the time domain, is
given by “sinc” functions:
sinc(t) = sin(πt)
πt
(.)
and
sinc[n] =
⎧
⎪
⎨
⎪⎩
sin(Nn∕)
sin(Nn∕)
n ≠πk
(−)k(N−)
n = πk
(.)
These functions have to be understood in terms of their properties as ideal ﬁlters in the
frequency domain. Finally, rectangular windows p(t) and p[n] are also useful signals for
dealing with time-local properties of the signals.
Linear and Time-Invariant Systems
Systems are those physical or mathematical entities that transform signals into other
signals, which in general we will denote as y(t) = T{x(t)} and y[n] = T{x[n]} when they
act on continuous or discrete data streams. Mathematical properties of the systems are
intimately related to their physical behavior. For instance, an output signal at a given

Introduction to Digital Signal Processing
15
time instant only depends on the input at that time instant in systems with memory.
Also, an output signal at a time instant does not depend on the future of input or
output signals in causal systems. Stable systems always yield a bounded output signal
as their response to a bounded input signal, and linear systems fulﬁll additivity and
scaling properties between input and output signals. Finally, time-invariant systems
yield the same time-shifted response for any shifted input signal. For those systems
which simultaneously fulﬁll time invariance and linearity, their response to the unit
impulse (often denoted as h(t), h[n]) can be readily obtained in the time domain by using
Equation ., which yields the continuous- and discrete-time convolution operators,
respectively given by
y(t) = x(t) ∗h(t) = ∫
+∞
−∞
x(𝜏)h(t −𝜏) d𝜏
(.)
y[n] = x[n] ∗h[n] =
+∞
∑
k=−∞
x[k]h[n −k],
(.)
where symbol ∗is a common compact notation of the convolution operation, referred
to continuous-time and discrete-time according to the context. Convolution stands for
a shared method for analytical representation and practical working with linear and
time-invariant (LTI) systems. A vast number of physical systems admit this fundamental
representation, which makes LTI systems theory a fundamental framework for signal
analysis.
Complex Signals and Complex Algebra
Complex representation of magnitudes in signal processing is often necessary depend-
ing on the nature of the signals to be processed. The most representative examples
can be found in communications, where complex notation provides a natural expres-
sion of signals that make their manipulation easier and compact. The justiﬁcation of
the use of complex algebra arises from the fact that any band-pass signal centered
at any given frequency admits a representation that can be decomposed into two
orthogonal components. They are called in-phase and quadrature-phase components.
The in-phase component is Hermitic around the central frequency, while the quadra-
ture phase is anti-Hermitic. The combination of both components reconstructs any
band-pass signal.
Usually, communication signals are treated at the receiver in their complex envelope
form; that is, the central frequency is removed because it does not convey any informa-
tion, thus leading to a signal which is complex in nature. While both components can be
processed independently, usually, channel distortion produces in-phase and quadrature
components that are linear combinations of the original ones. Then, the reception of
the signal must include an equalization procedure; that is, a recovery of the original
components from the received ones. Equalization is most easily formulated by the use
of complex algebra.
Signal detection in communications or radio detection and ranging (radar for short)
necessarily includes observation noise. Among all noise sources, thermal noise is the
most prevalent one. Usually, thermal noise is assumed to be Gaussian and circu-
larly symmetric in the complex plane. A complex signal z(t) is said to have circular

16
Digital Signal Processing with Kernel Methods
symmetry if e𝕚𝜃z(t) has the same probability distribution as z(t) for any value of t.
In this case the least squares (LS) criterion is optimal. Actually, LS-criterion-based
algorithms are straightforwardly formulated in complex algebra, and the noise is fully
characterized by its variance in each complex component. In some situations, however,
non-Gaussian noise components may appear, thus making LS suboptimal. In these
situations, other criteria than the ones based on maximum margin must be formulated,
where the use of complex numbers is still possible. Moreover, noise can still be Gaussian
but noncircularly symmetric; thus, in general, three parameters are needed for its
characterization, which are the variances of each component and their covariance. In
order to properly apply an optimization criterion, Wirtinger algebra has been applied
in Bouboulis and coworkers (Bouboulis and Theodoridis, , ; Bouboulis et al.,
) and Ogunfunmi and Paul ().
Frequency-Domain Representations
Unit impulses, pulses, and sinc functions have a clear determination in time properties.
However, the velocity of the variation of a signal cannot be easily established in these
basic signals. The simplest signals for which their change velocity can be determined are
sinusoids, x(t) = A sin(𝜔t + 𝜙) and x[n] = A sin[𝜔n + 𝜙], in terms of their angular
frequency. Also, complex exponentials form a basis for the signal space, x(t) = A est,
with A,s ∈ℂ, or x[n] = Azn, with z ∈ℂ, and the subset of purely imaginary exponentials,
x(t) = e𝕚wt and x[n] = e𝕚𝜔n are straightforwardly related to sinusoids and makes
operative their mathematical handling.
These signals, having their rate of change properties well determined, open the ﬁeld
to an alternative basis of signal spaces. For periodic signals, fulﬁlling x(t) = x(t + T)
or x[n] = x[n + N], Fourier showed that they can be expressed in terms of imaginary
exponentials:
x(t) =
+∞
∑
k=−∞
ak e𝕚k(π∕T)t
(.)
x[n] =
N−
∑
k=
ak e𝕚k(π∕N)n
(.)
ak = ∫
+∞
−∞
x(t) e−𝕚k(π∕T)t dt
(.)
ak =
N−
∑
k=
x[n] e−𝕚k(π∕N)n,
(.)
where exponential coeﬃcients stand for the relative importance of each harmonic
frequency in the periodic signal under analysis, and can be readily obtained by cross-
comparison between the signal and each harmonic component. Therefore, the so-called
Fourier series for periodic signals represents a set of comparisons between the periodic
signal and the sinusoid harmonics, which gives a precise description of the variation
rates present in a signal.
This precise description is also desirable for nonperiodic signals. This is given by the
continuous- and discrete-time Fourier transforms (FTs), which are generalized from
Equations .and .by limit concepts as follows:

Introduction to Digital Signal Processing
17
X(𝕚𝜔) = {x(t)} = ∫
+∞
−∞
x(t) e−𝕚𝜔t dt
(.)
X(𝕚Ω) = {x[n]} =
∞
∑
n=−∞
x[n] e−𝕚Ωn
(.)
x(t) = 
π ∫
+∞
−∞
X(𝕚𝜔) e𝕚𝜔t dw
(.)
x[n] = 
π ∫
+π
−π
X(jΩ) e𝕚Ω n dΩ,
(.)
where {x(t)} and {x[n]} denote the FT of a signal in the continuous-time and in
the discrete-time domain, depending on the context. Among many implications for LTI
system theory, the previous equations mean that the convolution operator in the time
domain can be handled as a product-template in the frequency domain aﬀecting the
modulus and phase of the input signal, Y(𝕚𝜔) = H(𝕚𝜔)X(𝕚𝜔) and Y(𝕚Ω) = H(𝕚Ω)X(𝕚Ω).
An additional extremely relevant consequence of using the frequency domain is the
analytical availability of the modulation property, as far as signal product in the time
domain is equivalent to the convolution operation in the frequency domain. For
instance, given the signal c(t) = x(t) e𝕚𝜔t, its frequency expression C(𝕚w) is a function of
the frequency transform of the original signal x(t) simply noted as C(𝕚𝜔) = X(𝕚𝜔−𝕚𝜔).
Sampling
A discrete-time signal can be obtained from a continuous-time signal, x[n] = x(nts),
which is known as a sampling process, and the most basic scheme is given by
xs(t) = x(t)p𝛿(t) = x(t)
∞
∑
k=−∞
𝛿(t −kts) =
∞
∑
k=−∞
x(kts)𝛿(t −kts),
(.)
where ts denotes the sampling period. Hence, processing of a continuous-time signal
can be made in the discrete-time domain, which is closer to software implementations
and more prone to ﬂexibility. Recovering a continuous-time signal from a discrete-time
one is made through an ideal ﬁlter whose impulse response is the sinc signal with an
adequate bandwidth,
x(t) =
∞
∑
k=−∞
x(nts)sinc
(t −nts
ts
)
,
(.)
which shows that the signal can be expressed as an expansion of shifted and scaled sinc
functions. Recall that the spectrum of a discrete-time signal is periodic, and it can be
related to its continuous time counterpart by
X(𝕚Ω) =
∞
∑
k=−∞
X(𝕚𝜔−𝕚k𝜔s).
(.)
This makes a necessary assumption that no aliasing in the spectral replications in
the discrete-time spectrum can be allowed. This is known informally as the Nyquist

18
Digital Signal Processing with Kernel Methods
theorem. It was ﬁrst formulated by Harry Nyquist in , but proved by Shannon
(). It is stated as follows:
Theorem ..
Let x(t) be a continuous, square integrable signal whose FT X(𝜔) is
deﬁned in the interval [−W, W]. Then signal x(t) admits a representation in terms of its
sampled counterpart x(n∕fs) as
x(t) =
∑
n
x
(
n
fs
) sin
[
πfs
(
t −n
fs
)]
πfs
(
t −n
fs
)
=
∑
n
x
(
n
fs
)
sinc
[
fs
(
t −n
fs
)]
(.)
if and only if fs = ∕ts ≥W.
The theorem says that the continuous-time signal can be recovered as far as fs > W
(Nyquist frequency), where W is given by the largest frequency in the spectrum with
non-null Fourier component.
Energy-Defined and Power-Defined Signals
Signals are mathematically compared in terms of their properties. Among them, the
magnitude of a signal is a key intuitive concept to determine whether or not a signal
is larger than another. Two main kinds of signals can be described by the concepts of
power and energy of a signal, which are deﬁned as follows in the continuous and discrete
cases:
Px = lim
T→+∞

T ∫
T
−T
|x(t)|dt
(.)
Px = lim
N→+∞

N + 
N
∑
−N
|x[n]|
(.)
Ex = ∫
∞
−∞
|x(t)|dt
(.)
Ex =
∞
∑
−∞
|x[n]|.
(.)
These concepts have been inherited from the ﬁeld of linear circuit theory, but open
the ﬁeld to algebraic comparisons in terms of dot products and projections onto signal
Hilbert spaces. When convergence conditions are fulﬁlled, a signal is said to be energy
deﬁned (its energy is non-null and ﬁnite), or power deﬁned (its power is non-null).
In addition, the power or the energy of a given signal can be expressed with precision
in the transformed frequency domain, according to the so called energy spectral density
(ESD) and power spectral density (PSD), which are given by
Sx(𝜔) = |X(𝜔)|= |||x(t) e−𝕚𝜔t dt|||

(.)
Px(𝜔) = lim
T→∞E (|XT(𝜔)|) ,
(.)

Introduction to Digital Signal Processing
19
where
XT(𝜔) =

T ∫
T
−T
x(t) e−𝕚𝜔t dt
(.)
for continuous-time signals. ESD is more appropriate for transients (time-duration lim-
iting or evanescent signals), whereas PSD is more appropriate for stationary or periodic
signals. A fundamental theorem in signal processing actually relates the energy/power
preservation in both domains: Parseval’s theorem indicates that the energy (or the
power) of a signal can be interchangeably obtained from its time-domain expression
or from its spectral density; that is:
Ex = 
π ∫
∞
−∞
Sx(𝜔) d𝜔
(.)
Px = 
π ∫
∞
−∞
Px(𝜔) d𝜔.
(.)
The theorem is intuitively telling us that, after all, the representation domain is not
changing the intrinsic properties of the signal.
Autocorrelation and Spectrum
The autocorrelation concept is closely related to the PSD concept. If we deﬁne the
autocorrelation of a signal x(t) as
𝛾(𝜏) = x(t) ∗x(−t) = ∫
+∞
−∞
x(𝜏)x(t + 𝜏) d𝜏,
(.)
then the PSD of x(t) and its autocorrelation are a transform pair; that is,
Sxx(𝜔) = ∫∞
−∞𝛾(𝜏) e−𝕚𝜔𝜏d𝜏. Hence, the autocorrelation conveys relevant information
in the time and frequency of the signal.
Symmetric and Hermitian Signals
An additional set of properties of the signals is given by Equation symmetry. A real signal
x(t) is symmetric or even when x(−t) = x(t). The function is antisymmetric or odd if
x(−t) = −x(t). A complex function z(t) is said to be Hermitian if z(−t) = z∗(t), where
∗is the conjugate operator. If z(−v) = −z∗(t), the function is said to be anti-Hermitian.
Hermitian signals have important properties often exploited in signal processing.
Property 
The real part of a Hermitian signal is symmetric and its imaginary part is
antisymmetric. Let the complex signal z(v) = zR(v) + 𝕚zI(v), where zR(v) and zI(v) are its
real and imaginary parts, be a Hermitian signal. Then
z(−t) = zR(−t) + 𝕚zI(−t) = z∗(t) = zR(t) −𝕚zI(t)
(.)

20
Digital Signal Processing with Kernel Methods
Property 
If a signal x(t) is real, its FT X(𝕚𝜔) is Hermitic. Let x(t) be a real signal. Its
FT is deﬁned as (see Equation .)
X(𝕚𝜔) = ∫
+∞
−∞
x(t) e−𝕚𝜔t dt.
(.)
Since e−𝕚wt = cos(wt) −𝕚sin(wt), then
X(𝕚𝜔) = ∫
+∞
−∞
x(t) cos(𝜔t) −𝕚∫
+∞
−∞
x(t) sin(𝜔t) dt.
(.)
Changing the sign of the argument w and knowing that functions cos and sin are
respectively symmetric and antisymmetric gives
X(−𝕚𝜔) = ∫
+∞
−∞
x(t) cos(−𝜔t) −𝕚∫
+∞
−∞
x(t) sin(−𝜔t) dt
= ∫
+∞
−∞
x(t) cos(𝜔t) + 𝕚∫
+∞
∞
x(t) sin(−𝜔t) dt.
(.)
Clearly, the real part of X(𝕚𝜔) is symmetric and its imaginary part is antisymmetric.
Then, by Property , the function is Hermitian.
Property 
If a real signal x(t) is symmetric, its FT X(𝕚𝜔) is real and symmetric. If a
function is symmetric, then x(t) = x(−t); hence their FTs should be equal. The transform
of x(−t) is then equal to X(𝕚𝜔) and it can be expressed as
X(𝕚𝜔) = ∫
+∞
−∞
x(−t) cos(𝜔t) dt −𝕚∫
+∞
−∞
x(−t) sin(𝜔t) dt
= ∫
+∞
−∞
x(t) cos(−𝜔t) dt −𝕚∫
+∞
−∞
x(t) sin(−𝜔t) dt
= ∫
+∞
−∞
x(t) cos(𝜔t) dt + 𝕚∫
+∞
−∞
x(t) sin(𝜔t) dt
(.)
Similarly, X(𝕚𝜔) is deﬁned as X(𝕚𝜔) = ∫+∞
−∞x(t) cos(𝜔t) −𝕚∫+∞
−∞x(t) sin(𝜔t) dt; hence
𝕚∫
+∞
−∞
x(t) sin(𝜔t) dt = ,
(.)
which proves that the FT is real. It is straightforward to see that ∫+∞
−∞x(t) cos(𝜔t) dt is
symmetric when x(t) is symmetric.
Property can be proven by simply stating that if signal x(t) is symmetric, the product
x(t) sin(𝜔t) is antisymmetric, and then its integral between −∞and ∞is zero.

Introduction to Digital Signal Processing
21
2.1.2
Digital Filtering
In signal processing, a ﬁlter is a device that removes some unwanted components (or
features) from an observation (a signal coming out of a natural or physical system).
A broader and updated deﬁnition of a ﬁlter is “a system that operates on an input
signal to produce an output signal according to some computational algorithm.” The
ﬁeld of digital-ﬁlter theory builds upon extensive earlier work done in classic circuit
theory, numerical analysis, and more recently sampled data systems. Actually, digital
ﬁltering is a vast ﬁeld, also intimately related to engineering ﬁelds of system control
and identiﬁcation, statistical branches of signal interpolation and smoothing, as well
as the machine-learning paradigm of regression and function approximation. This
section reviews the fundamental basis of digital ﬁlter theory, and advanced concepts
and applications are treated in further sections.
Digital Filters as Input–Output Systems in the Time Domain
Digital systems were originally introduced as discrete-time systems because signals
were deﬁned as a function of time. A digital system can be cast as a transformation T
operating on an input signal x[n] to produce an output signal y[n] = T{x[n]}. There are
two main ﬁlters in DSP. The conventional MA ﬁlter, also known as a FIR ﬁlter, models
the output as a linear combination of the previous inputs to the system and is deﬁned
according to
y[n] =
Q
∑
l=
blx[n −l].
(.)
The AR ﬁlter, also known as an IIR ﬁlter, models the output as a linear combination of
previous outputs of the system and is deﬁned as
y[n] =
P
∑
m=
amy[n −m],
(.)
where ak and bk are the ﬁlter coeﬃcients, and P and Q are the order (or tap delay) of
the AR and MA ﬁlters respectively. The notation ARMA(P, Q) combines AR and MA
processes, and refers to a model with P autoregressive terms and Q + moving-average
terms. Note that we are restricted here to causal ﬁlters, meaning that future information
cannot be used for modeling (i.e., k < n), also known as nonanticipative ﬁlters. Diﬀerent
classes of input–output ﬁlters (or systems) are deﬁned by placing constraints on the
properties of transformation T(⋅). Filters are considered stable if, for a bounded input
signal, the ﬁlter returns a bounded output signal. Looking at the models, one may think
of ﬁlters implemented by either convolution (FIR) or by recursion (IIR).
Filtering in the Frequency Domain
Filtering in the time domain is convenient when the information is encoded in the shape
of the signal. Time-domain ﬁltering is widely used for signal smoothing, DC removal,
and waveform shaping, among many others. However, ﬁlters are very often designed
in the frequency domain to remove some frequencies while keeping others of interest.
Frequency-domain ﬁlters are used when the relevant information is contained not only

22
Digital Signal Processing with Kernel Methods
in the signal amplitude with respect to time but in the frequency and phase of the
signal components (sinusoids if the chosen basis is the Fourier basis). Designing ﬁlters
in the frequency domain requires deﬁning speciﬁcations in this domain (Proakis and
Manolakis, ).
The frequency response (transfer or system function) of a ﬁlter (system or device) can
be deﬁned as the quantitative measure of the output versus the input signal spectral
contents. It can also be cast as a measure of magnitude and phase of the output as
a function of frequency. The frequency response is characterized by the magnitude
of the system response, typically measured in decibels (dB) or as a decimal, and the
phase, measured in radians or degrees, versus frequency in radians per second or
hertz (Hz).One typically represents both the Bode magnitude and phase plots and
analyzes diﬀerent features, such as cut-oﬀfrequencies, band-pass maximum allowed
ripples, or band width. These are the characteristics that deﬁne the type of ﬁlter,
such as low-pass, high-pass, band-pass, and band-stop ﬁlters (Proakis and Manolakis,
).
The representation of discrete signals using the z-transform is very useful to work in
the frequency domain. Essentially, the z-transform of signal x[n] is deﬁned as
X(z) =
∞
∑
n=−∞
x[n]z−n,
(.)
where variable z is deﬁned in a region of convergence such that r< |z| < r. Note the
relation with the FT if z is expressed in the polar domain, z = r e𝕚𝜔, where r = |z| and
𝜔= ∠z. Actually, the FT is just the z-transform of the signal evaluated in the unit circle,
provided that it lies within the region of convergence. Equivalently, the z-transform of
the impulsive response h[n] characterizing a ﬁlter is
H(z) =
∞
∑
n=−∞
h[n]z−n.
(.)
The frequency response of a system is thus typically expressed as a function of (polyno-
mials in) z rather than the more natural variable Ω, which allows us to study the stability
of the system easily and to design ﬁlters accordingly.
Finally, we should note that, if the system under investigation is nonlinear,
applying purely linear frequency-domain analysis will not reveal all the system’s
characteristics. In such cases, and in order to overcome these limitations, generalized
frequency-response functions and nonlinear output frequency-response functions
have been deﬁned that allow the user to analyze complex nonlinear dynamic eﬀects
(Pavlov et al., ).
Filter Design and Implementation
The design of frequency-selective ﬁlters involves ﬁtting (selecting) the coeﬃcients of
a causal FIR or IIR ﬁlter that tightly approximates the desired frequency-response
A Bode plot shows the transfer function of an LTI system versus frequency, plotted with a log-frequency
axis.

Introduction to Digital Signal Processing
23
speciﬁcations. Which type of ﬁlter to use (namely, FIR or IIR) depends on the nature
of the problem and speciﬁcations. FIR ﬁlters exhibit a linear phase characteristic and
stability, but their design typically requires high ﬁlter orders (sometimes referred to
as “taps”), and thus their implementation is less simple than for an equivalent IIR
ﬁlter. Contrarily, IIR ﬁlters show a nonlinear phase characteristic and are potentially
unstable, but they can be implemented with lower ﬁlter orders, thus giving rise to
simpler circuits. In practice, however, using one or the other ultimately depends on
the speciﬁcities of the signal and on the ﬁlter phase characteristic. Speech signals, for
example, can be processed in systems with nonlinear phase characteristic. When it is
the frequency response that matters, it is better to use IIR digital ﬁlters, which have far
lower order (Rabiner et al., ).
The implementation of discrete-time systems such as FIR and IIR admits many
approaches. Among them, FIR ﬁlters are typically implemented with direct-form,
cascade-form, frequency-sampling, and lattice structures, while IIR systems adopt
parallel and lattice–ladder as well. In addition to the ﬁlter design methods based on the
analog-to-digital domain transformations, there are several methods directly deﬁned
in the discrete-time domain, such as the LS method, which is particularly useful for
designing IIR ﬁlters and FIR Wiener ﬁlters. The literature of digital ﬁlters design is vast
and overwhelming. As reference books, we refer the reader to the excellent old treaties
in Kaiser (), Golden and Kaiser (), Helms (), Constantinides (), and
Burrus and Parks ().
Besides selecting the structure and optimizing the weights, some other practical
problems arise with the hardware real implementation of ﬁlters. The main issues
concern the ﬁnite resolution for the numerical representation of the coeﬃcients, that
is, the quantization and round-oﬀeﬀects appearing in real implementations. These are
nonnegligible problems that need to be addressed and have been the subject of intense
research in the ﬁeld (Parker and Girard, ).
Linear Prediction and Optimal Linear Filters
Designing ﬁlters for signal estimation is a recurrent problem in communications and
control systems, among many other disciplines. Filter design has been approached
from a statistical viewpoint as well. The most common approach consists of assum-
ing a linear model and the minimization of the mean-square error, thus focusing
only on second-order statistics of the stationary process (autocorrelation and cross-
correlation). The approach leads to solving a set of linear equations, typically with the
(computationally eﬃcient) Levinson–Durbin and the Schür algorithms (Proakis and
Manolakis, ).
The ﬁeld of optimal linear ﬁlter estimation is very large, and many algorithms have
been (and constantly are being) presented. The pioneering work was developed by
Wiener in for ﬁltering statistically stationary signals (Wiener, ). The Wiener
ﬁlter estimates a desired (or target) random process by LTI ﬁltering of an observed
noisy process, assuming known stationary signal and noise spectra, and additive noise.
The Wiener ﬁlter essentially minimizes the mean square error between the estimated
random process and the desired process, and hence is known as a minimum mean
square error (MMSE) estimator. The Wiener ﬁlter has had a crucial impact on signal
and image-processing applications (Wiener, ).

24
Digital Signal Processing with Kernel Methods
Adaptive Filtering
Adaptive ﬁltering (Haykin, ) is a central topic in signal processing. An adaptive ﬁlter
is a ﬁlter structure (e.g., FIR, IIR) provided with an adaptive algorithm that tunes the
transfer function typically driven by an error signal. Adaptive ﬁlters are widely applied in
nonstationary environments because they can adapt their transfer function to match the
changing parameters of the system generating the incoming data (Hayes ; Widrow
et al. ). Adaptive ﬁlters have become ubiquitous in current DSP, mainly due to the
increase in computational power and the need to process data streams. Adaptive ﬁlters
are now routinely used in all communication applications for channel equalization,
array beamforming or echo cancellation, and in other areas of signal processing such
as image processing or medical equipment.
2.1.3
Spectral Analysis
The concept of frequency is omnipresent in any signal-processing application. Spectral
representations give fundamental information valuable in many signal processing tasks
in the experimental plane as well as in applications. From a practical point of view,
spectral analysis is used in a large number of applications, the most obvious one being
spectral visualization for research and development. But also, spectral analysis is used
in voice recognition and synthesis, wireless digital telephony and data communications,
medical imaging, or antenna array processing, just to mention some of the most
common applications.
The ﬁeld of spectral analysis is wide, and thus a great many techniques exist depending
on the particular application at hand. These techniques can be decomposed into two
main families, all of them under the umbrella of Fourier analysis. The ﬁrst one is
the set of classical techniques called nonparametric spectral analysis. These embrace
a very wide set of diﬀerent technologies, and are the most widely used nowadays.
The second set of analysis methods are parametric spectral analysis, and the family
can be subdivided into time-series analysis methods (using AR models) and subspace
methods. When these methods ﬁrst appeared in the literature, they were barely used
due to the hardware limitations of the digital signal processors at those times. One of
the most important novelties of parametric methods was their frequency resolution,
which dramatically over-passed the nonparametric ones (Kay and Marple, ). Owing
to algorithmic improvements and computational power of digital signal processors,
their use has exponentially increased, and parametric methods have been incorporated
in many real-life applications, ranging from voice processing to radar applications, to
mention just a couple.
From a theoretical viewpoint, a spectral representation can be understood as the
similarity of an arbitrary stationary signal to each one of a set of orthogonal base
signals. Usually, complex exponentials are used since they are eigenfunctions of linear
systems. In these cases, the Fourier analysis is used as a fundamental analysis tool,
but, nevertheless, other basis functions can be used to decompose a signal into
components (e.g., Hadamard, Walsh). For a discrete signal x[n] which is absolutely
summable – that is, ∑∞
−∞|x[n]|< ∞– its DFT at a frequency Ω can be conceived
as the dot product (measure of similarity) between the signal and a complex sinusoid

Introduction to Digital Signal Processing
25
{e𝕚Ωn}∞
n=−∞at this frequency; that is, X(𝕚Ω) = {x[n]}. We can write the spectrum of a
signal as
Sx(𝕚Ω) = X(𝕚Ω)X∗(𝕚Ω).
(.)
It is obvious that in practice the number of observed samples available for the
computation is limited. This estimated spectrum ̂S(e𝕚Ω) can be modeled as an
expectation of the product in time of the time-unlimited signal with a time-limited
window w[n] of N samples:
̂Sx(𝕚Ω) = E
(N−
∑
n=
N−
∑
m=
e−𝕚Ωnx[n]x∗[m] e𝕚Ωm
)
.
(.)
Now, deﬁning Rxx as the autocorrelation matrix of the signal, and the complex sinusoid
vector v(Ω) = [, e𝕚Ω, … , e𝕚Ω(N−)]T, we obtain an expression for the spectrum estimation
with the form
̂S(𝕚Ω) = vH(Ω)Rxxv(Ω),
(.)
where H stands for the Hermitic transpose operator. Since in the frequency domain the
product turns into a convolution, the estimated spectrum ̂S(𝕚Ω) can be written as
̂S(𝕚Ω) = S(𝕚Ω) ∗|W(𝕚Ω)|,
(.)
where |W(𝕚Ω)|is the spectrum of the (implicit or explicit) time window w[n] used
when a ﬁnite set of observations is available. This window has a spectrum with a main
lobe centered at the origin, and with side lobes. The width of the main lobe and the
amplitude of the side lobes tend to zero as N increases. The estimated spectrum is
then distorted by the temporal window by spreading it and reducing its resolution.
Changing the shape of the window can be useful to reduce the side lobes at the expense
of widening the main side lobe.
The signal will be also embedded in noise, whose random nature will corrupt the
estimated spectrum. The spectrum variance due to noise will decrease with the number
of spectra computed at diﬀerent time windows and used to estimate the expectation
of Equation .. This procedure is known as the Welch spectrum estimation (Welch,
). Since the number of samples is limited, the procedure described will impose a
trade-oﬀbetween the number of available spectra and the length of their corresponding
time signals. Less windows of longer lengths will produce spectral estimations with
more resolution; and conversely, more windows of smaller lengths will produce esti-
mations with less noise corruption.
An alternative spectral representation can be obtained by assuming that signal x[n]
under analysis is the result of convolving a white random process e[n] with a linear AR
system. Speciﬁcally, the signal model is
x[n] =
P
∑
m=
amx[n −m] + e[n].
(.)

26
Digital Signal Processing with Kernel Methods
The spectrum of the signal is straightforwardly computed as
̂S(𝕚Ω) =
𝜎
e
|−∑P
m=ame−𝕚Ωm|,
(.)
where 𝜎
e is the variance of e[n], also called the innovation process. Note that this expres-
sion includes the FT of the model coeﬃcients. In order to estimate these coeﬃcients,
a simple LS algorithm can be applied to minimize the estimation error, though other
methods also based on minimum mean square error (MMSE) have been proposed;
for example, see Burg’s method (Burg, ; Byrne and Fitzgerald, ). Spectral
estimation with AR models has shown much better performance than any method
based on signal windowing. Alternative strategies also exist to estimate an optimal
length for the AR model (Akaike, ; Rissanen, ).
All these methods are based on an optimization criterion, which is usually the LS.
Actually, LS leads to the ML estimator when the statistics of the signal are Gaussian.
Indeed, the FT itself is derived from the LS optimization of the approximation of a
time series with a linear combination of orthogonal complex exponentials. AR spectrum
estimation is the most obvious form of spectral analysis that uses this criterion, but also
the so-called minimum variance distortionless response (MVDR) (Van Trees, )
provides a theoretical result where one can see that the LS criterion is the ML estimate
when the signal is buried under Gaussian noise. The minimum power distortionless
response (MPDR) method (Capon, ) can be seen as a sample estimate of the MVDR,
which is, in turn, an LS method. The most common subspace method is the so-called
multiple signal classiﬁcation (MUSIC) (Stoica and Sharman, ) method, for which a
high number of variants exist. This method can be interpreted as a modiﬁcation of the
MPDR.
The MPDR can be formulated as follows. The spectrum of a signal x[n] can be
estimated by convolving the signal with a set of linear FIR ﬁlters with impulse response
wk[n] that are centered at diﬀerent frequencies Ωk, k = , … , N −. The output of the
ﬁlter will be y[n] = ∑M
m=w[m]x[n−m]. The power of the output ﬁlters can be computed
as the following expectation:
̂S(𝕚Ωk) = E
(N−
∑
m=
N−
∑
m′=
wk[m]x[n −m]x∗[n −m′]w∗
k[m′]
)
= wH
k Rxxwk,
(.)
where w = [w[], … , w[N −]]T is the coeﬃcient vector for the ﬁlter centered at
frequency Ωk. The MPDR criterion consists of minimizing the power output while
producing an undistorted response for a complex sinusoid with unit amplitude centered
at frequency Ωk. That is, we minimize
wH
k Rxxwk
(.)
subject to
N−
∑
n=
e𝕚Ωknwk[n] = .
(.)

Introduction to Digital Signal Processing
27
A Lagrange optimization of this minimization problem for Equation .gives the
following solution:
̂S(𝕚Ωk) =

v(Ωk)HR−
xxv(Ωk)
.
(.)
When compared with Equation ., it can be seen that the MPDR spectrum is the
inverse of the FT of the inverse of the autocorrelation matrix. An intuitive explanation
of this spectrum arises from the fact that in R−
xx the eigenvectors are the same as in
the signal autocorrelation matrix but the eigenvalues are inverted. We assume that, in a
high signal-to-noise ratio (SNR) regime, each sinusoid is represented by an eigenvector
with a high associated eigenvalue and that the rest of eigenvectors represent the noise
subspace, thus corresponding to low eigenvalues. In that case, the denominator of
Equation .will take a low value when frequency Ω is equal to one of the sinusoids
in the signal, which will correspond to the inverse of the corresponding eigenvalue.
For other frequencies not corresponding to the sinusoids, values will be higher. Then,
Equation .will show high values at the frequencies of the sinusoids. This shows a
much better performance than Welch methods, and it is considered a hyperresolution
method since it allows one to detect two sinusoids that are very close to each other,
much beyond the DFT limit.
A step forward can be done if the signal eigenvectors are just removed from the
autocorrelation matrix. This is exactly equivalent to changing the signal eigenvalues
by zeros. One can also change the noise eigenvalues by ones. In that case, when the
frequency of the denominator corresponds to one of the sinusoids, Equation .
will tend to inﬁnity. The procedure consists of expressing the inverted autocorrelation
as a function of its eigenvectors, and eigenvalues as R−
xx
= Q𝜦−QH, where Q is
the eigenvectors matrix and 𝜦is a diagonal matrix containing the corresponding
eigenvalues. We assume that the inverse of the signal eigenvalues have negligible values.
The following approximation is valid for spectrum estimation purposes:
R−
xx = Q𝜦−QH = Qs𝜦−
s QH
s + Qn𝜦−
n QH
n ≈Qn𝜦−
n QH
n ,
(.)
where in this context the subindices “s” and “n” respectively denote signal and noise
eigenvectors and eigenvalues.
The MUSIC method can be simply understood as a modiﬁcation of the MPDR method
where R−
xx is approximated as in Equation ., but with the noise eigenvectors changed
by ones, leading to
̂S(𝕚Ω) =

v(Ω)HQnQH
n v(Ω)
.
(.)
Note that this expression corresponds exactly to computing the DFT norm of the noise
eigenvectors, adding them together, and then computing the inverse of the result. The
DFT norm will show zeros in the frequencies corresponding to the sinusoids, since
all noise eigenvectors are mutually orthogonal. Then, its inverse will tend to inﬁnity
at these frequencies. MUSIC is another hyperresolution method that, with the price of
increasing the computational burden with respect to the MVDR, improves its resolution
performance.

28
Digital Signal Processing with Kernel Methods
The performance of a given spectral analysis method can be measured in terms of
its frequency resolution; that is, the ability to discriminate between two signals having
close frequencies. Resolution is then expressed in terms of the minimum frequency
diﬀerence that can be measured. But there are other performance measurements that
are important, such as the probability of signal detection and accuracy of frequency
and amplitude estimation. Also important is the robustness against non-Gaussian
noise and the minimum number of samples needed to provide one with a spectrum
estimation that ﬁts certain speciﬁcations over the rest of performance parameters. The
LS criterion can impose a limitation in the two latter performance criteria, since it
can produce poor results in the presence of non-Gaussian noise, and using it with no
regularization strategies may result in severe overﬁtting to the noisy observations. This
can be improved in many situations by replacing the LS criterion by alternative criteria
forcing sparsity (simplicity) of the solution, as we will show using particular SVM-based
formulations for spectral density estimation (Rojo-Álvarez et al., ).
2.1.4
Deconvolution
Deconvolution can be deﬁned as the process of recovering a signal x[n] that has been
transmitted through a medium that presents an LTI behavior and whose impulse
response h[n] is unknown. Since the original signal has been convolved with the impulse
response, the convolution consists of solving the solution of the implicit equation
y[n] = h[n] ∗x[n],
(.)
where ∗stands for the convolution operator and y[n] is the observed signal. The
earliest application of deconvolution was reﬂection seismology, introduced in by
E. Robinson (Mendel ; Silvia and Robinson ). The particular problem by then
was the study of the layered structure of the ground by the transmission of a signal and
the recording and observation of its reﬂected versions in the diﬀerent layers. In this
application, the ground impulse response, called reﬂectivity in this context, is assumed
to consist of a series of Dirac delta functions; that is, the original signal is reﬂected from
the diﬀerent layers and received at diﬀerent time instants later. Hence, this impulse
response is sparse in time. Since the spectrum of a single Dirac delta function is a
constant, the spectrum of a train of deltas suﬃciently far apart can be considered
approximately constant, so the impulse response can be approximated by a constant
too. Under these conditions, the spectrum of the received signal is an attenuated
replica of the transmitted signal. The reﬂectivity is modeled as a series of Dirac delta
functions whose amplitudes and time delays are estimated. Deconvolution has also been
extensively used in image processing for those applications where due to optical eﬀects
the image has been distorted. In these cases, the optical eﬀects can be conveniently
modeled by a spread function from the study of the physical phenomenon that blurs the
image. Similar techniques can be found in the ﬁeld of interferometry, radio astronomy,
and computational photography.
Usually, deconvolution methods use the LS criterion for estimating the Dirac delta
coeﬃcients or amplitudes, since it is a computationally aﬀordable and theoretically
simple approach. However, LS methods tend to give poor estimation performance in
situations where the noise is non-Gaussian. Speciﬁcally, LS methods are sensitive to data
outliers, deﬁned as those samples that can be considered as having a low likelihood. For

Introduction to Digital Signal Processing
29
example, a sample drawn from a Gaussian distribution can be considered an outlier if its
amplitude is higher than a given number of standard deviations. In addition, the number
of available samples in some applications is small when compared with the number of
parameters to be estimated. In those cases, the optimization problem may turn to be
ill-posed, thus leading to solutions highly biased from the optimal solution due to the
noise in the estimation of the inverse matrix of the method.
As in other signal processing problems, Tikhonov regularization (Tikhonov and
Arsenin, ) has been applied to deconvolution aiming to alleviate the previously
mentioned problems. Roughly speaking, Tikhonov regularization is a solution
smoothing, meaning that the regularized solution is smoothed or simpliﬁed with
respect to the nonregularized one; for instance, in terms of the inverse autocorrelation
matrix eigenvalues as estimated by the LS criterion. A very usual form of Tikhonov
regularization consists of including an additional term into the functional to be
optimized, usually the 𝓁norm of the parameter vector, where, in general, the 𝓁p norm
of a vector x = [x, … , xN]T is deﬁned as
𝓁p(x) = ‖x‖p ∶=
( N
∑
i=1
|xi|p
)∕p
.
(.)
Diﬀerent forms of Tikhonov regularization can be found in ridge regression, total
LS, or covariance-shaping LS methods (Hoerl and Kennard, ; Van Huﬀel and
Vandewalle, ). The minimization of this norm, resulting in a smoothed solution,
can produce reasonable approximations in ill-posed problems. Nevertheless, forcing a
minimization of the 𝓁norm of the parameters results in the solutions being less sparse.
Given that the 𝓁norm is not adequate in those cases where sparse solutions are
needed, the 𝓁norm has instead been proposed in many studies. Nevertheless, 𝓁norm
tends to drop variables correlated to one dominant signal; for example, in cases where
instead of “isolated spikes” the structure of the function to be estimated is structured in
“clusters of spikes.” In these cases, the combination of 𝓁and 𝓁norms can give more
realistic solutions.
Deconvolution problems have been addressed by making use of ML techniques
in state-variable models (Kormylo and Mendel, ). A more general solution was
presented by Mendel (), where the sparse signal presents a Bernoulli–Gauss
distribution, and modeling the impulse response was done by assuming an ARMA
model. Gaussian mixtures (GMs) have also been used in deconvolution, by assuming
that the prior distribution of the signal is a mixture of a narrow and a broad Gaussian.
Expectation–maximization (EM) algorithms can be used to estimate the posterior of
these signals.
Like other signal-processing problems, performance of the deconvolution process
degrades due to diﬀerent causes. First, the algorithm can result in numerically ill-
posed inversion problems for which regularization methods are required, as mentioned
previously. Second, sometimes the knowledge about the noise nature is limited, so
the LS criterion is typically adopted, thus yielding suboptimal results. Often, these
solutions require inverse ﬁltering modeled as ARMA models that typically fail when
the signal to be estimated does not have minimum phase (thus leading to unsta-
ble solutions). In order to alleviate these inconveniences, SVM solutions have been

30
Digital Signal Processing with Kernel Methods
proposed (Rojo-Álvarez et al., ), since their optimization criterion is regularized
and intrinsically robust to non-Gaussian noise sources, as will be seen in subsequent
chapters.
2.1.5
Interpolation
Signal interpolation is an old, yet widely studied research area (Butzer and Stens
; Jerri ). Interpolation in the information and communication applications
has its origins in sampling theory, and more speciﬁcally in the Whittaker–Shannon–
Kotel’nikov equation, also known as Shannon’s sampling theorem (García ; Shan-
non ). This theorem states, in simple terms, that a band-limited, noise-free signal
can be reconstructed accurately from a uniformly sampled sequence of its values,
as far as the sampling period is properly chosen according to the signal bandwidth.
The nonuniform sampling of a band-limited signal is a nontrivial diﬀerent situation,
which can also be addressed when the average sampling period still fulﬁlls Shannon’s
sampling theorem and the dispersion is moderate (ﬁnite). Nonuniform sampling is
naturally present and used in a number of applications, such as approximation of
geophysical potential ﬁelds, tomography, or synthetic aperture radar (Jackson et al.,
; Strohmer, ).
Given that noise is often present in real-world registered signals, the reconstruction
of a band-limited and noise-corrupted signal from its nonuniformly sampled observa-
tions becomes an even a harder problem. According to Unser (), two strategies
are mainly followed in the nonuniform interpolation literature: () considering shift-
invariant spaces, with a similar theoretical background as for uniformly sampled time
series; and () deﬁnition of a new signal basis (or new signal spaces) that can be better
suited to the nonuniform structure of the problem. The ﬁrst approach has been most
widely studied, where the sinc function is used as an interpolation kernel (Yen, ).
Although Yen’s interpolator is theoretically optimal in the LS sense, ill-posing is often
present when computing the interpolated values numerically. This is also due to the
sensitivity of the inverse autocorrelation matrix estimation of noise, in a similar way to
that introduced in the preceding section for deconvolution problems. To overcome this
limitation, numerical regularization has also been widely used in the nonuniform inter-
polation literature. Alternatively, a number of iterative methods have been proposed,
including alternating mapping, projections onto convex sets, and conjugate gradient (CG)
(Strohmer ; Yeh and Stark ), which alleviate this algorithmic inconvenience
in some speciﬁc cases. Others have used noniterative methods, such as ﬁlter banks,
either to reconstruct the continuous time signal or to interpolate to uniformly spaced
samples (Eldar and Oppenheim ; Unser ; Vaidyanathan ), but none of
these methods is optimal in the LS sense, and thus many approximate forms of the
Yen interpolator have been developed (Choi and Munson, a,b). The previously
mentioned methods have addressed the reconstruction of band-limited signals, but the
practical question of whether a signal that is not strictly band limited can be recovered
from its samples has emerged. In fact, a ﬁnite set of samples from a continuous-time
function can be seen as a duration-limited discrete-time signal in practice, and then
it cannot be band limited. In this case, the reconstruction of a signal from its samples
depends on the a priori information that we have about the signal structure or the nature
of the problem, and the classical sinc kernel has been replaced by more general kernels

Introduction to Digital Signal Processing
31
that are not necessarily band limited (Vaidyanathan, ). This issue has been mostly
studied in problems of uniformly sampled time series. A vast amount of research on
nonuniform interpolation has been also made in polynomial interpolation (Atkinson,
), which can be related theoretically to Information Theory fundamentals and
ultimately to regression. Nevertheless, this literature usually focuses on fast methods
for large amounts of available samples, such as image processing and ﬁnite elements
applications.
The following main elements can be either implicitly or explicitly considered by signal
interpolation algorithms: the kind of sampling (uniform or nonuniform), the noise
(present or not in the signal model), the spectral content (band limited or not), and
the use (or not) of numerical regularization to compensate noise sensitivity of the
interpolation methods. However, and despite the huge amount of work developed to
date, the search for new, eﬃcient interpolation procedures is still an active research
area, which especially remains relevant today for multidimensional problems.
2.1.6
System Identification
The ﬁeld of nonlinear system identiﬁcation has been studied for many years and is
still an active research area (Billings ; Giannakis and Serpedin ; Kalouptsidis
and Theodoridis ; Nelles ; Sjöberg et al. ). The main objective of system
identiﬁcation is to build mathematical models of dynamical systems from observed,
possibly corrupted data (Ljung, ). Modeling real systems is important not only to
analyze but also to predict and simulate their behavior. Actually, the ﬁeld of system iden-
tiﬁcation extends to other areas of engineering, from the design of system controllers,
the optimization of processes and systems, as well as the diagnosis and fault detection
of components and processes.
Processes and systems are interchangeable terms in this ﬁeld, always referring to the
plant under study. The processes and systems can be either static or dynamic, as well
as linear or nonlinear. In general, system identiﬁcation consists of trying to infer the
functional relationship between a system input and output, based on observations of
the in- and outgoing signals. For linear systems, identiﬁcation follows a uniﬁed approach
thanks to the superposition principle, which states that the output of a linear combina-
tion of input signals to a linear system is the same linear combination of the outputs of
the system corresponding to the individual componentsNonlinear systems, however,
do not satisfy the superposition principle, and there is not an equivalent canonical
representation for all nonlinear systems. Consequently, the traditional approach to
study nonlinear systems is to consider only one class of systems at a time, and to develop
a (possibly parametric) description that ﬁts this class and allows one to analyze it.
The response of an LTI system to a signal is characterized by the convolution of the signal with the
impulse response of the system (see Section ..). Given a system with an impulse response h[n] and two
signals x[n] and x[n], then the response of the system to a linear combination of both signals is
y[n] = h[n] ∗(𝛼x[n] + 𝛽x[n]) = 𝛼h[n] ∗x[n] + 𝛽h[n] ∗x[n] by virtue of the linearity of the convolution
operator.

32
Digital Signal Processing with Kernel Methods
Classical Linear System Identification
The generalization of the Wiener ﬁlter theory to dynamical systems with random
inputs was developed by Kalman in the s in the context of state-space mod-
els. Discrete-time (and dynamical) linear ﬁlters can be nicely expressed in the state-
space formulation (or Markovian) representation, by which two coupled equations are
deﬁned as
y[n] = Cx[n] + Du[n]
(.)
x[n + ] = Ax[n] + Bu[n],
(.)
where x[n] is the vector states at time n, u[n] is the input at time n, y[n] is the output at
time n, A is the state-transition matrix and determines the dynamics of the system, B is
the input-to-state transmission matrix, C is the state-to-output transmission matrix,
and D is the input-to-output transmission matrix. The state-space representation is
especially powerful for multi-input multi-output (MIMO) linear systems, time-varying
linear systems, and has found many applications in the ﬁelds of system identiﬁcation,
economics, control systems, and communications (Ljung, ).
The KF is the most widely known instantiation of a state-space formulation. The
KF uses a series of noisy measurements observed over time to produce estimates
of unknown variables that tend to be more precise than those based on a single
measurement alone. Essentially, the KF operates recursively on noisy input signals to
produce a statistically optimal estimate of the underlying system state. Notationally, the
KF assumes that the true state at time n evolves from the previous state at time n −
following
x[n] = F[n]x[n −] + B[n]u[n] + w[n],
(.)
where F[n] is the state transition model which is applied to the previous state
x[n −], B[n] is the control-input model applied to the control vector u[n], and w[n]
is the process noise assumed to be drawn from a zero-mean multivariate normal
distribution with covariance Q[n]; that is, w[n] ∼(𝟎, Q[n]). The impact of KFs in
the ﬁeld of dynamical system identiﬁcation has been enormous, and many speciﬁc
applied ﬁelds, such as econometrics or weather forecasting, have beneﬁted from
it in recent decades. The main limitation of the estimation of the KF is that it is
designed to represent a linear system, but many of the real systems are intrinsically
nonlinear. A solution is to “linearize” the system ﬁrst and then apply a KF, thus yielding
the extended KF, which has long been the de facto standard for nonlinear state-
space estimation, mainly due to its simplicity, robustness, and suitability for real-time
implementations.
Classical Nonlinear System Identification: Volterra, Wiener, and Hammerstein
One of the earliest approaches to parametrize nonlinear systems was introduced by
Volterra, who proposed to extend the standard convolutional description of linear
systems by a series of polynomial integral operators Hp with increasing degree

Introduction to Digital Signal Processing
33
of nonlinearity (Volterra, a,b,c). For discrete-time systems, this description
becomes simply
y[n] = Ho +
∞
∑
p=
Hp(x[n]),
(.)
where x[n] and y[n] are the input and output signals. Assuming the nonlinear system
described is causal and with ﬁnite memory, functionals Hp can be expanded in Volterra
kernels that actually depend of the previous inputs until time n. These Volterra kernels
can then be described as linear combinations of products between elements x[n −m],
≤m ≤M, M being the memory of the system model. Then, Equation .is then
described by the coeﬃcients of these linear combinations (Alper, ).
Note that Volterra series extend Taylor series by allowing the represented system
output to depend on past inputs. Interestingly, if the input signals are restricted to
a suitable subset of the input function space, it can be shown that any continuous,
nonlinear system can be uniformly approximated up to arbitrary accuracy by a Volterra
series of ﬁnite order (Boyd and Chua ; Brilliant ; Fréchet ). Thanks to this
approximation capability, Volterra series have become a well-studied subject (Franz
and Schölkopf ; Giannakis and Serpedin ; Schetzen ) with applications
in many ﬁelds.
Although the functional expansion of Volterra series provides an adequate represen-
tation for a large class of nonlinear systems, practical identiﬁcation schemes based on
this description often result in an excessive computational load. It is for this reason that
several authors have considered the identiﬁcation of speciﬁc conﬁgurations of nonlinear
systems, notably cascade systems composed of linear subsystems with memory and
continuous zero-memory nonlinear elements (Gardiner ; Narendra and Gallman
). When the output of a system depends nonlinearly on its inputs, it is possible to
decompose the input–output relation as either a Wiener system (linear stage followed
by a nonlinearity) or a Hammerstein system (nonlinearity stage followed by a linear
system).
The Wiener system (Billings and Fakhouri, ) consists of a linear ﬁlter followed by
a static memoryless nonlinearity. When an FIR ﬁlter with length L samples is chosen as
the linear part, the system output for a given input signal x[n] is obtained as
y[n] = f
( L−
∑
m=
h[m]x[n −m]
)
,
(.)
where h[n] represents the impulse response of the linear ﬁlter and f (⋅) is the nonlinear-
ity. The Wiener model is a much more simpliﬁed version of Wiener’s original nonlinear
system characterization (Wiener, ). However, despite its simplicity, it has been used
successfully to describe a number of nonlinear systems appearing in practice. Common
applications include biomedical engineering (Hunter and Korenberg, ; Westwick
and Kearney, ), control systems (Billings and Fakhouri, ; Greblicki, ),
digital satellite communications (Feher, ), digital magnetic recording (Sands and
Cioﬃ, ), optical ﬁber communications (Kawakami Harrop Galvão et al., ), and
chemical processes (Pajunen, ).

34
Digital Signal Processing with Kernel Methods
The Hammerstein system (Billings and Fakhouri, ) consists of a static memoryless
nonlinearity followed by a linear ﬁlter. When the linear part is represented by an FIR
ﬁlter, the output of a Hammerstein system is obtained as
y[n] =
L−
∑
m=
h[m] f (x[n −m]).
(.)
Hammerstein systems are encountered, for instance, in electrical drives (Balestrino
et al., ), acoustic echo cancellation (Ngia and Sjobert, ), heat exchangers,
and biomedical modeling (Westwick and Kearney, ). Wiener and Hammerstein
systems form particular types of block-oriented structures (Chen, ; Giannakis and
Serpedin, ). Other popular cascade models include the so-called sandwich models
that combine more than two blocks; for instance, the Hammerstein–Wiener model,
which consists of a regular Hammerstein system followed by an additional nonlinearity.
Modern Nonlinear System Identification
The area of nonlinear system identiﬁcation is huge and any attempt to summarize the
contributions and techniques is certainly futile (Ljung, ). The previous building
blocks have generated a plethora of identiﬁcation approaches for Wiener and Hammer-
stein systems since the late seventies. Most of them are supervised techniques, although
in the last decade a number of blind, unsupervised identiﬁcation methods have been
proposed as well.
Supervised Nonlinear System Identification
Following the excellent book by Nelles (), one may split the ﬁeld according to the
static or dynamic nature of the models. The family of nonlinear static model archi-
tectures comprises simple linear models and parametric polynomial models, which
advantageously are linear in the parameters but limited in terms of generalization
capabilities and exhibit poor behavior in high-dimensional scenarios as well. One also
ﬁnds look-up tables (LUTs), which essentially dominate in industrial applications, being
simple and fast to implement in microcontrollers. The LUT approach has as the main
feature that the parameters do not necessarily come from any regression method, but
are typically determined directly. The main problems of LUTs come with even a low
number of inputs. A more robust and eﬃcient family of nonlinear static models are
NNs, whose most successful models are the classical MLP (typically working with RBF
nodes, hence called RBF networks), and neuro-fuzzy models which may shed light on
the problem due to the implementation of tunable membership functions. Additionally,
local linear (Takagi–Sugeno) neuro-fuzzy models have been very popular in recent
decades, especially because of the eﬃcient training, interpretability of the model, and
relative robustness in high-dimensional problems.
On the other hand, the family of nonlinear dynamic models builds upon the previous
family of static models. Therefore, one also encounters approaches based on polyno-
mials, NNs, fuzzy model architectures, and linear neuro-fuzzy models. The ﬁeld of
dynamic NNs is a distinct one: these are NN architectures encoding the dynamics
internally, and have been very popular for speech processing, natural language pro-
cessing, control, and biomedical signal applications. The most successful architectures
involve extensions of the MLP following two main philosophies. The ﬁrst one considers

Introduction to Digital Signal Processing
35
including additional recurrent layers that process the predictions to feed either the
input (Elman’s net) or the hidden (Jordan’s net) layer. The second approach considers
replacing the standard scalar weights in the MLP by vectors and multiplications by dot
products. If an FIR ﬁlter is included one speaks about the FIR NN that processes the
temporal information internally in the network synapses, not as a particular ad hoc
embedding at the input layer. Other more sophisticated ﬁlters can replace the weights,
such as gamma ﬁlters that introduce recursive connections locally. Interestingly, there
are extensions of the back-propagation algorithm for these architectures in which error
terms are symmetrically ﬁltered backward through the network.
Including the System Structure
Traditional supervised methods do not make any assumption about the system
structure. Nonlinear identiﬁcation was tackled by considering nonlinear structures
such as MLPs (Erdogmus et al., a), recurrent NNs (Kechriotis et al., ),
or piecewise linear networks (Adali and Liu, ). Other black-box identiﬁcation
methods include techniques based on orthogonal LS expansion (Chen et al., ;
Korenberg, ) and separable LS (Bruls et al., ). In order to improve the
identiﬁcation results, a number of algorithms were introduced that exploit the
Wiener or Hammerstein structure explicitly. This can be done, for instance, by an
identiﬁcation scheme that mimics the unknown system and estimates its param-
eters iteratively (Billings and Fakhouri , ; Dempsey and Westwick ;
Greblicki , ; Haykin ; Hunter and Korenberg ; Narendra and
Gallman ; Westwick and Kearney ; Wigren ). Another approach that
exploits the system structure consists of a two-step procedure that consecutively
estimates the linear part and the nonlinearity of the Wiener or Hammerstein
systems. Most of the proposed two-step techniques are based on predeﬁned test
signals (Bai, ; Pawlak et al., ; Wang et al., ).
A diﬀerent proposition is found in Aschbacher and Rupp (), where both blocks
of the nonlinear system are estimated simultaneously through a coupled regression on
the unknown intermediate signal. A generalization of this technique for robust identi-
ﬁcation based on nonlinear canonical correlation analysis has also been presented (Van
Vaerenbergh and Santamaría, ; Van Vaerenbergh et al., ). Extensions to
the standard system identiﬁcation settings have also been proposed; for instance, to
account for complex signals (Cousseau et al., ) and MIMO Wiener or Hammerstein
systems (Goethals et al., a).
Unsupervised Nonlinear System Identification
Although all the aforementioned techniques are supervised approaches (i.e., input
and output signals are known during estimation), there have also recently been a
few attempts to identify Wiener and Hammerstein systems blindly. Most of these
techniques make certain assumptions about the input signal; for instance, requiring it
to be Gaussian and white (Gómez and Baeyens, ; Vanbeylen et al., ). A less
restrictive method in Taleb et al. () only assumed that the input signal was i.i.d.,
and the resulting technique aimed to recover the input signal by minimizing the mutual
information of the inversion system output.

36
Digital Signal Processing with Kernel Methods
2.1.7
Blind Source Separation
Blind source separation (BSS) is an important problem in signal processing. The topic is
recurrent in many applications, including communications, speech processing, hyper-
spectral image analysis, and biomedical signal processing. In general, the goal of BSS is
to recover a number of source signals from their observed linear or nonlinear mixtures
(Cardoso ; Comon et al. ). Three main problem settings can be identiﬁed,
depending on the type of the mixing process, the number of sources ns, and the number
of available mixtures m.
Standard Problem Setting
The most basic scenario of BSS assumes a simple linear model in which the measure-
ment random vector y ∈ℝm can be described as an instantaneous mixture, y = As + n,
where s ∈ℝns is a zero-mean independent random vector representing a statistically
independent source, A ∈ℝns×m is the unknown mixing matrix, and n ∈ℝm is an
independent random vector representing sensor noise.
Many techniques have been developed for the case where as many mixtures as
unknown sources are available (m = ns). Most of them are based on independent
component analysis (ICA) (Comon, ; Hyvärinen et al., ), a statistical technique
whose goal is to represent a set of random variables as linear functions of statistically
independent components. In the absence of noise, this reduces to the problem of
estimating the unmixing matrix W = A−, which allows one to retrieve the estimated
sources as
̂x = Wy = WAs.
(.)
The unmixing matrix W is found by minimizing the dependence between the elements
of the transformed vector ̂x. In order to estimate this matrix, a suitable dependence
measure (or contrast function) must be deﬁned and minimized with respect to W.
Most ICA contrast functions can be derived using the ML principle. For this problem
setting, an impressive amount of algorithms have been proposed in the literature. Note
that these methods only exploit the spatial structure of the mixtures, which is a suitable
approach for mixtures of i.i.d. sources. If the sources have temporal structure, this
information can also be exploited; for instance, see Molgedey and Schuster (). We
will brieﬂy review some of the most interesting techniques.
Among them, the most relevant ones are: () the infomax principle/algorithm (Bell
and Sejnowski, ), which tries to maximize the log-likehood function of the mix-
ing matrix, and it is simple and computationally eﬃcient; () the joint approximate
diagonalization of eigenmatrices algorithm (Cardoso and Souloumiac, ), which
is based on higher order statistics, whose computation makes it unsuitable in large-
scale scenarios; () the FastICA algorithm (Hyvärinen and Oja, ), which uses a
fast “ﬁxed-point” iterative scheme to maximize the non-Gaussianity of the estimated
sources, and it is fast and eﬃcient, but its solution is too ﬂexible and dependent on the
chosen nonlinearity; () the kernel ICA (KICA) (Bach and Jordan, b), which uses
a kernel canonical correlation analysis based contrast function to obtain the unmixing
matrix W. KICA outperforms the rest in accuracy and robustness, but it comes at the
price of higher computational burden.

Introduction to Digital Signal Processing
37
Underdetermined Blind Source Separation
The previous methods dealt with the standard BSS problem, where the number of
mixtures and sources are the same. If more mixtures than sources are available (m > ns),
the redundancy in information can be used to extend existing techniques to achieve
additional noise reduction (Joho et al., ). If less mixtures than sources are available
(m < ns), a more complex scenario arises. This so-called underdetermined BSS problem
can only be solved if one relies on a priori information about the sources. Two-step
procedures are typically deployed: ﬁrst the mixing matrix is estimated, often based on
geometric properties, and then the original sources are recovered.
An extreme case of underdetermined BSS problems occurs when only one mixture is
available. In audio applications, this scenario is known as the single-microphone source
separation problem (Bach and Jordan ; Roweis and Saul ). In more general
problems, it is often assumed that the data can be transformed to a domain in which the
sources are sparse, meaning that they equal zero most of the time. For instance, audio
separation algorithms typically work in the time–frequency domain, where overlap of
the sources is likely to be much smaller than in the time domain (Belouchrani and Amin,
).
Under a geometrical viewpoint, several algorithms have been proposed to separate
sparse sources in an underdetermined problem setting (Boﬁll and Zibulevsky ;
Lee et al. ; Luengo et al. ; Vielva et al. ). In a ﬁrst stage, these methods
estimate the mixing matrix A by identifying the main vectors to which the samples in
the scatter plot are aligned. A large number of estimators have been proposed for this
purpose, among them a Parzen windowing-based method (Erdogmus et al., b), a
line spectrum estimation method (Vielva et al., ), and a kernel principal component
analysis (KPCA) based technique (Desobry and Févotte, ). Once A is known, the
original source samples are estimated. The most straightforward method to do this is
by using the pseudo-inverse of A. Better estimates can be obtained by the shortest path
(𝓁-norm) method introduced in Boﬁll and Zibulevsky () or the maximum a
posteriori (MAP) estimator described in Vielva et al. ().
Post-nonlinear Blind Source Separation
A considerable amount of research has also been done on the so-called post-nonlinear
BSS problem, in which the sources are ﬁrst mixed linearly and then transformed
nonlinearly, hence boiling down to a sort of Hammerstein model. In this scenario, the
m sensors that measure the mixtures reveal some kind of saturation eﬀect or another
nonlinearity, which results in the extension of the basic model to a post-nonlinear
mixture model:
x = f (As) + n,
(.)
where f (⋅) returns an m × vector of nonlinear pointwise functions and x ∈ℝm is the
measurement random vector. In this model, each component fi(⋅) of f (⋅) aﬀects only one
mixture so that there are no cross-nonlinearities. In the underdetermined case (m < n),
the addition of the post-nonlinear transformations introduces an additional diﬃculty
for the estimation of the mixing matrix. Therefore, it is not suﬃcient to apply a clustering
technique on the angle histogram.

38
Digital Signal Processing with Kernel Methods
For an equal number of mixtures and sources (m = n), some algorithms have been
proposed (Babaie-Zadeh et al. ; Solazzi et al. ; Taleb and Jutten ; Tan
and Wang ) that treat the post-nonlinear problem. However, these algorithms
cannot deal with the more restricted problem of underdetermined post-nonlinear BSS.
An underdetermined algorithm was successfully proposed in Theis and Amari (),
which requires the number of active sources at each instant to be lower than the total
number of available mixtures m and assumes noiseless mixtures.
2.2
From Time–Frequency to Compressed Sensing
2.2.1
Time–Frequency Distributions
TFDs aim to represent the energy (or intensity) of a signal in time and frequency
simultaneously. Standard spectral analysis (i.e., Fourier decomposition) gives us the
signal components (frequencies) and their relative intensity, but it does not inform us
about the speciﬁc time in which those frequencies dominate the signal. If we want to
analyze the spectra of a signal that changes over time, a straightforward way to do so
is to obtain its spectrum in time intervals of duration t. This simple strategy is valid
if the signal changes at a rate slower than t, otherwise we have to decrease the time
interval accordingly. This is the idea behind the so-called short-time FT (STFT), or
spectrogram, which nowadays is still a standard and powerful method to describe time-
varying signals. The spectrogram was initially developed during the s to analyze
human speech. Despite its widespread use, it has two main drawbacks. First, some
signals can change so quickly that it makes it almost impossible to ﬁnd a short-time
window for which the signal can be considered stationary; that is, the time resolution
is bounded. Second, shortening the time window reduces the frequency resolution.
These two problems are tied in such a way that we have a trade-oﬀbetween time and
spectral resolutions for any signal representation: improving one resolution decreases
the resolution of the other. Section ..discusses the intuition behind this trade-oﬀ
and the relation to the Heisenberg principle in quantum physics.
A time distribution represents the intensity of a signal per unit time at time t. Similarly,
a frequency distribution represents the intensity of a signal per unit frequency at
frequency 𝜔. Therefore, a straightforward deﬁnition of a TFD is the one representing
the energy or intensity of a signal per unit time and per unit frequency. Therefore, a joint
distribution P(t, 𝜔) describes the intensity at time t and frequency 𝜔, and P(t, 𝜔)ΔtΔ𝜔
is the fractional energy in the time–frequency interval ΔtΔ𝜔at (t, 𝜔). Ideally, a TFD
should fulﬁll the following properties:
Property 
|x(t)|= ∫P(t, 𝜔) d𝜔
Property 
|X(𝜔)|= ∫P(t, 𝜔) dt
Property 
E = ∫P(t, 𝜔) d𝜔dt
Property states that adding up the energy distribution for all frequencies at a particular
time should give the instantaneous energy at that time, |x(t)|. On the other hand,
property states that adding up over all times at a particular frequency should give
the energy density spectrum, |X(𝜔)|. Finally, integrating both in frequency and time
gives the total energy E of the signal (property ).

Introduction to Digital Signal Processing
39
From the theoretical deﬁnition of what a TFD should be, the question is whether
there is a practical TFD that can fulﬁll the aforementioned properties and whether it
would be a true density distribution. As it turns out that this is not actually possible,
the question is then to ﬁnd reasonable approaches to theoretical TFDs, according to
diﬀerent speciﬁcations for diﬀerent problems.
In the following, we will use P(t, 𝜔) for denoting a theoretical TFD of x(t), but we
will use instead X(t, 𝜔) for denoting the diﬀerent deﬁnitions of the existing and speciﬁc
time–frequency transforms.
As we mentioned before, a simple yet powerful way to deal with signals changing
both in time and frequency is the STFT. In the continuous case, the time function to be
transformed is multiplied by a window function, and then the FT is taken as the window
moves along the time axis:
X(t, 𝜔) = ∫x(𝜏)w(𝜏−t) e−𝕚𝜔𝜏d𝜏,
(.)
where w(t) is a window function, such as Hamming, Hanning, Han, or Gaussian win-
dows. The spectrogram is obtained by taking the squared magnitude; that is: |X(t, Ω)|.
The discrete case can be expressed as
X(n, Ω) =
∞
∑
n=−∞
x[k]w[k −n] e−𝕚Ωk,
(.)
where in this case n is discrete, but Ω is still a continuous variable.
The ﬁrst studies that considered the question of a joint distribution function in time–
frequency were presented by Gabor () and Ville (). The Gabor transform,
although not a TFD, is still a special case of an STFT where the window is a Gaussian
function; that is:
X(t, 𝜔) = ∫x(𝜏) e−π(𝜏−t)e−𝕚𝜔𝜏d𝜏,
(.)
where we can easily identify the window function as w = e−π(𝜏−t).
On the other hand, Ville obtained a TFD from the distribution presented in Wigner
() years before in the ﬁeld of quantum statistical mechanics. The Wigner–Ville
distribution, often referred to only as the Wigner distribution, is given by
X(t, 𝜔) = 
π ∫x
(
t + 
𝜏
)
x∗(
t −
𝜏
)
e−𝕚𝜔𝜏d𝜏.
(.)
The Wigner distribution satisﬁes the marginals property; that is, we can obtain the
instantaneous energy in a given time instant or for a given frequency, integrating for
all frequencies or for all time instants respectively. However, the Wigner distribution is
a nonlinear transform. When the input signal has more than one component, a cross-
term appears. Another problem of the Wigner–Ville distribution is that it is not always
positive for an arbitrary signal, which makes it diﬃcult (or even impossible) to obtain a
physical interpretation in some cases. In contrast, the STFT is always positive, although
it does not satisfy the marginals property.

40
Digital Signal Processing with Kernel Methods
Another interesting particularity of the Wigner distribution is that, in general, it can
have nonzero values even when there is no signal. A simple example of a signal that
exhibits this behavior is one which has nonzero values initially, then is zero in the next
time interval, and then is again nonzero in the next time interval. The Wigner TFD for
such a signal shows nonzero values in the time–frequency representation just whenever
the signal is zero. This is caused by the cross-terms mentioned before.
Another well-known TFD was devised by Kirkwood () a year after Wigner
proposed his, also in the ﬁeld of quantum mechanics. However, this TFD is known as the
Rihaczek distribution (Rihaczek, ), derived from the Kirkwood distribution taking
into account physical considerations:
X(t, 𝜔) =

√
π
x(t)X∗(𝜔) e−𝕚𝜔t.
(.)
One of the reasons the Rihaczek distribution is less popular than the Wigner distri-
bution is because it is complex and thus it can hardly represent a physical process.
Nevertheless, the Wigner distribution has its physical issues too, giving in some cases
negative values for the energy. Moreover, the real part of the Rihaczek distribution is also
a TFD, called the Margenau–Hill distribution (Margenau and Hill, ), that satisﬁes
the marginals property.
Page () derived another famous TFD, introducing the concept of a running
spectrum, based on the assumption that we have access to signal samples until time t and
hence no information is available about the future, 𝜏≥t. Note that this is in contrast
to other distributions where the integration is done covering the full time axis. Taking
the FT of such a signal and deriving it with regard to time t gives the Page distribution,
expressed as:
X(t, 𝜔) = 𝜕
𝜕t
||||||

√
π ∫
t
−∞
x(𝜏) e−𝕚𝜔𝜏d𝜏
||||||

.
(.)
Interestingly, the method used by Page () to derive this distribution was employed
extensively to obtain other TFDs.
All the distributions we have seen here are valid TFDs. There is none that may be said
to be better than another. All of them satisfy the marginals, and each one has interesting
and useful properties. Choosing one or another depends on the speciﬁc problem at
hand. By the time these TFDs were discovered and presented, there was the unanswered
question of how many diﬀerent TFDs existed, and what properties they might have. It
was Cohen () who showed that an inﬁnite number of TFDs can be obtained from
the following general equation:
X(t, 𝜔) =

π∫∫∫e−𝕚𝜃t−𝕚𝜏𝜔+𝕚𝜃u𝜙(𝜃, 𝜏)x∗(
u −
𝜏
)
x
(
u + 
𝜏
)
du d𝜏d𝜃,
(.)

Introduction to Digital Signal Processing
41
where 𝜙(𝜃, 𝜏) is a general function called the kernel. Choosing diﬀerent kernels leads to
diﬀerent distributions. For instance, ﬁxing 𝜙(𝜃, 𝜏) = yields the Wigner distribution,
setting 𝜙(𝜃, 𝜏) = e𝕚𝜃𝜏∕gives the Rihaczek distribution, and if 𝜙(𝜃, 𝜏) = e𝕚𝜃|𝜏|∕the Page
distribution emerges. This procedure to generate all distributions has many advantages.
Most notably, it allows one to compare the properties of the diﬀerent distributions.
It is also worth noting that these properties can be obtained directly from the kernel
deﬁnition.
In order to illustrate the usefulness of having a uniﬁed way to derive TFDs based on
a kernel function, we will ﬁnish this section showing the Choi–Williams distribution.
Choi and Willians () addressed the problems of the Wigner distribution related
to the cross-terms. Starting from considering a signal made up of components, Choi
and Willians () used the Cohen equation to obtain an expression where the self-
and cross-terms appeared in a separate form. They realized that by choosing the kernel
wisely it was possible to minimize the cross-terms, retaining at the same time most parts
of the self-terms. The kernel they engineered was 𝜙(𝜃, 𝜏) = e−𝜃𝜏∕𝜎, where 𝜎is a free
parameter that controls the ability of the Choi–Williams distribution to suppress the
cross-terms. With this kernel, the obtained TFD is
X(t, 𝜔) =

π∕∫∫

√
𝜏∕𝜎
e−[(u−t)∕(𝜏∕𝜎)]−𝕚𝜔𝜏x∗(
u −
𝜏
)
x
(
u + 
𝜏
)
du d𝜏,
(.)
which is the Choi–Williams distribution, which greatly alleviates the cross-terms prob-
lem of the Wigner distribution.
2.2.2
Wavelet Transforms
Time–frequency representations of nonstationary signals are based on the strategy
of taking windows of the signal and analyzing them separately. That way, under the
assumption that the signal is almost stationary inside each window, one can observe
its frequency properties as an approximation. Nevertheless, this approach will be more
or less eﬀective depending on the length of the window with respect to the properties
of the signal. If it is highly nonstationary, frequency properties may only be observed if
the window is small. But the frequency eﬀect of the windowing is the convolution of the
FT of the signal with the transform of the window. If the window is narrow in time, its
FT will be wide, thus blurring the frequency components of the signal that we want to
analyze. This is, straightforwardly, a form of Heisenberg’s principle, which states, in this
case, that it is not possible to know at the same time the frequency and the exact time
where a signal has been measured. This indicates that a time–frequency representation
of a signal is eventually not possible. A trade-oﬀis then imposed by nature between the
accuracy of a measure in its frequency and in its time.
Although this limitation is a fundamental rule, there are ways to improve the joint
representation of a signal. Roughly speaking, it is possible to use fully scalable mod-
ulated windows to analyze the properties of a signal. The idea is to use diﬀerent
versions of the window in diﬀerent scales, thus obtaining a multiscale representation
of the signal. The result will be a representation with diﬀerent levels of detail, from

42
Digital Signal Processing with Kernel Methods
a more general representation, covering longer time intervals, to a more detailed
representations, with shorter intervals. This allows an analysis that goes from global to
local signal representations. This is in fact the underlying idea of the so-called wavelet
representations.
A wavelet is a time-limited wave whose limits continuously tend to zero when time
tends to minus and plus inﬁnity. The basic properties of the wavelets can be summarized
as follows (Sheng, ): if a transform is used to construct a time–frequency, or more
generally speaking, a time–scale representation, it should be possible to reconstruct the
original signal from its representation.
In DSP, only discrete wavelets are considered. Nevertheless, the fastest way to describe
the properties of wavelets is to ﬁrst describe the continuous-time wavelet transform and
their properties. This paves the way to later introduce then the discrete-time wavelet
transform, which is constructed as a discretized version of a continuous one, thus
keeping its properties.
Continuous-Time Wavelet Transform Analysis
The wavelet transform of a time function x(t) can be described as the linear operation
X(s, 𝜏) = ∫t
x(t)𝜙∗
s,𝜏(t) dt,
(.)
where 𝜙s,𝜏(t) is the wavelet function. This function has two parameters. The ﬁrst one is
the scale s, which determines the width of the function in the time domain. The second
one is the delay 𝜏, and determines the time instant where the signal is centered with
respect to the time origin. Then, a wavelet transform is obtained for each pair (s, 𝜏), and
hence this is a D transform. The continuous wavelet set is constructed from a basic
function called a mother wavelet through its scaling and translation. That is, 𝜙s,𝜏(t) =
s−∕𝜙( t−𝜏
s ). Factor s−∕is added to keep the energy of the wavelets constant with the
scale factor s. The wavelet transform is the convolution of the wavelet with the signal
under analysis:
X(s, 𝜏) = ∫t
x(t)𝜙∗(t −𝜏
s
)
dt.
(.)
This deﬁnition, translated into the discrete domain, leads to the idea of ﬁlter bank
analysis. For a wavelet to be valid, the following inverse transform must be satisﬁed:
x(t) = ∫s,𝜏
X(s, 𝜏)𝜙s,𝜏(t) ds d𝜏.
(.)
An example of continuous wavelet is the so-called Mexican hat wavelet, whose
expression and FT are respectively:
𝜙(t) = (−t) e−(t∕)
(.)
𝛷(𝜔) = 𝜔e−(𝜔∕).
(.)

Introduction to Digital Signal Processing
43
Note that the function in time has a maximum in the origin, and it vanishes with time
tending to inﬁnity. Its spectrum is band-pass. This function is the second derivative of
the Gaussian function, which is also an admissible wavelet.
Discrete-Time Wavelet Transform Analysis
Under certain restrictions, the original signal can be recovered even if the wavelet
functions are discretized in delay 𝜏and scale s. In particular, wavelet functions are
discretized by taking sj = sj
and 𝜏j = ksj
𝜏, where k and j are integers and usually
𝜏= and s= .
A discrete time signal x[n] can be represented as a function of wavelets as
x[n] = c𝜙
∑
k
∑
j
aj,k𝜙j,k(n),
(.)
where aj,k = ∑
n x[n]𝜙j,k[n], and c𝜙is an amplitude factor.
The discrete wavelet transform analyzes the signal using two diﬀerent functions called
scaling functions l[n] (low-pass ﬁlters) and wavelet functions h[n] (high-pass ﬁlters).
The decomposition of the signal is simply performed by successive decomposition of the
signal in high-pass and low-pass signals. An example is the subband coding algorithm.
In that algorithm, the signal is convolved with a scaling and a wavelet function acting
as two half-band ﬁlters. After the ﬁltering, the resulting signals can be downsampled by
two since they have half the bandwidth. This downsampling is what actually produces
the scaling of the wavelet functions. The low-pass signal is ﬁltered again to produce
two new subbands, downsampled again, and the process is iterated until, due to the
downsampling, just one sample is kept at the output of the ﬁlters. The step j of the
procedure can be summarized as follows:
ylow,j[k] =
∑
n
ylow,j−[n]l[k −n]
(.)
yhigh,j[k] =
∑
n
ylow,j−[n]h[k −n].
(.)
Note that the total number of samples is not increased with respect to the original signal.
The highest band will contain W∕, W being the bandwidth of the original signal, and
the subsequent bands will have a decreasing bandwidth W∕j.
This representation of the signal has a major advantage with respect to the FT, and it is
that the time localization of the signal is never lost. The most important frequencies in
the signal will be detected by a higher amplitude in the corresponding subband. Higher
frequency subbands will have a higher time resolution, but a lower frequency resolution,
since these subbands have a higher bandwidth. Lower bands will have an increasing
frequency resolution, and thus a decreasing time resolution. Note that the signal can
be compressed by discarding those bands whose energy is negligible (typically via
thresholding), thus resulting in a compressed signal that can be further reconstructed
using a procedure inverse to the one in Equations .and ..

44
Digital Signal Processing with Kernel Methods
Applications
Applications of wavelet transforms in communications are diverse. Multiresolution
analysis is useful to decompose the signal of interest in meaningful components. An
example of multiresolution analysis can be found in the detection of transient eﬀects
in power lines. Such analysis is able to distinguish transient events, present in higher
frequencies at much lower power, from the baseline low-pass high-power signal. This
helps to characterize the occurrence instants and the nature of the transients (Wilkinson
and Cox, ). In communications, wavelets have been used to detect weak signals in
noise (Abbate et al., ). Wavelets have also been used in biomedical signals in order
to characterize ECG waves for diagnostics (Huang and Wang, ).
Relevant applications can also be found in audio compression, where the ﬁlter
functions approximate the critical bands of the human hearing system. After a subband
decomposition, only bands with relevant energy are preserved, and the rest are dis-
carded. This provides reasonable rates of compression without a noticeable distortion,
or high compression rates with lower – still reasonable – quality levels (Sinha and Tew-
ﬁk, ). D wavelet transforms are also used for image and video compression (Ohm,
). These compression strategies are present in the JPEGstandards. Wavelets
have also been proposed for texture classiﬁcation, denoising, system identiﬁcation,
speech recognition, and many other applications in signal processing and related areas
(e.g., Randen and Husoy, ).
2.2.3
Sparsity, Compressed Sensing, and Dictionary Learning
The last decade has witnessed a growing interest in the ﬁeld of sparse learning, mostly
revived by the achievements of compressed sensing (Candes et al., ) and dictionary
learning (Tropp and Wright, ). These two ﬁelds are at the intersection of signal
processing and machine learning, and have revolutionized signal processing and many
application domains dealing with sensory data. The reason for that is in the core of signal
processing: both approaches try to ﬁnd signal descriptions and feature representation
spaces in terms of a few, yet fundamental, components. Let us brieﬂy review the main
elements in this ﬁeld.
Sparse Representations
As we have seen in the previous section, an adequate representation of the signal
is crucial to obtain good results. The wavelet domain is an adequate representation
domain for natural signals (like audio and images) because it captures the essence
of the underlying process generating the data with minimal redundancy (i.e., high
compact representation). The representation domain is called sparse because only a few
coeﬃcients (much lower than the signal dimensionality) are needed to represent the
data accurately. Sparsity is an attribute that is met in many natural signals, since nature
tends to be parsimonious and optimal in encoding information. Current models of the
brain suggest that the diﬀerent hierarchically layered structures in the visual, olfactory,
and auditory signal pathways pursue lower-to-higher abstract (more independent and
sparse) representations.
Signal processing has not been blind to this description of living sensory systems.
Many applications have beneﬁted from sparse representations of the signal. For exam-
ple, one may ﬁnd the case of Internet telephony and acoustic and network environments

Introduction to Digital Signal Processing
45
(Arenas-García and Figueiras-Vidal ; Naylor et al. ), where the echo path,
represented by a vector comprising the values of the impulse response samples, is
sparse. A similar situation is found in wireless communication systems involving
multipath channels. More recently, sparsity has been exploited in channel estimation
for multicarrier systems, both for single antenna ands for MIMO systems (Eiwen et al.,
a,b). Sparsity in multipath communication systems has been extensively treated
as well (Bajwa et al., ). But perhaps the most widely known example of exploiting
sparse signal representations has to do with signal compression: the DFT, the DCT,
and the discrete wavelet transform (DWT) are the most widely known and successful
examples. While the DCT is the basis for the familiar JPEG algorithm, the DWT is in
the core of the more recent JPEG-coding algorithm (Gersho and Gray, ).
Notationally, a transform 𝜱is simply an operation that projects the acquired signal
y ∈ℝd into new coordinate axes, thus giving rise to a new signal representation
x = 𝜱Hy (known as the analysis equation). Typically, such a transform is required to
be orthonormal; that is, 𝜱H𝜱= I. In the case of orthonormal transforms, the inverse of
x reduces to y = 𝜱x (known as the synthesis equation). The new representation is called
sparse (and the signal is termed compressible) if one can throw away a large number of
components n −m of x, for m ≪n, in such a way that a good representation of x can
still be recovered with minimal error by inverting the largest m components of x only.
More formally, the signal x is called m-sparse if it is a linear combination of only m basis
vectors; that is, only m coeﬃcients in y are nonzero.
Dictionary Learning
We have identiﬁed common transforms for 𝜱such as the DWT. These transforms lead
to compact signal representations, but they involve ﬁxed operations that might be not
adapted to the signal characteristics. The ﬁeld of dictionary learning deals with the more
ambitious goal of inferring the transform directly from the data. Therefore, the goal
of attaining a sparse representation boils down to simply expressing y of dimension d
as a linear combination of a smaller number of signals taken from a database called
a dictionary. The columns of the DWT compose a dictionary, which in the case of an
orthonormal transform is a complete dictionary. Elements in the dictionary are typically
unit norm functions called atoms. Let us denote in this context the dictionary as and
the atoms as 𝜙k, k = , … , N, where N is the size of the dictionary. When we have
more atoms in the dictionary than signal samples (i.e., N ≫d), the dictionary is called
overcomplete.
The signal is represented as a linear combination of atoms in the dictionary as before,
y = 𝜱x. The goal in dictionary learning is to estimate both the signal x and the dictionary
𝜱with as few measurements as possible. The dictionary needs to be large enough (rep-
resentative, expressive), and hence one typically works with overcomplete dictionaries.
This poses a diﬃcult problem, since the atoms often become linearly dependent, which
gives rise to undetermined systems of equations and hence nonunique solutions for x.
It is here precisely when imposing sparsity in the solution alleviates the problem: we
essentially aim to ﬁnd a sparse linear expansion with an error e bounded to be lower
than a threshold parameter 𝜀. A possible formulation of the problem is
min
𝜱,x ‖x‖
(.)

46
Digital Signal Processing with Kernel Methods
subject to
y = 𝜱x + e and ‖e‖
< 𝜀,
(.)
where ‖ ⋅‖is the 𝓁-norm. This problem, however, is NP-hard and cannot be solved
exactly. Several approximations exist, either based on greedy algorithms, such as the
matching pursuit (Mallat and Zhang, ) or the orthogonal matching pursuit (Tropp,
), or convex relaxation methods such as the basis pursuit denoising (Chen et al.,
) or the least absolute shrinkage and selection operator (LASSO) (Tibshirani, ),
which solve
min
𝜱,x
{‖y −𝜱x‖
+ 𝜆‖x‖
} ,
(.)
where 𝜆is a trade-oﬀparameter controlling the sparseness versus the accuracy of the
solution. This convex relaxation replaced the nonconvex 𝓁-norm with the 𝓁-norm
that has surprisingly good properties.
There are alternative algorithms for solving this problem, and a good review can be
found in Tropp and Wright (). Tosic and Frossard () pointed out that three
main research directions have been followed in methods for dictionary learning: ()
probabilistic algorithms, such as those based on ML (Olshausen and Fieldt, ),
MAP (Kreutz-Delgado et al., ), and Kulback–Leibler divergences, as proxies to the
𝓁-norm (Bradley and Bagnell, ); () vector quantization algorithms, such as the
Schmid–Suageon method for video coding, or the kernel singular value decomposition
(SVD) algorithm (Aharon et al., ); () structure-preserving algorithms that encode
some form of prior knowledge during learning, which are found especially useful for
audio (Yaghoobi et al., ) and image applications (Olshausen et al., ).
On the other hand, supervised dictionary learning is a vast ﬁeld with many develop-
ments, and can be seen as another form of encoding explicit knowledge. The typical
formulations include additional terms in charge of encouraging class separability,
such as
min
𝜱,x
{‖y −𝜱x‖
+ 𝜆(𝜱, x)} ,
(.)
where (𝜱, x) measures class separability when data are represented by the dictionary, or
min
𝜱,x
{‖y −𝜱x‖
+ 𝜆‖x‖+ 𝜆(𝜱, x, 𝜽)} ,
(.)
where (𝜱, x, 𝜽) is a classiﬁcation term that depends on the dictionary and a particular
statistical classiﬁer (e.g., an SVM) parametrized with weights 𝜽.
Learning dictionaries and compact signal representations have been found very
useful in image and audio applications. Despite the great advances in the last decade,
some open problems still remain, such as learning in distorted, unevenly sensed and
noisy data, or, perhaps more importantly, learning dictionaries capable of dealing with
nonlinearities.

Introduction to Digital Signal Processing
47
Compressive Sensing
Sparse representations are the foundation of transform coding, and the core of many
technological developments like digital cameras. After acquisition of an image y, a
transform 𝜱is applied, and only the positions and values of the largest coeﬃcients of
x are encoded, stored, or transmitted. Image compression is simply this. Unfortunately,
this procedure is obviously suboptimal because: () even for a small value of m, the
number of sampling points may be large; () one has to compute all the n coeﬃcients
while most of them will be later discarded; and () encoding implies storing not
only the most important coeﬃcients but also their locations, which is obviously a
wasteful strategy. The brain does not work like that. Compressive sensing (CS) addresses
these ineﬃciencies by directly acquiring a compressed signal representation without
going through the intermediate stage of acquiring n samples (Candes and Wakin,
; Candes et al., ; Donoho, ). The main achievement of CS is that the
number of measurements needed is based on the sparsity of the signal, which permits
signal recovery from far fewer measurements than those required by the traditional
Shannon/Nyquist sampling theorem.
In CS we assume that a one-dimensional (D) discrete-time signal x ∈ℝd can be
expressed as a linear combination of basis functions 𝜙k, k = , … , d which, in matrix
notation, is given by x = 𝜳s. Here, the d×d dictionary matrix 𝜳is assumed orthonormal
and describes the domain where x accepts a sparse representation. Accordingly, we
assume that the signal x is k-sparse. Let us now consider a general linear measurement
process that computes dot products between acquisitions, and a second set of basis
vectors 𝜓j, j = , … , m, for m < d, that are collectively grouped in the matrix 𝜳.
Therefore, the m measurement vector can be expressed as
y = 𝜱x = 𝜱𝜳s = 𝜣s,
(.)
where 𝜣is an m×d matrix. Here, the measurement process is not adaptive; that is, 𝜱is
ﬁxed and does not depend on the signal x. The problem in compressed sensing involves
the design of: () a stable measurement matrix 𝜱such that the dimensionality reduction
from x ∈ℝd to y ∈ℝm does not hurt the k-sparse representation of the signal; and ()
a reconstruction algorithm to recover x from only m ≈k measurements of y. For the
ﬁrst step, one can achieve stability by simply selecting 𝜱as a random matrix.For the
second step, one may resort to the techniques presented before for dictionary learning.
Note that, here, the signal reconstruction algorithm must take the m measurements in
the vector y, the random measurement matrix 𝜱(or the random seed that generated
it), and the basis 𝜳, and reconstruct the signal x or, equivalently, its sparse coeﬃcient
vector s. The problem can be expressed as
min
s
‖s‖s.t. y = 𝜣s,
(.)
The restricted isometry property and a related condition called incoherence require that the rows of 𝜱
cannot sparsely represent the columns of 𝜳and vice versa. The selection of a random Gaussian matrix for 𝜱
has two nice properties: it is incoherent with the basis 𝜳= I of delta spikes, and it is universal in the sense
that 𝜣will be i.i.d. Gaussian, thus holding the restricted isometry property.

48
Digital Signal Processing with Kernel Methods
for which an equivalent optimization based on the 𝓁-norm is
min
s
{
‖y −𝜣s‖
+ 𝜆‖s‖
}
,
(.)
which typically yields good results and can recover k-sparse signals.
It is important to note that x needs not be stored and can be obtained anytime once s is
retrieved. Additionally, measurements y can be obtained directly from an analog signal
x(t), even before obtaining its sampled discrete version x. Intuitively, one can see CS
as a technique that tackles sampling and compression simultaneously. These properties
have been used to develop eﬃcient cameras and MRI systems that take advantage of
signal characteristics and smart sampling designs.
Finally, we should mention that, similar to dictionary learning, the k-sparse concept
in CS has also recently been extended to nonlinear k-sparse by means of kernel methods
(Gangeh et al. ; Gao et al. ; Nguyen et al. ; Qi and Hughes ). Natural
audio and images lie in particular low-dimensional manifolds and exhibit this sort of
nonlinear sparsity, which is not taken into account with previous linear algorithms.
2.3
Multidimensional Signals and Systems
Multidimensional signal processing refers to the application of DSP techniques to
signals that can be represented in a space of two or more dimensions. The most
straightforward examples of multidimensional signals are images and videos, but there
are many other signals generated from physical systems that can be described in high-
dimensional spaces. Surprisingly, classical DSP has not been naturally extended in a
natural way. In many applications the operations of mathematical morphology are
more useful than the convolution operations employed in signal processing because
the morphological operators are related directly to shape (Haralick et al., ). Other
types of techniques based in part on heuristic considerations like local scale-invariant
feature extraction appear as natural extensions of classical D signal-processing tech-
niques (Lowe, ).
When moving to multidimensional signal processing, a modern approach is taken
from the point of view of statistical (machine) learning, but we should not forget the
solid signal-processing concepts applied to this topic. Machine learning has also been
used for image and video processing, and actually dominates the particular ﬁeld of
computer vision. That said, classical DSP is also used in image processing by extending
the deﬁnitions and tools to two dimensions for many tasks, from object recognition to
denoising. In addition, multidimensional processing is not just exclusive to D imaging,
with it also having applications in other kinds of multidimensional signals (Dudgeon and
Merserau, ).
Medical imaging can be seen as a source of D or four-dimensional (D) signals.
For example, computer-aided tomography scan images are D images, while func-
tional MRI of the brain (Friston et al., ) is a D signal in nature. The processing
of such signals includes classical DSP techniques extended to the multidimensional
ﬁeld. Other ﬁelds where one can ﬁnd multidimensional signals include antenna array
processing (Van Trees, ). Planar arrays, consisting of a layer of radio sensors or
antennas placed in a D structure, are a source of multidimensional signals. In a basic
framework, a snapshot or sample of the excitation current produced by an incoming

Introduction to Digital Signal Processing
49
radioelectric wave constitutes itself a D signal susceptible to being processed by D
DSP in order to obtain information about the incoming wave. This information can be
its angle of arrival, the symbols carried by this signal in a communications application,
or even just the power of that signal in, for example, a radiometry application. Many
times, this process is carried out in the frequency domain of the signal.
When dealing with dispersive channels, the process turns out to be D and often
time–space processing strategies are used (Verdú, ). Sometimes, the application
is not targeted to the observation of a single or a few arbitrary discrete directions of
arrival, but it focuses on the reconstruction of radar images of surfaces. An example of
this is synthetic aperture array techniques (Van Trees, ), where using conventional
time–frequency DSP techniques, high-resolution radar images of remote areas can
be reconstructed. In summary, multidimensional DSP is a framework that inherits
concepts and elements from DSP in order to implement them in applications such as
image processing, radar, communications, or biomedical imaging, while still borrowing
others from machine learning and statistics.
2.3.1
Multidimensional Signals
Fundamental multidimensional DSP is constructed by the extension of the elements
of DSP to more than one dimension. The ﬁrst and more straightforward extension is
the deﬁnition of a D signal. A D signal or sequence can be represented as a function
ordered in pairs of integers:
x[n] = x[n, n].
(.)
The deﬁnition of special sequences follows straightforwardly. For example, one can say
that a sequence is separable if x[n, n] = x[n]x[n]. A sequence is periodic if it
satisﬁes the properties x[n+ N, n]x[n, n] and x[n, n+ N]x[n, n], with Nand
Nbeing the periods of the sequence. In general, a multidimensional sequence can be
expressed as x[n], where n = [n, … , nK]T.
We know that a fundamental operator in DSP is convolution. The convolution
operator can be deﬁned in a multidimensional space as well, and appears recurrently in
machine learning in general and kernel methods in particular, as we will see in the book.
Given two multidimensional sequences x[n] and x[n], its convolution is expressed as
x[n] ∗x[n] =
∑
k
x[k]x[n −k],
(.)
where ∑
k = ∑∞
k=−∞⋯∑∞
kK=−∞.
Multidimensional Sampling
Multidimensional signals can be uniformly sampled in an arbitrary way, depending on
the directions of the sampling. Usually, images are signals sampled in a rectangular
pattern. In a D space, positions tu, tv of the samples can be represented with the
following pair of dot products:
tu = uT[n, ]T = dn,
tv = vT[, n]T = dn,
(.)

50
Digital Signal Processing with Kernel Methods
where u = [d, ]T and v = [, d]T are a vector basis and d is the sampling distance.
A more general type of sampling can be performed by using an arbitrary, non-
necessarily orthogonal vector basis u = [u, u]T and v = [v, v]T. Then, the expression
becomes
tu = uT[n, n]T,
tv = vT[n, n]T.
(.)
This sampling procedure can be obviously extended to any number of dimensions.
DFT of a Multidimensional Signal
The Fourier transform of a multidimensional signal is, as its unidimensional coun-
terpart, a projection of the signal onto a basis of complex sinusoid functions. In
order to ﬁnd an expression for a multidimensional sequence, it is only necessary to
follow the same procedure as for unidimensional signals. Consider that x[n] is a linear
multidimensional impulse response of a linear multidimensional system. Since the
complex exponential signals are the eigenfunctions of linear systems, the convolution of
such a signal with any linear system will produce a new complex sinusoid with the same
frequency, but with diﬀerent amplitude and phase. The behavior of the amplitude and
phase with respect to the frequency is equivalent to the FT. Since the impulse response
fully characterizes the system, so will its FT. A complex multidimensional sinusoid can
be expressed as
v(𝜴) = v(Ωn, … , ΩKnK) = exp
(
𝕚
∑
k
Ωknk
)
= exp(𝕚𝛀Tn)T,
(.)
where discrete frequencies can be expressed as Ωk = 𝜋∕Nk, with Nk being each one of
the periods of the multidimensional signal. The convolution of both sequences is
x[n] ∗exp(𝕚𝜴Tn) =
∑
k
exp(𝕚(𝜴Tn −k))x[k],
= exp(𝕚𝜴Tn)
∑
k
exp(−𝕚𝜴Tk)x[k].
(.)
The term exp(𝕚𝜴Tn) in the right side of Equation .shows that the output is indeed a
complex sinusoid with the same frequency as the input. The second term is the complex
amplitude response of the system, and equals its FT, which is simply the dot product
between the multidimensional sequence and the exponential sequence. Therefore:
(x[n]) = X(𝜴) =
∑
k
exp(−𝕚𝜴Tk)x[k].
(.)
The DFT has eﬃcient ways of computation based on the well-known butterﬂy algorithm
for eﬃcient computation of the unidimensional DFT.

Introduction to Digital Signal Processing
51
2.3.2
Multidimensional Systems
FIR ﬁlters are widely used in multidimensional DSP because they are relatively easy to
design and, as their unidimensional counterparts, they are inherently stable (Hertz and
Zeheb, ). Nevertheless, multidimensional FIR ﬁlter design involves a much higher
computational burden and design constraints that need to be taken into account in their
operation and design. Since the FIR can always be designed, the most straightforward
way to use an FIR ﬁlter is the direct convolution of Equation .of this response with
the signal to be processed. That is, if h[n] and x[n] are an FIR and a signal, the ﬁltering
operation is simply
y[n] =
∑
k
x[k]h[n −k].
(.)
Nevertheless, if the ﬁlter lengths in each dimension are N, N… , NK, each output
sample needs (N⋅N⋯K) multiplications and additions, though this number
can be further reduced when the ﬁlter presents symmetries. A reduced computational
burden can be achieved if the FT of the ﬁlter is used for the output computation. In
that case, provided that the dimensions of the input signal are M, M, … , MK, the
computational burden per sample is 𝛱kNk(+ logNk)∕𝛱Nk. This is advantageous
when the ﬁlter order is high, but the whole signal needs to be stored, which can challenge
the capability of certain processing structures. Other strategies, such as the block con-
volution, allow processing the signal in smaller (eventually disjoint) blocks, thus saving
internal memory. This can turn an infeasible ﬁlter operation into an aﬀordable one.
Regarding the design of multidimensional FIR ﬁlters, the use of standard windows
of low-pass ﬁltering (or other forms) is widely extended in image processing. In
certain situations, ad hoc ﬁlters need to be designed for speciﬁc applications. Then,
the MMSE criterion can be applied. In addition, approximation theory extended to the
multidimensional domain is often used (Bose and Basu, ).
A more compact, and hence less computationally heavy, approach consists of
using IIR ﬁlters for multidimensional signal processing (Gorinevsky and Boyd, ;
Madanayake et al., , ). Here, the nature of the impulse response makes
direct or indirect use of a recursive structure mandatory for the process. Basically, the
implementation of an IIR ﬁlter translates into the implementation of a multidimensional
ﬁnite diﬀerence equation of the form
∑
m,…,mK
am…mKy[n−m… nk −mk] =
∑
p−,…,pK
bp…pKx[n−p… nK −pk].
Note that the output is multidimensional. The recursive computation of this diﬀer-
ence equation is possible by means of just four multidimensional matrices containing
coeﬃcients a and b and for input and output sequences. Nevertheless, it is important
to notice that, in order to compute an output, the rest of them must be known. This
implies that some samples must be computed before others. If a computation order
cannot be established, the ﬁlter cannot be implemented recursively. As for FIR ﬁlters,
implementations in the frequency domain exist for IIR ﬁlters through the use of a
multidimensional extension of the z-transform, which in turn allows us to study stability
as well.

52
Digital Signal Processing with Kernel Methods
2.4
Spectral Analysis on Manifolds
An interesting branch of multidimensional signals and systems analysis, diﬀerent from
the previously described approaches, is that of spectral analysis on manifolds. The
previously described discrete Fourier analysis was deﬁned to transform uniformly
sampled signals from the temporal domain to the frequency domain. However, this
transformation cannot be directly applied to D discrete triangle meshes because
they are irregularly sampled; that is, they have diﬀerent resolutions at diﬀerent places
depending on the required details in each place. For that reason, a large number of
spectral mesh processing methods have been proposed for a variety of computer graph-
ics applications, such as clustering, mesh parameterization, mesh compression, mesh
smoothing, mesh segmentation, remeshing, surfaces reconstruction, texture mapping,
or watermarking, among others (Sorkine, ; Zhang et al., ). In this section we
provide an introduction to these kinds of techniques. Some background on graph theory
is assumed. The interested reader will ﬁnd other kernel approaches based on graphs in
Chapters , , and .
2.4.1
Theoretical Fundamentals
A mesh is a piecewise-linear representation of a smooth surface with arbitrary topology,
and it is deﬁned by a set of vertices V, edges E, and faces F (see Figure .). Each vertex
vi ∈V has an associated position in the D space vi = [xi, yi, zi], and each edge and face
connect two and three vertices, respectively. The vertex positions and the faces capture
the geometry of the surface, and the connectivity between vertices (edges) captures the
topology of the surface. Here, we consider a mesh as a manifold (-manifold embedded
in a D space), which obeys the following two rules: () every edge is adjacent to one or
V = 
F = 
0
0
0
0
0
0
0
0
0
0
0
0
1.62
1.62
1.62
–1.62
–1.62
–1.62
–1.62
1.62
1.62
–1.62
–1.62
1.00
1.00
1.00
1.00
1.00
1.00
2
2
1
1
3
3
3
4
4
4
5
5
5
5
5
6
6
6
6
6
7
7
7
7
7
8
8
8
8
8
9
9
9
9
9
10
10
10
10
10
11
11
11
11
11
12
12
12
12
12
1
1
1
2
2
2
3
3
4
4
1.00
–1.00
–1.00
–1.00
–1.00
–1.00
–1.00
(a)
(b)
10
7
1
2
11
5
9
3
6
Figure 2.1 Icosahedron mesh (a) and its vertex and face matrices (b).

Introduction to Digital Signal Processing
53
two faces; and () the faces around every vertex form a closed or open fan. In addition,
the manifold mesh fulﬁlls the Euler–Poincaré formula for closed polygonal meshes:
V −E + F = 𝜒, where 𝜒= (−G) and G is the genus of the surface (i.e., the number
of holes).
Current developments in spectral mesh processing are based on the work published
by (Taubin, ), which showed that the well-known FT of a signal can be seen
as the decomposition of this signal into a linear combination of Laplacian operator
eigenvectors. Hence, most of the spectral methods for mesh processing deﬁne the most
appropriate Laplacian operator for their spectral application (Sorkine, ; Zhang
et al., ).
The Laplacian operator of a twice-diﬀerentiable real-valued function f is deﬁned by
the divergence of its gradient as
Δ f = div(grad f ) = ∇⋅∇f .
(.)
When functions are deﬁned over a Riemannian or pseudo-Riemannian manifold
M with metric g – for example, the coordinate function given by the vertex position
(xi, yi, zi) – the Laplace operator is known as the Laplace–Beltrami operator and is
deﬁned as
ΔM f = divM(gradM f ).
(.)
The deﬁnition of the Laplacian can also be extended to k-forms, and then it is called the
Laplace–de Rham operator (Rosenberg, ). This operator is deﬁned as Δ = 𝛿d + d𝛿,
where d is the exterior derivative on k-forms and 𝛿is the codiﬀerential, acting on k −-
forms. In the Laplace–de Rham operator for functions on a manifold (-forms) it can be
seen that to d𝛿vanishes, and the result Δ = 𝛿d is the actual deﬁnition of the Laplace–
Beltrami operator.
In general, the procedure for mesh spectral analysis follows the following three stages:
() compute a Laplacian matrix, normally a sparse matrix; () compute the eigende-
composition of this Laplacian matrix (i.e., eigenvalues and eigenvectors); () process
these eigenvectors and eigenvalues for the speciﬁc purpose of the application. A wide
range of Laplacian operators, namely combinational graph Laplacian and geometric
mesh Laplacian, have been proposed with the aim of ﬁnding a Laplacian operator
which fulﬁlls all the desired properties in a discrete Laplacian, such as symmetry,
orthogonal eigenvectors, positive semi-deﬁniteness, or zero row sum. Readers are
referred to Wardetzky et al. () for a detailed explanation of these properties and
their compliance in diﬀerent Laplace operators. The combinational graph Laplacians
only take into account the graph of the mesh (vertices and connectivity), whereas the
geometry information is not included explicitly. Hence, two diﬀerent embeddings of
the same graph yield the same eigenfunctions, and two diﬀerent meshings of the same
object yield diﬀerent eigenfunctions (Vallet and Lévy, ).
While the graph Laplacian can be seen as a discrete analogue of the Laplace–Beltrami
operator, the geometric mesh Laplacian is the direct discretization of the Laplace–
Beltrami operator, which includes both the topology and geometry information about
the mesh (face angles and edge lengths). Dyer et al. () showed a higher robustness
to changes in sampling density and connectivity of the geometric Laplacian than the

54
Digital Signal Processing with Kernel Methods
graph Laplacian. For both Laplacian operators, the Laplacian matrix can be written in
a local form as
𝚫f (vi) = 
bi
∑
vj∈N(vi)
wij(f (vi) −f (vj)),
(.)
where f (vi) is the value of the function f in the position of the vertex vi, N(vi) is the
one-ring ﬁrst-order neighbor of vi, wij is the symmetric weight (wij = wji), and bi is a
positive value.
Once the Laplacian 𝚫is deﬁned, its eigendecomposition on a manifold satisﬁes
−𝚫Hk = 𝜆kHk
(.)
where Hk are the eigenvectors, which deﬁne a basis for functions over the surface;
and 𝜆k are the eigenvalues, which represent the mesh spectrum (frequencies). Note
that the negative sign is required to have positive eigenvalues. The basis functions are
ordered according to the increasing eigenvalues; hence, lower eigenvectors correspond
to a basis function associated with lower frequencies. Applying this concept to Fourier
analysis, we can prove that the complex exponential of the DFT, e𝕚Ωx, is an eigenfunction
(orthonormal basis) of the Laplace operator:
Δe𝕚Ωx = −d
dxe𝕚Ωx = (Ω)e𝕚Ωx
(.)
Note that the set of basis functions in the DFT are ﬁxed; meanwhile, the eigenvectors of
the Laplacian operator could change based on the mesh geometry and connectivity. As
in Fourier analysis, the spectral coeﬃcients can be obtained by projecting the vertices
on the Hk basis as
ak =
n
∑
i=
viHk
i
(.)
where n is the number of vertices, and the reconstruction of the original mesh can be
obtained as
̂vi =
m
∑
k=
akHk
i
(.)
where m is the number of coeﬃcients used for reconstruction. As lower eigenvalues
(and their associated eigenvectors) correspond to the general shape of the mesh (lower
frequencies), a low-pass-ﬁltered shape of the mesh is obtained for m ≪n. The original
mesh is reconstructed when m = n.
2.4.2
Laplacian Matrices
Among the wide variety of combinational graph Laplacians and geometric graph
Laplacians, we present here a representative set of examples. For a more detailed survey

Introduction to Digital Signal Processing
55
of combinational and geometric Laplacians, readers are referred to Sorkine () and
Zhang et al. (). The three well-known combinational mesh Laplacians are the graph
Laplacian (also known as Kirchoﬀoperator), the Tutte Laplacian, and the normalized
graph Laplacian (also known as the graph Laplacian).
The graph Laplacian matrix L is obtained from the adjacent matrix W and the degree
matrix D as L = D −W. Both W and D are n × n matrices deﬁned as
W ij =
{

if (i, j) ∈E

otherwise
(.)
Dij =
{
|N(vi)|
if i = j

otherwise
(.)
where |N(vi)| is the number of ﬁrst-order neighbors of vi. Here, bi = N(vi), and wij = 
in Equation ..
The Tutte Laplacian T is deﬁned as T = D−L = I −D−W and it was used by Taubin
() for surface smoothing (as a linear low-pass ﬁlter of the natural frequencies of the
surface). In Equation ., bi = −∕N(vi), and wij = for the Tutte Laplacian. Given that
T is not a symmetric matrix, several variations of this Laplacian have been proposed in
order to symmetrize T, such as T∗= (T + T′)∕(Lévy, ) or T∗∗= T′ ⋅T (Zhang
et al., ).
Finally, the normalized graph Laplacian is deﬁned as Q = D−∕LD−∕, where bi =
−∕(N(vi)N(vj))∕, and wij = in Equation .. Q and T have the same spectrum due
to T = D−∕QD−∕, and although Q is not a Laplacian in Equation ., its symmetry
can help to calculate the eigenvectors of T.
The geometric mesh Laplacians use cotangent weights in Equation .to compute
the Laplacian:
wij =
cot(𝛽ij) + cot(𝛼ij)

,
(.)
where bi = and 𝛽ij and 𝛼ij are the opposite angles to the edge between vi and vj, as is
shown in Figure .. Pinkall and Polthier () presented this ﬁrst cotangent scheme
for discrete minimal surfaces. In a diﬀerent way, Meyer et al. () also obtained the
same cotangent weights but the factor bi = |Ai|, where |Ai| is the area of the barycell
Ai which is formed by connecting the barycenters of the triangles adjacent to vi with
the midpoints of the edges incident to the triangles. Given that the cotangent can give
problems when the angle is close to π radians, Floater () proposed an alternative
scheme called mean-value coordinates and deﬁned as
wij =
tan(𝜃ij) + tan(𝛾ij)
||vi −vj||
,
(.)
where 𝜃ij and 𝛾ij are the angles shown in Figure .. The ﬁnite-element method also pro-
vides a new formulation of the geometric Laplacian by redeﬁning Equation .as (Val-
let and Lévy, )

56
Digital Signal Processing with Kernel Methods
αij
γij
θij
νi
νj
βij
Figure 2.2 Angles in the cotangent weights.
−Qhk = 𝜆kQhk
(.)
where hk = [Hk
, Hk
, … , Hk
n]; Q is the stiﬀness matrix and B is the mass matrix, deﬁned
as follows:
Qij =
⎧
⎪
⎨
⎪⎩
cot(𝛽ij)+cot(𝛼ij)

if (i, j) ∈E
−∑
j Qij
if i = j

otherwise
(.)
Bij =
⎧
⎪
⎨
⎪⎩
|t𝛼|+|t𝛽|

if i ≠j
∑
j∈N(vi) |tj|

if i = j

otherwise
(.)
where |t𝛼| and |t𝛽| are the areas of the faces which share the edge (i, j), and |tj| is the area
of jth one-ring ﬁrst-order neighbor of vi. The previous formulation can be simpliﬁed as
−D−Qhk = 𝜆khk, where D is a diagonal matrix deﬁned as
Dii =
∑
j
Bij =
∑
j∈N(vi) |tj|

.
(.)
Finally, the most recent geometric Laplacian deﬁnition has been derived by using
discrete exterior calculus, a new language of diﬀerential geometry and mathematical
physics (Desbrun et al., ; Grinspun et al., ). Vallet and Lévy () proposed

Introduction to Digital Signal Processing
57
a symmetrized Laplace–de Rham operator for mesh ﬁltering by using discrete exterior
calculus:
𝚫ij =
⎧
⎪
⎪
⎨
⎪
⎪⎩
−
cot(𝛽ij)+cot(𝛼ij)
√
|vi||vj|
if i ≠j
−∑
k∈N(vi) 𝚫ik
if i = j

otherwise
(.)
where |vi| is the area of the Voronoi region of the vertex vi in its one-ring neighbor.
2.5
Tutorials and Application Examples
In this section we present a selected set of illustrative examples to review the basic
concepts of signal processing and standard techniques. In particular, we will introduce:
) Examples of real and complex signals along with their basic graphical representa-
tions.
) Basic concepts on convolution, FT, and spectrum for discrete-time signals.
) Continuous-time signals concepts which are highlighted by using symbolic repre-
sentations, and a simple case study analyzed for the reconstruction via Fourier series
of a synthetic squared periodic wave.
) Filtering concepts by using some simple synthetic signals and one real cardiac
recording.
) Nonparametric and parametric spectrum estimation, which will be compared for
real sinusoidal signals buried in noise, for highlighting some of the advantages and
drawbacks of each method in practice.
) Standard source separation techniques and wavelet analysis, which will be brieﬂy
presented in a couple of case studies.
) Finally, spectral analysis on meshes by using manifold harmonic analysis is studied
with a toy example application with known solution to highlight the main elements
of this kind of harmonic analysis in a multidimensional and nonconventional subse-
quent application for cardiac meshes analysis.
2.5.1
Real and Complex Signal Processing and Representations
Complex signals are a convenient representation in signal processing and communi-
cations, where the information is transmitted using both the amplitude and the phase
of the signal. In this ﬁrst example, a Hanning pulse is convolved with a train of delta
functions with positive or negative values representing one- or zero-valued bits. The
ﬁrst panel of Figure .shows the pulse represented with samples, where its FT
(which is real) is represented in the second panel. The third panel represents the pulse
modulation, which is a train of three pulses carrying bits , , and . The FT of this
train of pulses is seen in the fourth panel of Figure .. Since the train of pulses is a real
signal, its FT has a symmetric real part and an antisymmetric imaginary part. The code
generating this ﬁgure is in Listing .. Note that the seed initialization of the ﬁrst line
can be changed to generate other random realizations.

58
Digital Signal Processing with Kernel Methods
20
40
60
80
100
120
n
0
0.5
1
Hamming pulse
–0.1
–0.08
–0.06
–0.04
–0.02
0
0.02
0.04
0.06
0.08
0.1
–40
–20
0
20
40
60
Hamming pulse DFT
50
100
150
200
250
300
350
n
–0.5
0
0.5
Signal
–200
–100
0
100
200
DFT, real part
–200
–100
0
100
200
DFT, imag. part
–0.1
0.1
0
–0.05
0.05
–0.1
0.1
0
–0.05
0.05
Ω (rad)
Ω (rad)
Ω (rad)
Figure 2.3 Example of representation of real signals. The first panel shows a Hanning pulse. The
second panel shows the discrete-time FT of the pulse. In the third panel, a binary modulation is
shown, consisting of a train of pulses multiplied by +1 or −1 to represent bits 1 and 0. The fourth
panel shows the real (left) and imaginary (right) parts of the FT of the signal in the third panel. Since
this signal is real, its transform is Hermitian.
randn('seed',2)
x = hanning(128);
figure(1), subplot(411), plot(x)
axis([1 128,-0.1,1.1]), xlabel('n'), title('Hamming pulse')
subplot(412)
plot(linspace(-pi,pi,1024), real(fftshift(fft(x,1024))))
axis([-0.1,0.1,-40,75]), xlabel('\Omega (rad)'), title('Hamming pulse ...
DFT')
s = sign(randn(1,3));
y = kron(s',x);
subplot(413), plot(y), xlabel('n'), title('Signal'), axis tight

Introduction to Digital Signal Processing
59
subplot(427),
plot(linspace(-pi,pi,65535), real(fftshift(fft(y,65535))))
axis([-0.1,0.1,-200,200]), xlabel('\Omega (rad)'), title('DFT, real part')
subplot(428)
plot(linspace(-pi,pi,65535), imag(fftshift(fft(y,65535))))
axis([-0.1,0.1,-200,200]), xlabel('\Omega (rad)')
Listing 2.1 DFT example: real and complex counterparts (dft.m).
This signal is modulated in signal z[n], as seen in Figure ., ﬁrst panel. Here, the pulse
train y[n] is multiplied by a sinusoid cos(Ωn) = .exp(𝕚Ωn) + .𝕚exp(𝕚Ωn). This
panel shows the modulated sinusoid, where it can be clearly seen that the information
of the symbols is carried in the phase of the signal. Indeed, the third pulse is shifted π
radians with respect to the ﬁrst and second ones.
Multiplying a signal in time by a complex exponential e𝕚Ωn is equivalent to shifting
its FT in Ωradians. Since the sinusoid is composed of two complex exponentials of
positive and negative sign, the modulation appears in frequency as a pair of signals
shifted to the positive and negative parts of the frequency axis (second panel). The
spectra are centered around ±Ω. Note that both the pulse train and the modulation
are Hermitian with respect to the origin and with respect to ±Ω.
Such modulations are called double side-band modulations, since both the subbands
at the right and left sides of ±Ωcarry the same information. This property is exploited
to recover the modulating pulse. Both sides of the spectrum contain a signal which is
simply a shifted version of the original spectrum. In order to recover the signal, we only
need to remove the negative part and shift the positive part Ω radians to the left.
Signal Z(𝕚Ω) in the second panel of Figure .can be analytically represented as
Z(𝕚Ω) = 
Y(𝕚Ω −𝕚Ω) + 
Y(𝕚Ω + 𝕚Ω).
(.)
Hence, signal ̂Z(𝕚Ω) of the third panel is simply ̂Z(𝕚Ω) = Z(𝕚Ω −𝕚Ω), which in time is
equal to the complex signal ̂z[n] = exp(𝕚Ωn). In order to recover the original signal, we
simply multiply this complex signal by a complex exponential of frequency −Ω. The FT
and time versions of the recovered signal are shown in the ﬁfth panel of of Figure ..
The corresponding code of the experiment is in Listing .. Note in particular that the
modulation is carried out by using a carrier of π∕rad/s.
figure(2)
p = cos(2*pi/16.*(0:length(y)-1)); % Carrier
z = p'.*y;
subplot(511), plot(z)
axis tight, xlabel('n'), title('DSB modulation')
subplot(523)
plot(linspace(-pi,pi,65536),real(fftshift(fft(z,65536))))
axis([-0.8 0.8 -100 100]), xlabel('\Omega (rad)'), title('DFT, real part')
subplot(524)
plot(linspace(-pi,pi,65536),imag(fftshift(fft(z,65536))))
axis([-0.8 0.8 -100 100]), xlabel('\Omega (rad)'), title('DFT, imag. ...
part')
z2 = z;
Z2 = fftshift(fft(z,65536));
Z2(1:end/2) = 0;

60
Digital Signal Processing with Kernel Methods
subplot(525), plot(linspace(-pi,pi,65536),real(Z2))
axis([-0.8 0.8 -100 100])
xlabel('\Omega (rad)'), ylabel('Negative part removal')
subplot(526), plot(linspace(-pi,pi,65536),imag(Z2))
axis([-0.8 0.8 -100 100])
xlabel('\Omega (rad)'), ylabel('Negative part removal')
fc = 6553;
Z3 = [2*Z2(4097:end) ; zeros(4096,1)];
subplot(527), plot(linspace(-pi,pi,65536),real(Z3))
axis([-0.1 0.1 -200 200])
xlabel('\Omega (rad)'), ylabel('Frequency shift')
subplot(5,2,8), plot(linspace(-pi,pi,65536),imag(Z3))
axis([-0.1 0.1 -200 200])
xlabel('\Omega (rad)'), ylabel('Frequency shift')
subplot(515), plot(real(ifft(fftshift(Z3))))
axis([1 length(y) -1 1]), title('Recovered signal'), xlabel('n')
Listing 2.2 DFT example: real and complex counterparts (dsb.m).
This example modulates a signal in a double side band, which carries redundancy in
its spectrum, in exchange for very inexpensive forms of demodulation. Nevertheless,
it is possible to process the signal using just half the bandwidth since, as said before,
both sides of the spectrum around the central frequency Ω carry the same information.
Then, it is possible to just transmit one of the sides of the spectrum and then reconstruct
the whole spectrum in the receiver by forming the symmetric and antisymmetric
counterparts parts of the real and imaginary components of the transmitted signal.
This is depicted in Figures .and .. The second panel of Figure .shows the
FT of the signal where the negative part has been removed, since it carries exactly the
same information as the positive one. The idea is to transmit this signal, which requires
only half of the bandwidth. This signal is complex, since the FT is neither symmetric
nor antisymmetric, and the real part of this signal in time can be readily shown to be
identical to the original signal. Real component yR[n] and imaginary component yI[n]
are presented in the third and fourth panels of the ﬁgure. The reader can use the code
in Listing .to reproduce this part of the experiment.
figure(3)
y = kron(s',x);
subplot(411), plot(y), title('Signal'), xlabel('n'), axis tight
Y = fftshift(fft(y,65535));
Y(1:end/2) = 0;
subplot(423), plot(linspace(-pi,pi,65535),real(Y))
axis([-0.1,0.1,-200,200])
ylabel('Negative spectrum removal'), xlabel('\Omega (rad )')
subplot(424), plot(linspace(-pi,pi,65535),imag(Y))
axis([-0.1,0.1,-200,200])
xlabel('\Omega (rad)'), ylabel('Negative spectrum removal')
yssb = ifft(fftshift(Y));
yssb = yssb(1:length(y));
subplot(413), plot(real(yssb)), title('Baseband SSB, real part')
xlabel('n'), axis tight
subplot(414), plot(imag(yssb))
axis tight, title('Baseband SSB, imag. part'), xlabel('n')
Listing 2.3 DFT example: transmitting just one side of the spectrum (ssb.m).

Introduction to Digital Signal Processing
61
50
100
150
200
250
300
350
n
–0.5
0
0.5
DSB modulation
–0.5
0
0.5
–100
0
100
DFT, real part
–0.5
0
0.5
–100
0
100
DFT, imag. part
–0.5
0
0.5
–100
0
100
Negative part removal
–0.5
0
0.5
–100
0
100
Negative part removal
–0.1
–0.05
0
0.05
0.1
–200
0
200
Frequency shift
–0.1
–0.05
0
0.05
0.1
–200
0
200
Frequency shift
50
100
150
250
200
300
350
–1
0
1
Recovered signal
n
Ω (rad)
Ω (rad)
Ω (rad)
Ω (rad)
Ω (rad)
Ω (rad)
Figure 2.4 Example of modulation and demodulation of the signal of Figure 2.3, panel 3. The first
panel shows the modulation of the signal using a sinusoid function. The corresponding FT can be
seen in the second panel. Since the signal is real, its FT is Hermitian. In the third panel, the negative
components of the frequency-domain panel show the real and imaginary parts of the FT of the signal
are removed, resulting in a complex version of the signal in time. The pulse train is recovered by
simply shifting the signal in time toward the origin, which produces a Hermitian FT, and hence a real
signal in time (fourth and fifth panels).
Figure .shows the modulated signal. The modulation process consists of embed-
ding both the real and imaginary parts of the spectrum in a real signal. To this end, we
simply construct a signal symmetric in the frequency domain with the FT of yR[n] and
an antisymmetric one with the FT of yI[n], as shown in the second panel of the ﬁgure.
These will constitute the real and imaginary parts of the Hermitian FT of the modulated
signal, which is real. This is equivalent to constructing a modulated signal of the form
z[n] = yR[n] cos(Ωn) −yI[n] sin(Ωn),
(.)

62
Digital Signal Processing with Kernel Methods
50
100
150
200
250
300
350
n
–0.5
0
0.5
Signal
–0.1
–0.05
0
0.05
0.1
–200
–100
0
100
200
Negative
spectrum removal
–0.1
–0.05
0
0.05
0.1
–200
–100
0
100
200
Negative
spectrum removal
50
100
150
200
250
300
350
n
–0.4
–0.2
0
0.2
0.4
Baseband SSB, real part
50
100
150
200
250
300
350
n
–0.2
0
0.2
0.4
Baseband SSB, imag. part
Ω(rad)
Ω(rad)
Figure 2.5 Example of single side-band modulation and demodulation of the signal of Figure 2.3,
third panel. The original signal is in the first panel. Subsequent panels show the FT of the signal with
the negative part removed. This results in a complex signal whose real part is identical to the original
one (third panel), but which has an imaginary part (shown in fourth panel).
whose proof is proposed as an exercise. The demodulation is performed in an identical
way as in the modulation of Figure ., since the real part of the single side-band signal,
once shifted back to the origin (Figure ., third panel), is identical to the original pulse
train. Finally, the part of the experiment depicted in Figure .can be reproduced with
the code in Listing ..
figure(4)
p = cos(2*pi*4096/65536.*(0:length(y)-1));
q = sin(2*pi*4096/65536.*(0:length(y)-1));
z = p'.*real(yssb)-q'.*imag(yssb);
subplot(511), plot(z), title('Modlulated SSB signal'), axis tight

Introduction to Digital Signal Processing
63
subplot(523), plot(linspace(-pi,pi,65536),real(fftshift(fft(z,65536))))
axis([-0.8 0.8 -100 100]), xlabel('\Omega (rad)'), title('DFT, real part')
subplot(524), plot(linspace(-pi,pi,65536),imag(fftshift(fft(z,65536))))
axis([-0.8 0.8 -100 100]), xlabel('\Omega (rad)'), title('DFT, imag. ...
part')
z2 = z;
Z2 = fftshift(fft(z,65536));
Z2(1:end/2+1) = 0;
subplot(525), plot(linspace(-pi,pi,65536),real(Z2))
axis([-0.8 0.8 -100 100])
ylabel('Negative spectrum removal'), xlabel('\Omega (rad )')
subplot(526), plot(linspace(-pi,pi,65536),imag(Z2))
axis([-0.8 0.8 -100 100])
ylabel('Negative spectrum removal'), xlabel('\Omega (rad )')
fc = 6553;
Z3 = [2*Z2(4097:end);zeros(4096,1)];
subplot(527), plot(linspace(-pi,pi,65536),real(Z3))
axis([-0.1 0.1 -200 200])
ylabel('Frequency shift'), xlabel('\Omega (rad )')
subplot(5,2,8), plot(linspace(-pi,pi,65536),imag(Z3))
axis([-0.1 0.1 -200 200])
ylabel('Frequency shift'), xlabel('\Omega (rad )')
subplot(515), plot(real(ifft(fftshift(2*Z3)))), axis([1 length(y) -1 1])
title('Recovered signal, real part'), xlabel('n')
Listing 2.4 DFT example: modulated signal (ssb2.m).
2.5.2
Convolution, Fourier Transform, and Spectrum
Let x[n] be a discrete time signal with samples deﬁned between n = and n = as
follows:
x[n] =
{
n∕
if ≤n ≤

otherwise
(.)
and h[n] the impulse response of a given discrete time signal, with the form of a pulse
with samples length:
h[n] =
{

if ≤n ≤

otherwise
(.)
The response y[n] of the system h[n] to the signal x[n] can be computed as the
convolution of both signals as in Equation .:
y[n] =
∞
∑
k=−∞
h[k]x[n −k]
(.)
By inspection, it can be seen that there are ﬁve diﬀerent cases of this equation (see
Figure .). The ﬁrst one is when n ≤. In this case, if k ≥, then x[n −k] = , and
for k ≤then h[k] = , so the convolution is zero for this interval of n, as shown in

64
Digital Signal Processing with Kernel Methods
50
100
150
200
250
300
350
–0.5
0
0.5
Modulated SSB signal
–0.5
0
0.5
–100
0
100
DFT, real part
–0.5
0
0.5
–100
0
100
DFT, imag. part
–0.5
0
0.5
–100
0
100
Negative spectrum
removal
–0.5
0
0.5
–100
0
100
Negative spectrum
removal
–0.1
–0.05
0
0.05
0.1
–200
0
200
Frequency shift
–0.1
–0.05
0
0.05
0.1
–200
0
200
Frequency shift
50
100
150
200
250
300
350
n
–1
0
1
Recovered signal, real part
Ω (rad)
Ω (rad)
Ω (rad)
Ω (rad)
Ω (rad)
Ω (rad)
Figure 2.6 Example of modulation and demodulation of the signal of Figure 2.3, panel 3. The first
panel shows the modulation, which contains a shifted version of one side of the base-band frequency
components of the pulse train (second panel). The demodulation consists of discarding the negative
frequency components (third panel) plus a frequency shifting back to the origin (fourth panel). The
real part of the signal is identical to the original pulse train.
the lower panel of the ﬁgure. The second case corresponds to the interval ≤n ≤.
In this case, x[n −k] is nonzero for ≤k < n and the convolution corresponds to the
integral of a triangle with base equal to n and height equal to n∕, and whose value is
given by n∕. The third case corresponds to the interval ≤n ≤. There, the
convolution for all values of n is equal to the area of a triangle of unit height and base
, which is . The fourth for n between and corresponds to the diﬀerence
between a triangle of area and the triangle deﬁned between k = and k = n.
This triangle has a base equal to n −, and a height equal to (n −)∕. Then the

Introduction to Digital Signal Processing
65
–50
0
50
100
150
200
250
k
0
0.5
1
h [k]
–50
0
50
100
150
200
250
k
0
0.5
1
x [n −k]
n= 0
n = 50
n = 100
n = 150
n=200
n=250
–50
0
50
100
150
200
250
n
0
50
x [n] * h [n]
Figure 2.7 Convolution of signal x[n] with system h[n]. The impulse response of the system h[k] is
shown in the upper panel. The middle panel shows x[n −k] for different values of n. The lower panel
shows the result of the convolution. The circles show the values of the convolution for the values of n
shown in the middle panel.
convolution is −[(n −)∕]. The ﬁfth case corresponds to k ≤, where the
convolution is zero.
The convolution can be written as
y[n] =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪⎩

if n ≤
n

if ≤n ≤

if ≤n ≤
−(n−)

if ≤n ≤

otherwise.
(.)
The corresponding MATLAB code is given in Listing ..
h = [zeros(1,100), ones(1,150) zeros(1,100)]; % h[n]
x = [1:100]/100; % x[n]
xa = [fliplr(x),zeros(1,250)];
% x[-n]
xb = [zeros(1,50),fliplr(x),zeros(1,200)]; % x[50-n]
xc = [zeros(1,100),fliplr(x),zeros(1,150)];% x[100-n]
xd = [zeros(1,150),fliplr(x),zeros(1,100)];% x[150-n]
xe = [zeros(1,200),fliplr(x),zeros(1,50)]; % x[200-n]
xf = [zeros(1,250),fliplr(x)];
% x[250-n]
% Represent impulse response
figure(1), subplot(311), plot([-99:250],h)
% Represent x[t-n]
subplot(312), plot([-99:250],xa), hold all
plot([-99:250],xb), plot([-99:250],xc)
plot([-99:250],xd), plot([-99:250],xe)

66
Digital Signal Processing with Kernel Methods
plot([-99:250],xf)
% Convolution
y = [zeros(1,99) conv(h,x,'valid')];
% Represent convolution
subplot(313), plot([-99:250],y)
hold all, stem(0:50:250,y(100:50:end))
Listing 2.5 Convolution example (convolution.m).
Consider now a pulse x[n] of length N + centered around the origin. Its FT can be
computed as in Equation ., where the domain is −N ≤n ≤N; that is:
X(𝕚Ω) = {x[n]} =
N
∑
n=−N
e−𝕚Ωn.
(.)
The series can be split into the positive and the negative parts, and their values can
be computed using the result ∑N
n=rn = (r −rN+)∕(−r) with r = e𝕚Ωn and e−𝕚Ωn
respectively:
X(𝕚Ω) =
N
∑
n=
e−𝕚Ωn +
N
∑
n=
e𝕚Ωn + 
= e−𝕚Ω −e−𝕚(N+)Ω
−e−𝕚Ω
+ e𝕚Ω −e𝕚(N+)Ω
−e𝕚Ω
+ 
= e−𝕚(Ω∕) −e−𝕚[N+(∕)]Ω
e𝕚(Ω∕) −e−𝕚(Ω∕)
+ −e𝕚(Ω∕) + e𝕚[N+(∕)]Ω
−e−𝕚(Ω∕) + e𝕚(Ω∕)
+ 
= e𝕚[N+(∕)]Ω −e−𝕚[N+(∕)]Ω
e𝕚(Ω∕) −e−𝕚(Ω∕)
= sin[N + (∕)]Ω
sin(Ω∕)
.
(.)
Note that since the signal in time is symmetric, then its FT is real and symmetric (see
Property ). As can be seen in the equation, there is an indetermination at the origin.
The limit of the function when x →is [N +(∕)]. Note also that the signal is periodic
of period 𝜋. Its representation for N = (see Figure .) can be obtained with the code
snippet in Listing ..
N = 5;
X = sin((N+0.5)*Omega)./sin(Omega/2); % Theoretical sinc
X(end/2) = 2*(N+0.5); % Remove the indetermination at the origin
subplot(211), plot(Omega,X)
% Signal in time domain
x2 = [ones(1,N+1) zeros(1,256-2*N-1), ones(1,N)];
% Its transform
X2 = fftshift(fft(x2));
subplot(212), plot(Omega,X2)
Listing 2.6 Example on FT (fftshiftdemo.m).
Assume now a periodic signal ̃x[n] where each period is a version of x[n] delayed lM
samples; that is:
̃x[n] =
∞
∑
l=−∞
x[n −lM].
(.)

Introduction to Digital Signal Processing
67
–3
–2
–1
0
1
2
3
Ω
–2
0
2
4
6
8
10
Figure 2.8 FT of a pulse with 256 samples length and five samples width.
By virtue of the convolution and Kronecker delta properties, it can be expressed as
̃x[n] = x[n] ∗
∞
∑
l=−∞
𝛿[n −lM].
(.)
Since the FT of an inﬁnite sequence of deltas can be written as an inﬁnite sequence of
deltas (show it as proposed exercise), then the FT of ̃x[n] can be expressed as
̃X(𝕚Ω) = X(𝕚Ω)
∞
∑
l=−∞
𝛿
[
Ω −π
lM
]
.
(.)
This can be interpreted as function X(𝕚Ω) being sampled at frequencies π∕lM. A
discrete representation of this function is
̃X(𝕚Ω) = X
(
𝕚π
lM
)
,
(.)
which is known as the DFT). For a periodic version of period M of the pulse with width
N + , the corresponding DFT is
X
(
𝕚lπ
M
)
= sin[N + (∕)](lπ∕M)
sin(lπ∕M)
,
(.)
which becomes a single delta at the origin when N = M + .
2.5.3
Continuous-Time Signals and Systems
The MATLAB Symbolic toolbox allows one to work with functions and signals standing
for continuous time. A symbolic variable is explicitly declared, and the result of scalar
and symbolic variables is another symbolic variable. In Listing ., we see, for instance,

68
Digital Signal Processing with Kernel Methods
how to graphically represent symbolic variables and how to obtain ﬁrst- and higher-
order derivatives easily.
syms t
x = sin(2*pi*t);
y = exp(-t/10);
z = x*y;
whos
figure(1), clc,
subplot(3,1,1); ezplot(x,[-10 10]);
subplot(3,1,2); ezplot(y,[-10 10]);
subplot(3,1,3); ezplot(z,[-10 10]);
dx = diff(x);
ezplot(x,[-1 1]);
hold on
ezplot(dx,[-1 1]); hold off
grid on
d2x = diff(x,2)
d3x = diff(x,3)
Listing 2.7 Definition and plot of symbolic functions (symbolic.m).
Symbolic expressions are not always easy to read; for instance, consider the signal
s(t) = ln
√
+ cos(t)
−cos(t).
(.)
If we calculate and show its derivative, then the result is not readily read. We can use
commands simple and pretty for simplifying and for providing with a readable text
form respectively, as seen in Listing ..
s = log(sqrt((1+cos(t))/(1-cos(t))))
ds = diff(s)
pretty(ds)
simple(ds)
pretty(simple(ds))
Listing 2.8 Example of symbolic simplifications (symbolic2.m).
The integration operator is provided by command int(Integrand,VarInd,
LimInf,LimSup), where Integrand is the symbolic signal to integrate; VarInd
is the integration independent variable; LimInf and LimSup are the limits. Consider,
for instance, the simple example
∫t arctan t dt,
(.)
or this, another simple example:
∫
∞

e−t dt
(.)
Both are obtained immediately in Listing ..

Introduction to Digital Signal Processing
69
% One example
f = t * atan(t);
F = int(f,t);
pretty(F)
% Another example
g = exp(-3*t);
G = int(g,t,0,Inf);
pretty(F)
Listing 2.9 Simple examples of integration (symbolic3.m).
The independent time variable can be readily substituted for signal sampling by means
of subs(x,Expres1,Expres2) which for expression x substitutes the expression
Expres1 by Expres2 in every case, such as in Listing ..
x = 1/t; x2 = subs(x,t,t-2); x3 = subs(x,t,-t);
figure(1), subplot(2,1,1);
ezplot(x); subplot(2,1,2); ezplot(x2);
figure(2), subplot(2,1,1);
ezplot(x); subplot(2,1,2); ezplot(x3);
Listing 2.10 Example of substituting the time variable for sampling (symbolic4.m).
As an example, you are invited to write a function generating the following
continuous-time signal:
x(t) = e−𝛼t sin(𝜔t)u(t)
(.)
given input parameters for the exponent and for the frequency, and recalling that u(t) is
obtained by MATLAB function heaviside(t). Check the eﬀect of the independent
variable transformation for y(t) = x(t −) and z(t) = y(t), compared with r(t) = x(t)
and v(t) = r(t −).
As another example, we can visualize the well-known eﬀect of approximating a
squared periodic wave with its Fourier series with increasing order. Note the resulting
eﬀect of terms with a unit amplitude and period wave, by completing the code in
Listing ., where you can analyze the convergence issues problems that are visible
through the development.
syms k a_k a_kbis serie t
T = 2;
serie = 0;
for k = 1:50
a_k = 1/T*(int(...,0,1) + int(...,1,2));
a_kbis = ... ;
serie = serie + a_k * exp(j*2*pi/T*k*t) + ...
a_kbis * exp(-j*2*pi/T*k*t);
serie = simple(serie); % Note the effect of simplifying
if rem(k,5) == 0;
ezplot(serie,[-4 4]); legend(num2str(k)); pause
end
end
Listing 2.11 Example to complete: Fourier series (symbolic5.m).

70
Digital Signal Processing with Kernel Methods
An additional example can be used for providing the continuous-time convolution of
elementary theoretical signals; for instance, if we deﬁne the function in Listing ., the
reader can check it working for the signals
x(t) = e−tu(t)
(.)
v(t) = e−|t|
(.)
w(t) = (−|t|)(u(t + ) −u(t −))
(.)
in the following continuous-time convolutions:
ya(t) = x(t) ∗v(t)
(.)
yb(t) = x(t) ∗w(t)
(.)
yc(t) = yb(t) ∗𝛿(t −)
(.)
function y = symbolic6(x,h)
% Example of use on command window:
% syms t
% x = exp(-3*t) .* heaviside(t);
% h = heaviside(t) - heaviside(t-3);
% y = symbolic6(x,h)
syms t tau
x = subs(x,t,tau);
h = subs(h,t,t-tau);
y = int( x.*h, tau, -Inf, Inf);
Listing 2.12 Example of symbolic convolutions (symbolic6.m).
Finally, we can check some simple examples for the continuous-time FT with the
function in Listing ..
function symbolic7
syms t w
x = heaviside(t+2) - heaviside(t-2);
X = fourier(x,t,w);
ezplot(X,[-2*pi,2*pi]), axis tight, grid on
Listing 2.13 Example of symbolic FT calculation (symbolic7.m).
Note, however, that symbolic calculations can be limited, even for simple signals
involved, and will not always return a nice and explicit result.
2.5.4
Filtering Cardiac Signals
Here, we show some ﬁltering examples in synthetic and in cardiac signals. First, the
eﬀect of including additional terms on Fourier series in a cardiac signal is explored. Car-
diac signals are not exactly periodic, but rather small changes are present on a beat basis.
For this part, you can use the code in Listing .. As seen in Figure .a, we can check

Introduction to Digital Signal Processing
71
Order: 5
10
–10
–30
–40
–50
–20
0
0.4
0.2
–0.2
0.1
–0.1
0
0.5
0.6
0.7
0.8
0.9
s
s
1
1.1
1.2
1.3
0.4
10
–10
–20
–30
–40
–50
–10
–20
–30
–40
–50
0
15
10
5
0
10
0
100
200
Frequency (Hz)
Frequency (Hz)
300
400
500
300
400
500
0.5
0.5
0.4
0.2
0.1
0
100
200
0.3
1
1.5
2
2.5
3
Time (s)
Time (s)
4
3.5
4.5
0
0.5
1
1.5
2
2.5
3
4
3.5
4.5
0
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
(a)
(b)
(c)
10
0
0
4
3
2
1
0
100
200
300
400
500
0.5
1
1.5
Time (s)
Frequency (Hz)
2
2.5
3
3.5
4
4.5
–10
–20
–30
–40
–50
×10–5
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 0
200
400
600
800
1000
1200
1
Samples
Samples
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 0
200
400
600
800
1000
1200
(d)
(e)
(f)
Figure 2.9 Examples for filtering (I). (a) Demo for Fourier series on a cardiac and near-to-periodic
signal. (b–d) Demo for taking the infinite limit on a periodic signal and its effect on the Fourier
coefficients, leading to the FT. (e,f) Simple examples for low-pass and high-pass filtering on a pulse
signal.

72
Digital Signal Processing with Kernel Methods
that the ventricular activity, given by the most energetic waveform in a given cardiac
cycle, is not suﬃciently represented until a large enough number of terms are included.
load EGM_bipolar
SeriesFourier2(EGM(2,:));
Listing 2.14 Example of Fourier series for a cardiac signal (cardiac1.m).
Second, we review the well-known mathematical foundation for introducing the FT
from Fourier series. For this purpose, you can use the code in Listing .. In this
example, we artiﬁcially build a periodic signal by repeating a single cardiac cycle with
changing period on each step. As seen in Figure .b–d, a longer signal period accounts
for Fourier coeﬃcients on a frequency basis which are getting closer, with an extreme
case limit of inﬁnite period corresponding to an aperiodic signal represented by its
Fourier coeﬃcients envelope.
egm = EGM(2,340:380);
fs = 1e3; ts = 1/fs;
t = (0:5000-1) * ts;
for ciclo = [80 160 500 1000 2000 5000];
x = zeros(1,5000);
x(1:ciclo:end) = 1;
v = filter(egm,1,x);
subplot(211), plot(t,v),xlabel('time (s)'), axis tight
subplot(212), [P,f] = pwelch(v,[],length(v)/2,[],fs);
plot(f,P), axis tight, xlabel('frequency (Hz)'), pause
end
Listing 2.15 Example of Fourier series becoming an FT in the limit for a cardiac signal (cardiac2.m).
Third, we examine a simple ﬁltering case for an artiﬁcial signal, given Listing .. As
seen in Figure .e and f and in Figure .a, the diﬀerent eﬀects of low-pass, high-pass,
and notch ﬁltering can be readily observed.
fs = 1e2;
h = fir1(256,1e-9/fs);
x = [zeros(1,2*fs),ones(1,5*fs),zeros(1,4*fs)];
y1 = filtfilt(h,1,x);
[H,f] = freqz(h,1,1024,fs);
plot(f,abs(H));
h2=fir1(128,30/fs,'high');
y2 = filtfilt(h2,1,x);
v1 = exp(1j*pi*50/fs);
v = [v1 conj(v1)]; h3=poly(v);
H = freqz(h3);
h3 = h3/max(abs(H));
n = .1*cos(2*pi*50/200*(1:length(x)));
xx = x+n;
y3 = filtfilt(h3,1,xx);
Listing 2.16 Example of filtering synthetic signals (cardiac3.m).
Finally, Figure .b–l shows the eﬀect of low-pass ﬁltering an intracardiac signal for
trying to remove its noise, which has been generated with Listing .. The reader is
encouraged to modify the text and obtain the results for diﬀerent cut-oﬀfrequencies
(in the ﬁgure, and Hz have been used).

Introduction to Digital Signal Processing
73
1
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
(j)
(k)
(l)
0.8
0.6
0.4
0.2
0
100
200
300
400
500
600
Samples
t (s)
0
0.4
0.3
0.2
0.1
0
mV
X (f)
–0.1
–0.2
–0.3
–0.4
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
t (s)
0
0.4
0.3
0.2
0.1
0
mV
–0.1
–0.2
–0.3
–0.4
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
0
0.4
0.3
0.2
0.1
0
mV
–0.1
–0.2
–0.3
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
t (s)
t (s)
0
0.4
0.3
0.2
0.1
0
mV
–0.1
–0.2
–0.3
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
f (Hz)
f (Hz)
500
fc = 300 Hz
0
–500
500
0
–500
30
25
20
15
10
5
X (f)
30
25
20
15
10
5
f (Hz)
fc = 100 Hz
500
0
–500
X (f)
30
25
20
15
10
5
f (Hz)
fc = 75 Hz
500
0
–500
X (f)
30
25
20
15
10
5
700
800
900
1000
1100
1
0.8
0.9
0.7
0.5
0.6
0.4
0.2
0.3
0.1
0
50
100
150
200
250
f (Hz)
300
350
400
450
1
0.8
0.9
0.7
0.5
0.6
0.4
0.2
0.3
0.1
0
50
100
150
200
250
f (Hz)
300
350
400
450
0
50
100
150
200
250
f (Hz)
300
350
400
450
1
0.8
0.9
0.7
0.5
0.6
0.4
0.2
0.3
0.1
Figure 2.10 Examples for filtering (II). (a) Simple example for band-pass filtering on a pulse signal. (b,c)
Examples for filtering intracardiac recorded signals, cardiac signal, and its FT. (d–f) Filtered signal,
frequency response of the filter, and resulting spectrum for cutting frequency fc = 300 Hz. (g–l) Same
as previous for fc = 100 Hz (g–i) and fc = 75 Hz (j–l).
load filtradoegm
fs = 1e3; ts = 1/fs;
N = length(x1);
t = ts*(0:N-1);
figure(1), plot(t,x1); axis tight,

74
Digital Signal Processing with Kernel Methods
xlabel('t (s)'), ylabel('mV');
X1 = abs(fftshift(fft(x1)));
f=linspace(-fs/2,fs/2,length(X1));
figure(2), plot(f,X1), axis tight,
xlabel('f (Hz)'), ylabel('X(f)');
% Filter at 300 Hz
h1 = fir1(64+1,2*300/fs);
[H1,f1] = freqz(h1,1,1024,fs);
y1 = filtfilt(h1,1,x1);
figure(3), plot(t,y1);axis tight,
xlabel('t (s)'), ylabel('mV');
figure(4), plot(f1,abs(H1))
axis tight, xlabel('f(Hz)');
Y1 = abs(fftshift(fft(y1)));
figure(5), plot(f,Y1), axis tight,
xlabel('f (Hz)'), ylabel('X(f)');
title('fc = 300 Hz')
Listing 2.17 Example of filtering cardiac signals (cardiac4.m).
Note that for an extremely low-pass cutting frequency the noise is removed, but also
the morphology of the ventricular activation (sharp peaks) is severely modiﬁed, which
can have an impact on the diagnosis when it is based on morphological information.
The reader is encouraged to represent the FT of the cardiac signal, see its spectrum, and
build two simple ﬁlters, one as a low-pass and another as a notch ﬁlter, for removing the
noise while respecting as much as possible the signal morphology.
2.5.5
Nonparametric Spectrum Estimation
This section compares the diﬀerent nonparametric spectrum estimation approaches
presented in Section ... The ﬁrst approach is based on the expectation estimation
of the FT of a window of the signal (Welch periodogram); see Equation .. As will
become apparent in the experiments, this is a low-resolution procedure, and it will
depend on the length of the window. The resolution increases only when the window
width increases, which makes this solution impractical where high resolution is needed.
The second and third approaches are based on the so-called MVDR algorithm in
Equation ., the third one being an approximation based on the eigenvectors of the
data autocorrelation matrix in Equation ..
Assume a signal x[n], ≤n ≤N with N = , containing three sinusoids of
diﬀerent frequencies: Ω= π∕, Ω= π∕, and Ω= π∕. The signal is
corrupted with additive white Gaussian noise with standard deviation 𝜎= .. The
code to produce the signal is given in Listing ..
N = 1024;
t = 0:N-1;
w1 = 0.3*pi; w2 = 0.7*pi; w3 = 0.8*pi; % Frequencies
x = sin(w1*t) + sin(w2*t) + sin(w3*t); % Signal
x = x + 0.4 * randn(size(x));
% Noise added
Listing 2.18 Nonparametric spectrum estimation example: signal with noise (spectrum1.m).
The Welch spectrum is estimated by computing the DFT of all the windows of the
signal containing samples from x[n] to x[n + T −], ≤n ≤N −T + . The spectrum
of each window is computed as the squared norm of the DFT at each frequency, and

Introduction to Digital Signal Processing
75
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
2πrad
2πrad
2πrad
0
20
0
20
0
0.2
0.4
0.6
0.8
1
0
20
Figure 2.11 Welch spectrum of a signal containing signals of frequencies 0.2π, 0.7π, and 0.8π radians,
using windows of 16, 32, and 64 samples.
then all the spectra are averaged. The code in Listing .performs these operations for
a window length of samples.
T = 32; % Window length
x2 = buffer(x,T,T-1,'nodelay'); % Windowed signal vectors
X = abs(fft(x2,256)).^2; % FFT
X = mean(X,2);
% Expectation
figure(1), plot(linspace(0,1,128),10*log10(X(1:end/2)))
Listing 2.19 Nonparametric spectrum estimation example: windowing and averaging the spectra
(spectrum2.m).
Function buffer(T,T-K) constructs a matrix whose columns contain windows of
length T, each one with a delay of K samples from the previous one. Figure .depicts
the Welch spectra for windows of , , and samples. As a result, when the window
is of samples, the algorithm cannot resolve the two closest frequencies.
The MVDR spectrum is computed by using the autocorrelation of the windowed
signals; that is, computing the sample expectation:
̂Rxx = 
N
N−T+
∑
n=
x[n]xH[n]
(.)
where x[n] = [x[n] ⋯x[n+T−]]T. The spectrum at a given frequency Ω is computed as
the inverse of dot product v(Ω)H ̂R
−
xxv(Ω). Provided that Rxx is Hermitian (i.e., Rxx = RH
xx),
it can be proven (exercise) that the product is simply the sum of the DFT of all the rows
or the columns of the autocorrelation matrix. Then, a simple code that produces the
spectrum is given in Listing .. A representation of the MVDR spectrum for three
diﬀerent window lengths is shown in Figure ..

76
Digital Signal Processing with Kernel Methods
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
2π rad
2πrad
2πrad
–5
0
5
–8
–6
–4
–2
0
2
–10
–8
–6
–4
–2
Figure 2.12 MVDR spectrum of a signal containing signals of frequencies 0.2π, 0.7π and 0.8π radians,
using windows of 16, 32, and 64 samples.
x2 = buffer(x,T,T-1,'nodelay'); % Windowed signal vectors
R = x2 * x2' / size(x2,2);
X = 1./sum(abs(fft(pinv(R),256)),2);
plot(linspace(0,1,128),10*log10(X(1:end/2)))
Listing 2.20 Example of MVDR spectrum estimation (spectrum3.m).
Similarly, the MUSIC spectrum is estimated by computing the inverse of the sum of
the DFTs of all columns of matrix QnQH
n . The noise eigenvectors in matrix Qn are found
by excluding the eigenvectors with highest eigenvalues. The signal of the experiment
has three real sinusoids. A real sinusoidal is a linear combination of two complex
exponentials, since
e𝕚Ωn = cos(Ωn) + 𝕚sin(Ωn),
(.)
so the cosine function is expressed as (∕) exp(𝕚Ωn)+(∕) exp(−𝕚Ωn), and similarly for
the sine function. Hence, the three signals are represented by six complex exponentials,
or, in other words, six eigenvectors. Then, the noise eigenvectors are obtained here by
removing the six eigenvectors with higher eigenvalues. The corresponding simple code
is given in Listing ., and Figure .shows the MUSIC spectrum for three diﬀerent
window lengths.
x2 = buffer(x,T,T-1,'nodelay'); % Windowed signal vectors
R = x2 * x2' / 1024;
[Q,L] = eig(R);
X = 1./sum(abs(fft(Q(:,1:end-6) * Q(:,1:end-6)',256)),2);
plot(linspace(0,1,128),10*log10(X(1:end/2)))
Listing 2.21 Example of MUSIC spectrum estimation (spectrum4.m).

Introduction to Digital Signal Processing
77
0
0.2
0.4
0.6
0.8
1
2πrad
0
0.2
0.4
0.6
0.8
1
2πrad
0
0.2
0.4
0.6
0.8
1
2πrad
5
10
15
2
4
6
8
10
2
4
6
8
Figure 2.13 MUSIC spectrum of a signal containing signals of frequencies 0.2π, 0.7π and 0.8π radians,
using windows of 16, 32, and 64 samples.
2.5.6
Parametric Spectrum Estimation
An example of parametric estimation is the previously described AR method of Equa-
tion .. The goal here is to construct an AR model of the signal whose parameters are
those of the AR model in Equation .. The model can be constructed by minimizing
a cost function over the error:
e[n] = x[n] −
P
∑
m=
amx[n −m].
(.)
If the MMSE criterion is chosen, and deﬁning e ∶= [e, … , eN]T, then the cost function
is
𝔼[‖e‖] = 𝔼
⎡
⎢
⎢⎣
||||||
x[n] −
P
∑
m=
amx[n −m]
||||||
⎤
⎥
⎥⎦
.
(.)
Assuming that the signal is stationary, the expectation can be approximated by a sample
average as
𝔼[‖e‖] ≈
N
∑
n=P+
||||||
x[n] −
P
∑
k=
akx[n −k]
||||||

.
(.)

78
Digital Signal Processing with Kernel Methods
We can write the following set of linear equations:
⎛
⎜
⎜
⎜⎝
x[P + ]
x[p + ]
⋮
x[N]
⎞
⎟
⎟
⎟⎠
−
⎛
⎜
⎜
⎜⎝
a
a
⋮
ap
⎞
⎟
⎟
⎟⎠
T ⎛
⎜
⎜
⎜⎝
x[]
⋯
x[N −P]
x[]
⋯
x[N −P + ]
⋮
⋯
⋮
x[P]
⋯
x[N −]
⎞
⎟
⎟
⎟⎠
=
⎛
⎜
⎜
⎜⎝
e[]
e[]
⋮
e[N]
⎞
⎟
⎟
⎟⎠
(.)
that can be expressed in matrix form as
x −aTX = e.
(.)
The minimization of Equation .is equivalent to the minimization of the norm of
vector e with respect to a, which leads to the solution
a = (XXT)−Xx,
(.)
whose proof is left as an exercise for the reader.
Nevertheless, this solution is not convenient since it does not guarantee that the poles
in Equation .are inside the unit circle. Thus, the corresponding estimator is not
necessarily stable. A solution that minimizes the mean square error (MSE) with the con-
straint of providing a stable solution for the AR estimator is the Burg algorithm (Marple,
), which is based on a recursive computation of the model parameters.
In the following example, we use a signal containing a fragment of the “Etude for
guitar No. in E minor” by Spanish guitar composer Francisco de Aís Tárrega i Eixea
(Castellón de la Plana, Spain, – Barcelona, Spain, ). The example shows the
spectrum computed with the estimation provided in Equation .. The spectrum
described in Section ..can be computed as follows. We ﬁrst construct a matrix x
whose column vectors take the form
x[] = [x[], … , x[P]]T
x[] = [x[P −W], … , x[P −W]]T
⋯
x[k] = [x[kP −kW], … , x[kP −W]]T
Therefore, the matrix contains windows of the signal of length P, and there is an overlap
W between consecutive windows. Each column is multiplied by a window. In our
example we choose rectangular, triangular, Hamming, and Blackman windows. The
windowing helps to reduce the side lobes of the FFT. The code can be written as seen in
Listing ..
function param1
% Load the samples, contained in variable 'data'
% with frequency fs=11025 samples/second
load guitar; y=data;
%% Variable setup
ts = 1/fs;
% sampling period
N = length(y);
% signal length

Introduction to Digital Signal Processing
79
tw = 0.025;
% window length in seconds
to = 0.0125;
% window overlap in seconds
nw = floor(tw / ts)+1;
% window length in samples
no = floor(to / ts)+1;
% window overlap in samples
res1 = fs / N;
% Signal resolution
res2 = fs / nw;
% Window resolution
Nt
= floor(N/no); % Time length of the spectrogram
% in samples
Nw
= nw/2;
% Frequency length onf the spectrogram
% in samples
t = no/fs*(0:Nt-1); % time axis for the spectrum
tt = 1/fs*(0:N-1);
% time axis for the time representation
f = fs*(0:Nw-1)/2/Nw; %frequency axis
%% Signal representation
figure(1), plot(tt,y);
axis tight, xlabel('t(s)'), ylabel('mV')
%% Windows used for the representation
wwT = repmat(triang(nw),1,Nt);
wwH = repmat(hamming(nw),1,Nt);
wwB = repmat(blackman(nw),1,Nt);
%% Signal buffering and windowing
s = buffer(y,nw,no,'nodelay');
sT = s .* wwT;
sH = s .* wwH;
sB = s .* wwB;
%% Spectrogram computation
S = spectrogr(s);
ST = spectrogr(sT);
SH = spectrogr(sH);
SB = spectrogr(sB);
%% Representation
figure(2), subplot(211), imagesc(t,f,S);
xlabel('t(s)'), ylabel('dBm')
title('Spectrogram with a rectangular window')
subplot(212), imagesc(t,f,ST);
xlabel('t(s)'), ylabel('dBm')
title('Spectrogram with a triangular window')
figure(3), subplot(211), imagesc(t,f,SH);
xlabel('t(s)'), ylabel('dBm')
title('Spectrogram with a Hamming window')
subplot(212), imagesc(t,f,SB);
xlabel('t(s)'), ylabel('dBm')
title('Spectrogram with a Blackman window')
%% Auxiliary function
function S = spectrogr(s,Nw)
Nw = size(s,1) / 2;
S = 10 * log10(abs(fftshift(fft(s))));
S = S(Nw+1:2*Nw,:);
Listing 2.22 Signal windowing (param1.m).

80
Digital Signal Processing with Kernel Methods
0
5
10
15
20
25
30
t (s)
–0.5
–0.4
–0.3
–0.2
–0.1
0
0.1
0.2
0.3
0.4
0.5
mV
Figure 2.14 Time representation of the analyzed signal.
0
0
dBm
dBm
5
10
15
20
25
30
0
5
10
15
t (s)
t (s)
Spectrogram with a triangular window
Spectrogram with a rectangular window
20
25
30
1000
2000
3000
4000
5000
0
1000
2000
3000
4000
5000
Figure 2.15 Upper panel: spectrogram of the signal with a rectangular windowing. Lower panel:
spectrogram with a triangular window.
The signal has a sampling frequency of samples per second, and thus the
spectrum representation ranges from to Hz. Figure .shows the time
representation of the signal. Figures .and .show the spectrograms of the signal
with diﬀerent windows. In these ﬁgures, the peaks correspond to the pulsation of

Introduction to Digital Signal Processing
81
0
0
dBm
5
10
15
20
25
30
t (s)
1000
2000
3000
4000
5000
0
0
dBm
5
10
15
20
25
30
t (s)
1000
2000
3000
4000
5000
Spectrogram with a Hamming window
Spectrogram with a Blackman window
Figure 2.16 Upper panel: spectrogram of the signal with a Hamming windowing. Lower panel:
spectrogram with a Blackman window.
the diﬀerent notes of the music in the guitar. The rectangular window exhibits better
frequency resolution, while the rest of the windows exhibit lower resolution, but the
peaks can actually be better seen thanks to the reduction of the secondary lobes.
2.5.7
Source Separation
The following example uses the well-known FastICA algorithm for blind source separa-
tion. The example is reproduced from the one in Murphy (), but using a simpliﬁed
algorithm for demonstration purposes.
The simulated scenario consists of a set of four sources, one of them being white noise.
The sources are supposed to be in diﬀerent places of the space and they are recorded by
four sensors. The recorded signals are simulated by a × mixing matrix, given by
W =
⎛
⎜
⎜
⎜⎝


.
.
.


.
.

.
.

.
.
.
⎞
⎟
⎟
⎟⎠
(.)
The observed data are then
x[n] = Wz[n]
(.)

0
50
100
150
200
250
300
350
400
450
500
0
50
100
150
200
250
300
350
400
450
500
0
50
100
150
200
250
300
350
400
450
500
0
50
100
150
200
250
300
350
400
450
500
0
50
100
150
200
250
300
350
400
450
500
0
50
100
150
200
250
300
350
400
450
500
0
50
100
150
200
250
300
350
400
450
500
0
50
100
150
200
250
300
350
400
450
500
0
50
100
150
200
250
300
350
400
450
500
0
50
100
150
200
250
300
350
400
450
500
0
50
100
150
200
250
300
350
400
450
500
0
50
100
150
200
250
300
350
400
450
500
0
50
100
150
200
250
300
350
400
450
500
0
50
100
150
200
250
300
350
400
450
500
0
50
100
150
200
250
300
350
400
450
500
0
50
100
150
200
250
300
350
400
450
500
−0.5
0
0.5
−0.5
0
0.5
−5
0
5
−0.05
0
0.05
−2
0
2
−5
0
5
−5
0
5
−2
0
2
−2
0
2
−2
0
2
−5
0
5
−5
0
5
−2
0
2
−5
0
5
−2
0
2
−5
0
5
Figure 2.17 Upper left panel: sources of the ICA experiment. Upper right panel: observations at the four sensors. Lower left panel: result of the principal
component analysis (PCA) procedure. Lower right panel: result of the source separation using ICA.

Introduction to Digital Signal Processing
83
where z[n] is the vector of samples of the four sources. Then, the source separation task
consists of the estimation of matrix V = W −. The corresponding MATLAB code for
the data generation is given in Listing .. Figure .(upper left panel) shows the data
sources zi[n], and the upper right panel shows the mixed signals xi[n] received by the
sensors.
x
= (-14:14) / 29;
z1 = kron(ones(1,18),x); z1 = z1(1:500);
x
= (-9:9) / 25;
z2 = kron(ones(1,28),x.^3)/0.1; z2 = z2(1:500);
z3 = 5 * sin(2*pi*32*(0:499)/500)/2;
z4 = 0.01 * randn(1,500);
W
= [1 1 0.5 0.2;0.2 1 1 0.5;0.2 1 1.4 0.4; 1 0.3 0.5 0.5];
X
= W' * [z1;z2;z3;z4];
Listing 2.23 Data generation for BSS example (bss1.m).
The ICA approach assumes that the data is non-Gaussian, so the observations have
to be be whitened using a PCA. After that, the covariance of the observations is given
by
Cov[x] = WW T.
(.)
The ICA procedure consists of computing the log likelihood of matrix V according to a
set of prior probability distribution models for the source signals, and then maximizing
them. The FastICA approach assumes that vectors in matrix V are orthonormal and
adds it as a constraint of the optimization. It iteratively maximizes the log likelihood by
using its gradient and Hessian. The code used in the experiments is given in Listing ..
The code iteratively updates the values of the vectors and forces their orthonormality
by using a Gram–Schmidt orthonormalization. Figure .shows the results. The left
lower panel shows the PCA approach for source separation and the left lower one shows
the result of the ICA algorithm.
function [V Z iter L] = bss2(X,epsilon)
[Dim Ndata] = size(X);
V = randn(Dim);
L = [];
for j = 1:Dim
current = V(:,j);
last = zeros(Dim,1);
iter_max = 40;
iter(j) = 0;
while ~(abs(current-last)<epsilon)
last = V(:,j);
V(:,j) = 1 / Ndata * X * tanh(V(:,j)'*X)' - 1 / Ndata * ...
sum((1-tanh(V(:,j)'*X).^2)) * V(:,j);
V(:,1:j) = gramsmith(V(:,1:j));
current = V(:,j);
P = -log(cosh(V'*X));
L = [L 1/Ndata*sum(P(:))];
iter(j) = iter(j)+1;
if iter(j) > iter_max
break

84
Digital Signal Processing with Kernel Methods
end
end
end
Z = V' * X;
function V = gramsmith(V)
for i = 1:size(V,2)
V(:,i) = V(:,i) - V(:,1:i-1) * V(:,1:i-1)' * V(:,i);
V(:,i) = V(:,i) / norm(V(:,i));
end
Listing 2.24 BSS example with iterative simple algorithm (bss2.m).
2.5.8
Time–Frequency Representations and Wavelets
As stated in Section .., the DWT decomposes the signal using a cascade of low-
pass and high-pass ﬁlters. Wavelets have revolutionized the ﬁeld of signal processing,
image processing, and computer vision in recent decades. Many problems exploit these
data representations to analyze and process signals of any kind, from time series to
images and videos. Being an invertible transformation, one can do smart operations
in the transformed domain and invert back the result to the original domain.
In what follows, we will just illustrate this as a naive suppression of signal/image
features, but one could actually combine components, downweight others, and think
of many alternative analyses. Actually, deﬁning appropriate operations in wavelet
representations is a whole ﬁeld in its own right. We can currently ﬁnd in the literature
lots of algorithms for image restoration, image compression, image fusion, and object
detection and recognition.
Wavelets for Signal Decomposition
Let us start with a very simple example of D signal decomposition. One example of
how this can be used is in the decomposition of the observed (and possibly noise-
corrupted) signal into its signal and noise components. We will illustrate the use of
(time–frequency) representations, and in particular of wavelets, in both signal and
image processing.
Let us assume ﬁrst we have a simple observed signal composed of a low-frequency
sinusoid buried on high-frequency Gaussian noise of low variance, 𝜎
n = .. Such
a signal can be generated and represented with the code in Listing .. We can use
the DWT with only two scales of decomposition to separate high- and low-frequency
components, as seen in the code, where cA and cD are the low and high DWT frequency
coeﬃcients respectively. Now, we can use these coeﬃcients to perform a full or a partial
reconstruction of the signal, since we are using an orthogonal (hence invertible) wavelet
transform. Finally, we represent and compare the three reconstructions.
% Signal generation
N = 1000;
n = linspace(0,pi,N);
s = cos(20.*n) + 0.5.*rand(1,N);
figure(1), clf, subplot(221),
plot(n,s), grid on, axis tight
title('Original signal')
% Two scales decomposition
[cA,cD] = dwt(s,'db2');

Introduction to Digital Signal Processing
85
ssf = idwt(cA,cD,'db2');
% Full reconstruction
ssl = idwt(cA,zeros(1,N/2+1),'db2'); % Inverse using LF
ssh = idwt(zeros(1,N/2+1),cD,'db2'); % Inverse using HF
% Representations
subplot(222), plot(n,ssf)
grid on, axis tight, title('Reconstructed signal')
subplot(223), plot(n,ssl)
grid on, axis tight, title('Low freq. reconstruction')
subplot(224), plot(n,ssh)
grid on, axis tight, title('High freq. reconstruction')
Listing 2.25 Simple example for DWT signal decomposition (wdt1.m).
The results are shown in Figure ., where the full observation, its wavelet decom-
position in low- and high-frequency components, and the reconstructed signals are
represented.
Wavelets for Image Decomposition
Let us now illustrate the use of wavelets in a D example, so that the signal is now
a grayscale image. Images and time series are both structured domains with clear
correlations (either in time or space) among nearby samples. Such correlations may
0
1
2
3
−0.5
0
0.5
1
Reconstructed signal
0
1
2
3
–1
–0.5
0
0.5
1
Low freq. reconstruction
0
1
2
3
–0.2
–0.1
0
0.1
0.2
High freq. reconstruction
0
1
2
3
–0.5
0
0.5
1
Original signal
Figure 2.18 Full, low-only and high-only frequency reconstructions of a low-frequency sinusoid signal
contaminated with high-frequency noise using DWT and inverse DWT.

86
Digital Signal Processing with Kernel Methods
Figure 2.19 Illustration of the use of wavelet-based image decomposition (left) and the
reconstruction error when the horizontal component is removed (right).
occur at particular (frequency) scales, but in the case of images the correlations
also arise at spatial orientations. Therefore, a wavelet decomposition of an image
is an alternative representation composed of a number of scales and orientations.
Typically, only three orientations are considered (horizontal, vertical, and diagonal),
but there is a plethora of wavelet transformations accounting for more angular
scales.
In this illustrative case, we just plan to take an image and represent the wavelet coeﬃ-
cients, to do a simple operation there by suppressing some scales, and to invert back the
result, as seen in Figure .. The wavelet decomposition of the selected standard image
of barbara reveals relevant information about the image structure by highlighting
horizontal, vertical, or diagonal edges in the corresponding scale only. Actually, by
setting the horizontal coeﬃcients to zero and inverting back, the reconstruction error
concentrates in such image features (i.e., horizontal edges are aﬀected by large errors).
The MATLAB code to reproduce this simple example of image decomposition is given
in Listing ..
load barbara512
% Single-level decomposition of X of a particular wavelet
wname = 'db1'; % wname = 'sym4';
[CA,CH,CV,CD] = dwt2(X,wname);
decompo = [ CA/max(max(CA)), CH/max(max(CH));
CV/max(max(CV)), CD/max(max(CD)) ];
figure, imshow(decompo,[])
% Let's remove the horizontal coefficients and invert the representation
CH = zeros(size(CH));
Xr = idwt2(CA,CH,CV,CD,wname);
% Plot the reconstruction error
figure, imshow(abs(X-Xr),[]);
Listing 2.26 Example of DTW for image decomposition (wdt2.m).

Introduction to Digital Signal Processing
87
Figure 2.20 Illustration of the use of wavelet decomposition for image fusion.
Wavelets for Image Fusion
Another example of the use of wavelet decomposition is the ﬁeld of image fusion,
in which two (distorted) acquisitions of the same scene are decomposed, only the
interesting components are smartly combined in the (wavelet) representation space,
and then inverted back to the image representation space.
Figure .shows an example of this process of image fusion with wavelets. Essen-
tially, we merge the two images from wavelet decompositions at level by taking the
maximum of absolute value of the coeﬃcients, for both approximations and details
components. The code reproducing this is given in Listing ..
% Load two fuzzy versions of an original image
load checkboard X1 X2
% Merge two images from wavelet decompositions at level 5
% using sym4 by taking the maximum of absolute value of the
% coefficients for both approximations and details
XFUS = wfusimg(X1,X2,'sym4',5,'max','max');
% Plot original and synthesized images
figure, imshow(X1), axis square off, title('Catherine 1')
figure, imshow(X2), axis square off, title('Catherine 2')
figure, imshow(XFUS), axis square off, title('Synthesized image')
Listing 2.27 Example of DWT for image fusion (wdt3.m).
2.5.9
Examples for Spectral Analysis on Manifolds
In this section, the spectral analysis of a mesh is presented by using a combinational
graph Laplacian and a geometric mesh Laplacian. First, an example of the graph Lapla-
cian shows the transformation from the spatial domain to the frequency domain and
the transformation back to the spatial domain. In addition, a mesh deformation is done
by using this frequency transformation. The second example uses a geometric Laplacian
to ﬁlter a noisy mesh. Finally, the graph Laplacian and the geometric Laplacian are com-
pared by computing the mean absolute error (MAE) during the reconstruction process.
Let the -manifold triangular mesh be M = (V, F, E); the graph Laplacian of this mesh
can be computed by
Lij =
⎧
⎪
⎨
⎪⎩
di
if i = j
−
if (i, j) is an edge

otherwise,
(.)

88
Digital Signal Processing with Kernel Methods
where di = |N(vi)| is the number of vertices in a one-ring ﬁrst-order neighbor. Here,
we use a tear-shaped mesh of vertices. The reader can access a variety of meshes
in Stanford University (a,b). The eigendecomposition of the graph Lagrangian L
provides the eigenvectors (basis) and the eigenvalues (spectrum) of the mesh. Lower
eigenvalues (lower frequencies) correspond to ﬁrst basis components.
The code for the spectral analysis is given in Listing ., and the code for
graph_Laplacian.m is in Listing ..
Figure .shows the basis (eigenvectors) projected on the surface of the mesh.
Note that the ﬁrst eigenvector is constant and the variation of the basis (frequency)
increases when the a higher basis is considered. As was deﬁned in Section ., the
spectral coeﬃcients can be obtained by using Equation .and the reconstruction of
the original mesh with Equation ..
% Load the vertex and faces matrices
load vertex;
load faces;
n = length(vertex);
% The Laplacian is computed
L = graph_Laplacian(n,faces);
% Eigendecompositon of K:
[H,A] = eig(L);
% Plot the eigenvectors on the mesh surface
figure;
selec_basis = [1:5 20 50 100 200 400];
for i=1:length(selec_basis)
subplot(2,5,i);
h = patch('vertices',vertex,'faces',faces,...
'FaceVertexCData', H(:,selec_basis(i)), ...
'FaceColor','interp');
title(['H_{' num2str(selec_basis(i)) '}'],'FontSize', 20)
caxis([-0.05 0.05]);
axis([20
60
-30
60])
lighting phong;
camlight('right');
camproj('perspective');
shading interp;
axis off;
colormap('jet')
end
% Spectral coefficients
a = H'*vertex;
figure;
plot(a,'Linewidth',2);
axis tight; title('Spectral coefficients');
legend('a_x coefficients','a_y coefficients','a_z coefficients');
% Mesh reconstruction
figure; colormap gray(256);
selec_basis = [1:5 50 200 482];
for i=1:length(selec_basis)
subplot(2,4,i);
x_hat = H(:,1:selec_basis(i))*a(1:selec_basis(i),:);
if selec_basis(i)>2
patch('vertices',x_hat,'faces',faces, ...
'FaceVertexCData',[0.5 0.5 0.5],'FaceColor','flat');
title(['H_1 - H_{' num2str(selec_basis(i)) '}'])

Introduction to Digital Signal Processing
89
axis([20
60
-30
60]);
axis off;
lighting phong;
camlight('right');
camproj('perspective');
else
plot3(x_hat(:,1),x_hat(:,2),x_hat(:,3),'k.-');
title('H_1','FontSize', 20);
axis off;
end
end
Listing 2.28 Basis projected on the surface of the mesh (MHexamples1.m).
function L = graph_Laplacian(n,faces)
% Firstly, the edges and their indices are obtained from the face matrix
edges = [faces(:,1) faces(:,2); faces(:,1) faces(:,3);
faces(:,2) faces(:,3); faces(:,2) faces(:,1);
faces(:,3) faces(:,1); faces(:,3) faces(:,2)];
indices = (edges(:,2)-1)*n + edges(:,1);
% Adjacency matrix
W = zeros(n,n);
W(indices) = 1;
% Degree matrix
D = diag(sum(W,2));
% Graph Laplacian
L = D - W;
Listing 2.29 Graph Laplacian code (graph_Laplacian.m).
Figure .shows the process of reconstruction when an increased number of
components are included in the reconstruction. Note that all vertex positions are
H1
H20
H50
H100
H200
H400
H2
H3
H4
H5
Figure 2.21 Eigenvectors (basis) of the tear-shaped mesh graph Laplacian projected on the surface of
this mesh.

90
Digital Signal Processing with Kernel Methods
H1
H1
H1– H3
H1–H4
H1–H482
H1– H200
H1– H50
H1– H5
Figure 2.22 Reconstruction of the tear-shaped mesh from the eigendecomposition of the graph
Laplacian.
(a)
(b)
Figure 2.23 The green mesh is the original tear-shaped mesh and the red mesh is the modified one by
increasing the fourth spectral component (a) or decreasing the fourth spectral component (b).
concentrated in small area when only the ﬁrst component, which corresponds to the
constant eigenvector, is considered for the reconstruction. If one more component
(two components in total) is included in the reconstruction, the vertex positions are
distributed along a line. For the ﬁrst three components, the vertices form a plane,
and ﬁnally the D mesh is obtained when four components are used. When more
components are included in the reconstruction, a more detailed mesh is obtained;
hence, the last components with no additional information or noise could be discarded
for a compression or denoising application.
As with ﬁltering in signal processing, it is also possible to modify the geometry of
the mesh by applying a ﬁlter in the frequency domain. For example, Figure .shows

Introduction to Digital Signal Processing
91
Figure 2.24 Left atrial mesh and its noisy version.
the modiﬁcation of the fourth spectral component (equivalent to a notch ﬁlter in the
frequency associated with the fourth component) by increasing it with a value of 
(Figure .a), or decreasing with a value of .(Figure .b). These modiﬁcations
respectively widen or narrow the tear-shape mesh. The additional code for the mesh
modiﬁcation is given in Listing ..
% Filter
filtcoef = ones(size(vertex));
filtcoef(4,:)=2;
% Reconstructed mesh
x_hat_n = H*(filtcoef.*a);
Listing 2.30 Code for the mesh modification (MHexamples2.m).
As pointed out in the previous example, the mesh compression can be obtained by
just taking the ﬁrst components which provide the shape of the mesh. This compression
can also be considered as a low-pass ﬁltering procedure. Now, we explore further this
ﬁltering feature by using a more complex mesh. The left mesh of Figure .shows the
aforementioned mesh, which was obtained by segmenting the computed tomography
image of a left atrium. Also shown is its noisy version (Gaussian noise) in the right part of
the ﬁgure. This mesh had hundreds of thousands of vertices; however, it was decimated
to vertices in order to reduce the computational cost of the eigendecomposition
algorithm.
For this ﬁltering example, we use the second type of Laplacian, the geometric
Laplacian L, which is deﬁned as
Lij =
⎧
⎪
⎨
⎪⎩
−∑
vj∈N(vi) wij
if i = j
wij
if (i, j) is an edge

otherwise,
(.)
where wij was deﬁned in Equation .. The eigendecomposition of L, the spectral
coeﬃcients, and the mesh reconstruction are obtained as in the previous example.
Figure .shows the evolution of the reconstruction for the noisy left atrial mesh.
When components have been included in the reconstruction, the atrial mesh has

92
Digital Signal Processing with Kernel Methods
H1–H4
H1– H7
H1–H20
H1–H30
H1– H500
H1–H250
H1–H100
H1–H50
Figure 2.25 Reconstruction process of the noisy left atrial mesh.
its complete shape but without the additional noise. Hence, a low-pass ﬁlter of the mesh
is computed by just reconstructing the mesh with the ﬁrst few components. The code
for the computation of the geometric mesh Laplacian is given in Listing ..
function L = geometric_Laplacian(vertex,faces)
n = length(vertex);
L=zeros(n,n);
for i=1:n
vi=vertex(i,:);
% Search the faces which share the vertex vi
ind_vi=[find(faces(:,1)==i); find(faces(:,2)==i); ...
find(faces(:,3)==i)];
faces_vi = faces(ind_vi,:);
% vi neighborgs
Ni=setdiff(faces_vi,i);
% For each neighborg of vi
for j=1:length(Ni)
vj=vertex(Ni(j),:);
% Search the two faces shared by the edge vi-vj
ind_vj=[find(faces_vi(:,1)==Ni(j)); ...
find(faces_vi(:,2)==Ni(j)); find(faces_vi(:,3)==Ni(j))];
% Search the third vertex in the face
for k = 1: length(ind_vj)
ind_vk=setdiff(faces_vi(ind_vj(k),:),[i Ni(j)]);
% Compute the angle opposite to the edge vi-vj
va = vi - vertex(ind_vk,:);
vb = vj - vertex(ind_vk,:);

Introduction to Digital Signal Processing
93
% Angles alpha and beta
opp_angle = acos(dot(va,vb)/(norm(va)*norm(vb)));
L(i,Ni(j))= L(i,Ni(j)) + cot(opp_angle);
L(Ni(j),i)= L(Ni(j),i) + cot(opp_angle);
end
L(Ni(j),i) = L(Ni(j),i)/2;
L(i,Ni(j)) = L(i,Ni(j))/2;
end
end
L= diag(sum(L,2)) - L;
Listing 2.31 Code for geometric mesh Laplacian computation (geometric_Laplacian.m).
Finally, as Dyer et al. () remarked that the robustness of the geometric Laplacian
is greater than the graph Laplacian, we also consider the graph Laplacian in the last
example in order to compare the quality of the reconstruction with both Laplacians.
For this purpose, we use the MAE as a merit ﬁgure of comparison. The code for the
comparison is in Listing ..
% Load the vertex and faces matrices
load vertex;
load faces;
n = length(vertex);
L = geometric_Laplacian(vertex,faces);
K = graph_Laplacian(n,faces);
% Eigendecompositon of L and K:
[H_K,A_K] = eig(K);
[H_L,A_L] = eig(L);
% Spectral coefficients
a_K = H_K'*vertex;
a_L = H_L'*vertex;
% Mesh reconstruction error and mean absolute error
for i=1:n
x_hat_K = H_K(:,1:i)*a_K(1:i,:);
MAE_K(i) = mean(abs(vertex(:)-x_hat_K(:)));
x_hat_L = H_L(:,1:i)*a_L(1:i,:);
MAE_L(i) = mean(abs(vertex(:)-x_hat_L(:)));
end
% Plot the error figure
figure; set(gcf,'color','w');
set(gca,'FontSize',20);
plot(MAE_K,'LineWidth',2);
hold on; plot(MAE_L,'r','LineWidth',2); axis tight;
xlabel('Components');
ylabel('Mean absolute error')
legend('Graph Laplacian','Geometric Laplacian')
Listing 2.32 Comparisons in terms of the MAE (MHexamples3.m).
Figure .shows the MAE for the mesh reconstruction using the graph Laplacian
and the geometric Laplacian. During all the reconstruction process the quality of
the reconstructed mesh (less error) is superior for the geometric Laplacian than the

94
Digital Signal Processing with Kernel Methods
14
12
10
8
6
4
2
100
200
Components
Mean absolute error
300
400
500
Graph Laplacian
Geometric Laplacian
Figure 2.26 MAE for the mesh reconstruction using the graph Laplacian and the geometric Laplacian.
graph Laplacian. Note that the highest error is obtained for the ﬁrst components
because the mesh obtained a D shape when four component are included in the
reconstruction.
2.6
Questions and Problems
Exercise ..
Explain in a physical sense and with your own words why we need to
use complex algebra for deﬁning the FT.
Exercise ..
Does the PSD contain all the relevant information of a signal? Is the
same response true for a stochastic process?
Exercise ..
Can you describe three eﬀects of windowing a stochastic process on
its PSD?
Exercise ..
Write the equations of deconvolution with the MMSE criterion, and
then repeat this exercise after adding a term with an 𝓁regularization term of model
parameters. Whath are the main diﬀerences between both equations?
Exercise ..
Program a simple example for your equations in Exercise .., where
the signal to be estimated is a sparse signal (only some few samples are nonzero). What
is the eﬀect of the presence or absence of regularization on the estimated amplitude for
the samples?
Exercise ..
Write the equations of a regular sampling interpolation with sinc
signals using the MMSE criterion, and then repeat the derivation adding an 𝓁regu-
larization term. Program a simple example where you can see the eﬀect of interpolating

Introduction to Digital Signal Processing
95
a signal, and see the eﬀect of regularization in the same signal when sampled at times
the sampling period used for the observed samples.
Exercise ..
Can you explain qualitatively the diﬀerences between the Gabor
transform, the Wigner–Ville distribution, and the Margenau–Hill distribution?
Exercise ..
When would it be better to use the CWT and when the DWT?
Exercise ..
Can you explain qualitatively what is the physical meaning of the
Laplacian operator in the manifold harmonics algorithm? Can you explain the meaning
of the eigenvectors and eigenvalues in simple geometrical terms?

97
3
Signal Processing Models
3.1
Introduction
Classical taxonomies on signal processing and analysis often distinguish between
estimation and detection problems, which are often known in machine learning as
regression and classiﬁcation respectively. Up to four main data models in statistical
Learning were pointed out further by Cherkassky and Mulier (); namely, clustering,
density estimation, classiﬁcation, and regression. Models in these paradigms can be
adjusted (tuned, trimmed, ﬁtted) to the available datasets by following diﬀerent induc-
tive principles, including MMSE and ML criteria. On the other hand, time series analysis
has provided a wide variety of paradigms for parametric modeling to be adjusted in
terms of their time properties, which can be summarized in the autocorrelation and
cross-correlation statistical descriptions. The good news with SVM and kernel methods
is that they provide at least two main advantageous elements, such as the kernel
trick and the single solution inductive principle. These advantages allowed the kernel
methods community to revisit old, well-established data models successfully. However,
the existing richness of data models in time series analysis, statistics, and many other
data processing ﬁelds has not always been taken into account, so there is still room for
improving experimental performance and more solid theoretical analysis.
Signal estimation, regression, and function approximation are old, largely studied
problems in signal processing, statistics, and machine learning. The problem boils down
to optimizing a loss (cost, energy) function over a class of functions. The problem of
building function approximation models is interesting and highly relevant in DSP. A
model that can interpolate and extrapolate well is useful to identify a system that gen-
erated the observations and to generalize (give predictions) to unseen data. Signal esti-
mation is ubiquitous in many signal processing settings, such as system identiﬁcation,
time-series prediction, channel equalization, and signal denoising, just to name a few.
A major goal of this book is to show how to adapt a good existing algorithm given
the speciﬁcities of the data and a proper signal model, and how to do this via kernel
methods. This algorithmic ductility and malleability of kernel methods has already
yielded an unprecedented number of mathematical models to be adapted and delivered,
rediscovered, or brought to the fore during the last decade. But kernel methods also
bring new opportunities to develop new algorithms for dealing with high-dimensional
data from D signal principles and models. Therefore, the main purpose of this chapter
and Chapter is to pay attention to a number of signal models, which will be widely
Digital Signal Processing with Kernel Methods, First Edition. José Luis Rojo-Álvarez, Manel Martínez-Ramón,
Jordi Muñoz-Marí, and Gustau Camps-Valls.
© John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.

98
Digital Signal Processing with Kernel Methods
used throughout the book. We will make emphasis on understanding, in a simple
way, those paradigms that take into account temporal relationships or correlations
(signal processing models), and those paradigms that do not make any assumption on
the correlation properties of the data (data processing models). We will see that the
latter can be seen as just instances of the former in their multidimensional versions.
This observation holds when we replace the need of accounting for multidimensional
correlations with random sampling of the multidimensional spaces.
This chapter conveys the ideas from DSP to be clearly kept in mind when working
on kernel-based signal processing algorithms. Basic concepts of signal Hilbert spaces,
noise, and optimization are presented. Attention is paid to well-known signal models
in terms of signal expansions in orthogonal and nonorthogonal signal spaces; that is,
nonparametric spectral estimation (sinusoidal expansions), autoregressive exogenous
(ARX) and 𝛾-ﬁlter system identiﬁcation (lagged signals expansions), sinc interpolation
and deconvolution (impulse response expansions), and array processing (subspace and
other expansions). Other fundamental concepts in DSP for time series are introduced,
speciﬁcally from adaptive ﬁltering concepts, noise concepts, and complex signals, which
will be used throughout the rest of the book. A set of synthetic and real examples is
presented for clarifying these concepts from simple MATLAB scripts.
3.2
Vector Spaces, Basis, and Signal Models
We start the chapter with a brief overview of vector spaces and basis. This will allow
us to express signals and signal models in vector (and more generally in matrix) form.
By doing so we inherit all the powerful and well-established tools of linear algebra,
which in turn makes it easy to build algorithms and derive mathematical proofs.
Another advantage of expressing signals in vector/matrix form is that standard software
packages, such as R and MATLAB, provide extensive tools to work with them.
3.2.1
Basic Operations for Vectors
Let us start by deﬁning a pointin a vector space . We express a point in ℝN as a
(column) vector of the form
x = [x, x, … , xN]T,
where xi, i = , … , N, is an element of that vector, and the symbol T indicates the
transpose operation. Note that by deﬁnition our points are column vectors. We will
distinguish the notation for a vector and for its components by using boldfaced fonts
for vectors; that is, x.
Deﬁnition ..(Sum of two vectors)
The sum of two vectors x, y ∈, is given by
the sum of their individual components: x + y = [x+ y, x+ y, … , xN + yN]T, where
xi is the ith component of vector x.
Throughout the book we will use the terms “sample,” “example,” “pattern,” or “instance” interchangeably to
denote a point in a geometrical space interchangeably.

Signal Processing Models
99
Deﬁnition ..(Inner product)
The inner (scalar, or dot) product of two vectors
x, y ∈is deﬁned as the sum of the products of all elements of the vectors, and is
denoted as ⟨x, y⟩= ∑N
k=xkyk. Very often one omits the subscript indicating the
space where the inner product occurs, being obvious from the context, and simply
denote ⟨x, y⟩.
Given two vectors x and 𝜆y, its diﬀerence vector x −𝜆y has a positive norm for all
𝜆∈ℝ:
‖x −𝜆y‖= ⟨x −𝜆y, x −𝜆y⟩≥.
(.)
Now assume that the diﬀerence vector x −𝜆y is orthogonal to y. In this case, it is
straightforward to prove that ⟨x −𝜆y, y⟩= ⟨x, y⟩−𝜆‖y‖= , and thus necessarily
𝜆= ⟨x, y⟩∕‖y‖, and the Cauchy–Schwartz inequality follows:
⟨x, y⟩≤‖x‖‖y‖.
(.)
Also, 𝜆y is the projection of x over y, and then 𝜆‖y‖ = ‖x‖ cos 𝜃, where 𝜃is the angle
between these vectors. In that case, with the previous value of 𝜆, it is straightforward to
show that ⟨x, y⟩= ‖x‖‖y‖ cos 𝜃. This second deﬁnition shows clearly that the inner
product is related to the concept of similarity between vectors, and therefore is of
crucial importance in statistics, and in signal processing in particular, because ﬁltering,
correlation, and classiﬁcation are ultimately concerned about estimating similarities
and ﬁnding patterns in the data.
Deﬁnition ..(𝓁q-norm)
The 𝓁q-norm of a vector x is denoted as ‖x‖q
=
(∑N
i=xq
i )∕q. Usually the 𝓁norm is employed (called simply norm of a vector) and
the subindex is omitted, being ‖x‖= ‖x‖ = ⟨x, x⟩∕.
Deﬁnition ..(Euclidean distance)
Given the 𝓁norm, the Euclidean distance
between two vectors x and y is deﬁned as
d(x, y) = ‖x −y‖=
√
√
√
√
N
∑
k=
(xk −yk).
Deﬁnition ..(Linear independence)
A set of M vectors xi ∈ℝN, i = , … , M,
are linearly independent if and only if ∑M
i=bixi = ⇔bi = ∀i. Linear independence
is a very important property related to the deﬁnition of basis.
Deﬁnition ..(Basis)
A basis is any set of vectors, xi, i = , … , M, M ≥N, from
which we can express any vector in ℝN. In other words, for any vector y ∈ℝN we can
ﬁnd a set of M coeﬃcients, ai ∈ℝso y can be expressed as
y =
M
∑
i=
aixi

100
Digital Signal Processing with Kernel Methods
A set of vectors xi being a basis contains at least N linear independent vectors. Among
all the possible bases, we are usually interested in orthonormal bases, which are those
formed with orthogonal vectors of norm :
⟨xi, xj⟩= 𝛿i,j =
{

i = j

i ≠j
(.)
for i = , … , M, where 𝛿i,j is the Kronecker function.
An important and well-known basis in ℝN is the canonical basis. Vectors xi of this
basis have their kth elements of the form xi,k = 𝛿i,k. Another relevant basis is the Fourier
basis, consisting of functions
wi =
[
, 
N e−𝕚πi
N , … , 
N e−𝕚πi(N−)
N
]T
,
whose components are wi,k = e−𝕚(πik∕N), where k = , … , N −. The dot product
between two vectors is
⟨wl, wm⟩=

N
N−
∑
k=
e−𝕚(πlk∕N) e−𝕚(πmk∕N) = 
N
N−
∑
k=
e−𝕚(π(l−m)k∕N),
(.)
where we want to note that if l = m then the argument of the summation is , and thus
the dot product is equal to ∕N. Otherwise, since the exponential function is a zero
mean function, the dot product is zero.
3.2.2
Vector Spaces
In the following deﬁnitions, x, y, and z are a set of vectors V ∈ℝN, and a is any complex
number, a ∈ℂ.
Deﬁnition ..(Vector space)
A vector space, (V, S), is formed by a set of vectors
V ∈ℝN and a set of scalars S ∈ℂfulﬁlling the following properties.
●Commutative: x + y = y + x.
●Associative: (x + y) + z = x + (y + z).
●Distributive with respect to scalar multiplication:
a(x + y) = ax + ay
(a + b)x = ax + bx
a(bx)
= (ab)x.
●Existence of null vector in V for the sum operation: x + 𝟎= x, where the null vector
𝟎= [, … , ]T has the same size as x.
●Existence of the inverse of a vector: x + (−x) = 𝟎.
●Existence of an identity element for scalar multiplication: ⋅x = x ⋅= x.
Deﬁnition ..(Inner product space)
An inner product space is a vector space
equipped with an inner product. The inner product allows one to deﬁne the notion of

Signal Processing Models
101
distance between vectors in a given vector space. An inner product for a vector space is
deﬁned as a function f ∶V × V →S satisfying the following properties.
●Distributive with respect vector addition: ⟨x + y, z⟩= ⟨x, z⟩+ ⟨y, z⟩.
●Scaling with respect to scalar multiplication:
⟨x, ay⟩
=
a⟨x, y⟩
⟨ax, y⟩
=
a∗⟨x, y⟩.
●Commutative: ⟨x, y⟩= ⟨y, x⟩∗.
●Self-product is positive: ⟨x, x⟩≥, and ⟨x, x⟩= ⇔x = .
From the deﬁnition of the inner product, the following properties arise:
●Orthogonality. Two nonzero vectors x and y are orthogonal, denoted as x ⟂y, if and
only if ⟨x, y⟩= .
●The 𝓁norm of a vector is deﬁned as ‖x‖ = ⟨x, x⟩∕.
●The norm satisﬁes the Cauchy–Schwartz inequality: ‖⟨x, y⟩‖ ≤‖x‖ ⋅‖y‖, and
‖⟨x, y⟩‖ = ‖x‖ ⋅‖y‖ ⇔x = ay, a ∈ℝ.
●Triangle inequality: ‖x + y‖ ≤‖x‖ + ‖y‖, and ‖x + y‖ = ‖x‖ + ‖y‖ ⇔x = ay, a ∈ℝ.
●Pythagorean theorem: for orthogonal vectors, x ⟂y, ‖x + y‖= ‖x‖+ ‖y‖.
3.2.3
Hilbert Spaces
Hilbert spaces are of crucial interest in signal processing because they allow us to deal
with signals of inﬁnite length. Note that given ℂN, letting N go to inﬁnity to obtain
ℂ∞in a standard Euclidean space will raise problems, even with trivial signals such as
xi = for all i, because operations like the inner product (and consequently the norm)
will be impossible to compute. The proper way to generalize ℂN to an inﬁnite number
of dimensions is through Hilbert spaces. Hilbert spaces impose a set of constraints
that ensure that divergence problems will not arise, and that it would be possible to
perform all the required operations, like the inner product. Essentially, the constraints
imposed in Hilbert spaces ensure that inﬁnite signals embedded into them have ﬁnite
energy.
We now have the fundamental bricks to deﬁne a Hilbert space: a vector space and
an inner product space. A Hilbert space is an inner product space (, V) that fulﬁlls
the property of completeness. Completeness is the property by which any convergent
sequence of vectors in V has a limit in V. An example of a Hilbert space is the vector
space ℂN with a sum operation and the inner product deﬁned as ⟨x, y⟩= ∑N
i=x∗
i yi,
where superscript ∗is the conjugate operation.
Deﬁnition ..(Subspace)
A subspace is a closed region of a Hilbert space, meaning
that all vector operations in that subspace remain inside it. A subspace has its own
bases describing it, just as the vector space it belongs to. Formally, given a Hilbert space
(V, S), S ∈ℂ, a subspace is a subset P ⊆V that satisﬁes that any linear combination
of vectors of that subspace also belong to the subspace. Therefore, given x, y ∈P, for
any pair of scalars a, b ∈ℂ, vector ax + ay belongs to P.

102
Digital Signal Processing with Kernel Methods
A set xi, i = , … , M, of M vectors from a subspace P is a basis for that subspace if
they are linearly independent. Also note that any vector in P can be expressed as a linear
combination of the elements of the set; that is, ∀y ∈P:
y =
M
∑
i=
aixi, ai ∈S,
(.)
which is called the Fourier analysis equation. If all vectors in a base of P are orthonormal,
then they form an orthonormal base for P, and they satisfy that ⟨xi, xj⟩= 𝛿i,j, ≤
i, j ≤M. Actually, an interesting property of orthonormal bases is that the coeﬃcients
in the Fourier analysis Equation .can be retrieved from
ai = ⟨xi, y⟩,
(.)
which are called Fourier coeﬃcients, although this term is most often used to refer to
the particular base where these coeﬃcients have the form ak = e−𝕚𝜔k. On the other
hand, Equation .is known as the synthesis formula. Another important property
of orthonormal bases is the Parseval identity, ‖y‖= ∑M
i=|⟨xi, y⟩|. From a physical
point of view, the norm of a vector is related to the energy, and the Parseval identity
is an expression of energy conservation, irrespective of the orthogonal base used for
representation.
3.2.4
Signal Models
The SVM framework for DSP that will be presented in Chapter uses several basic tools
and procedures. We will need to deﬁne a general signal model equation for considering
a time-series structure in our observed data, consisting of an expansion in terms of a
set of signals spanning a Hilbert signal subspace and a set of model coeﬃcients to be
estimated. Then, we next deﬁne a general signal model for considering a time-series
structure in our observed data, consisting of an expansion in terms of a set of signals
spanning a Hilbert signal subspace, and a set of model coeﬃcients to be estimated with
some criteria.
Deﬁnition ..(General signal model)
Given a time series y[n], we use the
samples set {yn} consisting of N + observations of y[n], with n = , … , N, which
will also be denoted in vector form as y = [y[], y[], … , y[N]]T. An expansion for
approximating the set of samples for this signal can be built with a set of signals {s(k)
n },
with k = , … , K, spanning a projection Hilbert signal subspace. This expansion is
given by
yn =
K
∑
k=
zks(k)
n + en,
(.)
where zk are the expansion coeﬃcients, to be estimated according to some adequate
criterion.
Importantly, before continuing, let us note that after this point we will be using
y[n] for denoting the inﬁnite time series, yn for denoting the set of N + time

Signal Processing Models
103
samples, and y when vector notation is more convenient for the same N + samples.
The set {s(k)
n } are the explanatory signals, which are selected in this signal model to
include the a priori knowledge about the time-series structure of the observations, and
en stands for the error approximation signal, or error approximation in each sample;
that is, yn = ̂yn + en. The estimation of the expansion coeﬃcients has to be done by
optimizing some convenient criterion, as seen next.
In the following section, we review several signal models and problems and write
them with this form as a signal model equation speciﬁed for the DSP problems which
will be addressed in Chapter . After we have deﬁned this general signal model, we
need a criterion to ﬁnd its parameters optimizing some criterion. We want to note the
following points at this moment:
) It can be assumed, without loss of generality, that estimation and regression are
diﬀerent names for the same kind of problem within diﬀerent knowledge ﬁelds.
Throughout this book, sometimes we refer to one and sometimes to the other, trying
to emphasize the use of regression for the familiar estimation from samples of a
continuous output.
) Moreover, in this and in following chapters, we will use a number of signal models
that correspond to estimation problem statements based on time samples, and also
(linear and nonlinear) regression problem statements.
For these reasons, we follow the approach of describing the general signal model in
Deﬁnition .., and the general regression model in Deﬁnition ... We also deﬁne
a general form for the optimization functional of estimation problems, in terms of a
regression model, so that it can be valid for both signal models and regression models
in subsequent chapters.
Deﬁnition ..(Optimization functional)
Given a set of observations yi and a
corresponding set of explanatory variables xi, an optimization functional is used to
estimate model coeﬃcients w. The functional consists of the combination of two terms;
namely, a loss term (sometimes referred as to cost, risk, or energy function) and a
regularization term. The framework of statistical learning theory (Vapnik, ) calls
these two terms the empirical risk emp and the structural risk str respectively, and it
seeks for predictive functions f (x, w), parametrized by a set of coeﬃcients w, such that
we jointly optimize (minimize) both terms; that is:
reg = emp + 𝜆str =
N
∑
i=
V(xi, yi, f (xi, w)) + 𝜆Ω(f ),
(.)
where V is a loss/cost/risk term acting on the N available examples for learning, 𝜆is a
trade-oﬀparameter between the cost and the regularization terms, and Ω(f ) is deﬁned
in the space of functions, f ∈.
Note that the previous functional can be readily particularized for the general signal
model using coeﬃcient vector z and the signal observations s(k)
i
to obtain prediction
functions f (s(k)
i , z). This leads to minimizing a quite similar functional given by

104
Digital Signal Processing with Kernel Methods
reg = emp + 𝜆str =
N
∑
i=
V(s(k)
i , yi, f (s(k)
i , z)) + 𝜆Ω(f ).
(.)
In order to ensure unique solutions, many algorithms use strictly convex loss functions.
The regularizer Ω(f ) limits the capacity of the classiﬁer to minimize the empirical risk
emp and favors smooth functions f such that close enough examples should provide
close-by predictions. Diﬀerent losses and regularizers can be adopted for solving the
problem, involving completely diﬀerent families of models and solutions.
Therefore, a general problem involving time-series modeling consists ﬁrst in looking
for an advantageous set of explanatory signals (hence giving the signal expansion), and
then estimating the coeﬃcients with some adequate criterion for the residuals and
for these coeﬃcients. In what follows, we will pay attention to some signal models
representing relevant DSP problems:
●In nonparametric spectral estimation, the signal model hypothesis is the sum of a set of
sinusoidal signals. The observed signal is expressed as a set of sinusoidal waveforms
acting as explanatory signals within a grid of possible oscillation frequencies, with
unknown amplitudes and phases to be determined from the data.
●In parametric system identiﬁcation and time series prediction, a diﬀerence equation
signal model is hypothesized. The observed signal is built by using explanatory
signals, which are delayed versions of the observed signal and in system identiﬁcation
problems by delayed versions of an exogenous signal.
●In system identiﬁcation with 𝛾-ﬁltering, a particular case of parametric system identi-
ﬁcation is deﬁned. Here, the signal model can be tuned by means of a single parameter
related to memory, while ensuring signal stability in the exogenous signal.
●In sinc interpolation, a band-limited signal model is hypothesized, and the explana-
tory signals are delayed sinc functions. This is known to be a very ill-posed DSP
problem, especially for nonuniform sampling and noisy conditions.
●In sparse deconvolution, a convolutional signal model is hypothesized, and the
explanatory signals are the delayed versions of the impulse response from a previously
known LTI system.
●Finally, in array processing, a complex-valued, spatio-temporal signal model is needed
to conﬁgure the properties of an array of antennas in several signal processing
applications.
All of the aforementioned signal models have a signal structure that is better analyzed
by taking into account their correlation information. In Chapter we will place them in
a framework that will allow us to indicate their diﬀerences and commonalities.
3.2.5
Complex Signal Models
Complex signal models are advantageous in those situations where the signal to be
represented is a band-pass signal around a given frequency. Assume an arbitrary real-
valued discrete-time signal y[n]. It is immediate to prove that its FT Y(Ω) is Hermitic
around the origin; that is, its real part is symmetric with respect to the vertical axis, and
its imaginary part is antisymmetric with respect to the coordinates origin.

Signal Processing Models
105
Re{Y(Ω)} =
∞
∑
n=−∞
y[n] cos(Ωn),
(.)
Im{Y(Ω)} = −
∞
∑
n=−∞
y[n] sin(Ωn),
(.)
Actually, from which follows that, when signal y[n] is real, its FT is Hermitic. Now
assume that the signal is band-pass; that is, that its FT is diﬀerent from zero between
two given frequencies 𝜔and 𝜔, and hence it is also nonzero between −𝜔and −𝜔
since it is real, and thus Hermitic around the origin. For this reason, both positive and
negative intervals will carry a signal with the same information.
Let us now assume an arbitrary central frequency 𝜔deﬁned by convenience as
𝜔≤𝜔≤𝜔. A base-band representation ̃y[n] of the signal that carries all the
information can be used by representing just the positive part of the signal shifted to
the origin a quantity equal to the central frequency. The original FT of the signal can
then be represented as
Y(Ω) = ̃Y(Ω) e𝕚𝜔+ ̃Y(−Ω) e−𝕚𝜔.
(.)
It is easy to prove that this signal is Hermitic, and hence its inverse FT is real.
Owing to the above representation, ̃Y(Ω) is often called the complex envelope of Y(Ω).
Nevertheless, ̃Y(Ω) does not need to be Hermitic, so its inverse FT is not necessarily
real, but instead it is complex in general. The inverse FT of Y(Ω) is
y[n] = e𝕚𝜔
∫
∞
−∞
̃Y(Ω) e−𝕚𝜔n dΩ + e−𝕚𝜔
∫
∞
−∞
̃Y(−Ω)e−𝕚Ωn dΩ.
(.)
The second integral on the right side of the equation corresponds to the inverse FT of
̃y∗[n]. This leads to
y[n] = e𝕚𝜔̃y[n] + e−𝕚𝜔̃y∗[n] = Re{̃y[n]} cos(Ωn) −Im{̃y[n]} sin(Ωn).
(.)
The real and imaginary parts Re{̃y[n]} and Im{̃y[n]} of complex envelope ̃y[n] are
often called in-phase and quadrature-phase signals, and denoted as yI[n] and yQ[n].
Equation .can be rewritten as
y[n] = Re{̃y[n] e𝕚𝜔n} = yI[n] cos(Ωn) −yQ[n] sin(Ωn)
(.)
and the envelope is expressed as ̃y[n] = yI[n] + iyQ[n].
3.2.6
Standard Noise Models in DSP
In Section ..we revised the main aspects of the signal models encountered in DSP.
While the vast majority of the problems in DSP are tackled by ﬁtting a signal model
that captures the relations between the observed data, it is imperative to describe the
behavior of the noise or disturbances in the acquired data. Remember that a key for
the estimation of parameters is to deﬁne a proper loss function, and the residuals

106
Digital Signal Processing with Kernel Methods
−5
0
5
0
0.5
1
1.5
Gaussian
Laplacian
Gamma
Rayleigh
Uniform
Figure 3.1 Examples of pdfs for widely used noise models.
minimization is one of its components. On the one hand, it is typically assumed that
the observations, called now x[n], come from the direct sum of a signal s[n] and a noise
component e[n]; thus, x[n] = s[n] + e[n]. In this case, one says that an additive noise
model was assumed. While this additive assumption is very convenient to formalize
the problems (either function approximation, deconvolution, or unmixing), it is widely
acknowledged to be somewhat limited. Multiplicative noise given by x[n] = s[n]e[n] is
a better assumption in some application ﬁelds (e.g., in radar signal processing). In such
cases, a simple logarithmic transformation of the observations returns the problem to
the additive setting; that is, log x[n] = log s[n] + log e[n].
On the other hand, even though many noise sources in DSP problems can be modeled
to fairly follow a Gaussian model (e.g., thermal noise in acquisition systems), many sys-
tems are governed by non-Gaussian and colored (noise power is frequency dependent)
noise sources. Finally, we should introduce the assumption of i.i.d. observations being
ubiquitous in statistics and signal processing.
Common noise distributions used in signal processing are the Gaussian, the Lapla-
cian, or the uniform. Gaussian noise provides a good noise model in many signal and
image systems aﬀected by thermal noise. The probability density function (pdf) in this
case is
pe(e|𝜎) =

π𝜎exp(−e∕𝜎),
(.)
where e refers to the noise and 𝜎accounts for the standard deviation (see Figure .). The
Gaussian distribution has an important property: to estimate the mean of a stationary
Gaussian random variable, one cannot do any better than the linear average. This makes
Gaussian noise a worst-case scenario for nonlinear restoration ﬁlters, in the sense that
the improvement over linear ﬁlters is least for Gaussian noise. To improve on linear
ﬁltering results, nonlinear ﬁlters can exploit only the non-Gaussianity of the signal
distribution.

Signal Processing Models
107
A second important pdf is the Laplacian noise model, also called the bi-exponential,
which follows the pdf given by
pe(e|𝜎) =

√
𝜎
exp(−
√
|e|∕𝜎),
(.)
where e refers to the noise, and 𝜎accounts for the standard deviation (see Figure .).
The bi-exponential distribution is particularly useful for modeling events that occur
randomly over time. Nonlinear estimators can provide a much more accurate estimate
of the mean of a stationary Laplacian random variable than the linear average.
Other common pdfs in signal processing applications are, ﬁrst, the Gamma distribu-
tion (see Figure .), modeled by
pe(e|a, b) =
ea−
ba𝛤(a) exp(−e∕b),
(.)
where 𝛤(⋅) is the Gamma function, and a and b are parameters controlling the shape
and scale of the pdf; and second, the Rayleigh distribution (see Figure .), expressed as
pe(e|c) = e
cexp(−e∕c),
(.)
where c is a scale parameter. The Gamma distribution is present, for instance, in
reliability models of lifetimes for some natural processes, while the Rayleigh (along with
Nakagami and Rician distributions) commonly appears in communications theory to
model scattered signals that reach a receiver by multiple paths (depending on the density
of the scatter, the signal will display diﬀerent fading characteristics).
Finally, we mention the case of uniform noise models. Even though they are not often
encountered in real-world systems, they still provide a useful comparison with Gaussian
noise sources. The linear average is comparatively a poor estimator for the mean of a
uniform distribution, and this implies that nonlinear ﬁlters should be better in removing
uniform noise than Gaussian noise. Figure .illustrates the aforementioned pdf for
zero-mean, unit-variance noise.
3.2.7
The Role of the Cost Function
Many of the inference problems that we will see throughout this book will require
explicit or implicit assumptions about both the signal and the noise distributions.
The standard assumption about the signal is that the observations are i.i.d., which we
actually point out as one of the most important problems of the standard machine
learning approximation to signal processing problems. When talking about the noise,
one typically observes that the approaches rely on two important assumptions: ﬁrst, the
independence between the signal and the noise counterparts; and second, the implicit
assumption of a noise model through the minimization of a particular cost (or loss)
function.
On the one hand, the SNR is deﬁned as the ratio of signal power to the noise
power. This concept has been extraordinarily useful in signal processing for decades,
since it allows us to quantify the quality and robustness of systems and models.

108
Digital Signal Processing with Kernel Methods
Noise reduction is an issue in signal processing, typically tackled by controlling the
acquisition environment. Alternatively, one can ﬁlter the signal by looking at the noise
characteristics. As we will see, ﬁlter design and regularization are intimately related
topics, since maximizing the SNR can be cast as a way to seek for smooth solutions
preserving signal characteristics and discarding features mostly inﬂuenced by the noise.
In this context, one typically looks for transformations of the observed signal such
that the SNR is maximized, or alternatively for transformations that minimize the
fraction of noise. This issue will be treated in detail in Chapter by introducing kernel
transformations suited to cope with signal-to-noise correlations.
On the other hand, one admits that most of the signal processing problems can be cast
as estimation problems, known in statistics as curve ﬁtting or function approximation,
and in machine learning simply as regression. Regression models involve the following
variables: the unknown parameters (weights) w, the independent variables (input
variables, features) x, and the dependent variable (output, target, observation) to be
approximated y. A model can be deﬁned as a function f that relates y with x; that is,
y = f (x, w)+r, where r = y−f (x, w) are called residuals, discrepancies, or simply errors.
The question here reduces to choosing a good set of weights that allows the signal model
to perform well in unseen (test) data. Many techniques are available in the literature, but
mostly involve selecting a quality (performance, objective) criterion to be optimized.
Taking into account Deﬁnition .., the ﬁrst task is to select a cost/loss/risk/energy
function. The most common cost functions are depicted in Figure ..
Once the loss is selected, the selection of the parameters and the model weights is
the second crucial issue. A standard approach relies on the ML estimation. In statistics
in general and in signal processing in particular, ML plays a crucial role as perhaps
−3
−2
−1
0
1
2
3
0
1
2
3
4
y−f(x)
Square
ε−insensitive
Huber
Figure 3.2 Standard cost functions (losses) in signal processing and in machine learning.
Note that the observation noise is diﬀerent from the residuals, as far as the former refers to the error in
the abstract mathematical signal or data model, whereas the latter refers to the approximation errors after
the model has been ﬁt to the data or observations.

Signal Processing Models
109
Table 3.1 Common cost functions and their assumed noise models.
Loss/cost
emp(y, f(x, w))
Noise model, p(e)
Squared loss
.(y −f (x, w))

√
π exp(−e∕)
Absolute loss, Laplacian
|y −f (x, w)|

exp(−|e|)
𝜀-insensitive loss
(|y −f (x, w)| −𝜀)+

√
(+𝜀) exp(−|e|𝜀)
Huber loss
{
e
|e| ≤𝛿
𝛿(|e| −𝛿
)
otherwise
{
exp(−e∕)
|e| ≤𝛿
exp(𝛿∕−|e|)
otherwise
one of the most widely used methods for estimating the parameters of a statistical
model. Roughly speaking, ML reduces to ﬁnding the most likely function (model) that
generated the data, and hence this criterion tries to maximize
max
w {p(x, y|w)} = max
w
{
∏
i
p(xi, yi|w)
}
= max
w
{
∏
i
p(yi|xi, w)p(xi)
}
= max
w
{
∏
i
p(yi|xi, w)
}
≈max
w
{−log(p(x, y|w))}
(.)
When a particular noise model is used in this optimization problem, a likelihood
function is obtained, thus giving rise to a particular noise model. Table .summarizes
the main cost functions and the associated noise models that are implicitly assumed.
Detailed derivation of the ML relationship between the cost function and the noise
models can be found in Smola and Schölkopf ().
3.2.8
The Role of the Regularizer
The minimization of a particular function yields a model that can be used to make
predictions by using new incoming data. However, the problem of generalization arises
here. Generalization is the capability of a method to extrapolate to unseen situations;
that is, the function f should accurately predict the label y∗∈for a new input
example x∗∈, i.e. ̂y∗= f (x∗, w). Generalization has recurrently appeared in the
statistics literature for decades under the names of bias–variance dilemma, capacity,
and complexity control.
The underlying idea in regularization is to constrain the ﬂexibility of a class of
functions in order to avoid overﬁtting to the training data. For example, it is well
known that if one aims to ﬁt ﬁve points with a polynomial of order , the curve
will pass through all of them, being a too-ﬁtted and too-wavy function unable to
estimate well other unseen points. Thus, the class of polynomial functions is a ﬂexible
one, and one has to control it, for example, selecting lower orders or constraining
coeﬃcients variance. We will come back later to this issue, which is fundamental in
kernel machines. Actually, regularization is one of the keys to prevent overﬁtting and
to simplify the solution. The idea is essentially to explicitly (or implicitly) penalize
the excess of model capacity based on the number of eﬀective parameters. Another

110
Digital Signal Processing with Kernel Methods
perspective to the issue of regularization is provided by Bayesian inference, as far as
Bayesian methods use a prior probability that gives lower probability to more complex
models. Either way, selecting an appropriate regularizer is key in statistical inference.
According to Deﬁnition .., selecting regularizer Ω(f ) is our second task to fully
deﬁne the optimization functional.
Regularizer Ω(f ) plays the role of encoding prior knowledge about the signal (and
possibly noise) characteristics. Very often, the regularizer acts on model weights,
imposing a particular (expected or desired) characteristic. Deﬁning the regularizer
typically reduces to acting on model weights norms. Recall that the q-norm is deﬁned
as ‖w‖q = (∑
i |wi|q)∕q and the zero-norm ‖w‖reduces to compute the sum of all
nonzero entries in the vector w. For example, one may wish to limit their variance
(to prevent unstable predictions in unseen data) or the number of active coeﬃcients
(to achieve simple, sparse, compact models). The former strategy uses an 𝓁-norm of
model weights (or coeﬃcients), while the latter would aim to use an 𝓁-norm which
essentially accounts for the number of nonzero components in the vector w. In this
context, Tikhonov regularization is the most well-known and widely used regularizer in
machine learning and statistics.
Deﬁnition ..(Tikhonov regularization)
Given a linear function f parametrized
by w, the Tikhonov regularizer (Tikhonov and Arsenin, ) constrains some form of
energy of the function Ω(f ) = ‖f ‖
H = w⊤Hw. Matrix H is known as the regularization
matrix, and in many approaches the identity matrix is chosen in order to force minimum
norm solutions.
Many models have actually used 𝓁and 𝓁norms as alternatives to the more amenable
and tractable 𝓁-norm. The rationale behind this choice is that Ω = ‖w‖promotes
sparsity of the solution and constitutes a relaxation of the more computationally
demanding problem induced by using the 𝓁-norm. A geometrical interpretation of
the most commonly used regularizers is depicted in Figure .. It can be shown that:
() for q < , the regularizer is not convex and hence the corresponding function
is not a true norm; () for q ≥, large values become the dominant ones and the
optimization algorithm will penalize them to reduce the error, so that the overall cost
is reduced. Interestingly, it can be shown that: () the regularizer is not convex for
values q < ; () the value q = is the smallest one for which convexity is retained;
() for large values of q > , the contribution of small values of the regularization
function to the respective norm becomes insigniﬁcant. Actually, the extreme case is
when one considers the 𝓁-norm, because even a small increase of a component from
zero makes its contribution to the norm very large, so making an element nonzero has
q = 4
q = 2
q = 1
q = 0.5
q = 0.1
Figure 3.3 Geometrical interpretation of the most commonly used regularizers. The isocurves for
different values of q = {4, 2, 1, 0.5, 0.1} in the 2D space. Note that for the 𝓁0-norm, the respective
values cover the two axes with the exception of the point (0, 0). For the 𝓁1-norm the isovalue curve is
a rhombus, and for the 𝓁2-norm (Euclidean) norm it is a circle.

Signal Processing Models
111
Table 3.2 Combination of cost functions and regularizers result in different forms
of statistical inference models.
Model
Loss
Regularizer
Akaike information criterion/Bayesian information
criterion (Akaike, )
(y−f (x, w))
‖w‖
Ridge regression, kriging, kernel ridge regression/-
Gaussian process (Rasmussen and Williams, )
(y−f (x, w))
‖w‖
Support vector regression (Smola and Schölkopf,
)
|y−f (x, w)|𝜀
‖w‖
LASSO (Tibshirani, )
(y−f (x, w))
‖w‖
Basis pursuit denoising (Chen et al., )
(y−f (x, w))
‖w‖
Least absolute deviations (LADs), errors (LAEs),
residuals (LARs) (Bloomﬁeld and Steiger, )
|y −f (x, w)|
‖w‖
a dramatic impact on the solution. Note that diﬀerent losses and regularizers can be
adopted for solving an inference problem. Each combination will lead to a completely
diﬀerent family of models and solutions (see Table .and references therein).
3.3
Digital Signal Processing Models
The basic assumption in the vast majority of statistical inference problems is that the
observations are i.i.d. This assumption of independence between samples is not fulﬁlled
in DSP problems in general, and in time-series analysis in particular; instead, statistical
similarity among samples plays a key role in their analysis. Algorithms that do not take
into account temporal (or spatial) dependencies are ignoring relevant structures of the
analyzed signals, such as their autocorrelation or their cross-correlation functions.
The list of examples of this is very long. As a reminder, the following are some
examples. The acquired signals in many system identiﬁcation and control problems
give rise to strong temporal correlations and fast decaying spectra. Speech signals are
usually coded in digital telephony and media, proﬁting from the temporal correlation
in small signal windows of some milliseconds in which stationarity can be assumed and
providing good voice signal representation by just using a few modeling coeﬃcients.
Audio coding techniques for high-ﬁdelity daily applications greatly exploit the temporal
redundancy of the signals. Biomedical signals, such as ECGs or electroencephalo-
grams (EEGs), usually exhibit well-deﬁned spectral characteristics, which are intimately
related to the autocorrelation properties, and allow the use from simple DSP techniques
(such as ﬁltering) to sophisticated analysis (such as power law representations). Also,
current models of natural images assume they are spatially smooth signals, as far as
the joint pdf of the luminance samples is highly uniform, the covariance matrix is
highly nondiagonal, and the autocorrelation functions are broad and have generally a
∕f band-limited spectrum. And in the case of color images, the correlation between
the tri-stimulus values of the natural colors is typically high.

112
Digital Signal Processing with Kernel Methods
Standard machine learning and signal processing approaches typically consider
signals as a collection of independent samples, mainly because the estimation of the
underlying pdf is very diﬃcult, and the random sampling of the input space is a
convenient (if not the only) assumption allowing to aboard this problem. This, however,
can sometimes be alternatively alleviated by introducing prior knowledge on the signal
and noise structure, and by imposing suitable signal models (as we will see throughout
this and the next chapters).
Several signal models have been paid attention in signal processing and information
theory, whose signal structure is better analyzed by taking into account their particular
features. This section builds upon Deﬁnition ..of the general signal model and
introduces speciﬁc signal models for the most common problems in DSP; namely,
nonparametric spectral estimation, system identiﬁcation, interpolation, deconvolution,
and array processing. These signal models will be used in Chapter for building speciﬁc
algorithms for particular applications in DSP.
3.3.1
Sinusoidal Signal Models
Nonparametric spectral analysis is usually based on the adjustment of the amplitudes,
frequencies, and phases of a set of sinusoidal signals, so that their linear combination
minimizes a given optimization criterion. In general, this adjustment is a hard problem
with local minima; thus, a simpliﬁed solution consists of the optimization of the
amplitudes and phases of a set of orthogonal sinusoidal signals in a grid of previously
speciﬁed oscillation frequencies. This is the basis of the classical nonparametric spectral
analysis.
When the signal to be spectrally analyzed is uniformly sampled, the LS criterion
yields FT-based methods, such as the Welch periodogram and the Blackman–Tukey
correlogram (Marple, ), which establish a bias–variance trade-oﬀand allow one
to adjust the frequency resolution in terms of the application requirements. These
estimators are based on the FT representation of the observed time series and on its
estimated autocorrelation function respectively. However, when the signal is nonuni-
formly (unevenly) sampled in time, the basis of the signal Hilbert space is chosen such
that their in-phase and quadrature-phase components are orthogonal at the uneven
sampling times, which leads to the Lomb periodogram (Lomb, ). Methods based
on the FT are computationally lighter than those based on LS. However, the latter are
optimal for the case of Gaussian noise, but more sensitive to the presence of outliers,
which is often encountered in communications applications.
The following is a signal model formulation for sinusoidal representation of a given
signal in the presence of noise. This signal model is widely known and was analyzed in
detail in Rojo-Álvarez et al. ().
Deﬁnition ..(Sinusoidal signal model)
Given a set of observations {yn}, which
is known to present a spectral structure, its signal model can be stated as
̂yn =
K
∑
k=
zks(k)
n = A+
K
∑
k=
Ak cos(k𝜔tn + 𝜙k)
=
K
∑
k=
(ck cos(k𝜔tn) + dk sin(k𝜔tn)),
(.)

Signal Processing Models
113
where angular frequencies are assumed to be previously known or ﬁxed in a regular
grid with spacing 𝜔; Ais the mean value; Ak and phik are the amplitudes and phases
of the kth components, and ck = Ak cos(𝜙k) and dk = Ak sin(𝜙k) are the in-phase
and in-quadrature model coeﬃcients, respectively; and {tn} are the (possibly unevenly
separated) sampling time instants.
Note that the sinusoidal signal model corresponds to the general signal model in
Deﬁnition ..for zk given by ck and dk, and for {s(k)
n } given by sin(k𝜔tn) and
by cos(k𝜔tn). Note that this signal model also accounts for the spectral analysis of
continuous-time unevenly sampled time series.
3.3.2
System Identification Signal Models
A common problem in DSP is to model a functional relationship between two simulta-
neously recorded discrete-time processes (Ljung, ). When this relationship is linear
and time invariant, it can be addressed by using an ARMA diﬀerence equation, and
when a simultaneously observed signal {xn} is available, called an exogenous signal, the
parametric model is called an ARX signal model for system identiﬁcation. Essentially, in
these cases of (parametric) system identiﬁcation and time series prediction, the signal
model is driven by one or several diﬀerence equations, and the explanatory signals are
delayed versions of the same observed signal, and possibly (for system identiﬁcation) by
delayed versions of an exogenous signal.
In this section we introduce two signal models for system identiﬁcation that will be
used in the following chapters when dealing with kernel functions; namely, the ARX
signal model and a particular instantiation of an ARMA ﬁlter called the 𝛾-ﬁlter. The
section also presents the state-space models and signal models which are used for
recursion in many DSP applications.
Real and Complex Autoregressive Exogenous Signal Models
The signal model for ARX time series can be stated as follows.
Deﬁnition ..(ARX signal model)
Given a set of output observations {yn} from a
system, and its simultaneously observed input signal {xn}, an ARX signal model can be
stated between them in terms of a parametric model described by an ARMA diﬀerence
equation, given by delayed versions of both processes:
̂yn =
K
∑
k=
aks(k)
n =
M
∑
m=
amyn−m +
L
∑
l=
blxn−l,
(.)
where {xn} is the exogenous signal; am and bl are the AR and the exogenous (X) model
coeﬃcients respectively, and the system identiﬁcation is an ARX signal model.
Note that the ARX system identiﬁcation model is the general signal model in Deﬁni-
tion ..for zk given by am and bl, and s(k)
n given by yn−m and xn−l.
An important ﬁeld of application of DSP can be found in digital communications
problems, such as array processing, equalization, channel estimation, and multiuser
detection. In these applications, the complex envelope of modulated signals is
commonly used. For instance, many important communications systems, such as

114
Digital Signal Processing with Kernel Methods
Universal Mobile Telecom Service, perform modulation through quadrature schemes,
like quadrature-phase shift keying (QPSK). Other modulation schemes (for instance,
Digital Video and Audio Broadcasting) use quadrature amplitude modulations
(QAMs). In all these cases, receivers need to deal with complex signals. The statement of
complex-valued ARX signal models is also remarkable in the ﬁeld of DSP, especially for
communication applications dealing with many channels and complex constellations.
System Identification with Constrained Recursivity
Identiﬁcation by means of parametric structures using AR substructures may eventually
lead to unstable solutions if no constraints regarding the allocation of the system poles is
considered, which is not easy to design. A reasonable solution to force stability consists
of using structures that are intrinsically stable. This is the case of the 𝛾-ﬁlter structure
introduced in Principe et al. (). The ﬁlter is a parsimonious structure that has been
used for echo cancellation (Palkar and Principe, ), time-series prediction (Kuo
et al., ), system identiﬁcation (Principe et al., ), and noise reduction (Kuo and
Principe, ). This structure provides stable models, while allowing the measurement
of the memory depth of a model (i.e., how much past information the model can retain)
in a very simple and intuitive way. As seen in Figure ., the 𝛾-ﬁlter structure is deﬁned
by using the diﬀerence equations of the linear ARMA model of a discrete time series yn
as a function of a given input sequence xn as follows.
Deﬁnition ..(𝛾-ﬁlter signal model)
Given an observed signal {yn} and a simul-
taneously observed exogenous signal {xn}, a parametric signal model can be stated
between them by a 𝛾-structure given by
yn =
P
∑
p=
wpxp
n + en
(.)
1–μ
1–μ
+
+
+
+
+
μ
μ
+
1–μ
xP
n
xn
3
xn
2
xn
wP
w2
w1
w0
Z –1
Z –1
Z–1
yn
Figure 3.4 The 𝛾structure. The 𝛾filter can be regarded as a cascade of IIR filters where loops are kept
local and loop gain is constant.

Signal Processing Models
115
xp
n =
⎧
⎪
⎨
⎪⎩
xn,
p = 
(−𝜇)xp
n−+ 𝜇xp−
n−,
p = , … , p
(.)
where 𝜇is a free parameter of the model and wp are the model coeﬃcients.
The structure is stable for < 𝜇< if the transfer function is low pass, and for
< 𝜇< if it is high pass. For 𝜇= the recursion vanishes and the ﬁlter has the
Widrow adaline structure. Since P is the number of samples, note that M = P∕𝜇is an
approximate measure of memory depth (Principe et al., ). Other IIR structures with
restricted recursivity have been developed, such as Laguerre- and gamma-II-type ﬁlter
structures (De Vries and Principe, ), whose signal models can be trivially derived
in a similar way.
State-Space Models and Signal Models for Recursion
A generalization of the Wiener ﬁlter theory to dynamical systems with random inputs
was developed by Kalman in the s in the context of state-space models (Kalman,
). Discrete-time (and dynamical) linear ﬁlters can be nicely expressed in the state-
space formulation (or Markovian) representation, by which two coupled equations are
deﬁned as seen in Chapter . State-space models give rise to the important property
of recursivity in the system equations. Given a set of samples {xn, yn}N
n=, an arbitrary
(Markovian) signal model representation of recursion between input–output time-
series pairs can be deﬁned as
yn = f (⟨wi, xi
n⟩|𝜃f ) + ex
n
xi
n = g(xi
n−k, yn−|𝜃g) + ey
n,
∀k > , l ≥
(.)
where xi
n is the signal at the input of the ith ﬁlter tap at time n, yn−l is the previous output
at time n −l, ex is the state noise, and ey is the measurement noise. Here, f and g are
linear functions parametrized by model parameters wi and hyperparameters 𝜃f and 𝜃g.
The KF is the most widely known instantiation of a state-space formulation. Also note
that, when a linear model is imposed for f (⋅) and g(⋅), the AR and MA ﬁlter structures
for time-series prediction readily emerge from Equation ., since a conventional MA
ﬁlter structure is given by
yn =
L
∑
l=
blxn−l + en,
(.)
and a conventional AR ﬁlter structure is deﬁned by
yn =
M
∑
m=
amyn−m + en.
(.)

116
Digital Signal Processing with Kernel Methods
The state-space representation is especially powerful for MIMO linear systems, as well
as for time-varying linear systems, and it has found many applications in the ﬁelds of
system identiﬁcation, economics, control systems, and communications (Ljung, ).
3.3.3
Sinc Interpolation Models
In sinc interpolation, a band-limited signal model is hypothesized, and the explanatory
signals are delayed sincs. The sinc kernel provides the perfect reconstruction of an
evenly sampled noise-free signal (Oppenheim and Schafer, ). In the presence of
noise, the sinc reconstruction of a possibly nonuniformly sampled time series is an ill-
posed problem (Choi and Munson, a; Yen, ; Ying and Munson, ). The
main elements that are considered by signal interpolation algorithms in the literature
are the type of sampling (uniform or nonuniform), the noise (present or not), the
spectral content (band-limited or not), and the numerical regularization adopted (none,
Tikhonov, or other forms). Despite of the great amount of work developed to date in this
setting, the search for new eﬃcient interpolation procedures is still an active research
area. Accordingly, we propose to use the following signal model for sinc interpolation
in DSP.
Deﬁnition ..(Sinc kernel signal model)
Let x(t) be a band-limited, possibly
Gaussian noise-corrupted, signal and be {xk = x(tk), k = , … , N}, a set of N + 
nonuniformly sampled observations. The sinc interpolation problem consists of ﬁnding
an approximating function ̂x(t) ﬁtting the data, ̂x(t) = ∑N
k=fksinc(𝜎(t −tk)). The pre-
vious continuous-time model, after nonuniform sampling, is expressed as the following
discrete-time model:
xn = ̂xn + en =
N
∑
k=
fksinc(𝜎(tn −tk)) + en,
(.)
where sinc(t) = sin(t)
t , and 𝜎=
π
Tis the sinc units bandwidth.
Therefore, we are using an expansion of sinc kernels for interpolation of the observed
signal. The sinc kernel interpolation signal model straightforwardly corresponds to the
general signal model in Deﬁnition ..for explanatory signals s(k)
n corresponding to
sinc(𝜎(tn −tk)). An optimal band-limited interpolation algorithm, in the MMSE sense,
was ﬁrst proposed in Yen (). As explained next, note that in Yen’s algorithm we
have as many free parameters as observations, so that this is an ill-posed problem. It can
be checked that in the presence of (even a low level of) noise, some of the coeﬃcient
estimations grow dramatically, leading to huge interpolation errors far away from the
observed samples. To overcome this limitation, the regularization of the quadratic loss
was also proposed in the same work.
In this section, two nonuniform interpolation algorithms are presented; namely, the
Wiener ﬁlter (Kay, ) and the Yen regularized interpolator (Kakazu and Munson
; Yen ). Although many more interpolation methods have already been pro-
posed in the literature, we limit ourselves to these cases for two reasons: () they
are representative cases of optimal algorithms (with a diﬀerent inductive principle for

Signal Processing Models
117
each case); and () they have a straightforward spectral interpretation, which allows an
interesting comparison with the algorithms proposed in later chapters.
Wiener Filter for Nonuniform Interpolation
For deﬁning the Wiener ﬁlter in nonuniform interpolation, the following signal model
and slightly diﬀerent notation are followed. Let x(t) be a continuous-time signal with
ﬁnite energy, consisting of a possibly band-limited signal z(t), which can be seen as
a realization of a random process, corrupted with additive noise e(t); that is, x(t) =
z(t)+e(t), where the noise is modeled as a zero-mean wide sense stationary process. This
signal has been observed on a set of N unevenly spaced time instants, {tn}, n = , … , N,
obtaining the set of observations x = [x(t), … , x(tn), … , x(tN)]T. Then, the nonuniform
interpolation problem consists of ﬁnding a continuous-time signal ̂z(t) that approxi-
mates the noise-free interpolated signal in a set of K time instants, {t′
k, k = , … , K}.
As described by Kay (), a Bayesian approach to solve this problem amounts to
the Wiener ﬁlter. Assuming that z(t) is zero mean, the linear estimator is given by
̂z(t′
k) = aT
kx
for k = , … , K,
(.)
where ak is the Wiener ﬁlter estimator for the signal in time instant tk. The scalar MMSE
estimator is obtained when ak is chosen to minimize the MSE and takes the following
form (Kay, ):,
̂z(t′
k) = rT
zkC−
xxx
for k = , … , K
(.)
Vector rzk contains the cross-covariance values between the observed signal and the
signal interpolated at time t′
k; that is:
rzk = [rzz(t′
k −t), … , rzz(t′
k −tN)]T,
(.)
where rzz(𝜏) is the autocorrelation of the noise-free signal for a time shift 𝜏. Note that
Cxx is the covariance matrix of the observations and, assuming wide sense stationary
data with zero mean, it is computed as
Cxx = Rzz + Rnn,
(.)
where Rzz is the autocovariance matrix of the signal with component i, j given by
Rzz(i, j) = rzz(ti −tj),
(.)
and Rnn is the noise covariance matrix. For the i.i.d. case, Rnn = 𝜎
nIN, with 𝜎
n the noise
power and IN the identity matrix of size N × N. Thus, ̂z(t′
k) is given by
̂z(t′
k) = ((Rzz + 𝜎
nIN)−rzk)Tx.
(.)
Although the solution in Equation .is optimal in the MMSE sense, two main
drawbacks can arise when using it in real practice:

118
Digital Signal Processing with Kernel Methods
) It implies the inversion of a matrix that can be almost singular, so the problem can
become numerically ill-posed.
) The knowledge of the autocorrelation of the signal rzz(𝜏) at every 𝜏= ti−tj is needed,
so it must be estimated from the observed samples, which is not an easy task in
general.
A frequency-domain analysis can be done from this Wiener ﬁlter for interpolation.
The solution of the MMSE estimator given by Equation .can be seen as the
convolution of the observations with a ﬁlter with impulse response h(k)
W [n] = a[k −n].
For a ﬁnite number of nonuniform samples, the solution cannot be converted into
a time-invariant ﬁlter since it depends on the k index, which is signiﬁcantly diﬀerent
with the uniform-sampling case. However, in order to provide a simple spectral inter-
pretation of the interpolator we assume that N →∞and then Equation .can be
approximated as the convolution of the observations with a time-invariant ﬁlter with
response hW[n], which does not depend on the time index k (Kay, ); that is:
̂z(t′
k) =
∞
∑
n=−∞
hW[n]x(tk −tn).
(.)
In this case, the coeﬃcients of the ﬁlter hW[n] can be computed by using the Wiener–
Hopf equations, also known as the normal equations. By applying the FT to these
equations, the transfer function of the ﬁlter is ﬁnally obtained:
HW(Ω) =
Pzz(Ω)
Pzz(Ω) + Pee(Ω) =
𝜂(Ω)
𝜂(Ω) + ,
(.)
where Pzz(Ω) and Pee(Ω) are the PSD of the original signal and the noise respectively,
and
𝜂(Ω) = Pzz(Ω)
Pee(Ω)
(.)
represents the local SNR in frequency Ω. Obviously, ≤HW(Ω) ≤, tending to unity
(to zero) in spectral bands with high (low) SNR. Hence, the Wiener ﬁlter enhances
(attenuates) the signal in those bands with high (low) SNR, and the autocorrelation of
the process to be interpolated is a natural indicator of the relevance of each spectral
band in terms of SNR.
Yen Regularized Interpolator
Inspired by Shannon’s sampling theorem, a priori information can be used for band-
limited signal interpolation by means of a sinc kernel. In this case, the signal is modeled
with a sinc kernel expansion as
x(t′
k) = z(t′
k) + e(t′
k) = f Tsk + e(t′
k),
(.)
for k = , … , K, with sk being an N × column vector with components
sk(i) = sinc(𝜎(t′
k −tn)),
(.)

Signal Processing Models
119
where sinc(t) = sin(t)∕t and parameter 𝜎= π∕Tis the sinc function bandwidth. Then,
the interpolator can be stated as
̂z(t′
k) = f Tsk
for k = , … , K.
(.)
When LS strategy is used to estimate f , the Yen solution is obtained; see Theorem IV in
Yen (). If a regularization term is used to prevent numerical ill-posing, f is obtained
by minimizing
reg = ‖x −Sf ‖+ 𝜆‖f ‖,
(.)
where S is a square matrix with elements S(n, k) = sinc(𝜎(tn −t′
k)), and 𝜆tunes the
trade-oﬀbetween solution smoothness and the errors in the observed data. In this case,
f is given by
f = (S+ 𝜆𝐈N)−Sx.
(.)
In this case, the use of the regularization term leads to solutions that are suboptimal in
the MSE sense.
Similar to the previous Wiener interpolation, the frequency-domain analysis can be
established here as follows. An asymptotic analysis similar to the one presented for the
Wiener ﬁlter can be done based on Equations .and .. Using a continuous-time
equivalent model for the interpolation algorithm (see Rojo-Álvarez et al. () for
further details), the interpolation algorithm can be interpreted as a ﬁltering process
over the input signal; that is:
̂z(t) = hY(t) ∗x(t),
(.)
where ∗again denotes the convolution operator. Now, the transfer function of the ﬁlter
is given by
HY(Ω) =
Pss(Ω)
Pss(Ω) + 𝜆,
(.)
where Pss(Ω) is the PSD of sinc(𝜎t), and since this one is deterministic, Pss(Ω) = |S(Ω)|,
with S(Ω) the FT of the sinc function. This is just a rectangular pulse of width 𝜎. Note
now that HY(Ω) takes the value ∕(+ 𝜆) inside the pass band of Pss(Ω) and outside.
Therefore, if 𝜎is equal to the signal bandwidth, the ﬁlter attenuates the noise outside
the signal band and does not aﬀect the components inside the band. A comparison
between Equations .and .reveals that both interpolators can be interpreted as
ﬁlters in the frequency domain, but in the case of Yen’s algorithm the local SNR 𝜂(Ω) is
simply approximated by the sinc kernel PSD, Pss(Ω).
Remarks
It can be seen that both Yen and Wiener ﬁlter algorithms use the LS (regularized for Yen
method) criterion. However, the Wiener algorithm is linear with the observations, and it
does not assume any a priori decomposition of the signal in terms of building functions.

120
Digital Signal Processing with Kernel Methods
Instead, it relies on the knowledge of the autocorrelation function, which can be hard
to estimate in a number of applications. Alternatively, Yen’s algorithm is nonlinear with
respect to the observations and assumes an a priori model based on sinc kernels. Hence,
the knowledge of the signal autocorrelation is not needed. As seen in later chapters, the
SVM interpolation uses a diﬀerent optimization criterion, which is the structural risk
minimization (SRM), and its solution is nonlinear with respect to the observations since
it assumes a signal decomposition in terms of a given Mercer kernel.
3.3.4
Sparse Deconvolution
In sparse deconvolution, the signal model is given by a convolutional combination
of signals, and the explanatory signals are the delayed and scaled versions of the
impulse response from a previously known LTI system. More speciﬁcally, the sparse
deconvolution problem consists on the estimation of an unknown sparse sequence
which has been convolved with a time series (impulse response of the known system)
in the presence of noise. The non-null samples of the sparse series contain relevant
information about the underlying physical phenomenon in a number of applications.
One of the main ﬁelds for sparse deconvolution is seismic signal analysis, where the
users send a known waveform through the ground and then collect the echoes produced
in the interfaces between two layers of diﬀerent materials. The returning signal is the
convolution of a waveform hn and a sparse time series xn (ideally, a sparse train of
Kronecker delta functions) that contains information about the depth of diﬀerent layers.
Deﬁnition ..(Sparse deconvolution signal model)
Let {yn} be a discrete-time
signal given by N + observed samples of a time series, which is the result of the
convolution between an unknown sparse signal {xn}, to be estimated, and a known time
series {hn} (with K + duration). Then, the following convolutional signal model can
be stated for these signals:
̂yn = ̂xn ∗hn =
K
∑
j=
̂xjhn−j
(.)
where ∗denotes the discrete-time convolution operator, and ̂xn is the estimation of the
unknown input signal.
The sparse deconvolution signal model corresponds to the general signal model in
Deﬁnition ..for model coeﬃcients ak given by ̂xk and explanatory signals s(k)
n given
by hn−k.
The performance of sparse deconvolution algorithms can be degraded by several
causes. First, they can result in ill-posed problems (Santamaría-Caballero et al., ),
and regularization methods are often required. Also, when the noise is non-Gaussian,
either LS or ML deconvolution can yield suboptimal solutions (Kassam and Poor, ).
Finally, if hn has nonminimum phase, some sparse deconvolution algorithms built by
using inverse ﬁltering become unstable.

Signal Processing Models
121
3.3.5
Array Processing
An antenna array is a group of ideally identical electromagnetic radiator elements placed
in diﬀerent positions of the space. This way, an electromagnetic ﬂat wave illuminating
the array produces currents that have diﬀerent amplitudes and phases depending of the
direction of arrival (DOA) of the wave and of the position of each radiating element.
The discrete time signals collected from the array elements can be seen as a time and
space discrete process.
The fundamental property of an array is that it is able to spatially process an incoming
signal, thus being able to discriminate among various signals depending on their DOA.
Applications of antenna array processing range from radar systems (which minimize
the mechanical components by electronic positioning of the array radiation beam), to
communication systems (in which the system capacity is increased by the use of spatial
diversity), and to radio astronomy imaging systems, among many others.
The simplest system model in array processing consists of a linear array of K + 
elements equally spaced a distance d, whose output is a time series of vector samples
xn = [x
n, … , xK
n ]T or snapshots (Van Trees, ). In order to obtain an expression of
such a signal, we ﬁrst model the output signal from transmitter l, received by the sensor
located at the right end of Figure ., which can be written as
̃xl
n = bl,I
n cos(πfct + 𝜙l) −bl,Q
n sin(πfct + 𝜙l),
(.)
where bl,I and bl,Q are the in-phase and quadrature phase of the transmitted modulation,
and fc is the carrier frequency of the radio signal. This expression can be written as
̃xl
n = Re{bl
n ei(πfct+𝜙l)},
(.)
where bl
n = bl,I
n + 𝕚bl,Q
n . The signal will illuminate the rest of elements with a time delay
that will depend on the array spacing and the illumination angle. In particular, the time
delay between two consecutive elements will be 𝜏l = (d∕c) sin(𝜃l), c being the light
k=0
k =K−1
θ1
θ0
θL−1
Figure 3.5 Array of K antennas and L transmitters.

122
Digital Signal Processing with Kernel Methods
speed. The delay will be zero if the element is in the broadside direction (𝜃l = ) and
maximum if the direction is the endﬁre (𝜃= ±◦). Then, knowing that the phase delay
between elements will be 𝜑l = πfc𝜏and that c = 𝜆fc, with 𝜆being the wavelength, the
vector of amplitudes measured in the array produced by signal xl[n] can be written
̃xl
n = Re{bl
n e𝕚(πfct+𝜙l)}Re{[, e𝕚π(d∕𝜆) sin(𝜃l), … , e𝕚π[(K−)d]∕𝜆sin(𝜃l)]T}.
(.)
Usually, these signals are represented by their complex low-pass equivalent signals
xl[n]. Indeed, the radiofrequency signal can be removed from the expression since it
is common to all signals. Also, we assume that all phase delays 𝜙l are zero for simplicity
(see Equation .). Then, a given source with constant amplitude, whose DOA and
wavelength are 𝜃l and 𝜆respectively, yields the following array output (so-called steering
vector):
vl = (, e𝕚π(d∕𝜆) sin(𝜃l), … , e𝕚π[(K−)d]∕𝜆sin(𝜃l))T.
(.)
If L transmitters are present, the snapshot can be represented as
xn = Ebn + en,
(.)
where E is a matrix containing all steering vectors of the L transmitters, bn is a column
vector containing (complex-valued) symbols transmitted by all users, and en is the
thermal noise present at the output of each antenna. The application consists of the
estimation of the symbols received from transmitted (or desired user) in the presence
of noise and L −interfering transmitters.
Deﬁnition ..(Array processing signal model)
Let {yn} be a discrete time signal
given by N −symbols transmitted by user , and be xk
n, ≤k ≤K, a set of discrete-
time processes representing time samples of the measured current at each of the array
elements. The array processing problem consists of estimating yn as
̂sn = yn =
K−
∑
k=
wkxk
n.
(.)
The problem is called array processing with temporal reference estimation when a
set of transmitted signals is previously observed for training purposes. Whenever the
DOA of the desired user is known, the problem is an array processing with spatial
reference estimation. Note that this signal model agrees with the general signal model
in Deﬁnition ..for sk
n given by xk
n.
3.4
Tutorials and Application Examples
We next include a set of examples for highlighting the concepts in the preceding
sections:
) First, we work on noise examples from diﬀerent applications; namely, for images, for
communication systems, and for biomedical signals.

Signal Processing Models
123
) Some basic examples on system identiﬁcation models are presented on synthetic
data for ﬁtting ARX models, and nonlinear system identiﬁcation is also included as
an example by using Volterra models.
) Sinusoidal signal models adjustment from an LS criterion are used in simple syn-
thetic data and in heart rate variability (HRV) signals.
) Sinc interpolation models are addressed by using an LS criterion as well as simple
methods in the literature.
) Examples of sparse deconvolution are scrutinized for synthetic data.
) Finally, array antenna processing shows an example of handling complex algebra data
structures.
3.4.1
Examples of Noise Models
A probabilistic description of noise provides a solid mathematical framework to cope
with many noise sources. However, many signal processing problems exhibit signals
contaminated with speciﬁc noise distributions which cannot be easily modeled proba-
bilistically. Very often we ﬁnd that Gaussian noise is assumed in DSP models because
of lack of knowledge or for ease of treatment. Nevertheless, this assumption may be far
from being realistic and lead to suboptimal results and even meaningless conclusions.
Correctly characterizing the noise in our observations is a relevant ﬁrst step in DSP
applications.
In this section we ﬁrst visualize and review standard noise sources found in images, as
this gives an intuitive idea of the nature and properties of some noise distributions. For
instance, accounting for particular characteristics of the noise, such as revealing heavy
tails, autocorrelation structures, or asymmetry, can make the diﬀerence in accuracy
and robustness of the models. Then, we present some examples of noise in signals
from communication systems, where Gaussian and non-Gaussian distributions are well
characterized in current telecommunications. Finally, we show some examples of the
diﬀerent nature of the noise that can be present in physiological signals, speciﬁcally in
ECG signals stored in long-term monitoring Holter recordings.
Noise in Images
Images are aﬀected by many kinds of noise and distortions, depending on the acqui-
sition process and the imaging system, but also due to artifacts occurring in the
image transmission and storage. While academically most of the denoising (some-
times referred to as restoration) algorithms work under the assumption of Gaussian
uncorrelated noise, this is actually far from being realistic. Images can be aﬀected by
missed pixels due to errors in the acquisition, ﬂickering pixels, shifts and optical image
aberrations, vertical striping in some satellite imaging sensors and thermal cameras,
spatial–spectral distortion produced by image compression, fast-fading noise due to
passing the image through a communication transmission channel, or speckle and
coherent noise in ultrasound images and radar imagery. Figure .illustrates the explicit
appearance of the noise in the familiar image of Barbara.
It is important to note that dealing with these heterogeneous noise sources constitutes
a very complex problem for many denoising algorithms because the noise pdf is known
(or can be fairly well estimated) for the Gaussian noise only. This compromises the use
of parametric models for doing denoising, and in many cases nonparametric algorithms

124
Digital Signal Processing with Kernel Methods
Gaussian, σ 2 = 400
JPEG2000
JPEG
WiFi
IRIS
VS
Figure 3.6 Illustration of different noise sources in a common standard image: Gaussian white noise,
JPEG and JPEG2000 quantization noise, vertical striping acquisition noise, a simulated infrared
imaging system (IRIS) noise as a mixture of uncorrelated Gaussian noise with paired salt-and-pepper
noise and lines interleaving, and WiFi fast-fading transmission noise.
typically excel (Laparra et al., ). Actually, a successful class of image denoising
methods is based on Bayesian approaches working in wavelet representations. The per-
formance of these methods improves when information a priori for the local frequency
coeﬃcients is explicitly included. However, in these techniques, analytical estimates
can be obtained only for particular combinations of analytical models of signal and
noise, thus precluding its straightforward extension to deal with other arbitrary noise
sources.
We here recommend the reader interested in image processing to consult the MAT-
LAB toolboxes ViStaCoRe referred to on this book web page. The toolbox module
on image restoration contains implementations of classical regularization methods,
Bayesian methods, and an SVM regression method applied in wavelet domains. The
toolbox includes other schemes based on regularization functionals that also take
into account the relations among local frequency coeﬃcients (Gutiérrez et al., ).
Two families of methods are included therein. On the one hand, the toolbox con-
tains techniques for denoising and deconvolution by regularization in local-Fourier
domains, such as the second derivative regularization functional, AR models (Banham
and Katsaggelos, ), and regularization by perceptually based nonlinear divisive
normalization functionals (Gutiérrez et al., ). On the other hand, the second family

Signal Processing Models
125
of methods considers denoising in wavelet domains, such as classical thresholding
techniques (Donoho and Johnstone, ), Bayesian denoising assuming Gaussian noise
and signal with Gaussian marginals in the wavelet domain (Figueiredo and Nowak,
), and mutual information kernel-based regularization in the steerable wavelet
domain (Laparra et al., ). The toolbox allows one to study the nature of diﬀerent
noise sources in the particular case of images since it includes image degradation
routines to generate controlled image blurring plus white (or tunable colored) Gaussian
noise.
Noise in Communication Systems
In a typical communications system, the signal demodulated by the receiver can be
expressed as a train of pulses of the form
p(t) = sinc(t −T) = sin[π(t −T)∕T]
π(t −T)∕t
.
(.)
The pulse is represented in Figure .. This is known as a Nyquist pulse (Proakis and
Salehi, ), a pulse with all samples equal to zero except at instant T, where the
sampling frequency fs = ∕T. Actually, it is easy to show that limt→T sinc(t −T) = and
limt→nT sinc(t−T) = if n ≠. Then, when a train of pulses sinc(t−mT) is modulated as
x(t) =
N
∑
m=
amsinc(t −mT),
(.)
–10
–5
0
5
10
Time (ms)
–0.4
–0.2
0
0.2
0.4
0.6
0.8
1
sinc
t
10−3
Figure 3.7 Sinc pulse with a period of 1 ms.

126
Digital Signal Processing with Kernel Methods
Time
–2
0
2
Amplitude
Signal
200
400
600
800
1000
1200
200
400
600
800
1000
1200
Time
–1
0
1
Amplitude
Bits
Figure 3.8 Upper panel: sequence of binary symbols. Lower panel: train of sinc pulses modulated by
the symbol sequence of the upper panel.
and it is sampled at a frequency fs = ∕T, the resulting discrete signal is
xn = x(nT) =
N
∑
n=
am𝛿n−m,
(.)
where 𝛿n is the Kronecker delta, thus recovering each one of the transmitted symbols
without intersymbol interference. That is, at each instant nT a symbol an is recovered.
This pulse has a bandwidth equal to ∕T, so it can be transmitted through a channel
having the same bandwidth without distortion.
Figure .depicts a sequence of binary signals that modulate the sinc pulse as
in Equation .. Figure .shows the detailed modulation process. The thick line
represents the modulated signal, which is a composition of each one of the pulses
delayed a time nT, for ≤n ≤. Each pulse is multiplied by a binary symbol of value
±. The squares represent the sampling instants, where it is clear that the pulse sequence
has a value equivalent to the corresponding bit.
Figure .represents the so-called eye diagram, which is a representation of the
superposition of each interval (n −)T ≤t ≤(n + )T. The center instant of the graph
contains all the sampling instants of the signal. It is easy to see that at instants nT the
signal is either +or −, where the zero crosses correspond to the initial and ﬁnal tails
of the signal.
In the conditions of the aforementioned model, the signal can be recovered in the
receiver without error. The received signals are nevertheless corrupted by additive noise
of power 𝜎
n. The noise is usually Gaussian in nature, and it is the main source or
detection error. If the noise has a power spectral density NW/Hz, then the received
samples will contain a noise of power 𝜎
n = N∕T. Hence, the noise power will increase
with the symbol rate. Thus, noise poses a limit in the symbol rate of a communication

Signal Processing Models
127
–2
0
2
–100
–50
0
50
100
150
–100
–50
0
50
100
150
–1
0
1
Figure 3.9 Detail of a train of pulses modulated by five binary symbols. The signals represented with
thin lines in the lower panel are each one of the pulses whose linear combination is the modulated
signal, represented in the upper panel with a thick line. Each circle corresponds to the amplitude of
the signal at sampling instants nT.
50
100
150
200
Time
–2
–1
0
1
2
Figure 3.10 Eye diagram of a pulse train modulated by a binary sequence of symbols.
system. As an example, assume a QAM transmission (e.g., see Proakis and Salehi
()); this is a signal carrying M.= possible symbols in the in-phase and
quadrature-phase components (see Section ..for a more detailed description of
phase and quadrature representations of a signal). The signal corrupted by additive
Gaussian noise is expressed as
x(t) =
N
∑
m=
am,Isinc(t −mT) + i
N
∑
m=
am,Qsinc(t −mT).
(.)

128
Digital Signal Processing with Kernel Methods
Time
–10
–5
0
5
50
100
150
200
50
100
150
200
Time
–10
–5
0
5
Figure 3.11 Representation of the in-phase or quadrature-phase eye diagram of a 64 QAM. Upper
panel shows the noise-free diagram, where it can be seen that all symbols cross a specific level during
the sampling instant. Lower panel shows the same signal, but corrupted by a white noise of power
𝜎2
n = 0.05 W.
When sampled at a frequency fs = ∕T, the resulting discrete signal is
xn = x(nT) =
N
∑
m=
(am,I𝛿n−m + iam,Q𝛿n−m).
(.)
In the presence of noise, the eye diagram of the signal will not cross the symbol
amplitudes at the sampling instant, but an amplitude error will be measured. Figure .
shows the eye diagrams of one of the phases of periods of the signal (in phase
or quadrature phase) in a noise-free environment and in the presence of noise. The
recovered symbols, once sampled, can be represented in the complex plane. This
representation is called a signal constellation, and gives a visual representation of the
noise corruption. Figure .shows the constellations of the signals of Figure ..

Signal Processing Models
129
In phase
–10
–5
0
5
10
Quadrature phase
–10
–5
0
5
10
–10
–5
0
5
10
In phase
–10
–5
0
5
10
Quadrature phase
Figure 3.12 Constellation of a 64 QAM. Upper panel shows the noise-free constellation, where it can
be seen that all symbols can be detected without error. Lower panel shows the constellation of the
QAM, corrupted by a white noise of power 𝜎2
n = 0.05 W. Here, the signal detection is subject to an
error rate that increases with the noise power.
Noise in Biomedical Signals
A white Gaussian noise distribution is a reasonable assumption for many DSP scenarios,
but it is not always the best choice for physiological and medical signals. A good example
can be found in ECG noise. Holter monitoring systems are used for long-term recording
of the cardiac activity in patients with suspect of underlying arrhythmia which cannot be
elucidated with the usual s of signals during consultation with the cardiologist. In this
situation, an ambulatory device is used by the patient over about h, hence increasing
the amount of information about the heart rhythm and behavior (Lee et al., ). The
daily activity, in combination with the adherence of the patches to the patient’s skin, are
sources of very diﬀerent kinds of noise.
Figure .shows several examples of noise in ECG recordings during Holter mon-
itoring. One of the noise sources most often present is baseline wander, which is
a low-frequency noise with changing bandwidth, often due to patient activity or

130
Digital Signal Processing with Kernel Methods
movement, respiration, and changes in the time scale of several seconds. A diﬀerent
kind of noise or artifacts is due to the movement or slipping of the ECG electrode
patches on the skin. This exhibits some nonphysiological patterns which are not easy to
characterize and to eliminate from the recording. Also, the ECG quantization is usually
done with or bits in the analog to digital conversion, which is a quite generous rate
compared with other physiological signals. However, when the ECG amplitude becomes
extremely small (maybe due to a change in the impedance or contact conditions), the
low-amplitude recording can be noticeably aﬀected by the quantization noise, which is
characterized by a step-like appearance of the signal, and has a distorting impact on the
ECG morphology and clinical measurements.
We already discussed in Section ..that the presence of high-frequency noise can be
distorting, especially due to the network interference at or Hz (and maybe several
of their harmonics). This is a well-localized noise that can often be cancelled with notch-
ﬁltering, which is implemented in all the cardiac signals processing systems. However,
another high-frequency noise of diﬀerent nature is due to the interference of the muscle
activity, which typically lasts some seconds. Muscle noise could be easily confounded
1.3655
ECG-1 (mV)
–2.5
–2
–1.5
–1
–0.5
1.3656
1.3657
1.3658
1.3659
1.366
1.3661
1.3662
1.3663
1.3664
1.3665
×104
3.5394
ECG-1 (mV)
–0.5
0
0.5
1
1.5
3.5395
3.5396
3.5397
3.5398
3.5399
3.54
3.5401
3.5402
3.5403
3.5404
×104
1.6972
1.6973
1.6974
1.6975
1.6976
1.6977
1.6978
1.6979
1.698
1.6981
1.6982
ECG-2 (mV)
–3.5
–3
–2.5
–2
–1.5
–1
×104
Figure 3.13 Different kinds of noise in ECG recorded during Holter monitoring, from top to bottom:
baseline wander; probably lack of electrode contact; step in the middle; some other unknown source
of distortion, affecting the ECG morphology; strong quantification noise; and weak quantification
noise. Time in seconds from short segments of long-term monitoring recordings.

Signal Processing Models
131
1.4441
1.4442
1.4443
1.4444
1.4445
1.4446
1.4447
1.4448
1.4449
1.445
1.4451
ECG-1 (mV)
–1
–0.5
0
0.5
1
×104
7.3916
7.3917
7.3918
7.3919
7.392
7.3921
7.3922
7.3923
7.3924
7.3925
7.3926
ECG-1 (mV)
–0.745
–0.74
–0.75
–0.735
–0.73
–0.725
–0.72
–0.715
–0.71
–0.705
–0.7
×104
ECG-2 (mV)
2.5793
2.5794
2.5795
2.5796
2.5797
2.5798
2.5799
2.58
2.5801
2.5802
2.5803
–1.85
–1.9
–1.8
–1.75
–1.7
–1.65
–1.6
–1.55
–1.5
×104
Figure 3.13 (Continued)
with network noise in the time domain. Nevertheless, its observation in the frequency
domain shows that muscle interference cannot be readily ﬁltered out, as far as it may
consist of a broadband activity, strongly overlapped with the ECG band of interest. This
can be seen in Figure ., which has been made using the code in Listing ..
%% Load data examples
load Noisy_ecg.mat
fs = 600;
% We know the sampling frequency
ts = 1/fs; n = length(clean_ecg); t = ts*(0:n-1);
% Time axis to plot
tmax = 20; % [s]
ind = find(t<tmax);
%% Plots in time
figure(1),
subplot(311), plot(t(ind),clean_ecg(ind)); ylabel('mV'),
subplot(312), plot(t(ind),noise(ind)); ylabel('mV'),
subplot(313), plot(t(ind),ecg_signal(ind));
ylabel('mV'), xlabel('t (s)')
%% Plots in frequency
X1 = abs(fftshift(fft(clean_ecg(ind)-mean(clean_ecg(ind)))));
X2 = abs(fftshift(fft(noise(ind)-mean(noise(ind)))));
X3 = abs(fftshift(fft(ecg_signal(ind)-mean(ecg_signal(ind)))));

132
Digital Signal Processing with Kernel Methods
f = linspace(-fs/2,fs/2,length(X1));
figure(2),
subplot(311), plot(f,X1); ylabel('FFT'),
axis tight, ax = axis; ax(1:2)=[-100 100]; axis(ax);
subplot(312), plot(f,X2); ylabel('FFT'),
axis(ax); subplot(313), plot(f,X3); ylabel('FFT'), xlabel('f (Hz)')
%% Try to filter out ...
filtord = 256+1;
% Try other orders
fc = 50;
% Try e.g. cutting freq 80 and 50
b = fir1(filtord,2*fc/fs);
[H,f] = freqz(b,1,1024,fs);
% Check and plot the design is OK
figure(3),
subplot(211), stem(b); axis tight; xlabel('n');
subplot(212), plot(f,abs(H)); axis tight; xlabel('f (Hz)');
%% ... but check for distortion in the residuals
ecg_filtered = filtfilt(b,1,ecg_signal);
resid = ecg_signal - ecg_filtered;
% Plot results
figure(4)
subplot(211), plot(t(ind),ecg_filtered(ind)), ylabel('mV'); axis tight
subplot(212), plot(t(ind),resid(ind)), ylabel('mV'); axis tight; ...
xlabel('t(s)');
Listing 3.1 Frequency analysis and filtering of ECG signals (ecg1.m).
It is worth noting that ﬁltering can be useful in many cases, but not always. If
we do not ﬁlter with caution, we could be distorting the ECG morphology, which is
highly valuable from a clinical point of view. In Figure ., we can see the eﬀect of
ﬁltering on several of the preceding example signals. One of the cautions to decide
the suitability of the ﬁlter is to plot the residuals. Residuals showing visible peaks
coming from the ECG peaks suggest that we have distorted the ECG signal in the
QRS complex. These are the prominent and narrow visible peaks corresponding to the
ventricular depolarization. Then, many clinically meaningful measurements made on
the QRS complex, such as its time duration, will be likely distorted. The reader can
analyze the code for generating the previous examples, and is encouraged to change
the ﬁlter settings in order to see the eﬀect on the signal distortion and on the noise
distribution.
3.4.2
Autoregressive Exogenous System Identification Models
Let us now deal with system identiﬁcation problems. In particular, we will work with
the following test system to be identiﬁed:
yn = .yn−−.yn−+ xn −.xn−+ .xn−.
(.)
This system is used here because of the diﬀerent orders of magnitude of the samples
in its impulse response. In order to identify this process we will use as input a white
Gaussian noise discrete-time process with unit variance, denoted as xn ∼(, ).
To make the problem harder and more interesting, we will corrupt the corresponding
output by an additive, small-variance random process en ∼(, .). This leads to an
observed process on = yn + en. The number of observed samples is N = . We use an

Signal Processing Models
133
0
mV
–2
0
2
(a)
5
10
15
20
t (s)
0
mV
–2
0
2
5
10
15
20
mV
–0.5
0
0.5
0
5
10
15
20
(b)
100
–100
–50
0
50
100
FFT
200
300
100
–100
–50
0
50
100
FFT
200
300
100
–100
–50
0
f (Hz)
50
100
FFT
200
300
Figure 3.14 Muscle noise and its overlapping with the ECG spectrum. (a) Clean ECG, muscle noise, and
noisy ECG. (b) Spectrum for each of the aforementioned signals.

134
Digital Signal Processing with Kernel Methods
–1.5
–1
–0.5
0
0.5
1
2
0
mV
4
6
8
10
12
14
16
18
–0.1
0
0.1
2
0
4
6
8
10
t (s)
12
14
16
18
–1.5
–1
–0.5
0
0.5
1
2
0
mV
4
6
8
10
12
14
16
18
–0.2
0
0.2
2
0
mV
4
6
8
10
t (s)
12
14
16
18
(a)
(b)
mV
Figure 3.15 Muscle noise and its overlapping with the ECG spectrum. ECG noise filtering (upper
panel) and the residuals after filtering (lower panel), for (a) fs = 80 Hz and (b) fs = 50 Hz.

Signal Processing Models
135
ARX model with LS to identify the system, and we assume that we know the order of
the input system. The sample generation code is given in Listing ., and the MATLAB
implementation with the arx function is given in Listing ..
N = 100;
x = randn(N,1);
b = [3 -0.5
0.2];
a = [1 -0.03 0.01];
y = filter(b,a,x);
% ... plus noise ...
yy = y + sqrt(.1) * randn(size(y));
% Filter impulse response
h = impz(b,a,10);
Listing 3.2 Sample generation from a system and impulse response (sysid1.m).
% Assume we know the order of the system to be identified
p = 2; q = 3; m = arx([yy(:),x(:)],[p,q,0]); [a_est,b_est] = polydata(m);
% Instead of comparing the obtained coefficients, we will compare the ...
response of the estimated system in terms of the impulse and ...
frequency responses
fvtool(b,a,b_est,a_est);
Listing 3.3 System identification via ARX and frequency analysis (sysid2.m).
Figure .shows that the method is able to successfully simulate (and thus to identify)
the system, despite the Gaussian noise present in the output signal. However, the LS
method is very sensitive to outliers. To illustrate this point, let us add some impulsive
noise to the output signal. The impulsive noise is generated as a sparse sequence with
% of randomly placed, high-amplitude samples, given by ±+(, ), where (a, b)
denotes the uniform distribution in interval [a, b], and the remaining are zero samples.
This noise sequence is denoted by jn. The observations consists of input xn and the
observed output plus impulsive noise; that is, on + 𝜎wjn. Values of 𝜎w are in the range
from −to dB. The code is given in Listing ., and Figure .shows the results.
As we can see, the identiﬁcation is now very poor. We will see in the next chapters how
kernel methods can successfully deal with this kind of noise.
% Jitter generation
w = ceil(N*rand(30,1));
w = unique(w); % indexes for jitter
jit = rand(size(w)) - .5;
jit = (sign(jit)*10)+jit;
% Mixing
yy(w) = jit;
% Now we repeat the same procedure to model the system ...
p = 2; q = 3;
m = arx([yy(:),x(:)],[p,q,0]);
[a_est,b_est] = polydata(m);
% Compare results using fvtool
fvtool(b,a,b_est,a_est);
Listing 3.4 Identification of a system with ARX under jitter noise (sysid3.m).

136
Digital Signal Processing with Kernel Methods
0
0.2
0.4
0.6
0.8
1
8
8.5
9
9.5
10
10.5
11
11.5
Normalized frequency
Magnitude (dB)
Magnitude response
0
2
4
6
8
10
−0.5
0
0.5
1
1.5
2
2.5
3
Samples
Amplitude
Impulse response
0
0.2
0.4
0.6
0.8
1
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
Normalized frequency
Phase (radians)
Phase response
(a)
(b)
(c)
h
hest
h
hest
h
hest
Figure 3.16 Comparison between the original and the estimated systems in terms of the impulse
response (top), the magnitude response (middle), and the phase response (bottom).

Signal Processing Models
137
(a)
0
2
4
6
8
10
−0.5
0
0.5
1
1.5
2
2.5
3
Samples
Amplitude
Impulse response
(b)
0
0.2
0.4
0.6
0.8
1
5
6
7
8
9
10
11
12
Normalized frequency
Magnitude (dB)
Magnitude response
(c)
0
0.2
0.4
0.6
0.8
1
0
0.05
0.1
0.15
0.2
0.25
Normalized frequency
Phase (radians)
Phase response
h
hest
h
hest
h
hest
Figure 3.17 Comparison between the original and the estimated systems in terms of the impulse
response (top), the magnitude response (middle), and the phase response (bottom).

138
Digital Signal Processing with Kernel Methods
3.4.3
Nonlinear System Identification Using Volterra Models
A system that shows a nonlinear behavior can be identiﬁed by constructing a polynomial
function of the output with a Volterra model. Assume a signal xn passing through a
system as
yn = f (xn, … , xn−K),
(.)
where f is a nonlinear function. A model to identify this signal can be expressed as
yn = b+
D
∑
k=
bkxn−k +
D
∑
k=
D
∑
k=
bk,kxn−kxn−k
+
D
∑
k=
D
∑
k=
D
∑
k=
bk,k,kxn−kxn−kxn−k+ ⋯,
(.)
which contains all possible monomials xn−kxn−kxn−k⋯up to order P among all sample
signals xn to xn. Model coeﬃcients bk,k,… of the monomials can be adjusted by a simple
MMSE strategy. The algorithm consists of applying a nonlinear transformation 𝝓(xn) to
vector xn = [xn, … , xn−D]T of the form
𝝓(xn) =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜⎝
𝟏
xn
⋮
xn−kxn−k
⋮
xn−kxn−kxn−k
⋮
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟⎠
(.)
that contains all the monomials, and a weight vector of the form
w =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜⎝
b
⋮
bk
⋮
bk,k
⋮
bk,k,k
⋮
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟⎠
(.)
such that the model can be written in the compact form yn = wT𝝓(xn).
Clearly, a simple way to ﬁnd a solution of these parameters is
w = (𝜱⊤𝜱)−𝜱y,
(.)
where 𝜱is a matrix containing all training vectors 𝝓(xn), and y is a vector containing
all training observations yn. We should note here that, while this Volterra approach

Signal Processing Models
139
considers the explicit calculation of matrix 𝜱, we will see in subsequent chapters that
this is not really necessary, and we can work by using kernel functions that implicitly
approximate this matrix.
function volterra1
for P = 1:2:7 % From order 1 to 7
[b,a] = butter(4,0.005); randn('seed', 0);
D = 5; % Dimension of the input
C = volt(D,P); % Compute the volterrra monomial indexes
% System, training data
m = randn(1,1000);
xm = filter(b,a,m)+fliplr(filter(b,a,m));
x = 10*sin(0.1*(1:1000)).*xm; % Modulated signal
y = system(x); % Apply distortion
% System, test data
m = randn(1,1000);
xm = filter(b,a,m)+fliplr(filter(b,a,m));
xt = 10*sin(0.1*(1:1000)).*xm;
yt = system(xt);
%
Train
X = buffer(x,D,D-1,'nodelay');
XX = volterraMatrix(X,C);
XX = [XX ; ones(1,size(XX,2))]; %
Add a constant for the bias
% Compute the MMSE coefficients
w = pinv(XX*XX')*XX*y(1:size(XX,2));
% Test
Xtst = buffer(xt,D,D-1,'nodelay');
XXtst = volterraMatrix(Xtst,C);
XXtst = [XXtst ; ones(1,size(XXtst,2))];
Ytst_ = XXtst'*w;
% Represent the signal
hold off, plot(Ytst_),hold all, plot(yt), drawnow
pause
end
%%
Callback functions
%
Compute the indexes of the monomials
function C = volt(D,P)
x = (1:D)';
for i = 1:P-1
x = coefficients(x);
C.(['x' num2str(i+1)]) = x;
end
C.P = P; % Store the order
%
Compute the next set of indices from indices x
function z = coefficients(x)
z = [];
d = max(x(:));
for i = 1:size(x,1)
for k = x(i,end):d
z = [z; [x(i,:) k]];
end
end

140
Digital Signal Processing with Kernel Methods
% Compute all monomials of vectors in X
function XX = volterraMatrix(X,C)
XX = X;
P = C.P;
for i = 2:P
aux = 1;
idx = C.(['x' num2str(i)]);
for j = 1:size(idx,2)
aux = aux.* X(idx(:,i),:);
end
XX = [XX ; aux];
end
% Nonlinear system that filters and compresses the signal
function y = system(x)
a = [0.1 0.1 0.43 0.9 0.81]';
X = buffer(x,5,4,'nodelay');
z = X'*a; y = tansig(0.7*z);
Listing 3.5 Example of Volterra identification (volterra1.m).
An implementation of the algorithm can be found in Listing ., where a modulated
sinusoidal is linearly ﬁltered and processed through a hyperbolic tangent function
that simulates a signal distortion by saturation. The system is identiﬁed by a Volterra
expansion of order P. The input signal is buﬀered in a vector of dimension D = .
Function volt computes the indexes corresponding to all Volterra monomials and
then function volterraMatrix computes all these monomials to construct a matrix
containing the Volterra expansion of all input vectors. The script ﬁrst synthesizes a
training dataset to adjust the parameters, and then the estimator is tested with a
second dataset. The system identiﬁcation method has been tested for diﬀerent degrees
of the Volterra model from P = (linear identiﬁcation) to P = (higher order
relations).
Figure .shows the result. The ﬁrst panel shows the system output and the output
simulated through linear identiﬁcation, which presents high diﬀerences with respect
to the original, due to its inability to identify the nonlinearities. The next panels
show the approximation with orders P = , , and increasing the approximation
quality.
3.4.4
Sinusoidal Signal Models
In this subsection we present some examples of spectral estimation in nonuniform
sampling conditions with the sinusoidal signal model. Let us start by building the LS
solution. Using matrix notation in Equation ., we can state the problem as that of
minimizing the quadratic error, and thus the minimization functional can be written as
(b, c) = ‖e‖= ‖y −Mb −Nc‖.
(.)
We encourage the reader to obtain the gradient with respect to the unknowns and to
make it equal to zero in order to obtain the solution. After doing so, the MATLAB
function in Listing .can be used.

Signal Processing Models
141
–1
0
1
–1
0
1
–1
0
1
200
400
600
800
200
400
600
800
200
400
600
800
200
400
600
800
–0.5
0
0.5
Figure 3.18 Amplitude-modulated sinusoidal processed by a linear system and by a Volterra plant
identification system. The first panel shows the system output (dots) and the output simulated
through linear identification (continuous line), which presents high differences with respect to the
original, due to its inability to identify the nonlinearities. The next panels show the approximation
with orders P = 3, 5, and 7.
function [B,C,A,Phi,y_test] = sinusoidals(y_train,t_train,t_test,w)
% Data
n = length(t_train); m = length(w);
M = zeros(m,n);
% Data matrix
N = zeros(m,n);
for i = 1:m;
M(i,:) = cos(w(i)*t_train');
N(i,:) = -sin(w(i)*t_train');
end
MN = [M',N'];
gam = 1e-5;
D = inv(MN'*MN + gam*eye(2*m)) * (MN'*y_train);
% Solution in polar coordinates
B = D(1:m);
C = D(m+1:end);
% Solution in cartesian coordinates
A = sqrt(B.^2+C.^2);
Phi = atan(C./B);
% Test estimation
y_test = zeros(size(t_test));
for i = 1:m
y_test = y_test + A(i)*cos(w(i)*t_test+Phi(i));
end
Listing 3.6 LS optimization of a sinusoidal signal model (sinusoidals.m).
Note that this function requires a vector of frequency hypothesis, as well as the pairs of
time sampling and measured samples. It also supports a test set of time samples, which

142
Digital Signal Processing with Kernel Methods
usually consist of a grid of equally sampled time instants, to check the interpolation
achieved by the method. One should note also that a regularization term is included,
which controls the smoothness of the solution. The code in Listing .can be executed
for testing these results. Note that, in this case, we are using a regularization value
𝛾= −.
%% Example of synthetic data
ntr = 100; nsecs = 20;
t_train = nsecs*rand(ntr,1);
fdata = 0.3;
Ampdata = 3;
phidata = 0.5;
% Generate data
y_train = Ampdata*cos(2*pi*fdata*t_train + phidata);
% Hypothesis frequencies
f = 0:.1:15;
f = f(2:end);
% zero mean
%f = [0.1 0.2 0.3 0.4 0.5];
ntest = 1000;
t_test = linspace(0,nsecs,ntest);
% Least squares optimization of the sinusoidal model
[B,C,A,Phi,y_test] = sinusoidals(y_train,t_train,t_test,2*pi*f);
% Plot results
figure(1), plot(t_train,y_train,'.r'); hold on; plot(t_test,y_test,'b'); ...
hold off;
xlabel('t (s)'); ylabel('Estimated and training signal')
figure(2), subplot(211), plot(f,A); grid on; xlabel('f (Hz)'); ylabel('A')
subplot(212), plot(f,Phi); grid on; xlabel('f (Hz)'); ylabel('\phi');
Listing 3.7 Testing a sinusoidal-based interpolator (sinusoidals2.m).
As shown in Figure .a and b, the time reconstruction is loose; however, the
amplitude is detected to be higher in the sinusoid frequency. Nevertheless, there is
noticeable spectral noise due to the nonuniform sampling and to the grid of hypothesis
frequencies. Better results now can be obtained in this trivial example by using a more
speciﬁc frequency grid; for instance, f = [0.1 0.2 0.3 0.4 0.5] instead of the
detailed grid yields the solution in Figure .c and d, so that the amplitudes and phases
are now more accurately estimated, as well as the reconstructed signal.
The reader is encouraged to check and explain the eﬀect of changing the regulariza-
tion parameter, as well as to use a frequency grid with values close to (but not exactly)
the actual frequency of the sinusoidal data. Also, the eﬀect of the presence of noise in
the data can be readily scrutinized. All these checks will give an idea on the sensitivity
of the method to the selection of the free parameters.
Another example is given by the analysis of an HRV signal. In this case, we use an
example of an RR signal (sequence of times between consecutive beats) and extract
time samples. Note that the beat sequence itself is used for the time instants when
each RR interval is measured, hence giving a temporal basis in seconds. This is usually
known as instantaneous cycle representation of the HRV, whereas the consideration of
the discrete time series for the consecutive RR intervals, just in terms of their ordinal
index, is called a tachogram. Hence, the instantaneous cycle has a clearer sense of actual
time when the beats occur, and the corresponding spectral representations will be in

Signal Processing Models
143
a basis of hertz, whereas the tachogram is usually represented in terms of discrete
frequency (often known as beatquency domain in the HRV literature).
% Example for HRV signal
load ejemploHRV RR
auxRR = RR(500:2500); auxt = cumsum(auxRR);
auxRR = auxRR/1000; auxt = auxt/1000;
% in seconds
% Plot
figure(1), clf
subplot(311), plot(auxRR), grid, hold on, axis tight
subplot(312), plot(auxt,auxRR), grid, hold on, axis tight
%% Remove ectopic beats
aux = find(abs(diff(auxRR))>0.1);
aux = unique([aux,aux+1]);
aux = setdiff(1:length(auxRR), aux);
auxt = auxt(aux);
auxRR = auxRR(aux);
subplot(313), plot(auxt,auxRR); grid, hold on, axis tight
%% Plot spectra
X = abs(fftshift(fft(auxRR-mean(auxRR))));
f = linspace(-0.5, 0.5, length(X));
% Plot results
figure(2), subplot(211), plot(f,X,'b'); hold on; ejes = axis; ejes(1)=0; ...
axis(ejes); xlabel('f (tachogram)');
% Estimation
t_test = 0:.25:length(auxt);
ff = linspace(0,0.5,400);
[B,C,A,Phi,y_test] = sinusoidalLS(auxRR-mean(auxRR),auxt,t_test,2*pi*ff);
subplot(212), plot(ff,A), axis tight, xlabel('f (Hz)'), ...
ylabel('Estimated amplitude');
% Plot results
figure(3),
subplot(211), plot(auxt,auxRR-mean(auxRR)), grid, axis tight, xlabel('t ...
(s)'), ylabel('RR - mean (s)')
subplot(212), plot(t_test,y_test), grid, axis tight, xlabel('t (s)'), ...
ylabel('RR - mean (s)')
Listing 3.8 Example of an HRV signal analysis (hrv1.m).
The code in Listing .is used to perform a set of preprocessing transformations
on the RR sequence and to make a simple analysis of such an HRV signal in terms of
tachogram and sinusoidal interpolation. First, the time basis is recovered, in seconds.
Second, possible ectopic beats are removed, by just establishing a threshold in the diﬀer-
ence of consecutive beats. Note that this is a rough step for this example, and the HRV
signal will be strongly sensitive to this stage and to the threshold choice. The spectra are
represented in terms of both instantaneous cycle and beatquency. Then, we use the LS
criterion to perform an estimate of the spectrum with a hypothesis grid for the frequen-
cies in the signal. Note that the zero frequency is removed from the frequency grid, but
also the mean is removed from the observations, as far as it has not been included in our
LS equations. The LS solution for the amplitudes is represented in terms of frequency
(physical units or hertz). Figure .shows the results of the code. The reader is encour-
aged to determine which error is taking place in the estimation with the LS criterion and
to propose a modiﬁcation in the code in order to to obtain a better estimate.

144
Digital Signal Processing with Kernel Methods
3.4.5
Sinc-based Interpolation
In this section we evaluate the performance of ﬁve standard interpolation techniques.
We will follow the steps in the toolbox simpleInterp available on this book’s webpage.
Details on the toolbox structure and main routines can also be found in Appendix .A.
The reader is encouraged to spend some time mastering this, as it will be used in
subsequent chapters for dealing with many of the signal problems in the framework in
a systematic way. Note that in order to run experiments with simpleInterp, we will
need for each problem: () an experiment conﬁguration ﬁle; () a data load or generation
function; () the algorithm function to be used; and () some code to visualize the
results. We will go through all these steps in each of the examples using this toolbox.
0
–4
–2
0
2
4
6
8
10
5
Estimated and training signal
10
t (s)
15
20
(a)
(b)
0
Phases
–2
–1
0
1
2
5
10
f (Hz)
15
f (Hz)
0
Amplitudes
0
0.2
0.4
0.6
0.8
1
5
10
15
Figure 3.19 Examples for LS solution of the sinusoidal signal model: (a) training set of samples and
solution for regularized inversion matrix; (b) estimated amplitudes and phases; (c,d) the same for a
more reduced set of frequency hypothesis.

Signal Processing Models
145
0
–3
–2
–1
0
1
2
3
5
Estimated and training signal
10
t (s)
15
20
(c)
(d)
0.1
Amplitudes
0
2
1
3
0.15
0.2
0.25
0.3
f (Hz)
0.35
0.4
0.45
0.5
0.1
Phases
–1.5
–0.5
0.5
0
–1
1
0.15
0.2
0.25
0.3
f (Hz)
0.35
0.4
0.45
0.5
Figure 3.19 (Continued)
Interpolation Algorithms
Many nonoptimal interpolation algorithms have been proposed in the literature, which
were benchmarked in Rojo-Álvarez et al. (). In this example, we include a detailed
explanation of how to perform the comparison among methods. The Jacobian weight-
ing (Choi and Munson, a) uses the following direct interpolation equation:
x(t) = xJ(t) + e(t) =
N
∑
k=
fkxksinc(𝜎(t −tk)) + e(t),
(.)

146
Digital Signal Processing with Kernel Methods
0.4
0.6
0.8
1
200
400
600
800
1000
1200
1400
1600
0.7
0.8
0.9
1
1.1
200
400
600
800
1000
1200
1400
1600
200
0.4
0.6
0.8
1
400
600
800
1000
1200
1400
1600
1800
2000
(a)
–0.1
0
0.1
0.2
0.3
0.4
0.5
200
0
RR -mean (s)
400
600
800
1000
t (s)
1200
1400
1600
1800
200
–0.2
–0.1
0
0.1
0.2
0.3
400
600
800
1000
1200
1400
1600
(b)
Figure 3.20 Examples for LS solution of the HRV estimation with the sinusoidal signal model. (a)
Original signal in terms of tachogram, of instantaneous cycle, and without ectopic beats. (b) Original
and LS reconstructed. (c) Estimated spectrum with FT, and estimated amplitudes with LS.

Signal Processing Models
147
0
Estimated amplitude
0.005
0.01
0.015
0.02
0.05
0.1
0.15
0.2
0.25
f (Hz)
0.3
0.35
0.4
0.45
0.5
0
0
10
20
30
40
0.05
0.1
0.15
0.2
0.25
f (tachogram)
0.3
0.35
0.4
0.45
0.5
(c)
Figure 3.20 (Continued)
where coeﬃcients fi are chosen to be the sample spacings; that is, fk = tk+−tk. This
corresponds to a Riemann sum approximation to the following integral:
x(t) = ∫
+∞
−∞
x(𝜏)sinc(𝜎(t −𝜏)) d𝜏,
(.)
where we are assuming that 𝜎is the true bandwidth of x(t). This algorithm sometimes
has poor performance in terms of interpolation accuracy, but it requires an extremely
reduced computation burden, which can provide a nice trade-oﬀin problems with a
large number of available observations.
Another suboptimal yet rather improved approach was proposed by Choi and Mun-
son (b), where a generalization of the sinc kernel interpolator was presented. The
model relies on a minimax optimality criterion as an approximate design strategy, and
it yields the following expression for the coeﬃcients:
fk = π
𝜎
( N
∑
k=
sinc(𝜎(tn −tk))
)−
.
(.)
Both the performance and the computational burden of this approach are between
the Yen and Jacobian sinc kernel interpolators. However, all these approaches exhibit
some limitations, such as poor performance in low SNR scenarios, or in the presence of

148
Digital Signal Processing with Kernel Methods
non-Gaussian noise (in this last case, as a direct consequence of the use of a quadratic
loss function).
In this example, the following signal interpolators are considered:
) Yen’s interpolator without regularization (Y).
function [y,coefs] = Y1(tk,yk,t,T0)
nk = length(tk);
S = sinc((repmat(tk,1,nk) - repmat(tk',nk,1))/T0)/T0;
B = pinv(S);
[y,coefs] = interpSinc(tk,yk,t,T0,B);
Listing 3.9 Yen’s interpolator without regularization – Y1 algorithm (Y1.m).
) Yen’s interpolator with regularization (Y).
function [y,coefs] = Y2(tk,yk,t,T0,gamma)
nk = length(tk);
S = sinc((repmat(tk,1,nk) - repmat(tk',nk,1))/T0)/T0;
B = (S'*S + gamma*eye(size(S))) \ S';
[y,coefs] = interpSinc(tk,yk,t,T0,B);
Listing 3.10 Yen’s interpolator with regularization – Y2 algorithm (Y2.m).
) Sinc interpolator with uniform weighting (S).
function [y,coefs] = S1(tk,yk,t,T0)
B = 1; [y,coefs] = interpSinc(tk,yk,t,T0,B);
Listing 3.11 Sinc interpolator with uniform weighting – S1 algorithm (S1.m).
) Sinc interpolator with Jacobian weighting (S).
function [y,coefs] = S2(tk,yk,t,T0)
nk = length(tk);
tkdesp = zeros(nk,1);
tkdesp(2:nk) = tk(1:nk-1);
subtr = tk(2:end) - tkdesp(2:end);
subtr = [subtr(:); mean(subtr(1:end-1))];
B = diag(subtr,0);
[y,coefs] = interpSinc(tk,yk,t,T0,B);
Listing 3.12 Sinc interpolator with Jacobian weighting – S2 algorithm (S2.m).
) Sinc interpolator with minimax weighting (S).
function [y,coefs] = S3(tk,yk,t,T0)
nk = length(tk);
S = sinc((repmat(tk,1,nk) - repmat(tk',nk,1))/T0);
b = T0./sum(S.^2);
B = diag(b,0);
[y,coefs] = interpSinc(tk,yk,t,T0,B);
Listing 3.13 Sinc interpolator with minimax weighting – S3 algorithm (S3.m).
All these methods use the interpSinc function, which obtains a y reconstruction
from the yk signal in a t nonuniform interpolation grid from the sinc base B. Listing .
shows the code for the common part of all of these interpolation algorithms.
function [y,Ak] = interpSinc(tk,yk,t,T0,B)
Ak = B * yk;
% Coefficients
y=zeros(size(t)); % Reconstruction

Signal Processing Models
149
for i=1:length(Ak)
y = y + Ak(i)*(sinc((t-tk(i))/T0)/T0);
end
Listing 3.14 Common part of interpolation algorithms (interpSinc.m).
Some of these methods have free parameters and they have to be tuned. In the present
set of experiments, we assume that a test set is available, which is not a usual situation,
though it can be addressed when dealing with synthetic data. Note that, in this case, a
grid search procedure is used by means of the gridSearch.m function.
Signals Generation and Settings
In order to benchmark all previous methods, simulations with known solutions were
conducted. Our experimental setup is adapted from Choi and Munson (b), where
a set of signals with stochastic band-limited spectra were generated. Nevertheless, here
we used a signal with deterministic band-limited spectra instead. The signals were
constructed by adding two squared sincs, one of them being a smaller and amplitude-
modulated version of the other:
x(t) = sinc(πt)
[
+ 
sin(πft)
]
+ e(t),
(.)
where f
= .Hz and e(t) is additive noise. Note that, in spite of the simplicity
of Equation ., it cannot be trivially adjusted by a weighted combination of sinc
basis.
A set of L samples can be used with averaged sampling interval T s. The sampling
instants are obtained by adding uniform noise, in the range [−.T, .T], to equally
spaced time points {tk = kT}L
k=. Diﬀerent values of L can be taken, changing accord-
ingly averaged sampling interval T; that is, when L = m × samples are considered,
the averaged sampling interval is changed to T = .∕m s, with m = , , , . Diﬀerent
SNRs were explored (no noise, dB, dB, dB, and dB). Sampling intervals
falling outside [, LT] are wrapped inside. The code for generating the signal is given in
Listing ., and the interpSignal function was implemented as in Listing ..
function data = genDataInterp(conf)
L = conf.data.L; LT = L*conf.data.T;
% Uniform sampling generation to training and reconstruction
Xtrain = linspace(1, LT, L); Xtrain = Xtrain(:);
Xtest = linspace(1, LT, L*16); Xtest = Xtest(:);
% Nonuniform sampling generation
Xtrain(2:L-1) = Xtrain(2:L-1)+ 2*conf.u .* (rand(L-2,1)-0.5);
% Avoiding samples out of the signal limits
Xtrain(Xtrain>LT) = 2*LT-Xtrain(Xtrain>LT);
Xtrain(Xtrain<1)
= 2-Xtrain(Xtrain<1);
% Signal generation
[Ytrain, Ytest] = interpSignal(conf,Xtrain,Xtest);
potY = mean(Ytest.^2);
N = sqrt(potY./(10.^(conf.SNR./10)));
Ytrain = Ytrain + N*randn(size(Ytrain));
% Return the data structure
data.Xtrain=Xtrain; data.Xtest=Xtest; data.Ytrain=Ytrain; data.Ytest ...
=Ytest;
Listing 3.15 Data generation for interpolation examples (genDataInterp.m).

150
Digital Signal Processing with Kernel Methods
function [varargout] = interpSignal(conf,varargin)
cf = conf.data;
varargout = cell(nargout,1);
for m = 1:nargout,
t = varargin{m};
y = zeros(size(t));
x = ((t-cf.mu)./cf.sigma);
switch cf.FNAME
case 'MSSF'
y = cos(2*pi*cf.f.*t) .* ...
(1/(cf.sigma*sqrt(2*pi))).*exp((-1/2).*(x.^2));
case 'DMGF'
sigma=3; f1=.75; f2=0.25;
y = (cos(2*pi*f1.*t) + cos(2*pi*f2.*t)) .* ...
(1/(sigma*sqrt(2*pi))).*exp((-1/2).*((x*cf.
sigma/sigma).^2));
case 'gauss'
y = (1/(cf.sigma*sqrt(2*pi))).*exp((-1/2).*(x.^2));
case 'sinc2'
y = sinc(t/(cf.T0*2)).^2;
case 'doblesinc2'
t0 = 6*cf.T0;
w = 2/(3*cf.T0);
tc = t - (t(1)+t(end))/2;
y = sinc(tc/t0).^2 + cf.k*(sinc(tc/t0).^2) .* sin(w*tc);
case '50sincs'
A = rand(50,1);
mu = (rand(50,1)+cf.DESP).*cf.L*cf.T;
for n = 1:50,
sinc_aux = A(n) .* sinc((t-mu(n))/cf.T0);
y = y + sinc_aux;
end
end
varargout{m} = y;
end
Listing 3.16 Signal generation part for interpolation examples (interpSignal.m).
We generated realizations for each set of experiments. The performance of the
interpolators was measured on a validation set given by a noise-free, uniformly sampled
version of the output signal (sampling interval T∕). The S∕E ratio is computed in
decibels in the training set as
( S
E
)
dB = log
(
𝔼[(xN
n )]
𝔼[e
n]
)
.
(.)
Means and standard deviations of S∕E were averaged over the realizations. The code
for calculating this signal-to-error ratio is as simple as seen in Listing ..
function value = SE(ypred,y)
signal = sum(y.^2);
error = sum((ypred - y).^2);
value = signal/error;
Listing 3.17 Signal-to-error ratio (SE.m).

Signal Processing Models
151
Table 3.3 S/E ratios (mean ± std) for Gaussian noise.
S∕E ratio
Method
No noise
40 dB
30 dB
20 dB
10 dB
Y
.± .
.± .
.± .
.± .
.± .
Y
.± .
.± .
.± .
.± .
.± .
S
−.± .
−.± .
−.± .
−.± .
−.± .
S
.± .
.± .
.± .
.± .
.± .
S
.± .
.± .
.± .
.± .
.± .
All the conﬁguration parameters deﬁning these experiments are ﬁxed in the conﬁgu-
ration ﬁle conﬁg_interp.m available on the book webpage.
Experimental Results
Table .shows the performance of the algorithms in the presence of additive and Gaus-
sian noise, as a function of SNR. The poorest performance was noticeably exhibited by
S, and some improvement was observed with Sand S. Yyielded good performance
only for low noise levels, whereas Yshowed good performance for all the noise levels,
which was in agreement with its theoretical optimality for the Gaussian noise.
The top panels in Figure .show a representative example of a modulated sinc signal
with SNR = dB. The interpolation in the time domain was shown to provide better
approximation to the validation signal for Y(theoretical optimum). These results will
be compared with the SVM algorithm implementations in Chapters and . The code
in Listing .can be used to plot the results, in the time and in the frequency domain.
function solution_summary = display_results_interp(solution,conf)
% Initials
algs = fields(solution);
nalgs = length(algs);
Xtest = solution.(algs{1})(end).Xtest;
fm = 1/conf.data.T;
f = linspace(-fm/2,fm/2,length(Xtest))';
Ytf = abs(fftshift(fft(solution.(algs{1})(end).Ytest)));
options = {'FontSize',20,'Interpreter','Latex'};
options2 = {'FontSize',18,'FontName','Times New Roman'};
% Figures
figure
for ia = 1:nalgs
subplot(3,nalgs,ia)
plot(solution.(algs{ia})(end).Xtrain,solution.(algs{ia})
(end).Ytrain,'ok'); hold on
plot(Xtest,solution.(algs{ia})(end).Ytest);
plot(Xtest,solution.(algs{ia})(end).Ytestpred,'r')
title(strrep(algs{ia},'_','-'),options{:});
if ia==1; H=legend('$y_{train}$','$y_{test}$','$y_{pred}$'); ...
set(H,options{:}); end
axis tight; set(gca,options2{:})
subplot(3,nalgs,nalgs+ia)

152
Digital Signal Processing with Kernel Methods
stem(solution.(algs{ia})(end).coefs);
axis tight; set(gca,options2{:})
subplot(3,nalgs,2*nalgs+ia)
plot(f, Ytf, 'b'); hold on;
plot(f,abs(fftshift(fft(solution.(algs{ia})(end)
.Ytestpred))),'r');
xlabel('f',options{:}); xlim([0, .2]); set(gca,options2{:})
if ia == 1; H = legend('Test','Pred.'); set(H,options{:}); end
end
suptitle(['SNR=',num2str(conf.SNR),' - u=',num2str(conf.u)])
% Performance summary
solution_summary = results_summary(solution,conf);
Listing 3.18 Displaying interpolation results (display_results_interp.m).
3.4.6
Sparse Deconvolution
Let yn be a discrete-time signal that contains in its lags ≤n ≤N a set of N +observed
samples of a time series obtained as the convolution between an unknown sparse signal
xn, whose samples in the lags ≤n ≤M we want to estimate, and a time series hn,
with known samples in lags ≤n ≤Q −. Samples of yn, xn, and hn are treated as null
outside the observation intervals, and N = M + Q + . Then, the following signal model
can be used:
yn = ̂xn ∗hn + en =
M
∑
j=
̂xjhn−j + en,
(.)
where ∗denotes the discrete-time convolution operator, ̂xn is the estimation of the
unknown input signal, and en is the noise. Equation .is required to be fulﬁlled for
lags n = , … , N of observed signal yn.
The solution is usually regularized (Tikhonov and Arsenin, ) by minimizing
the qth power of the q-norm of estimate ̂xn, which promotes smooth solutions. The
regularized functional to be minimized can be written as
JD = ‖e‖p
p + 𝜆‖̂x‖q
q
(.)
where vector notation has been introduced for the observed set of samples and the
errors, and parameter 𝜆tunes the trade-oﬀbetween model complexity and the min-
imization of estimation errors.It is worth noting again that diﬀerent norms can be
adopted, hence involving diﬀerent families of models and solutions. For instance, setting
p = and 𝜆= yields the LS criterion, whereas setting p = , 𝜆≠, and q = yields
the well-known Tikhonov–Miller regularized criterion (Tikhonov and Arsenin, ).
Also, for p = and 𝜆= we obtain the 𝓁criterion, and setting p = , 𝜆≠, and q = 
provides the 𝓁-penalized method (O’Brien et al., ).
From the point of view of ML deconvolution (Kormylo and Mendel ; Mendel
and Burrus ) or GM (Santamaría-Caballero et al., ), sparse deconvolution
algorithms can be written as the minimization of the (power of the) 𝓁p norm of the
Note that, for notation simplicity, sometimes we refer to en as the noise and at other times as the residuals,
although it is well known that, in general, residuals can be seen as an approximation ̂en to the noise en.

0
5
10
Y1
15
0.5
1
0
5
10
Y2
15
0.5
1
0
10
20
30
0.2
0.4
0.6
0
5
10
n
n
n
n
n
15
0.2
–0.2
0.4
0.8
0.6
0
5
10
15
0.2
–0.2
0.4
0.8
0.6
0
5
10
15
0.2
–0.2
0.4
0.8
0.6
0
5
10
15
0.2
–0.2
0.4
0.8
0.6
0
5
10
15
0.2
–0.2
0.4
0.8
0.6
0
5
10
S1
15
1
2
0
5
10
S2
15
0.5
1
0
5
10
S3
15
0.5
1
0
10
20
30
0.2
0.4
0.6
0
10
20
30
0.2
0.4
0.6
0
10
20
30
0.5
1
0
10
20
30
0.2
0.4
200
100
0
0
0.1
f
Test
Kernel
0.2
200
100
0
0
0.1
f
0.2
200
100
0
0
0.1
f
0.2
200
100
0
0
0.1
f
0.2
200
100
0
0
0.1
f
0.2
Ytrain
Ytest
Ypred
Figure 3.21 Examples of interpolation in the time domain. The second row shows the coefficients obtained to construct the kernel of each interpolation
algorithm. The third and fourth rows display the kernel of each algorithm in the time and frequency domains respectively for Gaussian noise (SNR = 20 dB,
L = 32 samples).

154
Digital Signal Processing with Kernel Methods
residuals minus a regularization term which consists of the log-likelihood of the sparse
time series for an appropriate statistical model; that is:
JML = ‖e‖p
p −𝜆log l(̂x),
(.)
where l(̂x) denotes the likelihood. White Laplacian statistics lead to the 𝓁-penalized
criterion, whereas non-Gaussian input signal statistics have been proposed, such as
Bernoulli–Gaussian (BG) and GM distributions (Mendel and Burrus, ; Santamaría-
Caballero et al., ). If the prior distribution of the sparse signal can be approximated
with a mixture of two zero-mean Gaussians, one of them being narrow (small variance)
and the other being broad (larger variance) (Santamaría-Caballero et al., ), then the
functional to be minimized in GM deconvolution is
JGM = ‖e‖−𝜆GM
N
∑
n=
log

∑
j=
πjpj(̂xn),
(.)
where πj are the prior probabilities of each Gaussian component, and
pj(̂xn) = (𝜎j
√
π)−exp
(
−
̂x
n
𝜎
j
)
(.)
are their probability densities. These functionals contain a term that is intended to
minimize the training error of the solution plus a regularization term from a Tikhonov
point of view. As stated by Girosi et al. () and Poggio and Smale (), these
stabilizer terms used in Equations .–.are also seen as smoothers, as lower values
of these functions produce smoother solutions.
Deconvolution Algorithms for Benchmarking
We choose 𝓁-penalized minimizing Equation .and GM minimizing Equation .
as algorithms for benchmarking sparse deconvolution, and following the experiments
in Rojo-Álvarez et al. (). MATLAB codes for these two methods are given in
Listings .and ..
function [predict,coefs] = deconv_L1(y,h,u,lambda)
% Init
N
= length(y); coefx = ones(N,1); coefe = ones(N,1); lb
= ...
zeros(1,4*N);
% Construct kernel matrix
Haux = convmtx(h,N);
H
= Haux(1:N,:);
A
= [H -H eye(N) -eye(N)];
% Deconvolution
options = optimset('Display','off');
fo
= [lambda*coefx; lambda*coefx ; coefe ; coefe];
x
= linprog(fo,[],[],A,y,lb,[],[],options);
predict = x(1:N)-x(N+1:2*N);
coefs
= filter(h,1,predict);
if ~isempty(u); aux=predict; predict=coefs; coefs=aux; end
Listing 3.19 𝓁1-regularized deconvolution algorithm (deconv_L1.m).

Signal Processing Models
155
function [predict,coefs] = deconv_GM(z,h,u,gamma,alfa)
% Initials
pr
= [0.5 0.5];
sn
= [0.5 1]*mean(z.^2);
m
= [1000 50];
mu
= 1e-2;
N
= length(z);
x
= zeros(N,1);
% Construct kernel matrix
Haux = convmtx(h,length(x));
H
= Haux(1:length(x),:);
% Initial norm-2 loop
for i = 1:m(1)
x = x+2*mu*H'*(z-H*x);
end
% Main loop
for i = 1:m(2)
% Posteriori
p(:,1) = gauss(0,sn(2),x);
p(:,2) = gauss(0,sn(1),x);
pp
= (pr(2)*p(:,1)+pr(1)*(p(:,2)));
r(:,1) = pr(2)*p(:,1)./pp;
r(:,2) = pr(1)*p(:,2)./pp;
% q caltulation
q
= x.*(r(:,1)/sn(2)+r(:,2)/sn(1));
% Intermediate steps
c
= 2*H'*(H*x-z)+alfa*q;
A
= H'*H;
lam
= max(eig(A,'nobalance'));
v
= c.*x;
d
= zeros(length(x),1);
d(v<0) = 1/(lam+alfa/(2*sn(1)));
d(v>0) = min(1/(lam+alfa/(2*sn(1))), x(v>0)./c(v>0));
D
= diag(d);
% Main step
x
= x-D*c;
x
= x*(max(z))/max(abs(x));
% Updating posteriori
p(:,1) = gauss(0,sn(2),x);
p(:,2) = gauss(0,sn(1),x);
pp
= (pr(2)*p(:,1)+pr(1)*(p(:,2)));
r(:,1) = pr(2)*p(:,1)./pp;
r(:,2) = pr(1)*p(:,2)./pp;
% Updating parameters
sn(2)
= gamma*sn(2) + (1-gamma)*sum(x.^2.*r(:,1))/sum(r(:,1));
sn(1)
= gamma*sn(1) + (1-gamma)*sum(x.^2.*r(:,2))/sum(r(:,1));
pr(2)
= gamma*pr(2) + (1-gamma)*mean(r(:,1));
pr(1)
= gamma*pr(1) + (1-gamma)*mean(r(:,2));
end
predict = x;
coefs = filter(h,1,x);
if ~isempty(u); predict=coefs; coefs=x; end
Listing 3.20 GM deconvolution algorithm (deconv_GM.m).

156
Digital Signal Processing with Kernel Methods
Signals Generation and Experimental Setup
A deterministic sparse signal with samples and ﬁve nonzero values (x= , x=
., x= −., x= , and x= −.) (Figueiras-Vidal et al., ) was used
for simulations. The other time series consists of the ﬁrst samples of the impulse
response of
H(z) =
z −.
z−.z + ..
(.)
A noise sequence was added, with a variance corresponding to SNR from to dB.
Performance was studied for three diﬀerent kinds of additive noise; namely, Gaussian,
Laplacian, and uniform. The MATLAB code in Listing .can be used to generate the
sparse input signal, where getsignal and getri are given in Listings .and .
respectively.
function data = genDataDeconv(conf)
% Signal
x
= getsignal(conf.data.N,1);
% Convolution
h
= getri(conf.data.Q);
y
= filter(h,1,x);
% Noise Variance calculation
SNR
= 10.^(conf.SNR/10);
signal = norm(y)^2/length(y);
noise
= sqrt(signal./SNR);
% Signal with noise
switch conf.data.typeNoise
case 1 % Signal with Gaussian noise
z = y + noise*randn(size(y));
case 2 % Signal with uniform noise
z = y + noise*rand(size(y));
case 3 % Signal with laplacian noise
z = y + laplace_noise(length(y),1,noise);
end
% Return data structure
data.z=z; data.y=y; data.h=h; data.x=x; data.u=[];
Listing 3.21 Data generation for sparse deconvolution (genDataDeconv.m).
function x
= getsignal(N,tipo)
x = zeros(N,1);
switch tipo
case 1
x([20 25 47 71 95]) = [8 6.845 -5.4 4 -3.6];
case 2
Q = 15;
p = 0.05;
while ~sum(x)
aux = rand(N-Q,1);
q
= (aux<=p);
x(1:N-Q) = randn(N-Q,1).*q;
end
end
Listing 3.22 Signal generation for sparse deconvolution (getsignal.m).

Signal Processing Models
157
function h = getri(Q)
num = [1 -.6];
den = poly([.8*exp(1j*5*pi/12) .8*exp(-1j*5*pi/12)]);
h = impz(num,den,Q);
Listing 3.23 Impulsive response generation for sparse deconvolution (getri.m).
The free parameters for the 𝓁-penalized and for the GM algorithms are 𝜆and 𝜆GM
respectively. Estimation quality was quantiﬁed by using
MSE =

N + 
N
∑
n=
(̂xn −xn),
(.)
F =
∑
xn≠
(̂xn −xn),
(.)
Fnull =
∑
xn=
(̂xn −xn)=
∑
xn=
̂x
n,
(.)
as proposed by O’Brien et al. (). These quality estimations are coded as seen in
Listings ., ., and ..
function value = MSE(ypred,y)
value = mean((ypred - y).^2);
Listing 3.24 MSE (MSE.m).
function f = F(ypred,y)
indx = find(y ~= 0);
f = norm(ypred(indx) - y(indx));
Listing 3.25 F measure for peak samples (F.m).
function fnull = Fnull(ypred,y)
fnull = norm(ypred(y == 0));
Listing 3.26 Fnull measure for null samples (Fnull.m).
All the conﬁguration parameters that deﬁne these experiments are ﬁxed in the
conﬁguration ﬁle conﬁg_deconv.m on the book’s webpage. See again the help and
demos of the simpleInterp toolbox and Appendix .A.
Experimental Results
Records of estimated signals show a high variability. In Figure .we present some
representative results of those corresponding to previously presented performance
ﬁgures. Optimum values were adjusted for the free parameters in this example.
The 𝓁-penalized deconvolution detected almost all the peaks, but it also produced
a number of small spurious peaks. In the example, the GM procedure reaches
higher estimation performance. Table .summarizes the averaged performance in
realizations. The results can be readily visualized by running the speciﬁc code
display_results_deconv.m .
3.4.7
Array Processing
In this example we simulate an array of K = equally spaced elements and three signals
modulated by a QPSK modulation, with unitary amplitude and illuminating the array

158
Digital Signal Processing with Kernel Methods
20
–4
–2
0
2
4
6
8
40
60
80
100
120
20
–4
–2
0
2
4
6
8
40
60
80
100
120
(a)
(b)
Figure 3.22 An example of sparse deconvolution results versus data with SNR = 10 dB for the
𝓁1-penalized algorithm (a) and the GM algorithm (b), with time in samples.
Table 3.4 S / E ratios (mean ± std) for Gaussian.
Method
MSE
F
Fnull
𝓁-regularized
.± .
.± .
.± .
GM
.± .
.± .
.± .
–1
–0.5
0
0.5
1
102 samples, σ =0
–90
–60
–30
0
30
60
90
0
0.5
1
0.5
–1
–0.5
0
1
102 samples, σ =0.3
–90
–60
–30
0
30
60
90
0
0.5
1
0.5
–1
–0.5
0
1
103 samples, σ= 0
–90
–60
–30
0
30
60
90
0
0.5
1
0.5
–1
–0.5
0
1
103 samples, σ= 0.3
–90
–60
–30
0
30
60
90
0
0.5
1
Figure 3.23 Experimental setup for the simulation and resulting array beams (training with 100 and
1000 samples).
from angles of arrival 𝜃= ◦, 𝜃= ◦, and 𝜃O = −◦with respect to the direction
perpendicular to the array. Figure .shows the experimental setup of the simulation.
The array processor output is yn = wTxn = ∑K−
k=wkxk
n, where xn represents the complex

Signal Processing Models
159
envelope of the current measured at each one of the elements of the array. Low-pass
complex modulations are simulated for the three independent transmitters, and then a
train of snapshots or antenna array measurements are further simulated by multiplying
each modulation by its corresponding steering vector, as described in Section ... The
data are then corrupted with additive Gaussian noise, and the array output is linearly
processed to detect the desired user signal arriving from 𝜃= ◦. The parameters can
be considered as a ﬁlter in the space, since all samples in snapshot xn are taken at the
same time at diﬀerent directions of the arrival space.
In order to characterize the array, the FT of array parameter vector w has been
computed and represented with respect to all spatial frequencies. Since the phase delay
between elements corresponding to a DOA 𝜃l is 𝜙l = π(d∕𝜆) sin(𝜃l), we can make a
representation with respect to the angle of arrival using the inverse of this function
𝜃l = arcsin[(𝜆∕πd)𝜙l]. The resulting graphs, represented in polar coordinates and
normalized to , are the corresponding array beams. Listing .shows the code for
this array processing example.
function array
% Transmitted signal
N = 100;
K = 5;
d = 0.51;
lambda = 1;
theta1 = 30*pi/180;
% Desired signal angle
theta2 = 60*pi/180;
% Interference 1
theta3 = -60*pi/180; % Interference 2
% Visible angle
Theta = asin(-linspace(-pi,pi,1024)'*lambda/2/pi/d)*180/pi;
sigma = 0.00; % Noise variance
% Signal and interferences
[X1,b] = signal(N,K,lambda,d,theta1);
X2 = signal(N,K,lambda,d,theta2);
X3 = signal(N,K,lambda,d,theta3);
X = X1 + X2 + X3;
X = X + sigma*randn(size(X)); % Additive noise
% MMSE array optimization
w = pinv(X')*b;
p = abs(fftshift(fft(w,1024)));
p = p/max(p);
figure(1), polar(Theta*pi/180,p)
function [X,b] = signal(N,K,lambda,d,theta)
% QPSK symbol stream transmitted
b = sign(randn(N,1))+1j*sign(randn(N,1));
% Steering vector
a = exp(1j*2*pi*d/lambda*[0:K]*sin(theta));
% Complex envelope signal
X = a'*b';
Listing 3.27 Array processing example (array.m).
Figure .shows the resulting array beams after a training with and 
samples, with no noise and with noise of standard deviation equal to .. Under
noise-free conditions, both beams are similar and place two zeros at ◦and −◦

160
Digital Signal Processing with Kernel Methods
in order to cancel the interfering signals, while the main lobe is pointed toward the
desired signal at ◦. In the presence of noise, the performance of the beamforming is
degraded, but a signiﬁcant attenuation at both interfering angles of arrival can still be
observed.
3.5
Questions and Problems
Exercise ..
Write the equation of a signal model diﬀerent from the ones expressed
in this chapter. Compare it with the general signal model.
Exercise ..
Write a simple program for generating the noise distributions
described in this chapter.
Exercise ..
Write the code for generating the examples of the “Noise in Commu-
nication Systems” section.
Exercise ..
In the example for ECG analysis, change the ﬁlter settings to see the
eﬀect on the signal and on the noise distribution. Plot the residuals histograms and the
residuals in time. What can you say?
Exercise ..
In the example of system identiﬁcation, explore the results in terms
of the estimated order of the system to be estimated. How does this aﬀect the system
estimation quality?
Exercise ..
In the examples of sinusoidal models, write down the equations for the
MMSE solutions. For the synthetic example, analyze the eﬀect of regularization, of the
frequency grid, and of the presence of additive noise. For the HRV example, determine
which error is taking place in the MMSE criterion, and propose an alternative aiming
to correct it.
Exercise ..
Analyze the code of the free parameters tuning, provided with the
toolbox. Think in a real application problem where the dataset can have dependencies,
such as several measurements in the same individual. How would you change the free
parameters tuning for ensuring the independence from the individual?
Exercise ..
In the example for sinc interpolation, plot the error in the time and
frequency domains for each method. What can you say about the behavior of the
diﬀerent methods?
Exercise ..
In the deconvolution example, plot the eﬀect of the regularization
parameter on the estimated input sparse signal.
Exercise ..
Complete the intermediate steps for going from Equation .to the
primal functional in Equation ., to the Lagrangian functional in Equation ., and
to the dual functional in Equation ..

Signal Processing Models
161
Exercise ..
Prove the relationship between w and the residual cost function in
Equation .. Give a result for the MMSE cost and for the Vapnik cost.
Exercise ..
Prove Equation .for the robust expansion coeﬃcients property.
Exercise ..
Propose two signal models for time series problems. Can the prob-
lems that you proposed be stated in the form of the general signal model in Equation .?
Why?
3.A
MATLAB simpleInterp Toolbox Structure
This appendix describes the MATLAB implementation structure of the simpleIn-
terp toolbox. The toolbox will be extensively used for interpolation and deconvolution
problems, both using linear and kernel signal models. In particular, the toolbox will be
used in Chapters , , and . In order to run experiments with the toolbox, we will need:
() an experiment conﬁguration ﬁle; () a data load or data generation function; () the
algorithm function to be used; and () some code to visualize the results. In the toolbox
you can ﬁnd the script simpleInterpDemo.m that will set up everything needed (such
as paths or experiment to run).
The main parts of the toolbox are:
) The main experiments part, where each of the speciﬁc parts of a given experiment is
invoked.
) The shared functions for diﬀerent experiments; namely, the calculation of measure-
ment evaluation (such as MSE, SNRs, or bit error rate (BER)).
) The necessary methods for tuning the free parameters in each algorithm, and
including:
●cross-validation (CV), which splits the training set into diﬀerent partitions, possi-
bly even for one partition per sample, which corresponds to the well-known leave-
one-out (LOO) validation;
●grid search;
●sequential search.
The two last methods for free parameters tuning use a diﬀerent set of observations
for validating the performance. The choice of each search method can be achieved in
terms of the needs and available resources from the reader. In general, with unlimited
computational resources, the best option will be CV, or even LOO–CV. On the other
hand, with time or computational burden limitations, the other two methods are
recommendable. The grid search method can be recommendable from a strict point
of view, as far as sequential search could suﬀer from local minima stacking in some
problems, though in many problems it is enough with this sequential exploration in a
valid range of parameter values. Even though the free parameters tuning has to be done
with a dataset diﬀerent from the test set (usually by splitting the original set), sometimes
this will be not a simple and eﬀective approach in signal processing tasks, where time
correlation among samples is strong by their own nature.

162
Digital Signal Processing with Kernel Methods
We present in Listings .and .the shared parts of the code allowing the
global and structured execution of the examples following this structure, and the main
function.
% Configuration file is loaded to execute a particular example
config_file
% Double loop is allowed in order to evaluate different conditions
n1 = length(conf.vector_loop1);
n2 = length(conf.vector_loop2);
solution = cell(n1,n2); summary = solution;
for i = 1:n1
eval(['conf.',conf.varname_loop1,'=conf.vector_loop1(i);
']);
for j = 1:n2
eval(['conf.',conf.varname_loop2,'=conf.vector_loop2
(j);']);
solution{i,j} = main(conf); % Common main structure is called
summary{i,j} = conf.displayfunc(solution{i,j},conf); % ...
Particular visualization
end
end
Listing 3.28 Run execution (run.m).
function solution = main(conf)
for m = 1:conf.NREPETS
conf.I = m; fprintf('Executing run %d...\n',m);
% Specified dataset is generated or loaded
[Xtrain,Ytrain,Xtest,Ytest] = load_data(conf);
% Specified algorithms are used
for ialg = 1:length(conf.machine.algs)
conf.machine.function_name = conf.machine.algs{ialg};
alg = func2str(conf.machine.function_name);
% Free parameters are selected by a searching function
conf.machine.value_range = ...
conf.cv.value_range(conf.machine.ind_params{ialg});
param_selec = conf.cv.searchfunc(Xtrain,Ytrain,conf,Xtest,Ytest);
ind_params = conf.machine.ind_params{ialg};
for ip = 1:length(ind_params)
solution.(alg)(m).(conf.machine.params_name
{ind_params(ip)}) = param_selec{ip};
end
% Adjusted algorithms are trained and applied to predict
[Ypred,coefs] = conf.machine.function_name(Xtrain,Ytrain,Xtest,
param_selec{:});
solution.(alg)(m).coefs = coefs;
solution.(alg)(m).Ytestpred = Ypred;
solution.(alg)(m).Ytest = Ytest;
solution.(alg)(m).Xtest = Xtest;
solution.(alg)(m).Xtrain = Xtrain;
solution.(alg)(m).Ytrain = Ytrain;
for ie = 1:length(conf.evalfuncs)
evalfunc = conf.evalfuncs{ie};
evalname = func2str(evalfunc);

Signal Processing Models
163
solution.(alg)(m).(evalname) = evalfunc(Ypred,Ytest);
end
end
end
Listing 3.29 Main function (main.m).
The search functions can be seen in the code URL, and their names are
cross_validation.m, gridSearch.m, and sequentialSearch.m. Though the
functions calculating the evaluation measurements can be used in diﬀerent examples,
they will be presented in subsequent sections when their use is needed. Function
load_data.m in Listing .addresses the uniﬁcation of the data structures for
two diﬀerent kind of tasks; namely, convolutional problems and machine-learning
problems.
function [Xtrain,Ytrain,Xtest,Ytest] = load_data(conf)
data = conf.data.loadDataFuncName(conf);
if isfield(data,'h') % Deconvolution problem
Xtrain = data.z; Xtest = data.u;
Ytrain = data.h; Ytest = data.x;
elseif isfield(data,'Xtrain') % Learning task
Xtrain = data.Xtrain; Xtest = data.Xtest;
Ytrain = data.Ytrain; Ytest = data.Ytest;
else
error('ErrorTests:convertTest', 'Unexpected data structure ...
generated...\nFor further information see: load_data.m');
end
Listing 3.30 Loading structured data (load_data.m).
The results obtained are always given back with the same structure, and the code for
this is in Listing ..
function solution_summary = results_summary(solution,conf)
algs = fields(solution);
addstr = ''; extra1 = '%.2f'; extra2 = extra1;
cname1 = conf.(conf.varname_loop1); cname2 = conf.(conf.varname_loop2);
if iscell(cname1); cname1 = cname1{1}; extra1 = '%s'; end
if iscell(cname2); cname2 = cname2{1}; extra2 = '%s'; end
cnf1 = sprintf(['%s = ',extra1],conf.varname_loop1,cname1);
cnf2 = sprintf(['%s = ',extra2],conf.varname_loop2,cname2);
cv1 = conf.varname_loop1; cv2 = conf.varname_loop2;
if ~strcmp(cv1,'idle1') && ~strcmp(cv2,'idle2');
addstr = sprintf('(%s, %s)',cnf1,cnf2);
elseif ~strcmp(cv1,'idle1')
addstr = sprintf('(%s)',cnf1);
elseif ~strcmp(cv2,'idle2')
addstr = sprintf('(%s)',cnf2);
end
fprintf('\n---------------------------------------------------
----------\n');
fprintf(' Performance summary %s\n',addstr);
fprintf('---------------------------------------------------
----------\n');
for ie = 1:length(conf.evalfuncs)

164
Digital Signal Processing with Kernel Methods
evalname = func2str(conf.evalfuncs{ie});
fprintf('%s\n',evalname)
for ia = 1:length(algs)
err = cat(1,solution.(algs{ia}).(evalname));
solution_summary.(algs{ia}).([evalname,'_mean']) = mean(err);
solution_summary.(algs{ia}).([evalname,'_std']) = std(err);
solution_summary.(algs{ia}).([evalname,'_vector']) = err;
fprintf('
\t- %s \t %s: Mean = %.2e
Std = %.2e \n',...
algs{ia}, evalname,...
solution_summary.(algs{ia}).([evalname,'_mean']),...
solution_summary.(algs{ia}).([evalname,'_std']));
end
end
fprintf('----------------------------------------------------
---------\n');
Listing 3.31 Displaying performance summary (results_summary.m).
Finally, and given that an SVM is going to be a very used algorithm throughout
Chapters , , and in terms of diﬀerent kernel matrix used as inputs, we present in
Listing .the shared part of each of the presented solutions using the SVM algorithm.
function [predict,model] = SVM(X,Y,Xtest,inparams)
% svm_train = @mysvmtrain;
% svm_predict = @mysvmpredict;
% Training SVM
model = mexsvmtrain(Y,X,inparams);
% Prediction
predict = zeros(size(Xtest,1),1);
if ~isempty(model) && ~isempty(model.SVs) && ~isempty(Xtest)
predict = mexsvmpredict(zeros(size(Xtest,1),1),Xtest,model);
end
Listing 3.32 Common SVM code (SVM.m).
Note that the trained machine model is ﬁrst obtained with a set of training data X and
Y, and with some previously tuned free parameters inparams, and then the prediction
predict is obtained for a given test set Xtest, as indicated.
Functions mysvmtrain and mysvmpredict are modiﬁed versions of LibSvm (Chang
and Lin, ), and they allow us to include some additional parameters. The input
parameters format is just the same as in LibSvm.

165
4
Kernel Functions and Reproducing Kernel Hilbert Spaces
Whereas Chapter gave a (moderately deep) introduction to the signal processing
concepts to be used in this book, in this chapter we put together fundamental and
advanced relevant concepts on Mercer’s kernels and reproducing kernel Hilbert spaces
(RKHSs). The fundamental building block of the kernel learning theory is the kernel
function, which provides an elegant framework to compare complex and nontrivial
objects. After its introduction, we review the concept of an RKHS, and state the
representer theorem. Then we study the main properties on kernel functions and their
construction, as well as the basic ideas to work with complex objects and reproducing
spaces. The support vector regression (SVR) algorithm is also introduced in detail, as it
will be widely used and modiﬁed in Part II for building many of the DSP algorithms
with kernel methods therein. We end up the chapter with some synthetic examples
illustrating the concepts and tools presented.
4.1
Introduction
Kernel methods build upon the notion of kernel functions and RKHSs. Roughly speak-
ing, a Mercer kernel in a Hilbert space is a function that computes the inner product
between two vectors embedded in that space. These vectors are maps of vectors in an
Euclidean space, where the mapping function can be nonlinear. This informal deﬁnition
will be formalized through Mercer’s theorem, which gives the analytical power to kernel
methods.
We will see that kernel functions are very often dot products in inaccessible Hilbert
spaces. This means that vectors in that space are not explicitly deﬁned; hence, only
the dot products are accessible, but not the mapped vector coordinates therein. In
spite of this, the expression of the dot product is the only tool needed to operate in
such space. The RKHSs that are summarized in this chapter allow us to implicitly map
a ﬁnite-dimension vector from an Euclidean space (typically known as input space
) into a higher dimension Hilbert space (referred as feature space ) by expressing
the dot product inside this space as a function of the input space vectors solely. This
property has allowed kernel methods to become a preferred tool in machine learning
and pattern recognition for years, being an attractive alternative to traditional methods
in statistics and signal processing. In the past decade, methods based on kernels have
gained popularity in almost all applications of machine learning because of several
Digital Signal Processing with Kernel Methods, First Edition. José Luis Rojo-Álvarez, Manel Martínez-Ramón,
Jordi Muñoz-Marí, and Gustau Camps-Valls.
© John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.

166
Digital Signal Processing with Kernel Methods
Classification
Linear discriminant analysis
Decision trees
Neural networks
Clustering
k-means, fuzzy k-means
Hierarchical clustering
Gaussian mixture models
Density estimation
Gaussian mixtures
Parzen windows
RBIG
Dependence
Mutual information
Correlation
Cross-covariance
Time series
ARMA
Kalman filters
Volterra filters
Linear regression
Decision trees
Splines
Neural networks
Regression
Feature extraction
Principal component analysis
Independent component analysis
Partial least squares
Figure 4.1 Different machine learning algorithms are available to tackle particular machine learning
and signal-processing problems.
fundamental reasons. In particular, we can identify three main motivations for the use
of kernel techniques in machine learning and signal processing.
The ﬁrst one consists of the change of paradigm from classical nonlinear algorithms to
kernel techniques that still rely on linear algebra. Linear methods used in statistics and
machine learning typically resort to linear algebra operations, are very well established,
and their properties and limits are mostly known. These facts give the user the tools
needed to adapt linear algorithms to the application at hand and to obtain solutions
very easily. Linear algorithms, however, are restricted to capturing linear relations
between the features and dependent variables, and very often only capture second-order
statistical relations. Such limitations call for extensions to nonlinear and higher order
statistics algorithms. For every learning problem encountered in machine learning and
signal processing, a full toolbox of nonlinear extensions is available (see Figure .).
Diﬀerent approaches, however, include the nonlinearity in very diﬀerent ways, as well
as a particularly diﬀerent set of parameters to tune and criteria to optimize their design.
For example, the most well-known family of nonlinear algorithms is probably that of
artiﬁcial NNs. In this case, a point-wise nonlinearity is included in every node/neuron
in the network, and several parameters have to be tuned to control the model capacity
(in this case, its architecture).
NNs dominated the ﬁeld and applications in the s and s, and often produce
improved performance with respect to linear algorithms. Nevertheless, a clear short-
coming was identiﬁed, which was that the knowledge about linear techniques cannot
be easily translated into the design of the nonlinear ones. Actually, when using linear
algorithms, a well-established theory and eﬃcient methods are often available, but
such advantages are lost when translating the linear neuron model into even a simple
sigmoid-shaped neuron model. Kernel methods oﬀer the opportunity to translate linear

Kernel Functions and Reproducing Kernel Hilbert Spaces
167
models into nonlinear models while still working with linear algebra. In this way, kernel
methods will allow us to exploit all the intuitions and properties of linear algorithms.
Essentially, kernel methods embed the data set S deﬁned over the input or attribute
feature space (S ⊆) into a higher (possibly inﬁnite-dimensional) Hilbert space ,
also known as kernel feature space, and a linear algorithm is built on this last for yielding
a nonlinear algorithm with respect to the input data space.
The second reason is related to the paradigm change introduced by kernel methods.
They allow the user to take linear algorithms expressed in an Euclidean space and, by
means of an often straightforward kernelization procedure, consisting of deriving a
nonlinear counterpart of the algorithm, they provide nonlinear properties to the linear
algorithms. The kernelization procedure consists of two basic steps. The ﬁrst of them
tries to ﬁnd an expression of the linear algorithm as a function of dot products between
data only. This representation is called the dual representation in contrast to the primal
representation.This is always possible for a linear algorithm by just taking into account
that the parameters learned by the linear algorithm lie inside the subspace spanned by
the data. A representation of these parameters as a linear combination of the mapped
data straightforwardly leads to a dual representation, the combination parameters being
the so-called dual parameters. The second step consists of the substitution of the
Euclidean dot product by a dot product into an RKHS. From this point of view, kernel
methods are methods that are linear in the parameters, or alternatively they are linear
inside the RKHS. Notationally, the mapping function is denoted as 𝝓∶→.
Whereas the nonlinear version of linear algorithms will exhibit increased ﬂexibility,
the computational load should increase, especially when we need to compute the sample
new coordinates explicitly in that high-dimensional space. But this computation can be
omitted by using the kernel trick; that is, when an algorithm can be expressed using
dot products in the input space, then its (nonlinear) kernel version only needs the dot
products between mapped samples. Kernel methods compute the similarity between
examples (samples, objects, patterns, points) S = {xi}N
i=using inner products between
mapped examples, and thus the so-called kernel matrix K, with entries K i,j = K(xi, xj)
= ⟨𝝓(xi), 𝝓(xj)⟩, contains all the necessary information to perform many classical linear
algorithms in the feature space.
Figure .illustrates the concepts of kernel feature mapping, 𝝓, and the exploitation
of the kernel trick. The latter will allow us to measure similarities in the Hilbert space
using a reproducing kernel K that solely works with input examples from . In this
example, the input space is not a proper representation space because the endowed
Euclidean metric would wrongly tell that distant points along the manifold, xi and
xj, are close. This is remedied by mapping the input examples to a Hilbert spaces 
such that unfolds the original banana-shaped data distribution such that the linear dot
product is an appropriate one. Computing such a dot product in would be impossible
unless the feature mapping is explicit. Fortunately, as we will see later, neither knowing
the mapping nor the coordinates of the mapped data are strictly necessary, as we can
The dual representation is known in multivariate statistics as the Q-mode as opposed to the primal
R-mode. Operating in Q-mode typically endows the method with numerical stability and computational
eﬃciency in high-dimensional problems.

168
Digital Signal Processing with Kernel Methods
ϕ(·)
ϕ(·)
Similarity
measure
xi
T xj
ϕ(xi)
K(xi, xj) = ϕ(xi)T ϕ(xj)
xi
xj
ϕ(xj)
Figure 4.2 Illustration of the kernel feature mapping, 𝜙, and the kernel trick by which we can measure
similarities in using a reproducing kernel that solely works with input examples from .
ﬁnd a kernel function K that reproduces the similarity in solely working with input
examples from .
By comparison again with linear classical techniques, there is a third very important
motivation for the use of kernel methods that is related to the complexity of nonlinear
machines. If the structural complexity of a machine is too high, it is known that it may
be unable to properly generalize. This is known as the problem of overﬁtting. In practice
this problem boils down to models capable of adjusting the parameters so that the
training data are fully explained, but then being unable to properly explain new, unseen,
test data. Nonlinear learning machines typically introduce many parameters (on some
occasions more than examples in the dataset) so they become extremely ﬂexible and able
to overﬁt the training set well. Limiting the capacity of the class of functions of interest
is intimately related to the concept of regularization, already introduced in Chapter 
for DSP signal models. We will see here how kernel methods include regularization
in a very natural way, even working in inaccessible feature spaces. We will see also
that kernel methods can generalize the concept of Tikhonov’s regularization to inﬁnite-
dimensional feature spaces.
Finally, we should mention that kernel methods have been widely adopted in applied
communities such as computer vision, time-series analysis and econometrics, physical
sciences, as well as signal and image processing, because these methods typically lead to
neat, simple, and intuitive algorithms in these ﬁelds. But, in addition, kernel methods
allow the exploitation of a vast variety of kernel functions that can be designed and
adapted to the application at hand. For example, combining and fusing heterogeneous
signal sources (text, images, graphs) can be done easily via concepts of multiple kernel
learning (MKL). Also, learning the kernel function directly from data is possible via
generative models and exploitation of the wealth of information in the unlabeled data.
Kernel design is an open research topic in machine learning and signal processing that
does not stop giving theoretical results and designs for many diﬀerent applications. In
this chapter, we also review the main properties of kernel methods to construct new
kernel functions.

Kernel Functions and Reproducing Kernel Hilbert Spaces
169
4.2
Kernel Functions and Mappings
Kernel methods rely on the properties of kernel functions. As we will see next, they
reduce to computing dot products of vectors mapped in Hilbert spaces through an
implicit (not necessarily known) mapping function. A scalar product space (or pre-
Hilbert space) is a space endowed with a scalar product. If the space is complete (i.e.,
every Cauchy sequence converges inside the space), then it is called a Hilbert space.
This section conveys a short introduction to kernel methods, functions approxima-
tion, and RKHSs. The tools reviewed here are of fundamental relevance for the next
sections, and build upon solid branches of mathematics such as linear algebra (Golub
and Van Loan, ) and functional analysis (Reed and Simon, ).
4.2.1
Measuring Similarity with Kernels
The dot (scalar or inner) product between two vectors is an algebraic operation that
measures the similarity between them. Intuitively, a dot or scalar product measures
how much of a vector is contained in the other, or alternatively how much the two
vectors point in the same direction. The question raised here is whether one should
naively rely on the dot product in the input space to measure the similarity of (possibly
complex) objects or not. If the representation space of those objects is rich enough, a
dot product in input space should suﬃce. Unfortunately, many times the input feature
space is limited in resolution, and expressive power and a new richer representation
is needed. Kernel methods rely on the notion of similarity between examples in a
higher (possibly inﬁnite-dimensional) Hilbert space. Consider the set of empirical data
(x, y), … , (xn, yn) ∈× , in which xi are the inputs taken from and yi ∈are
called the outputs. The learning from samples process consists of using these data pairs
to predict a new set of test examples x ∈. In order to develop learning machines
capable of generalizing well, kernel methods often support a good approximation to
the structure of the data and incorporate regularization in a natural way.
In some of those cases when does not stand for a good support of similarity,
examples can be mapped to a (dot product) space by using a mapping 𝝓∶→
, x ↦𝝓(x). The mapping function can be deﬁned explicitly (if some prior knowledge
about the problem is available) or implicitly, as in the case of kernel methods. The
similarity between the elements in can now be measured using its associated dot
product ⟨⋅, ⋅⟩. Here, we deﬁne a function that computes that similarity , K ∶× →
ℝ, such that (x, x′) ↦K(x, x′). This function, often called a kernel, is required to satisfy
K(x, x′) = ⟨𝝓(x), 𝝓(x′)⟩.
(.)
The mapping 𝝓is its feature map, the space is the reproducing Hilbert feature space,
and K is the reproducing kernel function since it reproduces dot products in without
even mapping data explicitly therein.
4.2.2
Positive-Definite Kernels
The types of kernels that can be written in the form of Equation .coincide with the
class of positive-deﬁnite kernels. The property of positive deﬁniteness is crucial in the
ﬁeld of kernel methods.

170
Digital Signal Processing with Kernel Methods
Deﬁnition ..
A function K ∶× →ℝis a positive-deﬁnite kernel if and only if
there exists a Hilbert space and a feature map 𝝓∶→such that for all x, x′ ∈
we have K(x, x′) = ⟨𝝓(x), 𝝓(x′)⟩for some inner-product space such that ∀x ∈
and 𝝓∈.
Deﬁnition ..
A kernel matrix (or Gram matrix) K is the matrix that results from
applying the kernel function K to all pairs of points in the set {xi}N
i=; that is:
K =
⎛
⎜
⎜
⎜⎝
K(x, x)
K(x, x)
⋯
K(x, xN)
K(x, x)
K(x, x)
⋯
K(x, xN)
⋮
⋮
⋱
⋮
K(xN, x)
K(xN, x)
⋯
K(xN, xN)
⎞
⎟
⎟
⎟⎠
,
(.)
and whose entries are denoted as K i,j = K(xi, xj).
Kernel functions must be symmetric since inner products are always symmetric. In
order to show that K is a valid kernel, it is not always suﬃcient to show that a mapping
𝝓exists; rather, this is a nontrivial theoretical task. In practice, a real symmetric N ×
N matrix K, whose entries are K(xi, xj) or simply K i,j, is called positive-deﬁnite if, for
all 𝛼, … , 𝛼N ∈ℝ, ∑N
i,j=𝛼i𝛼jK i,j ≥. A positive-deﬁnite kernel produces a positive-
deﬁnite Gram matrix in the RKHS.
Deﬁnition ..
Kernel matrices that are constructed from a kernel corresponding
to a strict inner product space are positive semideﬁnite.
This last property is easy to prove. Note that, by construction, we have K i,j
=
K(xi, xj) = ⟨𝝓(xi), 𝝓(xj)⟩; thus, for any vector 𝜶∈ℝN:
𝜶TK𝜶=
N
∑
i=
N
∑
j=
𝛼iK i,j𝛼j =
N
∑
i=
N
∑
j=
𝛼i⟨𝝓(xi), 𝝓(xj)⟩𝛼j
=
⟨
∑
i
𝛼i𝝓(xi),
∑
j
𝛼j𝝓(xj)
⟩

=
‖‖‖‖‖‖
N
∑
i=
𝛼i𝝓(xi)
‖‖‖‖‖‖


≥.
(.)
Accordingly, any algorithm which operates on the data in such a way that it can
be mathematically described in terms of dot products can be used with any positive-
deﬁnite kernel by simply replacing ⟨𝝓(x), 𝝓(x′)⟩with kernel evaluations K(x, x′),
a technique known as kernelization or a kernel trick (Schölkopf and Smola, ).
Moreover, for a positive-deﬁnite kernel we do not need to know the explicit form of
the feature map, but instead it is implicitly deﬁned through the kernel calculation.
4.2.3
Reproducing Kernel in Hilbert Space and Reproducing Property
In this section we will deﬁne the notion of an RKHS through the reproducing property.
Then, we will see that a kernel function is a positive-deﬁnite and symmetric function.

Kernel Functions and Reproducing Kernel Hilbert Spaces
171
It is also relevant to observe that the kernel fully generates the space, and that for
a given kernel there is a unique RKHS; conversely, every RKHS contains a single
kernel.
Let us assume a Hilbert space where its elements are functions, provided with a
dot product ⟨⋅, ⋅⟩. We will denote f (⋅) as one element of the space, and f (x) as its value
at a particular argument x. We will assume that arguments belong to a real or complex
Euclidean space; that is, x ∈ℝN or x ∈ℂN respectively.
Deﬁnition ..
RKHS (Aronszajn, ). A Hilbert space is said to be an RKHS
if: () the elements of are complex- or real-valued functions f (⋅) deﬁned on any set of
elements x; and () for every element x, f (⋅) is bounded.
The name of these spaces comes from the so-called reproducing property. Indeed, in
an RKHS , there exists a function K(⋅, ⋅) such that
f (x) = ⟨f (⋅), K(⋅, x)⟩,
f ∈
(.)
by virtue of the Riesz representation theorem (Riesz and Nagy, ). This function is
called a kernel.
Property 
The kernel K(⋅, ⋅) is a positive-deﬁnite and symmetric function.
Assume that f (⋅) = K(⋅, x) in Equation .. Hence:
K(x, x′) = ⟨K(⋅, x), K(⋅, x′)⟩.
(.)
Applying the complex conjugate operator ∗in the kernel and the dot product leads to
K∗(x, x′) = (⟨K(⋅, x), K(⋅, x′)⟩)∗= ⟨K(⋅, x′), K(⋅, x)⟩= K(x′, x),
(.)
which proves that a kernel is symmetric. In addition, consider the series
N
∑
i=
𝛼iK(⋅, xi),
(.)
where 𝛼i is any ﬁnite set of complex numbers. The norm of this series can be computed
as
‖‖‖‖‖‖
N
∑
i=
𝛼iK(⋅, xi)
‖‖‖‖‖‖

=
⟨n
∑
i=
𝛼iK(⋅, xi),
N
∑
j=
𝛼j, K(⋅, xj)
⟩
.
(.)
By virtue of the reproducing property, K(xi, xj) = ⟨K(⋅, xi), K(⋅, xj)⟩, and by the linearity
of the dot product, Equation .can be written as
‖‖‖‖‖‖
N
∑
i=
𝛼iK(⋅, xi)
‖‖‖‖‖‖

=
n
∑
i=
n
∑
j=
𝛼i𝛼j, K(xi, xj) ≥,
(.)

172
Digital Signal Processing with Kernel Methods
which proves that the kernel function is positive deﬁnite.
Property 
The RKHS with a kernel K is generated by K(⋅, x), where x belongs to
any set.
It follows from the reproducing property that if ⟨f (x), K(⋅, x)⟩= , then, necessarily
f (x) = ; hence, all elements in are generated by the kernel.
Property 
An RKHS contains a single reproducing kernel. Conversely, a reproducing
kernel uniquely deﬁnes an RKHS.
If we consider the linear space generated by K, we can deﬁne
⟨K(⋅, x), K(⋅, x′)⟩= K(x, x′).
(.)
Let us now consider two sets of scalars 𝜇i and 𝜆j, ≤i, j ≤N; then:
⟨N
∑
i=
𝜇iK(⋅, xi),
N
∑
j=
𝜆jK(⋅, xj)
⟩
=
N
∑
i=
N
∑
j=
𝜇i𝜆jK(xi, xj).
(.)
This expression satisﬁes the reproducing property and the requirements of dot product
as follows:
K(x, x′)
=
K∗(x′, x′)
⟨K(⋅, x) + K(⋅, x′), K(⋅, x′′)⟩
=
⟨K(⋅, x), K(⋅, x′)⟩+ ⟨K(⋅, x), K(⋅, x′′)⟩
⟨𝜆K(⋅, x), K(⋅, x′)⟩
=
𝜆⟨K(⋅, x), K(⋅, x′)⟩
⟨K(⋅, x), K(⋅, x)⟩
=
if and only if K(⋅, x) = .
The last requirement can be proven by using the Cauchy–Schwartz inequality:
‖‖‖‖‖‖
N
∑
i=
𝜆iK(⋅, xi), K(⋅, x)⟩
‖‖‖‖‖‖

≤
N
∑
i=
𝜆iK(⋅, xi),
N
∑
j=
𝜆jK(⋅, xj)⟩⟨K(⋅, x), K(⋅, x)⟩.
(.)
Hence:
⟨N
∑
i=
𝜆iK(⋅, xi),
N
∑
j=
𝜆jK(⋅, xj)
⟩
= 
(.)
implies
∑
i=
𝜆iK(x, xi) = 
(.)
for every x. Therefore, K is unique in and it generates a unique RKHS.

Kernel Functions and Reproducing Kernel Hilbert Spaces
173
4.2.4
Mercer’s Theorem
Mercer’s theorem is one of the best known results of the mathematician James Mercer
(January , –February , ), and of fundamental importance in the context
of kernel methods. Mercer’s theorem is the key idea behind the so-called kernel trick,
which allows one to solve a variety of nonlinear optimization problems through the
construction of kernelized counterparts of linear algorithms. Mercer’s theorem can be
stated as follows.
Assume that K(⋅, ⋅) is a continuous kernel function satisfying the properties in
Section ... Assume further that the kernel belongs to the family of square integrable
functions.
Theorem ..(Mercer’s theorem (Aizerman et al., ))
Let K(x, x′) be a bivari-
ate function fulﬁlling the Mercer condition; that is, ∫ℝNr ×ℝNr K(x, x′)f (x)f (x′)dxdx′ ≥
for any square integrable function f (x). Then, there exists a RKHS and a mapping 𝝓(⋅)
such that K(x, x′) = ⟨𝝓(x), 𝝓(x′)⟩.
We will now consider a set D embedded in a space; typically, we will use only ℝd or
ℂd. From Mercer’s theorem, it follows that a mapping function 𝝓∶→can be
expressed as a (possibly inﬁnite dimension) column vector:
𝝓(x) = {
√
𝜆i𝝓i(x)}∞
i=.
(.)
The dot product between two of these maps 𝝓(xi) and 𝝓(x) is deﬁned then as the kernel
function of vectors x and x′ as
⟨𝝓(x), 𝝓(x′)⟩= 𝝓(x)⊤𝝓(x′) =
∞
∑
i=
𝜆i𝝓i(x)𝝓i(x′) = K(x, x′).
(.)
Mercer’s theorem shows that a mapping function into an RKHS and a dot product K(⋅, ⋅)
exist if and only if K(⋅, ⋅) is a positive-deﬁnite function. Hence, if a given function ×
→ℝ(or ℂ) is proven to be positive deﬁnite, then it is the kernel of a given RKHS.
Example ..
Kernel function of band-limited signals. Consider the interval 𝜔∈
[−W, W], and the subspace of square integrable functions generated by complex
exponentials as
𝝓(t) = e𝕚𝜔t.
(.)
Since the dot product of square integrable functions is the integral of their product, the
kernel function inside this RKHS is
⟨h(t), h(t)⟩=

W ∫
W
−W
e𝕚𝜔te−𝕚𝜔td𝜔= sin(W(t−t))
W(t−t)
.
(.)

174
Digital Signal Processing with Kernel Methods
This is then the kernel of the RKHS of signals that are band-limited to interval W.
Moreover, the scalar dot product between functions is then
⟨f (⋅), g(⋅)⟩= ∫
W
−W
F(𝜔)G∗(𝜔) d𝜔,
(.)
where F(𝜔) and G(𝜔) are the FT of f (t) and g(t) respectively. Indeed, if we denote the
FT operator as () and use this dot product, the reproducing property
f (t) = ⟨f (⋅), K(⋅, t)⟩= ∫
W
−W
F(𝜔)∗(K(⋅, t)) d𝜔
(.)
holds. From Equation ., it follows that (K(⋅, t)) = e−𝕚𝜔t. Then:
f (t) = ⟨f (⋅), K(⋅, t)⟩= ∫
W
−W
F(𝜔) e𝕚𝜔t d𝜔.
(.)
Example ..
Second-order polynomial kernel. Kernel K(xi, xj) = (+xT
i xj)is a par-
ticular case of nth-order polynomial kernels. Assuming that xi ∈ℝ, it is straightforward
to ﬁnd the mapping function 𝝓∶ℝ→. If xi = {x()
i , x()
i }, the dot product is
K(xi, xj) = (+ xT
i xj)=
(
+ x()
i x()
j
+ x()
i x()
j
)
,
(.)
which can be expanded as
K(xi, xj) = + (x()
i x()
j )+ (x()
i x()
j )+ x()
i x()
j
+ x()
i x()
j
+ x()
i x()
j x()
i x()
j .
By visual inspection, the corresponding mapping can be found to be
𝝓(x) = (, (x()), (x()),
√
x(),
√
x(),
√
x()x())T.
(.)
Each element of this vector is an eigenfunction of this space. The corresponding
eigenvectors are respectively 𝜆= 𝜆= 𝜆= , and 𝜆= 𝜆= 𝜆=
√
.
4.3
Kernel Properties
This section pays attention to important properties of kernel methods: the issue of
regularization, the representer’s theorem, and basic operations that can be implicitly
done in RKHSs via kernels.

Kernel Functions and Reproducing Kernel Hilbert Spaces
175
4.3.1
Tikhonov’s Regularization
Regularization methods are those methods intended to turn an ill-posed problem
into a well-posed one such that a stable solution exists. Well-posedness is a concept
introduced by Hadamard in to determine the solvability of mathematical models
of physical phenomena. He deﬁned a problem as well posed if a solution of the problem
exists and the solution is unique. Also, the solution must be stable; that is, when an
initial condition slightly changes, the solution must not change abruptly. In particular,
if a parameter estimation problem does not verify these three conditions, it is said to
be unstable. A regularization method that assures well-posedness of problems is the
Tikhonov minimization, which ensures stability of solutions in this sense (Tikhonov
and Arsenin, ) (see Section ..).
Notationally, let us assume a set of N pairs of data examples {xi, yi}; the problem
consists of ﬁnding an estimation function f parameterized by a set of weights a such that
we approximate observations as yi = f (xi, a) + 𝜖i, where 𝜖i is the estimation error. The
regularization procedure consists of constructing a functional (see Deﬁnition ..):
=
N
∑
i=
V(yi, f (xi, a)) + 𝜆Ω( f ),
(.)
where V(⋅) is a cost function over the empirical error or risk of the estimation procedure,
and Ω(⋅) plays the role of a regularizer over the parameters of the estimation function
f (⋅). The idea behind the application of this functional is that, whereas the empirical risk
is used in order to choose those parameters that produce the best prediction of yi given
xi, the regularizer is used to account for smoothness (ﬂatness) of the solution. Let us
exemplify the concept of smoothness through a simple example.
Example ..
Assume an arbitrary system expressed by
f (x) = −.x−.x + .
(.)
for x ∈ℝ; this produces outputs yi that are corrupted with zero mean white Gaussian
noise of 𝜎n = .. A th-order polynomial deﬁned as
̂f (x) = ax+ ax+ ax+ ax+ ax+ ax + a
(.)
is proposed for approximating the system’s output. This model clearly has a complexity
much higher than needed, and seven parameters including a bias term need to be
adjusted, which are collectively grouped in a = [a, … , aM], M = . A simple LS
adjustment can be used to approximate the data. The polynomial shows a small error
over the training sample, as is seen by the dotted line in Figure .. Nevertheless, since
the output data contain a given amount of noise, the test results in poor estimations
with respect to the real function (continuous line). A regularized functional of the form
=
N
∑
i=
‖yi −f (xi, a)‖+ 𝜆
M
∑
j=
a
j ,
(.)

176
Digital Signal Processing with Kernel Methods
Figure 4.3 Nonlinear problem solved using two 6th-order polynomials. Circles represent points
generated by the function (solid line) plus i.i.d. Gaussian noise. The dashed line corresponds to a
regularized solution, while the dotted line corresponds to an unregularized solution (𝜆= 0).
which is known as ridge regression (Hoerl and Kennard, ) (see Exercise ..) can
be applied in order to obtain a smoother function (dashed line) that better approximates
the real function. The estimated parameters of the polynomial are as follows:
a0
a1
a2
a3
a4
a5
a6
LS
.
.
−.

−.
.
−.
RR
.
.
.
.
−.
−.
−.
which show how the range and variance of the obtained weights are constrained in the
regularized solution, while the unregularized LS solution (𝜆= ) yields big and uneven
weights that produce unstable results (see Figure .). Section ..shows the extension
of regularization to the ﬁeld of RKHSs.
4.3.2
Representer Theorem and Regularization Properties
The representer theorem (Kimeldorf and Wahba, ) is a fundamental result in the
ﬁeld of kernel machines, since it establishes a generalization of the idea of regularization
into an RKHS. The ﬁrst motivation of the theorem is that, given a set of data, a linear
expression of a solution based on a regularized functional exists in terms of a linear
combination of the maps of the training data inside the RKHS. Moreover, recall that
many kernels are expressed in inﬁnite-dimension Hilbert spaces. This may completely
preclude a good generalization, since the use of certain kernel functions may lead to a
perfect ﬁt of any data regardless of their statistical properties. Regularization becomes
mandatory in all these cases. The second motivation is related to the fact that the
implementation of smoothness is attached to the particular kernel chosen to solve the
estimation problem. The theorem statement is as follows.
Theorem
..
(Representer
theorem)
(Kimeldorf
and
Wahba,
)
Let
Ω ∶[, ∞) →ℝbe a strictly monotonic increasing function; let V ∶(× ℝ)N →
ℝ∪{∞} be an arbitrary loss function; and let be an RKHS with reproducing kernel
K. Then:

Kernel Functions and Reproducing Kernel Hilbert Spaces
177
f ∗= min
f ∈{V ((f (x), x, y), … , (f (xN), xN, yN)) + Ω(‖f ‖
)}
(.)
admits a representation
f ∗(⋅) =
N
∑
i=
𝛼iK(⋅, xi),
𝛼i ∈ℝ, 𝜶∈ℝn×
(.)
That is, the function that minimizes the (regularized) optimization functional in Equa-
tion .is a linear function of dot products between data mapped into RKHS .
Since the mapped training data 𝝓(xi) span a subspace inside the RKHS, a solution
f may be expressed as a linear combination of these data plus a function g belonging to
⟂:
f =
N
∑
i=
𝛼iK(⋅, xi) + g(⋅)
(.)
for which, obviously,
⟨𝝓(xi), g⟩= .
(.)
Then, on the one hand, by virtue of the reproducing property, for any training data point
xj the approximation function is
f (xj) =
N
∑
i=
𝛼iK(xj, xi) + ⟨xj, g(⋅)⟩
(.)
and, on the other hand, the regularization function Ω satisﬁes
Ω
(‖‖‖‖‖‖
N
∑
i=
𝛼iK(⋅, xi) + g(⋅)
‖‖‖‖‖‖
)
= Ω
⎛
⎜
⎜
⎜⎝
√
√
√
√
√
‖‖‖‖‖‖
N
∑
i=
𝛼iK(⋯, xi)
‖‖‖‖‖‖

+
√
‖g(⋅)‖
⎞
⎟
⎟
⎟⎠
≥Ω
(
‖
N
∑
i=
𝛼iK(⋅, xi)‖
)
,
(.)
which proves the theorem. We should stress that the representer theorem (Kimeldorf
and Wahba, ) states that the solutions of a large class of optimization problems can
be expressed by only a ﬁnite number of kernel functions. Hence its importance.
Example ..
Let us consider a linear prediction model f (x) = ⟨f , K(⋅, x)⟩. A
common regularizer in machine learning and statistics is the 𝓁-norm ‖ f ‖
.

178
Digital Signal Processing with Kernel Methods
By virtue of the representer theorem, we can express the learning function f as a linear
combination of kernel functions f = ∑N
i=𝛼iK(⋅, xi); then:
‖ f ‖= ⟨f , f ⟩=
⟨N
∑
i=
𝛼iK(⋅, xi),
N
∑
j=
𝛼jK(⋅, xj)
⟩
=
N
∑
i=
N
∑
j=
𝛼i𝛼j
⟨K(⋅, xi)K(⋅, xj)⟩
N
∑
i=
N
∑
j=
𝛼i𝛼jK(xi, xj) = 𝜶TK𝜶.
(.)
Note that if we express f (x) = ⟨w, K(⋅, x)⟩= wT𝝓(x) then w = ∑N
i=𝛼i𝝓(xi) and
wTw = 𝜶TK𝜶.
4.3.3
Basic Operations with Kernels
We now review some basic properties with kernels. The interest here is to intro-
duce some basic operations that allow us to operate with objects embedded in high-
dimensional, often inaccessible, feature spaces. Essentially, we will show that we can
compute distances, norms, and angles, as well as perform projections and normaliza-
tions, in the feature space via kernels in an implicit way.
Translation. A translation in feature space can be expressed as the modiﬁed fea-
ture map ̃𝝓(x) = 𝝓(x) + Γ, Γ ∈. Then, the translated dot product for
⟨̃𝝓(x), ̃𝝓(x′)⟩can be computed if we restrict Γ to lie in the span of the functions
{𝝓(x), … , 𝝓(xN)} ∈.
Centering. The previous translation allows us to center data {xi}N
i=∈in the feature
space. The mean of the data in is 𝝓𝜇= c(∕N) ∑N
i=𝝓(xi), which is a linear
combination of the span of functions, and hence it fulﬁlls the requirement for
Γ. One can center data in by computing ̃K = HKH, where entries of H are
Hij = 𝛿ij −(∕N), and the Kronecker symbol is 𝛿i,j = if i = j and zero otherwise.
Subspace projections. Given two points 𝝍and 𝜸in the feature space, the projection
of 𝝍onto the subspace spanned by 𝜸is
𝝍′ = ⟨𝜸, 𝝍⟩
‖𝜸‖

𝜸.
Therefore, one can compute the projection 𝝍′ expressed solely in terms of kernel
evaluations. If one has access to vectors 𝝍and 𝜸, the projection can be computed
explicitly. Otherwise, one can compute the dot products of the pre-image vectors
using reproducing kernels. For example, assuming pre-images 𝝍= 𝝓(x) and 𝜸=
𝝓(z), the norm of a vector in Hilbert spaces can be computed using a reproducing
kernel; that is, ‖𝜸‖
= ⟨𝝓(z), 𝝓(z)⟩= KΓ(z, z), and ⟨𝜸, 𝝍⟩= ⟨𝝓(z), 𝝓(x)⟩=
K𝛹(z, x).

Kernel Functions and Reproducing Kernel Hilbert Spaces
179
Computing distances. Given that a kernel corresponds to a dot product in a Hilbert
space , we can compute distances between mapped samples entirely in terms of
kernel evaluations:
‖𝝓(x) −𝝓(x′)‖=
√
K(x, x) + K(x′, x′) −K(x, x′).
(.)
Normalization. Exploiting the previous property, one can also normalize data in
feature spaces implicitly:
K′(x, x′) =
⟨𝝓(x)
‖𝝓(x)‖,
𝝓(x′)
‖𝝓(x′)‖
⟩

=
K(x, x′)
√
K(x, x)K(x′, x′)
,
(.)
where the result is a new kernel function K′. Note that the operation is useless
for some kernel functions. For example, the RBF kernel is given by K(xi, xj) =
exp(−‖xi −xj‖∕(𝜎)), so sample self-similarities are K(xi, xi) = and normal-
ization does not aﬀect the resulting kernel. Note that the RBF kernel function
implicitly maps examples to a hypersphere with unit norm, ‖𝝓(x)‖ = .
4.4
Constructing Kernel Functions
4.4.1
Standard Kernels
Any kernel method has its foundations in the deﬁnition of a kernel mapping function
𝝓that accurately measures the similarity among samples in some sense. But not all the
kernel similarity functions can be used; instead, valid kernels are only those fulﬁlling
Mercer’s theorem. Roughly speaking, kernel functions yield positive-deﬁnite similarity
matrices for a set of data measurements, and the most common ones in this setting are
the linear K(x, z) = ⟨x, z⟩, the polynomial K(x, z) = (⟨x, z⟩+ )d, d ∈ℤ+, and the RBF
K(x, z) = exp(−‖x −z‖∕𝜎), 𝜎∈ℝ+. Note that, by Taylor series expansion, the RBF
kernel is a polynomial kernel with inﬁnite degree. Thus, the corresponding Hilbert space
is inﬁnite dimensional, which corresponds to a mapping into the space of functions ∞.
Table .summarizes the most useful kernels and their main characteristics.
The relation between similarity and distance has produced a high number of kernel
functions that rely on standard distance measures, such as Mahalanobis kernels (Hof-
mann et al., ). Most of the methods rely on the observation that the Gaussian kernel
is of the form K(x, x′) = exp(−a d(x, x′)), where d ∶× →ℝ+ represents a distance
function and a > . Unfortunately, this instantiation in general is not true (Cortes et al.,
). If K is positive deﬁnite then −K is negative deﬁnite, but negative deﬁnite, but
the converse is not true in general. For example K(x, x′) = (x −x′)b is negative deﬁnite
for b ∈[, ). The RBF kernel is also of practical convenience (stability and only one
parameter to be tuned), and it is the preferred kernel function in standard applications.
Nevertheless, speciﬁc applications need particular kernel functions. Reviewing all the
possible kernels is beyond the scope of this chapter. For a more general overview,
including examples for other data structures such as graphs, trees, strings, and others,
we refer the reader to Hofmann et al. (), Schölkopf and Smola (), Bakır et al.
(), and Shawe-Taylor and Cristianini ().

180
Digital Signal Processing with Kernel Methods
Table 4.1 Main kernel functions used in the literature.
Kernel function
Expression
Linear kernel
K(x, y) = xTy + c
Polynomial kernel
K(x, y) = (𝛼xTy + c)d
RBF kernel
K(x, y) = exp
(
−‖x −y‖
𝜎
)
Exponential kernel
K(x, y) = exp
(
−‖x −y‖
𝜎
)
Laplacian kernel
K(x, y) = exp
(
−‖x −y‖
𝜎
)
Analysis of variance kernel
K(x, y) = ∑d
k=exp[−𝜎(x(k) −y(k))]d
Hyperbolic tangent (sigmoid) kernel
K(x, y) = tanh(𝛼xTy + c)
Rational quadratic kernel
K(x, y) = −
‖x −y‖
‖x −y‖+ c
Multiquadric kernel
K(x, y) =
√
‖x −y‖+ c
Inverse multiquadric kernel
K(x, y) =

√
‖x −y‖+ c
Power kernel
K(x, y) = −‖x −y‖d
Log kernel
K(x, y) = −log(‖x −y‖d + )
Cauchy kernel
K(x, y) =

+ (‖x −y‖∕𝜎)
Chi-square kernel
K(x, y) = −∑d
k=
(x(k) −y(k))

(x(k) + y(k))
Histogram (or min) intersection kernel
K(x, y) = ∑d
k=min(x(k), y(k))
Generalized histogram intersection kernel
K(x, y) = ∑m
k=min(|x(k)|𝛼, |y(k)|𝛽)
Generalized T-Student kernel
K(x, y) =

+ ‖x −y‖d
4.4.2
Properties of Kernels
Taking advantage of some algebra and functional analysis properties (Golub and
Van Loan ; Reed and Simon ), very useful properties of kernels can be derived.
Let Kand Kbe two positive-deﬁnite kernels on × , be a symmetric positive
semideﬁnite matrix, d(⋅, ⋅) be a metric fulﬁlling distance properties, and 𝜇> . Then,
the following kernels (Schölkopf and Smola, ) are valid:
K(x, x′) = K(x, x′) + K(x, x′)
(.)
K(x, x′) = 𝜇K(x, x′)
(.)
K(x, x′) = K(x, x′) × K(x, x′)
(.)
K(x, x′) = xTx′
(.)
K(x, x′) = K(f (x), f (x′))
(.)
These basic properties make it easy to construct reﬁned similarity measures better
ﬁtted to the data characteristics. One can sum dedicated kernels to diﬀerent portions

Kernel Functions and Reproducing Kernel Hilbert Spaces
181
of the feature space, to diﬀerent data representations, or even to diﬀerent temporal
or spatial scales through Equation .. A scaling factor for each kernel can also be
used (see Equation .). Recent advances for kernel development also involve the
following:
Convex combinations. By exploiting Equations .and ., one can build new
kernels by linear combinations of kernels:
K(x, x′) =
M
∑
m=
dmKm(x, x′),
(.)
where each kernel Km could, for instance, work with particular feature subsets
with eventually a diﬀerent kernel function. This ﬁeld of research is known as MKL,
and many algorithms have been proposed to optimize jointly the weights and the
kernel parameters (Rakotomamonjy et al., ). Note that this composite kernel
oﬀers some insight into the problem as well, since relevant features receive higher
values of dm, and the corresponding kernel parameters 𝜃m yield information about
similarity scales.
Deforming kernels. The ﬁeld of semi-supervised kernel learning deals with techniques
to modify the values of the training kernel, including the information from the
whole data distribution. In this setting, the kernel K is either deformed with a
graph distance matrix built with both labeled and unlabeled samples, or using
kernels built from clustering solutions (Belkin et al. ; Sindhwani et al. ).
Generative kernels. Exploiting Equation ., one can construct kernels from prob-
ability distributions by deﬁning K(x, x′) = K(p, p′), where p, p′ are deﬁned on
the space (Jaakkola and Haussler, ). This family of kernels is known as
probability product kernels between distributions and deﬁned as
K(p, p′) = ⟨p, p′⟩= ∫
p(x)p′(x) dx.
(.)
Joint input–output mappings. Kernels are typically built on a set of input samples.
During recent years, the framework of structured output learning has dealt with
the deﬁnition of joint input–output kernels, K((x, y), (x′, y′)) (Bakır et al. ;
Weston et al. ).
4.4.3
Engineering Signal Processing Kernels
The properties of kernel methods presented before have been widely used to develop
new kernel functions suitable for tackling the peculiarities of the signals under analysis
in a given DSP application. Let us review now some of the most relevant families of
engineered kernels of special interest in signal processing.

182
Digital Signal Processing with Kernel Methods
Translation- or Shift-Invariant Kernels
A particular class of kernels is translation-invariant kernels, also known as shift-
invariant kernels, which fulﬁll
K(u, v) = K(u −v).
(.)
A necessary and suﬃcient condition for a translation-invariant kernel to be Mercer’s
kernel (Zhang et al., ) is that its FT must be real and nonnegative; that is:

π ∫
+∞
v=−∞
K(v) e−𝕚π⟨f ,v⟩dv ≥
∀f ∈ℝd
(.)
Bochner’s theorem (Reed and Simon, ) states that a continuous shift-invariant
kernel K(x, x′) = K(x −x′) on ℝd is positive deﬁnite if and only if the FT of K is
nonnegative. If a shift-invariant kernel K is properly scaled, its FT p(𝝎) is a proper
probability distribution.
As we will see in Chapter , this property has been recently used to approximate
kernel functions and matrices with linear projections on a number of D randomly
generated features as follows:
K(x, x′) = ∫ℝd p(𝝎) e−𝕚𝝎T(x−x′) d𝝎≈
∑D
i=

D e−𝕚𝝎T
i x e𝕚𝝎T
i x′,
(.)
where p(𝝎) is set to be the inverse FT of K, and 𝝎i ∈ℝd is randomly sampled from a
data-independent distribution p(𝝎) (Rahimi and Recht, ). In this case, we deﬁne
a D-dimensional feature map z(x) ∶ℝdTℝD, which can be explicitly constructed
as z(x) ∶= [e(𝕚𝝎T
x), … , exp (𝕚𝝎T
Dx)]T. In matrix notation, given N data points, the
kernel matrix 𝐊∈ℝN×N can be approximated with the explicitly mapped data, Z =
[z⋯zn]T ∈ℝN×D, and will be denoted as ̂K ≈ZZT. This property can be used to
approximate any shift-invariant kernel. For instance, the familiar squared exponential
(SE) Gaussian kernel K(x, x′) = exp(−‖x −x′‖∕(𝜎)) can be approximated by using
𝝎i ∼(, 𝜎−I), ≤i ≤D.
Autocorrelation Kernels
A kernel related to the shift-invariant kernels family is the autocorrelation-induced
kernel. Notationally, let {hn} be an (N + )-samples limited-duration discrete-time real
signal (i.e., hn = , ∀n ∉(, N)), and let Rh
n = hn ∗h−n be its autocorrelation function.
Then, the following shift-invariant kernel can be built:
Kh(n, m) = Rh
n(n −m),
(.)
which is called an autocorrelation-induced kernel, or simply an autocorrelation kernel.
As Rh
n is an even signal, its spectrum is real and nonnegative, as expressed in Equa-
tion .; hence, an autocorrelation kernel is always a Mercer kernel.
The previous two types of kernels are clear examples of the importance of including
classic concepts of DSP in kernel-based algorithms. We will come back to this kernel in
chapters in Part II.

Kernel Functions and Reproducing Kernel Hilbert Spaces
183
Convolution Kernels
Convolution kernels, sometimes called part kernels or bag of words kernels, constitute
an interesting development. The underlying idea for them is that the signal components
(objects) are structured in any sense; thus, rather than measuring similarities between
complete objects, one aggregates similarities of individual components (or feature
subsets). A simple deﬁnition of a convolution kernel between x and x′ is
K(x, x′) =
∑
̄xi
∑
̄x′
i
P
∏
p=
Kp(xp, x′
p),
(.)
where ̄x and ̄x′ are the sets of parts (p = , … , P) of examples x and x′ respectively, and
Kp(xp, x′
p) is a kernel between the pth part of x and x′.
It is easy to show that the RBF kernel is a convolution kernel by simply letting each
of the P dimensions of x be a part, and using Gaussian kernels Kp(xp, x′
p) = exp(−‖xp −
x′
p‖∕(𝜎)). However, note also that the linear kernel K(x, x′) = ∑P
p=xpx′
p is not a
convolution kernel, since we would need to sum products of more than one term.
Spatial Kernels
The latter convolution kernels have been extensively used in many ﬁelds of signal
processing. In particular, the ﬁelds of image processing and computer vision have con-
tributed with many kernel functions, especially designed to deal with the peculiarities
of image features. Let us review two interesting spatial kernels to deal with structured
domains, such as images.
A spatial kernel for image processing is inspired in the well-known concepts of locality
and receptive ﬁelds of NNs. A kernel value is here computed by using not all features of
an image, but only those which fall into a particular region inside a window (Schölkopf,
). Extensions of that kernel to deal with hierarchical organizations lead to the
related spatial pyramid match kernel (Lazebnik et al., ) and the pyramid match
kernel (Grauman and Darrell, ). A standard approach in these methods is to divide
the image into a grid of × , × , … equally spaced windows, resembling a pyramid,
whose depth is referred as level. Then, for each level l, a histogram hl is computed by
concatenating the histograms of all subwindows within the level. Two images x, x′ are
compared by combining the similarity of the individual levels:
K(x, x′) =
L−
∑
l=
dlKl(hl, h′
l),
(.)
where dl ∈ℝ+ is an extra weighting parameter for each level, which must be optimized.
As can be noted, the resulting kernel is just the result of applying the property of sum
of kernels.
A second interesting kernel engineering was presented by Laparra et al. () to
deal with image denoising in the wavelet domain. Here, an SVR algorithm used a
combination of kernels adapted to diﬀerent scales, orientations, and frequencies in the
wavelet domain. The speciﬁc signal relations were encoded in an anisotropic kernel
obtained from mutual information measures computed on a representative image

184
Digital Signal Processing with Kernel Methods
database. In particular, a set of Laplacian kernels was used to consider the intraband
oriented relations within each wavelet subband:
K𝛼(pi, pj) = exp(−((pi −pj)TGT
𝛼𝜮−G𝛼(pi −pj))∕),
(.)
where 𝜮= diag(𝜎, 𝜎), 𝜎, and 𝜎are the widths of the kernels, pi ∈ℝdenotes the
spatial position of wavelet coeﬃcient yi within a subband, and G𝛼is the D rotation
matrix with rotation angle 𝛼, corresponding to the orientation of each subband.
Time–Frequency and Wavelet Kernels
Time–frequency signal decompositions in general, and wavelet analysis in particular,
are at the core of signal and image processing. These signal representations have been
widely studied and employed in practice. Inspired by wavelet theory, particular wavelet
kernels have been proposed (Zhang et al., ), which can be roughly approximated as
K(x, x′) =
N
∏
i=
h
(xi −c
a
)
h
(x′
i −c
a
)
,
(.)
where a and c represent the wavelet dilation and translation coeﬃcients respectively. A
translation-invariant version of this kernel can be given by
K(x, x′) =
N
∏
i=
h
(xi −x′
i
a
)
,
(.)
where in both kernels the function h(x) denotes a mother wavelet function, which is
typically chosen to be h(x) = cos(.x) exp(−x∕), as it yields to a valid admissible
kernel function (Zhang et al., ). Note that, in this case, the wavelet kernel is the
result of applying the properties of direct sum and direct product of kernels.
4.5
Complex Reproducing Kernel in Hilbert Spaces
The complex representation of a magnitude is of important interest in signal processing,
particularly in communications, since it provides a natural and compact expression
that makes signal manipulation easier. The justiﬁcation for the use of complex num-
bers in communications arises from the fact that any band-pass real signal centered
around a given frequency admits a representation in terms of in-phase and quadrature
components. Indeed, assume a real-valued signal x(t) whose spectrum lies between two
limits 𝜔min and 𝜔max. This function can the be expressed as
x(t) = AI(t) cos 𝜔t −AQ(t) sin 𝜔t,
(.)
with AI(t) and AQ(t) being the in-phase and quadrature components. This expression
can be rewritten as
x(t) = Re{(AI(t) + 𝕚AQ(t)) e𝜔t} = Re{A(t) e𝜔t}
(.)

Kernel Functions and Reproducing Kernel Hilbert Spaces
185
for some arbitrary frequency 𝜔called central frequency or carrier, A(t) being any
arbitrary function called a complex envelope of x(t). Since in-phase and quadrature
components are orthogonal with respect to the dot product of Lfunctions, it is straight-
forward that A(t) can be modulated as in Equation .without loss of information.
The central frequency is usually known and removed during the signal processing, thus
obtaining the complex envelope. This signal can either be processed as a pair of real-
valued signals or as a complex signal, thus obtaining a more compact notation.
Though the concept of a complex-valued Mercer kernel is classic (Aronszajn, ),
it was proposed by Martínez-Ramón et al. (); Martínez-Ramón et al. () for
its use in antenna array processing, and with a more formal treatment and rigorous
justiﬁcation by Bouboulis and coworkers (Bouboulis and Theodoridis, , ;
Bouboulis et al., ) and Ogunfunmi and Paul () for its use with the kernel LMS
(KLMS) (Liu et al., , ) with the objective to extend this algorithm to a complex
domain, as given in
𝝓(x) = 𝝓(x) + 𝕚𝝓(x) = K((xℝ, x𝕀), ⋅) + 𝕚K((xℝ, x𝕀), ⋅),
(.)
which is a transformation of the data x = xℝ+ 𝕚x𝕀∈ℂd into a complex RKHS. This is
known as the complexiﬁcation trick, where the kernel is deﬁned over real numbers.
Since the complex LMS algorithm involves the use of a complex gradient, the
Wirtinger calculus must be introduced in the notation. The reason is that the cost
function of the algorithm is real valued, and it is deﬁned over a complex domain.
Hence, it is non-holomorphic and complex derivatives cannot be used. A convenient
way to compute such derivatives is to use Wirtinger derivatives. Assume a variable
x = xℝ+ 𝕚x𝕀∈ℂand a non-holomorﬁc function f (x) = fℝ(x) + f𝕀(x), then its Wirtinger
derivatives with respect to x and x∗are
𝜕f
𝜕x = 

( 𝜕fℝ
𝜕xℝ
+ 𝜕f𝕀
𝜕x𝕀
)
+ 𝕚

( 𝜕f𝕀
𝜕xℝ
−𝜕fℝ
𝜕x𝕀
)
𝜕f
𝜕x∗= 

( 𝜕fℝ
𝜕xℝ
−𝜕f𝕀
𝜕x𝕀
)
+ 𝕚

( 𝜕f𝕀
𝜕xℝ
+ 𝜕fℝ
𝜕x𝕀
)
.
(.)
This concept is restricted to complex-valued functions deﬁned in ℂ. Authors gen-
eralize this concept to functions deﬁned in n RKHS through the deﬁnition of Fréchet
diﬀerentiability. The kernel function in this RKHS is deﬁned, from Equation ., as
̂K(x, x′) = 𝝓H(x)𝝓(x′)
= (𝜙(x) −𝕚𝜙(x))(𝜙T(x) + 𝕚𝜙t(x)) = K(x, x′).
(.)
The representer theorem (see Theorem ..) can be rewritten here as follows:
f ∗(⋅) =
N
∑
i=
𝛼iK(⋅, xi) + 𝕚𝛽iK(⋅, xi),
(.)
where 𝛼i, 𝛽i ∈ℝ. Note, however, that Theorem ..is already deﬁned over complex
numbers. Bouboulis et al. () use pure complex kernels; that is, kernels deﬁned over
ℂ. In this case, the representer theorem can be simply written as

186
Digital Signal Processing with Kernel Methods
f ∗(⋅) =
N
∑
i=
(𝛼i + 𝕚𝛽i
) K∗(⋅, xi),
(.)
which resembles the standard expansion when working with real data except for the
complex nature of the weights and the conjugate operation on the kernel.
4.6
Support Vector Machine Elements for Regression and
Estimation
This section introduces the instantiation of a kernel method for regression and function
approximation; namely, the SVR (Schölkopf and Smola ; Smola and Schölkopf
; Vapnik ). This method will accompany us in the following chapters, and it
conveys all the key elements to work with a variety of estimation problems, including
convexity of the optimization problem, regularized solution, sparsity, ﬂexibility for
nonlinear modeling, and adaptation to diﬀerent sources of noise. Let us review two
important deﬁnitions; namely, the SVR data model and the loss function used in the
minimization problem. We will come back to these deﬁnitions later in order to deﬁne
alternative signal models and cost functions, as well as to study model characteristics.
Departing from the primal problem, we derive the SVR equations, and then we sum-
marize the main properties of the SVR signal model.
4.6.1
Support Vector Regression Signal Model and Cost Function
Deﬁnition ..(Nonlinear SVR signal model)
Let a labeled training i.i.d. data set
{(vi, yi), i = , … , N}, where vi ∈ℝd and yi ∈ℝ. The SVR signal model ﬁrst maps
the observed explanatory vectors to a higher dimensional kernel feature space using a
nonlinear mapping 𝝓∶ℝN →, and then it calculates a linear regression model inside
that feature space; that is:
̂yi = ⟨w, 𝝓(vi)⟩+ b,
(.)
where w is a weight vector in and b is the regression bias term. Model residuals are
given by ei = yi −̂yi.
In order to obtain the model coeﬃcients, the SVR minimizes a cost function of the
residuals, which is often regularized with the 𝓁norm of w. That is, we minimize with
regard w the following primal functional:

‖w‖+
N
∑
i=
(ei).
(.)
In the standard SVR formulation, the Vapnik 𝜀-insensitive cost is often used (Vapnik,
).

Kernel Functions and Reproducing Kernel Hilbert Spaces
187
Deﬁnition ..(Vapnik’s 𝜺-insensitive cost)
Given a set or residual errors ei in an
estimation problem, the 𝜀-insensitive cost is given by
𝜀(ei) = C max(|ei| −𝜀, ),
(.)
where C represents a trade-oﬀbetween regularization and losses. Those residuals lower
than 𝜀are not penalized, whereas those larger ones have linear cost.
Here, we want to highlight an important issue regarding the loss. The Vapnik
𝜀-insensitive cost function results in a suboptimal estimator in many applications when
combined with a regularization term (Schölkopf and Smola ; Smola and Schölkopf
; Vapnik ). This is because a linear cost is not the most suitable one to deal
with Gaussian noise, which will be a usual situation in a number of time-series analysis
applications. This fact has been previously taken into account in the formulation of LS-
SVM (Suykens et al., ), also known as kernel ridge regression (KRR) (Shawe-Taylor
and Cristianini, ), where a quadratic cost is used, but in this case, the property of
sparsity is lost.
The 𝜀-Huber cost was proposed by Rojo-Álvarez et al. (), combining both the
quadratic and the 𝜀-insensitive zones, and it was shown to be a more appropriate
residual cost not only for time-series problems, but also for function approximation
problems in general where the data can be fairly considered to be i.i.d. (Camps-Valls.
et al., ).
Deﬁnition ..(𝜺-Huber cost function)
The 𝜀-Huber cost is given by
𝜀H(ei) =
⎧
⎪
⎨
⎪⎩
,
|ei| ≤𝜀

𝛿(|ei| −𝜀),
𝜀≤|ei| ≤eC
C(|ei| −𝜀) −
𝛿C,
|ei| ≥eC
(.)
where eC = 𝜀+ 𝛿C; 𝜀is the insensitive parameter, and 𝛿and C control the trade-oﬀ
between the regularization and the losses.
The three diﬀerent regions in 𝜀-Huber cost can work with diﬀerent kinds of noise.
First, the 𝜀-insensitive zone neglects absolute residuals lower than 𝜀. Second, the
quadratic cost zone is appropriate for Gaussian noise. Third, the linear cost zone is
eﬃcient for limiting the impact of possibly present outliers on the model coeﬃcients
estimation. Note that Equation .represents the Vapnik 𝜀-insensitive cost function
when 𝛿is small enough, the MMSE criterion for 𝛿C →∞and 𝜀= , and Huber cost
function when 𝜀= (see Figure .).
4.6.2
Minimizing Functional
In order to adjust the model parameters, an optimality criterion must be chosen.
Typically in machine learning, one selects a risk function based on the pdf of data,
which is unfortunately unknown. Hence, an induction principle is needed to best ﬁt
the real pdf based on the available data. A common choice is to minimize the so-called

188
Digital Signal Processing with Kernel Methods
Input space
xi
xj
Φ
ξj
ξj
ξj
ξ*i
ξ*i
ξ*i
L(ei)
L(ei)
Kernel space
ϕ(xi)
ϕ(xj)
0
+ε
+ε
–ε
–ε
+ε
–ε
e
e
ec
–ec
δC
(b)
(a)
Figure 4.4 SVR signal model and cost functions. Samples in the input space are mapped onto an
RKHS, and a linear regression is performed therein. All samples outside a fixed tube of size 𝜀are
penalized, and they are the support vectors (double circles). Penalization is given by (a) the Vapnik
𝜀-insensitive or (b) the 𝜀-Huber cost functions.
empirical risk; that is, the error in the training data set. However, to alleviate the problem
of overﬁtting and control model complexity, regularization is usually adopted (Tikhonov
and Arsenin, ), which is carried out in practice by minimizing the norm of the
model parameters. This is intuitively equivalent to ﬁnd the estimator which uses the
minimum possible energy of the data to estimate the output. The resulting functional
should take into account both this complexity term and an empirical error measurement
term. The latter is deﬁned according to an a-priori determined cost function of the
committed errors.
Hence, the problem of estimating the coeﬃcients can be stated as the minimization
of the following functional:
F(w, ei) = 
‖w‖+
N
∑
i=
𝜀H(ei).
(.)
Introducing the previous loss in Equation .into Equation ., we obtain the
following functional:

‖w‖+ 
𝛿
∑
i∈I
(𝜉
i + 𝜉∗
i ) + C
∑
i∈I
(𝜉i + 𝜉∗
i ) −
∑
i∈I
𝛿C

(.)
to be minimized with respect to w and {𝜉(∗)
i }, and constrained to
yi −wT𝝓(vi) −b ≤𝜀+ 𝜉i
(.)
−yi + wT𝝓(vi) + b ≤𝜀+ 𝜉∗
i
(.)
𝜉i, 𝜉∗
i ≥
(.)
for i = , ⋯, N. For notation simplicity, {𝜉(∗)
i } will denote both {𝜉i} and {𝜉∗
i } hereafter.
Here, {𝜉(∗)
i } are slack variables or losses, which are introduced to handle the residuals
according to the robust cost function. Iand Iare the sets of samples for which losses

Kernel Functions and Reproducing Kernel Hilbert Spaces
189
are required to have a quadratic or a linear cost respectively, and these sets are not
necessarily static during the optimization procedure.
The optimization problem involves two operations: a minimization and a set of con-
straints that cannot be violated. This a standard problem typically solved using Lagrange
multipliers.Essentially, one has to include linear constraints in Equations .–.
into Equation ., which gives us the dual form of the problem. Then, the primal–dual
functional (sometimes referred to as the Lagrange functional) is given by
LPD = 
‖w‖+ 
𝛿
∑
i∈I
(𝜉
i + 𝜉∗
i ) + C
∑
i∈I
(𝜉i + 𝜉∗
i ) −
∑
i∈I
𝛿C

−
∑
i
(𝛽i𝜉i + 𝛽∗
i 𝜉∗
i ) +
∑
i
(𝛼i −𝛼∗
i ) (yi −wT𝝓(vi) −b −𝜀−𝜉i
)
(.)
constrained to 𝛼(∗)
i , 𝛽(∗)
i , 𝜉(∗)
i
≥. By making zero the gradient of LPD with respect to the
primal variables (Rojo-Álvarez et al., ), the following conditions are obtained:
𝛼(∗)
i
= 
𝛿𝜉(∗)
i
(i ∈I)
(.)
𝛼(∗)
i
=C −𝛽(∗)
i
(i ∈I),
(.)
as well as the following expression relating the primal and dual model weights:
w =
∑
n
(𝛼i −𝛼∗
i )𝝓(vi).
(.)
Constraints in Equations ., ., and .are then included into the Lagrange
functional in Equation .in order to remove the primal variables. Note that the
dual problem can be obtained and expressed in matrix form, and it corresponds to the
maximization of
−
(𝜶−𝜶∗)T[K + 𝛿I](𝜶−𝜶∗) + (𝜶−𝜶∗)Ty −𝜀T(𝜶+ 𝜶∗)
(.)
constrained to
C ≥𝛼(∗)
i
≥,
(.)
Giuseppe Lodovico (Luigi) Lagrangia (–) was the father of a famous methodology for
optimization of functions of several variables subject to equality and inequality constraints. The method of
Lagrange multipliers essentially solves maxx,y f (x, y) s.t. g(x, y) = c. He proposed to introduce a new
variable (called the “Lagrange multiplier” 𝛼) and optimize the new function:
maxx,y,𝜆{Λ(x, y, 𝛼)} s.t. f (x, y) −𝛼(g(x, y) −c), which is equivalent to solving the dual problem
maxx,y,𝜆{f (x, y) −𝛼(g(x, y) −c)} s.t. 𝛼≥. Interestingly enough, the method is applicable to any number of
variables and constraints, min{f(x)} s.t. fi(x) ≤and hj(x) = . Then, the dual problem reduces to
minimizing Λ(x, 𝛼, 𝜇) = f(x) −∑
i 𝛼ifi(x) −∑
j 𝜇jhi(x). The proposed procedure to solve these types of
problems is very simple: ﬁrst, one has to derive this primal–dual problem and equate to zero,
∇𝜆Λ(x, y, 𝛼) = , then the constraints obtained are stationary points of the solution, which are eventually
included in the problem again. These give rise to a linear or quadratic problem, for which there are very
eﬃcient solvers nowadays.

190
Digital Signal Processing with Kernel Methods
where 𝜶(∗) = [𝛼(∗)
, ⋯, 𝛼(∗)
N ]T. Then, after obtaining Lagrange multipliers 𝜶(∗), the time-
series model for a new sample at time instant m can be readily expressed as
yj = f (vj) =
N
∑
i=
(𝛼i −𝛼∗
i )𝝓(vi)T𝝓(vj) =
N
∑
i=
(𝛼i −𝛼∗
i )K(vi, vj),
(.)
where the dot product 𝝓(vi)T𝝓(vj) has been ﬁnally replaced by the kernel function
working solely on input samples vi and vj; that is: K(vi, vj) = 𝝓(vi)T𝝓(vj), which is a
function of weights in the input space associated with nonzero Lagrange multipliers.
More details on the derivation of these equations can be found in the literature for
SVR (Smola et al., ) and for linear SVM–ARMA (Rojo-Álvarez et al., ).
By including the 𝜀-Huber residual cost into Equation ., the SVR coeﬃcients can be
estimated by solving a quadratic programming (QP) problem (Rojo-Álvarez et al. ;
Smola and Schölkopf ). Several relevant properties can be highlighted, which can
be shown by stating the Lagrange functional, setting the Karush–Khun–Tucker (KKT)
conditions, and then obtaining the dual functional. These properties are summarized
next.
Property (SVR sparse solution and support vectors)
The weight vector in can be
expanded in a linear combination of the transformed input data:
w =
N
∑
i=
𝜂i𝝓(vi),
(.)
where 𝜂i = (𝛼i −𝛼∗
i ) are the model weights, and 𝛼(∗)
i
are the Lagrange multipliers
corresponding to the positive and negative residuals in the nth observation. Observa-
tions with nonzero associated coeﬃcients are called support vectors, and the solution
is expressed as a function of them solely.
This property describes the sparse nature of the SVM solution for estimation
problems.
Property (Robust expansion coeﬃcients)
A nonlinear relationship between the
residuals and the model coeﬃcients for the 𝜀-Huber cost is given by
𝜂i = 𝜕L𝜀H(e)
𝜕e
|||e=ei =
⎧
⎪
⎨
⎪⎩
,
|ei| ≤𝜀

𝛿⋅sgn(ei)(|ei| −𝜀),
𝜀< |ei| ≤𝜀+ 𝛾C
C ⋅sgn(ei),
|ei| > 𝜀+ 𝛾C.
(.)
Therefore, the impact of a large residual ei on the coeﬃcients is limited by the value of
C in the cost function, which yields estimates of the model coeﬃcients that are robust
in the presence of outliers.
The kernel trick in SVM consists of stating a data-processing algorithm in terms of dot
products in the RKHS, and then substituting those products by Mercer kernels. The ker-

Kernel Functions and Reproducing Kernel Hilbert Spaces
191
nel expression is actually used in any kernel machine, but neither the mapping function
𝝓(⋅) nor the RKHS need to be known explicitly. The Lagrangian of Equation .is used
to obtain the dual problem, which in turn yields the Lagrange multipliers used as model
coeﬃcients.
Property (Regularization in the dual)
The dual problem of Equation .for the
𝜀-Huber cost corresponds to the maximization of
−
(𝜶−𝜶∗)T(K + 𝛿I)(𝜶−𝜶∗) + (𝜶−𝜶∗)Ty −𝜀T(𝜶+ 𝜶∗)
(.)
constrained to ≤𝛼(∗)
i
≤C. Here, 𝜶(∗) = [𝛼(∗)
, ⋯, 𝛼(∗)
n ]T, y = [y, ⋯, yNr]T, K represents
the kernel matrix, given by K i,j = K(vi, vj) = ⟨𝝓(vi), 𝝓(vj)⟩, is an all-ones column
vector, and I is the identity matrix.
The use of the quadratic zone in the 𝜀-Huber cost function gives rise to a numerical
regularization. The eﬀect of 𝛿in the solution was analyzed by Rojo-Álvarez et al. ().
Property (Estimator as an expansion of kernels)
The estimator is given by a
linear regression in the RKHS, and it can be expressed only in terms of the Lagrange
multipliers and Mercer kernels as
̂y(v) = ⟨w, 𝝓(v)⟩+ b =
N
∑
i=
𝜂iK(vi, v) + b,
(.)
where only the support vectors (i.e., training examples whose corresponding Lagrange
multipliers are nonzero) contribute to the solution.
4.7
Tutorials and Application Examples
In this section, we ﬁrst present some simple examples of the concepts developed with
kernels so far. Then, we present a set of real examples with several data bases focusing
on SVR performance and the impact of the cost function.
4.7.1
Kernel Calculations and Kernel Matrices
In these ﬁrst examples we will see how to compute some of the kernel matrices in
Table .. We will assume we have a matrix of vector samples for training, X ∈ℝn×d,
where n is the number of vectors and d its dimensionality (or number of features), and
a matrix of vector samples for testing, X∗∈ℝm×d, where m is the number of samples
in the test set. The ﬁrst two kernels in the table can be straightforwardly computed in
Listing ..
% Assuming we have Xt (training) and Xv (test)
Kt = Xt * Xt'; % Linear kernel for training
Kv = Xv * Xt'; % Linear kernel for validation/test
% Polynomial kernel

192
Digital Signal Processing with Kernel Methods
Kt = (Xt * Xt' + c).^d; % c: bias, d: polynomial degree
Kv = (Xv * Xt' + c).^d;
Listing 4.1 Linear and polynomial kernels (kernels1.m).
Kernel matrices must be positive deﬁnite to be valid. In MATLAB, there are several
ways to test if a matrix is positive deﬁnite. In a positive-deﬁnite matrix all eigenvalues
are positive; therefore, we can use eig to obtain all eigenvalues and check if all of them
are positive (see Listing .).
% Eigenvectors of Kt
e = eig(Kt);
if any(e < 0),
error('Matrix is not P.D.')
end
Listing 4.2 Checking positive definiteness via eigenvalues (kernels2.m).
However, the preferred way to test for positive deﬁniteness is using chol, as in
Listing ..
% Test is matris is P.D. using chol
[R,p] = chol(Kt);
if p > 0,
error('Matrix is not P.D.')
end
Listing 4.3 Checking positive definiteness using chol (kernels3.m).
Note that sometimes this test can fail due to numerical precision. In those cases, the
eigenvalues may have a very small negative value, but diﬀerent from zero; therefore,
the test is not passed. If you have the theoretical guarantee that your kernel function is
positive deﬁnite, a way to work around numerical issues is to add a small value to the
diagonal; for instance, like Kt + 2*eps*eye(size(Kt)).
In many cases it is very useful to inspect the kernel matrix to visualize structures or
groups. Kernels measure the similarity between vectors in the feature space, and a visual
representation of the kernel should reveal some structure. In the following example, in
Listing ., we will generate three random D sets, compute a basic linear kernel, and
see its structure. Figure .shows the result of the example. It is clear that the kernel
reveals the relationship between diﬀerent groups.
np
= 200;
dist = 2.5;
m1
= [-dist ; dist];
m2
= [0 ; 0];
m3
= [2*dist ; 2*dist];
% Generate samples
X1 = randn(2,np) + repmat(m1,1,np);
X2 = randn(2,np) + repmat(m2,1,np);
X3 = randn(2,np) + repmat(m3,1,np);
% All samples matrix
X = [X1 X2 X3]';
% Normalization
Xn = zscore(X);
% Look at the generated examples

Kernel Functions and Reproducing Kernel Hilbert Spaces
193
figure(1)
plot(Xn(1:np,1), Xn(1:np,2), 'xb', ...
Xn((1:np)+1*np,1), Xn((1:np)+1*np,2), 'xr', ...
Xn((1:np)+2*np,1), Xn((1:np)+2*np,2), 'xk')
% Linear kernel
K = Xn * Xn';
% Inspecting the kernel
figure(2), imagesc(K), axis square off
Listing 4.4 Generating random sets and inspecting the linear kernel (kernels4.m).
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
–2.5
–2
–1.5
–1
−0.5
0
0.5
1
1.5
2
2.5
100
200
300
400
500
600
100
200
300
400
500
600
(a)
(b)
Figure 4.5 Example of kernel matrix: (a) the three sets; (b) generated linear kernel.

194
Digital Signal Processing with Kernel Methods
One the of most used kernels is the RBF or Gaussian kernel, also known as the squared
exponential kernel, due to its simplicity (it only has one free parameter to adjust), and
because it is a universal characteristic kernel that includes other kernels as particular
cases. By exploiting the decomposition of the Euclidean distance we can eﬃciently
compute this kernel, as seen in Listing ..
% For the train kernel we have
nt = size(Xt,2);
ntsq = sum(Xt.^2,1);
Dt = 2 * (ntsq * ones(nt,1) - Xt' * Xt);
Kt = exp(-Dt / (2*sigma^2)); % sigma is the width of the RBF
% The test kernel can be computed as
nv = size(Xv,2);
nvsq = sum(Xv.^2,1);
Dv = ntsq' * ones(1,nv) + ones(nt,1) * nvsq - 2 * Xt' * Xv;
Kv = exp(-Dv / (2*sigma^2));
Listing 4.5 Computation of the RBF kernel (kernels5.m).
The exponential and Laplacian kernels can be obtained as in the RBF kernel just by
taking the root of the Dt or Dv matrices in Listing .. Similarly, it is easy to obtain the
rational, multi-quadratic, power, log, Cauchy, and generalized T-Student kernels from
these matrices. On the other hand, the chi-square and histogram intersection kernels
are usually employed in computer vision, and they are particularly suitable for data in
the form of normalized histograms. In general, given two vectors x and z of dimension
d, we need to compute d kernels, one kernel per feature, and sum them all. For the chi-
square kernel (other kernels are obtained similarly) a naive MATLAB implementation
would be as given in Listing ..
% Chi-Square kernel
d = size(X1,2); n1 = size(X1,1); n2 = size(X2,1);
K = 0;
for i = 1:d
num = 2 * X1(:,i) * X2(:,i)';
den = X1(:,i) * ones(1,n2) + ones(n1,1) * X2(:,i)';
K = K + num./den;
end
Listing 4.6 Chi-square kernel (kernels6.m).
In the example in Listing ., as the reader may notice, we have not computed the
chi-square kernel as deﬁned in Table .. The reason, as pointed out by Vedaldi and
Zisserman (), is that this kernel is only conditionally positive deﬁnite. It can be
obtained as
K(x, z) =
d
∑
i=
(xi −zi)
(xi + zi) ,
which is the kernel we calculate with Listing ..
4.7.2
Basic Operations with Kernels
Let us start this subsection with a fundamental property in many kernel machines,
which is the data centering in Hilbert space. This will need the property of translation

Kernel Functions and Reproducing Kernel Hilbert Spaces
195
of vectors. The example in Listing .shows how to center data, although it can easily
modiﬁed to move the kernel to any other place in the Hilbert space.
% Assuming a pre-computed kernel matrix in Kt, centering is done as:
[Ni,Nj] = size(Kt);
Kt = Kt - ( mean(Kt,2)*ones(1,Nj) - ones(Ni,1)*mean(Kt,1) + mean(Kt(:)) );
% Sum columns and divide by the number of rows
S = sum(Kt) / Ni;
% Centered test kernel w.r.t. a train kernel
Kv = Kv - ( S' * ones(1,Nj) - ones(Ni,1) / Ni * sum(Kv) + sum(S) / Ni );
Listing 4.7 Centering kernels (kernels7.m).
Another important property of kernel methods is that one can estimate linear
projections onto subspaces of the Hilbert feature space explicitly. We are given a
data matrix X ∈ℝN×d ⊂which is mapped to an RKHS, 𝝓∈ℝN×d⊂. Now we
want to project the data onto a D-dimensional subspace of , where D < dgiven by
a projection matrix V ∈ℝN×N.
In order to map new input data X′
∈
ℝm×d into a subspace of dimension-
ality D in , one has to ﬁrst map the data to the Hilbert space 𝛷′, and then
express the projection matrix V as a linear combination of the n examples therein,
V = 𝛷T:
D(X′) = 𝛷V = 𝛷′𝛷T= K,
(.)
where , K ∈ℝN×N, K ij ∶= ⟨𝝓(x′
i), 𝝓(xj)⟩, and hence projecting in a ﬁnite dimension
D can be easily done by retaining (truncating) a number D of columns of .
This can be illustrated in Listing .. Let us for a moment assume that the eigenvectors
of the kernel matrix are the ones spanning the subspace we are interested in. The oper-
ation essentially reduces to an eigendecomposition of an N × N matrix, truncation of
the eigenvectors matrix, and linear projection. This property and the latter application
are widely used in kernel feature extraction methods, such as the KPCA, which will be
revised thoroughly in Chapter .
K
= kernelmatrix('rbf',X,X,sigma);
% compute kernel matrix n x n
K2 = kernelmatrix('rbf',X2,X,sigma); % compute kernel matrix n x m
n = size(K,2);
% n: number of samples used in the kernel matrix
D = 10;
% subspace dimensionality (D<n)
[A L] = eigs(K,n);
% extract the top D eigenvectors of the kernel matrix
P_X2 = K2*A;
% projection of X2 onto the subspace of size m x n
P_X2 = K2*A(:,1:D); % projection of X2 onto the subspace of size m x D
Listing 4.8 Projections with kernels (kernels8.m).
Very often we aim to estimate distances in the Hilbert space explicitly. Given two data
matrices 𝐗and 𝐘, one can compute the distances between the data points implicitly via
kernels. Essentially, one is interested in mapping the data to a Hilbert space which
yields 𝛷x and 𝛷y, and estimating the (squared) Euclidean distance therein:
d
xy ∶= ‖𝛷x −𝛷y‖
= K x + K y −K xy,
(.)

196
Digital Signal Processing with Kernel Methods
which can be done in MATLAB with the following code: D2_H = Kx+Ky-2*Kxy;.
It is worth noting that if we use an RBF kernel function, self-similarities become one,
K(xi, xi) = , and then all points are mapped to a hypersphere and hypersphere and
computing distances reduces to simply D2_H = 2*(1-Kxy).
Another useful operation when working with kernels is about computing distances
to the empirical center of mass. As an extension of the previous exercise, the reader
may consider computing the distance from the given data points to its (empirical)
center of mass in feature spaces. This would require computing the empirical mean in
Hilbert space, but we can do this implicitly with kernels, as in Listing .. Related to this
example, one may think of normalizing the energy of the data points in Hilbert spaces.
This operation of normalization is also possible via kernel functions only. Given a kernel
matrix K, it is trivial to compute its normalized version as in Listing ..
n = size(K,1);
D = sum(K) / ell;
E = sum(D) / ell;
D2 = diag(K) - 2 * D' + E * ones(n,1);
Listing 4.9 Computing distances to the empirical center of mass (kernels9.m).
D = diag(1./sqrt(diag(K)));
Kn = D * K * D;
Listing 4.10 Kernel normalization (kernels10.m).
Note that in all these operations, only linear algebra operations, such as matrix multi-
plications or SVDs, are typically involved.
A quite useful operation with kernels is to measure how well aligned two ker-
nel matrices are, either to discard redundant kernel parameters or to optimize the
(in)dependence among them. Imagine that you are given a supervised problem like the
“two moons” shown in Figure .in which you need to classify samples belonging to
one of the two classes colored in blue or red. Building a proper kernel matrix requires
1
Alignment
Alignment
10-fold SVM
0.8
0.6
0.4
0.2
0
σ
10–4
10–2
100
102
104
Figure 4.6 Illustration of the operation of alignment for the selection of hyperparameters.

Kernel Functions and Reproducing Kernel Hilbert Spaces
197
typically tuning parameters, which is commonly done via cross-validation. A ﬁrst guess
of the proper parameter can be estimated via kernel alignment in a very cheap way
without training any classiﬁer, just by estimating the kernel hyperparameter that aligns
best the mapped samples with the kernel matrix K x(𝜽) and the labels y (through the
so-called ideal kernel K ideal = yyT) in feature space. The problem then boils down to
estimating the alignment measure
𝜽∗= arg min
𝜽‖K −yyT‖
F,
(.)
where ‖⋅‖F is the Frobenius norm. The (normalized) alignment measure for a particular
hyperparameter can be easily computed as in Listing .. Figure .shows the results
obtained when using an RBF kernel parametrized with the length-scale 𝜎parameter,
comparing the results with an expensive training of an SVM by grid search of the
hyperparameters.
temp = F_norm(K,Y);
A = temp/sqrt(F_norm(K,K)*F_norm(Y,Y));
function F = F_norm(A,B)
F = trace(A'*B);
Listing 4.11 Normalized alignment measure (kernels11.m).
4.7.3
Constructing Kernels
In this section we illustrate two examples of kernel construction, as a follow-up of
Section ... In particular, let us discuss the standard convex kernel combinations.
Commonly used kernels families include the SE, periodic (Per), linear (Lin), and rational
quadratic (RQ); see Table .. We illustrate these basic kernels and some functions
drawings in Figure .. These basic kernels can be actually combined by following
simple operations, such as summation, multiplication, or convolution. This way, one
may build sophisticated kernels from simpler ones. The code in Listing .illustrates
the construction of such kernels and several convex combinations.
%% Data
X_o = [-1.5 -1 -0.75 -0.4 -0.3 0]';
Y_o = [-1.6 -1.3 -0.5 0 0.3 0.6]';
%% Define a SE kernel
nu = 1.15; % scale of the kernel
s
= 0.30; % lengthscale sigma parameter of the kernel
kf = @(x,x2) nu^2 * exp( (x-x2).^2
/(-1*s^2) );
sn = 0.01; % known noise on observed data
ef = @(x,x2) sn^2 * (x==x2); % iid noise
% Let's sum the signal and noise kernel functions:
k = @(x,x2) kf(x,x2) + ef(x,x2);
% Let's plot the kernel function of domain x
np = 100; x = linspace(-1,1,np)';
y = kf(zeros(length(x),1),x);
figure(1), plot(x,y,'k','LineWidth',8), axis([-1 1 0 1.5]), axis off
% Now let's draw two random functions out of the kernel
K = zeros(length(x));

198
Digital Signal Processing with Kernel Methods
for i = 1:length(x)
K(i,:) = kf(x(i) * ones(length(x),1), x);
end
% Due to numerical precision, we need to add a small factor to the ...
diagonal
% to ensure positive definiteness
sphi = chol(K + 1e-12 * eye(size(K)))';
rf = sphi * randn(length(x),2);
figure(2), plot(x,rf(:,1)+1,x,rf(:,2)-1,'LineWidth',8), axis off
%% Done!
% As a proposed exercise, try to do the same with the
% rational-quadratic kernel, the periodic kernel and combinations.
% We give you some hint material:
% 1) Rational-quadratic kernel,
c = 0.05;
krq = @(x1,x2) sigma_f^2 * (1 - (x1-x2).^2 ./ ((x1-x2).^2 + c)) + ...
ef(x1,x2);
% 2) Periodic kernel
p = 0.4; % periode
s = 1;
% SE kernel lengthscale
kper = @(x1,x2) sigma_f^2 * exp(-sin(pi*(x1-x2)/p).^2/l^2) + ef(x1,x2);
% 3) Linear kernel
offset = 0;
kl = @(x1,x2) sigma_f^2 * (x1 - offset) .* (x2 - offset) + 1e-5*ef(x1,x2);
Listing 4.12 Examples of kernel construction (kernels12.m).
In Figure .from Chapter , all the base kernels are D. Nevertheless, kernels over
multidimensional inputs can actually be constructed by adding and multiplying kernels
over individual dimensions; namely, (a) linear, (b) SEl (or RBF), (c) rational quadratic,
and (d) periodic. See Table .for the explicit functional form of each kernel. Some
simple kernel combinations are represented in the two last panels of Figure .. For
instance, a linear plus periodic covariances may capture structures that are periodic
with trend (e), while a linear plus SE covariances can accommodate structures with
increasing variation (f). By summing kernels, we can model the data as a superposition
of independent functions, possibly representing diﬀerent structures in the data. For
example, in multitemporal image analysis, one could, for instance, dedicate a kernel for
the time domain (perhaps trying to capture trends and seasonal eﬀects) and another
kernel function for the spatial domain (equivalently capturing spatial patterns and
autocorrelations).
In time-series models, sums of kernels can express superposition of diﬀerent pro-
cesses, possibly operating at diﬀerent scales. Very often, changes in geophysical vari-
ables through time occur at diﬀerent temporal resolutions (hours, days, etc.), and this
can be incorporated in the prior covariance with those simple operations. In multiple
dimensions, summing kernels gives additive structure over diﬀerent dimensions, similar
to generalized additive models (Hastie et al., ). Alternatively, multiplying kernels
allows us to account for interactions between diﬀerent input dimensions or diﬀerent
notions of similarity. In the following subsections, we also show how to design kernels
that incorporate particular time resolutions, trends, and periodicities.

Kernel Functions and Reproducing Kernel Hilbert Spaces
199
4.7.4
Complex Kernels
This example shows a nonlinear complex channel equalization with a linear ﬁlter and a
kernel ﬁlter. A QPSK data burst dn = dr,n + 𝕚di,n, where dr,n, di,n ∈{−, } is sent over a
channel with a linear dispersive section of impulse response hn = 𝛿n +(.−.𝕚)𝛿n−+
(.+ .𝕚)𝛿n−, and a nonlinear section at its output with the function x = f (u) =
(∕)u + u−∕, where u is the linear channel output. The nonlinear channel output has
a complex additive white Gaussian noise of standard deviation 𝜎n = .. The linear
transversal equalizer has the expression
y = wxH
n ,
(.)
where xn = (xn ⋯xn−L+), H is the complex transpose (Hermitian) operator, and L is the
ﬁlter length.
The ﬁlter weights are adjusted using a ridge regression procedure, where regulariza-
tion parameter 𝜆is adjusted to the channel noise (i.e., 𝜆= 𝜎
n), and thus the ﬁlter is
intended to minimize the cost function
L = 𝔼[‖dn −wxH
n ‖] + 𝜎
n‖w‖.
(.)
The optimization is performed simply by nulling the gradient of the cost function with
respect to the parameter vector, using the Wirtinger derivative. The expression of the
optimal ﬁlter is simply
w = dX(XHX + 𝜎
nI)−,
(.)
where X is the matrix containing all training vectors xn, and d is a row vector containing
the transmitted signals, assumed to be known by the receiver for training purposes.
The channel output data can be transformed using a nonlinear transformation into
an RKHS, and then the expression for the weight vector is
w = d𝛷(𝛷H𝛷+ 𝜎
nI)−,
(.)
where 𝛷is a matrix containing all row vectors 𝝓(xn), and 𝝓(⋅) is a mapping into a given
RKHS.
A kernelized version of this ﬁlter can be constructed using the complex version of the
representer theorem (Equation .), given by
w =
N
∑
i=
(𝛼i + 𝕚𝛽i)K(⋅, xi) =
N
∑
i=
(𝛼i + 𝕚𝛽i)𝝓(xi) = (𝜶+ 𝕚𝛽)𝛷,
(.)
where 𝜶+ 𝕚𝜷is a row vector containing all complex coeﬃcients 𝛼i + 𝕚𝛽i.

200
Digital Signal Processing with Kernel Methods
–1.5
–1
–2
–0.5
0
0.5
1
1.5
2
2.5
–1.5
–1
–2
–0.5
0
0.5
1
1.5
2
–2
–1.5
–1
–0.5
0
0.5
1
1.5
2
–1.5
–1
–0.5
0
0.5
1
1.5
Figure 4.7 Estimated symbols from a burst of 1000 training data. Upper panel: the linear equalizer
estimation; lower panel: the complex kernel equalizer estimation.
The combination of Equations .and .gives the result
𝜶+ 𝕚𝜷= d(𝛷𝛷H + 𝜎
nI)−.
(.)
The complex kernel estimator is then expressed as
yn =
N
∑
i=
(𝛼i + 𝕚𝛽i)K(xi, xn).
(.)
In our example, a Gaussian kernel is used. The upper panel of Figure .shows
the representation of the estimated complex symbols of a test burst after training a
linear equalizer with Equation ., and the lower panel the symbols estimated with

Kernel Functions and Reproducing Kernel Hilbert Spaces
201
the equalizer in Equation .using the optimization in Equation .. As can be
shown, the linear equalizer produces a poor estimation due to the nonlinear behavior
of the channel. Nevertheless, the complex kernel estimator can model and invert this
nonlinear behavior. Listing .has been used to obtain these results.
function kernels13
sn = 0.05;
% Noise std
L = 4;
% Filter length
p = sqrt(2)*2; % Kernel parameter
N = 1000;
h = [1 0.5-1j*0.5 .1+1j*.1]; % Linear channel impulse response
% Data
d = sign(randn(1,N)) + 1j*sign(randn(1,N));
d_test = sign(randn(1,N)) + 1j*sign(randn(1,N));
% Channel
x = channel(d,h,sn);
x_test = channel(d_test,h,sn);
% Data matrix
X = buffer(x,L,L-1)';
X_test = buffer(x_test,L,L-1)';
% Equalization with Linear MMSE
R = X' * X;
R = R + sn^2*eye(size(R));
w = d * X / R;
y = w * X_test';
figure(1), plotresults(y,d_test), title('Linear MMSE eq.')
% Equalization with Kernel MMSE
K = kernel_matrix(X,X,p);
alpha = d * pinv(K + sn^2*eye(size(K)));
K_test = kernel_matrix(X_test,X,p);
y = alpha * K_test;
figure(2), plotresults(y,d_test), title('Kernel MMSE eq.')
function x = channel(d,h,s)
% Channel: Linear section
xl = filter(h,1,d);
% Channel: Nonlinear section
xnl = xl.^(1/3) + xl / 3;
% Noise
x = xnl + s*(randn(size(xl)) + 1j*randn(size(xl)));
function K = kernel_matrix(X1,X2,p)
N1 = size(X1,1);
N2 = size(X2,1);
aux = kron(X1,ones(N2,1)) - kron(ones(1,N1),X2')';
D = buffer(sum(aux.*conj(aux),2),N2,0);
K = exp(-D/(2*p));
function plotresults(y,d)
plot(real(y(d == 1+1j)),
imag(y(d == 1+1j)),
'bo'), hold on
plot(real(y(d == -1-1j)), imag(y(d == -1-1j)), 'rs')

202
Digital Signal Processing with Kernel Methods
plot(real(y(d == 1-1j)),
imag(y(d == 1-1j)),
'm^')
plot(real(y(d == -1+1j)), imag(y(d == -1+1j)), 'kv'), hold off
axis equal, grid on
Listing 4.13 Complex kernel (kernels13.m).
4.7.5
Application Example for Support Vector Regression Elements
In this subsection, three types of loss-based SVRs are benchmarked: () the standard
𝜀-insensitive SVR (𝜀-SVR); () the squared-loss SVR (also known as kernel ridge
regression or LS-SVM); and () the 𝜀-Huber-SVR. We have chosen a real regression
example that involves diﬀerent noise sources for the sake of illustration purposes.
Results are obtained with NNs and with classical empirical models. These models are
compared in terms of accuracy and bias of the estimations, and also their robustness is
scrutinized when a low number of training samples are available in two diﬀerent data
sets related to the estimation of oceanic chlorophyll concentration from satellite data.
More details can be found in Camps-Valls et al. (c).
Data Description
We use here two diﬀerent datasets for illustration purposes. The ﬁrst dataset simulates
the data acquired by the medium-resolution imaging spectrometer (MERIS) on-board
at the Envisat satellite (MERIS dataset), and in particular the spectral behavior of chloro-
phyll concentration C in the subsurface waters (Cipollini et al., ). The data were
generated according to a ﬁxed and noise-free model, and the total number of samples
(pairs of in-situ concentrations and acquired radiances) available for our experiments
was . These samples were randomly divided into three sets: a training set (
samples, model building), a validation set (samples, free parameters tuning), and a
test set (samples).
The second dataset (SeaBAM dataset, (O’Reilly and Maritorena, ; O’Reilly et al.,
)) consists of a compilation of in-situ measurements of chlorophyll concen-
tration around the USA and Europe, all of them related to ﬁve diﬀerent multispectral
remote-sensing reﬂectance corresponding to the SeaWiFS wavelengths. The available
data were randomly split into three sets: samples for training, samples for
validation, and the remaining samples for testing the performance. In both cases, we
transformed the concentration data logarithmically, y = log(C), according to Cipollini
et al. (). Hereafter, units of all accuracy and bias measurements are referred to y
[log(mg∕m)] instead of C [mg∕m].
Model Accuracy and Comparison
Table .presents results in the test set for all the SVR models and datasets. Results
are compared with a feedforward NN trained with backpropagation (NN-BP) for both
datasets. In the case of the SeaBAM dataset, we also include results with the models
Morel-, Morel-, and CalCOFI -band (cubic and linear), as they performed best
among a set of empirical estimation models of the chlorophyll-a concentration
in O’Reilly et al. (), and results from the 𝜀-SVR obtained in Zhan et al. ().
Several prediction error measures are considered, including mean error (ME), root
mean-square error (RMSE), and mean absolute error (ABSE), as well as the correlation

Kernel Functions and Reproducing Kernel Hilbert Spaces
203
Table 4.2 Results in the test set in the two datasets.
ME
RMSE
ABSE
r
MERIS dataset
NN-BP, hidden nodes
−.× −
.
.
.
𝜀-SVR
−.× −
.
.
.
L-loss SVR
−.× −
.
.
.
Proposed 𝜀-Huber-SVR
−.× −
.
.
.
SeaBAM dataset
NN-BP, hidden nodes
−.
.
.
.
𝜀-SVR in Zhan et al. ()
—
.
—
.
𝜀-SVR
−.
.
.
.
L-loss SVR
−.
.
.
.
𝜀-Huber-SVR
−.
.
.
.
coeﬃcient between the desired output and the output oﬀered by the models as a
measure of ﬁt. Source code for running the experiments can be found in this book’s
repository.
Table .shows that the kernel technique is more accurate (RMSE, ABSE) and
unbiased (ME). For the MERIS data, signiﬁcant numerical (see Table .) and statistical
diﬀerences between 𝜀-Huber-SVR and the rest of the models were observed for both
bias (F = ., p < .) and accuracy (F = ., p < .). For the SeaBAM
dataset, the kernel method improved the performance, but no statistical diﬀerences
were observed in bias (F = ., p = .) or accuracy (F = ., p = .). These
results are similar to those previously reported in Zhan et al. (), but we were able to
use half of the training samples, a particularly interesting situation when working with
few in-situ measures usually available in biophysical problems.
Model Flexibility and Uncertainty
In order to model the accuracy demonstrated in the previous section, the 𝜀-Huber
cost function provides some relevant information about the reliability of the available
data, which is extremely important when dealing with uncertainty. Figure .shows
the histograms for residuals obtained with the 𝜀-Huber-SVR model. If no uncertainty is
present in data (MERIS data, synthetic data generated according to a physical model),
we can see that the 𝛿C parameter covers a wide range of residuals and mainly penalizes
them with a squared loss function (assuming Gaussian noise), and that the residual tails
are linearly penalized (Cipollini et al., ).
Model Robustness to Reduced Datasets
The number of the available in-situ measurements available is often scarce. In this
experiment, we test the capabilities of the models to deal with low-sized datasets.

204
Digital Signal Processing with Kernel Methods
−0.02
−0.01
0
0.01
0.02
50
100
150
200
250
(a)
δ C 
ε
Outliers
Gaussian
noise
−0.5
0
0.5
0
10
20
30
40
50
Residuals
(b)
Gaussian
noise
Outliers
 δ C 
ε
Figure 4.8 Histograms of residuals obtained with the 𝜀-Huber-SVR model for (a) MERIS and (b)
SeaBAM datasets.
Figure .shows the behavior of the RMSE versus the number of training samples in
the SeaBAM data. We show the average value for random diﬀerent trials. The gain
of the 𝜀-SVR algorithm is noticeable, obtaining an average improvement of .–%
in RMSE with respect to the other models. This eﬀect is especially signiﬁcant when
working with very reduced training sets, as seen in the average gain of .% in RMSE
compared with 𝜀-SVR when using only training samples. We can conclude that, when
the number of available samples is limited (low to moderate) and samples are aﬀected
by uncertainty factors, the versatility to accommodate diﬀerent noise models to the
available data makes 𝜀-Huber-SVR an eﬃcient and robust model.

Kernel Functions and Reproducing Kernel Hilbert Spaces
205
101
102
0.15
0.2
0.25
0.3
RMSE
Number of training samples
ε−SVR
L2−Loss SVR
ε−Huber−SVR
NN−BP
Figure 4.9 RMSE in the test SeaBAM dataset as a function of the number of training samples.
4.8
Concluding Remarks
In this chapter we introduced the most basic concepts of Mercer’s kernels. The concepts
of kernel function, feature mapping, positive deﬁniteness and reproducing kernel
Hilbert spaces were introduced as the core concepts that will accompany us throughout
the remainder of the book.
The main idea in kernel methods is that positive deﬁnite kernels provide a measure of
similarity between possibly complex objects. With the regularized risk framework one
can implement the search over rich classes of functions and still obtain functions that
can be expressed in ﬁnitely many terms of kernel evaluations. Another beneﬁt is that
this search can be made convex and thus yield problems which can be solved eﬃciently.
Kernel methods generalize linear algorithms and allows us to develop nonlinear coun-
terparts easily and still requiring only linear algebra. This is obviously of great practical
convenience since linear algebra is perhaps one of the most solid and well-established
part of mathematics.
The use of kernel methods in machine learning has expanded in the last decades, and
they now constitute a preferred toolbox for practitioners in many disciplines, and in
signal processing in particular. In the following chapters we will build upon the concepts
provided here, and will subsequently introduce more advanced concepts to develop new
kernel methods that tackle speciﬁc signal processing problems.
4.9
Questions and Problems
Exercise ..
Reproduce Example ..for the case of a discrete ﬁnite FT.

206
Digital Signal Processing with Kernel Methods
Exercise ..
Consider a nonnegative function F(𝜔). Prove that the inverse FT of
this function deﬁnes a shift-invariant kernel in a given RKHS of the form K(𝛿t) with
𝛿t = t −t′.
Exercise ..
The technique that optimizes parameters w of a linear estimator ̂y =
f (x) = xTa by minimizing the regularized functional
=
N
∑
i=
‖yi −f (xi)‖+ 𝜆
M
∑
j=
a
j
(.)
is known as ridge regression (Hoerl and Kennard, ). Derive the particular
ridge regression algorithm for the example in Figure .. In order to derive the
algorithm, its parameters must be identiﬁed as the coeﬃcients of the model; that is,
x ∶= [, x, x, … , x]T.
Exercise ..
Deﬁne as the set of all complex-valued functions that are analytic
inside the unit disk D = {x ∶‖z‖ < , x ∈ℂ} of the complex plane, and square
integrable. If an orthonormal basis of is
𝝓(x) =
√
n + 
π
xn,
then prove that the kernel of (Bergman kernel) (Halmos, ) is
K(x, x) = 
π

(−xx∗
).
Exercise ..
Express the polynomial model in the example in Figure .as a func-
tion of Mercer kernels. Identify the kernel and its corresponding nonlinear mapping
according to Mercer’s theorem. Rewrite the ridge regression algorithm in Exercise ..
in terms of kernel dot products.
Exercise ..
Show that the following is a kernel function:
K(x, y) = 
(‖x + y‖
K + ‖x −y‖
K),
for the norm deﬁned by any kernel function K.
Exercise ..
Let us compute the diﬀerence in feature space between two mapped
vectors x and y; that is, 𝝓= 𝝓(x) −𝝓(y). Compute the reproducing kernel function that
acts solely on examples in input space. Show that this is a valid kernel function.

Kernel Functions and Reproducing Kernel Hilbert Spaces
207
Exercise ..
Let us compute the pointwise division in feature space between two
mapped vectors x and y; that is, 𝝓= 𝝓(x)∕𝝓(y). Compute the reproducing kernel
function that acts solely on examples in input space. Show that this is a valid kernel
function.
Exercise ..
Demonstrate the properties in Section ...
Exercise ..
Demonstrate that a continuous kernel K(x, y) = K(x −y) on ℝd is
positive deﬁnite if and only if K(Δ) is the FT of a nonnegative measure. Using this
theorem, also demonstrate that if a shift-invariant kernel K(Δ) is properly scaled, its
FT p(𝝎) is a proper probability distribution.
Exercise ..
Build a simple script for performing SVR on a sinc function in terms
of an RBF kernel for a nonregularly sampled time grid. Search for the optimal width
of the RBF kernel. What can you conclude by observing the spectra of the observation
signal, of the interpolated signal, and of the tuned RBF kernel? Plot the residuals in terms
of the coeﬃcients and discuss the related properties.

209
Part II
Function Approximation and Adaptive Filtering

211
5
A Support Vector Machine Signal Estimation Framework
5.1
Introduction
SVMs were originally conceived as eﬃcient methods for pattern recognition and
classiﬁcation (Vapnik, ), and the SVR was subsequently proposed as the SVM
implementation for regression and function approximation (Shawe-Taylor and Cris-
tianini, ; Smola and Schölkopf, ). Nowadays, the SVR and other kernel-based
regression methods have become a mature and recognized tool in DSP. This is not
incidental, as the widespread adoption of SVM by researchers and practitioners in DSP
is a direct consequence of their good performance in terms of accuracy, sparsity, and
ﬂexibility.
Early studies of time series with supervised SVM algorithms paid attention mainly
to two DSP signal models; namely, nonlinear system identiﬁcation and time series
prediction (Drezet and Harrison ; Goethals et al. b; Gretton et al. b;
Mattera ; Pérez-Cruz and Bousquet ; Suykens ; Suykens et al. b).
However, the algorithm used in both of them was the conventional SVR, just working
on time-lagged samples of the available time series (i.e., essentially an ad hoc time
embedding). Although good results have been reported with this approach, several
concerns can be raised from a conceptual viewpoint of estimation theory:
) The basic assumption for the regression problem statement, in an MMSE sense,
is i.i.d. observations. This assumption is not at all fulﬁlled in time-series analysis,
and algorithms neglecting temporal dependencies such as autocorrelations or
cross-correlations can be missing crucial a priori information.
) The use of Vapnik’s 𝜀-insensitive cost function might not be the most appropriate
loss function in the case of Gaussian noise in the data.
) The kernel trick (Aizerman et al., ) used to develop nonlinear versions from well-
established linear techniques is not the only advantage of the SVM methodology that
would be desirable in DSP.
In this part, we introduce a framework coping with all these issues, which was
previously proposed by Rojo-Álvarez et al. (, ). Starting from the last point,
note that, for instance, SVMs are intrinsically regularized algorithms which, unlike
MMSE methods, are quite resilient to overﬁtting and they are robust in low number
of available training samples and high-dimensional datasets. SVMs also produce sparse
solutions induced by the cost function used, which in turn is advantageous for the
Digital Signal Processing with Kernel Methods, First Edition. José Luis Rojo-Álvarez, Manel Martínez-Ramón,
Jordi Muñoz-Marí, and Gustau Camps-Valls.
© John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.

212
Digital Signal Processing with Kernel Methods
model interpretability and computational eﬃciency. SVMs also involve few model
parameters to be tuned and lead to convex optimization problems. SVM algorithms
are rooted on a solid mathematical background, and hence bounds of performance,
uniqueness, stability, generalization, and optimality conditions can be established, and
they can beneﬁt from the theory of reproducing kernel functions to treat heterogeneous
information in a uniﬁed way, as we will see in the following chapters.
In recent years, several SVM algorithms for DSP applications have been proposed
aiming to overcome the aforementioned limitations. A ﬁrst approach to nonparametric
spectral analysis, using the robust SVM optimization criterion, instead of MMSE, was
introduced by Rojo-Álvarez et al. (). The robustness properties of the SVM were
further exploited by proposing linear approaches for 𝛾ﬁltering (Camps-Valls et al.,
), ARMA modeling (Rojo-Álvarez et al., ), array beamforming (Martínez-
Ramón et al. ; Pérez-Cruz and Bousquet ), and subspace-based spectrum
estimation (Gonnouni et al., ). The nonlinear generalization of ARMA ﬁlters with
kernels (Martínez-Ramón et al., ) and temporal and spatial reference antenna
beamforming using kernels and SVM (Martínez-Ramón et al., ) have also been
proposed. The use of convolutional signal mixtures has been proposed for interpolation
and sparse deconvolution problems (Rojo-Álvarez et al., , ), thanks to the
autocorrelation kernel concept, a straightforward property which has opened the ﬁeld
for a number of unidimensional and multidimensional extensions of communications
problems (Figuera et al., , ).
This chapter starts to pave the way to treat all these problems within the ﬁeld of kernel
machines, and it presents the fundamentals for a simple, yet uniﬁed, framework for
tackling estimation problems in DSP using SVM, following the ideas presented by (Rojo-
Álvarez et al., ). The second part of this chapter and Chapters and are devoted to
the particular models and approximations deﬁned within the framework. Even though
models in these chapters are formulated for SVM algorithms, they can be easily included
in other kernel methods, as we will see later. Chapter reviews some advances in kernel-
based methods for regression and function approximation beyond SVMs. We end this
part with an important topic in DSP: learning in online nonstationary scenarios with
kernel-based adaptive ﬁltering methods. The main approximations are reviewed and
exempliﬁed in Chapter .
Hence, in this chapter, we formalize the ﬁeld in a simple, bottom-up framework
for constructing kernel algorithms for DSP. In Section .we propose a three-block
framework to treat SVM estimation in DSP problems. We think that such a framework
is essential to develop new methods in a consistent and systematic way, but also to
characterize recent methods in the kernel methods community. Then, we review the
elements of the (nonlinear) SVR algorithm, which all have the fundamental properties
required for general estimation problems.
A class of linear SVM-DSP algorithms come from the so-called primal signal model
(PSM) (Rojo-Álvarez et al., ). In this framework, rather than the prediction of the
observed signal, the goal of the SVM is the estimation of a set of model coeﬃcients
containing the relevant information of that signal. This chapter presents the DSP
algorithms using SVM for linear estimation, known as PSM algorithms. In this class of
algorithms, the signal model is readily stated in the primal model of the SVM, which can
be casted as a simple substitution of the linear regression data model by other particular
time-series data models. A generic notation for this set of algorithms is given in terms

A Support Vector Machine Signal Estimation Framework
213
of the expression of a signal using a given signal space expansion. The kernel trick is not
used at this stage, but rather the use of diﬀerent well-known signal models instead of
regression or classiﬁcation can be readily analyzed.
We conclude this chapter with some examples of use of SVR in these problems,
and on PSM problem statements. Particular signal space expansions are given for
nonparametric spectral analysis, for system identiﬁcation and time series prediction, for
digital communications, for convolutional models, and for array processing (temporal
reference). Synthetic examples are also used to consolidate the main concepts, and
application examples are further provided. In the following sections, taking into account
the general signal model in previous chapters, a general PSM is stated which allows
us to highlight the common and the problem-speciﬁc steps in the design of an SVM
algorithm in this setting. Examples of the use of this general signal model for stating new
SVM-DSP linear algorithms are given by making SVM algorithms for well-known signal
processing estimation problems; namely, sinusoidal decomposition, ARX and 𝛾-ﬁlter
system identiﬁcation, digital communications, sinc interpolation, sparse deconvolution,
and array processing with temporal reference. Several application examples show the
performance and scope of the PSM in SVM-DSP algorithms.
5.2
A Framework for Support Vector Machine Signal Estimation
This section formalizes a uniﬁed framework for developing SVM algorithms for super-
vised estimation in DSP (Rojo-Álvarez et al., ). The framework focuses on time-
series analysis, in which the time structure of the data is highly informative, but often
discarded or at least mistreated. Figure .shows the main model building strategies in
the SVM estimation framework, and examples of dedicated problem formulations that
can be developed within it.
The treatment of other structure domains (e.g., images or video) can be also formal-
ized under the same principles. Speciﬁc detailed model instantiations and empirical
Primal Signal Models (PSM)
SVM spectral estimation
SVM ARX system identification
SVM sinc kernel interpolation
SVM sparse deconvolution
SVM antenna array processing
RKHS Signal Models (RSM)
Nonlinear system identification
Nonlinear array processing
Dual Signal Models (DSM)
Non-uniform interpolation
Nonlinear sparse deconvolution
SVM framework for DSP problems
Figure 5.1 A schematic overview of the SVM framework for signal estimation.

214
Digital Signal Processing with Kernel Methods
evidence of performance will be given in this and in the following chapters. This
framework can be summarized in the following types of signal models:
●PSMs. When a speciﬁc time-signal structure is assumed to model the underlying
system that generated the data, the linear signal model (the so-called PSM) can
actually be used. We will treat this formulation later in this chapter in detail. The
statement of linear signal model equations in the primal, or SVM PSM for short,
allows us to obtain robust estimators of the model coeﬃcients (Rojo-Álvarez et al.,
) and to take advantage of almost all the characteristics of the SVM methodology
in classical DSP problems, such as ARX time series modeling, spectral analysis
(Camps-Valls et al. ; Rojo-Álvarez et al. , ), and antenna array signal
processing (Martínez-Ramón et al., ). Then, nonlinear versions of the signal
structure can be readily developed by the following two further approaches.
●RKHS signal models (RSMs). This is the ﬁrst choice for nonlinear model building.
The signal model equation is written in an RKHS by using the well-known RKHS
model formulation and the kernel trick, in such a way that Mercer kernels can be
readily used to replace dot products (Martínez-Ramón et al. ; Martínez-Ramón
et al. ; Vapnik ). We will treat this formulation in Chapter in detail.
This constitutes the standard kernelization approach to signal processing, which has
provided many kernel algorithms beyond SVM, as will be reviewed in Chapter .
●Dual signal models (DSMs). This second (and not so much usual) nonlinear alterna-
tive considers a signal expansion made by an auxiliary signal model equation, which in
turn is given by a nonlinear regression of each time instant in the observed time series.
We will treat this formulation in Chapter with detail. DSMs have been previously
proposed in an implicit way, and were based on the nonlinear regression of the time
instants with appropriate Mercer kernels (Rojo-Álvarez et al., , ). While
RSM allows us to scrutinize the statistical properties in the RKHS, the DSM can
give us an interesting and straightforward interpretation of the SVM algorithm under
study, in connection with classical LTI system theory.
This framework is summarized in Tables .and ., where the key formulas are shown
for better understanding the signal model equations. The tables give a principled frame-
work for building eﬃcient SVM linear and nonlinear algorithms in DSP applications.
Note that there is an almost endless variety of signal model problems and equa-
tions in DSP. Among them, we choose the following motivating pathways, following
Rojo-Álvarez et al. ():
●From the viewpoint of the nature of the signals that can be used, we consider
global-time and local-time signal expansions. The former are given by basis signals
whose duration expands in a nondecaying way throughout the time interval where
the estimated signal is observed. In particular, sinusoids in nonparametric spectral
estimation, and delayed versions of exogenous and endogenous signals in diﬀerence
equation models. The latter are given by basis signals which are either duration limited
or decaying, which is the case of sinc functions in time-series interpolation, or energy-
deﬁned impulse responses in deconvolution problems. Illustrative examples of these
kinds of equations are summarized in Table ..
●A diﬀerent, yet related approach comes when diﬀerent applications can be stated
according to diﬀerent unknown terms of the same speciﬁc signal model equation. An

Table 5.1 Scheme of the DSP-SVM framework (I): equations of the time-series models for signal estimation.
Global time, ̂yn
Local time, ̂yn
Regression, ̂y
Spectral
ARx
Sinc interpolation
Deconvolution
PSM
⟨w, x⟩+ b
∑K
k=ak cos(k𝜔𝜔tn + 𝜙𝜙k)
∑Q
p=Dpyn−p + ∑Q
q=Eqxn−q+
∑N
k=aksinc(t −tk)
xn ∗hn
RSM
⟨w, 𝜙𝜙(x)⟩+ b
—
⟨wd, 𝜙𝜙d(yn)⟩+ ⟨we, 𝜙𝜙e(xn)⟩
—
—
DSM
∑N
i=𝛼𝛼iK(xi, x) + b
—
—
K(t) ∗∑
k 𝜂𝜂k𝛿𝛿(t −tk)
𝜂𝜂n ∗Rh
n

216
Digital Signal Processing with Kernel Methods
Table 5.2 Scheme of the DSP-SVM framework (II): equations for
signal models in antenna array processing.
Temporal reference
Spatial reference
PSM
⟨a, xn⟩
—
RSM
⟨w, 𝜙(xn)⟩
⟨w, 𝜙(bia)⟩−b
DSM
—
—
excellent example in this setting is antenna array processing for beamforming, where
the same signal model equation supports temporal reference signal detection, and
spatial reference estimation problems. As illustrated in Table ., PSM and RSM have
been proposed for temporal reference problems, in a similar way to DSP problems
in Table .. And interestingly, a slightly diﬀerent signal model is used in spatial
reference, expressed in both cases in terms of possibly nonlinear mapping. This aims
to illustrate that, in this case, we switch to an eigenproblem statement, which is
a better representation for the data model, both for the linear and the nonlinear
cases.
The equations in Tables .and .will be explained in detail in this and the following
chapters, so the reader is encouraged to come back to these tables after their ﬁrst
reading. Note also that, in these tables, many problems have not been addressed
yet in the literature (as indicated by “—”), showing that our intention is to motivate
the interested reader to complete and expand this table according to their own DSP
application needs.
5.3
Primal Signal Models for Support Vector Machine Signal
Processing
We start by deﬁning the transverse vector of a set of explanatory signals in the general
signal model in Deﬁnition .., and then formulating the SVM algorithm from that
expansion. This will be called the PSM for building time-series DSP algorithms.
Deﬁnition ..(Time-transversal vector of a signal expansion)
Let {yn} be a
discrete time series in a Hilbert space, and given the general signal model in Def-
inition .., then the nth time-transversal vector of the signals in the generating
expansion set {s(k)
n } is deﬁned as
sn = [s()
n , s()
n , … , s(K)
n ]T.
(.)
Hence, it is given by the nth samples of each of the signals generating the signal subspace
where the signal approximation is made.
Figure .depicts an example of the time-transversal vector for a sinusoidal model.
Note that, for determining the time-transversal vectors in a given signal model, we start

A Support Vector Machine Signal Estimation Framework
217
2
4
6
8
10
12
14
16
18
20
−1
0
1
z1[n]
zp[n]
zP[n]
2
4
6
8
10
12
14
16
18
20
−0.5
0
0.5
1
2
4
6
8
10
12
14
16
18
20
−1
0
1
n
v10
Figure 5.2 Time-transversal vectors used for PSM in the statement of linear SVM-DSP algorithms.
by identifying the signals considered in the expansions and then we work on a time basis
for identifying the elements of each of those signals for a given discrete time. The SVM
problem statement can be stated as follows.
Theorem ..(PSM problem statement)
Let {yn} be a discrete-time series in a
Hilbert space, and given Deﬁnition .., then the optimization of

‖a‖+
N
∑
n=
𝜀H(en),
(.)
with a = [a, a, … , aK]T, gives an expansion solution whose signal model is
̂yn =
K
∑
k=
aks(k)
n = ⟨a, sn⟩
(.)
ak =
N
∑
n=
𝜂nsk
n ⇒a =
N
∑
n=
𝜂nsn,
(.)
where 𝜂n are the SVM Lagrange multipliers, and the solution at instant m is
̂ym =
N
∑
n=
𝜂n⟨sn, sm⟩.
(.)
Only instants n with 𝜂n ≠are part of the solution (support time instants).

218
Digital Signal Processing with Kernel Methods
Therefore, each expansion coeﬃcient ak can be expressed as a linear combination of
input space vectors. Note at this point that sparseness can be obtained in coeﬃcients
ak, but not in coeﬃcients 𝜂n, and also that robustness is ensured for coeﬃcients ak when
using the 𝜀-Huber cost. The Lagrange multipliers are obtained from the dual problem,
which is built in terms of a kernel matrix depending on the signal correlation.
Deﬁnition ..(Correlation matrix from time-transverse vectors)
Given a set of
time-transverse vectors, the correlation matrix of the PSM is deﬁned as
Rs
m,n ≡⟨sm, sn⟩.
(.)
Property (Correlation matrix and dual problem)
Given the general PSM in Equa-
tion .and the correlation matrix from the time-transverse vectors in Equation .,
the dual problem yielding the Lagrange multipliers consists of maximizing
−
(𝜶−𝜶∗)T(Rs + 𝛿I)(𝜶−𝜶∗) + (𝜶−𝜶∗)Ty −𝜀T(𝜶+ 𝜶∗)
(.)
constrained to ≤𝛼n, 𝛼∗
n ≤C.
This property can be readily shown from considerations on the Lagrange functional
and the associated KKT conditions (Rojo-Álvarez et al., ). Therefore, by taking into
account the PSM for a given DSP problem, one can determine the signals s(k)
n generating
the Hilbert subspace where the observations are projected to, and then the remaining
elements and steps of the SVM methodology (such as the input space, the input space
correlation matrix, the dual QP problem, and the solution) can be straightforwardly
obtained.
5.3.1
Nonparametric Spectrum and System Identification
The ﬁrst SVM-DSP algorithms that were proposed using the PSM framework were
the sinusoidal decomposition (Rojo-Álvarez et al., ), the ARX system identiﬁca-
tion (Rojo-Álvarez et al., ), and the 𝛾-ﬁlter structure (Camps-Valls et al., ).
We next point out the relevant elements that can be identiﬁed in these algorithms.
Property (PSM coeﬃcients for nonparametric spectral analysis)
Given the sig-
nal model for nonparametric spectral analysis in Deﬁnition .., the estimated coef-
ﬁcients using the PSM are
ck =
N
∑
n=
𝜂n cos(k𝜔tn);
dk =
N
∑
n=
𝜂n sin(k𝜔tn).
(.)
Property (PSM correlation and dual problem for nonparametric spectral analysis)
Given the signal model hypothesis in Deﬁnition .., the correlation matrix is given by
the sum of two terms:

A Support Vector Machine Signal Estimation Framework
219
Rcos
m,n =
K
∑
k=
cos(k𝜔tm) cos(k𝜔tn)
(.)
Rsin
m,n =
K
∑
k=
sin(k𝜔tm) sin(k𝜔tn)
(.)
and the dual functional is given by Equation .using Rs = Rcos + Rsin.
The identiﬁcation of the corresponding time-transverse vectors is straightforward.
The derivation of this algorithm in Rojo-Álvarez et al. () is obtained by using these
two properties in the PSM. Similar considerations can be drawn for the ARX system
identiﬁcation algorithm in Rojo-Álvarez et al. () using the next two properties.
Property (PSM coeﬃcients for ARX system identiﬁcation)
Given the signal model
for ARX system identiﬁcation in Deﬁnition .., the estimated PSM coeﬃcients are
given by
ak =
N
∑
n=
𝜂nyn−k;
bk =
N
∑
n=
𝜂nxn−k+
(.)
Property (PSM Correlation and dual problem for ARX system identiﬁcation)
Given the signal model hypothesis for ARX system identiﬁcation in Deﬁnition ..,
the correlation matrix is given by the sum of two terms:
Ry
m,n =
P
∑
k=
ym−kyn−k
(.)
Rx
m,n =
Q
∑
k=
xm−k+xn−k+
(.)
These equations represent the time-local Pth- and Qth-order sample estimators of
the values of the (non-Toeplitz) autocorrelation functions of the input and the output
discrete time processes respectively. The dual functional to be maximized is given by
Equation .using Rs = Ry + Rx.
As described in previous chapters, the 𝛾-ﬁlter in Camps-Valls et al. () is deﬁned
by the following expressions:
yn =
P
∑
i=
wixi
n
(.)
xi
n =
⎧
⎪
⎨
⎪⎩
xn,
i = 
(−𝜇)xi
n−+ 𝜇xi−
n−,
i = , … , P
(.)

220
Digital Signal Processing with Kernel Methods
where yn is the ﬁlter output signal, xn is the ﬁlter input signal, xi
n is the signal present
at the input of the ith gamma tap, n is the time index, and 𝜇is a free parameter.
The SVM 𝛾-ﬁlter algorithm in the PSM formulation is derived by means of the next
property.
Property (Time-transverse vector in 𝛾-ﬁlter system identiﬁcation)
The time-
transverse vector in the 𝛾-ﬁlter for system identiﬁcation in a PSM is given by
sq = [xq
, xq
, … , xq
N]T,
q = , … , Q.
(.)
The signals generating the projection subspace are the input vector signals after
each 𝛾unit loop. For a previously ﬁxed value of 𝜇, the generating vectors of the
Hilbert projection space are straightforwardly determined. By using this deﬁnition, the
obtention of the signal model, the coeﬃcients, and the dual problem can be readily
found by just obtaining the autocorrelation matrix from Equation .and solving the
dual problem in Equation ..
5.3.2
Orthogonal Frequency Division Multiplexing Digital Communications
Fernández-Getino et al. () proposed an SVM robust algorithm for channel esti-
mation that is speciﬁcally adapted to the orthogonal frequency division multiplexing
(OFDM) data structure. There were two main novelties in this proposal. First, the
use of complex regression in SVM formulation provided with a simpler scheme than
describing OFDM signal with either multilevel or nested binary SVM classiﬁcation
algorithms, as addressed by some previous approaches to OFDM with SVM. Second,
the adequacy of free parameters in the 𝜀-Huber robust cost function (Rojo-Álvarez
et al., ) was investigated in the presence of impulse noise. Although robustness
of some digital communication receivers against impulse noise had been examined by
using M-estimates (Bai et al. ; Ghosh ), there was no previous work about the
performance of SVM algorithms in digital communications under this condition. We
summarize here the main concepts and results in that work.
For simplicity, a linear dispersive channel with non-Gaussian noise is analyzed here.
The discrete-time received OFDM signal for a system with N subcarriers is
rn =
N−
∑
k=
SkHk e𝕚(π∕N)kn + wn + bngn,
(.)
where rn (n = , … , N −) are time-domain samples before DFT transformation, Hk is
the channel’s frequency response at kth frequency, Sk is the complex symbol transmitted
at kth subcarrier, and wn is the complex white Gaussian noise process N(, 𝜎
w). The
impulse noise is modeled as a BG process; that is, the product of a real Bernoulli process
bn with Pr(bn = ) = p and a complex Gaussian process gn ∼N(, 𝜎
i ) (Ghosh,
). Then, the residual noise at the receiver side is given by the sum of both terms
zn = wn + bngn.
In coherent OFDM systems, pilot symbols are usually inserted for channel estimation
purposes. Then, the channel frequency response can be ﬁrst estimated over a subset p

A Support Vector Machine Signal Estimation Framework
221
of subcarriers, with cardinality Np = |p|, and then interpolated over the remaining
subcarriers (N −Np) by using, for example, DFT-based techniques with zero-padding
in the time domain (Edfors et al., ). Now, the OFDM system can be expressed as
rn =
∑
k∈{p}
PkHk e𝕚(𝜋∕N)kn +
∑
k∉{p}
XkHk e𝕚(𝜋∕N)kn + zn,
(.)
where Pk and Xk are respectively the complex pilot and data symbol transmitted at the
kth subcarrier. It is well known that if the channel impulse response has a maximum
of L resolvable paths (and hence of degrees of freedom), then Np must be at least equal
to L (Fernández-Getino García et al., ). The signal model for OFDM-SVM is as
follows:
rn =
∑
k∈{p}
PkHk e𝕚(𝜋∕N)kn + en,
(.)
where en = ∑
k∉{p} XkHk e𝕚(𝜋∕N)kn +zn contains the residual noise plus the term due to
data symbols. Here, these unknown symbols carrying information will be considered as
noise during the training phases. Channel estimation via an MMSE cost function is no
longer the ML criterion when dealing with this sort of noise (Papoulis, ). In order
to improve the performance of the estimation algorithm, a robust cost function must
be introduced.
Here, and for complex en, we deﬁne 𝜀H(en) = 𝜀H(Re{en}) + 𝜀H(Im{en}), where
Re{⋅} and Im{⋅} denote real and imaginary parts respectively. The primal problem can
be stated as minimizing


∑
k∈{p}
|Hk|+ 
𝛿
∑
n∈I
(𝜉n + 𝜉+
n )+ C
∑
n∈I
(𝜉n + 𝜉+
n )
+ 
𝛿
∑
n∈I
(𝜁n + 𝜁+
n )+ C
∑
n∈I
(𝜁n + 𝜁+
n ) −
∑
n∈I,I
𝛿C
(.)
constrained to
Re{rn} −
∑
k∈{p}
Re{PkHk e𝕚(𝜋∕N)kn} ≤𝜀+ 𝜉n
(.)
Im{rn} −
∑
k∈{p}
Im{PkHk e𝕚(𝜋∕N)kn} ≤𝜀+ 𝜁n
(.)
−Re{rn} +
∑
k∈{p}
Re{PkHk e𝕚(𝜋∕N)kn} ≤𝜀+ 𝜉+
n
(.)
−Im{rn} +
∑
k∈{p}
Im{PkHk e𝕚(𝜋∕N)kn} ≤𝜀+ 𝜁+
n
(.)
𝜉(+)
n , 𝜁(+)
n
≥
(.)
for n = , … , N −, where pairs of slack variables are introduced for both real (𝜉(+)
n ) and
imaginary (𝜁(+)
n ) residuals; superscript plus signs and no superscript stand for positive

222
Digital Signal Processing with Kernel Methods
and negative components of residuals respectively; and I–I(I–I) are the set of
samples for which real (imaginary) parts of the residuals are in the quadratic–linear
cost zone.
For giving support material on complex formulation, we next present the complete
derivation of this algorithm. In brief, the primal–dual functional is obtained by intro-
ducing the constraints into the primal functional by means of Lagrange multipliers
{𝛼R,n}, {𝛼+
R,n}, {𝛼I,n}, and {𝛼+
I,n} for the real (subscript R) and imaginary (subscript I)
parts of the residuals. By making zero the primal–dual functional gradient with respect
to Hk, we have the following expression for channel estimated values at pilot positions:
̂Hk =
N−
∑
n=
𝜓nPk,
(.)
where 𝜓n = (𝛼R,n −𝛼+
R,n) + 𝕚(𝛼I,n −𝛼+
I,n). For notation, we deﬁne the following column
vector components:
vn(k) = [Pk e𝕚(𝜋∕N)kn],
k ∈{p},
(.)
and the following Gram matrix as Rn,m = vH
n vm. Now, by placing the optimal solution
from Equation .into the primal–dual functional and grouping terms, a compact
form of the functional problem can be stated in vector form, that consists of maximizing
−
𝜓H(R + 𝛿I)𝜓+ Re(𝜓Hr) −(𝜶R + 𝜶+
R + 𝜶I + 𝜶+
I )𝜀
(.)
constrained to ≤{𝛼R,n}, {𝛼+
R,n}, {𝛼I,n}, {𝛼+
I,n} ≤C, where 𝝍= [𝜓, … , 𝜓N−]; I, 
are the identity matrix and the all-ones column vector respectively; 𝛼R is the vector
containing the corresponding Lagrange multipliers, the other subsets being similarly
represented; and r = [r, … , rN−]T.
The channel values at pilot positions in Equation .can be obtained by optimizing
Equation .with respect to {𝛼R,n}, {𝛼+
R,n}, {𝛼I,n} and {𝛼+
I,n} and then substituting into
Equation ..
5.3.3
Convolutional Signal Models
Convolutional signal models are simply those models that contain a convolutive mix-
ture in their formulation. Some of the most representative ones are the nonuniform
interpolation (using sinc kernels, RBF kernels, or others) and the sparse deconvolution,
presented in Rojo-Álvarez et al. (, ). Their analysis also gives us the founda-
tions of the DSM to be subsequently used in a variety of signal problem statements.
We next focus on summarizing the properties that are relevant for giving a signal pro-
cessing block structure, which will be used for their analysis, following the framework
proposed by Rojo-Álvarez et al. ().

A Support Vector Machine Signal Estimation Framework
223
Property (PSM coeﬃcients for sinc interpolation)
Given the signal model in
Deﬁnition ..for sinc kernel interpolation, the PSM coeﬃcients are
ak =
N
∑
n=
𝜂nsinc(𝜎(tk −tn)).
(.)
Property (PSM correlation and dual problem for sinc interpolation)
Given the
signal model hypothesis in Deﬁnition .., the correlation matrix is given by
Rsinc
m,n =
N
∑
k=
sinc(𝜎(tm −tk))sinc(𝜎(tn −tk)).
(.)
The dual functional to be maximized is given by Equation .using Rs = Rsinc.
Coeﬃcients in Equation .are proportional to the cross-correlation of coeﬃcients
𝜂n and a set of sinc functions, each centered at time instants tn (Rojo-Álvarez et al.,
). Similar considerations can be made about the sparse deconvolution signal model,
as follows:
Property (PSM coeﬃcients for sparse deconvolution)
The estimated PSM coeﬃ-
cients of the signal model in Deﬁnition ..are
̂xn =
N
∑
i=
𝜂ihi−n.
(.)
Property (PSM correlation and dual problem for sparse deconvolution)
The dual
problem corresponding to Deﬁnition ..is found by using the time-transversal vector
sp = [hn, hn−, hn−, … , hn−p+]T.
(.)
The correlation matrix Rh is given by Equation ., and in this case it represents the
correlation matrix of hn. The dual functional to be maximized is given by Equation ..
In order to express the SVM-DSP algorithm for sparse deconvolution in terms of
signal processing blocks, we can use a well-known relationship between model residuals
and Lagrange multipliers, valid for general SVM algorithms.
Property (Nonlinear relationship between Lagrange multipliers and residuals)
Given the model residuals en and the corresponding model coeﬃcient 𝜂n in an SVM
algorithm for estimation, the following relationship between them holds:
𝜂n = 𝜕L𝜀H(e)
𝜕e
||||e=en
= L′
𝜀H(en),
(.)
which is a nonlinear relationship depending on the free parameters of the 𝜀-Huber cost
function.

224
Digital Signal Processing with Kernel Methods
Therefore, the Lagrange multipliers (or, equivalently, the model coeﬃcients) are
mapped from the model residuals by using a static nonlinear map which is given by the
ﬁrst derivative of the cost function (in our case, the 𝜀-Huber cost). This property can be
easily shown by using the appropriate set of KKT conditions (Rojo-Álvarez et al., ),
and it indicates that the model coeﬃcients are a piecewise linear function of the model
residuals.
According to this expression of the Lagrange multipliers as a time series, the sparse
deconvolution model can be further analyzed as follows.
Property (PSM block diagram for sparse deconvolution)
Let a discrete time signal
be deﬁned by model coeﬃcients 𝜂n for n = , … , N, and being zero otherwise. Then,
we can express the relationship between the model coeﬃcients and the estimated signal
as follows:
̂xn = 𝜂n ∗h−n ∗𝛿n+M
(.)
where 𝛿n is the Kronecker delta sequence (for n = and elsewhere), and ∗denotes
the discrete-time convolution operator.
This is an interesting property from the signal processing point of view, and it can
be easily obtained by examining the KKT in the PSM of the sparse deconvolution
problem (Rojo-Álvarez et al., ). Hence, we can consider a joint equivalent closed-
loop system, given in Figure .a, which contains all the elements of the SVM algorithm
expressed as signals or systems. Speciﬁcally, one is a nonlinear system, given by
Property , and the remaining ones are linear, time-invariant systems. According to the
preceding property, estimated signal ̂xn will not be sparse in general, because it is the
sparseness of 𝜂n that can be controlled with the 𝜀parameter, but there is a convolutional
relationship between ̂xn and 𝜂n that will depend on the impulse response, which in
general does not have to be sparse.
In addition, note that there is no Mercer kernel appearing explicitly in this problem
statement of PSM for sparse deconvolution, as should be expected in an SVM approach.
However, the block diagram highlights that there is an implicitly present autocorrelation
kernel, given by
Rh
n = hn ∗h−n,
(.)
in the case we associate the two systems containing the original system impulse
response and its reversed version. From linear system theory, the order of the blocks
could be changed without modifying the total system. However, the solution signal is
embedded between these two blocks, which precludes the explicit use of this autocor-
relation kernel in this PSM formulation. Finally, the role of delay system 𝛿n+M can be
interpreted as just an index compensation that makes the total system causal.
In summary, the PSM yields a regularized solution, in which an autocorrelation kernel
is implicitly used, but it does not allow one to control the sparseness of the estimated
signal. These properties will be used later in the DSM for proposing high-performance
sparse deconvolution algorithms.

A Support Vector Machine Signal Estimation Framework
225
(a)
(b)
(c)
+
+
+
ηn
ηn = xn
ηn = xn
η
δn+Q
δn+Q
ε
–ε
e
C
–C
η
ε
–ε
e
C
–C
η
ε
–ε
e
C
–C
Rh
n = Kn
Kn = Rh
n
hn* h–n
h–n
hn
yn
yn
yn
zn
en
en
zn
en
yn
xn
xn
Kn
Figure 5.3 Signal model for SVM-DSP sparse deconvolution. (a) Block diagram, elements, and signals
in the PSM. (b) Convolutional model for sinc interpolation using the DSM problem statement, where
the impulse response of the convolutional model is a valid Mercer kernel. (c) Convolutional model for
sparse deconvolution, for an arbitrary impulse response, where the Mercer kernel is the
autocorrelation of the impulse response signal.
5.3.4
Array Processing
We present here the algorithm formulation from SVM-DSP framework both for tem-
poral reference and for spatial reference in array processing. The array processing
algorithm needs a complex-valued formulation. The complex Lagrange coeﬃcients can
be expressed as 𝜓n = 𝜂n + 𝕚𝜈n, and 𝜂n = 𝛼n −𝛼∗
n and 𝜈n = 𝛽n −𝛽∗
n are the Lagrange
coeﬃcients generated by the real and imaginary parts of the error.
Property (PSM for temporal reference array processing)
Given the array process-
ing signal deﬁned in Deﬁnition .., the PSM coeﬃcients for this problem are given by
ak =
N
∑
n=
𝜓nxk
n.
(.)

226
Digital Signal Processing with Kernel Methods
Property (Dual problem for temporal reference array processing)
Let yn, with ≤
n ≤N, be a set of desired signals available for training purposes, then the problem
is known as temporal reference array processing. The incoming signal kernel matrix is
deﬁned as
K l,m =
K
∑
i=
xk
l xk
m.
(.)
The dual functional to be maximized is a complex-valued extension of Equation .;
that is:
𝝍TK𝝍+ Re(𝝍Ty) −𝜀T(𝜶+ 𝜶∗+ 𝜷+ 𝜷∗),
(.)
where y stands for the complex conjugate of y.
While the prior knowledge that is exploited in temporal reference algorithms is a set
of training observations for which the corresponding transmitted symbols are yn, this
information is not available in spatial reference. Instead, the prior knowledge is the DOA
of the signal of interest. Thus, the spatial ﬁlter must be optimized for minimum variance
of its output, subject to the constraint of producing the right response in the presence
of a signal coming from the given DOA.
The incoming signal is then ﬁltered by a steering vector in the DOA (see Equa-
tion .). The training data of the SVM must be a set of arbitrary signals containing this
DOA (Martínez-Ramón et al., ). Thus, we must satisfy the following minimization:
min
w wHRw.
(.)
If prior to the minimization the data are whitened with the transformation
̃xn = 𝜦∕QHx,
(.)
then the expression equivalent to the previous minimization is simply
min
w wHw,
(.)
where we used the eigendecomposition of autocorrelation matrix R as R = Q𝜦QH.
Then, the minimization can be expressed as one of a regular SVM. The training vectors
have the form of steering vectors ascaled with arbitrary amplitudes dn. Thus, after
whitening these vectors, the weight vector can be expressed as
w = 𝜦−∕Qao
N
∑
n=
𝜓ndn.
(.)
Property (PSM spatial reference array processing)
Let ̃a
= 𝚲−∕Qabe a
whitened version of the steering vector aas expressed in Equation ., coming from
DOA 𝜃. The PSM coeﬃcients for this problem are given by

A Support Vector Machine Signal Estimation Framework
227
ak =
N
∑
n=
𝜓ndn ̃ak
,
(.)
where dn is an arbitrary complex amplitude.
Property (Dual problem for spatial reference array processing)
Let dn, ≤n ≤N,
be a set of arbitrary complex amplitudes. The incoming signal kernel matrix is deﬁned as
K l,m =
K
∑
i=
dl̃a
k
̃ak
dm,
(.)
where the last expression follows from the fact that the steering vector is normalized.
5.4
Tutorials and Application Examples
The cost function in terms of robustness to outlying samples, the eﬀect of the 𝛿
parameter, and the sparsity property are analyzed with the SVM algorithm for non-
parametric spectral analysis (Rojo-Álvarez et al., ). We also illustrate here the
performance of some of the preceding PSM algorithms; namely, linear system iden-
tiﬁcation with the SVM 𝛾-ﬁlter structure (Camps-Valls et al., ; Rojo-Álvarez
et al., ), SVM-ARMA formulation for parametric spectral estimation, OFDM
communications (Fernández-Getino et al., ), and PSM algorithms performance
for antenna processing.
5.4.1
Nonparametric Spectral Analysis with Primal Signal Models
A synthetic data example shows the usefulness of PSM for dealing with outliers in the
data, where SVM-NSA has been considered (see MATLAB code in Listing .for more
details).
function [predict,coefs] = SVM_NSA(~,Ytrain,~,gamma,epsilon,C)
% Initialization
ts=1; fs=1;
N = length(Ytrain);
t = ts*(0:N-1); % ts is the sampling period
w0 = 2*pi*fs/N; % fs is the sampling frequency, w0 is the angular ...
frequency resolution
% Construct kernel matrix
KC = cos((1:N)'*w0*t);
KS = sin((1:N)'*w0*t);
H = KC'*KC + KS'*KS;
% Train SVM and predict
inparams=sprintf('-s 3 -t 4 -g %f -c %f -p %f -j 1', gamma, C, epsilon);
[predict,model]=SVM(H,Ytrain,H,inparams);
coefs=getSVs(Ytrain,model);
Listing 5.1 SVM-NSA algorithm (SVM_NSA.m).

228
Digital Signal Processing with Kernel Methods
Data Generation
Let yn = sin(πfn) + ev
n + ej
n, where f = ., ev
n is a white, Gaussian noise sequence (zero
mean, variance .), and ej
n is an impulsive noise process, generated as a sparse sequence
for which % of the randomly placed samples have high amplitude values given by
±+ (−., .), where () denotes the uniform distribution in the given interval,
and the remaining are null samples. The length is samples, and we set N𝜔= .
We ﬁxed 𝜀= and 𝛿= . The MATLAB code in Listing .can be used to create this
synthetic data example.
function data = genDataSpectral(conf)
rng(500)
cf = conf.data;
% Signal
y = sin(2*pi*cf.f*(1:cf.n))';
% Gaussian noise
y = y + sqrt(cf.var_noise)*randn(size(y));
% Impulsive noise
nin = round(cf.p*cf.n);
affected_samples = randperm(cf.n,nin);
imp_noise = cf.A*sign(rand(1,nin)-.5)+(rand(1,nin)-0.5);
y(affected_samples)=imp_noise;
% Create data structure
data.Xtrain=cf.f; data.Ytrain=y; data.Xtest=cf.f; data.Ytest=y;
Listing 5.2 Generating spectral analysis data (genDataSpectral.m).
Experiment Settings and Results
Parameter C can be chosen according to Equation .. Results can be easily generated
using the code conﬁg_spectral.m and display_results_spectral.m on this
book’s web page. Results show that, for C low enough, large residual amplitudes can be
present without distorting the solution.
5.4.2
System Identification with Primal Signal Model γ-filter
In this example, we focused on the main advantages of both the 𝛾-ﬁlter structure
(stability and memory depth) and the SVM with PSM (regularization). We compared
the memory parameter 𝜇in the LS, the SVM, and the regularized 𝛾-ﬁlter algorithms.
We identiﬁed the third-order elliptic low-pass ﬁlter with coeﬃcients
yn = .xn −.xn−−.xn−+ .xn−
+ .yn−−.yn−+ .yn−
(.)
H(z) = .−.z−−.z−+ .z−
−.z−+ .z−−.z−
,
(.)
which was proposed in Principe et al. () because of its long impulse response.
A -sample input discrete process {xn} was a white, Gaussian noise sequence with
zero mean and unit variance. The output signal {yn} was corrupted by additive, small
variance (𝜎
e = .) noise. An independent set of samples was used for testing, and

A Support Vector Machine Signal Estimation Framework
229
μ
P = 1
P = 2
P = 3
P = 4
(a)
(b)
(c)
(d)
LS γ–filter
SVM γ – filter
Regularized γ – filter
1
1.1
0.9
0.8
0.7
J
1
1.1
0.9
0.8
0.7
0.6
0.5
J
1
1.1
0.9
0.8
0.7
0.6
0.5
J
0.95
0.9
0.85
J
0.8
0.75
0.7
0.2
0.4
0.6
0.8
μ
0.2
0.4
0.6
0.8
μ
0.2
0.4
0.6
0.8
μ
0.2
0.4
0.6
0.8
Figure 5.4 Performance for the system identification by the LS 𝛾-filter and the SVM 𝛾-filter for different
orders P. The optimal 𝜇parameter is indicated (◦, □, △).
the experiment was repeated times. Figure .shows the performance criterion
( Jmin = var(en)∕var(yn)) (Principe et al., ) as a function of 𝜇and Q in the test set. In
all cases, the adaline structure (𝜇= ) performed worse than the 𝛾structures, and the
SVM 𝛾-ﬁlter clearly improved the results of the LS and the regularized versions. For all
methods, memory depth M for a ﬁxed Q increased with decreasing 𝜇, especially for the
regularized 𝛾-ﬁlter, but at the expense of poor performance, whereas the SVM 𝛾-ﬁlter
still presented a good trade-oﬀbetween memory depth and performance. Some code
snippets for running the 𝛾-ﬁlter are included in Listing ..
% System
A_true=[1 0.5 0.3]; B_true=[1 0.5];
% Data
x
= randn(100,1); Pn = 0.01;
y
= filter(B_true,[1 A_true],x)+sqrt(Pn)*randn(size(x));
% gamma-SVM parameters
epsilon = 0; gamma = 1e-1; C = 10; p = 5; mu = (.1:.1:1.9);
% Try different mu values
for i = 1:length(mu)
disp([num2str(i) ' de ' num2str(length(mu))]);

230
Digital Signal Processing with Kernel Methods
[a, b, ypred,er(i)] = svm_gamma_wls(x,y,p,mu(i),epsilon,C,gamma);
end
% Get best results
[m,k] = min(er); mimu = mu(k); [a,b,ypred,ee] = ...
svm_gamma_wls(x,y,p,mimu,epsilon,C,gamma);
% Plot figure
figure(1), clf, subplot(211), plot(y), hold on, plot(ypred,'r'),hold ...
off; subplot(212),plot(mu,er);
function [a,b,ypred,er] = svm_gamma_wls(x,y,p,mu,e,C,gam)
%
%
SVM_GAMMA_WLS Support Vector for ARMA modeling with WLS optimization
%
%
Usage: [a b ypred] = svm_gamma_wls(x,y,p,mu,e,C,gam)
%
%
Parameters: x
- Plant input
%
y
- Plant output
%
p
- Order of a
%
mu
- gamma memory
%
e
- Insensitivity on prediction error
%
C
- Upper bound and linear cost param
%
gam
- Quadratic cost param
%
a
b
- Model coeffs
%
ypred
- Predicted output
% Init
y = y(:); x = x(:); N = length(y);
% Construct X matrix and w vector
k0 = p+1; X = []; b = mu; a = [1 -(1-mu)];
xold = x';
for i = 1:p
X = [X;xold]; xold = filter(b,a,xold);
end
X = X(:,k0:end); yold = y; y = y(k0:end);
%%% >>> Insert here your SVM solver: [W,mu] = SVMsolver(X,y)
<<<<
ypred = X'*W; ypred = [zeros(p,1) ; ypred];
er = norm(ypred(p+1:end) - yold(p+1:end));
Listing 5.3 𝛾-filter (gammafilterDemo.m).
5.4.3
Parametric Spectral Density Estimation with Primal Signal Models
Most of the available frequency-domain estimation methods can be grouped into four
main categories: parametric, nonparametric (or periodogram-based), high-resolution
(or subspaces), and time–frequency methods (Marple, ). In parametric methods,
the observed signal is modeled as the output of a linear, time-invariant system that is
driven by white noise, and the coeﬃcients of the linear system are estimated by Yule–
Walker, Burg, or covariance methods (Ljung, ). Parametric spectral methods tend
to produce better PSD estimates than classical nonparametric methods when the signal
length is short, the spectrum is spiky, or some a priori knowledge about the signal is
available.
The most widely used linear system model for parametric spectral estimation is the
all-pole structure (Marple, ). The output yn of such a ﬁlter for a white noise input

A Support Vector Machine Signal Estimation Framework
231
is an AR process of order P, AR(P), which can be expressed as yn = ∑P
p=apyn−p + en,
where ap are the AR parameters and en denotes the samples of the innovation process.
Once the coeﬃcients ap of the AR process are calculated, the PSD estimation is
Py( f ) = 
fs
𝜎
n
|||−∑P
p=ap e−𝕚πpf ∕fs|||
,
(.)
where fs is the sampling frequency and 𝜎
n is the variance of the residuals.
We used data generated as an ARMA process with en given by white Gaussian noise
with zero mean and unit variance. Two systems, previously introduced (Söderström and
Stoica, ), were analyzed; namely, an AR() process, given by
yn = en −.yn−−.yn−−.yn−,
(.)
and a narrow-band ARMA(, ) process, given by
yn = en + .en−+ .en−+ .en−+ .en−
+ .yn−−.yn−+ .yn−−.yn−.
(.)
The input discrete process was an (, ) sequence with sample length L = .
The output signal was corrupted by additive noise (, .), and % of samples were
aﬀected by impulsive noise from a zero mean and unit variance uniform distribution
and randomly placed. These L samples were used for training the model, and 
samples more were used for validation. For all simulations, parameters C and 𝛿were
searched in the range [−, ], and we ﬁxed 𝜀= . The performance criterion used
for the general estimate of PY( f ) was the integrated mean-square error (IMSE), given by
IMSE =

NF
∑NF
f =|PY( f )−̂PY( f )|, where NF is the number of estimated frequencies in
the spectrum. The experiment was repeated times and the best model was selected
according to the estimated IMSE in the validation set.
Figure .illustrates the eﬀect of diﬀerent power of outliers Po on the estimation
accuracy. In both systems, the SVM-AR method outperformed the standard methods
in all situations, with an average gain of .–dB. Diﬀerences between the methods are
lower with increasing noise, specially for the ARMA(, ).
5.4.4
Temporal Reference Array Processing with Primal Signal Models
A linear array of six elements spaced d = .𝜆, was used to detect the signal from a
desired user in the presence of diﬀerent interferences in an environment with Gaussian
noise (Martínez-Ramón et al., ). The desired signal was assumed to experiment
small multipath propagation coming from DOAs −.π and −.π, with amplitudes 
and .respectively and diﬀerent phases. Two interferences signals came from −.π,
.π, and .π, all of them with unit amplitude and phase . The desired signal was
organized in bursts of training samples plus test samples.
The SVM was compared with the standard LS algorithm for array processing. Since
the noise was assumed to be thermal, then its variance could be assumed to be approx-
imately known. In communications, parameter validation is usually not aﬀordable due

232
Digital Signal Processing with Kernel Methods
(a)
6
5
4
3
2
1–16
–12
Yule-Walker
Burg
Covariance
SV-AR
–8
P𝚘 (dB)
–4
0
IMSE (dB)
(b)
23
24
25
26
–16
IMSE (dB)
–12
–8
P𝚘 (dB)
–4
0
Figure 5.5 Evolution of IMSE for different power of outliers for (a) an AR(3) process and (b) an
ARMA(4,4) process.
10–2
10–3
10–4
10–5
0
5
10
Noise power (dB)
15
BER
Figure 5.6 BER performance of the SVM (continuous) and regular LS (dashed) beamformers.
the small amount of available data and the low computational power of systems. There-
fore, parameter 𝛿of the SVM cost function was set to −, and then 𝛿C was adjusted
to the thermal noise standard deviation. Hence, residuals for samples corrupted mainly
by thermal noise were likely in the quadratic cost, and residuals for samples with high
error (when interfering signals added in phase) were likely in the linear cost.
Results are shown in Figure .. Each BER was evaluated by averaging indepen-
dent trials. The LS criterion is highly biased by the non-Gaussian nature of the data
produced by the multipath environment plus the interfering signals. SVM is closer to
the linear optimal and oﬀers a processing gain of several decibels with respect to the LS.

A Support Vector Machine Signal Estimation Framework
233
5.4.5
Sinc Interpolation with Primal Signal Models
As commented in Section .., standard interpolation algorithms can exhibit some
limitations, such as poor performance in low signal-to-noise scenarios or in the pres-
ence of non-Gaussian noise. In addition, these methods result in non-sparse solutions.
These limitations can be alleviated by accommodating the SVM formulation with the
nonuniform sampling problem. In order to benchmark the PSM for sinc interpolation
with methods Y, Y, S, S, and S, which were described in Section .., the PSM
with sinc kernel is considered, and its MATLAB code is in Listing ..
function [predict,coefs] = ...
SVM_Primal(Xtrain,Ytrain,Xtest,T0,gamma,epsilon,C)
% Initials
tk
= Xtrain;
N = length(tk);
Ntest = length(Xtest);
% Construct kernel matrix
S = sinc((repmat(tk,1,N) - repmat(tk',N,1))/T0);
Stest = sinc((repmat(Xtest,1,N) - repmat(tk',Ntest,1))/T0);
H = zeros(N,N);
Htest = zeros(Ntest,N);
for m = 1:N
H(:,m) = sum(S.*repmat(sinc((tk(m)-tk)/T0)',N,1),2);
Htest(:,m) = sum(Stest.*repmat(sinc((tk(m)-tk)/T0)',Ntest,1),2);
end
% Train SVM and predict
inparams = ['-s 3 -t 4 -g ',num2str(gamma),' -c ',num2str(C),' -p ...
',num2str(epsilon)];
[predict,model] = SVM(H,Ytrain,Htest,inparams);
coefs = getSvmWeights(model);
Listing 5.4 SVM-P algorithm (SVM_Primal.m).
In this code, the SVM function is presented in the MATLAB code in Listing .. At
this moment, we postpone to look at the results of this algorithmic implementation until
the following chapters, so that we can have a complete landscape for the comparison.
5.4.6
Orthogonal Frequency Division Multiplexing with Primal Signal Models
In order to test the performance of the OFDM-SVM scheme, detailed in Fernández-
Getino et al. (), a scenario for IEEE .ﬁxed Broadband Wireless Access
Standard has been considered (Erceg et al., ). In second-generation systems for
these types of applications, non-line-of-sight conditions are present. To simulate this
environment, we use the modiﬁed Stanford University interim SUI-channel model
for omnidirectional antennas, with L = taps, a maximum delay spread of 𝜏max = μs,
and maximum Doppler frequency fm = .Hz (Erceg et al., ). The main parameters
of this channel are summarized in Table .. It can be observed that the channel exhibits
an RMS delay spread of 𝜏rms = .μs. Also, it must be noted that K-factors are given
in linear values, and not in decibel values; values shown in Table .mean that % of
the cell locations have K-factors greater than or equal to the K-factor speciﬁed. Finally,
the speciﬁed Doppler is the maximum frequency parameter fm of the round-shaped
spectrum. Additionally, the SUI-channel models specify a normalization factor equal

234
Digital Signal Processing with Kernel Methods
Table 5.3 SUI-3 channel model parameters for multi-
path fading.
Tap 1
Tap 2
Tap 3
Units
Delay

.

μs
Power

−
−
dB
K factor (%)



Linear
Doppler
.
.
.
Hz
to −.dB, which must be added to each tap power to get dB as the total mean
power.
Subsequent distortion as impulse noise is modeled with a BG process (p = .). This
OFDM system consists of N = subcarriers conveying QPSK symbols. We consider
a packet-based transmission, where each packet consists of a header at the beginning of
the packet with a known training sequence or preamble to carry out channel estimation,
followed by a certain number Lx = of OFDM data symbols. At the preamble,
there are two OFDM symbols with Np = pilot subcarriers with randomly generated
symbols. Each OFDM symbol is appended a cyclic preﬁx to overcome the delay spread
𝜏max of the channel. For transmission, we have chosen a channel bandwidth of W =
MHz. Since we sample at the Nyquist rate, this yields a sampling interval Ts = ∕fs =
.μs; this means a length for the cyclic preﬁx LCP of two samples. The total length
of each OFDM symbol becomes (N + LCP) samples. Detection is carried out with a
hard-decision slicer over the equalized data. The code used to generate this data is in
Listing ..
function data=genDataOFDM(conf)
cf=conf.data;
% Training sequence (preamble)
% (orthogonality is imposed with frequency-domain symbols)
preamble=zeros(cf.Npil,cf.Lp);
for kl=1:cf.Lp
preamble(:,kl)=exp(1i*2*pi*rand(cf.Npil,1)); % Phase reference ...
random (PN-seq)
end
% TRANSMITTER
s_bits=round(rand(1,cf.Nb_frm));
% generate random 0/1's
s_trx=modcohe(s_bits,cf.N,cf.PC,cf.M,preamble,cf.Nsymb_frm);
%modulation
% CHANNEL
[~,~,paths]=chan_SUI3(conf.NREPETS,conf.I); % generate channel taps for ...
SUI/3 model (independent channels)
xch=conv(s_trx,paths(:,conf.I)); % convolution with the channel
r_rx_chan=xch(1:length(s_trx));
% fix same length
if cf.sirFlag
% 'normal','only_preamble','only_data'

A Support Vector Machine Signal Estimation Framework
235
r_rx=channelAWGN_IMPULSIVE(r_rx_chan,...
cf.SNR,conf.SIR,cf.p,'only_preamble',cf.Npil);
% IMPULSIVE + AWGN noise
else
r_rx=channelAWGN_IMPULSIVE(r_rx_chan,conf.SNR(rr),conf.
SIR, cf.p);
% IMPULSIVE + AWGN noise
end
% Preamble reception
Lpil=cf.Npil+cf.PC;
for l=1:cf.Lp % take Lp preamble OFDM-symbols and eliminate PC
preamble_rx(:,l) = ...
r_rx((cf.inicSync+cf.PC:cf.inicSync+cf.PC+cf.Npil-1)+
((l-1)*Lpil)).';
end
% For Equalization: ZF
inic_dat=cf.inicSync+(Lpil*cf.Lp); %init of data samples at the frame
for l=1:cf.Nsymb_frm,
s_datos_fd(:,l) = ...
fft(r_rx((inic_dat+cf.PC:inic_dat+cf.PC+cf.N-1)+((l-1)
*cf.L)).');
end
data.Xtrain=preamble; data.Ytrain=preamble_rx;
data.Xtest=s_datos_fd; data.Ytest.input=s_bits; ...
data.Ytest.path=paths(:,conf.I);
Listing 5.5 OFDM data generation (genDataOFDM.m).
Channel Estimation Algorithms
Channel estimation for the coherent demodulation of this OFDM system is performed
with the SVM algorithm (see Listing .). For comparison purposes, LS channel esti-
mates in the frequency domain (after DFT demodulation) are simultaneously obtained
in all cases (see Listing .). In both algorithms, a DFT-based technique with zero-
padding in the time domain is used to interpolate the channel’s frequency response for
data subcarrier positions.
function [predict,coefs] = SVM_OFDM(X,Y,Xtest,N,M,epsilon,gamma,C)
% Initials
[Np,Lp]=size(X);
n = (0:Np-1)';
Y = Np*Y; % Denormalizing
coefs = zeros(Np,Lp);
for i=1:Lp
% Construct kernel matrix
H = zeros(Np);
for j=1:Np
H(:,j)=X(j,i)*exp(1i*2*pi/Np*(j-1)*n);
end
% Train SVM and obtain coefficients
inparams=sprintf('-s 3 -t 4 -g %f -c %f -p %f -j 1', gamma, C, ...
epsilon);
[~,model]=SVM(H,Y(:,i),H,inparams);
coefs(:,i) = getSVs(Y(:,i),model);

236
Digital Signal Processing with Kernel Methods
end
% Channel estimation
ch_est_fd=mean(coefs,2); % mean over Lp OFDM-symbols (only Np subcarriers)
ch_est_td=ifft(ch_est_fd); % per sub-carrier (for N subcarriers)
ch_est_td_N=[ch_est_td(1:3); zeros(N-3,1)];
predict.ch_est_fd=fft(ch_est_td_N);
% Equalization (ZF) and detection
Nsymb_frm=size(Xtest,2);
predict.output=zeros(Nsymb_frm*N*M,1);
for l=1:Nsymb_frm
bits_simbofdm=degray(Xtest(:,l)./predict.ch_est_fd,M);
predict.output((1:N*M)+(l-1)*N*M)=bits_simbofdm;
end
Listing 5.6 SVM-OFDM algorithm (SVM_OFDM.m).
function [predict,coefs] = LS_OFDM(X,Y,Xtest,N,M)
coefs=zeros(size(Y));
for l=1:size(Y,2)
coefs(:,l)=fft(Y(:,l));
end
%channel estimation
ch_est_fd=mean(coefs./X,2); % mean over Lp OFDM-symbols (only Npil ...
subcarriers)
ch_est_td=ifft(ch_est_fd); % per sub-carrier (for N subcarriers)
ch_est_td_N=[ch_est_td(1:3); zeros(N-3,1)];
predict.ch_est_fd=fft(ch_est_td_N);
% Equalization (ZF) and detection
Nsymb_frm=size(Xtest,2);
predict.output=zeros(Nsymb_frm*N*M,1);
for l=1:Nsymb_frm
bits_symbofdm=degray(Xtest(:,l)./predict.ch_est_fd,M);
predict.output((1:N*M)+(l-1)*N*M)=bits_symbofdm;
end
Listing 5.7 LS-OFDM algorithm (LS_OFDM.m).
Experiment Settings and Results
The signal-to-impulse ratio (SIR) is deﬁned as SIR = 𝔼[rn −zn]∕𝔼[gn], and it ranged
from around −to +dB. The BER and the MSE improvements that can be attained
with SVM are represented in Figure .. For high SIR values, a performance similar to
LS can be obtained, whereas for low SIR values the SVM outperforms LS by properly
choosing C, 𝛿C, and 𝜀.
The diﬀerence in BER between SVM and LS was analyzed, given the deﬁnition of
Δ log(BER) = log(BERSVM) −log(BERLS), in order to easily detect working zones
where SVM works better (Δ log(BER) > ), similar (Δ log(BER) = ) or worse
(Δ log(BER) < ) than LS. Figure .shows that the BER performance of the SVM
algorithm can be superior to LS for properly chosen values of the free parameters, and

A Support Vector Machine Signal Estimation Framework
237
–18
–20
–22
–24
–26
–28
–30
–20
–15
–10
–5
0
SIR (dB)
MSE (dB)
LS
OFDM–SVM
5
10
15
20
–20
10–3
10–2
–15
–10
–5
0
SIR (dB)
BER
5
10
15
20
Figure 5.7 Performance of OFDM-SVM versus LS. BER and MSE performance as a function of SIR.
that there are a wide range of values for it. All the these conﬁguration parameters are
deﬁned in the MATLAB ﬁle conﬁg_OFDM.m available in the simpleInterp toolbox.
In code conﬁg_OFDM.m, BER and MSE evaluation measures are calculated as seen in
Listing ..
function values = BER_and_MSE(ypred,y)
% BER
[M,Nb_frm]=size(y.input);
values.ber=zeros(M,1);
for kt=1:M
values.ber(kt,1)=sum(y.input(kt,:)~=ypred.output(kt,:))/
Nb_frm;
end
% MSE
N=length(ypred.ch_est_fd);
L=length(y.path);
% frequency domain
values.mse_fd=mean(abs(ypred.ch_est_fd-fft(y.path,N)).^2);
% temporal domain
ch_est_td=ifft(ypred.ch_est_fd);
values.mse_td=mean(abs(ch_est_td(1:L)-y.path).^2);
Listing 5.8 BER and MSE evaluation functions (BER_and _MSE.m).

238
Digital Signal Processing with Kernel Methods
2
–2
–1.5
–1
Δ log (BER)
–0.5
0
1.5
1
0.5
20
10
0
–10
SIR (dB)
log (C)
–20
Δlog (BER)
201
0.5
ɛ
0
10
0
–10
SIR
–20
–0.6
–0.5
–0.4
–0.3
–0.2
–0.1
0
Figure 5.8 Performance of OFDM-SVM with free parameters, compared with LS in terms of Δ log(BER),
for SIR versus C (up), and for SIR versus 𝜀(down).
5.5
Questions and Problems
Exercise ..
Write the time-transverse vectors for nonparametric spectral analysis,
for ARX system identiﬁcation, for OFDM robust estimation, and for sinc interpolation.
Exercise ..
Show the form of the autocorrelation matrix in the dual problem for
the OFDM complex formulation.
Exercise ..
Write the signal model, transverse functions, autocorrelation, and
solution for a time-series expansion given by a triangular-shaped kernel.

A Support Vector Machine Signal Estimation Framework
239
Exercise ..
Make a spectral analysis with sunspot numbers and nonparametric
spectral analysis with the PSM. Represent the residuals, the multipliers, and the corre-
lation matrix for that problem.
Exercise ..
Make the spectral analysis now with AR decomposition of the signals.
Represent the Lagrange multipliers and the coeﬃcients. Represent graphically the
matrix and study its main properties.
Exercise ..
Make an interpolation of sunspot numbers with the sinc kernel. Make
the spectral representation. Represent the Lagrange multipliers, the coeﬃcients, the
matrix, and the error.
Exercise ..
Reproduce the example of Section ..but using spatial reference.
Construct a set of example signals coming from the two diﬀerent desired directions of
arrival, and synthesize a set of incoming signals including the interferences. Compare
the results as a function of the number of example signals. Represent the spectrum of
the resulting primal weight vectors.
Exercise ..
Work the code for showing the main results on the 𝛾-ﬁlter application
example, on the array with spatial reference example, and on the array with temporal
reference example.

241
6
Reproducing Kernel Hilbert Space Models for Signal Processing
6.1
Introduction
In this chapter we introduce a set of signal models properly deﬁned in an RKHS. The set
of processing algorithms presented here are collectively termed an RSM because they
share a distinct feature; namely, all of them intrinsically implement a particular signal
model (like those previously described in Chapter ) whose equations are written in
the RKHS generated with kernels. For doing that, we exploit the well-known kernel
trick (Schölkopf and Smola, ), and this is the most usual approach to kernel-
based signal processing problems in the literature. Quite often one is interested in
describing the signal characteristics by using a speciﬁc signal model whose performance
is hampered by the (commonly strong) assumption of linearity. As we already know,
the use of the theory of reproducing kernels can circumvent this problem by deﬁning
nonlinear algorithms by simply replacing dot products in the feature space by an
appropriate Mercer kernel function (Figure .). The most famous example of this kind
of approaches is the support vector classiﬁcation (SVC) algorithm, which has yielded a
vast number of applications in the ﬁeld of signal processing, from speech recognition
to image classiﬁcation.
We should note ﬁrst that nonlinear SVM for DSP algorithms can be obtained from
nonlinear versions of linear algorithms presented in Chapter . However, nonlinear
SVM for DSP algorithms can be developed from two diﬀerent general approaches,
which are presented in this chapter and in Chapter . In this chapter, we will focus
on a class of SVM for DSP algorithms that consists of stating the signal model of the
time-series structure in the RKHS, and hence they are called RSM algorithms. Several
examples of SVM-DSP in this setting are nonlinear system identiﬁcation and antenna
array processing. These and some other examples are summarized in this chapter, both
in terms of theoretical foundations and of practical applications.
In particular, the chapter pays attention to the fundamental elements of the RSM
approach. We concentrate on particular signal structures which have been previously
studied. After their deﬁnition, nonlinear versions of the signal models will be developed.
Speciﬁcally, the chapter is devoted to the study of:
●nonlinear ARX system identiﬁcation techniques;
●nonlinear FIR and 𝛾-ﬁlter structures;
●array processing, both with temporal and spatial reference;
●semiparametric regression (SR).
Digital Signal Processing with Kernel Methods, First Edition. José Luis Rojo-Álvarez, Manel Martínez-Ramón,
Jordi Muñoz-Marí, and Gustau Camps-Valls.
© John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.

242
Digital Signal Processing with Kernel Methods
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
0
0.2
0.4
0.6
0.8
yn−1
yn−2
−2
−1
0
1
2
0
0.5
1
1.5
0
0.2
0.4
0.6
0.8
xn−1
xn−2
Figure 6.1 In SVM estimation problems, a nonlinear relationship between data points in the input
space is transformed into a linear relationship between mapped data into the (higher dimensional)
RKHS. Different signal model equations can be used in the RKHS, as far as the problem statement is
expressed in terms of dot products.
Examples are used to consolidate the main concepts (nonlinear system identiﬁcation),
and application examples are further provided (electric network modeling and array
structures from communication systems).
6.2
Reproducing Kernel Hilbert Space Signal Models
The background for stating signal models in the RKHS is well established, and it has
been widely used in the kernel literature. In this section, we limit ourselves to stating
a general signal model for estimation with discrete-time series notation, which will

Reproducing Kernel Hilbert Space Models for Signal Processing
243
be useful for immediately summarizing the relevant elements of three SVM for DSP
algorithms in this setting, following the framework in Rojo-Álvarez et al. (). On the
one hand, nonlinear ARX system identiﬁcation and 𝛾-ﬁlter system identiﬁcation use a
signal model equation in the RKHS relating an exogenous time series and an observed
data time-series. On the other hand, antenna array processing with spatial reference
uses an energy expression in the RKHS, together with complex-valued algebra.
Theorem ..(RSM problem statement)
Let {yn} be a discrete-time series, whose
signal model to be estimated can be expressed in the form of a dot product of a weight
vector a and a vector observation for each time instant vn; that is:
̂yn = ⟨a, vn⟩+ b,
(.)
where b is a bias term that often will be convenient, given that an unbiased estimator
in the input space will not necessarily correspond to an unbiased estimator when
formulated in the RKHS. Then, a nonlinear signal model can be stated by transforming
the weight vector and the input vectors at time instant n to an RKHS:
̂yn = ⟨w, 𝝋(vn)⟩+ b,
(.)
where the same signal model is now used with weight vector w = ∑N
n=𝜂n𝝋(vn), and the
solution can be expressed as
̂ym =
N
∑
n=
𝜂n⟨𝝋(vn), 𝝋(vm)⟩=
N
∑
n=
𝜂nK(vn, vm).
(.)
The proof is straightforward (and similar to the demonstration for deriving the PSM),
by using the conventional Lagrangian functional and dual problem. This is the mostly
used approach to tackle nonlinear problems with kernels in general and to tackle signal
processing problems with SVM in particular. This theorem is next used to obtain the
nonlinear equations for several DSP problems. Before doing that, we summarize two
relevant properties that will be useful for further developments.
A concept that has been largely used in RKHS in SVM for DSP is the composite
kernel, which will give ﬂexibility for deﬁning relationships between two observed signals
(exogenous and system output), by means of an RKHS system identiﬁcation model.
Property (Composite summation kernel)
A simple composite kernel comes from
the concatenation of nonlinear transformations of c ∈ℝc and d ∈ℝd. If we construct
the transformation
𝝓(c, d) = [𝝓T
(c), 𝝓T
(d)]T
(.)
as the concatenation of (column) vectors 𝝓(c) and 𝝓(d) deﬁned in and ,
respectively, then the corresponding dot product between vectors is simply
K(c, d; c, d) = ⟨𝝓(c, d), 𝝓(c, d)⟩= K(c, c) + K(d, d),
(.)
which is known as summation kernel.

244
Digital Signal Processing with Kernel Methods
Note that this measure of similarity between diﬀerent data observations is diﬀerent
to the traditional stacked approach in the input space. The common approach when
dealing with diﬀerent data entities, variables, or observations, here c and d, is to
concatenate them in the input space and then to deﬁne a mapping function 𝝓and its
corresponding kernel function to work with them. This approach has been widely used
in time series analysis, as we will see in Section ... By doing so, however, one loses
the signal model structure.
An important second diﬀerence is that stacking vectors in feature spaces leads to
considering dedicated kernels to each variable separately, whose similarity functions
are then summed up. The model is limited in the sense that variable cross-relations are
not considered. The summation kernel expression can be readily modiﬁed to account for
the cross-information among the diﬀerent variables, in our case between an exogenous
and an output observed data time series.
Property (Composite kernels for cross-information)
Assume a nonlinear map-
ping 𝝋(⋅) into a Hilbert space and three linear transformations Ai from to i,
for i = , , . We construct the following composite vector:
𝝓(c, d) = [A𝝋(c), A𝝋(d), A(𝝋(c) + 𝝋(d))]T.
(.)
If the dot product is computed, we obtain
K(c, c; d, d) = 𝝋T(c)R𝝋(c) + 𝝋T(d)R𝝋(d) + 𝝋T(c)R𝝋(d)
+ 𝝋T(d)R𝝋(c) = K(c, c) + K(d, d) + K(c, d) + K(d, c),
(.)
where R= AT
A+ AT
A, R= AT
A+ AT
A, and R= AT
Aare three independent
deﬁnite-positive matrices.
Note that c and d must have the same dimension for the formulation to be valid,
otherwise Kcannot be computed.
6.2.1
Kernel Autoregressive Exogenous Identification
As we have seen in the introductory chapters, a common problem in DSP is to
model a functional relationship between two simultaneously recorded discrete-time
processes (Ljung, ). When this relationship is linear and time invariant it can be
addressed by using an ARMA diﬀerence equation, and when a simultaneously observed
set of signal samples {xn} is available, called an exogenous signal, the parametric model
is called an ARX signal model for system identiﬁcation.
Many approaches have been considered to tackle this important problem of ARX
system identiﬁcation. General nonlinear models, such as artiﬁcial neural networks,
wavelets, and fuzzy models, are common and eﬀective choices (Ljung, ; Nelles,
), though their temporal structure cannot be easily analyzed, because it remains
inside a black-box model. In the last decade, enormous interest has been paid to
kernel methods in general and SVM in particular in this setting. The ﬁrst approaches
considered the SVM version for regression, the so-called SVR method. In particular,
Drezet and Harrison (), Goethals et al. (b), and Espinoza et al. () used the
SVR algorithm for nonlinear system identiﬁcation. However, in all these studies the time

Reproducing Kernel Hilbert Space Models for Signal Processing
245
series structure of the data was not scrutinized and the approach essentially consisted
of stacking the signals that were then fed to the SVR. Alternatively, Rojo-Álvarez
et al. () explicitly formulated SVM for modeling linear time-invariant ARMA
systems (linear SVM-ARMA), and this kind of formulation has been recently extended
to a general framework for linear signal processing problems (Rojo-Álvarez et al.,
). However, if linearity cannot be assumed, then nonlinear system identiﬁcation
techniques are required.
We next summarize several SVM for DSP procedures for nonlinear system identi-
ﬁcation. The material has been presented in detail by Martínez-Ramón et al. (),
Camps-Valls et al. (b), and Camps-Valls et al. (a), so we summarize the
most relevant details here. First, the stacked SVR algorithm for nonlinear system
identiﬁcation is brieﬂy examined in order to check that, though eﬃcient, this approach
does not correspond explicitly to an ARX model in the RKHS.
Let {xn} and {yn} be two discrete-time signals, which are the input and the output
respectively of a nonlinear system. Let yn = [yn−, yn−, … , yn−M]T and xn = [xn, xn−, …,
xn−Q+]T denote the states of input and output at time instant n. The stacked-kernel
system identiﬁcation algorithm (Gretton et al., a; Suykens et al., a) can be
described as follows.
Property (Stacked-kernel signal model in SVM for nonlinear system identiﬁcation)
Assuming a nonlinear transformation 𝝓([yT
n, xT
n]) for the concatenation of the input and
output discrete-time processes to a B-dimensional feature space, 𝜙∶ℝM+Q →, a
linear regression model can be built in , its corresponding equation being
yn = ⟨w, 𝝓([yT
n, xT
n])⟩+ en,
(.)
where w is a vector of coeﬃcients in the RKHS, given by
w =
N
∑
n=
𝜂n𝝓([yT
n, xT
n]),
(.)
and the following Gram matrix containing the dot products can be identiﬁed:
G(m, n) = ⟨𝝓([yT
m, xT
m]), 𝝓([yT
n, xT
n]) = K([yT
m, xT
m], [yT
n, xT
n]),
(.)
where the nonlinear mappings do not need to be explicitly computed, but instead the
dot product in the RKHS can be replaced by Mercer kernels. The predicted output for
newly observed [yT
m, xT
m] is given by
̂ym =
N
∑
n=
𝜂nK([yT
m, xT
m], [yT
n, xT
n]).
(.)
Note that this is the expression for a general nonlinear system identiﬁcation, but it
does not correspond to an ARX structure in the RKHS. Moreover, though the reported
performance of the algorithm is high when compared with other approaches, this
formulation does not allow us to scrutinize the statistical properties of the time series

246
Digital Signal Processing with Kernel Methods
that are being modeled in terms of autocorrelation and/or cross-correlation between
the input and the output time series.
Composite kernels can be introduced at this point, allowing us to next introduce a
nonlinear version of the linear SVM-ARX algorithm by using actually an ARX scheme
on the RSM. After noting that the cross-information between the input and the output
is lost with the stacked-kernel signal model, the use of composite kernels is proposed
for taking into account this information and for improving the model versatility.
Property (SVM-DSP in RKHS for ARX nonlinear system identiﬁcation)
If we
separately map the state vectors of both the input and the output discrete-time signals
to , using a nonlinear mapping given by 𝝓e(xn) ∶ℝM →e and 𝝓d(yn) ∶ℝQ →d,
then a linear ARX model can be stated in , the corresponding diﬀerence equation
being given by
yn = ⟨wd, 𝝓d(yn)⟩+ ⟨we, 𝝓e(xn)⟩+ en,
(.)
where wd and we are respectively the vectors determining the AR and the MA
coeﬃcients of the system in (possibly diﬀerent) RKHSs. The vector coeﬃcients are
given by
wd =
N
∑
n=
𝜂n𝝓d(yn),
we =
N
∑
n=
𝜂n𝝓e(xn).
(.)
Two diﬀerent kernel functions can be further identiﬁed:
Ry
m,n = ⟨𝝓d(ym), 𝝓d(yn)⟩= Kd(ym, yn)
(.)
Rx
m,n = ⟨𝝓e(xm), 𝝓e(xk)⟩= Ke(xm, xn),
(.)
which account for the sample estimators of input and output time-series autocorrelation
functions (Papoulis, ) respectively in the RKHS. Speciﬁcally, they are proportional
to the non-Toeplitz estimator of each time series autocorrelation matrix.
The dual problem consists of maximizing the PSM dual problem (see Equation .)
with Rs = Rx + Ry, and the output for a new observation vector is obtained as
̂ym =
N
∑
n=n
𝜂n(Kd(yn, ym) + Ke(xn, xm)).
(.)
The kernels in the preceding equation correspond to correlation matrices computed
into the direct summation of kernel spaces and . Hence, the autocorrelation
matrices’ components given by xn and yn are expressed in their corresponding RKHS
and the cross-correlation component is computed in the direct summation space. A
third space can be used to compute the cross-correlation component, which introduces
generality to the model.
Property (Composite kernels for general cross-information in system identiﬁcation)
Assuming a nonlinear mapping 𝝓(⋅) into a Hilbert space and three linear transforma-
tions Ai from to i, for i = , , , we can construct the following composite vector:

Reproducing Kernel Hilbert Space Models for Signal Processing
247
𝝓(y, x) =
⎛
⎜
⎜⎝
A𝝓(x)
A𝝓(y)
A(𝝓(x) + 𝝓(y))
⎞
⎟
⎟⎠
According to Property , we have
K(ym, yn; xm, xn) = 𝝓T(ym)R𝝓(yn) + 𝝓T(ym)R𝝓(xn)
+ 𝝓T(xm)R𝝓(yn) + 𝝓T(xm)R𝝓(yn)
= K(ym, yn) + K(xm, xn) + K(ym, xn) + K(xm, yn),
where it is straightforward to identify K= Kd and K= Ke.
Note that, in this case, xn and yn need to have the same dimension, which can be
naively accomplished by zero completion of the embeddings.
Property (General composite kernels)
A general composite kernel, which can be
obtained as a combination of the previous ones, is given by
K(xm, ym; yn, xn) = K(ym, yn) + K(xm, xn)
+ K(ym, xn) + K(xm, yn) + K(zm, zn).
(.)
Therefore, despite the fact that SVM-ARX and SVR nonlinear system identiﬁcations
are diﬀerent problem statements, both models can be easily combined.
6.2.2
Kernel Finite Impulse Response and the γ-filter
As described in previous chapters, many NN structures with a linear memory stage
followed by a non-linear memoryless stage are commonly used in signal processing,
such as the time delay NN and the focused 𝛾-network. These networks oﬀer good
performance at the expense of increasing the dimensionality of the state vector of the
linear memory stage, and thus training the memoryless stage involves both high com-
putational burden and risk of overﬁtting. In Camps-Valls et al. (b), a set of kernel
methods are introduced in order to develop nonlinear 𝛾-ﬁlters in a straightforward
yet principled way. These RKHS algorithms for nonlinear system identiﬁcation with
𝛾-ﬁlters are summarized next.
Property (Nonlinear 𝛾-ﬁlter with the kernel trick)
Let us express the SVM 𝛾-ﬁlter
PSM as
ym =
N
∑
n=
𝜂n
Q
∑
q=
⟨sq
n, sq
m⟩.
(.)
Then, the delayed line outputs sq
n can now be transformed to an RKHS using a nonlinear
mapping 𝝓. The autocorrelation function is Rs
n,m
∶= ∑Q
q=⟨𝝓(sq
n), 𝝓(sq
m)⟩, and the
solution becomes inherently nonlinear:

248
Digital Signal Processing with Kernel Methods
̂ym =
N
∑
n=
𝜂n
Q
∑
q=
⟨𝝓(sq
n), 𝝓(sq
m)⟩=
N
∑
n=
𝜂n
Q
∑
q=
K(sq
m, sq
n).
(.)
This property can be readily used to derive the nonlinear 𝛾-ﬁlters with composite
kernels and with tensor-product kernels, as detailed and benchmarked in Camps-Valls
et al. (b).
6.2.3
Kernel Array Processing with Spatial Reference
An antenna array is a group of (usually identical) electromagnetic radiator elements
placed in diﬀerent positions of the space. This way, an electromagnetic ﬂat wave
illuminating the array produces currents that have diﬀerent amplitudes and phases
depending on the DOA of the wave and of the position of each radiating element. The
discrete-time signals collected from the array elements can be seen as a time and space
discrete process.
The fundamental property of the array is that it is able to detect the DOA of one or
several incoming signals or it is able to discriminate one among various incoming signals
provided they have diﬀerent DOAs (Van Trees, ). Applications of antenna array
processing range from radar systems (which minimize the mechanical components
by electronic positioning of the array radiation beam), to communication systems
(in which the system capacity is increased by the use of spatial diversity), and to
radioastronomy imaging systems, among many others.
The array processing problem stated in Equation .can be solved when there are
no training symbols available, but just a set of incoming data and information about the
angle of arrival of the desired user. In this case, the algorithm to be applied consists of
a processor that detects without distortion (distortionless property) the signal from the
desired DOA while minimizing the total output energy. The signal can be easily mapped
to an RKHS, and the algorithm must optimize
min
w {wH𝝓(xn)𝝓(xn)Hw} = min
w {wHRw} ≈min
w {wH𝜱𝜱Hw}
(.)
for a given set of previously collected snapshots, and 𝜱is a matrix containing all mapped
snapshots 𝝓(xn).
Property (Spatial reference signal model in RKHS)
In order to introduce the afore-
mentioned distortionless property, constraints must be applied to a set of canonical
signals whose steering vector in Equation .contains the desired direction of arrival
𝜃, carrying a set of symbols bi. These are the spatial reference signals. The reference
signal model is then
bi = wH𝝓(bia) −b,
(.)

Reproducing Kernel Hilbert Space Models for Signal Processing
249
where ais the steering vector corresponding to the desired signal. Then, a primal
functional must contain the following constraints:
Re{bi −wH𝝓(bia) −b} ≤𝜀+ 𝜉i
−Re{bi −wH𝝓(bia) −b} ≤𝜀+ 𝜉′
i
Im{bi −wH𝝓(bia) −b} ≤𝜀+ 𝜁i
−Im{bi −wH𝝓(bia) −b} ≤𝜀+ 𝜁′
i
(.)
with si being all possible transmitted symbols in a given amplitude range, and 𝜉i, 𝜁i, 𝜉′
i,
and 𝜁′
i being the slack variables corresponding to the real and imaginary constraints.
Property (Spatial reference primal coeﬃcients)
An SVM procedure applied to this
constrained optimization problem must include the minimization of Equation ., and
it gives the solution
w =
∑
i
R−𝝓(bia)𝜓i.
(.)
Property (Spatial reference kernel)
The application of Equation .in Equa-
tion .implicitly gives the kernels
K(bia, bja) = 𝝓(bia)TR−𝝓(bja).
(.)
These kernels cannot be directly used because an expression for R is not available in an
inﬁnite-dimension RKHS. A kernel eigenanalysis introduced by Schölkopf et al. ()
leads to the kernel expression
K(bia, bja) = N𝝓(bia)T𝜱K −K 𝜱T𝝓(bja),
(.)
where 𝜱is a matrix containing all the incoming data used to compute the autocorrela-
tion matrix R, and K is a kernel matrix containing all dot products 𝝓(s
na)T𝝓(s
ma).
These kernels can be used to solve a dual problem equal to the one of Property . The
primal coeﬃcients can be expressed as
w = N𝜱K −K 𝝍,
(.)
where 𝝍n = 𝜂n + 𝕚𝜈n are complex-valued dual coeﬃcients.
6.2.4
Kernel Semiparametric Regression
SR has been a widely studied topic in conventional statistics. It supports the idea that
some phenomena under analysis can be represented with parametric models, especially
using linear regression, and by nonparametric models, when the explicit relationship
between the input and the output turns to be more complicated to know. Given that the
knowledge path can be thought of as usually going from nonparametric to parametric
models, SR combines both approaches. The most widely known method for SR is the

250
Digital Signal Processing with Kernel Methods
Nadayara–Watson (NW) estimator. In this section, we start deﬁning the fundamentals
of this classical estimator, and then we show how it can be readily expressed in terms of
a composite kernel in the RSM framework.
In addition, we introduce in this chapter the use of bootstrap resampling techniques
(BRTs) in SVM for RSM models. We focus on the role that it has played for model
diagnosis and for analyzing the statistics of the model parameters, specially for SR.
This section paves the way toward the use of these popular nonparametric methods for
working with conﬁdence intervals and for establishing statistical tests in this setting.
Nadayara–Watson Estimator for Semiparametric Regression
Let yi be the ith observation of a response variable and xi = [, x
i, … , xK
i ]T be the
K-dimensional ith vector containing the observed predictor variables (i = , … , N),
where T denotes the transposed vector. The constant unit level is introduced to take
into account the interception component.
The simplest parametric regression model is the general linear model:
𝔼[yi|xi] =
K
∑
k=
𝛽kxk
i = ⟨𝜷, xi⟩,
(.)
where 𝔼[⋅|⋅] denotes conditional statistical expectation, and ⟨⋅, ⋅⟩is the dot product.
Model coeﬃcients 𝛽, … , 𝛽K, and interception 𝛽, are estimated by ordinary LS.
Nonparametric kernel regression (Ruppert et al., ) computes a local weighted
average of the criterion variable given the values of the predictors; that is:
𝔼[yi|xi] = m(xi),
(.)
where m(⋅) is a nonparametric function. For instance, the NW estimator consists of a
constant-kernel approximation given by
m(xi) =
N
∑
t=
wt(xi)yi,
(.)
with
wt(xi) =
K
(
xi−xt
𝜎
)
∑N
s=K
(
xi−xs
𝜎
)
(.)
and
K(x) = (π)K∕
K
∏
i=
exp
(
−x
i

)
.
(.)
Parameter 𝜎is called the bandwidth, and it represents the neighborhood of inﬂuence
for each observation. A smaller bandwidth will give a lower bias but increased variance

Reproducing Kernel Hilbert Space Models for Signal Processing
251
in the estimator, whereas a larger bandwidth will produce higher bias (model mismatch)
despite a reduced variance estimator.
The SR model can be very useful when mixed nature predictor variables are available
(for instance, metric and dichotomic), because diﬀerent components can be modeling
the contribution of each subset of variables. Also, sometimes we have some a priori
knowledge about the model that we can assume for a subset of variables, but the
remaining subset has a completely unknown nature. Without loss of generality, let us
assume observation vectors that are composed of D dichotomic variables and M metric
variables, xi = [xmT
i
, xdT
i ]T. Let us use a parametric, linear component for the dichotomic
variables, and a nonparametric, nonlinear component for the metric variables. The
corresponding SR model is
𝔼[yi|xi] = m(xm
i ) + ⟨𝜷, xd
i ⟩.
(.)
The estimation method is described in Heerde et al. () and Lee and Nelder (),
and it is brieﬂy presented here. The model can be written down as
yi = m(xm
i ) + ⟨𝜷, xd
i ⟩+ ei,
(.)
where ei denotes the ith residual. By taking the conditional average with respect to the
parametric variables, we obtain
𝔼[yi|xm] = m(xm
i ) + ⟨𝜷, 𝔼[xd
i |xm]⟩.
(.)
Then, by subtracting Equations .and ., we have
yi = 𝔼[yi|xm] + ⟨𝜷, (xd
i −𝔼[xd
i |xm])⟩+ ei.
(.)
Therefore, a three-step procedure can be stated. First, averages conditional to the
nonparametric variables are estimated for the response variable and for the parametric
variables, given by
̂𝔼[yi|xm] =
∑
j≠i K[(xm
j −xm
i )∕𝜎]yj
∑
j≠i K((xm
j −xm
i )∕𝜎) ,
(.)
̂𝔼[xd
i |xm] =
∑
j≠i K((xm
j −xm
i )∕𝜎)xd
j
∑
j≠i K[(xm
j −xm
i )∕𝜎] .
(.)
Note that bandwidths 𝜎and 𝜎must be properly chosen for the statistical estimation.
Second, the new response variable ̃y = y −̂𝔼[y|xm] and the new predictor variables
̃xd = xd −̂𝔼[xd|xm] are used to ﬁnd ̃𝜷by solving
̃yi = ⟨̃𝜷, ̃xd
i ⟩+ ei
(.)

252
Digital Signal Processing with Kernel Methods
by means of ordinary LS. Note that the intercept term disappears at this step because of
the mean subtraction, so that it remains inside the nonparametric component. Third,
the nonparametric component is obtained by nonparametric regression on ̃y; that is:
̂m(xm
i ) =
∑N
j=K((xm
i −xm
j )∕𝜎)̃yj
∑N
j=K[(xm
i −xm
j )∕𝜎]
,
(.)
where bandwidth 𝜎must be also properly ﬁxed. The choice of three diﬀerent band-
widths must be addressed, with two of them coming from a nonparametric estimation
of the pdf and the third one coming from the nonparametric regression process. Several
bandwidth selection techniques are available, among which are Lee’s rule of thumb,
cross-validation, or subjective approach (Heerde et al. ; Lee and Nelder ).
Some limitations of the procedure could be present according to this formulation:
●Automatic selection procedures can deteriorate when low-sized data sets are
analyzed.
●The denominators in Equations ., ., and .can become very small for a
newly tested point that is far enough from the observations, and this produces an
unbounded predicted output. Again, this situation can be more present when reduced
data sets are under analysis. A solution for this drawback can be the introduction
of a small threshold parameter in the exponential exponent, which is, in fact, a
regularization procedure. This threshold should be also found as a free parameter.
●Finally, all the available observations are used for building the solution in Equa-
tion ., independently of their adequacy or noise level. This solution will not be
operational for studies with high number of observations.
These limitations can be alleviated by the SVM formulation for SR, which is presented
next.
Semiparametric Regression Approach using the Support Vector Machine
An alternative formulation of SR can be stated by using the RSM framework. Let us
assume that the model can be expressed with two additive contributions: one from the
parametric and another from the nonparametric predictor variables. Let us assume also
that there exists a possibly nonlinear transformation of the metric predictor variables
into a higher (possibly inﬁnite) dimensionality space, 𝝋(xm) ∶ℝM →m, where m
is known as the feature space for the metric model component. The main property
of this nonlinear transformation is that a linear regression operator can be found
in the feature space, given by vector wm ∈m. Let us ﬁnally assume that, for the
dichotomic model component, there exists another transformation of the dichotomic
vectors 𝝓(xd) ∶ℝD →d to a diﬀerent feature space d where another linear regression
operator vd ∈d can be properly adjusted.
In these conditions, the joint regression model is given by
yi = ⟨wm, 𝝋(xm
i )⟩+ ⟨vd, 𝝓(xd
i )⟩+ br + ei.
(.)
Here, the reference interception term br can be previously ﬁxed, in order to make
easy the comparison to a given level. For instance, br could be the average sales level

Reproducing Kernel Hilbert Space Models for Signal Processing
253
constrained to nonpromotional periods, so that the resulting model will provide us with
information about the predictors either increasing or decreasing that level.
The SVM-SR algorithm is stated as the minimization of the 𝜖-Huber cost plus a
regularization term given by the 𝓁norm of the weights in the feature spaces; that is:

(‖wm‖+ ‖vd‖) + 
𝛾
∑
i∈I
(𝜉
i + 𝜉∗
i
) + C
∑
i∈I
(𝜉i + 𝜉∗
i ) −
∑
i∈I
𝛾C

(.)
constrained to
yi −⟨wm, 𝝋(xm
i )⟩−⟨vd, 𝝓(xd
i )⟩−br ≤𝜀+ 𝜉i,
(.)
−yi + ⟨wm, 𝝋(xm
i )⟩+ ⟨vd, 𝝓(xd
i )⟩+ br ≤𝜀+ 𝜉∗
i ,
(.)
𝜉i, 𝜉∗
i ≥,
(.)
where 𝜉i and 𝜉∗
i (in the following, denoted jointly by 𝜉(∗)
i ) are the slack variables used to
account for the residuals in the model; Iis the set of samples for which 𝜀≤𝜉(∗)
i
≤eC,
and Iis the set of samples for which 𝜉(∗)
i
> eC. Following the usual SVM formulation
methodology, Lagrangian functional can be written down, and by making zero its
gradient with respect to the primal variables we obtain
∇wm= ⇒wm =
N
∑
i=
(𝛼i −𝛼∗
i )𝝋(xm
i ),
(.)
∇vd= ⇒vd =
N
∑
i=
(𝛼i −𝛼∗
i )𝝓(xd
i ),
(.)
𝜕
𝜕𝜉(∗)
i
= ⇒≤𝛼(∗)
i
≤C,
(.)
with 𝛼i and 𝛼∗
i denoting the Lagrange multipliers that correspond to Equation .and
Equation .respectively. Matrix notation is introduced as follows:
y = [y, … , yN]T,
(.)
𝜶(∗) = [𝛼(∗)
, … , 𝛼(∗)
N ]T,
(.)
Mij = ⟨𝝋(xm
i ), 𝝋(xm
j )⟩,
(.)
Dij = ⟨𝝓(xd
i ), 𝝓(xd
j )⟩.
(.)
Finally, the dual problem can be stated (Rojo-Álvarez et al., ) as the maximization
of
−
(𝜶−𝜶∗)T(M + D + 𝛾I)(𝜶−𝜶∗)
+(𝜶−𝜶∗)T(y −br) −𝜀(𝜶+ 𝜶∗)T
(.)

254
Digital Signal Processing with Kernel Methods
constrained to Equation ., with respect to dual variables 𝛼(∗)
i . After this quadratic
programming problem is solved, and according to Equations ., ., and .,
the ﬁnal expression of the solution can be easily shown to be given by the following
expression:
̂y =
N
∑
i=
𝜂i(⟨𝝋(xm
i ), 𝝋(xm)⟩+ ⟨𝝓(xd
i ), 𝝓(xd)⟩) + br,
(.)
where 𝜂i = (𝛼i −𝛼∗
i ). Note that the estimated response variable ̂y is calculated from a
weighted expansion of dot products in the feature spaces. With this expression for the
solution, the explicit calculation of wm and vd is not required.
Here, we are mainly concerned about two properties of Mercer kernels: () the sum
of two Mercer kernels is a Mercer kernel; and () the product of a Mercer kernel times
a positive constant is a Mercer kernel. These simple properties allows us to propose the
use of a scaled linear kernel that generates the parametric component:
⟨𝝓(xd
i ), 𝝓(xd
j )⟩= Kl(xm
i , xd
j ) = 𝛿⟨xd
i , xd
j ⟩,
(.)
with 𝛿∈ℝ+, plus a nonlinear kernel that generates the nonparametric component:
⟨𝝋(xm
i ), 𝝋(xm
j )⟩= Knl(xm
i , xm
j ).
(.)
Constant 𝛿can be chosen for giving a balance between the parametric and nonpara-
metric components. Thus, the ﬁnal solution of SVM-SR can be readily expressed as
̂y =
N
∑
i=
𝜂iKnl(xm
i , xm) + 𝛿⟨𝜷, xd⟩+ br.
(.)
Taking Equations .and .into account, coeﬃcients 𝜂i determine completely both
the parametric and the nonparametric components.
Bootstrap Resampling for Model Diagnosis
One of the main limitations of current SR methods is the diﬃculty in establishing
clear cut-oﬀtests for the nonparametric variables of the model, and much eﬀort is
being done in this framework (Ruppert et al., ). Also, this aspect has not yet been
completely solved in the SVM literature, and systematic procedures for establishing
feature selection, signiﬁcance levels, and conﬁdence intervals (CIs) for model diagnosis
have been developed (Lal et al., ). An interesting approach to the model diagnosis
and feature selection issues in SVM for SR can be given by BRTs, which were ﬁrst
proposed as possibly nonparametric procedures for estimating the pdf of an estimator
from a limited, but informative enough, set of observations (Efron and Tibshirani, ).
BRTs have been successfully used before for ﬁxing the free parameters of SVM classiﬁers
(Rojo-Álvarez et al., ) and as a feature selection strategy using the robust SVM
linear maximum margin classiﬁer (Soguero-Ruiz et al., a,b). We propose here to
extend their use to model diagnosis and free parameter selection for SVM problems
with the SR algorithm.

Reproducing Kernel Hilbert Space Models for Signal Processing
255
For a given set of N observations v, the dependence between the predictor variables
and the response variable can be fully described by using their joint distribution:
py,x(xm, xd, y) ↦v = {(xm
i , xd
i , yi)|i = , … , N}.
(.)
In order to obtain the SVM-SR model, Equation .is maximized. This estimation
process is denoted by operator s(⋅), and it depends on observations v, and on the free
parameters of the model that have been ﬁxed a priori. Those free parameters can be
grouped in a vector 𝜽, that consists of the 𝜀-Huber cost parameters and of the kernel-
related parameters; that is, for the Gaussian kernel, 𝜽= {𝜀, C, 𝛾, 𝛿, 𝜎}. The SVM-SR
Lagrange multipliers obtained by using the observations and a given a priori ﬁxed 𝜽are
𝜼= s(v, 𝜽).
(.)
The model performance can be measured with the empirical risk, which can be deﬁned
as a merit ﬁgure of the model that is evaluated at the observations used for building the
model. It can be expressed as
̂Remp = t(𝜼, v),
(.)
where t(⋅) represents an operator that stands for the empirical risk calculation. Two
usual merit ﬁgures for the model are the coeﬃcient of determination Rand the RMSE:
R=
∑N
i=(yi −my)(̂yi −m̂y)
√∑N
i=(yi −my)∑N
i=(yi −m̂y)
,
(.)
RMSE = 
N
√
√
√
√
N
∑
i=
(yi −̂yi),
(.)
where my and m̂y are the averages of the observations and of the model predicted
response respectively. Note that 𝜌should be as small (close to zero) as possible, whereas
Rshould be as high (close to +) as possible. As merit ﬁgures are random variables that
depend on the observations, they are more accurately described in terms of CIs, which
can be denoted by
PR(R
l ≤R≤R
u|y, x) ≥q,
(.)
P𝜌(𝜌l ≤𝜌≤𝜌
u|y, x) ≥q,
(.)
where PR(⋅) and P𝜌(⋅) are the pdfs of each merit ﬁgure; q ∈(, ) is the conﬁdence level;
and subscripts “l” and “u” are the lower and upper limits respectively of the CI.
Given that the SVM-SR model does not rely on any a priori distribution of the data,
it is not easy to know the functional form of the pdf of the merit ﬁgures. Moreover,
the sample merit ﬁgures’ estimators can be optimistically biased, especially for some
degenerate choices of the free parameters; for instance, when too much emphasis is
put on the cost of the residuals or when a too small bandwidth is used. Therefore, the

256
Digital Signal Processing with Kernel Methods
empirical risk criterion is not a good criterion for ﬁxing 𝜽. Cross-validation can be useful
in some cases, but it requires one to reduce the training set, which can lead to important
information loss in the model when a low number of observations are available. A
method for estimating the joint pdf of the observations, necessary for a statistical
characterization of the SVM-SR model, is given by a BRT, and it can compensate the
optimistic bias in the merit ﬁgures’ estimators (Rojo-Álvarez et al., ).
A bootstrap resample is a data subset that is drawn from the observation set according
to their empirical distribution ̂py,x(xm, xd, y). Hence, the true pdf is approximated by the
empirical pdf of the observations, and the bootstrap resample can be seen as a sampling
with replacement process of the observed data; that is:
̂py,x(xm, xd, y) ↦v∗= {(xm∗
i , xd∗
i , y∗
i )|i = , … , N},
(.)
where superscript ∗represents, in general, any observation, functional, or estimator that
arises from the bootstrap resampling process. Therefore, resample v∗contains elements
of v appearing none, one, or several times. The resampling process is repeated for
b = , … , B times.
A partition of v in terms of resample v∗(b) is given by
v = {v∗
in(b), v∗
out(b)},
(.)
where v∗
in(b) is the subset of observations that are included in resample b, and v∗
out(b) is
the subset of nonincluded observations. The SVM-SR coeﬃcients for resample (b) are
obtained by
𝜼∗(b) = s(v∗
in(b), 𝜽).
(.)
A bootstrap replication of an estimator is given by its calculation constrained to
the observations included in the bootstrap resample. The bootstrap replication of the
empirical risk estimator is
̂R∗
emp(b) = t(𝜼∗(b), v∗
in(b)).
(.)
The scaled histogram obtained from B resamples is an approximation to the pfd of the
empirical risk. However, further advantage can be obtained by calculating the bootstrap
replication of the risk estimator on the nonincluded observations. By doing so, rather
than estimating the empirical risk, we are in fact obtaining the replication of the actual
risk; that is:
̂R∗
act(b) = t(𝜼∗(b), v∗
out(b)).
(.)
The bootstrap replication of the averaged actual risk can be obtained by just taking
the average of ̂R∗
act(b) for b = , … , B. Simple search strategies can be used for ﬁnding
the free parameters that minimize the averaged actual risk (Rojo-Álvarez et al., ;
Soguero-Ruiz et al., b). Moreover, the replications of the pdf of the model merit
ﬁgures can provide CIs for the performance, by fulﬁlling

Reproducing Kernel Hilbert Space Models for Signal Processing
257
P∗
R(R∗
l
≤R∗≤R∗
u |y∗, x∗) ≥q,
(.)
P∗
𝜌(𝜌∗
l ≤𝜌∗≤𝜌∗
u|y∗, x∗) ≥q.
(.)
A typical range for B in practical applications can be from to bootstrap resam-
ples. Other useful statistical characterizations can be readily achieved, for instance, CIs
for the response variable, CIs for the parametric coeﬃcients, and signiﬁcance levels for
the inclusion of nonparametric variables.
Confidence Intervals for the Response Variable
Frequently, it is not enough to report the predicted response variable, but it is also
convenient to characterize the uncertainty on this prediction. This can be done by
reporting the CI for the average of each prediction, if no strong statistical dependence is
present in the response, or by reporting conﬁdence bands, if output samples are strongly
dependent. For the ﬁrst case, the CI for each average output level can be readily obtained
by calculating the pdf of the replications for each response variable in Equation .
given by model in Equation .as
P∗
yi(y∗
i,l ≤y∗
i ≤y∗
i,u|y, x) ≥q,
(.)
where Pyi denotes the pdf of the ith observation of the response, i = , … , N. When
statistical independence of the response variable can no longer be assumed, conﬁdence
bands should be used instead of CIs (Politis et al., ). Prediction intervals for the
output level can also be calculated.
Inference estimators can be obtained for each of the kth parametric coeﬃcients of
Equation .by obtaining the bootstrap replications of the parametric coeﬃcients in
each model in Equation .as follows:
P∗
𝛽k(𝛽k∗
l
≤𝛽k∗≤𝛽k∗
u |y, x) ≥q,
(.)
where P𝛽k denotes the bootstrap estimated pdf of the kth parametric coeﬃcient,
k = , … , K. For a cut-oﬀtest, a CI overlapping zero level corresponds to a nonsigniﬁ-
cant parametric variable, and it can be excluded from the model.
Significance Level for Nonparametric Features
It is not possible, in general, to obtain CIs for coeﬃcients related to the nonparametric
variables, as these variables remain in a nonlinear, diﬃcult to observe, equation. How-
ever, the performance of the complete model (all the nonparametric variables included)
can be statistically compared with the performance of a reduced model (only a subset of
them included). This can be made by comparing the CI of the merit ﬁgure of the reduced
model with the bias-corrected merit ﬁgure of the complete model. For instance, let ̄R∗
denote the bootstrap bias-corrected correlation coeﬃcient for the complete model, and
let Sbe the correlation coeﬃcient for the reduced model, with CI estimated by fulﬁlling
P∗
S(S∗
l
≤S∗≤S∗
u |y, x) ≥q.
(.)

258
Digital Signal Processing with Kernel Methods
A possible test is the following:
{
H∶
R= S,
or
̄R∗∈(S∗
l , S∗
u )
H∶
R> S,
or
̄R∗< S∗
u .
(.)
That is, the null hypothesis Hstates that the reduced model is suﬃcient, whereas
the alternative hypothesis Hindicates that there is an important loss of ﬁtness when
considering only the reduced model.
6.3
Tutorials and Application Examples
This section illustrates three RKHS signal model formulations; namely, SVM kernel
ARX framework for system identiﬁcation, the family of 𝛾-ﬁlters with kernels for time
series prediction and system identiﬁcation, the SVM for SR in two real examples
(telecontrol network modeling and promotional impact prediction), and the spatial
reference for antenna array processing.
6.3.1
Nonlinear System Identification with Support Vector
Machine–Autoregressive and Moving Average
The performance of RSM for nonlinear system identiﬁcation was benchmarked by our
group in Martínez-Ramón et al. (). We used diﬀerent kernel combinations; namely,
separated kernels for input and output processes (called SVM-ARMAK), accounting
for the input–output cross-information (SVM-ARMAK), and diﬀerent combinations
of nonlinear SVR and SVM-ARMA models (see Listing .). We used the RBF kernel in
all of them.
function [K,K2] = BuildKernels(X,Y,X2,Y2,ker,params,method)
switch lower(method)
case {'svr'}
K = kernelmatrix(ker,[X;Y],[X;Y],params.z);
K2 = kernelmatrix(ker,[X;Y],[X2;Y2],params.z);
case {'2k'} % SVM-ARMA_{2K}
K = kernelmatrix(ker,X,X,params.x)+kernelmatrix(ker,Y,Y,
params.y);
K2 = kernelmatrix(ker,X,X2,params.x)+kernelmatrix(ker,Y,Y2,
params.y);
case {'svr+2k'} % SVR-ARMA_{2K}
K = kernelmatrix(ker,X,X,params.x)+kernelmatrix(ker,Y,Y,
params.y) +kernelmatrix(ker,[X;Y],[X;Y],params.z);
K2 = kernelmatrix(ker,X,X2,params.x)+kernelmatrix(ker,Y,Y2,
params.y) + kernelmatrix(ker,[X;Y],[X2;Y2],params.z);
case {'4k'}
% SVM-ARMA_{4K}
K = kernelmatrix(ker,X,X,params.x)+kernelmatrix(ker,Y,Y,
params.y)
+ kernelmatrix(ker,X,Y,params.xy) +kernelmatrix(ker,Y,X,params.xy);
K2 = kernelmatrix(ker,X,X2,params.x)+kernelmatrix(ker,Y,Y2,
params.y)

Reproducing Kernel Hilbert Space Models for Signal Processing
259
+ kernelmatrix(ker,X,Y2,params.xy)+kernelmatrix(ker,Y,X2,
params.xy);
case {'svr+4k'} % SVR-ARMA_{4K}
K = kernelmatrix(ker,X,X,params.x)+kernelmatrix(ker,Y,Y,
params.y)
+ kernelmatrix(ker,X,Y,params.xy) +kernelmatrix(ker,Y,X,params.xy)
+ kernelmatrix(ker,[X;Y],[X;Y],params.z);
K2 = kernelmatrix(ker,X,X2,params.x)+kernelmatrix(ker,Y,Y2,
params.y)
kernelmatrix(ker,X,Y2,params.xy) +kernelmatrix(ker,Y,X2,params.xy)
kernelmatrix(ker,[X;Y],[X2;Y2],params.z);
end
Listing 6.1 Kernel combinations for SVR, SVM–ARMA2K, SVR–ARMA2K, SVM–ARMA4K, and
SVR–ARMA4K (BuildKernels.m).
The system that generated the data is illustrated in Figure .. The input discrete-time
signal to the system was generated by sampling the Lorenz system, given by diﬀerential
equations dx∕dt = −𝜌x + 𝜌y, dy∕dt = −xz + rx −y, and dz∕dt = xy −bz, with 𝜌= ,
r = , and b = ∕for yielding a chaotic time series. Only the x component was
used as input signal to the system. This signal was then passed through an eighth-order
low-pass ﬁlter H(z) with cutoﬀfrequency Ωn = .and normalized gain of −dB at
+
+
–
45
40
35
30
25
20
15
10
5
–20
–15
x
–10
–5
0
5
10
15
20
Low-pass filter
H(z)
High-pass filter
G(z)
Non-linear feedback system
f(.)=log(.)
Figure 6.2 System that generates the input–output signals to be modeled in the SVM-DSP nonlinear
system identification example.

260
Digital Signal Processing with Kernel Methods
Table 6.1 ME, MSE, MAE, and RMSE 𝜌of models in the
test set.
ME
MSE
MAE
𝝆
SVR
.
.
.
.
SVM-ARMAK
−.
.
.
.
SVM-ARMAK
.
.
.
.
SVR+SVM-ARMAK
−.
.
.
.
SVR+SVM-ARMAK
.
.
.
.
Ωn. The output signal was then passed through a feedback loop consisting of a high-
pass minimum-phase channel, given by on = gn −.on−−.on−−.on−,
where on and gn denote the output and the input signals to the channel. Output on
was nonlinearly distorted with f (⋅) = log(⋅). We generated input–output sets
of observations, and these were split into a cross-validation dataset (free parameter
selection, samples) and a test set (model performance, following samples). The
experiment was repeated times with randomly selected starting points, and the free
parameters were adjusted with a cross-validation method in all the experiments.
Table .shows the averaged results. The best models were obtained when combining
the SVR and SVM-ARMA models, though no numerical diﬀerences were observed
between SVR+SVM-ARMAK and SVR+SVM-ARMAK. In this example, all models
considering cross-terms in the kernel formulation signiﬁcantly improved the results
from the standard SVR, indicating the gain given by the inclusion of input–output cross-
information in the model.
6.3.2
Nonlinear System Identification with the γ-filter
In this section we study the performance of the kernel 𝛾-ﬁlter for nonlinear system
identiﬁcation and time series prediction. We ﬁrst deﬁne the following composite kernels
for their use in this section:
●summation composite kernel (SK)
K(xn−, xk−) = K(xi
n−, xi
k−) + K(xi−
n−, xi−
k−);
(.)
●tensor product composite kernel (TP)
K(xn−, xk−= K(xi
n−, xi
k−) ⋅K(xi−
n−, xi−
k−);
(.)
●cross-information composite kernel (CT)
K(xn−, xk−) = K(xi
n−, xi
k−) + K(xi−
n−, xi−
k−)
+ K(xi
n−, xi
k−) + K(xi−
n−, xi−
k−);
(.)

Reproducing Kernel Hilbert Space Models for Signal Processing
261
●and extended composite kernels obtaining by using three transformations (K+S)
K(xn−, xk−) = Kzz(zn−, zk−) + K(xi
n−, xi
k−) + K(xi−
n−, xi−
k−)
(.)
or by deﬁning a mapping that leads to the summation of the cross-terms composite
kernel and the KT matrix (K+T)
K(xn−, xk−) = K(xi
n−, xi
k−) + K(xi−
n−, xi−
k−)
+ K(xi
n−, xi−
k−) + K(xi−
n−, xi
k−) + Kzz(zn−, zk−).
(.)
Model Development
Model building requires tuning diﬀerent free parameters depending on the SVM
formulation (𝜎ker, C, 𝜀) and ﬁlter parameters (𝜇, P). A nonexhaustive iterative search
strategy (T iterations) was used, and values of T = and M = exhibited good
performance in our simulations in terms of the averaged normalized MSE:
nMSE = log
[

N ̂𝜎
N
∑
i=
(yi −̂yi)
]
,
(.)
where the ̂𝜎is the estimated variance of the data. Most of MATLAB source code for
the experiments is available and linked in the book’s web page.
Nonlinear Feedback System Identification
We now consider the system previously described and illustrated in Figure. .. This
system was used to generate input–output sample pairs (xn, f (g(xn))), that were
split into a training set () and a test set (following samples). The experiment was
repeated times with randomly selected starting points. Table .shows the average
results for all composite kernels. The best results are obtained with the summation
kernel, followed by the kernel trick.
The Mackey–Glass Time Series
Our next experiment deals with the standard Mackey–Glass time series prediction
problem, which is generated by the delay diﬀerential equation dx∕dt = −.xn +
.xn−Δ∕(+ x
n−Δ), with delays Δ = and Δ = , thus yielding the time series MG
and MGrespectively. We considered training samples and used the next 
for free parameter selection (validation set), following the same approach as Mukherjee
et al. (). The results are shown in Table .for both time series, suggesting that a
more complex model is necessary to obtain good results in the prediction of this time
series, which exhibits more complex dynamics.
Electroencephalogram Prediction
This additional and real-life experiment deals with the EEG signal prediction four
samples ahead. This is a very challenging nonlinear problem with high levels of noise and
uncertainty. We used ﬁle “SLPA” from the MIT-BIH Polysomnographic Database.
Data available at http://www.physionet.org/physiobank/database/slpdb/slpdb.shtml.

262
Digital Signal Processing with Kernel Methods
Table6.2 ThenMSEforthekernel𝛾-filtersinnonlinearfeedbacksystemidentification(NLSYS),Mackey–
Glass time series prediction with Δ = 17 and Δ = 30, and EEG prediction. Bold and italics respectively
indicate the best and second best results for each problem. The left side of the table includes the results
of Casdagli and Eubank (1992) for comparison.
KT
SK
TP
CT
K+S
K+T
Method Poly
Rat
Loc1
Loc2
MLP
Eq. 6.19 Eq. 6.75 Eq. 6.76 Eq. 6.77 Eq. 6.78 Eq. 6.79
NLSYS
−.−.−.−.−.–.
−.
−.
−.
−.
−.
MG
−.−.−.−.−.−.
−.
−.
−.
–.
−.
MG
−.−.−.−.−.
−.
−.
−.
–.
−.
−.
EEG
−.−.−.−.−.−.
−.
−.
–.
–.
−.
The ﬁle contains samples; hence, we used as training samples, the next
samples for free parameter selection (validation set), and the remainder for
testing.
Average test results are shown in Table ., showing that the kernel trick com-
bined with the summation or cross-terms kernel performs best, and suggesting that
the high complexity of the underlying signal model has been retrieved by the data
model.
On Model Complexity and Nonlinear Time Scales
Attending to the numerical results (nMSE) in Table ., one could identify EEG and
NLSYS as high-complexity problems, and MGand MGas moderate-complexity
problems. However, diﬀerent kernel structures may accommodate the problem diﬃ-
culty better than others. Certainly, complexity and versatility is an important aspect for
time-series analysis. In this sense, Figure .reports the results for the four nonlinear
time-series problems in terms of machine complexity (SVs (%)), needed tap delays P,
memory requirements 𝜇, and its attendant memory depth M = P∕𝜇, which quantiﬁes
the past information retained and it has units of time samples (Principe et al., ).
The memory depth M serves to uncover the eﬃciency in modeling the (nonlinear)
time scales. On the one hand, it is worth noting that in complex problems (NLSYS and
EEG) the kernel trick (KT) yields slightly higher values of M at the expense of poor
numerical results (see Table .).
The code used for the last three examples (MG, MGand EEG) is shown in
Listing .. First, we select the problem and the method, and then we load the data and
generate the input–output data matrices. Finally, we build the kernel using the code
previously described and train the regression algorithm.

Reproducing Kernel Hilbert Space Models for Signal Processing
263
SVs [%]
P
µ
M = P/µ
NLSYS
0
50
100
KT
SK
TP
CT
K+S
K+T
0
10
20
KT
SK
TP
CT
K+S
K+T
0
0.5
1
KT
SK
TP
CT
K+S
K+T
0
10
20
KT
SK
TP
CT
K+S
K+T
MG17
0
50
100
KT
SK
TP
CT
K+S
K+T
0
10
20
KT
SK
TP
CT
K+S
K+T
0
0.5
1
KT
SK
TP
CT
K+S
K+T
0
20
40
60
KT
SK
TP
CT
K+S
K+T
MG30
0
50
100
KT
SK
TP
CT
K+S
K+T
0
10
20
KT
SK
TP
CT
K+S
K+T
0
0.5
1
KT
SK
TP
CT
K+S
K+T
0
10
20
KT
SK
TP
CT
K+S
K+T
EEG
0
50
100
KT
SK
TP
CT
K+S
K+T
0
10
20
KT
SK
TP
CT
K+S
K+T
0
0.5
1
KT
SK
TP
CT
K+S
K+T
0
20
40
60
KT
SK
TP
CT
K+S
K+T
Figure 6.3 Machine complexity (SVs (%)), tap delays (P), memory requirements (𝜇), and memory
depth (M = P∕𝜇) for all kernel methods and problems.
% -----------------------------------------------------------
% Problem and method
% problems={'mg17','mg30', 'EEG_SLP01A'};
% methods = {'svr' '2k' 'svr+2k' '4k' 'svr+4k'};
method
= '4k'; problem = 'mg17';
% ------------------------------------------------------------
% Free Parameters (cost function and data)
D = 0; % no delay of the signal
p = 2; % number of taps
e = 1e-5; % epsilon insensitivity zone
C = 1e2; % Penalization parameter
gam = 1e-4; % Regularization factor of the kernel matrix (gam*C
% is the amount of Gaussian noise region in the loss)
% -------------------------------------------------
%------------
% Free Parameters (kernel)
ker
= 'rbf';
kparams.x
= 2;
% Gaussian kernel width for the input
kparams.y
= 2;
% Gaussian kernel width for the output

264
Digital Signal Processing with Kernel Methods
kparams.z
= 2;
% Gaussian kernel width for the stacked
kparams.xy = 2; % Gaussian kernel width for the cross-information
% ---------------------------------------------------
%-----------
% Load data
load mg17.dat
N = 500; M = 1000;
X = mg17(1:N-1); Y = mg17(2:N); X2 = mg17(N+1:N+M-1); Y2 = mg17(N+2:N+M);
% --------------------------------------------------------
%---------
% Generate I/O data matrices with a given signal delay D and tap order p
Hx
= buffer(X(D+2:end),p,p-1,'nodelay');
% input, train
Hy
= buffer(Y(1:end-1-D),p,p-1,'nodelay');
% output, train
Hx2 = buffer(X2(D+2:end),p,p-1,'nodelay');
% input, test
Hy2 = buffer(Y2(1:end-1-D),p,p-1,'nodelay');
% output, test
% --------------------------------------------------
%---------------
% Build kernel matrices from these data matrices:
[K,K2] = BuildKernels(Hx,Hy,Hx2,Hy2,ker,kparams,method);
% ----------------------------------------------------
%---------------
% Train a regression algorithm with the previous kernel matrices
[Y2hat,results,model] = TrainKernel(K,K2,Y,Y2,D,p,gam,e,C);
Listing 6.2 Nonlinear system identification with 𝛾filter (NonlinSysIdGammaFilter.m).
6.3.3
Electric Network Modeling with Semiparametric Regression
High reliability communication networks (HRCNs), as is the case with electric net-
works, are characterized by very high performance in terms of availability periods,
and very low failure rates, as the classical problems of network optimization. In these
networks, an extremely low number of events can be observed each year. Then, special
models have to be built. It is well known that SVMs have been demonstrated to be
especially eﬃcient in scenarios where a low number of samples are available, such as
signal analysis or image processing, among others (Camps-Valls et al., c; Soguero-
Ruiz et al., b). An SVM approach was followed in this application encouraged by
its previous performance and robustness.
Network System and Network Model
In the Spanish electrical grid, power ﬂows continuously from the power generators
to the consumption centers (Feijoo et al., ). To achieve reliability, one of the
main issues of this electrical grid consists of designing the telecontrol service with
two redundant but physically diﬀerent paths. Figure .shows an example of path
calculation in which a simple description of the link availability based on an exponential
failure probability with the distance link has been used to determine a reliable double
path from a given origin node to the destination node assigned by the telecontrol
system using Bayesian networks. The reliability of the telecommunication system can
be estimated from data obtained and depends critically on the accuracy of the link and
node availability estimates. In what follows, a method based on composite kernel and
multiresolution is introduced and studied.

Reproducing Kernel Hilbert Space Models for Signal Processing
265
Path2
Path1
Double path routing
Origin
Destination
Figure 6.4 Example of double path calculation in the telecontrol network model with a previously
developed Bayesian network.
Composite Kernels and Multiresolution
In this application example, we used SVR for grouping and dealing with subsets of
features of a diﬀerent nature. This is usually addressed by using a Gaussian kernel, and
in that case a free parameter 𝜎is used for nonlinearly mapping input vectors. Input
feature vectors xi can be redeﬁned into L disjoint feature groups: xi = [xT
i , … , xLT
i ]T,
with cardinality Dl, such that D = ∑L
l=Dl. In that case, we can start from a multiple or
composite (multiple kernels) regression model, with a vector accounting for each subset
of input variables:
yi =
L
∑
l=
⟨wl, 𝝓l(xl
i)⟩+ b + ei.
(.)
Following Equation ., a solution can be obtained for a new input vector:
̂y =
N
∑
i=
𝜂i
L
∑
l=
⟨𝝓l(xl), 𝝓l(xl
i)⟩+ b.
(.)
A scaled kernel for each term in the sum can be used (see Camps-Valls et al. (c)
and Soguero-Ruiz et al. (b)), according to
⟨𝝓l(xl
i), 𝝓l(xl
j)⟩= 𝜆lKl(xl
i, xl
j),
(.)
with 𝜆l ≥. Thus, the ﬁnal solution of the SVM can be readily expressed as
̂y =
N
∑
i=
𝜂i
( L
∑
l=
𝜆lKl(xl, xl
i)
)
+ b,
(.)

266
Digital Signal Processing with Kernel Methods
where 𝜆l represents the contribution of the lth group of input variables to the model. We
can get mutiresolution by adjusting the widths 𝜎l for each kernel, so that it can adapt to
diﬀerent sources of diﬀerent nature. Note also that weight vectors in feature spaces wl
have no physical meaning, but instead they are a mathematical tool for giving support
to the kernel trick.
The methodology is evaluated in both a simulated and a real network. In the simulated
network, the statistical distribution for the link failure – the rate consisting of geograph-
ical eﬀects and link length eﬀects – is known and allows us to generate failures on the
links. Instead of using the simulated network to have a realistic link failure model, its
ﬁnal goal is to benchmark the performance of link failure estimators based on SVM and
NN methods in a known scenario. Furthermore, it can be useful to obtain conclusions
about the selection of the free model parameters, which were addressed in the HRCN
power system.
Surrogate Data Model
A surrogate model was built for the underlying law of failure rate according to the real
links and nodes of the network; that is, using the coordinates. Three diﬀerent eﬀects
were considered:
●Smooth spatial variation, giving higher link failure probability Pfor northern than for
southern links (the relative diﬀerence when comparing northern and southern links
is ﬁve times larger than when comparing eastern and western links (see Figure .,
left), following a linear trend (see Listing .), as follows:
P(lat, lon) = ⋅lat + long + c,
(.)
where cis an oﬀset constant.
%% Smooth Spatial Variation
syms xlon ylat
v= -5*ylat + xlon;
Listing 6.3 Smooth spatial variation (SmoothSpatialVariation.m).
●Fast spatial variation, yielding higher failure probabilities in network regions with
higher grid density. This variation was given by a smoothed version (Gaussian kernel
ﬁltering) of the bidimensional histogram of the number of links in a region (see Matlab
code in Listing .). We denoted it as P(lat, lon) (see Figure ., middle).
%% Fast Spatial Variation
% Create a grid
minx = min(X(:,1)); miny = min(X(:,2));
maxx = max(X(:,1)); maxy = max(X(:,2));
nlinks = length(X(:,1));
npoints = 100;
lineaLat = linspace(miny,maxy,npoints); % Latitud
lineaLon = linspace(minx,maxx,npoints); % Longitud
[Llon,Llat] = meshgrid(lineaLon,lineaLat);
aux = zeros(size(Llon));
for i=1:nlinks
[kk,indx] = min(abs( X(i,1) - lineaLon ));
[kk,indy] = min(abs( X(i,2) - lineaLat ));

Reproducing Kernel Hilbert Space Models for Signal Processing
267
0.8
6
4
2
0
42
40
38
36
42
40
38
36
–10
–5
0
–10
–5
0
0
6
4
2
Longitude
Geographical smooth variation
Link-density dependent variation
Longitude
Latitude
Latitude
0.6
0.4
0.2
50
Fiber length (km)
Pevent
Pevent
Pevent
100
150
200
Figure 6.5 Surrogate model for link failure, and conditional components of the event model used.
aux(indy,indx) = aux(indy,indx) + 1;
end
Listing 6.4 Fast spatial variation (FastSpatialVariation.m).
●Link failure probability, related to the link length. The ﬁber length increases exponen-
tially to unity, following
P(L) = −exp(−L∕),
(.)
and it is depicted in Figure .(right) (see Listing .).
% Link failure probability
fiberLength = X(:,3);
failFiber = 1 - exp(-5e-2*sort(fiberLength));
Listing 6.5 Link failure probability (LinkFailure.m).
The ﬁnal probability of a link failure happening in a given link was given by an additive
mixture of Pand Pspatial eﬀects, and a multiplicative mixture of the spatial and the
ﬁber length, as follows:

268
Digital Signal Processing with Kernel Methods
Pevent(L, lat, lon) = c ⋅P(L)(P(lat, lon) + P(lat, lon)),
(.)
where c is a numerical normalization constant. We use the pdfs for deﬁning the link
events in terms of its geometric enter and its length (see their smooth variation in Figure
.) as follows:
f (lat, lon, L) = Pevent(lat, lon, L) ×
K
∑
k=
𝛿(lat −latk, lon −lonk, L −Lk).
(.)
Annual and Asymptotic Data Generation
We generated an annual event series given by a list of events that have been observed s[n]
during the nth year, with Ny the number of annual events, given by sn, with n = , … , Ny,
where for each event sn consists of a three-tuple including the coordinates and length
of each link. The annual event can be accumulated as follows:
Sn =
n
⋃
m=
sm,
(.)
allowing us to use a frequentist estimation of the event rates at each link:
gk
n = Number of events in link k in Sn
nNy
,
(.)
and it can be trivially shown to converge asymptotically to the actual event rate of the
kth link; that is, limn→∞gk
n = Pevent(latk, lonk, Lk). These asymptotic event probabilities
were used as a benchmark for comparison of the estimated event probability given by
learning from algorithms.
Simulations with Surrogate Data
We used SVM and NN algorithms to estimate gk
n obtaining ̂gk
n; that is, the frequentist
estimation of the events for each link up to year n, given the inputs variables latitude,
longitude, and ﬁber length, for each link (latk, lonk, Lk). Three diﬀerent approaches were
built using SVM algorithms with a Gaussian kernel in all of them: () SVM-K uses just
a single kernel for a vector containing the three input variables; () SVM-K has two
kernels, one for coordinates and one for length; and () SVM-K using three kernels,
one for each input variable. The results obtained were benchmarked with a generalized
regression NN (GRNN) for function approximation (Demuth and Beale, ), which
is also a nonlinear method and only requires a free parameter (Gaussian width) to be
previously ﬁxed.
In the SVM models, a cross-validation strategy (% for training and % for valida-
tion) was applied to tune the free parameters; namely, the ones from the cost function
(C, 𝛾, 𝜖), the width (𝜎i) for each kernel and the relevance parameter 𝜆i. For each model,
and using given free parameters, the MAE was calculated. We want to tune the set of
free parameters that minimize the MAE. To that end, we started with ﬁxed initial values
for C, 𝜀, 𝜎i, 𝜆i (i = , , or ), we obtained the variation of MAEn with 𝛾. We then ﬁxed
parameter 𝛾to the value minimizing MAE[n], and we obtained the variation of MAEn

Reproducing Kernel Hilbert Space Models for Signal Processing
269
as a function of C (while keeping the rest of the parameters to their ﬁxed values). We
continued this process until we explored all parameters.
The purpose of this surrogate data model consists of evaluating the methodology in a
theoretic way, and comparing the estimated and the asymptotic results:
MAE∞
n =

K∕
K
∑
k=K∕+
|gkk,∞−̂gk
n|,
(.)
where ̂gk
n is the estimated output after tuning the free parameters of the SVM with the
training and validation sets, and gk,∞is the asymptotic corresponding value (n = 
for Equation ., as previously described).
A similar procedure was followed for the GRNN, in which only the width parameter
had to be searched in the rank 𝜎∈(−, ). The same considerations for the training
and test set were followed. MAEn, and MAE∞
n were also calculated for comparison
purposes.
Nonobserved Events
In this application, we studied two possibilities for dealing with null events in the links
of the networks: () including the set of samples for building the model (both training
and testing) by giving them a null numerical value; and () excluding them from the
training and validation process.
Results on Observed and Asymptotic Data
In this section we show results in terms of observed error and asymptotic error, which
are given by MAEn, and MAE∞,n respectively, when evaluating SVM-K, SVM-K, and
SVM-K, and an NN scheme using the GRNN. We used independent realizations.
Listing .shows how to obtain both the asymptotic model and when considering
–years.
function SurrogateDataModel
% First, we create the asymptotic model
[X,Ysim] = synteticModel(50000,0);
% For different years
nyears = logspace(0,4,20);
err = [];
for n=round(nyears)
[X,Y] = synteticModel(n,0);
err = [err,mean(abs(Y-Ysim))];
end
% Compute MAE among Y real and Ysim
err = [err,mean(abs(Y-Ysim))];
% Plot the evolution of the error
figure(4), semilogx(nyears,err,'.-.');
xlabel('# years'), ylabel('MAE');
% Plot the asymptotic probability
figure(5), plot(Ysim), xlabel('# node'), ylabel('P_{event}'), axis tight
Listing 6.6 Surrogate data model for asymptotic data and considering different number of years
(SurrogateDataModel.m).
Figure .shows the box plots for all the methods in the cases of including and not
including the null events in the learning procedure. Panels (a) and (c) show the MAEn

270
Digital Signal Processing with Kernel Methods
(a)
×10–4
×10–4
×10–4
×10–4
×10–4
×10–4
×10–4
×10–4
×10–4
×10–4
×10–3
×10–4
×10–3
×10–3
×10–3
×10–3
10
5
0
2
5
10
# years
50 10000
2
5
10
# years
50 10000
2
5
10
# years
50 10000
2
5
10
# years
50 10000
2
5
10
# years
50 10000
2
0
1
2
3
5
10
# years
50 10000
2
5
10
# years
50 10000
2
5
10
# years
50 10000
2
5
10
# years
50 10000
2
5
10
# years
50 10000
2
5
10
# years
50 10000
2
5
10
# years
50 10000
2
5
10
# years
50 10000
2
5
10
# years
50 10000
2
5
10
# years
50 10000
2
5
10
# years
50 10000
(b)
(c)
(d)
Obs. Err. SVM–1K
Obs. Err. SVM–2K
10
5
0
10
5
0
10
5
0
Obs. Err. SVM–3K
10
5
0
10
5
0
10
5
0
Obs. Err. SVM–1K
Obs. Err. SVM–2K
Obs. Err. NN
Obs. Err. NN
Obs. Err. SVM–3K
As. Err. SVM–3K
As. Err. SVM–1K
As. Err. SVM–3K
8
6
4
2
0
8
15
10
5
0
As. Err. SVM–1K
15
10
5
0
As. Err. SVM–2K
15
10
5
0
15
10
5
0
6
4
2
0
As. Err. SVM–2K
As. Err. NN
As. Err. NN
8
6
4
2
0
8
6
4
2
0
Figure 6.6 Simulations with surrogate data including null events into the training and test set (a, b)
and without including them (c, d), after 10 independent realizations. (a, c) Box plot of MAEn (observed
error in n years). (b, d) Box plot of MAE∞,n (asymptotic error, computed using Equation 6.91).
that should be observed with the events available up to year n, and it can be compared
with the gold standard given by MAE∞
n in panels (b) and (d) in terms of the box plots.
Box plots use a box and whisker plot for the merit ﬁgures from each number of available
years n in the conventional way, and have lines at the lower quartile, median, and upper
quartile values. The plus sign represents the outliers.
The observed error (i.e., the MAEn in n years) showed a too-optimistic bias with very
few years of events available, in particular, up to years and years respectively when
considering and when not considering the null events in the training procedure. It also
can be concluded that the asymptotic error (i.e., the MAE∞
n ) clearly stabilized after an
initial period of years when considering the null events and of years when not
considering them. Finally, and overall, including the links with no events in the training
procedure yielded lower asymptotic error in this case, which can be seen for instance in
Figure .b and d, with lower asymptotic error (MAE∞
n ) being obtained for –years
for this approach. The synthetic model created for the surrogate data model is shown
in Listing ..

Reproducing Kernel Hilbert Space Models for Signal Processing
271
function [Xsim,Ysim,Llon,Llat,failGeo,fiberLength,failFiber] = ...
synteticModel(numbYears,plotflag)
% Load estimated coordinates (lon,lat)
load network
[X,Y]=loadcoordinatesReal(red);
% Number of year
if nargin==0, numbYears
= 1; end
% Smooth Spatial Variation
SmoothSpatialVariation
% Fast Spatial Variation
FastSpatialVariation
% Normalized variations
B = ones(5); aux2 = filter2(B,aux,'same');
failSmooth = zeros(npoints);
for i=1:npoints;
for j=1:npoints;
xlon = lineaLon(i); ylat = lineaLat(j);
failSmooth(j,i) = eval(v);
end
end
failSmooth = failSmooth - min(failSmooth(:));
failSmooth = failSmooth/max(failSmooth(:));
failDens = aux2/10; failGeo = failSmooth + failDens;
%% Link failure probability
LinkFailure
% Generate events with uniforme probability
list = zeros(nlinks,1);
for i=1:nlinks;
[kk,indx] = min(abs( X(i,1) - lineaLon ));
[kk,indy] = min(abs( X(i,2) - lineaLat ));
list(i) = failGeo(indy,indx);
end
list = list.*failFiber; list = cumsum(list); list = list/list(end);
Listing 6.7 Synthetic model (SyntheticModel.m).
It can be concluded that there are smooth partial relationships among variables,
such as spatial or link ﬁber dependence. The methodology allows the study of complex
interaction in HRCNs, increasing the accuracy with the number of availability events.
We analyzed the eﬀect of including or excluding the links with unobserved events in the
training set. Finally, we benchmarked that better performance is obtained with SVM
models, especially with few years of available data.
A Case Study
We also analyzed the historical data consisting of the events and the failures in the
optical links from a real HRCN during years. We used the same three input variables
(longitude, latitude, and ﬁber length) to characterize each link. This real HRCN had a
low number of observed events, and for Sand Srespectively. The same SVM
algorithms were tested by using one, two, and three Gaussian kernels. Free parameters
were tuned by using a cross-validation technique, by randomly splitting the available
data into training and validation subsets. We used MAEon the validation to assess
the performance of the model. To give a statistical description of the accuracy, the
estimations were repeated times with diﬀerent randomization.

272
Digital Signal Processing with Kernel Methods
SVM-1K
0
–6
–5
–4
–3
–2
–1
0
20
40
60
0–6
–5
–4
–3
–2
–1
0
20
40
60
SVM-2K
0
–6
–5
–4
–3
–2
–1
0
20
40
60
SVM-3K
log MAE[2]
Figure 6.7 Histograms of the log(MAE2) in the validation sets for each SVM algorithm in the real case
study.
Figure .shows the histograms of MAE, in logarithmic units, for the three SVM
algorithms. It is important to emphasize the bimodality in the error histograms,
showing that there are two cluster of training subsets: one provides suboptimal results,
whereas the other yields good performance. The MAEvalues for SVM-K, SVM-K,
and SVM-K algorithms were −., −., and −.respectively. The conclusion is
that using two kernels provides a signiﬁcant improvement, whereas the inclusion of a
third kernel did not further enhance performance.
6.3.4
Promotional Data
In this subsection, we ﬁrst analyze from simulations the eﬀect on the number of input
features in diﬀerent SR models. Then, we use SVM-SR for an application example based
on price promotions eﬀects. In both cases, the use of bootstrap resampling for diﬀerent
model diagnoses is presented.
Effect of the Number of Input Features
One of the main limitations of SR is that, due to the nonparametric component of the
model, its performance sensibly deteriorates with the number of input features (curse of
dimensionality). For classiﬁcation and regression problems, the SVM has been shown to
be robust when working with high-dimensional input spaces, in part due to the sparsity
enforced on the solution. We present a simple simulation example that compares the

Reproducing Kernel Hilbert Space Models for Signal Processing
273
0
SVM–SR
NW–SR
0.8
1
1.2
1.4
1.6
ρ
1.8
2
2.2
10
5
15
20
25
M
30
35
40
45
Figure 6.8 Quality score 𝜌for SVM-SR and for NW-SR in the simulation example as a function of the
number of input features M.
behavior of SVM-SR with the SR using the NW estimator (NW-SR), in terms of the
dimension of the input space.
The following model was used to generate the observations:
ym = arctan (Q ⋅xm),
(.)
yd = ⟨𝜷, xd⟩,
(.)
y = ym
𝜎m
+ yd
𝜎d
+ en,
(.)
where 𝜎m and 𝜎d are the standard deviations of the ym
n and yd
n processes respectively;
Q is an M × M constant matrix, whose elements are i.i.d. and drawn from a rectiﬁed
Gaussian pdf, N(, ); 𝛽is a D × vector, whose elements are i.i.d. and drawn from a
uniform pdf U(−, ); xm is an M × random vector, drawn from an N(, ); xd is a
D × random vector, drawn from a Bernoulli pdf with -probability of .; and en is an
N(, .) perturbation. Performance is evaluated for SVM-SR and for NW-SR, over a
rank M ∈(, ), and M = D. The training and the validation sets consist of and
observations respectively. The experiment was repeated for runs.
Figure .shows the runs-averaged 𝜌for both methods. Whereas the performance of
NW-SR gets worse with the number of input variables, the error in SVM-SR remains
almost at the same level. For M > , performance is signiﬁcantly diﬀerent for both
methods.
Deal Effect Curve Estimation in Marketing
As a real application example, we describe next an approach to the analysis of the deal
eﬀect curve shape used in promotional analysis, by using the SVM-SR in an available
database (Soguero-Ruiz et al., ). Our data set is constructed from store-level
scanner data of a Spanish supermarket for the period January until December .

274
Digital Signal Processing with Kernel Methods
Table 6.3 Qualitative and quantitative prices (in pesetas) of coffee brands considered in
the study.
Brand
#1
#2
#3
#4
#5
#6
Price
Low
High
High
High
High
Low
Min–max
–
–
–
–
–
–
The number of days in the data set is . To account for the eﬀects of price discounts
with promotional periods, data were represented on a daily basis. Ground coﬀee
category is considered, as it is a storable product with a daily rate of sales. Brands
occupying the major positions in the market (more than % of sales) and being sold on
promotion were selected, leading to the selection of six brands: two national low-priced
brands (#, #), and four national high-priced brands (#to #), as seen in Table ..
The predicted variable is the sales units yk
i sold, at day i, i = , … , , for a certain
brand k, k = , … , , in the category. Brand #was neither modeled nor considered in
the model for the other brands, because it had no promotional discounts.
To capture the inﬂuence of the day of the week on the sales obtained on each day of
the promotional period, we introduce two groups of dichotomic variables: for brand
k and day i, variables xk,d
i,, … , xk,d
i,are the indicators of the day of week (Monday () to
Saturday ()) during promotional periods, whereas xk,d
i,, … , xk,d
i,are the indicators of the
day of week (Monday () to Saturday ()) during nonpromotional periods in brand k.
By distinguishing between both groups of variables, we can observe the gap in sales
between promotional and nonpromotional periods due to the seasonal component.
One of the characteristics of price discounts that researchers have commonly ana-
lyzed is the inﬂuence of promotional prices ending in the digit in the sales obtained
by the retailer (Blattberg and Neslin, ). To capture the inﬂuence of -ending
promotional prices in the sales obtained in the category, we introduced an indicator
variable xk,d
i,that takes the unit value when the promotional price of brand k is -ending.
We also considered in our model the inﬂuence of the promotional price. In order
to remove the eﬀect of the price, the amplitude of the promotional discount was
considered instead of the actual price, as proposed in Heerde et al. (). The price
index, or ratio between the discounted price and the regular price, was introduced using
a metric variable for each brand, xk,m
i,, … , xk,m
i,, with m = , … , . Although we know
that the retailer had used some kind of feature advertising and displays during the period
considered, we do not have any information referring to their usage, so these important
eﬀects could not be included in the model.
For each SVM-SR model, the free parameters were found using B = bootstrap
resamples for the Restimator. The observations were split into training (%) and
testing. For the best set of free parameters, the CI for merit ﬁgures R, 𝜌, and the CI
for parametric variables were obtained (% content). As collinearity between metric
variables for price indexes and dichotomic variables for promotional day of the week was
suspected, a test for the exclusion of the metric variables was performed. Interaction
eﬀects between pairs of price indexes were obtained for each model.

Reproducing Kernel Hilbert Space Models for Signal Processing
275
Table 6.4 Significance for the inclusion of metric variables.
R2
CI S2
𝜌
CI ϱ
#
.†
(.,.)
.†
(.,.)
#
.†
(.,.)
.†
(.,.)
#
.†
(.,.)
.†
(.,.)
#
.†
(.,.)
.†
(.,.)
#
.
(.,.)
.
(.,.)
† indicates signiﬁcant at the % level.
Models for all the brands reached a signiﬁcant R(see Table .). Two examples
of model ﬁtting are shown in Figure .a and d for a high-quality and a low-quality
brand respectively. Their corresponding CIs for the parametric variables are depicted in
Figure .b and e. It can be observed that the weekly pattern is signiﬁcantly diﬀerent in
both of them, with the amplitude of the amplitude of the promotional oscillations being
greater than the nonpromotional. The highest rates of sale correspond to promotional
weekend periods. This behavior is present in all the other models (not shown). With
respect to the -ending eﬀect, it signiﬁcantly increased the sales in brand #, but not in
brand #. The self-eﬀects of the in brand #. The self-eﬀects of the promotion are shown
in Figure .c and f. The increment of sales in brand #(high quality).
Table .shows the results for the test of signiﬁcance of the price indexes in the model.
In all but one (brand #), nonparametric variables were relevant to explain the sales
jointly with the weekly oscillations. This can be seen as two diﬀerent eﬀects due to the
promotion: an increase in the average level of sales (function of the price index), and a
ﬁxed increase in the weekly oscillations amplitude.
Once the relevance of the price indexes has been established, it is worth exploring
the complex, nonlinear relationship among them. We only describe here two examples.
Figure .g shows the cross-eﬀects of brand #on brand #model. For the simultaneous
promotion situation, brand in a stronger competence than brand #, as sales of the later
fall. However, Figure .h illustrates a weaker competence, as promotion in brand #
increases the sales even despite the simultaneous promotion in brand #.
6.3.5
Spatial and Temporal Antenna Array Kernel Processing
We benchmarked in Martínez-Ramón et al. () the kernel temporal reference
(SVM-TR) and the spatial reference (SVM-SRef) array processors, together with their
kernel LS counterparts (kernel-TR and kernel-SRef) and to the linear with temporal
reference (MMSE) and with spatial reference (MVDM). We used a Gaussian kernel
in all cases. The scenario consisted of a multiuser environment with four users, one
being the desired user and the rest being the interferences. The modulated signals were
independent QPSK. The noise was assumed to be thermal, simulated by additive white
Gaussian noise. The desired signal was structured in bursts containing training
symbols, followed by test symbols. Free parameters were chosen in the ﬁrst
experiment and ﬁxed after checking the stability of the algorithms with respect to them.

50
100
150
200
0
20
40
60
80
20
40
60
0
20
40
−10
0
10
20
30
40
50
0.8
0.85
0.9
0.95
1
0
5
10
15
20
25
Price index
Predicted increment on sale units
Brand #2
50
100
150
200
Estimated
Estimated
Train
Train
0
40
(a)
(b)
(c)
(a)
(b)
(c)
20
Sales units
Sales units
Sales units
Sales units
Sales units
R2 : 0.85 −− ρ: 2.35
R2 : 0.88 −− ρ: 4.45
R2 : 0.35 −− ρ: 9.40
R2 : 0.81 −− ρ: 2.78
20
40
60
0
20
40
# day
# day
Test
0
5
10
0
10
20
30
40
# variable
Sales units
13 14 15
0
5
10
# variable
0
2
4
6
8
Intercept
13 14 15
0
8
6
4
2
16
14
12
10
Intercept
0.84 0.86 0.88
0.9
0.92 0.94 0.96 0.98
1
0
0.5
1
1.5
2
2.5
Price Index
Predicted increment on sale units
Brand #1
Estimated
Test
Estimated
Figure 6.9 Examples of results for deal effect curve analysis, showing model fitting (a), CI for parametric variables and intercept (b), and self-effect of discounts
(c) for brands #1 (upper row) and #2 (middle row). Cross-item effect are shown for brand #1 on brand #3 model (a) and for brand #5 on brand #4 model.
Confidence bands for averaged output are shown in brand #2 (detail).

0.8
0.85
0.9
0.95
1
0.85
0.9
0.95
1
0
10
20
30
40
(a)
(b)
(c)
Brand #1
Brand #3
Sales units
0.9
0.95
1
0.8
0.85
0.9
0.95
1
0
0.5
1
1.5
2
2.5
3
Brand #5
Brand #4
Sales units
250
260
270
280
290
300
10
20
30
40
50
60
70
# day
Sales Units
Cross-item effect
Conﬁdence bands
Brand #2
Figure 6.9 (Continued)

278
Digital Signal Processing with Kernel Methods
LS
SVM
100
101
10−3
10−2
10−1
100
δ
BER
100
101
10−3
10−2
10−3
10−2
10−1
100
10−1
100
δ
100
101
δ
BER
BER
10−4
10−3
10−2
10−1
100
100
101
δ
BER
10−4
Figure 6.10 BER performance as a function of RBF kernel parameter 𝛿, of the TR (squares) and the SR
(circles) in an array of seven (top) and five (bottom) elements and with three interferent signals.
Continuous line corresponds to the performance of the linear algorithms.
The BER was measured as a function of kernel parameter 𝛿for arrays of ﬁve and seven
elements, in an environment of three interferences from angles of arrival of ◦, ◦and
−◦, and unitary amplitudes, while the desired signal came from an angle of arrival of
◦with the same amplitude as the interferences. Results in Figure .show the BER as a
function of the RBF kernel width for the temporal and spatial reference SVM algorithms.
These results were compared with the temporal and spatial kernel LS algorithms
(i.e., kernel-TR and -SR), and for seven and ﬁve array-elements. The noise power was
of −dB for seven elements and −dB for ﬁve elements. The results of the second
experiment are shown in Figure .. The experiment measured the BER of the four
nonlinear processors as a function of the thermal noise power in an environment with
three interfering signals from angles of arrival of −◦, ◦, and ◦. The desired signal
direction of arrival was ◦.
Performance was compared with the linear MVDR and MMSE algorithms. All non-
linear approaches showed similar performance, and an improvement of several decibels
with respect to the linear algorithms. SVM approaches showed similar or slightly better

Reproducing Kernel Hilbert Space Models for Signal Processing
279
1
2
3
4
5
6
10−6
10−5
10−4
10–3
10–2
10–1
–10 log10σ2
BER
Linear
SVM–TR
Kernel–TR
SVM–SR
Kernel–SR
Figure 6.11 BER performance as a function of thermal noise power for linear algorithms, SVM SVM-TR,
SVM-SR, Kernel-TR, and Kernel-SR.
performance than nonlinear LS algorithms, and with lower test computational burden
due to their sparseness properties.
6.4
Questions and Problems
Exercise ..
Propose a data model in which composite kernels can provide an
RKHS signal model. You can search in classic statistics, in DSP literature, or in machine
learning literature.
Exercise ..
Propose an example of signal or image analysis where you need to
combine diﬀerent sources of data, and hence where the use of a composite kernel can
be advantageous.
Exercise ..
Program the simple simulation example used to compare NW with
SVM for SR in terms of the number of input features.
Exercise ..
With the previous example, adjust the NW estimator and the SVM
estimator. Are your results consistent with the ones shown in the chapter?
Exercise ..
Provide CIs for the statistical estimates described in the bootstrap
resampling section for this same synthetic problem.

281
7
Dual Signal Models for Signal Processing
7.1
Introduction
In this chapter, a third set of signal processing algorithms with kernels is introduced: the
so-called DSMs. The shared property among them is that the primal model consists of
a nonlinear mapping from sampled or discrete time instants of a unidimensional time
series to an RKHS in a conventional nonlinear regression signal model, and then one
uses the kernel trick, and a time series expansion is stated in terms of a kernel comparing
each two diﬀerent time instants of the series. The use of autocorrelation kernels at this
point, with well-known properties in time and spectral domains, supports the genera-
tion of algorithms with underlying convolutional signal models, such as for classical sinc
interpolation and nonblind deconvolution. Advanced topics on autocorrelation kernels
are also given in terms of the fundamentals for spectrally adapted Mercer kernels for
signal interpolation. In the second part of the chapter we give empirical evidence of the
performance of these dual-form signal processing models via dedicated tutorials and
real-life examples: HRV estimation, Doppler ultrasound processing for fault detection,
Doppler cardiac images in M-mode for heart-ﬁlling monitoring, indoor location based
on power measurements from mobile devices, and interpolation of cardiac meshes
representations from cardiac navigation systems.
7.2
Dual Signal Model Elements
As explained in Chapter , and following the framework introduced in Rojo-Álvarez
et al. (), a particular class of kernels are the shift-invariant kernels, which are those
fulﬁlling K(u, v) = K(u −v). A suﬃcient condition for shift-invariant kernels to be
Mercer’s kernels (Zhang et al., ) is their FT being nonnegative.
Let sn be a discrete-time and real signal, with sn = ∀n ∉[, S −], and let Rs
n =
sn ∗s−n be its autocorrelation. Then, the following kernel can be built:
Ks
m,n = Rs
m−n,
(.)
which is called the autocorrelation kernel induced by signal sn. The spectrum of an
autocorrelation is nonnegative, and given that Equation .is also a shift-invariant
kernel, this is always a valid Mercer kernel.
Digital Signal Processing with Kernel Methods, First Edition. José Luis Rojo-Álvarez, Manel Martínez-Ramón,
Jordi Muñoz-Marí, and Gustau Camps-Valls.
© John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.

282
Digital Signal Processing with Kernel Methods
Now, an additional class of nonlinear SVM for DSP algorithms can be obtained
by considering the nonlinear regression on the time lags or the time instants of the
observed signals and using an appropriate choice of Mercer kernel. These methods
are termed DSM-based SVM algorithms. Here, we summarize this approach and pay
attention to the interesting and simple interpretation of these SVM algorithms under
study in connection with LTI system theory. The interested reader can see Rojo-Álvarez
et al. () and Figuera et al. () for further details.
We use the SVR problem statement as support for the algorithm, by making a non-
linear mapping of each time instant to an RKHS; however, the signal model of the DSP
to be implemented will be the resulting kernel-based solution by using autocorrelation
kernels suitable with the problem at hand. We summarize these ideas in the following
theorem. Although here we use nonlinear kernels in a nonlinear regression problem
statement as a starting point, the resulting DSP models are linear (and time invariant)
in terms of the model equation obtained.
Theorem ..(DSM problem statement)
Let {yn} be a ﬁnite set of samples for a
discrete-time signal in a Hilbert space, which is to be approximated in terms of the
SVR model in Deﬁnition .., and let the explanatory signals be just the (possibly
nonuniformly sampled) time instants tn that are mapped to an RKHS. Then, the signal
model is given by
yn = y(t)|t=tn = ⟨w, 𝝓(tn)⟩
(.)
and the expansion solution has the following form:
̂y|t=tm(t) =
N
∑
n=
𝜂nKh(tn, tm) =
N
∑
n=
𝜂nRh(tn −tm),
(.)
where Kh(⋅) is an autocorrelation kernel originated from signal h(t). Model coeﬃcients
𝜂n can be obtained from the optimization of Equation .(nonlinear SVR signal model
hypothesis in Property ..), with the kernel matrix given by
K h
n,m = ⟨𝝓(tn), 𝝓(tm)⟩= Rh(tn −tm).
(.)
Hence, the problem is equivalent to nonlinearly transforming time instants tn and
tm and making the dot product in the RKHS. For discrete-time DSP models, it is
straightforward to use discrete time n for nth sampled time instant tn = nts, where
ts is the sampling period in seconds.
This theorem is used later to obtain the nonlinear equations for several DSP problems.
In particular, the statement of the classical sinc interpolation SVM algorithm can be
addressed from a DSM (Figuera et al., ), and its interpretation in terms of linear
system theory allows one to propose a DSM algorithm also for sparse deconvolution,
even in some cases when the impulse response is not an autocorrelation (Rojo-Álvarez
et al., ). In addition, the use of autocorrelation kernels provides an interesting
interpretation of the SVM with DSM which can be connected to the well-known Wiener
ﬁlter in estimation theory. All these cases are studied in what follows.

Dual Signal Models for Signal Processing
283
7.3
Dual Signal Model Instantiations
7.3.1
Dual Signal Model for Nonuniform Signal Interpolation
The sinc function in the sinc interpolator has a nonnegative FT, and hence it can be used
as a Mercer kernel in SVM algorithms. Note that for a signal to be an autocorrelation
kernel Rh
n, we do not need to know explicitly signal hn that is its origin.
Property (DSM for sinc interpolation)
Given the sinc interpolation signal model
in Deﬁnition .., and given that the sinc function is an autocorrelation kernel, a DSM
algorithm can be obtained by using an expansion solution as follows:
̂ym =
N
∑
n=
𝜂nK(tn, tm) =
n
∑
n=
𝜂nsinc(𝜎(tn −tm)),
(.)
where Lagrange multipliers 𝜂n can be obtained accordingly.
This equation can be compared with the sinc interpolation signal model in Deﬁni-
tion ..in terms of model coeﬃcients and explanatory signals. For uniform sampling
conditions, Equation .can be seen as the continuous-time reconstruction of the
samples given by an LTI ﬁlter with impulse response given by the sinc function, and
the input signal given by the sequence of the Lagrange multipliers corresponding to
each time instant.
A Dirac delta train can be used for highlighting this continuous-time equivalent signal
model. For uniform sampling, if we assume that 𝜂n are observations from a discrete-time
process and that Kh(tn) is the continuous-time version of the sinc kernel given by Kh
n,
then solution xn can be written down as a convolutional model; that is:
xn = 𝜂n ∗Kn.
(.)
Figure .b illustrates this situation. However, a noncausal impulse response hn is used
here, which for some applications is not an acceptable assumption, so that this scheme
cannot be proposed for all the convolutional problems. Nevertheless, by allowing 𝜀to
be nonzero, only a subset of the Lagrange multipliers will be nonzero, thus providing a
sparse solution, a highly desirable property in a number of convolutional problems.
In order to qualitatively compare the sinc kernel PSMs and DSMs for nonuniform
interpolation, the following expansion of the solution for the PSM approach given in
Equation .can be written as
̂ym =
N
∑
n=
ansinc(𝜎(tn −tm))
=
N
∑
n=
( N
∑
r=
𝜂rsinc(𝜎(tm −tr)
)
sinc(𝜎(tn −tm)).
(.)
A comparison between Equation .and Equation .reveals that these are quite diﬀer-
ent approaches using SVM for solving a similar signal processing problem. For the PSM

284
Digital Signal Processing with Kernel Methods
formulation, limiting the value of C prevents these coeﬃcients from growing without
control; that is, exhibiting low robustness to outliers and heavy tailed noise. For the
DSM formulation, the SRM principle, which is implicit in the SVM formalism (Vapnik,
), will lead to a reduced number of nonzero coeﬃcients.
7.3.2
Dual Signal Model for Sparse Signal Deconvolution
Given the observations of two discrete-time sequences yn and hn, we recall that
deconvolution consists of ﬁnding the discrete-time sequence ̂xn fulﬁlling the following
convolutional signal model:
yn = ̂xn ∗hn + en.
(.)
In many practical situations, xn is a sparse signal, so that solving this problem using an
SVM algorithm can provide with its sparsity property in the dual coeﬃcients. If impulse
response hn is itself an autocorrelation signal, then the deconvolution problem can be
stated in a similar way to the sinc interpolation problem in Section .., using hn signal
instead of the sinc signal as a Mercer kernel. If an autocorrelation signal is used as a
kernel (as we did in the Section ..for the sinc interpolation), then hn is necessarily a
noncausal LTI system.
For a causal system, the impulse response cannot be an autocorrelation, and in this
case a ﬁrst approach is the statement of the PSM in Section .. The solution can be
expressed as
̂xn =
N
∑
i=
𝜂ihi−n.
(.)
Hence, an implicit signal model can be written down, which is given by
̂xn =
N
∑
i=M
𝜂ihi−n = 𝜂n ∗h−n+M = 𝜂n ∗h−n ∗𝛿n+M.
(.)
This means that the estimated signal is built as the convolution of the Lagrange
multipliers with the time-reversed impulse response and with an M-lagged time-oﬀset
delta function 𝛿n.
Figure .b and c shows the schemes of both SVM algorithms. According to the
KKT conditions, the residuals between the observations and model’s output are used
to control the Lagrange multipliers. In the DSM-based SVM algorithms, the Lagrange
multipliers are the input to an LTI noncausal system whose impulse response is the
Mercer kernel (Figure .b and c). Interestingly, for the PSM-based SVM algorithms,
the Lagrange multipliers can be seen as the input to a single LTI system, whose global
input response is heq
n = hn ∗h−n ∗𝛿n−M (see Figure .a). It is easy to show that heq
n is
the expression for a Mercer kernel, which emerges naturally from the PSM formulation.
This provides us with a new direction to explore the properties of the DSM algorithms
in connection with classical LTI system theory, which is described in the following.

Dual Signal Models for Signal Processing
285
Property (DSM for sparse deconvolution problem statement)
Given the sparse
deconvolution signal model in Deﬁnition .., and given a set of observations {yn},
these observations can be transformed into
zn = yn ∗h−n ∗𝛿n−M,
(.)
and hence a DSM-SVM algorithm can be obtained by using an expansion solution with
the following form:
̂ym =
n
∑
n=
𝜂nK(n, m) = 𝜂n ∗hn ∗h−n = 𝜂n ∗Rh
n,
(.)
where Rh
n is the autocorrelation of hn, and Lagrange multipliers 𝜂n can be readily
obtained according to the DSM theorem.
Figure .c depicts this new situation, which now can be easily solved. This simple
transformation of the observations allows one to address the sparse deconvolution
problem for a number of impulse responses hn to be considered in practical applications,
especially for FIRs.
7.3.3
Spectrally Adapted Mercer Kernels
Shannon’s work on uniform sampling (García ; Shannon ) states that a noise-
free, band-limited, uniformly sampled continuous-time signal can be perfectly recov-
ered whenever the sampling rate is larger than or equal to twice the signal bandwidth.
These results from yesteryear have been extended both in theoretical studies (Jerri ;
Meigering ; Unser ; Vaidyanathan ) and in practical applications (Choi
and Munson a; Jackson et al. ; Strohmer ). However, the interpolation
problem when these assumptions are not met becomes a very hard one from an
estimation theory point of view, and many approaches have been proposed by extending
Shannon’s original idea.
A seminal work in this setting was Yen’s algorithm (Yen, ), where an expression
for the uniquely deﬁned interpolator of a nonuniformly sampled band-limited signal
is computed by minimizing the energy of the reconstructed signal. The solution is
given as the weighted sum of sinc kernels with the same bandwidth as the signal. This
algorithm suﬀers from ill-posing due to the degrees of freedom of the solution (Choi
and Munson, a), and this limitation can be alleviated with the inclusion of a
regularization term (Diethron and Munson, ). Other interpolation algorithms using
the sinc kernel have been proposed (Choi and Munson, a,b), in which the sinc
weights are obtained according to the minimization of the maximum error on the
observed data. These algorithms, which use the sinc kernel as their basic interpolation
function, implicitly assume a band-limited signal to be interpolated. For non-band-
limited signals, other algorithms have considered a non-band-limited kernel, such as the
Gaussian kernel (Vaidyanathan, ). Additionally, very eﬃcient methods have been
developed to reduce the computational complexity of the interpolator; for example, by
using ﬁlter banks (Tertinek and Vogel, ), or a modiﬁed weighted version of the
Lagrange interpolator (Selva, ) (see also references therein). It is interesting to note

286
Digital Signal Processing with Kernel Methods
that the well-known Wiener ﬁlter has received less attention than the use of Gaussian
or sinc kernel expansions for nonuniform-sampled signal interpolation problems (Kay,
), probably due to the diﬃculty of formulation in nonuniform signals problems.
As explained before, SVM algorithms have been proposed for nonuniform-sampled
signal interpolation using sinc and Gaussian kernels, showing good performance for
low-pass signals in terms of robustness, sparseness, and regularization capabilities. The
suitability of using the autocorrelation of the observed process as the SVM kernel for
interpolation problems was addressed in Figuera et al. (), where an analysis was
made on the SVM kernel choice better taking into account the spectral adaptation
between the observed signal, the kernel itself, and the Lagrange multipliers yielded
by the model. In this section, we summarize the theoretical contributions in that
work, and we introduce several Mercer kernels that are spectrally adapted to the
signal to be interpolated. We ﬁrst analyze the relationship between the Wiener ﬁlter
and the SVM algorithm for this problem, by using the previously presented spectral
interpretation of both algorithms. Then, according to this analysis, we examine diﬀerent
SVM interpolation kernels accounting for diﬀerent degrees of spectral adaptation and
performance; namely, band-pass kernels, estimated signal autocorrelation kernels, and
actual signal autocorrelation kernels.
Comparison with Support Vector Machine Interpolation
We compare DSM for signal interpolation with a Wiener ﬁlter (Kay, ) and Yen
interpolator (Yen, ) for two reasons. First, they are representative cases of optimal
algorithms (with a diﬀerent optimality concept for each case). And second, they have
a straightforward spectral interpretation, which allows an interesting comparison with
new and standard algorithms.
As we have seen before, in the DSM formulation we can assume a nonuniform
interpolator of the form
̂z(t′
k) = ⟨a, 𝝋(t′
k)⟩,
(.)
where a is a weight vector which deﬁnes the solution and 𝝋(t′
k) is a nonlinear transfor-
mation of the time instants to a Hilbert space , provided with a dot product
⟨𝝋(t), 𝝋(t)⟩= K(t, t),
(.)
with K(⋅, ⋅) being a kernel that satisﬁes the Mercer theorem. The solution of the SVM is
stated in terms of dots products of the transformed input samples. Hence, Equation .
indicates that the nonlinear transformation in Equation .will be done implicitly by
means of a kernel function.
In order to construct the interpolator, vector a must be found. The primal functional
has to be optimized in order to obtain a, and we can use the 𝜀-Huber cost. The solution is
a =
N
∑
n=
(𝛼n −𝛼∗
n)𝝋(tn) =
N
∑
n=
𝛽n𝝋(tn),
(.)
where 𝛽n = 𝛼n −𝛼∗
n are the Lagrange multipliers for constraints in the residuals. Finally,
by combining and expanding the scalar product into a summation, the interpolated
signal is given by

Dual Signal Models for Signal Processing
287
̂z(t′
k) =
N
∑
n=
𝛽n⟨𝝋(tn), 𝝋(t′
k)⟩=
N
∑
n=
𝛽nK(tn, t′
k) =
N
∑
n=
𝛽nK(tn −t′
k),
(.)
where for the last equality we have assumed that the kernel fulﬁlls the condition
K(u, v) = K(u −v). Note again that, in that case, the kernel can be thought of as an
LTI system, and this provides us with a convolutional model for the solution.
It has been seen that both Yen and Wiener ﬁlter algorithms use the LS (regularized
for Yen) criterion. However, the Wiener algorithm is linear with the observations, and it
does not assume any a priori decomposition of the signal in terms of building functions.
Instead, it relies on the knowledge of the autocorrelation function, which can be hard
to estimate in a number of applications. Alternatively, the Yen algorithm is nonlinear
with respect to the observations and assumes an a priori model based on sinc kernels.
Hence, the knowledge of the signal autocorrelation is not always needed. The SVM
interpolation uses a diﬀerent optimization criterion, which is the SRM, and its solution
is nonlinear with respect to the observations since it assumes a signal decomposition in
terms of a given Mercer kernel.
Continuous-Time Equivalent System for Interpolation
Here, we present a continuous-time equivalent system for nonuniform interpolation
(CESNI), which represents the solution of the interpolation problem based on the
SVM approach. The objective of presenting a continuous-time equivalent system is to
establish a frequency-domain description of the interpolation SVM algorithm. Based on
the analysis of the CESNI model, several eﬀective Mercer kernels are proposed for SVM-
based nonuniform sampling interpolation. These kernels account for diﬀerent degrees
of spectral adaptation to the observed data.
Deﬁnition ..(CESNI)
Given the DSM algorithm described in Equation ., we
deﬁne its continuous-time equivalent system as
̂z(t) = {x(t)},
(.)
with x(t) = z(t)+w(t), ̂z(t) the estimation of z(t), and {⋅} a continuous-time nonlinear
feedback system. If {⋅} is evaluated in a set of N time instants {tn, n = , … , N}, taken
from a uniform random distribution, the system deﬁned by Equation .is obtained.
In order to deﬁne {⋅}, recall that Lagrange coeﬃcients are related to the observed
data by the derivative of the cost function; that is, 𝛽n = ′
𝜀H(en) ≡d𝜀H(en)∕de and
that en = x(tn) −̂z(tn). Using these results, it can be seen that the solution deﬁned
in Equation .can be modeled as a feedback system, and we will deﬁne {⋅} as its
continuous-time version, which is represented in Figure .. CESNI elements are next
scrutinized.
Property (Residual continuous-time signal)
Given the CESNI of SVM algorithm
for unidimensional signal interpolation, the residual continuous-time signal is given by
e(t) = x(t) −̂z(t),
(.)
and it corresponds to the continuous-time signal from which the residuals are sampled.

288
Digital Signal Processing with Kernel Methods
L′εH(•)
βe(t)
β(t)
s(t)
· · ·
t
e(t)
ω(t)
z(t)
z(t)
K(t)
ˆ
Figure 7.1 CESNI for the SVM interpolation algorithm. The interpolated signal ̂z(t) is built by filtering
the continuous-time sampled version of the Lagrange coefficients 𝛽(t) with the SVM kernel K(t).
Property (Model coeﬃcient continuous-time signal)
In the CESNI of SVM algo-
rithm for unidimensional signal interpolation, the model coeﬃcient continuous-time
signal is given by the following set of equations:
𝛽e(t) = ′
𝜀H(e(t))
(.)
s(t) =
N
∑
n=
𝛿(tn)
(.)
𝛽(t) = 𝛽e(t)s(t) =
N
∑
n=
𝛽n𝛿(tn),
(.)
where 𝛽e(t) is the equivalent continuous signal for the Lagrange coeﬃcient sequence,
𝛽(t) is its sampled version, and 𝛿(t) represents the Dirac delta function. Hence,
Equation .represents the discrete set of the model coeﬃcients given by the SVM
algorithm as obtained by random sampling of a continuous-time signal 𝛽e(t).
Property (Recovered continuous-time signal)
In the CESNI of SVM algorithm for
unidimensional signal interpolation, the recovered continuous-time signal is given by
̂z(t) = K(t) ∗𝛽(t),
(.)
which shows that the kernel works as a linear, time-invariant ﬁlter and that the Lagrange
coeﬃcients are the inputs to that ﬁlter.
Consequently, by denoting the PSD of ̂z(t), K(t), and 𝛽(t) as P ̂Z(f ), P( f ), and P( f ),
respectively, the recovered signal PSD is given by P ̂Z( f ) = P( f )P( f ). Hence, we can
conclude that the kernel is shaping the output in the frequency domain.
On the one hand, an appropriate adaptation of the kernel spectrum to that of the
original signal will improve the interpolation performance. On the other hand, if the
signal and kernel spectra are not in the same band, the performance will be really
poor. This is the case of the sinc or the Gaussian kernels when used for band-pass
signal interpolation. This suggests that Mercer kernels represent the transfer function
which should emphasize the recovered signal in those bands with higher SNR. Looking
at the Wiener ﬁlter transfer function in Equation ., we can see that the signal
autocorrelation could be used for this purpose, since its FT is the PSD of the signal.

Dual Signal Models for Signal Processing
289
Nevertheless, despite these being well-known principles of signal processing, little
attention has been paid to the possibility of using spectrally adapted Mercer kernels
in SVM-based interpolation algorithms. According to these considerations, several
Mercer kernels were proposed by Figuera et al. () with diﬀerent degrees of spectral
adaptation; namely, modulated and autocorrelation kernels.
Property (Modulated kernels)
If z(t) is a band-pass signal centered at f, modu-
lated versions of RBF and sinc kernels given by
K(tn, t′
k) = sinc[𝜎(tn −t′
k)] sin[πf(tn −t′
k)]
(.)
K(tn, t′
k) = exp
[
−
(tn −t′
k)
𝜎

]
sin[πf(tn −t′
k)]
(.)
are suitable Mercer kernels. Moreover, their spectra are adapted to the signal spectrum.
Note that, in this case, an additional free parameter 𝜔has to be settled for the kernel.
Property (Autocorrelation kernels)
Similar to the Wiener ﬁlter, the autocorrela-
tion of the signal to be interpolated (z(t)) or its noisy observations (x(tn)) can be used
to deﬁne the following kernels:
Kideal(tn, t′
k) = rzz(tn −t′
k)
(.)
Kest(tn, t′
k) = rxx(tn −t′
k)
(.)
which are respectively the ideal (actual) autocorrelation function computed from the
underlying process and autocorrelation function estimated from the observations.
If the second-order statistics of the process are known, the kernel deﬁned in Equa-
tion .can be used. When the autocorrelation of the process is not known, an
estimation procedure must be considered. Note that this problem is not exclusive of the
SVM interpolator, but is also present in the Wiener ﬁlter problems in general. However,
owing to the robustness of the SVM algorithm, simple procedures for estimating the
autocorrelation functions can often be used.
Figure .illustrates the eﬀect of using diﬀerent kernels. The signal to be interpolated
is band pass, so its interpolation with low-pass kernels, either RBF (Figure .a) or sinc
(Figure .b) can be a loose spectral adaptation which indeed emphasizes the noise in
the low-pass band. In Figure .c, the use of a modulated band-pass sinc kernel allows
us to enhance the transfer function spectral adaptation to the signal spectral proﬁle,
which is further reﬁned in Figure .d when using the estimated autocorrelation as
interpolation kernel.
7.4
Tutorials and Application Examples
This section highlights the diﬀerences between PSM and DSM algorithms for DSP,
focusing on nonuniform interpolation and sparse deconvolution. Whereas the theo-
retical sections in this chapter dealt with unidimensional signals, application examples

290
Digital Signal Processing with Kernel Methods
Modulated sinc kernel
f
Autocorrelation kernel
f
RBF kernel
f
Sinc kernel
f
|X(f)|
|K(f)|
Noise
Figure 7.2 Illustration of the spectral adaptation of the kernels to the observations for a band-pass
signal.
are extended to higher dimensional cases. The following set of application examples are
included:
●First, we benchmark the interpolation algorithms scrutinized in the previous chapters
with the DSM interpolation algorithm, and we analyze the implications when using a
sinc and an RBF interpolation kernel.
●Next, we show the performance of the DSP sparse deconvolution algorithm, also
compared with implementations in previous chapters, both from classical and from
SVM problem statements and for synthetic data.
●A real data application example is provided for using the DSM algorithm for sparse
deconvolution, from the fault detection in materials through exploration with ultra-
sound probes.
●Another set of examples analyzes the diﬀerent spectrally adapted Mercer kernels in
synthetic and known-solution signals.
●A speciﬁc example in real data is devoted to reconstruct the cardiac signals obtained
from HRV measurement in long-term monitoring recordings, in terms of the esti-
mated autocorrelation of a stochastic process.
●A real data application shows the use of autocorrelation kernels in a D problem, given
by the denoising of Doppler ultrasound medical images.
●Afterwards, another real data application shows the use of autocorrelation kernels in
a diﬀerent problem, which can be approached by D and D formulations, consisting
of the indoor location of mobile devices.
●And ﬁnally, a real data application is used to show the D approach to reconstruction
of meshes of the heart in cardiac navigation systems for arrhythmia ablation.
7.4.1
Nonuniform Interpolation with the Dual Signal Model
In this subsection we evaluate the performance of the three SVM-based signal inter-
polators. Methods are compared with the standard interpolation techniques deﬁned in
Section ... As described therein, conventional approaches to interpolation problems

Dual Signal Models for Signal Processing
291
can exhibit some limitations, such as loose performance in low signal-to-noise scenar-
ios, or in the presence of non-Gaussian noise. In addition, these methods usually result
in nonsparse solutions. These limitations can be alleviated by accommodating the SVM
formulation with the nonuniform sampling problem in the DSM problem statements.
The following additional signal interpolators are considered here for benchmarking
SVM-DSP interpolation with methods Y, Y, S, S, and Sdescribed in Section ..,
as well as with the SVM-PSM algorithm in Section ..:
) SVM-DSM with sinc kernel (SVM-D), as seen in Listing ..
function [predict,coefs] = ...
SVM_Dual(Xtrain,Ytrain,Xtest,T0,gamma,epsilon,C)
% Initials
tk = Xtrain;
N = length(tk);
Ntest = length(Xtest);
% Construct kernel matrix
H = sinc((repmat(tk,1,N)-repmat(tk',N,1))/T0);
Htest = sinc((repmat(Xtest,1,N)-repmat(tk',Ntest,1))/T0);
% Train SVM and predict
inparams = sprintf('-s 3 -t 4 -g %f -c %f -p %f -j 1', gamma, C, ...
epsilon);
[predict,model] = SVM(H,Ytrain,Htest,inparams);
coefs = getSVs(Ytrain,model);
Listing 7.1 SVM-D algorithm (SVM_Dual.m).
) SVM-DSM with RBF kernel (SVM-R), as seen in Listing ..
function [predict,coefs] = ...
SVM_RBF(Xtrain,Ytrain,Xtest,T0,gamma,epsilon,C)
% Initials
tk = Xtrain;
N = length(tk);
Ntest = length(Xtest);
% Construct kernel matrix
H = exp(-(repmat(tk,1,N)-repmat(tk',N,1)).^2/(2*T0^2));
Htest = exp(-(repmat(Xtest,1,N)-repmat(tk',Ntest,1)).^2/
(2*T0^2));
% Train SVM and predict
inparams=sprintf('-s 3 -t 4 -g %f -c %f -p %f -j 1', gamma, C, epsilon);
[predict,model] = SVM(H,Ytrain,Htest,inparams);
coefs = getSVs(Ytrain,model);
Listing 7.2 SVM-R algorithm (SVM_RBF.m).
Free Parameters and Settings
Four free parameters have to be tuned in the SVM algorithms, which are cost function
parameters (𝜀, C, 𝛾) and the parameter 𝜎(or, equivalently, the time duration T) for
the RBF Gaussian kernel. These free parameters need to be a priori ﬁxed, either by
theoretical considerations or by cross-validation search with an additional validation
dataset, whenever this is available. Note that, in the case of working with signals, the
available training set is the set of available samples. Note that, in general, 𝜎and 𝛾can
be optimized by using the same methodology as in Kwok (), but such analysis

292
Digital Signal Processing with Kernel Methods
is beyond the scope of the present example. For each interpolator here, the optimal
free parameters were searched according to the reconstruction on the validation set.
For SVM interpolators, cost function parameters and kernel parameter were optimally
adjusted. For the other algorithms, the best kernel width was obtained, and for Ythe
best regularization parameter was determined by using a speciﬁed validation set.
For comparison purposes with algorithms in Section ..we use here the same signal
generation and experimental conﬁguration settings therein (see again Listings .
and .). In addition, the required code lines were included in the conﬁguration for
validating the free parameters and then executing the new scrutinized algorithms, as
indicated in Listing ..
% Parameters defining algorithms and theirs free parameters
conf.machine.algs = {@Y1, @Y2, @S1, @S2, @S3, @SVM_Primal, @SVM_Dual, ...
@SVM_RBF};
conf.machine.params_name = {'T0', 'gamma', 'epsilon', 'C'};
conf.machine.ind_params = {1, 1:2, 1, 1, 1, 1:4, 1:4, 1:4};
conf.cv.value_range{1} = linspace(conf.data.T/2, 2*conf.data.T, 10); % T0
conf.cv.value_range{2} = logspace(-9,1,10);
% gamma
conf.cv.value_range{3} = logspace(-6,1,5);
% epsilon
conf.cv.value_range{4} = logspace(-1,3,5);
% C
Listing 7.3 Aditional configuration parameters for SVM-based interpolators (config_interp_SVM.m).
Results
Table .shows the performance of the algorithms in the presence of additive, Gaussian
noise as a function of SNR. Recall that Table .in Section ..compared the set
of algorithms Y, Y, S, S, and Sand allowed us to observe that Yshows the
best performance for all noise levels in these experiments, according to its theoretical
optimality (from an ML and regularization point of view) for Gaussian noise, and the
last one is only included in this table for comparison with SVM methods. Overall, all
the SVM approaches remain close to this optimum for high SNR, and for worsening
SNR the SVM algorithms tend to improve. Interestingly, SVM-R provides with the best
performance in these conditions.
7.4.2
Sparse Deconvolution with the Dual Signal Model
In this section the SVM-DSP algorithm for sparse deconvolution is benchmarked with
the methods presented in Section ... We denote here this algorithm as AKSM, as
seen in Listing ..
Table 7.1 S/E ratios (mean plus/minus std) for Gaussian and sinc kernels.
Method
No noise
40 dB
30 dB
20 dB
10 dB
Y
.± .
.±.
.±.
.±.
.±.
SVM-P
.±.
.±.
.±.
.±.
.±.
SVM-D
.±.
.±.
.±.
.±.
.±.
SVM-R
.±.
.± .
.± ..± .
.± .

Dual Signal Models for Signal Processing
293
function [predict,coefs] = deconv_AKSM(z,h,u,gamma,epsilon,C)
% Initials
ht
= h(end:-1:1);
% Anticausal
z1
= filter(ht,1,z); % Anticausal filtering
y
= [z1(length(h):end)' zeros(1,length(h)-1)]';
N
= length(y);
hauto = xcorr(h);
% Autocorr. del filtro
L
= (length(hauto)+1)/2;
aux
= hauto(L:end);
Kh
= [aux' zeros(1,N-L)];
% Construct kernel matrix
H
= toeplitz(Kh);
% Deconvolution
inparams = ['-s 3 -t 4 -g ',num2str(gamma),' -c ',num2str(C),' -p ...
',num2str(epsilon) ' -j 1']; %,' -q'];
[~,model] = SVM(H,y,[],inparams);
predict = getSVs(y,model);
coefs = filter(h,1,predict);
if ~isempty(u); aux=predict; predict=coefs; coefs=aux; end
Listing 7.4 AKSM deconvolution algorithm (deconv_AKSM.m).
Free Parameters and Settings
The merit ﬁgures were obtained by averaging the results of the algorithms in
realizations. The same set of realizations was used for each SNR and for
each tested trade-oﬀparameter in order to analyze their impact on the solution.
Figure .shows the smooth averaged trend in the free parameters for these kinds of
problems.
Aiming to benchmark SVM-DSP algorithms for sparse deconvolution with conven-
tional algorithms in Section .., we used the same signal generation process and the
same conﬁguration parameters in the experiments (see Listings ., ., and .).In
addition, Listing .shows the conﬁguration ﬁle for validating the free parameters and
running the new algorithm presented in this section.
% Parameters defining algorithms and theirs free parameters
conf.machine.algs={@deconv_L1, @deconv_GM, @deconv_AKSM};
conf.machine.params_name={'lambda','gamma','alfa','gamma',
'epsilon','C'};
conf.machine.ind_params={1, 2:3, 4:6};
conf.cv.value_range{1} = 2.0;
% linspace(0,8,10);
% lambda
conf.cv.value_range{2} = 0.8;
% logspace(-9,1,10); % gamma (GM)
conf.cv.value_range{3} = 8.0;
% linspace(0,8,10);
% alfa
conf.cv.value_range{4} = 1e-3; % logspace(-6,1,10); % gamma (AKSM)
conf.cv.value_range{5} = 2.4;
% logspace(-9,1,10); % epsilon
conf.cv.value_range{6} = 100;
% logspace(-1,3,5);
% C
% Parameters defining visualization function
conf.displayfunc = @display_results_deconv;
% Vector with different values of SNR (Signal Noise Rate)
conf.varname_loop1 = 'SNR';
conf.vector_loop1 = [4,10,16,20]; %9
Listing 7.5 Aditional configuration parameters for AKSM deconvolution (config_interp_AKSM.m).

294
Digital Signal Processing with Kernel Methods
10−6
10−4
10−2
100
102
0
0.2
0.4
0.6
0.8
1
1.2
1.4
γ
MSE
10−3
10−2
10−1
100
101
102
103
0
0.2
0.4
0.6
0.8
1
1.2
1.4
C
MSE
Figure 7.3 MSE of the AKSM algorithm as a function of free parameters 𝛾and C in the 𝜀-Huber cost
function, with SNR = 4 dB.
Results
In order to show the advantages of the AKSM algorithm when other algorithms fail due
to low SNR, a summary of performances for four diﬀerent SNR values (, , , and
dB) is shown in Table .. As seen, AKSM outperforms other methods for low SNR
values. This advantage can be highlighted in Figure ., when SNR = dB (only dB
less than in Figure .). In this case, AKSM reaches a high estimation performance and
detects all the peaks. However, the GM algorithm does not detect a large peak and 𝓁
penalized deconvolution produces a number of low-amplitude spurious peaks.
7.4.3
Doppler Ultrasound Processing for Fault Detection
Rojo-Álvarez et al. () benchmarked sparse deconvolution with SVM for analyzing
a B-scan given by an ultrasonic transducer array from a layered composite material.

Dual Signal Models for Signal Processing
295
Table 7.2 S/E ratios (mean plus/minus std) for Gaussian.
Method
4 dB
10 dB
16 dB
20 dB
L-regularized
.± .
.± .
.± .
.± .
GM
.± .
.± .
.± .
.± .
AKSM
.± .
.± .
.± .
.± .
8
(a)
(b)
(c)
6
4
2
0
–2
–4
20
40
60
80
100
120
8
6
4
2
0
–2
–4
20
40
60
80
100
120
8
6
4
2
0
–2
–4
20
40
60
80
100
120
Figure 7.4 An example of spurious peak detection with SNR = 9 dB for (a) the 𝓁1-penalized algorithm,
(b) the GM algorithm, and (c) the AKSM algorithm.
Complete details on this application can be found in Olofsson (). In order to
compare the deconvolution algorithms described in Sections ..and ..(see
Listings ., ., and .for details) in this real-world application, it is only necessary
to load the data structure (see Listing .) and to deﬁne the experiment settings in
conﬁg_ultrasound.m in the book supplementary material.

296
Digital Signal Processing with Kernel Methods
function data = genDataUltrasound(conf)
% Each column is a scan (time signal) from a location
load CompPlate4
% Normalization
Bscan4 = Bscan4/max(abs(Bscan4(:)));
% Return structured data
data.h = Bscan4(225:end-60,5);
data.z = Bscan4(:,conf.I);
data.u = 1;
data.x = data.z;
Listing 7.6 Loading and preprocessing ultrasound data (genDataUltrasound.m).
Figure .a shows a signal example (A-scan) of the sparse signal estimated by interpo-
lation methods. The same panel also shows the reconstructed observed signal. The 𝓁
deconvolution yielded a good quality solution with a noticeably number of spurious
peaks, the DSM algorithm yielded a good quality solution with less spurious peaks,
and the GM algorithm often failed at detecting the low-amplitude peaks. Figure .b
shows the reconstruction of the complete B-scan data obtained by each algorithm.
These results can be displayed by using the visualization code in Listing ..
function solution_summary = display_results_ultrasound(solution,conf)
algs=fields(solution);
y = solution.(algs{1})(50).Ytest;
n = length(y); nalgs=length(algs);
figure
for ia=1:nalgs
xpred=solution.(algs{ia})(50).coefs;
ypred=solution.(algs{ia})(50).Ytestpred;
subplot(nalgs,2,(ia-1)*2+1); plot(1:n,xpred); ...
title(algs{ia}(8:end)), axis tight; xlim([1 n]); ylim([-.1 .5])
subplot(nalgs,2,(ia-1)*2+2); plot(1:n,y,':g',1:n,ypred(1:n),'r'); ...
title(algs{ia}(8:end)), axis tight; xlim([1 n]); ylim([-.6 .6])
end
figure
Bscan4 = cat(2,solution.deconv_AKSM.Ytest);
subplot(2,2,1), surf(Bscan4), title('B-scan')
axis tight, shading interp, alpha(.5)
for ia=1:nalgs
alg=func2str(conf.machine.algs{ia});
Xpred = cat(2,solution.(algs{ia}).coefs);
subplot(2,2,ia+1), surf(Xpred), title(algs{ia}(8:end))
axis tight, shading interp, alpha(.5)
end
% Performance summary
solution_summary = results_summary(solution,conf);
Listing 7.7 Displaying results for ultrasound example (display_results_ultrasound.m).
7.4.4
Spectrally Adapted Mercer Kernels
In this section the DSM algorithms for spectrally adapted Mercer kernels are bench-
marked in detail. We ﬁrst analyze their performance when interpolating a band-pass
signal. Then, we evaluate the interpolation of two signals with very diﬀerent spectral

Dual Signal Models for Signal Processing
297
B-scan
Time
100
–1
–0.5
0
0.5
200
300
50
100
150
Space
AKSM
Time
100
1
0
0.5
200 300
50
100
150
Space
L1
Time
100
1
0
0.5
200 300
50
100
150
Space
GM
Time
100
1
0
0.5
200 300
50
100
150
Space
0
100
200
300
AKSM
L1
0.2
0.4
0
100
200
300
0.2
0.4
GM
0
100
200
300
0.2
0.4
GM
–0.5
100
200
300
0
0.5
L1
–0.5
100
200
300
0
0.5
AKSM
–0.5
100
200
300
0
0.5
Figure 7.5 Example of real data application: sparse deconvolution of the impulse response in an
ultrasound transducer for the analysis of a layered composite material.

298
Digital Signal Processing with Kernel Methods
Table 7.3 List of algorithms benchmarked in the experiments.
Algorithm description
Label
Yen algorithm with regularization
Yen
Wiener ﬁlter with estimated autocorrelation
Wiener
SVM with low-pass RBF kernel
SVM-RBF
SVM with low-pass sinc kernel
SVM-Dual
SVM with estimated autocorrelation kernel
SVM-Corr
proﬁles in order to assess the eﬀect of the kernel spectral adaptation. We also test
diﬀerent levels of nonuniformity in the sampling process, diﬀerent number of training
samples, and non-Gaussian noise. In a second set of experiments, we test the algorithms
for several D functions with diﬀerent and representative spectral proﬁles.
Interpolation Algorithms for Benchmarking
We benchmarked the interpolation algorithms in Table .. For the Wiener and SVM-
Corr algorithms, the autocorrelation function had to be estimated from the observed
samples. Note that the autocorrelation had to be computed for every time shift 𝜏=
tn −t′
k, so it had to be estimated over a grid with a resolution much higher than that of
the observed samples. Hence, two steps can be carried out:
) Estimating the autocorrelation from the observed samples.
) Interpolating it for every time shift 𝜏= tn −t′
k.
Although many methods exist for these purposes, let us analyze here a simple procedure
based on frequency-domain interpolation. The main reason for this choice is that the
overall procedure is simple and well established. Speciﬁcally, the method consist of:
) Using a Lomb periodogram to estimate the PSD of the signal (Laguna et al., ).
) Using a zero-padding technique in the frequency domain to carry out the interpola-
tion step.
) Finally, using inverse FT of the zero-padded PSD for computing the autocorrelation
function.
Listings .and .show the implementation of these two new algorithms.
function [predict,coefs] = Wiener(Xtrain,Ytrain,Xtest,path_acorr)
% Initials
if nargin == 4 && ischar(path_acorr)
load(path_acorr) % Precomputed autocorrelation of all available data
else
% Computing autocorrelation with training data
if nargin < 4; paso = 1; else paso = path_acorr; end
[acorr,acs] = aCorr(Xtrain,Ytrain,paso);
end
% Contruct kernel matrix
resolution = mean(diff(acs));
% Train
[x,y] = meshgrid(Xtrain,Xtrain);

Dual Signal Models for Signal Processing
299
tau = abs(x-y);
idx = round(tau / resolution) + 1;
H = acorr(idx);
% Test
[x,y] = meshgrid(Xtrain,Xtest);
tau = abs(x-y);
idx = round(tau / resolution) + 1;
Htest = acorr(idx);
% Train and predict
coefs = H \ Htest';
predict = coefs'*Ytrain;
coefs = mean(coefs,2);
Listing 7.8 Wiener filter with estimated autocorrelation – Wiener algorithm (Wiener.m).
function [predict,coefs] = ...
SVM_aCorr(Xtrain,Ytrain,Xtest,gamma,epsilon,C,path_acorr)
% Initialization
if nargin == 7 && ischar(path_acorr)
load(path_acorr) % Precomputed autocorrelation of all available data
else
% Computing autocorrelation with training data
if nargin < 7; paso = 1; else paso = path_acorr; end
[acorr,acs] = aCorr(Xtrain,Ytrain,paso);
end
% Contruct kernel matrix
resolution = mean(diff(acs));
% Train
[x,y] = meshgrid(Xtrain,Xtrain);
tau = abs(x-y);
idx = round(tau / resolution) + 1;
H = acorr(idx);
% Test
[x,y] = meshgrid(Xtrain,Xtest);
tau = abs(x-y);
idx = round(tau / resolution) + 1;
Htest = acorr(idx);
% Train SVM and predict
inparams=sprintf('-s 3 -t 4 -g %f -c %f -p %f -j 1', gamma, C, epsilon);
[predict,model] = SVM(H,Ytrain,Htest,inparams);
coefs = getSVs(Ytrain,model);
Listing 7.9 SVM with estimated autocorrelation kernel – SVM-Corr algorithm (SVM_aCorr.m).
Note that function aCorr is used in both algorithms (see Listing .), and it
estimates the autocorrelation required to construct the kernel matrix by means a Lomb
periodogram.
function [rxx,t_rxx] = aCorr(Xtrain,Ytrain,paso)
% Initials
K = 4;
N = length(Xtrain);
Nc_ent = K*N;
fs_corr = 1/paso;
fs = mean(diff(Xtrain));
T = paso * round(1/(fs*paso));

300
Digital Signal Processing with Kernel Methods
fs = 1/T;
F = fs_corr/fs;
Nc_test = K*N*F;
% Autocorrelation
deltaf = fs / Nc_ent;
f = linspace(-fs/2,fs/2-deltaf,Nc_ent);
Yf = sqrt(lombperiod(Ytrain,Xtrain,f));
Yf_flip = Yf(end:-1:1);
Yf_flip = [0; Yf_flip(1:end-1)];
Yf = fftshift(Yf + Yf_flip);
Yfcent = fftshift(Yf); Yfcent = Yfcent(:);
N_der = ceil((Nc_test-Nc_ent)/2); N_izq = N_der;
Yf_padded=fftshift([zeros(N_izq,1); Yfcent; zeros(N_der,1)]);
acorr = fftshift(real(ifft(abs(Yf_padded).^2)));
rxx = (acorr(ceil(Nc_test/2)+1:end))./max(acorr);
t_rxx = (0:1/fs_corr:(length(rxx)-1)*(1/fs_corr));
Listing 7.10 Estimating the autocorrelation with Lomb periodogram (aCorr.m).
Signals Generation
For synthetic experiments, a D signal with spectral information contained in
[−B∕, B∕] was interpolated. This signal was sampled in a set of L uneven time instants,
diﬀerent for each realization, with an average sampling interval T, such that BT = .
The interpolation instants lie on a uniform grid with step Tint = T∕F, with F the
interpolation factor. The nonuniform sampling time instants were simulated by adding
a random quantity taken from a uniform distribution in the range [−u, u] to the equally
spaced time instants tk = kT, k = , , … , L.
In order to simplify the computation of the kernels, each time instant was rounded
to be a multiple of Tint. The performance of each algorithm was measured by using the
S∕E indicator; that is, the ratio between the power of the signal and the power of the
error in decibels. Each experiment was repeated times.
Two new signals are generated in order to show the advantages of these new algo-
rithms:
) A modulated squared sinc function (MSSF), deﬁned by
f (t) = sinc
(
π
T
t
)
cos (πft),
(.)
where Tand fare chosen in order that the signal bandwidth fulﬁlls BT = . The
spectrum of this signal is a triangle centered at f. The experiment was carried out
with L = samples, T = .s, a nonuniformity parameter u = T∕, and for
Gaussian noise with diﬀerent values of SNR.
) A signal consisting of two Gaussian functions modulated at diﬀerent frequencies and
added together – called a double modulated Gaussian function (DMGF) – given by
f (t) =

𝜎
√
π
exp
(
−|t −𝜇|
𝜎
)
[cos(πft) + cos(πft)],
(.)
with 𝜎= , f= .Hz, and f= .Hz.

Dual Signal Models for Signal Processing
301
Experiment Settings and Results
Three diﬀerent results are displayed in order to provide insight on the advantages of the
new algorithms. To do this, the additional experimental settings for interpolation are in
Listing ..
% Parameters defining the data generation (in @genDataInterp)
conf.data.FNAME = 'MSSF';
conf.data.f = 0.8;
conf.vector_loop1 = conf.data.T/10; % (u)
conf.vector_loop2 = [40 30 20 10]; % dB (SNR)
% Parameters defining algorithms and theirs free parameters
conf.machine.algs={@Y2, @SVM_Dual, @SVM_RBF, @SVM_aCorr, @Wiener};
conf.machine.params_name={'T0','gamma','epsilon','C','paso'};
conf.machine.ind_params={1:2, 1:4, 1:4, 2:5, 5};
conf.cv.value_range{1} = linspace(conf.data.T/2, 2*conf.data.T, 10); % T0
conf.cv.value_range{2} = logspace(-9,1,10);
% gamma
conf.cv.value_range{3} = logspace(-6,1,5);
% epsilon
conf.cv.value_range{4} = logspace(-1,3,5);
% C
conf.cv.value_range{5} = [1e-3 1e-2 1e-1];
% paso
Listing 7.11 Experimental settings for spectrally adapted Mercer kernels (config_interp_Mercer.m).
Interpolation of Band-Pass Signals
To get a ﬁrst insight of the impact of the kernel spectral adaptation on the algorithm
performance, we compared them when interpolating a test signal generated with the
MSSF function. Figure .shows the spectra of the original and reconstructed signals
and the error of reconstruction of the Yen and the SVM algorithms. The error at low
frequencies (where there is no signiﬁcant signal power) is high for the SVM with low-
pass kernels, since in this band the noise is enhanced by the kernel spectrum. On the
contrary, it can be seen that the error produced by the autocorrelation kernel is lower,
since it is adapted to the signal spectrum.
Table .represents the performance of all the algorithms for diﬀerent SNRs. It can be
observed that the SVM with estimated autocorrelation kernels has a good performance.
Note that it clearly outperforms the version of the Wiener ﬁlter. Finally, SVM with low-
pass kernels and Yen algorithms provide a performance lower than that of the others.
Spectral Adaptation of Support Vector Machine Kernel
In order to investigate the importance of the spectral adaptation of the kernel, we
analyzed the performance of two SVM-based interpolators on the two test signals MSSF
and DMGF. Following the same setup as in the previous experiments, we interpolated
these two functions by using two SVM algorithms, one with the ideal autocorrelation
kernel and the other with the modulated sinc band-pass kernel (SVM-Dual) given by
Equation ., where 𝜎and whave been chosen to obtain the same spectrum as the
one of the MSSF function.
In Figure .a, the spectra of both functions and the modulated sinc kernel are shown.
It can be seen that the modulated sinc kernel is spectrally adapted to the MSSF, but
not to the DMGF. Figure .b shows the S∕E performance for both algorithms when
interpolating both functions. The modulated sinc kernel performs well for the MSSF,
since the spectrum is similar, but the performance degrades for the DMGF. Notably,
the autocorrelation kernel is able to adapt its spectrum to both signals, and therefore it
performs well with both of them.

0.04
0.02
0
5
10
15
ytrain
Y2
0.04
0.02
0
0
10
20
30
10
20
30
0.01
0.02
0.03
0
0
0
0.1
f
0.2
0.2
0.4
0.6
5
10
15
×10–3
5
10
15
SVM-Dual
SVM-RBF
SVM - aCorr
Wiener
0.04
0.02
0
5
10
15
0.04
0.02
0
5
10
15
0.04
0.02
0
5
10
15
ytest
ypred
0
10
20
30
0.01
0.02
0.03
0
10
20
30
0.02
0.04
0.06
–0.5
10
20
30
0
0.5
–1
0
0
0.1
f
0.2
0.2
0.4
0.6
0
0
0.1
f
0.2
0.2
0.4
0.6
0
0
0.1
f
0.2
0.2
0.4
0.6
0.8
0
0
0.1
f
0.2
0.2
0.4
0.6
0.8
Test
Pred.
Figure 7.6 Example of the spectra of the original and reconstructed signals in time (top) and frequency domain (bottom ) for the MSSF function. T = 0.5 s,
L = 32, SNR = 10 dB, and u = T/10. The middle panels show the reconstruction error.

Dual Signal Models for Signal Processing
303
Table7.4 MeanS/E andstd(inparentheses)withSNRforaband-pass
signal interpolation, T = 0.5 s, L = 32, and u = T/10.
Algorithm
40 dB
30 dB
20 dB
10 dB
Yen
.(.)
.(.)
.(.)
.(.)
Wiener
.(.)
.(.)
.(.)
.(.)
SVM-Corr
.(.)
.(.)
.(.)
.(.)
SVM-RBF
.(.)
.(.)
.(.)
.(.)
SVM-Dual
.(.)
.(.)
.(.)
.(.)
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
1.2
f (Hz)
Signals and modulated sinc kernel
MSSF
DMGF
Mod sinc kernel
(a)
0
5
10
15
20
25
30
35
40
0
5
10
15
20
25
30
35
40
SNR (dB)
S/E (dB)
MSSF: SVM-CorrId
MSSF: SVM-ModSinc 
DGMF: SVM-CorrId
DGMF: SVM-ModSinc
(b)
Figure 7.7 Spectrally adapted Mercer kernels. (a) Spectrum for the MSSF, DMGF, and modulated sinc
kernel. (b) S/E ratio for SVM-CorrId (solid lines) and SVM-ModSinc (dashed lines) algorithms when
interpolating MSSF (circles) and DMGF (triangles). T = 0.5 s, L = 32, u = T/10.

304
Digital Signal Processing with Kernel Methods
Table 7.5 Mean S/E and std (in parentheses) with the
numberofsamplesL,foraband-passsignalinterpolation,
and u = T/10.
Algorithm
L = 32
L = 64
L = 128
Yen
.(.)
.(.)
.(.)
Wiener
.(.)
.(.)
.(.)
SVM-Corr
.(.)
.(.)
.(.)
SVM-RBF
.(.)
.(.)
.(.)
SVM-Dual
.(.)
.(.)
.(.)
Effect of the Sampling Process
In this experiment we further investigated the performance of all the algorithms for
diﬀerent numbers of samples, using the same function and parameters as in the previous
sections. Note that if the function bandwidth is not changed, the comparison would
be unfair. Hence, we increased it accordingly with the number of samples. Table .
shows the results for diﬀerent lengths of the training set. Most algorithms perform
similarly for all the values of L. However, the Yen algorithm has a poorer behavior,
since the numerical problems associated with the matrix inversion become more
patent. To obtain these results, the following two code lines have been modiﬁed in the
experimental settings:
conf.varname_loop1 = 'data.L';
conf.vector_loop1 = [32 64 128];
7.4.5
Interpolation of Heart Rate Variability Signals
HRV is a relevant marker of the autonomic nervous system control on the heart.
This marker has been proposed for risk stratiﬁcation of lethal arrhythmias after acute
myocardial infarction, as well as for prognosis of sudden death events (Task Force of
the European Society of Cardiology and the North American Society of Pacing and
Electrophysiology, ). When analyzing the HRV time series, the sequence of time
intervals between two consecutive beats (called the RR-interval time series) is often
used, which is by nature sampled at unevenly spaced time instants.
Advanced methods for spectral analysis have shown that the HRV signal contains
well-deﬁned oscillations that account for diﬀerent physiological information. The spec-
trum of the HRV could be divided into three bands: very low frequency (VLF) band,
between and .Hz; low-frequency (LF) band, between .and .Hz; and high-
frequency (HF) band, between .and .Hz. LF and HF bands have been shown to
convey information about the autonomic nervous system control on the heart rhythm,
representing the balance between the sympathetic and parasympathetic contributions.
Spectral-based methods, such as FT or AR modeling, require the RR-interval time series
to be resampled into a uniform sampling grid.
The analysis of the HRV is often performed on h Holter recordings, and a common
procedure is to divide the RR-intervals time series into min segments, in order to

Dual Signal Models for Signal Processing
305
study the evolution of the spectral components along time. Classic techniques for
computing the spectrum of HRV signals aim to obtain a good estimate of LF and
HF components, but due to the nonuniform sampling and the noisy nature of the
measurements, estimating the HRV spectrum is a very hard problem, especially in the
LF and HF ranges. In this example, we applied the SVM algorithms for interpolating
two HRV signals with this purpose.
Methodology
A h Holter recording from a patient with congestive heart failure (labeled with a “D”)
and another one from a healthy patient (labeled with an “H”) have been used for this
experiment. These recordings were divided into min segments, and two preprocessing
steps were done:
) Discarding the segments with more than a % of invalid measurements, usually due
to low signal amplitude or ectopic origin of the beat.
) Applying a detrending algorithm to subtract the mean value and the constant trend
of each segment, for reducing their distortion in the VLF region.
Two algorithms were compared: SVM-Corr and SVM-RBF. Both algorithms were
used to interpolate each segment, by using the RR-intervals in each segment as the
training samples and interpolating the signal over a uniform grid of ms. The
SVM-RBF and SVM-Corr solutions were obtained by using Listings .and .
respectively.
function [predict,coefs] = ...
SVM_Corr(Xtrain,Ytrain,Xtest,gamma,epsilon,C,path_acorr)
% Initials
load(path_acorr)
resolution = mean(diff(t_rxx));
% Contruct kernel matrix
% Train
[x,y] = meshgrid(Xtrain,Xtrain);
tau = abs(x-y);
idx = round(tau / resolution) + 1;
H = rxx(idx); %H.H_ent = interp1(t_rxx,rxx,tau,'linear');
% Test
[x,y] = meshgrid(Xtrain,Xtest);
tau = abs(x-y);
idx = round(tau / resolution) + 1;
Htest = rxx(idx); %interp1(t_rxx,rxx,tau,'linear');
% Train SVM and predict
inparams=sprintf('-s 3 -t 4 -g %f -c %f -p %f -j 1', gamma, C, epsilon);
[predict,model]=SVM(H,Ytrain,Htest,inparams);
coefs = getSvmWeights(model);
Listing 7.12 SVM-Corr algorithm (SVM_Corr.m).
Here, MATLAB code rxx and t_rxx are the data autocorrelation and its temporal
indices precomputed in Listing ..

306
Digital Signal Processing with Kernel Methods
function data = genDataHRV(conf)
load('patient_HRV_preprocessed.mat')
if
strcmp(conf.recordType, 'NN') % NN intervals
auxX = patient.sNNx2;
auxY = patient.sNNy2;
else
% RR intervals
auxX = patient.sRRx;
auxY = patient.sRRy;
end
% Segments (with less than 10% of NaNs) to train and test
X = auxX(patient.idx_segmentos_utiles);
Y = auxY(patient.idx_segmentos_utiles);
data.Xtrain = X{conf.I};
data.Ytrain = Y{conf.I} - mean(Y{conf.I});
data.Xtest = data.Xtrain(1):conf.Tinterp:data.Xtrain(end);
data.Ytest = zeros(length(data.Xtest),1);
% Segments used to obtain autocorrelation vector
idx_without_nan = patient.idx_segmentos_sin_nan; % segments without NaNs
if ~conf.load_acorr && conf.I==1 % rxx is calculated only the first time
X_withoutNaNs = auxX(idx_without_nan(selected_idx_for_rxx));
Y_withoutNaNs = auxY(idx_without_nan(selected_idx_for_rxx));
[rxx,t_rxx] = calcularRxx(X_withoutNaNs, Y_withoutNaNs, conf);
save(conf.path_acorr,'rxx','t_rxx')
end
Listing 7.13 Preprocessing HRV data procedure (genDataHRV.m).
The autocorrelation kernel for each patient was estimated as follows:
) A set of segments (around ) with low noise and high power in the LF and HF
regions were previously selected.
) An estimate of the autocorrelation of each of these segments was computed by using
the method described in Section .., over a ﬁne grid with a step of ms.
) Mean estimated autocorrelation was calculated from this set of estimates, in order
to reduce the noise level.
A subjective evaluation based on the spectrograms (see Listing .for more details)
and some examples were used to compare them.
function [axisF, axisT, EG] = getSpectrogram(X, Y, conf)
% Getting spectrogram
EG = zeros(length(Y),conf.Lfft/2);
for n = 1:length(Y)
y = detrend(Y{n});
aux = abs(fft(y,conf.Lfft));
aux = aux(1:length(aux)/2);
EG(n,:) = aux;
end
EG = EG ./ max(max(EG));
% Getting frequency and time axis

Dual Signal Models for Signal Processing
307
f = X;
if iscell(X),
T = 0; lX = length(X);
for n = 1:lX
T = T + mean(diff(X{n}))/lX;
end
f = linspace(0, (1000/T)/2, conf.Lfft/2);
end
t = 1:length(Y);
[axisF, axisT] = meshgrid(f, t);
Listing 7.14 Getting spectrogram of a signal (getSpectrogram.m).
All of these experiment settings are deﬁned in the conﬁguration ﬁle in Listing ..
% Paramaters of data generation
% conf.path_acorr='data/Section_8.7.4/acorr_HRV.mat';
conf.path_acorr='acorr_HRV.mat';
conf.NREPETS=279;
% It is used to load different segments
conf.load_acorr=1;
% 0: calculate a new rxx, 1: load a precomputed one
conf.recordType = 0;
% 0: in order to use RR intervals, 1: NN intervals
conf.Finterp
= 2;
% interpolation average factor
conf.Lfft = 2048;
conf.Tinterp = 500;
%ms % DELTA T is the time vector to interpolate
conf.T = 810;
%ms Delta T approximated average, used for the ...
first spectral estimation in autocorr kernel
conf.resolution = 5;
%ms
conf.fs = 1/(conf.Tinterp/1000);
conf.f = linspace(-conf.fs/2,conf.fs/2,conf.Lfft);
%conf.filterBy_p_vlf = 1;
conf.correct_vlf = 0;
% 0: do not correct the power excess in very low ...
freqs (vlf, f<0.04Hz).
% 1: the ls is corrected (fft(rxx)) to remove ...
content of vlf freqs
% 2: the vlf is corrected with a ramp
% 3: the vlf is corrected with a parabola
% Parameters defining algorithms
conf.evalfuncs={@MSE};
conf.cv.searchfunc=@gridSearch;
conf.cv.evalfunc = @MSE;
conf.machine.params_name={'T0','gamma_RBF','epsilon_RBF',
'C_RBF', 'gamma_Corr','epsilon_Corr','C_corr','path_acorr'};
conf.machine.algs={@SVM_RBF, @SVM_Corr};
conf.machine.ind_params={1:4, 5:8};
conf.cv.value_range{1} = 600;
% T0
(of SVM_RBF)
conf.cv.value_range{2} = 1e-3;
% gamma
(of SVM_RBF)
conf.cv.value_range{3} = 1e-3;
% epsilon (of SVM_RBF)
conf.cv.value_range{4} = 100;
% C
(of SVM_RBF)
conf.cv.value_range{5} = 0.1;
% gamma
(of SVM_Corr)
conf.cv.value_range{6} = 0.1;
% epsilon (of SVM_Corr)
conf.cv.value_range{7} = 50;
% C
(of SVM_Corr)
conf.cv.value_range{8} = conf.path_acorr;
% acorr
(of SVM_Corr)
conf.displayfunc = @display_results_HRV;
Listing 7.15 Experiment settings for HRV example (config_HRV.m).

308
Digital Signal Processing with Kernel Methods
Estimated autocorrelation
Patient D
0
0
0.5
1
10
20
30
t (s)
40
50
Spectrally adapted
Patient D
0
0.5
1
0.1
0.2
0.3
f (Hz)
0.4
0.5
Patient H
0
0.5
1
0.1
0.2
0.3
f (Hz)
0.4
0.5
0
0
0.5
1
10
20
30
t (s)
40
50
Patient H
Figure 7.8 Autocorrelation kernels in time and frequency for the HRV segments of patients D and H.
Results
The autocorrelation kernels and their spectra for both patients D and H are shown in
Figure .. Although they are still noisy, note that in patient D only one peak is present
(probably due to the disease) and both peaks LF and HF are present in patient H. Using
the SVM-RBF and the SVM-Corr with these kernels, we interpolated each segment.
The main eﬀect was that SVM-Corr was able to ﬁlter the noise in order to highlight
the LF and HF peaks better than the RBF algorithm, especially where the density of
noise was very high in frequency bands out of the regions of interest, as can be seen in
the examples for both patients shown in Figure .a for patient D and Figure .b for
patient H. For the latter, two details for a region of interest and for a region with noise
are shown in the lower plots. In the three examples it can be checked that the SVM-
Corr was able to reduce the noise level better than the RBF algorithm. Note that the
SVM-Corr algorithm is able to ﬁlter these misleading measurements much better than
SVM-RBF. These results can be displayed by using Listing .for visualization.
function solution_summary = display_results_HRV(solution,conf)
% Init. params.
fmin = 0.04;
% Loading data
load('patient_HRV_preprocessed.mat')
if
strcmp(conf.recordType, 'NN') % NN intervals
auxX = patient.sNNx2;
auxY = patient.sNNy2;
else
% RR intervals
auxX = patient.sRRx;
auxY = patient.sRRy;
end

Dual Signal Models for Signal Processing
309
X = auxX(patient.idx_segmentos_utiles);
Y = auxY(patient.idx_segmentos_utiles);
% Figures
fs = 1000 / conf.Tinterp;
f_interp = linspace(0, fs/2, conf.Lfft/2);
algs=['Original';fields(solution)];
nalgs=length(algs);
for ia=1:nalgs
alg=algs{ia};
if ia==1
[F.(alg), T.(alg), EG.(alg)] = getSpectrogram(X, Y, conf);
else
for i=1:length(solution.(alg)); ...
Ypred{i}=solution.(alg)(i).Ytestpred; end
[F.(alg), T.(alg), EG.(alg)] = getSpectrogram(f_interp, Ypred, ...
conf);
end
fmin2 = find(F.(alg)(1,:) > fmin, 1); fmax2 = find(F.(alg)(1,:) > ...
0.5, 1);
subplot(3,1,ia);
mesh(F.(alg)(:,fmin2:fmax2), T.(alg)(:,fmin2:fmax2), ...
EG.(alg)(:,fmin2:fmax2)); view(10, 30)
title([strrep(alg,'_','-'),' Interpolated HRV spectrogram']);
axis([0 0.5 1 length(X) 0 0.2]);
xlabel('f (Hz)'); ylabel('Segment index');
end
figure;
extra={' segment',' interpolated',' interpolated'};
for ia=1:nalgs
alg=algs{ia};
subplot(3,1,ia); hold on;
for n = 1:size(EG.(alg),1),
plot(F.(alg)(1,:),EG.(alg)(n,:),'k');
end
axis([0 0.5 0 1]);
title([strrep(alg,'_','-'),extra{ia},' spectra (accumulated plot)']);
end
xlabel('Hz');
%% Performance summary
solution_summary = results_summary(solution,conf);
Listing 7.16 Displaying results for HRV example (display_results_HRV.m).
7.4.6
Denoising in Cardiac Motion-Mode Doppler Ultrasound Images
Doppler echocardiography has become a widespread noninvasive technique to assess
cardiovascular function in clinical practice. It has been really useful to measure the
blood ﬂow within the heart (Weyman, ). A frequency shift occurs when the
ultrasound waves interact with objects in motion, such as red cells within blood.
This detectable frequency shift is used to determine the velocity of the blood ﬂow.
Not only the blood velocity, but also the intracardiac pressure diﬀerences can be
obtained noninvasively by using ultrasound images under certain conditions (Bermejo
et al., ). Motion mode (M-mode) echocardiography is used to evaluate the
morphology of structures, movement, and velocity of cardiac values and timing of

310
Digital Signal Processing with Kernel Methods
0
0.1
0.2
0.3
0.4
0.5
2
4
6
f (Hz)
0.15
0.2
0.25
1
2
3
f (Hz)
0.3
0.32
0.34
0.36
0.38
0.4
2
4
6
f (Hz)
0
0.1
0.2
0.3
0.4
0.5
0.05
0.1
0.15
HRV segment spectrum
0.2
0.25
0.3
0.35
0.4
(a)
(b)
f (Hz)
Patient D → Example 1
Patient D → Example 2
0
0.1
0.2
0.3
0.4
0.5
f (Hz)
SVM-RBF
SVM-Corr
Original spectrum
Patient H
Figure 7.9 Examples of HRV segments of patients D (a) and H (b). Two details of the HRV spectrum for
patient H are shown in the lower plots.
cardiac events. Whereas more advanced modes of ultrasound in echocardiography have
been developed, M-mode is still a fundamental part of the routine echocardiographic
exams.
In order to denoise the blood velocity from color Doppler M-mode (CDMM) images,
we study here an SVM algorithm with a kernel able to get a better adaptation to
these types of images, such as the kernel based on the image autocorrelation func-
tion described so far in this chapter. Conde-Pardo et al. () earlier proposed an
SVM algorithm based on a multiple-width RBF kernel for this goal. Extending those
results, Soguero-Ruiz et al. (a) used an autocorrelation kernel to characterize the

Dual Signal Models for Signal Processing
311
smoothness of these types of images. We include this application here to show how
to generalize the autocorrelation concepts from D to higher dimensional domains.
The next applications will also scrutinize the usefulness of the autocorrelation kernel
in multidimensional signals and problems.
Dual Signal Model for Color Doppler Motion-Mode Image Denoising
A nonlinear regression model for CDMM images denoising using SVM can be
described as follows. We denote the velocity ﬁeld and the acquired image by vb(s, t)
and {V i,j = v(i𝛿s, j𝛿t), i = , … , Ns, j = , … , Nt} respectively. In this case, [i, j]
denotes the image coordinates of each pixel V i,j, and I denotes the set of coordinates
for all the image pixels. This set can be divided into a training Itr and a test Itest subsets.
Then, the acquired image can be expressed as follows:
V ij = ⟨w, 𝝓([i, j])⟩+ b + ei,j,
(.)
where ei,j is the model residual for each pixel, 𝝓([i, j]) is a nonlinear application of
coordinate vector [i, j] to a high-dimensional feature space , and b is a bias term.
A linear regression for the pixel value is given by the dot product of nonlinearly
transformed pixel coordinates and w ∈. Given this image model, and using as usual
the 𝜀-Huber cost, the primal model can be stated as minimizing

‖w‖+ 
𝛿
∑
[i,j]∈Itr

(𝜉
i,j + 𝜉⋆
i,j ) + C
∑
[i,j]∈Itr

(𝜉i,j + 𝜉⋆
i,j) −
∑
[i,j]∈Itr

𝛿C

(.)
with respect to wp, {𝜉i,j}, {𝜉⋆
i,j}, and b, and constrained to
V i,j −⟨w, 𝝓([i, j])⟩−b ≤𝜀+ 𝜉i,j
(.)
−V i,j + ⟨w, 𝝓([i, j])⟩+ b ≤𝜀+ 𝜉⋆
i,j,
(.)
and to 𝜉i,j, 𝜉⋆
i,j ≥, for [i, j] ∈Itr; {𝜉i,j} and {𝜉⋆
i,j} are slack variables or losses, and Itr
and
Itr
are the subsets of pixels for which losses are in the quadratic or in the linear cost zone
respectively.
The dual problem can be expressed in terms of the correlation matrix of input space
pixel pairs, R([i, j], [k, l]) ≡⟨𝝓([i, j]), 𝝓([k, l])⟩, maximizing
−
(𝜶−𝜶⋆)T[R + 𝛿I](𝜶−𝜶⋆) + (𝜶−𝜶⋆)TV −𝜀T(𝜶+ 𝜶⋆)
(.)
constrained to C
≥
𝛼(⋆)
i,j
≥
, where 𝛼i,j and 𝛼∗
i,j are the Lagrange multipliers
corresponding to Equations .and .; and 𝜶(⋆) = [𝛼(⋆)
i,j ] and V = [V i,j], for [i, j] ∈Itr,
are column vectors. After obtaining 𝜶(⋆), the velocity for a pixel at [k, l] is
̂V k,l =
∑
[i,j]∈Itr
𝛽i,j⟨𝝓([i, j]), 𝝓([k, l])⟩+ b,
(.)

312
Digital Signal Processing with Kernel Methods
with 𝛽i,j = 𝛼i,j −𝛼⋆
i,j, which is a weighted function of the nonlinearly observed times in
the feature space.
By using Mercer kernels, we can substitute K([i, j], [k, l]) = ⟨𝜙([i, j]), 𝜙([k, l])⟩. We will
use here the Gaussian kernel, given by
KG([i, j], [k, l]) = exp
(−‖[i, j] −[k, l]‖
𝜎
)
,
where 𝜎is the width parameter. In Conde-Pardo et al. (), a double-width RBF
kernel was proposed, given by
KD([i, j], [k, l]) = exp
(−|i −k|
𝜎
s
)
exp
(
−|j −l|
𝜎
t
)
,
in order to separate the contribution of the two diﬀerent dimensions (space and time).
Taking into account that CDMMs are characterized by their smoothness, we study
the following autocorrelation kernel, adapted from results in this chapter so far:
K([i, j], [k, l]) = 𝜌v(k −i, l −j),
(.)
with 𝜌v denoting the image autocorrelation. Thus, the CDMM image model can ﬁnally
be expressed as
̂V k,l =
∑
[i,j]∈Itr
𝛽i,jK([i, j], [k, l]) + b.
(.)
We used a cross-validation strategy to tune the free parameters of the SVM cost function
(𝜀, 𝛿, C), as seen in Listing ..
function [predict,coefs] = ...
SVM_AutoCorr(Xtrain,Ytrain,Xtest,gamma,nu,C,path_acorr)
% Initialize params
if nargin==7 && ischar(path_acorr)
load(path_acorr) % Precomputed autocorrelation of all available data
else
% Computing autocorrelation with training data
if nargin<7; paso=path_acorr; else paso=1; end
x = (min(Xtrain(:)):paso:max(Xtrain(:))+paso);
axs=cell(1,size(Xtrain,2)); axs(:)={x(:)};
[ACs{1:size(Xtrain,2)}]=ndgrid(axs{:});
interpAxis = griddatan(Xtrain,Ytrain,cat(2,ACs{:}));
interpAxis(isnan(interpAxis)) = 0;
[acorr,acs] = autocorr(reshape(interpAxis,size(ACs{1})),ACs{:});
end
% Construct kernel matrix
H = getAutoCorrKernel(Xtrain,Xtrain,acorr,acs{:});
% Train SVM and predict
nchunks=16; ntest=length(Xtest); % Chunks partition for speeding up
chunks=floor(ntest/nchunks); % If Xtest is an image: 4x4 chunks
chunksTest = mat2cell(Xtest,...

Dual Signal Models for Signal Processing
313
[repmat(chunks,1,nchunks-1),ntest-(nchunks-1)*chunks],2);
%inparams=sprintf('-s 3 -t 4 -g %f -c %f -p %f -j 1', gamma, C, epsilon);
inparams=sprintf('-s 4 -t 4 -g %f -c %f -n %f -j 1', gamma, C, nu);
chunksPredict=cell(1,nchunks);
for i=1:nchunks
Htest = getAutoCorrKernel(Xtrain,chunksTest{i},acorr,acs{:});
[chunksPredict{i},model]=SVM(H,Ytrain,Htest,inparams);
end
predict=cell2mat(chunksPredict');
coefs = getSvmWeights(model);
Listing 7.17 Autocorrelation calculation for CDMM image denoising (SVM_Autocorr.m).
A simple synthetic model of diastolic transmitral ﬂow in CDMM was created by
addition of three bivariate Gaussian components, given by
vb(s, t) =

∑
i=
ai exp
{
−
[si, ti]𝛴−
i [si, ti]T}
,
(.)
where [si, ti] denotes a bidimensional row vector, 𝛴i is the covariance matrix of each
component, and si, ti, and 𝛴i are given in Table .. This table shows the parameters
that were adjusted to match physiological values of both waves. Time zero was deﬁned
at the QRS onset. Listing .provides the code implementation, which is represented
in Figure ..
function velocityModel
% Simulated velocity model of diastole, with three waves (E1, E2, A).
syms f v s t ro Sigma
% Parameters
s1=8;
s2=4;
%
limits of the spatial integration
Sigma = [1 -0.5; -0.5 1];
at= 0.05;
as= 1.5;
v0 = 100*exp(-(1/2) *[s,t] * inv(Sigma)*[s;t]);
% E (symbolic)
v1 = subs(v0,{s,t},{(s-6)/as, ((t+0.25)/at)});
% Substitution for
E1
v2 = 0.5 * subs(v0,{s,t},{(s-7)/as,(t+0.08)/at});
% Substitution for E2
Sigma2 = [.8 -.05; -.05 .1];
v4 = 100*exp(-(1/2) *[s,t] * inv(Sigma2)*[s;t]);
% A (symbolic)
v3 = .75*subs(v4,{s,t},{(s-6)/as, ((t+0.3)/at)});
% Substitution for A
% To avoid values higher than 100 cm/s
v1 = .9*v1; v2 = .9*v2; v3 = .65*v3;
v = v1+v2+v3;
Listing 7.18 Velocity model image for CDMM image (velocityModel.m).
Sample Selection Criteria
As described before, the dataset was divided into two subsets: one for training and
one for testing. Note that this is a special application of machine learning principles,
in which we actually have available all the existing samples (i.e., the pixels in an image
for denoising). Therefore, diﬀerent strategies for training samples selection are expected

314
Digital Signal Processing with Kernel Methods
Table 7.6 Parameter values of color-Doppler transmitral flow model
(ai cm/s, ti s).
i
1
2
3
ti
t + .
.
t + .
.
t + .
.
ai


.
si
s −
.
s −
.
s −
.
𝛴i
(

−.
−.

)
(

−.
−.

)
(
.
−.
−.
.
)
vb(s,t)
10
–0.5
–0.4
–0.3
–0.2
Time (s)
Depth (cm)
–0.1
0
0
–20
–40
–60
–80
–100
20
40
60
80
100
0.1
9
8
7
6
5
4
3
2
vb(s,t)
–0.4
–0.3
–0.2
Time (s)
–0.1
0
–50
–100
50
100
11
10
Depth (cm)
9
8
7
6
5
4
3
2
Figure 7.10 Synthetic (up) and real (down) CDMM images used in this example.

Dual Signal Models for Signal Processing
315
to have an impact on the ﬁnal performance. Therefore, we scrutinized several training
set selection criteria:
) Random. Samples for training and test subsets are selected randomly.
) Based on amplitudes. All pixels are ranked in a descending way based on their
amplitude values. Greater weights are given to those samples (pixels) with greater
amplitudes following a Gaussian function, and the training and test sets are ﬁnally
constituted.
) Based on gradients. The image pixels are ranked in order of their gradient amplitude
from greatest to least. The amplitude gradient is obtained by merging (in module) the
amplitude changes between adjacent pixels in both image directions. Then, by using a
Gaussian function, greater weights are given to those samples (pixels) corresponding
to greater gradients values. Finally, the training and test sets are constituted.
) Based on second derivatives. This criterion is based on the gradient one. The steps
to follow are the same as those in the gradient-based criterion by considering the
second derivatives as the changes in the amplitude gradient between adjacent pixels
in both image directions.
) Based on edges. By using a D edge ﬁlter (Sobel ﬁlter (Farid and Simoncelli, )),
only those samples from image regions with sharp amplitude changes are considered
to build the machine. A threshold can be considered to obtain a training set with a
suﬃcient numbers of samples. Finally, the training and test sets are constituted.
See
randomCriterion.m,
amplitudeCrierion.m,
gradientCriterion.m,
SeconderivativeCriterion.m, and EdgeCriterion.m for the implementation
of each of these criteria.
Experiments and Results
A DSM algorithm with three diﬀerent kernels was used to denoise the CDMM image;
speciﬁcally, RBF, D-RBF, and autocorrelation kernels, considering diﬀerent sample
selection criteria and and training samples in each approach. Table .
shows the results obtained in terms of MAE, concluding that better approximations are
obtained when an autocorrelation kernel is considered and when the number of training
samples increases. Then, we evaluate diﬀerent sample selection criteria (random, edges,
amplitude, gradient, second derivative) considering a higher number of training samples
(, , , and ) (see Table .). In general, lower MAE values were obtained
as the number of training samples increased. Hence, a sample selection criterion based
on edges provides good approximations.
Table7.7 Syntheticimage.MSEinCDMMIapproximation
considering different kernels.
No. training pixels
RBF
D-RBF
Corr

.
.
.

.
.
.

316
Digital Signal Processing with Kernel Methods
Table 7.8 Synthetic image. MSE in CDMMI approximation considering different sample
selection criteria.
No. training pixels
Random
Edges
Amplitude
Gradient
Second derivative

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
Table 7.9 Real image. MSE in CDMMI approximation considering different sample selec-
tion criteria.
No. training pixels
Random
Edges
Amplitude
Gradient
Second derivative

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
After obtaining results based on synthetic data, we analyzed results based on a real
CDMM image (×) from a healthy volunteer. In this case, an SVM algorithm with
an autocorrelation kernel was considered for the image approximation, as well as the
same sample selection criterion as the one evaluated with the synthetic image, for ,
, , and training pixels, respectively (see Table .). As expected, the higher
the number of training samples, the better the approximation obtained. Figure .
shows the ﬂow velocity recording for a healthy volunteer (a) and the residual obtained
when using diﬀerent sample selection criteria: (b) random; (c) based on edges; (d) based
on amplitudes, (e) based on gradients; and (f) based on second derivative considering
training pixels. In general, the CDMM image residuals are lower with an edges
sample selection criteria.
7.4.7
Indoor Location from Mobile Devices Measurements
In recent years, a great number of location-dependent computer services have been
proposed using wireless mobile devices. For these services to work properly, a key issue
is the knowledge of the position of the user, which is a complex task, especially in
the context of indoor scenarios. For cost-eﬀective systems, one of the most common
approaches to provide indoor location is to use the received signal strength (RSS)
measured by .devices to estimate the user location (Bahl and Padmanabhan,
). Fingerprinting techniques are used for this purpose, which consist of ﬁnding the
relationship between the RSS and the position from a set of measurements in known
locations (which can be dealt with as a training set of samples) and then using the
learned relationship to estimate the position of the user according to its received power.
The k-nearest neighbors (k-NN) scheme is a very simple statistical approach quite often
used for location purposes (Bahl and Padmanabhan ; Figuera et al. , ).

Dual Signal Models for Signal Processing
317
10
0
20
MAE
40
60
80
100
8
6
4
2
–0.4
–0.3
–0.2
–0.1
Depth (cm)
Time (s)
10
0
20
MAE
40
60
80
100
8
6
4
2
–0.4
–0.3
–0.2
–0.1
Depth (cm)
Time (s)
10
0
20
MAE
40
60
80
100
8
6
4
2
–0.4
–0.3
–0.2
–0.1
Depth (cm)
Time (s)
10
0
20
MAE
40
60
80
100
8
6
4
2
–0.4
–0.3
–0.2
–0.1
Depth (cm)
Time (s)
10
0
20
MAE
40
60
80
100
8
6
4
2
–0.4
–0.3
–0.2
–0.1
Depth (cm)
Time (s)
2
–0.4 –0.35 –0.3 –0.25 –0.2 –0.15 –0.1 –0.5
4
Depth (cm)
Time (s)
6
8
10
–120
–100
–80
–60
–40
–20
0
20
40
60
(a)
(b)
(c)
(d)
(e)
(f)
Figure 7.11 Color Doppler M-mode trasmitral flow velocity recording for a healthy volunteer (a) and
residuals using different sample selection criteria: (b) random; (c) based on edges; (d) based on
amplitudes, (e) based on gradients; and (f) based on second derivative.
With this method, the measured RSS is compared with those in the training set, ﬁnding
the k most similar entries and then averaging them to estimate the location. Several
probabilistic algorithms and other technically advanced approaches have also been
proposed to perform signal ﬁngerprinting.
Though statistical learning approaches can estimate the unknown mapping from
a set of examples, the result can be signiﬁcantly improved by incorporating a priori
knowledge about the problem, in the same way that better inference is provided when
a priori knowledge is considered in a Bayesian framework. To exploit the SVM potential

318
Digital Signal Processing with Kernel Methods
RSS 3
RSS 2
RSS 1
Transformation from RSS to xy space
r
y
x
v
Figure 7.12 Conceptual representation of function v = f(r) for three APs.
in the location context from regression algorithms, two ways were proposed by Figuera
et al. () for incorporating a priori knowledge in the SVM design:
) Choosing a suitable kernel, adapted to the speciﬁc task, and consisting of the
multidimensional autocorrelation of RSS space from multiple WiFi nodes.
) Taking advantage of the potential relationship between the two spatial coordinates
(in bidimensional location) and designing a single SVM scheme to provide the two
coordinates simultaneously by means of complex algebra. This takes advantage of
the fact that the two coordinates are in fact strongly correlated.
Regarding the ﬁrst method, the standard kernel is replaced by an alternative one that
incorporates the spatial structure of the training set. This adapted kernel is obtained
as the autocorrelation function relating RSS to spatial position. The second proposal
considers the possible dependence between the two spatial coordinates by applying a
complex SVM formulation (Martínez-Ramón et al., ), so that real and imaginary
parts of the complex output correspond to the x and y coordinates respectively. A third
proposal was developed therein by simply combining the autocorrelation kernel and
the complex output. In this section, we present some relevant results from the work in
Figuera et al. (). The equations of the SVM models can be found detailed in that
reference.
Signal Model for Received Signal Strength Indoor Location
The D location problem based on ﬁngerprinting consists of estimating a function
which provides the user position v = [vx, vy]T from the measured RSS (see Figure .)
as follows:
v = (𝐫) + 𝐞(𝐫),
(.)

Dual Signal Models for Signal Processing
319
Real path
SVM estimated path
KNN estimated path
Figure 7.13 Estimation of the location of a user walking from the office to the elevator and back
obtained with the SVM-CC algorithm and with the k-NN algorithm.
where r is the ℝQ vector containing the RSS from the Q access points (APs) used for
the location services, f is the mapping ℝQ ⟶ℝ, and e accounts for the noise and the
nondeterministic part of the mapping between the RSS and position.
The general procedure of a ﬁngerprinting technique based on SVM methodology is
as follows. First, a set of measurements is registered at L known locations. This set of
measurements is then preprocessed to build the input space for the algorithm. The
output of this process is the training set ≡{A, … , AL} with Al = (rl, vl), where
rl = [r
l , … , rQ
l ]T are the RSS measurements from the available Q APs at location vl.
Using this set the algorithm can then “learn” the dependence between the received
power and the position. For this purpose, several parameters have to be adjusted. The
ﬁrst one is the kernel of the SVM, which in this section can be either the RBF kernel or
the autocorrelation kernel. The rest of the parameters are related to the cost function to
optimize (here, the 𝜖-Huber cost), and they are regularization parameter 𝛿and trade-oﬀ
parameter C.
Usually, the APs used in the location system are placed when the WiFi network is
deployed only for communications purposes. In these cases, when the location system
is implemented, the position of the APs is not a design parameter, but a ﬁxed constraint.
In our setup, the only APs which could be used for location tasks were those represented
in Figure .. We selected AP, AP, and APfor our experiments since they had a full
coverage over the experimental area.
A Complex Support Vector Machine Algorithm
With this general procedure in mind, a complex SVR with autocorrelation kernel is used
to solve the indoor location problem, which is given in Listing ..

320
Digital Signal Processing with Kernel Methods
function [predict,coefs] = ...
SVM_CC(Xtrain,Ytrain,Xtest,gamma,epsilon,C,path_acorr)
% Initialize params
if nargin==7 && ischar(path_acorr)
load(path_acorr) % Precomputed autocorrelation of all available data
else
% Computing autocorrelation with training data
Ytrain = Ytrain(:,1) + 1i * Ytrain(:,2);
if nargin<7; stepA=path_acorr; else stepA=1; end
x = (min(Xtrain(:)):stepA:max(Xtrain(:))+stepA);
axs=cell(1,size(Xtrain,2)); axs(:)={x(:)};
[ACs{1:size(Xtrain,2)}]=ndgrid(axs{:});
interpAxis = griddatan(Xtrain,Ytrain,reshape(cat(2,ACs{:}),numel
(ACs{1}),length(ACs)));
interpAxis(isnan(interpAxis)) = 0;
[acorr,acs] = autocorr(reshape(interpAxis,size(ACs{1})),ACs{:});
Ytrain = [real(Ytrain); imag(Ytrain)];
end
% Construct kernel matrix
H = real(getAutoCorrKernel(Xtrain,Xtrain,acorr,acs{:}));
Htest = real(getAutoCorrKernel(Xtrain,Xtest,acorr,acs{:}));
H = blkdiag(H,H);
Htest = blkdiag(Htest,Htest);
% Train SVM and predict
%params.ggamma=1e-1; params.epsilon=1e-6; params.C=100;
inparams=sprintf('-s 3 -t 4 -g %f -c %f -p %f -j 1', gamma, C, epsilon);
[ypred,model]=SVM(H,Ytrain,Htest,inparams);
Ntest=size(Xtest,1);
Ytestpred = ypred(1:Ntest) + 1i*ypred(Ntest+1:end);
predict = [real(Ytestpred), imag(Ytestpred)];
coefs = getSvmWeights(model);
Listing 7.19 A complex SVR with autocorrelation kernel algorithm (SVM_CC.m).
Note that autocorr function is used to compute the autocorrelation required to
construct the kernel matrix. This autocorrelation kernel matrix is then obtained by the
code in Listings .and ..
function [ac,Taus] = autocorr(f,varargin)
[sizes{1:nargin-1}]=size(varargin{1});
t=cell(1,nargin-1);
for i=1:nargin-1
ind=prod(cell2mat(sizes(1:i-1)));
Ts = varargin{i}(ind+1)-varargin{i}(1);
Rng = max(varargin{i}(:))-min(varargin{i}(:));
t{i} = -Rng:Ts:Rng;
end
if nargin==2
% 1D
ac = xcorr(f);
elseif nargin==3 % 2D
ac = xcorr2(f);
else
% 3D
ac = convn(f,f(end:-1:1,end:-1:1,end:-1:1));
end

Dual Signal Models for Signal Processing
321
ac = ac/max(ac(:));
[Taus{1:length(varargin)}] = ndgrid(t{:});
Listing 7.20 Estimating the autocorrelation (autocorr.m).
function H = getAutoCorrKernel(Xtrain,Xtest,acorr,acx,acy,acz)
[str1,str2]=size(Xtrain);
stst1=size(Xtest,1);
deltaR = zeros(stst1,str1,str2);
for nx=1:stst1
deltaR(nx,:,:) = bsxfun(@minus,Xtest(nx,:),Xtrain);
end
deltaRv = reshape(deltaR,[stst1*str1,str2]);
dRv = num2cell(deltaRv,1);
NN=1e6; % Maximum size for batch execution
NTEST=size(deltaRv,1);
if NTEST<NN; % Batch execution
if str2==2
Hv=interp2(acx',acy',acorr',dRv{:},'*linear');
elseif str2==3
acx=permute(acx,[2 1 3]); acy=permute(acy,[2 1 3]);
acz=permute(acz,[2 1 3]); acorr=permute(acorr,[2 1 3]);
Hv=interp3(acx,acy,acz,acorr,dRv{:},'*linear');
end
else % Obtaining kernel matrix by chunks
Hv=zeros(NTEST,1);
L=ceil(NTEST/NN);
for k=1:L
range=(k-1)*NN+1:min(k*NN,NTEST);
dRv = num2cell(deltaRv(range,:),1);
if str2==2
Hv(range)=interp2(acx',acy',acorr',dRv{:}, '*linear');
else
acx=permute(acx,[2 1 3]); acy=permute(acy,[2 1 3]);
acz=permute(acz,[2 1 3]); acorr=permute(acorr,[2 1 3]);
Hv(range)=interp3(acx,acy,acz,acorr,dRv{:}, '*linear');
end
end
end
H = reshape(Hv,stst1,str1);
Listing 7.21 Getting the autocorrelation kernel matrix (getAutoCorrKernel.m).
The reader is encouraged to check the code presented with the original equations in
Figuera et al. ().
Experiment Settings and Results
In order to test the solution, a min walk was recorded in real time. Figure .
displays the estimation of this walk with the SVM-CC algorithm against the real user
walking, as well as the estimation obtained with the classical k-NN method, which is
used as a reference algorithm. The code for setting the experiment and for visualizing
the estimations is given in supplementary code conﬁg_indoorLocation.m and
display_results_indoorLocation.m.

322
Digital Signal Processing with Kernel Methods
The estimation of the position was made during a walk from an oﬃce to the elevator
and back. The experiment was performed during working hours, so several people
performing normal tasks were present during the experiment. Also, no attempt was
made to control the environmental conditions. For example, orientation of the mobile
device, or whether the doors remained open or closed. Moreover, training and free
parameter tuning were based on the usual measurement settings. These measurements,
taken during day , were obtained under a completely diﬀerent set of conditions
compared with training data. The RSSs of AP, AP, and APare used, and the SVM-
CC is used for the location estimation. The whole path was completed in min, and the
estimation of each location lasted less than ms with a MATLAB program running
on a laptop with a .GHz microprocessor. As can be observed in the ﬁgures, small
acceptable errors were observed along the path.
In summary, including a priori information provided by the autocorrelation of the
training set and using the complex output leads to improvements in the perfor-
mance of the learning algorithm, which in this case is an SVM-based technique.
Among the proposed algorithms, the SVM-CC uses the autocorrelation kernel and
the complex output, and provides the best performance in terms of all the quality
indicators tested. Also, these results show that the combined complex SVM with
autocorrelation kernel is a powerful tool for dealing with multidimensional estimation
problems.
7.4.8
Electroanatomical Maps in Cardiac Navigation Systems
In the cardiac arrhythmia treatment, the knowledge of the arrhythmia mechanisms
is decisive for the application of a successful therapy. Among the current treatments,
the cardiac ablation in electrophysiology (EP) studies is one of the most eﬀective for
some kind of arrhythmia, such as ventricular or supraventricular tachycardia. During
ablation procedures, several catheters are introduced inside the heart with the purpose
of searing the tissue that is producing the arrhythmia using radiofrequency or intense
cold (Morady, ). Until the late s, D X-ray images were used to guide and locate
the catheters inside the heart during EP studies. The projection of bones and the heart
shape were used as reference.
However, this technique did not provide electric information about the arrhythmia,
and the exposure to radiation was a hazard for both the patient and the medical team.
For that reason, cardiac navigation systems (CNSs) were created as an alternative to the
X-ray technique. These systems are able to build a D electroanatomic map (EAM) using
catheters inside the heart, which register and store both their D anatomical position
and the electrical signal associated with each position (Cross et al., ).
EAMs represent the anatomical cardiac surface by means of a triangular mesh
(vertices corresponding to the spatial position joined by triangular faces) and an
electrical feature, such as activation time or voltage amplitude, projected onto the
surface and associated with each vertex. The electrical feature is obtained by processing
the registered electrical signal, a so-called electrogram, measured with the catheters.
These EAMs support the electrophysiologist’s decisions and determining the ablation
targets (Juneja ; Shpun et al. ).
CNSs also show a representation of the catheters inside the EAM with high accuracy
during the EP procedure and allow including relevant anatomical information of cardiac

Dual Signal Models for Signal Processing
323
Figure 7.14 Bipolar (left) and unipolar (right) voltage EAM of a left ventricle generated by the Carto 3®
system (Biosense Webster®).
geometry obtained from the segmentation of computed tomography or magnetic
resonance images (Cross et al., ). Figure .shows the bipolar and unipolar voltage
EAM of a left ventricle during the EP study of a ventricular tachycardia.
Although current CNSs use multi-electrode catheters for the fast acquisition of a
high number of anatomical vertices (the D position), the associated electrical feature
requires several seconds to be registered, and hence not all vertices have an associated
electrical feature. For that reason, an interpolation method is used to estimate the
electrical feature in the vertices where the electrogram was not registered.
The accuracy of the EAM is determined by the number of registered vertices with
associated electrical information and the interpolation method; hence, the aim was
to evaluate visually and quantitatively the eﬀect of the interpolation method in the
electrical feature comparing the EAM provided by the CNS with the EAM obtained
with three interpolation methods; namely, () linear interpolation (LI), () thin-plate
spline (TPS), and () the 𝜈-SVR with RBF kernel.
Methods
The LI method creates ﬁrst a Delaunay triangulation formed by a set of adjacent,
continuous, and nonoverlapping triangles, which represent a convex surface of the set of
vertices vi. A triangulation is a Delaunay triangulation when the circumcircle about each
triangle contains no other vertices in its interior. It has several advantageous properties;,
for instance, it maximizes the minimum angle in the plane or it is the convex hull of
the vertices. Readers are referred to de Berg et al. () for a detailed explanation
of the Delaunay triangulation properties. The value of the new interpolated vertex is

324
Digital Signal Processing with Kernel Methods
0
8
6
4
4
6
8
2
1
2
3
4
(a)
(b)
0
10
10
5
5
0
0
1
2
3
4
Figure 7.15 (a) Example of function f(x,y) = sin(x) + sin(y) + 2 used to evaluate the analyzed
interpolation methods (i.e., LI, TPS, and 𝜈-SVR). The vertices (samples) used to interpolate are marked
with blue points. (b) LI of the function in (a) using a Delaunay triangulation and a set of 12 vertices
(blue points). First, the triangulation is computed from the vertices, and then the value of new vertex
(red asterisk) is obtained by using the triangle which encloses this new vertex.
obtained by ﬁnding the triangle which encloses it and computing the weighted sum
of the corresponding three vertices associated with the enclosed triangle. Figure .b
shows an example of the Delaunay triangulation for a set of selected vertices (marked
with blue points) from the function f (x, y) = sin(x) + sin(y) + (see Figure .a), and
the LI of a new vertex (marked with a red asterisk) by using this triangulation.
The TPS method estimates a thin, smooth surface that passes through all given
vertices. This surface is constructed by selecting a function f minimizing the bending
energy of a surface (Bookstein ; Friedman et al. ; Powell ). Thus, the name
TPS refers to the analogy of bending a thin sheet of metal. For a set of N vertices in ℝ,
where vi = [xT
i , yT
i ]T and zi = f (vi) = f (xi, yi), the functional to minimize is deﬁned as
N
∑
i=
(hi −f (vi))+ 𝜆I(f ),
(.)
where I( f ) is a penalty functional, which takes the following form in ℝ:
I( f ) = ∫∫ℝ
(df
dx+ df
dx dy + df
dy
)
dx dy.
(.)
𝜆is a positive constant that controls the smoothing in order to relax the interpolation
in the presence of noise. 𝜆= corresponds to the interpolation function without
regularization (no smoothing), whereas 𝜆→∞shows a least-squares plane solution.
Figure .shows the interpolation of the aforementioned set of vertices using the
TPS method and diﬀerent 𝜆values (i.e., , , , , , and ). Meanwhile, the
surface passes through all the vertices (no regularization) for 𝜆= ; the surface is a
plane for 𝜆= .

Dual Signal Models for Signal Processing
325
0
8
6
4
8
6
4
2
1
2
3
λ= 0
0
8
6
4
8
6
4
2
1
2
3
λ = 1
0
8
6
4
8
6
4
2
1
2
3
λ=10
λ = 100
0
8
6
4
8
6
4
2
1
2
3
λ = 1000
0
8
6
4
8
6
4
2
1
2
3
0
8
6
4
8
6
4
2
1
2
3
λ=10000
Figure 7.16 TPS interpolation of the function f defined in Figure 7.15a using 𝜆= 0, 1, 10, 100, 1000,
and 10 000.
The solution to this minimization problem is as follows:
f (vj) = 𝛽+ 𝜷Tvj +
N
∑
i=
𝛼is(r),
(.)
where s(r) = rlog(r), r = ‖vj −vi‖ is an RBF expression, 𝛽is a constant, 𝜷represent a
linear regression (i.e., 𝛽xxi + 𝛽yyi), and the coeﬃcients 𝜶i have a set of constraints:
N
∑
i=
𝜶i = ;
N
∑
i=
𝜶ixi = ;
N
∑
i=
𝜶iyi = .
(.)
The solution can also be obtained by following a matrix notation:
[
K + 𝜆I
PT
P

] [
𝜶
𝜷
]
=
[
z
o
]
,
(.)
where K is an N × N matrix, the elements of K are deﬁned as Kij = s(rij) = s((‖vj −
vi‖)log(‖vj −vi‖). P is a × N matrix where the ith row of P is (, xi, yi). is a × 
matrix of zeros, z is a column vector of target values, and o is a column vector of three
zeros. The column vectors 𝜶and 𝜷correspond to the coeﬃcients of Equation .and
they can be computed by solving the following matrix equation:
[
𝜶
𝜷
]
=
[
K + 𝜆I
PT
P

]−[
z
o
]
.
(.)

326
Digital Signal Processing with Kernel Methods
Although TPS was initially deﬁned for a D interpolation, the formulation can be
extended to higher dimensions (Goshtasby, ).
The 𝜈-SVR is a learning algorithm based on the SRM principle (Chang and Lin
; Schölkopf et al. ). This method estimates a regression hyperplane in a high-
dimensional space using the nonlinear mapping 𝝓(v) of the vertex coordinates (as input
vectors). The regression function is deﬁned as
f (vi) = ⟨w, 𝝓(vi)⟩+ b + e,
(.)
where w is a linear weight vector, b is the bias term of the nonlinear regression data
model, and e is the error term. In order to obtain both w and b in the previous model,
the functional to minimize in the 𝜈-SVR is as follows:
min
w,𝜖,𝝃(∗),b

‖w‖+ C𝜈𝜖+ 
N
N
∑
i=
(𝜉i + 𝜉∗
i )
subject to
(⟨w, 𝝓(vi)⟩+ b) −zi ≤𝜖+ 𝜉i,
zi −(⟨w, 𝝓(vi)⟩+ b) ≤𝜖−𝜉∗
i ,
𝜉(∗)
i
≥, 𝜖≥,
(.)
where i = , … , N, C ∈(, ∞) is the regularization parameter, and 𝜈∈(, ) parameter
is an upper bound on the fraction of margin errors and it is also used to control the
amount of support vectors in the resulting model; that is, it is a lower bound on the
fraction of support vectors.
The 𝜖-insensitive loss function is used and 𝜖is automatically minimized (Schölkopf
et al., ), and 𝜉is a positive slack variable. Including the constraints in Equation .,
the dual form of the problem is obtained and the Lagrangian is deﬁned as
L(w, 𝝃(∗), b, 𝜖, 𝜶(∗), 𝜷, 𝜂(∗)) = 
‖w‖+ C𝜈𝜖+ C
N
N
∑
i=
𝜂i𝜉i + 𝜂∗
i 𝜉∗
i
−
N
∑
i=
𝛼i(𝜉i + zi −(wT𝝓(vi)) −b + 𝜖)
−
N
∑
i=
𝛼∗
i (𝜉∗
i + (wT𝝓(vi)) + b −zi + 𝜖),
where (𝛽i, 𝛼(∗)
i , and 𝜂(∗)) ≥are the Lagrange multipliers. This function has to be
maximized with respect to the dual variables 𝛽i, 𝛼(∗)
i , and 𝜂(∗). The KKT conditions are
obtained by deriving with the respect to the primal variables and equating to zero. A new
problem of optimization, known as the Wolfe dual problem, is achieved by substituting
the previous conditions into L.

Dual Signal Models for Signal Processing
327
Rewriting the constraints and substituting a kernel K for the dot product – that is,
K(vi, vj) = 𝜙(vi) ⋅𝜙(vj) – the 𝜈-SVR optimization problem is deﬁned as
maximize
𝛼∗
i ,𝛼i
W(𝜶(∗)) =
N
∑
i=
(𝛼∗
i −𝛼i)yi −

N
∑
i,j=
(𝛼∗
i −𝛼i)(𝛼∗
j −𝛼j)K(vi, vj)
subject to
N
∑
i=
(𝛼∗
i −𝛼i) = ,
𝛼(∗)
i
∈
[
, C
N
]
,
N
∑
i=
(𝛼∗
i + 𝛼i) ≤C ⋅𝜈.
The regularization parameter C, the percentage of support vectors 𝜈, and the Gaussian
width 𝜎have to be searched and tuned during the training process. Finally, Equa-
tion .takes the form
f (v) =
N
∑
i=
(𝛼∗
i −𝛼i)K(vi, v) + b,
(.)
where b can be computed by using the constraints of Equation .and 𝜉(∗)
i
= . Here,
we use an RBF kernel; that is, K(vi, vj) = exp(−‖vi −vj‖∕(𝜎)), where 𝜎∈ℝ+.
Figure .shows an example of diﬀerent values of C, 𝜈, and 𝜎when 𝜈-SVR estimates
the function shown in the Figure .a. Overﬁtting occurs for low values of 𝜎(i.e.,
𝜎
=
.or .) because a very narrow Gaussian is used in the model and the
regression model is being adjusted exclusively to the training data. When 𝜎values are
increased, the estimated interpolation function approximates better to the real one until
the regression model starts to soften the interpolation function. Low values of the C
parameter also soften the interpolation function. For values of C = or , the
interpolated function is better approximated to the original one. For the 𝜈parameter,
the interpolation function is obtained when 𝜈≥..
Results and Conclusions
Visual and quantitative comparisons of the EAM obtained with the three interpolation
methods LI, TPS, and 𝜈-SVR are presented and compared with the EAM of a CNS.
The CNS provides N irregularly spaced vertices V = {v, … , vN}, where vi ∶=
[xi, yi, zi] and xi, yi and zi are the D coordinates of the vertex i. These vertices have
associated a measured and known electrical feature hi (activation time or voltage
amplitude); that is, hi = f (vi). This set of vertices allows us to deﬁne the function f
by using the LI, TPS, and 𝜈-SVR interpolation methods.
Also, the CNS provides a triangular mesh (surface) with a new set of m irregularly
spaced vertices U = {u, u, … , um}, which describes the anatomy of the cardiac
chamber. In this new set of vertices, the spatial position of the vertices is known but
the electrical feature is not registered; however, the CNS interpolates the feature with

328
Digital Signal Processing with Kernel Methods
0
8
6 4
8
6
4
1
2
2
3
(a)
(b)
(c)
C= 1
0
8
6 4
8
6
4
1
2
2
3
C=10
0
8
6 4
8
6
4
1
2
2
3
C = 100
0
8
6 4
8
6
4
1
2
2
3
C = 1000
0
8
6 4
8
6
4
1
2
2
3
0
8 6
4
8
6
4
1
2
2
3
ν=0.05
σ=0.1
0
8 6
4
8
6
4
1
2
2
3
σ =0.5
0
8 6
4
8
6
4
1
2
2
3
σ =1
0
8 6
4
8
6
4
1
2
2
3
σ= 2
0
8 6
4
8
6
4
1
2
2
3
σ= 3
0
8 6
4
8
6
4
1
2
2
3
σ= 4
ν =0.3
ν= 0.55
ν= 0.8
0
8
6 4
8
6
4
1
2
2
3
0
8
6 4
8
6
4
1
2
2
3
0
8
6 4
8
6
4
1
2
2
3
Figure 7.17 𝜈-SVR interpolation of the function f defined in Figure 7.15a using different values of C, 𝜈,
and 𝜎: (a) C = [1, 10, 100, 1000], 𝜈= 0.5, and 𝜎= 2; (b) C = 100, 𝜈= [0.05, 0.3, 0.55, 0.8], and 𝜎= 2;
(c) C = 100, 𝜈= 0.5, and 𝜎= [0.1, 0.5, 1, 2, 3, 4].
its own interpolation method and the set of vertices v in order to show a representation
of spatial anatomical distribution of the electrical feature in this mesh.
As was mentioned earlier, a CNS is able to register the spatial position of the catheters
inside the heart instantaneously, but the electrical feature is registered by capturing
the electrical signal during several seconds; hence, the number of vertices in the set
V, which includes spatial position and measured feature, is lower than the number of
vertices in U; that is, N ≪m. Note that both sets of vertices V and U have to be diﬀerent
and they have not to lie on a straight line, and this means N ≥and m ≥to avoid
incorrect estimations. In this application, the set of vertices V is used as training data
and the set of vertices U as test data.
Ten bipolar voltage and activation time EAMs from CNSs were used for the
evaluation of these three methods. The CNS EAMs have an average and standard
deviation of .±.vertices for the set V and .±.vertices for the set U
in the bipolar voltage EAMs, and .±.vertices for the set V and .±.

Dual Signal Models for Signal Processing
329
vertices for the set U in the activation time EAMs. The MSE between the interpolated
feature and the feature provided by the CNS for the set of vertices U is used as a merit
ﬁgure. Note that the feature provided by the CNSs is not a gold standard because
these systems use their own interpolation algorithm to build the EAM; however, it
is the EAM used by the electrophysiologist to apply the treatment and it is a quality
reference.
As deﬁned in Chapter , the script to start the execution is run.m, while the
additional code needed is included in the supplementary material: data preprocessing
and training–test split in genDataEAM.m, while conﬁg_ﬁle.m and conﬁg_EAM.m
contain conﬁguration ﬁles. In particular, the function genDataEAM.m reads the .mat
ﬁles, which contain the set of vertices V and U. Note that the set of vertices U has
several vertices with the feature value equal to −, which corresponds to nonactive
vertices. Although these vertices are not considered in the evaluation of the methods,
they are not deleted because they are included in the triangular mesh. The function
conﬁg_EAM.m deﬁnes the path of data and the CNS color map, the interpolation
method functions with their associated parameters, the range of the parameters for
these methods, the strategy of search the optimum parameter and the function to
display the results. In this application, the tuning of the 𝜆parameter for the TPS, and C,
𝜈, and 𝜎parameters for 𝜈-SVR is done by searching the most appropriate value following
a grid-search strategy deﬁned in gridSearch.m.
The LI and TPS interpolation methods are deﬁned in Listings .and .respec-
tively. Listing .shows the 𝜈-SVR interpolation method
function [predict, coefs] = LI(Xtrain,Ytrain,Xtest,~)
F=scatteredInterpolant(Xtrain(:,1),Xtrain(:,2),Xtrain(:,3),
Ytrain,'linear');
predict=F(Xtest(:,1),Xtest(:,2),Xtest(:,3));
coefs=[];
Listing 7.22 LI method (LI.m).
function [predict,coefs] = TPS(Xtrain,Ytrain,Xtest,lambda)
[N,D]=size(Xtrain);
Ntest=size(Xtest,1);
% Contruct kernel matrix
distances = sqrt(dist2(Xtrain,Xtrain));
K = K_matrix(distances);
P = [ones(N,1) Xtrain];
% Train
K_reg = K + lambda*eye(size(K,1));
L = [K_reg P; P' zeros(D+1,D+1)];
Q = [Ytrain; zeros(D+1,1)];
coefs = L\Q;
% Predict
distances = sqrt(dist2(Xtest,Xtrain));
matrix_eta= K_matrix(distances);
predict = [ones(Ntest,1) Xtest]*coefs(N+1:end)+matrix_eta*coefs(1:N);
Listing 7.23 TPS method (TPS.m).
The functions dist2.m and K_matrix.m are deﬁned in Listings .and ..

330
Digital Signal Processing with Kernel Methods
function distance = dist2(x1,x2)
n1 = size(x1,1);
n2 = size(x2,1);
distance = (ones(n2,1) * sum((x1.^2)', 1))' + ...
ones(n1,1) * sum((x2.^2)',1) - ...
2.*(x1*(x2'));
distance(distance<0) = 0;
Listing 7.24 Distance 𝓁2 computation (dist2.m).
function K = K_matrix(r)
[controlP,newP] = size(r);
new_r = r(:);
K = zeros(size(r));
repVert = find(new_r>0);
K(repVert) = single(log(r(repVert).^2).*r(repVert).^2);
K = reshape(K,controlP,newP);
Listing 7.25 K_matrix (K_matrix.m).
function [predict,coefs] = SVR(Xtrain,Ytrain,Xtest,sigma,nu,C)
% Train SVM and predict
gamma = 1/(2*sigma)^2;
inparams=sprintf('-s 4 -t 2 -g %f -c %f -n %f -j 1',gamma,C,nu);
[predict,model]=SVM(Xtrain,Ytrain,Xtest,inparams);
coefs = getSvmWeights(model);
Listing 7.26 Code for training the 𝜈-SVR (SVR.m).
Finally, the function to store the results and display the MSE results and the visual
interpolation is display_results_EAM.m, which is given in the supplementary
material.
Table .shows the average and the standard deviation of the MSE between the
interpolated feature and the feature provided by the CNS for the set of vertices U and
for each interpolation method. LI and TPS yielded better performance than 𝜈-SVR for
both bipolar voltage and activation time EAMs. Activation time EAMs also yielded
lower MSE than bipolar voltage EAMs. Note that this comparison is made by using
the EAM provided by the CNS and used by electrophysiologist, but it is not the gold
standard. Figures .and .show the original and the interpolated bipolar voltage
and activation time EAMs using LI, TPS, and 𝜈-SVR.
Table 7.10 Average and standard deviation MSE between the electrical feature
provided by the CNS and the interpolated feature with LI, TPS, and 𝜈-SVR methods
using 10 bipolar voltage EAMs and 10 activation time EAMs.
EAM
LI
TPS
ν-SVR
Bipolar voltage (mV)
.± .
.± .
.± .
Activation time (ms)
.± .
.± .
.± .

Dual Signal Models for Signal Processing
331
0
0.5
1
1.5
Bipolar voltage (mV)
2
2.5
Figure 7.18 Original and interpolated bipolar voltage (mV) EAMs using LI, TPS, and 𝜈-SVR (from left to
right). The black points show the position of the known feature positions.
–20
0
20
Activation time (msec)
40
60
80
Figure 7.19 Original and interpolated activation time (ms) EAMs using LI, TPS, and 𝜈-SVR (from left to
right). The black points show the position of the known feature positions.
Slight diﬀerences between EAMs are presented mainly in the places where there are
less vertices with a measured feature. In general, LI and TPS yielded better performance
than 𝜈-SVR in terms of similarity to the EAM provided by the CNS, but the EAM
accuracy is dependent on the EAM type, on the cardiac chamber, and on the number
of vertices with an associated measured feature (size of set v).
7.5
Questions and Problems
Exercise ..
Show that heq
n fulﬁlls the properties to be a Mercer kernel.
Exercise ..
Show that D and D autocorrelation functions are valid Mercer
kernels.

332
Digital Signal Processing with Kernel Methods
Exercise ..
Show that the multidimensional Lomb periodogram provides a suit-
able expression for its use as Mercer kernel.
Exercise ..
Can you propose and implement two more ways for estimating mul-
tidimensional autocorrelation functions? Hint: think on multidimensional spline inter-
polation, as well as on nearest neighbors interpolation.
Exercise ..
Starting from the sinc nonuniform interpolation code provided, make
the necessary modiﬁcations to benchmark the impact of using the following three
methods for estimating the single-dimensional autocorrelation kernel from data: (a)
Lomb periodogram; (b) splines; (c) k-NN. What is the impact on the interpolation
quality of the method used?
Exercise ..
For the indoor location example, check the impact on the performance
of each of the methods presented for estimating the multidimensional autocorrelation.
Explain the observed diﬀerences with the single-dimensional case.
Exercise ..
For the HRV code, benchmark the impact of using a single signal for
the estimation of the autocorrelation kernel (instead of an averaged one). Explain the
results obtained.

333
8
Advances in Kernel Regression and Function Approximation
8.1
Introduction
Kernel methods constitute a proper framework to tackle regression problems that
encompass ﬁtting and regularization. In this chapter we will pay attention to two
particularly interesting ways of treating the regression problem: based on discriminative
kernel regression, and based on generative Bayesian nonparametric regression. Both
families have found wide application in signal processing in the last decade (Pérez-Cruz
et al., ; Rojo-Álvarez et al., ).
The chapter is divided into two main sections. In Section ., we will depart from the
standard SVR method (Smola and Schölkopf, ) extensively used in the previous
chapters. The method allows many alternative formulations to deal with the speciﬁcities
of the DSP problems; for example, dealing with multiple outputs, unlabeled data,
and signal-dependent noise sources and heteroscedastic models. Recent approaches
to treat such problems will be introduced and experimentally evaluated. In all of
them, the important role of both the loss and the regularizer will appear, as well as
the design of the kernel function. In Section ., as an alternative to this treatment
based on the SVR algorithm, we will review two instantiations of the ﬁeld of Bayesian
nonparametrics: relevance vector machines (RVMs) (Tipping, ) and Gaussian
processes (Rasmussen and Williams, ). We study the important issue of model
selection and hyperparameters search procedures in both Sections .and .. Then in
Section .we illustrate the algorithms presented in both synthetic and real application
examples.
8.2
Kernel-Based Regression Methods
This section treats the problem of regression and function approximation with kernels.
The basic approach here is to cast the problem in a supervised way. As we have
seen before, a large class of regression problems in particular are deﬁned as the joint
minimization of a loss function accounting for errors of the function f
∈to
be learned, and a regularization term 𝛺(f ) that controls its smoothness and, as a
consequence, its excess of ﬂexibility. More speciﬁcally, in this chapter we focus on the
problem of inferring the function f (x) ∶⊆ℝd →ℝ, given a set of observed data pairs
{(xi, yi)}N
i=, where x ∶= [x, … , xd]T ∈ℝd, y ∈ℝ, and we assume an additive noise
Digital Signal Processing with Kernel Methods, First Edition. José Luis Rojo-Álvarez, Manel Martínez-Ramón,
Jordi Muñoz-Marí, and Gustau Camps-Valls.
© John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.

334
Digital Signal Processing with Kernel Methods
observation model, yi = f (xi) + ei, i = , … , N, where ei represents the perturbation
errors. As we have see in previous chapters, the problem of inferring the function f
reduces in general to selecting a class of models and to optimizing a functional that
considers both a ﬁtting cost or loss function and a regularizer term.
We also saw that there exist several possible choices of both the loss and the
regularizer, each one leading to diﬀerent kernel regression algorithms; see Chapter .
8.2.1
Advances in Support Vector Regression
The SVR is the SVM implementation for regression and function approxima-
tion (Schölkopf and Smola, ; Smola and Schölkopf, ); see Chapter . The
standard SVR formulation uses Vapnik’s 𝜀-insensitive cost function
𝜀(e) = C max(, |e| −𝜀),
C > ,
(.)
in which an error e up to 𝜀is not penalized, otherwise a linear penalization will
be incurred. The regression estimation problem is regarded as ﬁnding the mapping
between an incoming vector x ∈ℝd and an observable (unidimensional) output y ∈ℝ,
from a given training set of N data pairs, {(xi, yi)}N
i=. The standard SVR (Smola and
Schölkopf ; Smola et al. ; Vapnik ) solves this problem by ﬁnding the
weights of a linear regressor f (x) = wT𝝓(x) + b – that is, w and b – where as usual 𝝓(⋅)
is a feature mapping to a higher (possibly inﬁnite-dimensional) Hilbert space. Brieﬂy,
SVR estimates weights w by minimizing the following regularized functional:
min
w,b
{
C
N
∑
i=
𝜀(yi −wT𝝓(xi) −b) + 
‖w‖
}
,
(.)
where C is the penalization parameter applied to the errors and acts as a trade-oﬀ
parameter between the minimization of errors and smoothness of the solution (ﬂatness
of the weights, ‖w‖). Solving the problem with arbitrary losses is, in general, done by
introducing the Lagrange theory of multipliers. In the case of the 𝜀-insensitive cost, we
deﬁne in the following 𝜉i and 𝜉(∗)
i
as positive slack variables, to deal with training samples
with a prediction error larger than 𝜀. Then, the problem in Equation .can be shown
to be equivalent to the constrained optimization problem with objective function
min
w,b,𝜉i,𝜉∗
i
{
C
N
∑
i=
(𝜉i + 𝜉∗
i ) + 
‖w‖
}
(.)
constrained to
yi −⟨w, 𝝓(xi)⟩−b ≤𝜀+ 𝜉i
∀i = , … , N
(.)
⟨w, 𝝓(xi)⟩+ b −yi ≤𝜀+ 𝜉∗
i
∀i = , … , N
(.)
𝜉i, 𝜉∗
i ≥
∀i = , … , N.
(.)
This is a QP problem. The usual procedure for solving SVRs introduces the linear
restrictions in Equations .–.into Equation .using Lagrange multipliers 𝛼i and
𝛼∗
i (one per inequality constraint in Equation .and .respectively), computes the

Advances in Kernel Regression and Function Approximation
335
KKT conditions, and solves the dual problem using QP procedures (Fletcher, ),
which yields
w =
N
∑
i=
(𝛼i −𝛼∗
i )𝝓(xi),
(.)
which expresses the solution as a linear combination of the mapped examples. Note
that the primal problem, whose solution is essentially expressed as a function of w,
was transformed into a dual problem expressed as a function of the dual weights
𝜶= [𝛼−𝛼∗
, … , 𝛼N −𝛼∗
N]T. These weights are directly obtained from solving the
corresponding problem; see Chapter . Note that one can easily obtain the prediction
for an input test xj as
̂yj = f (xj) = ⟨w, 𝝓(xj)⟩=
N
∑
i=
(𝛼i −𝛼∗
i )⟨𝝓(xi), 𝝓(xj)⟩=
N
∑
i=
(𝛼i −𝛼∗
i )K(xi, xj).
(.)
As in the SVM classiﬁer, here only the examples with nonzero multipliers are relevant
for regression, and are called support vectors. Sparsity in the SVR is a direct consequence
of the loss function adopted; as the value of 𝜀increases, the number of support vectors
is reduced. The SVR methodology allows us to develop new, powerful SVR methods by
modifying the functional. Let us now see two simple modiﬁcations.
The ε-Huber Support Vector Regression
In the previous chapters we extensively focused on the use of the 𝜀-Huber loss 𝜀H(e)
instead of the Vapnik’s 𝜀(e). The 𝜀-Huber cost is expressed as
𝜀H(e) =
⎧
⎪
⎨
⎪⎩
,
|e| ≤𝜀

𝛿(|e| −𝜀),
𝜀≤|e| ≤eC
C(|e| −𝜀) −
𝛿C,
|e| ≥eC,
(.)
where eC = 𝜀+ 𝛿C; 𝜀is the insensitive parameter, and 𝛿and C control the trade-oﬀ
between the regularization and the losses. The cost considers a particular combination
of three losses: insensitive for tolerated errors, quadratic to deal with Gaussian noise,
and linear cost to cope with outliers. Once optimized, the corresponding optimal
parameters 𝜀, 𝛿, and C yield some insight on the signal and noise characteristics.
Interestingly, introducing this cost function does not alter much the QP problem to
be solved; see Section ... This particular SVR was successfully used in several signal
processing models in Chapters –.
The ν-Support Vector Regression
Another alternative functional was proposed in Schölkopf et al. (). Here, the tube
size 𝜀is traded oﬀagainst model complexity and slack variables via an additional
parameter 𝜈≥:
min
w,b,𝜉i,𝜉∗
i
{
C
N
∑
i=
(𝜉i + 𝜉∗
i ) + CN𝜈𝜀+ 
‖w‖
}
(.)

336
Digital Signal Processing with Kernel Methods
subject to
yi −⟨w, 𝝓(xi)⟩−b ≤𝜀+ 𝜉i
⟨w, 𝝓(xi)⟩+ b −yi ≤𝜀+ 𝜉∗
i
𝜉i, 𝜉∗
i ≥
𝜀≥,
(.)
where 𝜈acts as an upper bound on the fraction of errors and a lower bound on the
fraction of support vectors (Schölkopf et al., ). The so-called 𝜈-SVR formulation
reduces to solving again a QP problem. Here, we show an alternative procedure
known as the iterated reweighted LS (IRWLS), which has already been demonstrated
to be more eﬃcient in both time and memory requirements (Pérez-Cruz and Artés-
Rodríguez, ). This procedure is based on the fact that a Lagrangian can be con-
structed with the form
LP = 
‖w‖+ 

N
∑
i=
(aie
i + a∗
i (e∗
i )) + 𝜀(CN𝜈)
(.)
with the deﬁnitions
ei = yi −⟨w, 𝝓(xi)⟩−b −𝜀
e∗
i = ⟨w, 𝝓(xi)⟩+ b −yi −𝜀
ai =
𝛼i
yi −⟨w, 𝝓(xi)⟩−b −𝜀= 𝛼i
ei
a∗
i =
𝛼∗
i
⟨w, 𝝓(xi)⟩+ b −yi −𝜀=
𝛼∗
i
e∗
i
(.)
(see Pérez-Cruz and Artés-Rodríguez () for the derivation details). The optimiza-
tion consists in minimizing Equation .with respect to w, b, and 𝜀, while keeping ai
and a∗
i constant, and then recalculating ai and a∗
i with
a(∗)
i
=
⎧
⎪
⎨
⎪⎩
,
e(∗)
i
< 
C
e(∗)
i
,
e(∗)
i
≥,
(.)
where ei = yi −⟨w, 𝝓(xi)⟩−b−𝜀and e∗
i = ⟨w, 𝝓(xi)⟩+b−yi −𝜀. The process is repeated
until convergence.
The ﬁrst step, consisting of computing the gradient of Equation .with respect
to w, b and 𝜀, can be solved in block. By virtue of the representer’s theorem
(see Section ..), w is a linear combination of a subset of training samples
w = ∑N
i=𝛽i𝝓(xi). By making this substitution in the following, we can write the
following algorithm:

Advances in Kernel Regression and Function Approximation
337
) Solve the linear system:
⎡
⎢
⎢⎣
K + D−
(a+a∗)
𝟏
E
𝟏T


ET


⎤
⎥
⎥⎦
⎡
⎢
⎢⎣
𝜷
b
𝜀
⎤
⎥
⎥⎦
=
⎡
⎢
⎢⎣
y

CN∗𝜈
⎤
⎥
⎥⎦
,
(.)
where we have deﬁned
Kij = ⟨𝝓(xi)𝝓(xj)⟩= K(xi, xj)
(D−
a+a∗)ij = 𝛿(i −j)
ai + a∗
i
E =
[a−a∗

a+ a∗

, … ,
aN −a∗
N
aN + a∗
N
]T
.
) Recompute ai and a∗
i with Equation ..
) Repeat until convergence.
The column vectors y, a, a∗, 𝜷, and 𝟏present the obvious expressions, and K is known
as the kernel matrix, since it is only formed by inner products of the training samples
in the feature space. Consequently, neither the minimizing procedure nor the use of
the regressor needs to know the explicit form of the nonlinear mapping 𝝓(⋅), but
only its kernel representation K(⋅, ⋅). The transformations needed to obtain the IRWLS
procedure from the minimization of Equation .are detailed in Pérez-Cruz and Artés-
Rodríguez ().
The Profile-Dependent Support Vector Regression
The proﬁle-dependent SVR (PD-SVR) exploits the previous 𝜈-SVR to tailor speciﬁc
penalization C and tolerated error 𝜀for every example (Camps-Valls et al., ).
The inspiration comes from the classiﬁcation setting. In classiﬁcation problems with
unbalanced classes one typically sets diﬀerent penalization factors for each class (Lin
et al., ; Orr and Müller, ). This way, the SVM learns to classify patterns
independently from the class they belong to, which is not possible when using an overall
constraint C. In the ﬁeld of time-series prediction, we can extrapolate this fact and
consider that the most recent samples contain, in principle, more information and con-
sequently should receive more penalization. Therefore, problems with nonstationary
processes can be alleviated using a diﬀerent penalization factor for each training sample
i according to a certain conﬁdence function ci on the samples. This idea can be extended
by using a diﬀerent margin (insensitivity zone) for each sample. This allows the SVR
machine to follow the pdf variations over time.
The PD-SVR functional implies a simple modiﬁcation of the original SVR:
min
w,𝜉i,𝜉∗
i
{
C
∑
i
ci(𝜉i + 𝜉∗
i ) + 
‖w‖
}
(.)

338
Digital Signal Processing with Kernel Methods
where we introduce a penalization per example ci, and now restrictions over slack
variables become sample-dependent too:
yi −𝝓T(xi)w −b ≤𝜀
ci
+ 𝜉i
∀i = , … , N
(.)
𝝓T(xi)w + b −yi ≤𝜀
ci
+ 𝜉∗
i
∀i = , … , N
(.)
𝜉i, 𝜉∗
i ≥
∀i = , … , N.
(.)
With regard to the 𝜈-SVR (Schölkopf et al., ), the primal function becomes
min
w,𝜉i,𝜉∗
i
{
C
N
∑
i=
ci(𝜉i + 𝜉∗
i ) + CN∗𝜈𝜀+ 
‖w‖
}
,
(.)
where N∗= ∑N
i=ci in order to restrict 𝜈to be in the range [, ]. By including linear
restrictions in Equations .–.in the corresponding functional, we can rewrite
Equation .or .to its dual description, which once again constitutes a QP problem,
which can be solved with the eﬃcient IRWLS procedure as well.
New Support Vector Regression Algorithms
The previous SVR formulation has allowed developing powerful algorithms by modi-
fying the loss function or the regularizer. An alternative pathway to develop improved
SVR methods is to focus on introducing new kernel functions that respect the signal
structure and characteristics. The ease of algorithmic design and the convex solutions
provided yielded a plethora of exciting developments. Interestingly, the SVR methodol-
ogy allows us to go beyond, and to study key concepts in DSP, such as the signal and
noise characteristics and relations, the heteroscedasticity of the noise, or to exploit
unlabeled data to improve regression and to accommodate multi-output problems.
We will present techniques to study precisely these issues in the next sections: the
kernel signal-to-noise regression (KSNR) to cope with signal and noise dependencies as
an eﬃcient form of regularization, the semi-supervised SVR (SS-SVR) to incorporate
the manifold structure through the use of unlabeled data, and the multi-output SVR
(MSVR) to deal with several, eventually correlated output variables.
8.2.2
Multi-output Support Vector Regression
Let us now treat a more challenging problem than single-output regression. The so-
called MSVR was introduced in Pérez-Cruz et al. () and Matifor DSP and
MIMO channel equalization, and later applied to remote sensing and geoscience prob-
lems of multiple biophysical parameter estimation in Tuia et al. (a). Notationally,
imagine that we aim to predict Q > several variables simultaneously. Of course,
a trivial approach is to develop Q diﬀerent SVR models to approximate each one of
the variables. This, however, has two main problems: the computational cost of both
training and testing and, perhaps more importantly, the fact that the output variable
relations are neglected. In the case the observable output is a vector with Q variables to
be predicted, y ∈ℝQ, a more appropriate strategy is to solve a multidimensional regres-
sion estimation problem, in which we have to ﬁnd a regressor w = [w, … , wM]T ∈ℝM

Advances in Kernel Regression and Function Approximation
339
per output variable; that is, a weight matrix W = [w| ⋯|wQ] ∈ℝM×Q. Then one can
directly generalize the D SVR to solve the multidimensional case, whose problem can
be stated as the minimization of the following cost function:
min
W
{
C
N
∑
i=
L(ui) + 
‖W‖
}
= min
wk
{
C
N
∑
i=
L(ui) + 

Q
∑
k=
‖wk‖
}
,
(.)
where
L(u) =
{
,
u < 𝜀
u−u𝜀+ 𝜀,
u ≥𝜀,
(.)
ui = ‖ei‖ =
√
eT
i ei, and the residuals are computed as ei = yi −W T𝝓(xi). Note that we
here use the cost L(u). This is due to the fact that extending the Vapnik 𝜀cost function
to multiple dimensions makes the problem complexity grow linearly with the number
of dimensions (besides, note that the inequality constraints should be coupled for all
pairs of data points). If, instead of this piecewise 𝓁-norm, we use a 𝓁-based norm as
the one in L(u), all dimensions can be considered into a unique restriction yielding a
single support vector for all dimensions.
For 𝜀= this problem reduces to an independent regularized kernel LS regression
for each component, but for a nonzero 𝜀the solution takes into account all outputs to
construct each individual regressor. This way, a structured-output model is built and
the cross-output relations are exploited, thus leading to obtain in principle more robust
predictions. The price to pay is that the resolution of the proposed problem cannot
be done straightforwardly and we have to rely on an iterative procedure to obtain the
desired solution. A quasi-Newton approach has been devised, in which each iteration
has at most the same computational complexity as an LS procedure for each component.
In particular, the MSVR is solved using the IRWLS explained before, which boils down
to a weighted LS problem, and the number of iterations needed to obtain the ﬁnal result
is typically small, making the procedure only slightly more computationally demanding
than LS regression for each component.
8.2.3
Kernel Ridge Regression
An alternative formulation to SVR consists of replacing the 𝓁with an 𝓁-norm cost
function. Such a kernel-based method is known as KRR (also known as the LS SVM),
and can be seen as the kernel version of the regularized LS linear regression (Shawe-
Taylor and Cristianini ; Suykens et al. ). Here, we introduce a simple multi-
output formulation, similarly to the framework considered in the previous subsection.
Notationally, let xi ∈ℝd (for simplicity, here we consider ⊆ℝd) and the multiple-
output yi ∈ℝQ, i = , … , N. We deﬁne an output (target) matrix Y = [y| ⋯|yN]T ∈
ℝN×Q. We want to perform a regularized linear LS regression where the samples have
been transformed by the nonlinear mapping 𝝓(x) ∈. Now, using all mapping
functions for all examples xi, we deﬁne data matrix 𝜱= [𝝓(x)| ⋯|𝝓(xN)]T and weight
matrix W = [w| ⋯|wQ], where column vectors wk ∈.

340
Digital Signal Processing with Kernel Methods
The assumed observation model in this case is
Y = 𝜱W + E,
(.)
where E is an i.i.d. Gaussian noise, E is an N × Q noise matrix with entries ei,k, and
i = , … , N, k = , … , Q, and ei,k ∼(, 𝜆), where 𝜆denotes the variance of each
component of the noise. Now, following the regularized LS linear regression setting,
the KRR minimizes the following cost function:
min
W {‖Y −𝜱W‖
F + 𝜆‖W‖
F},
(.)
where the Frobenius norm is indicated as ‖ ⋅‖F, and the parameter 𝜆is a positive scalar
value which should be tuned by the user. We have deliberately dropped the bias term for
the sake of simplicity.Solving the problem with respect to W, one obtains an analytical
solution:
W ∗= (𝜱T𝜱+ 𝜆I)−𝜱TY.
(.)
The prediction vector at x can be expressed as
̂F(x) = W ∗T𝝓(x).
(.)
Applying the representer theorem, we can readily express the solution as a linear
combination of mapped samples:
W = 𝜱TA,
(.)
where A is an N ×Q matrix with components A = [a| ⋯|aQ] with ak = [ak,, … , ak,N]T
(i.e., we have one weight per sample, i = , … , N and output observation variable, k =
,…,Q). It is straightforward to show that the optimal dual weights are
A∗= (𝜱𝜱T + 𝜆I)−Y = (K + 𝜆I)−Y,
(.)
where K = 𝜱𝜱T. Using Equation .and Equation ., we can rewrite the solution
as
̂F(x) = W ∗T𝝓(x) = (𝝓T(x)𝜱T(K + 𝜆I)−Y)T.
(.)
Now let us deﬁne the kernel vector
k = 𝜱𝝓(x) = [K(x, x), … , K(x, xN)]T ∈ℝN×,
(.)
which contains the kernel dot products between point x and all the N training data
points. The ﬁnal expression of the vector of predictions is
̂F(x) = kT(K + 𝜆I)−Y.
(.)
The bias term can be actually considered an additional constant weight for all input data points in W.

Advances in Kernel Regression and Function Approximation
341
Note also that by combining Equation .with Equation .we obtain the dual
functional
min
A {‖Y −KA‖
F + 𝜆ATKA}
(.)
by simply using expression K = 𝜱𝜱T.
8.2.4
Kernel Signal-to-Noise Regression
DSP problems typically deal with observations that are corrupted by diﬀerent types
of noise sources. Most of the algorithms, however, assume that the noise is i.i.d. and
additive. Also, signal and noise may exhibit relations that need to be disentangled. In
this context, one typically looks for transformations of the observed signal such that
the SNR is maximized, or alternatively for transformations that minimize the fraction
of noise.This is the case of the minimum noise fraction (MNF) transform (Green et al.,
), which extends PCA by maximizing the signal variance while minimizing the
estimated noise variance. Although eﬃcient in many applications, MNF cannot cope
with problems when signal and noise are nonlinearly correlated.
The kernel SNR (KSNR) is the kernelization of the canonical MNF transforma-
tion. In the standard case, observations and the estimated noise in the original input
space are transformed to a high-dimensional Hilbert feature space via suitable map-
pings endorsed with the reproducing kernel property and the SNR is maximized
therein (Nielsen, ). This formulation has been extended in Gómez-Chova et al.
(), where a KSNR scheme able to deal with signal and noise relations explic-
itly in Hilbert space has been presented. The KSNR is a modiﬁcation of the KRR
method (Shawe-Taylor and Cristianini, ) described in the previous subsection, by
including the covariance matrix of the noise deﬁned in the Hilbert space into the model.
This matrix is incorporated into the regularizer of the primal cost function. Clearly,
the realization of the noise and the corresponding covariance matrix is unknown, and
the estimation of the covariance matrix is a key point of the KSNR technique. We will
discuss this issue later.
Let us consider the same multi-output formulation as in the previous sections. In this
case, we want to minimize the following cost function:
min
W {‖Y −𝜱W‖
F + 𝜆W T𝜮eW},
(.)
where 𝜮e = 𝜱T
e𝜱e represents the noise covariance in the Hilbert space, and
𝜱e = [𝝓(e)| ⋯|𝝓(eN)]T,
SNR is deﬁned as the ratio of signal power to the noise power. This concept has been extraordinarily
useful in signal processing for decades since it allows us to quantify the quality and robustness of systems
and models. Noise reduction is an issue in signal processing, typically tackled by controlling the acquisition
environment. Alternatively, one can ﬁlter the signal by looking at the noise characteristics. Actually, ﬁlter
design and regularization are intimately related as maximizing the SNR can be casted as a way to seek for
smooth solutions preserving signal characteristics and discarding features mostly inﬂuenced by the noise.

342
Digital Signal Processing with Kernel Methods
where ei, i = , … , N, are the estimated noise samples using some technique (see
Section ..). Minimizing Equation ., we obtain
W ∗= (𝜱T𝜱+ 𝜆𝜮e)−𝜱TY.
(.)
Alternatively, using the representer theorem, W = 𝜱TA, we obtain the dual cost
function
min
A {‖Y −KA‖
F + 𝜆ATK xeK exA}
(.)
that is minimized with respect to A and where K ex = 𝜱e𝜱T and K xe = 𝜱𝜱T
e are
N × N matrices containing the kernel dot products between the observations and their
estimated noise counterparts; that is, the i, jth element of K ex is a scalar product in the
Hilbert space, ⟨𝝓(xi), 𝝓(ej)⟩. It is important to note that, in general, K ex ≠K xe, but
the asymmetry is resorted by the appearance of the product K xeK ex in the functional.
The solution of the problem expressed as a function of the dual weights A is
A∗= (K + 𝜆K xeK ex)−KY,
(.)
which can be alternatively written as
A∗= (K + 𝜆K −K xeK ex)−Y = (K + 𝜆𝜴)−Y.
(.)
This equation draws some intuition about the method: the regularization term 𝜴=
K −K xeK ex is essentially discounting the impact in the solution of the noisy samples and
reinforcing the noise-free ones. This essentially follows the line of discovering relevant
directions in feature spaces mainly governed by signal and less aﬀected by noise (Braun
et al. ; Mika et al. ). As in the standard KRR, we ﬁnally need to show that we
never actually need access to the mapped feature vector 𝝓(x), which could be of inﬁnite
dimension. Indeed, the prediction for x can be simply written as in Equation ..
Note that the KSNR regression generalizes KRR to cases of non-i.i.d. noise. For i.i.d.
noise in Hilbert space, 𝜮e = 𝜎
nI, it is trivial to show that the solution (Equation .)
reduces to the standard KRR solution, and 𝜆is related to the noise variance 𝜎
n. Oﬀ-
diagonal entries in 𝜴stand out and account for signal-to-noise feature relations not
accounted for when assuming signal and noise to be independent.
Noise Estimation
The important question of noise estimation remains unanswered for the KSNR method,
which can be a diﬃcult task. In audio and image processing and time-series analysis, the
most common approach consists of assuming locally stationary signals that allow one
to estimate the noise as a simple diﬀerence between observations, ̂ni ≈xi −xi−. Other,
more elaborated, approaches approximate the observed signal using autoregressive
models to describe the local relations in structured domains. Good examples of these
local relations are previous values in a time series or close pixels in an image, which
allow one to estimate the noise as ̂ni ≈xi −∑
j∈Wi ajxj, denoting Wi the neighborhood
for the sample xi. In problems in which there is not a clear structured domain, it is

Advances in Kernel Regression and Function Approximation
343
possible to calculate k-NN estimates of the noise ̂ni ≈xi −∕k ∑
j∈C xj, denoting C
the set of k neighbors of xi. This simple way of noise estimation follows the line of
the delta test, which was proposed for time-series analysis by Pi and Peterson (),
and intuitively seeks to estimate the residuals support. For convenience, we typically
use simple diﬀerentiation for the noise estimation in structured domains, and k-NN
approximation in unstructured domains. Other strategies allow estimating the noise
explicitly in Hilbert spaces (Gómez-Chova et al., ).
8.2.5
Semi-supervised Support Vector Regression
Kernel machines in DSP estimation problems typically exploit the labeled examples
only. Both the unlabeled information (that accounts for the pdf characteristics) and the
geometrical relationship between labeled and unlabeled samples is very often obviated.
Including the unlabeled information in the regression method may improve the results,
which is the focus of a ﬁeld called semi-supervised learning (SSL) (Chapelle et al. ;
Sindhwani et al. ).
The key issue in SSL is the assumption of consistency, which means that nearby points
are likely to have the same label, and points on the same structure (typically referred to as
“cluster” or “manifold”) are likely to have the same label. Note that the ﬁrst assumption
is local, whereas the second one is global. Therefore, depending on the data complexity,
either the cluster or manifold assumption may be not completely fulﬁlled, and thus
no clear improvement may be eventually obtained with semi-supervised approaches
(Chapelle et al. ; Sindhwani et al. ).
A simple yet eﬀective way to estimate the marginal data distribution and then include
this information into any kernel method consists of “deforming” the structure of the
kernel matrix according to the unlabeled data structure. The deformation can be
designed by clustering algorithms (Chapelle et al., ), or by deforming a valid kernel
with graph-based methods that account for the geometrical relations between labeled
and unlabeled. In this section, we exploit this latter regularization framework proposed
in Sindhwani et al. () and Belkin and Niyogi () to develop a semi-supervised
version of the SVR method.
In SSL, we are given a set of labeled data {xi, yi}l
i=and some unlabeled data {xi}l+u
i=l+.
Typically having access to large labeled datasets is not easy, and one faces problems
where u ≫l. Let us deﬁne the evaluation map of the predictive function considering
all available examples S( f ) = [ f (x), … , f (xl+u)]T, and its semi-norm ‖S( f )‖= f TMf
given by a symmetric semi-deﬁnite matrix M. The explicit form of the corresponding
reproducing kernel ̃K(xi, xj) can be deﬁned (Sindhwani et al., ) as
̃K(xi, xj) = K(xi, xj) −kT
i (I + MK)−Mkj,
(.)
where i, j ∈{, … , l+u}; K is the (complete) kernel matrix, I is the identity matrix, and
ki = [K(x, xi), … , K(xl+u, xi)]T. Intuitively, the similarity between examples is modiﬁed
by discounting the unexplained information conveyed by the labeled samples only, and
that it is present when accounting for all, labeled and unlabeled, samples.

344
Digital Signal Processing with Kernel Methods
e1
e3
e2
v1
v2
v3
v4
v5
v6
v7
v8
v9
v10
Figure 8.1 In the toy graph (left), edges thickness represents the similarity between samples. Graph
methods group the unlabeled samples according to the weighted distance, not just to the shortest
path lengths. The two clusters are intuitively correct, even being connected by a thin weak edge. In
the toy hypergraph (right), the same vertices and three sets contain different potentially
interconnected subgraphs.
The geometry of the data (both labeled and unlabeled) is included through a proper
deﬁnition of the matrix M, which is usually deﬁned in terms of a graph or hypergraph
containing both labeled and unlabeled points:
●Graph Laplacians. A graph G(V, E) is deﬁned with a set of nodes V connected by a set
of edges E. The edge connecting nodes i and j has an associated weight Wij (Chapelle
et al., ). The nodes are the samples, and the edges represent the similarity among
samples in the dataset (see Figure .(left)). A proper deﬁnition of the graph is the
key to accurately introducing data structure in the regression machine. For computing
Wij, the k-NN algorithm is typically used.
The common choice takes M = 𝛾L, where 𝛾∈[, ∞) is a free parameter that
controls the deformation of the kernel, and L is the graph Laplacian, which measures
the (smooth) variation of the function along the graph (Chapelle et al., ). The
graph Laplacian is given by L = D −W, where D is the diagonal degree matrix of 𝐖;
that is, Dii = ∑l+u
j=Wij.
●Hypergraph Laplacian. A hypergraph is a generalization of a graph, where edges can
connect any number of vertices (Zhou et al., ). While graph edges are pairs of
nodes, hyperedges are arbitrary sets of nodes, and can therefore contain an arbitrary
number of nodes (see Figure .(right)). A hypergraph is also called a set system or a
family of sets drawn from the universal set. Hypergraphs can be viewed as incidence
structures and vice versa.
Notationally, a hypergraph can be represented by a matrix H(V, E) with entries
h(v, e) = if v ∈e and otherwise, called the incidence matrix. The hypergraph
Laplacian is given by L =

(I −D−∕
v
HWD−
e HTD−∕
v
), where Dv and De denote
the diagonal matrices containing the vertex and hyperedge degrees respectively. One
can build the hypergraph by ﬁrst performing k-means clustering, and then taking the
centroids as hyperedges. The weights for all hyperedges are then simply set to . Other
more sophisticated forms of deﬁning H exist. Since hypergraph links may have any

Advances in Kernel Regression and Function Approximation
345
cardinality, there are multiple notions of the concept of a subgraph: subhypergraphs,
partial hypergraphs, and section hypergraphs.
Independently of using either the graph or hypergraph Laplacian, note that by ﬁxing
𝛾= the original (undeformed) kernel is obtained. Therefore, a proper selection of
this free parameter should lead to improved performance over supervised regression.
The use of the SS-SVR in practice reduces to simply ﬁrst computing the deformed kernel
using Equation .and then plugging it into a standard kernel solver, such as the SVR. It
is worth noting that the very same methodology could be equally applied to any kernel-
based method, either for regression, classiﬁcation, or feature extraction.
8.2.6
Model Selection in Kernel Regression Methods
All kernel methods seen in previous sections have a series of hyperparameters that
should be correctly tuned to obtain good results. The parameters of the kernel function
are common to all methods, and some algorithms use several kernel functions and
corresponding parameters (e.g., KSNR). The polynomial kernel and the Gaussian kernel
are perhaps the most widely used kernel functions as they include just one parameter,
the polynomial degree d and the length-scale – spread or width of the Gaussian
function – parameter 𝜎.
Besides kernel parameters, each method introduces an additional regularization
hyperparameter, which trades oﬀpenalizing errors to ﬁnding smooth solutions in order
to combat overﬁtting. For instance, the KRR method introduces 𝜆(Equation .),
which for the case of the SVR, sometimes referred to as 𝜀-SVR, we used C = ∕𝜆.
In SVR, we also have an additional parameter to control the complexity of the solution:
recall that 𝜀essentially controls the sparsity (see Figure .). The 𝜀-Huber cost function
introduces an extra hyperparameter, 𝛿, which together with C controls penalization to
account for Gaussian noise.
In order to ﬁnd a set of optimal hyperparameters, a typical approach is to partition the
training set into subsets and conduct a grid search where several parameter combina-
tions are tested, until the best combination is found (Schölkopf et al., ). A standard
two-thirds–one-third partition is commonly used. However, most practitioners prefer
to use more exhaustive (and often more reliable) cross-validation techniques, such as
the so-called v-fold (Duda et al., ). In v-fold, the training set is divided into v subsets,
and each combination of hyperparameters is tried on each one, using v −subsets to
train a model and the vth subset to test it. The hyperparameters that on average work
best for all v subsets are selected.
A diﬀerent perspective to the issue of parameter tuning is that oﬀered by bootstrap-
ping (Efron and Tibshirani, ). There are two main approaches to using bootstrap-
ping to select hyperparameters, and both of them require a previous partition of the
training set, for instance a two-thirds training–one-third test partition:
) For each combination of hyperparameters, the training set is sampled with repetition
to obtain many training sets, which are then used to train models that are evaluated
in the separated test partition. As in v-fold cross-validation, the combination that
obtains the best performance on average is selected.
) In the second approach, for each combination of hyperparameters a single model is
obtained using the training partition and evaluated on the test subset. After this step,

346
Digital Signal Processing with Kernel Methods
the residuals are sampled with repetition to obtain a better and reliable estimation
of the mean error. In this setup, the hyperparameters with the lowest error (which
in the end is the same as obtaining the best performance) are selected.
The main problem of bootstrapping approaches is that they are computationally very
demanding (especially the ﬁrst one), as they require the repetition of the experiment in
the order of hundreds of times to obtain a valid estimation (Efron and Tibshirani, ).
Although the techniques described are the most widely used, it is possible (and
advisable) to take into account the properties of the signals: looking at the dynamic
range of the signal and the histograms of the features and observations may help deﬁne
a proper range of variation. In the following subsections, we provide some suggestions
to select proper values or ranges for the most used hyperparameters in kernel regression
models.
Gaussian Kernel Length-scale Parameter
One of the most used kernel functions is the RBF Gaussian kernel. The are several
reasons to explain this selection: it covers other kernel functions as a particular case (for
instance, the polynomial or the linear kernel (Schölkopf and Smola, )), it (usually)
obtains similar or better performances than other kernel functions, and, last but not
least, it only has one parameter to tune, the width of the Gaussian function, 𝜎. Let us
remember the form of the RBF Gaussian kernel:
K(x, x′) = exp
(−‖x −x′‖
𝜎
)
.
Intuitively, one can see that large values of 𝜎will cover large “spaces,” thus making the
diﬀerence of vectors x and x′ less evident. On the other hand, small values of 𝜎will cause
that even close vectors (in the Euclidean distance sense) obtain low values of the kernel
function, meaning these two vectors will be considered as “diﬀerent.” Given this, a way
to have an idea of a proper value for 𝜎is to take into account some statistics about the
distances between the vectors in the training set; for example, the mean distance, the
median, or the mode. Indeed, given a dataset, it does not make much sense to use values
of 𝜎that are much further than, for instance, md ± 𝜎d, where md and 𝜎d would be the
mean and standard deviation of all Euclidean distances in the dataset; 𝜎values much
lower than this mean that all samples in the training set would be treated as “diﬀerent”
(orthogonal), whereas large 𝜎values would treat all samples as similar to each other.
Intuitively, one wants values of 𝜎that are able to distinguish between groups of samples,
not to treat all of them as diﬀerent, or as equal. Therefore, a rule of thumb to choose a
good value for 𝜎would be to set it as md. For a more extensive tuning, reasonable values
of 𝜎should varied in the interval md ± 𝜎d.
Some other heuristics can be found in the literature. The code in Listing .gives the
code for the implementation of some of the most widely used.
% Assume X data matrix: N samples, d features: N x d
% 1: 'mean': Average distance between all samples
D = pdist(X); sigma.mean = mean(D(D>0));
% 2:
'median': Median of the distances between all samples
D = pdist(X); sigma.median = median(D(D>0));

Advances in Kernel Regression and Function Approximation
347
% 3:
'quantiles': 10 values in the range of 0.05-0.95 distance ...
percentiles
D = pdist(X); sigma.quantiles = quantile(D(D>0),linspace(0.05,0.95,10));
% 4:
'histo': Sigma proportional to the dimensionality and feature ...
variance
sigma.sampling = sqrt(d) * median(var(X));
% 5:
'range': 10 values to try in the range of 0.2-0.5 of the feature ...
range
mm = minmax(X'); sigma.range = ...
median(mm(:,2)-mm(:,1))*linspace(0.2,0.5,10);
% 6:
'silverman': Median of the Silverman's rule per feature
sigma.silverman = median( ((4/(d+2))^(1/(d+4))) * n^(-1/(d+4)).*std(X,1) ...
);
% 7:
'scott': Median of the Scott's rule per feature
sigma.scott = median( diag( n^(-1/(d+4))*cov(X).^(1/2)) );
Listing 8.1 Estimation of the with in the RBF kernel function.
The previous rule of thumb criterion can be extremely useful in the case of unlabeled
datasets. When some labeled samples are available, kernel alignment may help; see
Chapter . Assume a data matrix X and the corresponding label vector y. In kernel
alignment one essentially constructs an ideal kernel using the training labels, K ideal =
yyT, and selects the RBF length-scale 𝜎that minimizes the Frobenius norm of the
generated kernel matrix and the ideal kernel, 𝜎∗= min𝜎{‖K 𝜎−K ideal‖
F}. Note that this
heuristic exploits the labeled information, and it does not require costly cross-validation
strategies as it only involves constructing kernel matrices and involves simple norm
calculations.
Regularization and Sparsity Parameters
The second parameter in all kernel-based algorithms has to do with the selected cost
function. When an 𝓁-norm is selected, as in KRR, this is the only one parameter to
be selected besides the kernel parameters. For the SVR, nevertheless, two parameters
in the loss need to be chosen: the 𝜀error insensitivity (to control the sparseness of
the solution) and the regularization parameter C (to control the capacity of the class
of functions used). Let us review the main prescriptions and intuitions behind tuning
these.
Regularization Parameter
This parameter controls the trade-oﬀbetween penalizing errors and obtaining smooth
(therefore, not overﬁtted) solutions. Large values of C will penalize errors more aggres-
sively, making the model function to tightly ﬁt the training samples. In principle, this
is not bad per se, but it may lead to lack of generalization on unseen, testing samples.
In this case, we say that the model is overﬁtted to the training set. On the other hand,
small values of C allow for errors when developing the model, giving more importance
to obtain a smooth solution. Having a smooth solution is rather desirable (most of
the natural signals are smooth) because it will probably work better on unseen (test)
samples. Nevertheless, if C is too loose (very small values of C), the committed errors
will be so large that the model just will not describe the problem adequately.
Many prescriptions for selecting reasonable values of C are available in the literature.
For example, Cherkassky and Ma () propose an analytic parameter value selection

348
Digital Signal Processing with Kernel Methods
from the training data. It can be shown that |̂y(x)| ≤C ⋅NSV, where NSV is the
number of support vectors. The problem is that one does not know this number until all
hyperparameters have been tuned (speciﬁcally, 𝜀is the parameter related to the number
of support vectors (Schölkopf et al., )). Therefore, an initial good guess for the value
of C is |̂y(x)|; that is, the range of output values. However, this value is very sensitive to
the presence of outliers. To make the selection robust to outliers, Cherkassky and Ma
() proposed to select C as max(|̄y + 𝜎y|, |̄y −𝜎y|), where ̄y and 𝜎y are the mean
and the standard deviation of the training samples y.
Insensitivity Error
This parameter deﬁnes the insensitive zone of the SVR (see Figure .), and it is related
to the noise present in the input signal. Several authors (Cherkassky and Mulier ;
Kwok ; Smola et al. ) have shown that there is a linear relationship between
the noise and the value of 𝜀. If the level of noise 𝜎n is known, or can be estimated, one
can start with a value of 𝜀= 𝜎n. However, this value does not take into account the
size of the training set and the eventual noise correlations either. With the same level of
noise, 𝜀should intuitively be smaller for a large number of training samples than for a
small one. For linear regression, the variance of observations is proportional to 𝜎
n∕N,
where 𝜎
n is the noise variance and N the number of training samples. Therefore, a better
selection for epsilon is 𝜀∝𝜎n∕N. However, Cherkassky and Ma () empirically
found that large values of N yield values of 𝜀that are too small, and proposed setting
𝜀= 𝜎n(ln N∕N)∕.
Gaussian Noise Penalization
This parameter has to be tuned when the 𝜀-Huber cost function is used. It is related
to the amount of Gaussian noise present in the observation, and together with C it
deﬁnes the area in the cost function dealing with this kind of noise. Therefore, it is
usual to tune 𝛿C together instead or C and 𝛿separately, although of course the latter is
possible. Interestingly, the use of the 𝜀-Huber cost function is beneﬁcial to obtain some
knowledge about the underlying noise process, since the optimal value of 𝛿C will return
a good estimate of the Gaussian noise variance in the observed signal. And vice versa, if
we know the amount of Gaussian noise present in the signal, given a value for C, 𝛿can
be deduced so their product matches the amount of Gaussian noise in the input.
8.3
Bayesian Nonparametric Kernel Regression Models
This section deals with nonparametric models following Bayesian principles. We will
depart from a standard linear basis function model in Hilbert spaces, but here we will
give a Bayesian treatment. The primal and dual forms will readily become apparent,
giving rise to useful model instantiations: the Gaussian process regression (GPR) model
and the RVM. The former algorithm has captured the attention of researchers in signal
processing. GPR has shown great utility to solve real-life signal processing problems.
Actually, in the last decade, many variants of GPs have been published to deal with
speciﬁcities of the signal structure (through kernel design) and the noise characteristics

Advances in Kernel Regression and Function Approximation
349
(via heteroscedastic GPs). We will review some of these recent developments for DSP
problem solving as well.
8.3.1
Gaussian Process Regression
The linear basis function model is a a simple extension of the standard linear Bayesian
regression model (Bishop, ), and it is the basis of the well-known GPs (O’Hagan
and Kingman, ; Rasmussen and Williams, ). GPs are Bayesian state-of-the-
art tools for discriminative machine learning; that is, regression (Williams and Ras-
mussen, ), classiﬁcation (Kuss and Rasmussen, ), and dimensionality reduc-
tion (Lawrence, ). GPs were ﬁrst proposed in statistics by Tony O’Hagan (O’Hagan
and Kingman, ) and they are well known to the geostatistics community as kriging.
However, owing to their high computational complexity they did not become widely
applied tools in machine learning until the early st century (Rasmussen and Williams,
). GPs can be interpreted as a family of kernel methods with the additional advan-
tage of providing a full conditional statistical description for the predicted variable,
which can be primarily used to establish credible intervals and to set hyperparameters.
In a nutshell, GPs assume that a GP prior governs the set of possible latent functions
(which are unobserved), and the likelihood (of the latent function) and observations
shape this prior to produce posterior probabilistic estimates. Consequently, the joint
distribution of training and test data is a multidimensional Gaussian and the predicted
distribution is estimated by conditioning on the training data. As we will see later,
GPs for regression can be cast as a natural nonlinear extension to optimal Wiener
ﬁltering.
The linear basis function model is based on a parametric approach that it is strictly
related to other the nonparametric methods described later. More speciﬁcally, this
approach employs nonlinear basis functions, but the model is still linear with respect
to the parameters. Indeed, the observation model is again given by
yi = wT𝝓(xi) + ei,
i = , … , N
(.)
with xi ∈ℝd, 𝝓(xi) ∈ℝM, w ∈ℝM, ei, yi ∈ℝor, in a more compact matrix form:
y = 𝜱w + 𝐞,
(.)
where 𝜱= [𝝓(x)| ⋯|𝝓(xN)]T ∈ℝN×M is a matrix formed by input vectors in rows,
y = [y, … , yN]T ∈ℝN×is the output column vector, w ∈ℝM×is a vector of weights
which we desire to infer, and e ∈ℝN×is an additive noise vector in which each
component is Gaussian distributed with zero mean and variance 𝜎
n (i.e., ei ∼(, 𝜎
n)
for all i = , … , N). Therefore, the likelihood function associated with the model in
Equation .is
p(y|𝜱, w) =
N
∏
i=
p(yi|𝝓(xi), w) =
N
∏
i=

√
π𝜎
n
exp
[
−(yi −wT𝝓(xi))
𝜎
n
]
=

(π𝜎
n)N∕exp
(
−‖y −𝜱w‖
𝜎
n
)
,
(.)

350
Digital Signal Processing with Kernel Methods
which represents the pdf of a normal distribution (𝜱w, 𝜎
n𝐈). Note that we assume
that the observations yi are i.i.d. so we can factorize the joint pdf as a product of N
marginal pdfs. Furthermore, since we consider a Bayesian approach, we assume a prior
pdf p(w) over the vector of weights w. More speciﬁcally, we consider
p(w) =

√
(π)N|𝜮p|
exp
(
−
wT𝜮−
p w
)
,
(.)
which is Gaussian with zero mean and covariance matrix 𝜮p of size M × M; that is,
w ∼(𝟎, 𝜮p) (note that |𝜮p| is determinant of 𝜮p). Using Bayes’ rule, we obtain the
expression of the posterior pdf of the parameter vector w given the observations y and
the matrix 𝜱:
p(w|y, 𝜱) = p( y|𝜱, w)p(w)
p( y|𝜱)
.
(.)
The marginal likelihood p( y|𝜱) acts as a normalizing factor in Equation ., and it is
obtained by integration:
p(y|𝜱) = ∫p(y|𝜱, w)p(w) dw,
(.)
which does not depend on w. Hence, plugging the likelihood and prior (Equations .
and .) into Equation .leads to
p(w|y, 𝜱) ∝exp
[
−(y −𝜱w)T(y −𝜱w)
𝜎
n
]
exp
(
−
wT𝜮−
p w

)
.
(.)
We consider the computation of the MMSE estimator, which consists of computing the
expected value of the posterior pdf :
̄w = 𝔼[w|y, 𝜱] = ∫w p(w|y, 𝜱) dw.
(.)
Given the assumed model, it is possible to compute 𝔼[w|y, 𝜱] analytically and to obtain
̄w = 
𝜎
n
(

𝜎
n
𝜱T𝜱+ 𝜮−
p
)−
𝜱Ty = (𝜱T𝜱+ 𝜎
n𝜮−
p )−𝜱Ty.
(.)
It is important to remark on the close relationship between this estimator and the
(primal) solutions of the KRR and KSNR methods provided in the previous sections.
This MMSE estimator can also be expressed as
̄w = 
𝜎
n
𝜦−𝜱Ty,
(.)

Advances in Kernel Regression and Function Approximation
351
where
𝜦= 
𝜎
n
𝜱T𝜱+ 𝜮−
p
(.)
also represents the precision of the posterior; that is, the inverse of its covariance matrix.
Hence, we can write
p(w|y, 𝜱) ∝exp
[
−
(w −̄w)T𝜦(w −̄w)
]
,
= (w| ̄w, 𝜦−) = 
(
w
|||||

𝜎
n
𝜦−𝜱Ty, 𝜦−
)
,
where (w| ̄w, 𝜦−) denotes a Gaussian pdf with mean ̄w and covariance matrix 𝜦−.
Therefore, considering the assumed parametric form of the predictor wT𝝓(x), we can
consider the prediction vector ̂f (x) at some test input x as
̂f (x) = ̄wT𝝓(x) = 𝝓(x)T ̄w = 
𝜎
n
𝝓(x)T𝜦−𝜱Ty.
(.)
However, in the Bayesian framework, the predictions are not just pointwise estimates,
but are the result of summarizing (e.g., with the average estimator) the predictive
density. A complete characterization of the posterior pdf of the hidden function f (⋅)
evaluated at some test input x can be provided. The predictive mean is obtained by
averaging over all possible parameter values weighted by their posterior probability. In
this case, for instance, we can compute analytically the following predictive pdf :
p( f (x)|x, X, y) = ∫p( f (x)|x, w)p(w|y, 𝜱) dw
= 
(
f (x)
|||||

𝜎
n
𝝓(x)T𝜦−𝜱Ty𝝓(x)T𝜦−𝝓(x)
)
= ( f (x)|̂f (x), ̂𝜎(x)),
(.)
where the predictive mean is
̂f (x) = 
𝜎
n
𝝓(x)T𝜦−𝜱Ty = 𝝓(x)T(𝜱T𝜱+ 𝜎
n𝜮−
p )−𝜱Ty,
(.)
which coincides with Equation ., and the predictive variance is
̂𝜎(x) = 𝝓(x)T𝜦−𝝓(x) = 𝝓(x)T
(

𝜎
n
𝜱T𝜱+ 𝜮−
p
)−
𝝓(x).
(.)
Remark.
In the linear basis function model, we can easily draw random functions
according to the posterior distribution of the weights given the observed data following
the procedure

352
Digital Signal Processing with Kernel Methods
(a) draw a weight ws according to the posterior p(w|y, 𝜱) and then
(b) compute the sample function fS(x) = wT
s 𝝓(x) for all the test points x.
See an experimental example in Figure ..
The possibility to obtain a predictive variance is one of the beneﬁts of a Bayesian prob-
abilistic treatment with respect to the strategies considered in Section .. Furthermore
considering the observation model and an input test x∗, we have y∗= f (x∗) + e∗, and
then we can also ﬁnd the predictive distribution of the noisy observations; that is:
p(y∗|x, X, y) = (y|̂f (x), ̂𝜎(x) + 𝜎
n).
(.)
In the following subsections, we derive some interesting alternative formulations for the
predictive mean and variance.
Alternative Formulation for the Predictive Mean
Now we work on the expression 𝜦−𝜮p𝜱T in order to obtain an interesting alternative
formulation of the predictive mean. Noting that
𝜦𝜮p𝜱T =
(

𝜎
n
𝜱T𝜱+ 𝜮−
p
)
𝜮p𝜱T = 
𝜎
n
𝜱T𝜱𝜮p𝜱T + 𝜮−
p 𝜮p𝜱
= 
𝜎
n
𝜱T𝜱𝜮p𝜱T + 𝜱T
(.)
we ﬁnally obtain
𝜦𝜮p𝜱T = 
𝜎
n
𝜱T(𝜱𝜮p𝜱T + 𝜎
nI)
(.)
Given this latter equality, multiplying both sides by 𝜦−from the left and by (𝜱𝜮p𝜱T +
𝜎
nI)−from the right, we arrive at the expression
𝜮p𝜱T(𝜱𝜮p𝜱T + 𝜎
nI)−= 
𝜎
n
𝜦−𝜱T.
(.)
Hence, using that equality, we can rewrite ̂f (x) as
̂f (x) = 
𝜎
n
𝝓(x)T𝜦−𝜱Ty = 𝝓(x)T
(

𝜎
n
𝜦−𝜱T
)
y
= 𝝓(x)T𝜮p𝜱T(𝜱𝜮p𝜱T + 𝜎
nI)−y.
(.)
Now let us deﬁne the kernel function
K(x, x′) = 𝝓(x)T𝜮p𝝓(x′) = 𝝍(x)T𝝍(x′),
(.)
where 𝝍(x) ∶= 𝜮∕
p 𝝓(x) (recall that 𝜮p is deﬁnite positive). Also, we can deﬁne
k = 𝝓(x)T𝜮p𝜱T = [K(x, x), … , K(x, xN)]T ∈ℝN×N
(.)

Advances in Kernel Regression and Function Approximation
353
and
K = 𝜱𝜮p𝜱T.
(.)
With these deﬁnitions we can write
̂f (x) = kT(K + 𝜎
nI)−y,
(.)
which plays the role of the dual solution and it is essentially the same predictive function
in the previous KRR and KSNR methods.
Alternative Formulation for the Predictive Variance
Considering the following generic matrices Z of size M × M, U of size N × M, L of size
N ×N, and V of size M×N, the following matrix inversion lemma (see Press et al. ()
and Appendix A in Rasmussen and Williams ()) is satisﬁed:
(Z + ULV T)−= Z−−Z−U(L−+ V TZ−U)−V TZ−.
(.)
Using this matrix inversion lemma with Z−= 𝜮p, L−= 𝜎
nI, and U = V = 𝜱T, we
can develop the following term appearing in Equation .:
(
𝜮−
p + 
𝜎
n
𝜱T𝜱
)−
= 𝜮p −𝜮p𝜱T(𝜱𝜮p𝜱T + 𝜎
nI)−𝜱𝜮p,
(.)
which is plugged into Equation .to obtain
̂𝜎(x) = 𝝓(x)T[𝜮p −𝜮p𝜱T(𝜱𝜮p𝜱T + 𝜎
nI)−𝜱𝜮p]𝝓(x),
= 𝝓(x)T𝜮p𝝓(x) −𝝓(x)T𝜮p𝜱T(𝜱𝜮p𝜱T + 𝜎
nI)−𝜱𝜮p𝝓(x).
(.)
Using the expressions in Equations .and .we obtain
̂𝜎(x) = k −kT(K + 𝜎
nI)−k,
(.)
where we have taken into account that 𝜮p is symmetric; that is, 𝜮p = 𝜮T
p.
Model Selection and Hyperparameter Inference
The choice of hyperparameters of the model (such as the variance of the noise 𝜎
n and
any other parameters of the basis functions) often involves the study of the marginal
likelihood p( y|𝜱) in Equation ., also known as model evidence or just evidence.
The corresponding hyperparameters 𝜽are typically selected by type-II ML, maximiz-
ing the marginal likelihood of the observations, which is also analytical:
log p( y|𝜽) = log ( y|, K + 𝜎
nI)
∝−log |(K + 𝜎
nI)| −yT (
K + 𝜎
nI
)−y.
(.)
When the derivatives of Equation .are also analytical, which is often the case, con-
jugated gradient ascent is typically used for optimization; see related code in Listing ..

354
Digital Signal Processing with Kernel Methods
It goes without saying that, when the number of hyperparameters is not high, one
could apply standard cross-validation techniques to infer them. We intentionally omit
all these technicalities in this book; the reader is addressed to Chapter in Rasmussen
and Williams () for a more rigorous treatment.
There are other strategies to infer hyperparameters. For instance, an alternative is
to consider some prior over the hyperparameters in order to make inference about
them and provide a full Bayesian solution (Robert and Casella, ). Both strategies,
optimization or sampling, can be performed by using Monte Carlo methods (Doucet
and Wang ; Liu ; Robert and Casella ) such as importance sampling
techniques (Bugallo et al. ; Martino et al. a, ; Remondo et al. ; Yuan
et al. ) and Markov chain–Monte Carlo (MCMC) (Andrieu et al. ; Larocque
and Reilly ; Martino et al. b, ) algorithms.
On the Covariance Function
The core of a kernel method like GPs is the appropriate deﬁnition of the covariance (or
kernel) function. A standard, widely used covariance function is the squared exponen-
tial, K(xi, xj) = exp(−‖xi −xj‖∕(𝜎)), which captures sample similarity well in most
of the (unstructured) problems, and only one hyperparameter 𝜎needs to be tuned.
In the context of GPs, kernels with more hyperparameters can be eﬃciently inferred.
This is an opportunity to exploit asymmetries in the feature space by including a
parameter per feature, as in the very common anisotropic SE kernel function:
K(xi, xj) = 𝜈exp
(
−
d
∑
k=
(xi,k −xj,k)
𝜎
k
)
,
(.)
where 𝜈is a scaling factor, and we have one 𝜎k for each input feature, k = , … , d.
This is a very ﬂexible covariance function that typically suﬃces to tackle most of the
problems. Furthermore, this kernel function is also used for the so-called automatic
relevance determination (Bishop, ); that is, studying the inﬂuence of each input
component to the output. Table .summarized the most common kernel functions in
standard applications with kernel methods.
Time-Based Covariance for Gaussian Process Regression
Signals to be processed typically show particular characteristics, with time-dependent
cycles and trends. One could include time ti as an additional feature in the deﬁnition of
the input samples. This stacked approach (Camps-Valls et al., a) essentially relies
on a covariance function K(zi, zj), where zi = [ti, xT
i ]T. The shortcoming is that the time
relations are naively left to the nonlinear regression algorithm, and hence no explicit
time structure model is assumed. To cope with this, one can use a linear combination
(or composite) of diﬀerent kernels, one dedicated to capture the diﬀerent temporal
characteristics and the other to the feature-based relations, to construct a composite
time-based covariance for GPR (TGPR).
The issue here is how to design kernels capable of dealing with nonstationary pro-
cesses. A possible approach is to use a stationary covariance operating on the variable
of interest after being mapped with a nonlinear function engineered to discount such
undesired variations. This approach was used by Sampson and Guttorp () to

Advances in Kernel Regression and Function Approximation
355
model spatial patterns of solar radiation with GPR. It is also possible to adopt an SE
as stationary covariance acting on the time variable mapped to a D periodic space
z(t) = [cos(t), sin(t)]T, as explained in Rasmussen and Williams ():
K(ti, tj) = exp
(
−
‖z(ti) −z(tj)‖
𝜎
t
)
,
(.)
which gives rise to the following periodic covariance function:
K(ti, tj) = exp
(
−
sin[(ti −tj)∕]
𝜎
t
)
,
(.)
where 𝜎t is a hyperparameter characterizing the periodic scale and needs to be inferred.
It is not clear, though, that the seasonal trend is exactly periodic, so we modify this
equation by taking the product with an SE component, to allow a decay away from exact
periodicity:
K(ti, tj) = 𝛾exp
(
−
sin[π(ti −tj)]
𝜎
t
−
(ti −tj)
𝜏
)
,
(.)
where 𝛾gives the magnitude, 𝜎t the smoothness of the periodic component, 𝜏represents
the decay time for the periodic component, and the period has been ﬁxed to year.
Therefore, our ﬁnal covariance is expressed as
K([xi, ti], [xj, tj]) = K(xi, xj) + K(ti, tj),
(.)
which is parameterized by only three more hyperparameters collected in 𝜽
=
{𝜈, 𝜎, … , 𝜎d, 𝜎n, 𝜎t, 𝜏, 𝛾}. See Table .for a real-life application of this temporal
covariance.
Variational Heteroscedastic Gaussian Process Regression
The standard GPR is essentially homoscedastic; that is, assumes constant noise power
𝜎
n for all observations. This assumption can be too restrictive for some problems.
Heteroscedastic GPs, on the other hand, let noise power vary smoothly throughout
input space, by changing the prior over ei ∼(, 𝜎
n) to ei ∼(, exp(g(xi))), and
placing a GP prior over g(x) ∼(, Kg(x, x′)). Note that the exponential is needed
in order to describe the nonnegative variance. The hyperparameters of the covariance
functions of both GPs are collected in 𝝈f and 𝝈g, accounting for the signal and the noise
relations respectively.
Relaxing the homoscedasticity assumption into heteroscedasticity yields a richer,
more ﬂexible model that contains the standard GP as a particular case corresponding to
a constant g(x). Unfortunately, this also hampers analytical tractability, so approximate
methods must be used to obtain posterior distributions for f (x) and g(x), which
Of course, other transformations are possible, but are just not as convenient.

356
Digital Signal Processing with Kernel Methods
are in turn required to compute the predictive distribution over y∗. Next, we sum-
marize previous approaches to deal with the problem and the proposed variational
alternative.
The heteroscedastic GP model was ﬁrst described in Goldberg et al. (), where
an MCMC procedure was used in order to implement full Bayesian inference (Andrieu
et al., ; Liu, ; Robert and Casella, ). A faster but more limited method
is presented in Kersting et al. () in order to perform MAP estimation. These
approaches have certain limitations: MCMC is in general slower, but this provides much
more statistical information (Andrieu et al., ; Doucet and Wang, ; Martino
et al., b), whereas MAP estimation does not integrate out all latent variables and is
prone to overﬁtting. As an alternative to these costly previous approaches, variational
techniques allow one to approximate intractable integrals arising in Bayesian inference
and machine learning in general. They are typically used to: () provide analytical
approximations to the posterior probability of the unobserved variables, and hence
do statistical inference over these variables; and () derive a lower bound for the
marginal likelihood (or “evidence”) of the observed data, which allows model selection
because higher marginal likelihoods relate to greater probabilities of a model generating
the data.
In order to overcome the aforementioned problems, a sophisticated variational
approximation called the marginalized variational approximation was introduced
in Lázaro-Gredilla and Titsias (). The marginalized variational approximation
renders (approximate) Bayesian inference in the heteroscedastic GP model both
fast and accurate. In Lázaro-Gredilla and Titsias (), an analytical expression for
the Kullback–Leibler (KL) divergence between a proposal distribution and the true
posterior distribution of f (x) and g(x) (up to a constant) was provided. Minimizing
this quantity with regard to both the proposal distribution and the hyperparameters
yields an accurate estimation of the true posterior while simultaneously performing
model selection. Furthermore, the expression of the approximate mean and variance
of the posterior of y∗(i.e., predictions) can be computed in closed form. We will
refer to this variational approximation for heteroscedastic GP regression as variational
heteroscedastic GPR (VHGPR).
Warped Gaussian Process Regression
Even though GPs are very ﬂexible priors for the latent function, they might not be
suitable to model all types of data. It is often the case that applying a logarithmic
transformation to the target variable of some regression task (e.g., those involving stock
prices, measured sound power) can enhance the ability of GPs to model it. One can use a
GP model that automatically learns the optimal transformation. The proposed method
is called a warped GP (WGP) (Snelson et al., ) or Bayesian WGP (Lázaro-Gredilla,
). The models assume a warping function g(⋅) and a new model:
yi = g(f (xi)) + ei,
where f is a possibly noisy latent function with d inputs, and g is an arbitrary warping
function with scalar inputs. The Bayesian WGP essentially places another prior for
g(x) ∼(f , c(f , f ′)).

Advances in Kernel Regression and Function Approximation
357
Snelson et al. () showed that it is possible to include a nonlinear preprocessing
of output data h(y) (called a warping function in this context) as part of the modeling
process and learn it. In more detail, a parametric form for z = h(y) is selected, and
then z (which depends on the parameters of h(y)) is regarded as a GP; and ﬁnally, the
parameters of h(y) are selected by maximizing the evidence of such GP (i.e., an ML
approach). The authors suggested using
h(y) =
L
∑
l=
al tanh(bl(y + cl))
as the parametric form of the warping function, which is parametrized as 𝜃= [al, bl, cl]
for l = , … , L, and where L is the number of piecewise functions needed to warp the
signal that has to be selected by the user. Any alternative option resulting in a monotonic
function is still valid too. A nonparametric version of warped GPs using a variational
approximation has been proposed by (Lázaro-Gredilla, ). See the related example
in Heteroscedastic and Warped Gaussian Process Regression in Section ..for a real-
life application of both the VHGPR and the WGP regression (WGPR) methods.
Ensembles of Gaussian Processes
An interesting possibility of GPR is about exploiting the credible intervals that can be
derived. This is very often a valuable piece of information to understand the problem,
discard outliers, or to design more appropriate sampling methodologies. Another possi-
bility is to use them to give more credit to highly conﬁdent models, and downweight the
unsure ones in committees of GPR experts. Given R trained GPR models, the ensemble
is conducted here by exploiting the posterior variance. Essentially, one can weight the
posterior meanŝfr with the posterior variances ̂𝜎
r , r = , … , R. This will give the overall
posterior mean and variance estimates respectively:
̂f =
∑R
r=̂fr∕𝜎
r
∑R
r=∕𝜎
r
,
̂𝜎=

∑R
r=∕̂𝜎
r
,
where intuitively one downweights the contribution of the less-conﬁdent model predic-
tions. Figure .and Listing .in Ensemble Gaussian Process Regression Models in
Section ..show an example of ensemble GPR.
Efficiency in Gaussian Process Regression
The reduction of the computation complexity of GPs is usually targeted by approximat-
ing the matrix inverse. The approximation methods can be broadly classiﬁed as in sparse
methods, localized regression methods, and matrix multiplication methods.
Sparse methods, also known as low-rank covariance matrix approximation methods,
are based on approximating the full posterior by expressions using matrices of lower
rank M ≪N, where the M samples are typically selected to represent the dataset (e.g.,
from clustering or smart sampling). These global methods are well suited for modeling
smooth-varying functions with long length scales. Methods in this family are based
on substituting the joint prior with a reduced one using a set of m latent variables
u = [u, … , uM]T called inducing variables (Quiñonero-Candela and Rasmussen,

358
Digital Signal Processing with Kernel Methods
Table 8.1 Predictive distribution for the low-rank approximation methods described in Section 8.3.1,
including the computational complexity for training, predictive mean, and predictive variance. N is the
number of samples, M is the number of latent inducing variables, and B = M/N is the number of blocks
for methods that use them. We denote Qa,b ≡Ka,uK−
u,uKu,b.
Method
Predictive mean, 𝝁∗
Predictive variance, 𝝈∗
Training
Test mean
Test variance
SoR
Q∗f (Qﬀ+ 𝜎I)−y
Qf ∗−Q∗f (Qﬀ+ 𝜎I)−Qf ∗
(NM)
(M)
(M)
DTC
Q∗f (Qﬀ+ 𝜎I)−y
Kf ∗−Q∗f (Qﬀ+ 𝜎I)−Qf ∗
(NM)
(M)
(M)
FITC
Q∗f (Qﬀ+ 𝜦)−y
Kf ∗−Q∗f (Qﬀ+ 𝜦)−Qf ∗
(NM)
(M)
(M)
PITC
As FITC, but 𝜦≡blkdiag[Kﬀ−Qﬀ+ 𝜎I]
(NM)
(M)
(M)
PIC
KPIC
∗f (Qﬀ+ 𝜦)−y
Kf ∗−KPIC
∗f (Qﬀ+ 𝜦)−Qf ∗
(NM)
(M + B)
((M + B))
). These latent variables are values of the GP corresponding to a set of input
locations Xu, called inducing inputs. By adopting a “subsets of data” approach, the
computational complexity drastically reduces to (M), being M ≪N. Examples
of these approximation methods are the subsets of regressors (SoR), deterministic
training conditional (DTC), fully independent training conditional (FICT), partially
independent training conditional (PICT) (Quiñonero-Candela and Rasmussen,
), and partially independent conditional (PIC) (Snelson and Ghahramani,
). Table .summarizes their predictive distributions and their computational
complexities.
Localized regression methods have also been proposed in this setting. When M is too
small, then the representation of the whole set is poor and the performance of the associ-
ated GP is low. Local GPs are obtained by just dividing the region of interest and training
a GP in each division, as far as dividing in B blocks such as B = N∕M reduces the com-
putational complexity from (N) to (NM). Some disadvantages, such as discontinu-
ities at the limits and loose performance when predicting in regions far from their local-
ity, have been solved by recent new approximate methods, including PIC (Snelson and
Ghahramani, ), FITC, and pure local GP (see Snelson and Ghahramani () for
details).
Matrix vector multiplication approximation methods are based on speeding up the
solving of the linear system (K + 𝜎I)𝜶= y using an iterative method, such as
the conjugate gradient (CG). Each iteration of the CG method requires a matrix
vector multiplication which takes O(N). The CG method obtains the exact solution if
iterated N times, but one can obtain an approximate solution if the method is stopped
earlier.
In recent years, we have witnessed a huge improvement in GP runtime and memory
demands. Inducing methods became popular but may lack expressive power of the
kernel. A very useful approach is the sparse spectrum GPs (Lázaro-Gredilla et al., ).
On the other hand, there are methods that try to exploit structure in the kernel, either
based on Kronecker or Toeplitz methods. The limitations of these methods to deal with
data in a grid have been remedied recently with the KISS GP (Wilson and Nickisch,
), which generalizes inducing point methods for scalable GPs, and scales (N) in
time and storage for GP inference.

Advances in Kernel Regression and Function Approximation
359
8.3.2
Relevance Vector Machines
The RVMs regression method – seeTipping () and Chapter in Bishop () – is
a Bayesian sparse kernel technique which shares many features with SVM. In particular,
the RVM is a special case of linear basis function model where
a) M = N; that is, the number of basis functions is equal to the number of data points N.
b) The basis functions are localized functions centered around the data points, and
satisfy all the properties required for the kernel functions. Namely, we have N
localized basis functions 𝝓i(⋅), i = , … , N, which are kernel functions K(x, ⋅) =
𝝓i(x).
c) The prior pdf over w is
p(w|𝜶) =
N
∏
i=
(wi|, 𝛼−
i ) =
N
∏
i=
√𝛼i
π exp(−
𝛼iw
i )
∝exp(−
wT𝜮−
p w),
(.)
where 𝜶= [𝛼, … , 𝛼N]T and 𝜮p = diag(𝜶)−.
Namely, we have N basis functions and hence N weights, one per data point, as well as
N independent hyperparameters 𝜶= [𝛼, … , 𝛼N]T, one per weight (or basis function)
which moderate the scale of the prior pdf. Recall that an additional hyperparameter is
the noise variance 𝜎
n, then the total number of hyperparameters in RVM is N + .
Remark. The previous observation is a key to obtain sparsity. Tuning the hyper-
parameters (i.e., maximizing the evidence), a signiﬁcant proportion of 𝜶= [𝛼, … , 𝛼M]T
diverges, and the corresponding posterior pdf s of the weights wj are concentrated in
zero. Namely, the posterior pdf of the weights wj with 𝛼j →∞tends to be a delta
function centered around zero. Thus, the corresponding weight is virtually deleted
from the model. The corresponding basis functions are eﬀectively pruned out. The
remaining examples with nonzero associated weights are called the relevance vectors
(RVs), resembling the support vectors in the SVM framework (Bishop, ).
The predictive mean in an RVM is expressed as a linear combination of M basis
functions, and can be expressed as
̂f (x) =
M
∑
i=
̄wi𝝓i(x) =
N
∑
i=
̄wiK(x, xi),
(.)
where in the second equality we used K(x, xi) = 𝝓i(x) because we have as many
basis functions as points N = M. Observe that an RVM is something in the middle
of the primal and dual approaches explained before. The mean weight vector ̄w =
[ ̄w, … , ̄wN]T is provided in Equations .and .; that is:
̄w = (𝜱T𝜱+ 𝜎
n𝜮−
p )−𝜱Ty,
(.)
where, in this case, 𝜱is an N × M or N × N matrix, 𝜱= [𝝓(x)| ⋯|𝝓(xM)]T =
[𝝓(x)| ⋯|𝝓(xN)]T, and 𝝓(x) = [𝝓(x), … , 𝝓N(x)]T ∶ℝd →ℝN. Since we deﬁned the
kernel function as K(x, xi) = 𝝓i(x), in this case 𝜱≡K, where K is the kernel matrix.

360
Digital Signal Processing with Kernel Methods
Now, given the aforementioned deﬁnitions of 𝝓i(x) and 𝜱, the predictive mean and
variance are respectively
̂f (x) = 𝝓(x)⊤(𝜱T𝜱+ 𝜎
n𝜮−
p )−𝜱Ty
(.)
and
̂𝜎(x) = 𝝓(x)T
(

𝜎
n
𝜱T𝜱+ 𝜮−
p
)−
𝝓(x),
(.)
as derived previously in this section. The tuning of the hyperparameters 𝜶and 𝜎
n in the
RVM is obtained by maximizing the marginal likelihood p( y|𝜱, 𝜶, 𝜎
n).
Remark. When localized basis functions are employed, the predictive variance for
linear regression models becomes small in the regions of input space where no basis
functions are located. This is the case of when the basis functions are centered on
data points (see Listing .). This behavior is clearly counterintuitive and undesirable;
see Rasmussen and Quiñonero-Candela () and Chapter in Bishop (). The
posterior distribution in GPR does not suﬀer from this problem, and this is one of the
main advantages of a GPR approach (see Figure .c in Section ..).
8.4
Tutorials and Application Examples
This section gives empirical evidence on synthetic and real examples of the methods
presented. In addition, some worked examples, tutorials, and pointers to useful tool-
boxes in MATLAB are provided for selected methods.
8.4.1
Comparing Support Vector Regression, Relevance Vector Machines, and
Gaussian Process Regression
Let us start by illustrating the behavior of diﬀerent models we considered in this
chapter in a standard toy example function approximation problem of the sinc function.
Speciﬁcally, we compare the SVR, RVM, and GPR methods. We generated a noisy
version y from the sinc(s) function with additive Gaussian noise (, .). We used
% of the points for training the models and the rest for prediction. Training was done
via standard ﬁvefold cross-validation for SVR and by maximizing the log-likelihood for
RVM and GPR. Figure .shows the clean signal s, the observations (black circles), the
selected support vectors/RVs (red circles), and the approximation function ̂y. For the
GPR model, we also give the CIs for the predictions. It is clear that all methods perform
very well (high correlation coeﬃcients with the signal) and oﬀer diﬀerent advantages:
() the SVR gives the best overall performance in ﬁt but is less sparse than expected
(of course, this issue could be trimmed by properly tuning the 𝜀parameter); () the
RVM oﬀers a rather sparse model but its performance is compromised; and () the GPR
performs well in ﬁt, but it is unfortunately not a sparse model, though advantageously
we can derive CIs for the predictions. The code in Listing .generates Figure .and
needs the simpleR MATLAB toolbox available on the book’s webpage.

Advances in Kernel Regression and Function Approximation
361
SVR (0.95, 53%)
RVM (0.94, 17%)
GPR (0.95, 100%)
Actual
Observations
SVR
SVs
Actual
Observations
RVM
RVs
Actual
Observations
KRR/GPR
μy± σy
Figure 8.2 Solutions of the different regression models revised in this chapter. Pearson’s correlation
coefficient and sparsity level are shown in parentheses.
% Comparing SVR, KRR and GPR (simpleR MATLAB toolbox needed)
addpath(genpath('../simpleR/')), addpath('../libsvm/')
% Generating Data
n = 100; X = linspace(-pi,pi,n)'; s = sinc(X); y = s + 0.1*randn(n,1);
% Split training and testing sets
n = length(y); r = randperm(n); ntr = round(0.3*n);
idtr = r(1:ntr); idts = r(ntr+1:end);
Xtr = X(idtr,:); ytr = y(idtr,:); Xts = X(idts,:); yts = y(idts,:);
%% SVR
model1 = trainSVR(Xtr,ytr); ypred1 = testSVR(model1,X);
ypred1ts = testSVR(model1,Xts);
%% RVM
model2 = trainRVM(Xtr,ytr); ypred2 = testRVM(model2,X);
ypred2ts = testRVM(model2,Xts);
%% GPR
model3 = trainGPR(Xtr,ytr); [ypred3 s3] = testGPR(model3,X);
ypred3ts = testGPR(model3,Xts);
%% Results in the test set
r1 = assessment(yts,ypred1ts,'regress');
r2 = assessment(yts,ypred2ts,'regress');
r3 = assessment(yts,ypred3ts,'regress');
%% Plots
figure(1), clf, plot(X,y,'ko'), hold on, plot(X,s,'k')
plot(X,ypred1,'r'), plot(Xtr(model1.idx),ytr(model1.idx),'ro'),
legend('Actual','Observations','SVR','SVs'), axis off

362
Digital Signal Processing with Kernel Methods
figure(2), clf, plot(X,y,'ko'), hold on, plot(X,s,'k')
plot(X,ypred2,'r'), plot(Xtr(model2.used),ytr(model2.used),'ro')
legend('Actual','Observations','RVM','RVs'), axis off
figure(3), clf, plot(X,y,'ko'), hold on, plot(X,s,'k')
plot(X,ypred3,'r'), plot(X,ypred3+sqrt(s3),'r--'),
plot(X,ypred3-sqrt(s3),'r--'),
legend('Actual','Observations','KRR/GPR','\mu_y \pm \sigma_y'), axis off
Listing 8.2 Code related to Figure 8.2.
In contrast to the SVR-based algorithms in the previous sections, the solution
obtained with GPR/KRR is not sparse, as the weights A in Equation .are in principle
all diﬀerent from zero. This is indeed a drawback compared with SVR, where the sparsity
of the models drives to less operations and memory consumption in order to obtain
predictions. In addition, dense solutions may give rise to strong overﬁtting because all
examples seen in the training set are stored/memorized in the model’s weights. On the
other hand, the GPR/KRR solution reduces to minimizing a matrix inversion problem,
for which there are very eﬃcient solvers and approximate solutions.
8.4.2
Profile-Dependent Support Vector Regression
Let us now exemplify the performance of the PD-SVR, for which one has to design
proﬁles for the variation of C and 𝜀(Camps-Valls et al., , ). A standard
approach in time-series prediction reduces to considering exponential memory decays
in which one assumes that recent past samples contain relatively more information than
distant samples. This leads to
ci = 𝜆tN−ti,
𝜆∈(, ],
(.)
where tN is the actual time sample and ti is the time instant for sample i. This proﬁle
reduces the penalization parameter and enlarges the 𝜀-insensitive region of previous
samples as new samples are obtained, as illustrated in Figure .. The inclusion of
a temporal conﬁdence function in the SVR formulation oﬀers some advantages. The
overall number of support vectors remains constant through time, and better results
are obtained when abrupt changes appear in the time series.
We test the performance of PD-SVR in a bioengineering signal processing problem.
Speciﬁcally, we are concerned about the estimation of concentration of erythropoi-
etin A for its dosage individualization. The high cost of this medication, its side
eﬀects, and the phenomenon of potential resistance which some individuals suﬀer all
justify the need for a model that is capable of optimizing dosage individualization.
A group of patients and several patient factors were used to develop the model.
To build the predictive models, we collected several factors for each patient such as
age (years), weight (kilograms), plasmatic concentration of hemoglobin (grams per
deciliter), hematocrit concentration (percent), ferritin (milligrams per liter), dosage
of intravenous iron (milligrams per month), number of administrations, isoform and
dosage of erythropoietin (IU per week). The variable to be predicted is the plasmatic
concentration of hemoglobin (Hb). Patients were randomly assigned to two groups: one
for training the models (patients, data points) and another for their validation
(patients, data points).

Advances in Kernel Regression and Function Approximation
363
(a)
x
x
x
x
x
x
x
x
x
x
x
x
x
(b)
x
x
x
x
x
x
x
x
x
x
x
x
x
x
Figure 8.3 The 𝜀-insensitive region for the standard SVR (a) and for the PD-SVR with an exponential
memory decay profile (b). Only points outside the shaded regions contribute to the cost function. The
square symbol indicates the actual sample to predict using the past samples. In the SVR, all samples
outside a fixed tube of size 𝜀are penalized, independently from their time sample. In the PD-SVR case,
only the recent past samples are considered to be relevant to the solution. In the PD-SVR, only the
points next to the actual sample are penalized and, consequently, become support vectors. Since
recent samples contain, in principle, more information about the future, only these are penalized in a
PD-SVR. This, in turn, yields solutions with a lower fraction of support vectors.
Table 8.2 ME, MSE, and correlation coefficient 𝜌of models in the
validation set. Recognition rates (RRs) with admissible error of 0.5 and
0.25 g/dL are also given.
ARCH
MLP
RBF kernels
Model
(1)
16 × 8 × 1
ν-SVR
PD-SVR
𝜌
.
.
.
.
ME (g/dL)
.
−.
−.
−.
RMSE (g/dL)
.
.
.
.
% RR
.
.
.
.
(error < .g/dL)
% RR
.
.
.
.
(error < .g/dL)
In Table .the results of the PD-SVR are benchmarked with an MLP trained
using the back-propagation algorithm, with the standard 𝜈-SVR, and an autoregressive
conditional heteroscedasticity (ARCH) model which serves as a natural benchmark
for the forecast performance of heteroscedastic models. The table also shows the
recognition rates for () correct predictions according to the experts (when committed
errors are less than .g/dL) and for () predictions with a high grade of precision (when
committed errors are less than .g/dL).
The ARCH model achieved acceptable results, but its performance is inferior to the
other models. In fact, excellent results are obtained in prediction using the MLP and

364
Digital Signal Processing with Kernel Methods
the PD-SVR models. Good results were oﬀered by the standard SVR model, but worse
than those obtained with MLP and PD-SVR, possibly because of the high inter-subjects
variability (coeﬃcient of variation, CV = %), intra-subjects variability (CV = %),
and the shortness of the time series (three to ten samples per patient). The simple
exponential decay proﬁle improved results. In general, the results of MLP and PD-
SVR models were very similar, but the best performance in the validation group was
demonstrated by the PD-SVR. Moreover, our proposal achieved a smoother (lower
values of C) and simpler (lower number of support vectors) solution than the standard
formulation.
8.4.3
Multi-output Support Vector Regression
Let us illustrate the MSVR algorithm for MIMO nonlinear channel estimation (Sánchez-
Fernández et al., ). Comparison is done for diﬀerent channels, SNRs measured at
the receiver inputs, and training sequence lengths. The goal in the channel estimation
problem is to obtain a good approximation to the actual channel, modeling the
dependence between transmitted and received signals. With the MMSE method, this
relation is restricted to be linear, and the channel estimate can be explicitly given.
This holds for the MSVR and SVR methods with linear kernel, but it is no longer
possible when other (nonlinear) kernels are used. In pilot-aided channel estimation,
it is necessary to use a training sequence known a priori by both the transmitter
and the receiver. Once the channel has been modeled, the expected received vector
y without noise, corresponding to each possible transmitted QPSK codeword x,
is calculated. During operation, each received signal is decoded using the nearest-
neighbor criterion.
We here reproduce some simulation results for MIMO systems (nT = , nR = )
originally presented in Sánchez-Fernández et al. (). For this purpose, two diﬀerent
channels are used. In the ﬁrst channel model, at the receiver end, there is a lineal mixture
of the transmitted symbols even though each of them is a nonlinear function of the
information symbol; noise considered here is Gaussian and white. In the second channel
model, a nonlinear mixture of all the transmitted symbols at the receiver and the noise
can no longer be considered Gaussian. The number of test words has been chosen to
assure that at most one erroneous bit occurs for each bits received, and all results
have been averaged over trials.
The experiment shows results when the nonlinearities between input and output
signals of the channel are introduced by the transmitter equipment due to, for example,
ampliﬁers driven near their saturation zone. The channel is modeled with coeﬃcients
𝛼= .and 𝛼= . MSVR is able to parameterize nonlinearities eﬀectively, as is
seen in Figure ., and obtains lower BER than the RBF network for the variable SNR
level. The improvement of the MSVR method is especially noticeable for short training
lengths, although the diﬀerence is only slightly reduced for the larger training sets. The
saturation point of the curves, for which the BER is no longer improved, increases as the
SNR grows. In any case, this point is reached in ﬁrst place by the MSVR. We have split
the results into two plots, grouping in each one alternatives SNRs. Results of the SVR
are comparable to those of the MSVR, but come at a higher computational cost. While
MSVR requires just a few iterations of the IRWLS algorithm to converge, SVR needs

Advances in Kernel Regression and Function Approximation
365
50
100
150
200
250
10−4
10−3
10−2
10−1
100
Training words
(a)
BER
RBFN
SVR
MSVR
SNR = 10 dB
SNR=4dB
100
10−1
10−2
10−3
(b)
50
100
150
200
250
Training words
BER
RBFN
SVR
MSVR
SNR = 2 dB
SNR = 6 dB
Figure 8.4 BER with MSVR, SVR, and RBF network (RBFN) as a function of the length of the training
sequence used during the training phase, generated with SNR = 4 dB and 10 dB (a), and with SNR =
2 dB and 6 dB (b). The nonlinearity phenomenon occurs in the transmitter, then affecting solely the
input signals. SVR-based methods improvement is evident.
approximately two orders of magnitude more iterations. Besides, the complexity of SVR
increases both with nR and the length of the training sequence, while that of MSVR does
not depend on nR. Listing .shows a simple example of MSVR with arbitrary data and
hyperparameters.

366
Digital Signal Processing with Kernel Methods
%% Multioutput Support Vector Rregression (MSVR)
%
- C
: penalization (regularization) parameter
%
- epsilon: tolerated error, defines the insensitivity zone
%
- sigma
: lengthscale of the RBF kernel
%
- tol
: early stopping tolerance of the IRWLS algorithm
ker = 'rbf'; epsilon = 0.1; C = 2; sigma = 1; tol = 1e-10;
% Training with pairs Xtrain-Ytrain
[Beta,NSV,val] = msvr(Xtrain,Ytrain,ker,C,epsilon,sigma,tol);
% Prediction on new test data Xtest
Ktest = kernelmatrix(ker,Xtest',Xtrain',sigma);
Ntest = size(Xtest,1); testmeans = repmat(my,Ntest,1);
Ypred = Ktest * Beta + testmeans;
Listing 8.3 Example of MSVR (demoMSVR_simpler.m).
An operational demo for MSVR is available on the book web page.
8.4.4
Kernel Signal-to-Noise Ratio Regression
In this section we illustrate the main features of the KSNR in a simple toy exam-
ple with non-Gaussian i.i.d noise. A total of samples were drawn from
the sinc function st
=
sin(t)∕t in the range [−π, +π], and diﬀerent noises were
added, yt
=
st + et: () Gaussian noise, et
∼
(, 𝜎
n); () uniform noise,
et ∼(, ); () Poisson noise, et ∼(𝜆), 𝜆∈[, .]; and () scale-dependent
multiplicative noise, et = mt × |st|, where m ∼(, 𝜎
n). We used samples for
cross-validation and the remaining examples for testing. Figure .shows the averaged
test results in all situations as a function of the SNR for KRR and the KSNR methods. We
also show the bound of performance in which KSNR works with the true (inaccessible)
signal samples st. These results conﬁrm some basic facts: low gain is obtained with
uncorrelated Gaussian and uniform noise sources, but noticeable gains can be obtained
in either correlated noise or strong non-Gaussian noise levels. Related MATLAB code
can be found on the book’s web page.
Now, let us test the KSNR model in a real example involving signal-to-noise relations.
We test the performance of KSNR considering a real dataset. Speciﬁcally, the motor-
cycle dataset from Silverman (), consisting of accelerometer readings through
time following a simulated motorcycle crash during an experiment to determine the eﬃ-
cacy of crash-helmets. The code in Listing .compares KRR and KSNR in this scenario.
% Comparing KRR and KSNR
clear, clc
% Setup paths
addpath(genpath('../simpleR/'))
% Load Data
load data/motorcycle.mat; Y = y;
%% Split training-testing data
rate = 0.1; [n d] = size(X);
% samples x dimensions
r = randperm(n);
% random index
ntrain = round(rate*n);
% #training samples
Xtrain = X(r(1:ntrain),:);
% training set
Ytrain = Y(r(1:ntrain),:);
% observed training variable
Xtest = X(r(ntrain+1:end),:);
% test set
Ytest = Y(r(ntrain+1:end),:);
% observed test variable
ntest = size(Ytest,1);

Advances in Kernel Regression and Function Approximation
367
%% Remove the mean of Y for training only
my = mean(Ytrain); Ytrain = Ytrain - repmat(my,ntrain,1);
% KRR
modelKRR = trainKRR(Xtrain,Ytrain);
Yp_KRR = testKRR(modelKRR,Xtest) + repmat(my,ntest,1);
results_KRR = assessment(Yp_KRR,Ytest,'regress')
% KSNR
modelKSNR = trainKSNR(Xtrain,Ytrain);
Yp_KSNR = testKSNR(modelKSNR,Xtest) + repmat(my,ntest,1);
results_KSNR = assessment(Yp_KSNR,Ytest,'regress')
% Scatter plots and marginal distributions
figure(1), scatterhist(Ytest, Yp_KRR), title('KRR'); grid on
xlabel('Observed signal'); ylabel('Predicted signal')
figure(2), scatterhist(Ytest, Yp_KSNR), title('KSNR'); grid on
xlabel('Observed signal'); ylabel('Predicted signal')
Listing 8.4 Regression with KSNR considering the motorcycle dataset.
10
15
20
25
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
SNR [dB]
RMSE
KRR
Gaussian
KSNR implicit
KSNR explicit
Bound
Uniform
10
15
20
25
0
0.02
0.04
0.06
0.08
0.1
SNR [dB]
RMSE
KRR
KSNR implicit
KSNR explicit
Bound
Poisson
10
15
20
25
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
SNR [dB]
RMSE
Scale - dependent
KRR
KSNR implicit
KSNR explicit
Bound
10
15
20
25
0
0.02
0.04
0.06
0.08
0.1
0.12
SNR [dB]
RMSE
Figure 8.5 Averaged test RMSE for the approximation of a sinc function with different noise sources
and levels.

368
Digital Signal Processing with Kernel Methods
10
0.9
1
1.1
1.2
RMSE
1.4
1.3
1.5
1.6
1.7
1.8
15
20
25
30
Training samples [%]
35
40
45
SVR
Graph SVR
Hypergraph SVR
Figure 8.6 RMSE in the test set as a function of the rate of training samples for LAI estimation from
CHRIS/PROBA data.
8.4.5
Semi-supervised Support Vector Regression
This experiment shows results in a complex real problem in geosciences: estimating
physical parameters from satellite sensor images (Camps-Valls et al., a). In particu-
lar, here we are concerned with estimating the leaf-area index (LAI), which characterizes
plant canopies, from hyperspectral satellite images. The problem is characterized by
high uncertainty and ill-conditioned data.
We used data from the ESA Spectra Barrax Campaigns (SPARC). During the cam-
paign, multiangular hyperspectral CHRIS/PROBA images were acquired. Field non-
destructive measurements of LAI were made by means of the digital analyzer LI-COR
LAI-. The LAI parameter corresponds to the eﬀective plant area index since no
corrections of leaf clumping were done and the inﬂuence of additional plant compo-
nents was not removed. The ﬁnal database consists of only LAI measurements
and their associated surface reﬂectance spectra from the (very high-dimensional) 
CHRIS reﬂectance bands (Vuolo et al., ), which leads to a -dimensional input
space.
Figure .compares the standard SVR, and the graph and hypergraph SVR methods.
The proposed graph-based methods clearly improve the results of the supervised
SVR (between a % and % improvement in RMSE) when a suﬃcient number of
labeled training samples are used (>%; i.e., samples). In this application, the use
of hypergraphs shows only a slight improvement over the graph. This could be due to
the fact that data may be governed by simple one-to-one cluster relations, or to the
low number of available data and the high uncertainty in the data acquisition. A simple
code snippet showing the calculation of the deformed kernel using the graph Laplacian
is given in Listing .. A complete MATLAB demo can be obtained from the book
web page.

Advances in Kernel Regression and Function Approximation
369
% SS-SVR on LAI estimation
clear, clc
rng('default'); rng(0); addpath('../libsvm/')
% Supervised free parameters
sigma = 1; C = 1; epsilon = 0.1;
% Semi-supervised free parameters
gamma = 1e-2; nn = 5;
% Generate dummy data: labeled (X,Y) and unlabeled (Xu, Yu)
% X, Y, Xu, Yu (only for testing)
% Kernel matrix: (l+u) x (l+u)
K = kernelmatrix('rbf',[X;Xu]',[X;Xu]',sigma);
% Graph Laplacian
lap_options.NN = nn; % neighbors
L = laplacian([X;Xu],'nn',lap_options);
M = gamma*L; I = eye(size(L,1));
INVERSE = (I + M*K) \ M;
Khat = zeros(size(K));
for x = 1:length(K)
Khat(x,:) = K(x,:) - K(:,x)' * INVERSE * K;
end
% Train and test with the deformed kernel
model = mexsvmtrain(Y,Khat,params);
Yp = mexsvmpredict(Y,Khat,model);
Listing 8.5 Related code of SS-SVR for LAI estimation (DemoSemiSVR.m).
8.4.6
Bayesian Nonparametric Model
Let us now illustrate an important feature provided by a Bayesian treatment of the
regression problem. Since we have access to the full posterior distribution, we can
calculate moments as the predictive mean and variance. Listing .generates Figure .,
which shows the predictive mean and variance in a Bayesian nonparametric model
considering artiﬁcial data and nonlocalized polynomial basis functions.
% Generate data
sig_e = 0.5; % std of the noise
X = [-12 -10 -3 -2
2 8 12 14]';
N = length(X);
Y = sin(X) + sig_e * randn(N,1); % N x 1
% Basis functions
M = 4; % number of basis functions
phi = @(x) [1 x.^(1:M-1)];
% Basis function matrix
PHI = zeros(N,M);
for i = 1:N
PHI(i,:) = phi(X(i)); % N x M
end
% Prior variance
sig_prior = 10;
Sigma_p = sig_prior^2 * eye(M); % Cov-Matrix of the prior over w
% Moments of the posterior of w
w_mean = ((PHI' * PHI) + sig_e^2 * inv(Sigma_p)) \ PHI' * Y; % M x 1
w_Covar_Matrix = ((1/sig_e^2) * (PHI'*PHI) + inv(Sigma_p)) \ eye(M); % M ...
x M

370
Digital Signal Processing with Kernel Methods
% Test inputs
x_pr = -16:0.2:16;
% posterior mean and variance of f
f_mean = zeros(1,length(x_pr)); sigma_post = f_mean;
for i = 1:length(x_pr)
f_mean(i) = phi(x_pr(i)) * w_mean;
sigma_post(i) = phi(x_pr(i)) * w_Covar_Matrix * phi(x_pr(i))';
end
% Sampling functions from Posterior of w
Num_Funct = 3;
w_sample = mvnrnd(w_mean, w_Covar_Matrix, Num_Funct);
f_samples_from_post = zeros(length(x_pr), Num_Funct);
for i = 1:length(x_pr)
f_samples_from_post(i,:) = phi(x_pr(i)) * w_sample';
end
% Plot results
V1 = f_mean - sqrt(sigma_post); V2 = f_mean + sqrt(sigma_post);
figure(1), clf
fill([x_pr, fliplr(x_pr)], [V1, fliplr(V2)], 'k', ...
'FaceAlpha', 0.3, 'EdgeAlpha', 0.7); hold on
plot(x_pr,f_mean, 'k', 'LineWidth', 5)
plot(x_pr, f_samples_from_post, '--', 'LineWidth', 3);
plot(X, Y, 'ro', 'MarkerFaceColor', 'r', 'MarkerSize', 10)
axis([-16 16 -4.2 4.2]), set(gca, 'Fontsize', 20), box on
% Alternative procedure: Sampling functions from Posterior
PHI_pr = zeros(length(x_pr),M);
for i = 1:length(x_pr)
PHI_pr(i,:) = phi(x_pr(i));
end
CovMatr_post = PHI_pr * w_Covar_Matrix * PHI_pr';
Num_Funct = 3;
f_samples_from_post = mvnrnd(f_mean, CovMatr_post, Num_Funct);
% Alternative formulation of f_mean
w_mean_alt = Sigma_p * PHI' / (PHI*Sigma_p*PHI' + sig_e^2*eye(N)) * Y;
w_mean - w_mean_alt
% check equality
w_Covar_Matrix_alt = Sigma_p - Sigma_p * PHI' / ...
(PHI*Sigma_p*PHI' + sig_e^2*eye(N)) * PHI*Sigma_p;
w_Covar_Matrix - w_Covar_Matrix_alt % check equality
f_mean_alt = zeros(1,length(x_pr));
for i = 1:length(x_pr)
f_mean_alt(i) =
phi(x_pr(i)) * w_mean_alt;
end
figure(2), clf, plot(x_pr, sigma_post, 'k--', 'LineWidth', 3), hold on,
plot(X, zeros(1,N), 'ro', 'MarkerFaceColor', 'r', 'MarkerSize', 10)
axis([-22 22 0 1.5]), set(gca, 'Fontsize', 20), box on
Listing 8.6 Code of a linear basis function regression model related to Figure 8.7 using polynomial
basis functions (Demo_LinBasisFun.m).
8.4.7
Gaussian Process Regression
In this section we will give intuitions and empirical evidence of the performance of the
vanilla GPR and advanced versions in both synthetic and real examples, as well as the
design of the covariance function.

Advances in Kernel Regression and Function Approximation
371
–15
–10
–5
–4
–2
0
2
4
(a)
(b)
0
5
10
15
0
0.2
0.4
0.6
0.8
1
–20
–10
0
10
20
Figure 8.7 Example of linear basis regression model using nonlocalized polynomial basis functions
(see Listing 8.6). (a) Posterior mean̂f(x) (solid line) and̂f(x) ±
√
̂𝜎(x) (colored area), three random
functions drawn according to the posterior distribution (dashed line), as shown in Listing 8.6. (b)
Posterior variance ̂𝜎(x) as function of x. We can observe that, with nonlocalized basis, ̂𝜎(x) is smaller
closer to the observed data and higher far away (it is the expected behavior).
Toy Examples
Let us start with a simple illustration of the GPR in a toy example. In Figure .we
include an illustrative example with six training points in the range between −and
+. We ﬁrst depict several random functions drawn from the GP prior and then we
include functions drawn from the posterior. We have chosen an isotropic Gaussian
kernel and 𝜎= .. We have plotted the mean function plus/minus two standard
deviations (corresponding to a % CI). Typically, the hyperparameters are unknown,
as well as the mean, covariance, and likelihood functions.
We assumed an SE covariance function and learned the optimal hyperparame-
ters by minimizing the negative log marginal likelihood (NLML) with respect to the
hyperparameters. We observe three diﬀerent regions in the ﬁgure. Below x = −.,
we do not have samples and the GPR provides the solution given by the prior (zero
mean and ±). At the center, where most of the data points lie, we have a very accurate
view of the latent function with small error bars (close to ±𝜎n). For x > , we do not
have training samples either, so we have the same behavior. GPs typically provide an
accurate solution where the data lies and high error bars where we do not have available
information; consequently, we presume that the prediction in that area is not accurate.
This is why in regions of the input space without points the CIs are wide, resembling
the prior distribution.
Structured, Nonstationary, and Multiscale in Gaussian Process Regression
Commonly used kernels families include the SE, periodic (Per), linear (Lin), and rational
quadratic (RQ); see Table .and Figure .. These base kernels can be actually
combined following simple operations: summation, multiplication or convolution. This
way one may build sophisticated covariances from simpler ones. Note that the same
essential property of kernel methods apply here: a valid covariance function must be

372
Digital Signal Processing with Kernel Methods
−2
−1.5
−1
−0.5
0
0.5
1
−5
0
5
Input, x
Output, y
Sampled priors
−2
−1.5
−1
−0.5
0
0.5
1
−5
0
5
Input, x
Output, y
Sampled posteriors
Figure 8.8 Example of a Gaussian process. Up: some functions drawn at random from the GP prior.
Down: some random functions drawn from the posterior; that is, the prior conditioned on six
noise-free observations indicated in big black dots. The shaded area represents the pointwise mean
plus and minus two times the standard deviation for each input value (95% confidence region). Note
the larger CIs for regions far from the observations.
positive semideﬁnite. In general, the design of the kernel should rely on the information
that we have for each estimation problem and should be designed to get the most
accurate solution with the least amount of samples. The related MATLAB code to
generate random draws from such base kernels and GP priors is given in Listing ..

Advances in Kernel Regression and Function Approximation
373
RQ
Linear
SE
Periodic
Lin+Per
Lin+SE
Figure 8.9 Base kernels (top) and two random draws from a GP with each respective kernel and
combination of kernels (bottom).
% Drawing from a GP prior
kernelType = 'SE';
% squared exponential kernel
switch
kernelType
case 'SE' % SE kernel
sig = 0.2; k = @(x1,x2) exp(-(x1-x2).^2/sig^2);
case 'RQ' % Rational-quadratic kernel
c = 0.05; k = @(x1,x2) (1 - (x1-x2).^2 ./ ((x1-x2).^2 + c));
case 'Per' % Periodic kernel
p = 0.5; % period
sig = 0.7; % SE kernel length
k = @(x1,x2) exp(-sin(pi*(x1-x2)/p).^2/sig^2);
case 'Lin' % Linear kernel
offset = 0; k = @(x1,x2)
(x1 - offset) .* (x2 - offset);
case 'Lin+Per' %% Linear+PER kernel
offset = -1; p = 0.5; % period
sig = 0.7; % SE kernel length
k = @(x1,x2) (x1 - offset).*(x2 - offset) + ...
exp(-sin(pi*(x1-x2)/p).^2/sig^2);
case 'Lin+SE' % Linear+SE kernel
offset = 0; sig=0.1;
k = @(x1,x2)
(x1 - offset).*(x2 - offset) + ...
exp(-(x1-x2).^2/sig^2);
end
% Generate plot of correspoding kernels
stepInp = 0.04; xforFig = 0; x = -1:0.02:1;
figure(1), clf, plot(x,k(x,xforFig),'k','LineWidth',8)
axis([-1 1 -1 max(k(x,xforFig))+0.5]), axis off
% Create kernel matrix
x = -1:stepInp:1; z = meshgrid(x); K = k(z,z');
% Proxy to phi transform using Cholesky decomp.
sphi = chol(K + 1e-6*eye(size(K)))';
Num_of_Funct = 5; % number of i.i.d. functions from GP Prior
% Genration of iid random functions - vectors
Functions_from_GP_Prior = sphi * randn(length(x), Num_of_Funct);
% Generate plot of the drawn functions
figure(2), clf, plot(x,Functions_from_GP_Prior,'LineWidth',8), axis off
Listing 8.7 Code generating GPs of Figure 8.9 (demo_GP_FunFromPrior.m).
In Listing .we show how to sample from the GP posterior and how to compute the
marginal likelihood.

374
Digital Signal Processing with Kernel Methods
%% Sampling GP Posterior and Computation of Marginal Likelihood
clear, clc
% Generating data
sig_e = 0; %
std of the noise
X = [-10 -3 2 8 12]; N = length(X); Y = sin(X) + sig_e * randn(1,N);
% kernel function
delta = 3; k = @(z1,z2) exp(-(z1-z2).^2/(2*delta^2));
% kernel matrix
z = meshgrid(X); K = k(z,z');
Lambda = (K + sig_e^2 * eye(N));
% inputs for test
x_pr = -20:0.2:20;
% posterior mean and variance
Kxx = zeros(length(x_pr),N);
f_mean = zeros(1,length(x_pr)); sig_post = f_mean;
for i = 1:length(x_pr)
kx = k(x_pr(i),X);
Kxx(i,:) = kx';
f_mean(i) = kx / Lambda * Y'; %
posterior mean
sig_post(i) = k(x_pr(i),x_pr(i)) - kx / Lambda * kx'; %
posterior ...
var.
end
% Sampling GP Posterior
z = meshgrid(x_pr); K_pr = k(z,z');
Sigma_pr = K_pr - Kxx / Lambda * Kxx';
f_samples_from_post = mvnrnd(f_mean, Sigma_pr, 2);
% Plot
figure(1), clf
V1 = f_mean - sqrt(sig_post); V2 = f_mean + sqrt(sig_post);
fill([x_pr, fliplr(x_pr)], [V1, fliplr(V2)], ...
'k','FaceAlpha', 0.3, 'EdgeAlpha', 0.7);
hold on; plot(x_pr, f_samples_from_post, '--', 'LineWidth', 3)
plot(x_pr, f_mean, 'k', 'LineWidth', 5)
plot(X, Y, 'ro', 'MarkerFaceColor', 'r', 'MarkerSize', 15);
axis([-20 20 -3.5 3.5]), set(gca, 'Fontsize', 20), box on
% Computation of the Marginal Likelihood
LogMargLike = -1/2 * (Y / Lambda * Y' + log(det(Lambda)) + N * log(2*pi));
Listing 8.8 Code for sampling from the GP posterior and computing the marginal likelihood
(Demo_GP_posterior.m).
Gaussian Process Regression with a Composite Time Covariance
We show the advantage of encoding such prior knowledge and structure in the relevant
problem of solar irradiation prediction using GPR. Noting the cyclostationary behavior
of the signal, we developed a particular time-based composite covariance to account for
the relevant seasonal signal variations. Data from the AEMET, http://www.aemet.es/,
radiometric observatory of Murcia (southern Spain, .◦N, .◦W) were used.
Table .reports the results obtained with GPR models and several statistical regres-
sion methods: regularized linear regression (RLR), SVR, RVM, and GPR. All methods
were run with and without using two additional dummy time features containing the
year and day-of-year. We will indicate the former case with a subscript (e.g., SVRt).
First, including time information improves all baseline models. Second, the best overall
results are obtained by the GPR models, whether including time information or not.

Advances in Kernel Regression and Function Approximation
375
Table 8.3 Results for the estimation of the daily solar
irradiation of linear and nonlinear regression models. Sub-
script Methodt indicates that the method includes time as
input variable. Best results are highlighted in bold, and the
second best in italics.
Method
ME
RMSE
MAE
R
RLR
.
.
.
.
RLRt
.
.
.
.
SVR
.
.
.
.
SVRt
.
.
.
.
RVM
.
.
.
.
RVMt
.
.
.
.
GPR
.
.
.
.
GPRt
.
.
.
.
TGPR
.
.
.
.
Third, in particular, the TGPR model outperforms the rest in accuracy (RMSE, MAE)
and goodness-of-ﬁt 𝜌, and closely follows the elastic net in bias (ME). TGPR performs
better than GPR and GPRt in all quality measures. From the analysis of variance,
statistical diﬀerences between TGPR and the rest of the models were observed both
for bias (F = ., p < .) and accuracy (F = ., p < .).
Ensemble Gaussian Process Regression Models
We now give an example of an ensemble of GPR machines where the use of the posterior
variance is used to determine the conﬁdence of the models obtained. The code is
provided in Listing ., and the resulting results are given in Figure ..
% Example of EGPR
clear, clc
% GENERATING DATA
sig_e = 0.6; % std of the noise
X = -15:0.6:15; N = length(X); Y = sin(X) + sig_e * randn(1,N);
% kernel function
delta = 2; k = @(z1,z2) exp(-(z1-z2).^2/(2*delta^2));
% Inputs test
x_pr = -20:0.2:20;
% Complete GPR
% Kernel matrix
z1 = meshgrid(X); K = k(z1,z1');
Lambda = (K + sig_e^2 * eye(N));
% Posterior mean and variance of the complete GP

376
Digital Signal Processing with Kernel Methods
–20
–10
0
10
20
–3
–2
–1
0
1
2
3
(a)
(b)
–20
–10
0
10
20
–3
–2
–1
0
1
2
3
Ensemble GP
Partial GPs
Ensemble GP
Standard GP
Figure 8.10 Example of ensemble GPR considering R = 5 partial GPs. (a) Ensemble solution̂f and R = 5
partial solutionŝfr. (b) Ensemble posterior mean̂f and its corresponding posterior mean obtained by a
standard complete GP considering jointly all the data.
f_mean = zeros(1,length(x_pr)); sig_post = f_mean;
for i = 1:length(x_pr)
kx = k(x_pr(i),X);
f_mean(i) = kx / Lambda * Y'; % posterior mean
sig_post(i) = k(x_pr(i),x_pr(i)) -
kx / Lambda * kx'; % posterior ...
var.
end
% Partial GPs
R = 5; % number of partial GPs
Nr = fix(N/5); % number of data per GP (other choices are possible)
fr = zeros(R,length(x_pr)); sr = fr;
for r = 1:R

Advances in Kernel Regression and Function Approximation
377
pos = randperm(N,Nr); % random pick
Xr = X(pos); Yr = Y(pos);
zr = meshgrid(Xr); Kr = k(zr,zr');
Lambda_r = (Kr + sig_e^2 * eye(Nr)); % for the partial GPs
% posterior mean and variance for the partial GPs
for i = 1:length(x_pr)
kx_r = k(x_pr(i),Xr);
fr(r,i) = kx_r / Lambda_r * Yr'; % partial posterior mean
sr(r,i) = k(x_pr(i),x_pr(i)) - kx_r / Lambda_r * kx_r'; % p.p. ...
var.
end
end
% Ensemble GP solution
Prec = 1./sr; % precision
Den = sum(1./sr); Wn_prec = Prec./repmat(Den,R,1);
% normalized weights
f_EGP = sum(Wn_prec.*fr); sig_ESP = mean(Wn_prec);
% Represent
figure(1), clf, plot(x_pr,f_EGP,'b','LineWidth',5), hold on
plot(x_pr,fr,'--','LineWidth',2)
plot(X,Y,'ro','MarkerFaceColor','r','MarkerSize',12)
axis([-20 20 -3.5 3.5]), set(gca,'Fontsize',20)
legend('Ensemble GP','Partial GPs'); box on
figure(2), clf, plot(x_pr,f_EGP,'b','LineWidth',5), hold on
plot(x_pr,f_mean,'k--','LineWidth',2)
plot(X,Y,'ro','MarkerFaceColor','r','MarkerSize',12)
axis([-20 20 -3.5 3.5]), set(gca,'Fontsize',20)
legend('Ensemble GP','Standard GP'); box on
Listing 8.9 Simple example of ensemble GPR code.
Heteroscedastic and Warped Gaussian Process Regression
We focus now on a real problem in geosciences: the estimation of chlorophyll-a
concentrations from remote sensing upwelling radiance just above the ocean surface. A
variety of bio-optical algorithms have been developed to relate measurements of ocean
radiance to in situ concentrations of phytoplankton pigments, and ultimately most of
these algorithms demonstrate the potential of quantifying chlorophyll-a concentrations
accurately from multispectral satellite ocean color data. In this context, robust and
stable non-linear regression methods that provide inverse models are desirable. In
addition, we should note that most of the bio-optical models (such as Morel, CalCOFI
and OC/OCmodels) often rely on empirically adjusted nonlinear transformation of
the observed variable (which is traditionally a ratio between bands).
The experiment uses the SeaBAM dataset (Maritorena and O’Reilly, ; O’Reilly
et al., ), which gathers in situ pigment measurements around the United States
and Europe. The dataset contains coincident in situ chlorophyll concentration and
remote sensing reﬂectance measurements (Rrs(𝜆), [sr−]) at some wavelengths (,
, , and nm) that are present in the SeaWiFS ocean color satellite sensor.
The chlorophyll concentration values range from .to .mg/m(revealing a
clear exponential distribution). Actually, even though SeaBAM data originate from
various researchers, the variability in the radiometric data is limited. In fact, at high
Chl-a concentrations, Ca [mg/m], the dispersion of radiance ratios Rrs()/Rrs()

378
Digital Signal Processing with Kernel Methods
Table 8.4 Bias (ME), accuracy (RMSE, MAE) and fitness (Pearson’s 𝜌) for different rates of
training samples using both raw and empirically transformed observation variables.
Raw
Empirically based
ME
RMSE
MAE
𝝆
ME
RMSE
MAE
𝝆
Rate = 10%
GPR
.
.
.
.
.
.
.
.
VHGPR
.
.
.
.
.
.
.
.
WGPR
.
.
.
.
.
.
.
.
Rate = 40%
GPR
.
.
.
.
.
.
.
.
VHGPR
.
.
.
.
.
.
.
.
WGPR
.
.
.
.
.
.
.
.
increases, mostly because of the presence of Case II waters. The shape of the scatterplots
is approximately sigmoidal (in log-log space). At lowest concentrations the highest
Rrs()/Rrs() ratios are slightly lower than the theoretical limit for clear natural
waters. See analysis in (Camps-Valls et al., b). More information about the data
can be obtained from http://seabass.gsfc.nasa.gov/seabam/seabam.html.
Table .shows diﬀerent scores (ME, accuracy, Pearson correlation) between the
observed and predicted variable when using the raw data (no ad hoc transform at
all) and the empirically adjusted transform. Results are shown for three ﬂavors of
GPs: the standard GPR (Rasmussen and Williams, ), the VHGPR (Lázaro-Gredilla
et al., ), and the proposed WGPR (Lázaro-Gredilla, ; Snelson et al., )
for diﬀerent rates of training samples. Several conclusions can be obtained: () as
expected, better results are obtained by all models when using more training sam-
ples to learn the hyperparameters; () empirically based warping slightly improves
the results over working with raw data for the same number of training samples,
but this requires prior knowledge about the problem, and time and eﬀort to ﬁt an
appropriate function; () WGPR outperforms the rest of the GPs in all comparisons
over standard GPR and VHGPR (≈–%); and ﬁnally () WGPR nicely compensates
the lack of prior knowledge about the (possibly skewed) distribution of the observation
variable.
An interesting advantage of WGPR is to observe the learned warping function. We
plot these for diﬀerent rates of training samples in Figure ., which shows that: (a) as
more samples are provided for learning, the warping function becomes more nonlinear
for low concentration values; (b) the learned warping function actually looks linear
(in log scale) for high observation values and strongly nonlinear for low values. The
empirically based warping function typically used in most bio-optical models is a log
function. Therefore, it seems that the WGPR accuracy comes from the better modeling
of the nonlinearity for low chlorophyll values, which are the great majority in the
database. The interested reader, may ﬁnd the code of the VHGPR, along with other
methods described in this chapter, such as SVR, KRR, RVM, and GPR, on the book web
page.

Advances in Kernel Regression and Function Approximation
379
−0.5
0
0.5
10–2
10–1
100
101
Normalized observation range, y
5%
10%
20%
40%
Warping, g
Figure 8.11 Learned warping with WGPR for different rates of training samples.
8.4.8
Relevance Vector Machines
This section exempliﬁes the RVM in simple D function approximation toy examples,
and in complex time-series prediction in chaotic time series. We also give some remarks
and analysis concerning the CIs obtained compared with those obtained with GPR.
Toy Examples
Listing .contains an example of an RVM model for regression and its comparison
with the result of a GP for regression. Figure .shows the result.
% RVM compared with the GPR solution
clear, clc
% Generating data
sig_e = 0.5; % std of the noise
X = [-12 -10 -3 -2
2 8 12 14]';
N = length(X); % number of data
Y = sin(X) + sig_e * randn(N,1); % N x 1
% Basis functions
M = N; % number of basis functions
=
data size (RVM)
sig = 2; phi = @(x1,x2) exp(-(x1-x2).^2/sig^2);
% Basis function matrix
z = meshgrid(X); PHI = phi(z,z'); % N x M
% Prior variance
% For simplicity, we assume that we learn in training
% the same sigma_prior for each basis function/weight
% hence, we assume the same relevance for each basis function
sig_prior = 1;
Sigma_p = sig_prior^2 * eye(M); % Cov-Matrix of the prior over w

380
Digital Signal Processing with Kernel Methods
(b)
–15
–4
–2
0
2
4
–10
–5
0
RVM solution
5
10
15
–15
–4
–2
0
2
4
(a)
–10
–5
0
RVM solution
5
10
15
0
0.2
0.4
0.6
0.8
1
1.2
(c)
–20
–10
0
10
20
Variances σ^ (x)
GP
RVM
RVM
GP
Data points
Figure 8.12 Example of an RVM model. (a) Using localized Gaussian basis functions with three
random functions drawn according to the posterior distribution (dashed line); see Listing 8.10. (b) GPR
solution given the same data points. (c) Comparison of the two predictive and posterior variances.
Note that the variance of the RVM with localized basis functions is higher when closer to the data
points, which is an undesirable effect. Big dots show the positions of the data (xi, i = 1,…,N).
% moments of the posterior of w
w_mean = ((PHI'*PHI) + sig_e^2/sig_prior^2*eye(M)) \ PHI' * Y; % M x 1
w_Covar_Matrix = ((1/sig_e^2)*(PHI'*PHI)+inv(Sigma_p)) \ eye(M); % M x M
% Test inputs
x_pr = -16:0.2:16;
Lambda = (PHI + sig_e^2 * eye(N)); % FOR GP comparison
% Posterior mean and variance of f
f_mean = zeros(1,length(x_pr)); sigma_post = f_mean;
f_meanGP = f_mean; sigma_postGP = f_meanGP;
for i = 1:length(x_pr)
Phix = phi(x_pr(i),X)';
f_mean(i) =
Phix * w_mean; % RVM
sigma_post(i) = Phix * w_Covar_Matrix * Phix'; % RVM
f_meanGP(i) = Phix / Lambda * Y; % GP
sigma_postGP(i) = phi(x_pr(i),x_pr(i)) - Phix / Lambda * Phix'; % GP

Advances in Kernel Regression and Function Approximation
381
end
% Sampling functions from RVM Posterior
Num_Funct = 3;
w_sample = mvnrnd(w_mean, w_Covar_Matrix, Num_Funct);
f_samples_from_post = zeros(length(x_pr), Num_Funct);
for i = 1:length(x_pr)
Phix = phi(x_pr(i),X)';
f_samples_from_post(i,:) = Phix * w_sample';
end
% RVM
V1 = f_mean - sqrt(sigma_post); V2 = f_mean + sqrt(sigma_post);
figure(1), clf, plot(x_pr, f_mean, 'k', 'LineWidth', 5), hold on
fill([x_pr, fliplr(x_pr)], [V1, fliplr(V2)], 'k', ...
'FaceAlpha', 0.3, 'EdgeAlpha', 0.7)
plot(x_pr, f_samples_from_post,'--', 'LineWidth', 3);
plot(X, Y, 'ro', 'MarkerFaceColor', 'r', 'MarkerSize', 10)
axis([-16 16 -4.2 4.2]), set(gca, 'Fontsize', 20)
h = legend('RVM'); set(h,'Box','off'), box on
% GP
V1 = f_meanGP - sqrt(sigma_postGP); V2 = f_meanGP + sqrt(sigma_postGP);
figure(2), clf, plot(x_pr, f_meanGP, 'k', 'LineWidth', 5), hold on
fill([x_pr, fliplr(x_pr)], [V1, fliplr(V2)], 'k', ...
'FaceAlpha', 0.3, 'EdgeAlpha', 0.7)
plot(X, Y, 'ro', 'MarkerFaceColor', 'r', 'MarkerSize', 10)
axis([-16 16 -4.2 4.2]), set(gca, 'Fontsize', 20),
h = legend('GP'); set(h,'Box','off'), box on
% RVM & GP
figure(3); clf, plot(x_pr, sigma_post, 'k', 'LineWidth', 5),
hold on
plot(x_pr, sigma_postGP, 'r--', 'LineWidth', 5),
plot(X, zeros(1,N), 'ro', 'MarkerFaceColor', 'r', 'MarkerSize', 10)
axis([-22 22 0 max(sigma_postGP)+0.4]), set(gca, 'Fontsize', 20),
h = legend('RVM','GP','Data points'); set(h,'Box','off'), box on
Listing 8.10 Code comparing RVM and GP solutions.
Time Series Prediction
Despite these good characteristics, the use of RVM for nonlinear system identiﬁcation
and time-series prediction is limited to a few studies (Anguita and Gagliolo ;
Bishop and Tipping ; D’Souza et al. ; Nikolaev and Tino ; Quiñonero-
Candela ; Wipf et al. ), and in all cases the approach consisted of stacking the
input and output discrete time processes into a training sample and then applying the
traditional RVM formulation. This approach, though powerful, does not consider the
input–output discrete time process relationships in the modeling, which can ultimately
lead to suboptimal results. Here, we introduce a more general class of RVM-based
system identiﬁcation algorithms by means of composite kernels following the RKHS
models introduced in Chapter , and show that the previous stacked approach is a
particular case. Several algorithms for nonlinear system identiﬁcation are presented,
accounting for the input and output time processes either separately or jointly, allowing
diﬀerent levels of ﬂexibility and sophistication for model development.

382
Digital Signal Processing with Kernel Methods
Table 8.5 Results for the MG time series prediction problem.
RVM
RVM2K
RRVM2K
RVM4K
RRVM4K
MG
nMSE
−.
−.
−.
−.
–.
P





̂𝜎n
.
.
.
.
.
%RVs
.
.
.
.
.
MG
nMSE
−.
−.
−.
−.
–.
P





̂𝜎n
.
.
.
.
.
%RVs
.
.
.
.
.
We test the RKHS models introduced in Chapter in the RVM for time-series
prediction. More results and theoretical analysis of the methods can be found in Camps-
Valls et al. (a). In particular, we compare the models performance in the standard
Mackey–Glass time-series prediction problem, which is well known for its strong
nonlinearity. This classical high-dimensional chaotic system is generated by the delay
diﬀerential equation: dx∕dt = −.xn + .xn−Δ∕(+ x
n−Δ), with delays Δ = 
and Δ = , thus yielding the time series MGand MGrespectively. We con-
sidered training samples and used the next for free parameter selection
(validation set).
Results are shown in Table .. The RVM yields very good results, compared with
previous results in the literature (Mattera, ). The RKHS models for RVM never-
theless outperform the standard RVM, especially signiﬁcant for the MGtime series.
In the case of MGthe diﬀerences are not signiﬁcant, but there is still a preference
for RVMK-based models, suggesting that this is a more complicated system. Several
interesting issues can be noticed. First, the fact that this dataset has virtually no output
noise is better detected with all composite methods (lower ̂𝜎n than the standard RVM).
Second, the expected zero noise variance along with the chaotic nature of the time series
prevent sparsity from arising in a trivial way. We observe, however, that the number of
RVs retained by the methods is smaller than the standard RVM in MG, but as the
dynamics complexity is increased (i.e., MG), kernel methods need more RVs to attain
competitive results. The MATLAB code to reproduce the results can be downloaded
from the book web page.
8.5
Concluding Remarks
This chapter treated the problem of advanced regression and function approximation
with kernels, under the two main existing approximations: methods based on the
SVR and the alternative Bayesian nonparametric treatment of regression modeling.

Advances in Kernel Regression and Function Approximation
383
We reviewed several techniques in each one of the approaches and illustrated their
performance in real scenarios in signal and data processing problems. In particular,
for the SVR-based regression approaches, we studied an SVR version for multi-
output function approximation, a kernel-based regression to cope with signal and
noise dependencies (KSNR), and the SS-SVR to incorporate the wealth of unlabeled
samples in the regression function. On the other, we summarized some important
achievements in the ﬁeld of Bayesian nonparametrics and focused on the RVM and
diﬀerent developments of GPR algorithms. Both families have found wide application
in signal processing in the last decade (Pérez-Cruz et al., ; Rojo-Álvarez et al.,
). In both cases, we paid attention to the important problem of hyperparameter
estimation, and provided some prescriptions and procedures for this core task. Methods
performance was illustrated in both synthetic and real application examples, and
tutorials and code snippets were provided.
The ﬁeld of regression and function approximation is very vast, and there are plenty
of new algorithmic developments in both families, and more algorithms are to come.
In the near future, we foresee two main directions of research: () design methods
that adapt to the signal characteristics, and () algorithms that self-tune or infer their
hyperparameters from the data in an autonomous way. Chapter will complete the
topic by treating adaptive ﬁltering and dynamical systems with kernels.
8.6
Questions and Problems
Exercise ..
Derive some of the iterative weighted recursive least squares (RLS)
equations for 𝜈-SVR. Use the primal Equation .and constraints in Equation .to
prove that a Lagrangian of the form
LP = 
‖w‖+
N
∑
i=
𝛼i(yi −⟨w, 𝝓(xi)⟩−b −𝜀)
+
N
∑
i=
𝛼∗
i (⟨w, 𝝓(xi)⟩) + b −yi −𝜀) + 𝜀(CN𝜈−𝜆)
(.)
exists, and then compute its gradient from 𝜉i, 𝜉∗
i to prove, in combination with the
deﬁnitions in Equation ., that the values of ai, a∗
i are the ones in Equation ..
Exercise ..
Derive some more of the iterative weighted RLS equations for 𝜈-
SVR. Using the deﬁnitions in Equation .derive Equation .. Then, computing
its gradient with respect variables w, b, and 𝜀and using the representer theorem, ﬁnd
Equation ..

384
Digital Signal Processing with Kernel Methods
Exercise ..
Prove that the equivalent IRWLS updates for ai and a∗
i in the PD-
SVR are
a(∗)
i
=
⎧
⎪
⎨
⎪⎩
,
e(∗)
i
< 
ciC
e(∗)
i
,
e(∗)
i
≥.
Exercise ..
The solution of the KRR (and equivalently the GPR one) is closed-form;
that is, 𝜶= (K + 𝜆I)−y. Derive the equations for solving the problem using gradient
descent.
Exercise ..
Kernel methods in general are gray machines. A possibility to under-
stand the implemented function comes from the concept of sensitivity analysis, which
measures how the approximating function varies with respect to the inputs. The
sensitivity of feature xnj in example xn ∶= [xn, … , xnd]T is deﬁned as
sj = ∫
(𝜕𝝓(x)
𝜕xj
)
p(x) dx,
where p(x) is the pdf over the d-dimensional input vector x and 𝝓(x) represents either
the predictive mean, 𝜇GP∗, or variance, 𝜎
GP∗. The empirical estimate of the sensitivity
for the jth feature can be written as
sj = 
N
N
∑
n=
(𝜕𝝓(xn)
𝜕xnj
)
,
where N denotes the number of training samples. Derive the sensitivity maps for both
the predictive mean and the variance assuming an SE kernel function.
Exercise ..
Assuming that the joint process {y, ̂f (x)}, where ̂f (x) is the predictive
mean over x in a GP, has a Gaussian distribution with zero mean, prove that its
covariance function is
[ K + 𝜎I
k
kT
K(x, x)
]
.
Using Bayes’ rule and this joint distribution, derive the predictive variance of the GP
shown in Equation .(Rasmussen and Williams, ).
Exercise ..
A crucial point in kernel regression methods is the inversion of a
potentially big N × N matrix. An alternative is to perform a Cholesky decomposition
such that K = RRT. Write a MATLAB code to do the inversion of the (regularized)
kernel matrix, and analyze the computational gain in terms of the rank of K.

Advances in Kernel Regression and Function Approximation
385
Exercise ..
Derive the equations of a randomized GPR in which the kernel func-
tion is approximated with random Fourier features (Rahimi and Recht, ). Given D
random features, what is the order of memory and computational gains.
Exercise ..
Study and develop a MATLAB function to run the sparsiﬁcation
(reduced-rank) procedure for KRR presented in Cawley and Talbot ().
Exercise ..
Derive the leave-one-out estimates for GPs predictive mean and
predictive variance.
Exercise ..
Discuss the issue of considering nonlocalized basis functions in the
context of the RVM. Hint: use the remark in Section .., and the study of Rasmussen
and Quiñonero-Candela ().

387
9
Adaptive Kernel Learning for Signal Processing
9.1
Introduction
Adaptive ﬁltering is a central topic in signal processing. An adaptive ﬁlter is a ﬁlter
structure provided with an adaptive algorithm that tunes the transfer function, typically
driven by an error signal. Adaptive ﬁlters are widely applied in nonstationary environ-
ments because they can adapt their transfer function to match the changing parameters
of the system that generates the incoming data (Hayes ; Widrow et al. ). They
have become ubiquitous in current DSP, mainly due to the increase in computational
power and the need to process streamed data. Adaptive ﬁlters are now routinely used in
all communication applications for channel equalization, array beamforming, or echo
cancellation, to cite just a few, and in other areas of signal processing, such as image
processing or medical equipment.
By applying linear adaptive ﬁltering principles in the kernel feature space, powerful
nonlinear adaptive ﬁltering algorithms can be obtained. This chapter introduces the
wide topic of adaptive signal processing, and it explores the emerging ﬁeld of kernel
adaptive ﬁltering (KAF). Its orientation is diﬀerent from the preceding ones, as adaptive
processing can be used in a variety of scenarios. Attention is paid to kernel LMS and
RLS algorithms, to previous taxonomies of adaptive kernel methods, and to emergent
kernel methods for online and recursive KAF. MATLAB code snippets are included as
well to illustrate the basic operations of the most common kernel adaptive ﬁlters. Tuto-
rial examples are provided on applications, including chaotic time-series prediction,
respiratory motion prediction, and nonlinear system identiﬁcation.
9.2
Linear Adaptive Filtering
Let us ﬁrst deﬁne some basic concepts of linear adaptive ﬁltering theory. The goal of
adaptive ﬁltering is to model an unknown, possibly time-varying system by observing
the inputs and outputs of this system over time. We will denote the input to the system
on time instant n as xn, and its output as dn. The input signal xn is assumed to be zero-
mean. We represent it as a vector, and it will often represent a time-delay vector of L
taps of a signal xn on time instant n as xn = [xn, xn−, … , xn−L+]T.
Digital Signal Processing with Kernel Methods, First Edition. José Luis Rojo-Álvarez, Manel Martínez-Ramón,
Jordi Muñoz-Marí, and Gustau Camps-Valls.
© John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.

388
Digital Signal Processing with Kernel Methods
xn
Unknown system
−
yn
+
dn
Adaptive algorithm
en
wn
Figure 9.1 A linear adaptive filter for system identification.
A diagram for a linear adaptive ﬁlter is depicted in Figure .. The input to the adaptive
ﬁlter at time instant n is xn, and its response yn is obtained through the following linear
operation:
yn = wH
n xn,
(.)
where H is the Hermitian operator.
Linear adaptive ﬁltering follows the online learning framework, which consists of two
basic steps that are repeated at each time instant n. First, the online algorithm receives
an observation xn for which it calculates the estimated image yn, based on its current
estimate of wn. Next, the algorithm receives the desired output dn (also known as a
symbol in communications), which allows it to calculate the estimation error en = dn −
yn and update its estimate for wn. In some situations dn is known a priori; that is, the
received signal xn is one of a set of training signals provided with known labels. The
procedure is then called supervised. When dn belongs to a ﬁnite set of quantized labels,
and it can be assumed that the error will be likely much smaller than the quantization
step or minimum Euclidean distance between labels, the desired label is estimated by
quantizing yn to the closest label. In these cases, the algorithm is called decision directed.
9.2.1
Least Mean Squares Algorithm
The classical adaptive optimization techniques have their roots in the theoretical
approach called the steepest-descent algorithm. Assume that the expectation of the
squared error signal, Jn = 𝔼[|en|] can be computed. Since this error is a function of
the vector wn, the idea of the algorithm is to modify this vector toward the direction
of the steepest descent of Jn. This direction is just opposite to its gradient 𝛻wJn. Indeed,
assuming complex stationary signals, the error expectation is
𝔼[|en|] = 𝔼[|dn −wH
n xn|]
= 𝔼[|dn|+ wH
n xnxH
n wn −wH
n xnd∗
n]
= 𝜎
d + wH
n Rxxwn −wH
n pxd,
(.)
where Rxx is the signal autocorrelation matrix, pxd is the cross-correlation vector
between the signal and the ﬁlter output, and 𝜎
d is the variance of the system output.
Its gradient with respect to vector wn is expressed as

Adaptive Kernel Learning for Signal Processing
389
𝛻wJn = Rxxwn −pxd.
(.)
The adaptation rule based on steepest descent thus becomes
wn+= wn −𝜂𝛻wJn,
(.)
where 𝜂is the step size or learning rate of the algorithm.
The LMS algorithm, introduced by Widrow in (Widrow et al., ), is a very
simple and elegant method of training a linear adaptive system to minimize the MSE
that approximates the gradient 𝛻wJn using an instantaneous estimate of the gradient.
From Equation ., an approximation can be written as
𝛻wJn ≈xnxH
n wn −xnd∗
n.
(.)
Using this approximation in Equation .leads to the well-known stochastic gradient
descent update rule, which is the core of the LMS algorithm:
wn+= wn −𝜂xn(xH
n wn −d∗
n)
= wn + 𝜂xne∗
n.
(.)
This optimization procedure is also the basis for tuning nonlinear ﬁlter structures such
as NNs (Haykin, ) and some of the kernel-based adaptive ﬁlters discussed later in
this chapter. A detailed analysis including that of convergence and misadjustment is
given in Haykin (). The MATLAB code for the LMS training step on a new data
pair (x, d) is displayed in Listing ..
y = x' * w; % evaluate filter output
err = d - y; % instantaneous error
w = w + mu * x * err'; % update filter coefficients
Listing 9.1 Training step of the LMS algorithm on a new datum (x, d).
Under the stationarity assumption, the LMS algorithm converges to the Wiener
solution in mean, but the weight vector wn shows a variance that converges to a value
that is a function of 𝜂. Therefore, low variances are only achieved at low adaptation
speed. A more sophisticated approach with faster convergence is found in the the RLS
algorithm.
9.2.2
Recursive Least-Squares Algorithm
The RLS algorithm was ﬁrst introduced in (Plackett, ). In a stationary scenario,
it converges to the Wiener solution in mean and variance, improving also the slow rate
of adaptation of the LMS algorithm. Nevertheless, this gain in convergence speed comes
at the price of a higher complexity, as we will see later.
Recursive Update
The basis of the RLS algorithm consists of recursively updating the vector w that
minimizes a regularized version of the cost function Jn:
Jn =
n
∑
i=
|di −xH
i w|+ 𝛿wHw,
(.)

390
Digital Signal Processing with Kernel Methods
where 𝛿is a positive constant regularization factor. The regularization factor penalizes
the squared norm of the solution vector so that this solution does not apply too much
weight to any speciﬁc dimension.The solution that minimizes the LS cost function (.)
is well known and given by
w = (Rxx + 𝛿I)−pxd.
(.)
The regularization 𝛿guarantees that the inverse in Equation .exists. In the absence
of regularization (i.e., for 𝛿= ), the solution requires one to invert the matrix Rxx,
which may be rank deﬁcient. For a detailed derivation of the RLS algorithm we refer the
reader to Haykin () and Sayed (). In the following, we will provide its update
equations and a short discussion of its properties compared with LMS.
We denote the autocorrelation matrix for the data xtill xn as Rn:
Rn =
n
∑
i=
xH
i xi.
(.)
RLS requires the introduction of an inverse autocorrelation matrix Pn, deﬁned as
Pn = (Rn + 𝛿I)−.
(.)
At step n −of the recursion, the algorithm has processed n −data, and its estimate
wn−is the optimal solution for minimizing the squared cost function (Equation .) at
time step n −. When a new datum xn is obtained, the inverse autocorrelation matrix
is updated as
Pn = Pn−−gngH
n Pn−,
(.)
where gn is the gain vector of the RLS algorithm:
gn =
Pn−
+ xH
n Pn−xn
.
(.)
The update of the solution itself reads
wn = wn−+ gnen,
(.)
in which en represents the usual prediction error en = dn −xH
n wn−. The RLS algorithm
starts by initializing its solution to w= , and the estimate of the inverse autocorrela-
tion matrix Pn to P= 𝛿−I. Then, it recursively updates its solution by including one
datum xi at a time and performing the calculations from Equations .–.. Owing to
the matrix multiplications involved in the RLS updates, the computational complexity
per time step for RLS is quadratic in terms of the data dimension, (L), while the LMS
algorithm has only linear complexity, (L).
A slightly more general formulation involves a regularization matrix which can penalize the individual
elements of the solution diﬀerently (Sayed, ).

Adaptive Kernel Learning for Signal Processing
391
Exponentially Weighted Recursive Least-Squares
The RLS algorithm takes into account all previous data when it updates its solution
with a new datum. This kind of update yields a faster convergence than LMS, which
guides its update based only on the performance on the newest datum. Nevertheless,
by guaranteeing that its solution is valid for all previous data, the RLS algorithm is in
essence looking for a stationary solution, and thus it cannot adapt to nonstationary
scenarios, where a tracking algorithm is required. LMS, on the other hand, deals
correctly with nonstationary scenarios, thanks to the instantaneous nature of its update,
which forgets older data and only adapts to the newest datum.
A tracking version of RLS can be obtained by including a forgetting factor 𝜆∈(, ]
in its cost function as follows:
Jn =
n
∑
i=
𝜆n−i‖di −xH
i w‖+ 𝜆n𝛿wHw.
(.)
The resulting algorithm is called exponentially weighted RLS (Haykin, ; Sayed,
). The inclusion of the forgetting factor assigns lower weights to older data,
which allows the algorithm to adapt gradually to changes. The update for the inverse
autocorrelation matrix becomes
Pn = 𝜆−Pn−−𝜆−gnxH
n Pn−,
(.)
and the new gain vector becomes
gn =
𝜆−Pn−
+ 𝜆−xH
n Pn−xn
.
(.)
The MATLAB code for the training step of the exponentially weighted RLS algorithm
is displayed in Listing ..
y = x' * w; % evaluate filter output
err = d - y; % instantaneous error
g = P * x / (lambda + x'*P*x); % gain vector
w = w + g * err; % update filter coefficients
P = lambda \ (P - g*x'*P); % update inverse autocorrelation matrix
Listing 9.2 Training step of the exponentially weighted RLS algorithm on a new datum (x, d).
Recursive estimation algorithms play a crucial role for many problems in adaptive
control, adaptive signal processing, system identiﬁcation, and general model building
and monitoring problems (Ljung, ). In the signal-processing literature, great atten-
tion has been paid to their eﬃcient implementation. Linear AR models require relatively
few parameters and allow closed-form analysis, while ladder or lattice implementation
of linear ﬁlters has long been studied in signal theory. However, when the system
generating the data is driven by nonlinear dynamics, the model speciﬁcation and
parameter estimation problems increase their complexity, and hence nonlinear adaptive
ﬁltering becomes strictly necessary.
Note that, in the ﬁeld of control theory, a range of sequential algorithms for
nonlinear ﬁltering have been proposed since the s, notably the extended KF

392
Digital Signal Processing with Kernel Methods
xn
Unknown system
κ(xn,·)
−
yn
+
dn
Adaptive algorithm
en
αn
Figure 9.2 A kernel adaptive filter for nonlinear system identification.
(Lewis et al., ) and the unscented KF (Julier and Uhlmann, ), which
are both nonlinear extensions of the celebrated KF (Kalman, ), and particle
ﬁlters (Del Moral, ). These methods generally require knowledge of a state-space
model, and while some of them are related to adaptive ﬁltering, we will not deal with
them explicitly in this chapter.
9.3
Kernel Adaptive Filtering
The nonlinear ﬁltering problem and the online adaptation of model weights were ﬁrst
addressed by NNs in the s (Dorﬀner ; Narendra and Parthasarathy ).
Throughout the last decade, a great interest has been devoted to developing nonlinear
versions of linear adaptive ﬁlters by means of kernels (Liu et al., ). The goal is to
develop machines that learn over time in changing environments, and at the same time
adopt the nice characteristics of convexity, convergence, and reasonable computational
complexity, which was not successfully implemented in NNs.
Kernel adaptive ﬁltering aims to formulate the classic linear adaptive ﬁlters in RKHS,
such that a series of convex LS problems is solved. Several basic kernel adaptive ﬁlters
can be obtained by applying a linear adaptive ﬁlter directly on the transformed data,
as illustrated in Figure .. This requires the reformulation of scalar-product-based
operations in terms of kernel evaluations. The resulting algorithms typically consist
of algebraically simple expressions, though they feature powerful nonlinear ﬁltering
capabilities. Nevertheless, the design of these online kernel methods requires dealing
with some of the challenges that typically arise when dealing with kernels, such as
overﬁtting and computational complexity issues.
In the following we will discuss two families of kernel adaptive ﬁlters in detail, namely
KLMS and kernel RLS (KRLS) algorithms. Several related kernel adaptive ﬁlters will be
reviewed brieﬂy as well.
9.4
Kernel Least Mean Squares
The early approach to kernel adaptive ﬁltering introduced a kernel version of the
celebrated ADALINE in Frieß and Harrison (), though this method was not online.
Kivinen et al. () proposed an algorithm to perform stochastic gradient descent in

Adaptive Kernel Learning for Signal Processing
393
RKHS: the so-called naive online regularized risk minimization algorithm (NORMA)
introduces a regularized risk that can be solved online and can be shown to be equivalent
to a kernel version of leaky LMS, which itself is a regularized version of LMS.
9.4.1
Derivation of Kernel Least Mean Squares
As an illustrative guiding example of a kernel adaptive ﬁlter, we will take the kerneliza-
tion of the standard LMS algorithm, known as KLMS (Liu et al., ). The approach
employs the traditional kernel trick. Essentially, a nonlinear function 𝝓(⋅) maps the data
xn from the input space to 𝝓(xn) in the feature space. Let wbe the weight vector in this
space such that the ﬁlter output is yn = w,nTxn, where w,n is the estimate of wat time
instant n. Note that we will be taking scalar products of real-valued vectors from now
on. Given a desired response dn, we wish to minimize squared loss, Jw,n, with respect to
w. Similar to Equation ., the stochastic gradient descent update rule obtained reads
w,n = w,n−+ 𝜂en𝝓(xn).
(.)
By initializing the solution as w,= (and hence e= d= ), the solution after n
iterations can be expressed in closed form as
w,n = 𝜂
n
∑
i=
ei𝝓(xi).
(.)
By exploiting the kernel trick, one obtains the prediction function
y∗= 𝜂
n
∑
i=
ei⟨𝝓(xi), 𝝓(x∗)⟩= 𝜂
n
∑
i=
eiK(xi, x∗),
(.)
where x∗represents an arbitrary input datum and K(⋅, ⋅) is the kernel function; for
instance, the commonly used Gaussian kernel K(xi, xj) = exp(−‖xi −xj‖∕𝜎) with
kernel width 𝜎. Note that the weights w,n of the nonlinear ﬁlter are not used explicitly
in the KLMS algorithm. Also, since the present output is determined solely by previous
inputs and all the previous errors, it can be readily computed in the input space. These
error samples are similar to innovation terms in sequential state estimation (Haykin,
), since they add new information to improve the output estimate. Each new input
sample results in an output, and hence a corresponding error, which is never modiﬁed
further and incorporated in the estimate of the next output. This recursive computation
makes KLMS especially useful for online (adaptive) nonlinear signal processing.
Liu et al. () showed that the KLMS algorithm is well posed in RKHS without
the need of an extra regularization term in the ﬁnite training data case, because the
solution is always forced to lie in the subspace spanned by the input data. The lack of
an explicit regularization term leads to two important advantages. First of all, it has a
simpler implementation than NORMA, as the update equations are straightforward
kernel versions of the original linear ones. Second, it can potentially provide better
results because regularization biases the optimal solution. In particular, it was shown
that a small enough step size can provide a suﬃcient “self-regularization” mechanism.

394
Digital Signal Processing with Kernel Methods
Moreover, since the space spanned by the mapped samples is possibly inﬁnite dimen-
sional, the projection error of the desired signal dn could be very small, as is well known
from Cover’s theorem (Haykin, ). On the downside, the speed of convergence and
the misadjustment also depend upon the step size. As a consequence, they conﬂict with
the generalization ability.
9.4.2
Implementation Challenges and Dual Formulation
Another important drawback of the KLMS algorithm becomes apparent when analyz-
ing its update, Equation .. In order to make a prediction, the algorithm requires
storing all previous errors ei and all processed input data xi, for i = , , … , n. In
online scenarios, where data are continuously being received, the size of the KLMS
network will continuously grow, posing implementation challenges. This becomes even
more evident if we recast the weight update from Equation .into a more standard
ﬁltering formulation, by relying on the representer theorem (Schölkopf et al., ).
This theorem states that the solution w,n can be expressed as a linear combination of
the transformed input data:
w,n =
n
∑
i=
𝛼i𝝓(xi).
(.)
This allows the prediction function to be written as the familiar kernel expansion
y∗=
n
∑
i=
𝛼iK(xi, x∗).
(.)
The expansion coeﬃcients 𝛼i are called the dual variables and the reformulation of
the ﬁltering problem in terms of 𝛼i is called the dual formulation. The update from
Equation .now becomes
n
∑
i=
𝛼i𝝓(xi) =
n−
∑
i=
𝛼i𝝓(xi) + 𝜂en𝝓(xn),
(.)
and after multiplying both sides with the new datum 𝝓(xn) and adopting a vector
notation, we obtain
𝜶nTkn = 𝜶n−Tkn−+ 𝜂enknn,
(.)
where 𝜶n = [𝛼, 𝛼, … , 𝛼n]T, the vector kn contains the kernels of the n data and the
newest point, kn = [K(x, xn), K(x, xn), … , K(xn, xn)], and knn = K(xn, xn). KLMS
resolves this relationship by updating 𝜶n as
𝜶n =
[
𝜶n−
𝜂en
]
.
(.)
The MATLAB code for the complete KLMS training step on a new data pair (x, d) is
displayed in Listing ..

Adaptive Kernel Learning for Signal Processing
395
k = kernel(dict,x,kerneltype,kernelpar); % kernels between dictionary ...
and x
y = k' * alpha; % evaluate function output
err = d - y; % instantaneous error
kaf.dict = [kaf.dict; x]; % add base to dictionary
kaf.alpha = [kaf.alpha; kaf.eta*err]; % add new coefficient
Listing 9.3 Training step of the KLMS algorithm on a new datum (x, d).
The update in Equation .emphasizes the growing nature of the KLMS network,
which precludes its direct implementation in practice. In order to design a practical
KLMS algorithm, the number of terms in the kernel expansion in Equation .should
stop growing over time. This can be achieved by implementing an online sparsiﬁca-
tion technique, whose aim is to identify terms in the kernel expansion that can be
omitted without degrading the solution. We will discuss several diﬀerent sparsiﬁcation
approaches in Section .. Finally, observe that the computational complexity and
memory complexity of the KLMS algorithm are both linear in terms of the number
of data it stores, (n). Recall that the complexity of the LMS algorithm is also linear,
though not in terms of the number of data but in terms of the data dimension.
9.4.3
Example on Prediction of the Mackey–Glass Time Series
We demonstrate the online learning capabilities of the KLMS kernel adaptive ﬁlter by
predicting the Mackey–Glass time series, which is a classic benchmark problem (Liu
et al., ). The Mackey–Glass time series is well known for its strong nonlinearity. It
corresponds to a high-dimensional chaotic system, and its output is generated by the
following time-delay diﬀerential equation:
dxn
dn = −bxn +
axn−Δ
+ x
n−Δ
.
(.)
We focus on the sequence with parameters b = ., a = ., and time delay Δ = ,
better known as the MGtime series. The prediction problem consists of predicting
the nth sample, given all samples of the time series up till the n −th sample.
Time-series prediction with kernel adaptive ﬁlters is typically performed by consid-
ering a time-delay vector xn = [xn, xn−, … , xn−L+]T as the input and the next sample
of the time series as the desired output, dn = xn+. This approach casts the prediction
problem into the well-known ﬁltering framework.Prediction of several steps ahead
can be obtained by choosing a prediction horizon h > , and dn = xn+h. For time series
generated by a deterministic process, a principled tool to ﬁnd the optimal embedding is
Takens’ theorem (Takens, ). In the case of the MGtime series, Takens’ theorem
indicates that the optimal embedding is around L = (Van Vaerenbergh et al., a).
We consider samples for online training of the KLMS algorithm and use the next
data for testing. The step size of KLMS is ﬁxed to ., and we use the Gaussian
kernel with 𝜎= . Figure .displays the prediction results after training steps.
The left plot shows the learning curve of the algorithm, obtained as the MSE of the
prediction on the test set, at each iteration of the online training process. As a reference,
Note that some particular time-series models have been exploited to deﬁne explicit recursivity in the
RKHS (Li and Príncipe ; Tuia et al. ), as we will see later on in this chapter.

396
Digital Signal Processing with Kernel Methods
LMS
KLMS
Original
KLMS prediction
0
200
400
−25
−20
−15
−10
−5
0
Training iteration
MSE (db)
0
20
40
60
80
100
0.2
0.4
0.6
0.8
1
1.2
1.4
Test sample
Figure 9.3 KLMS predictions on the Mackey–Glass time series. Left: learning curve over 500 training
iterations. Right: test samples of the Mackey–Glass time-series and the predictions provided by KLMS.
we include the learning curve of the linear LMS algorithm with a suitable step size. The
right plot shows the test samples of the original time series, as the full line, and
KLMS’ prediction on these test samples after training steps. These predictions are
calculated by evaluating the prediction equation (Equation .) on the test samples.
The code for this experiment and all subsequent ones is included in the accompanying
material in the book webpage.
9.4.4
Practical Kernel Least Mean Squares Algorithms
In the Mackey–Glass experiment described, the KLMS algorithm requires to store 
coeﬃcients 𝛼i and the corresponding data xi. The stored data xi are referred to as
its dictionary. If the online learning process were to continue indeﬁnitely, the algorithm
would require ever-growing memory and computation per time step. This issue was
identiﬁed as a major roadblock early on in the research on kernel adaptive ﬁlters, and it
has led to the development of several procedures to slow down the dictionary growth
by sparsifying the dictionary.
A sparsiﬁcation procedure based on Gaussian elimination steps on the Gram matrix
was proposed in Pokharel et al. (). This method is successful in limiting the
dictionary size in the nth training step to some m < n, but in order to do so it requires
(m) complexity, which defeats the purpose of using a KLMS algorithm.
Kernel Normalized Least Mean Squares and Coherence Criterion
Around the same time the KLMS algorithm was published, a kernelized version of
the aﬃne projection algorithm was proposed (Richard et al., ). Aﬃne projection
algorithms hold the middle ground between LMS and RLS algorithms by calculating
an estimate of the correlation matrix based on the p last data. For p = the algorithm
reduces to a kernel version of the normalized LMS algorithm (Haykin, ), called
kernel normalized least-mean squares (KNLMS), and its update reads
𝜶n =
[
𝜶n−

]
+
𝜂
𝜖+ ‖kn‖enkn.
(.)
Note that this algorithm updates all coeﬃcients in each iteration. This is in contrast to
KLMS, which updates just one coeﬃcient.

Adaptive Kernel Learning for Signal Processing
397
The kernel aﬃne projection and KNLMS algorithms introduced in Richard et al.
() also included an eﬃcient dictionary sparsiﬁcation procedure, called the coher-
ence criterion. Coherence is a measure to characterize a dictionary in sparse approxi-
mation problems, deﬁned in a kernel context as
𝜇= max
i≠j |K(xi, xj)|.
(.)
The coherence of a dictionary will be large if it contains two bases xi and xj that are very
similar, in terms of the kernel function. Owing to their similarity, such bases contribute
almost identical information to the algorithm, and one of them may be considered
redundant. The online dictionary sparsiﬁcation procedure based on coherence operates
by only including a new datum xn into the current dictionary n−if it maintains the
dictionary coherence below a certain threshold:
max
j∈n−
|K(xn, xj)| < 𝜇.
(.)
If the new datum fulﬁlls this criterion, it is included in the dictionary, and the KNLMS
coeﬃcients are updated through Equation .. If the coherence criterion in Equa-
tion .is not fulﬁlled, the new datum is not included in the dictionary, and a reduced
update of the KNLMS coeﬃcients is performed:
𝜶n = 𝜶n−+
𝜂
𝜖+ ‖kn‖enkn.
(.)
This update does not increase the number of coeﬃcients, and therefore it maintains
the algorithm’s computational complexity ﬁxed during that iteration. The MATLAB
code for the complete KNLMS training step on a new data pair (x, d) is displayed in
Listing ..
k = kernel(dict,x,kerneltype,kernelpar); % kernels between dictionary ...
and x
if (max(k) <= mu0), % coherence criterion
dict = [dict; x]; % add base to dictionary
alpha = [alpha; 0]; % reserve spot for new coefficient
end
k = kernel(dict,x,kerneltype,kernelpar); % kernels with new dictionary
y = k' * alpha; % evaluate function output
err = d - y; % instantaneous error
alpha = alpha + eta/(eps + k'*k)*err*k; % update coefficients
Listing 9.4 Training step of the KNLMS algorithm on a new datum (x, d).
The coherence criterion is computationally eﬃcient, in that it has a complexity that
does not exceed that of the kernel adaptive ﬁlter itself, and it has been demonstrated to
be successful in practical situations (Van Vaerenbergh and Santamaría, ).
Quantized Kernel Least Mean Squares
Recently, a KLMS algorithm was proposed that combines elements from the original
KLMS algorithm and the coherence criterion, called quantized KLMS (QKLMS) (Chen

398
Digital Signal Processing with Kernel Methods
et al., ). In particular, when the sparsiﬁcation criterion decides to include a datum
into the dictionary, the algorithm updates its coeﬃcients as follows:
𝜶n =
[
𝜶n−
𝜂en
]
.
(.)
When the datum does not fulﬁll the coherence criterion, it is not included in the
dictionary. Instead, the closest dictionary element is retrieved, and the corresponding
coeﬃcient is updated as follows
𝛼n,j = 𝛼n−,j + 𝜂en,
(.)
where j is the dictionary index of the element that is closest. Though conceptually very
simple, this algorithm obtains state-of-the-art results in several applications when only
a low computational budget is available. The MATLAB code for the complete QKLMS
training step on a new data pair (x, d) is displayed in Listing ..
k = kernel(dict,x,kerneltype,kernelpar); % kernels between dictionary ...
and x
y = k' * alpha; % evaluate function output
err = d - y; % instantaneous error
[d2,j] = min(sum((dict - repmat(x,m,1)).^2,2)); % distance to dictionary
if d2 <= epsu^2,
alpha(j) = alpha(j) + eta*err; % reduced coefficient update
else
dict = [dict; x]; % add base to dictionary
alpha = [alpha; eta*err]; % add new coefficient
end
Listing 9.5 Training step of the QKLMS algorithm on a new datum (x, d).
9.5
Kernel Recursive Least Squares
In linear adaptive ﬁltering, the RLS algorithm represents an alternative to LMS, with
faster convergence and typically lower bias, at the expense of a higher computational
complexity. RLS is obtained by designing a recursive solution to the LS problem.
Analogously, a recursive solution can be designed for the KRR problem, yielding KRLS
algorithms.
9.5.1
Kernel Ridge Regression
Let us review the KRR algorithm seen in Chapter . This will be the key to obtain the
kernel-based version of the regularized LS cost function in Equation .. We essentially
ﬁrst transform the data into the kernel feature space:
Jn =
n
∑
i=
|di −𝝓(xi)Tw|+ 𝜆wTw,
= ‖d −K𝜶‖+ 𝜆𝜶TK𝜶,
(.)

Adaptive Kernel Learning for Signal Processing
399
where we have applied the kernel trick to obtain the second equality. Here, vector d
contains the n desired values, d = [d, d, … , dn]T, and K is the kernel matrix with
elements Kij = K(xi, xj). Equation .represents the KRR problem (Saunders et al.,
), and its solution is given by
𝜶= (K + 𝜆I)−d.
(.)
The prediction for a new datum x∗is obtained as
y∗= k∗T𝜶= k∗T(K + 𝜆I)−d.
(.)
9.5.2
Derivation of Kernel Recursive Least Squares
The KRLS algorithm (Engel et al., ) formulates a recursive procedure to obtain the
solution of the regression problem in Equation .in the absence of regularization,
𝜆= . Without regularization, the solution (Equation .) reads
𝜶= K −d.
(.)
KRLS guarantees the invertibility of the kernel matrix K by excluding those data xi from
the dictionary that are linearly dependent on the already included data, in the feature
space. As we will see, this is achieved by applying a speciﬁc online sparsiﬁcation pro-
cedure, which guarantees both that K is invertible and that the algorithm’s dictionary
stays compact.
Assume the solution after processing n −data is available, given by
𝜶n−= K −
n−dn−,
(.)
In the next iteration n, a new data pair (xn, dn) is received and we wish to obtain
the new solution 𝜶n by applying a low-complexity update on the previous solution
(Equation .). We ﬁrst calculate the predicted output
yn = knT𝜶n−,
(.)
and we obtain the a-priori error for this datum, en = dn −yn. The updated kernel matrix
can be written as
K n =
[
K n−
kn
knT
knn
]
.
(.)
By introducing the variables
an = K −
n−kn
(.)
and
𝛾n = knn −knTan,
(.)

400
Digital Signal Processing with Kernel Methods
the update for the inverse kernel matrix can be written as
K −
n = 
𝛾n
[
𝛾nK −
n−+ ananT
−an
−an

]
.
(.)
Equation .is obtained by applying the Sherman–Morrison–Woodbury formula for
matrix inversion; see, for instance, Golub and Van Loan (). Finally, the updated
solution 𝜶n is obtained as
𝜶n =
[
𝜶n−

]
−en∕𝛾n
[
an
−
]
.
(.)
Equations .and .are eﬃcient updates that allow one to obtain the new solution
in (n) time and memory, based on the previous solution. Directly applying Equa-
tion .at iteration n would require (n) cost, so the recursive procedure is preferred
in online scenarios. A detailed derivation of this result can be found in Engel et al. ()
and Van Vaerenbergh et al. (b).
Online Sparsification by Approximate Linear Dependency
The KRLS algorithm from Engel et al. () follows the described recursive solution.
In order to slow down the dictionary growth, shown in Equations .and ., it
introduces a sparsiﬁcation criterion based on approximate linear dependency (ALD).
According to this criterion, a new datum xn should only be included in the dictionary if
𝝓(xn) cannot be approximated suﬃciently well in feature space by a linear combination
of the already present data.
Given a dictionary of data xj and a new training point xn, we need to ﬁnd a set of
coeﬃcients a = [a, a, … , am]T that satisfy the ALD condition
min
a
‖‖‖‖‖‖
m
∑
j=
aj𝝓(xj) −𝝓(xn)
‖‖‖‖‖‖

≤𝜈,
(.)
where m is the cardinality of the dictionary. Interestingly, it can be shown that these
coeﬃcients are already calculated by the KRLS update itself, and they are available at
each iteration n as an = K −
n−kn, see Equation .. The ALD condition can therefore
be veriﬁed by simply comparing 𝛾n with the ALD threshold:
𝛾n = knn −knTan ≤𝜈.
(.)
If 𝛾n > 𝜈, then we must add the newest datum xn to the dictionary, n = n−∪{xn},
before updating the solution through Equation .. If 𝛾n ≤𝜈then the datum xn is
already represented suﬃciently well by the dictionary. In this case the dictionary is not
expanded, n = n−, and a reduced update of the solution is performed; see Engel
et al. () for details. The MATLAB code for the complete KRLS training step on a
new data pair (x, d) is displayed in Listing ..

Adaptive Kernel Learning for Signal Processing
401
k = kernel(dict,x,kerneltype,kernelpar); % kernels between dictionary ...
and x
kxx = kernel(x,x,kaf.kerneltype,kaf.kernelpar); % kernel on x
a = Kinv * k; % coefficients of closest linear combination in feature ...
space
gamma = kxx - k' * a; % residual of linear approximation in feature space
y = k' * alpha; % evaluate function output
err = d - y; % instantaneous error
if gamma>nu % new datum is not approximately linear dependent
dict = [dict; x]; % add base to dictionary
Kinv = 1/gamma*[gamma*Kinv+a*a',-a;-a',1]; % update inv. kernel matrix
Z = zeros(size(P,1),1);
P = [P Z; Z' 1]; % add linear combination coeff. to projection matrix
ode = 1/gamma*err;
alpha = [alpha - a*ode; ode]; % full update of coefficients
else % perform reduced update of alpha
q = P * a / (1 + a'*P*a);
P = P - q * (a' * P); % update projection matrix
alpha = alpha + Kinv * q * err; % reduced update of coefficients
end
Listing 9.6 Training step of the KRLS algorithm on a new datum (x, d).
9.5.3
Prediction of the Mackey–Glass Time Series with Kernel Recursive
Least Squares
The update equations for KRLS require substantially more computation than KLMS. In
particular, KRLS has quadratic complexity, (m), in terms of its dictionary size, and
KLMS has linear complexity, (m). On the other hand, KRLS has faster convergence
and lower bias. We illustrate these properties by applying KRLS on the Mackey–Glass
prediction experiment from Section ...
Figure .shows the results of training the KRLS algorithm on the Mackey–Glass
time series. KRLS is applied with a Gaussian kernel with 𝜎= , and its precision
parameter was ﬁxed to 𝜈= −. The left plot compares the learning curves of KLMS
and KRLS, demonstrating a slightly faster initial convergence rate for KRLS, after which
the algorithm converges to a much lower MSE than KLMS. The low bias is also visible
in the right plot, which shows the prediction results on the test data.
0
20
40
60
80
100
0.2
0.4
0.6
0.8
1
1.2
1.4
Test sample
Original
KRLS prediction
0
200
400
−50
−40
−30
−20
−10
0
Training iteration
MSE (db)
KLMS
KRLS
Figure 9.4 KRLS predictions on the Mackey–Glass time series. Left: learning curve over 500 training
iterations, including comparison with KLMS. Right: test samples and KRLS predictions.

402
Digital Signal Processing with Kernel Methods
9.5.4
Beyond the Stationary Model
An important limitation of the KRLS algorithm is that it always assumes a stationary
model, and therefore it cannot track changes in the true underlying data model. This
is a somewhat odd property for an adaptive ﬁlter, though note that this is also the
case for the original RLS algorithm; see Section ... In order to enable tracking and
make a truly adaptive KRLS algorithm, several modiﬁcations have been presented in
the literature. An exponentially weighted KRLS algorithm was proposed by including
a forgetting factor, and an extended KRLS algorithm was designed by assuming a
simple state-space model, though both algorithms show numerical instabilities in
practice (Liu et al., ). In the following we brieﬂy discuss two diﬀerent approaches
that successfully allow KRLS to adapt to changing environments.
Sliding-Window Kernel Recursive Least Squares
The KRLS algorithm summarizes past information into a compact formulation that does
not allow easy manipulation. For instance, there does not exist a straightforward manner
to include a forgetting factor to exclude the inﬂuence of older data.
Van Vaerenbergh et al. () proposed sliding-window KRLS (SW-KRLS). This
algorithm stores a window of the last m data as its dictionary, and once a datum is older
than m time steps it is simply discarded. In each step the algorithm adds the new datum
and discards the oldest datum, leading to a sliding-window approach. The algorithm
stores the inverse regularized kernel matrix, (K n + 𝜆I)−, calculated on its current
dictionary, and a vector of the corresponding desired outputs, dn. By storing these
variables it can calculate the solution vector by simply evaluating 𝜶n = (K n + 𝜆I)−dn;
see Equations .and ..
The inverse kernel matrix is updated in two steps. First, the new datum is added, which
requires expanding the matrix with one row and one column. This is carried out by
performing the operation from Equation ., similar to in the KRLS algorithm. Second,
the oldest datum is discarded, which requires removing one row and column from the
inverse kernel matrix. This can be achieved by writing the kernel matrix and its inverse
as follows:
K n−=
[
a
bT
b
D
]
,
K −
n−=
[
e
f T
f
G
]
,
(.)
after which the inverse (regularized) kernel matrix is found as
D−= G −f f T∕e.
(.)
Details can be found in Van Vaerenbergh et al. (). Figure .illustrates the kernel
matrix updates when using a sliding window, compared with the classical growing-
window approach of KRLS. The MATLAB code for the SW-KRLS training step on a
new data pair (x, d) is displayed in Listing ..
dict = [dict; x]; % add base to dictionary
dict_d = [dict_d; d];
% add d to output dictionary
k = kernel(dict,x,kerneltype,kernelpar); % kernels between dictionary ...
and x
Kinv = grow_kernel_matrix(Kinv,k,c); % calculate new inverse kernel matrix
if (size(dict,1) > M) % prune

Adaptive Kernel Learning for Signal Processing
403
dict(1,:) = []; % remove oldest base from dictionary
dict_d(1) = []; % remove oldest d from output dictionary
Kinv = prune_kernel_matrix(Kinv); % prune inverse kernel matrix
end
alpha = Kinv * dict_d; % obtain new filter coefficients
Listing 9.7 Training step of the SW-KRLS algorithm on a new datum (x, d). The functions
“grow_kernel_matrix”and “prune_kernel_matrix”implement the operations in Equations 9.41
and 9.46.
SW-KRLS is a conceptually very simple algorithm that obtains reasonable perfor-
mance in a wide range of scenarios, most notably in nonstationary environments.
Nevertheless, its performance is limited by the quality of the bases in its dictionary,
over which it has little control. In particular, it has no means to avoid redundancy in
its dictionary or to maintain older bases that are relevant to its kernel expansion. In
order to improve this performance, a ﬁxed-budget KRLS (FB-KRLS) algorithm was
proposed in Van Vaerenbergh et al. (). Instead of discarding the oldest data point in
each iteration, FB-KRLS discards the data point that causes the least error upon being
discarded, using a least a-posteriori-error-based pruning criterion we will discuss in
Section .. In stationary scenarios, FB-KRLS obtains signiﬁcantly better results.
Kernel Recursive Least Squares Tracker
The tracking limitations of previous KRLS algorithms were overcome by development
of the KRLS tracker (KRLS-T) algorithm (Van Vaerenbergh et al., b), which has its
roots in the probabilistic theory of GP regression. Similar to the FB-KRLS algorithm,
this algorithm uses a ﬁxed memory size and has a criterion on which data to discard
in each iteration. But unlike FB-KRLS, the KRLS-T algorithm incorporates a forgetting
mechanism to gradually downweight older data.
We will provide more details on this algorithm in the discussion on probabilistic
kernel adaptive ﬁltering of Section .. As a reference, we list the MATLAB code for the
KRLS-T training step on a new data pair (x, d) in Listing .. While this algorithm has
similar complexity to other KRLS-type algorithms, its implementation is more complex
owing its fully probabilistic treatment of the regression problem. Note that some
additional checks to avoid numerical problems have been left out. The complete code
K1
K2
K3
K4
K5
K6
...
x1
x2
x3
x4
x5
x6
...
x1 x2 x3 x4 x5 x6 . . .
...
Kn −1
Kn
Kn+1
...
...
xn−1
xn
xn+ 1
...
. . .
xn −1
xn
xn + 1
. . .
Figure 9.5 Different forms of updating the kernel matrix during online learning. In KRLS-type
algorithms the update involves calculating the inverse of each kernel matrix, given the inverse of the
previous matrix. Left: growing kernel matrix, as constructed in KRLS (omitting sparsification for
simplicity). Right: sliding-window kernel matrix of a fixed size, as constructed in SW-KRLS.

404
Digital Signal Processing with Kernel Methods
can be found in the kernel adaptive ﬁltering MATLAB toolbox (KAFBOX), discussed
in Section ., and available on the book web page.
% perform one forgetting step
Sigma = lambda * Sigma + (1-lambda) * K; % forgetting on covariance matrix
mu = sqrt(lambda) * mu; % forgetting on mean vector
% kernels
k = kernel(dict,x,kerneltype,kernelpar); % kernels between dictionary ...
and x
kxx = kernel(x,x,kaf.kerneltype,kaf.kernelpar); % kernel on x
q = Q * k;
y_mean = q' * mu; % predictive mean of new datum
gamma2 = kxx - k' * q; % projection uncertainty
h = Sigma * q;
sf2 = gamma2 + q' * h; % noiseless prediction variance
sy2 = sn2 + sf2; % unscaled predictive variance of new datum
y_var = s02 * sy2; % predictive variance of new datum
% include new sample and add a basis
Qold = Q; % old inverse kernel matrix
p = [q; -1];
Q = [Q zeros(m,1);zeros(1,m) 0] + 1/gamma2*(p*p');
% updated inverse matrix
err = d - y_mean; % instantaneous error
p = [h; sf2];
mu = [mu; y_mean] + err / sy2 * p; % posterior mean
Sigma = [Sigma h; h' sf2] - 1 / sy2 * (p*p'); % posterior covariance
dict = [dict; x]; % add base to dictionary
% estimate scaling power s02 via ML
nums02ML = nums02ML + lambda * (y - y_mean)^2 / sy2;
dens02ML = dens02ML + lambda; s02 = nums02ML / dens02ML;
% delete a basis if necessary
m = size(dict,1);
if m > M
% MSE pruning criterion
errors = (Q*mu) ./ diag(Q);
criterion = abs(errors);
[~, r] = min(criterion); % remove element which incurs in the min. ...
err.
smaller = 1:m; smaller(r) = [];
if r == m, % remove the element we just added (perform reduced update)
Q = Qold;
else
Qs = Q(smaller, r);
qs = Q(r,r); Q = Q(smaller, smaller);
Q = Q - (Qs*Qs') / qs; % prune inverse kernel matrix
end
mu = mu(smaller); % prune posterior mean
Sigma = Sigma(smaller, smaller); % prune posterior covariance
dict = dict(smaller,:); % prune dictionary
end
Listing 9.8 Training step of the KRLS-T algorithm on a new datum (x, d).

Adaptive Kernel Learning for Signal Processing
405
9.5.5
Example on Nonlinear Channel Identification and Reconvergence
In order to demonstrate the tracking capabilities of some of the reviewed kernel adaptive
ﬁlters we perform an experiment similar to the setup described in Van Vaerenbergh
et al. () and Lázaro-Gredilla et al. (). Speciﬁcally, we consider the problem of
online identiﬁcation of a communication channel in which an abrupt change (switch)
is triggered at some point.
A signal xn ∈(, ) is fed into a nonlinear channel that consists of a linear FIR
channel followed by the nonlinearity y = tanh(z), where z is the output of the linear
channel. During the ﬁrst iterations the impulse response of the linear channel is
chosen as = [, −., −., ., .], and at iteration it is switched
to = [, −., ., −., −.]. Finally, dB of Gaussian white noise
is added to the channel output.
We perform an online identiﬁcation experiment with the algorithms LMS, QKLMS,
SW-KRLS, and KRLS-T. Each algorithm performs online learning of the nonlinear
channel, processing one input datum (with a time-embedding of ﬁve taps) and one
output sample per iteration. At each step, the MSE performance is measured on a set
of data points that are generated with the current channel model. The results are
averaged out over simulations.
The kernel adaptive ﬁlters use a Gaussian kernel with 𝜎= . LMS and QKLMS use a
learning rate 𝜂= .. The sparsiﬁcation threshold of QKLMS is set to 𝜖𝕌= ., which
leads to a ﬁnal dictionary of size around m = at the end of the experiment. The
regularization of SW-KRLS and KRLS-T is set to match the true value of the noise-
to-signal ratio, .. Regarding memory, SW-KRLS and KRLS-T are given a maximum
dictionary size of m = . Finally, KRLS-T uses a forgetting factor of 𝜆= ..
The results are shown in Figure .. LMS performs worst, as it is not capable of
modeling the nonlinearities in the system. QKLMS shows good results, given its low
complexity, but a slow convergence. SW-KRLS and KRLS-T converge to a value that is
mostly limited by its dictionary size (m = ), and both show fast convergence rates.
0
500
1000
1500
−30
−25
−20
−15
−10
Iteration
MSE (dB)
LMS
QKLMS
SW−KRLS
KRLS−T
Figure 9.6 MSE learning curves of different kernel adaptive filters on a communications channel that
shows an abrupt change at iteration 500.

406
Digital Signal Processing with Kernel Methods
All algorithms are capable of reconverging after the switch, though their convergence
rate is typically slower at that point.
9.6
Explicit Recursivity for Adaptive Kernel Models
Recursivity plays a key role for many problems in adaptive estimation, including control,
signal processing, system identiﬁcation, and general model building and monitoring
problems (Ljung, ). Much eﬀort has been devoted to the eﬃcient implementation
of these kinds of algorithms in DSP problems, including linear AR models (which need
few parameters and allow closed-form analysis) and ladder or lattice implementation
of linear ﬁlters. For nonlinear dynamics present in the system generating the data,
the model speciﬁcation and parameter estimation problems become far more complex
to handle, and methods such as NNs were proposed and intensely scrutinized in this
scenario in the s (Dorﬀner ; Narendra and Parthasarathy ).
Recursion and adaptivity concepts are strongly related to each other. As seen in this
chapter so far, kernel-based adaptive ﬁltering, in which model weights are updated
online, has been visited starting from the sequential SVM in De Freitas et al. (),
which used the Kalman recursions for updating the model weights. Other recent
proposals are kernel KFs (Ralaivola and d’Alche Buc, ), KRLS (Engel et al., ),
KLMS, and speciﬁc kernels for dynamic modeling (Liu et al., ). Mouattamid and
Schaback () focused on subspaces of native spaces induced by subsets, which
ultimately led to a recursive space structure and its associated recursive kernels for
interpolation.
Tuia et al. () presented and benchmarked an elegant formalization of recursion
in kernel spaces in online and adaptive settings. In a few words, the fundamentals of
this formalization rise from deﬁning the signal model recursion explicitly in the RKHS
rather than mapping the samples and then just embedding them into a kernel. The
methodology was deﬁned in three steps, starting from deﬁning the (recursive) model
structure directly in a suitable RKHS, then exploiting a proper representer’s theorem
for the model weights, and ﬁnally applying standard recursive equations from classical
DSP now in the RKHS. We next describe the recursive formalization therein, in order
to provide the necessary equations and background for supporting with detail some
application examples.
9.6.1
Recursivity in Hilbert Spaces
We start by considering a set of observed N time sample pairs {(xn, yn)}, and deﬁning
an arbitrary Markovian model representation of recursion between input–output time-
series pairs:
yn = f (⟨wi, xi
n⟩|𝜃f ) + ex
xi
n = g(⟨xi
n−k, yn−l⟩|𝜃g) + ey,
∀k > , l ≥,
(.)
where xi
n is the signal at the input of the ith ﬁlter tap at time n, yn−l is the previous output
at time n −l, ex is the state noise, and ey is the measurement noise. Here, f and g are
linear functions parametrized by model parameters wi and hyperparameters 𝜃f and 𝜃g.

Adaptive Kernel Learning for Signal Processing
407
Now we can deﬁne a feature mapping 𝝓(⋅) to an RKHS with the reproducing
property. One could replace samples by their mapped counterparts in Equation .
with
yn = f (⟨wi, 𝝓(g(⟨𝝓(xi
n−k), yn−l⟩|𝜃g))⟩|𝜃f )
(.)
and then try to express this in terms of dot products to replace them by kernel
functions. However, this operation may turn into an extremely complex one, and even
be unsolvable, depending on the parameterization of the recursive function g at hand.
While standard linear ﬁlters can be easily “kernelized,” some other ﬁlters introducing
nonlinearities in g are diﬃcult to handle with the standard kernel framework. Existing
smart solutions still require pre-imaging or dictionary approximations, such as the
kernel Kalman ﬁltering and the KRLS (Engel et al., ; Ralaivola and d’Alche Buc,
).
The alternative in the recursive formalization instead proposes to write down the
recursive equations directly in the RKHS model. In this case, the linear model is deﬁned
explicitly in as
yn = f (⟨wi, 𝝓i
n⟩|𝜃), +nx
𝝓i
n = g(⟨𝝓i
n−k, yn−l⟩|𝜃) + ny,
∀k > , l ≥.
(.)
Now, samples 𝝓i
n do not necessarily have 𝝓(xi
n) as a pre-image, and model parameters wi
are deﬁned in the possibly inﬁnite-dimensional feature space , while the hyperparam-
eters have the same meaning (as they deﬁne recursion according to an explicit model
structure regardless of the space where they are deﬁned).
This problem statement may be solved by ﬁrst deﬁning a convenient and tractable
reproducing property. For example, we deﬁne
wi =
N
∑
m=
𝛽i
m𝝓i
m,
(.)
which provides us with a fully recursive solution, even when deﬁned by assumption,
as seen later. In order to formally deﬁne a reproducing property, we still need to
demonstrate the associated representer theorem. We will alternatively show in the next
pages the existence and uniqueness of the mapping for a particular instantiation of
a digital ﬁlter in RKHS. In other words, the inﬁnite-dimensional feature vectors are
spanned by linear combinations of signals ﬁltered in the feature space. Note that this is
diﬀerent from the traditional ﬁlter-and-map approach.
For now, let us focus on the reproducing property deﬁned in Equation .. Note that
this is diﬀerent from the traditional ﬁlter-and-map approach. Plugging Equation .
into Equation .and applying the kernel trick yields
̂yn = f
(⟨N
∑
m=
𝛽i
m𝝓i
m, 𝝓i
n
⟩
||𝜃
)
= f
( N
∑
m=
𝛽i
mKi(m, n) ||𝜃
)
.
(.)
As a direct consequence of using this reproducing property, the solution is solely
expressed in terms of current and previous kernel values. When a number N of training

408
Digital Signal Processing with Kernel Methods
samples is available, one can explicitly compute the kernel and eventually adapt their
parameters. In the more interesting online case, the previous expression will also be
useful because, as we will see next, the kernel function Ki(m, n) for a particular ﬁlter
instantiation can be expressed as a function only of previous kernel evaluations.
A model length can be assumed for the input samples and then mapping can be
applied to vectors made up of delayed windows of the signals. Note that Ki(m, n) is not
necessarily equivalent to K(xi
m, xi
n), and hence the problem is far diﬀerent from those
deﬁned in previous kernel adaptive ﬁlters. To compute this kernel, one can actually
resort to applying recursion formulas in Equation .and explicitly deﬁne Ki(m, n)
as a function of previously computed kernels. This methodology is next illustrated for
the particular recursive model structure of the 𝛾-ﬁlter. In this way, not only do we attain
a recursive model in feature spaces, but we also avoid applying approximate methods
and pre-images.
9.6.2
Recursive Filters in Reproducing Kernel Hilbert Spaces
Here, we illustrate the proposed methodology for building recursive ﬁlters in the RKHS.
First we review the recursive AR and MA ﬁlters in kernel feature spaces, and underline
their application shortcomings. Then, noting that inclusion of short-term memory in
learning machines is essential for processing time-varying information, we analyze
the particular case of the 𝛾-ﬁlter (Principe et al., ), which provides a remarkable
compromise between stability and simplicity of adaptation.
Recursive Autoregressive and Moving-Average Filters in Reproducing Kernel Hilbert Spaces
The MA ﬁlter structure for time-series prediction is deﬁned as
yn =
P
∑
i=
bixi
n + en,
(.)
where xi
n = xn−i. Accordingly, the corresponding structure can be described explicitly
in RKHS:
yn =
P
∑
i=
bT
i 𝝓i
n + en,
(.)
and given that 𝝓i
n = 𝝋(xn−i) and that we can approximately expand each vector bi =
∑N
m=𝛽i
m𝝓i
m, we have
yn =
P
∑
i=
N
∑
m=
𝛽i
m⟨𝝋(xn−i), 𝝋(xm−i)⟩+ en =
P
∑
i=
N
∑
m=
𝛽i
mK(xn−i, xm−i) + en.
(.)
The AR ﬁlter structure is deﬁned according to the following expression:
yn =
P
∑
i=
aiyi
n + en,
(.)

Adaptive Kernel Learning for Signal Processing
409
where yi
n ∶= yn−i. Accordingly, the corresponding nonlinear structure can be described
by
yn =
P
∑
i=
aT
i 𝝓i
n + en.
(.)
Given that 𝝓i
n ∶= 𝝋(yn−i) and that we can approximately expand each vector ai =
∑N
m=𝛽i
m𝝓i
m, we have
yn =
P
∑
i=
N
∑
m=
𝛽i
m𝝋(yn−i)T𝝋(ym−i) + en =
P
∑
i=
N
∑
m=
𝛽i
mK(yn−i, ym−i) + en.
(.)
The deﬁnition of recursive kernel ARMA ﬁlters poses challenging problems, mainly
due the speciﬁcation of proper parameters to ensure stability. This problem is even
harder when working explicitly in RKHS, because we do not have direct access to the
mapped samples. We circumvent these problems both in the linear and kernel versions
by considering the 𝛾-ﬁlter structure next.
The Recursive γ-Filter in Reproducing Kernel Hilbert Spaces
The standard 𝛾-ﬁlter is deﬁned by
yn =
P
∑
i=
wixi
n
(.)
xi
n =
{
xn,
i = 
(−𝜇)xi
n−+ 𝜇xi−
n−,
≤i ≤P,
(.)
where yn is the ﬁlter output signal, xn is the ﬁlter input signal, xi
n is the signal present
at the input of the ith gamma tap, n is the time index, and 𝜃f = P, and 𝜃g = 𝜇are free
parameters controlling stability and memory depth.
A formulation of the 𝛾-ﬁlter is possible in the RKHS as follows. Essentially, the same
recursion can be expressed in a Hilbert space:
yn =
P
∑
i=
wT
i 𝝓i
n
(.)
𝝓i
n =
{
𝝋(xn),
i = 
(−𝜇)𝝓i
n−+ 𝜇𝝓i−
n−,
≤i ≤P,
(.)
where 𝝋(⋅) is a nonlinear transformation in an RKHS, and 𝝓i
n is a vector in this RKHS
that may not be an image of a vector of the input space; that is, 𝝓i
n ≠𝝋(xi
n). As
mentioned, this model assumes linear relations between samples in the RKHS. If this
assumption is insuﬃcient to solve the identiﬁcation problem at hand, the scalar sample
xn can be easily changed by vector zn = [xn, xn−⋯xn−P]T, where P is the selected time-
embedding.

410
Digital Signal Processing with Kernel Methods
Nevertheless, the weight vectors wi of Equation .are linearly spanned by the N
training vectors, wi = ∑N
m=𝛽i
m𝝓i
m. By including this expansion in Equation .and
applying the kernel trick, Ki(m, n) = ⟨𝝓i
m, 𝝓i
n⟩, we obtain
yn = ∑P
i=
∑N
m=𝛽i
m(𝝓i
m)T𝝓i
n = ∑P
i=
∑N
m=𝛽i
mKi(m, n).
(.)
The dot products can be computed by using the kernel trick and model recursion
deﬁned in Equation .as
Ki(m, n) =
{
K(xm, xn),
i = 
⟨(−𝜇)𝝓i
m−+ 𝜇𝝓i−
m−, (−𝜇)𝝓i
n−+ 𝜇𝝓i−
n−⟩
≤i ≤P.
(.)
Arranging terms in the second case of Equation ., we get
Ki(m, n) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪⎩
K(xm, xn),
i = 
(−𝜇)Ki(m −, n −)
+ 𝜇Ki−(m −, n −)
+ 𝜇(−𝜇)⟨𝝓i
m−, 𝝓i−
n−⟩
+ 𝜇(−𝜇)⟨𝝓i−
m−, 𝝓i
n−⟩
≤i ≤P
(.)
The second part still has two (interestingly nonsymmetric) dot products that are not
straightforwardly computable. Nevertheless, applying recursion again in Equation .,
this can be rewritten as
⟨𝝓i
m−, 𝝓i−
n−⟩=
{
,
i = 
⟨(−𝜇)𝝓i
m−+ 𝜇𝝓i−
m−, 𝝓i−
n−⟩
≤i ≤P,
which, in turn, can be rearranged as
⟨𝝓i
m−, 𝝓i−
n−⟩=
{
,
i = 
(−𝜇)⟨𝝓i
m−, 𝝓i−
n−⟩+ 𝜇Ki−(m −, n −)
≤i ≤P.
Term Ki−(m −, n −) and the dot product in the second case can be recursively
computed using Equations .and .. Assuming that 𝝓i
n = for n < , we obtain
the recursion
⟨𝝓i
m−, 𝝓i−
n−⟩=
{
,
i = 
𝜇∑m
j=(−𝜇)j−Ki−(m −j, n −)
i ≤P

Adaptive Kernel Learning for Signal Processing
411
and ﬁnally, the recursive kernel can be rewritten as
Ki(m, n) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪⎩
K(xm, xn),
i = 
(−𝜇)Ki(m −, n −)
+ 𝜇Ki−(m −, n −)
+ 𝜇
m−
∑
j=
(−𝜇)j−[Ki−(m −j, n −)
+ Ki−(m −, n −j)],
≤i ≤P.
(.)
9.7
Online Sparsification with Kernels
The idea behind sparsiﬁcation methods is to construct a sparse dictionary of bases that
represent the remaining data suﬃciently well. As a general rule in learning theory, it
is desirable to design a network with as few processing elements as possible. Sparsity
reduces the complexity in terms of computation and memory, and it usually gives
better generalization ability to unseen data (Platt, ; Vapnik, ). In the context
of kernel methods, sparsiﬁcation aims to identify the bases in the kernel expansion
y∗= ∑n
i=𝛼iK(xi, x∗) (see Equation .) that can be discarded without incurring a
signiﬁcant performance loss.
Online sparsiﬁcation is typically performed by starting with an empty dictionary,

= ∅, and, in each iteration, adding the input datum xi if it fulﬁlls a chosen
sparsiﬁcation criterion. We denote the dictionary at time instant n−as n−= {ci}mn−
i=,
where ci is the ith stored center, taken from the input data x received up till this
instant, and mn−is the dictionary cardinality at this instant. When a new input–output
pair (xn, dn) is received, a decision is made whether or not xn should be added to
the dictionary as a center. If the sparsiﬁcation criterion is fulﬁlled, xn is added to the
dictionary, n = n−∪{xn}. If the criterion is not fulﬁlled, the dictionary is maintained,
n = n−, to preserve its sparsity.
Figure .illustrates the dictionary construction process for diﬀerent sparsiﬁcation
approaches. Each horizontal line represents the presence of a center in the dictionary.
At any given iteration, the elements in the dictionary are indicated by the horizontal
lines that are present at that iteration. In the following we discuss each approach in
detail.
9.7.1
Sparsity by Construction
We will ﬁrst give a general overview of the diﬀerent online sparsiﬁcation methods in the
literature, some of which we have already introduced in the context of the algorithms
for which they were proposed. We distinguish three criteria that achieve sparsity by
construction: novelty criterion, ALD criterion, and coherence criterion. If the dictionary
is not allowed to grow beyond a speciﬁed maximum size, it may be necessary to discard
bases at some point. This process is referred to as pruning, and we will review the most
important pruning criteria later.

412
Digital Signal Processing with Kernel Methods
Evergrowing
Iteration
Dictionary element
0
10
20
30
40
50
Limited growth
Sliding window
Iteration
0
10
20
30
40
50
0
10
20
30
40
50
Iteration
Iteration
0
10
20
30
40
50
0
10
20
30
40
50
Dictionary element
0
10
20
30
40
50
Dictionary element
0
10
20
30
40
50
Dictionary element
0
10
20
30
40
50
Fixed budget
Figure 9.7 Dictionary construction processes for different sparsification approaches. Each horizontal
line marks the presence of a center in the dictionary. Top left: the ever-growing dictionary
construction, in which the dictionary contains n elements in iteration n. Top right: online sparsification
by slowing down the dictionary growth, as obtained by the coherence and ALD criteria. Bottom left:
sliding-window approach, displayed with 10 elements in the dictionary. Bottom right: fixed-budget
approach, in which the pruning criterion discards one element per iteration, displayed with dictionary
size 10.
Novelty criterion. The novelty criterion is a data selection method introduced by
Platt (). It was used to construct resource-allocating networks, which are
essentially growing RBF networks. When a new data point xn is obtained by
the network, the novelty criterion calculates the distance of this point to the
current dictionary, minj∈n−‖xn −cj‖. If this distance is smaller than some preset
threshold, xn is added to the dictionary. Otherwise, it computes the prediction
error, and only if this error en is larger than another preset threshold will the datum
xn be accepted as a new center.
ALD criterion. A more sophisticated dictionary growth criterion was introduced for
the KRLS algorithm in Engel et al. (): each time a new datum xn is observed,
the ALD criterion measures how well the datum can be approximated in the
feature space as a linear combination of the dictionary bases in that space. It does
so by checking if the ALD condition holds; see Equation .:
min
a
‖‖‖‖‖‖
m
∑
j=
aj𝝓(cj) −𝝓(xn)
‖‖‖‖‖‖

≤𝜈.
Evaluating the ALD criterion requires quadratic complexity, (m), and therefore
it is not suitable for algorithms with linear complexity such as KLMS.
Coherence criterion. The coherence criterion is a straightforward criterion to check
whether the newly arriving datum is suﬃciently informative. It was introduced in

Adaptive Kernel Learning for Signal Processing
413
the context of the KNLMS algorithm (Richard et al., ). Given the dictionary
Dn−at iteration n −and the newly arriving datum xn, the coherence criterion
to include the datum reads
max
j∈n−
|K(xn, cj)| < 𝜇.
(.)
In essence, the coherence criterion checks the similarity, as measured by the
kernel function, between the new datum and the most similar dictionary center.
Only if this similarity is below a certain threshold 𝜇is the datum inserted into
the dictionary. The higher the threshold 𝜇is chosen, the more data will be
accepted in the dictionary. It is an eﬀective criterion that has linear computational
complexity in each iteration: it only requires to calculate m kernel functions,
making it suitable for KLMS-type algorithms. Chen et al. () introduced a
similar criterion, minj∈n−‖xn −cj‖ > 𝜖u, which is essentially equivalent to the
coherence criterion with a Euclidean distance-based kernel.
9.7.2
Sparsity by Pruning
In practice, it is often necessary to specify a maximum dictionary size m, or budget, that
may not be exceeded; for instance, due to limitations on hardware or execution time. In
order to avoid exceeding this budget, one could simply stop including any data in the
dictionary once the budget is reached, hence locking the dictionary. Nevertheless, it is
very probable that at some point after locking the dictionary a new datum is received
that is very informative. In this case, the quality of the algorithm’s solution may improve
by pruning the least relevant center of the dictionary and replacing it with the new, more
informative datum.
The goal of a pruning criterion is to select a datum out of a given set, such that
the algorithm’s performance is least aﬀected. This makes pruning criteria conceptually
diﬀerent from the previously discussed online sparsiﬁcation criteria, whose goal is
to decide whether or not to include a datum. Pruning techniques have been studied
in the context of NN design (Hassibi et al. ; LeCun et al. ) and kernel
methods (De Kruif and De Vries, ; Hoegaerts et al., ). We brieﬂy discuss the
two most important pruning criteria that appear in kernel adaptive ﬁltering: sliding-
window criterion and error criterion.
Sliding window. In time-varying environments, it may be useful to discard the oldest
bases, as these were observed when the underlying model was most diﬀerent from
the current model. This strategy is at the core of sliding-window algorithms such
as NORMA (Kivinen et al., ) and SW-KRLS (Van Vaerenbergh et al., ).
In every iteration, these algorithms include the new datum in the dictionary and
discard the oldest datum, thereby maintaining a dictionary of ﬁxed size.
Error criterion. Instead of simply discarding the oldest datum, the error-based criterion
determines the datum that will cause the least increase of the squared-error
performance after it is pruned. This is a more sophisticated pruning strategy that
was introduced by Csató and Opper () and De Kruif and De Vries ()
and requires quadratic complexity to evaluate, (m). Interestingly, if the inverse

414
Digital Signal Processing with Kernel Methods
kernel matrix is available, it is straightforward to evaluate this criterion. Given
the ith element on the diagonal of the inverse kernel matrix, [K −]ii, and the ith
expansion coeﬃcient 𝛼i, the squared error after pruning the ith center from a
dictionary is 𝛼i∕[K −]ii. The error-based pruning criterion therefore selects the
index for which this quantity is minimized,
arg min
i
𝛼i
[K −]ii
.
(.)
This criterion is used in the ﬁxed-budget algorithms FB-KRLS (Van Vaerenbergh
et al., ) and KRLS-T (Van Vaerenbergh et al., b). An analysis performed
by Lázaro-Gredilla et al. () shows that the results obtained by this criterion
are very close to the optimal approach, which is based on minimization of the KL
divergence between the original and the approximate posterior distributions.
Currently, the most successful pruning criteria used in the kernel adaptive ﬁltering
literature have quadratic complexity, (m) and therefore they can only be used in
KRLS-type algorithms. Optimal pruning in KLMS is a particularly challenging problem,
as it is hard to deﬁne a pruning criterion that can be evaluated with linear computational
complexity. A simple criterion is found in (Rzepka, ), where the center with the least
weight is pruned, and weight is determined by the associated expansion coeﬃcient,
arg min
i
|𝛼i|.
(.)
The design of more sophisticated pruning strategies is currently an open topic in KLMS
literature. Some recently proposed criteria can be found in Zhao et al. (, ).
9.8
Probabilistic Approaches to Kernel Adaptive Filtering
In many signal processing applications, the problem of signal estimation is addressed.
Probabilistic models have proven to be very useful in this context (Arulampalam et al.
; Rabiner ). One of the advantages of probabilistic approaches is that they force
the designer to specify all the prior assumptions of the model, and that they make a clear
separation between the model and the applied algorithm. Another beneﬁt is that they
typically provide a measure of uncertainty about the estimation. Such an uncertainty
estimate is not provided by classical kernel adaptive ﬁltering algorithms, which produce
a point estimate without any further guarantees.
In this section, we will review how the probabilistic framework of GPs allows us
to extend kernel adaptive ﬁlters to probabilistic methods. The resulting GP-based
algorithms not only produce an estimate of an unknown function, but also an entire
probability distribution over functions; see Figure .. Before we describe any proba-
bilistic kernel adaptive ﬁltering algorithms, it is instructive to take a step back to the
nonadaptive setting, and consider the KRR problem in Equation .. We will adopt
the GP framework to analyze this problem from a probabilistic point of view.

Adaptive Kernel Learning for Signal Processing
415
5
0
–5
–3
–2
–1
0
1
2
3
5
0
–5
–3
–2
–1
0
1
2
3
Input, x
Output, f(x)
Input, x
Output, f(x)
Figure 9.8 Functions drawn from a GP with a squared exponential covariance
K(x, x′) = exp(−‖x −x′‖2∕2𝜎2). The 95% confidence interval is plotted as the shaded area. Up: draws
from the prior function distribution. Down: draws from the posterior function distribution, which is
obtained after five data points (blue crosses) are observed. The predictive mean is displayed in black.
9.8.1
Gaussian Processes and Kernel Ridge Regression
Let us assume that the observed data in a regression problem can be described by the
model dn = f (xn) + 𝜖n, in which f represents an unobservable latent function and
𝜖n ∼(, 𝜎) is zero-mean Gaussian noise. Note that, unlike in previous chapters, here
we use 𝜎for the noise variance and not for the Gaussian kernel length-scale parameter
to avoid confusion with the time instant subscript n. We will furthermore assume a
zero-mean GP prior on f (x)
f (x) ∼(m(x), K(x, x′))
(.)
and a Gaussian prior on the noise 𝜖, 𝜖∼(, 𝜎
n). In the GP literature, the kernel func-
tion K(x, x′) is referred to as the covariance, since it speciﬁes the a priori relationship
between values f (x) and f (x′) in terms of their respective locations, and its parameters
are called hyperparameters.
By deﬁnition, the marginal distribution of a GP at a ﬁnite set of points is a joint
Gaussian distribution, with its mean and covariance being speciﬁed by the functions

416
Digital Signal Processing with Kernel Methods
m(x) and K(x, x′) evaluated at those points (Rasmussen and Williams, ). Thus,
the joint distribution of outputs d = [d, … , dn]T and the corresponding latent vector
f = [ f (x), f (x), … , f (xn)]T is
[
d
f
]
∼
(
𝟎,
[
K + 𝜎I
K
K
K
])
.
(.)
By conditioning on the observed outputs y, the posterior distribution over the latent
vector can be inferred
p( f |d) = ( f |K(K + 𝜎I)−d, K −K(K + 𝜎I)−K) = ( f |𝝁, 𝜮).
(.)
Assuming this posterior is obtained for the data up till time instant n −, the predictive
distribution of a new output dn at location xn is computed as
p(dn|xn, dn−) = (dn|𝜇GP,n, 𝜎
GP,n)
(.a)
𝜇GP,n = knT(K n−+ 𝜎I)−dn−
(.b)
𝜎
GP,n = 𝜎+ knn −knT(K n−+ 𝜎I)−kn.
(.c)
The mode of the predictive distribution, given by 𝜇GP,n in Equation .b, coincides
with the prediction of KRR, given by Equation ., showing that the regularization in
KRR can be interpreted as a noise power 𝜎. Furthermore, the variance of the predictive
distribution, given by 𝜎
GP,n in Equation .c, coincides with Equation ., which is
used by the ALD dictionary criterion for KRLS.
9.8.2
Online Recursive Solution for Gaussian Processes Regression
A recursive update of the complete GP in Equation .was proposed by Csató and
Opper (), as the sparse online GP (SOGP) algorithm. We will follow the notation
of Van Vaerenbergh et al. (b), whose solution is equivalent to SOGP but whose
choice of variables allows for an easier interpretation. Speciﬁcally, the predictive mean
and covariance of the GP solution in Equation .can be updated as
p( f n|Xn, dn) = ( f n|𝝁n, 𝜮n)
(.a)
𝝁n =
[𝝁n−
̂dn
]
+ en
̂𝜎
dn
[hn
̂𝜎
fn
]
(.b)
𝜮n =
[𝜮n−
hn
hnT
̂𝜎
fn
]
−

̂𝜎
dn
[hn
̂𝜎
fn
] [hn
̂𝜎
fn
]
T,
(.c)
where Xn contains the n input data, hn = 𝜮n−K −
n−kn, and ̂𝜎
fn and ̂𝜎
dn are the predictive
variances of the latent function and the new output respectively, calculated at the new
input. Details can be found in Lázaro-Gredilla et al. () and Van Vaerenbergh et al.
(b). In particular, the update of the predictive mean can be shown to be equivalent
to the KRLS update. The advantage of using a full GP model is that not only does it
allow us to update the predictive mean, as does KRLS, but it keeps track of the entire

Adaptive Kernel Learning for Signal Processing
417
predictive distribution of the solution. This allows us, for instance, to establish CIs when
predicting new outputs.
Similar to KRLS, this online GP update assumes a stationary model. Interestingly,
however, the Bayesian approach (and in particular its handling of the uncertainty) does
allow for a principled extension that performs tracking, as we brieﬂy discuss in what
follows.
9.8.3
Kernel Recursive Least Squares Tracker
Van Vaerenbergh et al. (b) presented a KRLS-T algorithm that explicitly handles
uncertainty about the data, based on the probabilistic GP framework. In stationary
environments it operates identically to the earlier proposed SOGP from Csató and
Opper (), though it includes a forgetting mechanism that enables it to handle
nonstationary scenarios as well. During each iteration, KRLS-T performs a forgetting
operation in which the mean and covariance are replaced through
𝝁←
√
𝜆𝝁
(.a)
𝜮←𝜆𝜮+ (−𝜆)K.
(.b)
The eﬀect of this operation on the predictive distribution is shown in Figure .. For
illustration purposes, the forgetting factor is chosen unusually low, 𝜆= ..
This particular form of forgetting corresponds to blending the informative posterior
with a “noise” distribution that uses the same color as the prior. In other words,
forgetting occurs by taking a step back toward the prior knowledge. Since the prior has
zero mean, the mean is simply scaled by the square root of the forgetting factor 𝜆. The
covariance, which represents the posterior uncertainty on the data, is pulled toward the
covariance of the prior. Interestingly, a regularized version of RLS (known as extended
RLS) can be obtained by using a linear kernel with the BP forgetting procedure.
−5
−4
−3
−2
−1
0
1
2
3
4
5
−3
−2
−1
0
1
2
3
Input, x
Output, f(x)
Mean after forgetting
95% confidence after forgetting
Figure 9.9 Forgetting operation of KRLS-T. The original predictive mean and variance are indicated as
the black line and shaded gray area, as in Figure 9.8. After one forgetting step, the mean becomes the
dashed red curve, and the new 95% CI is indicated in blue.

418
Digital Signal Processing with Kernel Methods
Standard RLS can be obtained by using a diﬀerent forgetting rule; see Van Vaerenbergh
et al. (b).
The KRLS-T algorithm can be seen as a probabilistic extension of KRLS that obtains
CIs and is capable of adapting to time-varying environments. It obtains state-of-the-art
performance in several nonlinear adaptive ﬁltering problems – see Van Vaerenbergh
and Santamaría () and the results of Figure .– though it has a more complex
formulation than most other kernel adaptive ﬁlters and it requires a higher com-
putational complexity. We will explore these aspects through additional examples in
Section ..
9.8.4
Probabilistic Kernel Least Mean Squares
The success of the probabilistic approach for KRLS-like algorithms has led several
researchers to investigate the design of probabilistic KLMS algorithms. The low com-
plexity of KLMS-type algorithms makes them very popular in practical solutions.
Nevertheless, this low computational complexity is also a limitation that makes the
design of a probabilistic KLMS algorithm a particularly hard research problem.
Some advances have already been made in this direction. Speciﬁcally, Park et al. ()
proposed a probabilistic KLMS algorithm, though it only considered the MAP estimate.
Van Vaerenbergh et al. (a) showed that several KLMS algorithms can be obtained
by imposing a simplifying restriction on the full SOGP model, thereby linking KLMS
algorithms and online GP approaches directly.
9.9
Further Reading
A myriad of diﬀerent kernel adaptive ﬁltering algorithms have appeared in the literature.
We described the most prominent algorithms, which represent the state of the art.
While we only focused on their online learning operation, several other aspects are
worth studying. In this section, we brieﬂy introduce the most interesting topics that
are the subject of current research.
9.9.1
Selection of Kernel Parameters
As we saw in Chapter , hyperparameters of GP models are typically inferred by type-II
ML. This corresponds to optimal choices for KRR, due to the correspondence between
GP regression and KRR, and for several kernel adaptive ﬁlters as well. A case study for
KRLS and KRLS-T can be found in Van Vaerenbergh et al. (a). The following should
be noted, however. In online scenarios, it would be interesting to perform an online
estimation of the optimal hyperparameters. This, however, is a diﬃcult open research
problem for which only a handful of methods have been proposed; see for instance
Soh and Demiris (). In practice, it is still more appropriate to perform type-II ML
oﬄine on a batch of training data, before running the online learning procedure using
the hyperparameters found.

Adaptive Kernel Learning for Signal Processing
419
9.9.2
Multi-Kernel Adaptive Filtering
In the last decade, several methods have been proposed to consider multiple kernels
instead of a single one (Bach et al., ; Sonnenburg et al., a). The diﬀerent kernels
may correspond to diﬀerent notions of similarity, or they may address information
coming from multiple, heterogeneous data sources. In the ﬁeld of linear adaptive
ﬁltering, it was shown that a convex combination of adaptive ﬁlters can improve the
convergence rate and tracking performance (Arenas-García et al., ) compared with
running a single adaptive ﬁlter. Multi-kernel adaptive ﬁltering combines ideas from the
aforementioned two approaches (Yukawa, ). Its learning procedure activates those
kernels whose hyperparameters correspond best to the currently observed data, which
could be interpreted as a form of hyperparameter learning. Furthermore, the adaptive
nature of these algorithms allows them to track the importance of each kernel in
time-varying scenarios, possibly giving them an advantage over single-kernel adaptive
ﬁltering. Several multi-kernel adaptive ﬁltering algorithms have been proposed in the
recent literature (Gao et al. e.g., ; Ishida and Tanaka e.g., ; Pokharel et al. e.g.,
; Yukawa e.g., ). While they show promising performance gains over single-
kernel adaptive ﬁltering algorithms, their computational complexity is much higher.
This is an important aspect inherent to the combination of multiple kernel methods,
and it is a topic of current research.
9.9.3
Recursive Filtering in Kernel Hilbert Spaces
Modeling and prediction of time series with kernel adaptive ﬁlters is usually addressed
by time-embedding the data, thus considering each time lag as a diﬀerent input
dimension. As we have already seen in previous chapters, this approach presents some
important drawbacks, which become more evident in adaptive, online settings: ﬁrst,
the optimal ﬁlter order may change over time, which would require an additional
tracking mechanism; second, if the optimal ﬁlter order is high, as for instance in audio
applications (Van Vaerenbergh et al., b), the method be aﬀected by the curse of
dimensionality. For some problems, the concept of an optimal ﬁlter order may not even
make sense.
An alternative approach to modeling and predicting time series is to construct
recursive kernel machines, which implement recursive models explicitly in the RKHS.
Preliminary work in this direction considered the design of a recursive kernel in the con-
text of inﬁnite recurrent NNs (Hermans and Schrauwen, ). More recently, explicit
recursive versions of the AR, MA, and gamma ﬁlters in RKHS were proposed (Tuia
et al., ), as we have revised before. By exploiting properties of functional analysis
and recursive computation, this approach avoids the reduced-rank approximations that
are required in standard kernel adaptive ﬁlters. Finally, a kernel version of the ARMA
ﬁlter was presented by Li and Príncipe ().
9.10
Tutorials and Application Examples
This section presents experiments in which kernel adaptive ﬁlters are applied to
time-series prediction and nonlinear system identiﬁcation. These experiments are

420
Digital Signal Processing with Kernel Methods
implemented using code based on the KAFBOX toolbox available on the book’s
web page (Van Vaerenbergh and Santamaría, ). Also, some examples of explicit
recursivity in RKHS are provided.
9.10.1
Kernel Adaptive Filtering Toolbox
KAFBOX is a MATLAB benchmarking toolbox to evaluate and compare kernel adaptive
ﬁltering algorithms. It includes a large list of algorithms that have appeared in the
literature, and additional tools for hyperparameter estimation and algorithm proﬁling,
among others.
The kernel adaptive ﬁltering algorithms in KAFBOX are implemented as objects using
the classdef syntax. Since all KAF algorithms are online methods, each of them
includes two basic operations: () obtaining the ﬁlter output, given a new input x∗;
and () training on a new data pair (xn, dn). These operations are implemented as the
methods evaluate and train respectively.
As an example, we list the code for the KLMS algorithm in Listing .. The object
deﬁnition contains two sets of properties: one for the hyperparameters and one for the
variables it will learn. The ﬁrst method is the object’s constructor method, which copies
the speciﬁed hyperparameter settings. The second method is the evaluate function,
which performs the operation y∗= ∑n
i=𝛼iK(xi, x∗). It includes an if clause to check if
the algorithm has at least performed one training step yet. If not, zeros are returned as
predictions. Finally, the train method implements a single training step of the online
learning algorithm. This method typically handles algorithm initialization as well, such
that functions that operate on a KAF object do not have to worry about initializing.
The training step itself is summarized in very few lines of MATLAB code, for many
algorithms. For the following experiments, we will use v.of KAFBOX.
% Kernel Least-Mean-Square algorithm
%
W. Liu, P.P. Pokharel, and J.C. Principe, "The Kernel ...
Least-Mean-Square % Algorithm," IEEE Transactions on Signal ...
Processing, vol. 56, no. 2, pp. % 543-554, Feb. 2008, ...
http://dx.doi.org/10.1109/TSP.2007.907881
%
% Remark: implementation includes a maximum dictionary size M
% This file is part of the Kernel Adaptive Filtering Toolbox for Matlab.
% https://github.com/steven2358/kafbox/
classdef klms < handle
properties (GetAccess = 'public', SetAccess = 'private')
eta = .5; % learning rate
M = 10000; % maximum dictionary size
kerneltype = 'gauss'; % kernel type
kernelpar = 1; % kernel parameter
end
properties (GetAccess = 'public', SetAccess = 'private')
dict = []; % dictionary
alpha = []; % expansion coefficients
end
methods
function kaf = klms(parameters) % constructor
if (nargin > 0) % copy valid parameters

Adaptive Kernel Learning for Signal Processing
421
for fn = fieldnames(parameters)',
if ismember(fn,fieldnames(kaf)),
kaf.(fn{1}) = parameters.(fn{1});
end
end
end
end
function y_est = evaluate(kaf,x) % evaluate the algorithm
if size(kaf.dict,1)>0
k = kernel(kaf.dict,x,kaf.kerneltype,kaf.kernelpar);
y_est = k'*kaf.alpha;
else
y_est = zeros(size(x,1),1);
end
end
function train(kaf,x,y) % train the algorithm
if (size(kaf.dict,1)<kaf.M), % avoid infinite growth
y_est = kaf.evaluate(x);
err = y - y_est;
kaf.alpha = [kaf.alpha; kaf.eta*err]; % grow
kaf.dict = [kaf.dict; x]; % grow
end
end
end
end
Listing 9.9 MATLAB code for the KLMS algorithm object class, from KAFBOX.
9.10.2
Prediction of a Respiratory Motion Time Series
In the ﬁrst experiment we apply KAF algorithms to predict a biomedical time series,
more speciﬁcally a respiratory motion trace. These data come from robotic radio-
surgery, in which a photon beam source is used to ablate tumors. The beam is operated
by a robot arm that aims to move the beam source to compensate for the motion
of internal organs. Traditionally, this is achieved by recording the motion of markers
applied to the body surface and by using this motion to draw conclusions about the
tumor position. Although this method signiﬁcantly increases the targeting accuracy,
the system delay arising from data processing and positioning of the beam results in
a systematic error. This error can be decreased by predicting the motion of the body
surface (Ernst, ).
The data were recorded at Georgetown University Hospital using CyberKnife® equip-
ment, and it represents the recorded position of one of the markers attached to the
body surface.A snapshot of this motion trace is shown in Figure .. The delay to
compensate totals ms, which, at a sampling frequency of Hz, corresponds to
three samples. The task thus consists of three-step ahead prediction. We use a time-
embedding of eight samples. Since the breathing pattern may change over time, we
employed only tracking algorithms. Their parameters are listed in Table .. The MSE
results of the four algorithms are displayed in the last column of last column of this
Data available at http://signals.rob.uni-luebeck.de/.

422
Digital Signal Processing with Kernel Methods
455
460
465
470
475
480
485
490
495
−4
−2
0
2
4
6
8
Time (s)
Figure 9.10 A snapshot of the respiratory motion trace.
Table 9.1 Parameters used for predicting the respiratory motion trace, MSE result for three-
step ahead prediction, and measured training time.
Algorithm
Parameters
MSE performance (dB)
Training time (s)
NORMA
𝜆= −, 𝜏= 
−.
.
QKLMS
𝜂= ., 𝜖𝕌= 
−.
.
SWKRLS
c = −, m = 
−.
.
KRLS-T
𝜎
n = −, m = , 𝜆= .
−.
.
0
5
10
15
20
25
30
35
−2
0
2
4
6
8
10
Time (s)
Original
SWKRLS prediction
Figure 9.11 The respiratory motion trace and the three-step ahead prediction of a KAF algorithm.
table. A comparison of the original series and the predictions of one of the algorithms is
shown in Figure .. The code to reproduce these results can be found in Listing ..
% 3-step ahead prediction on the respiratory motion time series.
clear, clc
currentdir = pwd; cd ../kafbox/; install; cd(currentdir)
%% Parameters
h = 3; % prediction horizon
L = 8; % embedding
n = 1000; % number of data
sigma = 7; % kernel parameter
% Uncomment one of the following algorithms. All use a Gaussian kernel.

Adaptive Kernel Learning for Signal Processing
423
% kaf = norma(struct('lambda',1E-4,'tau',30,'kernelpar',sigma,'eta'
,0.99));
% kaf = qklms(struct('epsu',1,'kernelpar',sigma,'eta',0.99));
kaf = swkrls(struct('M',50,'kernelpar',sigma,'c',1E-4));
% kaf = krlst(struct('M',50,'lambda',0.999,'sn2',1E-4,'kernelpar',
sigma));
%% Prepare data
data = load('respiratorymotion3.dat');
X = zeros(n,L);
for i = 1:L,
X(i:n,i) = data(1:n-i+1,1); % time embedding
end
y = data((1:n)+h);
%% Run algorithm
MSE = zeros(n,1);
y_est_all = zeros(n,1);
title_ = upper(class(kaf)); % store algorithm name
fprintf('Training %s',title_)
for i = 1:n,
if ~mod(i,floor(n/10)), fprintf('.'); end % progress indicator
xi = X(i,:);
yi = y(i);
y_est = kaf.evaluate(xi); % evaluate on test data
MSE(i) = (yi-y_est)^2; % test error
y_est_all(i) = y_est;
kaf = kaf.train(xi,yi); % train with one input-output pair
end
fprintf('\n');
%% Output
fprintf('Mean MSE: %.2fdB\n\n',10*log10(mean(MSE)));
figure(1); clf; hold all;
t = (1:n)/26; % sample rate is 26 Hz
plot(t,y), plot(t,y_est_all,'r')
legend({'original',sprintf('%s prediction',title_)},'Location','SE');
Listing 9.10 MATLAB code for running KAF algorithms on the respiratory motion prediction problem.
9.10.3
Online Regression on the KIN40K Dataset
In the second experiment we train the online algorithms to perform regression of
the KINK dataset (Ghahramani, ),which is a standard regression problem in
the machine-learning literature. The KINK data set is obtained from the forward
kinematics of an eight-link all-revolute robot arm, similar to the one depicted in
Figure .. It contains examples, each consisting of an eight-dimensional input
vector and a scalar output. KINK was generated with maximum nonlinearity and
little noise, representing a very diﬃcult regression test.
In this experiment, we ﬁrst determine the optimal hyperparameters for the kernel
adaptive ﬁlters by running the tool kafbox_parameter_estimation, which is
based on the GPML toolbox from Rasmussen and Williams (). We use 
randomly selected data points for the hyperparameter optimization. In the literature,
an anisotropic kernel function, which has a diﬀerent kernel width per dimension,
Data available at http://www.cs.toronto.edu/∼delve/data/datasets.html.

424
Digital Signal Processing with Kernel Methods
Waist 320° (joint 1)
Shoulder 250° (joint 2)
Elbow 270° (joint 3)
Wrist bend 200°
(joint 5)
Flange 532°
(joint 6)
Wrist rotation 300°
(joint 4)
17.0 in.
(432 mm)
17.0 in.
(432 mm)
26.5 in.
(660 mm)
Figure 9.12 Sketch of a five-link all-revolute robot arm. The data used in the KIN40K experiment were
generated by simulating an eight-link extension of this arm.
Table 9.2 Optimal hyperparameters found for the
KIN40K regression problem.
Parameter
Optimal value
Kernel width 𝜎
.
Regularization
.× −
Forgetting factor 𝜆

Table 9.3 Additional parameters used in the KIN40K regression experiment, final
dictionary size, and measured training time.
Algorithm
Parameters
Final dictionary size
Training time (s)
QKLMS
𝜂= ., 𝜖𝕌= .

.
KRLS
𝜈= .

.
FBKRLS
m = 

.
KRLS-T
m = 

.
is commonly used on these data. For simplicity, though, we employ an isotropic
Gaussian kernel. The hyperparameters found by the optimization procedure are listed
in Table .. The forgetting factor is only used by KRLS-T, though its optimal value is
determined to be , which indicates that no forgetting takes place in practice.
From the remaining data we randomly select data points for training and 
data for testing the regression. Apart from the hyperparameters that are determined
automatically in this experiment, the kernel adaptive ﬁlters have some parameters
relating to memory size and learning rate. The values chosen for these parameters
are listed in Table .. The values for QKLMS are chosen such that it obtains optimal

Adaptive Kernel Learning for Signal Processing
425
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
−10
−8
−6
−4
−2
0
QKLMS
KRLS
FBKRLS
KRLST
Figure 9.13 Learning curves of different algorithms on the KIN40K data.
performance after training with a dictionary size that is one order of magnitude larger
than that of the KRLS algorithms. The precision parameter 𝜈of KRLS is tuned to yield a
dictionary size of around m = at the end of the experiment, which is the budget of
SWKRLS and KRLS-T. The learning curves for this experiment are shown in Figure ..
The code for reproducing this experiment is available on the book’s web page and in the
KAFBOX toolbox demos.
9.10.4
The Mackey–Glass Time Series
The Mackey–Glass time-series prediction is a benchmarking problem in nonlinear
time-series modeling. We discussed this time series and the prediction results for
KLMS and KRLS in Sections ..and ..respectively. The learning curves, shown
in Figure ., indicate that KLMS converges very slowly, and that KRLS can obtain
a much lower MSE in less iterations. On the other hand, KRLS requires an order of
magnitude more computation and memory. These results are in line with the intuitions
from linear adaptive ﬁltering, in which LMS and RLS represent two diﬀerent choices
in the compromise between complexity and convergence rate. Nevertheless, there is a
fundamental diﬀerence between the complexity analysis of linear and kernel adaptive
ﬁltering algorithms. While in linear adaptive ﬁlters the complexity depends on the data
dimension, in KAF algorithms it depends on the dictionary size. And, importantly, the
latter is a parameter that can be controlled.
A KRLS-type algorithm with a large dictionary can converge faster than a KLMS-type
algorithm with a similarly sized dictionary, at the expense of a higher computational
complexity. But it would be instructive to ask how a KRLS-type algorithm with a small
dictionary compares with a KLMS-type algorithm with a large dictionary. Can the KRLS
algorithm obtain similar complexity as KLMS, while maintaining its better convergence
rate? This question is answered in the diagrams of Figure .. We have included two
KAF algorithms, QKLMS and KRLS-T, that allow easy control over their dictionary size.
Figure .presents the results obtained by the KAF proﬁler tool included in KAF-
BOX. The MATLAB code to reproduce this ﬁgure is available on the book’s web page.
The proﬁler tool runs each algorithm several times with diﬀerent conﬁgurations, whose
parameters are shown in Table ., producing one point in the plot per algorithm
conﬁguration. It calculates several variables, such as the number of FLOPS, the memory
(in bytes) used, and execution time.

426
Digital Signal Processing with Kernel Methods
Steady-state MSE
Max flops
1E2
1E3
1E4
1E5
1E6
1E7
Q-KLMS
KRLS-T
Steady-state MSE
–28
–26
–24
–22
–20
–18
–16
–14
–28
–26
–24
–22
–20
–18
–16
–14
Max bytes
1E2.5
1E3
1E3.5
1E4
1E4.5
1E5
1E5.5
1E6
Q-KLMS
KRLS-T
Figure 9.14 MSE versus complexity trade-off comparisons for prediction of the Mackey–Glass time
series. Up: maximum number of floating point operations (FLOPS) per iteration as a function of the
steady-state MSE. Down: maximum number of bytes per iteration as a function of the steady-state
MSE. Each marker represents a single run of one of the algorithms with a single set of parameters. The
start of each parameter sweep is indicated by a black dot.
Table 9.4 Parameters used in the Mackey–Glass time-series prediction. A Gaussian
kernel with 𝜎= 1 was used.
Algorithm
Fixed parameters
Varying parameter
QKLMS
𝜂= .
𝜖𝕌∈{−, −, −, ., ., ., ., ., }
KRLS-T
𝜆= , 𝜎
n = −
m ∈{, , , , , , , }

Adaptive Kernel Learning for Signal Processing
427
By plotting the MSE versus the FLOPS or memory, we get an idea of the resources
required to obtain a desired MSE result. If, for instance, we are working in a scenario
with a restriction on computational complexity, we should select the algorithm that
performs best under this restriction by determining which performance curve is most
to the left for the amount of FLOPS available. In the same manner, by ﬁxing a maximum
on MSE we obtain the FLOPS and memory required by each algorithm. In the left plot of
Figure .we observe that if the available computational complexity is very limited, it
may be more interesting to use QKLMS. In other cases, KRLS-T is preferred as a better
MSE is obtained for the same amount of FLOPS. In terms of memory used, it appears
that it is always advantageous to use KRLS-T, as can be seen in the right plot.
9.10.5
Explicit Recursivity on Reproducing Kernel in Hilbert Space
and Electroencephalogram Prediction
This experiment deals with a real EEG signal prediction problem with four-samples
ahead. This is a very challenging nonlinear problem with high levels of noise and
uncertainty. We used ﬁle “SLPA” from the MIT-BIH Polysomnographic Database.
All recordings in the database include an ECG signal, an invasive blood pressure signal,
an EEG signal, and a respiration signal. In this experiment, we considered the prediction
of the EEG signal. As for the MG experiments, we used {, , } training samples,
while the next samples were used for prediction. Listing .gives a simple piece
of code for the implementation of the recursive kernel 𝛾-ﬁlter, which can be used to
generate the appropriate code for obtaining the results.
% RECURSIVEKERNELMATRIX
% Kimn = recursivekernelmatrix(x1,x2,parameters);
%
% Inputs:
%
x1: data matrix with training samples in rows and features in columns
%
x2: data matrix with test samples in rows and features in columns % ...
parameters: ker: {'lin' 'poly' 'rbf'}
%
p1
%
width of the RBF kernel
%
bias in the linear and polinomial kernel
%
degree in the polynomial kernel
%
P
filter order (tap)
%
mu gamma and Laguerre filters free parameter
%
% Output:
%
Kimn: recursive kernels (one per tap!)
function Kimn = recursivekernelmatrix(x1,x2,sigma,p,mu)
N1 = length(x1);
% Training samples
N2 = length(x2);
% Test samples
% Compute Kimn = K^i(m,n).
Kimn = zeros(p,N1,N2);
Kimn(1,:,:) = kernelmatrix('rbf',x1,x2,sigma*sigma);
http://www.physionet.org/physiobank/database/slpdb/slpdb.shtml.

428
Digital Signal Processing with Kernel Methods
for i = 2:p
for m = 2:N1
for n = 2:N2
Kaux1 = 0;
if m > 2
for j = 2:(m-1)
Kaux1 = Kaux1 + (1-mu)^(j-1) * Kimn(i-1,m-j,n-1);
end
end
Kaux2 = 0;
if n > 2
for j = 2:(n-1)
Kaux2 = Kaux2 + (1-mu)^(j-1) * Kimn(i-1,m-1,n-j);
end
end
Kimn(i,m,n) = (1-mu)^2*Kimn(i,m-1,n-1) + ...
mu^2 * Kimn(i-1,m-1,n-1) + mu^2 * (Kaux1+Kaux2);
end
end
end
Listing 9.11 MATLAB code for the recursive kernel 𝛾-filter.
Figure .shows the KRR prediction of two diﬀerent EEG segments in the test set
for the diﬀerent approaches; all the methods show the tendency to oversmooth data
where peaks are observed. Nonetheless, the K kernel always outperforms the other
approaches, conﬁrming the numerical results.
9.10.6
Adaptive Antenna Array Processing
This last real example illustrates the method in a purely adaptive setting of high
complexity. The problem deals with array processing using a planar antenna array model
that consists of × circular printed patches tuned at a frequency of .GHz. The
distance between patches has been set to .𝜆. The chosen substrate was PVC. We
computed the excitation voltage Vn of each element for ﬁve diﬀerent angles of arrival.
In order to compute these excitations, we assume that Vn = V +
n + V −
n , where V +
n is the
ideal excitation, and V −
n is the one produced by the mutual coupling between antenna
elements (Pozar, ). We run ﬁve ﬁnite-diﬀerence time-domain-based simulations
using CST Microwave Studio (http://www.cst.com) to obtain the active reﬂection
coeﬃcients 𝛤n, and then the excitation voltages Vn produced by each angle of arrival.
The azimuth and elevation angles of arrival were ◦, −◦, ◦, ◦and −◦and ◦,
−◦, ◦, −◦and −◦degrees respectively.
Transmitted signals consisted of frames containing binary phase-shift keying
symbols, of which the central ones (midamble) are training samples, and the rest con-
tain the transmitted information. Users are assumed to transmit frames synchronously.
In the simulated scenario, realizations contained frames. At the beginning of
each simulation, the ﬁve users transmitted independent signals with equal power P. At
frame , users and switch oﬀand users , , and change their power to .P. At
frame , user switches on and all users transmit at power P. The SNR with respect
to user (taken as the desired one) was dB all times.

Adaptive Kernel Learning for Signal Processing
429
50
100
150
200
250
300
350
−0.04
−0.02
0
0.02
0.04
0.06
t [ms]
y [t]
Actual
Kc
K2
Kl
1050
1100
1150
1200
1250
1300
−0.1
−0.08
−0.06
−0.04
−0.02
0
0.02
0.04
0.06
t [ms]
y [t]
Actual
Kc
K2
Kl
Figure 9.15 Predictions in two different EEG segments for the recursive Kc, the standard KRR K2, and
the lagged Kl kernels.
Figure .illustrates the BER as a function of time for the desired user. We compare
our method (RKHS–KRLS) with the KLMS (Liu et al., ), KRLS (Engel et al., ),
along with their sparse versions via “surprise criterion” (Liu et al., ) using the
standard RBF kernel. The regularization factor was set to 𝜂= ., the forgetting factor
to ., and the threshold used in approximate linear dependency was set to .. The
recursive kernel used 𝜇= .. The experiment was repeated times and the average
curves are shown. The recursive kernel has the same computational as embedding-
based kernels (Table .). The optimization computational time of the recursive kernel
algorithm is (t), which is the same as KRLS. The added computational burden comes
from the computation of the kernels. Both algorithms need to compute a vector of

430
Digital Signal Processing with Kernel Methods
KLMS–SC
KRLS–SC
KLMS
KRLS
RKHS–KRLS
0
50
100
# Training frames
150
200
10–1
10–2
10–3
Bit Error Rate (BER) in test
Figure 9.16 BER as a function of time (training frames) for the desired user.
Table 9.5 Computational complexity at iteration t.
Algorithm
LMS
KLMS
KRLS
Proposed RKHS recursivity
Computation
(n)
(t)
(t)
((P + )t)
Memory
(n)
(t)
(t)
((P + )t)
kernel products per iteration. The recursive kernel needs an additional computational
time of (Pt) to implement the recursion of Equation .. A higher memory space is
needed because the P kernel matrices of the recursion plus their combination need to
be stored. Following Equation ., kernel entries depend solely on previous kernels.
In addition, the information introduced by each sample into the ﬁrst kernel (P = )
propagates through the following kernels in subsequent recursion steps with a decaying
factor depending on −𝜇. Also, the recursive kernel yields, in general, lower error
bounds in steady state and shows competitive performance with KRLS when the
channel changes. The method is implemented here using only scalars, and hence it
shows a slightly longer convergence time than the rest of the methods. Extension of
the method to considering deeper embeddings is straightforward and will be studied in
the future. The recursive kernel used in this example is available on the book’s web page.
9.11
Questions and Problems
Exercise ..
Discuss when it is more useful to use a KLMS-like algorithm and when
a KRLS-like algorithm.

Adaptive Kernel Learning for Signal Processing
431
Exercise ..
Demonstrate that the computational complexity of KLMS is (m),
where m is the number of data in its dictionary.
Exercise ..
Demonstrate that the computational complexity of KRLS is (m),
where m is the number of data in its dictionary.
Exercise ..
List the advantages and disadvantages of using a sliding-window
approach for determining a ﬁlter dictionary, as used for instance by SW-KRLS and
NORMA.
Exercise ..
Demonstrate that the recursive kernel gamma ﬁlter is unique and
bounded.
Exercise ..
In the tracking experiment of Section .., the slope of the learning
curve for some algorithms is less steep just after the switch than at the beginning of the
experiment. Identify for which algorithms this happens, in Figure ., and explain for
each of these algorithms why this is the case.

433
Part III
Classification, Detection, and Feature Extraction

435
10
Support Vector Machine and Kernel Classification Algorithms
10.1
Introduction
This chapter introduces the basics of SVM and other kernel classiﬁers for pattern
recognition and detection. We start by introducing the main elements and concept
underlying the successful binary SVM. We analyze the issue of regularization and model
sparsity. The latter serves to introduce the 𝜈-SVM, a version of the SVM that allows us to
control capacity of the classiﬁer easily. A recurrent topic in classiﬁcation settings is that
of how to tackle problems involving several classes; for that we then introduce several
available extensions to cope with multiclass and multilabel problems. Other kernel
classiﬁers, such as the LSs SVM and the kernel Fisher’s discriminant (KFD) analysis,
are also summarized and compared experimentally in this chapter.
After this ﬁrst section aimed at introducing the basic classiﬁcation structures, we
introduce more advanced topics in SVM for classiﬁcation, including large margin ﬁlter-
ing (LMF), SSL, active learning, and large-scale classiﬁcation using SVMs. Section .
has to do with large-scale implementations of SVMs, either involving new eﬃcient
algorithmic extensions or parallelization techniques.
10.2
Support Vector Machine and Kernel Classifiers
10.2.1
Support Vector Machines
This section describes the perhaps more useful and impactful machine-learning clas-
siﬁer presented in recent decades: the SVM (Boser et al. ; Cortes and Vapnik
; Gu and Sheng ; Gu et al. ; Schölkopf and Smola ; Vapnik ).
The SVM for classiﬁcation became very popular after its inception due to its out-
standing performance, comparable to those of sophisticated NNs trained with high
computational cost algorithms in complex tasks. SVMs were originally conceived as
eﬃcient methods for pattern recognition and classiﬁcation (Vapnik, ). Right after
their introduction, researchers used these algorithms in a plethora of DSP classiﬁcation
problems and applications, such as speech recognition (Picone et al., ; Xing and
Hansen, ), computer vision and image processing (Cremers et al., ; Kim et al.,
; Xu et al., ), channel equalization (Chen et al., ), multiuser detection
(Bai et al. ; Chen et al. a,b), and array processing (Rohwer et al., ).
Adaptive SVM detectors and estimators for communication system applications were
Digital Signal Processing with Kernel Methods, First Edition. José Luis Rojo-Álvarez, Manel Martínez-Ramón,
Jordi Muñoz-Marí, and Gustau Camps-Valls.
© John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.

436
Digital Signal Processing with Kernel Methods
also introduced (Navia-Vázquez et al., ). We will start the section by introducing
the basic elements behind SVMs, then we introduce the notation and formulations, and
end up the section with some useful variants: the 𝜈-SVM introduced by (Schölkopf et al.,
), and the 𝓁-norm SVM introduced by (Bradley and Mangasarian, ). In the
next sections we will pay attention to other kernel methods for classiﬁcation.
Support Vector Machine Elements
Let (x, y), … (xN, yN) ∈× be a labeled data training set where xi represents an
input data point and yi its corresponding label. In a binary classiﬁcation context, labels
have values {−, }, and in a multiclass context labels have arbitrary values correspond-
ing to each one of the possible classes present in the problem. Another possible context
is the multilabel one. In this context, examples xi are associated simultaneously with
multiple labels. For example, one can classify vehicles in an image. They simultaneously
belong to diﬀerent classes, as a function of their size (motorbike/car/truck), usage
(police/ﬁrst responder/private/cab), color, engine type, and so on.
Deﬁnition ..(Nonlinear SVM binary classiﬁer)
Let 𝝓(⋅) be a nonlinear operator
that maps the samples xi into a higher dimensional Hilbert space; that is, 𝝓∶→.
The SVM classiﬁer is a linear operation of the transformed data into this Hilbert space
as
̂yi = ⟨w, 𝝓(xi)⟩+ b,
(.)
where w is a weight vector inside the RKHS and b is the bias term.
In binary classiﬁcation, the information regarding the predicted class for xi is con-
tained in the sign of the estimated label ̂yi. Hence, the estimation function for classiﬁ-
cation, or classiﬁer, is described as
sgn(̂yi) = sgn(⟨w, 𝝓(xi)⟩+ b).
(.)
The SVM approach uses an indicator function: the classiﬁcation is correct if its sign
is positive and incorrect when its sign is negative. Then, a loss or slack variable 𝜉i is
introduced to work with these two options simultaneously:
𝜉i = −yi
(
⟨w, 𝝓(xi)⟩+ b
)
,
(.)
which is, obviously, always positive if the classiﬁcation is correct. The magnitude of
the error depends on the distance between the sample and the classiﬁcation boundary
described by the set ∶= {x|⟨w, 𝝓(x)⟩+ b = }.
The idea underlying the SVM consists of the so-called SRM along with the empirical
risk minimization described in Section ..((Boser et al., ; Burges, ; Cortes
and Vapnik, )). While the empirical risk is deﬁned as the mean of the cost function
applied over the losses, the structural risk can be deﬁned as the diﬀerence between the
empirical risk over the training samples and the empirical risk over test samples. Thus,
a small structural risk reveals a machine where the estimation function or classiﬁer
boundary is close to the optimal one over all possible choices of the class of estimation

Support Vector Machine and Kernel Classification Algorithms
437
functions. If the structural risk is high, then the machine is suboptimal, which means
that the classiﬁer has been trained to minimize the empirical risk over the training
set rather than to capture the statistical properties of the input samples, hence being
limited to predict out of the sample. This eﬀect is known as overﬁtting, and it is a
consequence of two combined causes. The ﬁrst one is that model’s complexity may be
too high, then having an expression capability able to classify any set of data regardless
of its statistical properties. Then, the model will show a poor performance when used
to classify data not present in the training set. The other cause is, obviously, a lack of
statistical information in the training dataset. Indeed, the data used for training is ﬁnite,
and the presence or not of enough information about its statistical structure will depend
on the number of available samples and their representativity. The model must have a
complexity according to the amount of statistical information of the data.
It has been proven that, in order to minimize the complexity of the classiﬁcation
machine (and thus the structural risk), one must minimize the norm of a model’s
parameters w. Interestingly, this is in turn equivalent to the maximization of the
classiﬁer margin. The margin of a classiﬁer is deﬁned as the distance between the
hyperplanes
Π∶= ⟨w, 𝝓(x)⟩+ b = 
(.)
Π−∶= ⟨w, 𝝓(x)⟩+ b = −.
(.)
The SVM approach is intended to minimize the empirical risk over the training samples
and, at the same time, maximize the distance between these two hyperplanes. The
distance vector between both hyperplanes is proportional to the parameter vector w;
that is, let 𝝓and 𝝓be two vectors belonging to each one of the hyperplanes:
wT𝝓+ b = 
wT𝝓+ b = −
(.)
and force
𝝓= 𝛼wT + 𝝓.
(.)
If this is the case, then the distance between both planes is d = ‖𝝓−𝝓‖. By changing
vector 𝝓in Equation .by Equation ., it is straightforward to see that d = ∕‖w‖
.
This means that minimizing ‖w‖
is equivalent to maximizing the distance between the
hyperplanes, or, in other words, the classiﬁer margin (see Figure .).
Support Vector Machine
Following the criterion described earlier, the binary SVM classiﬁcation algorithm can be
described as the minimization of the empirical error plus the minimization of the norm
of the classiﬁer parameters w. The empirical error is measured as the sum of losses or
slack variables deﬁned in Equation .corresponding to the training samples. Then, a
(primal) functional to be optimized can be constructed as

438
Digital Signal Processing with Kernel Methods
Kernel space
d=
1
w 2
w⊤ϕ+b=1
w⊤ϕ + b = – 1
ϕ(xj)
ϕ(xi)
ξj
ξi
Figure 10.1 Example of margin in the kernel space. Sample 𝜙(xi) is wrongly classified, so its
associated slack variable 𝜉i is higher than 1. Sample 𝜙(xj) is correctly classified, but inside the margin,
so its associated slack variable 𝜉j is positive and less than 1.
min
w,𝜉i
{

‖w‖
+ C
N
∑
i=
𝜉i
}
subject to:
yi
(⟨w, 𝝓(xi)⟩+ b) ≥−𝜉i,
i = , … , N
𝜉i ≥,
i = , … , N,
(.)
where the term ∕has been added to simplify the result. Note that this functional forces
the losses or slack variables 𝜉i to be positive or zero. That is, when the product between
the label and the prediction is higher than unity this means that the corresponding
sample has been correctly classiﬁed and it is out of the margin, so its distance to the
classiﬁcation hyperplane is higher than d. The implicit cost function is equal to the slack
variable 𝜉i (or absolute value of the diﬀerence between the prediction and the label) if
the slack variable is positive, and zero if the slack variable is negative. This cost function
is known as the hinge loss function, with the following deﬁnition.
Deﬁnition ..(Hinge loss)
Given a sample xi, yi and an estimator ̂f (xi) = ̂yi =
⟨w, 𝜙(xi)⟩+ b), the hinge loss can be deﬁned as
H(yi, ̂f (xi)) ∶= max(, −yîf (xi)) = max(, −yîyi),
(.)
which is sometimes indicated as H(yi, ̂f (xi)) ∶= [, −yîf (xi)]+.
Note that this deﬁnition matches that of slack variable 𝜉i; therefore, it can be said
that the cost function used in SVM is a hinge loss. Parameter C stands for the trade-oﬀ
between the minimization of the structural and empirical risks, as in the functional for

Support Vector Machine and Kernel Classification Algorithms
439
SVR (see Section ..). This functional can be optimized using Lagrange optimization.
Let positive-valued variables 𝛼i and 𝜇i be the corresponding Lagrange coeﬃcients for
the two sets of constraints. Then, a Lagrangian functional can be constructed as
min
w,𝜉i
{

‖w‖
+ C
N
∑
i=
𝜉i −
N
∑
i=
𝛼i(yi(⟨w, 𝝓(xi)⟩+ b) −+ 𝜉i) −
N
∑
i=
𝜇i𝜉i
}
.
(.)
Diﬀerentiating with respect to the primal variables w and b gives the results
w =
N
∑
i=
𝛼iyi𝝓(xi),
(.)
N
∑
i=
𝛼iyi = ,
(.)
and
C −𝛼i −𝜇i = .
(.)
Adding these three results to the Lagrangian (Equation .) gives the dual problem to
be maximized with respect the multipliers 𝛼i only:
max
𝜶
{
−

N
∑
i=
N
∑
j=
𝛼iyi⟨𝝓(xi), 𝝓(xj)⟩yj𝛼j +
N
∑
i=
𝛼i
}
,
(.)
where it can be observed the (kernel) dot product between samples ⟨𝝓(xi), 𝝓(xj)⟩=
K(xi, xj). The equation can be written in matrix notation as
Ld = −𝜶TYKY𝜶+ 𝜶T,
(.)
where matrix K contains all dot products between the mapped samples, 𝜶is a column
vector containing all Lagrange multipliers, Y is a diagonal matrix containing all labels,
and is an all ones column vector. The result in Equation .gives a constraint over
the Lagrange multipliers. Since 𝜇i is deﬁned as positive, then
≤𝛼i ≤C.
(.)
Besides, the result 𝜇i𝜉i = implies that when 𝜉i = , 𝛼i = C. In summary if the sample
is correctly classiﬁed and it is out of the margin, then its corresponding slack variable
is zero, which implies that its Lagrange multiplier 𝛼i is zero too. If the sample is well
classiﬁed and inside the margin, or wrongly classiﬁed inside or outside the margin, then
its Lagrange multiplier will be strictly between zero and C. For those well-classiﬁed
samples which are right in the margin, the Lagrange multiplier will be equal to C.

440
Digital Signal Processing with Kernel Methods
Appropriate choice of nonlinear mapping 𝝓guarantees that the transformed samples
are more likely to be linearly separable in the (higher dimension) feature space. The
regularization parameter C controls the generalization capability of the classiﬁer, and
it must be selected by the user. Note that the decision function for any test point x∗is
given by
̂y∗= f (x∗) = sgn
( N
∑
i=
yi𝛼iK(xi, x∗) + b
)
,
(.)
where 𝛼i are Lagrange multipliers obtained from solving the QP problem in Equa-
tion .. The support vectors are those training samples xi with nonzero Lagrange
multipliers 𝛼i ≠. Typically, the bias term b is calculated by using the unbounded
Lagrange multipliers as b = ∕k ∑k
i=(yi −⟨𝝓(xi), w⟩), where k is the number of
unbounded Lagrange multipliers (≤𝛼i < C) and w = ∑N
i=yi𝛼i𝝓(xi) (Schölkopf and
Smola, ).
The ν-Support Vector Machine for Classification
An interesting variation of the SVM is the 𝜈-SVM introduced in (Schölkopf et al., ).
In the SVM formulation, the soft margin is controlled by parameter C, which may take
any positive value. This makes it diﬃcult to adjust when training the classiﬁer. The idea
of the 𝜈-SVM is forcing the soft margin to lie in the range [, ]. This is carried out by
redeﬁning the problem to solve
min
w,𝜉i
{

‖w‖
+ 𝜈𝜌+ 
N
N
∑
i=
𝜉i
}
(.)
subject to:
yi(⟨𝝓(xi), w⟩+ b) ≥𝜌−𝜉i
∀i = , … , N
(.)
𝜌≥, 𝜉i ≥
∀i = , … , N.
(.)
In this new formulation, parameter C has been removed and a new variable 𝜌with
coeﬃcient 𝜈has been introduced. This new variable 𝜌adds another degree of freedom
to the margin, the size of the margin increasing linearly with 𝜌. The old parameter C
controlled the trade-oﬀbetween the training error and the generalization error. In the
𝜈-SVM formulation, this is done adjusting 𝜈in the range [, ], which acts as an upper
bound on the fraction of margin errors, and it is also a lower bound on the fraction of
support vectors.
The 𝓁1-Norm Support Vector Machine for Classification
The standard SVM works with the 𝓁-norm on the model’s weights, because it makes
the problem more easily solvable in practice via QP routines. Other norms can be used
though, such as the 𝓁-norm. The 𝓁-SVM was proposed by (Bradley and Mangasarian,
) and shows some advantages over the standard 𝓁-norm SVM, especially when
dealing with redundant noisy features. The formulation is as follows:

Support Vector Machine and Kernel Classification Algorithms
441
min
w
{
‖w‖+
N
∑
i=
H(yi, ̂f (xi))
}
,
(.)
where ‖w‖∶= ∑d
i=|wi|, ̂f (xi) = 𝝓(xi)Tw + b, and recall that H(y, ̂f (x)) = max(, −
ŷf (x)) is the hinge loss deﬁned in Equation .. Note that the 𝓁-norm cost is not
diﬀerentiable at zero, which explains why the machine can delete many noisy features
by estimating their coeﬃcients by zero. The model can be actually considered a better
option when the underlying signal structure is sparse. However, optimization is not
that easy, and several approaches have been proposed (Bradley and Mangasarian, ;
Weston et al., ; Zhu et al., ).
10.2.2
Multiclass and Multilabel Support Vector Machines
Many methods have been proposed for multiclass and multilabel classiﬁcation on SVM
literature. The simplest approach consists of the so-called one-against-all (OAA) or one-
vs-all, where each class is compared with the rest of the classes. This approach is very
popular because of its simplicity and good results in general (Rifkin and Klautau, ).
The OAA implies training L−classiﬁers for problems with L classes. This strategy gives
rise to binary classiﬁers that will cope with biased problems: there will be L −more
examples in one class than in the other. In order to alleviate the potential inconveniences
of this approach, the one-against-one (OAO) (Allwein et al. ; Friedman ;
Hastie and Tibshirani ; Hsu and Lin ) strategy was proposed and popularized.
In this scheme, authors proposed to compare groups of two classes. During the training
phase, the data are organized in all possible pairs of two classes, and a classiﬁer is trained
with each pair. During the test phase, each data point is presented to all classiﬁers, and a
decision is taken based on the most consistent classiﬁcation. In other words, the sample
is classiﬁed as belonging to the most voted class. Friedman () prove that the OAO
classiﬁer is equivalent to the Bayes rule when the class posterior probabilities are known.
Hastie and Tibshirani () improved the OAO classiﬁcation performance by using
probability estimates of the binary decision of each classiﬁer and then combining these
estimates in a joint probability estimator for all classes. This multiclass classiﬁer needs
the use of L(L −)∕binary classiﬁers; thus, its potential drawback is the computational
cost, but this issue depends on the number of examples per class as well.
Single Optimization Approaches
The ﬁrst single machine compact multiclass SVM was presented by (Weston and
Watkins, ). The idea of the multiclass SVM consists of constructing L diﬀerent
classiﬁcation machines. For a training input pattern xi with an associated label yi ∈
, … , L, each machine has the form
fl(xi) = ⟨wl, 𝝓(xi)⟩+ bl,
(.)
and it is intended to produce a positive output if yi
=
l and a negative output
otherwise. This can also be viewed as an OAA machine, but constructed using a
For instance, the popular LIBSVM toolbox implements the OAO strategy by default.

442
Digital Signal Processing with Kernel Methods
compact formulation. Since all machines are optimized at the same time, this alleviates
the eﬀect of the unbalanced sets. These classiﬁcation machines can be used to construct
a primal function, given by
min
w,𝜉l
i
{


L
∑
l=
‖wl‖+ C
N
∑
i=
∑
l≠yi
𝜉l
i
}
(.)
subject to
⟨wyi, 𝝓(xi)⟩+ byi ≥⟨wl, 𝝓(xi)⟩+ bl −𝜉l
i
𝜉l
i ≥
l ∈, … , L ⧵yi.
(.)
The constraints force the output of the classiﬁer to which the sample belongs to be
higher than the rest of the machines. The diﬀerences between the machine correspond-
ing to the class yi and the rest of the classes are the slack variables 𝜉l
i. If the diﬀerence is
negative, this means that the machine yk produces a higher response than the machine
l ≠yk, and then the slack variable is constrained to zero. When the diﬀerence is positive,
the sample has been wrongly classiﬁed.
The primal functional can be optimized using Lagrange optimization. See Weston and
Watkins () for a complete derivation of the dual. The optimization must include
variables
cl
i =
{

if yi = l

if yi ≠l
and
Ai =
L
∑
l=
𝛼l
i.
(.)
The dual formulation for the optimization problem is then
min
𝛼l
i
{

∑
i,l
𝛼l
i +
∑
i,j,l
(
−
cyi
j AiAj + 𝛼l
i𝛼yi
j −
𝛼l
i𝛼l
j
)
K(xixj)
}
(.)
with the linear constraints
N
∑
i=
𝛼l
i −
N
∑
i=
cl
iAi = ,
l ∈, … , L
(.)
≤𝛼l
i ≤C,
𝛼yi
i = .
(.)
As pointed out by Rifkin and Klautau (), other approaches that solve the clas-
siﬁcation problem using a single optimization scheme have been introduced that
present diﬀerences with the aforementioned multiclass SVM. The machine presented by
Crammer and Singer () simpliﬁes the formulation by including in the constraints
only the machine belonging to the class yi and the machine with largest output among
the remaining machines. Lee et al. () proved that while the standard SVM tends
to the Bayes solution when the number of training patterns tends to inﬁnity, the OAA
machines do not have this property. Thus, in this work, a multiclass SVM is formulated

Support Vector Machine and Kernel Classification Algorithms
443
with this asymptotic property. Other approaches based on a single machine can be
found in Bredensteiner and Bennett (), where the standard regularization of the
SVM is changed by the regularization ‖wi −wj‖. Finally, the Cliﬀord SVM presented
in Bayro-Corrochano and Arana-Daniel () is worth mentioning; here, the authors
reformulate the multiclass problem using a d-dimensional algebra and a geometric (or
Cliﬀord) product to obtain a compact formulation of the optimization problem.
Error-Correcting Approaches
An alternative approach presented by Dietterich and Bakiri () is rooted in the
idea of the error correction codes (ECCs) widely used in communications. The error-
correcting approach takes the classiﬁcation procedure as a channel whose input is the
pattern, and whose output is a code associated with the pattern label. In the error-
correcting approach, each label is then associated with a binary sequence. If there are L
diﬀerent classes and the binary string has length S, then a codebook can be constructed
with Hamming distances equal to d = S −L + . Then, classiﬁcation machines can be
constructed to produce codes as a response of each input pattern. When a response
is produced with an error with respect to the actual label which is less than half the
Hamming distance (i.e., less than or equal to ⌊(d −)∕⌋) the error can be corrected
to produce a correct classiﬁcation. In other cases, at least, the error can be detected
and the classiﬁcation identiﬁed as wrong. In particular, the approach considers a binary
matrix M of size L×S, where S is equal to the string length and this equals the number of
classiﬁers to be trained. As in ECCs used in communications (e.g., Cover and Thomas,
), here one has to satisfy two main properties. First, codewords (or the rows of
matrix M) should be well separated in terms of the Hamming distance. Also, each bit
of each classiﬁer should be uncorrelated with the same bit of other classiﬁers. This can
be achieved by forcing the Hamming distance between each column of matrix M and
the other columns to be large and forcing the Hamming distance between each and the
complement of the other columns to be large.
In a code with L classes, a set of L sequences with L bits can be constructed. In order
to satisfy both conditions, one must remove the columns with all zeros and all ones,
and the columns that are complementary to other columns. For example, a codebook
for L = is depicted in Table .. Each column of this table codes digits from to
in a binary notation. In this table, columns l and −l are complementary, so one
put of each two can be removed, so only half of the columns are useful. However, one
of the columns will be all zeros or ones, so only three columns are left (see Table .).
Obviously, in that case, the Hamming distance will be one and there will be no possibility
Table 10.1 Code for a three class problem (Dietterich
and Bakiri, 1995).
Class
f0
f1
f2
f3
f4
f5
f6
f7




























444
Digital Signal Processing with Kernel Methods
Table 10.2 Pruned code for a three-class problem.
Class
f0
f1
f2
f3















of error correction. In general case is at most L−−. For L = , codes of length are
available, and for L = , the corresponding length will be . In that case, the Hamming
distance will be d = .
The error-correcting code approach uses a maximum number of classiﬁers that expo-
nentially grows with the number of classes. Nevertheless, in this approach, this number
must be taken as an upper bound, since the number of outputs will determine the
Hamming distance of the codes used that, in turn, will determine the error correction
ability of the classiﬁer. For example, for L = , the maximum number of outputs will
be , with a Hamming distance d = , which allows one to correct up to bits. In
most applications, a much lower correcting code will be good enough. Once a set of
machines is trained to output the desired code for each sample, the test is performed
simply by computing the corresponding output of each machine for a given sample.
Then a decision is made as a function of the distance between the output obtained and
each one of the codes contained in the codebook.
The idea of margin maximization in error-correcting codes was introduced by All-
wein et al. (). They, in particular, introduced a general framework for multiclass
classiﬁers that minimizes loss functions expressed in terms of yif (xi), including what is
known as the AdaBoost algorithm, which minimizes a cost function that is a function
of exp(−yif (xi)). In these studies, the output matrix contains label in addition to −
and +, in order to automatically exclude all examples with label equal to during the
training. This trick allows us to provide a unifying theory that includes OAO, OAA,
and ECC classiﬁers. See Figure .for a numerical comparison between the diﬀerent
multiclass strategies.
The multiclass estimation function that uses the Hamming distance for the decoding
can be constructed as
f (x) = arg min
l∈,…,L
S
∑
i=
(−sgn(Ml,ifi(x))

)
,
(.)
where Ml,i is the element (l, i) of code matrix M. Here, if Ml,i = then its contribution
to the cost is ∕. If the signs of fix and Ml,i match, then the contribution is zero,
and one otherwise. This output only accounts for the possible events of prediction
agreement, disagreement with the label, plus oﬀering the possibility of ignoring the
output, but provided that only the sign of the prediction is used, this approach discards
the magnitude of the predictions, which can be an approach to the conﬁdence of the
output. A better estimation function is then

Support Vector Machine and Kernel Classification Algorithms
445
1
2
3
4
5
6
7
0
10
20
30
40
50
60
70
Number of classes
Number of classifiers
OAA
OAO
ECC
Figure 10.2 Number of classifiers as a function of the number of classes for the various multiclass
classification strategies. In an L class classification problem, the OAA and the single machine
approaches have L −1 outputs, while the OAO uses (1∕2)N(N −1) outputs. The ECC approach has a
maximum number of outputs equal to of 2L−1 −1.
f (x) = arg min
l∈,…,L
S
∑
i=
L(Ml,ifi(x)),
(.)
where the sgn function has been removed. In an OAA approach, where only Ml,i =
and the rest of the labels are negative, then the function can be rewritten as
arg minl∈,…,L L(fr(x)) −∑
i≠l SL(−fi(x)).
Multilabel Support Vector Machine Classifiers
A multilabel classiﬁer considers the problem of classifying data that simultaneously
belongs to more than one class. Multilabel classiﬁcation methods are required in
bioinformatics problems; for example, see the protein function classiﬁcation example in
Zhang and Zhou () and text categorization in Li et al. (), among many others.
In these problems, each instance is not associated with a single label as in binary or
multiclass classiﬁcation, but rather with a set labels. In general, the number of labels of
each pattern is variable. For example, in a text mining task, a single text may belong to
several topics (e.g., politics and religion), while others will belong to a single topic only
(e.g., soccer). Schapire and Singer () introduced a boosting multilabel system for
text categorization. In this work, they state that overﬁtting is very important in this
class of problems, so regularization is fundamental in multilabel problems. Elisseeﬀ
and Weston () introduced a maximum margin method intended to provide a good
control of the complexity. The study introduces a class of SVM multilabel classiﬁers that
can easily be kernelized.

446
Digital Signal Processing with Kernel Methods
Assume a set of examples xi, ≤i ≤N, which can be multilabeled among L
diﬀerent labels. The maximum number of possible multilabels is then L, and they
can be represented with binary vectors yi of L components, in a way similar to the
multiclass OAA labeling, but in this case more than one position of the vector can hold a
value +due to the multilabel feature of the patterns. One straightforward approach to
solve the multilabel problem consists of setting up L diﬀerent binary classiﬁers, one for
each element of the label vectors (Joachims, b). Nevertheless, as is pointed out by
Elisseeﬀand Weston (), McCallum (), and Pavlidis et al. (), this approach
does not take into account the possible dependencies between labels; that is, with such
an approach the information that can be captured about the statistical structure of the
data will be partial unless the labels are independent in practice. This model is clearly
incomplete in a wide variety of real-world problems.
The approach considered by Elisseeﬀand Weston () is based on a ranking
procedure, where all classes are jointly taken into account. The ranking machine simply
sorts the data depending on an estimation of the probability of belonging to each one
of the classes. Assume a set of L linear binary classiﬁers
fl(xi) = ⟨wl, 𝝓(xi)⟩+ bl
(.)
constructed to produce a ranked output. In order to produce a maximum margin
training strategy, the following criterion is applied to the parameters of the classiﬁers:
min
j∈y,k∈̄y
fj(xi) −fk(xi)
‖wj −wk‖,
(.)
where ̄y represents the complementary of label y, j represents the index of the correct
label, and k the set of incorrect labels. Moreover, if one assumes that all training
samples are correctly labeled and well ranked, then the diﬀerence between outputs is
approximated by the lower bound
fj(xi) −fk(xi) ≥.
(.)
Then, the criterion in Equation .can be approximated as in Elisseeﬀand Weston
() by the functional
min
j∈y,k∈̄y

‖wj −wk‖≈min
L
∑
j=
‖wj‖.
(.)
In general, the patterns will not be linearly separable; that is, some of the patterns will
be inside the margin or wrongly classiﬁed. Then, a nonnegative slack variable must be
included, which leads to the constraint
fj(xi) −fk(xi) ≥−𝜉ikj.
(.)

Support Vector Machine and Kernel Classification Algorithms
447
Hence, the optimization criterion for the multilabel classiﬁcation reduces to
min
w,𝜉ikj
{ L
∑
j=
‖wj‖+ C
L
∑
i=

‖yi‖‖̂yi‖
∑
j,k∈yi×̄yi
𝜉ikj
}
subject to
fj(xi) −fk(xi) ≥−𝜉ikj
𝜉ikj ≥.
(.)
This functional can be expressed in terms of dual variables, and then kernelized, though
it has been suggested to use approximate methods due to the high computational
burden of quadratic methods.
10.2.3
Least-Squares Support Vector Machine
The LS-SVM (Suykens and Vandewalle, , ) uses an alternative quadratic form
for the empirical risk while maintaining the structural risk and the margin-related
constraints, although now including equality constraints. The primal functional for the
LSSVM is
min
w
{

‖w‖+ 
C
N
∑
i=
e
i
}
subject to yi(⟨w, 𝝓(xi)⟩+ b) = −ei.
(.)
A Lagrange analysis similar to that of the standard SVM can be applied here. The
Lagrangian consists of simply solving
min
w,ei,𝛼i,b
{

‖w‖+ 
C
N
∑
i=
e
i −
N
∑
i=
𝛼i
(yi(⟨w, 𝝓(xi)⟩+ b) −+ ei
)
}
.
(.)
This Lagrangian is optimized by computing its gradient with respect to all primal and
dual variables, to obtain the solution
⎛
⎜
⎜
⎜⎝
I


−𝜱TY



−yT


CI
−I
Y𝜱
y
I

⎞
⎟
⎟
⎟⎠
⎛
⎜
⎜
⎜⎝
w
b
e
𝜶
⎞
⎟
⎟
⎟⎠
=
⎛
⎜
⎜
⎜⎝




⎞
⎟
⎟
⎟⎠
,
(.)
where y is a column vector containing all training labels, Y is a diagonal matrix
containing y, e is a column vector containing all errors, represents a vector of zeros,
and is a column vector of ones. A dual solution of the equations is found by eliminating
w and e as
(

−yT
y
YKY + CI
) (
b
𝜶
)
=
(


)
.
(.)
This equation can be solved in block instead of using QP, with the advantage of
a much less computational burden. Note that the matrix to be inverted is positive

448
Digital Signal Processing with Kernel Methods
deﬁnite; hence, the solution exists and is unique. However, the values of the dual
variables are proportional to the errors; hence, the solution is not sparse. Authors
provide solutions to obtain sparse approximations to these solutions via pruning when
the quantity of training samples is not aﬀordable for the application at hand.
10.2.4
Kernel Fisher’s Discriminant Analysis
A related formulation to the LS-SVM is the KFD proposed by Mika et al. ().
Assume that Nout of N training samples belong to class −and Nto class +. Let
𝝁be the mean of the whole set, and 𝝁−and 𝝁+ the means for classes −and +
respectively. Analogously, let 𝜮be the covariance matrix of the whole set, and 𝜮−and
𝜮+ the covariance matrices for the two classes. The linear Fisher’s discriminant (LFD)
seeks for projections that maximize the interclass variance and minimize the intraclass
variance (Fisher, ; Hastie et al., ). By deﬁning the between-class scatter matrix
SB = (𝝁−−𝝁+)(𝝁−−𝝁+)T and the within-class scatter matrix SW = 𝜮−+ 𝜮+, the
problem reduces to maximizing the ratio
J(w) = wTSBw
wTSWw.
(.)
The problem is usually reformatted as follows. If w is a solution of Equation ., any
scalar multiple of it will be also. To avoid multiple solutions the arbitrary constraint
wTSBw = is imposed. This is equivalent to imposing wT(𝝁−−𝝁+) = . The optimiza-
tion problem becomes minimizing
wTSWw
subject to wT(𝝁−−𝝁+) = .
(.)
Solving the Lagrange function associated with the above problem, a closed-form
solution for w is obtained, w = 𝜆S−
W (𝝁−−𝝁+), where 𝜆is the Lagrange multiplier
obtained by
𝜆=

(𝝁−−𝝁+)TS−
W (𝝁−−𝝁+)
.
(.)
When classes are normally distributed with equal covariance, w is in the same direction
as the discriminant in the corresponding Bayes optimal classiﬁer. Hence, for this special
case, LFD is equivalent to the Bayes optimal classiﬁer (the one deﬁned by linear
discriminant analysis (LDA)).
The KFD is obtained by deﬁning the LFD in a high-dimensional feature space . Now,
the problem reduces to maximizing
J(w) =
wTS𝝓
Bw
wTS𝝓
Ww
,
(.)
where w, S𝝓
B and S𝝓
W are deﬁned in , S𝝓
B = (𝝁𝝓
−−𝝁𝝓
+)(𝝁𝝓
−−𝝁𝝓
+)T, and S𝝓
W = 𝜮𝝓
−+ 𝜮𝝓
+.
We need to express Equation .in terms of dot products only. According to the rep-
resenter theorem (Schölkopf and Smola, ), any solution w ∈can be represented
as a linear combination of training samples in . Therefore, w = ∑N
i=𝛼i𝝓(xi), and then

Support Vector Machine and Kernel Classification Algorithms
449
⟨w, 𝝁𝝓
i ⟩= 
Ni
N
∑
j=
Ni
∑
k=
𝛼jK(xj, xi
k) = 𝜶TMi,
(.)
where xi
k represents samples xk of class i, and (Mi)j = (∕Ni) ∑Ni
k=K(xj, xi
k). Taking the
deﬁnition of S𝜙
B and Equation ., the numerator of Equation .can be rewritten
as wTS𝜙
Bw = 𝜶TM𝜶, and the denominator as wTS𝝓
Ww = 𝜶TN𝜶, where
M = (M−−M+)(M−−M+)T,
(.)
N =
∑
j={−,+}
K j(I −nj)K T
j .
(.)
K j is an N × Nj matrix with (K j)nm = K(xn, xj
m) (the kernel matrix for class j), I is the
identity, and Nj is a matrix with all entries set to ∕Nj. Finally, the FLD in is solved
by maximizing
J(𝜶) = 𝜶TM𝜶
𝜶TN𝜶,
(.)
which is solved as in the linear case by ﬁnding the leading eigenvector of N−M. The
projection of a new sample x onto the discriminant w can be computed through the
kernel function implicitly:
⟨w, 𝜙(x)⟩=
N
∑
i=
𝛼iK(xi, x).
(.)
The diﬀerence between the SVM and the Fisher discriminant is that the latter forces
the maximum separation between class distributions (see Figure .for an illustrative
comparison between KFD and SVM). In order to use these projections for classiﬁcation,
one needs to ﬁnd a suitable threshold which can be chosen as the mean of the average
projections of the two classes.
The dimension of the feature space is equal to, or higher than, the number of training
samples, which makes regularization necessary, otherwise overﬁtting is guaranteed.
Since KFD analysis minimizes the variance of data along the projection and maximizes
the distance between average outputs for each class, one can equivalently solve a QP
problem that contains a regularizer ‖w‖of the form (Mika et al., )
min
w,𝜉i,b
{

‖w‖
+ C
N
∑
i=
𝜉
i
}
(.)
constrained to
𝜙(xi)w + b = yi + 𝜉i,
i = , … , N.
(.)
This minimization procedure can be intuitively interpreted by noting that KFD attempts
to obtain a regularized solution (term ‖w‖
in Equation .) in which the output

450
Digital Signal Processing with Kernel Methods
(a)
Maximum separating hyperplane 
in feature Hilbert space
xj
ξj
xi
ξi
ω
Class distribution in feature Hilbert space
μ+
σ–
σ+
μ–
w
(b)
Figure 10.3 Comparison between two kernel classifiers. (a) SVM: linear decision hyperplanes in a
nonlinearly transformed, feature space, where slack variables 𝜉i are included to deal with errors.
(b) KFD: separates the classes by projecting them onto a hyperplane where the difference of the
projected means (𝜇+, 𝜇−) is large, and the variance around means 𝜎+ and 𝜎−is small.
for each sample is forced to move forward its corresponding class label (restriction in
Equation .), the variance of the errors is minimized (term ∑
i 𝜉
i in Equation .).
The solution of this primal problem leads to a dual with the solution
w = 
C
N
∑
i=
𝛼i𝝓(x)
(.)
and a dual optimization problem
max
𝜶
{
−
𝜶T𝜶−
C 𝜶TK𝜶+ yT𝜶
}
(.)
subject to
𝜶T𝟏= .
(.)
Minimizing the functional in Equation .leads to a nonsparse solution; that is, all
training samples are taken into account and weighted in the solution obtained. This
may be a dramatic problem when working with a high number of labeled samples,
inducing problems of high computational cost and memory requirements. This fact
is illustrated in Figure ., where the distribution of nonzero Lagrange multipliers
illustrates the concept of sparsity for the SVM and KFD. Moreover, the issue of sparsity
poses the question of the computational burden. For large datasets, the evaluation of
wT𝝓(x) in KFD is very computationally demanding, and thus the optimization becomes
more diﬃcult. In fact, clever tricks like chunking (Osuna et al., ) or sequential
minimal optimization (SMO) (Platt, a) cannot be applied, or only at a much higher

Support Vector Machine and Kernel Classification Algorithms
451
0
5
10
15
20
25
30
0
200
400
600
800
1000
1200
1400
αi yi
Sorted histogram of |αi|
SVM
KFD
Figure 10.4 Illustration of the sorted density of Lagrange multipliers 𝛼i for the best SVM and KFD
classifiers. The concept of sparsity in SVMs and the nonsparse solution offered by KFD are evident.
computational cost. This problem has been previously pointed out, and sparse versions
of the KFD have been proposed (Mika et al., ).
Experimental Comparison
Here we compare the performance of 𝜈-SVM, LFD, and KFD methods in a remote-
sensing multisource image classiﬁcation problem: the identiﬁcation of classes “urban”
and “nonurban.” The images used are from ERSsynthetic aperture radar (SAR) and
Landsat TM sensors acquired in over the area of Naples, Italy (Gómez-Chova
et al., ). The dataset has seven Landsat bands, two SAR backscattering intensities
(–days), and the SAR interferometric coherence. Since these features come from
diﬀerent sensors, the ﬁrst step was to perform a speciﬁc processing and conditioning of
optical and SAR data, and to co-register all images. Then, all features were stacked at a
pixel level. A small area of the image of × pixels was selected.
We used randomly selected pixels (samples) of each class to train the classiﬁers
(only “urban” samples for the one-class experiment). Except for the LFD, the other
classiﬁers have free parameters that must be tuned in the training process. To do this, the
training set was split following a v-fold strategy. For all methods, we used the RBF kernel
where 𝜎was tuned in the range [−, ] in logarithmic increments of . The 𝜈-SVM
has an additional parameter to tune: 𝜈was varied in the range [., .] in increments
of .. Experiments were repeated times with diﬀerent random realizations of the
training sets. Averaged results are shown using four diﬀerent error measures obtained
from the confusion matrices: the estimated 𝜅statistic (Congalton and Green, );
the precision P, deﬁned as the ratio between the number of true positives and the sum
of true positives and false positives; and the recall R, deﬁned as the ratio between the

452
Digital Signal Processing with Kernel Methods
Table 10.3 Mean and standard deviation of estimated 𝜿statistic, precision, recall, F-measure, and rate
of support vectors for the 10 realizations. Best results are in bold.
Method
κ
Precision
Recall
F-Measure
% SVs
𝜈-SVM lin
.± .
.± .
.± .
.± .
± .
𝜈-SVM RBF
.± .
.± .
.± .
.± .
± .
LFD
.± .
.± .
.± .
.± .
—
KFD
.± .
.± .
.± .
.± .
—
number of true positives and the sum of true positives and false negatives. The last one
is the F-measure (or unbiased F-score), computed as F = (PR)∕P + R, which combines
both measures.
From Table ., the linear kernel yields slightly better results, but the diﬀerences
with the RBF kernel are not statistically signiﬁcant. On the contrary, KFD is better than
the linear kernel LFD. Algorithms based on SVM using the RBF kernel yield slightly
lower performance than the KFD algorithm, probably due to the low number of training
samples used.
10.3
Advances in Kernel-Based Classification
This section introduces advanced kernel machines for classiﬁcation. We ﬁrst introduce
the LMF method that performs both signal ﬁltering and classiﬁcation simultaneously
by learning the most appropriate ﬁlters, then we pay attention to the manifold learning
framework and introduce SSL with SVMs that exploit the information contained in both
labeled and unlabeled examples. We also review useful kernel developments such as the
MKL, which allows to combine diﬀerent pieces of information in SVM classiﬁcation.
When the outputs are related, structured learning (SL) SVMs can be very useful, and
we will exemplify their use as well. We ﬁnish the section summarizing the ﬁeld of active
learning (AL), where the training set is optimized such that the SVM classiﬁcation
accuracy is optimal with a reduced number of informative points.
10.3.1
Large Margin Filtering
Many signal processing problems are tackled by ﬁrst ﬁltering the signal to obtain
useful features, and subsequently performing a feature classiﬁcation or regression.
Both steps are critical and need to be designed carefully to deal with the particular
statistical characteristics of both the signal and the noise. Signal sequence labeling
is a paradigmatic example where one aims at tagging speech. Another example is
encountered in biomedical engineering where one typically ﬁlters electrocardiographic
signals before performing arrhythmia prediction. However, optimal design of the ﬁlter
and the classiﬁer are typically tackled in a separated way, thus leading to subop-
timal classiﬁcation schemes. The LMF method presented in Flamary et al. ()
is an eﬃcient methodology to learn an optimal signal ﬁlter and an SVM classiﬁer
jointly. After introducing the basic formulation, we illustrate the performance of the

Support Vector Machine and Kernel Classification Algorithms
453
method in a challenging real-life dataset of brain–computer interface (BCI) time-series
classiﬁcation.
The setting in which LMF is placed is as follows. Imagine we want to predict a
sequence of labels either from a multichannel signal or from multichannel features
extracted from that signal by learning from examples. We consider that the training
samples are gathered in a matrix X ∈ℝN×d containing d channels and N samples. Xi,j
is the value of channel j for the ith sample (Xi,⋅). The vector 𝐲∈{−, }N contains the
class for each sample. In the following, multiclass problems are handled by means of
pairwise binary classiﬁers. Let us deﬁne the ﬁlter applied to X by the matrix F ∈ℝ𝜏×d.
Each column of F is a ﬁlter for the corresponding channel in X, and 𝜏is the size of the
FIR ﬁlters. We deﬁne the ﬁltered data matrix ̃X by
̃Xi,j =
𝜏∑
m=
Fm,jXi+−m+n,j = Xi,j ⊗F⋅,j,
(.)
where the sum is a unidimensional convolution (⊗) of each channel by the ﬁlter in the
appropriate column of F. In this setting, the problem that LMF solves is
min
g,F
{

‖g‖
+ C
N
N
∑
i=
H(𝐲i, g( ̃Xi,⋅))p + 𝜆𝛺(F)
}
,
(.)
where 𝜆is a regularization parameter and Ω(⋅) represents a diﬀerentiable regularization
function of F. Note that the two leftmost parts of Equation .reduce to a standard
SVM for ﬁltered samples ̃X as deﬁned in Equation .. However, here, F is a variable
to be minimized instead of being a ﬁxed ﬁlter structure. When jointly optimizing over
the decision function g and the ﬁlter F, the objective function is typically nonconvex.
However, the problem deﬁned by Equation .is convex with respect to g(⋅) for any
ﬁxed ﬁlter F, and in such a case it boils down to solving the SVM problem. Therefore,
in order to take into account this speciﬁc structure of the problem, authors proposed to
solve the problem through the following min–max approach:
min
F {J(F)} = min
F {J′(F) + 𝜆Ω(F)},
(.)
where J′(F) is the objective value of the following primal problem:
J′(F) = min
g
{

‖g‖
+ C
N
N
∑
i=
H(yi, g( ̃Xi,⋅))
}
,
(.)
Instead of solving the problem in Equation .through a min–max approach, one could have
considered a gradient-descent approach on joint parameters F and g(⋅). However, such an approach presents
several disadvantages over the chosen one. First of all, it does not take into account the structure of the
problem, which is the well-studied SVM optimization problem for a ﬁxed F. Hence, by separating the
optimization over F and over g(⋅), one takes advantage of the SVM optimization framework and any
improvements made to SVM solvers. Furthermore, as stated in Chapelle (), addressing the nonlinear
SVM problem directly in the primal does not lead to improved computational eﬃciency; therefore, no speed
gain should be expected by solving the problem in Equation .directly.

454
Digital Signal Processing with Kernel Methods
where H is the hinge loss function (see Equation .), and g(⋅) implements the classiﬁer
over the ﬁltered signal; that is, g( ̃X′
i,⋅) = ∑N
j=𝜶j𝐲j K( ̃X′
i,⋅, ̃Xj,⋅) + b. Note that, owing
to the strong duality of the SVM problem, J′(⋅) can be expressed in either its primal
or dual form. For solving the optimization problem, authors proposed a CG descent
algorithm along F with a line search method (Flamary et al., ). We will refer to
the method as the kernel ﬁltering SVM (KF-SVM) in the following (see example in
Section ..).
10.3.2
Semi-supervised Learning
Signal classiﬁcation can be a diﬃcult task because very often only a small number of
labeled points are available (Hughes, ). In this setting, SSL naturally appears as a
promising tool for combining labeled and unlabeled information (Chapelle et al. ;
Zhu ). SSL techniques rely on the assumption of consistency, in terms of nearby
points likely having the same label, and points on the same data structure (cluster
or manifold) likely having the same label. This argument is often called the cluster
assumption (Chapelle et al. ; Seeger ). Traditional SSL methods are based on
generative models estimating the conditional density, and they have been widely used
in signal and image applications (Jackson and Landgrebe, ).
Recently, more attention has been paid to discriminative approaches, such as: () the
transductive SVM (TSVM) (Vapnik, ), which maximizes the margin for labeled and
unlabeled samples simultaneously; () graph-based methods, in which each example
spreads its label information to its neighbors until a global steady state is achieved on the
whole dataset (Camps-Valls et al. b; Zhou et al. ); and () the Laplacian SVM
(LapSVM) (Belkin et al., ), which deforms the kernel matrix of a standard SVM
with the relations found by building the graph Laplacian (we already saw a regression
instantiation of this approach in Chapter ). Also, the design of cluster and bagged
kernels (Chapelle et al., ) has shown successful results in general signal processing
problems, and in image processing in particular (Gómez-Chova et al. ; Tuia and
Camps-Valls ); here, the essential idea is to modify the eigenspectrum of the
kernel matrix via clustering the data. Figure .illustrates a typical SSL situation
where the distribution of unlabeled samples helps improve the generalization of the
classiﬁer.
Manifold-Based Regularization Framework
Regularization helps in producing smooth decision functions that avoid overﬁtting to
the training data. Since the work of Tikhonov (), many regularized algorithms have
been proposed to control the capacity of the classiﬁer (Evgeniou et al., ; Schölkopf
and Smola, ). As we have already seen, regularization becomes strictly necessary
when few labeled samples are available compared with the high dimensionality of
the problem. In the last decade, the most paradigmatic case of regularized nonlinear
algorithm is the SVM; as we have seen, maximizing the margin is equivalent to applying
a kind of regularization to model weights (Camps-Valls and Bruzzone, ; Schölkopf
and Smola, ). These regularization methods are especially appropriate when a low
number of samples are available, but are not concerned about the geometry of the
marginal data distribution.

Support Vector Machine and Kernel Classification Algorithms
455
Figure 10.5 Left: classifier obtained using labeled data (grey and black circles denote different
classes). Right: classifier obtained using labeled data plus unlabeled data distribution (black points
denote unlabeled data).
Semi-supervised Regularization Framework
The classical regularization framework has been recently extended to the use of unla-
beled samples (Belkin et al., ) as follows. Notationally, we are given a set of l labeled
sample pairs {(xi, yi)}l
i=and a set of u unlabeled samples {xi}l+u
i=l+. Let us now assume
a general-purpose decision function f . The regularized functional to minimize is
= 
l
l∑
i=
V(xi, yi, f ) + 𝛾L‖f ‖
+ 𝛾M‖f ‖
,
(.)
where V represents a generic cost function of the committed errors on the labeled
samples (next, we will replace V with the hinge loss H), 𝛾L controls the complexity of f in
the associated Hilbert space , and 𝛾M controls its complexity in the intrinsic geometry
of the data distribution. For example, if the probability distribution is supported on a
low-dimensional manifold, ‖f ‖
penalizes f along that manifold . Note that this SSL
framework allows us to develop many diﬀerent algorithms just by playing around with
the loss function V and the regularizers ‖f ‖
and ‖f ‖
.
Laplacian Support Vector Machines
Here, we brieﬂy review the LapSVM as an instantiation of the previous framework.
More details can be found in Belkin et al. (), and its application to image classiﬁ-
cation in Gómez-Chova et al. (). The LapSVM uses the same hinge loss function
as the traditional SVM; that is, V(xi, yi, f ) = max(, −yif (xi)), where f represents the
decision function implemented by the selected classiﬁer and the predicted labels are
y∗= sgn(f (x∗)). Hereafter, unlabeled or test samples are highlighted with ∗.
The decision function used by the LapSVM is f (x∗)
=
⟨w, 𝝓(x∗)⟩+ b, where
𝝓(⋅) is a nonlinear mapping to a higher dimensional Hilbert space , and w and
b deﬁne a linear decision function in that space. The decision function is given by

456
Digital Signal Processing with Kernel Methods
f (x∗) = ∑l+u
i=𝛼iK(xi, x∗) + b. The regularization term ‖f ‖
can be fully expressed in
terms of the corresponding kernel matrix and the expansion coeﬃcients 𝜶:
‖f ‖
= ‖w‖= (𝜱𝜶)T(𝜱𝜶) = 𝜶TK𝜶.
(.)
For manifold regularization, the LapSVM relies on the Laplacian eigenmaps, which try
to map nearby input points/samples to nearby outputs (e.g., corresponding class labels),
thus preserving the neighborhood relations between samples. Therefore, the geometry
of the data is modeled with a graph in which nodes represent both labeled and unlabeled
samples connected by weights Wij (Chapelle et al., ). Regularizing the graph follows
from the smoothness (or manifold) assumption and is intuitively equivalent to penalizing
“rapid changes” of the classiﬁcation function f evaluated between nearby samples in the
graph:
‖f ‖
=

(l + u)
l+u
∑
i,j=
Wij(f (xi) −f (xj))=
f TLf
(l + u),
(.)
where L = D −W is the graph Laplacian, whose entries are sample- and graph-
dependent; D is the diagonal degree matrix of W given by Dii = ∑l+u
j=Wij and Dij = for
i ≠j; the normalizing coeﬃcient ∕(l + u)is the natural scale factor for the empirical
estimate of the Laplace operator (Belkin et al., ); and f = [ f (x), … , f (xl+u)]T =
K𝜶, where we deliberately dropped the bias term b.
Now, by plugging Equations .and .into Equation ., we obtain the
regularized function to be minimized:
min
𝜉i∈ℝl
𝜶∈ℝl+u
{

l
l∑
i=
𝜉i + 𝛾L𝜶TK𝜶+
𝛾M
(l + u)𝜶TKLK𝜶
}
(.)
subject to:
yi
(l+u
∑
j=
𝛼jK(xi, xj) + b
)
≥−𝜉i, i = , … , l
(.)
𝜉i ≥, i = , … , l,
(.)
where 𝜉i are slack variables to deal with committed errors in the labeled samples.
Introducing the restrictions in Equations .and .into the primal functional
in Equation .through Lagrange multipliers, 𝛽i and 𝜂i, and taking derivatives with
respect to b and 𝜉i, we obtain
min
𝜶,𝜷
{

𝜶T
(
𝛾LK +
𝛾M
(l + u)KLK
)
𝜶−𝜶TKJTY𝜷+
l∑
i=
𝛽i
}
,
(.)
where J = [I 𝟎] is an l × (l + u) matrix with I as the l × l identity matrix (the ﬁrst l
points are labeled) and Y = diag(y, … , yl). Taking derivatives again with respect to 𝜶,
we obtain the solution (Belkin et al., )

Support Vector Machine and Kernel Classification Algorithms
457
𝜶=
(
𝛾LI + 
𝛾M
(l + u)LK
)−
JTY𝜷∗.
(.)
Now, substituting again Equation .into the dual functional Equation ., we
obtain the following QP problem to be solved:
𝜷∗= max
𝜷
{ l∑
i=
𝛽i −
𝜷TQ𝜷
}
(.)
subject to ∑l
i=𝛽iyi = and ≤𝛽i ≤∕l, i = , … , l, where
Q = YJK
(
𝛾LI + 
𝛾M
(l + u)LK
)−
JTY.
(.)
Therefore, the basic steps for obtaining the weights 𝛼i are: () build the weight matrix
W and compute the graph Laplacian L = D −W; () compute the kernel matrix K; ()
ﬁx regularization parameters 𝛾L and 𝛾M; and ﬁnally () compute 𝜶using Equation .
after solving the problem Equation ..
Transductive Support Vector Machine
The TSVM was originally proposed by Vapnik (), and aims at choosing a decision
boundary that maximizes the margin on both labeled and unlabeled data. The TSVM
optimizes a loss function similar to Equation ., but 𝛾M‖f ‖
is replaced by a term
related to the distance of unlabeled samples to the margin. The TSVM functional to be
minimized is
= 
l
l∑
i=
V(xi, yi, f ) + 𝛾L‖f ‖
+ 𝜆
l+u
∑
j=l+
L∗(f (x∗
j )),
(.)
where 𝜆is a free parameter that controls the relevance of unlabeled samples, and L∗is
the symmetric hinge loss function:
L∗(f (x∗)) = max(, −|f (x∗)|).
(.)
The optimization of L∗can be seen as “self-learning”; that is, we use the prediction for
x∗for training the mapping for that same example. Minimizing Equation .pushes
away unlabeled samples from the margin, either negative or positive, thus minimizing
the absolute value.
Graph-Based Label Propagation
Let us review a semi-supervised classiﬁer that solely relies on the graph Laplacian (Zhou
and Schölkopf, ). Given a dataset = {x, …, xl, xl+, …, xN} ⊂ℝd, and a label set
= {, … , c}, the ﬁrst l points xi (i ≤l) are labeled as yi ∈and the remaining points
xu (l+≤u ≤N) are unlabeled. The goal in SSL is to predict the labels of the unlabeled
points.

458
Digital Signal Processing with Kernel Methods
Let denote the set of N × c matrices with nonnegative entries. A matrix F =
[f | ⋯|f N]T corresponds to a classiﬁcation on the dataset by labeling each point xi
with a label yi = arg maxj≤c Fij. We can understand F as a vectorial function F ∶→ℝc
which assigns a vector f i to each point xi. Deﬁne an N × c matrix Y ∈with Yij = 
if xi is labeled as yi = j and Yij = otherwise. Note that Y is consistent with the initial
labels assigned according to the decision rule. At each iteration t, the algorithm can be
summarized as follows:
) Calculate the aﬃnity matrix W, for instance using the RBF kernel:
Wij ≡W(xi, xj) = exp(−‖xi −xj‖∕𝜎),
∀i ≠j
(.)
and make Wii = to avoid self-similarity.
) Construct the matrix
S = D−∕WD−∕,
(.)
where D is a diagonal matrix with its (i, i) element equal to the sum of the ith row of
W. Note that this step corresponds to the normalization in feature spaces. Certainly,
if we consider a semi-deﬁnite kernel matrix formed by the dot products of mapped
samples, Wij = ⟨𝜙(xi), 𝜙(xj)⟩, the normalized version is given by
̂W(xi, xj) =
⟨𝝓(xi)
‖𝝓(xi)‖,
𝝓(xj)
‖𝝓(xj)‖
⟩
=
W(xi, xj)
√
W(xi, xi)W(xj, xj)
.
(.)
) Iterate the following spreading function until convergence:
F(t + ) = 𝛼SF(t) + (−𝛼)Y,
(.)
where 𝛼is a parameter in (, ).
These three steps should be iteratively repeated until convergence. Now, if F∗denotes
the limit of the sequence {F(t)}, the predicted labels for each point xi are done using
yi = arg max
j≤c F∗
ij.
(.)
However, it is worth noting here that one can demonstrate (Zhou et al., ) that in
the limit
F∗= lim
t→∞F(t) = (−𝛼)(I −𝛼S)−Y,
(.)
and thus the ﬁnal estimating function F∗can be computed directly without iterations.
This algorithm can be understood intuitively in terms of spreading activation
networks from experimental psychology (Anderson, ; Shrager et al., ), and
explained as random walks on graphs (Zhou and Schölkopf, ). Basically, the
method can be interpreted as a graph G = (V, E) deﬁned on , where the vertex set V

Support Vector Machine and Kernel Classification Algorithms
459
is just and the edges E are weighted by W. In the second step, the weight matrix W of
G is normalized symmetrically, which is necessary for the convergence of the following
iteration. The ﬁrst two steps are exactly the same as in spectral clustering (Ng et al.,
). During the third step, each sample receives the information from its neighbors
(ﬁrst term), and also retains its initial information (second term).
With regard to the free parameter 𝛼, one can see that it speciﬁes the relative amount
of the information from its neighbors and its initial label information. It is worth noting
that self-reinforcement is avoided since the diagonal elements of the aﬃnity matrix are
set to zero in the ﬁrst step. Moreover, the information is spread symmetrically since S
is a symmetric matrix. Finally, the label of each unlabeled point is set to be the class of
which it has received most information during the iterative process.
Relations between Semi-supervised Classifiers
The LapSVM is intimately related to other unsupervised and semi-supervised classi-
ﬁers. This is because the method incorporates both the concepts of kernels and graphs in
the same classiﬁer, thus having connections with transduction, clustering, graph-based,
and label propagation methods. The minimizing functional used in the standard TSVM
considers a diﬀerent regularization parameter for labeled and unlabeled samples, which
is the case in this framework; see Equation .. Also, LapSVM is directly connected
with the soft-margin SVM (𝛾M = ), the hard margin SVM (𝛾L →, 𝛾M = ), the graph-
based regularization method (𝛾L →, 𝛾M > ), the label-propagation regularization
method (𝛾L →, 𝛾M →, 𝛾M ≫𝛾L), and spectral clustering (𝛾M = ). In conclusion, by
optimizing parameters 𝛾L and 𝛾M over a wide enough range, the LapSVM theoretically
outperforms the aforementioned classiﬁers. See Belkin et al. () for deeper details
and theoretical comparison.
Experimental Results for Semi-supervised Classification
This section presents the experimental results of semi-supervised methods in a real
image classiﬁcation problem. We are here concerned about a pixelwise binary classiﬁca-
tion problem to distinguish between the urban and nonurban class using satellite image
pixels as inputs (Gómez-Chova et al., ). Diﬀerent sets of labeled and unlabeled
training samples were used.Training and validation sets consisting of l = labeled
samples (samples per class) were generated, and u = unlabeled (randomly
selected) pixels (samples) from the analyzed images were added to the training set
for the LapSVM and TSVM. We focus on the ill-posed scenario and vary the rate of
both labeled and unlabeled samples independently; that is, {, , , , , }% of
the labeled/unlabeled samples of the training set were used to train the models in each
experiment.
Both linear and RBF kernels were used in the SVM, LapSVM, and TSVM. The graph
Laplacian, L, consisted of l + u nodes connected using k-NNs, and computed the edge
weights Wij using the Euclidean distance among samples. Free parameters 𝛾L and 𝛾M
were varied in steps of one decade in the range {−, }, the number of neighbors k
used to compute the graph Laplacian was varied from three to nine, and the Gaussian
width was tuned in the range 𝜎= {−, … , } for the RBF kernel. The selection of the
best subset of free parameters was done by cross-validation.
Figure .shows the validation results for the analyzed SVM-based classiﬁers. Sev-
eral conclusions can be obtained. First, LapSVM classiﬁers produce better classiﬁcation

460
Digital Signal Processing with Kernel Methods
1
10
100
1
10
100
0.85
0.9
0.95
1
% unlabeled
% labeled
Kappa statistic, κ
1
10
100
0.75
0.8
0.85
0.9
0.95
Kappa statistic, κ
% Labeled samples
RBF-LapSVM
RBF-SVM
LIN-LapSVM
LIN-SVM
RBF-TSVM
88
90
92
94
96
98
Overall accuracy, OA[%]
% Labeled samples
RBF-LapSVM
RBF-SVM
LIN-LapSVM
LIN-SVM
1
10
100
RBF-TSVM
Figure 10.6 Results for the urban classification. First: Overall accuracy OA[%]; Second: 𝜿statistic over
the validation set as a function of the rate of labeled training samples used to build models. Third: 𝜿
statistic surface over the validation set for the best RBF-LapSVM classifier as a function of the rate of
both labeled and unlabeled training samples.
results than SVM in all cases (note that SVM is a particular case of the LapSVM
for 𝛾M = ) for both the linear and the RBF kernels. LapSVM also produces better
classiﬁcation results than TSVM when the number of labeled samples is increased.
Diﬀerences among methods are numerically very similar when a low number of labeled
samples are available. The 𝜅surface for the LapSVM highlights the importance of the
labeled information in this problem.
10.3.3
Multiple Kernel Learning
The idea of combining kernels referring to diﬀerent sources of data was previously
explored in the composite kernels framework (Camps-Valls et al., a) (see Chap-
ter ), and extensively exploited in regression and system identiﬁcation problems in
previous chapters. The idea exploits the summation and scaling properties of kernels;
for example, to deﬁne
K(xi, xj) = 𝜇K(xi, xj) + 𝜇K(xi, xj),
(.)
where one accounts for the relative importance of kernels by tuning the scalings
𝜇,≥. Composite kernels have been shown to work well in practice, as they provide
an intuitive way of trading oﬀthe importance of diﬀerent feature sets, used to compute

Support Vector Machine and Kernel Classification Algorithms
461
MKL-SVM
K(xi
w, xj
w)
K(xi
r, xj
r)
K(xi
c, xj
c)
K(xi
s, xj
s)
K(xi, xj)
μ1
μ2
μ3
μ4
dm
σm
+
Figure 10.7 General idea behind MKLg. Given different sources of registered data, a linear
combination of the different similarity matrices (the kernels) is found.
each kernel, or even to discover the best kernel function with a ﬁxed feature set but
diﬀerent hyperparameters. However, they are not suitable for cases involving more than
a few kernels, since the tuning of a set of 𝜇weights by cross-validation would become
computationally expensive. The MKL framework (Rakotomamonjy et al., ) answers
to this call, as it aims at learning (i.e., via optimization) the optimal linear combination
of 𝜇weights.
Simple Multiple Kernel Learning
The idea of MKL is summarized in Figure .. We have = , … , m, … , M views of the
same data (M blocks of features), which can be spectral bands, groups of bands, image
time sequences, or spatial ﬁlters of diﬀerent scale or nature. For each view we build a
separate kernel indexed by m, each one of the most appropriate type and with the most
appropriate parameters. For example, an MKL system can easily combine a histogram
kernel on bag of words features with a Gaussian kernel dedicated to particular subsets
of features. We aim at ﬁnding the best combination of the form
K(xi, xj) =
M
∑
m=
𝜇mKm(xi, xj),
s.t.
𝜇m ≥
(.)
M
∑
m=
𝜇m = .
MKL aims at optimizing a convex linear combination of kernels (i.e., the 𝜇m weights) at
the same time as it trains the classiﬁer. In the case of the SVM, the optimization of the
𝜇m weights involves gradient descent over the SVM objective value (Rakotomamonjy
et al., ). Globally, we adopt a minimization strategy alternating two steps: ﬁrst, we
solve an SVM with the composite kernel deﬁned by current 𝜇m and then we update 𝜇m
by gradient descent.
If we use the kernel in Equation ., and then plug it into the SVM dual formulation,
we obtain the following problem:
max
𝜶
{ N
∑
i=
𝛼i −

N
∑
i,j=
𝛼i𝛼jyiyj
M
∑
m=
𝜇mKm(xi, xj)
}
(.)

462
Digital Signal Processing with Kernel Methods
constrained to ≤𝛼i ≤C, ∑
i 𝛼iyi = , ∀i = , … , n, ∑
m 𝜇m = , and 𝜇m ≥. The dual
corresponds to a standard SVM in which the kernel is composed of a linear combination
of sub-kernels as in Equation .. One can show (see Rakotomamonjy et al. () for
details) that maximizing the dual problem in Equation .is equivalent to solving the
problem
min
𝝁J(𝝁)
such that
M
∑
m=
𝜇m = ,
𝜇m ≥
(.)
where
J(𝝁) =
⎧
⎪
⎨
⎪⎩
minw,b,𝜉


∑M
m=

𝜇m ‖wm‖+ C ∑N
i=𝜉i
s.t.
yi(∑M
m=⟨wm, 𝝓m(xi)⟩+ b) ≥−𝜉i
𝜉i ≥
(.)
and wm represents the weights of the partial decision function of the subproblem m
with associated kernel mapping 𝝓m(xi). In other words, we have now an objective
to optimize by gradient descent over the vector of possible 𝜇m values. We therefore
alternate the solution of an SVM (providing the current error) and the optimization of
𝜇m. The 𝓁norm constraint on the kernel weights forces dm coeﬃcients to be zero, thus
encouraging sparsity of the solution and, in turn, a natural feature selection.
Experimental Results for Multiple Kernel Learning
Let us illustrate the performance of the simpleMKL in a computer vision application:
the classiﬁcation of ﬂower types from color images. In particular, we are interested
in improving the classiﬁcation by combining diﬀerent features of inherently diﬀerent
nature such as features related to color, shape, and attention. For this example, we used
the Oxford Flowers dataset.Feature extraction involved computing descriptors (bag-
of-words histograms) that account for shape (through SIFT), color, self-similarity (SS),
and color attention (CA). A total of classes with each images yielded a total of 
images. Several images were used to train an SVM with multiple kernel combinations
of features. The results are shown in Figure .for diﬀerent settings: simpleMKL with
all features, only with CA and SS, a linear SVM, a diﬀerent combinations of kernels
using CA and SIFT only. The simpleMKL solution with all kernels was further analyzed
in Figurre .b by plotting the weights for all features, which allows some problem
insight. The results suggest that the most useful information is contained in CA and
SIFT.
10.3.4
Structured-Output Learning
Traditional kernel classiﬁers assume independence among the classiﬁcation outputs.
As a consequence, each misclassiﬁcation receives the same weight in the loss function.
Moreover, the kernel function only takes into account the similarity between input
Available at http://www.robots.ox.ac.uk/∼vgg/data/ﬂowers//index.html.

Support Vector Machine and Kernel Classification Algorithms
463
200
300
400
500
600
700
800
900
1000
72
74
76
78
80
82
84
86
88
# samples
Accuracy rate (%)
0
10
20
30
40
50
60
70
80
90
100
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
(a)
(b)
CA
Color
SIFT
SS
SimpleMKL, all features
SimpleMKL, only CA and SS
Lin SVM (CA+SS)
CA, 1 linear kernel
CA and SIFT, 2 kernels
CA and SIFT, 31 kernels
CA and SIFT, 62 kernels
Figure 10.8 (a) Accuracy rates for different training set sizes. (b) Weights of the four kernels obtained
for the four groups of features.
values and ignores possible relationships between the classes to be predicted. These
assumptions are not consistent for most of real-life DSP problems; for example, in
speech recognition, computer vision, or image processing this is not a good assumption
either. For instance, segmentation of images often deals with a supervised setting in
which a predeﬁned set of classes of interest is deﬁned. In this situation, classes share
strong relationships, both colorimetrically (e.g., a tree is naturally more similar to grass
than to water) and semantically (e.g., a chair is more likely to be next to a table).
Structured-output learning tries to encode those output relations in a classiﬁer. In
what follows, we review the structured SVM (StructSVM), where the output space

464
Digital Signal Processing with Kernel Methods
structure is encoded using a hierarchical tree, and these relations are added to the
model in both the kernel and the loss function. The methodology gives rise to a set
of new tools for structured classiﬁcation, and generalizes the traditional nonstructured
classiﬁcation methods. Several ways can be considered to introduce interdependencies
in the output space. The ﬁrst attempts to develop structured SVM classiﬁers are found in
Joachims et al. (), Tsochantaridis et al. (, ), and Zien et al. (). Altun
et al. () presented an excellent review of the state of the art in structured learning.
Despite the conﬁnement of these methods in the machine-learning community, the ﬁrst
applications of structured-output learning appeared in other DSP-related disciplines,
such as natural language learning (Joachims et al., ), object localization (Blaschko
and Lampert, ), and image segmentation (Tuia et al., b).
The classical SVM method assumes independence between the outputs. This choice
is justiﬁed by the fact that, in principle, no assumptions about the distribution of the
outputs may be done. However, a given sample can be associated with a structured
output that considers a more complex relational information, for instance, through a
hierarchical tree structure showing several classiﬁcation levels. In the literature, this
output information is very often exploited indirectly through the use of cost matrices
and balancing constraints, typically encoded a priori. This approach is not useful when
few (or not representative) labeled samples are available. Besides, these are second-
order output class relations, and they are not learned from data but ﬁxed before
learning. Structured-output learning (SL) (Altun et al., ) formalizes the problem
of output space relations. This SL framework aims at predicting complex objects, such
as trees, sequences, or web queries, where, contrarily to usual machine-learning algo-
rithms, the relationship between the associated outputs plays a role in the prediction.
Notationally, we may deﬁne the aim of SL for classiﬁcation as that of learning a
function h ∶→, where is the space of inputs and is the space of structured
outputs. Using an i.i.d. training sample = {(x, y), (x, y), … , (xN, yN)}, h is the
function minimizing the empirical risk
RΔ
E (h) = 
N
N
∑
i=
Δ(yi, ̂yi),
(.)
where ̂yi is the prediction and y is the correct class assignment. The quality of the
model is evaluated by a loss function: Δ(yi, ̂yi) = if the label is correctly assigned
and Δ(yi, ̂yi) ≥otherwise. Note that the classical /loss is a particular case of this
function returning the loss for each wrong output. Coming back to the h functions,
they are of the form
h(x) ∶arg max
y∈f (x, y),
(.)
where f ∶× →ℝis a joint function between inputs and outputs evaluating
how well a certain prediction matches the observed output. The joint function f can
be represented as f (x, y) = wT𝜳(𝐱, y), where w is a weight vector and the joint input–
output mapping 𝜳relates inputs x and outputs y. In order to account for a structured
output, two main ingredients must be modiﬁed: the mapping 𝜳and the loss function Δ.

Support Vector Machine and Kernel Classification Algorithms
465
Figure 10.9 Toy example of hierarchical structure: the classes of interest
are the three leaves in the tree, which can be semantically grouped in
superclasses.
1
2
3
5
4
6
They will lead to structured-output SVM (SSVM) formulations that are application
dependent.
The Joint Input–Output Mapping
The goal is to encode the structure of the tree into the mapping 𝜱(𝐱), resulting
into a joint input–output mapping 𝜳(x, y). Altun et al. () proposed a mapping
considering tree-structures for taxonomies. Consider a taxonomy as a set of elements
⊇ordered by a partial order ≺, and let 𝛽(y,z) be a measure of similarity respecting
the order ≺. The representation of the outputs 𝜦(y) can be generalized as
𝜆z(y) =
{ 𝛽(y,z)
if y ≺z or y = z

otherwise
(.)
This way, the similarity between two outputs sharing common superclasses will be
higher than between outputs that are distant in the tree structure. Then, we can deﬁne
the joint input–output feature map via a tensor product:
𝜳(x, y) = 𝜱(x) ⊗𝜦(y).
(.)
This formulation introduces a weight vector wz for every node in the hierarchy. The
inner product of the joint feature map decomposes into kernels over input and output
space (using the properties proposed in Schölkopf and Smola ()):
⟨𝜳(x, y), 𝜳(x′, y′)⟩= K⊗((𝜱(x), 𝜦(y)), (𝜱(x′), 𝜦(y′))
= ⟨𝜦(y), 𝜦(y′)⟩K(x, x′).
(.)
In order to illustrate this principle, consider a three-class problem with the structure
shown in Figure .. Classes , , and are the superclasses giving the tree structure.
In the nonstructured version of the algorithm (equivalent to the usual multiclass
classiﬁcation), the 𝜦and 𝜳matrices take the form
𝜦(y) =
⎛
⎜
⎜⎝









⎞
⎟
⎟⎠
𝜳(y) =
⎛
⎜
⎜⎝
x



x



x
⎞
⎟
⎟⎠
(.)

466
Digital Signal Processing with Kernel Methods
Taking into account the structure shown in Figure ., 𝜦and 𝜳become
𝜦(y) =
⎛
⎜
⎜⎝


















⎞
⎟
⎟⎠
𝜳(y) =
⎛
⎜
⎜⎝
x


x

x

x

x

x


x

x
x
⎞
⎟
⎟⎠
.
(.)
The linear dot product between the two ﬁrst classes will result in ⟨x, x′⟩, while between
the classes and (and and ) it is of ⟨x, x′⟩only. Thus, using a joint input–output
mapping, output structure participates in the similarity between samples.
The Loss Function
To deﬁne the loss function, we can modify the classical /loss by exploiting the tree-
based output structure. The proposed tree-based loss assumes a common superclass in
the tree at level l = {, ..., L} as follows:
Δ(y, ̂y) =
{
(l −)∕L
if
yl = ̂yl

otherwise
(.)
Using this loss, errors predicting “far away” classes are penalized more than “close”
errors. A class predicted correctly will receive a loss of zero (l −= ), while the
prediction of a class not sharing any superclass with the true class will receive a loss
of . The loss function presented in Equation .assumes equal distance between
the classes and their superclasses; this can be reﬁned by constructing ad hoc class
distances from the labeled data or by learning interclass distances through, for example,
clustering.
The N-Slack and 1-Slack Structured-Output Support Vector Machine
The modiﬁcation of the loss and the mapping allows us the integration of output-space
similarities into the kernel function. However, to exploit this new source of information,
the whole SVM must be reformulated: it is easy to see that the mapping 𝜳(𝐱, y) cannot
be computed for test points, for which the class membership is obviously unknown. In
order to solve this general problem in structured learning, speciﬁc SVM formulations
must be developed. Several strategies have been proposed for the SSVM (Altun et al.,
; Joachims et al., ; Taskar et al., ; Tsochantaridis et al., , ), but
the formulation of Tsochantaridis et al. () is the most general as it includes the
rest as particular cases. This formulation is usually referred to as the N-slack SSVM
(N-SSVM), since it assigns a diﬀerent slack variable to each of the N training examples.
Speciﬁcally, in the margin-rescaling version of Tsochantaridis et al. (), the position
of the hinge is adapted while the slope is ﬁxed. Each possible output is considered
and the model is constrained iteratively by adding constraints on the (x, y) pairs that
most violate the SVM solution (note that y has become a vector containing all possible
outputs). In other words, a sort of regularization is done, restricting the set of possible
functions h. This way, the formulation becomes

Support Vector Machine and Kernel Classification Algorithms
467
min
w,𝜉
{

‖w‖+ C
N
N
∑
i=
𝜉i
}
(.)
∀i ∶𝜉i ≥
∀̂y ∈, ∀i
⏟⏞⏞⏞⏟⏞⏞⏞⏟
(a)
∶⟨w, 𝜳(𝐱i, yi)⟩
⏟⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏟
(b)
−⟨w, 𝜳(xi, ̂yi)⟩
⏟⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏟
(c)
≥Δ(yi, ̂y)
⏟⏟⏟
(d)
−𝜉i.
(.)
The objective is the conventional regularized risk used in SVMs. The constraints
state that for each incorrect label and for each training example (xi, yi) (a), the score
⟨w, 𝜳(xi, yi)⟩of the correct structure yi (b) must be greater than the score ⟨w, 𝜳(𝐱i, ̂yi)⟩
of all incorrect structures ̂y (c) by a required margin (d). If the margin is violated, the
slack variable 𝜉i becomes nonzero.
This quadratic program involves a very large, possibly inﬁnite, number of linear
inequality constraints, which makes it impossible to be optimized explicitly. Alterna-
tively, the problem is solved by using delayed constraint generation, where only a ﬁnite
and small subset of constraints is taken into account. An eﬃcient method to solve
the problem is the N-slack algorithm originally reformulated by Joachims et al. ():
the -slack SSVM (-SSVM). The model has a unique slack variable 𝜉applied to all the
constraints. The interested reader can ﬁnd the proof for the equivalence with the N-
slack formulation in Joachims et al. (). The n cutting planes of the previous model
are replaced by a single cutting plane for the sum of the hinge losses. In this sense,
Equations .and .can be replaced by
min
w,𝜉
{ 
‖w‖+ C
N 𝜉
}
(.)
∀̂y ∈∶
N
N
∑
i=
(⟨w, 𝜳(xi, yi)⟩−⟨w, 𝜳(xi, ̂yi)⟩) ≥
N
N
∑
i=
Δ(yi, ̂y) −𝜉,
(.)
where 𝜉= ∕N ∑
i 𝜉i. The -SSVM, as proposed in Joachims et al. (), starts with
an empty working set of constraints = ∅, and then the solution is computed over
the current , ﬁnding the most violated constraint (just one for all the training points)
and adding it up to the working set. The algorithm terminates when no constraint is
added in the previous iteration; that is, when all the constraints are fulﬁlled up to a
precision 𝜖. Unlike the N-SSVM, only one constraint is added at each iteration. This
new formulation has ||N constraints, one for each possible combination of labels
[y, … , yN], but only one slack variable 𝜉is shared across all constraints. See Joachims
et al. () for details.
An illustrative example using a linear SVM and the SVMstruct toolbox (see pointers
on the book web page) is given in Listing .. Complete operational source code can
be found in the toolbox ﬁle test_svm_struct_learn.m available from the links
before. The goal here is to learn a predictor function x ↦y that smoothly ﬁts the training
data. The structured SVM formulation of such a prediction function is
̂y(x) = argmaxy∈{−,+} F(x, y; w),
F(x, y; w) = ⟨w, 𝜳(x, y)⟩,
where w is the parameter vector to be learned, 𝜳(x, y) ∈ℝis the joint feature map, and
F(x, y; w) is an auxiliary function usually interpreted as a compatibility score between

468
Digital Signal Processing with Kernel Methods
input x and output y. In this example, a simple 𝜳(x, y) = xy∕is deﬁned, which makes
the structured SVM formulation equivalent to a standard binary SVM (see featureCB
function). Then we deﬁne a loss function Δ(y, ̂y) measuring how well the prediction ̂y
matches the ground truth y. We use the -loss here, which, for our binary labels, writes
Δ(y, ̂y) = (−ŷy)∕(see lossCB function). Finally, one deﬁnes the constraint generation
function which captures the structure of the problem. Generating a constraint for an
input–output pair (x, y) means identifying what is the most incorrect output ̂y that
the current model still deems to be compatible with the input x (see constraintCB
function). Function weights w can be accessed in model.w. The SVM-struct call sets
the C constant of the SVM to (-c 1.0), allows slack rescaling (-o 1), and verbosity
is activated using (-v 1).
function ssvmcode
% Code snippet simplifying the function test_svm_struct_learn.m from
% Andrea Vedaldi available in the SVM-Struct toolbox in
%
http://www.robots.ox.ac.uk/~vedaldi/svmstruct.html#download
-and-install
%
% - Assume have a set of input data X with structured outputs Y
% Initialize parameters
parm.patterns = X;
% cellarray of patterns (inputs).
parm.labels = Y;
% cellarray of labels (outputs).
parm.lossFn = @lossCB;
% loss function callback.
parm.constraintFn
= @constraintCB; % constraint generation callback.
parm.featureFn = @featureCB;
% feature map callback.
parm.dimension = 2;
% feature dimension.
% Run SVM struct
model = svm_struct_learn(' -c 1.0 -o 1 -v 1 ', parm);
% Return model weights
w = model.w ;
% Some SVM struct callbacks
function psi = featureCB(param, x, y)
psi = sparse(y*x/2) ;
end
function delta = lossCB(param, y, ybar)
delta = double(y ~= ybar) ;
end
function yhat = constraintCB(param, model, x, y)
% slack resaling: argmax_y delta(yi, y) (1 + <psi(x,y), w> - <psi(x,yi), ...
w>)
% margin rescaling: argmax_y delta(yi, y) + <psi(x,y), w>
if dot(y*x, model.w) > 1, yhat = y ; else yhat = - y ; end
end
Listing 10.1 Illustration of the structured SVM (ssvmcode.m).
10.3.5
Active Learning
When designing a supervised classiﬁer, the performance of the model depends strongly
on the quality of the labeled information available. This constraint makes the generation
of an appropriate training set a diﬃcult and expensive task requiring extensive manual

Support Vector Machine and Kernel Classification Algorithms
469
human interaction. Therefore, in order to make the models as eﬃcient as possible, the
training set should be kept as small as possible and focused on the samples that really
help to improve the performance of the model. The basic idea is that a classiﬁer trained
on a small set of well-chosen examples can perform as well as a classiﬁer trained on a
larger number of randomly chosen examples (Cohn et al. , ; MacKay ).
AL aims at constructing eﬀective and compact training sets. AL uses the interaction
between the user and the classiﬁer to achieve this goal. The model returns to the user the
samples whose classiﬁcation outcomes are the most uncertain, the user provides their
label, and the samples are reused to improve accuracy. This way, the model is optimized
on well-chosen diﬃcult examples, maximizing its generalization capabilities.
Active Learning Concepts and Definitions
Let X = {(xi, yi)}l
i=be a training set of labeled samples, with xi ∈and yi = {, … , l}.
is the d-dimensional input space ℝd. Let also U = {xi}l+u
i=l+∈, with u ≫l, be the
pool of candidates, a set of unlabeled patterns to be sampled. AL algorithms are iterative
sampling schemes, where a classiﬁcation model is adapted regularly by feeding it with
new labeled samples corresponding to the most beneﬁcial ones to improve the model’s
performance. These samples are usually the ones lying in the areas of uncertainty of the
model, and their inclusion in the training set forces the model to solve such uncertainty
regions of low conﬁdence. For a given iteration t, the algorithm selects, from the pool
Ut, the q candidates that simultaneously maximize the performance and reduce model
uncertainty if they are added to the current training set Xt. Once the batch of samples
St = {xm}q
m=⊂U has been selected, it is labeled by an oracle (usually a human
expert); that is, the labels {ym}q
m=are discovered. Finally, the set St is both added to
the current training set (Xt+= Xt ∪St) and removed from the pool (Ut+= Ut∖St),
and the process is iterated until a stopping criterion is met. Algorithm summarizes the
active selection process. From now on, iteration index t will be omitted for the sake of
clarity.
Algorithm General AL algorithm.
Inputs
- Initial training set Xt = {xi, yi}l
i=(X ∈, t = ).
- Pool of candidates Ut = {xi}l+u
i=l+(U ∈, t = ).
- Number of samples q to add at each iteration (deﬁne set S).
: repeat
:
Train a model with current training set Xt.
:
for each candidate in Ut do
:
Evaluate a user-deﬁned heuristic
:
end for
:
Rank the candidates in Ut according to the score of the heuristic
:
Select the q most interesting samples. St = {xk}q
k=
:
The user assigns a label to selected samples. St = {xk, yk}q
k=
:
Add the batch to the training set Xt+= Xt ∪St.
:
Remove the batch from the pool of candidates Ut+= Ut∖St
:
t = t + 
: until a stopping criterion is met.

470
Digital Signal Processing with Kernel Methods
An AL process requires interaction between the oracle and the model: the former
provides the labeled information and the knowledge about the desired classes, while
the latter provides both its interpretation of the classes distribution and the most
relevant samples that would be needed in the future in order to solve the encoun-
tered discrepancies. This is the key point for the success of an AL algorithm: the
machine needs a heuristic to rank the samples in the pool U. The heuristics are what
diﬀerentiate the algorithms proposed in the next sections, and can be divided into
three main families: () committee-based heuristics; () large-margin-based heuristics;
and () posterior probability based heuristics. Let us now review each one of the
families.
Committee-Based Active Learning
Committee-based AL methods quantify the uncertainty of a sample by considering
a committee of learning models (Freund et al. ; Seung et al. ). Each model of the
committee is based on diﬀerent hypotheses about the classiﬁcation problem, and thus
provides diﬀerent labels for the samples in the pool of candidates. The committee-based
heuristic is based on selecting the samples showing maximal disagreement between the
diﬀerent classiﬁcation models in the committee. Examples of these methods are the one
presented in Freund et al. (), which uses Gibbs sampling to build the committees,
or methods based on bagging and boosting (Abe and Mamitsuka, ). The main
advantage of these methods is that they are applicable to any kind of classiﬁers.
Normalized Entropy Query-by-Bagging
Abe and Mamitsuka () proposed bagging (bootstrap aggregation (Breiman, ))
to build the committee. First, k training sets built drawing with replacement of the
original data are deﬁned. These sets account for a part of the available labeled samples
only. Then, each set is used to train a classiﬁer and to predict the u labels of the
candidates. At the end of the procedure, k predictions are provided for each candidate
xi ∈U. Once we have these k predictions, a heuristic has to be deﬁned in order to
rank them. In Tuia et al. (), the entropy HBAG of the distribution of the predictions
provided by the k classiﬁers for each sample xi in U is used as the heuristic. In Copa
et al. (), this measure is normalized in order to bound it with respect to the number
of classes predicted by the committee and avoid hot spots of the value of uncertainty
in regions where several classes overlap. The normalized entropy query-by-bagging
(nEQB) heuristic is deﬁned as
̂xnEQB = arg maxxi∈U
{HBAG(xi)
log(Ni)
}
,
(.)
where
HBAG(xi) = −
Ni
∑
𝜔=
pBAG(y∗
i = 𝜔|xi) log[pBAG(y∗
i = 𝜔|xi)]
(.)
with
pBAG(y∗
i = 𝜔|xi) =
∑k
m=𝛿(y∗
i,m, 𝜔)
∑k
m=
∑Ni
j=𝛿(y∗
i,m, 𝜔j)

Support Vector Machine and Kernel Classification Algorithms
471
is an empirical measure of entropy, y∗
i is the prediction for the sample xi, and pBAG(y∗
i =
𝜔|xi) is the observed probability to have the class 𝜔predicted using the training set X
by the committee of k models for the sample xi. Ni is the number of classes predicted
for sample xi by the committee, with ≤Ni ≤N. The 𝛿(y∗
i,m, 𝜔) operator returns if the
classiﬁer using the mth bag classiﬁes the sample xi into class 𝜔and otherwise. Entropy
maximization is a natural multiclass heuristic. A candidate for which all the classiﬁers in
the committee agree is associated with null entropy, and its inclusion in the training set
will not provide additional information. On the other hand, a candidate with maximum
disagreement between the classiﬁers results in maximum entropy, and including it in
the training set will provide much information.
Adaptive Maximum Disagreement
When working with high-dimensional datasets it is useful to construct the committee
by splitting the feature space into a number of subsets, or views (Muslea, ). Di and
Crawford () exploit this principle to generate diﬀerent views of a particular image
on the basis of the block-diagonal structure of the covariance matrix. By generating
views corresponding to the diﬀerent blocks, independent classiﬁcations of the same
sample can be generated and an entropy-based heuristic can be used similarly to nEQB.
Given a partition of the d-dimensional input space into V disjoint views accounting
for data subsets xv such that ⋃V
v=xv = x, the adaptive maximum disagreement (AMD)
heuristic selects candidates according to
̂xAMD = arg maxxi∈UHMV(xi),
(.)
where the multiview entropy HMV is deﬁned for each view v as
HMV(xi) = −
Ni
∑
𝜔=
pMV(y∗
i,v = 𝜔|xv
i ) log[pMV(y∗
i,v = 𝜔|xv
i )]
(.)
where
pMV(y∗
i = 𝜔|xv
i ) =
∑V
v=W t−(v, 𝜔)𝛿(y∗
i,v, 𝜔)
∑V
v=
∑Ni
j=W t−(v, 𝜔)
,
where the 𝛿(y∗
i,v, 𝜔) operator returns if the classiﬁer using the view v classiﬁes the
sample yi into class 𝜔and otherwise. W t−is an N × V weighting matrix accounting
for the abilities of discrimination of the views in the diﬀerent classes. At each iteration,
W t−is updated using the true labels of the samples obtained at iteration t −:
W t(v, 𝜔) = W t−(v, 𝜔) + 𝛿(yi,v, 𝜔),
∀i ∈S,
(.)
and its columns are normalized to unitary sum. This matrix weights the conﬁdence of
each view to predict a given class.
Large-Margin-Based Active Learning
This family of methods is speciﬁc to margin-based classiﬁers such as SVM. SVM and
similar methods are naturally good base methods for large-margin AL: the distance
to the separating hyperplane is a straightforward way of estimating the classiﬁer

472
Digital Signal Processing with Kernel Methods
conﬁdence on an unseen sample. Let us ﬁrst consider a binary problem; the distance
of a sample xi from the SVM hyperplane is given by
f (xi) =
N
∑
j=
𝛼jyjK(xj, xi) + b,
(.)
where K(xj, xi) is the kernel, yj are the labels of the support vectors 𝛼j, and b is the bias
term.
This evaluation of the distance is the base ingredient of almost all large-margin
heuristics. Roughly speaking, these heuristics use the intuition that a sample away
from the decision boundary (with a high f (xi)) has a high conﬁdence about its class
assignment and is thus not interesting for future sampling.
In the particular case of SVM, since SVMs rely on a sparse representation of the data,
large-margin-based heuristics aim at ﬁnding the samples in U that are most likely to
receive a nonzero 𝛼i weight if added to X. That is, the points more likely to become
support vectors are the ones lying within the margin of the current model (Tong and
Koller, ). The heuristic based on this idea is called margin sampling (MS) (Campbell
et al., ; Schohn and Cohn, ). A variation on this idea, aiming at minimizing
the risk of selecting points that will not become support vectors, can be found in Zomer
et al. () and Cheng and Shih ().
Margin Sampling
MS is based on SVM geometrical properties; in particular, on the fact that unbounded
support vectors are labeled samples that lie on the margin with a decision function
value of exactly (Boser et al., ; Schölkopf and Smola, ). For example, look
at the pool of candidates of Figure .a referring to a three-class toy problem.
Considering a multiclass OAA setting, the distance to each hyperplane is represented
by Figure .d–f.
The MS heuristic selects the samples from the pool of candidates that minimize
̂xMS = arg minxi∈U{min
𝜔|f (xi, 𝜔)|},
(.)
where f (xi, 𝜔) is the distance of the sample to the hyperplane deﬁned for class 𝜔
in an OAA setting for multiclass problems. The MS heuristic for the toy problem
is reported in Figure .b. The MS heuristic can be found in the literature with
diﬀerent names, such as most ambiguous (Ferecatu and Boujemaa, ), binary-level
uncertainty (Demir et al., ), or SVMSIMPLE (Di and Crawford, ).
Multiclass Level Uncertainty
Demir et al. () extended the idea of MS to multiclass uncertainty; see Vlachos
(). The MCLU considers the diﬀerence between the distance to the margin for the
two most probable classes
̂xMCLU =arg minxi∈U{f (xi)MC}
(.)
where
f (xi)MC = max
𝜔∈N {f (xi, 𝜔)} −max
𝜔∈N∖𝜔+{f (xi, 𝜔)},
(.)

Support Vector Machine and Kernel Classification Algorithms
473
0
1
2
(a)
0.2
0.4
0.6
0.8
1
1.2
1.4
MS, Eq. 10.100
(b)
0.5
1
1.5
2
2.5
3
3.5
MCLU, Eq. 10.102
(c)
0.5
1
1.5
2
2.5
(d)
0.5
1
1.5
2
(e)
0.2
0.4
0.6
0.8
1
1.2
1.4
(f)
│f(x, ω0)│
│f(x, ω1)│
│f(x, ω2)│
Figure 10.10 (a) Large-margin heuristics for a three-class toy example. The color intensity represents
the distance from the hyperplane: (b) MS heuristic; (c) multiclass level uncertainty (MCLU) heuristic;
darker areas are the ones with maximal uncertainty, minimizing Equation 10.100 or 10.102
respectively. (d)–(f) Absolute values of per-class distances.
and 𝜔+ is the class showing maximal conﬁdence. In contrast with the MS heuristic,
which only considers the most uncertain class of the SVM, the MCLU assesses the
uncertainty between the two most likely classes. A high value of this heuristic corre-
sponds to samples assigned with high certainty to the most conﬁdent class, whereas a
low value represents the opposite. Figure .c shows this heuristic compared with MS.
Although very similar, MCLU shows diﬀerences in areas where the three classes mix.
Significance Space Construction
In the signiﬁcance space construction (SSC) heuristic (Pasolli and Melgani, ), the
training samples are used to deﬁne a second classiﬁcation function f (x)SSC where the
training samples with 𝛼j > (support vectors) are classiﬁed against the training samples
with 𝛼j = (non-support vectors). When applied to the pool of candidates, this second
classiﬁer predicts which samples are likely to become support vectors:
̂xSSC = argxi∈U{f (xi)SSC > }.
(.)
The candidates more likely to become support vectors are added to the training set X.

474
Digital Signal Processing with Kernel Methods
Algorithm General diversity-based heuristic (for a single iteration).
Inputs
- Current training set Xt = {xi, yi}l
i=(X ∈).
- Subset of candidates pool minimizing Eq. .or .U∗= {xi} (U∗∈and U∗⊂
Ut).
- Number of samples q to add at each iteration (deﬁne set S).
: Add the sample minimizing Eq. .or .to S.
: repeat
:
Compute the diversity criterion between samples in U∗and in S.
:
Select the sample xD maximizing diversity with current batch.
:
Add xD to current batch S = S ∪xD.
:
Remove xD to current list of cadidates U∗= U∗⧵xD.
: until batch S contains q elements.
: The user labels the selected samples S = {xk, yk}q
k=.
: Add batch to the training set Xt+= Xt ∪S.
: Remove batch from the pool of candidates Ut+= Ut∖S.
Diversity Criteria
In addition to have a heuristic to rank and select samples among the pool of candi-
dates, before adding them to the training set it is also important to ensure they are
diverse (Brinker, ). A diversity criterion allows one to reject candidates that rank
high according to the heuristic but are redundant between each other. Algorithm 
shows the general diversity heuristic.
In Ferecatu and Boujemaa () the margin sampling heuristic is constrained with
a measure of the angle between candidates in the feature space. This heuristic, named
most ambiguous and orthogonal (MAO) is as follows: () samples are selected by MS;
() from this subset, the algorithm iteratively selects the samples minimizing the highest
values between the candidates and the samples already included in the training set. That
is, each new sample in selected using
̂xMAO = arg minxi∈UMS{max
xj∈S K(xi, xj)}.
(.)
Demir et al. () applied the MAO criterion among a subset of U maximizing the
MCLU criterion (instead of MS), deﬁning the MCLU angle-based diversity (MCLU-
ABD) heuristic. But more importantly, the authors generalize the MAO heuristic to
any type of kernels by including normalization in the feature space:
̂xMCLU-ABD = arg minxi∈UMCLU
⎧
⎪
⎨
⎪⎩
𝜆f (xi)MC + (−𝜆) max
xj∈S
K(xi, xj)
√
K(xi, xi)K(xj, xj)
⎫
⎪
⎬
⎪⎭
(.)
where f (xi)MC is the uncertainty function deﬁned by Equation ..
Another diversity criterion is the one proposed in Tuia et al. (), where ﬁrst the MS
heuristic is used to build a subset, UMS, and then the diversity is obtained by choosing

Support Vector Machine and Kernel Classification Algorithms
475
samples near to diﬀerent support vectors. This approach improved the diversiﬁcation
of the MS heuristic selecting samples as a function of the geometrical distribution of
the support vectors. However, this approach does not guarantee diversity among the
selected samples, because a pair of samples associated with diﬀerent support vectors can
be in fact close to each other. The closest support vector diversity heuristic is deﬁned as
̂xcSV = arg minxi∈UMS{|f (xi, 𝜔)| | cSVi ∉cSV𝜃}
(.)
where 𝜃= [, … , q −] are the indices of the already selected candidates and cSV is the
set of selected closest support vectors.
Another way of ensuring diversity is using clustering in the feature space. Kernel k-
means (Dhillon et al. ; Girolami a; Shawe-Taylor and Cristianini ) was
used by Demir et al. () to cluster the samples selected by MCLU and select diverse
batches. First, a set of samples selected using the MCLU criterion, UMCLU, is obtained.
Then this set is partitioned into q clusters using kernel k-means. Finally, the MCLU
enhanced cluster-based diversity (MCLU-ECBD) selects a single sample per cluster
minimizing the following function:
̂xMCLU-ECBD = arg minxi∈cm{f (xi)MC},
m = [, … , q], xi ∈UMCLU
(.)
where cm is one among the q clusters.
A binary hierarchical variant of this method is proposed in Volpi et al. () that aims
to exclude the less informative samples from the selected batch, which are the ones more
likely to become bounded support vectors. Cluster kernel k-means is used to reduce the
redundancy among samples in the feature space, whereas building clusters excluding
bounded support vectors maximizes the informativeness of each added sample. At
each iteration, the informative hierarchical margin cluster sampling (hMCS-i) builds
a dataset composed of a subset of samples selected using the MCLU criterion, UMCLU,
and the bounded support vectors obtained at the previous iteration. Then this subset
is iteratively partitioned using kernel k-means in a binary way. In each partition the
biggest cluster is always selected. The partition process continues until q clusters with
no bounded support vectors are obtained. Then candidates are selected among these q
clusters according to
̂xhMCS-i = arg minxi∈cm{f (xi)MC},
m = [, … , q|nbSV
cm
= ], xi ∈UMCLU
(.)
Posterior-Probability-Based Active Learning
The third class of AL methods uses the estimation of posterior probabilities of class
membership (i.e., p(y|x)) to rank the candidates. The main drawback of this method is
that it can only be applied to classiﬁers providing probabilistic outputs.
KL-max
The idea of these methods is to select samples whose inclusion in the training set would
maximize the changes in the posterior distribution. In (Roy and McCallum, ), the
proposed heuristic maximizes the KL divergence between the distributions before and

476
Digital Signal Processing with Kernel Methods
after adding the candidate. Each candidate is removed from U and included in the
training set using the label maximizing its posterior probability. The KL divergence
is then computed between the posterior distributions of the models with and without
the candidate. After computing this measure for all candidates, those maximizing the
criterion KL-max are selected:
̂xKL-max = arg maxxi∈U
{
∑
𝜔∈N

(u −)KL(p+(𝜔|x)||p(𝜔|x))p(y∗
i = 𝜔|xi)
}
(.)
where
KL(p+(𝜔|x)||p(𝜔|x)) =
∑
xj∈U⧵xi
p+(y∗
j = 𝜔|xj) log
p+(y∗
j = 𝜔|xj)
p(y∗
j = 𝜔|xj)
(.)
and p+(𝜔|x) is the posterior distribution for class 𝜔and sample x, estimated using
the increased training set X+ = [X, (xi, y∗
i )], with y∗
i being the class maximizing the
posterior. Jun and Ghosh () extended this approach proposing to use boosting
to weight samples that were previously selected but were no longer relevant for the
current classiﬁer. These heuristics are only feasible when using classiﬁers with low
computational cost, since in each iteration u + models have to be trained.
Breaking Ties
This strategy, similar to nEQB presented in Section .., consists of assessing the
conditional probability of predicting a given label p(y∗
i = 𝜔|xi) for each candidate xi ∈
U. This is estimated as the predictions for all the candidates y∗
i = arg max𝜔∈Nf (xi, 𝜔).
Some classiﬁcation algorithms already provide these estimations, such as probabilistic
NNs or ML classiﬁers. For SVM, Platt’s probability estimations can be used (Platt,
b). Once the posterior probabilities are obtained, it is straightforward to assess the
uncertainty of the class membership for each candidate. The breaking ties (BT) heuristic
chooses candidates showing a near-uniform probability of belonging to each class; that
is, p(y∗
i = 𝜔|xi) = ∕N, ∀𝜔∈N.
The BT heuristic for a binary problem is based on the smallest diﬀerence of the
posterior probabilities for each sample (Luo et al., ). In a multiclass setting,
and independently of the number of classes, the diﬀerence between the two high-
est probabilities reveals how a sample is considered by the classiﬁer. If the two highest
probabilities for a given sample are similar, then the classiﬁer’s conﬁdence on assigning
that sample to one of the classes is low. The BT heuristic is formulated as
̂xBT = arg minxi∈U
{
max
𝜔∈N {p(y∗
i = 𝜔|x)} −max
𝜔∈N∖𝜔+{p(y∗
i = 𝜔|x)}
}
(.)
where 𝜔+ is the class showing maximal probability. Comparing Equation .with
Equation ., the relationship between BT and the MCLU heuristic presented in
Section ..is clear.

Support Vector Machine and Kernel Classification Algorithms
477
10.4
Large-Scale Support Vector Machines
This section pays attention to the important caveat of SVM and kernel classiﬁers:
the huge computational cost involved when many input data are available. We will
summarize then the main algorithmic approximations existing nowadays in the liter-
ature to alleviate such computationally demanding cost. We will pay special attention
to approaches that either break a large QP problem into a series of manageable QP
subproblems or approximate the kernel with random feature projections. For the former
we will study the SMO (Platt, a) and other related approaches, while for the latter
we will review the method of random Fourier features (Rahimi and Recht, ). We will
ﬁnish the +section by paying attention to standard techniques for parallelizing SVMs.
10.4.1
Large-Scale Support Vector Machine Implementations
Some alternatives to solve the QP problem implied in SVMs exist. Note that, in the case
of the SVM, the QP problem is directly expressed as a function of the training kernel
matrix K, which contains the similarity (distance) among all training samples. Thus,
solving this problem requires storing the matrix and making operations with it. One
of the most eﬀective interior point methods (IPMs) for solving the QP problem with
linear constraints is the primal–dual IPM (Mehrotra, ), which essentially tries to
remove inequality constraints using a bounding function, and then exploit the iterative
Newton’s method to solve the KKT conditions related to the Hessian matrix Q. This
is very computationally demanding, as it grows as (N) in time and (N) in space,
where N is the number of training samples.
Some alternative algorithms have appeared to solve the problem in reasonable time
and amount of memory for several thousands of training samples. A powerful approach
to scale up SVM training is by using decomposition methods (Osuna et al., ),
which break a large QP problem into a series of manageable QP subproblems. The
most popular decomposition method is the SMO (Platt, a), in which smaller QP
problems are solved analytically, which avoids using a time-consuming numerical QP
optimization as an inner loop. The amount of memory required for SMO is linear in the
training set size, which allows SMO to handle very large training sets. Without kernel
caching, SMO scales somewhere between linear and quadratic in the training set size
for various test problems, while a standard projected CG chunking algorithm scales
somewhere between linear and cubic in the training set size. SMO’s computation time
is dominated by SVM evaluation; hence, SMO is fastest for linear SVMs and sparse
data sets. Despite these advantages, the SMO algorithm still requires a large amount of
computation time for solving large-scale problems, and also the kernel storage, which
is of the size of the square of training points.
In recent years, other scale-up strategies have been proposed. The SimpleSVM algo-
rithm (Vishwanathan et al., ) makes use of a greedy working set selection strategy
to identify the most relevant samples (support vectors) for incremental updating of
the kernel submatrix. Bordes et al. () proposed an online SVM learning together
with active example selection. In Sonnenburg et al. (b), special data structures for
fast computations of the kernel in their chunking algorithm are exploited. In addition,
instead of maximizing the dual problem as is usually done, Chapelle () proposed
directly minimizing the primal problem. Some other powerful algorithms for solving

478
Digital Signal Processing with Kernel Methods
SVM have recently appeared, such as the Core SVM or the ball vector machine.
However, large-scale problems involving N>samples require more eﬃciency that
is still needed. However, the reader should consult Bottou et al. () for a more up-
to-date treatment of large-scale SVMs.
Since these initial approaches, many eﬀorts have been made to deliver large-scale ver-
sions of kernel machines able to work with several thousands of examples (Bottou et al.,
). They typically resort to reducing the dimensionality of the problem by decom-
posing the kernel matrix using a subset of basis: for instance, using Nyström eigende-
compositions (Sun et al., ), sparse and low-rank approximations (Arenas-García
et al., ; Fine et al., ), or smart sample selection (Bordes et al., ). However,
there is no clear evidence that these approaches work in general, given that they are
mere approximations to the kernel computed with all (possibly millions of) samples.
10.4.2
Random Fourier Features
Let us now review a diﬀerent approximation to deal with the kernel parameterization
and optimization simultaneously. Selecting and optimizing a kernel function is very
challenging even with moderate amounts of data. An alternative pathway to that of
optimization has recently captured much attention in machine learning: rather than
optimization, one trusts to randomization. While it may appear odd at ﬁrst glance,
the approach has surprisingly yielded competitive results in recent years, being able to
exploit many samples at a fraction of the computational cost. Besides its practical con-
venience, the approximation of the kernel with random basis is theoretically consistent
as well. The seminal work of Rahimi and Recht () presented the randomization
framework, which relies on an approximation to the empirical kernel mapping of
the form
K(x, 𝐲) = ⟨𝝓(x), 𝝓(y)⟩≈z(x)Tz(y),
where the implicit mapping 𝝓(⋅) is replaced with an explicit (low-dimensional) feature
mapping z(⋅) of dimension D.
Consequently, one can simply transform the input with z, and then apply fast linear
learning methods to approximate the corresponding nonlinear kernel machine. This
approach not only provides extremely fast learning algorithms, but also excellent
performance in the test phase: for a given test point x, instead of f (x) = ∑N
i=𝛼iK(xi, x),
which requires (Nd) operations, one simply does a linear projection f (x) = wTz, which
requires (D+d) operations. The question now is how to construct eﬃcient and sensible
z mappings. Rahimi and Recht () also introduced a particular technique to do so-
called random kitchen sinks (RKSs).
Bochner’s theorem (Reed and Simon, ) states that a continuous kernel K(x, x′) =
K(x −x′) on ℝd is positive deﬁnite if and only if K is the FT of a nonnegative measure.
If a shift-invariant kernel K is properly scaled, its FT p(w) is a proper probability
distribution. This property is used to approximate kernel functions and matrices with
linear projections on a number of D random features as follows:
K(x, x′) = ∫ℝd p(w) e−jwT(x−x′) dw ≈
∑D
i=

D e−jwT
i x ejwT
i x′,

Support Vector Machine and Kernel Classification Algorithms
479
RBF, ideal
RKS, D = 1
RKS, D = 5
RKS, D = 1000
Figure 10.11 Illustration of the effect of randomly sampling D bases from the Fourier domain on the
kernel matrix. With sufficiently large D, the kernel matrix generated by an RKS approximates that of
the RBF kernel, at a fraction of the time.
Table 10.4 Computational and memory costs for different approximate kernel methods in problems
with d dimensions, D features, N samples.
Method
Train time
Test time
Train memory
Test memory
Naive (Shawe-Taylor and Cristianini, )
(Nd)
(Nd)
(Nd)
(Nd)
Low rank (Fine et al., )
(NDd)
(Dd)
(Dd)
(Dd)
RKS (Rahimi and Recht, )
(NDd)
(Dd)
(Dd)
(Dd)
where p(w) is set to be the inverse FT of K, and wi ∈ℝd is randomly sampled from a
data-independent distribution p(w) (Rahimi and Recht, ). Note that we can deﬁne
a D-dimensional randomized feature map z(x) ∶ℝd →ℝD, which can be explicitly
constructed as z(x) ∶= [exp(jwT
x), … , exp(jwT
Dx)]T. Other deﬁnitions are possible;
one could, for instance, expand the exponentials in pairs [cos(wT
i x), sin(wT
i x)], but this
increases the mapped data dimensionality to ℝD, while approximating exponentials by
[cos(wT
i x+bi)], where bi ∼(, π), is more eﬃcient (still mapping to ℝD) but has been
revealed as less accurate, as noted in Sutherland and Schneider ().
In matrix notation, given N data points, the kernel matrix K
∈ℝN×N can be
approximated with the explicitly mapped data, Z = [z⋯zN]T ∈ℝN×D, and will be
denoted as ̂K ≈ZZT. This property can be used to approximate any shift-invariant
kernel. For instance, the familiar SE Gaussian kernel K(x, x′) = exp(−‖x −x′‖∕(𝜎))
can be approximated using wi ∼(, 𝜎−I), ≤i ≤D. An illustrative example of how
an RKS approximates K with random bases is given in Figure .. The method is very
eﬃcient in both speed and memory requirements, as shown in Table ..
We can use such recipe to derive an approximated version of the LS kernel regres-
sion/classiﬁcation, by simply replacing the mapped feature vectors (now explicitly) in
the canonical normal equations. A simple code snippet is given in Listing .for
illustration purposes.

480
Digital Signal Processing with Kernel Methods
% Random Kitchen Sinks (RKS) for Least squares kernel regression
%
% We are given the following training-test data matrices
% Xtrain: N x d, Xtest: M x d
% Ytrain: N x 1, Ytest: M x 1
% Training
D
= 100;
% number of random features
lambda = 1e-3;
% regularization parameter
W
= randn(D,d);
% random projection matrix
Z
= exp(1i * Xtrain * W);
% explicitly projected features, N x D
alpha
= (Z'*Z + lambda*eye(D)) \ (Z'*Ytrain);
% primal weights, D x 1
% Testing
Ztest = exp(1i * Xtest * W);
% M x D
Ypred = real(Ztest * alpha);
% M x 1
Listing 10.2 Example of a kernel LS regression approximated with random features (rks.m).
The RKS algorithm can actually exploit other approximating functions besides Fourier
expansions. Randomly sampling distribution functions impacts the deﬁnition of the
corresponding RKHS: sampling the Fourier bases with z𝜔(x) =
√
cos(𝜔T
ox+b) actually
leads to the Gaussian RBF kernel K(x, y) = exp(−‖x−y‖∕(𝜎)), while a random stump
(i.e., sigmoid-shaped functions) sampling deﬁned by z𝜔(x) = sgn(x −𝜔) leads to the
kernel K(x, y) = −(∕a)‖x −y‖.
10.4.3
Parallel Support Vector Machine
Thanks to the increase in computational capabilities of current CPUs and GPUs, as
well as large facilities and commodity clusters, it is nowadays possible to train SVMs
on large-scale problems in parallel. There are several SVM implementations. Let us
just mention two of them. Cao et al. () presented a parallel implementation of
the SMO. The parallel SMO was developed using message passing interface by ﬁrst
partitioning the entire training set into smaller subsets and then simultaneously running
multiple CPU processors to deal with each of the partitioned datasets. Experiments
showed a great speedup. Nevertheless, the algorithm had to work with the samples, not
with the kernel matrix, which may oﬀer interesting opportunities of multimodal data
assimilation.
An interesting alternative to the parallelization of the SMO has been presented
by Chang et al. (). This parallel SVM algorithm (PSVM) is able to reduce the
memory use and to parallelize both data loading and computation, and at the same
time works directly with the precomputed kernel matrix. Given N training instances
each with d dimensions, the PSVM ﬁrst loads the training data in a round-robin fashion
onto m machines. Next, PSVM performs a parallel row-based incomplete Cholesky
factorization (ICF) on the loaded data (Golub and Van Loan, ). At the end of parallel
ICF, each machine stores only a fraction of the factorized matrix. PSVM then performs
parallel IPM to solve the QP optimization problem, while the computation and the
memory demands are improved with respect to other decomposition-based algorithms,
such as the SVMLight (Joachims, a), libSVM (Chang and Lin, ), SMO (Platt,
a), or SimpleSVM (Vishwanathan et al., ). Let us summarize the main aspects
of this PSVM algorithm.

Support Vector Machine and Kernel Classification Algorithms
481
Parameters for Parallel Algorithm Design
There are multiple, sometimes conﬂicting, design goals to be considered when devel-
oping a parallel algorithm. One could maximize static runtime characteristics such as
speedup (sequential execution runtime divided by parallel execution time) or eﬃciency
(speedup divided by the number of processors employed). Another goal could be
maximizing scalability – the ability to maintain constant eﬃciency when both the
problem size and the number of processors are increased; that is, in an attempt to
increase the longevity of a solution in the face of continuously improving computation
power. Yet another dimension could be to maximize productivity (the usefulness of a
solution as a function of time divided by its costs) or in particular development time
productivity, which is deﬁned as speedup divided by relative eﬀort (the eﬀort needed to
develop a serial version divided by the eﬀort needed to develop a parallel solution) (Funk
et al., ).
Parallel Support Vector Machine Via Cholesky Factorization
The PSVM method originally introduced by Chang et al. () is aimed at reducing
memory use through performing a row-based, approximate matrix factorization. The
key step of PSVM is the parallel ICF. Traditional column-based ICF (Bach and Jor-
dan, a; Fine et al., ) can reduce computational cost, but the initial memory
requirement is not eﬃcient for very large datasets. Alternatively, the PSVM performs
parallel row-based ICF as its initial step, which loads training instances onto parallel
machines and performs factorization simultaneously on these machines. Once parallel
ICF has loaded N training data distributedly on m machines, and reduced the size
of the kernel matrix through factorization, IPM can be solved on parallel machines
simultaneously.
Efficient Implementation of the Algorithm
Notationally, ICF approximates the Hessian matrix Q (of size N ×N) by a smaller matrix
H (of size p×N); that is, Q = HHT. ICF, together with the exploitation of the Sherman–
Morrison–Woodbury formula,can greatly reduce the computational complexity in
solving an N × N linear system. The work of Fine et al. () provides a theoretical
analysis of how ICF inﬂuences the optimization problem in Equation ..
The PSVM in Chang et al. () iterates until the approximation of Q by HkHT
k
(measured by Tr(Q −HkHT
k)) is satisfactory, or the predeﬁned maximum iterations
(or, say, the desired rank of the ICF matrix) p is reached. As suggested in Golub and
Van Loan (), a parallelized ICF algorithm can be obtained by constraining the
parallelized Cholesky factorization algorithm, iterating at most p times. However, in the
proposed algorithm (Golub and Van Loan, ), matrix H is distributed by columns in
a round-robin way on m machines (hence it is called “column-based parallelized ICF”).
Such a column-based approach is optimal for the single-machine setting, but cannot
gain full beneﬁt from parallelization because of (i) the large memory requirement (as
each machine must be able to store a local copy of the training data) and (ii) the
limited parallelizable computation, since the summation of local inner product result
and the vector update must be performed on one single machine. Therefore, rather
The Sherman–Morrison–Woodbury formula from linear algebra states that (C + AB)−= C−−C−A
(I + BC)−A)−BC−, where C is an invertible N × N matrix, A ∈ℝN×m and B ∈ℝm×N.

482
Digital Signal Processing with Kernel Methods
than performing column-wise, a row-based approach starts by initializing variables and
loading training data onto m machines in a round-robin fashion. The algorithm then
performs the ICF main loop until the termination criteria are satisﬁed (e.g., the rank of
matrix H reaches p).
At the end of the algorithm, H is stored distributedly on m machines, ready for parallel
IPM (Chang et al., ). Parallel ICF enjoys three advantages: (i) parallel memory
use ((Np∕m)), (ii) parallel computation ((pN∕m)), and (iii) low communication
overhead ((plog(m))). Particularly on the communication overhead, its fraction of
the entire computation time shrinks as the problem size grows. We will verify this in
the experimental section. This pattern permits a larger problem to be solved on more
machines to take advantage of parallel memory use and computation. More details can
be found in Chang et al. (). The method loads only essential data to each machine
to perform parallel computation. Let N denote the number of training instances, p the
reduced matrix dimension after factorization (p is signiﬁcantly smaller than N), and
m the number of machines. PSVM reduces the memory requirement from (N) to
(Np∕m), and improves computation time to (Np∕m).
Example of Parallelized Support Vector Machine Classification
We illustrate the capabilities of the PSVM for classiﬁcation of large-scale real data
problems. In particular, here we are concerned with the classiﬁcation of multispectral
images in a large facility.In particular, we build a PSVM to detect cloud presence in
multispectral images.
Experiments were carried out using two MERIS Level b (Lb) images taken over
Barrax (Spain), which are part of the data acquired in the framework of the SPARC
and ESA campaigns (ESA-SPARC Project). These images were acquired on
July of two consecutive years (and ). For our experiments, we used as
input spectral bands (MERIS bands and were removed since they are aﬀected
by atmospheric absorptions) and six physically inspired features extracted from MERIS
bands in a previous study (Gómez-Chova et al., ): cloud brightness and whiteness
in the visible and near-infrared spectral ranges, along with atmospheric oxygen and
water vapor absorption features. Cloud presence is considered as the target class. Cloud
screening is specially well suited to semi-supervised approaches since cloud features
change to a great extent depending on the cloud type, thickness, transparency, height,
and background (being extremely diﬃcult to obtain a representative training set); and
cloud screening must be carried out before atmospheric correction (being the input data
aﬀected by the atmospheric conditions). In the selected image, the presence of snow can
be easily confused with clouds, which increases the diﬃculty of the problem.
Scalability experiments were run with three large datasets obtained from randomly
subsampling the MERIS image ({, , } samples). Note that, for the case N = ,
MareNostrum comprises JScompute nodes (blades) and pservers. Every blade has two
processors at .GHz running Linux operating system with GB of memory RAM and GB local disk
storage. All the servers provide a total of TB of disk storage accessible from every blade through GPFS
(Global Parallel File System). The networks that interconnect the MareNostrum are: () Myrinet Network:
high-bandwidth network used by parallel applications communications; and () Gigabit Network: Ethernet
network used by the blades to mount remotely their root ﬁle system from the servers and the network over
which GPFS works. More information is available at http://www.bsc.es/.

Support Vector Machine and Kernel Classification Algorithms
483
a single machine cannot store the factorized matrix H in local memory, so we show
results for the cases m>. The running time consists of three parts: computation
(“Comp”), communication (“Comm”), and synchronization (“Sync”). Figure .shows
the scores for diﬀerent numbers of machines and the three data sizes. The PSVM
provides excellent performance and achieves a steady state in computation cost for m
>. For more than machines, the performance deteriorates. In this case, especially
important for m>, smaller problems per machine have to be solved, which results in
an overall increase of support vectors, and thus both training and test processing times
increase.
Figure .shows the speedup curves for diﬀerent data sizes, along with the linear
speedup line. This parallel version of the SVM cannot achieve linear speedup beyond a
limit, which depends on the number of machines and the size of the dataset. This result
has already been reported in Chang et al. (). The fact that the problem is split in
many machines also increases the time needed for communication and synchronization
overheads. Communication time is incurred when message passing takes place between
machines. Synchronization overhead is incurred when the master machine waits for
task completion on the slowest machine. Note that, in homogeneous schemes, there are
not obviously “slower” machines but harder tasks to be performed, as in our case. The
computation speedup becomes sublinear when adding machines beyond a threshold
(around machines). This is due to the fact that the algorithm computes the inverse of a
matrix whose size depends on the number of machines m; but fortunately, the larger the
dataset is, the smaller is this (unparallelizable) time fraction. Therefore, more machines
(larger m) can be employed for larger datasets (larger N) to gain speedup. Finally, we
should note that the highest impact on speedup is the communication overhead, rather
than the synchronization.
10.4.4
Outlook
Blink! And the Moore’s Law breaks…
We are facing a new era in which data are ﬂooding our processing capabilities.
Kernel machines are hampered by their swallow architecture to cope with such big data
problems. In this section we have reviewed the main approximations to computational
eﬃciency using kernel machines. We have seen that there are algebraic solutions to
decompose QP problems into smaller problems, we also have methods to approach
the problem of large eigendecomposition and matrix inversion problems, many tricks
to select the most relevant (informative) points entering the kernel machine, and to
approximate the kernel expansion into a subset of basis functions. We have also seen
that such basis can even be random features drawn from our favorite basis in signal
processing, the Fourier basis. And we should not forget about the important issues of
adaptive, online, and recursive algorithms reviewed in Chapter . In a nutshell, all these
are very useful ways to solve these small- to medium- size problems. And then, we have
reviewed recent approximations to resort not only to software eﬃciency but also to
hardware eﬃciency; there are also parallelized versions of standard kernel machines like
SVMs, and smart strategies to decompose a big problem into smaller ones following the
divide-and-conquer strategy.
But even with all these advances, kernel machines are not still prepared to tackle
really big data problems. However, there is something more fundamental than the

Time [sec]
1
2
3
4
5
10 20 30 40 50 100200500
0
50
100
150
200
250
#Machines
Time [sec]
10
20
30
40
50
100
200
500
0
1000
2000
3000
4000
5000
6000
#Machines
Time [sec]
Sync
Calc
Comm
Sync
Calc
Comm
1
2
3
4
5
10 20 30 40 50 100200500
0
50
100
150
200
250
300
#Machines
Time [sec]
10
20
30
40
50
100
200
500
0
20
40
60
80
100
120
140
160
180
200
#Machines
Time [sec]
Training
1
2
3
4
5
10 20 30 40 50 100200500
0
1
2
3
4
5
6
7
8
9
10
#Machines
Time [sec]
104 Samples
105 Samples
106 Samples
1
2
3
4
5
10 20 30 40 50 100200500
0
0.5
1
1.5
2
2.5
3
#Machines
104 Samples
105 Samples
106 Samples
Sync
Calc
Comm
Sync
Calc
Comm
Sync
Calc
Comm
Sync
Calc
Comm
Test
Figure 10.12 Cost a function of the number of training and test samples (104, 105, 106) and detailed in “synchronization,”“communication,”and “computation”
for a number of processors between 1 and 500 used.

Support Vector Machine and Kernel Classification Algorithms
485
0
100
200
300
400
500
0
100
200
300
400
500
104 samples
105 samples
106 samples
0
50
100
0
50
100
150
100
0
15
# Machines
Speedup
Figure 10.13 Speedup for different data sizes using the PSVM.
availability of such huge amounts of data ready to be exploited. One should question
if all the acquired samples are useful or worth using. The answer is, of course, “No, it
depends on the problem.” We have seen how SVMs discard all samples not involved
in the deﬁnition of the margin, while GPR assigns diﬀerent relevance and predictive
variance to diﬀerent samples. Here, it seems we enter into contradiction with a current
line of thought in machine learning by which bigger datasets are a blessing rather than
a curse, and one should actually trust the unreasonable eﬀectiveness of data rather than
trying to develop a system that mimics the natural physical rules (Halevy et al., ).
But, even in big data processing, one often discards uninformative samples. Eﬃciency
is not only a question of computationally capacity but also of smart strategy to learn the
problem with as few data as possible: just like the human brain, we are fed with tons of
sensory data that is massively and continuously screened and (a high portion of it) is
discarded.
10.5
Tutorials and Application Examples
This section includes some examples and MATLAB code to reproduce applications
in the previous section as well as to illustrate relevant concepts in kernel-based
classiﬁcation.
10.5.1
Examples of Support Vector Machine Classification
Empirical and Structural Risks
The following example illustrates the behavior of the empirical and structural risks
as a function of the free parameter C. The synthetic data used in this example are

486
Digital Signal Processing with Kernel Methods
embedded in an -dimension space. In this space, data are randomly generated from
eight Gaussians whose centers form a D cube inside this space. The lengths of the cube
edges are 𝜎and the Gaussians are spherically symmetric with a standard deviation
equal to 𝜎. The data generated out of these Gaussians describing one face of the
cube have been labeled as +, and the rest as −. The optimal separating surface is
a hyperplane containing the origin, with an associate vector proportional to uT =
[, … , ]. The maximum complexity of a linear classiﬁer is then the one that can be
achieved in a space of dimensions, where the complexity needed for an optimal
classiﬁcation is the one of a machine embedded in a space of three dimensions. Thus, a
linear classiﬁer inside this space is prone to overﬁt.
In order to illustrate this, and how an SVM works to reduce the overﬁtting, an SVM
classiﬁer is trained with samples and then tested with another samples. The
training and test errors have been averaged times for each of the values of C
ranging from .to , with a logarithmic spacing. Figure .shows the behavior of
the errors. The training error, which is an estimation of the empirical risk, decreases
as C increases. The eﬀect of this parameter is to establish a trade-oﬀbetween the
minimization of the complexity and the minimization of the training error. Then, as
C increases, the machine gives more importance to the training errors, thus tending
to overﬁt. This can be seen by observing that, as the training error decreases, the test
error (which is an estimation of the actual risk) increases. On the other side, when
C decreases, the test error goes to lower values up to a minimum that is around
(−−) × −. At this point, the value of C gives the optimal complexity to the machine.
Figure .also shows the diﬀerence between both errors, which is an estimate of the
structural risk. The example can be run using Listing .. The implementation of the
SVM used in this example is the well-known LIBSVM (Chang and Lin, ).
10–1
100
101
C
0.005
0.01
0.015
0.02
Re
Rt
Rs
Figure 10.14 The decreasing line corresponds to the training error average (estimation of the
empirical risk) over 1000 realizations, as a function of parameter C. The soft grey line corresponds to
the test error average (estimation of the actual risk), and the black line is the difference between them
(estimation of the structural risk).

Support Vector Machine and Kernel Classification Algorithms
487
function EmpStruct
addpath('../libsvm/')
% In this function we compute the Empirical risk and the test risk for
% different values of C. Changing C will change the complexity of the
% solution. The difference between the empirical and the test risk is a
% bound of the structural risk.
c = logspace(-1.5,1,100);
% This is the range of C that we sweep
Remp = zeros(1000,length(c)); % Matrix to store the Empirical Risk
R = Remp;
% Matrix to store the Tesr risk
for i = 1:1000 % Average 1000 times
for j = 1:length(c)
[X,Y] = data(100,1);
% Produce data for ...
training
options = ['-s 0 -t 0 -c ' num2str(c(j))]; % SVM options
model = mexsvmtrain(Y,X,options);
% Train the SVM
[~,p] = mexsvmpredict(Y,X,model);
% Test the SVM with ...
the train data
Remp(i,j) = 1-p(1)/100;
% Compute the ...
empirical risk
[X,Y] = data(1000,1);
% Produce test data
[~,p] = mexsvmpredict(Y,X,model);
% Test with it
R(i,j) = 1-p(1)/100;
% Compute the test risk
end
% Plot figures
figure(1)
semilogx(c,mean(Remp(1:i,:)),'r'), hold on
% Empirical risk (red)
semilogy(c,mean(R(1:i,:)))
% Test (actual) risk ...
(blue)
% Difference between both risks, which is a bound on the Structural ...
risk
semilogy(c,mean(R(1:i,:))-mean(Remp(1:i,:)),'k'), hold off; drawnow
end
function [X,Y] = data(N,sigma)
w = ones(1,10)/sqrt(10); % A vector in a 10 dimension space
w1 = w .* [ 1
1
1
1
1 -1 -1 -1 -1 -1]; % One more orthogonal to the ...
first
w2 = w .* [-1 -1
0
1
1 -1 -1
0
1
1]; % One more orthogonal to the ...
previous ones
w2 = w2 / norm(w2);
% Normalize
% The following four vectors are centers of four clusters forming a square
x(1,:) = zeros(1,10); x(2,:) = x(1,:) + sigma * w1;
x(3,:) = x(1,:) + sigma * w2; x(4,:) = x(3,:) + sigma * w1;
% The following data are the four clusters pf data labelled with +1 and -1
X1 = x + sigma * repmat(w,4,1) / 2; X2 = x - sigma * repmat(w,4,1) / 2;
% The previous eight clusters are the edges of a cube in a 3D space
X1 = repmat(X1,2*N,1); X2 = repmat(X2,2*N,1); X = [X1;X2];
Y = [ones(4*2*N,1) ; -ones(4*2*N,1)]; Z = randperm(8*2*N);
Z = Z(1:N); X = X(Z,:) + 0.2 * sigma * randn(size(X(Z,:))); Y = Y(Z);
Listing 10.3 Script for the example in Figure 10.14 (EmpStruct.m).
Consistency of Learning
A desirable property of a training algorithm is the consistency of learning. When the
number of training data increases, the empirical error increases and the structural

488
Digital Signal Processing with Kernel Methods
error should decrease as a consequence of the improvement of the generalization of the
machine. If the learning machine is consistent, then the training error and the test error
should uniformly converge to the same value. In the example of Figure ., the training
and test errors of the previous example are computed for a diﬀerent number of training
data. For small training sets, the training error is low, where the test error is high. This
suggests that there is high overﬁtting. Actually, when the number of samples is too low,
the machine tends to adapt to the sample, thus producing high-biased solutions that
produce a high test error (high structural risk). When the number of samples increases,
the SVM tends to better generalize, and both errors converge to the same value. The
code for this example can be seen in Listing ..
function consistency
addpath ../libsvm/
c = 1;
% Set C = 1 in the functional
NN = 10:500;
% Number of samples from 10 to 500
Remp = zeros(1000,190); % Matrix to store empirical risc
R = Remp;
% Matrix to store test risk
for i = 1:100
for j = 1:length(NN)
[X,Y] = data(NN(j),1);
% Produce training data
options = ['-s 0 -t 0 -c ' num2str(c)]; % SVM options
model = mexsvmtrain(Y,X,options);
% Train the SVM
[~,p] = mexsvmpredict(Y,X,model);
% Test the SVM with ...
training data
Remp(i,j) = 1-p(1)/100;
% Compute the empirical ...
risk
[X,Y] = data(1000,1);
% Produce test data
[~,p] = mexsvmpredict(Y,X,model);
% Test with it
R(i,j) = 1-p(1)/100;
% Compute the test risk
end
% Plot the results
figure(1), plot(NN,mean(Remp(1:i,:)),'r'), grid on, hold on
plot(NN,mean(R(1:i,:))), hold off; drawnow
end
keyboard % this allows to inspect variables before exiting the function
function [X,Y] = data(N,sigma)
w = ones(1,10) / sqrt(10); % A vector in a 10 dimension space
w1 = w .* [ 1
1
1
1
1 -1 -1 -1 -1 -1]; % One more orthogonal to the ...
first
w2 = w .* [-1 -1
0
1
1 -1 -1
0
1
1]; % One more orthogonal to the ...
previous ones
w2 = w2 / norm(w2);
% Normalize
% The following four vectors are centers of four clusters forming a square
x(1,:) = zeros(1,10); x(2,:) = x(1,:) + sigma * w1;
x(3,:) = x(1,:) + sigma * w2; x(4,:) = x(3,:) + sigma * w1;
% The following data are the four clusters pf data labelled with +1 and -1
X1 = x + sigma * repmat(w,4,1) / 2; X2 = x - sigma * repmat(w,4,1) / 2;
% The previous eight clusters are the edges of a cube in a 3D space
X1 = repmat(X1,2*N,1); X2 = repmat(X2,2*N,1); X = [X1;X2];
Y = [ones(4*2*N,1) ; -ones(4*2*N,1)]; Z = randperm(8*2*N); Z = Z(1:N);
X = X(Z,:) + 0.4 * sigma * randn(size(X(Z,:))); Y = Y(Z);
Listing 10.4 Script for the example in Figure 10.14 (consistency.m).

Support Vector Machine and Kernel Classification Algorithms
489
0
100
200
300
400
500
Number of training samples
0
0.05
0.1
0.15
0.2
0.25
0.3
Re
Rt
Figure 10.15 Training (Re) and test (Rt) errors as a function of the number of training data for the
example in Listing 10.4. It can be seen that, since the SVM is consistent, both errors converge to the
same value.
XOR Problem and Overfitting
The example in Figure .shows the classical XOR problem. This problem consists of
the binary classiﬁcation data generated by four Gaussian distributions centered in four
quadrants in a D space at positions ±, ±. The Gaussians centered at [−, −] and [, ]
are labeled −and the other two are labeled +.
An SVM using the Gaussian kernel with parameter 𝜎= was trained with two
diﬀerent values of C. High values of C will give high importance to the training error,
then ignoring the minimization of the structural error. Such a solution may suﬀer from
overﬁtting, while low values of C will produce solutions with a more balanced trade-
oﬀbetween structural and empirical risk. The left and central panels of Figure .
show the classiﬁcation boundary achieved with a value of C equal to and . It
is obvious that these boundaries are not close to the optimal value, but it is rather
constructed to produce a small number of errors during the training phase; in other
words, this is a graphical example of overﬁtting. With a value of C set to .(right panel
of Figure .), a more reasonable solution is produced that approaches the optimum
separating surface. The script for data generation and ﬁgure representation is shown in
the MATLAB Listing ..
function xor_example
% Toy example of the solution of the XOR problem using kernels and SVMs
addpath('../libsvm/'), addpath('../simpleR/'), rng(1);
C = 0.1;
% Complexity parameter
sigma = 0.5;
% Kernel parameter
c = [ 1 -1 -1 1 ; 1 -1 1 -1 ];
% Centers of the 4 clusters
% Generation of 400 datapoints
x = [ repmat(c(:,1:2),1,100) repmat(c(:,3:4),1,100) ];
x = x + 0.4 * randn(size(x));
y = [ ones(1,200) -ones(1,200) ]';
K = kernelmatrix('rbf', x, x, sigma);
% Kernel matrix

490
Digital Signal Processing with Kernel Methods
options = ['-s 0 -t 4 -c ' num2str(C)];
model = mexsvmtrain(y,K,options);
% Train the SVM
% alpha = pinv(K + gamma * eye(size(K))) * y; % Compute dual parameters
% Note the use of a diag matrix
[X1,X2] = meshgrid(-3:.05:3,-3:.05:3);
% Grid for representing data
% representation of the data
xt = [X1(:) X2(:)]';
% Vectorize it to use it in test
Ktest = kernelmatrix('rbf', xt, x, sigma);
[~,~,z] = mexsvmpredict(zeros(size(xt,2),1), Ktest, model); % Predictions
Z = buffer(z, size(X1,1), 0);
% convert output vector into matrix
% Plot data
figure(1), clf
plot(x(1,y>0), x(2,y>0), 'bo', x(1,y<0),x(2,y<0), 'ro'); hold on
contour(X1, X2, Z, [0 0], 'LineWidth', 2, 'Color', 'Black'), hold off
figure(2), surfc(X1,X2,Z), shading interp, camlight right, view([-38 30])
xlabel('$x_1$','Interpreter','latex'); ...
ylabel('$x_2$','Interpreter','latex')
Listing 10.5 Script for the example in Figure 10.16 (xor_example.m) using an SVM classifier.
x1
–3
–2
–1
0
1
2
3
x2
x1
–3
–2
–1
0
1
2
3
x2
–3
–2
–1
3
–3
–2
–1
3
0
1
2
0
1
2
–3
–2
–1
0
1
2
3
x1
–3
–2
–1
0
1
2
3
x2
Figure 10.16 Results of the XOR example. The first and second panels show a solution where the SVM
has been set to give too much importance to the training samples (C = 100 and 10), where the third
panel shows a solution with C = 0.1.

Support Vector Machine and Kernel Classification Algorithms
491
–0.05
0
0.05
–0.05
0
0
0.05
–0.05
0
0.05
–0.05
0.05
0
–0.05
0
0.05
–0.05
0.05
Figure 10.17 Result of the Fibonacci example for three different values of the SVM parameter C, from
a high one (first) to a low one (second). Increasing the value of C increases the complexity of the
solution, making it prone to overfitting, as is observed in the first panel.
Double Fibonacci Spiral
In some cases, the complexity needed by a machine to solve a classiﬁcation problem
depends on the position of the data in the space. This is the case of the toy problem
called the double Fibonacci spiral. In this problem, two sets of data labeled with labels
+and −are generated following two Fibonacci spirals. This spiral has a radius that
increases exponentially, so the complexity needed to classify decreases when the radius
increases (see Figure .). The code to generate this example is in Listing ..
function spirals
% Setup paths
addpath('../libsvm/'), addpath('../simpleR/')
% Generate and plot a set of data in two spirals
rng(1); sigma = 0.01;
[X1,X2,y] = fibonacci(300);
x = [X1 X2]; % Data stored in a matrix. Labels are stored in y
K = kernelmatrix('rbf', x, x, sigma); % Training kernel matrix
nfig = 1;
for C = [1000 100 10 1]

492
Digital Signal Processing with Kernel Methods
options = ['-s 0 -t 4 -c ' num2str(C)];
% Compute the dual parameters and store them in a column vector ...
called alpha.
model = mexsvmtrain(y,K,options);
M = max(abs(x(:))) * 0.9;
[X1_,X2_] = meshgrid(-M:M/100:M, -M:M/100:M); % Grid of points for
xt = [X1_(:) X2_(:)]';
% Vectorize it to use it in test
Ktest = kernelmatrix('rbf', xt, x, sigma);
% Test matrix
% Classify the test data
[~,~,z] = mexsvmpredict(zeros(size(xt,2),1), Ktest, model);
% Convert the output vector into a matrix for representation
Z = buffer(z, size(X1_,1), 0);
% Plotting results
figure(nfig), nfig = nfig + 1; clf
plot(X1(1,:), X1(2,:), 'bo', X2(1,:), X2(2,:), 'ro'); hold on
contour(X1_, X2_, Z, [0 0], 'LineWidth', 2, 'Color', 'Black'); hold ...
off
axis square; title(['C = ' num2str(C)])
end
function [X1,X2,y] = fibonacci(N)
theta = rand(1,N) * (log10(3.5*pi));
theta = 10.^theta + pi; theta = 4*pi - theta;
a = 0.01; b = 0.2; r = a * exp(b*theta);
x1 = r .* cos(theta); y1 = r .* sin(theta);
d = 0.14 * sqrt(x1.^2 + y1.^2);
x1 = x1 + randn(size(x1)) .* d; y1 = y1 + randn(size(y1)) .* d;
X1 = [x1;y1]; x2 = r .* cos(theta+pi); y2 = r .* sin(theta+pi);
d = 0.14 * sqrt(x2.^2 + y2.^2); x2 = x2 + randn(size(x2)) .* d;
y2 = y2 + randn(size(y2)) .* d; X2 = [x2;y2];
y = [ones(1,300) -ones(1,300)]';
Listing 10.6 Script for the example in Figure 10.17 (spirals.m).
10.5.2
Example of Least-Squares Support Vector Machine
We provide in Listing .a simple implementation of the LS-SVM. For the sake of
brevity, we only list the two lines of code needed to train and test the LS-SVM, and the
corresponding LS-SVM function. We let as an exercise for the reader the implementa-
tion of examples XOR Problem and Overﬁtting and Double Fibonacci Spiral (see Exer-
cise ..). Note that this implementation simply reproduces Equation .. The clas-
siﬁer has exactly the same formulation as the one of the standard SVM classiﬁer; that is:
̂y∗= f (x∗) = sgn
( N
∑
i=
yi𝛼iK(xi, x∗) + b
)
,
In the implementation in Listing ., the dual variables 𝛼i are multiplied by the labels
inside the function to add simplicity to the implementation.
function y = ls_svm(y,K,Ktest,lambda)
% Example of LS-SVM
% Train

Support Vector Machine and Kernel Classification Algorithms
493
w = pinv([ 0 -y'; y diag(y)*K*diag(y) + lambda*eye(size(K,1)) ]) * ...
[ 0 ; ones(size(y)) ];
alpha = w(2:end) .* y;
b = w(1);
% Test
y = Ktest' * alpha + b;
Listing 10.7 Code snippets needed to implement the function LSSVM (ls_svm.m).
10.5.3
Kernel-Filtering Support Vector Machine for Brain–Computer Interface
Signal Classification
The BCI problem considered here is one of the problems presented in BCI Competition
III (Blankertz et al., ). The objective is to obtain a sequence of labels out of brain
activity signals for three diﬀerent human subjects. The data consist of channels
containing PSD features for diﬀerent band-pass (three training sessions and one test
session, with N
≃samples per session) and three possible labels (left arm,
right arm, or a word). Several classes were tackled through a classical OAA strategy.
For the nonlinear approaches, % of the available data was used for training. The
regularization parameters were tuned using a grid search strategy on the third training
session. In these experiments, Nwas set to zero, as we are interested in predicting the
current mental task with no delay.
The test error for diﬀerent methods and ﬁlter lengths is summarized in Table .. For
the linear models, the best methods for all tested ﬁlter sizes are KF-SVM, SKF-SVM, and
SWinSVM; see details in Flamary et al. (). This shows the advantage of taking into
account the neighborhood of the samples for decisions and the importance of a proper
regularization. Longer ﬁltering provides the best results, especially in conjunction with
regularization that helps to avoid overﬁtting. The best overall results are obtained by KF-
SVM and SKF-SVM with the ﬁlter length 𝜏= . The results follow the same trends for
the nonlinear models, showing that for this task a linear classiﬁer is suﬃcient. However,
one should keep in mind that, in these cases, the decision functions are learned from
only % of the samples. In this case, the Avg-SVM performs well, since the noise is
in the high frequencies and the nonlinearities that can be induced by overﬁltering are
handled by the Gaussian kernel.
Links to the MATLAB code for LMF are available on the book’s web page. A
simple demo illustrating how the functions are called is given in Listing .. Essen-
tially, one has to start with scaled data, initialize the options’ MATLAB structure
for the routines, and call each model with the corresponding training and testing
functions. The code shows how to run a comparison between a canonical SVM working
directly with raw data (no ﬁltering), using SVM on ﬁltered data, and the LMF-SVM
method where the optimal ﬁlters are learned such that SVM classiﬁcation accuracy is
maximized.
% Sketch of the use of LMF in the BCI problem
%
- You need to download and install FilterSVM toolbox from
%
http://remi.flamary.com/soft/soft-filtersvm.html
%
- Assume that the data is stored in matrix X and Y (training),

494
Digital Signal Processing with Kernel Methods
%
Xte and Yte (test), and Xval and Yval (validation)
% Initialize parameters
f
= 10; % filter length f (=tau)
options.f
= f;
options.decy
= 0;
% y time shift...
options.F
= ones(f,d)/(f); % init filter weights
options.stopvarx
= 0;
% threshold for stopping on F changes
options.stopvarj
= 1e-3;
options.usegolden
= 1;
options.goldensearch_deltmax = .01;
options.goldensearch_stepmax = 1;
options.numericalprecision
= 1e-3;
options.multiclass
= 1;
linear = 1;
% let's try with a linear kernel
if linear == 1
options.kernel = 'linear';
options.kerneloption = 1;
options.C = 1e0; % C parameter in the SVM
options.regnorm2 = 1e0;
options.regnorm12 = 0e-3;
options.solver = 3;
options.use_cg = 1;
else
options.kernel = 'gaussian';
options.kerneloption = 10;
options.C = 1e3; % C parameter in the SVM
options.regnorm2 = 1e1;
options.regnorm12 = 0e2;
options.solver = 3;
options.use_cg = 1;
end
% No filtering at all
[svm,obj] = svmclass2(X,Y,options);
% Filtering and then classify
[svmf,obj] = svmclass2(mfilter(options,X),Y,options);
% LMF models: learn the optimal filters for classification
[sigsvm,obj] = filtersvmclass(X,Y,options);
% Evaluate in the test set
ysvm = svmval2(Xte,svm); prec_svm = get_precision(Yte,ysvm);
[ypred] = svmval2(mfilter(options,Xte),svmf); prec_filt = ...
get_precision(Yte,ypred);
[ypred,yval] = filtersvmval(Xte,sigsvm); prec_filtsvm = ...
get_precision(Yte,ypred);
Listing 10.8 Comparison between different approaches to classification under noisy data: SVM
working without filtering the data, using SVM on filtered data, and the LMF-SVM where optimal filters
are learned for optimal classification (demoLMF.m).
10.5.4
Example of Laplacian Support Vector Machine
Let us exemplify the use of the Laplacian SVM with a demo involving the familiar two
moons example (see Listing .). In this particular example we used u = unlabeled
data points and only l = labeled points per class.

Table 10.5 Classification error rate for the BCI dataset for different methods (see details in Flamary et al. (2012)) and filter
length 𝜏𝜏. Results are given for the linear model (top) and for the nonlinear model (bottom). The three best methods are
highlighted in bold.
τ = 10
τ = 20
τ = 50
Method
S1
S2
S3
Avg
S1
S2
S3
Avg
S1
S2
S3
Avg
Linear model
SVM
.
.
.
.
.
.
.
.
.
.
.
.
Avg-SVM
.
.
.
.
.
.
.
.
.
.
.
.
KF-SVM
.
.
.
.
.
.
.
.
.
.
.
.
SKF-SVM
.
.
.
.
.
.
.
.
.
.
.
.
WinSVM
.
.
.
.
.
.
.
.
.
.
.
.
SWinSVM
.
.
.
.
.
.
.
.
.
.
.
.
Nonlinear model (Gaussian kernel)
SVM
.
.
.
.
.
.
.
.
.
.
.
.
Avg-SVM
.
.
.
.
.
.
.
.
.
.
.
.
KF-SVM
.
.
.
.
.
.
.
.
.
.
.
.
SKF-SVM
.
.
.
.
.
.
.
.
.
.
.
.
WinSVM
.
.
.
.
.
.
.
.
.
.
.
.

496
Digital Signal Processing with Kernel Methods
function [X,Y] = moons(n)
rng(2);
space = 1.2; noise = 0.1;
r = randn(n,1) * noise + 1; theta = randn(n,1) * pi;
r1 = 1.1 * r; r2 = r;
X1 = ([r1 .* cos(theta) abs(r2 .* sin(theta))]);
Y1 = ones(n,1); % labels
r = randn(n,1) * noise + 1; theta = randn(n,1) * pi + 2*pi;
r1 = 1.1 * r; r2 = r;
X2 = ([r1 .* cos(theta) + space*rand -abs(r2 .* sin(theta)) + 0.2 ]);
Y2 = -ones(n,1); % labels
X = [X1;X2]; Y = [Y1;Y2]; v = randperm(2*n); X = X(v,:); Y = Y(v);
Listing 10.9 Generating data drawn from the two moons manifold (moons.m).
The ﬁrst operation is building a graph Laplacian, which uses all examples l + u. Links
to the MATLAB code for this are available on the book’s web page. After that we build
the kernel matrix K using kernelmatrix for both training and testing, and solve the
LapSVM equations (Equations .–.). The code is given in the wrapper function
in Listing .. Note that the QP involved can be solved with the LIBSVM toolbox, and
the penalization parameter C in the standard SVM algorithm is now embedded in the
𝛾A of the LapSVM.
function [alpha,b]=lapsvm(K,Y,L,lambda1,lambda2)
I = eye(size(K,1)); lab = find(Y); l = length(lab);
if isempty(L) || lambda2 == 0 % SVM
G = I / (2*lambda1) ;
% Csvmmax = 1 / (2*lambda1*l) = 1/(2*gA*l)
else
% Laplacian SVM
G = (2*lambda1*I + 2*lambda2*L*K) \ I;
end
Gram = K*G; Gram = Gram(lab,lab); Ylab=Y(lab);
% Calling libSVM
C = 1; % C = 1 in Laplacian SVM
model = mexsvmtrain(Ylab, Gram, ['-t 4 -s 0 -c ' num2str(C)]);
betay = model.sv_coef; svs = model.SVs; nlab = model.Label;
% b = model.rho;
% nsv = model.nSV;
if nlab(1) == -1, betay = -betay; end;
Betay = zeros(l,1); Betay(svs+1) = betay';
alpha = G(:,lab) * Betay; b = 0; % Laplacian SVM dual weights
Listing 10.10 Learning the Laplacian SVM (lapsvm.m).
We put all these pieces together in a simple demo for running the LapSVM; see
Listing .. Several values of 𝛾L and 𝛾M were tuned to illustrate its eﬀect while ﬁxing
the 𝜎parameter of the Gaussian kernel function. Results and the decision surfaces
are shown in Figure .for diﬀerent combinations of regularization parameters.
We can observe that when 𝛾M = , LapSVM disregards the information conveyed by the
unlabeled data and returns the SVM decision boundary which is ﬁxed by the location of

Support Vector Machine and Kernel Classification Algorithms
497
σ = 0.25,
γL = 1e–05,  γM = 1e–05
σ = 0.25,
γL = 1e–05, γM = 0.07
σ = 0.25,
γL = 1e–05, γM = 1
Figure 10.18 Laplacian SVM with RBF kernels for various values of 𝛾M = {10−5, 0.07, 1} and fixed
𝛾L = 10−5. Labeled points are shown in circles, while unlabeled points are given in black dots.
the two labeled points. As 𝛾M is increased, the intrinsic regularizer ‖f ‖
incorporates
unlabeled data and causes the decision surface to appropriately adjust according to the
geometry of the two classes.
%% Demo for Laplacian SVM
clear, clc; rng('default')
%% Setup paths
addpath('../simpleR/'), addpath('../libsvm/')
%% Training data
l = 2; u = 300; [Xl,Yl] = moons(l/2);
%% Unlabeled data
Xu = moons(u/2); Yu = zeros(u,1); X1 = [Xl;Xu]; Y1 = [Yl;Yu];
%% Test data
[X2,Y2] = moons(200);
%% Scale data
X1 = scale(X1); X2 = scale(X2);
%% Build the graph Laplacian L of the adjacency graph W with all data l+u
options.type = 'nn';
% nearest neighbors to obtain the adjacent points
options.NN = 12;
% number of neighbors
options.GraphDistanceFunction = 'euclidean'; % distance for the adjacency
options.GraphWeights = 'heat'; % heat function applied to the distances ...
in W
options.GraphWeightParam = 1;
% width for heat kernel (t) if 'heat'
options.GraphNormalize = 1;
% normalizes the adjacencies of each point
L = laplacian(X1, 'nn', options);
%% Hyperparameters: sigma, gamma_A, gamma_I
sigma = 0.3;
gL = 0.05; lambda1 = gL;
gM = 5;
lambda2 = gM; % Use gM = 0 for standard SVM
%% Train LapSVM
K1 = kernelmatrix('rbf', X1', X1', sigma);
K2 = kernelmatrix('rbf', X2', X1', sigma);
[alpha,b] = lapsvm(K1, Y1, L, lambda1, lambda2);
Y2pred = sign(K2 * alpha - b);
OA = 100 * length(find(Y2pred == Y2)) / length(Y2)
%% Plot results
x = -0.25:0.05:1.25; y = -0.25:0.05:1.25; % x and y range
[xx,yy] = meshgrid(x,y); X2 = [xx(:) yy(:)];
K2 = kernelmatrix('rbf', X2', X1', sigma);

498
Digital Signal Processing with Kernel Methods
z
= (K2 * alpha - b); Z = reshape(z,length(x),length(y));
figure(1), clf
[cs,h] = contourf(xx, yy, Z, 1); shading flat; colormap(summer) ...
%colormap(cmr)
hold on, plot(X1(Y1==0,1), X1(Y1==0,2), 'k.');
plot(X1(Y1==+1,1), X1(Y1==+1,2), 'bo', X1(Y1==-1,1), X1(Y1==-1,2), 'ro');
axis off square
Listing 10.11 Demo for the Laplacian SVM in the two moons example (demoLapSVM.m).
10.5.5
Example of Graph-Based Label Propagation
Listing .gives the code to run the graph-based label propagation algorithm in Zhou
and Schölkopf (). The function arguments require the input matrix X where each
row contains an example, a label matrix Y with -of-k encoding (i.e., if there are three
classes, k = and then the row vectors of Y are possibly [, , ], [, , ], [, , ], and
[, , ] for the unlabeled points), a 𝜎parameter for the width of the Gaussian aﬃnity
matrix, and the propagation parameter 𝛼between [, ]. The output of the function
returns in C the ﬁnal classiﬁcation, where the elements of C are scalars from to k,
where k is the number of classes.
function
C = semi(X,Y,sigma,alpha)
N = size(X, 1);
% Step 1: Affinity matrix
M = zeros(N, N); % norm matrix
for i = 1:N % compute the pairwise norm
for j = (i+1):N
M(i, j) = norm(X(i, :) - X(j, :));
M(j, i) = M(i, j);
end
end
% Use a Gaussian to form an affinity matrix
K = exp(-M.^2/(2*sigma^2));
K = K - eye(N);
% Step 2: Symmetrical normalization
D = diag(1./sqrt(sum(K))); % inverse of the square root of the degree ...
matrix
S = D*K*D; % normalize the affinity matrix
% Step 3(a): Compute the classification function
F = (eye(N) - alpha * S) \ Y;
% Step 3(b): Predictions
[~,C] = max(F, [], 2);
Listing 10.12 Semisupervised graph-based classification example (semi.m).
10.5.6
Examples of Multiple Kernel Learning
Besides the simpleMKL algorithm, another relevant study on kernel combination was
presented in Cortes et al. (), where new and eﬀective algorithms for learning kernels
were introduced. The simplicity of the framework, and the good results obtained in

Support Vector Machine and Kernel Classification Algorithms
499
general, are good reasons to be studied. The framework presents several techniques
to learn the relevance of the features and how to exploit the discovered structure to
combine dedicated kernels. Interestingly, these approaches do not involve an exhaustive
training of SVMs or any particular classiﬁer, but just simple matrix operations.
Let us consider just an approximation to the kernel combination, the so-called
alignment maximization algorithm, in which the correlation between the base kernel
matrices is taken into account. The goal is to determine a combination of kernels, one
per dimension d; that is:
K𝜇(xi, xj) =
d
∑
f =
𝜇f Kf (xf
i , xf
j ),
where xf
i is the feature f of point xi. Let us assume a supervised problem in which we are
given the input data matrix X ∈ℝn×d and the corresponding outputs (targets, labels)
y ∈ℝn×. Here, the aim is to learn a weight af per dimension. The approach considers
aligning the kernel K 𝜇with the labels kernel (sometimes referred as to the ideal kernel)
expressed as K y = yyT. Let us recall the kernel matrix alignment operation between two
kernel matrices K and K ′ is expressed as
A =
⟨K, K ′⟩
‖K‖F‖K ′‖F
,
which can be calculated easily in MATLAB using the code snippet in Listing ..
function A = alignment(Ker,Y)
A = trace(Ker'*Y) / sqrt(trace(Ker'*Ker) * trace(Y'*Y));
Listing 10.13 Code snippet for calculating the kernel matrix alignment (alignment.m).
Cortes et al. () proposed two particular strategies to learn the weights 𝜇f , f =
, … , d: one exploits an LS linear combination (according to Proposition in Cortes
et al. ()) and the other performs a convex combination by solving a QP problem
(Proposition in Cortes et al. ()).
Linear combination The linear combination approach considers optimizing
𝝁opt = arg max𝝁
{
⟨K 𝝁, yyT⟩
‖K 𝝁‖F
}
s.t. ‖𝝁‖= ,
where we should note that the denominator does not contain ‖yyT‖F because this
does not depend on 𝝁. It can be shown that the solution to this problem is
𝝁opt =
M−a
‖M−a‖
,
where M is an invertible matrix with entries Mkl = ⟨̃K l, ̃K l⟩for k, l ∈[, d], and
a ∶= [⟨̃K , yyT⟩, … , ⟨̃K d, yyT⟩]T, where the tilde indicates the matrix is centered
(see Listing .).

500
Digital Signal Processing with Kernel Methods
function K = centering(K)
[Ni,Nj] = size(K);
K = K - mean(K,2)*ones(1,Nj) - ones(Ni,1)*mean(K,1) + mean(K(:));
Listing 10.14 Centering a kernel matrix (centering.m).
Convex combination This approach constrains the values of 𝝁to be positive and sum
to one, and solves the problem
𝝁opt = arg max𝝁
{𝝁TaaT𝝁
𝝁TM𝝁
}
s.t. ‖𝝁‖= , and 𝝁≥,
whose solution reduces to 𝝁opt = vopt∕‖vopt‖, where vopt is the solution of the QP
problem
vopt = arg minv≥{vTMv −vTa}.
Listing .gives MATLAB code for these two multiple kernel combinations.
% Choose method
mklmethod = 'align'; % or alignf
% Given X and y
[n d] = size(X);
% Optimal kernel
Y = y * 2 - 1; Ky = Y * Y'; Kyc = centering(Ky);
% The best combination of kernels with different sigmas
ker = 'rbf';
Kc = zeros(size(K,1),size(K,2),d); a = zeros(1,d);
for dim = 1:d
sigma = median(pdist(X(:,dim)));
K = kernelmatrix(ker,X(:,dim)',X(:,dim)',sigma);
Kc(:,:,dim) = centering(K);
a(dim) = alignment(Kc(:,:,dim),Ky);
end
% Compute the M kernel matrix
M = zeros(d);
for i = 1:d
for j = 1:d
M(i,j) = sum(sum(Kc(:,:,i) .* Kc(:,:,j)));
end
end
% Solve the problems
if strcmpi(mklmethod,'align') % Linear combination ("align", Prop. 8)
v = M \ a';
elseif strcmpi(mklmethod, 'alignf') % Convex combination ("alignf", ...
Prop. 9)
options = optimset('Algorithm', 'interior-point-convex');
v = quadprog(M,-a',-1*ones(1,length(a)),0,[],[],[],[],[],
options);
end
mu = v/norm(v);
% The combined kernels
Kbest = zeros(n);

Support Vector Machine and Kernel Classification Algorithms
501
for dim = 1:d
Kbest = Kbest + mu(dim) * squeeze(Kc(:,:,dim));
end
Listing 10.15 Combination of kernels via alignment and optimization (automaticSimplerMKL.m).
10.6
Concluding Remarks
This chapter has provided a summary of the SVM for classiﬁcation. The SVM is a linear
algorithm that is solved using a dual formulation. Hence, nonlinear versions of the
SVM using the kernel trick can be readily obtained by changing the scalar dot product
by any positive -deﬁnite kernel function. We have seen how the SVM also admits
straightforward multiclass and multilabel, and several modiﬁcations on the regularizer,
such as 𝓁p-norms or the 𝜈-SVM to enforce all kinds of sparsity and thus control for the
compactness of the solution. Also, we revised novel machine-learning paradigms that
grew under the SVM framework, such as SSL, AL, MKL, and SL.
The reason for the success of the SVM is based on its ability to control the complexity
and regularization of the solution through the inclusion of the SRM as part of the
optimization criterion of the machine. Also an important property of SVM is the
convexity of the functional to be solved, which is not shared by many other algorithms.
In practice, the SRM translates into the minimization of the norm of the machine
parameters w, which, in turn, is equivalent to the maximization of the classiﬁcation
margin. This apparently simple criterion makes the nonlinear extension of the SVM
to inﬁnite-dimension Hilbert spaces, where the maximum possible complexity is equal
to the number of training data, and where other approaches that do not control the
complexity are guaranteed to overﬁt.
Besides the accuracy, sparsity, and ﬂexibility of SVMs, the availability of many eﬃcient
toolboxes has also helped popularize the technique. This versatility, together with the
simplicity of the formulation of the SVM and its moderate computational burden,
mainly bounded by the number of training data (and not its dimension), allowed
researchers to apply the SVM to problems that were previously solved using more
computationally heavy solutions. Nevertheless, one of the main drawbacks of the
original SVM for classiﬁcation (yet also for regression problems and feature extraction)
is that the computation time for the optimization increases dramatically with the
number of data; as a consequence of that, the dual formulation involves the use of kernel
matrices. This drawback has been tackled in numerous studies in the last decade, and
we have revised the main current advances, either based on approximating the kernel
matrices with randomized versions, Nyström approximations, or smart strategies for
parallelizing the algorithms.
SVMs have played a key role for solving classiﬁcation problems during this last
decade, and it is expected that further developments will appear in the next years.
We can only speculate about the directions for this evolution, as probably unexpected
approaches will emerge naturally. Given the rise of deep learning techniques in the last
years, it is expected that advanced and solid schemes will be given by integrating the best
of both. Another naturally incoming scenario is the big data technology, where some
advances have already started to be developed, as we have seen in this chapter. Another

502
Digital Signal Processing with Kernel Methods
long-known yet unresolved topic is feature selection, and SVMs are well equipped to
tackle it.
10.7
Questions and Problems
Exercise ..
Reproduce the example of Figure .using a classiﬁer based on the
ridge regression criterion and compare it with the SVM classiﬁer. In order to reproduce
the example, write a code that computes the dual variables 𝛼i of the estimator and store
them in a column vector. Insert the code in line of the code provided in Listing ..
Then, write a code that classiﬁes the training data stored in matrix xt and insert it in
line . The function computes and represents the double spiral and the classiﬁcation
boundary.
For the ridge regression, use diﬀerent values of the variable 𝛾that regularizes the
kernel matrix. For the SVM, use diﬀerent values of C to force diﬀerent levels of
complexity.
Exercise ..
Represent the support vectors obtained with code in Listing .in
the previous exercise.
Exercise ..
Derive Equation .corresponding to the solution of the LS-SVM
Lagrangian. by optimizing it with respect to its parameters. Then derive Equation .
by applying the assumption that the primal parameters are a linear combination of the
data.
Exercise ..
Use the result of the exercise above to modify the script in Listing .
and obtain a solution that uses the LS-SVM. In order to work out this exercise, the
reader can use the code in Listing .. In this code, the name lambda is used for the
regularization parameter, instead of C. Represent the values of the dual parameters 𝛼i
as a function of the error. What relationship between both can be observed?
Exercise ..
Given a vector x, a valid transformation into a higher dimension
Hilbert space is
𝜙(x) =
(
𝝓(x)
𝝓(x)
)
,
(.)
where 𝝓(⋅) and 𝝓(⋅) are two transformations into two diﬀerent RKHSs endowed with
kernels K(⋅, ⋅) and K(⋅, ⋅). Prove that the kernel of the transformation in Equation .
is the one of Equation ..
Exercise ..
Modify the code of Listing .to obtain a ridge regression version
of the algorithm. Use it to reproduce the “XOR Problem and Overﬁtting” example.
Compare their performances in accuracy and computational time for a large amount
of data.

503
11
Clustering and Anomaly Detection with Kernels
Many problems in signal processing deal with the identiﬁcation of the right subspace
where signals can be better represented. Finding good and representative groups or
clusters in the data is of course the main venue. We will ﬁnd many kernel algorithms
to approach the problem, by ‘kernelizing’ either the metric or the algorithm. When
there is just one class of interest, alternative methods exist under the framework
of one-class detection or anomaly detection. The ﬁeld of anomaly detection has the
challenge of ﬁnding the anomalous patterns in the data. Anomaly detection from data
has many practical implications, such as the classiﬁcation of rare events in time series,
the identiﬁcation of target signals buried in noise, or the online identiﬁcation of changes
and anomalies in the wild.
The problem of deﬁning regularity, cluster, membership, and abnormality is a con-
ceptually challenging one, for which there exist many lines of attack and assumptions
involved. This chapter will review the recent advances of kernel methods in the ﬁelds
of clustering, domain description (also known as outlier identiﬁcation or one-class
classiﬁcation) and subspace detectors. It goes without saying that all these methods
are unsupervised or weakly supervised, so the challenge is even bigger. The chapter
will present examples dealing with synthetic data as well as real problems in all these
domains.
11.1
Introduction
Detecting patterns and groups in data sets is perhaps the most challenging problem
in machine learning. The problem is unsupervised and requires many assumptions to
attain signiﬁcant results. For example, the user should ask elusive questions, such as:
How many (semantically meaningful) groups characterize the data distribution? What
is the appropriate notion of distance? What is the intrinsic (subspace) dimensionality
where the data live? How do you derive robust statistics for assessing the signiﬁcance of
the results and to measure the diﬀerent error types? In this chapter, we review the main
developments in the ﬁeld of kernel methods to tackle these problems.
Nowadays, we have kernel methods for clustering data in diﬀerent forms and sophis-
tication. Kernel methods have been used to “kernelize” the metric, perform clustering
explicitly in feature spaces by exploiting the familiar kernel trick on standard clustering
algorithms, and to describe the data distribution via support vectors (Filippone et al.,
). Once the data distribution is described or distances between samples correctly
Digital Signal Processing with Kernel Methods, First Edition. José Luis Rojo-Álvarez, Manel Martínez-Ramón,
Jordi Muñoz-Marí, and Gustau Camps-Valls.
© John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.

504
Digital Signal Processing with Kernel Methods
captured, one can use the clustering solution for data visualization, classiﬁcation, or
anomaly detection. The latter implies that the incoming data points are identiﬁed as
normal or anomalous by simply computing the distance (or similarity) to those data
instances used in the optimization of the kernel decision function in the particular
clustering algorithm.
Clustering and density estimation are tightly related machine-learning tasks, and
sometimes are collectively referred as to domain description (Tax and Duin, ).
When there is not an underlying statistical law for the anomalous/outlying patterns, or
when we cannot assume one, nonparametric kernel methods can do the job eﬃciently.
Actually, these two cases are quite common; very often the number of extreme/anoma-
lous/rare events is very low, and most of the times they change over time, space, or
both. This is the ideal situation to describe (the boundary of) the target class instead
of tackling the more challenging problem of density estimation especially in high-
dimensional feature spaces, since that requires large datasets and dealing with often
intractable integrals. In any case, the use of kernel methods – which are in nature related
to classical statistical techniques such as Parzen’s windowing – for density estimation
have witnessed an increasing interest in recent years, guided either by maximum
variance and entropic kernel components, or by the introduction of an appealing family
of density ratio estimation methods that measure the relevance of examples in feature
spaces under a density estimation perspective.
The ﬁeld of anomaly detection in signal processing is vast. Very often one is
confronted with the task of deciding if an event is rare or unusual compared with
the expected behavior of the system. Just to name a few examples, anomaly detection
techniques have found wide application for fraud detection, intruder detection, fault
detection in security systems, military surveillance, network traﬃc monitoring, and
so on. Anomaly detection tries to ﬁnd patterns in the data that do not follow the
expected behavior of the system’s mechanism generating the observations. These
strange, anomalous nonconforming patterns are often referred to as anomalies,
outliers, exceptions, aberrations, surprises, peculiarities, or contaminants in diﬀerent
application domains (Chandola et al., ). Actually, one often uses the terms
“anomalies” and “outliers” interchangeably. Either way, detecting anomalies should be
put in the context of modeling what is normal, expected, regular, and pervasive, which
boils down to properly modeling the pdf or to ﬁnding representative and compact
groups/clusters in the data. The problem of anomaly detection can be tackled via
unsupervised learning, supervised learning, and SSL. Unsupervised anomaly detection
techniques detect anomalies in an unlabeled test dataset under the assumption that the
majority of the instances in the data set are normal by looking for instances that seem
to ﬁt least to the remainder of the dataset. Supervised anomaly detection techniques
require a data set that has been labeled as “normal” and “abnormal” and involves
training a classiﬁer. Semi-supervised anomaly detection techniques construct a model
representing normal behavior from a given normal training dataset, and then testing
the likelihood of a test instance to be generated by the learnt model. In this chapter,
however, we will only focus on the unsupervised setting, as it is more common in DSP.
A diﬀerent approach to anomaly detection is that of subspace learning. Since its inven-
tion, matched (subspace) ﬁlters have been revealed as eﬃcient for a myriad of signal-
and image-processing applications. Essentially, a matched ﬁlter (originally known as a
North ﬁlter) is obtained by correlating a known signal, or template, with an unknown

Clustering and Anomaly Detection with Kernels
505
signal to detect the presence (or absence) of the template in the unknown signal.
Matched subspace detection exploits this and assumes a subspace mixture model for
target detection where the target and background points are represented by their
corresponding subspaces. A high number of kernel-subspace methods have been intro-
duced, relying on diﬀerent assumptions and criteria; for example, maximum variance
subspace, orthogonality between anomalies and clutter, Reed–Xiaoli criterion (Kwon
and Nasrabadi, c), maximum separation of clusters, or maximum eigenspace
separation (Nasrabadi, ). We will review the most representative kernel subspace
matched detectors, including the kernel orthogonal subspace projection (KOSP) (Kwon
and Nasrabadi, b) and the kernel spectral angle mapper (KSAM) (Camps-Valls,
), as well as a family of kernel anomaly change detection (KACD) algorithms that
include the popular RX and cronochrome anomaly change detectors as special cases,
both under Gaussianity and elliptically contoured (EC) distributions (Longbotham and
Camps-Valls, ).
The issue of testing whether or not a sample belongs to a given (contrast) distribution
can be approached from signal detection concepts. This is the ﬁeld of hypothesis testing.
Kernel-based methods provide a rich and elegant framework for developing nonpara-
metric detection procedures for signal processing. Several recently proposed methods
can be simply described using basic concepts of RKHS embeddings of probability distri-
butions, mainly mean elements and covariance operators (Harchaoui et al., ). The
ﬁeld draws relationships with information divergences between distributions, and has
connections with information-theoretic learning concepts (Principe, ) introduced
in the ﬁeld of signal processing contemporaneously. Intimately related problems to
anomaly detection under the previous perspectives are those of change detection and
change-point identiﬁcation; here, given two subsequent instants of a data distribution,
one may be interested in identifying the points that changed and if the change was
signiﬁcant or not.
All the previous families of approaches are interconnected and share common char-
acteristics.This chapter will review them organized in the following taxonomy:
) Clustering, domain description, and one-class classiﬁcation are concerned with the
task of ﬁnding groups in the data, or to detect one class of interest (the normality
class) and discard the rest (the anomalous class).
) Density estimation focuses on modeling the distribution of the normal patterns.
) Matched subspace detection tries to identify a subspace where target and background
features can be separated.
) Change detection methods tackle the problem of detecting changes in the data.
) Hypothesis testing aims to detect changes in the data distributions under a certain
hypothesis of data regularity.
This is equivalent to convolving the unknown signal with a conjugated time-reversed version of the
template. The matched ﬁlter is the optimal linear ﬁlter for maximizing the SNR in the presence of additive
stochastic noise.
Clustering and anomaly detection is related to the ﬁeld of feature extraction and dimensionality
reduction, for which a plethora of kernel methods has been proposed. The exploitation of kernel
multivariate analysis methods (Arenas-García et al., ) that seek for directions in feature spaces that
highlight groups or anomalies, thus requiring projections on subspaces, will be the subject of Chapter .

506
Digital Signal Processing with Kernel Methods
We note some clear relations among the families and approaches, and for the sake of
simplicity we grouped them in three main dedicated sections in the chapter; namely,
clustering, domain description and matched subspace detection, and hypothesis test-
ing. Illustrative methods from each family are theoretically studied and illustrated
experimentally through both synthetic and real-life examples.
11.2
Kernel Clustering
As in the majority of kernel methods in this book, kernel clustering is based on reformu-
lating existing clustering methods with kernels. Such reformulation, nevertheless, can
take two diﬀerent pathways: either “kernelize” a standard clustering algorithm that relies
solely on dot products between samples or that relies on distances between samples.
Either way, the key point is that given a mapping function 𝝓∶ℝd →that maps the
input data to a high-dimensional feature space, we need to deﬁne a measure of distance
between points therein in terms of dot products of mapped samples to readily replace
them by kernel evaluations.
11.2.1
Kernelization of the Metric
A common place in clustering algorithms is to compute distances (e.g., Euclidean)
between points. If we map data into , we can compute distances therein without
explicitly knowing the mapping 𝝓or the explicit coordinates of the mapped data points
as follows:
‖𝝓(xi) −𝝓(xj)‖
= (𝝓(xi) −𝝓(xj))T(𝝓(xi) −𝝓(xj))
= 𝝓(xi)T𝝓(xi) + 𝝓(xj)T𝝓(xj) −𝝓(xi)T𝝓(xj)
= K(xi, xi) + K(xj, xj) −K(xi, xj).
(.)
Actually, for any positive-deﬁnite kernel, we assume that the mapped data in 
are distributed in a surface smooth enough to be considered a Riemannian
manifold (Burges, ). The line element of can be expressed as
ds= gab d𝝓a(x) d𝝓b(x) = g𝜇𝜈dx𝜇dx𝜈,
(.)
where superscripts a and b correspond to the vector space , g𝜇𝜈is the induced metric,
and the surface is parameterized by x𝜇. Note that Einstein’s summation convention
over repeated indices is used. Computing the components of the (symmetric) metric
tensor only needs the kernel function:
g𝜇𝜈= (∕)𝜕x𝜇𝜕x𝜈K(x, x) −{𝜕x′
𝜇𝜕x′
𝜈K(x, x′)}x′=x.
(.)
For the RBF kernel with a given 𝜎parameter, this metric tensor becomes ﬂat, g𝜇𝜈=
𝛿𝜇𝜈∕𝜎, and the squared geodesic distance between 𝝓(x) and 𝝓(x′) becomes
‖𝝓(xi) −𝝓(xj)‖
= 
[
−exp
(
−
‖xi −xj‖

𝜎
)]
= (−K(xi, xj)).
(.)

Clustering and Anomaly Detection with Kernels
507
Note that the metric solely depends on the original data points yet computed implicitly
in a higher dimensional feature space , whose notion of distance is controlled by the
parameter 𝜎: the larger 𝜎is the smoother (linear) is the space. Actually, 𝜎→∞reduces
the RBF kernel to approximately compute the Euclidean distance between vectors,
which reduces the metric tensor to g𝜇𝜈= .
Kernel Online Anomaly Detector
Let us imagine now that we receive incoming samples online. The problem now is to
select the informative samples and discard the anomalous ones. This way the machine
will not grow indeﬁnitely and will retain only the information contained in the most
useful incoming data points. Studying how rare a sample is becomes very complex with
kernels because we do not have the inverse function from Hilbert space to input space
to measure normality in physically meaningful units. Remember that, even though we
never actually map samples to Hilbert spaces explicitly, the model and the metric are
both deﬁned therein. However, we can actually estimate distances in Hilbert spaces
implicitly via reproducing kernels. For this, one only has to check if, given a previous
set of N examples xi, a test incoming point x∗is inside a suﬃciently big ball of mean
𝝓𝜇∈:
‖𝝓(x∗) −𝝓𝜇‖ ≥max
≤i≤N{‖𝝓(xi) −𝝓𝜇‖}.
(.)
Note that one has to estimate distances to empirical means in Hilbert spaces. On the
one hand, it is straightforward to show that we can compute distances between points
using kernels, as before:
d(xi, xj) = ‖𝝓(xi) −𝝓(xj)‖=
√
K(xi, xi) + K(xj, xj) −K(xi, xj).
(.)
On the other hand, one can readily deﬁne the distance to the empirical mean in as
𝝓𝜇= 
N
∑N
j=𝝓(xj), and we can compute it via kernels:
d(xi, 𝝓𝜇)∶= K(xi, xi) −
N
N
∑
j=
K(xi, xj) + 
N
N
∑
j=
N
∑
k=
K(xj, xk).
(.)
Therefore, we only have to test the following condition over x∗to check for largely
anomalous points in Hilbert spaces:
K(x∗, x∗) −
N
N
∑
j=
K(x∗, xj) + 
N
N
∑
j=
N
∑
k=
K(xj, xk) ≥max
≤i≤N{ d(xi, 𝝓𝜇)}.
(.)
Therefore, from an operational point of view, one only has to store a scalar 𝜃=
max≤i≤N d(xi, 𝝓𝜇) accounting for the maximum similarity (minimum distance)
among all the training samples, and then to check if the new example x∗is beyond
that threshold 𝜃. This procedure is known as the KOAD, and has been successfully
used in several applications, including the detection of anomalies in backbone routers
communications (Ahmed et al., ).

508
Digital Signal Processing with Kernel Methods
Difference Kernels for Change Detection
We can use the previous property of kernels to deﬁne particular kernels for change
detection. Let us now deﬁne a temporal classiﬁcation scenario, in which we aim to
detect the presence or absence of changes in the acquisition. This is a standard problem
in surveillance applications and change detection from satellite imagery (Camps-Valls
et al., ). A standard, extremely simple technique is called the change vector analysis
in which a threshold on the diﬀerence vector between two consecutive acquisitions
t and t −(i.e., d = xt −xt−) is applied to detect changes. This diﬀerence vector
can be formulated in the kernel feature space by deﬁning a proper kernel mapping
function:
K(xt
i, xt
j) = Kt(xt
i, xt
j) + Kt−(xt−
i
, xt−
j
) −Kt,t−(xt
i, xt−
j
) −Kt−,t(xt−
i
, xt
j).
We should note that the implementation is easy as it only involves application of
kernel functions to deﬁne the diﬀerential metric among observations. Such a diﬀerence
kernel can then be included in any supervised or unsupervised kernel classiﬁer.
11.2.2
Clustering in Feature Spaces
A complementary way to perform clustering in feature spaces is to follow the standard
kernelization procedure. Let us introduce in this section the main concepts involved
before deﬁning the following kernel methods. Clustering methods try to obtain parti-
tions of the data based on the optimization of a particular objective function, giving
as result separation hypersurfaces among clusters. To handle nonlinearly separable
clusters, the methods often deﬁne multiple centroids to provide a richer description
of the dataset at hand.
Notationally, let us consider a set of data points ∶= {x, … , xN} so xi ∈ℝd. The set
of centroids for this dataset is called a codebook, and is deﬁned as ∶= {v, … , vc},
with c ≪N. Each element of , vi ∈ℝd, is called a centroid or codevector. Each
centroid vi deﬁnes the so-called Voronoi region i as the set of vectors fulﬁlling the
following condition (Aurenhammer, ): i = {z ∈ℝd|i = arg minj‖z −vi‖}. Each
Voronoi region is convex and its boundaries are linear segments. A further deﬁnition
we need is the Voronoi set, i, of the centroid vi, being the subset of for which the
centroid vi is the nearest vector; that is, i = {x ∈|i = arg minj‖x −vj‖}. A
partition of ℝd induced by all Voronoi regions is called a Voronoi tessellation or Dirichlet
tessellation (Aurenhammer, ).
Kernel k-Means
Let us start by summarizing the formulation and optimization of the standard k-means,
and then we will show how to kernelize it. The goal of the canonical k-means is to
construct a Voronoi tessellation by moving k centroids to the arithmetic mean of their
Voronoi sets. To achieve this, the k-means algorithm searches the elements of that
jointly minimize the empirical quantization error
We should note that this kernel raises some issues about its validity because it may become non-positive
deﬁnite. In such cases, extra regularization can be needed to make it positive deﬁnite.

Clustering and Anomaly Detection with Kernels
509
E() =

N
k∑
i=
∑
x∈i
‖x −vi‖,
(.)
which can be achieved if each centroid vi fulﬁlls the centroid condition
∑
x∈i
𝜕‖x −vi‖
𝜕vi
= .
(.)
In the case of using the Euclidean distance, such a condition reduces to
vi =

|i|
∑
x∈i
x,
(.)
where |i| denotes the cardinality of i. The k-means algorithm is deﬁned by the
following steps: () choose the number k of clusters; () initialize with a set of centroids
vi randomly selected; () compute the Voronoi set i for each centroid vi; () move
each centroid vi to the mean of i using Equation .; and () stop the algorithm if
no changes are observed, otherwise go to step . Note that the k-means algorithm is an
example of an EM algorithm. Step is the expectation, and step is the maximization.
The convergence of the k-means algorithm is guaranteed since each EM algorithm is
always convergent to a local minimum (Bottou and Bengio, ).
In order to obtain the kernel k-means algorithm, we just translate the deﬁnitions and
concepts before to a feature space. The ﬁrst step is mapping our dataset to a high-
dimensional feature space using a nonlinear map 𝜙. The codebook is deﬁned in the
feature space as V𝜙= {𝝓(v), … , 𝝓(vk)}. We have a Voronoi region in the feature space
deﬁned by 𝜙
i = {𝝓(x) ∈F|i = arg minj‖𝝓(x) −𝝓(vj)‖}, and the Voronoi set i
𝜙of the
centroid 𝝓(vi) deﬁned as i
𝜙= {x ∈|i = arg minj‖𝝓(x) −𝝓(vj)‖}. In this case, the
Voronoi regions induce a Voronoi tessellation in the feature space.
The steps of the k-means algorithm in the feature space are the same as the ones
described in Section ... The only diﬀerence is that the maximization step is com-
puted using the following equation:
𝝓(vi) =

|i
𝜙|
∑
x∈i
𝜙
𝝓(x).
(.)
The problem is that we do not explicitly know 𝝓and cannot move the centroid using
Equation .. This issue is solved using the representer theorem (Schölkopf and Smola,
), so that we can write each centroid in the feature space as a linear combination
of the mapped data vectors, 𝝓(vi) = ∑N
j=𝛼ij𝝓(xj). By replacing the expansion into
Equation ., and noting that 𝛼ij should be zero if xj ∉i
𝜙, we obtain
‖𝝓(xi) −𝝓(vj)‖
= ‖𝝓(xi) −
N
∑
j=
𝛼ij𝝓(xj)‖
(.)
= K(xi, xi) −
N
∑
j=
𝛼ijK(xi, xj) +
N
∑
l=
N
∑
m=
𝛼il𝛼imK(xl, xm),

510
Digital Signal Processing with Kernel Methods
which allows us to compute the closest feature space centroid for each sample, and
update the coeﬃcients 𝛼ij accordingly. As in the standard k-means algorithm, the
procedure is repeated until all 𝛼ij do not change signiﬁcantly.
Kernel Fuzzy Clustering
Fuzzy clustering is essentially dealing with the elusive deﬁnition of hard and fuzzy
partitions (Pal et al., ). Clustering solutions, such those oﬀered by k-means, do
not oﬀer disambiguation of cluster membership of samples that do not ﬁt exactly to
one single cluster but to several clusters in diﬀerent degrees. Let us deﬁne a fuzzy-c
partition (or clustering) as
∶=
{
𝐔
||||
≤Uij ≤
∀i, j;
s.t.
c∑
i=
Uij = ∀j; <
N
∑
j=
Uij < n , ∀i
}
,
(.)
where the set can be deﬁned with the membership matrix 𝐔of size c × N, with ≤
c < N and Uij ∈ℝ. Each element Uij is the membership of the jth pattern to the ith
cluster, and the constraints ensure that () the sum of the membership of a pattern to all
clusters is one (probabilistic constraint), and () a cluster cannot by empty or contain
all samples.
Now, given a codebook and a membership set , the fuzzy c-means algorithm (Pal
et al., ) minimizes the functional
J(, ) =
c∑
i=
N
∑
j=
Um
ij ‖xj −vi‖,
(.)
subject to ∑c
i=Uij = , ∀j. The parameter m establishes the fuzziness of the membership.
For m = the classic k-means hard clustering algorithm is obtained, whereas large
values of m tend to set all memberships the same, and hence no structure is found on
the data.
The functional in Equation .is minimized deﬁning a Lagrangian function for each
sample xj:
Lj =
c∑
i=
Um
ij ‖xj −vi‖+ 𝛼j
( c∑
i=
Uij −
)
,
(.)
and deriving with respect to vi and Uij and setting to zero we obtain the following two
equations:
vi =
∑N
j=Um
ij xj
∑N
j=Um
ij
,
(.)
Uij =
(‖xj −vi‖)−∕(m−)
∑c
h=(‖xj −vh‖)−∕(m−) .
(.)

Clustering and Anomaly Detection with Kernels
511
This pair of equations is used with a Picard iteration method (Pal et al., ) composed
of two steps. In the ﬁrst step, the membership variables Uij are kept ﬁxed and the code-
vectors vi are optimized. Then, the second step optimizes the membership variables
while the codevectors are ﬁxed. These two steps are repeated until the variables change
less than a predeﬁned threshold. Equation .shows that the fuzzy membership of an
input sample decreases as the distance increases. The sum in the denominator acts as a
normalization factor.
Next, we show two approaches to obtain a kernel version of the fuzzy clustering
algorithm. One is based in the kernelization of the metric, and the other directly solves
the equations of fuzzy c-means in feature space.
Kernel Fuzzy Clustering Based on the Metric Kernelization
The functional to minimize is the same as in Equation .but in a high-dimensional
feature space:
J𝝓(, ) =
N
∑
j=
c∑
i=
Um
ij ‖𝝓(xj) −𝝓(vi)‖,
(.)
subject to ∑Uij = , ∀i. In order to minimize this cost function, we take derivatives with
respect to vj and Uij. To be able to apply the kernel trick and obtain a practical solution,
we explicitly compute the derivatives of the kernel function, which for the RBF kernel
the result is as follows:
𝜕K(xj, vi)
𝜕vi
= −
(xj −vi)
𝜎
K(xj, vi).
Computing all the derivatives and equating them to zero we obtain the following
equations for the Picard iteration method:
vi =
∑N
j=Um
ij K(xj, vi)xj
∑N
j=Um
ij K(xj, vi)
,
(.)
Uij =
(−K(xj, vi))−∕(m−)
∑c
h=(−K(xj, vh))−∕(m−) ,
(.)
which are now solely based on evaluating kernel functions, and hence the method can
be easily implemented.
Kernel Fuzzy Clustering in Feature Space
The functional to minimize is shown in Equation .. Instead of obtaining a solution
taking derivatives using vi and Uij, now we use the representer theorem to express 𝝓(vi)
as a linear combination of samples in feature space; that is, 𝝓(vi) = ∑N
h=𝛽ih𝝓(xh). This
will allow us to compute the distance appearing in Equation .just in terms of the
kernel function as in Equation .. Therefore, let us rewrite the functional as

512
Digital Signal Processing with Kernel Methods
J𝝓(, ) =
N
∑
j=
c∑
i=
Um
ij (K(xj, xj) −𝜷ikT
j + 𝜷iK𝜷T
i ),
(.)
where kj is the jth row of the kernel; that is, kj ∶= [K(xj, x), … , K(xj, xN)]T, and
𝜷
= [𝛽, … , 𝛽N] are the weights for the 𝝓(v) codevector, and K is the whole kernel
matrix. This functional can be derived with respect to the variables 𝜷i and Uij, obtaining
the equations we need for the Picard iteration method:
𝜷i =
∑N
j=Um
ij kjK −
∑N
j=Um
ij
,
(.)
Uij =
(K(xj, xj) −𝜷ikT
j + 𝜷iK𝜷T
i )−∕(m−)
∑c
h=(K(xj, xj) −𝜷hkT
j + 𝜷hK𝜷T
h)−∕(m−) .
(.)
There is an alternative pair of equivalent equations that we can obtain using the
following reasoning. Deriving the functional in Equation .directly with respect to
𝝓(vi) we will obtain, similarly to Equation ., the expansion
𝝓(vi) =
∑N
j=Um
ij 𝝓(xj)
∑N
j=Um
ij
= ai
N
∑
j=
Um
ij 𝝓(xj),
(.)
where we have deﬁned ai as
a−
i
=
N
∑
h=
(uih)m.
(.)
With Equations .and .we can rewrite Equation .as
Uij =
(K(xj, xj) −aiuikT
j + uiKuT
i )−∕(m−)
∑c
h=(K(xj, xj) −ahuhkT
j + uhKuT
h)−∕(m−) ,
(.)
where ui ∶= [(ui)m, … , (uin)m]T. It is clear that Equations .and .are equiv-
alent, noting that, by deﬁnition, 𝛽ij = aiUm
ij . However, the Picard iteration method is
easier to implement using Equations .and ..
Kernel Self-Organizing Maps
A self-organized map (SOM) (Kohonen, ) is a type of artiﬁcial NN trained to
obtain a low-dimensional (most of the times D) representation of the input data. The
training phase is unsupervised where SOM deﬁnes a function that tries to preserve
the topological properties of the input dataset. SOMs are typically used to obtain
low-dimensional visualizations of high-dimensional data. As for other NNs, an SOM
consists of a series of interconnected nodes or neurons. Each node has a weight vector
of the same dimension as the input data samples, and a location in the map space, while

Clustering and Anomaly Detection with Kernels
513
each vector of the input space is placed on the map space by ﬁnding the closest/nearest
node in the map space.
Let us deﬁne our coodebook, denoted , as the set of neurons (centroids) that will
compose the map. The ﬁrst step of the training phase is to randomly select the weights
of the neurons in . Alternatively, Kohonen () showed that it is possible to speed up
the training phase by evenly sampling the subspace spanned by the two largest principal
component eigenvectors, instead of randomly selecting the ﬁrst neurons’ weights. The
second step is to take an input sample x from and obtain the nearest neuron using
a distance function d (Euclidean of Manhattan distances are the most often used). The
shortest distance would be the one that solves minvj∈V ‖x −vj‖. Then the weights of the
neurons are adapted using a standard gradient descent strategy:
vj(t + ) = vj(t) + 𝜀(t)h(d)(x −vj),
(.)
where h is a decreasing function of the distance d; for example, h(d) = exp(−d∕(𝜎(t))).
Here, 𝜎(t) is a decreasing function of t, 𝜎(t) = 𝜎i(𝜎f∕𝜎i)t∕tmax, 𝜀(t) is also a decreasing
function of t, 𝜀(t) = 𝜀i(𝜀f∕𝜀i)t∕tmax, and where the subscripts “i” and “f” indicate the
initial and ﬁnal values respectively. The adaptation of neurons vj in Equation .is
repeated until a predeﬁned number of iterations is achieved; that is, t = tmax.
In order to reformulate the SOM in the feature space, we need to ﬁrst deﬁne our set of
initial neurons 𝝓(vj). Since we do not know 𝜙explicitly, we use the representer theorem
to write each neuron as a combination of mapped input samples:
𝝓(vj) =
N
∑
l=
𝛼jl𝝓(xl),
(.)
and we randomly initialize the coeﬃcients 𝛼jl. Then, given an input sample 𝝓(xi), we
need to compute the nearest neuron to it:
arg min
𝝓(vj)∈V
{
‖𝝓(xi) −𝝓(vj)‖
}
,
which, after applying the kernel trick, translates into
arg min
𝝓(vj)∈V
(
K(xi, xi) −
∑
l
𝛼jlK(xi, xl) +
∑
r
∑
s
𝛼jr𝛼jsK(xr, xs)
)
.
(.)
Finally, the neurons in the feature space are updated as
𝝓(vj(t + )) = 𝝓(vj(t)) + 𝜀(t)h(d)(𝝓(x) −𝝓(vj)),
(.)
which according to Equation .can be expressed as
N
∑
l=
𝛼jl(t + )𝝓(xl) =
N
∑
l=
𝛼jl𝝓(xl) + 𝜀(t)h(d)
(
𝝓(xi) −
N
∑
l=
𝛼jl𝝓(xl)
)
,
(.)

514
Digital Signal Processing with Kernel Methods
from which we can now obtain the update rule for 𝛼jl as follows:
𝛼jl(t + ) =
{ 𝛼jlt(−𝜀(t)h(d)),
if
i ≠j,
𝛼jlt(−𝜀(t)h(d)) + 𝜀(t)h(d),
otherwise.
(.)
Note that the updating rule can be obtained iteratively until convergence.
11.3
Domain Description Via Support Vectors
As an alternative to the previous approaches for clustering with kernels, the description
of the domain can be done via support vectors. Instead of directly kernelizing the
metric or the clustering algorithm, one attempts here to describe the data distribution
in feature spaces with a minimum volume hypersphere enclosing all data points but the
outliers, which translates into a hypersurface in the original input space. This idea leads
to several algorithms, such as the one-class SVM (OC-SVM) and the related support
vector domain description (SVDD). After computing the ball with these methods, the
SVC method permits one to assign labels to patterns in input space enclosed by the
same surface. In the next subsections we will review the formulation of these three main
approaches.
11.3.1
Support Vector Domain Description
A diﬀerent problem statement for classiﬁcation is given by the SVDD (Tax and Duin,
). The SVDD is a method to solve one-class problems, where one tries to describe
one class of objects, distinguishing them from all other possible objects.
The problem notation is deﬁned as follows. Let {xi}N
i=be a dataset belonging to a
given class of interest. The purpose is to ﬁnd a minimum volume hypersphere in a high-
dimensional feature space , with radius R > and center a ∈, which contains most
of these data objects (Figure .a). Since the training set may contain outliers, a set of
slack variables 𝜉i ≥is introduced, and the problem becomes
min
R,a
{
R+ C
N
∑
i=
𝜉i
}
(.)
constrained to
‖𝝓(xi) −𝐚‖≤R+ 𝜉i
∀i = , … , N
(.)
𝜉i ≥
∀i = , … , N,
(.)
where parameter C controls the trade-oﬀbetween the volume of the hypersphere and
the permitted errors. Parameter 𝜈, deﬁned as 𝜈= ∕(NC), can be used as a rejection
fraction parameter to be tuned, as noted by Schölkopf et al. ().
The dual functional is a QP problem that yields a set of Lagrange multipliers (𝛼i)
corresponding to constraints in Equation .. When free parameter C is adjusted
properly, most of the 𝛼i are zero, giving a sparse solution. The Lagrange multipliers are
also used to calculate the distance from a test point to the center R(𝐱∗):

Clustering and Anomaly Detection with Kernels
515
O
Hypersphere in feature Hilbert space H
(a)
(b)
a
R
xi
ξi
xi
ξi
O
Hyperplane in feature Hilbert space H
Figure 11.1 Illustration of the one-class kernel classifiers. (a) In the SVDD, the hypersphere containing
the target data is described by center a and radius R, and samples in the boundary and outside the
ball are unbounded and bounded support vectors, respectively. (b) In the case of the OC-SVM, all
samples from the target class are mapped with maximum distance to the origin.
R(𝐱∗) = K(𝐱∗, 𝐱∗) −
N
∑
i=
K(xi, 𝐱∗) +
N
∑
i,j=
K(xi, xj),
(.)
which is compared with ratio R. Unbounded support vectors are those samples xi
satisfying ⩽𝛼i < C, while bounded support vectors are samples whose associated
𝛼i = C, which are considered outliers.
11.3.2
One-Class Support Vector Machine
In the OC-SVM, instead of deﬁning a hypersphere containing all examples, a hyperplane
that separates the data objects from the origin with maximum margin is deﬁned
(Figure .b). It can be shown that, when working with normalized data and the RBF
Gaussian kernel, both methods yield the same solution (Schölkopf et al., ).
Notationally, in the OC-SVM, we want to ﬁnd a hyperplane w which separates
samples xi from the origin with margin 𝜌. The problem thus becomes
min
w,𝜌,𝜉
{

‖w‖−𝜌+ 
𝜈N
N
∑
i=
𝜉i
}
(.)
constrained to
⟨w, 𝝓(xi)⟩≥𝜌−𝜉i
∀i = , … , N
(.)
𝜉i ≥
∀i = , … , N.
(.)
The problem is solved through its Lagrangian dual introducing a set of Lagrange
multipliers 𝛼i. The margin 𝜌can be computed as 𝜌= ⟨w, 𝝓(xi)⟩= ∑
j 𝛼jK(xi, xj).

516
Digital Signal Processing with Kernel Methods
11.3.3
Relationship Between Support Vector Domain Description
and Density Estimation
Let us now point out the connection between pdf estimation and the previous one-
class kernel classiﬁers. The goal of kernel density estimation (KDE) is to estimate the
unknown pdf p(x) given N i.i.d. samples {x, … , xN} drawn from it. The kernel (Parzen)
estimate of the pdf using an arbitrary kernel function K𝜎(⋅) parameterized with a length-
scale parameter 𝜎is given by
̂p(x) =

N𝜎
N
∑
i=
K
(x −xi
𝜎
)
,
(.)
where K is typically a Gaussian kernel, and the hyperparameters of the model (e.g.,
the length scale or kernel width) can be obtained via ML estimation or ﬁxed through
ad hoc rule of thumb rules, such as Silverman’s rule. Other kernel functions can be
used: uniform, normal, triangular, bi-weighted, tri-weighted, just to name a few. The
Epanechnikov kernel, for example, is optimal in an MSE sense. The interested reader can
explore the MATLAB function ksdensity.m and the MATLAB KDE toolbox available
oin the book’s web page.
Density estimates can be used to characterize distributions and to design anomaly
detection schemes. Note, for instance, that once ̂p(x) is obtained, an anomaly detector
can be readily deﬁned simply as
(x∗) = −̂p(x∗).
(.)
Now, by plugging the previous Gaussian kernel estimate in here and deﬁning a centroid
in the feature space as the mean of the mapped samples, a KDE-based anomaly detector
can be readily derived by simply kernelizing the distance between a test point x∗and all
the training points (see Section ..):
(x∗) = constant −
N
N
∑
i=
K(x∗, xi).
(.)
Note that deﬁnition of SVDD seen previously in Equations .and .leads to an
SVDD anomaly detector in the form
(x) = ‖𝝓(x) −a‖= constant −
∑
i
𝛼iK(x, xi),
(.)
where 𝛼i > are support vectors. Comparing Equation .with Equation .
we see they are very similar, except that the SVDD anomaly detector places unequal
weights on the training points xi. In other words, while the kernel KDE works as an
unsupervised method where the centroid is placed at 𝝁𝜙= (∕N) ∑N
i=𝝓(xi), the SVDD
is a supervised method that uses training (labeled) samples to place the centroid at
a = ∑
i 𝛼i𝝓(xi). Figure .shows a toy example comparing the boundaries obtained
by the SVDD (centroid at 𝐚) and KDE (centroid at 𝝁𝜙).

Clustering and Anomaly Detection with Kernels
517
Figure 11.2 Boundary comparison
between SVDD (sphere centered at 𝐚)
and KDE (sphere centered at 𝝁𝜙).
O
a+
θφ
+
11.3.4
Semi-supervised One-Class Classification
Exploiting unlabeled information in one-class classiﬁers can be done via semi-
supervised versions of the OC-SVM. A particularly convenient instantiation is the
biased-SVM (b-SVM) classiﬁer, which was originally proposed by Liu et al. ()
for text classiﬁcation using only target and unlabeled data. The underlying idea of the
method lies on the assumption that if the sample size is large enough, minimizing the
number of unlabeled samples classiﬁed as targets while correctly classifying the labeled
target samples will give an accurate classiﬁer.
Let us consider again the dataset {xi}l+u
i=∈ℝd made up of l labeled samples and
u unlabeled samples (N = l + u). The l labeled samples all belong to the same class:
the target class. Concerning unlabeled samples, they are treated by the algorithm as
being all outliers, although their class is unknown. This characteristic makes b-SVM a
sort of semi-supervised classiﬁer. Under these assumptions, the b-SVM formulation is
deﬁned as
min
w,𝜉
{

‖w‖+ Ct
l∑
i=
𝜉i + Co
l+u
∑
i=l+
𝜉i
}
∀i = , … , N
(.)
subject to
⟨w, 𝝓(xi)⟩≥−𝜉i,
(.)
where 𝜉i ≥are slack variables, and Ct and Co are the costs assigned to errors on target
and outlier (unlabeled) classes, respectively. The two cost values should be adjusted to
fulﬁll the goal of classifying the target class correctly while at the same time trying to
minimize the number of unlabeled samples classiﬁed as target class. To achieve this
goal, Ct should have a large value, as initially we trust our labeled training set, and Co a
small value, because it is unknown whether the samples unlabeled are actually targets
or outliers.

518
Digital Signal Processing with Kernel Methods
11.4
Kernel Matched Subspace Detectors
Target detection from data is of great interest in many applications. Detecting targets
is typically described as a two-step methodology: ﬁrst, a speciﬁc detector identiﬁes
anomalies; second, a classiﬁer is aimed at identifying whether the anomaly is a target
or natural clutter. In many application domains, this step is only possible if the target
signature is known, which can be obtained from a library of “pure elements” or
learned from data by using subspace matched ﬁlters. Several techniques have been
proposed in the literature, such as the Reed–Xiaoli anomaly detector (Reed and Yu,
), the orthogonal subspace projection (OSP) (Harsanyi and Chang, ), the
Gaussian mixture model (GMM) (Stein et al., ), the cluster-based detector (Stein
et al., ), or the signal subspace processor (Ranney and Soumekh, ). In recent
years, many detection algorithms based on spectral matched (subspace) ﬁlters have
been reformulated under the kernel methods framework: matched subspace detec-
tor, orthogonal subspace detector, as well as adaptive subspace detectors (Kwon and
Nasrabadi, a). Certainly, the use of kernel methods alleviates several key problems
and oﬀers many advantages: they combat the high-dimensionality problem, make the
method robust to noise, and allow for ﬂexible nonlinear mappings with controlled
(regularized) complexity (Shawe-Taylor and Cristianini, ).
This section deals with target/anomaly detection and its corresponding kernelized
extensions under the viewpoint of matched subspace detectors. This topic has been
very active in the last decade, and also in the particular ﬁeld of signal and image
anomaly detection. We pay attention to diﬀerent kernel anomaly subspace detectors:
KOSP, KSAM and the family of KACD that generalizes several anomaly detectors under
Gaussian and EC distributions.
11.4.1
Kernel Orthogonal Subspace Projection
The OSP algorithm (Harsanyi and Chang, ) tries to maximize the SNR in the
subspace orthogonal to the background subspace. OSP has been widely adopted in the
community of automatic target recognition because of its simplicity (only depends on
the noise second-order statistics) and its good behavior. In the standard formulation of
the OSP algorithm (Harsanyi and Chang, ), a linear mixing model is assumed for
each d-dimensional observed point r as follows:
r = M𝜶+ n,
where M is the matrix of size (d×p) containing the p endmembers (i.e., pure constituents
of the observed mixture) contributing to the mixed point r, 𝜶∈ℝp×contains the
weights in the mixture (sometimes called abundance vector), and n ∈ℝd×stands for an
additive zero-mean Gaussian noise vector. In order to identify a particular desired point
d in a data set (e.g., a pixel in an image) with corresponding abundance 𝛼p, the earlier
expression can be organized by rewriting the M matrix in two submatrices M = [U|d]
and 𝜶= [𝛾, 𝛼p]T, so that
r = U𝜸+ d𝛼p + n,
(.)

Clustering and Anomaly Detection with Kernels
519
Outliers
Background
Outliers
Residuals
Singular Value Decomposition (SVD)
Subspace projection for 
background suppression
Figure 11.3 The standard (linear) OSP method first performs a linear transformation that looks for
projections that identify the subspace spanned by the background, and then the background
suppression is carried out by projecting the data onto the subspace orthogonal to the one spanned
by the background components. The kernel version of this algorithm consists of the same procedure
yet performed in the kernel feature space.
where the columns of U represent the undesired points and 𝜸contains their abun-
dances. The OSP operator that maximizes the SNR performs two steps. First, an
annihilating operator rejects the background points so that only the desired point
should remain in the data subspace. This operator is given by the (d × d) matrix P⟂
U =
I −UU†, where U† is the right Moore–Penrose pseudoinverse of U. The second step
of the OSP algorithm is represented by the ﬁlter w that maximizes the SNR of the ﬁlter
output; that is, the matched ﬁlter w = kd, where k is an arbitrary constant (Harsanyi and
Chang, ; Kwon and Nasrabadi, b). The OSP detector is given by qT
OSP = dTP⟂
Ur,
and the output of the OSP classiﬁer is
DOSP = qT
OSPr = dTP⟂
Ur.
(.)
By using the singular-value decomposition of U = B𝜮AT, where B is the eigenvector
of UUT = B𝜮𝜮TBT and A is the eigenvector of UTU = A𝜮T𝜮A. The annihilating
projector operator becomes P⟂
U = I −BBT, where the columns of B are obtained from
the eigenvectors of the covariance matrix of the background samples. This ﬁnally yields
the expression
DOSP = dT(I −BBT)r.
(.)
Figure .shows the eﬀect of the OSP algorithm on a toy data set.
A nonlinear kernel-based version of the OSP algorithm can be devised by deﬁning
a linear OSP in an appropriate Hilbert feature space where samples are mapped to
through a kernel mapping function 𝝓(⋅). Similar to the previous linear case, let us deﬁne
the annihilating operator now in the feature space as P⟂
𝝓= I𝝓−B𝝓BT
𝝓. Then, the output
of the OSP classiﬁer in the feature space is now trivially given by
DKOSP = 𝝓(d)T(I𝝓−B𝝓BT
𝝓)𝜱(r).
(.)

520
Digital Signal Processing with Kernel Methods
The columns of matrix B𝝓(e.g., denoted as bj
𝝓) are the eigenvectors of the covariance
matrix of the undesired background signatures. Clearly, each eigenvector in the feature
space can be expressed as a linear combination of the input vectors in the feature
space transformed through the function 𝝓(⋅). Hence, one can write bj
𝝓= 𝜱b𝜷j –
where 𝜱b contains the mapped background points Xb, and 𝜷j are the eigenvectors of
the (centered) Gram matrix K bb′, whose entries are Kbb(bi, bj) = 𝝓(bi)T𝝓(bj), and bi
are background points.The kernelized version in Equation .is given (Kwon and
Nasrabadi, b) by
DKOSP = kT
m,d𝜰𝜰Tkm,r −kT
b,dTkb,r,
(.)
where k are column vectors referred to as the empirical kernel maps and the subscripts
indicate the elements entering the corresponding kernel function: d for the desired
target, r the observed point, and subscript m indicates the concatenation of both; that
is, Xm = [Xb|d]. Also is the matrix containing the eigenvectors 𝜷j described earlier;
and 𝜰is the matrix containing the eigenvectors 𝝊j, similar to 𝜷j yet obtained from the
centered kernel matrix K m,m thus working with the expanded matrix Xm. Listing .
illustrates the simplicity of the code for the KOSP method.
function [D,sigmas] = KOSP(patterns,target,background)
ss = logspace(-2,2,21);j = 0;
for sigma = ss
j = j+1;sigmas(j) = sigma; Xbd = [background;target];
K_mm = kernelmatrix('rbf',Xbd',Xbd',sigma);
K_bb = kernelmatrix('rbf',background',background',sigma);
K_m = kernelmatrix('rbf',Xbd',target',sigma);
K_mr = kernelmatrix('rbf',Xbd',patterns',sigma);
K_bd = kernelmatrix('rbf',background',target',sigma);
K_br = kernelmatrix('rbf',background',patterns',sigma);
[Gamma values] = eig(K_mm);
Gamma = Gamma./repmat(diag(values)',size(Gamma,1),1);
[Beta values] = eig(K_bb);
Beta = Beta./repmat(diag(values)',size(Beta,1),1);
D(j,:) = (K_md'*(Gamma*Gamma')*K_mr)-(K_bd'*(Beta*Beta')*K_br);
end
Listing 11.1 MATLAB code for the KOSP method (KOSP.m).
11.4.2
Kernel Spectral Angle Mapper
The spectral angle mapper (SAM) is a method for directly comparing image spec-
tra (Kruse et al., ). Since its seminal introduction, SAM has been widely used
in chemometrics, astrophysics, imaging, hyperspectral image analysis, industrial engi-
neering, and computer vision applications (Ball and Bruce ; Cho et al. ; García-
Allende et al. ; Hecker et al. ), just to name a few. The measure achieved
widespread popularity thanks to its implementation in software packages, such as
ENVI or ArcGIS. The reason is simple: SAM is invariant to (unknown) multiplicative
See Chapter for the implementation of the centering and scaling operations in kernel methods.

Clustering and Anomaly Detection with Kernels
521
scalings of spectra due to diﬀerences in illumination and angular orientation (Keshava,
). SAM is widely used for fast spectral classiﬁcation, as well as to evaluate the
quality of extracted endmembers, and the spectral quality of an image compression
algorithm.
The popularity of SAM is mainly due to its simplicity and geometrical interpretability.
However, the main limitation of the measure is that it only considers second-order angle
dependencies between spectra. In the following, we will see how to generalize the SAM
measure to the nonlinear case by means of kernels (Camps-Valls and Bruzzone, ).
This short note aims to characterize KSAM theoretically, and to show its practical
convenience over the widespread linear counterpart. KSAM is very easy to compute,
and can be used in any kernel machine. KSAM also inherits all properties of the original
distance, is a valid reproducing kernel, and is universal. The induced metric by the kernel
will be easily derived by traditional tensor analysis, and tuning the kernel hyperparam-
eter of the squared exponential kernel function used is shown to be stable. KSAM is
tested here in a target detection scenario for illustration purposes. The spatially explicit
detection and metric maps give us useful information for semiautomatic anomaly/target
detection.
Given two d-dimensional samples, x, z ∈ℝd, the SAM measures the angle 𝜃between
them:
𝜃= arccos
(
xTz
‖x‖‖z‖
)
,
≤𝜃≤π
.
(.)
KSAM requires the deﬁnition of a feature mapping 𝝓(⋅) to a Hilbert space endorsed
with the kernel reproducing property. Now, if we simply map the original data to with
a mapping 𝝓∶ℝd →, the SAM can be expressed in as
𝜃K = arccos
(
𝝓(x)T𝝓(z)
‖𝝓(x)‖‖𝝓(z)‖
)
,
≤𝜃K ≤π
.
(.)
Having access to the coordinates of the new mapped data is not possible unless one
explicitly deﬁnes the mapping function 𝝓. It is possible to compute the new measure
implicitly via kernels:
𝜃K = arccos
(
K(x, z)
√
K(x, x)
√
K(z, z)
)
,
≤𝜃K ≤π
.
(.)
Popular examples of reproducing kernels are the linear kernel K(x, z) = xTz, the
polynomial K(x, z) = (xTz + )d, and the RBF kernel K(x, z) = exp(−(∕𝜎)‖x −z‖).
In the linear kernel, the associated RKHS is the space ℝd itself and KSAM reduces to
the standard linear SAM. In polynomial kernels of degree d, KSAM eﬀectively only
compares moments up to order d. For the Gaussian kernel, the RKHS is of inﬁnite
dimension and KSAM measures higher order feature dependences. In addition, note
that for RBF kernels, self-similarity K(x, x) = , and thus the measure simply reduces
to 𝜃K = arccos(K(x, z)), ≤𝜃K ≤π∕. Following the code in Listing ., we show a
simple (just one line) snippet of KSAM in Listing ..

522
Digital Signal Processing with Kernel Methods
[errKRadians] = acos(kernelmatrix('rbf',target', patterns',sigma));
Listing 11.2 MATLAB code for the KSAM method (KSAM.m).
11.5
Kernel Anomaly Change Detection
Anomalous change detection (ACD) diﬀers from standard change detection in that the
goal is to ﬁnd anomalous or rare changes that occurred between two datasets. The
distinction is important both statistically and from the practitioner’s point of view. Sta-
tistically speaking, standard change detection emphasizes all the diﬀerences between
the two data distributions, while ACD is more interested in modeling accurately the
tails of the diﬀerence distribution.
The interest in ACD is vast, and many methods have been proposed in the litera-
ture, ranging from regression-based approaches like in the chronocrome (Schaum and
Stocker, ), where big residuals are associated with anomalies, to equalization-
based approaches that rely on whitening principles (Mayer et al., ), as well as
multivariate methods (Arenas-García et al., ) that reinforce directions in feature
spaces associated with noisy or rare events (Green et al., ; Nielsen et al., ).
Theiler and Perkins () formalized the main diﬀerences between the two ﬁelds and
introduced a framework for ACD. The framework assumes data Gaussianity, yet the
derived detector delineates hyperbolic decision functions. Even though the Gaussian
assumption reports some advantages (e.g., problem tractability and generally good
performance) it is still an ad hoc assumption that it is not necessarily fulﬁlled in
practice. This is the motivation in Theiler et al. (), where the authors introduce
EC distributions that generalize the Gaussian distribution and prove more appropriate
to modeling fatter tail distributions and thus detect anomalies more eﬀectively. The
EC decision functions are pointwise nonlinear and still rely on second-order feature
relations. In this section we will see the extension of the methods in Theiler et al. ()
to cope with higher order feature relations through the theory of reproducing kernels.
11.5.1
Linear Anomaly Change Detection Algorithms
A family of linear ACD can be framed using solely cross-covariance matrices, as
illustrated in Theiler et al. (). Notationally, given two samples x, y ∈ℝd, the
decision of anomalousness is given by
(x, y) = 𝜉z −𝛽x𝜉x −𝛽y𝜉y,
where 𝜉z = zTC−
z z, 𝜉x = xTC−
x x, and 𝜉y = yTC−
y y, z = [x, y] ∈ℝd, and Cz = ZTZ,
Cx = XTX, and Cy = Y TY are the estimated covariance matrices with the available data.
Parameters 𝛽x, 𝛽y ∈{, } and the diﬀerent combinations give rise to diﬀerent anomaly
detectors (see Table .). These methods and some variants have been widely used
in image analysis settings because of their simplicity and generally good performance
(Chang and Lin ; Kwon et al. ; Reed and Yu ).
However, these methods are hampered by a fundamental problem: the assumption of
Gaussianity that is implicit in the formulation. Accommodating other data distributions
may not be easy in general. Theiler et al. () introduced an alternative ACD to cope

Clustering and Anomaly Detection with Kernels
523
Table 11.1 A family of ACD algorithms (HACD:
hyperbolic ACD).
ACD algorithm
𝜷x
𝜷y
RX


Chronocrome y|x


Chronocrome x|y


HACD


with EC distributions (Cambanis et al., ): roughly speaking, the idea is to model the
data using the multivariate Student distribution. The formulation introduced in (Theiler
et al., ) gives rise to the following decision of EC anomalousness:
EC(x, y)
= (d + 𝜈) log
(
+
𝜉z
𝜈−
)
−𝛽x(d + 𝜈) log
(
+
𝜉x
𝜈−
)
−𝛽y(d + 𝜈) log
(
+
𝜉y
𝜈−
)
,
(.)
where 𝜈controls the shape of the generalized Gaussian: for 𝜈→∞the solution
approximates the Gaussian and for 𝜈→it diverges.
11.5.2
Kernel Anomaly Change Detection Algorithms
Note that the previous methods solely depend on estimating covariance matrices with
the available data, and use them as a metric for testing. The methods are fast to
apply, delineate pointwise nonlinear decision boundaries, but still rely on second-order
statistics. We solve the issue as usual using reproducing kernel functions.
For the sake of simplicity, we just show how to estimate the anomaly term 𝜉x in the
Hilbert space, 𝜉
x . The other terms are derived in the same way. Notationally, let us map
all the observations to a higher dimensional Hilbert feature space by means of the
feature map 𝝓∶x →𝝓(x). The mapped training data matrix X ∈ℝN×d is now denoted
as 𝜱∈ℝN×d. Note that one could think of diﬀerent mappings: 𝝓∶x →𝝓(x) and
𝝍∶y →𝝍(y). However, in our case, we are forced to consider mapping to the same
Hilbert space because we have to stack the mapped vectors to estimate 𝜉z; that is, = .
The mapped training data to Hilbert spaces are denoted as 𝜱, 𝜳∈ℝN×drespectively.
In order to estimate 𝜉
x for a test example x∗, we ﬁrst map the test point 𝝓(x∗) and then
apply
𝜉
x = 𝝓(x∗)T(𝜱T𝜱)−𝝓(x∗).
Note that we do not have access to either the samples in the feature space or the
covariance. We can, nevertheless, estimate the eigendecomposition of the covariance

524
Digital Signal Processing with Kernel Methods
matrix in kernel feature space; that is, C= 𝜱T𝜱= V 𝜦V T
, where 𝜦is a diagonal
matrix containing the eigenvalues and V contains the eigenvectors in columns. It
is worth noting that the maximum number of eigenvectors equals the number of
examples used N. Plugging the eigendecomposition of the covariance pseudoinverse
C†
= V 𝜦−
V T
, and after some linear algebra, one can express the term as
𝜉
x = 𝝓(x∗)T𝜱T (𝜱𝜱T𝜱𝜱T)−𝜱𝝓(x∗).
We can replace all dot products by reproducing kernel functions using the representer
theorem (Kimeldorf and Wahba, ; Shawe-Taylor and Cristianini, ), and hence
𝜉
x
= kT
∗(KK)−k∗, where k∗∈ℝN×contains the similarities between x∗and all
training points X; that is, k∗∶= [K(x∗, x), … , K(x∗, xN)]T, and K ∈ℝN×N stands for
the kernel matrix containing all training data similarities. The solution may need extra
regularization 𝜉
x = kT
∗(KK+𝜆I)−k∗, 𝜆∈ℝ+. Therefore, in the kernel case, the decision
of anomalousness is given by
(x, y) = 𝜉
z −𝛽x𝜉
x −𝛽y𝜉
y ,
where now 𝜉
z = kT
z (K zK z + 𝜆I)−kz, 𝜉
x = kT
x(K xK x + 𝜆I)−kx, and 𝜉
y = kT
y (K yK y +
𝜆I)−ky.
Equivalently, including these expressions in Equation ., one obtains the EC kernel
versions 
EC(x, y):

EC(x, y)
= (d + 𝜈) log
(
+
𝜉
z
𝜈−
)
−𝛽x(d + 𝜈) log
(
+
𝜉
x
𝜈−
)
−𝛽y(d + 𝜈) log
(
+
𝜉
y
𝜈−
)
.
(.)
In the case of 𝛽x = 𝛽y = , the algorithm reduces to kernel RX which was introduced
in Kwon and Nasrabadi (c). As in the linear case, one has to center the data before
computing either covariance or Gram (kernel) matrices. This is done via the kernel
matrix operation K ←HKH, where Hij = 𝛿ij −(∕n), and 𝛿represents the Kronecker
delta 𝛿i,j = if i = j and zero otherwise.
Two parameters need to be tuned in the kernel versions: the regularization parameter
𝜆and the kernel parameter 𝜎. In the examples, we will use two diﬀerent isotropic kernel
functions, K(xi, xj) = exp(−d
ij∕(𝜎)): the standard Gaussian kernel dij = ‖xi −xj‖, and
the SAM kernel dij = arccos(xT
i xj∕(‖xi‖‖xj‖)). We should note that, when a linear kernel
is used, K(xi, xj) = xT
i xj, the proposed algorithms reduce to the linear counterparts
proposed in Theiler et al. (). Working in the dual (or Q-mode) with the linear kernel
instead of the original linear versions can be advantageous only in the case of higher
dimensionality than available samples, d > N.

Clustering and Anomaly Detection with Kernels
525
11.6
Hypothesis Testing with Kernels
Statistical hypothesis testing plays a crucial role in statistical inference. After all, the
core of science and engineering is about testing hypotheses,and in machine learning
we do it through data analysis. Testing hypotheses is concurrently one of the key
topics in statistical signal processing as well (Kay, ). A plethora of examples exists:
from testing for equality of observations as in speaker veriﬁcation (Bimbot et al.,
; Rabiner and Schafer, ), to assessing statistical diﬀerences between data
acquisitions, such as in change-detection analysis (Basseville and Nikiforov, ) as
seen before. Testing for a change-point in a time series is also an important problem
in DSP (Fearnhead, ), such as in monitoring applications involving biomedical or
geoscience time series.
Very often we aim to compare two real datasets, or a real dataset with a synthetically
generated one by a model that encodes the system’s governing equations. The result
of such a test will tell us about the plausibility and the origin of the observations. In
both cases, we need a solid measure to test the signiﬁcance of their statistical diﬀerence,
compared with an idealized null hypothesis in which no relationship is assumed (Kay,
). Hypothesis tests are used in determining what outcomes of a study would lead
to a rejection of the null hypothesis for a prespeciﬁed level of signiﬁcance. Imagine,
presented with two datasets, we are interested in testing whether the underlying
distributions are statistically diﬀerent or not in the mean. Essentially, we have to test
between two competing hypotheses, and decide upon them:
H∶𝜇= 𝜇
(.)
HA ∶𝜇≠𝜇.
(.)
The hypothesis His referred to as the null hypothesis, and HA is the alternative hypoth-
esis. The process of distinguishing between the null hypothesis and the alternative
hypothesis is aided by identifying two conceptually diﬀerent types of errors (type I and
type II), and by specifying parametric limits on, for example, how much type I error
could be allowed. If the detector decides HA but His true, we commit a type I error
(also known as the false-alarm rate), whereas if we decide Hbut HA is true, we make a
type II error (also known as the missed-detection rate).
Classical approaches to statistical hypothesis testing and detection (of changes and
diﬀerences between samples) are parametric in nature (Basseville and Nikiforov, ).
Procedures such as the CUSUM statisticor Hotelling’s T-squared test assume that
the data is Gaussian, whereas the 𝜒and mutual information statistics can only be
applied in ﬁnite-valued data. These test statistics are widely used due to their simplicity
and good results in general when the assumptions about the underlying distributions
are fulﬁlled. Alternatively, non-parametric hypothesis testing allows more robust and
reliable results over larger classes of data distributions as this does not assume any
Roughly speaking, a statistical hypothesis will be testable on the basis of observing a process that is
modeled via a set of random variables.
The CUSUM algorithm involves the calculation of a cumulative sum. This way, samples from a process xi
are assigned weights wi, which are then summed up: S= and Si+= max(, Si + xi −wi). Detection
occurs when the value of S is higher than a predeﬁned threshold value.

526
Digital Signal Processing with Kernel Methods
particular form. Recently, nonparametric kernel-based methods have appeared in the
literature (Harchaoui et al., ).
In recent years, several kernel-based hypothesis testing methods have been intro-
duced. For a recent sound review, the reader is addressed to Harchaoui et al. ().
We next review some of these methods under the framework of distribution embed-
dings and the concepts of mean elements and covariance operators (Eric et al. ;
Fukumizu et al. ). These test statistics rely on the eigenvector basis of particular
covariance operators and, under the alternative hypothesis, the tests correspond to con-
sistent estimators of well-known information divergences between probability distribu-
tions (Cover and Thomas, ). We give two instantiations of kernel hypothesis tests
in dependence estimation and anomaly detection, while Chapter will pay attention
to a broader family of these kernel dependence measures for feature extraction.
11.6.1
Distribution Embeddings
Recall that our current motivation is to test statistical diﬀerences between distri-
butions with kernels. Therefore, it is natural to ask ourselves about which are the
appropriate descriptors of distributions embedded into possibly inﬁnite-dimensional
Hilbert spaces. The distributions’ components therein can be described by the so-
called covariance operators and mean elements (Eric et al. ; Fukumizu et al. ;
Harchaoui et al. ; Vakhania et al. ).
Notationally, let us consider a random variable x taking values in and a probability
distribution ℙ. The mean element 𝜇ℙassociated with x is the unique element of the
RKHS , such that, for all f ∈, we obtain
⟨𝜇ℙ, f ⟩= 𝔼ℙ[ f (x)],
and the covariance operator 𝚺ℙ∶⊗→ℝattached to x fulﬁlls
⟨f , 𝚺ℙg⟩∶= Cov( f (x), g(x)) = 𝔼ℙ[ f (x)g(x)] −⟨𝜇ℙ, f ⟩⟨𝜇ℙ, g⟩.
In the empirical case, we are given a set of i.i.d. samples {x, … , xN} drawn from ℙ, so
we have to replace the previous population statistics by their empirical approximations
to obtain the empirical mean ̂𝜇
⟨̂𝜇, f ⟩= 
N
N
∑
i=
f (xi)
and the empirical covariance operator ̂𝚺:
⟨f , ̂𝚺g⟩=
N
∑
i=
( f (xi) −⟨̂𝜇, f ⟩)(g(xi) −⟨̂𝜇, g⟩).
Interestingly, it can be shown that, for a large class of kernels, an embedding function
m(⋅) that maps data into the kernel feature mean is injective (Harchaoui et al., ).
Now, if we consider two probability distributions ℙand ℚon , and for all f ∈

Clustering and Anomaly Detection with Kernels
527
the equality ⟨𝜇ℙ, f ⟩= ⟨𝜇ℚ, f ⟩holds, then ℙ= ℚfor dense kernel functions K.
This property has been exploited to deﬁne independence tests and hypothesis testing
methods based on kernels, since it ensures that two distributions have the same RKHS
mean iﬀthey are the same, as we will see in Chapter .
11.6.2
Maximum Mean Discrepancy
Among several hypothesis testing algorithms, such as the kernel Fisher discriminant
analysis and the kernel density-ratio test statistic revised in Harchaoui et al. (), a
simple and eﬀective one is the maximum mean discrepancy (MMD) statistic. Gretton
et al. (b) introduced the MMD statistic for comparing the means of two samples
in kernel feature spaces; see Figure .. Notationally, we are given samples {xi}N
i=
from a so-called source distribution, ℙx, and {yi}M
i=samples drawn from a second target
distribution, ℙy. MMD reduces to estimate the distance between the two sample means
in an RKHS where data are embedded:
MMD(, ) ∶=
‖‖‖‖‖‖

N
N
∑
i=
𝝓(xi) −
M
M
∑
i=
𝝓(yi)
‖‖‖‖‖‖


.
This estimate can be shown to be equivalent to
MMD(, ) = tr(KL),
where
K =
( K xx
K xy
K yx
K yy
)
,
and Lij = ∕Nif xi and xj belong to the source domain, ̃Lij = ∕Mif xi and xj belong
to the target domain, and Lij = −∕(NM) if xi and xj belong to the cross-domain. MMD
tends asymptotically to zero when the two distributions ℙx and ℙy are the same.
It is worth mentioning that MMD was concurrently introduced in the ﬁeld of signal
processing under the formalism of information-theoretic learning (Principe, ). In
such a framework, it can be shown that MMD corresponds to a Euclidean divergence.
This can be shown easily. Let us assume two distributions p(x) and q(x) with Nand N
samples. The Euclidean divergence between them can be computed by exploiting the
relation to the quadratic Rényi entropies (see Chapter of Principe ()):
DED(p, q) = 
N

N
∑
i=
N
∑
j=
G(xi, xj) + 
N

N
∑
i=
N
∑
j=
G(xi, xj) −

NN
N
∑
i=
N
∑
j=
G(xi, xj),
where G(⋅) denotes Gaussian kernels with standard deviation 𝜎. This divergence reduces
to compute the norm of the diﬀerence between the means in feature spaces:
DED(p, q) = ‖𝝁‖+ ‖𝝁‖−𝝁T
𝝁= ‖𝝁−𝝁‖,

528
Digital Signal Processing with Kernel Methods
which is equivalent to the MMD estimate introduced from the theory of covariance
operators in Gretton et al. (b).
11.6.3
One-Class Support Measure Machine
Let us give now an example of a kernel method for anomaly detection based on RKHS
embeddings. A ﬁeld of anomaly detection that has captured the attention recently is
the so-called grouped anomaly detection. The interest here is that the anomalies often
appear not only in the data themselves, but also as a result of their interactions. There-
fore, instead of looking for pointwise anomalies, here one is interested in groupwise
anomalies. A recent method, based on SVC, was been introduced by Muandet and
Schölkopf (). An anomalous group could be deﬁned as a group of anomalous
samples, which is typically easy to detect. However, anomalous groups are often more
diﬃcult to detect because of their particular behavior as a core group and the higher
order statistical relations among them. Importantly, detection can only be possible in
the space of distributions, which can be characterized by recent kernel methods relying
on concepts of RKHS embeddings of probability distributions, mainly mean elements
and covariance operators. The method presented therein is called the one-class support
measure machine (OC-SMM) and makes uses of the mean embedding representation,
which is deﬁned as follows.
Let denote an RKHS of functions f ∶→ℝendorsed with a reproducing kernel
K ∶× →ℝ. The kernel mean map from the set of all probability distributions 
into is deﬁned as
𝝁∶→,
ℙ→∫
K(x, ⋅)dℙ(x).
Assuming that K(x, ⋅) is bounded for any x ∈, we can show that for any ℙ, letting
𝝁ℙ= 𝝁(ℙ), the 𝔼P[ f ] = ⟨𝝁ℙ, f ⟩, for all f ∈.
The primal optimization problem for the OC-SMM can be formulated in an analogous
way to the OC-SVM (or 𝜈-SVM):
min
w,𝜌,𝜉
{

‖w‖−𝜌+ 
𝜈N
N
∑
i=
𝜉i
}
(.)
constrained to
⟨𝝁ℙi, w⟩≥𝜌−𝜉i
∀i = , … , N
(.)
𝜉i ≥
∀i = , … , N,
(.)
where the meaning of the hyperparameters stands the same, but they are now instead
associated with distributions (note the appearance of the mean distribution appears as
a constraint in Equation .). By operating in the same way, one obtains that the dual
form is again a QP problem that depends on the inner product ⟨𝝁ℙi, 𝝁ℙj⟩that can be
computed using the mean map kernel function:

Clustering and Anomaly Detection with Kernels
529
K( ̂ℙi, ̂ℙj) =

NiNj
Ni
∑
k=
Nj
∑
l=
K(xi
k, xj
l),
where xi
k are the samples belonging to distribution i, and Ni represents its cardinality.
Links to MATLAB source code for this method are available on the book’s web page.
11.7
Tutorials and Application Examples
We next include a set of simple examples for providing insight on the topics of this
chapter. Some of the functions are not included in the text, as indicated, but the
interested reader can obtain the code from the book’s repository.
11.7.1
Example on Kernelization of the Metric
Let us see the relation described in this chapter for kernelization of the metric in a simple
code example, given in Listing .. We generate the well-known Swiss roll dataset, then
compute both the Euclidean distances in the original space and the Hilbert space (with
a speciﬁc 𝜎parameter for the kernel).
Figure .illustrates the behavior of the distance values in the input space against
the distances computed in the Hilbert space. We can see how some of the distances
have decreased values with respect to the input space in the range [, ∕] and the
remaining distances have increased in the Hilbert space. The shape of the curve shows
the nonlinear relation between distances.
0
0.5
1
1.5
0
0.5
1
1.5
Distances in input space
Distances in hilbert space
Figure 11.4 Distance values computed in input space against computed in the Hilbert space (through
reproducing kernels). This figure is generated with Listing 11.3.
n = 100 % number of training points per class
[X,Y] = generate_toydata(n,'swissroll');
% Compute the RBF kernel matrix

530
Digital Signal Processing with Kernel Methods
sigma = median(pdist(X));
% heuristic
K = kernelmatrix('rbf',X',X',sigma);
% Compute distances between data points in Hilbert space from the kernel
for i=1:2*n
for j=1:2*n
Dist2_X(i,j) = norm(X(i,:)-X(j,:),'fro');
Dist2_H(i,j) = K(i,i)+K(j,j)-2*K(i,j);
end
end
figure, plot(Dist2_X,Dist2_H,'k.','markersize',10), grid on, axis tight
xlabel('Distances in input space'), ylabel('Distances in Hilbert space')
Listing 11.3 Comparing distances between the input and Hilbert feature spaces using an RBF kernel
(DistancesXvsH.m).
11.7.2
Example on Kernel k-Means
The code in Listing .runs the k-means function of MATLAB and an implementation
of kernel k-means for the problem of concentric rings. Figure .illustrates the ﬁnal
solution of both methods: k-means could not achieve a good solution (left) compared
to kernel k-means which ﬁnally converges to the correct solution (right).
function kmeans_kernelkmeans
rand('seed',12345), randn('seed',12345)
close all, clear, clc
%% Generate data (2-class concentric rings problem)
N1=200;N2=500;d=2;
x1 = 2*pi*rand(N1,1);r1 = rand(N1,1);
x2 = 2*pi*rand(N2,1);r2 = 2+rand(N2,1);
X = [r1.*cos(x1) r1.*sin(x1);r2.*cos(x2) r2.*sin(x2)];
Y = [ones(n1,1);2*ones(n2,1)];
%% Some parameters
k = 2;[N] = size(X,1);
%% k-means (Matlab) function
[idx, V] = kmeans(X, k, 'Display', 'iter');c1 = idx == 1;c2 = idx == 2;
%% plot
figure(1),clf, scatter(X(c1,1),X(c1,2),'+'),hold on, grid on,
scatter(X(c2,1),X(c2,2),'ro')
%% Kernel k-means
sigma = 1;Ke
= kernelmatrix('rbf',X',X',sigma);
Si = [ones(N,1) zeros(N,1)];Si(1,:) = [0 1];Si(2,:) = [0 1];
Si_prev = Si + 1;
while norm(Si-Si_prev,'fro') > 0
Si_prev = Si;Si = assign_cluster(Ke,Si,k);
end
%% Plot results
Si = logical(Si);
figure(2), clf, scatter(X(Si(:,1),1),X(Si(:,1),2),'+'),hold on, grid on,
scatter(X(Si(:,2),1),X(Si(:,2),2),'ro')
end
%% Callback function
function Si = assign_cluster(Ke,Si,kc)
[n] = size(Ke,1);Nk = sum(Si,1);dist = zeros(N,kc);
for k = 1:kc

Clustering and Anomaly Detection with Kernels
531
dist(:,k) = diag(Ke) - (2/(Nk(k)))*sum(repmat(Si(:,k)',N,1).*Ke,2) + ...
...
Nk(k)^(-2)*sum(sum((Si(:,k)*Si(:,k)').*Ke));
end
Si = real(dist == repmat(min(dist,[],2),1,kc));
end
Listing 11.4 Simple code to perform kmeans and kernel kmeans (kmeans_kernelkmeans.m).
−3
−2
−1
0
1
2
3
−2
−1
0
1
2
−3
−2
−1
0
1
2
3
−2
−1
0
1
2
Figure 11.5 Results obtained by k-means (left) and kernel k-means (right) in the concentric ring
problem.
11.7.3
Domain Description Examples
This section compares diﬀerent standard one-class classiﬁers and kernel detectors:
Gaussian (GDD), mixture of Gaussians (MoGDD), k-NN (KnnDD), and the SVDD
(with the RBF kernel). All one-class domain description classiﬁers have a parameter to
adjust, which is the fraction rejection, that controls the percentage of target samples the
classiﬁer can reject during training. Some other parameters need to be tuned depending
on the classiﬁer. For instance, the Gaussian classiﬁer has a regularization parameter in
the range [, ] to estimate the covariance matrix, using all training samples or only
diagonal elements.
With regard to the mixture of Gaussian classiﬁer, four parameters must be adjusted:
the shape of the clusters to estimate the covariance matrix (e.g., full, diagonal, or
spherical), a regularization parameter in the range [, ] used in the estimation of the
covariance matrices, the number of clusters used to model the target class, and the
number of clusters to model the outlier class. The most important advantage of this
classiﬁer with respect to the Gaussian classiﬁer, apart from the obvious improvement
of modeling a class with several Gaussian distributions, is the possibility of using outliers
information when training the classiﬁer. This allows the tracing of a more precise
boundary around the target class, improving classiﬁcation results notably. However,
having a total of ﬁve parameters to adjust constitutes a serious drawback for this
method, making it diﬃcult to obtain a good working model.
With respect to the k-NN classiﬁer, only the number of k neighbors used to compute
the distance of each new sample to its class must be tuned. Finally, for the SVDD, the

532
Digital Signal Processing with Kernel Methods
Table 11.2 The coefficient of accuracy 𝜿and the overall accuracy in parentheses
(percent) obtained for the synthetic problems.
GDD
MoGDD
KnnDD
SVDD
Two Gaussians
.(.)
.(.)
.(.)
.(.)
Mixed Gaussian–log
.(.)
.(.)
.(.)
.(.)
width of the RBF kernel (𝜎) has to be adjusted. In all the experiments, except the last
one, 𝜎is selected among the following set of discrete values between and . It is
worth stressing here that the SVDD is one of the methods among those considered,
together with the mixture of Gaussians, that allows one to use outliers information to
better deﬁne the target class boundary. In addition, it has the important advantage that
only two free parameters have to be adjusted, thus making it relatively easier to deﬁne
a good model.
In all the experiments, % of the samples available are used to train each classiﬁer. In
order to adjust free parameters, a cross-validation strategy with four folds is employed.
Once the classiﬁer is trained and adjusted, the ﬁnal test is done using the remaining
% of the samples. In all our experiments, we used the dd_tools and the libSVM
software packages.
Two diﬀerent synthetic problems are tested here. In each problem, binary classiﬁca-
tion is addressed, thus having two sets of targets and outliers. Each class contains 
samples generated from diﬀerent distributions. The two problems analyzed are mixture
of two Gaussian distributions and a mixture of Gaussian and logarithmic distributions;
both of them are strongly overlapped. Our goal here is testing the eﬀectiveness of the
diﬀerent methods with standard and simple models but in very critical conditions on
the overlapping of class distributions.
In order to measure the capability of each classiﬁer to accept targets and reject
outliers, the confusion matrix for each problem is obtained, and the kappa coeﬃcient
(𝜿) is estimated, which gives a good trade-oﬀbetween the capability of the classiﬁer
to accept its samples (targets) and reject the others (outliers). Table .shows the
results for the tests, and Figure .shows graphical representations of the classiﬁcation
boundary deﬁned by each method. In each plot, the classiﬁer decision boundary
levels is represented with a blue curve. Listing .shows the code that has been
used for generating these results. Functions getsets.m, train_basic.m, and
predict_basic.m can be seen in the code (not included here for brevity).
function demo_svdd
% dependencies: assessment.m
% add paths to prtools and ddtols (suposed to be in the actual directory)
addpath(genpath('.'))
seed = 1;rand('state',seed);randn('state',seed);
N = 500; dif = 1;
XX =[randn(1,N)-dif randn(1,N)+dif; 2*randn(1,N)-dif 2*randn(1,N)+dif]';
YY =[2*ones(1,N) ones(1,N)]';
percent=0.3;vfold=4;par=[1 sqrt(10) 5 10 25 50 100];frac=[1e-2 .1:.1:.5];
% Sets of classes to train
trainclasses=1;w = cell(1,length(trainclasses));

Clustering and Anomaly Detection with Kernels
533
% Construct validation and test sets
allclasses = unique(YY);
[train_set,test_set] = getsets(YY, allclasses, percent, 0);
% General training data
data.classifier = 'incsvdd';classifier = 'incsvdd';data.ktype = 'r';
% Reserve memory
kappa = zeros(vfold,1); oa = zeros(vfold,1);
% For every class to be trained
for cl = 1:length(trainclasses)
% Reserve memory
errors{cl} = zeros(length(par),length(frac),2);
data.class = trainclasses(cl);
% For every par value
for pit = 1:length(par)
fprintf('Par %f ...\n', par(pit))
% For every fraction rejection value
for fracit = 1:length(frac)
% Train classifier
fprintf('
frac %f ... ', frac(fracit))
data.par
= par(pit);data.frac = frac(fracit);
% Perform cross validation
state = rand('state');rand('state',0);
idx = randperm(length(train_set));
rand('state',state);groups = ...
ceil(vfold*idx/length(train_set));
for vf = 1:vfold,
in
= find(groups ~= vf);out = find(groups == vf);
% Train
data.XX = XX(train_set(in),:);data.YY = ...
YY(train_set(in),:);
data.w
= train_basic(data);
% Validation
data.XX = XX(train_set(out),:);data.YY = ...
YY(train_set(out));
[kappa(vf) oa(vf)] = predict_basic(data);
end
errors{cl}(pit,fracit,:) = [mean(kappa), mean(oa)];
fprintf('
Mean K: %f \t OA: %f\n', errors{cl}(pit,fracit,:))
end
end
% Show optimum values for Kappa
[val,ifrac] = max(max([errors{:}(:,:,1)]));
[val,ipar]
= max(max([errors{:}(:,:,1)]'));
% Now train and test with best parameters
data.XX
= XX(train_set,:); data.YY
= YY(train_set);
data.par
= par(ipar);data.frac = frac(ifrac);
data.w
= train_basic(data);data.XX
= XX(test_set,:);
data.YY
= YY(test_set);
[data.kappa(cl) data.oa(cl) data.yp(cl,:)] = predict_basic(data);
fprintf('Results %s(%02d) Par: %f, f.r.: %f, K: %f, OA: %f\n', ...
classifier, trainclasses(cl),par(ipar), frac(ifrac), data.kappa, ...
data.oa)
end
% Plot the distributions
plot(XX(1:N,1),XX(1:N,2),'r.'),hold ...
on,plot(XX(N+1:end,1),XX(N+1:end,2),'kx')

534
Digital Signal Processing with Kernel Methods
plotc(data.w,'b.',4),set(gca,'LineWidth',1)
end
function [ct,cv] = getsets(YY, classes, percent, vfold, randstate)
if nargin < 5;randstate = 0;end
s = rand('state');rand('state',randstate);ct = []; cv = [];
for ii=1:length(classes)
idt = find(YY == classes(ii));
if vfold
ct = union(ct,idt);cv = ct;
else
lt
= fix(length(idt)*percent); idt = idt(randperm(length(idt)));
idv = idt([lt+1:end]); % remove head
idt = setdiff(idt,idv);ct
= union(ct,idt);cv
= union(cv,idv);
end
end
rand('state',s);
end
%% Callback functions
function [w,res] = train_basic(d)
% Basic funcion to train a one-class classifier, used by xs
targets
= find(d.YY == d.class);outliers = find(d.YY ~= d.class);
x1 = gendatoc(d.XX(targets,:),d.XX(outliers,:));
w = incsvdd(x1,d.frac,d.ktype,d.par);
end
function [kappa,oa,yp] = predict_basic(d)
yt = d.YY;oc = gendatoc(d.XX);rr = oc * d.w;dd = +rr;
idx = find((dd(:,1) - dd(:,2)) > 0);yp = zeros(size(yt));yp(idx) = ...
d.class;
yt(yt ~= d.class) = 0;
res = assessment(yt,yp,'class');kappa = res.Kappa;oa = res.OA;
end
Listing 11.5 Code to reproduce SVDD results over Gaussian synthetic set in Figure 11.6.
Several conclusions can be obtained from these tests. First, MoGDD and SVDD
obtain the highest accuracies in our problems. When mixing two Gaussians, none of
the classiﬁers shows a good behavior, as in our synthetic data the distributions are
strongly overlapped. This is a common situation in the remote-sensing ﬁeld when
trying to classify classes very similar to each other. Mixture of overlapped Gaussian and
logarithmic features is a diﬃcult problem. In this synthetic example, we can see that the
SVDD performs slightly better, since it does not assume any a priori data distribution.
11.7.4
Kernel Spectral Angle Mapper and Kernel Orthogonal
Subspace Projection Examples
The following two experiments have the aim of comparing KSAM and KSOM (and
also their linear versions) in terms of real data problems. In the former, a QuickBird
image of a residential neighborhood of Zürich, Switzerland, is used for illustration
purposes. The image size is (× ) pixels. A total of pixels were labeled
by photointerpretation and assigned to nine land-use classes (see Figure .). Four
target detectors are compared in the task of detecting the class “Soil”: OSP (Harsanyi
and Chang, ), its kernel counterpart KOSP (Kwon and Nasrabadi, ), standard
SAM (Kruse et al., ), and the extension KSAM.

Clustering and Anomaly Detection with Kernels
535
−4
−2
0
2
4
−8
−6
−4
−2
0
2
4
6
−4
−2
0
2
4
−8
−6
−4
−2
0
2
4
6
−4
−2
0
2
4
−8
−6
−4
−2
0
2
4
6
−8
−6
−4
−2
0
2
4
6
−4
−2
0
2
4
−6
−4
−2
0
2
4
6
8
10−2
100
102
10−2
100
102
−6
−4
−2
0
2
4
6
8
−6
−4
−2
0
2
4
6
8
10−2
100
102
−6
−4
−2
0
2
4
6
8
10−2
100
102
Figure 11.6 Plot of the decision boundaries obtained with GDD, MoGDD, KnnDD, and SVDD for the
two Gaussians problem (top) and the mixed Gaussian–logarithm problem (bottom). Note the
semilogarithmic scale in this latter case.

536
Digital Signal Processing with Kernel Methods
Figure .shows the receiver operating characteristic (ROC) curves and the area
under the ROC curves (AUC) as a function of the kernel length-scale parameter 𝜎.
KSAM shows excellent detection rates, especially remarkable in the inset plot (note
the logarithmic scale). Perhaps more importantly, the KSAM method is relatively
insensitive to the selection of the kernel parameter compared with the KOSP detector,
provided that a large enough value is speciﬁed.
The latter experiments allow us to use a traditional prescription to ﬁx the RBF kernel
parameter 𝜎for both KOSP and KSAM as the mean distance among all spectra, dM.
Note that after data standardization and proper scaling, this is a reasonable heuristic
𝜎≈dM = . The thresholds were optimized for all methods. OSP returns a decision
function strongly contaminated by noise, while the KOSP detector results in a correct
detection. Figure .shows the detection maps and the metric learned. The (linear)
SAM gives rise to very good detection but with strong false alarms in the bottom left
side of the image, where the roof of the commercial center saturates the sensor and thus
returns a ﬂat spectrum for its pixels in the morphological features. As a consequence,
both the target and the roof vectors have ﬂat spectra and their spectral angle is almost
null. KSAM can eﬃciently cope with these (nonlinear) saturation problems. In addition,
the metric space derived from the kernel suggests high discriminative (and spatially
localized) capabilities.
11.7.5
Example of Kernel Anomaly Change Detection Algorithms
In this section we show the usage of the proposed methods in several simulated and real
examples of pervasive and anomalous changes. Special attention is given to detection
ROC curves, robustness to number of training samples, low-rank approximation of
the solution, and the estimation of Gaussianity in kernel feature space. Comparisons
between the hyperbolic (HACD), elliptical (EC), kernelized (K-HACD), and elliptical
kernelized (K-EC-HACD) versions of the algorithms are included. Results can be
reproduced using the script demoKACD.m available in the book’s repository.
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
(a)
(b)
False positive rate
True positive rate
OSP
KOSP
SAM
KSAM
10−5
100
0
0.2
0.4
0.6
0.8
1
10−2
100
102
0
0.2
0.4
0.6
0.8
1
σ
AUC
OSP
KOSP
SAM
KSAM
Figure 11.7 (a) ROC curves and (b) AUC as a function of the kernel length-scale parameter.

Clustering and Anomaly Detection with Kernels
537
RGB
OSP (91.84, 0.62)
KOSP (97.70, 0.85)
SAM (97.20, 0.84)
KSAM (98.97, 0.94)
KSAM metric
Figure 11.8 Detection results for different algorithms (accuracy, 𝜿statistic).
Figure 11.9 Hyperspectral image (left panel) captured with AVIRIS, and four illustrative chips of
simulated changes (right panel). The original (leftmost) image is used to simulate an anomalous
change image (rightmost) by adding Gaussian noise and randomly scrambling 1% of the pixels.
This example follows the simulation framework used in Theiler (). The dataset
(see Figure .) is an image acquired over the Kennedy Space Center, Florida, on
March , , by the Airbone Visible Infrared Imaging Spectrometer (AVIRIS). The

538
Digital Signal Processing with Kernel Methods
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
False positive rate
True positive rate
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
False positive rate
True positive rate
10−4
10−2
100
0
0.5
1
HACD (0.48)
EC−HACD (0.68)
K−HACD (0.75)
K−EC−HACD (0.81)
HACD (0.90)
EC−HACD (0.97)
K−HACD (0.95)
K−EC−HACD (0.98)
Figure 11.10 ROC curves and AUC obtained for the hyperspectral experiments by linear and kernel
detectors for 100 (up) and 500 (down) training examples.
data were acquired from an altitude of km and have a spatial resolution of m.
After removing low SNR and water absorption bands, a total of bands remain for
analysis. More information can be found at http://www.csr.utexas.edu/. To simulate a
pervasive change, global Gaussian noise, (, .), was added to all the pixels in the
image to produce a second image. Then, anomalous changes are produced by scrambling
the pixels in the target image. This yields anomalous change pixels whose components
are not individually anomalous.

Clustering and Anomaly Detection with Kernels
539
In all experiments, we used hyperbolic detectors (i.e., 𝛽x = 𝛽y = ), as they generalize
RX and chronocrome ACD algorithms, and have shown improved performance (Theiler
et al., ). In this example, we use the SAM kernel, which is more appropriate to
capture hyperspectral pixel similarities than the standard RBF kernel. We tuned all the
parameters involved (estimated covariance Cz and kernel K z, 𝜈for the EC methods,
length-scale 𝜎parameter for the kernel versions) through standard cross-validation in
the training set, and show results on the independent test set. We used 𝜆= −∕N,
where N is the number of training samples, and the 𝜎kernel length-scale parameter
is tuned in the range of .–.percentiles of the distances between all training
samples, dij.
Figure .shows the ROC curves obtained for the linear ACD and KACD methods.
The dataset was split into small training sets of only and pixels, and results are
given for test samples. The results show that the kernel versions improve upon
−10
−5
0
5
10
0
0.05
0.1
0.15
0.2
x
PDF
0
0.05
0.1
0.15
0.2
PDF
−10
−5
0
5
10
0
0.05
0.1
0.15
0.2
x
PDF
Px
Py
Px
Py
Px
Py
Px
Py
100
101
x
−10
−5
0
5
10
0
0.05
0.1
0.15
0.2
0.25
x
PDF
Figure 11.11 The two-sample problem reduces to detecting whether two distributions ℙx and ℙy are
different or not. Up, left: whenever we have two Gaussians with different means one can assess
statistical differences by means of a standard t test of different means (the distance between the
empirical means is dxy ∶= ‖𝜇x −𝜇y‖ = 3.91). Up, right: when we have two Gaussians with the same
means but different variance (dxy = 0.02), the idea may be to look (down, left) at difference in means
of transformed random variables (for the Gaussian case, second-order features of the form x2 suffice
for discrimination, dxy = 13.26). Down, right: when distributions come from different pdfs (in this case,
Gaussian and Laplace distributions) with the same mean and variance, one has to map the RVs to
higher order features for discrimination (dxy = 0.06 in the original domain and dxy = 75.80 for a
fifth-order mapping).

540
Digital Signal Processing with Kernel Methods
their linear counterparts (between –% in Gaussian and –% in EC detectors).
The EC variants outperform their Gaussian counterparts, especially in the low-sized
training sets (+% over HACD and +% over EC-HACD in AUC terms). Also
noticeable is that results improve for all methods when using training samples.
The EC-HACD is very competitive compared with the kernel versions in terms of AUC,
but still K-EC-HACD leads to longer tails of false-positive detection rates (right ﬁgure,
inset plot in log-scale).
11.7.6
Example on Distribution Embeddings and Maximum Mean Discrepancy
Let us exemplify the use of the concept of distribution embeddings and MMD in
a simple example comparing two distributions ℙx and ℙy. We draw samples from
(i) two Gaussians with diﬀerent means, (ii) Gaussians with the same means but diﬀerent
variance, and (iii) from a Gaussian and a Laplace distributions. A code snippet is given
in Listing .illustrating the ability of MMD to detect such diﬀerences. The results
obtained are shown in Figure ..
function mmd_demo
clc;close all; randn('seed',1234); rand('seed',1234)
% Two gaussians with different means
n=500;
x = linspace(-10,10,n);
mu1 = -2; si1 = +2; mu2 = +2; si2 = +2;
PX = normpdf(x,mu1,si1); PY = normpdf(x,mu2,si2);
Xe = mu1 + si1*randn(1,n); Ye = mu2 + si2*randn(1,n);
muXe = mean(Xe); muYe = mean(Ye); d = abs(muXe-muYe)
figure,plot(x,PX,'k-',x,PY,'r-'),
legend('Px','Py'), xlabel('x'), ylabel('PDF'), grid
% Two gaussians with the same mean but different variances
mu1 = 0; si1 = +2; mu2 = 0; si2 = +6;
PX = normpdf(x,mu1,si1); PY = normpdf(x,mu2,si2);
Xe = mu1 + si1*randn(1,n); Ye = mu2 + si2*randn(1,n);
muXe = mean(Xe); muYe = mean(Ye); d = abs(muXe-muYe)
figure,plot(x,PX,'k-',x,PY,'r-'),
legend('Px','Py'),xlabel('x'),ylabel('PDF'),grid
% Two gaussians with different variances, features transformed into x^2
mu1 = 0; si1 = +2; mu2 = 0; si2 = +4;
PX = normpdf(x.^2,mu1,si1); PY = normpdf(x.^2,mu2,si2);
Xe = mu1 + si1*randn(1,n); Ye = mu2 + si2*randn(1,n);
muXe = mean(Xe); muYe = mean(Ye); d = abs(muXe-muYe)
muXe2 = mean(Xe.^2); muYe2 = mean(Ye.^2); d = abs(muXe2-muYe2)
figure,semilogx(x.^2,PX,'k-',x.^2,PY,'r-'),
hold on, stem(log(muXe2),max(PX),'k'),stem(log(muYe2),max(PX),'r'),
legend('Px','Py'),xlabel('x'),ylabel('PDF'),grid, xlim([1,10])
% A gaussian and a Laplacian with same mean and variance
mu1 = 0; si1 = +2; mu2 = 0; si2 = +2;
PX = normpdf(x,mu1,si1); PY = lappdf(x,mu2,si2);
Xe = mu1 + si1*randn(1,n); Ye = mu2 + si2*randn(1,n);

Clustering and Anomaly Detection with Kernels
541
muXe = mean(Xe); muYe = mean(Ye); d = abs(muXe-muYe)
figure,plot(x,PX,'k-',x,PY,'r-'),
legend('Px','Py'), xlabel('x'), ylabel('PDF'), grid
function x = randlap(mu,st,n)
u = rand(n) - 0.5;
x = mu - st*sign(u) .* log(1-2*abs(u));
function p = lappdf(x,mu,st)
p = 1/(2*st)*exp(-abs(x-mu)/st);
Listing 11.6 Comparing distributions of different nature with MMD (mmd_demo.m).
11.8
Concluding Remarks
This chapter treated the relevant topic of clustering and anomaly detection with kernels.
The ﬁeld is undoubtedly in the core of machine learning, and has many practical impli-
cations. The conceptual/philosophical problem of deﬁning regularity, cluster, member-
ship, and anomaly is an elusive one, so that there are the proper statistical deﬁnition of
signiﬁcant diﬀerence, hypothesis testing, and similarity measures between probabilities.
Consequently, many methods and approaches have been proposed to tackle these
problems. We reviewed several families available and related to a certain extent. We
organized the kernel-based approaches systematically, and revised them organized in a
simple taxonomy: () clustering, () density estimation (sometimes referred as to domain
description), () matched subspace detectors, () anomaly change detection, and ()
statistical hypothesis testing.
The treatment in this chapter was far from exhaustive; rather, we tried to cover the
main ﬁelds and to give an intuition on the relations among particular approaches. We
observed clear links between clustering, subspace projection, and density estimation
methods. The ﬁrst essentially considered density characterization via kernelization of
the metric or the algorithm, and the last relied on sound concepts of distribution
embeddings.
We should also mention that the ﬁeld of clustering and anomaly detection has arrived
at a certain point of maturity with the introduction of relevant new paradigms from
machine learning, such as SSL and AL, and from signal processing such as online and
adaptive ﬁltering. In many problems, introducing some labeled information properly
helps in discerning the anomaly; also, including the user in the loop via active detection
may help the detectors. The problem of anomaly detection in signal processing prob-
lems has to do very often with adaptation to changing environments/channels, so both
adapting the classiﬁer and/or the data representation has also been approached in the
ﬁeld. Other relevant issues to the DSP community, such as change-point detection, were
intentionally omitted for the sake of conciseness.
The number of applications of the techniques presented will probably increase in the
following years. The identiﬁcation of events that are far from normal is an extremely
appealing target for many real science and engineering applications, from health to
traﬃc monitoring and climate science. In this setting, many big data environments
or long-term monitoring from diverse kinds and nature will require DSP techniques

542
Digital Signal Processing with Kernel Methods
running in parallel or even embedded with solid anomaly detection and domain
description algorithms.
11.9
Questions and Problems
Exercise ..
Given an arbitrary dataset, obtain and compare the clustering solu-
tions for k = {, , , } using k-means and kernel k-means with (a) the linear kernel
and (b) the RBF Gaussian kernel for several values of the length-scale 𝜎parameter.
Exercise ..
Obtain Equations .and ..
Exercise ..
Demonstrate these latter two equations in Equation ..
Exercise ..
Obtain the update rule for 𝛼jl in Equation ..
Exercise ..
Demonstrate that SVDD and OC-SVM reduce to the same equations
when using the RBF kernel.
Exercise ..
Exploit the kernelization of the metric in the SAM similarity measure,
which is commonly used in change detection, and that is given by
𝜃= arccos(
𝐱T𝐳
‖𝐱‖‖𝐳‖),
(.)
where ≤𝜃≤π∕. Show that KSAM is a valid Mercer kernel and that KSAM is
universal.
Exercise ..
A critical issue in kernel dependence estimation is about the selection
of the kernel function and its hyperparameters. Very often one simply selects the SE
kernel and sets the 𝜎length-scale to the average distance between all samples. Test
other ways based on the ﬁeld of density estimation (e.g., Silverman’s rule and related
heuristics)). Figure out a way to tune Hilbert–Schmidt independence criterion (HSIC)
parameters in a supervised manner.
Exercise ..
Derive the HSIC empirical estimate with regard to the 𝜎parameter
for the sake of optimization.
Exercise ..
Demonstrate the ECd kernel versions in Equations .and ..
Exercise ..
The performance of the linear and kernel EC algorithms can be
severely aﬀected by the value of 𝜈. Theiler et al. () suggested a heuristic based
on the ratio of moments of the distribution, 𝜈= + m𝜅m∕(𝜅m −(d + m)), where
𝜅m = ⟨rm+⟩∕⟨rm⟩, r = 𝜉∕
z
and ﬁxed m = to control the impact of outliers. Discuss
on the motivation, and its suitability in the kernel case. Can you say anything about the
Gaussianity in feature spaces?

543
12
Kernel Feature Extraction in Signal Processing
Kernel-based feature extraction and dimensionality reduction are becoming increas-
ingly important in advanced signal processing. This is particularly relevant in applica-
tions dealing with very high-dimensional data. Current methods tackle important prob-
lems in signal processing: from signal subspace identiﬁcation to nonlinear blind source
separation, as well as nonlinear transformations that maximize particular criteria, such
as variance (KPCA), covariance (kernel PLS (KPLS)), MSE (kernel orthonormalized PLS
(KOPLS)), correlation (kernel canonical correlation analysis (KCCA)), mutual infor-
mation (kernel generalized variance) or SNR (kernel SNR), just to name a few. Kernel
multivariate analysis (KMVA) has closed links to KFD analysis as well as interesting
relations to information theoretic learning. The application of the methods is hampered
in two extreme cases: when few examples are available the extracted features are
either overﬁtted or meaningless, while in large-scale settings the computational cost is
prohibitive. Semi-supervised and sparse learning have entered the ﬁeld to alleviate these
problems. Another ﬁeld of intense activity is that of domain adaptation and manifold
alignment, for which kernel method feature extraction is currently being used. All these
topics are the subject of this chapter. We will review the main kernel feature extraction
and dimensionality reduction methods, dealing with supervised, unsupervised and
semi-supervised settings. Methods will be illustrated in toy examples, as well as real
datasets.
12.1
Introduction
In the last decade, the amount and diversity of sensory data has increased vastly. Sensors
and systems acquire signals at higher rate and resolutions, and very often data from
diﬀerent sensors need to be combined. In this scenario, appropriate data representation,
adaptation, and dimensionality reduction become important concepts to be considered.
Among the most important tasks of machine learning are feature extraction and
dimensionality reduction, mainly due to the fact that, in many ﬁelds, practitioners need
to manage data with a large dimensionality; for example, in image analysis, medical
imaging, spectroscopy, or remote sensing, and where many heterogeneous feature
components are computed from data and grouped together for machine-learning
tasks.
Extraction of relevant features for the task at hand boils down to ﬁnding a
proper representation domain of the problem. Diﬀerent objectives will give diﬀerent
Digital Signal Processing with Kernel Methods, First Edition. José Luis Rojo-Álvarez, Manel Martínez-Ramón,
Jordi Muñoz-Marí, and Gustau Camps-Valls.
© John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.

544
Digital Signal Processing with Kernel Methods
optimization problems. The ﬁeld dates back to the early th century with the works
of Hotelling, Wold, Pearson, and Fisher, just to name a few.They originated the ﬁeld
of multivariate analysis (MVA) in its own right. The methods developed for MVA
address the problems of dimensionality reduction in a very principled way, and typically
resort to linear algebra operations. Owing to their simplicity and uniqueness of the
solution, MVA methods have been successfully used in several scientiﬁc areas (Wold,
a).
Roughly speaking, the goal of MVA methods is to exploit dependencies among the
variables to ﬁnd a reduced set of them that are relevant for the learning task. PCA, PLS,
or CCA are amongst the best known MVA methods. PCA uses only the correlation
between the input dimensions in order to maximize the variance of the data over a
chosen subspace, disregarding the target data. PLS and CCA choose subspaces where
the projections maximize the covariance and the correlation between the features and
the targets, respectively. Therefore, they should in principle be preferred to PCA for
regression or classiﬁcation problems. In this chapter, we consider also a fourth MVA
method known as OPLS that is also well suited to supervised problems, with certain
optimality in LS multiregression.
In any case, all these MVA methods are constrained to model linear relationships
between the input and output; thus, they will be suboptimal when these relation-
ships are actually nonlinear. This issue has been addressed by constructing nonlinear
versions of MVA methods. They can be classiﬁed into two fundamentally diﬀerent
approaches (Rosipal, ): () the modiﬁed methods in which the linear relations
among the latent variables are substituted by nonlinear relations (Laparra et al. ,
; Qin and McAvoy ; Wold et al. ); and () variants in which the algorithms
are reformulated to ﬁt a kernel-based approach (Boser et al. ; Schölkopf et al.
; Shawe-Taylor and Cristianini ). An appealing property of the resulting
kernel algorithms is that they obtain the ﬂexibility of nonlinear expressions using
straightforward methods from linear algebra.
The ﬁeld has captured the attention of many researchers from other ﬁelds mainly
due to interesting relations to KFD analysis algorithms (a plethora of variants has been
introduced in the last decade in the ﬁelds of pattern recognition and computer vision),
These are names every statistician should know quite well. Harold Hotelling was an American statistician
highly inﬂuential in economics, and he is well known for his T-squared distribution in statistics and the
perhaps most important method in data analysis and multivariate statistics: the PCA method. Herman Ole
Andreas Wold was a Norwegian-born econometrician and statistician who developed his career in Sweden.
Wold is known for his excellent works in mathematical economics, in time-series analysis, and in
econometrics. He was ahead of his time: Wold contributed to the development of multivariate methods
such as PLS, the Cramér–Wold theorem characterizing the normal distribution, and his advances in causal
inference from empirical data. Karl Pearson was an English mathematician and biostatistician, whose
contributions in statistics are outstanding: the correlation coeﬃcient to measure association between
random variables, the method of moments, the 𝜒-distance, the p-value, and PCA are the most famous
examples of contributions in the ﬁeld. And what to say about Fisher! Sir Ronald Aylmer Fisher was an
English statistician and biologist who combined mathematics and genetics, explaining most of the processes
of natural selection. When it comes to signal processing and machine learning, however, he is well known
because of the development of LDA, Fisher’s information score, the F-distribution, the Student
t-distribution or relevant contributions in suﬃcient statistics, just to name a few. Fisher was a proliﬁc
author, a controverted individual, a genius after all.

Kernel Feature Extraction in Signal Processing
545
and the more appealing relations to particular instantiations of kernel methods for
information theoretic learning (e.g., links between covariance operators in Hilbert
spaces and information concepts such as mutual information or Rényi entropy). In
Section .we will show some of the relations between KMVA and other feature
extraction methods based on nonparametric kernel dependence estimates.
The fact that KMVA methods need the construction of a Gram matrix of kernel
dot products between data precludes their direct use in applications involving large
datasets, and the fact that in many applications only a small number of labeled samples
is also strongly limiting. The ﬁrst diﬃculty is usually minimized by the use of sparse or
incremental versions of the algorithms, and the second has been overcome by the ﬁeld
of SSL. Approaches to tackle these problems include special types of regularization,
guided either by selection of a reduced number of basis functions or by considering
the information about the manifold conveyed by the unlabeled samples. This chapter
reviews all these approaches (Section .).
Domain adaptation, transfer learning, and manifold alignment are relevant topics
nowadays. Note that classiﬁcation or regression algorithms developed with data coming
from one domain (system, source) cannot be directly used in another related domain,
and hence adaptation of either the data representation or the classiﬁer becomes strictly
imperative (Quiñonero-Candela et al., ; Sugiyama and Kawanabe, ). The ﬁelds
of manifold alignment and domain adaptation have captured high interest nowadays
in pattern analysis and machine learning. On many occasions, it is more convenient
to adapt the data representations to facilitate the use of standard classiﬁers, and
several kernel methods have been proposed trying to map diﬀerent data sources to a
common latent space where the diﬀerent source manifolds are matched. Section .
reviews some kernel methods to perform domain adaptation while still using linear
algebra.
12.2
Multivariate Analysis in Reproducing Kernel Hilbert Spaces
This section ﬁxes the notation used in the chapter, and reviews the framework of
MVA both in the linear case and with kernel methods. An experimental evaluation
on standard UCI datasets and real high-dimensional image segmentation problems
illustrate the performance of all the methods.
12.2.1
Problem Statement and Notation
Assume any supervised regression or classiﬁcation problem, were X and Y are column-
wise centered input and target data matrices of sizes N ×d and N ×m respectively. Here,
N is the number of training data points in the problem, and d and m are the dimensions
of the input and output spaces respectively. The output response data (sometimes
referred to as the target) Y are typically a set of variables in columns that need to be
approximated in regression settings, or a matrix that encodes the class membership
information in classiﬁcation settings. The sample covariance matrices are given by
Cx = (∕N)XTX and Cy = (∕N)Y TY, whereas the input–output cross-covariance is
expressed as Cxy = (∕N)XTY.

546
Digital Signal Processing with Kernel Methods
The objective of standard linear multiregression is to adjust a linear model for
predicting the output variables from the input ones; that is, ̂Y
= XW, where W
contains the regression model coeﬃcients. The standard LS solution is W = X†Y, where
X† = (XTX)−XT is the Moore–Penrose pseudoinverseof X. If some input variables
are highly correlated, the corresponding covariance matrix will be rank deﬁcient, and
thus its inverse will not exist. The same situation will be encountered if the number N
of data is less that the dimension d of the space. In order to turn these problems into
well-conditioned ones it is usual to apply a Tikhonov regularization term; for instance,
by also minimizing the Frobenius norm of the weights matrix ‖W‖
F one obtains the
regularized LS solution W = (XTX+𝜆I)−XTY, where parameter 𝜆controls the amount
of regularization.
MVA techniques solve the aforementioned problems by projecting the input data
into a subspace that preserves the information useful for the current machine-learning
problem. MVA methods transform the data into a set of features through a transfor-
mation X′ = XU, where U = [u| ⋯|unf] will be referred hereafter as the projection
matrix, ui being the ith projection vector and nf the number of extracted features. Some
MVA methods consider also a feature extraction in the output space, Y ′ = YV, with
V = [v| ⋯|vnf].
Generally speaking, MVA methods look for projections of the input data that are
“maximally aligned” with the targets. Diﬀerent methods are characterized by the
speciﬁc objectives they maximize. Table .summarizes the MVA methods that are
being presented in this section. It is important to be aware of that MVA methods
are based on the ﬁrst- and second-order moments of the data, so these methods can
be formulated as a generalized eigenvalue problem; hence, they can be solved using
standard algebraic techniques.
12.2.2
Linear Multivariate Analysis
The best known MVA method, and probably the oldest one, is PCA (e.g., Pearson,
), also known as the Hotelling transform or the Karhunen–Loève transform (Jolliﬀe,
). PCA selects the maximum variance projections of the input data, imposing
an orthonormality constraint for the projection vectors (see Table .). This method
assumes that the projections that contain the information are the ones that have the
highest variance. PCA is an unsupervised feature extraction method. Even if supervised
methods, which include the target information, may be preferred, PCA and its kernel
version, KPCA, are used as preprocessing stages in many supervised problems, possibly
because of their simplicity and ability to discard irrelevant directions (Abrahamsen and
Hansen ; Braun et al. ).
In signal processing applications, maximizing the variance of the signal may not be
a good idea per se, and one typically looks for transformations of the observed signal
such that the SNR is maximized, or alternatively for transformations that minimize the
fraction of noise. This is the case of the MNF transform (Green et al., ), which
In MATLAB we have functions inv and pinv to compute the inverse and pseudoinverse of a matrix
respectively.

Kernel Feature Extraction in Signal Processing
547
Table 12.1 Summary of linear and KMVA methods. We state for all methods treated in this section the
objective to maximize, constraints for the optimization, and maximum number of features np that can
be extracted, and how the data are projected onto the extracted features.
Method
PCA
SNR
PLS
CCA
OPLS
maxu
uTCxu
uTC−
n Cxu
uTCxyv
uTCxyv
uTCxyCT
xyu
s.t.
UTU = I
UTCnU = I
UTU = I
V TV = I
UTCxU = I
V TCyV = I
UTCxU = I
np
r(X)
r(X)
r(X)
r(Cxy)
r(Cxy)
P(X∗)
X∗U
X∗U
X∗U
X∗U
X∗U
KPCA
KSNR
KPLS
KCCA
KOPLS
max𝜶
𝜶TK
x𝜶
𝜶T(KxnKnx)−K
x𝜶
𝜶TKxYv
𝜶TKxYv
𝜶TKxYY TKx𝜶
s.t.
ATKxA = I
ATKxnKnxA = I
ATKxA = I
V TVV = I
ATK
xA = I
V TCyV = I
ATK
xA = I
np
r(Kx)
r(Kx)
r(Kx)
r(KxY)
r(KxY)
P(X∗)
Kx(X∗, X)A
Kx(X∗, X)A
Kx(X∗, X)A
Kx(X∗, X)A
Kx(X∗, X)A
Vectors 𝐮and 𝜶are column vectors in matrices U and A respectively. r(⋅) denotes the rank of a matrix.
extends PCA by maximizing the signal variance while minimizing the estimated noise
variance. Let us assume that observations xi follow an additive noise model; that is,
xi = si + ni, where ni may not necessarily be Gaussian. The observed data matrix is
assumed to be the sum of a “signal” and a “noise” matrix, X = S + N, being typically
N > d. Matrix ̃X indicates the centered version of X, and Cx = (∕N) ̃XT ̃X represents the
empirical covariance matrix of the input data. Very often, signal and noise are assumed
to be orthogonal, STN = NTS = 𝟎, which is very convenient for SNR maximization
and blind-source separation problems (Green et al., ; Hundley et al., ). Under
this assumption, one can incorporate easily the information about the noise through
the noise covariance matrix Cn = (∕N) ̃NT ̃N.
The PLS algorithm (Wold, b) is a supervised method based on latent variables
that account for the information in the cross-covariance matrix Cxy. The strategy
here consists of constructing the projections that maximize the covariance between
the projections of the input and the output, while keeping certain orthonormality
constraints. The procedure is solved iteratively or in block through an eigenvalue

548
Digital Signal Processing with Kernel Methods
problem. In iterative schemes, datasets X and Y are recursively transformed in a process
which subtracts the information contained in the already estimated latent variables. This
process is usually called deﬂation, and it can be done in several ways that deﬁne the
many variants of PLS existing in the literature.The algorithm does not involve matrix
inversion, and it is robust against highly correlated data. These properties made it usable
in many ﬁelds, such as chemometrics and remote sensing, where signals typically are
acquired in a range of highly correlated spectral wavelengths.
The main feature of CCA is that it maximizes the correlation between projected input
and output data (Hotelling, ). This is why CCA can manage spatial dimensions in
the input or output data that show high variance but low correlation between input and
output, which would be emphasized by PLS. CCA is a standard method for aligning data
sources, with interesting relations to information-theoretic approaches. We will come
back to this issue in Sections .and ..
An interesting method we will pay attention to is OPLS, also known as multilinear
regression (Borga et al., ) or semi-penalized CCA (Barker and Rayens, ). OPLS
is optimal for performing multilinear LS regression on the features extracted from the
training data; that is:
U∗= arg min
U
‖Y −X′W‖
F,
(.)
with W = X′†Y being the matrix containing the optimal regression coeﬃcients. It can
be shown that this optimization problem is equivalent to the one stated in Table ..
This problem can be compared with maximizing a Rayleigh coeﬃcient that takes into
account the projections of input and output data,
(uTCxyv)
(uTCxu)(vTv). This is why the method
is called semi-penalized CCA. Indeed, it does not take into account the variance of
the projected input data, but emphasizes those input dimensions that better predict
large variance projections of the target data. This asymmetry makes sense in supervised
subspace learning where matrix Y contains target values to be approximated from the
extracted input features.
Actually, OPLS (for classiﬁcation problems) is equivalent to LDA provided an appro-
priate labeling scheme is used for Y (Barker and Rayens, ). However, in “two-
view learning” problems (also known as domain adaptation or manifold alignment),
in which X and Y represent diﬀerent views of the data (Shawe-Taylor and Cristianini,
, Section .), one would like to extract features that can predict both data
representations simultaneously, and CCA could be preferred to OPLS. We will further
discuss this matter in Section ..
Now we can establish a simple common framework for PCA, SNR, PLS, CCA, and
OPLS, following the formalization introduced in Borga et al. (), where it was shown
Perhaps the most popular PLS method was presented by Wold et al. (). The algorithm, hereafter
referred to as PLS, assumes a linear relation between X and Y that implies a certain deﬂation scheme,
where the latent variable of X is used to deﬂate also Y (Shawe-Taylor and Cristianini, , : ). Several
other variants of PLS exist, such as “PLS Mode A” and PLS-SB; see Geladi () for a discussion of the early
history of PLS and Kramer and Rosipal () for a well-written overview.

Kernel Feature Extraction in Signal Processing
549
that these methods can be reformulated as (generalized) eigenvalue problems, so that
linear algebra packagescan be used to solve them. Concretely:
PCA ∶
Cxu = 𝜆u
SNR ∶
Cxu = 𝜆Cnu
PLS ∶
( 
Cxy
CT
xy

) (
u
v
)
= 𝜆
(
u
v
)
OPLS ∶CxyCT
xyu = 𝜆Cxu
CCA ∶
( 
Cxy
CT
xy

) (
u
v
)
= 𝜆
( Cx


Cy
) (
u
v
)
.
(.)
Note that CCA and OPLS require the inversion of matrices Cx and Cy. Whenever these
are rank deﬁcient, it becomes necessary to ﬁrst extract the dimensions with nonzero
variance using PCA, and then solve the CCA or OPLS problems. Alternative approaches
include adding extra regularization terms. A very common approach is to solve the
aforementioned problems using a two-step iterative procedure: ﬁrst, the projection vec-
tors corresponding to the largest (generalized) eigenvalue are chosen, for which there
exist eﬃcient methods such as the power method; and then in the second step known
as deﬂation, one removes from the data (or the covariance matrices) the covariance that
can be already obtained from the features extracted in the ﬁrst step. Equivalent solutions
can be found by formulating them as regularized LS problems. As an example, sparse
versions of PCA, CCA, and OPLS were introduced by adding sparsity promotion terms,
such as LASSO or 𝓁-norm on the projection vectors, to the LS functional (Hardoon
and Shawe-Taylor ; Van Gerven et al. ; Zou et al. ).
12.2.3
Kernel Multivariate Analysis
The framework of KMVA algorithms is aimed at extracting nonlinear projections while
actually working with linear algebra. Let us ﬁrst consider a function 𝜱∶ℝd →that
maps input data into a Hilbert feature space . The new mapped dataset is deﬁned as
𝜱= [𝜱(x)| ⋯|𝜱(xl)]T, and the features extracted from the input data will now be
given by 𝜱′ = 𝜱U, where matrix U is of size dim() × nf. The direct application
of this idea suﬀers from serious practical limitations when the is very large. To
implement practical KMVA algorithms we need to rewrite the equations in the ﬁrst
Among the most well-known package to deal with linear algebra problems we ﬁnd LAPACK
http://www.netlib.org/lapack/. LAPACK is written in Fortran and provides routines for solving systems
of simultaneous linear equations, LSs solutions of linear systems of equations, eigenvalue problems, and
singular value problems, as well as the associated matrix factorizations (LU, Cholesky, QR, SVD, Schur,
generalized Schur). The routines in LAPACK can deal with dense and banded matrices. LAPACK has been
sponsored by MathWorks and Intel for many years, which ensures lifelong updated routines.

550
Digital Signal Processing with Kernel Methods
half of Table .in terms of inner products in only. For doing so, we rely on the
availability of a kernel matrix K x = 𝜱𝜱T of dimension N × N, and on the representer
theorem (Kimeldorf and Wahba, ), which states that the projection vectors can
be written as a linear combination of the training samples; that is, U = 𝜱TA, matrix
A = [𝜶| ⋯|𝜶nf] being the new argument for the optimization.Equipped with the
kernel trick, one readily develops kernel versions of the previous linear MVA, as shown
in Table ..
For PCA, it was Schölkopf et al. () who introduced a kernel version denoted
KPCA. Lai and Fyfe in ﬁrst introduced the kernel version of CCA denoted
KCCA (Lai and Fyfe, a; Shawe-Taylor and Cristianini, ). Later, Rosipal and
Trejo () presented a nonlinear kernel variant of PLS. In that paper, K x and the Y
matrix are deﬂated the same way, which is more in line with the PLSvariant than with
the traditional algorithm “PLS Mode A,” and therefore we will denote it as KPLS. A
kernel variant of OPLS was presented in (Arenas-García et al., ) and is here referred
to as KOPLS. Recently, the kernel MNF was introduced in (Nielsen, ) but required
the estimation of the noise signal in input space. This kernel SNR method was extended
in (Gómez-Chova et al., ) to deal with signal and noise relations in Hilbert space
explicitly. That is, in the explicit KSNR (KSNRe) the noise is estimated in Hilbert spaces
rather than in the input space and then mapped with an appropriate reproducing kernel
function (which we refer to as implicit KSNR, KSNRi).
As for the linear case, KMVA methods can be implemented as (generalized) eigen-
value problems:
KPCA ∶
K x𝜶= 𝜆𝜶
KSNR ∶
K x
𝜶= 𝜆K xn ̃K T
xn𝜶
KPLS ∶
(

K xY
YK x 
) (
𝜶
v
)
= 𝜆
(
𝜶
v
)
KOPLS ∶K xYY TK x𝜶= 𝜆K xK x𝜶
KCCA ∶
(

K xY
YK x 
) (
𝜶
v
)
= 𝜆
( K xK x 

Cy
) (
𝜶
v
)
.
(.)
Note that the output data could also be mapped to some feature space , as was
considered for KCCA for a multiview learning case (Lai and Fyfe, a). Here, we
consider instead that it is the actual labels in Y which need to be well represented by the
extracted input features, so we will deal with the original representation of the output
data.
In this chapter, we assume that data are centered in feature space, which can easily be done through a
simple modiﬁcation of the kernel matrix (see Chapter ).

Kernel Feature Extraction in Signal Processing
551
12.2.4
Multivariate Analysis Experiments
In this section we illustrate through diﬀerent application examples the use and capabili-
ties of the kernel multivariate feature extraction framework. We start with a toy example
where we can visualize the features, and therefore get intuition about what is doing
each method. After that we compare the performance of linear and KMVA methods in
a real classiﬁcation problem of (high-dimensional) satellite images (Arenas-García and
Camps-Valls ; Arenas-García et al. ).
Toy Example
In this ﬁrst experiment we illustrate the methods presented using toy data. Figure .
illustrates the features extracted by the methods for a toy classiﬁcation problem with
three classes. Figure .can be reproduced by downloading the SIMFEAT toolbox
and executing the script Demo_Fig_14_1.m from the book’s repository. A simpliﬁed
version of the code is shown in Listing .. Note how all the methods are based on
eigendecompositions of covariance matrices or kernels.
%% Data
NN = 200; Nc = 2; YY = binarize([ones(NN,1); 2*ones(NN,1); 3*ones(NN,1)]);
XX(:,1) = [rand(3*NN,1)*2];
XX(:,2) = [(XX(1:NN,1)-1).^2; (XX(NN+1:NN*2,1)-1).^2+0.2 ; ...
(XX(2*NN+1:NN*3,1)-1).^2+0.4]+0.04*randn(3*NN,1);
XX = XX-repmat(mean(XX),size(XX,1),1);
ii = randperm(3*NN); X = XX(ii(1:NN),:); Y = YY(ii(1:NN),:);
Xts = XX(ii(NN+1:3*NN),:); Yts = YY(ii(NN+1:3*NN),:);
%% Number of features (projections, components) to be extracted
nf = 2;
%% Covariances for Linear Methods
Cxx = cov(X); Cxy = X'*Y; Cyy = cov(Y);
% PCA
[U_pca D] = eig(Cxx);
XtsProj_PCA = Xts * U_pca(:,1:nf); XProj_PCA = X * U_pca(:,1:nf);
Ypred_PCA = classify(XtsProj_PCA,XProj_PCA,Y);
% PLS
U_pls = svds(Cxy);
XtsProj_PLS = Xts * U_pls(:,1:nf); XProj_PLS = X * U_pls(:,1:nf);
Ypred_PLS = classify(XtsProj_PLS,XProj_PLS,Y);
% OPLS
U_opls = gen_eig(Cxy*Cxy',Cxx);
XtsProj_OPLS = Xts * U_opls(:,1:nf); XProj_OPLS = X * U_opls(:,1:nf);
Ypred_OPLS = classify(XtsProj_OPLS,XProj_OPLS,Y);
% CCA
U_cca = gen_eig(Cxy*inv(Cyy)*Cxy',Cxx);
XtsProj_CCA = Xts * U_cca(:,1:nf); XProj_CCA = X * U_cca(:,1:nf);
Ypred_CCA = classify(XtsProj_CCA,XProj_CCA,Y);
%% Kernels for Nonlinear methods
sigmax = estimateSigma(X,X);
K = kernel('rbf',X,X,sigmax); Kc=kernelcentering(U_kpca.Ktrain);
Ktest=kernel('rbf',X,Xts,sigmax); Kctest=kernelcentering(Ktest,Kc);
% KPCA
npmax = min([nf,size(X,1)]);
U_kpca = eigs(Kc,npmax);

552
Digital Signal Processing with Kernel Methods
XProj_KPCA = Kc' * U_kpca; XtsProj_KPCA
= Kctest' * U_kpca;
Ypred_KPCA = classify(XtsProj_KPCA,XProj_KPCA,Y);
% KPLS
U_kpls=svds(Kc*Y,nf);
XProj_KPLS = Kc' * U_kpls; XtsProj_KPLS
= Kctest' * U_kpls;
Ypred_KPLS = classify(XtsProj_KPLS,XProj_KPLS,Y);
% KOPLS
npmax = min([nf,max(Y)]);
U_kopls = gen_eig(K*Y*Y'*K,K'*K,npmax);
XProj_OKPLS = Kc' * U_kopls; XtsProj_OKPLS
= Kctest' * U_kopls;
Ypred_OKPLS = classify(XtsProj_OKPLS,XProj_OKPLS,Y);
% KCCA
npmax = min([nf,size(X,1)]);
U_kcca = gen_eig(Kc*Y*inv(Cyy)*Y'*Kc,Cxx,npmax);
XProj_KCCA = Kc' * U_kcca; XtsProj_KCCA
= Kctest' * U_kcca;
Ypred_KCCA = classify(XtsProj_KCCA,XProj_KCCA,Y);
Listing 12.1 MATLAB code of a KMVA application.
The data were generated from three noisy parabolas, so that a certain overlap exists
between classes. For the ﬁrst extracted feature we show its sample variance, the largest
correlation and covariance that can be achieved with a linear transformation of the out-
put, and the optimum MSE of the feature when used to approximate the target. All these
are shown above the scatter plot. The ﬁrst projection for each method maximizes the
variance (PCA), covariance (PLS), and correlation (CCA), while OPLS ﬁnds the MMSE
projection. However, since these methods can just perform linear transformations of the
data, they are not able to capture any nonlinear relations between the input variables.
For illustrative purposes, we have included in Figure .the projections obtained using
KMVA methods with an RBF kernel. Input data were normalized to zero mean and unit
variance, and the kernel width 𝜎was selected as the median of all pairwise distances
between samples (Blaschko et al., ). The same 𝜎has been used for all methods, so
that features are extracted from the same mapping of the input data. We can see that
the nonlinear mapping improves class separability.
Figure 12.1 Toy example of MVA methods in a three-class problem. Top: features extracted using the
training set. Figure shows the variance (var), MSE when the projection is used to approximate y, and
the largest covariance (cov) and correlation (corr) achievable using a linear projection of the target
data. The first thing we see is that linear methods (top row) perform a rotation on the original data,
while the features extracted by the nonlinear methods (second row) are based on more complicated
transforms. There is a clear difference between the unsupervised methods (PCA and KPCA) and the
supervised methods, which try to pull apart the data coming from different classes. Bottom:
classification of test data using a linear classifier trained in the transformed domain, numerical
performance is given by the overall accuracy (OA). Since in this case there is no dimensionality
reduction, all the linear methods obtain the same accuracy. However, the nonlinear methods obtain
very different results. In general, nonlinear methods obtain better results than the linear ones. A
special case is KPCA, which obtains very poor results. KPCA is unsupervised and in this case looking for
high-variance dimensions is not useful in order to discern between the classes. KPLS obtains lower
accuracy than KOPLS and KCCA, which in turn obtain a similar result since their formulation is almost
equivalent (only an extra normalization step is performed by KCCA).

Original train data
PCA
var = 0.32; MSE = 36.68
cov = 28.84; corr = 0.51
PLS
var = 0.12; MSE = 36.39
cov = 28.82; corr = 0.51
OPLS
var = 0.003; MSE = 36.37
cov = 4.89; corr = 0.51
CCA
var = 0.003; MSE = 36.38
cov = 4.93; corr = 0.52
KPCA
var = 15.95; MSE = 36.72
cov = 186.13; corr = 0.3
KPLS
var = 0.66; MSE = 35.78
cov = 115.38; corr = 0.58
KOPLS
var = 0.01; MSE = 36.36
cov = 14.72; corr = 0.36
KCCA
var = 0.003; MSE = 34.05
cov = 13.72; corr = 0.96
Original test data
PCA
OA:44.5%
PLS
OA:44.5%
OPLS 
OA:44.5%
CCA 
OA:44.5%
KPCA 
OA:38%
KPLS 
OA:84%
KOPLS 
OA:94.17%
KCCA 
OA:95.67%
Figure 12.1 (Continued)

554
Digital Signal Processing with Kernel Methods
Satellite Image Classification
Nowadays, multi- and hyperspectral sensors mounted on satellite and airborne plat-
forms may acquire the reﬂected energy by the Earth with high spatial detail and
in several spectral wavelengths. Here, we pay attention to the performance of sev-
eral KMVA methods for image segmentation of hyperspectral images (Arenas-García
and Petersen, ). The input data X is the spectral radiance obtained for each
pixel (sample) of dimension d equal to the number of spectral channels considered,
and the output target variable Y corresponds to the class label of each particular
pixel.
The data used is the standard AVIRIS image obtained from NW Indiana’s Indian
Pine test site in June .The noisy bands corresponding to the region of water
absorption have been removed, and we used pixels of dimension d = spectral
bands. The high number of narrow spectral bands induces a high collinearity among
features. Discriminating among the major crop classes in the area can be very diﬃcult.
The image is × pixels and contains quite unbalanced classes (ranging from
to pixels each). Among the available N = labeled pixels, % were used
to train the feature extractors, where the remaining % were used to test the methods.
The discriminative properties of the extracted features were assessed using a simple
classiﬁer constructed with a linear model adjusted with LS plus a “winner takes all”
activation function.
The result of the classiﬁcation accuracy during the test for diﬀerent number nf of
extracted features is shown in Figure .. For linear models, OPLS performs better
than all other methods for any number of extracted features. Even though CCA provides
similar results for nf = , it involves a slightly more complex generalized eigenproblem.
When the maximum number of projections is used, all methods result in the same error.
Nevertheless, while PCA and PLSrequire nf = features (i.e., the dimensionality
of the input space), CCA and OPLS only need nf = features to achieve virtually the
same performance.
We also considered nonlinear KPCA, KPLS, KOPLS, and KCCA, using an RBF
kernel. The width parameter of the kernel was adjusted with a ﬁvefold cross-validation
over the training set. The same conclusions obtained for the linear case apply also
to MVA methods in kernel feature space. The features extracted by KOPLS allow us
to achieve a slightly better overall accuracy than KCCA, and both methods perform
signiﬁcantly better than KPLSand KPCA. In the limit of nf, all methods achieve similar
accuracy. The classiﬁcation maps obtained for nf = conﬁrm these conclusions:
higher accuracies lead to smoother maps and smaller error in large spatially homoge-
neous vegetation covers.
Finally, we transformed all the original bands into a lower dimensional space of 
features and assessed the quality of the extracted features by standard PCA, SNR/MNF,
KPCA, and KSNR approaches. Figure .shows the extracted features in descending
order of relevance. In this unsupervised setting it is evident that incorporating the
signal-to-noise relations in the extraction with KSNR provides some clear advantages
in the form of more “noise-free” features than the other methods.
The calibrated data are available online (along with detailed ground-truth information) from
http://dynamo.ecn.purdue.edu/∼biehl/MultiSpec.

Kernel Feature Extraction in Signal Processing
555
100
101
102
Number of features
Overall accuracy (%)
kPCA
kPLS2
kCCA
PCA
PLS2
CCA
OPLS
PLS2 (61.5%)
CCA (67.5%)
kOPLS (80.4%)
RGB composite
kOPLS
40
50
60
70
80
90
Figure 12.2 Average classification accuracy (percent) for linear and KMVA methods as a function of
the number of extracted features, along some classification maps for the case of nf = 10 extracted
features.
12.3
Feature Extraction with Kernel Dependence Estimates
In this section we review the connections of feature extraction when using dependency
estimation as optimization criterion. First, we present a popular method for dependency
estimation based on kernels, the HSIC (Gretton et al., c). We will analyze the
connections between HSIC and classical feature extraction methods. After that, we will
review two methods that employ HSIC as optimization criterion to ﬁnd interesting

556
Digital Signal Processing with Kernel Methods
nf
1−3
4−6
7−9
10−12
13−15
16−18
PCA
MNF/SNR
KPCA
KSNRi
KSNRe
Figure 12.3 Extracted features from the original image by PCA, MNF/SNR, KPCA, standard (implicit)
KSNR, and explicit KSNR in the kernel space for the first nf = 18 components, plotted in RGB
composite of three top components ordered in descending importance (images converted to grey
scale for the book).
features: the Hilbert–Schmidt component analysis (HSCA) (Daniušis and Vaitkus,
) and the kernel dimensionality reduction (KDR) (Fukumizu et al. , ).
A related problem found in DSP is the BSS problem; here, one aims to ﬁnd a trans-
formation that separates the input observed signal into a set of independent signal
components. We will review diﬀerent BSS methods based on kernels.
12.3.1
Feature Extraction Using Hilbert–Schmidt Independence Criterion
MVA methods optimize diﬀerent criteria mostly based on second-order relations,
which is a limited approach in problems exhibiting higher order nonlinear feature
relations. Here, we analyze diﬀerent methods to deal with higher order statistics using
kernels. The idea is to go beyond simple association statistics. We will build our
discussion on the HSIC (Gretton et al., c), which is a measure of dependence

Kernel Feature Extraction in Signal Processing
557
between random variables. The goal of this measure is to discern if sensed variables
are related and to measure the strength of such relations. By using this measure, we can
look for features that maximize directly the amount of information extracted from the
data instead of using a proxy like correlation.
Hilbert–Schmidt Independence Criterion
The HSIC method measures cross-covariances in an adequate RKHS by using the entire
spectrum of the cross-covariance operator. As we will see, the HSIC empirical estimator
has low computational burden and nice theoretical and practical properties. To ﬁx
notation, let us consider two spaces ⊆ℝdx and ⊆ℝdy, where input and output
observations (x, y) are sampled from distribution ℙxy. The covariance matrix is
xy = 𝔼xy(xyT) −𝔼x(x)𝔼y(yT),
(.)
where 𝔼xy is the expectation with respect to ℙxy, and 𝔼x is the marginal expectation
with respect to ℙx (here and below, we assume that all these quantities exist). First-
order dependencies between variables are included into the covariance matrix, and
the Hilbert–Schmidt norm is a statistic that eﬀectively summarizes the content of
this matrix. The squared sum of its eigenvalues 𝛾i is equal to the square of this
norm:
‖xy‖
HS =
∑
i
𝛾
i .
(.)
This quantity is zero only in the case where there is no ﬁrst-order dependence
between x and y. Note that the Hilbert–Schmidt norm is limited to the detection
of ﬁrst-order relations, and thus more complex (higher order eﬀects) cannot be
captured.
The previous notion of covariance operator in the ﬁeld of kernel functions and
measures was proposed in (Gretton et al., c). Essentially, assume a (non-linear)
mapping 𝝓∶→in such a way that the dot product between features is given by
a positive deﬁnite kernel function Kx(x, x′) = ⟨𝝓(x), 𝝓(x′)⟩. The feature space has the
structure of an RKHS. Assume another mapping 𝝍∶→with associated kernel
Ky( y, y′) = ⟨𝝍( y), 𝝍( y′)⟩. Under these conditions, a conditions, a cross-covariance
operator between these mappings can be deﬁned, similar to the covariance matrix in
Equation .. The cross-covariance is a linear operator xy ∶→of the form
xy = 𝔼xy[(𝝓(x) −𝜇x) ⊗(𝝍(y) −𝜇y)],
(.)
where ⊗is the tensor product, 𝜇x = 𝔼x[𝝓(x)], and 𝜇y = 𝔼y[𝝍(y)]. See more details
in Baker () and Fukumizu et al. (). The HSIC is deﬁned as the squared norm

558
Digital Signal Processing with Kernel Methods
of the cross-covariance operator, ‖xy‖
HS, and it can be expressed in terms of kernels
(Gretton et al., c):
HSIC(, , ℙxy) = ‖xy‖
HS
= 𝔼xx′YY ′[Kx(x, x′)Ky(y, y′)]
+ 𝔼xx′[Kx(x, x′)]𝔼yy′[Ky(y, y′)]
−𝔼xy[𝔼x′[Kx(x, x′)]𝔼y′[Ky(y, y′)]],
where 𝔼xx′yy′ is the expectation over both (x, y) ∼ℙxy and an additional pair of variables
(x′, y′) ∼ℙxy drawn independently according to the same law.
Now, given a sample dataset Z = {(x, y), … , (xN, yN)} of size N drawn from ℙxy, an
empirical estimator of HSIC is (Gretton et al., c)
HSIC(, , ℙxy) = 
NTr(K xHK yH) = 
NTr(HK xH K y),
where Tr is the trace (the sum of the diagonal entries), K x and K y are the kernel matrices
for the data x and the labels y respectively, and Hij = 𝛿ij −(∕N) centers the data and
the label features in and . Here, 𝛿represents the Kronecker symbol, where 𝛿ij = if
i = j, and zero otherwise.
Summarizing, HSIC (Gretton et al., c) is a simple yet very eﬀective method
to estimate statistical dependence between random variables. HSIC corresponds to
estimating the norm of the cross-covariance in , whose empirical (biased) estimator
is HSIC ∶= [∕(N −)]Tr(K xK y), where K x and K y are kernels working on data from
sets and . It can be shown that, if the RKHS kernel is universal, such as the RBF
or Laplacian kernels, HSIC asymptotically tends to zero when the input and output
data are independent, and HSIC = if ℙx = ℙy. Note that, in practice, the actual
HSIC is the Hilbert–Schmidt norm of an operator mapping between potentially inﬁnite-
dimensional spaces, and thus would give rise to an inﬁnitely large matrix. However, due
to the kernelization, the empirical HSIC only depends on computable matrices of size
N × N. We will come back to the issue of computational cost later.
Relation between Hilbert–Schmidt Independence Criterion
and Maximum Mean Discrepancy
In Chapter we introduced the MMD for hypothesis testing. We can show that
MMD is related to HSIC. Let us deﬁne the product space × with a kernel
⟨𝝓(x, y), 𝝓(x′, y′)⟩= K((x, y), (x′, y′)) = K(x, y)L(x′, y′), and the mean elements are
given by
⟨𝜇x,y, 𝝓(x, y)⟩= 𝔼x′,y′⟨𝝓(x′, y′), 𝝓(x, y)⟩= 𝔼x,x′K(x, x′)L( y, y′),
and
⟨𝜇x⟂y, 𝝓(x, y)⟩= 𝔼x′,y′⟨𝝓(x′, y′), 𝝓(x, y)⟩= 𝔼x′K(x, x′)𝔼y′L( y, y′).

Kernel Feature Extraction in Signal Processing
559
Therefore, the MMD between these two mean elements is
MMD(ℙ, ℙx, ℙy, × ) = ‖𝜇x,y −𝜇x⟂y‖
×= HSIC(ℙ, , ).
Relation between Hilbert–Schmidt Independence Criterion and Other Kernel
Dependence Measures
HSIC was not the ﬁrst kernel dependence estimate presented in the literature. Actually,
Bach and Jordan () derived a regularized correlation operator from the covariance
and cross-covariance operators, and its largest singular value (the KCCA) was used
as a statistic to test independence. Later, Gretton et al. (a) proposed the largest
singular value of the cross-covariance operator as an eﬃcient alternative that needs
no regularization. This test is called the constrained covariance (COCO) (Gretton
et al., a). A variety of empirical kernel quantities derived from bounds on the
mutual information that hold near independence were also proposed, namely the kernel
generalized variance (KGV) and the kernel mutual information (kMI) (Gretton et al.,
b). Later, HSIC (Gretton et al., c) was introduced, hence extending COCO
by using the entire spectrum of the cross-covariance operator, not just the largest
singular value. As we have seen, a related statistic is the MMD criterion, which tests
for statistical diﬀerences between embeddings of probability distributions into RKHS.
When MMD is applied to the problem of testing for independence, the test statistic
reduces to HSIC. A simple comparison between all the methods treated in this section
is given in Figure .. Listing .gives a simple MATLAB implementation comparing
Pearson’s correlation and HSIC in this particular example. The book’s repository pro-
vides MATLAB tools and links to relevant toolboxes on (kernel) dependence estimation.
% Number of examples
N = 1000;
% Distribution 1: correlation/linear association happens
X = rand(N,1); Y = X + 0.25*rand(N,1); X = X + 0.25*rand(N,1);
% Distribution 2: no correlation, but dependence happens
t = 2*pi*(rand(N,1)-0.5);
X = cos(t) + 0.25*rand(N,1); Y = sin(t) + 0.25*rand(N,1);
% Distribution 3: neither correlation nor dependence exist
X = 0.25*rand(N,1); Y = 0.25*rand(N,1);
% Get dependence estimates
C
= corr(X,Y); % Correlation
HSIClin
= hsic(X,Y,'lin',[]);
sigmax
= estimateSigma(X,X); HSICrbf = hsic(X,Y,'rbf',sigma);
MI
= mutualinfo(X,Y);
function h = hsic(X,Y,ker,sigma)
Kx = kernelmatrix(ker,X,X,sigma); Kxc = kernelcentering(Kx);
Ky = kernelmatrix(ker,Y,Y,sigma); Kyc = kernelcentering(Ky);
h
= trace(Kxc*Kyc)/(size(Kxc,1).^2);
function m = mutualinfo(X,Y)
binsx = round(sqrt(size(X,1))); [hh rr] = hist(X,binsx);
pp = hh/sum(hh); h1 = -sum(pp.*log2(pp)) + log2(rr(3)-rr(2));
binsy = round(sqrt(size(Y,1))); [hh rr] = hist(Y,binsy);
pp = hh/sum(hh); h2 = -sum(pp.*log2(pp)) + log2(rr(3)-rr(2));

560
Digital Signal Processing with Kernel Methods
[hh rr] = hist3([X Y],[binsx binsy]); pp = hh(:)/sum(hh(:));
h12 = -sum(pp.*log2(pp)) + log2((rr{1}(3)-rr{1}(2))*(rr{1}(2)-rr{2}(2)));
MI3 = h1 + h2 - h12;
Listing 12.2 MATLAB code comparing correlation and dependence estimation, using Pearson’s
correlation coefficient, mutual information, and the HSIC with both the linear and the RBF kernels.
Pearson’s R
Mutual information
HSIC (linear kernels)
HSIC (RBF kernels)
MMD
COCO
KGV
kMI (θ=1/2)
0.9571
0.0075
–0.0646
1.1564
0.7220
0.0190
0.8979
0.0000
0.0000
0.0560
0.0048
0.0006
0.0018
0.0018
0.0058
0.2205
0.0715
0.0694
8.3427
6.0186
4.2353
7.8198
5.5939
3.8900
Figure 12.4 Dependence estimates for three examples revealing (left) high correlation (and hence
high dependence), (middle) high dependence but null correlation, and (right) zero correlation and
dependence. The Pearson correlation coefficient R and linear HSIC only capture second-order statistics
(linear correlation), while the rest capture in general higher order dependences. Note that, for MMD,
the higher the more divergent (independent), while KGV upper bounds kMI and mutual information.
Relations between Hilbert–Schmidt Independence Criterion and the Kernel
Multivariate Analysis Framework
HSIC constitutes an interesting framework to study KMVA methods for dimension-
ality reduction. We summarize the relations between the methods under the HSIC
perspective in Table .. For instance, HSIC reduces to PCA for the case of linear
kernels. Recall that PCA reduces to ﬁnd an orthogonal projection matrix V such that
the projected data, XV, preserves the maximum variance; that is, maxV ‖XV‖=
maxV Tr((XV)T(XV)) subject to V TV = I, which may be written as follows:
max
V
Tr(V TXTXV) = max
V
Tr(XTX VV T)
s.t.
V TV = I.
Therefore, PCA can be interpreted as ﬁnding V such that its linear kernel has maximum
dependence with the kernel obtained from the original data. Assuming that X is again
centered, there is no need for including the centering matrix H in the HSIC estimate.
From HSIC, one can also reach KPCA (Schölkopf et al., ) by using a kernel matrix
K for the data X and requiring the second random variable y to be orthogonal and unit
norm; that is:
max
Y
Tr(KYY T) = max
Y
Tr(Y TKY)
s.t.
Y TY = I,

Kernel Feature Extraction in Signal Processing
561
Table 12.2 Relation between dimensionality reduction methods and HSIC, HSIC ∶= Tr(HKxHKy), where
all techniques use the orthogonality constraint YTY = I.
Method
Kx
Ky
Comments
PCA (Jolliﬀe, )
XXT
YY T
—
KPCA (Schölkopf et al., )
Kx
YY T
Typically RBF kernels used
Maximum variance unfolding
(MVU) (Song et al., )
Kx
LLT
Subject to positive deﬁniteness Kx ⪰,
preservation of local distance structure
Kii + Kjj −Kij = d
ij, and L is a coloring
matrix accounting for the side informa-
tion
Multidimensional scaling
(MDS) (Cox and Cox, )
−
HDH
YY T
D is the matrix of all pairwise Euclidean
distances
Isomap (Tenenbaum et al.,
)
−
HDgH
YY T
Dg is the matrix of all pairwise geodesic
distances
Locally linear embedding
(LLE) (Roweis and Saul, )
L†, or 𝜆maxI −L
YY T
L = (I−V)(I−V)T, where V is the matrix
of locally embedded weights
Laplacian eigenmaps (Belkin
and Niyogi, )
L†, or 𝜆maxI −L
YY T
L = D −W, where W is a positive
symmetric aﬃnity matrix and D is the
corresponding degree matrix
where Y is any real-valued matrix of size N × d. Note that KPCA does not require any
other constraint imposed on Y, but for other KMVA methods, as well as for clustering
and metric learning techniques, one may require to design Y to include appropriate
restrictions.
Maximizing HSIC has also been used in dimensionality reduction with (colored)
MVU (Song et al., ). This manifold learning method maximizes HSIC between
source and target data, and accounts for the manifold structure by imposing local
distance constraints on the kernel. In the original version of the MVU, a kernel K is
learned via maximizing Tr(K) subject to some constraints. Assuming a similarity/dis-
similarity matrix B, one can modify the original MVU by maximizing Tr(KB) instead
of Tr(K) subject to the same constraints. This way, one simultaneously maximizes the
dependence between the learned kernel K and the kernel B storing our side information.
Note that, in this formulation, we center matrix K via one of the constraints in the
problem, and thus can exclude matrix H in the empirical estimation of HSIC.
The HSIC framework can also help us to analyze relations to standard dimensionality
reduction methods like MDS (Cox and Cox, ), Isomap (Tenenbaum et al., ),
LLE (Roweis and Saul, ) and Laplacian eigenmaps (Belkin and Niyogi, )
depending on the deﬁnition of the kernel K used in Equation ... Ham et al. ()
pointed out the relation between KPCA and these methods.
Hilbert–Schmidt Component Analysis
The HSIC empirical estimate can be speciﬁcally incorporated in feature extraction
schemes. For example, the so-called HSCA method in Daniušis and Vaitkus ()

562
Digital Signal Processing with Kernel Methods
iteratively seeks for projections that maximize dependence with the target variables
and simultaneously minimize the dependence with previously extracted features, both
in HSIC terms. This can be seen as a Rayleigh coeﬃcient that leads to the iterative
resolution of the following generalized eigendecomposition problem:
K xK yK x𝜦= 𝜆K xK f K x𝜦,
(.)
where K f is a kernel matrix of already extracted projections in the previous iteration.
Note that if one is only interested in maximizing source–target dependence in HSIC
terms, and uses a linear kernel for the targets K y = YY T, the problem reduces to
K xK yK x𝜦= 𝜆K xK x𝜦,
(.)
which interestingly is the same problem solved by KOPLS (see Table .). The equiv-
alence means that features extracted with KOPLS are those that maximize statistical
dependence (measured by HSIC) between the projected source data and the target
data (Izquierdo-Verdiguier et al., ).
Kernel Dimensionality Reduction
KDR is a supervised feature extraction method that seeks a linear transformation of
the data such that it maximizes the conditional HSIC on the labels. Notationally, the
input data matrix X ∈ℝN×d, the output (label) matrix is Y ∈ℝN×m, and W ∈ℝd×r
is a projection matrix from the d-dimensional space to a r-dimensional space, r ≤d.
Hence, the linear projection is Z = XW, which is constrained to be orthogonal; that
is, W TW = I. The optimal W in HSIC terms is obtained by solving the constrained
problem
max
W
HSIC(XW, Y) = max
W Tr( ̃K XW ̃K Y)
s.t.
W TW = I.
This approach has been followed for both supervised and unsupervised settings:
●Fukumizu et al. (, ) introduced KDR under the theory of covariance
operators (Baker, ). KDR reduces to the KGV introduced by Gretton et al.
(b) as a contrast function for ICA, in which the goal is to minimize mutual
information. They showed that KGV is in fact an approximation of the mutual
information among the recovered sources around the factorized distributions. The
interest in KDR is diﬀerent: the goal is to maximize the mutual information as a good
proxy to the problem of dimensionality reduction. Interestingly, the computational
problem involved in KDR is the same as in KICA (Bach and Jordan, ).
●The unsupervised KDR (Wang et al., ) reduces to seeking W such that the signal
X and its (regression-based) approximation ̂X = f (XW) are mutually independent,
given the projection; that is X ⟂
̂X | XW. The unsupervised KDR method then
reduces to
max
W
HSIC(XW, Y) = max
W Tr( ̃K X ̃K XW)
s.t.
W TW = I.

Kernel Feature Extraction in Signal Processing
563
The problem is typically solved by gradient-descent techniques with line search, which
constrains the projection matrix W to lie on the Grassmann–Stiefel manifold of
W TW = I.
Let us illustrate the performance of the supervised KDR compared with other linear
dimensionality reduction methods for the Wine dataset, which is a -dimensional
problem obtained from the UCI repository (http://archive.ics.uci.edu/ml/), where here
it is used to illustrate projection onto a D subspace. Figure .shows the projection
onto the D subspace estimated by each method. This example illustrates that optimiz-
ing HSIC to achieve linear separability is an alternative valid approach to maximize
input–output covariance (PLS) or maximize correlation (CCA). Figure .can be
reproduced by downloading the corresponding MATLAB toolboxes from the book’s
repository and executing the script Demo_Fig_14_4.m in the supplementary material
provided. A simpliﬁed version of the code is shown in Listing ..
%% Data
[XX Yb] = wine_dataset;
XX = XX';
XX = XX-repmat(min(XX),size(XX,1),1);
XX = XX./repmat(max(XX),size(XX,1),1);
[YY aa] = find(Yb==1);
ii = randperm(size(XX,1));
X = XX(ii(1:size(XX,1)),:);
Y = YY(ii(1:size(XX,1)),:);
Xts = XX(ii(size(XX,1)/2+1:size(XX,1)),:);
Yts = YY(ii(size(XX,1)/2+1:size(XX,1)),:);
%% Feature extraction settings and projections
nf = 2;
% PLS
[U_pls Ypred_PLS]=predictPLS(X,X,Y,nf);
XtsProj_PLS = Xts * U_pls.basis(:,1:nf);
XProj_PLS = X * U_pls.basis(:,1:nf);
Ypred_PLS = classify(XtsProj_PLS,XProj_PLS,Y);
% CCA
U_cca = cca(X,Y,nf);
XtsProj_CCA = Xts * U_cca.basis(:,1:nf);
XProj_CCA = X * U_cca.basis(:,1:nf);
Ypred_CCA = classify(XtsProj_CCA,XProj_CCA,Y);
% KDR
[U_kdr.basis, t] = KernelDeriv(X,Y,2,1,1,1);
XtsProj_KDR = Xts * U_kdr.basis(:,1:nf);
XProj_KDR = X * U_kdr.basis(:,1:nf);
Ypred_KDR = classify(XtsProj_KDR,XProj_KDR,Y);
Listing 12.3 MATLAB code comparing PLS, CCA, and KDR for feature extraction.
12.3.2
Blind Source Separation Using Kernels
Extracting sources from observed mixed signals without supervision is the problem of
BSS; see Chapter for an introductory review to the ﬁeld. Several approaches exist
in the literature to solve the problem, but most of them have relied on ICA (Comon
; Gutmann et al. ; Hyvärinen et al. ). ICA seeks for a basis system
where the dimensions of the signal are as independent as possible. Kernel dependence

564
Digital Signal Processing with Kernel Methods
Original
PLS
CCA
KDR
Figure 12.5 Projections obtained by different linear projection methods on the Wine data available at
the UCI repository (http://archive.ics.uci.edu/ml/). Black and two different grey intensities represent
three different classes.
estimates based either on KCCA or HSIC have given rise to kernelized versions of ICA,
leveraging better performance at the cost of higher computational cost. An alternative
approach to ICA-like techniques consists of exploiting second-order BSS methods in
an RKH,S like ﬁnding orthonormal basis of the submanifold formed by kernel-mapped
data (Harmeling et al., ).
Independent Component Analysis
The instantaneous noise-free ICA model takes the form X = SA, where S ∈ℝN×s is a
matrix containing N observations of s sources, A ∈ℝs×s is the mixing matrix (assumed
to have full rank), and X ∈ℝN×s contains the observed mixtures. We denote as s and x
single rows of matrices S and X respectively, and si is the ith source in s. ICA is based
on the assumption that the components si of components si of s, for all i = , … , s
are mutually statistically independent; that is, the observed vector x depends only on
the source vector s at each instant and the source samples s are drawn independently
and identically from the pdf ℙs. As a conclusion, the mixture samples x are likewise
drawn independently and identically from the pdf ℙx. The task of ICA is to recover the
independent sources via an estimate B ∈ℝs×s of the inverse of the mixing matrix A such
that the recovered signals Y = SAB ∈ℝN×s have mutually independent components.
When the sources s are Gaussian, A can be identiﬁed up to an ordering and scaling
of the recovered sources via a permutation matrix P ∈ℝs×s and a scaling matrix 𝐃.
Very often the mixtures X are pre-whitened via PCA, W = XV = SAV ∈ℝN×s, so
𝔼[wwT] = I, where W contains the whitened observations. Now, assuming that the
sources si have zero mean and unit variance, AV is orthogonal and the unmixing model
becomes Y = WQ, where Q is an orthogonal unmixing matrix (QQT = I) and Y ∈
ℝN×s contains the estimates of the sources. The idea of ICA can also be employed for
optimizing CCA-like problems (for instance, see Gutmann et al. ()), where higher
order relations are taken into account in order to ﬁnd the basis to represent diﬀerent
datasets in a canonical space.
Kernel Independent Component Analysis Using Kernel Canonical Correlation Analysis
The original version of KICA was introduced by Bach and Jordan (b), and uses a
KCCA-based contrast function to obtain the unmixing matrix W. For “rich enough”
kernels, such as the Gaussian kernel, it was shown that the components of the ran-
dom vector x are independent if and only if their ﬁrst canonical correlation in the

Kernel Feature Extraction in Signal Processing
565
Original
(7.87, 12.96)
Mixed signals
(7.12, 7.80)
Unmixed KICA
(7.87, 12.97)
Unmixed ICA
(7.87, 12.96)
Figure 12.6 Demixing example using KICA. From left to right we show the original source signals, the
original (linearly) mixed signals, and the results obtained by KICA and ICA. We indicate the kurtosis in
every dimension.
corresponding RKHS is equal to zero. KICA utilizes a gradient-descent approach to
minimize a KCCA-based contrast function, taking into account that whitening the
features will have the eﬀect of that the unmixing matrix W will have orthonormal
vectors. KICA has better performance than the previous three algorithms and also it
is more robust against outliers and near-Gaussianity of the sources. However, these
performance improvements come at a higher computational cost. Figure .shows
an example of using KICA to unmix signals that have been mixed using a linear
combination. The example can be reproduced using the KICA toolbox linked in the
book’s repository and the code in Listing ..
%% Example of ICA and Kernel ICA
% Number of samples
N = 1000;
s(1,:) = sign(randn(1,N)).*abs(randn(1,N)).^2;
s(2,:) = sign(randn(1,N)).*abs(randn(1,N)).^2;
randn('seed',2); W = randn(2);
% Mixing
x
= W*s;
% ICA Unmixing
[ss Aica Wica] = fastica(x); % function in fastICA toolbox
s_hat_ica
= Wica*x;
% kICA Unmixing
Wcca = kernel_ica(x); % function in Bach's toolbox
s_hat_kica
= Wcca*x;
Listing 12.4 MATLAB snippet for applying ICA and KICA.
Kernel Independent Component Analysis Using Hilbert–Schmidt Independence Criterion
KICA based on minimizing HSIC as a contrast function has been recently introduced
(Shen et al. , ). The motivation is clear: in the ICA model, the components si
of the sources s are mutually statistically independent if and only if their pdf factorizes
ℙs = ∏s
i=ℙsi. The problem in this assumption is that pairwise independence does
not imply mutual independence (while it holds vice versa). The solution here is that,
in the ICA setting, unmixed components can be uniquely identiﬁed using only the

566
Digital Signal Processing with Kernel Methods
pairwise independence between components of the recovered sources Y, since pairwise
independence between components of Y in this case implies their mutual independence
(and thus recovery of the sources S). Hence, by summing all unique pairwise HSIC
measures, an HSIC-based contrast function over the estimated signals Y is deﬁned as
H(Q) =
m
∑
≤i<j≤m
𝔼kl[𝝓(qT
i wkl)𝝓(qT
j wkl)]
(.)
+ 𝔼kl[𝝓(qT
i wkl)]𝔼kl[𝝓(qT
j wkl)]
(.)
−𝔼k[𝔼l𝝓(qT
i wkl)]𝔼k[𝝓(qT
j wkl)],
(.)
where wkl = wk −wl ∈ℝm is the diﬀerence between the kth and the lth samples of the
whitened observations, and 𝔼kl[⋅] represents the empirical expectation over all k and l.
The expression of H(Q) reduces to just summing over all possible HSIC estimates of Y.
Intuitively, one can see that the goal in KICA is to ﬁnd an unmixing matrix Q such that
the dependence between the estimated unmixed sources QTW is minimal in terms of
the HSIC between all pairs of signals. The algorithm was ﬁrst introduced by Bach and
Jordan (), and later extended to a fast implementation known as FastKICA that
uses an approximate Newton method to perform this optimization (Shen et al. ,
). The fastKICA package is a convenient implementation of the method.
Kernel Blind Source Separation
The previous approaches are based on introducing kernel dependence estimates as con-
trast functions in ICA. Alternatively, one can follow the standard “kernelization” pro-
cedure: ﬁrst the data are (implicitly) mapped to a high (possibly inﬁnite)-dimensional
kernel feature space, and then a linear algorithm is deﬁned and solved (implicitly)
therein by means of the kernel trick. Let us review the two main approaches available
in the literature, and their relations to KMVA.
Harmeling et al. () followed the standard kernelization of BSS based on second-
order statistics. In particular, the method exploits temporal decorrelation in Hilbert
spaces via the time-delayed second-order projection (TDSEP) technique, which relies
on simultaneous diagonalization techniques to perform linear blind source separation
on the projected data. Therefore, one obtains a number of linear directions of separated
nonlinear components in input space. The algorithm was coined kernel TDSEP. After
embedding, data form a smaller submanifold in feature space, which is typically smaller
than the number of training data points. The approach then tries to adapt to this sort
of eﬀective dimension as a preprocessing step and to construct an orthonormal basis of
this submanifold. This ﬁrst step performs standard dimensionality reduction.
In particular, Harmeling et al. () proposed to construct an orthonormal basis in
feature spaces from a subset of selected data points therein. Given T observed signals
{xt ∈ℝd}T
t=, and N vectors {vi ∈ℝd}N
i=, we deﬁne the corresponding kernel matrices
K x = 𝜱x𝜱T
x and K x = 𝜱v𝜱T
v. Now let us assume that the span of 𝜱x is the same as the
span of 𝜱v and the rank of 𝜱v is N. Since 𝜱v constitutes a basis, K v is full rank and has an
inverse. The orthonormal basis can thus be deﬁned as V = (𝜱v𝜱T
v)−∕𝜱v, and hence
projecting new data onto the basis reduces simply to 𝜳x[t] = (𝜱v𝜱T
v)−∕𝜱v𝜱T
x[t] =
K −
v K vx. The deﬁnition of the basis could be actually done extracting actually done

Kernel Feature Extraction in Signal Processing
567
extracting the most relevant directions in kernel feature spaces via the diagonalization of
the full kernel matrix; that is, via KPCA. Nevertheless, the alternative approach to form
the basis is very useful for making the subsequent application of BSS linear methods
computationally and
Kernel Entropy Component Analysis
Kernel entropy component analysis (KECA) was proposed by Jenssen (, ) to
implement a feature extractor according to the entropy components. KECA, like KPCA,
is a spectral method based on the kernel similarity matrix. Nevertheless, KECA does not
necessarily use the top eigenvalues and eigenvectors of the kernel matrix. Unlike KPCA,
which preserves maximally the second-order statistics of the dataset, KECA is founded
on information theory and tries to preserve the maximum Rényi entropy of the input
space dataset. KECA has proven useful in, for example, remote sensing (Gómez-Chova
et al., ; Luo and Wu, ; Luo et al., ), face recognition (Shekar et al., ), and
some other applications (Hu et al. ; Jiang et al. ; Xie and Guan ). Several
extensions have been proposed for feature selection (Zhang and Hancock, ), class-
dependent feature extraction (Cheng et al., ), and SSL as well (Myhre and Jenssen,
).
The KECA algorithm relies on the eigendecomposition of the (uncentered) kernel
matrix, and sorts the eigenvectors according to the entropy values of the projections.
Given a dataset = {x, … , xN} of dimensionality d, the entropy may be estimated
through kernel density estimation (Silverman, ) as −log V, where Vis the
so-called information potential (Principe, ):
V(x) =
Nc
∑
j=
( N
∑
i=
Aij
)
.
(.)
In this expression, Nc ≤N is the number or retained components and the matrix
A is obtained from the (N × N) kernel or Gram matrix K whose entries are the
kernel function values K(xi, xj). Equation .is based on the kernel decomposition
introduced in Jenssen ():
K = AAT = (ED

)(D

ET),
(.)
where E contains the eigenvectors in columns, E = [e| ⋯|eN], and D is a diagonal
matrix containing the eigenvalues of K; that is, Dii = 𝜆i. An illustrative MATLAB code
snippet for KECA is provided in Listing ..
%% KECA implementation
sigma
= estimateSigma(X);
K
= kernelmatrix('rbf',X,X,sigma);
[E D]
= eig(K);
V2
= sum((E*(D.^0.5))).^2;
[V2 ind] = sort(V2,'descend');
Vkeca
= E(:,ind);
Listing 12.5 MATLAB code example for the implementation of KECA.

568
Digital Signal Processing with Kernel Methods
KECA uses the Rényi entropy to sort the basis extracted by PCA. A novel pro-
posal (Izquierdo-Verdiguier et al., ), the optimized KECA (OKECA) method, looks
directly for the basis that maximizes the Rényi entropy, therefore maximizing the
information using as few components as possible. OKECA is based on a similar idea to
ICA, and optimizes an extra rotation matrix on top of PCA directions. The new kernel
matrix decomposition is expressed as
K = FFT = (ED

Q)(QTD

ET),
(.)
where W is an orthonormal linear transformation; that is, QQT = I. In order to solve
the OKECA decomposition, OKECA resorts to a gradient-ascent approach. Note that
OKECA is more computationally demanding than KECA because not only requires
an eigendecomposition of a kernel matrix but also the gradient ascent procedure to
reﬁne the ﬁnal features. An illustrative MATLAB code snippet for OKECA is provided
in Listing ..
%% OKECA implementation
% (Step 1) KECA-like part
% X
: input data
% Q
: extra rotation matrix
% gdit: #iterations in gradient descent
% dim : number of dimensions
Q = zeros(dim);
sigma = estimateSigma(X,X); K=kernelmatrix('rbf',X,X,sigma);
[E D] = eigs(K); [V2 ind] = sort(diag(D)); E = E(:,ind);
dD = diag(D); D = diag(dD(ind)); A = E*(D.^0.5);
% (Step 2) Optimization of the projections via gradient descent
tau = 1; % learning rate of the gradient descent procedure
for di = 1:size(K,1)
m = rand(dim,1);
m = m/sqrt(sum(m.^2));
if di<dim
for it = 1:gdit
dJ = 2*(sum(A*m))*sum(A,1)';
mn = m
+ tau*dJ;
mn = mn - sum((repmat((Q'*mn)',dim,1).*Q),2);
mn = mn/sqrt(sum(mn.^2));
end
end
Q(:,di) = mn;
end
F = A*Q;
Listing 12.6 MATLAB code example for OKECA.
In general, both KECA and OKECA are diﬀerent from (but still intimately related to)
KMVA methods. On the one hand, they maintain a probabilistic input space interpre-
tation, seek to capture the entropy of the data in a reduced number of components,
and constitute a convergence point between kernel methods and information theoretic
learning (Jenssen, ). On the other hand, KPCA, KCCA, and KPLS maximize the
variance, correlation, or alignment (covariance) with the output variables in kernel
feature space respectively. The similarities between these methods arise from the use of
a kernel function and from the exploitation of the spectral (eigenvalues and eigenvectors)

Kernel Feature Extraction in Signal Processing
569
PDF
nf
1
2
3
4
5
σML
KECA
OKECA
σd2
KECA
OKECA
Figure 12.7 Density estimation for the ring dataset by KECA and OKECA using different number of
extracted features nf and estimates of the kernel length-scale parameter 𝜎(ML or squared distance).
Black color represents low pdf values and yellow color high pdf values.
properties of the corresponding kernel matrices. KECA and OKECA can be used for
density estimation. As explained by Girolami (b), if the decomposition of the
uncentered kernel matrix follows the form K = EDET, where E is orthonormal and
D is a diagonal matrix, then the kernel-based density estimation may be expressed as
̂p(𝐱∗) = 𝟏T
NEnfET
nfk∗,
(.)
where Enf is the reduced version of E by keeping columns for nf < N, and as usual
k∗∶= [K(x∗, x), … , K(x∗, xN)]T. Therefore, density estimation ̂p can be readily done by
ﬁxing a number of extracted features nf. Figure .illustrates the ability of KECA and
OKECA for density estimation in a ring distribution. Note that OKECA concentrates
most of the entropy information in the ﬁrst projection. This agrees with the fact that
few components are needed to obtain a good pdf estimation. On the contrary, KECA
cannot estimate correctly the pdf using only the ﬁrst component and actually needs at
least ﬁve components. We show results for two standard ways to estimate the 𝜎length-
scale parameter: an ML strategy, 𝜎ML following Duin (), and setting 𝜎to % of the
median distance between points following prescriptions in Jenssen (). In both cases
OKECA outperforms KECA.

570
Digital Signal Processing with Kernel Methods
12.4
Extensions for Large-Scale and Semi-supervised Problems
An important problem in kernel-based feature extraction is related to the computa-
tional cost. Since K x is of size N × N, method complexity scales quadratically with N
in terms of memory, and cubically with respect to the computation time. The opposite
situation often occurs as well: when N is small, the extracted features may be useless,
especially for high-dimensional (Abrahamsen and Hansen, ). These issues limit
the applicability of kernel-based feature extraction methods in real-life scenarios with
either very large or very small labeled datasets. We next summarize some extensions
to deal with large-scale problems and semi-supervised situations in which few labeled
data are available.
12.4.1
Efficiency with the Incomplete Cholesky Decomposition
Very often, feature extraction methods on kernels rely on the calculation of traces on
products of kernel matrices; for instance, see HSCA or KICA that try to maximize
the HSIC measure between some random variables. This operation is simple yet
computationally demanding when a high number of samples are available. In order to
speed this operation up, one can rely on sparse approximations of the basis spanning
the solution (as we will see in the next section), by intelligent dataset subsampling
(e.g., via AL), or Nyström approximations of the kernel matrices. A convenient alter-
native is to exploit low-rank approximations of the kernel matrices via Cholesky
decomposition.
Notationally, given two kernel matrices K and K , the cost associated with the
operation Tr(K K ) can be greatly reduced via incomplete Cholesky estimation. An
incomplete Cholesky decomposition of a Gram matrix K i yields a low-rank approxi-
mation, K i ≈GiGT
i that greedily minimizes Tr(K i −GiGT
i ). The cost of computing the
N × r matrix Gi is (Nr), with r ≪N. Greater values of r result in a more accurate
reconstruction of K i. Interestingly, it is well known that the spectrum of a Gram matrix
based on the (RBF) Gaussian kernel generally decays rapidly, and a small r yields a
very good approximation. Plugging the Cholesky eigendecomposition of the two kernels
leads to
Tr(K K ) = Tr(GGT
GGT
) = Tr(GT
GGT
G),
which involves an equivalent trace of a much smaller matrix, so we can avoid computing
the N × N matrices and just do r× rmatrix calculations. This technique has been
widely used in KMVA and also in kernel dependence estimation methods such as the
HSIC (Gretton et al., b).
12.4.2
Efficiency with Random Fourier Features
We have already seen the eﬀectiveness of approximating shift-invariant kernels, using
projections on random Fourier features (Rahimi and Recht, , ). This technique
was illustrated in classiﬁcation and regression settings; see Section ... Let us show
the application of the approximation in HSIC presented in Pérez-Suay and Camps-Valls

Kernel Feature Extraction in Signal Processing
571
(). Following previous notation, recall that K ∈ℝN×N can be approximated with
the explicitly mapped data via z(x) ∶= [exp(𝕚wT
x), … , exp(𝕚wT
Dx)]T, and collectively
grouped in the matrix Z = [z| ⋯|zN]T ∈ℝN×D, and will be denoted as ̂K ≈ZZT.
The familiar SE Gaussian kernel K(x, x′) = exp(−‖x −x′‖∕(𝜎)) can be approximated
using D random features, wi ∼(𝟎, 𝜎−I), ≤i ≤D. The randomized HSIC (RHSIC)
for fast dependence estimation in large-scale problems is developed as follows: kernel
matrices K x and K y are approximated using complex exponentials of projected data
matrices, Zx = exp(𝕚XW x)∕
√
Dx ∈ℂN×Dx and Zx = exp(𝕚XW x)∕
√
Dx ∈ℂN×Dx and
Zy = exp(𝕚YW y)∕
√
Dy ∈ℂN×Dy, where W y = [wy
| ⋯|wy
Dy] ∈ℝdy×Dy, both drawn from
a Gaussian. Now, plugging the corresponding kernel approximations, ̂K x = ̃Zx ̃Z
T
x and
̂K y = ̃Zy ̃Z
T
y , into Equation .., and after manipulating terms, we obtain
RHSIC(, , ℙ𝐱𝐲) = 
NRe{Tr( ̃Z
T
x ̃Zy ̃Z
T
y ̃Zx)},
(.)
which corresponds to the Hilbert–Schmidt norm of the randomized cross-covariance
operator, ̂𝐱𝐲= Re{ ̃Z
T
x ̃Zy} ∈ℝDx×Dy, which can be computed explicitly and just once.
The computational complexity of RHSIC reduces considerably over the original HSIC.
A naive implementation of HSIC runs (N), while the RHSIC cost is (ND), where
D = Dx = Dy, since computing matrices Z only involves matrix multiplications and
exponentials. We want to note that other KMVA methods have enjoyed use of this
randomization technique to expedite performance, such as KCCA and KPCA (López-
Paz et al., ).
12.4.3
Sparse Kernel Feature Extraction
To address the problems of large-scale computation and dense solutions in KMVA,
several solutions have been proposed. The family of KMVA sparse methods tries to
obtain solutions that can be expressed as a combination of a reduced subset of the
training data, and therefore require only r kernel evaluations per sample (being r ≪N)
for feature extraction. In contrast to the many linear MVA algorithms that induce
sparsity with respect to the original variables, we will only review methods attaining
sparse solutions in terms of the samples (i.e., sparsity in the 𝜶i vectors).
Roughly speaking, sparsiﬁcation methods can be classiﬁed into low-rank approxima-
tion methods that aim at working with reduced r × r matrices (r ≪N), and reduced set
methods that work with N × r matrices. Following the ﬁrst approach, the Nyström low-
rank approximation of an N ×N kernel matrix K NN is expressed as ̃K NN = K NrK −
rr K rN,
where subscripts indicate row and column dimensions. This method was ﬁrst used in
Gaussian processes, and later by Hoegaerts et al. () in order to approximate the fea-
ture mapping instead of the kernel, which leads to sparse versions of KPLS and KCCA.
Among the reduced set methods, a sparse KPCA (sKPCA) was proposed by Tipping
(), where the sparsity in the representation is obtained by assuming a generative
model for the data in that follows a normal distribution and includes a noise term
with variance vn. The ML estimation of the covariance matrix is shown to depend on
just a subset of the training data, and so it does the resulting solution. A sparse KPLS

572
Digital Signal Processing with Kernel Methods
(sKPLS) was introduced by Momma and Bennett (). The method computes the
projections with a reduced set of the training samples. Each one of the projections is
found using a loss that is similar to the 𝜀insensitive loss used in SVR. The sparsiﬁcation
is induced via a multistep adaptation with high computational burden.
The algorithms in Tipping () and Momma and Bennett (), however, still
require the computation of the full kernel matrix during the training.
A reduced complexity KOPLS (rKOPLS) was proposed by Arenas-García et al. ()
by imposing sparsity in the projection vectors representation a priori, U = 𝜱T
r 𝜷, where
𝜱r is a subset of the training data containing r samples (r ≪N) and 𝜷is the new
argument for the maximization problem, which now becomes
max 𝜷TK rNYY TK T
rN𝜷
subject to ∶𝜷TK rNK T
rN𝜷= ,
(.)
Since kernel matrix K rN = 𝜱r𝜱T involves the inner products in of all training
points with the patterns in the reduced set, this method still takes into account all
data during the training phase, and is therefore diﬀerent from simple subsampling. This
“sparsiﬁcation” procedure avoids the computation of the full kernel matrix at any step
of the algorithm. An additional advantage of this method is that matrices K rNYY TK T
rN
and K rNK T
rN are both of size r × r, and they can be computed as sums over the training
data. This fact makes the storage space grow quadratically with r. Also, there is an
implicit regularization imposed by the sparsiﬁcation, which decreases the overﬁtting
risk of the method. Reduced complexity versions of KPCA and KKCA are shown in the
experimental section that use the same sparsiﬁcation method, and will be referred as
rKPCA and rKCCA.
Interestingly, the extension to KPLSis not straightforward, since the deﬂation step
would still require the full kernel matrix K NN. Alternatively, two sparse KPLS schemes
were presented by Dhanjal et al. () under the name of sparse maximal alignment
(SMA) and sparse maximal covariance (SMC). Here, KPLS iteratively estimates projec-
tions that either maximize the kernel alignment or the covariance of the projected data
and the true labels.
Table .summarizes some computational and implementation issues of the afore-
mentioned sparse KMVA methods, and of standard nonsparse KMVA and linear
methods. Some aspects that can be helpful to choose an algorithm for a particular
application can be obtained from an analysis of the properties of each algorithm. An
important step is to choose an adequate kernel and parameters. KMVA methods can
be adjusted using cross-validation to reduce overﬁtting at the expense of an increased
computational burden, but regularization through sparsiﬁcation can help to further
reduce overﬁtting. Also, most methods can be implemented as either eigenvalue or
generalized eigenvalue problems, whose complexity typically scales cubically with the
size of the matrices analyzed. Therefore, both for memory and computational reasons,
only linear MVA and the sparse approaches from Arenas-García et al. () and
Dhanjal et al. () are aﬀordable when dealing with large datasets. A ﬁnal advantage
of sparse KMVA is the reduced number of kernel evaluations to extract features for new
out-of-the-sample data.

Kernel Feature Extraction in Signal Processing
573
Table 12.3 Main properties of (K)MVA methods. Computational complexity and implementation issues
are categorized for the considered dense and sparse methods in terms of the free parameters, number
of kernel evaluations (KEs) during training, and storage requirements. Notation: N refers to the size of
the labeled dataset, while d and m are the dimensions of the input and output spaces respectively.
Method
Parameters
KE (training)
Storage requirements
PCA
None
None
(d)
PLS
None
None
((d + m))
CCA
None
None
((d + m))
OPLS
None
None
(d)
KPCA
Kernel
N
(N)
KPLS
Kernel
N
((N + m))
KCCA
Kernel
N
((N + m))
KOPLS
Kernel
N
(N)
sKPCA (Tipping, )
Kernel, vn
N
(N)
sKPLS (Momma and Bennett, )
Kernel, 𝜈, 𝜀
N
(N)
rKPCA
Kernel, r
rN
(r)
rKCCA
Kernel, r
rN
((r + m))
rKOPLS (Arenas-García et al., )
kernel, r
rN
(r)
SMA/SMC (Dhanjal et al., )
kernel, r
N
(N)
12.4.4
Semi-supervised Kernel Feature Extraction
When few labeled samples are available, the extracted features do not capture the
structure of the data manifold well, which may lead to very poor results. SSL approaches
have been introduced to alleviate these problems. Two approaches are encountered: the
information conveyed by the unlabeled samples is either modeled with graphs or via
kernel functions derived from generative clustering models.
Notationally, we are given l labeled and u unlabeled samples, a total of N = l + u.
The semi-supervised KCCA (ss-KCCA) was introduced in Blaschko et al. () by
using the graph Laplacian. The method essentially solves the standard KCCA equations
using kernel matrices computed with both labeled and unlabeled data, which are further
regularized with the graph Laplacian:
(
𝟎
K x
NlK y
lN
K x
NlK y
lN
𝟎
) (
𝜶
v
)
= 𝜆
(K x
NlK x
lN + Rx
NN
𝟎
𝟎
K y
NlK y
lN + Ry
NN
) (
𝜶
v
)
,
where Rx
NN = 𝛼xK x
NN + 𝛾xK x
NNx
NNK x
NN and Ry
NN = 𝛼yK y
NN + 𝛾yK y
NNy
NNK y
NN. For
the sake of notation compactness, subindices indicate the size of the corresponding
matrices while superscripts denote whether they involve input or output data. Param-
eters 𝛼x, 𝛼y, 𝛾x, and 𝛾y trade oﬀthe contribution of labeled and unlabeled samples, and

574
Digital Signal Processing with Kernel Methods
L = D−∕(D −M)D∕represents the (normalized) graph Laplacian for the input and
target domains, where D is the degree matrix whose entries are the sums of the rows
of the corresponding similarity matrix M; that is, Dii = ∑
j Mij. It should be noted that
for N = l and null regularization one obtains the standard KCCA (see Equation .).
Note also that this form of regularization through the graph Laplacian can be applied
to any KMVA method. A drawback of this approach is that it involves tuning several
parameters and working with larger matrices of size N × N, which can make its
application diﬃcult in practice.
Alternatively, cluster kernels – a form of generative kernel functions learned from
data – have been used to develop semi-supervised versions of kernel methods in general,
and of KMVA methods in particular. The approach was used for KPLS and KOPLS by
Izquierdo-Verdiguier et al. (). Essentially, the method relies on combining a kernel
function based on labeled information only, Ks(xi, xj), and a generative kernel directly
learned by clustering all (labeled and unlabeled) data, Kc(xi, xj). Building Kc requires
ﬁrst running a clustering algorithm, such as EM assuming a GMM with diﬀerent
initializations, q = , … , Q, and with diﬀerent number of clusters, g = , … , G +. This
results in Q⋅G cluster assignments where each sample xi has its corresponding posterior
probability vector 𝝅i(q, g) ∈ℝg. The probabilistic cluster kernel Kc is computed by
averaging all the dot products between posterior probability vectors:
Kc(xi, xj) = 
Z
Q
∑
q=
G+
∑
g=
𝝅i(q, g)T𝝅j(q, g),
where Z is a normalization factor. The ﬁnal kernel function is deﬁned as the weighted
sum of both kernels, K(xi, xj) = 𝛽Ks(xi, xj) + (−𝛽)Kc(xi, xj), where 𝛽∈[, ] is a
scalar parameter to be adjusted. Intuitively, the cluster kernel accounts for probabilistic
similarities at small and large scales (number of clusters) between all samples along the
data manifold, and hence its name: the probabilistic cluster kernel (PCK) (Izquierdo-
Verdiguier et al., ). The method does not require computationally demanding
procedures (e.g., current GMM clustering algorithms scale linearly with N), and the
KMVA still relies just on the labeled data, and thus requires an l × N kernel matrix. All
these properties are quite appealing from the practitioner’s point of view. Listing .
gives a MATLAB code snippet showing how to compute the PCK. Note that this
particular this particular kernel function is semi-supervised and can be plugged into
any kernel method.
%
Xtrain:
Train samples [samples x features]
%
Xtest:
Test samples
[samples x features]
%
C_tot:
Gaussian mixture parameter estimates
%
IDX_tot: Clusters from Gaussian mixture distribution
KbagUL = zeros(size(Xtrain,1),size(Xtest,1));
nk=0;
for k=Kvector % Number of clustering realizations
nk=nk+1;
for n=1:N
% Number of realizations
if ~isempty(C_tot{nk,n})
Pgmm = posterior(C_tot{nk,n},Xtrain);
[ID_UL PP]= closerClusterGMM(Xtest,C_tot{nk,n},'resoft');
idx_tot=IDX_tot{nk,n};

Kernel Feature Extraction in Signal Processing
575
idx_tot=idx_tot(1:size(Xtrain,1),:);
for p=1:length(ID_UL)
for q=1:length(idx_tot)
KbagUL(q,p) = KbagUL(q,p) + PP(p,:)*Pgmm(q,:)';
end
end
end
end
end
KbagUL = KbagUL/max(max(KbagUL));
Listing 12.7 MATLAB code example for PCK.
Figure .shows an illustrative toy example evaluating the RBF, PCK, Fisher’s
(Jaakkola et al., ), and Jensen–Shannon’s (Bicego et al., ) kernels. The data were
generated by the composition of two normal distributions, (, .) and (, .). We
look at the structure of the kernel matrix through the ﬁrst eigenvectors, and compute
the Frobenius norm of the residuals with the ideal kernel. The length-scale of the RBF
was ﬁxed to the average distance of all examples, while we used Q = and G = 
for Kc. The Fisher kernel used feature vectors extracted from a GMM using the same
number of clusters as for the PCK, and the Jensen–Shannon kernel was built from diver-
gence Jensen–Shannon obtained from Shannon entropy. Since the Fisher and Jensen–
Shannon kernels are not intrinsically multiscale, here we implement a multiscale version
of the Fisher and Jensen–Shannon kernels for the sake of a fair comparison. The ﬁgure
shows that the PCK and the Jensen–Shannon return more discriminative eigenvectors
and substantially diﬀerent from the Fourier-like basis obtained by the RBF and the
Fisher kernel. The PCK better captures the local structure than the Jensen–Shannon
kernel does. This clearly draws from the visual comparison of the kernel matrices,
which is also supported by the Frobenius norm of the diﬀerences to the ideal kernel
(Figure ., given in parentheses).
12.5
Domain Adaptation with Kernels
Domain adaptation constitutes a ﬁeld of high interest nowadays in pattern analysis and
machine learning. Classiﬁcation or regression algorithms developed with data from one
domain cannot be directly used in another related domain, and hence adaptation of
either the data representation or the classiﬁer becomes strictly imperative (Quiñonero-
Candela et al., ; Sugiyama and Kawanabe, ). There is actually strong evidence
that a signiﬁcant degradation in the performance of state-of-the-art image classiﬁers is
due to test domain shifts such as changing image sensors and noise conditions (Saenko
et al., ), pose changes (Farhadi and Tabrizi, ), consumer versus commer-
cial video (Duan et al., ), and, more generally, datasets biased due to changing
acquisition procedures (Torralba and Efros, ). Unlike in Hoﬀman et al. (),
we focus on adapting data representations to facilitate the use of standard classiﬁers.
Roughly speaking, domain adaptation solves a learning problem in a target domain
by utilizing the training data in a diﬀerent but related source domain. Intuitively,
discovering a good feature representation across domains is crucial. The problem
has captured the attention of researchers in many ﬁelds, also in the kernel methods
community.

RBF kernel (438.64)
Jensen–Shannon kernel (388.72)
0
200
400
600
First eigenvector
Second eigenvector
Third eigenvector
800
1000
0
2
4
6
8
0
2
4
6
8
0
2
4
6
8
Fisher kernel (704.36)
0
2
4
6
8
0
2
4
6
8
Original data
PCK (333.81)
Figure 12.8 Example of differences between the local properties of the RBF, Jensen–Shannon kernel, Fisher’s kernel, and the PCK. We indicate in parentheses
the Frobenius norm of the residuals with the ideal kernel, ‖K −yyT‖2
F.

Kernel Feature Extraction in Signal Processing
577
The signal- and image-processing communities have also greatly contributed to this
community, yet under diﬀerent names. Domain adaptation is sometimes referred to as
simply adaptation, transportability, inter-calibration, or feature invariance learning. A
collection of images must be compensated for illumination changes when a classiﬁer
must be used across domains, or speech signals from diﬀerent users must be equalized
for annotation and recognition steps. In general, data collected for the same prob-
lem in diﬀerent instants, from diﬀerent angles, by diﬀerent systems and procedures
show similar features but also reveal changes in their data manifold that need to be
compensated before the direct exploitation. The problem is very challenging in high-
dimensional and low-sized datasets, which is very often the case in signal-processing
problems.
Recently, several approaches have been proposed to learn a common feature represen-
tation for domain adaptation. Daumé III and Marcu () presented a simple heuristic
nonlinear mapping function to map the data from both source and target domains
to a high-dimensional feature space, where standard machine-learning methods are
used to train classiﬁers. On the other hand, Blitzer et al. () proposed the so-
called structural correspondence learning algorithm to induce correspondences among
features from the diﬀerent domains. This method depends on the heuristic selections
of pivot features that appear frequently in both domains. Although it is experimentally
shown that structural correspondence learning can reduce the diﬀerence between
domains based on the distance measure (Ben-David et al., ), the heuristic criterion
of pivot feature selection may be sensitive to diﬀerent applications. Pan et al. ()
proposed a new dimensionality reduction method, the so-called maximum mean
discrepancy embedding (MMDE), for domain adaptation. The method aims at learning
a shared latent space underlying the domains where distance between distributions
can be reduced. However, MMDE suﬀers from two major limitations: () MMDE is
transductive, and does not generalize to out-of-sample patterns; () MMDE learns
the latent space by solving a semi-deﬁnite program (SDP), which is a very expensive
optimization problem. Alternatives to these problems have been recently introduced in
the literature.
Besides changing the data representation space via kernel feature extraction, another
possibility is to correct for biases in the data distributions operating on the sam-
ples directly. The problem of covariate shift is intimately related to the problem of
adaptation (Sugiyama and Kawanabe, ). Correcting for biases in the (chang-
ing) distributions can be addressed by upweighting and downweighting the relevance
of the samples in the incoming target distribution. This technique has been widely
exploited in many studies (Bickel et al. ; Huang et al. ; Kanamori et al. ):
Huang et al. () proposed the kernel mean matching (KMM) to reweight instances
in an RKHS, Bickel et al. () proposed to integrate the distribution correcting
process into a kernelized logistic regression, and Kanamori et al. () introduced
a method called unconstrained LS importance ﬁtting (uLSIF) to estimate sample
relevance.
In this section, we review three examples of successful kernel methods for domain
adaptation, each following diﬀerent (yet obviously interconnected) philosophies: ()
sample reweighting strategies (KMM), () transfer component analysis (TCA), and ()
kernel manifold alignment (KEMA) for domain adaptation. Some illustrative examples
of performance in signal processing tasks are given for illustration purposes.

578
Digital Signal Processing with Kernel Methods
12.5.1
Kernel Mean Matching
KMM is a kernel-based method for sample reweighting. Note that, in general, learning
methods typically try to minimize the expected risk, [ℙ, 𝜃, 𝓁], where ℙis the pdf
where samples are drawn from, and 𝓁(x, y, 𝜃) is the empirical loss function that depends
that depends on a parameter 𝜃. Since one typically has access only to pairs of examples
(x, y) drawn from a density ℙ(x, y), the problem is solved by computing the empirical
average over all available data, emp = ∑N
i=𝓁(xi, yi, 𝜃), which is normally regularized to
avoid overﬁtting to the training set. However, the problem turns out to be more complex
when the training and test probability functions diﬀer, even slightly; that is, ℙtrain(x, y) ≠
ℙtest(x, y). Actually, what one would desire is to minimize the ℙtest(x, y), but one has
only access to data drawn from ℙtrain(x, y). The ﬁeld of statistics known as importance
sampling is concerned precisely with this problem: estimating properties of a particular
distribution while only having samples generated from a estimating properties of a
particular distribution while only having samples generated from a diﬀerent distribution
rather than the distribution of interest. The methods in this ﬁeld are frequently used to
estimate posterior densities or expectations in probabilistic models that are hard to treat
analytically, as in
[ℙtest, 𝜃, 𝓁(x, y, 𝜃)] = 𝔼(x,y)∼ℙtest[𝓁(x, y, 𝜃)] = 𝔼(x,y)∼ℙtrain
[ ℙtest(x, y)
ℙtrain(x, y)𝓁(x, y 𝜃)
]
= [ℙtrain, 𝜃, 𝛽(x, y)𝓁(x, y, 𝜃)],
provided that the support of ℙtest is contained in the support of ℙtrain. Therefore, we can
compute the test risk from a modiﬁed version of the train risk, and this modiﬁcation
essentially translates into estimating the relevance parameter 𝛽per training example.
This approach, however, involves density estimation of ℙtrain(x, y) and ℙtest(x, y), which
in general is not an easy problem (recall Vapnik’s dictatum in Chapter ). In addition,
a deﬁcient estimation of such densities may give rise to inaccurate estimates for the
weights 𝛽, and consequently it may happen that the algorithm with adaptation mecha-
nism works substantially worse than one without it. An alternative to both problems is
given by the empirical KMM optimization.
The KMM essentially tries to ﬁnd weights 𝛽
∈
ℝd in order to minimize the
discrepancy between means subject to constraints 𝛽i ∈[, B] and |(∕d) ∑d
i=𝛽i−| ≤𝜀,
which limits the discrepancy between ℙtrain and ℙtest, as well as ensure that 𝛽(x)ℙtrain(x)
approaches a probability distribution. The objective function is given by the discrepancy
term between the two empirical means. Using kernels [K]ij = K(xi, xj), and 𝜅i =
(d∕d′) ∑d′
i=K(xi, x′
j), one can check that
‖‖‖‖‖‖

d
d
∑
i=
𝛽i𝝓(xi) −
d′
d′
∑
i=
𝛽i𝝓(x′
i)
‖‖‖‖‖‖

= 
d𝜷TK𝜷−
d𝜅T𝜷+ constant terms,

Kernel Feature Extraction in Signal Processing
579
which reduces to solving a QP problem to optimize the weight vector 𝛽:
min
𝜷
{ 
𝜷TK𝜷−𝜅T𝜷
}
s.t.
𝛽i ∈[, B]
and
|
d
∑
i=
𝛽i −d| ≤m𝜀.
Note that KMM resembles the OC-SVM using the 𝜈-trick, yet modiﬁed by the linear
correction term involving 𝜅, which measures sample relevance as well.
12.5.2
Transfer Component Analysis
TCA (Pan and Yang, ) tries to learn some transfer components across domains in
an RKHS using the MMD measure (Borgwardt et al., ) introduced in Chapter 
and related to the HSIC introduced before. In the subspace spanned by these transfer
components, data distributions in diﬀerent domains are close to each other. As a result,
with the new representations in this subspace, one can apply standard machine-learning
methods to train classiﬁers or regression models in the source domain for use in the
target domain. TCA does so in a very eﬀective way. TCA essentially minimizes the
distance between probability distributions of a source and a target domain. There are
many distances in the literature to evaluate the diﬀerence between probability distri-
butions: Kullback–Leibler divergence, Jensen–Shannon divergence, Bhattacharyya dis-
tance, and so on. However, these methods are aﬀected by the data dimensionality, with
the necessary probability density estimations becoming infeasible in high-dimensional
spaces.
TCA considers a setting where the target domain has plenty of unlabeled data. TCA
also assumes that some labeled data are available in a source domain s, while only
unlabeled data are available in the target domain t. Data will be denoted as belonging
to either source or target domains with the corresponding superscript; that is, xs
i for
the source and xt
i for the target. The MMD estimate presented in Borgwardt et al.
() proposes a new indicator for comparing distributions based on the diﬀerence
of the mean of the distributions computed in a common RKHS. This nonparametric
measure is easily calculated no matter the number of variables describing the examples.
Essentially, the empirical estimate of the MMD between the distribution of a given
source dataset Xs and that of a related target dataset Xt is given by
MMD(Xs, Xt) =
‖‖‖‖‖‖

Ns
Ns
∑
i=
𝝓(xs
i) −
Nt
Nt
∑
i=
𝝓(xt
i)
‖‖‖‖‖‖


.
The problem thus reduces to ﬁnding the nonlinear transformation 𝝓(⋅) explicitly, which
in principle is not possible. Instead, by using the kernel trick, one deﬁnes a reproducing
kernel as K(xi, xj) = 𝝓(xi)T𝝓(xj). It can be demonstrated that the problem of estimating
the distance between the empirical means of the two domains reduces to
MMD(Xs, Xt) = Tr(KL),

580
Digital Signal Processing with Kernel Methods
where
K =
(
K ss
K st
K ts
K tt
)
,
is a kernel matrix with Ns +Nt entries formed by three kernel matrices built on the data
in the source domain K ss, target domain K tt, and cross domains, K st; and the matrix
L is a positive semi-deﬁnite matrix with entries Lij = ∕N
s if xi and xj belong to the
source domain, Lij = ∕N
t if xi and xj belong to the target domain, and Lij = ∕(NsNt)
otherwise. Listing .gives an example for computing the MMD.
% Xs = data in source domain, Xt = data in the target domain
sigmax = estimateSigma(X,X);
Kss = kernel('rbf',Xs,Xs,sigmax);
Ktt = kernel('rbf',Xt,Xt,sigmax);
Kst = kernel('rbf',Xs,Xt,sigmax);
Kts = kernel('rbf',Xt,Xs,sigmax);
K = [Kss Kst;
Kts Ktt];
L = [1/(length(Xs).^2)*ones(length(Xs)) ...
1/(length(Xs)*length(Xt))*ones(length(Xs),length(Xt));
1/(length(Xs)*length(Xt))*ones(length(Xt),length(Xs)) ...
1/(length(Xt).^2)*ones(length(Xt))]
MMD = trace(K*L);
Listing 12.8 MATLAB code example for MMD.
In the transductive setting, learning the kernel can be solved by learning the kernel
matrix K instead. In Pan et al. (), the resultant kernel matrix learning problem is
formulated as an SDP, and PCA is then applied on the learned kernel matrix to ﬁnd a
low-dimensional latent space across domains. This is referred to as MMDE. Three main
problems are devised with this approach. First, it is transductive and cannot generalize
on unseen patterns. Second, the criterion to be optimized requires the kernel to be
positive semi-deﬁnite and the resultant kernel learning problem has to be solved by
expensive SDP solvers.
TCA instead relies on kernel feature extraction to solve the problem and simply
introduces the considerations () we want to optimize a projection matrix such that
the MMD is minimized after transformation, and () the extractor should respect the
main properties of the source and target data. For the ﬁrst condition, and starting from
the kernel matrix K given before, it is possible to use a projection matrix W to compute
the kernel matrix between mapped samples as K = KWW TK. Afterwards, to obtain
the MMD measure for the mapped samples, we simply have
MMD(Xs, Xt) = Tr(KWW TKL) = Tr(W TKLKW),
which can be minimized with respect to W. For the second condition, the mapping
𝝓should not harm the target supervised learning task by deforming too much the
input space, so a regularization term is introduced able to preserve (and maximize) the

Kernel Feature Extraction in Signal Processing
581
initial data variance in the newly created subspace, ‖W‖. Combining both terms, TCA
reduces to solve
min
W {Tr(W TKLKW) + 𝜆Tr(W TW)}
s.t.
𝚺= I,
where 𝜮= W TKHKW is the covariance matrix of the projected data, H is a centering
matrix in Hilbert spaces, H = I −∕(NsNt)𝟏𝟏T, and 𝜆is a trade-oﬀparameter for
tuning the inﬂuence of the regularization term and thus controlling the complexity of
W. Such an optimization problem can be reformulated as a trace maximization problem
yielding the following solution: the mapping matrix W is obtained by performing the
eigendecomposition of
M = (KLK + 𝜆I)−KHK,
and keeping the r eigenvectors associated with the r largest eigenvalues. Once W
is available, one can readily compute the r coordinates (the r uncorrelated transfer
components) of the mapped samples as (X) = KW. In this latent subspace where
distribution diﬀerences are reduced it is now possible to train a supervised classiﬁer on
the mapped source labeled samples and subsequently use it to classify the target image
embedded in the same subspace. The reader may ﬁnd MATLAB code for TCA in the
book’s repository.
12.5.3
Kernel Manifold Alignment
The problem of aligning data manifolds reduces to ﬁnding projections between dif-
ferent sets of data, assuming that all lie on a common manifold. Roughly speaking,
manifold alignment is a new form of MVA that dates back to the work of Hotelling
in on CCA (Hotelling, ), where projections try to correlate the data sources
onto a common target domain. The renewed concept of manifold alignment was ﬁrst
introduced by Ham et al. (), where a manifold constraint was added to the general
problem of correlating sets of high-dimensional vectors. The main problem of CCA, its
kernel counterpart KCCA (Lai and Fyfe, b), and many other domain adaptation
methods (Blitzer et al. ; Daumé III and Marcu ; Duan et al. , ; Ham
et al. ) when confronted with manifold alignment problems is that, unfortunately,
points in diﬀerent sources must be corresponding pairs, which is often hard to meet
in real applications. Actually, methods typically assume that the source and target
domains are represented by the same features and instances, and try to match the only
changing entity, namely the data distributions. Methods in the related ﬁeld of transfer
learning also commonly assume the availability of enough labeled data shared across
domains (Dai et al., ; Pan and Yang, ); but again, this is a strong assumption in
most of real-life problems.
The problem of manifold alignment without correspondences was tackled by Ham
et al. () and Wang and Mahadevan () by means of mapping diﬀerent domains
to a new latent space, concurrently matching the corresponding instances and pre-
serving the topology of each input domain. The exploitation of unlabeled samples in
a semi-supervised setting improves the performance. Nevertheless, while appealing,

582
Digital Signal Processing with Kernel Methods
these methods still require specifying a small amount of cross-domain correspondence
relationships. This problem was addressed by Wang and Mahadevan () essentially
relaxing the constraint of paired correspondence between feature vectors with the
constraint of having the same class labels in all domains. Hence, the algorithm, here-
after called semi-supervised manifold alignment (SSMA), tries to project data from
D domains to a latent space where samples belonging to the same class become
closer, those of diﬀerent classes are pushed far apart, and the geometry of each domain
is preserved. The linear projection method performs well in general but cannot cope
with strong nonlinear deformations of the manifolds and high-dimensional problems.
Notationally, we are given D domains of data representation, and the corresponding
data matrices deﬁned therein, Xi ∈ℝNi×di, i = , … , D, containing Ni examples (labeled
and unlabeled) of dimension di, and N = ∑
i Ni. The method maps all the data to
a latent space such that samples belonging to the same class become closer, those
of diﬀerent classes are pushed far apart, and the geometry of the data manifolds is
preserved. Therefore, three entities have to be considered per domain: () a similarity
matrix W s will have components W mn
s
= if xm and xn belong to the same class, and
zero otherwise; () a dissimilarity matrix W d will have entries W mn
d
= if xm and xn
belong to the same class, and one otherwise; and () a similarity matrix that represents
the topology of each given domain, W (e.g., an RBF kernel.) The three diﬀerent entities
lead to three diﬀerent graph Laplacians: Ls, Ld, and L respectively. Then, the embedding
must minimize a joint cost function essentially given by the eigenvectors corresponding
to the smallest nonzero eigenvalues of the generalized eigenvalue problem:
ZT(L + 𝜇Ls)ZV = 𝜆ZTLdZV,
where Z is a block diagonal matrix containing the data matrices Xi and V contains in the
columns the eigenvectors organized in rows for the particular domain, V = [v| ⋯|vD]T.
The method allows one to extract a maximum of nf = ∑D
i=di features, which serve
for projecting the data to the common latent domain (hereafter called f ) as Pf (Xi) =
Xivi. An interesting possibility of the SSMA method is that one can easily project data
between domains j and i by simply mapping the source data in j to the latent domain
and inverting back to the target domain in i as Pi(Xj) = Xjvjv†
i , where † represents the
pseudo-inverse of the eigenvectors of the target domain. Listing .gives an example
for computing the SSMA.
% X = labeled data, U = unlabeled data
Z = blkdiag([X1,U1],[X2,U2]);
% (n1+n2) x (d1+d2)
[L,Ls,Ld] = Laplacians(X1,U1,Y1,X2,U2,Y2);
n = n1+n2; d = d1+d2;
% Combine graph Laplacians
mu = 0.1;
A
= L + mu*Ls; % (n1+n2) x (n1+n2)
B
= Ld;
% (n1+n2) x (n1+n2)
% Solve the generalized eigenproblem
[V D] = eigs(Z'*A*Z,Z'*B*Z,d,'SM');
nf = 4; % features
E1 = V(1:d1,1:nf); E2 = V(d1+1:end,1:nf);
% Project data
PX12 = X1*E1;
Listing 12.9 MATLAB code example for SSMA.

Kernel Feature Extraction in Signal Processing
583
The kernelization of the previous method was presented by Tuia and Camps-Valls
() as KEMA. The KEMA method consists of the following steps: ﬁrst, map the data
to a Hilbert space, then apply the representer theorem and replace the dot products
therein with reproducing kernel functions. Let us ﬁrst map the D diﬀerent datasets
with D mapping functions to D in principle diﬀerent Hilbert spaces i of dimension
Hi, 𝝓i(⋅) ∶x →𝝓i(x) ∈i, i = , … , D. Now, by replacing all the samples with their
mapped feature vectors, the problem becomes
𝜱T(L + 𝜇Ls)𝜱W = 𝜆𝜱TLd𝜱W,
where
𝜱
is
a
block
diagonal
matrix
containing
the
data
matrices
𝜱i = [𝝓i(x)| ⋯|𝝓i(xNi)]T and W contains in in the columns the eigenvectors organized
in rows for the particular domain deﬁned in Hilbert space i, W = [w| ⋯|wH]T, where
H = ∑D
i Hi. This operation is possible thanks to the use of the direct sum of Hilbert
spaces, a well-known theory (Reed and Simon, ). Note that the eigenvectors wi are
of possibly inﬁnite dimension and cannot be explicitly computed. Instead, we resort to
the deﬁnition of D corresponding Riesz representation theorems (see Chapter ) so the
eigenvectors can be expressed as a linear combination of mapped samples, wi = 𝜱T
i 𝜶i,
and in matrix notation W = 𝜱T𝜦. After replacing in the linear counterpart equation, by
premultiplying both sides by 𝜱, and replacing the dot products with the corresponding
kernels, K i = 𝜱i𝜱T
i we obtain the ﬁnal solution:
K(L + 𝜇Ls)K𝜦= 𝜆KLdK𝜦,
where K is a block diagonal matrix containing the kernel matrices K i. Now the
eigenproblem becomes of size N × N instead of d × d, and the number of extracted
features becomes nf = ∑D
i=Ni. Listing .gives an example for computing the
KEMA, while the full MATLAB KEMA toolbox is available on the book web page.
% X = labeled data, U = unlabeled data
K1 = kernelmatrix(X1,U1);
% n1 x n1
K2 = kernelmatrix(X2,U2);
% n2 x n
K
= blkdiag(K1,K2);
% (n1+n2) x (n1+n2)
[L,Ls,Ld] = Laplacians(X1,U1,Y1,X2,U2,Y2);
n
= n1+n2; d = d1+d2;
% Combine graph Laplacians
mu = 0.1;
A = L + mu*Ls;
%(n1+n2) x (n1+n2)
B = Ld; % (n1+n2) x (n1+n2)
% Solve the generalized eigenproblem
[A D] = eigs(K'*A*K,K'*B*K,d,'SM');
nf = 4; % n_f features
A1 = A(1:n1,1:nf); A2 = A(n1+1:end,1:nf);
% Project data
PX12 = K1*A1;
Listing 12.10 MATLAB code example for KEMA.
Let us illustrate the performance of KEMA in a toy example involving the alignment
of data matrices Xand X, which are spirals with three classes. Then, a series of
deformations are applied to the second domain: scaling, rotation, inversion of the order
of the classes, the shape of the domain (spiral or line) or the data dimensionality. For

584
Digital Signal Processing with Kernel Methods
Input spaces
Domains    Classes
KEMA (linear kernel)
Domains    Classes
KEMA (RBF kernel)
Domains     Classes
Error rates [%]
Dom. #1  Dom. #2
Exp.
Exp.#1
0
5
10
5
10
20
Nf
Error [%]
0
5
10
5
10
20
Nf
Error [%]
0
5
10
5
10
20
Nf
Error [%]
0
5
10
5
10
20
Nf
Error [%]
0
5
10
5
10
20
Nf
Error [%]
0
5
10
5
10
20
Nf
Error [%]
0
5
10
5
10
20
Nf
Error [%]
0
5
10
5
10
20
Nf
Error [%]
Exp.#2
Exp.#3
Exp.#4
d• = 3
d• = 3
d• = 3
d• = 3
d• = 3
d• = 2
d• = 2
d• = 2
Figure 12.9 Illustration of linear and KEMA on the toy experiments. Left to right: data in the original
domains (X1: ∙; X2: ∙) and per class (∙, ∙and ∙), data projected with the linear and the RBF kernels, and
error rates as a function of the extracted features when predicting data for the first (left inset) or the
second (right inset) domain (KEMALin, KEMARBF, SSMA, Baseline).
each experiment, labeled pixels per class were sampled in each domain, as well as
unlabeled samples that were randomly selected. Classiﬁcation performance was
assessed on held-out samples from each domain.
The projections found by KEMA together with a linear and an RBF kernel and the
classiﬁcation errors for the source and target domains can be seen in Figure .. The
linear version of KEMA (SSMA) produced good results in experiments #and #,
because they are linear transformations of the data. Nevertheless, its performance in
experiments #and #is poor, where the manifolds have undergone stronger defor-
mations. A nonlinear, more ﬂexible version produced better performance, producing
an unfolding plus alignment in all cases. The linear classiﬁer trained on the projections
of KEMAlin and SSMA does not do a good job in the classiﬁcation of both domains,
in spite of a correct alignment, while the KEMARBF solution provides a latent space
where both domains can be classiﬁed correctly. In experiment #, the solution is quite
diﬀerent: the baseline error depicted by the green line is signiﬁcantly lower in the source
domain. This is because the dataset in three dimensions is linearly separable. Even if
the classiﬁcation of this ﬁrst domain (∙) is correct for all methods, classiﬁcation after
SSMA/KEMAlin projection of the second domain (∙) is poor, since their projection in the
latent space does not unfold the blue spiral. KEMARBF provides the best result. Similar
results as in experiment #can be seen in experiment #. In experiment #one can see
a very accurate baseline (both domains are linearly separable in the input spaces) and all
methods provide accurate classiﬁcation accuracies. Again, KEMARBF provides the best
match between the domains in the latent space.

Kernel Feature Extraction in Signal Processing
585
Table 12.4 Properties of manifold alignment and domain adaptation methods.
Method
Supervised Multisource Unpaired Nonlinear Invertible Cost
PCA (Jolliﬀe, )
×
×
√
×
√
(d)
KPCA (Schölkopf et al., )
×
×
√
×
×
(N)
CCA (Hotelling, )
×
√
×
√
√
(d)
KCCA (Lai and Fyfe, b)
×
√
×
√
×
(d)
KTA (Pan and Yang, )
√
×
×
√
√
(N)
TCA (Pan and Yang, )
×
×
√
×
×
(N)
GM (Tuia et al., )
×
×
√
×
√
(dN)
SSMA (Wang and Mahadevan,
)
√
√
√
×
√
(d)
KEMA (Tuia and Camps-Valls,
)
√
√
√
√
√
(N)
12.5.4
Relations between Domain Adaptation Methods
The properties of the methods analyzed are summarized and compared in Table ..
We include in the table some of the methods presented in Section .for comparison.
The KMM method is not looking for a transformation in strict terms and depends on the
optimization of a QP problem; thus, we do not include it in the table since comparison
could be misleading. We have included KTA in the table, although it is usually used to ﬁt
parameters instead of to ﬁnd a transformation. SSMA and KEMA, like CCA and KCCA,
can in principle align multisource data, but importantly they do not need unpaired data
points. This may be useful to align datasets of diﬀerent features and number of samples,
provided that a reasonable amount of labeled information is available. Contrarily to
PCA, KPCA, TCA, and graph matching, KEMA does not necessarily require a leading
source domain to which all the others are aligned to. TCA and KEMA, however, are
more computationally demanding than the linear counterpart as the eigenproblem
becomes N ×N instead of d×d. Nevertheless, two clariﬁcations must be done here: ﬁrst,
both SSMA and KEMA involve the same construction of the graph Laplacian, which
takes most of the time but can be computed just once and oﬄine; and second, KEMA
with linear kernels can eﬃciently solve the SSMA problem for high-dimensional data.
12.5.5
Experimental Comparison between Domain Adaptation Methods
We here evaluate numerically the standard manifold aligment algorithms. In partic-
ular, we compare KEMA (Tuia and Camps-Valls, ), SGF (Gopalan et al., ),
GFK (Gong et al., ), OT-lab (Courty et al., ), and MMDT (Hoﬀman et al., ).
The task is to ﬁnd discriminative aligning projections for visual object recognition tasks.
We use the dataset introduced in Saenko et al. () in which we consider four domains
Webcam (W), Caltech (C), Amazon (A), and DSLR (D), and selected the common
classes in the four datasets following Gong et al. (). By doing so, the domains
contain (Webcam), (Caltech), (Amazon) and (DSLR) images. The

Table 12.5 Accuracy in the visual object recognition study (C: Caltech, A: Amazon, D: DSLR, W: Webcam). 1-NN classification
testing on all samples from the target domain.
Train on
Domain adaptation method
Train on
source
Unsupervised
Labeled from s only
Labeled from s + t
target
No adapt.
SGF∗
GFK∗
GFK∗
OT-lab∗
GFK†
MMDT†
KEMA Ki
KEMA Kχ2
No. adapt.
NS










NT










C →A
.
.
.
.
.
.
.
.
.
.
C →D
.
.
.
.
.
.
.
.
.
.
A →C
.
.
.
.
.
.
.
.
.
.
A →W
.
.
.
.
.
.
.
.
.
.
W →C
.
.
.
.
.
.
.
.
.
.
W →A
.
.
.
.
.
.
.
.
.
.
D →A
.
.
.
.
.
.
.
.
.
.
D →W
.
.
.
.
.
.
.
.
.
.
Mean
.
.
.
.
.
.
.
.
.
.
Ndomain: number of labels per class; ∗: results from Courty et al. (); †: results from Hoﬀman et al. ().

Kernel Feature Extraction in Signal Processing
587
features were extracted as described in Saenko et al. (): we use an -dimensional
normalized histogram of visual words obtained from a codebook constructed from a
subset of the Amazon dataset on points of interest detected by the “speeded up robust
features” method. We used the same experimental setting as Gopalan et al. () and
Gong et al. (), in order to compare with these unsupervised domain adaptation
methods.
For all methods, we used labeled pixels per class in the source domain for the
C, A, and D domains and eight samples per class for the W domain. After alignment, an
ordinary -NN classiﬁer was trained with the labeled samples. The same labeled samples
in the source domain were used to deﬁne the PLS eigenvectors for GFK and OT-lab.
For all the methods using labeled samples in the target domain (including KEMA), we
used three labeled samples in the target domain to deﬁne the projections. In all cases,
we used sensible kernels for this problem in KEMA: the (fast) histogram intersection
kernel, Ki(𝐱j, 𝐱k) = ∑
d min{xd
j , xd
k}, and the 𝜒kernel, K𝜒(𝐱j, 𝐱k) = exp[−𝜒∕(𝜎)],
with 𝜒= (∕) ∑
d(xd
j −xd
k)∕(xd
j + xd
k) (Sreekanth et al., ). We used u = 
unlabeled samples to compute the graph Laplacians, for which a k-NN graph with
k = was used.
The performances in all problems are shown in Table .: KEMA has the best
performance of all methods and, almost in all cases, it improves the results obtained
by the semi-supervised methods using labels in the source domain only. In three of the
eight settings, the best results are obtained by KEMA, when compared with state-of-
the-art (semi-)supervised algorithms, and similar performance to state-of-the-art GFK
in six out of the eight settings. The advantage of KEMA is that it handles domains
of diﬀerent dimensionality without requiring a discriminative classiﬁer to align the
domains, such as for MMDT.
12.6
Concluding Remarks
We reviewed the ﬁeld of kernel feature extraction and dimensionality reduction. This
ﬁeld of signal processing and machine learning deals with the challenging problem
of ﬁnding a proper feature representation for the data. We have seen that selecting
diﬀerent criteria to optimize gives rise to particularly diﬀerent optimization problems,
solutions, and methods. We started our trip under the framework of multivariate
analysis. The techniques described in this chapter are used in real-world application
with an increased level of popularity. Following the popular PCA technique, there is
a large amount of linear and kernel-based methods that in general perform better in
supervised applications, for they ﬁnd projections that maximize the alignment with
the target variables. The common features and diﬀerences between methods have been
analyzed, as well as the relationships to existing kernel discriminative feature extraction
and statistical dependence estimation approaches. We also studied recent methods to
make kernel MVA more suitable to real-life applications, both for large-scale data sets
and for problems with few labeled data. Solutions include sparse and SSL extensions.
Actually, seeking for the appropriate features that facilitate classiﬁcation or regression
cuts to the heart of manifold learning. We provided examples that show the properties
of the methods using challenging real-world data which exhibit complex manifolds. We

588
Digital Signal Processing with Kernel Methods
completed the chapter by reviewing the ﬁeld of domain adaptation and the diﬀerent
approaches to the problem from the kernel formulation point of view.
12.7
Questions and Problems
Exercise ..
When it is useful to use KPCA instead of PCA?
Exercise ..
In what situation is it useful to use PLS instead of PCA?
Exercise ..
Using the iris dataset from MATLAB, predict the fourth dimension
using the remainder three, but reducing previously the dimensionality to two dimen-
sions using the methods proposed in Section ..
Exercise ..
In the glass dataset from MATLAB, compute the HSIC between each
pair of variables.
Exercise ..
Develop mathematically how Equation .can be reduced to Equa-
tion ..
Exercise ..
Reproduce Figure .using the crab dataset from MATLAB.
Exercise ..
In what situation is it useful to use KICA instead of linear ICA?
Exercise ..
Obtain the actualization rule for OKECA.
Exercise ..
Implement HSIC using Cholesky decomposition.
Exercise ..
Implement KPCA imposing sparsity in the projection vectors.

589
References
Abbate, A., Koay, J., Frankel, J., Schroeder, S., and Das, P. (). Signal detection and noise
suppression using a wavelet transform signal processor: application to ultrasonic ﬂaw
detection. IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control,
(), –.
Abe, N. and Mamitsuka, H. (). Query learning strategies using boosting and bagging.
In J. Shavlik, editor, ICML ’Proceedings of the Fifteenth International Conference on
Machine Learning, pages –. Morgan Kaufmann, San Francisco, CA.
Abrahamsen, T. J. and Hansen, L. K. (). A cure for variance inﬂation in high
dimensional kernel principal component analysis. Journal of Machine Learning
Research, , –.
Adali, T. and Liu, X. (). Canonical piecewise linear network for nonlinear ﬁltering and
its application to blind equalization. Signal Processing, (), –.
Aharon, M., Elad, M., and Bruckstein, A. (). K-SVD: an algorithm for designing
overcomplete dictionaries for sparse representation. IEEE Transactions on Signal
Processing, (), –.
Ahmed, T., Oreshkin, B., and Coates, M. (). Machine learning approaches to network
anomaly detection. In J. Chase and I. Cohen, editors, SYSML’Proceedings of the nd
USENIX Workshop on Tackling Computer Systems Problems with Machine Learning
Techniques, SYSML’, pages :–:. USENIX Association, Berkeley, CA.
Aizerman, M. A., Braverman, E. M., and Rozoner, L. (). Theoretical foundations of the
potential function method in pattern recognition learning. Automation and Remote
Control, , –.
Akaike, H. (). A new look at the statistical model identiﬁcation. IEEE Transactions on
Automatic Control, (), –.
Allwein, E. L., Schapire, R. E., and Singer, Y. (). Reducing multiclass to binary: a
unifying approach for margin classiﬁers. Journal of Machine Learning Research, ,
–.
Alper, P. (). A consideration of the discrete volterra series. IEEE Transactions on
Automatic Control, (), –.
Altun, Y., Tsochantaridis, I., and Hofmann, T. (). Hidden Markov support vector
machines. In T. Fawcett and N. Mishra, editors, Proceedings of the Twentieth
International Conference on Machine Learning (ICML-), Washington, DC, pages
–. AAAI Press, Menlo Park, CA.
Digital Signal Processing with Kernel Methods, First Edition. José Luis Rojo-Álvarez, Manel Martínez-Ramón,
Jordi Muñoz-Marí, and Gustau Camps-Valls.
© John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.

590
References
Altun, Y., Hofmann, T., and Tsochantaridis, I. (). Support vector learning for
interdependent and structured output spaces. In G. Bakır, T. Hofmann, B. Schölkopf,
A. J. Smola, and S. Vishwanathan, editors, Predicting Structured Data, pages –.
MIT Press.
Anderson, J. R. (). The Architecture of Cognition. Harvard University Press,
Cambridge, MA, USA.
Andrieu, C., de Freitas, N., Doucet, A., and Jordan, M. (). An introduction to MCMC
for machine learning. Machine Learning, , –.
Anguita, D. and Gagliolo, M. (). MDL based model selection for relevance vector
regression. In J. Dorronsoro, editor, Artiﬁcial Neural Networks – ICANN , pages
–. Springer, Berlin.
Arenas-García, J. and Camps-Valls, G. (). Eﬃcient kernel orthonormalized PLS for
remote sensing applications. IEEE Transactions on Geoscience and Remote Sensing, ,
–.
Arenas-García, J. and Figueiras-Vidal, A. R. (). Adaptive combination of
proportionate ﬁlters for sparse echo cancellation. IEEE Transactions on Audio, Speech &
Language Processing, (), –.
Arenas-García, J. and Petersen, K. B. (). Kernel multivariate analsis in remote sensing
feature extraction. In G. Camps-Valls and L. Bruzzone, editors, Kernel Methods for
Remote Sensing Data Analysis. John Wiley & Sons.
Arenas-García, J., Figueiras-Vidal, A. R., and Sayed, A. H. (). Mean-square
performance of a convex combination of two adaptive ﬁlters. IEEE Transactions on
Signal Processing, (), –.
Arenas-García, J., Petersen, K. B., and Hansen, L. K. (). Sparse kernel
orthonormalized PLS for feature extraction in large data sets. In B. Schölkopf, J. C.
Platt, and T. Hoﬀman, editors, NIPS’Proceedings of the th International Conference
on Neural Information Processing Systems. MIT Press, Cambridge, MA.
Arenas-García, J., Petersen, K. B., Camps-Valls, G., and Hansen, L. K. (). Kernel
multivariate analysis framework for supervised subspace learning. Signal Processing
Magazine, (), –.
Aronszajn, N. (). Theory of reproducing kernels. Transactions of the American
Mathematical Society, (), –.
Arulampalam, M. S., Maskell, S., Gordon, N., and Clapp, T. (). A tutorial on particle
ﬁlters for online nonlinear/non-Gaussian Bayesian tracking. IEEE Transactions on
Signal Processing, (), –.
Aschbacher, E. and Rupp, M. (). Robustness analysis of a gradient identiﬁcation
method for a nonlinear Wiener system. In Proceedings of the IEEE/SP th
Workshop on Statistical Signal Processing (SSP ), Bordeaux, France.
Atkinson, K. (). An Introduction to Numerical Analysis. John Wiley & Sons.
Aurenhammer, F. (). Voronoi diagrams – a survey of a fundamental geometric data
structure. ACM Computing Surveys, (), –.
Babaie-Zadeh, M., Jutten, C., and Nayebi, K. (). A geometric approach for separating
post nonlinear mixtures. In Proceedings of the th European Signal Processing
Conference (EUSIPCO), volume II, pages –. IEEE.
Bach, F. and Jordan, M. (). Blind one-microphone speech separation: a spectral
learning approach. In L. Saul, Y. Weiss, and L. Bottou, editors, Proceedings of the th

References
591
Annual Conference on Neural Information Processing Systems (NIPS ), pages
–. MIT Press, Cambridge, MA.
Bach, F. and Jordan, M. (a). Predictive low-rank decomposition for kernel methods.
In ICML ’Proceedings of the nd International Conference on Machine Learning.
ACM, New York.
Bach, F. and Jordan, M. I. (). Kernel independent component analysis. Journal of
Machine Learning Research, , –.
Bach, F. R. and Jordan, M. I. (b). Predictive low-rank decomposition for kernel
methods. In ICML ’Proceedings of the nd International Conference on Machine
Learning, pages –. ACM, New York.
Bach, F. R., Lanckriet, G. R., and Jordan, M. I. (). Multiple kernel learning, conic
duality, and the SMO algorithm. In ICML ’Proceedings of the st International
Conference on Machine Learning, page . ACM, New York.
Bahl, P. and Padmanabhan, V. (). RADAR: a in-building RF based user location and
tracking system. In IEEE INFOCOM ’Proceedings of the th Annual Joint
Conference of the IEEE Computer and Communications Societies (Cat. No.CH),
pages –. IEEE Press.
Bai, E.-W. (). An optimal two stage identiﬁcation algorithm for Hammerstein–Wiener
nonlinear systems. In Proceedings of the American Control Conference, volume ,
pages –.
Bai, W., He, C., Jiang, L. G., and Li, X. X. (). Robust channel estimation in
MIMO–OFDM systems. Electronics Letters, (), –.
Bajwa, W. U., Haupt, J., Sayeed, A. M., and Nowak, R. (). Compressed channel sensing:
a new approach to estimating sparse multipath channels. Proceedings of the IEEE, (),
–.
Baker, C. (). Joint measures and cross-covariance operators. Transactions of the
American Mathematical Society, , –.
Bakır, G., Hofmann, T., Schölkopf, B., Smola, A., Taskar, B., and Vishwanathan, S., editors
(). Predicting Structured Data. MIT Press, Cambridge, MA.
Balestrino, A., Landi, A., Ould-Zmirli, M., and Sani, L. (). Automatic nonlinear
auto-tuning method for Hammerstein modeling of electrical drives. IEEE Transactions
on Industrial Electronics, (), –.
Ball, J. E. and Bruce, L. M. (). Level set hyperspectral image classiﬁcation using best
band analysis. IEEE Transactions on Geoscience and Remote Sensing, (),
–.
Banham, M. and Katsaggelos, A. (). Digital image restoration. IEEE Signal Processing
Magazine, , –.
Barker, M. and Rayens, W. (). Partial least squares for discrimination. Journal of
Chemometrics, , –.
Basseville, M. and Nikiforov, I. V. (). Detection of Abrupt Changes: Theory and
Application. Prentice-Hall, Upper Saddle River, NJ.
Bayro-Corrochano, E. J. and Arana-Daniel, N. (). Cliﬀord support vector machines for
classiﬁcation, regression, and recurrence. IEEE Transactions on Neural Networks,
(), –.
Belkin, M. and Niyogi, P. (). Laplacian eigenmaps for dimensionality reduction and
data representation. Neural Computation, (), –.

592
References
Belkin, M. and Niyogi, P. (). Semi-supervised learning on Riemannian manifolds.
Machine Learning, Special Issue on Clustering, , –.
Belkin, M., Niyogi, P., and Sindhwani, V. (). Manifold regularization: a geometric
framework for learning from labeled and unlabeled examples. Journal of Machine
Learning Research, , –.
Bell, A. J. and Sejnowski, T. J. (). An information-maximization approach to blind
separation and blind deconvolution. Neural Computation, (), –.
Belouchrani, A. and Amin, M. G. (). Blind source separation based on time–frequency
signal representations. IEEE Transactions on Signal Processing, (), –.
Ben-David, S., Blitzer, J., Crammer, K., and Pereira, O. (). Analysis of representations
for domain adaptation. In B. Schölkopf, J. Platt, and T. Hofmann, editors, Advances in
Neural Information Processing Systems , Proceedings of the Conference. MIT
Press, Cambridge, MA.
Bermejo, J., Antoranz, J., Burwash, I., Rojo-Álvarez, J. L., Moreno, M., García-Fernández,
M., and Otto, C. (). In-vivo analysis of the instantaneous transvalvular pressure
diﬀerence in aortic valve stenosis. implications of unsteady ﬂuid-dynamics for the
clinical assessment of disease severity. Journal of Heart Valve Disease, (), –.
Bicego, M., Ula¸s, A., Castellani, U., Perina, A., Murino, V., Martins, A. F. T., Aguiar, P.
M. Q., and Figueiredo, M. A. T. (). Combining information theoretic kernels with
generative embeddings for classiﬁcation. Neurocomputing, , –.
Bickel, S., Brückner, M., and Scheﬀer, T. (). Discriminative learning under covariate
shift. Journal of Machine Learning Research, , –.
Billings, S. (). Identiﬁcation of nonlinear systems: a survey. Proceedings of IEE, Part D,
, –.
Billings, S. A. and Fakhouri, S. Y. (). Identiﬁcation of nonlinear systems using the
Wiener model. Electronics Letters, (), –.
Billings, S. A. and Fakhouri, S. Y. (). Nonlinear system identiﬁcation using the
Hammerstein model. International Journal of System Sciences, (), –.
Billings, S. A. and Fakhouri, S. Y. (). Identiﬁcation of systems containing linear
dynamic and static nonlinear elements. Automatica, , –.
Bimbot, F., Bonastre, J.-F., Fredouille, C., Gravier, G., Magrin-Chagnolleau, I., Meignier, S.,
Merlin, T., Ortega-García, J., Petrovska-Delacrétaz, D., and Reynolds, D. A. (). A
tutorial on text-independent speaker veriﬁcation. EURASIP Jounal of Applied Signal
Processing, , –.
Bishop, C. (). Pattern Recognition and Machine Learning. Springer.
Bishop, C. and Tipping, M. (). Variational relevance vector machines. In C. Boutilier
and M. Goldszmidt, editors, Proceedings of th Conference on Uncertainty in Artiﬁcial
Intelligence, pages –. Morgan Kaufmann, San Francisco, CA.
Blankertz, B., Muller, K.-R., Curio, G., Vaughan, T., Schalk, G., Wolpaw, J., Schlogl, A.,
Neuper, C., Pfurtscheller, G., Hinterberger, T., Schroder, M., and Birbaumer, N. ().
The BCI competition : progress and perspectives in detection and discrimination
of EEG single trials. IEEE Transactions on Biomedical Engineering, (), –.
Blaschko, M. and Lampert, C. (). Learning to localize objects with structured output
regression. In D. Forsyth, P. Torr, and A. Zisserman, editors, Computer Vision: ECCV
, pages –. Springer.
Blaschko, M., Shelton, J., Bartels, A., Lampert, C., and Gretton, A. (). Semi-supervised
kernel canonical correlation analysis with application to human fMRI. Pattern
Recognition Letters, , –.

References
593
Blattberg, R. and Neslin, S. (). Sales promotion models. In J. Eliashberg and G. Lilien,
editors, Handbooks in Operations Research and Management Science: Marketing
Models, pages –. North-Holland, Amsterdam.
Blitzer, J., McDonald, R., and Pereira, F. (). Domain adaptation with structural
correspondence learning. In EMNLP ’Proceedings of the Conference on
Empirical Methods in Natural Language Processing, pages –. Association for
Computational Linguistics, Stroudsburg, PA.
Bloomﬁeld, P. and Steiger, W. (). Least absolute deviations curve-ﬁtting. SIAM Journal
on Scientiﬁc Computing, (), –.
Boﬁll, P. and Zibulevsky, M. (). Underdetermined blind source separation using sparse
representations. Signal Processing, (), –.
Bookstein, F. (). Principal warps: thin-plate splines and the decomposition of
deformations. IEEE Transactions on Pattern Analysis and Machine Intelligence, (),
–.
Bordes, A., Ertekin, S., Weston, J., and Bottou, L. (). Fast kernel classiﬁers with online
and active learning. Journal of Machine Learning Research, , –.
Borga, M., Landelius, T., and Knutsson, H. (). A uniﬁed approach to PCA, PLS, MLR
and CCA. Technical Report LiTH-ISY-R, -, Linköping University.
Borgwardt, K., Gretton, A., Rasch, M., Kriegel, H.-P., Schoelkopf, B., and Smola, A. ().
Integrating structured biological data by kernel maximum mean discrepancy.
Bioinformatics (ISMB), (), e–e.
Bose, N. K. and Basu, S. (). Multidimensional systems theory: matrix Padé
approximants. In IEEE Conference on Decision and Control including the th
Symposium on Adaptive Processes, pages –. IEEE Press.
Boser, B. E., Guyon, I., and Vapnik, V. N. (). A training algorithm for optimal margin
classiﬁers. In COLT ’Proceedings of the Fifth Annual Workshop on Computational
Learning Theory, pages –. ACM.
Bottou, L. and Bengio, Y. (). Convergence properties of the k-means algorithms. In
G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, Advances in Neural Information
Processing Systems , pages –. MIT Press, Cambridge, MA.
Bottou, L., Chapelle, O., DeCoste, D., and Weston, J., editors (). Large Scale Kernel
Machines. MIT Press, Cambridge, MA.
Bouboulis, P. and Theodoridis, S. (). The complex Gaussian kernel LMS algorithm. In
K. Diamantaras, W. Duch, and L. Iliadis, editors, Artiﬁcial Neural Networks – ICANN
, volume of Lecture Notes in Computer Science, pages –. Springer,
Berlin.
Bouboulis, P. and Theodoridis, S. (). Extension of Wirtinger’s calculus to reproducing
kernel Hilbert spaces and the complex kernel LMS. IEEE Transactions on Signal
Processing, (), –.
Bouboulis, P., Theodoridis, S., and Mavroforakis, M. (). The augmented complex
kernel LMS. IEEE Transactions on Signal Processing, (), –.
Boyd, S. and Chua, L. (). Fading memory and the problem of approximating nonlinear
operators with Volterra series. IEEE Transactions on Circuits and Systems, (),
–.
Bradley, D. and Bagnell, J. (). Diﬀerentiable sparse coding. In D. Koller,
D. Schuurmans, Y. Bengio, and L. Bottou, editors, NIPS’Proceedings of the st
International Conference on Neural Information Processing Systems, pages –.
Curran Associates.

594
References
Bradley, P. S. and Mangasarian, O. L. (). Feature selection via concave minimization
and support vector machines. In J. Shavlik, editor, ICML ’Proceedings of the Fifteenth
International Conference on Machine Learning, pages –. Morgan Kaufmann, San
Francisco, CA.
Braun, M. L., Buhmann, J. M., and Müller, K.-R. (). On relevant dimensions in kernel
feature spaces. Journal of Machine Learning Research, , –.
Bredensteiner, E. J. and Bennett, K. P. (). Multicategory classiﬁcation by support
vector machines. Computational Optimization and Applications, (), –.
Breiman, L. (). Bagging predictors. Technical Report , University of California at
Berkley.
Brilliant, M. B. (). Theory of the analysis of nonlinear systems. RLE Technical Report
, MIT.
Brinker, K. (). Incorporating diversity in active learning with support vector
machines. In ICML’Proceedings of the Twentieth International Conference on
International Conference on Machine Learning. AAAI Press.
Bruls, J., Chou, C., Haverkamp, B., and Verhaegen, M. (). Linear and nonlinear system
identiﬁcation using separable least-squares. European Journal of Control Engineering
Practice, (), –.
Bugallo, M. F., Martino, L., and Corander, J. (). Adaptive importance sampling in
signal processing. Digital Signal Processing, , –.
Burg, J. P. (). Maximum entropy spectral analysis. In Proceedings of th Meeting of
Society of Exploration Geophysicists, Oklahoma City, OK.
Burges, C. (). A tutorial on support vector machines for pattern recognition. Data
Mining and Knowledge Discovery, (), –.
Burges, C. J. C. (). Geometry and invariance in kernel based methods. In B. Schölkopf,
C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods – Support Vector
Learning, pages –. MIT Press, Cambridge, MA.
Burrus, C. S. and Parks, T. W. (). Time domain design of recursive digital ﬁlters. IEEE
Transactions on Audio and Electroacoustics, (), –.
Butzer, P. L. and Stens, R. L. (). Sampling for not necessarily band-limited functions: a
historical overview. SIAM Review, (), –.
Byrne, C. L. and Fitzgerald, R. (). An approximation theoretic approach to maximum
entropy spectral analysis. IEEE Transactions on Acoustics, Speech and Signal Processing,
(), –.
Cambanis, S., Huang, S., and Simons, G. (). On the theory of elliptically contoured
distributions. Journal of Multivariate Analysis, (), – .
Campbell, C., Cristianini, N., and Smola, A. (). Query learning with large margin
classiﬁers. In P. Langley, editor, ICML ’Proceedings of the Seventeenth International
Conference on Machine Learning, pages – . Morgan Kaufmann, San Francisco,
CA.
Camps-Valls, G. (). Kernel spectral angle mapper. Electronics Letters, (),
–.
Camps-Valls, G. and Bruzzone, L. (). Kernel-based methods for hyperspectral image
classiﬁcation. IEEE Transactions on Geoscience and Remote Sensing, , –.
Camps-Valls, G. and Bruzzone, L., editors (). Kernel methods for Remote Sensing Data
Analysis. Wiley & Sons, UK.

References
595
Camps-Valls, G., Soria-Olivas, E., Pérez-Ruixo, J., Artés-Rodríguez, A., Pérez-Cruz, F., and
Figueiras-Vidal, A. (). A proﬁle-dependent kernel-based regression for
cyclosporine concentration prediction. In Neural Information Processing Systems,
NIPS’. Workshop on New Directions in Kernel-based Learning Methods.
Camps-Valls, G., Soria-Olivas, E., Pérez-Ruixo, J., Artés-Rodríguez, A., Pérez-Cruz, F., and
Figueiras-Vidal, A. (). Cyclosporine concentration prediction using clustering and
support vector regression methods. Electronics Letters, (), –.
Camps-Valls, G., Martínez-Ramón, M., Rojo-Álvarez, J. L., and Soria-Olivas, E. ().
Robust 𝛾-ﬁlter using support vector machines. Neurocomputing, , –.
Camps-Valls, G., Gómez-Chova, L., Muñoz-Marí, J., Vila-Francés, J., and Calpe-Maravilla,
J. (a). Composite kernels for hyperspectral image classiﬁcation. IEEE Geoscience
and Remote Sensing Letters, (), –.
Camps-Valls, G., Gómez-Chova, L., Vila-Francés, J., Amorós-López, J., Muñoz-Marí, J.,
and Calpe-Maravilla, J. (b). Retrieval of oceanic chlorophyll concentration with
relevance vector machines. Remote Sensing of Environment, (), –.
Camps-Valls, G., Bruzzone, L., Rojo-Álvarez, J. L., and Melgani, F. (c). Robust support
vector regression for biophysical parameter estimation from remotely sensed images.
IEEE Geoscience and Remote Sensing Letters, (), –.
Camps-Valls., G., Rojo-Álvarez, J. L., and Martínez-Ramón, M., editors (). Kernel
Methods in Bioengineering, Signal, and Image Processing. Idea Group Inc., Hershey, PA.
Camps-Valls, G., Martínez-Ramón, M., Rojo-Álvarez, J. L., and Muñoz-Marí, J. (a).
Non-linear system identiﬁcation with composite relevance vector machines. IEEE
Signal Processing Letters, (), –.
Camps-Valls, G., Bandos, T., and Zhou, D. (b). Semi-supervised graph-based
hyperspectral image classiﬁcation. IEEE Transactions on Geoscience and Remote
Sensing, (), –.
Camps-Valls, G., Gómez-Chova, L., Muñoz-Marí, J., Rojo-Álvarez, J. L., and
Martínez-Ramón, M. (). Kernel-based framework for multitemporal and
multisource remote sensing data classiﬁcation and change detection. IEEE Transactions
on Geoscience and Remote Sensing, (), –.
Camps-Valls, G., Muñoz-Marí, J., Gómez-Chova, L., Richter, K., and Calpe-Maravilla, J.
(a). Biophysical parameter estimation with a semisupervised support vector
machine. IEEE Geoscience and Remote Sensing Letters, (), –.
Camps-Valls, G., Muñoz-Marí, J., Martínez-Ramón, M., Requena-Carrion, J., and
Rojo-Álvarez, J. L. (b). Learning non-linear time-scales with kernel gamma-ﬁlters.
Neurocomputing, (–, SI), –.
Candes, E. J. and Wakin, M. B. (). An introduction to compressive sampling. IEEE
Signal Processing Magazine, (), –.
Candes, E. J., Romberg, J., and Tao, T. (). Robust uncertainty principles: exact signal
reconstruction from highly incomplete frequency information. IEEE Transactions on
Information Theory, (), –.
Cao, L. J., Keerthi, S. S., Ong, C.-J., Zhang, J. Q., Periyathamby, U., Fu, X. J., and Lee, H. P.
(). Parallel sequential minimal optimization for the training of support vector
machines. IEEE Transactions on Neural Networks, , –.
Capon, J. (). High-resolution frequency-wavenumber spectrum analysis. Proceedings
of the IEEE, (), –.

596
References
Cardoso, J. (). Blind signal separation: statistical principles. Proceedings of the IEEE,
(), –.
Cardoso, J.-F. and Souloumiac, A. (). Blind beamforming for non-Gaussian signals.
IEE Proceedings F – Radar and Signal Processing, (), –.
Casdagli, M. and Eubank, S., editors (). Nonlinear Modeling and Forecasting, volume
XII. Addison-Wesley, Reading, MA.
Cawley, G. C. and Talbot, N. L. C. (). Reduced rank kernel ridge regression. Neural
Processing Letters, (), –.
Chandola, V., Banerjee, A., and Kumar, V. (). Anomaly detection: a survey. ACM
Compututing Surveys, (), :–:.
Chang, C.-C. and Lin, C.-J. (). Training nu-support vector regression: theory and
algorithms. Neural Computation, (), –.
Chang, E., Zhu, K., Wang, H., Bai, H., Li, J., Qiu, Z., and Cui, H. (). Parallelizing
support vector machines on distributed computers. In J. C. Platt, D. Koller, Y. Singer,
and S. Roweis, editors, Advances in Neural Information Processing Systems , pages
–. MIT Press, Cambridge, MA.
Chapelle, O. (). Training a support vector machine in the primal. Neural
Computation, (), –.
Chapelle, O., Weston, J., and Schölkopf, B. (). Cluster kernels for semi-supervised
learning. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural
Information Processing Systems , pages –. MIT Press, Cambridge, MA.
Chapelle, O., Schölkopf, B., and Zien, A. (). Semi-Supervised Learning. MIT Press,
Cambridge, MA.
Chen, B., Zhao, S., Zhu, P., and Príncipe, J. C. (). Quantized kernel least mean square
algorithm. IEEE Transactions on Neural Networks and Learning Systems, (), –.
Chen, H.-W. (). Modeling and identiﬁcation of parallel nonlinear systems: structural
classiﬁcation and parameter estimation methods. Proceedings of the IEEE, (), –.
Chen, S., Billings, S. A., and Luo, W. (). Orthogonal least squares methods and their
application to non-linear system identiﬁcation. International Journal of Control, ,
–.
Chen, S., Gunn, S., and Harris, C. (). Decision feedback equalizer design using
support vector machines. IEE Proceedings – Vision, Image and Signal Processing,
(), –.
Chen, S., Sanmigan, A. K., and Hanzo, L. (a). Adaptive mutiuser receiver using a
support vector machine technique. In IEEE VTS rd Vehicular Technology Conference,
Spring . Proceedings (Cat. No.CH), pages –. IEEE Press,
Piscataway, NJ.
Chen, S., Sanmigan, A. K., and Hanzo, L. (b). Support vector machine multiuser
receiver for DS-CDMA signals in multipath channels. Neural Networks, (), –.
Chen, S. S., Donoho, D. L., Michael, and Saunders, A. (). Atomic decomposition by
basis pursuit. SIAM Journal on Scientiﬁc Computing, , –.
Cheng, M., Pun, C.-M., and Tang, Y. Y. (). Nonnegative class-speciﬁc entropy
component analysis with adaptive step search criterion. Pattern Analysis and
Applications, (), –.
Cheng, S. and Shih, F. Y. (). An improved incremental training algorithm for support
vector machines using active query. Pattern Recognition, , – .

References
597
Cherkassky, V. and Ma, Y. (). Practical selection of SVM parameters and noise
estimation for SVM regression. Neural Networks, (), –.
Cherkassky, V. S. and Mulier, F. (). Learning from Data: Concepts, Theory, and
Methods. John Wiley & Sons, Inc., New York, NY.
Cho, M. A., Debba, P., Mathieu, R., Naidoo, L., van Aardt, J., and Asner, G. P. ().
Improving discrimination of savanna tree species through a multiple-endmember
spectral angle mapper approach: canopy-level analysis. IEEE Transactions on
Geoscience and Remote Sensing, (), –.
Choi, H. and Munson, D. C. (a). Analysis and design of minimax-optimal
interpolators. IEEE Transactions on Signal Processing, (), –.
Choi, H. and Munson, D. C. (b). Direct-Fourier reconstruction in tomography and
synthetic aperture radar. International Journal of Imaging Systems and Technology, (),
–.
Choi, H. and Willians, W. (). Improved time–frequency representation of
multicomponent signals using exponential kernels. IEEE Transactions on Acoustics,
Speech, and Signal Processing, (), –.
Christodoulou, C. and Georgiopoulos, M. (). Applications of Neural Networks in
Electromagnetics. Artech House.
Cipollini, P., Corsini, G., Diani, M., and Grass, R. (). Retrieval of sea water optically
active parameters from hyperspectral data by means of generalized radial basis function
neural networks. IEEE Transactions on Geoscience and Remote Sensing, , –.
Cohen, L. (). Generalized phase-space distribution functions. Journal of
Mathematical Physics, , –.
Cohn, D., Atlas, L., and Ladner, R. (). Improving generalization with active learning.
Machine Learning, (), –.
Cohn, D., Ghaharamani, Z., and Jordan, M. I. (). Active learning with statistical
models. Journal of Artiﬁcial Intelligence Research, , –.
Comon, P. (). Independent component analysis – a new concept? Signal Processing,
, –.
Comon, P., Jutten, C., and Herault, J. (). Blind separation of sources, part II: problem
statement. Signal Processing, , –.
Conde-Pardo, P., Guerrero-Curieses, A., Rojo-Álvarez, J. L, Yotti, R., Requena-Carrion, J.,
Antoranz, J., and Bermejo, J. (). A new method for single-step robust
post-processing of ﬂow color Doppler M-mode images using support vector machines.
In Computers in Cardiology, pages –. IEEE Press, Piscataway, NJ.
Congalton, R. and Green, K. (). Assessing the Accuracy of Remotely Sensed Data:
Principles and Practices. Lewis Publishers, Boca Raton, FL.
Constantinides, A. G. (). Spectral transformations for digital ﬁlters. Proceedings of the
IEE, , –.
Copa, L., Tuia, D., Volpi, M., and Kanevski, M. (). Unbiased query-by-bagging active
learning for VHR image classiﬁcation. In Proceedings of the SPIE Remote Sensing
Conference, Toulouse, France.
Cortes, C. and Vapnik, V. (). Support Vector Networks. Machine Learning, ,
–.
Cortes, C., Haﬀner, P., and Mohri, M. (). Positive deﬁnite rational kernels. In
B. Schölkopf and M. Warmuth, editors, Proceedings of the th Annual Conference on

598
References
Computational Learning Theory (COLT ), volume of Lecture Notes in
Computer Science, pages –. Springer, Berlin.
Cortes, C., Mohri, M., and Rostamizadeh, A. (). Algorithms for learning kernels based
on centered alignment. Journal of Machine Learning Research, (), –.
Courty, N., Flamary, R., and Tuia, D. (). Domain adaptation with regularized optimal
transport. In T. Calders, F. Esposito, E. Hüllermeier, and R. Meo, editors, Domain
Adaptation with Regularized Optimal Transport, volume of Lecture Notes in
Computer Science, pages –. Springer, Berlin.
Cousseau, J. E., Figueroa, J. L., Werner, S., and Laakso, T. I. (). Eﬃcient nonlinear
Wiener model identiﬁcation using a complex-valued simplicial canonical piecewise
linear ﬁlter. IEEE Transactions on Signal Processing, (), –.
Cover, T. M. and Thomas, J. A. (). Elements of Information Theory. Wiley-Interscience.
Cox, T. and Cox, M. (). Multidimensional Scaling. Chapman and Hall.
Crammer, K. and Singer, Y. (). On the algorithmic implementation of multiclass
kernel-based vector machines. Journal of Machine Learning Research, , –.
Cremers, D., Kohlberger, T., and Schnörr, C. (). Shape statistics in kernel space for
variational image segmentation. Pattern Recognition, (), –.
Cross, B., Garlitski, A., and Estes, III, N. (). Advances in electroanatomic mapping
systems. In S. Saksena, R. Damiano, Jr, N. Estes III, and F. Marchlinski, editors,
Interventional Cardiac Electrophysiology: A Multidisciplinary Approach, pages –.
Cardiotext Publishing, Minneapolis, MN.
Csató, L. and Opper, M. (). Sparse online Gaussian processes. Neural Computation,
(), –.
Dai, W., Chen, Y., Xue, G.-R., Yang, Q., and Yu, Y. (). Translated learning: transfer
learning across diﬀerent feature spaces. In D. Koller, D. Schuurmans, Y. Bengio, and
L. Bottou, editors, NIPS’Proceedings of the st International Conference on Neural
Information Processing Systems, pages –. Curran Associates.
Daniušis, P. and Vaitkus, P. (). Supervised feature extraction using Hilbert–Schmidt
norms. In E. Corchado and H. Yin, editors, Intelligent Data Engineering and Automated
Learning – IDEAL , volume of Lecture Notes in Computer Science, pages
–, Berlin. Springer-Verlag.
Daumé III, H. and Marcu, D. (). Domain adaptation for statistical classiﬁers. Journal of
Artiﬁcial Intelligence Research, (), –.
de Berg, M., Cheong, O., van Kreveld, M., and Overmars, M. (). Computational
Geometry: Algorithms and Applications. Springer-Verlag, Berlin.
De Freitas, N., Milo, M., Clarkson, P., Niranjan, M., and Gee, A. (). Sequential support
vector machines. In Neural Networks for Signal Processing IX: Proceedings of the 
IEEE Signal Processing Society Workshop (Cat. No.TH), pages –. IEEE Press.
De Kruif, B. J. and De Vries, T. J. A. (). Pruning error minimization in least squares
support vector machines. IEEE Transactions on Neural Networks, (), –.
De Vries, B. and Principe, J. C. (). Short term memory structures for dynamic neural
networks. In Conference Record of the Twenty-Sixth Asilomar Conference on Signals,
Systems & Computers. IEEE Press, Piscataway, NJ.
Del Moral, P. (). Non-linear ﬁltering: interacting particle resolution. Markov Processes
and Related Fields, (), –.
Demir, B., Persello, C., and Bruzzone, L. (). Batch mode active learning methods for
the interactive classiﬁcation of remote sensing images. IEEE Transactions on Geoscience
and Remote Sensing, (), –.

References
599
Dempsey, E. J. and Westwick, D. T. (). Identiﬁcation of Hammerstein models with
cubic spline nonlinearities. IEEE Transactions on Biomedical Engineering, (),
–.
Demuth, H. and Beale, M. (). Neural Network Toolbox: For Use with MATLAB: User’s
Guide. The Mathworks, Natick, MA.
Deng, L. and Li, X. (). Machine learning paradigms for speech recognition: an
overview. IEEE Transactions on Audio, Speech, and Language Processing, (),
–.
Desbrun, M., Hirani, A., and Leok, M.and Marsden, J. (). Discrete exterior calculus.
arXiv preprint math/.
Desobry, F. and Févotte, C. (). Kernel PCA based estimation of the mixing matrix in
linear instantaneous mixtures of sparse sources. In IEEE International Conference
on Acoustics Speech and Signal Processing Proceedings, volume . IEEE Press,
Piscataway, NJ.
Dhanjal, C., Gunn, S., and Shawe-Taylor, J. (). Eﬃcient sparse kernel feature
extraction based on partial least squares. IEEE Transactions on Pattern Analysis and
Machine Intelligence, (), –.
Dhillon, I., Guan, Y., and Kulis, B. (). A uniﬁed view of kernel k-means, spectral
clustering and graph cuts. Technical Report UTCS Technical Report No. TR--,
University of Texas, Austin, Departement of Computer Science.
Di, W. and Crawford, M. (). Multi-view adaptive disagreement based active learning
for hyperspectral image classiﬁcation. In IEEE International Geoscience and
Remote Sensing Symposium. IEEE Press, Piscataway, NJ.
Diethron, E. and Munson, Jr, D. (). A linear time-varying system framework for
noniterative discrete-time band-limited signal extrapolation. IEEE Transactions on
Signal Processing, (), –.
Dietterich, T. and Bakiri, G. (). Solving multiclass learning problems via
error-correcting output codes. Journal of Artiﬁcial Intelligence Research, ,
–.
Donoho, D. and Johnstone, I. (). Adapting to unknown smoothness via wavelet
shrinkage. Journal of the American Statistical Association, (),
–.
Donoho, D. L. (). Compressed sensing. IEEE Transactions on Information Theory,
(), –.
Dorﬀner, G. (). Neural networks for time series processing. Neural Network World, ,
–.
Doucet, A. and Wang, X. (). Monte Carlo methods for signal processing. IEEE Signal
Processing Magazine, (), –.
Drezet, P. and Harrison, R. (). Support vector machines for system identiﬁcation. In
Proceedings of UKACC International Conference on Control’, pages –.
Institution of Electrical Engineers, London.
D’Souza, A., Vijayakumar, S., and Schaal, S. (). The Bayesian backﬁtting relevance
vector machine. In ICML ’Proceedings of the Twenty-First International Conference
on Machine Learning. ACM, New York, NY.
Duan, L., Tsang, I. W., Xu, D., and Chua, T.-S. (). Domain adaptation from multiple
sources via auxiliary classiﬁers. In ICML ’The th Annual International Conference
on Machine Learning held in conjunction with the International Conference on
Inductive Logic Programming, pages –. ACM, New York, NY.

600
References
Duan, L., Xu, D., Tsang, I. W.-H., and Luo, J. (). Visual event recognition in videos by
learning from web data. IEEE Transactions on Pattern Analysis and Machine
Intelligence, (), –.
Duda, R. O., Hart, P. E., and Stork, D. G. (). Pattern Classiﬁcation and Scene Analysis:
Part I Pattern Classiﬁcation. John Wiley & Sons, nd edition.
Dudgeon, D. E. and Merserau, R. M. (). Multidimensional Signal Processing. Signal
Processing Series. Prentice Hall, Englewood Cliﬀs, NJ.
Duin, R. P. W. (). On the choice of smoothing parameters for Parzen estimators of
probability density functions. Computers, IEEE Transactions on, C-(), –.
Dyer, R., Zhang, R. H., Möller, T., and Clements, A. (). An investigation of the spectral
robustness of mesh Laplacians. Technical report, SFU CS School, Vancouver, Canada.
Edfors, O., Sandell, M., van de Beek, J., Wilson, S., and Börjesson, P. O. (). Analysis of
DFT-based channel estimators for OFDM. Research report TULEA :, Div Sig
Proc, Luleå University of Technology, Sweden.
Efron, B. and Tibshirani, R. J. (). An Introduction to the Bootstrap. Chapman & Hall,
New York.
Eiwen, D., Taubock, G., Hlawatsch, F., and Feichtinger, H. G. (a). Group sparsity
methods for compressive channel estimation in doubly dispersive multicarrier systems.
In IEEE th International Workshop on Signal Processing Advances in Wireless
Communications (SPAWC), pages –. IEEE Press, Piscataway, NJ.
Eiwen, D., Taubock, G., Hlawatsch, F., Rauhut, H., and Czink, N. (b).
Multichannel-compressive estimation of doubly selective channels in MIMO–OFDM
systems: exploiting and enhancing joint sparsity. In IEEE International Conference
on Acoustics Speech and Signal Processing, pages –. IEEE Press, Piscataway,
NJ.
Eldar, Y. C. and Oppenheim, A. V. (). Filterbank reconstruction of bandlimited signals
from nonuniform and generalized samples. IEEE Transactions on Signal Processing,
(), –.
Elisseeﬀ, A. and Weston, J. (). Kernel methods for multi-labelled classiﬁcation and
categorical regression problems. In In Advances in Neural Information Processing
Systems , pages –. MIT Press.
Engel, Y., Mannor, S., and Meir, R. (). The kernel recursive least-squares algorithm.
IEEE Transactions on Signal Processing, (), –.
Erceg, V., Hari, K., Smith, M., Baum, D., Soma, P., Greenstein, L., Michelson, D.,
Ghassemzadeh, S., Rustako, A., Roman, R., Sheikh, K., Tappenden, C., Costa, J., Bushue,
C., Sarajedini, A., Scwartz, R., Branlund, D., Kaitz, T., and Trinkwon, D. ().
Channel models for ﬁxed wireless applications, IEEE .a-/.
http://wirelessman.org/tga/docs/a-_.pdf.
Erdogmus, D., Rende, D., Príncipe, J. C., and Wong, T. F. (a). Nonlinear channel
equalization using multilayer perceptrons with information-theoric criterion. In Neural
Networks for Signal Processing XI: Proceedings of the IEEE Signal Processing Society
Workshop (IEEE Cat. No.TH), pages –. IEEE Press, Piscataway, NJ.
Erdogmus, D., Vielva, L., and Príncipe, J. C. (b). Nonparametric estimation and
tracking of the mixing matrix for underdetermined blind source separation. In
Proceedings of the rd International Conference on Independent Component Analysis
and Blind Signal Separation (ICA ), San Diego, CA, USA, pages –, San
Diego, California, USA.

References
601
Eric, M., Bach, F. R., and Harchaoui, Z. (). Testing for homogeneity with kernel Fisher
discriminant analysis. In J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis, editors,
Advances in Neural Information Processing Systems , pages –. Curran
Associates.
Ernst, F. (). Compensating for Quasi-periodic Motion in Robotic Radiosurgery.
Springer.
Espinoza, M., Suykens, J. A., and De Moor, B. (). Imposing symmetry in least squares
support vector machines regression. In Proceedings of the th IEEE Conference on
Decision and Control, pages –, Piscataway, NJ. IEEE Press.
Evgeniou, T., Pontil, M., and Poggio, T. (). Regularization networks and support
vector machines. Advances in Computational Mathematics, (), –.
Farhadi, A. and Tabrizi, M. K. (). Learning to recognize activities from the wrong view
point. In D. Forsyth, P. Torr, and A. Zisserman, editors, Computer Vision – ECCV ,
volume of Lecture Notes in Computer Science, pages –. Springer,
Berlin.
Farid, H. and Simoncelli, E. (). Diﬀerentiation of discrete multi-dimensional signals.
IEEE Transactions on Image Processing, (), –.
Fearnhead, P. (). Exact and eﬃcient bayesian inference for multiple changepoint
problems. Statistics and Computing, (), –.
Feher, K. (). Digital Communications: Satellite/Earth Station Engineering.
Prentice-Hall, Englewood Cliﬀs, NJ.
Feijoo, J., Rojo-Álvarez, J. L., Cid-Sueiro, J., Conde-Pardo, P., and Mata-Vigil-Escalera, J.
(). Modeling link events in high reliability networks with support vector machines.
IEEE Transactions on Reliability, , –.
Ferecatu, M. and Boujemaa, N. (). Interactive remote sensing image retrieval using
active relevance feedback. IEEE Transactions on Geoscience and Remote Sensing, (),
–.
Fernández-Getino, M., Rojo-Álvarez, J. L., Alonso-Atienza, F., and Martínez-Ramón, M.
(). Support vector machines for robust channel estimation in OFDM. IEEE Signal
Processing Letters, , –.
Fernández-Getino García, M. J., Páez-Borrallo, J. M., and Zazo, S. (). DFT-based
channel estimation in D-pilot-symbol-aided OFDM wireless systems. In IEEE VTS
rd Vehicular Technology Conference, Spring . Proceedings (Cat. No.CH),
volume , pages –. IEEE Press, Piscataway, NJ.
Figueiras-Vidal, A., Docampo-Amoedo, D., Casar-Corredera, J., and Artés-Rodríguez, A.
(). Adaptive iterative algorithms for spiky deconvolution. IEEE Transactions on
Acoustics, Speech, and Signal Processing, , –.
Figueiredo, M. A. T. and Nowak, R. D. (). Wavelet-based image estimation: an
empirical Bayes approach using Jeﬀrey’s noninformative prior. IEEE Transactions on
Image Processing, (), –.
Figuera, C., Mora-Jiménez, I., Guerrero-Curieses, A., Rojo-Álvarez, J. L., Everss, E., Wilby,
M., and Ramos-López, J. (). Nonparametric model comparison and uncertainty
evaluation for signal strength indoor location. IEEE Trans. Mobile Computing, (),
–.
Figuera, C., Rojo-Álvarez, J. L., Mora-Jiménez, I., Guerrero-Curieses, A., Wilby, M., and
Ramos-López, J. (). Time–space sampling and mobile device calibration for WiFi
indoor location systems. IEEE Transactions on Mobile Computing, , –.

602
References
Figuera, C., Rojo-Álvarez, J. L., Wilby, M., Mora-Jiménez, I., and Caamaño, A. J. ().
Advanced support vector machines for .indoor location. Signal Processing, (),
–.
Figuera, C., Barquero-Pérez, Ó., Rojo-Álvarez, J. L., Martínez-Ramón, M.,
Guerrero-Curieses, A., and Caamaño, A. J. (). Spectrally adapted Mercer kernels
for support vector nonuniform interpolation. Signal Processing, , –.
Filippone, M., Camastra, F., Masulli, F., and Rovetta, S. (). A survey of kernel and
spectral methods for clustering. Pattern Recognition, (), –.
Fine, S., Scheinberg, K., Cristianini, N., Shawe-Taylor, J., and Williamson, B. ().
Eﬃcient SVM training using low-rank kernel representations. Journal of Machine
Learning Research, , –.
Fisher, R. A. (). The use of multiple measurements in taxonomic problems. Annals of
Eugenics, , –.
Flamary, R., Tuia, D., Labbé, B., Camps-Valls, G., and Rakotomamonjy, A. (). Large
margin ﬁltering. IEEE Transactions on Signal Processing, (), –.
Fletcher, R. (). Practical Methods of Optimization. John Wiley & Sons, Inc. nd
Edition.
Floater, M. S. (). Mean value coordinates. Computer Aided Geometric Design, (),
–.
Franz, M. O. and Schölkopf, B. (). A unifying view of Wiener and Volterra theory and
polynomial kernel regression. Neural Computation, (), –.
Fréchet, M. (). Sur les fonctionelles continues. Annales Scientiﬁques de L’École
Normale Supérieure, , –.
Freund, Y., Seung, H. S., Shamir, E., and Tishby, N. (). Selective sampling using the
query by committee algorithm. Machine Learning, , –.
Friedman, J., Hastie, T., and Tibshirani, R. (). The Elements of Statistical Learning:
Data Mining, Inference, and Prediction. Springer Series in Statistics. Springer, Berlin,
edition.
Friedman, J. H. (). Another approach to polychotomous classiﬁcation. Technical
report, Department of Statistics, Stanford University, Stanford, CA.
Frieß, T.-T. and Harrison, R. F. (). A kernel-based adaline. In Proceedings of the th
European Symposium on Artiﬁcial Neural Networks (ESANN ), Bruges, Belgium,
pages –.
Friston, K. J., Holmes, A. P., Worsley, K. J., Poline, J.-P., Frith, C. D., and Frackowiak, R. S. J.
(). Statistical parametric maps in functional imaging: a general linear approach.
Human Brain Mapping, , –.
Fukumizu, K., Bach, F. R., and Jordan, M. I. (). Kernel dimensionality reduction for
supervised learning. In S. Thrun, L. Saul, and P. Schölkopf, editors, Advances in Neural
Information Processing Systems . MIT Press, Cambridge, MA.
Fukumizu, K., Bach, F. R., and Jordan, M. I. (). Dimensionality reduction for
supervised learning with reproducing kernel Hilbert spaces. Journal of Machine
Learning Research, , –.
Fukumizu, K., Bach, F., and Gretton, A. (). Statistical consistency of kernel canonical
correlation analysis. Journal of Machine Learning Research, , –.
Funk, A., Basili, V., Hochstein, L., and Kepner, J. (). Application of a development time
productivity metric to parallel software development. In SE-HPCS ’: Proceedings of
the Second International Workshop on Software Engineering for High Performance
Computing System Applications, pages –. ACM, New York, NY.

References
603
Gabor, D. (). Theory of communications. Journal of the Institution of Electrical
Engineers (London), , –.
Gangeh, M., Ghodsi, A., and Kamel, M. S. (). Kernelized supervised dictionary
learning. IEEE Transactions on Signal Processing, (), –.
Gao, S., Tsang, I. W.-H., and Chia, L.-T. (). Kernel sparse representation for image
classiﬁcation and face recognition. In K. Daniilidis, P. Maragos, and N. Paragios,
editors, Computer Vision – ECCV , volume of Lecture Notes in Computer
Science, pages –. Springer, Berlin.
Gao, W., Richard, C., Bermudez, J.-C. M., and Huang, J. (). Convex combinations of
kernel adaptive ﬁlters. In IEEE International Workshop on Machine Learning for
Signal Processing (MLSP), pages –. IEEE Press, Piscataway, NJ.
García, A. (). Orthogonal sampling formulas: a uniﬁed approach. SIAM Reviews,
(), –.
García-Allende, P. B., Conde, O. M., Mirapeix, J., Cubillas, A. M., and Lopez-Higuera, J. M.
(). Data processing method applying principal component analysis and spectral
angle mapper for imaging spectroscopic sensors. IEEE Sensors Journal, (),
–.
Gardiner, A. B. (). Identiﬁcation of processes containing single-valued nonlinearities.
International Journal of Control, (), –.
Geladi, P. (). Notes on the history and nature of partial least squares (PLS) modelling.
Journal of Chemometrics, , –.
Gersho, A. and Gray, R. (). Vector Quantization and Signal Compression. Kluwer,
Norwell, MA.
Ghahramani, Z. (). The kin datasets.
Ghosh, M. (). Analysis of the eﬀect of impulse noise on multicarrier and single carrier
QAM systems. IEEE Transactions on Communications, (), –.
Giannakis, G. and Serpedin, E. (). A bibliography on nonlinear system identiﬁcation.
Signal Processing, (), –.
Girolami, M. (a). Mercer kernel-based clustering in feature space. IEEE Transactions
on Neural Nets, (), –.
Girolami, M. (b). Orthogonal series density estimation and the kernel eigenvalue
problem. Neural Computation, (), –.
Girosi, F., Jones, M., and Poggio, T. (). Regularization theory and neural networks
architectures. Neural Computation, (), –.
Goethals, I., Pelckmans, K., Suykens, J. A. K., and Moor, B. D. (a). Identiﬁcation of
MIMO Hammerstein models using least squares support vector machines. Automatica,
(), –.
Goethals, I., Pelckmans, K., Suykens, J. A. K., and Moor, B. D. (b). Subspace
identiﬁcation of Hammerstein systems using least squares support vector machines.
IEEE Transactions on Automatic Control, (), –.
Goldberg, P., Williams, C., and Bishop, C. (). Regression with input-dependent noise: a
Gaussian process treatment. In NIPS ’Proceedings of the Conference on
Advances in Neural Information Processing Systems , pages –. MIT Press,
Cambridge, MA.
Golden, R. M. and Kaiser, J. F. (). Design of wideband sampled-data ﬁlters. The Bell
System Technical Journal, (), –.
Golub, G. H. and Van Loan, C. F. (). Matrix Computations. The Johns Hopkins
University Press.

604
References
Gómez, J. C. and Baeyens, E. (). Subspace-based blind identiﬁcation of IIR Wiener
systems. In Proceedings of the th European Signal Processing Conference (EUSIPCO),
Poznañ, Poland.
Gómez-Chova, L., Fernández-Prieto, D., Calpe, J., Soria, E., Vila-Francés, J., and
Camps-Valls, G. (). Urban monitoring using multitemporal SAR and multispectral
data. Pattern Recognition Letters, (), –.
Gómez-Chova, L., Camps-Valls, G., Calpe, J., Guanter, L., and Moreno, J. ().
Cloud-screening algorithm for ENVISAT/MERIS multispectral images. IEEE
Transactions on Geoscience and Remote Sensing, (), –.
Gómez-Chova, L., Camps-Valls, G., Muñoz-Marí, J., and Calpe-Maravilla, J. ().
Semi-supervised image classiﬁcation with Laplacian support vector machines. IEEE
Geoscience and Remote Sensing Letters, (), –.
Gómez-Chova, L., Camps-Valls, G., Bruzzone, L., and Calpe-Maravilla, J. (). Mean
map kernel methods for semisupervised cloud classiﬁcation. IEEE Transactions on
Geoscience and Remote Sensing, (), –.
Gómez-Chova, L., Jenssen, R., and Camps-Valls, G. (). Kernel entropy component
analysis for remote sensing image clustering. IEEE Geoscience and Remote Sensing
Letters, (), –.
Gómez-Chova, L., Nielsen, A. A., and Camps-Valls, G. (). Explicit signal to noise ratio
in reproducing kernel Hilbert spaces. IEEE Transactions on Neural Networks and
Learning Systems.
Gong, B., Shi, Y., Sha, F., and Grauman, K. (). Geodesic ﬂow kernel for unsupervised
domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages –. IEEE Press, Piscataway, NJ.
Gonnouni, A. E., Martínez-Ramón, M., Rojo-Álvarez, J. L., Camps-Valls, G.,
Figueiras-Vidal, A. R., and Christodoulou, C. G. (). A support vector machine
music algorithm. IEEE Transactions on Antennas and Propagation, (), –.
Gopalan, R., Li, R., and Chellappa, R. (). Domain adaptation for object recognition: an
unsupervised approach. In ICCV ’Proceedings of the International Conference on
Computer Vision, pages –. IEEE Computer Society, Barcelona, Spain.
Gorinevsky, D. and Boyd, S. (). Optimization-based design and implementation of
multidimensional zero-phase iir ﬁlters. IEEE Transactions on Circuits and Systems I:
Regular Papers, (), –.
Goshtasby, A. (). -D and -D Image Registration for Medical, Remote Sensing, and
Industrial Applications, volume . John Wiley & Sons, Inc., Hoboken, NJ.
Grauman, K. and Darrell, T. (). The pyramid match kernel: discriminative
classiﬁcation with sets of image features. In ICCV ’: Proceedings of the Tenth IEEE
International Conference on Computer Vision, pages –. IEEE Computer
Society, Washington, DC.
Greblicki, W. (). Nonparametric approach to Wiener system identiﬁcation. IEEE
Transactions on Circuits and Systems I: Fundamental Theory and Applications, (),
–.
Greblicki, W. (). Nonlinearity recovering in Wiener system driven with correlated
signal. IEEE Transactions on Automatic Control, (), –.
Green, A. A., Berman, M., Switzer, P., and Craig, M. D. (). A transformation for
ordering multispectral data in terms of image quality with implications for noise
removal. IEEE Transactions on Geoscience and Remote Sensing, (), –.

References
605
Gretton, A., Davy, M., Doucet, A., and Rayner, P. J. W. (a). Nonstationary signal
classiﬁcation using support vector machines. In th IEEE Workshop on Statistical
Signal Processing, pages –. IEEE Signal Processing Society, Piscataway, NY.
Gretton, A., Doucet, A., Herbrich, R., Rayner, P., and Schölkopf, B. (b). Support vector
regression for black-box system identiﬁcation. In th IEEE Workshop on Statistical
Signal Processing, pages –. IEEE Signal Processing Society, Piscataway, NY.
Gretton, A., Smola, A., Bousquet, O., Herbrich, R., Belitski, A., Augath, M., Murayama, Y.,
Pauls, J., Schölkopf, B., and Logothetis, N. (a). Kernel constrained covariance for
dependence measurement. In R. G. Cowell and Z. Ghahramani, editors, Proceedings of
the Tenth International Workshop on Artiﬁcial Intelligence and Statistics, pages –.
Society for Artiﬁcial Intelligence and Statistics.
Gretton, A., Herbrich, R., and Hyvärinen, A. (b). Kernel methods for measuring
independence. Journal of Machine Learning Research, , –.
Gretton, A., Bousquet, O., Smola, A., and Schölkopf, B. (c). Measuring statistical
dependence with Hilbert-Schmidt norms. In S. Jain, H. Simon, and E. Tomita, editors,
Algorithmic Learning Theory. ALT , volume of Lecture Notes in Computer
Science, pages –. Springer, Berlin.
Grinspun, E., Desbrun, M., Polthier, K., Schröder, P., and Stern, A. (). Siggraph 
course notes. discrete diﬀerential geometry: An applied introduction, .
http://geometry.caltech.edu/pubs/GSD.pdf.
Gu, B. and Sheng, V. (). A robust regularization path algorithm for 𝜈-support vector
classiﬁcation. IEEE Transactions on Neural Networks and Learning Systems, (),
–.
Gu, B., Wang, J. D., Zheng, G., and Yu, Y. (). Regularization Path for 𝜈-Support Vector
classiﬁcation. IEEE Transactions on Neural Networks and Learning Systems, (),
–.
Gutiérrez, J., Ferri, F., and Malo, J. (). Regularization operators for natural images
based on nonlinear perception models. IEEE Transactions on Image Processing, (),
–.
Gutmann, M. U., Laparra, V., Hyvärinen, A., and Malo, J. (). Spatio-chromatic
adaptation via higher-order canonical correlation analysis of natural images. PLoS ONE,
(), –.
Halevy, A., Norvig, P., and Pereira, F. (). The unreasonable eﬀectiveness of data. IEEE
Intelligent Systems, (), –.
Halmos, P. R. (). Measure Theory. Springer.
Ham, J., Lee, D. D., and Saul, L. K. (). Learning high-dimensional correspondences
from low-dimensional manifolds. In Workshop on The Continuum from Labeled to
Unlabled Data in Machine Learning and Data Mining at Twentieth International
Conference on Machine Learning, pages –.
Ham, J., Lee, D. D., Mika, S., and Scholkopf, B. (). A kernel view of the dimensionality
reduction of manifolds. In ICML ’Proceedings of the Twenty-First International
Conference on Machine Learning, page . ACM, New York, NY.
Ham, J., Lee, D., and Saul, L. (). Semisupervised alignment of manifolds. In th
International Workshop on Artiﬁcial Intelligence and Statistics, pages –.
Haralick, R. M., Sternberg, S. R., and Zhuang, X. (). Image analysis using
mathematical morphology. IEEE Transactions on Pattern Analysis and Machine
Intelligence, (), –.

606
References
Harchaoui, Z., Bach, F., Cappe, O., and Moulines, E. (). Kernel-based methods for
hypothesis testing: a uniﬁed view. IEEE Signal Processing Magazine, (), –.
Hardoon, D. R. and Shawe-Taylor, J. (). Sparse canonical correlation analysis. Machine
Learning, , –.
Harmeling, S., Ziehe, A., Kawanabe, M., and Müller, K.-R. (). Kernel-based nonlinear
blind source separation. Neural Computation, (), –.
Harsanyi, J. C. and Chang, C. I. (). Hyperspectral image classiﬁcation and
dimensionality reduction: an orthogonal subspace projection approach. IEEE
Transactions on Geoscience and Remote Sensing, (), –.
Hassibi, B., Stork, D. G., and Wolﬀ, G. J. (). Optimal brain surgeon and general network
pruning. In IEEE International Conference on Neural Networks, pages –. IEEE.
Hastie, T. and Tibshirani, R. (). Classiﬁcation by pairwise coupling. In M. Jordan,
M. Kearns, and S. Solla, editors, Advances in Neural Information Processing Systems ,
pages –. MIT Press, Cambridge, MA.
Hastie, T., Tibishirani, R., and Friedman, J. (). The Elements of Statistical Learning.
Springer-Verlag, New York, NY.
Hastie, T., Tibshirani, R., and Friedman, J. H. (). The Elements of Statistical Llearning:
Data Mining, Inference, and Prediction. Springer-Verlag, New York, NY, nd edition.
Hayes, M. (). Statistical Digital Signal Processing and Modeling. John Wiley & Sons,
Inc., New York, NY.
Haykin, S. (). Neural Networks – A Comprehensive Foundation. Prentice Hall, nd
edition.
Haykin, S. (). Adaptive Filter Theory. Prentice Hall, th edition.
Hecker, C., van der Meijde, M., van der Werﬀ, H., and van der Meer, F. D. (). Assessing
the inﬂuence of reference spectra on synthetic sam classiﬁcation results. IEEE
Transactions on Geoscience and Remote Sensing, (), –.
Heerde, H. V., Leeﬂang, P., and Wittink, D. (). Semiparametric analysis to estimate the
deal eﬀect curve. Journal of Marketing Research, (), –.
Helms, H. D. (). Fast fourier transform method of computing diﬀerence equations and
simulating ﬁlters. IEEE Transactions on Audio and Electroacoustics, (), –.
Hermans, M. and Schrauwen, B. (). Recurrent kernel machines: computing with
inﬁnite echo state networks. Neural Computation, (), –.
Hertz, D. and Zeheb, E. (). Suﬃcient conditions for stability of multidimensional
discrete systems. Proceedings of the IEEE, (), –.
Hoegaerts, L., Suykens, J., Vandewalle, J., and De Moor, B. (). A comparison of
pruning algorithms for sparse least squares support vector machines. In N. Pal,
N. Kasabov, R. Mudi, S. Pal, and S. Parui, editors, Neural Information Processing.
ICONIP , volume of Lecture Notes in Computer Science, pages –.
Springer.
Hoegaerts, L., Suykens, J. A. K., Vanderwalle, J., and Moor, B. D. (). Subset based least
squares subspace regression in RKHS. Neurocomputing, , –.
Hoerl, A. E. and Kennard, R. W. (). Ridge regression: biased estimation for
nonorthogonal problems. Technometrics, , –.
Hoﬀman, J., Rodner, E., Donahue, J., Saenko, K., and Darrell, T. (). Eﬃcient learning of
domain invariant image representations. In Proceedings of International Conference on
Learning Representations (ICLR ), Scottsdale, Arizona.
Hofmann, T., Schölkopf, B., and Smola, A. J. (). Kernel methods in machine learning.
Annals of Statistics, , –.

References
607
Hotelling, H. (). Relations between two sets of variates. Biometrika, (/), –.
Hsu, C.-W. and Lin, C.-J. (). A comparison of methods for multiclass support vector
machines. IEEE Transactions on Neural Networks, (), –.
Hu, Y. D., Pan, J. C., and Tan, X. (). High-dimensional data dimension reduction based
on KECA. Applied Mechanics and Materials, –, –.
Huang, B. and Wang, Y. (). QRS complexes detection by using the principal
component analysis and the combined wavelet entropy for -lead electrocardiogram
signals. In CIT. Ninth IEEE International Conference on Computer and Information
Technology, ., volume , pages –. IEEE Press, Piscataway, NJ.
Huang, J., Smola, A. J., Gretton, A., Borgwardt, K. M., and Scholkopf, B. (). Correcting
sample selection bias by unlabeled data. In B. Scholköpf, J. Platt, and T. Hoﬀman,
editors, Advances in Neural Information Processing Systems , pages –. MIT
Press, Cambridge, MA.
Hughes, G. F. (). On the mean accuracy of statistical pattern recognizers. IEEE
Transactions on Information Theory, (), –.
Hundley, D. R., Kirby, M. J., and Anderle, M. (). Blind source separation using the
maximum signal fraction approach. Signal Processing, (), –.
Hunter, I. W. and Korenberg, M. J. (). The identiﬁcation of nonlinear biological
systems: Wiener and Hammerstein cascade models. Biological Cybernetics, (),
–.
Hyvärinen, A. and Oja, E. (). A fast ﬁxed-point algorithm for independent component
analysis. Neural Computation, (), –.
Hyvärinen, A., Karhunen, J., and Oja, E. (). Independent Component Analysis. Wiley
Interscience.
Ishida, T. and Tanaka, T. (). Multikernel adaptive ﬁlters with multiple dictionaries and
regularization. In Asia-Paciﬁc Signal and Information Processing Association
Annual Summit and Conference (APSIPA), pages –. IEEE Press,
Piscataway, NJ.
Izquierdo-Verdiguier, E., Arenas-García, J., Muñoz-Romero, S., Gómez-Chova, L., and
Camps-Valls, G. (). Semisupervised kernel orthonormalized partial least squares.
In IEEE International Workshop on Machine Learning for Signal Processing. IEEE
Press, Piscataway, NJ.
Izquierdo-Verdiguier, E., Laparra, V., Jenssen, R., Gómez-Chova, L., and Camps-Valls, G.
(). Optimized kernel entropy components. IEEE Transactions on Neural Networks
and Learning Systems, (), –.
Jaakkola, T. and Haussler, D. (). Exploiting generative models in discriminative
classiﬁers. In M. Kearns, S. Solla, and D. Cohn, editors, Advances in Neural Information
Processing Systems , pages –. MIT Press, Cambridge, MA.
Jaakkola, T., Diekhans, M., and Haussler, D. (). Using the Fisher kernel method to
detect remote protein homologies. In Proceedings of the Seventh International
Conference on Intelligent Systems for Molecular Biology, pages –.
AAAI Press.
Jackson, J. I., Meyer, C. H., Nishimura, D. G., and Macovski, A. (). Selection of a
convolution function for Fourier inversion using gridding. IEEE Transactions on
Medical Imaging, (), –.
Jackson, Q. and Landgrebe, D. (). An adaptive classiﬁer design for high-dimensional
data analysis with a limited training data set. IEEE Transactions on Geoscience and
Remote Sensing, (), –.

608
References
Jenssen, R. (). Information theoretic learning and kernel methods. In
F. Emmert-Streib and M. Dehmer, editors, Information Theory and Statistical Learning,
pages –. Springer US.
Jenssen, R. (). Kernel entropy component analysis. IEEE Transactions on Pattern
Analysis and Machine Intelligence, (), –.
Jenssen, R. (). Entropy-relevant dimensions in the kernel feature space:
cluster-capturing dimensionality reduction. IEEE Signal Processing Magazine, (),
–.
Jerri, A. J. (). The Shannon sampling theorem – its various extensions and
applications: a tutorial review. Proceedings of the IEEE, (), –.
Jiang, Q., Yan, X., Lv, Z., and Guo, M. (). Fault detection in nonlinear chemical
processes based on kernel entropy component analysis and an angular structure.
Korean Journal of Chemical Engineering, (), .
Joachims, T. (a). Making large-scale support vector machine learning practical. In
B. Schölkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods: Support
Vector Machines, pages –. MIT Press, Cambridge, MA.
Joachims, T. (b). Text categorization with suport vector machines: learning with many
relevant features. In ECML ’Proceedings of the th European Conference on
Machine Learning, pages –, London, UK, UK. Springer-Verlag.
Joachims, T., Galor, T., and Elber, R. (). Learning to align sequences: a
maximum-margin approach. In B. Leimkuhler, C. Chipot, R. Elber, A. Laaksonen,
A. Mark, T. Schlick, C. Schütte, and R. Skeel, editors, New Algorithms for
Macromolecular Simulation, volume of Lecture Notes in Computational Science and
Engineering, pages –. Springer.
Joachims, T., Finley, T., and Yu, C.-N. J. (). Cutting-plane training of structural SVMs.
Machine Learning, , .
Joho, M., Mathis, H., and Lambert, R. (). Overdetermined blind source separation:
using more sensors than source signals in a noisy mixture. In Proceedings of the nd
International Conference on Independent Component Analysis and Blind Signal
Separation (ICA ), pages –, Helsinki, Finland.
Jolliﬀe, I. T. (). Principal Component Analysis. Springer-Verlag, Berlin.
Julier, S. J. and Uhlmann, J. K. (). A general method for approximating nonlinear
transformations of probability distributions. Technical report, Robotics Research
Group, Department of Engineering Science, University of Oxford.
Jun, G. and Ghosh, J. (). An eﬃcient active learning algorithm with knowledge
transfer for hyperspectral remote sensing data. In IGARSS – IEEE
International Geoscience and Remote Sensing Symposium. IEEE Press, Piscataway, NJ.
Juneja, R. (). Radiofrequency ablation for cardiac tachyarrhythmias: principles and
utility of D mapping systems. Current Science, (), –.
Kaiser, J. (). Design methods for sampled data ﬁlters. In L. Rabiner and C. Rader,
editors, Digital Signal Processing, pages –. IEEE Press, New York, NY. Reprinted
from Proceedings of the First Annual Allerton Conference on Circuit Systems Theory,
.
Kakazu, G. and Munson, Jr, D. (). A frequency-domain characterization of
interpolation from nonuniformly spaced data. In IEEE International Symposium on
Circuits and Systems, pages –. IEEE Press, Piscataway, NJ.

References
609
Kalman, R. E. (). A new approach to linear ﬁltering and prediction problems. Journal
of Basic Engineering, (), –.
Kalouptsidis, N. and Theodoridis, S. (). Adaptive System Identiﬁcation and Signal
Processing Algorithms. Prentice-Hall, Englewood Cliﬀs, NJ.
Kanamori, T., Hido, S., and Sugiyama, M. (). A least-squares approach to direct
importance estimation. Journal of Machine Learning Research, , –.
Kassam, S. A. and Poor, H. V. (). Robust techniques for signal processing: a survey.
Proceedings of the IEEE, (), –.
Kawakami Harrop Galvão, R., Hadjiloucas, S., Izhac, A., Becerra, V. M., and Bowen, J. W.
(). Wiener-system subspace identiﬁcation for mobile wireless mm-wave networks.
IEEE Transactions on Vehicular Technology, (), –.
Kay, S. and Marple, Jr., S. (). Spectrum analysis – a modern perspective. Proceedings of
the IEEE, (), –.
Kay, S. M. (). Fundamentals of Statistical Signal Processing: Detection Theory,
volume . Prentice Hall, Upper Saddle River, NJ.
Kechriotis, G., Zarvas, E., and Manolakos, E. S. (). Using recurrent neural networks
for adaptive communication channel equalization. IEEE Transactions on Neural
Networks, , –.
Kersting, K., Plagemann, C., Pfaﬀ, P., and Burgard, W. (). Most likely heteroscedastic
Gaussian processes regression. In ICML ’Proceedings of the th International
Conference on Machine Learning, pages –.
Keshava, N. (). Distance metrics and band selection in hyperspectral processing with
applications to material identiﬁcation and spectral libraries. IEEE Transactions on
Geoscience and Remote Sensing, (), –.
Kim, K. I., Franz, M. O., and Scholkopf, B. (). Iterative kernel principal component
analysis for image modeling. IEEE Transactions on Pattern Analysis and Machine
Intelligence, (), –.
Kimeldorf, G. and Wahba, G. (). Some results on Tchebycheﬃan spline functions.
Journal of Mathematical Analysis and Applications, (), –.
Kirkwood, J. (). Quantum statistics of almost classical ensembles. Physical Review, ,
–.
Kivinen, J., Smola, A. J., and Williamson, R. C. (). Online learning with kernels. IEEE
Transactions on Signal Processing, (), –.
Kohonen, T. (). The self-organizing map. Proceedings of the IEEE, (), –.
Koller, D. and Friedman, N. (). Probabilistic Graphical Models: Principles and
Techniques. MIT Press.
Korenberg, M. J. (). A robust orthogonal algorithm for system identiﬁcation and
time-series analysis. Biological Cybernetics, (), –.
Kormylo, J. and Mendel, J. M. (). Maximum likelihood detection and estimation of
Bernoulli–Gaussian processes. IEEE Transactions on Information Theory, , –.
Kormylo, J. J. and Mendel, J. M. (). Maximum-likelihood seismic deconvolution. IEEE
Transactions on Geoscience and Remote Sensing, GE-(), –.
Kramer, N. and Rosipal, R. (). Overview and recent advances in partial least squares.
In C. Saunders, M. Grobelnik, S. Gunn, and J. Shawe-Taylor, editors, Subspace, Latent
Structure and Feature Selection Techniques, volume of Lecture Notes in Computer
Science, pages –. Springer, Berlin.

610
References
Kreutz-Delgado, K., Murray, J. F., Rao, B. D., Engan, K., Lee, T.-W., and Sejnowski, T. J.
(). Dictionary learning algorithms for sparse representation. Neural Computation,
(), –.
Kruse, F. A., Lefkoﬀ, A. B., Boardman, J. W., Heidebrecht, K. B. Shapiro, A. T., Barloon, P. J.,
and Goetz, A. F. H. (). The spectral image processing system (SIPS) – interactive
visualization and analysis of imaging spectrometer data. Remote Sensing of
Environment, (–), –.
Kuo, J. M. and Principe, J. (). Noise reduction in state space using the focused gamma
model. In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal
Processing, volume , pages –. IEEE Press, Piscataway, NJ.
Kuo, J. M., Celebi, S., and Principe, J. (). Adaptation of memory depth in the gamma
ﬁlter. In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal
Processing, volume , pages –. IEEE Press, Piscataway, NJ.
Kuss, M. and Rasmussen, C. (). Assessing approximate inference for binary Gaussian
process classiﬁcation. Machine LearningResearch, , –.
Kwok, J. T. (). The evidence framework applied to support vector machines. IEEE
Transactions in Neural Networks, (), –.
Kwok, J. T. (). Linear dependency between 𝜀and the input noise in 𝜀-support vector
regression. In G. Dorﬀner, H. Bischof, and K. Hornik, editors, Artiﬁcial Neural
Networks – ICANN , volume of Lecture Notes in Computer Science, pages
–. Springer, Berlin.
Kwon, H. and Nasrabadi, N. (). A comparative analysis of kernel subspace target
detectors for hyperspectral imagery. EURASIP Journal of Advances in Signal Processing,
, .
Kwon, H. and Nasrabadi, N. M. (a). Kernel matched signal detectors for hyperspectral
target detection. In IEEE Computer Society Conference on Computer Vision and
Pattern Recognition (CVPR’) – Workshops, page . IEEE Press, Piscataway, NJ.
Kwon, H. and Nasrabadi, N. M. (b). Kernel orthogonal subspace projection for
hyperspectral signal classiﬁcation. IEEE Transactions on Geoscience and Remote
Sensing, (), –.
Kwon, H. and Nasrabadi, N. M. (c). Kernel RX-algorithm: a nonlinear anomaly
detector for hyperspectral imagery. IEEE Transactions on Geoscience and Remote
Sensing, (), –.
Kwon, H., Der, S. Z., and Nasrabadi, N. M. (). Adaptive anomaly detection using
subspace separation for hyperspectral imagery. Optical Engineering, (), –.
Laguna, P., Moody, G., and Mark, R. (). Power spectral density of unevenly sampled
data by least-square analysis: performance and application to heart rate signals. IEEE
Transactions on Biomedical Engineering, , –.
Lai, P. L. and Fyfe, C. (a). Kernel and non-linear canonical correlation analysis.
International Journal of Neural Systems, , –.
Lai, P. L. and Fyfe, C. (b). Kernel and nonlinear canonical correlation analysis. In
Proceedings of the IEEE–INNS–ENNS International Joint Conference on Neural
Networks. IJCNN . Neural Computing: New Challenges and Perspectives for the
New Millennium, pages –. IEEE Press, Piscataway, NJ.
Lal, T. N., Schroder, M., Hinterberger, T., Weston, J., Bogdan, M., Birbaumer, N., and
Scholkopf, B. (). Support vector channel selection in BCI. IEEE Transactions on
Biomedical Engineering, (), –.

References
611
Laparra, V., Gutiérrez, J., Camps-Valls, G., and Malo, J. (). Image denoising with
kernels based on natural image relations. Journal of Machine Learning Research, ,
–.
Laparra, V., Jiménez, S., Tuia, D., Camps-Valls, G., and Malo, J. (). Principal
polynomial analysis. International Journal of Neural Systems, (), .
Laparra, V., Malo, J., and Camps-Valls, G. (). Dimensionality reduction via regression
in hyperspectral imagery. IEEE Journal on Selected Topics in Signal Processing, (),
–.
Larocque, J. R. and Reilly, P. (). Reversible jump MCMC for joint detection and
estimation of sources in colored noise. IEEE Transactions on Signal Processing, (),
–.
Lawrence, N. (). Probabilistic non-linear principal component analysis with Gaussian
process latent variable models. Machine Learning Research, , –.
Lázaro-Gredilla, M. (). Bayesian warped Gaussian processes. In F. Pereira, C. Burges,
L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing
Systems , pages –. Curran Associates.
Lázaro-Gredilla, M. and Titsias, M. K. (). Variational heteroscedastic Gaussian process
regression. In ICML’Proceedings of the th International Conference on Machine
Learning, pages –, Madison, WI. Omnipress.
Lázaro-Gredilla, M., Candela, J. Q., Rasmussen, C. E., and Figueiras-Vidal, A. R. ().
Sparse spectrum Gaussian process regression. Journal of Machine Learning Research,
, –.
Lázaro-Gredilla, M., Van Vaerenbergh, S., and Santamaría, I. (). A Bayesian approach
to tracking with kernel recursive least-squares. In IEEE International Workshop on
Machine Learning for Signal Processing, pages –. IEEE Press, Piscataway, NJ.
Lázaro-Gredilla, M., Titsias, M. K., Verrelst, J., and Camps-Valls, G. (). Retrieval of
biophysical parameters with heteroscedastic gaussian processes. IEEE Geoscience and
Remote Sensing Letters, (), –.
Lazebnik, S., Schmid, C., and Ponce, J. (). Beyond bags of features: spatial pyramid
matching for recognizing natural scene categories. In CVPR ’: Proceedings of the
IEEE Computer Society Conference on Computer Vision and Pattern Recognition,
pages –. IEEE Computer Society, Washington, DC.
LeCun, Y., Denker, J. S., Solla, S. A., Howard, R. E., and Jackel, L. D. (). Optimal brain
damage. In D. Touretzky, editor, Advances in Neural Information Processing Systems ,
pages –. Morgan Kaufmann, San Francisco, CA.
Lee, J., McManus, D. D., Merchant, S., and Chon, K. H. (). Automatic motion and
noise artifact detection in Holter ECG data using empirical mode decomposition and
statistical approaches. IEEE Transactions on Biomedical Engineering, (),
–.
Lee, T. W., Lewicki, M. S., Girolami, M., and Sejnowski, T. J. (). Blind source
separation of more sources than mixtures using overcomplete representations. IEEE
Signal Processing Letters, , –.
Lee, Y. and Nelder, J. A. (). Hierarchical generalized linear models. Journal of the
Royal Statistical Society. Series B (Methodological), (), –.
Lee, Y., Lin, Y., and Wahba, G. (). Multicategory support vector machines: theory and
application to the classiﬁcation of microarray data and satellite radiance data. Journal of
the American Statistical Association, (), –.

612
References
Lévy, B. (). Laplace–Beltrami eigenfunctions towards an algorithm that “understands”
geometry. In SMI ’Proceedings of the IEEE International Conference on Shape
Modeling and Applications , page . IEEE Computer Society, Washington, DC.
Lewis, F. L., Xie, L., and Popa, D., editors (). Optimal and Robust Estimation: With an
Introduction to Stochastic Control Theory, volume of Automation and Control
Engineering. CRC Press.
Li, K. and Príncipe, J. C. (). The kernel adaptive autoregressive–moving-average
algorithm. IEEE Transactions on Neural Networks and Learning Systems, (),
–.
Li, T., Zhu, S., and Ogihara, M. (). Using discriminant analysis for multi-class
classiﬁcation: an experimental investigation. Knowledge and Information Systems, (),
–.
Lid Hjort, N., Holmes, C., Muller, P., and Walker, S. (). Bayesian Nonparametrics.
Cambridge University Press, New York, NY.
Lin, Y., Lee, Y., and Wahba, G. (). Support vector machines for classiﬁcation in
nonstandard situations. Department of Statistics TR , University of
Wisconsin-Madison.
Liu, B., Dai, Y., Li, X., Lee, W. S., and Yu, P. S. (). Building text classiﬁers using positive
and unlabeled examples. In ICDM ’Proceedings of the Third IEEE International
Conference on Data Mining, page , Washington, DC. IEEE Computer Society.
Liu, J. S. (). Monte Carlo Strategies in Scientiﬁc Computing. Springer.
Liu, W., Pokharel, P. P., and Príncipe, J. C. (). The kernel least-mean-square algorithm.
IEEE Transactions on Signal Processing, (), –.
Liu, W., Park, I., Wang, Y., and Príncipe, J. C. (). Extended kernel recursive least
squares algorithm. IEEE Transactions on Signal Processing, (), –.
Liu, W., Príncipe, J. C., and Haykin, S. (). Kernel Adaptive Filtering: A Comprehensive
Introduction. John Wiley & Sons.
Ljung, L. (). System Identiﬁcation: Theory for the User. Prentice-Hall, Upper Saddle
River, NJ.
Ljung, L. (). Perspectives on system identiﬁcation. In Plenary talk at the Proceedings
of the th IFAC World Congress, Seoul, South Korea.
Lomb, N. (). Least-squares frequency analysis of unequally spaced data. Astrophysics
and Space Science, , –.
Longbotham, N. and Camps-Valls, G. (). A family of kernel anomaly change detectors.
In IEEE Workshop on Hyperspectral Image and Signal Processing, Whispers, Lausanne,
Switzerland.
López-Paz, D., Sra, S., Smola, A. J., Ghahramani, Z., and Schölkopf, B. (). Randomized
nonlinear component analysis. In ICML’Proceedings of the st International
Conference on International Conference on Machine Learning, volume , pages
II-–II-. JMLR.org.
Lowe, D. G. (). Object recognition from local scale-invariant features. In The
Proceedings of the Seventh IEEE International Conference on Computer Vision, ,
volume , pages –. IEEE Computer Society, Washington, DC.
Luengo, D., Santamaría, I., and Vielva, L. (). A general solution to blind inverse
problems for sparse input signals. Neurocomputing, (–), –.
Luo, T., Kramer, K., Golgof, D. B., Hall, L. O., Samson, S., Remsen, A., and Hopkins, T.
(). Active learning to recognize multiple types of plankton. J. Mach. Learn. Res., ,
–.

References
613
Luo, X. Q. and Wu, X. J. (). Fusing remote sensing images using a statistical model.
Applied Mechanics and Materials, –, –.
Luo, X. Q., Wu, X. J., and Zhang, Z. (). Regional and entropy component analysis
based remote sensing images fusion. Journal of Intelligent and Fuzzy Systems, (),
–.
MacKay, D. J. C. (). Information based objective functions for active data selection.
Neural Computation, (), –.
Madanayake, A., Wijenayake, C., Dansereau, D. G., Gunaratne, T. K., Bruton, L. T., and
Williams, S. B. (). Multidimensional (MD) circuits and systems for emerging
applications including cognitive radio, radio astronomy, robot vision and imaging. IEEE
Circuits and Systems Magazine, (), –.
Madanayake, A., Wijenayake, C., Lin, Z., and Dornback, N. (). Recent advances in
multidimensional systems and signal processing: an overview. In IEEE
International Symposium on Circuits and Systems (ISCAS), pages –.
Mallat, S. and Zhang, Z. (). Matching pursuit with time-frequency dictionaries. IEEE
Transactions on Signal Processing, , –.
Margenau, H. and Hill, R. N. (). Correlation between measurements in quantum
theory. Progress in Theoretical Physics, , –.
Maritorena, S. and O’Reilly, J. (). OCv: update on the unitial operational SeaWiFS
chlorophyll a algorithm. In S. Hooker and E. Firestone, editors, SeaWiFS Postlaunch
Calibration and Validation Analyses, volume of NASA Technical Memorandum
–, pages –. John Wiley & Sons.
Marple, S. (). Digital Spectral Analysis with Applications. Prentice-Hall, Englewood
Cliﬀs, NJ.
Martínez-Ramón, M., Xu, N., and Christodoulou, C. (). Beamforming using support
vector machines. IEEE Antennas and Wireless Propagation Letters, , –.
Martínez-Ramón, M., Rojo-Álvarez, J. L., Camps-Valls, G., Navia-Vázquez, A.,
Muñoz-Marí, J., Soria-Olivas, E., and Figueiras-Vidal, A. (). Support vector
machines for nonlinear kernel ARMA system identiﬁcation. IEEE Transactions on
Neural Networks, , –.
Martínez-Ramón, M., Rojo-Álvarez, J. L., Camps-Valls, G., and Christodoulou, C. ().
Kernel antenna array processing. IEEE Transactions on Antennas and Propagation,
(), –.
Martino, L., Elvira, V., Luengo, D., and Corander, J. (a). An Adaptive Population
Importance Sampler: Learning from the uncertanity. IEEE Transactions on Signal
Processing, (), –.
Martino, L., Yang, H., Luengo, D., Kanniainen, J., and Corander, J. (b). The FUSS
algorithm: a fast universal self-tuned sampler within Gibbs. Digital Signal Processing,
, –.
Martino, L., Elvira, V., Luengo, D., Corander, J., and Louzada, F. (). Orthogonal parallel
MCMC methods for sampling and optimization. Digital Signal Processing, , –.
Martino, L., Elvira, V., Luengo, D., and Corander, J. (). Layered adaptive importance
sampling. Statistics and Computing, (), –.
Mattera, D. (). Support vector machines for signal processing. In L. Wang, editor,
Support Vector Machines: Theory and Applications, volume of Studies in Fuzziness
and Soft Computing, pages –. Springer, Berlin.
Mayer, R., Bucholtz, F., and Scribner, D. (). Object detection by using
“whitening/dewhitening” to transform target signatures in multitemporal hyperspectral

614
References
and multispectral imagery. IEEE Transactions on Geoscience and Remote Sensing, (),
–.
McCallum, A. K. (). Multi-label text classiﬁcation with a mixture model trained by
EM. In AAAI-Workshop on Text Learning.
Mehrotra, S. (). On the implementation of a primal-dual interior point method. SIAM
Journal on Optimization, (), –.
Meigering, E. (). A chronology of interpolation: from ancient astronomy to modern
signal and image processing. Proceedings of the IEEE, (), –.
Mendel, J. and Burrus, C. S. (). Maximum-Likelihood Deconvolution: A Journey into
Model-Based Signal Processing. Springer-Verlag, New York, NY.
Mendel, J. M. (). Some modeling problems in reﬂection seismology. IEEE ASSP
Magazine, (), –.
Meyer, M., Desbrun, M., Schröder, P., and Barr, A. H. (). Discrete
diﬀerential-geometry operators for triangulated -manifolds. In H.-C. Hege and
K. Polthier, editors, Visualization and Mathematics III, pages –. Springer.
Mika, S., Raetsch, G., Weston, J., Schölkopf, B., and Müller, K.-R. (). Fisher
discriminant analysis with kernels. In Neural Networks for Signal Processing IX:
Proceedings of the IEEE Signal Processing Society Workshop (Cat. No.TH),
pages –. IEEE Press, Piscataway, NJ.
Mika, S., Rätsch, G., Weston, J., Schölkopf, B., Smola, A., and Müller, K. ().
Constructing descriptive and discriminative nonlinear features: Rayleigh coeﬃcients in
kernel feature spaces. IEEE Transactions on Pattern Analysis and Machine Intelligence,
, –.
Molgedey, L. and Schuster, H. G. (). Separation of a mixture of independent signals
using time delayed correlations. Physical Review Letters, , –.
Momma, M. and Bennett, K. (). Sparse kernel partial least squares regression. In
B. Schölkopf and M. Warmuth, editors, Learning Theory and Kernel Machines, volume
of Lecture Notes in Computer Science, pages –. Springer, Berlin.
Morady, F. (). Radio-frequency ablation as treatment for cardiac arrhythmias. New
England Journal of Medicine, (), –.
Mouattamid, M. and Schaback, R. (). Recursive kernels. Analysis in Theory and
Applications, , –.
Muandet, K. and Schölkopf, B. (). One-class support measure machines for group
anomaly detection. In A. Nicholson and P. Smyth, editors, UAI’Proceedings of the
Twenty-Ninth Conference on Uncertainty in Artiﬁcial Intelligence, pages –.
AUAI Press, Arlington, VA.
Mukherjee, S., Osuna, E., and Girosi, F. (). Nonlinear prediction of chaotic time series
using a support vector machine. In J. Principe, L. Gile, N. Morgan, and E. Wilson,
editors, Neural Networks for Signal Processing VII. Proceedings of the IEEE Signal
Processing Society Workshop, pages –, New York. IEEE Press.
Murphy, K. (). Machine Learning. A Probabilistic Perspective. MIT Press, Cambridge,
MA.
Muslea, I. (). Active learning with multiple views. Journal of Artiﬁcial Intelligence
Research, , –.
Myhre, J. and Jenssen, R. (). Mixture weight inﬂuence on kernel entropy component
analysis and semi-supervised learning using the LASSO. In IEEE International
Workshop in Machine Learning for Signal Processing, pages –, Piscataway, NJ. IEEE
Press.

References
615
Narendra, K. and Gallman, P. (). An iterative method for the identiﬁcation of
nonlinear systems using a Hammerstein model. IEEE Transactions on Automatic
Control, (), –.
Narendra, K. S. and Parthasarathy, K. (). Identiﬁcation and control of dynamical
systems using neural networks. IEEE Transactions of Neural Networks, (), –.
Nasrabadi, N. M. (). Regularization for spectral matched ﬁlter and RX anomaly
detector. In S. Shen and P. Lewis, editors, Algorithms and Technologies for Multispectral,
Hyperspectral, and Ultraspectral Imagery XIV, volume of Proceedings of SPIE,
page . SPIE Press, Bellingham, WA.
Navia-Vázquez, A., Pérez-Cruz, F., Artés-Rodríguez, A., and Figueiras-Vidal, A. R. ().
Weighted least squares training of support vector classiﬁers leading to compact and
adaptive schemes. IEEE Transactions on Neural Networks, (), –.
Naylor, P. A., Cui, J., and Brookes, M. (). Adaptive algorithms for sparse echo
cancellation. Signal Processing, (), –.
Nelles, O. (). Nonlinear System Identiﬁcation. Springer-Verlag, Berlin.
Ng, A., Jordan, M., and Weiss, Y. (). On spectral clustering: analysis and an algorithm.
In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural
Information Processing Systems , pages –. MIT Press, Cambridge, MA.
Ngia, K. S. H. and Sjobert, J. (). Nonlinear acoustic echo cancellation using a
Hammerstein model. In Proceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing, , volume , pages –. IEEE Press,
Piscataway, NJ.
Nguyen, H. V., Patel, V. M., Nasrabadi, N. M., and Chellappa, R. (). Kernel dictionary
learning. In IEEE International Conference on Acoustics, Speech and Signal
Processing, pages –. IEEE Press, Piscataway, NJ.
Nielsen, A. A. (). Kernel maximum autocorrelation factor and minimum noise
fraction transformations. IEEE Transactions on Image Processing, (), –.
Nielsen, A. A., Conradsen, K., and Simpson, J. J. (). Multivariate alteration detection
(MAD) and MAF post-processing in multispectral bi-temporal image data: new
approaches to change detection studies. Remote Sensing of Environment, (), –.
Nikolaev, N. and Tino, P. (). Sequential relevance vector machine learning from time
series. In Proceedings of International Joint Conference on Neural Networks, volume ,
pages –. IEE Press, Montreal, Canada.
O’Brien, S., Sinclair, A., and Kramer, S. (). Recovery of a sparse spike time series by L
norm deconvolution. IEEE Transactions on Signal Processing, , –.
Ogunfunmi, T. and Paul, T. (). On the complex kernel-based adaptive ﬁlter. In 
IEEE International Symposium on Circuits and Systems (ISCAS), pages –.
IEEE Press, Piscataway, NJ.
O’Hagan, A. and Kingman, J. F. C. (). Curve ﬁtting and optimal design for prediction.
Journal of the Royal Statistical Society. Series B (Methodological), (), –.
Ohm, J.-R. (). Advances in scalable video coding. Proceedings of the IEEE, (),
–.
Olofsson, T. (). Semi-sparse deconvolution robust to uncertainties in the impulse
responses. Ultrasonics, , –.
Olshausen, B. A. and Fieldt, D. J. (). Sparse coding with an overcomplete basis set: a
strategy employed by V? Vision Research, , –.
Olshausen, B. A., Sallee, P., and Lewicki, M. S. (). Learning sparse multiscale image
representations. Advances in Neural Information Processing Systems, , –.

616
References
Oppenheim, A. V. and Schafer, R. W. (). Discrete-Time Signal Processing. Prentice
Hall, Englewood Cliﬀs, NJ.
O’Reilly, J. E. and Maritorena, S. (). SeaBAM evaluation data set. In The SeaWiFS
Bio-Optical Algorithm Mini-Workshop (SeaBAM). https://seabass.gsfc.nasa.gov/
seabam/pub/maritorena_oreilly_schieber/SBAMset.ps (accessed July , ).
O’Reilly, J. E., Maritorena, S., Mitchell, B. G., Siegel, D. A., Carder, K., Garver, S. A., Kahru,
M., and McClain, C. (). Ocean color chlorophyll algorithms for SeaWiFS. Journal
of Geophysical Research, (C), –.
Orr, G. B. and Müller, K.-R. (). Neural Networks: Tricks of the Trade. Springer-Verlag,
Berlin.
Osuna, E., Freund, R., and Girosi, F. (). Improved training algorithm for support
vector machines. In Neural Networks for Signal Processing VII. Proceedings of the 
IEEE Signal Processing Society Workshop, pages –. IEEE Press, Piscataway, NJ.
Page, C. (). Instantaneous power spectra. Journal of Applied Physics, ,
–.
Pajunen, G. A. (). Adaptive control of Wiener type nonlinear systems. Automatica,
(), –.
Pal, N. R., Pal, K., Keller, J. M., and Bezdek, J. C. (). A possibilistic fuzzy c-means
clustering algorithm. IEEE Transactions on Fuzzy Systems, (), –.
Palkar, M. and Principe, J. (). Echo cancellation with the gamma ﬁlter. In IEEE
International Conference on Acoustics, Speech, and Signal Processing, volume , pages
III/–III/. IEEE Press, Piscataway, NJ.
Pan, S. J. and Yang, Q. (). A survey on transfer learning. IEEE Transactions on
Knowledge and Data Engineering, (), –.
Pan, S. J. and Yang, Q. (). Domain adaptation via transfer component analysis. IEEE
Transactions on Neural Networks, , –.
Pan, S. J., Kwok, J. T., and Yang, Q. (). Transfer learning via dimensionality reduction.
In AAAI’Proceedings of the rd National Conference on Artiﬁcial Intelligence,
volume , pages –. AAAI Press.
Papoulis, A. (). Probability Random Variables and Stochastic Processes. McGraw-Hill,
New York, NY, edition.
Park, I. M., Seth, S., and Van Vaerenbergh, S. (). Probabilistic kernel least mean
squares algorithms. In IEEE International Conference on Acoustics, Speech and
Signal Processing, pages –. IEEE Press, Piscataway, NJ.
Parker, S. and Girard, P. (). Correlated noise due to roundoﬀin ﬁxed point digital
ﬁlters. IEEE Transactions on Circuits and Systems, (), –.
Pasolli, E. and Melgani, F. (). Model-based active learning for SVM classiﬁcation of
remote sensing images. In IEEE International Geoscience and Remote Sensing
Symposium, IGARSS, Hawaii, USA.
Pavlidis, P., Weston, J., Cai, J., and Grundy, W. N. (). Gene functional classiﬁcation
from heterogeneous data. In RECOMB ’Proceedings of the Fifth Annual International
Conference on Computational Biology, pages –. ACM, New York, NY.
Pavlov, A., van de Wouw, N., and Nijmeijer, H. (). Frequency response functions for
nonlinear convergent systems. IEEE Transactions on Automatic Control, (),
–.
Pawlak, M., Hasiewicz, Z., and Wachel, P. (). On nonparametric identiﬁcation of
Wiener systems. IEEE Transactions on Signal Processing, (), –.

References
617
Pearson, K. (). On lines and planes of closest ﬁt to systems of points in space.
Philosophical Magazine, , –.
Pérez-Cruz, F. and Artés-Rodríguez, A. (). A new optimizing procedure for 𝜈-support
vector regressor. In IEEE International Conference on Acoustics, Speech, and
Signal Processing. Proceedings (Cat. No.CH), volume , pages –. IEEE
Press, Piscataway, NJ.
Pérez-Cruz, F. and Bousquet, O. (). Kernel methods and their potential use in signal
processing. IEEE Signal Processing Magazine, (), –.
Pérez-Cruz, F., Camps-Valls, G., Soria-Olivas, E., Pérez-Ruixo, J. J., Figueiras-Vidal, A. R.,
and Artés-Rodríguez, A. (). Multi-dimensional function approximation and
regression estimation. In J. Dorronsoro, editor, Artiﬁcial Neural Networks – ICANN
, volume of Lecture Notes in Computer Science, pages –.
Springer-Verlag, Berlin.
Pérez-Cruz, F., Van Vaerenbergh, S., Murillo-Fuentes, J. J., Lazaro-Gredilla, M., and
Santamaria, I. (). Gaussian processes for nonlinear signal processing: an overview
of recent advances. IEEE Signal Processing Magazine, (), –.
Pérez-Suay, A. and Camps-Valls, G. (). Sensitivity maps of the Hilbert–Schmidt
independence criterion. Applied Soft Computing. in press.
https://doi.org/./j.asoc....
Pi, H. and Peterson, C. (). Finding the embedding dimension and variable
dependencies in time series. Neural Computation, (), –.
Picone, J., Ganapathiraju, A., and Hamaker, J. (). Applications of kernel theory to
speech recognition. In G. Camps-Valls, J. L. Rojo-Álvarez, and M. Martínez-Ramón,
editors, Kernel Methods in Bioengineering, Signal and Image Processing, pages –.
Idea Group Publishing, Hershey, PA.
Pinkall, U. and Polthier, K. (). Computing discrete minimal surfaces and their
conjugates. Experimental Mathematics, (), –.
Plackett, R. L. (). Some theorems in least squares. Biometrika, , –.
Platt, J. (). A resource-allocating network for function interpolation. Neural
computation, (), –.
Platt, J. (a). Fast training of support vector machines using sequential minimal
optimization. In B. Schölkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in
Kernel Methods – Support Vector Learning, pages –, Cambridge, MA. MIT Press.
Platt, J. (b). Probabilistic outputs for support vector machines and comparisons to
regularized likelihood methods. In A. J. Smola and P. J. Bartlett, editors, Advances in
Large Margin Classiﬁers, pages –. MIT Press, Cambridge, MA.
Poggio, T. and Smale, S. (). The mathematics of learning: dealing with data. Notices of
the American Mathematical Society, (), –.
Pokharel, P. P., Liu, W., and Príncipe, J. C. (). Kernel least mean square algorithm with
constrained growth. Signal Processing, (), –.
Pokharel, R., Seth, S., and Principe, J. C. (). Mixture kernel least mean square. In The
International Joint Conference on Neural Networks (IJCNN), pages –. IEEE Press,
Piscataway, NJ.
Politis, D. N., Romano, J. P., and Lai, T.-L. (). Bootstrap conﬁdence bands for spectra
and cross-spectra. IEEE Transactions on Signal Processing, (), –.
Powell, M. (). Some algorithms for thin plate spline interpolation to functions of two
variables. In H. Dikshit and C. Micchelli, editors, Proceedings of the Conference on

618
References
Advances in Computational Mathematics: New Delhi, India, pages –. World
Scientiﬁc, Singapore.
Pozar, D. M. (). A relation between the active input impedance and the active element
pattern of a phased array. IEEE Transactions on Antennas and Propagation, (),
–.
Press, W. H., Teukolsky, S. A., Vetterling, W. T., and Flannery, B. P. (). Numerical
Recipes in C. Cambridge University Press, nd edition.
Principe, J. C. (). Information Theoretic Learning: Renyi’s Entropy and Kernel
Perspectives. Springer.
Principe, J. C., deVries, B., and deOliveira, P. G. (). The gamma ﬁlter – a new class of
adaptive IIR ﬁlters with restricted feedback. IEEE Transactions on Signal Processing,
(), –.
Proakis, J. G. and Manolakis, D. K. (). Digital Signal Processing. Prentice Hall, Upper
Saddle River, NJ, th edition.
Proakis, J. G. and Salehi, M. (). Fundamentals of Communication Systems. Prentice
Hall, Upper Saddle River, NJ, st edition.
Qi, H. and Hughes, S. M. (). Using the kernel trick in compressive sensing: accurate
signal recovery from fewer measurements. In IEEE International Conference on
Acoustics, Speech and Signal Processing, pages –. IEEE Press, Piscataway, NJ.
Qin, S. and McAvoy, T. (). Non-linear PLS modelling using neural networks.
Computers & Chemical Engineering, , –.
Quiñonero-Candela, J. (). Learning with Uncertainty – Gaussian Processes and
Relevance Vector Machines. Ph.D. thesis, Technical University of Denmark, Informatics
and Mathematical Modelling, Kongens Lyngby, Denmark.
Quiñonero-Candela, J. and Rasmussen, C. E. (). A unifying view of sparse
approximate gaussian process regression. Journal of Machine Learning Research, ,
–.
Quiñonero-Candela, J., Sugiyama, M., Schwaighofer, A., and Lawrence, N. D. ().
Dataset Shift in Machine Learning. Neural Information Processing Series. MIT Press,
Cambridge, MA.
Rabiner, L. (). A tutorial on hidden Markov models and selected applications in
speech recognition. Proceedings of the IEEE, (), –.
Rabiner, L. R. and Schafer, R. W. (). Introduction to digital speech processing.
Foundations and Trends® in Signal Processing, (–), –.
Rabiner, L. R., Kaiser, J. F., Herrmann, O., and Dolan, M. T. (). Some comparisons
between FIR and IIR digital ﬁlters. The Bell System Technical Journal, (), –.
Rahimi, A. and Recht, B. (). Random features for large-scale kernel machines. In J. C.
Platt, D. Koller, Y. Singer, and S. T. Roweis, editors, NIPS’Proceedings of the th
International Conference on Neural Information Processing Systems, pages –.
Curran Associates.
Rahimi, A. and Recht, B. (). Weighted sums of random kitchen sinks: Replacing
minimization with randomization in learning. In D. Koller, D. Schuurmans, Y. Bengio,
and L. Bottou, editors, Advances in Neural Information Processing Systems , pages
–. Curran Associates.
Rakotomamonjy, A., Bach, F., Grandvalet, Y., and Canu, S. (). SimpleMKL. Journal of
Machine Learning Research, , –.

References
619
Ralaivola, L. and d’Alche Buc, F. (). Dynamical modeling with kernels for nonlinear
time series prediction. In S. Thrun, L. K. Saul, and B. Schölkopf, editors, Advances in
Neural Processing Systems , pages –. MIT Press, Cambridge, MA.
Ralaivola, L. and d’Alche Buc, F. (). Time series ﬁltering, smoothing and learning using
the kernel kalman ﬁlter. In IJCNN ’. Proceedings. IEEE International Joint
Conference on Neural Networks, volume , pages –.
Randen, T. and Husoy, J. (). Filtering for texture classiﬁcation: a comparative study.
IEEE Transactions on Pattern Analysis and Machine Intelligence, (), –.
Ranney, K. I. and Soumekh, M. (). Hyperspectral anomaly detection within the signal
subspace. IEEE Geoscience and Remote Sensing Letters, (), –.
Rasmussen, C. E. and Quiñonero-Candela, J. (). Healing the relevance vector machine
by augmentation. In Proceedings of the nd International Conference on Machine
Learning, pages –. ACM, New York, NY.
Rasmussen, C. E. and Williams, C. K. I. (). Gaussian Processes for Machine Learning.
The MIT Press, Cambridge, MA.
Reed, I. S. and Yu, X. (). Adaptive multiple-band CFAR detection of an optical pattern
with unknown spectral distribution. IEEE Transactions on Acoustics, Speech and Signal
Processing, (), –.
Reed, M. and Simon, B. (). Functional Analysis, volume of Methods of Modern
Mathematical Physics. Academic Press, edition.
Remondo, D., Srinivasan, R., Nicola, V. F., van Etten, W. C., and Tattje, H. E. P. ().
Adaptive importance sampling for performance evaluation and parameter optimization
of communication systems. IEEE Transactions on Communications, (),
–.
Richard, C., Bermudez, J. C. M., and Honeine, P. (). Online prediction of time series
data with kernels. IEEE Transactions on Signal Processing, (), –.
Riesz, F. and Nagy, B. S. (). Functional Analysis. Frederick Ungar Publishing Co.
Rifkin, R. and Klautau, A. (). In defense of one-versus-all classiﬁcation. Journal of
Machine Learning Research, , –.
Rihaczek, W. (). Signal energy distribution in time and frequency. IEEE Transactions
on Information Theory, , –.
Rissanen, J. (). Modeling by shortest data description. Automatica, (), –.
Robert, C. P. and Casella, G. (). Monte Carlo Statistical Methods. Springer.
Rohwer, J. A., Abdallah, C. T., and Christodoulou, C. G. (). Least squares support
vector machines for direction of arrival estimation with error control and validation. In
Global Telecommunications Conference, . GLOBECOM ’. IEEE, volume , pages
–. IEEE Press, Piscataway, NJ.
Rojo-Álvarez, J. L., Martínez-Ramón, M., Figueiras-Vidal, A., García-Armada, A., and
Artés-Rodríguez, A. (). A robust support vector algorithm for nonparametric
spectral analysis. IEEE Signal Processing Letters, (), –.
Rojo-Álvarez, J. L., Arenal-Maiz, A., and Artes-Rodriguez, A. (). Discriminating
between supraventricular and ventricular tachycardias from EGM onset analysis. IEEE
Engineering in Medicine and Biology Magazine, (), –.
Rojo-Álvarez, J. L., Martínez-Ramón, M., de Prado-Cumplido, M., Artés-Rodríguez, A.,
and Figueiras-Vidal, A. R. (). Support vector method for robust ARMA system
identiﬁcation. IEEE Transactions on Signal Processing, (), –.

620
References
Rojo-Álvarez, J. L., Camps-Valls, G., Martínez-Ramón, M., Soria-Olivas, E., A. Navia
Vázquez, and Figueiras-Vidal, A. R. (). Support vector machines framework for
linear signal processing. Signal Processing, (), –.
Rojo-Álvarez, J. L., Figuera-Pozuelo, C., Martínez-Cruz, C. E., Camps-Valls, G.,
Alonso-Atienza, F., and Martínez-Ramón, M. (). Nonuniform interpolation of
noisy signals using support vector machines. IEEE Transactions on Signal Processing,
(), –.
Rojo-Álvarez, J. L., , Martínez-Ramón, M., Muñoz-Marí, J., Camps-Valls, G., Cruz, C.
E. M., and Figueiras-Vidal, A. R. (). Sparse deconvolution using support vector
machines. EURASIP Journal on Advances in Signal Processing, , .
Rojo-Álvarez, J. L., Martínez-Ramón, M., Jordi, Muñoz-Marí, and Camps-Valls, G. ().
A uniﬁed SVM framework for signal estimation. Digital Signal Processing, , –.
Rosenberg, S. (). The Laplacian on a Riemannian Manifold: An Introduction to
Analysis on Manifolds. Number in London Mathematical Society Student Texts.
Cambridge University Press.
Rosipal, R. (). Nonlinear partial least squares: an overview. In H. Lodhi and
Y. Yamanishi, editors, Chemoinformatics and Advanced Machine Learning Perspectives:
Complex Computational Methods and Collaborative Techniques, pages –.
Medical Information Science Reference, Hershey, PA.
Rosipal, R. and Trejo, L. J. (). Kernel partial least squares regression in reproducing
hilbert spaces. Journal of Machine Learning Research, , –.
Roweis, S. T. and Saul, L. K. (). Nonlinear dimensionality reduction by locally linear
embedding. Science, (), –.
Roy, N. and McCallum, A. (). Toward optimal active learning through sampling
estimation of error reduction. In C. Brodley and A. Danyluk, editors, ICML ’
Proceedings of the Eighteenth International Conference on Machine Learning, pages
–. Morgan Kaufmann, San Francisco, CA.
Ruppert, D., Wand, M. P., and Carroll, R. J. (). Semiparametric Regression. Number 
in Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University
Press.
Rzepka, D. (). Fixed-budget kernel least mean squares. In IEEE th
International Conference on Emerging Technologies & Factory Automation (ETFA),
pages –. IEEE Press, Piscataway, NJ.
Saenko, K., Kulis, B., Fritz, M., and Darrell, T. (). Adapting visual category models to
new domains. In K. Daniilidis, P. Maragos, and N. Paragios, editors, Computer Vision –
ECCV’: th European Conference on Computer Vision, Heraklion, Crete, Greece,
September –, , Proceedings, Part IV, pages –. Springer-Verlag, Berlin.
Sampson, P. and Guttorp, P. (). Nonparametric estimation of nonstationary spatial
covariance structure. Journal of the American Statistical Association, (),
–.
Sánchez-Fernández, M., de Prado-Cumplido, M., Arenas-García, J., and Pérez-Cruz, F.
(). Support vector machine techniques for nonlinear equalization. IEEE
Transactions on Signal Processing, (), –.
Sands, N. P. and Cioﬃ, J. M. (). Nonlinear channel models for digital magnetic
recording. IEEE Transactions on Magnetism, , –.
Santamaría-Caballero, I., Pantaleón-Prieto, C. J., and Artés-Rodríguez, A. (). Sparse
deconvolution using adaptive mixed-Gaussian models. Signal Processing, , –.

References
621
Saunders, C., Gammerman, A., and Vovk, V. (). Ridge regression learning algorithm in
dual variables. In J. W. Shavlik, editor, ICML ’Proceedings of the Fifteenth
International Conference on Machine Learning, pages –. Morgan Kaufmann,
San Francisco, CA.
Sayed, A. H. (). Fundamentals of Adaptive Filtering. Wiley–IEEE Press.
Schapire, R. E. and Singer, Y. (). BoosTexter: a boosting-based system for text
categorization. Machine Learning, (–), –.
Schaum, A. and Stocker, A. (). Long-interval chronochrome target detection. In
Proceedings of the International Symposium on Spectral Sensing Research, pages
–.
Schetzen, M. (). The Volterra and Wiener Theories of Nonlinear Systems. Krieger
Publishing, New York, NY.
Schohn, G. and Cohn, D. (). Less is more: active learning with support vector
machines. In P. Langley, editor, ICML ’Proceedings of the Seventeenth
International Conference on Machine Learning, pages –. Morgan Kaufmann,
San Francisco, CA.
Schölkopf, B. (). Support Vector Learning. R. Oldenbourg Verlag, Munich.
Schölkopf, B. and Smola, A. (). Learning with Kernels – Support Vector Machines,
Regularization, Optimization and Beyond. MIT Press, Cambridge, MA.
Schölkopf, B., Smola, A., and Müller, K. (). Nonlinear component analysis as a kernel
eigenvalue problem. Neural Computation, (), –.
Schölkopf, B., Burges, C. J. C., and Smola, A. J., editors (). Advances in Kernel
Methods: Support Vector Learning. MIT Press, Cambridge, MA.
Schölkopf, B., Bartlett, P. L., Smola, A., and Williamson, R. (). Shrinking the tube: a
new support vector regression algorithm. In M. S. Kearns, S. Solla, and D. A. Cohn,
editors, Advances in Neural Information Processing Systems , pages –,
Cambridge, MA. MIT Press.
Schölkopf, B., Williamson, R. C., Smola, A., and Shawe-Taylor, J. (). Support vector
method for novelty detection. In T. K. L. Sara A. Solla and K.-R. Müller, editors,
Advances in Neural Information Processing Systems , pages –. MIT Press,
Cambridge, MA.
Schölkopf, B., Smola, A. J., Williamson, R. C., and Bartlett, P. L. (). New support
vector algorithms. Neural Computation, (), –.
Schölkopf, B., Herbrich, R., and Smola, A. J. (). A generalized representer theorem. In
D. Helmbold and B. Williamson, editors, Computational Learning Theory. COLT ,
volume of Lecture Notes in Computer Science, pages –. Springer, Berlin.
Seeger, M. (). Learning with labeled and unlabeled data. Technical Report TR.,
Institute for Adaptive and Neural Computation, University of Edinburgh.
Selva, J. (). Functionally weighted Lagrange interpolation of band-limited signals from
nonuniform samples. IEEE Transactions on Signal Processing, (), –.
Seung, H. S., Opper, M., and Sompolinsky, H. (). Query by committee. In COLT ’
Proceedings of the Fifth Annual Workshop on Computational Learning Theory, pages
–. ACM, New York, NY.
Shannon, C. E. (). The Mathematical Theory of Communication. University of Illinois
Press, st edition.
Shannon, C. E. (). Classic paper: Communication in the presence of noise.
Proceedings of the IEEE, (), –.

622
References
Shawe-Taylor, J. and Cristianini, N. (). Kernel Methods for Pattern Analysis.
Cambridge University Press.
Shekar, B. H., Kumari, M. S., Mestetskiy, L. M., and Dyshkant, N. F. (). Face recognition
using kernel entropy component analysis. Neurocomputing, (), –.
Shen, H., Jegelka, S., and Gretton, A. (). Fast kernel ICA using an approximate
Newton method. Proceedings of Machine Learning Research, PLMR-, –.
Shen, H., Jegelka, S., and Gretton, A. (). Fast kernel-based independent component
analysis. IEEE Transactions on Signal Processing, (), –.
Sheng, Y. (). Wavelet transform. In A. D. Poularika, editor, The Transforms and
Applications Handbook, pages –. CRC Press, Boca Raton, FL.
Shpun, S., Gepstein, L., Hayam, G., and Ben-Haim, S. A. (). Guidance of
radiofrequency endocardial ablation with real-time three-dimensional magnetic
navigation system. Circulation, (), –.
Shrager, J., Hogg, T., and Huberman, B. A. (). Observation of phase transitions in
spreading activation networks. Science, , –.
Silverman, B. (). Some aspects of the spline smoothing approach to non-parametric
curve ﬁtting. Journal of the Royal Statistical Society, B-, –.
Silverman, B. (). Density Estimation for Statistics and Data Analysis. Chapman and
Hall, London.
Silvia, M. T. and Robinson, E. A. (). Deconvolution of Geophysical Time Series in the
Exploration for Oil and Natural Gas, volume of Developments in Petroleum Science.
Elsevier Scientiﬁc, Amsterdam.
Sindhwani, V., Niyogi, P., and Belkin, M. (). Beyond the point cloud: from transductive
to semi-supervised learning. In ICML ’: Proceedings of the nd International
Conference on Machine Learning, pages –, New York, NY. ACM.
Sinha, D. and Tewﬁk, A. (). Low bit rate transparent audio compression using adapted
wavelets. IEEE Transactions on Signal Processing, (), –.
Sjöberg, J., Zhang, Q., Ljung, L., Benveniste, A., Deylon, B., Glorennec, P.-Y., Hjalmarsson,
H., and Juditsky, A. (). Nonlinear black-box modeling in system identiﬁcation: a
uniﬁed overview. Automatica, , –.
Smola, A. J. and Schölkopf, B. (). A tutorial on support vector regression. Statistics
and Computing, , –.
Smola, A. J., Murata, N., Schölkopf, B., and Müller, K.-R. (). Asymptotically optimal
choice of 𝜀-loss for support vector machines. In L. Niklasson, M. Bodén, and T. Ziemke,
editors, ICANN , Perspectives in Neural Computing, pages –. Springer,
London.
Snelson, E. and Ghahramani, Z. (). Local and global sparse Gaussian process
approximations. Proceedings of Machine Learning Research, PMLR-, –.
Snelson, E., Rasmussen, C., and Ghahramani, Z. (). Warped Gaussian processes. In
S. Thrun, L. Saul, and P. Schölkopf, editors, Advances in Neural Information Processing
Systems (NIPS ), pages –. MIT Press, Cambridge, MA.
Söderström, T. and Stoica, P. (). System Identiﬁcation. Prentice-Hall, Englewood Cliﬀs,
NJ.
Soguero-Ruiz, C., Gimeno-Blanes, F.-J., Mora-Jiménez, I., Martínez-Ruiz, M. P., and
Rojo-Álvarez, J.-L. (). On the diﬀerential benchmarking of promotional eﬃciency
with machine learning modeling (I): principles and statistical comparison. Expert
Systems with Applications, (), –.

References
623
Soguero-Ruiz, C., Hindberg, K., Rojo-Álvarez, J. L., Skrøvseth, S., Godtliebsen, F.,
Mortensen, K., Revhaug, A., Lindsetmo, R.-O., Mora-Jiménez, I., Augestad, K., and
Jenssen, R. (a). Bootstrap resampling feature selection and support vector machine
for early detection of anastomosis leakage. In IEEE–EMBS International
Conference on Biomedical and Health Informatics (BHI), pages –. IEEE Press,
Piscataway, NJ.
Soguero-Ruiz, C., Hindberg, K., Rojo-Álvarez, J. L., Skrøvseth, S. O., Godtliebsen, F.,
Mortensen, K., Revhaug, A., Lindsetmo, R.-O., Augestad, K. M., and Jenssen, R. (b).
Support vector feature selection for early detection of anastomosis leakage from
bag-of-words in electronic health records. IEEE Journal of Biomedical and Health
Informatics, (), –.
Soguero-Ruiz, C., Palancar, F., Bermejo, J., Antoranz, J. L., and Rojo-Álvarez, J. (a).
Autocorrelation kernel support vector machines for Doppler ultrasound M-mode
images denoising. In Computing in Cardiology Conference (CinC), volume ,
pages –. IEEE Press, Piscataway, NJ.
Soguero-Ruiz, C., Hindberg, K., Mora-Jiménez, I., Rojo-Álvarez, J. L., Skrøvseth, S. O.,
Godtliebsen, F., Mortensen, K., Revhaug, A., Lindsetmo, R.-O., Augestad, K. M., et al.
(b). Predicting colorectal surgical complications using heterogeneous clinical data
and kernel methods. Journal of Biomedical Informatics, , –.
Soh, H. and Demiris, Y. (). Spatio-temporal learning with the online ﬁnite and inﬁnite
echo-state Gaussian processes. IEEE Transactions on Neural Networks and Learning
Systems, (), –.
Solazzi, M., Parisi, R., and Uncini, A. (). Blind source separation in nonlinear mixtures
by adaptive spline neural networks. In Proceedings of the rd International Conference
on Independent Component Analysis and Blind Signal Separation (ICA ), San
Diego, California, USA, pages –.
Song, L., Smola, A. J., Borgwardt, K. M., and Gretton, A. (). Colored maximum
variance unfolding. In J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis, editors, Advances
in Neural Information Processing Systems (NIPS ), pages –. Curran
Associates.
Sonnenburg, S., G. Rätsch, C. S., and Schölkopf, B. (a). Large scale multiple kernel
learning. Journal of Machine Learning Research, , –.
Sonnenburg, S., Rätsch, G., Schäfer, C., and Schölkopf, B. (b). Large scale multiple
kernel learning. Journal of Machine Learning Research, , –.
Sorkine, O. (). Laplacian mesh processing. In Y. Chrysanthou and M. Magnor,
editors, EUROGRAPHICS ’STAR – State of the Art Report, pages –. The
Eurographics Association.
Sreekanth, V., Vedaldi, A., Jawahar, C. V., and Zisserman, A. (). Generalized RBF
feature maps for eﬃcient detection. In F. Labrosse, R. Zwiggelaar, Y. Liu, and
B. Tiddeman, editors, British Machine Vision Conference, pages .–.. BMVA Press.
Stanford University (a). PLY ﬁles an ASCII polygon format.
http://people.sc.fsu.edu/ jburkardt/data/ply/ply.html (accessed July , ).
Stanford University (b). The Stanford D scanning repository.
http://graphics.stanford.edu/data/Dscanrep/ (accessed July , ).
Stein, D. W. J., Beaven, S. G., Hoﬀ, L. E., Winter, E. M., Schaum, A. P., and Stocker, A. D.
(). Anomaly detection from hyperspectral imagery. IEEE Signal Processing
Magazine, (), –.

624
References
Stoica, P. and Sharman, K. C. (). Novel eigenanalysis method for direction estimation.
IEE Proceedings F, Radar and Signal Processing, (), –.
Strohmer, T. (). Numerical analysis of the non-uniform sampling problem. Journal of
Computational and Applied Mathematics, (–), –.
Sugiyama, M. and Kawanabe, M. (). Machine Learning in Non-Stationary
Environments: Introduction to Covariate Shift Adaptation. MIT Press, Cambridge, MA.
Sun, S., Zhao, J., and Zhu, J. (). A review of Nyström methods for large-scale machine
learning. Information Fusion, (C), –.
Sutherland, D. J. and Schneider, J. G. (). On the error of random Fourier features.
http://arxiv.org/abs/..
Suykens, J., Vandewalle, J., and Moor, B. D. (a). Intelligence and cooperative search by
coupled local minimizers. International Journal of Bifurcation and Chaos, (),
–.
Suykens, J. A. K. (). Support vector machines: a nonlinear modelling and control
perspective. European Journal of Control, (–), –.
Suykens, J. A. K. and Vandewalle, J. (). Least squares support vector machine
classiﬁers. Neural Processing Letters, (), –.
Suykens, J. A. K. and Vandewalle, J. (). Recurrent least squares support vector
machines. IEEE Transactions on Circuits and Systems-I, (), –.
Suykens, J. A. K., Vandewalle, J., and Moor, B. D. (b). Optimal control by least squares
support vector machines. Neural Networks, (), –.
Suykens, J. A. K., Gestel, T. V., Brabanter, J. D., Moor, B. D., and Vandewalle, J., editors
(). Least Squares Support Vector Machines. World Scientiﬁc, Singapore.
Takens, F. (). Detecting strange attractors in turbulence. Dynamical Systems and
Turbulence, , –.
Taleb, A. and Jutten, C. (). Source separation in post-nonlinear mixtures. IEEE
Transactions on Signal Processing, , –.
Taleb, A., Solé, J., and Jutten, C. (). Quasi-nonparametric blind inversion of Wiener
systems. IEEE Transactions on Signal Processing, (), –.
Tan, Y. and Wang, J. (). Nonlinear blind source separation using higher order statistics
and a genetic algorithm. IEEE Transactions on Evolutionary Computation, (),
–.
Task Force of the European Society of Cardiology and the North American Society of
Pacing and Electrophysiology (). Heart rate variability – standards of measurement,
physiological interpretation, and clinical use. Circulation, (), –.
Taskar, B., Guestrin, C., and Koller, D. (). Max-margin Markov networks. In S. Thrun,
L. Saul, and P. Schölkopf, editors, Advances in Neural Information Processing Systems ,
pages –. MIT Press, Cambridge, MA.
Taubin, G. (). A signal processing approach to fair surface design. In S. Mair and
R. Cook, editors, Proceedings of the nd Annual Conference on Computer Graphics and
Interactive Techniques, pages –. ACM, New York, NY.
Tax, D. and Duin, R. P. W. (). Support vector domain description. Pattern Recognition
Letters, , –.
Tenenbaum, J. B., de Silva, V., and Langford, J. C. (). A global geometric framework
for nonlinear dimensionality reduction. Science, (), .
Tertinek, S. and Vogel, C. (). Reconstruction of nonuniformly sampled bandlimited
signals using a diﬀerentiator–multiplier cascade. IEEE Transactions on Circuits and
Systems I, (), –.

References
625
Theiler, J. (). Quantitative comparison of quadratic covariance-based anomalous
change detectors. Applied Optics, (), –.
Theiler, J. and Perkins, S. (). Proposed framework for anomalous change detection. In
ICML Workshop on Machine Learning Algorithms for Surveillance and Event Detection,
pages –.
Theiler, J., Scovel, C., Wohlberg, B., and Foy, B. R. (). Elliptically contoured
distributions for anomalous change detection in hyperspectral imagery. IEEE
Geoscience and Remote Sensing Letters, (), –.
Theis, F. J. and Amari, S. (). Postnonlinear overcomplete blind source separation using
sparse sources. In C. Puntonet and A. Prieto, editors, Independent Component Analysis
and Blind Signal Separation. ICA , volume of Lecture Notes in Computer
Science, pages –. Springer, Berlin.
Tibshirani, R. (). Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society, Series B, , –.
Tikhonov, A. N. (). Regularization of incorrectly posed problems. Soviet Mathematics
– Doklady, , –.
Tikhonov, A. N. and Arsenin, V. I. A. (). Solutions of Ill-Posed Problems. Scripta Series
in Mathematics. John Wiley & Sons, Ltd, Chichester. Translated by F. John.
Tipping, M. E. (). The relevance vector machine. In S. A. Solla, T. K. Leen, and K.-R.
Müller, editors, Advances in Neural Information Processing Systems , pages –.
MIT Press, Cambridge, MA.
Tipping, M. E. (). Sparse Bayesian learning and the relevance vector machine. Journal
of Machine Learning Research, , –.
Tong, S. and Koller, D. (). Support vector machine active learning with applications to
text classiﬁcation. Journal of Machine Learning Research, , –.
Torralba, A. and Efros, A. A. (). Unbiased look at dataset bias. In CVPR’Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pages
–. IEEE Computer Society, Washington, DC.
Tosic, I. and Frossard, P. (). Dictionary learning. IEEE Signal Processing Magazine,
(), –.
Tropp, J. A. (). Greed is good: algorithmic results for sparse approximation. IEEE
Transactions on Information Theory, , –.
Tropp, J. A. and Wright, S. J. (). Computational methods for sparse solution of linear
inverse problems. Proceedings of the IEEE, (), –.
Tsochantaridis, I., Hofmann, T., Joachims, T., and Altun, Y. (). Support vector
machine learning for interdependent and structured output spaces. In ICML ’
Proceedings of the Twenty-First International Conference on Machine Learning, page
. ACM, New York, NY.
Tsochantaridis, I., Finley, T., Joachims, T., Hofmann, T., and Altun, Y. (). Large margin
methods for structured and interdependent output variables. Journal of Machine
Learning Research, , –.
Tuia, D. and Camps-Valls, G. (). Semisupervised remote sensing image classiﬁcation
with cluster kernels. IEEE Geoscience and Remote Sensing Letters, (), –.
Tuia, D. and Camps-Valls, G. (). Kernel manifold alignment for domain adaptation.
PLoS One, (), e.
Tuia, D., Ratle, F., Paciﬁci, F., Kanevski, M., and Emery, W. J. (). Active learning
methods for remote sensing image classiﬁcation. IEEE Transactions on Geoscience and
Remote Sensing, (), –.

626
References
Tuia, D., Verrelst, J., Alonso, L., Pérez-Cruz, F., and Camps-Valls, G. (a). Multioutput
support vector regression for remote sensing biophysical parameter estimation. IEEE
Geoscience and Remote Sensing Letters, (), –.
Tuia, D., Muñoz-Marí, J., Kanevski, M., and Camps-Valls, G. (b). Structured output
SVM for remote sensing image classiﬁcation. Journal of Signal Processing Systems,
(), –.
Tuia, D., Muñoz-Marí, J., Gómez-Chova, L., and Malo, J. (). Graph matching for
adaptation in remote sensing. IEEE Transactions on Geoscience and Remote Sensing,
(), –.
Tuia, D., Muñoz-Marí, J., Rojo-Álvarez, J. L., Martínez-Ramón, M., and Camps-Valls, G.
(). Explicit recursive and adaptive ﬁltering in reproducing kernel Hilbert spaces.
IEEE Transactions on Neural Networks and Learning Systems, (), –.
Unser, M. (). Sampling – years after Shannon. Proceedings of the IEEE, (),
–.
Vaidyanathan, P. P. (). Generalizations of the sampling theorem: Seven decades after
Nyquist. IEEE Transactions on Circuits and Systems I, (), –.
Vakhania, N., Tarieladze, V., and Chobanyan, S. (). Probability Distributions on
Banach Spaces. Springer Science & Business Media.
Vallet, B. and Lévy, B. (). Spectral geometry processing with manifold harmonics.
Computer Graphics Forum, (), –.
Vallet, B. and Lévy, B. (). Manifold harmonics. Technical report, INRIA – ALICE
Project Team.
Van Gerven, M., Chao, Z., and Heskes, T. (). On the decoding of intracranial data
using sparse orthonormalized partial least squares. Journal of Neural Engineering, (),
.
Van Huﬀel, S. and Vandewalle, J. (). The Total Least Squares Problem. Society for
Industrial and Applied Mathematics.
Van Trees, H. (). Detection, Estimation, and Modulation Theory. John Wiley & Sons,
Inc., New York, NY.
Van Vaerenbergh, S. and Santamaría, I. (). A spectral clustering approach to
underdetermined postnonlinear blind source separation of sparse sources. IEEE
Transactions on Neural Networks, (), –.
Van Vaerenbergh, S. and Santamaría, I. (). A comparative study of kernel adaptive
ﬁltering algorithms. In IEEE Digital Signal Processing Workshop and Signal
Processing Education Meeting. DSP/SPE , Proceedings, pages –. IEEE Press,
Piscataway, NJ.
Van Vaerenbergh, S., Vía, J., and Santamaría, I. (). A sliding-window kernel RLS
algorithm and its application to nonlinear channel identiﬁcation. In IEEE
International Conference on Acoustics Speech and Signal Processing Proceedings,
volume , pages –. IEEE Press, Piscataway, NJ.
Van Vaerenbergh, S., Vía, J., and Santamaría, I. (). A kernel canonical correlation
analysis algorithm for blind equalization of oversampled Wiener systems. In MLSP
: Proceedings of the IEEE Workshop on Machine Learning for Signal
Processing, pages –. IEEE Press, Piscataway, NJ.
Van Vaerenbergh, S., Santamaría, I., Liu, W., and Príncipe, J. C. (). Fixed-budget kernel
recursive least-squares. In IEEE International Conference on Acoustics, Speech and
Signal Processing, pages –. IEEE Press, Piscataway, NJ.

References
627
Van Vaerenbergh, S., Santamaría, I., and Lázaro-Gredilla, M. (a). Estimation of the
forgetting factor in kernel recursive least squares. In IEEE International Workshop
on Machine Learning for Signal Processing, pages –. IEEE Press, Piscataway, NJ.
Van Vaerenbergh, S., Lázaro-Gredilla, M., and Santamaría, I. (b). Kernel recursive
least-squares tracker for time-varying regression. IEEE Transactions on Neural
Networks and Learning Systems, (), –.
Van Vaerenbergh, S., Fernández-Bes, J., and Elvira, V. (a). On the relationship between
online Gaussian process regression and kernel least mean squares algorithms. In 
IEEE th International Workshop on Machine Learning for Signal Processing (MLSP),
pages –. IEEE Press, Piscataway, NJ.
Van Vaerenbergh, S., Comminiello, D., and Azpicueta-Ruiz, L. A. (b). A split kernel
adaptive ﬁltering architecture for nonlinear acoustic echo cancellation. In th
European Signal Processing Conference (EUSIPCO), pages –. IEEE Press,
Piscataway, NJ.
Vanbeylen, L., Pintelon, R., and Schoukens, J. (). Application of blind identiﬁcation to
nonlinear calibration. IEEE Transactions on Instrumentation and Measurement, (),
–.
Vapnik, V. (). Statistical Learning Theory, Adaptive and Learning Systems for Signal
Processing, Communications, and Control. John Wiley & Sons.
Vapnik, V. N. (). The Nature of Statistical Learning Theory. Springer-Verlag, New
York, NY.
Vedaldi, A. and Zisserman, A. (). Eﬃcient additive kernels via explicit feature maps.
In IEEE Computer Society Conference on Computer Vision and Pattern
Recognition, pages –. IEEE Press, Piscataway, NJ.
Vepa, R. (). A review of techniques for machine learning of real-time control
strategies. Intelligent Systems Engineering, (), –.
Verdú, S. (). Multiuser Detection. Cambridge University Press, Cambridge, UK.
Vielva, L., Erdogmus, D., and Príncipe, J. C. (). Underdetermined blind source
separation using a probabilistic source sparsity model. In Proceedings of the rd
International Conference on Independent Component Analysis and Blind Signal
Separation (ICA ), San Diego, California, USA, pages –.
Vielva, L., Erdogmus, D., Pantaleon, C., Santamaría, I., Pereda, J., and Príncipe, J. C. ().
Underdetermined blind source separation in a time-varying environment. In
Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal
Processing, volume , pages –. IEEE Press, Piscataway, NJ.
Ville, J. (). Thèorie et applications de la notion de signal analytique. Cables et
Transmission, A, –.
Vishwanathan, S., Smola, A., and Murty, M. (). SimpleSVM. In T. Fawcett and
N. Mishra, editors, ICML’Proceedings of the Twentieth International Conference on
International Conference on Machine Learning, pages –. AAAI Press.
Vlachos, A. (). A stopping criterion for active learning. Computer Speech & Language,
(), –.
Volpi, M., Tuia, D., and Kanevski, M. (). Memory-based cluster sampling for remote
sensing image classiﬁcation. IEEE Transactions on Geoscience and Remote Sensing,
(), –.
Volterra, V. (a). Sopra le funzioni che dipendono da altre funzioni. nota . Atti della
Reale Accademia dei Lincei, Serie IV, (Semestre ), –.

628
References
Volterra, V. (b). Sopra le funzioni che dipendono da altre funzioni. nota . Atti della
Reale Accademia dei Lincei, Serie IV, (Semestre ), –.
Volterra, V. (c). Sopra le funzioni che dipendono da altre funzioni. nota . Atti della
Reale Accademia dei Lincei, Serie IV, (Semestre ), –.
Vuolo, F., D’Urso, G., and Dini, L. (). Cost-eﬀectiveness of vegetation biophysical
parameters retrieval from remote sensing data. In IEEE International Symposium
on Geoscience and Remote Sensing, pages –. IEEE Press, Piscataway, NJ.
Wang, C. and Mahadevan, S. (). A general framework for manifold alignment. In
AAAI Fall Symposium: Manifold Learning and Its Applications, pages –. AAAI
Press.
Wang, C. and Mahadevan, S. (). Heterogeneous domain adaptation using manifold
alignment. In T. Walsh, editor, IJCAI’Proceedings of the Twenty-Second International
Joint Conference on Artiﬁcial Intelligence – Volume Two, pages –. AAAI Press.
Wang, J., Sano, A., Chen, T., and Huang, B. (). Blind Hammerstein identiﬁcation for
MR damper modeling. In American Control Conference, pages –. IEEE
Press, Piscataway, NJ.
Wang, M., Sha, F., and Jordan, M. I. (). Unsupervised kernel dimension reduction. In
J. D. Laﬀerty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, editors,
NIPS’Proceedings of the rd International Conference on Neural Information
Processing Systems, pages –. Curran Associates.
Wardetzky, M., Mathur, S., Kälberer, F., and Grinspun, E. (). Discrete Laplace
operators: no free lunch. In SGP ’Proceedings of the Fifth Eurographics Symposium on
Geometry Processing, pages –. Eurographics Association.
Welch, P. D. (). The use of fast Fourier transform for the estimation of power spectra: a
method based on time averaging over short, modiﬁed periodograms. IEEE Transactions
on Audio and Electroacoustics, (), –.
Weston, J. and Watkins, C. (). Support vector machines for multiclass pattern
recognition. In Proceedings of the Seventh European Symposium on Artiﬁcial Neural
Networks.
Weston, J., Elisseeﬀ, A., and Schölkopf, B. (). Use of the 𝓁-norm with linear models
and kernel methods. Technical report, Biowulf Technologies.
Weston, J., Chapelle, O., Elisseeﬀ, A., Schölkopf, B., and Vapnik, V. (). Kernel
dependency estimation. In S. T. S. Becker and K. Obermayer, editors, Advances in
Neural Information Processing Systems , pages –. MIT Press, Cambridge, MA.
Westwick, D. T. and Kearney, R. E. (). Nonparametric identiﬁcation of nonlinear
biomedical systems, part : theory. Critical Reviews in Biomedical Engineering, ,
–.
Westwick, D. T. and Kearney, R. E. (). Identiﬁcation of a Hammerstein model of the
stretch reﬂex EMG using separable least squares. In Proceedings of the nd Annual
International Conference of the IEEE Engineering in Medicine and Biology Society (Cat.
No.CH), volume , pages –. IEEE Press, Piscataway, NJ.
Weyman, A. (). Principles and Practice of Echocardiography. Lea & Febiger, nd
edition.
Widrow, B., McCool, J., and Ball, M. (). The complex LMS algorithm. Proceedings of
the IEEE, (), –.
Wiener, N. (). Nonlinear Problems in Random Theory. MIT Press/John Wiley & Sons,
Inc., Cambridge, MA/New York, NY, ﬁrst edition.

References
629
Wiener, N. (). Extrapolation, Inerpolation, and Smoothing of Stationary Time Series.
MIT Press, Cambridge, MA.
Wigner, E. (). On the quantum correction for thermodynamic equilibrium. Physical
Review, , –.
Wigren, T. (). Convergence analysis of recursive identiﬁcation algorithms based on
the nonlinear Wiener model. IEEE Transactions on Automatic Control, (),
–.
Wilkinson, W. A. and Cox, M. D. (). Discrete wavelet analysis of power system
transients. IEEE Transactions on Power Systems, (), –.
Williams, C. and Rasmussen, C. (). Gaussian processes for regression. In D. S.
Touretzky, M. C. Mozer, and M. E. Hasselmo, editors, Advances in Neural Information
Processing Systems , pages –. MIT Press, Cambridge, MA.
Wilson, A. G. and Nickisch, H. (). Kernel interpolation for scalable structured
gaussian processes (KISS-GP). In Proceedings of the nd International Conference on
Machine Learning, ICML , Lille, France, -July , pages –.
Wipf, D., Palmer, J., and Rao, B. (). Perspectives on sparse Bayesian learning. In
L. K. S. Sebastian Thrun and and B. Schölkopf, editors, Advances in Neural Information
Processing Systems , pages –. MIT Press, Cambridge, MA.
Wold, H. (a). Estimation of principal components and related models by iterative least
squares. In P. R. Krishnaiah, editor, Multivariate Analysis, pages –. Academic
Press.
Wold, H. (b). Nonlinear estimation by iterative least squares procedures. In F. David
and E. Fix, editors, Research Papers in Statistics: Festschrift for J. Neyman, page
–. John Wiley & Sons, London.
Wold, S., Albano, C., Dunn, III, W., Edlund, U., Esbensen, K., Geladi, P., Hellberg, S.,
Johansson, E., Lindberg, W., and Sjöström, M. (). Multivariate data analysis in
chemistry. In B. Kowalski, editor, Chemometrics: Mathematics and Statistics in
Chemistry, volume of Nato Science Series C, pages –. Reidel, Dordrecht.
Wold, S., Kettaneh-Wold, N., and Skagerberg, B. (). Nonlinear PLS modeling.
Chemometrics and Intelligent Laboratory Systems, , –.
Xie, Z. and Guan, L. (). Multimodal information fusion of audio emotion recognition
based on kernel entropy component analysis. In IEEE International Symposium on
Multimedia, pages –. IEEE Press, Piscataway, NJ.
Xing, H. and Hansen, J. (). Single sideband frequency oﬀset estimation and correction
for quality enhancement and speaker recognition. IEEE/ACM Transactions on Audio,
Speech, and Language Processing, (), –.
Xu, M., Jiang, L., Sun, X., Ye, Z., and Wang, Z. (). Learning to detect video saliency
with HEVC features. IEEE Transactions on Image Processing, (), –.
Yaghoobi, M., Daudet, L., and Davies, M. E. (). Parametric dictionary design for
sparse coding. IEEE Transactions on Signal Processing, (), –.
Yeh, S. and Stark, H. (). Iterative and one-step reconstruction from nonuniform
samples by convex projections. Journal of the Optical Society of America, (), –.
Yen, J. L. (). On nonuniform sampling of bandwidth-limited signals. IRE Transactions
on Circuit Theory, CT-, –.
Ying, L. and Munson, D. C. (). Approximation of the minmax interpolator. In 
IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings
(Cat. No.CH), volume , pages –. IEEE Press, Piscataway, NJ.

630
References
Yuan, X., Lu, Z., and Yue, C. Z. (). A novel adaptive importance sampling algorithm
based on Markov chain and low-discrepancy sequence. Aerospace Science and
Technology, , –.
Yukawa, M. (). Multikernel adaptive ﬁltering. IEEE Transactions on Signal Processing,
(), –.
Zhan, H., Shi, P., and Chen, C. (). Retrieval of oceanic chlorophyll concentration using
support vector machines. IEEE Transactions on Geoscience and Remote Sensing, (),
–.
Zhang, H., Van Kaick, O., and Dyer, R. (). Spectral mesh processing. Computer
Graphics Forum, (), –.
Zhang, L., Weida, Z., and Jiao, L. (). Wavelet support vector machine. IEEE
Transactions on Systems, Man, and Cybernetics B, (), –.
Zhang, M.-L. and Zhou, Z.-H. (). A k-nearest neighbor based algorithm for
multi-label classiﬁcation. In IEEE International Conference on Granular
Computing, volume , pages –. IEEE Press, Piscataway, NJ.
Zhang, Z. and Hancock, E. (). Kernel entropy-based unsupervised spectral feature
selection. International Journal of Pattern Recognition and Artiﬁcial Intelligence, (),
.
Zhao, Q. and Principe, J. C. (). Support vector machines for SAR automatic target
recognition. IEEE Transactions on Aerospace and Electronic Systems, (), –.
Zhao, S., Chen, B., Zhu, P., and Príncipe, J. C. (). Fixed budget quantized kernel
least-mean-square algorithm. Signal Processing, (), –.
Zhao, S., Chen, B., Cao, Z., Zhu, P., and Principe, J. C. (). Self-organizing kernel
adaptive ﬁltering. EURASIP Journal on Advances in Signal Processing, (), .
Zhou, D. and Schölkopf, B. (). A regularization framework for learning from graph
data. In ICML Workshop on Statistical Relational Learning and its Connections to other
Fields, Banﬀ, Alberta, Canada, pages –.
Zhou, D., Bousquet, O., Lal, T. N., Weston, J., and Schölkopf, B. (). Learning with local
and global consistency. In S. Thrun, L. K. Saul, and B. Schölkopf, editors, Advances in
Neural Information Processing Systems , pages –. MIT Press, Cambridge, MA.
Zhou, D., Huang, J., and Schölkopf, B. (). Learning with hypergraphs: clustering,
classiﬁcation, and embedding. In B. Schölkopf, J. Platt, and T. Hoﬀman, editors,
Advances in Neural Information Processing Systems , pages –. MIT Press,
Cambridge, MA.
Zhu, J., Rosset, S., Hastie, T., and Tibshirani, R. (). -norm support vector machines.
In S. Thrun, L. K. Saul, and B. Schölkopf, editors, Advances in Neural Information
Processing Systems , pages –. MIT Press, Cambridge, MA.
Zhu, X. (). Semi-supervised learning literature survey. Technical Report ,
Computer Sciences, University of Wisconsin-Madison, USA.
Zien, A., Brefeld, U., and Scheﬀer, T. (). Transductive support vector machines for
structured variables. In Z. Ghahramani, editor, ICML ’Proceedings of the th
International Conference on Machine Learning, pages –. ACM, New York, NY.
Zomer, S., Sànchez, M. N., Brereton, R. G., and Pérez-Pavón, J. (). Active learning
support vector machines for optimal sample selection in classiﬁcation. Journal of
Chemometrics, (), –.
Zou, H., Hastie, T., and Tibshirani, R. (). Sparse principal component analysis.
Journal of Computational and Graphical Statistics, , –.

631
Index
a
abundance

access point

active learning
, , , 
adaptive ﬁltering
xix, , , , , ,
, , 
adaptive kernel learning
, 
additive noise
, , , ,
, 
additive noise model
, 
alternative hypothesis
, 
analysis equation
, 
anomaly change detection
, , 
anomaly detection
, , , 
antenna array
, , , , , ,
, 
anti-causal systems

array processing
, , , , , ,
, , , , , 
audio
, , , , , , , 
audio compression

autocorrelation
, , 
autocorrelation-induced kernel,
autocorrelation kernel

autocorrelation kernel
, , , ,
, , , , , 
autocorrelation matrix
, , , ,
, , 
autoregressive (AR)

autoregressive and moving average
(ARMA)

autoregressive and exogenous
(ARX)
, , , , , , ,
, 
b
bag of words features

bag of words kernel

band-pass
, , , , , , ,
, , 
bandwidth
, , , , , , ,
, , , , , 
base-band representation

basis

basis pursuit
, 
Bayesian nonparametric
, ,
, 
Bernoulli–Gauss distribution
, 
Bernoulli process

bias–variance dilemma

bi-exponential distribution see
Laplacian noise
big data

biomedical signals
, , , 
biophysical
, 
bit error rate (BER)
, , , ,
, 
Blackman–Tukey correlogram

blind source separation (BSS)
,
, 
Bootstrap resampling
, , , ,
, 
B-scan

Burg’s method

butterﬂy algorithm

c
canonical basis

cardiac mesh

Digital Signal Processing with Kernel Methods, First Edition. José Luis Rojo-Álvarez, Manel Martínez-Ramón,
Jordi Muñoz-Marí, and Gustau Camps-Valls.
© John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.

632
Index
cardiac navigation systems
, , 
cardiac signal
, 
cardiag image

Cauchy–Schwartz inequality
,
, 
Cauchy sequence

causal systems

centering
, , , , 
change detection
, , 
change vector analysis

channel estimation
, , ,
, 
chaotic
, , , , , 
Choi–Williams distribution

Cholesky decomposition

Cholesky factorization
, 
chronocrome

classiﬁcation
, , , , , , ,
, , , , , , 
clustering
, , , , , , ,
, , , , , 
codebook
, , , 
collinearity
, 
color image
, 
communication
, , , , , , ,
, , , , , , ,
, 
complex algebra
, , , 
complex envelope

complex exponential
, , , , ,
, , , 
complexiﬁcation trick

complex signal
, , , , , ,
, 
composite kernel
, , , , ,
, , 
compressed sensing
, , , 
conﬁdence interval (CI)
, ,
, 
constrained covariance (COCO)

continuous-time equivalent system for
nonuniform interpolation
, 
continuous-time signals
, , , ,
, , 
convex
, , , , , , ,
, , , , , , 
convolution
, , , , , , ,
, 
convolution (multidimensional)

correlation
, , , , , , ,
, , , , , , 
correlogram

cost function
, , , , , ,
, , , , 
covariance function
, , 
covariance operator
, , , 
covariate shift

cross-correlation
, , , ,
, 
cross-covariance
, , , ,
, 
cross-information
, , , 
cross-validation
, , , , ,
, , , , , , 
CUSUM

d
deal eﬀect curve
, 
decision tree

deconvolution
, , , , ,
, 
denoising
, , , , , ,
, 
determination coeﬃcient

dictionary learning

digital ﬁltering

dimensionality reduction
, , ,
, , , 
Dirac delta
, , , 
direction of arrival (DOA)

discrete cosine transform (DCT)
, 
discrete-time signals
, , , ,
, 
domain adaptation
, , ,
, 
domain description
, , ,
, 
dot product
, , , , , , ,
, , , , , , , ,
, , , 
double side band

dual parameters


Index
633
dual representation
, 
dual signal model (DSM)
, , ,
, 
e
eigenfunctions

electric networks

electroanatomic map (EAM)
, 
electrocardiogram (ECG)

electroencephalogram (EEG)
,
, 
elliptical
, , 
empirical kernel map

empirical risk
, , , , ,
, , 
endmember
, , 
energy

energy spectral density

equalization
, , , , , ,
, 
Euclidean distance

Euclidean divergence

Euler–Poincaré formula

evidence
, 
expectation–maximization

eye diagram

f
feature map
, , , , , ,
, , , 
feature mapping
, , , 
feature space

feedback
, 
ﬁlter

ﬁlter bank analysis

ﬁltering
, , , , , , , , ,
, , , , , , , ,
, , , 
ﬁnite impulse response (FIR)
, , ,
, , , , , 
Fisher discriminant
, 
Fourier coeﬃcients
, 
Fourier transform

fractal

free parameters
, , , , ,
, , , , , , 
frequency
, , , , , , , ,
, , , , , , ,
, 
functional analysis
, , 
function approximation
, , , ,
, , , , , , , 
fuzzy
, , , 
fuzzy clustering

g
Gabor transform

gamma distribution

gamma-ﬁlter
, , , , , ,
, , , , ,
, 
gamma function

Gaussian distribution
, , ,
, 
Gaussian mixture model
(GMM)
, 
Gaussian mixtures

Gaussian noise

Gaussian processes
, , , 
generative kernel
, 
genetic
, 
Gram matrix

graph
, , , , , , , ,
, , 
graph Laplacian
, , , ,
, 
graph Laplacian matrix

Grassman–Stiefel manifold

grayscale image

greedy algorithms

h
Hammerstein system

Hammerstein–Wiener model

Hanning pulse

heart rate variability (HRV)
, , ,
, 
Heisenberg’s principle

Hermitian signal

Hermitic transpose operator

heteroscedastic
, , , ,
, 

634
Index
Hilbert–Schmidt component analysis
(HSCA)
, 
Hilbert–Schmidt independence criterion
(HSIC)
, , , , 
Hilbert space
, , , , , ,
, , , , , , , ,
, , , , , 
hinge loss
, , , , 
histogram kernel

Holter
, , 
homoscedastic

Hotelling’s test
, , , , ,

Huber
, , , , , , ,
, , , , , , 
𝜖-Huber
, , , , , , 
Huber loss

hyperparameters

hyperresolution method

hyperspectral
, , , , ,
, 
hypothesis testing
, , , 
i
ill-posed problem
, , 
incomplete Cholesky decomposition
(ICD)

independent component analysis
(ICA)
, 
indoor location
, , 
information divergence
, 
information potential

information-theoretic learning
, ,

inner product
, , , see also
dot product; scalar product
inner product space

innovation process
, 
in-phase and quadrature-phase
, 
input features
, , 
input space
, 
𝜖-insensitive loss
, , 
interception

interpolation
, , , , , , ,
, , , , , , , 
invariance learning
, , 
isomap

j
Jacobian weighting

Jensen–Shannon
, 
jitter

joint input-output mapping
, 
k
Kalman
, , , , 
Kalman ﬁlter
, , , 
Karhunen–Loeve

Karush–Kuhn–Tucker conditions

kernel

kernel adaptive ﬁltering
, 
kernel alignment
, , 
kernel autoregressive and moving average
(KARMA)
, 
kernel blind source separation
(KBSS)
, 
kernel canonical correlation analysis
(KCCA)
, , 
kernel density estimation
, 
kernel dependence estimation
, 
kernel dimensionality reduction
(KDR)
, 
kernel entropy component analysis
(KECA)

Kernel Fisher’s discriminant analysis
(KFDA)
, 
kernel generalized variance (KGV)

kernel independent component analysis
(KICA)
, , 
kernelization

kernel least mean squares (KLMS)
,
, 
kernel manifold alignment
(KEMA)
, 
kernel matrix
, 
kernel mean matching (KMM)

kernel methods

kernel multivariate analysis
(KMVA)
, 
kernel mutual information (KMI)

kernel orthonormalized partial least
squares (KOPLS)
, , , ,

kernel partial least squares (KPLS)
,
, 

Index
635
kernel principal component analysis
(KPCA)

kernel recursive least squares
(KRLS)

kernel ridge regression (KRR)
, ,
, 
kernel signal to noise ratio (KSNR)
,

kernel trick
, , , , , ,
, , , , , 
Kirchoﬀoperator

Kirkwood distribution taking

k-means
, , , , ,
, 
k nearest neighbors (k-NN)
, , 
Kronecker delta

l
Label Propagation
, 
Lagrange functional
, 
Laplace–Beltrami operator

Laplace–de Rham operator
, 
Laplacian eigenmaps (LE)
, 
Laplacian noise

Laplacian operator

Large Margin Filtering (LMF)

large-scale
, , , , , 
latent space
, , , 
least absolute deviation (LAD)

least absolute shrinkage and selection
operator (LASSO)

least mean squares (LMS)
, 
least squares (LS)
, , , , ,
, , , 
Least-Squares Support Vector Machine
(LS-SVM)
, 
linear and time-invariant (LTI)
systems

linear discriminant analysis (LDA)
,
, , , 
linear independence

l-norm
, , , , 
l-norm
, , , , , 
locally linear embedding (LLE)

logistic regression

Lomb periodogram
, , 
Lorenz

m
Mackey–Glass
, , , ,
, 
magnetic resonance imaging (MRI)
, 
manifold alignment
, , ,
, 
manifolds
, , , , , , ,
, , 
Margenau–Hill distribution

marketing

Markov chain

matched ﬁlter
, 
maximum a posteriori (MAP)

maximum likelihood (ML)
, , , ,
, , , , , , , 
maximum mean discrepancy
(MMD)
, , 
minimum power distortionless response
(MPDR)

maximum variance unfolding
(MVU)

mean map kernel

medical imaging
, , 
memory depth
, , , 
Mercer, James

Mercer’s kernel
, , 
Mercer’s theorem
, 
M-estimate

metric
, , , , , ,
, 
Mexican hat wavelet

MIMO see multi-input multi-output
(MIMO)
minimax

minimum mean square error (MMSE)

minimum noise fraction (MNF)
,
, 
minimum phase

minimum variance distortionless response
(MVDR)

model diagnosis
, 
modulated kernel

modulation
, , , , , , , 
moving average (MA)

multiclass
, , , , , ,
, 
multidimensional sampling


636
Index
multidimensional scaling (MDS)
,

multidimensional signal
, 
multi-input multi-output (MIMO)
, 
multilabel
, , , 
multi-output
, , 
multiple kernel learning (MKL)
,
, 
multiple signal classiﬁcation (MUSIC)

multiresolution analysis

multispectral remote-sensing

multiuser detection
, 
mutual information
, , , ,
, , 
n
Nadayara–Watson (NW)

natural signals
, , 
neural networks
, , , 
neuron
, 
noise

nonlinear algorithms
, , 
nonlinear channel identiﬁcation

nonlinearity/nonlinearities
, , ,
, , , , , , 
nonlinear signal model

nonlinear SVM
, , , 
nonlinear system identiﬁcation
, ,
, , , , , , ,
, 
nonparametric
, , , , , , ,
, , , , , , , ,

nonparametric spectral analysis
, ,
, , 
nonuniform interpolation
, , ,
, 
nonuniform sampling
, , , ,
, 
normal equation
, 
normalization
, , , , ,
, , , , , , 
null hypothesis

Nyquist pulse

Nyquist theorem

o
one-against-one (OAO)

one-class classiﬁcation
, 
one-class support measure machines
(OC-SMM)

one-class support vector machine
,

online learning
, , , , 
online regression

online sparsiﬁcation

optimization functional

optimized kernel entropy component
analysis (OKECA)

orthogonal base
, 
orthogonal frequency division
multiplexing (OFDM)
, 
orthogonality

orthogonal subspace projection
(OSP)
, 
orthonormal base

outlier
, , , , , , ,

overﬁtting
, , , , , ,
, 
p
Page distribution

parallelization
, 
parameter estimation
, , , ,
, 
parametric spectral analysis
, 
Parseval identity

Parseval’s theorem

parsimonious
, 
Parzen windows
, , , 
pervasive change

phase
, , , , , , , ,
, , , , , , , ,
, , 
posterior probability
, , , 
power

power spectral density

pre-image
, 
primal-dual functional
, 
primal representation
, , 

Index
637
principal component analysis (PCA)
,

prior probability
, 
probabilistic cluster kernel

probability density function

probability product kernel

projections
, , , , , ,
, , , 
promotion
, , , 
pseudoinverse
, , 
pyramid match kernel

Pytagorean theorem

q
Q-mode
, 
QRS complex

quadrature amplitude modulation
(QAM)

quadrature-phase see in-phase and
quadrature-phase
quadrature-phase shift keying
(QPSK)

r
radar
, , , , , , , 
radial basis function (RBF)
, , ,
, 
random Fourier features (RFF)
, ,

rank
, , , , , , , ,
, , 
Rayleigh distribution

received signal strength

recursive ﬁlters

recursive least squares
, 
recursivity
, , , , , 
reﬂectivity

regression
, , , , , , ,
, , , , , , , ,
, , , , , , ,
, 
regularization
, , , 
relevance vector machine (RVM)
,
, 
reliability
, , 
remote sensing
, , 
Rényi entropy
, 
replication (bootstrap)

representer theorem

reproducing kernel Hilbert space
(RKHS)
, , , , , ,

reproducing property

resample (bootstrap)
, 
residual

Riesz representation theorem
, 
Rihaczek distribution

RKHS signal model (RSM)
, , ,

R-mode

running spectrum

s
sample selection
, 
sampling

sampling period
, , , , , 
satellite image
, , , , 
scalar product
, see also inner
product
seismology

self-organizing map (SOM)

semiparametric regression (SR)
, ,

semisupervised
, , , , ,
, 
Shannon

Shannon’s sampling theorem
, , 
shift-invariant
, , , , ,

signal

signal detection
, , , , 
signal interpolation
, , , , ,

signal model
, , , 
signal space

signal-to-noise ratio (SNR)
, 
similarity
, , , 
sinc function

sinc interpolation
, , , , ,
, , , 
sinc interpolator

single side-band


638
Index
slack variable
, , , , ,
, , 
snapshot
, , , , 
social networks

sparse deconvolution
, , ,
, 
sparse kernel feature extraction

sparse learning

sparsity
, , , , , , ,
, , , , , , 
spatial reference

spectral
, , , , , , , ,
, , 
spectral angle mapper (SAM)
,
, 
spectrogram

spectrum
, 
speech recognition
, , , 
stacked kernel
, , , , 
state-space representation
, 
steering vector
, , , 
stiﬀness matrix

structural risk
, , , , 
structured output learning
, 
structure-preserving algorithms

subband coding algorithm

subspace

subspace detector
, , 
subspace methods

support vector
, , , , , ,
, , , , ,
, 
support vector domain description
(SVDD)
, 
support vector machine for digital signal
processing (SVM for DSP)
, ,
, 
support vector machines (SVMs)
,
, , , , 
support vector regression (SVR)
,
, , , 
surrogate

synthesis equation

system identiﬁcation
, , , , ,
, , , , , , , ,
, 
systems with memory

t
tachogram

temporal reference

tensor-product kernel

tessellation

texture classiﬁcation

thermal noise
, , , , 
thin plate spline

Tikhonov regularization
, , ,
, , 
time series prediction
, , , ,
, , 
transductive support vector machine
(TSVM)
, 
transfer component analysis (TCA)
,

transfer learning

transform coding

translation-invariant kernel

triangle inequality

Tutte Laplacian

u
ultrasound
, , , ,
, 
unmixing
, , 
unscented Kalman ﬁlter (UKF)

unsupervised
, , , , , ,
, 
v
Vapnik-Chervonenkis capacity
(VCC)
, 
variance
, , , , , , , ,
, , , , , , , ,
, , , 
vector quantization algorithms

vector space
, 
Volterra
, , 
Voronoi
, 
w
warped Gaussian Process Regression
(WGP)
, 
wavelet function
, , , 

Index
639
Welch periodogram
, 
Wiener

Wiener ﬁlter

Wiener system

Wigner–Ville distribution

y
Yen’s interpolator

z
z-transform



Statistical Implications of Turing’s Formula

Statistical Implications of Turing’s Formula
Zhiyi Zhang
Department of Mathematics and Statistics, University of North Carolina,
Charlotte, NC, US

Copyright © 2017 by John Wiley & Sons, Inc. All rights reserved
Published by John Wiley & Sons, Inc., Hoboken, New Jersey
Published simultaneously in Canada
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any
form or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise,
except as permitted under Section 107 or 108 of the 1976 United States Copyright Act, without
either the prior written permission of the Publisher, or authorization through payment of the
appropriate per-copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers,
MA 01923, (978) 750-8400, fax (978) 750-4470, or on the web at www.copyright.com. Requests to
the Publisher for permission should be addressed to the Permissions Department, John Wiley &
Sons, Inc., 111 River Street, Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008, or online at
http://www.wiley.com/go/permissions.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best
eﬀorts in preparing this book, they make no representations or warranties with respect to the
accuracy or completeness of the contents of this book and speciﬁcally disclaim any implied
warranties of merchantability or ﬁtness for a particular purpose. No warranty may be created or
extended by sales representatives or written sales materials. The advice and strategies contained
herein may not be suitable for your situation. You should consult with a professional where
appropriate. Neither the publisher nor author shall be liable for any loss of proﬁt or any other
commercial damages, including but not limited to special, incidental, consequential, or other
damages.
For general information on our other products and services or for technical support, please
contact our Customer Care Department within the United States at (800) 762-2974, outside the
United States at (317) 572-3993 or fax (317) 572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in
print may not be available in electronic formats. For more information about Wiley products,
visit our web site at www.wiley.com.
Library of Congress Cataloging-in-Publication Data:
Names: Zhang, Zhiyi, 1960-
Title: Statistical implications of Turing’s formula / Zhiyi Zhang.
Description: Hoboken, New Jersey : John Wiley & Sons, Inc., [2017] | Includes
bibliographical references and index.
Identiﬁers: LCCN 2016027830| ISBN 9781119237068 (cloth) | ISBN 9781119237099
(epub)
Subjects: LCSH: Mathematical statistics–Textbooks. |
Probabilities–Textbooks.
Classiﬁcation: LCC QA273 .Z43 2017 | DDC 519.5–dc23 LC record available at
https://lccn.loc.gov/2016027830
Cover image courtesy Author Zhiyi Zhang
Set in 10/12pt, WarnockPro by SPi Global, Chennai, India.
Printed in the United States of America
10
9
8
7
6
5
4
3
2
1

To my family and all my teachers

vii
Contents
Preface
xi
1
Turing’s Formula
1
1.1
Turing’s Formula
3
1.2
Univariate Normal Laws
10
1.3
Multivariate Normal Laws
22
1.4
Turing’s Formula Augmented
27
1.5
Goodness-of-Fit by Counting Zeros
33
1.6
Remarks
42
1.7
Exercises
45
2
Estimation of Simpson’s Indices
49
2.1
Generalized Simpson’s Indices
49
2.2
Estimation of Simpson’s Indices
52
2.3
Normal Laws
54
2.4
Illustrative Examples
61
2.5
Remarks
66
2.6
Exercises
68
3
Estimation of Shannon’s Entropy
71
3.1
A Brief Overview
72
3.2
The Plug-In Entropy Estimator
76
3.2.1
When K Is Finite
76
3.2.2
When K Is Countably Inﬁnite
81
3.3
Entropy Estimator in Turing’s Perspective
86
3.3.1
When K Is Finite
88
3.3.2
When K Is Countably Inﬁnite
94
3.4
Appendix
107
3.4.1
Proof of Lemma 3.2
107
3.4.2
Proof of Lemma 3.5
110

viii
Contents
3.4.3
Proof of Corollary 3.5
111
3.4.4
Proof of Lemma 3.14
112
3.4.5
Proof of Lemma 3.18
116
3.5
Remarks
120
3.6
Exercises
121
4
Estimation of Diversity Indices
125
4.1
A Uniﬁed Perspective on Diversity Indices
126
4.2
Estimation of Linear Diversity Indices
131
4.3
Estimation of Rényi’s Entropy
138
4.4
Remarks
142
4.5
Exercises
145
5
Estimation of Information
149
5.1
Introduction
149
5.2
Estimation of Mutual Information
162
5.2.1
The Plug-In Estimator
163
5.2.2
Estimation in Turing’s Perspective
170
5.2.3
Estimation of Standardized Mutual Information
173
5.2.4
An Illustrative Example
176
5.3
Estimation of Kullback–Leibler Divergence
182
5.3.1
The Plug-In Estimator
184
5.3.2
Properties of the Augmented Plug-In Estimator
186
5.3.3
Estimation in Turing’s Perspective
189
5.3.4
Symmetrized Kullback–Leibler Divergence
193
5.4
Tests of Hypotheses
196
5.5
Appendix
199
5.5.1
Proof of Theorem 5.12
199
5.6
Exercises
204
6
Domains of Attraction on Countable Alphabets
209
6.1
Introduction
209
6.2
Domains of Attraction
212
6.3
Examples and Remarks
223
6.4
Appendix
228
6.4.1
Proof of Lemma 6.3
228
6.4.2
Proof of Theorem 6.2
229
6.4.3
Proof of Lemma 6.6
232
6.5
Exercises
236
7
Estimation of Tail Probability
241
7.1
Introduction
241
7.2
Estimation of Pareto Tail
244
7.3
Statistical Properties of AMLE
248

Contents
ix
7.4
Remarks
253
7.5
Appendix
256
7.5.1
Proof of Lemma 7.7
256
7.5.2
Proof of Lemma 7.9
263
7.6
Exercises
267
References
269
Author Index
275
Subject Index
279

xi
Preface
This book introduces readers to Turing’s formula and then re-examines several
core statistical issues of modern data science from Turing’s perspective.
Turing’s formula was a remarkable invention of Alan Turing during World
War II in an early attempt to decode the German enigmas. The formula
looks at the world of randomness through a unique and powerful binary
perspective – unmistakably of Turing. However, Turing’s formula was not
well understood for many years. Research amassed during the last decade has
brought to light profound and new statistical implications of the formula that
were previously not known. Recently, and only recently, a relatively clear and
systematic description of Turing’s formula, with its statistical properties and
implications, has become possible. Hence this book.
Turing’s formula is often perceived as having a mystical quality. I was
awestruck when I ﬁrst learned of the formula 10 years ago. Its anti-intuitive
implication was simply beyond my immediate grasp. However, I was not along
in this regard. After turning it over in my mind for a while, I mentioned to
two of my colleagues, both seasoned mathematicians, that there might be a
way to give a nonparametric characterization to tail probability of a random
variable beyond data range. To that, their immediate reaction was, “tell us
more when you have ﬁgured it out.” Some years later, a former doctoral student
of mine said to me, “I used to refuse to think about anti-intuitive mathematical
statements, but after Turing’s formula, I would think about a statement at least
twice however anti-intuitive it may sound.” Still another colleague of mine
recently said to me, “I read everything you wrote on the subject, including
details of the proofs. But I still cannot see intuitively why the formula works.”
To that, I responded with the following two points:
1) Our intuition is a bounded mental box within which we conduct intellectual
exercises with relative ease and comfort, but we must admit that this box
also reﬂects the limitations of our experience, knowledge, and ability to
reason.
2) If a fact known to be true does not ﬁt into one’s current box of intuition, is it
not time to expand the boundary of the box to accommodate the true fact?

xii
Preface
My personal journey in learning about Turing’s formula has proved to be a
rewarding one. The experience of observing Turing’s formula totally outside of
my box of intuition initially and then having it gradually snuggled well within
the boundary of my new box of intuition is one I wish to share.
Turing’s formula itself, while extraordinary in many ways, is not the only
reason for this book. Statistical science, since R.A. Fisher, has come a long way
and continues to evolve. In fact, the frontier of Statistics has largely moved
on to the realm of nonparametrics. The last few decades have witnessed great
advances in the theory and practice of nonparametric statistics. However in
this realm, a seemingly impenetrable wall exists: how could one possibly make
inference about the tail of a distribution beyond data range? In front of this
wall, many, if not most, are discouraged by their intuition from exploring
further. Yet it is often said in Statistics that “it is all in the tail.” Statistics needs
a trail to get to the other side of the wall. Turing’s formula blazes a trail, and
this book attempts to mark that trail.
Turing’s formula is relevant to many key issues in modern data sciences, for
example, Big Data. Big Data, though as of yet not a ﬁeld of study with a clearly
deﬁned boundary, unambiguously points to a data space that is a quantum
leap away from what is imaginable in the realm of classical statistics in terms of
data volume, data structure, and data complexity. Big Data, however deﬁned,
issues fundamental challenges to Statistics. To begin, the task of retrieving and
analyzing data in a vastly complex data space must be in large part delegated
to a machine (or software), hence the term Machine Learning. How does a
machine learn and make judgment? At the very core, it all boils down to a
general measure of association between two observable random elements (not
necessarily random variables). At least two fundamental issues immediately
present themselves:
1) High Dimensionality. The complexity of the data space suggests that
a data observation can only be appropriately registered in a very
high-dimensional space, so much so that the dimensionality could be
essentially inﬁnite. Quickly, the usual statistical methodologies run into
fundamental conceptual problems.
2) Discrete and Non-ordinal Nature. The generality of the data space suggests
that possible data values may not have a natural order among themselves:
diﬀerent gene types in the human genome, diﬀerent words in text, and
diﬀerent species in an ecological population are all examples of general data
spaces without a natural “neighborhood” concept.
Such issues would force a fundamental transition from the platform of random
variables (on the real line) to the platform of random elements (on a general
set or an alphabet). On such an alphabet, many familiar and fundamental
concepts of Statistics and Probability no longer exist, for example, moments,
correlation, tail, and so on. It would seem that Statistics is in need of a rebirth
to tackle these issues.

Preface
xiii
The rebirth has been taking place in Information Theory. Its founding
father, Claude Shannon, deﬁned two conceptual building blocks: entropy (in
place of moments) and mutual information (in place of correlation) in his
landmark paper (Shannon, 1948). Just as important as estimating moments and
coeﬃcient of correlation for random variables, entropy and mutual information
must be estimated for random elements in practice. However, estimation of
entropy and estimation of mutual information are technically diﬃcult problems
due to the curse of “High Dimensionality” and “Discrete and Non-ordinal
Nature.” For about 50 years since (Shannon, 1948), advances in this arena have
been slow to come. In recent years however, research interest, propelled by
the rapidly increasing level of data complexity, has been reinvigorated and,
at the same time, has been splintered into many diﬀerent perspectives. One
in particular is Turing’s perspective, which has brought about signiﬁcant and
qualitative improvement to these diﬃcult problems. This book presents an
overview of the key results and updates the frontier in this research space.
The powerful utility of Turing’s perspective can also be seen in many other
areas. One increasingly important modern concept is Diversity. The topics of
what it is and how to estimate it are rapidly moving into rigorous mathematical
treatment. Scientists have passionately argued about them for years but largely
without consensus. Turing’s perspective gives some very interesting answers
to these questions. This book gives a uniﬁed discussion of diversity indices,
hence making good reading for those who are interested in diversity indices
and their estimation. The ﬁnal two chapters of the book speak to the issues
of tail classiﬁcation and, if classiﬁed, how to perform a reﬁned analysis for a
parametric tail model via Turing’s perspective. These issues are scientiﬁcally
relevant in many ﬁelds of study.
I intend this book to serve two groups of readers:
1) Textbook for graduate students. The material is suitable for a topic course
at the graduate level for students in Mathematics, Probability, Statistics,
Computer Science (Artiﬁcial Intelligence, Machine Learning, Big Data),
and Information Theory.
2) Reference book for researchers and practitioners. This book oﬀers an
informative presentation of many of the critical statistical issues of
modern data science and with updated new results. Both researchers and
practitioners will ﬁnd this book a good learning resource and enjoy the
many relevant methodologies and formulas given and explained under one
cover.
For a better ﬂow of the presentation, some of the lengthy but instructive proofs
are placed at the end of each chapter.
The seven chapters of this book may be naturally organized into three
groups. Group 1 includes Chapters 1 and 2. Chapter 1 gives an introduction to
Turing’s formula; and Chapter 2 translates Turing’s formula into a particular
perspective (referred to as Turing’s perspective) as embodied in a class of

xiv
Preface
indices (referred to as Generalized Simpson’s Indices). Group 1 may be
considered as the theoretical foundation of the whole book. Group 2 includes
Chapters 3–5. Chapter 3 takes Turing’s perspective into entropy estimation,
Chapter 4 takes it into diversity estimation, and Chapter 5 takes it into
estimation of various information indices. Group 2 may be thought of as
consisting of applications of Turing’s perspective. Chapters 6 and 7 make
up Group 3. Chapter 6 discusses the notion of tail on alphabets and oﬀers a
classiﬁcation of probability distributions. Chapter 7 oﬀers an application of
Turing’s formula in estimating parametric tails of random variables. Group 3
may be considered as a pathway to further research.
The material in this book is relatively new. In writing the book, I have made
an eﬀort to let the book, as well as its chapters, be self-contained. On the one
hand, I wanted the material of the book to ﬂow in a linearly coherent manner
for students learning it for the ﬁrst time. In this regard, readers may experience
a certain degree of repetitiveness in notation deﬁnitions, lemmas, and even
proofs across chapters. On the other hand, I wanted the book to go beyond
merely stating established results and referring to proofs published elsewhere.
Many of the mathematical results in the book have instructive value, and their
proofs indicate the depth of the results. For this reason, I have included many
proofs that might be judged overly lengthy and technical in a conventional
textbook, mostly in the appendices.
It is important to note that this book, as the title suggests, is essentially a
monograph on Turing’s formula. It is not meant to be a comprehensive learning
resource on topics such as estimation of entropy, estimation of diversity, or
estimation of information. Consequently, many worthy methodologies in these
topics have not been included. By no means do I suggest that the methodologies
discussed in this book are the only ones with scientiﬁc merit. Far from it, there
are many wonderful ideas proposed in the existing literature but not mentioned
among the pages of this book, and assuredly many more are yet to come.
I wish to extend my heartfelt gratitude to those who have so kindly allowed
me to bend their ears over the years. In particular, I wish to thank Hongwei
Huang, Stas Molchanov, and Michael Grabchak for countless discussions on
Turing’s formula and related topics; my students, Chen Chen, Li Liu, Ann
Stewart, and Jialin Zhang for picking out numerous errors in an earlier draft
of the book; and The University of North Carolina at Charlotte for granting
me a sabbatical leave in Spring 2015, which allowed me to bring this book
to a complete draft. Most importantly, I wish to thank my family, wife Carol,
daughter Katherine, and son Derek, without whose love and unwavering
support this book would not have been possible.
Zhiyi Zhang
May 2016
Charlotte, North Carolina

1
1
Turing’s Formula
Consider the population of all birds in the world along with all its diﬀerent
species, say {sk; k ≥1}, and denote the corresponding proportion distribution
by {pk; k ≥1} where pk is the proportion of the kth bird species in the popula-
tion. Suppose a random sample of n = 2000 is to be taken from the population,
and let the bird counts for the diﬀerent species be denoted by {Yk; k ≥1}. If it is
of interest to estimate p1 the proportion of birds of species s1 in the population,
then ̂p1 = Y1∕n is an excellent estimator; and similarly so is ̂pk = Yk∕n for pk for
every particular k.
To illustrate, consider a hypothetical sample of size n = 2000 with bird counts
given in Table 1.1 or a version rearranged in decreasing order of the observed
frequencies as in Table 1.2.
With this sample, one would likely estimate p1 by ̂p1 = 300∕2000 = 0.15 and
p2 by ̂p2 = 200∕2000 = 0.10, and so on.
The total number of bird species observed in this sample is 30. Yet it is clear
that the bird population must have more than just 30 diﬀerent species. A natural
follow-up question would then be as follows:
What is the total population proportion of birds belonging to species other
than those observed in the sample?
The follow-up question implies a statistical problem of estimation with a tar-
get, or estimand, being the collective proportion of birds of the species not
represented in the sample. For convenience, let this target be denoted as 𝜋0.
It is important to note that 𝜋0 is a random quantity depending on the sample
and therefore is not an estimand in the usual statistical sense. In the statistics
Statistical Implications of Turing’s Formula, First Edition. Zhiyi Zhang.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

2
Statistical Implications of Turing’s Formula
Table 1.1 Bird sample
sk
1
2
3
4
5
6
7
8
9
10
yk
300
200
300
200
100
100
100
100
0
100
sk
11
12
13
14
15
16
17
18
19
20
yk
100
80
70
0
30
50
6
1
2
1
sk
21
22
23
24
25
26
27
28
29
30
yk
1
1
0
0
1
1
1
1
1
1
sk
31
32
33
34
35
36
37
38
39
…
yk
50
100
1
1
0
0
0
0
0
…
Table 1.2 Rearranged bird sample
sk
1
3
2
4
5
6
7
8
10
11
yk
300
300
200
200
100
100
100
100
100
100
sk
32
12
13
16
31
15
17
19
18
20
yk
100
80
70
50
50
30
6
2
1
1
sk
21
22
25
26
27
28
29
30
33
34
yk
1
1
1
1
1
1
1
1
1
1
sk
9
14
23
24
35
36
37
38
39
…
yk
0
0
0
0
0
0
0
0
0
…
literature, 1 −𝜋0 is often referred to as the sample coverage of the population,
or in short sample coverage , or just coverage. Naturally 𝜋0 may be referred to as
the noncoverage.
The noncoverage 𝜋0 deﬁned with a random sample of size n is an interesting
quantity. It is sometimes interpreted as the “probability” of discovering a new
species, because, in a loose sense, the chance that the “next” bird is of a new, or
previously unobserved, species is 𝜋0. This interpretation is however somewhat
misleading. The main issue of such an interpretation is the lack of clariﬁcation
of the underlying experiment (and its sample space). Words such as “probabili-
ty” and “next” can only have meaning in a well-speciﬁed experiment. While it is
quite remarkable that 𝜋0 could be reasonably and nonparametrically estimated
by Turing’s formula, 𝜋0 is not a probability associated with the sample space of

Turing’s Formula
3
the experiment where the sample of size n is drawn. Further discussion of this
point is given in Section 1.5.
Turing’s formula, also sometimes known as the Good–Turing formula, is an
estimator of 𝜋0 introduced by Good (1953) but largely credited to Alan Turing.
Let N1 denote the number of species each of which is represented by exactly
one observation in a random sample of size n. Turing’s formula is given by T1 =
N1∕n. For the bird example given in Table 1.2, N1 = 12, n = 2000, and therefore
T1 = 0.006 or 0.6%.
1.1
Turing’s Formula
Let 𝒳= {𝓁k; k ≥1} be a countable alphabet with letters 𝓁1, … ; and let
p = {pk; k ≥1} be a probability distribution on 𝒳. Let Y = {Yk; k ≥1} be
the observed frequencies of the letters in an identically and independently
distributed (iid) random sample of size n. For any integer r, 1 ≤r ≤n, let the
number of letters in the alphabet that are each represented exactly r times in
the sample be denoted by
Nr =
∑
k≥1
1[Yk = r]
(1.1)
where 1[⋅] is the indicator function. Let the total probability associated with
letters that are represented exactly r −1 times in the sample be denoted by
𝜋r−1 =
∑
k≥1
pk1[Yk = r −1].
(1.2)
Of special interest is the case of r = 1 when
N1 =
∑
k≥1
1[Yk = 1]
and
𝜋0 =
∑
k≥1
pk1[Yk = 0]
(1.3)
representing, respectively,
N1: the number of letters of 𝒳that each appears exactly once; and
𝜋0: the total probability associated with the unobserved letters of 𝒳in an
iid sample of size n.
The following expression is known as Turing’s formula:
T1 = N1
n .
(1.4)
Turing’s formula has been extensively studied during the many years fol-
lowing its introduction by Good (1953) and has been demonstrated, mostly

4
Statistical Implications of Turing’s Formula
through numerical simulations, to provide a satisfactory estimate of 𝜋0 for a
wide range of distributions. These studies put forth a remarkable yet puzzling
implication: the total probability on the unobserved subset of 𝒳may be well
estimated nonparametrically. No satisfactory interpretation was given in the
literature until Robbins (1968).
Robbins’ Claim: Let 𝜋0 be deﬁned with an iid random sample of size n as in
(1.3). Let Turing’s formula be deﬁned with an augmented iid sample of size
n + 1 by adding one new iid observation to the original sample of size n; and let
the resulting estimator be denoted by T+
1 . Then T+
1 is an unbiased estimator of
𝜋0, in the sense of E (T+
1 −𝜋0) = 0.
Robbins’ claim is easily veriﬁed. Let N+
1 be the number of letters represented
exactly once in the augmented sample of size n + 1, with observed frequencies
{Y +
k ; k ≥1}.
E (N+
1 ) =
∑
k≥1
E (1[Y +
k = 1]) =
∑
k≥1
(n + 1)pk(1 −pk)n and
E (T+
1 ) = E(N+
1 ∕(n + 1)) =
∑
k≥1
pk(1 −pk)n.
On the other hand, with the sample of size n,
E (𝜋0) =
∑
k≥1
pkE (1[Yk = 0]) =
∑
k≥1
pk(1 −pk)n.
Hence, E (T+
1 −𝜋0) = 0.
Robbins’ claim provides an intuitive interpretation in the sense that
1) T+
1 is an unbiased and, therefore, a good estimator of 𝜋0;
2) the diﬀerence between T1 and T+
1 should be small; and therefore
3) T1 should be a good estimator of 𝜋0.
However, Robbins’ claim still leaves much to be desired. Suppose for a moment
that E (T+
1 −𝜋0) = 0 is an acceptable notion of a good estimator for T+
1 , it does
not directly address the performance of T1 as an estimator of 𝜋0, not even the
bias E (T1 −𝜋0), let alone other important statistical properties of T1 −𝜋0. In
short, the question whether Turing’s formula works at all, or if it does how well,
is not entirely resolved by Robbins’ claim.
In addition to the unusual characteristic that the estimand 𝜋0 is a random
variable, it is to be noted that both the estimator T1 and the estimand 𝜋0 con-
verge to zero in probability (see Exercise 6). Therefore, it is not suﬃciently
adequate to characterize the performance of T1 by the bias E (T1 −𝜋0) alone.
Any reasonable characterization of its performance must take into considera-
tion the vanishing rate of the estimand 𝜋0.
To put the discussion in perspective, a notion of performance is needed how-
ever minimal it may be. Let X denote a data sample. Let ̂𝜃n(X) be an estimator

Turing’s Formula
5
of a random estimand 𝜃n(X). Let
E ( ̂𝜃n(X) −𝜃n(X))
E (𝜃n(X))
(1.5)
whenever E (𝜃n(X)) ≠0 be referred to as the relative bias of ̂𝜃n(X) estimating
𝜃n(X).
Deﬁnition 1.1
̂𝜃n(X) is said to be an asymptotically relatively unbiased esti-
mator of 𝜃n(X) if
E ( ̂𝜃n(X) −𝜃n(X))
E (𝜃n(X))
→0
as n →∞, provided that E (𝜃n(X)) ≠0.
When 𝜃n(X) = 𝜃≠0 does not depend on sample data X, an asymptotically
relatively unbiased estimator is an asymptotically unbiased estimator in the
usual statistical sense.
Let K = ∑
k≥11[pk > 0] be referred to as the eﬀective cardinality of 𝒳, or just
simply cardinality whenever there is no risk of ambiguity. K < ∞implies that
there are only ﬁnite letters in 𝒳with positive probabilities.
Example 1.1
For any probability distribution {pk} such that 1 < K < ∞, Tur-
ing’s formula T1 is not an asymptotically relatively unbiased estimator of 𝜋0. This
is so because, letting p∧= min{pk > 0; k ≥1},
0 < E (T1 −𝜋0)
E (𝜋0)
=
∑
k≥1 pk(1 −pk)n−1 −∑
k≥1 pk(1 −pk)n
∑
k≥1 pk(1 −pk)n
=
∑
k≥1 p2
k(1 −pk)n−1
∑
k≥1 pk(1 −pk)n
=
∑
k≥1 p2
k[(1 −pk)∕(1 −p∧)]n−1
∑
k≥1 pk(1 −pk)[(1 −pk)∕(1 −p∧)]n−1
→
p2
∧
p∧(1 −p∧) =
p∧
(1 −p∧) ≠0
as n →∞.
Example 1.1 demonstrates that there exist distributions on 𝒳under which
Turing’s formula is not an asymptotically relatively unbiased estimator. In
retrospect, this is not so surprising. On a ﬁnite alphabet, the probabilities
associated with letters not covered by a large sample, 𝜋0, should rapidly
approach zero, at least on an intuitive level, as manifested by the fact that

6
Statistical Implications of Turing’s Formula
E (𝜋0) = ((1 −p∧)n) (see Exercise 7). On the other hand, the number of
singletons, that is, letters that are each observed exactly once in the sample
(∑
k≥11[Yk = 1]), should also approach zero rapidly, as manifested by the fact
that E(∑
k≥11[Yk = 1)]) = (n(1 −p∧))n. The fact that T1 and 𝜋0 are vanishing
at the same rate leads to the result of Example 1.1. This example also suggests
that perhaps Turing’s formula may not always be a good estimator of its
target 𝜋0, at least in its current form. However, Turing’s formula is not an
asymptotically relatively unbiased estimator only when K < ∞.
Theorem 1.1
Turing’s formula T1 is an asymptotically relatively unbiased
estimator of 𝜋0 if and only if K = ∞.
To prove Theorem 1.1, the following three lemmas are needed.
Lemma 1.1
Suppose {pk} is a strictly decreasing inﬁnite probability sequence
with pk > 0 for every k, k ≥1. Then, as n →∞,
rn =
∑∞
k=1 p2
k(1 −pk)n
∑∞
k=1 pk(1 −pk)n →0.
(1.6)
Proof: Let k0 be any speciﬁc index. rn may be re-expressed as follows.
0 < rn =
∑∞
k=1 p2
k(1 −pk)n
∑∞
k=1 pk(1 −pk)n
=
∑k0−1
k=1 p2
k(1 −pk)n + ∑∞
k=k0 p2
k(1 −pk)n
∑∞
k=1 pk(1 −pk)n
≤
∑k0−1
k=1 p2
k(1 −pk)n + pk0
∑∞
k=k0 pk(1 −pk)n
∑∞
k=1 pk(1 −pk)n
=
∑k0−1
k=1 p2
k(1 −pk)n
∑∞
k=1 pk(1 −pk)n +
pk0
∑∞
k=k0 pk(1 −pk)n
∑∞
k=1 pk(1 −pk)n
=∶rn,1 + rn,2
where the symbol “∶=” stands for the deﬁning equality, for example, “a ∶= b”
and “a =∶b” represent “a is deﬁned by b” and “a deﬁnes b” respectively.
For any given ε > 0, let k0 = min{k ∶pk < ε∕2}. First it is observed that
0 < rn,2 <
pk0
∑∞
k=k0 pk(1 −pk)n
∑∞
k=k0 pk(1 −pk)n
= pk0 < ε∕2.
Next it is observed that, letting qk = 1 −pk,
0 < rn,1 =
∑k0−1
k=1 p2
k(1 −pk)n
∑∞
k=1 pk(1 −pk)n <
∑k0−1
k=1 p2
k(1 −pk)n
pk0(1 −pk0)n
=
∑k0−1
k=1 p2
k(qk∕qk0)n
pk0
.

Turing’s Formula
7
In the numerator of the above-mentioned last expression, 0 < qk∕qk0 < 1 for
every k = 1, … , k0 −1, therefore (qk∕qk0)n →0 as n →∞. In addition, since
there are ﬁnite terms in the summation, rn,1 →0. That is to say that, for any
ε > 0 as given earlier, there exists an integer Nε such that for every n > Nε, rn,1 <
ε∕2, or rn ≤rn,1 + rn,2 < ε∕2 + ε∕2 = ε.
◽
Lemma 1.2
Suppose {pk} is a strictly decreasing inﬁnite probability sequence
with pk > 0 for every k, k ≥1. Then, as n →∞,
r∗
n =
∑∞
k=1 p2
k(1 −pk)n−1
∑∞
k=1 pk(1 −pk)n
→0.
(1.7)
Proof: By Lemma 1.1,
rn = r∗
n −
∑∞
k=1 p3
k(1 −pk)n−1
∑∞
k=1 pk(1 −pk)n
→0,
and therefore it suﬃces to show that
∑∞
k=1 p3
k(1 −pk)n−1
∑∞
k=1 pk(1 −pk)n
→0.
Noting p∕(1 −p) is an increasing function for p ∈(0, 1), for each pk
of
any
given
distribution
{pk; k ≥1},
pk∕(1 −pk) ≤p∨∕(1 −p∨)
where
p∨= max{pk; k ≥1}. Therefore,
∑∞
k=1 p3
k(1 −pk)n−1
∑∞
k=1 pk(1 −pk)n
=
∑∞
k=1 p2
k(1 −pk)n (
pk
1−pk
)
∑∞
k=1 pk(1 −pk)n
≤
(
p∨
1 −p∨
) ∑∞
k=1 p2
k(1 −pk)n
∑∞
k=1 pk(1 −pk)n
=
(
p∨
1 −p∨
)
rn →0.
◽
With a little adjustment, the proof for Lemma 1.1 goes through with any inﬁ-
nite probability sequence that is not necessarily strictly decreasing.
Lemma 1.3
If {pk} is a probability sequence with positive mass on each letter
in an inﬁnite subset of 𝒳, then rn →0 and r∗
n →0 as n →∞where rn and r∗
n are
as in (1.6) and (1.7).
The proof of Lemma 1.3 is left as an exercise (see Exercise 8).
Proof of Theorem 1.1: The suﬃciency part of the theorem immediately follows
Lemma 1.3. The necessity part of the theorem follows the fact that if K < ∞,
then Example 1.1 provides a counterargument.
◽

8
Statistical Implications of Turing’s Formula
Both random quantities, T1 and 𝜋0, converge to zero in probability. One
would naturally wonder if a naive estimator, say ̂𝜋0 = 1∕n, could perform
reasonably well in estimating 𝜋0. In view of (1.5), the relative bias of ̂𝜋0 = 1∕n,
when K < ∞as in Example 1.1, is
E ( ̂𝜋0 −𝜋0)
E (𝜋0)
=
1∕n
∑
k≥1 p2
k(1 −pk)n−1 −1 >
1∕n
(1 −p∧)n−1 −1 →∞
indicating ̂𝜋0 as a very inadequate estimator. In fact, since the decreasing rate
of E(𝜋0) →0 could vary over a wide range of diﬀerent distributions on 𝒳, for
any naive estimator of the form ̂𝜋0 = g(n) →0, there exist arbitrarily many dis-
tributions under which the relative bias of such a ̂𝜋0 diverges to inﬁnity. This
fact will become clearer in Chapter 6.
Consider next the variance of T1 −𝜋0.
𝜎2
n = Var (T1 −𝜋0) = E ((T1 −𝜋0)2) −(E (T1 −𝜋0) )2
= E (T2
1 −2T1 𝜋0 + 𝜋2
0) −
[
∑
k≥1
p2
k(1 −pk)n−1
]2
= E
(
n−2 ∑
i≥1
∑
j≥1
1[Yi = 1]1[Yj = 1]
)
−2E
(
n−1 ∑
i≥1
∑
j≥1
pj1[Yi = 1]1[Yj = 0]
)
+ E
(
∑
i≥1
∑
j≥1
pi pj1[Yi = 0]1[Yj = 0]
)
−
[
∑
k≥1
p2
k(1 −pk)n−1
]2
= n−2
(
n
∑
i≥1
pi(1 −pi)n−1 + n(n −1)
∑
i≠j,i≥1,j≥1
pi pj(1 −pi −pj)n−2
)
−2
(
∑
i≠j,i≥1,j≥1
pi pj(1 −pi −pj)n−1
)
+
(
∑
i≥1
p2
i (1 −pi)n +
∑
i≠j,i≥1,j≥1
pi pj(1 −pi −pj)n
)
−
[
∑
k≥1
p2
k(1 −pk)n−1
]2
= 1
n
(
∑
k≥1
pk(1 −pk)n−1 −
∑
i≠j,i≥1,j≥1
pi pj(1 −pi −pj)n−2
)
+
∑
i≠j,i≥1,j≥1
pi pj(1 −pi −pj)n−2(pi + pj)2 +
∑
i≥1
p2
k(1 −pk)n−1

Turing’s Formula
9
−
∑
k≥1
p3
k(1 −pk)n−1 −
[
∑
k≥1
p2
k(1 −pk)n−1
]2
= 1
n
∑
k≥1
pk(1 −pk)n−1
−1
n
∑
i≥1
∑
j≥1
pi pj(1 −pi −pj)n−2 + 1
n
∑
i≥1
p2
i (1 −2pi)n−2
+
∑
i≥1
∑
j≥1
pi pj(1 −pi −pj)n−2(pi + pj)2 −4
∑
k≥1
p4
k(1 −2pk)n−2
+
∑
k≥1
p2
k(1 −pk)n −
[
∑
k≥1
p2
k(1 −pk)n−1
]2
.
(1.8)
It can be veriﬁed that limn→∞𝜎2
n = 0 (see Exercise 10).
Denote 𝜁2,n−1 = ∑
k≥1 p2
k(1 −pk)n−1. By Chebyshev’s theorem, for any m > 0,
P (|(T1 −𝜋0) −𝜁2,n−1| ≤m𝜎n) ≥1 −1
m2
or
P (T1 −𝜁2,n−1 −m𝜎n ≤𝜋0 ≤T1 −𝜁2,n−1 + m𝜎n) ≥1 −1
m2 .
(1.9)
Replacing the {pk} in 𝜁2,n−1 and 𝜎n as in (1.8) by {̂pk} based on a sample and
denoting the resulting values as ̂𝜁2,n−1, and ̂𝜎n respectively, one may choose to
consider
(T1 −̂𝜁2,n−1
) ± m̂𝜎n.
(1.10)
as a conservative and approximate conﬁdence interval for 𝜋0 at a conﬁdence
level of 1 −1∕m2.
Example 1.2
Use the data in Table 1.2 to ﬁnd a 95% conﬁdence interval for 𝜋0.
For 1 −1∕m2 = 0.95, m =
√
20 = 4.4721. Using n = 2000, T1 = 0.006, ̂𝜁2,n−1 =
1.2613 × 10−6 and ̂𝜎n = 0.0485, a conservative and approximate 95% conﬁdence
interval according to (1.10) is (−0.2109, 0.2229).
Remark 1.1
While the statement of (1.9) is precisely correct under any distri-
bution {pk} on 𝒳, (1.10) is a very weakly justiﬁed conﬁdence interval for 𝜋0. In
addition to the fact that the conﬁdence level associated with (1.10) is a conser-
vative lower bound, ̂𝜁2,n−1 and ̂𝜎n are merely crude approximations to 𝜁2,n−1 and
𝜎n, respectively. One must exercise caution when (1.10) is used.
Remark 1.2
By Deﬁnition 1.1, it is clear that a good estimator is one with
a bias, namely E (T1 −𝜋0), that vanishes faster than the expected value of the
target, namely E (𝜋0), as n increases indeﬁnitely. Given that the bias is 𝜁2,n−1 =
∑
k≥1 p2
k(1 −pk)n−1, one could possibly entertain the idea of a modiﬁcation to

10
Statistical Implications of Turing’s Formula
Turing’s formula T1 to reduce the bias, possibly to the extent such that, in view
of Example 1.1, a modiﬁed Turing’s formula may become asymptotically and
relative unbiased even for distributions with K < ∞. As it turns out, there are
many such modiﬁcations, one of which is presented in Section 1.4.
1.2
Univariate Normal Laws
To better understand several important normal laws concerning the asymptotic
behavior of T1 −𝜋0, consider ﬁrst a slightly more general probability model on
the alphabet 𝒳, namely {pk,n; k ≥1}, instead of {pk; k ≥1}. The subindex n
in pk,n suggests that the probability distribution could, but not necessarily, be
dynamically changing as n changes. In this sense, {pk; k ≥1} is a special case
of {pk,n; k ≥1}.
Thirty years after Good (1953) introduced Turing’s formula, Esty (1983)
established an asymptotic normal law of T1 −𝜋0 as stated in Theorem 1.2.
Theorem 1.2
Let the probability distribution {pk,n; k ≥1} be such that
1) E (N1∕n) →c1 where 0 < c1 < 1, and
2) E (N2∕n) →c2 ≥0.
Then
n (T1 −𝜋0)
√
N1 + 2N2 −N2
1∕n
L
−−−→N(0, 1)
(1.11)
where N1 and N2 are given by (1.1), 𝜋0 is given by (1.2), and T1 is given by (1.4).
Example 1.3
Let {pk,n; k = 1, … , n} be such that pk,n = 1∕n for every k.
E(N1∕n) =
∑
k≥1
E(1[Yk = 1])∕n =
∑
k≥1
P(Yk = 1)∕n
=
∑
k≥1
npk,n(1 −pk,n)n−1∕n = (1 −1∕n)n−1 →e−1;
E(N2∕n) =
∑
k≥1
E(1[Yk = 2])∕n =
∑
k≥1
P(Yk = 2)∕n
=
∑
k≥1
[n(n −1)∕2!] p2
k,n(1 −pk,n)n−2∕n
= (1 −1∕n)n−1∕2 →1
2e−1.
The conditions of Theorem 1.2 are satisﬁed and therefore (1.11) holds.
Example 1.3 has a subtle but comforting implication in applications: there
exist probability distributions on ﬁnite alphabets of cardinality Kn (ﬁnite
at least at the time a sample of size n is taken) such that the normality of

Turing’s Formula
11
Theorem 1.2 holds. In this sense, Theorem 1.2 provides a theoretical justiﬁca-
tion for approximate inferential procedures derived from (1.11) to be used in
applications where the cardinalities of alphabets are often more appropriately
assumed to be ﬁnite.
The asymptotic normality of Turing’s formula given in (1.11) was the ﬁrst of
its kind and the proof of Theorem 1.2 was a technical trailblazer. However, the
suﬃcient condition in Theorem 1.2 is overly restrictive and consequently no
ﬁxed distribution {pk; k ≥1} satisﬁes the suﬃcient condition. The last fact is
demonstrated by Lemma 1.4.
Lemma 1.4
Let {pk; k ≥1} be any probability distribution on alphabet 𝒳.
Then
a) limn→∞E (N1∕n) = 0, and
b) limn→∞E (N2∕n) = 0.
Proof: For Part (a), noting ﬁrst E (N1∕n) = ∑
k≥1 pk(1 −pk)n−1 and then
pk(1 −pk)n−1 ≤pk and ∑
k≥1 pk = 1, by the dominated convergence theorem,
lim
n→∞E (N1∕n) =
∑
k≥1
pk lim
n→∞(1 −pk)n−1 = 0.
For Part (b), noting ﬁrst E (N2∕n) = (1∕2)∑
k≥1(n −1)p2
k(1 −pk)n−2 and then
that (n −1)p(1 −p)n−2 attains its maximum at p = 1∕(n −1), one has
(n −1)pk(1 −pk)n−2 ≤[(n −2)∕(n −1)]n−2 < 1.
Again by the dominated convergence theorem,
lim
n→∞E (N2∕n) = (1∕2)
∑
k≥1
lim
n→∞[(n −1)p2
k(1 −pk)n−2] = 0.
◽
Zhang and Huang (2008) modiﬁed Esty’s suﬃcient condition in Theorem 1.2
and established Theorems 1.3 and 1.4 for ﬁxed probability distributions
{pk; k ≥1}.
Condition 1.1
There exists a 𝛿∈(0, 1∕4) such that, as n →∞,
1) n1−4𝛿E (N1∕n) →c1 ≥0,
2) n1−4𝛿E (N2∕n) →c2∕2 ≥0, and
3) c1 + c2 > 0.
Theorem 1.3
If a given probability distribution {pk; k ≥1} satisﬁes Condi-
tion 1.1, then
n1−2𝛿(T1 −𝜋0)
L
−−−→N(0, c1 + c2).
(1.12)

12
Statistical Implications of Turing’s Formula
The proof of Theorem 1.3 is lengthy and therefore is omitted here. Interested
readers may refer to Zhang and Huang (2008) for details. The rate of the weak
convergence of (1.12) plays an important role throughout this book and there-
fore is assigned a special notation for convenience
g(n, 𝛿) = n1−2𝛿.
(1.13)
Let
̂c1 = g2(n, 𝛿)
n2
N1
and
̂c2 = 2g2(n, 𝛿)
n2
N2.
(1.14)
Lemma 1.5
Under Condition 1.1,
̂c1
p
−−−→c1
and
̂c2
p
−−−→c2.
The proof of Lemma 1.5 is left as an exercise (see Exercise 11).
Theorem 1.4 is a corollary of Theorem 1.3.
Theorem 1.4
If a given probability distribution {pk; k ≥1} satisﬁes Condi-
tion 1.1, then
1)
n (T1 −𝜋0)
√
E(N1) + 2E(N2)
L
−−−→N(0, 1), and
(1.15)
2)
n (T1 −𝜋0)
√
N1 + 2N2
L
−−−→N(0, 1).
(1.16)
Proof: For Part 1,
n (T1 −𝜋0)
√
E(N1) + 2E(N2)
=
n
n1−2𝛿
√c1 + c2
√
E(N1) + 2E(N2)
g(n, 𝛿) (T1 −𝜋0)
√c1 + c2
=
(
√c1 + c2
√
n−4𝛿E(N1) + 2n−4𝛿E(N2)
) [
g(n, 𝛿) (T1 −𝜋0)
√c1 + c2
]
.
In the above-mentioned last expression, the ﬁrst factor converges to 1 by Con-
dition 1.1 and the second factor converges weakly to the standard normal dis-
tribution by Theorem 1.3. The result of (1.15) follows Slutsky’s theorem.
For Part 2,
n (T1 −𝜋0)
√
N1 + 2N2
=
n
n1−2𝛿
√c1 + c2
√
N1 + 2N2
g(n, 𝛿) (T1 −𝜋0)
√c1 + c2
=
(
√c1 + c2
√
n−4𝛿N1 + 2n−4𝛿N2
) [
g(n, 𝛿) (T1 −𝜋0)
√c1 + c2
]
.

Turing’s Formula
13
In the above-mentioned last expression, the ﬁrst factor converges to 1 by
Lemma 1.5 and the second factor converges weakly to the standard normal
distribution by Theorem 1.3. The result of (1.16) follows Slutskys theorem.
◽
An interesting feature of Theorem 1.4 is that, although Condition 1.1 calls
for the existence of a 𝛿∈(0, 1∕4), the results of Theorem 1.4 do not depend
on such a 𝛿, as evidenced by its total absence in (1.15) and (1.16). Also to be
noted is the fact that the conditions of Theorems 1.2 and 1.3 do not overlap.
The condition of Theorem 1.2 essentially corresponds to 𝛿= 1∕4 in the form of
the condition of Theorem 1.4, which is on the boundary of the interval (0, 1∕4)
but not in the interior of the interval.
Remark 1.3
The value of 𝛿represents a tail property of the underlying dis-
tribution. The fact that (1.15) does not depend on 𝛿may be considered as a
consequence of a domain of attraction of certain types of distributions sharing
the same asymptotic normality via Turing’s formula. Discussion of domains of
attraction on alphabets is given in Chapter 6.
The following example shows that the class of probability distributions satis-
fying the condition of Theorem 1.4 is in fact nonempty. It also gives a descrip-
tion of the bulk of distributions in the family that supports the normal laws in
(1.15) and (1.16).
Example 1.4
If pk = c𝜆k−𝜆for all k ≥1 where 𝜆> 1 and c𝜆is such that
∑
k≥1 pk = 1, then {pk; k ≥1} satisﬁes the conditions of Theorem 1.4.
To see that the claim of Example 1.4 is true, the following two lemmas are
needed.
Lemma 1.6
(Euler–Maclaurin)
For each n, n = 1, 2, … , let fn(x) be a con-
tinuous function of x on [x0, ∞) where x0 is a positive integer. Suppose there exists
a sequence of real values, xn satisfying xn ≥x0, such that fn(x) is increasing on
[x0, xn] and decreasing on [xn, ∞). If fn(x0) →0 and fn(xn) →0, then
lim
n→∞
∑
k≥x0
fn(k) = lim
n→∞∫
∞
x0
fn(x) dx.
Proof: It can be veriﬁed that
∑
x0≤k≤xn
fn(k) −fn(xn) ≤∫
xn
x0
fn(x) dx ≤
∑
x0+1≤k<xn
fn(k) + fn(xn)
and
∑
k>xn
fn(k) −fn(xn) ≤∫
∞
xn
fn(x) dx ≤
∑
k≥xn
fn(k) + fn(xn).
Adding the corresponding parts of the above-mentioned two expressions and
taking limits give

14
Statistical Implications of Turing’s Formula
lim
n→∞
∞
∑
k=x0
fn(k) −2 lim
n→∞fn(xn) ≤lim
n→∞∫
∞
x0
fn(x) dx
≤lim
n→∞
∞
∑
k=x0
fn(k) −lim
n→∞fn(x0) + 2 lim
n→∞fn(xn).
The desired result follows from the conditions of the lemma.
◽
Condition 1.2
There exists a 𝛿∈(0, 1∕4) such that, as n →∞,
1) n1−4𝛿∑
k≥1 pk e−npk →c1 ≥0,
2) n2−4𝛿∑
k≥1 p2
ke−npk →c2 ≥0, and
3) c1 + c2 > 0.
Lemma 1.7
Conditions 1.1 and 1.2 are equivalent.
The proof of Lemma 1.7 is nontrivial and is found in Zhang (2013a), where a
more general statement is proved in the appendix.
In Example 1.4, let 𝛿= (4𝜆)−1 and therefore g(n, 𝛿) = n1−2𝛿= n1−1
2𝜆. By
Lemma 1.7, it suﬃces to check Condition 1.2 instead of Condition 1.1:
g2(n, 𝛿)
n
∑
k≥1
pk e−npk = n1−1
𝜆
∞
∑
k=k0
c𝜆k−𝜆e−nc𝜆k−𝜆=
∞
∑
k=k0
fn(k)
where
fn(x) = n1−1
𝜆c𝜆x−𝜆e−nc𝜆x−𝜆.
Since it is easily veriﬁed that
f ′
n(x) = −𝜆c𝜆n1−1
𝜆x−(𝜆+1) (1 −nc𝜆x−𝜆) e−nc𝜆x−𝜆,
fn(x) increases and decreases, respectively, over the following two sets
[1, (nc𝜆)1∕𝜆]
and
[(nc𝜆)1∕𝜆, ∞) .
Let x0 = 1 and xn = (nc𝜆)1∕𝜆. It is clear that fn(x0) →0 and
fn(xn) = n1−1
𝜆c𝜆(nc𝜆)−1e−nc𝜆(nc𝜆)−1
= n1−1
𝜆c𝜆(nc𝜆)−1e−1
= e−1n−1∕𝜆→0.
Invoking the Euler–Maclaurin lemma, with changes of variable t = x−𝜆and
then s = nc𝜆t,
n1−1
𝜆
∞
∑
k=1
pk e−npk ≃∫
∞
x0
n1−1
𝜆c𝜆x−𝜆e−nc𝜆x−𝜆dx

Turing’s Formula
15
= c𝜆
𝜆∫
x−𝜆
0
0
n1−1
𝜆t−1
𝜆e−nc𝜆tdt
= c𝜆
𝜆n1−1
𝜆∫
x−𝜆
0
0
(nc𝜆t)−1
𝜆(nc𝜆)−1+ 1
𝜆e−nc𝜆td(nc𝜆t)
= c𝜆
𝜆n1−1
𝜆(nc)−1+ 1
𝜆∫
nc𝜆x−𝜆
0
0
s−1
𝜆e−sds
= (c𝜆)
1
𝜆
𝜆
n0
∫
nc𝜆x−𝜆
0
0
s−1
𝜆e−sds
= (c𝜆)
1
𝜆
𝜆
∫
nc𝜆x−𝜆
0
0
s
(
1−1
𝜆
)
−1e−sds
= (c𝜆)
1
𝜆
𝜆
Γ
(
1 −1
𝜆
) ⎡
⎢
⎢
⎢⎣
1
Γ
(
1 −1
𝜆
) ∫
nc𝜆x−𝜆
0
0
s
(
1−1
𝜆
)
−1e−sds
⎤
⎥
⎥
⎥⎦
→(c𝜆)
1
𝜆
𝜆
Γ (1 −𝜆−1) > 0
where “≃” indicates equality in limit as n →∞.
Thus, the suﬃcient condition of Theorem 1.4 is veriﬁed and therefore the
claim in Example 1.4 is true.
It is important to note that Condition 1.1 (and therefore Condition 1.2) is in
fact a tail property of the underlying distribution, that is, the values of pk for
k < k0 for any arbitrarily ﬁxed positive integer k0 are completely irrelevant to
whether these conditions hold. To see this fact, it suﬃces to note, for example,
E (N1∕n) =
∑
k≥1
pk(1 −pk)n−1 =
∑
k<k0
pk(1 −pk)n−1 +
∑
k≥k0
pk(1 −pk)n−1.
The ﬁrst of the two terms in the above-mentioned last expression converges to
zero exponentially fast, and therefore the question whether E (N1∕n) converges
to zero at a power decaying rate rests entirely with the second of the two terms.
A similar argument applies to E (N2∕n).
Example 1.5
Let {pk} be such that pk is not speciﬁed for k < k0 and pk = ck−𝜆
for k ≥k0 for some integer k0, where 𝜆> 1 and c are such that ∑
k≥1 pk = 1, then
{pk; k ≥1} satisﬁes Condition 1.1.
The proof of Example 1.5 is left as an exercise (see Exercise 12).
Using Theorem 1.4, an approximate 100(1 −𝛼)% conﬁdence interval for 𝜋0
can be devised:
N1
n ± z𝛼∕2
√
N1 + 2N2
n
.
(1.17)

16
Statistical Implications of Turing’s Formula
Example 1.6
Use the data in Table 1.1 to construct a 95% conﬁdence interval
for 𝜋0. With n = 2000, N1 = 12, and N2 = 1 by (1.17), a 95% conﬁdence interval
for 𝜋0 is
N1
n ± z𝛼∕2
√
N1 + 2N2
n
=
12
2000 ± 1.96
√
12 + 2 × 1
2000
= 0.006 ± 0.0037 = (0.0023, 0.0097).
While the suﬃcient conditions of asymptotic normality in Theorems 1.2 and
1.4 are often easy to check, they are both superseded by a necessary and suﬃ-
cient condition given by Zhang and Zhang (2009). Let
s2
n = n
∑
k≥1
pk,n e−npk,n + n2 ∑
k≥1
p2
k,ne−npk,n,
where {pk,n} is a probability distribution on the alphabet 𝒳. Consider
Condition 1.3 below.
Condition 1.3
As n →∞,
1) E(N1) + E(N2) →∞, and
2)
(
n
sn
)2∑
k≥1 p2
k,ne−npk,n1[npk,n > εsn] →0, for any ε > 0.
The following theorem is due to Zhang and Zhang (2009).
Theorem 1.5
A probability distribution {pk,n} on 𝒳satisﬁes Condition 1.3 if
and only if
n(T1 −𝜋0)
√
E(N1) + 2E(N2)
L
−−−→N(0, 1).
(1.18)
Condition 1.3 is less restrictive than either of the conditions of Theorems 1.2
and 1.4. Condition 1.3 is quite subtle and can be diﬃcult to verify for a particular
distribution. However, it does identify some distributions that do not admit the
asymptotic normality (1.18) for Turing’s formula.
Example 1.7
If {pk} is a probability distribution on a ﬁnite alphabet, then the
normality of (1.18) does not hold.
To see this, one only needs to check the ﬁrst part of Condition 1.3. Let p∧=
min{pk > 0; k ≥1}:
E(N1) = n
∑
k≥1
pk(1 −pk)n−1 ≤n(1 −p∧)n−1 →0,

Turing’s Formula
17
E(N2) = n(n −1)
2
∑
k≥1
p2
k(1 −pk)n−2 ≤n2(1 −p∧)n−2 →0,
and therefore E(N1) + E(N2) →0.
Example 1.8
If {pk} is such that pk = c0 e−k where c0 = e −1 for all k ≥1,
then the normality of (1.18) does not hold.
A veriﬁcation of the statement in Example 1.8 will be a lengthy and tedious
one and therefore is left as an exercise. (See Exercise 14. This is a diﬃcult
exercise and readers may wish to defer it until Chapter 6.)
Theorem 1.6 below is a corollary to Theorem 1.5.
Theorem 1.6
If a probability distribution {pk,n} on 𝒳satisﬁes Condition 1.3,
then
n(T1 −𝜋0)
√
N1 + 2N2
L
−−−→N(0, 1)
(1.19)
Remark 1.4
In view of Theorems 1.4 and 1.5 and of Examples 1.4, 1.7, and
1.8, one might be tempted to reach a conclusion that Turing’s formula would
admit asymptotic normality when the underlying distribution has a thick tail.
As it turns out, the notion of a “thin tail” or a “thick tail” is not easily deﬁned
on an alphabet 𝒳. The natural and intuitive notion of a tail of a distribution
on the real line does not carry over well to a countable alphabet. The asymptotic
behavior of quantities, such as E(N1), could be quite intricate and will be dealt
with in all other subsequent chapters, in particular Chapter 6.
Remark 1.5
Theorem 1.5 is established (and is stated earlier) under a gen-
eral distribution family {pk,n} on 𝒳whose members may dynamically change
with the sample size n. The theorem remains valid if restricted to the subclass of
ﬁxed distributions {pk}. For pedagogical and statistical simplicity, the distribu-
tions on 𝒳considered in the remainder of this book are all members of the ﬁxed
family, unless otherwise speciﬁed.
In the same spirit of the relative bias given in Deﬁnition 1.1, one may also be
interested in various asymptotic aspects of the following random variable
T1 −𝜋0
𝜋0
= T1
𝜋0
−1.
Condition 1.4
There exists a 𝛿∈(0, 1∕4) such that, as n →∞,
1) n1−4𝛿E (N1∕n) →c1 > 0, and
2) n1−4𝛿E (N2∕n) →c2
2 ≥0.

18
Statistical Implications of Turing’s Formula
Condition 1.4 is slightly more restrictive than Condition 1.1 in that c1 in Part
1 of Condition 1.4 is required to be strictly greater than zero.
Theorem 1.7
If {pk; k ≥1} satisﬁes Condition 1.4, then
n2𝛿
(T1 −𝜋0
𝜋0
)
= n2𝛿
(T1
𝜋0
−1
)
L
−−−→N(0, 𝜎2)
(1.20)
where 𝜎2 = (c1 + c2)∕c2
1.
Proof: Note ﬁrst
n2𝛿
(T1 −𝜋0
𝜋0
)
=
(
n2𝛿
g(n, 𝛿)𝜋0
)
[g(n, 𝛿) (T1 −𝜋0) ]
=
(
1
n1−4𝛿𝜋0
)
[g(n, 𝛿) (T1 −𝜋0) ] .
By Theorem 1.3, g(n, 𝛿)(T1 −𝜋0)
L
−−−→N(0, c1 + c2), and therefore it suﬃces to
show that n1−4𝛿𝜋0
p→c1 > 0. Toward that end, consider ﬁrst
E (n1−4𝛿𝜋0
) = n1−4𝛿∑
k≥1
pk(1 −pk)n
=
(
n
n + 1
)1−4𝛿
(n + 1)1−4𝛿∑
k≥1
pk(1 −pk)n
=
(
n
n + 1
)1−4𝛿[
g2(n + 1, 𝛿)
n + 1
∑
k≥1
pk(1 −pk)n
]
.
In the above-mentioned last expression, the ﬁrst factor converges to 1 and the
second factor converges to c1 due to Part 1 of Condition 1.4. Therefore, it is
established that E (n1−4𝛿𝜋0) →c1 > 0.
Consider next
Var (n1−4𝛿𝜋0) = n2−8𝛿[E(𝜋2
0) −(E(𝜋0))2]
= n2−8𝛿
⎡
⎢
⎢⎣
E
⎛
⎜
⎜⎝
(
∑
k≥1
pk1[Yk = 0
)2⎞
⎟
⎟⎠
−
(
∑
k≥1
pk(1 −pk)n
)2⎤
⎥
⎥⎦
= n2−8𝛿
[
∑
k≥1
p2
k(1 −pk)n +
∑
i≠j
pi pj(1 −pi −pj)n
−
(
∑
k≥1
pk(1 −pk)n
)2⎤
⎥
⎥⎦

Turing’s Formula
19
= n2−8𝛿
(
∑
k≥1
p2
k(1 −pk)n −
∑
k≥1
p2
k(1 −pk)2n
)
+ n2−8𝛿
(
∑
i≠j
pi pj(1 −pi −pj)n
−
∑
i≠j
pi pj(1 −pi)n(1 −pj)n
)
≤n2−8𝛿∑
k≥1
p2
k(1 −pk)n.
The last inequality holds since for every (pi, pj),
1 −pi −pj ≤1 −pi −pj + pi pj = (1 −pi)(1 −pk).
However, by Condition 1.4,
n2−8𝛿∑
k≥1
p2
k(1 −pk)n =
[
n2−8𝛿
(n + 2)2−4𝛿
] [
2(n + 2)2
(n + 2)(n + 1)
]
×
[
g2(n + 2, 𝛿)
(n + 2)2
∑
k≥1
(n + 2
2
)
p2
k(1 −pk)n
]
→0,
since the three factors in the last expression converge, respectively, to zero at
the rate of (n−4𝛿), 2, and c2∕2. It follows that Var (n1−4𝛿𝜋0) →0, and therefore
n1−4𝛿𝜋0
p→c1. The result of the theorem immediately follows Slutsky’s theorem.
◽
Remark 1.6
The respective weak convergences of g(n, 𝛿)(T1 −𝜋0) and
n2𝛿(T1∕𝜋0 −1) provide two diﬀerent perspectives in terms of inference regarding
𝜋0. It may be interesting to note that, under Condition 1.4, the rates of weak
convergence of (T1 −𝜋0) and (T1∕𝜋0 −1) are, respectively, g(n, 𝛿) = n1−2𝛿and
n2𝛿. Since 𝛿∈(0, 1∕4), it follows that 2𝛿∈(0, 1∕2) and that 1 −2𝛿∈[1∕2, 1),
and therefore n1−2𝛿always increases at a faster rate than n2𝛿. The faster
convergence of (T1 −𝜋0) suggests that the associated additive perspective of
Turing’s formula (as opposed to the multiplicative perspective associated with
T1∕𝜋0) may perhaps be intrinsically more eﬀective.
Corollary 1.1
If {pk; k ≥1} satisﬁes Condition 1.4, then
1)
E(N1) (T1∕𝜋0 −1)
√
E(N1) + 2E(N2)
L
−−−→N(0, 1), and
(1.21)
2)
N1 (T1∕𝜋0 −1)
√
N1 + 2N2
L
−−−→N(0, 1).
(1.22)

20
Statistical Implications of Turing’s Formula
Proof: For Part 1, by Theorem 1.7 and Condition 1.4,
n2𝛿(T1∕𝜋0 −1)
√
E(N1)∕n4𝛿+2E(N2)∕n4𝛿
(E(N1)∕n4𝛿)2
= (T1∕𝜋0 −1)
√
E(N1)+2E(N2)
(E(N1))2
= E(N1) (T1∕𝜋0 −1)
√
E(N1) + 2E(N2)
L
−−−→N(0, 1).
For Part 2, the result immediately follows the facts that N1∕n4𝛿p→c1 and that
N2∕n4𝛿p→c2∕2 of Lemma 1.5.
◽
Once again, while the convergence rate of T1∕𝜋0 −1 depends on the under-
lying distribution, the left-hand side of Part 2 of Corollary 1.1 is a function of
observable components except 𝜋0, thus allowing statistical inference regarding
𝜋0. Using (1.22), it can be veriﬁed that an approximate 100 × (1 −𝛼)% conﬁ-
dence interval for 𝜋0 is
T1
1 + z𝛼∕2
√
N1+2N2
N1
≤𝜋0 ≤
T1
1 −z𝛼∕2
√
N1+2N2
N1
(1.23)
(see Exercise 24).
Example 1.9
Use (1.23) and the data in Table 1.1 to construct a 95% conﬁ-
dence intervalfor 𝜋0. With n = 2000, N1 = 12, N2 = 1, and T1 = 0.006,
z𝛼∕2
√
N1 + 2N2
N1
= 1.96
√
12 + 2
12
= 0.6111,
the 95% conﬁdence interval is
(
0.006
1 + 0.6111,
0.006
1 −0.6111
)
= (0.0037, 0.0150).
In comparing the two 95% conﬁdence intervals of Examples 1.6 and 1.9, that
is, (0.0023, 0.0097) and (0.0037, 0.0150), respectively, it may be interesting to
note that one is larger than the other, and that the smaller interval is not entirely
nested in the larger interval.
Lemma 1.8
Let Xn be a sequence of random variables satisfying
h(n)(Xn −𝜃)
L
−−−→N(0, 𝜎2)
where h(⋅) is a strictly increasing function satisfying limt→∞h(t) = ∞, 𝜃and
𝜎> 0 are two ﬁxed constants. Then, for any given ε > 0,
lim
n→∞P(|Xn −𝜃| < ε) = 1.

Turing’s Formula
21
Proof: Without loss of generality, assume that 𝜎= 1:
P(|Xn −𝜃| < ε) = P(−ε < Xn −𝜃< ε)
= P(Xn −𝜃< ε) −P(Xn −𝜃≤−ε)
= P(h(n)(Xn −𝜃) < h(n)ε) −P(h(n)(Xn −𝜃) ≤−h(n)ε).
It is desired to show that, for any given 𝛼, there exists a suﬃciently large inte-
ger N such that for all n > N, P(|Xn −𝜃| < ε) > 1 −𝛼.
Toward that end, let z𝛼∕2 be the 100(1 −𝛼∕2)th percentile and N1 =
⌈g−1(z𝛼∕2∕ε)⌉where h−1(⋅) is the inverse function of h(⋅). For any n > N1,
noting n > h−1(z𝛼∕2∕ε),
P(|Xn −𝜃| < ε) ≥P(h(n)(Xn −𝜃) < z𝛼∕2) −P(h(n)(Xn −𝜃) ≤−z𝛼∕2).
On the other hand, by the given asymptotic normality, there exist suﬃciently
large integers N2 and N3 such that for all n > N2
|P(h(n)(Xn −𝜃) < z𝛼∕2) −Φ(z𝛼∕2)| < 𝛼∕2
and for all n > N3
|P(h(n)(Xn −𝜃) < −z𝛼∕2) −Φ(−z𝛼∕2)| < 𝛼∕2
where Φ(⋅) is the cdf of the standard normal distribution. Consequently for all
n > max{N2, N3},
P(h(n)(Xn −𝜃) < z𝛼∕2) −P(h(n)(Xn −𝜃) < −z𝛼∕2) > 1 −𝛼.
The proof is completed by deﬁning N = max{N1, N2, N3}.
◽
Corollary 1.2
If {pk; k ≥1} satisﬁes Condition 1.4, then
T1
𝜋0
p
−−−→1.
(1.24)
Proof: The result immediately follows Theorem 1.7 and Lemma 1.8.
◽
Property (1.24) is sometimes referred to as the multiplicative consistency of
Turing’s formula. Interested readers may wish to see Ohannessian and Dahleh
(2012) for a more detailed discussion on such consistency under slightly more
general conditions.
Corollary 1.3
If {pk; k ≥1} satisﬁes Condition 1.4, then
1)
E(N1) (ln T1 −ln 𝜋0)
√
E(N1) + 2E(N2)
L
−−−→N(0, 1) and
(1.25)

22
Statistical Implications of Turing’s Formula
2)
N1 (ln T1 −ln 𝜋0)
√
N1 + 2N2
L
−−−→N(0, 1).
(1.26)
The proof of Corollary 1.3 is left as an exercise (see Exercise 23).
1.3
Multivariate Normal Laws
Turing’s formula T1 is only one member of a family introduced by Good (1953),
albeit the most famous one. Recall the notations, for every integer r, 1 ≤r ≤n,
Nr, and 𝜋r−1 as deﬁned in (1.1) and (1.2). The following may be referred to as
the rth order Turing’s formula.
Tr =
(
n
r −1
) (n
r
)−1
Nr =
r
n −r + 1Nr.
(1.27)
Tr may be thought of as an estimator of 𝜋r−1. The main objective of this section
is to give a multivariate normal law for the vector
(T1, T2, … , TR)𝜏
where R is an arbitrarily ﬁxed positive integer. Toward that end, a normal law
for Tr, where r is any positive integer, is ﬁrst given.
The following is a suﬃcient condition under which many of the subsequent
results are established. Let g(n, 𝛿) = n1−2𝛿as in (1.13).
Condition 1.5
There exists a 𝛿∈(0, 1∕4) such that as n →∞,
1) g2(n, 𝛿)
n2
E(Nr) →cr
r! ≥0,
2) g2(n, 𝛿)
n2
E(Nr+1) →
cr+1
(r + 1)! ≥0, and
3) cr + cr+1 > 0.
Theorem 1.8
Under Condition 1.5,
g(n, 𝛿)(Tr −𝜋r−1)
L
−−−→N
(
0, cr+1 + rcr
(r −1)!
)
.
The proof of Theorem 1.8 is based on a direct evaluation of the characteristic
function of g(n, 𝛿)(Tr −𝜋r−1), which can be shown to have a limit, as n →∞, of
exp
{
−t2
2
[
cr+1
(r −1)! +
rcr
(r −1)!
] }
.
The details of the proof are found in Zhang (2013a).
Theorem 1.9 below is a restatement of Theorem 1.8.

Turing’s Formula
23
Theorem 1.9
Under Condition 1.5,
n(Tr −𝜋r−1)
√
r2E(Nr) + (r + 1)rE(Nr+1)
L
−−−→N(0, 1).
The proof of Theorem 1.9 is straightforward and is left as an exercise (see
Exercise 16).
Lemma 1.9
Let
̂cr = r!g2(n, 𝛿)
n2
Nr.
Under Condition 1.5, ̂cr converges to cr in probability.
By Condition 1.5 and Chebyshev’s inequality, it suﬃces to show that
Var(̂cr) →0.
A proof of that is found in Zhang (2013a).
Theorem 1.10
Under Condition 1.5,
n(Tr −𝜋r−1)
√
r2Nr + (r + 1)rNr+1
L
−−−→N(0, 1).
Theorem 1.10 is a direct consequence of Theorem 1.9 and Lemma 1.9. The
proof is left as an exercise (see Exercise 17).
Similar to the asymptotic results in Theorems 1.3 and 1.4, it may be of interest
to note that the results of Theorems 1.9 and 1.10 require no further knowledge
of g(n, 𝛿), that is, the knowledge of 𝛿, other than its existence.
To establish the asymptotic bivariate distribution of (Tr1, Tr2) where r1 and r2
are two diﬀerent positive integers, the following condition plays a central role.
Condition 1.6
Let g(n, 𝛿) be as in (1.13). There exists a 𝛿∈(0, 1∕4) such that
as n →∞,
1) g2(n, 𝛿)
n2
E(Nr1) →
cr1
r1! ≥0,
2) g2(n, 𝛿)
n2
E(Nr1+1) →
cr1+1
(r1 + 1)! ≥0,
3) cr1 + cr1+1 > 0,
4) g2(n, 𝛿)
n2
E(Nr2) →
cr2
r2! ≥0,
5) g2(n, 𝛿)
n2
E(Nr2+1) →
cr2+1
(r2 + 1)! ≥0, and
6) cr2 + cr2+1 > 0.

24
Statistical Implications of Turing’s Formula
Lemma 1.10
For any two constants, a and b satisfying a2 + b2 > 0, assuming
that r1 < r2 −1 and that Condition 1.6 holds, then
g(n, 𝛿)[a(Tr1 −𝜋r1−1) + b(Tr2 −𝜋r2−1)]
L
−−−→N(0, 𝜎2)
where
𝜎2 = a2 cr1+1 + r1 cr1
(r1 −1)!
+ b2 cr2+1 + r2 cr2
(r2 −1)!
.
Lemma 1.11
For any two constants, a and b satisfying a2 + b2 > 0, assuming
that r1 = r2 −1 and that Condition 1.6 holds, then
g(n, 𝛿)[a(Tr1 −𝜋r1−1) + b(Tr2 −𝜋r2−1)]
L
−−−→N(0, 𝜎2)
where
𝜎2 = a2 cr1+1 + r1 cr1
(r1 −1)!
−2ab
cr2
(r1 −1)! + b2 cr2+1 + r2 cr2
(r2 −1)!
.
The proofs of both Lemmas 1.10 and 1.11 are based on a straightforward
evaluation of the characteristic functions of the underlying statistic. They are
lengthy and tedious and they are found in Zhang (2013a).
Let
𝜎2
r = r2E(Nr) + (r + 1)rE(Nr+1),
𝜌r(n) = −r(r + 1)E(Nr+1)∕(𝜎r 𝜎r+1),
𝜌r = lim
n→∞𝜌r(n),
̂𝜎2
r = r2Nr + (r + 1)rNr+1,
̂𝜌r = ̂𝜌r(n) = −r(r + 1)
Nr+1
√
̂𝜎2
r ̂𝜎2
r+1
.
By the consistency of Lemma 1.9 and the deﬁnition of multivariate normality
of a random vector, the following two corollaries and two theorems are
immediate.
Corollary 1.4
Assume that r1 < r2 −1 and that Condition 1.6 holds, then
n
(
Tr1 −𝜋r1−1
𝜎r1
,
Tr2 −𝜋r2−1
𝜎r2
)𝜏
L
−−−→MVN (∅, I2×2)
where ∅is a zero vector and I2×2 is the two-dimensional identity matrix.
Corollary 1.5
Assume that r1 = r2 −1 and that Condition 1.6 holds, then
n
(
Tr1 −𝜋r1−1
𝜎r1
,
Tr2 −𝜋r2−1
𝜎r2
)𝜏
L
−−−→MVN
(
∅,
( 1 𝜌r1
𝜌r1
1
) )
.

Turing’s Formula
25
Corollaries 1.4 and 1.5 suggest that, in the following series
{
n
(Tr −𝜋r−1
𝜎r
)
; r ≥1
}
,
any two entries are asymptotically independent unless they are immediate
neighbors.
Theorem 1.11
For any positive integer R, if Condition 1.6 holds for every r,
1 ≤r ≤R, then
n
(T1 −𝜋0
𝜎1
, T2 −𝜋1
𝜎2
, … , TR −𝜋R−1
𝜎R
)𝜏
L
−−−→MVN(∅, Σ)
where ∅is a zero vector, Σ = (ai,j) is an R × R covariance matrix with all
the diagonal elements being ar,r = 1 for r = 1, … , R, the super-diagonal and
the sub-diagonal elements being ar,r+1 = ar+1,r = 𝜌r for r = 1, … , R −1, and
all the other oﬀ-diagonal elements being zeros.
Let ̂Σ be the resulting matrix of Σ with 𝜌r replaced by ̂𝜌r(n) for all r. Let ̂Σ−1
denote the inverse of ̂Σ and ̂Σ−1∕2 denote any R × R matrix satisfying ̂Σ−1 =
( ̂Σ−1∕2)𝜏̂Σ−1∕2.
Theorem 1.12
For any positive integer R, if Condition 1.6 holds for every r,
1 ≤r ≤R, then
n ̂Σ−1∕2
(T1 −𝜋0
̂𝜎1
, T2 −𝜋1
̂𝜎2
, … , TR −𝜋R−1
̂𝜎R
)𝜏
L
−−−→MVN(∅, IR×R).
Theorem 1.12 has a profound statistical implication that can be explored by
considering the following three points:
1) The alphabet 𝒳= {𝓁k; k ≥1} is dynamically partitioned by a sample of size
n into n + 1 groups
{𝓁k ∶Yk = 0}, {𝓁k ∶Yk = 1}, … , {𝓁k ∶Yk = r}, … , {𝓁k ∶Yk = n}.
The probability sequence {pk; k ≥1} is also dynamically sorted accordingly
into the n + 1 groups with respect to the partition above, that is,
{pk ∶Yk = 0}, {pk ∶Yk = 1}, … , {pk ∶Yk = r}, … , {pk ∶Yk = n}.
2) For any chosen ﬁxed integer R ≥1, the respective group total probabilities
for the ﬁrst R −1 groups, the entries in the second row below,
{ℓk : Yk = 0}
{ℓk : Yk = 1}
. . .
{ℓk : Yk = R −1}

k:Yk=0 pk

k:Yk=1 pk
. . .

k:Yk=R−1 pk
(1.28)
may be statistically estimated according to Theorem 1.12.

26
Statistical Implications of Turing’s Formula
3) The union of the ﬁrst R partitions, that is,
∪R−1
r=0 {𝓁k ∶Yk = r},
dynamically covers a subset of 𝒳of letters with very low probabilities, or
informally a “tail”, for any n large or small.
These three points are perhaps better demonstrated via an example. Consider
a special case of a discrete distribution with {pk} following a discrete power law,
known as the Pareto law, in the tail, that is,
pk = Ck−𝜆
(1.29)
for all k > k0 where C > 0 and 𝜆> 1 are unknown parameters describing the
tail of the probability distribution beyond an unknown positive integer k0. For
k = 1, … , k0, the probabilities are nonnegative but are otherwise not speciﬁed.
This partially parametric probability model is subsequently referred to as the
“power tail model” or the “Pareto tail model,” for which Condition 1.6 may be
veriﬁed. Suppose it is of interest to estimate C and 𝜆. An estimation procedure
may be devised based on Theorem 1.12. This problem gets a full treatment in
Chapter 7. However, the following gives an intuitive perspective to the problem,
and the perspective is relevant in much of the development throughout the
subsequent chapters.
Under the model (1.29), 𝒳= {1, 2, … } and the R groups in the partition
by a sample with observed frequencies, {Y1, Y2, … }, are subsets of the positive
integers on the real line. Two key issues are to be resolved in estimating C and 𝜆.
1) The basis of a statistical estimation.
2) The issue of an unknown k0.
The basis of a statistical argument is the asymptotic normality of
Theorem 1.12 with each 𝜋r represented under the semiparametric tail
model, that is, for each r = 1, … , R
𝜋r−1 =
∑
k∶Yk=r−1
pk =
∑
k∶Yk=r−1
Ck−𝜆.
(1.30)
Noting that Tr is observable and 𝜋r−1 is also an observable function of C and 𝜆,
the asymptotic distribution in Theorem 1.12 has an explicit likelihood function,
which will be referred to as the asymptotic likelihood function and is a function
of the data and the parameters C and 𝜆. Let ( ̂C, ̂𝜆) be the maximum likeli-
hood estimator of (C, 𝜆) based on the asymptotic distribution of Theorem 1.12,
which is shown to exist uniquely in Chapter 7 and is referred to thereafter as the
asymptotic maximum likelihood estimator or AMLE. The AMLE is also shown
to be consistent.
Regarding the issue of the unknown k0, since the semiparametric model is
only valid for k > k0, the second equality in (1.30) may not necessarily be true
for some k. However for every r, as n increases indeﬁnitely, the minimum value
of k in the rth partition group, that is, min{k ∶Yk = r −1} rapidly increases

Turing’s Formula
27
in probability to pass the ﬁxed albeit unknown k0 (see Exercise 18). This argu-
ment essentially validates the second equality in (1.30) and, in turn, validates
the above-mentioned asymptotic argument. The fact that the estimation pro-
cedure does not require a preset threshold for the unknown k0, in contrast to the
required abundance threshold in Hill’s estimator (see Hill (1975)) for continu-
ous random variables, is quite remarkable. In short, by Theorem 1.12, the vector
(T1, … , TR)𝜏may be viewed as a statistical “window on the tail” in capturing
a parametric tail behavior of an underlying probability distribution, possibly
beyond data range.
1.4
Turing’s Formula Augmented
For r = 1, … , n, recall
Nr =
∑
k≥1
1[Yk = r]
which is the total number of letters represented exactly r times in the sam-
ple. Chao, Lee, and Chen (1988) proposed the following augmented Turing’s
formula:
T♯=
n
∑
r=1
(−1)r+1 (n
r
)−1
Nr.
(1.31)
Noting that both n and r are ﬁnite integers,
E (T♯−𝜋0) =
n
∑
r=1
(−1)r+1 (n
r
)−1
E(Nr) −E(𝜋0)
=
n
∑
r=1
(−1)r+1 (n
r
)−1 (n
r
) ∑
k≥1
pr
k(1 −pk)n−r
−
∑
k≥1
pk(1 −pk)n
=
∑
k≥1
pk(1 −pk)n−1 −
∑
k≥1
pk(1 −pk)n
+
n
∑
r=2
(−1)r+1 ∑
k≥1
pr
k(1 −pk)n−r
=
∑
k≥1
p2
k(1 −pk)n−1 +
n
∑
r=2
(−1)r+1 ∑
k≥1
pr
k(1 −pk)n−r
=
∑
k≥1
p2
k(1 −pk)n−1 −
∑
k≥1
p2
k(1 −pk)n−2
+
n
∑
r=3
(−1)r+1 ∑
k≥1
pr
k(1 −pk)n−r

28
Statistical Implications of Turing’s Formula
= −
∑
k≥1
p3
k(1 −pk)n−2 +
n
∑
r=3
(−1)r+1 ∑
k≥1
pr
k(1 −pk)n−r
⋮
= (−1)n+1 ∑
k≥1
pn+1
k
.
The bias of T♯is therefore, letting p∨= max{pk; k ≥1},
|E (T♯−𝜋0) | =
∑
k≥1
pn+1
k
≤pn
∨.
(1.32)
It is clear that unless p∨= 1, that is, {pk} has all of its probability mass on a
single letter, the bias of T♯decays at least exponentially fast in sample size n.
Theorem 1.13
If the eﬀective cardinality of 𝒳is greater than or equal to three,
that is, K ≥3, then the augmented Turing’s formula T♯is asymptotically and
relatively unbiased.
Proof: To be instructive, consider ﬁrst the case that p∨< 1∕2, which implies
that p∨∕(1 −p∨) ∈(0, 1). By Deﬁnition 1.1, as n →∞,
|E (T♯−𝜋0) |
E(𝜋0)
=
∑
k≥1 pn+1
k
∑
k≥1 pk(1 −pk)n ≤
(
p∨
1 −p∨
)n
→0.
If p∨≥1∕2, then since K ≥3, there must exist a k = k𝜏such that 0 < pk′ < 1 −
p∨and hence 1 −pk′ > p∨, that is, p∨∕(1 −pk′) < 1. On the other hand,
|E (T♯−𝜋0) |
E(𝜋0)
=
∑
k≥1 pn+1
k
∑
k≥1 pk(1 −pk)n ≤
pn
∨
pk′(1 −pk′)n
= 1
pk′
(
p∨
1 −pk′
)n
→0.
◽
Theorem 1.13 suggests that the augmented Turing’s formula is of some
material improvement to Turing’s formula, particularly in view of Example 1.1.
In addition, the augmented Turing’s formula T♯also admits asymptotic
normality.
Theorem 1.14
Let T♯be as in (1.31). If the probability distribution {pk; k ≥1}
satisﬁes Condition 1.1, then
n (T♯−𝜋0)
√
E(N1) + 2E(N2)
L
−−−→N(0, 1).
(1.33)
To prove Theorem 1.14, the following two lemmas are useful.

Turing’s Formula
29
Lemma 1.12
For each integer n > 0, let Xn be a nonnegative random variable
with ﬁnite mean mn = E(Xn). If limn→∞mn = 0, then Xn
p→0.
The proof is left as an exercise (see Exercise 9).
Lemma 1.13
For any constant 𝛿∈(0, 1) and a probability sequence {pk;
k ≥1}, as n →∞,
1. n𝛿∑
k≥1
p2
k(1 −pk)n →0
and
2. n𝛿∑
k∈S
p2
k(1 −pk)n →0
where S is any subset of {k; k ≥1}.
Proof: For Part 1, since 𝛿∈(0, 1), there exists 𝛿1 ∈(𝛿, 1). Let
I = {k ∶pk < 1∕n𝛿1}
and
Ic = {k ∶pk ≥1∕n𝛿1} .
0 ≤n𝛿∑
k≥1
p2
k(1 −pk)n
= n𝛿∑
k∈Ic
p2
k(1 −pk)n + n𝛿∑
k∈I
p2
k(1 −pk)n
≤n𝛿∑
k∈Ic
pk (1 −1∕n𝛿1)n + n𝛿∑
k∈I
pk∕n𝛿1
≤n𝛿(1 −1∕n𝛿1)n𝛿1+(1−𝛿1) + n𝛿∕n𝛿1
= n𝛿[ (1 −1∕n𝛿1)n𝛿1]n1−𝛿1 + n−(𝛿1−𝛿) →0.
Part 1 implies Part 2.
◽
Proof of Theorem 1.14: Let 𝜎n =
√
E(N1) + 2E(N2). By Slutsky’s theorem, the
objective is to show
n(T♯−𝜋0)
𝜎n
−n(T1 −𝜋0)
𝜎n
p
−−−→0.
(1.34)
Note ﬁrst that by Condition 1.1,
𝜎n
n2𝛿=
√
E(N1) + 2E(N2)
n4𝛿
→c1 + c2 > 0.
Note second that
n(T♯−𝜋0)
𝜎n
−n(T1 −𝜋0)
𝜎n
= n
𝜎n
(T♯−T1)
= n
𝜎n
[ n
∑
r=2
(−1)r+1 (n
r
)−1
Nr
]
=
(
n2𝛿
𝜎n
)
n1−2𝛿
[ n
∑
r=2
(−1)r+1 (n
r
)−1
Nr
]
.

30
Statistical Implications of Turing’s Formula
Therefore, (1.34) holds if and only if
n1−2𝛿
[ n
∑
r=2
(−1)r+1 (n
r
)−1
Nr
]
p
−−−→0.
(1.35)
Separating the ∑n
r=2 in (1.35) into two parts, ∑
odd and ∑
even, where ∑
odd sums
over all odd values of r and ∑
even sums over all even values of r in {2, … , n},
the left-hand side of (1.35) may be re-expressed as
an −bn ∶= n1−2𝛿∑
odd
(n
r
)−1
Nr −n1−2𝛿∑
even
(n
r
)−1
Nr.
Now the objective becomes to show, respectively,
an
p
−−−→0
and
bn
p
−−−→0.
However, since both an and bn are nonnegative, it suﬃces to show that
an + bn
p→0, that is,
an + bn = n1−2𝛿
n
∑
r=2
(n
r
)−1
Nr
p
−−−→0.
By Lemma 1.12, it suﬃces to show
E(an + bn) = n1−2𝛿
n
∑
r=2
∑
k≥1
pr
k(1 −pk)n−r −−−→0.
Let 𝛿∗= 1 −2𝛿,
I1 = {k∶pk = 1∕2},
I2 = {k∶pk > 1∕2},
I3 = {k∶pk < 1∕2}.
0 ≤n𝛿∗
n
∑
r=2
∑
k≥1
pr
k(1 −pk)n−r
= n𝛿∗
n
∑
r=2
(
∑
k∈I1
+
∑
k∈I2
+
∑
k∈I3
)
pr
k(1 −pk)n−r
=∶d1 + d2 + d3.
In the following, it remains to show that di →0, for each i ∈{1, 2, 3},
respectively.
If I1 is empty, then d1 = 0. If not, then for each index k ∈I1,
n𝛿∗
n
∑
r=2
pr
k(1 −pk)n−r = n𝛿∗
n
∑
r=2
(1∕2)n = n𝛿∗(n −1)(1∕2)n →0.
Since there are at most two indices in I1, d1 →0.

Turing’s Formula
31
If I2 is empty, then d2 = 0. If not, then for each index k ∈I2,
n𝛿∗
n
∑
r=2
pr
k(1 −pk)n−r = n𝛿∗
[ n
∑
r=2
(
pk
1 −pk
)r]
(1 −pk)n
= n𝛿∗
⎧
⎪
⎨
⎪⎩
(
pk
1 −pk
)2 ⎡
⎢
⎢
⎢⎣
1 −
(
pk
1−pk
)n−1
1 −
pk
1−pk
⎤
⎥
⎥
⎥⎦
⎫
⎪
⎬
⎪⎭
× (1 −pk)n
= n𝛿∗
[
p2
k(1 −pk)n−1
1 −2pk
] [
1 −
pn−1
k
(1 −pk)n−1
]
= n𝛿∗
[
p2
k(1 −pk)n−1
1 −2pk
]
−n𝛿∗
[
p2
k(1 −pk)n−1
1 −2pk
]
pn−1
k
(1 −pk)n−1
= n𝛿∗
[
p2
k(1 −pk)n−1
1 −2pk
]
−n𝛿∗
(
pn+1
k
1 −2pk
)
→0.
Since there is at most one index in I2, d2 →0.
For d3, using the same argument as that for d2 earlier,
d3 = n𝛿∗∑
k∈I3
p2
k(1 −pk)n−1(1 −2pk)−1 −n𝛿∗∑
k∈I3
pn+1
k
(1 −2pk)−1
=∶d31 −d32.
It suﬃces to show d31 →0 and d32 →0. Let p∨= max{pk; k ∈I3}. Then
p∨< 1∕2 and
d31 = n𝛿∗∑
k∈I3
p2
k(1 −pk)n−1(1 −2pk)−1
≤n𝛿∗(1 −p∨)−1(1 −2p∨)−1 ∑
k∈I3
p2
k(1 −pk)n →0
by Part 2 of Lemma 1.13. Finally,
d32 = n𝛿∗∑
k∈I3
pn+1
k
(1 −2pk)−1 ≤n𝛿∗(1 −2p∨)−1(1∕2)n →0.
◽
The consistency of Lemma 1.5 immediately leads to the following practically
useful statement.

32
Statistical Implications of Turing’s Formula
Theorem 1.15
Let T♯be as in (1.31). Under Condition 1.1,
n (T♯−𝜋0)
√
N1 + 2N2
L
−−−→N(0, 1).
(1.36)
Example 1.10
Construct a 95% conﬁdence interval for 𝜋0 using Theorem 1.15
and the bird data in Table 1.2.
Solution: There are only a few values of r for which Nr is nonzero in the data set
and they are as follows:
r
1
2
6
30
50
70
80
100
200
300
Nr
12
1
1
1
2
1
1
7
2
2
By the sheer magnitude of
(
n
r
)
, and hence the insigniﬁcance of
(
n
r
)−1
, with
large values of r, the additive terms in (1.31) do not register but in a few terms.
Consequently, T♯= 0.00599, and by Theorem 1.15 and Example 1.6, a 95% con-
ﬁdence interval for 𝜋0 is
0.0059995 ± 0.0037 = (0.0022995, 0.0096995).
One drawback of T♯is that it may theoretically assume a negative value
though practically impossible with a suﬃciently large n. One may wish to
further augment T♯by means of, for example,
T♯♯= max {T♯, 0} .
(1.37)
It can be veriﬁed that T♯♯also admits asymptotic normality of Theorems 1.14
and 1.15 (see Exercise 19).
In the same spirit of (1.31), the rth order Turing’s formula may also
be augmented similarly. Noting for each integer r, 1 ≤r ≤n −1, 𝜋r−1 =
∑
k≥1 pk1[Yk = r −1],
E
( (
n
r −1
)−1
𝜋r−1
)
=
∑
k≥1
pr
k(1 −pk)n−(r−1)
=
∑
k≥1
pr
k(1 −pk)n−r −
∑
k≥1
pr+1
k
(1 −pk)n−r
=
∑
k≥1
pr
k(1 −pk)n−r −
∑
k≥1
pr+1
k
(1 −pk)n−r−1 + · · ·
+ (−1)n−r+1 ∑
k≥1
pn+1
k
,
and
E
( (n
r
)−1
Nr
)
=
∑
k≥1
pr
k(1 −pk)n−r,

Turing’s Formula
33
it may be veriﬁed that the augmented rth order Turing’s formula,
T♯
r =
(
n
r −1
)
n
∑
m=r
(−1)m−r ( n
m
)−1
Nm
(1.38)
has bias
|E(T♯
r −𝜋r−1)| =
(
n
r −1
) ∑
k≥1
pn+1
k
≤
(
n
r −1
)
pn
∨= (nr−1pn
∨) ,
where p∨= max{pk; k ≥1}, decaying rapidly when r is a small ﬁxed integer.
1.5
Goodness-of-Fit by Counting Zeros
Turing’s formula is mostly about sorting letters, observed in an iid sample of
size n, into equal frequency groups and counting the diﬀerent letters in each
group, namely Nr, r = 1, … , n. In the special case of a ﬁnite and known K,
N0 =
∑
k≥1
1[Yk = 0]
is also observable. The distribution of Nr, where r = 0, 1, … , n, is discussed by
many in more detail under the topic of occupancy problems. Both Johnson and
Kotz (1977) and Kolchin, Sevastyanov, and Chistyakov (1978) oﬀer an excellent
coverage of the topic.
In an experiment of randomly allocating n balls into K < ∞baskets according
to a distribution p = {pk; k = 1, … , K}, N0 is the number of empty or unoccu-
pied baskets among a total of K baskets. When N0 is observable, it has a quite
interesting statistical feature as stated in Theorem 1.16.
Theorem 1.16
For a ﬁnite alphabet 𝒳= {𝓁1, … , 𝓁K} where K ≥2, let K
denote the collection of all possible probability distributions on 𝒳. N0 satisﬁes
min{Ep(N0) ∶p ∈K} = K
(
1 −1
K
)n
,
(1.39)
and Ep(N0) attains its minimum value at p being uniform, that is, pk = 1∕K for
every k = 1, … , K.
Proof: Since
Ep(N0) =
K
∑
k=1
E(1[Yk = 0]) =
K
∑
k=1
(1 −pk)n,
(1.40)
it suﬃces to show that for any pair of probabilities, denoted by p1 and p2 (with-
out loss of generality), if p1 ≠p2, then there would exist a p∗∈K such that
Ep∗(N0) > Ep(N0). Toward that end, let
g(x1, x2) = (1 −x1)n + (1 −x2)n

34
Statistical Implications of Turing’s Formula
and consider the behavior of
g(x1, s −x1) = (1 −x1)n + [1 −(s −x1)]n
for x1 ∈[0, s], where s = p1 + p2 is held ﬁxed:
dg(x1, s −x1)
dx1
= −n(1 −x1)n−1 + n(1 −s + x1)n−1.
Since
dg(x1, s −x1)
dx1
||||x1=0
= −n + n(1 −s)n−1 ≤0,
dg(x1, s −x1)
dx1
||||x1=s
= −n(1 −s)n−1 + n ≥0, and
dg(x1, s −x1)
dx1
= 0 if and only if x1 = s
2,
it is then clear that g(x1, x2) = (1 −x1)n + (1 −x2)n, subject to the constraint
x1 + x2 = p1 + p2, reaches its minimum at x1 = x2 = (p1 + p2)∕2. This implies
Ep∗(N0) < Ep(N0) where
p∗=
{p1 + p2
2
, p1 + p2
2
, p3, … , pK
}
.
Therefore, Ep(N0) is not minimized unless pk = 1∕K for every k, 1 ≤k ≤K, in
which case, it takes on the value of K(1 −1∕K)n.
◽
Theorem 1.16 suggests that, under the assumption of independent alloca-
tions, the uniform distribution would lead to the least expected number of
empty baskets. Any other distribution would have a tendency for a larger num-
ber of empty baskets. If an observed N0 diﬀers signiﬁcantly from K(1 −1∕K)n,
then either the uniformity assumption or the independence assumption, or
both, would become questionable. This feature provides an interesting perspec-
tive to the statistical question of “goodness-of-ﬁt.”
For any p = {pk; k = 1, … , K} ∈K, in addition to Ep(N0) given in (1.40),
Var(N0) = E (N2
0) −(E(N0))2
= E
⎛
⎜
⎜⎝
(
∑
1≤k≤K
1[Yk = 0]
)2⎞
⎟
⎟⎠
−(E(N0))2
= E
(
∑
1≤k≤K
1[Yk = 0]
+
∑
i≠j;i,j=1, … ,K
1[Yi = 0]1[Yj = 0]
)
−(E(N0))2

Turing’s Formula
35
=
∑
1≤k≤K
(1 −pk)n +
∑
i≠j;i,j=1, … ,K
(1 −pi −pj)n −(E(N0))2.
(1.41)
Under the assumption of the uniform distribution, that is, pk = 1∕K for every
k, 1 ≤k ≤K, (1.40) and (1.41) become
E(N0) = K
(
1 −1
K
)n
,
(1.42)
Var(N0) = K
(
1 −1
K
)n
+ K(K −1)
(
1 −2
K
)n
−K2 (
1 −1
K
)2n
. (1.43)
Further suppose n = 𝜆K for some constant 𝜆> 0, then (1.42) and (1.43)
become
E(N0) = K
(
1 −1
K
)𝜆K
,
(1.44)
Var(N0) = K
(
1 −1
K
)𝜆K
+ K(K −1)
(
1 −2
K
)𝜆K
−K2 (
1 −1
K
)2𝜆K
.
(1.45)
More speciﬁcally, when 𝜆= 1 or K = n, (1.42) and (1.45) become
E(N0) = K
(
1 −1
K
)K
,
(1.46)
Var(N0) = K
(
1 −1
K
)K
+ K(K −1)
(
1 −2
K
)K
−K2 (
1 −1
K
)2K
.
(1.47)
For any suﬃciently large K, (1.42) and (1.47) become
E(N0) ≈Ke−1,
(1.48)
Var(N0) ≈K (e−1 −e−2) .
(1.49)
Let
P0 = N0
K ,
(1.50)
that is, the proportion of baskets not occupied in an experiment of randomly
allocating n = K balls into K baskets, or more generally, the proportion of
letters in 𝒳not observed in an iid sample under the uniform distribution,
pk = 1∕K. By (1.46)–(1.49),
E(P0) =
(
1 −1
K
)K
≈1∕e,
(1.51)
Var(P0) = Var(N0)
K2
= 1
K
(
1 −1
K
)K
+ K −1
K
(
1 −2
K
)K
−
(
1 −1
K
)2K
≈e −1
Ke2 .
(1.52)

36
Statistical Implications of Turing’s Formula
At this point, an interesting observation can be made. If n = 100 balls are ran-
domly allocated into K = 100 baskets, one should expect to see approximately
100e−1 ≈36.78% × 100 ≈37 empty baskets. The expected number of empty
baskets in this experiment at 37 is counterintuitively high for most minds. The
most common intuitive guess of E(N0) is 25, which is signiﬁcantly below 37.
It is perhaps more interesting to contemplate why an intuitive mind would
arrive at a number so much lower than 37. An intuitive mind would try to
balance the extreme event of a “perfectly uniform allocation” of one ball in
each basket (with probability P(N0 = 0) = K!∕KK) and another extreme event
of a “completely skewed allocation” of all balls in one basket (with probability
P(N0 = K −1) = K∕KK). The fact that the ﬁrst event is so much more likely
than the second event would pull an intuitive guess of E(N0) toward the lower
end. Even without the notion of likelihoods of these events, the event of N0 = 0
conforms much better to the notion of “uniformly distributed allocation” than
that of N0 = 99. Nevertheless, the fact remains that E(N0) ≈37.
In fact, in terms of proportion, that is, P0 = N0∕K, the above-observed sta-
tistical feature becomes more pronounced since, as K →∞
E(P0) →e−1,
𝜎P0 ≃
√
e −1
e
√
K
→0.
A goodness-of-ﬁt statistical test may be devised based on the above-observed
feature.
Given two positive integers, n and m, satisfying n ≥m, the Stirling number of
the second kind is the number of ways of partitioning a set of n elements into
m nonempty sets. For example, n = 3 elements in the set {𝓁1, 𝓁2, 𝓁3} can be
partitioned into m = 3 subsets in one way, that is, {{𝓁1}, {𝓁2}, {𝓁3}}; the same
n = 3 elements can be partitioned into m = 2 subsets in three ways, that is,
{{𝓁1, 𝓁2}, {𝓁3}},
{{𝓁1, 𝓁3}, {𝓁2}},
{{𝓁2, 𝓁3}, {𝓁1}};
and the same n = 3 elements can be partitioned into m = 1 subset in 1 way,
that is, {{𝓁2, 𝓁2, 𝓁3}}. In general, the Stirling number of the second kind is often
denoted and computed by
S(n, m) = 1
m!
m
∑
i=0
(−1)i (m
i
)
(m −i)n.
(1.53)
For a more detailed discussion, interested readers may refer to Stanley (1997).
In an experiment of randomly allocating n balls into K baskets, let x be the
number of empty baskets. Subject to 1 ≤K −x ≤n,
1) there are a total of Kn ways to place n balls in K baskets;
2) there are
(
K
K−x
)
(K −x)! ways to select and to arrange the ﬁlled baskets;
and

Turing’s Formula
37
3) there are S(n, K −x) ways to completely occupy the selected K −x ﬁlled bas-
kets with n balls.
Therefore, under an iid allocation with the uniform distribution pk = 1∕K for
k = 1, … , K, the probability of observing exactly x empty baskets is
P(N0 = x) =
(
K
K−x
)
(K −x)!S(n, K −x)
Kn
.
(1.54)
Simplifying (1.54) leads to
P(N0 = x) =
K!
x!(K −x)!Kn
K−x
∑
i=0
(−1)i (K −x
i
)
(K −x −i)n
=
K!
x!(K −x)!Kn
K−x
∑
i=0
(−1)i
(K −x)!
i!(K −x −i)!(K −x −i)n
=
K−x
∑
i=0
(−1)i
K!
x!i!(K −x −i)!
(
1 −x + i
K
)n
.
(1.55)
For simplicity in demonstration, (1.55) is reduced to the case of n = K, that
is,
P(N0 = x) =
K−x
∑
i=0
(−1)i
K!
x!i!(K −x −i)!
(
1 −x + i
K
)K
.
(1.56)
Table 1.3 is the exact probability distribution of N0 represented by (1.56)
when K = 10. Table 1.4 includes several eﬀectively positive terms of the cumu-
lative distribution function (cdf ), whose probability mass function is repre-
sented by (1.56) for K = 100.
However, the numerical evaluation of (1.56) is computationally demanding
for even moderately large values of K.
Theorem 1.16 and the explicit expression of (1.56) together enable a means
of testing the hypothesis that a sample is iid under the uniform distribution.
Example 1.11 below illustrates how a test based on N0 may be used to quantify
statistical evidence against such a hypothesis.
Table 1.3 Exact probabilities of (1.56) for K = n = 10
x
P (N0 = x)
0
0.000362880
1
0.016329600
2
0.136080000
3
0.355622400
4
0.345144240
x
P (N0 = x)
5
0.128595600
6
0.017188920
7
0.000671760
8
0.000004599
9
0.000000001

38
Statistical Implications of Turing’s Formula
Table 1.4 Partial cdf of N0 based on (1.56) for K = n = 100
x
P (N0 = x)
24
0.000038799
25
0.000148654
26
0.000509259
27
0.001564749
28
0.004325109
29
0.010787039
30
0.024349639
31
0.049907939
32
0.093206939
x
P (N0 = x)
35
0.362190439
36
0.487643439
37
0.614165439
38
0.729399439
39
0.824210739
40
0.894694539
41
0.942042739
33
0.159224739
34
0.249901439
x
P (N0 = x)
42
0.970782939
43
0.986544139
44
0.994351189
45
0.997842829
46
0.999252139
47
0.999765198
48
0.999933542
49
0.999983287
50
0.999996511
Example 1.11
The following data set contains n = 100 iid observations from
𝒳= {𝓁1, … , 𝓁100} under the triangular distribution
p = {pk = k∕5050; k = 1, … , 100},
summarized by the observed letters and their corresponding frequencies, yk.
letters
𝓁3
𝓁14
𝓁15
𝓁16
𝓁17
𝓁18
𝓁22
𝓁25
𝓁30
𝓁33
𝓁36
𝓁37
yk
1
1
1
1
1
1
2
1
3
1
1
1
letters
𝓁38
𝓁42
𝓁44
𝓁45
𝓁46
𝓁49
𝓁50
𝓁54
𝓁56
𝓁57
𝓁58
𝓁59
yk
2
1
1
1
1
1
2
1
1
1
2
2
letters
𝓁61
𝓁64
𝓁65
𝓁66
𝓁68
𝓁69
𝓁70
𝓁71
𝓁72
𝓁73
𝓁74
𝓁75
yk
1
1
1
1
5
1
1
2
3
1
1
2
letters
𝓁76
𝓁77
𝓁78
𝓁79
𝓁81
𝓁82
𝓁83
𝓁85
𝓁86
𝓁87
𝓁88
𝓁89
yk
3
2
1
2
2
2
3
2
3
4
1
3
letters
𝓁90
𝓁91
𝓁92
𝓁93
𝓁94
𝓁95
𝓁96
𝓁97
𝓁98
yk
2
1
2
2
1
3
2
2
6
Using the distribution of Table 1.4 to ﬁnd the p-value in testing, the hypothesis
H0: p is uniform on 𝒳.
Solution: Since it is known that K = 100, the number of missing letters in the
sample is n0 = 100 −∑100
k=1 1[yk ≠0] = 100 −57 = 43. By Theorem 1.16, this is
a one-sided test, and therefore
the p-value = P(N0 ≥43|H0) = 0.0135.

Turing’s Formula
39
There is moderately strong statistical evidence against H0 of the uniform
distribution.
The deviation quantiﬁed in Example 1.11 is due to the diﬀerence between the
true distribution p (the triangular distribution) and the hypothesized uniform
distribution. The test could also be used to detect deviation from the assump-
tion of independence between observations.
Consider a special Markov chain with memory m, that is,
X0, X1, X2, … , Xt−2, Xt−1, Xt, … ,
(1.57)
such that
1) X0 is a random element on the state space 𝒳= {𝓁1, … , 𝓁K}, with the uni-
form distribution, that is, P(X0 = 𝓁k) = 1∕K, for every k, 1 ≤k ≤K;
2) Xt, t ≤m, is a random element on the state space 𝒳= {𝓁1, … , 𝓁K}, such
that, given Xt−1 = xt−1, … , X0 = x0, Xt is only allowed to visit the states,
which have not been visited by any of X0 to Xt−1, with equal probabilities;
3) Xt, t > m, is a random element on the state space 𝒳= {𝓁1, … , 𝓁K}, such
that, given Xt−1 = xt−1, … , Xt−m = xt−m, Xt is only allowed to visit the states,
which have not been visited by any of Xt−1 to Xt−m, with equal probabilities.
A simple argument based on symmetry would establish that, for every t, Xt
is unconditionally a uniformly distributed random element on 𝒳.
The following table includes a segment of n = 100 consecutive observations
of a randomly generated sequence of Xt according to the above-described spe-
cial Markov chain with memory m = 10 and K = 100.
𝓁90
𝓁59
𝓁63
𝓁26
𝓁40
𝓁72
𝓁36
𝓁11
𝓁68
𝓁67
𝓁29
𝓁82
𝓁30
𝓁62
𝓁23
𝓁35
𝓁02
𝓁22
𝓁58
𝓁69
𝓁67
𝓁93
𝓁56
𝓁11
𝓁42
𝓁29
𝓁73
𝓁21
𝓁19
𝓁84
𝓁37
𝓁98
𝓁24
𝓁15
𝓁70
𝓁13
𝓁26
𝓁91
𝓁80
𝓁56
𝓁73
𝓁62
𝓁96
𝓁81
𝓁05
𝓁25
𝓁84
𝓁27
𝓁36
𝓁46
𝓁29
𝓁13
𝓁57
𝓁24
𝓁95
𝓁82
𝓁45
𝓁14
𝓁67
𝓁34
𝓁64
𝓁43
𝓁50
𝓁87
𝓁08
𝓁76
𝓁78
𝓁88
𝓁84
𝓁03
𝓁51
𝓁54
𝓁99
𝓁32
𝓁60
𝓁68
𝓁39
𝓁12
𝓁26
𝓁86
𝓁94
𝓁95
𝓁70
𝓁34
𝓁78
𝓁67
𝓁01
𝓁97
𝓁02
𝓁17
𝓁92
𝓁52
𝓁56
𝓁80
𝓁86
𝓁41
𝓁65
𝓁89
𝓁44
𝓁19
Example 1.12
In the above-described Markov chain with memory m, suppose
it is known a priori that Xt for every t is unconditionally a uniformly distributed
random element on 𝒳and it is of interest to test the hypothesis, H0 ∶m = 0. Use
the above-mentioned data to ﬁnd the appropriate p-value for the test.

40
Statistical Implications of Turing’s Formula
Solution: Counting the number of missing letters in the data set gives n0 = 27.
Since any positive value of m would tend to enlarge the coverage of 𝒳and hence
to shrink N0, a one-sided test is appropriate and correspondingly, by Table 1.4,
the p-value = P(N0 ≤27|H0) = 0.0016.
There is strong statistical evidence in the data against H0.
It is sometime conjectured that the digits of 𝜋= 3.14159265358 … in
nonoverlapping d-digit fragments, where d is any given positive integer,
imitate well a sequence of iid uniform random variables on alphabet that
contains all nonnegative integers from 0 to ∑d−1
i=0 9 × 10i, that is, a number
consisted of d nines. For example, if d = 1, then any single digit would be one
of the 10 digits, ranging from 0 to 9. For another example, if d = 3, then any
consecutive three digits would be an integer ranging from 0 to 999.
Example 1.13
Use the sequence of the ﬁrst one billion digits after the deci-
mal point in 𝜋to evaluate the resemblance of this sequence to one with a bil-
lion iid uniform random digits from alphabet 𝒳= {0, 1, … , 9} by Pearson’s
goodness-of-ﬁt test statistic.
Solution: There are at least two perspectives in which the resemblance to uniform
random digits can be quantiﬁed. The ﬁrst is a straightforward goodness-of-ﬁt test
for the distribution of P(0) = P(1) = · · · = P(9) = 1∕10. This test may be thought
of as an “evenness” test, as it directly tests the equality in proportionality of the
10 digits. The second is a goodness-of-ﬁt test for the distribution in Table 1.3. This
test may be thought of as an “anti-evenness” test as it tests a particular pattern
in Table 1.3, which deviates signiﬁcantly from the “perfect” case of evenness at
n0 = 0.
In the ﬁrst case, the sequence of the ﬁrst billion digits after the decimal point
is hypothesized to be a realization of a sample of n = 1 billion iid observations
under the hypothesized uniform distribution, that is, H0: P(x) = 1∕10 for x =
0, … , 9. In the following, the observed and expected frequencies, Ox and Ex, are
given.
n0 = x
Observed frequency (Ox)
Expected frequency (Ex)
0
99 993 942
100 000 000.0
1
99 997 334
100 000 000.0
2
100 002 410
100 000 000.0
3
99 986 911
100 000 000.0
4
100 011 958
100 000 000.0
5
99 998 885
100 000 000.0
6
100 010 387
100 000 000.0
7
99 996 061
100 000 000.0
8
100 001 839
100 000 000.0
9
100 000 273
100 000 000.0

Turing’s Formula
41
Pearson’s chi-squared test statistic is
𝜒2 =
9
∑
x=0
(Ox −Ex)2
Ex
= 4.92
with degrees of freedom 9. The corresponding p-value is P(𝜒2 ≥4.92) = 0.8412
where 𝜒2 is a chi-squared random variable with degrees of freedom 9. There is
no statistical evidence in the sample against H0.
In the second case, the sequence of the ﬁrst billion digits after the decimal point
is viewed as a sequence of consecutive fragments or samples of size 10 digits, that
is,
{1, 4, 1, 5, 9, 2, 6, 5, 3, 5}, {8, 9, 7, 9, 3, 2, 3, 8, 4, 6}, … .
For each sample of size n = 10, the number of unobserved digits n0 is noted,
resulting in the following observed/expected frequency table, where the expected
frequencies are based on the distribution of Table 1.3.
n0 = x
Observed frequency (Ox)
Expected frequency (Ex)
0
36 270
36 288.0
1
1 633 243
1 632 960.0
2
13 609 335
13 608 000.0
3
35 556 620
35 562 240.0
4
34 518 949
34 514 424.0
5
12 859 799
12 859 560.0
6
1 718 714
1 718 892.0
7
66 617
67 176.0
8
453
459.9
9
0
0.1
Combining the frequencies of the last two categories, n0 = 8 and n0 = 9, Pear-
son’s chi-squared test statistic is
𝜒2 =
8∗
∑
x=0
(Ox −Ex)2
Ex
= 6.45
where O8∗= 453 and E8∗= 460, with degrees of freedom 8. The corresponding
p-value is P(𝜒2 ≥645) = 0.5971 where 𝜒2 is a chi-squared random variable
with degrees of freedom 8. There is no statistical evidence in the sample
against H0.
In summary, the ﬁrst one billion decimal digits of 𝜋seem to resemble well a
randomly generated iid digits from 0 to 9 under the uniform distribution.
Since by (1.51) and (1.52), E(P0) and Var(P0) are easily and precisely calcu-
lated, a large sample test may be devised for the hypothesis, H0: the data are iid

42
Statistical Implications of Turing’s Formula
observations under the uniform distribution on 𝒳. Consider the experiment
in which a sample of n iid uniform random elements is drawn from an alphabet
with K letters and the proportion of empty baskets, P0 = N0∕K, is noted. Sup-
pose such an experiment is repeated m times, resulting in, P0,1, … , P0,m, and
denote the sample mean as P0. Then by the central limit theorem, provided that
m is suﬃciently large,
Z =
√
m
[
P0 −
(
1 −1
K
)K]
√
1
K
(
1 −1
K
)K
+ K−1
K
(
1 −2
K
)K
−
(
1 −1
K
)2K
(1.58)
converges weakly to a standard normal random variable under H0.
Example 1.14
Use (1.58) to test the hypothesis, H0, that the sequence of the
three-digit nonoverlapping segments starting from the ﬁrst digit after the deci-
mal point in 𝜋, that is,
141,592, 653,589, 793,238, 462,643, 383,279, …
is a sequence of iid observations from alphabet 𝒳= {0, 1, … , 999} under the
uniform distribution.
Solution: This is a case corresponding to K = 1000. The ﬁrst sample of size n =
1000 consists of the ﬁrst 3000 digits of 𝜋after the decimal point, breaking into
1000 nonoverlapping three-digit fragments. For this sample of size n = 1000,
P0 = N0∕K is calculated. The next 3000 digits gave a second sample of size 1000,
for which P0 = N0∕K is calculated. The step is repeated m = 300000 times, and
the mean of these P0s is calculated and denoted by P0. Finally, (1.58) takes a
value Z = 0.0036, which indicates no evidence against H0.
The idea of a goodness-of-ﬁt test based on N0 was perhaps ﬁrst suggested by
David (1950). There it is proposed that, for the hypothesis H0 that an iid sample
is taken from a population with known cdf F(x), K points z0 = −∞< z1 < · · · <
zK−1 < zK = ∞can be chosen such that F(zk) −F(zk−1) = 1∕K, and that the test
rejects H0 if the p-value P(N0 ≥n0|H0), where n0 is the observed number of
intervals (zk−1, zk) containing no observation, is less than a preﬁxed 𝛼∈(0, 1).
1.6
Remarks
Turing’s formula is often perceived to project a counterintuitive impression.
This impression is possibly linked to the implied utility of Turing’s formula in
making nonparametric inferences regarding certain distributional character-
istics beyond the data range. To better discern the spirit of Turing’s formula,
it is perhaps helpful to, at least pedagogically, consider the formula in several
diﬀerent perspectives.

Turing’s Formula
43
To begin, one may be beneﬁted by a brief revisit to a dilemma in the practice of
statistical estimation. Primarily consider a simple example in which the popu-
lation is assumed to be N(𝜇, 𝜎2) with an unknown 𝜇and a known 𝜎2 > 0. An iid
sample of size n, X = {X1, … , Xn}, is to be taken from the population, and the
objective is to estimate the unknown population mean 𝜇based on the sample.
In the realm of Statistics, this process is carried out in two stages corresponding
to two distinct chronological time points, t1 and t2, that is, t1 < t2.
At time t1, before the sampling experiment is conducted, the observations of
X = {X1, … , Xn} are random, and so is the estimator ̂𝜇= X. One contemplates
the statistical behavior of the random quantity X, establishes all of its good
mathematical properties, derives the distributional characteristics of
√
n(X −
𝜇)∕𝜎, and ﬁnally claims that the true value of 𝜇is likely, say with probability
1 −𝛼= 0.99, contained in the random interval
(
X −z𝛼∕2
𝜎
√
n
, X + z𝛼∕2
𝜎
√
n
)
.
This is a scientiﬁcally meaningful and mathematically correct statement. Unfor-
tunately an issue is left unresolved: the occurrence of the event that the random
interval contains the true value of 𝜇is not veriﬁable, not at time t1 before the
sampling experiment is conducted nor at time t2 after the sampling experiment
is conducted.
At time t2, a realization of the sample x = {x1, … , xn} is obtained, but all the
scientiﬁc statements made about X at time t1 become much less relevant. First,
one does not know whether the true value of 𝜇is contained in x ± z𝛼∕2𝜎∕
√
n,
and therefore one does not know how close 𝜇is to the observed x. Second, one
cannot even claim that 𝜇is “probably” in the interval x ± z𝛼∕2𝜎∕
√
n since the
meaning of probability ends as the sampling experiment ends. At this point,
one has no other choices but to believe that the true value of 𝜇is “close” to x in
a very vague sense. However, it is important to remember that such a chosen
belief is essentially faith-based.
A parallel argument can be put forth for a more general statistical model,
where a parameter with an unknown value, say 𝜃, is estimated by an estimator,
say ̂𝜃(X). A statement at time t1, for example,
P (𝜃∈(a(X), b(X)) ) = 1 −𝛼
and the question whether 𝜃∈(a(x), b(x)) at time t2 are somewhat discon-
nected.
However, whether this is a valid issue or, if so, how awkward a role this
dilemma plays in the logic framework of statistical estimation is largely philo-
sophical and is, therefore, in addition to the points of the current discussion.
The relevant points here are as follows.
1) The interpretation of a statistic could change signiﬁcantly from t1 to t2.

44
Statistical Implications of Turing’s Formula
2) In the usual statistical situation, the estimand 𝜃is unambiguously deﬁned
and invariantly meaningful at both time points t1 and t2.
This is however not the case for the estimand of Turing’s formula since
𝜋0 =
∑
k≥1
pk1[Yk = 0]
is a random variable, which makes the statistical implication somewhat
unusual.
To make the subsequent discussion clearer, consider the following three dif-
ferent sampling experiments with three diﬀerent sample spaces and hence three
diﬀerent perspectives, involving four diﬀerent time points, t1, t2, t3, and t4, in a
chronological order.
Experiment 1: Draw an iid sample of size n from 𝒳at time t2.
Experiment 2: Draw an iid sample of size m from 𝒳at time t4.
Experiment 3: A two-step experiment:
Step 1: Draw an iid sample of size n from 𝒳at time t2,
and then
Step 2: Draw an additional iid sample of size m from 𝒳
at time t4.
At time t1, one anticipates Experiment 1, and both Turing’s formula T1 and
its estimand 𝜋0 are random variables. However, 𝜋0 is not a probability of any
event in the 𝜎-algebra entailed by Experiment 1 and its sample space 𝒳n. As a
random variable, 𝜋0 has some unusual statistical properties, for example, even
at time t3 when the sample has an observed realization {x1, … , xn},
˚π0 =
∑
k≥1
pk1[yk = 0]
(1.59)
is unobservable under the assumption that {pk} is unknown (however, see
Exercise 15). Since the target of the estimation is random at t1, it is not clear
what a good practical interpretation of 𝜋0 could be.
At time t2, Experiment 1 is conducted and a realization of a sample
{x1, … , xn} is observed.
At time t3, the estimand 𝜋0 takes a ﬁxed value in the form of˚π0.˚π0 becomes
a probability, not a probability associated with Experiment 1 and its 𝜎-algebra,
but a probability of an event in the 𝜎-algebra associated with Experiment 2 (if
Experiment 2 is to be conducted with m = 1) and its sample space 𝒳1. The fact
that the estimand 𝜋0 with a vague practical interpretation at time t1 transforms
itself into the probability of a very interesting and meaningful event associated
with Experiment 2 may, in part, be the reason for the oracle-like impression
about Turing’s formula.

Turing’s Formula
45
In addition to 𝜋0, there are several interesting variants of 𝜋0 worth noting.
The ﬁrst is the expected value of 𝜋0, denoted by 𝜁1,n,
𝜁1,n = E(𝜋0) = E
(
∑
k≥1
pk1[Yk = 0]
)
=
∑
k≥1
pk(1 −pk)n.
Incidentally 𝜁1,n has a probabilistic interpretation in Experiment 3 with
m = 1. Let Xi, i = 1, … , n, be the iid observations of the sample in the ﬁrst
step of Experiment 3, and Xn+1 be the random letter taken by the single
observation in the second step of Experiment 3. Let E1 be the event that Xn+1
is not one of the letters taken into the sample of size n in the ﬁrst step. Then
P(E1) =
∑
k≥1
P(X1 ≠𝓁k, … , Xn ≠𝓁k, Xn+1 = 𝓁k)
=
∑
k≥1
P(X1 ≠𝓁k, … , Xn ≠𝓁k|Xn+1 = 𝓁k)P(Xn+1 = 𝓁k)
=
∑
k≥1
pk
n
∏
i=1
P(Xi ≠𝓁k) =
∑
k≥1
pk(1 −pk)n = 𝜁1,n.
(1.60)
Note that 𝜁1,n = P(E1) is a probability associated with the 𝜎-algebra of Exper-
iment 3 and its sample space 𝒳n+1, not to be confused with˚π0.
Furthermore, consider Experiment 3 yet again with m = n. Let the two sub-
samples in the ﬁrst and the second steps be represented by the two rows as
follows:
X1,
X2,
… ,
Xn,
Xn+1, Xn+2, … , Xn+n.
Let Ej be the event that Xn+j is not one of the letters taken into the sample
of size n in the ﬁrst step. Clearly P(Ej) = ∑
k≥1 pk(1 −pk)n = 𝜁1,n for every j,
j = 1, 2, … , n. The expected number of observations in the second subsample,
whose observed letters are not included in the ﬁrst subsample, is then
𝜏n ∶=E
( n
∑
j=1
1[Ej]
)
=
n
∑
j=1
P(Ej) = n
∑
k≥1
pk(1 −pk)n = n𝜁1,n.
Both 𝜁1,n and 𝜏n = n𝜁1,n will play fundamental roles in the subsequent chapters.
1.7
Exercises
1
Consider the following three events when tossing diﬀerent number of bal-
anced dice:
a) A: observing at least one six when tossing six dice.
b) B: observing at least two sixes when tossing 12 dice.
c) C: observing at least three sixes when tossing 18 dice.
Which event has the highest probability?

46
Statistical Implications of Turing’s Formula
2
A random variable X is said to have a Poisson distribution if
pk = P(X = k) = e−𝜆𝜆k∕k!, where 𝜆> 0 is a parameter, for k = 0, 1, … .
a) Show that when 𝜆is not an integer, pk has a unique maximum. Find
that maximum.
b) When 𝜆is a positive integer, show that pk attains its maximum at two
distinct values of k. Find these two values of k.
3
One hundred families live in a remote village. The following table sum-
marizes the proportions of families with various numbers of school-aged
children.
Number of children
0
1
2
3
4
Proportion of families
0.3
0.3
0.2
0.1
0.1 .
All school-aged children from the village go to the same village school and
all students of the school are from the village.
a) Let X be the number of children in a randomly selected family in the
village. Find E(X).
b) Let Y be the number of children in the family of a randomly selected
student in the village school. Find E(Y).
4
In a sequence of identically distributed and independent Bernoulli trials,
each of which is with probability of “success” p > 0, a geometric random
variable X is such that X = k, k = 1, 2, … , if and only if the ﬁrst “success”
comes on the kth trial, that is,
pk = P(X = k) = (1 −p)k−1p,
for k = 1, … . Show that E(X) = 1∕p and that Var(X) = (1 −p)∕p2.
5
Show
a) E (T1) = ∑
k pk(1 −pk)n−1 and
b) E (𝜋0) = ∑
k pk(1 −pk)n.
6
Show T1
p→0 and 𝜋0
p→0.
7
Show E (𝜋0) ∼c(1 −p∧)n for some c > 0, where ∼represents equality in
rate of divergence or convergence and p∧= min{pk; k ≥1} for any prob-
ability distribution {pk; k ≥1} on a ﬁnite alphabet.
8
Prove Lemma 1.3.
9
Let Xn be a sequence of random variables satisfying
a) P(Xn ≥0) = 1 and
b) E(Xn) →0 as n →∞.
Then Xn
p→0.

Turing’s Formula
47
10
Show limn→∞𝜎2
n = 0 where 𝜎2
n = Var(T1 −𝜋0) under any probability
distribution.
11
Let c1 and c2 be as in Condition 1.1, and let ̂c1 and ̂c2 be as in (1.14). Show
that, if {pk; k ≥1} satisﬁes Condition 1.1,
̂c1
p
−−−→c1and ̂c2
p
−−−→c1.
(Hint: E(̂c1) →c1, Var(̂c1) →0, E(̂c2) →c2, and Var(̂c2) →0.)
12
Show that the statement of Example 1.5 is true.
13
Show that, for any real number x ∈(0, 1∕2),
1
1−x < 1 + 2x.
14
If {pk} is such that pk = c0 e−k where c0 = e −1 for all k ≥1, then there
exists an M > 0 such that E (N1) + E (N2) < M for all n ≥1.
15
Describe a sampling scheme with which ˚π0 in (1.59) is observed even
under the assumption that {pk} is unknown. (Hint: Consider the case of a
known ﬁnite K.)
16
Rewrite Theorem 1.8 into Theorem 1.9.
17
Prove Theorem 1.10.
18
Let {pk} be such that pk = Ck−𝜆for k > k0, where C, 𝜆, and a positive inte-
ger k0 are unknown parameters. Show that
P(min{k; Yk = r −1} ≥k0) →1
as n increases indeﬁnitely.
19
Show that under Condition 1.1,
n (T♯♯−𝜋0)
√
E(N1) + 2E(N2)
L
−−−→N(0, 1).
20
Verify S(3, 1) = 1, S(3, 2) = 3, and S(3, 3) = 1 by (1.53).
21
Let 𝒳= {𝓁1, 𝓁2, 𝓁3, 𝓁4}. Verify S(4, 2) = 7 by (1.53) and list all possible
partitions.
22
Show that each individual random element Xt, t = 0, … , in the special
Markov chain with memory m, deﬁned in (1.57), follows (unconditionally)
the uniform distribution on the alphabet 𝒳.

48
Statistical Implications of Turing’s Formula
23
Provide a proof for each part of Corollary 1.3. (Hint: For Part 1, use
the delta method, but consider ﬁrst an augmented Turing’s formula
T = T1 + n−n so that ln T is well deﬁned.)
24
Use Part 2 of Corollary 1.3 to verify (1.23).

49
2
Estimation of Simpson’s Indices
Consider a probability distribution {pk; k ≥1} on a countable alphabet
𝒳= {𝓁k; k ≥1} where each letter 𝓁k may be viewed as a species in a biological
population and each pk as the proportion of kth species in the population.
Simpson (1949) deﬁned a biodiversity index
𝜆=
K
∑
k=1
p2
k
for a population with a ﬁnite number of species K, which has an equivalent
form
𝜁1,1 = 1 −𝜆=
K
∑
k=1
pk(1 −pk).
(2.1)
𝜁1,1 assumes a value in [0, 1) with a higher level of 𝜁1,1 indicating a more diverse
population and is widely used across many ﬁelds of study. 𝜁1,1 is also known as
the Gini–Simpson index, an attribution to Corrado Gini for his pioneer work
in Gini (1912).
2.1
Generalized Simpson’s Indices
Simpson’s biodiversity index can be naturally and beneﬁcially generalized in
two directions. First, the size of the alphabet may be extended from a ﬁnite K
to inﬁnity. Second, 𝜁1,1 may be considered as a special member of the following
family:
𝜁u,v =
∑
k≥1
pu
k(1 −pk)v
(2.2)
where u ≥1 and v ≥0 are two integers. Since
𝜁u,v =
∑
k≥1
[pu−1
k
(1 −pk)v−1] pk(1 −pk),
Statistical Implications of Turing’s Formula, First Edition. Zhiyi Zhang.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

50
Statistical Implications of Turing’s Formula
it may be viewed as a weighted version of (2.1) with the weight for each k being
wk = pu−1
k
(1 −pk)v−1. For examples, 𝜁1,2 loads higher weight on minor species
(those with smaller pks); and 𝜁2,1 loads higher weight on major species, etc.
Equation (2.2) is proposed by Zhang and Zhou (2010) and is referred to as the
generalized Simpson’s indices.
As u assumes each positive integer value and v assumes each nonnegative
integer value, (2.2) may be expressed as a panel in (2.3). This panel of indices
is the centerpiece of many of the statistical procedures based on Turing’s per-
spective:
{𝜁u,v} = {𝜁u,v; u ≥1, v ≥0} =
⎛
⎜
⎜
⎜
⎜
⎜
⎜⎝
𝜁1,0
𝜁1,1
𝜁1,2
· · ·
𝜁1,v
· · ·
𝜁2,0
𝜁2,1
𝜁2,2
· · ·
𝜁2,v
· · ·
⋮
⋮
⋮
⋮⋮⋮
⋮
⋮⋮⋮
𝜁u,0
𝜁u,1
𝜁u,2
· · ·
𝜁u,v
· · ·
⋮
⋮
⋮
⋮⋮⋮
⋮
⋮⋮⋮
⎞
⎟
⎟
⎟
⎟
⎟
⎟⎠
(2.3)
For any probability sequence p = {pk; k ≥1} on 𝒳, let p↓be the
non-increasingly rearranged p and let {𝜁u,0; u ≥1} be the ﬁrst column of
the index panel (2.3).
Lemma 2.1
For any given probability distribution p = {pk; k ≥1} on 𝒳, p↓
and {𝜁u,0; u ≥1} uniquely determine each other.
Proof: Without loss of generality, assume p = {pk; k ≥1} is non-increasingly
ordered. It is obvious that {pk; k ≥1} uniquely determines the sequence
{𝜁u,0; u ≥1}. It suﬃces to show that {𝜁u,0;} uniquely determines {pk; k ≥1}.
Toward that end, suppose that there exists another nonincreasingly ordered
probability sequence, {qk}, on 𝒳satisfying
∑
k≥1
pu
k =
∑
k≥1
qu
k
for all u ≥1.
Let k0 = min{k ∶pk ≠qk}. If k0 does not exist, then {pk} = {qk}. If k0 exists,
then
∑
k≥k0
pu
k =
∑
k≥k0
qu
k
(2.4)
for each and every integer u ≥1. It can be easily shown that
lim
u→∞
∑
k≥k0pu
k
pu
k0
= rp ≥1
and
lim
u→∞
∑
k≥k0qu
k
qu
k0
= rq ≥1
(2.5)

Estimation of Simpson’s Indices
51
where rp and rq are, respectively, multiplicities of the value pk0 in {pk} and of
the value qk0 in {qk}. But by (2.4),
∑
k≥k0pu
k
pu
k0
=
∑
k≥k0qu
k
qu
k0
(
qk0
pk0
)u
.
(2.6)
The right-hand side of (2.6) approaches 0 or ∞if pk0 ≠qk0, which contradicts
(2.5). Therefore, k0 does not exist and {pk} = {qk}.
◽
Since {𝜁u,0; u ≥1} is a subset of {𝜁u,v; u ≥1, v ≥0}, the following theorem is
an immediate consequence of Lemma 2.1.
Theorem 2.1
For any given probability distribution p = {pk; k ≥1} on 𝒳, p↓
and {𝜁u,v; u ≥1, v ≥0} uniquely determine each other.
Theorem 2.1 has an intriguing implication: the complete knowledge of {pk}
up to a permutation and the complete knowledge of {𝜁u,v} are equivalent. In
other words, all the generalized Simpson’s indices collectively and uniquely
determine the underlying distribution {pk} upto a permutation on the index
set {k; k ≥1}. This implication is another motivation for generalizing Simp-
son’s diversity index beyond 𝜁1,1.
Theorem 2.2
For any given probability distribution p = {pk; k ≥1} on 𝒳, p↓
and {𝜁1,v; v ≥0} uniquely determine each other.
Proof: By Theorem 2.1, it suﬃces to show that {𝜁1,v; v ≥0} uniquely determines
{𝜁u,v; u ≥1, v ≥0}. For every pair of (u, v),
𝜁u,v =
∑
k≥1
pu
k(1 −pk)v
=
∑
k≥1
pu−1
k
[1 −(1 −pk)](1 −pk)v
=
∑
k≥1
pu−1
k
(1 −pk)v −
∑
k≥1
pu−1
k
(1 −pk)v+1
= 𝜁u−1,v −𝜁u−1,v+1.
That is to say that every 𝜁u,v in (2.3) may be expressed as a linear combination of
two indices in the row above, namely 𝜁u−1,v and 𝜁u−1,v+1, which in turn may be
expressed as linear combinations of indices in the row further above. Therefore,
each 𝜁u,v may be expressed as a linear combination of, and hence is uniquely
determined by, the indices in the top row, which is denoted as
{𝜁v; v ≥0} ∶= {𝜁1,v; v ≥0} = {𝜁1,0, 𝜁1,1, 𝜁1,2,…}.
◽
Theorem 2.2 suggests that the complete knowledge of {pk} is essentially cap-
tured by the complete knowledge of {𝜁v}. In this sense, {𝜁v} may be considered

52
Statistical Implications of Turing’s Formula
as a reparameterization of {pk}. This is in fact the basis for the statistical per-
spective oﬀered by Turing’s formula.
2.2
Estimation of Simpson’s Indices
Let X1, … , Xi, … , Xn be an iid sample under {pk} from 𝒳. Xi may be written
as Xi = (Xi,k; k ≥1) where for every i, Xi,k takes 1 only for one k and 0 for all
other k values. Let Yk = ∑n
i=1 Xi,k and ̂pk = Yk∕n. Yk is the number of observa-
tions of the kth species found in the sample.
For a ﬁxed pair (u, v) and an iid sample of size m = u + v, let Y (m)
k
be the
frequency of 𝓁k in the sample for every k ≥1 and
N(m)
u
=
∑
k≥1
1
[
Y (m)
k
= u
]
(2.7)
the number of species each of which is represented exactly u times in the sample
of size m = u + v. It may be easily veriﬁed that
̂𝜁(m)
u,v =
(u + v
u
)−1
N(m)
u
(2.8)
is an unbiased estimator of 𝜁u,v (see Exercise 8).
Remark 2.1
It is important to note that counting the number of diﬀerent let-
ters in a sample with exactly r observed frequency, Nr, and using that to estimate
various other quantities is the very essence of the perspective brought forward by
Turing’s formula. To honor Alan Turing’s contribution in this regard, any statis-
tical estimator based on Nr for r ≥1 in the subsequent text will be referred to as
an estimator in Turing’s perspective. The fact that (2.8) is an unbiased estima-
tor of 𝜁u,v, though straightforward, is one of the most important consequences of
Turing’s perspective.
Next follow a U-statistics construction by considering (2.8) for each of the
(
n
u+v
)
possible subsamples of size m = u + v from the original sample of size n.
Each subsample gives an unbiased estimator of 𝜁u,v, and therefore the average
of these
(
n
u+v
)
unbiased estimators
Zu,v =
(
n
u + v
)−1 ∑
∗
̂𝜁(m)
u,v =
(
n
u + v
)−1(u + v
u
)−1 ∑
∗
N(m)
u ,
(2.9)
where the summation ∑
∗is over all possible size-m subsamples of the original
size-n sample, is in fact an unbiased estimator.
On the other hand, noting (2.7) and the fact
∑
∗
N(m)
u
=
∑
k≥1
(
∑
∗
1
[
Y (m)
k
= u
])
,

Estimation of Simpson’s Indices
53
∑
∗N(m)
u
is simply the total number of times exactly u observations are found in
a same species among all possible subsamples of size m = u + v taken from the
sample of size n.
In counting the total number of such events, it is to be noted that, for any
ﬁxed u, only for species that are represented in the sample u times or more can
such an event occur. Therefore,
∑
∗
N(m)
u
=
∑
k≥1
1 [Yk ≥u]
(Yk
u
) (n −Yk
v
)
.
Furthermore, assuming v ≥1,
1 [Yk ≥u]
(Yk
u
) (n −Yk
v
)
= 1 [Yk ≥u]
[Yk(Yk −1) · · · (Yk −u + 1)
u!
]
×
[(n −Yk)(n −Yk −1) · · · (n −Yk −v + 1)
v!
]
= nu+v
u!v! 1
[
̂pk ≥u
n
] [
̂pk
(
̂pk −1
n
)
· · ·
(
̂pk −u
n + 1
n
)]
×
[
(1 −̂pk)
(
1 −̂pk −1
n
)
· · ·
(
1 −̂pk −v
n + 1
n
)]
= nu+v
u!v! 1
[
̂pk ≥u
n
] [u−1
∏
i=0
(
̂pk −i
n
)] [ v−1
∏
j=0
(
1 −̂pk −j
n
)]
and
(
n
u + v
)−1(u + v
u
)−1
= (n −u −v)!(u + v)!
n!
u!v!
(u + v)!
= (n −u −v)!u!v!
n!
.
Therefore, for every pair of (u, v) where u ≥1 and v ≥1,
Zu,v = [n −(u + v)]!nu+v
n!
×
∑
k≥1
{
1
[
̂pk ≥u
n
] [u−1
∏
i=0
(
̂pk −i
n
)] [ v−1
∏
j=0
(
1 −̂pk −j
n
)]}
.
(2.10)
In case of u ≥1 and v = 0, a similar argument leads to
Zu,v =
[u−1
∏
j=1
(
n
n −j
)]
∑
k≥1
{
1
[
̂pk ≥u
n
] [u−1
∏
i=0
(
̂pk −i
n
)]}
(2.11)
(see Exercise 9).

54
Statistical Implications of Turing’s Formula
For a few special pairs of u and v, Zu,v reduces to
Z1,1 =
n
n −1
∑
k≥1
̂pk(1 −̂pk)
Z2,0 =
n
n −1
∑
k≥1
1[̂pk ≥2∕n]̂pk(̂pk −1∕n)
Z3,0 =
n2
(n −1)(n −2)
∑
k≥1
1[̂pk ≥3∕n]̂pk(̂pk −1∕n)(̂pk −2∕n)
Z2,1 =
n2
(n −1)(n −2)
∑
k≥1
1[̂pk ≥2∕n]̂pk(̂pk −1∕n)(1 −̂pk)
Z1,2 =
n2
(n −1)(n −2)
∑
k≥1
1[̂pk ≥1∕n]̂pk(1 −̂pk)(1 −1∕n −̂pk).
(2.12)
Zu,v is an unbiased estimator of 𝜁u,v. This fact is established by the U-statistic
construction of the above-mentioned estimator. In fact, Zu,v is a uniformly min-
imum variance unbiased estimator (umvue) of 𝜁u,v when the cardinality of 𝒳,
K, is ﬁnite. Since Zu,v is unbiased, by the Lehmann–Scheﬀé theorem, it suﬃces
to note that {̂pk} is a set of complete and suﬃcient statistics under {pk} (see
Exercise 12).
2.3
Normal Laws
The same U-statistic construction paves the path for establishing the asymp-
totic normality of Zu,v −𝜁u,v. For a review of the theory of U-statistics, readers
may wish to refer to Serﬂing (1980) or Lee (1990).
Let
1) X1,…, Xn be an iid sample under a distribution F;
2) 𝜃= 𝜃(F) be a parameter of interest;
3) h(X1,…, Xm) where m < n be a symmetric kernel satisfying
EF{h(X1,…, Xm)} = 𝜃(F);
4) Un = U(X1,…, Xn) =
(
n
m
)−1∑
∗h(Xi1,…, Xim) where the summation ∑
∗is
over all possible subsamples of size m from the original sample of size n;
5) h1(x1) = EF(h(x1, X2,…, Xm)) be the conditional expectation of h given
X1 = x1; and
6) 𝜎2
1 = VarF(h1(X1)).
The following lemma is due to Hoeﬀding (1948).

Estimation of Simpson’s Indices
55
Lemma 2.2
If EF(h2) < ∞and 𝜎2
1 > 0, then
√
n(Un −𝜃)
L
−−−→N(0, m2𝜎2
1).
To apply Lemma 2.2 to the U-statistic Zu,v, the two conditions EF(h2) < ∞
and 𝜎2
1 > 0 must be checked.
Let
m = u + v,
h = h(X1,…, Xm) =
(m
u
)−1
N(m)
u ,
F = {pk}.
Clearly in this case, EF(h2) < ∞since N(m)
u
is bounded above by every m.
Therefore, only the second condition of Lemma 2.2, namely 𝜎2
1 > 0, needs to
be checked.
Toward that end, suppose u ≥1 and v ≥1. Given
X1 = x1 = (x1,1, x1,2,…, x1,k, · · ·),
(m
u
)
h1(x1) =
(m
u
)
EF(h(x1, X2,…, Xm))
= EF (N(m)
u |X1 = x1)
=
∑
k≥1
1 [x1,k = 1]
(m −1
u −1
)
pu−1
k
(1 −pk)v
+
∑
k≥1
1 [x1,k = 0]
(m −1
u
)
pu
k(1 −pk)v−1
=
∑
k≥1
(m −1
u
)
pu
k(1 −pk)v−1
+
∑
k≥1
1[x1,k = 1]
(m −1
u
)
pu−1
k
(1 −pk)v−1[
(1−pk)u
v −pk
]
=
(m −1
u
) ∑
k≥1
pu
k(1 −pk)v−1
+
(m −1
u
)∑
k≥1
1[x1,k = 1]pu−1
k
(1 −pk)v−1[
(1−pk)u
v −pk
]
,
or
h1(x1) =
v
u + v
∑
k≥1
pu
k(1 −pk)v−1
+
v
u + v
∑
k≥1
1[x1,k = 1]pu−1
k
(1 −pk)v−1 [
(1 −pk)u
v −pk
]
,

56
Statistical Implications of Turing’s Formula
and therefore
𝜎2
1(u, v) = VarF(h1(X1))
=
(
v
u + v
)2
VarF
{
∑
k≥1
1[X1,k = 1]pu−1
k
(1 −pk)v−1 [
(1 −pk)u
v −pk
]}
=
(
v
u + v
)2 ⎧
⎪
⎨
⎪⎩
EF
{
∑
k≥1
1[X1,k = 1]pu−1
k
(1 −pk)v−1 [
(1 −pk)u
v −pk
]}2
−
{
∑
k≥1
pu
k(1 −pk)v−1 [
(1 −pk)u
v −pk
]}2⎫
⎪
⎬
⎪⎭
=
(
v
u + v
)2 ⎧
⎪
⎨
⎪⎩
∑
k≥1
p2u−1
k
(1 −pk)2v−2[
(1 −pk)u
v −pk
]2
−
{
∑
k≥1
pu
k(1 −pk)v−1 [
(1 −pk)u
v −pk
]}2⎫
⎪
⎬
⎪⎭
= u2
v2
(
v
u + v
)2 ∑
k≥1
p2u−1
k
q2v
k −2u
v
(
v
u + v
)2 ∑
k≥1
p2u
k q2v−1
k
+
(
v
u + v
)2 ∑
k≥1
p2u+1
k
(1 −pk)2v−2
−
(
v
u + v
)2[
u
v
∑
k≥1
pu
k(1 −pk)v −
∑
k≥1
pu+1
k
(1 −pk)v−1
]2
= u2
v2
(
v
u + v
)2
𝜁2u−1,2v −2u
v
(
v
u + v
)2
𝜁2u,2v−1
+
(
v
u + v
)2
𝜁2u+1,2v−2
−
(
v
u + v
)2(u
v 𝜁u,v −𝜁u+1,v−1
)2
=
u2
(u + v)2 𝜁2u−1,2v −
2uv
(u + v)2 𝜁2u,2v−1
+
v2
(u + v)2 𝜁2u+1,2v−2 −
v2
(u + v)2
(u
v 𝜁u,v −𝜁u+1,v−1
)2
≥0.
(2.13)
The last inequality in (2.13) becomes an equality if and only if h(X1) is a con-
stant that occurs when all the positive probabilities of {pk} are equal.

Estimation of Simpson’s Indices
57
Suppose u ≥1 and v = 0 and therefore
(
m
u
)
= 1. It is easy to see that
h1(x1) =
∑
k≥1
1[x1,k = 1]pu−1
k
and
𝜎2
1(u, 0) = VarF(h1(X1)) = 𝜁2u−1,0 −𝜁2
u,0 ≥0
(2.14)
(see Exercise 10). The strict inequality holds for all cases except when {pk} is
uniform.
Thus, the following theorem is established.
Theorem 2.3
For any given pair of u and v,
1) if {pk} is such that 𝜎2
1(u, v) > 0, then
√
n(Zu,v −𝜁u,v)
L
−−−→N(0, (u + v)2𝜎2
1(u, v));
2) if {pk} is such that 𝜎2
1(u, 0) > 0, then
√
n(Zu,0 −𝜁u,0)
L
−−−→N(0, u2𝜎2
1(u, 0)).
Theorem 2.3 immediately implies consistency of Zu,v to 𝜁u,v and the consis-
tency of Zu,0 to 𝜁u,0 for any u ≥1 and v ≥1 under the stated condition.
By the last expressions of (2.13) and (2.14), and Theorem 2.3, it is easily seen
that when u ≥1 and v ≥1,
̂𝜎2
1(u, v) =
u2
(u + v)2 Z2u−1,2v
−
2uv
(u + v)2 Z2u,2v−1
+
v2
(u + v)2 Z2u+1,2v−2
−
v2
(u + v)2
(u
v Zu,v −Zu+1,v−1
)2
(2.15)
and
̂𝜎2
1(u, 0) = Z2u−1,0 −Z2
u,0
(2.16)
are consistent estimators of 𝜎2
1(u, v) and of 𝜎2
1(u, 0), respectively, and hence the
following corollary is established.
Corollary 2.1
Suppose, for any given pair of u and v, the condition of
Theorem 2.3 for each of the two parts is respectively satisﬁed. Then the following

58
Statistical Implications of Turing’s Formula
cases occur:
1) if {pk} is such that 𝜎2
1(u, v) > 0, then
√
n(Zu,v −𝜁u,v)
(u + v)̂𝜎1(u, v)
L
−−−→N(0, 1)
(2.17)
2) if {pk} is such that 𝜎2
1(u, 0) > 0, then
√
n(Zu,0 −𝜁u,0)
û𝜎1(u, 0)
L
−−−→N(0, 1),
(2.18)
where ̂𝜎2
1(u, v) and ̂𝜎2
1(u, 0) are as in (2.15) and (2.16) respectively.
Remark 2.2
The condition 𝜎2
1(u, v) > 0 of Part 1 for both Theorem 2.3 and
Corollary 2.1 is diﬃcult to verify in practice. However under the following two
special cases, the condition has an equivalent form.
1) When u = v = 1, 𝜎2
1(u, v) > 0 if and only if {pk} is not a uniform distribution
on 𝒳.
2) When u ≥1 and v = 0, 𝜎2
1(u, 0) > 0 if and only if {pk} is not a uniform dis-
tribution on 𝒳.
As a case of special interest when u = v = 1, the computational formula of
Z1,1 is given in (2.12) and
√
n(Z1,1 −𝜁1,1)
2̂𝜎1(1, 1)
L
−−−→N(0, 1)
(2.19)
where ̂𝜎1(1, 1) is such that
4̂𝜎2
1(1, 1) = Z1,2 −2Z2,1 + Z3,0 −(Z1,1 −Z2,0)2
and Z1,2, Z2,1, Z3,0, and Z2,0 are all given in (2.12). Equation (2.19) may be used
for large sample inferences with respect to Simpson’s index, 𝜁1,1, whenever the
nonuniformity of the underlying distribution is deemed reasonable.
Zu,v is asymptotically eﬃcient when K is ﬁnite. This fact is established
by recognizing ﬁrst that {̂pk} is the maximum likelihood estimator (mle)
of {pk}, second that ̂𝜁∗
u,v = ∑̂pu
k(1 −̂pk)v is the mle of 𝜁u,v, and third that
√
n(Zu,v −̂𝜁∗
u,v)
p
−−−→0.
In simplifying the expression of Zu,v in (2.10), let
Zu,v = an(u + v)Z∗
u,v
where
an(u + v) = [n −(u + v)]!nu+v
n!

Estimation of Simpson’s Indices
59
and
Z∗
u,v =
K
∑
k=1
1
[
̂pk ≥u
n
] [u−1
∏
i=0
(
̂pk −i
n
) v−1
∏
j=0
(
1 −̂pk −j
n
)]
.
Since
√
n(Zu,v −̂𝜁∗
u,v) =
√
n(an(u + v) −1)Z∗
u,v +
√
n(Z∗
u.v −̂𝜁∗
u,v),
(2.20)
the third fact may be established by showing that each of the above two terms
converges to zero in probability, respectively. Toward that end, consider the
following steps:
Step 1. Z∗
u,v ≤1. This is so because every summand in Z∗
u,v is nonnegative and
bounded above by ̂pk and ∑K
k=1 ̂pk = 1.
Step 2.
√
n(an(u + v) −1) →0 as n →∞. Let m = u + v be any ﬁxed positive
integer. This statement is proven by induction. If m = 1, then
√
n(an(1) −1) =
√
n
[(n −1)!n
n(n −1)! −1
]
= 0.
Assume that for any positive integer m −1,
√
n(an(m −1) −1) →0 as
n →∞. Then
√
n(an(m) −1) =
√
n
[(n −m)!nm
n!
−1
]
=
√
n
{[n −(m −1)]!nm−1n
n![n −(m −1)]
−1
}
=
√
n
[ an(m −1)n
(n −m + 1) −an(m−1) + an(m−1)−1
]
=
√
nan(m −1)
m −1
n −m + 1 +
√
n(an(m−1)−1).
Noting an(m −1) →1 and, by the inductive assumption
√
n(an(m −
1) −1) →0, it follows that
√
n(an(m) −1) →0. At this point, it has
been established that the ﬁrst term in (2.20) converges to zero in
probability, that is,
√
n(an(u + v) −1)Z∗
u,v
p
−−−→0. It only remains to
show
√
n(Z∗
u.v −̂𝜁∗
u,v).
Step 3. Re-expressing the mle of 𝜁u,v in
̂𝜁∗
u,v =
K
∑
k=1
1
[
̂pk ≥u
n
]
̂pu
k(1 −̂pk)v +
K
∑
k=1
1
[
̂pk < u
n
]
̂pu
k(1 −̂pk)v
=∶̂𝜁(1)
u,v + ̂𝜁(2)
u,v.

60
Statistical Implications of Turing’s Formula
It suﬃces to show that
√
n
{ K
∑
k=1
1
[
̂pk ≥u
n
][u−1
∏
i=0
(
̂pk −i
n
) v−1
∏
j=0
(
1 −̂pk −j
n
)]
−̂𝜁(1)
u,v
}
−
√
n ̂𝜁(2)
u,v
=
√
n
{ K
∑
k=1
1
[
̂pk ≥u
n
][u−1
∏
i=0
(
̂pk −i
n
) v−1
∏
j=0
(
1−̂pk −j
n
)
−̂pu
k(1 −̂pk)v
]}
−
√
n ̂𝜁(2)
u,v
p
−−−→0,
(2.21)
or to show that each of the above-mentioned two terms converges to
zero in probability.
Step 4. Consider ﬁrst v = 0, in which case
v−1
∏
j=0
(
1 −̂pk −j
n
)
= 1.
∏u−1
i=0
(
̂pk −i
n
)
may be written as a sum of ̂pu
k and ﬁnitely many other
terms each of which has the following form:
s1
ns2 ̂ps3
k
where s1, s2 ≥1, and s3 ≥1 are ﬁnite ﬁxed integers. Since
0 ≤
√
n
K
∑
k=1
1
[
̂pk ≥u
n
] |s1|
ns2 ̂ps3
k ≤
√
n
K
∑
k=1
|s1|
ns2 ̂pk =
√
n|s1|
ns2 →0
as n →∞, the ﬁrst term of (2.21) converges to zero in probability. The
second term of (2.21) converges to zero when u = 1 is an obvious case
since ̂𝜁(2)
u,v = 0. It also converges to zero in probability when u ≥2 since
0 ≤
√
n
K
∑
k=1
1
[
̂pk < u
n
]
̂pu
k ≤
√
n
K
∑
k=1
(u
n
)
̂pk ≤
u
√
n
→0.
Step 5. Next consider the case of v ≥1. ∏u−1
i=0
(
̂pk −i
n
) ∏v−1
j=0
(
1 −̂pk −j
n
)
may
be written as a sum of ̂pu
k(1 −̂pk)v and ﬁnitely many other terms each
of which has the following form:
s1
ns2 ̂ps3
k (1 −̂pk)s4

Estimation of Simpson’s Indices
61
where s1, s2 ≥1, s3 ≥1, and s4 ≥1 are ﬁnite ﬁxed integers. Since
0 ≤
√
n
K
∑
k=1
1
[
̂pk ≥u
n
] |s1|
ns2 ̂ps3
k (1 −̂pk)s4
≤
√
n
K
∑
k=1
|s1|
ns2 ̂pk <
√
n|s1|
ns2 →0
as n →∞, the ﬁrst term of (2.21) converges to zero in probability. The
second term of (2.21) remains the same as in Step 4 and therefore con-
verges to zero in probability.
Thus, the asymptotic eﬃciency of Zu,v is established.
2.4
Illustrative Examples
In practice, it is often of interest to have a statistical tool to test the equality of
diversity between two populations against a diﬀerence across time or space. Let
the said diversity be measured by 𝜁u,v. The appropriate hypothesis to be tested
is often of the form:
H0 ∶𝜁(1)
1,1 −𝜁(2)
1,1 = D0 versus Ha ∶𝜁(1)
1,1 −𝜁(2)
1,1 > D0 (or < D0 or ≠D0)
where D0 is a preﬁxed known constant, and 𝜁(1)
1,1 and 𝜁(2)
1,1 are, respectively, the
true 𝜁u,v values for the two underlying populations. Similarly, the superscripts
(1) and (2) are to index all mathematical objects associated with the two popu-
lations whenever appropriate.
Suppose there are available two large independent iid samples of sizes n1 and
n2. Based on Corollary 2.1, under H0,
Z =
(
Z(1)
u,v −Z(2)
u,v
)
−D0
(u + v)
√
(̂𝜎(1)
1 (u,v))
2
n1
+ (̂𝜎(2)
1 (u,v))
2
n2
L
−−−→N(0, 1)
(2.22)
(see Exercise 15) where (̂𝜎(i)
1 (u, v))2 is as given in (2.15) with the ith sample,
i = 1, 2. In regard to the hypothesis, one would reject H0 in favor of Ha if the
test statistic Z assumes a value deemed statistically unusual.
Wang and Dodson (2006) oﬀered an analysis on several aspects of dinosaur
diversity based on an interesting fossil data set, which includes two subsamples,
one early Cretaceous and one late Cretaceous. The early Cretaceous set con-
tains n1 = 773 pieces of fossil belonging to 139 genera; the late Cretaceous set
contains n2 = 1348 pieces of fossil belonging to 237 genera. Let r be the num-
ber of observations in each observed genus and Nr be the number of observed
genera containing exactly r observations, as in (1.1). The two subsamples are
presented in Tables 2.1 and 2.2 in terms of r and Nr.

62
Statistical Implications of Turing’s Formula
Table 2.1 Early Cretaceous
r
1
2
3
4
5
6
7
8
nr
90
19
12
3
2
1
1
1
r
9
13
14
15
30
38
124
300
nr
2
1
2
1
1
1
1
1
Table 2.2 Late Cretaceous
r
1
2
3
4
5
6
7
8
9
10
nr
136
19
17
7
13
6
4
2
2
1
r
11
12
13
16
17
18
20
24
25
26
nr
1
4
3
2
1
2
3
1
2
1
r
29
30
33
40
41
43
51
81
201
nr
1
2
1
1
1
1
1
1
1
Table 2.3 Zu,v of early Cretaceous
Z(e)
1,1
Z(e)
2,0
Z(e)
3,0
Z(e)
2,1
Z(e)
1,2
0.8183
0.1817
0.0623
0.1193
0.6990
Suppose it is of interest to detect a decrease in the Gini–Simpson diver-
sity index 𝜁1,1 from the early Cretaceous period to the late Cretaceous period.
Denote the true Simpson’s diversity indices at the early and the late Cretaceous
periods by 𝜁(e)
1,1 and 𝜁(l)
1,1, respectively. The appropriate hypothesis to be tested is
H0 ∶𝜁(e)
1,1 −𝜁(l)
1,1 = 0 versus Ha ∶𝜁(e)
1,1 −𝜁(l)
1,1 > 0.
(2.23)
Assuming the two fossil samples are iid, the value of the test statistic Z in
(2.22) is calculated to complete the test. Toward that end, the respective values
of Z1,1, Z2,0, Z3,0, Z2,1, and Z1,2 in (2.12) for the two samples are summarized in
Tables 2.3 and 2.4.
By (2.15),
(̂𝜎(e)
1 (1, 1))2 = 1
4 × 0.6990 −1
2 × 0.1193 + 1
4 × 0.0623
−1
4 × (0.8183 −0.1817)2
= 0.0293

Estimation of Simpson’s Indices
63
Table 2.4 Zu,v of late Cretaceous
Z(l)
1,1
Z(l)
2,0
Z(l)
3,0
Z(l)
2,1
Z(l)
1,2
0.9642
0.0358
0.0037
0.0321
0.9321
and
(̂𝜎(l)
1 (1, 1))2 = 1
4 × 0.9321 −1
2 × 0.0321 + 1
4 × 0.0037
−1
4 × (0.9642 −0.0358)2
= 0.0024
which lead to, by (2.22),
Z = 0.8183 −0.9642
2
√
0.0293
773 + 0.0024
1348
= −11.5706,
which provides no statistical evidence of a decrease in diversity, as measured
by the Gini–Simpson index, from the early Cretaceous period to the late
Cretaceous period. In fact, the highly negative value of the test statistic may
even suggest an increase in diversity over that time period.
Simpson’s index was originally used to measure the diversity of subspecies of
the dinosaur population hundreds of million years ago. The idea was to infer
from fossil data whether the diversity of dinosaur population had a gradual
diminishing period prior to its sudden extinction about 65 million years ago.
If such a diminishing period existed, then the extinction of dinosaurs may be a
consequence of natural evolution. If not, then the hypothesis of the Cretaceous
extinction, due to a sudden extraterrestrial impact, such as a huge asteroid or a
massive round of volcanoes, may be deemed more creditable.
In practice, an appropriate choice of speciﬁc pair of integer values (u, v) often
varies according to situational need. In general, a higher value of v assigns more
weight to species with smaller proportions, that is, with smaller pks, and a
higher value of u assigns more weight to species with larger proportions, that
is, with larger pks. However, one may choose to estimate 𝜁u,v for a set of (u, v)
values, also known as a proﬁle. For example,
{𝜁1,v; v = 1, 2,…}
(2.24)
is a proﬁle and it may be considered as a function of v on ℕ. Using proﬁles
to compare the diversity of two ecological communities was ﬁrst discussed by
Taillie (1979) and Patil and Taillie (1982), among others, and has since become
an important tool for ecologists.

64
Statistical Implications of Turing’s Formula
Denote the two proﬁles of the two underlying populations and the diﬀerence
between them as, respectively,
{
𝜁(1)
1,v
}
,
{
𝜁(2)
1,v
}
,
{𝛿v
} =
{
𝜁(1)
1,v −𝜁(2)
1,v
}
,
where {𝛿v} = {𝛿v; v ≥1} is sometimes called a diﬀerence proﬁle.
̂𝛿v = Z(1)
1,v −Z(2)
1,v
(2.25)
gives an unbiased estimator of 𝛿v for every v, v = 1,…, min{n1 −1, n2 −1}.
Furthermore, (2.22) with u = 1 gives an approximate (1 −𝛼) × 100% point-wise
conﬁdence band
̂𝛿v ± z𝛼∕2(v + 1)
√
√
√
√
√
(
̂𝜎(1)
1 (1, v)
)2
n1
+
(
̂𝜎(2)
1 (1, v)
)2
n2
.
(2.26)
To illustrate, two cases of authorship attribution are considered next. The
concept of species diversity in an ecological population is often applied to quan-
titative linguistics as that of vocabulary diversity or lexical richness. It is often
thought that diﬀerent authors share a common alphabet, say the English vocab-
ulary, but that each has a propensity for using certain word types over others
and hence a unique distribution of word types. If this notion is deemed reason-
able, then the diﬀerence between the two authors’ distributions of word types
may be captured by certain diversity measures or a proﬁle of them.
Consider the following three text samples:
1) The collection of 154 sonnets, which are commonly believed to have
been penned by William Shakespeare, in a book titled “SHAKESPEARES
SONNETS” published by Thomas Thorpe in 1609. This collection contains
n = 17539 word types and is used as the corpus in the two cases of
authorship attribution.
2) The collection of three sonnets in the script of Romeo and Juliet,
a) the prologue to Act I;
b) Act I, Scene 5, “If I profane with my unworthiest hand … Thus from my
lips, by thine, my sin is purg’d.”; and
c) the prologue to Act II.
This collection contains n = 339 word types.
3) The collection of the ﬁrst four sonnets from Philip Sidney’s Astrophel and
Stella sequence,
a) “Loving in truth, … My Muse said to me ‘Fool, look in your heart and
write.’”;
b) “Not at ﬁrst sight, … While with sensitive art I depict my self in hell.”;
c) “Let dainty wits cry on the Sisters nine, … Is to copy what Nature has
written in her.”; and
d) “Virtue, … Virtue, will be in love with her.”.
This collection contains n = 1048 word types.

Estimation of Simpson’s Indices
65
Case 1 consists of a comparison between the corpus of 154 Shakespearean
sonnets, indexed by (1), and the sample of three Shakespearean sonnets in the
script of Romeo and Juliet, indexed by (2). Case 2 consists of a comparison
between the same corpus of Shakespearean sonnets, indexed by (1), and the
sample of the ﬁrst four sonnets in Philip Sidney’s and Stella sequence, index
by (2). Let the proﬁle {𝜁1,v; v ≥1} of each underlying population of interest be
restricted to a subset, that is, {𝜁1,v; 1 ≤v ≤r}, and consequently consider only
the estimation of the correspondingly restricted diﬀerence proﬁle {𝛿v; 1 ≤v ≤
r}, where r = 50.
Using (2.25) and (2.26) with 𝛼= 0.05, the estimated proﬁle diﬀerences and
their corresponding 95% point-wise conﬁdence bands are represented graphi-
cally in Figures 2.1 and 2.2, where the curve in the middle represents the proﬁle
diﬀerences while the upper and lower curves are, respectively, the upper bound
and lower bound of the conﬁdence band. It may be interesting to note that, in
Figure 2.1, the 95% conﬁdence band well contains the zero line (the dashed line)
for all v, 1 ≤v ≤50, indicating no statistical evidence of a diﬀerence, as one
would expect. However, in Figure 2.2, the 95% conﬁdence band mostly bounds
away from the zero line, indicating a diﬀerence in pattern of word usage, partic-
ularly the less commonly used words as they are assigned with higher weights
by higher values of v.
30
Estimated profile difference
20
10
0
–0.10
–0.05
0.00
0.05
0.10
v
40
50
Figure 2.1 Estimated diﬀerence proﬁle: Shakespeare versus Shakespeare.

66
Statistical Implications of Turing’s Formula
30
Estimated profile difference
20
10
0
–0.10
–0.05
0.00
0.05
0.10
v
40
50
Figure 2.2 Estimated diﬀerence proﬁle: Shakespeare versus Sidney.
2.5
Remarks
By the U-statistic construction, Zu,v of (2.10) exists and is unbiased only if m =
u + v is less or equal to the sample size n. However, for Corollary 2.1 to hold,
m = u + v must further satisfy 2u + 2v −1 ≤n or u + v ≤(n + 1)∕2 due to the
required existence of Z2u−1,2v, Z2u,2v−1, and Z2u+1,2v−2. This is indeed a restric-
tion on the choices of u and v in practice. However, it must be noted that for a
suﬃciently large n, any one 𝜁u,v is estimable.
Since Simpson (1949), research on diversity has proliferated. However, even
today diversity or biodiversity is still an unsettled concept. There are quite a few
diﬀerent schools of thought on what diversity is and how it should be precisely
deﬁned. Consequently, a universally accepted diversity index has not yet come
to be although several well-known diversity indices are quite popular among
practitioners, for example, the Gini–Simpson index 𝜁1,1, Shannon’s entropy by
Shannon (1948), Rényi’s entropy by Rényi (1961), and Tsallis’ index by Tsal-
lis (1988), among others. The indices in (2.3), while a natural generalization of
Simpson’s diversity index, have far-reaching implications beyond the realm of
diversity. For reasons to be seen more clearly in subsequent chapters, the subset
of the panel, namely {𝜁v; v ≥1}, captures all the information about the under-
lying distribution {pk}, particularly the tail information. Although this fact is

Estimation of Simpson’s Indices
67
made clear in Theorem 2.2, the importance of estimability of 𝜁v, up to v = n −1,
cannot be overstated.
The perspective on {pk; k ≥1} by way of {𝜁v; v ≥1} oﬀers several advantages
in certain situations. However, when it comes to statistical estimation based
on a sample, it is not without a cost. For example, for every k, there exists an
unbiased estimator of pk, namely ̂pk = Yk∕n. (In fact, it is an unvue.) In other
words, the entire distribution {p1,…} is estimable. This is however not the case
for {𝜁v; v ≥1}. Given an iid sample of size n, only the ﬁrst n −1 components in
{𝜁v; v ≥1} are estimable. For example, 𝜁n is not estimable. The following is an
argument to support the claim.
For any n,
𝜁n =
∑
k≥1
pk(1 −pk)n
=
∑
k≥1
pk(1 −pk)n−1 −
∑
k≥1
p2(1 −pk)n−1
=
∑
k≥1
pk(1 −pk)n−1 −
∑
k≥1
p2
k(1 −pk)n−2 +
∑
k≥1
p3
k(1 −pk)n−2
=
n
∑
v=1
[
(−1)v−1 ∑
k≥1
pv
k(1 −pk)n−v
]
+ (−1)n+1 ∑
k≥1
pn+1
k
.
(2.27)
By the existence of Zn,0, Zn−1,1,…, Z1,n−1, the sum in the ﬁrst term of the
last expression of (2.27) is estimable. Therefore, if 𝜁n were estimable, then the
last term of (2.27), 𝜁n+1,0 = ∑
k≥1pn+1
k
, would be estimable. In the following text,
it is shown that 𝜁n+1,0 is not estimable based on a sample of size n. In fact, it
is to be shown, more generally, that 𝜁(c)
n+1,0 = ∑
k≥1ckpn+1
k
, where {ck; k ≥1} are
known coeﬃcients with at least one nonzero element, is not estimable based
on a sample of size n.
First consider a ﬁnite alphabet containing K ≥2 letters. Suppose 𝜁(c)
n+1,0
were estimable, that is, there existed an unbiased estimator ̂𝜁(c)
n+1,0(X) where
X = (X1,…, Xn)′ and each Xi takes one of the K letters in the alphabet
according to {pk}, such that
E
(
̂𝜁(c)
n+1,0(X)
)
=
∑
k≥1
ckpn+1
k
.
(2.28)
The left-hand side of (2.28) is of the form
∑
∗
̂𝜁(c)
n+1,0(x)P(X = x),
(2.29)
where the summation ∑
∗is over all Kn possible conﬁgurations of a sample of
size n and x is a realization of X. Equation (2.29) is a K-variable (p1,…, pK) poly-
nomial of degree n. For (2.28) to hold for all possible {pk; k ≥1} on the ﬁnite
alphabet, the coeﬃcients of the corresponding terms must be equal. In particu-
lar, (c1,…, cK)′ = (0,…, 0)
′, which contradicts the assumption that at least one

68
Statistical Implications of Turing’s Formula
of them is nonzero. This establishes the claim that 𝜁n = ∑
k≥1pk(1 −pk)n is not
estimable based on an iid sample of size n when the alphabet is ﬁnite. This result
can be immediately extended from the case of a ﬁnite alphabet to the case of a
countable alphabet since the former is a special case of the latter.
With a cost of the perspective by way of {𝜁v; v ≥1} established earlier, why
would one then prefer {𝜁v; v ≥1} over {pk; k ≥1}? The answer to the ques-
tion lies in the fact that, in Statistics, often the object of estimation is not p =
{pk; k ≥1} itself but a functional of it, say F(p). In estimating certain types of
F(p), the estimability of the entire p becomes much less important than that of
the ﬁrst n −1 component of the {𝜁v; v ≥1}.
For an example, suppose the object of estimation is the Gini–Simpson index,
that is,
F(p) = 𝜁1 =
∑
k≥1
pk(1 −pk).
The estimability of {pk; k ≥1} does not directly translate to that of F(p), by
means of, say, the plug-in
̂F(p) = F(̂p) =
∑
k≥1
̂pk(1 −̂pk),
which is an estimator with a sizable negative bias. It is not surprising that the
plug-in estimator has a sizable bias because when the cardinality of the alpha-
bet, K, is relatively large compared to the sample size n, many (possibly inﬁnite)
letters in the alphabet would not be represented in the sample. Consequently,
the corresponding ̂pks would be zero and therefore causing ̂pk(1 −̂pk) = 0, all
the while the sum of pk(1 −pk)s of the corresponding indices ks could accumu-
late to be a signiﬁcant value. Therefore, the plug-in F(̂p) underestimates F(p).
On the other hand, the partial estimability of {𝜁v; v ≥1} directly translates to
that of F(p), that is, E(Z1,1) = F(p) = 𝜁1.
In summary, Turing’s perspective by way of {𝜁v; v ≥1} is not necessarily a
better perspective than that by way of {pk; k ≥1} in general. However, in situa-
tions where tail characteristics play a greater role, Turing’s perspective leads to a
more eﬃcient way to amplify and to extract tail-relevant statistical information.
2.6
Exercises
1
Let X be a Bernoulli random variable with parameter p ∈(0, 1), that is,
P(X = 1) = p and P(X = 0) = 1 −p. Find the ﬁrst to the fourth central
moments of X.
2
Let X be a Poisson random variable with parameter 𝜆> 0, that is, pk =
P(X = k) = e−𝜆𝜆k∕k! for k = 0, 1, · · ·. Find
E[X(X −1)(X −2) · · · (X −k + 1)]
for any positive integer k.

Estimation of Simpson’s Indices
69
3
An experiment involves repeatedly tossing together a fair coin and bal-
anced die until a “3” appears on the die. Let Y be the number of tosses
needed to see the ﬁrst “3” on the die, and let X be the total number of
heads observed on the coin when the experiment ends.
a) Find the probability distribution of Y and calculate E(Y).
b) Find the probability distribution of X and calculate E(X).
4
Let X1 be a binomial random variable with parameters n1 = 5 and p1 =
1∕2; and let X2 be an independent binomial random variable with param-
eters n2 = 5 and p2 = 1∕4. Find the conditional probability P(X1 = 2|X1 +
X2 = 4).
5
If the eﬀective cardinality of 𝒳is ﬁnite, that is, K < ∞, show that the
Gini–Simpson index 𝜁1,1 of (2.1) is maximized at the uniform distribution,
that is, pk = 1∕K for every k, k = 1,…, K.
6
Derive (2.5).
7
For any probability sequence {pk; ≥1} on 𝒳, show that {pk; ≥1} is equiv-
alent in permutation to {𝜁u,v; u ≥1} where v is any ﬁxed nonnegative
integer.
8
Show that ̂𝜁(m)
u,v given in (2.8) is an unbiased estimator of 𝜁u,v.
9
Show that, in case of u ≥1 and v = 0, Zu,v of (2.10) reduces to Zu,v of (2.11).
10
Verify (2.14).
11
Show that for any pair of u ≥1 and v ≥0, as n →∞,
[n −(u + v)]!nu+v
n!
→1.
12
Suppose the cardinality of 𝒳is a known and ﬁnite integer K ≥2. Show
that {̂pk; k ≥1} is a set of jointly complete and suﬃcient statistics.
13
For every pair of (u, v), where u ≥1 and v ≥0, show that
𝜁u,v =
u
∑
i=1
(u −1
i −1
)
(−1)u−i𝜁1,u+v−i.
14
The estimator ̂𝜁∗
u,v of 𝜁u,v is a mle when the cardinality K is ﬁnite and
known. When K is ﬁnite but unknown, the estimator ̂𝜁∗
u,v is still well
deﬁned, but is no longer an mle of 𝜁u,v (why?). In this case, it is called the

70
Statistical Implications of Turing’s Formula
plug-in estimator of 𝜁u,v. Show in case of an unknown but ﬁnite K, the
estimator retains its asymptotic eﬃciency.
15
Verify (2.22).
16
Verify the entries of Table 2.3.
17
Verify the entries of Table 2.4.
18
Show that the plug-in estimator
̂𝜁∗
1,1 =
∑
k≥1
̂pk(1 −̂pk)
of 𝜁1,1 is a biased estimator.
19
Show that
a) when u = v = 1, 𝜎2
1(u, v) > 0 if and only if {pk} is not a uniform distri-
bution on 𝒳;
b) when u ≥1 and v = 0, 𝜎2
1(u, 0) > 0 if and only if {pk} is not a uniform
distribution on 𝒳.
(Hint: Consider the case when h1(x1) is a constant.)
20
Starting at 0, a random walk on the x-axis takes successive one-unit steps,
going to the right with probability p and to the left with probability 1 −p.
The steps are independent. Let X denote the position after n steps. Find
the distribution of X.
21
Let X1 and X2 be, respectively, binomial random variables with parameters
(n, p1) and (n, p2). Show that, if p1 < p2,
P(X1 ≤k) ≥P(X2 ≤k)
for every k, k = 0, 1,…, n.
22
Let X1,…, Xm,…, Xn be a sample of iid random variable X with E(X) =
𝜇< ∞. Let Sm = ∑m
i=1 Xi and Sn = ∑n
i=1 Xi, where n ≥m.
a) Find E(Sn|Sm = sm).
b) Find E(Sm|Sn = sn).
23
Suppose that X is a Poisson random variable with intensity parameter
𝜆X > 0 and that, given X = k, Y is a binomial random variable with param-
eters n = k and p > 0. Show that Y is a Poisson random variable with
intensity parameter 𝜆Y = 𝜆Xp.

71
3
Estimation of Shannon’s Entropy
Let 𝒳= {𝓁k; k ≥1} be a countable alphabet and p = {pk; k ≥1} be an
associated probability distribution of the alphabet. Shannon (1948) deﬁned
entropy of p in the form of HS = −∑
k≥1pklog2 pk. Often Shannon’s entropy is
also expressed with the natural logarithm,
H = −
∑
k≥1
pk ln pk,
(3.1)
which diﬀers from HS by a constant factor, H = HS ln 2. Throughout this book,
H of (3.1) is referred to as Shannon’s entropy. H is of profound importance
in many branches of modern data sciences. In recent decades as entropy
ﬁnds its way into a wider range of applications, the estimation of entropy
also becomes increasingly important. The volume of literature on the general
topic of entropy estimation is quite large and covers a wide range of models
with diﬀerent stochastic structures. The two most commonly cited review
articles are by Beirlant, Dudewicz, Györﬁ, and Meulen (2001) and Paninski
(2003). While Beirlant et al. (2001) include results under a broader range of
models, Paninski (2003) mainly focuses on the issues of entropy estimation
on a countable alphabet, the most basic and therefore the most fundamental
model for entropy estimation.
Entropy estimation is generally considered a diﬃcult problem regardless
of the domain type for the underlying distribution. The cause of diﬃculty is
twofold. First, Shannon’s entropy is a measure sensitive to small perturbations
on the probabilities in the tail, as evidenced by
d(−p ln p)
dp
= −ln p −1,
which assumes a large positive value for small p. This sensitivity could amplify
ordinary sampling errors at lower probability region of the sampling space into
signiﬁcant variability in estimation. Second, a sample of size n, however large
it may be, could often have a hopelessly poor coverage of the alphabet. If the
cardinality of 𝒳is inﬁnite, then that of the subset not covered by a sample
is always inﬁnite. Even for a ﬁnite alphabet, the cardinality of the subset not
Statistical Implications of Turing’s Formula, First Edition. Zhiyi Zhang.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

72
Statistical Implications of Turing’s Formula
covered by a sample is often much larger than that of the subset covered by the
sample. In light of this fact, it is clear that a good entropy estimator must ﬁnd
a way to utilize certain characteristics of the probability distribution over the
not-covered subset of the alphabet.
Two fundamental questions are immediately raised. The ﬁrst question is
whether such information exists in a ﬁnite sample; and the second question is,
if it does, whether it can be extracted nonparametrically. After an overview
on the mainlines of research in entropy estimation during the past decades, a
nonparametric entropy estimator in Turing’s perspective is constructed; some
of its statistical properties are established; and, in turn, the nonparametric
entropy estimator in Turing’s perspective will give an aﬃrmative answer to
each of the above-mentioned two questions.
3.1
A Brief Overview
Let {Yk} = {Yk; k ≥1} be the sequence of observed counts of letters of the
alphabet in an iid sample of size n and let {̂pk = Yk∕n}. The general nonpara-
metric estimator that plays a central role in the literature is commonly known
as the plug-in estimator and is given by
̂H = −
∑
k≥1
̂pk ln ̂pk.
(3.2)
The plug-in estimator is appealing to practitioners since it is simple, intu-
itive, easily calculated, and asymptotically eﬃcient in some cases (when it
is a maximum likelihood estimator). However, the plug-in estimator has a
well-established issue in large and complex bias. For example, according to
Harris (1975), when 𝒳is of ﬁnite cardinality K < ∞, the bias of ̂H up to the
second order is of the form
E ( ̂H −H) = −K −1
2n
+
1
12n2
(
1 −
K
∑
k=1
1
pk
)
+ (n−3) .
(3.3)
The two immediate features of the ﬁrst-order term in (3.3) are as follows:
1) ̂H tends to underestimate H since the term is negative; and
2) in case of a large K relative to the sample size n, the magnitude of the bias
could be substantial.
There are plenty of situations in application where a relatively large K meets
a relatively small n. For example, there are more than 50000 (K > 50000) com-
mon English word types in current usage and a typical sample text, for example,
a web page, often contains less than 300 words (n < 300). For another example,
there are approximately 35000 gene types (K ≈35000) in the human genome
and genetic studies often have sample sizes in hundreds (100 < n < 500).

Estimation of Shannon’s Entropy
73
In fact, the complexity of the bias in (3.3) is beyond the ﬁrst-order term. Just
for a moment, suppose the value of K is ﬁnite and known a priori. Then one
could adjust ̂H by letting
̂H+ = ̂H + K −1
2n
to obtain an estimator with a bias decaying at the rate of (n−2). Unfortunately
such an adjustment does not solve the general problem because the next leading
term is
1
12n2
(
1 −
K
∑
k=1
1
pk
)
which, given n, could assume a hugely negative value if just for one index k,
0 < pk ≪1. Similar observations can be made into higher order terms of (3.3)
for K < ∞. When K is countably inﬁnite, much less is known about the bias of
the plug-in estimator in (3.2).
Historically much of the research on entropy estimation has been inspired by
the practical need to reduce the bias of the plug-in estimator. A large number
of estimators have been proposed. Many of these proposed estimators may be
roughly classiﬁed into three categories: (i) correcting the bias of ̂H by estimating
K, (ii) jackknife, and (iii) Bayesian.
By estimating K and assuming a satisfactory estimator ̂K can be found, the
plug-in estimator may be naturally adjusted by means of an additive term
according to (3.3), that is,
̂H∗= ̂H +
̂K −1
2n .
However estimating K is a diﬃcult statistical problem on its own. There are no
unbiased nonparametric estimators of K under general models; therefore, the
ﬁrst-order term in (3.3) cannot be entirely eliminated by such a simple adjust-
ment. In addition, the very action of estimating K implies the assumption that
the cardinality K of the alphabet 𝒳is ﬁnite. This assumption puts a concep-
tual limitation on the models for which such estimator can be justiﬁably used.
Nevertheless, the general concept of such approaches is practically sound. For
an introduction to the statistical problem of estimating K, interested readers
may wish to refer to Chao (1987) and Bunge, Willis, and Walsh (2014).
A popular and frequently cited entropy estimator in this category is the one
proposed by Miller (1955), also known as the Miller–Madow estimator, which
uses
̂m =
∑
k≥1
1[Yk > 0],
the number of covered letters in the sample, in place of ̂K, that is,
̂HMM = ̂H + ̂m −1
2n
.
(3.4)

74
Statistical Implications of Turing’s Formula
The jackknife methodology is another commonly used technique to reduce
bias of the plug-in estimator. The basic elements of the jackknife estimator are
1) to construct n plug-in estimators based on a subsample of size n −1 by
leaving one (the ith) observation out, ̂H(i), which has, based on (3.3), a bias
with a leading ﬁrst-order term
−K −1
2(n −1);
2) to obtain ̂H(i) = n ̂H −(n −1) ̂H(i) for i = 1,…, n; and then
3) to deﬁne the jackknife estimator by
̂HJK =
∑n
i=1 ̂H(i)
n
= ̂H + n −1
n
n
∑
i=1
( ̂H −̂H(i)) .
(3.5)
The jackknife methodology was ﬁrst applied to the plug-in estimator by
Zahl (1977). It may be veriﬁed that the bias of ̂HJK is
E ( ̂HJK −H) = −
1
12n(n −1)
(
1 −
K
∑
k=1
1
pk
)
+ (n−3),
(3.6)
speciﬁcally noting that the leading term is positive and decays at the rate of
(n−2) (see Exercise 4). Another noteworthy methodology in this area is that
by Strong, Koberle, de Ruyter van Steveninck, and Bialek (1998), where an inter-
esting extension of the jackknife procedure can be found to correct bias simul-
taneously for several terms of diﬀerent orders in (3.3).
Another important group of entropy estimators is based on Bayesian
perspectives. Consider the underlying probability distribution {p1,…, pK} as
a point in the K-dimensional unit cube satisfying the following constraints:
1) pk ∈(0, 1) for i = 1,…, K, and
2) ∑K
k=1 pk = 1.
A prior distribution is imposed on {p1,…, pK} in the form of a Dirichlet den-
sity function
f (p1,…, pK; 𝛼1,…, 𝛼K) =
1
B(𝛼)
K
∏
k=1
p𝛼k−1
k
(3.7)
where 𝛼= (𝛼1,…, 𝛼K)𝜏is such that 𝛼k > 0 for each k, k = 1,…, K;
B(𝛼) =
∏K
k=1 Γ(𝛼k)
Γ
(∑K
k=1 𝛼k
) ;

Estimation of Shannon’s Entropy
75
and Γ(⋅) is the gamma function. Under the prior distribution in (3.7), the
posterior is also a Dirichlet distribution in the form of
f (p1,…, pK|y1,…, yK) =
1
B(y + 𝛼)
K
∏
k=1
p(yk+𝛼k)−1
k
(3.8)
where y = (y1,…, yK)𝜏is a realization of the observed letter frequencies in the
sample. Under (3.8), the posterior mean for each pk is
̂p∗
k =
yk + 𝛼k
n + ∑K
k=1 𝛼k
,
and, in turn, the Bayes estimator of entropy is given by the plug-in form
̂HBayes = −
K
∑
k=1
̂p∗
k ln ̂p∗
k.
(3.9)
For various sets of preﬁxed {𝛼1,…, 𝛼K}, this group of entropy estimators
includes those proposed by Krichevsky and Troﬁmov (1981), Holste, Große,
and Herzel (1998), and Schürmann and Grassberger (1996), among many
others.
The Dirichlet prior, while a natural conjugate to {pk; k = 1,…, K}, is by no
means the only reasonable family of priors to be used. It may be modiﬁed or
extended in many ways. For example, Nemenman, Shafee, and Bialek (2002)
adopted a hierarchical Bayesian perspective by
1) reducing (3.7) to a one-parameter Dirichlet model by setting 𝛽= 𝛼k for all
k, and
2) imposing a hyper-prior on the single hyper-parameter 𝛽, designed to reduce
the impact of observed data y when n is small.
The resulting estimator is known as the NSB estimator.
There is a long list of other notable entropy estimators that do not neces-
sarily fall into these categories. Examples are Hausser and Strimmer (2009)
for James-Stein type of shrinkage estimators; Chao and Shen (2003) and Vu,
Yu, and Kass (2007) for coverage-adjusted estimators; and Valiant and Valiant
(2011) and Zhang and Grabchak (2013) for general bias reduction. There is also
an interesting unpublished manuscript by Montgomery-Smith and Schürmann
(2007) that established an unbiased entropy estimator under a sequential sam-
pling scheme. In summary, there is no shortage of proposed entropy estimators
in the literature and many more are certainly to come.
There are however several fundamental issues that are not addressed suﬃ-
ciently and satisfactorily by the existing research on entropy estimation.
1) Convergence rates of most of the proposed estimators are largely unknown.
The majority of the research thus far focuses on bias reduction, and the
primary methodology of evaluating the performance is by simulations.
While simulations are important tools, they cannot replace theoretical

76
Statistical Implications of Turing’s Formula
assessment of statistical procedures. The lack of understanding of various
convergence rates for most of the proposed estimators, for example,
in distribution, in bias, in variance, in mean squared error, etc., leaves
users vulnerable to potentially poor performances, with n of all sizes. As
certain as the fact that most of the proposed entropy estimators have their
respective merits, the best of them could be easily proven inadequate under
some underlying distributions {pk}.
2) Distributional characteristics of most of the proposed estimators are largely
unknown. It may serve as a reminder to practitioners everywhere that only
when the sampling distribution of an estimator is known (or can be approx-
imated in some sense) can one reasonably believe that the behavior of that
estimator is statistically understood.
3) What happens if the cardinality of the alphabet 𝒳is countably inﬁnite, that
is, K = ∑
k≥11[pk > 0] = ∞? The inspiration of bias reduction comes mainly
from the bias expression of (3.3). Consequently the probabilistic platforms
for developing bias-reduced estimators implicatively assume a ﬁnite K. In
fact, all of the estimators in the Bayesian category with a Dirichlet prior
require a known K for the argument to make sense. If K = ∞, it is not clear
how the bias can be expressed and how it behaves under various distribu-
tional classes.
These statistical issues are at the very center of entropy estimation and
hence of information theory. Future research eﬀorts must move toward
resolving them.
3.2
The Plug-In Entropy Estimator
The plug-in entropy estimator of (3.2) is a natural reference estimator for all
others. Despite its notoriety associated with a large bias when n is not suﬃ-
ciently large, the plug-in estimator has quite a lot to oﬀer. Firstly, when K is ﬁnite
and known, it is a maximum likelihood estimator. As such, it is asymptotically
eﬃcient, that is essentially saying that when the sample is suﬃciently large, no
other estimator could perform much better. Secondly, it oﬀers a relatively less
convoluted mathematical form and therefore makes the establishing of its sta-
tistical properties relatively easy. Thirdly, its various statistical properties would
shed much light on those of other estimators. It is fair to say that understanding
the statistical properties of the plug-in is a ﬁrst step toward understanding the
statistical properties of many other estimators.
3.2.1
When K Is Finite
Several statistical properties of the plug-in estimator of (3.2) are summarized
in this section.

Estimation of Shannon’s Entropy
77
Theorem 3.1
Let H be as in (3.1) and let ̂H be as in (3.2) based on an iid
sample of size n. Suppose K < ∞. Then
1) E( ̂H −H) = −K−1
2n +
1
12n2
(
1 −∑K
k=1
1
pk
)
+ (n−3);
2) Var( ̂H) = 1
n
(∑K
k=1 pkln2pk −H2)
+ K−1
2n2 + (n−3); and
3) E( ̂H −H)2 = 1
n
(∑K
k=1 pkln2pk −H2)
+ K2−1
4n2 + (n−3).
The proof of Theorem 3.1 is found in Harris (1975). Theorems 3.2 and 3.3 are
due to Miller and Madow (1954).
Theorem 3.2
Let H be as in (3.1) and let ̂H be as in (3.2) based on an iid
sample of size n. Suppose K < ∞and {pk} is a nonuniform distribution. Then
√
n( ̂H −H)
L
−−−→N(0, 𝜎2)
(3.10)
where
𝜎2 =
K
∑
k=1
pk (ln pk + H)2.
(3.11)
Proof: Let ̂p−= (̂p1,…, ̂pK−1)
′and p−= (p1,…, pK−1)
′. Noting
√
n(̂p−−p−)
L
−−−→MVN(∅, Σ)
where ∅is a (K −1)-dimensional vector of zeros, Σ = (𝜎i,j) is a (K −1) × (K −1)
covariance matrix with diagonal and oﬀ-diagonal elements being, respectively,
𝜎i,i = pi(1 −pi),
𝜎i,j = −pipj
for i = 1,…, K −1, j = 1,…, K −1, and i ≠j. The result of the theorem follows
an application of the delta method with function H = g(p−). (The details of the
proof are left as an exercise; see Exercise 7.)
◽
The condition of {pk} being a nonuniform distribution is required by
Theorem 3.2 because otherwise 𝜎2 in (3.11) would be zero (see Exercise 6) and
consequently the asymptotic distribution degenerates.
Theorem 3.3
Let H be as in (3.1) and let ̂H be as in (3.2) based on an iid
sample of size n. Suppose K < ∞and pk = 1∕K for each k, k = 1,…, K. Then
2n(H −̂H)
L
−−−→𝜒2
K−1
(3.12)
where 𝜒2
K−1 is a chi-squared random variable with degrees of freedom K −1.

78
Statistical Implications of Turing’s Formula
Proof: Noting the proof of Theorem 3.2, the result of the theorem follows an
application of the second-order delta method with function H = g(p−). (The
details of the proof are left as an exercise; see Exercise 8.)
◽
Theorem 3.2 is important not only in its own right but also in passing asymp-
totic normality to other estimators beyond the plug-in. Many bias-adjusted
estimators in the literature have a general form of
̂Hadj = ̂H + ̂B.
(3.13)
Corollary 3.1
Let ̂Hadj be as in (3.13). If
√
n̂B
p
−−−→0, then
√
n ( ̂Hadj −H)
√∑K
k=1 pkln2pk −H2
L
−−−→N(0, 1).
(3.14)
Proof: Noting ﬁrst
√
n ( ̂Hadj −H) =
√
n( ̂H −H) +
√
n(̂B)
L
−−−→N(0, 𝜎2)
and second
𝜎2 =
K
∑
k=1
pk(ln pk + H)2
=
K
∑
k=1
pk(ln2pk + 2 ln pkH + H2)
=
K
∑
k=1
pkln2pk −H2,
the desired result follows Slutsky’s lemma.
◽
Example 3.1
Consider the Miller–Madow estimator
̂HMM = ̂H + ̂m −1
2n
where the adjusting term of (3.13) is ̂B = ( ̂m −1)∕(2n) ≥0. Noting, as n →∞,
0 ≤
√
n̂B =
√
n
(
̂m −1
2n
)
≤K −1
2
√
n
→0,
̂B
a.s.
−−−→0, ̂B
p
−−−→0, and therefore the Miller–Madow estimator admits the
asymptotic normality of Corollary 3.1.

Estimation of Shannon’s Entropy
79
In order to use (3.14) of Corollary 3.1 for inference about entropy, one must
have not only a consistent estimator of entropy but also a consistent estimator
of
H2 =
K
∑
k=1
pkln2pk.
(3.15)
As will be seen in subsequent text, H2 appears in the asymptotic variance of
many entropy estimators and therefore is a key element of statistical inference
about H. It is suﬃciently important to deserve a deﬁned name.
Deﬁnition 3.1
For a probability distribution {pk; k ≥1} on alphabet 𝒳and
a positive integer r, the rth entropic moment is deﬁned to be
Hr = (−1)r ∑
k≥1
pklnrpk.
(3.16)
To give an intuitive justiﬁcation for the names of entropic moments of Deﬁ-
nition 3.1, one may consider a random variable H deﬁned by a given {pk}, with
no multiplicities among the probabilities, in the following tabulated form:
H
−ln p1
−ln p2
· · ·
−ln pk
· · ·
P(H)
p1
p2
· · ·
pk
· · ·
The ﬁrst moment of H gives entropy H = E(H) = −∑
k≥1pk ln pk and the
second moment of H gives H2 = E(H2) = ∑
k≥1pkln2pk, etc.
Remark 3.1
A cautionary note must be taken with the random variable H. If
there is no multiplicity among the probabilities in {pk}, then
Var(H) =
∑
k≥1
pkln2pk −H2.
However, if there exists at least one pair of (i, j), i ≠j, such that pi = pj, then H
is not a well-deﬁned random variable, and neither therefore are the notions of
E(H) and Var(H). This point is often overlooked in the existing literature.
To estimate H2 is however not easy. Its plug-in form of
̂H2 =
∑
k≥1
̂pkln2 ̂pk
suﬀers all the same drawbacks of ̂H in estimating H. In fact, the drawbacks are
much worse, exacerbated by the heavier weights of wk = ln2pk in comparison
to wk = −ln pk in the weighted average of the probabilities, that is, ∑
k≥1wkpk.
In a relatively small sample, the smaller sample coverage causes tendency in ̂H2

80
Statistical Implications of Turing’s Formula
to underestimate H2, to underestimate the asymptotic variance, and in turn to
inﬂate the rate of Type I error (false positive) in hypothesis testing.
In summary, sound inferential procedures about entropy must be supported
by good estimators of not only entropy H but also the second entropic moment
H2.
The eﬀective cardinality K = ∑
k≥11[pk > 0] of {pk; k = 1,…, K} may be
extended to K(n) of {pn,k; k = 1,…, K(n)}, where K(n) changes dynamically
as n increases. In this case, the corresponding entropy is more appropriately
denoted by Hn as a quantity varying with n. Some of the statistical properties
established above for ﬁxed K may also be extended accordingly. The following
theorem is due to Zubkov (1973).
Theorem 3.4
Let {pn,k; k = 1,…, K(n)} be a probability distribution on 𝒳
and let Hn be its entropy. Suppose there exist two ﬁxed constants ε and 𝛿, satis-
fying ε > 𝛿> 0, such that, as n →∞,
1)
n1−ε
K(n)
(K(n)
∑
k=1
pn,kln2pn,k −H2
n
)
→∞,
2)
max
1≤k≤K(n)
{
1
npn,k
}
= 
(K(n)
n1−𝛿
)
,
and
3)
K(n)
√
√
√
√n
(
K(n)
∑
k=1
pn,kln2pn,k −H2
n
) →0.
Then
√
n ( ̂H −Hn
)
√∑K(n)
k=1 pn,k ln2pn,k −H2
n
L
−−−→N(0, 1).
(3.17)
Paninski (2003) established the same asymptotic normality under a slightly
weaker suﬃcient condition as stated in the theorem below.
Theorem 3.5
Let {pn,k; k = 1,…, K(n)} be a probability distribution on 𝒳
and let Hn be its entropy. Suppose there exists a ﬁxed constant ε > 0, such that,
as n →∞,

Estimation of Shannon’s Entropy
81
1)
K(n)
√
n
→0,
and
2)
lim inf
n→∞n1−ε
(K(n)
∑
k=1
pn,kln2pn,k −H2
n
)
> 0.
Then
√
n( ̂H −Hn)
√∑K(n)
k=1 pn,kln2pn,k −H2
n
L
−−−→N(0, 1).
(3.18)
The proofs of both Theorems 3.4 and 3.5 are not presented here. Interested
readers may refer to Zubkov (1973) and Paninski (2003), respectively, for
details.
3.2.2
When K Is Countably Inﬁnite
When there are inﬁnitely many positive probabilities in {pk; k ≥1}, the ﬁrst
issue encountered is whether entropy is ﬁnite. The convergence of
−
m
∑
k=1
pk ln pk
depends on the rate of decay of pk as k increases. For H to be ﬁnite, −pk ln pk
must decay suﬃciently fast or equivalently pk must decay suﬃciently fast.
Example 3.2
Let, for k = 1, 2,… ,
pk = C𝜆
k𝜆,
where 𝜆> 1 and C𝜆> 0 are two ﬁxed real numbers. In this case, H < ∞.
The details of the argument for Example 3.2 are left as an exercise (see
Exercise 9).
Example 3.3
Let, for k = 1, 2,… ,
pk =
1
C(k + 1)ln2(k + 1)

82
Statistical Implications of Turing’s Formula
where the constant C > 0 is such that ∑
k≥1pk = 1:
H = −
∑
k≥1
pk ln pk
=
∑
k≥1
(−ln pk)pk
=
∑
k≥1
ln(C(k + 1) ln2(k + 1))
C(k + 1) ln2(k + 1)
=
∑
k≥1
ln C + ln(k + 1) + 2 ln ln(k + 1)
C(k + 1) ln2(k + 1)
= ln C +
∑
k≥1
1
C(k + 1) ln(k + 1)
+
∑
k≥1
2 ln ln(k + 1)
C(k + 1) ln2(k + 1)
=
(
ln C + ln ln 2
Cln22
)
+
∑
k≥1
1
C(k + 1) ln(k + 1)
+
∑
k≥2
2 ln ln(k + 1)
C(k + 1) ln2(k + 1)
.
Noting that
ln C + ln ln 2
Cln22
is a ﬁxed number,
∑
k≥2
2 ln ln(k + 1)
C(k + 1) ln2(k + 1)
is a sum of positive terms, and by the integral test for series convergence,
lim
m→∞
m
∑
k=1
1
(k + 1) ln(k + 1) = ∞.
The result H = ∞follows.
However, for the purpose of statistical estimation of entropy, only the distri-
butions with ﬁnite entropy are considered. In subsequent text, unless otherwise
speciﬁed, the general discussion is within the class of all {pk; k ≥1} with ﬁnite
entropy, that is,
𝒫=
{
{pk; k ≥1} ∶H = −
∑
k≥1
pk ln pk < ∞
}
.

Estimation of Shannon’s Entropy
83
Several important statistical properties of ̂H are established in Antos and
Kontoyiannis (2001) and are summarized in the following theorem without
proofs.
Theorem 3.6
For any {pk} ∈𝒫, as n →∞,
1) E( ̂H −H)2 →0;
2) for all n, E( ̂H) ≤H; and
3) for all n,
Var( ̂H) ≤ln2n
nln22
.
(3.19)
Part 1 of Theorem 3.6 implies comforting assurance that ̂H converges to H in
mean square error (MSE), which in turn implies |E( ̂H −H)| →0, but does not
say anything about its convergence rate. Part 3 gives an upper bound of Var( ̂H),
which decays at the rate of (ln2n∕n). Under the general class 𝒫, there does not
seem to be an established upper bound for E( ̂H −H)2 in the existing literature.
Given the upper bound for Var( ̂H) in Part 3, it is easy to see that the lack of
upper bound for E( ̂H −H)2 is due to that for the bias, |E( ̂H −H)|.
Antos and Kontoyiannis (2001) also provided another interesting result as
stated in Theorem 3.7.
Theorem 3.7
Let ̂H∗be any entropy estimator based on an iid sample of size
n. Let an > 0 be any sequence of positive numbers converging to zero. Then there
exists a distribution {pk} ∈𝒫such that
lim sup
n→∞
E(| ̂H∗−H|)
an
= ∞.
Theorem 3.7 may be stated in a more intuitive way: given a positive sequence
0 < an →0 with a decaying rate however slow, there exists a distribution {pk} ∈
𝒫such that E(| ̂H∗−H|) decays with a slower rate. This theorem is sometimes
known as the slow convergence theorem of entropy estimation. It may be beneﬁ-
cial to call attention to two particular points of the theorem. First, the estimator
̂H∗is referring to not only the plug-in estimator but any arbitrary estimator. In
this sense, the so-called slow convergence is not a consequence of the estima-
tor ̂H∗but of the estimand H. Second, the criterion of convergence used is the
mean absolute error (MAE) E(| ̂H∗−H|), and not the bias |E( ̂H∗−H)|. There-
fore, the nonexistence of a uniformly decaying upper bound for E(| ̂H∗−H|) on
𝒫does not necessarily imply the same for the bias |E( ̂H∗−H)|. Of course this
by no means suggests that such an upper bound would exist for some ̂H∗.
Naturally, convergence rates could be better described in subclasses of 𝒫.
Antos and Kontoyiannis (2001) studied distributions of the types pk = Ck−𝜆

84
Statistical Implications of Turing’s Formula
where 𝜆> 1 and pk = C(kln𝜆k)−1 where 𝜆> 2 and gave their respective rates
of convergence.
Zhang and Zhang (2012) established a normal law for the plug-in estimator
under a distribution {pk} on a countably inﬁnite alphabet. The result is stated
in Theorem 3.8.
Theorem 3.8
Let {pk; k ≥1} ∈𝒫be a probability distribution satisfying
𝜎2 = ∑
k≥1pkln2pk −H2 < ∞. If there exists an integer-valued function K(n)
such that as, n →∞,
1) K(n) →∞,
2) K(n) = o(
√
n), and
3)
√
n∑
k≥K(n)pk ln pk →0,
then √
n( ̂H −H)
𝜎
L
−−−→N(0, 1).
Let
̂𝜎2 =
∑
k≥1
̂pkln2 ̂pk −̂H2 < ∞
(3.20)
or any other consistent estimator of 𝜎2 = ∑
k≥1pkln2pk −H2 < ∞. The follow-
ing corollary, also due to Zhang and Zhang (2012), provides a means of large
sample inference on H.
Corollary 3.2
Under the same condition of Theorem 3.8,
√
n( ̂H −H)
̂𝜎
L
−−−→N(0, 1).
Example 3.4
Let pk = C𝜆k−𝜆. The suﬃcient condition of Theorem 3.8 holds for
𝜆> 2 but not for 𝜆∈(1, 2).
To support the claim of Example 3.4, consider
√
n
∑
k≥K(n)
(−pk ln pk)
∼−
√
n ∫
∞
K(n)
C𝜆
x𝜆ln
(C𝜆
x𝜆
)
dx
=
C𝜆𝜆
(𝜆−1)
√
n ln K(n)
(K(n))𝜆−1 +
[
C𝜆𝜆
(𝜆−1)2 −C𝜆ln C𝜆
𝜆−1
]
√
n
(K(n))𝜆−1
(see Exercise 10)
∼
C𝜆𝜆
(𝜆−1)
√
n ln K(n)
(K(n))𝜆−1 ,

Estimation of Shannon’s Entropy
85
where “∼” indicates equality in convergence or divergence rate of two
sequences.
If 𝜆> 2, letting K(n) ∼n1∕𝜆,
C𝜆𝜆
(𝜆−1)
√
n ln K(n)
(K(n))𝜆−1
∼
C𝜆
(𝜆−1)
ln n
n1∕2−1∕𝜆→0.
If 1 < 𝜆≤2, for any K(n) ∼o(
√
n) and a suﬃciently large n,
C𝜆𝜆
(𝜆−1)
√
n ln K(n)
(K(n))𝜆−1
>
C𝜆𝜆
(𝜆−1)
√
n
n1∕2−1∕𝜆=
C𝜆𝜆
(𝜆−1)n1∕𝜆→∞.
Example 3.5
Let pk = C𝜆e−𝜆k. The suﬃcient condition of Theorem 3.8 holds.
This is so because, letting K(n) ∼𝜆−1 ln n, for a suﬃciently large n,
√
n
∑
k≥K(n)
(−pk ln pk)
∼−
√
n ∫
∞
ln n1∕𝜆C𝜆e−𝜆x ln (C𝜆e−𝜆x) dx
∼C𝜆
𝜆
ln n
√
n
→0.
Example 3.6
Let pk = C(k2ln2k)−1. The suﬃcient condition of Theorem 3.8
holds.
Let K(n) ∼
√
n∕ln ln n. For a suﬃciently large n, consider
√
n
∑
k≥K(n)
(−pk ln pk)
∼
√
nC ∫
∞
K(n)
2 ln x + 2 ln ln x −ln C
x2ln2x
dx
∼2
√
nC ∫
∞
K(n)
1
x2 ln xdx
≤
2C
√
n
K(n) ln K(n) →0.
Example 3.7
Let pk = C(k2 ln k)−1. The suﬃcient condition of Theorem 3.8
does not hold.

86
Statistical Implications of Turing’s Formula
For any K(n) ∼o(
√
n) and a suﬃciently large n,
√
n
∑
k≥K(n)
(−pk ln pk)
∼
√
nC ∫
∞
K(n)
2 ln x + 2 ln ln x −ln C
x2ln2x
dx
>
√
nC ∫
∞
K(n)
2
x2 dx
= 2C
√
n
K(n) →∞.
3.3
Entropy Estimator in Turing’s Perspective
An entropy estimator in Turing’s perspective is introduced in this section. The
centerpiece of that estimator is an alternative representation of entropy, which
is diﬀerent from that of Shannon’s. Consider any distribution {pk; k ≥1} ∈𝒫
and its entropy:
H = −
∑
k≥1
pk ln pk,
(3.21)
H =
∑
k≥1
pk
∞
∑
v=1
1
v(1 −pk)v,
(3.22)
H =
∞
∑
v=1
1
v
∑
k≥1
pk(1 −pk)v.
(3.23)
The ﬁrst equation in (3.21) is Shannon’s deﬁnition of entropy. The second
equation in (3.22) is due to an application of Taylor’s expansion of −ln pk
for each and every k at p0 = 1. The third equation in (3.23) is due to Fubini’s
theorem , which allows the interchange of the summations. The two conditions
required by Fubini’s theorem are satisﬁed here because
a) the summation H, as in Shannon’s entropy, is ﬁnite by assumption, and
b) the summands are all positive.
Entropy in the form of (3.23) is a representation in Turing’s perspective since
it is a linear combination of elements in {𝜁v; v ≥1} where
𝜁v =
∑
k≥1
pk(1 −pk)v
for every nonnegative integer v as described in Chapter 2. Therefore, entropy
in Turing’s perspective is
H =
∞
∑
v=1
1
v 𝜁v,
(3.24)
a weighted linear form of {𝜁v} .

Estimation of Shannon’s Entropy
87
An interesting heuristic comparison can be made between (3.21) and (3.24).
The representation of Shannon’s entropy series may be viewed as a stack of
positive pieces −pk ln pk over k; and the representation of (3.24) may also be
viewed as a stack of positive pieces 𝜁v∕v over v.
Consider the following two side-by-side comparisons.
1) Parts versus Whole.
a) For each k, the summand −pk ln pk pertains to only the probability of one
letter 𝓁k of 𝒳, and says nothing about other probabilities or the whole
distribution.
b) For each v, the summand 𝜁v∕v = ∑
k≥1pk(1 −pk)v∕v is a characteristic of
the whole underlying distribution {pk}.
2) Estimability.
a) For each k, there is no unbiased estimator of −pk ln pk.
b) For each v, the estimator Z1,v of (2.4) is an uniformly minimum variance
unbiased estimator (umvue) of 𝜁v, provided v ≤n −1, and hence Z1,v∕v
is umvue of 𝜁v∕v.
The estimability of 𝜁v up to v = n −1 implies that the ﬁrst part of (3.25) is
estimable.
H =
n−1
∑
v=1
1
v 𝜁v +
∞
∑
v=n
1
v 𝜁v.
(3.25)
That is, letting
Z1,v = n1+v[n −(1 + v)]!
n!
∑
k≥1
[
̂pk
v−1
∏
j=0
(
1 −̂pk −j
n
) ]
,
(3.26)
̂Hz =
n−1
∑
v=1
1
v Z1,v
(3.27)
is an unbiased estimator of ∑n−1
v=1
1
v 𝜁1,v. As an estimator of H, the second addi-
tive part in (3.25) is the bias of (3.27),
|Bn| = H −E( ̂Hz) =
∞
∑
v=n
1
v 𝜁v.
(3.28)
The explicit expression of the bias of ̂Hz under general distributions enables
easier computation and evaluation and is one of many advantages brought
about by Turing’s perspective.
Remark 3.2
To express entropy as a series via Taylor’s expansion for bias
reduction of entropy estimation was perhaps ﬁrst discussed in the literature by
Blyth (1959). There an application of Taylor’s expansion was used for pk ln pk at
p0 = 0.5 under the assumption that K < ∞, an estimator was proposed, and its

88
Statistical Implications of Turing’s Formula
reduced bias was demonstrated. However, the center of the expansion p0 = 0.5
was somehow arbitrary. The argument provided by Blyth (1959) would hold for
any reasonable value of p0 ∈(0, 1). For some reason, that line of research was
not rigorously pursued in the history. On the contrary, the expansion used above
is for −ln pk and is at p0 = 1. There are two important consequences of such an
expansion. First, the polynomials in the expansion has a particular form, namely
pk(1 −pk)v, which transforms the entire problem to a local binary platform.
The binary platform allows statistical inferences to be made based on whether
the letter 𝓁k is either observed or not observed in an iid sample. In fact, this
is the very reason the perspective is named in honor of Alan Turing although it
is not known whether Alan Turing actually thought about the problem of entropy
estimation in his living years. Second, the coeﬃcient of the expansion in Turing’s
perspective is of a simple form, namely 1∕v, which makes the derivation of math-
ematical properties of the corresponding estimator ̂Hz much easier. The said ease
is perhaps one of the main reasons why many good properties of the perspective
are established, as will be seen throughout the subsequent text.
3.3.1
When K Is Finite
When the eﬀective cardinality of 𝒳is ﬁnite, that is, K < ∞, an immediate ben-
eﬁt of Turing’s perspective is stated in the following theorem.
Theorem 3.9
If K = ∑K
k=1 1[pk > 0] < ∞, then
|Bn| = H −E( ̂Hz) ≤(1 −p∧)n
np∧
where p∧= min{pk > 0; k = 1,…, K}.
Proof: By (3.28),
|Bn| =
∞
∑
v=n
1
v
K
∑
k=1
pk(1 −pk)v
≤1
n
∞
∑
v=n
K
∑
k=1
pk(1 −p∧)v
= 1
n
∞
∑
v=n
(1 −p∧)v
= (1 −p∧)n
np∧
.
◽
Three important points are to be noted here. First, the exponential decay of
the bias of ̂Hz is a qualitative improvement from the plug-in estimator, which

Estimation of Shannon’s Entropy
89
has a (n−1) decaying rate in n. Second, the upper bound depends on the distri-
bution {pk} via p∧, and therefore it is not a universal upper bound even within
the class of distributions on a ﬁnite alphabet since p∧> 0 could be arbitrarily
small. Third, when n is small and p∧> 0 is very small, the bias could still be
sizable.
Several asymptotic properties of ̂Hz, when K < ∞, were established in
Zhang (2013b). Let H2 be as in Deﬁnition 3.1.
Theorem 3.10
Let {pk; 1 ≤k ≤K} be a nonuniform probability distribution
on a ﬁnite alphabet 𝒳. Then
√
n ( ̂Hz −H)
𝜎
L
−−−→N(0, 1)
where 𝜎2 = H2 −H2.
Toward proving Theorem 3.10, let
w(v + 1, n) = nv+1[n −(v + 1)]!
n!
(3.29)
and consider the following re-expression of ̂Hz,
̂Hz =
K
∑
k=1
{
̂pk
{n−1
∑
v=1
1
vw(v + 1, n)
[ v−1
∏
j=0
(
1 −̂pk −j
n
) ] } }
=∶
K
∑
k=1
{̂pk {̂gk,n} } .
(3.30)
Of ﬁrst interest is an asymptotic normal law of ̂pk ̂gk,n for each k. For simplic-
ity, consider ﬁrst a binomial distribution with parameters n and p ∈(0, 1) and
functions
gn(p) =
n−1
∑
v=1
{
1
vw(v + 1, n)
[ v−1
∏
j=0
(
1 −p −j
n
) ]
1[v ≤n(1 −p) + 1]
}
and hn(p) = pgn(p). Let h(p) = −p ln(p).
The following two lemmas are needed to prove Theorem 3.10. Lemma 3.1 is
easily proved by induction (see Exercise 11). A proof of Lemma 3.2 is given in
Appendix at the end of this chapter.
Lemma 3.1
Let aj, j = 1,…, n, be real numbers satisfying |aj| ≤1 for every j.
Then |||
∏n
j=1 aj −1||| ≤∑n
j=1 |aj −1|.
Lemma 3.2
Let ̂p = X∕n where X is a binomial random variable with param-
eters n and p.
1)
√
n[hn(p) −h(p)] →0 uniformly in p ∈(c, 1) for any c, 0 < c < 1.

90
Statistical Implications of Turing’s Formula
2)
√
n|hn(p) −h(p)| < A(n) = (n3∕2) uniformly in p ∈[1∕n, c] for any c,
0 < c < p.
3) P(̂p ≤c) < B(n) = (n−1∕2e−nC) where C = (p −c)2∕[p(1 −p)] for any
c ∈(0, p).
Proof of Theorem 3.10: Without loss of generality, consider the sample propor-
tions of the ﬁrst two letters of the alphabet ̂p1 and ̂p2 in an iid sample of size n.
√
n (̂p1 −p1, ̂p2 −p2)′
L
−−−→N(0, Σ), where Σ = (𝜎ij), i, j = 1, 2, 𝜎ii = pi(1 −pi),
and 𝜎ij = −pipj when i ≠j. Write
√
n [(hn(̂p1) + hn(̂p2)) −(−p1 ln p1 −p2 ln p2)]
=
√
n [(hn(̂p1) + hn(̂p2)) −(h(̂p1) + h(̂p2))]
+
√
n [(h(̂p1) + h(̂p2)) −(−p1 ln p1 −p2 ln p2)]
=
√
n (hn(̂p1) −h(̂p1)) +
√
n (hn(̂p2) −h(̂p2))
+
√
n [(h(̂p1) + h(̂p2)) −(−p1 ln p1 −p2 ln p2)]
=
√
n [hn(̂p1) −h(̂p1)] 1[̂p1 ≤p1∕2]
+
√
n (hn(̂p2) −h(̂p2)) 1[̂p2 ≤p2∕2]
+
√
n (hn(̂p1) −h(̂p1)) 1[̂p1 > p1∕2]
+
√
n (hn(̂p2) −h(̂p2)) 1[̂p2 > p2∕2]
+
√
n [(h(̂p1) + h(̂p2)) −(−p1 ln p1 −p2 ln p2)] .
The third and fourth terms in the last expression above converge to zero
almost surely by Part 1 of Lemma 3.2. The last term, by the delta method, con-
verges in law to N(0, 𝜏2) where after a few algebraic steps
𝜏2 = (ln p1 + 1)2p1(1 −p1) + (ln p2 + 1)2p2(1 −p2)
−2(ln p1 + 1)(ln p2 + 1)p1p2
= (ln p1 + 1)2p1 + (ln p2 + 1)2p2
−[(ln p1 + 1)p1 + (ln p1 + 1)p1]2.
It remains to show that the ﬁrst term (the second term will admit the same argu-
ment) converges to zero in probability. However, this fact can be established by
the following argument. By Part 2 and then Part 3 of Lemma 3.2,
E (
√
n |hn(̂p1) −h(̂p1)| 1[̂p1 ≤p1∕2]) )
≤A(n)P(̂p1 ≤p1∕2)
≤A(n)B(n)
= (n3∕2)(n−1∕2e−nC) →0

Estimation of Shannon’s Entropy
91
for some positive constant C. This fact, noting that
√
n |hn(̂p1) −h(̂p1)| ≥0,
gives immediately the desired convergence in probability, that is,
√
n |hn(̂p1) −h(̂p1)| 1[̂p1 ≤p1∕2]
p
−−−→0.
In turn, it gives the desired weak convergence for
√
n [(hn(̂p1) + hn(̂p2)) −(−p1 ln p1 −p2 ln p2)] .
The desired result follows a generalization to K terms.
◽
To use Theorem 3.10 for inferences, one must ﬁnd a consistent estimator for
𝜎2 = H2 −H2. Here the plug-in estimators for both H and H2, as well as any
other consistent estimators, in principle could serve the purpose, but could
perform poorly due to their respective heavy biases.
Let
̂H2z =
n−1
∑
v=1
{[v−1
∑
i=1
1
i(v −i)
]
w(v + 1, n)
K
∑
k=1
[
̂pk
v−1
∏
j=0
(
1 −̂pk −j
n
) ]}
.
(3.31)
It may be veriﬁed that (Exercise 12), for any given distribution with ﬁnite K,
|E( ̂H2z −H2)| decays exponentially in n.
Corollary 3.3
Let {pk; 1 ≤k ≤K} be a nonuniform probability distribution
on a ﬁnite alphabet and ̂H2z be as in (3.31). Then
√
n
⎛
⎜
⎜
⎜⎝
̂Hz −H
√
̂H2z −̂H2
z
⎞
⎟
⎟
⎟⎠
L
−−−→N(0, 1).
Toward proving Corollary 3.3, let
Cv =
v−1
∑
i=1
1
i(v −i)
(3.32)
for v ≥2 (and deﬁne C1 = 0) and write
̂H2z =
n−1
∑
v=1
CvZ1,v.
For better clarity in proving Corollary 3.3, a few more notations and two
well-known lemmas in U-statistics are ﬁrst given.
For each i, 1 ≤i ≤n, let Xi be a random element such that Xi = 𝓁k indi-
cates the event that the kth letter of the alphabet is observed and P(Xi = 𝓁k) =
pk. Let X1,…, Xn be an iid sample, and denote x1,…, xn as the corresponding

92
Statistical Implications of Turing’s Formula
sample realization. A U-statistic is an n-variable function obtained by averag-
ing the values of an m-variable function (kernel of degree m, often denoted by
𝜓) over all n!∕[m!(n −m)!] possible subsets of m observations from the set of
n observations. Interested readers may refer to Lee (1990) for an introduction.
In Zhang and Zhou (2010), it is shown that Z1,v is a U-statistic with kernel 𝜓
being Turing’s formula with degree m = v + 1. Let
𝜓c(x1,…, xc) = E(𝜓(x1,…, xc, Xc+1,…, Xm))
and
𝜎2
c = Var(𝜓c(X1,…, Xc)).
Lemmas 3.3 and 3.4 are due to Hoeﬀding (1948).
Lemma 3.3
Let Un be a U-statistic with kernel 𝜓of degree m.
Var(Un) =
( n
m
)−1
m
∑
c=1
(m
c
) (n −m
m −c
)
𝜎2
c .
Lemma 3.4
Let Un be a U-statistic with kernel 𝜓of degree m.
For any two nonnegative integers, c and d, satisfying 0 ≤c ≤d ≤m,
𝜎2
c
c ≤
𝜎2
d
d .
Lemma 3.5
Var(Z1,v) ≤1
n𝜁1,v + v + 1
n
𝜁2
1,v−1.
A proof of Lemma 3.5 is given in the Appendix at the end of this chapter.
Proof of Corollary 3.3: Since E(Z1,v) = 𝜁v,
E( ̂H2z) =
n−1
∑
v=1
Cv
K
∑
k=1
pk(1 −pk)v
=
K
∑
k=1
pk
n−1
∑
v=1
Cv(1 −pk)v
→
K
∑
k=1
pk(−ln pk)2 =
K
∑
k=1
pkln2pk.

Estimation of Shannon’s Entropy
93
It only remains to show Var( ̂H2z) →0.
Var( ̂H2z) =
n−1
∑
v=1
n−1
∑
w=1
CvCwCov(Z1,v, Z1,w)
≤
n−1
∑
v=1
n−1
∑
w=1
CvCw
√
Var(Z1,v)Var(Z1,w)
=
(n−1
∑
v=1
Cv
√
Var(Z1,v)
)2
.
Noting
Cv =
v−1
∑
i=1
1
i(v −i) ≤
v−1
∑
i=1
1
v −1 = 1,
𝜁v ≤𝜁v−1,
𝜁2
v−1 ≤𝜁v−1,
𝜁v−1 =
K
∑
k=1
pk(1 −pk)v−1 ≤
K
∑
k=1
pk(1 −p∧)v−1 = (1 −p∧)v−1,
where p∧= min{pk > 0; k = 1,…, K}, and by Lemma 3.5 for v ≥2,
√
Var(Z1,v) ≤
1
√
n
√
(v + 2)𝜁1,v−1 ≤
√
2v1∕2
√
n
(1 −p∧)(v−1)∕2.
As n →∞,
n−1
∑
v=1
Cv
√
Var(Z1,v) ≤
√
2
√
n
n
∑
v=1
v1∕2 (
√
1 −p∧) v−1
=
√
2
√
n
⌊n1∕4⌋
∑
v=1
v1∕2 (
√
1 −p∧) v−1
+
√
2
√
n
n
∑
v=⌊n1∕4⌋+1
v1∕2 (
√
1 −p∧) v−1
≤
√
2
√
n
n1∕4 (n1∕4) 1∕2
+
√
2
√
n
n1∕2 (√
1 −p∧
)⌊n1∕4⌋
1
1 −
√
1 −p∧
=
√
2n−1∕8 +
√
2
(√
1 −p∧
)⌊n1∕4⌋
1
1 −
√
1 −p∧
→0,

94
Statistical Implications of Turing’s Formula
and Var( ̂H2z) →0 follows. Hence ̂H2z
p→H2. The fact of ̂Hz
p→H is implied by
Theorem 3.10. Finally, the corollary follows Slutsky’s lemma.
◽
Theorem 3.11
Let {pk; 1 ≤k ≤K} be a nonuniform probability distribution
on a ﬁnite alphabet 𝒳. Then ̂Hz is asymptotically eﬃcient.
Proof: First consider the plug-in estimator ̂H. It can be veriﬁed that
√
n( ̂H −
H) →N(0, 𝜎2) where 𝜎2 = 𝜎2({pk}) is as in Theorem 3.10. It is of interest to
show ﬁrst that ̂H is asymptotically eﬃcient in two separate cases: (i) when K is
known and (ii) when K is unknown. If K is known, then the underlying model
{pk; 1 ≤k ≤K} is a (K −1)-parameter multinomial distribution, and therefore
̂H is the maximum likelihood estimator of H, which implies that it is asymp-
totically eﬃcient. Since the estimator ̂H takes the same value, given a sample,
regardless whether K is known or not, its asymptotic variance is the same.
Therefore, ̂H must be asymptotically eﬃcient when K is ﬁnite but unknown,
or else, it would contradict the fact that ̂H is asymptotically eﬃcient when K is
known. The asymptotic eﬃciency of ̂Hz follows from the fact that
√
n( ̂Hz −H)
and
√
n( ̂H −H) have identical limiting distribution.
◽
3.3.2
When K Is Countably Inﬁnite
The following lemma gives a necessary and suﬃcient condition for entropy H
to be ﬁnite.
Lemma 3.6
For a given {pk}, H = −∑
kpk ln(pk) < ∞if and only if there exists
a strictly increasing divergent sequence of positive real numbers {an} such that
∑∞
n=1 (nan)−1 < ∞and, as n →∞,
an
∞
∑
k=1
pk(1 −pk)n →1.
Proof: If an an satisfying the conditions exists, then there also exists a positive
integer n0 such that, for all n > n0, an
∑
kpk(1 −pk)n < 2. Therefore,
∞
∑
v=1
1
v
∑
k
pk(1 −pk)v−1
=
n0
∑
v=1
1
v
∑
k
pk(1 −pk)v−1 +
∞
∑
v=n0+1
1
v
∑
k
pk(1 −pk)v−1
≤
n0
∑
v=1
1
v
∑
k
pk(1 −pk)v−1 + 2
∞
∑
v=n0+1
1
vav
< ∞.

Estimation of Shannon’s Entropy
95
On the other hand, if ∑∞
v=1(1∕v)𝜁v < ∞, then letting an = 1∕𝜁n the following
arguments show that all conditions of Lemma 3.6 are satisﬁed. First, 𝜁n is strictly
decreasing and therefore an is strictly increasing. Second, 𝜁n = ∑
kpk(1 −pk)n ≤
∑
kpk = 1; and by the dominated convergence theorem, 𝜁n →0 and hence
an →∞. Third, ∑∞
v=1 1∕(vav) = ∑∞
v=1(1∕v)𝜁v < ∞and an𝜁n = 1 by assumption
and deﬁnition.
◽
Lemma 3.6 serves two primary purposes. First, it encircles all the discrete
distributions that may be of interest with regard to entropy estimation, that is,
distributions with ﬁnite entropy. Second, it provides a characterization of the
tail of a distribution in terms of a sequence an and its conjugative relationship
to 𝜁n. The rate of divergence of an characterizes the rate of tail decay of the
underlying distribution {pk} as k increases. A faster (slower) rate of divergence
of an signals a thinner (thicker) probability tail.
Let
Mn = 5
n
(n−1
∑
v=1
1
v 𝜁1∕2
v−1
)2
.
(3.33)
The next lemma provides an upper bound for Var( ̂Hz) under the general con-
ditions and plays a central role in establishing many of the subsequent results.
Lemma 3.7
For any probability distribution {pk}, Var( ̂Hz) ≤Mn.
The proof of Lemma 3.7 is based on a U-statistic argument. The details of the
proof are found in Zhang (2012).
Corollary 3.4
For any probability distribution {pk},
Var( ̂Hz) < 5
{[1 + ln(n −1)]2
n
}
= (ln2n∕n).
Proof: Referring to (3.33) and noting 𝜁v ≤1 and that ∑n−1
k=1 1∕v is the harmonic
series hence with upper bound 1 + ln(n −1),
Mn < 5
{[1 + ln(n −1)]2
n
}
= (ln2n∕n).
◽
Corollary 3.5
For any probability distribution {pk} with ﬁnite entropy H,
Var( ̂Hz) < 5(1 + H)
[1 + ln(n −1)
n
]
= (ln n∕n).
A proof of Corollary 3.5 is given in the Appendix of Chapter 3.

96
Statistical Implications of Turing’s Formula
The implications of Corollary 3.5 are quite signiﬁcant. The upper bound
in Corollary 3.5 for every distribution in the entire class of 𝒫decays faster
than the upper bound for Var( ̂H), that is, (ln2n∕n), established in Antos and
Kontoyiannis (2001) for the same class of distributions by a factor of ln n. The
improvement in variance suggests that entropy estimation in view of ̂Hz is
fundamentally more eﬃcient than that of the plug-in, ̂H, in some sense. This
may also be viewed as an advantage brought about by Turing’s perspective.
Nevertheless, the fact that the upper bound is proportional to 1 + H suggests
that, for any ﬁxed sample size n, a distribution with ﬁnite entropy can be found
to have an upper bound arbitrarily large since H can be arbitrarily large.
Lemma 3.7 implies the following general statements about ̂Hz under the con-
dition of ﬁnite entropy. Theorem 3.12 is stated for a convenient comparison to
Theorem 3.6 regarding the plug-in estimator ̂H.
Theorem 3.12
For any {pk} ∈𝒫, as n →∞,
1) E( ̂Hz −H)2 →0;
2) for all n, E( ̂Hz) ≤H; and
3) for all n,
Var( ̂Hz) ≤5(1 + H)
(1 + ln(n −1)
n
)
.
(3.34)
Proof: For Part 1, since E( ̂Hz −H)2 = Var( ̂Hz) + |Bn|2, it suﬃces to show
Var( ̂Hz) →0, which is implied by Corollary 3.5, and |Bn| →0 which is implied
by H being ﬁnite. Part 2 is true simply because the bias ∑∞
v=n 𝜁v∕v > 0. Part 3 is
a restatement of Corollary 3.5.
◽
In diﬀerent subclasses of 𝒫, the convergence rates of ̂Hz could be better
described. Naturally 𝒫may be partitioned into layers characterized by the rate
of divergence of 𝜁n or equivalently by tn = n 𝜁n. Consider the following condi-
tions.
Condition 3.1
For a probability sequence {pk} ∈𝒫, there exists a constant
C > 0 such that n 𝜁n ≤C for all n.
Condition 3.2
For a probability sequence {pk} ∈𝒫,
1) n 𝜁n →∞as n →∞, and
2)
√
n 𝜁n ≤C for some constant C > 0 and all n.
Condition 3.3
For a probability sequence {pk} ∈𝒫,
1)
√
n 𝜁n →∞as n →∞, and
2) n𝛿𝜁n ≤C for a constant 𝛿∈(0, 1∕2), some constant C > 0 and all n.

Estimation of Shannon’s Entropy
97
Conditions 3.1–3.3 are mutually exclusive conditions. Let 𝒫i, i = 1, 2, 3,
be the subclasses of 𝒫
under Conditions 3.1–3.3, respectively. Let
𝒫4 = (𝒫1 ∪𝒫2 ∪𝒫3)c where the complement is with respect to 𝒫.
It may be instructive to make several observations at this point. First, 𝒫i,
1 ≤i ≤4, are deﬁned in an order of the tail decaying rate of the underlying dis-
tribution with 𝒫1 having the fastest decaying rate. Second, it can be veriﬁed that
pk = c𝜆e−𝜆k where 𝜆> 0 satisﬁes n𝜁n < C for some C > 0 and hence Condition
3.1. Condition 3.1 is therefore satisﬁed by all distributions with faster decaying
tails than that of pk = c𝜆e−𝜆k, including the distributions on any ﬁnite alphabet.
Third, it can also be veriﬁed that, for pk = c𝜆k−𝜆where 𝜆> 1, n𝛿𝜁n →C > 0
where 𝛿= 1 −1∕𝜆. It is seen here that pk = c𝜆k−𝜆where 𝜆≥2 belongs to 𝒫2
and that pk = c𝜆k−𝜆where 1 < 𝜆< 2 belongs to 𝒫3. 𝒫4 holds all other very
thick-tailed distributions whose 𝜁n’s converge so slowly that they cannot be
bounded above by a sequence bn →0 at a rate of (n−ε) for any small ε > 0.
Theorem 3.13
For any probability distribution {pk},
1) if {pk} ∈𝒫1 ∪𝒫2, then there exists M1(n) = (n−1) such that E( ̂Hz −H)2 ≤
M1(n); and
2) if {pk} ∈𝒫3, then there exists M2(n) = (n−2𝛿) such that E( ̂Hz −H)2 ≤
M2(n).
Proof: By Lemma 3.7, E( ̂Hz −H)2 ≤Mn + |Bn|2 in general where Mn is as in
(3.33) and Bn = ∑∞
v=n 𝜁v∕v. For Part 1, if Condition 3.1 holds, then
Mn = 5
n
n−1
∑
v=1
1
v 𝜁1∕2
v−1
= 5
n 𝜁1∕2
0
+ 5
n
n−1
∑
v=2
1
v 𝜁1∕2
v−1
≤5
n + 5
√
C
n
n−1
∑
v=2
1
v(v −1)1∕2 =∶V1(n) = (n−1).
On the other hand,
Bn =
∞
∑
v=n
1
v 𝜁v ≤C
∞
∑
v=n
1
v2 =∶B1(n) = (n−1).
Letting M1(n) = V1(n) + B2
1(n) = (n−1) establishes the desired result.
Similarly if Condition 3.2 holds, then
Mn = 5
n 𝜁1∕2
0
+ 5
n
n−1
∑
v=2
1
v 𝜁1∕2
v−1
≤5
n + 5
√
C
n
n−1
∑
v=2
1
v(v −1)1∕4 =∶V1(n) = (n−1).

98
Statistical Implications of Turing’s Formula
Bn =
∞
∑
v=n
1
v 𝜁1,v ≤C
∞
∑
v=n
1
v1+1∕2 =∶B1(n) = (n−1).
Letting M1(n) = V1(n) + B2
1(n) = (n−1) establishes the desired result.
For Part 2, if Condition 3.3 holds, then
Mn = 5
n 𝜁1∕2
0
+ 5
n
n−1
∑
v=2
1
v 𝜁1∕2
v−1
≤5
n + 5
√
C
n
n−1
∑
v=2
1
v(v −1)𝛿∕2 =∶V2(n) = (n−1).
On the other hand,
Bn =
∞
∑
v=n
1
v 𝜁1 ≤C
∞
∑
v=n
1
v1+𝛿=∶B2(n) = (n−𝛿).
Since in this case 2𝛿< 1, letting M2(n) = V2(n) + B2
2(n) = (n−2𝛿) establishes
the desired result.
◽
The statements of Theorem 3.13 give the convergence rates of upper bounds
in mean-squared errors for various types of distribution. Statement 1 says that,
for all distributions with fast decaying tails, the bias of ̂Hz decays suﬃciently
fast so that |Bn|2 is dominated by Var( ̂Hz), which converges at a rate of (1∕n).
It may be interesting to note that the so-called fast decaying distributions here
include those with power decaying tails down to a threshold 𝛿= 1∕2. State-
ment 2 says that, for each of the thick-tailed distributions, the squared bias
|Bn|2 dominates the convergence in mean-squared errors.
Example 3.8
Suppose pk = Ck−𝜆, where C > 0 and 𝜆> 1 are constants,
for all k ≥k0 where k0 is some positive integer. It can be veriﬁed that
n𝛿∑
kpk(1 −pk)n →L > 0 where 𝛿= 1 −1∕𝜆for some constant L (see Exercise
13). By Theorem 3.13,
1) if 𝜆≥2, then {pk} ∈𝒫2 and therefore
E( ̂Hz −H)2 ≤(n−1) ;
and
2) if 1 < 𝜆< 2, then {pk} ∈𝒫3 and therefore
E( ̂Hz −H)2 = 
(
n−2(𝜆−1)
𝜆
)
.
Example 3.9
Suppose pk = Ce−𝜆k, where C > 0 and 𝜆> 0 are constants, for
all k ≥k0 where k0 is some positive integer. It can be veriﬁed that n𝜁n < U for
some constant U > 0. By Theorem 3.13, E( ̂Hz −H)2 = (n−1).

Estimation of Shannon’s Entropy
99
Furthermore, a suﬃcient condition may be established for a normal law of
√
n( ̂Hz −H). Recall H2 = ∑
k≥1pkln2pk, w(v + 1, n) of (3.29), and Cv of (3.30).
Let
̂H2z =
n−1
∑
v=1
{
Cv w(v + 1, n)
∑
k≥1
[
̂pk
v−1
∏
j=0
(
1 −̂pk −j
n
) ] }
.
(3.35)
Theorem 3.14
For a nonuniform distribution {pk; k ≥1} ∈𝒫satisfying
H2 < ∞, if there exists an integer-valued function K(n) such that, as n →∞,
1) K(n) →∞,
2) K(n) = o(
√
n∕ln n), and
3)
√
n∑
k≥K(n)pk ln pk →0,
then
√
n ( ̂Hz −H)
L
−−−→N(0, 𝜎2)
where 𝜎2 = H2 −H2.
Corollary 3.6
Under the conditions of Theorem 3.14,
√
n
⎛
⎜
⎜
⎜⎝
̂Hz −H
√
̂H2z −̂H2
z
⎞
⎟
⎟
⎟⎠
L
−−−→N(0, 1).
Toward proving Theorem 3.14 and Corollary 3.6, Z1,v, ̂Hz, and ̂H2z are
re-expressed as follows:
Z1,v = nv+1[n −(v + 1)]!
n!
∑
k≥1
[
̂pk
v−1
∏
j=0
(
1 −̂pk −j
n
) ]
=
∑
k≥1
[
nv+1[n −(v + 1)]!
n!
(
̂pk
v−1
∏
j=0
n −Yk −j
n
) ]
=
∑
k≥1
(
̂pk
v−1
∏
j=0
n −Yk −j
n −j −1
)
=
∑
k≥1
[
̂pk
v
∏
j=1
(
1 −Yk −1
n −j
) ]
,

100
Statistical Implications of Turing’s Formula
and therefore,
̂Hz =
n−1
∑
v=1
1
vZ1,v =
n−1
∑
v=1
1
v
∑
k
̂pk
v
∏
j=1
(
1 −Yk −1
n −j
)
=
∑
k
̂pk
n−1
∑
v=1
1
v
v
∏
j=1
(
1 −Yk −1
n −j
)
=
∑
k
̂pk
n−Yk
∑
v=1
1
v
v
∏
j=1
(
1 −Yk −1
n −j
)
and
̂H2z =
n−1
∑
v=1
CvZ1,v =
∑
k
̂pk
n−Yk
∑
v=1
Cv
v
∏
j=1
(
1 −Yk −1
n −j
)
.
Consider a modiﬁed probability distribution {pk,n; k = 1,…, K(n)} of {pk} as
follows.
Let
pk,n =
{
pk,
for
1 ≤k ≤K(n) −1
∑k ≥K(n)pk =∶pK(n),n,
for
k = K(n),
and
Yk,n =
{
Yk,
for
1 ≤k ≤K(n) −1
∑k ≥K(n)Yk =∶YK(n),n,
for
k = K(n),
and ̂pk,n = Yk,n∕n for 1 ≤k ≤K(n). Consequently, the following quantities are
also modiﬁed accordingly,
𝜁∗
v =
K(n)
∑
k=1
pk,n(1 −pk,n)v,
Z∗
1,v = n1+v[n −(1 + v)]!
n!
K(n)
∑
k=1
[
̂pk,n
v−1
∏
j=0
(
1 −̂pk,n −j
n
) ]
,
H∗=
K(n)
∑
k=1
(−pk,n ln pk,n) =
∞
∑
v=1
1
v 𝜁∗
v ,

Estimation of Shannon’s Entropy
101
̂H∗
z =
n−1
∑
v=1
1
vZ∗
1,v =
K(n)
∑
k=1
[
̂pk,n
n−Yk,n
∑
v=1
1
v
v
∏
j=1
(
1 −
Yk,n −1
n −j
) ]
,
̂H∗=
K(n)
∑
k=1
(−̂pk,n ln ̂pk,n),
H∗
2 =
∞
∑
v=1
Cv 𝜁∗
v ,
̂H∗
2z =
n−1
∑
v=1
CvZ∗
1,v =
K(n)
∑
k=1
[
̂pk,n
n−Yk,n
∑
v=1
Cv
v
∏
j=1
(
1 −
Yk,n −1
n −j
) ]
,
̂H∗
2 =
K(n)
∑
k=1
̂pk,nln2 ̂pk,n.
(3.36)
The following two facts are needed throughout the proofs:
1) E(Z1,v) = 𝜁v and E(Z∗
1,v) = 𝜁∗
v ; and
2)
Cv =
v−1
∑
i=1
1
i(v −i) = 1
v
v−1
∑
i=1
(1
i +
1
v −i
)
≤2(ln v + 1)
v
.
The ﬁrst fact is due to Zhang and Zhou (2010), and the second is easily veriﬁed
(see Exercise 14).
Lemmas 3.8 and 3.9 are due to Zhang and Zhang (2012).
Lemma 3.8
For any nonuniform distribution {pk; k ≥1} satisfying H2 < ∞,
if there exists an integer-valued function K(n) such that, as n →∞,
1) K(n) →∞,
2) K(n) = o(
√
n), and
3)
√
n∑
k≥K(n)pk ln pk →0,
then
a)
√
n
( ̂H −H
𝜎
)
L
−−−→N(0, 1),
b)
√
n
( ̂H∗−H∗
𝜎
)
L
−−−→N(0, 1).
where ̂H and ̂H∗are the plug-in estimators of H and H∗, respectively, and 𝜎2 =
H2 −H2.

102
Statistical Implications of Turing’s Formula
Lemma 3.9
For a probability distribution {pk; k ≥1}, if there exists
an integer-valued function K(n) such that as n →∞, K(n) →∞, and
√
n∑
k≥K(n)pk ln pk →0, then
a) (
√
n ln n)pK(n),n →0, and
b) −
√
npK(n),n ln pK(n),n →0.
Five more lemmas, Lemmas 3.10 through 3.14, are also needed.
Lemma 3.10
Under the conditions of Theorem 3.14,
√
n( ̂Hz −̂H∗
z ) = op(1).
Proof: Noting that for any k ≥K(n), Yk,n ≤YK(n),n and
0 ≤
n−Yk,n
∑
v=1
1
v
v
∏
j=1
(
1 −
Yk,n −1
n −j
)
−
n−YK(n),n
∑
v=1
1
v
v
∏
j=1
(
1 −
YK(n),n −1
n −j
)
≤
n−Yk,n
∑
v=1
1
v
v
∏
j=1
(
1 −
Yk,n −1
n −j
)
≤
n−Yk,n
∑
v=1
1
v ≤ln n + 1,
therefore,
0 ≤
√
n( ̂Hz −̂H∗
z )
=
√
n
∑
k≥K(n)
̂pk
n−Yk
∑
v=1
1
v
v
∏
j=1
(
1 −Yk −1
n −j
)
−
√
n̂pK(n),n
n−YK(n),n
∑
v=1
1
v
v
∏
j=1
(
1 −
YK(n),n −1
n −j
)
=
√
n
∑
k≥K(n)
̂pk
[n−Yk
∑
v=1
1
v
v
∏
j=1
(
1 −Yk −1
n −j
)
−
n−YK(n),n
∑
v=1
1
v
v
∏
j=1
(
1 −
YK(n),n −1
n −j
) ]
≤
√
n(ln n + 1)
∑
k≥K(n)
̂pk.
By Lemma 3.9,
√
n(ln n + 1)E
(
∑
k≥K(n)
̂pk,n
)
=
√
n(ln n + 1)
∑
k≥K(n)
pk,n →0,
hence
√
n ( ̂Hz −̂H∗
z
) = op(1) follows Chebyshev’s inequality.
◽

Estimation of Shannon’s Entropy
103
Lemma 3.11
As n →∞, under the conditions of Theorem 3.14,
√
n(E( ̂H∗
z ) −
H∗) →0.
Proof: The lemma follows the fact below.
0 ≤
√
n(H∗
n −E( ̂H∗
z ))
=
√
n
∞
∑
v=1
1
v 𝜁∗
v −
√
n
n−1
∑
v=1
1
v 𝜁∗
v
=
√
n
∞
∑
v=n
1
v 𝜁∗
v
=
√
n
∞
∑
v=n
1
v
K(n)
∑
k=1
pk,n(1 −pk,n)v
=
√
n
K(n)
∑
k=1
pk,n
∞
∑
v=n
1
v(1 −pk,n)v
≤
1
√
n
K(n)
∑
k=1
pk,n
∞
∑
v=n
(1 −pk,n)v
≤
1
√
n
K(n)
∑
k=1
pk,n
(1 −pk,n)n
pk,n
=
1
√
n
K(n)
∑
k=1
(1 −pk,n)n
≤K(n)
√
n
→0.
◽
Lemma 3.12
As n →∞, under the conditions of Theorem 3.14,
√
n(E( ̂H∗) −
H∗) →0.
Proof: Since f (x) = −x ln(x) is a concave function for x > 0, by Jensen’s
inequality,
√
n
K(n)
∑
k=1
E(−̂pk,n ln ̂pk,n + pk,n ln pk,n) ≤0.
Also by (3.3),
√
n
K(n)
∑
k=1
(E( ̂H∗
n) −H∗
n)
=
√
n
K(n)
∑
k=1
E(−̂pk,n ln ̂pk,n + pk,n ln pk,n)1[pk,n ≥1∕n]

104
Statistical Implications of Turing’s Formula
+
√
n
K(n)
∑
k=1
E(−̂pk,n ln ̂pk,n + pk,n ln pk,n)1[pk,n < 1∕n]
≥
√
n
[
−K(n) −1
2n
+
1
12n2
(
1 −
K(n)
∑
k=1
1
pk,n
1[pk,n ≥1∕n]
)
+ O(n−3)
]
+
√
n
K(n)
∑
k=1
(pk,n ln pk,n)1[pk,n < 1∕n]
≥−
√
nK(n)
2n
−
√
nK(n)n
12n2
−
√
nK(n) ln n
n
→0.
Therefore,
√
n(E( ̂H∗) −H∗) →0.
◽
Lemma 3.13
If a and b are such that 0 < a < b < 1, then for any integer
m ≥0,
bm −am ≤mbm−1(b −a).
Proof: Noting that f (x) = xm is convex on interval (0, 1) and
df (b)
db
= mbm−1,
the result follows immediately.
◽
Lemma 3.14
Under the conditions of Theorem 3.14,
√
n( ̂H∗
z −̂H∗) = op(1).
A proof of Lemma 3.14 is given in the appendix of this chapter.
Proof of Theorem 3.14: Note that
√
n( ̂Hz −H) −
√
n( ̂H∗−H∗) =
√
n( ̂Hz −̂H∗) −
√
n(H −H∗)
=
√
n( ̂Hz −̂H∗
z ) +
√
n( ̂H∗
z −̂H∗) −
√
n(H −H∗)
=
√
n( ̂Hz −̂H∗
z ) +
√
n( ̂H∗
z −̂H∗)
+
√
n
∑
k≥K(n)
(pk ln pk) −
√
n
pK(n),n ln pK(n),n.
(3.37)
Since
√
n( ̂H∗−H∗)
L
−−−→N(0, 𝜎2) by Part (b) of Lemma 3.8, it suﬃces to show
that each of the four terms in the right-hand side of (3.37) is op(1).
The ﬁrst two terms are op(1) by Lemmas 3.10 and 3.14, respectively, the third
term goes to 0 by the conditions of Theorem 3.14, and the fourth term goes
to 0 by Lemma 3.9. Therefore, the statement of the theorem follows Slutsky’s
lemma.
◽
To prove Corollary 3.6, a few more lemmas are needed.

Estimation of Shannon’s Entropy
105
Lemma 3.15
Under the conditions of Theorem 3.14, ̂H2 −̂H∗
2 = op(1).
Proof:
0 ≤̂H2 −̂H∗
2
=
∑
k≥K(n)
̂pk,nln2 ̂pk,n −̂pK(n),nln2 ̂pK(n),n
=
∑
k≥K(n)
̂pk,nln2 ̂pk,n −
∑
k≥K(n)
̂pk,nln2 ̂pK(n),n
=
∑
k≥K(n)
̂pk,n(ln2 ̂pk,n −ln2 ̂pK(n),n)
≤
∑
k≥K(n)
̂pk,nln2 ̂pk,n
≤ln2n
∑
k≥K(n)
̂pk,n.
By Lemma 3.9, ln2nE (∑
k≥K(n) ̂pk,n
) = (ln2n)pK(n),n →0, ̂H2 −̂H∗
2 = op(1) fol-
lows Chebyshev’s inequality.
◽
Lemma 3.16
Under the conditions of Theorem 3.14, ̂H2z −̂H∗
2z = op(1).
Proof: Noting that for any k ≥K(n), Yk,n ≤YK(n),n and
0 ≤
n−Yk,n
∑
v=1
Cv
v
∏
j=1
(
1 −
Yk,n −1
n −j
)
−
n−YK(n),n
∑
v=1
Cv
v
∏
j=1
(
1 −
YK(n),n −1
n −j
)
≤
n−Yk,n
∑
v=1
Cv
v
∏
j=1
(
1 −
Yk,n −1
n −j
)
≤
n−Yk,n
∑
v=1
Cv
≤
n
∑
v=1
2(ln v + 1)
v
≤2(ln n + 1)2,
therefore
0 ≤̂H2z −̂H∗
2z
=
∑
k≥K(n)
̂pk,n
n−Yk,n
∑
v=1
Cv
v
∏
j=1
(
1 −
Yk,n −1
n −j
)

106
Statistical Implications of Turing’s Formula
−̂pK(n),n
n−YK(n),n
∑
v=1
Cv
v
∏
j=1
(
1 −
YK(n),n −1
n −j
)
=
∑
k≥K(n)
̂pk,n
[n−Yk,n
∑
v=1
Cv
v
∏
j=1
(
1 −
Yk,n −1
n −j
)
−
n−YK(n),n
∑
v=1
Cv
v
∏
j=1
(
1 −
YK(n),n −1
n −j
) ]
≤2(ln n + 1)2 ∑
k≥K(n)
̂pk,n.
By Lemma 3.9,
(ln n + 1)2E
(
∑
k≥K(n)
̂pk,n
)
= (ln n + 1)2 ∑
k≥K(n)
pk,n →0,
therefore, ̂H2z −̂H∗
2z = op(1) follows Chebyshev’s inequality.
◽
Lemma 3.17
As n →∞, under the conditions of Theorem 3.14,
E( ̂H∗
2z) −H∗
2 →0.
Proof:
0 ≤H∗
2 −E( ̂H∗
2z)
=
∞
∑
v=1
Cv𝜁∗
v −
n−1
∑
v=1
Cv𝜁∗
v
=
∞
∑
v=n
Cv𝜁∗
v
=
∞
∑
v=n
Cv
K(n)
∑
k=1
pk,n(1 −pk,n)v
=
K(n)
∑
k=1
pk,n
∞
∑
v=n
Cv(1 −pk,n)v
≤2(ln n + 1)
n
K(n)
∑
k=1
pk,n
∞
∑
v=n
(1 −pk,n)v
≤2(ln n + 1)
n
K(n)
∑
k=1
pk,n
(1 −pk,n)n
pk,n

Estimation of Shannon’s Entropy
107
= 2(ln n + 1)
n
K(n)
∑
k=1
(1 −pk,n)n
≤2(ln n + 1)K(n)
n
→0.
◽
Lemma 3.18
Under the conditions of Theorem 3.14, ̂H∗
2z −̂H∗
2 = op(1).
A proof of Lemma 3.18 is given in the appendix of this chapter.
Proof of Corollary 3.6: Note that
̂H2z −H2 = ( ̂H2z −̂H∗
2z) + ( ̂H∗
z −̂H∗
2) + ( ̂H∗
2 −̂H2) + ( ̂H2 −H2).
Each of the ﬁrst three terms in the above-mentioned equation is op(1) by Lem-
mas 3.16, 3.18, and 3.15, respectively. Also, it is shown, in the proof of Corol-
lary 1 of Zhang and Zhang (2012), that ̂H2 −H2 = op(1). Therefore, ̂H2z −H2 =
op(1).
By Theorem 3.14, and the fact that ̂H2
z
p
−−−→H2,
̂H2z −̂H2
z
p
−−−→H2 −H2 = 𝜎2.
Finally, the corollary follows Slutsky’s lemma.
◽
Remark 3.3
The suﬃcient condition (Part 2) given in Theorem 3.14 for the
normality of ̂Hz is slightly more restrictive than that of the plug-in estimator
̂H as stated in Theorem 1 of Zhang and Zhang (2012), and consequently sup-
ports a smaller class of distributions. It can be shown that the suﬃcient condi-
tions of Theorem 3.14 still holds for pk = C𝜆k−𝜆where 𝜆> 2, but not for pk =
C∕(k2ln2k), which satisﬁes the suﬃcient conditions of Theorem 1 of Zhang and
Zhang (2012). However, simulation results indicate that the asymptotic normal-
ity of ̂Hz in Theorem 3.14 and Corollary 3.6 may still hold for pk = C∕(k2ln2k) for
k ≥1 though not covered by the suﬃcient condition, which leads to a conjecture
that the suﬃcient condition of Theorem 3.14 could be further relaxed.
3.4
Appendix
3.4.1
Proof of Lemma 3.2
Proof of Part 1: As the notation in gn(p) suggests, the range for v is from 1 to
min{n −1, n(1 −p) + 1}. Noting
0 ≤1 −j∕[n(1 −p)]
1 −j∕n
≤1

108
Statistical Implications of Turing’s Formula
subject to j ≤n(1 −p), by Lemma 3.1,
| w(v + 1, n)
v−1
∏
j=0
(
1 −p −j
n
)
−(1 −p)v |
= (1 −p)v
|||||||
v−1
∏
j=0
⎛
⎜
⎜⎝
1 −
j
n(1−p)
1 −j+1
n
⎞
⎟
⎟⎠
−1
|||||||
= (1 −p)v
|||||||
(
n
n −v
)
v−1
∏
j=0
⎛
⎜
⎜⎝
1 −
j
n(1−p)
1 −j
n
⎞
⎟
⎟⎠
−1
|||||||
= (1 −p)v
|||||||
(
1 +
v
n −v
)
v−1
∏
j=0
⎛
⎜
⎜⎝
1 −
j
n(1−p)
1 −j
n
⎞
⎟
⎟⎠
−1
|||||||
≤(1 −p)v (
v
n −v
)
+ (1 −p)v
|||||||
v−1
∏
j=0
⎛
⎜
⎜⎝
1 −
j
n(1−p)
1 −j
n
⎞
⎟
⎟⎠
−1
|||||||
≤(1 −p)v (
v
n −v
)
+ (1 −p)v
v−1
∑
j=0
|||||||
⎛
⎜
⎜⎝
1 −
j
n(1−p)
1 −j
n
⎞
⎟
⎟⎠
−1
|||||||
= (1 −p)v (
v
n −v
)
+ (1 −p)v
p
1 −p
v−1
∑
j=1
j
n −j
≤(1 −p)v−1 (
v
n −v
)
+ (1 −p)v−1
v−1
∑
j=1
j
n −j
= (1 −p)v−1
v∑
j=1
j
n −j ≤(1 −p)v−1
v2
n −v.
For a suﬃciently large n, let Vn = ⌊n1∕8⌋where ⌊⋅⌋is the ﬂoor of a real value.
√
n|hn(p) −h(p)|
≤
√
np
⌊n(1−p)+1⌋
∑
v=1
1
v
||||||
w(v + 1, n)
v−1
∏
j=0
(
1 −p −j
n
)
−(1 −p)v
||||||
+
√
np
∞
∑
v=⌊n(1−p)+2⌋
1
v(1 −p)v
=
√
np
Vn
∑
v=1
1
v
||||||
w(v + 1, n)
v−1
∏
j=0
(
1 −p −j
n
)
−(1 −p)v
||||||

Estimation of Shannon’s Entropy
109
+
√
np
⌊n(1−p)+1⌋
∑
v=Vn+1
1
v
||||||
w(v + 1, n)
v−1
∏
j=0
(
1 −p −j
n
)
−(1 −p)v
||||||
+
√
np
∞
∑
v=⌊n(1−p)+2⌋
1
v(1 −p)v
=∶Δ1 + Δ2 + Δ3.
Δ1 ≤
√
np
Vn
∑
v=1
v
n −v(1 −p)v−1
≤
n5∕8
n −n1∕8 →0.
Δ2 ≤p
√
n
⌊n(1−p)+1⌋
∑
v=Vn+1
v
n −v(1 −p)v−1
≤p
√
n[n(1 −p) + 1]
np −1
⌊n(1−p)+1⌋
∑
v=Vn+1
(1 −p)v−1
≤
√
n[n(1 −p) + 1]
np −1
(1 −p)⌊n1∕8⌋
≤
√
n[n(1 −c) + 1]
nc −1
(1 −c)⌊n1∕8⌋→0.
Δ3 ≤
√
n
n(1 −p)(1 −p)⌊n(1−p)+2⌋
=
1
√
n
(1 −p)⌊n(1−p)+1⌋
≤
1
√
n
→0.
Hence, supp∈(c,1)
√
n|hn(p) −h(p)| →0.
◽
Proof of Part 2: The proof is identical to that of the above-mentioned Part 1
until the expression Δ1 + Δ2 + Δ3 where each term is to be evaluated on the
interval [1∕n, c]. It is clear that Δ1 ≤O(n−3∕8). For Δ2, since n(1 −p) + 1 at
p = 1∕n is n > n −1, it follows that
Δ2 ≤p
√
n
min{n−1,⌊n(1−p)+1⌋}
∑
v=Vn+1
v
n −v(1 −p)v−1
≤p
√
n
min{n−1,⌊n(1−1∕n)+1⌋}
∑
v=Vn+1
v
n −v(1 −p)v−1

110
Statistical Implications of Turing’s Formula
= p
√
n
n−1
∑
v=Vn+1
v
n −v(1 −p)v−1
< p
√
n(n −1)
n−1
∑
v=Vn+1
(1 −p)v−1
<
√
n(n −1)(1 −p)Vn <
√
n(n −1) = O(n3∕2).
Δ3 = p
√
n
∞
∑
v=min{n−1,⌊n(1−p)+1⌋}+1
1
v(1 −p)v
< p
√
n
∞
∑
v=1
1
v(1 −p)v <
√
n = O(n1∕2).
Therefore, Δ1 + Δ2 + Δ3 = O(n3∕2).
◽
Proof of Part 3: Let Z and 𝜙(z) be a standard normal random variable and its
density function, respectively, and let “≃” denote asymptotic equality. Since
√
n(̂p −p)
L
−−−→N(0, p(1 −p)),
P(̂p ≤c) ≃∫
√
n(c−p)∕
√
p(1−p)
−∞
𝜙(z)dz
= ∫
∞
√
n(p−c)∕
√
p(1−p)
𝜙(z)dz
< ∫
∞
√
n(p−c)∕
√
p(1−p)
e−z [
√
n(p−c)∕
√
p(1−p)] dz
=
√
p(1 −p)
√
n(p −c) ∫
∞
[
√
n(p−c)∕
√
p(1−p)]2 e−xdx
=
√
p(1 −p)
√
n(p −c)
exp{−[
√
n(p −c)∕
√
p(1 −p)]2}
= n−1∕2
√
p(1 −p)
(p −c)
exp
{
−n(p −c)2
p(1 −p)
}
.
◽
3.4.2
Proof of Lemma 3.5
Proof: Let m = v + 1. By Lemmas 3.3 and 3.4, and identity
( n
m
)−1
m
∑
c=1
c
(m
c
) (n −m
m −c
)
= m2
n ,
Var(Z1,v) ≤
( n
m
)−1
m
∑
c=1
c
(m
c
) (n −m
m −c
) 𝜎2
m
m = m
n 𝜎2
m.
(3.38)

Estimation of Shannon’s Entropy
111
Consider
𝜎2
m = Var(𝜓(X1,…, Xm))
= E(𝜓(X1,…, Xm))2 −
[ K
∑
k=1
pk(1 −pk)m−1
]2
.
Let Y (m)
k
denote the frequency of the kth letter in the sample of size m.
𝜎2
m ≤E(𝜓(X1,…, Xm))2
= 1
m2 E
[ ( K
∑
k=1
1[Yk = 1]
) ( K
∑
k′=1
1[Yk′ = 1]
) ]
= 1
m2 E
( K
∑
k=1
1[Yk = 1] + 2
∑
1≤k<k′≤K
1[Yk = 1]1[Yk′ = 1]
)
= 1
m
K
∑
k=1
pk(1 −pk)m−1 + 2(m −1)
m
∑
1≤k<k′≤K
pkpk′ (1 −pk −pk′ )m−2
≤1
m
K
∑
k=1
pk(1 −pk)m−1 + 2
∑
1≤k<k′≤K
pkpk′(1 −pk −pk′ + pkpk′ )m−2
= 1
m
K
∑
k=1
pk(1 −pk)m−1 + 2
∑
1≤k<k′≤K
[pk(1 −pk)m−2pk′(1 −pk′)m−2]
≤1
m
K
∑
k=1
pk(1 −pk)m−1 +
[ K
∑
k=1
pk(1 −pk)m−2
]2
= 1
m 𝜁m−1 + 𝜁2
m−2.
By (3.38), Var(Z1,v) ≤1
n 𝜁v + v+1
n
𝜁2
v−1.
◽
3.4.3
Proof of Corollary 3.5
Proof: Noting 𝜁v ≥𝜁w if v ≤w,
Mn = 5
n
(n−1
∑
v=1
1
v 𝜁1∕2
v−1
)2
= 5
n
(n−1
∑
v=1
1
v 𝜁1∕2
v−1
) (n−1
∑
w=1
1
w 𝜁1∕2
w−1
)

112
Statistical Implications of Turing’s Formula
= 5
n
(n−1
∑
v=1
1
v2 𝜁v−1 + 2
∑
1≤v<w≤n−1
1
vw 𝜁1∕2
1,v−1 𝜁1∕2
1,w−1
)
≤5
n
(n−1
∑
v=1
1
v2 𝜁v−1 + 2
∑
1≤v<w≤n−1
1
vw 𝜁v−1
)
= 5
n
(n−1
∑
w=1
1
w
n−1
∑
v=1
1
v 𝜁v−1
)
= 5
n
(n−1
∑
w=1
1
w
) (n−1
∑
v=1
1
v 𝜁v−1
)
.
The expression in the ﬁrst pair of parentheses above is the harmonic series,
which has a well-known upper bound 1 + ln(n −1). Consider the expression in
the second pair of parentheses.
n−1
∑
v−1
1
v 𝜁v−1 = 1 +
n−1
∑
v=2
1
v 𝜁v−1 < 1 +
n−1
∑
v=2
1
v −1 𝜁v−1
= 1 +
n
∑
v=1
1
v 𝜁v
< 1 +
∞
∑
v=1
1
v 𝜁v
= 1 + H.
Therefore,
Mn < 5(1 + H)
(1 + ln(n −1)
n
)
= (ln n∕n).
◽
3.4.4
Proof of Lemma 3.14
Proof:
√
n( ̂H∗
z −̂H∗) =
√
n
(
̂H∗
z +
K(n)
∑
k=1
̂pk,n ln ̂pk,n
)
=
{
√
n
[
̂H∗
z −
K(n)
∑
k=1
n−Yk,n
∑
v=1
1
v ̂pk,n(1 −̂pk,n)v
] }
−
{
√
n
K(n)
∑
k=1
∞
∑
v=n−Yk,n+1
1
v ̂pk,n(1 −̂pk,n)v
}
=∶{1} −{2} .

Estimation of Shannon’s Entropy
113
Since
0 ≤2 =
√
n
K(n)
∑
k=1
∞
∑
v=n−Yk,n+1
1
v ̂pk,n(1 −̂pk,n)v
≤
K(n)
∑
k=1
√
n
n −Yk,n + 1 ̂pk,n
∞
∑
v=n−Yk,n+1
(1 −̂pk,n)v
=
K(n)
∑
k=1
√
n
n −Yk,n + 1(1 −̂pk,n)n−Yk,n+1
≤
1
√
n
K(n)
∑
k=1
1
1 −̂pk,n + 1∕n(1 −̂pk,n)n−Yk,n+1
=
1
√
n
K(n)
∑
k=1
1
1 −̂pk,n + 1∕n(1 −̂pk,n)n−Yk,n+11[̂pk,n < 1]
≤
1
√
n
K(n)
∑
k=1
1
1 −̂pk,n
(1 −̂pk,n)n−Yk,n+11[̂pk,n < 1]
=
1
√
n
K(n)
∑
k=1
(1 −̂pk,n)n−Yk,n1[̂pk,n < 1] ≤K(n)
√
n
,
2
a.s.
−−−→0 and therefore 2
p
−−−→0.
Before considering 1, following are several facts that are to be noted. First
(
1 −
Yk,n −1
n −j
)
≥
(
1 −
Yk,n
n
)
= (1 −̂pk,n)
if and only if
0 ≤j ≤
n
Yk,n + 1[Yk,n = 0] =∶
1
̂p∗
k,n
;
(3.39)
and second, after a few algebraic steps, Z∗
1,v may be expressed as
Z∗
1,v =
K(n)
∑
k=1
̂pk,n
v
∏
j=1
(
1 −
Yk,n −1
n −j
)
=
K(n)
∑
k=1
̂pk,n
Jk∧v
∏
j=1
(
1 −
Yk,n −1
n −j
)
v
∏
j=Jk∧v+1
(
1 −
Yk,n −1
n −j
)
(3.40)
where Jk = ⌊1∕̂p∗
k,n⌋and ∏b
v=a(⋅) = 1 if a > b.

114
Statistical Implications of Turing’s Formula
1 =
√
n
[
̂H∗
z −
K(n)
∑
k=1
n−Yk,n
∑
v=1
1
v ̂pk,n(1 −̂pk,n)v
]
=
√
n
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
1
v
[Jk∧v
∏
j=1
(
1 −
Yk,n −1
n −j
)
v
∏
j=Jk∧v+1
(
1 −
Yk,n −1
n −j
)
=
{
√
n
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
1
v
[Jk∧v
∏
j=1
(
1 −
Yk,n −1
n −j
)
×
v
∏
j=Jk∧v+1
(
1 −
Yk,n −1
n −j
)
−
Jk∧v
∏
j=1
(
1 −
Yk,n −1
n −j
)
(1 −̂pk,n)0∨(v−Jk)
] }
+
{
√
n
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
1
v
[Jk∧v
∏
j=1
(
1 −
Yk,n −1
n −j
)
(1 −̂pk,n)0∨(v−Jk)
=∶{1,1} + {1,2} .
By (3.39), 1,1 ≤0 and 1,2 ≥0. It suﬃces to show that E(1,1) →0 and
E(1,2) →0, respectively.
1,1 =
√
n ( ̂H∗
z −H∗)
−
√
n
[K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
1
v
Jk∧v
∏
j=1
(
1 −
Yk,n −1
n −j
)
(1 −̂pk,n)0∨(v−Jk)
−H∗
]
=
{√
n ( ̂H∗
z −H∗)}
−
{
√
n
[K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
1
v
Jk∧v
∏
j=1
(
1 −
Yk,n −1
n −j
)
(1 −̂pk,n)0∨(v−Jk)
−
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
1
v (1 −̂pk,n)v
] }
−
{
√
n
[K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
1
v (1 −̂pk,n)v −H∗
] }
=∶{1,1,1} −{1,1,2} −{1,1,3} .

Estimation of Shannon’s Entropy
115
E(1,1,1) →0 follows Lemma 3.11. Then,
1,1,3 =
√
n
[K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
1
v (1 −̂pk,n)v −H∗
]
=
√
n
[K(n)
∑
k=1
̂pk,n
∞
∑
v=1
1
v (1 −̂pk,n)v −H∗
]
−
√
n
K(n)
∑
k=1
̂pk,n
∞
∑
v=n−Yk,n+1
1
v (1 −̂pk,n)v
= {
√
n ( ̂H∗−H∗) } −
{
√
n
K(n)
∑
k=1
̂pk,n
∞
∑
v=n−Yk,n+1
1
v (1 −̂pk,n)v
}
=∶{1,1,3,1} −{2} .
E(1,1,3,1) →0 by Lemma 3.12, E(2) →0 is established above, and therefore
E(1,1,3) →0.
1,1,2 =
√
n
[K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
1
v
Jk∧v
∏
j=1
(
1 −
Yk,n −1
n −j
)
(1 −̂pk,n)0∨(v−Jk)
−
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
1
v (1 −̂pk,n) v
]
≤
√
n
[K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
1
v
Jk∧v
∏
j=1
(
1 −
Yk,n −1
n −1
)
(1 −̂pk,n)0∨(v−Jk)
−
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
1
v (1 −̂pk,n) v
]
=
√
n
[K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
1
v
(
1 −
Yk,n −1
n −1
)
Jk∧v(1 −̂pk,n)0∨(v−Jk)
−
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
1
v (1 −̂pk,n) Jk∧v(1 −̂pk,n)0∨(v−Jk)
]
=
√
n
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
1
v
[ (
1 −
Yk,n −1
n −1
)
Jk∧v −(1 −̂pk,n) Jk∧v
]
× (1 −̂pk,n)0∨(v−Jk)

116
Statistical Implications of Turing’s Formula
≤
√
n
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
1
v
[ (
1 −
Yk,n −1
n −1
)Jk∧v
−(1 −̂pk,n)Jk∧v
]
≤
√
n
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
1
v
[
(Jk ∧v)
(
1 −
Yk,n −1
n −1
)Jk∧v−1 n −Yk,n
n(n −1)
]
(by Lemma 3.13)
≤
√
n
n −1
K(n)
∑
k=1
̂pk,n(1 −̂pk,n)
[n−Yk,n
∑
v=1
1
v(Jk ∧v)
]
=
√
n
n −1
K(n)
∑
k=1
̂pk,n(1 −̂pk,n)
[ Jk
∑
v=1
1
v(Jk ∧v) +
n−Yk,n
∑
v=Jk+1
1
v(Jk ∧v)
]
=
√
n
n −1
K(n)
∑
k=1
̂pk,n(1 −̂pk,n)
(
Jk + Jk
n−Yk,n
∑
v=Jk+1
1
v
)
≤
√
n
n −1
K(n)
∑
k=1
̂pk,n
(
Jk + Jk
n
∑
v=1
1
v
)
≤
√
n
n −1
K(n)
∑
k=1
Yk,n
n
n
Yk,n + 1[Yk,n = 0] (ln n + 2)
≤
√
nK(n)(ln n + 2)
n −1
.
Therefore,
E(1,1,2) ≤
( √
nK(n) ln n
n
)
→0.
Finally, E(1,2) = E(1,1,2) →0. It follows that
√
n( ̂H∗
z −̂H∗) = op(1).
◽
3.4.5
Proof of Lemma 3.18
Proof:
̂H∗
2z −̂H∗
2 = ̂H∗
2z −
K(n)
∑
k=1
̂pk,nln2 ̂pk,n
=
{
̂H∗
2z −
K(n)
∑
k=1
n−Yk,n
∑
v=1
Cv ̂pk,n(1 −̂pk,n)v
}
−
{K(n)
∑
k=1
∞
∑
v=n−Yk,n+1
Cv ̂pk,n(1 −̂pk,n)v
}
=∶{1} −{2} .

Estimation of Shannon’s Entropy
117
Since
0 ≤2 =
K(n)
∑
k=1
∞
∑
v=n−Yk,n+1
Cv ̂pk,n(1 −̂pk,n)v
≤
K(n)
∑
k=1
2(ln n + 1)
n −Yk,n + 1 ̂pk,n
∞
∑
v=n−Yk,n+1
(1 −̂pk,n)v
=
K(n)
∑
k=1
2(ln n + 1)
n −Yk,n + 1(1 −̂pk,n)n−Yk,n+1
≤2(ln n + 1)
n
K(n)
∑
k=1
1
1 −̂pk,n + 1∕n(1 −̂pk,n)n−Yk,n+1
= 2(ln n + 1)
n
K(n)
∑
k=1
1
1 −̂pk,n + 1∕n(1 −̂pk,n)n−Yk,n+11[̂pk,n < 1]
≤2(ln n + 1)
n
K(n)
∑
k=1
1
1 −̂pk,n
(1 −̂pk,n)n−Yk,n+11[̂pk,n < 1]
= 2(ln n + 1)
n
K(n)
∑
k=1
(1 −̂pk,n)n−Yk,n1[̂pk,n < 1]
≤2(ln n + 1)K(n)
n
,
2
a.s.
−−−→0 and therefore 2
p
−−−→0.
Next,
1 = ̂H∗
2z −
K(n)
∑
k=1
n−Yk,n
∑
v=1
Cv ̂pk,n(1 −̂pk,n)v
=
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
Cv
[Jk∧v
∏
j=1
(
1 −
Yk,n −1
n −j
)
v
∏
j=Jk∧v+1
(
1 −
Yk,n −1
n −j
)
−(1 −̂pk,n)v
]
=
{K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
Cv
[Jk∧v
∏
j=1
(
1 −
Yk,n −1
n −j
)
v
∏
j=Jk∧v+1
(
1 −
Yk,n −1
n −j
)
−
Jk∧v
∏
j=1
(
1 −
Yk,n −1
n −j
)
(1 −̂pk,n)0∨(v−Jk)
] }
+
{K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
Cv
[Jk∧v
∏
j=1
(
1 −
Yk,n −1
n −j
)
(1 −̂pk,n)0∨(v−Jk)(1 −̂pk,n)v
]}
=∶{1,1} + {1,2} .

118
Statistical Implications of Turing’s Formula
By (3.39), 1,1 ≤0 and 1,2 ≥0. It suﬃces to show that E(1,1) →0 and
E(1,2) →0, respectively.
1,1 = ( ̂H∗
2z −H∗
2)
−
[K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
Cv
Jk∧v
∏
j=1
(
1 −
Yk,n −1
n −j
)
(1 −̂pk,n)0∨(v−Jk)
−H∗
2
]
= { ̂H∗
2z −H∗
2
}
−
{K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
Cv
Jk∧v
∏
j=1
(
1 −
Yk,n −1
n −j
)
(1 −̂pk,n)0∨(v−Jk)
−
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
Cv (1 −̂pk,n)v
}
−
{K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
Cv (1 −̂pk,n)v −H∗
2
}
=∶{1,1,1} −{1,1,2} −{1,1,3} .
E(1,1,1) →0 follows Lemma 3.17.
Next,
1,1,3 =
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
Cv (1 −̂pk,n)v −H∗
2
=
[K(n)
∑
k=1
̂pk,n
∞
∑
v=1
Cv (1 −̂pk,n)v −H∗
2
]
−
K(n)
∑
k=1
̂pk,n
∞
∑
v=n−Yk,n+1
Cv (1 −̂pk,n)v
= { ̂H∗
2 −H∗
2} −
{K(n)
∑
k=1
̂pk,n
∞
∑
v=n−Yk,n+1
Cv (1 −̂pk,n)v
}
=∶{1,1,3,1} −{2} .
The proof of E(1,1,3,1) →0 is implied by the proof of Corollary 1 of Zhang
and Zhang (2012). Also, E(2) →0 is established above, and therefore,
E(1,1,3) →0.

Estimation of Shannon’s Entropy
119
Next,
1,1,2 =
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
Cv
Jk∧v
∏
j=1
(
1 −
Yk,n −1
n −j
)
(1 −̂pk,n)0∨(v−Jk)
−
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
Cv (1 −̂pk,n)v
≤
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
Cv
Jk∧v
∏
j=1
(
1 −
Yk,n −1
n −1
)
(1 −̂pk,n)0∨(v−Jk)
−
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
Cv (1 −̂pk,n)v
=
[K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
Cv
(
1 −
Yk,n −1
n −1
)Jk∧v
(1 −̂pk,n)0∨(v−Jk)
−
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
Cv (1 −̂pk,n)Jk∧v(1 −̂pk,n)0∨(v−Jk)
]
=
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
Cv
[ (
1 −
Yk,n −1
n −1
)Jk∧v
−(1 −̂pk,n) Jk∧v] (1 −̂pk,n)0∨(v−Jk)
≤
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
Cv
[ (
1 −
Yk,n −1
n −1
)Jk∧v
−(1 −̂pk,n)Jk∧v
]
≤
K(n)
∑
k=1
̂pk,n
n−Yk,n
∑
v=1
Cv
[
(Jk ∧v)
(
1 −
Yk,n −1
n −1
)Jk∧v−1 n −Yk,n
n(n −1)
]
(by Lemma 3.13)
≤
1
n −1
K(n)
∑
k=1
̂pk,n(1 −̂pk,n)
[n−Yk,n
∑
v=1
Cv(Jk ∧v)
]
≤
1
n −1
K(n)
∑
k=1
̂pk,n(1 −̂pk,n)Jk
n−Yk,n
∑
v=1
Cv
≤
1
n −1
K(n)
∑
k=1
̂pk,n(1 −̂pk,n)Jk
n
∑
v=1
2(ln n + 1)
v
≤2(ln n + 1)
n −1
K(n)
∑
k=1
̂pk,nJk(ln n + 1)

120
Statistical Implications of Turing’s Formula
≤2(ln n + 1)2
n −1
K(n)
∑
k=1
Yk,n
n
n
Yk,n + 1[Yk,n = 0]
≤2(ln n + 1)2K(n)
n −1
.
Therefore,
E(1,1,2) ≤
(
K(n)ln2n
n
)
→0.
Finally, E(1,2) = E(1,1,2) →0. It follows that ̂H∗
2z −̂H∗
2 = op(1).
◽
3.5
Remarks
The existing literature on entropy estimation seems to suggest a heavy focus
on bias reduction. Although bias reduction is undoubtedly important, an argu-
ment could be made to support a shift in research focus to other worthy direc-
tions. For example, the distributional characteristics of proposed estimators
are, with a few exceptions, largely ignored in the literature. The lack of results
in weak convergence of proposed entropy estimators is perhaps partially due to
the inherent diﬃculty in establishing such results in theory. Nevertheless, one
must have such results in order to make reasonable statistical inference about
the underlying entropy.
As evidenced by each and every one of the many normal laws introduced
in this chapter, the asymptotic variance always contains the second entropic
moment H2 = ∑
k≥1pkln2pk. To make inference about the underlying entropy
H, one must have not only a good estimator for H but also a good estimator for
H2. Future research on nonparametric estimation of H2 would be beneﬁcial.
Another issue needing more investigation is the sample size consideration in
estimating entropy. How large should sample size n be in order to produce a
reasonably reliable estimate of H? Answers to this question would be interest-
ing both in theory and in practice. Many papers in the existing literature discuss
bias reduction in cases of small samples with sizes as small as n = 10 or n = 20.
Given the fact that entropy is meant to be a measure of chaos in a complex
system, it would seem quite forced to estimate entropy based on such small
samples. In the existing literature, there does not seem to exist a guideline on
how large a sample must be to make entropy estimation realistically meaning-
ful. On the other hand, there is a natural perspective based on Turing’s formula.
Simply put, Turing’s formula T1 estimates the sample noncoverage of the alpha-
bet, and therefore 1 −T1 estimates the sample coverage of the alphabet. Given a
sample data set, if 1 −T1 is small, then perhaps one should not force an estimate
of entropy. For example, if a data set contains only singletons, that is, N1 = n,
then 1 −T1 = 0, suggesting that the sample is perhaps not large enough to have
any meaningful coverage.

Estimation of Shannon’s Entropy
121
3.6
Exercises
1
Let X be the number of heads observed when a fair coin is tossed four
times. Find the entropy of X.
2
A fair coin is repeatedly tossed until the ﬁrst head is observed. Let X be
the number of tosses required to observe the ﬁrst head. Find the entropy
of X.
3
Verify the bias (3.3) for the plug-in estimator ̂H in (3.2).
4
Verify the bias (3.6) for the jackknife estimator ̂HJK in (3.5).
5
Show the following.
a) The Dirichlet prior of (3.7) leads to the Dirichlet posterior of (3.8).
b) For each k, the mean of Dirichlet posterior of (3.8) is
E(pk|y) =
yk + 𝛼k
n + ∑K
k=1 𝛼k
.
6
Show that if {pk} is uniformly distributed, that is, pk = 1∕K for each
k = 1,…, K, then 𝜎2 (3.11) is zero.
7
Prove Theorem 3.2.
8
Prove Theorem 3.3.
9
Verify that H < ∞under the distribution {pk; k ≥1} where pk = c𝜆k−𝜆
and 𝜆is any real value satisfying 𝜆> 1.
10
In Example 3.4, show
√
n ∫
∞
K(n)
C𝜆
x𝜆ln
(C𝜆
x𝜆
)
dx = C𝜆𝜆
𝜆−1
√
n ln K(n)
(K(n))𝜆−1
+
[
C𝜆𝜆
(𝜆−1)2 −C𝜆ln C𝜆
𝜆−1
]
√
n
(K(n))𝜆−1 .
11
Let aj, j = 1,…, n, be real numbers satisfying |aj| ≤1 for every j. Show
that |||
∏n
j=1 aj −1||| ≤∑n
j=1 |aj −1|.

122
Statistical Implications of Turing’s Formula
12
For any given distribution {pk} with ﬁnite K, |E( ̂Hz2 −H2)| decays expo-
nentially in n.
13
Suppose pk = Ck−𝜆, where C > 0 and 𝜆> 1 are constants, for all k ≥k0
where k0 is some positive integer. Show that
lim
n→∞n𝛿∑
k
pk(1 −pk)n,
where 𝛿= 1 −1∕𝜆, exists. (Hint: Use Euler–Maclaurin lemma.)
14
Show that
Cv =
v−1
∑
i=1
1
i(v −i) = 1
v
v−1
∑
i=1
(1
i +
1
v −i
)
≤2(ln v + 1)
v
.
15
Show
a) limp→0 p ln p = 0; and
b) limp→0 pln2p = 0.
16
Show that for any p ∈(0, 1) and any nonnegative integer j less or equal to
n(1 −p),
0 ≤1 −j∕[n(1 −p)]
1 −j∕n
≤1.
17
Show that Z1,v in (3.26) may be re-expressed as
Z1,v =
∑
k≥1
[
̂pk
v
∏
j=1
(
1 −Yk −1
n −j
) ]
.
18
Show that
∑
k≥1
ln ln(k + 1)
(k + 1)ln2(k + 1)
< ∞.
19
Let X be a discrete random variables deﬁned on the set of natural
numbers ℕ, with probability distribution {px; x ∈ℕ}. Show that if
E(X𝜆) < ∞for some constant 𝜆> 0, then the entropy of X exists, that is,
H(X) = −∑∞
x=1 px ln px < ∞.
20
Let X1 and X2 be two random elements on the same alphabet
𝒳= {𝓁1, 𝓁2}
with
probability
distributions
p1 = {p1, 1 −p1}
and
p2 = {p2, 1 −p2}, respectively. Let 𝜃be a bivariate random variable,

Estimation of Shannon’s Entropy
123
independent of X1 and X2, such that P(𝜃= 1) = q and P(𝜃= 2) = 1 −q.
Let
X𝜃= X11[𝜃= 1] + X21[𝜃= 2].
Show that
H(X𝜃) ≥qH(X1) + (1 −q)H(X2),
with equality if and only if H(𝜃) = 0.

125
4
Estimation of Diversity Indices
Diversity is a general concept pertaining to the nature of assortment in a
population with multiple species. Originated in ecology where the diversity
of species in an ecosystem is of great interest, the concept of diversity has
become increasingly relevant in many other ﬁelds of study in modern sciences,
for example, of genetic diversity within a biological species, of word diversity
of an author, of diversity of an investment portfolio, and so on. More generally,
in information science, one is interested in the diversity of letters of some
alphabet. While the meaning of the word, diversity, is quite clear, it is not
always obvious how it may be committed to a quantitative measure. As a
central theme in ecology, a large volume of research can be found on how a
diversity index should be deﬁned and how it could be estimated. Proposed
diversity indices and their estimators are quite numerous. Shannon’s entropy
introduced in Shannon (1948) and Simpson’s index introduced in Simpson
(1949) are among the earliest diversity indices found in the literature. A set of
popular diversity indices include Emlen’s index, the Gini–Simpson index, Hill’s
diversity number, Rényi’s entropy, and Tsallis entropy. These are, respectively,
named after the authors of Emlen (1973), Gini (1912), Hill (1973), Rényi’s
(1961), and Tsallis (1988). For a comprehensive and in-depth introduction
of the subject, readers may wish to refer to Krebs (1999), Magurran (2004),
and Marcon (2014). In summary, while the ideas of what constitute diversity
indices and how to estimate them are quite diverse, the implied consensus
in the current literature seems to be that diversity is a multifaceted concept
and therefore no single mathematical index should be expected to capture it
entirely.
This chapter describes a uniﬁed perspective for all of the above-mentioned
diversity indices based on a re-parameterization and a general nonparametric
estimation procedure for these indices.
Statistical Implications of Turing’s Formula, First Edition. Zhiyi Zhang.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

126
Statistical Implications of Turing’s Formula
4.1
A Uniﬁed Perspective on Diversity Indices
Consider a population with countably many species, 𝒳= {𝓁k; k ≥1},
where each letter 𝓁k stands for one distinct species. Let pk = P(X = 𝓁k) and
p = {pk; k ≥1} where X is a random element drawn from the population.
Denote the cardinality of 𝒳by K = ∑
k≥11[pk > 0].
Let a general index of diversity be deﬁned as a function 𝜃= 𝜃(𝒳, p), which
assigns a real value to every given pair {𝒳, p}. Axiom 4.1 is commonly accepted
as minimal constraints on a diversity index 𝜃:
Axiom 4.1
A diversity index 𝜃= 𝜃(𝒳, p) satisﬁes
01∶𝜃= 𝜃(𝒳, p) = 𝜃(p) and
02∶𝜃(p) = 𝜃(p∗),
where p∗is any rearrangement of p.
Axiom 01 implies that the value of a diversity index only depends on the
probability distribution on the alphabet and not on the alphabet 𝒳itself. In
other words, a diversity index is determined only by the species proportions in
the population regardless of what the species are.
Axiom A02 implies that a diversity index 𝜃does not alter its value if any two
indices, k1 and k2, exchange their integer values, that is to say, 𝜃assumes a
value regardless of the order in which the species are arranged in the popula-
tion. Axiom A02 was ﬁrst formally introduced by Rényi (1961) as the symmetry
axiom, and it was also known as the permutation invariant property of diversity
indices.
Many more reasonable axiomatic conditions have been discussed in the
diversity literature, but Axion 4.1 encircles the realm of discussion in this
chapter. The following is a list of several commonly used diversity indices
satisfying Axion 4.1.
Example 4.1
Simpson’s index 𝜆= ∑
k≥1p2
k.
Example 4.2
The Gini–Simpson index 1 −𝜆= ∑
k≥1pk(1 −pk).
The Gini–Simpson index is one of the earliest diversity indices discussed in
the literature. It is popular for many reasons. In addition to its simplicity, it
has a probabilistic interpretation with regard to the intuitive notion of diver-
sity. Consider two independently and identically distributed random elements
from 𝒳, say X1 and X2, under a probability distribution, {pk; k ≥1}. A more
(or less) diverse population could be partially characterized by a higher (or a

Estimation of Diversity Indices
127
lower) value of P(X1 ≠X2), that is, the probability that X1 and X2 are not equal.
However,
P(X1 ≠X2) = 1 −P(X1 = X2) = 1 −
∑
k≥1
P(X1 = X2 = 𝓁k)
= 1 −
∑
k≥1
p2
k = 1 −𝜆.
Example 4.3
Shannon’s entropy H = −∑
k≥1 pk ln pk.
Example 4.4
Rényi’s entropy H𝛼= (1 −𝛼)−1 ln (∑
k≥1 p𝛼
k
) for any 𝛼> 0,
𝛼≠1.
Example 4.5
Tsallis’ entropy T𝛼= (1 −𝛼)−1 (∑
k≥1 p𝛼
k −1) for any 𝛼> 0,
𝛼≠1.
Example 4.6
Hill’s diversity number N𝛼= (∑
k≥1 p𝛼
k
)1∕(1−𝛼) for any 𝛼> 0,
𝛼≠1.
Example 4.7
Emlen’s index D = ∑
kpke−pk.
Example 4.8
The richness index K = ∑
k≥11[pk > 0].
To better facilitate an investigation into the commonalities of these diversity
indices, a notion of equivalence between two diversity indices was introduced
by Zhang and Grabchak (2016) and is given in Deﬁnition 4.1 below.
Deﬁnition 4.1
Two diversity indices, 𝜃1 and 𝜃2, are said to be equivalent if
there exists a strictly increasing continuous function g(⋅) such that 𝜃1 = g(𝜃2).
The equivalence is denoted by 𝜃1 ⇔𝜃2.
Consider two populations, PA and PB, in diversity comparison. Suppose PA
is more diverse than PB by means of an index 𝜃2, that is, 𝜃2(PA) > 𝜃2(PB). Then
since g(⋅) is strictly increasing,
𝜃1(PA) = g(𝜃2(PA)) > g(𝜃2(PB)) = 𝜃1(PB).
The reverse of the above-mentioned argument is also true because the inverse
function of g, g−1, exists and it is also strictly increasing (see Exercise 3). This
argument suggests that the equivalence of Deﬁnition 4.1 is an order-preserving
relationship. That said however, it is to be noted that this relationship does not
preserve the incremental diﬀerence in diversity between PA and PB, that is, it is
not guaranteed that 𝜃1(PA) −𝜃1(PB) = 𝜃2(PA) −𝜃2(PB).

128
Statistical Implications of Turing’s Formula
Example 4.9
Rényi’s entropy H𝛼, Tsallis’ entropy T𝛼, and Hill’s diversity num-
ber N𝛼are equivalent to each other for 𝛼∈(0, 1). To verify this claim, it suﬃces
(why?) to show that all three indices are equivalent to a common core index
h𝛼=
∑
k≥1
p𝛼
k,
(4.1)
where 𝛼is a constant satisfying 𝛼∈(0, 1). Toward that end, consider the follow-
ing three continuous functions as the function g(x) in Deﬁnition 4.1 for Rényi’s,
Tsallis’, and Hill’s indices, respectively,
g1(x) =
ln x
(1 −𝛼), g2(x) = x −1
(1 −𝛼), and g3(x) = x1∕(1−𝛼),
where 𝛼is a ﬁxed constant in (0, 1). By the fact that each of these functions is con-
tinuous and strictly increasing for x ∈(0, ∞) and by Deﬁnition 4.1, the desired
result follows.
Recall, in Chapter 2, a special subclass of the generalized Simpson’s indices
is denoted as
𝜻1 =
{
𝜁v =
∑
k≥1
pk(1 −pk)v; v ≥0
}
.
(4.2)
Also recall, in Chapter 3, a ﬁnite Shannon’s entropy has a representation
H =
∑
v≥1
1
v 𝜁v,
which is a linear combination of all the elements in 𝜻1. Finally, recall
Theorem 2.2, which states that a distribution {pk; k ≥1} on 𝒳and its asso-
ciated panel of generalized Simpson’s indices {𝜁v; v ≥1} uniquely determine
each other up to a permutation on the index set {k; k ≥}.
In light of the above-mentioned recalled facts, 𝜻∶= 𝜻1 = {𝜁v; v ≥1} deserves
a name for future reference.
Deﬁnition 4.2
For every probability sequence {pk; k ≥1} on 𝒳, 𝜻= {𝜁v; v ≥
1} is said to be the entropic basis of p = {pk; k ≥1}.
The following theorem provides a unifying perspective on all diversity indices
satisfying Axioms 4.1.
Theorem 4.1
Given an alphabet 𝒳= {𝓁k; k ≥1} and an associated proba-
bility distribution p = {pk; k ≥1}, a diversity index 𝜃satisfying Axioms 4.1 is a
function of the entropic basis 𝜁= {𝜁v; v ≥0}.
Proof: By Theorem 2.2, 𝜻determines p up to a permutation and, in turn, deter-
mines any diversity index 𝜃satisfying Axioms 4.1.
◽

Estimation of Diversity Indices
129
The statement of Theorem 4.1 holds for any probability distribution
p regardless of whether K is ﬁnite or inﬁnite. Theorem 4.1 essentially
oﬀers a re-parameterization of p (up to a permutation) in terms of 𝜁. This
re-parameterization is not just an arbitrary one, it has several statistical
implications. First of all, every element of 𝜁contains information about the
entire distribution and not just one frequency pk. This helps to deal with
the problem of estimating probabilities of unobserved species. Second, for a
random sample of size n, there are good estimators of 𝜁v for v = 1, 2,…, n −1.
These estimators are introduced in Chapter 2 and are brieﬂy recalled in the
following section.
While a general diversity index can be any function of the entropic basis,
most commonly used diversity indices in practice are either linear functions or
equivalent to linear functions of the entropic basis.
Deﬁnition 4.3
A diversity index 𝜃is said to be a linear diversity index if it is
a linear combination of the members of the entropic basis, that is,
𝜃= 𝜃(p) =
∞
∑
v=0
wv𝜁v =
∞
∑
v=0
wv
∑
k≥1
pk(1 −pk)v
(4.3)
where wv is a function of v such that ∑n
v=0 wv𝜁v < ∞.
Deﬁnition 4.3 identiﬁes a subclass of indices among all functions of 𝜁, that
is, all diversity indices satisfying Axioms 4.1. Every member of this subclass is
a weighted linear form of {𝜁v}. While there are no fundamental reasons why
a search of a good diversity index should be restricted to this subclass, it hap-
pens to cover all of the popular indices in the literature, up to the equivalence
relationship given in Deﬁnition 4.1. The following examples demonstrate the
linearity of several popular diversity indices.
Example 4.10
Simpson’s index.
𝜆=
∑
k≥1
p2
k = 𝜁0 −𝜁1.
(See Exercise 6.)
Example 4.11
The Gini–Simpson index.
1 −𝜆=
∑
k≥1
pk(1 −pk) = 𝜁1.
Example 4.12
Shannon’s entropy.
H = −
∑
k≥1
pk ln pk =
∞
∑
v=1
1
v 𝜁v,
as shown in Chapter 3.

130
Statistical Implications of Turing’s Formula
Example 4.13
The index in (4.1),
h𝛼=
∑
k≥1
p𝛼
k = 𝜁0 +
∞
∑
v=1
[ v
∏
i=1
(i −𝛼
i
)
𝜁v
]
,
where 𝛼> 0 and 𝛼≠1. (See Exercise 7.)
Example 4.14
Emlen’s index.
D =
∑
k≥1
pke−pk =
∞
∑
v=0
e−1
v!
𝜁v
(see Exercise 8).
Example 4.15
The richness index.
K =
∑
k≥1
1[pk > 0]
=
∑
k≥1
1[pk > 0]
pk
1 −(1 −pk)
=
∑
k≥1
1[pk > 0]pk
∞
∑
v=0
(1 −pk)v
=
∞
∑
v=0
∑
k≥1
1[pk > 0]pk(1 −pk)v =
∞
∑
v=0
𝜁v.
The richness indices, K, and the evenness indices, for example, Gini–
Simpson’s 1 −𝜆, are generally thought as two qualitatively very diﬀerent types
of indices. Many arguments for such a diﬀerence can be found in the existing
literature (see, for example, Peet (1974); Heip, Herman, and Soetaert (1998),
and Purvis and Hector (2000)). However, Example 4.15 demonstrates that, in
the perspective of entropic basis, they both are linear diversity indices and
merely diﬀer in the weighting scheme wv in Deﬁnition 4.3.
Example 4.16
The generalized Simpson’s indices.
𝜁u,m =
∑
k≥1
pu
k(1 −pk)m
=
∑
k≥1
pk[1 −(1 −pk)]u−1(1 −pk)m
=
∑
k≥1
pk
u−1
∑
v=0
(−1)u−1−v (u −1
v
)
(1 −pk)u−1−v(1 −pk)m
=
u−1
∑
v=0
(−1)v (u −1
v
)
𝜁m+v.

Estimation of Diversity Indices
131
Linear diversity indices may also be derived from a general form of
𝜃=
∑
k≥1
pkh(pk)
(4.4)
where h(p) is an analytic function on an open interval containing (0, 1]. More
speciﬁcally, (4.4) may be re-expressed as, provided that 0 ≤𝜃< ∞,
𝜃=
∑
k≥1
∑
v≥0
h(v)(1)
v!
pk(pk −1)v
=
∑
k≥1
∑
v≥0
(−1)vh(v)(1)
v!
pk(1 −pk)v
=
∑
v≥0
(−1)vh(v)(1)
v!
𝜁v
(4.5)
where h(v)(1) is the vth derivative of h(p) evaluated at p = 1. The exchange of the
two summations in the above-mentioned expression is supported by Fubini’s
lemma, assuming 𝜃is ﬁnite. (4.5) conforms with (4.3).
4.2
Estimation of Linear Diversity Indices
In this section, two general approaches to nonparametric statistical estimation
of linear diversity indices are described. Let X1, X2,…, Xn be independent and
identically distributed (iid) random observations from 𝒳according to p. Let
{Yk = ∑n
i=1 1[Xi = 𝓁k]} be the sequence of observed counts in an iid sample of
size n and let ̂p = {̂pk = Yk∕n} be the observed sample proportions. Suppose
it is of interest to estimate 𝜃of (4.3) where {wv; v ≥0} is pre-chosen but p =
{pk; k ≥1} is unknown.
Replacing each pk in (4.3) with ̂pk, the resulting estimator,
̂𝜃=
∑
v≥0
wv
∑
k≥1
̂pk(1 −̂pk)v,
(4.6)
is often referred to as the plug-in estimator. The plug-in estimator is an intu-
itive estimator and has many desirable properties asymptotically. However, it is
known to have a slowly decaying bias, particularly when the sample size n is rel-
atively small and the alphabet 𝒳is relatively large. For this reason, the plug-in
estimator is not always the ﬁrst one chosen in practice. Nevertheless, since its
statistical properties are better known than those of other estimators, it is often
used as a reference estimator.
On the other hand, by Z1,v in (3.26) and the fact E(Z1,v) = 𝜁v for every v sat-
isfying 1 ≤v ≤n −1, the following estimator in Turing’s perspective is readily
available,
̂𝜃♯=
n−1
∑
v=0
wvZ1,v,
(4.7)
and is the main object of this chapter.

132
Statistical Implications of Turing’s Formula
Several statistical properties of ̂𝜃♯may be described under the following con-
dition.
Condition 4.1
A probability distribution p = {pk; k ≥1} and an associated
linear diversity index in the form of (4.3) are such that
1) the eﬀective cardinality of 𝒳is ﬁnite, that is, K = ∑
k≥11[pk > 0] < ∞; and
2) the weights, wv, for all v ≥0, are bounded, that is, there exists an M > 0 such
that |wv| ≤M for all v ≥0.
Condition 4.1 guarantees that the summation in (4.3) always converges (see
Exercise 10). It can be veriﬁed that the assumption that |wn| ≤M is satisﬁed by
all of the linear diversity indices discussed in Examples 4.10 through 4.16 (see
Exercise 11). Condition 4.1 also guarantees that estimator ̂𝜃♯has a bias that
decays exponentially fast in sample size n.
Theorem 4.2
Under Condition 4.1,
|E( ̂𝜃♯) −𝜃| ≤MK(1 −p∧)n
(4.8)
where p∧= min{pk,…, pK}.
Proof: The U-statistics construction of Z1,v in Chapter 3 establishes the fact
that E(Z1,v) = 𝜁v for v satisfying v = 1,…, n −1. Therefore,
|E( ̂𝜃♯) −𝜃| =
||||||
n−1
∑
v=1
wvE(Z1,v) −
∞
∑
v=0
wv𝜁v
||||||
=
|||||
∞
∑
v=n
wv𝜁v
|||||
≤|wv|
∞
∑
v=n
K
∑
k=1
pk(1 −pk)v = |wv|
K
∑
k=1
(1 −pk)n
≤MK(1 −p∧)n.
◽
Remark 4.1
While decaying rapidly, the bias of ̂𝜃♯could still be sizable when
M, K are relatively large and n and p∧are relatively small. Practitioners often
ﬁnd bias reduction desirable in many realistic situations. In the existing litera-
ture, there are many proposed ways to reduce bias and to enhance accuracy in
estimation of diversity indices. For a recent update of this research space, readers
may wish to refer to Chao and Jost (2012) and Chao and Jost (2015). Zhang and
Grabchak (2013) proposed a bias reduction methodology that is particularly
suitable for general estimators in the form of (4.7).
As in any sound statistical practice, reliability of estimated diversity indices
must be supported by distributional characteristics of the estimators. Toward
that end, the asymptotic normality of ̂𝜃♯is established in the following. The

Estimation of Diversity Indices
133
approach here consists of two parts. The ﬁrst part is to establish asymptotic
normality of the plug-in estimator ̂𝜃in (4.6), and then the second part is to show
that ̂𝜃♯and ̂𝜃are suﬃciently close to warrant the same asymptotic distributional
behavior.
Let
v = (p1, p2,…, pK−1)𝜏
and
̂v = (̂p1, ̂p2,…, ̂pK−1)𝜏.
(4.9)
Lemma 4.1 is a well-known multivariate normal approximation to the multi-
nomial distribution (see Exercise 17).
Lemma 4.1
Suppose the probability distribution p = {pk; k = 1,…, K} is
such that pk > 0 for each k:
√
n(̂v −v)
L
−−−→MVN(∅, Σ(v)),
(4.10)
where ∅is a (K −1)-dimensional zero vector and Σ(v) is the (K −1) × (K −1)
covariance matrix given by
Σ(v) =
⎛
⎜
⎜
⎜⎝
p1(1 −p1)
−p1p2
· · ·
−p1pK−1
−p2p1
p2(1 −p2) · · ·
−p2pK−1
⋮
⋮
· · ·
⋮
−pK−1p1
−pK−1p2 · · · pK−1(1 −pK−1)
⎞
⎟
⎟
⎟⎠
.
(4.11)
Let h(p) = ∑
v≥0wv(1 −p)v, write the index 𝜃of (4.3) as a function of
(p1,…, pK−1), and denote the resulting function as
G(v) =
K
∑
k=1
pkh(pk) =
K−1
∑
k=1
pkh(pk) +
(
1 −
K−1
∑
k=1
pk
)
h
(
1 −
K−1
∑
k=1
pk
)
.
Let the gradient of G(v) be denoted as
g(v) = ∇G(v) =
(
∂
∂p1
G(v),…,
∂
∂pK−1
G(v)
)𝜏
=∶(g1(v),…, gK−1(v))𝜏.
(4.12)
For each j, j = 1,…, K −1,
gj(v) = ∂
∂pj
G(v) = h(pj) + pjh′(pj) −h
(
1 −
K−1
∑
k=1
pk
)
−
(
1 −
K−1
∑
k=1
pk
)
h′
(
1 −
K−1
∑
k=1
pk
)
=
∑
v≥0
wv(1 −pj)v −pj
∑
v≥0
wvv(1 −pj)v−1
−
∑
v≥0
wv(1 −pK)v + pK
∑
v≥0
wvv(1 −pK)v−1

134
Statistical Implications of Turing’s Formula
=
∑
v≥0
wv[(1 −pj)v −(1 −pK)v]
−
∑
v≥0
wvv[pj(1 −pj)v−1 −pK(1 −pK)v−1].
(4.13)
An application of the delta method gives the following result.
Theorem 4.3
Suppose Condition 4.1 holds and g𝜏(v)Σ(v)g(v) > 0, then
√
n( ̂𝜃−𝜃)(g𝜏(v)Σ(v)g(v))−1
2
L
−−−→N(0, 1).
(4.14)
Tanabe and Sagae (1992) showed that, provided that K ≥2 and that pk > 0 for
each and every k, k = 1,…, K, the covariance matrix Σ(v) is a positive deﬁnite
matrix. Therefore, g𝜏(v)Σ(v)g(v) > 0 if and only if g(v) ≠0. However, for some
combinations of h(p) and p, g(v) = 0 and the condition of Theorem 4.3 does
not hold.
Example 4.17
Shannon’s entropy. Let 𝜃= ∑K
k=1 pk ln (1∕pk). Then g𝜏(v)Σ(v)
g(v) = 0 if and only if p = {pk; k = 1,…, K} is uniform.
Proof: In this case, h(p) = −ln p. For each j, j = 1,…, K −1,
gj(v) = ∂
∂pj
𝜃(p) = ∂
∂pj
K
∑
k=1
pkh(pk)
= −ln (pj) −1 + ln pK + 1 = ln (pK∕pj).
If pj = 1∕K for every j, then gj(v) = 0. On the other hand, if ln (pK∕pj) = 0 for
each and every j, then pj = pK = 1∕K.
◽
Example 4.18
Emlen’s index. Let 𝜃= ∑K
k=1 pkepk. Then g𝜏(v)Σ(v)g(v) = 0 if
and only if p = {pk; k = 1,…, K} is uniform.
Proof: In this case, h(p) = e−p. For each j, j = 1,…, K −1,
gj(v) = ∂
∂pj
𝜃(p) = e−pj(1 −pj) −e−pK(1 −pK).
If pj = 1∕K for every j, then gj(v) = 0. Suppose for each and every j gj(v) = 0,
then
e−pj(1 −pj) = e−pK(1 −pK).
(4.15)
Noting f (p) = e−p(1 −p) is a strictly monotone function on interval (0, 1),
(4.15) implies pj = pK for every j (see Exercise 19).
◽

Estimation of Diversity Indices
135
As evidenced by the last expression of (4.3), g(v) is a continuous function of
v for v ∈(0, 1]K−1. The said continuity and the fact ̂v
p→v imply that g(̂v)
p→g(v)
by the continuous mapping theorem of Mann and Wald (1943). The following
corollary follows immediately from Slutsky’s theorem.
Corollary 4.1
Under the condition of Theorem 4.3,
√
n( ̂𝜃−𝜃)(g𝜏(̂v)Σ(̂v)g(̂v))−1
2
L
−−−→N(0, 1).
Corollary 4.1 provides a means of statistical inference. It is to be noted
however that in practice it is often the case that ̂pk = 0 for some index values
of k, that is, the letters in the alphabet that are not covered by a sample of size
n. For simplicity, suppose ̂pK > 0 (if not, a switch of two indices will suﬃce).
Two issues exist in such a situation. First, noting gj(v) for each j is only deﬁned
for pj > 0, some components of g(̂v) = (g1(̂v),…, gK−1(̂v))𝜏may not be well
deﬁned if some components of ̂v are zeros, say gj(̂v). Estimating gj(v) by a
not well-deﬁned statistic gj(̂v) is problematic. This issue can be resolved by
setting gj(̂v) = 0, or any ﬁxed value for that matter, since gj(̂v) converges to the
true value of gj(v) and therefore is bounded away from the deﬁned value in
probability. Second, if ̂pk = 0 for some index values of k, though pk > 0 and
therefore Σ(v) is positive deﬁnite, Σ(̂v) is not. This raises the question on the
positivity of g𝜏(̂v)Σ(̂v)g(̂v). However, it may be shown that g𝜏(̂v)Σ(̂v)g(̂v), after
redeﬁning gj(̂v) for all ̂pj = 0, is positive (see Exercise 18).
Another interesting feature of Corollary 4.1 is that it does not distinguish a
species that does not exist in the population, that is, pk = 0, from a species that
does exist but is not observed in the sample, that is, pk > 0 but ̂pk = 0. This is
so because both Σ(̂v) and the redeﬁned g(̂v) have zeros in locations that would
make the diﬀerence inconsequential in computation. For this reason, Corollary
4.1 may be used regardless of whether K is known a priori.
The asymptotic normal laws established in Theorem 4.3 and Corollary 4.1
for the plug-in estimator may be extended to that for the estimator of Turing’s
perspective, deﬁned in (4.7).
Theorem 4.4
Let
̂𝜃♯be as in (4.7). Suppose Condition 4.1 holds and
g𝜏(v)Σ(v)g(v) > 0. Then
√
n ( ̂𝜃♯−𝜃) (g𝜏(v)Σ(v)g(v))−1
2
L
−−−→N(0, 1).
Corollary 4.2
Let ̂𝜃♯be as in (4.7). Suppose Condition 4.1 holds and
g𝜏(v)Σ(v)g(v) > 0. Then
√
n ( ̂𝜃♯−𝜃) (g𝜏(̂v)Σ(̂v)g(̂v))−1
2
L
−−−→N(0, 1).
(4.16)

136
Statistical Implications of Turing’s Formula
To prove Theorem 4.4, two lemmas, Lemmas 4.2 and 4.3, are needed. Both
lemmas are stated in the following without proofs. A proof of Lemma 4.2 can be
found in Zhang and Grabchak (2016), and a proof of Lemma 4.3 can be found
in Hoeﬀding (1963).
For p ∈[0, 1] and n ∈ℕ= {1, 2, … }, consider the following two functions:
fn(p) = p
⌊n(1−p)+1⌋
∑
v=1
wv
v
∏
j=1
(
1 −np −1
n −j
)
and
f (p) = p
∞
∑
v=1
wv(1 −p)v.
Lemma 4.2
Suppose Condition 4.1 holds.
1) If 0 < c < d < 1, then
lim
n→∞sup
p∈[c,d]
√
n|fn(p) −f (p)| = 0.
2) Let pn ∈[0, 1] be such that npn ∈{0, 1, 2,…, n}, then
√
n|fn(pn) −f (pn)| ≤
√
n(n + 1) ≤2n3∕2.
Lemma 4.3
(Hoeﬀding’s inequality)
Let Y be a binomial random variable
with parameters n and p > 0 and ̂p = Y∕n. For any ε > 0,
1) P(̂p −p ≤−ε) ≤e−2ε2n and
2) P(̂p −p ≥ε) ≤e−2ε2n.
Recall the result of Exercise 17 of Chapter 3 that Z1,v in (3.26) may be
re-expressed as
Z1,v =
∑
k≥1
[
̂pk
v
∏
j=1
(
1 −Yk −1
n −j
)]
.
(4.17)
As a result of (4.17), ̂𝜃♯in (4.7) may be re-expressed as
̂𝜃♯= w0 +
n−1
∑
v=1
wv
K
∑
k=1
̂pk
v
∏
j=1
(
1 −Yk −1
n −j
)
(4.18)
= w0 +
K
∑
k=1
̂pk
n−Yk
∑
v=1
wv
v
∏
j=1
(
1 −Yk −1
n −j
)
(4.19)
= w0 +
K
∑
k=1
̂𝜃♯
k,
(4.20)

Estimation of Diversity Indices
137
where
̂𝜃♯
k = ̂pk
n−Yk
∑
v=1
wv
v
∏
j=1
(
1 −Yk −1
n −j
)
= ̂pk
∞
∑
v=1
wv
v
∏
j=1
(
1 −Yk −1
n −j
)
. (4.21)
Also recall from (4.6)
̂𝜃= w0 +
K
∑
k=1
̂𝜃k,
(4.22)
where ̂𝜃k = ̂pk
∑∞
v=1 wv(1 −̂pk)v.
Proof of Theorem 4.4: Since
√
n( ̂𝜃♯−𝜃) =
√
n( ̂𝜃♯−̂𝜃) +
√
n( ̂𝜃−𝜃),
by Theorem 4.3 and Slutsky’s theorem it suﬃces to show that
√
n( ̂𝜃♯−̂𝜃)
p→0.
However, from (4.20) and (4.22),
√
n( ̂𝜃♯−̂𝜃) =
K
∑
k=1
√
n( ̂𝜃♯
k −̂𝜃k),
and therefore it suﬃces to show that, for each k,
√
n( ̂𝜃♯
k −̂𝜃k)
p→0.
Toward that end,
√
n( ̂𝜃♯
k −̂𝜃k) =
{√
n
(
̂𝜃♯
k −̂𝜃k
)
1[̂pk ≤pk∕2]
+
√
n
(
̂𝜃♯
k −̂𝜃k
)
1[̂pk ≥(1 + pk)∕2]
}
+
{√
n
(
̂𝜃♯
k −̂𝜃k
)
1[pk∕2 < ̂pk < (1 + pk)∕2]
}
= ∶{1} + {2}.
By Part 2 of Lemmas 4.2 and 4.3,
E|1| ≤2n3∕2
(
P
(
̂pk > 1 + pk
2
)
+ P
(
̂pk ≤pk
2
))
= 2n3∕2
(
P
(
̂pk −pk
1 −pk
2
)
+ P
(
̂pk −pk ≤−pk
2
))
≤2n3∕2[e−n(1−pk)2∕2 + e−np2
k∕2] →0.
Thus, it follows that 1
p→0. By Part 1 of Lemma 4.2, it follows that
2
p→0.
◽
Finally, the same justiﬁcation for Corollary 4.1 also applies to Corollary 4.2.

138
Statistical Implications of Turing’s Formula
4.3
Estimation of Rényi’s Entropy
In the last section, a general nonparametric estimation approach to linear
diversity indices is introduced. To extend the derived results further to
diversity indices that are not linear but only equivalent to a linear kernel,
𝜃LK = ∑∞
v=0 wv 𝜁v, consider 𝜃= 𝜓(𝜃LK) where 𝜓(⋅) is a function diﬀerentiable
at 𝜃LK. Let 𝜓′(⋅) be the ﬁrst derivative of 𝜓(⋅) and
̃𝜃♯= 𝜓( ̂𝜃♯)
(4.23)
where ̂𝜃♯, as in (4.7), is an estimator of 𝜃= 𝜓(𝜃LK). An application of the delta
method immediately gives the following theorem and corollary.
Theorem 4.5
Suppose Condition 4.1 holds and g𝜏(v)Σ(v)g(v) > 0. Then
√
n (𝜓( ̂𝜃♯) −𝜃) (g𝜏(v)Σ(v)g(v)𝜓′(𝜃LK))−1
2
L
−−−→N(0, 1).
Corollary 4.3
Suppose Condition 4.1 holds and g𝜏(v)Σ(v)g(v) > 0. Then
√
n (𝜓( ̂𝜃♯) −𝜃) (g𝜏(̂v)Σ(̂v)g(̂v)𝜓′( ̂𝜃♯))−1
2
L
−−−→N(0, 1).
Furthermore, if |𝜓′(t)| < D for some D > 0 on the interval of t between 𝜃LK
and inf∗{ ̂𝜃♯} and the interval between 𝜃LK and sup∗{ ̂𝜃♯}, where both inf∗and
sup∗are taken over all possible samples of all sizes, then the bias of 𝜓( ̂𝜃♯) decays
at least as fast as the bias of ̂𝜃♯, which is exponential in n under Condition 4.1.
To see this, it suﬃces to consider the ﬁrst-order Taylor expansion for 𝜓( ̂𝜃♯):
𝜓( ̂𝜃♯) −𝜓(𝜃LK) = 𝜓′(𝜉)( ̂𝜃♯−𝜃LK)
(4.24)
where the random variable 𝜉is between 𝜃LK and ̂𝜃♯∈(inf∗{ ̂𝜃♯}, sup∗{ ̂𝜃♯}).
Taking expectation on both sides of (4.24) gives
|||E(𝜓( ̂𝜃♯) −𝜓(𝜃LK))||| ≤D |||E( ̂𝜃♯−𝜃LK)||| ≤DMK(1 −p∧)n
(4.25)
where the last inequality is due to (4.8).
Rényi’s entropy is one of the most frequently used diversity indices by ecolo-
gists, and it is one of the two diversity indices among the many discussed in this
chapter that are not linear in entropic basis, the other one being Hill’s diversity
number that is a logarithmic transformation of Rényi’s entropy. Theorem 4.5
and Corollary 4.3 may be applied to Rényi’s entropy.
For a ﬁxed 𝛼> 0 such that 𝛼≠1, let 𝜓(t) = (1 −𝛼)−1 ln t. Note that
H𝛼= 𝜓(h𝛼) = ln h𝛼
1 −𝛼,
where H𝛼is Rényi’s entropy and h𝛼= ∑K
k=1 p𝛼
k is Rényi’s equivalent entropy, as
in Example 4.13. Let

Estimation of Diversity Indices
139
̂h♯
𝛼= 1 +
K
∑
k=1
̂pk
n−Yk
∑
v=1
wv
v
∏
j=1
(
1 −Yk −1
n −j
)
,
(4.26)
where for v ≥1
wv =
v
∏
i=1
(i −𝛼
i
)
.
̂h♯
𝛼is the estimator of h𝛼in the form of (4.7). Let
̂H♯
𝛼= ln ̂h♯
𝛼
1 −𝛼= 𝜓(̂h♯
𝛼).
Since
𝜓′(t) =
1
(1 −𝛼)t ,
(4.27)
Theorem 4.5 and the fact that h𝛼> 0 imply the following.
Theorem 4.6
Provided that g𝜏(v)Σ(v)g(v) > 0,
√
n
(
̂H♯
𝛼−H𝛼
)
|1 −𝛼|h𝛼[g𝜏(v)Σ(v)g(v)]−1
2
L
−−−→N(0, 1).
Corollary 4.4
Provided that g𝜏(v)Σ(v)g(v) > 0,
√
n( ̂H♯
𝛼−H𝛼)|1 −𝛼|̂h♯
𝛼[g𝜏(̂v)Σ(̂v)g(̂v)]−1
2
L
−−−→N(0, 1).
Next it is to establish that when 𝛼∈(0, 1), the bias of ̂H♯
𝛼decays exponentially
in the sample size n. This fact follows the following three observations.
1) First, observe that the linear kernel of Rényi’s entropy, or Rényi’s equivalent
index, as in Example 4.13, satisﬁes
𝜃LK = h𝛼=
∑
k≥1
p𝛼
k > 0
for any 𝛼> 0. In particular, for 𝛼∈(0, 1),
𝜃LK = h𝛼=
∑
k≥1
p𝛼
k ≥1.
2) Second, consider ̂h♯
𝛼of (4.26). Since 𝛼∈(0, 1), (i −𝛼) > 0 for all i = 1, … ,
wv > 0, and hence ̂𝜃♯= ̂h♯
𝛼≥1, one has
m = min
{
h𝛼, inf
∗
{
̂h♯
𝛼
}}
≥1.
3) Third, since 𝜓′(t) of (4.27) satisﬁes
𝜓′(t) =
1
(1 −𝛼)t ≤
1
(1 −𝛼)m =∶D
for t ∈[m, +∞), Equation (4.25) holds. Therefore, the bias of ̂H♯
𝛼decays
exponentially in n.

140
Statistical Implications of Turing’s Formula
Finally, a large-sample statistical testing procedure for a diﬀerence in the
diversity measured by Rényi’s entropy between two populations (possibly
deﬁned by two diﬀerent locations or two diﬀerent time points) is described in
the following.
Suppose two independent iid samples of sizes n1 and n2 are to be taken from a
same alphabet 𝒳under two diﬀerent distributions: {p1,k; k ≥1} and {p2,k; k ≥
1}. The samples are summarized, respectively, by two independent sets of the
observed frequencies
Y1 = {Y1,1, Y1,2,…, Y1,k, … }
Y2 = {Y2,1, Y2,2,…, Y2,k, … }
and their corresponding observed relative frequencies
̂p1 = {̂p1,1, ̂p1,2,…, ̂p1,k, … } = {Y1,1∕n1, Y1,2∕n1,…, Y1,k∕n1, … }
̂p2 = {̂p2,1, ̂p2,2,…, ̂p2,k, … } = {Y2,1∕n2, Y2,2∕n2,…, Y2,k∕n2, … }.
For a ﬁxed 𝛼> 0 and 𝛼≠1, denote Rényi’s entropies for the two populations
by H𝛼(1) and H𝛼(2). Suppose it is of interest to test the hypothesis
H0 ∶H𝛼(1) = H𝛼(2)
versus
Ha ∶H𝛼(1) ≠H𝛼(2)
at a level of signiﬁcance 𝛽∈(0, 1).
The testing procedure may be carried out in the following 10 steps.
1) To calculate, using (4.26),
̂h♯
𝛼(1) = 1 +
K
∑
k=1
̂p1,k
n1−Y1,k
∑
v=1
wv
v
∏
j=1
(
1 −
Y1,k −1
n1 −j
)
and
̂h♯
𝛼(2) = 1 +
K
∑
k=1
̂p2,k
n2−Y2,k
∑
v=1
wv
v
∏
j=1
(
1 −
Y2,k −1
n2 −j
)
,
where
wv =
v
∏
i=1
(i −𝛼
i
)
.
2) To calculate
̂H♯
𝛼(1) = ln ̂h♯
𝛼(1)
1 −𝛼
and
̂H♯
𝛼(2) = ln ̂h♯
𝛼(2)
1 −𝛼.
3) To calculate vectors
g(̂v1) = (𝛼(̂p𝛼−1
1,1 −̂p𝛼−1
1,K
) ,…, 𝛼(̂p𝛼−1
1,K−1 −̂p𝛼−1
1,K
))𝜏and
g(̂v2) = (𝛼(̂p𝛼−1
2,1 −̂p𝛼−1
2,K
) ,…, 𝛼(̂p𝛼−1
2,K−1 −̂p𝛼−1
2,K
))𝜏.

Estimation of Diversity Indices
141
4) To make adjustment to g(̂v1) and g(̂v2) by setting the entries, when ̂pi,j = 0,
i = 1, 2, and j = 1, 2,…, K −1, to zero, that is,
g(̂v1) = (𝛼(̂p𝛼−1
1,1 −̂p𝛼−1
1,K )1[̂p1,1 > 0] ,
… , 𝛼(̂p𝛼−1
1,K−1 −̂p𝛼−1
1,K )1[̂p1,K−1 > 0])𝜏, and
g(̂v2) = (𝛼(̂p𝛼−1
2,1 −̂p𝛼−1
2,K )1[̂p2,1 > 0] ,
… , 𝛼(̂p𝛼−1
2,K−1 −̂p𝛼−1
2,K )1[̂p2,K−1 > 0])𝜏.
5) To calculate the two (K −1) × (K −1) covariance matrices
Σ(̂v1) =
⎛
⎜
⎜
⎜⎝
̂p1,1(1 −̂p1,1)
−̂p1,1 ̂p1,2
· · ·
−̂p1,1 ̂p1,K−1
−̂p1,2 ̂p1,1
̂p1,2(1 −̂p1,2) · · ·
−̂p1,2 ̂p1,K−1
⋮
⋮
· · ·
⋮
−̂p1,K−1 ̂p1,1
−̂p1,K−1 ̂p1,2 · · ·
̂p1,K−1(1 −̂p1,K−1)
⎞
⎟
⎟
⎟⎠
and
Σ(̂v2) =
⎛
⎜
⎜
⎜⎝
̂p2,1(1 −̂p2,1)
−̂p2,1 ̂p2,2
· · ·
−̂p2,1 ̂p2,K−1
−̂p2,2 ̂p2,1
̂p2,2(1 −̂p2,2) · · ·
−̂p2,2 ̂p2,K−1
⋮
⋮
· · ·
⋮
−̂p2,K−1 ̂p2,1
−̂p2,K−1 ̂p2,2 · · ·
̂p2,K−1(1 −̂p2,K−1)
⎞
⎟
⎟
⎟⎠
.
6) To calculate the two estimated variances
̂𝜎2
1 = (1 −𝛼)2(
̂h♯
𝛼(1)
)2
(g
𝜏(̂v1)Σ(̂v1)g(̂v1))
and
̂𝜎2
2 = (1 −𝛼)2(
̂h♯
𝛼(2)
)2
(g
𝜏(̂v2)Σ(̂v2)g(̂v2)).
7) To ﬁnd the estimated combined variance
̂𝜎2 = ̂𝜎2
1 + n1
n2
̂𝜎2
2.
8) To calculate the test statistic
Z𝛼=
√n1( ̂H♯
𝛼(1) −̂H♯
𝛼(2))
̂𝜎
.
(4.28)
9) To make a decision to
a) reject H0 if |Z𝛼| ≥z𝛽∕2, or
b) not to reject H0 if |Z𝛼| < z𝛽∕2
where z𝛽∕2 is the [100 × (1 −𝛽∕2)]th percentile of the standard normal dis-
tribution.
10) To calculate a [100 × (1 −𝛽)]% conﬁdence interval for the diﬀerence in
Rényi’s entropy,
(
̂H♯
𝛼(1) −̂H♯
𝛼(2)
)
± z𝛽∕2
̂𝜎
√n1
.
(4.29)

142
Statistical Implications of Turing’s Formula
Many ecologists like to compare Rényi’s diversity proﬁles of two populations
based on two samples. The estimated single-sample proﬁles are obtained by
̂H♯
𝛼(1) and ̂H♯
𝛼(2) as functions of 𝛼varying from a to b, satisfying 0 < a < b.
The estimated diﬀerence of the two proﬁles is obtained by ̂H♯
𝛼(1) −̂H♯
𝛼(2) as
a function 𝛼. Letting 𝛼vary over (a, b) satisfying 0 < a < b, (4.29) provides a
[100 × (1 −𝛽)]% point-wise conﬁdence band for the proﬁle diﬀerence in Rényi’s
diversity entropy.
4.4
Remarks
The ﬁrst two fundamental questions in biodiversity studies are as follows:
1) What is diversity?
2) How should it be mathematically measured?
Much discussion of diversity has ensued since (Fisher, Corbet, and Williams
(1943), MacArthur (1955), Margalef (1958)), and it is still ongoing. One of the
main objectives of the discussion has been to pin down one mathematical diver-
sity index that would be universally accepted. However, after more than half
of a century of intense discussion, it should be abundantly clear that any one
diversity index would, at its best, be a summary parameter of the underlying
distribution {pk; k ≥1} to reﬂect a subjectively perceived notion of “diversity.”
A conceptual parallel of diversity may be found in Statistics, where the disper-
sion of a random variable is of interest. For example, the variance of a random
variable is commonly used as a measure of dispersion associated with an under-
lying distribution. However, the popularity of variance has more to do with the
mathematical convenience it entails than its appropriateness in measuring dis-
persion. In fact, the variance does not have any more or less intrinsic merit
in measuring dispersion per se than, say, the expectation of absolute deviation
from the mean or any even central moments of the random variable, when they
exist. One should never lose sight of the ultimate end objective in Statistics: the
knowledge of the underlying distribution. Moments are merely the means and
not the end. Similarly, in diversity studies of ecological populations, or more
generally 𝒳and its associated {pk}, any diversity indices are merely the means
but the distribution of all species is the end.
Much has been published in diversity literature, but rarely can a simpler and
wiser statement be found than the opening paragraph of Purvis and Hector
(2000):
To proceed very far with the study of biodiversity, we need to pin the
concept down. … However, any attempt to measure biodiversity quickly
runs into the problem that it is a fundamentally multidimensional con-
cept: it cannot be reduced sensibly to a single number.

Estimation of Diversity Indices
143
That said however, it must be acknowledged that any eﬀorts toward a sys-
tematic understanding of what axiomatic conditions would constitute diversity
indices should very much be of value. Earlier in this chapter, it is established that
under the two very basic axioms, 01 and 02, any diversity index is a function
of {𝜁v; v ≥1}, which already oﬀers a powerful unifying perspective. There are
many possible additional axioms that could be contemplated. For example, the
following maximization axiom, call it M, is often mentioned in the literature.
Axiom 4.2
Given an alphabet 𝒳with eﬀective cardinality K and its associ-
ated probability distribution p, a diversity index 𝜃= 𝜃(p) must satisfy
M∶max{𝜃(p); all p on 𝒳} = 𝜃(pu)
where pu is the uniform distribution, that is, pk = 1∕K for every k = 1,…, K.
A stronger version of Axiom M is given in the following.
Axiom 4.3
Given an alphabet 𝒳with eﬀective cardinality K and its associ-
ated probability distribution p, a diversity index 𝜃= 𝜃(p) must satisfy
∗
M∶𝜃(p) attains its maximum at, and only at, p = pu
where pu is the uniform distribution, that is, pk = 1∕K for every k = 1,…, K, p.
Axioms M and ∗
Mi are heuristically reasonable if the ecological interest of
all diﬀerent species is equal, which often is not the case in practice. The question
then becomes whether such an additional constraint should be included in the
general axiomatic system for diversity indices.
The following is another axiom, also known as the minimization axiom. The
minimization axiom is thought by many ecologists to be more fundamental
than any other requirements on reasonable diversity indices.
Axiom 4.4
A diversity index 𝜃= 𝜃(p) must satisfy
m∶min{𝜃(p); all p on 𝒳} = 𝜃(po)
where po is a single-point-mass distribution, that is, p1 = 1 and K = 1.
A stronger version of Axiom M is given in the following.
Axiom 4.5
A diversity index 𝜃= 𝜃(p) must satisfy
∗
m∶𝜃(p) attains its minimum at, and only at, p = po
where po is a single-point-mass distribution, that is, p1 = 1 and K = 1.
Example 4.19
The Gini–Simpson diversity index, 𝜃= ∑
k≥1pk(1 −pk), satis-
ﬁes both M and m (see Exercise 12).

144
Statistical Implications of Turing’s Formula
Grabchak, Marcon, Lang and Zhang (2016) showed that the generalized
Simpson’s indices also satisfy M and m under mild restrictions.
There is yet another interesting axiom, known as the replication principle.
The replication principle requires that “if there are N equally large, equally
diverse groups with no species in common, the diversity of the pooled groups
must be N times the diversity of a single group.” For a detailed discussion on the
replication principle, readers may wish to refer to Jost (2006) and Chao, Chiu,
and Jost (2010).
Remark 4.2
The phrases “equally large” and “equally diverse” are subject to
interpretation. However, they are interpreted here within as “equal in number of
existing species” and “equal in diversity as measured by the underlying diversity
index,” respectively.
The replication principle is a strong axiom, so strong it pins down essentially
on one index under mild conditions.
Let 𝜃(p) be a diversity index satisfying the replication principle. Suppose
𝜃(p) also satisﬁes 01 and 02. Then 𝜃0 = 𝜃(po) is uniquely deﬁned, that is, the
index assumes a same value, namely 𝜃0 = 𝜃(po), for every group containing
one species. This implies that any two singleton alphabets, {𝓁1} and {𝓁2}, are
equally diverse by the underlying index 𝜃(p). Since any alphabet with ﬁnite
eﬀective cardinality K = ∑
k≥11[pk > 0] ≥2 may be viewed as a combination
of K singleton alphabets, it follows that
𝜃(p) = 𝜃0K
(4.30)
for any distribution p = {p1,…, pK} such that pk > 0 for each k.
Suppose 𝜃(p) also satisﬁes ∗
m. Then it follows that 𝜃0 > 0. Or else, if 𝜃0 ≤0,
then 𝜃0K ≤𝜃0 ≤0 would violate ∗
m. This argument establishes that: if a diver-
sity index, 𝜃(p), satisﬁes 01, 02, ∗
m, and the replication principle, then it
must be of the form in (4.30) with 𝜃0 being a positive constant. Up to a positive
multiplicative factor, 𝜃0, there is essentially only one index in the form of (4.30),
the richness index K given in Example 4.15. Pinning down on a unique index is
a major advantage of the replication principle. However, as in every situation of
viewing a high-dimensional object in a reduced dimensional space, some ﬁner
aspects of the object often are lost. For example, a common complaint about
the richness index, K, is that it does not distinguish between {p1 = 0.5000, p2 =
0.5000} and {p1 = 0.9999, p2 = 0.0001} in diversity.
Of course even if one has pinned down a good diversity index, it is often
not observable in practice, and therefore the issues of sample-based estimation
come to play. For all practical purposes, a third fundamental question should be
added to the ﬁrst two given above in the beginning of this section. The question
is: how to estimate a diversity index. The answer to this question is unfortu-
nately not trivial at all. A glance at the popular diversity indices in the literature

Estimation of Diversity Indices
145
reveals that, as argued earlier in this chapter, most of the indices are of the form,
or equivalent to,
𝜃=
∑
k≥1
pkh(pk)
where the weight h(p) loads heavily on small values of p. Such is the nature of
many diversity indices. However, estimation of such indices runs into the same
problems as those of estimation of Shannon’s entropy, that is, it is diﬃcult to
recover the values of pkh(pk) for the letters, 𝓁k’s, that are not observed in a
sample. The plug-in estimator tends to have large biases for the same reason
the plug-in estimator of Shannon’s entropy does.
Similar to the literature of Shannon’s entropy estimation, there is no lack of
proposed estimators of diversity indices that have reduced biases. However, not
enough attention is paid to the distributional characteristics of these estima-
tors. While it is perhaps diﬃcult to develop asymptotic distributions for many
of the proposed estimators, they nevertheless are necessary knowledge in prac-
tice. Only when suﬃcient distributional characteristics are known about an
estimator, can one make scientiﬁc inferences based on it about the underlying
population. This chapter summarizes a few distributional results for estimators
of various diversity indices, but much more research is needed in this direction.
4.5
Exercises
1
Suppose X1, X2,…, Xv+1 are v + 1 iid random elements drawn from 𝒳=
{𝓁k; k ≥1} with probability distribution {pk; k ≥1}. Find the probability
that not all v + 1 random elements take a same letter. Discuss the rele-
vance of the above probabilities to diversity.
2
Suppose X1, X2, X3 are iid random elements drawn from 𝒳= {𝓁k; k ≥1}
with probability distribution {pk; k ≥1}.
a) Find the probability that the random elements take three diﬀerent let-
ters.
b) Find the probability that the three random elements take exactly two
diﬀerent letters.
c) Find the probability that the three random elements take at least two
diﬀerent letters and discuss the relevance of the probability to diversity.
3
Let g(x) be a diﬀerentiable function with domain (−∞, +∞). Suppose g(x)
is strictly increasing.
a) Show that the inverse function of g(x), g−1(t), exists on some interval
(a, b).
b) Show that g−1(t) is strictly increasing on (a, b).

146
Statistical Implications of Turing’s Formula
4
Suppose f (s) and g(t) are both continuous and strictly increasing func-
tions with domain (−∞, +∞). Let h(t) = f (g(t)).
a) Show that h(t) is continuous on (−∞, +∞).
b) Show that h(t) is strictly increasing on (−∞, +∞).
5
Let 𝜃, 𝜃1, and 𝜃2 be three diversity indices. Use Deﬁnition 4.1 to show that
if 𝜃1 and 𝜃2 are both equivalent to 𝜃, then 𝜃1 is equivalent to 𝜃2.
6
Show that, for Simpson’s index, 𝜆= ∑
k≥1p2
k = 𝜁0 −𝜁1.
7
Show that, for Rényi’s equivalent entropy in (4.1),
h𝛼=
∑
k≥1
pr
k = 𝜁0 +
∞
∑
v=1
[ v
∏
i=1
(i −𝛼
i
)
𝜁v
]
,
where 𝛼> 0 and 𝛼≠1.
8
Show that, for Emlen’s index,
D =
∑
k≥1
pke−pk =
∞
∑
v=0
e−1
v!
𝜁v.
9
Show that the last equation of Example 4.16 holds, that is,
∑
k≥1
pk
u−1
∑
v=0
(−1)u−1−v (u −1
v
)
(1 −pk)u−1−v(1 −pk)m
=
u−1
∑
v=0
(−1)v (u −1
v
)
𝜁m+v.
10
Show that, under Condition 4.1, 𝜃in (4.3) is well deﬁned, that is,
lim
m→∞
m
∑
v=0
wv
∑
k≥1
pk(1 −pk)v
exists.
11
Verify that, in each of Examples 4.10 through 4.16, the linear coeﬃcients
wv satisﬁes |wv| ≤M for some M > 0.
12
Show that the Gini–Simpson diversity index, 𝜃= ∑K
k=1 pk(1 −pk), satis-
ﬁes both M and m.

Estimation of Diversity Indices
147
13
Show that Shannon’s entropy
H = −
K
∑
k=1
pk ln pk
satisﬁes both M and m.
14
Show that Rényi’s entropy
H𝛼=
1
1 −𝛼ln
( K
∑
k=1
p𝛼
k
)
,
where 𝛼> 0 and 𝛼≠1, satisﬁes both M and m.
15
For the generalized Simpson’s indices,
𝜁u,v =
K
∑
k=1
pu
k(1 −pk)v,
a) show that m is satisﬁed for any pair u ≥1 and v ≥1;
b) show that M is satisﬁed for any pair u = v ≥1; and
c) give an example of {pk}, u and v, such that M is not satisﬁed.
16
For a distribution {pk; k = 1,…, K} on a ﬁnite alphabet 𝒳, let
𝜃=
K
∑
k=1
pkln 2pk.
a) Show that m is satisﬁed.
b) Give an example of {pk} such that M is not satisﬁed.
c) Discuss suﬃcient conditions on {pk} such that M is satisﬁed.
17
Consider a multinomial distribution with K = 3 categories with proba-
bilities, p1, p2, and p3 = 1 −p1 −p2, respectively. Let Y1, Y2, and Y3 be the
observed frequencies of the three diﬀerent categories in an iid sample of
size n. Show that
√
n
((
Y1∕n
Y2∕n
)
−
(
p1
p2
))
L
−−−→MVN(∅, Σ)
where Σ =
(
p1(1 −p1)
−p1p2
−p1p2
p2(1 −p2)
)
.
18
Let v and ̂v be deﬁned as in (4.9). Let Σ(v) be deﬁned as in (4.11). For each
j, j = 1,…, K −1, let gj(̂v) be deﬁned as in (4.12) if ̂pj > 0, but let gj(̂v) = 0
if ̂pj = 0. Suppose ̂pK > 0. Show g𝜏(̂v)Σ(̂v)g(̂v) > 0. (Hint: Consider

148
Statistical Implications of Turing’s Formula
̂p = {̂p1,…, ̂pK}. Deleting all the zeros in ̂p, reenumerating and renam-
ing the remaining nonzero components give q = {q1,…, qK∗} where
K∗= ∑
k=11[̂pk ≠0], which is a probability distribution.)
19
Suppose f (x) is a strictly monotone function on interval [a, b]. Then for
any x1 ∈[a, b] and x2 ∈[a, b], f (x1) = f (x2) implies x1 = x2.
20
Let
p1 = {0.5, 0.5} and p2 = {0.9, 0.1}.
(4.31)
For each of the two probability distributions, calculate the diversity
indices of the following.
a) The Gini–Simpson index.
b) Shannon’s entropy.
c) Rényi’s entropy.
d) Tsallis’ entropy.
e) Hill’s diversity number.
f) Emlen’s index.
g) The richness index.
21
Show that Rényi’s entropy, H𝛼, approaches Shannon’s entropy, H, as 𝛼
approaches 1, that is,
lim
𝛼→1
ln (∑
k≥1p𝛼
k)
1 −𝛼
= −
∑
k≥1
pk ln pk.
22
Sketch and compare 𝜃p = ∑
k≥1pk(1 −pk)v over the interval from v = 1 to
v = 50 for both probability distributions in (4.31) of Exercise 20.
23
Sketch and compare 𝜃p = ln (∑
k≥1p𝛼
k)∕(1 −𝛼) over the interval from 𝛼=
0 to 𝛼= 5 for both probability distributions in (4.31) of Exercise 20.

149
5
Estimation of Information
5.1
Introduction
Consider the following three countable alphabets
𝒳= {xi; i ≥1},
𝒴= {yj; j ≥1},
𝒳× 𝒴= {(xi, yj); i ≥1, j ≥1}.
Given a joint probability distribution
pX,Y = {pi,j; i ≥1, j ≥1}
on 𝒳× 𝒴, consider the two marginal probability distributions on 𝒳and 𝒴,
respectively,
pX =
{
pi,⋅=
∑
j≥1
pi,j; i ≥1
}
pY =
{
p⋅,j =
∑
i≥1
pi,j; i ≥1
}
,
and denote the two underlying random elements as X and Y. Furthermore, let
pXY = {pi,⋅p⋅,j; i ≥1, j ≥1}
(to be distinguished from pX,Y = {pi,j; i ≥1, j ≥1}).
Consider also the conditional probability distributions, respectively, of X on
𝒳given Y = yj where j ≥1 is a speciﬁc index value and of Y on 𝒴given X = xi
where i ≥1 is a speciﬁc index value, that is,
pX|yj =
{
pxi|yj =
pi,j
∑
k≥1pk,j
; i ≥1
}
pY|xi =
{
pyj|xi =
pi,j
∑
k≥1pi,k
; j ≥1
}
.
Statistical Implications of Turing’s Formula, First Edition. Zhiyi Zhang.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

150
Statistical Implications of Turing’s Formula
By Deﬁnition 3.1, entropy may be deﬁned for each of these probability distri-
butions, that is, for pX,Y, pX, pY, pX|yj, and pY|xi, and the respective entropies are
H(X, Y) = −
∑
i≥1
∑
j≥1
pi,j ln pi,j
H(X) = −
∑
i≥1
pi,⋅ln pi,⋅
H(Y) = −
∑
j≥1
p⋅,j ln p⋅,j
H(X|Y = yj) = −
∑
i≥1
pxi|yj ln pxi|yj
H(Y|X = xi) = −
∑
j≥1
pyj|xi ln pyj|xi.
Deﬁnition 5.1
Given a joint probability distribution pX,Y = {pi,j} on 𝒳× 𝒴,
the (expected) conditional entropy of Y given X is
H(Y|X) =
∑
i≥1
pi,⋅H(Y|X = xi).
(5.1)
An exchange of X and Y in Deﬁnition 5.1 gives the expected conditional
entropy of X given Y is
H(X|Y) =
∑
j≥1
p⋅,jH(X|Y = yj).
In information theory, H(Y|X) is often referred to as “the conditional
entropy of Y given X.” However, it is to be noted that it is not entropy of a
probability distribution in the sense of Deﬁnition 3.1. H(Y|X = xi) is entropy
of a conditional probability distributionand so is H(X|Y = yj). H(Y|X) is a
weighted average of conditional entropies H(Y|X = xi), i ≥1, with respect to
the marginal probability distribution of X, and H(X|Y) is a weighted average
of conditional entropies H(X|Y = yj), j ≥1, with respect to the marginal
probability distribution of Y.
Lemma 5.1
Given a joint probability distribution pX,Y = {pi,j; i ≥1, j ≥1} on
𝒳× 𝒴,
1) H(X, Y) = H(X) + H(Y|X) and
2) H(X, Y) = H(Y) + H(X|Y).
Proof: For Part 1, noting that, for each pair of indices (i, j), pi,j ≤pi,⋅,
ln pi,j ≤ln pi,⋅, and hence −ln pi,j ≥−ln pi,⋅≥0,
H(X, Y) = −
∑
i≥1
∑
j≥1
pi,j ln pi,j
= −
∑
i≥1
∑
j≥1
pi,j ln(pyj|xipi,⋅)

Estimation of Information
151
= −
∑
i≥1
∑
j≥1
pi,j ln pyj|xi −
∑
i≥1
∑
j≥1
pi,j ln pi,⋅
= −
∑
i≥1
pi,⋅
∑
j≥1
pyj|xi ln pyj|xi −
∑
i≥1
pi,⋅ln pi,⋅
= H(Y|X) + H(X).
Part 2 follows from the symmetry of H(X, Y) with respect to X and Y.
◽
Corollary 5.1
Given a joint probability distribution pX,Y = {pi,j; i ≥1, j ≥1}
on 𝒳× 𝒴,
1) H(X) ≤H(X, Y),
2) H(Y) ≤H(X, Y), and
3) H(X) + H(Y) −H(X, Y) ≤H(X, Y).
Proof: Since, for each i ≥1, the entropy of the conditional probability dis-
tribution H(Y|X = xi) is non-negative, the expected conditional entropy
H(Y|X) ≥0, being a positively weighted average of the conditional entropies,
is also non-negative. For Part 1, by Lemma 5.1, the desired result follows from
H(X, Y) = H(X) + H(Y|X) ≥H(X).
Part 2 follows similarly. Part 3 is a consequence of Parts 1 and 2.
◽
By Corollary 5.1, the condition H(X, Y) < ∞implies both H(X) < ∞and
H(Y) < ∞.
Consider two probability distributions
p = {pk; k ≥1}
and
q = {qk; k ≥1}
on a same alphabet 𝒳.
Deﬁnition 5.2
For two probability distributions p and q on a same alphabet
𝒳, the relative entropy or the Kullback–Leibler divergence of p and q is
D(p||q) =
∑
k≥1
pk ln
(pk
qk
)
,
(5.2)
observing the conventions that, for each summand p ln(p∕q),
1) p ln (p∕q) = 0, if p = 0, and
2) p ln (p∕q) = +∞, if p > 0 and q = 0.
By the above-mentioned deﬁnition, if there exists a k such that pk > 0 and
qk = 0, then D(p||q) = +∞.

152
Statistical Implications of Turing’s Formula
Suppose p and q have the same support, that is, for each k, pk > 0 if and only
if qk > 0. Then letting {ri; i ≥1} be the sequence of distinct values in
{qk∕pk ∶k ≥1 and pk > 0}
and
p∗
i =
∑
k∶qk∕pk=ri
pk,
the following table deﬁnes a random variable R and its probability distribution.
R
r1
· · ·
ri
· · ·
P(r)
p∗
1
· · ·
p∗
i
· · ·.
(5.3)
Theorem 5.1
Let p and q be two probability distributions on a same alphabet
𝒳and with the same support. Suppose E(R) < ∞where R is as given in (5.3).
Then
D(p||q) ≥0.
(5.4)
Moreover, the equality holds if and only if p = q.
To prove Theorem 5.1, Jensen’s inequality is needed and is stated below as a
lemma without proof.
Lemma 5.2
(Jensen’s inequality)
If g(x) is a convex function on an open
interval (a, b) and X is a random variable with P(X ∈(a, b)) = 1 and ﬁnite
expectation, then
E(g(X)) ≥g(E(X)).
(5.5)
Moreover, if g(x) is strictly convex, then E(g(X)) = g(E(X)) if and only if
P(X = c) = 1 where c is some constant.
Proof of Theorem 5.1: By Deﬁnition 5.2 and noting g(t) = −ln(t) is a strictly
convex function on (0, ∞),
D(p||q) =
∑
k≥1
pk ln
(pk
qk
)
=
∑
k∶pk>0
pk ln
(pk
qk
)
=
∑
i≥1
p∗
i (−ln ri)
= E(−ln R) ≥−ln E(R)
(5.6)
= −ln
(
∑
i≥1
p∗
i ri
)
= −ln
(
∑
i≥1
(
∑
k∶qk∕pk=ri
pk
)
ri
)
= −ln
(
∑
i≥1
(
∑
k∶qk∕pk=ri
pk
qk
pk
))
= −ln
(
∑
i≥1
(
∑
k∶qk∕pk=ri
qk
))
= −ln 1 = 0
where the inequality in (5.6) is due to Lemma 5.2.

Estimation of Information
153
If D(p||q) = 0, then the inequality in (5.6) is forced to be an equality, which,
again by Lemma 5.2, implies that R is constant with probability one, that is,
qk∕pk = r or qk = pkr for every k for which pk > 0. Since p and q have the
same support, summing over all k gives r = 1, that is, pk = qk for every k. This
completes the proof.
◽
Theorem 5.1 gives fundamental support to the Kullback–Leibler divergence
as a reasonable measure of the diﬀerence between two distributions on a
common alphabet. However, it is to be noted that D(p||q) is not symmetric
with respect to p and q, and therefore not a distance measure. For this reason,
the following symmetrized Kullback–Leibler divergence, as given in Kullback
and Leibler (1951), is sometimes used.
D(p, q) = D(p||q) + D(q||p).
(5.7)
Deﬁnition 5.3
The mutual information (MI) of random elements, (X, Y) ∈
𝒳× 𝒴with a joint probability distribution pX,Y, is
MI = MI(X, Y) = D(pX,Y||pXY) =
∑
i≥1
∑
j≥1
pi,j ln
( pi,j
pi,⋅p⋅,j
)
.
(5.8)
Mutual information may be viewed as a degree of dependence between X and
Y as measured by the Kullback–Leibler divergence between pX,Y and pXY on
𝒳× 𝒴. Deﬁnition 5.3 and Theorem 5.1 immediately give Theorem 5.2.
Theorem 5.2
For any joint distribution pX,Y on 𝒳× 𝒴,
MI(X, Y) ≥0.
(5.9)
Moreover, the equality holds if and only if X and Y are independent.
Theorem 5.3
Suppose H(X, Y) < ∞for a joint distribution pX,Y on 𝒳× 𝒴.
Then
MI(X, Y) = H(X) + H(Y) −H(X, Y),
(5.10)
MI(X, Y) = H(Y) −H(Y|X),
(5.11)
MI(X, Y) = H(X) −H(X|Y),
(5.12)
MI(X, Y) = MI(Y, X),
(5.13)
MI(X, X) = H(X), and
(5.14)
MI(X, Y) ≤H(X, Y).
(5.15)
Proof: By Corollary 5.1, H(X, Y) < ∞implies H(X) < ∞and H(Y) < ∞, and
it follows that
MI(X, Y) =
∑
i≥1
∑
j≥1
pi,j ln
( pi,j
pi,⋅p⋅,j
)

154
Statistical Implications of Turing’s Formula
=
∑
i≥1
∑
j≥1
pi,j(ln pi,j −ln pi,⋅−ln p⋅,j)
= H(X) + H(Y) −H(X, Y),
and therefore (5.10) holds.
By Parts 1 and 2 of Lemma 5.1,
MI(X, Y) = H(X) + H(Y) −H(X, Y) = H(Y) −H(Y|X),
MI(X, Y) = H(X) + H(Y) −H(X, Y) = H(X) −H(X|Y),
and therefore (5.11) and (5.12) hold.
Equation (5.13) is obvious by deﬁnition.
Noting P(X = x, X = x) = P(X = x), MI(X, X) = H(X) by deﬁnition, that is,
(5.14) holds.
Equation (5.15) is a restatement of Part 3 of Corollary 5.1.
◽
In Theorem 5.3, MI is expressed in various linear forms of entropies. These
forms are well deﬁned if the joint distribution pX,Y has a ﬁnite support in
𝒳× 𝒴. However, if the support is inﬁnite, these linear forms of entropies
only have well-deﬁned meaning under the condition of H(X, Y) < ∞, which
implies H(X) < ∞and H(Y) < ∞.
Example 5.1
Let, for i = 1, 2,…,
pi =
1
C(i + 1)ln2(i + 1)
where C is a constant such that ∑
i≥1pi = 1. It was demonstrated in Example 3.3
that this distribution has an inﬁnite entropy. Let the joint distribution pX,Y be
such that
pi,j = pi × pj
for each pair of (i, j), where i ≥1 and j ≥1. By Deﬁnition 5.3, MI = 0. However,
by Corollary 5.1, H(X, Y) = ∞, and therefore, the condition of Theorem 5.3 does
not hold.
Entropy may be viewed as a dispersion measure on an alphabet, much like
the variance of a random variable on the real line. It is perhaps interesting
to note that the expression in (5.11) (and therefore also that in (5.12)) has an
ANOVA-like intuitive interpretation as follows.
ANOVA-Like Interpretation of Mutual Information: H(Y) is the total
amount of random dispersion in Y. H(Y|X) is the (average) amount of
dispersion in Y that is unexplained by X. The remainder, H(Y) −H(Y|X),
therefore may be viewed as the amount of dispersion in Y that is explained by

Estimation of Information
155
X. The larger the value of MI(X, Y) = H(Y) −H(Y|X), the higher the level of
association is between X and Y.
To illustrate the characteristics of MI, consider the following three examples.
Example 5.2
Let pX,Y = {pi,j} be such that
pi,j =
{ 1
33
if 1 ≤i ≤11 and 1 ≤j ≤3
0
otherwise
where i and j are positive integers. Let 𝒳be a subset of the xy-plane containing
the following 33 points, denoted by (xi, yj):
(xi, yj) = (0.1(i −1), 0.6 −0.1(j −1))
where i = 1,…, 11 and j = 1, 2, 3 (see Figure 5.1a).
Consider the pair of random variables (X, Y) such that
P((X, Y) = (xi, yj)) = pi,j.
Since X and Y are independent,
𝜌=
Cov(X, Y)
√
Var(X) × VarY
= 0
and
MI(X, Y) = 0.
(5.16)
Example 5.3
Let pX,Y = {pi,j} be such that
pi,j =
{ 1
42
if 1 ≤i ≤21 and 1 ≤j ≤2
0
otherwise
where i and j are positive integers. Let 𝒳be a subset of the xy-plane containing
the following 42 points, denoted by (xi, yj):
(xi, yj) = (0.05(i −1), 0.05(2 + i −2j))
where i = 1,…, 21 and j = 1, 2 (see Figure 5.1b).
Consider the pair of random variables (X, Y) such that
P((X, Y) = (xi, yj)) = pi,j.
H(X) = ln 21,
(5.17)
H(Y) = 4
42 ln 42 + 19
21 ln 21,
(5.18)
H(X, Y) = ln 42.
(5.19)
It may be veriﬁed that
𝜌=
Cov(X, Y)
√
Var(X) × VarY
= 0.9866
and
MI(X, Y) = 2.4274.
(5.20)

156
Statistical Implications of Turing’s Formula
Example 5.4
Let pX,Y = {pi,j} be such that
pi,j =
{ 1
80
if 1 ≤i ≤80 and j = 1
0
otherwise
where i and j are positive integers. Let 𝒳be a subset of the xy-plane containing
the following 80 points, denoted by (xi, yj):
(xi, yj) =
⎧
⎪
⎨
⎪⎩
(
−2𝜋+ (i −1)4𝜋
80 , cos(xi)
)
if 1 ≤i ≤40
(
−2𝜋+ i4𝜋
80 , cos(xi)
)
if 41 ≤i ≤80
(see Figure 5.1c).
Consider the pair of random variables (X, Y) such that
P((X, Y) = (xi, yj)) = pi,j.
H(X) = ln 80,
(5.21)
H(Y) = 19
( 4
80
)
ln
(80
4
)
+ 2
( 2
80
)
ln
(80
2
)
,
(5.22)
H(X, Y) = ln 80.
(5.23)
It may be veriﬁed that
𝜌=
Cov(X, Y)
√
Var(X) × VarY
= 0
and
MI(X, Y) = 3.0304.
(5.24)
Examples 5.2–5.4 demonstrate that MI has an advantage over the coeﬃcient
of correlation 𝜌in capturing a possibly nonlinear relationship between a pair
of random variables (X, Y). In fact, an even stronger characteristic of MI is
demonstrated by revisiting Example 5.4.
Suppose, in Example 5.4, the random pair (X, Y) is redeﬁned in such a way
that it amounts to an exchange of positions along the x-axis (see Figure 5.2a) or
an exchange of positions along the y-axis (see Figure 5.2b). The values of mutual
information MI(X, Y) remain unchanged under such permutations on the
index sets, {i; i ≥1} or {j; j ≥1}, respectively. In fact, Figure 5.2c represents
the set of (xi, yj) points in Example 5.4 but after a sequence of permutations,
ﬁrst on {i; i ≥1} and then on {j; j ≥1}, respectively. The resulting data set, at a
ﬁrst glance, lacks any visible relationship between X and Y, linear or nonlinear.
Yet, the mutual information MI(X, Y) in Figure 5.2c remains unchanged from
Figure 5.1c. The seemingly anti-intuitive phenomenon here is caused by the
fact that one is often conditioned and, therefore, expects to see a continuous
functional relationship between X and Y. Such a functional relationship does
not necessarily exist on a general joint alphabet 𝒳× 𝒴, where the letters
do not necessarily have any natural orders much less numerical meanings.

Estimation of Information
157
0
0.5
1
0
0.5
1
(a)
0
0.5
1
0
0.5
1
(b)
0
π
–2π
–π
2π
−1
0
1
(c)
𝒴
𝒴
𝒴
𝒳
𝒳
𝒳
Figure 5.1 Letters with positive probabilities: (a) Example 5.2, (b) Example 5.3, and
(c) Example 5.4

158
Statistical Implications of Turing’s Formula
–2π
–π
0
π
2π
–2π
–π
0
π
2π
–2π
–π
0
π
2π
−1
0
1
(a)
−1
0
1
−1
0
1
(c)
𝒴
𝒴
𝒴
𝒳
(b)
𝒳
𝒳
Figure 5.2 Invariance of MI under respective permutations by letters in 𝒳and 𝒴
The unchanged positive MI in Figure 5.2c from Figure 5.1c is explained by the
fact that, for each given xi in Figure 5.2c, there is one and only one yj carrying
positive probability. That is to say that the y-value is uniquely determined
by an x-value, and hence a higher degree of association and MI. Since this
relationship is not changed from Figure 5.1c, the MI remains unchanged.

Estimation of Information
159
While MI as in Deﬁnition 5.3 has been shown to have many desirable
properties, it does not have a uniform upper bound since entropies can be
arbitrarily large. The lack of such an upper bound for MI makes it diﬃcult to
convey an intuitive sense of how strong the association between X and Y is
even when it is known to be positive. To alleviate this issue, the standardized
mutual information (SMI), 𝜅, between X and Y is given in Deﬁnition 5.4
below.
Deﬁnition 5.4
Provided that H(X, Y) < ∞, the standardized MI is given by
𝜅=
MI
H(X, Y) = H(X) + H(Y) −H(X, Y)
H(X, Y)
= H(X) + H(Y)
H(X, Y)
−1.
(5.25)
Deﬁnition 5.5
Random elements X ∈𝒳and Y ∈𝒴are said to have a
one-to-one correspondence under a joint probability distribution pX,Y on
𝒳× 𝒴if
1) for every i satisfying P(X = xi) > 0, there exists a unique j such that
P(Y = yj|X = xi) = 1, and
2) for every j satisfying P(Y = yj) > 0, there exists a unique i such that
P(X = xi|Y = yj) = 1.
Theorem 5.4
Suppose H(X, Y) < ∞. Then
0 ≤𝜅≤1.
(5.26)
Moreover,
1) 𝜅= 0 if and only if X and Y are independent, and
2) 𝜅= 1 if and only if X and Y have a one-to-one correspondence.
Proof: By Theorem 5.2 and Corollary 5.1,
0 ≤MI ≤H(X, Y).
Dividing all three parts above by H(X, Y) gives (5.26).
Since 𝜅= 0 if and only if MI = 0, 𝜅= 0 if and only if X and Y are independent
by Theorem 5.2.
If X and Y have a one-to-one correspondence, then for each i satisfying P(X =
xi) > 0 let the unique corresponding j be denoted by ji. Noting pi,ji = pi,⋅and
therefore ln(pi,ji∕pi,⋅) = 0,
H(X, Y) = −
∑
i≥1,j≥1
pi,j ln pi,j
= −
∑
j≥1
∑
i≥1
pi,⋅
(pi,j
pi,⋅
)
ln
[
pi,⋅
(pi,j
pi,⋅
)]

160
Statistical Implications of Turing’s Formula
= −
∑
j≥1
∑
i≥1
pi,⋅
(pi,j
pi,⋅
)
ln pi,⋅−
∑
j≥1
∑
i≥1
pi,⋅
(pi,j
pi,⋅
)
ln
(pi,j
pi,⋅
)
= −
∑
i≥1
pi,⋅ln pi,⋅−
∑
i≥1
pi,⋅
(pi,ji
pi,⋅
)
ln
(pi,ji
pi,⋅
)
= H(X).
Similarly, H(X, Y) = H(Y), and therefore H(X) + H(Y) = 2H(X, Y), in turn,
it implies
𝜅= H(X) + H(Y)
H(X, Y)
−1 = 2 −1 = 1.
On the other hand, if 𝜅= 1, then H(X) + H(Y) = 2H(X, Y) and therefore
(H(X) −H(X, Y)) + (H(Y) −H(X, Y)) = 0.
However, by Corollary 5.1, both of the above-mentioned additive terms are
nonpositive, which implies
H(X) = H(X, Y)
and
H(Y) = H(X, Y),
which in turn, by Lemma 5.1, imply
H(X|Y) = 0
and
H(Y|X) = 0.
By Deﬁnition 5.1, H(Y|X) = 0 implies H(Y|X = xi) = 0 for every i satisfying
P(X = xi) > 0, which in turn implies that the conditional probability distribu-
tion of Y given X = xi puts a probability of mass one on a single point in 𝒴. By
symmetry, H(X|Y) = 0 implies that the conditional probability distribution of
X given Y = yj also puts a probability of mass one on a single point in 𝒳.
◽
Example 5.5
Revisiting Example 5.2 and noting H(X, Y) = ln 33 > 0,
𝜌= 0, MI = 0, and 𝜅= 0.
Example 5.6
Revisiting Example 5.3 and noting H(X, Y) = ln 42,
𝜌= 0.9866, MI = 2.4174, and 𝜅= 0.6468.
Example 5.7
Consider the following set of ﬁve points, (x, y), on the xy-plane
with the given associated probability distribution.
(X, Y)
(−2, 2) (−1, −1) (0, 0) (1, 1) (2, −2)
P(X = x, Y = y)
0.05
0.2
0.5
0.2
0.05
See Figure 5.3.

Estimation of Information
161
−2
−1
0
1
2
−2
−1
0
1
2
𝒳
𝒴
Figure 5.3 Points with positive probabilities in Example 5.7
It may be veriﬁed that
H(X) = H(Y) = H(X, Y) = 1.2899
(5.27)
and that
𝜌= 0,
MI(X, Y) = 1.2899 ,
and
𝜅= 1.
(5.28)
Note that the fact 𝜅= 1 may be directly derived from Theorem 5.4.
Example 5.8
Revisiting Example 5.4 and noting H(X, Y) = ln 80,
𝜌= 0,
MI = 3.0304,
and
𝜅= 0.6916.
Note that 𝜅< 1 in this case because, while X uniquely determines Y, Y does
not uniquely determine X. Readers are reminded that the information being
measured by MI, and hence 𝜅, is “mutual,” and it requires a one-to-one
correspondence between X and Y for 𝜅= 1.
The next example illustrates how the standardized MI, 𝜅, changes as the joint
probability distribution changes.
Example 5.9
Consider the following family of joint probability distributions
of (X, Y) on {0, 1} × {0, 1}. For each m, m = 0, 1, 2,…, 50,
X = 0
X = 1
Y = 0
0.50 −0.01 m
0.01 m
Y = 1
0.01 m
0.50 −0.01 m
or
0.50
0.00
0.00
0.50 , 0.49
0.01
0.01
0.49 ,...,
0.25
0.25
0.25
0.25 ,...,
0.00
0.50
0.50
0.00 .
(5.29)

162
Statistical Implications of Turing’s Formula
30
20
m
10
0
0
0.25
0.5
0.75
1
40
50
κ
Figure 5.4 Plot of 𝜅of (5.30) for m = 0,…, 50.
When m = 1 or m = 50, 𝜅= 1, by Theorem 5.4. For any other m,
𝜅= ln 2 −[(m∕50) ln(50∕m −1) −ln(1 −m∕50)]
ln 2 + [(m∕50) ln(50∕m −1) −ln(1 −m∕50)].
(5.30)
See Exercise 20.
A plot of 𝜅as a function of m is given in Figure 5.4. As m increases from
m = 0, the distribution at the head of (5.29) moves away from a perfect
one-to-one correspondence between X and Y in increments of 2% each. First, 𝜅
decreases rapidly from 𝜅= 1, then changes slowly as it moves closer to 𝜅= 0 at
m = 25, which represents a state of independence between X and Y, and ﬁnally
accelerates its ascend to 𝜅= 1 again as it approaches m = 50.
The sharp drop of 𝜅from the one observed in Example 5.9 as m moves away
from zero is quite typical for the standardized MI, when a small perturbation
is imposed on a joint distribution with a perfect one-to-one correspondence.
Consequently, in most realistic data sets, the calculated standardized MI is
often signiﬁcantly below one. For a comprehensive discussion on some of the
above-mentioned indices of information, as well as many others, readers may
refer to Cover and Thomas (2006).
5.2
Estimation of Mutual Information
In this section, two estimators of MI are described, one is the plug-in estimator
and the other is based on Turing’s perspective. Both of them have asymptotic
normality. Toward that end, let
𝒳= {xi; i = 1,…, K1}
and
𝒴= {yj; j = 1,…, K2}
(5.31)

Estimation of Information
163
be two ﬁnite alphabets with cardinalities K1 < ∞and K2 < ∞, respectively.
Consider the Cartesian product 𝒳× 𝒴with a joint probability distribution
pX,Y = {pi,j}. Assume that pi,⋅> 0 and p⋅,j > 0 for all 1 ≤i ≤K1 and 1 ≤j ≤K2.
Let
K =
∑
i,j
1[pi,j > 0]
(5.32)
be the number of positive joint probabilities in {pi,j}.
For every pair of (i, j), let fi,j be the observed frequency of the random pair
(X, Y) taking value (xi, yj), where i = 1,…, K1 and j = 1,…, K2, in an iid sample
of size n from 𝒳× 𝒴; and let ̂pi,j = fi,j∕n be the corresponding relative fre-
quency. Consequently, ̂pX,Y = {̂pi,j}, ̂pX = {̂pi,⋅}, and ̂pY = {̂p⋅,j} are the sets of
observed joint and marginal relative frequencies.
5.2.1
The Plug-In Estimator
To be instructive, consider ﬁrst the case of K = K1K2, that is, pi,j > 0 for every
pair (i, j) where i = 1,…, K1 and i = 1,…, K2. Let p be a speciﬁcally arranged
pi,j as follows.
p = (p1,1, p1,2,…, p1,K2, p2,1, p2,2,…, p2,K2,…, pK1,1,…, pK1,K2−1)𝜏.
Accordingly let
̂p = ( ̂p1,1, ̂p1,2,…, ̂p1,K2, ̂p2,1, ̂p2,2,…, ̂p2,K2,…, ̂pK1,1,…, ̂pK1,K2−1)𝜏.
For notation convenience, the speciﬁcally rearranged pi,js and ̂pi,js in p and ̂p
are re-enumerated by a single index k and are denoted by
v = (p1,…, pK−1)𝜏
and
̂v = (̂p1,…, ̂pK−1)𝜏
(5.33)
where K = K1K2. It is to be noted that for any pair of (i, j),
k = (i −1)K2 + j.
The multivariate central limit theorem gives that
√
n(̂v −v)
L
−−−→MVN(0, Σ)
(5.34)
where Σ is a (K1K2 −1) × (K1K2 −1) covariance matrix as given in the
following.
Σ ∶= Σ(v) ∶=
⎛
⎜
⎜
⎜
⎜
⎜
⎜⎝
p1(1 −p1)
−p1p2
· · ·
−p1pK−1
−p1p2
p2(1 −p2) · · ·
−p2pK−1
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
−pK−1p1
−pK−1p2 · · · pK−1(1 −pK−1)
⎞
⎟
⎟
⎟
⎟
⎟
⎟⎠
.
(5.35)

164
Statistical Implications of Turing’s Formula
Shannon’s entropies for 𝒳, 𝒴, and 𝒳× 𝒴, with their respective distribu-
tions, are deﬁned as
GX(v) = H(X) = −
∑
i
pi,⋅ln pi,⋅,
(5.36)
GY(v) = H(Y) = −
∑
j
p⋅,j ln p⋅,j,
(5.37)
GXY(v) = H(X, Y) = −
∑
i
∑
j
pi,j ln pi,j = −
∑
k
pk ln pk.
(5.38)
Let ̂H(X), ̂H(Y), and ̂H(X, Y) be the plug-in estimators of H(X), H(Y), and
H(X, Y), respectively, that is,
̂H(X) = −
∑
i
̂pi,⋅ln ̂pi,⋅,
(5.39)
̂H(Y) = −
∑
j
̂p⋅,j ln ̂p⋅,j,
(5.40)
̂H(X, Y) = −
∑
i
∑
j
̂pi,j ln ̂pi,j = −
∑
k
̂pk ln ̂pk.
(5.41)
The respective gradients for GX(v), GY(v), and GXY(v) are
gX(v) = ∇GX(v) =
(
∂
∂p1
GX(v),…,
∂
∂pK−1
GX(v)
)𝜏
,
gY(v) = ∇GY(v) =
(
∂
∂p1
GY(v),…,
∂
∂pK−1
GY(v)
)𝜏
,
gXY(v) = ∇GXY(v) =
(
∂
∂p1
GXY(v),…,
∂
∂pK−1
GXY(v)
)𝜏
.
For every k, k = 1,…, K −1, such that pk = pi,j, the following facts can be
veriﬁed (see Exercise 11):
∂
∂pk
GX(v) = ∂H(X)
∂pk
= ln pK1,⋅−ln pi,⋅,
(5.42)
∂
∂pk
GY(v) = ∂H(Y)
∂pk
= ln p⋅,K2 −ln p⋅,j,
(5.43)
∂
∂pk
GXY(v) = ∂H(X, Y)
∂pk
= ln pK −ln pk.
(5.44)
Let
A ∶= A(v) ∶=
⎛
⎜
⎜
⎜⎝
a𝜏
1
a𝜏
2
a𝜏
3
⎞
⎟
⎟
⎟⎠
∶=
⎛
⎜
⎜
⎜
⎜
⎜
⎜⎝
∂
∂p1
GX(v) · · ·
∂
∂pK−1
GX(v)
∂
∂p1
GY(v) · · ·
∂
∂pK−1
GY(v)
∂
∂p1
GXY(v) · · ·
∂
∂pK−1
GXY(v)
⎞
⎟
⎟
⎟
⎟
⎟
⎟⎠
.
(5.45)

Estimation of Information
165
According to (5.42)–(5.44), the ﬁrst row of A, written in (K1 −1) blocks of
size K2 and 1 block of size (K2 −1), is
a𝜏
1 = (
ln(pK1,⋅∕p1,⋅),
· · ·
ln(pK1,⋅∕p1,⋅),
ln(pK1,⋅∕p1,⋅),
ln(pK1,⋅∕p2,⋅),
· · ·
ln(pK1,⋅∕p2,⋅),
ln(pK1,⋅∕p2,⋅),
⋮
⋮⋮⋮
⋮
⋮
ln(pK1,⋅∕pK1−1,⋅),
· · ·
ln(pK1,⋅∕pK1−1,⋅),
ln(pK1,⋅∕pK1−1,⋅),
0,
· · ·
0
);
the second row of A, written in (K1 −1) blocks of size K2 and 1 block of size
(K2 −1), is
a𝜏
2 = (
ln(p⋅,K2∕p⋅,1),
ln(p⋅,K2∕p⋅,2),
· · ·
ln(p⋅,K2∕p⋅,K2−1),
0,
ln(p⋅,K2∕p⋅,1),
ln(p⋅,K2∕p⋅,2),
· · ·
ln(p⋅,K2∕p⋅,K2−1),
0,
⋮
⋮
⋮⋮⋮
⋮
⋮
ln(p⋅,K2∕p⋅,1),
ln(p⋅,K2∕p⋅,2),
· · ·
ln(p⋅,K2∕p⋅,K2−1),
0,
ln(p⋅,K2∕p⋅,1),
ln(p⋅,K2∕p⋅,2),
· · ·
ln(p⋅,K2∕p⋅,K2−1)
);
and the third row of A, written in (K1 −1) blocks of size K2 and 1 block of size
(K2 −1), is
a𝜏
3 = (
ln(pK∕p1,1),
· · ·
ln(pK∕p1,K2−1),
ln(pK∕p1,K2),
ln(pK∕p2,1),
· · ·
ln(pK∕p2,K2−1),
ln(pK∕p2,K2),
⋮
⋮⋮⋮
⋮
⋮
ln(pK∕pK1,1),
· · ·
ln(pK∕pK1,K2−1) );
An application of the delta method gives Lemma 5.3 below.
Lemma 5.3
Let 𝒳and 𝒴be as in (5.31), let pX,Y = {pi,j} be a joint probability
distribution on 𝒳× 𝒴, and let v and ̂v be as in (5.33). Suppose pi,j > 0 for every
pair (i, j) where i = 1,…, K1 and j = 1,…, K2. Then
√
n
⎡
⎢
⎢⎣
⎛
⎜
⎜⎝
̂H(X)
̂H(Y)
̂H(X, Y)
⎞
⎟
⎟⎠
−
⎛
⎜
⎜⎝
H(X)
H(Y)
H(X, Y)
⎞
⎟
⎟⎠
⎤
⎥
⎥⎦
L
−−−→MVN(0, ΣH)
(5.46)
where
ΣH = AΣA𝜏,
Σ is as in (5.35), and A is as in (5.45) according to (5.42)–(5.44).
More generally for the case of K ≤K1K2, that is, pi,j may not be positive
for some pairs of (i, j), a result similar to Lemma 5.3 can be derived. For any

166
Statistical Implications of Turing’s Formula
arbitrary but ﬁxed re-enumeration of the K positive probabilities in {pi,j},
denoted as
{pk; k = 1,…, K},
(5.47)
consider the following two partitions
{S1,…, SK1}
and
{T1,…, TK2}
of the index set {1, 2,…, K} such that
1) {pk; k ∈Ss} is the collection of all positive probabilities in {pi,j; i = s} for
each s, s = 1,…, K1; and
2) {pk; k ∈Tt} is the collection of all positive probabilities in {pi,j; j = t} for
each t, t = 1,…, K2.
By the construction of the partitions,
∑
k∈Si
pk = pi,⋅
and
∑
k∈Tj
pk = p⋅,j.
Without loss of generality, it may be assumed that K ∈SK1 ∩TK2. If not, then
K ∈Si0 ∩Tj0 for some i0 and j0, by a rearrangement of the indices (i, j), K ∈
SK1 ∩TK2 will be true.
Letting
v = (p1,…, pK−1)𝜏
and
̂v = (̂p1,…, ̂pK−1)𝜏,
(5.48)
an application of multivariate central limit theorem gives
√
n(̂v −v)
L
−−−→MVN(0, Σ)
(5.49)
where Σ is the (K −1) × (K −1) covariance matrix given by
Σ ∶= Σ(v) ∶=
⎛
⎜
⎜
⎜⎝
p1(1 −p1)
−p1p2
· · ·
−p1pK−1
−p2p1
p2(1 −p2) · · ·
−p2pK−1
· · ·
· · ·
· · ·
· · ·
−pK−1p1
−pK−1p2 · · · pK−1(1 −pK−1)
⎞
⎟
⎟
⎟⎠
.
(5.50)
Noting (5.36)–(5.38), the following facts can be veriﬁed:
∂GX(v)
∂pk
= ∂H(X)
∂pk
=
{
ln pK1,⋅−ln pi,⋅, if k ∈Si ≠SK1
0,
if k ∈SK1
(5.51)
∂GY(v)
∂pk
= ∂H(Y)
∂pk
=
{
ln p⋅,K2 −ln p⋅,j, if k ∈Tj ≠TK2
0,
if k ∈TK2
(5.52)
∂GXY(v)
∂pk
= ∂H(X, Y)
∂pk
= ln pK −ln pk, for 1 ≤k ≤K −1.
(5.53)

Estimation of Information
167
To show (5.51) is true, consider ﬁrst
GX(v) = −
K1
∑
s=1
(
∑
l∈Ss
pl
)
ln
(
∑
l∈Ss
pl
)
= −
K1−1
∑
s=1
(
∑
l∈Ss
pl
)
ln
(
∑
l∈Ss
pl
)
−
⎛
⎜
⎜⎝
∑
l∈SK1
pl
⎞
⎟
⎟⎠
ln
⎛
⎜
⎜⎝
∑
l∈SK1
pl
⎞
⎟
⎟⎠
= −
K1−1
∑
s=1
(
∑
l∈Ss
pl
)
ln
(
∑
l∈Ss
pl
)
−
[
1 −
K1−1
∑
s=1
(
∑
l∈Ss
pl
)]
ln
(
1 −
K1−1
∑
s=1
(
∑
l∈Ss
pl
))
.
For the case of k ∈Si ≠SK1,
∂GX(v)
∂pk
= −∂
∂pk
[(
∑
l∈Si
pl
)
ln
(
∑
l∈Si
pl
)]
−
∂
∂pk
{[
1 −
K1−1
∑
s=1
(
∑
l∈Ss
pl
)]
ln
(
1 −
K1−1
∑
s=1
(
∑
l∈Ss
pl
))}
= −ln
(
∑
l∈Si
pl
)
−1 −
{
−ln
(
1 −
K1−1
∑
s=1
(
∑
l∈Ss
pl
))
−1
}
= −ln pi,⋅+ ln
(
1 −
K1−1
∑
s=1
(
∑
l∈Ss
pl
))
= −ln pi,⋅+ ln pK1,⋅= ln pK1,⋅−ln pi,⋅.
For the case of k ∈SK1, ∂GX(v)∕∂pk = 0.
A similar argument would give the proof for (5.52) (see Exercise 11).
Equation (5.53) is obvious.
For any arbitrary but ﬁxed re-enumeration of the positive terms of {pi,j} in
(5.47), an application of the delta method, based on (5.49), gives Lemma 5.4
below.
Lemma 5.4
Let 𝒳and 𝒴be as in (5.31), let pX,Y = {pi,j} be a joint probability
distribution on 𝒳× 𝒴, and let v and ̂v be as in (5.48):
√
n
⎡
⎢
⎢⎣
⎛
⎜
⎜⎝
̂H(X)
̂H(Y)
̂H(X, Y)
⎞
⎟
⎟⎠
−
⎛
⎜
⎜⎝
H(X)
H(Y)
H(X, Y)
⎞
⎟
⎟⎠
⎤
⎥
⎥⎦
L
−−−→MVN(0, ΣH)
(5.54)
where
ΣH = AΣA𝜏,
(5.55)
Σ is as in (5.50) and A is as in (5.45) according to (5.51)–(5.53).

168
Statistical Implications of Turing’s Formula
Remark 5.1
Lemma 5.3 is a special case of Lemma 5.4, and it is sepa-
rately stated for its instructive quality. While symbolically identical, the
matrices A and Σ in Lemma 5.4 may take on diﬀerent forms from those in
Lemma 5.3, depending on K, the number of positive probabilities in {pi,j}, and
the re-enumeration in (5.48).
Note that the covariance matrix ΣH in Lemma 5.4 (as well as that in
Lemma 5.3) may not necessarily be of full rank. For ΣH to be of full rank, some
additional conditions need to be imposed.
To estimate MI = MI(X, Y), noting
MI(X, Y) = (1, 1, −1)
⎛
⎜
⎜⎝
H(X)
H(Y)
H(X, Y)
⎞
⎟
⎟⎠
and letting
̂
MI = ̂
MI (X, Y) = (1, 1, −1)
⎛
⎜
⎜⎝
̂H(X)
̂H(Y)
̂H(X, Y)
⎞
⎟
⎟⎠
,
(5.56)
an application of the multivariate delta method gives Theorem 5.5 below.
Theorem 5.5
Under the conditions of Lemma 5.4 and suppose further that
𝜎2 = (1, 1 −1)ΣH(1, 1, −1)𝜏> 0
(5.57)
where ΣH is as in (5.35). Then
√
n( ̂
MI −MI)
L
−−−→N(0, 𝜎2).
(5.58)
Let
̂Σ = Σ(̂v) =
⎛
⎜
⎜
⎜
⎜⎝
̂p1(1 −̂p1)
−̂p1 ̂p2
· · ·
−̂p1 ̂pK−1
−̂p2 ̂p1
̂p2(1 −̂p2) · · ·
−̂p2 ̂pK−1
· · ·
· · ·
· · ·
· · ·
−̂pK−1 ̂p1
−̂pK−1 ̂p2 · · · ̂pK−1(1 −̂pK−1)
⎞
⎟
⎟
⎟
⎟⎠
,
(5.59)
̂A = A(̂v) =
⎛
⎜
⎜
⎜
⎜
⎜
⎜⎝
∂
∂p1
GX(v) · · ·
∂
∂pK−1
GX(v)
∂
∂p1
GY(v) · · ·
∂
∂pK−1
GY(v)
∂
∂p1
GXY(v) · · ·
∂
∂pK−1
GXY(v)
⎞
⎟
⎟
⎟
⎟
⎟
⎟⎠
|||||||||||||v=̂v
,
(5.60)
̂ΣH = ̂A ̂Σ ̂A𝜏,
(5.61)
̂𝜎2 = (1, 1, −1) ̂ΣH(1, 1, −1)𝜏.
(5.62)

Estimation of Information
169
By the continuous mapping theorem, ̂Σ = Σ(̂v) is a consistent estimator of
Σ = Σ(v), ̂A = A(̂v) is a consistent estimator of A, ̂ΣH = ̂AΣ(̂v) ̂A𝜏is a consistent
estimator of ΣH = Σ(v), and ̂𝜎2 is a consistent estimator of 𝜎2. The said
consistency and Slutsky’s theorem give immediately Corollary 5.2 below.
Corollary 5.2
Under the conditions of Theorem 5.5 ,
√
n( ̂
MI −MI)
̂𝜎
L
−−−→N(0, 1)
(5.63)
where ̂𝜎is as given in (5.62).
Corollary 5.2 provides the means of a statistical tool to make inferences
regarding MI. However, its validity depends on the condition in (5.57). To
decipher (5.57), consider ﬁrst the case of K = K1K2, that is, pi,j > 0 for every
pair (i, j).
Since Σ is positive deﬁnite (see Exercise 6), (5.57) holds if and only if
(1, 1, −1)A ≠0, or equivalently X and Y are not independent.
The equivalence of (1, 1, −1) A ≠0 and that X and Y are not independent
is established by the fact that (1, 1, −1)A = 0 if and only if X and Y are
independent. To see this fact, suppose (1, 1, −1) A = 0, that is, for every pair
(i, j), by (5.45),
ln(pK1,⋅∕pi,⋅) + ln(p⋅,K2∕p⋅,j) = ln(pK∕pi,j),
or after a few algebraic steps,
pi,j =
pK
pK1,⋅p⋅,K2
pi,⋅p⋅,j.
Summing both sides over i and j gives pK = pK1,⋅p⋅,K2, and therefore
pi,j = pi,⋅p⋅,j
for every pair of (i, j), that is, X and Y are independent. The converse is
obviously true.
Next consider the case of K < K1K2. Without loss of generality, it may
be assumed that pi,⋅> 0 for every i and p⋅,j > 0 for every j. (Or else, a
re-enumeration of the index i or j would ensure the validity of this assumption.)
An similar argument to the case of K = K1K2 would establish that pi,j = pi,⋅p⋅,j
for every pair (i, j) such that pi,j ≠0 (see Exercise 12). At this point, it is estab-
lished in general that (5.57) is equivalent to that X and Y are not independent.
Since MI is a linear combination of three entropies, its estimation is closely
related to that of entropy. For entropy, the plug-in estimator ̂H based on an iid
sample of size n, though asymptotically eﬃcient, is known to have a large bias,
as given in (3.3). Based on (3.3), it is easy to see that the bias for ̂
MI is
E( ̂
MI −MI) = K −(K1 + K2) + 1
2n
+ 𝒪(n−2)

170
Statistical Implications of Turing’s Formula
where K is an integer between max{K1, K2} and K1K2, but often is in the order
of K1K2, which leads to an overestimation for MI. For this reason, seeking a
good MI estimator with smaller bias has been the focal point of much of the
research in MI estimation.
In the next section, an estimator of MI based on Turing’s perspective is
described. This estimator qualitatively alleviates the bias issue observed in the
plug-in estimator ̂
MI.
5.2.2
Estimation in Turing’s Perspective
In Chapter 3, an entropy estimator based on Turing’s perspective is introduced
as ̂Hz in (3.27). This estimator naturally extends to the following estimator of
MI:
̂
MIz = ̂
MIz(X, Y)
(5.64)
= ̂Hz(X) + ̂Hz(Y) −̂Hz(X, Y)
=
n−1
∑
v=1
1
v
{
nv+1[n −(v + 1)]!
n!
K1
∑
i=1
[
̂pi,⋅
v−1
∏
k=0
(
1 −̂pi,⋅−k
n
)]}
+
n−1
∑
v=1
1
v
{
nv+1[n −(v + 1)]!
n!
K2
∑
j=1
[
̂p⋅,j
v−1
∏
k=0
(
1 −̂p⋅,j −k
n
)]}
−
n−1
∑
v=1
1
v
{
nv+1[n −(v + 1)]!
n!
K1
∑
i=1
K2
∑
j=1
[
̂pi,j
v−1
∏
k=0
(
1 −̂pi,j −k
n
)]}
.
By (3.28), it is easy to see that the bias of ̂
MIz in (5.64) is
E( ̂
MIz −MI) = −
∞
∑
v=n
1
v
K1
∑
i=1
pi,⋅(1 −pi,⋅)v
−
∞
∑
v=n
1
v
K2
∑
j=1
p⋅,j(1 −p⋅,j)v
+
∞
∑
v=n
1
v
K1
∑
i=1
K2
∑
j=1
pi,j(1 −pi,j)v
and it is decaying at least exponentially fast in n since each of the three additive
terms is decaying at least exponentially fast in n, as argued in Theorem 3.9. In
fact, letting
p∨= max
i,j {pi,j > 0},
p∧= min
i,j {pi,j > 0},
p∨,⋅= max
i {pi,⋅> 0},

Estimation of Information
171
p∧,⋅= min
i {pi,⋅> 0},
p⋅,∨= max
j {p⋅,j > 0},
and
p⋅,∧= min
j {p⋅,j > 0},
it is easily veriﬁed that
−
∞
∑
v=n
1
v(1 −p∧,⋅)v −
∞
∑
v=n
1
v(1 −p⋅,∧)v +
∞
∑
v=n
1
v(1 −p∨)v
≤E( ̂
MIz −MI) ≤
(5.65)
−
∞
∑
v=n
1
v(1 −p∨,⋅)v −
∞
∑
v=n
1
v(1 −p⋅,∨)v +
∞
∑
v=n
1
v(1 −p∧)v,
and that all the terms of both sides of (5.65) converge to zero exponentially fast
since all of p∨, p∧, p∨,⋅, p∧,⋅, p⋅,∨, and p⋅,∧are ﬁxed positive constants in (0, 1). The
inequalities of (5.65) are in fact one of the main advantages of the estimator ̂
MIz
in (5.64).
Theorem 5.6
Under the conditions of Theorem 5.5 and suppose further that X
and Y are not independent. Then
√
n( ̂
MIz −MI)
L
−−−→N(0, 𝜎2),
(5.66)
where 𝜎is as in (5.57).
The asymptotic normality of Theorem 5.6 is derived from Theorem 5.5
by showing that ̂
MI and ̂
MIz are suﬃciently close as n →∞. Toward
that end, consider ﬁrst the problem of ̂Hz in (3.27) estimating Shannon’s
entropy H = −∑K′
k=1 pk ln pk deﬁned with {pk; k = 1,…, K′} on alphabet
ℒ= {𝓁k; k = 1, ⋅, K′} where K′ is ﬁnite. Let {̂pk} be the observed relative
frequencies of letters in an iid sample of size n and ̂H = −∑K′
k=1 ̂pk ln ̂pk be the
plug-in estimator of H.
Lemma 5.5
Suppose {pk} is a nonuniform distribution on ℒ= {𝓁k}. Then
√
n( ̂Hz −̂H)
p
−−−→0.
To prove Lemma 5.5, Lemma 3.2 is needed, which is restated as Lemma 5.6
below for easy reference. Let, for any p ∈(0, 1),
gn(p) =
n−1
∑
v=1
{
1
v
{nv+1[n −(v + 1)]!
n!
}[ v−1
∏
j=0
(
1 −p −j
n
)]
1[v≤n(1−p)+1]
}
,
hn(p) = pgn(p), and h(p) = −p ln p.

172
Statistical Implications of Turing’s Formula
Lemma 5.6
Let ̂p = X∕n where X is a binomial random variable with param-
eters n and p. Then
1)
√
n[hn(p) −h(p)] →0 uniformly in p ∈(c, 1) for any c, 0 < c < 1;
2)
√
n|hn(p) −h(p)| < A(n) = 𝒪(n3∕2) uniformly in p ∈[1∕n, c] for any c,
0 < c < p; and
3) P( ̂p ≤c) < B(n) = 𝒪(n−1∕2 exp {−nC}) where C = (p −c)2∕[p(1 −p)] for
any c ∈(0, p).
Proof of Lemma 5.5: Without loss of generality, consider the sample pro-
portions of the ﬁrst letter of the alphabet ̂p1. It is of interest to show that
Δn =
√
n[hn( ̂p1) −h( ̂p1)]
p→0, that is, for any ε > 0, as n →∞, P(Δn > ε) →0.
Toward that end, observe that for any ﬁxed c ∈(0, p1)
P(Δn > ε) = P(
√
n[hn( ̂p1) −h( ̂p1)] > ε|̂p1 ∈(c, 1))P( ̂p1 ∈(c, 1))
+ P(
√
n[hn( ̂p1) −h( ̂p1)] > ε|̂p1 ∈[1∕n, c]))P( ̂p1 ∈[1∕n, c])
< P(
√
n[hn( ̂p1) −h( ̂p1)] > ε|̂p1 ∈(c, 1)) + P( ̂p1 ∈[1∕n, c]).
By Part 1 of Lemma 5.6, P(
√
n[hn( ̂p1) −h( ̂p1)] > ε|̂p1 ∈(c, 1))
p→0; and
by Part 3 of Lemma 5.6, P(̂p1 ∈[1∕n, c])
p→0. Hence Δn
p→0. The result
of Lemma 5.5 is immediate from the following expression in a ﬁnite sum,
√
n( ̂Hz −̂H) = ∑K
k=1
√
n(hn(̂pk) −h(̂pk)).
◽
Lemma 5.5 immediately gives the following corollary.
Corollary 5.3
Under the conditions of Lemma 5.4,
√
n
⎡
⎢
⎢⎣
⎛
⎜
⎜⎝
̂Hz(X)
̂Hz(Y)
̂Hz(X, Y)
⎞
⎟
⎟⎠
−
⎛
⎜
⎜⎝
H(X)
H(Y)
H(X, Y)
⎞
⎟
⎟⎠
⎤
⎥
⎥⎦
L
−−−→MVN(0, ΣH)
(5.67)
where
ΣH = AΣA𝜏,
(5.68)
Σ is as in (5.50) and A is as in (5.45) according to (5.51)–(5.53).
Proof of Theorem 5.6: In light of Theorem 5.5 and Slutsky’s theorem, it suﬃces
to show that
√
n( ̂
MIz −̂
MI)
p→0. However, this is trivial since
√
n( ̂
MIz −̂
MI) =
√
n( ̂Hz(X) −̂H(X))
+
√
n( ̂Hz(Y) −̂H(Y))
−
√
n( ̂Hz(X, Y) −̂H(X, Y)).

Estimation of Information
173
Applying Lemma 5.5 to each of the three additive terms above, respectively,
gives the result of Theorem 5.6.
◽
From Theorem 5.6 and Slutsky’s theorem, Corollary 5.4 follows.
Corollary 5.4
Under the conditions of Theorem 5.6,
√
n( ̂
MIz −MI)
̂𝜎
L
−−−→N(0, 1),
(5.69)
where ̂𝜎is as given in (5.62).
Corollary 5.4, like Corollary 5.2, provides a large sample statistical tool for
inference about MI, but with a qualitatively lower bias.
5.2.3
Estimation of Standardized Mutual Information
The SMI 𝜅in Deﬁnition 5.4 is a practically useful measure that rescales MI to
within interval [0, 1], particularly with the necessary and suﬃcient conditions
of Theorem 5.4. In this section, several estimators of 𝜅are described. However,
𝜅is not the only one that has the desirable properties. Since
0 ≤MI(X, Y) ≤min{H(X), H(Y)}
≤
√
H(X)H(Y)
≤(H(X) + H(Y))∕2
≤max{H(X), H(Y)}
≤H(X, Y),
(see Exercise 7), each of the following, assuming H(X, Y) < ∞,
𝜅1 =
MI(X, Y)
min{H(X), H(Y)},
𝜅2 =
MI(X, Y)
√
H(X)H(Y)
,
𝜅3 =
MI(X, Y)
(H(X) + H(Y))∕2,
𝜅4 =
MI(X, Y)
max{H(X), H(Y)},
may be considered a SMI and satisﬁes
0 ≤𝜅i ≤1,
where i ∈{1, 2, 3, 4}. However the necessary and suﬃcient condition of
Theorem 5.4 holds only for 𝜅2, 𝜅3 and 𝜅4, but not for 𝜅1. See Exercise 8. Further
discussion on these and other variants of SMI may be found in Kvalseth

174
Statistical Implications of Turing’s Formula
(1987), Strehl and Ghosh (2002), Yao (2003), and Vinh, Epps, and Bailey
(2010). A detailed discussion of the estimation of various standardized mutual
information may be found in Zhang and Stewart (2016).
In this section, the plug-in estimators of 𝜅1, 𝜅2, and 𝜅3, as well as those
based on Turing’s perspective, are described, and their respective asymptotic
distributional properties are given. Toward that end, consider the following
three functions of triplet (x1, x2, x3), with domain, x1 > 0, x2 > 0, and x3 > 0,
𝜅(x1, x2, x3) = x1 + x2
x3
−1,
𝜅2(x1, x2, x3) = x1 + x2 −x3
√x1x2
,
𝜅3(x1, x2, x3) = 2
(
1 −
x3
x1 + x2
)
.
The gradients of these functions are, respectively,
g𝜅(x1, x2, x3) =
(
1
x3
, 1
x3
, −x1 + x2
x2
3
)𝜏
,
g𝜅2(x1, x2, x3) =
(
1
(x1x2)1∕2 −x2(x1 + x2 −x3)
2(x1x2)3∕2
,
1
(x1x2)1∕2 −x1(x1 + x2 −x3)
2(x1x2)3∕2
, −
1
(x1x2)1∕2
)𝜏
,
g𝜅3(x1, x2, x3) =
(
2x3
(x1 + x2)2 ,
2x3
(x1 + x2)2 , −
2
x1 + x2
,
)𝜏
.
See Exercise 13.
Let the following be the plug-in estimators of 𝜅, 𝜅2, and 𝜅3.
̂𝜅=
̂H(X) + ̂H(Y)
̂H(X, Y)
−1 =
̂H(X) + ̂H(Y) −̂H(X, Y)
̂H(X, Y)
,
(5.70)
̂𝜅2 =
̂H(X) + ̂H(Y) −̂H(X, Y)
√̂H(X) ̂H(Y)
,
(5.71)
̂𝜅3 = 2
(
1 −
̂H(X, Y)
̂H(X) + ̂H(Y)
)
=
̂H(X) + ̂H(Y) −̂H(X, Y)
( ̂H(X) + ̂H(Y))∕2
.
(5.72)
Theorem 5.7
Under the conditions of Lemma 5.4,
1)
√
n(̂𝜅−𝜅)
L
−−−→N(0, 𝜎2
̂𝜅),
2)
√
n(̂𝜅2 −𝜅2)
L
−−−→N(0, 𝜎2
̂𝜅2),
3)
√
n(̂𝜅3 −𝜅3)
L
−−−→N(0, 𝜎2
̂𝜅3),

Estimation of Information
175
where
𝜎2
̂𝜅= g𝜏
𝜅(H(X), H(Y), H(X, Y)) ΣH g𝜅(H(X), H(Y), H(X, Y)),
𝜎2
̂𝜅2 = g𝜏
𝜅2(H(X), H(Y), H(X, Y)) ΣH g𝜅2(H(X), H(Y), H(X, Y)),
𝜎2
̂𝜅3 = g𝜏
𝜅3(H(X), H(Y), H(X, Y)) ΣH g𝜅3(H(X), H(Y), H(X, Y)),
and ΣH is as in (5.55).
Proof: An application of the delta method to (5.54) completes the proof.
◽
Corollary 5.5
Under the conditions of Theorem 5.7,
1)
√
n(̂𝜅−𝜅)
̂𝜎̂𝜅
L
−−−→N(0, 1),
2)
√
n(̂𝜅2 −𝜅2)
̂𝜎̂𝜅2
L
−−−→N(0, 1),
3)
√
n(̂𝜅3 −𝜅3)
̂𝜎̂𝜅3
L
−−−→N(0, 1),
where
̂𝜎2
̂𝜅= g𝜏
𝜅( ̂H(X), ̂H(Y), ̂H(X, Y)) ̂ΣH g𝜅( ̂H(X), ̂H(Y), ̂H(X, Y)),
̂𝜎2
̂𝜅2 = g𝜏
𝜅2( ̂H(X), ̂H(Y), ̂H(X, Y)) ̂ΣH g𝜅2( ̂H(X), ̂H(Y), ̂H(X, Y)),
̂𝜎2
̂𝜅3 = g𝜅3( ̂H(X), ̂H(Y), ̂H(X, Y)) ̂ΣH g𝜏
𝜅3( ̂H(X), ̂H(Y), ̂H(X, Y)),
and ̂ΣH is as in (5.61).
Proof: The consistency of ̂𝜎2
̂𝜅, ̂𝜎2
̂𝜅2, and ̂𝜎2
̂𝜅3 and the Slutsky’s theorem completes
the proof.
◽
Let ̂Hz(X), ̂Hz(Y), and ̂Hz(X, Y) be as in (5.64). Let
̂𝜅z =
̂Hz(X) + ̂Hz(Y)
̂Hz(X, Y)
−1 =
̂Hz(X) + ̂Hz(Y) −̂Hz(X, Y)
̂Hz(X, Y)
,
(5.73)
̂𝜅2z =
̂Hz(X) + ̂Hz(Y) −̂Hz(X, Y)
√
̂Hz(X) ̂Hz(Y)
,
(5.74)
̂𝜅3z = 2
(
1 −
̂Hz(X, Y)
̂Hz(X) + ̂Hz(Y)
)
=
̂Hz(X) + ̂Hz(Y) −̂Hz(X, Y)
( ̂Hz(X) + ̂Hz(Y))∕2
.
(5.75)
Theorem 5.8
Under the conditions of Corollary 5.3,
1)
√
n(̂𝜅z −𝜅)
L
−−−→N(0, 𝜎2
̂𝜅z),

176
Statistical Implications of Turing’s Formula
2)
√
n(̂𝜅2z −𝜅2)
L
−−−→N(0, 𝜎2
̂𝜅2z),
3)
√
n(̂𝜅3z −𝜅3)
L
−−−→N(0, 𝜎2
̂𝜅3z),
where
𝜎2
̂𝜅z = g𝜏
𝜅(H(X), H(Y), H(X, Y)) ΣH g𝜅(H(X), H(Y), H(X, Y)),
𝜎2
̂𝜅2z = g𝜏
𝜅2(H(X), H(Y), H(X, Y)) ΣH g𝜅2(H(X), H(Y), H(X, Y)),
𝜎2
̂𝜅3z = g𝜏
𝜅3(H(X), H(Y), H(X, Y)) ΣH g𝜅3(H(X), H(Y), H(X, Y)),
and ΣH is as in (5.55).
Proof: An application of the delta method to (5.67) completes the proof.
◽
Corollary 5.6
Under the conditions of Theorem 5.8,
1)
√
n(̂𝜅z −𝜅)
̂𝜎̂𝜅z
L
−−−→N(0, 1),
2)
√
n(̂𝜅z2 −𝜅2)
̂𝜎̂𝜅2z
L
−−−→N(0, 1),
3)
√
n(̂𝜅z3 −𝜅3)
̂𝜎̂𝜅3z
L
−−−→N(0, 1),
where
̂𝜎2
̂𝜅z = g𝜏
𝜅( ̂Hz(X), ̂Hz(Y), ̂Hz(X, Y)) ̂ΣH g𝜅( ̂Hz(X), ̂Hz(Y), ̂Hz(X, Y)),
̂𝜎2
̂𝜅2z = g𝜏
𝜅2( ̂Hz(X), ̂Hz(Y), ̂Hz(X, Y)) ̂ΣH g𝜅2( ̂Hz(X), ̂Hz(Y), ̂Hz(X, Y)),
̂𝜎2
̂𝜅3z = g𝜅3( ̂Hz(X), ̂Hz(Y), ̂Hz(X, Y)) ̂ΣH g𝜏
𝜅3( ̂Hz(X), ̂Hz(Y), ̂Hz(X, Y)),
and ̂ΣH is as in (5.61).
All the plug-in estimators of MI, 𝜅, 𝜅2, and 𝜅3 in this section are asymp-
totically eﬃcient since they are all maximum likelihood estimators. All the
estimators based on Turing’s perspective in this section are also asymptotically
eﬃcient because each of them has the identical asymptotic variance as that of
its plug-in counterpart.
5.2.4
An Illustrative Example
A random sample of size n = 1000 adults in the United States were selected.
The favorite color (among K1 = 11 choices) and the ethnicity (among K2 = 4
choices) for each individual in the selected group were obtained. The survey
resulted in the data set in Table 5.1. Letting X be the color preference
and Y be the ethnicity of a randomly selected individual, i takes values in

Estimation of Information
177
Table 5.1 Frequency data of ethnicity and color
preference.
Black
Asian
White
Hispanic
Black
15
5
46
19
Blue
48
19
285
59
Brown
5
0
7
2
Gray
4
1
7
3
Green
15
4
99
19
Orange
0
3
13
5
Pink
1
3
26
3
Purple
20
4
85
30
Red
15
1
66
25
White
0
1
0
0
Yellow
3
4
26
4
{1,…, 11 = K1} corresponding to colors Black to Yellow in the given vertical
order in Table 5.1, j takes values in {1,…, 4 = K2} corresponding to ethnic
groups Black to Hispanic in the given horizontal order of the same table, and
K = K1K2 = 11 × 4 = 44 assuming pi,j > 0 for every pair (i, j). The entries of
Table 5.1 are observed frequencies, fi,j, for example, f1,1 = 15.
In practice, n may not necessarily be large enough to ensure that ̂pi,j > 0 for
every pair of (i, j), for which pi,j > 0. In fact, the data set in Table 5.1 contains
ﬁve such cells, (6, 1), (10, 1), (3, 2), (10, 3), and (10, 4). When this is the case,
estimating the positive deﬁnite covariance matrix Σ = Σ(v) of (5.50) by means
of the plug-in ̂Σ = ̂Σ(v) fails to produce a positive deﬁnite matrix, which in turn
may fail to produce a useful estimated ΣH of (5.55) by means of the plug-in
̂ΣH = ̂A ̂Σ ̂A𝜏of (5.61), where ̂A is as in (5.60).
To account for such a situation, many techniques may be employed. For
example, one may choose to plug an augmented ̂v into Σ(v), resulting in
̂Σ = Σ(̂v∗), where
̂v∗= (̂p∗
1,1,…, ̂p∗
1,K2, ̂p∗
2,1,…, ̂p∗
2,K2,…, ̂p∗
K1,1,…, ̂p∗
K1,K2−1)𝜏
where, for any pair of (i, j),
̂p∗
i,j = ̂pi,j + (1∕n)1[̂pi,j = 0].
(5.76)
Clearly such an augmentation does not impact any of the asymptotic properties
established above but does resolve the computational issue.
In the same conﬁguration as that of Table 5.1, the observed joint and marginal
distributions are given in Table 5.2.

178
Statistical Implications of Turing’s Formula
Table 5.2 Relative frequency data of ethnicity and
color preference.
̂pi,j
j = 1
j = 2
j = 3
j = 4
̂pi,⋅
i = 1
0.015
0.005
0.046
0.019
0.085
i = 2
0.048
0.019
0.285
.0059
0.411
i = 3
0.005
0.000
0.007
0.002
0.014
i = 4
0.004
0.001
0.007
0.003
0.015
i = 5
0.015
0.004
0.099
0.019
0.137
i = 6
0.000
0.003
0.013
0.005
0.021
i = 7
0.001
0.003
0.026
0.003
0.033
i = 8
0.020
0.004
0.085
0.030
0.139
i = 9
0.015
0.001
0.066
0.025
0.107
i = 10
0.000
0.001
0.000
0.000
0.001
i = 11
0.003
0.004
0.026
0.004
0.037
̂p⋅,j
0.126
0.045
0.660
0.169
1.000
By (5.39)–(5.41) and (5.56),
̂H(X) = 1.8061,
(5.77)
̂H(Y) = 0.9753,
(5.78)
̂H(X, Y) = 2.7534, and
(5.79)
̂
MI = 0.0280.
(5.80)
Further by (5.70)–(5.72),
̂𝜅= 0.0102, ̂𝜅2 = 0.0211,
and
̂𝜅3 = 0.0201.
(5.81)
Letting pi,j be enumerated as {pk; k = 1, 2,…, K} by counting the probabilities
from left to right in each row, one has the K = K1K2 = 44 dimensional vector
̂p = (0.015, 0.005, 0.046, 0.019, 0.048,…, 0.026, 0.004)𝜏,
and the K −1 = 43 dimensional vector
̂v = (0.015, 0.005, 0.046, 0.019, 0.048,…, 0.026)𝜏.
Substituting the zeros in ̂v by 1∕n = 0.001 according to (5.76), at ̂v10 = 0,
̂v21 = 0, ̂v37 = 0, ̂v39 = 0, and ̂v40 = 0, one has ̂v∗given in Table 5.3.

Estimation of Information
179
Table 5.3 ̂v∗for data in Table 5.2.
̂v∗= ( 0.015,
0.005,
0.046,
0.019,
0.048,
0.019,
0.285,
0.059,
0.005,
0.001,
0.007,
0.002,
0.004,
0.001,
0.007,
0.003,
0.015,
0.004,
0.099,
0.019,
0.001,
0.003,
0.013,
0.005,
0.001,
0.003,
0.026,
0.003,
0.020,
0.004,
0.085,
0.030,
0.015,
0.001,
0.066,
0.025,
0.001,
0.001,
0.001,
0.001,
0.003,
0.004,
0.026
)𝜏
Letting ̂A = A(̂v∗) by means of (5.45), the three rows of ̂A are
̂a𝜏
1 = (
−0.8317,
−0.8317,
−0.8317,
−0.8317,
−2.4077,
−2.4077,
−2.4077,
−2.4077,
0.9719,
0.9719,
0.9719,
0.9719,
0.9029,
0.9029,
0.9029,
0.9029,
−1.3091,
−1.3091,
−1.3091,
−1.3091,
0.5664,
0.5664,
0.5664,
0.5664,
0.1144,
0.1144,
0.1144,
0.1144,
−1.3236,
−1.3236,
−1.3236,
−1.3236,
−1.0619,
−1.0619,
−1.0619,
−1.0619,
3.6109,
3.6109,
3.6109,
3.6109,
0.0000,
0.0000,
0.0000
);
̂a𝜏
2 = (
0.2936
1.3232
−1.3623
0.0000,
0.2936
1.3232
−1.3623
0.0000,
0.2936
1.3232
−1.3623
0.0000,
0.2936
1.3232
−1.3623
0.0000,
0.2936
1.3232
−1.3623
0.0000,
0.2936
1.3232
−1.3623
0.0000,
0.2936
1.3232
−1.3623
0.0000,
0.2936
1.3232
−1.3623
0.0000,
0.2936
1.3232
−1.3623
0.0000,
0.2936
1.3232
−1.3623
0.0000,
0.2936
1.3232
−1.3623
);

180
Statistical Implications of Turing’s Formula
and
̂a𝜏
3 = (
−1.3216,
−0.2231,
−2.4423,
−1.5581,
−2.4849,
−1.5581,
−4.2662,
−2.6912,
−0.2231,
1.3863,
−0.5596,
0.6931,
0.0000,
1.3863,
−0.5596,
0.2877,
−1.3218,
0.0000,
−3.2088,
−1.5581,
1.3863,
0.2877,
−1.1787,
−0.2231,
1.3863,
0.2877,
−1.8718,
0.2877,
−1.6094,
0.0000,
−3.0564,
−2.0149,
−1.3218,
1.3863,
−2.8034,
−1.8326,
1.3863,
1.3863,
1.3863,
1.3863,
0.2877,
0.0000,
−1.8718
).
By (5.61) and (5.62),
̂ΣH = ̂A ̂Σ ̂A𝜏=
⎛
⎜
⎜⎝
0.9407 0.0661 0.9364
0.0661 0.6751 0.7041
0.9364 0.7041 1.5927
⎞
⎟
⎟⎠
,
̂𝜎2 = (1, 1, −1) ̂ΣH(1, 1, −1)𝜏= 0.0597.
Based on the plug-in estimator of MI, an approximate 1 −𝛼= 90% conﬁ-
dence interval for MI is
̂
MI ± z𝛼∕2
̂𝜎
√
n
= 0.0280 ± 1.645
√
0.0597
√
1000
= (0.0153, 0.0407).
Noting
g𝜅( ̂H(X), ̂H(Y), ̂H(X, Y)) = (0.3632, 0.3632, −0.3669)𝜏,
g𝜅2( ̂H(X), ̂H(Y), ̂H(X, Y)) = (0.7477, 0.7477, −0.7535)𝜏,
g𝜅3( ̂H(X), ̂H(Y), ̂H(X, Y)) = (0.7119, 0.7119, −0.7191)𝜏,
and based on Corollary 5.5,
̂𝜎2
̂𝜅( ̂H(X), ̂H(Y), ̂H(X, Y)) = 0.0078,
̂𝜎2
̂𝜅2( ̂H(X), ̂H(Y), ̂H(X, Y)) = 0.0330,
̂𝜎2
̂𝜅3( ̂H(X), ̂H(Y), ̂H(X, Y)) = 0.0298.
Based on the plug-in estimators of 𝜅, 𝜅2, and 𝜅3, the following are their respec-
tive approximate 1 −𝛼= 90% conﬁdence intervals:
̂𝜅± z𝛼∕2
̂𝜎̂𝜅
√
n
= 0.0102 ± 1.645
√
0.0081
√
1000
= (0.0056, 0.0148),
̂𝜅2 ± z𝛼∕2
̂𝜎̂𝜅2
√
n
= 0.0211 ± 1.645
√
0.0341
√
1000
= (0.0116, 0.0306),

Estimation of Information
181
̂𝜅3 ± z𝛼∕2
̂𝜎̂𝜅3
√
n
= 0.0201 ± 1.645
√
0.0298
√
1000
= (0.0111, 0.0291).
On the other hand, the corresponding estimators based on Turing’s
perspective are
̂Hz(X) = 1.8112,
(5.82)
̂Hz(Y) = 0.9768,
(5.83)
̂Hz(X, Y) = 2.7731, and
(5.84)
̂
MIz = 0.0148.
(5.85)
Further by (5.70)–(5.72), but with ̂Hz(X), ̂Hz(Y), and ̂Hz(X, Y), in places of
̂H(X), ̂H(Y), and ̂H(X, Y), respectively,
̂𝜅z = 0.0054,
̂𝜅2z = 0.0112,
and
̂𝜅3z = 0.0106.
(5.86)
Noting
g𝜅( ̂Hz(X), ̂Hz(Y), ̂Hz(X, Y)) = (0.3606, 0.3606, −0.3625)𝜏,
g𝜅2( ̂Hz(X), ̂Hz(Y), ̂Hz(X, Y)) = (0.7488, 0.7488, −0.7518)𝜏,
g𝜅3( ̂Hz(X), ̂HzY), ̂Hz(X, Y)) = (0.7136, 0.7136, −0.7174)𝜏,
and based on Corollary 5.6,
̂𝜎2
̂𝜅z( ̂Hz(X), ̂Hz(Y), ̂Hz(X, Y)) = 0.0077,
̂𝜎2
̂𝜅2z( ̂Hz(X), ̂Hz(Y), ̂Hz(X, Y)) = 0.0333,
̂𝜎2
̂𝜅3z( ̂Hz(X), ̂Hz(Y), ̂Hz(X, Y)) = 0.0302.
The approximate 1 −𝛼= 90% conﬁdence intervals for MI, 𝜅, 𝜅2, and 𝜅3 based
on Turing’s perspective are
̂
MIz ± z𝛼∕2
̂𝜎
√
n
= 0.0148 ± 1.645
√
0.0597
√
1000
= (0.0021, 0.0275),
̂𝜅z ± z𝛼∕2
̂𝜎̂𝜅z
√
n
= 0.0054 ± 1.645
√
0.0077
√
1000
= (0.0008, 0.0100),
̂𝜅2z ± z𝛼∕2
̂𝜎̂𝜅2z
√
n
= 0.0112 ± 1.645
√
0.0333
√
1000
= (0.0017, 0.0207),
̂𝜅3z ± z𝛼∕2
̂𝜎̂𝜅3z
√
n
= 0.0106 ± 1.645
√
0.0302
√
1000
= (0.0016, 0.0196).
In addition to the illustration of the example, at least two points are worth
noting. First, the sample data suggest a very low level of association between

182
Statistical Implications of Turing’s Formula
ethnicity and color preference, as measured by MI and its derivatives, and yet
the association is supported with fairly strong statistical evidence.
Second, it may be of interesting to note again that the plug-in estimator of
entropy, ̂H, has a much larger negative bias compared to ̂Hz, which also has a
negative bias. Consequently, the entropy estimators in the left block are con-
sistently lower than those in the right block below.
ˆH(X)
1.8061
ˆH(Y )
0.9753
ˆH(X, Y )
2.7534
ˆHz(X)
1.8112
ˆHz(Y )
0.9768
ˆHz(X, Y )
2.7731
On the other hand, due to the dominance of the negative bias of the plug-in
estimator ̂H(X, Y), ̂
MI has a pronounced positive bias as numerically evidenced
by the following blocks in comparison.

MI
0.0280
ˆκ
0.0102
ˆκ2
0.0211
ˆκ3
0.0201

MIz
0.0148
ˆκz
0.0054
ˆκ2z
0.0112
ˆκ3z
0.0106
For more examples regarding the over-estimation of mutual information
using ̂
MI and ̂
MIz, readers may wish to refer to Zhang and Zheng (2015).
5.3
Estimation of Kullback–Leibler Divergence
Let p = {pk} and q = {qk} be two discrete probability distributions on a same
ﬁnite alphabet, 𝒳= {𝓁k; k = 1,…, K}, where K ≥2 is a ﬁnite integer. Suppose
it is of interest to estimate the Kullback–Leibler divergence, also known as rel-
ative entropy,
D = D(p||q)
(5.87)
=
K
∑
k=1
pk ln( pk∕qk)
=
K
∑
k=1
pk ln( pk) −
K
∑
k=1
pk ln(qk).
Here and throughout, the standard conventions that 0 log (0∕q) = 0 if q ≥0
and p ln(p∕0) = ∞if p > 0 are adopted. The Kullback–Leibler divergence is
a measure of the diﬀerence between two probability distributions, which was

Estimation of Information
183
ﬁrst introduced by Kullback and Leibler (1951), and is an important measure
of information in information theory. This measure is not a metric since it
does not satisfy the triangle inequality and it is not symmetric. A symmetric
modiﬁcation of Kullback–Leibler divergence is considered in Section 5.3.3
below.
Assume that two independent iid samples of sizes n and m are to be available,
respectively, according to two unknown distributions p and q.
Let
{
X1,
X2,
· · · ,
XK
}
{
Y1,
Y2,
· · · ,
YK
}
be the sequences of observed frequencies of letters {𝓁1, 𝓁2,…, 𝓁K} in the two
samples, respectively, and let
̂p = {̂pk} = {
X1∕n,
X2∕n,
· · · ,
XK∕n
}
̂q = {̂qk} = {
Y1∕m,
Y2∕m,
· · · ,
YK∕m
}
be the sequences of corresponding relative frequencies.
For simplicity, the following is further imposed.
Condition 5.1
The probability distributions, p and q, and the observed sam-
ple distributions, ̂p and ̂q, satisfy
A1. pk > 0 and qk > 0 for each k = 1,…, K, and
A2. there exists a 𝜆∈(0, ∞) such that n∕m →𝜆, as n →∞.
Note that it is not assumed that K is known, only that it is some ﬁnite integer
greater than or equal to two.
Perhaps the most intuitive estimator of D(p||q) is the “plug-in” estimator
given by
̂D = ̂D(p||q) =
K
∑
k=1
̂pk ln(̂pk) −
K
∑
k=1
̂pk ln(̂qk).
(5.88)
In Section 5.3.1, it is shown that this estimator is consistent and asymptotically
normal, but with an inﬁnite bias. Then a modiﬁcation of this estimator is
introduced and is shown that it is still consistent and asymptotically normal,
but now with a ﬁnite bias. It is also shown that this bias decays no faster than
𝒪(1∕n). In Section 5.3.2, the following estimator of D(p||q),
̂D♯= ̂D♯(p||q)
=
K
∑
k=1
̂pk
[m−Yk
∑
v=1
1
v
v
∏
j=1
(
1 −
Yk
m −j + 1
)
−
n−Xk
∑
v=1
1
v
v
∏
j=1
(
1 −Xk −1
n −j
)]
,
(5.89)
based on Turing’s perspective and proposed by Zhang and Grabchak (2014),
is discussed and is shown to have asymptotic consistency and normality.

184
Statistical Implications of Turing’s Formula
This estimator is also shown to have a bias decaying exponentially fast in sam-
ple sizes n and m. Finally, in Section 5.3.3, the estimation of the symmetrized
Kullback–Leibler divergence is discussed.
Before proceeding let
A =
K
∑
k=1
pk ln(pk)
and
B = −
K
∑
k=1
pk ln(qk),
(5.90)
and note that D(p||q) = A + B. The ﬁrst term, A, is the negative of the entropy
of the distribution of p. In the subsequent text of this section, an estimator
of D(p||q) is occasionally expressed as the sum of an estimator of A and an
estimator of B.
5.3.1
The Plug-In Estimator
In this section, it is shown that ̂D, the plug-in estimator given in (5.88), is a
consistent and asymptotically normal estimator of D, but with an inﬁnite bias.
Deﬁne the (2K −2)-dimensional vectors
v = (p1,…, pK−1, q1,…, qK−1)𝜏and ̂v = (̂p1,…, ̂pK−1, ̂q1,…, ̂qK−1)𝜏,
and note that ̂v
p→v as n →∞. Moreover, by the multivariate normal approxi-
mation to the multinomial distribution
√
n(̂v −v)
L
−−−→MVN(0, Σ(v)),
(5.91)
where Σ(v) is the (2K −2) × (2K −2) covariance matrix given by
Σ(v) =
(
Σ1(v) 0
0
Σ2(v)
)
.
(5.92)
Here Σ1(v) and Σ2(v) are (K −1) × (K −1) matrices given by
Σ1(v) =
⎛
⎜
⎜
⎜⎝
p1(1 −p1)
−p1p2
· · ·
−p1pK−1
−p2 p1
p2(1 −p2) · · ·
−p2 pK−1
· · ·
· · ·
· · ·
· · ·
−pK−1p1
−pK−1p2
· · · pK−1(1 −pK−1)
⎞
⎟
⎟
⎟⎠
and
Σ2(v) = 𝜆
⎛
⎜
⎜
⎜⎝
q1(1 −q1)
−q1q2
· · ·
−q1qK−1
−q2q1
q2(1 −q2) · · ·
−q2qK−1
· · ·
· · ·
· · ·
· · ·
−qK−1q1
−qK−1q2
· · · qK−1(1 −qK−1)
⎞
⎟
⎟
⎟⎠
.

Estimation of Information
185
Let
G(v) =
K−1
∑
k=1
pk(ln pk −ln qk)
+
(
1 −
K−1
∑
k=1
pk
) [
ln
(
1 −
K−1
∑
k=1
pk
)
−ln
(
1 −
K−1
∑
k=1
qk
)]
(5.93)
and
g(v) ∶= ∇G(v)
=
(
∂
∂p1
G(v),…,
∂
∂pK−1
G(v), ∂
∂q1
G(v),…,
∂
∂qK−1
G(v)
)𝜏
.
(5.94)
For each j, j = 1,…, K −1,
∂
∂pj
G(v) = ln pj −ln qj −
[
ln
(
1 −
K−1
∑
k=1
pk
)
−ln
(
1 −
K−1
∑
k=1
qk
)]
= ln pj −ln qj −(ln pK −ln qK)
= ln
pj
qj
−ln pK
qK
−
pj
qj
+ 1 −∑K−1
k=1 pk
1 −∑K−1
k=1 qk
= −
pj
qj
+ pK
qK
.
The delta method immediately gives the following result.
Theorem 5.9
Provided that g𝜏(v)Σ(v)g(v) > 0,
√
n( ̂D −D)
√
g𝜏(v)Σ(v)g(v)
L
−−−→N(0, 1).
(5.95)
By the continuous mapping theorem, Σ(̂v) is a consistent estimator of Σ(v)
and g(̂v) is a consistent estimator of g(v). Corollary 5.7 follows from Slutsky’s
theorem.
Corollary 5.7
Provided that g𝜏(v)Σ(v)g(v) > 0,
√
n( ̂D −D)
̂𝜎̂D
L
−−−→N(0, 1),
(5.96)
where ̂𝜎2
̂D = g𝜏(̂v)Σ(̂v)g(̂v).
Although ̂D is a consistent and asymptotically normal estimator of D, it has
an inﬁnite bias. To see this, consider P({̂pk > 0} ∩{̂qk = 0}) > 0 for every k,
k = 1,…, K. For each of such k, E(̂pk ln(̂qk)) = ∞. The problem is clearly caused

186
Statistical Implications of Turing’s Formula
by the fact that ̂qk may be zero even though qk > 0. To deal with this minor issue,
the following augmentation is added:
̂q∗
k = ̂qk + 1[Yk = 0]
m
,
(5.97)
where k = 1,…, K.
5.3.2
Properties of the Augmented Plug-In Estimator
Deﬁne an augmented plug-in estimator of D by
̂D∗=
K
∑
k=1
̂pk ln( ̂pk) −
K
∑
k=1
̂pk ln(̂q∗
k) =∶̂A + ̂B∗.
(5.98)
It is shown that ̂D∗is a consistent and asymptotically normal estimator of D,
with a bias that decays no faster than 𝒪(1∕n).
Theorem 5.10
Provided that g𝜏(v)Σ(v)g(v) > 0,
√
n( ̂D∗−D)
√
g𝜏(v)Σ(v)g(v)
L
−−−→N(0, 1).
(5.99)
Proof: In light of Theorem 5.9, it suﬃces to show that
√
n( ̂D∗−̂D)
p→0. The
fact that, for any ε > 0,
P(|
√
n( ̂D∗−̂D)| > ε) ≤P
( K
⋃
k=1
[Yk = 0]
)
≤
K
∑
k=1
P(Yk = 0)
=
K
∑
k=1
(1 −qk)m →0,
gives the result.
◽
Arguments similar to these for Corollary 5.7 give the following corollary.
Corollary 5.8
Provided that g𝜏(v)Σ(v)g(v) > 0,
√
n( ̂D∗−D)
̂𝜎̂D∗
L
−−−→N(0, 1),
(5.100)
where ̂𝜎2
̂D∗= g𝜏(̂v∗)Σ(̂v∗)g(̂v∗) and ̂v∗= (̂p1,…, ̂pK−1, ̂q∗
1,…, ̂q∗
K−1)𝜏.
Example 5.10
A commonly used special case is when K = 2. In this case, p
and q are both Bernoulli distributions with probabilities of success being p and
q (note that q ≠1 −p), respectively. In this case,
v = (p, q)𝜏,

Estimation of Information
187
Σ(v) =
(
p(1 −p)
0
0
𝜆q(1 −q)
)
,
g(v) =
(
ln
(p(1 −q)
q(1 −p)
)
,
q −p
q(1 −q)
)𝜏
,
and 𝜎2 ∶= g𝜏(v)Σ(v)g(v) reduces to
𝜎2 = p(1 −p)
[
ln
(p(1 −q)
q(1 −p)
)]2
+ 𝜆(p −q)2
q(1 −q) .
(5.101)
By Theorems 5.9 and 5.10, it follows that
√
n( ̂D −D)
L
−−−→N(0, 𝜎2)
and
√
n( ̂D∗−D)
L
−−−→N(0, 𝜎2).
Next, consider the bias of ̂D∗. In Miller (1955) (see also Paninski (2003) for a
more formal treatment), it is shown that
E( ̂A) −A ≥0
and
E( ̂A) −A = 𝒪(1∕n).
(5.102)
It will be shown below that, for suﬃciently large n,
E(̂B∗) −B ≥0
and
E(̂B∗) −B ≥𝒪(1∕n).
(5.103)
Once this is established, so is the following theorem.
Theorem 5.11
For a suﬃciently large n,
E( ̂D∗) −D ≥0
and
E( ̂D∗) −D ≥𝒪(1∕n).
(5.104)
Before proving (5.103), and hence the theorem, the following result for
general “plug-in” estimators of B is ﬁrst given.
Lemma 5.7
Let ̃qk be any estimator of qk such that ̃qk is independent of ̂pk and
0 < ̃qk ≤2 a.s. for all k = 1,…, K. If ck = E(̃qk) and
̃B = −
K
∑
k=1
̂pk ln ̃qk
then
E(̃B) −B =
K
∑
k=1
pk
[
ln(qk∕ck) + 1
2Var(̃qk) + Rk
]
,
where
Rk =
∞
∑
v=3
1
vE[(1 −̃qk)v −(1 −ck)v)].
If, in addition, 0 < ̃qk ≤1 a.s., then Rk ≥0 a.s.

188
Statistical Implications of Turing’s Formula
Proof: By the Taylor expansion of the logarithm
E(ln(ck) −ln(̃qk)) =
∞
∑
v=1
1
vE((1 −̃qk)v −(1 −ck)v) = 1
2Var(̃qk) + Rk.
Since ̂pk is an unbiased estimator of pk and it is independent of ̃qk
E(̃B) −B =
K
∑
k=1
pkE(ln(qk) −ln(ck) + ln(ck) −ln(̃qk))
=
K
∑
k=1
pk
(
ln(qk∕ck) + 1
2Var(̃qk) + Rk
)
.
If 0 < ̃qk ≤1, then Jensen’s inequality implies that
E((1 −̃qk)v) ≥(E(1 −̃qk))v = (1 −ck)v,
and thus Rk ≥0.
◽
Proof of Theorem 5.11: It suﬃces to show that both inequalities in (5.103)
are
true.
Note
that
Yk ∼Bin(m, qk)
and
ck ∶= E(̂q∗
k) = qk + ek,
where
ek = (1 −qk)m∕m. By Lemma 5.7,
E(̂B∗
n) −B =
K
∑
k=1
pk
[
ln
(
qk
qk + ek
)
+ 1
2Var(̂q∗
k)
+
∞
∑
v=3
1
vE((1 −̂q∗
k)v −(1 −qk −ek)v)
]
.
Note that
1
2Var(̂q∗
k) = Var(Yk) + Var(1[Yk = 0]) + 2Cov(Yk, 1[Yk = 0])
2m2
= qk(1 −qk)
2m
+
1
2m2
[(1 −qk)m −(1 −qk)2m −2mqk(1 −qk)m]
= qk(1 −qk)
2m
+ ek
2m −
e2
k
2 −qkek,
and for suﬃciently large m, by the Taylor expansion of the logarithm,
ln
(
qk
qk + ek
)
= −ln(1 + ek∕qk) = −ek∕qk + rk,
where the remainder term rk ≥0 by properties of alternating series. This means
that, for a suﬃciently large m,
E(̂B∗) −B =
K
∑
k=1
pk
(
−ek(qk + 1∕qk) + rk + qk(1 −qk)
2m
+ ek
2m −
e2
k
2
+
∞
∑
v=3
1
vE((1 −̃qk)v −(1 −qk −ek)v)
)
,

Estimation of Information
189
where the only negative terms are −ek(qk + 1∕qk) and −e2
k∕2. But
ek
2m −
e2
k
2 ≥0
and for suﬃciently large m
qk(1 −qk)
2m
−ek(qk + 1∕qk) = qk(1 −qk)
4m
+ ek(qk + 1∕qk)
(
qk(1 −qk)
4(qk + 1∕qk)mek
−1
)
≥qk(1 −qk)
4m
,
where the ﬁnal inequality follows by the fact that limm→∞mek = 0. Thus, for
suﬃciently large m
E(̂B∗
n) −B ≥
1
4m
K
∑
k=1
pkqk(1 −qk) = 𝒪(1∕m) = 𝒪(1∕n),
which completes the proof.
◽
5.3.3
Estimation in Turing’s Perspective
In this section, the estimator, ̂D♯, of D as given in (5.89) is motivated, and is
shown to have asymptotic normality and a bias decaying exponentially fast in
sample size. Recall that D(p||q) = A + B. The estimator of D(p||q) is derived by
that of A and that of B separately.
Since A is the negative of the entropy of p, it can be estimated by the negative
of an entropy estimator, speciﬁcally the negative of the estimator based on
Turing’s perspective, −̂Hz, which has exponentially decaying bias. This leads
to the estimator
̂A♯= −
n−1
∑
v=1
1
v Z1,v,
(5.105)
where
Z1,v = n1+v[n −(1 + v)]!
n!
K
∑
k=1
[
̂pk
v−1
∏
j=0
(
1 −̂pk −j
n
)]
.
(5.106)
Let B1 ∶= E( ̂A♯) −A. It is demonstrated in Chapter 3 that
0 ≤B1 =
∞
∑
v=n
1
v
K
∑
k=1
pk(1 −pk)v ≤𝒪(n−1(1 −p∧)n),
(5.107)
and therefore the bias decays exponentially fast in n.

190
Statistical Implications of Turing’s Formula
Remark 5.2
The product term in (5.106) assumes a value of 0 when
v ≥n −Xk + 1. This is because if v ≥n −Xk + 1, then when j = n −Xk one has
(1 −̂pk −j∕n) = 0. Combining this observation with some basic algebra gives
̂A♯= −
K
∑
k=1
̂pk
n−Xk
∑
v=1
1
v
v
∏
j=1
(
1 −Xk −1
n −j
)
.
(5.108)
See Exercise 9.
Next an estimator of B is derived. By the Taylor expansion of the logarithmic
function
B =
m
∑
v=1
1
v
K
∑
k=1
pk(1 −qk)v +
∞
∑
v=m+1
1
v
K
∑
k=1
pk(1 −qk)v
=∶𝜂m + B2,m,
(5.109)
where m is the size of the second sample.
First, for each v, v = 1,…, m, an unbiased estimator of (1 −qk)v is derived.
For each ﬁxed k, k = 1,…, K, and each ﬁxed v, v = 1,…, m, consider a
subsample of size v of the sample of size-m from q. Let {Y∘
1 ,…, Y∘
K} and
{̂q∘
1,…, ̂q∘
K} be the counts and the relative frequencies of the subsample.
Note that E(1[Y∘
k = 0]) = (1 −qk)v, thus 1[Y∘
k = 0] is an unbiased estimator
of (1 −qk)v. Since there are
(
m
v
)
diﬀerent subsamples of size v ≤m from the
sample of size m, the U-statistic
Uk,v =
(m
v
)−1 ∑
∗
1[Y∘
k = 0],
(5.110)
where the sum, ∑
∗, is taken over all possible subsamples of size v ≤m, is also
an unbiased estimator of (1 −qk)v. Note that ∑
∗1[Y∘
k = 0] is simply counting
the number of subsamples of size v in which 𝓁k, the kth letter of the alphabet,
is missing. This count can be re-expressed and summarized in the following
cases:
1) if v ≤m −Yk, the count is
(m −Yk
v
)
= mv
v!
v−1
∏
j=0
(
1 −̂qk −j
m
)
,
(5.111)
2) if v ≥m −Yk + 1, the count is zero.
Since, in the second case,
v−1
∏
j=0
(
1 −̂qk −j
m
)
= 0,

Estimation of Information
191
a general form for both cases becomes
Uk,v =
(m
v
)−1 mv
v!
v−1
∏
j=0
(
1 −̂qk −j
m
)
=
v−1
∏
j=0
(
1 −
Yk
m −j
)
.
(5.112)
Noting that ̂pk is an unbiased estimator of pk and it is independent of Uk,v, for
each v, 1 ≤v ≤m,
K
∑
k=1
̂pkUk,v
is an unbiased estimator of ∑K
k=1 pk(1 −qk)v.
This leads to an estimator of B given by
̂B♯=
m
∑
v=1
1
v
K
∑
k=1
̂pkUk,v.
(5.113)
By construction E(̂B♯) = 𝜂m, and by (5.109) the bias B2,m = B −E(̂B♯) satisﬁes
0 ≤B2,m ≤
1
m + 1
∞
∑
v=m+1
(1 −q∧)v = (1 −q∧)m+1
(m + 1)q∧
,
(5.114)
where q∧= min{q1,…, qK}. Thus, the bias of ̂B♯decays exponentially in m, and
hence in n.
Combining the fact that the product term in (5.112) assumes a value of 0 for
any v ≥m −Yk + 1 and some basic algebra shows that
̂B♯=
K
∑
k=1
̂pk
m−Yk
∑
v=1
1
v
v
∏
j=1
(
1 −
Yk
m −j + 1
)
.
(5.115)
Let an estimator of D(p||q) be given by
̂D♯= ̂A♯+ ̂B♯.
(5.116)
Combining (5.108) and (5.115), it can be shown that this estimator is as in
(5.89). By (5.107) and (5.114), the bias of ̂D♯
n is given by
E
(
̂D♯
n −D
)
= B1,n −B2,m
=
K
∑
k=1
pk
( ∞
∑
v=n
1
v(1 −pk)v −
∞
∑
v=m+1
1
v(1 −qk)v
)
.
(5.117)
Since the bias in parts decays exponentially fast, the bias in whole decays at
least exponentially as well. In particular when the distributions and the sample
sizes are similar, the bias tends to be smaller.
Two normal laws of ̂D♯are given in Theorem 5.12 and Corollary 5.9.

192
Statistical Implications of Turing’s Formula
Theorem 5.12
Provided that g𝜏(v)Σ(v)g(v) > 0,
√
n( ̂D♯−D)
√
g𝜏(v)Σ(v)g(v)
L
−−−→N(0, 1).
Corollary 5.9
Provided that g𝜏(v)Σ(v)g(v) > 0,
√
n( ̂D♯−D)
̂𝜎̂D♯
L
−−−→N(0, 1),
where ̂𝜎2
̂D♯= g𝜏(̂v∗)Σ(̂v∗)g(̂v∗) and ̂v∗is as in Corollary 5.8.
Given Theorem 5.12, the proof of Corollary 5.9 is similar to that of
Corollary 5.7.
Toward proving Theorem 5.12, Lemmas 5.8 and 5.9 are needed.
Lemma 5.8
If 0 < a < b < 1, then for any integer m ≥0,
bm −am ≤mbm−1(b −a).
Proof: Noting that f (x) = xm is a convex function on the interval (0, 1) and
f ′(b) = mbm−1, the result follows immediately.
◽
Lemma 5.9
If Y ∼Bin(n, p) and X = Y + 1[Y = 0], then
lim sup
n→∞
E(n∕X) < ∞.
Proof: Let
hn(x) =
x + 1
x(n −x)
for x = 1, 2,…, n −1. Note that hn attains its maximum at a value xn, which is
either ⌊
√
n + 1⌋−1 or ⌊
√
n + 1⌋(here ⌊⋅⌋is the ﬂoor of a nonnegative value).
Clearly xn = 𝒪(
√
n) and hn(xn) = 𝒪(n−1). Therefore,
E(n∕X) = n(1 −p)n + n
n
∑
x=1
1
x
(n
x
)
px(1 −p)n−x
= n(1 −p)n + pn
+ n
n−1
∑
x=1
n!
(x + 1)!(n −x −1)!
x + 1
x(n −x)px(1 −p)n−x
≤n(1 −p)n + pn
+ nhn(xn)1 −p
p
n−1
∑
x=1
n!
(x + 1)![n −(x + 1)]!px+1(1 −p)n−(x+1)

Estimation of Information
193
= n(1 −p)n + pn + nhn(xn)1 −p
p
n
∑
x=2
n!
x!(n −x)!px(1 −p)n−x
= n(1 −p)n + pn + nhn(xn)1 −p
p
[1 −(1 −p)n −np(1 −p)n−1].
From here, the result follows.
◽
A proof of Theorem 5.12 is given in the appendix of this chapter.
5.3.4
Symmetrized Kullback–Leibler Divergence
The symmetrized Kullback–Leibler divergence is deﬁned to be
S = S(p, q) = 1
2(D(p||q) + D(q||p))
(5.118)
= 1
2
( K
∑
k=1
pk ln pk −
K
∑
k=1
pk ln qk
)
+ 1
2
( K
∑
k=1
qk ln qk −
K
∑
k=1
qk ln pk
)
.
The plug-in estimator of S(p, q) is deﬁned by
̂Sn = ̂Sn(p, q) = 1
2
( ̂Dn(p||q) + ̂Dm(q||p))
(5.119)
= 1
2
( K
∑
k=1
̂pk ln ̂pk −
K
∑
k=1
̂pk ln ̂qk
)
+ 1
2
( K
∑
k=1
̂qk ln ̂qk −
K
∑
k=1
̂qk ln ̂pk
)
,
and the augmented plug-in estimator of S(p, q) is deﬁned by
̂S∗
n = ̂S∗
n(p, q) = 1
2
( ̂D∗
n(p||q) + ̂D∗
m(q||p))
(5.120)
= 1
2
( K
∑
k=1
̂pk ln ̂pk −
K
∑
k=1
̂pk ln ̂q∗
k
)
+ 1
2
( K
∑
k=1
̂qk ln ̂qk −
K
∑
k=1
̂qk ln ̂p∗
k
)
where q∗
k is as in (5.97) and
̂p∗
k = ̂pk + 1[xk = 0]∕n
for k = 1,…, K.
Let Gs(v) be a function such that
2Gs(v) =
K−1
∑
k=1
pk(ln pk −ln qk)
+
(
1 −
K−1
∑
k=1
pk
) [
ln
(
1 −
K−1
∑
k=1
pk
)
−ln
(
1 −
K−1
∑
k=1
qk
)]
+
K−1
∑
k=1
qk(ln qk −ln pk)

194
Statistical Implications of Turing’s Formula
+
(
1 −
K−1
∑
k=1
qk
) [
ln
(
1 −
K−1
∑
k=1
qk
)
−ln
(
1 −
K−1
∑
k=1
pk
)]
and let gs(v) = ∇Gs(v).
For each j, j = 1,…, K −1,
∂
∂pj
Gs(v) = 1
2
(
ln
pj
qj
−ln pK
qK
)
−1
2
(qj
pj
−qK
pK
)
and
∂
∂qj
Gs(v) = 1
2
(
ln
qj
pj
−ln qK
pK
)
−1
2
(pj
qj
−pK
qK
)
.
By arguments similar to the proofs of Theorems 5.9 and 5.10 and
Corollaries 5.7 and 5.8, one gets Theorem 5.13 and Corollary 5.10.
Theorem 5.13
Provided that g𝜏
s (v)Σ(v)gs(v) > 0,
√
n(̂S −S)
√
g𝜏
s (v)Σ(v)gs(v)
L
−−−→N(0, 1)
and
√
n(̂S∗−S)
√
g𝜏
s (v)Σ(v)gs(v)
L
−−−→N(0, 1).
Corollary 5.10
Provided that g𝜏
s (v)Σ(v)gs(v) > 0,
√
n(̂S −S)
̂𝜎̂S
L
−−−→N(0, 1),
where ̂𝜎2
̂S = g𝜏
s (̂v)Σ(̂v)gs(̂v), and
√
n(̂S∗−S)
̂𝜎̂S∗
L
−−−→N(0, 1),
where ̂𝜎2
̂S∗= g𝜏
s (̂u∗)Σ(̂u∗)gs(̂u∗) and ̂u∗= (̂p∗
1,…, ̂p∗
K−1, ̂q∗
1,…, ̂q∗
K−1)𝜏.
Example 5.11
Revisiting Example 5.10 where both p and q have Bernoulli
distributions, with probabilities of success being p and q, one has
√
n(̂S −S)
L
−−−→N(0, 𝜎2
s )
and
√
n(̂S∗−S)
L
−−−→N(0, 𝜎2
s ),
where
𝜎2
s = p(1 −p)
4
[
ln
(p(1 −q)
q(1 −p)
)
+
p −q
p(1 −p)
]2
+ 𝜆q(1 −q)
4
[
ln
(q(1 −p)
p(1 −q)
)
+
q −p
q(1 −q)
]2
.

Estimation of Information
195
Remark 5.3
Since ̂D(p||q) has an inﬁnite bias as an estimator of D(p||q) and
̂D(q||p) has an inﬁnite bias as an estimator of D(q||p), it follows that ̂S has an
inﬁnite bias as an estimator of S(p, q). Similarly, from Theorem 5.11, it follows
that ̂S∗has a bias that decays no faster than 𝒪(1∕n).
By modifying the estimator given in (5.89), an estimator of S(p, q), whose bias
decays exponentially fast, may be comprised as follows.
Let
̂S♯= ̂S♯(p, q) = 1
2
( ̂D♯(p||q) + ̂D♯(q||p))
(5.121)
= 1
2
K
∑
k=1
̂pn
[m−Yk
∑
v=1
1
v
v
∏
j=1
(
1 −
Yk
m −j + 1
)
−
n−Xk
∑
v=1
1
v
v
∏
j=1
(
1 −Xk −1
n −j
)]
+ 1
2
K
∑
k=1
̂qn
[n−Xk
∑
v=1
1
v
v
∏
j=1
(
1 −
Xk
n −j + 1
)
−
m−Yk
∑
v=1
1
v
v
∏
j=1
(
1 −Yk −1
m −j
)]
.
By (5.117), the bias of this estimator is
E(̂S♯−S) =
K
∑
k=1
pk
( ∞
∑
v=n
1
v(1 −pk)v −
∞
∑
v=m+1
1
v(1 −qk)v
)
+
K
∑
k=1
qk
( ∞
∑
v=m
1
v(1 −qk)v −
∞
∑
v=n+1
1
v(1 −pk)v
)
,
which decays at least exponentially fast as n, m →∞since each part does.
Theorem 5.14
Provided that g𝜏
s (v)Σ(v)gs(v) > 0,
√
n(̂S♯−S)
√
g𝜏
s (v)Σ(v)gs(v)
L
−−−→N(0, 1).
Proof: By Theorem 5.13 and Slutsky’s theorem, it suﬃces to show that
√
n(̂S♯−̂S∗)
p
−−−→0.
This however follows from the facts that
√
n( ̂D♯(p||q) −̂D∗(p||q))
p
−−−→0
and
√
n( ̂D♯(q||p) −̂D∗(q||p))
p
−−−→0,
which follow from the proof of Theorem 5.12.
◽
By arguments similar to the proof of Corollary 5.7, the following corollary is
established.

196
Statistical Implications of Turing’s Formula
Corollary 5.11
Provided that g𝜏
s (v)Σ(v)gs(v) > 0,
√
n(̂S♯−S)
̂𝜎̂S♯
L
−−−→N(0, 1),
where ̂𝜎2
̂S♯= g𝜏
s (̂u∗)Σ(̂u∗)gs(̂u∗) and ̂u∗is as in Corollary 5.10.
5.4
Tests of Hypotheses
In the preceding sections of this chapter, estimators of various information
indices are discussed and their respective asymptotic normalities are presented
under various conditions. Of MI-related indices MI, 𝜅, 𝜅2, and 𝜅3, the main
estimators discussed are
̂
MI,
̂𝜅,
̂𝜅2,
and
̂𝜅3,
respectively, given in (5.56), (5.70), (5.71), and (5.72), and
̂
MIz,
̂𝜅z,
̂𝜅2z,
and
̂𝜅3z,
respectively, given in (5.64), (5.73), (5.74), and (5.75). Of Kullback–Leibler
divergence indices, D = D(p||q) and S = S(p, q), the main estimators discussed
are
̂D
and
̂S,
respectively, given in (5.88) and (5.119), and
̂D♯
and
̂S♯.
respectively, given in (5.89) and (5.121).
Each of these estimators could lead to a large sample test of hypothesis if
its asymptotic normality holds. The key conditions supporting the asymptotic
normalities are
(1, 1 −1)ΣH(1, 1, −1)𝜏> 0
(5.122)
in Theorem 5.5 for MI, 𝜅, 𝜅2, and 𝜅3, and
g𝜏(v)Σ(v)g(v) > 0
(5.123)
in Theorem 5.9 for D and S. It may be veriﬁed that if X and Y are independent
random elements on joint alphabet 𝒳× 𝒴, that is, MI = 0, then (5.122) is not
satisﬁed, and that if p = q on a same ﬁnite alphabet 𝒳, that is, D = D(p||q) = 0,
then (5.123) is not satisﬁed (see Exercises 14 and 15). This implies that the
asymptotic normalities of this chapter do not directly render valid large sample
tests for hypotheses such as H0 ∶MI = 0 or H0 ∶D = 0. However, they do
support tests for H0 ∶MI = c0 (H0 ∶𝜅= c0, H0 ∶𝜅2 = c0 or H0 ∶𝜅3 = c0)
or H0 ∶D = c0 (H0 ∶S = c0) where c0 > 0 is some appropriately scaled ﬁxed
positive value.

Estimation of Information
197
For completeness as well as conciseness, a general form of such a test is given.
Let (𝜃, ̂𝜃) be any one of the following pairs:
(MI, ̂
MI), (MI, ̂
MIz), (𝜅, ̂𝜅), (𝜅, ̂𝜅z), (𝜅2, ̂𝜅2), (𝜅2, ̂𝜅2z), (𝜅3, ̂𝜅3), (𝜅3, ̂𝜅3z),
(D, ̂D), (D, ̂D∗), (D, ̂D♯), (S, ̂S), (S, ̂S∗), (S, ̂S♯).
Under H0 ∶𝜃= 𝜃0 > 0, the following statistic is approximately a standard
normal random variable:
Z =
√
n( ̂𝜃−𝜃0)
̂𝜎̂𝜃
where ̂𝜎2
̂𝜃is as in Table 5.4. It is to be speciﬁcally noted that this large sample
normal test is valid only for 𝜃0 > 0. When 𝜃0 = 0, the underlying asymptotic
normality does not hold for any of the listed ̂𝜃.
In practice, it is often of interest to test the hypothesis of H0 ∶MI = 0 versus
Ha ∶MI > 0, or just as often to test the hypothesis H0 ∶D(p||q) = 0 versus
Ha ∶D(p||q) > 0. For these hypotheses, provided that the sizes of the under-
lying alphabets, K1 and K2, are ﬁnite, there exist well-known goodness-of-ﬁt
tests in the standard statistical literature.
For example, by Theorem 5.1, D(p||q) = 0 if and only if p = q. Therefore,
to test H0 ∶D(p||q) = 0 versus Ha ∶D(p||q) > 0, one may use Pearson’s
goodness-of-ﬁt test. In the case of a one-sample goodness-of-ﬁt test
Table 5.4 Estimated information indices with
estimated variances.
𝜽
̂𝜽
̂𝝈2
̂𝜽
As in
MI
̂
MI
̂𝜎2
Equation (5.62)
MI
̂
MIz
̂𝜎2
Equation (5.62)
𝜅
̂𝜅
̂𝜎2
̂𝜅
Corollary 5.5
𝜅
̂𝜅z
̂𝜎2
̂𝜅z
Corollary 5.6
𝜅2
̂𝜅2
̂𝜎2
̂𝜅2
Corollary 5.5
𝜅2
̂𝜅2z
̂𝜎2
̂𝜅2z
Corollary 5.6
𝜅3
̂𝜅3
̂𝜎2
̂𝜅3
Corollary 5.5
𝜅3
̂𝜅3z
̂𝜎2
̂𝜅3z
Corollary 5.6
D
̂Dn
̂𝜎2
̂Dn
Corollary 5.7
D
̂D∗
n
̂𝜎2
̂D∗
n
Corollary 5.8
D
̂D♯
n
̂𝜎2
̂D♯
n
Corollary 5.9
S
̂Sn
̂𝜎2
̂Sn
Corollary 5.10
S
̂S∗
n
̂𝜎2
̂S∗
n
Corollary 5.10
S
̂S♯
n
̂𝜎2
̂S♯
n
Corollary 5.11

198
Statistical Implications of Turing’s Formula
for H0 ∶p = q where q = {q1,…, qK} is a pre-ﬁxed known probability
distribution,
Q1 =
K
∑
k=1
(Fk −nqk)2
nqk
,
(5.124)
where Fk is the observed frequency of the kth letter in the sample of size n,
behaves approximately as a chi-squared random variable with degrees of
freedom m = K −1.
In the case of a two-sample goodness-of-ﬁt test for H0 ∶p = q where neither
p or q is known a priori,
Q2 =
2
∑
i=1
K
∑
k=1
[Fi,k −niF⋅,k∕(n1 + n2)]2
niF⋅,k∕(n1 + n2)
,
(5.125)
where K is the common eﬀective cardinality of the underlying ﬁnite alphabet,
i indicates the ith sample, k indicates the kth letter of the alphabet, Fi,k is
the observed frequency of the kth letter in the ith sample, F⋅,k = F1,k + F2,k,
and ni is size of the ith sample. The statistic Q2 is known as a generalization
of the Pearson goodness-of-ﬁt statistic, Q1. Under H0, Q2 is asymptotically
a chi-squared random variable with degrees of freedom df = K −1. One
would reject H0 if Q2 takes a large value, say greater than 𝜒2
𝛼(K −1), the
(1 −𝛼) × 100th quantile of chi-squared distribution with degrees of freedom
K −1. A discussion of Q1 and Q2 is found in many textbooks, in particular,
Mood, Graybill, and Boes (1974).
Similar to Theorem 5.2, MI = 0 if and only if X and Y are independent,
that is, pi,j = pi,⋅p⋅,j for every pair (i, j). Therefore, to test H0 ∶MI = 0 versus
Ha ∶MI > 0, one may use the Pearson chi-squared statistic for independence
in two-way contingency tables,
Q3 =
K1
∑
i=1
K2
∑
j=1
(Fi,j −n̂pi,⋅̂p⋅,j)2
n̂pi,⋅̂p⋅,j
(5.126)
where Fi,j is the observed frequency of (xi, yj) ∈𝒳× 𝒴in an iid sample
of size n; ̂pi,j = Fi,j∕n; ̂pi,⋅= ∑K2
j=1 ̂pi,j; ̂p⋅,j = ∑K1
i=1 ̂pi,j; Under H0 ∶MI = 0, Q3
is asymptotically a chi-square random variable with degrees of freedom
(K1 −1)(K2 −1). One would reject H0 if Q3 takes a large value, say greater than
𝜒2
𝛼((K1 −1)(K2 −1)), the (1 −𝛼) × 100th quantile of chi-square distribution
with degrees of freedom (K1 −1)(K2 −1). This well-known test is also credited
to Pearson (1900, 1922) and sometimes to Fisher and Tippett (1922).
However it is well known that Pearson’s goodness-of-ﬁt tests perform
poorly when there are many low-frequency cells in the sample data. A rule
of thumb widely adopted in statistical practice for the adequacy of Pear-
son’s goodness-of-ﬁt test is that the expected (or similarly, the observed)
cell frequency is greater or equal to 5. This rule is largely credited to

Estimation of Information
199
Cochran (1952). Because of such a characteristic, tests based on Q1, Q2, and
Q3 often become less reliable in cases of small sample sizes, relative to the
eﬀective cardinality of the underlying alphabet, or a lack of sample coverage.
It would be of great interest to develop better-performing testing procedures
for H0 ∶MI = 0 and H0 ∶D = 0 with sparse data.
5.5
Appendix
5.5.1
Proof of Theorem 5.12
Proof: Since
√
n( ̂D♯−D) =
√
n( ̂D♯−̂D∗) +
√
n( ̂D∗−D),
by Theorem 5.10 and Slutsky’s theorem, it suﬃces to show that
√
n( ̂D♯−̂D∗)
p→0.
Note that
√
n( ̂D♯−̂D∗) =
√
n( ̂A♯−̂A) +
√
n(̂B♯−̂B∗) =∶1 + 2,
where ̂A and ̂B∗are as in (5.98), ̂A♯and ̂B♯are as in (5.108) and (5.115)
respectively. It is shown that 1
p→0 and 2
p→0. For a better presentation,
the proof is divided into two parts: Part 1 is the proof of 1
p→0, and Part 2 is
the proof of 2
p→0.
Part 1: First consider 1. By the Taylor expansion of the logarithm,
1 = −
√
n
(
−̂A♯−
K
∑
k=1
n−Xk
∑
v=1
1
v ̂pk(1 −̂pk)v
)
+
√
n
K
∑
k=1
∞
∑
v=n−Xk+1
1
v ̂pk(1 −̂pk)v
=∶−1,1 + 1,2.
Since
0 ≤1,2 ≤
√
n
K
∑
k=1
̂pk
n −Xk + 1
∞
∑
v=n−Xk+1
(1 −̂pk)v
=
√
n
K
∑
k=1
1
n −Xk + 1(1 −̂pk)n−Xk+1
=
1
√
n
K
∑
k=1
1
1 −̂pk + 1∕n(1 −̂pk)n−Xk+11[̂pk < 1]

200
Statistical Implications of Turing’s Formula
≤
1
√
n
K
∑
k=1
(1 −̂pk)n−Xk1[̂pk < 1] ≤K
√
n
,
(5.127)
1,2 →0 a.s. and therefore 1,2
p→0.
Before considering 1,1, note that for j = 0, 1,…, n −1
(
1 −Xk −1
n −j
)
≥
(
1 −Xk
n
)
= (1 −̂pk)
(5.128)
if and only if
0 ≤j ≤
n
Xk + 1[Xk = 0] =∶1
̂p∗
k
.
Let Jk = ⌊1∕̂p∗
k⌋. One can write
1,1 =
√
n
K
∑
k=1
̂pk
n−Xk
∑
v=1
1
v
[ v
∏
j=1
(
1 −Xk −1
n −j
)
−
Jk∧v
∏
j=1
(
1 −Xk −1
n −j
)
(1 −̂pk)0∨(v−Jk)
]
+
√
n
K
∑
k=1
̂pk
n−Xk
∑
v=1
1
v
[Jk∧v
∏
j=1
(
1 −Xk −1
n −j
)
(1 −̂pk)0∨(v−Jk)
−(1 −̂pk)v
]
=∶1,1,1 + 1,1,2.
By (5.128), 1,1,1 ≤0 and 1,1,2 ≥0. It is to show that E(1,1,1) →0 and
E(1,1,2) →0 respectively. Toward that end,
1,1,2 ≤
√
n
K
∑
k=1
̂pk
n−Xk
∑
v=1
1
v
[Jk∧v
∏
j=1
(
1 −Xk −1
n −1
)
(1 −̂pk)0∨(v−Jk)
−(1 −̂pk)v
]
≤
√
n
K
∑
k=1
̂pk
n−Xk
∑
v=1
1
v
[(
1 −Xk −1
n −1
)Jk∧v
−(1 −̂pk)Jk∧v
]
≤
√
n
K
∑
k=1
̂pk
n−Xk
∑
v=1
1
v
[
(Jk ∧v)
(
1 −Xk −1
n −1
)(Jk∧v)−1 n −Xk
n(n −1)
]
≤
√
n
n −1
K
∑
k=1
n
∑
v=1
1
v(Jk ∧v)

Estimation of Information
201
=
√
n
n −1
K
∑
k=1
(
Jk + Jk
n
∑
v=Jk+1
1
v
)
≤
√
n
n −1
K
∑
k=1
Jk
(
1 +
n
∑
v=2
1
v
)
≤
√
n
n −1
K
∑
k=1
Jk(1 + ln n),
where the third line follows by Lemma 5.8 and the fact that
1 −Xk −1
n −1 > 1 −̂pk.
Thus, by (5.128) and Lemma 5.9,
0 ≤E(1,1,2) ≤
(√
n ln(n)
n
)
→0.
Furthermore,
1,1,1 =
√
n[A −̂A♯]
−
√
n
K
∑
k=1
̂pk
n−Xk
∑
v=1
1
v
[Jk∧v
∏
j=1
(
1 −Xk −1
n −j
)
(1 −̂pk)0∨(v−Jk)
−(1 −̂pk)v
]
−
√
n
[ K
∑
k=1
̂pk
n−Xk
∑
v=1
1
v(1 −̂pk)v + A
]
∶= 1,1,1,1 −1,1,2 −1,1,1,3.
It has already been shown that E(1,1,2) →0, and from (5.107) it follows that
E(1,1,1,1) =
√
nB1 →0. Still further
1,1,1,3 =
√
n
[ K
∑
k=1
̂pk
∞
∑
v=1
1
v(1 −̂pk)v + A
]
−
√
n
K
∑
k=1
̂pk
∞
∑
v=n−Xk+1
1
v(1 −̂pk)v
=
√
n(A −̂An) −
√
n
K
∑
k=1
̂pk
∞
∑
v=n−Xk+1
1
v(1 −̂pk)v
∶= 1,1,1,3,1 −1,2,

202
Statistical Implications of Turing’s Formula
where the second line follows by the Taylor expansion of the logarithm. Since
E(1,1,1,3,1) →0 by (5.102) and E(1,2) →0 by (5.127). Thus, E(1,1,1,3) →0.
This means that 1,1,2
p→0. From here, it follows that 1
p→0.
Part 2: Consider now 2. Note that
2 =
√
n
m
K
∑
k=1
̂pk
√
m
[m−Yk
∑
v=1
1
vUk,v + ln ̂q∗
k
]
.
Since ̂pk
p→pk for every k = 1,…, K and n∕m →𝜆, by Slutsky’s theorem it suf-
ﬁces to show that for
√
m
[m−Yk
∑
v=1
1
vUk,v + ln(̂q∗
k)
]
p
−−−→0
for each k = 1, 2,…, K. By the Taylor expansion of the logarithm
√
m
[m−Yk
∑
v=1
1
vUk,v + ln(̂q∗
k)
]
=
√
m
[m−Yk
∑
v=1
1
vUk,v −
∞
∑
v=1
1
v(1 −̂q∗
k)v
]
=
√
m
{m−Yk
∑
v=1
1
v[Uk,v −(1 −̂q∗
k)v]
}
1[̂qk > 0]
+
√
m
{m−Yk
∑
v=1
1
v[Uk,v −(1 −̂q∗
k)v]
}
1[̂qk = 0]
−
√
m
∞
∑
v=m−Yk+1
1
v(1 −̂q∗
k)v
=∶2,1 + 2,2 −2,3.
Since ̂qk
p→qk and q∗
k
p→qk, Slutsky’s theorem implies that
0 ≤2,3 ≤
√
m
m −Yk + 1
∞
∑
v=m−Yk+1
(1 −̂q∗
k)v
=
√
m
m −Yk + 1
1
̂q∗
k
(1 −̂q∗
k)m−Yk+1
=
1
√
m̂q∗
k
1
1 −̂qk + 1∕m(1 −̂q∗
k)m−Yk+1
≤
1
√
m̂q∗
k
1
1 −̂qk + 1∕m
p
−−−→0
as m →∞, and hence 2,3
p→0.

Estimation of Information
203
Next, it is to show that 2,2
p→0. When ̂qk = 0, one has Uk,v = 1, ̂q∗
k = 1∕m,
and
0 ≤2,2 ≤
√
m
{ m
∑
v=1
1
v
[
1 −
(
1 −1
m
)v]}
1[̂qk = 0]
≤m3∕21[̂qk = 0].
Thus, 0 ≤E(2,2) ≤m3∕2(1 −qk)m →0, and therefore 2,2
p→0. Next, it is to
show that 2,1
p→0. Since that in this case ̂qk = ̂q∗
k, one has
2,1 =
√
m
m−Yk
∑
v=1
1
v
{[ v−1
∏
j=0
(
1 −
Yk
m −j
)]
−(1 −̂q∗
k)v
}
1[̂qk > 0]
≤
√
m
m−Yk
∑
v=1
1
v[(1 −̂qk)v −(1 −̂q∗
k)v]1[̂qk > 0] = 0.
To complete the proof, it is to show that E(2,1) →0. Toward that end,
2,1 + 2,2 =
√
m
[m−Yk
∑
v=1
1
vUk,v −
m
∑
v=1
1
v(1 −qk)v
]
−
√
m
[ ∞
∑
v=1
1
v(1 −̂q∗
k)v −
∞
∑
v=1
1
v(1 −qk)v
]
+
√
m
∞
∑
v=m−Yk+1
1
v(1 −̂q∗
k)v −
√
m
∞
∑
v=m+1
1
v(1 −qk)v
=∶2,1,1 −2,1,2 + 2,1,3 −2,1,4.
E(2,1,1) = 0 because
m−Yk
∑
v=1
1
vUk,v
is an unbiased estimator of
m
∑
v=1
1
v(1 −qk)v
by construction.
By the Taylor expansion of the logarithm 2,1,2 =
√
m[ln(qk) −ln(̂q∗
k)] and
E(2,1,2) →0 by an argument similar to the proof of Proposition 5.11. To show
that E(2,1,3) →0, note that
0 ≤2,1,3 ≤
√
m
m −Yk + 1
1
̂q∗
k
(1 −̂q∗
k)m−Yk+1

204
Statistical Implications of Turing’s Formula
≤
1
√
m(1 −̂qk + 1∕m)
1
̂q∗
k
(1 −̂qk)m−Yk+1
=
1
√
m̂q∗
k
1 −̂qk
(1 −̂qk + 1∕m)(1 −̂qk)m−Yk
≤
1
√
m̂q∗
k
.
Thus, by Lemma 5.9,
0 ≤E(2,1,3) ≤
1
m3∕2 E
(
m
̂q∗
k
)
→0
as m →∞. Finally, E(2,1,4) →0 because 2,1,4 is deterministic and
0 ≤2,1,4 ≤
√
m
∞
∑
v=m
1
v(1 −qk)v ≤
1
√
m
(1 −qk)m
qk
→0.
Hence, E(2,1 + 2,2) →0, but since E(2,2) →0 is already established, it
follows that E(2,1) →0, which implies 2
p→0. The result of the theorem
follows.
◽
5.6
Exercises
1
Prove Lemma 5.2, Jensen’s inequality.
2
Verify both parts in (5.16) of Example 5.2.
3
Verify (5.17)–(5.20) in Example 5.3.
4
Verify (5.21)–(5.24) in Example 5.4.
5
Verify (5.27) and (5.28) in Example 5.7.
6
Show that Σ in (5.50) is positive deﬁnite.
7
Prove each of the following six inequalities.
0 ≤MI(X, Y) ≤min{H(X), H(Y)}
≤
√
H(X)H(Y)
≤(H(X) + H(Y))∕2
≤max{H(X), H(Y)}
≤H(X, Y).

Estimation of Information
205
8
Let
𝜅1 =
MI(X, Y)
min{H(X), H(Y)},
𝜅2 =
MI(X, Y)
√
H(X)H(Y)
,
𝜅3 =
MI(X, Y)
(H(X) + H(Y))∕2,
and
𝜅4 =
MI(X, Y)
max{H(X), H(Y)},
and assume H(X, Y) < ∞. Show that Theorem 5.4 holds for
a) 𝜅2,
b) 𝜅3,
c) 𝜅4,
d) but not for 𝜅1.
9
Show that Z1,v of (5.106) is identical to ̂A♯
n of (5.108).
10
In mathematics, a distance function on a given set 𝒫is a function
d ∶𝒫× 𝒫→R,
where R denotes the set of real numbers, which satisﬁes the following con-
ditions: for any three objects, pX ∈𝒫, pY ∈𝒫, and pZ ∈𝒫,
i) d(pX, pY) ≥0;
ii) d(pX, pY) = 0 if and only if pX = pY;
iii) d(pX, pY) = d(pY, pX); and
iv) d(pX, pZ) ≤d(pX, pY) + d(pY, pZ).
Show that
a) the Kullback–Leibler divergence D(p||q) of (5.87) is not a distance
function on 𝒫, where 𝒫is the collection of all probability distribu-
tions on a countable alphabet 𝒳; but
b) the symmetrized Kullback–Leibler divergence S(p, q) of (5.118) is a
distance function on 𝒫.
11
Suppose a joint probability distribution pX,Y = {pi,j} on
𝒳× 𝒴= {xi; i = 1,…, K1} × {yj; j = 1,…, K2}
is such that pi,j > 0 for every pair (i, j). Let K = K1K2. Re-enumerating {pi,j}
to be indexed by a single index k, that is,
v = (p1,…, pK−1)𝜏
= (p1,1,…, p1,K2, p2,1,…, p2,K2,…, pK1,1,…, pK1,K2−1)𝜏,
show that each of (5.42)–(5.44) is true.

206
Statistical Implications of Turing’s Formula
12
Let {pi,j; i = 1,…, K1, j = 1,…, K2} be a joint probability distribution on
𝒳× 𝒴where 𝒳and 𝒴are as in (5.31). Suppose pi,⋅> 0 for each i, p⋅,j > 0
for each j, K1 ≥2 and K2 ≥2. Show that (5.57) of Theorem 5.5 holds if and
only if pi,j = pi,⋅p⋅,j, or every pair of (i, j) such that pi,j > 0.
13
Consider the following three functions of triplet (x1, x2, x3), with the same
domain, x1 > 0, x2 > 0, and x3 > 0,
𝜅(x1, x2, x3) = x1 + x2
x3
−1,
𝜅2(x1, x2, x3) = x1 + x2 −x3
√x1x2
,
𝜅3(x1, x2, x3) = 2
(
1 −
x3
x1 + x2
)
.
Show that the gradients of these functions are, respectively,
g𝜅(x1, x2, x3) =
(
1
x3
, 1
x3
, −x1 + x2
x2
3
)𝜏
,
g𝜅2(x1, x2, x3) =
(
1
(x1x2)1∕2 −x2(x1 + x2 −x3)
2(x1x2)3∕2
,
1
(x1x2)1∕2 −x1(x1 + x2 −x3)
2(x1x2)3∕2
, −
1
√x1x2
)𝜏
,
g𝜅3(x1, x2, x3) =
(
2x3
(x1 + x2)2 ,
2x3
(x1 + x2)2 , −
2
x1 + x2
,
)𝜏
.
14
In Theorem 5.5, show that if X and Y are independent random elements
on joint alphabet 𝒳× 𝒴, that is, MI = 0, then
(1, 1 −1)ΣH(1, 1, −1)𝜏= 0.
15
In Theorem 5.9, show that if p = q on a same ﬁnite alphabet 𝒳, that is,
D = D(p||q) = 0, then g𝜏(v)Σ(v)g(v) = 0.
16
Use the data in Table 5.1 and Pearson’s goodness-of-ﬁt statistic, Q3 of
(5.126), to test the hypothesis that the two random elements, Ethnicity
and Favorite Color, are independent, that is, H0 ∶MI = 0, at 𝛼= 0.05.
17
Suppose n = 600 tosses of a die yield the following results.
X
1
2
3
4
5
6
f
91 101 105 89 124 90

Estimation of Information
207
Use the data and Pearson’s goodness-of-ﬁt statistic, Q1 of (5.124), to test
the hypothesis that the dice is a balanced die, that is, H0 ∶D(p||q) = 0,
where q is the hypothesized uniform distribution for the six sides
(qk = 1∕6 for k = 1,…, 6) and p is the true probability distribution of the
die tossed, at 𝛼= 0.05.
18
Two dice, A and B, are independently tossed nA = 600 and nB = 300 times,
respectively, with the given results.
X
1
2
3
4
5
6
fA
91
101
105
89
124
90
fB
54
53
52
41
64
36
Let the underlying probability distributions of the two dice be p for A and
q for B, respectively.
a) Test the hypothesis that the two distributions are identical, that is,
H0 ∶p = q, at 𝛼= 0.05.
b) Test the hypothesis that both distributions are uniform, that is,
H0 ∶pk = qk = 1∕6 for k = 1,…, 6, at 𝛼= 0.05.
19
Let Xm be a sequence of chi-squared random variables with degrees of
freedom m. Show that
√
m(Xm −m)
L
−−−→N(0, 2) as m →∞.
20
Suppose the joint probability distributions of (X, Y) on {0, 1} × {0, 1} is
X = 0
X = 1
Y = 0
0.50 −0.01 m
0.01 m
Y = 1
0.01 m
0.50 −0.01 m
for some integer m ≥1. Show that
𝜅= ln 2 −[(m∕50) ln(50∕m −1) −ln(1 −m∕50)]
ln 2 + [(m∕50) ln(50∕m −1) −ln(1 −m∕50)].

209
6
Domains of Attraction on Countable Alphabets
6.1
Introduction
A domain of attraction is a family of probability distributions whose members
share a set of common properties, more speciﬁcally a set of common proper-
ties pertaining to the tail of a probability distribution. In the probability and
statistics literature, domains of attraction are usually discussed in the context
of extreme value theory. The simplest setup involves a sample of iid random
variables, X1,…, Xn, under a probability distribution with a diﬀerentiable
cumulative distribution function (cdf ), F(x). Let X(n) = max{X1,…, Xn}.
The asymptotic behavior of X(n) may be characterized by that of a properly
normalized X(n), namely Yn = (X(n) −bn)∕an where {an > 0} and {bn} are
normalizing sequences, in the sense that
P(Yn ≤y) = P(X(n) ≤any + bn) = [F(any + bn)]n →G(y)
where G(y) is a nondegenerated distribution function. Many distributions on
the real line may be categorized into three families or domains. The members in
each of the three domains have a G(y) belonging to a same parametric family.
These three families are often identiﬁed as Fréchet for thick-tailed distribu-
tions (e.g., with density function f (x) = x−21[x ≥1]), Gumbel for thin-tailed
distributions (e.g., with density function f (x) = e−x1[x ≥0]), and Weibull for
distributions with ﬁnite support (e.g., with density function f (x) = 1[0 ≤x ≤
1]). Extreme value theory is one of the most important topics in probability
and statistics, partially because of its far-reaching implications in applications.
Early works in this area include (Fréchet, 1927; Fisher and Tippett, 1928, and
von Mises, 1936). However, Gnedenko (1943) and Gnedenko (1948) are often
thought to be the ﬁrst rigorous mathematical treatment of the topic. For a com-
prehensive introduction to extreme value theory, readers may wish to refer to
de Haan and Ferreira (2006).
Consider a countable alphabet 𝒳= {𝓁k; k ≥1} and its associated probability
distribution p = {pk; k ≥1} ∈𝒫where 𝒫is the collection of all probability
distributions on 𝒳. Let X1,…, Xn be an iid random sample from 𝒳under p.
Let {Yk; k ≥1} and {̂pk = Yk∕n; k ≥1} be the observed frequencies and relative
Statistical Implications of Turing’s Formula, First Edition. Zhiyi Zhang.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

210
Statistical Implications of Turing’s Formula
frequencies of the letters in the iid sample of size n. This chapter presents a
discussion of domains of attraction in 𝒫.
Unlike random variables on the real line, the letters in 𝒳are not necessarily
on a numerical scale or even ordinal. The nature of the alphabet presents an
immediate issue inhibiting an intuitive parallel to the notions well conceived
with random variables on the real line, say for example, the maximum order
statistic X(n), which may be considered as an extreme value in an iid sample
of size n, or the notion of a “tail” of a distribution, P(X(n) > x) = 1 −Fn(x) for
large x. It is much less clear what an extreme value means on an alphabet. Can
an imitation game still be played? If so, in what sense?
For a random variable, the word “extreme” suggests an extremely large value
on the real line. However, an extremely large value on the real line necessarily
corresponds to an extremely small probability density. In that sense, an extreme
value could be taken as a value in a sample space that carries an extremely small
probability. If this notion is adopted, then it becomes reasonable to perceive an
extreme value on an alphabet to be a letter with extremely small probability.
In that sense, a subset of 𝒳with low probability letters may be referred to as
a “tail,” a subset of 𝒳with very low probability letters may be referred to as a
“distant tail,” and a distribution on a ﬁnite alphabet has “no tail.”
By the same argument, one may choose to associate the notion of extrem-
ity in an iid sample with the rarity of the letter in the sample. The values that
are observed exactly once in the sample are rarer than any other values in the
sample; and there could be and usually are many more than one such observed
value in a sample. There exist however rarer values than those even with fre-
quency one, and these would be the letters with frequency zero, that is, the
letters in 𝒳that are not represented in the sample. Though not in the sample,
the letters with zero observed frequencies are, nevertheless, associated with
and completely speciﬁed by the sample.
To bring the above-described notion of extremity into a probabilistic argu-
ment, consider a two-step experiment as follows:
Step 1: draw an iid sample of a large size n from 𝒳, and then
Step 2: draw another independent iid sample of size m from 𝒳.
Let the samples be denoted as
{X1,…, Xn}
and
{Xn+1,…, Xn+m}.
When m = 1, this experiment is the same as that described in Robbins’ Claim
in Chapter 1. Let E1 be the event that Xn+1 assumes a value from 𝒳that is not
assumed by any of the Xi in the ﬁrst sample, i.e.,
E1 = ∩n
i=1{Xn+1 ≠Xi}.
Event E1 may be thought of as a new discovery, and in this regard, an occurrence
of a rare event, and therefore is pertaining to the tail of the distribution. The

Domains of Attraction on Countable Alphabets
211
argument (1.60) in Chapter 1 gives P(E1) = 𝜁1,n =∶𝜁n. Clearly 𝜁n →0 as n →∞
for any probability distribution {pk} on 𝒳(see Exercise 1).
When m = n, a quantity of interest is the number of letters found in the sec-
ond sample {Xn+1,…, Xn+n} that are not found in the ﬁrst sample {X1,…, Xn},
that is,
tn =
n
∑
j=1
1[Ej]
(6.1)
where Ej = ∩n
i=1{Xn+j ≠Xi} for j = 1,…, n. The expected value of tn in (6.1) is
𝜏n = E (tn) = n𝜁n.
(6.2)
𝜏n is referred to as the tail index and plays a central role in this chapter. The rel-
evance of 𝜏n to the tail of the underlying distribution manifests in the relevance
of 𝜁n to the tail.
Furthermore, it is to be noted that, for any given integer k0 ≥1, the ﬁrst k0
terms in the re-expression of 𝜏n, as in
𝜏n =
∑
1≤k≤k0
npk(1 −pk)n +
∑
k>k0
npk(1 −pk)n,
converges to zero exponentially fast as n →∞. Therefore, the asymptotic
behavior of 𝜏n has essentially nothing to do with how the probabilities are
distributed over any ﬁxed and ﬁnite subset of 𝒳. Also to be noted is that 𝜏n is
invariant under any permutation on the index set {k; k ≥1}.
In contrast to the fact that 𝜁n →0 for any probability distribution {pk}, the
multiplicatively inﬂated version 𝜏n = n𝜁n has diﬀerent asymptotic behavior
under various distributions. 𝒫, the total collection of all probability distribu-
tions, on 𝒳splinters into diﬀerent domains of attraction characterized by the
asymptotic features of 𝜏n.
Finally, a notion of thinner/thicker tail between two distributions,
p = {pk; k ≥1} and q = {qk; k ≥1}, is to be mentioned. Although there
is no natural ordering among the letters in 𝒳, there is one on the index set
{k; k ≥1}. There therefore exists a natural notion of a distribution p having
a thinner tail than that of another distribution q, in the sense of pk ≤qk for
every k ≥k0 for some integer k0 ≥1, when p and q share a same alphabet 𝒳
and are enumerated by a same index set {k; k ≥1}. Whenever this is the case
in the subsequent text, p is said to have a thinner tail than q in the usual sense.
Deﬁnition 6.1
A distribution p = {pk} on 𝒳is said to belong to
1) Domain 0 if limn→∞𝜏n = 0,
2) Domain 1 if lim supn→∞𝜏n = cp for some constant cp > 0,
3) Domain 2 if limn→∞𝜏n = ∞, and
4) Domain T, or Domain Transient, if it does not belong to Domains 0, 1, or 2.

212
Statistical Implications of Turing’s Formula
The four domains so deﬁned above form a partition of 𝒫. The primary
results presented in this chapter include the following:
1) Domain 0 includes only probability distributions with ﬁnite support, that is,
there are only ﬁnitely many letters in 𝒳carrying positive probabilities.
2) Domain 1 includes distributions with thin tails such as pk ∝a−𝜆k,
pk ∝kra−𝜆k, and pk ∝a−𝜆k2, where a > 1, 𝜆> 0, and r ∈(−∞, ∞) are
constants.
3) Domain 2 includes distributions with thick tails satisfying pk+1∕pk →1 as
k →∞, for example, pk ∝k−𝜆, pk ∝(kln𝜆k)−1 where 𝜆> 1, and pk ∝e−k𝛿
where 𝛿∈(0, 1).
4) A relative regularity condition between two distributions (one dominates
the other) is deﬁned. Under this condition, all distributions on a countably
inﬁnite alphabet, which are dominated by a Domain 1 distribution, must
also belong to Domain 1; and all distributions on a countably inﬁnite alpha-
bet, which dominate a Domain 2 distribution, must also belong to Domain 2.
5) Domain T is not empty.
Other relevant results presented in this chapter include the following:
1) In Domain 0, 𝜏n →0 exponentially fast for every distribution.
2) The tail index 𝜏n of a distribution with tail pk =∝e−𝜆k where 𝜆> 0 in Domain
1 perpetually oscillates between two positive constants and does not have a
limit as n →∞.
3) There is a uniform positive lower bound for lim supn→∞𝜏n for all distribu-
tions with positive probabilities on inﬁnitely many letters of 𝒳.
Remark 6.1
To honor the great minds of mathematics whose works mark the
trail leading to the results of this chapter, Zhang (2017) named the distribu-
tions in Domain 0 as members of the Gini–Simpson family, after Corrado Gini
and Edward Hugh Simpson; those in Domain 1 as members of the Molchanov
family, after Stanislav Alekseevich Molchanov; and those in Domain 2 as the
Turing–Good family, after Alan Mathison Turing and Irving John Good. In
the subsequent text of this chapter, the domains are so identiﬁed according to
Zhang (2017).
6.2
Domains of Attraction
Let K be the eﬀective cardinality, or simply the cardinality when there is no
ambiguity, of 𝒳, that is, K = ∑
k≥11[pk > 0]. Let ℕbe the set of all positive
integers.
Lemma 6.1
If K = ∞, then there exist a constant c > 0 and a subsequence
{nk; k ≥1} in ℕ, satisfying nk →∞as k →∞, such that 𝜏nk > c for all suﬃ-
ciently large k.

Domains of Attraction on Countable Alphabets
213
Proof: Assume without loss of generality that pk > 0 for all k ≥1. Since 𝜁n is
invariant with respect to any permutation on the index set {k; k ≥1}, it can be
assumed without loss of generality that {pk} is nonincreasing in k. For every k,
let nk = ⌊1∕pk⌋. With nk so deﬁned,
1
nk + 1 < pk ≤1
nk
for every k and limk→∞nk = ∞though {nk} may not necessarily be strictly
increasing. By construction, the following are true about the nk, k ≥1.
1) {nk; k ≥1} is an inﬁnite subset of ℕ.
2) Every pk is covered by the interval (1∕(nk + 1), 1∕nk].
3) Every interval (1∕(nk + 1), 1∕nk] covers at least one pk and at most ﬁnitely
many pks.
Let fn(x) = nx(1 −x)n for x ∈[0, 1]. fn(x) attains its maximum at x = (n + 1)−1
with value
fn
(
1
n + 1
)
=
n
n + 1
(
1 −
1
n + 1
)n
=
(
1 −
1
n + 1
)n+1
→e−1.
Also
fn
(1
n
)
=
(
1 −1
n
)n
→e−1.
Furthermore, since f
′
n(x) < 0 for all x satisfying 1∕(n + 1) < x < 1, the follow-
ing is true
fn
(1
n
)
< fn(x) < fn
(
1
n + 1
)
for all x satisfying 1∕(n + 1) < x < 1∕n.
Since fn(1∕n) →e−1 and fn(1∕(n + 1)) →e−1, for any arbitrarily small but
ﬁxed ε > 0, there exists a positive Nε such that for any n > Nε,
fn
(
1
n + 1
)
> fn
(1
n
)
> e−1 −ε.
Since limk→∞nk = ∞and {nk} is nondecreasing, there exists an integer
Kε > 0 such that nk > Nε for all k > Kε. Consider the subsequence {𝜏nk; k ≥1}.
For any k > Kε,
𝜏nk =
∞
∑
i=1
nkpi(1 −pi)nk > fnk(pk).
Since pk ∈(1∕(nk + 1), 1∕nk] and fnk(x) is decreasing on the same interval,
fnk(pk) > fnk
(
1
nk
)
≥e−1 −ε,
and hence
𝜏nk > fnk(pk) ≥c = e−1 −ε
for all k > Kε.
◽

214
Statistical Implications of Turing’s Formula
Theorem 6.1
K < ∞if and only if
lim
n→∞𝜏n = 0.
(6.3)
Proof: Assuming that p = {pk; 1 ≤k ≤K} where K is ﬁnite and pk > 0 for all k,
1 ≤k ≤K, and denoting p∧= min{pk; 1 ≤k ≤K} > 0, the necessity of (6.3)
follows the fact that as n →∞
𝜏n = n
K
∑
k
pk(1 −pk)n ≤n
K
∑
k
pk(1 −p∧)n = n(1 −p∧)n →0.
The suﬃciency of (6.3) follows the fact that if K = ∞, then Lemma 6.1 would
provide a contradiction to (6.3).
◽
In fact, the proof of Theorem 6.1 also establishes the following corollary.
Corollary 6.1
K < ∞if and only if 𝜏n ≤𝒪(nqn
0) where q0 is a constant in (0, 1).
Theorem 6.1 and Corollary 6.1 ﬁrmly characterize the members of the
Gini–Simpson family as the distributions on ﬁnite alphabets. All distributions
outside of the Gini–Simpson family must have positive probabilities on
inﬁnitely many letters of 𝒳. The entire class of such distributions is denoted
as 𝒫+. In fact, in the subsequent text when there is no ambiguity, 𝒫+ will
denote the entire class of distributions with a positive probability on every 𝓁k
in 𝒳. For all distributions in 𝒫+, a natural group would be those for which
limn→∞𝜏n = ∞and so the Turing–Good family is deﬁned.
The next lemma includes two trivial but useful facts.
Lemma 6.2
1) For any real number x ∈[0, 1),
1 −x ≥exp
(
−
x
1 −x
)
.
2) For any real number x ∈(0, 1∕2),
1
1 −x < 1 + 2x.
Proof: For Part 1, the function y = (1 + t)−1et is strictly increasing over [0, ∞)
and has value 1 at t = 0. Therefore, (1 + t)−1et ≥1 for t ∈[0, ∞). The desired
inequality follows the change of variable x = t∕(1 + t). For Part 2, the proof is
trivial.
◽

Domains of Attraction on Countable Alphabets
215
Lemma 6.3
For any given probability distribution p = {pk; k ≥1} and two
constants c > 0 and 𝛿∈(0, 1), as n →∞,
n1−𝛿∑
k≥1
pk(1 −pk)n →c > 0
if and only if
n1−𝛿∑
k≥1
pke−npk →c > 0.
A proof of Lemma 6.3 is given in the appendix of this chapter.
Theorem 6.2
For any given probability distribution p = {pk; k ≥1}, if there
exists constants 𝜆> 1, c > 0 and integer k0 ≥1 such that for all k ≥k0
pk ≥ck−𝜆,
(6.4)
then limn→∞𝜏n = ∞.
A proof of Theorem 6.2 is given in the appendix of this chapter.
Theorem 6.2 puts distributions with power decaying tails, for example,
pk = c𝜆k−𝜆, and those with much more slowly decaying tails, for example,
pk = c𝜆(k ln 𝜆k)−1, where 𝜆> 1 and c𝜆> 0 is a constant, which may depend on
𝜆, in the Turing–Good family.
An interesting question at this point is whether there exist distributions with
thinner tails, thinner than those with power decaying tails, in the Turing–Good
family. An aﬃrmative answer is given by Theorem 6.3 below with a suﬃcient
condition on p = {pk; k ≥1}.
Theorem 6.3
If pk+1∕pk →1 as k →∞, then 𝜏n →∞as n →∞.
The proof of Theorem 6.3 requires the following lemma.
Lemma 6.4
For every suﬃciently large n, let kn = max{k; pk ≥1∕n}. If there
exist a constant c ∈(0, 1) and a positive integer Kc such that for all k, k ≥Kc,
pk+1∕pk ≥c, then 1 ≤npkn < c−1.
Proof: 1 ≤npkn holds by deﬁnition. It suﬃces to show only that npkn ≤c−1.
Toward that end, let n0 be suﬃciently large so that pkn0+1∕pkn0 ≥c, and there-
fore pkn+1∕pkn ≥c for every n ≥n0. But since cpkn ≤pkn+1 < 1∕n by deﬁnition,
it follows that npkn < c−1.
◽
Proof of Theorem 6.3: For any given arbitrarily small ε > 0, there exists
a Kε such that for all k ≥Kε, pk+1∕pk > 1 −ε. There exists a suﬃciently large

216
Statistical Implications of Turing’s Formula
nε such that for any n > nε, kn > Kε where kn = max{k; pk ≥1∕n}. For every
n > nε,
1) a) pkn+1 > (1 −ε)pkn;
b) pkn+2 > (1 −ε)pkn+1 > (1 −ε)2pkn, · · ·; and hence
c) pkn+j > (1 −ε)jpkn, for j = 1, 2, · · ·;
and
2) pkn+j < 1∕n, for j = 1, 2, · · ·.
Therefore, for every n > nε,
𝜏n > n
∑
k≥kn
pk(1 −pk)n
= n
∑
j≥0
pkn+j(1 −pkn+j)n
> n
∑
j≥0
(1 −ε)jpkn(1 −1∕n)n
= npkn(1 −1∕n)n ∑
j≥0
(1 −ε)j
= [npkn] [(1 −1∕n)n]
[1
ε
]
.
In the last expression above, the ﬁrst factor is bounded below by 1, the second
factor is bounded below by e−1∕2 for large n, and the third factor can be taken
over all bounds since ε is arbitrarily small. Hence, 𝜏n →∞.
◽
Example 6.1
Let pk = ck−𝜆for some constants c > 0 and 𝜆> 1. Since
pk+1∕pk = [k∕(k + 1)]𝜆→1, Theorem 6.3 applies and 𝜏n →∞.
Example 6.2
Let pk = ck−1(ln k)−𝜆for some constants c > 0 and 𝜆> 1. Since
pk+1∕pk = [k∕(k + 1)][ln k∕ln(k + 1)]𝜆→1, Theorem 6.3 applies and 𝜏n →∞.
Example 6.3
Let pk = ce−k𝛿for some constants c > 0 and 𝛿∈(0, 1). Since,
applying L’Hôpital’s rule whenever necessary,
pk+1∕pk = exp(k𝛿−(k + 1)𝛿) = exp
(
1−
(
1+ 1
k
)𝛿
k−𝛿
)
→e0 = 1,
Theorem 6.3 applies and 𝜏n →∞.

Domains of Attraction on Countable Alphabets
217
Example 6.4
Let pk = ce−k∕ln k for some constants c > 0. Since, applying
L’Hôpital’s rule whenever necessary,
pk+1∕pk = exp
(
k
ln k −
k + 1
ln(k + 1)
)
= exp
(
k
ln k −
k
ln(k + 1) −
1
ln(k + 1)
)
∼exp
(
k
ln k −
k
ln(k + 1)
)
= exp
⎛
⎜
⎜
⎜⎝
ln
(
1 + 1
k
)
(
ln k ln(k+1)
k
)
⎞
⎟
⎟
⎟⎠
∼exp
⎛
⎜
⎜
⎜⎝
(
1 + 1
k
)−1
ln k ln(k + 1) −ln(k + 1) −
k
k+1 ln k
⎞
⎟
⎟
⎟⎠
→e0 = 1,
Theorem 6.3 applies and 𝜏n →∞.
Example 6.5
Let pk = ce−k for some constant c > 0. Since pk+1∕pk = e−1 < 1,
Theorem 6.3 does not apply. In fact, it can be shown that 𝜏n is bounded above by
a constant (see Lemma 6.6).
Examples 6.3 and 6.4 are particularly interesting and somewhat surprising.
In the extreme value theory for continuous random variables, the maximum
of a sample under the density function f (x) = ce−x𝛿where 𝛿∈(0, 1) or f (x) =
ce−x∕ln x converges weakly to a member of the Gumbel family to which extreme
values under other thin-tailed distributions converge. In the current setting,
where the domains are deﬁned by the limiting behavior of 𝜏n, pk = ce−k𝛿and
pk = ce−k∕ln k belong to the domain shared by thick-tailed discrete distributions
such as pk = ck−𝜆where c > 0 and 𝜆> 1.
In view of Lemma 6.3 and Theorems 6.1 and 6.2, Domain 1, or the Molchanov
family, has a more intuitive deﬁnition as given in the following lemma.
Lemma 6.5
A distribution p on 𝒳belongs to the Molchanov family if and
only if
1) the eﬀective cardinality of 𝒳is countably inﬁnite, that is, K = ∞, and
2) 𝜏n ≤up for all n, where up > 0 is a constant that may depend on p.
The proof is left as an exercise.
Lemma 6.6
For any p = {pk} ∈𝒫+, if there exists an integer k0 ≥1 such that
pk = c0e−k, where c0 > 0 is a constant, for all k ≥k0, then

218
Statistical Implications of Turing’s Formula
1) 𝜏n ≤u for some upper bound u > 0; and
2) limn→∞𝜏n does not exist.
A proof of Lemma 6.6 is given in the appendix of this chapter.
A similar proof to that of Lemma 6.6 immediately gives Theorem 6.4 with a
slightly more general statement.
Theorem 6.4
For any given probability distribution p = {pk; k ≥1}, if there
exists constants a > 1 and integer k0 ≥1 such that for all k ≥k0
pk = ca−k,
(6.5)
then
1) 𝜏n ≤ua for some upper bound ua > 0, which may depend on a; and
2) limn→∞𝜏n does not exist.
Theorem 6.4 puts distributions with tails of geometric progression, for
example, pk = c𝜆e−𝜆k where 𝜆> 0 and c𝜆> 0 are constants or pk = 2−k in the
Molchanov family, a family of distributions with perpetually oscillating tail
indices.
Next, a notion of relative dominance of one probability distribution over
another is deﬁned on a countable alphabet within 𝒫+. Let #A denote the
cardinality of a set A.
Deﬁnition 6.2
Let q∗∈𝒫+ and p ∈𝒫+ be two distributions on 𝒳, and let
q = {qk} be a nonincreasingly ordered version of q∗. q∗is said to dominate p if
#{i; pi ∈(qk+1, qk], i ≥1} ≤M < ∞
for every k ≥1, where M is a ﬁnite positive integer.
It is easy to see that the notion of dominance by Deﬁnition 6.2 is a tail prop-
erty, and that it is transitive, that is, if p1 dominates p2 and p2 dominates p3,
then p1 dominates p3. It says in essence that if p is dominated by q, then the pis
do not get overly congregated locally into some intervals deﬁned by the qks.
The following examples illustrate the notion of dominance by Deﬁnition 6.2.
Example 6.6
Let pk = c1e−k2 and qk = c2e−k for all k ≥k0 for some integer
k0 ≥1 and other two constants c1 > 0 and c2 > 0. For every suﬃciently large k,
suppose
pj = c1e−j2 ≤qk = c2e−k,
then
−j2 ≤ln(c2∕c1) −k

Domains of Attraction on Countable Alphabets
219
and
j + 1 ≥[k + ln(c1∕c2) ]1∕2 + 1.
It follows that
pj+1 = c1e−(j+1)2
≤c1e
−
(√
k+ln(c1∕c2)+1
)2
= c1e−(k+ln(c1∕c2)+1)−2
√
k+ln(c1∕c2)
= c2e−(k+1)−2
√
k+ln(c1∕c2)
= c2e−(k+1) e−2
√
k+ln(c1∕c2)
≤c2e−(k+1) = qk+1.
This means that if pj ∈(qk+1, qk], then necessarily pj+1 ∉(qk+1, qk], which implies
that each interval (qk+1, qk] can contain only one pj at most for a suﬃciently large
k, that is, k ≥k00 ∶= max{k0, ln(c2∕c1)}. Since there are only ﬁnite pjs covered
by ∪1≤k<k00(qk, qk+1], q = {qk} dominates p = {pi}.
Example 6.7
Let pk = c1a−k and qk = c2b−k for all k ≥k0 for some integer k0 ≥
1 and other two constants a > b > 1. For every suﬃciently large k, suppose pj =
c1a−j ≤qk = c2b−k, then −j ln a ≤ln(c2∕c1) −k ln b and j + 1 ≥k(ln b∕ln a) +
1 + ln(c1∕c2) ∕ln a. It follows that
pj+1 = c1a
−
(
k ln b
ln a +1+
ln(c1∕c2)
ln a
)
= c1a
−
(
klog ab+1+
ln(c1∕c2)
ln a
)
= c1b−ka−1a−
ln(c1∕c2)
ln a
≤c1b−(k+1)a−log a (c1∕c2)
= c2b−(k+1)
= qk+1.
By a similar argument as that in Example 6.6, q = {qk} dominates p = {pi}.
Example 6.8
Let pk = c1k−re−𝜆k for some integer k0 ≥1 and constants 𝜆> 0
and r > 0, and qk = c2e−𝜆k for all k ≥k0. Suppose for a k ≥k0 there is a j such
that pj = c1j−re−𝜆j ∈(qk+1 = c2e−𝜆(k+1), qk = c2e−𝜆k], then
pj+1 = c1(j + 1)−re−𝜆(j+1)
= c1(j + 1)−re−𝜆je−𝜆
≤c1j−re−𝜆je−𝜆
≤c2e−𝜆ke−𝜆= qk+1,

220
Statistical Implications of Turing’s Formula
which implies that there is at most one pj in (qk+1, qk] for every suﬃciently large
k. Therefore, q = {qk} dominates p = {pi}.
Example 6.9
Let pk = c1kre−𝜆k for some integer k0 ≥1 and constants 𝜆> 0
and r > 0, and qk = c2e−(𝜆∕2)k for all k ≥k0. Suppose for any suﬃciently large j,
j ≥j0 ∶= [e𝜆∕(2r) −1]−1,
pj = c1jre−𝜆j ∈(qk+1 = c2e−(𝜆∕2)(k+1), qk = c2e−(𝜆∕2)k]
for some suﬃciently large k ≥k0, then
pj+1 = c1(j + 1)re−𝜆(j+1)
= c1(j + 1)re−𝜆je−𝜆
= c1jre−𝜆je−𝜆(j + 1)r
jr
≤c2e−𝜆
2 ke−𝜆
(j + 1
j
)r
= c2e−𝜆
2 (k+1)e−𝜆
2
(j + 1
j
)r
≤qk+1e−𝜆
2
(j0 + 1
j0
)r
= qk+1
which implies that there is at most one pj in (qk+1, qk] for every suﬃciently large
k. Therefore, q = {qk} dominates p = {pi}.
Example 6.10
Let pk = qk for all k ≥1. q = {qk} and p = {pk} dominate each
other.
While in each of Examples 6.6 through 6.9, the dominating distribution q has
a thicker tail than p in the usual sense, the dominance of Deﬁnition 6.2 in gen-
eral is not implied by such a thinner/thicker tail relationship. This is so because a
distribution p ∈𝒫+, satisfying pk ≤qk for all suﬃciently large k, could exist yet
congregate irregularly to have an unbounded supk≥1#{pi; pi ∈(qk+1, qk], i ≥1}.
One such example is given in Section 6.3. In this regard, the dominance of Def-
inition 6.2 is more appropriately considered as a regularity condition. However,
it may be interesting to note that the said regularity is a relative one in the sense
that the behavior of p is regulated by a reference distribution q. This relative
regularity gives an umbrella structure in the Molchanov family as well as in the
Turing–Good family, as demonstrated by Theorems 6.5 and 6.6 below.
Theorem 6.5
If two distributions p and q in 𝒫+ on a same countably inﬁnite
alphabet 𝒳are such that q is in the Molchanov family and q dominates p, then
p belongs to the Molchanov family.

Domains of Attraction on Countable Alphabets
221
Proof: Without loss of generality, it may be assumed that q is nonincreas-
ingly ordered. For every suﬃciently large n, there exists a kn such that
1
n+1 ∈(qkn+1, qkn]. Noting that the function np(1 −p)n increases in p over
(0, 1∕(n + 1)], attains its maximum value of [1 −1∕(n + 1)]n+1 < e−1 at
p = 1∕(n + 1), and decreases over [1∕(n + 1), 1], consider
𝜏n(p) =
∑
k≥1
npk(1 −pk)n
=
∑
k∶pk≤qkn+1
npk(1 −pk)n +
∑
k∶qkn+1<pk≤qkn
npk(1 −pk)n
+
∑
k∶pk>qkn
npk(1 −pk)n
≤M
∑
k≥kn+1
nqk(1 −qk)n +
∑
k;qkn+1<pk≤qkn
e−1 + M
∑
1≤k≤kn
nqk(1 −qk)n
= M
∑
k≥1
nqk(1 −qk)n +
∑
k∶qkn+1<pk≤qkn
e−1
≤M𝜏n(q) + Me−1 < ∞,
where M is as in Deﬁnition 6.2 and it exists because the assumed condition that
q dominates p. The desired result immediately follows.
◽
Corollary 6.2
Any distribution p on a countably inﬁnite alphabet 𝒳satis-
fying pk = ae−𝜆k, pk = be−𝜆k2, or pk = ckre−𝜆k for all k ≥k0, where k0 ≥1, 𝜆> 0,
r ∈(−∞, +∞), a > 0, b > 0, and c > 0 are constants, is in the Molchanov family.
Proof: The result is immediate following Theorem 6.5 and Examples 6.6
through 6.9.
◽
Theorem 6.6
If two distributions p and q in 𝒫+ on a same countably inﬁnite
alphabet 𝒳are such that p is in the Turing–Good family and q dominates p,
then q belongs to the Turing–Good family.
Proof: In the proof of Theorem 6.5, it is established that if q dominates p, then
𝜏n(p) ≤M𝜏n(q) + Me−1
for some positive constant M. The fact 𝜏n(p) →∞implies 𝜏n(q) →∞, as
n →∞.
◽
Figures 6.1–6.3 show graphic representations of 𝜏n for several distributions
in various domains. Figure 6.1 plots 𝜏n for pk = 0.01 for k = 1,…, 100 and qk =
0.02 for k = 1,…, 50. The tail indices are plotted on a same vertical scale from 0
to 40, ranging from n = 1 to n = 1000. When n increases indeﬁnitely, the rapid
decay in both indices, as suggested by Theorem 6.1 and Lemma 6.1, is visible.

222
Statistical Implications of Turing’s Formula
40
35
30
25
20
15
10
0
100
200
300
400
500
600
700
800
900
1000
5
0
Figure 6.1 𝜏n of pk = 0.01 (upper) for k = 1,…, 100 and qk = 0.02 (lower) for k = 1,…, 50.
1.1
1
0.9
0.8
0.7
0.6
0.5
0
100
200
300
400
500
600
(a)
700
800
900
1000
200
300
400
500
600
(b)
700
800
900
1000
0.85
0.83
0.81
0.79
0.77
0.75
0.73
0.71
0.67
0.65
0.69
0.4
0.98
0.985
0.99
0.995
1
1.005
Figure 6.2 𝜏n of pk = (e −1)e−k (upper) and pk = 3 × 4−k (lower), k ≥1. (a) 𝜏n from n = 1 to
n = 1000, one scale. (b) 𝜏n from n = 200 to n = 1000, two scales.

Domains of Attraction on Countable Alphabets
223
0
0
5
10
15
20
25
200
100
300
400
500
600
700
800
900
1000
Figure 6.3 𝜏n of pk = c1k−2 (upper) and qk = c2k−3 (lower), k ≥1.
Figure 6.2 gives 𝜏n for pk = (e −1)e−k and qk = 3 × 4−k, k ≥1. In Figure 6.2a,
the pair of tail indices are plotted on the same vertical scale for 𝜏n from 0.4 to 1.1,
ranging from n = 1 to n = 1000. The values for 𝜏n seem stable for large values of
n in Figure 6.2a. However, when the tail indices are plotted on diﬀerent vertical
scales, on the left from 0.98 to 1.005 for pk and on the right from 0.65 to 0.85
for qk, ranging from n = 200 to n = 1000, the oscillating patterns suggested by
Theorem 6.4 become visible.
Figure 6.3 gives 𝜏n for pk = c1k−2 and qk = c2k−3, k ≥1. The pair of tail indices
are plotted on the same vertical scale from 0 to 25, ranging from n = 1 to
n = 1000. The divergent patterns suggested by Theorem 6.2 are visible.
6.3
Examples and Remarks
Three constructed examples are given in this section, and each illustrates a
point of interest. The ﬁrst constructed example shows that the notion of thinner
tail, in the usual sense of pk ≤qk for k ≥k0 where k0 ≥1 is some ﬁxed integer
and p = {pk} and q = {qk} are two distributions, does not imply dominance of
q over p.
Example 6.11
Consider any strictly decreasing distribution q = {qk; k ≥1} ∈
𝒫+ and the following grouping of the index set {k; k ≥1}.
G1 ={1},
G2 ={2, 3},
⋮,
Gm =
{m(m −1)
2
+ 1,…, m(m −1)
2
+ m
}
,
⋮.

224
Statistical Implications of Turing’s Formula
{Gm; m ≥1} is a partition of the index set {k; k ≥1} and each group Gm
contains m consecutive indices. A new distribution p = {pk} is constructed
according to the following steps:
1) For each m ≥2, let pk = qm(m−1)∕2+m for all k ∈Gm.
2) p1 = 1 −∑
k≥2 pk.
In the ﬁrst step, m(m −1)∕2 + m = m(m + 1)∕2 is the largest index in Gm, and
therefore qm(m+1)∕2 is the smallest qk with index k ∈Gm. Since
0 ≤
∑
k≥2
pk =
∑
m≥2
mqm(m+1)∕2 <
∑
k≥2
qk ≤1,
p1 so assigned is a probability. The distribution p = {pk} satisﬁes pk ≤qk
for every k ≥2 = k0. However, the number of terms of pi in the interval
(qm(m+1)∕2+1, qm(m+1)∕2] is at least m and it increases indeﬁnitely as m →∞; and
hence q does not dominate p.
The second constructed example shows that the notion of dominance of
q = {qk} over p = {pk}, as deﬁned in Deﬁnition 6.2, does not imply that p has
thinner tail than q, in the usual sense of pk ≤qk for k ≥k0 where k0 ≥1 is
some ﬁxed integer.
Example 6.12
Consider any strictly decreasing distribution q = {qk; k ≥
1} ∈𝒫+ and the following grouping of the index set {k; k ≥1}.
G1 = {1, 2}, G2 = {3, 4},…, Gm = {2m −1, 2m}, … .
{Gm; m ≥1} is a partition of the index set {k; k ≥1} and each group Gm con-
tains two consecutive indices, the ﬁrst one odd and the second one even. The
construction of a new distribution p = {pk} is as follows: for each group Gm with
its two indices k = 2m −1 and k + 1 = 2m, let pk = pk+1 = (qk + qk+1)∕2. With
the new distribution p = {pk} so deﬁned, one has p2m < q2m and p2m−1 > q2m−1
for all m ≥1. Clearly q dominates p (p dominates q as well), but p does not have
a thinner tail in the usual sense.
At this point, it becomes clear that the notation of dominance of Deﬁni-
tion 6.2 and the notation of thinner/thicker tail in the usual sense are two inde-
pendent notions.
The next constructed example shows that there exists a distribution such
that the associated 𝜏n approaches inﬁnity along one subsequence of ℕand is
bounded above along another subsequence of ℕ, hence belonging to Domain
T. Domain T is not empty.
Example 6.13
Consider the probability sequence qj = 2−j, for j = 1, 2,… ,
along with a diﬀusion sequence di = 2i, for i = 1, 2,… . A probability sequence
{pk}, for k = 1, 2,… , is constructed by the following steps:

Domains of Attraction on Countable Alphabets
225
1st:
a) Take the ﬁrst value of di, d1 = 21, and assign the ﬁrst 2d1 = 22 = 4 terms
of qj,
q1 = 2−1, q2 = 2−2, q3 = 2−3, q4 = 2−4,
to the ﬁrst four terms of pk,
p1 = 2−1, p2 = 2−2, p3 = 2−3, p4 = 2−4.
b) Take the next unassigned term in qj, q5 = 2−5, and diﬀuse it into d1 = 2
equal terms, 2−6 and 2−6.
i) Starting at q5 in the sequence {qj}, look forwardly (j > 5) for terms
greater or equal to 2−6, if any, continue to assign them to pk. In this
case, there is only one such term q6 = 2−6 and it is assigned to p5 = 2−6.
ii) Take the d1 = 2 diﬀused terms and assign them to p6 = 2−6 and p7 =
2−6. At this point, the ﬁrst few terms of the partially assigned sequence
{pk} are
p1 = 2−1,
p2 = 2−2,
p3 = 2−3,
p4 = 2−4,
p5 = 2−6,
p6 = 2−6,
p7 = 2−6.
2nd:
a) Take the next value of di, d2 = 22, and assign the next 2d2 = 23 = 8 unused
terms of qj,
q7 = 2−7, … , q14 = 2−14,
to the next eight terms of pk,
p8 = 2−7, … , p15 = 2−14.
b) Take the next unassigned term in qj, q15 = 2−15, and diﬀuse it into d2 = 4
equal terms of 2−17 each.
i) Starting at q15 in the sequence of {qj}, look forwardly (j > 15) for terms
greater or equal to 2−17, if any, continue to assign them to pk. In this case,
there are 2 such terms
q16 = 2−16, q17 = 2−17,
and they are assigned to
p16 = 2−16, p17 = 2−17.
ii) Take the d2 = 22 = 4 diﬀused terms and assign them to
p18 = 2−17, … , p21 = 2−17.

226
Statistical Implications of Turing’s Formula
At this point, the ﬁrst few terms of the partially assigned sequence {pk}
are
p1 = 2−1,
p2 = 2−2,
p3 = 2−3,
p4 = 2−4,
p5 = 2−6,
p6 = 2−6,
p7 = 2−6,
p8 = 2−7,
p9 = 2−8,
… ,
p15 = 2−14, p16 = 2−16,
p17 = 2−17, p18 = 2−17,
… ,
p21 = 2−17.
ith:
a) In general, take the next value of di, say di = 2i, and assign the next
2di = 2i+1 unused terms of qj, say
qj0 = 2−j0, … , qj0+2i+1−1 = 2−(j0+2i+1−1),
to the next 2di = 2i+1 terms of pk, say
pk0 = 2−j0, … , pk0+2i+1−1 = 2−(j0+2i+1−1).
b) Take the next unassigned term in qj,
qj0+2i+1 = 2−(j0+2i+1),
and diﬀuse it into di = 2i equal terms, 2−(j0+i+2i+1) each.
i) Starting at qj0+2i+1 in the sequence of {qj}, look forwardly (j > j0 + 2i+1)
for terms greater or equal to 2−(j0+i+2i+1), if any, continue to assign them
to pk. Denote the last assigned pk as pk0.
ii) Take the di = 2i diﬀused terms and assign them to
pk0+1 = 2−(j0+i+2i+1), … , pk0+2i = 2−(j0+i+2i+1).
In essence, the sequence {pk} is generated based on the sequence {qj} with
inﬁnitely many selected j’s at each of which qj is diﬀused into increasingly
many equal probability terms according a diﬀusion sequence {di}. The diﬀused
sequence is then rearranged in a nonincreasing order.
By construction, it is clear that the sequence {pk; k ≥1} satisﬁes the following
properties:
1: {pk} is a probability sequence in a nonincreasing order.
2: As k increases, {pk} is a string of segments alternating between two diﬀerent
types: (i) a strictly decreasing segment and (ii) a segment (a run) of equal
probabilities.
3: As k increases, the length of the last run increases and approaches inﬁnity.
4: In each run, there are exactly di + 1 equal terms, di of which are diﬀused
terms and 1 of which belongs to the original sequence qj.
5: Between two consecutive runs (with lengths di + 1 and di+1 + 1, respec-
tively), the strictly decreasing segment in the middle has at least 2di+1 terms
and satisﬁes
2di+1 = 4di = di + 3di > di + di+1.
6: For any k, 1∕pk is a positive integer.

Domains of Attraction on Countable Alphabets
227
Next it is to show that there is a subsequence {ni} ∈ℕsuch that 𝜏ni deﬁned
with {pk} approaches inﬁnity. Toward that end, consider the subsequence
{pki; i ≥1} of {pk} where the index ki is such that pki is the ﬁrst term in the ith
run segment. Let {ni} = {1∕pki} which by 6 is a subsequence of ℕ. By 3 and
4,
𝜏ni = ni
∑
k≥1
pk(1 −pk)ni
> ni(di + 1)pki(1 −pki)ni
= (di + 1)
(
1 −1
ni
)ni
→∞.
Consider next the subsequence {pki−(di+1); i ≥1} of {pk} where the index ki is
such that pki is the ﬁrst term in the ith run segment, and therefore pki−(di+1) is the
(di + 1)th term counting backwards from pki−1, into the preceding segment of at
least 2di strictly decreasing terms. Let
{mi} =
{
1
pki−(di+1)
−1
}
(so pki−(di+1) = (mi + 1)−1) which by 6 is a subsequence of ℕ.
𝜏mi = mi
∑
k≥1
pk(1 −pk)mi
= mi
∑
k≤ki−(di+1)
pk(1 −pk)mi + mi
∑
k≥ki−di
pk(1 −pk)mi
∶= 𝜏mi,1 + 𝜏mi,2.
Before proceeding further, let several detailed facts be noted.
1) The function np(1 −p)n increases in [0, 1∕(n + 1)], attains maximum at
p = 1∕(n + 1), and decreases in [1∕(n + 1), 1].
2) Since pki−(di+1) = (mi + 1)−1, by 1 each summand in 𝜏mi,1 is bounded above
by
mipki−(di+1)(1 −pki−(di+1))mi
and each summand in 𝜏mi,2 is bounded above by
mipki−di(1 −pki−di)mi.
3) By 4 and 5, for each diﬀused term of pk′ with k′ ≤ki −(di + 1) in a run,
there is a diﬀerent nondiﬀused term pk′′ with k′′ ≤ki −(di + 1) such that
pk′ > pk′′ and therefore
mi pk′(1 −pk′)mi ≤mi pk′′(1 −pk′′)mi;

228
Statistical Implications of Turing’s Formula
and similarly, for each diﬀused term of pk′ with k′ ≥ki −di in a run, there
is a diﬀerent nondiﬀused term pk′′ with k′′ ≥ki −di such that pk′ < pk′′ and
therefore
mipk′ (1 −pk′ )mi ≤mipk′′(1 −pk′′)mi.
These facts imply that
𝜏mi = 𝜏mi,1 + 𝜏mi,2
= mi
∑
k≤ki−(di+1)
pk(1 −pk)mi + mi
∑
k≥ki−di
pk(1 −pk)mi
≤2mi
∑
j≥1
qj(1 −qj)mi < ∞
and the last inequality above is due to Corollary 6.2.
While the domains of attraction on alphabets have probabilistic merit, the
statistical implication is also quite signiﬁcant. Z1,v in (3.26) is an unbiased esti-
mator of 𝜁1,v, for v = 1,…, n −1, as is demonstrated in Chapter 2, and therefore
̂𝜏v = vZ1,v.
(6.6)
is an unbiased estimator of 𝜏v = v𝜁v. In fact, Zhang and Zhou (2010) established
several useful statistical properties of ̂𝜏v, including the asymptotic normality.
The availability of ̂𝜏v gives much added merit to the discussion of the domains
of attraction on alphabets as presented in this chapter. Speciﬁcally the fact that
the asymptotic behavior of 𝜏n characterizes the tail probability of the underlying
p and the fact that the trajectory of 𝜏v up to v = n −1 is estimable suggest that
much could be revealed by a suﬃciently large sample.
6.4
Appendix
6.4.1
Proof of Lemma 6.3
Proof: Let 𝛿∗= 𝛿∕8. Consider the partition of the index set
{k; k ≥1} = I ∪II
where
I = {k ∶pk ≤1∕n1−𝛿∗}
and
II = {k ∶pk > 1∕n1−𝛿∗} .
Since pe−np has a negative derivative with respect to p on interval (1∕n, 1] and
hence on (1∕n1−𝛿∗, 1] for large n, pke−npk attains its maximum at pk = 1∕n1−𝛿∗
for every k ∈II. Therefore, noting that there are at most n1−𝛿∗indices in II,

Domains of Attraction on Countable Alphabets
229
0 ≤n1−𝛿∑
k∈II
pk(1 −pk)n
≤n1−𝛿∑
k∈II
pke−npk
≤n1−𝛿∑
k∈II
(
1
n1−𝛿∗e−
n
n1−𝛿∗)
≤n1−𝛿n1−𝛿∗(
1
n1−𝛿∗e−
n
n1−𝛿∗)
= n1−𝛿e−n𝛿∗→0.
Thus
lim
n→∞n1−𝛿∑
k≥1
pk(1 −pk)n = lim
n→∞n1−𝛿∑
k∈I
pk(1 −pk)n
(6.7)
and
lim
n→∞n1−𝛿∑
k≥1
pke−npk = lim
n→∞n1−𝛿∑
k∈I
pke−npk.
(6.8)
On the other hand, since 1 −p ≤e−p for all p ∈[0, 1],
n1−𝛿∑
k∈I
pk(1 −pk)n ≤n1−𝛿∑
k∈I
pke−npk.
Furthermore, applying Parts 1 and 2 of Lemma 6.2 in the ﬁrst and the third
steps in the following, respectively, leads to
n1−𝛿∑
k∈I
pk(1 −pk)n ≥n1−𝛿∑
k∈I
pk exp
(
−npk
1 −pk
)
≥n1−𝛿∑
k∈I
pk exp
(
−
npk
1 −supi∈Ipi
)
≥n1−𝛿∑
k∈I
exp (−2n(supi∈I)2) pke−npk.
Noting the fact that
lim
n→∞exp(−2n(supi∈I)2) = 1
uniformly by the deﬁnition of I,
lim
n→∞n1−𝛿∑
k∈I
pk(1 −pk)n = lim
n→∞n1−𝛿∑
k∈I
pke−npk,
and hence, by (6.7) and (6.8), the lemma follows.
◽
6.4.2
Proof of Theorem 6.2
Proof: For clarity, the proof is given in two cases, respectively:
1) pk = ck−𝜆for all k ≥k0 for some k0 > 1, and
2) pk ≥ck−𝜆for all k ≥k0 for some k0 > 1.

230
Statistical Implications of Turing’s Formula
Case 1: Assuming pk = ck−𝜆for all k ≥k0, it suﬃces to consider the partial
series ∑
k≥k0npk(1 −pk)n. First consider
n1−1
𝜆
∞
∑
k=k0
pke−npk = n1−1
𝜆
∞
∑
k=k0
ck−𝜆e−nck−𝜆=
∞
∑
k=k0
fn(k)
where
fn(x) = n1−1
𝜆cx−𝜆e−ncx−𝜆.
Since it is easily veriﬁed that
f
′
n(x) = −𝜆cn1−1
𝜆x−(𝜆+1)(1 −ncx−𝜆)e−ncx−𝜆,
it can be seen that fn(x) increases over [1, (nc)1∕𝜆] and decreases
over
[(nc)1∕𝜆, ∞)
for every suﬃciently large n. Let x0 = k0 and
x(n) = (nc)1∕𝜆. It is clear that fn(x0) →0 and
fn(x(n)) = n1−1
𝜆c(nc)−1e−nc(nc)−1
= n1−1
𝜆c(nc)−1e−1
=
1
en1∕𝜆→0.
Invoking the Euler–Maclaurin lemma, with changes of variable
t = x−𝜆and then s = nct,
n1−1
𝜆
∞
∑
k=k0
pke−npk ∼∫
∞
x0
n1−1
𝜆cx−𝜆e−ncx−𝜆dx
= c
𝜆∫
x−𝜆
0
0
n1−1
𝜆t−1
𝜆e−nctdt
= c
𝜆n1−1
𝜆∫
x−𝜆
0
0
(nct)−1
𝜆(nc)−1+ 1
𝜆e−nctd(nct)
= c
𝜆n1−1
𝜆(nc)−1+ 1
𝜆∫
ncx−𝜆
0
0
s−1
𝜆e−sds
= c
1
𝜆
𝜆n0
∫
ncx−𝜆
0
0
s−1
𝜆e−sds
= c
1
𝜆
𝜆∫
ncx−𝜆
0
0
s
(
1−1
𝜆
)
−1e−sds
= c
1
𝜆
𝜆Γ
(
1 −1
𝜆
) ⎡
⎢
⎢
⎢⎣
1
Γ
(
1 −1
𝜆
) ∫
ncx−𝜆
0
0
s
(
1−1
𝜆
)
−1e−sds
⎤
⎥
⎥
⎥⎦
→c
1
𝜆
𝜆Γ
(
1 −1
𝜆
)
> 0.

Domains of Attraction on Countable Alphabets
231
Hence by Lemma 6.3,
n1−1∕𝜆
∞
∑
k=1
pk(1 −pk)n →c1∕𝜆𝜆−1Γ (1 −1∕𝜆) > 0,
and therefore 𝜏n →∞.
Case 2: Assuming pk ≥ck−𝜆=∶qk for all k ≥k0 for some k0 ≥1, one ﬁrst has
n1−1
𝜆
∑
k≥(nc)
1
𝜆
ck−𝜆e−nck−𝜆= n1−1
𝜆∑
k≥1
ck−𝜆e−nck−𝜆1
[
k ≥(nc)
1
𝜆
]
.
Since
fn(x) = n1−1
𝜆ck−𝜆e−nck−𝜆1
[
k ≥(nc)
1
𝜆
]
satisﬁes the condition of Lemma 1.6 (The Euler–Maclaurin lemma)
with x(n) = (nc)
1
𝜆and fn(x(n)) →0, one again has
n1−1
𝜆
∑
k≥[(n+1)c]
1
𝜆
ck−𝜆e−nck−𝜆
= c ∫
∞
1
n1−1
𝜆x−𝜆e−ncx−𝜆1
[
x ≥[(n + 1)c]
1
𝜆
]
dx
= c ∫
∞
[(n+1)c]
1
𝜆
n1−1
𝜆x−𝜆e−ncx−𝜆dx
= c
1
𝜆𝜆−1Γ
(
1 −1
𝜆
)
∫
n(n+1)c2
0
1
Γ
(
1 −1
𝜆
) s
(
1−1
𝜆
)
−1e−sds
→c
1
𝜆𝜆−1Γ
(
1 −1
𝜆
)
> 0.
(6.9)
On the other hand, for suﬃciently large n,
I∗=
{
k ∶pk ≤
1
n + 1
}
⊆{k; k ≥k0},
by Parts 1 and 2 of Lemma 6.2 at steps 2 and 4 below and (6.9) at
step 7, one has
n1−1∕𝜆∑
k∈I∗
pk(1 −pk)n ≥n1−1∕𝜆∑
k∈I∗
qk(1 −qk)n
≥n1−1∕𝜆∑
k∈I∗
qk exp
(
−nqk
1 −qk
)

232
Statistical Implications of Turing’s Formula
≥n1−1∕𝜆∑
k∈I∗
qk exp
⎛
⎜
⎜⎝
−
nqk
1 −sup
i∈I∗qi
⎞
⎟
⎟⎠
≥n1−1∕𝜆∑
k∈I∗
exp(−2n(sup
i∈I∗qi)2) qke−nqk
≥n1−1∕𝜆∑
k∈I∗
exp(−2∕n)qke−nqk
= exp(−2∕n)n1−1∕𝜆∑
k∈I∗
ck−𝜆e−nck−𝜆
→c
1
𝜆𝜆−1Γ
(
1 −1
𝜆
)
> 0.
Finally,
𝜏n = n
∑
k
pk(1 −pk)n ≥n1∕𝜆n1−1∕𝜆∑
k∈I∗
pk(1 −pk)n →∞
as n →∞.
◽
6.4.3
Proof of Lemma 6.6
Proof: For clarity, the proof of Lemma 6.6 is given in three parts, Part 1: Pre-
liminaries, Part 2: Part 1 of Lemma 6.6, and Part 3: Part 2 of Lemma 6.6.
Part 1: Preliminaries. Noting that the ﬁrst ﬁnite terms of 𝜏n vanishes expo-
nentially fast for any distribution, one may assume, without loss of generality,
that k0 = 1. For any given n suﬃciently large, deﬁne k∗= k∗(n) by
pk∗+1 <
1
n + 1 ≤pk∗.
(6.10)
Noting
c0e−(k∗+1) <
1
n + 1 ≤c0e−k∗,
e−(k∗+1) <
1
c0(n + 1) ≤e−k∗,
−(k∗+ 1) < −ln(c0(n + 1)) ≤−k∗,
and
k∗+ 1 > ln(c0(n + 1)) ≥k∗,
k∗may be expressed as
k∗= ⌊ln(c0(n + 1))⌋
for each n. That is to say that, although k∗is uniquely deﬁned by any given n,
each k∗may correspond to several consecutive integer values of n. For a given
integer value k∗, let the said consecutive integer values of n be denoted by
{nk∗, nk∗+ 1,…, nk∗+1 −1},
(6.11)
speciﬁcally noting that

Domains of Attraction on Countable Alphabets
233
1) nk∗is the smallest integer value of n corresponding to k∗by (6.10), that is,
k∗= ⌊ln(c0(n + 1))⌋;
2) nk∗+1 is the smallest integer value of n that satisﬁes k∗+ 1 = ⌊ln(c0(n + 1))⌋;
and
3) nk∗+1 −1 is the greatest integer value of n that shares the same value of k∗
with nk∗.
Since k∗= k∗(n) depends on n, one may express pk∗as, and deﬁne c(n) by,
pk∗= c(n)
n .
(6.12)
At this point, the following fact is established: for each given k∗,
pk∗= c(nk∗)
nk∗
= c(nk∗+ 1)
nk∗+ 1
= · · · = c(nk∗+1 −1)
nk∗+1 −1 .
(6.13)
There are two main consequences of the expression in (6.12). The ﬁrst is
that 𝜏n deﬁned in (6.2) may be expressed by (6.15); and the second is that the
sequence c(n) perpetually oscillates between 1 and e. Both facts are demon-
strated below.
Noting that the function fn(p) = np(1 −p)n increases for p ∈(0, 1∕(n + 1))
and decreases for p ∈(1∕(n + 1), 1), for any n
fn(pk) ≤fn(pk∗),
k ≤k∗
fn(pk) < fn(pk∗),
k ≥k∗+ 1.
(6.14)
For a given n, rewrite each pk in terms of pk∗, and therefore in terms of n and
c(n):
pk∗+i = e−i c(n)
n
and
pk∗−j = ej c(n)
n
for all appropriate positive integers i and j. Therefore,
fn(pk∗+i) = ne−i c(n)
n
(
1 −e−i c(n)
n
)n
= c(n)
ei
(
1 −c(n)
nei
)n
,
fn(pk∗−j) = nej c(n)
n
(
1 −ej c(n)
n
)n
= c(n)ej
(
1 −c(n)ej
n
)n
,

234
Statistical Implications of Turing’s Formula
and
𝜏n =
∑
k≤k∗−1
fn(pk) + fn(pk∗) +
∑
k≥k∗+1
fn(pk)
= c(n)
k∗−1
∑
j=1
ej
(
1 −c(n)ej
n
)n
+ c(n)
(
1 −c(n)
n
)n
+ c(n)
∞
∑
i=1
e−i
(
1 −c(n)
nei
)n
.
(6.15)
Next, it is to show that c(n) oscillates perpetually over the interval (n∕(n +
1), e), which approaches [1, e) as n increases indeﬁnitely. This is so because,
since k∗is deﬁned by (6.10),
c(n)
n e−1 ≤
1
n + 1 ≤c(n)
n
or
e−1 <
n
n + 1 ≤c(n) ≤
n
n + 1e < e.
(6.16)
At this point, the fact c(n) ∈[1, e) is established. What remains to be shown is
that c(n) oscillates perpetually in n. Toward that end, consider k∗(n) as a map-
ping, which maps every positive integer value of n ∈ℕto a positive integer
value of k∗∈ℕ. The inverse of k∗(n) maps every integer value k∗∈ℕto a set
as in (6.11). Let
ℕ= ∪{nk∗, nk∗+ 1,…, nk∗+1 −1}
(6.17)
where the union is over all possible integer values of k∗. (In fact, the smallest k∗
possible is k∗= 1 for nk∗= 1. For this case, p1 = 1 −e−1 ≈0.6321, p2 = p1e−1 ≈
0.2325, and (1 + 1)−1 = 0.5; therefore, k∗= 1 and n1 = 1.)
Observe the following three facts:
1) c(nk∗) < c(nk∗+ 1) < · · · < c(nk∗+1 −1). This is so because of (6.13): all of
them sharing the same k∗and therefore the same pk∗. Furthermore, the
increments of increase are all identical, namely, pk∗.
2) Consider {nk∗; k∗≥1} where nk∗is the smallest integer value in each parti-
tioning set of (6.17). One has c(nk∗) = nk∗pk∗→1. This is so because 1∕nk∗>
pk∗≥1∕(nk∗+ 1) or
1 −pk∗≤nk∗pk∗< 1,
(6.18)
which implies that nk∗pk∗for all suﬃciently large k∗(or equivalently suﬃ-
ciently large nk∗or suﬃciently large n),
c(nk∗) = nk∗pk∗∈(1 −ε, 1)
(6.19)
where ε > 0 is an arbitrarily small real value.

Domains of Attraction on Countable Alphabets
235
3) Consider {nk∗+1 −1; k∗≥1} where nk∗+1 −1 is the greatest integer value in
each partitioning set of (6.17). One has
c(nk∗+1 −1) = (nk∗+1 −1)pk∗→e.
This is so because
pk∗= pk∗+1e
= nk∗+1 −1
nk∗+1 −1pk∗+1e
=
1
nk∗+1 −1
(nk∗+1 −1
nk∗+1
)
(nk∗+1pk∗+1)e,
and therefore by (6.18)
c(nk∗+1 −1) =
(nk∗+1 −1
nk∗+1
)
(nk∗+1pk∗+1)e →e.
At this point, it has been established that the range of c(n) for n ≥n0, where
n0 is any positive integer, covers the entire interval [1, e).
Part 2: Part 1 of Lemma 6.6. Noting that e−1 ≤c(n) ≤e (see (6.16)) and that
1 −p ≤e−p for all p ∈[0, 1], the desired result follows the argument below.
𝜏n = c(n)
k∗−1
∑
j=1
ej
(
1 −c(n)ej
n
)n
+ c(n)
(
1 −c(n)
n
)n
+ c(n)
∞
∑
j=1
e−j
(
1 −c(n)
nej
)n
≤e
k∗−1
∑
j=1
ej
(
1 −ej−1
n
)n
+ e
(
1 −e−1
n
)n
+ e
∞
∑
j=1
e−j (
1 −
1
nej+1
)n
≤e
k∗−1
∑
j=1
eje−ej−1 + e
∞
∑
j=0
e−je−e−(j+1)
≤e2
k∗−1
∑
j=1
ej−1e−ej−1 + e2
∞
∑
j=0
e−(j+1)e−e−(j+1)
< e2
∞
∑
j=0
eje−ej + e2
∞
∑
j=1
e−je−e−j ∶= u.
Part 3: Part 2 of Lemma 6.6. Consider, for any ﬁxed c > 0,
𝜏∗
n = c
k∗−1
∑
j=1
ej
(
1 −cej
n
)n
+ c
(
1 −c
n
)n
+ c
∞
∑
j=1
e−j (
1 −c
nej
)n
.

236
Statistical Implications of Turing’s Formula
By dominated convergence theorem,
𝜏(c)∶= lim
n→∞𝜏∗
n = c
∞
∑
j=0
eje−cej + c
∞
∑
j=1
e−je−ce−j,
and 𝜏(c) is a nonconstant function in c on [1, e].
Noting that as k∗increases (or equivalently, nk∗increases or n increases),
1 ←c(nk∗) < c(nk∗+ 1) < · · · < c(nk∗+1 −1) →e
and that the increment between two consecutive terms pk∗→0, c(n) visits any
arbitrarily small closed interval [a, b] ⊂[1, e] inﬁnitely often, and therefore
there exists for each such interval a subsequence {nl; l ≥1} of ℕsuch that c(nl)
converges, that is, c(nl) →𝜃for some 𝜃∈[a, b]. Since 𝜏(c) is a nonconstant
function on [1, e], there exist two nonoverlapping closed intervals, [a1, b1] and
[a2, b2] in [1, e], satisfying
max
a1≤c≤b1
𝜏(c) < min
a2≤c≤b2 𝜏(c),
such that there exist two subsequences of ℕ, said {nl; l ≥1} and {nm; m ≥1},
such that c(nl) →𝜃1 for some 𝜃1 ∈[a1, b1] and c(nm) →𝜃2 for some
𝜃2 ∈[a2, b2].
Consider the limit of 𝜏n along {nl; l ≥1}, again by dominated convergence
theorem, as nl →∞,
𝜏nl =
[
c(nl)
k∗−1
∑
j=0
ej
(
1 −c(nl)ej
n
)n
+ c(nl)
∞
∑
j=1
e−j
(
1 −c(nl)
nej
)n]
→𝜃1
∞
∑
j=0
eje−𝜃1ej + 𝜃1
∞
∑
j=1
e−je−𝜃1e−j = 𝜏(𝜃1).
A similar argument gives limnm→∞𝜏nm = 𝜏(𝜃2), but 𝜏(𝜃1) ≠𝜏(𝜃2) by construc-
tion, and hence limn→∞𝜏n does not exist.
◽
6.5
Exercises
1
Show that 𝜁n = ∑
k≥1pk(1 −pk)n →0 as n →∞for any probability distri-
bution {pk} on 𝒳.
2
Prove Part 2 of Lemma 6.2, that is, for any real number x ∈(0, 1∕2),
1
1 −x < 1 + 2x.
3
Prove Lemma 6.5.
4
Use the ε–N language to show that

Domains of Attraction on Countable Alphabets
237
a) an = 1∕n converges to 0; and
b) bn = (n + 1)∕n2 converges to 0.
5
Use the ε–N language to show that
a) an = ∑n
i=1 3∕10i converges to 1∕3; and
b) bn = 5 + (−1)n+12
n
converges to 5.
6
Show that
a) limn→∞
(
1 + 1
n
)n
= e; and
b) limn→∞
(
1 −1
n
)n
= 1∕e.
7
If {an} is a convergent sequence, then every subsequence of that sequence
converges to the same limit.
8
If {an} is a sequence such that every possible subsequence extracted from
that sequences converge to the same limit, then the original sequence also
converges to that limit.
9
Show that
a) the sequence {an = sin n𝜋; n = 1, 2, · · ·} does not converge; and
b) the sequence {an = sin ln n; n = 1, 2, · · ·} does not converge.
10
Give a proof of the Bolzano–Weierstrass theorem: every bounded
sequence has a convergent subsequence. (Hint: Let a and b be the lower
and upper bounds of a bounded sequence. Consider the two intervals,
[a.(a + b)∕2] and [(a + b)∕2, b].)
11
The Squeeze rule: let {an}, {bn}, and {xn} be three sequences such that
a) an ≤xn ≤bn for every n ∈ℕ; and
b) limn→∞an = limn→∞bn = L,
for
some
ﬁnite
constant
L,
then
limn→∞xn = L. Use the ε–N language to prove the above-mentioned
Squeeze rule.
12
Show that every convergent sequence is bounded.
13
Let {an} be a positive sequence such that limn→∞an = L > 1. Prove that
{an
n} →∞. (Hint: limn→∞xn = ∞for any number x satisfying x > 1.)
14
Let
an = (−1)n + 1
n,
and consider the sequence {an; n = 1, 2,… }. Find lim sup an and
lim inf an.

238
Statistical Implications of Turing’s Formula
15
Let S be a nonempty subset of the real line R. Prove the following
statements.
a) Suppose that S is bounded above, that is, there exists a constant M such
that s ≤M for every s ∈S. Then 𝛼< sup S if and only if there exists an
a ∈S such that a > 𝛼.
b) Suppose that S is bounded below, that is, there exists a constant m such
that s ≥m for every s ∈S. Then 𝛽> inf S if and only if there exists an
b ∈S such that b < 𝛽.
16
For every sequence {an; n = 1, 2, · · ·}, let ∑∞
n=1 an = limn→∞
∑n
i=1 ai.
a) Find ∑∞
n=1 e−n.
b) Find ∑∞
n=10
1
n(n+1).
17
A series ∑n
i=1 an is said to be convergent if
∞
∑
i=1
an = lim
n→∞
n
∑
i=1
an = a
for some ﬁnite constant a. If such an a does not exists, then the series is
said to be divergent.
a) Show that ∑n
i=1 (−1)n is divergent.
b) Show that ∑n
i=1 n is divergent.
18
Show that ∑∞
n=1
(−1)n
n
= ln 2. (This series is known as the alternating har-
monic series. Hint: Consider the Taylor expansion of f (x) = ln x at x0 = 1.)
19
Show that the harmonic series, ∑∞
n=1
1
n, diverges to inﬁnity, that is,
lim
n→∞
∞
∑
n=1
1
n = ∞.
(Hint: Consider the inequalities below.
∞
∑
n=1
1
n = 1
1 + 1
2 + 1
3 + 1
4 + 1
5 + 1
6 + 1
7 + 1
8 + 1
9 + · · ·
> 1
1 + 1
2 + 1
4 + 1
4 + 1
8 + 1
8 + 1
8 + 1
8 + 1
16 + · · · .
)
20
Show that
1
n + ln n ≤
k∑
n=1
1
n ≤1 + ln n.

Domains of Attraction on Countable Alphabets
239
(Hint: For any decreasing function f (x),
∫
b+1
a
f (x)dx ≤
b
∑
i=a
f (i) ≤∫
b
a−1
f (x)dx,
where a and b are positive integers satisfying a < b.)
21
Find the limit of 1
n
∑n
k=2
1
ln k .
(Hint:
1
n
n
∑
k=2
1
ln k = 1
n
⌊ln n⌋
∑
k=2
1
ln k + 1
n
n
∑
k=⌊ln n⌋+1
1
ln k
< ⌊ln n⌋−1
n ln 2
+ n −⌊ln n⌋
n ln⌊ln n⌋.
)
22
Show that the notion of dominance by Deﬁnition 6.2 is transitive, that is,
if p1 dominates p2 and p2 dominates p3, then p1 dominates p3.

241
7
Estimation of Tail Probability
The remarkable Turing’s formula suggests that certain useful knowledge of the
tail of a distribution {pk; k ≥1} on 𝒳= {𝓁k; k ≥1} may be extracted from an
iid sample. This chapter demonstrates how such information could be used to
estimate parametric probability models in, and only in, the tail of the underlying
distribution, possibly beyond data range.
7.1
Introduction
Consider a discrete probability distribution {pk; k ≥1} for which there exists
an unknown integer k0 > 0 such that, for each and every k ≥k0,
pk = Ck−𝜆
(7.1)
where C > 0 and 𝜆> 1 are unknown parameters. Suppose the primary problem
of interest is to estimate C and 𝜆with an iid sample of size n.
There are two somewhat unusual characteristics about this problem. First, the
parametric model is only imposed on the tail. For any k < k0, pk is not restricted
to any particular parametric form. Second, the threshold k0 is unknown, and
therefore for any particular observed value of the underlying random element,
say X = 𝓁k, it is not known whether k ≥k0. This model will be referred to as
the power, or the Pareto, tail model hereafter.
In the existing literature, there exists a well-known similar problem in the
continuous domain where the underlying random variable X has a continu-
ous distribution function F(x) such that 1 −F(x) = Cx−𝜆for x ≥x0 and F(x)
is unspeciﬁed for x < x0 where x0, C > 0 and 𝜆> 0 are unknown parameters.
The primary interest of this problem is also to estimate C and 𝜆. The exist-
ing methodological approaches to this problem are largely based on extreme
value theory and domains of attraction. The reference list on this problem is
extensive. Some of the more commonly cited publications include Hill (1975),
Haeusler and Teugels (1985), Hall and Welsh (1985), Smith (1987), Hall and
Weissman (1997), and Pickands (1975). However, while many parallels can be
Statistical Implications of Turing’s Formula, First Edition. Zhiyi Zhang.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

242
Statistical Implications of Turing’s Formula
drawn between this problem and the one on hand, it must be said that the two
problems are fundamentally diﬀerent. The extreme value theory that supports
the approaches to the continuous tail problem has little relevance in the prob-
lem under current consideration due to its discrete nature. A solution to the
current problem calls for an entirely diﬀerent approach.
In a similar spirit, a more general parametric tail model may be entertained.
Consider a discrete probability distribution {pk; k ≥1} for which there exists
an unknown integer k0 > 0 such that, for each and every k ≥k0,
pk = p(k; 𝜽)
(7.2)
where 𝜽is a vector of several parameters.
Consider a multinomial distribution with its countably inﬁnite number of
prescribed categories indexed by ℕ= {k; k ≥1} and its category probabilities
denoted by {pk; k ≥1}, satisfying 0 < pk < 1 for all k and ∑
k≥1pk = 1. This
model will be referred to as the general model hereafter. Let the category
counts in an iid sample of size n from the underlying population be denoted by
{Yk; k ≥1} and its observed values by {yk; k ≥1}. For a given sample, there
are at most n nonzero yks.
For every integer r, 1 ≤r ≤n, recall the following three important random
variables from Chapter 1,
Nr =
∑
1[Yk = r],
Tr =
r
n −r + 1Nr,
𝜋r−1 =
∑
pk1[Yk = r −1],
as given in (1.1), (1.27), and (1.2).
For any given positive integer R, consider the following two random vectors
of length R,
T = (T1, … , TR)𝜏
𝝅= (𝜋0, … , 𝜋R−1)𝜏.
In Chapter 1, it is argued that T is a reasonable estimator of 𝝅, at least in
the sense of Theorem 1.11. In this chapter, it is argued that Theorem 1.11 may
support parametric tail probability models such as (7.2) in general and (7.1) in
speciﬁc.
At a ﬁrst glance, it would seem a bit far-fetched that a nonparametric result
like Theorem 1.11 could support a parametric model in the tail, let alone a
distant tail possibly beyond data range. Yet it is the very core implication of
Turing’s formulae. It is perhaps instructive to give a moment of thought to why
such an undertaking is possible.

Estimation of Tail Probability
243
Consider the following disjoint random subsets of ℕ= {k; k = 1, 2, … }, for
an iid sample of size n and a given positive integer R,
𝕂0 = {k ∶Yk = 0}
𝕂1 = {k ∶Yk = 1}
⋮
𝕂R−1 = {k ∶Yk = R −1}
and their union
𝕂tail = ∪R−1
r=0 𝕂r.
(7.3)
For simplicity, assuming pk > 0 for every k ∈ℕ, 𝕂tail in (7.3) may be thought
of as a tail of the index set ℕ= {k; k ≥1} because the following key fact that
lim
n→∞P (𝕂tail ⊂{k; k ≥k0}) = 1
(7.4)
where k0 is the unknown positive integer threshold in the parametric tail model
in (7.1) or (7.2) (see Exercise 1).
Several summarizing remarks can now be given:
1) The summands in each 𝜋r of 𝝅are subject to the event {Yk = r} for a ﬁxed
r. This implies that category probabilities pk included in 𝜋r are dynamically
shifting into the tail in probability as n increases. Therefore, for a ﬁxed R, the
entire panel 𝝅shifts into the tail in probability. As a result, in probability,
all nonzero summands in 𝝅will eventually have indices satisfying k ≥k0.
Under the power tail model, as n increases, 𝝅will only contain pks such that
pk = Ck−𝜆, or whatever the parametric tail model may be. This is a partial
reason why a consistent estimator of the tail is possible.
2) The equation in (7.4) holds regardless whether k0 is known or not. This fact
gives much ﬂexibility to the approach and bypasses the necessity of knowing
(or estimating) k0 a priori.
3) By Turing’s formulae in general and Theorem 1.11 in speciﬁc, T chases 𝝅
while 𝝅chases the tail. In fact, 𝕂tail of (7.3) may be thought of as a “window
on tail.” It is in this sense that the subsequently described estimator of the
tail is said to be in Turing’s perspective.
In the next several sections, the Pareto tail model in (7.1) is studied in detail. A
distributional relationship between the observable T and the observable func-
tions of C and 𝜆, namely 𝝅, is explored; an asymptotic distribution of T −𝝅
is established; an estimator ( ̂C, ̂𝜆) of (C, 𝜆) based on the likelihood of the said
asymptotic distribution is deﬁned; the consistency of the estimator is estab-
lished; and a large sample conﬁdence region for (C, 𝜆) is derived.

244
Statistical Implications of Turing’s Formula
7.2
Estimation of Pareto Tail
Let p = {pk; k ≥1} be a probability distribution on 𝒳and g(n, 𝛿) = n1−2𝛿where
𝛿> 0 is a constant. The following is a condition on p.
Condition 7.1
There exists a 𝛿∈(0, 1∕4) such that as n →∞,
1) g2(n, 𝛿)
n2
E(Nr) →cr
r! ≥0,
2) g2(n, 𝛿)
n2
E(Nr+1) →
cr+1
(r + 1)! ≥0, and
3) cr + cr+1 > 0
where r is a positive integer.
Let
𝜎2
r = r2E(Nr) + (r + 1)rE(Nr+1),
𝜌r(n) = −r(r + 1)E(Nr+1)∕(𝜎r𝜎r+1),
𝜌r = lim
n→∞𝜌r(n),
̂𝜎2
r = r2Nr + (r + 1)rNr+1, and
̂𝜌r = ̂𝜌r(n) = −r(r + 1)Nr+1∕
√
̂𝜎2
r ̂𝜎2
r+1.
The following two lemmas are re-statements of Theorems 1.11 and 1.12, and
they are given here for easy reference.
Lemma 7.1
For any positive integer R, if Condition 7.1 holds for every r, 1 ≤
r ≤R −1, then
n
(T1 −𝜋0
𝜎1
, … , TR −𝜋R−1
𝜎R
)𝜏
L
−−−→MVN(0, Σ)
where Σ = (ai,j) is a R × R covariance matrix with all the diagonal elements
being ar,r = 1 for r = 1, … , R, the super-diagonal and the sub-diagonal
elements being ar,r+1 = ar+1,r = 𝜌r for r = 1, … , R −1, and all the other
oﬀ-diagonal elements being zeros.
Let ̂Σ be the resulting matrix of Σ with 𝜌r replaced by ̂𝜌r(n) for all r. Let
̂Σ−1 denote the inverse of ̂Σ and ̂Σ−1∕2 denote any R × R matrix satisfying
̂Σ−1 = ( ̂Σ−1∕2)𝜏̂Σ−1∕2.
Lemma 7.2
For any positive integer R, if Condition 7.1 holds for every r, 1 ≤
r ≤R, then
n ̂Σ−1∕2
(T1 −𝜋0
̂𝜎1
, … , TR −𝜋R−1
̂𝜎R
)𝜏
L
−−−→MVN(0, IR×R).

Estimation of Tail Probability
245
Lemma 7.3
Under the power tail model (7.1), Condition 7.1 holds for every r,
1 ≤r ≤R −1.
Proof: Letting 𝛿= (4𝜆)−1, it can be veriﬁed that
nr−2g2(n, 𝛿)
∑
k≥1
pr
ke−npk →cr > 0
for every integer r > 0.
◽
See Exercise 2.
Corollary 7.1
Under the power tail model in (7.1), the results of both Lemmas
7.1 and 7.2 hold.
Under the power tail model in (7.1), for every r > 0, let every pk in 𝜋r−1 be
replaced by pk = Ck−𝜆and denote the resulting expression as 𝜋∗
r−1, that is,
𝜋∗
r−1 = C
∑
k≥1
k−𝜆1[Yk = r −1].
Then
𝜋r−1 −𝜋∗
r−1 =
k0−1
∑
k=1
1[Yk = r −1] (pk −Ck−𝜆) ,
and the sum is of ﬁnite terms. Since, for every k, both E(1[Yk = r −1])
and Var(1[Yk = r −1]) converge to zero exponentially in sample size n,
E(g(n, 𝛿)1[Yk = r −1]) and Var(g(n, 𝛿)1[Yk = r −1]) converge to zero, which
implies that g(n, 𝛿)1[Yk = r −1]
p→0, which in turn implies that
g(n, 𝛿)(𝜋r−1 −𝜋∗
r−1)
p
−−−→0
for every r. This argument leads to the following two lemmas.
Lemma 7.4
Under the power tail model in (7.1),
n
(T1 −𝜋∗
0
𝜎1
,…,
TR −𝜋∗
R−1
𝜎R
)𝜏
L
−−−→MVN(0, Σ)
where Σ is as in Lemma 7.1.
Lemma 7.5
Under the power tail model in (7.1),
n ̂Σ−1∕2
(T1 −𝜋∗
0
̂𝜎1
,…,
TR −𝜋∗
R−1
̂𝜎R
)𝜏
L
−−−→MVN(0, IR×R)
(7.5)
where ̂Σ−1∕2 is as in Lemma 7.2.

246
Statistical Implications of Turing’s Formula
The asymptotic likelihood given by (7.5) depends on the two model parame-
ters, 𝜆and C, only via
𝜋∗
r−1 = 𝜋∗
r−1(C, 𝜆) = C
∑
k≥1
1[Yk = r −1]k−𝜆,
(7.6)
r = 1,…, R.
Deﬁnition 7.1
If there exists a pair of values, C = ̂C and 𝜆= ̂𝜆, which max-
imizes the likelihood given by (7.5) in a neighborhood of C > 0 and 𝜆> 1, then
( ̂C, ̂𝜆) is said to be an asymptotic maximum likelihood estimator (AMLE) of
(C, 𝜆).
Let R = 2, and denote, hereafter, ( ̂C, ̂𝜆) as AMLE by Deﬁnition 7.1 but with
R = 2. The following is a corollary of Theorem 7.5.
Corollary 7.2
Under the model in (7.1),
̂Σ−1∕2
2
(T1 −𝜋∗
0, T2 −𝜋∗
1
)𝜏
L
−−−→MVN(0, I2×2)
(7.7)
where
̂Σ−1∕2
2
=
n
2
√
1 −̂𝜌2
1
( √
1 −̂𝜌1 +
√
1 + ̂𝜌1
√
1 −̂𝜌1 −
√
1 + ̂𝜌1
√
1 −̂𝜌1 −
√
1 + ̂𝜌1
√
1 −̂𝜌1 +
√
1 + ̂𝜌1
)
×
⎛
⎜
⎜⎝
1∕̂𝜎1
0
0
1∕̂𝜎2
⎞
⎟
⎟⎠
.
(7.8)
Given a sample, an AMLE minimizes
lln = lln(C, 𝜆)
∶= (T1 −𝜋∗
0, T2 −𝜋∗
1
) ̂Σ−1
2
(T1 −𝜋∗
0, T2 −𝜋∗
1
)𝜏.
The properties of AMLEs may be examined by the statistical behavior of an
appropriately proportioned version of lln, namely ln as deﬁned in (7.9). Let
g2(n, 𝛿)Ωn = ̂Σ−1
2 .
Recall that, under the power tail model,
g(n, 𝛿) = n1−1∕(2𝜆0)
where 𝜆0 is the true value of 𝜆. It can be veriﬁed that
Ωn = (𝜔ij(n))
p
−−−→Ω

Estimation of Tail Probability
247
where Ω = (𝜔ij) is some positive deﬁnite 2 × 2 matrix. The logarithmic likeli-
hood given by (7.5) is negatively proportional to
ln = ln(C, 𝜆)
∶= n−1∕𝜆0n2−1∕𝜆0(lln(C, 𝜆)∕n2−1∕𝜆0)
= n2−2∕𝜆0(T1 −𝜋∗
0, T2 −𝜋∗
1)Ωn (T1 −𝜋∗
0, T2 −𝜋∗
1)𝜏.
(7.9)
The examination of ln(C, 𝜆), a smooth random surface deﬁned on the param-
eter space
Θ = {(C, 𝜆); C > 0 and 𝜆> 1},
is carried out separately on three subsets of Θ. Let Θ = ∪3
s=1Θs where
1) Θ1 = {(C, 𝜆); 𝜆≥𝜆0}
2) Θ2 =
{
(C, 𝜆); max
(
𝜆0 −
1
2𝜆0 , 1
)
< 𝜆< 𝜆0
}
3) Θ3 =
{
(C, 𝜆); 1 < 𝜆≤𝜆0 −
1
2𝜆0
}
if 𝜆0 −
1
2𝜆0 > 1, or Θ3 is an empty set
otherwise.
{Θ1, Θ2, Θ3} is a partition of the parameter space along the interval 𝜆∈(1, ∞)
with two dividing points: 𝜆0 −1∕(2𝜆0) and 𝜆0, provided that the ﬁrst point is
on the right side of 1. The necessity of examining ln separately on these sets lies
in the qualitative diﬀerence in its respective behavior on the sets. As it turns
out, ln is well behaved on Θ1 as all of its components are convergent in some
sense; whereas on Θ2 and Θ3, many of the components of ln are divergent, and
consequently ln requires a diﬀerent treatment on each of these sets.
This section ends with a useful lemma below.
Lemma 7.6
Let Ω be a positive deﬁnite 2 × 2 matrix. For each r, r = 1, 2, let
ar(n) and Ar(n) be sequences of real values satisfying ar(n) →0 and Ar(n) →
−∞. Then
(A1(n), A2(n))Ω(A1(n), A2(n)))𝜏+ (a1(n), a2(n))Ω(A1(n), A2(n))𝜏→+∞.
Proof: Since Ω is positive deﬁnite, Ω = P𝜏ΛP where P is orthogonal and Λ is
diagonal matrix with two positive elements 𝜆1 and 𝜆2. Let
(Q1, Q2)𝜏= P(A1(n), A2(n))𝜏
and
(q1, q2)𝜏= P(a1(n), a2(n))𝜏.
Since
Q2
1 + Q2
2 = A2
1(n) + A2
2(n) →+∞,
|Q1| + |Q2| →+∞
and
hence
𝜆1|Q1| + 𝜆2|Q2| →+∞. Also P(a1(n), a2(n))𝜏= (q1, q2)𝜏→(0, 0)𝜏. On the
other hand, for a suﬃciently large n,
(A1(n), A2(n))Ω(A1(n), A2(n)) + (a1(n), a2(n))Ω(A1(n), A2(n))
= 𝜆1Q2
1 + 𝜆2Q2
2 + 𝜆1q1Q1 + 𝜆2q2Q2

248
Statistical Implications of Turing’s Formula
≥𝜆1Q2
11[|Q1| > 1] + 𝜆1Q2
11[|Q1| ≤1]
−1
3𝜆1|Q1|1[|Q1| > 1] −1
3𝜆1|Q1|1[|Q1| ≤1]
+ 𝜆2Q2
21[|Q2| > 1] + 𝜆2Q2
21[|Q2| ≤1]
−1
3𝜆2|Q2|1[|Q2| > 1] −1
3𝜆2|Q2|1[|Q2| ≤1]
= 𝜆1|Q1|1[|Q1| > 1]
(
|Q1| −1
3
)
+ 𝜆1|Q1|1[|Q1| ≤1]
(
|Q1| −1
3
)
+ 𝜆2|Q2|1[|Q2| > 1]
(
|Q2| −1
3
)
+ 𝜆2|Q2|1[|Q2| ≤1]
(
|Q2| −1
3
)
≥𝜆1|Q1|1[|Q1| > 1]2
3 −1
3𝜆1|Q1|1[|Q1| ≤1]
+ 𝜆2|Q2|1[|Q2| > 1]2
3 −1
3𝜆2|Q2|1[|Q2| ≤1]
≥2𝜆1
3 |Q1|1[|Q1| > 1] −1
3𝜆1 + 2𝜆2
3 |Q2|1[|Q2| > 1] −1
3𝜆2
= 2𝜆1
3 |Q1| −2𝜆1
3 |Q1|1[|Q1| ≤1] −1
3𝜆1 + 2𝜆2
3 |Q2|
−2𝜆2
3 |Q2|1[|Q2| ≤1] −1
3𝜆2
≥2
3(𝜆1|Q1| + 𝜆2|Q2|) −(𝜆1 + 𝜆2) →+∞.
◽
7.3
Statistical Properties of AMLE
The uniqueness and consistency of the AMLE of (C, 𝜆), ( ̂C, ̂𝜆), are respectively
established in this section.
Theorem 7.1
Under the power tail model in (7.1), the AMLE of (C, 𝜆) uniquely
exists for a suﬃciently large n in a neighborhood of (C0, 𝜆0), where C0 and 𝜆0 are
the true values of the parameters.
The proof of Theorem 7.1 is based on the fact that the probability that the
Hessian matrix of ln(C, 𝜆) at the true parameter values (C0, 𝜆0) is positive deﬁ-
nite converges to 1 as n →∞. To establish the said fact, Lemma 7.7 is needed.
Toward stating Lemma 7.7, several notations are needed. Let
un =
∑
k≥1
1[Yk = 0]k−𝜆,
vn =
∑
k≥1
1[Yk = 0](ln k)k−𝜆,
wn =
∑
k≥1
1[Yk = 0](ln k)2k−𝜆,

Estimation of Tail Probability
249
Un =
∑
k≥1
1[Yk = 1]k−𝜆,
Vn =
∑
k≥1
1[Yk = 1](ln k)k−𝜆, and
Wn =
∑
k≥1
1[Yk = 1](ln k)2k−𝜆.
It can be veriﬁed that
∂2ln
∂C2 = 2n2−2∕𝜆0(un, Un)Ωn(un, Un)𝜏,
∂2𝓁n
∂𝜆2 = −2n2−2∕𝜆0C(wn, Wn)Ωn(T1 −𝜋∗
0, T2 −𝜋∗
1)𝜏
+ 2n2−2∕𝜆0C2(vn, Vn)Ωn(vn, Vn)𝜏,
∂2𝓁n
∂𝜆∂C = 2n2−2∕𝜆0(vn, Vn)Ωn(T1 −𝜋∗
0, T2 −𝜋∗
1)𝜏
−2n2−2∕𝜆0C(vn, Vn)Ωn(un, Un)𝜏.
(7.10)
Let C0 and 𝜆0 be the true values of C and 𝜆. For notation simplicity, in the
statements of Lemma 7.7, let un, vn, wn, Un, Vn, Wn be those as deﬁned earlier
but evaluated speciﬁcally at (C, 𝜆) = (C0, 𝜆0).
Lemma 7.7
Under the power tail model in (7.1), for any ε > 0,
1) n1−1∕𝜆0un
p→
1
𝜆0C
1−1∕𝜆0
0
Γ(1 −1∕𝜆0),
2) n1−1∕𝜆0Un
p→
1
𝜆0C
1−1∕𝜆0
0
Γ(2 −1∕𝜆0),
3) P(n2−2∕𝜆0(vn, Vn)Ωn(vn, Vn)𝜏> 0) −−−→1,
4) n1−1∕𝜆0−εvn
p→0,
5) n1−1∕𝜆0−εVn
p→0,
6) n1−1∕𝜆0−εwn
p→0,
7) n1−1∕𝜆0−εWn
p→0,
8) P(n2−2∕𝜆0(vnUn −unVn) > 0) →1.
The proofs of the Parts 1, 3, 4, and 8 of Lemma 7.7 are given in Appendix. The
proofs of Parts 2, 5, 6, and 7 are left as exercises.
Proof of Theorem 7.1: The ﬁrst two expressions in (7.10) are the diagonal ele-
ments of the Hessian matrix of ln, and the third is the oﬀ-diagonal element.
The objective of establishing that the probability of the Hessian matrix being
positive deﬁnite, at (C0, 𝜆0), converges to 1 as n →∞is served by showing the

250
Statistical Implications of Turing’s Formula
probabilities of the following three events each, at (C0, 𝜆0), converges to 1 as
n →∞:
(a) ∂2ln
∂C2
|||||(C,𝜆)=(C0,𝜆0)
> 0,
(b) ∂2ln
∂𝜆2
|||||(C,𝜆)=(C0,𝜆0)
> 0,
and
(c)
(∂2ln
∂C2
) (∂2ln
∂𝜆2
)
−
( ∂2ln
∂𝜆∂C
)2|||||(C,𝜆)=(C0,𝜆0)
> 0.
By (1) and (2) of Lemma 7.7, and the fact that Ωn converges to a positive deﬁnite
matrix in probability, (a) follows immediately.
For (b), since n1−1∕(2𝜆0)(T1 −𝜋∗
0, T2 −𝜋∗
1)𝜏converges to a bivariate normal vec-
tor, by (6) and (7) of Lemma 7.7, the ﬁrst term of ∂2ln∕∂𝜆2|(C,𝜆)=(C0,𝜆0) is
−2n2−2∕𝜆0C(wn, Wn)Ωn(T1 −𝜋∗
0, T2 −𝜋∗
1)𝜏
= −2Cn1−3∕(2𝜆0)(wn, Wn)Ωn[n1−1∕(2𝜆0)(T1 −𝜋∗
0, T2 −𝜋∗
1)𝜏]
p
−−−→0.
Then (b) follows from (3) of Lemma 7.7.
For (c),
(∂2ln
∂C2
) (∂2ln
∂𝜆2
)
−
( ∂2ln
∂𝜆∂C
)2|||||(C,𝜆)=(C0,𝜆0)
= −4Cn4−4∕𝜆0[(un, Un)Ωn(un, Un)𝜏] [(wn, Wn)Ωn(T1 −𝜋∗
0, T2 −𝜋∗
1)𝜏]
+ 4C2n4−4∕𝜆0[(un, Un)Ωn(un, Un)𝜏] [(vn, Vn)Ωn(vn, Vn)𝜏]
−4n4−4∕𝜆0[(vn, Vn)Ωn(T1 −𝜋∗
0, T2 −𝜋∗
1)𝜏]2
−4C2n4−4∕𝜆0[(vn, Vn)Ωn(un, Un)𝜏]2
+ 8Cn4−4∕𝜆0[(vn, Vn)Ωn(T1 −𝜋∗
0, T2 −𝜋∗
1)𝜏] [(vn, Vn)Ωn(un, Un)𝜏].
The ﬁrst, third, and ﬁfth terms in the last expression above all converge to 0 in
probability, and therefore after a few algebraic steps,
(∂2ln
∂C2
) (∂2ln
∂𝜆2
)
−
( ∂2ln
∂𝜆∂C
)2
= 4C2n4−4∕𝜆0[(un, Un)Ωn(un, Un)𝜏] [(vn, Vn)Ωn(vn, Vn)𝜏]
−4C2n4−4∕𝜆0[(vn, Vn)Ωn(un, Un)𝜏]2 + op(1)
= 4C2n4−4∕𝜆0(vnUn −unVn)2 det(Ωn) + op(1).
The desired result follows from (8) of Lemma 7.7 and the fact that Ωn converges
to a positive deﬁnite matrix in probability.
◽
Theorem 7.1 establishes the convexity of ln in an inﬁnitesimal neighborhood
of (C0, 𝜆0), but says nothing about ln globally. However, the next lemma does.

Estimation of Tail Probability
251
Lemma 7.8
Under the model in (7.1), if (C, 𝜆) ≠(C0, 𝜆0), then there exists a
constant c > 0 such that, as n →∞,
P(ln(𝜆, C) −ln(C0, 𝜆0) > c) →1.
The following theorem is a corollary of Lemma 7.8.
Theorem 7.2
Under the power tail model in (7.1), the AMLE of (C, 𝜆) is unique
for a suﬃciently large n and is consistent.
Toward proving Lemma 7.8, the following re-expression of ln(C, 𝜆) in (7.9) is
needed.
ln(C, 𝜆)
= n2−2∕𝜆0(T1 −𝜋∗
0(C, 𝜆), T2 −𝜋∗
1(C, 𝜆))Ωn(T1 −𝜋∗
0(C, 𝜆), T2 −𝜋∗
1(C, 𝜆))𝜏
= ln(C0, 𝜆0) + [n2−2∕𝜆0(𝜋∗
0(C0, 𝜆0) −𝜋∗
0(C, 𝜆), 𝜋∗
1(C0, 𝜆0) −𝜋∗
1(C, 𝜆))Ωn
× (𝜋∗
0(C0, 𝜆0) −𝜋∗
0(C, 𝜆), 𝜋∗
1(C0, 𝜆0) −𝜋∗
1(C, 𝜆))𝜏]
+ [2n2−2∕𝜆0(T1 −𝜋∗
0(C0, 𝜆0), T2 −𝜋∗
1(C0, 𝜆0))Ωn
× (𝜋∗
0(C0, 𝜆0) −𝜋∗
0(C, 𝜆), 𝜋∗
1(C0, 𝜆0) −𝜋∗
1(C, 𝜆))𝜏] .
(7.11)
To prove Lemma 7.8, Lemma 7.9 is needed. In both proofs of Lemmas 7.9 and
7.8, it is necessary to distinguish the notation 𝜋∗
0(C, 𝜆) from that of 𝜋∗
0(C0, 𝜆0),
and 𝜋∗
1(C, 𝜆) from that of 𝜋∗
1(C0, 𝜆0).
Lemma 7.9
Under the power tail model in (7.1),
1) if (C, 𝜆) ∈Θ1 but (C, 𝜆) ≠(C0, 𝜆0), then
a) n1−1∕𝜆0𝜋∗
0(C, 𝜆)
p→0, and
b) n1−1∕𝜆0𝜋∗
1(C, 𝜆)
p→0;
2) if (C, 𝜆) ∈Θ2, then
a) n1−1∕𝜆0−1∕(2𝜆0)𝜋∗
0(C, 𝜆)
p→0, and
b) n1−1∕𝜆0−1∕(2𝜆0)𝜋∗
1(C, 𝜆)
p→0;
3) if (C, 𝜆) ∈Θ2 ∪Θ3, then
a) n1−1∕𝜆0𝜋∗
0(C, 𝜆)
p→+∞, and
b) n1−1∕𝜆0𝜋∗
1(C, 𝜆)
p→+∞.
The proof of Lemma 7.9 is given in Appendix.
Proof of Lemma 7.8: The proof is given for three separate cases:
1) (C, 𝜆) ∈Θ1 but (C, 𝜆) ≠(C0, 𝜆0);
2) (C, 𝜆) ∈Θ2; and
3) (C, 𝜆) ∈Θ2 ∪Θ3.

252
Statistical Implications of Turing’s Formula
For (1), it is desired to show (i) that the third term in (7.11) converges to 0
in probability, and (ii) that the second term in (7.11) converges to a positive
constant in probability. Toward proving (i), ﬁrst noting 𝜋∗
0(C0, 𝜆0) ≥0 and by
the proof of Case 1 of Theorem 6.2,
n1−1∕𝜆0E(𝜋∗
0(C0, 𝜆0)) →C1∕𝜆0
𝜆0
Γ
(
1 −1
𝜆0
)
> 0,
hence
n1−1∕𝜆0−1∕(2𝜆0)E(𝜋∗
0(C0, 𝜆0)) →0,
and therefore
n1−1∕𝜆0−1∕(2𝜆0)𝜋∗
0(C0, 𝜆0)
p
−−−→0.
By (1) of Lemma 7.9,
n1−1∕𝜆0−1∕(2𝜆0)𝜋∗
0(C, 𝜆)
p
−−−→0,
and therefore
n1−1∕𝜆0−1∕(2𝜆0)(𝜋∗
0(C0, 𝜆0) −𝜋∗
0(C, 𝜆))
p
−−−→0.
Similarly
n1−1∕𝜆0−1∕(2𝜆0)(𝜋∗
1(C0, 𝜆0) −𝜋∗
1(C, 𝜆))
p
−−−→0.
On the other hand, noting
n2−2∕𝜆0 = n1−1∕(2𝜆0)n1−1∕𝜆0−1∕(2𝜆0)
and by Theorem 1.8, each of
n1−1∕(2𝜆0)(T1 −𝜋∗
0(C0, 𝜆0))
and
n1−1∕(2𝜆0)(T2 −𝜋∗
1(C0, 𝜆0))
converges weakly to some normal random variable, and therefore (i) follows.
Again by the proof of Case 1 of Theorem 6.2 and (1) of Lemma 7.9, the vector
n1−1∕𝜆0(𝜋∗
0(C0, 𝜆0) −𝜋∗
0(C, 𝜆), 𝜋∗
1(C0, 𝜆0) −𝜋∗
1(C, 𝜆))
converges to a nonzero vector in probability. Since Ωn converges to a positive
deﬁnite matrix in probability, the second term in (7.11) converges to a constant
c > 0, and hence (ii) follows, and therefore Lemma 7.8 follows for the case of
(C, 𝜆) ∈Θ1 but (C, 𝜆) ≠(C0, 𝜆0).
For (2), it is desired to show (i) that the third term in (7.11) converges to
0 in probability, and (ii) that the second term in (7.11) converges to +∞in
probability. However, (i) immediately follows from (2) of Lemma 7.9 and
(ii) immediately follows from (3) of Lemma 7.9.
For (3), ﬁrst by Corollary 7.2,
an = n1−1∕𝜆0(T1 −𝜋∗
0(C0, 𝜆0))
p
−−−→0,
bn = n1−1∕𝜆0(T2 −𝜋∗
1(C0, 𝜆0))
p
−−−→0;

Estimation of Tail Probability
253
and second by Lemma 7.9,
An = n1−1∕𝜆0(𝜋∗
0(C0, 𝜆0) −𝜋∗
0(C, 𝜆))
p
−−−→−∞,
Bn = n1−1∕𝜆0(𝜋∗
1(C0, 𝜆0) −𝜋∗
1(C, 𝜆))
p
−−−→−∞.
By Lemma 7.6,
ln(C, 𝜆) −ln(C0, 𝜆0))
= (An, Bn)Ωn(An, Bn)𝜏+ (an, bn)Ωn(An, Bn)𝜏
p
−−−→+∞.
◽
Finally, by Corollary 7.2, an approximate 1 −𝛼conﬁdence region for (C, 𝜆)
may be given as follows:
(T1 −𝜋∗
0(C, 𝜆)
̂𝜎1
)2
−2 ̂𝜌1
(T1 −𝜋∗
0(C, 𝜆)
̂𝜎1
) (T2 −𝜋∗
1(C, 𝜆)
̂𝜎2
)
+
(T2 −𝜋∗
1(C, 𝜆)
̂𝜎2
)2
<
(1 −̂𝜌2
1)
n2
𝜒2
1−𝛼(2)
(7.12)
where 𝜒2
1−𝛼(2) is the 100 × (1 −𝛼) th percentile of the chi-squared distribution
with 2 degrees of freedom.
Statistical inference for (C, 𝜆) may be made by means of the conﬁdence region
given in (7.12).
7.4
Remarks
Deﬁnition 7.1 may be extended to more general tail models as described in (7.2).
Under (7.2), for every r > 0, let every pk in 𝜋r−1 be replaced by pk = p(k; 𝜽) and
denote the resulting expression as 𝜋∗
r−1, that is,
𝜋∗
r−1 =
∑
k≥1
p(k; 𝜽)1[Yk = r −1].
Then
𝜋r−1 −𝜋∗
r−1 =
k0−1
∑
k=1
1[Yk = r −1](pk −p(k; 𝜽)),
and the sum is of ﬁnite terms. Since, for every k, both E(1[Yk = r −1])
and Var(1[Yk = r −1]) converge to zero exponentially in sample size n,
E(g(n, 𝛿)1[Yk = r −1]) and Var(g(n, 𝛿)1[Yk = r −1]) converge to zero, which
implies that g(n, 𝛿)1[Yk = r −1]
p
−−−→0, which in turn implies that
g(n, 𝛿)(𝜋∗
r−1 −𝜋r−1)
p
−−−→0

254
Statistical Implications of Turing’s Formula
for every r. If the tail model in (7.2) supports
n ̂Σ−1∕2
(T1 −𝜋∗
0
̂𝜎1
,…,
TR −𝜋∗
R−1
̂𝜎R
)𝜏
L
−−−→MVN (0, IR×R)
(7.13)
where ̂Σ−1∕2 is as in Lemma 7.2, then the estimator of 𝜽given by the following
Deﬁnition may be studied fruitfully, as suggested by ( ̂C, ̂𝜆) of (C, 𝜆) in the Pareto
tail model in (7.1).
Deﬁnition 7.2
If there exists a vector value, 𝜽= ̂𝜽, which maximizes the like-
lihood given by (7.13) within the space allowed by the tail model in (7.2), then ̂𝜽
is said to be an AMLE of 𝜽.
The theoretical issues along this path include establishing (1) weak con-
vergence of the type similar to (7.13), (2) consistency, uniqueness, and other
desirable properties of the estimator ̂𝜽, none of which is easily accomplished.
However, if this path could be followed through, Turing’s perspective may lend
itself to support broader and ﬁner tail probability models.
Perhaps more interestingly, the methodology discussed in this chapter may
be extended to tail distributions beyond those with Pareto decay. Four cases
are oﬀered below for consideration.
Case 1. Suppose, instead of (7.1), the underlying tail model is
pk = Ck−𝜆(1 + o(1))
(7.14)
for all k, k ≥k0, where k0 ≥1 is a positive integer, C > 0, and 𝜆> 1 are
unknown parameters, and o(1) →0 as k →∞. It can be shown that
all the results established under the exact Pareto tail model (7.1) still
hold. As a result, the AMLE, ( ̂C, ̂𝜆), may be deﬁned as in Deﬁnition
7.1 and enjoys the same statistical properties, such as uniqueness and
consistency. The proofs unfortunately become doubly tedious and are
not presented in this book.
Case 2. One of the most popular thick tail models in the literature of extreme
value theory is a cumulative distribution function of an underlying ran-
dom variable X satisfying
1 −FX(x) = P(X > x) = Cx−𝜆
(7.15)
for all x ≥x0 where x0, C > 0, and 𝜆> 0 are unknown parameters.
Let (7.15) be referred to as the continuous Pareto tail model. The
estimation of the parameters in the tail model of (7.15) may also
be approached by the methodology of this chapter. Without loss of
generality, suppose the support of X is (0, ∞). Consider a 𝛿-binning,
where 𝛿> 0 is an arbitrary but ﬁxed positive real number, of the
support of X, that is,
(0, 𝛿], (𝛿, 2𝛿],…, ((k −1)𝛿, k𝛿],….

Estimation of Tail Probability
255
The probability of the kth bin is
pk = FX(k𝛿) −FX((k −1)𝛿)
= C {[(k −1)𝛿]−𝜆−(k𝛿)−𝜆}
= C
𝛿𝜆
[
1
(k −1)𝜆−1
k𝜆
]
= C
𝛿𝜆
1
k𝜆
[(
k
k −1
)𝜆
−1
]
= C
𝛿𝜆
1
k𝜆+1
[(
k
k −1
)𝜆
k −k
]
.
However, since
lim
k→∞
[(
k
k −1
)𝜆
k −k
]
= 𝜆,
(see Exercise 10)
pk = C𝜆
𝛿𝜆
1
k𝜆+1 (1 −o(1))
(7.16)
which has the form of (7.14). Consequently, by the argument in Case 1,
the AMLE of (C, 𝜆) of the continuous Pareto model in (7.15) may also
be obtained.
Case 3. Suppose the cumulative distribution function of an underlying random
variable X satisfying
1 −FX(x) = P(X > x) = Ce−𝜆x
(7.17)
for all x ≥x0 where x0, C > 0, and 𝜆> 0 are unknown parameters.
Let (7.17) be referred to as the continuous exponential tail model. The
estimation of the parameters in the tail model of (7.17) may also be
approached by the methodology of this chapter. Consider ﬁrst a trans-
formation of X,
Y = eX,
which entails
1 −FY(y) = Cy−𝜆
for suﬃciently large y, that is, y ≥ex0. By the argument of Case 2, the
AMLE of (C, 𝜆) of the continuous exponential model in (7.17) may also
be obtained.
Case 4. Suppose the cumulative distribution function of an underlying random
variable X satisfying
1 −FX(x) = P(X > x) = Ce−𝜆x2
(7.18)

256
Statistical Implications of Turing’s Formula
for all x ≥x0 where x0 ≥0, C > 0, and 𝜆> 0 are unknown parameters.
Let (7.18) be referred to as the continuous Gaussian tail model. The
estimation of the parameters in the tail model of (7.18) may also be
approached by the methodology of this chapter. Consider ﬁrst a trans-
formation of X,
Y = eX2,
which entails
1 −FY(y) = Cy−𝜆
for suﬃciently large y, that is, y ≥ex2
0. By the argument of Case 2, the
AMLE of (C, 𝜆) of the continuous Gaussian tail model in (7.18) may
also be obtained.
Cases 1–4 suggest that the methodology discussed in this chapter may be
extended to models beyond the family of distributions with power decay tails.
In fact, the methodology may be extended to tackle various tail models for con-
tinuous random variables, where the choice of 𝛿, possibly of the form 𝛿= 𝛿(n),
in practice may prove to be an interesting issue of investigation, among many
other meaningful research undertakings along this thread.
7.5
Appendix
7.5.1
Proof of Lemma 7.7
Proof: For Part 1, it suﬃces to show
n1−1∕𝜆E(un) →[1∕(𝜆C1−1∕𝜆)]Γ(1 −1∕𝜆), and
n2−2∕𝜆Var(un) →0.
By the argument in the proof of Lemma 6.3,
n1−1∕𝜆E(un) ∼n1−1∕𝜆∑
k−𝜆e−nCk−𝜆;
and invoking the Euler–Maclaurin lemma,
lim n1−1∕𝜆E(un) = lim n1−1∕𝜆
∫
∞
x=1
x−𝜆e−nCx−𝜆dx
=
1
𝜆C1−1∕𝜆lim ∫
nC
x=0
y−1∕𝜆e−ydy
=
1
𝜆C1−1∕𝜆Γ(1 −1∕𝜆).

Estimation of Tail Probability
257
Var(un) = E(u2
n) −[E(un)]2
=
[
∑
k≥1
(1 −Ck−𝜆)nk−2𝜆+
∑
k≠j
(1 −Ck−𝜆−Cj−𝜆)nk−𝜆j−𝜆
]
−
[
∑
k≥1
(1 −Ck−𝜆)2nk−2𝜆+
∑
k≠j
(1 −Ck−𝜆)n(1 −Cj−𝜆)nk−𝜆j−𝜆
]
≤
[
∑
k≥1
(1 −Ck−𝜆)nk−2𝜆+
∑
k≠j
(1 −Ck−𝜆)n(1 −Cj−𝜆)nk−𝜆j−𝜆
]
−
[
∑
k≥1
(1 −Ck−𝜆)2nk−2𝜆+
∑
k≠j
(1 −Ck−𝜆)n(1 −Cj−𝜆)nk−𝜆j−𝜆
]
=
∑
k≥1
(1 −Ck−𝜆)nk−2𝜆−
∑
k≥1
(1 −Ck−𝜆)2nk−2𝜆
≤
∑
k≥1
(1 −Ck−𝜆)nk−2𝜆.
Invoking the Euler–Maclaurin lemma whenever needed,
lim
n→∞n2−2∕𝜆Var(un) ≤lim n2−2∕𝜆∑
k≥1
(1 −Ck−𝜆)nk−2𝜆
= lim
n→∞n2−2∕𝜆
∫
∞
1
x−2𝜆e−Cnx−𝜆dx
= lim
n→∞
n−1∕𝜆
𝜆C2−1∕𝜆Γ(2 −1∕𝜆) = 0.
A similar argument establishes Part 2.
For Part 3, since Ωn converges to a positive deﬁnite matrix in probability, it
suﬃces to show that P(n1−1∕𝜆vn > 0) →1, which is clear by noting
n1−1∕𝜆vn = n1−1∕𝜆∑
k≥2
1[Yk = 0](ln k)k−𝜆≥n1−1∕𝜆ln 2
∑
k≥2
1[Yk = 0]k−𝜆
= −n1−1∕𝜆(ln 2)1[Y1 = 0] + n1−1∕𝜆ln 2
∑
1[Yk = 0]k−𝜆
= op(1) + n1−1∕𝜆(ln 2)un
p
−−−→
ln 2
𝜆C1−1∕𝜆Γ(1 −1∕𝜆) > 0.
For Part 4, ﬁrst consider the fact that a positive integer k0 exists such that
kε > ln k for all k ≥k0.
vn =
∑
k<k0
1[Yk = 0](ln k)k−𝜆+
∑
k≥k0
1[Yk = 0](ln k)k−𝜆
≤ln k0
∑
k<k0
1[Yk = 0] +
∑
k≥k0
1[Yk = 0]k−𝜆+ε.

258
Statistical Implications of Turing’s Formula
E
(
ln k0
∑
k<k0
1[Yk = 0]
)
= ln k0
∑
k<k0
(1 −Ck−𝜆)n ≤k0 ln k0
(1 −Ck−𝜆
0
)n.
lim
n→∞n1−1∕𝜆−εE
(
ln(k0)
∑
k<k0
1[xk = 0]
)
= 0.
Invoking the Euler–Maclaurin lemma whenever needed,
lim
n→∞E
(
∑
k≥k0
1[Yk = 0]k−𝜆+ε
)
= lim
n→∞∫
∞
1
x−𝜆+εe−nCx−𝜆dx
= 1
𝜆C−[1−(ε+1)∕𝜆)] lim
n→∞n−[1−(ε+1)∕𝜆]
∫
nC
0
y[1−(ε+1)∕𝜆)]−1e−ydy.
lim
n→∞n1−1∕𝜆−εE(vn)
= 1
𝜆C−[1−(ε+1)∕𝜆)] lim
n→∞n−(1−1∕𝜆)ε
∫
nC
0
y[1−(ε+1)∕𝜆)]−1e−ydy →0.
Var(vn) = E(v2
n) −[E(vn)]2
= E
[
∑
k≥1
1[Yk = 0](ln k)2k−2𝜆
+
∑
k≠j
1[Yk = 0, Yj = 0](ln k)(ln j)(kj)−𝜆
]
−
[
∑
k≥1
(1 −Ck−𝜆)2n(ln k)2k−2𝜆
+
∑
k≠j
(1 −Ck−𝜆)n(1 −Cj−𝜆)n(ln k)(ln j)(kj)−𝜆
]
=
[
∑
k≥1
(1 −Ck−𝜆)n(ln k)2k−2𝜆
+
∑
k≠j
(1 −Ck−𝜆−Cj−𝜆)n(ln k)(ln j)(kj)−𝜆
]
−
[
∑
k≥1
(1 −Ck−𝜆)2n(ln k)2k−2𝜆
+
∑
k≠j
(1 −Ck−𝜆)n(1 −Cj−𝜆)n ln(k) ln(j)(kj)−𝜆
]

Estimation of Tail Probability
259
≤
∑
k≥1
(1 −Ck−𝜆)n(ln k)2k−2𝜆
=
∑
k<k0
(1 −Ck−𝜆)n(ln k)2k−2𝜆
+
∑
k≥k0
(1 −Ck−𝜆)n(ln k)2k−2𝜆
≤(ln k0)2k0(1 −Ck−𝜆
0 )n +
∑
k≥1
(1 −Ck−𝜆)nk−2𝜆+2ε.
lim
n→∞n2−2∕𝜆−2ε(ln k0)2k0(1 −Ck−𝜆
0 )n →0.
Invoking the Euler–Maclaurin lemma,
lim
n→∞n2−2∕𝜆−2ε ∑
k≥1
(1 −Ck−𝜆)nk−2𝜆+2ε
= lim
n→∞n2−2∕𝜆−2ε
∫
∞
1
x−2(𝜆−ε)e−nCx−𝜆dx
= lim
n→∞n2−2∕𝜆−2ε
[(nC)−2+(2ε+1)∕𝜆
𝜆
∫
nC
0
y[2−(2ε+1)∕𝜆]−1e−ydy
]
= C−2+(2ε+1)∕𝜆
𝜆
lim
n→∞n−2+(2ε+1)∕𝜆n2−2∕𝜆−2ε
×
[
∫
nC
0
y[2−(2ε+1)∕𝜆]−1e−ydy
]
= C−2+(2ε+1)∕𝜆
𝜆
lim
n→∞n−1∕𝜆−2ε(1−1∕𝜆)
[
∫
nC
0
y[2−(2ε+1)∕𝜆]−1e−ydy
]
→0.
Therefore Var(n1−1∕𝜆−εvn) →0, and hence n1−1∕𝜆−εvn
p→0.
A similar argument establishes each of Parts 5–7.
For Part 8, it is to show that n2−2∕𝜆(vnUn −unVn) converges to a positive con-
stant in probability. By the results of Parts 1 and 2 already established, it is
equivalent to show that
n1−1∕𝜆
(Γ(2 −1∕𝜆)
𝜆C1−1∕𝜆
vn+1 −Γ(1 −1∕𝜆)
𝜆C1−1∕𝜆
Vn+1
)
= n1−1∕𝜆Γ(1 −1∕𝜆)
𝜆C1−1∕𝜆
[(1 −1∕𝜆)vn+1 −Vn+1]
converges to a positive constant in probability, or that
n1−1∕𝜆[(1 −1∕𝜆)vn+1 −Vn+1]
does.
Since
n1−1∕𝜆[(1 −1∕𝜆)vn+1 −Vn+1]
= n1−1∕𝜆[(1 −1∕𝜆)vn −Vn+1] −n1−1∕𝜆(1 −1∕𝜆)(vn −vn+1),

260
Statistical Implications of Turing’s Formula
it is ﬁrst to show n1−1∕𝜆(vn −vn+1)
p→0.
E(vn −vn+1) =
∑
k≥1
(1 −Ck−𝜆)n(ln k)k−𝜆−
∑
k≥1
(1 −Ck−𝜆)n+1(ln k)k−𝜆
= C
∑
k≥1
(1 −Ck−𝜆)n(ln k)k−2𝜆
≤C(ln k0)k0(1 −Ck−𝜆
0 )n + C
∑
k≥1
(1 −Ck−𝜆)nk−2𝜆+ε.
lim
n→∞n1−1∕𝜆C ln(k0)k0(1 −Ck−𝜆
0 )n = 0.
Let ε = 𝜆∕2. Invoking the Euler–Maclaurin lemma,
lim
n→∞n1−1∕𝜆C
∑
k≥1
(1 −Ck−𝜆)nk−2𝜆+ε
= lim
n→∞n1−1∕𝜆C ∫
∞
1
x−2𝜆+εe−nCx−𝜆dx
= C−1+(ε+1)∕𝜆
𝜆
lim
n→∞n−1+ε∕𝜆
∫
nC
0
y[2−(ε+1)−1]e−ydy
= 0.
Therefore, n1−1∕𝜆E(vn −vn+1) →0. Next, it is to show
Var (n1−1∕𝜆(vn −vn+1)) →0.
However, since
Var(vn −vn+1) ≤2(Var(vn) + Var(vn+1)),
it suﬃces to show
Var (n1−1∕𝜆vn
) →0.
In the proof of above-mentioned Part 4, it has already been shown that
Var(vn) ≤
∑
k≥1
(1 −Ck−𝜆)nk−2𝜆+2ε.
Therefore, letting ε = 1∕4 and invoking the Euler–Maclaurin lemma,
lim
n→∞n2−2∕𝜆Var(vn) ≤lim
n→∞n2−2∕𝜆∑
k≥1
(1 −Ck−𝜆)nk−2𝜆+2ε
= lim
n→∞n2−2∕𝜆
∫
∞
1
x−2𝜆+2εe−nCx−𝜆dx
= C−2+(2ε+1)∕𝜆
𝜆
lim
n→∞n−(1−2ε)∕𝜆
∫
nC
0
y[2−(2ε+1)∕𝜆]−1e−ydy
= 0.

Estimation of Tail Probability
261
Since n1−1∕𝜆(vn −vn+1)
p→0, it only remains to show that
n1−1∕𝜆[(1 −1∕𝜆)vn −Vn+1]
converges to a positive constant in probability.
E ((1 −1∕𝜆)vn −Vn+1
) = (1 −1∕𝜆)
∑
k≥1
(1 −Ck−𝜆)n(ln k)k−𝜆
−
∑
k≥1
n(1 −Ck−𝜆)nCk−𝜆(ln k)k−𝜆
=
∑
k≥1
(1 −Ck𝜆)n(ln k)[(1 −1∕𝜆)k−𝜆−nCk−2𝜆].
Letting y = nCx−𝜆and invoking the Euler–Maclaurin lemma, after a few
algebraic steps,
lim
n→∞n1−1∕𝜆E((1 −1∕𝜆)vn −Vn+1)
= lim
n→∞n1−1∕𝜆∑
k≥1
{(1 −Ck−𝜆)n(ln k) [(1 −1∕𝜆)k−𝜆−nCk−2𝜆]}
= lim
n→∞n1−1∕𝜆
∫
∞
1
(ln x) [(1 −1∕𝜆)x−𝜆−nCx−2𝜆] e−nCx−𝜆dx
=
1
𝜆2C1−1∕𝜆lim
n→∞∫
nC
0
ln(nC)
[(
1 −1
𝜆
)
y−1
𝜆−y1−1
𝜆
]
e−ydy
−
1
𝜆2C1−1∕𝜆lim
n→∞∫
nC
0
(ln y)
[(
1 −1
𝜆
)
y−1
𝜆−y1−1
𝜆
]
e−ydy.
The ﬁrst term of the last expression is zero. This is so because ﬁrst,
lim
n→∞∫
nC
0
ln(nC)
[(
1 −1
𝜆
)
y−1
𝜆−y1−1
𝜆
]
e−ydy
= lim
n→∞∫
∞
0
ln(nC)
[(
1 −1
𝜆
)
y−1
𝜆−y1−1
𝜆
]
e−ydy
−lim
n→∞∫
∞
nC
ln(nC)
[(
1 −1
𝜆
)
y−1
𝜆−y1−1
𝜆
]
e−ydy
= lim
n→∞∫
∞
nC
ln(nC)
[
y1−1
𝜆−
(
1 −1
𝜆
)
y−1
𝜆
]
e−ydy,
and second, (1 −1∕𝜆)y−1∕𝜆−y1−1∕𝜆> 0 for suﬃciently large y, and third, (1 −
1∕𝜆)y−1∕𝜆−y1−1∕𝜆e−y∕2 is decreasing for suﬃciently large y, and consequently,
lim
n→∞∫
nC
0
ln(nC)
[(
1 −1
𝜆
)
y−1
𝜆−y1−1
𝜆
]
e−ydy
≤lim
n→∞ln(nC)
[
(nC)1−1
𝜆−
(
1 −1
𝜆
)
(nC)−1
𝜆
]
e−(nC)∕2
∫
∞
nC
e−y∕2dy
≤2 lim
n→∞ln(nC)
[
(nC)1−1
𝜆−
(
1 −1
𝜆
)
(nC)−1
𝜆
]
e−(nC)∕2 = 0.

262
Statistical Implications of Turing’s Formula
This leads to
lim
n→∞n1−1∕𝜆E((1 −1∕𝜆)vn −Vn+1)
=
1
𝜆2C1−1∕𝜆∫
∞
0
(ln y)
[
y1−1
𝜆−
(
1 −1
𝜆
)
y−1
𝜆
]
e−ydy.
Denoting the integrand above as
K(y) = (ln y)[y1−1∕𝜆−(1 −1∕𝜆)y−1∕𝜆]e−y,
it is easy to see that K(y) is positive on (0, 1 −1∕𝜆), negative on (1 −1∕𝜆, 1), and
positive on (1, ∞).
∫
1−1∕𝜆
0
K(y)dy ≥e−(1−1∕𝜆)
∫
1−1∕𝜆
0
(ln y)
[
y1−1
𝜆−
(
1 −1
𝜆
)
y−1
𝜆
]
dy
= e−(1−1∕𝜆)
∫
∞
−ln(1−1∕𝜆)
t
[(
1 −1
𝜆
)
e−(1−1∕𝜆)t −e−(2−1∕𝜆)t]
dt,
∫
1
1−1∕𝜆
K(y)dy ≥e−(1−1∕𝜆)
∫
1
1−1∕𝜆
[
y1−1∕𝜆−
(
1 −1
𝜆
)
y−1∕𝜆]
ln(y)dy
= e−(1−1∕𝜆)
∫
−ln(1−1∕𝜆)
0
t
[(
1 −1
𝜆
)
e−(1−1∕𝜆)t −e−(2−1∕𝜆)t]
dt,
∫
∞
0
K(y)dy > ∫
1−1∕𝜆
0
K(y)dy + ∫
1
1−1∕𝜆
K(y)dy
≥e−(1−1∕𝜆)
∫
∞
0
t
[(
1 −1
𝜆
)
e−(1−1∕𝜆)t −e−(2−1∕𝜆)t]
dt
= e−(1−1∕𝜆)[1∕(1 −1∕𝜆) −1∕(2 −1∕𝜆)2] > 0,
that is,
n1−1∕𝜆E((1 −1∕𝜆)vn −Vn+1) →(𝜆2C1−1∕𝜆)−1
∫
∞
0
K(y)dy > 0.
It still remains to show that
Var(n1−1∕𝜆[(1 −1∕𝜆)vn −Vn+1]) →0.
However, since it has been shown above that
Var(n1−1∕𝜆vn) →0,
it suﬃces to show that
Var(n1−1∕𝜆Vn+1) →0.

Estimation of Tail Probability
263
Var(Vn+1) = E(V 2
n+1) −[E(Vn+1)]2
=
[
∑
k≥1
(n + 1)Ck−𝜆(1 −Ck−𝜆)n(ln k)2k−2𝜆
+
∑
k≠j
(n + 1)nCk−𝜆Cj−𝜆(1 −Ck−𝜆−Cj−𝜆)n−1(ln k)(ln j)k−𝜆j−𝜆
]
−
[
∑
k≥1
(n + 1)2C2k−2𝜆(1 −Ck−𝜆)2n(ln k)2k−2𝜆
+
∑
k≠j
(n + 1)2Ck−𝜆Cj−𝜆(1 −Ck−𝜆)n(1 −Cj−𝜆)n
× (ln k)(ln j)k−𝜆j−𝜆
]
≤(n + 1)C
∑
k≥1
(1 −Ck−𝜆)n(ln k)2k−3𝜆
= (n + 1)C
∑
k<k0
(1 −Ck−𝜆)n(ln k)2k−3𝜆
+ (n + 1)C
∑
k≥k0
(1 −Ck−𝜆)n(ln k)2k−3𝜆
≤(n + 1)C
∑
k<k0
(1 −Ck−𝜆)n(ln k)2k−3𝜆
+ (n + 1)C
∑
k≥1
(1 −Ck−𝜆)nk−3𝜆+2ε.
The ﬁrst term in the last expression converges to zero exponentially, and
therefore for ε = 1∕4, invoking the Euler–Maclaurin lemma,
lim
n→∞Var(n1−1∕𝜆Vn+1) ≤C lim
n→∞n2−2∕𝜆(n + 1) ∫
∞
1
x−3𝜆+2εe−nCx−𝜆dx
= C−2+(2ε+1)∕𝜆
𝜆
lim
n→∞n2−2∕𝜆(n + 1)n−3+(2ε+1)∕𝜆
× ∫
nC
0
y[3−(2ε+1)∕𝜆]−1e−ydy = 0.
7.5.2
Proof of Lemma 7.9
Proof of Part 1: For Part (a), ﬁrst consider the function
fn(x) = n1−1∕𝜆0(1 −C0x−𝜆0)nCx−𝜆
deﬁned for x > x0 where x0 is suﬃciently large a constant to ensure that
1 −C0x−𝜆0 ≥0. It can be easily veriﬁed that fn(x) is decreasing for all

264
Statistical Implications of Turing’s Formula
x ≥x(n) = [C0(n𝜆0 + 𝜆)∕𝜆]1∕𝜆0 and is increasing for x ∈[x0, x(n)]. For any 𝜆
satisfying 𝜆> 𝜆0 −1,
fn(x(n)) = CC−𝜆∕𝜆0
0
(
1 −
𝜆
n𝜆0 + 𝜆
)n
(1 + n𝜆0
𝜆)−𝜆∕𝜆0n1−1∕𝜆0
= 𝒪(n[(𝜆0−1)−𝜆]∕𝜆0) →0.
Invoking the Euler–Maclaurin lemma,
lim
n→∞n1−1∕𝜆0 ∑
k>x0
(1 −C0k−𝜆0)nCk−𝜆
= lim
n→∞n1−1∕𝜆0
∫
∞
x0
(1 −C0x−𝜆0)nCx−𝜆dx
= C lim
n→∞n1−1∕𝜆0
∫
∞
x0
e−nC0x−𝜆0 x−𝜆dx
=
CC(1−𝜆)∕𝜆0
0
𝜆0
lim
n→∞n(𝜆0−𝜆)∕𝜆0
∫
nC0x
−𝜆0
0
0
y(𝜆−1)∕𝜆0−1e−ydy.
The last expression is a positive constant if and only if 𝜆= 𝜆0 and is zero if and
only if 𝜆> 𝜆0. In fact, when 𝜆= 𝜆0 and C ≠C0, the limit is CC(1−𝜆0)∕𝜆0
0
𝜆−1
0 Γ(1 −
1∕𝜆0); when 𝜆= 𝜆0 and C = C0, the limit is C1∕𝜆0
0
𝜆−1
0 Γ(1 −1∕𝜆0). This leads to
n1−1∕𝜆0E(𝜋∗
0(C, 𝜆)) →0.
Next, it is to show n2−2∕𝜆0Var(𝜋∗
0(C, 𝜆)) →0. It can be veriﬁed that
Var(𝜋∗
0(C, 𝜆)) ≤
∑
k≥1
(1 −C0k−𝜆0)nC2k−2𝜆.
Let
fn(x) = n2−2∕𝜆0(1 −C0x−𝜆0)2C2k−2𝜆
be deﬁned for x > x0 where x0 is suﬃciently large a constant to ensure
that 1 −C0x−𝜆0 ≥0. It can be veriﬁed that fn(x) is decreasing for all
x ≥x(n) = [C0(n𝜆0 + 2𝜆)∕(2𝜆)]1∕𝜆0 and is increasing for x ∈[x0, x(n)]. For
any 𝜆satisfying 𝜆> 𝜆0 −1,
fn(x(n)) = C2C−2𝜆∕𝜆0
0
(
1 −
2𝜆
n𝜆0 + 2𝜆
)n(
1 + n 𝜆0
2𝜆
)−2𝜆∕𝜆0
n2−2∕𝜆0
= 𝒪
(
n
2
(𝜆0−1)−𝜆
𝜆0
)
→0.

Estimation of Tail Probability
265
Invoking the Euler–Maclaurin lemma,
lim
n→∞n2−2∕𝜆0 ∑
k>x0
(1 −C0k−𝜆0)nC2k−2𝜆
= C2 lim
n→∞n2−2∕𝜆0
∫
∞
x0
(1 −C0x−𝜆0)nx−2𝜆dx
= C2 lim
n→∞n2−2∕𝜆0
∫
∞
x0
e−nC0x−𝜆0x−2𝜆dx
=
C2C(1−2𝜆)∕𝜆0
0
𝜆0
lim
n→∞n[2(𝜆0−𝜆)−1]∕𝜆0
∫
nC0x
−𝜆0
0
0
y(2𝜆−1)∕𝜆0−1e−ydy.
The last limit is zero if and only if 𝜆> 𝜆0 −1∕2. Part (a) immediately follows.
For Part (b), ﬁrst consider the function
fn(x) = n1−1∕𝜆0(n + 1)(1 −C0x−𝜆0)nC0x−𝜆0Cx−𝜆
deﬁned for x > x0. It can be easily veriﬁed that fn(x) is decreasing for all x ≥x(n)
and is increasing for x ∈[x0, x(n)] where
x(n) = {C0[(n + 1)𝜆0 + 𝜆]∕(𝜆0 + 𝜆)}1∕𝜆0.
For any 𝜆> 𝜆0 −1,
fn(x(n)) = CC0(n + 1)n
1−1
𝜆0
[
1 −
𝜆0 + 𝜆
(n + 1)𝜆0 + 𝜆
]n
×
{
𝜆0 + 𝜆
C0[(n + 1)𝜆0 + 𝜆]
}1+ 𝜆
𝜆0
= 𝒪
(
n
1−1+𝜆
𝜆0
)
→0.
Invoking the Euler–Maclaurin lemma, for 𝜆> 𝜆0,
lim
n→∞n1−1∕𝜆0(n + 1)
∑
k>x0
(1 −C0k−𝜆0)nC0k−𝜆0Ck−𝜆
= CC0 lim
n→∞n1−1∕𝜆0(n + 1) ∫
∞
x0
(1 −C0x−𝜆0)nx−(𝜆0+𝜆)dx
=
CC1−𝜆∕𝜆0
0
𝜆0
lim
n→∞n
−𝜆
𝜆0 (n + 1) ∫
nC0x
−𝜆0
0
0
y(1+𝜆∕𝜆0−1∕𝜆0)−1e−ydy
= 𝒪
(
n
𝜆0−𝜆
𝜆0
)
→0.
At this point, it has been shown that n1−1∕𝜆0E(𝜋∗
1(C, 𝜆)) →0. Next, it is to
show
n2−2∕𝜆0Var(𝜋∗
1(C, 𝜆)) →0.

266
Statistical Implications of Turing’s Formula
Let
fn(x) = n2−2∕𝜆0C2C0(n + 1)(1 −C0x−𝜆0)nx−(2𝜆+𝜆0)
deﬁned for x > x0. It can be easily veriﬁed that fn(x) is decreasing for all
x ≥x(n) and is increasing for x ∈[x0, x(n)] where x(n) = {C0[(n + 1)𝜆0 + 2𝜆]∕
(𝜆0 + 2𝜆)}1∕𝜆0. For any 𝜆> 𝜆0 −1,
fn(x(n)) = C2C0(n + 1)n
2−2
𝜆0
[
1 −
𝜆0 + 2𝜆
(n + 1)𝜆0 + 2𝜆
]n
×
{C0[(n + 1)𝜆0 + 2𝜆]
𝜆0 + 2𝜆
}−
2𝜆+𝜆0
𝜆0
= 𝒪(n2[1−(1+𝜆)∕𝜆0]) →0.
Invoking the Euler–Maclaurin lemma, for 𝜆> 𝜆0 −1∕2,
lim
n→∞n2−2∕𝜆0(n + 1)C2C0
∑
k>x0
(1 −C0k−𝜆0)nk−(2𝜆+𝜆0)
= C2C0 lim
n→∞n2−2∕𝜆0(n + 1) ∫
∞
x0
(1 −C0x−𝜆0)nx−(𝜆0+2𝜆)dx
=
C2C(1−2𝜆)∕𝜆0
0
𝜆0
lim
n→∞n
1−1+2𝜆
𝜆0 (n + 1) ∫
nC0x
−𝜆0
0
0
y
2𝜆+𝜆0−1
𝜆0
−1e−ydy
= 𝒪
(
n
2−1+2𝜆
𝜆0
)
→0.
Part (b) follows.
◽
Proof of Part 2: In the proof of Lemma 7.9 Part 1, at each of the four times fn(x)
was deﬁned and evaluated at its corresponding x(n), lim fn(x) = 0 not only for
𝜆> 𝜆0 but also 𝜆> 𝜆0 −1∕(2𝜆0). This implies that the Euler–Maclaurin lemma
applies in each of the four cases when (C, 𝜆) ∈Θ2. In particular, it is easily ver-
iﬁed, ﬁrst
n1−1∕𝜆0−1∕(2𝜆0)E(𝜋∗
0(C, 𝜆)) →0
and
n2−2∕𝜆0−2∕(2𝜆0)Var(𝜋∗
0(C, 𝜆)) →0,
which implies
n1−1∕𝜆0−1∕(2𝜆0)𝜋∗
0(C, 𝜆)
p
−−−→0,
second
n1−1∕𝜆0−1∕(2𝜆0)E(𝜋∗
1(C, 𝜆)) →0
and
n2−2∕𝜆0−2∕(2𝜆0)Var(𝜋∗
1(C, 𝜆)) →0,

Estimation of Tail Probability
267
which implies
n1−1∕𝜆0−1∕(2𝜆0)𝜋∗
1(C, 𝜆)
p
−−−→0.
◽
Proof of Part 3: First consider the case of (C, 𝜆) ∈Θ2. Since the Euler–Maclaurin
lemma applies in all four cases of fn, it is easily veriﬁed that
n1−1∕𝜆0E(𝜋∗
0(C, 𝜆)) →+∞
and
n2−2∕𝜆0Var(𝜋∗
0(C, 𝜆)) →0,
which in turn implies that
n1−1∕𝜆0𝜋∗
0(C, 𝜆)
p
−−−→+∞.
Similarly,
n1−1∕𝜆0𝜋∗
1(C, 𝜆)
p
−−−→+∞.
Second consider the case of (C, 𝜆) ∈Θ3. For every (C, 𝜆) ∈Θ3, there exists a
point (C, 𝜆1) ∈Θ2, where 𝜆< 𝜆1, such that
𝜋∗
0(C, 𝜆) =
∑
k≥1
1[xk = 0]Ck−𝜆>
∑
k≥1
1[xk = 0]Ck−𝜆1 = 𝜋∗
0(C, 𝜆1).
Since 𝜋∗
0(C, 𝜆1)
p→+∞, 𝜋∗
0(C, 𝜆)
p→+∞. Similarly 𝜋∗
1(C, 𝜆)
p→+∞.
◽
7.6
Exercises
1
Assuming that the probability distribution {pk; k ≥1} on an countably
inﬁnite alphabet 𝒳= {𝓁k; k ≥1} satisﬁes that pk > 0 for every k, k ≥1,
show that
lim
n→∞P(𝕂tail ⊂{k; k ≥k0}) = 1
where 𝕂tail is as in (7.3).
2
Provide details of the proof for Lemma 7.3.
3
In Corollary 7.2, verify that ̂Σ−1∕2
2
has the form as given in (7.8).
4
Suppose the Pareto (power decay) tail model of (7.1) is true. Let
𝛿= (4𝜆0)−1, that is,
g(n, 𝛿) = n1−1∕(2𝜆0)

268
Statistical Implications of Turing’s Formula
where 𝜆0 is the true value of the parameter 𝜆. Let Ωn be deﬁned by
g2(n, 𝛿)Ωn = ̂Σ−1
2
where ̂Σ−1
2 is as in (7.8). Show that
Ωn
p
−−−→Ω
where Ω is some positive deﬁnite 2 × 2 matrix.
5
Let ln(C, 𝜆) be as in (7.9), show that
a)
∂2ln
∂C2 = 2n2−2∕𝜆0(un, Un)Ωn(un, Un)𝜏,
b)
∂2𝓁n
∂𝜆2 = −2n2−2∕𝜆0C(wn, Wn)Ωn(T1 −𝜋∗
0, T2 −𝜋∗
1)𝜏
+ 2n2−2∕𝜆0C2(vn, Vn)Ωn(vn, Vn)𝜏,
c)
∂2𝓁n
∂𝜆∂C = 2n2−2∕𝜆0(vn, Vn)Ωn(T1 −𝜋∗
0, T2 −𝜋∗
1)𝜏
−2n2−2∕𝜆0C(vn, Vn)Ωn(un, Un)𝜏.
6
Give a proof of Part 2 of Lemma 7.7.
7
Give a proof of Part 5 of Lemma 7.7.
8
Give a proof of Part 6 of Lemma 7.7.
9
Give a proof of Part 7 of Lemma 7.7.
10
Show that
lim
k→∞
[(
k
k −1
)𝜆
k −k
]
= 𝜆
where 𝜆> 0 is a constant.

269
References
Antos, A. and Kontoyiannis, I. (2001). Convergence properties of functional
estimates for discrete distributions. Random Structures & Algorithms, 19,
163–193.
Beirlant, J., Dudewicz, E.J., Györﬁ, L., and Meulen, E.C. (2001). Nonparametric
entropy estimation: an overview. International Journal of the Mathematical
Statistics Sciences, 6, 17–39.
Billingsley, P. (1995). Probability and Measure, John Wiley & Sons, Inc., New York.
Blyth, C.R. (1959). Note on estimating information. Annals of Mathematical
Statistics, 30, 71–79.
Bunge, J., Willis, A., and Walsh, F. (2014). Estimating the number of species in
microbial diversity studies. Annual Review of Statistics and its Application, 1,
427–445.
Chao, A. (1987). Estimating the population size for capture-recapture data with
unequal catchability. Biometrics, 43, 783–791.
Chao, A. and Jost, L. (2012). Coverage-based rarefaction and extrapolation:
standardizing samples by completeness rather than size. Ecology, 93,
2533–2547.
Chao, A. and Jost, L. (2015). Estimating diversity and entropy proﬁles via
discovery rates of new species. Methods in Ecology and Evolution, 6, 873–882.
Chao, A. and Shen, T.J. (2003). Nonparametric estimation of Shannon’s index of
diversity when there are unseen species. Environmental and Ecological
Statistics, 10, 429–443.
Chao, A., Chiu, C.-H., and Jost, L. (2010). Phylogenetic diversity measures based
on Hill numbers. Philosophical Transactions of the Royal Society B: Biological
Sciences, 365, 3599–3609.
Chao, A., Lee, S.-M., and Chen, T.-C. (1988). A generalized Good’s nonparametric
coverage estimator. Chinese Journal of Mathematics, 16(3), 189–199.
Cochran, W.G. (1952). The 𝜒2 test of goodness of ﬁt. Annals of Mathematical
Statistics, 25, 315–345.
Cover, T.M. and Thomas, J.A. (2006). Elements of Information Theory, 2nd ed.
John Wiley & Sons, Inc., Hoboken, NJ.
Statistical Implications of Turing’s Formula, First Edition. Zhiyi Zhang.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

270
References
David, F.N. (1950). Two combinatorial tests of whether a sample has come from a
given population. Biometrika, 37, 97–110.
de Haan, L. and Ferreira, A. (2006). Extreme Value Theory: An Introduction,
Springer Science+Business Media, LLC, New York.
Emlen, J.M. (1973). Ecology: An Evolutionary Approach, Addison-Wesley
Publishing Co., Reading, MA.
Esty, W. (1983). A normal limit law for a nonparametric estimator of the coverage
of a random sample. The Annals of Statistics, 11(3), 905–912.
Fisher, R.A. and Tippett, L.H.C. (1922). On the interpretation of chi-square from
contingency tables, and the calculation of P. Journal of the Royal Statistical
Society, 85, 87–94.
Fisher, R.A. and Tippett, L.H.C. (1928). Limiting forms of the
frequency-distribution of the largest or smallest member of a sample.
Mathematical Proceedings of the Cambridge Philosophical Society, 24, 180.
Fisher, R.A., Corbet, A.S., and Williams, C.B. (1943). The relationship between the
number of species and the number of individuals in a random sample of an
animal population. Journal of Animal Ecology, 12(1), 42–58.
Fréchet, M. (1927). Sur la loi de probabilité de l’écart maximum. Annales de la
Société Polonaise de Mathématique, 6, 92.
Gini, C. (1912). Variabilità e mutabilità. Reprinted in Memorie di metodologica
statistica (eds E. Pizetti and T. Salvemini), Libreria Eredi Virgilio Veschi, Rome
(1955).
Gnedenko, B.V. (1943). Sur la distribution limite du terme maximum d’une série
aléatoire. Annals of Mathematics, 44, 423–453.
Gnedenko, B.V. (1948). On a local limit theorem of the theory of probability.
Uspekhi Matematicheskikh Nauk, 3(25), 187–194.
Good, I.J. (1953). The population frequencies of species and the estimation of
population parameters. Biometrika, 40(3-4), 237–264.
Grabchak, M., Marcon, E., Lang, G., and Zhang, Z. (2016). The generalized
Simpson’s entropy is a measure of biodiversity <hal-01276738>.
Haeusler, E. and Teugels, J.L. (1985). On asymptotic normality of Hill’s estimator
for the exponent of regular variation. The Annals of Statistics, 13(2), 743–756.
Hall, P. and Weissman, I. (1997). On the estimation of extreme tail probabilities.
The Annals of Statistics, 25(3), 1311–1326.
Hall, P. and Welsh, A.H. (1985). Adaptive estimates of parameters of regular
variation. The Annals of Statistics, 13(1), 331–341.
Harris, B. (1975). The statistical estimation of entropy in the non-parametric case.
In Topics in Information Theory (ed. I. Csiszar), North-Holland, Amsterdam,
323–355.
Hausser, J. and Strimmer, K. (2009). Entropy inference and the James-Stein
estimator, with application to nonlinear gene association networks. Journal of
Machine Learning Research, 10, 1469–1484.
Heip, C.H.R., Herman, P.M.J., and Soetaert, K. (1998). Indices of diversity and
evenness. Oc eanis, 24(4), 61–87.

References
271
Hill, M.O. (1973). Diversity and evenness: a unifying notation and its
consequences. Ecology, 54, 427–431.
Hill, B.M. (1975). A simple general approach to inference about the tail of a
distribution. The Annals of Statistics, 3(5), 1163–1174.
Hoeﬀding, W. (1948). A class of statistics with asymptotically normal
distributions. The Annals of Statistics, 19(3), 293–325.
Hoeﬀding, W. (1963). Probability Inequalities for Sums of Bounded Random
Variables. Journal of the American Statistical Association, 58, 13–30.
Holste, D., Große, I., and Herzel, H. (1998). Bayes’ estimators of generalized
entropies. Journal of Physics A: Mathematical and General, 31, 2551–2566.
Johnson, N.L. and Kotz, S. (1977). URN Models and their Applications, John Wiley
& Sons, Inc., New York.
Jost, L. (2006). Entropy and diversity. Oikos, 113, 363–375.
Kolchin, V.F., Sevastyanov, B.A., and Chistyakov, V.P. (1978). Random Allocations,
V.H. Winston & Sons, Washington, DC.
Krebs, C.J. (1999). Ecological Methodology, 2nd ed. Addison-Welsey Educational
Publishers, Menlo Park, CA.
Krichevsky, R.E. and Troﬁmov, V.K. (1981). The performance of universal
encoding. IEEE Transactions on Information Theory, 27, 199–207.
Kullback, S. and Leibler, R.A. (1951). On information and suﬃciency. The Annals
of Mathematical Statistics, 22(1), 79–86.
Kvalseth, T.O. (1987). Entropy and correlation: some comments. IEEE
Transactions on Systems, Man, and Cybernetics, 17(3), 517–519.
Lee, A.J. (1990). U-Statistics: Theory and Practice, Marcel Dekker, New York.
MacArthur, R.H. (1955). Fluctuations of animal populations, and a measure of
community stability. Ecology, 36, 533–536.
Magurran, A.E. (2004). Measuring Biological Diversity, Blackwell Publishing, Ltd.,
Malden, MA.
Mann, H.B. and Wald, A. (1943). On stochastic limit and order relationships. The
Annals of Mathematical Statistics, 14(3), 217–226.
Marcon, E. (2014). Mesures de la Biodiversité. Technical Report, Ecologie des
forêts de Guyane.
Margalef, R. (1958). Temporal succession and spatial heterogeneity in
phytoplankton. In Perspectives in Marine biology (ed. A.A. Buzzati-Traverso),
University of California Press, Berkeley, CA, 323–347.
Miller, G.A. (1955). Note on the bias of information estimates. Information Theory
in Psychology; Problems and Methods, II-B, 95–100.
Miller, G.A. and Madow, W.G. (1954). On the maximum likelihood estimate of
the Shannon-Weaver measure of information. Air Force Cambridge Research
Center Technical Report AFCRC-TR-54-75, Operational Applications
Laboratory, Air Force, Cambridge Research Center, Air Research and
Development Command, New York.

272
References
Montgomery-Smith, S. and Schümann, T. (2007). Unbiased Estimators for Entropy
and Class Number. Unpublished preprint 2007, Department of Mathematics,
University of Missouri, Columbia, MO.
Mood, A.M., Graybill, F.A., and Boes, D.C. (1974). Introduction to the Theory of
Statistics, McGraw-Hill, Inc., New York.
Nemenman, I., Shafee, F., and Bialek, W. (2002). Entropy and inference, revisited.
Advances in Neural Information Processing Systems, vol. 14, MIT Press,
Cambridge, MA, 471–478.
Ohannessian, M.I. and Dahleh, M.A. (2012). Rare probability estimation under
regularly varying heavy tails. Journal of Machine Learning Research,
Proceedings Track, 23, 21.1–21.24.
Paninski, L. (2003). Estimation of entropy and mutual information. Neural
Computation, 15, 1191–1253.
Patil, G.P. and Taillie, C. (1982). Diversity as a concept and its measurement.
Journal of the American Statistical Association, 77, 548–567.
Pearson, K. (1900). On a criterion that a given system of deviations from the
probable in the case of a correlated system of variables is such that it can be
reasonably supposed to have arisen from random sampling. Philosophical
Magazine Series 5, 50, 157–175. (Reprinted 1948 in Karl Pearson’s Early
Statistical Papers, (ed. E.S. Pearson), Cambridge University Press, Cambridge.)
Pearson, K. (1922). On the 𝜒2 test of goodness of ﬁt. Biometrika, 14, 186–191.
Peet, R.K. (1974). The measurements of species diversity. Annual Review of
Ecology and Systematics, 5, 285–307.
Pickands, J. (1975). Statistical inference using extreme order statistics. The Annals
of Statistics, 3(1), 119–131.
Purvis, A. and Hector, A. (2000). Getting the measure of biodiversity. Nature, 405,
212–219.
Rényi, A. (1961). On measures of entropy and information. Proceedings of the 4th
Berkeley Symposium on Mathematical Statistics and Probabilities, Vol. 1,
547–561.
Robbins, H.E. (1968). Estimating the total probability of the unobserved outcomes
of an experiment. The Annals of Mathematical Statistics, 39(1), 256–257.
Schürmann, T. and Grassberger, P. (1996). Entropy estimation of symbol
sequences. Chaos, 6, 414–427.
Serﬂing, R.J. (1980). Approximation Theorems of Mathematical Statistics, John
Wiley & Sons, Inc., New York.
Shannon, C.E. (1948). A mathematical theory of communication. The Bell System
Technical Journal, 27, 379–423 & 623–656.
Simpson, E.H. (1949). Measurement of diversity. Nature, 163, 688.
Smith, R.L. (1987). Estimating tails of probability distributions. The Annals of
Statistics, 15(3), 1174–1207.
Stanley, R.P. (1997). Enumerative Combinatorics, Vol. 1, Cambridge University
Press, New York.

References
273
Strehl, A. and Ghosh, J. (2002). Cluster ensembles - a knowledge reuse framework
for combining multiple partitions. Journal of Machine Learning Research, 3,
583–617.
Strong, S.P., Koberle, R., de Ruyter van Steveninck, R.R., and Bialek, W. (1998).
Entropy and information in neural spike trains. Physical Review Letters, 80,
197–200.
Taillie, C. (1979). Species equitability: a comparative approach. In —it Ecological
Diversity in Theory and Practice (eds J.F. Grassle, G.P. Patil, W. Smith and C.
Taillie), International Cooperative Publishing House, Fairland, MD, 51–61.
Tanabe, K. and Sagae, M. (1992). An exact Cholesky decomposition and the
generalized inverse of the variance-covariance matrix of the multinomial
distribution, with applications. Journal of the Royal Statistical Society, Series B
Methodological, 54(1), 211–219.
Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of
Statistical Physics, 52, 479–487.
Valiant, G. and Valiant, P. (2011). Estimating the Unseen: an n∕log (n)-sample
estimator for entropy and support size, shown optimal via new CLTs. In
Proceedings of the 43rd Annual ACM Symposium on Theory of Computing,
STOC’11 Symposium on Theory of Computing (Co-located with FCRC 2011),
San Jose, CA, USA - June 06-08, 2011 (ed. L. Fortnow and S.P. Vadhan),
Association for Computing Machinery, New York, 685–694.
Vinh, N.X., Epps, J., and Bailey, J. (2010). Information theoretic measures for
clusterings comparison: variants, properties, normalization and correction for
chance. Journal of Machine Learning Research, 11, 2837–2854.
von Mises, R. (1936). La distribution de la plus grande de n valeurs. Reprinted
(1954) in Selected Papers 11271-294, American Mathematical Society,
Providence, RI.
Vu, V.Q., Yu, B., and Kass, R.E. (2007). Coverage-adjusted entropy estimation.
Statistical Analysis of Neuronal Data, 26(21), 4039–4060.
Wang, S.C. and Dodson, P. (2006). Estimating the diversity of dinosaurs.
Proceedings of the National Academy of Sciences of the United States of America,
103(37), 13601–13605.
Yao, Y.Y. (2003). Information-theoretic measures for knowledge discovery and
data mining. In: Entropy Measures, Maximum Entropy Principle and Emerging
Applications (ed. Karmeshu), 1st edition, Springer, Berlin, 115–136.
Zahl, S. (1977). Jackkniﬁng an index of diversity. Ecology, 58, 907–913.
Zhang, Z. (2012). Entropy estimation in Turing’s perspective. Neural
Computation, 24(5), 1368–1389.
Zhang, Z. (2013a). A multivariate normal law for Turing’s formulae. Sankhy¯a: The
Indian Journal of Statistics, 75-A(1), 51–73.
Zhang, Z. (2013b). Asymptotic normality of an entropy estimator with
exponentially decaying bias. IEEE Transactions on Information Theory, 59(1),
504–508.

274
References
Zhang, Z. (2017). Domains of attraction on countable alphabets. Bernoulli Journal
(to appear).
Zhang, Z. and Grabchak, M. (2013). Bias adjustment for a nonparametric entropy
estimator. Entropy, 15(6), 1999–2011.
Zhang, Z. and Grabchak, M. (2014). Nonparametric estimation of
Kullback-Leibler divergence. Neural Computation, 26(11), 2570–2593.
Zhang, Z. and Grabchak, M. (2016). Entropic representation and estimation of
diversity indices. Journal of Nonparametric Statistics, DOI:
10.1080/10485252.2016.1190357.
Zhang, Z. and Huang, H. (2008). A suﬃcient normality condition for Turing’s
formula. Journal of Nonparametric Statistics, 20(5), 431–446.
Zhang, Z. and Stewart, A.M. (2016). Estimation of standardized mutual
information. Technical Report No. 7, University of North Carolina at Charlotte.
Zhang, C.-H. and Zhang, Z. (2009). Asymptotic normality of a nonparametric
estimator of sample coverage. The Annals of Statistics, 37(5A), 2582–2595.
Zhang, Z. and Zhang, X. (2012). A normal law for the plug-in estimator of
entropy. IEEE Transactions on Information Theory, 58(5), 2745–2747.
Zhang, Z. and Zheng, L. (2015). A mutual information estimator with
exponentially decaying bias. Statistical Applications in Genetics and Molecular
Biology, 14(3), 243–252.
Zhang, Z. and Zhou, J. (2010). Re-parameterization of multinomial distribution
and diversity indices. Journal of Statistical Planning and Inference, 140(7),
1731–1738.
Zubkov, A.M. (1973). Limit distributions for a statistical estimate of the entropy.
Teoriya Veroyatnostei i Ee Primeneniya, 18(3), 643–650.

275
Author Index
a
Antos, A.
83, 96
b
Bailey, J.
174
Beirlant, J.
71
Bialek, W.
74, 75
Billingsley, P.
00
Blyth, C.R.
87, 88
Boes, D.C.
198
Bunge, J.
73
c
Chao, A.
27, 73, 75, 132, 144
Chen, T.-C.
27
Chistyakov, V.P.
33
Chiu, C.-H.
144
Cochran, W.G.
199
Corbet, A.S.
142
Cover, T.M.
162
d
Dahleh, M.A.
21
David, F.N.
42
de Haan, L.
209
de Ruyter van Steveninck, R.R.
74
Dodson, P.
61
Dudewicz, E.J.
71
e
Emlen, J.M.
125
Epps, J.
174
Esty, W.
10
f
Ferreira, A.
209
Fisher, R.A.
142, 198, 209
Fréchet, M.
209
g
Ghosh, J.
174
Gini, C.
49, 125
Gnedenko, B.V.
209
Good, I.J.
3, 10, 22
Grabchak, M.
75, 127, 132, 136, 144,
183
Grassberger, P.
75
Graybill, F.A.
198
Große, I.
75
Györﬁ, L.
71
h
Haeusler, E.
241
Hall, P.
241
Harris, B.
72, 77
Hausser, J.
75
Hector, A.
130, 142
Heip, C.H.R.
130
Herman, P.M.J.
130
Herzel, H.
75
Hill, B.M.
27, 241
Hill, M.O.
125
Hoeﬀding, W.
54, 92, 136
Holste, D.
75
Huang, H.
11, 12
Statistical Implications of Turing’s Formula, First Edition. Zhiyi Zhang.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

276
Author Index
j
Johnson, N.L.
33
Jost, L.
132, 144
k
Kass, R.E.
75
Koberle, R.
74
Kolchin, V.F.
33
Kontoyiannis, I.
83, 96
Kotz, S.
33
Krebs, C.J.
125
Krichevsky, R.E.
75
Kullback, S.
153, 183
Kvalseth, T.O.
173
l
Lang, G.
144
Lee, A.J.
54, 92
Lee, S.-M.
27
Leibler, R.A.
153, 183
m
MacArthur, R. H.
142
Madow, W.G.
77
Magurran, A.E.
125
Mann, H. B.
135
Marcon, E.
125, 144
Margalef, R.
142
Meulen, E.C.
71
Miller, G.A.
73, 77, 187
Montgomery-Smith, S.
75
Mood, A.M.
198
n
Nemenman, I.
75
o
Ohannessian, M.I.
21
p
Paninski, L.
71, 80, 81, 187
Patil, G.P.
63
Pearson, K.
198
Peet, R.K.
130
Pickands, J.
241
Pratt, J.W.
00
Purvis, A.
130, 142
r
Rényi, A.
66, 125, 126
Robbins, H.E.
4
s
Sagae, M.
134
Schürmann, T.
75
Serﬂing, R.J.
54
Sevastyanov, B.A.
33
Shafee, F.
75
Shannon, C.E.
66, 71, 125
Shen, T.J.
75
Simpson, E.H.
49, 66, 125
Smirnov, N.V.
00
Smith, R.L.
241
Soetaert, K.
130
Stanley, R.P.
00
Stewart, A.M.
174
Strehl, A.
174
Strimmer, K.
75
Strong, S.P.
74
t
Taillie, C.
63
Tanabe, K.
134
Teugels, J.L.
241
Thomas, J.A.
162
Tippett, L.H.C.
198, 209
Troﬁmov, V.K.
75
Tsallis, C.
66, 125
v
Valiant, G.
75
Valiant, P.
75
Vinh, N.X.
174
von Mises, R.
209
Vu, V.Q.
75
w
Wald, A.
135

Author Index
277
Walsh, F.
73
Wang, S.C.
61
Wang, Y.T.
00
Weissman, I.
241
Welsh, A.H.
241
Williams, C.B.
142
Willis, A.
73
x
Yao, Y.Y.
174
Yu, B.
75
z
Zahl, S.
74
Zhang, C.-H.
00
Zhang, X.
16, 84, 101, 107, 118
Zhang, Z.
11, 12, 14, 16, 22–24, 49,
75, 84, 89, 92, 95, 101, 107, 118,
127, 132, 136, 144, 174, 182,
183, 228
Zheng, L.
182
Zhou, J.
49, 92, 101, 228
Zubkov, A.M.
80, 81

279
Subject Index
a
alphabet
3
cardinality
5
countable
3
ﬁnite
5, 10
AMLE
26, 246, 254
consistency
248, 254
deﬁnition
246, 254
Hessian matrix
248, 251
statistical properties
248
uniqueness
248, 251
asymptotically eﬃcient
72, 76, 94
asymptotic maximum likelihood
estimator, AMLE
26, 246, 254
axioms of diversity
126, 143, 144
b
Bayesian
73, 74
hierarchical
75
hyper-prior
75
perspective
74
posterior distribution
75
Dirichlet
75
prior distribution
74, 75
Dirichlet
74
bias
4, 5
asymptotically relatively unbiased
estimator
5, 6, 28
relative bias
5
unbiased estimator
4
c
cardinality
5, 10
eﬀective cardinality
5
conﬁdence interval
9, 15, 16, 20, 32
conﬁdence region
243, 253
consistency
24, 31, 57, 169, 175, 183,
243, 248, 254
multiplicative
21
coverage
2
non-coverage
2
of population
2
sample coverage
2
d
diversity
49, 51, 61–64, 66, 126
axioms of
126, 128, 143
diﬀerence proﬁle
64–66
Emlen’s index
125, 127, 130, 134
entropic basis
128, 130, 138
equivalence
127, 129
estimators
125, 129, 131–133, 135,
138, 139, 145
normal laws
134, 135, 138, 139
Gini-Simpson index, the
49, 62, 63,
66–69, 125, 126, 129, 143, 146,
148
Hill’s diversity number
125, 127,
128, 138, 147
linear
128–132, 138, 139
proﬁle
63–66, 142
Statistical Implications of Turing’s Formula, First Edition. Zhiyi Zhang.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

280
Subject Index
diversity (contd.)
Rényi’s entropy
125, 127, 128,
138–142, 145–148
richness index
127, 130, 144, 147
Shannon’s entropy
71, 86, 87, 125,
127, 128, 129, 133, 145, 148
Simpson’s index
49, 58, 63, 125,
126, 129, 146
Tsallis’ entropy
125, 127, 128, 148
uniﬁed perspective
125, 126
domains of attraction
13, 209, 210,
211, 212, 228
deﬁnition
211
Domain
0, 211, 212
Domain
1, 211, 212, 217
Domain
2, 211, 212
Domain T, transient
211, 212
dominance
218, 220, 223, 224, 239
Fréchet
209
Gini-Simpson
212, 214
Gumbel
209, 217
Molchanov
212, 217, 218, 220
Turing-Good
212, 214, 215, 220,
221
Weibull
209
e
entropic basis
128, 130, 138
entropy
71
conditional entropy
150, 151
of conditional probability
distribution
150
expected conditional entropy
150
estimand
1, 4, 5, 44
Euler-Maclaurin lemma, the
13
extreme value
209, 210, 217
on alphabets
210
f
Fubini’s theorem
86
g
generalized Simpson’s index, the
49,
50, 51
Gini-Simpson family, the
212, 214
Goodness-of-ﬁt
33, 34, 36, 40, 42,
197, 198, 206, 207
Good-Turing formula, the
3
h
Hill’s diversity number
125, 127, 128,
138, 147
i
information
149, 150
estimation
149
relative entropy
151, 182
j
Jensen’s inequality
152
k
Kullback-Leibler divergence
151
deﬁnition
151
estimation
182
symmetrized
153, 184, 193
m
Molchanov family, the
212, 217, 218,
220
mutual information
153, 154, 157
deﬁnition
153
estimation
162
Kullback-Leibler divergence
182
standardized
159, 173
n
normal laws
15
of augmented Turing’s formula
28,
32
of diversity estimators
134, 135,
138, 139
of estimators of entropy
77, 78, 80,
81, 84, 89, 91, 92, 99, 101
of estimators of generalized
Simpson’s index
55, 57, 58,
61
of Turing’s formula
10, 13, 22–25

Subject Index
281
o
occupancy problems
33
oscillating tail indices
218
p
Pareto tail, the
26, 241, 243, 244, 254
power tail
26, 241
r
Relative bias
5
asymptotically and relatively
unbiased
28
asymptotically relatively unbiased
estimator
5
asymptotically unbiased
estimator
5
Robbins’ claim
4
Rényi’s entropy
125, 127, 128,
138–142, 145–148
Rényi’s equivalent
127, 128, 138, 139
s
sample
identically and independently
distributed (iid)
3
of size n
3
Shannon’s entropy
71, 86
convergence
81
estimators
Bayesian
73, 74
Dirichlet posterior
75
Dirichlet prior
74
hierarchical
75
hyper-parameter,
75
hyper-prior
75
prior distribution
74, 75
Jackknife
73, 74
Miller-Madow
73
normal laws
77, 78, 80, 81, 84,
89, 91, 92, 99, 101
plug-in
72, 76
in Turing’s perspective
86
the slow convergence theorem
83
Simpson’s indices
49
estimators
52–54
normal laws
54, 55, 57, 58
examples
dinosaur diversity
61–63
Philip Sidney
64
William Shakespeare
64
generalized, the
49–51, 128, 130,
144, 147
singletons
6
Stirling number of the second kind, the
36
t
tail
67, 68, 241–246, 248, 249,
251–256
distant
201
geometric progression
218
index
211, 212
of index set
243
no
210
on alphabets
210, 243
power
243,
thicker and thinner
249, 252, 253,
263, 265, 267, 268
tail probability
241
continuous exponential
255
continuous Gaussian
256
continuous Pareto
254
parametric tail model
242
Pareto tail, the
26, 241, 243, 244,
254
power tail
26, 241
Turing-Good family, the
212, 214,
215, 220
Turing’s formula
1, 3, 6, 10, 22, 27, 61,
110, 145, 287, 289
asymptotic normality
10–12,
16–19, 21, 23–25, 32
augmented
27
deﬁnition
3
Good-Turing formula, the
3
multivariate
24, 25
univariate
10–12, 16–19, 21, 23,
32

282
Subject Index
Turing’s perspective, xiii
50, 52, 68,
72, 86, 87, 88, 96, 131 135, 162,
170, 174, 176, 181, 183, 189,
243, 254
additive
19
multiplicative
19
u
uniform distribution
34, 35, 37,
39–42, 47
U-statistics
52, 54, 66, 91, 92, 132, 190
degree of kernel
92
kernel
54, 92
v
weighted
linear form of diversity
129
linear form of entropy
86
Simpson’s index
50
window on tail
243


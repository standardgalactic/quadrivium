Statistical Implications of Turingâ€™s Formula

Statistical Implications of Turingâ€™s Formula
Zhiyi Zhang
Department of Mathematics and Statistics, University of North Carolina,
Charlotte, NC, US

Copyright Â© 2017 by John Wiley & Sons, Inc. All rights reserved
Published by John Wiley & Sons, Inc., Hoboken, New Jersey
Published simultaneously in Canada
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any
form or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise,
except as permitted under Section 107 or 108 of the 1976 United States Copyright Act, without
either the prior written permission of the Publisher, or authorization through payment of the
appropriate per-copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers,
MA 01923, (978) 750-8400, fax (978) 750-4470, or on the web at www.copyright.com. Requests to
the Publisher for permission should be addressed to the Permissions Department, John Wiley &
Sons, Inc., 111 River Street, Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008, or online at
http://www.wiley.com/go/permissions.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best
eï¬€orts in preparing this book, they make no representations or warranties with respect to the
accuracy or completeness of the contents of this book and speciï¬cally disclaim any implied
warranties of merchantability or ï¬tness for a particular purpose. No warranty may be created or
extended by sales representatives or written sales materials. The advice and strategies contained
herein may not be suitable for your situation. You should consult with a professional where
appropriate. Neither the publisher nor author shall be liable for any loss of proï¬t or any other
commercial damages, including but not limited to special, incidental, consequential, or other
damages.
For general information on our other products and services or for technical support, please
contact our Customer Care Department within the United States at (800) 762-2974, outside the
United States at (317) 572-3993 or fax (317) 572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in
print may not be available in electronic formats. For more information about Wiley products,
visit our web site at www.wiley.com.
Library of Congress Cataloging-in-Publication Data:
Names: Zhang, Zhiyi, 1960-
Title: Statistical implications of Turingâ€™s formula / Zhiyi Zhang.
Description: Hoboken, New Jersey : John Wiley & Sons, Inc., [2017] | Includes
bibliographical references and index.
Identiï¬ers: LCCN 2016027830| ISBN 9781119237068 (cloth) | ISBN 9781119237099
(epub)
Subjects: LCSH: Mathematical statisticsâ€“Textbooks. |
Probabilitiesâ€“Textbooks.
Classiï¬cation: LCC QA273 .Z43 2017 | DDC 519.5â€“dc23 LC record available at
https://lccn.loc.gov/2016027830
Cover image courtesy Author Zhiyi Zhang
Set in 10/12pt, WarnockPro by SPi Global, Chennai, India.
Printed in the United States of America
10
9
8
7
6
5
4
3
2
1

To my family and all my teachers

vii
Contents
Preface
xi
1
Turingâ€™s Formula
1
1.1
Turingâ€™s Formula
3
1.2
Univariate Normal Laws
10
1.3
Multivariate Normal Laws
22
1.4
Turingâ€™s Formula Augmented
27
1.5
Goodness-of-Fit by Counting Zeros
33
1.6
Remarks
42
1.7
Exercises
45
2
Estimation of Simpsonâ€™s Indices
49
2.1
Generalized Simpsonâ€™s Indices
49
2.2
Estimation of Simpsonâ€™s Indices
52
2.3
Normal Laws
54
2.4
Illustrative Examples
61
2.5
Remarks
66
2.6
Exercises
68
3
Estimation of Shannonâ€™s Entropy
71
3.1
A Brief Overview
72
3.2
The Plug-In Entropy Estimator
76
3.2.1
When K Is Finite
76
3.2.2
When K Is Countably Inï¬nite
81
3.3
Entropy Estimator in Turingâ€™s Perspective
86
3.3.1
When K Is Finite
88
3.3.2
When K Is Countably Inï¬nite
94
3.4
Appendix
107
3.4.1
Proof of Lemma 3.2
107
3.4.2
Proof of Lemma 3.5
110

viii
Contents
3.4.3
Proof of Corollary 3.5
111
3.4.4
Proof of Lemma 3.14
112
3.4.5
Proof of Lemma 3.18
116
3.5
Remarks
120
3.6
Exercises
121
4
Estimation of Diversity Indices
125
4.1
A Uniï¬ed Perspective on Diversity Indices
126
4.2
Estimation of Linear Diversity Indices
131
4.3
Estimation of RÃ©nyiâ€™s Entropy
138
4.4
Remarks
142
4.5
Exercises
145
5
Estimation of Information
149
5.1
Introduction
149
5.2
Estimation of Mutual Information
162
5.2.1
The Plug-In Estimator
163
5.2.2
Estimation in Turingâ€™s Perspective
170
5.2.3
Estimation of Standardized Mutual Information
173
5.2.4
An Illustrative Example
176
5.3
Estimation of Kullbackâ€“Leibler Divergence
182
5.3.1
The Plug-In Estimator
184
5.3.2
Properties of the Augmented Plug-In Estimator
186
5.3.3
Estimation in Turingâ€™s Perspective
189
5.3.4
Symmetrized Kullbackâ€“Leibler Divergence
193
5.4
Tests of Hypotheses
196
5.5
Appendix
199
5.5.1
Proof of Theorem 5.12
199
5.6
Exercises
204
6
Domains of Attraction on Countable Alphabets
209
6.1
Introduction
209
6.2
Domains of Attraction
212
6.3
Examples and Remarks
223
6.4
Appendix
228
6.4.1
Proof of Lemma 6.3
228
6.4.2
Proof of Theorem 6.2
229
6.4.3
Proof of Lemma 6.6
232
6.5
Exercises
236
7
Estimation of Tail Probability
241
7.1
Introduction
241
7.2
Estimation of Pareto Tail
244
7.3
Statistical Properties of AMLE
248

Contents
ix
7.4
Remarks
253
7.5
Appendix
256
7.5.1
Proof of Lemma 7.7
256
7.5.2
Proof of Lemma 7.9
263
7.6
Exercises
267
References
269
Author Index
275
Subject Index
279

xi
Preface
This book introduces readers to Turingâ€™s formula and then re-examines several
core statistical issues of modern data science from Turingâ€™s perspective.
Turingâ€™s formula was a remarkable invention of Alan Turing during World
War II in an early attempt to decode the German enigmas. The formula
looks at the world of randomness through a unique and powerful binary
perspective â€“ unmistakably of Turing. However, Turingâ€™s formula was not
well understood for many years. Research amassed during the last decade has
brought to light profound and new statistical implications of the formula that
were previously not known. Recently, and only recently, a relatively clear and
systematic description of Turingâ€™s formula, with its statistical properties and
implications, has become possible. Hence this book.
Turingâ€™s formula is often perceived as having a mystical quality. I was
awestruck when I ï¬rst learned of the formula 10 years ago. Its anti-intuitive
implication was simply beyond my immediate grasp. However, I was not along
in this regard. After turning it over in my mind for a while, I mentioned to
two of my colleagues, both seasoned mathematicians, that there might be a
way to give a nonparametric characterization to tail probability of a random
variable beyond data range. To that, their immediate reaction was, â€œtell us
more when you have ï¬gured it out.â€ Some years later, a former doctoral student
of mine said to me, â€œI used to refuse to think about anti-intuitive mathematical
statements, but after Turingâ€™s formula, I would think about a statement at least
twice however anti-intuitive it may sound.â€ Still another colleague of mine
recently said to me, â€œI read everything you wrote on the subject, including
details of the proofs. But I still cannot see intuitively why the formula works.â€
To that, I responded with the following two points:
1) Our intuition is a bounded mental box within which we conduct intellectual
exercises with relative ease and comfort, but we must admit that this box
also reï¬‚ects the limitations of our experience, knowledge, and ability to
reason.
2) If a fact known to be true does not ï¬t into oneâ€™s current box of intuition, is it
not time to expand the boundary of the box to accommodate the true fact?

xii
Preface
My personal journey in learning about Turingâ€™s formula has proved to be a
rewarding one. The experience of observing Turingâ€™s formula totally outside of
my box of intuition initially and then having it gradually snuggled well within
the boundary of my new box of intuition is one I wish to share.
Turingâ€™s formula itself, while extraordinary in many ways, is not the only
reason for this book. Statistical science, since R.A. Fisher, has come a long way
and continues to evolve. In fact, the frontier of Statistics has largely moved
on to the realm of nonparametrics. The last few decades have witnessed great
advances in the theory and practice of nonparametric statistics. However in
this realm, a seemingly impenetrable wall exists: how could one possibly make
inference about the tail of a distribution beyond data range? In front of this
wall, many, if not most, are discouraged by their intuition from exploring
further. Yet it is often said in Statistics that â€œit is all in the tail.â€ Statistics needs
a trail to get to the other side of the wall. Turingâ€™s formula blazes a trail, and
this book attempts to mark that trail.
Turingâ€™s formula is relevant to many key issues in modern data sciences, for
example, Big Data. Big Data, though as of yet not a ï¬eld of study with a clearly
deï¬ned boundary, unambiguously points to a data space that is a quantum
leap away from what is imaginable in the realm of classical statistics in terms of
data volume, data structure, and data complexity. Big Data, however deï¬ned,
issues fundamental challenges to Statistics. To begin, the task of retrieving and
analyzing data in a vastly complex data space must be in large part delegated
to a machine (or software), hence the term Machine Learning. How does a
machine learn and make judgment? At the very core, it all boils down to a
general measure of association between two observable random elements (not
necessarily random variables). At least two fundamental issues immediately
present themselves:
1) High Dimensionality. The complexity of the data space suggests that
a data observation can only be appropriately registered in a very
high-dimensional space, so much so that the dimensionality could be
essentially inï¬nite. Quickly, the usual statistical methodologies run into
fundamental conceptual problems.
2) Discrete and Non-ordinal Nature. The generality of the data space suggests
that possible data values may not have a natural order among themselves:
diï¬€erent gene types in the human genome, diï¬€erent words in text, and
diï¬€erent species in an ecological population are all examples of general data
spaces without a natural â€œneighborhoodâ€ concept.
Such issues would force a fundamental transition from the platform of random
variables (on the real line) to the platform of random elements (on a general
set or an alphabet). On such an alphabet, many familiar and fundamental
concepts of Statistics and Probability no longer exist, for example, moments,
correlation, tail, and so on. It would seem that Statistics is in need of a rebirth
to tackle these issues.

Preface
xiii
The rebirth has been taking place in Information Theory. Its founding
father, Claude Shannon, deï¬ned two conceptual building blocks: entropy (in
place of moments) and mutual information (in place of correlation) in his
landmark paper (Shannon, 1948). Just as important as estimating moments and
coeï¬ƒcient of correlation for random variables, entropy and mutual information
must be estimated for random elements in practice. However, estimation of
entropy and estimation of mutual information are technically diï¬ƒcult problems
due to the curse of â€œHigh Dimensionalityâ€ and â€œDiscrete and Non-ordinal
Nature.â€ For about 50 years since (Shannon, 1948), advances in this arena have
been slow to come. In recent years however, research interest, propelled by
the rapidly increasing level of data complexity, has been reinvigorated and,
at the same time, has been splintered into many diï¬€erent perspectives. One
in particular is Turingâ€™s perspective, which has brought about signiï¬cant and
qualitative improvement to these diï¬ƒcult problems. This book presents an
overview of the key results and updates the frontier in this research space.
The powerful utility of Turingâ€™s perspective can also be seen in many other
areas. One increasingly important modern concept is Diversity. The topics of
what it is and how to estimate it are rapidly moving into rigorous mathematical
treatment. Scientists have passionately argued about them for years but largely
without consensus. Turingâ€™s perspective gives some very interesting answers
to these questions. This book gives a uniï¬ed discussion of diversity indices,
hence making good reading for those who are interested in diversity indices
and their estimation. The ï¬nal two chapters of the book speak to the issues
of tail classiï¬cation and, if classiï¬ed, how to perform a reï¬ned analysis for a
parametric tail model via Turingâ€™s perspective. These issues are scientiï¬cally
relevant in many ï¬elds of study.
I intend this book to serve two groups of readers:
1) Textbook for graduate students. The material is suitable for a topic course
at the graduate level for students in Mathematics, Probability, Statistics,
Computer Science (Artiï¬cial Intelligence, Machine Learning, Big Data),
and Information Theory.
2) Reference book for researchers and practitioners. This book oï¬€ers an
informative presentation of many of the critical statistical issues of
modern data science and with updated new results. Both researchers and
practitioners will ï¬nd this book a good learning resource and enjoy the
many relevant methodologies and formulas given and explained under one
cover.
For a better ï¬‚ow of the presentation, some of the lengthy but instructive proofs
are placed at the end of each chapter.
The seven chapters of this book may be naturally organized into three
groups. Group 1 includes Chapters 1 and 2. Chapter 1 gives an introduction to
Turingâ€™s formula; and Chapter 2 translates Turingâ€™s formula into a particular
perspective (referred to as Turingâ€™s perspective) as embodied in a class of

xiv
Preface
indices (referred to as Generalized Simpsonâ€™s Indices). Group 1 may be
considered as the theoretical foundation of the whole book. Group 2 includes
Chapters 3â€“5. Chapter 3 takes Turingâ€™s perspective into entropy estimation,
Chapter 4 takes it into diversity estimation, and Chapter 5 takes it into
estimation of various information indices. Group 2 may be thought of as
consisting of applications of Turingâ€™s perspective. Chapters 6 and 7 make
up Group 3. Chapter 6 discusses the notion of tail on alphabets and oï¬€ers a
classiï¬cation of probability distributions. Chapter 7 oï¬€ers an application of
Turingâ€™s formula in estimating parametric tails of random variables. Group 3
may be considered as a pathway to further research.
The material in this book is relatively new. In writing the book, I have made
an eï¬€ort to let the book, as well as its chapters, be self-contained. On the one
hand, I wanted the material of the book to ï¬‚ow in a linearly coherent manner
for students learning it for the ï¬rst time. In this regard, readers may experience
a certain degree of repetitiveness in notation deï¬nitions, lemmas, and even
proofs across chapters. On the other hand, I wanted the book to go beyond
merely stating established results and referring to proofs published elsewhere.
Many of the mathematical results in the book have instructive value, and their
proofs indicate the depth of the results. For this reason, I have included many
proofs that might be judged overly lengthy and technical in a conventional
textbook, mostly in the appendices.
It is important to note that this book, as the title suggests, is essentially a
monograph on Turingâ€™s formula. It is not meant to be a comprehensive learning
resource on topics such as estimation of entropy, estimation of diversity, or
estimation of information. Consequently, many worthy methodologies in these
topics have not been included. By no means do I suggest that the methodologies
discussed in this book are the only ones with scientiï¬c merit. Far from it, there
are many wonderful ideas proposed in the existing literature but not mentioned
among the pages of this book, and assuredly many more are yet to come.
I wish to extend my heartfelt gratitude to those who have so kindly allowed
me to bend their ears over the years. In particular, I wish to thank Hongwei
Huang, Stas Molchanov, and Michael Grabchak for countless discussions on
Turingâ€™s formula and related topics; my students, Chen Chen, Li Liu, Ann
Stewart, and Jialin Zhang for picking out numerous errors in an earlier draft
of the book; and The University of North Carolina at Charlotte for granting
me a sabbatical leave in Spring 2015, which allowed me to bring this book
to a complete draft. Most importantly, I wish to thank my family, wife Carol,
daughter Katherine, and son Derek, without whose love and unwavering
support this book would not have been possible.
Zhiyi Zhang
May 2016
Charlotte, North Carolina

1
1
Turingâ€™s Formula
Consider the population of all birds in the world along with all its diï¬€erent
species, say {sk; k â‰¥1}, and denote the corresponding proportion distribution
by {pk; k â‰¥1} where pk is the proportion of the kth bird species in the popula-
tion. Suppose a random sample of n = 2000 is to be taken from the population,
and let the bird counts for the diï¬€erent species be denoted by {Yk; k â‰¥1}. If it is
of interest to estimate p1 the proportion of birds of species s1 in the population,
then Ì‚p1 = Y1âˆ•n is an excellent estimator; and similarly so is Ì‚pk = Ykâˆ•n for pk for
every particular k.
To illustrate, consider a hypothetical sample of size n = 2000 with bird counts
given in Table 1.1 or a version rearranged in decreasing order of the observed
frequencies as in Table 1.2.
With this sample, one would likely estimate p1 by Ì‚p1 = 300âˆ•2000 = 0.15 and
p2 by Ì‚p2 = 200âˆ•2000 = 0.10, and so on.
The total number of bird species observed in this sample is 30. Yet it is clear
that the bird population must have more than just 30 diï¬€erent species. A natural
follow-up question would then be as follows:
What is the total population proportion of birds belonging to species other
than those observed in the sample?
The follow-up question implies a statistical problem of estimation with a tar-
get, or estimand, being the collective proportion of birds of the species not
represented in the sample. For convenience, let this target be denoted as ğœ‹0.
It is important to note that ğœ‹0 is a random quantity depending on the sample
and therefore is not an estimand in the usual statistical sense. In the statistics
Statistical Implications of Turingâ€™s Formula, First Edition. Zhiyi Zhang.
Â© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

2
Statistical Implications of Turingâ€™s Formula
Table 1.1 Bird sample
sk
1
2
3
4
5
6
7
8
9
10
yk
300
200
300
200
100
100
100
100
0
100
sk
11
12
13
14
15
16
17
18
19
20
yk
100
80
70
0
30
50
6
1
2
1
sk
21
22
23
24
25
26
27
28
29
30
yk
1
1
0
0
1
1
1
1
1
1
sk
31
32
33
34
35
36
37
38
39
â€¦
yk
50
100
1
1
0
0
0
0
0
â€¦
Table 1.2 Rearranged bird sample
sk
1
3
2
4
5
6
7
8
10
11
yk
300
300
200
200
100
100
100
100
100
100
sk
32
12
13
16
31
15
17
19
18
20
yk
100
80
70
50
50
30
6
2
1
1
sk
21
22
25
26
27
28
29
30
33
34
yk
1
1
1
1
1
1
1
1
1
1
sk
9
14
23
24
35
36
37
38
39
â€¦
yk
0
0
0
0
0
0
0
0
0
â€¦
literature, 1 âˆ’ğœ‹0 is often referred to as the sample coverage of the population,
or in short sample coverage , or just coverage. Naturally ğœ‹0 may be referred to as
the noncoverage.
The noncoverage ğœ‹0 deï¬ned with a random sample of size n is an interesting
quantity. It is sometimes interpreted as the â€œprobabilityâ€ of discovering a new
species, because, in a loose sense, the chance that the â€œnextâ€ bird is of a new, or
previously unobserved, species is ğœ‹0. This interpretation is however somewhat
misleading. The main issue of such an interpretation is the lack of clariï¬cation
of the underlying experiment (and its sample space). Words such as â€œprobabili-
tyâ€ and â€œnextâ€ can only have meaning in a well-speciï¬ed experiment. While it is
quite remarkable that ğœ‹0 could be reasonably and nonparametrically estimated
by Turingâ€™s formula, ğœ‹0 is not a probability associated with the sample space of

Turingâ€™s Formula
3
the experiment where the sample of size n is drawn. Further discussion of this
point is given in Section 1.5.
Turingâ€™s formula, also sometimes known as the Goodâ€“Turing formula, is an
estimator of ğœ‹0 introduced by Good (1953) but largely credited to Alan Turing.
Let N1 denote the number of species each of which is represented by exactly
one observation in a random sample of size n. Turingâ€™s formula is given by T1 =
N1âˆ•n. For the bird example given in Table 1.2, N1 = 12, n = 2000, and therefore
T1 = 0.006 or 0.6%.
1.1
Turingâ€™s Formula
Let ğ’³= {ğ“k; k â‰¥1} be a countable alphabet with letters ğ“1, â€¦ ; and let
p = {pk; k â‰¥1} be a probability distribution on ğ’³. Let Y = {Yk; k â‰¥1} be
the observed frequencies of the letters in an identically and independently
distributed (iid) random sample of size n. For any integer r, 1 â‰¤r â‰¤n, let the
number of letters in the alphabet that are each represented exactly r times in
the sample be denoted by
Nr =
âˆ‘
kâ‰¥1
1[Yk = r]
(1.1)
where 1[â‹…] is the indicator function. Let the total probability associated with
letters that are represented exactly r âˆ’1 times in the sample be denoted by
ğœ‹râˆ’1 =
âˆ‘
kâ‰¥1
pk1[Yk = r âˆ’1].
(1.2)
Of special interest is the case of r = 1 when
N1 =
âˆ‘
kâ‰¥1
1[Yk = 1]
and
ğœ‹0 =
âˆ‘
kâ‰¥1
pk1[Yk = 0]
(1.3)
representing, respectively,
N1: the number of letters of ğ’³that each appears exactly once; and
ğœ‹0: the total probability associated with the unobserved letters of ğ’³in an
iid sample of size n.
The following expression is known as Turingâ€™s formula:
T1 = N1
n .
(1.4)
Turingâ€™s formula has been extensively studied during the many years fol-
lowing its introduction by Good (1953) and has been demonstrated, mostly

4
Statistical Implications of Turingâ€™s Formula
through numerical simulations, to provide a satisfactory estimate of ğœ‹0 for a
wide range of distributions. These studies put forth a remarkable yet puzzling
implication: the total probability on the unobserved subset of ğ’³may be well
estimated nonparametrically. No satisfactory interpretation was given in the
literature until Robbins (1968).
Robbinsâ€™ Claim: Let ğœ‹0 be deï¬ned with an iid random sample of size n as in
(1.3). Let Turingâ€™s formula be deï¬ned with an augmented iid sample of size
n + 1 by adding one new iid observation to the original sample of size n; and let
the resulting estimator be denoted by T+
1 . Then T+
1 is an unbiased estimator of
ğœ‹0, in the sense of E (T+
1 âˆ’ğœ‹0) = 0.
Robbinsâ€™ claim is easily veriï¬ed. Let N+
1 be the number of letters represented
exactly once in the augmented sample of size n + 1, with observed frequencies
{Y +
k ; k â‰¥1}.
E (N+
1 ) =
âˆ‘
kâ‰¥1
E (1[Y +
k = 1]) =
âˆ‘
kâ‰¥1
(n + 1)pk(1 âˆ’pk)n and
E (T+
1 ) = E(N+
1 âˆ•(n + 1)) =
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)n.
On the other hand, with the sample of size n,
E (ğœ‹0) =
âˆ‘
kâ‰¥1
pkE (1[Yk = 0]) =
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)n.
Hence, E (T+
1 âˆ’ğœ‹0) = 0.
Robbinsâ€™ claim provides an intuitive interpretation in the sense that
1) T+
1 is an unbiased and, therefore, a good estimator of ğœ‹0;
2) the diï¬€erence between T1 and T+
1 should be small; and therefore
3) T1 should be a good estimator of ğœ‹0.
However, Robbinsâ€™ claim still leaves much to be desired. Suppose for a moment
that E (T+
1 âˆ’ğœ‹0) = 0 is an acceptable notion of a good estimator for T+
1 , it does
not directly address the performance of T1 as an estimator of ğœ‹0, not even the
bias E (T1 âˆ’ğœ‹0), let alone other important statistical properties of T1 âˆ’ğœ‹0. In
short, the question whether Turingâ€™s formula works at all, or if it does how well,
is not entirely resolved by Robbinsâ€™ claim.
In addition to the unusual characteristic that the estimand ğœ‹0 is a random
variable, it is to be noted that both the estimator T1 and the estimand ğœ‹0 con-
verge to zero in probability (see Exercise 6). Therefore, it is not suï¬ƒciently
adequate to characterize the performance of T1 by the bias E (T1 âˆ’ğœ‹0) alone.
Any reasonable characterization of its performance must take into considera-
tion the vanishing rate of the estimand ğœ‹0.
To put the discussion in perspective, a notion of performance is needed how-
ever minimal it may be. Let X denote a data sample. Let Ì‚ğœƒn(X) be an estimator

Turingâ€™s Formula
5
of a random estimand ğœƒn(X). Let
E ( Ì‚ğœƒn(X) âˆ’ğœƒn(X))
E (ğœƒn(X))
(1.5)
whenever E (ğœƒn(X)) â‰ 0 be referred to as the relative bias of Ì‚ğœƒn(X) estimating
ğœƒn(X).
Deï¬nition 1.1
Ì‚ğœƒn(X) is said to be an asymptotically relatively unbiased esti-
mator of ğœƒn(X) if
E ( Ì‚ğœƒn(X) âˆ’ğœƒn(X))
E (ğœƒn(X))
â†’0
as n â†’âˆ, provided that E (ğœƒn(X)) â‰ 0.
When ğœƒn(X) = ğœƒâ‰ 0 does not depend on sample data X, an asymptotically
relatively unbiased estimator is an asymptotically unbiased estimator in the
usual statistical sense.
Let K = âˆ‘
kâ‰¥11[pk > 0] be referred to as the eï¬€ective cardinality of ğ’³, or just
simply cardinality whenever there is no risk of ambiguity. K < âˆimplies that
there are only ï¬nite letters in ğ’³with positive probabilities.
Example 1.1
For any probability distribution {pk} such that 1 < K < âˆ, Tur-
ingâ€™s formula T1 is not an asymptotically relatively unbiased estimator of ğœ‹0. This
is so because, letting pâˆ§= min{pk > 0; k â‰¥1},
0 < E (T1 âˆ’ğœ‹0)
E (ğœ‹0)
=
âˆ‘
kâ‰¥1 pk(1 âˆ’pk)nâˆ’1 âˆ’âˆ‘
kâ‰¥1 pk(1 âˆ’pk)n
âˆ‘
kâ‰¥1 pk(1 âˆ’pk)n
=
âˆ‘
kâ‰¥1 p2
k(1 âˆ’pk)nâˆ’1
âˆ‘
kâ‰¥1 pk(1 âˆ’pk)n
=
âˆ‘
kâ‰¥1 p2
k[(1 âˆ’pk)âˆ•(1 âˆ’pâˆ§)]nâˆ’1
âˆ‘
kâ‰¥1 pk(1 âˆ’pk)[(1 âˆ’pk)âˆ•(1 âˆ’pâˆ§)]nâˆ’1
â†’
p2
âˆ§
pâˆ§(1 âˆ’pâˆ§) =
pâˆ§
(1 âˆ’pâˆ§) â‰ 0
as n â†’âˆ.
Example 1.1 demonstrates that there exist distributions on ğ’³under which
Turingâ€™s formula is not an asymptotically relatively unbiased estimator. In
retrospect, this is not so surprising. On a ï¬nite alphabet, the probabilities
associated with letters not covered by a large sample, ğœ‹0, should rapidly
approach zero, at least on an intuitive level, as manifested by the fact that

6
Statistical Implications of Turingâ€™s Formula
E (ğœ‹0) = îˆ»((1 âˆ’pâˆ§)n) (see Exercise 7). On the other hand, the number of
singletons, that is, letters that are each observed exactly once in the sample
(âˆ‘
kâ‰¥11[Yk = 1]), should also approach zero rapidly, as manifested by the fact
that E(âˆ‘
kâ‰¥11[Yk = 1)]) = îˆ»(n(1 âˆ’pâˆ§))n. The fact that T1 and ğœ‹0 are vanishing
at the same rate leads to the result of Example 1.1. This example also suggests
that perhaps Turingâ€™s formula may not always be a good estimator of its
target ğœ‹0, at least in its current form. However, Turingâ€™s formula is not an
asymptotically relatively unbiased estimator only when K < âˆ.
Theorem 1.1
Turingâ€™s formula T1 is an asymptotically relatively unbiased
estimator of ğœ‹0 if and only if K = âˆ.
To prove Theorem 1.1, the following three lemmas are needed.
Lemma 1.1
Suppose {pk} is a strictly decreasing inï¬nite probability sequence
with pk > 0 for every k, k â‰¥1. Then, as n â†’âˆ,
rn =
âˆ‘âˆ
k=1 p2
k(1 âˆ’pk)n
âˆ‘âˆ
k=1 pk(1 âˆ’pk)n â†’0.
(1.6)
Proof: Let k0 be any speciï¬c index. rn may be re-expressed as follows.
0 < rn =
âˆ‘âˆ
k=1 p2
k(1 âˆ’pk)n
âˆ‘âˆ
k=1 pk(1 âˆ’pk)n
=
âˆ‘k0âˆ’1
k=1 p2
k(1 âˆ’pk)n + âˆ‘âˆ
k=k0 p2
k(1 âˆ’pk)n
âˆ‘âˆ
k=1 pk(1 âˆ’pk)n
â‰¤
âˆ‘k0âˆ’1
k=1 p2
k(1 âˆ’pk)n + pk0
âˆ‘âˆ
k=k0 pk(1 âˆ’pk)n
âˆ‘âˆ
k=1 pk(1 âˆ’pk)n
=
âˆ‘k0âˆ’1
k=1 p2
k(1 âˆ’pk)n
âˆ‘âˆ
k=1 pk(1 âˆ’pk)n +
pk0
âˆ‘âˆ
k=k0 pk(1 âˆ’pk)n
âˆ‘âˆ
k=1 pk(1 âˆ’pk)n
=âˆ¶rn,1 + rn,2
where the symbol â€œâˆ¶=â€ stands for the deï¬ning equality, for example, â€œa âˆ¶= bâ€
and â€œa =âˆ¶bâ€ represent â€œa is deï¬ned by bâ€ and â€œa deï¬nes bâ€ respectively.
For any given Îµ > 0, let k0 = min{k âˆ¶pk < Îµâˆ•2}. First it is observed that
0 < rn,2 <
pk0
âˆ‘âˆ
k=k0 pk(1 âˆ’pk)n
âˆ‘âˆ
k=k0 pk(1 âˆ’pk)n
= pk0 < Îµâˆ•2.
Next it is observed that, letting qk = 1 âˆ’pk,
0 < rn,1 =
âˆ‘k0âˆ’1
k=1 p2
k(1 âˆ’pk)n
âˆ‘âˆ
k=1 pk(1 âˆ’pk)n <
âˆ‘k0âˆ’1
k=1 p2
k(1 âˆ’pk)n
pk0(1 âˆ’pk0)n
=
âˆ‘k0âˆ’1
k=1 p2
k(qkâˆ•qk0)n
pk0
.

Turingâ€™s Formula
7
In the numerator of the above-mentioned last expression, 0 < qkâˆ•qk0 < 1 for
every k = 1, â€¦ , k0 âˆ’1, therefore (qkâˆ•qk0)n â†’0 as n â†’âˆ. In addition, since
there are ï¬nite terms in the summation, rn,1 â†’0. That is to say that, for any
Îµ > 0 as given earlier, there exists an integer NÎµ such that for every n > NÎµ, rn,1 <
Îµâˆ•2, or rn â‰¤rn,1 + rn,2 < Îµâˆ•2 + Îµâˆ•2 = Îµ.
â—½
Lemma 1.2
Suppose {pk} is a strictly decreasing inï¬nite probability sequence
with pk > 0 for every k, k â‰¥1. Then, as n â†’âˆ,
râˆ—
n =
âˆ‘âˆ
k=1 p2
k(1 âˆ’pk)nâˆ’1
âˆ‘âˆ
k=1 pk(1 âˆ’pk)n
â†’0.
(1.7)
Proof: By Lemma 1.1,
rn = râˆ—
n âˆ’
âˆ‘âˆ
k=1 p3
k(1 âˆ’pk)nâˆ’1
âˆ‘âˆ
k=1 pk(1 âˆ’pk)n
â†’0,
and therefore it suï¬ƒces to show that
âˆ‘âˆ
k=1 p3
k(1 âˆ’pk)nâˆ’1
âˆ‘âˆ
k=1 pk(1 âˆ’pk)n
â†’0.
Noting pâˆ•(1 âˆ’p) is an increasing function for p âˆˆ(0, 1), for each pk
of
any
given
distribution
{pk; k â‰¥1},
pkâˆ•(1 âˆ’pk) â‰¤pâˆ¨âˆ•(1 âˆ’pâˆ¨)
where
pâˆ¨= max{pk; k â‰¥1}. Therefore,
âˆ‘âˆ
k=1 p3
k(1 âˆ’pk)nâˆ’1
âˆ‘âˆ
k=1 pk(1 âˆ’pk)n
=
âˆ‘âˆ
k=1 p2
k(1 âˆ’pk)n (
pk
1âˆ’pk
)
âˆ‘âˆ
k=1 pk(1 âˆ’pk)n
â‰¤
(
pâˆ¨
1 âˆ’pâˆ¨
) âˆ‘âˆ
k=1 p2
k(1 âˆ’pk)n
âˆ‘âˆ
k=1 pk(1 âˆ’pk)n
=
(
pâˆ¨
1 âˆ’pâˆ¨
)
rn â†’0.
â—½
With a little adjustment, the proof for Lemma 1.1 goes through with any inï¬-
nite probability sequence that is not necessarily strictly decreasing.
Lemma 1.3
If {pk} is a probability sequence with positive mass on each letter
in an inï¬nite subset of ğ’³, then rn â†’0 and râˆ—
n â†’0 as n â†’âˆwhere rn and râˆ—
n are
as in (1.6) and (1.7).
The proof of Lemma 1.3 is left as an exercise (see Exercise 8).
Proof of Theorem 1.1: The suï¬ƒciency part of the theorem immediately follows
Lemma 1.3. The necessity part of the theorem follows the fact that if K < âˆ,
then Example 1.1 provides a counterargument.
â—½

8
Statistical Implications of Turingâ€™s Formula
Both random quantities, T1 and ğœ‹0, converge to zero in probability. One
would naturally wonder if a naive estimator, say Ì‚ğœ‹0 = 1âˆ•n, could perform
reasonably well in estimating ğœ‹0. In view of (1.5), the relative bias of Ì‚ğœ‹0 = 1âˆ•n,
when K < âˆas in Example 1.1, is
E ( Ì‚ğœ‹0 âˆ’ğœ‹0)
E (ğœ‹0)
=
1âˆ•n
âˆ‘
kâ‰¥1 p2
k(1 âˆ’pk)nâˆ’1 âˆ’1 >
1âˆ•n
(1 âˆ’pâˆ§)nâˆ’1 âˆ’1 â†’âˆ
indicating Ì‚ğœ‹0 as a very inadequate estimator. In fact, since the decreasing rate
of E(ğœ‹0) â†’0 could vary over a wide range of diï¬€erent distributions on ğ’³, for
any naive estimator of the form Ì‚ğœ‹0 = g(n) â†’0, there exist arbitrarily many dis-
tributions under which the relative bias of such a Ì‚ğœ‹0 diverges to inï¬nity. This
fact will become clearer in Chapter 6.
Consider next the variance of T1 âˆ’ğœ‹0.
ğœ2
n = Var (T1 âˆ’ğœ‹0) = E ((T1 âˆ’ğœ‹0)2) âˆ’(E (T1 âˆ’ğœ‹0) )2
= E (T2
1 âˆ’2T1 ğœ‹0 + ğœ‹2
0) âˆ’
[
âˆ‘
kâ‰¥1
p2
k(1 âˆ’pk)nâˆ’1
]2
= E
(
nâˆ’2 âˆ‘
iâ‰¥1
âˆ‘
jâ‰¥1
1[Yi = 1]1[Yj = 1]
)
âˆ’2E
(
nâˆ’1 âˆ‘
iâ‰¥1
âˆ‘
jâ‰¥1
pj1[Yi = 1]1[Yj = 0]
)
+ E
(
âˆ‘
iâ‰¥1
âˆ‘
jâ‰¥1
pi pj1[Yi = 0]1[Yj = 0]
)
âˆ’
[
âˆ‘
kâ‰¥1
p2
k(1 âˆ’pk)nâˆ’1
]2
= nâˆ’2
(
n
âˆ‘
iâ‰¥1
pi(1 âˆ’pi)nâˆ’1 + n(n âˆ’1)
âˆ‘
iâ‰ j,iâ‰¥1,jâ‰¥1
pi pj(1 âˆ’pi âˆ’pj)nâˆ’2
)
âˆ’2
(
âˆ‘
iâ‰ j,iâ‰¥1,jâ‰¥1
pi pj(1 âˆ’pi âˆ’pj)nâˆ’1
)
+
(
âˆ‘
iâ‰¥1
p2
i (1 âˆ’pi)n +
âˆ‘
iâ‰ j,iâ‰¥1,jâ‰¥1
pi pj(1 âˆ’pi âˆ’pj)n
)
âˆ’
[
âˆ‘
kâ‰¥1
p2
k(1 âˆ’pk)nâˆ’1
]2
= 1
n
(
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)nâˆ’1 âˆ’
âˆ‘
iâ‰ j,iâ‰¥1,jâ‰¥1
pi pj(1 âˆ’pi âˆ’pj)nâˆ’2
)
+
âˆ‘
iâ‰ j,iâ‰¥1,jâ‰¥1
pi pj(1 âˆ’pi âˆ’pj)nâˆ’2(pi + pj)2 +
âˆ‘
iâ‰¥1
p2
k(1 âˆ’pk)nâˆ’1

Turingâ€™s Formula
9
âˆ’
âˆ‘
kâ‰¥1
p3
k(1 âˆ’pk)nâˆ’1 âˆ’
[
âˆ‘
kâ‰¥1
p2
k(1 âˆ’pk)nâˆ’1
]2
= 1
n
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)nâˆ’1
âˆ’1
n
âˆ‘
iâ‰¥1
âˆ‘
jâ‰¥1
pi pj(1 âˆ’pi âˆ’pj)nâˆ’2 + 1
n
âˆ‘
iâ‰¥1
p2
i (1 âˆ’2pi)nâˆ’2
+
âˆ‘
iâ‰¥1
âˆ‘
jâ‰¥1
pi pj(1 âˆ’pi âˆ’pj)nâˆ’2(pi + pj)2 âˆ’4
âˆ‘
kâ‰¥1
p4
k(1 âˆ’2pk)nâˆ’2
+
âˆ‘
kâ‰¥1
p2
k(1 âˆ’pk)n âˆ’
[
âˆ‘
kâ‰¥1
p2
k(1 âˆ’pk)nâˆ’1
]2
.
(1.8)
It can be veriï¬ed that limnâ†’âˆğœ2
n = 0 (see Exercise 10).
Denote ğœ2,nâˆ’1 = âˆ‘
kâ‰¥1 p2
k(1 âˆ’pk)nâˆ’1. By Chebyshevâ€™s theorem, for any m > 0,
P (|(T1 âˆ’ğœ‹0) âˆ’ğœ2,nâˆ’1| â‰¤mğœn) â‰¥1 âˆ’1
m2
or
P (T1 âˆ’ğœ2,nâˆ’1 âˆ’mğœn â‰¤ğœ‹0 â‰¤T1 âˆ’ğœ2,nâˆ’1 + mğœn) â‰¥1 âˆ’1
m2 .
(1.9)
Replacing the {pk} in ğœ2,nâˆ’1 and ğœn as in (1.8) by {Ì‚pk} based on a sample and
denoting the resulting values as Ì‚ğœ2,nâˆ’1, and Ì‚ğœn respectively, one may choose to
consider
(T1 âˆ’Ì‚ğœ2,nâˆ’1
) Â± mÌ‚ğœn.
(1.10)
as a conservative and approximate conï¬dence interval for ğœ‹0 at a conï¬dence
level of 1 âˆ’1âˆ•m2.
Example 1.2
Use the data in Table 1.2 to ï¬nd a 95% conï¬dence interval for ğœ‹0.
For 1 âˆ’1âˆ•m2 = 0.95, m =
âˆš
20 = 4.4721. Using n = 2000, T1 = 0.006, Ì‚ğœ2,nâˆ’1 =
1.2613 Ã— 10âˆ’6 and Ì‚ğœn = 0.0485, a conservative and approximate 95% conï¬dence
interval according to (1.10) is (âˆ’0.2109, 0.2229).
Remark 1.1
While the statement of (1.9) is precisely correct under any distri-
bution {pk} on ğ’³, (1.10) is a very weakly justiï¬ed conï¬dence interval for ğœ‹0. In
addition to the fact that the conï¬dence level associated with (1.10) is a conser-
vative lower bound, Ì‚ğœ2,nâˆ’1 and Ì‚ğœn are merely crude approximations to ğœ2,nâˆ’1 and
ğœn, respectively. One must exercise caution when (1.10) is used.
Remark 1.2
By Deï¬nition 1.1, it is clear that a good estimator is one with
a bias, namely E (T1 âˆ’ğœ‹0), that vanishes faster than the expected value of the
target, namely E (ğœ‹0), as n increases indeï¬nitely. Given that the bias is ğœ2,nâˆ’1 =
âˆ‘
kâ‰¥1 p2
k(1 âˆ’pk)nâˆ’1, one could possibly entertain the idea of a modiï¬cation to

10
Statistical Implications of Turingâ€™s Formula
Turingâ€™s formula T1 to reduce the bias, possibly to the extent such that, in view
of Example 1.1, a modiï¬ed Turingâ€™s formula may become asymptotically and
relative unbiased even for distributions with K < âˆ. As it turns out, there are
many such modiï¬cations, one of which is presented in Section 1.4.
1.2
Univariate Normal Laws
To better understand several important normal laws concerning the asymptotic
behavior of T1 âˆ’ğœ‹0, consider ï¬rst a slightly more general probability model on
the alphabet ğ’³, namely {pk,n; k â‰¥1}, instead of {pk; k â‰¥1}. The subindex n
in pk,n suggests that the probability distribution could, but not necessarily, be
dynamically changing as n changes. In this sense, {pk; k â‰¥1} is a special case
of {pk,n; k â‰¥1}.
Thirty years after Good (1953) introduced Turingâ€™s formula, Esty (1983)
established an asymptotic normal law of T1 âˆ’ğœ‹0 as stated in Theorem 1.2.
Theorem 1.2
Let the probability distribution {pk,n; k â‰¥1} be such that
1) E (N1âˆ•n) â†’c1 where 0 < c1 < 1, and
2) E (N2âˆ•n) â†’c2 â‰¥0.
Then
n (T1 âˆ’ğœ‹0)
âˆš
N1 + 2N2 âˆ’N2
1âˆ•n
L
âˆ’âˆ’âˆ’â†’N(0, 1)
(1.11)
where N1 and N2 are given by (1.1), ğœ‹0 is given by (1.2), and T1 is given by (1.4).
Example 1.3
Let {pk,n; k = 1, â€¦ , n} be such that pk,n = 1âˆ•n for every k.
E(N1âˆ•n) =
âˆ‘
kâ‰¥1
E(1[Yk = 1])âˆ•n =
âˆ‘
kâ‰¥1
P(Yk = 1)âˆ•n
=
âˆ‘
kâ‰¥1
npk,n(1 âˆ’pk,n)nâˆ’1âˆ•n = (1 âˆ’1âˆ•n)nâˆ’1 â†’eâˆ’1;
E(N2âˆ•n) =
âˆ‘
kâ‰¥1
E(1[Yk = 2])âˆ•n =
âˆ‘
kâ‰¥1
P(Yk = 2)âˆ•n
=
âˆ‘
kâ‰¥1
[n(n âˆ’1)âˆ•2!] p2
k,n(1 âˆ’pk,n)nâˆ’2âˆ•n
= (1 âˆ’1âˆ•n)nâˆ’1âˆ•2 â†’1
2eâˆ’1.
The conditions of Theorem 1.2 are satisï¬ed and therefore (1.11) holds.
Example 1.3 has a subtle but comforting implication in applications: there
exist probability distributions on ï¬nite alphabets of cardinality Kn (ï¬nite
at least at the time a sample of size n is taken) such that the normality of

Turingâ€™s Formula
11
Theorem 1.2 holds. In this sense, Theorem 1.2 provides a theoretical justiï¬ca-
tion for approximate inferential procedures derived from (1.11) to be used in
applications where the cardinalities of alphabets are often more appropriately
assumed to be ï¬nite.
The asymptotic normality of Turingâ€™s formula given in (1.11) was the ï¬rst of
its kind and the proof of Theorem 1.2 was a technical trailblazer. However, the
suï¬ƒcient condition in Theorem 1.2 is overly restrictive and consequently no
ï¬xed distribution {pk; k â‰¥1} satisï¬es the suï¬ƒcient condition. The last fact is
demonstrated by Lemma 1.4.
Lemma 1.4
Let {pk; k â‰¥1} be any probability distribution on alphabet ğ’³.
Then
a) limnâ†’âˆE (N1âˆ•n) = 0, and
b) limnâ†’âˆE (N2âˆ•n) = 0.
Proof: For Part (a), noting ï¬rst E (N1âˆ•n) = âˆ‘
kâ‰¥1 pk(1 âˆ’pk)nâˆ’1 and then
pk(1 âˆ’pk)nâˆ’1 â‰¤pk and âˆ‘
kâ‰¥1 pk = 1, by the dominated convergence theorem,
lim
nâ†’âˆE (N1âˆ•n) =
âˆ‘
kâ‰¥1
pk lim
nâ†’âˆ(1 âˆ’pk)nâˆ’1 = 0.
For Part (b), noting ï¬rst E (N2âˆ•n) = (1âˆ•2)âˆ‘
kâ‰¥1(n âˆ’1)p2
k(1 âˆ’pk)nâˆ’2 and then
that (n âˆ’1)p(1 âˆ’p)nâˆ’2 attains its maximum at p = 1âˆ•(n âˆ’1), one has
(n âˆ’1)pk(1 âˆ’pk)nâˆ’2 â‰¤[(n âˆ’2)âˆ•(n âˆ’1)]nâˆ’2 < 1.
Again by the dominated convergence theorem,
lim
nâ†’âˆE (N2âˆ•n) = (1âˆ•2)
âˆ‘
kâ‰¥1
lim
nâ†’âˆ[(n âˆ’1)p2
k(1 âˆ’pk)nâˆ’2] = 0.
â—½
Zhang and Huang (2008) modiï¬ed Estyâ€™s suï¬ƒcient condition in Theorem 1.2
and established Theorems 1.3 and 1.4 for ï¬xed probability distributions
{pk; k â‰¥1}.
Condition 1.1
There exists a ğ›¿âˆˆ(0, 1âˆ•4) such that, as n â†’âˆ,
1) n1âˆ’4ğ›¿E (N1âˆ•n) â†’c1 â‰¥0,
2) n1âˆ’4ğ›¿E (N2âˆ•n) â†’c2âˆ•2 â‰¥0, and
3) c1 + c2 > 0.
Theorem 1.3
If a given probability distribution {pk; k â‰¥1} satisï¬es Condi-
tion 1.1, then
n1âˆ’2ğ›¿(T1 âˆ’ğœ‹0)
L
âˆ’âˆ’âˆ’â†’N(0, c1 + c2).
(1.12)

12
Statistical Implications of Turingâ€™s Formula
The proof of Theorem 1.3 is lengthy and therefore is omitted here. Interested
readers may refer to Zhang and Huang (2008) for details. The rate of the weak
convergence of (1.12) plays an important role throughout this book and there-
fore is assigned a special notation for convenience
g(n, ğ›¿) = n1âˆ’2ğ›¿.
(1.13)
Let
Ì‚c1 = g2(n, ğ›¿)
n2
N1
and
Ì‚c2 = 2g2(n, ğ›¿)
n2
N2.
(1.14)
Lemma 1.5
Under Condition 1.1,
Ì‚c1
p
âˆ’âˆ’âˆ’â†’c1
and
Ì‚c2
p
âˆ’âˆ’âˆ’â†’c2.
The proof of Lemma 1.5 is left as an exercise (see Exercise 11).
Theorem 1.4 is a corollary of Theorem 1.3.
Theorem 1.4
If a given probability distribution {pk; k â‰¥1} satisï¬es Condi-
tion 1.1, then
1)
n (T1 âˆ’ğœ‹0)
âˆš
E(N1) + 2E(N2)
L
âˆ’âˆ’âˆ’â†’N(0, 1), and
(1.15)
2)
n (T1 âˆ’ğœ‹0)
âˆš
N1 + 2N2
L
âˆ’âˆ’âˆ’â†’N(0, 1).
(1.16)
Proof: For Part 1,
n (T1 âˆ’ğœ‹0)
âˆš
E(N1) + 2E(N2)
=
n
n1âˆ’2ğ›¿
âˆšc1 + c2
âˆš
E(N1) + 2E(N2)
g(n, ğ›¿) (T1 âˆ’ğœ‹0)
âˆšc1 + c2
=
(
âˆšc1 + c2
âˆš
nâˆ’4ğ›¿E(N1) + 2nâˆ’4ğ›¿E(N2)
) [
g(n, ğ›¿) (T1 âˆ’ğœ‹0)
âˆšc1 + c2
]
.
In the above-mentioned last expression, the ï¬rst factor converges to 1 by Con-
dition 1.1 and the second factor converges weakly to the standard normal dis-
tribution by Theorem 1.3. The result of (1.15) follows Slutskyâ€™s theorem.
For Part 2,
n (T1 âˆ’ğœ‹0)
âˆš
N1 + 2N2
=
n
n1âˆ’2ğ›¿
âˆšc1 + c2
âˆš
N1 + 2N2
g(n, ğ›¿) (T1 âˆ’ğœ‹0)
âˆšc1 + c2
=
(
âˆšc1 + c2
âˆš
nâˆ’4ğ›¿N1 + 2nâˆ’4ğ›¿N2
) [
g(n, ğ›¿) (T1 âˆ’ğœ‹0)
âˆšc1 + c2
]
.

Turingâ€™s Formula
13
In the above-mentioned last expression, the ï¬rst factor converges to 1 by
Lemma 1.5 and the second factor converges weakly to the standard normal
distribution by Theorem 1.3. The result of (1.16) follows Slutskys theorem.
â—½
An interesting feature of Theorem 1.4 is that, although Condition 1.1 calls
for the existence of a ğ›¿âˆˆ(0, 1âˆ•4), the results of Theorem 1.4 do not depend
on such a ğ›¿, as evidenced by its total absence in (1.15) and (1.16). Also to be
noted is the fact that the conditions of Theorems 1.2 and 1.3 do not overlap.
The condition of Theorem 1.2 essentially corresponds to ğ›¿= 1âˆ•4 in the form of
the condition of Theorem 1.4, which is on the boundary of the interval (0, 1âˆ•4)
but not in the interior of the interval.
Remark 1.3
The value of ğ›¿represents a tail property of the underlying dis-
tribution. The fact that (1.15) does not depend on ğ›¿may be considered as a
consequence of a domain of attraction of certain types of distributions sharing
the same asymptotic normality via Turingâ€™s formula. Discussion of domains of
attraction on alphabets is given in Chapter 6.
The following example shows that the class of probability distributions satis-
fying the condition of Theorem 1.4 is in fact nonempty. It also gives a descrip-
tion of the bulk of distributions in the family that supports the normal laws in
(1.15) and (1.16).
Example 1.4
If pk = cğœ†kâˆ’ğœ†for all k â‰¥1 where ğœ†> 1 and cğœ†is such that
âˆ‘
kâ‰¥1 pk = 1, then {pk; k â‰¥1} satisï¬es the conditions of Theorem 1.4.
To see that the claim of Example 1.4 is true, the following two lemmas are
needed.
Lemma 1.6
(Eulerâ€“Maclaurin)
For each n, n = 1, 2, â€¦ , let fn(x) be a con-
tinuous function of x on [x0, âˆ) where x0 is a positive integer. Suppose there exists
a sequence of real values, xn satisfying xn â‰¥x0, such that fn(x) is increasing on
[x0, xn] and decreasing on [xn, âˆ). If fn(x0) â†’0 and fn(xn) â†’0, then
lim
nâ†’âˆ
âˆ‘
kâ‰¥x0
fn(k) = lim
nâ†’âˆâˆ«
âˆ
x0
fn(x) dx.
Proof: It can be veriï¬ed that
âˆ‘
x0â‰¤kâ‰¤xn
fn(k) âˆ’fn(xn) â‰¤âˆ«
xn
x0
fn(x) dx â‰¤
âˆ‘
x0+1â‰¤k<xn
fn(k) + fn(xn)
and
âˆ‘
k>xn
fn(k) âˆ’fn(xn) â‰¤âˆ«
âˆ
xn
fn(x) dx â‰¤
âˆ‘
kâ‰¥xn
fn(k) + fn(xn).
Adding the corresponding parts of the above-mentioned two expressions and
taking limits give

14
Statistical Implications of Turingâ€™s Formula
lim
nâ†’âˆ
âˆ
âˆ‘
k=x0
fn(k) âˆ’2 lim
nâ†’âˆfn(xn) â‰¤lim
nâ†’âˆâˆ«
âˆ
x0
fn(x) dx
â‰¤lim
nâ†’âˆ
âˆ
âˆ‘
k=x0
fn(k) âˆ’lim
nâ†’âˆfn(x0) + 2 lim
nâ†’âˆfn(xn).
The desired result follows from the conditions of the lemma.
â—½
Condition 1.2
There exists a ğ›¿âˆˆ(0, 1âˆ•4) such that, as n â†’âˆ,
1) n1âˆ’4ğ›¿âˆ‘
kâ‰¥1 pk eâˆ’npk â†’c1 â‰¥0,
2) n2âˆ’4ğ›¿âˆ‘
kâ‰¥1 p2
keâˆ’npk â†’c2 â‰¥0, and
3) c1 + c2 > 0.
Lemma 1.7
Conditions 1.1 and 1.2 are equivalent.
The proof of Lemma 1.7 is nontrivial and is found in Zhang (2013a), where a
more general statement is proved in the appendix.
In Example 1.4, let ğ›¿= (4ğœ†)âˆ’1 and therefore g(n, ğ›¿) = n1âˆ’2ğ›¿= n1âˆ’1
2ğœ†. By
Lemma 1.7, it suï¬ƒces to check Condition 1.2 instead of Condition 1.1:
g2(n, ğ›¿)
n
âˆ‘
kâ‰¥1
pk eâˆ’npk = n1âˆ’1
ğœ†
âˆ
âˆ‘
k=k0
cğœ†kâˆ’ğœ†eâˆ’ncğœ†kâˆ’ğœ†=
âˆ
âˆ‘
k=k0
fn(k)
where
fn(x) = n1âˆ’1
ğœ†cğœ†xâˆ’ğœ†eâˆ’ncğœ†xâˆ’ğœ†.
Since it is easily veriï¬ed that
f â€²
n(x) = âˆ’ğœ†cğœ†n1âˆ’1
ğœ†xâˆ’(ğœ†+1) (1 âˆ’ncğœ†xâˆ’ğœ†) eâˆ’ncğœ†xâˆ’ğœ†,
fn(x) increases and decreases, respectively, over the following two sets
[1, (ncğœ†)1âˆ•ğœ†]
and
[(ncğœ†)1âˆ•ğœ†, âˆ) .
Let x0 = 1 and xn = (ncğœ†)1âˆ•ğœ†. It is clear that fn(x0) â†’0 and
fn(xn) = n1âˆ’1
ğœ†cğœ†(ncğœ†)âˆ’1eâˆ’ncğœ†(ncğœ†)âˆ’1
= n1âˆ’1
ğœ†cğœ†(ncğœ†)âˆ’1eâˆ’1
= eâˆ’1nâˆ’1âˆ•ğœ†â†’0.
Invoking the Eulerâ€“Maclaurin lemma, with changes of variable t = xâˆ’ğœ†and
then s = ncğœ†t,
n1âˆ’1
ğœ†
âˆ
âˆ‘
k=1
pk eâˆ’npk â‰ƒâˆ«
âˆ
x0
n1âˆ’1
ğœ†cğœ†xâˆ’ğœ†eâˆ’ncğœ†xâˆ’ğœ†dx

Turingâ€™s Formula
15
= cğœ†
ğœ†âˆ«
xâˆ’ğœ†
0
0
n1âˆ’1
ğœ†tâˆ’1
ğœ†eâˆ’ncğœ†tdt
= cğœ†
ğœ†n1âˆ’1
ğœ†âˆ«
xâˆ’ğœ†
0
0
(ncğœ†t)âˆ’1
ğœ†(ncğœ†)âˆ’1+ 1
ğœ†eâˆ’ncğœ†td(ncğœ†t)
= cğœ†
ğœ†n1âˆ’1
ğœ†(nc)âˆ’1+ 1
ğœ†âˆ«
ncğœ†xâˆ’ğœ†
0
0
sâˆ’1
ğœ†eâˆ’sds
= (cğœ†)
1
ğœ†
ğœ†
n0
âˆ«
ncğœ†xâˆ’ğœ†
0
0
sâˆ’1
ğœ†eâˆ’sds
= (cğœ†)
1
ğœ†
ğœ†
âˆ«
ncğœ†xâˆ’ğœ†
0
0
s
(
1âˆ’1
ğœ†
)
âˆ’1eâˆ’sds
= (cğœ†)
1
ğœ†
ğœ†
Î“
(
1 âˆ’1
ğœ†
) â¡
â¢
â¢
â¢â£
1
Î“
(
1 âˆ’1
ğœ†
) âˆ«
ncğœ†xâˆ’ğœ†
0
0
s
(
1âˆ’1
ğœ†
)
âˆ’1eâˆ’sds
â¤
â¥
â¥
â¥â¦
â†’(cğœ†)
1
ğœ†
ğœ†
Î“ (1 âˆ’ğœ†âˆ’1) > 0
where â€œâ‰ƒâ€ indicates equality in limit as n â†’âˆ.
Thus, the suï¬ƒcient condition of Theorem 1.4 is veriï¬ed and therefore the
claim in Example 1.4 is true.
It is important to note that Condition 1.1 (and therefore Condition 1.2) is in
fact a tail property of the underlying distribution, that is, the values of pk for
k < k0 for any arbitrarily ï¬xed positive integer k0 are completely irrelevant to
whether these conditions hold. To see this fact, it suï¬ƒces to note, for example,
E (N1âˆ•n) =
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)nâˆ’1 =
âˆ‘
k<k0
pk(1 âˆ’pk)nâˆ’1 +
âˆ‘
kâ‰¥k0
pk(1 âˆ’pk)nâˆ’1.
The ï¬rst of the two terms in the above-mentioned last expression converges to
zero exponentially fast, and therefore the question whether E (N1âˆ•n) converges
to zero at a power decaying rate rests entirely with the second of the two terms.
A similar argument applies to E (N2âˆ•n).
Example 1.5
Let {pk} be such that pk is not speciï¬ed for k < k0 and pk = ckâˆ’ğœ†
for k â‰¥k0 for some integer k0, where ğœ†> 1 and c are such that âˆ‘
kâ‰¥1 pk = 1, then
{pk; k â‰¥1} satisï¬es Condition 1.1.
The proof of Example 1.5 is left as an exercise (see Exercise 12).
Using Theorem 1.4, an approximate 100(1 âˆ’ğ›¼)% conï¬dence interval for ğœ‹0
can be devised:
N1
n Â± zğ›¼âˆ•2
âˆš
N1 + 2N2
n
.
(1.17)

16
Statistical Implications of Turingâ€™s Formula
Example 1.6
Use the data in Table 1.1 to construct a 95% conï¬dence interval
for ğœ‹0. With n = 2000, N1 = 12, and N2 = 1 by (1.17), a 95% conï¬dence interval
for ğœ‹0 is
N1
n Â± zğ›¼âˆ•2
âˆš
N1 + 2N2
n
=
12
2000 Â± 1.96
âˆš
12 + 2 Ã— 1
2000
= 0.006 Â± 0.0037 = (0.0023, 0.0097).
While the suï¬ƒcient conditions of asymptotic normality in Theorems 1.2 and
1.4 are often easy to check, they are both superseded by a necessary and suï¬ƒ-
cient condition given by Zhang and Zhang (2009). Let
s2
n = n
âˆ‘
kâ‰¥1
pk,n eâˆ’npk,n + n2 âˆ‘
kâ‰¥1
p2
k,neâˆ’npk,n,
where {pk,n} is a probability distribution on the alphabet ğ’³. Consider
Condition 1.3 below.
Condition 1.3
As n â†’âˆ,
1) E(N1) + E(N2) â†’âˆ, and
2)
(
n
sn
)2âˆ‘
kâ‰¥1 p2
k,neâˆ’npk,n1[npk,n > Îµsn] â†’0, for any Îµ > 0.
The following theorem is due to Zhang and Zhang (2009).
Theorem 1.5
A probability distribution {pk,n} on ğ’³satisï¬es Condition 1.3 if
and only if
n(T1 âˆ’ğœ‹0)
âˆš
E(N1) + 2E(N2)
L
âˆ’âˆ’âˆ’â†’N(0, 1).
(1.18)
Condition 1.3 is less restrictive than either of the conditions of Theorems 1.2
and 1.4. Condition 1.3 is quite subtle and can be diï¬ƒcult to verify for a particular
distribution. However, it does identify some distributions that do not admit the
asymptotic normality (1.18) for Turingâ€™s formula.
Example 1.7
If {pk} is a probability distribution on a ï¬nite alphabet, then the
normality of (1.18) does not hold.
To see this, one only needs to check the ï¬rst part of Condition 1.3. Let pâˆ§=
min{pk > 0; k â‰¥1}:
E(N1) = n
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)nâˆ’1 â‰¤n(1 âˆ’pâˆ§)nâˆ’1 â†’0,

Turingâ€™s Formula
17
E(N2) = n(n âˆ’1)
2
âˆ‘
kâ‰¥1
p2
k(1 âˆ’pk)nâˆ’2 â‰¤n2(1 âˆ’pâˆ§)nâˆ’2 â†’0,
and therefore E(N1) + E(N2) â†’0.
Example 1.8
If {pk} is such that pk = c0 eâˆ’k where c0 = e âˆ’1 for all k â‰¥1,
then the normality of (1.18) does not hold.
A veriï¬cation of the statement in Example 1.8 will be a lengthy and tedious
one and therefore is left as an exercise. (See Exercise 14. This is a diï¬ƒcult
exercise and readers may wish to defer it until Chapter 6.)
Theorem 1.6 below is a corollary to Theorem 1.5.
Theorem 1.6
If a probability distribution {pk,n} on ğ’³satisï¬es Condition 1.3,
then
n(T1 âˆ’ğœ‹0)
âˆš
N1 + 2N2
L
âˆ’âˆ’âˆ’â†’N(0, 1)
(1.19)
Remark 1.4
In view of Theorems 1.4 and 1.5 and of Examples 1.4, 1.7, and
1.8, one might be tempted to reach a conclusion that Turingâ€™s formula would
admit asymptotic normality when the underlying distribution has a thick tail.
As it turns out, the notion of a â€œthin tailâ€ or a â€œthick tailâ€ is not easily deï¬ned
on an alphabet ğ’³. The natural and intuitive notion of a tail of a distribution
on the real line does not carry over well to a countable alphabet. The asymptotic
behavior of quantities, such as E(N1), could be quite intricate and will be dealt
with in all other subsequent chapters, in particular Chapter 6.
Remark 1.5
Theorem 1.5 is established (and is stated earlier) under a gen-
eral distribution family {pk,n} on ğ’³whose members may dynamically change
with the sample size n. The theorem remains valid if restricted to the subclass of
ï¬xed distributions {pk}. For pedagogical and statistical simplicity, the distribu-
tions on ğ’³considered in the remainder of this book are all members of the ï¬xed
family, unless otherwise speciï¬ed.
In the same spirit of the relative bias given in Deï¬nition 1.1, one may also be
interested in various asymptotic aspects of the following random variable
T1 âˆ’ğœ‹0
ğœ‹0
= T1
ğœ‹0
âˆ’1.
Condition 1.4
There exists a ğ›¿âˆˆ(0, 1âˆ•4) such that, as n â†’âˆ,
1) n1âˆ’4ğ›¿E (N1âˆ•n) â†’c1 > 0, and
2) n1âˆ’4ğ›¿E (N2âˆ•n) â†’c2
2 â‰¥0.

18
Statistical Implications of Turingâ€™s Formula
Condition 1.4 is slightly more restrictive than Condition 1.1 in that c1 in Part
1 of Condition 1.4 is required to be strictly greater than zero.
Theorem 1.7
If {pk; k â‰¥1} satisï¬es Condition 1.4, then
n2ğ›¿
(T1 âˆ’ğœ‹0
ğœ‹0
)
= n2ğ›¿
(T1
ğœ‹0
âˆ’1
)
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2)
(1.20)
where ğœ2 = (c1 + c2)âˆ•c2
1.
Proof: Note ï¬rst
n2ğ›¿
(T1 âˆ’ğœ‹0
ğœ‹0
)
=
(
n2ğ›¿
g(n, ğ›¿)ğœ‹0
)
[g(n, ğ›¿) (T1 âˆ’ğœ‹0) ]
=
(
1
n1âˆ’4ğ›¿ğœ‹0
)
[g(n, ğ›¿) (T1 âˆ’ğœ‹0) ] .
By Theorem 1.3, g(n, ğ›¿)(T1 âˆ’ğœ‹0)
L
âˆ’âˆ’âˆ’â†’N(0, c1 + c2), and therefore it suï¬ƒces to
show that n1âˆ’4ğ›¿ğœ‹0
pâ†’c1 > 0. Toward that end, consider ï¬rst
E (n1âˆ’4ğ›¿ğœ‹0
) = n1âˆ’4ğ›¿âˆ‘
kâ‰¥1
pk(1 âˆ’pk)n
=
(
n
n + 1
)1âˆ’4ğ›¿
(n + 1)1âˆ’4ğ›¿âˆ‘
kâ‰¥1
pk(1 âˆ’pk)n
=
(
n
n + 1
)1âˆ’4ğ›¿[
g2(n + 1, ğ›¿)
n + 1
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)n
]
.
In the above-mentioned last expression, the ï¬rst factor converges to 1 and the
second factor converges to c1 due to Part 1 of Condition 1.4. Therefore, it is
established that E (n1âˆ’4ğ›¿ğœ‹0) â†’c1 > 0.
Consider next
Var (n1âˆ’4ğ›¿ğœ‹0) = n2âˆ’8ğ›¿[E(ğœ‹2
0) âˆ’(E(ğœ‹0))2]
= n2âˆ’8ğ›¿
â¡
â¢
â¢â£
E
â›
âœ
âœâ
(
âˆ‘
kâ‰¥1
pk1[Yk = 0
)2â
âŸ
âŸâ 
âˆ’
(
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)n
)2â¤
â¥
â¥â¦
= n2âˆ’8ğ›¿
[
âˆ‘
kâ‰¥1
p2
k(1 âˆ’pk)n +
âˆ‘
iâ‰ j
pi pj(1 âˆ’pi âˆ’pj)n
âˆ’
(
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)n
)2â¤
â¥
â¥â¦

Turingâ€™s Formula
19
= n2âˆ’8ğ›¿
(
âˆ‘
kâ‰¥1
p2
k(1 âˆ’pk)n âˆ’
âˆ‘
kâ‰¥1
p2
k(1 âˆ’pk)2n
)
+ n2âˆ’8ğ›¿
(
âˆ‘
iâ‰ j
pi pj(1 âˆ’pi âˆ’pj)n
âˆ’
âˆ‘
iâ‰ j
pi pj(1 âˆ’pi)n(1 âˆ’pj)n
)
â‰¤n2âˆ’8ğ›¿âˆ‘
kâ‰¥1
p2
k(1 âˆ’pk)n.
The last inequality holds since for every (pi, pj),
1 âˆ’pi âˆ’pj â‰¤1 âˆ’pi âˆ’pj + pi pj = (1 âˆ’pi)(1 âˆ’pk).
However, by Condition 1.4,
n2âˆ’8ğ›¿âˆ‘
kâ‰¥1
p2
k(1 âˆ’pk)n =
[
n2âˆ’8ğ›¿
(n + 2)2âˆ’4ğ›¿
] [
2(n + 2)2
(n + 2)(n + 1)
]
Ã—
[
g2(n + 2, ğ›¿)
(n + 2)2
âˆ‘
kâ‰¥1
(n + 2
2
)
p2
k(1 âˆ’pk)n
]
â†’0,
since the three factors in the last expression converge, respectively, to zero at
the rate of îˆ»(nâˆ’4ğ›¿), 2, and c2âˆ•2. It follows that Var (n1âˆ’4ğ›¿ğœ‹0) â†’0, and therefore
n1âˆ’4ğ›¿ğœ‹0
pâ†’c1. The result of the theorem immediately follows Slutskyâ€™s theorem.
â—½
Remark 1.6
The respective weak convergences of g(n, ğ›¿)(T1 âˆ’ğœ‹0) and
n2ğ›¿(T1âˆ•ğœ‹0 âˆ’1) provide two diï¬€erent perspectives in terms of inference regarding
ğœ‹0. It may be interesting to note that, under Condition 1.4, the rates of weak
convergence of (T1 âˆ’ğœ‹0) and (T1âˆ•ğœ‹0 âˆ’1) are, respectively, g(n, ğ›¿) = n1âˆ’2ğ›¿and
n2ğ›¿. Since ğ›¿âˆˆ(0, 1âˆ•4), it follows that 2ğ›¿âˆˆ(0, 1âˆ•2) and that 1 âˆ’2ğ›¿âˆˆ[1âˆ•2, 1),
and therefore n1âˆ’2ğ›¿always increases at a faster rate than n2ğ›¿. The faster
convergence of (T1 âˆ’ğœ‹0) suggests that the associated additive perspective of
Turingâ€™s formula (as opposed to the multiplicative perspective associated with
T1âˆ•ğœ‹0) may perhaps be intrinsically more eï¬€ective.
Corollary 1.1
If {pk; k â‰¥1} satisï¬es Condition 1.4, then
1)
E(N1) (T1âˆ•ğœ‹0 âˆ’1)
âˆš
E(N1) + 2E(N2)
L
âˆ’âˆ’âˆ’â†’N(0, 1), and
(1.21)
2)
N1 (T1âˆ•ğœ‹0 âˆ’1)
âˆš
N1 + 2N2
L
âˆ’âˆ’âˆ’â†’N(0, 1).
(1.22)

20
Statistical Implications of Turingâ€™s Formula
Proof: For Part 1, by Theorem 1.7 and Condition 1.4,
n2ğ›¿(T1âˆ•ğœ‹0 âˆ’1)
âˆš
E(N1)âˆ•n4ğ›¿+2E(N2)âˆ•n4ğ›¿
(E(N1)âˆ•n4ğ›¿)2
= (T1âˆ•ğœ‹0 âˆ’1)
âˆš
E(N1)+2E(N2)
(E(N1))2
= E(N1) (T1âˆ•ğœ‹0 âˆ’1)
âˆš
E(N1) + 2E(N2)
L
âˆ’âˆ’âˆ’â†’N(0, 1).
For Part 2, the result immediately follows the facts that N1âˆ•n4ğ›¿pâ†’c1 and that
N2âˆ•n4ğ›¿pâ†’c2âˆ•2 of Lemma 1.5.
â—½
Once again, while the convergence rate of T1âˆ•ğœ‹0 âˆ’1 depends on the under-
lying distribution, the left-hand side of Part 2 of Corollary 1.1 is a function of
observable components except ğœ‹0, thus allowing statistical inference regarding
ğœ‹0. Using (1.22), it can be veriï¬ed that an approximate 100 Ã— (1 âˆ’ğ›¼)% conï¬-
dence interval for ğœ‹0 is
T1
1 + zğ›¼âˆ•2
âˆš
N1+2N2
N1
â‰¤ğœ‹0 â‰¤
T1
1 âˆ’zğ›¼âˆ•2
âˆš
N1+2N2
N1
(1.23)
(see Exercise 24).
Example 1.9
Use (1.23) and the data in Table 1.1 to construct a 95% conï¬-
dence intervalfor ğœ‹0. With n = 2000, N1 = 12, N2 = 1, and T1 = 0.006,
zğ›¼âˆ•2
âˆš
N1 + 2N2
N1
= 1.96
âˆš
12 + 2
12
= 0.6111,
the 95% conï¬dence interval is
(
0.006
1 + 0.6111,
0.006
1 âˆ’0.6111
)
= (0.0037, 0.0150).
In comparing the two 95% conï¬dence intervals of Examples 1.6 and 1.9, that
is, (0.0023, 0.0097) and (0.0037, 0.0150), respectively, it may be interesting to
note that one is larger than the other, and that the smaller interval is not entirely
nested in the larger interval.
Lemma 1.8
Let Xn be a sequence of random variables satisfying
h(n)(Xn âˆ’ğœƒ)
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2)
where h(â‹…) is a strictly increasing function satisfying limtâ†’âˆh(t) = âˆ, ğœƒand
ğœ> 0 are two ï¬xed constants. Then, for any given Îµ > 0,
lim
nâ†’âˆP(|Xn âˆ’ğœƒ| < Îµ) = 1.

Turingâ€™s Formula
21
Proof: Without loss of generality, assume that ğœ= 1:
P(|Xn âˆ’ğœƒ| < Îµ) = P(âˆ’Îµ < Xn âˆ’ğœƒ< Îµ)
= P(Xn âˆ’ğœƒ< Îµ) âˆ’P(Xn âˆ’ğœƒâ‰¤âˆ’Îµ)
= P(h(n)(Xn âˆ’ğœƒ) < h(n)Îµ) âˆ’P(h(n)(Xn âˆ’ğœƒ) â‰¤âˆ’h(n)Îµ).
It is desired to show that, for any given ğ›¼, there exists a suï¬ƒciently large inte-
ger N such that for all n > N, P(|Xn âˆ’ğœƒ| < Îµ) > 1 âˆ’ğ›¼.
Toward that end, let zğ›¼âˆ•2 be the 100(1 âˆ’ğ›¼âˆ•2)th percentile and N1 =
âŒˆgâˆ’1(zğ›¼âˆ•2âˆ•Îµ)âŒ‰where hâˆ’1(â‹…) is the inverse function of h(â‹…). For any n > N1,
noting n > hâˆ’1(zğ›¼âˆ•2âˆ•Îµ),
P(|Xn âˆ’ğœƒ| < Îµ) â‰¥P(h(n)(Xn âˆ’ğœƒ) < zğ›¼âˆ•2) âˆ’P(h(n)(Xn âˆ’ğœƒ) â‰¤âˆ’zğ›¼âˆ•2).
On the other hand, by the given asymptotic normality, there exist suï¬ƒciently
large integers N2 and N3 such that for all n > N2
|P(h(n)(Xn âˆ’ğœƒ) < zğ›¼âˆ•2) âˆ’Î¦(zğ›¼âˆ•2)| < ğ›¼âˆ•2
and for all n > N3
|P(h(n)(Xn âˆ’ğœƒ) < âˆ’zğ›¼âˆ•2) âˆ’Î¦(âˆ’zğ›¼âˆ•2)| < ğ›¼âˆ•2
where Î¦(â‹…) is the cdf of the standard normal distribution. Consequently for all
n > max{N2, N3},
P(h(n)(Xn âˆ’ğœƒ) < zğ›¼âˆ•2) âˆ’P(h(n)(Xn âˆ’ğœƒ) < âˆ’zğ›¼âˆ•2) > 1 âˆ’ğ›¼.
The proof is completed by deï¬ning N = max{N1, N2, N3}.
â—½
Corollary 1.2
If {pk; k â‰¥1} satisï¬es Condition 1.4, then
T1
ğœ‹0
p
âˆ’âˆ’âˆ’â†’1.
(1.24)
Proof: The result immediately follows Theorem 1.7 and Lemma 1.8.
â—½
Property (1.24) is sometimes referred to as the multiplicative consistency of
Turingâ€™s formula. Interested readers may wish to see Ohannessian and Dahleh
(2012) for a more detailed discussion on such consistency under slightly more
general conditions.
Corollary 1.3
If {pk; k â‰¥1} satisï¬es Condition 1.4, then
1)
E(N1) (ln T1 âˆ’ln ğœ‹0)
âˆš
E(N1) + 2E(N2)
L
âˆ’âˆ’âˆ’â†’N(0, 1) and
(1.25)

22
Statistical Implications of Turingâ€™s Formula
2)
N1 (ln T1 âˆ’ln ğœ‹0)
âˆš
N1 + 2N2
L
âˆ’âˆ’âˆ’â†’N(0, 1).
(1.26)
The proof of Corollary 1.3 is left as an exercise (see Exercise 23).
1.3
Multivariate Normal Laws
Turingâ€™s formula T1 is only one member of a family introduced by Good (1953),
albeit the most famous one. Recall the notations, for every integer r, 1 â‰¤r â‰¤n,
Nr, and ğœ‹râˆ’1 as deï¬ned in (1.1) and (1.2). The following may be referred to as
the rth order Turingâ€™s formula.
Tr =
(
n
r âˆ’1
) (n
r
)âˆ’1
Nr =
r
n âˆ’r + 1Nr.
(1.27)
Tr may be thought of as an estimator of ğœ‹râˆ’1. The main objective of this section
is to give a multivariate normal law for the vector
(T1, T2, â€¦ , TR)ğœ
where R is an arbitrarily ï¬xed positive integer. Toward that end, a normal law
for Tr, where r is any positive integer, is ï¬rst given.
The following is a suï¬ƒcient condition under which many of the subsequent
results are established. Let g(n, ğ›¿) = n1âˆ’2ğ›¿as in (1.13).
Condition 1.5
There exists a ğ›¿âˆˆ(0, 1âˆ•4) such that as n â†’âˆ,
1) g2(n, ğ›¿)
n2
E(Nr) â†’cr
r! â‰¥0,
2) g2(n, ğ›¿)
n2
E(Nr+1) â†’
cr+1
(r + 1)! â‰¥0, and
3) cr + cr+1 > 0.
Theorem 1.8
Under Condition 1.5,
g(n, ğ›¿)(Tr âˆ’ğœ‹râˆ’1)
L
âˆ’âˆ’âˆ’â†’N
(
0, cr+1 + rcr
(r âˆ’1)!
)
.
The proof of Theorem 1.8 is based on a direct evaluation of the characteristic
function of g(n, ğ›¿)(Tr âˆ’ğœ‹râˆ’1), which can be shown to have a limit, as n â†’âˆ, of
exp
{
âˆ’t2
2
[
cr+1
(r âˆ’1)! +
rcr
(r âˆ’1)!
] }
.
The details of the proof are found in Zhang (2013a).
Theorem 1.9 below is a restatement of Theorem 1.8.

Turingâ€™s Formula
23
Theorem 1.9
Under Condition 1.5,
n(Tr âˆ’ğœ‹râˆ’1)
âˆš
r2E(Nr) + (r + 1)rE(Nr+1)
L
âˆ’âˆ’âˆ’â†’N(0, 1).
The proof of Theorem 1.9 is straightforward and is left as an exercise (see
Exercise 16).
Lemma 1.9
Let
Ì‚cr = r!g2(n, ğ›¿)
n2
Nr.
Under Condition 1.5, Ì‚cr converges to cr in probability.
By Condition 1.5 and Chebyshevâ€™s inequality, it suï¬ƒces to show that
Var(Ì‚cr) â†’0.
A proof of that is found in Zhang (2013a).
Theorem 1.10
Under Condition 1.5,
n(Tr âˆ’ğœ‹râˆ’1)
âˆš
r2Nr + (r + 1)rNr+1
L
âˆ’âˆ’âˆ’â†’N(0, 1).
Theorem 1.10 is a direct consequence of Theorem 1.9 and Lemma 1.9. The
proof is left as an exercise (see Exercise 17).
Similar to the asymptotic results in Theorems 1.3 and 1.4, it may be of interest
to note that the results of Theorems 1.9 and 1.10 require no further knowledge
of g(n, ğ›¿), that is, the knowledge of ğ›¿, other than its existence.
To establish the asymptotic bivariate distribution of (Tr1, Tr2) where r1 and r2
are two diï¬€erent positive integers, the following condition plays a central role.
Condition 1.6
Let g(n, ğ›¿) be as in (1.13). There exists a ğ›¿âˆˆ(0, 1âˆ•4) such that
as n â†’âˆ,
1) g2(n, ğ›¿)
n2
E(Nr1) â†’
cr1
r1! â‰¥0,
2) g2(n, ğ›¿)
n2
E(Nr1+1) â†’
cr1+1
(r1 + 1)! â‰¥0,
3) cr1 + cr1+1 > 0,
4) g2(n, ğ›¿)
n2
E(Nr2) â†’
cr2
r2! â‰¥0,
5) g2(n, ğ›¿)
n2
E(Nr2+1) â†’
cr2+1
(r2 + 1)! â‰¥0, and
6) cr2 + cr2+1 > 0.

24
Statistical Implications of Turingâ€™s Formula
Lemma 1.10
For any two constants, a and b satisfying a2 + b2 > 0, assuming
that r1 < r2 âˆ’1 and that Condition 1.6 holds, then
g(n, ğ›¿)[a(Tr1 âˆ’ğœ‹r1âˆ’1) + b(Tr2 âˆ’ğœ‹r2âˆ’1)]
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2)
where
ğœ2 = a2 cr1+1 + r1 cr1
(r1 âˆ’1)!
+ b2 cr2+1 + r2 cr2
(r2 âˆ’1)!
.
Lemma 1.11
For any two constants, a and b satisfying a2 + b2 > 0, assuming
that r1 = r2 âˆ’1 and that Condition 1.6 holds, then
g(n, ğ›¿)[a(Tr1 âˆ’ğœ‹r1âˆ’1) + b(Tr2 âˆ’ğœ‹r2âˆ’1)]
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2)
where
ğœ2 = a2 cr1+1 + r1 cr1
(r1 âˆ’1)!
âˆ’2ab
cr2
(r1 âˆ’1)! + b2 cr2+1 + r2 cr2
(r2 âˆ’1)!
.
The proofs of both Lemmas 1.10 and 1.11 are based on a straightforward
evaluation of the characteristic functions of the underlying statistic. They are
lengthy and tedious and they are found in Zhang (2013a).
Let
ğœ2
r = r2E(Nr) + (r + 1)rE(Nr+1),
ğœŒr(n) = âˆ’r(r + 1)E(Nr+1)âˆ•(ğœr ğœr+1),
ğœŒr = lim
nâ†’âˆğœŒr(n),
Ì‚ğœ2
r = r2Nr + (r + 1)rNr+1,
Ì‚ğœŒr = Ì‚ğœŒr(n) = âˆ’r(r + 1)
Nr+1
âˆš
Ì‚ğœ2
r Ì‚ğœ2
r+1
.
By the consistency of Lemma 1.9 and the deï¬nition of multivariate normality
of a random vector, the following two corollaries and two theorems are
immediate.
Corollary 1.4
Assume that r1 < r2 âˆ’1 and that Condition 1.6 holds, then
n
(
Tr1 âˆ’ğœ‹r1âˆ’1
ğœr1
,
Tr2 âˆ’ğœ‹r2âˆ’1
ğœr2
)ğœ
L
âˆ’âˆ’âˆ’â†’MVN (âˆ…, I2Ã—2)
where âˆ…is a zero vector and I2Ã—2 is the two-dimensional identity matrix.
Corollary 1.5
Assume that r1 = r2 âˆ’1 and that Condition 1.6 holds, then
n
(
Tr1 âˆ’ğœ‹r1âˆ’1
ğœr1
,
Tr2 âˆ’ğœ‹r2âˆ’1
ğœr2
)ğœ
L
âˆ’âˆ’âˆ’â†’MVN
(
âˆ…,
( 1 ğœŒr1
ğœŒr1
1
) )
.

Turingâ€™s Formula
25
Corollaries 1.4 and 1.5 suggest that, in the following series
{
n
(Tr âˆ’ğœ‹râˆ’1
ğœr
)
; r â‰¥1
}
,
any two entries are asymptotically independent unless they are immediate
neighbors.
Theorem 1.11
For any positive integer R, if Condition 1.6 holds for every r,
1 â‰¤r â‰¤R, then
n
(T1 âˆ’ğœ‹0
ğœ1
, T2 âˆ’ğœ‹1
ğœ2
, â€¦ , TR âˆ’ğœ‹Râˆ’1
ğœR
)ğœ
L
âˆ’âˆ’âˆ’â†’MVN(âˆ…, Î£)
where âˆ…is a zero vector, Î£ = (ai,j) is an R Ã— R covariance matrix with all
the diagonal elements being ar,r = 1 for r = 1, â€¦ , R, the super-diagonal and
the sub-diagonal elements being ar,r+1 = ar+1,r = ğœŒr for r = 1, â€¦ , R âˆ’1, and
all the other oï¬€-diagonal elements being zeros.
Let Ì‚Î£ be the resulting matrix of Î£ with ğœŒr replaced by Ì‚ğœŒr(n) for all r. Let Ì‚Î£âˆ’1
denote the inverse of Ì‚Î£ and Ì‚Î£âˆ’1âˆ•2 denote any R Ã— R matrix satisfying Ì‚Î£âˆ’1 =
( Ì‚Î£âˆ’1âˆ•2)ğœÌ‚Î£âˆ’1âˆ•2.
Theorem 1.12
For any positive integer R, if Condition 1.6 holds for every r,
1 â‰¤r â‰¤R, then
n Ì‚Î£âˆ’1âˆ•2
(T1 âˆ’ğœ‹0
Ì‚ğœ1
, T2 âˆ’ğœ‹1
Ì‚ğœ2
, â€¦ , TR âˆ’ğœ‹Râˆ’1
Ì‚ğœR
)ğœ
L
âˆ’âˆ’âˆ’â†’MVN(âˆ…, IRÃ—R).
Theorem 1.12 has a profound statistical implication that can be explored by
considering the following three points:
1) The alphabet ğ’³= {ğ“k; k â‰¥1} is dynamically partitioned by a sample of size
n into n + 1 groups
{ğ“k âˆ¶Yk = 0}, {ğ“k âˆ¶Yk = 1}, â€¦ , {ğ“k âˆ¶Yk = r}, â€¦ , {ğ“k âˆ¶Yk = n}.
The probability sequence {pk; k â‰¥1} is also dynamically sorted accordingly
into the n + 1 groups with respect to the partition above, that is,
{pk âˆ¶Yk = 0}, {pk âˆ¶Yk = 1}, â€¦ , {pk âˆ¶Yk = r}, â€¦ , {pk âˆ¶Yk = n}.
2) For any chosen ï¬xed integer R â‰¥1, the respective group total probabilities
for the ï¬rst R âˆ’1 groups, the entries in the second row below,
{â„“k : Yk = 0}
{â„“k : Yk = 1}
. . .
{â„“k : Yk = R âˆ’1}

k:Yk=0 pk

k:Yk=1 pk
. . .

k:Yk=Râˆ’1 pk
(1.28)
may be statistically estimated according to Theorem 1.12.

26
Statistical Implications of Turingâ€™s Formula
3) The union of the ï¬rst R partitions, that is,
âˆªRâˆ’1
r=0 {ğ“k âˆ¶Yk = r},
dynamically covers a subset of ğ’³of letters with very low probabilities, or
informally a â€œtailâ€, for any n large or small.
These three points are perhaps better demonstrated via an example. Consider
a special case of a discrete distribution with {pk} following a discrete power law,
known as the Pareto law, in the tail, that is,
pk = Ckâˆ’ğœ†
(1.29)
for all k > k0 where C > 0 and ğœ†> 1 are unknown parameters describing the
tail of the probability distribution beyond an unknown positive integer k0. For
k = 1, â€¦ , k0, the probabilities are nonnegative but are otherwise not speciï¬ed.
This partially parametric probability model is subsequently referred to as the
â€œpower tail modelâ€ or the â€œPareto tail model,â€ for which Condition 1.6 may be
veriï¬ed. Suppose it is of interest to estimate C and ğœ†. An estimation procedure
may be devised based on Theorem 1.12. This problem gets a full treatment in
Chapter 7. However, the following gives an intuitive perspective to the problem,
and the perspective is relevant in much of the development throughout the
subsequent chapters.
Under the model (1.29), ğ’³= {1, 2, â€¦ } and the R groups in the partition
by a sample with observed frequencies, {Y1, Y2, â€¦ }, are subsets of the positive
integers on the real line. Two key issues are to be resolved in estimating C and ğœ†.
1) The basis of a statistical estimation.
2) The issue of an unknown k0.
The basis of a statistical argument is the asymptotic normality of
Theorem 1.12 with each ğœ‹r represented under the semiparametric tail
model, that is, for each r = 1, â€¦ , R
ğœ‹râˆ’1 =
âˆ‘
kâˆ¶Yk=râˆ’1
pk =
âˆ‘
kâˆ¶Yk=râˆ’1
Ckâˆ’ğœ†.
(1.30)
Noting that Tr is observable and ğœ‹râˆ’1 is also an observable function of C and ğœ†,
the asymptotic distribution in Theorem 1.12 has an explicit likelihood function,
which will be referred to as the asymptotic likelihood function and is a function
of the data and the parameters C and ğœ†. Let ( Ì‚C, Ì‚ğœ†) be the maximum likeli-
hood estimator of (C, ğœ†) based on the asymptotic distribution of Theorem 1.12,
which is shown to exist uniquely in Chapter 7 and is referred to thereafter as the
asymptotic maximum likelihood estimator or AMLE. The AMLE is also shown
to be consistent.
Regarding the issue of the unknown k0, since the semiparametric model is
only valid for k > k0, the second equality in (1.30) may not necessarily be true
for some k. However for every r, as n increases indeï¬nitely, the minimum value
of k in the rth partition group, that is, min{k âˆ¶Yk = r âˆ’1} rapidly increases

Turingâ€™s Formula
27
in probability to pass the ï¬xed albeit unknown k0 (see Exercise 18). This argu-
ment essentially validates the second equality in (1.30) and, in turn, validates
the above-mentioned asymptotic argument. The fact that the estimation pro-
cedure does not require a preset threshold for the unknown k0, in contrast to the
required abundance threshold in Hillâ€™s estimator (see Hill (1975)) for continu-
ous random variables, is quite remarkable. In short, by Theorem 1.12, the vector
(T1, â€¦ , TR)ğœmay be viewed as a statistical â€œwindow on the tailâ€ in capturing
a parametric tail behavior of an underlying probability distribution, possibly
beyond data range.
1.4
Turingâ€™s Formula Augmented
For r = 1, â€¦ , n, recall
Nr =
âˆ‘
kâ‰¥1
1[Yk = r]
which is the total number of letters represented exactly r times in the sam-
ple. Chao, Lee, and Chen (1988) proposed the following augmented Turingâ€™s
formula:
Tâ™¯=
n
âˆ‘
r=1
(âˆ’1)r+1 (n
r
)âˆ’1
Nr.
(1.31)
Noting that both n and r are ï¬nite integers,
E (Tâ™¯âˆ’ğœ‹0) =
n
âˆ‘
r=1
(âˆ’1)r+1 (n
r
)âˆ’1
E(Nr) âˆ’E(ğœ‹0)
=
n
âˆ‘
r=1
(âˆ’1)r+1 (n
r
)âˆ’1 (n
r
) âˆ‘
kâ‰¥1
pr
k(1 âˆ’pk)nâˆ’r
âˆ’
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)n
=
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)nâˆ’1 âˆ’
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)n
+
n
âˆ‘
r=2
(âˆ’1)r+1 âˆ‘
kâ‰¥1
pr
k(1 âˆ’pk)nâˆ’r
=
âˆ‘
kâ‰¥1
p2
k(1 âˆ’pk)nâˆ’1 +
n
âˆ‘
r=2
(âˆ’1)r+1 âˆ‘
kâ‰¥1
pr
k(1 âˆ’pk)nâˆ’r
=
âˆ‘
kâ‰¥1
p2
k(1 âˆ’pk)nâˆ’1 âˆ’
âˆ‘
kâ‰¥1
p2
k(1 âˆ’pk)nâˆ’2
+
n
âˆ‘
r=3
(âˆ’1)r+1 âˆ‘
kâ‰¥1
pr
k(1 âˆ’pk)nâˆ’r

28
Statistical Implications of Turingâ€™s Formula
= âˆ’
âˆ‘
kâ‰¥1
p3
k(1 âˆ’pk)nâˆ’2 +
n
âˆ‘
r=3
(âˆ’1)r+1 âˆ‘
kâ‰¥1
pr
k(1 âˆ’pk)nâˆ’r
â‹®
= (âˆ’1)n+1 âˆ‘
kâ‰¥1
pn+1
k
.
The bias of Tâ™¯is therefore, letting pâˆ¨= max{pk; k â‰¥1},
|E (Tâ™¯âˆ’ğœ‹0) | =
âˆ‘
kâ‰¥1
pn+1
k
â‰¤pn
âˆ¨.
(1.32)
It is clear that unless pâˆ¨= 1, that is, {pk} has all of its probability mass on a
single letter, the bias of Tâ™¯decays at least exponentially fast in sample size n.
Theorem 1.13
If the eï¬€ective cardinality of ğ’³is greater than or equal to three,
that is, K â‰¥3, then the augmented Turingâ€™s formula Tâ™¯is asymptotically and
relatively unbiased.
Proof: To be instructive, consider ï¬rst the case that pâˆ¨< 1âˆ•2, which implies
that pâˆ¨âˆ•(1 âˆ’pâˆ¨) âˆˆ(0, 1). By Deï¬nition 1.1, as n â†’âˆ,
|E (Tâ™¯âˆ’ğœ‹0) |
E(ğœ‹0)
=
âˆ‘
kâ‰¥1 pn+1
k
âˆ‘
kâ‰¥1 pk(1 âˆ’pk)n â‰¤
(
pâˆ¨
1 âˆ’pâˆ¨
)n
â†’0.
If pâˆ¨â‰¥1âˆ•2, then since K â‰¥3, there must exist a k = kğœsuch that 0 < pkâ€² < 1 âˆ’
pâˆ¨and hence 1 âˆ’pkâ€² > pâˆ¨, that is, pâˆ¨âˆ•(1 âˆ’pkâ€²) < 1. On the other hand,
|E (Tâ™¯âˆ’ğœ‹0) |
E(ğœ‹0)
=
âˆ‘
kâ‰¥1 pn+1
k
âˆ‘
kâ‰¥1 pk(1 âˆ’pk)n â‰¤
pn
âˆ¨
pkâ€²(1 âˆ’pkâ€²)n
= 1
pkâ€²
(
pâˆ¨
1 âˆ’pkâ€²
)n
â†’0.
â—½
Theorem 1.13 suggests that the augmented Turingâ€™s formula is of some
material improvement to Turingâ€™s formula, particularly in view of Example 1.1.
In addition, the augmented Turingâ€™s formula Tâ™¯also admits asymptotic
normality.
Theorem 1.14
Let Tâ™¯be as in (1.31). If the probability distribution {pk; k â‰¥1}
satisï¬es Condition 1.1, then
n (Tâ™¯âˆ’ğœ‹0)
âˆš
E(N1) + 2E(N2)
L
âˆ’âˆ’âˆ’â†’N(0, 1).
(1.33)
To prove Theorem 1.14, the following two lemmas are useful.

Turingâ€™s Formula
29
Lemma 1.12
For each integer n > 0, let Xn be a nonnegative random variable
with ï¬nite mean mn = E(Xn). If limnâ†’âˆmn = 0, then Xn
pâ†’0.
The proof is left as an exercise (see Exercise 9).
Lemma 1.13
For any constant ğ›¿âˆˆ(0, 1) and a probability sequence {pk;
k â‰¥1}, as n â†’âˆ,
1. nğ›¿âˆ‘
kâ‰¥1
p2
k(1 âˆ’pk)n â†’0
and
2. nğ›¿âˆ‘
kâˆˆS
p2
k(1 âˆ’pk)n â†’0
where S is any subset of {k; k â‰¥1}.
Proof: For Part 1, since ğ›¿âˆˆ(0, 1), there exists ğ›¿1 âˆˆ(ğ›¿, 1). Let
I = {k âˆ¶pk < 1âˆ•nğ›¿1}
and
Ic = {k âˆ¶pk â‰¥1âˆ•nğ›¿1} .
0 â‰¤nğ›¿âˆ‘
kâ‰¥1
p2
k(1 âˆ’pk)n
= nğ›¿âˆ‘
kâˆˆIc
p2
k(1 âˆ’pk)n + nğ›¿âˆ‘
kâˆˆI
p2
k(1 âˆ’pk)n
â‰¤nğ›¿âˆ‘
kâˆˆIc
pk (1 âˆ’1âˆ•nğ›¿1)n + nğ›¿âˆ‘
kâˆˆI
pkâˆ•nğ›¿1
â‰¤nğ›¿(1 âˆ’1âˆ•nğ›¿1)nğ›¿1+(1âˆ’ğ›¿1) + nğ›¿âˆ•nğ›¿1
= nğ›¿[ (1 âˆ’1âˆ•nğ›¿1)nğ›¿1]n1âˆ’ğ›¿1 + nâˆ’(ğ›¿1âˆ’ğ›¿) â†’0.
Part 1 implies Part 2.
â—½
Proof of Theorem 1.14: Let ğœn =
âˆš
E(N1) + 2E(N2). By Slutskyâ€™s theorem, the
objective is to show
n(Tâ™¯âˆ’ğœ‹0)
ğœn
âˆ’n(T1 âˆ’ğœ‹0)
ğœn
p
âˆ’âˆ’âˆ’â†’0.
(1.34)
Note ï¬rst that by Condition 1.1,
ğœn
n2ğ›¿=
âˆš
E(N1) + 2E(N2)
n4ğ›¿
â†’c1 + c2 > 0.
Note second that
n(Tâ™¯âˆ’ğœ‹0)
ğœn
âˆ’n(T1 âˆ’ğœ‹0)
ğœn
= n
ğœn
(Tâ™¯âˆ’T1)
= n
ğœn
[ n
âˆ‘
r=2
(âˆ’1)r+1 (n
r
)âˆ’1
Nr
]
=
(
n2ğ›¿
ğœn
)
n1âˆ’2ğ›¿
[ n
âˆ‘
r=2
(âˆ’1)r+1 (n
r
)âˆ’1
Nr
]
.

30
Statistical Implications of Turingâ€™s Formula
Therefore, (1.34) holds if and only if
n1âˆ’2ğ›¿
[ n
âˆ‘
r=2
(âˆ’1)r+1 (n
r
)âˆ’1
Nr
]
p
âˆ’âˆ’âˆ’â†’0.
(1.35)
Separating the âˆ‘n
r=2 in (1.35) into two parts, âˆ‘
odd and âˆ‘
even, where âˆ‘
odd sums
over all odd values of r and âˆ‘
even sums over all even values of r in {2, â€¦ , n},
the left-hand side of (1.35) may be re-expressed as
an âˆ’bn âˆ¶= n1âˆ’2ğ›¿âˆ‘
odd
(n
r
)âˆ’1
Nr âˆ’n1âˆ’2ğ›¿âˆ‘
even
(n
r
)âˆ’1
Nr.
Now the objective becomes to show, respectively,
an
p
âˆ’âˆ’âˆ’â†’0
and
bn
p
âˆ’âˆ’âˆ’â†’0.
However, since both an and bn are nonnegative, it suï¬ƒces to show that
an + bn
pâ†’0, that is,
an + bn = n1âˆ’2ğ›¿
n
âˆ‘
r=2
(n
r
)âˆ’1
Nr
p
âˆ’âˆ’âˆ’â†’0.
By Lemma 1.12, it suï¬ƒces to show
E(an + bn) = n1âˆ’2ğ›¿
n
âˆ‘
r=2
âˆ‘
kâ‰¥1
pr
k(1 âˆ’pk)nâˆ’r âˆ’âˆ’âˆ’â†’0.
Let ğ›¿âˆ—= 1 âˆ’2ğ›¿,
I1 = {kâˆ¶pk = 1âˆ•2},
I2 = {kâˆ¶pk > 1âˆ•2},
I3 = {kâˆ¶pk < 1âˆ•2}.
0 â‰¤nğ›¿âˆ—
n
âˆ‘
r=2
âˆ‘
kâ‰¥1
pr
k(1 âˆ’pk)nâˆ’r
= nğ›¿âˆ—
n
âˆ‘
r=2
(
âˆ‘
kâˆˆI1
+
âˆ‘
kâˆˆI2
+
âˆ‘
kâˆˆI3
)
pr
k(1 âˆ’pk)nâˆ’r
=âˆ¶d1 + d2 + d3.
In the following, it remains to show that di â†’0, for each i âˆˆ{1, 2, 3},
respectively.
If I1 is empty, then d1 = 0. If not, then for each index k âˆˆI1,
nğ›¿âˆ—
n
âˆ‘
r=2
pr
k(1 âˆ’pk)nâˆ’r = nğ›¿âˆ—
n
âˆ‘
r=2
(1âˆ•2)n = nğ›¿âˆ—(n âˆ’1)(1âˆ•2)n â†’0.
Since there are at most two indices in I1, d1 â†’0.

Turingâ€™s Formula
31
If I2 is empty, then d2 = 0. If not, then for each index k âˆˆI2,
nğ›¿âˆ—
n
âˆ‘
r=2
pr
k(1 âˆ’pk)nâˆ’r = nğ›¿âˆ—
[ n
âˆ‘
r=2
(
pk
1 âˆ’pk
)r]
(1 âˆ’pk)n
= nğ›¿âˆ—
â§
âª
â¨
âªâ©
(
pk
1 âˆ’pk
)2 â¡
â¢
â¢
â¢â£
1 âˆ’
(
pk
1âˆ’pk
)nâˆ’1
1 âˆ’
pk
1âˆ’pk
â¤
â¥
â¥
â¥â¦
â«
âª
â¬
âªâ­
Ã— (1 âˆ’pk)n
= nğ›¿âˆ—
[
p2
k(1 âˆ’pk)nâˆ’1
1 âˆ’2pk
] [
1 âˆ’
pnâˆ’1
k
(1 âˆ’pk)nâˆ’1
]
= nğ›¿âˆ—
[
p2
k(1 âˆ’pk)nâˆ’1
1 âˆ’2pk
]
âˆ’nğ›¿âˆ—
[
p2
k(1 âˆ’pk)nâˆ’1
1 âˆ’2pk
]
pnâˆ’1
k
(1 âˆ’pk)nâˆ’1
= nğ›¿âˆ—
[
p2
k(1 âˆ’pk)nâˆ’1
1 âˆ’2pk
]
âˆ’nğ›¿âˆ—
(
pn+1
k
1 âˆ’2pk
)
â†’0.
Since there is at most one index in I2, d2 â†’0.
For d3, using the same argument as that for d2 earlier,
d3 = nğ›¿âˆ—âˆ‘
kâˆˆI3
p2
k(1 âˆ’pk)nâˆ’1(1 âˆ’2pk)âˆ’1 âˆ’nğ›¿âˆ—âˆ‘
kâˆˆI3
pn+1
k
(1 âˆ’2pk)âˆ’1
=âˆ¶d31 âˆ’d32.
It suï¬ƒces to show d31 â†’0 and d32 â†’0. Let pâˆ¨= max{pk; k âˆˆI3}. Then
pâˆ¨< 1âˆ•2 and
d31 = nğ›¿âˆ—âˆ‘
kâˆˆI3
p2
k(1 âˆ’pk)nâˆ’1(1 âˆ’2pk)âˆ’1
â‰¤nğ›¿âˆ—(1 âˆ’pâˆ¨)âˆ’1(1 âˆ’2pâˆ¨)âˆ’1 âˆ‘
kâˆˆI3
p2
k(1 âˆ’pk)n â†’0
by Part 2 of Lemma 1.13. Finally,
d32 = nğ›¿âˆ—âˆ‘
kâˆˆI3
pn+1
k
(1 âˆ’2pk)âˆ’1 â‰¤nğ›¿âˆ—(1 âˆ’2pâˆ¨)âˆ’1(1âˆ•2)n â†’0.
â—½
The consistency of Lemma 1.5 immediately leads to the following practically
useful statement.

32
Statistical Implications of Turingâ€™s Formula
Theorem 1.15
Let Tâ™¯be as in (1.31). Under Condition 1.1,
n (Tâ™¯âˆ’ğœ‹0)
âˆš
N1 + 2N2
L
âˆ’âˆ’âˆ’â†’N(0, 1).
(1.36)
Example 1.10
Construct a 95% conï¬dence interval for ğœ‹0 using Theorem 1.15
and the bird data in Table 1.2.
Solution: There are only a few values of r for which Nr is nonzero in the data set
and they are as follows:
r
1
2
6
30
50
70
80
100
200
300
Nr
12
1
1
1
2
1
1
7
2
2
By the sheer magnitude of
(
n
r
)
, and hence the insigniï¬cance of
(
n
r
)âˆ’1
, with
large values of r, the additive terms in (1.31) do not register but in a few terms.
Consequently, Tâ™¯= 0.00599, and by Theorem 1.15 and Example 1.6, a 95% con-
ï¬dence interval for ğœ‹0 is
0.0059995 Â± 0.0037 = (0.0022995, 0.0096995).
One drawback of Tâ™¯is that it may theoretically assume a negative value
though practically impossible with a suï¬ƒciently large n. One may wish to
further augment Tâ™¯by means of, for example,
Tâ™¯â™¯= max {Tâ™¯, 0} .
(1.37)
It can be veriï¬ed that Tâ™¯â™¯also admits asymptotic normality of Theorems 1.14
and 1.15 (see Exercise 19).
In the same spirit of (1.31), the rth order Turingâ€™s formula may also
be augmented similarly. Noting for each integer r, 1 â‰¤r â‰¤n âˆ’1, ğœ‹râˆ’1 =
âˆ‘
kâ‰¥1 pk1[Yk = r âˆ’1],
E
( (
n
r âˆ’1
)âˆ’1
ğœ‹râˆ’1
)
=
âˆ‘
kâ‰¥1
pr
k(1 âˆ’pk)nâˆ’(râˆ’1)
=
âˆ‘
kâ‰¥1
pr
k(1 âˆ’pk)nâˆ’r âˆ’
âˆ‘
kâ‰¥1
pr+1
k
(1 âˆ’pk)nâˆ’r
=
âˆ‘
kâ‰¥1
pr
k(1 âˆ’pk)nâˆ’r âˆ’
âˆ‘
kâ‰¥1
pr+1
k
(1 âˆ’pk)nâˆ’râˆ’1 + Â· Â· Â·
+ (âˆ’1)nâˆ’r+1 âˆ‘
kâ‰¥1
pn+1
k
,
and
E
( (n
r
)âˆ’1
Nr
)
=
âˆ‘
kâ‰¥1
pr
k(1 âˆ’pk)nâˆ’r,

Turingâ€™s Formula
33
it may be veriï¬ed that the augmented rth order Turingâ€™s formula,
Tâ™¯
r =
(
n
r âˆ’1
)
n
âˆ‘
m=r
(âˆ’1)mâˆ’r ( n
m
)âˆ’1
Nm
(1.38)
has bias
|E(Tâ™¯
r âˆ’ğœ‹râˆ’1)| =
(
n
r âˆ’1
) âˆ‘
kâ‰¥1
pn+1
k
â‰¤
(
n
r âˆ’1
)
pn
âˆ¨= îˆ»(nrâˆ’1pn
âˆ¨) ,
where pâˆ¨= max{pk; k â‰¥1}, decaying rapidly when r is a small ï¬xed integer.
1.5
Goodness-of-Fit by Counting Zeros
Turingâ€™s formula is mostly about sorting letters, observed in an iid sample of
size n, into equal frequency groups and counting the diï¬€erent letters in each
group, namely Nr, r = 1, â€¦ , n. In the special case of a ï¬nite and known K,
N0 =
âˆ‘
kâ‰¥1
1[Yk = 0]
is also observable. The distribution of Nr, where r = 0, 1, â€¦ , n, is discussed by
many in more detail under the topic of occupancy problems. Both Johnson and
Kotz (1977) and Kolchin, Sevastyanov, and Chistyakov (1978) oï¬€er an excellent
coverage of the topic.
In an experiment of randomly allocating n balls into K < âˆbaskets according
to a distribution p = {pk; k = 1, â€¦ , K}, N0 is the number of empty or unoccu-
pied baskets among a total of K baskets. When N0 is observable, it has a quite
interesting statistical feature as stated in Theorem 1.16.
Theorem 1.16
For a ï¬nite alphabet ğ’³= {ğ“1, â€¦ , ğ“K} where K â‰¥2, let îˆ¼K
denote the collection of all possible probability distributions on ğ’³. N0 satisï¬es
min{Ep(N0) âˆ¶p âˆˆîˆ¼K} = K
(
1 âˆ’1
K
)n
,
(1.39)
and Ep(N0) attains its minimum value at p being uniform, that is, pk = 1âˆ•K for
every k = 1, â€¦ , K.
Proof: Since
Ep(N0) =
K
âˆ‘
k=1
E(1[Yk = 0]) =
K
âˆ‘
k=1
(1 âˆ’pk)n,
(1.40)
it suï¬ƒces to show that for any pair of probabilities, denoted by p1 and p2 (with-
out loss of generality), if p1 â‰ p2, then there would exist a pâˆ—âˆˆîˆ¼K such that
Epâˆ—(N0) > Ep(N0). Toward that end, let
g(x1, x2) = (1 âˆ’x1)n + (1 âˆ’x2)n

34
Statistical Implications of Turingâ€™s Formula
and consider the behavior of
g(x1, s âˆ’x1) = (1 âˆ’x1)n + [1 âˆ’(s âˆ’x1)]n
for x1 âˆˆ[0, s], where s = p1 + p2 is held ï¬xed:
dg(x1, s âˆ’x1)
dx1
= âˆ’n(1 âˆ’x1)nâˆ’1 + n(1 âˆ’s + x1)nâˆ’1.
Since
dg(x1, s âˆ’x1)
dx1
||||x1=0
= âˆ’n + n(1 âˆ’s)nâˆ’1 â‰¤0,
dg(x1, s âˆ’x1)
dx1
||||x1=s
= âˆ’n(1 âˆ’s)nâˆ’1 + n â‰¥0, and
dg(x1, s âˆ’x1)
dx1
= 0 if and only if x1 = s
2,
it is then clear that g(x1, x2) = (1 âˆ’x1)n + (1 âˆ’x2)n, subject to the constraint
x1 + x2 = p1 + p2, reaches its minimum at x1 = x2 = (p1 + p2)âˆ•2. This implies
Epâˆ—(N0) < Ep(N0) where
pâˆ—=
{p1 + p2
2
, p1 + p2
2
, p3, â€¦ , pK
}
.
Therefore, Ep(N0) is not minimized unless pk = 1âˆ•K for every k, 1 â‰¤k â‰¤K, in
which case, it takes on the value of K(1 âˆ’1âˆ•K)n.
â—½
Theorem 1.16 suggests that, under the assumption of independent alloca-
tions, the uniform distribution would lead to the least expected number of
empty baskets. Any other distribution would have a tendency for a larger num-
ber of empty baskets. If an observed N0 diï¬€ers signiï¬cantly from K(1 âˆ’1âˆ•K)n,
then either the uniformity assumption or the independence assumption, or
both, would become questionable. This feature provides an interesting perspec-
tive to the statistical question of â€œgoodness-of-ï¬t.â€
For any p = {pk; k = 1, â€¦ , K} âˆˆîˆ¼K, in addition to Ep(N0) given in (1.40),
Var(N0) = E (N2
0) âˆ’(E(N0))2
= E
â›
âœ
âœâ
(
âˆ‘
1â‰¤kâ‰¤K
1[Yk = 0]
)2â
âŸ
âŸâ 
âˆ’(E(N0))2
= E
(
âˆ‘
1â‰¤kâ‰¤K
1[Yk = 0]
+
âˆ‘
iâ‰ j;i,j=1, â€¦ ,K
1[Yi = 0]1[Yj = 0]
)
âˆ’(E(N0))2

Turingâ€™s Formula
35
=
âˆ‘
1â‰¤kâ‰¤K
(1 âˆ’pk)n +
âˆ‘
iâ‰ j;i,j=1, â€¦ ,K
(1 âˆ’pi âˆ’pj)n âˆ’(E(N0))2.
(1.41)
Under the assumption of the uniform distribution, that is, pk = 1âˆ•K for every
k, 1 â‰¤k â‰¤K, (1.40) and (1.41) become
E(N0) = K
(
1 âˆ’1
K
)n
,
(1.42)
Var(N0) = K
(
1 âˆ’1
K
)n
+ K(K âˆ’1)
(
1 âˆ’2
K
)n
âˆ’K2 (
1 âˆ’1
K
)2n
. (1.43)
Further suppose n = ğœ†K for some constant ğœ†> 0, then (1.42) and (1.43)
become
E(N0) = K
(
1 âˆ’1
K
)ğœ†K
,
(1.44)
Var(N0) = K
(
1 âˆ’1
K
)ğœ†K
+ K(K âˆ’1)
(
1 âˆ’2
K
)ğœ†K
âˆ’K2 (
1 âˆ’1
K
)2ğœ†K
.
(1.45)
More speciï¬cally, when ğœ†= 1 or K = n, (1.42) and (1.45) become
E(N0) = K
(
1 âˆ’1
K
)K
,
(1.46)
Var(N0) = K
(
1 âˆ’1
K
)K
+ K(K âˆ’1)
(
1 âˆ’2
K
)K
âˆ’K2 (
1 âˆ’1
K
)2K
.
(1.47)
For any suï¬ƒciently large K, (1.42) and (1.47) become
E(N0) â‰ˆKeâˆ’1,
(1.48)
Var(N0) â‰ˆK (eâˆ’1 âˆ’eâˆ’2) .
(1.49)
Let
P0 = N0
K ,
(1.50)
that is, the proportion of baskets not occupied in an experiment of randomly
allocating n = K balls into K baskets, or more generally, the proportion of
letters in ğ’³not observed in an iid sample under the uniform distribution,
pk = 1âˆ•K. By (1.46)â€“(1.49),
E(P0) =
(
1 âˆ’1
K
)K
â‰ˆ1âˆ•e,
(1.51)
Var(P0) = Var(N0)
K2
= 1
K
(
1 âˆ’1
K
)K
+ K âˆ’1
K
(
1 âˆ’2
K
)K
âˆ’
(
1 âˆ’1
K
)2K
â‰ˆe âˆ’1
Ke2 .
(1.52)

36
Statistical Implications of Turingâ€™s Formula
At this point, an interesting observation can be made. If n = 100 balls are ran-
domly allocated into K = 100 baskets, one should expect to see approximately
100eâˆ’1 â‰ˆ36.78% Ã— 100 â‰ˆ37 empty baskets. The expected number of empty
baskets in this experiment at 37 is counterintuitively high for most minds. The
most common intuitive guess of E(N0) is 25, which is signiï¬cantly below 37.
It is perhaps more interesting to contemplate why an intuitive mind would
arrive at a number so much lower than 37. An intuitive mind would try to
balance the extreme event of a â€œperfectly uniform allocationâ€ of one ball in
each basket (with probability P(N0 = 0) = K!âˆ•KK) and another extreme event
of a â€œcompletely skewed allocationâ€ of all balls in one basket (with probability
P(N0 = K âˆ’1) = Kâˆ•KK). The fact that the ï¬rst event is so much more likely
than the second event would pull an intuitive guess of E(N0) toward the lower
end. Even without the notion of likelihoods of these events, the event of N0 = 0
conforms much better to the notion of â€œuniformly distributed allocationâ€ than
that of N0 = 99. Nevertheless, the fact remains that E(N0) â‰ˆ37.
In fact, in terms of proportion, that is, P0 = N0âˆ•K, the above-observed sta-
tistical feature becomes more pronounced since, as K â†’âˆ
E(P0) â†’eâˆ’1,
ğœP0 â‰ƒ
âˆš
e âˆ’1
e
âˆš
K
â†’0.
A goodness-of-ï¬t statistical test may be devised based on the above-observed
feature.
Given two positive integers, n and m, satisfying n â‰¥m, the Stirling number of
the second kind is the number of ways of partitioning a set of n elements into
m nonempty sets. For example, n = 3 elements in the set {ğ“1, ğ“2, ğ“3} can be
partitioned into m = 3 subsets in one way, that is, {{ğ“1}, {ğ“2}, {ğ“3}}; the same
n = 3 elements can be partitioned into m = 2 subsets in three ways, that is,
{{ğ“1, ğ“2}, {ğ“3}},
{{ğ“1, ğ“3}, {ğ“2}},
{{ğ“2, ğ“3}, {ğ“1}};
and the same n = 3 elements can be partitioned into m = 1 subset in 1 way,
that is, {{ğ“2, ğ“2, ğ“3}}. In general, the Stirling number of the second kind is often
denoted and computed by
S(n, m) = 1
m!
m
âˆ‘
i=0
(âˆ’1)i (m
i
)
(m âˆ’i)n.
(1.53)
For a more detailed discussion, interested readers may refer to Stanley (1997).
In an experiment of randomly allocating n balls into K baskets, let x be the
number of empty baskets. Subject to 1 â‰¤K âˆ’x â‰¤n,
1) there are a total of Kn ways to place n balls in K baskets;
2) there are
(
K
Kâˆ’x
)
(K âˆ’x)! ways to select and to arrange the ï¬lled baskets;
and

Turingâ€™s Formula
37
3) there are S(n, K âˆ’x) ways to completely occupy the selected K âˆ’x ï¬lled bas-
kets with n balls.
Therefore, under an iid allocation with the uniform distribution pk = 1âˆ•K for
k = 1, â€¦ , K, the probability of observing exactly x empty baskets is
P(N0 = x) =
(
K
Kâˆ’x
)
(K âˆ’x)!S(n, K âˆ’x)
Kn
.
(1.54)
Simplifying (1.54) leads to
P(N0 = x) =
K!
x!(K âˆ’x)!Kn
Kâˆ’x
âˆ‘
i=0
(âˆ’1)i (K âˆ’x
i
)
(K âˆ’x âˆ’i)n
=
K!
x!(K âˆ’x)!Kn
Kâˆ’x
âˆ‘
i=0
(âˆ’1)i
(K âˆ’x)!
i!(K âˆ’x âˆ’i)!(K âˆ’x âˆ’i)n
=
Kâˆ’x
âˆ‘
i=0
(âˆ’1)i
K!
x!i!(K âˆ’x âˆ’i)!
(
1 âˆ’x + i
K
)n
.
(1.55)
For simplicity in demonstration, (1.55) is reduced to the case of n = K, that
is,
P(N0 = x) =
Kâˆ’x
âˆ‘
i=0
(âˆ’1)i
K!
x!i!(K âˆ’x âˆ’i)!
(
1 âˆ’x + i
K
)K
.
(1.56)
Table 1.3 is the exact probability distribution of N0 represented by (1.56)
when K = 10. Table 1.4 includes several eï¬€ectively positive terms of the cumu-
lative distribution function (cdf ), whose probability mass function is repre-
sented by (1.56) for K = 100.
However, the numerical evaluation of (1.56) is computationally demanding
for even moderately large values of K.
Theorem 1.16 and the explicit expression of (1.56) together enable a means
of testing the hypothesis that a sample is iid under the uniform distribution.
Example 1.11 below illustrates how a test based on N0 may be used to quantify
statistical evidence against such a hypothesis.
Table 1.3 Exact probabilities of (1.56) for K = n = 10
x
P (N0 = x)
0
0.000362880
1
0.016329600
2
0.136080000
3
0.355622400
4
0.345144240
x
P (N0 = x)
5
0.128595600
6
0.017188920
7
0.000671760
8
0.000004599
9
0.000000001

38
Statistical Implications of Turingâ€™s Formula
Table 1.4 Partial cdf of N0 based on (1.56) for K = n = 100
x
P (N0 = x)
24
0.000038799
25
0.000148654
26
0.000509259
27
0.001564749
28
0.004325109
29
0.010787039
30
0.024349639
31
0.049907939
32
0.093206939
x
P (N0 = x)
35
0.362190439
36
0.487643439
37
0.614165439
38
0.729399439
39
0.824210739
40
0.894694539
41
0.942042739
33
0.159224739
34
0.249901439
x
P (N0 = x)
42
0.970782939
43
0.986544139
44
0.994351189
45
0.997842829
46
0.999252139
47
0.999765198
48
0.999933542
49
0.999983287
50
0.999996511
Example 1.11
The following data set contains n = 100 iid observations from
ğ’³= {ğ“1, â€¦ , ğ“100} under the triangular distribution
p = {pk = kâˆ•5050; k = 1, â€¦ , 100},
summarized by the observed letters and their corresponding frequencies, yk.
letters
ğ“3
ğ“14
ğ“15
ğ“16
ğ“17
ğ“18
ğ“22
ğ“25
ğ“30
ğ“33
ğ“36
ğ“37
yk
1
1
1
1
1
1
2
1
3
1
1
1
letters
ğ“38
ğ“42
ğ“44
ğ“45
ğ“46
ğ“49
ğ“50
ğ“54
ğ“56
ğ“57
ğ“58
ğ“59
yk
2
1
1
1
1
1
2
1
1
1
2
2
letters
ğ“61
ğ“64
ğ“65
ğ“66
ğ“68
ğ“69
ğ“70
ğ“71
ğ“72
ğ“73
ğ“74
ğ“75
yk
1
1
1
1
5
1
1
2
3
1
1
2
letters
ğ“76
ğ“77
ğ“78
ğ“79
ğ“81
ğ“82
ğ“83
ğ“85
ğ“86
ğ“87
ğ“88
ğ“89
yk
3
2
1
2
2
2
3
2
3
4
1
3
letters
ğ“90
ğ“91
ğ“92
ğ“93
ğ“94
ğ“95
ğ“96
ğ“97
ğ“98
yk
2
1
2
2
1
3
2
2
6
Using the distribution of Table 1.4 to ï¬nd the p-value in testing, the hypothesis
H0: p is uniform on ğ’³.
Solution: Since it is known that K = 100, the number of missing letters in the
sample is n0 = 100 âˆ’âˆ‘100
k=1 1[yk â‰ 0] = 100 âˆ’57 = 43. By Theorem 1.16, this is
a one-sided test, and therefore
the p-value = P(N0 â‰¥43|H0) = 0.0135.

Turingâ€™s Formula
39
There is moderately strong statistical evidence against H0 of the uniform
distribution.
The deviation quantiï¬ed in Example 1.11 is due to the diï¬€erence between the
true distribution p (the triangular distribution) and the hypothesized uniform
distribution. The test could also be used to detect deviation from the assump-
tion of independence between observations.
Consider a special Markov chain with memory m, that is,
X0, X1, X2, â€¦ , Xtâˆ’2, Xtâˆ’1, Xt, â€¦ ,
(1.57)
such that
1) X0 is a random element on the state space ğ’³= {ğ“1, â€¦ , ğ“K}, with the uni-
form distribution, that is, P(X0 = ğ“k) = 1âˆ•K, for every k, 1 â‰¤k â‰¤K;
2) Xt, t â‰¤m, is a random element on the state space ğ’³= {ğ“1, â€¦ , ğ“K}, such
that, given Xtâˆ’1 = xtâˆ’1, â€¦ , X0 = x0, Xt is only allowed to visit the states,
which have not been visited by any of X0 to Xtâˆ’1, with equal probabilities;
3) Xt, t > m, is a random element on the state space ğ’³= {ğ“1, â€¦ , ğ“K}, such
that, given Xtâˆ’1 = xtâˆ’1, â€¦ , Xtâˆ’m = xtâˆ’m, Xt is only allowed to visit the states,
which have not been visited by any of Xtâˆ’1 to Xtâˆ’m, with equal probabilities.
A simple argument based on symmetry would establish that, for every t, Xt
is unconditionally a uniformly distributed random element on ğ’³.
The following table includes a segment of n = 100 consecutive observations
of a randomly generated sequence of Xt according to the above-described spe-
cial Markov chain with memory m = 10 and K = 100.
ğ“90
ğ“59
ğ“63
ğ“26
ğ“40
ğ“72
ğ“36
ğ“11
ğ“68
ğ“67
ğ“29
ğ“82
ğ“30
ğ“62
ğ“23
ğ“35
ğ“02
ğ“22
ğ“58
ğ“69
ğ“67
ğ“93
ğ“56
ğ“11
ğ“42
ğ“29
ğ“73
ğ“21
ğ“19
ğ“84
ğ“37
ğ“98
ğ“24
ğ“15
ğ“70
ğ“13
ğ“26
ğ“91
ğ“80
ğ“56
ğ“73
ğ“62
ğ“96
ğ“81
ğ“05
ğ“25
ğ“84
ğ“27
ğ“36
ğ“46
ğ“29
ğ“13
ğ“57
ğ“24
ğ“95
ğ“82
ğ“45
ğ“14
ğ“67
ğ“34
ğ“64
ğ“43
ğ“50
ğ“87
ğ“08
ğ“76
ğ“78
ğ“88
ğ“84
ğ“03
ğ“51
ğ“54
ğ“99
ğ“32
ğ“60
ğ“68
ğ“39
ğ“12
ğ“26
ğ“86
ğ“94
ğ“95
ğ“70
ğ“34
ğ“78
ğ“67
ğ“01
ğ“97
ğ“02
ğ“17
ğ“92
ğ“52
ğ“56
ğ“80
ğ“86
ğ“41
ğ“65
ğ“89
ğ“44
ğ“19
Example 1.12
In the above-described Markov chain with memory m, suppose
it is known a priori that Xt for every t is unconditionally a uniformly distributed
random element on ğ’³and it is of interest to test the hypothesis, H0 âˆ¶m = 0. Use
the above-mentioned data to ï¬nd the appropriate p-value for the test.

40
Statistical Implications of Turingâ€™s Formula
Solution: Counting the number of missing letters in the data set gives n0 = 27.
Since any positive value of m would tend to enlarge the coverage of ğ’³and hence
to shrink N0, a one-sided test is appropriate and correspondingly, by Table 1.4,
the p-value = P(N0 â‰¤27|H0) = 0.0016.
There is strong statistical evidence in the data against H0.
It is sometime conjectured that the digits of ğœ‹= 3.14159265358 â€¦ in
nonoverlapping d-digit fragments, where d is any given positive integer,
imitate well a sequence of iid uniform random variables on alphabet that
contains all nonnegative integers from 0 to âˆ‘dâˆ’1
i=0 9 Ã— 10i, that is, a number
consisted of d nines. For example, if d = 1, then any single digit would be one
of the 10 digits, ranging from 0 to 9. For another example, if d = 3, then any
consecutive three digits would be an integer ranging from 0 to 999.
Example 1.13
Use the sequence of the ï¬rst one billion digits after the deci-
mal point in ğœ‹to evaluate the resemblance of this sequence to one with a bil-
lion iid uniform random digits from alphabet ğ’³= {0, 1, â€¦ , 9} by Pearsonâ€™s
goodness-of-ï¬t test statistic.
Solution: There are at least two perspectives in which the resemblance to uniform
random digits can be quantiï¬ed. The ï¬rst is a straightforward goodness-of-ï¬t test
for the distribution of P(0) = P(1) = Â· Â· Â· = P(9) = 1âˆ•10. This test may be thought
of as an â€œevennessâ€ test, as it directly tests the equality in proportionality of the
10 digits. The second is a goodness-of-ï¬t test for the distribution in Table 1.3. This
test may be thought of as an â€œanti-evennessâ€ test as it tests a particular pattern
in Table 1.3, which deviates signiï¬cantly from the â€œperfectâ€ case of evenness at
n0 = 0.
In the ï¬rst case, the sequence of the ï¬rst billion digits after the decimal point
is hypothesized to be a realization of a sample of n = 1 billion iid observations
under the hypothesized uniform distribution, that is, H0: P(x) = 1âˆ•10 for x =
0, â€¦ , 9. In the following, the observed and expected frequencies, Ox and Ex, are
given.
n0 = x
Observed frequency (Ox)
Expected frequency (Ex)
0
99 993 942
100 000 000.0
1
99 997 334
100 000 000.0
2
100 002 410
100 000 000.0
3
99 986 911
100 000 000.0
4
100 011 958
100 000 000.0
5
99 998 885
100 000 000.0
6
100 010 387
100 000 000.0
7
99 996 061
100 000 000.0
8
100 001 839
100 000 000.0
9
100 000 273
100 000 000.0

Turingâ€™s Formula
41
Pearsonâ€™s chi-squared test statistic is
ğœ’2 =
9
âˆ‘
x=0
(Ox âˆ’Ex)2
Ex
= 4.92
with degrees of freedom 9. The corresponding p-value is P(ğœ’2 â‰¥4.92) = 0.8412
where ğœ’2 is a chi-squared random variable with degrees of freedom 9. There is
no statistical evidence in the sample against H0.
In the second case, the sequence of the ï¬rst billion digits after the decimal point
is viewed as a sequence of consecutive fragments or samples of size 10 digits, that
is,
{1, 4, 1, 5, 9, 2, 6, 5, 3, 5}, {8, 9, 7, 9, 3, 2, 3, 8, 4, 6}, â€¦ .
For each sample of size n = 10, the number of unobserved digits n0 is noted,
resulting in the following observed/expected frequency table, where the expected
frequencies are based on the distribution of Table 1.3.
n0 = x
Observed frequency (Ox)
Expected frequency (Ex)
0
36 270
36 288.0
1
1 633 243
1 632 960.0
2
13 609 335
13 608 000.0
3
35 556 620
35 562 240.0
4
34 518 949
34 514 424.0
5
12 859 799
12 859 560.0
6
1 718 714
1 718 892.0
7
66 617
67 176.0
8
453
459.9
9
0
0.1
Combining the frequencies of the last two categories, n0 = 8 and n0 = 9, Pear-
sonâ€™s chi-squared test statistic is
ğœ’2 =
8âˆ—
âˆ‘
x=0
(Ox âˆ’Ex)2
Ex
= 6.45
where O8âˆ—= 453 and E8âˆ—= 460, with degrees of freedom 8. The corresponding
p-value is P(ğœ’2 â‰¥645) = 0.5971 where ğœ’2 is a chi-squared random variable
with degrees of freedom 8. There is no statistical evidence in the sample
against H0.
In summary, the ï¬rst one billion decimal digits of ğœ‹seem to resemble well a
randomly generated iid digits from 0 to 9 under the uniform distribution.
Since by (1.51) and (1.52), E(P0) and Var(P0) are easily and precisely calcu-
lated, a large sample test may be devised for the hypothesis, H0: the data are iid

42
Statistical Implications of Turingâ€™s Formula
observations under the uniform distribution on ğ’³. Consider the experiment
in which a sample of n iid uniform random elements is drawn from an alphabet
with K letters and the proportion of empty baskets, P0 = N0âˆ•K, is noted. Sup-
pose such an experiment is repeated m times, resulting in, P0,1, â€¦ , P0,m, and
denote the sample mean as P0. Then by the central limit theorem, provided that
m is suï¬ƒciently large,
Z =
âˆš
m
[
P0 âˆ’
(
1 âˆ’1
K
)K]
âˆš
1
K
(
1 âˆ’1
K
)K
+ Kâˆ’1
K
(
1 âˆ’2
K
)K
âˆ’
(
1 âˆ’1
K
)2K
(1.58)
converges weakly to a standard normal random variable under H0.
Example 1.14
Use (1.58) to test the hypothesis, H0, that the sequence of the
three-digit nonoverlapping segments starting from the ï¬rst digit after the deci-
mal point in ğœ‹, that is,
141,592, 653,589, 793,238, 462,643, 383,279, â€¦
is a sequence of iid observations from alphabet ğ’³= {0, 1, â€¦ , 999} under the
uniform distribution.
Solution: This is a case corresponding to K = 1000. The ï¬rst sample of size n =
1000 consists of the ï¬rst 3000 digits of ğœ‹after the decimal point, breaking into
1000 nonoverlapping three-digit fragments. For this sample of size n = 1000,
P0 = N0âˆ•K is calculated. The next 3000 digits gave a second sample of size 1000,
for which P0 = N0âˆ•K is calculated. The step is repeated m = 300000 times, and
the mean of these P0s is calculated and denoted by P0. Finally, (1.58) takes a
value Z = 0.0036, which indicates no evidence against H0.
The idea of a goodness-of-ï¬t test based on N0 was perhaps ï¬rst suggested by
David (1950). There it is proposed that, for the hypothesis H0 that an iid sample
is taken from a population with known cdf F(x), K points z0 = âˆ’âˆ< z1 < Â· Â· Â· <
zKâˆ’1 < zK = âˆcan be chosen such that F(zk) âˆ’F(zkâˆ’1) = 1âˆ•K, and that the test
rejects H0 if the p-value P(N0 â‰¥n0|H0), where n0 is the observed number of
intervals (zkâˆ’1, zk) containing no observation, is less than a preï¬xed ğ›¼âˆˆ(0, 1).
1.6
Remarks
Turingâ€™s formula is often perceived to project a counterintuitive impression.
This impression is possibly linked to the implied utility of Turingâ€™s formula in
making nonparametric inferences regarding certain distributional character-
istics beyond the data range. To better discern the spirit of Turingâ€™s formula,
it is perhaps helpful to, at least pedagogically, consider the formula in several
diï¬€erent perspectives.

Turingâ€™s Formula
43
To begin, one may be beneï¬ted by a brief revisit to a dilemma in the practice of
statistical estimation. Primarily consider a simple example in which the popu-
lation is assumed to be N(ğœ‡, ğœ2) with an unknown ğœ‡and a known ğœ2 > 0. An iid
sample of size n, X = {X1, â€¦ , Xn}, is to be taken from the population, and the
objective is to estimate the unknown population mean ğœ‡based on the sample.
In the realm of Statistics, this process is carried out in two stages corresponding
to two distinct chronological time points, t1 and t2, that is, t1 < t2.
At time t1, before the sampling experiment is conducted, the observations of
X = {X1, â€¦ , Xn} are random, and so is the estimator Ì‚ğœ‡= X. One contemplates
the statistical behavior of the random quantity X, establishes all of its good
mathematical properties, derives the distributional characteristics of
âˆš
n(X âˆ’
ğœ‡)âˆ•ğœ, and ï¬nally claims that the true value of ğœ‡is likely, say with probability
1 âˆ’ğ›¼= 0.99, contained in the random interval
(
X âˆ’zğ›¼âˆ•2
ğœ
âˆš
n
, X + zğ›¼âˆ•2
ğœ
âˆš
n
)
.
This is a scientiï¬cally meaningful and mathematically correct statement. Unfor-
tunately an issue is left unresolved: the occurrence of the event that the random
interval contains the true value of ğœ‡is not veriï¬able, not at time t1 before the
sampling experiment is conducted nor at time t2 after the sampling experiment
is conducted.
At time t2, a realization of the sample x = {x1, â€¦ , xn} is obtained, but all the
scientiï¬c statements made about X at time t1 become much less relevant. First,
one does not know whether the true value of ğœ‡is contained in x Â± zğ›¼âˆ•2ğœâˆ•
âˆš
n,
and therefore one does not know how close ğœ‡is to the observed x. Second, one
cannot even claim that ğœ‡is â€œprobablyâ€ in the interval x Â± zğ›¼âˆ•2ğœâˆ•
âˆš
n since the
meaning of probability ends as the sampling experiment ends. At this point,
one has no other choices but to believe that the true value of ğœ‡is â€œcloseâ€ to x in
a very vague sense. However, it is important to remember that such a chosen
belief is essentially faith-based.
A parallel argument can be put forth for a more general statistical model,
where a parameter with an unknown value, say ğœƒ, is estimated by an estimator,
say Ì‚ğœƒ(X). A statement at time t1, for example,
P (ğœƒâˆˆ(a(X), b(X)) ) = 1 âˆ’ğ›¼
and the question whether ğœƒâˆˆ(a(x), b(x)) at time t2 are somewhat discon-
nected.
However, whether this is a valid issue or, if so, how awkward a role this
dilemma plays in the logic framework of statistical estimation is largely philo-
sophical and is, therefore, in addition to the points of the current discussion.
The relevant points here are as follows.
1) The interpretation of a statistic could change signiï¬cantly from t1 to t2.

44
Statistical Implications of Turingâ€™s Formula
2) In the usual statistical situation, the estimand ğœƒis unambiguously deï¬ned
and invariantly meaningful at both time points t1 and t2.
This is however not the case for the estimand of Turingâ€™s formula since
ğœ‹0 =
âˆ‘
kâ‰¥1
pk1[Yk = 0]
is a random variable, which makes the statistical implication somewhat
unusual.
To make the subsequent discussion clearer, consider the following three dif-
ferent sampling experiments with three diï¬€erent sample spaces and hence three
diï¬€erent perspectives, involving four diï¬€erent time points, t1, t2, t3, and t4, in a
chronological order.
Experiment 1: Draw an iid sample of size n from ğ’³at time t2.
Experiment 2: Draw an iid sample of size m from ğ’³at time t4.
Experiment 3: A two-step experiment:
Step 1: Draw an iid sample of size n from ğ’³at time t2,
and then
Step 2: Draw an additional iid sample of size m from ğ’³
at time t4.
At time t1, one anticipates Experiment 1, and both Turingâ€™s formula T1 and
its estimand ğœ‹0 are random variables. However, ğœ‹0 is not a probability of any
event in the ğœ-algebra entailed by Experiment 1 and its sample space ğ’³n. As a
random variable, ğœ‹0 has some unusual statistical properties, for example, even
at time t3 when the sample has an observed realization {x1, â€¦ , xn},
ËšÏ€0 =
âˆ‘
kâ‰¥1
pk1[yk = 0]
(1.59)
is unobservable under the assumption that {pk} is unknown (however, see
Exercise 15). Since the target of the estimation is random at t1, it is not clear
what a good practical interpretation of ğœ‹0 could be.
At time t2, Experiment 1 is conducted and a realization of a sample
{x1, â€¦ , xn} is observed.
At time t3, the estimand ğœ‹0 takes a ï¬xed value in the form ofËšÏ€0.ËšÏ€0 becomes
a probability, not a probability associated with Experiment 1 and its ğœ-algebra,
but a probability of an event in the ğœ-algebra associated with Experiment 2 (if
Experiment 2 is to be conducted with m = 1) and its sample space ğ’³1. The fact
that the estimand ğœ‹0 with a vague practical interpretation at time t1 transforms
itself into the probability of a very interesting and meaningful event associated
with Experiment 2 may, in part, be the reason for the oracle-like impression
about Turingâ€™s formula.

Turingâ€™s Formula
45
In addition to ğœ‹0, there are several interesting variants of ğœ‹0 worth noting.
The ï¬rst is the expected value of ğœ‹0, denoted by ğœ1,n,
ğœ1,n = E(ğœ‹0) = E
(
âˆ‘
kâ‰¥1
pk1[Yk = 0]
)
=
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)n.
Incidentally ğœ1,n has a probabilistic interpretation in Experiment 3 with
m = 1. Let Xi, i = 1, â€¦ , n, be the iid observations of the sample in the ï¬rst
step of Experiment 3, and Xn+1 be the random letter taken by the single
observation in the second step of Experiment 3. Let E1 be the event that Xn+1
is not one of the letters taken into the sample of size n in the ï¬rst step. Then
P(E1) =
âˆ‘
kâ‰¥1
P(X1 â‰ ğ“k, â€¦ , Xn â‰ ğ“k, Xn+1 = ğ“k)
=
âˆ‘
kâ‰¥1
P(X1 â‰ ğ“k, â€¦ , Xn â‰ ğ“k|Xn+1 = ğ“k)P(Xn+1 = ğ“k)
=
âˆ‘
kâ‰¥1
pk
n
âˆ
i=1
P(Xi â‰ ğ“k) =
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)n = ğœ1,n.
(1.60)
Note that ğœ1,n = P(E1) is a probability associated with the ğœ-algebra of Exper-
iment 3 and its sample space ğ’³n+1, not to be confused withËšÏ€0.
Furthermore, consider Experiment 3 yet again with m = n. Let the two sub-
samples in the ï¬rst and the second steps be represented by the two rows as
follows:
X1,
X2,
â€¦ ,
Xn,
Xn+1, Xn+2, â€¦ , Xn+n.
Let Ej be the event that Xn+j is not one of the letters taken into the sample
of size n in the ï¬rst step. Clearly P(Ej) = âˆ‘
kâ‰¥1 pk(1 âˆ’pk)n = ğœ1,n for every j,
j = 1, 2, â€¦ , n. The expected number of observations in the second subsample,
whose observed letters are not included in the ï¬rst subsample, is then
ğœn âˆ¶=E
( n
âˆ‘
j=1
1[Ej]
)
=
n
âˆ‘
j=1
P(Ej) = n
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)n = nğœ1,n.
Both ğœ1,n and ğœn = nğœ1,n will play fundamental roles in the subsequent chapters.
1.7
Exercises
1
Consider the following three events when tossing diï¬€erent number of bal-
anced dice:
a) A: observing at least one six when tossing six dice.
b) B: observing at least two sixes when tossing 12 dice.
c) C: observing at least three sixes when tossing 18 dice.
Which event has the highest probability?

46
Statistical Implications of Turingâ€™s Formula
2
A random variable X is said to have a Poisson distribution if
pk = P(X = k) = eâˆ’ğœ†ğœ†kâˆ•k!, where ğœ†> 0 is a parameter, for k = 0, 1, â€¦ .
a) Show that when ğœ†is not an integer, pk has a unique maximum. Find
that maximum.
b) When ğœ†is a positive integer, show that pk attains its maximum at two
distinct values of k. Find these two values of k.
3
One hundred families live in a remote village. The following table sum-
marizes the proportions of families with various numbers of school-aged
children.
Number of children
0
1
2
3
4
Proportion of families
0.3
0.3
0.2
0.1
0.1 .
All school-aged children from the village go to the same village school and
all students of the school are from the village.
a) Let X be the number of children in a randomly selected family in the
village. Find E(X).
b) Let Y be the number of children in the family of a randomly selected
student in the village school. Find E(Y).
4
In a sequence of identically distributed and independent Bernoulli trials,
each of which is with probability of â€œsuccessâ€ p > 0, a geometric random
variable X is such that X = k, k = 1, 2, â€¦ , if and only if the ï¬rst â€œsuccessâ€
comes on the kth trial, that is,
pk = P(X = k) = (1 âˆ’p)kâˆ’1p,
for k = 1, â€¦ . Show that E(X) = 1âˆ•p and that Var(X) = (1 âˆ’p)âˆ•p2.
5
Show
a) E (T1) = âˆ‘
k pk(1 âˆ’pk)nâˆ’1 and
b) E (ğœ‹0) = âˆ‘
k pk(1 âˆ’pk)n.
6
Show T1
pâ†’0 and ğœ‹0
pâ†’0.
7
Show E (ğœ‹0) âˆ¼c(1 âˆ’pâˆ§)n for some c > 0, where âˆ¼represents equality in
rate of divergence or convergence and pâˆ§= min{pk; k â‰¥1} for any prob-
ability distribution {pk; k â‰¥1} on a ï¬nite alphabet.
8
Prove Lemma 1.3.
9
Let Xn be a sequence of random variables satisfying
a) P(Xn â‰¥0) = 1 and
b) E(Xn) â†’0 as n â†’âˆ.
Then Xn
pâ†’0.

Turingâ€™s Formula
47
10
Show limnâ†’âˆğœ2
n = 0 where ğœ2
n = Var(T1 âˆ’ğœ‹0) under any probability
distribution.
11
Let c1 and c2 be as in Condition 1.1, and let Ì‚c1 and Ì‚c2 be as in (1.14). Show
that, if {pk; k â‰¥1} satisï¬es Condition 1.1,
Ì‚c1
p
âˆ’âˆ’âˆ’â†’c1and Ì‚c2
p
âˆ’âˆ’âˆ’â†’c1.
(Hint: E(Ì‚c1) â†’c1, Var(Ì‚c1) â†’0, E(Ì‚c2) â†’c2, and Var(Ì‚c2) â†’0.)
12
Show that the statement of Example 1.5 is true.
13
Show that, for any real number x âˆˆ(0, 1âˆ•2),
1
1âˆ’x < 1 + 2x.
14
If {pk} is such that pk = c0 eâˆ’k where c0 = e âˆ’1 for all k â‰¥1, then there
exists an M > 0 such that E (N1) + E (N2) < M for all n â‰¥1.
15
Describe a sampling scheme with which ËšÏ€0 in (1.59) is observed even
under the assumption that {pk} is unknown. (Hint: Consider the case of a
known ï¬nite K.)
16
Rewrite Theorem 1.8 into Theorem 1.9.
17
Prove Theorem 1.10.
18
Let {pk} be such that pk = Ckâˆ’ğœ†for k > k0, where C, ğœ†, and a positive inte-
ger k0 are unknown parameters. Show that
P(min{k; Yk = r âˆ’1} â‰¥k0) â†’1
as n increases indeï¬nitely.
19
Show that under Condition 1.1,
n (Tâ™¯â™¯âˆ’ğœ‹0)
âˆš
E(N1) + 2E(N2)
L
âˆ’âˆ’âˆ’â†’N(0, 1).
20
Verify S(3, 1) = 1, S(3, 2) = 3, and S(3, 3) = 1 by (1.53).
21
Let ğ’³= {ğ“1, ğ“2, ğ“3, ğ“4}. Verify S(4, 2) = 7 by (1.53) and list all possible
partitions.
22
Show that each individual random element Xt, t = 0, â€¦ , in the special
Markov chain with memory m, deï¬ned in (1.57), follows (unconditionally)
the uniform distribution on the alphabet ğ’³.

48
Statistical Implications of Turingâ€™s Formula
23
Provide a proof for each part of Corollary 1.3. (Hint: For Part 1, use
the delta method, but consider ï¬rst an augmented Turingâ€™s formula
T = T1 + nâˆ’n so that ln T is well deï¬ned.)
24
Use Part 2 of Corollary 1.3 to verify (1.23).

49
2
Estimation of Simpsonâ€™s Indices
Consider a probability distribution {pk; k â‰¥1} on a countable alphabet
ğ’³= {ğ“k; k â‰¥1} where each letter ğ“k may be viewed as a species in a biological
population and each pk as the proportion of kth species in the population.
Simpson (1949) deï¬ned a biodiversity index
ğœ†=
K
âˆ‘
k=1
p2
k
for a population with a ï¬nite number of species K, which has an equivalent
form
ğœ1,1 = 1 âˆ’ğœ†=
K
âˆ‘
k=1
pk(1 âˆ’pk).
(2.1)
ğœ1,1 assumes a value in [0, 1) with a higher level of ğœ1,1 indicating a more diverse
population and is widely used across many ï¬elds of study. ğœ1,1 is also known as
the Giniâ€“Simpson index, an attribution to Corrado Gini for his pioneer work
in Gini (1912).
2.1
Generalized Simpsonâ€™s Indices
Simpsonâ€™s biodiversity index can be naturally and beneï¬cially generalized in
two directions. First, the size of the alphabet may be extended from a ï¬nite K
to inï¬nity. Second, ğœ1,1 may be considered as a special member of the following
family:
ğœu,v =
âˆ‘
kâ‰¥1
pu
k(1 âˆ’pk)v
(2.2)
where u â‰¥1 and v â‰¥0 are two integers. Since
ğœu,v =
âˆ‘
kâ‰¥1
[puâˆ’1
k
(1 âˆ’pk)vâˆ’1] pk(1 âˆ’pk),
Statistical Implications of Turingâ€™s Formula, First Edition. Zhiyi Zhang.
Â© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

50
Statistical Implications of Turingâ€™s Formula
it may be viewed as a weighted version of (2.1) with the weight for each k being
wk = puâˆ’1
k
(1 âˆ’pk)vâˆ’1. For examples, ğœ1,2 loads higher weight on minor species
(those with smaller pks); and ğœ2,1 loads higher weight on major species, etc.
Equation (2.2) is proposed by Zhang and Zhou (2010) and is referred to as the
generalized Simpsonâ€™s indices.
As u assumes each positive integer value and v assumes each nonnegative
integer value, (2.2) may be expressed as a panel in (2.3). This panel of indices
is the centerpiece of many of the statistical procedures based on Turingâ€™s per-
spective:
{ğœu,v} = {ğœu,v; u â‰¥1, v â‰¥0} =
â›
âœ
âœ
âœ
âœ
âœ
âœâ
ğœ1,0
ğœ1,1
ğœ1,2
Â· Â· Â·
ğœ1,v
Â· Â· Â·
ğœ2,0
ğœ2,1
ğœ2,2
Â· Â· Â·
ğœ2,v
Â· Â· Â·
â‹®
â‹®
â‹®
â‹®â‹®â‹®
â‹®
â‹®â‹®â‹®
ğœu,0
ğœu,1
ğœu,2
Â· Â· Â·
ğœu,v
Â· Â· Â·
â‹®
â‹®
â‹®
â‹®â‹®â‹®
â‹®
â‹®â‹®â‹®
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸâ 
(2.3)
For any probability sequence p = {pk; k â‰¥1} on ğ’³, let pâ†“be the
non-increasingly rearranged p and let {ğœu,0; u â‰¥1} be the ï¬rst column of
the index panel (2.3).
Lemma 2.1
For any given probability distribution p = {pk; k â‰¥1} on ğ’³, pâ†“
and {ğœu,0; u â‰¥1} uniquely determine each other.
Proof: Without loss of generality, assume p = {pk; k â‰¥1} is non-increasingly
ordered. It is obvious that {pk; k â‰¥1} uniquely determines the sequence
{ğœu,0; u â‰¥1}. It suï¬ƒces to show that {ğœu,0;} uniquely determines {pk; k â‰¥1}.
Toward that end, suppose that there exists another nonincreasingly ordered
probability sequence, {qk}, on ğ’³satisfying
âˆ‘
kâ‰¥1
pu
k =
âˆ‘
kâ‰¥1
qu
k
for all u â‰¥1.
Let k0 = min{k âˆ¶pk â‰ qk}. If k0 does not exist, then {pk} = {qk}. If k0 exists,
then
âˆ‘
kâ‰¥k0
pu
k =
âˆ‘
kâ‰¥k0
qu
k
(2.4)
for each and every integer u â‰¥1. It can be easily shown that
lim
uâ†’âˆ
âˆ‘
kâ‰¥k0pu
k
pu
k0
= rp â‰¥1
and
lim
uâ†’âˆ
âˆ‘
kâ‰¥k0qu
k
qu
k0
= rq â‰¥1
(2.5)

Estimation of Simpsonâ€™s Indices
51
where rp and rq are, respectively, multiplicities of the value pk0 in {pk} and of
the value qk0 in {qk}. But by (2.4),
âˆ‘
kâ‰¥k0pu
k
pu
k0
=
âˆ‘
kâ‰¥k0qu
k
qu
k0
(
qk0
pk0
)u
.
(2.6)
The right-hand side of (2.6) approaches 0 or âˆif pk0 â‰ qk0, which contradicts
(2.5). Therefore, k0 does not exist and {pk} = {qk}.
â—½
Since {ğœu,0; u â‰¥1} is a subset of {ğœu,v; u â‰¥1, v â‰¥0}, the following theorem is
an immediate consequence of Lemma 2.1.
Theorem 2.1
For any given probability distribution p = {pk; k â‰¥1} on ğ’³, pâ†“
and {ğœu,v; u â‰¥1, v â‰¥0} uniquely determine each other.
Theorem 2.1 has an intriguing implication: the complete knowledge of {pk}
up to a permutation and the complete knowledge of {ğœu,v} are equivalent. In
other words, all the generalized Simpsonâ€™s indices collectively and uniquely
determine the underlying distribution {pk} upto a permutation on the index
set {k; k â‰¥1}. This implication is another motivation for generalizing Simp-
sonâ€™s diversity index beyond ğœ1,1.
Theorem 2.2
For any given probability distribution p = {pk; k â‰¥1} on ğ’³, pâ†“
and {ğœ1,v; v â‰¥0} uniquely determine each other.
Proof: By Theorem 2.1, it suï¬ƒces to show that {ğœ1,v; v â‰¥0} uniquely determines
{ğœu,v; u â‰¥1, v â‰¥0}. For every pair of (u, v),
ğœu,v =
âˆ‘
kâ‰¥1
pu
k(1 âˆ’pk)v
=
âˆ‘
kâ‰¥1
puâˆ’1
k
[1 âˆ’(1 âˆ’pk)](1 âˆ’pk)v
=
âˆ‘
kâ‰¥1
puâˆ’1
k
(1 âˆ’pk)v âˆ’
âˆ‘
kâ‰¥1
puâˆ’1
k
(1 âˆ’pk)v+1
= ğœuâˆ’1,v âˆ’ğœuâˆ’1,v+1.
That is to say that every ğœu,v in (2.3) may be expressed as a linear combination of
two indices in the row above, namely ğœuâˆ’1,v and ğœuâˆ’1,v+1, which in turn may be
expressed as linear combinations of indices in the row further above. Therefore,
each ğœu,v may be expressed as a linear combination of, and hence is uniquely
determined by, the indices in the top row, which is denoted as
{ğœv; v â‰¥0} âˆ¶= {ğœ1,v; v â‰¥0} = {ğœ1,0, ğœ1,1, ğœ1,2,â€¦}.
â—½
Theorem 2.2 suggests that the complete knowledge of {pk} is essentially cap-
tured by the complete knowledge of {ğœv}. In this sense, {ğœv} may be considered

52
Statistical Implications of Turingâ€™s Formula
as a reparameterization of {pk}. This is in fact the basis for the statistical per-
spective oï¬€ered by Turingâ€™s formula.
2.2
Estimation of Simpsonâ€™s Indices
Let X1, â€¦ , Xi, â€¦ , Xn be an iid sample under {pk} from ğ’³. Xi may be written
as Xi = (Xi,k; k â‰¥1) where for every i, Xi,k takes 1 only for one k and 0 for all
other k values. Let Yk = âˆ‘n
i=1 Xi,k and Ì‚pk = Ykâˆ•n. Yk is the number of observa-
tions of the kth species found in the sample.
For a ï¬xed pair (u, v) and an iid sample of size m = u + v, let Y (m)
k
be the
frequency of ğ“k in the sample for every k â‰¥1 and
N(m)
u
=
âˆ‘
kâ‰¥1
1
[
Y (m)
k
= u
]
(2.7)
the number of species each of which is represented exactly u times in the sample
of size m = u + v. It may be easily veriï¬ed that
Ì‚ğœ(m)
u,v =
(u + v
u
)âˆ’1
N(m)
u
(2.8)
is an unbiased estimator of ğœu,v (see Exercise 8).
Remark 2.1
It is important to note that counting the number of diï¬€erent let-
ters in a sample with exactly r observed frequency, Nr, and using that to estimate
various other quantities is the very essence of the perspective brought forward by
Turingâ€™s formula. To honor Alan Turingâ€™s contribution in this regard, any statis-
tical estimator based on Nr for r â‰¥1 in the subsequent text will be referred to as
an estimator in Turingâ€™s perspective. The fact that (2.8) is an unbiased estima-
tor of ğœu,v, though straightforward, is one of the most important consequences of
Turingâ€™s perspective.
Next follow a U-statistics construction by considering (2.8) for each of the
(
n
u+v
)
possible subsamples of size m = u + v from the original sample of size n.
Each subsample gives an unbiased estimator of ğœu,v, and therefore the average
of these
(
n
u+v
)
unbiased estimators
Zu,v =
(
n
u + v
)âˆ’1 âˆ‘
âˆ—
Ì‚ğœ(m)
u,v =
(
n
u + v
)âˆ’1(u + v
u
)âˆ’1 âˆ‘
âˆ—
N(m)
u ,
(2.9)
where the summation âˆ‘
âˆ—is over all possible size-m subsamples of the original
size-n sample, is in fact an unbiased estimator.
On the other hand, noting (2.7) and the fact
âˆ‘
âˆ—
N(m)
u
=
âˆ‘
kâ‰¥1
(
âˆ‘
âˆ—
1
[
Y (m)
k
= u
])
,

Estimation of Simpsonâ€™s Indices
53
âˆ‘
âˆ—N(m)
u
is simply the total number of times exactly u observations are found in
a same species among all possible subsamples of size m = u + v taken from the
sample of size n.
In counting the total number of such events, it is to be noted that, for any
ï¬xed u, only for species that are represented in the sample u times or more can
such an event occur. Therefore,
âˆ‘
âˆ—
N(m)
u
=
âˆ‘
kâ‰¥1
1 [Yk â‰¥u]
(Yk
u
) (n âˆ’Yk
v
)
.
Furthermore, assuming v â‰¥1,
1 [Yk â‰¥u]
(Yk
u
) (n âˆ’Yk
v
)
= 1 [Yk â‰¥u]
[Yk(Yk âˆ’1) Â· Â· Â· (Yk âˆ’u + 1)
u!
]
Ã—
[(n âˆ’Yk)(n âˆ’Yk âˆ’1) Â· Â· Â· (n âˆ’Yk âˆ’v + 1)
v!
]
= nu+v
u!v! 1
[
Ì‚pk â‰¥u
n
] [
Ì‚pk
(
Ì‚pk âˆ’1
n
)
Â· Â· Â·
(
Ì‚pk âˆ’u
n + 1
n
)]
Ã—
[
(1 âˆ’Ì‚pk)
(
1 âˆ’Ì‚pk âˆ’1
n
)
Â· Â· Â·
(
1 âˆ’Ì‚pk âˆ’v
n + 1
n
)]
= nu+v
u!v! 1
[
Ì‚pk â‰¥u
n
] [uâˆ’1
âˆ
i=0
(
Ì‚pk âˆ’i
n
)] [ vâˆ’1
âˆ
j=0
(
1 âˆ’Ì‚pk âˆ’j
n
)]
and
(
n
u + v
)âˆ’1(u + v
u
)âˆ’1
= (n âˆ’u âˆ’v)!(u + v)!
n!
u!v!
(u + v)!
= (n âˆ’u âˆ’v)!u!v!
n!
.
Therefore, for every pair of (u, v) where u â‰¥1 and v â‰¥1,
Zu,v = [n âˆ’(u + v)]!nu+v
n!
Ã—
âˆ‘
kâ‰¥1
{
1
[
Ì‚pk â‰¥u
n
] [uâˆ’1
âˆ
i=0
(
Ì‚pk âˆ’i
n
)] [ vâˆ’1
âˆ
j=0
(
1 âˆ’Ì‚pk âˆ’j
n
)]}
.
(2.10)
In case of u â‰¥1 and v = 0, a similar argument leads to
Zu,v =
[uâˆ’1
âˆ
j=1
(
n
n âˆ’j
)]
âˆ‘
kâ‰¥1
{
1
[
Ì‚pk â‰¥u
n
] [uâˆ’1
âˆ
i=0
(
Ì‚pk âˆ’i
n
)]}
(2.11)
(see Exercise 9).

54
Statistical Implications of Turingâ€™s Formula
For a few special pairs of u and v, Zu,v reduces to
Z1,1 =
n
n âˆ’1
âˆ‘
kâ‰¥1
Ì‚pk(1 âˆ’Ì‚pk)
Z2,0 =
n
n âˆ’1
âˆ‘
kâ‰¥1
1[Ì‚pk â‰¥2âˆ•n]Ì‚pk(Ì‚pk âˆ’1âˆ•n)
Z3,0 =
n2
(n âˆ’1)(n âˆ’2)
âˆ‘
kâ‰¥1
1[Ì‚pk â‰¥3âˆ•n]Ì‚pk(Ì‚pk âˆ’1âˆ•n)(Ì‚pk âˆ’2âˆ•n)
Z2,1 =
n2
(n âˆ’1)(n âˆ’2)
âˆ‘
kâ‰¥1
1[Ì‚pk â‰¥2âˆ•n]Ì‚pk(Ì‚pk âˆ’1âˆ•n)(1 âˆ’Ì‚pk)
Z1,2 =
n2
(n âˆ’1)(n âˆ’2)
âˆ‘
kâ‰¥1
1[Ì‚pk â‰¥1âˆ•n]Ì‚pk(1 âˆ’Ì‚pk)(1 âˆ’1âˆ•n âˆ’Ì‚pk).
(2.12)
Zu,v is an unbiased estimator of ğœu,v. This fact is established by the U-statistic
construction of the above-mentioned estimator. In fact, Zu,v is a uniformly min-
imum variance unbiased estimator (umvue) of ğœu,v when the cardinality of ğ’³,
K, is ï¬nite. Since Zu,v is unbiased, by the Lehmannâ€“Scheï¬€Ã© theorem, it suï¬ƒces
to note that {Ì‚pk} is a set of complete and suï¬ƒcient statistics under {pk} (see
Exercise 12).
2.3
Normal Laws
The same U-statistic construction paves the path for establishing the asymp-
totic normality of Zu,v âˆ’ğœu,v. For a review of the theory of U-statistics, readers
may wish to refer to Serï¬‚ing (1980) or Lee (1990).
Let
1) X1,â€¦, Xn be an iid sample under a distribution F;
2) ğœƒ= ğœƒ(F) be a parameter of interest;
3) h(X1,â€¦, Xm) where m < n be a symmetric kernel satisfying
EF{h(X1,â€¦, Xm)} = ğœƒ(F);
4) Un = U(X1,â€¦, Xn) =
(
n
m
)âˆ’1âˆ‘
âˆ—h(Xi1,â€¦, Xim) where the summation âˆ‘
âˆ—is
over all possible subsamples of size m from the original sample of size n;
5) h1(x1) = EF(h(x1, X2,â€¦, Xm)) be the conditional expectation of h given
X1 = x1; and
6) ğœ2
1 = VarF(h1(X1)).
The following lemma is due to Hoeï¬€ding (1948).

Estimation of Simpsonâ€™s Indices
55
Lemma 2.2
If EF(h2) < âˆand ğœ2
1 > 0, then
âˆš
n(Un âˆ’ğœƒ)
L
âˆ’âˆ’âˆ’â†’N(0, m2ğœ2
1).
To apply Lemma 2.2 to the U-statistic Zu,v, the two conditions EF(h2) < âˆ
and ğœ2
1 > 0 must be checked.
Let
m = u + v,
h = h(X1,â€¦, Xm) =
(m
u
)âˆ’1
N(m)
u ,
F = {pk}.
Clearly in this case, EF(h2) < âˆsince N(m)
u
is bounded above by every m.
Therefore, only the second condition of Lemma 2.2, namely ğœ2
1 > 0, needs to
be checked.
Toward that end, suppose u â‰¥1 and v â‰¥1. Given
X1 = x1 = (x1,1, x1,2,â€¦, x1,k, Â· Â· Â·),
(m
u
)
h1(x1) =
(m
u
)
EF(h(x1, X2,â€¦, Xm))
= EF (N(m)
u |X1 = x1)
=
âˆ‘
kâ‰¥1
1 [x1,k = 1]
(m âˆ’1
u âˆ’1
)
puâˆ’1
k
(1 âˆ’pk)v
+
âˆ‘
kâ‰¥1
1 [x1,k = 0]
(m âˆ’1
u
)
pu
k(1 âˆ’pk)vâˆ’1
=
âˆ‘
kâ‰¥1
(m âˆ’1
u
)
pu
k(1 âˆ’pk)vâˆ’1
+
âˆ‘
kâ‰¥1
1[x1,k = 1]
(m âˆ’1
u
)
puâˆ’1
k
(1 âˆ’pk)vâˆ’1[
(1âˆ’pk)u
v âˆ’pk
]
=
(m âˆ’1
u
) âˆ‘
kâ‰¥1
pu
k(1 âˆ’pk)vâˆ’1
+
(m âˆ’1
u
)âˆ‘
kâ‰¥1
1[x1,k = 1]puâˆ’1
k
(1 âˆ’pk)vâˆ’1[
(1âˆ’pk)u
v âˆ’pk
]
,
or
h1(x1) =
v
u + v
âˆ‘
kâ‰¥1
pu
k(1 âˆ’pk)vâˆ’1
+
v
u + v
âˆ‘
kâ‰¥1
1[x1,k = 1]puâˆ’1
k
(1 âˆ’pk)vâˆ’1 [
(1 âˆ’pk)u
v âˆ’pk
]
,

56
Statistical Implications of Turingâ€™s Formula
and therefore
ğœ2
1(u, v) = VarF(h1(X1))
=
(
v
u + v
)2
VarF
{
âˆ‘
kâ‰¥1
1[X1,k = 1]puâˆ’1
k
(1 âˆ’pk)vâˆ’1 [
(1 âˆ’pk)u
v âˆ’pk
]}
=
(
v
u + v
)2 â§
âª
â¨
âªâ©
EF
{
âˆ‘
kâ‰¥1
1[X1,k = 1]puâˆ’1
k
(1 âˆ’pk)vâˆ’1 [
(1 âˆ’pk)u
v âˆ’pk
]}2
âˆ’
{
âˆ‘
kâ‰¥1
pu
k(1 âˆ’pk)vâˆ’1 [
(1 âˆ’pk)u
v âˆ’pk
]}2â«
âª
â¬
âªâ­
=
(
v
u + v
)2 â§
âª
â¨
âªâ©
âˆ‘
kâ‰¥1
p2uâˆ’1
k
(1 âˆ’pk)2vâˆ’2[
(1 âˆ’pk)u
v âˆ’pk
]2
âˆ’
{
âˆ‘
kâ‰¥1
pu
k(1 âˆ’pk)vâˆ’1 [
(1 âˆ’pk)u
v âˆ’pk
]}2â«
âª
â¬
âªâ­
= u2
v2
(
v
u + v
)2 âˆ‘
kâ‰¥1
p2uâˆ’1
k
q2v
k âˆ’2u
v
(
v
u + v
)2 âˆ‘
kâ‰¥1
p2u
k q2vâˆ’1
k
+
(
v
u + v
)2 âˆ‘
kâ‰¥1
p2u+1
k
(1 âˆ’pk)2vâˆ’2
âˆ’
(
v
u + v
)2[
u
v
âˆ‘
kâ‰¥1
pu
k(1 âˆ’pk)v âˆ’
âˆ‘
kâ‰¥1
pu+1
k
(1 âˆ’pk)vâˆ’1
]2
= u2
v2
(
v
u + v
)2
ğœ2uâˆ’1,2v âˆ’2u
v
(
v
u + v
)2
ğœ2u,2vâˆ’1
+
(
v
u + v
)2
ğœ2u+1,2vâˆ’2
âˆ’
(
v
u + v
)2(u
v ğœu,v âˆ’ğœu+1,vâˆ’1
)2
=
u2
(u + v)2 ğœ2uâˆ’1,2v âˆ’
2uv
(u + v)2 ğœ2u,2vâˆ’1
+
v2
(u + v)2 ğœ2u+1,2vâˆ’2 âˆ’
v2
(u + v)2
(u
v ğœu,v âˆ’ğœu+1,vâˆ’1
)2
â‰¥0.
(2.13)
The last inequality in (2.13) becomes an equality if and only if h(X1) is a con-
stant that occurs when all the positive probabilities of {pk} are equal.

Estimation of Simpsonâ€™s Indices
57
Suppose u â‰¥1 and v = 0 and therefore
(
m
u
)
= 1. It is easy to see that
h1(x1) =
âˆ‘
kâ‰¥1
1[x1,k = 1]puâˆ’1
k
and
ğœ2
1(u, 0) = VarF(h1(X1)) = ğœ2uâˆ’1,0 âˆ’ğœ2
u,0 â‰¥0
(2.14)
(see Exercise 10). The strict inequality holds for all cases except when {pk} is
uniform.
Thus, the following theorem is established.
Theorem 2.3
For any given pair of u and v,
1) if {pk} is such that ğœ2
1(u, v) > 0, then
âˆš
n(Zu,v âˆ’ğœu,v)
L
âˆ’âˆ’âˆ’â†’N(0, (u + v)2ğœ2
1(u, v));
2) if {pk} is such that ğœ2
1(u, 0) > 0, then
âˆš
n(Zu,0 âˆ’ğœu,0)
L
âˆ’âˆ’âˆ’â†’N(0, u2ğœ2
1(u, 0)).
Theorem 2.3 immediately implies consistency of Zu,v to ğœu,v and the consis-
tency of Zu,0 to ğœu,0 for any u â‰¥1 and v â‰¥1 under the stated condition.
By the last expressions of (2.13) and (2.14), and Theorem 2.3, it is easily seen
that when u â‰¥1 and v â‰¥1,
Ì‚ğœ2
1(u, v) =
u2
(u + v)2 Z2uâˆ’1,2v
âˆ’
2uv
(u + v)2 Z2u,2vâˆ’1
+
v2
(u + v)2 Z2u+1,2vâˆ’2
âˆ’
v2
(u + v)2
(u
v Zu,v âˆ’Zu+1,vâˆ’1
)2
(2.15)
and
Ì‚ğœ2
1(u, 0) = Z2uâˆ’1,0 âˆ’Z2
u,0
(2.16)
are consistent estimators of ğœ2
1(u, v) and of ğœ2
1(u, 0), respectively, and hence the
following corollary is established.
Corollary 2.1
Suppose, for any given pair of u and v, the condition of
Theorem 2.3 for each of the two parts is respectively satisï¬ed. Then the following

58
Statistical Implications of Turingâ€™s Formula
cases occur:
1) if {pk} is such that ğœ2
1(u, v) > 0, then
âˆš
n(Zu,v âˆ’ğœu,v)
(u + v)Ì‚ğœ1(u, v)
L
âˆ’âˆ’âˆ’â†’N(0, 1)
(2.17)
2) if {pk} is such that ğœ2
1(u, 0) > 0, then
âˆš
n(Zu,0 âˆ’ğœu,0)
uÌ‚ğœ1(u, 0)
L
âˆ’âˆ’âˆ’â†’N(0, 1),
(2.18)
where Ì‚ğœ2
1(u, v) and Ì‚ğœ2
1(u, 0) are as in (2.15) and (2.16) respectively.
Remark 2.2
The condition ğœ2
1(u, v) > 0 of Part 1 for both Theorem 2.3 and
Corollary 2.1 is diï¬ƒcult to verify in practice. However under the following two
special cases, the condition has an equivalent form.
1) When u = v = 1, ğœ2
1(u, v) > 0 if and only if {pk} is not a uniform distribution
on ğ’³.
2) When u â‰¥1 and v = 0, ğœ2
1(u, 0) > 0 if and only if {pk} is not a uniform dis-
tribution on ğ’³.
As a case of special interest when u = v = 1, the computational formula of
Z1,1 is given in (2.12) and
âˆš
n(Z1,1 âˆ’ğœ1,1)
2Ì‚ğœ1(1, 1)
L
âˆ’âˆ’âˆ’â†’N(0, 1)
(2.19)
where Ì‚ğœ1(1, 1) is such that
4Ì‚ğœ2
1(1, 1) = Z1,2 âˆ’2Z2,1 + Z3,0 âˆ’(Z1,1 âˆ’Z2,0)2
and Z1,2, Z2,1, Z3,0, and Z2,0 are all given in (2.12). Equation (2.19) may be used
for large sample inferences with respect to Simpsonâ€™s index, ğœ1,1, whenever the
nonuniformity of the underlying distribution is deemed reasonable.
Zu,v is asymptotically eï¬ƒcient when K is ï¬nite. This fact is established
by recognizing ï¬rst that {Ì‚pk} is the maximum likelihood estimator (mle)
of {pk}, second that Ì‚ğœâˆ—
u,v = âˆ‘Ì‚pu
k(1 âˆ’Ì‚pk)v is the mle of ğœu,v, and third that
âˆš
n(Zu,v âˆ’Ì‚ğœâˆ—
u,v)
p
âˆ’âˆ’âˆ’â†’0.
In simplifying the expression of Zu,v in (2.10), let
Zu,v = an(u + v)Zâˆ—
u,v
where
an(u + v) = [n âˆ’(u + v)]!nu+v
n!

Estimation of Simpsonâ€™s Indices
59
and
Zâˆ—
u,v =
K
âˆ‘
k=1
1
[
Ì‚pk â‰¥u
n
] [uâˆ’1
âˆ
i=0
(
Ì‚pk âˆ’i
n
) vâˆ’1
âˆ
j=0
(
1 âˆ’Ì‚pk âˆ’j
n
)]
.
Since
âˆš
n(Zu,v âˆ’Ì‚ğœâˆ—
u,v) =
âˆš
n(an(u + v) âˆ’1)Zâˆ—
u,v +
âˆš
n(Zâˆ—
u.v âˆ’Ì‚ğœâˆ—
u,v),
(2.20)
the third fact may be established by showing that each of the above two terms
converges to zero in probability, respectively. Toward that end, consider the
following steps:
Step 1. Zâˆ—
u,v â‰¤1. This is so because every summand in Zâˆ—
u,v is nonnegative and
bounded above by Ì‚pk and âˆ‘K
k=1 Ì‚pk = 1.
Step 2.
âˆš
n(an(u + v) âˆ’1) â†’0 as n â†’âˆ. Let m = u + v be any ï¬xed positive
integer. This statement is proven by induction. If m = 1, then
âˆš
n(an(1) âˆ’1) =
âˆš
n
[(n âˆ’1)!n
n(n âˆ’1)! âˆ’1
]
= 0.
Assume that for any positive integer m âˆ’1,
âˆš
n(an(m âˆ’1) âˆ’1) â†’0 as
n â†’âˆ. Then
âˆš
n(an(m) âˆ’1) =
âˆš
n
[(n âˆ’m)!nm
n!
âˆ’1
]
=
âˆš
n
{[n âˆ’(m âˆ’1)]!nmâˆ’1n
n![n âˆ’(m âˆ’1)]
âˆ’1
}
=
âˆš
n
[ an(m âˆ’1)n
(n âˆ’m + 1) âˆ’an(mâˆ’1) + an(mâˆ’1)âˆ’1
]
=
âˆš
nan(m âˆ’1)
m âˆ’1
n âˆ’m + 1 +
âˆš
n(an(mâˆ’1)âˆ’1).
Noting an(m âˆ’1) â†’1 and, by the inductive assumption
âˆš
n(an(m âˆ’
1) âˆ’1) â†’0, it follows that
âˆš
n(an(m) âˆ’1) â†’0. At this point, it has
been established that the ï¬rst term in (2.20) converges to zero in
probability, that is,
âˆš
n(an(u + v) âˆ’1)Zâˆ—
u,v
p
âˆ’âˆ’âˆ’â†’0. It only remains to
show
âˆš
n(Zâˆ—
u.v âˆ’Ì‚ğœâˆ—
u,v).
Step 3. Re-expressing the mle of ğœu,v in
Ì‚ğœâˆ—
u,v =
K
âˆ‘
k=1
1
[
Ì‚pk â‰¥u
n
]
Ì‚pu
k(1 âˆ’Ì‚pk)v +
K
âˆ‘
k=1
1
[
Ì‚pk < u
n
]
Ì‚pu
k(1 âˆ’Ì‚pk)v
=âˆ¶Ì‚ğœ(1)
u,v + Ì‚ğœ(2)
u,v.

60
Statistical Implications of Turingâ€™s Formula
It suï¬ƒces to show that
âˆš
n
{ K
âˆ‘
k=1
1
[
Ì‚pk â‰¥u
n
][uâˆ’1
âˆ
i=0
(
Ì‚pk âˆ’i
n
) vâˆ’1
âˆ
j=0
(
1 âˆ’Ì‚pk âˆ’j
n
)]
âˆ’Ì‚ğœ(1)
u,v
}
âˆ’
âˆš
n Ì‚ğœ(2)
u,v
=
âˆš
n
{ K
âˆ‘
k=1
1
[
Ì‚pk â‰¥u
n
][uâˆ’1
âˆ
i=0
(
Ì‚pk âˆ’i
n
) vâˆ’1
âˆ
j=0
(
1âˆ’Ì‚pk âˆ’j
n
)
âˆ’Ì‚pu
k(1 âˆ’Ì‚pk)v
]}
âˆ’
âˆš
n Ì‚ğœ(2)
u,v
p
âˆ’âˆ’âˆ’â†’0,
(2.21)
or to show that each of the above-mentioned two terms converges to
zero in probability.
Step 4. Consider ï¬rst v = 0, in which case
vâˆ’1
âˆ
j=0
(
1 âˆ’Ì‚pk âˆ’j
n
)
= 1.
âˆuâˆ’1
i=0
(
Ì‚pk âˆ’i
n
)
may be written as a sum of Ì‚pu
k and ï¬nitely many other
terms each of which has the following form:
s1
ns2 Ì‚ps3
k
where s1, s2 â‰¥1, and s3 â‰¥1 are ï¬nite ï¬xed integers. Since
0 â‰¤
âˆš
n
K
âˆ‘
k=1
1
[
Ì‚pk â‰¥u
n
] |s1|
ns2 Ì‚ps3
k â‰¤
âˆš
n
K
âˆ‘
k=1
|s1|
ns2 Ì‚pk =
âˆš
n|s1|
ns2 â†’0
as n â†’âˆ, the ï¬rst term of (2.21) converges to zero in probability. The
second term of (2.21) converges to zero when u = 1 is an obvious case
since Ì‚ğœ(2)
u,v = 0. It also converges to zero in probability when u â‰¥2 since
0 â‰¤
âˆš
n
K
âˆ‘
k=1
1
[
Ì‚pk < u
n
]
Ì‚pu
k â‰¤
âˆš
n
K
âˆ‘
k=1
(u
n
)
Ì‚pk â‰¤
u
âˆš
n
â†’0.
Step 5. Next consider the case of v â‰¥1. âˆuâˆ’1
i=0
(
Ì‚pk âˆ’i
n
) âˆvâˆ’1
j=0
(
1 âˆ’Ì‚pk âˆ’j
n
)
may
be written as a sum of Ì‚pu
k(1 âˆ’Ì‚pk)v and ï¬nitely many other terms each
of which has the following form:
s1
ns2 Ì‚ps3
k (1 âˆ’Ì‚pk)s4

Estimation of Simpsonâ€™s Indices
61
where s1, s2 â‰¥1, s3 â‰¥1, and s4 â‰¥1 are ï¬nite ï¬xed integers. Since
0 â‰¤
âˆš
n
K
âˆ‘
k=1
1
[
Ì‚pk â‰¥u
n
] |s1|
ns2 Ì‚ps3
k (1 âˆ’Ì‚pk)s4
â‰¤
âˆš
n
K
âˆ‘
k=1
|s1|
ns2 Ì‚pk <
âˆš
n|s1|
ns2 â†’0
as n â†’âˆ, the ï¬rst term of (2.21) converges to zero in probability. The
second term of (2.21) remains the same as in Step 4 and therefore con-
verges to zero in probability.
Thus, the asymptotic eï¬ƒciency of Zu,v is established.
2.4
Illustrative Examples
In practice, it is often of interest to have a statistical tool to test the equality of
diversity between two populations against a diï¬€erence across time or space. Let
the said diversity be measured by ğœu,v. The appropriate hypothesis to be tested
is often of the form:
H0 âˆ¶ğœ(1)
1,1 âˆ’ğœ(2)
1,1 = D0 versus Ha âˆ¶ğœ(1)
1,1 âˆ’ğœ(2)
1,1 > D0 (or < D0 or â‰ D0)
where D0 is a preï¬xed known constant, and ğœ(1)
1,1 and ğœ(2)
1,1 are, respectively, the
true ğœu,v values for the two underlying populations. Similarly, the superscripts
(1) and (2) are to index all mathematical objects associated with the two popu-
lations whenever appropriate.
Suppose there are available two large independent iid samples of sizes n1 and
n2. Based on Corollary 2.1, under H0,
Z =
(
Z(1)
u,v âˆ’Z(2)
u,v
)
âˆ’D0
(u + v)
âˆš
(Ì‚ğœ(1)
1 (u,v))
2
n1
+ (Ì‚ğœ(2)
1 (u,v))
2
n2
L
âˆ’âˆ’âˆ’â†’N(0, 1)
(2.22)
(see Exercise 15) where (Ì‚ğœ(i)
1 (u, v))2 is as given in (2.15) with the ith sample,
i = 1, 2. In regard to the hypothesis, one would reject H0 in favor of Ha if the
test statistic Z assumes a value deemed statistically unusual.
Wang and Dodson (2006) oï¬€ered an analysis on several aspects of dinosaur
diversity based on an interesting fossil data set, which includes two subsamples,
one early Cretaceous and one late Cretaceous. The early Cretaceous set con-
tains n1 = 773 pieces of fossil belonging to 139 genera; the late Cretaceous set
contains n2 = 1348 pieces of fossil belonging to 237 genera. Let r be the num-
ber of observations in each observed genus and Nr be the number of observed
genera containing exactly r observations, as in (1.1). The two subsamples are
presented in Tables 2.1 and 2.2 in terms of r and Nr.

62
Statistical Implications of Turingâ€™s Formula
Table 2.1 Early Cretaceous
r
1
2
3
4
5
6
7
8
nr
90
19
12
3
2
1
1
1
r
9
13
14
15
30
38
124
300
nr
2
1
2
1
1
1
1
1
Table 2.2 Late Cretaceous
r
1
2
3
4
5
6
7
8
9
10
nr
136
19
17
7
13
6
4
2
2
1
r
11
12
13
16
17
18
20
24
25
26
nr
1
4
3
2
1
2
3
1
2
1
r
29
30
33
40
41
43
51
81
201
nr
1
2
1
1
1
1
1
1
1
Table 2.3 Zu,v of early Cretaceous
Z(e)
1,1
Z(e)
2,0
Z(e)
3,0
Z(e)
2,1
Z(e)
1,2
0.8183
0.1817
0.0623
0.1193
0.6990
Suppose it is of interest to detect a decrease in the Giniâ€“Simpson diver-
sity index ğœ1,1 from the early Cretaceous period to the late Cretaceous period.
Denote the true Simpsonâ€™s diversity indices at the early and the late Cretaceous
periods by ğœ(e)
1,1 and ğœ(l)
1,1, respectively. The appropriate hypothesis to be tested is
H0 âˆ¶ğœ(e)
1,1 âˆ’ğœ(l)
1,1 = 0 versus Ha âˆ¶ğœ(e)
1,1 âˆ’ğœ(l)
1,1 > 0.
(2.23)
Assuming the two fossil samples are iid, the value of the test statistic Z in
(2.22) is calculated to complete the test. Toward that end, the respective values
of Z1,1, Z2,0, Z3,0, Z2,1, and Z1,2 in (2.12) for the two samples are summarized in
Tables 2.3 and 2.4.
By (2.15),
(Ì‚ğœ(e)
1 (1, 1))2 = 1
4 Ã— 0.6990 âˆ’1
2 Ã— 0.1193 + 1
4 Ã— 0.0623
âˆ’1
4 Ã— (0.8183 âˆ’0.1817)2
= 0.0293

Estimation of Simpsonâ€™s Indices
63
Table 2.4 Zu,v of late Cretaceous
Z(l)
1,1
Z(l)
2,0
Z(l)
3,0
Z(l)
2,1
Z(l)
1,2
0.9642
0.0358
0.0037
0.0321
0.9321
and
(Ì‚ğœ(l)
1 (1, 1))2 = 1
4 Ã— 0.9321 âˆ’1
2 Ã— 0.0321 + 1
4 Ã— 0.0037
âˆ’1
4 Ã— (0.9642 âˆ’0.0358)2
= 0.0024
which lead to, by (2.22),
Z = 0.8183 âˆ’0.9642
2
âˆš
0.0293
773 + 0.0024
1348
= âˆ’11.5706,
which provides no statistical evidence of a decrease in diversity, as measured
by the Giniâ€“Simpson index, from the early Cretaceous period to the late
Cretaceous period. In fact, the highly negative value of the test statistic may
even suggest an increase in diversity over that time period.
Simpsonâ€™s index was originally used to measure the diversity of subspecies of
the dinosaur population hundreds of million years ago. The idea was to infer
from fossil data whether the diversity of dinosaur population had a gradual
diminishing period prior to its sudden extinction about 65 million years ago.
If such a diminishing period existed, then the extinction of dinosaurs may be a
consequence of natural evolution. If not, then the hypothesis of the Cretaceous
extinction, due to a sudden extraterrestrial impact, such as a huge asteroid or a
massive round of volcanoes, may be deemed more creditable.
In practice, an appropriate choice of speciï¬c pair of integer values (u, v) often
varies according to situational need. In general, a higher value of v assigns more
weight to species with smaller proportions, that is, with smaller pks, and a
higher value of u assigns more weight to species with larger proportions, that
is, with larger pks. However, one may choose to estimate ğœu,v for a set of (u, v)
values, also known as a proï¬le. For example,
{ğœ1,v; v = 1, 2,â€¦}
(2.24)
is a proï¬le and it may be considered as a function of v on â„•. Using proï¬les
to compare the diversity of two ecological communities was ï¬rst discussed by
Taillie (1979) and Patil and Taillie (1982), among others, and has since become
an important tool for ecologists.

64
Statistical Implications of Turingâ€™s Formula
Denote the two proï¬les of the two underlying populations and the diï¬€erence
between them as, respectively,
{
ğœ(1)
1,v
}
,
{
ğœ(2)
1,v
}
,
{ğ›¿v
} =
{
ğœ(1)
1,v âˆ’ğœ(2)
1,v
}
,
where {ğ›¿v} = {ğ›¿v; v â‰¥1} is sometimes called a diï¬€erence proï¬le.
Ì‚ğ›¿v = Z(1)
1,v âˆ’Z(2)
1,v
(2.25)
gives an unbiased estimator of ğ›¿v for every v, v = 1,â€¦, min{n1 âˆ’1, n2 âˆ’1}.
Furthermore, (2.22) with u = 1 gives an approximate (1 âˆ’ğ›¼) Ã— 100% point-wise
conï¬dence band
Ì‚ğ›¿v Â± zğ›¼âˆ•2(v + 1)
âˆš
âˆš
âˆš
âˆš
âˆš
(
Ì‚ğœ(1)
1 (1, v)
)2
n1
+
(
Ì‚ğœ(2)
1 (1, v)
)2
n2
.
(2.26)
To illustrate, two cases of authorship attribution are considered next. The
concept of species diversity in an ecological population is often applied to quan-
titative linguistics as that of vocabulary diversity or lexical richness. It is often
thought that diï¬€erent authors share a common alphabet, say the English vocab-
ulary, but that each has a propensity for using certain word types over others
and hence a unique distribution of word types. If this notion is deemed reason-
able, then the diï¬€erence between the two authorsâ€™ distributions of word types
may be captured by certain diversity measures or a proï¬le of them.
Consider the following three text samples:
1) The collection of 154 sonnets, which are commonly believed to have
been penned by William Shakespeare, in a book titled â€œSHAKESPEARES
SONNETSâ€ published by Thomas Thorpe in 1609. This collection contains
n = 17539 word types and is used as the corpus in the two cases of
authorship attribution.
2) The collection of three sonnets in the script of Romeo and Juliet,
a) the prologue to Act I;
b) Act I, Scene 5, â€œIf I profane with my unworthiest hand â€¦ Thus from my
lips, by thine, my sin is purgâ€™d.â€; and
c) the prologue to Act II.
This collection contains n = 339 word types.
3) The collection of the ï¬rst four sonnets from Philip Sidneyâ€™s Astrophel and
Stella sequence,
a) â€œLoving in truth, â€¦ My Muse said to me â€˜Fool, look in your heart and
write.â€™â€;
b) â€œNot at ï¬rst sight, â€¦ While with sensitive art I depict my self in hell.â€;
c) â€œLet dainty wits cry on the Sisters nine, â€¦ Is to copy what Nature has
written in her.â€; and
d) â€œVirtue, â€¦ Virtue, will be in love with her.â€.
This collection contains n = 1048 word types.

Estimation of Simpsonâ€™s Indices
65
Case 1 consists of a comparison between the corpus of 154 Shakespearean
sonnets, indexed by (1), and the sample of three Shakespearean sonnets in the
script of Romeo and Juliet, indexed by (2). Case 2 consists of a comparison
between the same corpus of Shakespearean sonnets, indexed by (1), and the
sample of the ï¬rst four sonnets in Philip Sidneyâ€™s and Stella sequence, index
by (2). Let the proï¬le {ğœ1,v; v â‰¥1} of each underlying population of interest be
restricted to a subset, that is, {ğœ1,v; 1 â‰¤v â‰¤r}, and consequently consider only
the estimation of the correspondingly restricted diï¬€erence proï¬le {ğ›¿v; 1 â‰¤v â‰¤
r}, where r = 50.
Using (2.25) and (2.26) with ğ›¼= 0.05, the estimated proï¬le diï¬€erences and
their corresponding 95% point-wise conï¬dence bands are represented graphi-
cally in Figures 2.1 and 2.2, where the curve in the middle represents the proï¬le
diï¬€erences while the upper and lower curves are, respectively, the upper bound
and lower bound of the conï¬dence band. It may be interesting to note that, in
Figure 2.1, the 95% conï¬dence band well contains the zero line (the dashed line)
for all v, 1 â‰¤v â‰¤50, indicating no statistical evidence of a diï¬€erence, as one
would expect. However, in Figure 2.2, the 95% conï¬dence band mostly bounds
away from the zero line, indicating a diï¬€erence in pattern of word usage, partic-
ularly the less commonly used words as they are assigned with higher weights
by higher values of v.
30
Estimated profile difference
20
10
0
â€“0.10
â€“0.05
0.00
0.05
0.10
v
40
50
Figure 2.1 Estimated diï¬€erence proï¬le: Shakespeare versus Shakespeare.

66
Statistical Implications of Turingâ€™s Formula
30
Estimated profile difference
20
10
0
â€“0.10
â€“0.05
0.00
0.05
0.10
v
40
50
Figure 2.2 Estimated diï¬€erence proï¬le: Shakespeare versus Sidney.
2.5
Remarks
By the U-statistic construction, Zu,v of (2.10) exists and is unbiased only if m =
u + v is less or equal to the sample size n. However, for Corollary 2.1 to hold,
m = u + v must further satisfy 2u + 2v âˆ’1 â‰¤n or u + v â‰¤(n + 1)âˆ•2 due to the
required existence of Z2uâˆ’1,2v, Z2u,2vâˆ’1, and Z2u+1,2vâˆ’2. This is indeed a restric-
tion on the choices of u and v in practice. However, it must be noted that for a
suï¬ƒciently large n, any one ğœu,v is estimable.
Since Simpson (1949), research on diversity has proliferated. However, even
today diversity or biodiversity is still an unsettled concept. There are quite a few
diï¬€erent schools of thought on what diversity is and how it should be precisely
deï¬ned. Consequently, a universally accepted diversity index has not yet come
to be although several well-known diversity indices are quite popular among
practitioners, for example, the Giniâ€“Simpson index ğœ1,1, Shannonâ€™s entropy by
Shannon (1948), RÃ©nyiâ€™s entropy by RÃ©nyi (1961), and Tsallisâ€™ index by Tsal-
lis (1988), among others. The indices in (2.3), while a natural generalization of
Simpsonâ€™s diversity index, have far-reaching implications beyond the realm of
diversity. For reasons to be seen more clearly in subsequent chapters, the subset
of the panel, namely {ğœv; v â‰¥1}, captures all the information about the under-
lying distribution {pk}, particularly the tail information. Although this fact is

Estimation of Simpsonâ€™s Indices
67
made clear in Theorem 2.2, the importance of estimability of ğœv, up to v = n âˆ’1,
cannot be overstated.
The perspective on {pk; k â‰¥1} by way of {ğœv; v â‰¥1} oï¬€ers several advantages
in certain situations. However, when it comes to statistical estimation based
on a sample, it is not without a cost. For example, for every k, there exists an
unbiased estimator of pk, namely Ì‚pk = Ykâˆ•n. (In fact, it is an unvue.) In other
words, the entire distribution {p1,â€¦} is estimable. This is however not the case
for {ğœv; v â‰¥1}. Given an iid sample of size n, only the ï¬rst n âˆ’1 components in
{ğœv; v â‰¥1} are estimable. For example, ğœn is not estimable. The following is an
argument to support the claim.
For any n,
ğœn =
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)n
=
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)nâˆ’1 âˆ’
âˆ‘
kâ‰¥1
p2(1 âˆ’pk)nâˆ’1
=
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)nâˆ’1 âˆ’
âˆ‘
kâ‰¥1
p2
k(1 âˆ’pk)nâˆ’2 +
âˆ‘
kâ‰¥1
p3
k(1 âˆ’pk)nâˆ’2
=
n
âˆ‘
v=1
[
(âˆ’1)vâˆ’1 âˆ‘
kâ‰¥1
pv
k(1 âˆ’pk)nâˆ’v
]
+ (âˆ’1)n+1 âˆ‘
kâ‰¥1
pn+1
k
.
(2.27)
By the existence of Zn,0, Znâˆ’1,1,â€¦, Z1,nâˆ’1, the sum in the ï¬rst term of the
last expression of (2.27) is estimable. Therefore, if ğœn were estimable, then the
last term of (2.27), ğœn+1,0 = âˆ‘
kâ‰¥1pn+1
k
, would be estimable. In the following text,
it is shown that ğœn+1,0 is not estimable based on a sample of size n. In fact, it
is to be shown, more generally, that ğœ(c)
n+1,0 = âˆ‘
kâ‰¥1ckpn+1
k
, where {ck; k â‰¥1} are
known coeï¬ƒcients with at least one nonzero element, is not estimable based
on a sample of size n.
First consider a ï¬nite alphabet containing K â‰¥2 letters. Suppose ğœ(c)
n+1,0
were estimable, that is, there existed an unbiased estimator Ì‚ğœ(c)
n+1,0(X) where
X = (X1,â€¦, Xn)â€² and each Xi takes one of the K letters in the alphabet
according to {pk}, such that
E
(
Ì‚ğœ(c)
n+1,0(X)
)
=
âˆ‘
kâ‰¥1
ckpn+1
k
.
(2.28)
The left-hand side of (2.28) is of the form
âˆ‘
âˆ—
Ì‚ğœ(c)
n+1,0(x)P(X = x),
(2.29)
where the summation âˆ‘
âˆ—is over all Kn possible conï¬gurations of a sample of
size n and x is a realization of X. Equation (2.29) is a K-variable (p1,â€¦, pK) poly-
nomial of degree n. For (2.28) to hold for all possible {pk; k â‰¥1} on the ï¬nite
alphabet, the coeï¬ƒcients of the corresponding terms must be equal. In particu-
lar, (c1,â€¦, cK)â€² = (0,â€¦, 0)
â€², which contradicts the assumption that at least one

68
Statistical Implications of Turingâ€™s Formula
of them is nonzero. This establishes the claim that ğœn = âˆ‘
kâ‰¥1pk(1 âˆ’pk)n is not
estimable based on an iid sample of size n when the alphabet is ï¬nite. This result
can be immediately extended from the case of a ï¬nite alphabet to the case of a
countable alphabet since the former is a special case of the latter.
With a cost of the perspective by way of {ğœv; v â‰¥1} established earlier, why
would one then prefer {ğœv; v â‰¥1} over {pk; k â‰¥1}? The answer to the ques-
tion lies in the fact that, in Statistics, often the object of estimation is not p =
{pk; k â‰¥1} itself but a functional of it, say F(p). In estimating certain types of
F(p), the estimability of the entire p becomes much less important than that of
the ï¬rst n âˆ’1 component of the {ğœv; v â‰¥1}.
For an example, suppose the object of estimation is the Giniâ€“Simpson index,
that is,
F(p) = ğœ1 =
âˆ‘
kâ‰¥1
pk(1 âˆ’pk).
The estimability of {pk; k â‰¥1} does not directly translate to that of F(p), by
means of, say, the plug-in
Ì‚F(p) = F(Ì‚p) =
âˆ‘
kâ‰¥1
Ì‚pk(1 âˆ’Ì‚pk),
which is an estimator with a sizable negative bias. It is not surprising that the
plug-in estimator has a sizable bias because when the cardinality of the alpha-
bet, K, is relatively large compared to the sample size n, many (possibly inï¬nite)
letters in the alphabet would not be represented in the sample. Consequently,
the corresponding Ì‚pks would be zero and therefore causing Ì‚pk(1 âˆ’Ì‚pk) = 0, all
the while the sum of pk(1 âˆ’pk)s of the corresponding indices ks could accumu-
late to be a signiï¬cant value. Therefore, the plug-in F(Ì‚p) underestimates F(p).
On the other hand, the partial estimability of {ğœv; v â‰¥1} directly translates to
that of F(p), that is, E(Z1,1) = F(p) = ğœ1.
In summary, Turingâ€™s perspective by way of {ğœv; v â‰¥1} is not necessarily a
better perspective than that by way of {pk; k â‰¥1} in general. However, in situa-
tions where tail characteristics play a greater role, Turingâ€™s perspective leads to a
more eï¬ƒcient way to amplify and to extract tail-relevant statistical information.
2.6
Exercises
1
Let X be a Bernoulli random variable with parameter p âˆˆ(0, 1), that is,
P(X = 1) = p and P(X = 0) = 1 âˆ’p. Find the ï¬rst to the fourth central
moments of X.
2
Let X be a Poisson random variable with parameter ğœ†> 0, that is, pk =
P(X = k) = eâˆ’ğœ†ğœ†kâˆ•k! for k = 0, 1, Â· Â· Â·. Find
E[X(X âˆ’1)(X âˆ’2) Â· Â· Â· (X âˆ’k + 1)]
for any positive integer k.

Estimation of Simpsonâ€™s Indices
69
3
An experiment involves repeatedly tossing together a fair coin and bal-
anced die until a â€œ3â€ appears on the die. Let Y be the number of tosses
needed to see the ï¬rst â€œ3â€ on the die, and let X be the total number of
heads observed on the coin when the experiment ends.
a) Find the probability distribution of Y and calculate E(Y).
b) Find the probability distribution of X and calculate E(X).
4
Let X1 be a binomial random variable with parameters n1 = 5 and p1 =
1âˆ•2; and let X2 be an independent binomial random variable with param-
eters n2 = 5 and p2 = 1âˆ•4. Find the conditional probability P(X1 = 2|X1 +
X2 = 4).
5
If the eï¬€ective cardinality of ğ’³is ï¬nite, that is, K < âˆ, show that the
Giniâ€“Simpson index ğœ1,1 of (2.1) is maximized at the uniform distribution,
that is, pk = 1âˆ•K for every k, k = 1,â€¦, K.
6
Derive (2.5).
7
For any probability sequence {pk; â‰¥1} on ğ’³, show that {pk; â‰¥1} is equiv-
alent in permutation to {ğœu,v; u â‰¥1} where v is any ï¬xed nonnegative
integer.
8
Show that Ì‚ğœ(m)
u,v given in (2.8) is an unbiased estimator of ğœu,v.
9
Show that, in case of u â‰¥1 and v = 0, Zu,v of (2.10) reduces to Zu,v of (2.11).
10
Verify (2.14).
11
Show that for any pair of u â‰¥1 and v â‰¥0, as n â†’âˆ,
[n âˆ’(u + v)]!nu+v
n!
â†’1.
12
Suppose the cardinality of ğ’³is a known and ï¬nite integer K â‰¥2. Show
that {Ì‚pk; k â‰¥1} is a set of jointly complete and suï¬ƒcient statistics.
13
For every pair of (u, v), where u â‰¥1 and v â‰¥0, show that
ğœu,v =
u
âˆ‘
i=1
(u âˆ’1
i âˆ’1
)
(âˆ’1)uâˆ’iğœ1,u+vâˆ’i.
14
The estimator Ì‚ğœâˆ—
u,v of ğœu,v is a mle when the cardinality K is ï¬nite and
known. When K is ï¬nite but unknown, the estimator Ì‚ğœâˆ—
u,v is still well
deï¬ned, but is no longer an mle of ğœu,v (why?). In this case, it is called the

70
Statistical Implications of Turingâ€™s Formula
plug-in estimator of ğœu,v. Show in case of an unknown but ï¬nite K, the
estimator retains its asymptotic eï¬ƒciency.
15
Verify (2.22).
16
Verify the entries of Table 2.3.
17
Verify the entries of Table 2.4.
18
Show that the plug-in estimator
Ì‚ğœâˆ—
1,1 =
âˆ‘
kâ‰¥1
Ì‚pk(1 âˆ’Ì‚pk)
of ğœ1,1 is a biased estimator.
19
Show that
a) when u = v = 1, ğœ2
1(u, v) > 0 if and only if {pk} is not a uniform distri-
bution on ğ’³;
b) when u â‰¥1 and v = 0, ğœ2
1(u, 0) > 0 if and only if {pk} is not a uniform
distribution on ğ’³.
(Hint: Consider the case when h1(x1) is a constant.)
20
Starting at 0, a random walk on the x-axis takes successive one-unit steps,
going to the right with probability p and to the left with probability 1 âˆ’p.
The steps are independent. Let X denote the position after n steps. Find
the distribution of X.
21
Let X1 and X2 be, respectively, binomial random variables with parameters
(n, p1) and (n, p2). Show that, if p1 < p2,
P(X1 â‰¤k) â‰¥P(X2 â‰¤k)
for every k, k = 0, 1,â€¦, n.
22
Let X1,â€¦, Xm,â€¦, Xn be a sample of iid random variable X with E(X) =
ğœ‡< âˆ. Let Sm = âˆ‘m
i=1 Xi and Sn = âˆ‘n
i=1 Xi, where n â‰¥m.
a) Find E(Sn|Sm = sm).
b) Find E(Sm|Sn = sn).
23
Suppose that X is a Poisson random variable with intensity parameter
ğœ†X > 0 and that, given X = k, Y is a binomial random variable with param-
eters n = k and p > 0. Show that Y is a Poisson random variable with
intensity parameter ğœ†Y = ğœ†Xp.

71
3
Estimation of Shannonâ€™s Entropy
Let ğ’³= {ğ“k; k â‰¥1} be a countable alphabet and p = {pk; k â‰¥1} be an
associated probability distribution of the alphabet. Shannon (1948) deï¬ned
entropy of p in the form of HS = âˆ’âˆ‘
kâ‰¥1pklog2 pk. Often Shannonâ€™s entropy is
also expressed with the natural logarithm,
H = âˆ’
âˆ‘
kâ‰¥1
pk ln pk,
(3.1)
which diï¬€ers from HS by a constant factor, H = HS ln 2. Throughout this book,
H of (3.1) is referred to as Shannonâ€™s entropy. H is of profound importance
in many branches of modern data sciences. In recent decades as entropy
ï¬nds its way into a wider range of applications, the estimation of entropy
also becomes increasingly important. The volume of literature on the general
topic of entropy estimation is quite large and covers a wide range of models
with diï¬€erent stochastic structures. The two most commonly cited review
articles are by Beirlant, Dudewicz, GyÃ¶rï¬, and Meulen (2001) and Paninski
(2003). While Beirlant et al. (2001) include results under a broader range of
models, Paninski (2003) mainly focuses on the issues of entropy estimation
on a countable alphabet, the most basic and therefore the most fundamental
model for entropy estimation.
Entropy estimation is generally considered a diï¬ƒcult problem regardless
of the domain type for the underlying distribution. The cause of diï¬ƒculty is
twofold. First, Shannonâ€™s entropy is a measure sensitive to small perturbations
on the probabilities in the tail, as evidenced by
d(âˆ’p ln p)
dp
= âˆ’ln p âˆ’1,
which assumes a large positive value for small p. This sensitivity could amplify
ordinary sampling errors at lower probability region of the sampling space into
signiï¬cant variability in estimation. Second, a sample of size n, however large
it may be, could often have a hopelessly poor coverage of the alphabet. If the
cardinality of ğ’³is inï¬nite, then that of the subset not covered by a sample
is always inï¬nite. Even for a ï¬nite alphabet, the cardinality of the subset not
Statistical Implications of Turingâ€™s Formula, First Edition. Zhiyi Zhang.
Â© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

72
Statistical Implications of Turingâ€™s Formula
covered by a sample is often much larger than that of the subset covered by the
sample. In light of this fact, it is clear that a good entropy estimator must ï¬nd
a way to utilize certain characteristics of the probability distribution over the
not-covered subset of the alphabet.
Two fundamental questions are immediately raised. The ï¬rst question is
whether such information exists in a ï¬nite sample; and the second question is,
if it does, whether it can be extracted nonparametrically. After an overview
on the mainlines of research in entropy estimation during the past decades, a
nonparametric entropy estimator in Turingâ€™s perspective is constructed; some
of its statistical properties are established; and, in turn, the nonparametric
entropy estimator in Turingâ€™s perspective will give an aï¬ƒrmative answer to
each of the above-mentioned two questions.
3.1
A Brief Overview
Let {Yk} = {Yk; k â‰¥1} be the sequence of observed counts of letters of the
alphabet in an iid sample of size n and let {Ì‚pk = Ykâˆ•n}. The general nonpara-
metric estimator that plays a central role in the literature is commonly known
as the plug-in estimator and is given by
Ì‚H = âˆ’
âˆ‘
kâ‰¥1
Ì‚pk ln Ì‚pk.
(3.2)
The plug-in estimator is appealing to practitioners since it is simple, intu-
itive, easily calculated, and asymptotically eï¬ƒcient in some cases (when it
is a maximum likelihood estimator). However, the plug-in estimator has a
well-established issue in large and complex bias. For example, according to
Harris (1975), when ğ’³is of ï¬nite cardinality K < âˆ, the bias of Ì‚H up to the
second order is of the form
E ( Ì‚H âˆ’H) = âˆ’K âˆ’1
2n
+
1
12n2
(
1 âˆ’
K
âˆ‘
k=1
1
pk
)
+ îˆ»(nâˆ’3) .
(3.3)
The two immediate features of the ï¬rst-order term in (3.3) are as follows:
1) Ì‚H tends to underestimate H since the term is negative; and
2) in case of a large K relative to the sample size n, the magnitude of the bias
could be substantial.
There are plenty of situations in application where a relatively large K meets
a relatively small n. For example, there are more than 50000 (K > 50000) com-
mon English word types in current usage and a typical sample text, for example,
a web page, often contains less than 300 words (n < 300). For another example,
there are approximately 35000 gene types (K â‰ˆ35000) in the human genome
and genetic studies often have sample sizes in hundreds (100 < n < 500).

Estimation of Shannonâ€™s Entropy
73
In fact, the complexity of the bias in (3.3) is beyond the ï¬rst-order term. Just
for a moment, suppose the value of K is ï¬nite and known a priori. Then one
could adjust Ì‚H by letting
Ì‚H+ = Ì‚H + K âˆ’1
2n
to obtain an estimator with a bias decaying at the rate of îˆ»(nâˆ’2). Unfortunately
such an adjustment does not solve the general problem because the next leading
term is
1
12n2
(
1 âˆ’
K
âˆ‘
k=1
1
pk
)
which, given n, could assume a hugely negative value if just for one index k,
0 < pk â‰ª1. Similar observations can be made into higher order terms of (3.3)
for K < âˆ. When K is countably inï¬nite, much less is known about the bias of
the plug-in estimator in (3.2).
Historically much of the research on entropy estimation has been inspired by
the practical need to reduce the bias of the plug-in estimator. A large number
of estimators have been proposed. Many of these proposed estimators may be
roughly classiï¬ed into three categories: (i) correcting the bias of Ì‚H by estimating
K, (ii) jackknife, and (iii) Bayesian.
By estimating K and assuming a satisfactory estimator Ì‚K can be found, the
plug-in estimator may be naturally adjusted by means of an additive term
according to (3.3), that is,
Ì‚Hâˆ—= Ì‚H +
Ì‚K âˆ’1
2n .
However estimating K is a diï¬ƒcult statistical problem on its own. There are no
unbiased nonparametric estimators of K under general models; therefore, the
ï¬rst-order term in (3.3) cannot be entirely eliminated by such a simple adjust-
ment. In addition, the very action of estimating K implies the assumption that
the cardinality K of the alphabet ğ’³is ï¬nite. This assumption puts a concep-
tual limitation on the models for which such estimator can be justiï¬ably used.
Nevertheless, the general concept of such approaches is practically sound. For
an introduction to the statistical problem of estimating K, interested readers
may wish to refer to Chao (1987) and Bunge, Willis, and Walsh (2014).
A popular and frequently cited entropy estimator in this category is the one
proposed by Miller (1955), also known as the Millerâ€“Madow estimator, which
uses
Ì‚m =
âˆ‘
kâ‰¥1
1[Yk > 0],
the number of covered letters in the sample, in place of Ì‚K, that is,
Ì‚HMM = Ì‚H + Ì‚m âˆ’1
2n
.
(3.4)

74
Statistical Implications of Turingâ€™s Formula
The jackknife methodology is another commonly used technique to reduce
bias of the plug-in estimator. The basic elements of the jackknife estimator are
1) to construct n plug-in estimators based on a subsample of size n âˆ’1 by
leaving one (the ith) observation out, Ì‚H(i), which has, based on (3.3), a bias
with a leading ï¬rst-order term
âˆ’K âˆ’1
2(n âˆ’1);
2) to obtain Ì‚H(i) = n Ì‚H âˆ’(n âˆ’1) Ì‚H(i) for i = 1,â€¦, n; and then
3) to deï¬ne the jackknife estimator by
Ì‚HJK =
âˆ‘n
i=1 Ì‚H(i)
n
= Ì‚H + n âˆ’1
n
n
âˆ‘
i=1
( Ì‚H âˆ’Ì‚H(i)) .
(3.5)
The jackknife methodology was ï¬rst applied to the plug-in estimator by
Zahl (1977). It may be veriï¬ed that the bias of Ì‚HJK is
E ( Ì‚HJK âˆ’H) = âˆ’
1
12n(n âˆ’1)
(
1 âˆ’
K
âˆ‘
k=1
1
pk
)
+ îˆ»(nâˆ’3),
(3.6)
speciï¬cally noting that the leading term is positive and decays at the rate of
îˆ»(nâˆ’2) (see Exercise 4). Another noteworthy methodology in this area is that
by Strong, Koberle, de Ruyter van Steveninck, and Bialek (1998), where an inter-
esting extension of the jackknife procedure can be found to correct bias simul-
taneously for several terms of diï¬€erent orders in (3.3).
Another important group of entropy estimators is based on Bayesian
perspectives. Consider the underlying probability distribution {p1,â€¦, pK} as
a point in the K-dimensional unit cube satisfying the following constraints:
1) pk âˆˆ(0, 1) for i = 1,â€¦, K, and
2) âˆ‘K
k=1 pk = 1.
A prior distribution is imposed on {p1,â€¦, pK} in the form of a Dirichlet den-
sity function
f (p1,â€¦, pK; ğ›¼1,â€¦, ğ›¼K) =
1
B(ğ›¼)
K
âˆ
k=1
pğ›¼kâˆ’1
k
(3.7)
where ğ›¼= (ğ›¼1,â€¦, ğ›¼K)ğœis such that ğ›¼k > 0 for each k, k = 1,â€¦, K;
B(ğ›¼) =
âˆK
k=1 Î“(ğ›¼k)
Î“
(âˆ‘K
k=1 ğ›¼k
) ;

Estimation of Shannonâ€™s Entropy
75
and Î“(â‹…) is the gamma function. Under the prior distribution in (3.7), the
posterior is also a Dirichlet distribution in the form of
f (p1,â€¦, pK|y1,â€¦, yK) =
1
B(y + ğ›¼)
K
âˆ
k=1
p(yk+ğ›¼k)âˆ’1
k
(3.8)
where y = (y1,â€¦, yK)ğœis a realization of the observed letter frequencies in the
sample. Under (3.8), the posterior mean for each pk is
Ì‚pâˆ—
k =
yk + ğ›¼k
n + âˆ‘K
k=1 ğ›¼k
,
and, in turn, the Bayes estimator of entropy is given by the plug-in form
Ì‚HBayes = âˆ’
K
âˆ‘
k=1
Ì‚pâˆ—
k ln Ì‚pâˆ—
k.
(3.9)
For various sets of preï¬xed {ğ›¼1,â€¦, ğ›¼K}, this group of entropy estimators
includes those proposed by Krichevsky and Troï¬mov (1981), Holste, GroÃŸe,
and Herzel (1998), and SchÃ¼rmann and Grassberger (1996), among many
others.
The Dirichlet prior, while a natural conjugate to {pk; k = 1,â€¦, K}, is by no
means the only reasonable family of priors to be used. It may be modiï¬ed or
extended in many ways. For example, Nemenman, Shafee, and Bialek (2002)
adopted a hierarchical Bayesian perspective by
1) reducing (3.7) to a one-parameter Dirichlet model by setting ğ›½= ğ›¼k for all
k, and
2) imposing a hyper-prior on the single hyper-parameter ğ›½, designed to reduce
the impact of observed data y when n is small.
The resulting estimator is known as the NSB estimator.
There is a long list of other notable entropy estimators that do not neces-
sarily fall into these categories. Examples are Hausser and Strimmer (2009)
for James-Stein type of shrinkage estimators; Chao and Shen (2003) and Vu,
Yu, and Kass (2007) for coverage-adjusted estimators; and Valiant and Valiant
(2011) and Zhang and Grabchak (2013) for general bias reduction. There is also
an interesting unpublished manuscript by Montgomery-Smith and SchÃ¼rmann
(2007) that established an unbiased entropy estimator under a sequential sam-
pling scheme. In summary, there is no shortage of proposed entropy estimators
in the literature and many more are certainly to come.
There are however several fundamental issues that are not addressed suï¬ƒ-
ciently and satisfactorily by the existing research on entropy estimation.
1) Convergence rates of most of the proposed estimators are largely unknown.
The majority of the research thus far focuses on bias reduction, and the
primary methodology of evaluating the performance is by simulations.
While simulations are important tools, they cannot replace theoretical

76
Statistical Implications of Turingâ€™s Formula
assessment of statistical procedures. The lack of understanding of various
convergence rates for most of the proposed estimators, for example,
in distribution, in bias, in variance, in mean squared error, etc., leaves
users vulnerable to potentially poor performances, with n of all sizes. As
certain as the fact that most of the proposed entropy estimators have their
respective merits, the best of them could be easily proven inadequate under
some underlying distributions {pk}.
2) Distributional characteristics of most of the proposed estimators are largely
unknown. It may serve as a reminder to practitioners everywhere that only
when the sampling distribution of an estimator is known (or can be approx-
imated in some sense) can one reasonably believe that the behavior of that
estimator is statistically understood.
3) What happens if the cardinality of the alphabet ğ’³is countably inï¬nite, that
is, K = âˆ‘
kâ‰¥11[pk > 0] = âˆ? The inspiration of bias reduction comes mainly
from the bias expression of (3.3). Consequently the probabilistic platforms
for developing bias-reduced estimators implicatively assume a ï¬nite K. In
fact, all of the estimators in the Bayesian category with a Dirichlet prior
require a known K for the argument to make sense. If K = âˆ, it is not clear
how the bias can be expressed and how it behaves under various distribu-
tional classes.
These statistical issues are at the very center of entropy estimation and
hence of information theory. Future research eï¬€orts must move toward
resolving them.
3.2
The Plug-In Entropy Estimator
The plug-in entropy estimator of (3.2) is a natural reference estimator for all
others. Despite its notoriety associated with a large bias when n is not suï¬ƒ-
ciently large, the plug-in estimator has quite a lot to oï¬€er. Firstly, when K is ï¬nite
and known, it is a maximum likelihood estimator. As such, it is asymptotically
eï¬ƒcient, that is essentially saying that when the sample is suï¬ƒciently large, no
other estimator could perform much better. Secondly, it oï¬€ers a relatively less
convoluted mathematical form and therefore makes the establishing of its sta-
tistical properties relatively easy. Thirdly, its various statistical properties would
shed much light on those of other estimators. It is fair to say that understanding
the statistical properties of the plug-in is a ï¬rst step toward understanding the
statistical properties of many other estimators.
3.2.1
When K Is Finite
Several statistical properties of the plug-in estimator of (3.2) are summarized
in this section.

Estimation of Shannonâ€™s Entropy
77
Theorem 3.1
Let H be as in (3.1) and let Ì‚H be as in (3.2) based on an iid
sample of size n. Suppose K < âˆ. Then
1) E( Ì‚H âˆ’H) = âˆ’Kâˆ’1
2n +
1
12n2
(
1 âˆ’âˆ‘K
k=1
1
pk
)
+ îˆ»(nâˆ’3);
2) Var( Ì‚H) = 1
n
(âˆ‘K
k=1 pkln2pk âˆ’H2)
+ Kâˆ’1
2n2 + îˆ»(nâˆ’3); and
3) E( Ì‚H âˆ’H)2 = 1
n
(âˆ‘K
k=1 pkln2pk âˆ’H2)
+ K2âˆ’1
4n2 + îˆ»(nâˆ’3).
The proof of Theorem 3.1 is found in Harris (1975). Theorems 3.2 and 3.3 are
due to Miller and Madow (1954).
Theorem 3.2
Let H be as in (3.1) and let Ì‚H be as in (3.2) based on an iid
sample of size n. Suppose K < âˆand {pk} is a nonuniform distribution. Then
âˆš
n( Ì‚H âˆ’H)
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2)
(3.10)
where
ğœ2 =
K
âˆ‘
k=1
pk (ln pk + H)2.
(3.11)
Proof: Let Ì‚pâˆ’= (Ì‚p1,â€¦, Ì‚pKâˆ’1)
â€²and pâˆ’= (p1,â€¦, pKâˆ’1)
â€². Noting
âˆš
n(Ì‚pâˆ’âˆ’pâˆ’)
L
âˆ’âˆ’âˆ’â†’MVN(âˆ…, Î£)
where âˆ…is a (K âˆ’1)-dimensional vector of zeros, Î£ = (ğœi,j) is a (K âˆ’1) Ã— (K âˆ’1)
covariance matrix with diagonal and oï¬€-diagonal elements being, respectively,
ğœi,i = pi(1 âˆ’pi),
ğœi,j = âˆ’pipj
for i = 1,â€¦, K âˆ’1, j = 1,â€¦, K âˆ’1, and i â‰ j. The result of the theorem follows
an application of the delta method with function H = g(pâˆ’). (The details of the
proof are left as an exercise; see Exercise 7.)
â—½
The condition of {pk} being a nonuniform distribution is required by
Theorem 3.2 because otherwise ğœ2 in (3.11) would be zero (see Exercise 6) and
consequently the asymptotic distribution degenerates.
Theorem 3.3
Let H be as in (3.1) and let Ì‚H be as in (3.2) based on an iid
sample of size n. Suppose K < âˆand pk = 1âˆ•K for each k, k = 1,â€¦, K. Then
2n(H âˆ’Ì‚H)
L
âˆ’âˆ’âˆ’â†’ğœ’2
Kâˆ’1
(3.12)
where ğœ’2
Kâˆ’1 is a chi-squared random variable with degrees of freedom K âˆ’1.

78
Statistical Implications of Turingâ€™s Formula
Proof: Noting the proof of Theorem 3.2, the result of the theorem follows an
application of the second-order delta method with function H = g(pâˆ’). (The
details of the proof are left as an exercise; see Exercise 8.)
â—½
Theorem 3.2 is important not only in its own right but also in passing asymp-
totic normality to other estimators beyond the plug-in. Many bias-adjusted
estimators in the literature have a general form of
Ì‚Hadj = Ì‚H + Ì‚B.
(3.13)
Corollary 3.1
Let Ì‚Hadj be as in (3.13). If
âˆš
nÌ‚B
p
âˆ’âˆ’âˆ’â†’0, then
âˆš
n ( Ì‚Hadj âˆ’H)
âˆšâˆ‘K
k=1 pkln2pk âˆ’H2
L
âˆ’âˆ’âˆ’â†’N(0, 1).
(3.14)
Proof: Noting ï¬rst
âˆš
n ( Ì‚Hadj âˆ’H) =
âˆš
n( Ì‚H âˆ’H) +
âˆš
n(Ì‚B)
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2)
and second
ğœ2 =
K
âˆ‘
k=1
pk(ln pk + H)2
=
K
âˆ‘
k=1
pk(ln2pk + 2 ln pkH + H2)
=
K
âˆ‘
k=1
pkln2pk âˆ’H2,
the desired result follows Slutskyâ€™s lemma.
â—½
Example 3.1
Consider the Millerâ€“Madow estimator
Ì‚HMM = Ì‚H + Ì‚m âˆ’1
2n
where the adjusting term of (3.13) is Ì‚B = ( Ì‚m âˆ’1)âˆ•(2n) â‰¥0. Noting, as n â†’âˆ,
0 â‰¤
âˆš
nÌ‚B =
âˆš
n
(
Ì‚m âˆ’1
2n
)
â‰¤K âˆ’1
2
âˆš
n
â†’0,
Ì‚B
a.s.
âˆ’âˆ’âˆ’â†’0, Ì‚B
p
âˆ’âˆ’âˆ’â†’0, and therefore the Millerâ€“Madow estimator admits the
asymptotic normality of Corollary 3.1.

Estimation of Shannonâ€™s Entropy
79
In order to use (3.14) of Corollary 3.1 for inference about entropy, one must
have not only a consistent estimator of entropy but also a consistent estimator
of
H2 =
K
âˆ‘
k=1
pkln2pk.
(3.15)
As will be seen in subsequent text, H2 appears in the asymptotic variance of
many entropy estimators and therefore is a key element of statistical inference
about H. It is suï¬ƒciently important to deserve a deï¬ned name.
Deï¬nition 3.1
For a probability distribution {pk; k â‰¥1} on alphabet ğ’³and
a positive integer r, the rth entropic moment is deï¬ned to be
Hr = (âˆ’1)r âˆ‘
kâ‰¥1
pklnrpk.
(3.16)
To give an intuitive justiï¬cation for the names of entropic moments of Deï¬-
nition 3.1, one may consider a random variable H deï¬ned by a given {pk}, with
no multiplicities among the probabilities, in the following tabulated form:
H
âˆ’ln p1
âˆ’ln p2
Â· Â· Â·
âˆ’ln pk
Â· Â· Â·
P(H)
p1
p2
Â· Â· Â·
pk
Â· Â· Â·
The ï¬rst moment of H gives entropy H = E(H) = âˆ’âˆ‘
kâ‰¥1pk ln pk and the
second moment of H gives H2 = E(H2) = âˆ‘
kâ‰¥1pkln2pk, etc.
Remark 3.1
A cautionary note must be taken with the random variable H. If
there is no multiplicity among the probabilities in {pk}, then
Var(H) =
âˆ‘
kâ‰¥1
pkln2pk âˆ’H2.
However, if there exists at least one pair of (i, j), i â‰ j, such that pi = pj, then H
is not a well-deï¬ned random variable, and neither therefore are the notions of
E(H) and Var(H). This point is often overlooked in the existing literature.
To estimate H2 is however not easy. Its plug-in form of
Ì‚H2 =
âˆ‘
kâ‰¥1
Ì‚pkln2 Ì‚pk
suï¬€ers all the same drawbacks of Ì‚H in estimating H. In fact, the drawbacks are
much worse, exacerbated by the heavier weights of wk = ln2pk in comparison
to wk = âˆ’ln pk in the weighted average of the probabilities, that is, âˆ‘
kâ‰¥1wkpk.
In a relatively small sample, the smaller sample coverage causes tendency in Ì‚H2

80
Statistical Implications of Turingâ€™s Formula
to underestimate H2, to underestimate the asymptotic variance, and in turn to
inï¬‚ate the rate of Type I error (false positive) in hypothesis testing.
In summary, sound inferential procedures about entropy must be supported
by good estimators of not only entropy H but also the second entropic moment
H2.
The eï¬€ective cardinality K = âˆ‘
kâ‰¥11[pk > 0] of {pk; k = 1,â€¦, K} may be
extended to K(n) of {pn,k; k = 1,â€¦, K(n)}, where K(n) changes dynamically
as n increases. In this case, the corresponding entropy is more appropriately
denoted by Hn as a quantity varying with n. Some of the statistical properties
established above for ï¬xed K may also be extended accordingly. The following
theorem is due to Zubkov (1973).
Theorem 3.4
Let {pn,k; k = 1,â€¦, K(n)} be a probability distribution on ğ’³
and let Hn be its entropy. Suppose there exist two ï¬xed constants Îµ and ğ›¿, satis-
fying Îµ > ğ›¿> 0, such that, as n â†’âˆ,
1)
n1âˆ’Îµ
K(n)
(K(n)
âˆ‘
k=1
pn,kln2pn,k âˆ’H2
n
)
â†’âˆ,
2)
max
1â‰¤kâ‰¤K(n)
{
1
npn,k
}
= îˆ»
(K(n)
n1âˆ’ğ›¿
)
,
and
3)
K(n)
âˆš
âˆš
âˆš
âˆšn
(
K(n)
âˆ‘
k=1
pn,kln2pn,k âˆ’H2
n
) â†’0.
Then
âˆš
n ( Ì‚H âˆ’Hn
)
âˆšâˆ‘K(n)
k=1 pn,k ln2pn,k âˆ’H2
n
L
âˆ’âˆ’âˆ’â†’N(0, 1).
(3.17)
Paninski (2003) established the same asymptotic normality under a slightly
weaker suï¬ƒcient condition as stated in the theorem below.
Theorem 3.5
Let {pn,k; k = 1,â€¦, K(n)} be a probability distribution on ğ’³
and let Hn be its entropy. Suppose there exists a ï¬xed constant Îµ > 0, such that,
as n â†’âˆ,

Estimation of Shannonâ€™s Entropy
81
1)
K(n)
âˆš
n
â†’0,
and
2)
lim inf
nâ†’âˆn1âˆ’Îµ
(K(n)
âˆ‘
k=1
pn,kln2pn,k âˆ’H2
n
)
> 0.
Then
âˆš
n( Ì‚H âˆ’Hn)
âˆšâˆ‘K(n)
k=1 pn,kln2pn,k âˆ’H2
n
L
âˆ’âˆ’âˆ’â†’N(0, 1).
(3.18)
The proofs of both Theorems 3.4 and 3.5 are not presented here. Interested
readers may refer to Zubkov (1973) and Paninski (2003), respectively, for
details.
3.2.2
When K Is Countably Inï¬nite
When there are inï¬nitely many positive probabilities in {pk; k â‰¥1}, the ï¬rst
issue encountered is whether entropy is ï¬nite. The convergence of
âˆ’
m
âˆ‘
k=1
pk ln pk
depends on the rate of decay of pk as k increases. For H to be ï¬nite, âˆ’pk ln pk
must decay suï¬ƒciently fast or equivalently pk must decay suï¬ƒciently fast.
Example 3.2
Let, for k = 1, 2,â€¦ ,
pk = Cğœ†
kğœ†,
where ğœ†> 1 and Cğœ†> 0 are two ï¬xed real numbers. In this case, H < âˆ.
The details of the argument for Example 3.2 are left as an exercise (see
Exercise 9).
Example 3.3
Let, for k = 1, 2,â€¦ ,
pk =
1
C(k + 1)ln2(k + 1)

82
Statistical Implications of Turingâ€™s Formula
where the constant C > 0 is such that âˆ‘
kâ‰¥1pk = 1:
H = âˆ’
âˆ‘
kâ‰¥1
pk ln pk
=
âˆ‘
kâ‰¥1
(âˆ’ln pk)pk
=
âˆ‘
kâ‰¥1
ln(C(k + 1) ln2(k + 1))
C(k + 1) ln2(k + 1)
=
âˆ‘
kâ‰¥1
ln C + ln(k + 1) + 2 ln ln(k + 1)
C(k + 1) ln2(k + 1)
= ln C +
âˆ‘
kâ‰¥1
1
C(k + 1) ln(k + 1)
+
âˆ‘
kâ‰¥1
2 ln ln(k + 1)
C(k + 1) ln2(k + 1)
=
(
ln C + ln ln 2
Cln22
)
+
âˆ‘
kâ‰¥1
1
C(k + 1) ln(k + 1)
+
âˆ‘
kâ‰¥2
2 ln ln(k + 1)
C(k + 1) ln2(k + 1)
.
Noting that
ln C + ln ln 2
Cln22
is a ï¬xed number,
âˆ‘
kâ‰¥2
2 ln ln(k + 1)
C(k + 1) ln2(k + 1)
is a sum of positive terms, and by the integral test for series convergence,
lim
mâ†’âˆ
m
âˆ‘
k=1
1
(k + 1) ln(k + 1) = âˆ.
The result H = âˆfollows.
However, for the purpose of statistical estimation of entropy, only the distri-
butions with ï¬nite entropy are considered. In subsequent text, unless otherwise
speciï¬ed, the general discussion is within the class of all {pk; k â‰¥1} with ï¬nite
entropy, that is,
ğ’«=
{
{pk; k â‰¥1} âˆ¶H = âˆ’
âˆ‘
kâ‰¥1
pk ln pk < âˆ
}
.

Estimation of Shannonâ€™s Entropy
83
Several important statistical properties of Ì‚H are established in Antos and
Kontoyiannis (2001) and are summarized in the following theorem without
proofs.
Theorem 3.6
For any {pk} âˆˆğ’«, as n â†’âˆ,
1) E( Ì‚H âˆ’H)2 â†’0;
2) for all n, E( Ì‚H) â‰¤H; and
3) for all n,
Var( Ì‚H) â‰¤ln2n
nln22
.
(3.19)
Part 1 of Theorem 3.6 implies comforting assurance that Ì‚H converges to H in
mean square error (MSE), which in turn implies |E( Ì‚H âˆ’H)| â†’0, but does not
say anything about its convergence rate. Part 3 gives an upper bound of Var( Ì‚H),
which decays at the rate of îˆ»(ln2nâˆ•n). Under the general class ğ’«, there does not
seem to be an established upper bound for E( Ì‚H âˆ’H)2 in the existing literature.
Given the upper bound for Var( Ì‚H) in Part 3, it is easy to see that the lack of
upper bound for E( Ì‚H âˆ’H)2 is due to that for the bias, |E( Ì‚H âˆ’H)|.
Antos and Kontoyiannis (2001) also provided another interesting result as
stated in Theorem 3.7.
Theorem 3.7
Let Ì‚Hâˆ—be any entropy estimator based on an iid sample of size
n. Let an > 0 be any sequence of positive numbers converging to zero. Then there
exists a distribution {pk} âˆˆğ’«such that
lim sup
nâ†’âˆ
E(| Ì‚Hâˆ—âˆ’H|)
an
= âˆ.
Theorem 3.7 may be stated in a more intuitive way: given a positive sequence
0 < an â†’0 with a decaying rate however slow, there exists a distribution {pk} âˆˆ
ğ’«such that E(| Ì‚Hâˆ—âˆ’H|) decays with a slower rate. This theorem is sometimes
known as the slow convergence theorem of entropy estimation. It may be beneï¬-
cial to call attention to two particular points of the theorem. First, the estimator
Ì‚Hâˆ—is referring to not only the plug-in estimator but any arbitrary estimator. In
this sense, the so-called slow convergence is not a consequence of the estima-
tor Ì‚Hâˆ—but of the estimand H. Second, the criterion of convergence used is the
mean absolute error (MAE) E(| Ì‚Hâˆ—âˆ’H|), and not the bias |E( Ì‚Hâˆ—âˆ’H)|. There-
fore, the nonexistence of a uniformly decaying upper bound for E(| Ì‚Hâˆ—âˆ’H|) on
ğ’«does not necessarily imply the same for the bias |E( Ì‚Hâˆ—âˆ’H)|. Of course this
by no means suggests that such an upper bound would exist for some Ì‚Hâˆ—.
Naturally, convergence rates could be better described in subclasses of ğ’«.
Antos and Kontoyiannis (2001) studied distributions of the types pk = Ckâˆ’ğœ†

84
Statistical Implications of Turingâ€™s Formula
where ğœ†> 1 and pk = C(klnğœ†k)âˆ’1 where ğœ†> 2 and gave their respective rates
of convergence.
Zhang and Zhang (2012) established a normal law for the plug-in estimator
under a distribution {pk} on a countably inï¬nite alphabet. The result is stated
in Theorem 3.8.
Theorem 3.8
Let {pk; k â‰¥1} âˆˆğ’«be a probability distribution satisfying
ğœ2 = âˆ‘
kâ‰¥1pkln2pk âˆ’H2 < âˆ. If there exists an integer-valued function K(n)
such that as, n â†’âˆ,
1) K(n) â†’âˆ,
2) K(n) = o(
âˆš
n), and
3)
âˆš
nâˆ‘
kâ‰¥K(n)pk ln pk â†’0,
then âˆš
n( Ì‚H âˆ’H)
ğœ
L
âˆ’âˆ’âˆ’â†’N(0, 1).
Let
Ì‚ğœ2 =
âˆ‘
kâ‰¥1
Ì‚pkln2 Ì‚pk âˆ’Ì‚H2 < âˆ
(3.20)
or any other consistent estimator of ğœ2 = âˆ‘
kâ‰¥1pkln2pk âˆ’H2 < âˆ. The follow-
ing corollary, also due to Zhang and Zhang (2012), provides a means of large
sample inference on H.
Corollary 3.2
Under the same condition of Theorem 3.8,
âˆš
n( Ì‚H âˆ’H)
Ì‚ğœ
L
âˆ’âˆ’âˆ’â†’N(0, 1).
Example 3.4
Let pk = Cğœ†kâˆ’ğœ†. The suï¬ƒcient condition of Theorem 3.8 holds for
ğœ†> 2 but not for ğœ†âˆˆ(1, 2).
To support the claim of Example 3.4, consider
âˆš
n
âˆ‘
kâ‰¥K(n)
(âˆ’pk ln pk)
âˆ¼âˆ’
âˆš
n âˆ«
âˆ
K(n)
Cğœ†
xğœ†ln
(Cğœ†
xğœ†
)
dx
=
Cğœ†ğœ†
(ğœ†âˆ’1)
âˆš
n ln K(n)
(K(n))ğœ†âˆ’1 +
[
Cğœ†ğœ†
(ğœ†âˆ’1)2 âˆ’Cğœ†ln Cğœ†
ğœ†âˆ’1
]
âˆš
n
(K(n))ğœ†âˆ’1
(see Exercise 10)
âˆ¼
Cğœ†ğœ†
(ğœ†âˆ’1)
âˆš
n ln K(n)
(K(n))ğœ†âˆ’1 ,

Estimation of Shannonâ€™s Entropy
85
where â€œâˆ¼â€ indicates equality in convergence or divergence rate of two
sequences.
If ğœ†> 2, letting K(n) âˆ¼n1âˆ•ğœ†,
Cğœ†ğœ†
(ğœ†âˆ’1)
âˆš
n ln K(n)
(K(n))ğœ†âˆ’1
âˆ¼
Cğœ†
(ğœ†âˆ’1)
ln n
n1âˆ•2âˆ’1âˆ•ğœ†â†’0.
If 1 < ğœ†â‰¤2, for any K(n) âˆ¼o(
âˆš
n) and a suï¬ƒciently large n,
Cğœ†ğœ†
(ğœ†âˆ’1)
âˆš
n ln K(n)
(K(n))ğœ†âˆ’1
>
Cğœ†ğœ†
(ğœ†âˆ’1)
âˆš
n
n1âˆ•2âˆ’1âˆ•ğœ†=
Cğœ†ğœ†
(ğœ†âˆ’1)n1âˆ•ğœ†â†’âˆ.
Example 3.5
Let pk = Cğœ†eâˆ’ğœ†k. The suï¬ƒcient condition of Theorem 3.8 holds.
This is so because, letting K(n) âˆ¼ğœ†âˆ’1 ln n, for a suï¬ƒciently large n,
âˆš
n
âˆ‘
kâ‰¥K(n)
(âˆ’pk ln pk)
âˆ¼âˆ’
âˆš
n âˆ«
âˆ
ln n1âˆ•ğœ†Cğœ†eâˆ’ğœ†x ln (Cğœ†eâˆ’ğœ†x) dx
âˆ¼Cğœ†
ğœ†
ln n
âˆš
n
â†’0.
Example 3.6
Let pk = C(k2ln2k)âˆ’1. The suï¬ƒcient condition of Theorem 3.8
holds.
Let K(n) âˆ¼
âˆš
nâˆ•ln ln n. For a suï¬ƒciently large n, consider
âˆš
n
âˆ‘
kâ‰¥K(n)
(âˆ’pk ln pk)
âˆ¼
âˆš
nC âˆ«
âˆ
K(n)
2 ln x + 2 ln ln x âˆ’ln C
x2ln2x
dx
âˆ¼2
âˆš
nC âˆ«
âˆ
K(n)
1
x2 ln xdx
â‰¤
2C
âˆš
n
K(n) ln K(n) â†’0.
Example 3.7
Let pk = C(k2 ln k)âˆ’1. The suï¬ƒcient condition of Theorem 3.8
does not hold.

86
Statistical Implications of Turingâ€™s Formula
For any K(n) âˆ¼o(
âˆš
n) and a suï¬ƒciently large n,
âˆš
n
âˆ‘
kâ‰¥K(n)
(âˆ’pk ln pk)
âˆ¼
âˆš
nC âˆ«
âˆ
K(n)
2 ln x + 2 ln ln x âˆ’ln C
x2ln2x
dx
>
âˆš
nC âˆ«
âˆ
K(n)
2
x2 dx
= 2C
âˆš
n
K(n) â†’âˆ.
3.3
Entropy Estimator in Turingâ€™s Perspective
An entropy estimator in Turingâ€™s perspective is introduced in this section. The
centerpiece of that estimator is an alternative representation of entropy, which
is diï¬€erent from that of Shannonâ€™s. Consider any distribution {pk; k â‰¥1} âˆˆğ’«
and its entropy:
H = âˆ’
âˆ‘
kâ‰¥1
pk ln pk,
(3.21)
H =
âˆ‘
kâ‰¥1
pk
âˆ
âˆ‘
v=1
1
v(1 âˆ’pk)v,
(3.22)
H =
âˆ
âˆ‘
v=1
1
v
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)v.
(3.23)
The ï¬rst equation in (3.21) is Shannonâ€™s deï¬nition of entropy. The second
equation in (3.22) is due to an application of Taylorâ€™s expansion of âˆ’ln pk
for each and every k at p0 = 1. The third equation in (3.23) is due to Fubiniâ€™s
theorem , which allows the interchange of the summations. The two conditions
required by Fubiniâ€™s theorem are satisï¬ed here because
a) the summation H, as in Shannonâ€™s entropy, is ï¬nite by assumption, and
b) the summands are all positive.
Entropy in the form of (3.23) is a representation in Turingâ€™s perspective since
it is a linear combination of elements in {ğœv; v â‰¥1} where
ğœv =
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)v
for every nonnegative integer v as described in Chapter 2. Therefore, entropy
in Turingâ€™s perspective is
H =
âˆ
âˆ‘
v=1
1
v ğœv,
(3.24)
a weighted linear form of {ğœv} .

Estimation of Shannonâ€™s Entropy
87
An interesting heuristic comparison can be made between (3.21) and (3.24).
The representation of Shannonâ€™s entropy series may be viewed as a stack of
positive pieces âˆ’pk ln pk over k; and the representation of (3.24) may also be
viewed as a stack of positive pieces ğœvâˆ•v over v.
Consider the following two side-by-side comparisons.
1) Parts versus Whole.
a) For each k, the summand âˆ’pk ln pk pertains to only the probability of one
letter ğ“k of ğ’³, and says nothing about other probabilities or the whole
distribution.
b) For each v, the summand ğœvâˆ•v = âˆ‘
kâ‰¥1pk(1 âˆ’pk)vâˆ•v is a characteristic of
the whole underlying distribution {pk}.
2) Estimability.
a) For each k, there is no unbiased estimator of âˆ’pk ln pk.
b) For each v, the estimator Z1,v of (2.4) is an uniformly minimum variance
unbiased estimator (umvue) of ğœv, provided v â‰¤n âˆ’1, and hence Z1,vâˆ•v
is umvue of ğœvâˆ•v.
The estimability of ğœv up to v = n âˆ’1 implies that the ï¬rst part of (3.25) is
estimable.
H =
nâˆ’1
âˆ‘
v=1
1
v ğœv +
âˆ
âˆ‘
v=n
1
v ğœv.
(3.25)
That is, letting
Z1,v = n1+v[n âˆ’(1 + v)]!
n!
âˆ‘
kâ‰¥1
[
Ì‚pk
vâˆ’1
âˆ
j=0
(
1 âˆ’Ì‚pk âˆ’j
n
) ]
,
(3.26)
Ì‚Hz =
nâˆ’1
âˆ‘
v=1
1
v Z1,v
(3.27)
is an unbiased estimator of âˆ‘nâˆ’1
v=1
1
v ğœ1,v. As an estimator of H, the second addi-
tive part in (3.25) is the bias of (3.27),
|Bn| = H âˆ’E( Ì‚Hz) =
âˆ
âˆ‘
v=n
1
v ğœv.
(3.28)
The explicit expression of the bias of Ì‚Hz under general distributions enables
easier computation and evaluation and is one of many advantages brought
about by Turingâ€™s perspective.
Remark 3.2
To express entropy as a series via Taylorâ€™s expansion for bias
reduction of entropy estimation was perhaps ï¬rst discussed in the literature by
Blyth (1959). There an application of Taylorâ€™s expansion was used for pk ln pk at
p0 = 0.5 under the assumption that K < âˆ, an estimator was proposed, and its

88
Statistical Implications of Turingâ€™s Formula
reduced bias was demonstrated. However, the center of the expansion p0 = 0.5
was somehow arbitrary. The argument provided by Blyth (1959) would hold for
any reasonable value of p0 âˆˆ(0, 1). For some reason, that line of research was
not rigorously pursued in the history. On the contrary, the expansion used above
is for âˆ’ln pk and is at p0 = 1. There are two important consequences of such an
expansion. First, the polynomials in the expansion has a particular form, namely
pk(1 âˆ’pk)v, which transforms the entire problem to a local binary platform.
The binary platform allows statistical inferences to be made based on whether
the letter ğ“k is either observed or not observed in an iid sample. In fact, this
is the very reason the perspective is named in honor of Alan Turing although it
is not known whether Alan Turing actually thought about the problem of entropy
estimation in his living years. Second, the coeï¬ƒcient of the expansion in Turingâ€™s
perspective is of a simple form, namely 1âˆ•v, which makes the derivation of math-
ematical properties of the corresponding estimator Ì‚Hz much easier. The said ease
is perhaps one of the main reasons why many good properties of the perspective
are established, as will be seen throughout the subsequent text.
3.3.1
When K Is Finite
When the eï¬€ective cardinality of ğ’³is ï¬nite, that is, K < âˆ, an immediate ben-
eï¬t of Turingâ€™s perspective is stated in the following theorem.
Theorem 3.9
If K = âˆ‘K
k=1 1[pk > 0] < âˆ, then
|Bn| = H âˆ’E( Ì‚Hz) â‰¤(1 âˆ’pâˆ§)n
npâˆ§
where pâˆ§= min{pk > 0; k = 1,â€¦, K}.
Proof: By (3.28),
|Bn| =
âˆ
âˆ‘
v=n
1
v
K
âˆ‘
k=1
pk(1 âˆ’pk)v
â‰¤1
n
âˆ
âˆ‘
v=n
K
âˆ‘
k=1
pk(1 âˆ’pâˆ§)v
= 1
n
âˆ
âˆ‘
v=n
(1 âˆ’pâˆ§)v
= (1 âˆ’pâˆ§)n
npâˆ§
.
â—½
Three important points are to be noted here. First, the exponential decay of
the bias of Ì‚Hz is a qualitative improvement from the plug-in estimator, which

Estimation of Shannonâ€™s Entropy
89
has a îˆ»(nâˆ’1) decaying rate in n. Second, the upper bound depends on the distri-
bution {pk} via pâˆ§, and therefore it is not a universal upper bound even within
the class of distributions on a ï¬nite alphabet since pâˆ§> 0 could be arbitrarily
small. Third, when n is small and pâˆ§> 0 is very small, the bias could still be
sizable.
Several asymptotic properties of Ì‚Hz, when K < âˆ, were established in
Zhang (2013b). Let H2 be as in Deï¬nition 3.1.
Theorem 3.10
Let {pk; 1 â‰¤k â‰¤K} be a nonuniform probability distribution
on a ï¬nite alphabet ğ’³. Then
âˆš
n ( Ì‚Hz âˆ’H)
ğœ
L
âˆ’âˆ’âˆ’â†’N(0, 1)
where ğœ2 = H2 âˆ’H2.
Toward proving Theorem 3.10, let
w(v + 1, n) = nv+1[n âˆ’(v + 1)]!
n!
(3.29)
and consider the following re-expression of Ì‚Hz,
Ì‚Hz =
K
âˆ‘
k=1
{
Ì‚pk
{nâˆ’1
âˆ‘
v=1
1
vw(v + 1, n)
[ vâˆ’1
âˆ
j=0
(
1 âˆ’Ì‚pk âˆ’j
n
) ] } }
=âˆ¶
K
âˆ‘
k=1
{Ì‚pk {Ì‚gk,n} } .
(3.30)
Of ï¬rst interest is an asymptotic normal law of Ì‚pk Ì‚gk,n for each k. For simplic-
ity, consider ï¬rst a binomial distribution with parameters n and p âˆˆ(0, 1) and
functions
gn(p) =
nâˆ’1
âˆ‘
v=1
{
1
vw(v + 1, n)
[ vâˆ’1
âˆ
j=0
(
1 âˆ’p âˆ’j
n
) ]
1[v â‰¤n(1 âˆ’p) + 1]
}
and hn(p) = pgn(p). Let h(p) = âˆ’p ln(p).
The following two lemmas are needed to prove Theorem 3.10. Lemma 3.1 is
easily proved by induction (see Exercise 11). A proof of Lemma 3.2 is given in
Appendix at the end of this chapter.
Lemma 3.1
Let aj, j = 1,â€¦, n, be real numbers satisfying |aj| â‰¤1 for every j.
Then |||
âˆn
j=1 aj âˆ’1||| â‰¤âˆ‘n
j=1 |aj âˆ’1|.
Lemma 3.2
Let Ì‚p = Xâˆ•n where X is a binomial random variable with param-
eters n and p.
1)
âˆš
n[hn(p) âˆ’h(p)] â†’0 uniformly in p âˆˆ(c, 1) for any c, 0 < c < 1.

90
Statistical Implications of Turingâ€™s Formula
2)
âˆš
n|hn(p) âˆ’h(p)| < A(n) = îˆ»(n3âˆ•2) uniformly in p âˆˆ[1âˆ•n, c] for any c,
0 < c < p.
3) P(Ì‚p â‰¤c) < B(n) = îˆ»(nâˆ’1âˆ•2eâˆ’nC) where C = (p âˆ’c)2âˆ•[p(1 âˆ’p)] for any
c âˆˆ(0, p).
Proof of Theorem 3.10: Without loss of generality, consider the sample propor-
tions of the ï¬rst two letters of the alphabet Ì‚p1 and Ì‚p2 in an iid sample of size n.
âˆš
n (Ì‚p1 âˆ’p1, Ì‚p2 âˆ’p2)â€²
L
âˆ’âˆ’âˆ’â†’N(0, Î£), where Î£ = (ğœij), i, j = 1, 2, ğœii = pi(1 âˆ’pi),
and ğœij = âˆ’pipj when i â‰ j. Write
âˆš
n [(hn(Ì‚p1) + hn(Ì‚p2)) âˆ’(âˆ’p1 ln p1 âˆ’p2 ln p2)]
=
âˆš
n [(hn(Ì‚p1) + hn(Ì‚p2)) âˆ’(h(Ì‚p1) + h(Ì‚p2))]
+
âˆš
n [(h(Ì‚p1) + h(Ì‚p2)) âˆ’(âˆ’p1 ln p1 âˆ’p2 ln p2)]
=
âˆš
n (hn(Ì‚p1) âˆ’h(Ì‚p1)) +
âˆš
n (hn(Ì‚p2) âˆ’h(Ì‚p2))
+
âˆš
n [(h(Ì‚p1) + h(Ì‚p2)) âˆ’(âˆ’p1 ln p1 âˆ’p2 ln p2)]
=
âˆš
n [hn(Ì‚p1) âˆ’h(Ì‚p1)] 1[Ì‚p1 â‰¤p1âˆ•2]
+
âˆš
n (hn(Ì‚p2) âˆ’h(Ì‚p2)) 1[Ì‚p2 â‰¤p2âˆ•2]
+
âˆš
n (hn(Ì‚p1) âˆ’h(Ì‚p1)) 1[Ì‚p1 > p1âˆ•2]
+
âˆš
n (hn(Ì‚p2) âˆ’h(Ì‚p2)) 1[Ì‚p2 > p2âˆ•2]
+
âˆš
n [(h(Ì‚p1) + h(Ì‚p2)) âˆ’(âˆ’p1 ln p1 âˆ’p2 ln p2)] .
The third and fourth terms in the last expression above converge to zero
almost surely by Part 1 of Lemma 3.2. The last term, by the delta method, con-
verges in law to N(0, ğœ2) where after a few algebraic steps
ğœ2 = (ln p1 + 1)2p1(1 âˆ’p1) + (ln p2 + 1)2p2(1 âˆ’p2)
âˆ’2(ln p1 + 1)(ln p2 + 1)p1p2
= (ln p1 + 1)2p1 + (ln p2 + 1)2p2
âˆ’[(ln p1 + 1)p1 + (ln p1 + 1)p1]2.
It remains to show that the ï¬rst term (the second term will admit the same argu-
ment) converges to zero in probability. However, this fact can be established by
the following argument. By Part 2 and then Part 3 of Lemma 3.2,
E (
âˆš
n |hn(Ì‚p1) âˆ’h(Ì‚p1)| 1[Ì‚p1 â‰¤p1âˆ•2]) )
â‰¤A(n)P(Ì‚p1 â‰¤p1âˆ•2)
â‰¤A(n)B(n)
= îˆ»(n3âˆ•2)îˆ»(nâˆ’1âˆ•2eâˆ’nC) â†’0

Estimation of Shannonâ€™s Entropy
91
for some positive constant C. This fact, noting that
âˆš
n |hn(Ì‚p1) âˆ’h(Ì‚p1)| â‰¥0,
gives immediately the desired convergence in probability, that is,
âˆš
n |hn(Ì‚p1) âˆ’h(Ì‚p1)| 1[Ì‚p1 â‰¤p1âˆ•2]
p
âˆ’âˆ’âˆ’â†’0.
In turn, it gives the desired weak convergence for
âˆš
n [(hn(Ì‚p1) + hn(Ì‚p2)) âˆ’(âˆ’p1 ln p1 âˆ’p2 ln p2)] .
The desired result follows a generalization to K terms.
â—½
To use Theorem 3.10 for inferences, one must ï¬nd a consistent estimator for
ğœ2 = H2 âˆ’H2. Here the plug-in estimators for both H and H2, as well as any
other consistent estimators, in principle could serve the purpose, but could
perform poorly due to their respective heavy biases.
Let
Ì‚H2z =
nâˆ’1
âˆ‘
v=1
{[vâˆ’1
âˆ‘
i=1
1
i(v âˆ’i)
]
w(v + 1, n)
K
âˆ‘
k=1
[
Ì‚pk
vâˆ’1
âˆ
j=0
(
1 âˆ’Ì‚pk âˆ’j
n
) ]}
.
(3.31)
It may be veriï¬ed that (Exercise 12), for any given distribution with ï¬nite K,
|E( Ì‚H2z âˆ’H2)| decays exponentially in n.
Corollary 3.3
Let {pk; 1 â‰¤k â‰¤K} be a nonuniform probability distribution
on a ï¬nite alphabet and Ì‚H2z be as in (3.31). Then
âˆš
n
â›
âœ
âœ
âœâ
Ì‚Hz âˆ’H
âˆš
Ì‚H2z âˆ’Ì‚H2
z
â
âŸ
âŸ
âŸâ 
L
âˆ’âˆ’âˆ’â†’N(0, 1).
Toward proving Corollary 3.3, let
Cv =
vâˆ’1
âˆ‘
i=1
1
i(v âˆ’i)
(3.32)
for v â‰¥2 (and deï¬ne C1 = 0) and write
Ì‚H2z =
nâˆ’1
âˆ‘
v=1
CvZ1,v.
For better clarity in proving Corollary 3.3, a few more notations and two
well-known lemmas in U-statistics are ï¬rst given.
For each i, 1 â‰¤i â‰¤n, let Xi be a random element such that Xi = ğ“k indi-
cates the event that the kth letter of the alphabet is observed and P(Xi = ğ“k) =
pk. Let X1,â€¦, Xn be an iid sample, and denote x1,â€¦, xn as the corresponding

92
Statistical Implications of Turingâ€™s Formula
sample realization. A U-statistic is an n-variable function obtained by averag-
ing the values of an m-variable function (kernel of degree m, often denoted by
ğœ“) over all n!âˆ•[m!(n âˆ’m)!] possible subsets of m observations from the set of
n observations. Interested readers may refer to Lee (1990) for an introduction.
In Zhang and Zhou (2010), it is shown that Z1,v is a U-statistic with kernel ğœ“
being Turingâ€™s formula with degree m = v + 1. Let
ğœ“c(x1,â€¦, xc) = E(ğœ“(x1,â€¦, xc, Xc+1,â€¦, Xm))
and
ğœ2
c = Var(ğœ“c(X1,â€¦, Xc)).
Lemmas 3.3 and 3.4 are due to Hoeï¬€ding (1948).
Lemma 3.3
Let Un be a U-statistic with kernel ğœ“of degree m.
Var(Un) =
( n
m
)âˆ’1
m
âˆ‘
c=1
(m
c
) (n âˆ’m
m âˆ’c
)
ğœ2
c .
Lemma 3.4
Let Un be a U-statistic with kernel ğœ“of degree m.
For any two nonnegative integers, c and d, satisfying 0 â‰¤c â‰¤d â‰¤m,
ğœ2
c
c â‰¤
ğœ2
d
d .
Lemma 3.5
Var(Z1,v) â‰¤1
nğœ1,v + v + 1
n
ğœ2
1,vâˆ’1.
A proof of Lemma 3.5 is given in the Appendix at the end of this chapter.
Proof of Corollary 3.3: Since E(Z1,v) = ğœv,
E( Ì‚H2z) =
nâˆ’1
âˆ‘
v=1
Cv
K
âˆ‘
k=1
pk(1 âˆ’pk)v
=
K
âˆ‘
k=1
pk
nâˆ’1
âˆ‘
v=1
Cv(1 âˆ’pk)v
â†’
K
âˆ‘
k=1
pk(âˆ’ln pk)2 =
K
âˆ‘
k=1
pkln2pk.

Estimation of Shannonâ€™s Entropy
93
It only remains to show Var( Ì‚H2z) â†’0.
Var( Ì‚H2z) =
nâˆ’1
âˆ‘
v=1
nâˆ’1
âˆ‘
w=1
CvCwCov(Z1,v, Z1,w)
â‰¤
nâˆ’1
âˆ‘
v=1
nâˆ’1
âˆ‘
w=1
CvCw
âˆš
Var(Z1,v)Var(Z1,w)
=
(nâˆ’1
âˆ‘
v=1
Cv
âˆš
Var(Z1,v)
)2
.
Noting
Cv =
vâˆ’1
âˆ‘
i=1
1
i(v âˆ’i) â‰¤
vâˆ’1
âˆ‘
i=1
1
v âˆ’1 = 1,
ğœv â‰¤ğœvâˆ’1,
ğœ2
vâˆ’1 â‰¤ğœvâˆ’1,
ğœvâˆ’1 =
K
âˆ‘
k=1
pk(1 âˆ’pk)vâˆ’1 â‰¤
K
âˆ‘
k=1
pk(1 âˆ’pâˆ§)vâˆ’1 = (1 âˆ’pâˆ§)vâˆ’1,
where pâˆ§= min{pk > 0; k = 1,â€¦, K}, and by Lemma 3.5 for v â‰¥2,
âˆš
Var(Z1,v) â‰¤
1
âˆš
n
âˆš
(v + 2)ğœ1,vâˆ’1 â‰¤
âˆš
2v1âˆ•2
âˆš
n
(1 âˆ’pâˆ§)(vâˆ’1)âˆ•2.
As n â†’âˆ,
nâˆ’1
âˆ‘
v=1
Cv
âˆš
Var(Z1,v) â‰¤
âˆš
2
âˆš
n
n
âˆ‘
v=1
v1âˆ•2 (
âˆš
1 âˆ’pâˆ§) vâˆ’1
=
âˆš
2
âˆš
n
âŒŠn1âˆ•4âŒ‹
âˆ‘
v=1
v1âˆ•2 (
âˆš
1 âˆ’pâˆ§) vâˆ’1
+
âˆš
2
âˆš
n
n
âˆ‘
v=âŒŠn1âˆ•4âŒ‹+1
v1âˆ•2 (
âˆš
1 âˆ’pâˆ§) vâˆ’1
â‰¤
âˆš
2
âˆš
n
n1âˆ•4 (n1âˆ•4) 1âˆ•2
+
âˆš
2
âˆš
n
n1âˆ•2 (âˆš
1 âˆ’pâˆ§
)âŒŠn1âˆ•4âŒ‹
1
1 âˆ’
âˆš
1 âˆ’pâˆ§
=
âˆš
2nâˆ’1âˆ•8 +
âˆš
2
(âˆš
1 âˆ’pâˆ§
)âŒŠn1âˆ•4âŒ‹
1
1 âˆ’
âˆš
1 âˆ’pâˆ§
â†’0,

94
Statistical Implications of Turingâ€™s Formula
and Var( Ì‚H2z) â†’0 follows. Hence Ì‚H2z
pâ†’H2. The fact of Ì‚Hz
pâ†’H is implied by
Theorem 3.10. Finally, the corollary follows Slutskyâ€™s lemma.
â—½
Theorem 3.11
Let {pk; 1 â‰¤k â‰¤K} be a nonuniform probability distribution
on a ï¬nite alphabet ğ’³. Then Ì‚Hz is asymptotically eï¬ƒcient.
Proof: First consider the plug-in estimator Ì‚H. It can be veriï¬ed that
âˆš
n( Ì‚H âˆ’
H) â†’N(0, ğœ2) where ğœ2 = ğœ2({pk}) is as in Theorem 3.10. It is of interest to
show ï¬rst that Ì‚H is asymptotically eï¬ƒcient in two separate cases: (i) when K is
known and (ii) when K is unknown. If K is known, then the underlying model
{pk; 1 â‰¤k â‰¤K} is a (K âˆ’1)-parameter multinomial distribution, and therefore
Ì‚H is the maximum likelihood estimator of H, which implies that it is asymp-
totically eï¬ƒcient. Since the estimator Ì‚H takes the same value, given a sample,
regardless whether K is known or not, its asymptotic variance is the same.
Therefore, Ì‚H must be asymptotically eï¬ƒcient when K is ï¬nite but unknown,
or else, it would contradict the fact that Ì‚H is asymptotically eï¬ƒcient when K is
known. The asymptotic eï¬ƒciency of Ì‚Hz follows from the fact that
âˆš
n( Ì‚Hz âˆ’H)
and
âˆš
n( Ì‚H âˆ’H) have identical limiting distribution.
â—½
3.3.2
When K Is Countably Inï¬nite
The following lemma gives a necessary and suï¬ƒcient condition for entropy H
to be ï¬nite.
Lemma 3.6
For a given {pk}, H = âˆ’âˆ‘
kpk ln(pk) < âˆif and only if there exists
a strictly increasing divergent sequence of positive real numbers {an} such that
âˆ‘âˆ
n=1 (nan)âˆ’1 < âˆand, as n â†’âˆ,
an
âˆ
âˆ‘
k=1
pk(1 âˆ’pk)n â†’1.
Proof: If an an satisfying the conditions exists, then there also exists a positive
integer n0 such that, for all n > n0, an
âˆ‘
kpk(1 âˆ’pk)n < 2. Therefore,
âˆ
âˆ‘
v=1
1
v
âˆ‘
k
pk(1 âˆ’pk)vâˆ’1
=
n0
âˆ‘
v=1
1
v
âˆ‘
k
pk(1 âˆ’pk)vâˆ’1 +
âˆ
âˆ‘
v=n0+1
1
v
âˆ‘
k
pk(1 âˆ’pk)vâˆ’1
â‰¤
n0
âˆ‘
v=1
1
v
âˆ‘
k
pk(1 âˆ’pk)vâˆ’1 + 2
âˆ
âˆ‘
v=n0+1
1
vav
< âˆ.

Estimation of Shannonâ€™s Entropy
95
On the other hand, if âˆ‘âˆ
v=1(1âˆ•v)ğœv < âˆ, then letting an = 1âˆ•ğœn the following
arguments show that all conditions of Lemma 3.6 are satisï¬ed. First, ğœn is strictly
decreasing and therefore an is strictly increasing. Second, ğœn = âˆ‘
kpk(1 âˆ’pk)n â‰¤
âˆ‘
kpk = 1; and by the dominated convergence theorem, ğœn â†’0 and hence
an â†’âˆ. Third, âˆ‘âˆ
v=1 1âˆ•(vav) = âˆ‘âˆ
v=1(1âˆ•v)ğœv < âˆand anğœn = 1 by assumption
and deï¬nition.
â—½
Lemma 3.6 serves two primary purposes. First, it encircles all the discrete
distributions that may be of interest with regard to entropy estimation, that is,
distributions with ï¬nite entropy. Second, it provides a characterization of the
tail of a distribution in terms of a sequence an and its conjugative relationship
to ğœn. The rate of divergence of an characterizes the rate of tail decay of the
underlying distribution {pk} as k increases. A faster (slower) rate of divergence
of an signals a thinner (thicker) probability tail.
Let
Mn = 5
n
(nâˆ’1
âˆ‘
v=1
1
v ğœ1âˆ•2
vâˆ’1
)2
.
(3.33)
The next lemma provides an upper bound for Var( Ì‚Hz) under the general con-
ditions and plays a central role in establishing many of the subsequent results.
Lemma 3.7
For any probability distribution {pk}, Var( Ì‚Hz) â‰¤Mn.
The proof of Lemma 3.7 is based on a U-statistic argument. The details of the
proof are found in Zhang (2012).
Corollary 3.4
For any probability distribution {pk},
Var( Ì‚Hz) < 5
{[1 + ln(n âˆ’1)]2
n
}
= îˆ»(ln2nâˆ•n).
Proof: Referring to (3.33) and noting ğœv â‰¤1 and that âˆ‘nâˆ’1
k=1 1âˆ•v is the harmonic
series hence with upper bound 1 + ln(n âˆ’1),
Mn < 5
{[1 + ln(n âˆ’1)]2
n
}
= îˆ»(ln2nâˆ•n).
â—½
Corollary 3.5
For any probability distribution {pk} with ï¬nite entropy H,
Var( Ì‚Hz) < 5(1 + H)
[1 + ln(n âˆ’1)
n
]
= îˆ»(ln nâˆ•n).
A proof of Corollary 3.5 is given in the Appendix of Chapter 3.

96
Statistical Implications of Turingâ€™s Formula
The implications of Corollary 3.5 are quite signiï¬cant. The upper bound
in Corollary 3.5 for every distribution in the entire class of ğ’«decays faster
than the upper bound for Var( Ì‚H), that is, îˆ»(ln2nâˆ•n), established in Antos and
Kontoyiannis (2001) for the same class of distributions by a factor of ln n. The
improvement in variance suggests that entropy estimation in view of Ì‚Hz is
fundamentally more eï¬ƒcient than that of the plug-in, Ì‚H, in some sense. This
may also be viewed as an advantage brought about by Turingâ€™s perspective.
Nevertheless, the fact that the upper bound is proportional to 1 + H suggests
that, for any ï¬xed sample size n, a distribution with ï¬nite entropy can be found
to have an upper bound arbitrarily large since H can be arbitrarily large.
Lemma 3.7 implies the following general statements about Ì‚Hz under the con-
dition of ï¬nite entropy. Theorem 3.12 is stated for a convenient comparison to
Theorem 3.6 regarding the plug-in estimator Ì‚H.
Theorem 3.12
For any {pk} âˆˆğ’«, as n â†’âˆ,
1) E( Ì‚Hz âˆ’H)2 â†’0;
2) for all n, E( Ì‚Hz) â‰¤H; and
3) for all n,
Var( Ì‚Hz) â‰¤5(1 + H)
(1 + ln(n âˆ’1)
n
)
.
(3.34)
Proof: For Part 1, since E( Ì‚Hz âˆ’H)2 = Var( Ì‚Hz) + |Bn|2, it suï¬ƒces to show
Var( Ì‚Hz) â†’0, which is implied by Corollary 3.5, and |Bn| â†’0 which is implied
by H being ï¬nite. Part 2 is true simply because the bias âˆ‘âˆ
v=n ğœvâˆ•v > 0. Part 3 is
a restatement of Corollary 3.5.
â—½
In diï¬€erent subclasses of ğ’«, the convergence rates of Ì‚Hz could be better
described. Naturally ğ’«may be partitioned into layers characterized by the rate
of divergence of ğœn or equivalently by tn = n ğœn. Consider the following condi-
tions.
Condition 3.1
For a probability sequence {pk} âˆˆğ’«, there exists a constant
C > 0 such that n ğœn â‰¤C for all n.
Condition 3.2
For a probability sequence {pk} âˆˆğ’«,
1) n ğœn â†’âˆas n â†’âˆ, and
2)
âˆš
n ğœn â‰¤C for some constant C > 0 and all n.
Condition 3.3
For a probability sequence {pk} âˆˆğ’«,
1)
âˆš
n ğœn â†’âˆas n â†’âˆ, and
2) nğ›¿ğœn â‰¤C for a constant ğ›¿âˆˆ(0, 1âˆ•2), some constant C > 0 and all n.

Estimation of Shannonâ€™s Entropy
97
Conditions 3.1â€“3.3 are mutually exclusive conditions. Let ğ’«i, i = 1, 2, 3,
be the subclasses of ğ’«
under Conditions 3.1â€“3.3, respectively. Let
ğ’«4 = (ğ’«1 âˆªğ’«2 âˆªğ’«3)c where the complement is with respect to ğ’«.
It may be instructive to make several observations at this point. First, ğ’«i,
1 â‰¤i â‰¤4, are deï¬ned in an order of the tail decaying rate of the underlying dis-
tribution with ğ’«1 having the fastest decaying rate. Second, it can be veriï¬ed that
pk = cğœ†eâˆ’ğœ†k where ğœ†> 0 satisï¬es nğœn < C for some C > 0 and hence Condition
3.1. Condition 3.1 is therefore satisï¬ed by all distributions with faster decaying
tails than that of pk = cğœ†eâˆ’ğœ†k, including the distributions on any ï¬nite alphabet.
Third, it can also be veriï¬ed that, for pk = cğœ†kâˆ’ğœ†where ğœ†> 1, nğ›¿ğœn â†’C > 0
where ğ›¿= 1 âˆ’1âˆ•ğœ†. It is seen here that pk = cğœ†kâˆ’ğœ†where ğœ†â‰¥2 belongs to ğ’«2
and that pk = cğœ†kâˆ’ğœ†where 1 < ğœ†< 2 belongs to ğ’«3. ğ’«4 holds all other very
thick-tailed distributions whose ğœnâ€™s converge so slowly that they cannot be
bounded above by a sequence bn â†’0 at a rate of îˆ»(nâˆ’Îµ) for any small Îµ > 0.
Theorem 3.13
For any probability distribution {pk},
1) if {pk} âˆˆğ’«1 âˆªğ’«2, then there exists M1(n) = îˆ»(nâˆ’1) such that E( Ì‚Hz âˆ’H)2 â‰¤
M1(n); and
2) if {pk} âˆˆğ’«3, then there exists M2(n) = îˆ»(nâˆ’2ğ›¿) such that E( Ì‚Hz âˆ’H)2 â‰¤
M2(n).
Proof: By Lemma 3.7, E( Ì‚Hz âˆ’H)2 â‰¤Mn + |Bn|2 in general where Mn is as in
(3.33) and Bn = âˆ‘âˆ
v=n ğœvâˆ•v. For Part 1, if Condition 3.1 holds, then
Mn = 5
n
nâˆ’1
âˆ‘
v=1
1
v ğœ1âˆ•2
vâˆ’1
= 5
n ğœ1âˆ•2
0
+ 5
n
nâˆ’1
âˆ‘
v=2
1
v ğœ1âˆ•2
vâˆ’1
â‰¤5
n + 5
âˆš
C
n
nâˆ’1
âˆ‘
v=2
1
v(v âˆ’1)1âˆ•2 =âˆ¶V1(n) = îˆ»(nâˆ’1).
On the other hand,
Bn =
âˆ
âˆ‘
v=n
1
v ğœv â‰¤C
âˆ
âˆ‘
v=n
1
v2 =âˆ¶B1(n) = îˆ»(nâˆ’1).
Letting M1(n) = V1(n) + B2
1(n) = îˆ»(nâˆ’1) establishes the desired result.
Similarly if Condition 3.2 holds, then
Mn = 5
n ğœ1âˆ•2
0
+ 5
n
nâˆ’1
âˆ‘
v=2
1
v ğœ1âˆ•2
vâˆ’1
â‰¤5
n + 5
âˆš
C
n
nâˆ’1
âˆ‘
v=2
1
v(v âˆ’1)1âˆ•4 =âˆ¶V1(n) = îˆ»(nâˆ’1).

98
Statistical Implications of Turingâ€™s Formula
Bn =
âˆ
âˆ‘
v=n
1
v ğœ1,v â‰¤C
âˆ
âˆ‘
v=n
1
v1+1âˆ•2 =âˆ¶B1(n) = îˆ»(nâˆ’1).
Letting M1(n) = V1(n) + B2
1(n) = îˆ»(nâˆ’1) establishes the desired result.
For Part 2, if Condition 3.3 holds, then
Mn = 5
n ğœ1âˆ•2
0
+ 5
n
nâˆ’1
âˆ‘
v=2
1
v ğœ1âˆ•2
vâˆ’1
â‰¤5
n + 5
âˆš
C
n
nâˆ’1
âˆ‘
v=2
1
v(v âˆ’1)ğ›¿âˆ•2 =âˆ¶V2(n) = îˆ»(nâˆ’1).
On the other hand,
Bn =
âˆ
âˆ‘
v=n
1
v ğœ1 â‰¤C
âˆ
âˆ‘
v=n
1
v1+ğ›¿=âˆ¶B2(n) = îˆ»(nâˆ’ğ›¿).
Since in this case 2ğ›¿< 1, letting M2(n) = V2(n) + B2
2(n) = îˆ»(nâˆ’2ğ›¿) establishes
the desired result.
â—½
The statements of Theorem 3.13 give the convergence rates of upper bounds
in mean-squared errors for various types of distribution. Statement 1 says that,
for all distributions with fast decaying tails, the bias of Ì‚Hz decays suï¬ƒciently
fast so that |Bn|2 is dominated by Var( Ì‚Hz), which converges at a rate of îˆ»(1âˆ•n).
It may be interesting to note that the so-called fast decaying distributions here
include those with power decaying tails down to a threshold ğ›¿= 1âˆ•2. State-
ment 2 says that, for each of the thick-tailed distributions, the squared bias
|Bn|2 dominates the convergence in mean-squared errors.
Example 3.8
Suppose pk = Ckâˆ’ğœ†, where C > 0 and ğœ†> 1 are constants,
for all k â‰¥k0 where k0 is some positive integer. It can be veriï¬ed that
nğ›¿âˆ‘
kpk(1 âˆ’pk)n â†’L > 0 where ğ›¿= 1 âˆ’1âˆ•ğœ†for some constant L (see Exercise
13). By Theorem 3.13,
1) if ğœ†â‰¥2, then {pk} âˆˆğ’«2 and therefore
E( Ì‚Hz âˆ’H)2 â‰¤îˆ»(nâˆ’1) ;
and
2) if 1 < ğœ†< 2, then {pk} âˆˆğ’«3 and therefore
E( Ì‚Hz âˆ’H)2 = îˆ»
(
nâˆ’2(ğœ†âˆ’1)
ğœ†
)
.
Example 3.9
Suppose pk = Ceâˆ’ğœ†k, where C > 0 and ğœ†> 0 are constants, for
all k â‰¥k0 where k0 is some positive integer. It can be veriï¬ed that nğœn < U for
some constant U > 0. By Theorem 3.13, E( Ì‚Hz âˆ’H)2 = îˆ»(nâˆ’1).

Estimation of Shannonâ€™s Entropy
99
Furthermore, a suï¬ƒcient condition may be established for a normal law of
âˆš
n( Ì‚Hz âˆ’H). Recall H2 = âˆ‘
kâ‰¥1pkln2pk, w(v + 1, n) of (3.29), and Cv of (3.30).
Let
Ì‚H2z =
nâˆ’1
âˆ‘
v=1
{
Cv w(v + 1, n)
âˆ‘
kâ‰¥1
[
Ì‚pk
vâˆ’1
âˆ
j=0
(
1 âˆ’Ì‚pk âˆ’j
n
) ] }
.
(3.35)
Theorem 3.14
For a nonuniform distribution {pk; k â‰¥1} âˆˆğ’«satisfying
H2 < âˆ, if there exists an integer-valued function K(n) such that, as n â†’âˆ,
1) K(n) â†’âˆ,
2) K(n) = o(
âˆš
nâˆ•ln n), and
3)
âˆš
nâˆ‘
kâ‰¥K(n)pk ln pk â†’0,
then
âˆš
n ( Ì‚Hz âˆ’H)
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2)
where ğœ2 = H2 âˆ’H2.
Corollary 3.6
Under the conditions of Theorem 3.14,
âˆš
n
â›
âœ
âœ
âœâ
Ì‚Hz âˆ’H
âˆš
Ì‚H2z âˆ’Ì‚H2
z
â
âŸ
âŸ
âŸâ 
L
âˆ’âˆ’âˆ’â†’N(0, 1).
Toward proving Theorem 3.14 and Corollary 3.6, Z1,v, Ì‚Hz, and Ì‚H2z are
re-expressed as follows:
Z1,v = nv+1[n âˆ’(v + 1)]!
n!
âˆ‘
kâ‰¥1
[
Ì‚pk
vâˆ’1
âˆ
j=0
(
1 âˆ’Ì‚pk âˆ’j
n
) ]
=
âˆ‘
kâ‰¥1
[
nv+1[n âˆ’(v + 1)]!
n!
(
Ì‚pk
vâˆ’1
âˆ
j=0
n âˆ’Yk âˆ’j
n
) ]
=
âˆ‘
kâ‰¥1
(
Ì‚pk
vâˆ’1
âˆ
j=0
n âˆ’Yk âˆ’j
n âˆ’j âˆ’1
)
=
âˆ‘
kâ‰¥1
[
Ì‚pk
v
âˆ
j=1
(
1 âˆ’Yk âˆ’1
n âˆ’j
) ]
,

100
Statistical Implications of Turingâ€™s Formula
and therefore,
Ì‚Hz =
nâˆ’1
âˆ‘
v=1
1
vZ1,v =
nâˆ’1
âˆ‘
v=1
1
v
âˆ‘
k
Ì‚pk
v
âˆ
j=1
(
1 âˆ’Yk âˆ’1
n âˆ’j
)
=
âˆ‘
k
Ì‚pk
nâˆ’1
âˆ‘
v=1
1
v
v
âˆ
j=1
(
1 âˆ’Yk âˆ’1
n âˆ’j
)
=
âˆ‘
k
Ì‚pk
nâˆ’Yk
âˆ‘
v=1
1
v
v
âˆ
j=1
(
1 âˆ’Yk âˆ’1
n âˆ’j
)
and
Ì‚H2z =
nâˆ’1
âˆ‘
v=1
CvZ1,v =
âˆ‘
k
Ì‚pk
nâˆ’Yk
âˆ‘
v=1
Cv
v
âˆ
j=1
(
1 âˆ’Yk âˆ’1
n âˆ’j
)
.
Consider a modiï¬ed probability distribution {pk,n; k = 1,â€¦, K(n)} of {pk} as
follows.
Let
pk,n =
{
pk,
for
1 â‰¤k â‰¤K(n) âˆ’1
âˆ‘k â‰¥K(n)pk =âˆ¶pK(n),n,
for
k = K(n),
and
Yk,n =
{
Yk,
for
1 â‰¤k â‰¤K(n) âˆ’1
âˆ‘k â‰¥K(n)Yk =âˆ¶YK(n),n,
for
k = K(n),
and Ì‚pk,n = Yk,nâˆ•n for 1 â‰¤k â‰¤K(n). Consequently, the following quantities are
also modiï¬ed accordingly,
ğœâˆ—
v =
K(n)
âˆ‘
k=1
pk,n(1 âˆ’pk,n)v,
Zâˆ—
1,v = n1+v[n âˆ’(1 + v)]!
n!
K(n)
âˆ‘
k=1
[
Ì‚pk,n
vâˆ’1
âˆ
j=0
(
1 âˆ’Ì‚pk,n âˆ’j
n
) ]
,
Hâˆ—=
K(n)
âˆ‘
k=1
(âˆ’pk,n ln pk,n) =
âˆ
âˆ‘
v=1
1
v ğœâˆ—
v ,

Estimation of Shannonâ€™s Entropy
101
Ì‚Hâˆ—
z =
nâˆ’1
âˆ‘
v=1
1
vZâˆ—
1,v =
K(n)
âˆ‘
k=1
[
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
1
v
v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
) ]
,
Ì‚Hâˆ—=
K(n)
âˆ‘
k=1
(âˆ’Ì‚pk,n ln Ì‚pk,n),
Hâˆ—
2 =
âˆ
âˆ‘
v=1
Cv ğœâˆ—
v ,
Ì‚Hâˆ—
2z =
nâˆ’1
âˆ‘
v=1
CvZâˆ—
1,v =
K(n)
âˆ‘
k=1
[
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
Cv
v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
) ]
,
Ì‚Hâˆ—
2 =
K(n)
âˆ‘
k=1
Ì‚pk,nln2 Ì‚pk,n.
(3.36)
The following two facts are needed throughout the proofs:
1) E(Z1,v) = ğœv and E(Zâˆ—
1,v) = ğœâˆ—
v ; and
2)
Cv =
vâˆ’1
âˆ‘
i=1
1
i(v âˆ’i) = 1
v
vâˆ’1
âˆ‘
i=1
(1
i +
1
v âˆ’i
)
â‰¤2(ln v + 1)
v
.
The ï¬rst fact is due to Zhang and Zhou (2010), and the second is easily veriï¬ed
(see Exercise 14).
Lemmas 3.8 and 3.9 are due to Zhang and Zhang (2012).
Lemma 3.8
For any nonuniform distribution {pk; k â‰¥1} satisfying H2 < âˆ,
if there exists an integer-valued function K(n) such that, as n â†’âˆ,
1) K(n) â†’âˆ,
2) K(n) = o(
âˆš
n), and
3)
âˆš
nâˆ‘
kâ‰¥K(n)pk ln pk â†’0,
then
a)
âˆš
n
( Ì‚H âˆ’H
ğœ
)
L
âˆ’âˆ’âˆ’â†’N(0, 1),
b)
âˆš
n
( Ì‚Hâˆ—âˆ’Hâˆ—
ğœ
)
L
âˆ’âˆ’âˆ’â†’N(0, 1).
where Ì‚H and Ì‚Hâˆ—are the plug-in estimators of H and Hâˆ—, respectively, and ğœ2 =
H2 âˆ’H2.

102
Statistical Implications of Turingâ€™s Formula
Lemma 3.9
For a probability distribution {pk; k â‰¥1}, if there exists
an integer-valued function K(n) such that as n â†’âˆ, K(n) â†’âˆ, and
âˆš
nâˆ‘
kâ‰¥K(n)pk ln pk â†’0, then
a) (
âˆš
n ln n)pK(n),n â†’0, and
b) âˆ’
âˆš
npK(n),n ln pK(n),n â†’0.
Five more lemmas, Lemmas 3.10 through 3.14, are also needed.
Lemma 3.10
Under the conditions of Theorem 3.14,
âˆš
n( Ì‚Hz âˆ’Ì‚Hâˆ—
z ) = op(1).
Proof: Noting that for any k â‰¥K(n), Yk,n â‰¤YK(n),n and
0 â‰¤
nâˆ’Yk,n
âˆ‘
v=1
1
v
v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
âˆ’
nâˆ’YK(n),n
âˆ‘
v=1
1
v
v
âˆ
j=1
(
1 âˆ’
YK(n),n âˆ’1
n âˆ’j
)
â‰¤
nâˆ’Yk,n
âˆ‘
v=1
1
v
v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
â‰¤
nâˆ’Yk,n
âˆ‘
v=1
1
v â‰¤ln n + 1,
therefore,
0 â‰¤
âˆš
n( Ì‚Hz âˆ’Ì‚Hâˆ—
z )
=
âˆš
n
âˆ‘
kâ‰¥K(n)
Ì‚pk
nâˆ’Yk
âˆ‘
v=1
1
v
v
âˆ
j=1
(
1 âˆ’Yk âˆ’1
n âˆ’j
)
âˆ’
âˆš
nÌ‚pK(n),n
nâˆ’YK(n),n
âˆ‘
v=1
1
v
v
âˆ
j=1
(
1 âˆ’
YK(n),n âˆ’1
n âˆ’j
)
=
âˆš
n
âˆ‘
kâ‰¥K(n)
Ì‚pk
[nâˆ’Yk
âˆ‘
v=1
1
v
v
âˆ
j=1
(
1 âˆ’Yk âˆ’1
n âˆ’j
)
âˆ’
nâˆ’YK(n),n
âˆ‘
v=1
1
v
v
âˆ
j=1
(
1 âˆ’
YK(n),n âˆ’1
n âˆ’j
) ]
â‰¤
âˆš
n(ln n + 1)
âˆ‘
kâ‰¥K(n)
Ì‚pk.
By Lemma 3.9,
âˆš
n(ln n + 1)E
(
âˆ‘
kâ‰¥K(n)
Ì‚pk,n
)
=
âˆš
n(ln n + 1)
âˆ‘
kâ‰¥K(n)
pk,n â†’0,
hence
âˆš
n ( Ì‚Hz âˆ’Ì‚Hâˆ—
z
) = op(1) follows Chebyshevâ€™s inequality.
â—½

Estimation of Shannonâ€™s Entropy
103
Lemma 3.11
As n â†’âˆ, under the conditions of Theorem 3.14,
âˆš
n(E( Ì‚Hâˆ—
z ) âˆ’
Hâˆ—) â†’0.
Proof: The lemma follows the fact below.
0 â‰¤
âˆš
n(Hâˆ—
n âˆ’E( Ì‚Hâˆ—
z ))
=
âˆš
n
âˆ
âˆ‘
v=1
1
v ğœâˆ—
v âˆ’
âˆš
n
nâˆ’1
âˆ‘
v=1
1
v ğœâˆ—
v
=
âˆš
n
âˆ
âˆ‘
v=n
1
v ğœâˆ—
v
=
âˆš
n
âˆ
âˆ‘
v=n
1
v
K(n)
âˆ‘
k=1
pk,n(1 âˆ’pk,n)v
=
âˆš
n
K(n)
âˆ‘
k=1
pk,n
âˆ
âˆ‘
v=n
1
v(1 âˆ’pk,n)v
â‰¤
1
âˆš
n
K(n)
âˆ‘
k=1
pk,n
âˆ
âˆ‘
v=n
(1 âˆ’pk,n)v
â‰¤
1
âˆš
n
K(n)
âˆ‘
k=1
pk,n
(1 âˆ’pk,n)n
pk,n
=
1
âˆš
n
K(n)
âˆ‘
k=1
(1 âˆ’pk,n)n
â‰¤K(n)
âˆš
n
â†’0.
â—½
Lemma 3.12
As n â†’âˆ, under the conditions of Theorem 3.14,
âˆš
n(E( Ì‚Hâˆ—) âˆ’
Hâˆ—) â†’0.
Proof: Since f (x) = âˆ’x ln(x) is a concave function for x > 0, by Jensenâ€™s
inequality,
âˆš
n
K(n)
âˆ‘
k=1
E(âˆ’Ì‚pk,n ln Ì‚pk,n + pk,n ln pk,n) â‰¤0.
Also by (3.3),
âˆš
n
K(n)
âˆ‘
k=1
(E( Ì‚Hâˆ—
n) âˆ’Hâˆ—
n)
=
âˆš
n
K(n)
âˆ‘
k=1
E(âˆ’Ì‚pk,n ln Ì‚pk,n + pk,n ln pk,n)1[pk,n â‰¥1âˆ•n]

104
Statistical Implications of Turingâ€™s Formula
+
âˆš
n
K(n)
âˆ‘
k=1
E(âˆ’Ì‚pk,n ln Ì‚pk,n + pk,n ln pk,n)1[pk,n < 1âˆ•n]
â‰¥
âˆš
n
[
âˆ’K(n) âˆ’1
2n
+
1
12n2
(
1 âˆ’
K(n)
âˆ‘
k=1
1
pk,n
1[pk,n â‰¥1âˆ•n]
)
+ O(nâˆ’3)
]
+
âˆš
n
K(n)
âˆ‘
k=1
(pk,n ln pk,n)1[pk,n < 1âˆ•n]
â‰¥âˆ’
âˆš
nK(n)
2n
âˆ’
âˆš
nK(n)n
12n2
âˆ’
âˆš
nK(n) ln n
n
â†’0.
Therefore,
âˆš
n(E( Ì‚Hâˆ—) âˆ’Hâˆ—) â†’0.
â—½
Lemma 3.13
If a and b are such that 0 < a < b < 1, then for any integer
m â‰¥0,
bm âˆ’am â‰¤mbmâˆ’1(b âˆ’a).
Proof: Noting that f (x) = xm is convex on interval (0, 1) and
df (b)
db
= mbmâˆ’1,
the result follows immediately.
â—½
Lemma 3.14
Under the conditions of Theorem 3.14,
âˆš
n( Ì‚Hâˆ—
z âˆ’Ì‚Hâˆ—) = op(1).
A proof of Lemma 3.14 is given in the appendix of this chapter.
Proof of Theorem 3.14: Note that
âˆš
n( Ì‚Hz âˆ’H) âˆ’
âˆš
n( Ì‚Hâˆ—âˆ’Hâˆ—) =
âˆš
n( Ì‚Hz âˆ’Ì‚Hâˆ—) âˆ’
âˆš
n(H âˆ’Hâˆ—)
=
âˆš
n( Ì‚Hz âˆ’Ì‚Hâˆ—
z ) +
âˆš
n( Ì‚Hâˆ—
z âˆ’Ì‚Hâˆ—) âˆ’
âˆš
n(H âˆ’Hâˆ—)
=
âˆš
n( Ì‚Hz âˆ’Ì‚Hâˆ—
z ) +
âˆš
n( Ì‚Hâˆ—
z âˆ’Ì‚Hâˆ—)
+
âˆš
n
âˆ‘
kâ‰¥K(n)
(pk ln pk) âˆ’
âˆš
n
pK(n),n ln pK(n),n.
(3.37)
Since
âˆš
n( Ì‚Hâˆ—âˆ’Hâˆ—)
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2) by Part (b) of Lemma 3.8, it suï¬ƒces to show
that each of the four terms in the right-hand side of (3.37) is op(1).
The ï¬rst two terms are op(1) by Lemmas 3.10 and 3.14, respectively, the third
term goes to 0 by the conditions of Theorem 3.14, and the fourth term goes
to 0 by Lemma 3.9. Therefore, the statement of the theorem follows Slutskyâ€™s
lemma.
â—½
To prove Corollary 3.6, a few more lemmas are needed.

Estimation of Shannonâ€™s Entropy
105
Lemma 3.15
Under the conditions of Theorem 3.14, Ì‚H2 âˆ’Ì‚Hâˆ—
2 = op(1).
Proof:
0 â‰¤Ì‚H2 âˆ’Ì‚Hâˆ—
2
=
âˆ‘
kâ‰¥K(n)
Ì‚pk,nln2 Ì‚pk,n âˆ’Ì‚pK(n),nln2 Ì‚pK(n),n
=
âˆ‘
kâ‰¥K(n)
Ì‚pk,nln2 Ì‚pk,n âˆ’
âˆ‘
kâ‰¥K(n)
Ì‚pk,nln2 Ì‚pK(n),n
=
âˆ‘
kâ‰¥K(n)
Ì‚pk,n(ln2 Ì‚pk,n âˆ’ln2 Ì‚pK(n),n)
â‰¤
âˆ‘
kâ‰¥K(n)
Ì‚pk,nln2 Ì‚pk,n
â‰¤ln2n
âˆ‘
kâ‰¥K(n)
Ì‚pk,n.
By Lemma 3.9, ln2nE (âˆ‘
kâ‰¥K(n) Ì‚pk,n
) = (ln2n)pK(n),n â†’0, Ì‚H2 âˆ’Ì‚Hâˆ—
2 = op(1) fol-
lows Chebyshevâ€™s inequality.
â—½
Lemma 3.16
Under the conditions of Theorem 3.14, Ì‚H2z âˆ’Ì‚Hâˆ—
2z = op(1).
Proof: Noting that for any k â‰¥K(n), Yk,n â‰¤YK(n),n and
0 â‰¤
nâˆ’Yk,n
âˆ‘
v=1
Cv
v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
âˆ’
nâˆ’YK(n),n
âˆ‘
v=1
Cv
v
âˆ
j=1
(
1 âˆ’
YK(n),n âˆ’1
n âˆ’j
)
â‰¤
nâˆ’Yk,n
âˆ‘
v=1
Cv
v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
â‰¤
nâˆ’Yk,n
âˆ‘
v=1
Cv
â‰¤
n
âˆ‘
v=1
2(ln v + 1)
v
â‰¤2(ln n + 1)2,
therefore
0 â‰¤Ì‚H2z âˆ’Ì‚Hâˆ—
2z
=
âˆ‘
kâ‰¥K(n)
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
Cv
v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)

106
Statistical Implications of Turingâ€™s Formula
âˆ’Ì‚pK(n),n
nâˆ’YK(n),n
âˆ‘
v=1
Cv
v
âˆ
j=1
(
1 âˆ’
YK(n),n âˆ’1
n âˆ’j
)
=
âˆ‘
kâ‰¥K(n)
Ì‚pk,n
[nâˆ’Yk,n
âˆ‘
v=1
Cv
v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
âˆ’
nâˆ’YK(n),n
âˆ‘
v=1
Cv
v
âˆ
j=1
(
1 âˆ’
YK(n),n âˆ’1
n âˆ’j
) ]
â‰¤2(ln n + 1)2 âˆ‘
kâ‰¥K(n)
Ì‚pk,n.
By Lemma 3.9,
(ln n + 1)2E
(
âˆ‘
kâ‰¥K(n)
Ì‚pk,n
)
= (ln n + 1)2 âˆ‘
kâ‰¥K(n)
pk,n â†’0,
therefore, Ì‚H2z âˆ’Ì‚Hâˆ—
2z = op(1) follows Chebyshevâ€™s inequality.
â—½
Lemma 3.17
As n â†’âˆ, under the conditions of Theorem 3.14,
E( Ì‚Hâˆ—
2z) âˆ’Hâˆ—
2 â†’0.
Proof:
0 â‰¤Hâˆ—
2 âˆ’E( Ì‚Hâˆ—
2z)
=
âˆ
âˆ‘
v=1
Cvğœâˆ—
v âˆ’
nâˆ’1
âˆ‘
v=1
Cvğœâˆ—
v
=
âˆ
âˆ‘
v=n
Cvğœâˆ—
v
=
âˆ
âˆ‘
v=n
Cv
K(n)
âˆ‘
k=1
pk,n(1 âˆ’pk,n)v
=
K(n)
âˆ‘
k=1
pk,n
âˆ
âˆ‘
v=n
Cv(1 âˆ’pk,n)v
â‰¤2(ln n + 1)
n
K(n)
âˆ‘
k=1
pk,n
âˆ
âˆ‘
v=n
(1 âˆ’pk,n)v
â‰¤2(ln n + 1)
n
K(n)
âˆ‘
k=1
pk,n
(1 âˆ’pk,n)n
pk,n

Estimation of Shannonâ€™s Entropy
107
= 2(ln n + 1)
n
K(n)
âˆ‘
k=1
(1 âˆ’pk,n)n
â‰¤2(ln n + 1)K(n)
n
â†’0.
â—½
Lemma 3.18
Under the conditions of Theorem 3.14, Ì‚Hâˆ—
2z âˆ’Ì‚Hâˆ—
2 = op(1).
A proof of Lemma 3.18 is given in the appendix of this chapter.
Proof of Corollary 3.6: Note that
Ì‚H2z âˆ’H2 = ( Ì‚H2z âˆ’Ì‚Hâˆ—
2z) + ( Ì‚Hâˆ—
z âˆ’Ì‚Hâˆ—
2) + ( Ì‚Hâˆ—
2 âˆ’Ì‚H2) + ( Ì‚H2 âˆ’H2).
Each of the ï¬rst three terms in the above-mentioned equation is op(1) by Lem-
mas 3.16, 3.18, and 3.15, respectively. Also, it is shown, in the proof of Corol-
lary 1 of Zhang and Zhang (2012), that Ì‚H2 âˆ’H2 = op(1). Therefore, Ì‚H2z âˆ’H2 =
op(1).
By Theorem 3.14, and the fact that Ì‚H2
z
p
âˆ’âˆ’âˆ’â†’H2,
Ì‚H2z âˆ’Ì‚H2
z
p
âˆ’âˆ’âˆ’â†’H2 âˆ’H2 = ğœ2.
Finally, the corollary follows Slutskyâ€™s lemma.
â—½
Remark 3.3
The suï¬ƒcient condition (Part 2) given in Theorem 3.14 for the
normality of Ì‚Hz is slightly more restrictive than that of the plug-in estimator
Ì‚H as stated in Theorem 1 of Zhang and Zhang (2012), and consequently sup-
ports a smaller class of distributions. It can be shown that the suï¬ƒcient condi-
tions of Theorem 3.14 still holds for pk = Cğœ†kâˆ’ğœ†where ğœ†> 2, but not for pk =
Câˆ•(k2ln2k), which satisï¬es the suï¬ƒcient conditions of Theorem 1 of Zhang and
Zhang (2012). However, simulation results indicate that the asymptotic normal-
ity of Ì‚Hz in Theorem 3.14 and Corollary 3.6 may still hold for pk = Câˆ•(k2ln2k) for
k â‰¥1 though not covered by the suï¬ƒcient condition, which leads to a conjecture
that the suï¬ƒcient condition of Theorem 3.14 could be further relaxed.
3.4
Appendix
3.4.1
Proof of Lemma 3.2
Proof of Part 1: As the notation in gn(p) suggests, the range for v is from 1 to
min{n âˆ’1, n(1 âˆ’p) + 1}. Noting
0 â‰¤1 âˆ’jâˆ•[n(1 âˆ’p)]
1 âˆ’jâˆ•n
â‰¤1

108
Statistical Implications of Turingâ€™s Formula
subject to j â‰¤n(1 âˆ’p), by Lemma 3.1,
| w(v + 1, n)
vâˆ’1
âˆ
j=0
(
1 âˆ’p âˆ’j
n
)
âˆ’(1 âˆ’p)v |
= (1 âˆ’p)v
|||||||
vâˆ’1
âˆ
j=0
â›
âœ
âœâ
1 âˆ’
j
n(1âˆ’p)
1 âˆ’j+1
n
â
âŸ
âŸâ 
âˆ’1
|||||||
= (1 âˆ’p)v
|||||||
(
n
n âˆ’v
)
vâˆ’1
âˆ
j=0
â›
âœ
âœâ
1 âˆ’
j
n(1âˆ’p)
1 âˆ’j
n
â
âŸ
âŸâ 
âˆ’1
|||||||
= (1 âˆ’p)v
|||||||
(
1 +
v
n âˆ’v
)
vâˆ’1
âˆ
j=0
â›
âœ
âœâ
1 âˆ’
j
n(1âˆ’p)
1 âˆ’j
n
â
âŸ
âŸâ 
âˆ’1
|||||||
â‰¤(1 âˆ’p)v (
v
n âˆ’v
)
+ (1 âˆ’p)v
|||||||
vâˆ’1
âˆ
j=0
â›
âœ
âœâ
1 âˆ’
j
n(1âˆ’p)
1 âˆ’j
n
â
âŸ
âŸâ 
âˆ’1
|||||||
â‰¤(1 âˆ’p)v (
v
n âˆ’v
)
+ (1 âˆ’p)v
vâˆ’1
âˆ‘
j=0
|||||||
â›
âœ
âœâ
1 âˆ’
j
n(1âˆ’p)
1 âˆ’j
n
â
âŸ
âŸâ 
âˆ’1
|||||||
= (1 âˆ’p)v (
v
n âˆ’v
)
+ (1 âˆ’p)v
p
1 âˆ’p
vâˆ’1
âˆ‘
j=1
j
n âˆ’j
â‰¤(1 âˆ’p)vâˆ’1 (
v
n âˆ’v
)
+ (1 âˆ’p)vâˆ’1
vâˆ’1
âˆ‘
j=1
j
n âˆ’j
= (1 âˆ’p)vâˆ’1
vâˆ‘
j=1
j
n âˆ’j â‰¤(1 âˆ’p)vâˆ’1
v2
n âˆ’v.
For a suï¬ƒciently large n, let Vn = âŒŠn1âˆ•8âŒ‹where âŒŠâ‹…âŒ‹is the ï¬‚oor of a real value.
âˆš
n|hn(p) âˆ’h(p)|
â‰¤
âˆš
np
âŒŠn(1âˆ’p)+1âŒ‹
âˆ‘
v=1
1
v
||||||
w(v + 1, n)
vâˆ’1
âˆ
j=0
(
1 âˆ’p âˆ’j
n
)
âˆ’(1 âˆ’p)v
||||||
+
âˆš
np
âˆ
âˆ‘
v=âŒŠn(1âˆ’p)+2âŒ‹
1
v(1 âˆ’p)v
=
âˆš
np
Vn
âˆ‘
v=1
1
v
||||||
w(v + 1, n)
vâˆ’1
âˆ
j=0
(
1 âˆ’p âˆ’j
n
)
âˆ’(1 âˆ’p)v
||||||

Estimation of Shannonâ€™s Entropy
109
+
âˆš
np
âŒŠn(1âˆ’p)+1âŒ‹
âˆ‘
v=Vn+1
1
v
||||||
w(v + 1, n)
vâˆ’1
âˆ
j=0
(
1 âˆ’p âˆ’j
n
)
âˆ’(1 âˆ’p)v
||||||
+
âˆš
np
âˆ
âˆ‘
v=âŒŠn(1âˆ’p)+2âŒ‹
1
v(1 âˆ’p)v
=âˆ¶Î”1 + Î”2 + Î”3.
Î”1 â‰¤
âˆš
np
Vn
âˆ‘
v=1
v
n âˆ’v(1 âˆ’p)vâˆ’1
â‰¤
n5âˆ•8
n âˆ’n1âˆ•8 â†’0.
Î”2 â‰¤p
âˆš
n
âŒŠn(1âˆ’p)+1âŒ‹
âˆ‘
v=Vn+1
v
n âˆ’v(1 âˆ’p)vâˆ’1
â‰¤p
âˆš
n[n(1 âˆ’p) + 1]
np âˆ’1
âŒŠn(1âˆ’p)+1âŒ‹
âˆ‘
v=Vn+1
(1 âˆ’p)vâˆ’1
â‰¤
âˆš
n[n(1 âˆ’p) + 1]
np âˆ’1
(1 âˆ’p)âŒŠn1âˆ•8âŒ‹
â‰¤
âˆš
n[n(1 âˆ’c) + 1]
nc âˆ’1
(1 âˆ’c)âŒŠn1âˆ•8âŒ‹â†’0.
Î”3 â‰¤
âˆš
n
n(1 âˆ’p)(1 âˆ’p)âŒŠn(1âˆ’p)+2âŒ‹
=
1
âˆš
n
(1 âˆ’p)âŒŠn(1âˆ’p)+1âŒ‹
â‰¤
1
âˆš
n
â†’0.
Hence, suppâˆˆ(c,1)
âˆš
n|hn(p) âˆ’h(p)| â†’0.
â—½
Proof of Part 2: The proof is identical to that of the above-mentioned Part 1
until the expression Î”1 + Î”2 + Î”3 where each term is to be evaluated on the
interval [1âˆ•n, c]. It is clear that Î”1 â‰¤O(nâˆ’3âˆ•8). For Î”2, since n(1 âˆ’p) + 1 at
p = 1âˆ•n is n > n âˆ’1, it follows that
Î”2 â‰¤p
âˆš
n
min{nâˆ’1,âŒŠn(1âˆ’p)+1âŒ‹}
âˆ‘
v=Vn+1
v
n âˆ’v(1 âˆ’p)vâˆ’1
â‰¤p
âˆš
n
min{nâˆ’1,âŒŠn(1âˆ’1âˆ•n)+1âŒ‹}
âˆ‘
v=Vn+1
v
n âˆ’v(1 âˆ’p)vâˆ’1

110
Statistical Implications of Turingâ€™s Formula
= p
âˆš
n
nâˆ’1
âˆ‘
v=Vn+1
v
n âˆ’v(1 âˆ’p)vâˆ’1
< p
âˆš
n(n âˆ’1)
nâˆ’1
âˆ‘
v=Vn+1
(1 âˆ’p)vâˆ’1
<
âˆš
n(n âˆ’1)(1 âˆ’p)Vn <
âˆš
n(n âˆ’1) = O(n3âˆ•2).
Î”3 = p
âˆš
n
âˆ
âˆ‘
v=min{nâˆ’1,âŒŠn(1âˆ’p)+1âŒ‹}+1
1
v(1 âˆ’p)v
< p
âˆš
n
âˆ
âˆ‘
v=1
1
v(1 âˆ’p)v <
âˆš
n = O(n1âˆ•2).
Therefore, Î”1 + Î”2 + Î”3 = O(n3âˆ•2).
â—½
Proof of Part 3: Let Z and ğœ™(z) be a standard normal random variable and its
density function, respectively, and let â€œâ‰ƒâ€ denote asymptotic equality. Since
âˆš
n(Ì‚p âˆ’p)
L
âˆ’âˆ’âˆ’â†’N(0, p(1 âˆ’p)),
P(Ì‚p â‰¤c) â‰ƒâˆ«
âˆš
n(câˆ’p)âˆ•
âˆš
p(1âˆ’p)
âˆ’âˆ
ğœ™(z)dz
= âˆ«
âˆ
âˆš
n(pâˆ’c)âˆ•
âˆš
p(1âˆ’p)
ğœ™(z)dz
< âˆ«
âˆ
âˆš
n(pâˆ’c)âˆ•
âˆš
p(1âˆ’p)
eâˆ’z [
âˆš
n(pâˆ’c)âˆ•
âˆš
p(1âˆ’p)] dz
=
âˆš
p(1 âˆ’p)
âˆš
n(p âˆ’c) âˆ«
âˆ
[
âˆš
n(pâˆ’c)âˆ•
âˆš
p(1âˆ’p)]2 eâˆ’xdx
=
âˆš
p(1 âˆ’p)
âˆš
n(p âˆ’c)
exp{âˆ’[
âˆš
n(p âˆ’c)âˆ•
âˆš
p(1 âˆ’p)]2}
= nâˆ’1âˆ•2
âˆš
p(1 âˆ’p)
(p âˆ’c)
exp
{
âˆ’n(p âˆ’c)2
p(1 âˆ’p)
}
.
â—½
3.4.2
Proof of Lemma 3.5
Proof: Let m = v + 1. By Lemmas 3.3 and 3.4, and identity
( n
m
)âˆ’1
m
âˆ‘
c=1
c
(m
c
) (n âˆ’m
m âˆ’c
)
= m2
n ,
Var(Z1,v) â‰¤
( n
m
)âˆ’1
m
âˆ‘
c=1
c
(m
c
) (n âˆ’m
m âˆ’c
) ğœ2
m
m = m
n ğœ2
m.
(3.38)

Estimation of Shannonâ€™s Entropy
111
Consider
ğœ2
m = Var(ğœ“(X1,â€¦, Xm))
= E(ğœ“(X1,â€¦, Xm))2 âˆ’
[ K
âˆ‘
k=1
pk(1 âˆ’pk)mâˆ’1
]2
.
Let Y (m)
k
denote the frequency of the kth letter in the sample of size m.
ğœ2
m â‰¤E(ğœ“(X1,â€¦, Xm))2
= 1
m2 E
[ ( K
âˆ‘
k=1
1[Yk = 1]
) ( K
âˆ‘
kâ€²=1
1[Ykâ€² = 1]
) ]
= 1
m2 E
( K
âˆ‘
k=1
1[Yk = 1] + 2
âˆ‘
1â‰¤k<kâ€²â‰¤K
1[Yk = 1]1[Ykâ€² = 1]
)
= 1
m
K
âˆ‘
k=1
pk(1 âˆ’pk)mâˆ’1 + 2(m âˆ’1)
m
âˆ‘
1â‰¤k<kâ€²â‰¤K
pkpkâ€² (1 âˆ’pk âˆ’pkâ€² )mâˆ’2
â‰¤1
m
K
âˆ‘
k=1
pk(1 âˆ’pk)mâˆ’1 + 2
âˆ‘
1â‰¤k<kâ€²â‰¤K
pkpkâ€²(1 âˆ’pk âˆ’pkâ€² + pkpkâ€² )mâˆ’2
= 1
m
K
âˆ‘
k=1
pk(1 âˆ’pk)mâˆ’1 + 2
âˆ‘
1â‰¤k<kâ€²â‰¤K
[pk(1 âˆ’pk)mâˆ’2pkâ€²(1 âˆ’pkâ€²)mâˆ’2]
â‰¤1
m
K
âˆ‘
k=1
pk(1 âˆ’pk)mâˆ’1 +
[ K
âˆ‘
k=1
pk(1 âˆ’pk)mâˆ’2
]2
= 1
m ğœmâˆ’1 + ğœ2
mâˆ’2.
By (3.38), Var(Z1,v) â‰¤1
n ğœv + v+1
n
ğœ2
vâˆ’1.
â—½
3.4.3
Proof of Corollary 3.5
Proof: Noting ğœv â‰¥ğœw if v â‰¤w,
Mn = 5
n
(nâˆ’1
âˆ‘
v=1
1
v ğœ1âˆ•2
vâˆ’1
)2
= 5
n
(nâˆ’1
âˆ‘
v=1
1
v ğœ1âˆ•2
vâˆ’1
) (nâˆ’1
âˆ‘
w=1
1
w ğœ1âˆ•2
wâˆ’1
)

112
Statistical Implications of Turingâ€™s Formula
= 5
n
(nâˆ’1
âˆ‘
v=1
1
v2 ğœvâˆ’1 + 2
âˆ‘
1â‰¤v<wâ‰¤nâˆ’1
1
vw ğœ1âˆ•2
1,vâˆ’1 ğœ1âˆ•2
1,wâˆ’1
)
â‰¤5
n
(nâˆ’1
âˆ‘
v=1
1
v2 ğœvâˆ’1 + 2
âˆ‘
1â‰¤v<wâ‰¤nâˆ’1
1
vw ğœvâˆ’1
)
= 5
n
(nâˆ’1
âˆ‘
w=1
1
w
nâˆ’1
âˆ‘
v=1
1
v ğœvâˆ’1
)
= 5
n
(nâˆ’1
âˆ‘
w=1
1
w
) (nâˆ’1
âˆ‘
v=1
1
v ğœvâˆ’1
)
.
The expression in the ï¬rst pair of parentheses above is the harmonic series,
which has a well-known upper bound 1 + ln(n âˆ’1). Consider the expression in
the second pair of parentheses.
nâˆ’1
âˆ‘
vâˆ’1
1
v ğœvâˆ’1 = 1 +
nâˆ’1
âˆ‘
v=2
1
v ğœvâˆ’1 < 1 +
nâˆ’1
âˆ‘
v=2
1
v âˆ’1 ğœvâˆ’1
= 1 +
n
âˆ‘
v=1
1
v ğœv
< 1 +
âˆ
âˆ‘
v=1
1
v ğœv
= 1 + H.
Therefore,
Mn < 5(1 + H)
(1 + ln(n âˆ’1)
n
)
= îˆ»(ln nâˆ•n).
â—½
3.4.4
Proof of Lemma 3.14
Proof:
âˆš
n( Ì‚Hâˆ—
z âˆ’Ì‚Hâˆ—) =
âˆš
n
(
Ì‚Hâˆ—
z +
K(n)
âˆ‘
k=1
Ì‚pk,n ln Ì‚pk,n
)
=
{
âˆš
n
[
Ì‚Hâˆ—
z âˆ’
K(n)
âˆ‘
k=1
nâˆ’Yk,n
âˆ‘
v=1
1
v Ì‚pk,n(1 âˆ’Ì‚pk,n)v
] }
âˆ’
{
âˆš
n
K(n)
âˆ‘
k=1
âˆ
âˆ‘
v=nâˆ’Yk,n+1
1
v Ì‚pk,n(1 âˆ’Ì‚pk,n)v
}
=âˆ¶{îˆ­1} âˆ’{îˆ­2} .

Estimation of Shannonâ€™s Entropy
113
Since
0 â‰¤îˆ­2 =
âˆš
n
K(n)
âˆ‘
k=1
âˆ
âˆ‘
v=nâˆ’Yk,n+1
1
v Ì‚pk,n(1 âˆ’Ì‚pk,n)v
â‰¤
K(n)
âˆ‘
k=1
âˆš
n
n âˆ’Yk,n + 1 Ì‚pk,n
âˆ
âˆ‘
v=nâˆ’Yk,n+1
(1 âˆ’Ì‚pk,n)v
=
K(n)
âˆ‘
k=1
âˆš
n
n âˆ’Yk,n + 1(1 âˆ’Ì‚pk,n)nâˆ’Yk,n+1
â‰¤
1
âˆš
n
K(n)
âˆ‘
k=1
1
1 âˆ’Ì‚pk,n + 1âˆ•n(1 âˆ’Ì‚pk,n)nâˆ’Yk,n+1
=
1
âˆš
n
K(n)
âˆ‘
k=1
1
1 âˆ’Ì‚pk,n + 1âˆ•n(1 âˆ’Ì‚pk,n)nâˆ’Yk,n+11[Ì‚pk,n < 1]
â‰¤
1
âˆš
n
K(n)
âˆ‘
k=1
1
1 âˆ’Ì‚pk,n
(1 âˆ’Ì‚pk,n)nâˆ’Yk,n+11[Ì‚pk,n < 1]
=
1
âˆš
n
K(n)
âˆ‘
k=1
(1 âˆ’Ì‚pk,n)nâˆ’Yk,n1[Ì‚pk,n < 1] â‰¤K(n)
âˆš
n
,
îˆ­2
a.s.
âˆ’âˆ’âˆ’â†’0 and therefore îˆ­2
p
âˆ’âˆ’âˆ’â†’0.
Before considering îˆ­1, following are several facts that are to be noted. First
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
â‰¥
(
1 âˆ’
Yk,n
n
)
= (1 âˆ’Ì‚pk,n)
if and only if
0 â‰¤j â‰¤
n
Yk,n + 1[Yk,n = 0] =âˆ¶
1
Ì‚pâˆ—
k,n
;
(3.39)
and second, after a few algebraic steps, Zâˆ—
1,v may be expressed as
Zâˆ—
1,v =
K(n)
âˆ‘
k=1
Ì‚pk,n
v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
=
K(n)
âˆ‘
k=1
Ì‚pk,n
Jkâˆ§v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
v
âˆ
j=Jkâˆ§v+1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
(3.40)
where Jk = âŒŠ1âˆ•Ì‚pâˆ—
k,nâŒ‹and âˆb
v=a(â‹…) = 1 if a > b.

114
Statistical Implications of Turingâ€™s Formula
îˆ­1 =
âˆš
n
[
Ì‚Hâˆ—
z âˆ’
K(n)
âˆ‘
k=1
nâˆ’Yk,n
âˆ‘
v=1
1
v Ì‚pk,n(1 âˆ’Ì‚pk,n)v
]
=
âˆš
n
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
1
v
[Jkâˆ§v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
v
âˆ
j=Jkâˆ§v+1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
=
{
âˆš
n
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
1
v
[Jkâˆ§v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
Ã—
v
âˆ
j=Jkâˆ§v+1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
âˆ’
Jkâˆ§v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
(1 âˆ’Ì‚pk,n)0âˆ¨(vâˆ’Jk)
] }
+
{
âˆš
n
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
1
v
[Jkâˆ§v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
(1 âˆ’Ì‚pk,n)0âˆ¨(vâˆ’Jk)
=âˆ¶{îˆ­1,1} + {îˆ­1,2} .
By (3.39), îˆ­1,1 â‰¤0 and îˆ­1,2 â‰¥0. It suï¬ƒces to show that E(îˆ­1,1) â†’0 and
E(îˆ­1,2) â†’0, respectively.
îˆ­1,1 =
âˆš
n ( Ì‚Hâˆ—
z âˆ’Hâˆ—)
âˆ’
âˆš
n
[K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
1
v
Jkâˆ§v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
(1 âˆ’Ì‚pk,n)0âˆ¨(vâˆ’Jk)
âˆ’Hâˆ—
]
=
{âˆš
n ( Ì‚Hâˆ—
z âˆ’Hâˆ—)}
âˆ’
{
âˆš
n
[K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
1
v
Jkâˆ§v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
(1 âˆ’Ì‚pk,n)0âˆ¨(vâˆ’Jk)
âˆ’
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
1
v (1 âˆ’Ì‚pk,n)v
] }
âˆ’
{
âˆš
n
[K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
1
v (1 âˆ’Ì‚pk,n)v âˆ’Hâˆ—
] }
=âˆ¶{îˆ­1,1,1} âˆ’{îˆ­1,1,2} âˆ’{îˆ­1,1,3} .

Estimation of Shannonâ€™s Entropy
115
E(îˆ­1,1,1) â†’0 follows Lemma 3.11. Then,
îˆ­1,1,3 =
âˆš
n
[K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
1
v (1 âˆ’Ì‚pk,n)v âˆ’Hâˆ—
]
=
âˆš
n
[K(n)
âˆ‘
k=1
Ì‚pk,n
âˆ
âˆ‘
v=1
1
v (1 âˆ’Ì‚pk,n)v âˆ’Hâˆ—
]
âˆ’
âˆš
n
K(n)
âˆ‘
k=1
Ì‚pk,n
âˆ
âˆ‘
v=nâˆ’Yk,n+1
1
v (1 âˆ’Ì‚pk,n)v
= {
âˆš
n ( Ì‚Hâˆ—âˆ’Hâˆ—) } âˆ’
{
âˆš
n
K(n)
âˆ‘
k=1
Ì‚pk,n
âˆ
âˆ‘
v=nâˆ’Yk,n+1
1
v (1 âˆ’Ì‚pk,n)v
}
=âˆ¶{îˆ­1,1,3,1} âˆ’{îˆ­2} .
E(îˆ­1,1,3,1) â†’0 by Lemma 3.12, E(îˆ­2) â†’0 is established above, and therefore
E(îˆ­1,1,3) â†’0.
îˆ­1,1,2 =
âˆš
n
[K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
1
v
Jkâˆ§v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
(1 âˆ’Ì‚pk,n)0âˆ¨(vâˆ’Jk)
âˆ’
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
1
v (1 âˆ’Ì‚pk,n) v
]
â‰¤
âˆš
n
[K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
1
v
Jkâˆ§v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’1
)
(1 âˆ’Ì‚pk,n)0âˆ¨(vâˆ’Jk)
âˆ’
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
1
v (1 âˆ’Ì‚pk,n) v
]
=
âˆš
n
[K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
1
v
(
1 âˆ’
Yk,n âˆ’1
n âˆ’1
)
Jkâˆ§v(1 âˆ’Ì‚pk,n)0âˆ¨(vâˆ’Jk)
âˆ’
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
1
v (1 âˆ’Ì‚pk,n) Jkâˆ§v(1 âˆ’Ì‚pk,n)0âˆ¨(vâˆ’Jk)
]
=
âˆš
n
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
1
v
[ (
1 âˆ’
Yk,n âˆ’1
n âˆ’1
)
Jkâˆ§v âˆ’(1 âˆ’Ì‚pk,n) Jkâˆ§v
]
Ã— (1 âˆ’Ì‚pk,n)0âˆ¨(vâˆ’Jk)

116
Statistical Implications of Turingâ€™s Formula
â‰¤
âˆš
n
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
1
v
[ (
1 âˆ’
Yk,n âˆ’1
n âˆ’1
)Jkâˆ§v
âˆ’(1 âˆ’Ì‚pk,n)Jkâˆ§v
]
â‰¤
âˆš
n
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
1
v
[
(Jk âˆ§v)
(
1 âˆ’
Yk,n âˆ’1
n âˆ’1
)Jkâˆ§vâˆ’1 n âˆ’Yk,n
n(n âˆ’1)
]
(by Lemma 3.13)
â‰¤
âˆš
n
n âˆ’1
K(n)
âˆ‘
k=1
Ì‚pk,n(1 âˆ’Ì‚pk,n)
[nâˆ’Yk,n
âˆ‘
v=1
1
v(Jk âˆ§v)
]
=
âˆš
n
n âˆ’1
K(n)
âˆ‘
k=1
Ì‚pk,n(1 âˆ’Ì‚pk,n)
[ Jk
âˆ‘
v=1
1
v(Jk âˆ§v) +
nâˆ’Yk,n
âˆ‘
v=Jk+1
1
v(Jk âˆ§v)
]
=
âˆš
n
n âˆ’1
K(n)
âˆ‘
k=1
Ì‚pk,n(1 âˆ’Ì‚pk,n)
(
Jk + Jk
nâˆ’Yk,n
âˆ‘
v=Jk+1
1
v
)
â‰¤
âˆš
n
n âˆ’1
K(n)
âˆ‘
k=1
Ì‚pk,n
(
Jk + Jk
n
âˆ‘
v=1
1
v
)
â‰¤
âˆš
n
n âˆ’1
K(n)
âˆ‘
k=1
Yk,n
n
n
Yk,n + 1[Yk,n = 0] (ln n + 2)
â‰¤
âˆš
nK(n)(ln n + 2)
n âˆ’1
.
Therefore,
E(îˆ­1,1,2) â‰¤îˆ»
( âˆš
nK(n) ln n
n
)
â†’0.
Finally, E(îˆ­1,2) = E(îˆ­1,1,2) â†’0. It follows that
âˆš
n( Ì‚Hâˆ—
z âˆ’Ì‚Hâˆ—) = op(1).
â—½
3.4.5
Proof of Lemma 3.18
Proof:
Ì‚Hâˆ—
2z âˆ’Ì‚Hâˆ—
2 = Ì‚Hâˆ—
2z âˆ’
K(n)
âˆ‘
k=1
Ì‚pk,nln2 Ì‚pk,n
=
{
Ì‚Hâˆ—
2z âˆ’
K(n)
âˆ‘
k=1
nâˆ’Yk,n
âˆ‘
v=1
Cv Ì‚pk,n(1 âˆ’Ì‚pk,n)v
}
âˆ’
{K(n)
âˆ‘
k=1
âˆ
âˆ‘
v=nâˆ’Yk,n+1
Cv Ì‚pk,n(1 âˆ’Ì‚pk,n)v
}
=âˆ¶{îˆ®1} âˆ’{îˆ®2} .

Estimation of Shannonâ€™s Entropy
117
Since
0 â‰¤îˆ®2 =
K(n)
âˆ‘
k=1
âˆ
âˆ‘
v=nâˆ’Yk,n+1
Cv Ì‚pk,n(1 âˆ’Ì‚pk,n)v
â‰¤
K(n)
âˆ‘
k=1
2(ln n + 1)
n âˆ’Yk,n + 1 Ì‚pk,n
âˆ
âˆ‘
v=nâˆ’Yk,n+1
(1 âˆ’Ì‚pk,n)v
=
K(n)
âˆ‘
k=1
2(ln n + 1)
n âˆ’Yk,n + 1(1 âˆ’Ì‚pk,n)nâˆ’Yk,n+1
â‰¤2(ln n + 1)
n
K(n)
âˆ‘
k=1
1
1 âˆ’Ì‚pk,n + 1âˆ•n(1 âˆ’Ì‚pk,n)nâˆ’Yk,n+1
= 2(ln n + 1)
n
K(n)
âˆ‘
k=1
1
1 âˆ’Ì‚pk,n + 1âˆ•n(1 âˆ’Ì‚pk,n)nâˆ’Yk,n+11[Ì‚pk,n < 1]
â‰¤2(ln n + 1)
n
K(n)
âˆ‘
k=1
1
1 âˆ’Ì‚pk,n
(1 âˆ’Ì‚pk,n)nâˆ’Yk,n+11[Ì‚pk,n < 1]
= 2(ln n + 1)
n
K(n)
âˆ‘
k=1
(1 âˆ’Ì‚pk,n)nâˆ’Yk,n1[Ì‚pk,n < 1]
â‰¤2(ln n + 1)K(n)
n
,
îˆ®2
a.s.
âˆ’âˆ’âˆ’â†’0 and therefore îˆ®2
p
âˆ’âˆ’âˆ’â†’0.
Next,
îˆ®1 = Ì‚Hâˆ—
2z âˆ’
K(n)
âˆ‘
k=1
nâˆ’Yk,n
âˆ‘
v=1
Cv Ì‚pk,n(1 âˆ’Ì‚pk,n)v
=
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
Cv
[Jkâˆ§v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
v
âˆ
j=Jkâˆ§v+1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
âˆ’(1 âˆ’Ì‚pk,n)v
]
=
{K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
Cv
[Jkâˆ§v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
v
âˆ
j=Jkâˆ§v+1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
âˆ’
Jkâˆ§v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
(1 âˆ’Ì‚pk,n)0âˆ¨(vâˆ’Jk)
] }
+
{K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
Cv
[Jkâˆ§v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
(1 âˆ’Ì‚pk,n)0âˆ¨(vâˆ’Jk)(1 âˆ’Ì‚pk,n)v
]}
=âˆ¶{îˆ®1,1} + {îˆ®1,2} .

118
Statistical Implications of Turingâ€™s Formula
By (3.39), îˆ®1,1 â‰¤0 and îˆ®1,2 â‰¥0. It suï¬ƒces to show that E(îˆ®1,1) â†’0 and
E(îˆ®1,2) â†’0, respectively.
îˆ®1,1 = ( Ì‚Hâˆ—
2z âˆ’Hâˆ—
2)
âˆ’
[K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
Cv
Jkâˆ§v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
(1 âˆ’Ì‚pk,n)0âˆ¨(vâˆ’Jk)
âˆ’Hâˆ—
2
]
= { Ì‚Hâˆ—
2z âˆ’Hâˆ—
2
}
âˆ’
{K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
Cv
Jkâˆ§v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
(1 âˆ’Ì‚pk,n)0âˆ¨(vâˆ’Jk)
âˆ’
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
Cv (1 âˆ’Ì‚pk,n)v
}
âˆ’
{K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
Cv (1 âˆ’Ì‚pk,n)v âˆ’Hâˆ—
2
}
=âˆ¶{îˆ®1,1,1} âˆ’{îˆ®1,1,2} âˆ’{îˆ®1,1,3} .
E(îˆ®1,1,1) â†’0 follows Lemma 3.17.
Next,
îˆ®1,1,3 =
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
Cv (1 âˆ’Ì‚pk,n)v âˆ’Hâˆ—
2
=
[K(n)
âˆ‘
k=1
Ì‚pk,n
âˆ
âˆ‘
v=1
Cv (1 âˆ’Ì‚pk,n)v âˆ’Hâˆ—
2
]
âˆ’
K(n)
âˆ‘
k=1
Ì‚pk,n
âˆ
âˆ‘
v=nâˆ’Yk,n+1
Cv (1 âˆ’Ì‚pk,n)v
= { Ì‚Hâˆ—
2 âˆ’Hâˆ—
2} âˆ’
{K(n)
âˆ‘
k=1
Ì‚pk,n
âˆ
âˆ‘
v=nâˆ’Yk,n+1
Cv (1 âˆ’Ì‚pk,n)v
}
=âˆ¶{îˆ®1,1,3,1} âˆ’{îˆ®2} .
The proof of E(îˆ®1,1,3,1) â†’0 is implied by the proof of Corollary 1 of Zhang
and Zhang (2012). Also, E(îˆ®2) â†’0 is established above, and therefore,
E(îˆ®1,1,3) â†’0.

Estimation of Shannonâ€™s Entropy
119
Next,
îˆ®1,1,2 =
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
Cv
Jkâˆ§v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’j
)
(1 âˆ’Ì‚pk,n)0âˆ¨(vâˆ’Jk)
âˆ’
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
Cv (1 âˆ’Ì‚pk,n)v
â‰¤
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
Cv
Jkâˆ§v
âˆ
j=1
(
1 âˆ’
Yk,n âˆ’1
n âˆ’1
)
(1 âˆ’Ì‚pk,n)0âˆ¨(vâˆ’Jk)
âˆ’
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
Cv (1 âˆ’Ì‚pk,n)v
=
[K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
Cv
(
1 âˆ’
Yk,n âˆ’1
n âˆ’1
)Jkâˆ§v
(1 âˆ’Ì‚pk,n)0âˆ¨(vâˆ’Jk)
âˆ’
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
Cv (1 âˆ’Ì‚pk,n)Jkâˆ§v(1 âˆ’Ì‚pk,n)0âˆ¨(vâˆ’Jk)
]
=
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
Cv
[ (
1 âˆ’
Yk,n âˆ’1
n âˆ’1
)Jkâˆ§v
âˆ’(1 âˆ’Ì‚pk,n) Jkâˆ§v] (1 âˆ’Ì‚pk,n)0âˆ¨(vâˆ’Jk)
â‰¤
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
Cv
[ (
1 âˆ’
Yk,n âˆ’1
n âˆ’1
)Jkâˆ§v
âˆ’(1 âˆ’Ì‚pk,n)Jkâˆ§v
]
â‰¤
K(n)
âˆ‘
k=1
Ì‚pk,n
nâˆ’Yk,n
âˆ‘
v=1
Cv
[
(Jk âˆ§v)
(
1 âˆ’
Yk,n âˆ’1
n âˆ’1
)Jkâˆ§vâˆ’1 n âˆ’Yk,n
n(n âˆ’1)
]
(by Lemma 3.13)
â‰¤
1
n âˆ’1
K(n)
âˆ‘
k=1
Ì‚pk,n(1 âˆ’Ì‚pk,n)
[nâˆ’Yk,n
âˆ‘
v=1
Cv(Jk âˆ§v)
]
â‰¤
1
n âˆ’1
K(n)
âˆ‘
k=1
Ì‚pk,n(1 âˆ’Ì‚pk,n)Jk
nâˆ’Yk,n
âˆ‘
v=1
Cv
â‰¤
1
n âˆ’1
K(n)
âˆ‘
k=1
Ì‚pk,n(1 âˆ’Ì‚pk,n)Jk
n
âˆ‘
v=1
2(ln n + 1)
v
â‰¤2(ln n + 1)
n âˆ’1
K(n)
âˆ‘
k=1
Ì‚pk,nJk(ln n + 1)

120
Statistical Implications of Turingâ€™s Formula
â‰¤2(ln n + 1)2
n âˆ’1
K(n)
âˆ‘
k=1
Yk,n
n
n
Yk,n + 1[Yk,n = 0]
â‰¤2(ln n + 1)2K(n)
n âˆ’1
.
Therefore,
E(îˆ®1,1,2) â‰¤îˆ»
(
K(n)ln2n
n
)
â†’0.
Finally, E(îˆ®1,2) = E(îˆ®1,1,2) â†’0. It follows that Ì‚Hâˆ—
2z âˆ’Ì‚Hâˆ—
2 = op(1).
â—½
3.5
Remarks
The existing literature on entropy estimation seems to suggest a heavy focus
on bias reduction. Although bias reduction is undoubtedly important, an argu-
ment could be made to support a shift in research focus to other worthy direc-
tions. For example, the distributional characteristics of proposed estimators
are, with a few exceptions, largely ignored in the literature. The lack of results
in weak convergence of proposed entropy estimators is perhaps partially due to
the inherent diï¬ƒculty in establishing such results in theory. Nevertheless, one
must have such results in order to make reasonable statistical inference about
the underlying entropy.
As evidenced by each and every one of the many normal laws introduced
in this chapter, the asymptotic variance always contains the second entropic
moment H2 = âˆ‘
kâ‰¥1pkln2pk. To make inference about the underlying entropy
H, one must have not only a good estimator for H but also a good estimator for
H2. Future research on nonparametric estimation of H2 would be beneï¬cial.
Another issue needing more investigation is the sample size consideration in
estimating entropy. How large should sample size n be in order to produce a
reasonably reliable estimate of H? Answers to this question would be interest-
ing both in theory and in practice. Many papers in the existing literature discuss
bias reduction in cases of small samples with sizes as small as n = 10 or n = 20.
Given the fact that entropy is meant to be a measure of chaos in a complex
system, it would seem quite forced to estimate entropy based on such small
samples. In the existing literature, there does not seem to exist a guideline on
how large a sample must be to make entropy estimation realistically meaning-
ful. On the other hand, there is a natural perspective based on Turingâ€™s formula.
Simply put, Turingâ€™s formula T1 estimates the sample noncoverage of the alpha-
bet, and therefore 1 âˆ’T1 estimates the sample coverage of the alphabet. Given a
sample data set, if 1 âˆ’T1 is small, then perhaps one should not force an estimate
of entropy. For example, if a data set contains only singletons, that is, N1 = n,
then 1 âˆ’T1 = 0, suggesting that the sample is perhaps not large enough to have
any meaningful coverage.

Estimation of Shannonâ€™s Entropy
121
3.6
Exercises
1
Let X be the number of heads observed when a fair coin is tossed four
times. Find the entropy of X.
2
A fair coin is repeatedly tossed until the ï¬rst head is observed. Let X be
the number of tosses required to observe the ï¬rst head. Find the entropy
of X.
3
Verify the bias (3.3) for the plug-in estimator Ì‚H in (3.2).
4
Verify the bias (3.6) for the jackknife estimator Ì‚HJK in (3.5).
5
Show the following.
a) The Dirichlet prior of (3.7) leads to the Dirichlet posterior of (3.8).
b) For each k, the mean of Dirichlet posterior of (3.8) is
E(pk|y) =
yk + ğ›¼k
n + âˆ‘K
k=1 ğ›¼k
.
6
Show that if {pk} is uniformly distributed, that is, pk = 1âˆ•K for each
k = 1,â€¦, K, then ğœ2 (3.11) is zero.
7
Prove Theorem 3.2.
8
Prove Theorem 3.3.
9
Verify that H < âˆunder the distribution {pk; k â‰¥1} where pk = cğœ†kâˆ’ğœ†
and ğœ†is any real value satisfying ğœ†> 1.
10
In Example 3.4, show
âˆš
n âˆ«
âˆ
K(n)
Cğœ†
xğœ†ln
(Cğœ†
xğœ†
)
dx = Cğœ†ğœ†
ğœ†âˆ’1
âˆš
n ln K(n)
(K(n))ğœ†âˆ’1
+
[
Cğœ†ğœ†
(ğœ†âˆ’1)2 âˆ’Cğœ†ln Cğœ†
ğœ†âˆ’1
]
âˆš
n
(K(n))ğœ†âˆ’1 .
11
Let aj, j = 1,â€¦, n, be real numbers satisfying |aj| â‰¤1 for every j. Show
that |||
âˆn
j=1 aj âˆ’1||| â‰¤âˆ‘n
j=1 |aj âˆ’1|.

122
Statistical Implications of Turingâ€™s Formula
12
For any given distribution {pk} with ï¬nite K, |E( Ì‚Hz2 âˆ’H2)| decays expo-
nentially in n.
13
Suppose pk = Ckâˆ’ğœ†, where C > 0 and ğœ†> 1 are constants, for all k â‰¥k0
where k0 is some positive integer. Show that
lim
nâ†’âˆnğ›¿âˆ‘
k
pk(1 âˆ’pk)n,
where ğ›¿= 1 âˆ’1âˆ•ğœ†, exists. (Hint: Use Eulerâ€“Maclaurin lemma.)
14
Show that
Cv =
vâˆ’1
âˆ‘
i=1
1
i(v âˆ’i) = 1
v
vâˆ’1
âˆ‘
i=1
(1
i +
1
v âˆ’i
)
â‰¤2(ln v + 1)
v
.
15
Show
a) limpâ†’0 p ln p = 0; and
b) limpâ†’0 pln2p = 0.
16
Show that for any p âˆˆ(0, 1) and any nonnegative integer j less or equal to
n(1 âˆ’p),
0 â‰¤1 âˆ’jâˆ•[n(1 âˆ’p)]
1 âˆ’jâˆ•n
â‰¤1.
17
Show that Z1,v in (3.26) may be re-expressed as
Z1,v =
âˆ‘
kâ‰¥1
[
Ì‚pk
v
âˆ
j=1
(
1 âˆ’Yk âˆ’1
n âˆ’j
) ]
.
18
Show that
âˆ‘
kâ‰¥1
ln ln(k + 1)
(k + 1)ln2(k + 1)
< âˆ.
19
Let X be a discrete random variables deï¬ned on the set of natural
numbers â„•, with probability distribution {px; x âˆˆâ„•}. Show that if
E(Xğœ†) < âˆfor some constant ğœ†> 0, then the entropy of X exists, that is,
H(X) = âˆ’âˆ‘âˆ
x=1 px ln px < âˆ.
20
Let X1 and X2 be two random elements on the same alphabet
ğ’³= {ğ“1, ğ“2}
with
probability
distributions
p1 = {p1, 1 âˆ’p1}
and
p2 = {p2, 1 âˆ’p2}, respectively. Let ğœƒbe a bivariate random variable,

Estimation of Shannonâ€™s Entropy
123
independent of X1 and X2, such that P(ğœƒ= 1) = q and P(ğœƒ= 2) = 1 âˆ’q.
Let
Xğœƒ= X11[ğœƒ= 1] + X21[ğœƒ= 2].
Show that
H(Xğœƒ) â‰¥qH(X1) + (1 âˆ’q)H(X2),
with equality if and only if H(ğœƒ) = 0.

125
4
Estimation of Diversity Indices
Diversity is a general concept pertaining to the nature of assortment in a
population with multiple species. Originated in ecology where the diversity
of species in an ecosystem is of great interest, the concept of diversity has
become increasingly relevant in many other ï¬elds of study in modern sciences,
for example, of genetic diversity within a biological species, of word diversity
of an author, of diversity of an investment portfolio, and so on. More generally,
in information science, one is interested in the diversity of letters of some
alphabet. While the meaning of the word, diversity, is quite clear, it is not
always obvious how it may be committed to a quantitative measure. As a
central theme in ecology, a large volume of research can be found on how a
diversity index should be deï¬ned and how it could be estimated. Proposed
diversity indices and their estimators are quite numerous. Shannonâ€™s entropy
introduced in Shannon (1948) and Simpsonâ€™s index introduced in Simpson
(1949) are among the earliest diversity indices found in the literature. A set of
popular diversity indices include Emlenâ€™s index, the Giniâ€“Simpson index, Hillâ€™s
diversity number, RÃ©nyiâ€™s entropy, and Tsallis entropy. These are, respectively,
named after the authors of Emlen (1973), Gini (1912), Hill (1973), RÃ©nyiâ€™s
(1961), and Tsallis (1988). For a comprehensive and in-depth introduction
of the subject, readers may wish to refer to Krebs (1999), Magurran (2004),
and Marcon (2014). In summary, while the ideas of what constitute diversity
indices and how to estimate them are quite diverse, the implied consensus
in the current literature seems to be that diversity is a multifaceted concept
and therefore no single mathematical index should be expected to capture it
entirely.
This chapter describes a uniï¬ed perspective for all of the above-mentioned
diversity indices based on a re-parameterization and a general nonparametric
estimation procedure for these indices.
Statistical Implications of Turingâ€™s Formula, First Edition. Zhiyi Zhang.
Â© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

126
Statistical Implications of Turingâ€™s Formula
4.1
A Uniï¬ed Perspective on Diversity Indices
Consider a population with countably many species, ğ’³= {ğ“k; k â‰¥1},
where each letter ğ“k stands for one distinct species. Let pk = P(X = ğ“k) and
p = {pk; k â‰¥1} where X is a random element drawn from the population.
Denote the cardinality of ğ’³by K = âˆ‘
kâ‰¥11[pk > 0].
Let a general index of diversity be deï¬ned as a function ğœƒ= ğœƒ(ğ’³, p), which
assigns a real value to every given pair {ğ’³, p}. Axiom 4.1 is commonly accepted
as minimal constraints on a diversity index ğœƒ:
Axiom 4.1
A diversity index ğœƒ= ğœƒ(ğ’³, p) satisï¬es
îˆ­01âˆ¶ğœƒ= ğœƒ(ğ’³, p) = ğœƒ(p) and
îˆ­02âˆ¶ğœƒ(p) = ğœƒ(pâˆ—),
where pâˆ—is any rearrangement of p.
Axiom îˆ­01 implies that the value of a diversity index only depends on the
probability distribution on the alphabet and not on the alphabet ğ’³itself. In
other words, a diversity index is determined only by the species proportions in
the population regardless of what the species are.
Axiom A02 implies that a diversity index ğœƒdoes not alter its value if any two
indices, k1 and k2, exchange their integer values, that is to say, ğœƒassumes a
value regardless of the order in which the species are arranged in the popula-
tion. Axiom A02 was ï¬rst formally introduced by RÃ©nyi (1961) as the symmetry
axiom, and it was also known as the permutation invariant property of diversity
indices.
Many more reasonable axiomatic conditions have been discussed in the
diversity literature, but Axion 4.1 encircles the realm of discussion in this
chapter. The following is a list of several commonly used diversity indices
satisfying Axion 4.1.
Example 4.1
Simpsonâ€™s index ğœ†= âˆ‘
kâ‰¥1p2
k.
Example 4.2
The Giniâ€“Simpson index 1 âˆ’ğœ†= âˆ‘
kâ‰¥1pk(1 âˆ’pk).
The Giniâ€“Simpson index is one of the earliest diversity indices discussed in
the literature. It is popular for many reasons. In addition to its simplicity, it
has a probabilistic interpretation with regard to the intuitive notion of diver-
sity. Consider two independently and identically distributed random elements
from ğ’³, say X1 and X2, under a probability distribution, {pk; k â‰¥1}. A more
(or less) diverse population could be partially characterized by a higher (or a

Estimation of Diversity Indices
127
lower) value of P(X1 â‰ X2), that is, the probability that X1 and X2 are not equal.
However,
P(X1 â‰ X2) = 1 âˆ’P(X1 = X2) = 1 âˆ’
âˆ‘
kâ‰¥1
P(X1 = X2 = ğ“k)
= 1 âˆ’
âˆ‘
kâ‰¥1
p2
k = 1 âˆ’ğœ†.
Example 4.3
Shannonâ€™s entropy H = âˆ’âˆ‘
kâ‰¥1 pk ln pk.
Example 4.4
RÃ©nyiâ€™s entropy Hğ›¼= (1 âˆ’ğ›¼)âˆ’1 ln (âˆ‘
kâ‰¥1 pğ›¼
k
) for any ğ›¼> 0,
ğ›¼â‰ 1.
Example 4.5
Tsallisâ€™ entropy Tğ›¼= (1 âˆ’ğ›¼)âˆ’1 (âˆ‘
kâ‰¥1 pğ›¼
k âˆ’1) for any ğ›¼> 0,
ğ›¼â‰ 1.
Example 4.6
Hillâ€™s diversity number Nğ›¼= (âˆ‘
kâ‰¥1 pğ›¼
k
)1âˆ•(1âˆ’ğ›¼) for any ğ›¼> 0,
ğ›¼â‰ 1.
Example 4.7
Emlenâ€™s index D = âˆ‘
kpkeâˆ’pk.
Example 4.8
The richness index K = âˆ‘
kâ‰¥11[pk > 0].
To better facilitate an investigation into the commonalities of these diversity
indices, a notion of equivalence between two diversity indices was introduced
by Zhang and Grabchak (2016) and is given in Deï¬nition 4.1 below.
Deï¬nition 4.1
Two diversity indices, ğœƒ1 and ğœƒ2, are said to be equivalent if
there exists a strictly increasing continuous function g(â‹…) such that ğœƒ1 = g(ğœƒ2).
The equivalence is denoted by ğœƒ1 â‡”ğœƒ2.
Consider two populations, PA and PB, in diversity comparison. Suppose PA
is more diverse than PB by means of an index ğœƒ2, that is, ğœƒ2(PA) > ğœƒ2(PB). Then
since g(â‹…) is strictly increasing,
ğœƒ1(PA) = g(ğœƒ2(PA)) > g(ğœƒ2(PB)) = ğœƒ1(PB).
The reverse of the above-mentioned argument is also true because the inverse
function of g, gâˆ’1, exists and it is also strictly increasing (see Exercise 3). This
argument suggests that the equivalence of Deï¬nition 4.1 is an order-preserving
relationship. That said however, it is to be noted that this relationship does not
preserve the incremental diï¬€erence in diversity between PA and PB, that is, it is
not guaranteed that ğœƒ1(PA) âˆ’ğœƒ1(PB) = ğœƒ2(PA) âˆ’ğœƒ2(PB).

128
Statistical Implications of Turingâ€™s Formula
Example 4.9
RÃ©nyiâ€™s entropy Hğ›¼, Tsallisâ€™ entropy Tğ›¼, and Hillâ€™s diversity num-
ber Nğ›¼are equivalent to each other for ğ›¼âˆˆ(0, 1). To verify this claim, it suï¬ƒces
(why?) to show that all three indices are equivalent to a common core index
hğ›¼=
âˆ‘
kâ‰¥1
pğ›¼
k,
(4.1)
where ğ›¼is a constant satisfying ğ›¼âˆˆ(0, 1). Toward that end, consider the follow-
ing three continuous functions as the function g(x) in Deï¬nition 4.1 for RÃ©nyiâ€™s,
Tsallisâ€™, and Hillâ€™s indices, respectively,
g1(x) =
ln x
(1 âˆ’ğ›¼), g2(x) = x âˆ’1
(1 âˆ’ğ›¼), and g3(x) = x1âˆ•(1âˆ’ğ›¼),
where ğ›¼is a ï¬xed constant in (0, 1). By the fact that each of these functions is con-
tinuous and strictly increasing for x âˆˆ(0, âˆ) and by Deï¬nition 4.1, the desired
result follows.
Recall, in Chapter 2, a special subclass of the generalized Simpsonâ€™s indices
is denoted as
ğœ»1 =
{
ğœv =
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)v; v â‰¥0
}
.
(4.2)
Also recall, in Chapter 3, a ï¬nite Shannonâ€™s entropy has a representation
H =
âˆ‘
vâ‰¥1
1
v ğœv,
which is a linear combination of all the elements in ğœ»1. Finally, recall
Theorem 2.2, which states that a distribution {pk; k â‰¥1} on ğ’³and its asso-
ciated panel of generalized Simpsonâ€™s indices {ğœv; v â‰¥1} uniquely determine
each other up to a permutation on the index set {k; k â‰¥}.
In light of the above-mentioned recalled facts, ğœ»âˆ¶= ğœ»1 = {ğœv; v â‰¥1} deserves
a name for future reference.
Deï¬nition 4.2
For every probability sequence {pk; k â‰¥1} on ğ’³, ğœ»= {ğœv; v â‰¥
1} is said to be the entropic basis of p = {pk; k â‰¥1}.
The following theorem provides a unifying perspective on all diversity indices
satisfying Axioms 4.1.
Theorem 4.1
Given an alphabet ğ’³= {ğ“k; k â‰¥1} and an associated proba-
bility distribution p = {pk; k â‰¥1}, a diversity index ğœƒsatisfying Axioms 4.1 is a
function of the entropic basis ğœ= {ğœv; v â‰¥0}.
Proof: By Theorem 2.2, ğœ»determines p up to a permutation and, in turn, deter-
mines any diversity index ğœƒsatisfying Axioms 4.1.
â—½

Estimation of Diversity Indices
129
The statement of Theorem 4.1 holds for any probability distribution
p regardless of whether K is ï¬nite or inï¬nite. Theorem 4.1 essentially
oï¬€ers a re-parameterization of p (up to a permutation) in terms of ğœ. This
re-parameterization is not just an arbitrary one, it has several statistical
implications. First of all, every element of ğœcontains information about the
entire distribution and not just one frequency pk. This helps to deal with
the problem of estimating probabilities of unobserved species. Second, for a
random sample of size n, there are good estimators of ğœv for v = 1, 2,â€¦, n âˆ’1.
These estimators are introduced in Chapter 2 and are brieï¬‚y recalled in the
following section.
While a general diversity index can be any function of the entropic basis,
most commonly used diversity indices in practice are either linear functions or
equivalent to linear functions of the entropic basis.
Deï¬nition 4.3
A diversity index ğœƒis said to be a linear diversity index if it is
a linear combination of the members of the entropic basis, that is,
ğœƒ= ğœƒ(p) =
âˆ
âˆ‘
v=0
wvğœv =
âˆ
âˆ‘
v=0
wv
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)v
(4.3)
where wv is a function of v such that âˆ‘n
v=0 wvğœv < âˆ.
Deï¬nition 4.3 identiï¬es a subclass of indices among all functions of ğœ, that
is, all diversity indices satisfying Axioms 4.1. Every member of this subclass is
a weighted linear form of {ğœv}. While there are no fundamental reasons why
a search of a good diversity index should be restricted to this subclass, it hap-
pens to cover all of the popular indices in the literature, up to the equivalence
relationship given in Deï¬nition 4.1. The following examples demonstrate the
linearity of several popular diversity indices.
Example 4.10
Simpsonâ€™s index.
ğœ†=
âˆ‘
kâ‰¥1
p2
k = ğœ0 âˆ’ğœ1.
(See Exercise 6.)
Example 4.11
The Giniâ€“Simpson index.
1 âˆ’ğœ†=
âˆ‘
kâ‰¥1
pk(1 âˆ’pk) = ğœ1.
Example 4.12
Shannonâ€™s entropy.
H = âˆ’
âˆ‘
kâ‰¥1
pk ln pk =
âˆ
âˆ‘
v=1
1
v ğœv,
as shown in Chapter 3.

130
Statistical Implications of Turingâ€™s Formula
Example 4.13
The index in (4.1),
hğ›¼=
âˆ‘
kâ‰¥1
pğ›¼
k = ğœ0 +
âˆ
âˆ‘
v=1
[ v
âˆ
i=1
(i âˆ’ğ›¼
i
)
ğœv
]
,
where ğ›¼> 0 and ğ›¼â‰ 1. (See Exercise 7.)
Example 4.14
Emlenâ€™s index.
D =
âˆ‘
kâ‰¥1
pkeâˆ’pk =
âˆ
âˆ‘
v=0
eâˆ’1
v!
ğœv
(see Exercise 8).
Example 4.15
The richness index.
K =
âˆ‘
kâ‰¥1
1[pk > 0]
=
âˆ‘
kâ‰¥1
1[pk > 0]
pk
1 âˆ’(1 âˆ’pk)
=
âˆ‘
kâ‰¥1
1[pk > 0]pk
âˆ
âˆ‘
v=0
(1 âˆ’pk)v
=
âˆ
âˆ‘
v=0
âˆ‘
kâ‰¥1
1[pk > 0]pk(1 âˆ’pk)v =
âˆ
âˆ‘
v=0
ğœv.
The richness indices, K, and the evenness indices, for example, Giniâ€“
Simpsonâ€™s 1 âˆ’ğœ†, are generally thought as two qualitatively very diï¬€erent types
of indices. Many arguments for such a diï¬€erence can be found in the existing
literature (see, for example, Peet (1974); Heip, Herman, and Soetaert (1998),
and Purvis and Hector (2000)). However, Example 4.15 demonstrates that, in
the perspective of entropic basis, they both are linear diversity indices and
merely diï¬€er in the weighting scheme wv in Deï¬nition 4.3.
Example 4.16
The generalized Simpsonâ€™s indices.
ğœu,m =
âˆ‘
kâ‰¥1
pu
k(1 âˆ’pk)m
=
âˆ‘
kâ‰¥1
pk[1 âˆ’(1 âˆ’pk)]uâˆ’1(1 âˆ’pk)m
=
âˆ‘
kâ‰¥1
pk
uâˆ’1
âˆ‘
v=0
(âˆ’1)uâˆ’1âˆ’v (u âˆ’1
v
)
(1 âˆ’pk)uâˆ’1âˆ’v(1 âˆ’pk)m
=
uâˆ’1
âˆ‘
v=0
(âˆ’1)v (u âˆ’1
v
)
ğœm+v.

Estimation of Diversity Indices
131
Linear diversity indices may also be derived from a general form of
ğœƒ=
âˆ‘
kâ‰¥1
pkh(pk)
(4.4)
where h(p) is an analytic function on an open interval containing (0, 1]. More
speciï¬cally, (4.4) may be re-expressed as, provided that 0 â‰¤ğœƒ< âˆ,
ğœƒ=
âˆ‘
kâ‰¥1
âˆ‘
vâ‰¥0
h(v)(1)
v!
pk(pk âˆ’1)v
=
âˆ‘
kâ‰¥1
âˆ‘
vâ‰¥0
(âˆ’1)vh(v)(1)
v!
pk(1 âˆ’pk)v
=
âˆ‘
vâ‰¥0
(âˆ’1)vh(v)(1)
v!
ğœv
(4.5)
where h(v)(1) is the vth derivative of h(p) evaluated at p = 1. The exchange of the
two summations in the above-mentioned expression is supported by Fubiniâ€™s
lemma, assuming ğœƒis ï¬nite. (4.5) conforms with (4.3).
4.2
Estimation of Linear Diversity Indices
In this section, two general approaches to nonparametric statistical estimation
of linear diversity indices are described. Let X1, X2,â€¦, Xn be independent and
identically distributed (iid) random observations from ğ’³according to p. Let
{Yk = âˆ‘n
i=1 1[Xi = ğ“k]} be the sequence of observed counts in an iid sample of
size n and let Ì‚p = {Ì‚pk = Ykâˆ•n} be the observed sample proportions. Suppose
it is of interest to estimate ğœƒof (4.3) where {wv; v â‰¥0} is pre-chosen but p =
{pk; k â‰¥1} is unknown.
Replacing each pk in (4.3) with Ì‚pk, the resulting estimator,
Ì‚ğœƒ=
âˆ‘
vâ‰¥0
wv
âˆ‘
kâ‰¥1
Ì‚pk(1 âˆ’Ì‚pk)v,
(4.6)
is often referred to as the plug-in estimator. The plug-in estimator is an intu-
itive estimator and has many desirable properties asymptotically. However, it is
known to have a slowly decaying bias, particularly when the sample size n is rel-
atively small and the alphabet ğ’³is relatively large. For this reason, the plug-in
estimator is not always the ï¬rst one chosen in practice. Nevertheless, since its
statistical properties are better known than those of other estimators, it is often
used as a reference estimator.
On the other hand, by Z1,v in (3.26) and the fact E(Z1,v) = ğœv for every v sat-
isfying 1 â‰¤v â‰¤n âˆ’1, the following estimator in Turingâ€™s perspective is readily
available,
Ì‚ğœƒâ™¯=
nâˆ’1
âˆ‘
v=0
wvZ1,v,
(4.7)
and is the main object of this chapter.

132
Statistical Implications of Turingâ€™s Formula
Several statistical properties of Ì‚ğœƒâ™¯may be described under the following con-
dition.
Condition 4.1
A probability distribution p = {pk; k â‰¥1} and an associated
linear diversity index in the form of (4.3) are such that
1) the eï¬€ective cardinality of ğ’³is ï¬nite, that is, K = âˆ‘
kâ‰¥11[pk > 0] < âˆ; and
2) the weights, wv, for all v â‰¥0, are bounded, that is, there exists an M > 0 such
that |wv| â‰¤M for all v â‰¥0.
Condition 4.1 guarantees that the summation in (4.3) always converges (see
Exercise 10). It can be veriï¬ed that the assumption that |wn| â‰¤M is satisï¬ed by
all of the linear diversity indices discussed in Examples 4.10 through 4.16 (see
Exercise 11). Condition 4.1 also guarantees that estimator Ì‚ğœƒâ™¯has a bias that
decays exponentially fast in sample size n.
Theorem 4.2
Under Condition 4.1,
|E( Ì‚ğœƒâ™¯) âˆ’ğœƒ| â‰¤MK(1 âˆ’pâˆ§)n
(4.8)
where pâˆ§= min{pk,â€¦, pK}.
Proof: The U-statistics construction of Z1,v in Chapter 3 establishes the fact
that E(Z1,v) = ğœv for v satisfying v = 1,â€¦, n âˆ’1. Therefore,
|E( Ì‚ğœƒâ™¯) âˆ’ğœƒ| =
||||||
nâˆ’1
âˆ‘
v=1
wvE(Z1,v) âˆ’
âˆ
âˆ‘
v=0
wvğœv
||||||
=
|||||
âˆ
âˆ‘
v=n
wvğœv
|||||
â‰¤|wv|
âˆ
âˆ‘
v=n
K
âˆ‘
k=1
pk(1 âˆ’pk)v = |wv|
K
âˆ‘
k=1
(1 âˆ’pk)n
â‰¤MK(1 âˆ’pâˆ§)n.
â—½
Remark 4.1
While decaying rapidly, the bias of Ì‚ğœƒâ™¯could still be sizable when
M, K are relatively large and n and pâˆ§are relatively small. Practitioners often
ï¬nd bias reduction desirable in many realistic situations. In the existing litera-
ture, there are many proposed ways to reduce bias and to enhance accuracy in
estimation of diversity indices. For a recent update of this research space, readers
may wish to refer to Chao and Jost (2012) and Chao and Jost (2015). Zhang and
Grabchak (2013) proposed a bias reduction methodology that is particularly
suitable for general estimators in the form of (4.7).
As in any sound statistical practice, reliability of estimated diversity indices
must be supported by distributional characteristics of the estimators. Toward
that end, the asymptotic normality of Ì‚ğœƒâ™¯is established in the following. The

Estimation of Diversity Indices
133
approach here consists of two parts. The ï¬rst part is to establish asymptotic
normality of the plug-in estimator Ì‚ğœƒin (4.6), and then the second part is to show
that Ì‚ğœƒâ™¯and Ì‚ğœƒare suï¬ƒciently close to warrant the same asymptotic distributional
behavior.
Let
v = (p1, p2,â€¦, pKâˆ’1)ğœ
and
Ì‚v = (Ì‚p1, Ì‚p2,â€¦, Ì‚pKâˆ’1)ğœ.
(4.9)
Lemma 4.1 is a well-known multivariate normal approximation to the multi-
nomial distribution (see Exercise 17).
Lemma 4.1
Suppose the probability distribution p = {pk; k = 1,â€¦, K} is
such that pk > 0 for each k:
âˆš
n(Ì‚v âˆ’v)
L
âˆ’âˆ’âˆ’â†’MVN(âˆ…, Î£(v)),
(4.10)
where âˆ…is a (K âˆ’1)-dimensional zero vector and Î£(v) is the (K âˆ’1) Ã— (K âˆ’1)
covariance matrix given by
Î£(v) =
â›
âœ
âœ
âœâ
p1(1 âˆ’p1)
âˆ’p1p2
Â· Â· Â·
âˆ’p1pKâˆ’1
âˆ’p2p1
p2(1 âˆ’p2) Â· Â· Â·
âˆ’p2pKâˆ’1
â‹®
â‹®
Â· Â· Â·
â‹®
âˆ’pKâˆ’1p1
âˆ’pKâˆ’1p2 Â· Â· Â· pKâˆ’1(1 âˆ’pKâˆ’1)
â
âŸ
âŸ
âŸâ 
.
(4.11)
Let h(p) = âˆ‘
vâ‰¥0wv(1 âˆ’p)v, write the index ğœƒof (4.3) as a function of
(p1,â€¦, pKâˆ’1), and denote the resulting function as
G(v) =
K
âˆ‘
k=1
pkh(pk) =
Kâˆ’1
âˆ‘
k=1
pkh(pk) +
(
1 âˆ’
Kâˆ’1
âˆ‘
k=1
pk
)
h
(
1 âˆ’
Kâˆ’1
âˆ‘
k=1
pk
)
.
Let the gradient of G(v) be denoted as
g(v) = âˆ‡G(v) =
(
âˆ‚
âˆ‚p1
G(v),â€¦,
âˆ‚
âˆ‚pKâˆ’1
G(v)
)ğœ
=âˆ¶(g1(v),â€¦, gKâˆ’1(v))ğœ.
(4.12)
For each j, j = 1,â€¦, K âˆ’1,
gj(v) = âˆ‚
âˆ‚pj
G(v) = h(pj) + pjhâ€²(pj) âˆ’h
(
1 âˆ’
Kâˆ’1
âˆ‘
k=1
pk
)
âˆ’
(
1 âˆ’
Kâˆ’1
âˆ‘
k=1
pk
)
hâ€²
(
1 âˆ’
Kâˆ’1
âˆ‘
k=1
pk
)
=
âˆ‘
vâ‰¥0
wv(1 âˆ’pj)v âˆ’pj
âˆ‘
vâ‰¥0
wvv(1 âˆ’pj)vâˆ’1
âˆ’
âˆ‘
vâ‰¥0
wv(1 âˆ’pK)v + pK
âˆ‘
vâ‰¥0
wvv(1 âˆ’pK)vâˆ’1

134
Statistical Implications of Turingâ€™s Formula
=
âˆ‘
vâ‰¥0
wv[(1 âˆ’pj)v âˆ’(1 âˆ’pK)v]
âˆ’
âˆ‘
vâ‰¥0
wvv[pj(1 âˆ’pj)vâˆ’1 âˆ’pK(1 âˆ’pK)vâˆ’1].
(4.13)
An application of the delta method gives the following result.
Theorem 4.3
Suppose Condition 4.1 holds and gğœ(v)Î£(v)g(v) > 0, then
âˆš
n( Ì‚ğœƒâˆ’ğœƒ)(gğœ(v)Î£(v)g(v))âˆ’1
2
L
âˆ’âˆ’âˆ’â†’N(0, 1).
(4.14)
Tanabe and Sagae (1992) showed that, provided that K â‰¥2 and that pk > 0 for
each and every k, k = 1,â€¦, K, the covariance matrix Î£(v) is a positive deï¬nite
matrix. Therefore, gğœ(v)Î£(v)g(v) > 0 if and only if g(v) â‰ 0. However, for some
combinations of h(p) and p, g(v) = 0 and the condition of Theorem 4.3 does
not hold.
Example 4.17
Shannonâ€™s entropy. Let ğœƒ= âˆ‘K
k=1 pk ln (1âˆ•pk). Then gğœ(v)Î£(v)
g(v) = 0 if and only if p = {pk; k = 1,â€¦, K} is uniform.
Proof: In this case, h(p) = âˆ’ln p. For each j, j = 1,â€¦, K âˆ’1,
gj(v) = âˆ‚
âˆ‚pj
ğœƒ(p) = âˆ‚
âˆ‚pj
K
âˆ‘
k=1
pkh(pk)
= âˆ’ln (pj) âˆ’1 + ln pK + 1 = ln (pKâˆ•pj).
If pj = 1âˆ•K for every j, then gj(v) = 0. On the other hand, if ln (pKâˆ•pj) = 0 for
each and every j, then pj = pK = 1âˆ•K.
â—½
Example 4.18
Emlenâ€™s index. Let ğœƒ= âˆ‘K
k=1 pkepk. Then gğœ(v)Î£(v)g(v) = 0 if
and only if p = {pk; k = 1,â€¦, K} is uniform.
Proof: In this case, h(p) = eâˆ’p. For each j, j = 1,â€¦, K âˆ’1,
gj(v) = âˆ‚
âˆ‚pj
ğœƒ(p) = eâˆ’pj(1 âˆ’pj) âˆ’eâˆ’pK(1 âˆ’pK).
If pj = 1âˆ•K for every j, then gj(v) = 0. Suppose for each and every j gj(v) = 0,
then
eâˆ’pj(1 âˆ’pj) = eâˆ’pK(1 âˆ’pK).
(4.15)
Noting f (p) = eâˆ’p(1 âˆ’p) is a strictly monotone function on interval (0, 1),
(4.15) implies pj = pK for every j (see Exercise 19).
â—½

Estimation of Diversity Indices
135
As evidenced by the last expression of (4.3), g(v) is a continuous function of
v for v âˆˆ(0, 1]Kâˆ’1. The said continuity and the fact Ì‚v
pâ†’v imply that g(Ì‚v)
pâ†’g(v)
by the continuous mapping theorem of Mann and Wald (1943). The following
corollary follows immediately from Slutskyâ€™s theorem.
Corollary 4.1
Under the condition of Theorem 4.3,
âˆš
n( Ì‚ğœƒâˆ’ğœƒ)(gğœ(Ì‚v)Î£(Ì‚v)g(Ì‚v))âˆ’1
2
L
âˆ’âˆ’âˆ’â†’N(0, 1).
Corollary 4.1 provides a means of statistical inference. It is to be noted
however that in practice it is often the case that Ì‚pk = 0 for some index values
of k, that is, the letters in the alphabet that are not covered by a sample of size
n. For simplicity, suppose Ì‚pK > 0 (if not, a switch of two indices will suï¬ƒce).
Two issues exist in such a situation. First, noting gj(v) for each j is only deï¬ned
for pj > 0, some components of g(Ì‚v) = (g1(Ì‚v),â€¦, gKâˆ’1(Ì‚v))ğœmay not be well
deï¬ned if some components of Ì‚v are zeros, say gj(Ì‚v). Estimating gj(v) by a
not well-deï¬ned statistic gj(Ì‚v) is problematic. This issue can be resolved by
setting gj(Ì‚v) = 0, or any ï¬xed value for that matter, since gj(Ì‚v) converges to the
true value of gj(v) and therefore is bounded away from the deï¬ned value in
probability. Second, if Ì‚pk = 0 for some index values of k, though pk > 0 and
therefore Î£(v) is positive deï¬nite, Î£(Ì‚v) is not. This raises the question on the
positivity of gğœ(Ì‚v)Î£(Ì‚v)g(Ì‚v). However, it may be shown that gğœ(Ì‚v)Î£(Ì‚v)g(Ì‚v), after
redeï¬ning gj(Ì‚v) for all Ì‚pj = 0, is positive (see Exercise 18).
Another interesting feature of Corollary 4.1 is that it does not distinguish a
species that does not exist in the population, that is, pk = 0, from a species that
does exist but is not observed in the sample, that is, pk > 0 but Ì‚pk = 0. This is
so because both Î£(Ì‚v) and the redeï¬ned g(Ì‚v) have zeros in locations that would
make the diï¬€erence inconsequential in computation. For this reason, Corollary
4.1 may be used regardless of whether K is known a priori.
The asymptotic normal laws established in Theorem 4.3 and Corollary 4.1
for the plug-in estimator may be extended to that for the estimator of Turingâ€™s
perspective, deï¬ned in (4.7).
Theorem 4.4
Let
Ì‚ğœƒâ™¯be as in (4.7). Suppose Condition 4.1 holds and
gğœ(v)Î£(v)g(v) > 0. Then
âˆš
n ( Ì‚ğœƒâ™¯âˆ’ğœƒ) (gğœ(v)Î£(v)g(v))âˆ’1
2
L
âˆ’âˆ’âˆ’â†’N(0, 1).
Corollary 4.2
Let Ì‚ğœƒâ™¯be as in (4.7). Suppose Condition 4.1 holds and
gğœ(v)Î£(v)g(v) > 0. Then
âˆš
n ( Ì‚ğœƒâ™¯âˆ’ğœƒ) (gğœ(Ì‚v)Î£(Ì‚v)g(Ì‚v))âˆ’1
2
L
âˆ’âˆ’âˆ’â†’N(0, 1).
(4.16)

136
Statistical Implications of Turingâ€™s Formula
To prove Theorem 4.4, two lemmas, Lemmas 4.2 and 4.3, are needed. Both
lemmas are stated in the following without proofs. A proof of Lemma 4.2 can be
found in Zhang and Grabchak (2016), and a proof of Lemma 4.3 can be found
in Hoeï¬€ding (1963).
For p âˆˆ[0, 1] and n âˆˆâ„•= {1, 2, â€¦ }, consider the following two functions:
fn(p) = p
âŒŠn(1âˆ’p)+1âŒ‹
âˆ‘
v=1
wv
v
âˆ
j=1
(
1 âˆ’np âˆ’1
n âˆ’j
)
and
f (p) = p
âˆ
âˆ‘
v=1
wv(1 âˆ’p)v.
Lemma 4.2
Suppose Condition 4.1 holds.
1) If 0 < c < d < 1, then
lim
nâ†’âˆsup
pâˆˆ[c,d]
âˆš
n|fn(p) âˆ’f (p)| = 0.
2) Let pn âˆˆ[0, 1] be such that npn âˆˆ{0, 1, 2,â€¦, n}, then
âˆš
n|fn(pn) âˆ’f (pn)| â‰¤
âˆš
n(n + 1) â‰¤2n3âˆ•2.
Lemma 4.3
(Hoeï¬€dingâ€™s inequality)
Let Y be a binomial random variable
with parameters n and p > 0 and Ì‚p = Yâˆ•n. For any Îµ > 0,
1) P(Ì‚p âˆ’p â‰¤âˆ’Îµ) â‰¤eâˆ’2Îµ2n and
2) P(Ì‚p âˆ’p â‰¥Îµ) â‰¤eâˆ’2Îµ2n.
Recall the result of Exercise 17 of Chapter 3 that Z1,v in (3.26) may be
re-expressed as
Z1,v =
âˆ‘
kâ‰¥1
[
Ì‚pk
v
âˆ
j=1
(
1 âˆ’Yk âˆ’1
n âˆ’j
)]
.
(4.17)
As a result of (4.17), Ì‚ğœƒâ™¯in (4.7) may be re-expressed as
Ì‚ğœƒâ™¯= w0 +
nâˆ’1
âˆ‘
v=1
wv
K
âˆ‘
k=1
Ì‚pk
v
âˆ
j=1
(
1 âˆ’Yk âˆ’1
n âˆ’j
)
(4.18)
= w0 +
K
âˆ‘
k=1
Ì‚pk
nâˆ’Yk
âˆ‘
v=1
wv
v
âˆ
j=1
(
1 âˆ’Yk âˆ’1
n âˆ’j
)
(4.19)
= w0 +
K
âˆ‘
k=1
Ì‚ğœƒâ™¯
k,
(4.20)

Estimation of Diversity Indices
137
where
Ì‚ğœƒâ™¯
k = Ì‚pk
nâˆ’Yk
âˆ‘
v=1
wv
v
âˆ
j=1
(
1 âˆ’Yk âˆ’1
n âˆ’j
)
= Ì‚pk
âˆ
âˆ‘
v=1
wv
v
âˆ
j=1
(
1 âˆ’Yk âˆ’1
n âˆ’j
)
. (4.21)
Also recall from (4.6)
Ì‚ğœƒ= w0 +
K
âˆ‘
k=1
Ì‚ğœƒk,
(4.22)
where Ì‚ğœƒk = Ì‚pk
âˆ‘âˆ
v=1 wv(1 âˆ’Ì‚pk)v.
Proof of Theorem 4.4: Since
âˆš
n( Ì‚ğœƒâ™¯âˆ’ğœƒ) =
âˆš
n( Ì‚ğœƒâ™¯âˆ’Ì‚ğœƒ) +
âˆš
n( Ì‚ğœƒâˆ’ğœƒ),
by Theorem 4.3 and Slutskyâ€™s theorem it suï¬ƒces to show that
âˆš
n( Ì‚ğœƒâ™¯âˆ’Ì‚ğœƒ)
pâ†’0.
However, from (4.20) and (4.22),
âˆš
n( Ì‚ğœƒâ™¯âˆ’Ì‚ğœƒ) =
K
âˆ‘
k=1
âˆš
n( Ì‚ğœƒâ™¯
k âˆ’Ì‚ğœƒk),
and therefore it suï¬ƒces to show that, for each k,
âˆš
n( Ì‚ğœƒâ™¯
k âˆ’Ì‚ğœƒk)
pâ†’0.
Toward that end,
âˆš
n( Ì‚ğœƒâ™¯
k âˆ’Ì‚ğœƒk) =
{âˆš
n
(
Ì‚ğœƒâ™¯
k âˆ’Ì‚ğœƒk
)
1[Ì‚pk â‰¤pkâˆ•2]
+
âˆš
n
(
Ì‚ğœƒâ™¯
k âˆ’Ì‚ğœƒk
)
1[Ì‚pk â‰¥(1 + pk)âˆ•2]
}
+
{âˆš
n
(
Ì‚ğœƒâ™¯
k âˆ’Ì‚ğœƒk
)
1[pkâˆ•2 < Ì‚pk < (1 + pk)âˆ•2]
}
= âˆ¶{îˆ­1} + {îˆ­2}.
By Part 2 of Lemmas 4.2 and 4.3,
E|îˆ­1| â‰¤2n3âˆ•2
(
P
(
Ì‚pk > 1 + pk
2
)
+ P
(
Ì‚pk â‰¤pk
2
))
= 2n3âˆ•2
(
P
(
Ì‚pk âˆ’pk
1 âˆ’pk
2
)
+ P
(
Ì‚pk âˆ’pk â‰¤âˆ’pk
2
))
â‰¤2n3âˆ•2[eâˆ’n(1âˆ’pk)2âˆ•2 + eâˆ’np2
kâˆ•2] â†’0.
Thus, it follows that îˆ­1
pâ†’0. By Part 1 of Lemma 4.2, it follows that
îˆ­2
pâ†’0.
â—½
Finally, the same justiï¬cation for Corollary 4.1 also applies to Corollary 4.2.

138
Statistical Implications of Turingâ€™s Formula
4.3
Estimation of RÃ©nyiâ€™s Entropy
In the last section, a general nonparametric estimation approach to linear
diversity indices is introduced. To extend the derived results further to
diversity indices that are not linear but only equivalent to a linear kernel,
ğœƒLK = âˆ‘âˆ
v=0 wv ğœv, consider ğœƒ= ğœ“(ğœƒLK) where ğœ“(â‹…) is a function diï¬€erentiable
at ğœƒLK. Let ğœ“â€²(â‹…) be the ï¬rst derivative of ğœ“(â‹…) and
Ìƒğœƒâ™¯= ğœ“( Ì‚ğœƒâ™¯)
(4.23)
where Ì‚ğœƒâ™¯, as in (4.7), is an estimator of ğœƒ= ğœ“(ğœƒLK). An application of the delta
method immediately gives the following theorem and corollary.
Theorem 4.5
Suppose Condition 4.1 holds and gğœ(v)Î£(v)g(v) > 0. Then
âˆš
n (ğœ“( Ì‚ğœƒâ™¯) âˆ’ğœƒ) (gğœ(v)Î£(v)g(v)ğœ“â€²(ğœƒLK))âˆ’1
2
L
âˆ’âˆ’âˆ’â†’N(0, 1).
Corollary 4.3
Suppose Condition 4.1 holds and gğœ(v)Î£(v)g(v) > 0. Then
âˆš
n (ğœ“( Ì‚ğœƒâ™¯) âˆ’ğœƒ) (gğœ(Ì‚v)Î£(Ì‚v)g(Ì‚v)ğœ“â€²( Ì‚ğœƒâ™¯))âˆ’1
2
L
âˆ’âˆ’âˆ’â†’N(0, 1).
Furthermore, if |ğœ“â€²(t)| < D for some D > 0 on the interval of t between ğœƒLK
and infâˆ—{ Ì‚ğœƒâ™¯} and the interval between ğœƒLK and supâˆ—{ Ì‚ğœƒâ™¯}, where both infâˆ—and
supâˆ—are taken over all possible samples of all sizes, then the bias of ğœ“( Ì‚ğœƒâ™¯) decays
at least as fast as the bias of Ì‚ğœƒâ™¯, which is exponential in n under Condition 4.1.
To see this, it suï¬ƒces to consider the ï¬rst-order Taylor expansion for ğœ“( Ì‚ğœƒâ™¯):
ğœ“( Ì‚ğœƒâ™¯) âˆ’ğœ“(ğœƒLK) = ğœ“â€²(ğœ‰)( Ì‚ğœƒâ™¯âˆ’ğœƒLK)
(4.24)
where the random variable ğœ‰is between ğœƒLK and Ì‚ğœƒâ™¯âˆˆ(infâˆ—{ Ì‚ğœƒâ™¯}, supâˆ—{ Ì‚ğœƒâ™¯}).
Taking expectation on both sides of (4.24) gives
|||E(ğœ“( Ì‚ğœƒâ™¯) âˆ’ğœ“(ğœƒLK))||| â‰¤D |||E( Ì‚ğœƒâ™¯âˆ’ğœƒLK)||| â‰¤DMK(1 âˆ’pâˆ§)n
(4.25)
where the last inequality is due to (4.8).
RÃ©nyiâ€™s entropy is one of the most frequently used diversity indices by ecolo-
gists, and it is one of the two diversity indices among the many discussed in this
chapter that are not linear in entropic basis, the other one being Hillâ€™s diversity
number that is a logarithmic transformation of RÃ©nyiâ€™s entropy. Theorem 4.5
and Corollary 4.3 may be applied to RÃ©nyiâ€™s entropy.
For a ï¬xed ğ›¼> 0 such that ğ›¼â‰ 1, let ğœ“(t) = (1 âˆ’ğ›¼)âˆ’1 ln t. Note that
Hğ›¼= ğœ“(hğ›¼) = ln hğ›¼
1 âˆ’ğ›¼,
where Hğ›¼is RÃ©nyiâ€™s entropy and hğ›¼= âˆ‘K
k=1 pğ›¼
k is RÃ©nyiâ€™s equivalent entropy, as
in Example 4.13. Let

Estimation of Diversity Indices
139
Ì‚hâ™¯
ğ›¼= 1 +
K
âˆ‘
k=1
Ì‚pk
nâˆ’Yk
âˆ‘
v=1
wv
v
âˆ
j=1
(
1 âˆ’Yk âˆ’1
n âˆ’j
)
,
(4.26)
where for v â‰¥1
wv =
v
âˆ
i=1
(i âˆ’ğ›¼
i
)
.
Ì‚hâ™¯
ğ›¼is the estimator of hğ›¼in the form of (4.7). Let
Ì‚Hâ™¯
ğ›¼= ln Ì‚hâ™¯
ğ›¼
1 âˆ’ğ›¼= ğœ“(Ì‚hâ™¯
ğ›¼).
Since
ğœ“â€²(t) =
1
(1 âˆ’ğ›¼)t ,
(4.27)
Theorem 4.5 and the fact that hğ›¼> 0 imply the following.
Theorem 4.6
Provided that gğœ(v)Î£(v)g(v) > 0,
âˆš
n
(
Ì‚Hâ™¯
ğ›¼âˆ’Hğ›¼
)
|1 âˆ’ğ›¼|hğ›¼[gğœ(v)Î£(v)g(v)]âˆ’1
2
L
âˆ’âˆ’âˆ’â†’N(0, 1).
Corollary 4.4
Provided that gğœ(v)Î£(v)g(v) > 0,
âˆš
n( Ì‚Hâ™¯
ğ›¼âˆ’Hğ›¼)|1 âˆ’ğ›¼|Ì‚hâ™¯
ğ›¼[gğœ(Ì‚v)Î£(Ì‚v)g(Ì‚v)]âˆ’1
2
L
âˆ’âˆ’âˆ’â†’N(0, 1).
Next it is to establish that when ğ›¼âˆˆ(0, 1), the bias of Ì‚Hâ™¯
ğ›¼decays exponentially
in the sample size n. This fact follows the following three observations.
1) First, observe that the linear kernel of RÃ©nyiâ€™s entropy, or RÃ©nyiâ€™s equivalent
index, as in Example 4.13, satisï¬es
ğœƒLK = hğ›¼=
âˆ‘
kâ‰¥1
pğ›¼
k > 0
for any ğ›¼> 0. In particular, for ğ›¼âˆˆ(0, 1),
ğœƒLK = hğ›¼=
âˆ‘
kâ‰¥1
pğ›¼
k â‰¥1.
2) Second, consider Ì‚hâ™¯
ğ›¼of (4.26). Since ğ›¼âˆˆ(0, 1), (i âˆ’ğ›¼) > 0 for all i = 1, â€¦ ,
wv > 0, and hence Ì‚ğœƒâ™¯= Ì‚hâ™¯
ğ›¼â‰¥1, one has
m = min
{
hğ›¼, inf
âˆ—
{
Ì‚hâ™¯
ğ›¼
}}
â‰¥1.
3) Third, since ğœ“â€²(t) of (4.27) satisï¬es
ğœ“â€²(t) =
1
(1 âˆ’ğ›¼)t â‰¤
1
(1 âˆ’ğ›¼)m =âˆ¶D
for t âˆˆ[m, +âˆ), Equation (4.25) holds. Therefore, the bias of Ì‚Hâ™¯
ğ›¼decays
exponentially in n.

140
Statistical Implications of Turingâ€™s Formula
Finally, a large-sample statistical testing procedure for a diï¬€erence in the
diversity measured by RÃ©nyiâ€™s entropy between two populations (possibly
deï¬ned by two diï¬€erent locations or two diï¬€erent time points) is described in
the following.
Suppose two independent iid samples of sizes n1 and n2 are to be taken from a
same alphabet ğ’³under two diï¬€erent distributions: {p1,k; k â‰¥1} and {p2,k; k â‰¥
1}. The samples are summarized, respectively, by two independent sets of the
observed frequencies
Y1 = {Y1,1, Y1,2,â€¦, Y1,k, â€¦ }
Y2 = {Y2,1, Y2,2,â€¦, Y2,k, â€¦ }
and their corresponding observed relative frequencies
Ì‚p1 = {Ì‚p1,1, Ì‚p1,2,â€¦, Ì‚p1,k, â€¦ } = {Y1,1âˆ•n1, Y1,2âˆ•n1,â€¦, Y1,kâˆ•n1, â€¦ }
Ì‚p2 = {Ì‚p2,1, Ì‚p2,2,â€¦, Ì‚p2,k, â€¦ } = {Y2,1âˆ•n2, Y2,2âˆ•n2,â€¦, Y2,kâˆ•n2, â€¦ }.
For a ï¬xed ğ›¼> 0 and ğ›¼â‰ 1, denote RÃ©nyiâ€™s entropies for the two populations
by Hğ›¼(1) and Hğ›¼(2). Suppose it is of interest to test the hypothesis
H0 âˆ¶Hğ›¼(1) = Hğ›¼(2)
versus
Ha âˆ¶Hğ›¼(1) â‰ Hğ›¼(2)
at a level of signiï¬cance ğ›½âˆˆ(0, 1).
The testing procedure may be carried out in the following 10 steps.
1) To calculate, using (4.26),
Ì‚hâ™¯
ğ›¼(1) = 1 +
K
âˆ‘
k=1
Ì‚p1,k
n1âˆ’Y1,k
âˆ‘
v=1
wv
v
âˆ
j=1
(
1 âˆ’
Y1,k âˆ’1
n1 âˆ’j
)
and
Ì‚hâ™¯
ğ›¼(2) = 1 +
K
âˆ‘
k=1
Ì‚p2,k
n2âˆ’Y2,k
âˆ‘
v=1
wv
v
âˆ
j=1
(
1 âˆ’
Y2,k âˆ’1
n2 âˆ’j
)
,
where
wv =
v
âˆ
i=1
(i âˆ’ğ›¼
i
)
.
2) To calculate
Ì‚Hâ™¯
ğ›¼(1) = ln Ì‚hâ™¯
ğ›¼(1)
1 âˆ’ğ›¼
and
Ì‚Hâ™¯
ğ›¼(2) = ln Ì‚hâ™¯
ğ›¼(2)
1 âˆ’ğ›¼.
3) To calculate vectors
g(Ì‚v1) = (ğ›¼(Ì‚pğ›¼âˆ’1
1,1 âˆ’Ì‚pğ›¼âˆ’1
1,K
) ,â€¦, ğ›¼(Ì‚pğ›¼âˆ’1
1,Kâˆ’1 âˆ’Ì‚pğ›¼âˆ’1
1,K
))ğœand
g(Ì‚v2) = (ğ›¼(Ì‚pğ›¼âˆ’1
2,1 âˆ’Ì‚pğ›¼âˆ’1
2,K
) ,â€¦, ğ›¼(Ì‚pğ›¼âˆ’1
2,Kâˆ’1 âˆ’Ì‚pğ›¼âˆ’1
2,K
))ğœ.

Estimation of Diversity Indices
141
4) To make adjustment to g(Ì‚v1) and g(Ì‚v2) by setting the entries, when Ì‚pi,j = 0,
i = 1, 2, and j = 1, 2,â€¦, K âˆ’1, to zero, that is,
g(Ì‚v1) = (ğ›¼(Ì‚pğ›¼âˆ’1
1,1 âˆ’Ì‚pğ›¼âˆ’1
1,K )1[Ì‚p1,1 > 0] ,
â€¦ , ğ›¼(Ì‚pğ›¼âˆ’1
1,Kâˆ’1 âˆ’Ì‚pğ›¼âˆ’1
1,K )1[Ì‚p1,Kâˆ’1 > 0])ğœ, and
g(Ì‚v2) = (ğ›¼(Ì‚pğ›¼âˆ’1
2,1 âˆ’Ì‚pğ›¼âˆ’1
2,K )1[Ì‚p2,1 > 0] ,
â€¦ , ğ›¼(Ì‚pğ›¼âˆ’1
2,Kâˆ’1 âˆ’Ì‚pğ›¼âˆ’1
2,K )1[Ì‚p2,Kâˆ’1 > 0])ğœ.
5) To calculate the two (K âˆ’1) Ã— (K âˆ’1) covariance matrices
Î£(Ì‚v1) =
â›
âœ
âœ
âœâ
Ì‚p1,1(1 âˆ’Ì‚p1,1)
âˆ’Ì‚p1,1 Ì‚p1,2
Â· Â· Â·
âˆ’Ì‚p1,1 Ì‚p1,Kâˆ’1
âˆ’Ì‚p1,2 Ì‚p1,1
Ì‚p1,2(1 âˆ’Ì‚p1,2) Â· Â· Â·
âˆ’Ì‚p1,2 Ì‚p1,Kâˆ’1
â‹®
â‹®
Â· Â· Â·
â‹®
âˆ’Ì‚p1,Kâˆ’1 Ì‚p1,1
âˆ’Ì‚p1,Kâˆ’1 Ì‚p1,2 Â· Â· Â·
Ì‚p1,Kâˆ’1(1 âˆ’Ì‚p1,Kâˆ’1)
â
âŸ
âŸ
âŸâ 
and
Î£(Ì‚v2) =
â›
âœ
âœ
âœâ
Ì‚p2,1(1 âˆ’Ì‚p2,1)
âˆ’Ì‚p2,1 Ì‚p2,2
Â· Â· Â·
âˆ’Ì‚p2,1 Ì‚p2,Kâˆ’1
âˆ’Ì‚p2,2 Ì‚p2,1
Ì‚p2,2(1 âˆ’Ì‚p2,2) Â· Â· Â·
âˆ’Ì‚p2,2 Ì‚p2,Kâˆ’1
â‹®
â‹®
Â· Â· Â·
â‹®
âˆ’Ì‚p2,Kâˆ’1 Ì‚p2,1
âˆ’Ì‚p2,Kâˆ’1 Ì‚p2,2 Â· Â· Â·
Ì‚p2,Kâˆ’1(1 âˆ’Ì‚p2,Kâˆ’1)
â
âŸ
âŸ
âŸâ 
.
6) To calculate the two estimated variances
Ì‚ğœ2
1 = (1 âˆ’ğ›¼)2(
Ì‚hâ™¯
ğ›¼(1)
)2
(g
ğœ(Ì‚v1)Î£(Ì‚v1)g(Ì‚v1))
and
Ì‚ğœ2
2 = (1 âˆ’ğ›¼)2(
Ì‚hâ™¯
ğ›¼(2)
)2
(g
ğœ(Ì‚v2)Î£(Ì‚v2)g(Ì‚v2)).
7) To ï¬nd the estimated combined variance
Ì‚ğœ2 = Ì‚ğœ2
1 + n1
n2
Ì‚ğœ2
2.
8) To calculate the test statistic
Zğ›¼=
âˆšn1( Ì‚Hâ™¯
ğ›¼(1) âˆ’Ì‚Hâ™¯
ğ›¼(2))
Ì‚ğœ
.
(4.28)
9) To make a decision to
a) reject H0 if |Zğ›¼| â‰¥zğ›½âˆ•2, or
b) not to reject H0 if |Zğ›¼| < zğ›½âˆ•2
where zğ›½âˆ•2 is the [100 Ã— (1 âˆ’ğ›½âˆ•2)]th percentile of the standard normal dis-
tribution.
10) To calculate a [100 Ã— (1 âˆ’ğ›½)]% conï¬dence interval for the diï¬€erence in
RÃ©nyiâ€™s entropy,
(
Ì‚Hâ™¯
ğ›¼(1) âˆ’Ì‚Hâ™¯
ğ›¼(2)
)
Â± zğ›½âˆ•2
Ì‚ğœ
âˆšn1
.
(4.29)

142
Statistical Implications of Turingâ€™s Formula
Many ecologists like to compare RÃ©nyiâ€™s diversity proï¬les of two populations
based on two samples. The estimated single-sample proï¬les are obtained by
Ì‚Hâ™¯
ğ›¼(1) and Ì‚Hâ™¯
ğ›¼(2) as functions of ğ›¼varying from a to b, satisfying 0 < a < b.
The estimated diï¬€erence of the two proï¬les is obtained by Ì‚Hâ™¯
ğ›¼(1) âˆ’Ì‚Hâ™¯
ğ›¼(2) as
a function ğ›¼. Letting ğ›¼vary over (a, b) satisfying 0 < a < b, (4.29) provides a
[100 Ã— (1 âˆ’ğ›½)]% point-wise conï¬dence band for the proï¬le diï¬€erence in RÃ©nyiâ€™s
diversity entropy.
4.4
Remarks
The ï¬rst two fundamental questions in biodiversity studies are as follows:
1) What is diversity?
2) How should it be mathematically measured?
Much discussion of diversity has ensued since (Fisher, Corbet, and Williams
(1943), MacArthur (1955), Margalef (1958)), and it is still ongoing. One of the
main objectives of the discussion has been to pin down one mathematical diver-
sity index that would be universally accepted. However, after more than half
of a century of intense discussion, it should be abundantly clear that any one
diversity index would, at its best, be a summary parameter of the underlying
distribution {pk; k â‰¥1} to reï¬‚ect a subjectively perceived notion of â€œdiversity.â€
A conceptual parallel of diversity may be found in Statistics, where the disper-
sion of a random variable is of interest. For example, the variance of a random
variable is commonly used as a measure of dispersion associated with an under-
lying distribution. However, the popularity of variance has more to do with the
mathematical convenience it entails than its appropriateness in measuring dis-
persion. In fact, the variance does not have any more or less intrinsic merit
in measuring dispersion per se than, say, the expectation of absolute deviation
from the mean or any even central moments of the random variable, when they
exist. One should never lose sight of the ultimate end objective in Statistics: the
knowledge of the underlying distribution. Moments are merely the means and
not the end. Similarly, in diversity studies of ecological populations, or more
generally ğ’³and its associated {pk}, any diversity indices are merely the means
but the distribution of all species is the end.
Much has been published in diversity literature, but rarely can a simpler and
wiser statement be found than the opening paragraph of Purvis and Hector
(2000):
To proceed very far with the study of biodiversity, we need to pin the
concept down. â€¦ However, any attempt to measure biodiversity quickly
runs into the problem that it is a fundamentally multidimensional con-
cept: it cannot be reduced sensibly to a single number.

Estimation of Diversity Indices
143
That said however, it must be acknowledged that any eï¬€orts toward a sys-
tematic understanding of what axiomatic conditions would constitute diversity
indices should very much be of value. Earlier in this chapter, it is established that
under the two very basic axioms, îˆ­01 and îˆ­02, any diversity index is a function
of {ğœv; v â‰¥1}, which already oï¬€ers a powerful unifying perspective. There are
many possible additional axioms that could be contemplated. For example, the
following maximization axiom, call it îˆ­M, is often mentioned in the literature.
Axiom 4.2
Given an alphabet ğ’³with eï¬€ective cardinality K and its associ-
ated probability distribution p, a diversity index ğœƒ= ğœƒ(p) must satisfy
îˆ­Mâˆ¶max{ğœƒ(p); all p on ğ’³} = ğœƒ(pu)
where pu is the uniform distribution, that is, pk = 1âˆ•K for every k = 1,â€¦, K.
A stronger version of Axiom îˆ­M is given in the following.
Axiom 4.3
Given an alphabet ğ’³with eï¬€ective cardinality K and its associ-
ated probability distribution p, a diversity index ğœƒ= ğœƒ(p) must satisfy
îˆ­âˆ—
Mâˆ¶ğœƒ(p) attains its maximum at, and only at, p = pu
where pu is the uniform distribution, that is, pk = 1âˆ•K for every k = 1,â€¦, K, p.
Axioms îˆ­M and îˆ­âˆ—
Mi are heuristically reasonable if the ecological interest of
all diï¬€erent species is equal, which often is not the case in practice. The question
then becomes whether such an additional constraint should be included in the
general axiomatic system for diversity indices.
The following is another axiom, also known as the minimization axiom. The
minimization axiom is thought by many ecologists to be more fundamental
than any other requirements on reasonable diversity indices.
Axiom 4.4
A diversity index ğœƒ= ğœƒ(p) must satisfy
îˆ­mâˆ¶min{ğœƒ(p); all p on ğ’³} = ğœƒ(po)
where po is a single-point-mass distribution, that is, p1 = 1 and K = 1.
A stronger version of Axiom îˆ­M is given in the following.
Axiom 4.5
A diversity index ğœƒ= ğœƒ(p) must satisfy
îˆ­âˆ—
mâˆ¶ğœƒ(p) attains its minimum at, and only at, p = po
where po is a single-point-mass distribution, that is, p1 = 1 and K = 1.
Example 4.19
The Giniâ€“Simpson diversity index, ğœƒ= âˆ‘
kâ‰¥1pk(1 âˆ’pk), satis-
ï¬es both îˆ­M and îˆ­m (see Exercise 12).

144
Statistical Implications of Turingâ€™s Formula
Grabchak, Marcon, Lang and Zhang (2016) showed that the generalized
Simpsonâ€™s indices also satisfy îˆ­M and îˆ­m under mild restrictions.
There is yet another interesting axiom, known as the replication principle.
The replication principle requires that â€œif there are N equally large, equally
diverse groups with no species in common, the diversity of the pooled groups
must be N times the diversity of a single group.â€ For a detailed discussion on the
replication principle, readers may wish to refer to Jost (2006) and Chao, Chiu,
and Jost (2010).
Remark 4.2
The phrases â€œequally largeâ€ and â€œequally diverseâ€ are subject to
interpretation. However, they are interpreted here within as â€œequal in number of
existing speciesâ€ and â€œequal in diversity as measured by the underlying diversity
index,â€ respectively.
The replication principle is a strong axiom, so strong it pins down essentially
on one index under mild conditions.
Let ğœƒ(p) be a diversity index satisfying the replication principle. Suppose
ğœƒ(p) also satisï¬es îˆ­01 and îˆ­02. Then ğœƒ0 = ğœƒ(po) is uniquely deï¬ned, that is, the
index assumes a same value, namely ğœƒ0 = ğœƒ(po), for every group containing
one species. This implies that any two singleton alphabets, {ğ“1} and {ğ“2}, are
equally diverse by the underlying index ğœƒ(p). Since any alphabet with ï¬nite
eï¬€ective cardinality K = âˆ‘
kâ‰¥11[pk > 0] â‰¥2 may be viewed as a combination
of K singleton alphabets, it follows that
ğœƒ(p) = ğœƒ0K
(4.30)
for any distribution p = {p1,â€¦, pK} such that pk > 0 for each k.
Suppose ğœƒ(p) also satisï¬es îˆ­âˆ—
m. Then it follows that ğœƒ0 > 0. Or else, if ğœƒ0 â‰¤0,
then ğœƒ0K â‰¤ğœƒ0 â‰¤0 would violate îˆ­âˆ—
m. This argument establishes that: if a diver-
sity index, ğœƒ(p), satisï¬es îˆ­01, îˆ­02, îˆ­âˆ—
m, and the replication principle, then it
must be of the form in (4.30) with ğœƒ0 being a positive constant. Up to a positive
multiplicative factor, ğœƒ0, there is essentially only one index in the form of (4.30),
the richness index K given in Example 4.15. Pinning down on a unique index is
a major advantage of the replication principle. However, as in every situation of
viewing a high-dimensional object in a reduced dimensional space, some ï¬ner
aspects of the object often are lost. For example, a common complaint about
the richness index, K, is that it does not distinguish between {p1 = 0.5000, p2 =
0.5000} and {p1 = 0.9999, p2 = 0.0001} in diversity.
Of course even if one has pinned down a good diversity index, it is often
not observable in practice, and therefore the issues of sample-based estimation
come to play. For all practical purposes, a third fundamental question should be
added to the ï¬rst two given above in the beginning of this section. The question
is: how to estimate a diversity index. The answer to this question is unfortu-
nately not trivial at all. A glance at the popular diversity indices in the literature

Estimation of Diversity Indices
145
reveals that, as argued earlier in this chapter, most of the indices are of the form,
or equivalent to,
ğœƒ=
âˆ‘
kâ‰¥1
pkh(pk)
where the weight h(p) loads heavily on small values of p. Such is the nature of
many diversity indices. However, estimation of such indices runs into the same
problems as those of estimation of Shannonâ€™s entropy, that is, it is diï¬ƒcult to
recover the values of pkh(pk) for the letters, ğ“kâ€™s, that are not observed in a
sample. The plug-in estimator tends to have large biases for the same reason
the plug-in estimator of Shannonâ€™s entropy does.
Similar to the literature of Shannonâ€™s entropy estimation, there is no lack of
proposed estimators of diversity indices that have reduced biases. However, not
enough attention is paid to the distributional characteristics of these estima-
tors. While it is perhaps diï¬ƒcult to develop asymptotic distributions for many
of the proposed estimators, they nevertheless are necessary knowledge in prac-
tice. Only when suï¬ƒcient distributional characteristics are known about an
estimator, can one make scientiï¬c inferences based on it about the underlying
population. This chapter summarizes a few distributional results for estimators
of various diversity indices, but much more research is needed in this direction.
4.5
Exercises
1
Suppose X1, X2,â€¦, Xv+1 are v + 1 iid random elements drawn from ğ’³=
{ğ“k; k â‰¥1} with probability distribution {pk; k â‰¥1}. Find the probability
that not all v + 1 random elements take a same letter. Discuss the rele-
vance of the above probabilities to diversity.
2
Suppose X1, X2, X3 are iid random elements drawn from ğ’³= {ğ“k; k â‰¥1}
with probability distribution {pk; k â‰¥1}.
a) Find the probability that the random elements take three diï¬€erent let-
ters.
b) Find the probability that the three random elements take exactly two
diï¬€erent letters.
c) Find the probability that the three random elements take at least two
diï¬€erent letters and discuss the relevance of the probability to diversity.
3
Let g(x) be a diï¬€erentiable function with domain (âˆ’âˆ, +âˆ). Suppose g(x)
is strictly increasing.
a) Show that the inverse function of g(x), gâˆ’1(t), exists on some interval
(a, b).
b) Show that gâˆ’1(t) is strictly increasing on (a, b).

146
Statistical Implications of Turingâ€™s Formula
4
Suppose f (s) and g(t) are both continuous and strictly increasing func-
tions with domain (âˆ’âˆ, +âˆ). Let h(t) = f (g(t)).
a) Show that h(t) is continuous on (âˆ’âˆ, +âˆ).
b) Show that h(t) is strictly increasing on (âˆ’âˆ, +âˆ).
5
Let ğœƒ, ğœƒ1, and ğœƒ2 be three diversity indices. Use Deï¬nition 4.1 to show that
if ğœƒ1 and ğœƒ2 are both equivalent to ğœƒ, then ğœƒ1 is equivalent to ğœƒ2.
6
Show that, for Simpsonâ€™s index, ğœ†= âˆ‘
kâ‰¥1p2
k = ğœ0 âˆ’ğœ1.
7
Show that, for RÃ©nyiâ€™s equivalent entropy in (4.1),
hğ›¼=
âˆ‘
kâ‰¥1
pr
k = ğœ0 +
âˆ
âˆ‘
v=1
[ v
âˆ
i=1
(i âˆ’ğ›¼
i
)
ğœv
]
,
where ğ›¼> 0 and ğ›¼â‰ 1.
8
Show that, for Emlenâ€™s index,
D =
âˆ‘
kâ‰¥1
pkeâˆ’pk =
âˆ
âˆ‘
v=0
eâˆ’1
v!
ğœv.
9
Show that the last equation of Example 4.16 holds, that is,
âˆ‘
kâ‰¥1
pk
uâˆ’1
âˆ‘
v=0
(âˆ’1)uâˆ’1âˆ’v (u âˆ’1
v
)
(1 âˆ’pk)uâˆ’1âˆ’v(1 âˆ’pk)m
=
uâˆ’1
âˆ‘
v=0
(âˆ’1)v (u âˆ’1
v
)
ğœm+v.
10
Show that, under Condition 4.1, ğœƒin (4.3) is well deï¬ned, that is,
lim
mâ†’âˆ
m
âˆ‘
v=0
wv
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)v
exists.
11
Verify that, in each of Examples 4.10 through 4.16, the linear coeï¬ƒcients
wv satisï¬es |wv| â‰¤M for some M > 0.
12
Show that the Giniâ€“Simpson diversity index, ğœƒ= âˆ‘K
k=1 pk(1 âˆ’pk), satis-
ï¬es both îˆ­M and îˆ­m.

Estimation of Diversity Indices
147
13
Show that Shannonâ€™s entropy
H = âˆ’
K
âˆ‘
k=1
pk ln pk
satisï¬es both îˆ­M and îˆ­m.
14
Show that RÃ©nyiâ€™s entropy
Hğ›¼=
1
1 âˆ’ğ›¼ln
( K
âˆ‘
k=1
pğ›¼
k
)
,
where ğ›¼> 0 and ğ›¼â‰ 1, satisï¬es both îˆ­M and îˆ­m.
15
For the generalized Simpsonâ€™s indices,
ğœu,v =
K
âˆ‘
k=1
pu
k(1 âˆ’pk)v,
a) show that îˆ­m is satisï¬ed for any pair u â‰¥1 and v â‰¥1;
b) show that îˆ­M is satisï¬ed for any pair u = v â‰¥1; and
c) give an example of {pk}, u and v, such that îˆ­M is not satisï¬ed.
16
For a distribution {pk; k = 1,â€¦, K} on a ï¬nite alphabet ğ’³, let
ğœƒ=
K
âˆ‘
k=1
pkln 2pk.
a) Show that îˆ­m is satisï¬ed.
b) Give an example of {pk} such that îˆ­M is not satisï¬ed.
c) Discuss suï¬ƒcient conditions on {pk} such that îˆ­M is satisï¬ed.
17
Consider a multinomial distribution with K = 3 categories with proba-
bilities, p1, p2, and p3 = 1 âˆ’p1 âˆ’p2, respectively. Let Y1, Y2, and Y3 be the
observed frequencies of the three diï¬€erent categories in an iid sample of
size n. Show that
âˆš
n
((
Y1âˆ•n
Y2âˆ•n
)
âˆ’
(
p1
p2
))
L
âˆ’âˆ’âˆ’â†’MVN(âˆ…, Î£)
where Î£ =
(
p1(1 âˆ’p1)
âˆ’p1p2
âˆ’p1p2
p2(1 âˆ’p2)
)
.
18
Let v and Ì‚v be deï¬ned as in (4.9). Let Î£(v) be deï¬ned as in (4.11). For each
j, j = 1,â€¦, K âˆ’1, let gj(Ì‚v) be deï¬ned as in (4.12) if Ì‚pj > 0, but let gj(Ì‚v) = 0
if Ì‚pj = 0. Suppose Ì‚pK > 0. Show gğœ(Ì‚v)Î£(Ì‚v)g(Ì‚v) > 0. (Hint: Consider

148
Statistical Implications of Turingâ€™s Formula
Ì‚p = {Ì‚p1,â€¦, Ì‚pK}. Deleting all the zeros in Ì‚p, reenumerating and renam-
ing the remaining nonzero components give q = {q1,â€¦, qKâˆ—} where
Kâˆ—= âˆ‘
k=11[Ì‚pk â‰ 0], which is a probability distribution.)
19
Suppose f (x) is a strictly monotone function on interval [a, b]. Then for
any x1 âˆˆ[a, b] and x2 âˆˆ[a, b], f (x1) = f (x2) implies x1 = x2.
20
Let
p1 = {0.5, 0.5} and p2 = {0.9, 0.1}.
(4.31)
For each of the two probability distributions, calculate the diversity
indices of the following.
a) The Giniâ€“Simpson index.
b) Shannonâ€™s entropy.
c) RÃ©nyiâ€™s entropy.
d) Tsallisâ€™ entropy.
e) Hillâ€™s diversity number.
f) Emlenâ€™s index.
g) The richness index.
21
Show that RÃ©nyiâ€™s entropy, Hğ›¼, approaches Shannonâ€™s entropy, H, as ğ›¼
approaches 1, that is,
lim
ğ›¼â†’1
ln (âˆ‘
kâ‰¥1pğ›¼
k)
1 âˆ’ğ›¼
= âˆ’
âˆ‘
kâ‰¥1
pk ln pk.
22
Sketch and compare ğœƒp = âˆ‘
kâ‰¥1pk(1 âˆ’pk)v over the interval from v = 1 to
v = 50 for both probability distributions in (4.31) of Exercise 20.
23
Sketch and compare ğœƒp = ln (âˆ‘
kâ‰¥1pğ›¼
k)âˆ•(1 âˆ’ğ›¼) over the interval from ğ›¼=
0 to ğ›¼= 5 for both probability distributions in (4.31) of Exercise 20.

149
5
Estimation of Information
5.1
Introduction
Consider the following three countable alphabets
ğ’³= {xi; i â‰¥1},
ğ’´= {yj; j â‰¥1},
ğ’³Ã— ğ’´= {(xi, yj); i â‰¥1, j â‰¥1}.
Given a joint probability distribution
pX,Y = {pi,j; i â‰¥1, j â‰¥1}
on ğ’³Ã— ğ’´, consider the two marginal probability distributions on ğ’³and ğ’´,
respectively,
pX =
{
pi,â‹…=
âˆ‘
jâ‰¥1
pi,j; i â‰¥1
}
pY =
{
pâ‹…,j =
âˆ‘
iâ‰¥1
pi,j; i â‰¥1
}
,
and denote the two underlying random elements as X and Y. Furthermore, let
pXY = {pi,â‹…pâ‹…,j; i â‰¥1, j â‰¥1}
(to be distinguished from pX,Y = {pi,j; i â‰¥1, j â‰¥1}).
Consider also the conditional probability distributions, respectively, of X on
ğ’³given Y = yj where j â‰¥1 is a speciï¬c index value and of Y on ğ’´given X = xi
where i â‰¥1 is a speciï¬c index value, that is,
pX|yj =
{
pxi|yj =
pi,j
âˆ‘
kâ‰¥1pk,j
; i â‰¥1
}
pY|xi =
{
pyj|xi =
pi,j
âˆ‘
kâ‰¥1pi,k
; j â‰¥1
}
.
Statistical Implications of Turingâ€™s Formula, First Edition. Zhiyi Zhang.
Â© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

150
Statistical Implications of Turingâ€™s Formula
By Deï¬nition 3.1, entropy may be deï¬ned for each of these probability distri-
butions, that is, for pX,Y, pX, pY, pX|yj, and pY|xi, and the respective entropies are
H(X, Y) = âˆ’
âˆ‘
iâ‰¥1
âˆ‘
jâ‰¥1
pi,j ln pi,j
H(X) = âˆ’
âˆ‘
iâ‰¥1
pi,â‹…ln pi,â‹…
H(Y) = âˆ’
âˆ‘
jâ‰¥1
pâ‹…,j ln pâ‹…,j
H(X|Y = yj) = âˆ’
âˆ‘
iâ‰¥1
pxi|yj ln pxi|yj
H(Y|X = xi) = âˆ’
âˆ‘
jâ‰¥1
pyj|xi ln pyj|xi.
Deï¬nition 5.1
Given a joint probability distribution pX,Y = {pi,j} on ğ’³Ã— ğ’´,
the (expected) conditional entropy of Y given X is
H(Y|X) =
âˆ‘
iâ‰¥1
pi,â‹…H(Y|X = xi).
(5.1)
An exchange of X and Y in Deï¬nition 5.1 gives the expected conditional
entropy of X given Y is
H(X|Y) =
âˆ‘
jâ‰¥1
pâ‹…,jH(X|Y = yj).
In information theory, H(Y|X) is often referred to as â€œthe conditional
entropy of Y given X.â€ However, it is to be noted that it is not entropy of a
probability distribution in the sense of Deï¬nition 3.1. H(Y|X = xi) is entropy
of a conditional probability distributionand so is H(X|Y = yj). H(Y|X) is a
weighted average of conditional entropies H(Y|X = xi), i â‰¥1, with respect to
the marginal probability distribution of X, and H(X|Y) is a weighted average
of conditional entropies H(X|Y = yj), j â‰¥1, with respect to the marginal
probability distribution of Y.
Lemma 5.1
Given a joint probability distribution pX,Y = {pi,j; i â‰¥1, j â‰¥1} on
ğ’³Ã— ğ’´,
1) H(X, Y) = H(X) + H(Y|X) and
2) H(X, Y) = H(Y) + H(X|Y).
Proof: For Part 1, noting that, for each pair of indices (i, j), pi,j â‰¤pi,â‹…,
ln pi,j â‰¤ln pi,â‹…, and hence âˆ’ln pi,j â‰¥âˆ’ln pi,â‹…â‰¥0,
H(X, Y) = âˆ’
âˆ‘
iâ‰¥1
âˆ‘
jâ‰¥1
pi,j ln pi,j
= âˆ’
âˆ‘
iâ‰¥1
âˆ‘
jâ‰¥1
pi,j ln(pyj|xipi,â‹…)

Estimation of Information
151
= âˆ’
âˆ‘
iâ‰¥1
âˆ‘
jâ‰¥1
pi,j ln pyj|xi âˆ’
âˆ‘
iâ‰¥1
âˆ‘
jâ‰¥1
pi,j ln pi,â‹…
= âˆ’
âˆ‘
iâ‰¥1
pi,â‹…
âˆ‘
jâ‰¥1
pyj|xi ln pyj|xi âˆ’
âˆ‘
iâ‰¥1
pi,â‹…ln pi,â‹…
= H(Y|X) + H(X).
Part 2 follows from the symmetry of H(X, Y) with respect to X and Y.
â—½
Corollary 5.1
Given a joint probability distribution pX,Y = {pi,j; i â‰¥1, j â‰¥1}
on ğ’³Ã— ğ’´,
1) H(X) â‰¤H(X, Y),
2) H(Y) â‰¤H(X, Y), and
3) H(X) + H(Y) âˆ’H(X, Y) â‰¤H(X, Y).
Proof: Since, for each i â‰¥1, the entropy of the conditional probability dis-
tribution H(Y|X = xi) is non-negative, the expected conditional entropy
H(Y|X) â‰¥0, being a positively weighted average of the conditional entropies,
is also non-negative. For Part 1, by Lemma 5.1, the desired result follows from
H(X, Y) = H(X) + H(Y|X) â‰¥H(X).
Part 2 follows similarly. Part 3 is a consequence of Parts 1 and 2.
â—½
By Corollary 5.1, the condition H(X, Y) < âˆimplies both H(X) < âˆand
H(Y) < âˆ.
Consider two probability distributions
p = {pk; k â‰¥1}
and
q = {qk; k â‰¥1}
on a same alphabet ğ’³.
Deï¬nition 5.2
For two probability distributions p and q on a same alphabet
ğ’³, the relative entropy or the Kullbackâ€“Leibler divergence of p and q is
D(p||q) =
âˆ‘
kâ‰¥1
pk ln
(pk
qk
)
,
(5.2)
observing the conventions that, for each summand p ln(pâˆ•q),
1) p ln (pâˆ•q) = 0, if p = 0, and
2) p ln (pâˆ•q) = +âˆ, if p > 0 and q = 0.
By the above-mentioned deï¬nition, if there exists a k such that pk > 0 and
qk = 0, then D(p||q) = +âˆ.

152
Statistical Implications of Turingâ€™s Formula
Suppose p and q have the same support, that is, for each k, pk > 0 if and only
if qk > 0. Then letting {ri; i â‰¥1} be the sequence of distinct values in
{qkâˆ•pk âˆ¶k â‰¥1 and pk > 0}
and
pâˆ—
i =
âˆ‘
kâˆ¶qkâˆ•pk=ri
pk,
the following table deï¬nes a random variable R and its probability distribution.
R
r1
Â· Â· Â·
ri
Â· Â· Â·
P(r)
pâˆ—
1
Â· Â· Â·
pâˆ—
i
Â· Â· Â·.
(5.3)
Theorem 5.1
Let p and q be two probability distributions on a same alphabet
ğ’³and with the same support. Suppose E(R) < âˆwhere R is as given in (5.3).
Then
D(p||q) â‰¥0.
(5.4)
Moreover, the equality holds if and only if p = q.
To prove Theorem 5.1, Jensenâ€™s inequality is needed and is stated below as a
lemma without proof.
Lemma 5.2
(Jensenâ€™s inequality)
If g(x) is a convex function on an open
interval (a, b) and X is a random variable with P(X âˆˆ(a, b)) = 1 and ï¬nite
expectation, then
E(g(X)) â‰¥g(E(X)).
(5.5)
Moreover, if g(x) is strictly convex, then E(g(X)) = g(E(X)) if and only if
P(X = c) = 1 where c is some constant.
Proof of Theorem 5.1: By Deï¬nition 5.2 and noting g(t) = âˆ’ln(t) is a strictly
convex function on (0, âˆ),
D(p||q) =
âˆ‘
kâ‰¥1
pk ln
(pk
qk
)
=
âˆ‘
kâˆ¶pk>0
pk ln
(pk
qk
)
=
âˆ‘
iâ‰¥1
pâˆ—
i (âˆ’ln ri)
= E(âˆ’ln R) â‰¥âˆ’ln E(R)
(5.6)
= âˆ’ln
(
âˆ‘
iâ‰¥1
pâˆ—
i ri
)
= âˆ’ln
(
âˆ‘
iâ‰¥1
(
âˆ‘
kâˆ¶qkâˆ•pk=ri
pk
)
ri
)
= âˆ’ln
(
âˆ‘
iâ‰¥1
(
âˆ‘
kâˆ¶qkâˆ•pk=ri
pk
qk
pk
))
= âˆ’ln
(
âˆ‘
iâ‰¥1
(
âˆ‘
kâˆ¶qkâˆ•pk=ri
qk
))
= âˆ’ln 1 = 0
where the inequality in (5.6) is due to Lemma 5.2.

Estimation of Information
153
If D(p||q) = 0, then the inequality in (5.6) is forced to be an equality, which,
again by Lemma 5.2, implies that R is constant with probability one, that is,
qkâˆ•pk = r or qk = pkr for every k for which pk > 0. Since p and q have the
same support, summing over all k gives r = 1, that is, pk = qk for every k. This
completes the proof.
â—½
Theorem 5.1 gives fundamental support to the Kullbackâ€“Leibler divergence
as a reasonable measure of the diï¬€erence between two distributions on a
common alphabet. However, it is to be noted that D(p||q) is not symmetric
with respect to p and q, and therefore not a distance measure. For this reason,
the following symmetrized Kullbackâ€“Leibler divergence, as given in Kullback
and Leibler (1951), is sometimes used.
D(p, q) = D(p||q) + D(q||p).
(5.7)
Deï¬nition 5.3
The mutual information (MI) of random elements, (X, Y) âˆˆ
ğ’³Ã— ğ’´with a joint probability distribution pX,Y, is
MI = MI(X, Y) = D(pX,Y||pXY) =
âˆ‘
iâ‰¥1
âˆ‘
jâ‰¥1
pi,j ln
( pi,j
pi,â‹…pâ‹…,j
)
.
(5.8)
Mutual information may be viewed as a degree of dependence between X and
Y as measured by the Kullbackâ€“Leibler divergence between pX,Y and pXY on
ğ’³Ã— ğ’´. Deï¬nition 5.3 and Theorem 5.1 immediately give Theorem 5.2.
Theorem 5.2
For any joint distribution pX,Y on ğ’³Ã— ğ’´,
MI(X, Y) â‰¥0.
(5.9)
Moreover, the equality holds if and only if X and Y are independent.
Theorem 5.3
Suppose H(X, Y) < âˆfor a joint distribution pX,Y on ğ’³Ã— ğ’´.
Then
MI(X, Y) = H(X) + H(Y) âˆ’H(X, Y),
(5.10)
MI(X, Y) = H(Y) âˆ’H(Y|X),
(5.11)
MI(X, Y) = H(X) âˆ’H(X|Y),
(5.12)
MI(X, Y) = MI(Y, X),
(5.13)
MI(X, X) = H(X), and
(5.14)
MI(X, Y) â‰¤H(X, Y).
(5.15)
Proof: By Corollary 5.1, H(X, Y) < âˆimplies H(X) < âˆand H(Y) < âˆ, and
it follows that
MI(X, Y) =
âˆ‘
iâ‰¥1
âˆ‘
jâ‰¥1
pi,j ln
( pi,j
pi,â‹…pâ‹…,j
)

154
Statistical Implications of Turingâ€™s Formula
=
âˆ‘
iâ‰¥1
âˆ‘
jâ‰¥1
pi,j(ln pi,j âˆ’ln pi,â‹…âˆ’ln pâ‹…,j)
= H(X) + H(Y) âˆ’H(X, Y),
and therefore (5.10) holds.
By Parts 1 and 2 of Lemma 5.1,
MI(X, Y) = H(X) + H(Y) âˆ’H(X, Y) = H(Y) âˆ’H(Y|X),
MI(X, Y) = H(X) + H(Y) âˆ’H(X, Y) = H(X) âˆ’H(X|Y),
and therefore (5.11) and (5.12) hold.
Equation (5.13) is obvious by deï¬nition.
Noting P(X = x, X = x) = P(X = x), MI(X, X) = H(X) by deï¬nition, that is,
(5.14) holds.
Equation (5.15) is a restatement of Part 3 of Corollary 5.1.
â—½
In Theorem 5.3, MI is expressed in various linear forms of entropies. These
forms are well deï¬ned if the joint distribution pX,Y has a ï¬nite support in
ğ’³Ã— ğ’´. However, if the support is inï¬nite, these linear forms of entropies
only have well-deï¬ned meaning under the condition of H(X, Y) < âˆ, which
implies H(X) < âˆand H(Y) < âˆ.
Example 5.1
Let, for i = 1, 2,â€¦,
pi =
1
C(i + 1)ln2(i + 1)
where C is a constant such that âˆ‘
iâ‰¥1pi = 1. It was demonstrated in Example 3.3
that this distribution has an inï¬nite entropy. Let the joint distribution pX,Y be
such that
pi,j = pi Ã— pj
for each pair of (i, j), where i â‰¥1 and j â‰¥1. By Deï¬nition 5.3, MI = 0. However,
by Corollary 5.1, H(X, Y) = âˆ, and therefore, the condition of Theorem 5.3 does
not hold.
Entropy may be viewed as a dispersion measure on an alphabet, much like
the variance of a random variable on the real line. It is perhaps interesting
to note that the expression in (5.11) (and therefore also that in (5.12)) has an
ANOVA-like intuitive interpretation as follows.
ANOVA-Like Interpretation of Mutual Information: H(Y) is the total
amount of random dispersion in Y. H(Y|X) is the (average) amount of
dispersion in Y that is unexplained by X. The remainder, H(Y) âˆ’H(Y|X),
therefore may be viewed as the amount of dispersion in Y that is explained by

Estimation of Information
155
X. The larger the value of MI(X, Y) = H(Y) âˆ’H(Y|X), the higher the level of
association is between X and Y.
To illustrate the characteristics of MI, consider the following three examples.
Example 5.2
Let pX,Y = {pi,j} be such that
pi,j =
{ 1
33
if 1 â‰¤i â‰¤11 and 1 â‰¤j â‰¤3
0
otherwise
where i and j are positive integers. Let ğ’³be a subset of the xy-plane containing
the following 33 points, denoted by (xi, yj):
(xi, yj) = (0.1(i âˆ’1), 0.6 âˆ’0.1(j âˆ’1))
where i = 1,â€¦, 11 and j = 1, 2, 3 (see Figure 5.1a).
Consider the pair of random variables (X, Y) such that
P((X, Y) = (xi, yj)) = pi,j.
Since X and Y are independent,
ğœŒ=
Cov(X, Y)
âˆš
Var(X) Ã— VarY
= 0
and
MI(X, Y) = 0.
(5.16)
Example 5.3
Let pX,Y = {pi,j} be such that
pi,j =
{ 1
42
if 1 â‰¤i â‰¤21 and 1 â‰¤j â‰¤2
0
otherwise
where i and j are positive integers. Let ğ’³be a subset of the xy-plane containing
the following 42 points, denoted by (xi, yj):
(xi, yj) = (0.05(i âˆ’1), 0.05(2 + i âˆ’2j))
where i = 1,â€¦, 21 and j = 1, 2 (see Figure 5.1b).
Consider the pair of random variables (X, Y) such that
P((X, Y) = (xi, yj)) = pi,j.
H(X) = ln 21,
(5.17)
H(Y) = 4
42 ln 42 + 19
21 ln 21,
(5.18)
H(X, Y) = ln 42.
(5.19)
It may be veriï¬ed that
ğœŒ=
Cov(X, Y)
âˆš
Var(X) Ã— VarY
= 0.9866
and
MI(X, Y) = 2.4274.
(5.20)

156
Statistical Implications of Turingâ€™s Formula
Example 5.4
Let pX,Y = {pi,j} be such that
pi,j =
{ 1
80
if 1 â‰¤i â‰¤80 and j = 1
0
otherwise
where i and j are positive integers. Let ğ’³be a subset of the xy-plane containing
the following 80 points, denoted by (xi, yj):
(xi, yj) =
â§
âª
â¨
âªâ©
(
âˆ’2ğœ‹+ (i âˆ’1)4ğœ‹
80 , cos(xi)
)
if 1 â‰¤i â‰¤40
(
âˆ’2ğœ‹+ i4ğœ‹
80 , cos(xi)
)
if 41 â‰¤i â‰¤80
(see Figure 5.1c).
Consider the pair of random variables (X, Y) such that
P((X, Y) = (xi, yj)) = pi,j.
H(X) = ln 80,
(5.21)
H(Y) = 19
( 4
80
)
ln
(80
4
)
+ 2
( 2
80
)
ln
(80
2
)
,
(5.22)
H(X, Y) = ln 80.
(5.23)
It may be veriï¬ed that
ğœŒ=
Cov(X, Y)
âˆš
Var(X) Ã— VarY
= 0
and
MI(X, Y) = 3.0304.
(5.24)
Examples 5.2â€“5.4 demonstrate that MI has an advantage over the coeï¬ƒcient
of correlation ğœŒin capturing a possibly nonlinear relationship between a pair
of random variables (X, Y). In fact, an even stronger characteristic of MI is
demonstrated by revisiting Example 5.4.
Suppose, in Example 5.4, the random pair (X, Y) is redeï¬ned in such a way
that it amounts to an exchange of positions along the x-axis (see Figure 5.2a) or
an exchange of positions along the y-axis (see Figure 5.2b). The values of mutual
information MI(X, Y) remain unchanged under such permutations on the
index sets, {i; i â‰¥1} or {j; j â‰¥1}, respectively. In fact, Figure 5.2c represents
the set of (xi, yj) points in Example 5.4 but after a sequence of permutations,
ï¬rst on {i; i â‰¥1} and then on {j; j â‰¥1}, respectively. The resulting data set, at a
ï¬rst glance, lacks any visible relationship between X and Y, linear or nonlinear.
Yet, the mutual information MI(X, Y) in Figure 5.2c remains unchanged from
Figure 5.1c. The seemingly anti-intuitive phenomenon here is caused by the
fact that one is often conditioned and, therefore, expects to see a continuous
functional relationship between X and Y. Such a functional relationship does
not necessarily exist on a general joint alphabet ğ’³Ã— ğ’´, where the letters
do not necessarily have any natural orders much less numerical meanings.

Estimation of Information
157
0
0.5
1
0
0.5
1
(a)
0
0.5
1
0
0.5
1
(b)
0
Ï€
â€“2Ï€
â€“Ï€
2Ï€
âˆ’1
0
1
(c)
ğ’´
ğ’´
ğ’´
ğ’³
ğ’³
ğ’³
Figure 5.1 Letters with positive probabilities: (a) Example 5.2, (b) Example 5.3, and
(c) Example 5.4

158
Statistical Implications of Turingâ€™s Formula
â€“2Ï€
â€“Ï€
0
Ï€
2Ï€
â€“2Ï€
â€“Ï€
0
Ï€
2Ï€
â€“2Ï€
â€“Ï€
0
Ï€
2Ï€
âˆ’1
0
1
(a)
âˆ’1
0
1
âˆ’1
0
1
(c)
ğ’´
ğ’´
ğ’´
ğ’³
(b)
ğ’³
ğ’³
Figure 5.2 Invariance of MI under respective permutations by letters in ğ’³and ğ’´
The unchanged positive MI in Figure 5.2c from Figure 5.1c is explained by the
fact that, for each given xi in Figure 5.2c, there is one and only one yj carrying
positive probability. That is to say that the y-value is uniquely determined
by an x-value, and hence a higher degree of association and MI. Since this
relationship is not changed from Figure 5.1c, the MI remains unchanged.

Estimation of Information
159
While MI as in Deï¬nition 5.3 has been shown to have many desirable
properties, it does not have a uniform upper bound since entropies can be
arbitrarily large. The lack of such an upper bound for MI makes it diï¬ƒcult to
convey an intuitive sense of how strong the association between X and Y is
even when it is known to be positive. To alleviate this issue, the standardized
mutual information (SMI), ğœ…, between X and Y is given in Deï¬nition 5.4
below.
Deï¬nition 5.4
Provided that H(X, Y) < âˆ, the standardized MI is given by
ğœ…=
MI
H(X, Y) = H(X) + H(Y) âˆ’H(X, Y)
H(X, Y)
= H(X) + H(Y)
H(X, Y)
âˆ’1.
(5.25)
Deï¬nition 5.5
Random elements X âˆˆğ’³and Y âˆˆğ’´are said to have a
one-to-one correspondence under a joint probability distribution pX,Y on
ğ’³Ã— ğ’´if
1) for every i satisfying P(X = xi) > 0, there exists a unique j such that
P(Y = yj|X = xi) = 1, and
2) for every j satisfying P(Y = yj) > 0, there exists a unique i such that
P(X = xi|Y = yj) = 1.
Theorem 5.4
Suppose H(X, Y) < âˆ. Then
0 â‰¤ğœ…â‰¤1.
(5.26)
Moreover,
1) ğœ…= 0 if and only if X and Y are independent, and
2) ğœ…= 1 if and only if X and Y have a one-to-one correspondence.
Proof: By Theorem 5.2 and Corollary 5.1,
0 â‰¤MI â‰¤H(X, Y).
Dividing all three parts above by H(X, Y) gives (5.26).
Since ğœ…= 0 if and only if MI = 0, ğœ…= 0 if and only if X and Y are independent
by Theorem 5.2.
If X and Y have a one-to-one correspondence, then for each i satisfying P(X =
xi) > 0 let the unique corresponding j be denoted by ji. Noting pi,ji = pi,â‹…and
therefore ln(pi,jiâˆ•pi,â‹…) = 0,
H(X, Y) = âˆ’
âˆ‘
iâ‰¥1,jâ‰¥1
pi,j ln pi,j
= âˆ’
âˆ‘
jâ‰¥1
âˆ‘
iâ‰¥1
pi,â‹…
(pi,j
pi,â‹…
)
ln
[
pi,â‹…
(pi,j
pi,â‹…
)]

160
Statistical Implications of Turingâ€™s Formula
= âˆ’
âˆ‘
jâ‰¥1
âˆ‘
iâ‰¥1
pi,â‹…
(pi,j
pi,â‹…
)
ln pi,â‹…âˆ’
âˆ‘
jâ‰¥1
âˆ‘
iâ‰¥1
pi,â‹…
(pi,j
pi,â‹…
)
ln
(pi,j
pi,â‹…
)
= âˆ’
âˆ‘
iâ‰¥1
pi,â‹…ln pi,â‹…âˆ’
âˆ‘
iâ‰¥1
pi,â‹…
(pi,ji
pi,â‹…
)
ln
(pi,ji
pi,â‹…
)
= H(X).
Similarly, H(X, Y) = H(Y), and therefore H(X) + H(Y) = 2H(X, Y), in turn,
it implies
ğœ…= H(X) + H(Y)
H(X, Y)
âˆ’1 = 2 âˆ’1 = 1.
On the other hand, if ğœ…= 1, then H(X) + H(Y) = 2H(X, Y) and therefore
(H(X) âˆ’H(X, Y)) + (H(Y) âˆ’H(X, Y)) = 0.
However, by Corollary 5.1, both of the above-mentioned additive terms are
nonpositive, which implies
H(X) = H(X, Y)
and
H(Y) = H(X, Y),
which in turn, by Lemma 5.1, imply
H(X|Y) = 0
and
H(Y|X) = 0.
By Deï¬nition 5.1, H(Y|X) = 0 implies H(Y|X = xi) = 0 for every i satisfying
P(X = xi) > 0, which in turn implies that the conditional probability distribu-
tion of Y given X = xi puts a probability of mass one on a single point in ğ’´. By
symmetry, H(X|Y) = 0 implies that the conditional probability distribution of
X given Y = yj also puts a probability of mass one on a single point in ğ’³.
â—½
Example 5.5
Revisiting Example 5.2 and noting H(X, Y) = ln 33 > 0,
ğœŒ= 0, MI = 0, and ğœ…= 0.
Example 5.6
Revisiting Example 5.3 and noting H(X, Y) = ln 42,
ğœŒ= 0.9866, MI = 2.4174, and ğœ…= 0.6468.
Example 5.7
Consider the following set of ï¬ve points, (x, y), on the xy-plane
with the given associated probability distribution.
(X, Y)
(âˆ’2, 2) (âˆ’1, âˆ’1) (0, 0) (1, 1) (2, âˆ’2)
P(X = x, Y = y)
0.05
0.2
0.5
0.2
0.05
See Figure 5.3.

Estimation of Information
161
âˆ’2
âˆ’1
0
1
2
âˆ’2
âˆ’1
0
1
2
ğ’³
ğ’´
Figure 5.3 Points with positive probabilities in Example 5.7
It may be veriï¬ed that
H(X) = H(Y) = H(X, Y) = 1.2899
(5.27)
and that
ğœŒ= 0,
MI(X, Y) = 1.2899 ,
and
ğœ…= 1.
(5.28)
Note that the fact ğœ…= 1 may be directly derived from Theorem 5.4.
Example 5.8
Revisiting Example 5.4 and noting H(X, Y) = ln 80,
ğœŒ= 0,
MI = 3.0304,
and
ğœ…= 0.6916.
Note that ğœ…< 1 in this case because, while X uniquely determines Y, Y does
not uniquely determine X. Readers are reminded that the information being
measured by MI, and hence ğœ…, is â€œmutual,â€ and it requires a one-to-one
correspondence between X and Y for ğœ…= 1.
The next example illustrates how the standardized MI, ğœ…, changes as the joint
probability distribution changes.
Example 5.9
Consider the following family of joint probability distributions
of (X, Y) on {0, 1} Ã— {0, 1}. For each m, m = 0, 1, 2,â€¦, 50,
X = 0
X = 1
Y = 0
0.50 âˆ’0.01 m
0.01 m
Y = 1
0.01 m
0.50 âˆ’0.01 m
or
0.50
0.00
0.00
0.50 , 0.49
0.01
0.01
0.49 ,...,
0.25
0.25
0.25
0.25 ,...,
0.00
0.50
0.50
0.00 .
(5.29)

162
Statistical Implications of Turingâ€™s Formula
30
20
m
10
0
0
0.25
0.5
0.75
1
40
50
Îº
Figure 5.4 Plot of ğœ…of (5.30) for m = 0,â€¦, 50.
When m = 1 or m = 50, ğœ…= 1, by Theorem 5.4. For any other m,
ğœ…= ln 2 âˆ’[(mâˆ•50) ln(50âˆ•m âˆ’1) âˆ’ln(1 âˆ’mâˆ•50)]
ln 2 + [(mâˆ•50) ln(50âˆ•m âˆ’1) âˆ’ln(1 âˆ’mâˆ•50)].
(5.30)
See Exercise 20.
A plot of ğœ…as a function of m is given in Figure 5.4. As m increases from
m = 0, the distribution at the head of (5.29) moves away from a perfect
one-to-one correspondence between X and Y in increments of 2% each. First, ğœ…
decreases rapidly from ğœ…= 1, then changes slowly as it moves closer to ğœ…= 0 at
m = 25, which represents a state of independence between X and Y, and ï¬nally
accelerates its ascend to ğœ…= 1 again as it approaches m = 50.
The sharp drop of ğœ…from the one observed in Example 5.9 as m moves away
from zero is quite typical for the standardized MI, when a small perturbation
is imposed on a joint distribution with a perfect one-to-one correspondence.
Consequently, in most realistic data sets, the calculated standardized MI is
often signiï¬cantly below one. For a comprehensive discussion on some of the
above-mentioned indices of information, as well as many others, readers may
refer to Cover and Thomas (2006).
5.2
Estimation of Mutual Information
In this section, two estimators of MI are described, one is the plug-in estimator
and the other is based on Turingâ€™s perspective. Both of them have asymptotic
normality. Toward that end, let
ğ’³= {xi; i = 1,â€¦, K1}
and
ğ’´= {yj; j = 1,â€¦, K2}
(5.31)

Estimation of Information
163
be two ï¬nite alphabets with cardinalities K1 < âˆand K2 < âˆ, respectively.
Consider the Cartesian product ğ’³Ã— ğ’´with a joint probability distribution
pX,Y = {pi,j}. Assume that pi,â‹…> 0 and pâ‹…,j > 0 for all 1 â‰¤i â‰¤K1 and 1 â‰¤j â‰¤K2.
Let
K =
âˆ‘
i,j
1[pi,j > 0]
(5.32)
be the number of positive joint probabilities in {pi,j}.
For every pair of (i, j), let fi,j be the observed frequency of the random pair
(X, Y) taking value (xi, yj), where i = 1,â€¦, K1 and j = 1,â€¦, K2, in an iid sample
of size n from ğ’³Ã— ğ’´; and let Ì‚pi,j = fi,jâˆ•n be the corresponding relative fre-
quency. Consequently, Ì‚pX,Y = {Ì‚pi,j}, Ì‚pX = {Ì‚pi,â‹…}, and Ì‚pY = {Ì‚pâ‹…,j} are the sets of
observed joint and marginal relative frequencies.
5.2.1
The Plug-In Estimator
To be instructive, consider ï¬rst the case of K = K1K2, that is, pi,j > 0 for every
pair (i, j) where i = 1,â€¦, K1 and i = 1,â€¦, K2. Let p be a speciï¬cally arranged
pi,j as follows.
p = (p1,1, p1,2,â€¦, p1,K2, p2,1, p2,2,â€¦, p2,K2,â€¦, pK1,1,â€¦, pK1,K2âˆ’1)ğœ.
Accordingly let
Ì‚p = ( Ì‚p1,1, Ì‚p1,2,â€¦, Ì‚p1,K2, Ì‚p2,1, Ì‚p2,2,â€¦, Ì‚p2,K2,â€¦, Ì‚pK1,1,â€¦, Ì‚pK1,K2âˆ’1)ğœ.
For notation convenience, the speciï¬cally rearranged pi,js and Ì‚pi,js in p and Ì‚p
are re-enumerated by a single index k and are denoted by
v = (p1,â€¦, pKâˆ’1)ğœ
and
Ì‚v = (Ì‚p1,â€¦, Ì‚pKâˆ’1)ğœ
(5.33)
where K = K1K2. It is to be noted that for any pair of (i, j),
k = (i âˆ’1)K2 + j.
The multivariate central limit theorem gives that
âˆš
n(Ì‚v âˆ’v)
L
âˆ’âˆ’âˆ’â†’MVN(0, Î£)
(5.34)
where Î£ is a (K1K2 âˆ’1) Ã— (K1K2 âˆ’1) covariance matrix as given in the
following.
Î£ âˆ¶= Î£(v) âˆ¶=
â›
âœ
âœ
âœ
âœ
âœ
âœâ
p1(1 âˆ’p1)
âˆ’p1p2
Â· Â· Â·
âˆ’p1pKâˆ’1
âˆ’p1p2
p2(1 âˆ’p2) Â· Â· Â·
âˆ’p2pKâˆ’1
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
âˆ’pKâˆ’1p1
âˆ’pKâˆ’1p2 Â· Â· Â· pKâˆ’1(1 âˆ’pKâˆ’1)
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸâ 
.
(5.35)

164
Statistical Implications of Turingâ€™s Formula
Shannonâ€™s entropies for ğ’³, ğ’´, and ğ’³Ã— ğ’´, with their respective distribu-
tions, are deï¬ned as
GX(v) = H(X) = âˆ’
âˆ‘
i
pi,â‹…ln pi,â‹…,
(5.36)
GY(v) = H(Y) = âˆ’
âˆ‘
j
pâ‹…,j ln pâ‹…,j,
(5.37)
GXY(v) = H(X, Y) = âˆ’
âˆ‘
i
âˆ‘
j
pi,j ln pi,j = âˆ’
âˆ‘
k
pk ln pk.
(5.38)
Let Ì‚H(X), Ì‚H(Y), and Ì‚H(X, Y) be the plug-in estimators of H(X), H(Y), and
H(X, Y), respectively, that is,
Ì‚H(X) = âˆ’
âˆ‘
i
Ì‚pi,â‹…ln Ì‚pi,â‹…,
(5.39)
Ì‚H(Y) = âˆ’
âˆ‘
j
Ì‚pâ‹…,j ln Ì‚pâ‹…,j,
(5.40)
Ì‚H(X, Y) = âˆ’
âˆ‘
i
âˆ‘
j
Ì‚pi,j ln Ì‚pi,j = âˆ’
âˆ‘
k
Ì‚pk ln Ì‚pk.
(5.41)
The respective gradients for GX(v), GY(v), and GXY(v) are
gX(v) = âˆ‡GX(v) =
(
âˆ‚
âˆ‚p1
GX(v),â€¦,
âˆ‚
âˆ‚pKâˆ’1
GX(v)
)ğœ
,
gY(v) = âˆ‡GY(v) =
(
âˆ‚
âˆ‚p1
GY(v),â€¦,
âˆ‚
âˆ‚pKâˆ’1
GY(v)
)ğœ
,
gXY(v) = âˆ‡GXY(v) =
(
âˆ‚
âˆ‚p1
GXY(v),â€¦,
âˆ‚
âˆ‚pKâˆ’1
GXY(v)
)ğœ
.
For every k, k = 1,â€¦, K âˆ’1, such that pk = pi,j, the following facts can be
veriï¬ed (see Exercise 11):
âˆ‚
âˆ‚pk
GX(v) = âˆ‚H(X)
âˆ‚pk
= ln pK1,â‹…âˆ’ln pi,â‹…,
(5.42)
âˆ‚
âˆ‚pk
GY(v) = âˆ‚H(Y)
âˆ‚pk
= ln pâ‹…,K2 âˆ’ln pâ‹…,j,
(5.43)
âˆ‚
âˆ‚pk
GXY(v) = âˆ‚H(X, Y)
âˆ‚pk
= ln pK âˆ’ln pk.
(5.44)
Let
A âˆ¶= A(v) âˆ¶=
â›
âœ
âœ
âœâ
ağœ
1
ağœ
2
ağœ
3
â
âŸ
âŸ
âŸâ 
âˆ¶=
â›
âœ
âœ
âœ
âœ
âœ
âœâ
âˆ‚
âˆ‚p1
GX(v) Â· Â· Â·
âˆ‚
âˆ‚pKâˆ’1
GX(v)
âˆ‚
âˆ‚p1
GY(v) Â· Â· Â·
âˆ‚
âˆ‚pKâˆ’1
GY(v)
âˆ‚
âˆ‚p1
GXY(v) Â· Â· Â·
âˆ‚
âˆ‚pKâˆ’1
GXY(v)
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸâ 
.
(5.45)

Estimation of Information
165
According to (5.42)â€“(5.44), the ï¬rst row of A, written in (K1 âˆ’1) blocks of
size K2 and 1 block of size (K2 âˆ’1), is
ağœ
1 = (
ln(pK1,â‹…âˆ•p1,â‹…),
Â· Â· Â·
ln(pK1,â‹…âˆ•p1,â‹…),
ln(pK1,â‹…âˆ•p1,â‹…),
ln(pK1,â‹…âˆ•p2,â‹…),
Â· Â· Â·
ln(pK1,â‹…âˆ•p2,â‹…),
ln(pK1,â‹…âˆ•p2,â‹…),
â‹®
â‹®â‹®â‹®
â‹®
â‹®
ln(pK1,â‹…âˆ•pK1âˆ’1,â‹…),
Â· Â· Â·
ln(pK1,â‹…âˆ•pK1âˆ’1,â‹…),
ln(pK1,â‹…âˆ•pK1âˆ’1,â‹…),
0,
Â· Â· Â·
0
);
the second row of A, written in (K1 âˆ’1) blocks of size K2 and 1 block of size
(K2 âˆ’1), is
ağœ
2 = (
ln(pâ‹…,K2âˆ•pâ‹…,1),
ln(pâ‹…,K2âˆ•pâ‹…,2),
Â· Â· Â·
ln(pâ‹…,K2âˆ•pâ‹…,K2âˆ’1),
0,
ln(pâ‹…,K2âˆ•pâ‹…,1),
ln(pâ‹…,K2âˆ•pâ‹…,2),
Â· Â· Â·
ln(pâ‹…,K2âˆ•pâ‹…,K2âˆ’1),
0,
â‹®
â‹®
â‹®â‹®â‹®
â‹®
â‹®
ln(pâ‹…,K2âˆ•pâ‹…,1),
ln(pâ‹…,K2âˆ•pâ‹…,2),
Â· Â· Â·
ln(pâ‹…,K2âˆ•pâ‹…,K2âˆ’1),
0,
ln(pâ‹…,K2âˆ•pâ‹…,1),
ln(pâ‹…,K2âˆ•pâ‹…,2),
Â· Â· Â·
ln(pâ‹…,K2âˆ•pâ‹…,K2âˆ’1)
);
and the third row of A, written in (K1 âˆ’1) blocks of size K2 and 1 block of size
(K2 âˆ’1), is
ağœ
3 = (
ln(pKâˆ•p1,1),
Â· Â· Â·
ln(pKâˆ•p1,K2âˆ’1),
ln(pKâˆ•p1,K2),
ln(pKâˆ•p2,1),
Â· Â· Â·
ln(pKâˆ•p2,K2âˆ’1),
ln(pKâˆ•p2,K2),
â‹®
â‹®â‹®â‹®
â‹®
â‹®
ln(pKâˆ•pK1,1),
Â· Â· Â·
ln(pKâˆ•pK1,K2âˆ’1) );
An application of the delta method gives Lemma 5.3 below.
Lemma 5.3
Let ğ’³and ğ’´be as in (5.31), let pX,Y = {pi,j} be a joint probability
distribution on ğ’³Ã— ğ’´, and let v and Ì‚v be as in (5.33). Suppose pi,j > 0 for every
pair (i, j) where i = 1,â€¦, K1 and j = 1,â€¦, K2. Then
âˆš
n
â¡
â¢
â¢â£
â›
âœ
âœâ
Ì‚H(X)
Ì‚H(Y)
Ì‚H(X, Y)
â
âŸ
âŸâ 
âˆ’
â›
âœ
âœâ
H(X)
H(Y)
H(X, Y)
â
âŸ
âŸâ 
â¤
â¥
â¥â¦
L
âˆ’âˆ’âˆ’â†’MVN(0, Î£H)
(5.46)
where
Î£H = AÎ£Ağœ,
Î£ is as in (5.35), and A is as in (5.45) according to (5.42)â€“(5.44).
More generally for the case of K â‰¤K1K2, that is, pi,j may not be positive
for some pairs of (i, j), a result similar to Lemma 5.3 can be derived. For any

166
Statistical Implications of Turingâ€™s Formula
arbitrary but ï¬xed re-enumeration of the K positive probabilities in {pi,j},
denoted as
{pk; k = 1,â€¦, K},
(5.47)
consider the following two partitions
{S1,â€¦, SK1}
and
{T1,â€¦, TK2}
of the index set {1, 2,â€¦, K} such that
1) {pk; k âˆˆSs} is the collection of all positive probabilities in {pi,j; i = s} for
each s, s = 1,â€¦, K1; and
2) {pk; k âˆˆTt} is the collection of all positive probabilities in {pi,j; j = t} for
each t, t = 1,â€¦, K2.
By the construction of the partitions,
âˆ‘
kâˆˆSi
pk = pi,â‹…
and
âˆ‘
kâˆˆTj
pk = pâ‹…,j.
Without loss of generality, it may be assumed that K âˆˆSK1 âˆ©TK2. If not, then
K âˆˆSi0 âˆ©Tj0 for some i0 and j0, by a rearrangement of the indices (i, j), K âˆˆ
SK1 âˆ©TK2 will be true.
Letting
v = (p1,â€¦, pKâˆ’1)ğœ
and
Ì‚v = (Ì‚p1,â€¦, Ì‚pKâˆ’1)ğœ,
(5.48)
an application of multivariate central limit theorem gives
âˆš
n(Ì‚v âˆ’v)
L
âˆ’âˆ’âˆ’â†’MVN(0, Î£)
(5.49)
where Î£ is the (K âˆ’1) Ã— (K âˆ’1) covariance matrix given by
Î£ âˆ¶= Î£(v) âˆ¶=
â›
âœ
âœ
âœâ
p1(1 âˆ’p1)
âˆ’p1p2
Â· Â· Â·
âˆ’p1pKâˆ’1
âˆ’p2p1
p2(1 âˆ’p2) Â· Â· Â·
âˆ’p2pKâˆ’1
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
âˆ’pKâˆ’1p1
âˆ’pKâˆ’1p2 Â· Â· Â· pKâˆ’1(1 âˆ’pKâˆ’1)
â
âŸ
âŸ
âŸâ 
.
(5.50)
Noting (5.36)â€“(5.38), the following facts can be veriï¬ed:
âˆ‚GX(v)
âˆ‚pk
= âˆ‚H(X)
âˆ‚pk
=
{
ln pK1,â‹…âˆ’ln pi,â‹…, if k âˆˆSi â‰ SK1
0,
if k âˆˆSK1
(5.51)
âˆ‚GY(v)
âˆ‚pk
= âˆ‚H(Y)
âˆ‚pk
=
{
ln pâ‹…,K2 âˆ’ln pâ‹…,j, if k âˆˆTj â‰ TK2
0,
if k âˆˆTK2
(5.52)
âˆ‚GXY(v)
âˆ‚pk
= âˆ‚H(X, Y)
âˆ‚pk
= ln pK âˆ’ln pk, for 1 â‰¤k â‰¤K âˆ’1.
(5.53)

Estimation of Information
167
To show (5.51) is true, consider ï¬rst
GX(v) = âˆ’
K1
âˆ‘
s=1
(
âˆ‘
lâˆˆSs
pl
)
ln
(
âˆ‘
lâˆˆSs
pl
)
= âˆ’
K1âˆ’1
âˆ‘
s=1
(
âˆ‘
lâˆˆSs
pl
)
ln
(
âˆ‘
lâˆˆSs
pl
)
âˆ’
â›
âœ
âœâ
âˆ‘
lâˆˆSK1
pl
â
âŸ
âŸâ 
ln
â›
âœ
âœâ
âˆ‘
lâˆˆSK1
pl
â
âŸ
âŸâ 
= âˆ’
K1âˆ’1
âˆ‘
s=1
(
âˆ‘
lâˆˆSs
pl
)
ln
(
âˆ‘
lâˆˆSs
pl
)
âˆ’
[
1 âˆ’
K1âˆ’1
âˆ‘
s=1
(
âˆ‘
lâˆˆSs
pl
)]
ln
(
1 âˆ’
K1âˆ’1
âˆ‘
s=1
(
âˆ‘
lâˆˆSs
pl
))
.
For the case of k âˆˆSi â‰ SK1,
âˆ‚GX(v)
âˆ‚pk
= âˆ’âˆ‚
âˆ‚pk
[(
âˆ‘
lâˆˆSi
pl
)
ln
(
âˆ‘
lâˆˆSi
pl
)]
âˆ’
âˆ‚
âˆ‚pk
{[
1 âˆ’
K1âˆ’1
âˆ‘
s=1
(
âˆ‘
lâˆˆSs
pl
)]
ln
(
1 âˆ’
K1âˆ’1
âˆ‘
s=1
(
âˆ‘
lâˆˆSs
pl
))}
= âˆ’ln
(
âˆ‘
lâˆˆSi
pl
)
âˆ’1 âˆ’
{
âˆ’ln
(
1 âˆ’
K1âˆ’1
âˆ‘
s=1
(
âˆ‘
lâˆˆSs
pl
))
âˆ’1
}
= âˆ’ln pi,â‹…+ ln
(
1 âˆ’
K1âˆ’1
âˆ‘
s=1
(
âˆ‘
lâˆˆSs
pl
))
= âˆ’ln pi,â‹…+ ln pK1,â‹…= ln pK1,â‹…âˆ’ln pi,â‹….
For the case of k âˆˆSK1, âˆ‚GX(v)âˆ•âˆ‚pk = 0.
A similar argument would give the proof for (5.52) (see Exercise 11).
Equation (5.53) is obvious.
For any arbitrary but ï¬xed re-enumeration of the positive terms of {pi,j} in
(5.47), an application of the delta method, based on (5.49), gives Lemma 5.4
below.
Lemma 5.4
Let ğ’³and ğ’´be as in (5.31), let pX,Y = {pi,j} be a joint probability
distribution on ğ’³Ã— ğ’´, and let v and Ì‚v be as in (5.48):
âˆš
n
â¡
â¢
â¢â£
â›
âœ
âœâ
Ì‚H(X)
Ì‚H(Y)
Ì‚H(X, Y)
â
âŸ
âŸâ 
âˆ’
â›
âœ
âœâ
H(X)
H(Y)
H(X, Y)
â
âŸ
âŸâ 
â¤
â¥
â¥â¦
L
âˆ’âˆ’âˆ’â†’MVN(0, Î£H)
(5.54)
where
Î£H = AÎ£Ağœ,
(5.55)
Î£ is as in (5.50) and A is as in (5.45) according to (5.51)â€“(5.53).

168
Statistical Implications of Turingâ€™s Formula
Remark 5.1
Lemma 5.3 is a special case of Lemma 5.4, and it is sepa-
rately stated for its instructive quality. While symbolically identical, the
matrices A and Î£ in Lemma 5.4 may take on diï¬€erent forms from those in
Lemma 5.3, depending on K, the number of positive probabilities in {pi,j}, and
the re-enumeration in (5.48).
Note that the covariance matrix Î£H in Lemma 5.4 (as well as that in
Lemma 5.3) may not necessarily be of full rank. For Î£H to be of full rank, some
additional conditions need to be imposed.
To estimate MI = MI(X, Y), noting
MI(X, Y) = (1, 1, âˆ’1)
â›
âœ
âœâ
H(X)
H(Y)
H(X, Y)
â
âŸ
âŸâ 
and letting
Ì‚
MI = Ì‚
MI (X, Y) = (1, 1, âˆ’1)
â›
âœ
âœâ
Ì‚H(X)
Ì‚H(Y)
Ì‚H(X, Y)
â
âŸ
âŸâ 
,
(5.56)
an application of the multivariate delta method gives Theorem 5.5 below.
Theorem 5.5
Under the conditions of Lemma 5.4 and suppose further that
ğœ2 = (1, 1 âˆ’1)Î£H(1, 1, âˆ’1)ğœ> 0
(5.57)
where Î£H is as in (5.35). Then
âˆš
n( Ì‚
MI âˆ’MI)
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2).
(5.58)
Let
Ì‚Î£ = Î£(Ì‚v) =
â›
âœ
âœ
âœ
âœâ
Ì‚p1(1 âˆ’Ì‚p1)
âˆ’Ì‚p1 Ì‚p2
Â· Â· Â·
âˆ’Ì‚p1 Ì‚pKâˆ’1
âˆ’Ì‚p2 Ì‚p1
Ì‚p2(1 âˆ’Ì‚p2) Â· Â· Â·
âˆ’Ì‚p2 Ì‚pKâˆ’1
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
âˆ’Ì‚pKâˆ’1 Ì‚p1
âˆ’Ì‚pKâˆ’1 Ì‚p2 Â· Â· Â· Ì‚pKâˆ’1(1 âˆ’Ì‚pKâˆ’1)
â
âŸ
âŸ
âŸ
âŸâ 
,
(5.59)
Ì‚A = A(Ì‚v) =
â›
âœ
âœ
âœ
âœ
âœ
âœâ
âˆ‚
âˆ‚p1
GX(v) Â· Â· Â·
âˆ‚
âˆ‚pKâˆ’1
GX(v)
âˆ‚
âˆ‚p1
GY(v) Â· Â· Â·
âˆ‚
âˆ‚pKâˆ’1
GY(v)
âˆ‚
âˆ‚p1
GXY(v) Â· Â· Â·
âˆ‚
âˆ‚pKâˆ’1
GXY(v)
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸâ 
|||||||||||||v=Ì‚v
,
(5.60)
Ì‚Î£H = Ì‚A Ì‚Î£ Ì‚Ağœ,
(5.61)
Ì‚ğœ2 = (1, 1, âˆ’1) Ì‚Î£H(1, 1, âˆ’1)ğœ.
(5.62)

Estimation of Information
169
By the continuous mapping theorem, Ì‚Î£ = Î£(Ì‚v) is a consistent estimator of
Î£ = Î£(v), Ì‚A = A(Ì‚v) is a consistent estimator of A, Ì‚Î£H = Ì‚AÎ£(Ì‚v) Ì‚Ağœis a consistent
estimator of Î£H = Î£(v), and Ì‚ğœ2 is a consistent estimator of ğœ2. The said
consistency and Slutskyâ€™s theorem give immediately Corollary 5.2 below.
Corollary 5.2
Under the conditions of Theorem 5.5 ,
âˆš
n( Ì‚
MI âˆ’MI)
Ì‚ğœ
L
âˆ’âˆ’âˆ’â†’N(0, 1)
(5.63)
where Ì‚ğœis as given in (5.62).
Corollary 5.2 provides the means of a statistical tool to make inferences
regarding MI. However, its validity depends on the condition in (5.57). To
decipher (5.57), consider ï¬rst the case of K = K1K2, that is, pi,j > 0 for every
pair (i, j).
Since Î£ is positive deï¬nite (see Exercise 6), (5.57) holds if and only if
(1, 1, âˆ’1)A â‰ 0, or equivalently X and Y are not independent.
The equivalence of (1, 1, âˆ’1) A â‰ 0 and that X and Y are not independent
is established by the fact that (1, 1, âˆ’1)A = 0 if and only if X and Y are
independent. To see this fact, suppose (1, 1, âˆ’1) A = 0, that is, for every pair
(i, j), by (5.45),
ln(pK1,â‹…âˆ•pi,â‹…) + ln(pâ‹…,K2âˆ•pâ‹…,j) = ln(pKâˆ•pi,j),
or after a few algebraic steps,
pi,j =
pK
pK1,â‹…pâ‹…,K2
pi,â‹…pâ‹…,j.
Summing both sides over i and j gives pK = pK1,â‹…pâ‹…,K2, and therefore
pi,j = pi,â‹…pâ‹…,j
for every pair of (i, j), that is, X and Y are independent. The converse is
obviously true.
Next consider the case of K < K1K2. Without loss of generality, it may
be assumed that pi,â‹…> 0 for every i and pâ‹…,j > 0 for every j. (Or else, a
re-enumeration of the index i or j would ensure the validity of this assumption.)
An similar argument to the case of K = K1K2 would establish that pi,j = pi,â‹…pâ‹…,j
for every pair (i, j) such that pi,j â‰ 0 (see Exercise 12). At this point, it is estab-
lished in general that (5.57) is equivalent to that X and Y are not independent.
Since MI is a linear combination of three entropies, its estimation is closely
related to that of entropy. For entropy, the plug-in estimator Ì‚H based on an iid
sample of size n, though asymptotically eï¬ƒcient, is known to have a large bias,
as given in (3.3). Based on (3.3), it is easy to see that the bias for Ì‚
MI is
E( Ì‚
MI âˆ’MI) = K âˆ’(K1 + K2) + 1
2n
+ ğ’ª(nâˆ’2)

170
Statistical Implications of Turingâ€™s Formula
where K is an integer between max{K1, K2} and K1K2, but often is in the order
of K1K2, which leads to an overestimation for MI. For this reason, seeking a
good MI estimator with smaller bias has been the focal point of much of the
research in MI estimation.
In the next section, an estimator of MI based on Turingâ€™s perspective is
described. This estimator qualitatively alleviates the bias issue observed in the
plug-in estimator Ì‚
MI.
5.2.2
Estimation in Turingâ€™s Perspective
In Chapter 3, an entropy estimator based on Turingâ€™s perspective is introduced
as Ì‚Hz in (3.27). This estimator naturally extends to the following estimator of
MI:
Ì‚
MIz = Ì‚
MIz(X, Y)
(5.64)
= Ì‚Hz(X) + Ì‚Hz(Y) âˆ’Ì‚Hz(X, Y)
=
nâˆ’1
âˆ‘
v=1
1
v
{
nv+1[n âˆ’(v + 1)]!
n!
K1
âˆ‘
i=1
[
Ì‚pi,â‹…
vâˆ’1
âˆ
k=0
(
1 âˆ’Ì‚pi,â‹…âˆ’k
n
)]}
+
nâˆ’1
âˆ‘
v=1
1
v
{
nv+1[n âˆ’(v + 1)]!
n!
K2
âˆ‘
j=1
[
Ì‚pâ‹…,j
vâˆ’1
âˆ
k=0
(
1 âˆ’Ì‚pâ‹…,j âˆ’k
n
)]}
âˆ’
nâˆ’1
âˆ‘
v=1
1
v
{
nv+1[n âˆ’(v + 1)]!
n!
K1
âˆ‘
i=1
K2
âˆ‘
j=1
[
Ì‚pi,j
vâˆ’1
âˆ
k=0
(
1 âˆ’Ì‚pi,j âˆ’k
n
)]}
.
By (3.28), it is easy to see that the bias of Ì‚
MIz in (5.64) is
E( Ì‚
MIz âˆ’MI) = âˆ’
âˆ
âˆ‘
v=n
1
v
K1
âˆ‘
i=1
pi,â‹…(1 âˆ’pi,â‹…)v
âˆ’
âˆ
âˆ‘
v=n
1
v
K2
âˆ‘
j=1
pâ‹…,j(1 âˆ’pâ‹…,j)v
+
âˆ
âˆ‘
v=n
1
v
K1
âˆ‘
i=1
K2
âˆ‘
j=1
pi,j(1 âˆ’pi,j)v
and it is decaying at least exponentially fast in n since each of the three additive
terms is decaying at least exponentially fast in n, as argued in Theorem 3.9. In
fact, letting
pâˆ¨= max
i,j {pi,j > 0},
pâˆ§= min
i,j {pi,j > 0},
pâˆ¨,â‹…= max
i {pi,â‹…> 0},

Estimation of Information
171
pâˆ§,â‹…= min
i {pi,â‹…> 0},
pâ‹…,âˆ¨= max
j {pâ‹…,j > 0},
and
pâ‹…,âˆ§= min
j {pâ‹…,j > 0},
it is easily veriï¬ed that
âˆ’
âˆ
âˆ‘
v=n
1
v(1 âˆ’pâˆ§,â‹…)v âˆ’
âˆ
âˆ‘
v=n
1
v(1 âˆ’pâ‹…,âˆ§)v +
âˆ
âˆ‘
v=n
1
v(1 âˆ’pâˆ¨)v
â‰¤E( Ì‚
MIz âˆ’MI) â‰¤
(5.65)
âˆ’
âˆ
âˆ‘
v=n
1
v(1 âˆ’pâˆ¨,â‹…)v âˆ’
âˆ
âˆ‘
v=n
1
v(1 âˆ’pâ‹…,âˆ¨)v +
âˆ
âˆ‘
v=n
1
v(1 âˆ’pâˆ§)v,
and that all the terms of both sides of (5.65) converge to zero exponentially fast
since all of pâˆ¨, pâˆ§, pâˆ¨,â‹…, pâˆ§,â‹…, pâ‹…,âˆ¨, and pâ‹…,âˆ§are ï¬xed positive constants in (0, 1). The
inequalities of (5.65) are in fact one of the main advantages of the estimator Ì‚
MIz
in (5.64).
Theorem 5.6
Under the conditions of Theorem 5.5 and suppose further that X
and Y are not independent. Then
âˆš
n( Ì‚
MIz âˆ’MI)
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2),
(5.66)
where ğœis as in (5.57).
The asymptotic normality of Theorem 5.6 is derived from Theorem 5.5
by showing that Ì‚
MI and Ì‚
MIz are suï¬ƒciently close as n â†’âˆ. Toward
that end, consider ï¬rst the problem of Ì‚Hz in (3.27) estimating Shannonâ€™s
entropy H = âˆ’âˆ‘Kâ€²
k=1 pk ln pk deï¬ned with {pk; k = 1,â€¦, Kâ€²} on alphabet
â„’= {ğ“k; k = 1, â‹…, Kâ€²} where Kâ€² is ï¬nite. Let {Ì‚pk} be the observed relative
frequencies of letters in an iid sample of size n and Ì‚H = âˆ’âˆ‘Kâ€²
k=1 Ì‚pk ln Ì‚pk be the
plug-in estimator of H.
Lemma 5.5
Suppose {pk} is a nonuniform distribution on â„’= {ğ“k}. Then
âˆš
n( Ì‚Hz âˆ’Ì‚H)
p
âˆ’âˆ’âˆ’â†’0.
To prove Lemma 5.5, Lemma 3.2 is needed, which is restated as Lemma 5.6
below for easy reference. Let, for any p âˆˆ(0, 1),
gn(p) =
nâˆ’1
âˆ‘
v=1
{
1
v
{nv+1[n âˆ’(v + 1)]!
n!
}[ vâˆ’1
âˆ
j=0
(
1 âˆ’p âˆ’j
n
)]
1[vâ‰¤n(1âˆ’p)+1]
}
,
hn(p) = pgn(p), and h(p) = âˆ’p ln p.

172
Statistical Implications of Turingâ€™s Formula
Lemma 5.6
Let Ì‚p = Xâˆ•n where X is a binomial random variable with param-
eters n and p. Then
1)
âˆš
n[hn(p) âˆ’h(p)] â†’0 uniformly in p âˆˆ(c, 1) for any c, 0 < c < 1;
2)
âˆš
n|hn(p) âˆ’h(p)| < A(n) = ğ’ª(n3âˆ•2) uniformly in p âˆˆ[1âˆ•n, c] for any c,
0 < c < p; and
3) P( Ì‚p â‰¤c) < B(n) = ğ’ª(nâˆ’1âˆ•2 exp {âˆ’nC}) where C = (p âˆ’c)2âˆ•[p(1 âˆ’p)] for
any c âˆˆ(0, p).
Proof of Lemma 5.5: Without loss of generality, consider the sample pro-
portions of the ï¬rst letter of the alphabet Ì‚p1. It is of interest to show that
Î”n =
âˆš
n[hn( Ì‚p1) âˆ’h( Ì‚p1)]
pâ†’0, that is, for any Îµ > 0, as n â†’âˆ, P(Î”n > Îµ) â†’0.
Toward that end, observe that for any ï¬xed c âˆˆ(0, p1)
P(Î”n > Îµ) = P(
âˆš
n[hn( Ì‚p1) âˆ’h( Ì‚p1)] > Îµ|Ì‚p1 âˆˆ(c, 1))P( Ì‚p1 âˆˆ(c, 1))
+ P(
âˆš
n[hn( Ì‚p1) âˆ’h( Ì‚p1)] > Îµ|Ì‚p1 âˆˆ[1âˆ•n, c]))P( Ì‚p1 âˆˆ[1âˆ•n, c])
< P(
âˆš
n[hn( Ì‚p1) âˆ’h( Ì‚p1)] > Îµ|Ì‚p1 âˆˆ(c, 1)) + P( Ì‚p1 âˆˆ[1âˆ•n, c]).
By Part 1 of Lemma 5.6, P(
âˆš
n[hn( Ì‚p1) âˆ’h( Ì‚p1)] > Îµ|Ì‚p1 âˆˆ(c, 1))
pâ†’0; and
by Part 3 of Lemma 5.6, P(Ì‚p1 âˆˆ[1âˆ•n, c])
pâ†’0. Hence Î”n
pâ†’0. The result
of Lemma 5.5 is immediate from the following expression in a ï¬nite sum,
âˆš
n( Ì‚Hz âˆ’Ì‚H) = âˆ‘K
k=1
âˆš
n(hn(Ì‚pk) âˆ’h(Ì‚pk)).
â—½
Lemma 5.5 immediately gives the following corollary.
Corollary 5.3
Under the conditions of Lemma 5.4,
âˆš
n
â¡
â¢
â¢â£
â›
âœ
âœâ
Ì‚Hz(X)
Ì‚Hz(Y)
Ì‚Hz(X, Y)
â
âŸ
âŸâ 
âˆ’
â›
âœ
âœâ
H(X)
H(Y)
H(X, Y)
â
âŸ
âŸâ 
â¤
â¥
â¥â¦
L
âˆ’âˆ’âˆ’â†’MVN(0, Î£H)
(5.67)
where
Î£H = AÎ£Ağœ,
(5.68)
Î£ is as in (5.50) and A is as in (5.45) according to (5.51)â€“(5.53).
Proof of Theorem 5.6: In light of Theorem 5.5 and Slutskyâ€™s theorem, it suï¬ƒces
to show that
âˆš
n( Ì‚
MIz âˆ’Ì‚
MI)
pâ†’0. However, this is trivial since
âˆš
n( Ì‚
MIz âˆ’Ì‚
MI) =
âˆš
n( Ì‚Hz(X) âˆ’Ì‚H(X))
+
âˆš
n( Ì‚Hz(Y) âˆ’Ì‚H(Y))
âˆ’
âˆš
n( Ì‚Hz(X, Y) âˆ’Ì‚H(X, Y)).

Estimation of Information
173
Applying Lemma 5.5 to each of the three additive terms above, respectively,
gives the result of Theorem 5.6.
â—½
From Theorem 5.6 and Slutskyâ€™s theorem, Corollary 5.4 follows.
Corollary 5.4
Under the conditions of Theorem 5.6,
âˆš
n( Ì‚
MIz âˆ’MI)
Ì‚ğœ
L
âˆ’âˆ’âˆ’â†’N(0, 1),
(5.69)
where Ì‚ğœis as given in (5.62).
Corollary 5.4, like Corollary 5.2, provides a large sample statistical tool for
inference about MI, but with a qualitatively lower bias.
5.2.3
Estimation of Standardized Mutual Information
The SMI ğœ…in Deï¬nition 5.4 is a practically useful measure that rescales MI to
within interval [0, 1], particularly with the necessary and suï¬ƒcient conditions
of Theorem 5.4. In this section, several estimators of ğœ…are described. However,
ğœ…is not the only one that has the desirable properties. Since
0 â‰¤MI(X, Y) â‰¤min{H(X), H(Y)}
â‰¤
âˆš
H(X)H(Y)
â‰¤(H(X) + H(Y))âˆ•2
â‰¤max{H(X), H(Y)}
â‰¤H(X, Y),
(see Exercise 7), each of the following, assuming H(X, Y) < âˆ,
ğœ…1 =
MI(X, Y)
min{H(X), H(Y)},
ğœ…2 =
MI(X, Y)
âˆš
H(X)H(Y)
,
ğœ…3 =
MI(X, Y)
(H(X) + H(Y))âˆ•2,
ğœ…4 =
MI(X, Y)
max{H(X), H(Y)},
may be considered a SMI and satisï¬es
0 â‰¤ğœ…i â‰¤1,
where i âˆˆ{1, 2, 3, 4}. However the necessary and suï¬ƒcient condition of
Theorem 5.4 holds only for ğœ…2, ğœ…3 and ğœ…4, but not for ğœ…1. See Exercise 8. Further
discussion on these and other variants of SMI may be found in Kvalseth

174
Statistical Implications of Turingâ€™s Formula
(1987), Strehl and Ghosh (2002), Yao (2003), and Vinh, Epps, and Bailey
(2010). A detailed discussion of the estimation of various standardized mutual
information may be found in Zhang and Stewart (2016).
In this section, the plug-in estimators of ğœ…1, ğœ…2, and ğœ…3, as well as those
based on Turingâ€™s perspective, are described, and their respective asymptotic
distributional properties are given. Toward that end, consider the following
three functions of triplet (x1, x2, x3), with domain, x1 > 0, x2 > 0, and x3 > 0,
ğœ…(x1, x2, x3) = x1 + x2
x3
âˆ’1,
ğœ…2(x1, x2, x3) = x1 + x2 âˆ’x3
âˆšx1x2
,
ğœ…3(x1, x2, x3) = 2
(
1 âˆ’
x3
x1 + x2
)
.
The gradients of these functions are, respectively,
gğœ…(x1, x2, x3) =
(
1
x3
, 1
x3
, âˆ’x1 + x2
x2
3
)ğœ
,
gğœ…2(x1, x2, x3) =
(
1
(x1x2)1âˆ•2 âˆ’x2(x1 + x2 âˆ’x3)
2(x1x2)3âˆ•2
,
1
(x1x2)1âˆ•2 âˆ’x1(x1 + x2 âˆ’x3)
2(x1x2)3âˆ•2
, âˆ’
1
(x1x2)1âˆ•2
)ğœ
,
gğœ…3(x1, x2, x3) =
(
2x3
(x1 + x2)2 ,
2x3
(x1 + x2)2 , âˆ’
2
x1 + x2
,
)ğœ
.
See Exercise 13.
Let the following be the plug-in estimators of ğœ…, ğœ…2, and ğœ…3.
Ì‚ğœ…=
Ì‚H(X) + Ì‚H(Y)
Ì‚H(X, Y)
âˆ’1 =
Ì‚H(X) + Ì‚H(Y) âˆ’Ì‚H(X, Y)
Ì‚H(X, Y)
,
(5.70)
Ì‚ğœ…2 =
Ì‚H(X) + Ì‚H(Y) âˆ’Ì‚H(X, Y)
âˆšÌ‚H(X) Ì‚H(Y)
,
(5.71)
Ì‚ğœ…3 = 2
(
1 âˆ’
Ì‚H(X, Y)
Ì‚H(X) + Ì‚H(Y)
)
=
Ì‚H(X) + Ì‚H(Y) âˆ’Ì‚H(X, Y)
( Ì‚H(X) + Ì‚H(Y))âˆ•2
.
(5.72)
Theorem 5.7
Under the conditions of Lemma 5.4,
1)
âˆš
n(Ì‚ğœ…âˆ’ğœ…)
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2
Ì‚ğœ…),
2)
âˆš
n(Ì‚ğœ…2 âˆ’ğœ…2)
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2
Ì‚ğœ…2),
3)
âˆš
n(Ì‚ğœ…3 âˆ’ğœ…3)
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2
Ì‚ğœ…3),

Estimation of Information
175
where
ğœ2
Ì‚ğœ…= gğœ
ğœ…(H(X), H(Y), H(X, Y)) Î£H gğœ…(H(X), H(Y), H(X, Y)),
ğœ2
Ì‚ğœ…2 = gğœ
ğœ…2(H(X), H(Y), H(X, Y)) Î£H gğœ…2(H(X), H(Y), H(X, Y)),
ğœ2
Ì‚ğœ…3 = gğœ
ğœ…3(H(X), H(Y), H(X, Y)) Î£H gğœ…3(H(X), H(Y), H(X, Y)),
and Î£H is as in (5.55).
Proof: An application of the delta method to (5.54) completes the proof.
â—½
Corollary 5.5
Under the conditions of Theorem 5.7,
1)
âˆš
n(Ì‚ğœ…âˆ’ğœ…)
Ì‚ğœÌ‚ğœ…
L
âˆ’âˆ’âˆ’â†’N(0, 1),
2)
âˆš
n(Ì‚ğœ…2 âˆ’ğœ…2)
Ì‚ğœÌ‚ğœ…2
L
âˆ’âˆ’âˆ’â†’N(0, 1),
3)
âˆš
n(Ì‚ğœ…3 âˆ’ğœ…3)
Ì‚ğœÌ‚ğœ…3
L
âˆ’âˆ’âˆ’â†’N(0, 1),
where
Ì‚ğœ2
Ì‚ğœ…= gğœ
ğœ…( Ì‚H(X), Ì‚H(Y), Ì‚H(X, Y)) Ì‚Î£H gğœ…( Ì‚H(X), Ì‚H(Y), Ì‚H(X, Y)),
Ì‚ğœ2
Ì‚ğœ…2 = gğœ
ğœ…2( Ì‚H(X), Ì‚H(Y), Ì‚H(X, Y)) Ì‚Î£H gğœ…2( Ì‚H(X), Ì‚H(Y), Ì‚H(X, Y)),
Ì‚ğœ2
Ì‚ğœ…3 = gğœ…3( Ì‚H(X), Ì‚H(Y), Ì‚H(X, Y)) Ì‚Î£H gğœ
ğœ…3( Ì‚H(X), Ì‚H(Y), Ì‚H(X, Y)),
and Ì‚Î£H is as in (5.61).
Proof: The consistency of Ì‚ğœ2
Ì‚ğœ…, Ì‚ğœ2
Ì‚ğœ…2, and Ì‚ğœ2
Ì‚ğœ…3 and the Slutskyâ€™s theorem completes
the proof.
â—½
Let Ì‚Hz(X), Ì‚Hz(Y), and Ì‚Hz(X, Y) be as in (5.64). Let
Ì‚ğœ…z =
Ì‚Hz(X) + Ì‚Hz(Y)
Ì‚Hz(X, Y)
âˆ’1 =
Ì‚Hz(X) + Ì‚Hz(Y) âˆ’Ì‚Hz(X, Y)
Ì‚Hz(X, Y)
,
(5.73)
Ì‚ğœ…2z =
Ì‚Hz(X) + Ì‚Hz(Y) âˆ’Ì‚Hz(X, Y)
âˆš
Ì‚Hz(X) Ì‚Hz(Y)
,
(5.74)
Ì‚ğœ…3z = 2
(
1 âˆ’
Ì‚Hz(X, Y)
Ì‚Hz(X) + Ì‚Hz(Y)
)
=
Ì‚Hz(X) + Ì‚Hz(Y) âˆ’Ì‚Hz(X, Y)
( Ì‚Hz(X) + Ì‚Hz(Y))âˆ•2
.
(5.75)
Theorem 5.8
Under the conditions of Corollary 5.3,
1)
âˆš
n(Ì‚ğœ…z âˆ’ğœ…)
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2
Ì‚ğœ…z),

176
Statistical Implications of Turingâ€™s Formula
2)
âˆš
n(Ì‚ğœ…2z âˆ’ğœ…2)
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2
Ì‚ğœ…2z),
3)
âˆš
n(Ì‚ğœ…3z âˆ’ğœ…3)
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2
Ì‚ğœ…3z),
where
ğœ2
Ì‚ğœ…z = gğœ
ğœ…(H(X), H(Y), H(X, Y)) Î£H gğœ…(H(X), H(Y), H(X, Y)),
ğœ2
Ì‚ğœ…2z = gğœ
ğœ…2(H(X), H(Y), H(X, Y)) Î£H gğœ…2(H(X), H(Y), H(X, Y)),
ğœ2
Ì‚ğœ…3z = gğœ
ğœ…3(H(X), H(Y), H(X, Y)) Î£H gğœ…3(H(X), H(Y), H(X, Y)),
and Î£H is as in (5.55).
Proof: An application of the delta method to (5.67) completes the proof.
â—½
Corollary 5.6
Under the conditions of Theorem 5.8,
1)
âˆš
n(Ì‚ğœ…z âˆ’ğœ…)
Ì‚ğœÌ‚ğœ…z
L
âˆ’âˆ’âˆ’â†’N(0, 1),
2)
âˆš
n(Ì‚ğœ…z2 âˆ’ğœ…2)
Ì‚ğœÌ‚ğœ…2z
L
âˆ’âˆ’âˆ’â†’N(0, 1),
3)
âˆš
n(Ì‚ğœ…z3 âˆ’ğœ…3)
Ì‚ğœÌ‚ğœ…3z
L
âˆ’âˆ’âˆ’â†’N(0, 1),
where
Ì‚ğœ2
Ì‚ğœ…z = gğœ
ğœ…( Ì‚Hz(X), Ì‚Hz(Y), Ì‚Hz(X, Y)) Ì‚Î£H gğœ…( Ì‚Hz(X), Ì‚Hz(Y), Ì‚Hz(X, Y)),
Ì‚ğœ2
Ì‚ğœ…2z = gğœ
ğœ…2( Ì‚Hz(X), Ì‚Hz(Y), Ì‚Hz(X, Y)) Ì‚Î£H gğœ…2( Ì‚Hz(X), Ì‚Hz(Y), Ì‚Hz(X, Y)),
Ì‚ğœ2
Ì‚ğœ…3z = gğœ…3( Ì‚Hz(X), Ì‚Hz(Y), Ì‚Hz(X, Y)) Ì‚Î£H gğœ
ğœ…3( Ì‚Hz(X), Ì‚Hz(Y), Ì‚Hz(X, Y)),
and Ì‚Î£H is as in (5.61).
All the plug-in estimators of MI, ğœ…, ğœ…2, and ğœ…3 in this section are asymp-
totically eï¬ƒcient since they are all maximum likelihood estimators. All the
estimators based on Turingâ€™s perspective in this section are also asymptotically
eï¬ƒcient because each of them has the identical asymptotic variance as that of
its plug-in counterpart.
5.2.4
An Illustrative Example
A random sample of size n = 1000 adults in the United States were selected.
The favorite color (among K1 = 11 choices) and the ethnicity (among K2 = 4
choices) for each individual in the selected group were obtained. The survey
resulted in the data set in Table 5.1. Letting X be the color preference
and Y be the ethnicity of a randomly selected individual, i takes values in

Estimation of Information
177
Table 5.1 Frequency data of ethnicity and color
preference.
Black
Asian
White
Hispanic
Black
15
5
46
19
Blue
48
19
285
59
Brown
5
0
7
2
Gray
4
1
7
3
Green
15
4
99
19
Orange
0
3
13
5
Pink
1
3
26
3
Purple
20
4
85
30
Red
15
1
66
25
White
0
1
0
0
Yellow
3
4
26
4
{1,â€¦, 11 = K1} corresponding to colors Black to Yellow in the given vertical
order in Table 5.1, j takes values in {1,â€¦, 4 = K2} corresponding to ethnic
groups Black to Hispanic in the given horizontal order of the same table, and
K = K1K2 = 11 Ã— 4 = 44 assuming pi,j > 0 for every pair (i, j). The entries of
Table 5.1 are observed frequencies, fi,j, for example, f1,1 = 15.
In practice, n may not necessarily be large enough to ensure that Ì‚pi,j > 0 for
every pair of (i, j), for which pi,j > 0. In fact, the data set in Table 5.1 contains
ï¬ve such cells, (6, 1), (10, 1), (3, 2), (10, 3), and (10, 4). When this is the case,
estimating the positive deï¬nite covariance matrix Î£ = Î£(v) of (5.50) by means
of the plug-in Ì‚Î£ = Ì‚Î£(v) fails to produce a positive deï¬nite matrix, which in turn
may fail to produce a useful estimated Î£H of (5.55) by means of the plug-in
Ì‚Î£H = Ì‚A Ì‚Î£ Ì‚Ağœof (5.61), where Ì‚A is as in (5.60).
To account for such a situation, many techniques may be employed. For
example, one may choose to plug an augmented Ì‚v into Î£(v), resulting in
Ì‚Î£ = Î£(Ì‚vâˆ—), where
Ì‚vâˆ—= (Ì‚pâˆ—
1,1,â€¦, Ì‚pâˆ—
1,K2, Ì‚pâˆ—
2,1,â€¦, Ì‚pâˆ—
2,K2,â€¦, Ì‚pâˆ—
K1,1,â€¦, Ì‚pâˆ—
K1,K2âˆ’1)ğœ
where, for any pair of (i, j),
Ì‚pâˆ—
i,j = Ì‚pi,j + (1âˆ•n)1[Ì‚pi,j = 0].
(5.76)
Clearly such an augmentation does not impact any of the asymptotic properties
established above but does resolve the computational issue.
In the same conï¬guration as that of Table 5.1, the observed joint and marginal
distributions are given in Table 5.2.

178
Statistical Implications of Turingâ€™s Formula
Table 5.2 Relative frequency data of ethnicity and
color preference.
Ì‚pi,j
j = 1
j = 2
j = 3
j = 4
Ì‚pi,â‹…
i = 1
0.015
0.005
0.046
0.019
0.085
i = 2
0.048
0.019
0.285
.0059
0.411
i = 3
0.005
0.000
0.007
0.002
0.014
i = 4
0.004
0.001
0.007
0.003
0.015
i = 5
0.015
0.004
0.099
0.019
0.137
i = 6
0.000
0.003
0.013
0.005
0.021
i = 7
0.001
0.003
0.026
0.003
0.033
i = 8
0.020
0.004
0.085
0.030
0.139
i = 9
0.015
0.001
0.066
0.025
0.107
i = 10
0.000
0.001
0.000
0.000
0.001
i = 11
0.003
0.004
0.026
0.004
0.037
Ì‚pâ‹…,j
0.126
0.045
0.660
0.169
1.000
By (5.39)â€“(5.41) and (5.56),
Ì‚H(X) = 1.8061,
(5.77)
Ì‚H(Y) = 0.9753,
(5.78)
Ì‚H(X, Y) = 2.7534, and
(5.79)
Ì‚
MI = 0.0280.
(5.80)
Further by (5.70)â€“(5.72),
Ì‚ğœ…= 0.0102, Ì‚ğœ…2 = 0.0211,
and
Ì‚ğœ…3 = 0.0201.
(5.81)
Letting pi,j be enumerated as {pk; k = 1, 2,â€¦, K} by counting the probabilities
from left to right in each row, one has the K = K1K2 = 44 dimensional vector
Ì‚p = (0.015, 0.005, 0.046, 0.019, 0.048,â€¦, 0.026, 0.004)ğœ,
and the K âˆ’1 = 43 dimensional vector
Ì‚v = (0.015, 0.005, 0.046, 0.019, 0.048,â€¦, 0.026)ğœ.
Substituting the zeros in Ì‚v by 1âˆ•n = 0.001 according to (5.76), at Ì‚v10 = 0,
Ì‚v21 = 0, Ì‚v37 = 0, Ì‚v39 = 0, and Ì‚v40 = 0, one has Ì‚vâˆ—given in Table 5.3.

Estimation of Information
179
Table 5.3 Ì‚vâˆ—for data in Table 5.2.
Ì‚vâˆ—= ( 0.015,
0.005,
0.046,
0.019,
0.048,
0.019,
0.285,
0.059,
0.005,
0.001,
0.007,
0.002,
0.004,
0.001,
0.007,
0.003,
0.015,
0.004,
0.099,
0.019,
0.001,
0.003,
0.013,
0.005,
0.001,
0.003,
0.026,
0.003,
0.020,
0.004,
0.085,
0.030,
0.015,
0.001,
0.066,
0.025,
0.001,
0.001,
0.001,
0.001,
0.003,
0.004,
0.026
)ğœ
Letting Ì‚A = A(Ì‚vâˆ—) by means of (5.45), the three rows of Ì‚A are
Ì‚ağœ
1 = (
âˆ’0.8317,
âˆ’0.8317,
âˆ’0.8317,
âˆ’0.8317,
âˆ’2.4077,
âˆ’2.4077,
âˆ’2.4077,
âˆ’2.4077,
0.9719,
0.9719,
0.9719,
0.9719,
0.9029,
0.9029,
0.9029,
0.9029,
âˆ’1.3091,
âˆ’1.3091,
âˆ’1.3091,
âˆ’1.3091,
0.5664,
0.5664,
0.5664,
0.5664,
0.1144,
0.1144,
0.1144,
0.1144,
âˆ’1.3236,
âˆ’1.3236,
âˆ’1.3236,
âˆ’1.3236,
âˆ’1.0619,
âˆ’1.0619,
âˆ’1.0619,
âˆ’1.0619,
3.6109,
3.6109,
3.6109,
3.6109,
0.0000,
0.0000,
0.0000
);
Ì‚ağœ
2 = (
0.2936
1.3232
âˆ’1.3623
0.0000,
0.2936
1.3232
âˆ’1.3623
0.0000,
0.2936
1.3232
âˆ’1.3623
0.0000,
0.2936
1.3232
âˆ’1.3623
0.0000,
0.2936
1.3232
âˆ’1.3623
0.0000,
0.2936
1.3232
âˆ’1.3623
0.0000,
0.2936
1.3232
âˆ’1.3623
0.0000,
0.2936
1.3232
âˆ’1.3623
0.0000,
0.2936
1.3232
âˆ’1.3623
0.0000,
0.2936
1.3232
âˆ’1.3623
0.0000,
0.2936
1.3232
âˆ’1.3623
);

180
Statistical Implications of Turingâ€™s Formula
and
Ì‚ağœ
3 = (
âˆ’1.3216,
âˆ’0.2231,
âˆ’2.4423,
âˆ’1.5581,
âˆ’2.4849,
âˆ’1.5581,
âˆ’4.2662,
âˆ’2.6912,
âˆ’0.2231,
1.3863,
âˆ’0.5596,
0.6931,
0.0000,
1.3863,
âˆ’0.5596,
0.2877,
âˆ’1.3218,
0.0000,
âˆ’3.2088,
âˆ’1.5581,
1.3863,
0.2877,
âˆ’1.1787,
âˆ’0.2231,
1.3863,
0.2877,
âˆ’1.8718,
0.2877,
âˆ’1.6094,
0.0000,
âˆ’3.0564,
âˆ’2.0149,
âˆ’1.3218,
1.3863,
âˆ’2.8034,
âˆ’1.8326,
1.3863,
1.3863,
1.3863,
1.3863,
0.2877,
0.0000,
âˆ’1.8718
).
By (5.61) and (5.62),
Ì‚Î£H = Ì‚A Ì‚Î£ Ì‚Ağœ=
â›
âœ
âœâ
0.9407 0.0661 0.9364
0.0661 0.6751 0.7041
0.9364 0.7041 1.5927
â
âŸ
âŸâ 
,
Ì‚ğœ2 = (1, 1, âˆ’1) Ì‚Î£H(1, 1, âˆ’1)ğœ= 0.0597.
Based on the plug-in estimator of MI, an approximate 1 âˆ’ğ›¼= 90% conï¬-
dence interval for MI is
Ì‚
MI Â± zğ›¼âˆ•2
Ì‚ğœ
âˆš
n
= 0.0280 Â± 1.645
âˆš
0.0597
âˆš
1000
= (0.0153, 0.0407).
Noting
gğœ…( Ì‚H(X), Ì‚H(Y), Ì‚H(X, Y)) = (0.3632, 0.3632, âˆ’0.3669)ğœ,
gğœ…2( Ì‚H(X), Ì‚H(Y), Ì‚H(X, Y)) = (0.7477, 0.7477, âˆ’0.7535)ğœ,
gğœ…3( Ì‚H(X), Ì‚H(Y), Ì‚H(X, Y)) = (0.7119, 0.7119, âˆ’0.7191)ğœ,
and based on Corollary 5.5,
Ì‚ğœ2
Ì‚ğœ…( Ì‚H(X), Ì‚H(Y), Ì‚H(X, Y)) = 0.0078,
Ì‚ğœ2
Ì‚ğœ…2( Ì‚H(X), Ì‚H(Y), Ì‚H(X, Y)) = 0.0330,
Ì‚ğœ2
Ì‚ğœ…3( Ì‚H(X), Ì‚H(Y), Ì‚H(X, Y)) = 0.0298.
Based on the plug-in estimators of ğœ…, ğœ…2, and ğœ…3, the following are their respec-
tive approximate 1 âˆ’ğ›¼= 90% conï¬dence intervals:
Ì‚ğœ…Â± zğ›¼âˆ•2
Ì‚ğœÌ‚ğœ…
âˆš
n
= 0.0102 Â± 1.645
âˆš
0.0081
âˆš
1000
= (0.0056, 0.0148),
Ì‚ğœ…2 Â± zğ›¼âˆ•2
Ì‚ğœÌ‚ğœ…2
âˆš
n
= 0.0211 Â± 1.645
âˆš
0.0341
âˆš
1000
= (0.0116, 0.0306),

Estimation of Information
181
Ì‚ğœ…3 Â± zğ›¼âˆ•2
Ì‚ğœÌ‚ğœ…3
âˆš
n
= 0.0201 Â± 1.645
âˆš
0.0298
âˆš
1000
= (0.0111, 0.0291).
On the other hand, the corresponding estimators based on Turingâ€™s
perspective are
Ì‚Hz(X) = 1.8112,
(5.82)
Ì‚Hz(Y) = 0.9768,
(5.83)
Ì‚Hz(X, Y) = 2.7731, and
(5.84)
Ì‚
MIz = 0.0148.
(5.85)
Further by (5.70)â€“(5.72), but with Ì‚Hz(X), Ì‚Hz(Y), and Ì‚Hz(X, Y), in places of
Ì‚H(X), Ì‚H(Y), and Ì‚H(X, Y), respectively,
Ì‚ğœ…z = 0.0054,
Ì‚ğœ…2z = 0.0112,
and
Ì‚ğœ…3z = 0.0106.
(5.86)
Noting
gğœ…( Ì‚Hz(X), Ì‚Hz(Y), Ì‚Hz(X, Y)) = (0.3606, 0.3606, âˆ’0.3625)ğœ,
gğœ…2( Ì‚Hz(X), Ì‚Hz(Y), Ì‚Hz(X, Y)) = (0.7488, 0.7488, âˆ’0.7518)ğœ,
gğœ…3( Ì‚Hz(X), Ì‚HzY), Ì‚Hz(X, Y)) = (0.7136, 0.7136, âˆ’0.7174)ğœ,
and based on Corollary 5.6,
Ì‚ğœ2
Ì‚ğœ…z( Ì‚Hz(X), Ì‚Hz(Y), Ì‚Hz(X, Y)) = 0.0077,
Ì‚ğœ2
Ì‚ğœ…2z( Ì‚Hz(X), Ì‚Hz(Y), Ì‚Hz(X, Y)) = 0.0333,
Ì‚ğœ2
Ì‚ğœ…3z( Ì‚Hz(X), Ì‚Hz(Y), Ì‚Hz(X, Y)) = 0.0302.
The approximate 1 âˆ’ğ›¼= 90% conï¬dence intervals for MI, ğœ…, ğœ…2, and ğœ…3 based
on Turingâ€™s perspective are
Ì‚
MIz Â± zğ›¼âˆ•2
Ì‚ğœ
âˆš
n
= 0.0148 Â± 1.645
âˆš
0.0597
âˆš
1000
= (0.0021, 0.0275),
Ì‚ğœ…z Â± zğ›¼âˆ•2
Ì‚ğœÌ‚ğœ…z
âˆš
n
= 0.0054 Â± 1.645
âˆš
0.0077
âˆš
1000
= (0.0008, 0.0100),
Ì‚ğœ…2z Â± zğ›¼âˆ•2
Ì‚ğœÌ‚ğœ…2z
âˆš
n
= 0.0112 Â± 1.645
âˆš
0.0333
âˆš
1000
= (0.0017, 0.0207),
Ì‚ğœ…3z Â± zğ›¼âˆ•2
Ì‚ğœÌ‚ğœ…3z
âˆš
n
= 0.0106 Â± 1.645
âˆš
0.0302
âˆš
1000
= (0.0016, 0.0196).
In addition to the illustration of the example, at least two points are worth
noting. First, the sample data suggest a very low level of association between

182
Statistical Implications of Turingâ€™s Formula
ethnicity and color preference, as measured by MI and its derivatives, and yet
the association is supported with fairly strong statistical evidence.
Second, it may be of interesting to note again that the plug-in estimator of
entropy, Ì‚H, has a much larger negative bias compared to Ì‚Hz, which also has a
negative bias. Consequently, the entropy estimators in the left block are con-
sistently lower than those in the right block below.
Ë†H(X)
1.8061
Ë†H(Y )
0.9753
Ë†H(X, Y )
2.7534
Ë†Hz(X)
1.8112
Ë†Hz(Y )
0.9768
Ë†Hz(X, Y )
2.7731
On the other hand, due to the dominance of the negative bias of the plug-in
estimator Ì‚H(X, Y), Ì‚
MI has a pronounced positive bias as numerically evidenced
by the following blocks in comparison.

MI
0.0280
Ë†Îº
0.0102
Ë†Îº2
0.0211
Ë†Îº3
0.0201

MIz
0.0148
Ë†Îºz
0.0054
Ë†Îº2z
0.0112
Ë†Îº3z
0.0106
For more examples regarding the over-estimation of mutual information
using Ì‚
MI and Ì‚
MIz, readers may wish to refer to Zhang and Zheng (2015).
5.3
Estimation of Kullbackâ€“Leibler Divergence
Let p = {pk} and q = {qk} be two discrete probability distributions on a same
ï¬nite alphabet, ğ’³= {ğ“k; k = 1,â€¦, K}, where K â‰¥2 is a ï¬nite integer. Suppose
it is of interest to estimate the Kullbackâ€“Leibler divergence, also known as rel-
ative entropy,
D = D(p||q)
(5.87)
=
K
âˆ‘
k=1
pk ln( pkâˆ•qk)
=
K
âˆ‘
k=1
pk ln( pk) âˆ’
K
âˆ‘
k=1
pk ln(qk).
Here and throughout, the standard conventions that 0 log (0âˆ•q) = 0 if q â‰¥0
and p ln(pâˆ•0) = âˆif p > 0 are adopted. The Kullbackâ€“Leibler divergence is
a measure of the diï¬€erence between two probability distributions, which was

Estimation of Information
183
ï¬rst introduced by Kullback and Leibler (1951), and is an important measure
of information in information theory. This measure is not a metric since it
does not satisfy the triangle inequality and it is not symmetric. A symmetric
modiï¬cation of Kullbackâ€“Leibler divergence is considered in Section 5.3.3
below.
Assume that two independent iid samples of sizes n and m are to be available,
respectively, according to two unknown distributions p and q.
Let
{
X1,
X2,
Â· Â· Â· ,
XK
}
{
Y1,
Y2,
Â· Â· Â· ,
YK
}
be the sequences of observed frequencies of letters {ğ“1, ğ“2,â€¦, ğ“K} in the two
samples, respectively, and let
Ì‚p = {Ì‚pk} = {
X1âˆ•n,
X2âˆ•n,
Â· Â· Â· ,
XKâˆ•n
}
Ì‚q = {Ì‚qk} = {
Y1âˆ•m,
Y2âˆ•m,
Â· Â· Â· ,
YKâˆ•m
}
be the sequences of corresponding relative frequencies.
For simplicity, the following is further imposed.
Condition 5.1
The probability distributions, p and q, and the observed sam-
ple distributions, Ì‚p and Ì‚q, satisfy
A1. pk > 0 and qk > 0 for each k = 1,â€¦, K, and
A2. there exists a ğœ†âˆˆ(0, âˆ) such that nâˆ•m â†’ğœ†, as n â†’âˆ.
Note that it is not assumed that K is known, only that it is some ï¬nite integer
greater than or equal to two.
Perhaps the most intuitive estimator of D(p||q) is the â€œplug-inâ€ estimator
given by
Ì‚D = Ì‚D(p||q) =
K
âˆ‘
k=1
Ì‚pk ln(Ì‚pk) âˆ’
K
âˆ‘
k=1
Ì‚pk ln(Ì‚qk).
(5.88)
In Section 5.3.1, it is shown that this estimator is consistent and asymptotically
normal, but with an inï¬nite bias. Then a modiï¬cation of this estimator is
introduced and is shown that it is still consistent and asymptotically normal,
but now with a ï¬nite bias. It is also shown that this bias decays no faster than
ğ’ª(1âˆ•n). In Section 5.3.2, the following estimator of D(p||q),
Ì‚Dâ™¯= Ì‚Dâ™¯(p||q)
=
K
âˆ‘
k=1
Ì‚pk
[mâˆ’Yk
âˆ‘
v=1
1
v
v
âˆ
j=1
(
1 âˆ’
Yk
m âˆ’j + 1
)
âˆ’
nâˆ’Xk
âˆ‘
v=1
1
v
v
âˆ
j=1
(
1 âˆ’Xk âˆ’1
n âˆ’j
)]
,
(5.89)
based on Turingâ€™s perspective and proposed by Zhang and Grabchak (2014),
is discussed and is shown to have asymptotic consistency and normality.

184
Statistical Implications of Turingâ€™s Formula
This estimator is also shown to have a bias decaying exponentially fast in sam-
ple sizes n and m. Finally, in Section 5.3.3, the estimation of the symmetrized
Kullbackâ€“Leibler divergence is discussed.
Before proceeding let
A =
K
âˆ‘
k=1
pk ln(pk)
and
B = âˆ’
K
âˆ‘
k=1
pk ln(qk),
(5.90)
and note that D(p||q) = A + B. The ï¬rst term, A, is the negative of the entropy
of the distribution of p. In the subsequent text of this section, an estimator
of D(p||q) is occasionally expressed as the sum of an estimator of A and an
estimator of B.
5.3.1
The Plug-In Estimator
In this section, it is shown that Ì‚D, the plug-in estimator given in (5.88), is a
consistent and asymptotically normal estimator of D, but with an inï¬nite bias.
Deï¬ne the (2K âˆ’2)-dimensional vectors
v = (p1,â€¦, pKâˆ’1, q1,â€¦, qKâˆ’1)ğœand Ì‚v = (Ì‚p1,â€¦, Ì‚pKâˆ’1, Ì‚q1,â€¦, Ì‚qKâˆ’1)ğœ,
and note that Ì‚v
pâ†’v as n â†’âˆ. Moreover, by the multivariate normal approxi-
mation to the multinomial distribution
âˆš
n(Ì‚v âˆ’v)
L
âˆ’âˆ’âˆ’â†’MVN(0, Î£(v)),
(5.91)
where Î£(v) is the (2K âˆ’2) Ã— (2K âˆ’2) covariance matrix given by
Î£(v) =
(
Î£1(v) 0
0
Î£2(v)
)
.
(5.92)
Here Î£1(v) and Î£2(v) are (K âˆ’1) Ã— (K âˆ’1) matrices given by
Î£1(v) =
â›
âœ
âœ
âœâ
p1(1 âˆ’p1)
âˆ’p1p2
Â· Â· Â·
âˆ’p1pKâˆ’1
âˆ’p2 p1
p2(1 âˆ’p2) Â· Â· Â·
âˆ’p2 pKâˆ’1
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
âˆ’pKâˆ’1p1
âˆ’pKâˆ’1p2
Â· Â· Â· pKâˆ’1(1 âˆ’pKâˆ’1)
â
âŸ
âŸ
âŸâ 
and
Î£2(v) = ğœ†
â›
âœ
âœ
âœâ
q1(1 âˆ’q1)
âˆ’q1q2
Â· Â· Â·
âˆ’q1qKâˆ’1
âˆ’q2q1
q2(1 âˆ’q2) Â· Â· Â·
âˆ’q2qKâˆ’1
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
âˆ’qKâˆ’1q1
âˆ’qKâˆ’1q2
Â· Â· Â· qKâˆ’1(1 âˆ’qKâˆ’1)
â
âŸ
âŸ
âŸâ 
.

Estimation of Information
185
Let
G(v) =
Kâˆ’1
âˆ‘
k=1
pk(ln pk âˆ’ln qk)
+
(
1 âˆ’
Kâˆ’1
âˆ‘
k=1
pk
) [
ln
(
1 âˆ’
Kâˆ’1
âˆ‘
k=1
pk
)
âˆ’ln
(
1 âˆ’
Kâˆ’1
âˆ‘
k=1
qk
)]
(5.93)
and
g(v) âˆ¶= âˆ‡G(v)
=
(
âˆ‚
âˆ‚p1
G(v),â€¦,
âˆ‚
âˆ‚pKâˆ’1
G(v), âˆ‚
âˆ‚q1
G(v),â€¦,
âˆ‚
âˆ‚qKâˆ’1
G(v)
)ğœ
.
(5.94)
For each j, j = 1,â€¦, K âˆ’1,
âˆ‚
âˆ‚pj
G(v) = ln pj âˆ’ln qj âˆ’
[
ln
(
1 âˆ’
Kâˆ’1
âˆ‘
k=1
pk
)
âˆ’ln
(
1 âˆ’
Kâˆ’1
âˆ‘
k=1
qk
)]
= ln pj âˆ’ln qj âˆ’(ln pK âˆ’ln qK)
= ln
pj
qj
âˆ’ln pK
qK
âˆ’
pj
qj
+ 1 âˆ’âˆ‘Kâˆ’1
k=1 pk
1 âˆ’âˆ‘Kâˆ’1
k=1 qk
= âˆ’
pj
qj
+ pK
qK
.
The delta method immediately gives the following result.
Theorem 5.9
Provided that gğœ(v)Î£(v)g(v) > 0,
âˆš
n( Ì‚D âˆ’D)
âˆš
gğœ(v)Î£(v)g(v)
L
âˆ’âˆ’âˆ’â†’N(0, 1).
(5.95)
By the continuous mapping theorem, Î£(Ì‚v) is a consistent estimator of Î£(v)
and g(Ì‚v) is a consistent estimator of g(v). Corollary 5.7 follows from Slutskyâ€™s
theorem.
Corollary 5.7
Provided that gğœ(v)Î£(v)g(v) > 0,
âˆš
n( Ì‚D âˆ’D)
Ì‚ğœÌ‚D
L
âˆ’âˆ’âˆ’â†’N(0, 1),
(5.96)
where Ì‚ğœ2
Ì‚D = gğœ(Ì‚v)Î£(Ì‚v)g(Ì‚v).
Although Ì‚D is a consistent and asymptotically normal estimator of D, it has
an inï¬nite bias. To see this, consider P({Ì‚pk > 0} âˆ©{Ì‚qk = 0}) > 0 for every k,
k = 1,â€¦, K. For each of such k, E(Ì‚pk ln(Ì‚qk)) = âˆ. The problem is clearly caused

186
Statistical Implications of Turingâ€™s Formula
by the fact that Ì‚qk may be zero even though qk > 0. To deal with this minor issue,
the following augmentation is added:
Ì‚qâˆ—
k = Ì‚qk + 1[Yk = 0]
m
,
(5.97)
where k = 1,â€¦, K.
5.3.2
Properties of the Augmented Plug-In Estimator
Deï¬ne an augmented plug-in estimator of D by
Ì‚Dâˆ—=
K
âˆ‘
k=1
Ì‚pk ln( Ì‚pk) âˆ’
K
âˆ‘
k=1
Ì‚pk ln(Ì‚qâˆ—
k) =âˆ¶Ì‚A + Ì‚Bâˆ—.
(5.98)
It is shown that Ì‚Dâˆ—is a consistent and asymptotically normal estimator of D,
with a bias that decays no faster than ğ’ª(1âˆ•n).
Theorem 5.10
Provided that gğœ(v)Î£(v)g(v) > 0,
âˆš
n( Ì‚Dâˆ—âˆ’D)
âˆš
gğœ(v)Î£(v)g(v)
L
âˆ’âˆ’âˆ’â†’N(0, 1).
(5.99)
Proof: In light of Theorem 5.9, it suï¬ƒces to show that
âˆš
n( Ì‚Dâˆ—âˆ’Ì‚D)
pâ†’0. The
fact that, for any Îµ > 0,
P(|
âˆš
n( Ì‚Dâˆ—âˆ’Ì‚D)| > Îµ) â‰¤P
( K
â‹ƒ
k=1
[Yk = 0]
)
â‰¤
K
âˆ‘
k=1
P(Yk = 0)
=
K
âˆ‘
k=1
(1 âˆ’qk)m â†’0,
gives the result.
â—½
Arguments similar to these for Corollary 5.7 give the following corollary.
Corollary 5.8
Provided that gğœ(v)Î£(v)g(v) > 0,
âˆš
n( Ì‚Dâˆ—âˆ’D)
Ì‚ğœÌ‚Dâˆ—
L
âˆ’âˆ’âˆ’â†’N(0, 1),
(5.100)
where Ì‚ğœ2
Ì‚Dâˆ—= gğœ(Ì‚vâˆ—)Î£(Ì‚vâˆ—)g(Ì‚vâˆ—) and Ì‚vâˆ—= (Ì‚p1,â€¦, Ì‚pKâˆ’1, Ì‚qâˆ—
1,â€¦, Ì‚qâˆ—
Kâˆ’1)ğœ.
Example 5.10
A commonly used special case is when K = 2. In this case, p
and q are both Bernoulli distributions with probabilities of success being p and
q (note that q â‰ 1 âˆ’p), respectively. In this case,
v = (p, q)ğœ,

Estimation of Information
187
Î£(v) =
(
p(1 âˆ’p)
0
0
ğœ†q(1 âˆ’q)
)
,
g(v) =
(
ln
(p(1 âˆ’q)
q(1 âˆ’p)
)
,
q âˆ’p
q(1 âˆ’q)
)ğœ
,
and ğœ2 âˆ¶= gğœ(v)Î£(v)g(v) reduces to
ğœ2 = p(1 âˆ’p)
[
ln
(p(1 âˆ’q)
q(1 âˆ’p)
)]2
+ ğœ†(p âˆ’q)2
q(1 âˆ’q) .
(5.101)
By Theorems 5.9 and 5.10, it follows that
âˆš
n( Ì‚D âˆ’D)
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2)
and
âˆš
n( Ì‚Dâˆ—âˆ’D)
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2).
Next, consider the bias of Ì‚Dâˆ—. In Miller (1955) (see also Paninski (2003) for a
more formal treatment), it is shown that
E( Ì‚A) âˆ’A â‰¥0
and
E( Ì‚A) âˆ’A = ğ’ª(1âˆ•n).
(5.102)
It will be shown below that, for suï¬ƒciently large n,
E(Ì‚Bâˆ—) âˆ’B â‰¥0
and
E(Ì‚Bâˆ—) âˆ’B â‰¥ğ’ª(1âˆ•n).
(5.103)
Once this is established, so is the following theorem.
Theorem 5.11
For a suï¬ƒciently large n,
E( Ì‚Dâˆ—) âˆ’D â‰¥0
and
E( Ì‚Dâˆ—) âˆ’D â‰¥ğ’ª(1âˆ•n).
(5.104)
Before proving (5.103), and hence the theorem, the following result for
general â€œplug-inâ€ estimators of B is ï¬rst given.
Lemma 5.7
Let Ìƒqk be any estimator of qk such that Ìƒqk is independent of Ì‚pk and
0 < Ìƒqk â‰¤2 a.s. for all k = 1,â€¦, K. If ck = E(Ìƒqk) and
ÌƒB = âˆ’
K
âˆ‘
k=1
Ì‚pk ln Ìƒqk
then
E(ÌƒB) âˆ’B =
K
âˆ‘
k=1
pk
[
ln(qkâˆ•ck) + 1
2Var(Ìƒqk) + Rk
]
,
where
Rk =
âˆ
âˆ‘
v=3
1
vE[(1 âˆ’Ìƒqk)v âˆ’(1 âˆ’ck)v)].
If, in addition, 0 < Ìƒqk â‰¤1 a.s., then Rk â‰¥0 a.s.

188
Statistical Implications of Turingâ€™s Formula
Proof: By the Taylor expansion of the logarithm
E(ln(ck) âˆ’ln(Ìƒqk)) =
âˆ
âˆ‘
v=1
1
vE((1 âˆ’Ìƒqk)v âˆ’(1 âˆ’ck)v) = 1
2Var(Ìƒqk) + Rk.
Since Ì‚pk is an unbiased estimator of pk and it is independent of Ìƒqk
E(ÌƒB) âˆ’B =
K
âˆ‘
k=1
pkE(ln(qk) âˆ’ln(ck) + ln(ck) âˆ’ln(Ìƒqk))
=
K
âˆ‘
k=1
pk
(
ln(qkâˆ•ck) + 1
2Var(Ìƒqk) + Rk
)
.
If 0 < Ìƒqk â‰¤1, then Jensenâ€™s inequality implies that
E((1 âˆ’Ìƒqk)v) â‰¥(E(1 âˆ’Ìƒqk))v = (1 âˆ’ck)v,
and thus Rk â‰¥0.
â—½
Proof of Theorem 5.11: It suï¬ƒces to show that both inequalities in (5.103)
are
true.
Note
that
Yk âˆ¼Bin(m, qk)
and
ck âˆ¶= E(Ì‚qâˆ—
k) = qk + ek,
where
ek = (1 âˆ’qk)mâˆ•m. By Lemma 5.7,
E(Ì‚Bâˆ—
n) âˆ’B =
K
âˆ‘
k=1
pk
[
ln
(
qk
qk + ek
)
+ 1
2Var(Ì‚qâˆ—
k)
+
âˆ
âˆ‘
v=3
1
vE((1 âˆ’Ì‚qâˆ—
k)v âˆ’(1 âˆ’qk âˆ’ek)v)
]
.
Note that
1
2Var(Ì‚qâˆ—
k) = Var(Yk) + Var(1[Yk = 0]) + 2Cov(Yk, 1[Yk = 0])
2m2
= qk(1 âˆ’qk)
2m
+
1
2m2
[(1 âˆ’qk)m âˆ’(1 âˆ’qk)2m âˆ’2mqk(1 âˆ’qk)m]
= qk(1 âˆ’qk)
2m
+ ek
2m âˆ’
e2
k
2 âˆ’qkek,
and for suï¬ƒciently large m, by the Taylor expansion of the logarithm,
ln
(
qk
qk + ek
)
= âˆ’ln(1 + ekâˆ•qk) = âˆ’ekâˆ•qk + rk,
where the remainder term rk â‰¥0 by properties of alternating series. This means
that, for a suï¬ƒciently large m,
E(Ì‚Bâˆ—) âˆ’B =
K
âˆ‘
k=1
pk
(
âˆ’ek(qk + 1âˆ•qk) + rk + qk(1 âˆ’qk)
2m
+ ek
2m âˆ’
e2
k
2
+
âˆ
âˆ‘
v=3
1
vE((1 âˆ’Ìƒqk)v âˆ’(1 âˆ’qk âˆ’ek)v)
)
,

Estimation of Information
189
where the only negative terms are âˆ’ek(qk + 1âˆ•qk) and âˆ’e2
kâˆ•2. But
ek
2m âˆ’
e2
k
2 â‰¥0
and for suï¬ƒciently large m
qk(1 âˆ’qk)
2m
âˆ’ek(qk + 1âˆ•qk) = qk(1 âˆ’qk)
4m
+ ek(qk + 1âˆ•qk)
(
qk(1 âˆ’qk)
4(qk + 1âˆ•qk)mek
âˆ’1
)
â‰¥qk(1 âˆ’qk)
4m
,
where the ï¬nal inequality follows by the fact that limmâ†’âˆmek = 0. Thus, for
suï¬ƒciently large m
E(Ì‚Bâˆ—
n) âˆ’B â‰¥
1
4m
K
âˆ‘
k=1
pkqk(1 âˆ’qk) = ğ’ª(1âˆ•m) = ğ’ª(1âˆ•n),
which completes the proof.
â—½
5.3.3
Estimation in Turingâ€™s Perspective
In this section, the estimator, Ì‚Dâ™¯, of D as given in (5.89) is motivated, and is
shown to have asymptotic normality and a bias decaying exponentially fast in
sample size. Recall that D(p||q) = A + B. The estimator of D(p||q) is derived by
that of A and that of B separately.
Since A is the negative of the entropy of p, it can be estimated by the negative
of an entropy estimator, speciï¬cally the negative of the estimator based on
Turingâ€™s perspective, âˆ’Ì‚Hz, which has exponentially decaying bias. This leads
to the estimator
Ì‚Aâ™¯= âˆ’
nâˆ’1
âˆ‘
v=1
1
v Z1,v,
(5.105)
where
Z1,v = n1+v[n âˆ’(1 + v)]!
n!
K
âˆ‘
k=1
[
Ì‚pk
vâˆ’1
âˆ
j=0
(
1 âˆ’Ì‚pk âˆ’j
n
)]
.
(5.106)
Let B1 âˆ¶= E( Ì‚Aâ™¯) âˆ’A. It is demonstrated in Chapter 3 that
0 â‰¤B1 =
âˆ
âˆ‘
v=n
1
v
K
âˆ‘
k=1
pk(1 âˆ’pk)v â‰¤ğ’ª(nâˆ’1(1 âˆ’pâˆ§)n),
(5.107)
and therefore the bias decays exponentially fast in n.

190
Statistical Implications of Turingâ€™s Formula
Remark 5.2
The product term in (5.106) assumes a value of 0 when
v â‰¥n âˆ’Xk + 1. This is because if v â‰¥n âˆ’Xk + 1, then when j = n âˆ’Xk one has
(1 âˆ’Ì‚pk âˆ’jâˆ•n) = 0. Combining this observation with some basic algebra gives
Ì‚Aâ™¯= âˆ’
K
âˆ‘
k=1
Ì‚pk
nâˆ’Xk
âˆ‘
v=1
1
v
v
âˆ
j=1
(
1 âˆ’Xk âˆ’1
n âˆ’j
)
.
(5.108)
See Exercise 9.
Next an estimator of B is derived. By the Taylor expansion of the logarithmic
function
B =
m
âˆ‘
v=1
1
v
K
âˆ‘
k=1
pk(1 âˆ’qk)v +
âˆ
âˆ‘
v=m+1
1
v
K
âˆ‘
k=1
pk(1 âˆ’qk)v
=âˆ¶ğœ‚m + B2,m,
(5.109)
where m is the size of the second sample.
First, for each v, v = 1,â€¦, m, an unbiased estimator of (1 âˆ’qk)v is derived.
For each ï¬xed k, k = 1,â€¦, K, and each ï¬xed v, v = 1,â€¦, m, consider a
subsample of size v of the sample of size-m from q. Let {Yâˆ˜
1 ,â€¦, Yâˆ˜
K} and
{Ì‚qâˆ˜
1,â€¦, Ì‚qâˆ˜
K} be the counts and the relative frequencies of the subsample.
Note that E(1[Yâˆ˜
k = 0]) = (1 âˆ’qk)v, thus 1[Yâˆ˜
k = 0] is an unbiased estimator
of (1 âˆ’qk)v. Since there are
(
m
v
)
diï¬€erent subsamples of size v â‰¤m from the
sample of size m, the U-statistic
Uk,v =
(m
v
)âˆ’1 âˆ‘
âˆ—
1[Yâˆ˜
k = 0],
(5.110)
where the sum, âˆ‘
âˆ—, is taken over all possible subsamples of size v â‰¤m, is also
an unbiased estimator of (1 âˆ’qk)v. Note that âˆ‘
âˆ—1[Yâˆ˜
k = 0] is simply counting
the number of subsamples of size v in which ğ“k, the kth letter of the alphabet,
is missing. This count can be re-expressed and summarized in the following
cases:
1) if v â‰¤m âˆ’Yk, the count is
(m âˆ’Yk
v
)
= mv
v!
vâˆ’1
âˆ
j=0
(
1 âˆ’Ì‚qk âˆ’j
m
)
,
(5.111)
2) if v â‰¥m âˆ’Yk + 1, the count is zero.
Since, in the second case,
vâˆ’1
âˆ
j=0
(
1 âˆ’Ì‚qk âˆ’j
m
)
= 0,

Estimation of Information
191
a general form for both cases becomes
Uk,v =
(m
v
)âˆ’1 mv
v!
vâˆ’1
âˆ
j=0
(
1 âˆ’Ì‚qk âˆ’j
m
)
=
vâˆ’1
âˆ
j=0
(
1 âˆ’
Yk
m âˆ’j
)
.
(5.112)
Noting that Ì‚pk is an unbiased estimator of pk and it is independent of Uk,v, for
each v, 1 â‰¤v â‰¤m,
K
âˆ‘
k=1
Ì‚pkUk,v
is an unbiased estimator of âˆ‘K
k=1 pk(1 âˆ’qk)v.
This leads to an estimator of B given by
Ì‚Bâ™¯=
m
âˆ‘
v=1
1
v
K
âˆ‘
k=1
Ì‚pkUk,v.
(5.113)
By construction E(Ì‚Bâ™¯) = ğœ‚m, and by (5.109) the bias B2,m = B âˆ’E(Ì‚Bâ™¯) satisï¬es
0 â‰¤B2,m â‰¤
1
m + 1
âˆ
âˆ‘
v=m+1
(1 âˆ’qâˆ§)v = (1 âˆ’qâˆ§)m+1
(m + 1)qâˆ§
,
(5.114)
where qâˆ§= min{q1,â€¦, qK}. Thus, the bias of Ì‚Bâ™¯decays exponentially in m, and
hence in n.
Combining the fact that the product term in (5.112) assumes a value of 0 for
any v â‰¥m âˆ’Yk + 1 and some basic algebra shows that
Ì‚Bâ™¯=
K
âˆ‘
k=1
Ì‚pk
mâˆ’Yk
âˆ‘
v=1
1
v
v
âˆ
j=1
(
1 âˆ’
Yk
m âˆ’j + 1
)
.
(5.115)
Let an estimator of D(p||q) be given by
Ì‚Dâ™¯= Ì‚Aâ™¯+ Ì‚Bâ™¯.
(5.116)
Combining (5.108) and (5.115), it can be shown that this estimator is as in
(5.89). By (5.107) and (5.114), the bias of Ì‚Dâ™¯
n is given by
E
(
Ì‚Dâ™¯
n âˆ’D
)
= B1,n âˆ’B2,m
=
K
âˆ‘
k=1
pk
( âˆ
âˆ‘
v=n
1
v(1 âˆ’pk)v âˆ’
âˆ
âˆ‘
v=m+1
1
v(1 âˆ’qk)v
)
.
(5.117)
Since the bias in parts decays exponentially fast, the bias in whole decays at
least exponentially as well. In particular when the distributions and the sample
sizes are similar, the bias tends to be smaller.
Two normal laws of Ì‚Dâ™¯are given in Theorem 5.12 and Corollary 5.9.

192
Statistical Implications of Turingâ€™s Formula
Theorem 5.12
Provided that gğœ(v)Î£(v)g(v) > 0,
âˆš
n( Ì‚Dâ™¯âˆ’D)
âˆš
gğœ(v)Î£(v)g(v)
L
âˆ’âˆ’âˆ’â†’N(0, 1).
Corollary 5.9
Provided that gğœ(v)Î£(v)g(v) > 0,
âˆš
n( Ì‚Dâ™¯âˆ’D)
Ì‚ğœÌ‚Dâ™¯
L
âˆ’âˆ’âˆ’â†’N(0, 1),
where Ì‚ğœ2
Ì‚Dâ™¯= gğœ(Ì‚vâˆ—)Î£(Ì‚vâˆ—)g(Ì‚vâˆ—) and Ì‚vâˆ—is as in Corollary 5.8.
Given Theorem 5.12, the proof of Corollary 5.9 is similar to that of
Corollary 5.7.
Toward proving Theorem 5.12, Lemmas 5.8 and 5.9 are needed.
Lemma 5.8
If 0 < a < b < 1, then for any integer m â‰¥0,
bm âˆ’am â‰¤mbmâˆ’1(b âˆ’a).
Proof: Noting that f (x) = xm is a convex function on the interval (0, 1) and
f â€²(b) = mbmâˆ’1, the result follows immediately.
â—½
Lemma 5.9
If Y âˆ¼Bin(n, p) and X = Y + 1[Y = 0], then
lim sup
nâ†’âˆ
E(nâˆ•X) < âˆ.
Proof: Let
hn(x) =
x + 1
x(n âˆ’x)
for x = 1, 2,â€¦, n âˆ’1. Note that hn attains its maximum at a value xn, which is
either âŒŠ
âˆš
n + 1âŒ‹âˆ’1 or âŒŠ
âˆš
n + 1âŒ‹(here âŒŠâ‹…âŒ‹is the ï¬‚oor of a nonnegative value).
Clearly xn = ğ’ª(
âˆš
n) and hn(xn) = ğ’ª(nâˆ’1). Therefore,
E(nâˆ•X) = n(1 âˆ’p)n + n
n
âˆ‘
x=1
1
x
(n
x
)
px(1 âˆ’p)nâˆ’x
= n(1 âˆ’p)n + pn
+ n
nâˆ’1
âˆ‘
x=1
n!
(x + 1)!(n âˆ’x âˆ’1)!
x + 1
x(n âˆ’x)px(1 âˆ’p)nâˆ’x
â‰¤n(1 âˆ’p)n + pn
+ nhn(xn)1 âˆ’p
p
nâˆ’1
âˆ‘
x=1
n!
(x + 1)![n âˆ’(x + 1)]!px+1(1 âˆ’p)nâˆ’(x+1)

Estimation of Information
193
= n(1 âˆ’p)n + pn + nhn(xn)1 âˆ’p
p
n
âˆ‘
x=2
n!
x!(n âˆ’x)!px(1 âˆ’p)nâˆ’x
= n(1 âˆ’p)n + pn + nhn(xn)1 âˆ’p
p
[1 âˆ’(1 âˆ’p)n âˆ’np(1 âˆ’p)nâˆ’1].
From here, the result follows.
â—½
A proof of Theorem 5.12 is given in the appendix of this chapter.
5.3.4
Symmetrized Kullbackâ€“Leibler Divergence
The symmetrized Kullbackâ€“Leibler divergence is deï¬ned to be
S = S(p, q) = 1
2(D(p||q) + D(q||p))
(5.118)
= 1
2
( K
âˆ‘
k=1
pk ln pk âˆ’
K
âˆ‘
k=1
pk ln qk
)
+ 1
2
( K
âˆ‘
k=1
qk ln qk âˆ’
K
âˆ‘
k=1
qk ln pk
)
.
The plug-in estimator of S(p, q) is deï¬ned by
Ì‚Sn = Ì‚Sn(p, q) = 1
2
( Ì‚Dn(p||q) + Ì‚Dm(q||p))
(5.119)
= 1
2
( K
âˆ‘
k=1
Ì‚pk ln Ì‚pk âˆ’
K
âˆ‘
k=1
Ì‚pk ln Ì‚qk
)
+ 1
2
( K
âˆ‘
k=1
Ì‚qk ln Ì‚qk âˆ’
K
âˆ‘
k=1
Ì‚qk ln Ì‚pk
)
,
and the augmented plug-in estimator of S(p, q) is deï¬ned by
Ì‚Sâˆ—
n = Ì‚Sâˆ—
n(p, q) = 1
2
( Ì‚Dâˆ—
n(p||q) + Ì‚Dâˆ—
m(q||p))
(5.120)
= 1
2
( K
âˆ‘
k=1
Ì‚pk ln Ì‚pk âˆ’
K
âˆ‘
k=1
Ì‚pk ln Ì‚qâˆ—
k
)
+ 1
2
( K
âˆ‘
k=1
Ì‚qk ln Ì‚qk âˆ’
K
âˆ‘
k=1
Ì‚qk ln Ì‚pâˆ—
k
)
where qâˆ—
k is as in (5.97) and
Ì‚pâˆ—
k = Ì‚pk + 1[xk = 0]âˆ•n
for k = 1,â€¦, K.
Let Gs(v) be a function such that
2Gs(v) =
Kâˆ’1
âˆ‘
k=1
pk(ln pk âˆ’ln qk)
+
(
1 âˆ’
Kâˆ’1
âˆ‘
k=1
pk
) [
ln
(
1 âˆ’
Kâˆ’1
âˆ‘
k=1
pk
)
âˆ’ln
(
1 âˆ’
Kâˆ’1
âˆ‘
k=1
qk
)]
+
Kâˆ’1
âˆ‘
k=1
qk(ln qk âˆ’ln pk)

194
Statistical Implications of Turingâ€™s Formula
+
(
1 âˆ’
Kâˆ’1
âˆ‘
k=1
qk
) [
ln
(
1 âˆ’
Kâˆ’1
âˆ‘
k=1
qk
)
âˆ’ln
(
1 âˆ’
Kâˆ’1
âˆ‘
k=1
pk
)]
and let gs(v) = âˆ‡Gs(v).
For each j, j = 1,â€¦, K âˆ’1,
âˆ‚
âˆ‚pj
Gs(v) = 1
2
(
ln
pj
qj
âˆ’ln pK
qK
)
âˆ’1
2
(qj
pj
âˆ’qK
pK
)
and
âˆ‚
âˆ‚qj
Gs(v) = 1
2
(
ln
qj
pj
âˆ’ln qK
pK
)
âˆ’1
2
(pj
qj
âˆ’pK
qK
)
.
By arguments similar to the proofs of Theorems 5.9 and 5.10 and
Corollaries 5.7 and 5.8, one gets Theorem 5.13 and Corollary 5.10.
Theorem 5.13
Provided that gğœ
s (v)Î£(v)gs(v) > 0,
âˆš
n(Ì‚S âˆ’S)
âˆš
gğœ
s (v)Î£(v)gs(v)
L
âˆ’âˆ’âˆ’â†’N(0, 1)
and
âˆš
n(Ì‚Sâˆ—âˆ’S)
âˆš
gğœ
s (v)Î£(v)gs(v)
L
âˆ’âˆ’âˆ’â†’N(0, 1).
Corollary 5.10
Provided that gğœ
s (v)Î£(v)gs(v) > 0,
âˆš
n(Ì‚S âˆ’S)
Ì‚ğœÌ‚S
L
âˆ’âˆ’âˆ’â†’N(0, 1),
where Ì‚ğœ2
Ì‚S = gğœ
s (Ì‚v)Î£(Ì‚v)gs(Ì‚v), and
âˆš
n(Ì‚Sâˆ—âˆ’S)
Ì‚ğœÌ‚Sâˆ—
L
âˆ’âˆ’âˆ’â†’N(0, 1),
where Ì‚ğœ2
Ì‚Sâˆ—= gğœ
s (Ì‚uâˆ—)Î£(Ì‚uâˆ—)gs(Ì‚uâˆ—) and Ì‚uâˆ—= (Ì‚pâˆ—
1,â€¦, Ì‚pâˆ—
Kâˆ’1, Ì‚qâˆ—
1,â€¦, Ì‚qâˆ—
Kâˆ’1)ğœ.
Example 5.11
Revisiting Example 5.10 where both p and q have Bernoulli
distributions, with probabilities of success being p and q, one has
âˆš
n(Ì‚S âˆ’S)
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2
s )
and
âˆš
n(Ì‚Sâˆ—âˆ’S)
L
âˆ’âˆ’âˆ’â†’N(0, ğœ2
s ),
where
ğœ2
s = p(1 âˆ’p)
4
[
ln
(p(1 âˆ’q)
q(1 âˆ’p)
)
+
p âˆ’q
p(1 âˆ’p)
]2
+ ğœ†q(1 âˆ’q)
4
[
ln
(q(1 âˆ’p)
p(1 âˆ’q)
)
+
q âˆ’p
q(1 âˆ’q)
]2
.

Estimation of Information
195
Remark 5.3
Since Ì‚D(p||q) has an inï¬nite bias as an estimator of D(p||q) and
Ì‚D(q||p) has an inï¬nite bias as an estimator of D(q||p), it follows that Ì‚S has an
inï¬nite bias as an estimator of S(p, q). Similarly, from Theorem 5.11, it follows
that Ì‚Sâˆ—has a bias that decays no faster than ğ’ª(1âˆ•n).
By modifying the estimator given in (5.89), an estimator of S(p, q), whose bias
decays exponentially fast, may be comprised as follows.
Let
Ì‚Sâ™¯= Ì‚Sâ™¯(p, q) = 1
2
( Ì‚Dâ™¯(p||q) + Ì‚Dâ™¯(q||p))
(5.121)
= 1
2
K
âˆ‘
k=1
Ì‚pn
[mâˆ’Yk
âˆ‘
v=1
1
v
v
âˆ
j=1
(
1 âˆ’
Yk
m âˆ’j + 1
)
âˆ’
nâˆ’Xk
âˆ‘
v=1
1
v
v
âˆ
j=1
(
1 âˆ’Xk âˆ’1
n âˆ’j
)]
+ 1
2
K
âˆ‘
k=1
Ì‚qn
[nâˆ’Xk
âˆ‘
v=1
1
v
v
âˆ
j=1
(
1 âˆ’
Xk
n âˆ’j + 1
)
âˆ’
mâˆ’Yk
âˆ‘
v=1
1
v
v
âˆ
j=1
(
1 âˆ’Yk âˆ’1
m âˆ’j
)]
.
By (5.117), the bias of this estimator is
E(Ì‚Sâ™¯âˆ’S) =
K
âˆ‘
k=1
pk
( âˆ
âˆ‘
v=n
1
v(1 âˆ’pk)v âˆ’
âˆ
âˆ‘
v=m+1
1
v(1 âˆ’qk)v
)
+
K
âˆ‘
k=1
qk
( âˆ
âˆ‘
v=m
1
v(1 âˆ’qk)v âˆ’
âˆ
âˆ‘
v=n+1
1
v(1 âˆ’pk)v
)
,
which decays at least exponentially fast as n, m â†’âˆsince each part does.
Theorem 5.14
Provided that gğœ
s (v)Î£(v)gs(v) > 0,
âˆš
n(Ì‚Sâ™¯âˆ’S)
âˆš
gğœ
s (v)Î£(v)gs(v)
L
âˆ’âˆ’âˆ’â†’N(0, 1).
Proof: By Theorem 5.13 and Slutskyâ€™s theorem, it suï¬ƒces to show that
âˆš
n(Ì‚Sâ™¯âˆ’Ì‚Sâˆ—)
p
âˆ’âˆ’âˆ’â†’0.
This however follows from the facts that
âˆš
n( Ì‚Dâ™¯(p||q) âˆ’Ì‚Dâˆ—(p||q))
p
âˆ’âˆ’âˆ’â†’0
and
âˆš
n( Ì‚Dâ™¯(q||p) âˆ’Ì‚Dâˆ—(q||p))
p
âˆ’âˆ’âˆ’â†’0,
which follow from the proof of Theorem 5.12.
â—½
By arguments similar to the proof of Corollary 5.7, the following corollary is
established.

196
Statistical Implications of Turingâ€™s Formula
Corollary 5.11
Provided that gğœ
s (v)Î£(v)gs(v) > 0,
âˆš
n(Ì‚Sâ™¯âˆ’S)
Ì‚ğœÌ‚Sâ™¯
L
âˆ’âˆ’âˆ’â†’N(0, 1),
where Ì‚ğœ2
Ì‚Sâ™¯= gğœ
s (Ì‚uâˆ—)Î£(Ì‚uâˆ—)gs(Ì‚uâˆ—) and Ì‚uâˆ—is as in Corollary 5.10.
5.4
Tests of Hypotheses
In the preceding sections of this chapter, estimators of various information
indices are discussed and their respective asymptotic normalities are presented
under various conditions. Of MI-related indices MI, ğœ…, ğœ…2, and ğœ…3, the main
estimators discussed are
Ì‚
MI,
Ì‚ğœ…,
Ì‚ğœ…2,
and
Ì‚ğœ…3,
respectively, given in (5.56), (5.70), (5.71), and (5.72), and
Ì‚
MIz,
Ì‚ğœ…z,
Ì‚ğœ…2z,
and
Ì‚ğœ…3z,
respectively, given in (5.64), (5.73), (5.74), and (5.75). Of Kullbackâ€“Leibler
divergence indices, D = D(p||q) and S = S(p, q), the main estimators discussed
are
Ì‚D
and
Ì‚S,
respectively, given in (5.88) and (5.119), and
Ì‚Dâ™¯
and
Ì‚Sâ™¯.
respectively, given in (5.89) and (5.121).
Each of these estimators could lead to a large sample test of hypothesis if
its asymptotic normality holds. The key conditions supporting the asymptotic
normalities are
(1, 1 âˆ’1)Î£H(1, 1, âˆ’1)ğœ> 0
(5.122)
in Theorem 5.5 for MI, ğœ…, ğœ…2, and ğœ…3, and
gğœ(v)Î£(v)g(v) > 0
(5.123)
in Theorem 5.9 for D and S. It may be veriï¬ed that if X and Y are independent
random elements on joint alphabet ğ’³Ã— ğ’´, that is, MI = 0, then (5.122) is not
satisï¬ed, and that if p = q on a same ï¬nite alphabet ğ’³, that is, D = D(p||q) = 0,
then (5.123) is not satisï¬ed (see Exercises 14 and 15). This implies that the
asymptotic normalities of this chapter do not directly render valid large sample
tests for hypotheses such as H0 âˆ¶MI = 0 or H0 âˆ¶D = 0. However, they do
support tests for H0 âˆ¶MI = c0 (H0 âˆ¶ğœ…= c0, H0 âˆ¶ğœ…2 = c0 or H0 âˆ¶ğœ…3 = c0)
or H0 âˆ¶D = c0 (H0 âˆ¶S = c0) where c0 > 0 is some appropriately scaled ï¬xed
positive value.

Estimation of Information
197
For completeness as well as conciseness, a general form of such a test is given.
Let (ğœƒ, Ì‚ğœƒ) be any one of the following pairs:
(MI, Ì‚
MI), (MI, Ì‚
MIz), (ğœ…, Ì‚ğœ…), (ğœ…, Ì‚ğœ…z), (ğœ…2, Ì‚ğœ…2), (ğœ…2, Ì‚ğœ…2z), (ğœ…3, Ì‚ğœ…3), (ğœ…3, Ì‚ğœ…3z),
(D, Ì‚D), (D, Ì‚Dâˆ—), (D, Ì‚Dâ™¯), (S, Ì‚S), (S, Ì‚Sâˆ—), (S, Ì‚Sâ™¯).
Under H0 âˆ¶ğœƒ= ğœƒ0 > 0, the following statistic is approximately a standard
normal random variable:
Z =
âˆš
n( Ì‚ğœƒâˆ’ğœƒ0)
Ì‚ğœÌ‚ğœƒ
where Ì‚ğœ2
Ì‚ğœƒis as in Table 5.4. It is to be speciï¬cally noted that this large sample
normal test is valid only for ğœƒ0 > 0. When ğœƒ0 = 0, the underlying asymptotic
normality does not hold for any of the listed Ì‚ğœƒ.
In practice, it is often of interest to test the hypothesis of H0 âˆ¶MI = 0 versus
Ha âˆ¶MI > 0, or just as often to test the hypothesis H0 âˆ¶D(p||q) = 0 versus
Ha âˆ¶D(p||q) > 0. For these hypotheses, provided that the sizes of the under-
lying alphabets, K1 and K2, are ï¬nite, there exist well-known goodness-of-ï¬t
tests in the standard statistical literature.
For example, by Theorem 5.1, D(p||q) = 0 if and only if p = q. Therefore,
to test H0 âˆ¶D(p||q) = 0 versus Ha âˆ¶D(p||q) > 0, one may use Pearsonâ€™s
goodness-of-ï¬t test. In the case of a one-sample goodness-of-ï¬t test
Table 5.4 Estimated information indices with
estimated variances.
ğœ½
Ì‚ğœ½
Ì‚ğˆ2
Ì‚ğœ½
As in
MI
Ì‚
MI
Ì‚ğœ2
Equation (5.62)
MI
Ì‚
MIz
Ì‚ğœ2
Equation (5.62)
ğœ…
Ì‚ğœ…
Ì‚ğœ2
Ì‚ğœ…
Corollary 5.5
ğœ…
Ì‚ğœ…z
Ì‚ğœ2
Ì‚ğœ…z
Corollary 5.6
ğœ…2
Ì‚ğœ…2
Ì‚ğœ2
Ì‚ğœ…2
Corollary 5.5
ğœ…2
Ì‚ğœ…2z
Ì‚ğœ2
Ì‚ğœ…2z
Corollary 5.6
ğœ…3
Ì‚ğœ…3
Ì‚ğœ2
Ì‚ğœ…3
Corollary 5.5
ğœ…3
Ì‚ğœ…3z
Ì‚ğœ2
Ì‚ğœ…3z
Corollary 5.6
D
Ì‚Dn
Ì‚ğœ2
Ì‚Dn
Corollary 5.7
D
Ì‚Dâˆ—
n
Ì‚ğœ2
Ì‚Dâˆ—
n
Corollary 5.8
D
Ì‚Dâ™¯
n
Ì‚ğœ2
Ì‚Dâ™¯
n
Corollary 5.9
S
Ì‚Sn
Ì‚ğœ2
Ì‚Sn
Corollary 5.10
S
Ì‚Sâˆ—
n
Ì‚ğœ2
Ì‚Sâˆ—
n
Corollary 5.10
S
Ì‚Sâ™¯
n
Ì‚ğœ2
Ì‚Sâ™¯
n
Corollary 5.11

198
Statistical Implications of Turingâ€™s Formula
for H0 âˆ¶p = q where q = {q1,â€¦, qK} is a pre-ï¬xed known probability
distribution,
Q1 =
K
âˆ‘
k=1
(Fk âˆ’nqk)2
nqk
,
(5.124)
where Fk is the observed frequency of the kth letter in the sample of size n,
behaves approximately as a chi-squared random variable with degrees of
freedom m = K âˆ’1.
In the case of a two-sample goodness-of-ï¬t test for H0 âˆ¶p = q where neither
p or q is known a priori,
Q2 =
2
âˆ‘
i=1
K
âˆ‘
k=1
[Fi,k âˆ’niFâ‹…,kâˆ•(n1 + n2)]2
niFâ‹…,kâˆ•(n1 + n2)
,
(5.125)
where K is the common eï¬€ective cardinality of the underlying ï¬nite alphabet,
i indicates the ith sample, k indicates the kth letter of the alphabet, Fi,k is
the observed frequency of the kth letter in the ith sample, Fâ‹…,k = F1,k + F2,k,
and ni is size of the ith sample. The statistic Q2 is known as a generalization
of the Pearson goodness-of-ï¬t statistic, Q1. Under H0, Q2 is asymptotically
a chi-squared random variable with degrees of freedom df = K âˆ’1. One
would reject H0 if Q2 takes a large value, say greater than ğœ’2
ğ›¼(K âˆ’1), the
(1 âˆ’ğ›¼) Ã— 100th quantile of chi-squared distribution with degrees of freedom
K âˆ’1. A discussion of Q1 and Q2 is found in many textbooks, in particular,
Mood, Graybill, and Boes (1974).
Similar to Theorem 5.2, MI = 0 if and only if X and Y are independent,
that is, pi,j = pi,â‹…pâ‹…,j for every pair (i, j). Therefore, to test H0 âˆ¶MI = 0 versus
Ha âˆ¶MI > 0, one may use the Pearson chi-squared statistic for independence
in two-way contingency tables,
Q3 =
K1
âˆ‘
i=1
K2
âˆ‘
j=1
(Fi,j âˆ’nÌ‚pi,â‹…Ì‚pâ‹…,j)2
nÌ‚pi,â‹…Ì‚pâ‹…,j
(5.126)
where Fi,j is the observed frequency of (xi, yj) âˆˆğ’³Ã— ğ’´in an iid sample
of size n; Ì‚pi,j = Fi,jâˆ•n; Ì‚pi,â‹…= âˆ‘K2
j=1 Ì‚pi,j; Ì‚pâ‹…,j = âˆ‘K1
i=1 Ì‚pi,j; Under H0 âˆ¶MI = 0, Q3
is asymptotically a chi-square random variable with degrees of freedom
(K1 âˆ’1)(K2 âˆ’1). One would reject H0 if Q3 takes a large value, say greater than
ğœ’2
ğ›¼((K1 âˆ’1)(K2 âˆ’1)), the (1 âˆ’ğ›¼) Ã— 100th quantile of chi-square distribution
with degrees of freedom (K1 âˆ’1)(K2 âˆ’1). This well-known test is also credited
to Pearson (1900, 1922) and sometimes to Fisher and Tippett (1922).
However it is well known that Pearsonâ€™s goodness-of-ï¬t tests perform
poorly when there are many low-frequency cells in the sample data. A rule
of thumb widely adopted in statistical practice for the adequacy of Pear-
sonâ€™s goodness-of-ï¬t test is that the expected (or similarly, the observed)
cell frequency is greater or equal to 5. This rule is largely credited to

Estimation of Information
199
Cochran (1952). Because of such a characteristic, tests based on Q1, Q2, and
Q3 often become less reliable in cases of small sample sizes, relative to the
eï¬€ective cardinality of the underlying alphabet, or a lack of sample coverage.
It would be of great interest to develop better-performing testing procedures
for H0 âˆ¶MI = 0 and H0 âˆ¶D = 0 with sparse data.
5.5
Appendix
5.5.1
Proof of Theorem 5.12
Proof: Since
âˆš
n( Ì‚Dâ™¯âˆ’D) =
âˆš
n( Ì‚Dâ™¯âˆ’Ì‚Dâˆ—) +
âˆš
n( Ì‚Dâˆ—âˆ’D),
by Theorem 5.10 and Slutskyâ€™s theorem, it suï¬ƒces to show that
âˆš
n( Ì‚Dâ™¯âˆ’Ì‚Dâˆ—)
pâ†’0.
Note that
âˆš
n( Ì‚Dâ™¯âˆ’Ì‚Dâˆ—) =
âˆš
n( Ì‚Aâ™¯âˆ’Ì‚A) +
âˆš
n(Ì‚Bâ™¯âˆ’Ì‚Bâˆ—) =âˆ¶îˆ­1 + îˆ­2,
where Ì‚A and Ì‚Bâˆ—are as in (5.98), Ì‚Aâ™¯and Ì‚Bâ™¯are as in (5.108) and (5.115)
respectively. It is shown that îˆ­1
pâ†’0 and îˆ­2
pâ†’0. For a better presentation,
the proof is divided into two parts: Part 1 is the proof of îˆ­1
pâ†’0, and Part 2 is
the proof of îˆ­2
pâ†’0.
Part 1: First consider îˆ­1. By the Taylor expansion of the logarithm,
îˆ­1 = âˆ’
âˆš
n
(
âˆ’Ì‚Aâ™¯âˆ’
K
âˆ‘
k=1
nâˆ’Xk
âˆ‘
v=1
1
v Ì‚pk(1 âˆ’Ì‚pk)v
)
+
âˆš
n
K
âˆ‘
k=1
âˆ
âˆ‘
v=nâˆ’Xk+1
1
v Ì‚pk(1 âˆ’Ì‚pk)v
=âˆ¶âˆ’îˆ­1,1 + îˆ­1,2.
Since
0 â‰¤îˆ­1,2 â‰¤
âˆš
n
K
âˆ‘
k=1
Ì‚pk
n âˆ’Xk + 1
âˆ
âˆ‘
v=nâˆ’Xk+1
(1 âˆ’Ì‚pk)v
=
âˆš
n
K
âˆ‘
k=1
1
n âˆ’Xk + 1(1 âˆ’Ì‚pk)nâˆ’Xk+1
=
1
âˆš
n
K
âˆ‘
k=1
1
1 âˆ’Ì‚pk + 1âˆ•n(1 âˆ’Ì‚pk)nâˆ’Xk+11[Ì‚pk < 1]

200
Statistical Implications of Turingâ€™s Formula
â‰¤
1
âˆš
n
K
âˆ‘
k=1
(1 âˆ’Ì‚pk)nâˆ’Xk1[Ì‚pk < 1] â‰¤K
âˆš
n
,
(5.127)
îˆ­1,2 â†’0 a.s. and therefore îˆ­1,2
pâ†’0.
Before considering îˆ­1,1, note that for j = 0, 1,â€¦, n âˆ’1
(
1 âˆ’Xk âˆ’1
n âˆ’j
)
â‰¥
(
1 âˆ’Xk
n
)
= (1 âˆ’Ì‚pk)
(5.128)
if and only if
0 â‰¤j â‰¤
n
Xk + 1[Xk = 0] =âˆ¶1
Ì‚pâˆ—
k
.
Let Jk = âŒŠ1âˆ•Ì‚pâˆ—
kâŒ‹. One can write
îˆ­1,1 =
âˆš
n
K
âˆ‘
k=1
Ì‚pk
nâˆ’Xk
âˆ‘
v=1
1
v
[ v
âˆ
j=1
(
1 âˆ’Xk âˆ’1
n âˆ’j
)
âˆ’
Jkâˆ§v
âˆ
j=1
(
1 âˆ’Xk âˆ’1
n âˆ’j
)
(1 âˆ’Ì‚pk)0âˆ¨(vâˆ’Jk)
]
+
âˆš
n
K
âˆ‘
k=1
Ì‚pk
nâˆ’Xk
âˆ‘
v=1
1
v
[Jkâˆ§v
âˆ
j=1
(
1 âˆ’Xk âˆ’1
n âˆ’j
)
(1 âˆ’Ì‚pk)0âˆ¨(vâˆ’Jk)
âˆ’(1 âˆ’Ì‚pk)v
]
=âˆ¶îˆ­1,1,1 + îˆ­1,1,2.
By (5.128), îˆ­1,1,1 â‰¤0 and îˆ­1,1,2 â‰¥0. It is to show that E(îˆ­1,1,1) â†’0 and
E(îˆ­1,1,2) â†’0 respectively. Toward that end,
îˆ­1,1,2 â‰¤
âˆš
n
K
âˆ‘
k=1
Ì‚pk
nâˆ’Xk
âˆ‘
v=1
1
v
[Jkâˆ§v
âˆ
j=1
(
1 âˆ’Xk âˆ’1
n âˆ’1
)
(1 âˆ’Ì‚pk)0âˆ¨(vâˆ’Jk)
âˆ’(1 âˆ’Ì‚pk)v
]
â‰¤
âˆš
n
K
âˆ‘
k=1
Ì‚pk
nâˆ’Xk
âˆ‘
v=1
1
v
[(
1 âˆ’Xk âˆ’1
n âˆ’1
)Jkâˆ§v
âˆ’(1 âˆ’Ì‚pk)Jkâˆ§v
]
â‰¤
âˆš
n
K
âˆ‘
k=1
Ì‚pk
nâˆ’Xk
âˆ‘
v=1
1
v
[
(Jk âˆ§v)
(
1 âˆ’Xk âˆ’1
n âˆ’1
)(Jkâˆ§v)âˆ’1 n âˆ’Xk
n(n âˆ’1)
]
â‰¤
âˆš
n
n âˆ’1
K
âˆ‘
k=1
n
âˆ‘
v=1
1
v(Jk âˆ§v)

Estimation of Information
201
=
âˆš
n
n âˆ’1
K
âˆ‘
k=1
(
Jk + Jk
n
âˆ‘
v=Jk+1
1
v
)
â‰¤
âˆš
n
n âˆ’1
K
âˆ‘
k=1
Jk
(
1 +
n
âˆ‘
v=2
1
v
)
â‰¤
âˆš
n
n âˆ’1
K
âˆ‘
k=1
Jk(1 + ln n),
where the third line follows by Lemma 5.8 and the fact that
1 âˆ’Xk âˆ’1
n âˆ’1 > 1 âˆ’Ì‚pk.
Thus, by (5.128) and Lemma 5.9,
0 â‰¤E(îˆ­1,1,2) â‰¤îˆ»
(âˆš
n ln(n)
n
)
â†’0.
Furthermore,
îˆ­1,1,1 =
âˆš
n[A âˆ’Ì‚Aâ™¯]
âˆ’
âˆš
n
K
âˆ‘
k=1
Ì‚pk
nâˆ’Xk
âˆ‘
v=1
1
v
[Jkâˆ§v
âˆ
j=1
(
1 âˆ’Xk âˆ’1
n âˆ’j
)
(1 âˆ’Ì‚pk)0âˆ¨(vâˆ’Jk)
âˆ’(1 âˆ’Ì‚pk)v
]
âˆ’
âˆš
n
[ K
âˆ‘
k=1
Ì‚pk
nâˆ’Xk
âˆ‘
v=1
1
v(1 âˆ’Ì‚pk)v + A
]
âˆ¶= îˆ­1,1,1,1 âˆ’îˆ­1,1,2 âˆ’îˆ­1,1,1,3.
It has already been shown that E(îˆ­1,1,2) â†’0, and from (5.107) it follows that
E(îˆ­1,1,1,1) =
âˆš
nB1 â†’0. Still further
îˆ­1,1,1,3 =
âˆš
n
[ K
âˆ‘
k=1
Ì‚pk
âˆ
âˆ‘
v=1
1
v(1 âˆ’Ì‚pk)v + A
]
âˆ’
âˆš
n
K
âˆ‘
k=1
Ì‚pk
âˆ
âˆ‘
v=nâˆ’Xk+1
1
v(1 âˆ’Ì‚pk)v
=
âˆš
n(A âˆ’Ì‚An) âˆ’
âˆš
n
K
âˆ‘
k=1
Ì‚pk
âˆ
âˆ‘
v=nâˆ’Xk+1
1
v(1 âˆ’Ì‚pk)v
âˆ¶= îˆ­1,1,1,3,1 âˆ’îˆ­1,2,

202
Statistical Implications of Turingâ€™s Formula
where the second line follows by the Taylor expansion of the logarithm. Since
E(îˆ­1,1,1,3,1) â†’0 by (5.102) and E(îˆ­1,2) â†’0 by (5.127). Thus, E(îˆ­1,1,1,3) â†’0.
This means that îˆ­1,1,2
pâ†’0. From here, it follows that îˆ­1
pâ†’0.
Part 2: Consider now îˆ­2. Note that
îˆ­2 =
âˆš
n
m
K
âˆ‘
k=1
Ì‚pk
âˆš
m
[mâˆ’Yk
âˆ‘
v=1
1
vUk,v + ln Ì‚qâˆ—
k
]
.
Since Ì‚pk
pâ†’pk for every k = 1,â€¦, K and nâˆ•m â†’ğœ†, by Slutskyâ€™s theorem it suf-
ï¬ces to show that for
âˆš
m
[mâˆ’Yk
âˆ‘
v=1
1
vUk,v + ln(Ì‚qâˆ—
k)
]
p
âˆ’âˆ’âˆ’â†’0
for each k = 1, 2,â€¦, K. By the Taylor expansion of the logarithm
âˆš
m
[mâˆ’Yk
âˆ‘
v=1
1
vUk,v + ln(Ì‚qâˆ—
k)
]
=
âˆš
m
[mâˆ’Yk
âˆ‘
v=1
1
vUk,v âˆ’
âˆ
âˆ‘
v=1
1
v(1 âˆ’Ì‚qâˆ—
k)v
]
=
âˆš
m
{mâˆ’Yk
âˆ‘
v=1
1
v[Uk,v âˆ’(1 âˆ’Ì‚qâˆ—
k)v]
}
1[Ì‚qk > 0]
+
âˆš
m
{mâˆ’Yk
âˆ‘
v=1
1
v[Uk,v âˆ’(1 âˆ’Ì‚qâˆ—
k)v]
}
1[Ì‚qk = 0]
âˆ’
âˆš
m
âˆ
âˆ‘
v=mâˆ’Yk+1
1
v(1 âˆ’Ì‚qâˆ—
k)v
=âˆ¶îˆ­2,1 + îˆ­2,2 âˆ’îˆ­2,3.
Since Ì‚qk
pâ†’qk and qâˆ—
k
pâ†’qk, Slutskyâ€™s theorem implies that
0 â‰¤îˆ­2,3 â‰¤
âˆš
m
m âˆ’Yk + 1
âˆ
âˆ‘
v=mâˆ’Yk+1
(1 âˆ’Ì‚qâˆ—
k)v
=
âˆš
m
m âˆ’Yk + 1
1
Ì‚qâˆ—
k
(1 âˆ’Ì‚qâˆ—
k)mâˆ’Yk+1
=
1
âˆš
mÌ‚qâˆ—
k
1
1 âˆ’Ì‚qk + 1âˆ•m(1 âˆ’Ì‚qâˆ—
k)mâˆ’Yk+1
â‰¤
1
âˆš
mÌ‚qâˆ—
k
1
1 âˆ’Ì‚qk + 1âˆ•m
p
âˆ’âˆ’âˆ’â†’0
as m â†’âˆ, and hence îˆ­2,3
pâ†’0.

Estimation of Information
203
Next, it is to show that îˆ­2,2
pâ†’0. When Ì‚qk = 0, one has Uk,v = 1, Ì‚qâˆ—
k = 1âˆ•m,
and
0 â‰¤îˆ­2,2 â‰¤
âˆš
m
{ m
âˆ‘
v=1
1
v
[
1 âˆ’
(
1 âˆ’1
m
)v]}
1[Ì‚qk = 0]
â‰¤m3âˆ•21[Ì‚qk = 0].
Thus, 0 â‰¤E(îˆ­2,2) â‰¤m3âˆ•2(1 âˆ’qk)m â†’0, and therefore îˆ­2,2
pâ†’0. Next, it is to
show that îˆ­2,1
pâ†’0. Since that in this case Ì‚qk = Ì‚qâˆ—
k, one has
îˆ­2,1 =
âˆš
m
mâˆ’Yk
âˆ‘
v=1
1
v
{[ vâˆ’1
âˆ
j=0
(
1 âˆ’
Yk
m âˆ’j
)]
âˆ’(1 âˆ’Ì‚qâˆ—
k)v
}
1[Ì‚qk > 0]
â‰¤
âˆš
m
mâˆ’Yk
âˆ‘
v=1
1
v[(1 âˆ’Ì‚qk)v âˆ’(1 âˆ’Ì‚qâˆ—
k)v]1[Ì‚qk > 0] = 0.
To complete the proof, it is to show that E(îˆ­2,1) â†’0. Toward that end,
îˆ­2,1 + îˆ­2,2 =
âˆš
m
[mâˆ’Yk
âˆ‘
v=1
1
vUk,v âˆ’
m
âˆ‘
v=1
1
v(1 âˆ’qk)v
]
âˆ’
âˆš
m
[ âˆ
âˆ‘
v=1
1
v(1 âˆ’Ì‚qâˆ—
k)v âˆ’
âˆ
âˆ‘
v=1
1
v(1 âˆ’qk)v
]
+
âˆš
m
âˆ
âˆ‘
v=mâˆ’Yk+1
1
v(1 âˆ’Ì‚qâˆ—
k)v âˆ’
âˆš
m
âˆ
âˆ‘
v=m+1
1
v(1 âˆ’qk)v
=âˆ¶îˆ­2,1,1 âˆ’îˆ­2,1,2 + îˆ­2,1,3 âˆ’îˆ­2,1,4.
E(îˆ­2,1,1) = 0 because
mâˆ’Yk
âˆ‘
v=1
1
vUk,v
is an unbiased estimator of
m
âˆ‘
v=1
1
v(1 âˆ’qk)v
by construction.
By the Taylor expansion of the logarithm îˆ­2,1,2 =
âˆš
m[ln(qk) âˆ’ln(Ì‚qâˆ—
k)] and
E(îˆ­2,1,2) â†’0 by an argument similar to the proof of Proposition 5.11. To show
that E(îˆ­2,1,3) â†’0, note that
0 â‰¤îˆ­2,1,3 â‰¤
âˆš
m
m âˆ’Yk + 1
1
Ì‚qâˆ—
k
(1 âˆ’Ì‚qâˆ—
k)mâˆ’Yk+1

204
Statistical Implications of Turingâ€™s Formula
â‰¤
1
âˆš
m(1 âˆ’Ì‚qk + 1âˆ•m)
1
Ì‚qâˆ—
k
(1 âˆ’Ì‚qk)mâˆ’Yk+1
=
1
âˆš
mÌ‚qâˆ—
k
1 âˆ’Ì‚qk
(1 âˆ’Ì‚qk + 1âˆ•m)(1 âˆ’Ì‚qk)mâˆ’Yk
â‰¤
1
âˆš
mÌ‚qâˆ—
k
.
Thus, by Lemma 5.9,
0 â‰¤E(îˆ­2,1,3) â‰¤
1
m3âˆ•2 E
(
m
Ì‚qâˆ—
k
)
â†’0
as m â†’âˆ. Finally, E(îˆ­2,1,4) â†’0 because îˆ­2,1,4 is deterministic and
0 â‰¤îˆ­2,1,4 â‰¤
âˆš
m
âˆ
âˆ‘
v=m
1
v(1 âˆ’qk)v â‰¤
1
âˆš
m
(1 âˆ’qk)m
qk
â†’0.
Hence, E(îˆ­2,1 + îˆ­2,2) â†’0, but since E(îˆ­2,2) â†’0 is already established, it
follows that E(îˆ­2,1) â†’0, which implies îˆ­2
pâ†’0. The result of the theorem
follows.
â—½
5.6
Exercises
1
Prove Lemma 5.2, Jensenâ€™s inequality.
2
Verify both parts in (5.16) of Example 5.2.
3
Verify (5.17)â€“(5.20) in Example 5.3.
4
Verify (5.21)â€“(5.24) in Example 5.4.
5
Verify (5.27) and (5.28) in Example 5.7.
6
Show that Î£ in (5.50) is positive deï¬nite.
7
Prove each of the following six inequalities.
0 â‰¤MI(X, Y) â‰¤min{H(X), H(Y)}
â‰¤
âˆš
H(X)H(Y)
â‰¤(H(X) + H(Y))âˆ•2
â‰¤max{H(X), H(Y)}
â‰¤H(X, Y).

Estimation of Information
205
8
Let
ğœ…1 =
MI(X, Y)
min{H(X), H(Y)},
ğœ…2 =
MI(X, Y)
âˆš
H(X)H(Y)
,
ğœ…3 =
MI(X, Y)
(H(X) + H(Y))âˆ•2,
and
ğœ…4 =
MI(X, Y)
max{H(X), H(Y)},
and assume H(X, Y) < âˆ. Show that Theorem 5.4 holds for
a) ğœ…2,
b) ğœ…3,
c) ğœ…4,
d) but not for ğœ…1.
9
Show that Z1,v of (5.106) is identical to Ì‚Aâ™¯
n of (5.108).
10
In mathematics, a distance function on a given set ğ’«is a function
d âˆ¶ğ’«Ã— ğ’«â†’R,
where R denotes the set of real numbers, which satisï¬es the following con-
ditions: for any three objects, pX âˆˆğ’«, pY âˆˆğ’«, and pZ âˆˆğ’«,
i) d(pX, pY) â‰¥0;
ii) d(pX, pY) = 0 if and only if pX = pY;
iii) d(pX, pY) = d(pY, pX); and
iv) d(pX, pZ) â‰¤d(pX, pY) + d(pY, pZ).
Show that
a) the Kullbackâ€“Leibler divergence D(p||q) of (5.87) is not a distance
function on ğ’«, where ğ’«is the collection of all probability distribu-
tions on a countable alphabet ğ’³; but
b) the symmetrized Kullbackâ€“Leibler divergence S(p, q) of (5.118) is a
distance function on ğ’«.
11
Suppose a joint probability distribution pX,Y = {pi,j} on
ğ’³Ã— ğ’´= {xi; i = 1,â€¦, K1} Ã— {yj; j = 1,â€¦, K2}
is such that pi,j > 0 for every pair (i, j). Let K = K1K2. Re-enumerating {pi,j}
to be indexed by a single index k, that is,
v = (p1,â€¦, pKâˆ’1)ğœ
= (p1,1,â€¦, p1,K2, p2,1,â€¦, p2,K2,â€¦, pK1,1,â€¦, pK1,K2âˆ’1)ğœ,
show that each of (5.42)â€“(5.44) is true.

206
Statistical Implications of Turingâ€™s Formula
12
Let {pi,j; i = 1,â€¦, K1, j = 1,â€¦, K2} be a joint probability distribution on
ğ’³Ã— ğ’´where ğ’³and ğ’´are as in (5.31). Suppose pi,â‹…> 0 for each i, pâ‹…,j > 0
for each j, K1 â‰¥2 and K2 â‰¥2. Show that (5.57) of Theorem 5.5 holds if and
only if pi,j = pi,â‹…pâ‹…,j, or every pair of (i, j) such that pi,j > 0.
13
Consider the following three functions of triplet (x1, x2, x3), with the same
domain, x1 > 0, x2 > 0, and x3 > 0,
ğœ…(x1, x2, x3) = x1 + x2
x3
âˆ’1,
ğœ…2(x1, x2, x3) = x1 + x2 âˆ’x3
âˆšx1x2
,
ğœ…3(x1, x2, x3) = 2
(
1 âˆ’
x3
x1 + x2
)
.
Show that the gradients of these functions are, respectively,
gğœ…(x1, x2, x3) =
(
1
x3
, 1
x3
, âˆ’x1 + x2
x2
3
)ğœ
,
gğœ…2(x1, x2, x3) =
(
1
(x1x2)1âˆ•2 âˆ’x2(x1 + x2 âˆ’x3)
2(x1x2)3âˆ•2
,
1
(x1x2)1âˆ•2 âˆ’x1(x1 + x2 âˆ’x3)
2(x1x2)3âˆ•2
, âˆ’
1
âˆšx1x2
)ğœ
,
gğœ…3(x1, x2, x3) =
(
2x3
(x1 + x2)2 ,
2x3
(x1 + x2)2 , âˆ’
2
x1 + x2
,
)ğœ
.
14
In Theorem 5.5, show that if X and Y are independent random elements
on joint alphabet ğ’³Ã— ğ’´, that is, MI = 0, then
(1, 1 âˆ’1)Î£H(1, 1, âˆ’1)ğœ= 0.
15
In Theorem 5.9, show that if p = q on a same ï¬nite alphabet ğ’³, that is,
D = D(p||q) = 0, then gğœ(v)Î£(v)g(v) = 0.
16
Use the data in Table 5.1 and Pearsonâ€™s goodness-of-ï¬t statistic, Q3 of
(5.126), to test the hypothesis that the two random elements, Ethnicity
and Favorite Color, are independent, that is, H0 âˆ¶MI = 0, at ğ›¼= 0.05.
17
Suppose n = 600 tosses of a die yield the following results.
X
1
2
3
4
5
6
f
91 101 105 89 124 90

Estimation of Information
207
Use the data and Pearsonâ€™s goodness-of-ï¬t statistic, Q1 of (5.124), to test
the hypothesis that the dice is a balanced die, that is, H0 âˆ¶D(p||q) = 0,
where q is the hypothesized uniform distribution for the six sides
(qk = 1âˆ•6 for k = 1,â€¦, 6) and p is the true probability distribution of the
die tossed, at ğ›¼= 0.05.
18
Two dice, A and B, are independently tossed nA = 600 and nB = 300 times,
respectively, with the given results.
X
1
2
3
4
5
6
fA
91
101
105
89
124
90
fB
54
53
52
41
64
36
Let the underlying probability distributions of the two dice be p for A and
q for B, respectively.
a) Test the hypothesis that the two distributions are identical, that is,
H0 âˆ¶p = q, at ğ›¼= 0.05.
b) Test the hypothesis that both distributions are uniform, that is,
H0 âˆ¶pk = qk = 1âˆ•6 for k = 1,â€¦, 6, at ğ›¼= 0.05.
19
Let Xm be a sequence of chi-squared random variables with degrees of
freedom m. Show that
âˆš
m(Xm âˆ’m)
L
âˆ’âˆ’âˆ’â†’N(0, 2) as m â†’âˆ.
20
Suppose the joint probability distributions of (X, Y) on {0, 1} Ã— {0, 1} is
X = 0
X = 1
Y = 0
0.50 âˆ’0.01 m
0.01 m
Y = 1
0.01 m
0.50 âˆ’0.01 m
for some integer m â‰¥1. Show that
ğœ…= ln 2 âˆ’[(mâˆ•50) ln(50âˆ•m âˆ’1) âˆ’ln(1 âˆ’mâˆ•50)]
ln 2 + [(mâˆ•50) ln(50âˆ•m âˆ’1) âˆ’ln(1 âˆ’mâˆ•50)].

209
6
Domains of Attraction on Countable Alphabets
6.1
Introduction
A domain of attraction is a family of probability distributions whose members
share a set of common properties, more speciï¬cally a set of common proper-
ties pertaining to the tail of a probability distribution. In the probability and
statistics literature, domains of attraction are usually discussed in the context
of extreme value theory. The simplest setup involves a sample of iid random
variables, X1,â€¦, Xn, under a probability distribution with a diï¬€erentiable
cumulative distribution function (cdf ), F(x). Let X(n) = max{X1,â€¦, Xn}.
The asymptotic behavior of X(n) may be characterized by that of a properly
normalized X(n), namely Yn = (X(n) âˆ’bn)âˆ•an where {an > 0} and {bn} are
normalizing sequences, in the sense that
P(Yn â‰¤y) = P(X(n) â‰¤any + bn) = [F(any + bn)]n â†’G(y)
where G(y) is a nondegenerated distribution function. Many distributions on
the real line may be categorized into three families or domains. The members in
each of the three domains have a G(y) belonging to a same parametric family.
These three families are often identiï¬ed as FrÃ©chet for thick-tailed distribu-
tions (e.g., with density function f (x) = xâˆ’21[x â‰¥1]), Gumbel for thin-tailed
distributions (e.g., with density function f (x) = eâˆ’x1[x â‰¥0]), and Weibull for
distributions with ï¬nite support (e.g., with density function f (x) = 1[0 â‰¤x â‰¤
1]). Extreme value theory is one of the most important topics in probability
and statistics, partially because of its far-reaching implications in applications.
Early works in this area include (FrÃ©chet, 1927; Fisher and Tippett, 1928, and
von Mises, 1936). However, Gnedenko (1943) and Gnedenko (1948) are often
thought to be the ï¬rst rigorous mathematical treatment of the topic. For a com-
prehensive introduction to extreme value theory, readers may wish to refer to
de Haan and Ferreira (2006).
Consider a countable alphabet ğ’³= {ğ“k; k â‰¥1} and its associated probability
distribution p = {pk; k â‰¥1} âˆˆğ’«where ğ’«is the collection of all probability
distributions on ğ’³. Let X1,â€¦, Xn be an iid random sample from ğ’³under p.
Let {Yk; k â‰¥1} and {Ì‚pk = Ykâˆ•n; k â‰¥1} be the observed frequencies and relative
Statistical Implications of Turingâ€™s Formula, First Edition. Zhiyi Zhang.
Â© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

210
Statistical Implications of Turingâ€™s Formula
frequencies of the letters in the iid sample of size n. This chapter presents a
discussion of domains of attraction in ğ’«.
Unlike random variables on the real line, the letters in ğ’³are not necessarily
on a numerical scale or even ordinal. The nature of the alphabet presents an
immediate issue inhibiting an intuitive parallel to the notions well conceived
with random variables on the real line, say for example, the maximum order
statistic X(n), which may be considered as an extreme value in an iid sample
of size n, or the notion of a â€œtailâ€ of a distribution, P(X(n) > x) = 1 âˆ’Fn(x) for
large x. It is much less clear what an extreme value means on an alphabet. Can
an imitation game still be played? If so, in what sense?
For a random variable, the word â€œextremeâ€ suggests an extremely large value
on the real line. However, an extremely large value on the real line necessarily
corresponds to an extremely small probability density. In that sense, an extreme
value could be taken as a value in a sample space that carries an extremely small
probability. If this notion is adopted, then it becomes reasonable to perceive an
extreme value on an alphabet to be a letter with extremely small probability.
In that sense, a subset of ğ’³with low probability letters may be referred to as
a â€œtail,â€ a subset of ğ’³with very low probability letters may be referred to as a
â€œdistant tail,â€ and a distribution on a ï¬nite alphabet has â€œno tail.â€
By the same argument, one may choose to associate the notion of extrem-
ity in an iid sample with the rarity of the letter in the sample. The values that
are observed exactly once in the sample are rarer than any other values in the
sample; and there could be and usually are many more than one such observed
value in a sample. There exist however rarer values than those even with fre-
quency one, and these would be the letters with frequency zero, that is, the
letters in ğ’³that are not represented in the sample. Though not in the sample,
the letters with zero observed frequencies are, nevertheless, associated with
and completely speciï¬ed by the sample.
To bring the above-described notion of extremity into a probabilistic argu-
ment, consider a two-step experiment as follows:
Step 1: draw an iid sample of a large size n from ğ’³, and then
Step 2: draw another independent iid sample of size m from ğ’³.
Let the samples be denoted as
{X1,â€¦, Xn}
and
{Xn+1,â€¦, Xn+m}.
When m = 1, this experiment is the same as that described in Robbinsâ€™ Claim
in Chapter 1. Let E1 be the event that Xn+1 assumes a value from ğ’³that is not
assumed by any of the Xi in the ï¬rst sample, i.e.,
E1 = âˆ©n
i=1{Xn+1 â‰ Xi}.
Event E1 may be thought of as a new discovery, and in this regard, an occurrence
of a rare event, and therefore is pertaining to the tail of the distribution. The

Domains of Attraction on Countable Alphabets
211
argument (1.60) in Chapter 1 gives P(E1) = ğœ1,n =âˆ¶ğœn. Clearly ğœn â†’0 as n â†’âˆ
for any probability distribution {pk} on ğ’³(see Exercise 1).
When m = n, a quantity of interest is the number of letters found in the sec-
ond sample {Xn+1,â€¦, Xn+n} that are not found in the ï¬rst sample {X1,â€¦, Xn},
that is,
tn =
n
âˆ‘
j=1
1[Ej]
(6.1)
where Ej = âˆ©n
i=1{Xn+j â‰ Xi} for j = 1,â€¦, n. The expected value of tn in (6.1) is
ğœn = E (tn) = nğœn.
(6.2)
ğœn is referred to as the tail index and plays a central role in this chapter. The rel-
evance of ğœn to the tail of the underlying distribution manifests in the relevance
of ğœn to the tail.
Furthermore, it is to be noted that, for any given integer k0 â‰¥1, the ï¬rst k0
terms in the re-expression of ğœn, as in
ğœn =
âˆ‘
1â‰¤kâ‰¤k0
npk(1 âˆ’pk)n +
âˆ‘
k>k0
npk(1 âˆ’pk)n,
converges to zero exponentially fast as n â†’âˆ. Therefore, the asymptotic
behavior of ğœn has essentially nothing to do with how the probabilities are
distributed over any ï¬xed and ï¬nite subset of ğ’³. Also to be noted is that ğœn is
invariant under any permutation on the index set {k; k â‰¥1}.
In contrast to the fact that ğœn â†’0 for any probability distribution {pk}, the
multiplicatively inï¬‚ated version ğœn = nğœn has diï¬€erent asymptotic behavior
under various distributions. ğ’«, the total collection of all probability distribu-
tions, on ğ’³splinters into diï¬€erent domains of attraction characterized by the
asymptotic features of ğœn.
Finally, a notion of thinner/thicker tail between two distributions,
p = {pk; k â‰¥1} and q = {qk; k â‰¥1}, is to be mentioned. Although there
is no natural ordering among the letters in ğ’³, there is one on the index set
{k; k â‰¥1}. There therefore exists a natural notion of a distribution p having
a thinner tail than that of another distribution q, in the sense of pk â‰¤qk for
every k â‰¥k0 for some integer k0 â‰¥1, when p and q share a same alphabet ğ’³
and are enumerated by a same index set {k; k â‰¥1}. Whenever this is the case
in the subsequent text, p is said to have a thinner tail than q in the usual sense.
Deï¬nition 6.1
A distribution p = {pk} on ğ’³is said to belong to
1) Domain 0 if limnâ†’âˆğœn = 0,
2) Domain 1 if lim supnâ†’âˆğœn = cp for some constant cp > 0,
3) Domain 2 if limnâ†’âˆğœn = âˆ, and
4) Domain T, or Domain Transient, if it does not belong to Domains 0, 1, or 2.

212
Statistical Implications of Turingâ€™s Formula
The four domains so deï¬ned above form a partition of ğ’«. The primary
results presented in this chapter include the following:
1) Domain 0 includes only probability distributions with ï¬nite support, that is,
there are only ï¬nitely many letters in ğ’³carrying positive probabilities.
2) Domain 1 includes distributions with thin tails such as pk âˆaâˆ’ğœ†k,
pk âˆkraâˆ’ğœ†k, and pk âˆaâˆ’ğœ†k2, where a > 1, ğœ†> 0, and r âˆˆ(âˆ’âˆ, âˆ) are
constants.
3) Domain 2 includes distributions with thick tails satisfying pk+1âˆ•pk â†’1 as
k â†’âˆ, for example, pk âˆkâˆ’ğœ†, pk âˆ(klnğœ†k)âˆ’1 where ğœ†> 1, and pk âˆeâˆ’kğ›¿
where ğ›¿âˆˆ(0, 1).
4) A relative regularity condition between two distributions (one dominates
the other) is deï¬ned. Under this condition, all distributions on a countably
inï¬nite alphabet, which are dominated by a Domain 1 distribution, must
also belong to Domain 1; and all distributions on a countably inï¬nite alpha-
bet, which dominate a Domain 2 distribution, must also belong to Domain 2.
5) Domain T is not empty.
Other relevant results presented in this chapter include the following:
1) In Domain 0, ğœn â†’0 exponentially fast for every distribution.
2) The tail index ğœn of a distribution with tail pk =âˆeâˆ’ğœ†k where ğœ†> 0 in Domain
1 perpetually oscillates between two positive constants and does not have a
limit as n â†’âˆ.
3) There is a uniform positive lower bound for lim supnâ†’âˆğœn for all distribu-
tions with positive probabilities on inï¬nitely many letters of ğ’³.
Remark 6.1
To honor the great minds of mathematics whose works mark the
trail leading to the results of this chapter, Zhang (2017) named the distribu-
tions in Domain 0 as members of the Giniâ€“Simpson family, after Corrado Gini
and Edward Hugh Simpson; those in Domain 1 as members of the Molchanov
family, after Stanislav Alekseevich Molchanov; and those in Domain 2 as the
Turingâ€“Good family, after Alan Mathison Turing and Irving John Good. In
the subsequent text of this chapter, the domains are so identiï¬ed according to
Zhang (2017).
6.2
Domains of Attraction
Let K be the eï¬€ective cardinality, or simply the cardinality when there is no
ambiguity, of ğ’³, that is, K = âˆ‘
kâ‰¥11[pk > 0]. Let â„•be the set of all positive
integers.
Lemma 6.1
If K = âˆ, then there exist a constant c > 0 and a subsequence
{nk; k â‰¥1} in â„•, satisfying nk â†’âˆas k â†’âˆ, such that ğœnk > c for all suï¬ƒ-
ciently large k.

Domains of Attraction on Countable Alphabets
213
Proof: Assume without loss of generality that pk > 0 for all k â‰¥1. Since ğœn is
invariant with respect to any permutation on the index set {k; k â‰¥1}, it can be
assumed without loss of generality that {pk} is nonincreasing in k. For every k,
let nk = âŒŠ1âˆ•pkâŒ‹. With nk so deï¬ned,
1
nk + 1 < pk â‰¤1
nk
for every k and limkâ†’âˆnk = âˆthough {nk} may not necessarily be strictly
increasing. By construction, the following are true about the nk, k â‰¥1.
1) {nk; k â‰¥1} is an inï¬nite subset of â„•.
2) Every pk is covered by the interval (1âˆ•(nk + 1), 1âˆ•nk].
3) Every interval (1âˆ•(nk + 1), 1âˆ•nk] covers at least one pk and at most ï¬nitely
many pks.
Let fn(x) = nx(1 âˆ’x)n for x âˆˆ[0, 1]. fn(x) attains its maximum at x = (n + 1)âˆ’1
with value
fn
(
1
n + 1
)
=
n
n + 1
(
1 âˆ’
1
n + 1
)n
=
(
1 âˆ’
1
n + 1
)n+1
â†’eâˆ’1.
Also
fn
(1
n
)
=
(
1 âˆ’1
n
)n
â†’eâˆ’1.
Furthermore, since f
â€²
n(x) < 0 for all x satisfying 1âˆ•(n + 1) < x < 1, the follow-
ing is true
fn
(1
n
)
< fn(x) < fn
(
1
n + 1
)
for all x satisfying 1âˆ•(n + 1) < x < 1âˆ•n.
Since fn(1âˆ•n) â†’eâˆ’1 and fn(1âˆ•(n + 1)) â†’eâˆ’1, for any arbitrarily small but
ï¬xed Îµ > 0, there exists a positive NÎµ such that for any n > NÎµ,
fn
(
1
n + 1
)
> fn
(1
n
)
> eâˆ’1 âˆ’Îµ.
Since limkâ†’âˆnk = âˆand {nk} is nondecreasing, there exists an integer
KÎµ > 0 such that nk > NÎµ for all k > KÎµ. Consider the subsequence {ğœnk; k â‰¥1}.
For any k > KÎµ,
ğœnk =
âˆ
âˆ‘
i=1
nkpi(1 âˆ’pi)nk > fnk(pk).
Since pk âˆˆ(1âˆ•(nk + 1), 1âˆ•nk] and fnk(x) is decreasing on the same interval,
fnk(pk) > fnk
(
1
nk
)
â‰¥eâˆ’1 âˆ’Îµ,
and hence
ğœnk > fnk(pk) â‰¥c = eâˆ’1 âˆ’Îµ
for all k > KÎµ.
â—½

214
Statistical Implications of Turingâ€™s Formula
Theorem 6.1
K < âˆif and only if
lim
nâ†’âˆğœn = 0.
(6.3)
Proof: Assuming that p = {pk; 1 â‰¤k â‰¤K} where K is ï¬nite and pk > 0 for all k,
1 â‰¤k â‰¤K, and denoting pâˆ§= min{pk; 1 â‰¤k â‰¤K} > 0, the necessity of (6.3)
follows the fact that as n â†’âˆ
ğœn = n
K
âˆ‘
k
pk(1 âˆ’pk)n â‰¤n
K
âˆ‘
k
pk(1 âˆ’pâˆ§)n = n(1 âˆ’pâˆ§)n â†’0.
The suï¬ƒciency of (6.3) follows the fact that if K = âˆ, then Lemma 6.1 would
provide a contradiction to (6.3).
â—½
In fact, the proof of Theorem 6.1 also establishes the following corollary.
Corollary 6.1
K < âˆif and only if ğœn â‰¤ğ’ª(nqn
0) where q0 is a constant in (0, 1).
Theorem 6.1 and Corollary 6.1 ï¬rmly characterize the members of the
Giniâ€“Simpson family as the distributions on ï¬nite alphabets. All distributions
outside of the Giniâ€“Simpson family must have positive probabilities on
inï¬nitely many letters of ğ’³. The entire class of such distributions is denoted
as ğ’«+. In fact, in the subsequent text when there is no ambiguity, ğ’«+ will
denote the entire class of distributions with a positive probability on every ğ“k
in ğ’³. For all distributions in ğ’«+, a natural group would be those for which
limnâ†’âˆğœn = âˆand so the Turingâ€“Good family is deï¬ned.
The next lemma includes two trivial but useful facts.
Lemma 6.2
1) For any real number x âˆˆ[0, 1),
1 âˆ’x â‰¥exp
(
âˆ’
x
1 âˆ’x
)
.
2) For any real number x âˆˆ(0, 1âˆ•2),
1
1 âˆ’x < 1 + 2x.
Proof: For Part 1, the function y = (1 + t)âˆ’1et is strictly increasing over [0, âˆ)
and has value 1 at t = 0. Therefore, (1 + t)âˆ’1et â‰¥1 for t âˆˆ[0, âˆ). The desired
inequality follows the change of variable x = tâˆ•(1 + t). For Part 2, the proof is
trivial.
â—½

Domains of Attraction on Countable Alphabets
215
Lemma 6.3
For any given probability distribution p = {pk; k â‰¥1} and two
constants c > 0 and ğ›¿âˆˆ(0, 1), as n â†’âˆ,
n1âˆ’ğ›¿âˆ‘
kâ‰¥1
pk(1 âˆ’pk)n â†’c > 0
if and only if
n1âˆ’ğ›¿âˆ‘
kâ‰¥1
pkeâˆ’npk â†’c > 0.
A proof of Lemma 6.3 is given in the appendix of this chapter.
Theorem 6.2
For any given probability distribution p = {pk; k â‰¥1}, if there
exists constants ğœ†> 1, c > 0 and integer k0 â‰¥1 such that for all k â‰¥k0
pk â‰¥ckâˆ’ğœ†,
(6.4)
then limnâ†’âˆğœn = âˆ.
A proof of Theorem 6.2 is given in the appendix of this chapter.
Theorem 6.2 puts distributions with power decaying tails, for example,
pk = cğœ†kâˆ’ğœ†, and those with much more slowly decaying tails, for example,
pk = cğœ†(k ln ğœ†k)âˆ’1, where ğœ†> 1 and cğœ†> 0 is a constant, which may depend on
ğœ†, in the Turingâ€“Good family.
An interesting question at this point is whether there exist distributions with
thinner tails, thinner than those with power decaying tails, in the Turingâ€“Good
family. An aï¬ƒrmative answer is given by Theorem 6.3 below with a suï¬ƒcient
condition on p = {pk; k â‰¥1}.
Theorem 6.3
If pk+1âˆ•pk â†’1 as k â†’âˆ, then ğœn â†’âˆas n â†’âˆ.
The proof of Theorem 6.3 requires the following lemma.
Lemma 6.4
For every suï¬ƒciently large n, let kn = max{k; pk â‰¥1âˆ•n}. If there
exist a constant c âˆˆ(0, 1) and a positive integer Kc such that for all k, k â‰¥Kc,
pk+1âˆ•pk â‰¥c, then 1 â‰¤npkn < câˆ’1.
Proof: 1 â‰¤npkn holds by deï¬nition. It suï¬ƒces to show only that npkn â‰¤câˆ’1.
Toward that end, let n0 be suï¬ƒciently large so that pkn0+1âˆ•pkn0 â‰¥c, and there-
fore pkn+1âˆ•pkn â‰¥c for every n â‰¥n0. But since cpkn â‰¤pkn+1 < 1âˆ•n by deï¬nition,
it follows that npkn < câˆ’1.
â—½
Proof of Theorem 6.3: For any given arbitrarily small Îµ > 0, there exists
a KÎµ such that for all k â‰¥KÎµ, pk+1âˆ•pk > 1 âˆ’Îµ. There exists a suï¬ƒciently large

216
Statistical Implications of Turingâ€™s Formula
nÎµ such that for any n > nÎµ, kn > KÎµ where kn = max{k; pk â‰¥1âˆ•n}. For every
n > nÎµ,
1) a) pkn+1 > (1 âˆ’Îµ)pkn;
b) pkn+2 > (1 âˆ’Îµ)pkn+1 > (1 âˆ’Îµ)2pkn, Â· Â· Â·; and hence
c) pkn+j > (1 âˆ’Îµ)jpkn, for j = 1, 2, Â· Â· Â·;
and
2) pkn+j < 1âˆ•n, for j = 1, 2, Â· Â· Â·.
Therefore, for every n > nÎµ,
ğœn > n
âˆ‘
kâ‰¥kn
pk(1 âˆ’pk)n
= n
âˆ‘
jâ‰¥0
pkn+j(1 âˆ’pkn+j)n
> n
âˆ‘
jâ‰¥0
(1 âˆ’Îµ)jpkn(1 âˆ’1âˆ•n)n
= npkn(1 âˆ’1âˆ•n)n âˆ‘
jâ‰¥0
(1 âˆ’Îµ)j
= [npkn] [(1 âˆ’1âˆ•n)n]
[1
Îµ
]
.
In the last expression above, the ï¬rst factor is bounded below by 1, the second
factor is bounded below by eâˆ’1âˆ•2 for large n, and the third factor can be taken
over all bounds since Îµ is arbitrarily small. Hence, ğœn â†’âˆ.
â—½
Example 6.1
Let pk = ckâˆ’ğœ†for some constants c > 0 and ğœ†> 1. Since
pk+1âˆ•pk = [kâˆ•(k + 1)]ğœ†â†’1, Theorem 6.3 applies and ğœn â†’âˆ.
Example 6.2
Let pk = ckâˆ’1(ln k)âˆ’ğœ†for some constants c > 0 and ğœ†> 1. Since
pk+1âˆ•pk = [kâˆ•(k + 1)][ln kâˆ•ln(k + 1)]ğœ†â†’1, Theorem 6.3 applies and ğœn â†’âˆ.
Example 6.3
Let pk = ceâˆ’kğ›¿for some constants c > 0 and ğ›¿âˆˆ(0, 1). Since,
applying Lâ€™HÃ´pitalâ€™s rule whenever necessary,
pk+1âˆ•pk = exp(kğ›¿âˆ’(k + 1)ğ›¿) = exp
(
1âˆ’
(
1+ 1
k
)ğ›¿
kâˆ’ğ›¿
)
â†’e0 = 1,
Theorem 6.3 applies and ğœn â†’âˆ.

Domains of Attraction on Countable Alphabets
217
Example 6.4
Let pk = ceâˆ’kâˆ•ln k for some constants c > 0. Since, applying
Lâ€™HÃ´pitalâ€™s rule whenever necessary,
pk+1âˆ•pk = exp
(
k
ln k âˆ’
k + 1
ln(k + 1)
)
= exp
(
k
ln k âˆ’
k
ln(k + 1) âˆ’
1
ln(k + 1)
)
âˆ¼exp
(
k
ln k âˆ’
k
ln(k + 1)
)
= exp
â›
âœ
âœ
âœâ
ln
(
1 + 1
k
)
(
ln k ln(k+1)
k
)
â
âŸ
âŸ
âŸâ 
âˆ¼exp
â›
âœ
âœ
âœâ
(
1 + 1
k
)âˆ’1
ln k ln(k + 1) âˆ’ln(k + 1) âˆ’
k
k+1 ln k
â
âŸ
âŸ
âŸâ 
â†’e0 = 1,
Theorem 6.3 applies and ğœn â†’âˆ.
Example 6.5
Let pk = ceâˆ’k for some constant c > 0. Since pk+1âˆ•pk = eâˆ’1 < 1,
Theorem 6.3 does not apply. In fact, it can be shown that ğœn is bounded above by
a constant (see Lemma 6.6).
Examples 6.3 and 6.4 are particularly interesting and somewhat surprising.
In the extreme value theory for continuous random variables, the maximum
of a sample under the density function f (x) = ceâˆ’xğ›¿where ğ›¿âˆˆ(0, 1) or f (x) =
ceâˆ’xâˆ•ln x converges weakly to a member of the Gumbel family to which extreme
values under other thin-tailed distributions converge. In the current setting,
where the domains are deï¬ned by the limiting behavior of ğœn, pk = ceâˆ’kğ›¿and
pk = ceâˆ’kâˆ•ln k belong to the domain shared by thick-tailed discrete distributions
such as pk = ckâˆ’ğœ†where c > 0 and ğœ†> 1.
In view of Lemma 6.3 and Theorems 6.1 and 6.2, Domain 1, or the Molchanov
family, has a more intuitive deï¬nition as given in the following lemma.
Lemma 6.5
A distribution p on ğ’³belongs to the Molchanov family if and
only if
1) the eï¬€ective cardinality of ğ’³is countably inï¬nite, that is, K = âˆ, and
2) ğœn â‰¤up for all n, where up > 0 is a constant that may depend on p.
The proof is left as an exercise.
Lemma 6.6
For any p = {pk} âˆˆğ’«+, if there exists an integer k0 â‰¥1 such that
pk = c0eâˆ’k, where c0 > 0 is a constant, for all k â‰¥k0, then

218
Statistical Implications of Turingâ€™s Formula
1) ğœn â‰¤u for some upper bound u > 0; and
2) limnâ†’âˆğœn does not exist.
A proof of Lemma 6.6 is given in the appendix of this chapter.
A similar proof to that of Lemma 6.6 immediately gives Theorem 6.4 with a
slightly more general statement.
Theorem 6.4
For any given probability distribution p = {pk; k â‰¥1}, if there
exists constants a > 1 and integer k0 â‰¥1 such that for all k â‰¥k0
pk = caâˆ’k,
(6.5)
then
1) ğœn â‰¤ua for some upper bound ua > 0, which may depend on a; and
2) limnâ†’âˆğœn does not exist.
Theorem 6.4 puts distributions with tails of geometric progression, for
example, pk = cğœ†eâˆ’ğœ†k where ğœ†> 0 and cğœ†> 0 are constants or pk = 2âˆ’k in the
Molchanov family, a family of distributions with perpetually oscillating tail
indices.
Next, a notion of relative dominance of one probability distribution over
another is deï¬ned on a countable alphabet within ğ’«+. Let #A denote the
cardinality of a set A.
Deï¬nition 6.2
Let qâˆ—âˆˆğ’«+ and p âˆˆğ’«+ be two distributions on ğ’³, and let
q = {qk} be a nonincreasingly ordered version of qâˆ—. qâˆ—is said to dominate p if
#{i; pi âˆˆ(qk+1, qk], i â‰¥1} â‰¤M < âˆ
for every k â‰¥1, where M is a ï¬nite positive integer.
It is easy to see that the notion of dominance by Deï¬nition 6.2 is a tail prop-
erty, and that it is transitive, that is, if p1 dominates p2 and p2 dominates p3,
then p1 dominates p3. It says in essence that if p is dominated by q, then the pis
do not get overly congregated locally into some intervals deï¬ned by the qks.
The following examples illustrate the notion of dominance by Deï¬nition 6.2.
Example 6.6
Let pk = c1eâˆ’k2 and qk = c2eâˆ’k for all k â‰¥k0 for some integer
k0 â‰¥1 and other two constants c1 > 0 and c2 > 0. For every suï¬ƒciently large k,
suppose
pj = c1eâˆ’j2 â‰¤qk = c2eâˆ’k,
then
âˆ’j2 â‰¤ln(c2âˆ•c1) âˆ’k

Domains of Attraction on Countable Alphabets
219
and
j + 1 â‰¥[k + ln(c1âˆ•c2) ]1âˆ•2 + 1.
It follows that
pj+1 = c1eâˆ’(j+1)2
â‰¤c1e
âˆ’
(âˆš
k+ln(c1âˆ•c2)+1
)2
= c1eâˆ’(k+ln(c1âˆ•c2)+1)âˆ’2
âˆš
k+ln(c1âˆ•c2)
= c2eâˆ’(k+1)âˆ’2
âˆš
k+ln(c1âˆ•c2)
= c2eâˆ’(k+1) eâˆ’2
âˆš
k+ln(c1âˆ•c2)
â‰¤c2eâˆ’(k+1) = qk+1.
This means that if pj âˆˆ(qk+1, qk], then necessarily pj+1 âˆ‰(qk+1, qk], which implies
that each interval (qk+1, qk] can contain only one pj at most for a suï¬ƒciently large
k, that is, k â‰¥k00 âˆ¶= max{k0, ln(c2âˆ•c1)}. Since there are only ï¬nite pjs covered
by âˆª1â‰¤k<k00(qk, qk+1], q = {qk} dominates p = {pi}.
Example 6.7
Let pk = c1aâˆ’k and qk = c2bâˆ’k for all k â‰¥k0 for some integer k0 â‰¥
1 and other two constants a > b > 1. For every suï¬ƒciently large k, suppose pj =
c1aâˆ’j â‰¤qk = c2bâˆ’k, then âˆ’j ln a â‰¤ln(c2âˆ•c1) âˆ’k ln b and j + 1 â‰¥k(ln bâˆ•ln a) +
1 + ln(c1âˆ•c2) âˆ•ln a. It follows that
pj+1 = c1a
âˆ’
(
k ln b
ln a +1+
ln(c1âˆ•c2)
ln a
)
= c1a
âˆ’
(
klog ab+1+
ln(c1âˆ•c2)
ln a
)
= c1bâˆ’kaâˆ’1aâˆ’
ln(c1âˆ•c2)
ln a
â‰¤c1bâˆ’(k+1)aâˆ’log a (c1âˆ•c2)
= c2bâˆ’(k+1)
= qk+1.
By a similar argument as that in Example 6.6, q = {qk} dominates p = {pi}.
Example 6.8
Let pk = c1kâˆ’reâˆ’ğœ†k for some integer k0 â‰¥1 and constants ğœ†> 0
and r > 0, and qk = c2eâˆ’ğœ†k for all k â‰¥k0. Suppose for a k â‰¥k0 there is a j such
that pj = c1jâˆ’reâˆ’ğœ†j âˆˆ(qk+1 = c2eâˆ’ğœ†(k+1), qk = c2eâˆ’ğœ†k], then
pj+1 = c1(j + 1)âˆ’reâˆ’ğœ†(j+1)
= c1(j + 1)âˆ’reâˆ’ğœ†jeâˆ’ğœ†
â‰¤c1jâˆ’reâˆ’ğœ†jeâˆ’ğœ†
â‰¤c2eâˆ’ğœ†keâˆ’ğœ†= qk+1,

220
Statistical Implications of Turingâ€™s Formula
which implies that there is at most one pj in (qk+1, qk] for every suï¬ƒciently large
k. Therefore, q = {qk} dominates p = {pi}.
Example 6.9
Let pk = c1kreâˆ’ğœ†k for some integer k0 â‰¥1 and constants ğœ†> 0
and r > 0, and qk = c2eâˆ’(ğœ†âˆ•2)k for all k â‰¥k0. Suppose for any suï¬ƒciently large j,
j â‰¥j0 âˆ¶= [eğœ†âˆ•(2r) âˆ’1]âˆ’1,
pj = c1jreâˆ’ğœ†j âˆˆ(qk+1 = c2eâˆ’(ğœ†âˆ•2)(k+1), qk = c2eâˆ’(ğœ†âˆ•2)k]
for some suï¬ƒciently large k â‰¥k0, then
pj+1 = c1(j + 1)reâˆ’ğœ†(j+1)
= c1(j + 1)reâˆ’ğœ†jeâˆ’ğœ†
= c1jreâˆ’ğœ†jeâˆ’ğœ†(j + 1)r
jr
â‰¤c2eâˆ’ğœ†
2 keâˆ’ğœ†
(j + 1
j
)r
= c2eâˆ’ğœ†
2 (k+1)eâˆ’ğœ†
2
(j + 1
j
)r
â‰¤qk+1eâˆ’ğœ†
2
(j0 + 1
j0
)r
= qk+1
which implies that there is at most one pj in (qk+1, qk] for every suï¬ƒciently large
k. Therefore, q = {qk} dominates p = {pi}.
Example 6.10
Let pk = qk for all k â‰¥1. q = {qk} and p = {pk} dominate each
other.
While in each of Examples 6.6 through 6.9, the dominating distribution q has
a thicker tail than p in the usual sense, the dominance of Deï¬nition 6.2 in gen-
eral is not implied by such a thinner/thicker tail relationship. This is so because a
distribution p âˆˆğ’«+, satisfying pk â‰¤qk for all suï¬ƒciently large k, could exist yet
congregate irregularly to have an unbounded supkâ‰¥1#{pi; pi âˆˆ(qk+1, qk], i â‰¥1}.
One such example is given in Section 6.3. In this regard, the dominance of Def-
inition 6.2 is more appropriately considered as a regularity condition. However,
it may be interesting to note that the said regularity is a relative one in the sense
that the behavior of p is regulated by a reference distribution q. This relative
regularity gives an umbrella structure in the Molchanov family as well as in the
Turingâ€“Good family, as demonstrated by Theorems 6.5 and 6.6 below.
Theorem 6.5
If two distributions p and q in ğ’«+ on a same countably inï¬nite
alphabet ğ’³are such that q is in the Molchanov family and q dominates p, then
p belongs to the Molchanov family.

Domains of Attraction on Countable Alphabets
221
Proof: Without loss of generality, it may be assumed that q is nonincreas-
ingly ordered. For every suï¬ƒciently large n, there exists a kn such that
1
n+1 âˆˆ(qkn+1, qkn]. Noting that the function np(1 âˆ’p)n increases in p over
(0, 1âˆ•(n + 1)], attains its maximum value of [1 âˆ’1âˆ•(n + 1)]n+1 < eâˆ’1 at
p = 1âˆ•(n + 1), and decreases over [1âˆ•(n + 1), 1], consider
ğœn(p) =
âˆ‘
kâ‰¥1
npk(1 âˆ’pk)n
=
âˆ‘
kâˆ¶pkâ‰¤qkn+1
npk(1 âˆ’pk)n +
âˆ‘
kâˆ¶qkn+1<pkâ‰¤qkn
npk(1 âˆ’pk)n
+
âˆ‘
kâˆ¶pk>qkn
npk(1 âˆ’pk)n
â‰¤M
âˆ‘
kâ‰¥kn+1
nqk(1 âˆ’qk)n +
âˆ‘
k;qkn+1<pkâ‰¤qkn
eâˆ’1 + M
âˆ‘
1â‰¤kâ‰¤kn
nqk(1 âˆ’qk)n
= M
âˆ‘
kâ‰¥1
nqk(1 âˆ’qk)n +
âˆ‘
kâˆ¶qkn+1<pkâ‰¤qkn
eâˆ’1
â‰¤Mğœn(q) + Meâˆ’1 < âˆ,
where M is as in Deï¬nition 6.2 and it exists because the assumed condition that
q dominates p. The desired result immediately follows.
â—½
Corollary 6.2
Any distribution p on a countably inï¬nite alphabet ğ’³satis-
fying pk = aeâˆ’ğœ†k, pk = beâˆ’ğœ†k2, or pk = ckreâˆ’ğœ†k for all k â‰¥k0, where k0 â‰¥1, ğœ†> 0,
r âˆˆ(âˆ’âˆ, +âˆ), a > 0, b > 0, and c > 0 are constants, is in the Molchanov family.
Proof: The result is immediate following Theorem 6.5 and Examples 6.6
through 6.9.
â—½
Theorem 6.6
If two distributions p and q in ğ’«+ on a same countably inï¬nite
alphabet ğ’³are such that p is in the Turingâ€“Good family and q dominates p,
then q belongs to the Turingâ€“Good family.
Proof: In the proof of Theorem 6.5, it is established that if q dominates p, then
ğœn(p) â‰¤Mğœn(q) + Meâˆ’1
for some positive constant M. The fact ğœn(p) â†’âˆimplies ğœn(q) â†’âˆ, as
n â†’âˆ.
â—½
Figures 6.1â€“6.3 show graphic representations of ğœn for several distributions
in various domains. Figure 6.1 plots ğœn for pk = 0.01 for k = 1,â€¦, 100 and qk =
0.02 for k = 1,â€¦, 50. The tail indices are plotted on a same vertical scale from 0
to 40, ranging from n = 1 to n = 1000. When n increases indeï¬nitely, the rapid
decay in both indices, as suggested by Theorem 6.1 and Lemma 6.1, is visible.

222
Statistical Implications of Turingâ€™s Formula
40
35
30
25
20
15
10
0
100
200
300
400
500
600
700
800
900
1000
5
0
Figure 6.1 ğœn of pk = 0.01 (upper) for k = 1,â€¦, 100 and qk = 0.02 (lower) for k = 1,â€¦, 50.
1.1
1
0.9
0.8
0.7
0.6
0.5
0
100
200
300
400
500
600
(a)
700
800
900
1000
200
300
400
500
600
(b)
700
800
900
1000
0.85
0.83
0.81
0.79
0.77
0.75
0.73
0.71
0.67
0.65
0.69
0.4
0.98
0.985
0.99
0.995
1
1.005
Figure 6.2 ğœn of pk = (e âˆ’1)eâˆ’k (upper) and pk = 3 Ã— 4âˆ’k (lower), k â‰¥1. (a) ğœn from n = 1 to
n = 1000, one scale. (b) ğœn from n = 200 to n = 1000, two scales.

Domains of Attraction on Countable Alphabets
223
0
0
5
10
15
20
25
200
100
300
400
500
600
700
800
900
1000
Figure 6.3 ğœn of pk = c1kâˆ’2 (upper) and qk = c2kâˆ’3 (lower), k â‰¥1.
Figure 6.2 gives ğœn for pk = (e âˆ’1)eâˆ’k and qk = 3 Ã— 4âˆ’k, k â‰¥1. In Figure 6.2a,
the pair of tail indices are plotted on the same vertical scale for ğœn from 0.4 to 1.1,
ranging from n = 1 to n = 1000. The values for ğœn seem stable for large values of
n in Figure 6.2a. However, when the tail indices are plotted on diï¬€erent vertical
scales, on the left from 0.98 to 1.005 for pk and on the right from 0.65 to 0.85
for qk, ranging from n = 200 to n = 1000, the oscillating patterns suggested by
Theorem 6.4 become visible.
Figure 6.3 gives ğœn for pk = c1kâˆ’2 and qk = c2kâˆ’3, k â‰¥1. The pair of tail indices
are plotted on the same vertical scale from 0 to 25, ranging from n = 1 to
n = 1000. The divergent patterns suggested by Theorem 6.2 are visible.
6.3
Examples and Remarks
Three constructed examples are given in this section, and each illustrates a
point of interest. The ï¬rst constructed example shows that the notion of thinner
tail, in the usual sense of pk â‰¤qk for k â‰¥k0 where k0 â‰¥1 is some ï¬xed integer
and p = {pk} and q = {qk} are two distributions, does not imply dominance of
q over p.
Example 6.11
Consider any strictly decreasing distribution q = {qk; k â‰¥1} âˆˆ
ğ’«+ and the following grouping of the index set {k; k â‰¥1}.
G1 ={1},
G2 ={2, 3},
â‹®,
Gm =
{m(m âˆ’1)
2
+ 1,â€¦, m(m âˆ’1)
2
+ m
}
,
â‹®.

224
Statistical Implications of Turingâ€™s Formula
{Gm; m â‰¥1} is a partition of the index set {k; k â‰¥1} and each group Gm
contains m consecutive indices. A new distribution p = {pk} is constructed
according to the following steps:
1) For each m â‰¥2, let pk = qm(mâˆ’1)âˆ•2+m for all k âˆˆGm.
2) p1 = 1 âˆ’âˆ‘
kâ‰¥2 pk.
In the ï¬rst step, m(m âˆ’1)âˆ•2 + m = m(m + 1)âˆ•2 is the largest index in Gm, and
therefore qm(m+1)âˆ•2 is the smallest qk with index k âˆˆGm. Since
0 â‰¤
âˆ‘
kâ‰¥2
pk =
âˆ‘
mâ‰¥2
mqm(m+1)âˆ•2 <
âˆ‘
kâ‰¥2
qk â‰¤1,
p1 so assigned is a probability. The distribution p = {pk} satisï¬es pk â‰¤qk
for every k â‰¥2 = k0. However, the number of terms of pi in the interval
(qm(m+1)âˆ•2+1, qm(m+1)âˆ•2] is at least m and it increases indeï¬nitely as m â†’âˆ; and
hence q does not dominate p.
The second constructed example shows that the notion of dominance of
q = {qk} over p = {pk}, as deï¬ned in Deï¬nition 6.2, does not imply that p has
thinner tail than q, in the usual sense of pk â‰¤qk for k â‰¥k0 where k0 â‰¥1 is
some ï¬xed integer.
Example 6.12
Consider any strictly decreasing distribution q = {qk; k â‰¥
1} âˆˆğ’«+ and the following grouping of the index set {k; k â‰¥1}.
G1 = {1, 2}, G2 = {3, 4},â€¦, Gm = {2m âˆ’1, 2m}, â€¦ .
{Gm; m â‰¥1} is a partition of the index set {k; k â‰¥1} and each group Gm con-
tains two consecutive indices, the ï¬rst one odd and the second one even. The
construction of a new distribution p = {pk} is as follows: for each group Gm with
its two indices k = 2m âˆ’1 and k + 1 = 2m, let pk = pk+1 = (qk + qk+1)âˆ•2. With
the new distribution p = {pk} so deï¬ned, one has p2m < q2m and p2mâˆ’1 > q2mâˆ’1
for all m â‰¥1. Clearly q dominates p (p dominates q as well), but p does not have
a thinner tail in the usual sense.
At this point, it becomes clear that the notation of dominance of Deï¬ni-
tion 6.2 and the notation of thinner/thicker tail in the usual sense are two inde-
pendent notions.
The next constructed example shows that there exists a distribution such
that the associated ğœn approaches inï¬nity along one subsequence of â„•and is
bounded above along another subsequence of â„•, hence belonging to Domain
T. Domain T is not empty.
Example 6.13
Consider the probability sequence qj = 2âˆ’j, for j = 1, 2,â€¦ ,
along with a diï¬€usion sequence di = 2i, for i = 1, 2,â€¦ . A probability sequence
{pk}, for k = 1, 2,â€¦ , is constructed by the following steps:

Domains of Attraction on Countable Alphabets
225
1st:
a) Take the ï¬rst value of di, d1 = 21, and assign the ï¬rst 2d1 = 22 = 4 terms
of qj,
q1 = 2âˆ’1, q2 = 2âˆ’2, q3 = 2âˆ’3, q4 = 2âˆ’4,
to the ï¬rst four terms of pk,
p1 = 2âˆ’1, p2 = 2âˆ’2, p3 = 2âˆ’3, p4 = 2âˆ’4.
b) Take the next unassigned term in qj, q5 = 2âˆ’5, and diï¬€use it into d1 = 2
equal terms, 2âˆ’6 and 2âˆ’6.
i) Starting at q5 in the sequence {qj}, look forwardly (j > 5) for terms
greater or equal to 2âˆ’6, if any, continue to assign them to pk. In this
case, there is only one such term q6 = 2âˆ’6 and it is assigned to p5 = 2âˆ’6.
ii) Take the d1 = 2 diï¬€used terms and assign them to p6 = 2âˆ’6 and p7 =
2âˆ’6. At this point, the ï¬rst few terms of the partially assigned sequence
{pk} are
p1 = 2âˆ’1,
p2 = 2âˆ’2,
p3 = 2âˆ’3,
p4 = 2âˆ’4,
p5 = 2âˆ’6,
p6 = 2âˆ’6,
p7 = 2âˆ’6.
2nd:
a) Take the next value of di, d2 = 22, and assign the next 2d2 = 23 = 8 unused
terms of qj,
q7 = 2âˆ’7, â€¦ , q14 = 2âˆ’14,
to the next eight terms of pk,
p8 = 2âˆ’7, â€¦ , p15 = 2âˆ’14.
b) Take the next unassigned term in qj, q15 = 2âˆ’15, and diï¬€use it into d2 = 4
equal terms of 2âˆ’17 each.
i) Starting at q15 in the sequence of {qj}, look forwardly (j > 15) for terms
greater or equal to 2âˆ’17, if any, continue to assign them to pk. In this case,
there are 2 such terms
q16 = 2âˆ’16, q17 = 2âˆ’17,
and they are assigned to
p16 = 2âˆ’16, p17 = 2âˆ’17.
ii) Take the d2 = 22 = 4 diï¬€used terms and assign them to
p18 = 2âˆ’17, â€¦ , p21 = 2âˆ’17.

226
Statistical Implications of Turingâ€™s Formula
At this point, the ï¬rst few terms of the partially assigned sequence {pk}
are
p1 = 2âˆ’1,
p2 = 2âˆ’2,
p3 = 2âˆ’3,
p4 = 2âˆ’4,
p5 = 2âˆ’6,
p6 = 2âˆ’6,
p7 = 2âˆ’6,
p8 = 2âˆ’7,
p9 = 2âˆ’8,
â€¦ ,
p15 = 2âˆ’14, p16 = 2âˆ’16,
p17 = 2âˆ’17, p18 = 2âˆ’17,
â€¦ ,
p21 = 2âˆ’17.
ith:
a) In general, take the next value of di, say di = 2i, and assign the next
2di = 2i+1 unused terms of qj, say
qj0 = 2âˆ’j0, â€¦ , qj0+2i+1âˆ’1 = 2âˆ’(j0+2i+1âˆ’1),
to the next 2di = 2i+1 terms of pk, say
pk0 = 2âˆ’j0, â€¦ , pk0+2i+1âˆ’1 = 2âˆ’(j0+2i+1âˆ’1).
b) Take the next unassigned term in qj,
qj0+2i+1 = 2âˆ’(j0+2i+1),
and diï¬€use it into di = 2i equal terms, 2âˆ’(j0+i+2i+1) each.
i) Starting at qj0+2i+1 in the sequence of {qj}, look forwardly (j > j0 + 2i+1)
for terms greater or equal to 2âˆ’(j0+i+2i+1), if any, continue to assign them
to pk. Denote the last assigned pk as pk0.
ii) Take the di = 2i diï¬€used terms and assign them to
pk0+1 = 2âˆ’(j0+i+2i+1), â€¦ , pk0+2i = 2âˆ’(j0+i+2i+1).
In essence, the sequence {pk} is generated based on the sequence {qj} with
inï¬nitely many selected jâ€™s at each of which qj is diï¬€used into increasingly
many equal probability terms according a diï¬€usion sequence {di}. The diï¬€used
sequence is then rearranged in a nonincreasing order.
By construction, it is clear that the sequence {pk; k â‰¥1} satisï¬es the following
properties:
îˆ­1: {pk} is a probability sequence in a nonincreasing order.
îˆ­2: As k increases, {pk} is a string of segments alternating between two diï¬€erent
types: (i) a strictly decreasing segment and (ii) a segment (a run) of equal
probabilities.
îˆ­3: As k increases, the length of the last run increases and approaches inï¬nity.
îˆ­4: In each run, there are exactly di + 1 equal terms, di of which are diï¬€used
terms and 1 of which belongs to the original sequence qj.
îˆ­5: Between two consecutive runs (with lengths di + 1 and di+1 + 1, respec-
tively), the strictly decreasing segment in the middle has at least 2di+1 terms
and satisï¬es
2di+1 = 4di = di + 3di > di + di+1.
îˆ­6: For any k, 1âˆ•pk is a positive integer.

Domains of Attraction on Countable Alphabets
227
Next it is to show that there is a subsequence {ni} âˆˆâ„•such that ğœni deï¬ned
with {pk} approaches inï¬nity. Toward that end, consider the subsequence
{pki; i â‰¥1} of {pk} where the index ki is such that pki is the ï¬rst term in the ith
run segment. Let {ni} = {1âˆ•pki} which by îˆ­6 is a subsequence of â„•. By îˆ­3 and
îˆ­4,
ğœni = ni
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)ni
> ni(di + 1)pki(1 âˆ’pki)ni
= (di + 1)
(
1 âˆ’1
ni
)ni
â†’âˆ.
Consider next the subsequence {pkiâˆ’(di+1); i â‰¥1} of {pk} where the index ki is
such that pki is the ï¬rst term in the ith run segment, and therefore pkiâˆ’(di+1) is the
(di + 1)th term counting backwards from pkiâˆ’1, into the preceding segment of at
least 2di strictly decreasing terms. Let
{mi} =
{
1
pkiâˆ’(di+1)
âˆ’1
}
(so pkiâˆ’(di+1) = (mi + 1)âˆ’1) which by îˆ­6 is a subsequence of â„•.
ğœmi = mi
âˆ‘
kâ‰¥1
pk(1 âˆ’pk)mi
= mi
âˆ‘
kâ‰¤kiâˆ’(di+1)
pk(1 âˆ’pk)mi + mi
âˆ‘
kâ‰¥kiâˆ’di
pk(1 âˆ’pk)mi
âˆ¶= ğœmi,1 + ğœmi,2.
Before proceeding further, let several detailed facts be noted.
1) The function np(1 âˆ’p)n increases in [0, 1âˆ•(n + 1)], attains maximum at
p = 1âˆ•(n + 1), and decreases in [1âˆ•(n + 1), 1].
2) Since pkiâˆ’(di+1) = (mi + 1)âˆ’1, by îˆ­1 each summand in ğœmi,1 is bounded above
by
mipkiâˆ’(di+1)(1 âˆ’pkiâˆ’(di+1))mi
and each summand in ğœmi,2 is bounded above by
mipkiâˆ’di(1 âˆ’pkiâˆ’di)mi.
3) By îˆ­4 and îˆ­5, for each diï¬€used term of pkâ€² with kâ€² â‰¤ki âˆ’(di + 1) in a run,
there is a diï¬€erent nondiï¬€used term pkâ€²â€² with kâ€²â€² â‰¤ki âˆ’(di + 1) such that
pkâ€² > pkâ€²â€² and therefore
mi pkâ€²(1 âˆ’pkâ€²)mi â‰¤mi pkâ€²â€²(1 âˆ’pkâ€²â€²)mi;

228
Statistical Implications of Turingâ€™s Formula
and similarly, for each diï¬€used term of pkâ€² with kâ€² â‰¥ki âˆ’di in a run, there
is a diï¬€erent nondiï¬€used term pkâ€²â€² with kâ€²â€² â‰¥ki âˆ’di such that pkâ€² < pkâ€²â€² and
therefore
mipkâ€² (1 âˆ’pkâ€² )mi â‰¤mipkâ€²â€²(1 âˆ’pkâ€²â€²)mi.
These facts imply that
ğœmi = ğœmi,1 + ğœmi,2
= mi
âˆ‘
kâ‰¤kiâˆ’(di+1)
pk(1 âˆ’pk)mi + mi
âˆ‘
kâ‰¥kiâˆ’di
pk(1 âˆ’pk)mi
â‰¤2mi
âˆ‘
jâ‰¥1
qj(1 âˆ’qj)mi < âˆ
and the last inequality above is due to Corollary 6.2.
While the domains of attraction on alphabets have probabilistic merit, the
statistical implication is also quite signiï¬cant. Z1,v in (3.26) is an unbiased esti-
mator of ğœ1,v, for v = 1,â€¦, n âˆ’1, as is demonstrated in Chapter 2, and therefore
Ì‚ğœv = vZ1,v.
(6.6)
is an unbiased estimator of ğœv = vğœv. In fact, Zhang and Zhou (2010) established
several useful statistical properties of Ì‚ğœv, including the asymptotic normality.
The availability of Ì‚ğœv gives much added merit to the discussion of the domains
of attraction on alphabets as presented in this chapter. Speciï¬cally the fact that
the asymptotic behavior of ğœn characterizes the tail probability of the underlying
p and the fact that the trajectory of ğœv up to v = n âˆ’1 is estimable suggest that
much could be revealed by a suï¬ƒciently large sample.
6.4
Appendix
6.4.1
Proof of Lemma 6.3
Proof: Let ğ›¿âˆ—= ğ›¿âˆ•8. Consider the partition of the index set
{k; k â‰¥1} = I âˆªII
where
I = {k âˆ¶pk â‰¤1âˆ•n1âˆ’ğ›¿âˆ—}
and
II = {k âˆ¶pk > 1âˆ•n1âˆ’ğ›¿âˆ—} .
Since peâˆ’np has a negative derivative with respect to p on interval (1âˆ•n, 1] and
hence on (1âˆ•n1âˆ’ğ›¿âˆ—, 1] for large n, pkeâˆ’npk attains its maximum at pk = 1âˆ•n1âˆ’ğ›¿âˆ—
for every k âˆˆII. Therefore, noting that there are at most n1âˆ’ğ›¿âˆ—indices in II,

Domains of Attraction on Countable Alphabets
229
0 â‰¤n1âˆ’ğ›¿âˆ‘
kâˆˆII
pk(1 âˆ’pk)n
â‰¤n1âˆ’ğ›¿âˆ‘
kâˆˆII
pkeâˆ’npk
â‰¤n1âˆ’ğ›¿âˆ‘
kâˆˆII
(
1
n1âˆ’ğ›¿âˆ—eâˆ’
n
n1âˆ’ğ›¿âˆ—)
â‰¤n1âˆ’ğ›¿n1âˆ’ğ›¿âˆ—(
1
n1âˆ’ğ›¿âˆ—eâˆ’
n
n1âˆ’ğ›¿âˆ—)
= n1âˆ’ğ›¿eâˆ’nğ›¿âˆ—â†’0.
Thus
lim
nâ†’âˆn1âˆ’ğ›¿âˆ‘
kâ‰¥1
pk(1 âˆ’pk)n = lim
nâ†’âˆn1âˆ’ğ›¿âˆ‘
kâˆˆI
pk(1 âˆ’pk)n
(6.7)
and
lim
nâ†’âˆn1âˆ’ğ›¿âˆ‘
kâ‰¥1
pkeâˆ’npk = lim
nâ†’âˆn1âˆ’ğ›¿âˆ‘
kâˆˆI
pkeâˆ’npk.
(6.8)
On the other hand, since 1 âˆ’p â‰¤eâˆ’p for all p âˆˆ[0, 1],
n1âˆ’ğ›¿âˆ‘
kâˆˆI
pk(1 âˆ’pk)n â‰¤n1âˆ’ğ›¿âˆ‘
kâˆˆI
pkeâˆ’npk.
Furthermore, applying Parts 1 and 2 of Lemma 6.2 in the ï¬rst and the third
steps in the following, respectively, leads to
n1âˆ’ğ›¿âˆ‘
kâˆˆI
pk(1 âˆ’pk)n â‰¥n1âˆ’ğ›¿âˆ‘
kâˆˆI
pk exp
(
âˆ’npk
1 âˆ’pk
)
â‰¥n1âˆ’ğ›¿âˆ‘
kâˆˆI
pk exp
(
âˆ’
npk
1 âˆ’supiâˆˆIpi
)
â‰¥n1âˆ’ğ›¿âˆ‘
kâˆˆI
exp (âˆ’2n(supiâˆˆI)2) pkeâˆ’npk.
Noting the fact that
lim
nâ†’âˆexp(âˆ’2n(supiâˆˆI)2) = 1
uniformly by the deï¬nition of I,
lim
nâ†’âˆn1âˆ’ğ›¿âˆ‘
kâˆˆI
pk(1 âˆ’pk)n = lim
nâ†’âˆn1âˆ’ğ›¿âˆ‘
kâˆˆI
pkeâˆ’npk,
and hence, by (6.7) and (6.8), the lemma follows.
â—½
6.4.2
Proof of Theorem 6.2
Proof: For clarity, the proof is given in two cases, respectively:
1) pk = ckâˆ’ğœ†for all k â‰¥k0 for some k0 > 1, and
2) pk â‰¥ckâˆ’ğœ†for all k â‰¥k0 for some k0 > 1.

230
Statistical Implications of Turingâ€™s Formula
Case 1: Assuming pk = ckâˆ’ğœ†for all k â‰¥k0, it suï¬ƒces to consider the partial
series âˆ‘
kâ‰¥k0npk(1 âˆ’pk)n. First consider
n1âˆ’1
ğœ†
âˆ
âˆ‘
k=k0
pkeâˆ’npk = n1âˆ’1
ğœ†
âˆ
âˆ‘
k=k0
ckâˆ’ğœ†eâˆ’nckâˆ’ğœ†=
âˆ
âˆ‘
k=k0
fn(k)
where
fn(x) = n1âˆ’1
ğœ†cxâˆ’ğœ†eâˆ’ncxâˆ’ğœ†.
Since it is easily veriï¬ed that
f
â€²
n(x) = âˆ’ğœ†cn1âˆ’1
ğœ†xâˆ’(ğœ†+1)(1 âˆ’ncxâˆ’ğœ†)eâˆ’ncxâˆ’ğœ†,
it can be seen that fn(x) increases over [1, (nc)1âˆ•ğœ†] and decreases
over
[(nc)1âˆ•ğœ†, âˆ)
for every suï¬ƒciently large n. Let x0 = k0 and
x(n) = (nc)1âˆ•ğœ†. It is clear that fn(x0) â†’0 and
fn(x(n)) = n1âˆ’1
ğœ†c(nc)âˆ’1eâˆ’nc(nc)âˆ’1
= n1âˆ’1
ğœ†c(nc)âˆ’1eâˆ’1
=
1
en1âˆ•ğœ†â†’0.
Invoking the Eulerâ€“Maclaurin lemma, with changes of variable
t = xâˆ’ğœ†and then s = nct,
n1âˆ’1
ğœ†
âˆ
âˆ‘
k=k0
pkeâˆ’npk âˆ¼âˆ«
âˆ
x0
n1âˆ’1
ğœ†cxâˆ’ğœ†eâˆ’ncxâˆ’ğœ†dx
= c
ğœ†âˆ«
xâˆ’ğœ†
0
0
n1âˆ’1
ğœ†tâˆ’1
ğœ†eâˆ’nctdt
= c
ğœ†n1âˆ’1
ğœ†âˆ«
xâˆ’ğœ†
0
0
(nct)âˆ’1
ğœ†(nc)âˆ’1+ 1
ğœ†eâˆ’nctd(nct)
= c
ğœ†n1âˆ’1
ğœ†(nc)âˆ’1+ 1
ğœ†âˆ«
ncxâˆ’ğœ†
0
0
sâˆ’1
ğœ†eâˆ’sds
= c
1
ğœ†
ğœ†n0
âˆ«
ncxâˆ’ğœ†
0
0
sâˆ’1
ğœ†eâˆ’sds
= c
1
ğœ†
ğœ†âˆ«
ncxâˆ’ğœ†
0
0
s
(
1âˆ’1
ğœ†
)
âˆ’1eâˆ’sds
= c
1
ğœ†
ğœ†Î“
(
1 âˆ’1
ğœ†
) â¡
â¢
â¢
â¢â£
1
Î“
(
1 âˆ’1
ğœ†
) âˆ«
ncxâˆ’ğœ†
0
0
s
(
1âˆ’1
ğœ†
)
âˆ’1eâˆ’sds
â¤
â¥
â¥
â¥â¦
â†’c
1
ğœ†
ğœ†Î“
(
1 âˆ’1
ğœ†
)
> 0.

Domains of Attraction on Countable Alphabets
231
Hence by Lemma 6.3,
n1âˆ’1âˆ•ğœ†
âˆ
âˆ‘
k=1
pk(1 âˆ’pk)n â†’c1âˆ•ğœ†ğœ†âˆ’1Î“ (1 âˆ’1âˆ•ğœ†) > 0,
and therefore ğœn â†’âˆ.
Case 2: Assuming pk â‰¥ckâˆ’ğœ†=âˆ¶qk for all k â‰¥k0 for some k0 â‰¥1, one ï¬rst has
n1âˆ’1
ğœ†
âˆ‘
kâ‰¥(nc)
1
ğœ†
ckâˆ’ğœ†eâˆ’nckâˆ’ğœ†= n1âˆ’1
ğœ†âˆ‘
kâ‰¥1
ckâˆ’ğœ†eâˆ’nckâˆ’ğœ†1
[
k â‰¥(nc)
1
ğœ†
]
.
Since
fn(x) = n1âˆ’1
ğœ†ckâˆ’ğœ†eâˆ’nckâˆ’ğœ†1
[
k â‰¥(nc)
1
ğœ†
]
satisï¬es the condition of Lemma 1.6 (The Eulerâ€“Maclaurin lemma)
with x(n) = (nc)
1
ğœ†and fn(x(n)) â†’0, one again has
n1âˆ’1
ğœ†
âˆ‘
kâ‰¥[(n+1)c]
1
ğœ†
ckâˆ’ğœ†eâˆ’nckâˆ’ğœ†
= c âˆ«
âˆ
1
n1âˆ’1
ğœ†xâˆ’ğœ†eâˆ’ncxâˆ’ğœ†1
[
x â‰¥[(n + 1)c]
1
ğœ†
]
dx
= c âˆ«
âˆ
[(n+1)c]
1
ğœ†
n1âˆ’1
ğœ†xâˆ’ğœ†eâˆ’ncxâˆ’ğœ†dx
= c
1
ğœ†ğœ†âˆ’1Î“
(
1 âˆ’1
ğœ†
)
âˆ«
n(n+1)c2
0
1
Î“
(
1 âˆ’1
ğœ†
) s
(
1âˆ’1
ğœ†
)
âˆ’1eâˆ’sds
â†’c
1
ğœ†ğœ†âˆ’1Î“
(
1 âˆ’1
ğœ†
)
> 0.
(6.9)
On the other hand, for suï¬ƒciently large n,
Iâˆ—=
{
k âˆ¶pk â‰¤
1
n + 1
}
âŠ†{k; k â‰¥k0},
by Parts 1 and 2 of Lemma 6.2 at steps 2 and 4 below and (6.9) at
step 7, one has
n1âˆ’1âˆ•ğœ†âˆ‘
kâˆˆIâˆ—
pk(1 âˆ’pk)n â‰¥n1âˆ’1âˆ•ğœ†âˆ‘
kâˆˆIâˆ—
qk(1 âˆ’qk)n
â‰¥n1âˆ’1âˆ•ğœ†âˆ‘
kâˆˆIâˆ—
qk exp
(
âˆ’nqk
1 âˆ’qk
)

232
Statistical Implications of Turingâ€™s Formula
â‰¥n1âˆ’1âˆ•ğœ†âˆ‘
kâˆˆIâˆ—
qk exp
â›
âœ
âœâ
âˆ’
nqk
1 âˆ’sup
iâˆˆIâˆ—qi
â
âŸ
âŸâ 
â‰¥n1âˆ’1âˆ•ğœ†âˆ‘
kâˆˆIâˆ—
exp(âˆ’2n(sup
iâˆˆIâˆ—qi)2) qkeâˆ’nqk
â‰¥n1âˆ’1âˆ•ğœ†âˆ‘
kâˆˆIâˆ—
exp(âˆ’2âˆ•n)qkeâˆ’nqk
= exp(âˆ’2âˆ•n)n1âˆ’1âˆ•ğœ†âˆ‘
kâˆˆIâˆ—
ckâˆ’ğœ†eâˆ’nckâˆ’ğœ†
â†’c
1
ğœ†ğœ†âˆ’1Î“
(
1 âˆ’1
ğœ†
)
> 0.
Finally,
ğœn = n
âˆ‘
k
pk(1 âˆ’pk)n â‰¥n1âˆ•ğœ†n1âˆ’1âˆ•ğœ†âˆ‘
kâˆˆIâˆ—
pk(1 âˆ’pk)n â†’âˆ
as n â†’âˆ.
â—½
6.4.3
Proof of Lemma 6.6
Proof: For clarity, the proof of Lemma 6.6 is given in three parts, Part 1: Pre-
liminaries, Part 2: Part 1 of Lemma 6.6, and Part 3: Part 2 of Lemma 6.6.
Part 1: Preliminaries. Noting that the ï¬rst ï¬nite terms of ğœn vanishes expo-
nentially fast for any distribution, one may assume, without loss of generality,
that k0 = 1. For any given n suï¬ƒciently large, deï¬ne kâˆ—= kâˆ—(n) by
pkâˆ—+1 <
1
n + 1 â‰¤pkâˆ—.
(6.10)
Noting
c0eâˆ’(kâˆ—+1) <
1
n + 1 â‰¤c0eâˆ’kâˆ—,
eâˆ’(kâˆ—+1) <
1
c0(n + 1) â‰¤eâˆ’kâˆ—,
âˆ’(kâˆ—+ 1) < âˆ’ln(c0(n + 1)) â‰¤âˆ’kâˆ—,
and
kâˆ—+ 1 > ln(c0(n + 1)) â‰¥kâˆ—,
kâˆ—may be expressed as
kâˆ—= âŒŠln(c0(n + 1))âŒ‹
for each n. That is to say that, although kâˆ—is uniquely deï¬ned by any given n,
each kâˆ—may correspond to several consecutive integer values of n. For a given
integer value kâˆ—, let the said consecutive integer values of n be denoted by
{nkâˆ—, nkâˆ—+ 1,â€¦, nkâˆ—+1 âˆ’1},
(6.11)
speciï¬cally noting that

Domains of Attraction on Countable Alphabets
233
1) nkâˆ—is the smallest integer value of n corresponding to kâˆ—by (6.10), that is,
kâˆ—= âŒŠln(c0(n + 1))âŒ‹;
2) nkâˆ—+1 is the smallest integer value of n that satisï¬es kâˆ—+ 1 = âŒŠln(c0(n + 1))âŒ‹;
and
3) nkâˆ—+1 âˆ’1 is the greatest integer value of n that shares the same value of kâˆ—
with nkâˆ—.
Since kâˆ—= kâˆ—(n) depends on n, one may express pkâˆ—as, and deï¬ne c(n) by,
pkâˆ—= c(n)
n .
(6.12)
At this point, the following fact is established: for each given kâˆ—,
pkâˆ—= c(nkâˆ—)
nkâˆ—
= c(nkâˆ—+ 1)
nkâˆ—+ 1
= Â· Â· Â· = c(nkâˆ—+1 âˆ’1)
nkâˆ—+1 âˆ’1 .
(6.13)
There are two main consequences of the expression in (6.12). The ï¬rst is
that ğœn deï¬ned in (6.2) may be expressed by (6.15); and the second is that the
sequence c(n) perpetually oscillates between 1 and e. Both facts are demon-
strated below.
Noting that the function fn(p) = np(1 âˆ’p)n increases for p âˆˆ(0, 1âˆ•(n + 1))
and decreases for p âˆˆ(1âˆ•(n + 1), 1), for any n
fn(pk) â‰¤fn(pkâˆ—),
k â‰¤kâˆ—
fn(pk) < fn(pkâˆ—),
k â‰¥kâˆ—+ 1.
(6.14)
For a given n, rewrite each pk in terms of pkâˆ—, and therefore in terms of n and
c(n):
pkâˆ—+i = eâˆ’i c(n)
n
and
pkâˆ—âˆ’j = ej c(n)
n
for all appropriate positive integers i and j. Therefore,
fn(pkâˆ—+i) = neâˆ’i c(n)
n
(
1 âˆ’eâˆ’i c(n)
n
)n
= c(n)
ei
(
1 âˆ’c(n)
nei
)n
,
fn(pkâˆ—âˆ’j) = nej c(n)
n
(
1 âˆ’ej c(n)
n
)n
= c(n)ej
(
1 âˆ’c(n)ej
n
)n
,

234
Statistical Implications of Turingâ€™s Formula
and
ğœn =
âˆ‘
kâ‰¤kâˆ—âˆ’1
fn(pk) + fn(pkâˆ—) +
âˆ‘
kâ‰¥kâˆ—+1
fn(pk)
= c(n)
kâˆ—âˆ’1
âˆ‘
j=1
ej
(
1 âˆ’c(n)ej
n
)n
+ c(n)
(
1 âˆ’c(n)
n
)n
+ c(n)
âˆ
âˆ‘
i=1
eâˆ’i
(
1 âˆ’c(n)
nei
)n
.
(6.15)
Next, it is to show that c(n) oscillates perpetually over the interval (nâˆ•(n +
1), e), which approaches [1, e) as n increases indeï¬nitely. This is so because,
since kâˆ—is deï¬ned by (6.10),
c(n)
n eâˆ’1 â‰¤
1
n + 1 â‰¤c(n)
n
or
eâˆ’1 <
n
n + 1 â‰¤c(n) â‰¤
n
n + 1e < e.
(6.16)
At this point, the fact c(n) âˆˆ[1, e) is established. What remains to be shown is
that c(n) oscillates perpetually in n. Toward that end, consider kâˆ—(n) as a map-
ping, which maps every positive integer value of n âˆˆâ„•to a positive integer
value of kâˆ—âˆˆâ„•. The inverse of kâˆ—(n) maps every integer value kâˆ—âˆˆâ„•to a set
as in (6.11). Let
â„•= âˆª{nkâˆ—, nkâˆ—+ 1,â€¦, nkâˆ—+1 âˆ’1}
(6.17)
where the union is over all possible integer values of kâˆ—. (In fact, the smallest kâˆ—
possible is kâˆ—= 1 for nkâˆ—= 1. For this case, p1 = 1 âˆ’eâˆ’1 â‰ˆ0.6321, p2 = p1eâˆ’1 â‰ˆ
0.2325, and (1 + 1)âˆ’1 = 0.5; therefore, kâˆ—= 1 and n1 = 1.)
Observe the following three facts:
1) c(nkâˆ—) < c(nkâˆ—+ 1) < Â· Â· Â· < c(nkâˆ—+1 âˆ’1). This is so because of (6.13): all of
them sharing the same kâˆ—and therefore the same pkâˆ—. Furthermore, the
increments of increase are all identical, namely, pkâˆ—.
2) Consider {nkâˆ—; kâˆ—â‰¥1} where nkâˆ—is the smallest integer value in each parti-
tioning set of (6.17). One has c(nkâˆ—) = nkâˆ—pkâˆ—â†’1. This is so because 1âˆ•nkâˆ—>
pkâˆ—â‰¥1âˆ•(nkâˆ—+ 1) or
1 âˆ’pkâˆ—â‰¤nkâˆ—pkâˆ—< 1,
(6.18)
which implies that nkâˆ—pkâˆ—for all suï¬ƒciently large kâˆ—(or equivalently suï¬ƒ-
ciently large nkâˆ—or suï¬ƒciently large n),
c(nkâˆ—) = nkâˆ—pkâˆ—âˆˆ(1 âˆ’Îµ, 1)
(6.19)
where Îµ > 0 is an arbitrarily small real value.

Domains of Attraction on Countable Alphabets
235
3) Consider {nkâˆ—+1 âˆ’1; kâˆ—â‰¥1} where nkâˆ—+1 âˆ’1 is the greatest integer value in
each partitioning set of (6.17). One has
c(nkâˆ—+1 âˆ’1) = (nkâˆ—+1 âˆ’1)pkâˆ—â†’e.
This is so because
pkâˆ—= pkâˆ—+1e
= nkâˆ—+1 âˆ’1
nkâˆ—+1 âˆ’1pkâˆ—+1e
=
1
nkâˆ—+1 âˆ’1
(nkâˆ—+1 âˆ’1
nkâˆ—+1
)
(nkâˆ—+1pkâˆ—+1)e,
and therefore by (6.18)
c(nkâˆ—+1 âˆ’1) =
(nkâˆ—+1 âˆ’1
nkâˆ—+1
)
(nkâˆ—+1pkâˆ—+1)e â†’e.
At this point, it has been established that the range of c(n) for n â‰¥n0, where
n0 is any positive integer, covers the entire interval [1, e).
Part 2: Part 1 of Lemma 6.6. Noting that eâˆ’1 â‰¤c(n) â‰¤e (see (6.16)) and that
1 âˆ’p â‰¤eâˆ’p for all p âˆˆ[0, 1], the desired result follows the argument below.
ğœn = c(n)
kâˆ—âˆ’1
âˆ‘
j=1
ej
(
1 âˆ’c(n)ej
n
)n
+ c(n)
(
1 âˆ’c(n)
n
)n
+ c(n)
âˆ
âˆ‘
j=1
eâˆ’j
(
1 âˆ’c(n)
nej
)n
â‰¤e
kâˆ—âˆ’1
âˆ‘
j=1
ej
(
1 âˆ’ejâˆ’1
n
)n
+ e
(
1 âˆ’eâˆ’1
n
)n
+ e
âˆ
âˆ‘
j=1
eâˆ’j (
1 âˆ’
1
nej+1
)n
â‰¤e
kâˆ—âˆ’1
âˆ‘
j=1
ejeâˆ’ejâˆ’1 + e
âˆ
âˆ‘
j=0
eâˆ’jeâˆ’eâˆ’(j+1)
â‰¤e2
kâˆ—âˆ’1
âˆ‘
j=1
ejâˆ’1eâˆ’ejâˆ’1 + e2
âˆ
âˆ‘
j=0
eâˆ’(j+1)eâˆ’eâˆ’(j+1)
< e2
âˆ
âˆ‘
j=0
ejeâˆ’ej + e2
âˆ
âˆ‘
j=1
eâˆ’jeâˆ’eâˆ’j âˆ¶= u.
Part 3: Part 2 of Lemma 6.6. Consider, for any ï¬xed c > 0,
ğœâˆ—
n = c
kâˆ—âˆ’1
âˆ‘
j=1
ej
(
1 âˆ’cej
n
)n
+ c
(
1 âˆ’c
n
)n
+ c
âˆ
âˆ‘
j=1
eâˆ’j (
1 âˆ’c
nej
)n
.

236
Statistical Implications of Turingâ€™s Formula
By dominated convergence theorem,
ğœ(c)âˆ¶= lim
nâ†’âˆğœâˆ—
n = c
âˆ
âˆ‘
j=0
ejeâˆ’cej + c
âˆ
âˆ‘
j=1
eâˆ’jeâˆ’ceâˆ’j,
and ğœ(c) is a nonconstant function in c on [1, e].
Noting that as kâˆ—increases (or equivalently, nkâˆ—increases or n increases),
1 â†c(nkâˆ—) < c(nkâˆ—+ 1) < Â· Â· Â· < c(nkâˆ—+1 âˆ’1) â†’e
and that the increment between two consecutive terms pkâˆ—â†’0, c(n) visits any
arbitrarily small closed interval [a, b] âŠ‚[1, e] inï¬nitely often, and therefore
there exists for each such interval a subsequence {nl; l â‰¥1} of â„•such that c(nl)
converges, that is, c(nl) â†’ğœƒfor some ğœƒâˆˆ[a, b]. Since ğœ(c) is a nonconstant
function on [1, e], there exist two nonoverlapping closed intervals, [a1, b1] and
[a2, b2] in [1, e], satisfying
max
a1â‰¤câ‰¤b1
ğœ(c) < min
a2â‰¤câ‰¤b2 ğœ(c),
such that there exist two subsequences of â„•, said {nl; l â‰¥1} and {nm; m â‰¥1},
such that c(nl) â†’ğœƒ1 for some ğœƒ1 âˆˆ[a1, b1] and c(nm) â†’ğœƒ2 for some
ğœƒ2 âˆˆ[a2, b2].
Consider the limit of ğœn along {nl; l â‰¥1}, again by dominated convergence
theorem, as nl â†’âˆ,
ğœnl =
[
c(nl)
kâˆ—âˆ’1
âˆ‘
j=0
ej
(
1 âˆ’c(nl)ej
n
)n
+ c(nl)
âˆ
âˆ‘
j=1
eâˆ’j
(
1 âˆ’c(nl)
nej
)n]
â†’ğœƒ1
âˆ
âˆ‘
j=0
ejeâˆ’ğœƒ1ej + ğœƒ1
âˆ
âˆ‘
j=1
eâˆ’jeâˆ’ğœƒ1eâˆ’j = ğœ(ğœƒ1).
A similar argument gives limnmâ†’âˆğœnm = ğœ(ğœƒ2), but ğœ(ğœƒ1) â‰ ğœ(ğœƒ2) by construc-
tion, and hence limnâ†’âˆğœn does not exist.
â—½
6.5
Exercises
1
Show that ğœn = âˆ‘
kâ‰¥1pk(1 âˆ’pk)n â†’0 as n â†’âˆfor any probability distri-
bution {pk} on ğ’³.
2
Prove Part 2 of Lemma 6.2, that is, for any real number x âˆˆ(0, 1âˆ•2),
1
1 âˆ’x < 1 + 2x.
3
Prove Lemma 6.5.
4
Use the Îµâ€“N language to show that

Domains of Attraction on Countable Alphabets
237
a) an = 1âˆ•n converges to 0; and
b) bn = (n + 1)âˆ•n2 converges to 0.
5
Use the Îµâ€“N language to show that
a) an = âˆ‘n
i=1 3âˆ•10i converges to 1âˆ•3; and
b) bn = 5 + (âˆ’1)n+12
n
converges to 5.
6
Show that
a) limnâ†’âˆ
(
1 + 1
n
)n
= e; and
b) limnâ†’âˆ
(
1 âˆ’1
n
)n
= 1âˆ•e.
7
If {an} is a convergent sequence, then every subsequence of that sequence
converges to the same limit.
8
If {an} is a sequence such that every possible subsequence extracted from
that sequences converge to the same limit, then the original sequence also
converges to that limit.
9
Show that
a) the sequence {an = sin nğœ‹; n = 1, 2, Â· Â· Â·} does not converge; and
b) the sequence {an = sin ln n; n = 1, 2, Â· Â· Â·} does not converge.
10
Give a proof of the Bolzanoâ€“Weierstrass theorem: every bounded
sequence has a convergent subsequence. (Hint: Let a and b be the lower
and upper bounds of a bounded sequence. Consider the two intervals,
[a.(a + b)âˆ•2] and [(a + b)âˆ•2, b].)
11
The Squeeze rule: let {an}, {bn}, and {xn} be three sequences such that
a) an â‰¤xn â‰¤bn for every n âˆˆâ„•; and
b) limnâ†’âˆan = limnâ†’âˆbn = L,
for
some
ï¬nite
constant
L,
then
limnâ†’âˆxn = L. Use the Îµâ€“N language to prove the above-mentioned
Squeeze rule.
12
Show that every convergent sequence is bounded.
13
Let {an} be a positive sequence such that limnâ†’âˆan = L > 1. Prove that
{an
n} â†’âˆ. (Hint: limnâ†’âˆxn = âˆfor any number x satisfying x > 1.)
14
Let
an = (âˆ’1)n + 1
n,
and consider the sequence {an; n = 1, 2,â€¦ }. Find lim sup an and
lim inf an.

238
Statistical Implications of Turingâ€™s Formula
15
Let S be a nonempty subset of the real line R. Prove the following
statements.
a) Suppose that S is bounded above, that is, there exists a constant M such
that s â‰¤M for every s âˆˆS. Then ğ›¼< sup S if and only if there exists an
a âˆˆS such that a > ğ›¼.
b) Suppose that S is bounded below, that is, there exists a constant m such
that s â‰¥m for every s âˆˆS. Then ğ›½> inf S if and only if there exists an
b âˆˆS such that b < ğ›½.
16
For every sequence {an; n = 1, 2, Â· Â· Â·}, let âˆ‘âˆ
n=1 an = limnâ†’âˆ
âˆ‘n
i=1 ai.
a) Find âˆ‘âˆ
n=1 eâˆ’n.
b) Find âˆ‘âˆ
n=10
1
n(n+1).
17
A series âˆ‘n
i=1 an is said to be convergent if
âˆ
âˆ‘
i=1
an = lim
nâ†’âˆ
n
âˆ‘
i=1
an = a
for some ï¬nite constant a. If such an a does not exists, then the series is
said to be divergent.
a) Show that âˆ‘n
i=1 (âˆ’1)n is divergent.
b) Show that âˆ‘n
i=1 n is divergent.
18
Show that âˆ‘âˆ
n=1
(âˆ’1)n
n
= ln 2. (This series is known as the alternating har-
monic series. Hint: Consider the Taylor expansion of f (x) = ln x at x0 = 1.)
19
Show that the harmonic series, âˆ‘âˆ
n=1
1
n, diverges to inï¬nity, that is,
lim
nâ†’âˆ
âˆ
âˆ‘
n=1
1
n = âˆ.
(Hint: Consider the inequalities below.
âˆ
âˆ‘
n=1
1
n = 1
1 + 1
2 + 1
3 + 1
4 + 1
5 + 1
6 + 1
7 + 1
8 + 1
9 + Â· Â· Â·
> 1
1 + 1
2 + 1
4 + 1
4 + 1
8 + 1
8 + 1
8 + 1
8 + 1
16 + Â· Â· Â· .
)
20
Show that
1
n + ln n â‰¤
kâˆ‘
n=1
1
n â‰¤1 + ln n.

Domains of Attraction on Countable Alphabets
239
(Hint: For any decreasing function f (x),
âˆ«
b+1
a
f (x)dx â‰¤
b
âˆ‘
i=a
f (i) â‰¤âˆ«
b
aâˆ’1
f (x)dx,
where a and b are positive integers satisfying a < b.)
21
Find the limit of 1
n
âˆ‘n
k=2
1
ln k .
(Hint:
1
n
n
âˆ‘
k=2
1
ln k = 1
n
âŒŠln nâŒ‹
âˆ‘
k=2
1
ln k + 1
n
n
âˆ‘
k=âŒŠln nâŒ‹+1
1
ln k
< âŒŠln nâŒ‹âˆ’1
n ln 2
+ n âˆ’âŒŠln nâŒ‹
n lnâŒŠln nâŒ‹.
)
22
Show that the notion of dominance by Deï¬nition 6.2 is transitive, that is,
if p1 dominates p2 and p2 dominates p3, then p1 dominates p3.

241
7
Estimation of Tail Probability
The remarkable Turingâ€™s formula suggests that certain useful knowledge of the
tail of a distribution {pk; k â‰¥1} on ğ’³= {ğ“k; k â‰¥1} may be extracted from an
iid sample. This chapter demonstrates how such information could be used to
estimate parametric probability models in, and only in, the tail of the underlying
distribution, possibly beyond data range.
7.1
Introduction
Consider a discrete probability distribution {pk; k â‰¥1} for which there exists
an unknown integer k0 > 0 such that, for each and every k â‰¥k0,
pk = Ckâˆ’ğœ†
(7.1)
where C > 0 and ğœ†> 1 are unknown parameters. Suppose the primary problem
of interest is to estimate C and ğœ†with an iid sample of size n.
There are two somewhat unusual characteristics about this problem. First, the
parametric model is only imposed on the tail. For any k < k0, pk is not restricted
to any particular parametric form. Second, the threshold k0 is unknown, and
therefore for any particular observed value of the underlying random element,
say X = ğ“k, it is not known whether k â‰¥k0. This model will be referred to as
the power, or the Pareto, tail model hereafter.
In the existing literature, there exists a well-known similar problem in the
continuous domain where the underlying random variable X has a continu-
ous distribution function F(x) such that 1 âˆ’F(x) = Cxâˆ’ğœ†for x â‰¥x0 and F(x)
is unspeciï¬ed for x < x0 where x0, C > 0 and ğœ†> 0 are unknown parameters.
The primary interest of this problem is also to estimate C and ğœ†. The exist-
ing methodological approaches to this problem are largely based on extreme
value theory and domains of attraction. The reference list on this problem is
extensive. Some of the more commonly cited publications include Hill (1975),
Haeusler and Teugels (1985), Hall and Welsh (1985), Smith (1987), Hall and
Weissman (1997), and Pickands (1975). However, while many parallels can be
Statistical Implications of Turingâ€™s Formula, First Edition. Zhiyi Zhang.
Â© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

242
Statistical Implications of Turingâ€™s Formula
drawn between this problem and the one on hand, it must be said that the two
problems are fundamentally diï¬€erent. The extreme value theory that supports
the approaches to the continuous tail problem has little relevance in the prob-
lem under current consideration due to its discrete nature. A solution to the
current problem calls for an entirely diï¬€erent approach.
In a similar spirit, a more general parametric tail model may be entertained.
Consider a discrete probability distribution {pk; k â‰¥1} for which there exists
an unknown integer k0 > 0 such that, for each and every k â‰¥k0,
pk = p(k; ğœ½)
(7.2)
where ğœ½is a vector of several parameters.
Consider a multinomial distribution with its countably inï¬nite number of
prescribed categories indexed by â„•= {k; k â‰¥1} and its category probabilities
denoted by {pk; k â‰¥1}, satisfying 0 < pk < 1 for all k and âˆ‘
kâ‰¥1pk = 1. This
model will be referred to as the general model hereafter. Let the category
counts in an iid sample of size n from the underlying population be denoted by
{Yk; k â‰¥1} and its observed values by {yk; k â‰¥1}. For a given sample, there
are at most n nonzero yks.
For every integer r, 1 â‰¤r â‰¤n, recall the following three important random
variables from Chapter 1,
Nr =
âˆ‘
1[Yk = r],
Tr =
r
n âˆ’r + 1Nr,
ğœ‹râˆ’1 =
âˆ‘
pk1[Yk = r âˆ’1],
as given in (1.1), (1.27), and (1.2).
For any given positive integer R, consider the following two random vectors
of length R,
T = (T1, â€¦ , TR)ğœ
ğ…= (ğœ‹0, â€¦ , ğœ‹Râˆ’1)ğœ.
In Chapter 1, it is argued that T is a reasonable estimator of ğ…, at least in
the sense of Theorem 1.11. In this chapter, it is argued that Theorem 1.11 may
support parametric tail probability models such as (7.2) in general and (7.1) in
speciï¬c.
At a ï¬rst glance, it would seem a bit far-fetched that a nonparametric result
like Theorem 1.11 could support a parametric model in the tail, let alone a
distant tail possibly beyond data range. Yet it is the very core implication of
Turingâ€™s formulae. It is perhaps instructive to give a moment of thought to why
such an undertaking is possible.

Estimation of Tail Probability
243
Consider the following disjoint random subsets of â„•= {k; k = 1, 2, â€¦ }, for
an iid sample of size n and a given positive integer R,
ğ•‚0 = {k âˆ¶Yk = 0}
ğ•‚1 = {k âˆ¶Yk = 1}
â‹®
ğ•‚Râˆ’1 = {k âˆ¶Yk = R âˆ’1}
and their union
ğ•‚tail = âˆªRâˆ’1
r=0 ğ•‚r.
(7.3)
For simplicity, assuming pk > 0 for every k âˆˆâ„•, ğ•‚tail in (7.3) may be thought
of as a tail of the index set â„•= {k; k â‰¥1} because the following key fact that
lim
nâ†’âˆP (ğ•‚tail âŠ‚{k; k â‰¥k0}) = 1
(7.4)
where k0 is the unknown positive integer threshold in the parametric tail model
in (7.1) or (7.2) (see Exercise 1).
Several summarizing remarks can now be given:
1) The summands in each ğœ‹r of ğ…are subject to the event {Yk = r} for a ï¬xed
r. This implies that category probabilities pk included in ğœ‹r are dynamically
shifting into the tail in probability as n increases. Therefore, for a ï¬xed R, the
entire panel ğ…shifts into the tail in probability. As a result, in probability,
all nonzero summands in ğ…will eventually have indices satisfying k â‰¥k0.
Under the power tail model, as n increases, ğ…will only contain pks such that
pk = Ckâˆ’ğœ†, or whatever the parametric tail model may be. This is a partial
reason why a consistent estimator of the tail is possible.
2) The equation in (7.4) holds regardless whether k0 is known or not. This fact
gives much ï¬‚exibility to the approach and bypasses the necessity of knowing
(or estimating) k0 a priori.
3) By Turingâ€™s formulae in general and Theorem 1.11 in speciï¬c, T chases ğ…
while ğ…chases the tail. In fact, ğ•‚tail of (7.3) may be thought of as a â€œwindow
on tail.â€ It is in this sense that the subsequently described estimator of the
tail is said to be in Turingâ€™s perspective.
In the next several sections, the Pareto tail model in (7.1) is studied in detail. A
distributional relationship between the observable T and the observable func-
tions of C and ğœ†, namely ğ…, is explored; an asymptotic distribution of T âˆ’ğ…
is established; an estimator ( Ì‚C, Ì‚ğœ†) of (C, ğœ†) based on the likelihood of the said
asymptotic distribution is deï¬ned; the consistency of the estimator is estab-
lished; and a large sample conï¬dence region for (C, ğœ†) is derived.

244
Statistical Implications of Turingâ€™s Formula
7.2
Estimation of Pareto Tail
Let p = {pk; k â‰¥1} be a probability distribution on ğ’³and g(n, ğ›¿) = n1âˆ’2ğ›¿where
ğ›¿> 0 is a constant. The following is a condition on p.
Condition 7.1
There exists a ğ›¿âˆˆ(0, 1âˆ•4) such that as n â†’âˆ,
1) g2(n, ğ›¿)
n2
E(Nr) â†’cr
r! â‰¥0,
2) g2(n, ğ›¿)
n2
E(Nr+1) â†’
cr+1
(r + 1)! â‰¥0, and
3) cr + cr+1 > 0
where r is a positive integer.
Let
ğœ2
r = r2E(Nr) + (r + 1)rE(Nr+1),
ğœŒr(n) = âˆ’r(r + 1)E(Nr+1)âˆ•(ğœrğœr+1),
ğœŒr = lim
nâ†’âˆğœŒr(n),
Ì‚ğœ2
r = r2Nr + (r + 1)rNr+1, and
Ì‚ğœŒr = Ì‚ğœŒr(n) = âˆ’r(r + 1)Nr+1âˆ•
âˆš
Ì‚ğœ2
r Ì‚ğœ2
r+1.
The following two lemmas are re-statements of Theorems 1.11 and 1.12, and
they are given here for easy reference.
Lemma 7.1
For any positive integer R, if Condition 7.1 holds for every r, 1 â‰¤
r â‰¤R âˆ’1, then
n
(T1 âˆ’ğœ‹0
ğœ1
, â€¦ , TR âˆ’ğœ‹Râˆ’1
ğœR
)ğœ
L
âˆ’âˆ’âˆ’â†’MVN(0, Î£)
where Î£ = (ai,j) is a R Ã— R covariance matrix with all the diagonal elements
being ar,r = 1 for r = 1, â€¦ , R, the super-diagonal and the sub-diagonal
elements being ar,r+1 = ar+1,r = ğœŒr for r = 1, â€¦ , R âˆ’1, and all the other
oï¬€-diagonal elements being zeros.
Let Ì‚Î£ be the resulting matrix of Î£ with ğœŒr replaced by Ì‚ğœŒr(n) for all r. Let
Ì‚Î£âˆ’1 denote the inverse of Ì‚Î£ and Ì‚Î£âˆ’1âˆ•2 denote any R Ã— R matrix satisfying
Ì‚Î£âˆ’1 = ( Ì‚Î£âˆ’1âˆ•2)ğœÌ‚Î£âˆ’1âˆ•2.
Lemma 7.2
For any positive integer R, if Condition 7.1 holds for every r, 1 â‰¤
r â‰¤R, then
n Ì‚Î£âˆ’1âˆ•2
(T1 âˆ’ğœ‹0
Ì‚ğœ1
, â€¦ , TR âˆ’ğœ‹Râˆ’1
Ì‚ğœR
)ğœ
L
âˆ’âˆ’âˆ’â†’MVN(0, IRÃ—R).

Estimation of Tail Probability
245
Lemma 7.3
Under the power tail model (7.1), Condition 7.1 holds for every r,
1 â‰¤r â‰¤R âˆ’1.
Proof: Letting ğ›¿= (4ğœ†)âˆ’1, it can be veriï¬ed that
nrâˆ’2g2(n, ğ›¿)
âˆ‘
kâ‰¥1
pr
keâˆ’npk â†’cr > 0
for every integer r > 0.
â—½
See Exercise 2.
Corollary 7.1
Under the power tail model in (7.1), the results of both Lemmas
7.1 and 7.2 hold.
Under the power tail model in (7.1), for every r > 0, let every pk in ğœ‹râˆ’1 be
replaced by pk = Ckâˆ’ğœ†and denote the resulting expression as ğœ‹âˆ—
râˆ’1, that is,
ğœ‹âˆ—
râˆ’1 = C
âˆ‘
kâ‰¥1
kâˆ’ğœ†1[Yk = r âˆ’1].
Then
ğœ‹râˆ’1 âˆ’ğœ‹âˆ—
râˆ’1 =
k0âˆ’1
âˆ‘
k=1
1[Yk = r âˆ’1] (pk âˆ’Ckâˆ’ğœ†) ,
and the sum is of ï¬nite terms. Since, for every k, both E(1[Yk = r âˆ’1])
and Var(1[Yk = r âˆ’1]) converge to zero exponentially in sample size n,
E(g(n, ğ›¿)1[Yk = r âˆ’1]) and Var(g(n, ğ›¿)1[Yk = r âˆ’1]) converge to zero, which
implies that g(n, ğ›¿)1[Yk = r âˆ’1]
pâ†’0, which in turn implies that
g(n, ğ›¿)(ğœ‹râˆ’1 âˆ’ğœ‹âˆ—
râˆ’1)
p
âˆ’âˆ’âˆ’â†’0
for every r. This argument leads to the following two lemmas.
Lemma 7.4
Under the power tail model in (7.1),
n
(T1 âˆ’ğœ‹âˆ—
0
ğœ1
,â€¦,
TR âˆ’ğœ‹âˆ—
Râˆ’1
ğœR
)ğœ
L
âˆ’âˆ’âˆ’â†’MVN(0, Î£)
where Î£ is as in Lemma 7.1.
Lemma 7.5
Under the power tail model in (7.1),
n Ì‚Î£âˆ’1âˆ•2
(T1 âˆ’ğœ‹âˆ—
0
Ì‚ğœ1
,â€¦,
TR âˆ’ğœ‹âˆ—
Râˆ’1
Ì‚ğœR
)ğœ
L
âˆ’âˆ’âˆ’â†’MVN(0, IRÃ—R)
(7.5)
where Ì‚Î£âˆ’1âˆ•2 is as in Lemma 7.2.

246
Statistical Implications of Turingâ€™s Formula
The asymptotic likelihood given by (7.5) depends on the two model parame-
ters, ğœ†and C, only via
ğœ‹âˆ—
râˆ’1 = ğœ‹âˆ—
râˆ’1(C, ğœ†) = C
âˆ‘
kâ‰¥1
1[Yk = r âˆ’1]kâˆ’ğœ†,
(7.6)
r = 1,â€¦, R.
Deï¬nition 7.1
If there exists a pair of values, C = Ì‚C and ğœ†= Ì‚ğœ†, which max-
imizes the likelihood given by (7.5) in a neighborhood of C > 0 and ğœ†> 1, then
( Ì‚C, Ì‚ğœ†) is said to be an asymptotic maximum likelihood estimator (AMLE) of
(C, ğœ†).
Let R = 2, and denote, hereafter, ( Ì‚C, Ì‚ğœ†) as AMLE by Deï¬nition 7.1 but with
R = 2. The following is a corollary of Theorem 7.5.
Corollary 7.2
Under the model in (7.1),
Ì‚Î£âˆ’1âˆ•2
2
(T1 âˆ’ğœ‹âˆ—
0, T2 âˆ’ğœ‹âˆ—
1
)ğœ
L
âˆ’âˆ’âˆ’â†’MVN(0, I2Ã—2)
(7.7)
where
Ì‚Î£âˆ’1âˆ•2
2
=
n
2
âˆš
1 âˆ’Ì‚ğœŒ2
1
( âˆš
1 âˆ’Ì‚ğœŒ1 +
âˆš
1 + Ì‚ğœŒ1
âˆš
1 âˆ’Ì‚ğœŒ1 âˆ’
âˆš
1 + Ì‚ğœŒ1
âˆš
1 âˆ’Ì‚ğœŒ1 âˆ’
âˆš
1 + Ì‚ğœŒ1
âˆš
1 âˆ’Ì‚ğœŒ1 +
âˆš
1 + Ì‚ğœŒ1
)
Ã—
â›
âœ
âœâ
1âˆ•Ì‚ğœ1
0
0
1âˆ•Ì‚ğœ2
â
âŸ
âŸâ 
.
(7.8)
Given a sample, an AMLE minimizes
lln = lln(C, ğœ†)
âˆ¶= (T1 âˆ’ğœ‹âˆ—
0, T2 âˆ’ğœ‹âˆ—
1
) Ì‚Î£âˆ’1
2
(T1 âˆ’ğœ‹âˆ—
0, T2 âˆ’ğœ‹âˆ—
1
)ğœ.
The properties of AMLEs may be examined by the statistical behavior of an
appropriately proportioned version of lln, namely ln as deï¬ned in (7.9). Let
g2(n, ğ›¿)Î©n = Ì‚Î£âˆ’1
2 .
Recall that, under the power tail model,
g(n, ğ›¿) = n1âˆ’1âˆ•(2ğœ†0)
where ğœ†0 is the true value of ğœ†. It can be veriï¬ed that
Î©n = (ğœ”ij(n))
p
âˆ’âˆ’âˆ’â†’Î©

Estimation of Tail Probability
247
where Î© = (ğœ”ij) is some positive deï¬nite 2 Ã— 2 matrix. The logarithmic likeli-
hood given by (7.5) is negatively proportional to
ln = ln(C, ğœ†)
âˆ¶= nâˆ’1âˆ•ğœ†0n2âˆ’1âˆ•ğœ†0(lln(C, ğœ†)âˆ•n2âˆ’1âˆ•ğœ†0)
= n2âˆ’2âˆ•ğœ†0(T1 âˆ’ğœ‹âˆ—
0, T2 âˆ’ğœ‹âˆ—
1)Î©n (T1 âˆ’ğœ‹âˆ—
0, T2 âˆ’ğœ‹âˆ—
1)ğœ.
(7.9)
The examination of ln(C, ğœ†), a smooth random surface deï¬ned on the param-
eter space
Î˜ = {(C, ğœ†); C > 0 and ğœ†> 1},
is carried out separately on three subsets of Î˜. Let Î˜ = âˆª3
s=1Î˜s where
1) Î˜1 = {(C, ğœ†); ğœ†â‰¥ğœ†0}
2) Î˜2 =
{
(C, ğœ†); max
(
ğœ†0 âˆ’
1
2ğœ†0 , 1
)
< ğœ†< ğœ†0
}
3) Î˜3 =
{
(C, ğœ†); 1 < ğœ†â‰¤ğœ†0 âˆ’
1
2ğœ†0
}
if ğœ†0 âˆ’
1
2ğœ†0 > 1, or Î˜3 is an empty set
otherwise.
{Î˜1, Î˜2, Î˜3} is a partition of the parameter space along the interval ğœ†âˆˆ(1, âˆ)
with two dividing points: ğœ†0 âˆ’1âˆ•(2ğœ†0) and ğœ†0, provided that the ï¬rst point is
on the right side of 1. The necessity of examining ln separately on these sets lies
in the qualitative diï¬€erence in its respective behavior on the sets. As it turns
out, ln is well behaved on Î˜1 as all of its components are convergent in some
sense; whereas on Î˜2 and Î˜3, many of the components of ln are divergent, and
consequently ln requires a diï¬€erent treatment on each of these sets.
This section ends with a useful lemma below.
Lemma 7.6
Let Î© be a positive deï¬nite 2 Ã— 2 matrix. For each r, r = 1, 2, let
ar(n) and Ar(n) be sequences of real values satisfying ar(n) â†’0 and Ar(n) â†’
âˆ’âˆ. Then
(A1(n), A2(n))Î©(A1(n), A2(n)))ğœ+ (a1(n), a2(n))Î©(A1(n), A2(n))ğœâ†’+âˆ.
Proof: Since Î© is positive deï¬nite, Î© = PğœÎ›P where P is orthogonal and Î› is
diagonal matrix with two positive elements ğœ†1 and ğœ†2. Let
(Q1, Q2)ğœ= P(A1(n), A2(n))ğœ
and
(q1, q2)ğœ= P(a1(n), a2(n))ğœ.
Since
Q2
1 + Q2
2 = A2
1(n) + A2
2(n) â†’+âˆ,
|Q1| + |Q2| â†’+âˆ
and
hence
ğœ†1|Q1| + ğœ†2|Q2| â†’+âˆ. Also P(a1(n), a2(n))ğœ= (q1, q2)ğœâ†’(0, 0)ğœ. On the
other hand, for a suï¬ƒciently large n,
(A1(n), A2(n))Î©(A1(n), A2(n)) + (a1(n), a2(n))Î©(A1(n), A2(n))
= ğœ†1Q2
1 + ğœ†2Q2
2 + ğœ†1q1Q1 + ğœ†2q2Q2

248
Statistical Implications of Turingâ€™s Formula
â‰¥ğœ†1Q2
11[|Q1| > 1] + ğœ†1Q2
11[|Q1| â‰¤1]
âˆ’1
3ğœ†1|Q1|1[|Q1| > 1] âˆ’1
3ğœ†1|Q1|1[|Q1| â‰¤1]
+ ğœ†2Q2
21[|Q2| > 1] + ğœ†2Q2
21[|Q2| â‰¤1]
âˆ’1
3ğœ†2|Q2|1[|Q2| > 1] âˆ’1
3ğœ†2|Q2|1[|Q2| â‰¤1]
= ğœ†1|Q1|1[|Q1| > 1]
(
|Q1| âˆ’1
3
)
+ ğœ†1|Q1|1[|Q1| â‰¤1]
(
|Q1| âˆ’1
3
)
+ ğœ†2|Q2|1[|Q2| > 1]
(
|Q2| âˆ’1
3
)
+ ğœ†2|Q2|1[|Q2| â‰¤1]
(
|Q2| âˆ’1
3
)
â‰¥ğœ†1|Q1|1[|Q1| > 1]2
3 âˆ’1
3ğœ†1|Q1|1[|Q1| â‰¤1]
+ ğœ†2|Q2|1[|Q2| > 1]2
3 âˆ’1
3ğœ†2|Q2|1[|Q2| â‰¤1]
â‰¥2ğœ†1
3 |Q1|1[|Q1| > 1] âˆ’1
3ğœ†1 + 2ğœ†2
3 |Q2|1[|Q2| > 1] âˆ’1
3ğœ†2
= 2ğœ†1
3 |Q1| âˆ’2ğœ†1
3 |Q1|1[|Q1| â‰¤1] âˆ’1
3ğœ†1 + 2ğœ†2
3 |Q2|
âˆ’2ğœ†2
3 |Q2|1[|Q2| â‰¤1] âˆ’1
3ğœ†2
â‰¥2
3(ğœ†1|Q1| + ğœ†2|Q2|) âˆ’(ğœ†1 + ğœ†2) â†’+âˆ.
â—½
7.3
Statistical Properties of AMLE
The uniqueness and consistency of the AMLE of (C, ğœ†), ( Ì‚C, Ì‚ğœ†), are respectively
established in this section.
Theorem 7.1
Under the power tail model in (7.1), the AMLE of (C, ğœ†) uniquely
exists for a suï¬ƒciently large n in a neighborhood of (C0, ğœ†0), where C0 and ğœ†0 are
the true values of the parameters.
The proof of Theorem 7.1 is based on the fact that the probability that the
Hessian matrix of ln(C, ğœ†) at the true parameter values (C0, ğœ†0) is positive deï¬-
nite converges to 1 as n â†’âˆ. To establish the said fact, Lemma 7.7 is needed.
Toward stating Lemma 7.7, several notations are needed. Let
un =
âˆ‘
kâ‰¥1
1[Yk = 0]kâˆ’ğœ†,
vn =
âˆ‘
kâ‰¥1
1[Yk = 0](ln k)kâˆ’ğœ†,
wn =
âˆ‘
kâ‰¥1
1[Yk = 0](ln k)2kâˆ’ğœ†,

Estimation of Tail Probability
249
Un =
âˆ‘
kâ‰¥1
1[Yk = 1]kâˆ’ğœ†,
Vn =
âˆ‘
kâ‰¥1
1[Yk = 1](ln k)kâˆ’ğœ†, and
Wn =
âˆ‘
kâ‰¥1
1[Yk = 1](ln k)2kâˆ’ğœ†.
It can be veriï¬ed that
âˆ‚2ln
âˆ‚C2 = 2n2âˆ’2âˆ•ğœ†0(un, Un)Î©n(un, Un)ğœ,
âˆ‚2ğ“n
âˆ‚ğœ†2 = âˆ’2n2âˆ’2âˆ•ğœ†0C(wn, Wn)Î©n(T1 âˆ’ğœ‹âˆ—
0, T2 âˆ’ğœ‹âˆ—
1)ğœ
+ 2n2âˆ’2âˆ•ğœ†0C2(vn, Vn)Î©n(vn, Vn)ğœ,
âˆ‚2ğ“n
âˆ‚ğœ†âˆ‚C = 2n2âˆ’2âˆ•ğœ†0(vn, Vn)Î©n(T1 âˆ’ğœ‹âˆ—
0, T2 âˆ’ğœ‹âˆ—
1)ğœ
âˆ’2n2âˆ’2âˆ•ğœ†0C(vn, Vn)Î©n(un, Un)ğœ.
(7.10)
Let C0 and ğœ†0 be the true values of C and ğœ†. For notation simplicity, in the
statements of Lemma 7.7, let un, vn, wn, Un, Vn, Wn be those as deï¬ned earlier
but evaluated speciï¬cally at (C, ğœ†) = (C0, ğœ†0).
Lemma 7.7
Under the power tail model in (7.1), for any Îµ > 0,
1) n1âˆ’1âˆ•ğœ†0un
pâ†’
1
ğœ†0C
1âˆ’1âˆ•ğœ†0
0
Î“(1 âˆ’1âˆ•ğœ†0),
2) n1âˆ’1âˆ•ğœ†0Un
pâ†’
1
ğœ†0C
1âˆ’1âˆ•ğœ†0
0
Î“(2 âˆ’1âˆ•ğœ†0),
3) P(n2âˆ’2âˆ•ğœ†0(vn, Vn)Î©n(vn, Vn)ğœ> 0) âˆ’âˆ’âˆ’â†’1,
4) n1âˆ’1âˆ•ğœ†0âˆ’Îµvn
pâ†’0,
5) n1âˆ’1âˆ•ğœ†0âˆ’ÎµVn
pâ†’0,
6) n1âˆ’1âˆ•ğœ†0âˆ’Îµwn
pâ†’0,
7) n1âˆ’1âˆ•ğœ†0âˆ’ÎµWn
pâ†’0,
8) P(n2âˆ’2âˆ•ğœ†0(vnUn âˆ’unVn) > 0) â†’1.
The proofs of the Parts 1, 3, 4, and 8 of Lemma 7.7 are given in Appendix. The
proofs of Parts 2, 5, 6, and 7 are left as exercises.
Proof of Theorem 7.1: The ï¬rst two expressions in (7.10) are the diagonal ele-
ments of the Hessian matrix of ln, and the third is the oï¬€-diagonal element.
The objective of establishing that the probability of the Hessian matrix being
positive deï¬nite, at (C0, ğœ†0), converges to 1 as n â†’âˆis served by showing the

250
Statistical Implications of Turingâ€™s Formula
probabilities of the following three events each, at (C0, ğœ†0), converges to 1 as
n â†’âˆ:
(a) âˆ‚2ln
âˆ‚C2
|||||(C,ğœ†)=(C0,ğœ†0)
> 0,
(b) âˆ‚2ln
âˆ‚ğœ†2
|||||(C,ğœ†)=(C0,ğœ†0)
> 0,
and
(c)
(âˆ‚2ln
âˆ‚C2
) (âˆ‚2ln
âˆ‚ğœ†2
)
âˆ’
( âˆ‚2ln
âˆ‚ğœ†âˆ‚C
)2|||||(C,ğœ†)=(C0,ğœ†0)
> 0.
By (1) and (2) of Lemma 7.7, and the fact that Î©n converges to a positive deï¬nite
matrix in probability, (a) follows immediately.
For (b), since n1âˆ’1âˆ•(2ğœ†0)(T1 âˆ’ğœ‹âˆ—
0, T2 âˆ’ğœ‹âˆ—
1)ğœconverges to a bivariate normal vec-
tor, by (6) and (7) of Lemma 7.7, the ï¬rst term of âˆ‚2lnâˆ•âˆ‚ğœ†2|(C,ğœ†)=(C0,ğœ†0) is
âˆ’2n2âˆ’2âˆ•ğœ†0C(wn, Wn)Î©n(T1 âˆ’ğœ‹âˆ—
0, T2 âˆ’ğœ‹âˆ—
1)ğœ
= âˆ’2Cn1âˆ’3âˆ•(2ğœ†0)(wn, Wn)Î©n[n1âˆ’1âˆ•(2ğœ†0)(T1 âˆ’ğœ‹âˆ—
0, T2 âˆ’ğœ‹âˆ—
1)ğœ]
p
âˆ’âˆ’âˆ’â†’0.
Then (b) follows from (3) of Lemma 7.7.
For (c),
(âˆ‚2ln
âˆ‚C2
) (âˆ‚2ln
âˆ‚ğœ†2
)
âˆ’
( âˆ‚2ln
âˆ‚ğœ†âˆ‚C
)2|||||(C,ğœ†)=(C0,ğœ†0)
= âˆ’4Cn4âˆ’4âˆ•ğœ†0[(un, Un)Î©n(un, Un)ğœ] [(wn, Wn)Î©n(T1 âˆ’ğœ‹âˆ—
0, T2 âˆ’ğœ‹âˆ—
1)ğœ]
+ 4C2n4âˆ’4âˆ•ğœ†0[(un, Un)Î©n(un, Un)ğœ] [(vn, Vn)Î©n(vn, Vn)ğœ]
âˆ’4n4âˆ’4âˆ•ğœ†0[(vn, Vn)Î©n(T1 âˆ’ğœ‹âˆ—
0, T2 âˆ’ğœ‹âˆ—
1)ğœ]2
âˆ’4C2n4âˆ’4âˆ•ğœ†0[(vn, Vn)Î©n(un, Un)ğœ]2
+ 8Cn4âˆ’4âˆ•ğœ†0[(vn, Vn)Î©n(T1 âˆ’ğœ‹âˆ—
0, T2 âˆ’ğœ‹âˆ—
1)ğœ] [(vn, Vn)Î©n(un, Un)ğœ].
The ï¬rst, third, and ï¬fth terms in the last expression above all converge to 0 in
probability, and therefore after a few algebraic steps,
(âˆ‚2ln
âˆ‚C2
) (âˆ‚2ln
âˆ‚ğœ†2
)
âˆ’
( âˆ‚2ln
âˆ‚ğœ†âˆ‚C
)2
= 4C2n4âˆ’4âˆ•ğœ†0[(un, Un)Î©n(un, Un)ğœ] [(vn, Vn)Î©n(vn, Vn)ğœ]
âˆ’4C2n4âˆ’4âˆ•ğœ†0[(vn, Vn)Î©n(un, Un)ğœ]2 + op(1)
= 4C2n4âˆ’4âˆ•ğœ†0(vnUn âˆ’unVn)2 det(Î©n) + op(1).
The desired result follows from (8) of Lemma 7.7 and the fact that Î©n converges
to a positive deï¬nite matrix in probability.
â—½
Theorem 7.1 establishes the convexity of ln in an inï¬nitesimal neighborhood
of (C0, ğœ†0), but says nothing about ln globally. However, the next lemma does.

Estimation of Tail Probability
251
Lemma 7.8
Under the model in (7.1), if (C, ğœ†) â‰ (C0, ğœ†0), then there exists a
constant c > 0 such that, as n â†’âˆ,
P(ln(ğœ†, C) âˆ’ln(C0, ğœ†0) > c) â†’1.
The following theorem is a corollary of Lemma 7.8.
Theorem 7.2
Under the power tail model in (7.1), the AMLE of (C, ğœ†) is unique
for a suï¬ƒciently large n and is consistent.
Toward proving Lemma 7.8, the following re-expression of ln(C, ğœ†) in (7.9) is
needed.
ln(C, ğœ†)
= n2âˆ’2âˆ•ğœ†0(T1 âˆ’ğœ‹âˆ—
0(C, ğœ†), T2 âˆ’ğœ‹âˆ—
1(C, ğœ†))Î©n(T1 âˆ’ğœ‹âˆ—
0(C, ğœ†), T2 âˆ’ğœ‹âˆ—
1(C, ğœ†))ğœ
= ln(C0, ğœ†0) + [n2âˆ’2âˆ•ğœ†0(ğœ‹âˆ—
0(C0, ğœ†0) âˆ’ğœ‹âˆ—
0(C, ğœ†), ğœ‹âˆ—
1(C0, ğœ†0) âˆ’ğœ‹âˆ—
1(C, ğœ†))Î©n
Ã— (ğœ‹âˆ—
0(C0, ğœ†0) âˆ’ğœ‹âˆ—
0(C, ğœ†), ğœ‹âˆ—
1(C0, ğœ†0) âˆ’ğœ‹âˆ—
1(C, ğœ†))ğœ]
+ [2n2âˆ’2âˆ•ğœ†0(T1 âˆ’ğœ‹âˆ—
0(C0, ğœ†0), T2 âˆ’ğœ‹âˆ—
1(C0, ğœ†0))Î©n
Ã— (ğœ‹âˆ—
0(C0, ğœ†0) âˆ’ğœ‹âˆ—
0(C, ğœ†), ğœ‹âˆ—
1(C0, ğœ†0) âˆ’ğœ‹âˆ—
1(C, ğœ†))ğœ] .
(7.11)
To prove Lemma 7.8, Lemma 7.9 is needed. In both proofs of Lemmas 7.9 and
7.8, it is necessary to distinguish the notation ğœ‹âˆ—
0(C, ğœ†) from that of ğœ‹âˆ—
0(C0, ğœ†0),
and ğœ‹âˆ—
1(C, ğœ†) from that of ğœ‹âˆ—
1(C0, ğœ†0).
Lemma 7.9
Under the power tail model in (7.1),
1) if (C, ğœ†) âˆˆÎ˜1 but (C, ğœ†) â‰ (C0, ğœ†0), then
a) n1âˆ’1âˆ•ğœ†0ğœ‹âˆ—
0(C, ğœ†)
pâ†’0, and
b) n1âˆ’1âˆ•ğœ†0ğœ‹âˆ—
1(C, ğœ†)
pâ†’0;
2) if (C, ğœ†) âˆˆÎ˜2, then
a) n1âˆ’1âˆ•ğœ†0âˆ’1âˆ•(2ğœ†0)ğœ‹âˆ—
0(C, ğœ†)
pâ†’0, and
b) n1âˆ’1âˆ•ğœ†0âˆ’1âˆ•(2ğœ†0)ğœ‹âˆ—
1(C, ğœ†)
pâ†’0;
3) if (C, ğœ†) âˆˆÎ˜2 âˆªÎ˜3, then
a) n1âˆ’1âˆ•ğœ†0ğœ‹âˆ—
0(C, ğœ†)
pâ†’+âˆ, and
b) n1âˆ’1âˆ•ğœ†0ğœ‹âˆ—
1(C, ğœ†)
pâ†’+âˆ.
The proof of Lemma 7.9 is given in Appendix.
Proof of Lemma 7.8: The proof is given for three separate cases:
1) (C, ğœ†) âˆˆÎ˜1 but (C, ğœ†) â‰ (C0, ğœ†0);
2) (C, ğœ†) âˆˆÎ˜2; and
3) (C, ğœ†) âˆˆÎ˜2 âˆªÎ˜3.

252
Statistical Implications of Turingâ€™s Formula
For (1), it is desired to show (i) that the third term in (7.11) converges to 0
in probability, and (ii) that the second term in (7.11) converges to a positive
constant in probability. Toward proving (i), ï¬rst noting ğœ‹âˆ—
0(C0, ğœ†0) â‰¥0 and by
the proof of Case 1 of Theorem 6.2,
n1âˆ’1âˆ•ğœ†0E(ğœ‹âˆ—
0(C0, ğœ†0)) â†’C1âˆ•ğœ†0
ğœ†0
Î“
(
1 âˆ’1
ğœ†0
)
> 0,
hence
n1âˆ’1âˆ•ğœ†0âˆ’1âˆ•(2ğœ†0)E(ğœ‹âˆ—
0(C0, ğœ†0)) â†’0,
and therefore
n1âˆ’1âˆ•ğœ†0âˆ’1âˆ•(2ğœ†0)ğœ‹âˆ—
0(C0, ğœ†0)
p
âˆ’âˆ’âˆ’â†’0.
By (1) of Lemma 7.9,
n1âˆ’1âˆ•ğœ†0âˆ’1âˆ•(2ğœ†0)ğœ‹âˆ—
0(C, ğœ†)
p
âˆ’âˆ’âˆ’â†’0,
and therefore
n1âˆ’1âˆ•ğœ†0âˆ’1âˆ•(2ğœ†0)(ğœ‹âˆ—
0(C0, ğœ†0) âˆ’ğœ‹âˆ—
0(C, ğœ†))
p
âˆ’âˆ’âˆ’â†’0.
Similarly
n1âˆ’1âˆ•ğœ†0âˆ’1âˆ•(2ğœ†0)(ğœ‹âˆ—
1(C0, ğœ†0) âˆ’ğœ‹âˆ—
1(C, ğœ†))
p
âˆ’âˆ’âˆ’â†’0.
On the other hand, noting
n2âˆ’2âˆ•ğœ†0 = n1âˆ’1âˆ•(2ğœ†0)n1âˆ’1âˆ•ğœ†0âˆ’1âˆ•(2ğœ†0)
and by Theorem 1.8, each of
n1âˆ’1âˆ•(2ğœ†0)(T1 âˆ’ğœ‹âˆ—
0(C0, ğœ†0))
and
n1âˆ’1âˆ•(2ğœ†0)(T2 âˆ’ğœ‹âˆ—
1(C0, ğœ†0))
converges weakly to some normal random variable, and therefore (i) follows.
Again by the proof of Case 1 of Theorem 6.2 and (1) of Lemma 7.9, the vector
n1âˆ’1âˆ•ğœ†0(ğœ‹âˆ—
0(C0, ğœ†0) âˆ’ğœ‹âˆ—
0(C, ğœ†), ğœ‹âˆ—
1(C0, ğœ†0) âˆ’ğœ‹âˆ—
1(C, ğœ†))
converges to a nonzero vector in probability. Since Î©n converges to a positive
deï¬nite matrix in probability, the second term in (7.11) converges to a constant
c > 0, and hence (ii) follows, and therefore Lemma 7.8 follows for the case of
(C, ğœ†) âˆˆÎ˜1 but (C, ğœ†) â‰ (C0, ğœ†0).
For (2), it is desired to show (i) that the third term in (7.11) converges to
0 in probability, and (ii) that the second term in (7.11) converges to +âˆin
probability. However, (i) immediately follows from (2) of Lemma 7.9 and
(ii) immediately follows from (3) of Lemma 7.9.
For (3), ï¬rst by Corollary 7.2,
an = n1âˆ’1âˆ•ğœ†0(T1 âˆ’ğœ‹âˆ—
0(C0, ğœ†0))
p
âˆ’âˆ’âˆ’â†’0,
bn = n1âˆ’1âˆ•ğœ†0(T2 âˆ’ğœ‹âˆ—
1(C0, ğœ†0))
p
âˆ’âˆ’âˆ’â†’0;

Estimation of Tail Probability
253
and second by Lemma 7.9,
An = n1âˆ’1âˆ•ğœ†0(ğœ‹âˆ—
0(C0, ğœ†0) âˆ’ğœ‹âˆ—
0(C, ğœ†))
p
âˆ’âˆ’âˆ’â†’âˆ’âˆ,
Bn = n1âˆ’1âˆ•ğœ†0(ğœ‹âˆ—
1(C0, ğœ†0) âˆ’ğœ‹âˆ—
1(C, ğœ†))
p
âˆ’âˆ’âˆ’â†’âˆ’âˆ.
By Lemma 7.6,
ln(C, ğœ†) âˆ’ln(C0, ğœ†0))
= (An, Bn)Î©n(An, Bn)ğœ+ (an, bn)Î©n(An, Bn)ğœ
p
âˆ’âˆ’âˆ’â†’+âˆ.
â—½
Finally, by Corollary 7.2, an approximate 1 âˆ’ğ›¼conï¬dence region for (C, ğœ†)
may be given as follows:
(T1 âˆ’ğœ‹âˆ—
0(C, ğœ†)
Ì‚ğœ1
)2
âˆ’2 Ì‚ğœŒ1
(T1 âˆ’ğœ‹âˆ—
0(C, ğœ†)
Ì‚ğœ1
) (T2 âˆ’ğœ‹âˆ—
1(C, ğœ†)
Ì‚ğœ2
)
+
(T2 âˆ’ğœ‹âˆ—
1(C, ğœ†)
Ì‚ğœ2
)2
<
(1 âˆ’Ì‚ğœŒ2
1)
n2
ğœ’2
1âˆ’ğ›¼(2)
(7.12)
where ğœ’2
1âˆ’ğ›¼(2) is the 100 Ã— (1 âˆ’ğ›¼) th percentile of the chi-squared distribution
with 2 degrees of freedom.
Statistical inference for (C, ğœ†) may be made by means of the conï¬dence region
given in (7.12).
7.4
Remarks
Deï¬nition 7.1 may be extended to more general tail models as described in (7.2).
Under (7.2), for every r > 0, let every pk in ğœ‹râˆ’1 be replaced by pk = p(k; ğœ½) and
denote the resulting expression as ğœ‹âˆ—
râˆ’1, that is,
ğœ‹âˆ—
râˆ’1 =
âˆ‘
kâ‰¥1
p(k; ğœ½)1[Yk = r âˆ’1].
Then
ğœ‹râˆ’1 âˆ’ğœ‹âˆ—
râˆ’1 =
k0âˆ’1
âˆ‘
k=1
1[Yk = r âˆ’1](pk âˆ’p(k; ğœ½)),
and the sum is of ï¬nite terms. Since, for every k, both E(1[Yk = r âˆ’1])
and Var(1[Yk = r âˆ’1]) converge to zero exponentially in sample size n,
E(g(n, ğ›¿)1[Yk = r âˆ’1]) and Var(g(n, ğ›¿)1[Yk = r âˆ’1]) converge to zero, which
implies that g(n, ğ›¿)1[Yk = r âˆ’1]
p
âˆ’âˆ’âˆ’â†’0, which in turn implies that
g(n, ğ›¿)(ğœ‹âˆ—
râˆ’1 âˆ’ğœ‹râˆ’1)
p
âˆ’âˆ’âˆ’â†’0

254
Statistical Implications of Turingâ€™s Formula
for every r. If the tail model in (7.2) supports
n Ì‚Î£âˆ’1âˆ•2
(T1 âˆ’ğœ‹âˆ—
0
Ì‚ğœ1
,â€¦,
TR âˆ’ğœ‹âˆ—
Râˆ’1
Ì‚ğœR
)ğœ
L
âˆ’âˆ’âˆ’â†’MVN (0, IRÃ—R)
(7.13)
where Ì‚Î£âˆ’1âˆ•2 is as in Lemma 7.2, then the estimator of ğœ½given by the following
Deï¬nition may be studied fruitfully, as suggested by ( Ì‚C, Ì‚ğœ†) of (C, ğœ†) in the Pareto
tail model in (7.1).
Deï¬nition 7.2
If there exists a vector value, ğœ½= Ì‚ğœ½, which maximizes the like-
lihood given by (7.13) within the space allowed by the tail model in (7.2), then Ì‚ğœ½
is said to be an AMLE of ğœ½.
The theoretical issues along this path include establishing (1) weak con-
vergence of the type similar to (7.13), (2) consistency, uniqueness, and other
desirable properties of the estimator Ì‚ğœ½, none of which is easily accomplished.
However, if this path could be followed through, Turingâ€™s perspective may lend
itself to support broader and ï¬ner tail probability models.
Perhaps more interestingly, the methodology discussed in this chapter may
be extended to tail distributions beyond those with Pareto decay. Four cases
are oï¬€ered below for consideration.
Case 1. Suppose, instead of (7.1), the underlying tail model is
pk = Ckâˆ’ğœ†(1 + o(1))
(7.14)
for all k, k â‰¥k0, where k0 â‰¥1 is a positive integer, C > 0, and ğœ†> 1 are
unknown parameters, and o(1) â†’0 as k â†’âˆ. It can be shown that
all the results established under the exact Pareto tail model (7.1) still
hold. As a result, the AMLE, ( Ì‚C, Ì‚ğœ†), may be deï¬ned as in Deï¬nition
7.1 and enjoys the same statistical properties, such as uniqueness and
consistency. The proofs unfortunately become doubly tedious and are
not presented in this book.
Case 2. One of the most popular thick tail models in the literature of extreme
value theory is a cumulative distribution function of an underlying ran-
dom variable X satisfying
1 âˆ’FX(x) = P(X > x) = Cxâˆ’ğœ†
(7.15)
for all x â‰¥x0 where x0, C > 0, and ğœ†> 0 are unknown parameters.
Let (7.15) be referred to as the continuous Pareto tail model. The
estimation of the parameters in the tail model of (7.15) may also
be approached by the methodology of this chapter. Without loss of
generality, suppose the support of X is (0, âˆ). Consider a ğ›¿-binning,
where ğ›¿> 0 is an arbitrary but ï¬xed positive real number, of the
support of X, that is,
(0, ğ›¿], (ğ›¿, 2ğ›¿],â€¦, ((k âˆ’1)ğ›¿, kğ›¿],â€¦.

Estimation of Tail Probability
255
The probability of the kth bin is
pk = FX(kğ›¿) âˆ’FX((k âˆ’1)ğ›¿)
= C {[(k âˆ’1)ğ›¿]âˆ’ğœ†âˆ’(kğ›¿)âˆ’ğœ†}
= C
ğ›¿ğœ†
[
1
(k âˆ’1)ğœ†âˆ’1
kğœ†
]
= C
ğ›¿ğœ†
1
kğœ†
[(
k
k âˆ’1
)ğœ†
âˆ’1
]
= C
ğ›¿ğœ†
1
kğœ†+1
[(
k
k âˆ’1
)ğœ†
k âˆ’k
]
.
However, since
lim
kâ†’âˆ
[(
k
k âˆ’1
)ğœ†
k âˆ’k
]
= ğœ†,
(see Exercise 10)
pk = Cğœ†
ğ›¿ğœ†
1
kğœ†+1 (1 âˆ’o(1))
(7.16)
which has the form of (7.14). Consequently, by the argument in Case 1,
the AMLE of (C, ğœ†) of the continuous Pareto model in (7.15) may also
be obtained.
Case 3. Suppose the cumulative distribution function of an underlying random
variable X satisfying
1 âˆ’FX(x) = P(X > x) = Ceâˆ’ğœ†x
(7.17)
for all x â‰¥x0 where x0, C > 0, and ğœ†> 0 are unknown parameters.
Let (7.17) be referred to as the continuous exponential tail model. The
estimation of the parameters in the tail model of (7.17) may also be
approached by the methodology of this chapter. Consider ï¬rst a trans-
formation of X,
Y = eX,
which entails
1 âˆ’FY(y) = Cyâˆ’ğœ†
for suï¬ƒciently large y, that is, y â‰¥ex0. By the argument of Case 2, the
AMLE of (C, ğœ†) of the continuous exponential model in (7.17) may also
be obtained.
Case 4. Suppose the cumulative distribution function of an underlying random
variable X satisfying
1 âˆ’FX(x) = P(X > x) = Ceâˆ’ğœ†x2
(7.18)

256
Statistical Implications of Turingâ€™s Formula
for all x â‰¥x0 where x0 â‰¥0, C > 0, and ğœ†> 0 are unknown parameters.
Let (7.18) be referred to as the continuous Gaussian tail model. The
estimation of the parameters in the tail model of (7.18) may also be
approached by the methodology of this chapter. Consider ï¬rst a trans-
formation of X,
Y = eX2,
which entails
1 âˆ’FY(y) = Cyâˆ’ğœ†
for suï¬ƒciently large y, that is, y â‰¥ex2
0. By the argument of Case 2, the
AMLE of (C, ğœ†) of the continuous Gaussian tail model in (7.18) may
also be obtained.
Cases 1â€“4 suggest that the methodology discussed in this chapter may be
extended to models beyond the family of distributions with power decay tails.
In fact, the methodology may be extended to tackle various tail models for con-
tinuous random variables, where the choice of ğ›¿, possibly of the form ğ›¿= ğ›¿(n),
in practice may prove to be an interesting issue of investigation, among many
other meaningful research undertakings along this thread.
7.5
Appendix
7.5.1
Proof of Lemma 7.7
Proof: For Part 1, it suï¬ƒces to show
n1âˆ’1âˆ•ğœ†E(un) â†’[1âˆ•(ğœ†C1âˆ’1âˆ•ğœ†)]Î“(1 âˆ’1âˆ•ğœ†), and
n2âˆ’2âˆ•ğœ†Var(un) â†’0.
By the argument in the proof of Lemma 6.3,
n1âˆ’1âˆ•ğœ†E(un) âˆ¼n1âˆ’1âˆ•ğœ†âˆ‘
kâˆ’ğœ†eâˆ’nCkâˆ’ğœ†;
and invoking the Eulerâ€“Maclaurin lemma,
lim n1âˆ’1âˆ•ğœ†E(un) = lim n1âˆ’1âˆ•ğœ†
âˆ«
âˆ
x=1
xâˆ’ğœ†eâˆ’nCxâˆ’ğœ†dx
=
1
ğœ†C1âˆ’1âˆ•ğœ†lim âˆ«
nC
x=0
yâˆ’1âˆ•ğœ†eâˆ’ydy
=
1
ğœ†C1âˆ’1âˆ•ğœ†Î“(1 âˆ’1âˆ•ğœ†).

Estimation of Tail Probability
257
Var(un) = E(u2
n) âˆ’[E(un)]2
=
[
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)nkâˆ’2ğœ†+
âˆ‘
kâ‰ j
(1 âˆ’Ckâˆ’ğœ†âˆ’Cjâˆ’ğœ†)nkâˆ’ğœ†jâˆ’ğœ†
]
âˆ’
[
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)2nkâˆ’2ğœ†+
âˆ‘
kâ‰ j
(1 âˆ’Ckâˆ’ğœ†)n(1 âˆ’Cjâˆ’ğœ†)nkâˆ’ğœ†jâˆ’ğœ†
]
â‰¤
[
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)nkâˆ’2ğœ†+
âˆ‘
kâ‰ j
(1 âˆ’Ckâˆ’ğœ†)n(1 âˆ’Cjâˆ’ğœ†)nkâˆ’ğœ†jâˆ’ğœ†
]
âˆ’
[
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)2nkâˆ’2ğœ†+
âˆ‘
kâ‰ j
(1 âˆ’Ckâˆ’ğœ†)n(1 âˆ’Cjâˆ’ğœ†)nkâˆ’ğœ†jâˆ’ğœ†
]
=
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)nkâˆ’2ğœ†âˆ’
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)2nkâˆ’2ğœ†
â‰¤
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)nkâˆ’2ğœ†.
Invoking the Eulerâ€“Maclaurin lemma whenever needed,
lim
nâ†’âˆn2âˆ’2âˆ•ğœ†Var(un) â‰¤lim n2âˆ’2âˆ•ğœ†âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)nkâˆ’2ğœ†
= lim
nâ†’âˆn2âˆ’2âˆ•ğœ†
âˆ«
âˆ
1
xâˆ’2ğœ†eâˆ’Cnxâˆ’ğœ†dx
= lim
nâ†’âˆ
nâˆ’1âˆ•ğœ†
ğœ†C2âˆ’1âˆ•ğœ†Î“(2 âˆ’1âˆ•ğœ†) = 0.
A similar argument establishes Part 2.
For Part 3, since Î©n converges to a positive deï¬nite matrix in probability, it
suï¬ƒces to show that P(n1âˆ’1âˆ•ğœ†vn > 0) â†’1, which is clear by noting
n1âˆ’1âˆ•ğœ†vn = n1âˆ’1âˆ•ğœ†âˆ‘
kâ‰¥2
1[Yk = 0](ln k)kâˆ’ğœ†â‰¥n1âˆ’1âˆ•ğœ†ln 2
âˆ‘
kâ‰¥2
1[Yk = 0]kâˆ’ğœ†
= âˆ’n1âˆ’1âˆ•ğœ†(ln 2)1[Y1 = 0] + n1âˆ’1âˆ•ğœ†ln 2
âˆ‘
1[Yk = 0]kâˆ’ğœ†
= op(1) + n1âˆ’1âˆ•ğœ†(ln 2)un
p
âˆ’âˆ’âˆ’â†’
ln 2
ğœ†C1âˆ’1âˆ•ğœ†Î“(1 âˆ’1âˆ•ğœ†) > 0.
For Part 4, ï¬rst consider the fact that a positive integer k0 exists such that
kÎµ > ln k for all k â‰¥k0.
vn =
âˆ‘
k<k0
1[Yk = 0](ln k)kâˆ’ğœ†+
âˆ‘
kâ‰¥k0
1[Yk = 0](ln k)kâˆ’ğœ†
â‰¤ln k0
âˆ‘
k<k0
1[Yk = 0] +
âˆ‘
kâ‰¥k0
1[Yk = 0]kâˆ’ğœ†+Îµ.

258
Statistical Implications of Turingâ€™s Formula
E
(
ln k0
âˆ‘
k<k0
1[Yk = 0]
)
= ln k0
âˆ‘
k<k0
(1 âˆ’Ckâˆ’ğœ†)n â‰¤k0 ln k0
(1 âˆ’Ckâˆ’ğœ†
0
)n.
lim
nâ†’âˆn1âˆ’1âˆ•ğœ†âˆ’ÎµE
(
ln(k0)
âˆ‘
k<k0
1[xk = 0]
)
= 0.
Invoking the Eulerâ€“Maclaurin lemma whenever needed,
lim
nâ†’âˆE
(
âˆ‘
kâ‰¥k0
1[Yk = 0]kâˆ’ğœ†+Îµ
)
= lim
nâ†’âˆâˆ«
âˆ
1
xâˆ’ğœ†+Îµeâˆ’nCxâˆ’ğœ†dx
= 1
ğœ†Câˆ’[1âˆ’(Îµ+1)âˆ•ğœ†)] lim
nâ†’âˆnâˆ’[1âˆ’(Îµ+1)âˆ•ğœ†]
âˆ«
nC
0
y[1âˆ’(Îµ+1)âˆ•ğœ†)]âˆ’1eâˆ’ydy.
lim
nâ†’âˆn1âˆ’1âˆ•ğœ†âˆ’ÎµE(vn)
= 1
ğœ†Câˆ’[1âˆ’(Îµ+1)âˆ•ğœ†)] lim
nâ†’âˆnâˆ’(1âˆ’1âˆ•ğœ†)Îµ
âˆ«
nC
0
y[1âˆ’(Îµ+1)âˆ•ğœ†)]âˆ’1eâˆ’ydy â†’0.
Var(vn) = E(v2
n) âˆ’[E(vn)]2
= E
[
âˆ‘
kâ‰¥1
1[Yk = 0](ln k)2kâˆ’2ğœ†
+
âˆ‘
kâ‰ j
1[Yk = 0, Yj = 0](ln k)(ln j)(kj)âˆ’ğœ†
]
âˆ’
[
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)2n(ln k)2kâˆ’2ğœ†
+
âˆ‘
kâ‰ j
(1 âˆ’Ckâˆ’ğœ†)n(1 âˆ’Cjâˆ’ğœ†)n(ln k)(ln j)(kj)âˆ’ğœ†
]
=
[
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)n(ln k)2kâˆ’2ğœ†
+
âˆ‘
kâ‰ j
(1 âˆ’Ckâˆ’ğœ†âˆ’Cjâˆ’ğœ†)n(ln k)(ln j)(kj)âˆ’ğœ†
]
âˆ’
[
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)2n(ln k)2kâˆ’2ğœ†
+
âˆ‘
kâ‰ j
(1 âˆ’Ckâˆ’ğœ†)n(1 âˆ’Cjâˆ’ğœ†)n ln(k) ln(j)(kj)âˆ’ğœ†
]

Estimation of Tail Probability
259
â‰¤
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)n(ln k)2kâˆ’2ğœ†
=
âˆ‘
k<k0
(1 âˆ’Ckâˆ’ğœ†)n(ln k)2kâˆ’2ğœ†
+
âˆ‘
kâ‰¥k0
(1 âˆ’Ckâˆ’ğœ†)n(ln k)2kâˆ’2ğœ†
â‰¤(ln k0)2k0(1 âˆ’Ckâˆ’ğœ†
0 )n +
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)nkâˆ’2ğœ†+2Îµ.
lim
nâ†’âˆn2âˆ’2âˆ•ğœ†âˆ’2Îµ(ln k0)2k0(1 âˆ’Ckâˆ’ğœ†
0 )n â†’0.
Invoking the Eulerâ€“Maclaurin lemma,
lim
nâ†’âˆn2âˆ’2âˆ•ğœ†âˆ’2Îµ âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)nkâˆ’2ğœ†+2Îµ
= lim
nâ†’âˆn2âˆ’2âˆ•ğœ†âˆ’2Îµ
âˆ«
âˆ
1
xâˆ’2(ğœ†âˆ’Îµ)eâˆ’nCxâˆ’ğœ†dx
= lim
nâ†’âˆn2âˆ’2âˆ•ğœ†âˆ’2Îµ
[(nC)âˆ’2+(2Îµ+1)âˆ•ğœ†
ğœ†
âˆ«
nC
0
y[2âˆ’(2Îµ+1)âˆ•ğœ†]âˆ’1eâˆ’ydy
]
= Câˆ’2+(2Îµ+1)âˆ•ğœ†
ğœ†
lim
nâ†’âˆnâˆ’2+(2Îµ+1)âˆ•ğœ†n2âˆ’2âˆ•ğœ†âˆ’2Îµ
Ã—
[
âˆ«
nC
0
y[2âˆ’(2Îµ+1)âˆ•ğœ†]âˆ’1eâˆ’ydy
]
= Câˆ’2+(2Îµ+1)âˆ•ğœ†
ğœ†
lim
nâ†’âˆnâˆ’1âˆ•ğœ†âˆ’2Îµ(1âˆ’1âˆ•ğœ†)
[
âˆ«
nC
0
y[2âˆ’(2Îµ+1)âˆ•ğœ†]âˆ’1eâˆ’ydy
]
â†’0.
Therefore Var(n1âˆ’1âˆ•ğœ†âˆ’Îµvn) â†’0, and hence n1âˆ’1âˆ•ğœ†âˆ’Îµvn
pâ†’0.
A similar argument establishes each of Parts 5â€“7.
For Part 8, it is to show that n2âˆ’2âˆ•ğœ†(vnUn âˆ’unVn) converges to a positive con-
stant in probability. By the results of Parts 1 and 2 already established, it is
equivalent to show that
n1âˆ’1âˆ•ğœ†
(Î“(2 âˆ’1âˆ•ğœ†)
ğœ†C1âˆ’1âˆ•ğœ†
vn+1 âˆ’Î“(1 âˆ’1âˆ•ğœ†)
ğœ†C1âˆ’1âˆ•ğœ†
Vn+1
)
= n1âˆ’1âˆ•ğœ†Î“(1 âˆ’1âˆ•ğœ†)
ğœ†C1âˆ’1âˆ•ğœ†
[(1 âˆ’1âˆ•ğœ†)vn+1 âˆ’Vn+1]
converges to a positive constant in probability, or that
n1âˆ’1âˆ•ğœ†[(1 âˆ’1âˆ•ğœ†)vn+1 âˆ’Vn+1]
does.
Since
n1âˆ’1âˆ•ğœ†[(1 âˆ’1âˆ•ğœ†)vn+1 âˆ’Vn+1]
= n1âˆ’1âˆ•ğœ†[(1 âˆ’1âˆ•ğœ†)vn âˆ’Vn+1] âˆ’n1âˆ’1âˆ•ğœ†(1 âˆ’1âˆ•ğœ†)(vn âˆ’vn+1),

260
Statistical Implications of Turingâ€™s Formula
it is ï¬rst to show n1âˆ’1âˆ•ğœ†(vn âˆ’vn+1)
pâ†’0.
E(vn âˆ’vn+1) =
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)n(ln k)kâˆ’ğœ†âˆ’
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)n+1(ln k)kâˆ’ğœ†
= C
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)n(ln k)kâˆ’2ğœ†
â‰¤C(ln k0)k0(1 âˆ’Ckâˆ’ğœ†
0 )n + C
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)nkâˆ’2ğœ†+Îµ.
lim
nâ†’âˆn1âˆ’1âˆ•ğœ†C ln(k0)k0(1 âˆ’Ckâˆ’ğœ†
0 )n = 0.
Let Îµ = ğœ†âˆ•2. Invoking the Eulerâ€“Maclaurin lemma,
lim
nâ†’âˆn1âˆ’1âˆ•ğœ†C
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)nkâˆ’2ğœ†+Îµ
= lim
nâ†’âˆn1âˆ’1âˆ•ğœ†C âˆ«
âˆ
1
xâˆ’2ğœ†+Îµeâˆ’nCxâˆ’ğœ†dx
= Câˆ’1+(Îµ+1)âˆ•ğœ†
ğœ†
lim
nâ†’âˆnâˆ’1+Îµâˆ•ğœ†
âˆ«
nC
0
y[2âˆ’(Îµ+1)âˆ’1]eâˆ’ydy
= 0.
Therefore, n1âˆ’1âˆ•ğœ†E(vn âˆ’vn+1) â†’0. Next, it is to show
Var (n1âˆ’1âˆ•ğœ†(vn âˆ’vn+1)) â†’0.
However, since
Var(vn âˆ’vn+1) â‰¤2(Var(vn) + Var(vn+1)),
it suï¬ƒces to show
Var (n1âˆ’1âˆ•ğœ†vn
) â†’0.
In the proof of above-mentioned Part 4, it has already been shown that
Var(vn) â‰¤
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)nkâˆ’2ğœ†+2Îµ.
Therefore, letting Îµ = 1âˆ•4 and invoking the Eulerâ€“Maclaurin lemma,
lim
nâ†’âˆn2âˆ’2âˆ•ğœ†Var(vn) â‰¤lim
nâ†’âˆn2âˆ’2âˆ•ğœ†âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)nkâˆ’2ğœ†+2Îµ
= lim
nâ†’âˆn2âˆ’2âˆ•ğœ†
âˆ«
âˆ
1
xâˆ’2ğœ†+2Îµeâˆ’nCxâˆ’ğœ†dx
= Câˆ’2+(2Îµ+1)âˆ•ğœ†
ğœ†
lim
nâ†’âˆnâˆ’(1âˆ’2Îµ)âˆ•ğœ†
âˆ«
nC
0
y[2âˆ’(2Îµ+1)âˆ•ğœ†]âˆ’1eâˆ’ydy
= 0.

Estimation of Tail Probability
261
Since n1âˆ’1âˆ•ğœ†(vn âˆ’vn+1)
pâ†’0, it only remains to show that
n1âˆ’1âˆ•ğœ†[(1 âˆ’1âˆ•ğœ†)vn âˆ’Vn+1]
converges to a positive constant in probability.
E ((1 âˆ’1âˆ•ğœ†)vn âˆ’Vn+1
) = (1 âˆ’1âˆ•ğœ†)
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)n(ln k)kâˆ’ğœ†
âˆ’
âˆ‘
kâ‰¥1
n(1 âˆ’Ckâˆ’ğœ†)nCkâˆ’ğœ†(ln k)kâˆ’ğœ†
=
âˆ‘
kâ‰¥1
(1 âˆ’Ckğœ†)n(ln k)[(1 âˆ’1âˆ•ğœ†)kâˆ’ğœ†âˆ’nCkâˆ’2ğœ†].
Letting y = nCxâˆ’ğœ†and invoking the Eulerâ€“Maclaurin lemma, after a few
algebraic steps,
lim
nâ†’âˆn1âˆ’1âˆ•ğœ†E((1 âˆ’1âˆ•ğœ†)vn âˆ’Vn+1)
= lim
nâ†’âˆn1âˆ’1âˆ•ğœ†âˆ‘
kâ‰¥1
{(1 âˆ’Ckâˆ’ğœ†)n(ln k) [(1 âˆ’1âˆ•ğœ†)kâˆ’ğœ†âˆ’nCkâˆ’2ğœ†]}
= lim
nâ†’âˆn1âˆ’1âˆ•ğœ†
âˆ«
âˆ
1
(ln x) [(1 âˆ’1âˆ•ğœ†)xâˆ’ğœ†âˆ’nCxâˆ’2ğœ†] eâˆ’nCxâˆ’ğœ†dx
=
1
ğœ†2C1âˆ’1âˆ•ğœ†lim
nâ†’âˆâˆ«
nC
0
ln(nC)
[(
1 âˆ’1
ğœ†
)
yâˆ’1
ğœ†âˆ’y1âˆ’1
ğœ†
]
eâˆ’ydy
âˆ’
1
ğœ†2C1âˆ’1âˆ•ğœ†lim
nâ†’âˆâˆ«
nC
0
(ln y)
[(
1 âˆ’1
ğœ†
)
yâˆ’1
ğœ†âˆ’y1âˆ’1
ğœ†
]
eâˆ’ydy.
The ï¬rst term of the last expression is zero. This is so because ï¬rst,
lim
nâ†’âˆâˆ«
nC
0
ln(nC)
[(
1 âˆ’1
ğœ†
)
yâˆ’1
ğœ†âˆ’y1âˆ’1
ğœ†
]
eâˆ’ydy
= lim
nâ†’âˆâˆ«
âˆ
0
ln(nC)
[(
1 âˆ’1
ğœ†
)
yâˆ’1
ğœ†âˆ’y1âˆ’1
ğœ†
]
eâˆ’ydy
âˆ’lim
nâ†’âˆâˆ«
âˆ
nC
ln(nC)
[(
1 âˆ’1
ğœ†
)
yâˆ’1
ğœ†âˆ’y1âˆ’1
ğœ†
]
eâˆ’ydy
= lim
nâ†’âˆâˆ«
âˆ
nC
ln(nC)
[
y1âˆ’1
ğœ†âˆ’
(
1 âˆ’1
ğœ†
)
yâˆ’1
ğœ†
]
eâˆ’ydy,
and second, (1 âˆ’1âˆ•ğœ†)yâˆ’1âˆ•ğœ†âˆ’y1âˆ’1âˆ•ğœ†> 0 for suï¬ƒciently large y, and third, (1 âˆ’
1âˆ•ğœ†)yâˆ’1âˆ•ğœ†âˆ’y1âˆ’1âˆ•ğœ†eâˆ’yâˆ•2 is decreasing for suï¬ƒciently large y, and consequently,
lim
nâ†’âˆâˆ«
nC
0
ln(nC)
[(
1 âˆ’1
ğœ†
)
yâˆ’1
ğœ†âˆ’y1âˆ’1
ğœ†
]
eâˆ’ydy
â‰¤lim
nâ†’âˆln(nC)
[
(nC)1âˆ’1
ğœ†âˆ’
(
1 âˆ’1
ğœ†
)
(nC)âˆ’1
ğœ†
]
eâˆ’(nC)âˆ•2
âˆ«
âˆ
nC
eâˆ’yâˆ•2dy
â‰¤2 lim
nâ†’âˆln(nC)
[
(nC)1âˆ’1
ğœ†âˆ’
(
1 âˆ’1
ğœ†
)
(nC)âˆ’1
ğœ†
]
eâˆ’(nC)âˆ•2 = 0.

262
Statistical Implications of Turingâ€™s Formula
This leads to
lim
nâ†’âˆn1âˆ’1âˆ•ğœ†E((1 âˆ’1âˆ•ğœ†)vn âˆ’Vn+1)
=
1
ğœ†2C1âˆ’1âˆ•ğœ†âˆ«
âˆ
0
(ln y)
[
y1âˆ’1
ğœ†âˆ’
(
1 âˆ’1
ğœ†
)
yâˆ’1
ğœ†
]
eâˆ’ydy.
Denoting the integrand above as
K(y) = (ln y)[y1âˆ’1âˆ•ğœ†âˆ’(1 âˆ’1âˆ•ğœ†)yâˆ’1âˆ•ğœ†]eâˆ’y,
it is easy to see that K(y) is positive on (0, 1 âˆ’1âˆ•ğœ†), negative on (1 âˆ’1âˆ•ğœ†, 1), and
positive on (1, âˆ).
âˆ«
1âˆ’1âˆ•ğœ†
0
K(y)dy â‰¥eâˆ’(1âˆ’1âˆ•ğœ†)
âˆ«
1âˆ’1âˆ•ğœ†
0
(ln y)
[
y1âˆ’1
ğœ†âˆ’
(
1 âˆ’1
ğœ†
)
yâˆ’1
ğœ†
]
dy
= eâˆ’(1âˆ’1âˆ•ğœ†)
âˆ«
âˆ
âˆ’ln(1âˆ’1âˆ•ğœ†)
t
[(
1 âˆ’1
ğœ†
)
eâˆ’(1âˆ’1âˆ•ğœ†)t âˆ’eâˆ’(2âˆ’1âˆ•ğœ†)t]
dt,
âˆ«
1
1âˆ’1âˆ•ğœ†
K(y)dy â‰¥eâˆ’(1âˆ’1âˆ•ğœ†)
âˆ«
1
1âˆ’1âˆ•ğœ†
[
y1âˆ’1âˆ•ğœ†âˆ’
(
1 âˆ’1
ğœ†
)
yâˆ’1âˆ•ğœ†]
ln(y)dy
= eâˆ’(1âˆ’1âˆ•ğœ†)
âˆ«
âˆ’ln(1âˆ’1âˆ•ğœ†)
0
t
[(
1 âˆ’1
ğœ†
)
eâˆ’(1âˆ’1âˆ•ğœ†)t âˆ’eâˆ’(2âˆ’1âˆ•ğœ†)t]
dt,
âˆ«
âˆ
0
K(y)dy > âˆ«
1âˆ’1âˆ•ğœ†
0
K(y)dy + âˆ«
1
1âˆ’1âˆ•ğœ†
K(y)dy
â‰¥eâˆ’(1âˆ’1âˆ•ğœ†)
âˆ«
âˆ
0
t
[(
1 âˆ’1
ğœ†
)
eâˆ’(1âˆ’1âˆ•ğœ†)t âˆ’eâˆ’(2âˆ’1âˆ•ğœ†)t]
dt
= eâˆ’(1âˆ’1âˆ•ğœ†)[1âˆ•(1 âˆ’1âˆ•ğœ†) âˆ’1âˆ•(2 âˆ’1âˆ•ğœ†)2] > 0,
that is,
n1âˆ’1âˆ•ğœ†E((1 âˆ’1âˆ•ğœ†)vn âˆ’Vn+1) â†’(ğœ†2C1âˆ’1âˆ•ğœ†)âˆ’1
âˆ«
âˆ
0
K(y)dy > 0.
It still remains to show that
Var(n1âˆ’1âˆ•ğœ†[(1 âˆ’1âˆ•ğœ†)vn âˆ’Vn+1]) â†’0.
However, since it has been shown above that
Var(n1âˆ’1âˆ•ğœ†vn) â†’0,
it suï¬ƒces to show that
Var(n1âˆ’1âˆ•ğœ†Vn+1) â†’0.

Estimation of Tail Probability
263
Var(Vn+1) = E(V 2
n+1) âˆ’[E(Vn+1)]2
=
[
âˆ‘
kâ‰¥1
(n + 1)Ckâˆ’ğœ†(1 âˆ’Ckâˆ’ğœ†)n(ln k)2kâˆ’2ğœ†
+
âˆ‘
kâ‰ j
(n + 1)nCkâˆ’ğœ†Cjâˆ’ğœ†(1 âˆ’Ckâˆ’ğœ†âˆ’Cjâˆ’ğœ†)nâˆ’1(ln k)(ln j)kâˆ’ğœ†jâˆ’ğœ†
]
âˆ’
[
âˆ‘
kâ‰¥1
(n + 1)2C2kâˆ’2ğœ†(1 âˆ’Ckâˆ’ğœ†)2n(ln k)2kâˆ’2ğœ†
+
âˆ‘
kâ‰ j
(n + 1)2Ckâˆ’ğœ†Cjâˆ’ğœ†(1 âˆ’Ckâˆ’ğœ†)n(1 âˆ’Cjâˆ’ğœ†)n
Ã— (ln k)(ln j)kâˆ’ğœ†jâˆ’ğœ†
]
â‰¤(n + 1)C
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)n(ln k)2kâˆ’3ğœ†
= (n + 1)C
âˆ‘
k<k0
(1 âˆ’Ckâˆ’ğœ†)n(ln k)2kâˆ’3ğœ†
+ (n + 1)C
âˆ‘
kâ‰¥k0
(1 âˆ’Ckâˆ’ğœ†)n(ln k)2kâˆ’3ğœ†
â‰¤(n + 1)C
âˆ‘
k<k0
(1 âˆ’Ckâˆ’ğœ†)n(ln k)2kâˆ’3ğœ†
+ (n + 1)C
âˆ‘
kâ‰¥1
(1 âˆ’Ckâˆ’ğœ†)nkâˆ’3ğœ†+2Îµ.
The ï¬rst term in the last expression converges to zero exponentially, and
therefore for Îµ = 1âˆ•4, invoking the Eulerâ€“Maclaurin lemma,
lim
nâ†’âˆVar(n1âˆ’1âˆ•ğœ†Vn+1) â‰¤C lim
nâ†’âˆn2âˆ’2âˆ•ğœ†(n + 1) âˆ«
âˆ
1
xâˆ’3ğœ†+2Îµeâˆ’nCxâˆ’ğœ†dx
= Câˆ’2+(2Îµ+1)âˆ•ğœ†
ğœ†
lim
nâ†’âˆn2âˆ’2âˆ•ğœ†(n + 1)nâˆ’3+(2Îµ+1)âˆ•ğœ†
Ã— âˆ«
nC
0
y[3âˆ’(2Îµ+1)âˆ•ğœ†]âˆ’1eâˆ’ydy = 0.
7.5.2
Proof of Lemma 7.9
Proof of Part 1: For Part (a), ï¬rst consider the function
fn(x) = n1âˆ’1âˆ•ğœ†0(1 âˆ’C0xâˆ’ğœ†0)nCxâˆ’ğœ†
deï¬ned for x > x0 where x0 is suï¬ƒciently large a constant to ensure that
1 âˆ’C0xâˆ’ğœ†0 â‰¥0. It can be easily veriï¬ed that fn(x) is decreasing for all

264
Statistical Implications of Turingâ€™s Formula
x â‰¥x(n) = [C0(nğœ†0 + ğœ†)âˆ•ğœ†]1âˆ•ğœ†0 and is increasing for x âˆˆ[x0, x(n)]. For any ğœ†
satisfying ğœ†> ğœ†0 âˆ’1,
fn(x(n)) = CCâˆ’ğœ†âˆ•ğœ†0
0
(
1 âˆ’
ğœ†
nğœ†0 + ğœ†
)n
(1 + nğœ†0
ğœ†)âˆ’ğœ†âˆ•ğœ†0n1âˆ’1âˆ•ğœ†0
= ğ’ª(n[(ğœ†0âˆ’1)âˆ’ğœ†]âˆ•ğœ†0) â†’0.
Invoking the Eulerâ€“Maclaurin lemma,
lim
nâ†’âˆn1âˆ’1âˆ•ğœ†0 âˆ‘
k>x0
(1 âˆ’C0kâˆ’ğœ†0)nCkâˆ’ğœ†
= lim
nâ†’âˆn1âˆ’1âˆ•ğœ†0
âˆ«
âˆ
x0
(1 âˆ’C0xâˆ’ğœ†0)nCxâˆ’ğœ†dx
= C lim
nâ†’âˆn1âˆ’1âˆ•ğœ†0
âˆ«
âˆ
x0
eâˆ’nC0xâˆ’ğœ†0 xâˆ’ğœ†dx
=
CC(1âˆ’ğœ†)âˆ•ğœ†0
0
ğœ†0
lim
nâ†’âˆn(ğœ†0âˆ’ğœ†)âˆ•ğœ†0
âˆ«
nC0x
âˆ’ğœ†0
0
0
y(ğœ†âˆ’1)âˆ•ğœ†0âˆ’1eâˆ’ydy.
The last expression is a positive constant if and only if ğœ†= ğœ†0 and is zero if and
only if ğœ†> ğœ†0. In fact, when ğœ†= ğœ†0 and C â‰ C0, the limit is CC(1âˆ’ğœ†0)âˆ•ğœ†0
0
ğœ†âˆ’1
0 Î“(1 âˆ’
1âˆ•ğœ†0); when ğœ†= ğœ†0 and C = C0, the limit is C1âˆ•ğœ†0
0
ğœ†âˆ’1
0 Î“(1 âˆ’1âˆ•ğœ†0). This leads to
n1âˆ’1âˆ•ğœ†0E(ğœ‹âˆ—
0(C, ğœ†)) â†’0.
Next, it is to show n2âˆ’2âˆ•ğœ†0Var(ğœ‹âˆ—
0(C, ğœ†)) â†’0. It can be veriï¬ed that
Var(ğœ‹âˆ—
0(C, ğœ†)) â‰¤
âˆ‘
kâ‰¥1
(1 âˆ’C0kâˆ’ğœ†0)nC2kâˆ’2ğœ†.
Let
fn(x) = n2âˆ’2âˆ•ğœ†0(1 âˆ’C0xâˆ’ğœ†0)2C2kâˆ’2ğœ†
be deï¬ned for x > x0 where x0 is suï¬ƒciently large a constant to ensure
that 1 âˆ’C0xâˆ’ğœ†0 â‰¥0. It can be veriï¬ed that fn(x) is decreasing for all
x â‰¥x(n) = [C0(nğœ†0 + 2ğœ†)âˆ•(2ğœ†)]1âˆ•ğœ†0 and is increasing for x âˆˆ[x0, x(n)]. For
any ğœ†satisfying ğœ†> ğœ†0 âˆ’1,
fn(x(n)) = C2Câˆ’2ğœ†âˆ•ğœ†0
0
(
1 âˆ’
2ğœ†
nğœ†0 + 2ğœ†
)n(
1 + n ğœ†0
2ğœ†
)âˆ’2ğœ†âˆ•ğœ†0
n2âˆ’2âˆ•ğœ†0
= ğ’ª
(
n
2
(ğœ†0âˆ’1)âˆ’ğœ†
ğœ†0
)
â†’0.

Estimation of Tail Probability
265
Invoking the Eulerâ€“Maclaurin lemma,
lim
nâ†’âˆn2âˆ’2âˆ•ğœ†0 âˆ‘
k>x0
(1 âˆ’C0kâˆ’ğœ†0)nC2kâˆ’2ğœ†
= C2 lim
nâ†’âˆn2âˆ’2âˆ•ğœ†0
âˆ«
âˆ
x0
(1 âˆ’C0xâˆ’ğœ†0)nxâˆ’2ğœ†dx
= C2 lim
nâ†’âˆn2âˆ’2âˆ•ğœ†0
âˆ«
âˆ
x0
eâˆ’nC0xâˆ’ğœ†0xâˆ’2ğœ†dx
=
C2C(1âˆ’2ğœ†)âˆ•ğœ†0
0
ğœ†0
lim
nâ†’âˆn[2(ğœ†0âˆ’ğœ†)âˆ’1]âˆ•ğœ†0
âˆ«
nC0x
âˆ’ğœ†0
0
0
y(2ğœ†âˆ’1)âˆ•ğœ†0âˆ’1eâˆ’ydy.
The last limit is zero if and only if ğœ†> ğœ†0 âˆ’1âˆ•2. Part (a) immediately follows.
For Part (b), ï¬rst consider the function
fn(x) = n1âˆ’1âˆ•ğœ†0(n + 1)(1 âˆ’C0xâˆ’ğœ†0)nC0xâˆ’ğœ†0Cxâˆ’ğœ†
deï¬ned for x > x0. It can be easily veriï¬ed that fn(x) is decreasing for all x â‰¥x(n)
and is increasing for x âˆˆ[x0, x(n)] where
x(n) = {C0[(n + 1)ğœ†0 + ğœ†]âˆ•(ğœ†0 + ğœ†)}1âˆ•ğœ†0.
For any ğœ†> ğœ†0 âˆ’1,
fn(x(n)) = CC0(n + 1)n
1âˆ’1
ğœ†0
[
1 âˆ’
ğœ†0 + ğœ†
(n + 1)ğœ†0 + ğœ†
]n
Ã—
{
ğœ†0 + ğœ†
C0[(n + 1)ğœ†0 + ğœ†]
}1+ ğœ†
ğœ†0
= ğ’ª
(
n
1âˆ’1+ğœ†
ğœ†0
)
â†’0.
Invoking the Eulerâ€“Maclaurin lemma, for ğœ†> ğœ†0,
lim
nâ†’âˆn1âˆ’1âˆ•ğœ†0(n + 1)
âˆ‘
k>x0
(1 âˆ’C0kâˆ’ğœ†0)nC0kâˆ’ğœ†0Ckâˆ’ğœ†
= CC0 lim
nâ†’âˆn1âˆ’1âˆ•ğœ†0(n + 1) âˆ«
âˆ
x0
(1 âˆ’C0xâˆ’ğœ†0)nxâˆ’(ğœ†0+ğœ†)dx
=
CC1âˆ’ğœ†âˆ•ğœ†0
0
ğœ†0
lim
nâ†’âˆn
âˆ’ğœ†
ğœ†0 (n + 1) âˆ«
nC0x
âˆ’ğœ†0
0
0
y(1+ğœ†âˆ•ğœ†0âˆ’1âˆ•ğœ†0)âˆ’1eâˆ’ydy
= ğ’ª
(
n
ğœ†0âˆ’ğœ†
ğœ†0
)
â†’0.
At this point, it has been shown that n1âˆ’1âˆ•ğœ†0E(ğœ‹âˆ—
1(C, ğœ†)) â†’0. Next, it is to
show
n2âˆ’2âˆ•ğœ†0Var(ğœ‹âˆ—
1(C, ğœ†)) â†’0.

266
Statistical Implications of Turingâ€™s Formula
Let
fn(x) = n2âˆ’2âˆ•ğœ†0C2C0(n + 1)(1 âˆ’C0xâˆ’ğœ†0)nxâˆ’(2ğœ†+ğœ†0)
deï¬ned for x > x0. It can be easily veriï¬ed that fn(x) is decreasing for all
x â‰¥x(n) and is increasing for x âˆˆ[x0, x(n)] where x(n) = {C0[(n + 1)ğœ†0 + 2ğœ†]âˆ•
(ğœ†0 + 2ğœ†)}1âˆ•ğœ†0. For any ğœ†> ğœ†0 âˆ’1,
fn(x(n)) = C2C0(n + 1)n
2âˆ’2
ğœ†0
[
1 âˆ’
ğœ†0 + 2ğœ†
(n + 1)ğœ†0 + 2ğœ†
]n
Ã—
{C0[(n + 1)ğœ†0 + 2ğœ†]
ğœ†0 + 2ğœ†
}âˆ’
2ğœ†+ğœ†0
ğœ†0
= ğ’ª(n2[1âˆ’(1+ğœ†)âˆ•ğœ†0]) â†’0.
Invoking the Eulerâ€“Maclaurin lemma, for ğœ†> ğœ†0 âˆ’1âˆ•2,
lim
nâ†’âˆn2âˆ’2âˆ•ğœ†0(n + 1)C2C0
âˆ‘
k>x0
(1 âˆ’C0kâˆ’ğœ†0)nkâˆ’(2ğœ†+ğœ†0)
= C2C0 lim
nâ†’âˆn2âˆ’2âˆ•ğœ†0(n + 1) âˆ«
âˆ
x0
(1 âˆ’C0xâˆ’ğœ†0)nxâˆ’(ğœ†0+2ğœ†)dx
=
C2C(1âˆ’2ğœ†)âˆ•ğœ†0
0
ğœ†0
lim
nâ†’âˆn
1âˆ’1+2ğœ†
ğœ†0 (n + 1) âˆ«
nC0x
âˆ’ğœ†0
0
0
y
2ğœ†+ğœ†0âˆ’1
ğœ†0
âˆ’1eâˆ’ydy
= ğ’ª
(
n
2âˆ’1+2ğœ†
ğœ†0
)
â†’0.
Part (b) follows.
â—½
Proof of Part 2: In the proof of Lemma 7.9 Part 1, at each of the four times fn(x)
was deï¬ned and evaluated at its corresponding x(n), lim fn(x) = 0 not only for
ğœ†> ğœ†0 but also ğœ†> ğœ†0 âˆ’1âˆ•(2ğœ†0). This implies that the Eulerâ€“Maclaurin lemma
applies in each of the four cases when (C, ğœ†) âˆˆÎ˜2. In particular, it is easily ver-
iï¬ed, ï¬rst
n1âˆ’1âˆ•ğœ†0âˆ’1âˆ•(2ğœ†0)E(ğœ‹âˆ—
0(C, ğœ†)) â†’0
and
n2âˆ’2âˆ•ğœ†0âˆ’2âˆ•(2ğœ†0)Var(ğœ‹âˆ—
0(C, ğœ†)) â†’0,
which implies
n1âˆ’1âˆ•ğœ†0âˆ’1âˆ•(2ğœ†0)ğœ‹âˆ—
0(C, ğœ†)
p
âˆ’âˆ’âˆ’â†’0,
second
n1âˆ’1âˆ•ğœ†0âˆ’1âˆ•(2ğœ†0)E(ğœ‹âˆ—
1(C, ğœ†)) â†’0
and
n2âˆ’2âˆ•ğœ†0âˆ’2âˆ•(2ğœ†0)Var(ğœ‹âˆ—
1(C, ğœ†)) â†’0,

Estimation of Tail Probability
267
which implies
n1âˆ’1âˆ•ğœ†0âˆ’1âˆ•(2ğœ†0)ğœ‹âˆ—
1(C, ğœ†)
p
âˆ’âˆ’âˆ’â†’0.
â—½
Proof of Part 3: First consider the case of (C, ğœ†) âˆˆÎ˜2. Since the Eulerâ€“Maclaurin
lemma applies in all four cases of fn, it is easily veriï¬ed that
n1âˆ’1âˆ•ğœ†0E(ğœ‹âˆ—
0(C, ğœ†)) â†’+âˆ
and
n2âˆ’2âˆ•ğœ†0Var(ğœ‹âˆ—
0(C, ğœ†)) â†’0,
which in turn implies that
n1âˆ’1âˆ•ğœ†0ğœ‹âˆ—
0(C, ğœ†)
p
âˆ’âˆ’âˆ’â†’+âˆ.
Similarly,
n1âˆ’1âˆ•ğœ†0ğœ‹âˆ—
1(C, ğœ†)
p
âˆ’âˆ’âˆ’â†’+âˆ.
Second consider the case of (C, ğœ†) âˆˆÎ˜3. For every (C, ğœ†) âˆˆÎ˜3, there exists a
point (C, ğœ†1) âˆˆÎ˜2, where ğœ†< ğœ†1, such that
ğœ‹âˆ—
0(C, ğœ†) =
âˆ‘
kâ‰¥1
1[xk = 0]Ckâˆ’ğœ†>
âˆ‘
kâ‰¥1
1[xk = 0]Ckâˆ’ğœ†1 = ğœ‹âˆ—
0(C, ğœ†1).
Since ğœ‹âˆ—
0(C, ğœ†1)
pâ†’+âˆ, ğœ‹âˆ—
0(C, ğœ†)
pâ†’+âˆ. Similarly ğœ‹âˆ—
1(C, ğœ†)
pâ†’+âˆ.
â—½
7.6
Exercises
1
Assuming that the probability distribution {pk; k â‰¥1} on an countably
inï¬nite alphabet ğ’³= {ğ“k; k â‰¥1} satisï¬es that pk > 0 for every k, k â‰¥1,
show that
lim
nâ†’âˆP(ğ•‚tail âŠ‚{k; k â‰¥k0}) = 1
where ğ•‚tail is as in (7.3).
2
Provide details of the proof for Lemma 7.3.
3
In Corollary 7.2, verify that Ì‚Î£âˆ’1âˆ•2
2
has the form as given in (7.8).
4
Suppose the Pareto (power decay) tail model of (7.1) is true. Let
ğ›¿= (4ğœ†0)âˆ’1, that is,
g(n, ğ›¿) = n1âˆ’1âˆ•(2ğœ†0)

268
Statistical Implications of Turingâ€™s Formula
where ğœ†0 is the true value of the parameter ğœ†. Let Î©n be deï¬ned by
g2(n, ğ›¿)Î©n = Ì‚Î£âˆ’1
2
where Ì‚Î£âˆ’1
2 is as in (7.8). Show that
Î©n
p
âˆ’âˆ’âˆ’â†’Î©
where Î© is some positive deï¬nite 2 Ã— 2 matrix.
5
Let ln(C, ğœ†) be as in (7.9), show that
a)
âˆ‚2ln
âˆ‚C2 = 2n2âˆ’2âˆ•ğœ†0(un, Un)Î©n(un, Un)ğœ,
b)
âˆ‚2ğ“n
âˆ‚ğœ†2 = âˆ’2n2âˆ’2âˆ•ğœ†0C(wn, Wn)Î©n(T1 âˆ’ğœ‹âˆ—
0, T2 âˆ’ğœ‹âˆ—
1)ğœ
+ 2n2âˆ’2âˆ•ğœ†0C2(vn, Vn)Î©n(vn, Vn)ğœ,
c)
âˆ‚2ğ“n
âˆ‚ğœ†âˆ‚C = 2n2âˆ’2âˆ•ğœ†0(vn, Vn)Î©n(T1 âˆ’ğœ‹âˆ—
0, T2 âˆ’ğœ‹âˆ—
1)ğœ
âˆ’2n2âˆ’2âˆ•ğœ†0C(vn, Vn)Î©n(un, Un)ğœ.
6
Give a proof of Part 2 of Lemma 7.7.
7
Give a proof of Part 5 of Lemma 7.7.
8
Give a proof of Part 6 of Lemma 7.7.
9
Give a proof of Part 7 of Lemma 7.7.
10
Show that
lim
kâ†’âˆ
[(
k
k âˆ’1
)ğœ†
k âˆ’k
]
= ğœ†
where ğœ†> 0 is a constant.

269
References
Antos, A. and Kontoyiannis, I. (2001). Convergence properties of functional
estimates for discrete distributions. Random Structures & Algorithms, 19,
163â€“193.
Beirlant, J., Dudewicz, E.J., GyÃ¶rï¬, L., and Meulen, E.C. (2001). Nonparametric
entropy estimation: an overview. International Journal of the Mathematical
Statistics Sciences, 6, 17â€“39.
Billingsley, P. (1995). Probability and Measure, John Wiley & Sons, Inc., New York.
Blyth, C.R. (1959). Note on estimating information. Annals of Mathematical
Statistics, 30, 71â€“79.
Bunge, J., Willis, A., and Walsh, F. (2014). Estimating the number of species in
microbial diversity studies. Annual Review of Statistics and its Application, 1,
427â€“445.
Chao, A. (1987). Estimating the population size for capture-recapture data with
unequal catchability. Biometrics, 43, 783â€“791.
Chao, A. and Jost, L. (2012). Coverage-based rarefaction and extrapolation:
standardizing samples by completeness rather than size. Ecology, 93,
2533â€“2547.
Chao, A. and Jost, L. (2015). Estimating diversity and entropy proï¬les via
discovery rates of new species. Methods in Ecology and Evolution, 6, 873â€“882.
Chao, A. and Shen, T.J. (2003). Nonparametric estimation of Shannonâ€™s index of
diversity when there are unseen species. Environmental and Ecological
Statistics, 10, 429â€“443.
Chao, A., Chiu, C.-H., and Jost, L. (2010). Phylogenetic diversity measures based
on Hill numbers. Philosophical Transactions of the Royal Society B: Biological
Sciences, 365, 3599â€“3609.
Chao, A., Lee, S.-M., and Chen, T.-C. (1988). A generalized Goodâ€™s nonparametric
coverage estimator. Chinese Journal of Mathematics, 16(3), 189â€“199.
Cochran, W.G. (1952). The ğœ’2 test of goodness of ï¬t. Annals of Mathematical
Statistics, 25, 315â€“345.
Cover, T.M. and Thomas, J.A. (2006). Elements of Information Theory, 2nd ed.
John Wiley & Sons, Inc., Hoboken, NJ.
Statistical Implications of Turingâ€™s Formula, First Edition. Zhiyi Zhang.
Â© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

270
References
David, F.N. (1950). Two combinatorial tests of whether a sample has come from a
given population. Biometrika, 37, 97â€“110.
de Haan, L. and Ferreira, A. (2006). Extreme Value Theory: An Introduction,
Springer Science+Business Media, LLC, New York.
Emlen, J.M. (1973). Ecology: An Evolutionary Approach, Addison-Wesley
Publishing Co., Reading, MA.
Esty, W. (1983). A normal limit law for a nonparametric estimator of the coverage
of a random sample. The Annals of Statistics, 11(3), 905â€“912.
Fisher, R.A. and Tippett, L.H.C. (1922). On the interpretation of chi-square from
contingency tables, and the calculation of P. Journal of the Royal Statistical
Society, 85, 87â€“94.
Fisher, R.A. and Tippett, L.H.C. (1928). Limiting forms of the
frequency-distribution of the largest or smallest member of a sample.
Mathematical Proceedings of the Cambridge Philosophical Society, 24, 180.
Fisher, R.A., Corbet, A.S., and Williams, C.B. (1943). The relationship between the
number of species and the number of individuals in a random sample of an
animal population. Journal of Animal Ecology, 12(1), 42â€“58.
FrÃ©chet, M. (1927). Sur la loi de probabilitÃ© de lâ€™Ã©cart maximum. Annales de la
SociÃ©tÃ© Polonaise de MathÃ©matique, 6, 92.
Gini, C. (1912). VariabilitÃ  e mutabilitÃ . Reprinted in Memorie di metodologica
statistica (eds E. Pizetti and T. Salvemini), Libreria Eredi Virgilio Veschi, Rome
(1955).
Gnedenko, B.V. (1943). Sur la distribution limite du terme maximum dâ€™une sÃ©rie
alÃ©atoire. Annals of Mathematics, 44, 423â€“453.
Gnedenko, B.V. (1948). On a local limit theorem of the theory of probability.
Uspekhi Matematicheskikh Nauk, 3(25), 187â€“194.
Good, I.J. (1953). The population frequencies of species and the estimation of
population parameters. Biometrika, 40(3-4), 237â€“264.
Grabchak, M., Marcon, E., Lang, G., and Zhang, Z. (2016). The generalized
Simpsonâ€™s entropy is a measure of biodiversity <hal-01276738>.
Haeusler, E. and Teugels, J.L. (1985). On asymptotic normality of Hillâ€™s estimator
for the exponent of regular variation. The Annals of Statistics, 13(2), 743â€“756.
Hall, P. and Weissman, I. (1997). On the estimation of extreme tail probabilities.
The Annals of Statistics, 25(3), 1311â€“1326.
Hall, P. and Welsh, A.H. (1985). Adaptive estimates of parameters of regular
variation. The Annals of Statistics, 13(1), 331â€“341.
Harris, B. (1975). The statistical estimation of entropy in the non-parametric case.
In Topics in Information Theory (ed. I. Csiszar), North-Holland, Amsterdam,
323â€“355.
Hausser, J. and Strimmer, K. (2009). Entropy inference and the James-Stein
estimator, with application to nonlinear gene association networks. Journal of
Machine Learning Research, 10, 1469â€“1484.
Heip, C.H.R., Herman, P.M.J., and Soetaert, K. (1998). Indices of diversity and
evenness. Oc eanis, 24(4), 61â€“87.

References
271
Hill, M.O. (1973). Diversity and evenness: a unifying notation and its
consequences. Ecology, 54, 427â€“431.
Hill, B.M. (1975). A simple general approach to inference about the tail of a
distribution. The Annals of Statistics, 3(5), 1163â€“1174.
Hoeï¬€ding, W. (1948). A class of statistics with asymptotically normal
distributions. The Annals of Statistics, 19(3), 293â€“325.
Hoeï¬€ding, W. (1963). Probability Inequalities for Sums of Bounded Random
Variables. Journal of the American Statistical Association, 58, 13â€“30.
Holste, D., GroÃŸe, I., and Herzel, H. (1998). Bayesâ€™ estimators of generalized
entropies. Journal of Physics A: Mathematical and General, 31, 2551â€“2566.
Johnson, N.L. and Kotz, S. (1977). URN Models and their Applications, John Wiley
& Sons, Inc., New York.
Jost, L. (2006). Entropy and diversity. Oikos, 113, 363â€“375.
Kolchin, V.F., Sevastyanov, B.A., and Chistyakov, V.P. (1978). Random Allocations,
V.H. Winston & Sons, Washington, DC.
Krebs, C.J. (1999). Ecological Methodology, 2nd ed. Addison-Welsey Educational
Publishers, Menlo Park, CA.
Krichevsky, R.E. and Troï¬mov, V.K. (1981). The performance of universal
encoding. IEEE Transactions on Information Theory, 27, 199â€“207.
Kullback, S. and Leibler, R.A. (1951). On information and suï¬ƒciency. The Annals
of Mathematical Statistics, 22(1), 79â€“86.
Kvalseth, T.O. (1987). Entropy and correlation: some comments. IEEE
Transactions on Systems, Man, and Cybernetics, 17(3), 517â€“519.
Lee, A.J. (1990). U-Statistics: Theory and Practice, Marcel Dekker, New York.
MacArthur, R.H. (1955). Fluctuations of animal populations, and a measure of
community stability. Ecology, 36, 533â€“536.
Magurran, A.E. (2004). Measuring Biological Diversity, Blackwell Publishing, Ltd.,
Malden, MA.
Mann, H.B. and Wald, A. (1943). On stochastic limit and order relationships. The
Annals of Mathematical Statistics, 14(3), 217â€“226.
Marcon, E. (2014). Mesures de la BiodiversitÃ©. Technical Report, Ecologie des
forÃªts de Guyane.
Margalef, R. (1958). Temporal succession and spatial heterogeneity in
phytoplankton. In Perspectives in Marine biology (ed. A.A. Buzzati-Traverso),
University of California Press, Berkeley, CA, 323â€“347.
Miller, G.A. (1955). Note on the bias of information estimates. Information Theory
in Psychology; Problems and Methods, II-B, 95â€“100.
Miller, G.A. and Madow, W.G. (1954). On the maximum likelihood estimate of
the Shannon-Weaver measure of information. Air Force Cambridge Research
Center Technical Report AFCRC-TR-54-75, Operational Applications
Laboratory, Air Force, Cambridge Research Center, Air Research and
Development Command, New York.

272
References
Montgomery-Smith, S. and SchÃ¼mann, T. (2007). Unbiased Estimators for Entropy
and Class Number. Unpublished preprint 2007, Department of Mathematics,
University of Missouri, Columbia, MO.
Mood, A.M., Graybill, F.A., and Boes, D.C. (1974). Introduction to the Theory of
Statistics, McGraw-Hill, Inc., New York.
Nemenman, I., Shafee, F., and Bialek, W. (2002). Entropy and inference, revisited.
Advances in Neural Information Processing Systems, vol. 14, MIT Press,
Cambridge, MA, 471â€“478.
Ohannessian, M.I. and Dahleh, M.A. (2012). Rare probability estimation under
regularly varying heavy tails. Journal of Machine Learning Research,
Proceedings Track, 23, 21.1â€“21.24.
Paninski, L. (2003). Estimation of entropy and mutual information. Neural
Computation, 15, 1191â€“1253.
Patil, G.P. and Taillie, C. (1982). Diversity as a concept and its measurement.
Journal of the American Statistical Association, 77, 548â€“567.
Pearson, K. (1900). On a criterion that a given system of deviations from the
probable in the case of a correlated system of variables is such that it can be
reasonably supposed to have arisen from random sampling. Philosophical
Magazine Series 5, 50, 157â€“175. (Reprinted 1948 in Karl Pearsonâ€™s Early
Statistical Papers, (ed. E.S. Pearson), Cambridge University Press, Cambridge.)
Pearson, K. (1922). On the ğœ’2 test of goodness of ï¬t. Biometrika, 14, 186â€“191.
Peet, R.K. (1974). The measurements of species diversity. Annual Review of
Ecology and Systematics, 5, 285â€“307.
Pickands, J. (1975). Statistical inference using extreme order statistics. The Annals
of Statistics, 3(1), 119â€“131.
Purvis, A. and Hector, A. (2000). Getting the measure of biodiversity. Nature, 405,
212â€“219.
RÃ©nyi, A. (1961). On measures of entropy and information. Proceedings of the 4th
Berkeley Symposium on Mathematical Statistics and Probabilities, Vol. 1,
547â€“561.
Robbins, H.E. (1968). Estimating the total probability of the unobserved outcomes
of an experiment. The Annals of Mathematical Statistics, 39(1), 256â€“257.
SchÃ¼rmann, T. and Grassberger, P. (1996). Entropy estimation of symbol
sequences. Chaos, 6, 414â€“427.
Serï¬‚ing, R.J. (1980). Approximation Theorems of Mathematical Statistics, John
Wiley & Sons, Inc., New York.
Shannon, C.E. (1948). A mathematical theory of communication. The Bell System
Technical Journal, 27, 379â€“423 & 623â€“656.
Simpson, E.H. (1949). Measurement of diversity. Nature, 163, 688.
Smith, R.L. (1987). Estimating tails of probability distributions. The Annals of
Statistics, 15(3), 1174â€“1207.
Stanley, R.P. (1997). Enumerative Combinatorics, Vol. 1, Cambridge University
Press, New York.

References
273
Strehl, A. and Ghosh, J. (2002). Cluster ensembles - a knowledge reuse framework
for combining multiple partitions. Journal of Machine Learning Research, 3,
583â€“617.
Strong, S.P., Koberle, R., de Ruyter van Steveninck, R.R., and Bialek, W. (1998).
Entropy and information in neural spike trains. Physical Review Letters, 80,
197â€“200.
Taillie, C. (1979). Species equitability: a comparative approach. In â€”it Ecological
Diversity in Theory and Practice (eds J.F. Grassle, G.P. Patil, W. Smith and C.
Taillie), International Cooperative Publishing House, Fairland, MD, 51â€“61.
Tanabe, K. and Sagae, M. (1992). An exact Cholesky decomposition and the
generalized inverse of the variance-covariance matrix of the multinomial
distribution, with applications. Journal of the Royal Statistical Society, Series B
Methodological, 54(1), 211â€“219.
Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of
Statistical Physics, 52, 479â€“487.
Valiant, G. and Valiant, P. (2011). Estimating the Unseen: an nâˆ•log (n)-sample
estimator for entropy and support size, shown optimal via new CLTs. In
Proceedings of the 43rd Annual ACM Symposium on Theory of Computing,
STOCâ€™11 Symposium on Theory of Computing (Co-located with FCRC 2011),
San Jose, CA, USA - June 06-08, 2011 (ed. L. Fortnow and S.P. Vadhan),
Association for Computing Machinery, New York, 685â€“694.
Vinh, N.X., Epps, J., and Bailey, J. (2010). Information theoretic measures for
clusterings comparison: variants, properties, normalization and correction for
chance. Journal of Machine Learning Research, 11, 2837â€“2854.
von Mises, R. (1936). La distribution de la plus grande de n valeurs. Reprinted
(1954) in Selected Papers 11271-294, American Mathematical Society,
Providence, RI.
Vu, V.Q., Yu, B., and Kass, R.E. (2007). Coverage-adjusted entropy estimation.
Statistical Analysis of Neuronal Data, 26(21), 4039â€“4060.
Wang, S.C. and Dodson, P. (2006). Estimating the diversity of dinosaurs.
Proceedings of the National Academy of Sciences of the United States of America,
103(37), 13601â€“13605.
Yao, Y.Y. (2003). Information-theoretic measures for knowledge discovery and
data mining. In: Entropy Measures, Maximum Entropy Principle and Emerging
Applications (ed. Karmeshu), 1st edition, Springer, Berlin, 115â€“136.
Zahl, S. (1977). Jackkniï¬ng an index of diversity. Ecology, 58, 907â€“913.
Zhang, Z. (2012). Entropy estimation in Turingâ€™s perspective. Neural
Computation, 24(5), 1368â€“1389.
Zhang, Z. (2013a). A multivariate normal law for Turingâ€™s formulae. SankhyÂ¯a: The
Indian Journal of Statistics, 75-A(1), 51â€“73.
Zhang, Z. (2013b). Asymptotic normality of an entropy estimator with
exponentially decaying bias. IEEE Transactions on Information Theory, 59(1),
504â€“508.

274
References
Zhang, Z. (2017). Domains of attraction on countable alphabets. Bernoulli Journal
(to appear).
Zhang, Z. and Grabchak, M. (2013). Bias adjustment for a nonparametric entropy
estimator. Entropy, 15(6), 1999â€“2011.
Zhang, Z. and Grabchak, M. (2014). Nonparametric estimation of
Kullback-Leibler divergence. Neural Computation, 26(11), 2570â€“2593.
Zhang, Z. and Grabchak, M. (2016). Entropic representation and estimation of
diversity indices. Journal of Nonparametric Statistics, DOI:
10.1080/10485252.2016.1190357.
Zhang, Z. and Huang, H. (2008). A suï¬ƒcient normality condition for Turingâ€™s
formula. Journal of Nonparametric Statistics, 20(5), 431â€“446.
Zhang, Z. and Stewart, A.M. (2016). Estimation of standardized mutual
information. Technical Report No. 7, University of North Carolina at Charlotte.
Zhang, C.-H. and Zhang, Z. (2009). Asymptotic normality of a nonparametric
estimator of sample coverage. The Annals of Statistics, 37(5A), 2582â€“2595.
Zhang, Z. and Zhang, X. (2012). A normal law for the plug-in estimator of
entropy. IEEE Transactions on Information Theory, 58(5), 2745â€“2747.
Zhang, Z. and Zheng, L. (2015). A mutual information estimator with
exponentially decaying bias. Statistical Applications in Genetics and Molecular
Biology, 14(3), 243â€“252.
Zhang, Z. and Zhou, J. (2010). Re-parameterization of multinomial distribution
and diversity indices. Journal of Statistical Planning and Inference, 140(7),
1731â€“1738.
Zubkov, A.M. (1973). Limit distributions for a statistical estimate of the entropy.
Teoriya Veroyatnostei i Ee Primeneniya, 18(3), 643â€“650.

275
Author Index
a
Antos, A.
83, 96
b
Bailey, J.
174
Beirlant, J.
71
Bialek, W.
74, 75
Billingsley, P.
00
Blyth, C.R.
87, 88
Boes, D.C.
198
Bunge, J.
73
c
Chao, A.
27, 73, 75, 132, 144
Chen, T.-C.
27
Chistyakov, V.P.
33
Chiu, C.-H.
144
Cochran, W.G.
199
Corbet, A.S.
142
Cover, T.M.
162
d
Dahleh, M.A.
21
David, F.N.
42
de Haan, L.
209
de Ruyter van Steveninck, R.R.
74
Dodson, P.
61
Dudewicz, E.J.
71
e
Emlen, J.M.
125
Epps, J.
174
Esty, W.
10
f
Ferreira, A.
209
Fisher, R.A.
142, 198, 209
FrÃ©chet, M.
209
g
Ghosh, J.
174
Gini, C.
49, 125
Gnedenko, B.V.
209
Good, I.J.
3, 10, 22
Grabchak, M.
75, 127, 132, 136, 144,
183
Grassberger, P.
75
Graybill, F.A.
198
GroÃŸe, I.
75
GyÃ¶rï¬, L.
71
h
Haeusler, E.
241
Hall, P.
241
Harris, B.
72, 77
Hausser, J.
75
Hector, A.
130, 142
Heip, C.H.R.
130
Herman, P.M.J.
130
Herzel, H.
75
Hill, B.M.
27, 241
Hill, M.O.
125
Hoeï¬€ding, W.
54, 92, 136
Holste, D.
75
Huang, H.
11, 12
Statistical Implications of Turingâ€™s Formula, First Edition. Zhiyi Zhang.
Â© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

276
Author Index
j
Johnson, N.L.
33
Jost, L.
132, 144
k
Kass, R.E.
75
Koberle, R.
74
Kolchin, V.F.
33
Kontoyiannis, I.
83, 96
Kotz, S.
33
Krebs, C.J.
125
Krichevsky, R.E.
75
Kullback, S.
153, 183
Kvalseth, T.O.
173
l
Lang, G.
144
Lee, A.J.
54, 92
Lee, S.-M.
27
Leibler, R.A.
153, 183
m
MacArthur, R. H.
142
Madow, W.G.
77
Magurran, A.E.
125
Mann, H. B.
135
Marcon, E.
125, 144
Margalef, R.
142
Meulen, E.C.
71
Miller, G.A.
73, 77, 187
Montgomery-Smith, S.
75
Mood, A.M.
198
n
Nemenman, I.
75
o
Ohannessian, M.I.
21
p
Paninski, L.
71, 80, 81, 187
Patil, G.P.
63
Pearson, K.
198
Peet, R.K.
130
Pickands, J.
241
Pratt, J.W.
00
Purvis, A.
130, 142
r
RÃ©nyi, A.
66, 125, 126
Robbins, H.E.
4
s
Sagae, M.
134
SchÃ¼rmann, T.
75
Serï¬‚ing, R.J.
54
Sevastyanov, B.A.
33
Shafee, F.
75
Shannon, C.E.
66, 71, 125
Shen, T.J.
75
Simpson, E.H.
49, 66, 125
Smirnov, N.V.
00
Smith, R.L.
241
Soetaert, K.
130
Stanley, R.P.
00
Stewart, A.M.
174
Strehl, A.
174
Strimmer, K.
75
Strong, S.P.
74
t
Taillie, C.
63
Tanabe, K.
134
Teugels, J.L.
241
Thomas, J.A.
162
Tippett, L.H.C.
198, 209
Troï¬mov, V.K.
75
Tsallis, C.
66, 125
v
Valiant, G.
75
Valiant, P.
75
Vinh, N.X.
174
von Mises, R.
209
Vu, V.Q.
75
w
Wald, A.
135

Author Index
277
Walsh, F.
73
Wang, S.C.
61
Wang, Y.T.
00
Weissman, I.
241
Welsh, A.H.
241
Williams, C.B.
142
Willis, A.
73
x
Yao, Y.Y.
174
Yu, B.
75
z
Zahl, S.
74
Zhang, C.-H.
00
Zhang, X.
16, 84, 101, 107, 118
Zhang, Z.
11, 12, 14, 16, 22â€“24, 49,
75, 84, 89, 92, 95, 101, 107, 118,
127, 132, 136, 144, 174, 182,
183, 228
Zheng, L.
182
Zhou, J.
49, 92, 101, 228
Zubkov, A.M.
80, 81

279
Subject Index
a
alphabet
3
cardinality
5
countable
3
ï¬nite
5, 10
AMLE
26, 246, 254
consistency
248, 254
deï¬nition
246, 254
Hessian matrix
248, 251
statistical properties
248
uniqueness
248, 251
asymptotically eï¬ƒcient
72, 76, 94
asymptotic maximum likelihood
estimator, AMLE
26, 246, 254
axioms of diversity
126, 143, 144
b
Bayesian
73, 74
hierarchical
75
hyper-prior
75
perspective
74
posterior distribution
75
Dirichlet
75
prior distribution
74, 75
Dirichlet
74
bias
4, 5
asymptotically relatively unbiased
estimator
5, 6, 28
relative bias
5
unbiased estimator
4
c
cardinality
5, 10
eï¬€ective cardinality
5
conï¬dence interval
9, 15, 16, 20, 32
conï¬dence region
243, 253
consistency
24, 31, 57, 169, 175, 183,
243, 248, 254
multiplicative
21
coverage
2
non-coverage
2
of population
2
sample coverage
2
d
diversity
49, 51, 61â€“64, 66, 126
axioms of
126, 128, 143
diï¬€erence proï¬le
64â€“66
Emlenâ€™s index
125, 127, 130, 134
entropic basis
128, 130, 138
equivalence
127, 129
estimators
125, 129, 131â€“133, 135,
138, 139, 145
normal laws
134, 135, 138, 139
Gini-Simpson index, the
49, 62, 63,
66â€“69, 125, 126, 129, 143, 146,
148
Hillâ€™s diversity number
125, 127,
128, 138, 147
linear
128â€“132, 138, 139
proï¬le
63â€“66, 142
Statistical Implications of Turingâ€™s Formula, First Edition. Zhiyi Zhang.
Â© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.

280
Subject Index
diversity (contd.)
RÃ©nyiâ€™s entropy
125, 127, 128,
138â€“142, 145â€“148
richness index
127, 130, 144, 147
Shannonâ€™s entropy
71, 86, 87, 125,
127, 128, 129, 133, 145, 148
Simpsonâ€™s index
49, 58, 63, 125,
126, 129, 146
Tsallisâ€™ entropy
125, 127, 128, 148
uniï¬ed perspective
125, 126
domains of attraction
13, 209, 210,
211, 212, 228
deï¬nition
211
Domain
0, 211, 212
Domain
1, 211, 212, 217
Domain
2, 211, 212
Domain T, transient
211, 212
dominance
218, 220, 223, 224, 239
FrÃ©chet
209
Gini-Simpson
212, 214
Gumbel
209, 217
Molchanov
212, 217, 218, 220
Turing-Good
212, 214, 215, 220,
221
Weibull
209
e
entropic basis
128, 130, 138
entropy
71
conditional entropy
150, 151
of conditional probability
distribution
150
expected conditional entropy
150
estimand
1, 4, 5, 44
Euler-Maclaurin lemma, the
13
extreme value
209, 210, 217
on alphabets
210
f
Fubiniâ€™s theorem
86
g
generalized Simpsonâ€™s index, the
49,
50, 51
Gini-Simpson family, the
212, 214
Goodness-of-ï¬t
33, 34, 36, 40, 42,
197, 198, 206, 207
Good-Turing formula, the
3
h
Hillâ€™s diversity number
125, 127, 128,
138, 147
i
information
149, 150
estimation
149
relative entropy
151, 182
j
Jensenâ€™s inequality
152
k
Kullback-Leibler divergence
151
deï¬nition
151
estimation
182
symmetrized
153, 184, 193
m
Molchanov family, the
212, 217, 218,
220
mutual information
153, 154, 157
deï¬nition
153
estimation
162
Kullback-Leibler divergence
182
standardized
159, 173
n
normal laws
15
of augmented Turingâ€™s formula
28,
32
of diversity estimators
134, 135,
138, 139
of estimators of entropy
77, 78, 80,
81, 84, 89, 91, 92, 99, 101
of estimators of generalized
Simpsonâ€™s index
55, 57, 58,
61
of Turingâ€™s formula
10, 13, 22â€“25

Subject Index
281
o
occupancy problems
33
oscillating tail indices
218
p
Pareto tail, the
26, 241, 243, 244, 254
power tail
26, 241
r
Relative bias
5
asymptotically and relatively
unbiased
28
asymptotically relatively unbiased
estimator
5
asymptotically unbiased
estimator
5
Robbinsâ€™ claim
4
RÃ©nyiâ€™s entropy
125, 127, 128,
138â€“142, 145â€“148
RÃ©nyiâ€™s equivalent
127, 128, 138, 139
s
sample
identically and independently
distributed (iid)
3
of size n
3
Shannonâ€™s entropy
71, 86
convergence
81
estimators
Bayesian
73, 74
Dirichlet posterior
75
Dirichlet prior
74
hierarchical
75
hyper-parameter,
75
hyper-prior
75
prior distribution
74, 75
Jackknife
73, 74
Miller-Madow
73
normal laws
77, 78, 80, 81, 84,
89, 91, 92, 99, 101
plug-in
72, 76
in Turingâ€™s perspective
86
the slow convergence theorem
83
Simpsonâ€™s indices
49
estimators
52â€“54
normal laws
54, 55, 57, 58
examples
dinosaur diversity
61â€“63
Philip Sidney
64
William Shakespeare
64
generalized, the
49â€“51, 128, 130,
144, 147
singletons
6
Stirling number of the second kind, the
36
t
tail
67, 68, 241â€“246, 248, 249,
251â€“256
distant
201
geometric progression
218
index
211, 212
of index set
243
no
210
on alphabets
210, 243
power
243,
thicker and thinner
249, 252, 253,
263, 265, 267, 268
tail probability
241
continuous exponential
255
continuous Gaussian
256
continuous Pareto
254
parametric tail model
242
Pareto tail, the
26, 241, 243, 244,
254
power tail
26, 241
Turing-Good family, the
212, 214,
215, 220
Turingâ€™s formula
1, 3, 6, 10, 22, 27, 61,
110, 145, 287, 289
asymptotic normality
10â€“12,
16â€“19, 21, 23â€“25, 32
augmented
27
deï¬nition
3
Good-Turing formula, the
3
multivariate
24, 25
univariate
10â€“12, 16â€“19, 21, 23,
32

282
Subject Index
Turingâ€™s perspective, xiii
50, 52, 68,
72, 86, 87, 88, 96, 131 135, 162,
170, 174, 176, 181, 183, 189,
243, 254
additive
19
multiplicative
19
u
uniform distribution
34, 35, 37,
39â€“42, 47
U-statistics
52, 54, 66, 91, 92, 132, 190
degree of kernel
92
kernel
54, 92
v
weighted
linear form of diversity
129
linear form of entropy
86
Simpsonâ€™s index
50
window on tail
243


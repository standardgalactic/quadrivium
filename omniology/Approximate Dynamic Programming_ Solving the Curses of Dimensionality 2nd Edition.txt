

Approximate Dynamic
Programming


Approximate Dynamic
Programming
Solving the Curses of Dimensionality
Second Edition
Warren B. Powell
Princeton University
The Department of Operations Research and Financial Engineering
Princeton, NJ
A JOHN WILEY & SONS, INC., PUBLICATION

Copyright 2011 by John Wiley & Sons, Inc. All rights reserved
Published by John Wiley & Sons, Inc., Hoboken, New Jersey
Published simultaneously in Canada
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form
or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as
permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior
written permission of the Publisher, or authorization through payment of the appropriate per-copy fee
to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400,
fax (978) 750-4470, or on the web at www.copyright.com. Requests to the Publisher for permission
should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street,
Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008, or online at http://www.wiley.com/go/
permission.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts
in preparing this book, they make no representations or warranties with respect to the accuracy or
completeness of the contents of this book and speciﬁcally disclaim any implied warranties of
merchantability or ﬁtness for a particular purpose. No warranty may be created or extended by sales
representatives or written sales materials. The advice and strategies contained herein may not be
suitable for your situation. You should consult with a professional where appropriate. Neither the
publisher nor author shall be liable for any loss of proﬁt or any other commercial damages, including
but not limited to special, incidental, consequential, or other damages.
For general information on our other products and services or for technical support, please contact our
Customer Care Department within the United States at (800) 762-2974, outside the United States at
(317) 572-3993 or fax (317) 572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print
may not be available in electronic formats. For more information about Wiley products, visit our web
site at www.wiley.com.
Library of Congress Cataloging-in-Publication Data:
Powell, Warren B., 1955–
Approximate dynamic programming : solving the curses of dimensionality / Warren B. Powell.
– 2nd ed.
p. cm.
Includes bibliographical references and index.
ISBN 978-0-470-60445-8 (cloth)
1. Dynamic programming. I. Title.
T57.83.P76 2011
519.7′03–dc22
2010047227
Printed in the United States of America
oBook ISBN: 978-1-118-02917-6
ePDF ISBN: 978-1-118-02915-2
ePub ISBN: 978-1-118-02916-9
10
9
8
7
6
5
4
3
2
1

Contents
Preface to the Second Edition
xi
Preface to the First Edition
xv
Acknowledgments
xvii
1
The Challenges of Dynamic Programming
1
1.1
A Dynamic Programming Example: A Shortest Path
Problem, 2
1.2
The Three Curses of Dimensionality, 3
1.3
Some Real Applications, 6
1.4
Problem Classes, 11
1.5
The Many Dialects of Dynamic Programming, 15
1.6
What Is New in This Book?, 17
1.7
Pedagogy, 19
1.8
Bibliographic Notes, 22
2
Some Illustrative Models
25
2.1
Deterministic Problems, 26
2.2
Stochastic Problems, 31
2.3
Information Acquisition Problems, 47
2.4
A Simple Modeling Framework for Dynamic Programs, 50
2.5
Bibliographic Notes, 54
Problems, 54
3
Introduction to Markov Decision Processes
57
3.1
The Optimality Equations, 58
3.2
Finite Horizon Problems, 65
v

vi
contents
3.3
Inﬁnite Horizon Problems, 66
3.4
Value Iteration, 68
3.5
Policy Iteration, 74
3.6
Hybrid Value-Policy Iteration, 75
3.7
Average Reward Dynamic Programming, 76
3.8
The Linear Programming Method for Dynamic
Programs, 77
3.9
Monotone Policies*, 78
3.10
Why Does It Work?**, 84
3.11
Bibliographic Notes, 103
Problems, 103
4
Introduction to Approximate Dynamic Programming
111
4.1
The Three Curses of Dimensionality (Revisited), 112
4.2
The Basic Idea, 114
4.3
Q-Learning and SARSA, 122
4.4
Real-Time Dynamic Programming, 126
4.5
Approximate Value Iteration, 127
4.6
The Post-Decision State Variable, 129
4.7
Low-Dimensional Representations of Value Functions, 144
4.8
So Just What Is Approximate Dynamic Programming?, 146
4.9
Experimental Issues, 149
4.10
But Does It Work?, 155
4.11
Bibliographic Notes, 156
Problems, 158
5
Modeling Dynamic Programs
167
5.1
Notational Style, 169
5.2
Modeling Time, 170
5.3
Modeling Resources, 174
5.4
The States of Our System, 178
5.5
Modeling Decisions, 187
5.6
The Exogenous Information Process, 189
5.7
The Transition Function, 198
5.8
The Objective Function, 206
5.9
A Measure-Theoretic View of Information**, 211
5.10
Bibliographic Notes, 213
Problems, 214

contents
vii
6
Policies
221
6.1
Myopic Policies, 224
6.2
Lookahead Policies, 224
6.3
Policy Function Approximations, 232
6.4
Value Function Approximations, 235
6.5
Hybrid Strategies, 239
6.6
Randomized Policies, 242
6.7
How to Choose a Policy?, 244
6.8
Bibliographic Notes, 247
Problems, 247
7
Policy Search
249
7.1
Background, 250
7.2
Gradient Search, 253
7.3
Direct Policy Search for Finite Alternatives, 256
7.4
The Knowledge Gradient Algorithm for Discrete
Alternatives, 262
7.5
Simulation Optimization, 270
7.6
Why Does It Work?**, 274
7.7
Bibliographic Notes, 285
Problems, 286
8
Approximating Value Functions
289
8.1
Lookup Tables and Aggregation, 290
8.2
Parametric Models, 304
8.3
Regression Variations, 314
8.4
Nonparametric Models, 316
8.5
Approximations and the Curse of Dimensionality, 325
8.6
Why Does It Work?**, 328
8.7
Bibliographic Notes, 333
Problems, 334
9
Learning Value Function Approximations
337
9.1
Sampling the Value of a Policy, 337
9.2
Stochastic Approximation Methods, 347
9.3
Recursive Least Squares for Linear Models, 349
9.4
Temporal Difference Learning with a Linear Model, 356
9.5
Bellman’s Equation Using a Linear Model, 358

viii
contents
9.6
Analysis of TD(0), LSTD, and LSPE Using a Single
State, 364
9.7
Gradient-Based Methods for Approximate Value
Iteration*, 366
9.8
Least Squares Temporal Differencing with Kernel
Regression*, 371
9.9
Value Function Approximations Based on Bayesian
Learning*, 373
9.10
Why Does It Work*, 376
9.11
Bibliographic Notes, 379
Problems, 381
10
Optimizing While Learning
383
10.1
Overview of Algorithmic Strategies, 385
10.2
Approximate Value Iteration and Q-Learning Using
Lookup Tables, 386
10.3
Statistical Bias in the Max Operator, 397
10.4
Approximate Value Iteration and Q-Learning Using
Linear Models, 400
10.5
Approximate Policy Iteration, 402
10.6
The Actor–Critic Paradigm, 408
10.7
Policy Gradient Methods, 410
10.8
The Linear Programming Method Using Basis Functions, 411
10.9
Approximate Policy Iteration Using Kernel Regression*, 413
10.10
Finite Horizon Approximations for Steady-State
Applications, 415
10.11
Bibliographic Notes, 416
Problems, 418
11
Adaptive Estimation and Stepsizes
419
11.1
Learning Algorithms and Stepsizes, 420
11.2
Deterministic Stepsize Recipes, 425
11.3
Stochastic Stepsizes, 433
11.4
Optimal Stepsizes for Nonstationary Time Series, 437
11.5
Optimal Stepsizes for Approximate Value Iteration, 447
11.6
Convergence, 449
11.7
Guidelines for Choosing Stepsize Formulas, 451
11.8
Bibliographic Notes, 452
Problems, 453

contents
ix
12
Exploration Versus Exploitation
457
12.1
A Learning Exercise: The Nomadic Trucker, 457
12.2
An Introduction to Learning, 460
12.3
Heuristic Learning Policies, 464
12.4
Gittins Indexes for Online Learning, 470
12.5
The Knowledge Gradient Policy, 477
12.6
Learning with a Physical State, 482
12.7
Bibliographic Notes, 492
Problems, 493
13
Value Function Approximations for Resource Allocation
Problems
497
13.1
Value Functions versus Gradients, 498
13.2
Linear Approximations, 499
13.3
Piecewise-Linear Approximations, 501
13.4
Solving a Resource Allocation Problem Using
Piecewise-Linear Functions, 505
13.5
The SHAPE Algorithm, 509
13.6
Regression Methods, 513
13.7
Cutting Planes*, 516
13.8
Why Does It Work?**, 528
13.9
Bibliographic Notes, 535
Problems, 536
14
Dynamic Resource Allocation Problems
541
14.1
An Asset Acquisition Problem, 541
14.2
The Blood Management Problem, 547
14.3
A Portfolio Optimization Problem, 557
14.4
A General Resource Allocation Problem, 560
14.5
A Fleet Management Problem, 573
14.6
A Driver Management Problem, 580
14.7
Bibliographic Notes, 585
Problems, 586
15
Implementation Challenges
593
15.1
Will ADP Work for Your Problem?, 593
15.2
Designing an ADP Algorithm for Complex Problems, 594
15.3
Debugging an ADP Algorithm, 596

x
contents
15.4
Practical Issues, 597
15.5
Modeling Your Problem, 602
15.6
Online versus Ofﬂine Models, 604
15.7
If It Works, Patent It!, 606
Bibliography
607
Index
623

Preface to the Second Edition
The writing for the ﬁrst edition of this book ended around 2005, followed by a
year of editing before it was submitted to the publisher in 2006. As with everyone
who works in this very rich ﬁeld, my understanding of the models and algorithms
was strongly shaped by the projects I had worked on. While I was very proud of
the large industrial applications that were the basis of my success, at the time I had
a very limited understanding of many other important problem classes that help to
shape the algorithms that have evolved (and continue to evolve) in this ﬁeld.
In the ﬁve years that passed before this second edition went to the publisher, my
understanding of the ﬁeld and my breadth of applications have grown dramatically.
Reﬂecting my own personal growth, I realized that the book needed a fundamen-
tal restructuring along several dimensions. I came to appreciate that approximate
dynamic programming is much more than approximating value functions. After
writing an article that included a list of nine types of policies, I realized that every
policy I had encountered could be broken down into four fundamental classes:
myopic policies, lookahead policies, policy function approximations, and policies
based on value function approximations. Many other policies can be created by
combining these four fundamental classes into different types of hybrids.
I also realized that methods for approximating functions (whether they be pol-
icy function approximations or value function approximations) could be usefully
organized into three fundamental strategies: lookup tables, parametric models, and
nonparametric models. Of course, these can also be combined in different forms.
In preparing the second edition, I came to realize that the nature of the decision
variable plays a critical role in the design of an algorithm. In the ﬁrst edition, one
of my goals was to create a bridge between dynamic programming (which tended
to focus on small action spaces) and math programming, with its appreciation of
vector-valued decisions. As a result I had adopted x as my generic decision vari-
able. In preparing the new edition, I had come to realize that small action spaces
cover a very important class of problems, and these are also the problems that a
beginner is most likely to start with to learn the ﬁeld. Also action “a” pervades the
reinforcement learning community (along with portions of the operations research
community), to the point that it is truly part of the language. As a result the second
edition now uses action “a” for most of its presentation, but reverts to x speciﬁcally
xi

xii
preface to the second edition
for problems where the decisions are continuous and/or (more frequently) vectors.
The challenges of vector-valued decisions has been largely overlooked in the rein-
forcement learning community, while the operations research community that works
on these problems has largely ignored the power of dynamic programming.
The second edition now includes a new chapter (Chapter 6) devoted purely to
a discussion of different types of policies, a summary of some hybrid strategies,
and a discussion of problems that are well suited to each of the different strategies.
This is followed by a chapter (Chapter 7) that focuses purely on the issue of policy
search. This chapter brings together ﬁelds such as stochastic search and simulation
optimization. The chapter also introduces a new class of optimal learning strate-
gies based on the concept of the knowledge gradient, an idea that was developed
originally to address the exploration–exploitation problem before realizing that it
had many other applications.
I also acquired a much better understanding of the different methods for approx-
imating value functions. I found that the best way to communicate the rich set of
strategies that have evolved was to divide the material into three chapters. The ﬁrst
of these (Chapter 8) focuses purely on different statistical procedures for approxi-
mating value functions. While this can be viewed partly as a tutorial into statistics
and machine learning, the focus is on strategies that have been used in the approxi-
mate dynamic programming/reinforcement learning literature. ADP imposes special
demands on statistical learning algorithms, including the importance of recursive
estimation, and the need to start with a small number of observations (which works
better with a low-dimensional model) and transition to a larger number of obser-
vations with models that are high-dimensional in certain regions. Next, Chapter 9
summarizes different methods for estimating the value of being in a state using
sample information, with the goal of estimating the value function for a ﬁxed pol-
icy. Since I have found that a number of papers focus on a single policy without
making this apparent, this chapter makes this very explicit by indexing variables
that depend on a policy with a superscript π. Finally, Chapter 10 addresses the very
difﬁcult problem of estimating the value of being in a state while simultaneously
optimizing over policies.
Chapter 11 of this book is a reﬁned version of the old Chapter 6, which
focused on stepsize rules. Chapter 11 is streamlined, with a new discussion of
the implications of algorithms based on policy iteration (including least squares
policy evaluation (LSPE), least squares temporal differences) and algorithms based
on approximate value iteration and Q-learning. Following some recent research,
I use the setting of a single state to develop a much clearer understanding of the
demands on a stepsize that are placed by these different algorithmic strategy. A
new section has been added introducing a stepsize rule that is speciﬁcally optimized
for approximate value iteration.
Chapter 12, on the famous exploration–exploitation problem in approximate
dynamic programming, has been heavily revised to reﬂect a much more thorough
understanding of the general ﬁeld that is coming to be known as optimal learning.
This chapter includes a recently developed method for doing active learning in the
presence of a physical state, by way of the concept of the knowledge gradient.

preface to the second edition
xiii
While this method looks promising, the general area of doing active learning in the
context of dynamic programs (with a physical state) is an active area of research
at the time of this writing.
A major theme of the ﬁrst edition was to bridge the gap between disciplines,
primarily reinforcement learning (computer science), simulation, and math pro-
gramming (operations research). This edition reinforces this theme ﬁrst by adopting
more broadly the notation and vocabulary of reinforcement learning (which has
made most of the contributions to this ﬁeld) while retaining the bridge to math
programming, but now also including stochastic search and simulation optimization
(primarily in the context of policy search).
The mathematical level of the book continues to require only an understanding
of statistics and probability. A goal of the ﬁrst edition was that the material would
be accessible to an advanced undergraduate audience. With this second edition
a more accurate description would be that the material is accessible to a highly
motivated and well prepared undergraduate, but the breadth of the material is more
suitable to a graduate audience.
Warren B. Powell
Princeton, New Jersey
October 2010


Preface to the First Edition
The path to completing this book began in the early 1980s when I ﬁrst started
working on dynamic models arising in the management of ﬂeets of vehicles for
the truckload motor carrier industry. It is often said that necessity is the mother
of invention, and as with many of my colleagues in this ﬁeld, the methods that
emerged evolved out of a need to solve a problem. The initially ad hoc models and
algorithms I developed to solve these complex industrial problems evolved into
a sophisticated set of tools supported by an elegant theory within a ﬁeld that is
increasingly being referred to as approximate dynamic programming.
The methods in this book reﬂect the original motivating applications. I started
with elegant models for which academie is so famous, but my work with industry
revealed the need to handle a number of complicating factors that were beyond the
scope of these models. One of these was a desire from one company to understand
the effect of uncertainty on operations, requiring the ability to solve these large-
scale optimization problems in the presence of various forms of randomness (but
most notably customer demands). This question launched what became a multiple-
decade search for a modeling and algorithmic strategy that would provide practical,
but high-quality, solutions.
This process of discovery took me through multiple ﬁelds, including linear and
nonlinear programming, Markov decision processes, optimal control, and stochas-
tic programming. It is somewhat ironic that the framework of Markov decision
processes, which originally appeared to be limited to toy problems (three trucks
moving between ﬁve cities), turned out to provide the critical theoretical frame-
work for solving truly industrial-strength problems (thousands of drivers moving
between hundreds of locations, each described by complex vectors of attributes).
The ability to solve these problems required the integration of four major dis-
ciplines: dynamic programming (Markov decision processes), math programming
(linear, nonlinear and integer programming), simulation, and statistics. My desire
to bring together the ﬁelds of dynamic programming and math programming moti-
vated some fundamental notational choices (in particular, the use of x as a decision
variable). In this book there is as a result a heavy dependence on the Monte Carlo
methods so widely used in simulation, but a knowledgeable reader will quickly
see how much is missing. The book covers in some depth a number of important
xv

xvi
preface to the ﬁrst edition
techniques from statistics, but even this presentation only scratches the surface
of tools and concepts available from with ﬁelds such as nonparametric statistics,
signal processing and approximation theory.
Audience
The book is aimed primarily at an advanced undergraduate/masters audience with
no prior background in dynamic programming. The presentation does expect a ﬁrst
course in probability and statistics. Some topics require an introductory course in
linear programming. A major goal of the book is the clear and precise presentation
of dynamic problems, which means there is an emphasis on modeling and notation.
The body of every chapter focuses on models and algorithms with a minimum
of the mathematical formalism that so often makes presentations of dynamic pro-
grams inaccessible to a broader audience. Using numerous examples, each chapter
emphasizes the presentation of algorithms that can be directly applied to a variety
of applications. The book contains dozens of algorithms that are intended to serve
as a starting point in the design of practical solutions for real problems. Material for
more advanced graduate students (with measure-theoretic training and an interest
in theory) is contained in sections marked with **.
The book can be used quite effectively in a graduate level course. Several
chapters include “Why does it work” sections at the end that present proofs at an
advanced level (these are all marked with **). This material can be easily integrated
into the teaching of the material within the chapter.
Approximate dynamic programming is also a ﬁeld that has emerged from several
disciplines. I have tried to expose the reader to the many dialects of ADP, reﬂect-
ing its origins in artiﬁcial intelligence, control theory, and operations research. In
addition to the diversity of words and phrases that mean the same thing—but often
with different connotations—I have had to make difﬁcult notational choices.
I have found that different communities offer unique insights into different
dimensions of the problem. In the main, the control theory community has the most
thorough understanding of the meaning of a state variable. The artiﬁcial intelligence
community has the most experience with deeply nested problems (which require
numerous steps before earning a reward). The operations research community has
evolved a set of tools that are well suited for high-dimensional resource allocation,
contributing both math programming and a culture of careful modeling.
W. B. P.

Acknowledgments
The work in this book reﬂects the contributions of many. Perhaps most important
are the problems that motivated the development of this material. This work would
not have been possible without the corporate sponsors who posed these problems
in the ﬁrst place. I would like to give special recognition to Schneider National, the
largest truckload carrier in the United States, Yellow Freight System, the largest
less-than-truckload carrier, and Norfolk Southern Railroad, one of the four major
railroads that serves the United States. These three companies not only posed
difﬁcult problems, they provided years of research funding that allowed me to
work on the development of tools that became the foundation of this book. This
work would never have progressed without the thousands of hours of my two senior
professional staff members, Hugo Sim˜ao and Belgacem Bouzai¨ene-Ayari, who have
written hundreds of thousands of lines of code to solve industrial-strength problems.
It is their efforts working with our corporate sponsors that brought out the richness
of real applications, and therefore the capabilities that our tools needed to possess.
While industrial sponsors provided the problems, without the participation of
my graduate students, I would simply have a set of ad hoc procedures. It is the
work of my graduate students that provided most of the fundamental insights and
algorithms, and virtually all of the convergence proofs. In the order in which
they joined by research program, the students are Linos Frantzeskakis, Raymond
Cheung, Tassio Carvalho, Zhi-Long Chen, Greg Godfrey, Joel Shapiro, Mike
Spivey, Huseyin Topaloglu, Katerina Papadaki, Arun Marar, Tony Wu, Abraham
George, Juliana Nascimento, Peter Frazier, and Ilya Ryzhov, all of whom are my
current and former students and have contributed directly to the material presented
in this book. My undergraduate senior thesis advisees provided many colorful
applications of dynamic programming, and they contributed their experiences with
their computational work.
The presentation has beneﬁted from numerous conversations with profession-
als in this community. I am particularly grateful to Erhan C¸ inlar, who taught me
the language of stochastic processes that played a fundamental role in guiding my
notation in the modeling of information. I am also grateful for many conversa-
tions with Ben van Roy, Dimitri Bertsekas, Andy Barto, Mike Fu, Dan Adelman,
Lei Zhao, and Diego Klabjan. I would also like to thank Paul Werbos at NSF
xvii

xviii
acknowledgments
for introducing me to the wonderful neural net community in IEEE, which con-
tributed what for me was a fresh perspective on dynamic problems. Jennie Si, Don
Wunsch, George Lendaris and Frank Lewis all helped educate me in the language
and concepts of the control theory community.
For the second edition of the book, I would like to add special thanks to Peter
Frazier and Ilya Ryzhov, who contributed the research on the knowledge gradient
for optimal learning in ADP, and improvements in my presentation of Gittins
indices. The research of Jun Ma on convergence theory for approximate policy
iteration for continuous states and actions contributed to my understanding in a
signiﬁcant way. This edition also beneﬁted from the contributions of Warren Scott,
Lauren Hannah, and Emre Barut who have combined to improve my understanding
of nonparametric statistics.
This research was ﬁrst funded by the National Science Foundation, but the
bulk of my research in this book was funded by the Air Force Ofﬁce of Sci-
entiﬁc Research, and I am particularly grateful to Dr. Neal Glassman for his
support through the early years. The second edition has enjoyed continued support
from AFOSR by Donald Hearn, and I appreciate Don’s dedication to the AFOSR
program.
Many people have assisted with the editing of this volume through numerous
comments. Mary Fan, Tamas Papp, and Hugo Sim˜ao all read various drafts of
the ﬁrst edition cover to cover. I would like to express my appreciation to Boris
Defourny for an exceptionally thorough proofreading of the second edition. Diego
Klabjan and his dynamic programming classes at the University of Illinois provided
numerous comments and corrections. Special thanks are due to the students in my
own undergraduate and graduate dynamic programming classes who had to survive
the very early versions of the text. The second edition of the book beneﬁted from
the many comments of my graduate students, and my ORF 569 graduate seminar
on approximate dynamic programming. Based on their efforts, many hundreds of
corrections have been made, though I am sure that new errors have been introduced.
I appreciate the patience of the readers who understand that this is the price of
putting in textbook form material that is evolving so quickly.
Of course, the preparation of this book required tremendous patience from my
wife Shari and my children Elyse and Danny, who had to tolerate my ever-present
laptop at home. Without their support, this project could never have been completed.
W.B.P.

C H A P T E R
1
The Challenges of Dynamic
Programming
The optimization of problems over time arises in many settings, ranging from the
control of heating systems to managing entire economies. In between are examples
including landing aircraft, purchasing new equipment, managing blood inventories,
scheduling ﬂeets of vehicles, selling assets, investing money in portfolios, and just
playing a game of tic-tac-toe or backgammon. These problems involve making
decisions, then observing information, after which we make more decisions, and
then more information, and so on. Known as sequential decision problems, they
can be straightforward (if subtle) to formulate, but solving them is another matter.
Dynamic programming has its roots in several ﬁelds. Engineering and economics
tend to focus on problems with continuous states and decisions (these communities
refer to decisions as controls), which might be quantities such as location, speed,
and temperature. By contrast, the ﬁelds of operations research and artiﬁcial intel-
ligence work primarily with discrete states and decisions (or actions). Problems
that are modeled with continuous states and decisions (and typically in continuous
time) are often addressed under the umbrella of “control theory,” whereas problems
with discrete states and decisions, modeled in discrete time, are studied at length
under the umbrella of “Markov decision processes.” Both of these subﬁelds set up
recursive equations that depend on the use of a state variable to capture history in a
compact way. There are many high-dimensional problems such as those involving
the allocation of resources that are generally studied using the tools of mathemati-
cal programming. Most of this work focuses on deterministic problems using tools
such as linear, nonlinear, or integer programming, but there is a subﬁeld known as
stochastic programming that incorporates uncertainty. Our presentation spans all of
these ﬁelds.
Approximate Dynamic Programming: Solving the Curses of Dimensionality, Second Edition.
Warren B. Powell.
2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
1

2
the challenges of dynamic programming
q
r
1
3
2
17
10
14
5
3
15
8
Figure 1.1
Illustration of a shortest path problem from origin q to destination r.
1.1
A DYNAMIC PROGRAMMING EXAMPLE:
A SHORTEST PATH PROBLEM
Perhaps one of the best-known applications of dynamic programming is that faced
by a driver choosing a path in a transportation network. For simplicity (and this is
a real simpliﬁcation for this application), we assume that the driver has to decide
at each node (or intersection) which link to traverse next (we are not going to get
into the challenges of left turns versus right turns). Let I be the set of intersections.
If the driver is at intersection i, he can go to a subset of intersections I+
i at a cost
cij. He starts at the origin node q ∈I and has to ﬁnd his way to the destination
node r ∈I at the least cost. An illustration is shown in Figure 1.1.
The problem can be easily solved using dynamic programming. Let
vi = Cost to get from intersection i ∈I to the destination node r.
We assume that vr = 0. Initially we do not know vi, and so we start by setting
vi = M, where “M ” is known as “big M” and represents a large number. We can
solve the problem by iteratively computing
vi ←min

vi, min
j∈I+{cij + vj}

for all i ∈I.
(1.1)
Equation (1.1) has to be solved iteratively, where at each iteration, we loop over all
the nodes i in the network. We stop when none of the values vi change. It should
be noted that this is not a very efﬁcient way of solving a shortest path problem.
For example, in the early iterations it may well be the case that vj = M for all
j ∈I+. However, we use the method to illustrate dynamic programming.
Table 1.1 illustrates the algorithm, assuming that we always traverse the nodes
in the order (q, 1, 2, 3, r). Note that we handle node 2 before node 3, which is the
reason why, even in the ﬁrst pass, we learn that the path cost from node 3 to node
r is 15 (rather than 17). We are done after iteration 3, but we require iteration 4 to
verify that nothing has changed.
Shortest path problems arise in a variety of settings that have nothing to do with
transportation or networks. Consider, for example, the challenge faced by a college

the three curses of dimensionality
3
Table 1.1
Path cost from each node to node r after each node has been visited
Cost from Node
Iteration
q
1
2
3
r
100
100
100
100
0
1
100
100
10
15
0
2
30
18
10
15
0
3
26
18
10
15
0
4
26
18
10
15
0
freshman trying to plan her schedule up to graduation. By graduation, she must take
32 courses overall, including eight departmentals, two math courses, one science
course, and two language courses. We can describe the state of her academic
program in terms of how many courses she has taken under each of these ﬁve
categories. Let Stc be the number of courses she has taken by the end of semester
t in category c = {Total courses, Departmentals, Math, Science, Language}, and
let St = (Stc)c be the state vector. Based on this state, she has to decide which
courses to take in the next semester. To graduate, she has to reach the state S8 =
(32, 8, 2, 1, 2). We assume that she has a measurable desirability for each course
she takes, and that she would like to maximize the total desirability of all her
courses.
The problem can be viewed as a shortest path problem from the state S0 =
(0, 0, 0, 0, 0) to S8 = (32, 8, 2, 1, 2). Let St be her current state at the beginning
of semester t, and let at represent the decisions she makes while determining what
courses to take. We then assume we have access to a transition function SM(St, at),
which tells us that if she is in state St and takes action at, she will land in state
St+1, which we represent by simply using
St+1 = SM(St, at).
In our transportation problem, we would have St = i if we are at intersection i,
and at would be the decision to “go to j,” leaving us in the state St+1 = j.
Finally, let Ct(St, at) be the contribution or reward she generates from being
in state St and taking the action at. The value of being in state St is deﬁned by the
equation
Vt(St) = max
xt
{Ct(St, at) + Vt+1(St+1)}
∀st ∈St,
where St+1 = SM(St, at) and where St is the set of all possible (discrete) states
that she can be in at the beginning of the year.
1.2
THE THREE CURSES OF DIMENSIONALITY
All dynamic programs can be written in terms of a recursion that relates the value
of being in a particular state at one point in time to the value of the states that we

4
the challenges of dynamic programming
are carried into at the next point in time. For deterministic problems this equation
can be written
Vt(St) = max
at

Ct(St, at) + Vt+1(St+1)

.
(1.2)
where St+1 is the state we transition to if we are currently in state St and take
action at. Equation (1.2) is known as Bellman’s equation, or the Hamilton–Jacobi
equation, or increasingly, the Hamilton–Jacobi–Bellman equation (HJB for short).
Some textbooks (in control theory) refer to them as the “functional equation” of
dynamic programming (or the “recurrence equation”). We primarily use the term
“optimality equation” in our presentation, but often use the term “Bellman equation”
because this is so widely used in the dynamic programming community.
Most of the problems that we address in this volume involve some form of
uncertainty (prices, travel times, equipment failures, weather). For example, in a
simple inventory problem we might have St DVD players in stock. We might then
order at new DVD players, after which we satisfy a random demand ˆDt+1 that
follows some probability distribution. The state variable would be described by the
transition equation
St+1 = max{0, St + at −ˆDt+1}.
Assume that Ct(St, at) is the contribution we earn at time t, given by
Ct(St, at,
ˆDt+1) = pt min{St + at,
ˆDt+1} −cat.
To ﬁnd the best decision, we need to maximize the contribution we receive from
at plus the expected value of the state that we end up at (which is random). That
means we need to solve
Vt(St) = max
at
E{Ct(St, at,
ˆDt+1) + Vt+1(St+1)|St}.
(1.3)
This problem is not too hard to solve. Assume that we know Vt+1(St+1) for each
state St+1. We just have to compute (1.3) for each value of St, which then gives us
Vt(St). We can keep stepping backward in time to compute all the value functions.
For the vast majority of problems the state of the system is a vector. For example,
if we have to track the inventory of N different products, where we might have 0,
1, . . . , M −1 units of inventory of each product, then we would have MN different
states. As we can see, the size of the state space grows very quickly as the number of
dimensions grows. This is the widely known “curse of dimensionality” of dynamic
programming and is the most often-cited reason why dynamic programming cannot
be used.
In fact, there are many applications where there are three curses of dimension-
ality. Consider the problem of managing blood inventories. There are eight blood
types (AB+, AB−, A+, A−, B+, B−, O+, O−), which means we have eight
types of blood supplies and eight types of blood demands. Let Rti be the supply of
blood type i at time t (i = 1, 2, . . . , 8), and let Dti be the demand for blood type

the three curses of dimensionality
5
i at time t. Our state variable is given by St = (Rt, Dt), where Rt = (Rti)8
i=1 (Dt
is deﬁned similarly).
In each time period there are two types of randomness: random blood donations
and random demands. Let ˆRti be the random new donations of blood of type i in
week t, and let ˆDti be the random new demands for blood of type i in week t. We
are going to let Wt = ( ˆRt, ˆDt) be the vector of random information (new supplies
and demands) that becomes known in week t.
Finally, let xtij be the amount of blood type i used to satisfy a demand for
blood of type j. We switch to “x” for the action because this is the standard
notation used in the ﬁeld of mathematical programming for solving vector-valued
decision problems. There are rules that govern what blood types can substitute for
different demand types, shown in Figure 1.2.
We can quickly see that St and Wt have 16 dimensions each. If we have up to
100 units of blood of any type, then our state space has 10016 = 1032 states. If we
have up to 20 units of blood being donated or needed in any week, then Wt has
2016 = 6.55 × 1020 outcomes. We would need to evaluate 16 nested summations
to evaluate the expectation. Finally, xt has 27 dimensions (there are 27 feasible
substitutions of blood types for demand types). Needless to say, evaluating all
possible values of xt is completely intractable.
This problem illustrates what is, for many applications, the three curses of dimen-
sionality:
1. State space. If the state variable St = (St1, St2, . . . , Sti, . . . , StI) has I dimen-
sions, and if Sti can take on L possible values, then we might have up to LI
different states.
Figure 1.2
Different substitution possibilities between donated blood and patient types (from Cant,
2006).

6
the challenges of dynamic programming
2. Outcome space. The random variable Wt = (Wt1, Wt2, . . . , Wtj, . . . , WtJ)
might have J dimensions. If Wtj can take on M outcomes, then our outcome
space might take on up to MJ outcomes.
3. Action space. The decision vector xt = (xt1, xt2, . . . , xtk, . . . , xtK) might
have K dimensions. If xtk can take on N outcomes, then we might have
up to NK outcomes. In the language of math programming, we refer to the
action space as the feasible region, and we may assume that the vector xt is
discrete (integer) or continuous.
By the time we get to Chapter 14, we will be able to produce high-quality, imple-
mentable solutions not just to the blood problem (see Section 14.2) but for problems
that are far larger. The techniques that we are going to describe have produced pro-
duction quality solutions to plan the operations of some of the largest transportation
companies in the country. These problems require state variables with millions of
dimensions, with very complex dynamics. We will show that these same algorithms
converge to optimal solutions for special cases. For these problems we will pro-
duce solutions that are within 1 percent of optimality in a small fraction of the
time required to ﬁnd the optimal solution using classical techniques. However, we
will also describe algorithms for problems with unknown convergence properties
that produce solutions of uncertain quality and with behaviors that can range from
the frustrating to the mystifying. This is a very young ﬁeld.
Not all problems suffer from the three curses of dimensionality. Many problems
have small sets of actions (do we buy or sell?), easily computable expectations (did
a customer arrive or not?), and small state spaces (the nodes of a network). The
ﬁeld of dynamic programming has identiﬁed many problems, some with genuine
industrial applications, that avoid the curses of dimensionality. Our goal is to pro-
vide guidance for the problems that do not satisfy some or all of these convenient
properties.
1.3
SOME REAL APPLICATIONS
Our experiences using approximate dynamic programming have been driven by
problems in transportation with decision vectors with thousands or tens of thousands
of dimensions, and state variables with thousands to millions of dimensions. We
have solved energy problems with 175,000 time periods. Our applications have
spanned applications in air force, ﬁnance, and health.
As our energy environment changes, we have to plan new energy resources
(such as the wind turbines in Figure 1.3). A challenging dynamic problem requires
determining when to acquire or install new energy technologies (wind turbines,
solar panels, energy storage using ﬂywheels or compressed air, hydrogen cars)
and how to operate them. These decisions have to be made when considering
uncertainty in the demand, prices, and the underlying technologies for creating,
storing, and using energy. For example, adding ethanol capacity has to include
the possibility that oil prices will drop (reducing the demand for ethanol) or that
government regulations may favor alternative fuels (increasing the demand).

some real applications
7
Figure 1.3
Wind turbines are one form of alternative energy resources (from http://www.nrel.gov/data/
pix/searchpix.cgi).
An example of a very complex resource allocation problem arises in railroads
(Figure 1.4). In North America there are six major railroads (known as “Class
I” railroads) that operate thousands of locomotives, many of which cost over $1
million. Decisions have to be made now to assign locomotives to trains, taking into
account how the locomotives will be used at the destination. For example, a train
may be going to a location that needs additional power. Or a locomotive might have
to be routed to a maintenance facility, and the destination of a train may or may not
offer good opportunities for getting the locomotive to the shop. There are many
types of locomotives, and different types of locomotives are suited to different
types of trains (e.g., trains moving coal, grain, or merchandise). Other applications
of dynamic programming include the management of freight cars, where decisions
about when, where, and how many to move have to be made in the presence of
numerous sources of uncertainty, including customer demands, transit times, and
equipment problems.
The military faces a broad range of operational challenges that require posi-
tioning resources to anticipate future demands. The problem may be ﬁguring out
when and where to position tankers for mid-air refueling (Figure 1.5), or whether

8
the challenges of dynamic programming
Figure 1.4
Major railroads in the United States have to manage complex assets such as boxcars,
locomotives and the people who operate them. Courtesy Norfolk Southern.
a cargo aircraft should be modiﬁed to carry passengers. The air mobility command
needs to think about not only what aircraft is best to move a particular load of
freight but also the value of aircraft in the future (are there repair facilities near
the destination?). The military is further interested in the value of more reliable
aircraft and the impact of last-minute requests. Dynamic programming provides a
means to produce robust decisions, allowing the military to respond to last-minute
requests.
Managing the electric power grid requires evaluating the reliability of equipment
such as the transformers that convert high-voltage power to the voltages used by
homes and businesses. Figure 1.6 shows the high-voltage backbone network man-
aged by PJM Interconnections that provides electricity to the northeastern United
States. To ensure the reliability of the grid, PJM helps utilities maintain an appro-
priate inventory of spare transformers. They cost ﬁve million dollars each, weigh
over 200 tons, and require at least a year to deliver. We must make decisions about
how many to buy, how fast they should be delivered (fast delivery costs more),
and where to put them when they do arrive. If a transformer fails, the electric
power grid may have to purchase power from more expensive utilities to avoid
a bottleneck, possibly costing millions of dollars per month. As a result it is not
possible to wait until problems happen. Utilities also face the problem of pricing
their energy in a dynamic market, and purchasing commodities such as coal and
natural gas in the presence of ﬂuctuating prices.

some real applications
9
Figure 1.5
Mid-air refueling is a major challenge for air operations, requiring that tankers be positioned
in anticipation of future needs (from http://www.amc.af.mil/photos/).
Figure 1.6
High-voltage backbone network managed by PJM Interconnections provides electricity to
the northeastern United States. Courtesy PJM Interconnections.
Similar issues arise in the truckload motor carrier industry, where drivers are
assigned to move loads that arise in a highly dynamic environment. Large com-
panies manage ﬂeets of thousands of drivers, and the challenge at any moment in
time is to ﬁnd the best driver (Figure 1.7 is from Schneider National, the largest
truckload carrier in the United States). There is much more to the problem than

10
the challenges of dynamic programming
Figure 1.7
Schneider National, the largest truckload motor carrier in the United States, manages a
ﬂeet of over 15,000 drivers. Courtesy Schneider National.
simply ﬁnding the closest driver; each driver is characterized by attributes such as
his or her home location and equipment type as well as his or her skill level and
experience. There is a need to balance decisions that maximize proﬁts now versus
those that produce good long-run behavior. Approximate dynamic programming
produced the ﬁrst accurate model of a large truckload operation. Modeling this
large-scale problem produces some of the advances described in this volume.
Challenging dynamic programs can be found in much simpler settings. A good
example involves optimizing the amount of cash held in a mutual fund, which is
a function of current market performance (should more money be invested?) and
interest rates, illustrated in Figure 1.8. While this problem can be modeled with
just three dimensions, the lack of structure and need to discretize at a ﬁne level
produced a very challenging optimization problem. Other applications include port-
folio allocation problems and determining asset valuations that depend on portfolios
of assets.
A third problem class is the acquisition of information. Consider the problem
faced by the government that is interested in researching a new technology such
as fuel cells or converting coal to hydrogen. There may be dozens of avenues to
pursue, and the challenge is to determine the projects in which the government
should invest. The state of the system is the set of estimates of how well different
components of the technology work. The government funds research to collect
information. The result of the research may be the anticipated improvement, or the

problem classes
11
Figure 1.8
Value of holding cash in a mutual fund as a function of market performance and interest
rates.
results may be disappointing. The government wants to plan a research program
to maximize the likelihood that a successful technology is developed within a
reasonable time frame (e.g., 20 years). Depending on time and budget constraints,
the government may wish to fund competing technologies in the event that one does
not work. Alternatively, it may be more effective to fund one promising technology
and then switch to an alternative if the ﬁrst does not work out.
1.4
PROBLEM CLASSES
Most of the problems that we use as examples in this book can be described
as involving the management of physical, ﬁnancial, or informational resources.
Sometimes we use the term “assets,” which carries the connotation of money or
valuable resources (aircraft, real estate, energy commodities). But in some settings,
even these terms may seem inappropriate, for example, training computers to play a
game such as tic-tac-toe, where it will be more natural to think in terms of managing
an “entity.” Regardless of the term, there are a number of major problem classes
we consider in our presentation:
Budgeting. Here we face the problem of allocating a ﬁxed resource over a set
of activities that incurs costs that are a function of how much we invest
in the activity. For example, drug companies have to decide how much to

12
the challenges of dynamic programming
invest in different research projects or how much to spend on advertising
for different drugs. Oil exploration companies have to decide how much to
spend exploring potential sources of oil. Political candidates have to decide
how much time to spend campaigning in different states.
Asset acquisition with concave costs. A company can raise capital by issu-
ing stock or ﬂoating a bond. There are costs associated with these ﬁnancial
instruments independent of how much money is being raised. Similarly an oil
company purchasing oil will be given quantity discounts (or it may face the
ﬁxed cost of purchasing a tanker-load of oil). Retail outlets get a discount if
they purchase a truckload of an item. All of these are instances of acquiring
assets with a concave (or, more generally, nonconvex) cost function, which
means there is an incentive for purchasing larger quantities.
Asset acquisition with lagged information processes. We can purchase com-
modity futures that allow us to purchase a product in the future at a lower
cost. Alternatively, we may place an order for memory chips from a factory in
southeast Asia with one- to two-week delivery times. A transportation com-
pany has to provide containers for a shipper who may make requests several
days in advance or at the last minute. All of these are asset acquisition
problems with lagged information processes.
Buying/selling an asset. In this problem class the process stops when we either
buy an asset when it looks sufﬁciently attractive or sell an asset when market
conditions warrant. The game ends when the transaction is made. For these
problems we tend to focus on the price (the purchase price or the sales price),
and our success depends on our ability to trade off current value with future
price expectations.
General resource allocation problems. This class encompasses the problem of
managing reusable and substitutable resources over time (equipment, people,
products, commodities). Applications abound in transportation and logistics.
Railroads have to move locomotives and boxcars to serve different activities
(moving trains, moving freight) over time. An airline has to move aircraft and
pilots in order to move passengers. Consumer goods have to move through
warehouses to retailers to satisfy customer demands.
Demand management. There are many applications where we focus on man-
aging the demands being placed on a process. Should a hospital admit a
patient? Should a trucking company accept a request by a customer to move
a load of freight?
Storage problems. We face problems determining how much energy to store
in a water reservoir or battery, how much cash to hold in a mutual fund,
how many vaccines to order or blood to hold, and how much inventory
to keep. These are all examples of “storage” problems, which may involve
one, several or many types of resources, and where these decisions have to
be made in the context of state-of-the-world variables such as commodity
prices, interest rates, and weather.

problem classes
13
Table 1.2
Major problem classes
Attributes
Number of Entities
Simple
Complex
Single
Single, simple entity
Single, complex entity
Multiple
Multiple, simple entities
Multiple, complex entities
Shortest paths. In this problem class we typically focus on managing a single,
discrete entity. The entity may be someone playing a game, a truck driver
we are trying to route to return him home, a driver who is trying to ﬁnd
the best path to his destination, or a locomotive we are trying to route to its
maintenance shop. Shortest path problems, however, also represent a general
mathematical structure that applies to a broad range of dynamic programs.
Dynamic assignment. Consider the problem of managing multiple entities, such
as computer programmers, to perform different tasks over time (writing code
or ﬁxing bugs). Each entity and task is characterized by a set of attributes
that determines the cost (or contribution) from assigning a particular resource
to a particular task.
All these problems focus on the problem of managing physical or ﬁnancial
resources (or assets, or entities). It is useful to think of four major problem classes
(depicted in Table 1.2) in terms of whether we are managing a single or multiple
entities (e.g., one robot or a ﬂeet of trucks), and whether the entities are simple
(an entity may be described by its location on a network) or complex (an entity
may be a truck driver described by a 10-dimensional vector of attributes).
If we are managing a single, simple entity, then this is a problem that can
be solved exactly using classical algorithms described in Chapter 3. The problem
of managing a single, complex entity (e.g., playing backgammon) is commonly
studied in computer science under the umbrella of reinforcement learning, or in
the engineering community under the umbrella of control theory (e.g., landing an
aircraft). The problem of managing multiple, simple entities is widely studied in the
ﬁeld of operations research (managing ﬂeets of vehicles or distribution systems),
although this work most commonly focuses on deterministic models. By the end of
this book, we are going to show the reader how to handle (approximately) problem
classes that include multiple, complex entities, in the presence of different forms
of uncertainty.
There is a wide range of problems in dynamic programming that involve control-
ling resources, where decisions directly involve transforming resources (purchasing
inventory, moving robots, controlling the ﬂow of water from reservoirs), but there
are other important types of controls. Some examples include:
Pricing. Often the question being asked is, What price should be paid for an
asset? The right price for an asset depends on how it is managed, so it
should not be surprising that we often ﬁnd asset prices as a by-product from
determining how to best manage the asset.

14
the challenges of dynamic programming
Information collection. Since we are modeling sequential information and deci-
sion processes, we explicitly capture the information that is available when
we make a decision, allowing us to undertake studies that change the infor-
mation process. For example, the military uses unmanned aerial vehicles
(UAVs) to collect information about targets in a military setting. Oil compa-
nies drill holes to collect information about underground geologic formations.
Travelers try different routes to collect information about travel times. Phar-
maceutical companies use test markets to experiment with different pricing
and advertising strategies.
Technology switching. The last class of questions addresses the underlying
technology that controls how the physical process evolves over time. For
example, when should a power company upgrade a generating plant (e.g., to
burn oil and natural gas)? Should an airline switch to aircraft that ﬂy faster
or more efﬁciently? How much should a communications company invest in
a technology given the likelihood that better technology will be available in
a few years?
Most of these problems entail both discrete and continuous states and actions.
Continuous models would be used for money, for physical products such as oil,
grain, and coal, or for discrete products that occur in large volume (most consumer
products). In other settings, it is important to retain the integrality of the resources
being managed (people, aircraft, locomotives, trucks, and expensive items that
come in small quantities). For example, how do we position emergency response
units around the country to respond to emergencies (bioterrorism, major oil spills,
failure of certain components in the electric power grid)?
What makes these problems hard? With enough assumptions, none of these
problems are inherently difﬁcult. But in real applications, a variety of issues emerge
that can make all of them intractable. These include:
• Evolving information processes. We have to make decisions now before we
know the information that will arrive later. This is the essence of stochastic
models, and this property quickly turns the easiest problems into computational
nightmares.
• High-dimensional problems. Most problems are easy if they are small enough.
In real applications, there can be many types of resources, producing decision
vectors of tremendous size.
• Measurement problems. Normally we assume that we look at the state of our
system and from this determine what decision to make. In many problems
we cannot measure the state of our system precisely. The problem may be
delayed information (stock prices), incorrectly reported information (the truck
is in the wrong location), misreporting (a manager does not properly add up
his total sales), theft (retail inventory), or deception (an equipment manager
underreports his equipment so it will not be taken from him).
• Unknown models (information, system dynamics). We can anticipate the future
by being able to say something about what might happen (even if it is with

the many dialects of dynamic programming
15
uncertainty) or the effect of a decision (which requires a model of how the
system evolves over time).
• Missing information. There may be costs that simply cannot be computed and
that are instead ignored. The result is a consistent model bias (although we
do not know when it arises).
• Comparing solutions. Primarily as a result of uncertainty, it can be difﬁcult
comparing two solutions to determine which is better. Should we be better
on average, or are we interested in the best and worst solution? Do we have
enough information to draw a ﬁrm conclusion?
1.5
THE MANY DIALECTS OF DYNAMIC PROGRAMMING
Dynamic programming arises from the study of sequential decision processes. Not
surprisingly, these arise in a wide range of applications. While we do not wish to
take anything from Bellman’s fundamental contribution, the optimality equations
are, to be quite honest, somewhat obvious. As a result they were discovered inde-
pendently by the different communities in which these problems arise.
The problems arise in a variety of engineering problems, typically in continuous
time with continuous control parameters. These applications gave rise to what is
now referred to as control theory. While uncertainty is a major issue in these prob-
lems, the formulations tend to focus on deterministic problems (the uncertainty is
typically in the estimation of the state or the parameters that govern the system).
Economists adopted control theory for a variety of problems involving the control
of activities from allocating single budgets or managing entire economies (admit-
tedly at a very simplistic level). Operations research (through Bellman’s work) did
the most to advance the theory of controlling stochastic problems, thereby pro-
ducing the very rich theory of Markov decision processes. Computer scientists,
especially those working in the realm of artiﬁcial intelligence, found that dynamic
programming was a useful framework for approaching certain classes of machine
learning problems known as reinforcement learning.
As different communities discovered the same concepts and algorithms, they
invented their own vocabularies to go with them. As a result we can solve the Bell-
man equations, the Hamiltonian, the Jacobian, the Hamilton–Jacobian, or the all-
purpose Hamilton–Jacobian–Bellman equations (typically referred to as the HJB
equations). In our presentation we prefer the term “optimality equations,” but “Bell-
man” has become a part of the language, imbedded in algorithmic strategies such
as minimizing the “Bellman error” and “Bellman residual minimization.”
There is an even richer vocabulary for the types of algorithms that are the focal
point of this book. Everyone has discovered that the backward recursions required
to solve the optimality equations in Section 1.1 do not work if the state variable is
multidimensional. For example, instead of visiting node i in a network, we might
visit state St = (St1, St2, . . . , StB), where Stb is the amount of blood on hand of
type b. A variety of authors have independently discovered that an alternative
strategy is to step forward through time, using iterative algorithms to help estimate

16
the challenges of dynamic programming
the value function. This general strategy has been referred to as forward dynamic
programming, incremental dynamic programming, iterative dynamic programming,
adaptive dynamic programming, heuristic dynamic programming, reinforcement
learning, and neuro-dynamic programming. The term that is being increasingly
adopted is approximate dynamic programming, although perhaps it is convenient
that the initials, ADP, apply equally well to “adaptive dynamic programming.”
The different communities have each evolved their own vocabularies and nota-
tional systems. The notation developed for Markov decision processes, and then
adopted by the computer science community in the ﬁeld of reinforcement learning,
uses state S and action a. In control theory, it is state x and control u. The ﬁeld
of stochastic programming uses x for decisions. At ﬁrst, it is tempting to view
these as different words for the same quantities, but the cosmetic differences in
vocabulary and notation tend to hide more fundamental differences in the nature
of the problems being addressed by each community. In reinforcement learning,
there is typically a small number of discrete actions. In control theory, u is usually
a low-dimensional continuous vector. In operations research, it is not unusual for
x to have hundreds or thousands of dimensions.
An unusual characteristic of the reinforcement learning community is their habit
of naming algorithms after their notation. Algorithms such as Q-learning (named
from the use of Q-factors), TD(λ), ϵ-greedy exploration, and SARSA (which
stands for state-action–reward–state-action), are some of the best examples. As a
result care has to be taken when designing a notational system.
The ﬁeld continues to be characterized by a plethora of terms that often mean
the same thing. The transition function (which models the evolution of a system
over time) is also known as the system model, transfer function, state model, and
plant model. The behavior policy is the same as the sampling policy, and a stepsize
is also known as the learning rate or the gain.
There is a separate community that evolved from the ﬁeld of deterministic
math programming that focuses on problems with high-dimensional decisions. The
reinforcement learning community focuses almost exclusively on problems with
ﬁnite (and fairly small) sets of discrete actions. The control theory community is
primarily interested in multidimensional and continuous actions (but not very many
dimensions). In operations research it is not unusual to encounter problems where
decisions are vectors with thousands of dimensions.
As early as the 1950s the math programming community was trying to intro-
duce uncertainty into mathematical programs. The resulting subcommunity is called
stochastic programming and uses a vocabulary that is quite distinct from that
of dynamic programming. The relationship between dynamic programming and
stochastic programming has not been widely recognized, despite the fact that
Markov decision processes are considered standard topics in graduate programs
in operations research.
Our treatment will try to bring out the different dialects of dynamic program-
ming, although we will tend toward a particular default vocabulary for important
concepts. Students need to be prepared to read books and papers in this ﬁeld

what is new in this book?
17
that will introduce and develop important concepts using a variety of dialects. The
challenge is realizing when authors are using different words to say the same thing.
1.6
WHAT IS NEW IN THIS BOOK?
As of this writing, dynamic programming has enjoyed a relatively long history,
with many superb books. Within the operations research community, the original
text by Bellman (Bellman, 1957) was followed by a sequence of books focusing
on the theme of Markov decision processes. Of these, the current high-water mark
is Markov Decision Processes by Puterman, which played an inﬂuential role in
the writing of Chapter 3. The ﬁrst edition appeared in 1994, followed in 2005 by
the second edition. The dynamic programming ﬁeld offers a powerful theoretical
foundation, but the algorithms are limited to problems with very low-dimensional
state and action spaces.
This book focuses on a ﬁeld that is coming to be known as approximate
dynamic programming; it emphasizes modeling and computation for much harder
classes of problems. The problems may be hard because they are large (e.g.,
large state spaces), or because we lack a model of the underlying process that
the ﬁeld of Markov decision processes takes for granted. Two major references
preceded the ﬁrst edition of this volume. Neuro-dynamic Programming by Bert-
sekas and Tsitsiklis was the ﬁrst book to appear (in 1996) that integrated stochastic
approximation theory with the power of statistical learning to approximation value
functions, in a rigorous if demanding presentation. Reinforcement Learning by
Sutton and Barto, published in 1998 (but building on research that began in
1980), presents the strategies of approximate dynamic programming in a very
readable format, with an emphasis on the types of applications that are popu-
lar in the computer science/artiﬁcial intelligence community. Along with these
books, the survey of reinforcement learning in Kaelbling et al. (1996) is a major
reference.
There is a sister community that goes by the name of simulation optimization
that has evolved out of the simulation community that needs to select the best
from a set of designs. Nice reviews of this literature are given in Fu (2002) and
Kim and Nelson (2006). Books on the topic include Gosavi (2003), Chang et al.
(2007), and Cao (2007). Simulation optimization is part of a larger community
called stochastic search, which is nicely described in the book Spall (2003). As we
show later, this ﬁeld is directly relevant to policy search methods in approximate
dynamic programming.
This volume presents approximate dynamic programming with a much stronger
emphasis on modeling, with explicit and careful notation to capture the timing of
information. We draw heavily on the modeling framework of control theory with
its emphasis on transition functions, which easily handle complex problems, rather
than transition matrices, which are used heavily in both Bertsekas and Tsitsiklis
(1996) and Sutton and Barto (1998). We start with the classical notation of Markov
decision processes that is familiar to the reinforcement learning community, but

18
the challenges of dynamic programming
we build bridges to math programming so that by the end of the book, we are
able to solve problems with very high-dimensional decision vectors. For this rea-
son we adopt two notational styles for modeling decisions: a for discrete actions
common in the models solved in reinforcement learning, and x for the continuous
and sometimes high-dimensional decision vectors common in operations research
and math programming. Throughout the book, we use action a as our default
notation, but switch to x in the context of applications that require continuous or
multidimensional decisions.
Some other important features of this book are as follows:
• We identify the three curses of dimensionality that characterize some dynamic
programs, and develop a comprehensive algorithmic strategy for overcoming
them.
• We cover problems with discrete action spaces, denoted using a (standard
in Markov decision processes and reinforcement learning), and vector-valued
decisions, denoted using x (standard in mathematical programming). The book
integrates approximate dynamic programming with math programming, mak-
ing it possible to solve intractably large deterministic or stochastic optimization
problems.
• We cover in depth the concept of the post-decision state variable, which plays
a central role in our ability to solve problems with vector-valued decisions.
The post-decision state offers the potential for dramatically simplifying many
ADP algorithms by avoiding the need to compute a one-step transition matrix
or otherwise approximate the expectation within Bellman’s equation.
• We place considerable attention on the proper modeling of random variables
and system dynamics. We feel that it is important to properly model a problem
before attempting to solve it.
• The theoretical foundations of this material can be deep and rich, but our pre-
sentation is aimed at advanced undergraduate or masters level students with
introductory courses in statistics, probability, and for Chapter 14, linear pro-
gramming. For more advanced students, proofs are provided in “Why does it
work” sections. The presentation is aimed primarily at students in engineering
interested in taking real, complex problems, developing proper mathematical
models, and producing computationally tractable algorithms.
• We identify four fundamental classes of policies (myopic, lookahead, policies
based on value function approximations, and policy function approximations),
with careful treatments of the last three. An entire chapter is dedicated to policy
search methods, and three chapters develop the critical idea of using value
function approximations.
• We carefully deal with the challenge of stepsizes, which depend critically
on whether the algorithm is based on approximate value iteration (including
Q-learning and TD learning) or approximate policy iteration. Optimal stepsize
rules are given for each of these two major classes of algorithms.

pedagogy
19
Our presentation integrates the ﬁelds of Markov decision processes, math pro-
gramming, statistics, and simulation. The use of statistics to estimate value functions
dates back to Bellman and Dreyfus (1959). Although the foundations for proving
convergence of special classes of these algorithms traces its origins to the seminal
paper on stochastic approximation theory (Robbins and Monro, 1951), the use of
this theory (in a more modern form) to prove convergence of special classes of
approximate dynamic programming algorithms did not occur until 1994 (Tsitsik-
lis 1994; Jaakkola et al. 1994). The ﬁrst book to bring these themes together is
Bertsekas and Tsitsiklis (1996), which remains a seminal reference for researchers
looking to do serious theoretical work.
1.7
PEDAGOGY
The book is roughly organized into four parts. Part I comprises Chapters 1 to 4,
which provide a relatively easy introduction using a simple, discrete representation
of states. Part II covers modeling, a description of major classes of policies and pol-
icy optimization. Part III covers policies based on value function approximations,
along with efﬁcient learning. Part IV describes specialized methods for resource
allocation problems.
A number of sections are marked with an *. These can all be skipped when
ﬁrst reading the book without loss of continuity. Sections marked with ** are
intended only for advanced graduate students with an interest in the theory behind
the techniques.
Part I Introduction to dynamic programming using simple state represen-
tations —In the ﬁrst four chapters we introduce dynamic programming, using
what is known as a “ﬂat” state representation. That is to say, we assume that
we can represent states as s = 1, 2, . . . . We avoid many of the rich modeling
and algorithmic issues that arise in more realistic problems.
Chapter 1 Here we set the tone for the book, introducing the challenge of the
three “curses of dimensionality” that arise in complex systems.
Chapter 2 Dynamic programs are best taught by example. Here we describe
three classes of problems: deterministic problems, stochastic problems, and
information acquisition problems. Notation is kept simple but precise, and
readers see a range of different applications.
Chapter 3 This is an introduction to classic Markov decision processes. While
these models and algorithms are typically dismissed because of “the curse of
dimensionality,” these ideas represent the foundation of the rest of the book.
The proofs in the “why does it work” section are particularly elegant and
help provide a deep understanding of this material.
Chapter 4 This chapter provides a fairly complete introduction to approximate
dynamic programming, but focusing purely on estimating value functions
using lookup tables. The material is particularly familiar to the reinforcement
learning community. The presentation steps through classic algorithms, start-
ing with Q-learning and SARSA, and then, progressing through real-time

20
the challenges of dynamic programming
dynamic programming (which assumes you can compute the one-step transi-
tion matrix), approximate value iteration using a pre-decision state variable,
and ﬁnally, approximate value iteration using a post-decision state variable.
Along the way the chapter provides a thorough introduction to the concept of
the post-decision state variable, and introduces the issue of exploration and
exploitation, as well as on-policy and off-policy learning.
Part II Approximate dynamic programming with policy optimization —This block
introduces modeling, the design of policies, and policy optimization. Policy
optimization is the simplest method for making good decisions, but it is generally
restricted to relatively simple problems. As such, it makes for a good introduction
to ADP before getting into the complexities of designing policies based on value
function approximations.
Chapter 5 This chapter on modeling hints at the richness of dynamic problems.
To help with assimilating this chapter, we encourage readers to skip sections
marked with an * the ﬁrst time they go through the chapter. It is also useful
to reread this chapter from time to time as you are exposed to the rich set of
modeling issues that arise in real applications.
Chapter 6 This chapter introduces four fundamental classes of policies: myopic
policies, lookahead policies, policies based on value function approximations,
and policy function approximations. We note that there are three classes of
approximation strategies: lookup table, and parametric and nonparametric
models. These fundamental categories appear to cover all the variations of
policies that have been suggested.
Chapter 7 There are many problems where the structure of a policy is fairly
apparent, but it depends on tunable parameters. Here we introduce the reader
to communities that seek to optimize functions of deterministic parameters
(which determines the policy) where we depend on noisy evaluations to
estimate the performance of the policy. We cover classical stochastic search,
add algorithms from the ﬁeld of simulation optimization, and introduce the
idea of the knowledge gradient, which has proved to be a useful general-
purpose algorithmic strategy. In the process, the chapter provides an initial
introduction to the exploration-exploitation problem for (ofﬂine) ranking and
selection problems.
Part III Approximate dynamic programming using value function approxi-
mations —This is the best-known strategy for solving dynamic programs
(approximately), and also the most difﬁcult to master. We break this process
into three steps, organized into the three chapters below:
Chapter 8 This chapter covers the basics of approximating functions using
lookup tables (very brieﬂy), parametric models (primarily linear regression)
and a peek into nonparametric methods.
Chapter 9 Let V
π(s) be an approximation of the value of being in state s
while following a ﬁxed policy π. We ﬁt this approximation using sample
observations ˆvn. This chapter focuses on different ways of computing ˆvn for
ﬁnite and inﬁnite horizon problems, which can then be used in conjunction
with the methods in Chapter 8 to ﬁnd V
π(s).

pedagogy
21
Chapter 10 The real challenge is estimating the value of a policy while simul-
taneously searching for better policies. This chapter introduces algorithms
based on approximate value iteration (including Q-learning and TD learning)
and approximate policy iteration. The discussion covers issues of convergence
that arise while one simultaneously tries to estimate and optimize.
Chapter 11 Stepsizes are an often overlooked dimension of approximate
dynamic programming. This chapter reviews four classes of stepsizes:
deterministic formulas, heuristic stochastic formulas, optimal stepsize rules
based on signal processing (ideally suited for policy iteration), and a new
optimal stepsize designed purely for approximate value iteration.
Chapter 12 It is well known in the ADP/RL communities that it is sometimes
necessary to visit a state in order to learn about the value of being in a state.
Chapter 4 introduces this issue, and Chapter 7 returns to the issue again in
the context of policy search. Here we address the problem in its full glory,
making the transition from pure learning (no physical state) for both online
and ofﬂine problems, but also from learning in the presence of a physical
state.
Part IV Resource allocation and implementation challenges —We close with meth-
ods that are speciﬁcally designed for problems that arise in the context of
resource allocation:
Chapter 13 Resource allocation problems have special structure such as concav-
ity (or convexity for minimization problems). This chapter describes a series
of approximation techniques that are directly applicable for these problems.
Chapter 14 There are many problems that can be described under the umbrella
of “resource allocation” that offer special structure that we can exploit. These
problems tend to be high-dimensional, with state variables that can easily
have thousands or even millions of dimensions. However, when we combine
concavity with the post-decision state variable, we produce algorithms that
can handle industrial-strength applications.
Chapter 15 We close with a discussion of a number of more practical issues
that arise in the development and testing of ADP algorithms.
This material is best covered in order. Depending on the length of the course and
the nature of the class, an instructor may want to skip some sections, or to weave
in some of the theoretical material in the “why does it work” sections. Additional
material (exercises, solutions, datasets, errata) will be made available over time at
the website http://www.castlelab.princeton.edu/adp.htm (this can also be accessed
from the CASTLE Lab website at http://www.castlelab.princeton.edu/).
There are two faces of approximate dynamic programming, and we try to present
both of them. The ﬁrst emphasizes models and algorithms, with an emphasis on
applications and computation. It is virtually impossible to learn this material with-
out writing software to test and compare algorithms. The other face is a deeply
theoretical one that focuses on proofs of convergence and rate of convergence.
This material is advanced and accessible primarily to students with training in
probability and stochastic processes at an advanced level.

22
the challenges of dynamic programming
1.8
BIBLIOGRAPHIC NOTES
There have been three major lines of investigation that have contributed to approxi-
mate dynamic programming. The ﬁrst started in operations research with Bellman’s
seminal text (Bellman, 1957). Numerous books followed using the framework
established by Bellman, each making important contributions to the evolution
of the ﬁeld. Selected highlights include Howard (1960), Derman (1970), Ross
(1983), and Heyman and Sobel (1984). As of this writing, the best overall treat-
ment of what has become known as the ﬁeld of Markov decision processes is
given in Puterman (2005). However, this work has focused largely on theory, since
the ﬁeld of discrete Markov decision processes has not proved easy to apply,
as discrete representations of state spaces suffer from the well-known curse of
dimensionality, which restricts this theory to extremely small problems. The use of
statistical methods to approximate value functions originated with Bellman, in Bell-
man and Dreyfus (1959), but little subsequent progress was made within operations
research.
The second originated with efforts by computer scientists to get computers to
solve problems, starting with the work of Samuel (1959) to train a computer to
play checkers, helping to launch the ﬁeld that would become known in artiﬁcial
intelligence as reinforcement learning. The real origins of the ﬁeld lay in the sem-
inal work in psychology initiated by Andy Barto and Richard Sutton (Sutton and
Barto, 1981; Barto et al., 1981; Barto and Sutton, 1981). This team made many
contributions over the next two decades, leading up to their landmark volume
Reinforcement Learning (Sutton and Barto, 1998) which has effectively deﬁned
this ﬁeld. Reinforcement learning evolved originally as an intuitive framework for
describing human (and animal) behavior, and only later was the connection made
with dynamic programming, when computer scientists adopted the notation devel-
oped within operations research. For this reason reinforcement learning as practiced
by computer scientists and Markov decision processes as practiced by operations
research share a common notation, but a very different culture.
The third line of investigation started completely independently under the
umbrella of control theory. Instead of Bellman’s optimality equation, it was
the Hamiltonian or Jacobi equations, which evolved to the Hamilton–Jacobi
equations. Aside from different notation, control theorists were motivated by
problems of operating physical processes, and as a result focused much more on
problems in continuous time, with continuous states and actions. While analytical
solutions could be obtained for special cases, it is perhaps not surprising that
control theorists quickly developed their own style of approximate dynamic
programming, initially called heuristic dynamic programming (Werbos, 1974,
1989, 1992b). It was in this community that the ﬁrst connection was made
between the adaptive learning algorithms of approximate dynamic programming
and reinforcement learning, and the ﬁeld of stochastic approximation theory. The
seminal papers that made this connection were Tsitsiklis (1994), Tsitsiklis and
van Roy (1997), and the seminal book Neuro-dynamic Programming written
by Bertsekas and Tsitsiklis (1996). A major breakthrough in control theory was

bibliographic notes
23
the recognition that the powerful technology of neural networks (Haykin, 1999)
could be a general-purpose tool for approximating both value functions as well
as policies. Major contributions were also made within the ﬁeld of economics,
including Rust (1997) and Judd (1998).
While these books have received the greatest visibility, special recognition is
due to a series of workshops funded by the National Science Foundation under
the leadership of Paul Werbos, some of which have been documented in several
edited volumes (Werbos et al., 1990; White and Sofge, 1992; Si et al., 2004).
These workshops have played a signiﬁcant role in bringing different communities
together, the effect of which can be found throughout this volume.
Our presentation is primarily written from the perspective of approximate
dynamic programming and reinforcement learning as it is practiced in operations
research and computer science, but there are substantial contributions to this ﬁeld
that have come from the engineering controls community. The edited volume by
White and Sofge, (1992) provides the ﬁrst comprehensive coverage of ADP/RL,
primarily from the perspective of the controls community (there is a single chapter
by Andy Barto on reinforcement learning). Bertsekas and Tsitsiklis (1996) is also
written largely from a control perspective, although the inﬂuence of problems
from operations research and artiﬁcial intelligence are apparent. A separate and
important line of research grew out of the artiﬁcial intelligence community, which
is nicely summarized in the review by Kaelbling et al. (1996) and the introductory
textbook by Sutton and Barto (1998). More recently Si et al. (2004) brought
together papers from the engineering controls community, artiﬁcial intelligence,
and operations research.
Since the ﬁrst edition of this book appeared in 2007, a number of other important
references have appeared. The third edition of Dynamic Programming and Optimal
Control, volume 2, by Bertsekas contains a lengthy chapter entitled Approximate
Dynamic Programming, which he updates continuously and which can be down-
loaded directly from http://web.mit.edu/dimitrib/www/dpchapter.html. Sutton and
Barto released a version of their 1998 book to the Internet at http://www.cs.wmich.
edu/∼trenary/ﬁles/cs5300/RLBook/the-book.html, which makes their classic book
easily accessible. Two recent additions to the literature include Busoniu et al.
(2010) and Szepesvari (2010), the latter of which is also available as a free down-
load at http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005
AIM009. These are more advanced monographs, but both contain recent theoretical
and algorithmic developments.
Energy applications represent a growing area of research in approximate
dynamic programming. Applications include L¨ohndorf and Minner (2010) and
Powell et al. (2011). An application of ADP to truckload trucking at Schneider
National is described in Simao et al. (2009, 2010). The work on Air Force
operations is described in Wu et al. (2009). The application to the mutual fund
cash balance problem is given in Nascimento and Powell (2010b).


C H A P T E R
2
Some Illustrative Models
Dynamic programming is one of those incredibly rich ﬁelds that has ﬁlled the
careers of many. But it is also a deceptively easy idea to illustrate and use. This
chapter presents a series of applications that illustrate the modeling of dynamic pro-
grams. The goal of the presentation is to teach dynamic programming by example.
The applications range from problems that can be solved analytically, to those that
can be solved using fairly simple numerical algorithms, to very large scale prob-
lems that will require carefully designed applications. The examples in this chapter
effectively communicate the range of applications that the techniques in this book
are designed to solve.
It is possible, after reading this chapter, to conclude that “dynamic program-
ming is easy” and to wonder “why do I need the rest of this book?” The answer
is: sometimes dynamic programming is easy and requires little more than the
understanding gleaned from these simple problems. But there is a vast array of
problems that are quite difﬁcult to model, and where standard solution approaches
are computationally intractable.
We present three classes of examples: (1) deterministic problems, where every-
thing is known; (2) stochastic problems, where some information is unknown, that
are described by a known probability distribution; and (3) information acquisition
problems, where we have uncertainty described by an unknown distribution. In
the last problem class the focus is on collecting information so that we can better
estimate the distribution.
These illustrations are designed to teach by example. The careful reader will
pick up subtle modeling decisions, in particular the indexing with respect to time.
We defer to Chapter 5 a more complete explanation of our choices, where we
provide an in-depth treatment of how to model a dynamic program.
Approximate Dynamic Programming: Solving the Curses of Dimensionality, Second Edition.
Warren B. Powell.
2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
25

26
some illustrative models
2.1
DETERMINISTIC PROBLEMS
Dynamic programming is widely used in many deterministic problems as a tech-
nique for breaking down what might be a very large problem into a sequence
of much smaller problems. The focus of this book is on stochastic problems, but
the value of dynamic programming for solving some large, deterministic problems
should never be lost.
2.1.1
The Shortest Path Problem
Perhaps one of the most popular dynamic programming problems is known as the
shortest path problem. Although it has a many applications, it is easiest to describe
in terms of the problem faced by every driver when ﬁnding a path from one location
to the next over a road network. Let
I = the set of nodes (intersections) in the network,
L = the set of links (i, j) in the network,
cij = the cost (typically the time) to drive from node i to node j, i, j ∈I,
(i, j) ∈L,
I +
i = the set of nodes j for which there is a link (i, j) ∈L,
I −
j = the set of nodes i for which there is a link (i, j) ∈L.
We assume that a traveler at node i can choose to traverse any link (i, j), where
j ∈I +
i . Suppose that our traveler is starting at some node q and needs to get to a
destination node r at the least cost. Let
vj = the minimum cost required to get from node j to node r.
We do not know vj, but we do know that vr = 0. Let vn
j be our estimate, at iteration
n, of the cost to get from j to r. We can ﬁnd the optimal costs, vj, by initially
setting v0
j to a large number for j ̸= r and then iteratively looping over all the
nodes, ﬁnding the best link to traverse out of an intersection i by minimizing the
sum of the outbound link cost cij plus our current estimate of the downstream
value vn−1
j
. The complete algorithm is summarized in Figure 2.1. This algorithm
has been proved to converge to the optimal set of node values.
There is a substantial literature on solving shortest path problems. Because they
arise in so many applications, there is tremendous value in solving these problems
very quickly. Our basic algorithm is not very efﬁcient because we are often solving
equation (2.1) for an intersection i where vn−1
i
= M, and where vn−1
j
= M for all
j ∈I +
i . A more standard strategy is to maintain a candidate list of nodes C that
consists of an ordered list i1, i2, . . .. Initially the list will consist only of the
destination node r (since we are solving the problem of ﬁnding paths into the
destination node r). As we reach over links into node i in the candidate list, we

deterministic problems
27
Step 0. Let
v0
j =

M,
j ̸= r,
0,
j = r,
where “M ” is known as “big-M” and represents a large number. Let n = 1.
Step 1. Solve for all i ∈I,
vn
i = min
j∈I+
i

cij + vn−1
j

.
(2.1)
Step 2. If vn
i < vn−1
i
for any i, let n = n+1 and return to step 1. Else stop.
Figure 2.1
Basic shortest path algorithm.
Step 0. Let
vj =

M,
j ̸= r,
0,
j = r,
Let n = 1. Set the candidate list C = {q}.
Step 1. Choose node j ∈C from the top of the candidate list.
Step 2. For all nodes i ∈I−
j do:
Step 2a.
ˆvi = cij + vj.
(2.2)
Step 2b. If ˆvi < vi, then set vi = ˆvi. If i /∈C, add i to the candidate list: C = C ∪{i}
(i is assumed to be put at the bottom of the list).
Step 3. Drop node j from the candidate list. If the candidate list C is not empty, return to
step 1.
Figure 2.2
More efﬁcient shortest path algorithm.
may ﬁnd a betterw path from some node j, which is then added to the candidate
list (if it is not already there). The algorithm is illustrated in Figure 2.2.
Almost any (deterministic) discrete dynamic program can be viewed as a shortest
path problem. We can view each node i as representing a particular discrete state
of the system. The origin node q is our starting state, and the ending state r might
be any state at an ending time T. We can also have shortest path problems deﬁned
over inﬁnite horizons, although we would typically include a discount factor.
2.1.2
The Discrete Budgeting Problem
Assume that we have to allocate a budget of size R to a series of tasks T. Let
at be a discrete action representing the amount of money allocated to task t, and
let Ct(at) be the contribution (or reward) that we receive from this allocation. We

28
some illustrative models
would like to maximize our total contribution
max
a

t∈T
Ct(at)
(2.3)
subject to the constraint on our available resources

t∈T
at = R.
(2.4)
In addition we cannot allocate negative resources to any task, so we include
at ≥0.
(2.5)
We refer to (2.3) through (2.5) as the budgeting problem (other authors refer to
it as the “resource allocation problem,” a term we ﬁnd too general for such a
simple problem). In this example all data are deterministic. There are a number of
algorithmic strategies for solving this problem that depend on the structure of the
contribution function, but we are going to show how it can be solved without any
assumptions.
We will approach this problem by ﬁrst deciding how much to allocate to task
1, then to task 2, and so on, until the last task, T. In the end, however, we want a
solution that optimizes over all tasks. Let
Vt(Rt) = the value of having Rt resources remaining to
allocate to task t and later tasks.
Implicit in our deﬁnition of Vt(Rt) is that we are going to solve the problem of
allocating Rt over tasks t, t + 1, . . . , T in an optimal way. Imagine that we
somehow know the function Vt+1(Rt+1). The relationship between Rt+1 and Rt is
given by
Rt+1 = Rt −at.
(2.6)
In the language of dynamic programming, Rt is known as the state variable, which
captures all the information we need to model the system forward in time (we
provide a more careful deﬁnition in Chapter 5). Equation (2.6) is the transition
function that relates the state at time t to the state at time t + 1. Sometimes we
need to explicitly refer to the transition function (rather than just the state at time
t + 1), in which case we use
RM(Rt, at) = Rt −at.
(2.7)
Equation (2.7) is referred to in some communities as the system model, since it
models the physics of the system over time (hence our use of the superscript M ).
The relationship between Vt(Rt) and Vt+1(Rt+1) is given by
Vt(Rt) =
max
0≤at≤Rt

Ct(at) + Vt+1(RM(Rt, at))

.
(2.8)

deterministic problems
29
Equation (2.8) is the optimality equation and represents the foundational equation
for dynamic programming. It says that the value of having Rt resources for task t
is the value of optimizing the contribution from task t plus the value of then having
RM(Rt, at) = Rt+1 = Rt −at resources for task t + 1 (and beyond). It forces us
to balance the contribution from task t against the value that we would receive
from all future tasks (which is captured in Vt+1(Rt −at)). One way to solve (2.8)
is to assume that at is discrete. For example, if our budget is R = $10 million, we
might require at to be in units of $100,000 dollars. In this case we would solve
(2.8) simply by searching over all possible values of at (since it is a scalar, this is
not too hard). The problem is that we do not know what Vt+1(Rt+1) is.
The simplest strategy for solving our dynamic program in (2.8) is to start by
using VT +1(R) = 0 (for any value of R). Then we would solve
VT (RT ) =
max
0≤aT ≤RT
CT (aT )
(2.9)
for 0 ≤RT ≤R. Now we know VT (RT ) for any value of RT that might actually
happen. Next we can solve
VT −1(RT −1) =
max
0≤aT −1≤RT −1
(CT −1(aT −1) + VT (RT −1 −aT −1)) .
(2.10)
Clearly, we can play this game recursively, solving (2.8) for t = T−1, T−2, . . . , 1.
Once we have computed Vt for t = (1, 2, . . . , T), we can then start at t = 1 and
step forward in time to determine our optimal allocations.
This strategy is simple, easy, and optimal. It has the nice property that we do not
need to make any assumptions about the shape of Ct(at), other than ﬁniteness. We
do not need concavity or even continuity; we just need the function to be deﬁned
for the discrete values of at that we are examining.
2.1.3
The Continuous Budgeting Problem
It is usually the case that dynamic programs have to be solved numerically. In
this section we introduce a form of the budgeting problem that can be solved
analytically. Assume that the resources we are allocating are continuous (e.g., how
much money to assign to various activities), which means that Rt is continuous,
as is the decision of how much to budget. Throughout this book we use action
a whenever we have a ﬁnite set of discrete actions. Often we face the problem
of solving problems with continuous, and possibly vector-valued, decisions. For
these problems we use the notation x that is standard in the ﬁeld of mathematical
programming. We are then going to assume that the contribution from allocating
xt dollars to task t is given by
Ct(xt) = √xt.
This function assumes that there are diminishing returns from allocating additional
resources to a task, as is common in many applications. We can solve this problem

30
some illustrative models
exactly using dynamic programming. We ﬁrst note that if we have RT dollars left
for the last task, the value of being in this state is
VT (RT ) = max
xT ≤RT
√xT .
Since the contribution increases monotonically with xT , the optimal solution is
xT = RT , which means that VT (RT ) = √RT . Now consider the problem at time t
= T−1. The value of being in state RT −1 would be
VT −1(RT −1) =
max
xT −1≤RT −1
√xT −1 + VT (RT (xT −1))

,
(2.11)
where RT (xT −1) = RT −1 −xT −1 is the money left over from time period T−1.
Since we know VT (RT ), we can rewrite (2.11) as
VT −1(RT −1) =
max
xT −1≤RT −1
√xT −1 +
	
RT −1 −xT −1

.
(2.12)
We solve (2.12) by differentiating with respect to xT −1 and setting the derivative
equal to zero (we are taking advantage of the fact that we are maximizing a
continuously differentiable, concave function). Let
FT −1(RT −1, xT −1) = √xT −1 +
	
RT −1 −xT −1.
Differentiating FT −1(RT −1, xT −1) and setting this equal to zero gives
∂FT −1(RT −1, xT −1)
∂xT −1
= 1
2(xT −1)−1/2 −1
2(RT −1 −xT −1)−1/2
= 0.
This implies that
xT −1 = RT −1 −xT −1,
which gives
x∗
T −1 = 1
2RT −1.
We now have to ﬁnd VT −1. Substituting x∗
T −1 back into (2.12) gives
VT −1(RT −1) =

RT −1
2
+

RT −1
2
= 2

RT −1
2
.

stochastic problems
31
We can continue this exercise, but there seems to be a bit of a pattern forming (this
is a common trick when trying to solve dynamic programs analytically). It seems
that a general formula might be
VT −t+1(RT −t+1) = t

RT −t+1
t
(2.13)
or, equivalently,
Vt(Rt) = (T −t + 1)

Rt
T −t + 1.
(2.14)
How do we determine if this guess is correct? We use a technique known as proof
by induction. We assume that (2.13) is true for VT −t+1(RT −t+1) and then show
that we get the same structure for VT −t(RT −t). Since we have already shown that
it is true for VT and VT −1, this result would allow us to show that it is true for
all t.
Finally, we can determine the optimal solution using the value function in
equation (2.14). The optimal value of xt is found by solving
max
xt

√xt + (T −t)

Rt −xt
T −t

.
(2.15)
Differentiating and setting the result equal to zero gives
1
2(xt)−1/2 −1
2
Rt −xt
T −t
−1/2
= 0.
This implies that
xt = Rt −xt
T −t .
Solving for xt gives
x∗
t =
Rt
T −t + 1.
This gives us the very intuitive result that we want to evenly divide the available
budget among all remaining tasks. This is what we would expect since all the tasks
produce the same contribution.
2.2
STOCHASTIC PROBLEMS
Dynamic programming can be a useful algorithmic strategy for deterministic prob-
lems, but it is often an essential strategy for stochastic problems. In this section we
illustrate a number of stochastic problems, with the goal of illustrating the chal-
lenge of modeling the ﬂow of information (essential to any stochastic problem).
We also make the transition from problems with fairly simple state variables to
problems with extremely large state spaces.

32
some illustrative models
Do not use
weather report
Use weather report
Forecast sunny 0.6
Rain 0.8  –$2000
Clouds 0.2  $1000
Sun 0.0  $5000
Rain 0.8  –$200
Clouds 0.2  –$200
Sun 0.0  –$200
Schedule game
Cancel game
Rain 0.1  –$2000
Clouds 0.5  $1000
Sun 0.4 $5000
Rain 0.1  –$200
Clouds 0.5  –$200
Sun 0.4  –$200
Schedule game
Cancel game
Rain 0.1  –$2000
Clouds 0.2 $1000
Sun 0.7  $5000
Rain 0.1  –$200
Clouds 0.2  –$200
Sun 0.7  –$200
Schedule game
Cancel game
Rain 0.2  –$2000
Clouds 0.3  $1000
Sun 0.5  $5000
Rain 0.2  –$200
Clouds 0.3 –$200
Sun 0.5  –$200
Schedule game
Cancel game
Forecast cloudy 0.3
Forecast rain 0.1
– Decision nodes
– Outcome nodes
Figure 2.3
Decision tree showing decision nodes and outcome nodes.
2.2.1
Decision Trees
One of the most effective ways of communicating the process of making decisions
under uncertainty is to use decision trees. Figure 2.3 illustrates a problem facing a
Little League baseball coach trying to schedule a playoff game under the threat of
bad weather. The coach ﬁrst has to decide if he should check the weather report.
Then he has to decide if he should schedule the game. Bad weather brings a poor
turnout that reduces revenues from tickets and the concession stand. There are
costs if the game is scheduled (umpires, food, people to handle parking and the
concession stand) that need to be covered by the revenue the game might generate.
In Figure 2.3, squares denote decision nodes where we have to choose an action
(Does he check the weather report? Does he schedule the game?), while circles
represent outcome nodes where new (and random) information arrives (What will
the weather report say? What will the weather be?). We can “solve” the decision
tree (i.e., ﬁnd the best decision given the information available) by rolling backward
through the tree. In Figure 2.4a we ﬁnd the expected value of being at each of
the end outcome nodes. For example, if we check the weather report and see a
forecast of rain, the probability that it will actually rain is 0.80, producing a loss of
$2000; the probability that it will be cloudy is 0.20, producing a proﬁt of $1000;
the probability it will be sunny is zero (if it were sunny, we would make a proﬁt

stochastic problems
33
Do not use
weather report
Use weather report
Forecast sunny 0.6
Schedule game
Cancel game
Schedule game
Cancel game
Schedule game
Cancel game
Schedule game
Cancel game
Forecast cloudy 0.3
Forecast rain 0.1
–$1400
–$200
$2300
–$200
$3500
–$200
$2400
–$200
(a)
Do not use
weather report
Use weather report
Forecast sunny 0.6
Schedule game
Cancel game
Forecast cloudy 0.3
Forecast rain 0.1
–$200
$2300
$3500
$2400
–$200
(b)
Do not use
weather report
Use weather report
$2770
$2400
(c)
Figure 2.4
Evaluating a decision tree. (a) Evaluating the ﬁnal outcome nodes. (b) Evaluating the ﬁnal
decision nodes. (c) Evaluating the ﬁrst outcome nodes.
of $5000). The expected value of scheduling the game, when the weather forecast
is rain, is (0.8)(−$2000) + (0.2)($1000) + (0)($5000) = −$1400. Repeating this
calculation for each of the ending outcome nodes produces the results given in
Figure 2.4a.
At a decision node we get to choose an action, and of course we choose the
action with the highest expected proﬁt. The results of this calculation are given
in Figure 2.4b. Finally, we have to determine the expected value of checking the
weather report by again by multiplying the probability of each possible weather
forecast (rainy, cloudy, sunny) times the expected value of each outcome. Thus
the expected value of checking the weather report is (0.1)(−$200) + (0.3)($2300)
+ (0.6)($3500) = $2770, shown in Figure 2.4c. The expected value of making
decisions without the weather report is $2400, so the analysis shows that we should
check the weather report. Alternatively, we can interpret the result as telling us that
we would be willing to pay up to $300 for the weather report.
Almost any dynamic program with discrete states and actions can be modeled
as a decision tree. The problem is that decision trees are not practical when there
are a large number of states and actions.
2.2.2
A Stochastic Shortest Path Problem
We are often interested in shortest path problems where there is uncertainty in the
cost of traversing a link. For our transportation example it is natural to view the
travel time on a link as random, reﬂecting the variability in trafﬁc conditions on
each link. There are two ways we can handle the uncertainty. The simplest is to
assume that our driver has to make a decision before seeing the travel time over

34
some illustrative models
the link. In this case our updating equation would look like
vn
i = min
j∈I +
i
E{cij(W) + vn−1
j
},
where W is some random variable that contains information about the network
(e.g., travel times). This problem is identical to our original problem; all we have
to do is to let cij = E{cij(W)} be the expected cost on an arc.
An alternative model is to assume that we know the travel time on a link from
i to j as soon as we arrive at node i. In this case we would have to solve
vn
i = E

min
j∈I +
i

cij(W) + vn−1
j

.
Here the expectation is outside of the min operator that chooses the best decision,
capturing the fact that now the decision itself is random.
Note that our notation is ambiguous, in that with the same notation we have two
very different models. In Chapter 5 we are going to reﬁne our notation so that it
will be immediately apparent when a decision “sees” the random information and
when the decision has to be made before the information becomes available.
2.2.3
The Gambling Problem
A gambler has to determine how much of his capital he should bet on each round of
a game, where he will play a total of N rounds. He will win a bet with probability
p and lose with probability q = 1−p (assume q < p). Let sn be his total capital
after n plays, n = 1, 2, . . . , N , with s0 being his initial capital. For this problem
we refer to sn as the state of the system. Let an be the (discrete) amount he bets in
round n, where we require that an ≤sn−1. Our gambler wants to maximize ln sN
(this provides a strong penalty for ending up with a small amount of money at the
end and a declining marginal value for higher amounts).
Let
W n =

1
if the gambler wins the nth game,
0
otherwise.
The system evolves according to
Sn = Sn−1 + anW n −an(1 −W n).
Let V n(Sn) be the value of having Sn dollars at the end of the nth game. The value
of being in state Sn at the end of the nth round can be written
Vn(Sn) =
max
0≤an+1≤Sn E{V n+1(Sn+1)|Sn}
=
max
0≤an+1≤Sn E{V n+1(Sn + an+1W n+1 −an+1(1 −W n+1))|Sn}.

stochastic problems
35
Here we claim that the value of being in state Sn is found by choosing the decision
that maximizes the expected value of being in state Sn+1 given what we know at
the end of the nth round.
We solve this by starting at the end of the N th trial, and assuming that we have
ﬁnished with SN dollars. The value of this is
V N(SN) = ln SN.
Now step back to n = N −1, where we may write
V N−1(SN−1) =
max
0≤aN ≤SN−1 E{V N(SN−1 + aNW N −aN(1 −W N))|SN−1}
=
max
0≤aN ≤SN−1

p ln(SN−1 + aN) + (1 −p) ln(SN−1 −aN)

.
(2.16)
Let V N−1(SN−1, aN) be the value within the max operator. We can ﬁnd aN by
differentiating V N−1(SN−1, aN) with respect to aN, giving
∂V N−1(SN−1, aN)
∂aN
=
p
SN−1 + aN −
1 −p
SN−1 −aN
= 2SN−1p −SN−1 −aN
(SN−1)2 −(aN)2
.
Setting this equal to zero and solving for aN gives
aN = (2p −1)SN−1.
The next step is to plug this back into (2.16) to ﬁnd V N−1(sN−1) using
V N−1(SN−1) = p ln(SN−1 + SN−1(2p −1)) + (1 −p) ln(SN−1 −SN−1(2p −1))
= p ln(SN−12p) + (1 −p) ln(SN−12(1 −p))
= p ln SN−1 + (1 −p) ln SN−1 + p ln(2p) + (1 −p) ln(2(1 −p))



K
= ln SN−1 + K,
where K is a constant with respect to SN−1. Since the additive constant does not
change our decision, we may ignore it and use V N−1(SN−1) = ln SN−1 as our value
function for N −1, which is the same as our value function for N . Not surprisingly,
we can keep applying this same logic backward in time and obtain
V n(Sn) = ln Sn ( +KN)
for all n, where again, Kn is some constant that can be ignored. This means that
for all n, our optimal solution is
an = (2p −1)Sn−1.
The optimal strategy at each iteration is to bet a fraction β = (2p−1) of our current
money on hand. Of course, this requires that p > 0.5.

36
some illustrative models
2.2.4
Asset Valuation
Imagine that you are holding an asset that you can sell at a price that ﬂuctuates
randomly. In this problem we want to determine the best time to sell the asset and,
from this, to infer the value of the asset. For this reason this type of problem arises
frequently in the context of asset valuation and pricing.
Let ˆpt be the price that is revealed in period t, at which point you have to make
a decision:
at =

1
sell.
0
hold.
For our simple model we assume that ˆpt is independent of prior prices (a more typ-
ical model would assume that the change in price is independent of prior history).
With this assumption, our system has two states:
St =

1
we are holding the asset,
0
we have sold the asset.
Assume that we measure the state immediately after the price ˆpt has been revealed
but before we have made a decision. If we have sold the asset, then there is nothing
we can do. We want to maximize the price we receive when we sell our asset. Let
the scalar Vt be the value of holding the asset at time t. This can be written
Vt = max
at∈{0,1}

at ˆpt + (1 −at)γ EVt+1

.
So either we get the price ˆpt if we sell, or we get the discounted future value of
the asset. Assuming the discount factor γ < 1, we do not want to hold too long
simply because the value in the future is worth less than the value now. In practice,
we eventually will see a price ˆpt that is greater than the future expected value, at
which point we will stop the process and sell our asset.
The time at which we sell our asset is known as a stopping time. By deﬁnition,
aτ = 1. It is common to think of τ as the decision variable, where we wish to solve
max
τ
E ˆpτ.
(2.17)
Equation (2.17) is a little tricky to interpret. Clearly, the choice of when to stop is
a random variable since it depends on the price ˆpt. We cannot optimally choose a
random variable, so what is meant by (2.17) is that we wish to choose a function
(or policy) that determines when we are going to sell. For example, we would
expect that we might use a rule that says
At(St, p) =

1
if ˆpt ≥p and St = 1,
0
otherwise.
(2.18)

stochastic problems
37
In this case we have a function parameterized by p that allows us to write our
problem in the form
max
p
E
∞

t=1
γ tAt(St, p).
This formulation raises two questions. First, while it seems very intuitive that our
policy would take the form given in equation (2.18), there is the theoretical question
of whether this in fact is the structure of an optimal policy. The second question is
how to ﬁnd the best policy within this class. For this problem, that means ﬁnding
the parameter p. For problems where the probability distribution of the random
process driving prices is (assumed) known, this is a rich and deep theoretical
challenge. Alternatively, there is a class of algorithms from stochastic optimization
that allows us to ﬁnd “good” values of the parameter in a fairly simple way.
2.2.5
The Asset Acquisition Problem I
A basic asset acquisition problem arises in applications where we purchase product
at time t to be used during time interval t + 1. We are going to encounter this
problem again, sometimes as discrete problems (where we would use action a), but
often as continuous problems, and sometimes as vector-valued problems (when we
have to acquire different types of assets). For this reason we use x as our decision
variable.
We can model the problem using
Rt = assets on hand at time t before we make a new ordering decision, and
before we have satisﬁed any demands arising in time interval t;
xt = amount of product purchased at time t to be used during time interval
t + 1;
ˆDt = random demands that arise between t −1 and t.
We have chosen to model Rt as the resources on hand in period t before demands
have been satisﬁed. Our deﬁnition here makes it easier to introduce (in the next
section) the decision of how much demand we should satisfy.
We could purchase new assets at a ﬁxed price pp and sell them at a ﬁxed price
ps. The amount we earn between t−1 and t, including the decision we make at
time t, is then given by
Ct(xt) = ps min{Rt,
ˆDt} −ppxt.
Our inventory Rt is described using the equation
Rt+1 = Rt −min{Rt,
ˆDt} + xt.
We assume that any unsatisﬁed demands are lost to the system.

38
some illustrative models
This problem can be solved using Bellman’s equation. For this problem, Rt is
our state variable. Let Vt(Rt) be the value of being in state Rt. Then Bellman’s
equation tells us that
Vt(Rt) = max
xt

Ct(xt) + γ EVt+1(Rt+1)

.
where the expectation is over all the possible realizations of the demands ˆDt+1.
2.2.6
The Asset Acquisition Problem II
Many asset acquisition problems introduce additional sources of uncertainty. The
assets we are acquiring could be stocks, planes, energy commodities such as oil,
consumer goods, and blood. In addition to the need to satisfy random demands (the
only source of uncertainty we considered in our basic asset acquisition problem),
we might have randomness in the prices at which we buy and sell assets. We
may also include exogenous changes to the assets on hand due to additions (cash
deposits, blood donations, energy discoveries) and subtractions (cash withdrawals,
equipment failures, theft of product).
We can model the problem using
xp
t = assets purchased (acquired) at time t to be used during time interval
t + 1,
xs
t = amount of assets sold to satisfy demands during time interval t,
xt = (xp
t , xs
t ),
Rt = resource level at time t before any decisions are made,
Dt = demands waiting to be served at time t.
Of course, we are going to require that xs
t ≤min{Rt, Dt} (we cannot sell what we
do not have, and we cannot sell more than the market demand). We are also going
to assume that we buy and sell our assets at market prices that ﬂuctuate over time.
These are described using
pp
t = market price for purchasing assets at time t,
ps
t = market price for selling assets at time t,
pt = (ps
t , pp
t ).
Our system evolves according to several types of exogenous information processes
that include random changes to the supplies (assets on hand), demands, and prices.
We model these using
ˆRt = exogenous changes to the assets on hand that occur during time interval t,
ˆDt = demand for the resources during time interval t,
ˆpp
t = change in the purchase price that occurs between t −1 and t,
ˆps
t = change in the selling price that occurs between t −1 and t,
ˆpt = ( ˆpp
t , ˆps
t ).

stochastic problems
39
We assume that the exogenous changes to assets, ˆRt, occurs before we satisfy
demands.
For more complex problems such as this, it is convenient to have a generic
variable for exogenous information. We use the notation Wt to represent all the
information that ﬁrst arrives between t−1 and t, where for this problem, we would
have
Wt = ( ˆRt,
ˆDt, ˆpt).
The state of our system is described by
St = (Rt, Dt, pt).
We represent the evolution of our state variable generically using
St+1 = SM(St, xt, Wt+1).
Some communities refer to this as the “system model,” hence our notation. We
refer to this as the transition function. More speciﬁcally, the equations that make
up our transition function would be
Rt+1 = Rt −xs
t + xp
t + ˆRt+1,
Dt+1 = Dt −xs
t + ˆDt+1,
pp
t+1 = pp
t + ˆpp
t+1,
ps
t+1 = ps
t + ˆps
t+1.
The one-period contribution function is
Ct(St, xt) = ps
t xs
t −pp
t xt.
We can ﬁnd optimal decisions by solving Bellman’s equation
Vt(St) = max

Ct(St, xt) + γ EVt+1(SM
t+1(St, xt, Wt+1))|St

.
(2.19)
This problem allows us to capture a number of dimensions of the modeling of
stochastic problems. This is a fairly classical problem, but we have stated it in a
more general way by allowing for unsatisﬁed demands to be held for the future,
and by allowing for random purchasing and selling prices.
2.2.7
The Lagged Asset Acquisition Problem
A variation of the basic asset acquisition problem we introduced in Section 2.2.5
arises when we can purchase assets now to be used in the future. For example, a
hotel might book rooms at time t for a date t′ in the future. A travel agent might
purchase space on a ﬂight or a cruise line at various points in time before the trip
actually happens. An airline might purchase contracts to buy fuel in the future. In

40
some illustrative models
all these cases the assets purchased farther in advance will generally be cheaper,
although prices may ﬂuctuate. For this problem we are going to assume that selling
prices are
xtt′ = assets purchased at time t to be used to satisfy demands that become
known during time interval between t′ −1 and t′,
xt = (xt,t+1, xt,t+2, . . . , ),
= (xtt′)t′ > t,
ˆDt = demand for the resources that become known during time interval t,
Rtt′ = total assets acquired on or before time t that may be used to satisfy
demands that become known between t′ −1 and t′,
Rt = (Rtt′)t′≥t.
Now, Rtt is the resources on hand in period t that can be used to satisfy demands
ˆDt that become known during time interval t. In this formulation we do not allow
xtt, which would represent purchases on the spot market. If this were allowed,
purchases at time t could be used to satisfy unsatisﬁed demands arising during
time interval between t−1 and t.
The transition function is given by
Rt+1,t′ =

Rt,t −min(Rt,t,
ˆDt)

+ xt,t+1 + Rt,t+1,
t′ = t + 1,
Rtt′ + xtt′,
t′ > t + 1.
The one-period contribution function (measuring forward in time) is
Ct(Rt,
ˆDt) = ps min(Rt,t,
ˆDt) −

t′ > t
ppxtt′.
We can again formulate Bellman’s equation as in (2.19) to determine an optimal
set of decisions. From a computational perspective, however, there is a critical
difference. Now xt and Rt are vectors with elements xtt′ and Rtt′, which makes it
computationally impossible to enumerate all possible states (or actions).
2.2.8
The Batch Replenishment Problem
One of the classical problems in operations research is one that we refer to here as
the batch replenishment problem. To illustrate the basic problem, assume that we
have a single type of resource that is consumed over time. As the reserves of the
resource run low, it is necessary to replenish the resources. In many problems there
are economies of scale in this process. It is cheaper (on an average cost basis) to
increase the level of resources in one jump (see examples).
■
EXAMPLE 2.1
A startup company has to maintain adequate reserves of operating capital to
fund product development and marketing. As the cash is depleted, the ﬁnance

stochastic problems
41
ofﬁcer has to go to the markets to raise additional capital. There are ﬁxed costs
of raising capital, so this tends to be done in batches.
■
■
EXAMPLE 2.2
An oil company maintains an aggregate level of oil reserves. As these
are depleted, it will undertake exploration expeditions to identify new oil
ﬁelds, which will produce jumps in the total reserves under the company’s
control.
■
To introduce the core elements, let
ˆDt = demand for the resources during time interval t,
Rt = resource level at time t,
xt = additional resources acquired at time t to be used during time interval
t + 1.
The transition function is given by
RM
t+1(Rt, xt,
ˆDt+1) = max{0, (Rt + xt −ˆDt+1)}.
Our one period cost function (which we wish to minimize) is given by
ˆCt+1(Rt, xt,
ˆDt+1) = total cost of acquiring xt units of the resource
= cf I{xt > 0} + cpxt + chRM
t+1(Rt, xt,
ˆDt+1),
where
cf = ﬁxed cost of placing an order,
cp = unit purchase cost,
ch = unit holding cost.
For our purposes, ˆCt+1(Rt, xt,
ˆDt+1) could be any nonconvex function; this is
a simple example of one. Since the cost function is nonconvex, it helps to order
larger quantities at the same time.
Suppose that we have a family of decision functions Xπ(Rt), π ∈, for deter-
mining xt. For example, we might use a decision rule such as
Xπ(Rt) =

0
if Rt ≥s,
Q −Rt
if Rt < q.
where Q and q are speciﬁed parameters. In the language of dynamic programming,
a decision rule such as Xπ(Rt) is known as a policy (literally, a rule for making
decisions). We index policies by π, and denote the set of policies by . In this
example a combination (S, s) represents a policy, and  would represent all the
possible values of Q and q.
Our goal is to solve
min
π∈ E
 T

t=0
γ t ˆCt+1(Rt, Xπ(Rt),
ˆDt+1)

.

42
some illustrative models
This means that we want to search over all possible values of Q and q to ﬁnd the
best performance (on average).
The basic batch replenishment problem, where Rt and xt are scalars, is quite
easy (if we know things like the distribution of demand). But there are many real
problems where these are vectors because there are different types of resources.
The vectors may be small (different types of fuel, raising different types of funds)
or extremely large (hiring different types of people for a consulting ﬁrm or the
military; maintaining spare parts inventories). Even a small number of dimensions
would produce a very large problem using a discrete representation.
2.2.9
The Transformer Replacement Problem
The electric power industry uses equipment known as transformers to convert the
high-voltage electricity that comes out of power-generating plants into currents
with successively lower voltage, ﬁnally delivering the current we can use in our
homes and businesses. The largest of these transformers can weigh 200 tons, might
cost millions of dollars to replace, and may require a year or more to build and
deliver. Failure rates are difﬁcult to estimate (the most powerful transformers were
ﬁrst installed in the 1960s and have yet to reach the end of their natural lifetime).
Actual failures can be very difﬁcult to predict, as they often depend on heat, power
surges, and the level of use.
We are going to build an aggregate replacement model where we only capture
the age of the transformers. Let
r = age of a transformer (in units of time periods) at time t,
Rtr = number of active transformers of age r at time t.
Here and elsewhere, we need to model the attributes of a resource (in this case, the
age). While “a” might be the obvious notation, this conﬂicts with our notation for
actions. Instead, we use “r” for the attributes of a resource, which can be a scalar
or, in other applications, a vector.
For our model we assume that age is the best predictor of the probability that a
transformer will fail. Let
ˆRtr = number of transformers of age r that fail between t −1 and t,
pr = probability a transformer of age r will fail between t −1 and t.
Of course, ˆRtr depends on Rtr since transformers can only fail if we own them.
It can take a year or two to acquire a new transformer. Assume that we are
measuring time, and therefore age, in fractions of a year (e.g., three months).
Normally it can take about six time periods from the time of purchase before a
transformer is installed in the network. However, we may pay extra and get a new
transformer in as little as three quarters. If we purchase a transformer that arrives
in six time periods, then we might say that we have acquired a transformer that is
r = −6 time periods old. Paying extra gets us a transformer that is r = −3 time

stochastic problems
43
periods old. Of course, the transformer is not productive until it is at least r = 0
time periods old. Let
xtr = number of transformers of age r that we purchase at time t,
cr = cost of purchasing a transformer of age r.
If we have too few transformers, then we incur what are known as “congestion
costs,” which represent the cost of purchasing power from more expensive utilities
because of bottlenecks in the network. To capture this, let
R = target number of transformers that we should have available,
RA
t = actual number of transformers that are available at time t,
=

r≥0
Rtr,
Ct(RA
t , R) = expected congestion costs if RA
t transformers are available,
= c0

R/RA
t
β .
The function Ct(RA
t , R) captures the behavior that as RA
t falls belowR, the con-
gestion costs rise quickly.
Assume that xtr is determined immediately after Rtr is measured. The transition
function is given by
Rt+1,r = Rt,r−1 + xt,r−1 −ˆRt+1,r.
Let Rt, ˆRt, and xt be vectors with components Rtr, ˆRtr, and xtr, respectively. We
can write our system dynamics more generally as
Rt+1 = RM(Rt, xt,
ˆRt+1).
If we let Vt(Rt) be the value of having a set of transformers with an age distribution
described by Rt, then, as previously, we can write this value using Bellman’s
equation
Vt(Rt) = min
xt

cxt + EVt+1(RM(Rt, xt,
ˆRt+1))

.
For this application, our state variable Rt might have as many as 100 dimensions.
If we have, say, 200 transformers, each of which might be as many as 100 years
old, then the number of possible values of Rt could be 100200. Fortunately we can
develop continuous approximations that allow us to approximate problems such as
this relatively easily.
2.2.10
The Dynamic Assignment Problem
Consider the challenge of managing a group of technicians that help with the
installation of expensive medical devices (e.g., medical imaging equipment). As
hospitals install this equipment, they need technical assistance with getting the

44
some illustrative models
machines running and training hospital personnel. The technicians may have dif-
ferent skills (some may be trained to handle speciﬁc types of equipment), and as
they travel around the country, we may want to keep track not only of their current
location but also how long they have been on the road. We describe the attributes
of a technician using
rt =


r1
r2
r3

=


Location of the technician
Type of equipment the technician is trained to handle
Number of days the technician has been on the road

.
Since we have more than one technician, we can model the set of all technicians
using
Rtr = number of technicians with attribute r,
R = set of all possible values of the attribute vector r,
Rt = (Rtr)r∈R.
Over time, demands arise for technical services as the equipment is installed. Let
b = characteristics of a piece of equipment (location, type of equipment),
B = set of all possible values of the vector b,
ˆDtb = number of new pieces of equipment of type b that were installed between
t −1 and t( and now need service),
ˆDt = ( ˆDtb)b∈B,
Dtb = total number of pieces of equipment of type b that still need to be
installed at time t,
Dt = (Dtb)b∈B.
We next have to model the decisions that we have to make. Assume that at any
point in time, we can either assign a technician to handle a new installation or send
the technician home. Let
DH = set of decisions to send a technician home, where d ∈DH represents a
particular location;
DD = set of decisions to have a technician serve a demand, where d ∈D
represents a decision to serve a demand of type bd;
dφ = decision to “do nothing” with a technician;
D = DH ∪DD ∪dφ.
A decision has the effect of changing the attributes of a technician, as well as
possibly satisfying a demand. The impact on the resource attribute vector of a

stochastic problems
45
technician is captured using the resource attribute transition function, represented
using
rt+1 = rM(at, d).
For algebraic purposes, it is useful to deﬁne the indicator function
δr′(rt, d) =

1
for rM(rt, d) = r′,
0
otherwise.
A decision d ∈DD means that we are serving a piece of equipment described by
an attribute vector bd. This is only possible, of course, if Dtb > 0. Typically Dtb
will be 0 or 1, although our model will allow multiple pieces of equipment with
the same attributes. We indicate which decisions we have made using
xtrd = number of times we apply a decision of type d to a technician with
attribute r,
xt = (xtrd)r∈R,d∈D.
Similarly we deﬁne the cost of a decision to be
ctrd = cost of applying a decision of type d to a technician with attribute r,
ct = (ctrd)r∈R,d∈D.
We could solve this problem myopically by making what appears to be the best
decisions now, ignoring their impact on the future. We would do this by solving
min
xt

r∈A

d∈D
ctrdxtrd
(2.20)
subject to

d∈D
xtrd = Rtr,
(2.21)

r∈R
xtad ≤Dtbd ,
d ∈DD,
(2.22)
xtrd ≥0.
(2.23)
Equation (2.21) says that we have to either send a technician home or assign him
to a job. Equation (2.22) says that we can only assign a technician to a job of type
bd if there is in fact a job of type bd. Said differently, we cannot assign more than
one technician to a job. But we do not have to assign a technician to every job (we
may not have enough technicians).
The problem posed by equations (2.20) to (2.23) is a linear program. Real
problems may involve managing hundreds or even thousands of individual entities.
The decision vector xt = (xtrd)r∈R,d∈D may have over ten thousand dimensions.

46
some illustrative models
But commercial linear programming packages handle problems of this size quite
easily.
If we make decisions by solving (2.20) to (2.23), we say that we are using a
myopic policy since we are using only what we know now, and we are ignoring
the impact of decisions now on the future. For example, we may decide to send a
technician home rather than have him sit in a hotel room waiting for a job. But this
ignores the likelihood that another job may suddenly arise close to the technician’s
current location. Alternatively, we may have two different technicians with two
different skill sets. If we only have one job, we might assign what appears to be
the closest technician, ignoring the fact that this technician has specialized skills
that are best reserved for difﬁcult jobs.
Given a decision vector, the dynamics of our system can be described using
Rt+1,r =

r′∈R

d∈D
xtr′dδr(r′, d),
(2.24)
Dt+1,bd = Dt,bd −

r∈R
xtrd + ˆDt+1,bd,
d ∈DD.
(2.25)
Equation (2.24) captures the effect of all decisions (including serving demands) on
the attributes of a technician. This is easiest to visualize if we assume that all tasks
are completed within one time period. If this is not the case, then we simply have
to augment the state vector to capture the attribute that we have partially completed
a task. Equation (2.25) subtracts from the list of available demands any of type bd
that are served by a decision d ∈DD (recall that each element of DD corresponds
to a type of task, which we denote bd).
The state of our system is given by
St = (Rt, Dt).
The evolution of our state variable over time is determined by equations (2.24) and
(2.24). We can now set up an optimality recursion to determine the decisions that
minimize costs over time using
Vt = min
xt∈Xt (Ct(St, xt) + γ EVt+1(St+1)) ,
where St+1 is the state at time t + 1 given that we are in state St and action xt.
St+1 is random because at time t, we do not know ˆDt+1. The feasible region Xt is
deﬁned by equations (2.21) to (2.23).
Needless to say, the state variable for this problem is quite large. The dimension-
ality of Rt is determined by the number of attributes of our technician, while the
dimensionality of Dt is determined by the relevant attributes of a demand. In real
applications these attributes can become fairly detailed. Fortunately the methods of
approximate dynamic programming can handle these complex problems.

information acquisition problems
47
2.3
INFORMATION ACQUISITION PROBLEMS
Information acquisition is an important problem in many applications where we
face uncertainty about the value of an action, but the only way to obtain better
estimates of the value is to take the action. For example, a baseball manager may
not know how well a particular player will perform at the plate. The only way to
ﬁnd out is to put him in the lineup and let him hit. The only way a mutual fund
can learn how well a manager will perform may be to let her manage a portion
of the portfolio. A pharmaceutical company does not know how the market will
respond to a particular pricing strategy. The only way to learn is to offer the drug
at different prices in test markets.
Information acquisition plays a particularly important role in approximate
dynamic programming. Assume that a system is in state i and that a particular
action might bring the system to state j. We may know the contribution of this
decision, but we do not know the value of being in state j (although we may have
an estimate). The only way to learn is to try making the decision and then obtain
a better estimate of being in state j by actually visiting the state. This process of
approximating a value function, which we introduce in Chapter 4, is fundamental
to approximate dynamic programming. For this reason the information acquisition
problem is of special importance, even in the context of solving classical dynamic
programs.
The information acquisition problem introduces a new dimension to our thinking
about dynamic programs. In all the examples we have considered in this chapter, the
state of our system is the state of the resources we are managing. In the information
acquisition problem, our state variable has to also include our estimates of unknown
parameters. We illustrate this idea in the examples that follow.
2.3.1
The Bandit Problem
The classic information acquisition problem is known as the bandit problem. Con-
sider the situation faced by a gambler trying to choose which of K slot machines
to play. Now assume that the probability of winning may be different for each
machine, but the gambler does not know what these probabilities are. The only
way to obtain information is to actually play a slot machine. To formulate this
problem, let
xn
k =

1
if we choose to play the kth slot machine in the nth trial,
0
otherwise.
W n
k = winnings from playing the kth slot machine during the nth trial.
wn
k = our estimate of the expected winnings from playing the kth slot machine
after the nth trial.

48
some illustrative models
(s2
k)n = our estimate of the variance of the winnings from playing the kth slot
machine after the nth trial.
Nn
k = number of times we have played the kth slot machine after n trials.
If xn
k = 1, then we observe W n
k and can update wn
k and (s2
k)n using
wn
k =

1 −1
Nn
k

wn−1
k
+ 1
N n
k
W n
k ,
(2.26)
(s2
k)n = Nn
k −2
Nn
k −1(s2
k)n−1 + 1
Nn
k
(W n
k −wn−1
k
)2.
(2.27)
Equations (2.26) and (2.27) are equivalent to computing averages and variances
from sample observations. Let Nn
k be the iterations where xn
k = 1. Then we can
write the mean and variance using
wn
k = 1
Nn
k

m∈Nn
k
W m
k ,
(s2
k)n =
1
Nn
k −1

m∈Nn
k
(W m
k −wm
k )2.
The recursive equations (2.26) and (2.27), make the transition from state
(wn−1
k
, (s2
k)n−1) to (wn
k, (s2
k)n) more transparent.
All other estimates remain unchanged. For this system the “state” of our system
(measured after the nth trial) is
Sn = (wn
k, (s2
k)n, Nn
k )K
k=1.
We have to keep track of Nn
k for each bandit k since these are needed in the
updating equations.
It is useful to think of Sn as our “state of knowledge” since it literally captures
what we know about the system (given by wn
k) and how well we know it (given by
(s2
k)n). Some authors call this the information state, or the “hyperstate.” Equations
(2.26) and (2.27) represent the transition equations that govern how the state evolves
over time. Note that the state variable has 2K elements, which are also continuous
(if the winnings are integer, Sn takes on discrete outcomes, but this is of little
practical value).
This is a pure information acquisition problem. Normally we would choose to
play the machine with the highest expected winnings. To express this we would
write
xn = arg max
k
wn−1
k
.
Here “arg max” means the value of the decision variable (in this case k) that
maximizes the problem. We set xn equal to the value of k that corresponds to the
largest value of wn−1
k
(ties are broken arbitrarily). This rule ignores the possibility
that our estimate of the expected winnings for a particular machine might be wrong.

information acquisition problems
49
Since we use these estimates to help us make better decisions, we might want to
try a machine where the current estimated winnings are lower, because we might
obtain information that indicates that the true mean might actually be higher.
The problem can be solved, in theory, using Bellman’s equation
V n(Sn) = max
x
E

k
W n+1
k
xk + V n+1(Sn+1)|Sn

(2.28)
subject to 
k xk = 1. Equation (2.28) is hard to compute since the state variable
is continuous. Also we do not know the distribution of the random variables W n+1
(if we knew the distributions, then we would know the expected reward for each
bandit). However, the estimates of the mean and variance, speciﬁed by the state
Sn, allow us to infer the distribution of possible means (and variances) of the true
distribution.
There are numerous examples of information acquisition problems. The
examples provide some additional illustrations of bandit problems. These problems
can be solved optimally using something known as an index policy. In the case of
the bandit problem, it is possible to compute a single index for each bandit. The
best decision is to then choose the bandit with the highest index. The value of
this result is that we can solve a K-dimensional problem as K one-dimensional
problems. See Chapter 12 for more on this topic.
■
EXAMPLE 2.3
Consider someone who has just moved to a new city and who now has to ﬁnd
the best path to work. Let Tp be a random variable giving the time he will
experience if he chooses path p from a predeﬁned set of paths P. The only
way he can obtain observations of the travel time is to actually travel the path.
Of course, he would like to choose the path with the shortest average time, but
it may be necessary to try a longer path because it may be that he simply has
a poor estimate. The problem is identical to our bandit problem if we assume
that driving one path does not teach us anything about a different path (this is
a richer form of bandit problem).
■
■
EXAMPLE 2.4
A baseball manager is trying to decide which of four players makes the best
designated hitter. The only way to estimate how well they hit is to put them
in the batting order as the designated hitter.
■
■
EXAMPLE 2.5
A couple that has acquired some assets over time is looking to ﬁnd a good
money manager who will give them a good return without too much risk.
The only way to determine how well a money manager performs is to have
him actually manage the money for a period of time. The challenge then is

50
some illustrative models
determining whether to stay with the money manager or to switch to a new
money manager.
■
■
EXAMPLE 2.6
A doctor is trying to determine the best blood pressure medication for a patient.
Each patient responds differently to each medication, so it is necessary to try
a particular medication for a while, and then switch if the doctor feels that
better results can be achieved with a different medication.
■
2.3.2
An Information-Collecting Shortest Path Problem
Now suppose that we have to choose a path through a network, just as we did
in Sections 2.1.1 and 2.2.2, but this time we face the problem that we not only
do not know the actual travel time on any of the links of the network, we do
not even know the mean or variance (we might be willing to assume that the
probability distribution is normal). As with the two previous examples, we solve
the problem repeatedly, and sometimes we want to try new paths just to collect
more information.
There are two signiﬁcant differences between this simple problem and the two
previous problems. First, imagine that you are at a node i and you are trying to
decide whether to follow the link from i to j1 or from i to j2. We have an estimate
of the time to get from j1 and j2 to the ﬁnal destination. These estimates may be
correlated because they may share common links to the destination. Following the
path from j1 to the destination may teach us something about the time to get from
j2 to the destination (if the two paths share common links). The second difference
is that making the decision to go from node i to node j changes the set of options
that we face. In the bandit problem we always faced the same set of slot machines.
Information-collecting shortest path problems arise in any information collection
problem where the decision now affects not only the information you collect but
also the decisions you can make in the future. While we can solve basic bandit
problems optimally, this broader problem class remains unsolved.
2.4
A SIMPLE MODELING FRAMEWORK
FOR DYNAMIC PROGRAMS
Now that we have covered a number of simple examples, it is useful to brieﬂy
review the elements of a dynamic program. We are going to revisit this topic
in considerably greater depth in Chapter 5, but this discussion provides a brief
introduction. Our presentation focuses on stochastic dynamic programs that exhibit
a ﬂow of uncertain information. These problems, at a minimum, consist of the
following elements:
State variable. This captures all the information we need to make a decision,
as well as the information that we need to describe how the system evolves
over time.
Decision variable. Decisions/actions represent how we control the process.

a simple modeling framework for dynamic programs
51
Exogenous information. This is data that ﬁrst become known each time period
(e.g., the demand for product, or the price at which it can be purchased or
sold). In addition we have to be told the initial state of our system.
Transition function. This function determines how the system evolves from
the state St to the state St+1 given the decision that was made at time t
and the new information that arrived between t and t + 1.
Objective function. This function speciﬁes the costs being minimized, or the
contributions/rewards being maximized, over a time horizon.
We can illustrate these elements using the simple asset acquisition problem from
Section 2.2.6.
The state variable is the information we need to make a decision and compute
functions that determine how the system evolves into the future. In our asset acqui-
sition problem, we need three pieces of information. The ﬁrst is Rt, the resources
on hand before we make any decisions (including how much of the demand to
satisfy). The second is the demand itself, denoted Dt, and the third is the price pt.
We would write our state variable as St = (Rt, Dt, pt).
We have two decisions to make. The ﬁrst, denoted xD
t , is how much of the
demand Dt during time interval t that should be satisﬁed using available assets,
which means that we require xD
t
≤Rt. The second, denoted xO
t , is how many new
assets should be acquired at time t, which can be used to satisfy demands during
time interval t + 1.
The exogenous information process consists of three types of information. The
ﬁrst is the new demands that arise during time interval t, denoted ˆDt. The second
is the change in the price at which we can sell our assets, denoted ˆpt. Finally,
we are going to assume that there may be exogenous changes to our available
resources. These might be blood donations or cash deposits (producing positive
changes), or equipment failures and cash withdrawals (producing negative changes).
We denote these changes by ˆRt. We often use a generic variable Wt to represent
all the new information that is ﬁrst learned during time interval t, which for our
problem would be written Wt = ( ˆRt, ˆDt, ˆpt). In addition to specifying the types of
exogenous information, for stochastic models we also have to specify the likelihood
of a particular outcome. This might come in the form of an assumed probability
distribution for ˆRt,
ˆDt, and ˆpt, or we may depend on an exogenous source for
sample realizations (the actual price of the stock or the actual travel time on a path).
Once we have determined what action we are going to take from our decision
rule, we compute our contribution Ct(St, at), which might depend on our current
state and the action at (or xt if we have continuous or vector-valued decisions)
that we take at time t. For our asset acquisition problem (where the state variable
is Rt) the contribution function is
Ct(St, at) = ptaD
t −ctaO
t .
In this particular model Ct(St, at) is a deterministic function of the state and action.
In other applications the contribution from action at depends on what happens
during time t + 1.

52
some illustrative models
Next we have to specify how the state variable changes over time. This is done
using a transition function, which we might represent in a generic way as
St+1 = SM(St, at, Wt+1),
where St is the state at time t, at is the decision we made at time t, and Wt+1
is our generic notation for the information that arrives between t and t + 1. We
use the notation SM(·) to denote the transition function, where the superscript M
stands for “model” (or “system model” in recognition of vocabulary that has been
in place for many years in the engineering community). The transition function for
our asset acquisition problem is given by
Rt+1 = Rt −aD
t + aO
t + ˆRt+1,
Dt+1 = Dt −aD
t + ˆDt+1,
pt+1 = pt + ˆpt+1.
This model assumes that unsatisﬁed demands are held until the next time period.
Our ﬁnal step in formulating a dynamic program is to specify the objective
function. Suppose that we are trying to maximize the total contribution received
over a ﬁnite horizon t = (0, 1, . . . , T). If we were solving a deterministic problem,
we might formulate the objective function as
max
(at)T
t=0
T

t=0
Ct(St, at).
(2.29)
We would have to optimize (2.29) subject to a variety of constraints on the actions
(a0, a1, . . . , aT ).
If we have a stochastic problem, which is to say that there are a number of
possible realizations of the exogenous information process (Wt)T
t=0, then we have
to formulate the objective function in a different way. If the exogenous information
process is uncertain, we do not know which state we will be in at time t. Since the
state St is a random variable, then the choice of decision (which depends on the
state) is also a random variable.
We get around this problem by formulating the objective in terms of ﬁnding the
best policy (or decision rule) for choosing decisions. A policy tells us what to do
for all possible states, so regardless of which state we ﬁnd ourselves in at some
time t, the policy will tell us what decision to make. This policy must be chosen
to produce the best expected contribution over all outcomes. If we let Aπ(St) be
a particular decision rule indexed by π, and let  be a set of decision rules, then
the problem of ﬁnding the best policy would be written
max
π∈ E
T

t=0
Ct(St, Aπ(St)).
(2.30)
Exactly what is meant by ﬁnding the best policy out of a set of policies is very
problem speciﬁc. Our decision rule might be to order Aπ(Rt) = S −Rt if Rt < s

a simple modeling framework for dynamic programs
53
and order Aπ(Rt) = 0 if Rt ≥s. The family of policies is the set of all values of
the parameters (s, S) for s < S (here s and S are parameters to be determined, not
state variables). If we are selling an asset, we might adopt a policy of selling if the
price of the asset pt falls below some value p. The set of all policies is the set of
all values of p. However, policies of this sort tend to work only for very special
problems.
Equation (2.30) states our problem as one of ﬁnding the best policy (or decision
rule, or function Xπ) to maximize the expected value of the total contribution
over our horizon. There are a number of variations of this objective function. For
applications where the horizon is long enough to affect the time value of money,
we might introduce a discount factor γ and solve
max
π∈ E
T

t=0
γ tCt(St, Aπ(St)).
(2.31)
There is also considerable interest in inﬁnite horizon problems of the form
max
π∈ E
∞

t=0
γ tCt(St, Aπ(St)).
(2.32)
Equation (2.32) is often used when we want to study the behavior of a system in
steady state.
Equations such as (2.30), (2.31), and (2.32) are all easy to write on a sheet of
paper. Solving them computationally is a different matter. That challenge is the
focus of this book.
A complete speciﬁcation of a dynamic program requires that we specify both
data and functions, as follows:
Data.
• The initial state S0.
• The exogenous information process Wt. We need to know what information
is arriving from outside the system, and how it is being generated. For
example, we might be given the probability of an outcome, or given a
process that creates the data for us.
Functions.
• The contribution function C(St, at). This may be speciﬁed in the form
C(St, at, Wt+1). We may need to compute the expectation, or we may
work directly with this form of the contribution function.
• The transition function SM(St, at, Wt+1).
• The family of decision functions (Aπ(S))π∈.
This description provides only a taste of the richness of sequential decision
processes. Chapter 5 describes the different elements of a dynamic program in far
greater detail.

54
some illustrative models
2.5
BIBLIOGRAPHIC NOTES
Most of the problems in this chapter are fairly classic, in particular the deterministic
and stochastic shortest path problems (see Bertsekas et al. (1991)), asset acquisition
problem (e.g., see Porteus, 1990) and the batch replenishment problem (e.g., see
Puterman 2005).
Section 2.1.1 The shortest path problem is one of the most widely studied
problems in optimization. One of the early treatments of shortest paths is
given in the seminal book on network ﬂows by Ford and Fulkerson (1962).
It has long been recognized that shortest paths could be solved directly (if
inefﬁciently) using Bellman’s equation.
Section 2.2.2 Many problems in discrete stochastic dynamic programming can
at least conceptually be formulated as some form of stochastic shortest path
problem. There is an extensive literature on stochastic shortest paths (e.g.,
see Frank 1969; Sigal et al., 1980; Frieze and Grimmet, 1985; Andreatta and
Romeo, 1988; Psaraftis and Tsitsiklis, 1993; Bertsekas et al., 1991).
Section 2.2.10 The dynamic assignment problem is based on Spivey and Powell,
(2004).
Section 2.3.1 Bandit problems have long been studied as classic exercises in
information collection. For good introductions to this material, see Ross
(1983) and Whittle (1982). A more detailed discussion of bandit problems is
given in Chapter 12.
PROBLEMS
2.1
Give an example of a sequential decision process from your own experience.
Describe the elements of your problem following the framework provided
in Section 2.4. Then describe the types of rules you might use to make a
decision.
2.2
What is the state variable at each node in the decision tree in Figure 2.3?
2.3
Describe the gambling problem in Section 2.2.3 as a decision tree, assuming
that we can gamble only 0, 1, or 2 dollars in each round (this is just to keep
the decision tree from growing too large).
2.4
Repeat the gambling problem assuming that the value of ending up with SN
dollars is
√
SN.
2.5
Write out the steps of a shortest path algorithm, similar to that shown in
Figure 2.2, that starts at the destination and works backward to the origin.
2.6
Carry out the proof by induction described at the end of Section 2.1.3.
2.7
Repeat the derivation in Section 2.1.3 assuming that the reward for task t is
ct
√xt.
2.8
Repeat the derivation in Section 2.1.3 assuming that the reward for task t is
given by ln(x).

problems
55
2.9
Repeat the derivation in Section 2.1.3 one more time, but now assume that
all you know is that the reward is continuously differentiable, monotonically
increasing and concave.
2.10
What happens to the answer to the budget allocation problem in Section
2.1.3 if the contribution is convex instead of concave (e.g., Ct(xt) = x2
t )?
2.11
Consider three variations of a shortest path problem:
Case I. All costs are known in advance. Here we assume that we have a
real-time network tracking system that allows us to see the cost on each
link of the network before we start our trip. We also assume that the costs
do not change during the time when we start the trip to when we arrive
at the link.
Case II. Costs are learned as the trip progresses. In this case we assume that
we see the actual link costs for links out of node i when we arrive at
node i.
Case III. Costs are learned after the fact. In this setting we only learn the
cost on each link after the trip is ﬁnished.
Let vI
i be the expected cost to get from node i to the destination for case I.
Similarly let vII
i
and vIII
i
be the expected costs for cases II and III. Show
that vI
i ≤vII
i
≤vIII
i
.
2.12
We are now going to do a budgeting problem where the reward function
does not have any particular properties. It may have jumps, as well as being
a mixture of convex and concave functions. But this time we will assume
that R = $30 dollars and that the allocations xt must be in integers between
0 and 30. Assume that we have T = 5 products, with a contribution function
Ct(xt) = cf (xt) where c = (c1, . . . , c5) = (3, 1, 4, 2, 5) and where f (x)
is given by
f (x) =



0,
x ≤5,
5,
x = 6,
7,
x = 7,
10,
x = 8,
12,
x ≥9.
Find the optimal allocation of resources over the ﬁve products.
2.13
You suddenly realize toward the end of the semester that you have three
courses that have assigned a term project instead of a ﬁnal exam. You quickly
estimate how much each one will take to get 100 points (equivalent to an
A+) on the project. You then guess that if you invest t hours in a project,
which you estimated would need T hours to get 100 points, then for t < T
your score will be
R = 100
	
t/T .
That is, there are declining marginal returns to putting more work into a
project. So, if a project is projected to take 40 hours and you only invest 10,

56
some illustrative models
you estimate that your score will be 50 points (100 times the square root
of 10 over 40). You decide that you cannot spend more than a total of 30
hours on the projects, and you want to choose a value of t for each project
that is a multiple of 5 hours. You also feel that you need to spend at least 5
hours on each project (that is, you cannot completely ignore a project). The
time you estimate to get full score on each of the four projects is given by
Project
Completion time T
1
20
2
15
3
10
You decide to solve the problem as a dynamic program.
(a) What is the state variable and decision epoch for this problem?
(b) What is your reward function?
(c) Write out the problem as an optimization problem.
(d) Set up the optimality equations.
(e) Solve the optimality equations to ﬁnd the right time investment strategy.
2.14
Rewrite the transition function for the asset acquisition problem II (Section
2.2.6), assuming that Rt is the resources on hand after we satisfy the
demands.
2.15
Write out the transition equations for the lagged asset acquisition problem
in Section 2.2.7 when we allow spot purchases, which means that we may
have xtt > 0. xtt refers to purchases that are made at time t which can be
used to serve unsatisﬁed demands Dt that occur during time interval t.
2.16
You have to send a set of questionnaires to each of N population segments.
The size of each population segment is given by wi. You have a budget of
B questionnaires to allocate among the population segments. If you send xi
questionnaires to segment i, you will have a sampling error proportional to
f (xi) =
1
√xi
.
You want to minimize the weighted sum of sampling errors, given by
F(x) =
N

i=1
wif (xi)
You wish to ﬁnd the allocation x that minimizes F(x) subject to the budget
constraint N
i=1 xi ≤B. Set up the optimality equations to solve this prob-
lem as a dynamic program (needless to say, we are only interested in integer
solutions).
2.17
Identify three examples of problems where you have to try an action to learn
about the reward for an action.

C H A P T E R
3
Introduction to Markov
Decision Processes
There is a very elegant theory for solving stochastic, dynamic programs if we are
willing to live within some fairly limiting assumptions. Assume that we have a
discrete state space S = (1, 2, . . . , |S|), where S is small enough to enumerate.
Next assume that there is a relatively small set of decisions or actions, which we
denote by a ∈A, and that we can compute a cost (if minimizing) or contribution
(if maximizing) given by C(s, a). Finally, assume that we are given a transition
matrix pt(St+1|St, at) that gives the probability that if we are in state St (at time
t) and take action at, then we will next be in state St+1.
From time to time we are going to switch gears to consider problems where the
decision is a vector. When this happens, we will use x as our decision variable.
However, there are many applications where the number of actions is discrete and
small, and there are many algorithms that are speciﬁcally designed for small action
spaces. In particular, the material in this chapter is designed for small action spaces,
and as a result we use a for action throughout.
There are many problems where states are continuous, or the state variable is a
vector producing a state space that is far too large to enumerate. As a result, the
one-step transition matrix pt(St+1|St, at) can be difﬁcult or impossible to compute.
So why cover material that is widely acknowledged to work only on small or
highly specialized problems? First, some problems have small state and action
spaces and can be solved with these techniques. Second, the theory of Markov
decision processes can be used to identify structural properties that can dramatically
simplify computational algorithms. But far more important, this material provides
the intellectual foundation for the types of algorithms that we present in later
chapters. Using the framework in this chapter, we can prove very powerful results
that will provide a guiding hand as we step into richer and more complex problems
Approximate Dynamic Programming: Solving the Curses of Dimensionality, Second Edition.
Warren B. Powell.
2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
57

58
introduction to markov decision processes
in many real-world settings. Furthermore the behavior of these algorithms provide
critical insights on the behavior of algorithms for more general problems.
There is a rich and elegant theory behind Markov decision processes. Even if the
algorithms have limited application, the ideas behind these algorithms, which enjoy
a rich history, represent the fundamental underpinnings of most of the algorithms
in the remainder of this book. As with most of the chapters in the book, the body
of this chapter focuses on the algorithms, and the convergence proofs have been
deferred to the “Why does it work” section (Section 3.10). The intent is to allow
the presentation of results to ﬂow more naturally, but serious students of dynamic
programming are encouraged to delve into these proofs, which are quite elegant.
This is partly to develop a deeper appreciation of the properties of the problem as
well as to develop an understanding of the proof techniques that are used in this
ﬁeld.
3.1
THE OPTIMALITY EQUATIONS
In the last chapter we illustrated a number of stochastic applications that involve
solving the following objective function:
max
π
Eπ
 T

t= 0
γ tCπ
t (St, Aπ
t (St))

.
(3.1)
We sometimes wrote our objective function using E . . ., and in other cases we wrote
it as Eπ . . .. We defer until Chapter 5 a more complete discussion of this choice,
but in a nutshell it has to do with whether the decisions we make can inﬂuence the
information we observe. If this is the case, the notation Eπ is more general.
For most problems, solving equation (3.1) is computationally intractable, but it
provides the basis for identifying the properties of optimal solutions and ﬁnding
and comparing “good” solutions.
3.1.1
Bellman’s Equations
With a little thought, we realize that we do not have to solve this entire problem at
once. Suppose that we are solving a deterministic shortest path problem where St is
the index of the node in the network where we have to make a decision. If we are
in state St = i (i.e., we are at node i in our network) and take action at = j (i.e.,
we wish to traverse the link from i to j), our transition function will tell us that we
are going to land in some state St+1 = SM(St, at) (in this case, node j). What if
we had a function Vt+1(St+1) that told us the value of being in state St+1 (giving
us the value of the path from node j to the destination)? We could evaluate each
possible action at and simply choose the action at that has the largest one-period
contribution, Ct(St, at), plus the value of landing in state St+1 = SM(St, at), which
we represent using Vt+1(St+1). Since this value represents the money we receive

the optimality equations
59
one time period in the future, we might discount this by a factor γ . In other words,
we have to solve
a∗
t (St) = arg max
at ∈At

Ct(St, at) + γ Vt+1(St+1)

,
where “arg max” means that we want to choose the action at that maximizes the
expression in parentheses. We also note that St+1 is a function of St and at, meaning
that we could write it as St+1(St, at). Both forms are ﬁne. It is common to write
St+1 alone, but the dependence on St and at needs to be understood.
The value of being in state St is the value of using the optimal decision a∗
t (St).
That is,
Vt(St) = max
at∈At

Ct(St, at) + γ Vt+1(St+1(St, at))

= Ct(St, a∗
t (St)) + γ Vt+1(St+1(St, a∗
t (St))).
(3.2)
Equation (3.2) is the optimality equation for deterministic problems.
When we are solving stochastic problems, we have to model the fact that new
information becomes available after we make the decision at. The result can be
uncertainty both in the contribution earned and in the determination of the next
state we visit, St+1. For example, consider the problem of managing oil inventories
for a reﬁnery. Let the state St be the inventory in thousands of barrels of oil at
time t (we require St to be integer). Let at be the amount of oil ordered at time
t that will be available for use between t and t+1, and let ˆDt+1 be the demand
for oil between t and t+1. The state variable is governed by the simple inventory
equation
St+1(St, at, ˆDt+1) = max{0, St + at −ˆDt+1}.
We have written the state St+1 using St+1(St, at) to express the dependence on
St and at, but it is common to simply write St+1 and let the dependence on St
and at be implicit. Since ˆDt+1 is random at time t when we have to choose at,
we do not know St+1. But if we know the probability distribution of the demand
ˆD, we can work out the probability that St+1 will take on a particular value. If
PD(d) = P[ ˆD = d] is our probability distribution, then we can ﬁnd the probability
distribution for St+1 using
Prob(St+1 = s′) =



0
if s′ > St + at,
PD(St + at −s′)
if 0 < s′ ≤St + at,
∞
d=St+at PD(d)
if s′ = 0.
These probabilities depend on St and at, so we write the probability distribution
as
P(St+1|St, at) = probability of St+1 given St and at.

60
introduction to markov decision processes
We can then modify the deterministic optimality equation in (3.2) by simply adding
an expectation, giving us
Vt(St) = max
at ∈At

Ct(St, at) + γ

s′∈S
P(St+1 = s′|St, at)Vt+1(s′)

.
(3.3)
We refer to this as the standard form of Bellman’s equations, since this is the ver-
sion that is used by virtually every textbook on stochastic, dynamic programming.
An equivalent form that is more natural for approximate dynamic programming is
to write
Vt(St) = max
at∈At

Ct(St, at) + γ E{Vt+1(St+1(St, at, Wt+1))|St}

,
(3.4)
where we simply use an expectation instead of summing over probabilities. We
refer to this equation as the expectation form of Bellman’s equation. This version
forms the basis for our algorithmic work in later chapters.
Remark
Equation (3.4) is often written in the slightly more compact form
Vt(St) = max
at∈At

Ct(St, at) + γ E{Vt+1(St+1)|St}

,
(3.5)
where the functional relationship St+1 = SM(St, at, Wt+1) is implicit. At this point,
however, we have to deal with some subtleties of mathematical notation. In equation
(3.4) we have captured the functional dependence of St+1 on St and at, while
capturing the conditional dependence of St+1 (more speciﬁcally Wt+1) on the state
St,
Vt(St) = max
at∈At

Ct(St, at) + γ E{Vt+1(St+1)|St, at}

(3.6)
to capture the fact that St+1 may depend on at. However, it is important to
understand whether St+1 is functionally dependent on St and at, or whether the
distribution of St+1 is probabilistically dependent on St and at. To see the differ-
ence, imagine that we have a problem where Wt+1 is the wind or some exogenous
process whose outcomes are independent of St or at. Then it is perfectly valid to
write
Vt(St) = max
at∈At

Ct(St, at) + γ EVt+1(St+1 = SM(St, at, Wt+1))

,
where we are explicitly capturing the functional dependence of St+1 on St and at,
but where the expectation is not conditioned on anything, because the distribution
of Wt+1 does not depend on St or at. However, there are problems where Wt+1
depends on the state, such as the random perturbations of a robot that depend on
how close the robot is to a boundary. In this case St+1 depends functionally on
St, at and Wt+1, but the distribution of Wt+1 also depends on St, in which case the
expectation needs to be a conditional expectation. Then there are problems where
the distribution of Wt+1 depends on both St and at, such as the random changes

the optimality equations
61
in a stock that might be depressed if a mutual fund is holding large quantities (St)
and begins selling in large amounts (at). In this case the proper interpretation of
equation (3.6) is that we are computing the conditional expectation over Wt+1 that
now depends on both the state and action.
The standard form of Bellman’s equation (3.3) has been popular in the research
community since it lends itself to elegant algebraic manipulation when we assume
we know the transition matrix. It is common to write it in a more compact form.
Recall that a policy π is a rule that speciﬁes the action at given the state St. In this
chapter it is easiest if we always think of a policy in terms of a rule “when we are in
state s we take action a.” This is a form of “lookup table” representation of a policy
that is very clumsy for most real problems, but it will serve our purposes here. The
probability that we transition from state St = s to St+1 = s′ can be written as
pss′(a) = P(St+1 = s′|St = s, at = a).
We would say that “pss′(a) is the probability that we end up in state s′ if we start
in state s at time t when we are taking action a.” Now say that we have a function
Aπ
t (s) that determines the action a we should take when in state s. It is common
to write the transition probability pss′(a) in the form
pπ
ss′ = P(St+1 = s′|St = s, Aπ
t (s) = a).
We can write this in matrix form
P π
t = one-step transition matrix under policy π,
where pπ
ss′ is the element in row s and column s′. There is a different matrix P π
for each policy (decision rule) π.
Now let cπ
t be a column vector with element cπ
t (s) = Ct(s, Aπ
t (s)), and let vt+1
be a column vector with element Vt+1(s). Then (3.3) is equivalent to


...
vt(s)
...

= max
π




...
cπ
t (s)
...

+ γ


...
pπ
ss′
...




...
vt+1(s′)
...



,
(3.7)
where the maximization is performed for each element (state) in the vector. In
matrix/vector form, equation (3.7) can be written
vt = max
π

cπ
t + γ P π
t vt+1

.
(3.8)
Here we maximize over policies because we want to ﬁnd the best action for each
state. The vector vt is known widely as the value function (the value of being in
each state). In control theory it is known as the cost-to-go function, where it is
typically denoted as J .

62
introduction to markov decision processes
Equation (3.8) can be solved by ﬁnding at for each state s. The result is a
decision vector a∗
t = (a∗
t (s))s∈S, which is equivalent to determining the best policy.
This is easiest to envision when at is a scalar (how much to buy, whether to sell),
but in many applications at(s) is itself a vector. For example, suppose that our
problem is to assign individual programmers to different programming tasks, where
our state St captures the availability of programmers and the different tasks that
need to be completed. Of course, computing a vector at for each state St, which
is itself a vector, is much easier to write than to implement.
It is very easy to lose sight of the relationship between Bellman’s equation and
the original objective function that we stated in equation (3.1). To bring this out,
we begin by writing the expected proﬁts using policy π from time t onward:
F π
t (St) = E
T −1

t′= t
Ct′(St′, Aπ
t′(St′)) + CT (ST )|St

.
F π
t (St) is the expected total contribution if we are in state St in time t, and follow
policy π from time t onward. If F π
t (St) were easy to calculate, we would probably
not need dynamic programming. Instead, it seems much more natural to calculate
V π
t
recursively using
V π
t (St) = Ct(St, Aπ
t (St)) + E
'
V π
t+1(St+1)|St
(
.
It is not hard to show (by stepping backward in time) that
F π
t (St) = V π
t (St).
The proof, given in Section 3.10.1, uses a proof by induction: assume that it is true
for V π
t+1, and then show that it is true for V π
t
(not surprisingly, inductive proofs
are very popular in dynamic programming).
With this result in hand we can then establish the following key result. Let
Vt(St) be a solution to equation (3.4) (or (3.3)). Then
F ∗
t = max
π∈ F π
t (St)
= Vt(St).
(3.9)
Equation (3.9) establishes the equivalence between (1) the value of being in state
St and following the optimal policy and (2) the optimal value function at state
St. While these are indeed equivalent, the equivalence is the result of a theorem
(established in Section 3.10.1). However, it is not unusual to ﬁnd people who lose
sight of the original objective function. Later we have to solve these equations
approximately, and we will need to use the original objective function to evaluate
the quality of a solution.

the optimality equations
63
3.1.2
Computing the Transition Matrix
It is very common in stochastic, dynamic programming (more precisely, Markov
decision processes) to assume that the one-step transition matrix P π is given as
data (remember that there is a different matrix for each policy π). In practice, we
generally can assume that we know the transition function SM(St, at, Wt+1) from
which we have to derive the one-step transition matrix.
Assume that the random information Wt+1 that arrives between t and t+1 is
independent of all prior information. Let t+1 be the set of possible outcomes of
Wt+1 (for simplicity, we assume that t+1 is discrete), where P(Wt+1 = ωt+1) is
the probability of outcome ωt+1 ∈t+1. Also deﬁne the indicator function
1{X} =

1
if the statement “X” is true,
0
otherwise.
Here “X” represents a logical condition (e.g., “is St = 6?”). We now observe that
the one-step transition probability Pt(St+1|St, at) can be written
Pt(St+1|St, at) = E1{s′=SM(St,at,Wt+1)}
=

ωt+1∈t+1
P(ωt+1)1{s′=SM(St,at,ωt+1)}
So ﬁnding the one-step transition matrix means that all we have to do is to sum
over all possible outcomes of the information Wt+1 and add up the probabilities
that take us from a particular state-action pair (St, at) to a particular state St+1 = s′.
Sounds easy.
In some cases this calculation is straightforward (consider our oil inventory
example earlier in the section). But in other cases this calculation is impossible.
For example, Wt+1 might be a vector of prices or demands. Such a set of outcomes
t+1 can be much too large to enumerate. We can estimate the transition matrix
statistically, but in later chapters (starting in Chapter 4) we are going to avoid the
need to compute the one-step transition matrix entirely. For the remainder of this
chapter, we assume that the one-step transition matrix is available.
3.1.3
Random Contributions
In many applications the one-period contribution function is a deterministic func-
tion of St and at, and hence we routinely write the contribution as the deterministic
function Ct(St, at). However, this is not always the case. For example, a car trav-
eling over a stochastic network may choose to traverse the link from node i to
node j, and only learn the cost of the movement after making the decision. For
such cases the contribution function is random, and we might write it as
ˆCt+1(St, at, Wt+1) = contribution received in period t + 1 given the state St and
decision at, as well as the new information Wt+1 that
arrives in period t + 1.

64
introduction to markov decision processes
Then we simply bring the expectation in front, giving us
Vt(St) = max
at
E
)
ˆCt+1(St, at, Wt+1) + γ Vt+1(St+1)|St
*
.
(3.10)
Now let
Ct(St, at) = E{ ˆCt+1(St, at, Wt+1)|St}.
Thus we may view Ct(St, at) as the expected contribution given that we are in
state St and take action at.
3.1.4
Bellman’s Equation Using Operator Notation*
The vector form of Bellman’s equation in (3.8) can be written even more compactly
using operator notation. Let M be the “max” (or “min”) operator in (3.8) that can
be viewed as acting on the vector vt+1 to produce the vector vt. If we have a given
policy π, we can write
Mπv(s) = Ct(s, Aπ(s)) + γ

s′∈S
Pt(s′|s, Aπ(s))vt+1(s′).
Alternatively, we can ﬁnd the best action, which we represent using
Mv(s) = max
a

Ct(s, a) + γ

s′∈S
Pt(s′|s, a)vt+1(s′)

.
Here Mv produces a vector, and Mv(s) refers to element s of this vector. In
vector form we would write
Mv = max
π

cπ
t + γ P π
t vt+1

.
Now let V be the space of value functions. Then M is a mapping
M : V →V.
We may also deﬁne the operator Mπ for a particular policy π using
Mπ(v) = cπ
t + γ P πv
(3.11)
for some vector v ∈V. Mπ is known as a linear operator since the operations
that it performs on v are additive and multiplicative. In mathematics the function
cπ
t + γ P πv is known as an afﬁne function. This notation is particularly useful in
mathematical proofs (see in particular some of the proofs in Section 3.10), but we
will not use this notation when we describe models and algorithms.
We see later in the chapter that we can exploit the properties of this operator
to derive some very elegant results for Markov decision processes. These proofs
provide insights into the behavior of these systems, which can guide the design of
algorithms. For this reason it is relatively immaterial that the actual computation
of these equations may be intractable for many problems; the insights still apply.

ﬁnite horizon problems
65
3.2
FINITE HORIZON PROBLEMS
Finite horizon problems tend to arise in two settings. First, some problems have
a very speciﬁc horizon. For example, we might be interested in the value of an
American option where we are allowed to sell an asset at any time t ≤T, where
T is the exercise date. Another problem is to determine how many seats to sell at
different prices for a particular ﬂight departing at some point in the future. In the
same class are problems that require reaching some goal (but not at a particular
point in time). Examples include driving to a destination, selling a house, or winning
a game.
A second class of problems is actually inﬁnite horizon, but where the goal is to
determine what to do right now given a particular state of the system. For example,
a transportation company might want to know what drivers should be assigned to
a particular set of loads right now. Of course, these decisions need to consider the
downstream impact, so models have to extend into the future. For this reason we
might model the problem over a horizon T that, when solved, yields a decision of
what to do right now.
When we encounter a ﬁnite horizon problem, we assume that we are given
the function VT (ST ) as data. Often we simply use VT (ST ) = 0 because we are
primarily interested in what to do now, given by a0, or in projected activities
over some horizon t = 0, 1, . . . , T ph, where T ph is the length of a planning
horizon. If we set T sufﬁciently larger than T ph, then we may be able to
assume that the decisions a0, a1, . . . , aT ph are of sufﬁciently high quality to be
useful.
Solving a ﬁnite horizon problem, in principle, is straightforward. As outlined
in Figure 3.1, we simply have to start at the last time period, compute the value
function for each possible state s ∈S, and then step back another time period.
This way at time period t we have already computed Vt+1(S). Not surprisingly,
this method is often referred to as “backward dynamic programming.” The critical
element that attracts so much attention is the requirement that we compute the
value function Vt(St) for all states St ∈S.
Step 0. Initialization:
Initialize the terminal contribution VT (ST ).
Set t = T−1.
Step 1. Calculate:
Vt(St) = max
at


Ct(St, at) + γ

s′∈S
P(s′|St, at)Vt+1(s′)



for all St ∈S.
Step 2. If t>0, decrement t and return to step 1. Else stop.
Figure 3.1
Backward dynamic programming algorithm.

66
introduction to markov decision processes
We ﬁrst saw backward dynamic programming in Section 2.2.1 when we
described a simple decision tree problem. The only difference between the
backward dynamic programming algorithm in Figure 3.1 and our solution of the
decision tree problem is primarily notational. Decision trees are visual and tend to
be easier to understand, whereas in this section the methods are described using
notation. However, decision tree problems tend to be always presented in the
context of problems with relatively small numbers of states and actions (What job
should I take? Should the United States put a blockade around Cuba? Should the
shuttle launch have been canceled due to cold weather?).
Another popular illustration of dynamic programming is the discrete asset acqui-
sition problem. Say that you order a quantity at at each time period to be used in
the next time period to satisfy a demand ˆDt+1. Any unused product is held over to
the following time period. For this, our state variable St is the quantity of inventory
left over at the end of the period after demands are satisﬁed. The transition equation
is given by St+1 = [St + at −ˆDt+1]+ where [x]+ = max(x, 0). The cost function
(which we seek to minimize) is given by ˆCt+1(St, at) = chSt + coI{at > 0}, where
I{X} = 1 if X is true and 0 otherwise. Note that the cost function is nonconvex.
This does not create problems if we solve our minimization problem by searching
over different (discrete) values of at. Since all of our quantities are scalar, there is
no difﬁculty ﬁnding Ct(St, at).
To compute the one-step transition matrix, let  be the set of possible outcomes
of ˆDt, and let P( ˆDt = ω) be the probability that ˆDt = ω (if this use of ω seems
weird, get used to it—we are going to use it a lot).
The one-step transition matrix is computed using
P(s′|s, a) =

ω∈
P( ˆDt+1 = ω)1{s′=[s+a−ω]+},
where  is the set of (discrete) outcomes of the demand ˆDt+1.
Another example is the shortest path problem with random arc costs. Suppose
that you are trying to get from origin node q to destination node r in the shortest
time possible. As you reach each intermediate node i, you are able to observe the
time required to traverse each arc out of node i. Let Vj be the expected shortest
path time from j to the destination node r. At node i, you see the link time
ˆτij, which represents a random observation of the travel time. Now we choose
to traverse arc (i,j ∗), where j∗solves minj(ˆτij + Vj) (j∗is random since the
travel time is random). We would then compute the value of being at node i using
Vi = E{minj(ˆτij + Vj)}.
3.3
INFINITE HORIZON PROBLEMS
We typically use inﬁnite horizon formulations whenever we wish to study a prob-
lem where the parameters of the contribution function, transition function, and
the process governing the exogenous information process do not vary over time,
although they may vary in cycles (e.g., an inﬁnite horizon model of energy storage

inﬁnite horizon problems
67
from a solar panel may depend on time of day). Often we wish to study such
problems in steady state. More importantly, inﬁnite horizon problems provide a
number of insights into the properties of problems and algorithms, drawing off an
elegant theory that has evolved around this problem class. Even students who wish
to solve complex, nonstationary problems will beneﬁt from an understanding of
this problem class.
We begin with the optimality equations
Vt(St) = max
at∈A E {Ct(St, at) + γ Vt+1(St+1)|St} .
We can think of a steady-state problem as one without the time dimension. Letting
V (s) = limt→∞Vt(St) (and assuming the limit exists), we obtain the steady-state
optimality equations
V (s) = max
a∈A

C(s, a) + γ

s′∈S
P(s′|s, a)V (s′)

.
(3.12)
The functions V (s) can be shown (as we do later) to be equivalent to solving the
inﬁnite horizon problem
max
π∈ E
 ∞

t= 0
γ tCt(St, Aπ
t (St))

.
(3.13)
Now deﬁne
P π,t = t-step transition matrix, over periods 0, 1, . . . , t −1, given policy π
= t−1
t′= 0P π
t′ .
(3.14)
We further deﬁne P π,0 to be the identity matrix. As before, let cπ
t be the column
vector of the expected cost of being in each state given that we choose the action
at described by policy π, where the element for state s is cπ
t (s) = Ct(s, Aπ(s)).
The inﬁnite horizon, discounted value of a policy π starting at time t is given by
vπ
t =
∞

t′= t
γ t′−tP π,t′−tcπ
t′.
(3.15)
Assume that after following policy π0 we follow policy π1 = π2 = . . . = π. In this
case equation (3.15) can now be written as (starting at t = 0)
vπ0 = cπ0 +
∞

t′=1
γ t′P π,t′cπ
t′
(3.16)
= cπ0 +
∞

t′=1
γ t′ 
t′−1
t′′= 0P π
t′′

cπ
t′
(3.17)

68
introduction to markov decision processes
= cπ0 + γ P π0
∞

t′= 1
γ t′−1 
t′−1
t′′= 1P π
t′′

cπ
t′
(3.18)
= cπ0 + γ P π0vπ.
(3.19)
Equation (3.19) shows us that the value of a policy is the single period reward
plus a discounted terminal reward that is the same as the value of a policy starting
at time 1. If our decision rule is stationary, then π0 = π1 = . . . = πt = π, which
allows us to rewrite (3.19) as
vπ = cπ + γ P πvπ.
(3.20)
This allows us to solve for the stationary reward explicitly (as long as 0 ≤γ <
1), giving us
vπ =

I −γ P π−1 cπ.
We can also write an inﬁnite horizon version of the optimality equations using
our operator notation. Letting M be the “max” (or “min”) operator (also known
as the Bellman operator), the inﬁnite horizon version of equation (3.11) would be
written
Mπ(v) = cπ + γ P πv.
(3.21)
There are several algorithmic strategies for solving inﬁnite horizon problems.
The ﬁrst, value iteration, is the most widely used method. It involves iteratively
estimating the value function. At each iteration the estimate of the value function
determines which decisions we will make and as a result deﬁnes a policy. The
second strategy is policy iteration. At every iteration we deﬁne a policy (literally,
the rule for determining decisions) and then determine the value function for that
policy. Careful examination of value and policy iteration reveals that these are
closely related strategies that can be viewed as special cases of a general strategy
that uses value and policy iteration. Finally, the third major algorithmic strategy
exploits the observation that the value function can be viewed as the solution to a
specially structured linear programming problem.
3.4
VALUE ITERATION
Value iteration is perhaps the most widely used algorithm in dynamic programming
because it is the simplest to implement and, as a result, often tends to be the
most natural way of solving many problems. It is virtually identical to backward
dynamic programming for ﬁnite horizon problems. In addition most of our work
in approximate dynamic programming is based on value iteration.
Value iteration comes in several ﬂavors. The basic version of the value iteration
algorithm is given in Figure 3.2. The proof of convergence (see Section 3.10.2) is

value iteration
69
Step 0. Initialization:
Set v0(s) = 0 ∀s ∈S.
Fix a tolerance parameter ϵ > 0.
Set n = 1.
Step 1. For each s ∈S compute:
vn(s) = max
a∈A

C(s, a) + γ

s′∈S
P(s′|s, a)vn−1(s′)

.
(3.22)
Step 2. If ∥vn −vn−1∥< ϵ(1 −γ )/2γ , let πϵ be the resulting policy that solves (3.22),
and let vϵ = vn and stop; else set n = n+1 and go to step 1.
Figure 3.2
Value iteration algorithm for inﬁnite horizon optimization.
quite elegant for students who enjoy mathematics. The algorithm also has several
nice properties that we explore below.
It is easy to see that the value iteration algorithm is similar to the backward
dynamic programming algorithm. Rather than using a subscript t, which we decre-
ment from T back to 0, we use an iteration counter n that starts at 0 and increases
until we satisfy a convergence criterion. We stop the algorithm when
∥vn −vn−1∥< ϵ(1 −γ )/2γ,
where ∥v∥is the max-norm deﬁned by
∥v∥= max
s
|v(s)|.
Thus ∥v∥is the largest absolute value of a vector of elements. We stop if the largest
change in the value of being in any state is less than ϵ(1 −γ )/2γ , where ϵ is a
speciﬁed error tolerance.
Below we describe a Gauss–Seidel variant that is a useful method for acceler-
ating value iteration, and a version known as relative value iteration.
3.4.1
A Gauss–Seidel Variation
A slight variant of the value iteration algorithm provides a faster rate of conver-
gence. In this version (typically called the Gauss–Seidel variant), we take advantage
of the fact that when we are computing the expectation of the value of the future,
we have to loop over all the states s′ to compute 
s′ P(s′|s, a)vn(s′). For a par-
ticular state s, we would have already computed vn+1(ˆs) for ˆs = 1, 2, . . . , s −1.
By simply replacing vn(ˆs) with vn+1(ˆs) for the states we have already visited,
we obtain an algorithm that typically exhibits a noticeably faster rate of conver-
gence. The algorithm requires a change to step 1 of the value iteration, as shown
in Figure 3.3.

70
introduction to markov decision processes
Replace step 1 with
Step 1′. For each s ∈S compute
vn(s) = max
a∈A


C(s, a) + γ


s′<s
P(s′|s, a)vn(s′) +

s′≥s
P(s′|s, a)vn−1(s′)





Figure 3.3
Gauss–Seidel variation of value iteration.
3.4.2
Relative Value Iteration
Another version of value iteration is called relative value iteration; it is useful in
problems that do not have a discount factor or where the optimal policy converges
much more quickly than the value function, which may grow steadily for many
iterations. The relative value iteration algorithm is shown in Figure 3.4.
In relative value iteration we focus on the fact that we may be more interested
in the convergence of the difference ∥v(s) −v(s′)∥than we are in the values of
v(s) and v(s′). This would be the case if we are interested in the best policy rather
than the value function (this is not always the case). What often happens is that,
especially toward the limit, all the values v(s) start increasing by the same rate.
For this reason we can pick any state (denoted s∗in the algorithm) and subtract its
value from all the other states.
To provide a bit of formalism for our algorithm, we deﬁne the span of a vector
v as follows:
sp(v) = max
s∈S v(s) −min
s∈S v(s).
Note that our use of “span” is different than the way it is normally used in linear
algebra. Here and throughout this section, we deﬁne the norm of a vector as
∥v∥= max
s∈S v(s).
Step 0. Initialization:
• Choose some v0 ∈V.
• Choose a base state s∗and a tolerance ϵ.
• Let w0 = v0 −v0(s∗)e where e is a vector of ones.
• Set n = 1.
Step 1. Set
vn = Mwn−1,
wn = vn −vn(s∗)e.
Step 2. If sp(vn −vn−1) < (1 −γ )ϵ/γ , go to step 3; otherwise, go to step 1.
Step 3. Set aϵ = arg max
a∈A

C(a) + γ P πvn
.
Figure 3.4
Relative value iteration.

value iteration
71
Note that the span has the following six properties:
1. sp(v) ≥0.
2. sp(u + v) ≤sp(u) + sp(v).
3. sp(kv) = ∥k∥sp(v).
4. sp(v + ke) = sp(v).
5. sp(v) = sp(−v).
6. sp(v) ≤2∥v∥.
Property 4 implies that sp(v) = 0 does not mean that v = 0, and therefore it does
not satisfy the properties of a norm. For this reason it is called a semi-norm.
The relative value iteration algorithm is simply subtracting a constant from the
value vector at each iteration. Obviously this does not change the optimal decision,
but it does change the value. If we are only interested in the optimal policy, relative
value iteration often offers much faster convergence, but it may not yield accurate
estimates of the value of being in each state.
3.4.3
Bounds and Rates of Convergence
One important property of value iteration algorithms is that if our initial estimate
is too low, the algorithm will rise to the correct value from below. Similarly, if
our initial estimate is too high, the algorithm will approach the correct value from
above. This property is formalized in the following theorem:
Theorem 3.4.1
For a vector v ∈V:
a. if v satisﬁes v ≥Mv, then v ≥v∗.
b. if v satisﬁes v ≤Mv, then v ≤v∗.
c. if v satisﬁes v = Mv, then v is the unique solution to this system of equations
and v = v∗.
The proof is given in Section 3.10.3. It is a nice property because it provides
some valuable information on the nature of the convergence path. In practice, we
generally do not know the true value function, which makes it hard to know if we
are starting from above or below, although some problems have natural bounds
(e.g., nonnegativity).
The proof of the monotonicity property above also provides us with a nice
corollary. If V (s) = MV (s) for all s, then V (s) is the unique solution to this
system of equations, which must also be the optimal solution.
This result raises the question: What if some of our estimates of the value of
being in some states are too high, while others are too low? This means that the
values may cycle above and below the optimal solution, although at some point
we may ﬁnd that all the values have increased (decreased) from one iteration to

72
introduction to markov decision processes
the next. If this happens, then it means that the values are all equal to or below
(above) the limiting value.
Value iteration also provides a nice bound on the quality of the solution. Recall
that when we use the value iteration algorithm, we stop when
∥vn+1 −vn∥< ϵ(1 −γ )
2γ
,
(3.23)
where γ is our discount factor and ϵ is a speciﬁed error tolerance. It is possible that
we have found the optimal policy when we stop, but it is very unlikely that we have
found the optimal value functions. We can, however, provide a bound on the gap
between the solution vn and the optimal values v∗by using the following theorem:
Theorem 3.4.2
If we apply the value iteration algorithm with stopping parameter
ϵ and the algorithm terminates at iteration n with value function vn+1, then
∥vn+1 −v∗∥≤ϵ
2.
(3.24)
Let πϵ be the policy that we terminate with, and let vπϵ be the value of this policy.
Then
∥vπϵ −v∗∥≤ϵ.
The proof is given in Section 3.10.4. While it is nice that we can bound the error,
the bad news is that the bound can be quite poor. More important is what the
bound teaches us about the role of the discount factor.
We can provide some additional insights into the bound, as well as the rate of
convergence, by considering a trivial dynamic program. In this problem we receive
a constant reward c at every iteration. There are no decisions, and there is no
randomness. The value of this “game” is quickly seen to be
v∗=
∞

n=0
γ nc
=
1
1 −γ c.
(3.25)
Consider what happens when we solve this problem using value iteration. Starting
with v0 = 0, we would use the iteration
vn = c + γ vn−1.
After we have repeated this n times, we have
vn =
n−1

m=0
γ nc
= 1 −γ n
1 −γ c.
(3.26)

value iteration
73
Comparing equations (3.25) and (3.26), we see that
vn −v∗= −γ n
1 −γ c.
(3.27)
Similarly the change in the value from one iteration to the next is given by
∥vn+1 −vn∥=
....
γ n+1
1 −γ −
γ n
1 −γ
.... c
= γ n
....
γ
1 −γ −
1
1 −γ
.... c
= γ n
....
γ −1
1 −γ
.... c
= γ nc.
If we stop at iteration n+1, then it means that
γ nc ≤ϵ/2
1 −γ
γ

.
(3.28)
If we choose ϵ so that (3.28) holds with equality, then our error bound (from 3.24)
is
∥vn+1 −v∗∥≤ϵ
2
= γ n+1
1 −γ c.
From (3.27) we know that the distance to the optimal solution is
|vn+1 −v∗| = γ n+1
1 −γ c,
which matches our bound.
This little exercise conﬁrms that our bound on the error may be tight. It also
shows that the error decreases geometrically at a rate determined by the discount
factor. For this problem the error arises because we are approximating an inﬁnite
sum with a ﬁnite one. For more realistic dynamic programs, we also have the effect
of trying to ﬁnd the optimal policy. When the values are close enough that we have
in fact found the optimal policy, then we have only a Markov reward process (a
Markov chain where we earn rewards for each transition). Once our Markov reward
process has reached steady state, it will behave just like the simple problem we
have just solved, where c is the expected reward from each transition.

74
introduction to markov decision processes
3.5
POLICY ITERATION
In policy iteration we choose a policy and then ﬁnd the inﬁnite horizon, discounted
value of the policy. This value is then used to choose a new policy. The general
algorithm is described in Figure 3.5. Policy iteration is popular for inﬁnite horizon
problems because of the ease with which we can ﬁnd the value of a policy. As we
showed in Section 3.3, the value of following policy π is given by
vπ =

I −γ P π−1 cπ.
(3.29)
While computing the inverse can be problematic as the state space grows, it is, at
a minimum, a very convenient formula.
It is useful to illustrate the policy iteration algorithm in different settings. In the
ﬁrst, consider a batch replenishment problem where we have to replenish resources
(raising capital, exploring for oil to expand known reserves, hiring people) where
there are economies from ordering larger quantities. We might use a simple policy
where if our level of resources Rt < q for some lower limit q, then we order a
quantity at = Q −Rt. This policy is parameterized by (q, Q) and is written
Aπ(Rt) =

0,
Rt ≥q,
Q −Rt,
Rt < q.
(3.30)
For a given set of parameters π = (q, Q), we can compute a one-step transition
matrix P π and a contribution vector cπ.
Step 0. Initialization.
Step 0a. Select a policy π0.
Step 0b. Set n = 1.
Step 1. Given a policy π n−1:
Step 1a. Compute the one-step transition matrix P πn−1.
Step 1b. Compute the contribution vector cπn−1 where the element for state s is given
by cπn−1(s) = C(s, Aπn−1).
Step 2. Let vπ,n be the solution to
(I −γ P πn−1)v = cπn−1.
Step 3. Find a policy πn deﬁned by
an(s) = arg max
a∈A

C(a) + γ P πvn
.
This requires that we compute an action for each state s.
Step 4. If an(s) = an−1(s) for all states s, then set a∗= an; otherwise, set n = n + 1 and
go to step 1.
Figure 3.5
Policy iteration.

hybrid value-policy iteration
75
Policies come in many forms. For the moment, we simply view a policy as
a rule that tells us what decision to make when we are in a particular state. In
Chapter 6, we describe four fundamental classes of policies, which can be used as
the foundation for creating an even wider range of decision rules.
Given a transition matrix P π and contribution vector cπ, we can use equation
(3.29) to ﬁnd vπ, where vπ(s) is the discounted value of started in state s and
following policy π. From this vector, we can infer a new policy by solving
an(s) = arg max
a∈A

C(a) + γ P πvn
(3.31)
for each state s. For our batch replenishment example, it turns out that we can show
that an(s) will have the same structure as that shown in (3.30). So we can either
store an(s) for each s or simply determine the parameters (q, Q) that correspond
to the decisions produced by (3.31). The complete policy iteration algorithm is
described in Figure 3.5.
The policy iteration algorithm is simple to implement and has fast convergence
when measured in terms of the number of iterations. However, solving equation
(3.29) is quite hard if the number of states is large. If the state space is small,
we can use vπ = (I −γ P π)−1cπ, but the matrix inversion can be computationally
expensive. For this reason, we may use a hybrid algorithm that combines the
features of policy iteration and value iteration.
3.6
HYBRID VALUE-POLICY ITERATION
Value iteration is basically an algorithm that updates the value at each iteration and
then determines a new policy given the new estimate of the value function. At any
iteration, the value function is not the true, steady-state value of the policy. By
contrast, policy iteration picks a policy and then determines the true, steady-state
value of being in each state given the policy. Given this value, a new policy is
chosen.
It is perhaps not surprising that policy iteration converges faster in terms of
the number of iterations because it is doing a lot more work in each iteration
(determining the true steady-state value of being in each state under a policy).
Value iteration is much faster per iteration, but it is determining a policy given an
approximation of a value function and then performing a very simple updating of
the value function, which may be far from the true value function.
A hybrid strategy that combines features of both methods is to perform a some-
what more complete update of the value function before performing an update
of the policy. Figure 3.6 outlines the procedure where the steady-state evaluation
of the value function in equation (3.29) is replaced with a much easier iterative
procedure (step 2 in Figure 3.6). This step is run for M iterations, where M is a
user-controlled parameter that allows the exploration of the value of a better esti-
mate of the value function. Not surprisingly, it will generally be the case that M
should decline with the number of iterations as the overall process converges.

76
introduction to markov decision processes
Step 0. Initialization:
• Set n = 1.
• Select a tolerance parameter ϵ and inner iteration limit M .
• Select some v0 ∈V.
Step 1. Find a decision an(s) for each s that satisﬁes
an(s) = arg max
a∈A


C(s, a) + γ

s′∈S
P(s′|s, a)vn−1(s′)


,
which we represent as policy πn.
Step 2. Partial policy evaluation:
a. Set m = 0 and let un(0) = cπ + γ P πnvn−1.
b. If ∥un(0) −vn−1∥< ϵ(1 −γ )/2γ , go to step 3.
c. Else, while m < M do the following:
i. un(m + 1) = cπn + γ P πnun(m) = Mπun(m).
ii. Set m = m + 1 and repeat step i.
d. Set vn = un(M), n = n + 1 and return to step 1.
Step 3. Set aϵ = an+1 and stop.
Figure 3.6
Hybrid value/policy iteration.
3.7
AVERAGE REWARD DYNAMIC PROGRAMMING
There are settings where the natural objective function is to maximize the average
contribution per unit time. Suppose that we start in state s. Then the average reward
from starting in state s and following policy π is given by
max
π
F π(s) = lim
T →∞
1
T E
T

t=0
C(St, Aπ(St)).
(3.32)
Here F π(s) is the expected reward per time period. In matrix form the total value
of following a policy π over a horizon T can be written as
V π
T =
T

t=0
(P π)tcπ,
where V π
T is a column vector with element V π
T (s) giving the expected contribution
over T time periods when starting in state s. We can get a sense of how V π
T (s)
behaves by watching what happens as T becomes large. Assuming that our under-
lying Markov chain is ergodic (all the states communicate with each other with
positive probability), we know that (P π)T →P ∗where the rows of P ∗are all the
same.
Now deﬁne a column vector g given by
gπ = P ∗cπ.

the linear programming method for dynamic programs
77
F (s2 |T)
F (s1 |T)
h(s2) + Tg
h(s1) + Tg
T
Figure 3.7
Cumulative contribution over a horizon T when starting in states s1 and s2, showing growth
approaching a rate that is independent of the starting state.
Since the rows of P ∗are all the same, all the elements of gπ are the same, and
each element gives the average contribution per time period using the steady state
probability of being in each state. For ﬁnite T each element of the column vector
V π
T is not the same, since the contributions we earn in the ﬁrst few time periods
depends on our starting state. But it is not hard to see that as T grows large, we
can write
V π
T →hπ + T gπ,
where hπ captures the state-dependent differences in the total contribution, while
gπ is the state-independent average contribution in the limit. Figure 3.7 illustrates
the growth in V π
T toward a linear function.
If we wish to ﬁnd the policy that performs the best as T →∞, then clearly the
contribution of hπ vanishes, and we want to focus on maximizing gπ, which we
can now treat as a scalar.
3.8
THE LINEAR PROGRAMMING METHOD FOR
DYNAMIC PROGRAMS
Theorem 3.4.1 showed us that if
v ≥max
a

C(s, a) + γ

s′∈S
P(s′|s, a)v(s′)

,
then v is an upper bound (actually, a vector of upper bounds) on the value of being
in each state. This means that the optimal solution, which satisﬁes v∗= c + γ P v∗,
is the smallest value of v that satisﬁes this inequality. We can use this insight to
formulate the problem of ﬁnding the optimal values as a linear program. Let β be

78
introduction to markov decision processes
a vector with elements βs > 0, ∀s ∈S. The optimal value function can be found by
solving the following linear program:
min
v

s∈S
βsv(s)
(3.33)
subject to
v(s) ≥C(s, a) + γ

s′∈S
P(s′|s, a)v(s′)
for all s and a.
(3.34)
The linear program has a |S|-dimensional decision vector (the value of being in
each state), with |S| × |A| inequality constraints (equation (3.34)).
This formulation was viewed as primarily a theoretical result for many years,
since it requires formulating a linear program where the number of constraints is
equal to the number of states and actions. While even today this limits the size
of problems it can solve, modern linear programming solvers can handle prob-
lems with tens of thousands of constraints without difﬁculty. This size is greatly
expanded with the use of specialized algorithmic strategies, which are an active
area of research as of this writing. The advantage of the LP method over value iter-
ation is that it avoids the need for iterative learning with the geometric convergence
exhibited by value iteration. Given the dramatic strides in the speed of linear pro-
gramming solvers over the last decade, the relative performance of value iteration
over the linear programming method is an unresolved question. However, this ques-
tion only arises for problems with relatively small state and action spaces. While a
linear program with 50,000 constraints is considered large, dynamic programs with
50,000 states and actions often arises with relatively small problems.
3.9
MONOTONE POLICIES*
One of the most dramatic success stories from the study of Markov decision pro-
cesses has been the identiﬁcation of the structure of optimal policies. A common
example of structured policies is what are known as monotone policies. Simply
stated, a monotone policy is one where the decision gets bigger as the state gets
bigger, or the decision gets smaller as the state gets bigger (see examples).
■
EXAMPLE 3.1
A software company must decide when to ship the next release of its operating
system. Let St be the total investment in the current version of the software.
Let at = 1 denote the decision to ship the release in time period t while at = 0
means to keep investing in the system. The company adopts the rule that at = 1
if St ≥S. Thus, as St gets bigger, at gets bigger (this is true even though at
is equal to zero or one).
■

monotone policies
79
■
EXAMPLE 3.2
An oil company maintains stocks of oil reserves to supply its reﬁneries for
making gasoline. A supertanker comes from the Middle East each month, and
the company can purchase different quantities from this shipment. Let Rt be
the current inventory. The policy of the company is to order at = Q −St if
St < R. R is the reorder point, and Q is the “order up to” limit. The bigger St
is, the less the company orders.
■
■
EXAMPLE 3.3
A mutual fund has to decide when to sell its holding in a company. Its policy
is to sell the stock when the price ˆpt is greater than a particular limit p.
■
In each example the decision of what to do in each state is replaced by a
function that determines the decision (otherwise known as a policy). The function
typically depends on the choice of a few parameters. So, instead of determining the
right action for each possible state, we only have to determine the parameters that
characterize the function. Interestingly we do not need dynamic programming for
this. Instead, we use dynamic programming to determine the structure of the optimal
policy. This is a purely theoretical question, so the computational limitations of
(discrete) dynamic programming are not relevant.
The study of monotone policies is included partly because it is an important
part of the ﬁeld of dynamic programming. It is also useful in the study of approxi-
mate dynamic programming because it yields properties of the value function. For
example, in the process of showing that a policy is monotone, we also need to show
that the value function is monotone (i.e., it increases or decreases with the state
variable). Such properties can be exploited in the estimation of a value function
approximation.
To demonstrate the analysis of a monotone policy, we consider a classic batch
replenishment policy that arises when there is a random accumulation that is then
released in batches. Examples include dispatching elevators or trucks, moving oil
inventories away from producing ﬁelds in tankers, and moving trainloads of grain
from grain elevators.
3.9.1
The Model
For our batch model we assume that resources accumulate and are then reduced
using a batch process. For example, oil might accumulate in tanks before a tanker
removes it. Money might accumulate in a cash account before it is swept into an
investment.
Our model uses the following parameters:
cr = ﬁxed cost incurred each time we dispatch a new batch,
ch = penalty per time period for holding a unit of the resource,
K = maximum size of a batch.

80
introduction to markov decision processes
Our exogenous information process consists of
Qt = quantity of new arrivals during time interval t,
PQ(i) = Prob(Qt = i).
Our state variable is
Rt = resources remaining at time t before we have made a decision
to send a batch.
There are two decisions we have to make. The ﬁrst is whether to dispatch a batch,
and the second is how many resources to put in the batch. For this problem, once
we make the decision to send a batch, we are going to make the batch as large as
possible, so the “decision” of how large the batch should be seems unnecessary.
It becomes more important when we later consider multiple resource types. For
consistency with the more general problem with multiple resource types, we deﬁne
at =

1
if a batch is sent at time t,
0
otherwise,
bt = number of resources to put in the batch.
In theory, we might be able to put a large number of resources in the batch, but
we may face a nonlinear cost that makes this suboptimal. For the moment, we are
going to assume that we always want to put as many as we can, so we set
bt = at min{K, Rt},
Aπ(Rt) = decision function that returns at and bt given Rt.
The transition function is described using
Rt+1 = Rt −bt + Qt+1.
(3.35)
The objective function is modeled using
Ct(Rt, at, bt) = cost incurred in period t, given state Rt and dispatch decision at
= crat + ch(Rt −bt).
(3.36)
Our problem is to ﬁnd the policy Aπ
t (Rt) that solves
min
π∈ E
 T

t=0
Ct(Rt, Aπ
t (Rt))

.
(3.37)
where  is the set of policies. If we are managing a single asset class, then Rt and
bt are scalars, and the problem can be solved using standard backward dynamic
programming techniques of the sort that were presented in the chapter (provided

monotone policies
81
that we have a probability model for the demand). In practice, many problems
involve multiple asset classes, which makes standard techniques impractical. But
we can use this simple problem to study the structure of the problem.
If Rt is a scalar, and if we know the probability distribution for Qt, then we
can solve the simple asset problem using backward dynamic programming. Indeed,
this is one of the classic dynamic programming problems in operations research.
However, the solution to this problem seems obvious. We should dispatch a batch
whenever the level of resources Rt is greater than some number rt, which means
we only have to ﬁnd rt (if we have a steady-state inﬁnite horizon problem, then
we would have to ﬁnd a single parameter r). The remainder of this section helps
establish the theoretical foundation for making this argument. While not difﬁcult,
the mathematical level of this presentation is somewhat higher than our usual
presentation.
3.9.2
Submodularity and Other Stories
In the realm of optimization problems over a continuous set, it is important to
know a variety of properties about the objective function (e.g., convexity/concavity,
continuity, and boundedness). Similarly discrete problems require an understanding
of the nature of the functions we are maximizing, but there is a different set of
conditions that we need to establish.
One of the most important properties that we will need is supermodularity (sub-
modularity if we are minimizing). We are interested in studying a function g(u), u ∈
U, where U ⊆ℜn is an n-dimensional space. Consider two vectors u1, u2 ∈U where
there is no particular relationship between u1 and u2. Now deﬁne
u1 ∧u2 = min{u1, u2},
u1 ∨u2 = max{u1, u2},
where the min and max are deﬁned elementwise. Let u+ = u1 ∧u2 and u−=
u1 ∨u2. We ﬁrst have to ask the question of whether u+, u−∈U, since this is not
guaranteed. For this purpose, we deﬁne the following:
Deﬁnition 3.9.1
The space U is a lattice if for each u1, u2 ∈U, then u+ = u1 ∧
u2 ∈U and u−= u1 ∨u2 ∈U.
The term “lattice” for these sets arises if we think of u1 and u2 as the northwest
and southeast corners of a rectangle. In that conﬁguration these corners are u+ and
u−. If all four corners fall in the set (for any pair (u1, u2)), then the set can be
viewed as containing many “squares,” similar to a lattice.
For our purposes we assume that U is a lattice (if it is not, then we have to use
a more general deﬁnition of the operators “∨” and “∧”). If U is a lattice, then a
general deﬁnition of supermodularity is given by the following:

82
introduction to markov decision processes
Deﬁnition 3.9.2
A function g(u), u ∈U is supermodular if it satisﬁes
g(u1 ∧u2) + g(u1 ∨u2) ≥g(u1) + g(u2).
(3.38)
Supermodularity is the discrete analogue of a convex function. A function is sub-
modular if the inequality in equation (3.38) is reversed. There is an alternative
deﬁnition of supermodular when the function is deﬁned on sets. Let U1 and U2 be
two sets of elements, and let g be a function deﬁned on these sets. Then we have
Deﬁnition 3.9.3
A function g : U →ℜ1 is supermodular if it satisﬁes
g(U1 ∪U2) + g(U1 ∩U2) ≥g(U1) + g(U2).
(3.39)
We may refer to deﬁnition 3.9.2 as the vector deﬁnition of supermodularity, while
deﬁnition 3.9.3 as the set deﬁnition. We give both deﬁnitions for completeness, but
our work uses only the vector deﬁnition.
In dynamic programming we are interested in functions of two variables, as
in f (s, a), where s is a state variable and a is a decision variable. We want to
characterize the behavior of f (s, a) as we change s and a. If we let u = (s,
a), then we can put this in the context of our deﬁnition above. Suppose that we
have two states s+ ≥s−(again, the inequality is applied elementwise) and two
decisions a+ ≥a−. We form two vectors u1 = (s+, a−) and u2 = (s−, a+). With
this deﬁnition, we ﬁnd that u1 ∨u2 = (s+, a+) and u1 ∧u2 = (s−, a−). This gives
us the following:
Proposition 3.9.1
A function g( s, a) is supermodular if for s+ ≥s−and a+ ≥a−,
then
g(s+, a+) + g(s−, a−) ≥g(s+, a−) + g(s−, a+).
(3.40)
For our purposes equation (3.40) will be the version we will use.
A common variation on the statement of a supermodular function is the equiv-
alent condition
g(s+, a+) −g(s−, a+) ≥g(s+, a−) −g(s−, a−)
(3.41)
In this expression we are saying that the incremental change in s for larger val-
ues of a is greater than for smaller values of a. Similarly we may write the
condition as
g(s+, a+) −g(s+, a−) ≥g(s−, a+) −g(s−, a−),
(3.42)
which states that an incremental change in a increases with s.
Some examples of supermodular functions include
1. If g(s, a) = g1(s) + g2(a), meaning that it is separable, then (3.40) holds
with equality.

monotone policies
83
2. g(s, a) = h(s+a) where h(·) is convex and increasing.
3. g(s, a) = sa, s, a ∈ℜ1.
A concept that is related to supermodularity is superadditivity, deﬁned by the
following:
Deﬁnition 3.9.4
A superadditive function f : ℜn →ℜ1 satisﬁes
f (x) + f (y) ≤f (x + y).
(3.43)
Some authors use superadditivity and supermodularity interchangeably, but the
concepts are not really equivalent, and we need to use both of them.
3.9.3
From Submodularity to Monotonicity
It seems intuitively obvious that we should dispatch a batch if the state Rt (the
resources waiting to be served in a batch) is greater than some number (e.g., rt).
The dispatch rule that says we should dispatch if Rt ≥rt is known as a control limit
structure. Similarly we might be holding an asset and we feel that we should sell
it if the price pt (which is the state of our asset) is above (or perhaps below) some
number pt. A question arises: When is an optimal policy monotone? The following
theorem establishes sufﬁcient conditions for an optimal policy to be monotone.
Theorem 3.9.1
We want to maximize a total discounted contribution given that
a. Ct(R, a) is supermodular on R × A.
b. 
R′∈R P(R′|R, a)vt+1(R′) is supermodular on R × A.
Then there exists a decision rule Aπ(R) that is nondecreasing on R.
The proof of this theorem is provided in Section 3.10.6.
In the presentation that follows, we need to show submodularity (instead of
supermodularity) because we are minimizing costs rather than maximizing rewards.
It is obvious that Ct(R, a) is nondecreasing in R. So it remains to show that
Ct(R, a) satisﬁes
Ct(R+, 1) −Ct(R−, 1) ≤Ct(R+, 0) −Ct(R−, 0).
(3.44)
Substituting equation (3.36) into (3.44), we must show that
cr + ch(R+ −K)+ −cr −ch(R−−K)+ ≤chR+ −chR−.
This simpliﬁes to
(R+ −K)+ −(R−−K)+ ≤R+ −R−.
(3.45)
Since R+ ≥R−, (R+ −K)+ = 0 ⇒(R−−K)+ = 0. This implies there are three
possible cases for equation (3.45):

84
introduction to markov decision processes
Case 1.
(R+ −K)+ > 0 and (R−−K)+ > 0. Then (3.45) reduces to R+ −
R−= R+ −R−.
Case 2. (R+ −K)+ > 0 and (R−−K)+ = 0. Then, (3.45) reduces to R−≤K,
which follows since (R−−K)+ = 0 implies that R−≤K.
Case 3.
(R+ −K)+ = 0 and (R−−K)+ = 0. Then (3.45) reduces to R−≤
R+, which is true by construction.
Now we have to show submodularity of ∞
R′=0 P(R′|R, a)V (R′). We will do
this for the special case where the batch capacity is so large that it can be never
exceeded. A proof is available for the ﬁnite capacity case, but it is much more
difﬁcult.
Submodularity requires that for R−≤R+ we have
∞

R′=0
P(R′|R+, 1)V (R′) −
∞

R′=0
P(R′|R+, 0)V (R′) ≤
∞

R′=0
P(R′|R−, 1)V (R′)
−
∞

R′=0
P(R′|R−, 0)V (R′).
For the case that R−, R+ ≤K we have
∞

R′=0
PA(R′)V (R′) −
∞

R′=R+
PA(R′ −R+)V (R′) ≤
∞

R′=0
PA(R′)V (R′)
−
∞

R′=R−
PA(R′ −R−)V (R′),
which simpliﬁes to
∞

R′=0
PA(R′)V (R′) −
∞

R′=0
PA(R′)V (R′ + R+) ≤
∞

R′=0
PA(R′)V (R′)
−
∞

R′=0
PA(R′)V (R′ + R−).
Since V is nondecreasing, we have V (R′ + R+) ≥V (R′ + R−), proving the result.
3.10
WHY DOES IT WORK?**
The theory of Markov decision processes is elegant but not needed for computa-
tional work. An understanding of why Markov decision processes work will provide
a deeper appreciation of the properties of these optimization problems.

why does it work?
85
Section 3.10.1 provides a proof that the optimal value function satisﬁes the
optimality equations. Section 3.10.2 proves convergence of the value iteration algo-
rithm. Section 3.10.3 then proves conditions under which value iteration increases
or decreases monotonically to the optimal solution. Then Section 3.10.4 proves the
bound on the error when value iteration satisﬁes the termination criterion given in
Section 3.4.3. Section 3.10.5 closes with a discussion of deterministic and random-
ized policies, along with a proof that deterministic policies are always at least as
good as a randomized policy.
3.10.1
The Optimality Equations
Until now we have been presenting the optimality equations as though they were a
fundamental law of some sort. To be sure, they can easily look as though they were
intuitively obvious, but it is still important to establish the relationship between the
original optimization problem and the optimality equations. Because these equations
are the foundation of dynamic programming, we will work through the steps of
proving that they are actually true.
We start by remembering the original optimization problem:
F π
t (St) = E
T −1

t′=t
Ct′(St′, Aπ
t′(St′)) + CT (ST )|St

.
(3.46)
Since (3.46) is, in general, exceptionally difﬁcult to solve, we resort to the opti-
mality equations
V π
t (St) = Ct(St, Aπ
t (St)) + E
'
V π
t+1(St+1)|St
(
.
(3.47)
Our challenge is to show that these are the same. In order to establish this result,
it is going to help if we ﬁrst prove the following:
Lemma 3.10.1
Let St be a state variable that captures the relevant history up to
time t, and let Ft′(St+1) be some function measured at time t′ ≥t + 1 conditioned
on the random variable St+1. Then
E

E{Ft′|St+1}|St

= E [Ft′|St] .
(3.48)
Proof
This lemma is variously known as the law of iterated expectations or the
tower property. Assume, for simplicity, that Ft′ is a discrete ﬁnite random variable
that takes outcomes in F. We start by writing
E{Ft′|St+1} =

f ∈F
f P(Ft′ = f |St+1).
(3.49)
Recognizing that St+1 is a random variable, we may take the expectation of both
sides of (3.49), conditioned on St as follows:
E

E{Ft′|St+1}|St

=

St+1∈S

f ∈F
f P(Ft′ = f |St+1, St)P(St+1 = St+1|St).
(3.50)

86
introduction to markov decision processes
First, we observe that we can write P(Ft′ = f |St+1, St) = P(Ft′ = f |St+1) because
conditioning on St+1 makes all prior history irrelevant. Next we can reverse the
summations on the right-hand side of (3.50) (some technical conditions have to be
satisﬁed to do this, but these are satisﬁed if the random variables are discrete and
ﬁnite). This means that
E

E{Ft′|St+1 = St+1}|St

=

f ∈F

St+1∈S
f P(Ft′ = f |St+1, St)P(St+1 = St+1|St)
=

f ∈F
f

St+1∈S
P(Ft′ = f, St+1|St)
=

f ∈F
f P(Ft′ = f |St)
= E [Ft′|St] ,
which proves our result. Note that the essential step in the proof occurs in the ﬁrst
step when we add St to the conditioning.
□
We are now ready to show the following:
Proposition 3.10.1
F π
t (St) = V π
t (St).
Proof
To prove that (3.46) and (3.47) are equal, we use a standard trick in dynamic
programming: proof by induction. Clearly, F π
T (ST ) = V π
T (ST ) = CT (ST ). So we
assume that it holds for t + 1, t + 2, . . . , T. We want to show that the same is
true for t. This means that we can write
V π
t (St) = Ct(St, Aπ
t (St))
+ E


E



T −1

t′=t+1
Ct′(St′, Aπ
t′(St′)) + Ct(ST (ω))
......
St+1






F π
t+1(St+1)
............
St


.
We then use lemma 3.10.1 to write E

E {. . . |St+1} |St

= E [. . . |St]. Hence
V π
t (St) = Ct(St, Aπ
t (St)) + E


T −1

t′=t+1
Ct′(St′, Aπ
t′(St′)) + Ct(ST )|St

.

why does it work?
87
When we condition on St, Aπ
t (St) (and therefore Ct(St, Aπ
t (St))) is deterministic.
So we can pull the expectation out to the front, giving
V π
t (St) = E
/T −1

t′=t
Ct′(St′, yt′(St′)) + Ct(ST )|St
0
= F π
t (St),
which proves our result.
□
Using equation (3.47), we have a backward recursion for calculating V π
t (St) for
a given policy π. Now that we can ﬁnd the expected reward for a given π, we
would like to ﬁnd the best π. That is, we want to ﬁnd that
F ∗
t (St) = max
π∈ F π
t (St).
If the set  is inﬁnite, we replace the “max” with “sup.” We solve this problem
by solving the optimality equations. These are
Vt(St) = max
a∈A

Ct(St, a) +

s′∈S
pt(s′|St, a)Vt+1(s′)

.
(3.51)
We are claiming that if we ﬁnd the set of V’s that solves (3.51), then we have
found the policy that optimizes F π
t . We state this claim formally as:
Theorem 3.10.1
Let Vt(St) be a solution to equation (3.51). Then
F ∗
t = Vt(St)
= max
π∈ F π
t (St).
Proof
The proof is in two parts. First we show by induction that Vt(St) ≥F ∗
t (St)
for all St ∈S and t = 0, 1, . . . , T−1. Then we show that the reverse inequality
is true, which gives us the result.
Part 1
We resort again to our proof by induction. Since VT (ST ) = Ct(ST ) = F π
T (ST ) for
all ST and all π ∈, we get that VT (ST ) = F ∗
T (ST ).
Assume that Vt′(St′) ≥F ∗
t′(St′) for t′ = t + 1, t + 2, . . . , T, and let π be an
arbitrary policy. For t′ = t, the optimality equation tells us that
Vt(St) = max
a∈A

Ct(St, a) +

s′∈S
pt(s′|St, a)Vt+1(s′)

.

88
introduction to markov decision processes
By the induction hypothesis, F ∗
t+1(s) ≤Vt+1(s). So we get
Vt(St) ≥max
a∈A

Ct(St, a) +

s′∈S
pt(s′|St, a)F ∗
t+1(s′)

.
Of course, we have that F ∗
t+1(s) ≥F π
t+1(s) for an arbitrary π. Also let Aπ(St) be
the decision that would be chosen by policy π when in state St. Then
Vt(St) ≥max
a∈A

Ct(St, a) +

s′∈S
pt(s′|St, a)F π
t+1(s′)

≥Ct(St, Aπ(St)) +

s′∈S
pt(s′|St, Aπ(St))F π
t+1(s′)
= F π
t (St).
This means that
Vt(St) ≥F π
t (St)
for all π ∈,
which proves part 1.
Part 2
Now we are going to prove the inequality from the other side. Speciﬁcally, we
want to show that for any ϵ > 0 there exists a policy π that satisﬁes
F π
t (St) + (T −t)ϵ ≥Vt(St).
(3.52)
To do this, we start with the deﬁnition
Vt(St) = max
a∈A

Ct(St, a) +

s′∈S
pt(s′|St, a)Vt+1(s′)

.
(3.53)
We let at(St) be the decision rule that solves (3.53). This rule corresponds to the
policy π. In general, the set A may be inﬁnite, whereupon we have to replace the
“max” with a “sup” and handle the case where an optimal decision may not exist.
For this case we know that we can design a decision rule at(St) that returns a
decision a that satisﬁes
Vt(St) ≤Ct(St, a) +

s′∈S
pt(s′|St, a)Vt+1(s′) + ϵ.
(3.54)
We can prove (3.52) by induction. We ﬁrst note that (3.52) is true for t = T since
F π
T (St) = VT (ST ). Now assume that it is true for t′ = t + 1, t + 2, . . . , T. We
already know that
F π
t (St) = Ct(St, Aπ(St)) +

s′∈S
pt(s′|St, Aπ(St))F π
t+1(s′).

why does it work?
89
We can use our induction hypothesis, which says that F π
t+1(s′) ≥Vt+1(s′) −(T −
(t + 1))ϵ, to get
F π
t (St) ≥Ct(St, Aπ(St)) +

s′∈S
pt(s′|St, Aπ(St))[Vt+1(s′) −(T −(t + 1))ϵ]
= Ct(St, Aπ(St)) +

s′∈S
pt(s′|St, Aπ(St))Vt+1(s′)
−

s′∈S
pt(s′|St, Aπ(St)) [(T −t −1)ϵ]
=

Ct(St, Aπ(St)) +

s′∈S
pt(s′|St, Aπ(St))Vt+1(s′) + ϵ

−(T −t)ϵ.
Now, using equation (3.54), we replace the term in brackets with the smaller Vt(St)
(equation (3.54)):
F π
t (St) ≥Vt(St) −(T −t)ϵ,
which proves the induction hypothesis. We have shown that
F ∗
t (St) + (T −t)ϵ ≥F π
t (St) + (T −t)ϵ ≥Vt(St) ≥F ∗
t (St).
This proves the result.
□
Now we know that solving the optimality equations also gives us the optimal
value function. This is our most powerful result because we can solve the optimality
equations for many problems that cannot be solved any other way.
3.10.2
Convergence of Value Iteration
We undertake here the proof that the basic value function iteration converges to the
optimal solution. This is not only an important result; it is also an elegant one that
brings some powerful theorems into play. The proof is also quite short. However,
we will need some mathematical preliminaries:
Deﬁnition 3.10.1
Let V be a set of (bounded, real-valued) functions and deﬁne
the norm of v by
∥v∥= sup
s∈S
v(s)
where we replace the “sup” with a “max” when the state space is ﬁnite. Since
V is closed under addition and scalar multiplication and has a norm, it is a
normed linear space.

90
introduction to markov decision processes
Deﬁnition 3.10.2
T : V →V is a contraction mapping if there exists a γ ,
0≤γ <1, such that
∥T v −T u∥≤γ ∥v −u∥.
Deﬁnition 3.10.3
A sequence vn ∈V, n = 1, 2, . . ., is said to be a Cauchy
sequence if for all ϵ > 0 there exists N such that for all n, m ≥N:
∥vn −vm∥< ϵ.
Deﬁnition 3.10.4
A normed linear space is complete if every Cauchy sequence
contains a limit point in that space.
Deﬁnition 3.10.5
A Banach space is a complete normed linear space.
Deﬁnition 3.10.6
We deﬁne the norm of a matrix Q as
∥Q∥= max
s∈S

j∈S
|q(j|s)|,
that is, the largest row sum of the matrix. If Q is a one-step transition matrix, then
∥Q∥= 1.
Deﬁnition 3.10.7
The triangle inequality means that given two vectors a, b ∈ℜn:
∥a + b∥≤∥a∥+ ∥b∥.
The triangle inequality is commonly used in proofs because it helps us estab-
lish bounds between two solutions (and in particular, between a solution and the
optimum).
We now state and prove one of the famous theorems in applied mathematics
and then use it immediately to prove convergence of the value iteration algorithm.
Theorem 3.10.2
(Banach Fixed-Point Theorem) Let V be a Banach space, and let
T : V →V be a contraction mapping. Then
a. there exists a unique v∗∈V such that T v∗= v∗.
b. for an arbitrary v0 ∈V, the sequence vn deﬁned by vn+1 = T vn = T n+1v0
converges to v∗.
Proof
We start by showing that the distance between two vectors vn and vn+m
goes to zero for sufﬁciently large n and by writing the difference vn+m −vn using
vn+m −vn = vn+m −vn+m−1 + vn+m−1 −· · · −vn+1 + vn+1 −vn
=
m−1

k=0
(vn+k+1 −vn+k).

why does it work?
91
Taking norms of both sides and invoking the triangle inequality gives
∥vn+m −vn∥=
11111
m−1

k=0
(vn+k+1 −vn+k)
11111
≤
m−1

k=0
∥(vn+k+1 −vn+k)∥
=
m−1

k=0
∥(T n+kv1 −T n+kv0)∥
≤
m−1

k=0
γ n+k∥v1 −v0∥
= γ n(1 −γ m)
(1 −γ )
∥v1 −v0∥.
(3.55)
Since γ < 1, for sufﬁciently large n the right-hand side of (3.55) can be made
arbitrarily small, which means that vn is a Cauchy sequence. Since V is complete,
it must be that vn has a limit point v∗. From this we conclude that
lim
n→∞vn →v∗.
(3.56)
We now want to show that v∗is a ﬁxed point of the mapping T. To show this, we
observe that
0 ≤∥T v∗−v∗∥
(3.57)
= ∥T v∗−vn + vn −v∗∥
(3.58)
≤∥T v∗−vn∥+ ∥vn −v∗∥
(3.59)
= ∥T v∗−T vn−1∥+ ∥vn −v∗∥
(3.60)
≤γ ∥v∗−vn−1∥+ ∥vn −v∗∥.
(3.61)
Equation (3.57) comes from the properties of a norm. We play our standard trick
in (3.58) of adding and subtracting a quantity (in this case, vn), which sets up the
triangle inequality in (3.59). Using vn = T vn−1 gives us (3.60). The inequality in
(3.61) is based on the assumption of the theorem that T is a contraction mapping.
From (3.56) we know that
lim
n→∞∥v∗−vn−1∥= lim
n→∞∥vn −v∗∥= 0.
(3.62)
Combining (3.57), (3.61), and (3.62) gives
0 ≤∥T v∗−v∗∥≤0,

92
introduction to markov decision processes
from which we conclude that
∥T v∗−v∗∥= 0,
which means that T v∗= v∗.
We can prove uniqueness by contradiction. Suppose that there are two limit
points that we represent as v∗and u∗. The assumption that T is a contraction
mapping requires that
∥T v∗−T u∗∥≤γ ∥v∗−u∗∥.
But, if v∗and u∗are limit points, then T v∗= v∗and T u∗= u∗, which means
∥v∗−u∗∥≤γ ∥v∗−u∗∥.
Since γ <1, this is a contradiction, which means that it must be true that
v∗= u∗.
□
We can now show that the value iteration algorithm converges to the optimal
solution if we can establish that M is a contraction mapping. So we need to show
the following:
Proposition 3.10.2
If 0 ≤γ < 1, then M is a contraction mapping on V.
Proof
Let u, v ∈V, and assume that Mv ≥Mu where the inequality is applied
elementwise. For a particular state s let
a∗
s (v) ∈arg max
a∈A

C(s, a) + γ

s′∈S
P(s′|s, a)v(s′)

,
where we assume that a solution exists. Then
0 ≤Mv(s) −Mu(s)
(3.63)
= C(s, a∗
s (v)) + γ

s′∈S
P(s′|s, a∗
s (v))v(s′)
−

C(s, a∗
s (u)) + γ

s′∈S
P(s′|s, a∗
s (u))u(s′)

(3.64)
≤C(s, a∗
s (v)) + γ

s′∈S
P(s′|s, a∗
s (v))v(s′)
−

C(s, a∗
s (v)) + γ

s′∈S
P(s′|s, a∗
s (v))u(s′)

(3.65)

why does it work?
93
= γ

s′∈S
P(s′|s, a∗
s (v))[v(s′) −u(s′)]
(3.66)
≤γ

s′∈S
P(s′|s, a∗
s (v))∥v −u∥
(3.67)
= γ ∥v −u∥

s′∈S
P(s′|s, a∗
s (v))
(3.68)
= γ ∥v −u∥.
(3.69)
Equation (3.63) is true by assumption, while (3.64) holds by deﬁnition. The inequal-
ity in (3.65) holds because a∗
s (v) is not optimal when the value function is u, giving
a reduced value in the second set of parentheses. Equation (3.66) is a simple reduc-
tion of (3.65). Equation (3.67) forms an upper bound because the deﬁnition of
∥v −u∥is to replace all the elements [v(s) −u(s)] with the largest element of this
vector. Since this is now a vector of constants, we can pull it outside of the summa-
tion, giving us (3.68), which then easily reduces to (3.69) because the probabilities
add up to one.
This result states that if Mv(s) ≥Mu(s), then Mv(s) −Mu(s) ≤γ |v(s) −
u(s)|. If we start by assuming that Mv(s) ≤Mu(s), then the same reasoning
produces Mv(s) −Mu(s) ≥−γ |v(s) −u(s)|. This means that we have
|Mv(s) −Mu(s)| ≤γ |v(s) −u(s)|
(3.70)
for all states s ∈S. From the deﬁnition of our norm, we can write
sup
s∈S
|Mv(s) −Mu(s)| = ∥Mv −Mu∥
≤γ ∥v −u∥.
This means that M is a contraction mapping, which means that the sequence vn
generated by vn+1 = Mvn converges to a unique limit point v∗that satisﬁes the
optimality equations.
□
3.10.3
Monotonicity of Value Iteration
Inﬁnite horizon dynamic programming provides a compact way to study the the-
oretical properties of these algorithms. The insights gained here are applicable to
problems even when we cannot apply this model, or these algorithms, directly.
We assume throughout our discussion of inﬁnite horizon problems that the
reward function is bounded over the domain of the state space. This assumption
is virtually always satisﬁed in practice, but notable exceptions exist. For example,
the assumption is violated if we are maximizing a utility function that depends on
the log of the resources we have at hand (the resources may be bounded, but the
function is unbounded if the resources are allowed to hit zero).
Our ﬁrst result establishes a monotonicity property that can be exploited in the
design of an algorithm.

94
introduction to markov decision processes
Theorem 3.10.3
For a vector v ∈V,
a. if v satisﬁes v ≥Mv, then v ≥v∗.
b. if v satisﬁes v ≤Mv, then v ≤v∗.
c. if v satisﬁes v = Mv, then v is the unique solution to this system of equations
and v = v∗.
Proof
Part a requires that
v ≥max
π∈{cπ + γ P πv}
(3.71)
≥cπ0 + γ P π0v
(3.72)
≥cπ0 + γ P π0 
cπ1 + γ P π1v

(3.73)
= cπ0 + γ P π0cπ1 + γ 2P π0P π1v.
Equation (3.71) is true by assumption (part a of the theorem) and equation (3.72)
is true because π0 is some policy that is not necessarily optimal for the vector
v. By the similar reasoning, equation (3.73) is true because π1 is another policy
which, again, is not necessarily optimal. Using P π,(t) = P π0P π1 · · · P πt , we obtain
by induction
v ≥cπ0 + γ P π0cπ1 + · · · + γ t−1P π0P π1 · · · P πt−1cπt + γ tP π,(t)v.
(3.74)
Recall that
vπ =
∞

t=0
γ tP π,(t)cπt .
(3.75)
Breaking the sum in (3.75) into two parts allows us to rewrite the expansion in
(3.74) as
v ≥vπ −
∞

t′=t+1
γ t′P π,(t′)cπt′+1 + γ tP π,(t)v.
(3.76)
Taking the limit of both sides of (3.76) as t →∞gives us
v ≥lim
t→∞vπ −
∞

t′=t+1
γ t′P π,(t′)cπt′+1 + γ tP π,(t)v
(3.77)
≥vπ
∀π ∈.
(3.78)
The limit in (3.77) exists as long as the reward function cπ is bounded and γ <1.
Because (3.78) is true for all π ∈, it is also true for the optimal policy, which
means that
v ≥vπ∗
= v∗,

why does it work?
95
which proves part a of the theorem. Part b can be proved in an analogous way. Parts
a and b mean that v ≥v∗and v ≤v∗. If v = Mv, then we satisfy the preconditions
of both parts a and b, which means they are both true and therefore we must have
v = v∗.
□
This result means that if we start with a vector that is higher than the optimal
vector, then we will decline monotonically to the optimal solution (almost—we
have not quite proved that we actually get to the optimal). Alternatively, if we start
below the optimal vector, we will rise to it. Note that it is not always easy to ﬁnd
a vector v that satisﬁes either condition a or b of the theorem. In problems where
the rewards can be positive and negative, this can be tricky.
3.10.4
Bounding the Error from Value Iteration
We now want to establish a bound on our error from value iteration, which will
establish our stopping rule. We propose two bounds: one on the value function
estimate that we terminate with and one for the long-run value of the decision rule
that we terminate with. To deﬁne the latter, let πϵ be the policy that satisﬁes our
stopping rule, and let vπϵ be the inﬁnite horizon value of following policy πϵ.
Theorem 3.10.4
If we apply the value iteration algorithm with stopping parameter
ϵ and the algorithm terminates at iteration n with value function vn+1, then
∥vn+1 −v∗∥≤ϵ
2,
(3.79)
and
∥vπϵ −v∗∥≤ϵ.
(3.80)
Proof
We start by writing
∥vπϵ −v∗∥= ∥vπϵ −vn+1 + vn+1 −v∗∥
≤∥vπϵ −vn+1∥+ ∥vn+1 −v∗∥.
(3.81)
Recall that πϵ is the policy that solves Mvn+1, which means that Mπϵvn+1 =
Mvn+1. This allows us to rewrite the ﬁrst term on the right-hand side of (3.81) as
∥vπϵ −vn+1∥= ∥Mπϵvπϵ −Mvn+1 + Mvn+1 −vn+1∥
≤∥Mπϵvπϵ −Mvn+1∥+ ∥Mvn+1 −vn+1∥
= ∥Mπϵvπϵ −Mπϵvn+1∥+ ∥Mvn+1 −Mvn∥
≤γ ∥vπϵ −vn+1∥+ γ ∥vn+1 −vn∥.
Solving for ∥vπϵ −vn+1∥gives
∥vπϵ −vn+1∥≤
γ
1 −γ ∥vn+1 −vn∥.

96
introduction to markov decision processes
We can use similar reasoning applied to the second term in equation (3.81) to show
that
∥vn+1 −v∗∥≤
γ
1 −γ ∥vn+1 −vn∥.
(3.82)
The value iteration algorithm stops when ∥vn+1 −vn∥≤ϵ(1 −γ )/2γ . Substituting
this in (3.82) gives
∥vn+1 −v∗∥≤ϵ
2.
(3.83)
Recognizing that the same bound applies to ∥vπϵ −vn+1∥and combining these
with (3.81) gives us
∥vπϵ −v∗∥≤ϵ,
which completes our proof.
□
3.10.5
Randomized Policies
We have implicitly assumed that for each state, we want a single action. An alter-
native would be to choose a policy probabilistically from a family of policies. If
a state produces a single action, we say that we are using a deterministic policy.
If we are randomly choosing an action from a set of actions probabilistically, we
say we are using a randomized policy.
Randomized policies may arise because of the nature of the problem. For
example, you wish to purchase something at an auction, but you are unable to
attend yourself. You may have a simple rule (“purchase it as long as the price is
under a speciﬁc amount”), but you cannot assume that your representative will apply
the same rule. You can choose a representative, and in doing so, you are effectively
choosing the probability distribution from which the action will be chosen.
Behaving randomly also plays a role in two-player games. If you make the same
decision each time in a particular state, your opponent may be able to predict your
behavior and gain an advantage. For example, as an institutional investor you may
tell a bank that you not willing to pay any more than $14 for a new offering of
stock, while in fact you are willing to pay up to $18. If you always bias your initial
prices by $4, the bank will be able to guess what you are willing to pay.
When we can only inﬂuence the likelihood of an action, then we have an instance
of a randomized MDP. Let
qπ
t (a|St) = probability that decision a will be taken at time t given state
St and policy π (more precisely, decision rule Aπ).
In this case our optimality equations look like
V ∗
t (St) = max
π∈MR

a∈A
/
qπ
t (a|St)

Ct(St, a) +

s′∈S
pt(s′|St, a)V ∗
t+1(s′)
0
. (3.84)

why does it work?
97
Now let us consider the single best action that we could take. Calling this a∗, we
can ﬁnd it using
a∗= arg max
a∈A
/
Ct(St, a) +

s′∈S
pt(s′|St, a)V ∗
t+1(s′)
0
.
This means that
Ct(St, a∗) +

s′∈S
pt(s′|St, a∗)V ∗
t+1(s′) ≥Ct(St, a) +

s′∈S
pt(s′|St, a)V ∗
t+1(s′)
(3.85)
for all a ∈A. Substituting (3.85) back into (3.84) gives us
V ∗
t (St) = max
π∈MR

a∈A

qπ
t (a|St)

Ct(St, a) +

s′∈S
pt(s′|St, a)V ∗
t+1(s′)

≤max
π∈MR

a∈A

qπ
t (a|St)

Ct(St, a∗) +

s′∈S
pt(s′|St, a∗)V ∗
t+1(s′)

= Ct(St, a∗) +

s′∈S
pt(s′|St, a∗)V ∗
t+1(s′).
What this means is that if you have a choice between picking exactly the action
you want versus picking a probability distribution over potentially optimal and
nonoptimal actions, you would always prefer to pick exactly the best action. Clearly,
this is not a surprising result.
The value of randomized policies arises primarily in two-person games, where
one player tries to anticipate the actions of the other player. In such situations part
of the state variable is the estimate of what the other play will do when the game
is in a particular state. By randomizing his behavior, a player reduces the ability
of the other player to anticipate his moves.
3.10.6
Optimality of Monotone Policies
The foundational result that we use is the following technical lemma:
Lemma 3.10.2
If a function g(s, a) is supermodular, then
a∗(s) = max
)
a′ ∈arg max
a
g(s, a)
*
(3.86)
is monotone and nondecreasing in s.
If the function g(s, a) has a unique, optimal a∗(s) for each value of s, then we can
replace (3.86) with
a∗(s) = max
a
g(s, a).
(3.87)

98
introduction to markov decision processes
Discussion
The lemma is saying that if g(s, a) is supermodular, then as s grows
larger, the optimal value of a given s will grow larger. When we use the version of
supermodularity given in equation (3.42), we see that the condition implies that as
the state becomes larger, the value of increasing the decision also grows. As a result
it is not surprising that the condition produces a decision rule that is monotone in
the state vector.
Proof of the lemma
Assume that s+ ≥s−, and choose a ≤a∗(s−). Since a∗(s)
is, by deﬁnition, the best value of a given s, we have
g(s−, a∗(s−)) −g(s−, a) ≥0.
(3.88)
The inequality arises because a∗(s−) is the best value of a given s−. Supermodu-
larity requires that
g(s−, a) + g(s+, a∗(s−)) ≥g(s−, a∗(s−)) + g(s+, a).
(3.89)
Rearranging (3.89) gives us
g(s+, a∗(s−)) ≥
'
g(s−, a∗(s−)) −g(s−, a)
(



≥0
+g(s+, a)∀a ≤a∗(s−) (3.90)
≥g(s+, a)
∀a ≤a∗(s−).
(3.91)
We obtain equation (3.91) because the term in brackets in (3.90) is nonnegative
(from (3.88)).
Clearly,
g(s+, a∗(s+)) ≥g(s+, a∗(s−))
because a∗(s+) optimizes g(s+, a). This means that a∗(s+) ≥a∗(s−) since, other-
wise, we would simply have chosen a = a∗(s−).
□
Just as the sum of concave functions is concave, we have the following:
Proposition 3.10.3
The sum of supermodular functions is supermodular.
The proof follows immediately from the deﬁnition of supermodularity, so we leave
it as one of those proverbial exercises for the reader.
The main theorem regarding monotonicity is relatively easy to state and prove,
so we will do it right away. The conditions required are what make it a little more
difﬁcult.
Theorem 3.10.5
Assume that
a. Ct(s, a) is supermodular on S × A.
b. 
s′∈S P(s′|s, a)vt+1(s′) is supermodular on S × A.
Then there exists a decision rule a(s) that is nondecreasing on S.

why does it work?
99
Proof
Let
w(s, a) = Ct(s, a) +

s′∈S
P(s′|s, a)vt+1(s′).
(3.92)
The two terms on the right-hand side of (3.92) are assumed to be supermodular,
and we know that the sum of two supermodular functions is supermodular, which
tells us that w(s, a) is supermodular. Let
a∗(s) = arg max
a∈A
w(s, a)
From lemma 3.10.2, we obtain the result that the decision a∗(s) increases mono-
tonically over S, which proves our result.
□
The proof that the one-period reward function Ct(s, a) is supermodular must be
based on the properties of the function for a speciﬁc problem. Of greater concern
is establishing the conditions required to prove condition b of the theorem because
it involves the property of the value function, which is not part of the basic data
of the problem.
In practice, it is sometimes possible to establish condition b directly based
on the nature of the problem. These conditions usually require conditions on the
monotonicity of the reward function (and hence the value function) along with
properties of the one-step transition matrix. For this reason we will start by showing
that if the one-period reward function is nondecreasing (or nonincreasing), then
the value functions are nondecreasing (or nonincreasing). We will ﬁrst need the
following technical lemma:
Lemma 3.10.3
Let pj, p′
j, j ∈J be probability mass functions deﬁned over J
that satisfy
∞

j=j′
pj ≥
∞

j=j′
p′
j
∀j ′ ∈J,
(3.93)
and let vj, j ∈J be a nondecreasing sequence of numbers. Then
∞

j=0
pjvj ≥
∞

j=0
p′
jvj
(3.94)
We would say that the distribution represented by {pj}j∈J stochastically dominates
the distribution {p′
j}j∈J. If we think of pj as representing the probability a random
variable V = vj, then equation (3.94) is saying that EpV ≥Ep′V . Although this
is well known, a more algebraic proof is as follows:

100
introduction to markov decision processes
Proof
Let v−1 = 0, and write
∞

j=0
pjvj =
∞

j=0
pj
j

i=0
(vi −vi−1)
(3.95)
=
∞

j=0
(vj −vj−1)
∞

i=j
pi
(3.96)
=
∞

j=1
(vj −vj−1)
∞

j=i
pi + v0
∞

i=0
pi
(3.97)
≥
∞

j=1
(vj −vj−1)
∞

i=j
p′
j + v0
∞

i=0
p′
j
(3.98)
=
∞

j=0
p′
jvj.
(3.99)
In equation (3.95) we replace vj with an alternating sequence that sums to vj.
Equation (3.96) involves one of those painful change of variable tricks with sum-
mations. Equation (3.97) is simply getting rid of the term that involves v−1. In
equation (3.98) we replace the cumulative distributions for pj with the distribu-
tions for p′
j, which gives us the inequality. Finally, we simply reverse the logic to
get back to the expectation in (3.99).
□
We stated that lemma 3.10.3 is true when the sequences {pj} and {p′
j} are probabil-
ity mass functions because it provides an elegant interpretation as expectations. For
example, we may use vj = j, in which case equation (3.94) gives us the familiar
result that when one probability distribution stochastically dominates another, it
has a larger mean. If we use an increasing sequence vj instead of j, then this can
be viewed as nothing more than the same result on a transformed axis.
In our presentation, however, we need a more general statement of the lemma,
which follows:
Lemma 3.10.4
Lemma 3.10.3 holds for any real valued, nonnegative (bounded)
sequences {pj} and {p′
j}.
The proof involves little more than realizing that the proof of lemma 3.10.3 never
required that the sequences {pj} and {p′
j} be probability mass functions.
Proposition 3.10.4
Suppose that
a. Ct(s, a) is nondecreasing (nonincreasing) in s for all a ∈A and t ∈T.
b. CT (s) is nondecreasing (nonincreasing) in s.

why does it work?
101
c. qt(s|s, a) = 
s′≥s P(s′|s, a), the reverse cumulative distribution function for
the transition matrix, is nondecreasing in s for all s ∈S, a ∈A and t ∈T.
Then vt(s) is nondecreasing (nonincreasing) in s for t ∈T.
Proof
As always, we use a proof by induction. We will prove the result for the
nondecreasing case. Since vT (s) = Ct(s), we obtain the result by assumption for
t = T. Now assume the result is true for vt′(s) for t′ = t + 1, t + 2, . . . , T. Let
a∗
t (s) be the decision that solves
vt(s) = max
a∈A Ct(s, a) +

s′∈S
P(s′|s, a)vt+1(s′)
= Ct(s, a∗
t (s)) +

s′∈S
P(s′|s, a∗
t (s))vt+1(s′).
(3.100)
Let ˆs ≥s. Condition c of the proposition implies that

s′≥s
P(s′|s, a) ≤

s′≥s
P(s′|ˆs, a).
(3.101)
Lemma 3.10.4 tells us that when (3.101) holds, and if vt+1(s′) is nondecreasing
(the induction hypothesis), then

s′∈S
P(s′|s, a)vt+1(s′) ≤

s′∈S
P(s′|ˆs, a)vt+1(s′).
(3.102)
Combining equation (3.102) with condition a of proposition 3.10.4 into equation
(3.100) gives us
vt(s) ≤Ct(ˆs, a∗(s)) +

s′∈S
P(s′|ˆs, a∗(s))vt+1(s′)
≤max
a∈A Ct(ˆs, a) +

s′∈S
P(s′|ˆs, a)vt+1(s′)
= vt(ˆs),
which proves the proposition.
□
With this result we can establish condition b of theorem 3.10.5:
Proposition 3.10.5
If
a. qt(s|s, a) = 
s′≥s P(s′|s, a) is supermodular on S × A, and
b. v(s) is nondecreasing in s,
then 
s′∈S P(s′|s, a)v(s′) is supermodular on S × A.

102
introduction to markov decision processes
Proof
Supermodularity of the reverse cumulative distribution means that

s′≥s
P(s′|s+, a+) +

s′≥s
P(s′|s−, a−) ≥

s′≥s
P(s′|s+, a−) +

s′≥s
P(s′|s−, a+)
We can apply lemma 3.10.4 using ps = 
s′≥s P(s′|s+, a+) + 
s′≥s P(s′|s−, a−)
and p′
s = 
s′≥s P(s′|s+, a−) + 
s′≥s P(s′|s−, a+), which gives

s′∈S

P(s′|s+, a+) + P(s′|s−, a−)

v(s′) ≥

s′∈S

P(s′|s+, a−) + P(s′|s−, a+)

v(s′),
which implies that 
s′∈S P(s′|s, a)v(s′) is supermodular.
□
Remark
Supermodularity of the reverse cumulative distribution 
s′∈S P(s′|s, a)
may seem like a bizarre condition at ﬁrst, but a little thought suggests that it is
often satisﬁed in practice. As stated, the condition means that

s′∈S
P(s′|s+, a+) −

s′∈S
P(s′|s+, a−) ≥

s′∈S
P(s′|s−, a+) −

s′∈S
P(s′|s−, a−).
Assume that the state s is the water level in a dam, and the decision a controls
the release of water from the dam. Because of random rainfalls, the amount of
water behind the dam in the next time period, given by s′, is random. The reverse
cumulative distribution gives us the probability that the amount of water is greater
than s+ (or s−). Our supermodularity condition can now be stated: if the amount
of water behind the dam is higher one month (s+), then the effect of the decision
of how much water to release (a) has a greater impact than when the amount of
water is initially at a lower level (s−).” This condition is often satisﬁed because a
control frequently has more of an impact when a state is at a higher level than a
lower level.
For another example of supermodularity of the reverse cumulative distribution,
assume that the state represents a person’s total wealth, and the control is the level
of taxation. The effect of higher or lower taxes is going to have a bigger impact
on wealthier people than on those who are not as fortunate (but not always: think
about other forms of taxation that affect less afﬂuent people more than the wealthy,
and use this example to create an instance of a problem where a monotone policy
may not apply).
We now have the result that if the reward function Ct(s, a) is nondecreasing in s
for all a ∈A and the reverse cumulative distribution 
s′∈S P(s′|s, a) is supermod-
ular, then 
s′∈S P(s′|s, a)v(s′) is supermodular on S × A. Combine this with the
supermodularity of the one-period reward function, and we obtain the optimality
of a nondecreasing decision function.

problems
103
3.11
BIBLIOGRAPHIC NOTES
This chapter presents the classic view of Markov decision processes, for which the
literature is extensive. Beginning with the seminal text of Bellman (Bellman, 1957),
there have been numerous, signiﬁcant textbooks on the subject, including Howard
(1960), Nemhauser (1966), White (1969), Derman (1970), Bellman (1971), Dreyfus
and Law (1977), Dynkin and Yushkevich (1979), Denardo (1982), Ross (1983),
and Heyman and Sobel (1984). As of this writing, the current high watermark for
textbooks in this area is the landmark volume by Puterman (2005). Most of this
chapter is based on Puterman (2005), modiﬁed to our notational style.
Section 3.8
The linear programming method was ﬁrst proposed in Manne
(1960) (see subsequent discussions in Derman (1962) and Puterman (2005)).
The so-called linear programming method was ignored for many years
because of the large size of the linear programs that were produced, but the
method has seen a resurgence of interest using approximation techniques.
Recent research into algorithms for solving problems using this method are
discussed in Section 10.8.
Section 3.10.6
In addition to Puterman (2005), see also Topkins (1978).
PROBLEMS
3.1
A classical inventory problem works as follows: Assume that our state vari-
able Rt is the amount of product on hand at the end of time period t and
that Dt is a random variable giving the demand during time interval (t−1,t)
with distribution pd = P(Dt = d). The demand in time interval t must be
satisﬁed with the product on hand at the beginning of the period. We can
then order a quantity at at the end of period t that can be used to replenish
the inventory in period t+1.
(a) Give the transition function that relates Rt+1 to Rt if the order quantity
is at (where at is ﬁxed for all Rt).
(b) Give an algebraic version of the one-step transition matrix P π = {pπ
ij}
where pπ
ij = P(Rt+1 = j|Rt = i, Aπ = at).
3.2
Repeat the previous exercise, but now assume that we have adopted a policy
π that says we should order a quantity at = 0 if Rt ≥s and at = Q −Rt if
Rt < q (we assume that Rt ≤Q). Your expression for the transition matrix
will now depend on our policy π (which describes both the structure of the
policy and the control parameter s).
3.3
We are going to use a very simple Markov decision process to illustrate how
the initial estimate of the value function can affect convergence behavior. In

104
introduction to markov decision processes
fact we are going to use a Markov reward process to illustrate the behavior
because our process does not have any decisions. Suppose that we have a
two-stage Markov chain with one-step transition matrix
P =
2 0.7
0.3
0.05
0.95
3
.
The contribution from each transition from state i ∈{1, 2} to state j ∈{1, 2}
is given by the matrix
2 10
30
20
5
3
.
That is, a transition from state 1 to state 2 returns a contribution of 30.
Apply the value iteration algorithm for an inﬁnite horizon problem (note
that you are not choosing a decision so there is no maximization step). The
calculation of the value of being in each state will depend on your previous
estimate of the value of being in each state. The calculations can be easily
implemented in a spreadsheet. Assume that your discount factor is 0.8.
(a) Plot the value of being in state 1 as a function of the number of iterations
if your initial estimate of the value of being in each state is 0. Show the
graph for 50 iterations of the algorithm.
(b) Repeat this calculation using initial estimates of 100.
(c) Repeat the calculation using an initial estimate of the value of being in
state 1 of 100, and use 0 for the value of being in state 2. Contrast the
behavior with the ﬁrst two starting points.
3.4
Show that P(St+τ|St), given that we are following a policy π (for stationary
problems), is given by (3.14). [Hint: First show it for τ = 1, 2, and then use
inductive reasoning to show that it is true for general τ.]
3.5
Apply policy iteration to the problem given in exercise 3.3. Plot the average
value function (i.e., average the value of being in each state) after each
iteration alongside the average value function found using value iteration
after each iteration (for value iteration, initialize the value function to zero).
Compare the computation time for one iteration of value iteration and one
iteration of policy iteration.
3.6
Now apply the hybrid value-policy iteration algorithm to the problem given
in exercise 3.3. Show the average value function after each major iteration
(update of n) with M = 1, 2, 3, 5, 10. Compare the convergence rate to
policy iteration and value iteration.
3.7
An oil company will order tankers to ﬁll a group of large storage tanks. One
full tanker is required to ﬁll an entire storage tank. Orders are placed at the
beginning of each four-week accounting period but do not arrive until the

problems
105
end of the accounting period. During this period the company may be able
to sell 0, 1, or 2 tanks of oil to one of the regional chemical companies
(orders are conveniently made in units of storage tanks). The probability of
a demand of 0, 1 or 2 is 0.40, 0.40, and 0.20, respectively.
A tank of oil costs $1.6 million (M) to purchase and sells for $2M. It
costs $0.020M to store a tank of oil during each period (oil ordered in period
t, which cannot be sold until period t + 1, is not charged to any holding
cost in period t). Storage is only charged on oil that is in the tank at the
beginning of the period and remains unsold during the period. It is possible
to order more oil than can be stored. For example, the company may have
two full storage tanks, order three more, and then only sell one. This means
that at the end of the period, they will have four tanks of oil. Whenever they
have more than two tanks of oil, the company must sell the oil directly from
the ship for a price of $0.70M. There is no penalty for unsatisﬁed demand.
An order placed in time period t must be paid for in time period t even
though the order does not arrive until t + 1. The company uses an interest
rate of 20 percent per accounting period (i.e., a discount factor of 0.80).
(a) Give an expression for the one-period reward function r(s, d) for being
in state s and making decision d. Compute the reward function for all
possible states (0, 1, 2) and all possible decisions (0, 1, 2).
(b) Find the one-step probability transition matrix when your action is to
order one or two tanks of oil. The transition matrix when you order zero
is given by
From–to
0
1
2
0
1
0
0
1
0.6
0.4
0
2
0.2
0.4
0.4
(c) Write out the general form of the optimality equations and solve this
problem in steady state.
(d) Solve the optimality equations using the value iteration algorithm, start-
ing with V (s) = 0 for s = 0, 1, and 2. You may use a programming
environment, but the problem can be solved in a spreadsheet. Run the
algorithm for 20 iterations. Plot V n(s) for s = 0, 1, 2, and give the
optimal action for each state at each iteration.
(e) Give a bound on the value function after each iteration.
3.8
Every day a salesman visits N customers in order to sell the R identical
items he has in his van. Each customer is visited exactly once, and each
customer buys zero or one item. Upon arrival at a customer location, the
salesman quotes one of the prices 0 < p1 ≤p2 ≤. . . ≤pm. Given that the
quoted price is pi, a customer buys an item with probability ri. Naturally
ri is decreasing in i. The salesman is interested in maximizing the total
expected revenue for the day. Show that if ripi is increasing in i, then it is
always optimal to quote the highest price pm.

106
introduction to markov decision processes
3.9
You need to decide when to replace your car. If you own a car of age y
years, then the cost of maintaining the car that year will be c(y). Purchasing
a new car (in constant dollars) costs P dollars. If the car breaks down,
which it will do with probability b(y) (the breakdown probability), it will
cost you an additional K dollars to repair it, after which you immediately
sell the car and purchase a new one. At the same time you express your
enjoyment with owning a new car as a negative cost −r(y), where r(y) is a
declining function with age. At the beginning of each year, you may choose
to purchase a new car (z = 1) or to hold onto your old one (z = 0). You
anticipate that you will actively drive a car for another T years.
(a) Identify all the elements of a Markov decision process for this problem.
(b) Write out the objective function which will allow you to ﬁnd an optimal
decision rule.
(c) Write out the one-step transition matrix.
(d) Write out the optimality equations that will allow you to solve the
problem.
3.10
You are trying to ﬁnd the best parking space to use that minimizes the time
needed to get to your restaurant. There are 50 parking spaces, and you see
spaces 1, 2, . . . , 50 in order. As you approach each parking space, you
see whether it is full or empty. We assume, somewhat heroically, that the
probability that each space is occupied follows an independent Bernoulli
process, which is to say that each space will be occupied with probability p
but will be free with probability 1 −p, and that each outcome is independent
of the other.
It takes 2 seconds to drive past each parking space and it takes 8 seconds
to walk past. That is, if we park in space n, it will require 8(50 −n) seconds
to walk to the restaurant. Furthermore it would have taken you 2n seconds
to get to this space. If you get to the last space without ﬁnding an opening,
then you will have to drive into a special lot down the block, adding 30
seconds to your trip.
We want to ﬁnd an optimal strategy for accepting or rejecting a parking
space.
(a) Give the sets of state and action spaces and the set of decision epochs.
(b) Give the expected reward function for each time period and the expected
terminal reward function.
(c) Give a formal statement of the objective function.
(d) Give the optimality equations for solving this problem.
(e) You have just looked at space 45, which was empty. There are ﬁve more
spaces remaining (46 through 50). What should you do? Using p = 0.6,
ﬁnd the optimal policy by solving your optimality equations for parking
spaces 46 through 50.
(f) Give the optimal value of the objective function in part (e) corresponding
to your optimal solution.

problems
107
1
2
3
4
0.7
0.3
0.8
0.2
0.9
0.1
$20
$15
$10
1
2
3
4
$20
$15
$10
Figure 3.8
3.11
We have a four-state process (shown in Figure 3.8). In state 1, we will
remain in the state with probability 0.7 and will make a transition to state 2
with probability 0.3. In states 2 and 3, we may choose between two policies:
remain in the state waiting for an upward transition or make the decision
to return to state 1 and receive the indicated reward. In state 4, we return
to state 1 immediately and receive $20. We wish to ﬁnd an optimal long
run policy using a discount factor γ = .8. Set up and solve the optimality
equations for this problem.
3.12
Say that you have been applying value iteration to a four-state Markov
decision process, and that you have obtained the values over iterations 8
through 12 shown in the table below (assume a discount factor of 0.90). Say
that you stop after iteration 12. Give the tightest possible (valid) bounds on
the optimal value of being in each state.
Iteration
State
8
9
10
11
12
1
7.42
8.85
9.84
10.54
11.03
2
4.56
6.32
7.55
8.41
9.01
3
11.83
13.46
14.59
15.39
15.95
4
8.13
9.73
10.85
11.63
12.18
3.13
In the proof of theorem 3.10.3 we showed that if v ≥Mv, then v ≥v∗. Go
through the steps of proving the converse, that if v ≤Mv, then v ≤v∗.
3.14
Theorem 3.10.3 states that if v ≤Mv, then v ≤v∗. Show that if vn ≤
vn+1 = Mvn, then vm+1 ≥vm for all m ≥n.
3.15
Consider a ﬁnite-horizon MDP with the following properties:
• S ∈ℜn, the action space A is a compact subset of ℜn, A(s) = A for all
s ∈S.

108
introduction to markov decision processes
• Ct(St, at) = ctSt + gt(at), where gt(·) is a known scalar function, and
CT (ST ) = cT ST .
• If action at is chosen when the state is St at time t, the next state is
St+1 = AtSt + ft(at) + ωt+1,
where ft(·) is scalar function, and At and ωt are, respectively, n × n and
n × 1-dimensional random variables whose distributions are independent of
the history of the process prior to t.
(a) Show that the optimal value function is linear in the state variable.
(b) Show that there exists an optimal policy π ∗= (a∗
1, . . . , a∗
T −1) composed
of constant decision functions. That is, Aπ∗
t (s) = A∗
t for all s ∈S for
some constant A∗
t .
3.16
You have invested R0 dollars in a stock market that evolves according to
the equation
Rt = γ Rt−1 + εt,
where εt is a discrete, positive random variable that is independent and
identically distributed and where 0 < γ < 1. If you sell the stock at the end
of period t, it will earn a riskless return r until time T, which means it will
evolve according to
Rt = (1 + r)Rt−1.
You have to sell the stock, all on the same day, some time before T.
(a) Write a dynamic programming recursion to solve the problem.
(b) Show that there exists a point in time τ such that it is optimal to sell
for t ≥τ, and optimal to hold for t < τ.
(c) How does your answer to (b) change if you are allowed to sell only a
portion of the assets in a given period? That is, if you have Rt dollars
in your account, you are allowed to sell at ≤Rt at time t.
3.17
An airline has to decide when to bring an aircraft in for a major engine
overhaul. Let st represent the state of the engine in terms of engine wear,
and let dt be a nonnegative amount by which the engine deteriorates during
period t. At the beginning of period t the airline may decide to continue
operating the aircraft (zt = 0) or to repair the aircraft (zt = 1) at a cost
of cR, which has the effect of returning the aircraft to st+1 = 0. If the
airline does not repair the aircraft, the cost of operation is co(st), which is
a nondecreasing, convex function in st.
(a) Deﬁne what is meant by a control limit policy in dynamic programming,
and show that this is an instance of a monotone policy.

problems
109
(b) Formulate the one-period reward function Ct(st, zt), and show that it is
submodular.
(c) Show that the decision rule is monotone in st. (Outline the steps in the
proof, and then ﬁll in the details.)
(d) Assume that a control limit policy exists for this problem, and let γ
be the control limit. Now write Ct(st, zt) as a function of one variable:
the state s. Using the control limit structure of Section 8.2.2, we can
write the decision zt as the decision rule zπ(st). Illustrate the shape of
Ct(st, zπ(s)) by plotting it over the range 0 ≤s ≤3γ (in theory, we
may be given an aircraft with s > γ initially).
3.18
A dispatcher controls a ﬁnite capacity shuttle that works as follows: In
each time period a random number At arrives. After the arrivals occur,
the dispatcher must decide whether to call the shuttle to remove up to M
customers. The cost of dispatching the shuttle is c, which is independent of
the number of customers on the shuttle. Each time period that a customer
waits costs h. If we let z = 1 if the shuttle departs and 0 otherwise, then
our one-period reward function is given by
ct(s, z) = cz + h[s −Mz]+,
where M is the capacity of the shuttle. Show that ct(s, a) is submodular
where we would like to minimize r. Note that we are representing the state
of the system after the customers arrive.
3.19
Assume that a control limit policy exists for our shuttle problem in exercise
8.8 that allows us to write the optimal dispatch rule as a function of s, as
in zπ(s). We may write r(s, z) as a function of one variable, the state s.
(a) Illustrate the shape of r(s, z(s)) by plotting it over the range 0<s
<3M (since we are allowing there to be more customers than can ﬁll
one vehicle, assume that we are allowed to send z = 0, 1, 2, . . . vehicles
in a single time period).
(b) Let c = 10, h = 2, and M = 5, and assume that At = 1 with probability
0.6 and is 0 with probability 0.4. Set up and solve a system of linear
equations for the optimal value function for this problem in steady state.


C H A P T E R
4
Introduction to Approximate
Dynamic Programming
In Chapter 3 we saw that we could solve stochastic optimization problems of the
form
max
π
E
 T

t=0
γ tCt(St, Aπ
t (St))

(4.1)
by recursively computing the optimality equations
Vt(St) = max
at

Ct(St, at) + γ E{Vt+1(St+1)|St}

(4.2)
backward through time. As before, we use the shorthand notation St+1 =
SM(St, at, Wt+1) to express the dependency of state St+1 on the previous state St,
the action at and the new information Wt+1. Equation (4.1) can be computationally
intractable even for very small problems. The optimality equations (4.2) give us
a mechanism for solving these stochastic optimization problems in a simple and
elegant way. Unfortunately, in the vast majority of real applications we cannot
solve Bellman’s equation exactly.
Approximate dynamic programming offers a powerful set of strategies for prob-
lems that are hard because they are large, but this is not the only application. Even
small problems may be hard to solve if we lack a formal model of the information
process, or if we do not know the transition function. For example, we may have
observations of changes in prices in an asset but we do not have a mathematical
model that describes these changes. We may observe an opponent playing poker,
but we are unable to describe his logic for making decisions. In this case we are
not able to compute the expectation. Alternatively, consider the problem of model-
ing how the economy of a small country responds to loans from the International
Approximate Dynamic Programming: Solving the Curses of Dimensionality, Second Edition.
Warren B. Powell.
2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
111

112
introduction to approximate dynamic programming
Monetary Fund. If we do not now how the economy responds to the size of a loan,
then this means that we do not know the transition function.
There are also many problems which are intractable simply because of size.
Imagine a multi-skill call center that handles questions about personal computers.
People who call in identify the nature of their question through the options they
select on the phone’s menu. Calls are then routed to people with expertise in
those areas. Assigning phone calls to technicians is a dynamic assignment problem
involving multiattribute resources, creating a dynamic program with a state space
that is effectively inﬁnite.
We begin our presentation by revisiting the “curses of dimensionality.” The
remainder of the chapter provides an overview of the basic principles and vocab-
ulary of approximate dynamic programming. As with the previous chapters, we
retain the basic notation where our system occupies a discrete state St ∈S =
{1, 2, . . . , s, . . .}. In Chapter 5 we introduce a much richer notational framework
that makes it easier to develop approximation strategies.
4.1
THE THREE CURSES OF DIMENSIONALITY (REVISITED)
The concept of the backward recursion of dynamic programming is so powerful
that we have to remind ourselves again why its usefulness can be so limited for
many problems. Consider, again, our simple asset acquisition problem, but this time
assume that we have multiple asset classes. For example, we may wish to purchase
stocks each month for our retirement account using the money that is invested in it
each month. Our online brokerage charges $50 for each purchase order we place,
so we have an incentive to purchase stocks in reasonably sized lots. Let ptk be
the purchase price of asset type k ∈K in period t. Since our decision is a vector,
we model our decisions using xt = (xtk)k∈K, where xtk is the number of shares of
asset k we purchase in period t. The total purchase cost of our order is
Cp(xt) =

k∈K
−

50I{xtk > 0} + ptkxtk

,
where I{xtk > 0} = 1 if xtk > 0 is true, and 0 otherwise. Each month we have $2000 to
invest in our retirement plan. We have to make sure that 
k∈K ptkxtk ≤$2000 and,
of course, xtk ≥0. While it is important to diversify the portfolio, transaction costs
rise as we purchase a wider range of stocks, which reduces how much we can buy.
Let Rtk be the number of shares of stock k that we have on hand at the end of
period t. The value of the portfolio reﬂects the prices ptk which evolve over time
according to
pt,k = pt−1,k + ˆpt,k,
where ˆpt,k is the exogenous change in the price of stock k between t−1 and t. In
addition each stock returns a random dividend ˆdtk (between t−1 and t) given in
dollars per share that are reinvested. Thus the information we receive in each time
period is
Wt = ( ˆpt, ˆdt).

the three curses of dimensionality (revisited)
113
If we were to reinvest our dividends, then our transition function would be
Rt+1,k = Rtk +
ˆdt+1,k
pt+1,k
Rtk + xtk.
The state of our system is given by
St = (Rt, pt).
Let the total value of the portfolio be
Yt =

k∈K
ptkRtk,
which we evaluate using a concave utility function U(Yt). Our goal is to maximize
total expected discounted utility over a speciﬁed horizon.
We now have a problem where our state St variable has 2|K| dimensions (the
resource allocation Rt and the prices pt), the decision variable xt has |K| dimen-
sions, and our information process Wt has 2|K| dimensions (the changes in prices
and the dividends). To illustrate how bad this gets, assume that we are restricting
our attention to 50 different stocks and that we always purchase shares in blocks
of 100, but that we can own as many as 20,000 shares (or 200 blocks) of any
one stock at a given time. This means that the number of possible portfolios Rt is
20050. Now assume that we can discretize each price into 100 different prices. This
means that we have up to 10050 different price vectors pt. Thus we would have
to loop over equation (4.2) 20050 × 10050 times to compute Vt(St) for all possible
values of St. This is the classical “curse of dimensionality” that is widely cited as
the Achilles heel of dynamic programming.
As we look at our problem, we see the situation is much worse. For each
of the 20050 × 10050 states, we have to compute the expectation in (4.2). Since
our random variables might not be independent (the prices generally will not be)
we could conceive of ﬁnding a joint probability distribution and performing 2
× 50 nested summations to complete the expectation (we have 50 prices and 50
dividends). It seems that this can be avoided if we work with the one-step transition
matrix and use the form of the optimality equations expressed in (3.3). This form of
the recursion, however, only hides the expectation (the one-step transition matrix
is an expectation).
We are not ﬁnished. This expectation has to be computed for each action x.
Assume we are willing to purchase up to 10 blocks of 100 shares of any one of
the 50 stocks. Since we can also purchase 0 shares of a stock, we have upwards
of 1150 different combinations of xt that we might have to consider (the actual
number is smaller because we have a budget constraint of $2000 to spend).
So we see that we have three curses of dimensionality if we wish to work in
discrete quantities. In other words, vectors really cause problems. In this chapter
we are going to provide an introduction to approximate dynamic programming,
which has evolved to address these issues. We are not ready to handle vector-
valued decisions, so we are going to stay with our action notation a and assume
that we are choosing among a relatively small number of discrete actions.

114
introduction to approximate dynamic programming
4.2
THE BASIC IDEA
The foundation of approximate dynamic programming is based on an algorithmic
strategy that steps forward through time (earning it the name “forward dynamic pro-
gramming” in some communities). If we wanted to solve this problem using classi-
cal dynamic programming, we would have to ﬁnd the value function Vt(St) using
Vt(St) = max
at∈At
(C(St, at) + γ E{Vt+1(St+1)|St})
= max
at∈At

C(St, at) + γ

s′∈S
P(s′|St, at)Vt+1(s′)

for each value of St. We have written our maximization problem as one of
choosing the best at ∈At. Note that At depends on our state variable St; we
express this dependence by indexing the action set by time t.
This problem cannot be solved using the techniques we presented in Chapter 3,
which require that we loop over all possible states (in fact we usually have three
nested loops, two of which require that we enumerate all states while the third
enumerates all actions). With approximate dynamic programming we step forward
in time. In order to simulate the process forward in time, we need to solve two
problems. The ﬁrst is that we need a way to randomly generate a sample of what
might happen (in terms of the various sources of random information). The second
is that we need a way to make decisions. We start with the problem of making
decisions ﬁrst, and then turn to the problem of simulating random information.
4.2.1
Making Decisions (Approximately)
When we used exact dynamic programming, we stepped backward in time, exactly
computing the value function that we then used to produce optimal decisions. When
we step forward in time, we have not computed the value function, so we have to
turn to an approximation in order to make decisions.
Let Vt(St) be an approximation of the value function. This is easiest to under-
stand if we assume that we have an estimate Vt(St) for each state St, but since it
is an approximation, we may use any functional form we wish. For our portfolio
problem above, we might create an approximation Vt(Rt) that depends only on
the number of shares we own, rather than the prices. In fact we might even use a
separable approximation of the form
Vt(St) ≈

k∈K
Vtk(Rtk),
where Vtk(Rtk) is a nonlinear function giving the value of holding Rtk shares of
stock k. Obviously, assuming such a structure (where we are both ignoring the
price vector pt as well as assuming separability) will introduce errors. Welcome
to the ﬁeld of approximate dynamic programming! The challenge, of course, is
ﬁnding approximations that are good enough for the purpose at hand.

the basic idea
115
Approximate dynamic programming proceeds by estimating the approximation
Vt(St) iteratively. We assume we are given an initial approximation V
0
t for all t (we
often use V
0
t = 0 when we have no other choice). Let V
n−1
t
be the value function
approximation after n−1 iterations, and consider what happens during the nth
iteration. We are going to start at time t = 0 in state S0. We determine a0 by solving
a0 = arg max
a∈A0

C(S0, a) + γ E{V1(S1)|S0}

= arg max
a∈A0

C(S0, a) + γ

s′∈S
P0(s′|S0, a)V1(s′)

,
(4.3)
where a0 is the value of a that maximizes the right-hand side of (4.3), and
P(s′|S0, a) is the one-step transition matrix (which we temporarily assume is
given). Assuming this can be accomplished (or at least approximated), we can use
(4.3) to determine a0.
For the moment, we are going to assume that we now have a way of making
decisions.
4.2.2
Stepping Forward through Time
We are now going to step forward in time from S0 to S1. This starts by implementing
the decision a0, which we are going to do using our approximate value function.
Given our decision, we next need to know the information that arrived between
t = 0 and t = 1. At t = 0, this information is unknown, and therefore random.
Our strategy will be to simply pick a sample realization of the information (for our
example, this would be ˆd1 and ˆp1) at random, a process that is often referred to
as Monte Carlo simulation. Monte Carlo simulation refers to the popular practice
of generating random information, using some sort of artiﬁcial process to pick a
random observation from a population. For example, imagine that we think a price
will be uniformly distributed between 60 and 70. Most computer languages have
a random number generator that produces a number that is uniformly distributed
between 0 and 1. If we let RAND( ) denote this function, then the statement
U = RAND( )
produces a value for U that is some number between 0 and 1 (e.g., 0.804663917).
We can then create our random price using the equation
ˆpt = 60 + 10 ∗U
= 68.04663917.
While this is not the only way to obtain random samples of information, it is one
of the more popular approaches.
Using our random sample of new information, we can now compute the next
state we visit, given by S1. We do this by assuming that we have been given a

116
introduction to approximate dynamic programming
transition function, which we represent using
St+1 = SM(St, at, Wt+1),
where Wt+1 is a set of random variables representing the information that arrived
between t and t+1. We ﬁrst introduced this notation in Chapter 2, which provides
a number of examples.
Keeping in mind that we assume that we have been given Vt for all t, we simply
repeat the process of making a decision by solving
a1 = arg max
a∈A1

C(S1, a) + γ

s′∈S
P1(s′|S1, a)V2(s′)

.
(4.4)
Once we determine a1, we, again, sample the new information ( ˆp2, ˆd2), compute
S2, and repeat the process. Whenever we make a decision based on a value function
approximation (as we do in equation (4.4)), we refer to this as a greedy strategy.
This term, which is widely used in dynamic programming, can be somewhat mis-
leading since the value function approximation is trying to produce decisions that
balance rewards now with rewards in the future.
Fundamental to approximate dynamic programming is the idea of following a
sample path. A sample path refers to a particular sequence of exogenous informa-
tion. To illustrate, suppose that we have a problem with two assets, where the price
of each asset at time t is given by (pt1, pt2). Assume that these changes occur
randomly over time according to
pti = pt−1,i + ˆpti,
i = 1, 2.
Further assume that from history, we have constructed 10 potential realizations of
the price changes ˆpt, t = 1, 2, . . . , 8, which we show in Table 4.1. Each sample
path is a particular set of outcomes of the vector ˆpt for all time periods. For
historical reasons we index each potential set of outcomes by ω, and let  be the set
of all sample paths, where for our example  = {1, 2, . . . , 10}. Let Wt be a generic
random variable representing the information arriving at time t, where for this
example we would have Wt = ( ˆpt1, ˆpt2). Then we would let Wt(ω) be a particular
sample realization of the prices in time period t. For example, using the data in
Table 4.1, we see that W3(5) = (−9, 25). In an iterative algorithm, we might choose
ωn in iteration n, and then follow the sample path W1(ωn), W2(ωn), . . . , Wt(ωn).
In each iteration we have to choose a new outcome ωn.
The notation Wt(ω) matches how we have laid out the data in Table 4.1. We go
to row ω in column t to get the information we need. This layout seems to imply
that we have all the random data generated in advance, and we just pick out what
we want. In practice, this may not be how the code is actually written. It is often
the case that we are randomly generating information as the algorithm progresses.
Readers need to realize that when we write Wt(ω), we mean a random observation
of the information that arrived at time t, regardless of whether we just generated

the basic idea
117
Table 4.1
Illustration of a set of sample paths
t = 1
t = 2
t = 3
t = 4
t = 5
t = 6
t = 7
t = 8
ω
ˆp1
ˆp2
ˆp3
ˆp4
ˆp5
ˆp6
ˆp7
ˆp8
1 (−14, −26)
(7, 16)
(−10, 20)
(41, 7)
(15, −11)
(8, −29)
(−6, −50)
(13, −36)
2
(11, −33)
(−36, −50)
(−23, 0)
(3, −50)
(7, −23)
(12, −10)
(35, 46)
(−18, −10)
3 (−50, −12) (−2, −18)
(−15, 12)
(−31, 3)
(5, −2)
(−24, 33)
(10, 2)
(38, −19)
4
(1, −13)
(41, 20)
(32, −2)
(3, −4)
(−46, 34)
(−15, 14)
(38, 16)
(48, −49)
5
(2, −34)
(−24, 34)
(−9, 25)
(−19, −40) (1, −28)
(34, −7)
(36, −3)
(−46, −35)
6
(41, −49)
(−24, −33) (−5, −25)
(16, 36)
(8, 47)
(−17, −4)
(−29, 45)
(−48, −30)
7
(44, 37)
(7, −19)
(49, −40)
(−13, 5)
(38, 37)
(−30, 45)
(−48, −47)
(19, 41)
8
(−19, 37)
(−50, −35) (−28, 32) (−13, −17)
(2, −2)
(−10, −22)
(−2, 47)
(2, −24)
9
(−13, 48)
(−48, 25)
(−37, 39)
(−2, 30)
(−28, 33) (−35, −49) (−44, −13) (−6, −18)
10
(48, 5)
(37, −39)
(43, 34)
(−13, −6) (28, −37) (−47, −12)
(13, 28)
(26, −35)
the data, looked it up in a table, or were given the data from an exogenous process
(perhaps an actual price realization from the Internet).
Our sample path notation is useful because it is quite general. For example, it
does not require that the random observations be independent across time (by con-
trast, Bellman’s equation explicitly assumes that the distribution of Wt+1 depends
only on St and possibly at). In approximate dynamic programming we do not
impose any restrictions of this sort. ADP allows us to simulate arbitrarily complex
processes as we step forward in time. We do not have to approximate the physics.
We are only approximating how we make decisions.
4.2.3
Sampling Random Variables
Our ADP algorithm depends on having access to a sequence of sample realizations
of our random variable. How this is done depends on the setting. There are three
ways that we can obtain random samples:
1. Real world.
Random realizations may come from real physical processes.
For example, we may be trying to estimate average demand using sequences
of actual demands. We may also be trying to estimate prices, costs, travel
times, or other system parameters from real observations.
2. Computer simulations.
The random realization may be a calculation from
a computer simulation of a complex process. The simulation may be of a
physical system such as a supply chain or an asset allocation model. Some
simulation models can require extensive calculations (a single sample real-
ization could take hours or days on a computer).
3. Sampling from a known distribution.
This is the easiest way to sam-
ple a random variable. We can use existing tools available in most software
languages or spreadsheet packages to generate samples from standard prob-
ability distributions. These tools can be used to generate many thousands of
random observations extremely quickly.

118
introduction to approximate dynamic programming
The ability of ADP algorithms to work with real data (or data coming from a
complex computer simulation) means that we can solve problems without actually
knowing the underlying probability distribution. We may have multiple random
variables that exhibit correlations. For example, observations of interest rates or cur-
rency exchange rates can exhibit complex interdependencies that are difﬁcult to esti-
mate. As a result we may not be able to compute an expectation because we do not
know the underlying probability distribution. That we can still solve such a problem
(approximately) greatly expands the scope of problems that we can address.
When we do have a probability model describing our information process, we
can use the power of computers to generate random observations from a distribution
using a process that is generally referred to as Monte Carlo sampling. Although
most software tools come with functions to generate observations from major distri-
butions, it is often necessary to customize tools to handle more general distributions.
When this is the case, we often ﬁnd ourselves relying on sampling techniques that
depend on functions for generating random variables that are uniformly distributed
between 0 and 1, which we denote by U (often called RAND( ) in many computer
languages), or for generating random variables that are normally distributed with
mean 0 and variance 1 (the standard normal distribution), which we denote by Z.
Using the computer to generate random variables in this way is known as Monte
Carlo simulation. A “Monte Carlo sample” (or “Monte Carlo realization”) refers
to a particular observation of a random variable. Not surprisingly, it is much easier
to use a single observation of a random variable than the entire distribution.
If we need a random variable X that is uniformly distributed between a and b,
then we use the internal random number generator to produce U and calculate
X = a + (b −a)U.
Similarly, if we wish to randomly generate a random variable that is normally
distributed with mean µ and variance σ 2, then we can usually depend on an internal
random number generator to compute a random variable Z that has mean 0 and
variance 1. We can use the result to perform the transformation
X = µ + σZ
and obtain a random variable with mean µ and variance σ 2.
If we need a random variable X with a cumulative distribution function FX(x) =
P(X ≤x), then we can obtain observations of X using a simple property. Let
Y = Fx(X) be a random variable that we compute by taking the original random
variable X, and then computing FX(X). It is possible to show that Y is a random
variable with a uniform distribution between 0 and 1. This implies that F −1
X (U) is
a random variable that has the same distribution as X, which means that we can
generate observations of X using
X = F −1
X (U).
For example, consider the case of an exponential density function λe−λx with
cumulative distribution function 1 −e−λx. Setting U = 1 −e−λx and solving for x

the basic idea
119
gives
X = −1
λ ln(1 −U).
Since 1 −U is also uniformly distributed between 0 and 1, we can use
X = −1
λ ln(U).
Figure 4.1 illustrates using the inverse cumulative-distribution method to gener-
ate both uniformly distributed and exponentially distributed random numbers. After
generating a uniformly distributed random number in the interval [0, 1] (denoted
U (0, 1) in the ﬁgure), we map this number from the vertical axis to the horizontal
axis. If we want to ﬁnd a random number that is uniformly distributed between
a and b, the cumulative distribution simply stretches (or compresses) the uniform
(0, 1) distribution over the range (a, b).
a
b
P [X ≤ x]
1.0
x
U(0,1)
U(a,b)
P [X ≤ x]
1.0
x
U(0,1)
x
Fx(X)
e
(a) Generating uniform random variables
(b) Generating exponentially distributed random variables
Figure 4.1
Generating uniformly and exponentially distributed random variables using the inverse
cumulative distribution method.

120
introduction to approximate dynamic programming
There is an extensive literature on generating Monte Carlo random variables
that goes well beyond the scope of this book. This section provides only a brief
introduction.
4.2.4
An ADP Algorithm
Stepping forward through time following a single set of sample realizations would
not, of course, produce anything of value. The price of using approximate value
functions is that we have to do this over and over, using a fresh set of sample
realizations each time. Using our vocabulary of the previous section, we would say
that we follow a new sample path each time.
When we run our algorithm iteratively, we index everything by the iteration
counter n. We would let ωn represent the speciﬁc value of ω that we sampled for
iteration n. At time t, we would be in state Sn
t , and make decision an
t using the
value function approximation V
n−1. The value function is indexed n−1 because it
was computed using information from iterations 1 ,. . . , n−1. After ﬁnding an
t , we
would observe the information Wt+1(ωn) to obtain Sn
t+1. After reaching the end of
our horizon, we would increment n and start over again.
A basic approximate dynamic programming algorithm is summarized in
Figure 4.2. This is very similar to backward dynamic programming (see
Figure 3.1), except that we are stepping forward in time. We no longer have the
dangerous “loop over all states” requirement, but we have introduced a fresh set
of challenges. As this book should make clear, this basic algorithmic strategy is
Step 0. Initialization.
Step 0a. Initialize V
0
t (St) for all states St.
Step 0b. Choose an initial state S1
0.
Step 0c. Set n = 1.
Step 1. Choose a sample path ωn.
Step 2. For t = 0, 1, 2, . . . , T do:
Step 2a. Solve
ˆvn
t = max
at ∈Ant

Ct(Sn
t , at) + γ

s′∈S
P(s′|Sn
t , at)V
n−1
t+1 (s′)

,
and let an
t be the value of at that solves the maximization problem.
Step 2b. Update V
n−1
t
(St) using
V
n
t (St) =

ˆvn
t ,
St = Sn
t ,
V
n−1
t
(St),
otherwise.
Step 2c. Compute Sn
t+1 = SM(Sn
t , an
t , Wt+1(ωn)).
Step 3. Let n = n+1. If n<N , go to step 1.
Figure 4.2
Approximate dynamic programming algorithm using the one-step transition matrix.

the basic idea
121
exceptionally powerful but introduces a number of technical challenges that have
to be overcome (indeed this is what ﬁlls up the remainder of this book).
A brief note on notation is in order here. We use Vt(St) as the true value of
being in a state at time t, while V
n
t (s) is our statistical estimate after n sample
observations. When we are following sample path ωn, we make decisions using
V
n−1
t
(s). The value ˆvn
t is computed using information from sample path ωn (e.g.,
the fact that we are in state Sn
t ), and V
n−1(s). ˆvn
t is then used to update our value
function approximation to produce V
n
t (s). Our indexing tells us the information
content of a variable, so if we use V
n−1
t
(s) to make a decision, we know this
variable depends on the sample information from ω1, . . . , ωn−1.
4.2.5
Discussion
Our generic approximate dynamic programming algorithm introduces several prob-
lems that we have to solve:
• Forward dynamic programming avoids the problem of looping over all possi-
ble states, but it still requires the use of a one-step transition matrix, with the
equally dangerous 
s′∈S P(s′|Sn
t , at)[. . .].
• We only update the value of states we visit, but we need the value of states
that we might visit. We need a way to estimate the value of being in states
that we have not visited.
• We might get caught in a circle where states we have never visited look bad
(relative to states we have visited), so we never visit them. We may never
discover that a state that we have never visited is actually quite good!
• The algorithmic strategy is quite general, but at the same time it does not
provide any mechanism for taking advantage of problem structure.
In the remainder of this chapter, we provide an introduction to some of the major
strategies that have evolved. We limit our attention in this chapter to value function
approximations known as lookup tables, which means we store a value V (s) for
each discrete state s. Not surprisingly, this approach does not scale, and we are
going to need a much richer set of approximation strategies. Also we only consider
policies that depend on value function approximations, whereas there is a much
wider range of strategies that are used in this community.
In the remainder of the chapter, we consider the following algorithmic strategies:
• Q-learning.
We present this popular strategy for problems with small state
and action spaces, but where we do not have a mathematical model for how
the system evolves over time.
• Real-time dynamic programming (RTDP).
This is a minor variation of the
algorithm we presented in Figure 4.2, but with a twist that allows us to claim
that the algorithm actually converges.
• Approximate value iteration. Here we relax the assumption that we can com-
pute the one-step transition matrix.

122
introduction to approximate dynamic programming
• Approximate value iteration using the post-decision state variable.
We ﬁrst
introduce the concept of the post-decision state variable, and then demonstrate
how this simpliﬁes the algorithm.
4.3
Q-LEARNING AND SARSA
One of the oldest algorithms from the reinforcement learning community is known
as Q-learning, named after the variable Q(s, a), which is an estimate of the value of
being in a state s and taking action a. Q-learning, and its sister algorithm SARSA,
are not only the simplest introductions to approximate dynamic programming and
reinforcement learning, they also provide a nice mechanism for introducing the
important issue of exploration.
4.3.1
Q-Learning
Let Q
n(s, a) be a statistical estimate of the true value Q(s, a) after n iterations.
Suppose that we are in state Sn. We choose an action an using
an = arg max
a∈A
Q
n−1(Sn, a).
(4.5)
Note that in iteration n, we use the estimatesQ
n−1(Sn, a) from the previous itera-
tion. The important feature of Q-learning is that to ﬁnd an action an using equation
(4.5), we do not need to compute an expectation, and nor do we need an estimate
of the value of being in a downstream state in the future.
Some physical systems are so complex that we cannot describe them with math-
ematical models, but we are able to observe behaviors directly. Such applications
arise in operational settings where a model is running in production, allowing us to
observe exogenous outcomes and state transitions from physical processes rather
than depending on mathematical equations.
There are three key mathematical models used in dynamic programming. In
engineering communities, “model” refers to the transition function (also known
as the system model or plant model). Model-free dynamic programming refers to
applications where we do not have an explicit transition function (the physical
problem may be quite complex). In such problems we may make a decision but
then have to observe the results of the decision from a physical process.
The second “model” refers to the exogenous information process, where we may
not have a probability law describing the outcomes. The result is that we will not
be able to compute a one-step transition matrix, but it also means that we cannot
even run simulations because we do not know the likelihood of an outcome. In
such settings we assume that we have an exogenous process generating outcomes
(e.g., stock prices or demands).
The third use of the term model refers to the cost or contribution function.
For example, we may have a process where decisions are being made by a human
maximizing an unknown utility function. We may have a system where we observe
the behavior of a human and infer what action we should be taking given a state.

q-learning and sarsa
123
In the reinforcement learning community, the term model often refers to the
one-step transition matrix. Computing the one-step transition matrix not only means
that you know the transition function and the probability law behind the exogenous
information, it also means that your state space is discrete and small enough that
it makes sense to compute the transition matrix.
Once we choose an action an, we assume that we are allowed to observe a
contribution ˆC(Sn , an) (which may be random), and the next state Sn+1. We then
compute an updated value of being in state Sn and taking action an using
ˆqn = ˆC(Sn , an) + γ max
a′∈AQ
n−1(Sn+1, a′).
(4.6)
We use this information to update our Q-factors as follows:
Q
n
t (Sn, an) = (1 −αn−1)Q
n−1(Sn, an) + αn−1 ˆqn,
where αn−1 is a stepsize between 0 and 1. Chapter 11 addresses the issue of stepsize
selection in considerable depth. Common practice is to use a constant stepsize (e.g.,
αn = 0.05) or a declining rule such as a/(a + n −1). However, special care is
required in the choice of stepsize rule for this algorithm, and we urge the reader
to consider the issues raised in Chapter 11.
Given a set of Q-factors, we can quickly compute an estimate of the value of
being in a state s using
V
n(s) = max
a∈AQ
n(s, a).
We can use this simple substitution to replace (4.6) with
ˆqn = ˆC(Sn, an) + γV
n−1(Sn+1).
(4.7)
Now contrast this calculation with how we computed ˆvn
t in our ADP algorithm in
Figure 4.2:
ˆvn
t = max
at ∈Ant

Ct(Sn
t , at) + γ

s′∈S
P(s′|Sn
t , at)V
n−1
t+1 (s′)

.
We quickly see that to ﬁnd ˆvn
t , we have to compute the embedded expectation over
the potential downstream states that result from our action at. By contrast, since
Q-learning depends on observations from an exogenous process, we do not need
a transition function (known as a “model”) SM(Sn, an, W n+1). For example, we
might be trying to optimize our play at online poker. Given our cards (in our hand,
and anything displayed on the table), we make a decision, observe the play of our
opponent, and then observe the next state. We do not receive a contribution until
the end of the game.
If we choose the action using equation (4.5), the resulting algorithm would
not be guaranteed to lead to an optimal solution. The problem is that the factors
Q
n(s, a) might underestimate the value of a state-action pair. As a result we may

124
introduction to approximate dynamic programming
not choose actions that take us to this state, which means that we do not correct
the error, and we may end up ignoring actions that might be quite attractive. What
we need is a rule that forces us to explore states and actions that may not look
attractive, because we have not visited them often enough. One of the simplest
ways of overcoming this is to modify our policy for choosing an action using a
rule such as the ϵ-greedy policy. Using this policy, with probability ϵ we choose
an action at random from the set A. With probability 1 −ϵ we choose the action
according to equation (4.5), in which case we say that we are exploiting our current
knowledge of the value of each state-action pair.
The ϵ-greedy policy is simple and intuitive, and produces a guarantee that we
will visit every (reachable) state and action inﬁnitely often. This can work well
in practice for some problems, but not for others. Solving the problem of when
to explore and when to exploit is known as the exploration versus exploitation
problem. This is a difﬁcult problem that is an active area of research. See Chapter 12
for a more complete discussion of this problem.
Not surprisingly, Q-learning is difﬁcult to apply to problems with even modest
state and action spaces, but its value lies in its ability to solve problems without a
model.
4.3.2
SARSA
SARSA is an algorithm that is closely related to Q-learning that allows us to illus-
trate an important issue that pervades approximate dynamic programming. SARSA
works as follows: Say we are in a state s, choose an action a, after which we observe
a reward r, observe the next state s′, then choose an action a′. The sequence s, a,
r, s′, and a′ makes up the name of the algorithm.
Say
we
use
some
policy
Aπ
for
choosing
an = Aπ(Sn).
We
then
simulate
the
next
state
Sn+1,
perhaps
by
using
our
transition
function
Sn+1 = SM(Sn, an, W n+1), where W n+1 is a sampled observation of our random
noise. We choose the next action using an+1 = Aπ(Sn+1). For the purpose of this
section only, we use r(s, a) to represent our reward (rather than a contribution
C(s, a)). We then compute a sample estimate of the value of being in state Sn and
taking action an = Aπ(Sn) as follows:
ˆqn(Sn, an) = r(Sn, an) + γQ
n−1(Sn+1, an+1).
We can then use ˆqn(Sn, an) to updateQ
n−1(Sn, an) as we did with Q-learning.
SARSA is an algorithm for learning the value of a ﬁxed policy Aπ(s). The policy
has to guarantee that all actions are tested inﬁnitely often, and by construction, it
means that actions are tested with a frequency that is ﬁxed by the policy. Also,
by following the state from Sn to Sn+1, we ensure that we are going to sample
states with the correct distribution determined by the policy. However, we are not
searching for an optimal (or even good) policy; the algorithm only estimates the
value of a policy. While this seems like a modest result, such algorithms play an
important role in policy iteration algorithms, where we have to estimate the value
of following a policy before updating the policy.

q-learning and sarsa
125
4.3.3
On-policy, Off-policy, and the Exploration Problem
Q-learning and SARSA provide a nice mechanism for introducing the idea of off-
policy and on-policy algorithms. With SARSA, we use the same policy Aπ(s) for
choosing action an when we are in state sn as we do when we reach into the
future and choose action a′ = Aπ(Sn+1). As long as our policy ensures that all
actions will be chosen inﬁnitely often, we can guarantee that our Q-factors will
converge to the correct value of being in state s and choose action a while following
policy Aπ(s). Also, by following the trajectory from state Sn to state Sn+1 =
SM(Sn, Aπ(Sn), W n+1), we ensure that we are sampling states in the proportion
determined by the policy.
Q-learning, to the contrary, may use a policy such as ϵ-greedy to choose an
action a, but downstream, it uses maxaQ
n−1(Sn+1, a) to choose the next action. In
other words, one policy is used to decide which action to evaluate, and a separate
policy is used to choose the action in the future.
In approximate dynamic programming, we often like to choose what we think is
the best action given a state. Once a certain policy appears best, we would like to
evaluate this policy. But, if we insist on choosing actions we think are best, we may
get stuck visiting a small number of states that look good, while avoiding states
that may not look good, and simply end up with poor estimates. To overcome this
problem, we may introduce exploration strategies such as ϵ-greedy, but this means
we are using one policy to force exploration, while tracking the performance of a
policy that chooses what action to take.
The policy that determines which action to take, from which we determine
the next state to visit, is often referred to as the behavior policy, since it often
describes how a physical system is actually behaving. If we have an online system,
the behavior policy is describing how a system behaves, but in a simulation we
may choose this policy to control the process of sampling states. In this setting we
prefer to use the more descriptive term sampling policy.
The policy that determines the action that appears to be best is then called
the target policy in the reinforcement learning community, but is also sometimes
known by the more descriptive name learning policy (literally, the policy we are
trying to learn). Our goal is typically to improve the target policy, while using
a sampling policy to ensure that we visit states often enough. When the learning
policy and the sampling policy are the same (as they are with SARSA), this is
called on-policy learning. When the learning and sampling policies are different,
this is called off-policy learning.
The use of on-policy or off-policy learning is an issue that pervades approximate
dynamic programming and reinforcement learning. With on-policy learning we may
not be able to ensure that we are sampling different parts of the state space often
enough to estimate the value of being in these states. By contrast, off-policy learning
may interfere with our ability to learn the value of a learning policy, although this
is primarily an issue when we use parametric models to approximate a policy (see
Chapter 9 for a further discussion of this issue).

126
introduction to approximate dynamic programming
4.4
REAL-TIME DYNAMIC PROGRAMMING
The next algorithm we consider assumes that we can compute the one-step transi-
tion matrix (as we did in algorithm 4.2), but this time we are going to make a small
modiﬁcation that enables us to guarantee that the algorithm will eventually con-
verge to the optimal policy. The strategy was introduced in the research literature
under the name real-time dynamic programming, abbreviated RTDP.
The best way to understand RTDP is through its deterministic analogue known as
the A∗algorithm, which is a type of shortest path algorithm. Imagine a deterministic
network where cij is the deterministic contribution of traversing link (i, j). We want
to get from an origin q to a destination r while maximizing the total contribution.
Suppose that we are given a value vi that overestimates the contribution from node
i to the destination node r. Now, imagine that we are at some node i, choose
the link (i, j) out of i that maximizes cij + vj over all j. If (i, j∗) is the best
link, let vi = cij∗+ vj∗, traverse to j ∗, and repeat the process. The algorithm runs
by starting the process at node q, restarting at node q each time we reach the
destination (or otherwise become stuck). If we start with optimistic estimates of
the value of each node, we can guarantee that every time we update the estimate,
the resulting revised estimate is also optimistic. If we are at a node i and do not
traverse a link (i, j) (which would allow us to update v(j)), even if v(j) is incorrect,
we can ensure that we have chosen an even better path.
RTDP is a stochastic version of the A∗algorithm. Again, assume that we have
an optimistic estimate of V (s), the value of being at a state s. However, if we take
an action a, we now assume there is a known probability P(s′|s,a) that gives the
probability that we will transition to state s′. If we are in state Sn, we choose an
action an by solving
ˆvn = max
a∈An

C(Sn, a) + γ

s′
P(s′|Sn, a)V n−1(s′)

.
and let an be the resulting optimal action. We update V n(Sn) = ˆvn, leaving the
values of all other states unchanged. We then choose the next state using
Sn = SM(Sn, an, W n+1),
(4.8)
where W n+1 is a sample realization of a possible transition. A version of an RTDP
algorithm (there are different variations) is given in Figure 4.3.
As with the A∗algorithm, if V n−1(s) is optimistic, then we can guarantee that
V n(s) is also optimistic (for all states). As long as the values are optimistic, we
will explore actions that appear to be best. The use of optimistic estimates means
that if we do not select an action (even with an optimistic estimate), then we can
safely ignore the action. If we are choosing an action only because the downstream
value is optimistic, eventually this downstream value will be corrected.
Unfortunately, we pay a signiﬁcant price for this guarantee. First, we have to
compute the one-step transition matrix so that we can calculate the expectation
exactly. Second, the use of optimistic estimates means that we may be visiting
every state (multiple times), which is an issue with large state spaces.

approximate value iteration
127
Step 0. Initialization.
Step 0a. Initialize V
0(s) for all states s.
Step 0b. Choose an initial state S1.
Step 0c. Set n = 1.
Step 1. Choose a sample path ωn.
Step 2a. Solve
ˆvn = max
a∈An

C(Sn, a) + γ

s′∈S
P(s′|Sn, a)V
n−1(s′)

,
and let an be the value of a that solves the maximization problem.
Step 2b. Update V
n−1(Sn) using
V
n(S) =

ˆvn,
S = Sn,
V
n−1(S),
otherwise.
Step 2c. Compute Sn = SM(Sn, an, W(ωn)).
Step 3. Let n = n+1. If n < N , go to step 1.
Figure 4.3
Illustrative real-time dynamic programming algorithm.
4.5
APPROXIMATE VALUE ITERATION
For our next algorithm we are going to eliminate the assumption that we can
compute the one-step transition matrix. One strategy is to randomly generate a
sample of outcomes ˆn, in iteration n, of what W might be. Let pn(ω) be the
probability of outcome ˆω ∈ˆn, where, if we choose N observations in the set ˆn,
it might be the case that pn(ω) = 1/N. We could then approximate the expectation
using
EV
n−1(SM(Sn, a, W)) ≈

ω∈ˆn
pn(ω)V
n−1(SM(Sn, a, W(ω))),
ˆvn = max
a∈A

C(Sn, a) + γ

ˆω∈ˆn
pn( ˆω)V
n−1(SM(Sn, a, W(ω)))

,
(4.9)
which is an estimate of the value of being in state Sn. We then update our estimate
of the value of being in state Sn using
V
n(Sn) = (1 −αn−1)V
n−1(Sn) + αn−1 ˆvn.
(4.10)
Here αn−1 is known as a “stepsize” (among other things), and generally takes
on values between 0 and 1 (see Chapter 11 for an in-depth discussion of equation
(4.10)). Equation (4.10) is an operation that we see a lot of in approximate dynamic

128
introduction to approximate dynamic programming
programming. It is variously described as “smoothing,” (or “exponential smooth-
ing”), a “linear ﬁlter,” or “stochastic approximation.” Sometimes we use a constant
stepsize (e.g., αn = 0.1), a deterministic formula (e.g., αn = 1/n), or one that adapts
to the data. All are trying to do the same thing: use observations of noisy data (ˆvn)
to approximate the mean of the distribution from which the observations are being
drawn. V
n(Sn) is viewed as the best estimate of the value of being in state Sn after
iteration n.
The smoothing is needed only because of the randomness in ˆvn due to the
way we approximated the expectation. If we were able to compute the expectation
exactly, we would compute
V
n(Sn) = max
a∈An

C(Sn, a) + γ

s′∈S
P(s′|Sn, a)V n−1(s′)

.
This is similar to equation (4.11) using αn−1 = 1. It is also the same as our value
iteration algorithm (Section 3.4), except that now we are updating a single state Sn
rather than all states.
An outline of an approximate dynamic programming algorithm is provided in
Figure 4.4. In contrast with the algorithm in Figure 4.2, we now have completely
eliminated any step that requires looping over all the states in the state space. In the-
ory at least, we can now tackle problems with state spaces that are inﬁnitely large.
We face only the practical problem of ﬁlling in the details so that the algorithm
actually works on a particular application.
Step 0. Initialization.
Step 0a. Initialize V
0(S) for all states S.
Step 0b. Choose an initial state S1
0.
Step 0c. Set n = 1.
Step 1. Choose a random sample of outcomes ˆn ⊂ representing possible realizations of
the information arriving between t and t+1.
Step 2. Solve
ˆvn = max
a∈An

C(Sn, a) + γ

ˆω∈ˆn
pn( ˆω)V
n−1(SM(Sn, a, W(ω)))

.
(4.11)
Let an be the value of a that solves the maximization problem.
Step 3. Update V
n−1(Sn):
V
n(S) =

(1 −αn−1)V
n−1(Sn) + αn−1 ˆvn,
S = Sn,
V
n−1(S),
otherwise.
Step 4. Compute Sn+1 = SM(Sn, an, W(ωn)).
Step 5. Let n = n+1. If n<N , go to step 1.
Figure 4.4
Forward dynamic programming algorithm approximating the expectation.

the post-decision state variable
129
4.6
THE POST-DECISION STATE VARIABLE
There is a simple and elegant way of avoiding the ugly step of approximating the
expectation that will work for many applications. The idea uses a powerful construct
known as the post-decision state variable. The post-decision state variable is the
state of the system after we have made a decision but before any new information
has arrived. For a wide range of applications, the post-decision state is no more
complicated than the pre-decision state, and for many problems is it much simpler.
In fact the post-decision state is a device that opens up the door to solving vector-
valued decisions, a largely overlooked class of problems in approximate dynamic
programming.
Given the importance and power of the post-decision state, this section develops
this idea in considerable depth. We close by showing how the post-decision state
simpliﬁes a basic approximate value iteration algorithm.
4.6.1
Finding the Post-decision State Variable
For many problems it is possible to break down the effect of decisions and infor-
mation on the state variable. For example, we might write the transition function
for an inventory problem as
Rt+1 = max{Rt + at −ˆDt+1, 0}.
We can break this into two steps, given by
Ra
t = Rt + at,
Rt+1 = max{Ra
t −ˆDt+1, 0}.
Ra
t captures the pure effect of the decision at to order additional assets, while Rt+1
captures the effect of serving demands ˆDt+1.
If this is possible, we can break our original transition function
St+1 = SM(St, at, Wt+1)
(4.12)
into the two steps
Sa
t = SM,a(St, at),
St+1 = SM,W(Sa
t , Wt+1).
St is the state of the system immediately before we make a decision, while Sa
t is
the state immediately after we make a decision (which is why it is indexed by t).
For this reason we sometimes refer to St as the pre-decision state variable while
Sa
t is the post-decision state variable. Whenever we refer to a “state variable,” we
always mean the pre-decision state St. We refer to the pre-decision state only when
there is a speciﬁc need to avoid confusion.

130
introduction to approximate dynamic programming
We use the function SM,a(St, at) (or SM,x(St, xt) if we are using x as our
decision vector) extensively in our presentation. However, while there is some
elegance in deﬁning the “post- to pre-” transition function SM,W(Sa
t , Wt+1), we do
not actually need it in our algorithms. We use the pre- to post-transition function
SM,a(St, at) purely for the purpose of computing the value function, and then we
use the classical pre- to pre-transition in (4.12) when we step forward in time.
We ﬁrst saw pre- and post-decision state variables in Section 2.2.1 on decision
trees. Figure 4.5 illustrates a generic decision tree with decision nodes (squares) and
outcome nodes (circles). The information available at a decision node is the pre-
decision state, and the information available at an outcome node is the post-decision
state. The function SM,a(St, at) takes us from a decision node (pre-decision state)
to an outcome node (post-decision state). The function SM,W(Sa
t , Wt+1) takes us
from an outcome node to a decision node.
We revisit the use of post-decision state variables repeatedly through this book.
Its value is purely computational. The beneﬁts are problem speciﬁc, but in some
settings they are signiﬁcant.
4.6.2
A Perspective of the Post-decision State Variable
The decision of when to estimate the value function around a post-decision state
variable is fairly simple. If you can easily compute the expectation EVt+1(St+1),
S0
0
a
S
a0
W1
S1
a1
1
a
S
Figure 4.5
Generic decision tree, showing decision nodes (squares) and outcome nodes (circles). Solid
lines are decisions, and dotted lines are random outcomes.

the post-decision state variable
131
you should use the pre-decision state variable. If the expectation is hard (the “second
curse of dimensionality”), you might consider seeing if you can identify a compact
post-decision state variable. If you can compute an expectation exactly, taking
advantage of the structure of the problem, you are going to produce a more reliable
estimate than using Monte Carlo based methods.
The vast majority of papers in the ADP community use the pre-decision state
variable, reﬂecting, we believe, the classical development of Markov decision pro-
cesses where the ability to take expectations is taken for granted. A simple example
illustrates the potential of using the post-decision state variable. Imagine that we
are trying to predict unemployment for year t as a function of interest rates in
year t. If Ut is the unemployment rate in year t and It is the interest rate, we might
create a model of the form
Ut = θ0 + θ1(It)2,
(4.13)
where we would estimate θ0 and θ1 using historical data on unemployment and
interest rates. Now imagine that we wish to use our model to predict unemployment
next year. We would ﬁrst have to predict interest rates and then use our model to
predict unemployment.
If we use the same data to estimate the model, then
Ut = θ0 + θ1(It−1)2.
(4.14)
Estimating this model requires no additional work, but now we can use interest
rates this year to predict next year’s unemployment. Not surprisingly, this is a much
easier model to use.
Approximating a value function around the pre-decision state St+1 is compa-
rable to using equation (4.14). At time t, we ﬁrst have to forecast St+1 and then
compute Vt+1(St+1). Approximating the value function around the post-decision
state variable is comparable to using (4.14), which allows us to compute a variable
in the future (Ut+1) based on what we know now (It).
4.6.3
Examples of Post-decision State Variables
The power of the post-decision state depends largely on the nature of the underlying
application. Some problems offer considerable structure, but it takes some time to
develop a sense of how to construct a post-decision state. Below we provide some
examples that illustrate different settings.
Decision Trees—Outcome Nodes
In Figure 4.6 we see a typical decision tree representing a sequence of decisions
(solid lines) and information (dashed lines). At each node we show the current
estimate of the value of being at that node, written as vn
1, . . . , vn
4. Now assume
that at node 1, we have made a decision that takes us to outcome node 2, after
which a random event takes us to node 3, and we then made a decision that takes

132
introduction to approximate dynamic programming
1
2
3
4
12
ˆ
6
n+1
n+1
C
2
17
n
v
4
3
n
v
34
ˆ
11
C
Figure 4.6
Sequence of decision nodes and outcome nodes in a decision tree, with estimates of the
value of each node prior to observing costs ˆC.
us to node 4. The decision at node 1 produced a contribution of ˆCn+1
12
= 6, while
the decision at node 3 produced a contribution ˆCn+1
34
= 11.
The decision nodes (squares) represent the states where decisions are made. The
circles are typically referred to as outcome nodes, but these are also equivalent to
post-decision states. Let ˆvn+1
3
= ˆCn+1
34
+ vn
4 = 11 + 3 = 14. We can use ˆvn+1
3
to
update the value vn
2 of the previous, post-decision state using vn+1
2
= (1 −α)vn
2 +
α ˆvn+1
3
= (0.9)17 + (0.1)14 = 16.7.
In a classical decision tree an outcome node is equivalent to specifying a post-
decision state Sa
t = (St, at), which is to say a state-action pair. It is always possible
to specify a post-decision state as a state-action pair, but this means that the post-
decision state is higher dimensional than the pre-decision state.
A Stochastic Graph
Imagine that you are traversing a graph with random arc costs. As you arrive to
node i, you are allowed to see the actual cost on links out of node i that you will
incur if you traverse one of those links. If ˆctij is the sample realization of the cost
on link (i, j) when you reach node i, the pre-decision state is St = (i, (ˆctij)j). Now
assume that you choose to traverse the link (i, k). Once you make the decision,
and while you are still at node i, your post-decision state would be Sa
t = (k).
Your next pre-decision state would be St+1 = (k, (ˆckℓ)ℓ). In this example the post-
decision state is much simpler than the pre-decision state. In fact this situation is
quite common when the pre-decision state includes information needed to make a
decision (e.g., the costs on links out of a node), but otherwise are not needed to
model the transition.
Selling an Asset
The post-decision state is widely used to evaluate the value of an option to sell an
asset. Let
Rt =

1
if we are holding the asset at time t,
0
otherwise,
at =

1
if we sell the asset at time t,
0
otherwise,

the post-decision state variable
133
where
pt = price that we obtain for the asset if we sell at time t,
chold = cost of holding the asset during time interval t,
ˆpt = change in price that arrives during time interval t,
T = time by which the asset must be sold.
The pre-decision state variable is given by
St = (Rt, pt).
The post-decision state is given by Sa
t = (Ra
t , pa
t ), given by
Ra
t =

1
if Rt = 1 and at = 0,
0
otherwise,
pa
t = pt.
The next pre-decision state is then given by
Rt+1 = Ra
t ,
pt+1 = pa
t + ˆpt+1.
This is an important special case of a problem where there is a controllable
dimension of the state variable that evolves deterministically and an uncontrollable
dimension that evolves stochastically.
A Water Reservoir Problem
Water reservoirs have become an important component of an energy system that is
depending increasingly on energy from wind and solar, both notoriously variable
sources. As wind increases, it is relatively easy to cut back on the output of a
hydroelectric generator; similarly, if the wind drops, we can increase the output.
We also have the ability to pump water back into the reservoir during periods of
high wind and low demand.
At the foundation of these problems is a simple inventory equation given by
Rt+1 = Rt −xt + ˆRt+1,
where Rt is the amount of water in the reservoir at time t, xt is the amount of
water we choose to pump out at time t (if xt > 0) or pump back in (if xt < 0). ˆRt+1
represents random, exogenous inﬂows to the reservoir from precipitation and snow-
melt. Assume for this exercise that the sequence ˆR1, ˆR2, . . . , ˆRt is independent and
identically distributed.
Given this structure, the pre-decision state is given by St = Rt, and we might
model the post-decision state as Sx
t = Rx
t = Rt −xt. However, there is another
choice. LetRt be a forecast of ˆRt+1 given what we know at time t. We might also

134
introduction to approximate dynamic programming
represent the post-decision state as
Sx
t = Rx
t = Rt −xt +Rt.
Thus Sx
t can be viewed as our expected state at time t+1 given what we know at
time t.
Meeting Energy Demands—Creating Low-Dimensional Problems
Companies that operate our electric power grid have to match supplies of energy
from coal, nuclear, natural gas, wind, and solar to meet demands on a minute
by minute basis. Demands from electricity come from sectors such as residential,
commercial, light industrial, heavy industrial, and transportation. Let
Dtj = demand for energy by demand sectorj.
We assume that the demand for energy can be forecasted up to an exogenous noise
term using
Dt+1,j = µD
tj + ϵD
t+1,j,
where we are going to assume that the error terms ϵD
tj are independent and identi-
cally distributed over time.
We can represent the ﬂows of energy from each supply to each demand using
xtij = ﬂow of energy from source i to demand sector j at time t.
The supply of energy is governed by the physics of ramping up and shutting down
nuclear plants, coal plants, and natural gas facilities.
Assume also that we have a single reservoir attached to the grid, where we can
move energy into the reservoir by pumping water uphill during low demand periods
or withdraw energy from the reservoir (just as we did in the previous section). We
represent these ﬂows using
xti0 = ﬂow of energy from source i into the reservoir.
xt0j = ﬂow of energy from the reservoir to serve demand sector j.
Finally let Rt be a scalar variable representing the amount of energy held in the
reservoir (there will be a conversion from megawatthours to gallons of water). The
transition equations would be given by
Rt+1 = Rt +

i
xti0 −

j
xt0j + ˆRt+1,
where we again let ˆRt+1 be random inputs to the reservoir such as rainfall (but
now measured in megawatt-hours). The pre-decision state is given by
St = (Rt, Dt),

the post-decision state variable
135
where Rt is a scalar, but Dt is a vector (note that all these quantities are continuous).
Note that since µD
t
is a deterministic vector (representing the predictable patterns
of energy demand), we do not view it as part of the state variable. The post-decision
state variable would now be Sx
t = Rx
t , where
Rx
t = Rt+1 = Rt +

i
xti0 −

j
xt0j,
or
Rx
t = Rt+1 = Rt +

i
xti0 −

j
xt0j +Rt,
where againRt would be a forecast ofRt+1 made at time t. Note that while we have
high-dimensional state and decision variables, the post-decision state is a scalar.
This example illustrates how the post-decision state, for some problems, can
have much lower dimensionality than the pre-decision state. It also illustrates that
a process such as the demands Dt can vary stochastically in a way that makes
them drop out of the post-decision state variable. This would not be the case if we
modeled the evolution of demands using
Dt+1,j = Dtj + ϵtj.
In this case we would need to keep the vector Dt as part of the post-decision state
variable.
Selling Stocks—Action-Dependent Processes
Investment funds face a challenge when selling large blocks of stock because the
simple act of selling can depress the stock. Let
Rt = number of shares of the stock that are held at time t,
xt = number of shares to be sold at time t,
pt = current market price.
We might start with a simple model described using
Rt+1 = Rt −xt,
pt+1 = pt + ˆpt+1,
where ˆpt+1 is the exogenous change in the price. Our pre-decision state would be
St = (Rt, pt) while the post-decision state would be Sx
t = (Rt −xt, pt). However,
what if we introduce the dimension that ˆpt+1 depends on xt? We might assume, for
example, that E{ ˆpt+1|pt, xt} = −βxt, where β captures the downward pressure of
selling on the price. A brute-force way of handling this problem is to simply add
xt to the post-decision state, giving us Sx
t = (Rt −xt, pt, xt) or, more compactly,
Sx
t = (St, xt) (we would write this as Sa
t = (St, at) if we are using our notation

136
introduction to approximate dynamic programming
for discrete actions). We saw this earlier in our decision-tree example in Figure 4.5,
where the post-decision state is given by the outcome node. Note that there is one
outcome node for each state-action pair.
One attraction of this way of writing a post-decision state is that it is very
general—it applies to any problem. However, the price of this strategy is that the
post-decision state space, rather than possibly being more compact, was instead
multiplied by the size of the action space. If we have a discrete state space S and
a discrete action space A, the post-decision state space is now |S| × |A|.
Sometimes this growth of the post-decision state space cannot be avoided. How-
ever, for this particular application, we can use a more compact representation if
we take advantage of the deterministic way that actions inﬂuence future random-
ness. If xt has only the effect of shifting the mean of the price process downward
(without otherwise affecting the distribution), then our post-decision state for this
problem would be
Sx
t = (Rt −xt, pt −βxt).
Moving a Robot
Imagine that we have a robot that is moving left to right along a line. The robot
may occupy any one of six positions, and may move left or right at positions 2–5,
but must move right at position 1 and must move left at position 5. If the robot is
in position 2 and moves right, his intended position is 3, but there is noise in the
transition and the robot may end up at position 2, 3, or 4. Table 4.2 summarizes
all the states, actions, and the probabilities of landing in a subsequent state.
Given this table, we can represent the intended state as the post-decision state.
It is important that if we intend to land in state 2, the probability that it eventually
lands in state 1, . . . , 6 is independent of where the robot came from.
Now assume that the probabilities are as given in Table 4.3. This time, if
we are in location 3 and decide to go right to 4, the probability of where we
Table 4.2
Pre-decision state (location), action, post-decision state (intended
destination), and next pre-decision state
Probability of Next Pre-decision State
Location
Action
Post-decision State
1
2
3
4
5
6
2
Left
1
0.8
0.15
0.05
0
0
0
3
Left
2
0.1
0.8
0.1
0
0
0
4
Left
3
0
0.1
0.8
0.1
0
0
5
Left
4
0
0
0.1
0.8
0.1
0
6
Left
5
0
0
0
0.1
0.8
0.1
1
Right
2
0.1
0.8
0.1
0
0
0
2
Right
3
0
0.1
0.8
0.1
0
0
3
Right
4
0
0
0.1
0.8
0.1
0
4
Right
5
0
0
0
0.1
0.8
0.1
5
Right
6
0
0
0
0.05
0.15
0.8

the post-decision state variable
137
Table 4.3
Pre-decision state (location), action, post-decision state (intended
destination), and next pre-decision state
Probability of Next Pre-decision State
Location
Action
Post-decision State
1
2
3
4
5
6
2
Left
(2, Left)
0.8
0.15
0.05
0
0
0
3
Left
(3, Left)
0.13
0.8
0.07
0
0
0
4
Left
(4, Left)
0
0.13
0.8
0.07
0
0
5
Left
(5, Left)
0
0
0.13
0.8
0.07
0
6
Left
(6, Left)
0
0
0
0.13
0.8
0.07
1
Right
(1, Right)
0.07
0.8
0.13
0
0
0
2
Right
(2, Right)
0
0.07
0.8
0.13
0
0
3
Right
(3, Right)
0
0
0.07
0.8
0.13
0
4
Right
(4, Right)
0
0
0
0.07
0.8
0.13
5
Right
(5, Right)
0
0
0
0.05
0.15
0.8
eventually end up depends on the fact that we arrived at location 4 from location
3. If we land in location 4 from location 5, the next transition depends on where
we came from. In this case we would normally represent the post-decision state
as the state-action pair.
Homoscedastic Noise
Our robot problem is a special case of a more general stochastic system that evolves
according to the dynamics
St+1 = AtSt + Btxt + εt+1,
(4.15)
where St is a continuous n-dimensional vector, xt is a continuous m-dimensional
decision (control), At and Bt are suitably deﬁned matrices, and εt+1 is an n-
dimensional noise term that is independent and identically distributed. When the
noise does not depend on the state, it is called homoscedastic. It is fairly easy to
see that Sx
t = AtSt + Btxt is the post-decision state.
State-Dependent Noise
There are, of course, many applications where the noise depends on the state, which
means the noise is heteroscedastic. For example, if St is the speed of wind, the vari-
ation in the wind is much larger when the wind is faster. If the wind is fairly quiet,
the variation is smaller. In this setting the state transition is still given by equation
(4.14), but now εt+1 depends on St. However, even if this is the case, the post-
decision state is still given by Sx
t = AtSt + Btxt, since εt+1 does not depend on xt.
There may be problems where εt+1 is a random variable whose distribution
depends on St and xt. For example, imagine a mutual fund whose holding of a
stock is given by St, and xt describes decisions to buy and sell. The mutual fund
may be large enough that a large decision to buy (xt ≫0) may increase the price.

138
introduction to approximate dynamic programming
Let ˆpt+1 be the change in price so that our price dynamics are governed by
pt+1 = pt + ˆpt+1.
The distribution of ˆpt+1 may depend on xt, but perhaps we are willing to write the
random variable in the form
ˆpt+1 = θ0 sign(xt)
xt
θ1
2
+ ξt+1,
where θ0 and θ1 are parameters, sign(xt) = −1 if xt < 0, and +1 if xt > 0. Here
ξt+1 is a random variable with mean 0 that does not depend on pt or xt. Of course,
all we are doing is recognizing that our random variable ˆpt+1 has a deterministic
mean and a homoscedastic error term. Pulling out the mean leaves us with an error
term that does not depend on the state or action, which means we are back with
our homoscedastic setting. The point of this illustration is that it is important to
separate the truly random component of a random variable from any component
that is a deterministic function of the state and/or action.
4.6.4
The Optimality Equations Using the Post-decision State Variable
Just as we earlier deﬁned Vt(St) to be the value of being in state St (just before we
made a decision), let V a
t (Sa
t ) be the value of being in state Sa
t immediately after
we made a decision. There is a simple relationship between Vt(St) and V a
t (Sa
t ) that
is summarized as follows
V a
t−1(Sa
t−1) = E
'
Vt(St)|Sa
t−1
(
,
(4.16)
Vt(St) = max
at∈At

Ct(St, at) + γ V a
t (Sa
t )

,
(4.17)
V a
t (Sa
t ) = E
'
Vt+1(St+1)|Sa
t
(
.
(4.18)
where St = SM,W(Sa
t−1, Wt) in (4.15), Sa
t = SM,a(St, at) in equation (4.17), and
St+1 = SM,W(Sa
t , Wt+1) in equation (4.18). Equation (4.16) writes V a
t−1(Sa
t−1) as a
function of Vt(St). Equation (4.18) does the same for the next time period, whereas
equation (4.17) writes Vt(St) as a function of V a
t (Sa
t ). Note that we put the discount
factor γ only when we take the expectation, since this is where we step backward
in time.
If we substitute (4.17) into (4.16), we obtain the standard form of Bellman’s
equation
Vt(St) = max
at∈At

Ct(St, at) + γ E {Vt+1(St+1)|St} 
.
By contrast, if we substitute (4.16) into (4.15), we obtain the optimality equations
around the post-decision state variable
V a
t−1(Sa
t−1) = E

max
at∈At

Ct(St, at) + V a
t (Sa
t )
.... Sa
t−1

.
(4.19)

the post-decision state variable
139
What should immediately stand out is that the expectation is now outside of the max
operator. While this should initially appear to be a much more difﬁcult equation
to solve (we have to solve an optimization problem within the expectation), this
is going to give us a tremendous computational advantage. Equation (4.17) is a
deterministic optimization problem that can still be quite challenging for some
problem classes (especially those involving the management of discrete resources),
but there is a vast research base that we can draw on to solve these problems.
By contrast, the expectation in equation (4.16) or (4.18) may be easy, but it is
often computationally intractable. This is the step that typically requires the use of
approximation methods.
Writing the value function in terms of the post-decision state variable provides
tremendous computational advantages (as well as considerable simplicity). Since
Bellman’s equation, written around the pre-decision state variable, is absolutely
standard even in the ﬁeld of approximate dynamic programming, this represents
a signiﬁcant point of departure of our treatment of approximate dynamic
programming.
4.6.5
An ADP Algorithm Using the Post-decision State
Assume, as we did in Section 4.2, that we have found a suitable approximation
Vt(Sa
t ) for the value function around the post-decision state Sa
t . As before, we are
going to run our algorithm iteratively. Assume that we are in iteration n and that
at time t that we are in state Sn
t .
Our optimization problem at time t can now be written
ˆvn
t = max
at∈Ant

Ct(Sn
t , at) +V
n−1
t
(SM,a(Sn
a, at))

.
(4.20)
We use the notation Sn
t to indicate that we are at a particular state rather than
at a set of all possible states. We also note that the feasible region An
t depends
on the state Sn
t (the subscript t indicates that we are at time t with access to the
information in St, while the superscript n indicates that we are depending on the
nth sample realization).
Let an
t be the value of at that solves (4.19). What is very important about this
problem is that it is deterministic. We do not require directly computing or even
approximating an expectation as we did in equation (4.9) (the expectation is already
captured in the deterministic function V
n−1
t
(Sa
t )). For large-scale problems, which
we consider later, this will prove critical.
We next need to update our value function approximation. ˆvn
t is a sample real-
ization of the value of being in state Sn
t so that we can update our value function
approximation using
V
n
t (Sn
t ) = (1 −αn−1)V
n−1
t
(Sn
t ) + αn−1 ˆvn
t .
There is nothing wrong with this expression, except that it will give us an estimate
of the value of being in the pre-decision state St. When we are solving the decision

140
introduction to approximate dynamic programming
problem at time t (in iteration n), we would use V
n−1
t+1 (St+1), but since St+1 is
a random variable at time t, we have to compute (or at least approximate) the
expectation.
A much more effective alternative is to use ˆvn
t to update the value of being in
the post-decision state Sa,n
t−1. Keep in mind that the previous decision an
t−1 put us
in state Sa,n
t−1, after which a random outcome, Wt(ωn), put us in state Sn
t . So, while
ˆvn
t is a sample of the value of being in state Sn
t , it is also a sample of the value of
the decision that put us in state Sa,n
t−1. Thus we can update our post-decision value
function approximation using
V
n
t−1(Sa,n
t−1) = (1 −αn−1)V
n−1
t−1 (Sa,n
t−1) + αn−1 ˆvn
t .
(4.21)
Note that we have chosen not to use a superscript “a” for the value function
approximation V
n
t−1(Sa,n
t−1). In the remainder of this book, we are going to use the
value function around the post-decision state almost exclusively, so we suppress
the superscript “a” notation to reduce notational clutter. By contrast, we have to
retain the superscript for our state variable, since we will use both pre-decision and
post-decision state variables throughout.
The smoothing in equation (4.20) is where we are taking our expectation. If we
write
V a
t (Sa
t ) = E
'
Vt+1(St+1)|Sa
t
(
,
then the classical form of Bellman’s equation becomes
Vt(St) = max
at

Ct(St, at) + V a
t (Sa
t )

.
The key step is that we are writing E
'
Vt+1(St+1)|Sa
t
(
as a function of Sa
t rather
than St, where we take advantage of the fact that Sa
t is a deterministic function
of at.
A complete sketch of an ADP algorithm using the post-decision state variable
is given in Figure 4.7.
Forward dynamic programming using the post-decision state variable is more
elegant because it avoids the need to approximate the expectation explicitly within
the optimization problem. It also gives us another powerful device. Since the
decision function “sees” Vt(Sa
t ) directly (rather than indirectly through the approx-
imation of the expectation), we are able to control the structure of Vt(Sa
t ). This
feature is especially useful when the myopic problem maxat∈At Ct(St, at) is an
integer program or a difﬁcult linear or nonlinear program that requires special
structure. This feature comes into play in the context of more complex problems,
such as those discussed in Chapter 14.
Although it may not be entirely apparent now, this basic strategy makes it
possible to produce production quality models for large-scale industrial problems
that are described by state variables with millions of dimensions. The process
of stepping forward in time uses all the tools of classical simulation, where it is

the post-decision state variable
141
Step 0. Initialization.
Step 0a. Initialize V
0
t , t ∈T.
Step 0b. Set n = 1.
Step 0c. Initialize S1
0.
Step 1. Choose a sample path ωn.
Step 2. Do for t = 0, 1, 2, . . . , T:
Step 2a. Solve:
ˆvn
t = max
at ∈Ant

Ct(Sn
t , at) +V
n−1
t
(SM,a(Sn
t , at))

,
and let an
t be the value of at that solves the maximization problem.
Step 2b. If t > 0, update V
n−1
t−1 using
V
n
t−1(Sa,n
t−1) = (1 −αn−1)V
n−1
t−1 (Sa,n
t−1) + αn−1 ˆvn
t .
Step 2c. Find the post-decision state
Sa,n
t
= SM,a(Sn
t , an
t )
and the next pre-decision state
Sn
t+1 = SM(Sn
t , an
t , Wt+1(ωn)).
Step 3. Increment n. If n ≤N , go to step 1.
Step 4. Return the value functions (V
N
t )T
t=0.
Figure 4.7
Forward dynamic programming using the post-decision state variable.
possible to capture an arbitrarily high level of detail as we simulate forward in time.
Our interest in this book is obtaining the highest quality decisions as measured by
our objective function. The key here is creating approximate value functions that
accurately capture the impact of decisions now on the future. For problems with
possibly high-dimensional decision vectors, the optimization problem
max
at∈Ant

Ct(Sn
t , at) +V
n−1
t
(SM,a(Sn
t , at))

will have to be solved using the various tools of mathematical programming (lin-
ear programming, nonlinear programming, and integer programming). In order to
use these tools, we need to identify in the value function approximation certain
properties that may play a role in the design of the approximation strategy.
4.6.6
The Post-decision State and Q-Learning
There is a very important relationship between approximate dynamic programming
using the post-decision state and Q-learning. It stems from the simple observation

142
introduction to approximate dynamic programming
that a state-action pair is a form of post-decision state. In equations (4.16) and
(4.17) we set up Bellman’s equations around the pre- and post-decision states. For
a steady-state problem (but retaining the time-indexing on the states and actions,
so the sequencing is clear), these equations are given by
V (St) = max
at∈A

C(St, at) + γ V a(SM,a(St, at))

,
V a(Sa
t ) = E
'
V (St+1)|Sa
t
(
.
We next replace the value functions with value function approximations, where
we are going to use V(s) to approximate V (s) around the pre-decision state, and
V
a(Sa) to approximate the value around the post-decision state. Let
ˆva
t = V
a(St+1(ω)),
where St+1(ω) is a sample realization of St+1, which is to say St+1(ω) =
SM(St, at, Wt+1(ω)). ˆva
t
is a sample realization of the value of being at a
post-decision state Sa
t = SM,a(St, at) and then evolving to St+1(ω) under sample
realization ω. This would be the same as ˆq (in equation (4.6)) but we did not
capture the direct contribution from taking action at. For this reason we write ˆqt as
ˆqt = C(St, at) + γ ˆva
t .
If our contributions are random, we would use ˆC(St, at) = C(St, at, Wt+1(ω))
instead of C(St, at).
If we were using the approximate value iteration around the post-decision state,
we would perform smoothing of ˆva
t
to estimate V
a(Sa). With Q-learning (or
SARSA) we would smooth ˆqt to estimate Q(St, at). We easily see that V(St) =
maxatQ(St, at). This means that if our problem lends itself to a compact post-
decision state, Q-learning and approximate value iteration are basically the same.
4.6.7
The Post-decision State and Multidimensional Decision Vectors
The use of the post-decision state variable offers the advantage of eliminating the
embedded expectation, which is important for problems where this expectation is
difﬁcult to compute. For some problem classes it also offers the advantage that it
may be much simpler than the pre-decision state variable. But it also opens up the
door to handling multi-dimensional decision vectors.
Imagine that we have a ﬂeet of vehicles (robots, trucks, unmanned aerial vehi-
cles), with each vehicle described by its location i ∈I (we can make this a lot
more complicated, but this simple setting will do for now). Assume that we move
a vehicle to a location to serve some task at the location. Assume that the oppor-
tunity for earning a reward ˆCti at location i is random, but it becomes known at
time t. Let
Rti = number of vehicles at location i at time t,
xtij = number of vehicles we send from i to j at time t,
cij = cost of sending a vehicle from i to j,

the post-decision state variable
143
ˆCti = contribution earned for each vehicle at i at time t,
which is random before time t.
Our state vector at time t is given by St = (Rt, ˆCt) where Rt = Rti)i∈I and ˆCt =
( ˆCti)i∈I. The dynamics are given by
Rt+1,j =

i
xtij,
(4.22)
and our decisions are constrained by

i
xtij = Rti,
(4.23)
xtij ≥0.
(4.24)
Let Xt be the region deﬁned by constraints (4.22) and (4.23). Equation (4.21)
deﬁnes the transition function for our resources, which we can also represent
using Rt+1 = RM(Rt, xt), which is deterministic. The system transition function
SM(St, xt, Wt+1) is stochastic, where Wt+1 includes the realization of the contri-
butions ˆCt+1, but this does not affect our resource transition function.
The contribution function is given by
C(St, x) =

i
ˆCtiRti −

i

j
cijxtij.
We can only receive the contribution ˆCti if the vehicles are already at i.
Using approximate dynamic programming, we could in principle ﬁnd a decision
xt via
xt = arg min
x∈Xt

C(St, x) + γ E{Vt+1(St+1)|St}

.
The post-decision state for this problem is St = Rx
t , where Rx
t = RM(Rt, xt), given
by equation (4.21). Note that since the resource transition function is deterministic,
Rt+1 = Rx
t . If we approximate the value function around the post-decision state,
we would solve
xt = arg min
x∈Xt

C(St, x) +Vt(Rx
t )

.
Now we have to ﬁnd a good approximation for Vt(Rx
t ). A simple strategy is to use
a function that is linear in Rx
t , given by
Vt(Rx
t ) =

i∈I
vtiRx
ti.
This would then produce the following optimization problem:
xt = arg min
x∈Xt


i
ˆCtiRti −

i

j
cijxtij +

i∈I
vtiRx
ti

.
(4.25)

144
introduction to approximate dynamic programming
The optimization problem in (4.24) is a fairly simple linear program. So handling
vectors xt with hundreds or thousands of dimensions is relatively easy. We have
made the sudden transition from problems that are characterized by a small number
of discrete actions to problems with decision vectors with a thousand dimensions.
Chapter 14 describes a number of applications, including the use of ADP in pro-
duction applications in industry.
4.7
LOW-DIMENSIONAL REPRESENTATIONS OF
VALUE FUNCTIONS
Classical dynamic programming typically assumes a discrete representation of the
value function. This means that for every state s ∈S, we have to estimate a param-
eter vs that gives the value of being in state s. Forward dynamic programming may
eliminate the loop over all states that is required in backward dynamic program-
ming, but it does not solve the classic “curse of dimensionality” in the state space.
Forward dynamic programming focuses attention on the states that we actually
visit, but it also requires that we have some idea of the value of being in a state
that we might visit (we need this estimate to conclude that we should not visit the
state).
Virtually every large-scale problem in approximate dynamic programming will
focus on determining how to approximate the value function with a smaller number
of parameters. In backward discrete dynamic programming, we have one parameter
per state, and we want to avoid searching over a large number of states. In forward
dynamic programming, we depend on Monte Carlo sampling, and the major issue
is statistical error. It is simply easier to estimate a function that is characterized by
fewer parameters.
In practice, approximating value functions always requires understanding the
structure of the problem. However, there are general strategies that emerge. Below
we discuss two of the most popular.
4.7.1
Aggregation
In the early days of dynamic programming, aggregation was quickly viewed as
a way to provide a good approximation with a smaller state space, allowing the
tools described in Chapter 3 to be used. A major problem with aggregation when
we use the framework in Chapter 3 is that we have to solve the entire problem
in the aggregated state space. With approximate dynamic programming, we only
have to aggregate for the purpose of computing the value function approximation.
It is extremely valuable that we can retain the full state variable for the purpose of
computing the transition function, costs, and constraints. In the area of approximate
dynamic programming, aggregation provides a mechanism for reducing statistical
error. Thus, while it may introduce structural error, it actually makes a model more
accurate by improving statistical robustness.

low-dimensional representations of value functions
145
■
EXAMPLE 4.1
Consider the problem of evaluating a basket of securities where ˆpti is the price
of the ith security at time t, where the state variable St = ( ˆpti)i is the vector
of all the prices. We can discretize the prices, but we can dramatically simplify
the problem of estimating Vt(St) if we use a fairly coarse discretization of the
prices. We can discretize prices for the purpose of estimating the value function
without changing how we represent prices of each security as we step forward
in time.
■
■
EXAMPLE 4.2
A trucking company that moves loads over long distances has to consider the
proﬁtability of assigning a driver to a particular load. Estimating the value of
the load requires estimating the value of a driver at the destination of the load.
We can estimate the value of the driver at the level of the 5-digit zip code
of the location, or the 3-digit level, or at the level of a region (companies
typically represent the United States using about 100 regions, which is far
more aggregate than a 3-digit zip). The value of a driver in a location may
also depend on his home domicile, which can also be represented at several
levels of aggregation.
■
Aggregation is particularly powerful when there are no other structural properties
to exploit, which often arises when the state contains dimensions that are categorical
rather than numerical. When this is the case, we typically ﬁnd that the state space S
does not have any metric to provide a “distance” between two states. For example,
we have an intuitive sense that a disk drive company and a company that makes
screens for laptops both serve the personal computer industry. We would expect
that valuations in these two segments would be more closely correlated than they
would be with a textile manufacturer. But we do not have a formal metric that
measures this relationship.
In Chapter 12 we investigate the statistics of aggregation in far greater detail.
4.7.2
Continuous Value Function Approximations
In many problems the state variable does include continuous elements. Consider a
resource allocation problem where Rti is the number of resources of type i ∈I. Rti
may be either continuous (how much money is invested in a particular asset class,
and how long has it been invested there) or discrete (the number of people with a
particular skill set), but it is always numerical. The number of possible values of a
vector Rt = (Rti)i can be huge (the curse of dimensionality). Now consider what
happens when we replace the value function Vt(Rt) with a linear approximation of
the form
Vt =

i∈I
vtiRti.

146
introduction to approximate dynamic programming
Instead of having to estimate the value Vt(Rt) for each possible vector Rt, we have
only to estimate vti, one for each value of i ∈I. For some problems, this reduces
the size of the problem from 10100 or greater to one with several hundred or several
thousand parameters.
Not all problems lend themselves to linear-in-the-resource approximations, but
other approximations may emerge. Early in the development of dynamic program-
ming, Bellman realized that a value function could be represented using statistical
models and estimated with regression techniques. To illustrate, let V(R|θ) be a sta-
tistical model where the elements of Rt are used to create the independent variables,
and θ is the set of parameters. For example, we might specify
V(R|θ) =

i∈I

θ1iRi + θ2i(Ri)2
.
The formulation and estimation of continuous value function approximations is
one of the most powerful tools in approximate dynamic programming. The funda-
mentals of this approach are presented in considerably more depth in Chapters 8,
9 and 10.
4.7.3
Algorithmic Issues
The design of an approximation strategy involves two algorithmic challenges. First,
we have to be sure that our value function approximation does not unnecessarily
complicate the solution of the myopic problem. We have to assume that the myopic
problem is solvable in a reasonable period of time. If we are choosing our decision
at by enumerating all possible decisions (a “lookup table” decision function), then
this is not an issue, but if at is a vector, then this is not going to be possible. If
our myopic problem is a linear or nonlinear program, it is usually impossible to
consider a value function that is of the discrete lookup table variety. If our myopic
problem is continuously differentiable and concave, we do not want to introduce
a potentially nonconcave value function. By contrast, if our myopic problem is
a discrete scheduling problem that is being solved with a search heuristic, then
lookup table value functions can work just ﬁne.
Once we have decided on the structure of our functional approximation, we have
to devise an updating strategy. Value functions are basically statistical models that
are updated using classical statistical techniques. However, it is very convenient
when our updating algorithm is in a recursive form. A strategy that ﬁts a set of
parameters by implementing a sequence of observations using standard regression
techniques may be too expensive for many applications.
4.8
SO JUST WHAT IS APPROXIMATE DYNAMIC PROGRAMMING?
Approximate dynamic programming can be viewed from four very different per-
spectives. Depending on the problem you are trying to solve, ADP can be viewed
as a way of improving an observable, physical process; an algorithmic strategy for
solving complex dynamic programs; a way of making classical simulations more
intelligent; and a decomposition technique for large-scale mathematical programs.

so just what is approximate dynamic programming?
147
4.8.1
Reinforcement Learning
Approximate dynamic programming, as it is practiced under the umbrella of “rein-
forcement learning,” has long been viewed as a way of learning the state-action
pairs that produce the best results. In this view we have a physical process that
allows us to observe transitions from St to St+1 (model-free dynamic program-
ming). In a state St, we can choose actions according to a policy Aπ(s), and then
learn the value of following this policy. We can use various strategies to help us
update the policy Aπ(s).
4.8.2
ADP for Solving Complex Dynamic Programs
In the research community, approximate dynamic programming is primarily viewed
as a way of solving dynamic programs that suffer from “the curse of dimensionality”
(in this book, the three curses). Faced with the need to solve Bellman’s equation,
we have to overcome the classical problem of computing the value function V (S)
when the number of states S is too large to enumerate. We also consider problems
where the expectation cannot be computed, and where the action space is too
large to enumerate. The remainder of this book presents modeling and algorithmic
strategies for overcoming these problems.
4.8.3
ADP as an “Optimizing Simulator”
Part of the power of approximate dynamic programming is that it is basically
a form of classical simulation with more intelligent decision rules. Almost any
ADP algorithm looks something like the algorithm depicted in Figure 4.8, which
includes two major steps: an optimization step, where we choose a decision, and
a simulation step, where we capture the effects of random information. Viewed in
this way, ADP is basically an “optimizing simulator.” The complexity of the state
variable St and the underlying physical processes, captured by the transition func-
tion SM(Sa
t , xt, Wt+1), are virtually unlimited (as is the case with any simulation
model). We are, as a rule, limited in the complexity of the value function approxi-
mation Vt(Sa
t ). If Sa
t is too complex, we have to design a functional approximation
Step 0. Given an initial state S1
0 and value function approximations V
0
t (St) for all St and t,
set n = 1.
Step 1. Choose a sample path ωn.
Step 2. For t = 0, 1, 2, . . . , T do:
Step 2a. Optimization: Compute a decision an
t = Aπ
t (St) and ﬁnd the post-decision state
Sa,n
t
= SM,a(Sn
t , an
t ).
Step 2b. Simulation: Find the next pre-decision state using Sn
t = SM(Sn
t , an
t , Wt+1(ωn)).
Step 3. Update the value function approximation to obtain V
n
t (St) for all t.
Step 4. If we have not met our stopping rule, let n = n+1 and go to step 1.
Figure 4.8
Generic approximate dynamic programming algorithm.

148
introduction to approximate dynamic programming
that uses only the most important features. But it is critical in many applications
that we do not have to similarly simplify the state variable used to compute the
transition function.
There are many complex, industrial applications where it is very important to
capture the physical process at a high level of detail, but where the quality of
the decisions (i.e., how close the decisions are to the optimal decisions) is much
harder to measure. A modest relaxation in the optimality of the decisions may be
difﬁcult or impossible to measure, whereas simpliﬁcations in the state variable are
quickly detected because the system simply does not evolve correctly. It has been
our repeated experience in many industrial applications that it is far more important
to capture a high degree of realism in the transition function than it is to produce
truly optimal decisions.
4.8.4
ADP as a Decomposition Technique for Large-Scale Math Programs
Our foray into approximate dynamic programming began with our efforts to solve
very large-scale linear (and integer) programs that arise in a range of transportation
applications. Expressed as a linear program (and switching to a decision vector x),
these would look like
max
(xt)t=0,...,T
T

t=0
ctxt
subject to
A0x0 = R0,
Atxt −Bt−1xt−1 = ˆRt,
t = 1, . . . , T,
Dtxt ≤ut,
t = 1, . . . , T,
x ≥0
an integer.
Here R0 is the initial inventories of vehicles and drivers (or crews or pilots), while
ˆRt is the (deterministic) net inﬂow or outﬂow of resources to or from the system.
At captures ﬂow conservation constraints, while Bt−1 tells us where resources that
were moved at time t−1 end up in the future. Dt tells us which elements of xt are
limited by an upper bound ut.
In industrial applications, xt might easily have 10,000 elements, deﬁned over
50 time periods. At might have thousands of rows, or hundreds of thousands
(depending on whether we were modeling equipment or drivers).
Formulated as a single, large linear (or integer) program (over 50 time peri-
ods), we obtain mathematical programs with hundreds of thousands of rows and
upward of millions of columns. As this book is being written, there are numerous
researchers attacking these problems using a wide range of heuristic algorithms
and decomposition strategies. Yet our ﬁnding has been that even these large-scale
models ignore a number of operating issues (even if we ignore uncertainty).

experimental issues
149
ADP would solve this problem by solving sequences of problems that look like
max
xt

ctxt +Vt+1(Rt+1(xt))

subject to, for t = 1, . . . , T,
Atxt = Rt + ˆRt,
Rt+1 −Btxt = 0,
Dtxt ≤ut,
x ≥0
an integer,
where Rt = Bt−1xt−1 and Vt+1(Rt+1) is some sort of approximation that captures
the impact of decision at time t on the future. After solving the problem at time
t, we would compute Rt+1, increment t, and solve the problem again. We would
run repeated simulations over the time horizon while improving our approximation
Vt(Rt).
In this setting it is quite easy to introduce uncertainty, but the real value is
that instead of solving one extremely large linear program over many time periods
(something that even modern optimization packages struggle with), we solve the
problem one time period at a time. Our ﬁnding is that commercial solvers han-
dle these problems quite easily, even for the largest and most complex industrial
applications. The price is that we have to design an effective approximation for
Vt(Rt).
4.9
EXPERIMENTAL ISSUES
Once we have developed an approximation strategy, we have the problem of testing
the results. Two issues tend to dominate the experimental side: rate of convergence
and solution quality. The time required to execute a forward pass can range between
microseconds and hours, depending on the underlying application. You may be able
to assume that you can run millions of iterations, or you may have to work with
a few dozen. The answer depends on both your problem and the setting. Are you
able to run for a week on a large parallel processor to get the very best answer? Or
do you need real-time response on a purchase order or to run a “what if” simulation
with a user waiting for the results?
Within our time constraints we usually want the highest quality solution possible.
Here the primary interest tends to be in the rate of convergence. Monte Carlo
methods are extremely ﬂexible, but the price of this ﬂexibility is a slow rate of
convergence. If we are estimating a parameter from statistical observations drawn
from a stationary process, the quality of the solution tends to improve at a rate
proportional to the square root of the number of observations. Unfortunately, we
typically do not enjoy the luxury of stationarity. We usually face the problem of
estimating a value function that is steadily increasing or decreasing (it might well be
increasing for some states and decreasing for others) as the learning process evolves.

150
introduction to approximate dynamic programming
4.9.1
The Initialization Problem
All of the strategies above depend on setting an initial estimate V
0 for the value
function approximation. In Chapter 3 the initialization of the value function did
not affect our ability to ﬁnd the optimal solution; it only affected the rate of
convergence. In approximate dynamic programming it is often the case that the
value function approximation V
n−1 used in iteration n affects the choice of the
action taken and therefore the next state visited.
Consider the shortest path problem illustrated in Figure 4.9a, where we want to
ﬁnd the best path from node 1 to node 7.
Assume that at any node i, we choose the link that goes to node j, which
minimizes the cost cij from i to j, plus the cost vj of getting from node j to the
destination. If ˆvi = minj(cij + vj) is the value of the best decision out of node
i, then we will set vi = ˆvi. If we start with a low initial estimate for each v
(Figure 4.9a), we ﬁrst explore a more expensive path (Figure 4.9b) before ﬁnding
the best path (Figure 4.9c) where the algorithm stops.
Now assume we start with values for vj that are too high, as illustrated in
Figure 4.9d. We ﬁrst explore the lower path (Figure 4.9e), but we never ﬁnd the
better path.
If we have a deterministic problem and start with an optimistic estimate of
the value of being in a state (too low if we are minimizing, too high if we are
maximizing), then we are guaranteed to eventually ﬁnd the best solution. This
is a popular algorithm in the artiﬁcial intelligence community known as the A∗
algorithm (pronounced “A star”). However, we generally can never guarantee this
in the presence of uncertainty. Just the same, the principle of starting with optimistic
estimates remains the same. If we have an optimistic estimate of the value of being
in a state, we are more likely to explore that state. The problem is that if the
estimates are too optimistic, we may end up exploring too many states.
4.9.2
State Sampling Strategies
There are several strategies for sampling states in dynamic programming. In
Chapter 3 we used synchronous updating because we updated the value of all
states at the same time. The term is derived from parallel implementations of the
algorithm, where different processors might independently (but at the same time,
hence synchronously) update the value of being in each state. In approximate
dynamic programming, synchronous updating would imply that we are going to
loop over all the states, updating the value of being in each state (potentially
on different processors). An illustration of synchronous approximate dynamic
programming is given in Figure 4.10 for an inﬁnite horizon problem. Note that at
iteration n, all decisions are made using V
n−1(s).
Asynchronous dynamic programming, to the contrary, assumes that we are
updating one state at a time, after which we update the entire value function.
In asynchronous ADP we might choose states at random, allowing us to ensure
that we sample all states inﬁnitely often. The term “asynchronous” is based on par-
allel implementations where different processors might be updating different states

experimental issues
151
(a)
(b)
(c)
(d)
(e)
1
2
3
6
5
4
1
3
2
3
1
10
0
v
0
v
0
v
0
v
7
0
v
15
7
1
2
3
6
5
4
1
3
2
3
1
10
0
v
0
v
0
v
7
15
7
17
v
7
v
1
2
3
6
5
4
1
3
2
3
1
10
1
v
4
v
6
v
17
v
7
7
v
15
7
1
2
3
6
5
4
1
3
2
3
1
10
50
v
7
15
7
50
v
50
v
50
v
50
v
1
2
3
6
5
4
1
3
2
3
1
10
50
v
7
15
7
50
v
50
v
17
v
7
v
Figure 4.9
Shortest path problems with low (a, b, c) and high (d, e) initial estimates of the cost from
each node to the destination. (a) Shortest path problem with low initial estimates for the value at each
node. (b) After one pass; updates cost to destination along this path. (c) After second pass; ﬁnds optimal
path. (d) Shortest path problem with high initial estimates for the value at each node. (e) After one
pass, ﬁnds cost along more expensive path, but never ﬁnds best path.

152
introduction to approximate dynamic programming
Step 0. Initialize an approximation for the value function V
0(S) for all states S. Let n = 1.
Step 1. For each state s ∈S, do
ˆvn(s) = max
a∈A

C(s, a) + γ

s′∈S
P(s′|s, a)V
n−1(s′)

.
Step 2. Use the estimate ˆvn(s) to update V
n−1(s), giving us V
n(s).
Step 3. Let n = n+1. If n<N , go to step 1.
Figure 4.10
Synchronous dynamic programming.
Step 0. Initialize an approximation for the value function V
0(S) for all states S. Let n = 1.
Step 1. Randomly choose a state sn.
Step 2. Solve
ˆvn = max
a∈A

C(sn, a) + γ

s′∈S
P(s′|sn, a)V
n−1(s′)

.
Step 3. Use ˆvn to update the approximation V
n−1(s) for all s.
Step 4. Let n = n+1. If n<N , go to step 1.
Figure 4.11
Asynchronous approximate dynamic programming.
without any guarantee of the order or timing with which states are being updated.
The procedure is illustrated in Figure 4.11.
4.9.3
Exploration versus Exploitation
Closely related to the initialization problem is the classical issue of “exploration”
versus “exploitation.” Exploration implies visiting states speciﬁcally to obtain better
information about the value of being in a state, regardless of whether the decision
appears to be the best given the current value function approximation. Exploitation
means using our best estimate of the contributions and values, and making decisions
that seem to be the best given the information we have (we are “exploiting” our
estimates of the value function). Of course, exploiting our value function to visit
a state is also a form of exploration, but the literature typically uses the term
“exploration” to mean that we are visiting a state in order to improve our estimate
of the value of being in that state.
There are two ways to explore. After we update the value of one state, we might
then choose another state at random, without regard to the state that we just visited
(or the action we chose). The second is to randomly choose an action (even it is
not the best) that leads us to a somewhat randomized state. The value of the latter
is that it constrains our search to states that can be reasonably reached, avoiding
the problem of visiting states that can never be reached.

experimental issues
153
There is a larger issue when considering different forms of exploration. We have
introduced several problems (the blood management problem in Section 1.2, the
dynamic assignment problem in Section 2.2.10, and the asset acquisition problem
in Section 4.1) where both the state variable and the decision variable are multi-
dimensional vectors. For these problems the state space and the action space are
effectively inﬁnite. Randomly sampling a state or even an action for these problems
makes little sense.
Determining the right balance between exploring states just to estimate their
values, along with using current value functions to visit the states that appear to be
the most proﬁtable, represents one of the great unsolved problems in approximate
dynamic programming. If we only visit states that appear to be the most proﬁtable
given the current estimates (a pure exploitation strategy), then we run the risk
of landing in local optima unless the problem has special properties. There are
strategies that help minimize the likelihood of being trapped in a local optima
but at a cost of very slow convergence. Finding good strategies with fairly fast
convergence appears to depend on taking advantage of natural problem structure.
This topic is covered in considerably more depth in Chapter 12.
4.9.4
Evaluating Policies
A common question is whether a policy Aπ1 is better than another policy Aπ2.
Suppose that we are facing a ﬁnite horizon problem that can be represented by the
objective function
F π = E
 T

t=0
γ tCt(St, Aπ
t (St))

.
Since we cannot compute the expectation, we might choose a sample ˆ ⊆ and
then calculate
ˆF π(ω) =
T

t=0
γ tCt(Aπ
t (St(ω))),
F
π =

ω∈ˆ
ˆp(ω) ˆF π(ω),
where ˆp(ω) is the probability of the outcome ω ∈ˆ. If we have chosen the out-
comes in ˆ at random from within , then, letting N = | ˆ| be our sample size,
we would use
ˆp(ω) =
1
| ˆ|
= 1
N .
Alternatively, we may choose ˆ so that we control the types of outcomes in
 that are represented in ˆ. Such sampling strategies fall under names such as
stratiﬁed sampling or importance sampling. They require that we compute the

154
introduction to approximate dynamic programming
sample probability distribution ˆp to reﬂect the proper frequency of an outcome
ω ∈ˆ within the larger sample space .
The choice of the size of ˆ should be based on a statistical analysis ofF
π. For
a given policy π, it is possible to compute the variance ofF
π( ˆ) using

σ π2 = 1
N

1
N −1
 
ω∈ˆ

ˆF π(ω) −F
π2
.
This formula assumes that we are sampling outcomes “at random” from , which
means that they should be equally weighted. More effective strategies will use
sampling strategies that will overrepresent certain types of outcomes.
In most applications it is reasonable to assume that (σ π)2 is independent of the
policy, allowing us to use a single policy to estimate the variance of our estimate. If
we treat the estimates ofF
π1 andF
π2 as independent random variables the variance
of the difference is 2(σ π)2. If we are willing to assume normality, we can then
compute a conﬁdence interval on the difference using
4
(F
π1 −F
π2) −zα/2
	
2(σ π)2, (F
π1 −F
π2) + zα/2
	
2(σ π)2
5
,
(4.26)
where zα/2 is the standard normal deviate for a conﬁdence level α.
Typically we can obtain a much tighter conﬁdence interval by using the same
sample ˆ to test both policies. In this case F
π1 and F
π2 will not be independent,
and may in fact be highly correlated (in a way we can use to our beneﬁt). Instead
of computing an estimate of the variance of the value of each policy, we should
compute a sample realization of the difference
ˆπ1,π2(ω) = ˆF π1(ω) −ˆF π2(ω),
from which we can compute an estimate of the difference

π1,π2 =

ω∈ˆ
ˆp(ω) ˆπ1,π2(ω)
=F
π1 −F
π2.
When comparing two policies, it is important to compute the variance of the esti-
mate of the difference to see if it is statistically signiﬁcant. If we evaluate each
policy using a different set of random outcomes (e.g., ω1 and ω2), the variance of
the difference would be given by
Var
4
ˆπ1,π2
5
= Var
4
ˆF π1
5
+ Var
4
ˆF π2
5
.
(4.27)
This is generally not the best way to estimate the variance of the difference between
two policies. It is better to evaluate two policies using the same random sample for
each policy. In this case ˆF π1(ω) and ˆF π2(ω) are usually correlated, which means
the variance would be
Var
4
ˆπ1,π2
5
= Var
4
ˆF π1
5
+ Var
4
ˆF π2
5
−2 Cov
4
ˆF π1, ˆF π2
5
.

but does it work?
155
The covariance is typically positive, so this estimate of the variance will be smaller
(and possibly much smaller). One way to estimate the variance is to compute
ˆπ1,π2(ω) for each ω ∈ˆ and then compute
σ π1,π2 = 1
N

1
N −1
 
ω∈ˆ

ˆπ1,π2(ω) −
π2
.
In general, σ π1,π2 will be much smaller than 2(σ π)2, which we would obtain if we
chose independent estimates.
For some large-scale experiments it will be necessary to perform comparisons
using a single sample realization ω. In fact this is the strategy that would typically
be used if we were solving a steady-state problem. However, the strategy can be
used for any problem where the horizon is sufﬁciently long that the variance of
an estimate of the objective function is not too large. We again emphasize that
the variance of the difference in the estimates of the contribution of two different
policies may be much smaller than would be expected if we used multiple sample
realizations to compute the variance of an estimate of the value of a policy.
4.10
BUT DOES IT WORK?
The technique of stepping forward through time using Monte Carlo sampling is
a powerful strategy, but it effectively replaces the challenge of looping over all
possible states with the problem of statistically estimating the value of being in
“important states.” Furthermore it is not enough just to get reasonable estimates
for the value of being in a state. We have to get reasonable estimates for the value
of being in states we might want to visit.
There is a growing body of theory addressing the issue of convergence proofs.
As of this writing, formal proofs of convergence are limited to a small number of
very specialized algorithms. For lookup table representations of the value function,
it is not possible to obtain convergence proofs without a guarantee that all states
will be visited inﬁnitely often. This precludes pure exploitation algorithms where
we only use the decision that appears to be the best (given the current value function
approximation).
Compounding this lack of proofs is experimental work that illustrates cases
where the methods simply do not work. What has emerged from the various labo-
ratories doing experimental work are two themes:
• The functional form of an approximation has to reasonably capture the true
value function.
• For large problems it is essential to exploit the structure of the problem so
that a visit to one state provides improved estimates of the value of visiting a
large number of other states.
For example, a discrete lookup table function will always capture the general shape
of a (discrete) value function, but it does little to exploit what we have learned

156
introduction to approximate dynamic programming
from visiting one state in terms of updated estimates of the value of visiting other
states. As a result it is quite easy to design an approximate dynamic programming
strategy (e.g., using a lookup table value function) that either does not work at all
or provides a suboptimal solution that is well below the optimal solution.
At the same time approximate dynamic programming has proved itself to be
an exceptionally powerful tool in the context of speciﬁc problem classes. This
chapter has illustrated ADP in the context of very simple problems, but it has been
successfully applied to very complex resource allocation problems that arise in
some of the largest transportation companies. Approximate dynamic programming
(and reinforcement learning, as it is still called in artiﬁcial intelligence) has proved
to be very effective in problems ranging from playing games (e.g., backgammon)
to solving engine control problems (e.g., managing fuel mixture ratios in engines).
It is our belief that general-purpose results in approximate dynamic programming
will be few and far between. Our experience suggests instead that most results will
involve taking advantage of the structure of a particular problem class. Identifying
a value function approximation, along with a sampling and updating strategy, that
produces a high-quality solution represents a major contribution to the ﬁeld in
which the problem arises. The best we can offer in a general textbook on the
ﬁeld is to provide guiding principles and general tools, allowing domain experts to
devise the best possible solution for a particular problem class. We suspect that an
ADP strategy applied to a problem context is probably a patentable invention.
4.11
BIBLIOGRAPHIC NOTES
This chapter can be viewed as an introduction to approximate dynamic program-
ming, but very much from the perspective adopted in the reinforcement learning
community. The entire chapter assumes discrete states and actions, deferring until
later the algorithms for handling more complex states and actions.
Section 4.2
The core ideas of approximate dynamic programming and rein-
forcement learning date to the 1950s, but primarily to work done in the 1980s
and 1990s. See Bertsekas and Tsitsiklis (1996) and Sutton and Barto (1998).
There is a very rich literature on generating random variables. An easy intro-
duction to simulation is given in Ross (2002). More extensive treatments are
given in Banks et al. (1996) and Law and Kelton (2000).
Section 4.3
Q-learning was ﬁrst introduced by Watkins (1989) and Watkins
and Dayan (1992), and it is probably the most widely studied algorithm in
reinforcement learning. See Sutton and Barto (1998), Bertsekas and Tsitsiklis
(1996), and the references cited there.
Section 4.4
Real-time dynamic programming was introduced in Barto et al.
(1995).
Section 4.5
Approximate value iteration has been widely studied under the
name temporal-difference learning, and it is closely related to Q-learning,
especially when lookup tables are used. Bertsekas and Tsitsiklis (1996)

bibliographic notes
157
Table 4.4
Sampling of convergence proofs for ADP/RP
State
Action
Reward
Exp.
Noise
Policy
Conv.
Fixed Policy
Bradtke and Barti (1996)
D
D
G
N
S
FP
Y
Tsitsiklis (1994)
D
D
G
Y
S
FP
Y
Tsitsiklis and van Roy (1997)
D
D
G
Y
S
FP
Y
Papavassiliou and Russell (1999)
D
D
G
N
S
FP
Y
Precup (2001)
D
D
G
N
S
FP
Y
Melo and Ribeiro (2007)
C
D
G
N
S
FP
Y
Sutton et al. (2009b)
D
D
G
N
S
FP
Y
Sutton et al. (2009a)
D
D
G
N
S
FP
Y
Optimizing Policies, Discrete States
Watkins and Dayan (1992)
D
D
G
S
S
VI
Y
Baird (1995)
D
D
G
N
D
VI
Y
Tsitsiklis and Roy (1996)
D
D
G
Y
S
VI
Y
Gordon (2001)
D
D
G
N
S
VI
Y
Lagoudakis and Parr (2003)
D
D
G
N
S
API
N
Precup and Perkins (2003)
D
D
G
N
S
API
Y
Deisenroth et al. (2008)
D
D
G
Y
S
VI
N
Gordon (1995)
D
D
G
Y
S
VI
Y
Optimizing Policies, Continuous States
Bradtke et al. (1994)
C
C
Q
N
D
API
Y
Landelius and Knutsson (1997)
C
C
Q
N
D
VI/API
Y
Meyn (1997)
C
D
G
Y
S
EPI
Y
Ormoneit and Sen (2002)
C
D
G
N
S
VI
Y
Engel et al. (2005)
C
C
G
N
S
API
N
Munos and Szepesvari (2008)
C
D
G
N
S
VI
B
Szita (2007)
C
C
Q
N
S(G)
VI
Y
Antos et al. (2007)
C
C
G
N
S
API
B
Antos et al. (2008a)
C
D
G
N
S
API
B
Antos et al. (2008b)
C
D
G
N
S
API
B
Source: Adapted from Ma and Powell (2010b).
discuss several variations of approximate value iteration using lookup tables
or state aggregations (where the value of being in a set of states is a
constant) and establish conditions for convergence.
Section 4.6 Virtually every textbook on dynamic programming (or approximate
dynamic programming) sets up Bellman’s equations around the pre-decision
state variable. However, a number of authors have found that some problems
are naturally formulated around the post-decision state variable. The ﬁrst use
of the term “post-decision state variable” that we have found is in Bertsekas
et al. (1997), although uses of the post-decision state variable date as far
back as Bellman (1957). The ﬁrst presentation describing the post-decision

158
introduction to approximate dynamic programming
state as a general solution strategy (as opposed to a technique for a special
problem class) appears to be Powell and van Roy (2004). This book is the
ﬁrst book to systematically use the post-decision state variable as a way of
approximating dynamic programs.
Section 4.7
The vast majority of work in approximate dynamic programming
involves representing functions with fewer parameters. The use of aggregation
has been suggested since the origins of dynamic programming in the 1950s,
as has the use of statistical models, in Bellman and Dreyfus (1959).
Section 4.8
The A* algorithm is presented in a number of books on artiﬁcial
intelligence. See, for example, Pearl (1984). The concept of synchronous and
asynchronous dynamic programming is based on Bertsekas (1982) and in
particular Bertsekas and Tsitsiklis (1989).
PROBLEMS
4.1
Let U be a random variable that is uniformly distributed between 0 and 1.
Let R = −1
λ ln U. Show that P[R ≤x] = 1 −e−λx, which shows that R has
an exponential distribution.
4.2
Let R = U1 + U2, where U1 and U2 are independent, uniformly distributed
random variables between 0 and 1. Derive the probability density function
for R.
4.3
Let Z be a normally distributed random variable with mean 0 and variance
1, and let U be a uniform [0, 1] random variable. Let (z) = P(Z ≤z)
be the cumulative distribution, and let −1(u) be the inverse cumulative.
Then R = −1(U) is also normally distributed with mean 0 and variance
1. Use a spreadsheet to randomly generate 10,000 observations of U and
the associated observations of R. Let N (z) be the number of observa-
tions of R that are less than z. Compare N (z) to 10000(z) for z = −2,
−1, −0.5, 0, 0.5, 1, 2. [Note that in a spreadsheet, RAND( ) generates
a uniform [0,1] random variable, (z) = NORMSDIST(z) and −1(u) =
NORMSINV(u).]
4.4
Let X be a continuous random variable. Let F(x) = 1 −P[X ≤x], and
let F −1(y) be its inverse (i.e., if y = F(x), then F −1(y) = x). Show that
F −1(U) has the same distribution as X.
4.5
You are holding an asset that can be sold at time t = 1, 2, . . . , 10 at a
price ˆpt that ﬂuctuates from period to period. Assume that ˆpt is uniformly
distributed between 0 and 10. Further assume that ˆpt is independent of prior
prices. You have 10 opportunities to sell the asset, and you must sell it by
the end of the 10th time period. Let xt be the decision variable, where xt = 1
means sell and xt = 0 means hold. Assume that the reward you receive is
the price that you receive when selling the asset (10
t=1 xtpt).

problems
159
(a) Deﬁne the pre- and post-decision state variables for this problem. Plot
the shape of the value function around the pre- and post-decision state
variables.
(b) Set up the optimality equations, and show that there exists a price pt
where we will sell if pt > pt. Also show that pt ≥pt+1.
(c) Use backward dynamic programming to compute the optimal value func-
tion for each time period.
(d) Use an approximate dynamic programming algorithm with a pre-
decision state variable to estimate the optimal value function, and give
your estimate of the optimal policy (i.e., at each price, should we sell or
hold). Note that even if you sell the asset at time t, you need to stay in the
state that you are holding the asset for t+1 (we are not interested in esti-
mating the value of being in the state that we have sold the asset). Run
the algorithm for 1000 iterations using a stepsize of αn−1 = 1/n, and
compare the results to that obtained using a constant stepsize α = 0.20.
4.6
Suppose that we have an asset selling problem where we can sell at price
pt that evolves according to
pt+1 = pt + 0.5(120 −pt) + ˆpt+1,
where p0 = 100 and ˆpt+1 is uniformly distributed between −10 and +10.
Say we have to sell the asset within the ﬁrst 10 time periods. Solve the
problem using approximate dynamic programming. Unlike 4.5, now pt is
part of the state variable, and we have to estimate Vt(pt) (the value of
holding the asset when the price is pt). Since pt is continuous, deﬁne your
value function approximation by discretizing pt to obtain pt (e.g., rounded
to the nearest dollar or nearest ﬁve dollars). After training the value function
for 10,000 iterations, run 1000 samples (holding the value function ﬁxed)
and determine when the model decided to sell the asset. Plot the distribution
of times that the asset was held over these 1000 realizations. If δ is your
discretization parameter (e.g., δ = 5 means rounding to the nearest 5 dollars),
compare your results for δ = 1, 5, and 10.
4.7
Here we are going to solve a variant of the asset-selling problem using a
post-decision state variable. Suppose that we are holding a real asset and
we are responding to a series of offers. Let ˆpt be the tth offer, which is
uniformly distributed between 500 and 600 (all prices are in thousands of
dollars). Assume that each offer is independent of all prior offers. You are
willing to consider up to 10 offers, and your goal is to get the highest possible
price. If you have not accepted the ﬁrst nine offers, you must accept the 10th
offer.
(a) Write out the decision function you would use in an approximate
dynamic programming algorithm in terms of a Monte Carlo sample
of the latest price and a current estimate of the value function
approximation.

160
introduction to approximate dynamic programming
(b) Use the knowledge that ˆpt is uniform between 500 and 600 and derive
the exact value of holding the asset after each offer.
(c) Write out the updating equations (for the value function) you would use
after solving the decision problem for the tth offer using Monte Carlo
sampling.
(d) Implement an approximate dynamic programming algorithm using syn-
chronous state sampling (you sample both states at every iteration).
Using 100 iterations, write out your estimates of the value of being in
each state immediately after each offer.
(e) From your value functions, infer a decision rule of the form “sell if the
price is greater than pt.”
4.8
We are going to use approximate dynamic programming to estimate
F T = E
T

t=0
γ tRt,
where Rt is a random variable that is uniformly distributed between 0 and
100 and γ = 0.7. We assume that Rt is independent of prior history. We can
think of this as a single-state Markov decision process with no decisions.
(a) Using the fact that ERt = 50, give the exact value for F 20.
(b) Let ˆvn
t = T
t′=t γ t′−tRt′(ωn), where ωn represents the nth sample path
and Rn
t′ = Rt′(ωn) is the realization of the random variable Rt′ for the
nth sample path. Show that ˆvn
t = Rn
t + γ ˆvn
t+1, which means that ˆvn
0 is a
sample realization of RT .
(c) Propose an approximate dynamic programming algorithm to estimate
F T . Give the value function updating equation, using a stepsize αn−1 =
1/n for iteration n.
(d) Perform 100 iterations of the approximate dynamic programming algo-
rithm to produce an estimate of F 20. How does this compare to the true
value?
(e) Repeat part (d) 10 times, but now use a discount factor of 0.9. Average
the results to obtain an averaged estimate. Now use these 10 calculations
to produce an estimate of the standard deviation of your average.
(f) From your answer to part (e), estimate how many iterations would be
required to obtain an estimate where 95 percent conﬁdence bounds would
be within 2 percent of the true number.
4.9
Consider a batch replenishment problem where we satisfy demand in a period
from the available inventory at the beginning of the period and then order
more inventory at the end of the period. Deﬁne both the pre- and post-
decision state variables, and write the pre-decision and post-decision forms
of the transition equations.

problems
161
4.10
A mutual fund has to maintain a certain amount of cash on hand for redemp-
tions. The cost of adding funds to the cash reserve is $250 plus $0.005 per
dollar transferred into the reserve. The demand for cash redemptions is uni-
formly distributed between $5,000 and $20,000 per day. The mutual fund
manager has been using a policy of transferring in $500,000 whenever the
cash reserve goes below $250,000. He thinks he can lower his transaction
costs by transferring in $650,000 whenever the cash reserve goes below
$250,000. However, he pays an opportunity cost of $0.00005 per dollar per
day for cash in the reserve account.
(a) Use a spreadsheet to set up a simulation over 1000 days to estimate total
transaction costs plus opportunity costs for the policy of transferring in
$500,000, assuming that you start with $500,000 in the account. Perform
this simulation 10 times, and compute the sample average and variance
of these observations. Then compute the variance and standard deviation
of your sample average.
(b) Repeat (a), but now test the policy of transferring in $650,000. Repeat
this 1000 times and compute the sample average and the standard devi-
ation of this average. Use a new set of observations of the demands for
capital.
(c) Can you conclude which policy is better at a 95 percent conﬁdence
level? Estimate the number of observations of each that you think you
would need to conclude there is a difference at a 95 percent conﬁdence
level.
(d) How does your answer to (d) change if instead of using a fresh set of
observations for each policy, you use the same set of random demands
for each policy?
4.11
Let M be the dynamic programming “max” operator:
Mv(s) = max
a

C(s, a) + γ

s′
P(s′|s, a)v(s′)

.
Let
c = max
s
(Mv(s) −v(s)) ,
and deﬁne a policy π(v) using
a(s) = arg max
a

C(s, a) + γ E

s′
P(s′|s, a)v(s′)

.
Let vπ be the value of this policy. Show that
vπ ≤v +
c
1 −γ e,
where e is a vector of 1’s.

162
introduction to approximate dynamic programming
4.12
As a broker, you sell shares of a company you think your customers will
buy. Each day you start with st−1 shares left over from the previous day, and
then buyers for the brokerage add rt new shares to the pool of shares you
can sell that day (rt ﬂuctuates from day to day). Each day customer i ∈I
calls in asking to purchase qti units at price pti, but you do not conﬁrm
orders until the end of the day. Your challenge is to determine at, which is
the number of shares you wish to sell to the market on that day. You will
collect all the orders. Then at the end of the day you must choose at, which
you will allocate among the customers willing to pay the highest price. Any
shares you do not sell on one day are available for sale the next day. Let
pt = (pti)i∈I and qt = (qti)i∈I be the vectors of price/quantity offers from
all the customers on a given day.
(a) What is the exogenous information process for this system? What is a
history for the process?
(b) Give a general deﬁnition of a state variable. Is st a valid state variable
given your deﬁnition?
(c) Set up the optimality equations for this problem using the post-decision
state variable. Be precise!
(d) Set up the optimality equations for this problem using the pre-decision
state variable. Contrast the two formulations from a computational per-
spective.
4.13
The network depicted in Figure 4.12 has an origin at node 1 and a destination
at node 4. Each link has two possible costs with a probability of each
outcome.
(a) Write out the dynamic programming recursion to solve the shortest path
problem from 1 to 4. Assuming that the driver does not see the cost on
a link until he arrives at the link (i.e., he will not see the cost on link
(2,4) until he arrives to node 2). Solve the dynamic program and give
the expected cost of getting from 1 to 4.
(b) Set up and solve the dynamic program to ﬁnd the expected shortest path
from 1 to 4, assuming that the driver sees all the link costs before he
starts the trip.
1
4
3
2
Cost     Prob
2         0.4
10         0.6
Cost     Prob
3         0.5
9         0.5
Cost     Prob
4         0.5
8         0.5
Cost     Prob
2         0.2
8         0.8
Figure 4.12
Illustration of a stochastic graph with origin node 1 and destination node 4.

problems
163
(c) Set up and solve the dynamic program to ﬁnd the expected shortest path,
assuming that the driver does not see the cost on a link until after he
traverses the link.
(d) Give a set of inequalities relating the results from parts (a), (b), and (c)
and provide a coherent argument to support your relationships.
4.14
Here we are going to again solve a variant of the asset selling problem using a
post-decision state variable, but this time we are going to use asynchronous
state sampling (in Chapter 4 we used synchronous approximate dynamic
programming). We assume that we are holding a real asset and that we are
responding to a series of offers. Let ˆpt be the tth offer, which is uniformly
distributed between 500 and 600 (all prices are in thousands of dollars). We
also assume that each offer is independent of all prior offers. You are willing
to consider up to 10 offers, and your goal is to get the highest possible price.
If you have not accepted the ﬁrst nine offers, you must accept the 10th offer.
(a) Implement an approximate dynamic programming algorithm using asyn-
chronous state sampling, initializing all value functions to zero. Using
100 iterations, write out your estimates of the value of being in each
state immediately after each offer. Use a stepsize rule of 5/(5+n−1).
Summarize your estimate of the value of each state after each offer.
(b) Compare your results against the estimates you obtain using synchronous
sampling. Which produces better results?
4.15
The taxicab problem is a famous learning problem in the artiﬁcial intelli-
gence literature. The cab enters a grid (see Figure 4.13), and at each cell, it
can go up/down or left/right. The challenge is to teach it to ﬁnd the exit as
quickly as possible. Write an approximate dynamic programming algorithm
to learn the best path to the exit where at every iteration you are allowed to
loop over every cell and update the value of being in that cell.
Figure 4.13
The taxi problem with barriers, where the goal is to get from the upper left hand corner
to the lower right hand corner while avoiding the barriers.
4.16
Repeat the taxicab exercise, but now assume you can only update the value
of the cell that you are in, using the following initialization strategies:
(a) Initialize your value function with 0 everywhere.

164
introduction to approximate dynamic programming
(b) Initialize your value function where the value of being in cell (i,j) is 10
−i, where i is the row, and i = 1 is the bottom row.
(c) Initialize your value function using the formula (10 −i) + j. This
estimate pushes the system to look down and to the right.
4.17
Nomadic trucker project. We are going to create a version of the nomadic
trucker problem using randomly generated data. Suppose that our trucker
may visit any of 20 cities in a set I. The trucker may move loaded from i
to j earning a positive reward rij as long as there is a demand ˆDtij at time
t to move a load from i to j. Alternatively, the trucker may move empty
from i to j at a cost cij.
Assume that the demands ˆDtij follow a Poisson distribution with mean
λij. Randomly generate these means by ﬁrst generating a parameter ρi for
each i, where ρi is uniformly distributed between 0 and 1. Then set λij =
θρi(1 −ρj), where µ is a scaling parameter (for this exercise, use θ = 2).
Let dij be the distance between i and j. Randomly generate distances from a
uniform distribution between 100 and 1500 miles. Now let rij = 4ρidij and
cij = 1.2dij. Assume that if our trucker is in location i, he can only serve
demands out of location i, and that any demands not served at one point
in time are lost. Further assume that it takes one day (with one time period
per day) to get from location i to location j (regardless of the distance). We
wish to solve our problem over a horizon T = 21 days.
At location i, our trucker may choose to move loaded to j if ˆDij > 0, or
to move empty to j. Let xL
tij = 1 if he moves loaded from i to j on day t,
and 0 otherwise. Similarly let xE
tij = 1 if he moves empty from i to j on day
t, and 0 otherwise. Of course, 
j(xL
tij + xE
tij) = 1. We make the decision
by solving
max
x

j

(rij +V
n−1(j))xD
tij + (−cij +V
n−1(j))xE
tij

subject to the constraint that xL
ij = 0 if ˆDtij = 0.
(a) Say our trucker starts in city 1. Use a temporal-differencing algorithm
with λ = 0 using a stepsize of αn−1 = a/(a + n −1) with a = 10. Train
the value functions for 1000 iterations. Then, holding the value functions
constant, perform an additional 1000 simulations, and report the mean
and standard deviation of the proﬁts, as well as the number of times the
trucker visits each city.
(b) Repeat part (a), but this time insert a loop over all cities, so that for
each iteration n and time t, we pretend that we are visiting every city
to update the value of being in the city. Again, perform 1000 iterations
to estimate the value function (this will now take 10 times longer), and
then perform 1000 testing iterations.

problems
165
(c) Repeat part (a), but this time, after solving the decision problem for
location i and updating the value function, randomly choose the next
city to visit.
(d) Compare your results in terms of solution quality and computational
requirements (measured by how many times you solve a decision prob-
lem).
4.18
Connect-4 project. Connect-4 is a game played where players alterna-
tively drop in chips with two colors (one for each player), as illustrated in
Figure 4.14. The grid is six high by seven wide. Chips are dropped into the
top of a column where they then fall to the last available spot. The goal is to
get four in a row (vertically, horizontally, or diagonally) of the same color.
Since this is a two-player game, you will have to train two value functions:
one for the player that starts ﬁrst, and one for the player who starts second.
Once these are trained, you should be able to play against it.
Training a value function for board games will require playing the game
a few times, and identifying speciﬁc patterns that should be captured. These
patterns become your features (also known as basis functions). These could
be indicator variables (does your opponent have three in a row?) or a numer-
ical quantity (how many pieces are in a given row or column?). The simplest
representation is a separate indicator variable for each cell, producing a value
function with 42 parameters.
Figure 4.14
Connect-4 playing grid.
You must write software that allows a human to play your model after
your value functions have been trained. Allow a person to select into which
column to place his piece (no need for fancy graphics—we can keep track
of the game board on a sheet of paper or using a real game piece).


C H A P T E R
5
Modeling Dynamic Programs
Perhaps one of the most important skills to develop in approximate dynamic pro-
gramming is the ability to write down a model of the problem. Everyone who
wants to solve a linear program learns to write out
min
x cT x
subject to
Ax = b,
x ≥0.
This modeling framework allows people around the world to express their problem
in a standard format.
Stochastic, dynamic problems are much richer than a linear program, and require
the ability to model the ﬂow of information and complex system dynamics. Just the
same, there is a standard framework for modeling dynamic programs. We provided
a taste of this framework in Chapter 2, but that chapter only hinted at the richness
of the problem class.
In Chapters 2, 3, and 4 we used fairly standard notation, and we avoided dis-
cussing some important subtleties that arise in the modeling of stochastic, dynamic
systems. We intentionally overlooked trying to deﬁne a state variable, which we
have viewed as simply St, where the set of states was given by the indexed set
S = {1, 2, . . . , |S|}. We have avoided discussions of how to properly model time
or more complex information processes. This style has facilitated introducing some
basic ideas in dynamic programming, but would severely limit our ability to apply
these methods to real problems.
The goal of this chapter is to describe a standard modeling framework for
dynamic programs, providing a vocabulary that will allow us to take on a much
Approximate Dynamic Programming: Solving the Curses of Dimensionality, Second Edition.
Warren B. Powell.
2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
167

168
modeling dynamic programs
wider set of applications. Notation is not as critical for simple problems, as long as
it is precise and consistent. But what seems like benign notational decisions for a
simple problem can cause unnecessary difﬁculties, possibly making the model com-
pletely intractable as problems become more complex. Complex problems require
considerable discipline in notation because they combine the details of the orig-
inal physical problem with the challenge of modeling sequential information and
decision processes. The modeling of time can be particularly subtle. In addition to
a desire to model problems accurately, we also need to be able to understand and
exploit the structure of the problem, which can become lost in a sea of complex
notation.
Good modeling begins with good notation. The choice of notation has to balance
traditional style with the needs of a particular problem class. Notation is easier
to learn if it is mnemonic (the letters look like what they mean) and compact
(avoiding a profusion of symbols). Notation also helps bridge communities. For
example, it is common in dynamic programming to refer to actions using “a”
(where a is discrete); in control theory a decision (control) is “u” (which may
be continuous). For high-dimensional problems it is essential to draw on the ﬁeld
of mathematical programming, where decisions are typically written as “x” and
resource constraints are written in the standard form Ax = b. In this text many
of our problems involve managing resources where we are trying to maximize or
minimize an objective subject to constraints. For this reason, we adopt, as much
as possible, the notation of math programming to help us bridge the ﬁelds of math
programming and dynamic programming.
Sections 5.1 to 5.3 provide some foundational material. Section 5.1 begins by
describing some basic guidelines for notational style. Section 5.2 addresses the
critical question of modeling time, and Section 5.3 provides notation for modeling
resources that we will use throughout the remainder of the book.
The general framework for modeling a dynamic program is covered in Sections
5.4 to 5.8. There are ﬁve elements to a dynamic program, consisting of the follow-
ing:
1. State variables. These variables describe what we need to know at a point
in time (Section 5.4).
2. Decision variables. These are the variables we control. Choosing these vari-
ables (“making decisions”) represents the central challenge of dynamic pro-
gramming (Section 5.5).
3. Exogenous information processes. These variables describe information
that arrives to us exogenously, representing the sources of randomness
(Section 5.6).
4. Transition function. This is the function that describes how the state evolves
from one point in time to another (Section 5.7).
5. Objective function. We are either trying to maximize a contribution function
(proﬁts, revenues, rewards, utility) or minimize a cost function. This function
describes how well we are doing at a point in time (Section 5.8).

notational style
169
This chapter describes modeling in considerable depth, and as a result it is
quite long. A number of sections are marked with a asterisk (*), indicating that
these can be skipped on a ﬁrst read. There is a single section marked with a
double asterisks (**) which, as with all sections marked this way, is material
designed for readers with more advanced training in probability and stochastic
processes.
5.1
NOTATIONAL STYLE
Notation is a language: the simpler the language, the easier it is to understand the
problem. As a start it is useful to adopt notational conventions to simplify the style
of our presentation. For this reason we adopt the following notational conventions:
Variables. Variables are always a single letter. We would never use, for example,
CH for “holding cost.”
Modeling time. We always use t to represent a point in time, while we use τ to
represent an interval over time. When we need to represent different points
in time, we might use t, t′, t, tmax, and so on.
Indexing vectors. Vectors are almost always indexed in the subscript, as in xij.
Since we use discrete time models throughout, an activity at time t can be
viewed as an element of a vector. When there are multiple indexes, they
should be ordered from outside in the general order over which they might
be summed (think of the outermost index as the most detailed information).
So, if xtij is the ﬂow from i to j at time t with cost ctij, we might sum
up the total cost using 
t

i

j ctijxtij. Dropping one or more indices
creates a vector over the elements of the missing indexes to the right. So
xt = (xtij)∀i,∀j is the vector of all ﬂows occurring at time t. If we write xti,
this would be the vector of ﬂows out of i at time t to all destinations j. Time,
when present, is always the innermost index.
Indexing time. If we are modeling activities in discrete time, then t is an index
and should be put in the subscript. So xt would be an activity at time t, with
the vector x = (x1, x2, . . . , xt, . . . , xT ) giving us all the activities over time.
When modeling problems in continuous time, it is more common to write
t as an argument, as in x(t). xt is notationally more compact (try writing a
complex equation full of variables as x(t) instead of xt).
Flavors of variables. It is often the case that we need to indicate different ﬂavors
of variables, such as holding costs and order costs. These are always indicated
as superscripts, where we might write ch or chold as the holding cost. Note that
while variables must be a single letter, superscripts may be words (although
this should be used sparingly). We think of a variable like “ch” as a single
piece of notation. It is better to write ch as the holding cost and cp as the
purchasing cost than to use h as the holding cost and p as the purchasing cost
(the ﬁrst approach uses a single letter c for cost, while the second approach

170
modeling dynamic programs
uses up two letters—the roman alphabet is a scarce resource). Other ways
of indicating ﬂavors is hats (ˆx), bars (x), tildes (˜x), and primes (x ′).
Iteration counters. We place iteration counters in the superscript, and we pri-
marily use n as our iteration counter. So xn is our activity at iteration n.
If we are using a descriptive superscript, we might write xh,n to represent
xh at iteration n. Sometimes algorithms require inner and outer iterations. In
this case we use n to index the outer iteration and m for the inner iteration.
While this will prove to be the most natural way to index iterations, there is
potential for confusion where it may not be clear if the superscript n is an
index (as we view it) or raising a variable to the nth power. We make one
notable exception to our policy of indexing iterations in the superscript. In
approximate dynamic programming we make wide use of a parameter known
as a stepsize α where 0 ≤α ≤1. We often make the stepsize vary with the
iterations. However, writing αn looks too much like raising the stepsize to
the power of n. Instead, we write αn to indicate the stepsize in iteration n.
This is our only exception to this rule.
Sets. To represent sets, we use capital letters in a calligraphic font, such as X, F
or I. We generally use the lowercase roman letter as an element of a set, as
in x ∈X or i ∈I.
Exogenous information. Information that ﬁrst becomes available (from outside
the system) at time t is denoted using hats, for example, ˆDt or ˆpt. Our only
exception to this rule is Wt which is our generic notation for exogenous
information (since Wt always refers to exogenous information, we do not
use a hat).
Statistics. Statistics computed using exogenous information are generally indi-
cated using bars, for example xt or Vt. Since these are functions of random
variables, they are also random.
Index variables. Throughout, i, j, k, l, m, and n are always scalar indexes.
Of course, there are exceptions to every rule. It is extremely common in the trans-
portation literature to model the ﬂow of a type of resource (called a commodity
and indexed by k) from i to j using xk
ij. Following our convention, this should be
written xkij. Authors need to strike a balance between a standard notational style
and existing conventions.
5.2
MODELING TIME
A survey of the literature reveals different styles toward modeling time. When
using discrete time, some authors start at 1 whereas others start at zero. When
solving ﬁnite horizon problems, it is popular to index time by the number of time
periods remaining, rather than elapsed time. Some authors index a variable, say St,
as being a function of information up through t −1, while others assume it includes

modeling time
171
information up through time t. t may be used to represent when a physical event
actually happens, or when we ﬁrst know about a physical event.
The confusion over modeling time arises in large part because there are two
processes that we have to capture: the ﬂow of information, and the ﬂow of physical
and ﬁnancial resources. There are many applications of dynamic programming to
deterministic problems where the ﬂow of information does not exist (everything
is known in advance). Similarly there are many models where the arrival of the
information about a physical resource, and when the information takes effect in
the physical system, are the same. For example, the time at which a customer
physically arrives to a queue is often modeled as being the same as when the
information about the customer ﬁrst arrives. Similarly we often assume that we
can sell a resource at a market price as soon as the price becomes known.
There is a rich collection of problems where the information process and physical
process are different. A buyer may purchase an option now (an information event)
to buy a commodity in the future (the physical event). Customers may call an
airline (the information event) to ﬂy on a future ﬂight (the physical event). An
electric power company has to purchase equipment now to be used one or two
years in the future. All these problems represent examples of lagged information
processes and force us to explicitly model the informational and physical events
(see Section 2.2.7 for an illustration).
Notation can easily become confused when an author starts by writing down a
deterministic model of a physical process and then adds uncertainty. The problem
arises because the proper convention for modeling time for information processes
is different than what should be used for physical processes.
We begin by establishing the relationship between discrete and continuous time.
All the models in this book assume that decisions are made in discrete time (some-
times referred to as decision epochs). However, the ﬂow of information, and many
of the physical processes being modeled, are best viewed in continuous time. A
common error is to assume that when you model a dynamic program in discrete
time then all events (information events and physical events) are also occurring
in discrete time (in some applications, this is the case). Throughout this text deci-
sions are made in discrete time, while all other activities occur in continuous
time.
The relationship of our discrete time approximation to the real ﬂow of informa-
tion and physical resources is depicted in Figure 5.1. Above the line, “t” refers to
a time interval, whereas below the line, “t” refers to a point in time. When we are
modeling information, time t = 0 is special; it represents “here and now” with the
information that is available at the moment. The discrete time t refers to the time
interval from t −1 to t (illustrated in Figure 5.1a). This means that the ﬁrst new
information arrives during time interval 1. This notational style means that any
variable indexed by t, say St or xt, is assumed to have access to the information
that arrived up to time t, which means up through time interval t. This property
will dramatically simplify our notation in the future. For example, assume that ft
is our forecast of the demand for electricity. If ˆDt is the observed demand during

172
modeling dynamic programs
(a)
t = 1
t = 2
t = 3
t = 4
t = 0
t = 1
t = 2
t = 3
t = 4
(b)
t = 0
t = 1
t = 2
t = 3
t = 0
t = 1
t = 2
t = 3
Figure 5.1
Relationship between discrete and continuous time for information processes (a) and
physical processes (b).
time interval t, we would write our updating equation for the forecast using
ft = (1 −α)ft−1 + α ˆDt.
(5.1)
We refer to this form as the informational representation. Note that the forecast
ft is written as a function of the information that became available during time
interval t.
When we are modeling a physical process, it is more natural to adopt a different
convention (illustrated in Figure 5.1b): discrete time t refers to the time interval
between t and t + 1. This convention arises because it is most natural in deter-
ministic models to use time to represent when something is happening or when a
resource can be used. For example, let Rt be our cash on hand that we can use
during day t (implicitly, this means that we are measuring it at the beginning of the
day). Let ˆDt be the demand for cash during the day, and let xt represent additional
cash that we have decided to add to our balance (to be used during day t). We can
model our cash on hand using
Rt+1 = Rt + xt −ˆDt.
(5.2)
We refer to this form as the actionable representation. Note that the left-hand side
is indexed by t + 1, while all the quantities on the right-hand side are indexed by
t. This equation makes perfect sense when we interpret time t to represent when
a quantity can be used. For example, many authors would write our forecasting
equation (5.1) as
ft+1 = (1 −α)ft + α ˆDt.
(5.3)
This equation is correct if we interpret ft as the forecast of the demand that will
happen in time interval t.
A review of the literature quickly reveals that both modeling conventions are
widely used. It is important to be aware of the two conventions and how to interpret
them. We handle the modeling of informational and physical processes by using
two time indexes, a form that we refer to as the “(t, t′)” notation. For example,

modeling time
173
ˆDtt′ = demands that ﬁrst become known during time interval t to be served
during time interval t′.
ftt′ = forecast for activities during time interval t′ made using the information
available up through time t.
Rtt′ = resources on hand at time t that cannot be used until time t′.
xtt′ = decision to purchase futures at time t to be exercised during time
interval t′.
For each variable, t indexes the information content (literally, when the variable
is measured or computed), while t′ represents the time at which the activity takes
place. Each of these variables can be written as vectors, such as
ˆDt = ( ˆDtt′)t′≥t,
ft = (ftt′)t′≥t,
xt = (xtt′)t′≥t,
Rt = (Rtt′)t′≥t.
Note that these vectors are now written in terms of the information content. For
stochastic problems, this style is the easiest and most natural. If we were modeling
a deterministic problem, we would drop the ﬁrst index “t” and model the entire
problem in terms of the second index “t′.”
Each one of these quantities is computed at the end of time interval t (i.e., with
the information up through time interval t), and each represents a quantity that
can be used at time t′ in the future. We could adopt the convention that the ﬁrst
time index uses the indexing system illustrated in Figure 5.1a, while the second
time index uses the system in Figure 5.1b. While this convention would allow us
to easily move from a natural deterministic model to a natural stochastic model,
we suspect most people will struggle with an indexing system where time interval
t in the information process refers to time interval t −1 in the physical process.
Instead, we adopt the convention to model information in the most natural way,
and live with the fact that product arriving at time t can only be used during time
interval t + 1.
Using this convention it is instructive to interpret the special case where t = t′.
ˆDtt is simply demands that arrive during time interval t, where we ﬁrst learn of them
when they arrive. ftt makes no sense because we would never forecast activities
during time interval t after we have this information. Rtt represents resources that
we know about during time interval t and that can be used during time interval t.
Finally, xtt is a decision to purchase resources to be used during time interval t
given the information that arrived during time interval t. In ﬁnancial circles this is
referred to as purchasing on the spot market.

174
modeling dynamic programs
The most difﬁcult notational decision arises when ﬁrst starting to work on a
problem. It is natural at this stage to simplify the problem (often the problem
appears simple) and then choose notation that seems simple and natural. If the
problem is deterministic and you are quite sure that you will never solve a stochas-
tic version of the problem, then the actionable representation (Figure 5.1b) and
equation (5.2) is going to be the most natural. Otherwise, it is best to choose the
informational format. If you do not have to deal with lagged information processes
(e.g., ordering at time t to be used at some time t′ in the future), then you should
be able to get by with a single time index, but you need to remember that xt may
mean purchasing product to be used during time interval t + 1.
Care has to be used when taking expectations of functions. Consider what hap-
pens when we want to know the expected costs to satisfy customer demands ˆDt
that arose during time interval t given the decision xt−1 we made at time t −1.
We would have to compute E{Ct(xt−1, ˆDt)}, where the expectation is over the
random variable ˆDt. The function that results from taking the expectation is now
a function of information up through time t −1. Thus we might use the notation
Ct−1(xt−1) = E{Ct(xt−1, ˆDt)}.
This can take a little getting used to. The costs are incurred during time interval
t, but now we are indexing the function with time t −1. The problem is that
if we use a single time index, we are not capturing when the activity is actually
happening. An alternative is to switch to a double time index, as in
Ct−1,t(xt−1) = E{Ct(xt−1, ˆDt)},
where Ct−1,t(xt−1) is the expected costs that will be incurred during time interval
t using the information known at time t −1.
5.3
MODELING RESOURCES
There is a vast range of problems that can be broadly described in terms of
managing “resources.” Resources can be equipment, people, money, robots,
or even games such as backgammon or chess. Depending on the setting, we
might use the term asset (ﬁnancial applications, expensive equipment) or entity
(managing a robot, playing a board game). It is very common to start with
fairly simple models of these problems, but the challenge is to solve complex
problems.
There are four important problem classes that we consider in this volume, each
offering unique computational challenges. These can be described along two dimen-
sions: the number of resources or entities being managed, and the complexity of the
attributes of each resource or entity. We may be managing a single entity (a robot,
an aircraft, an electrical power plant) or multiple entities (a ﬂeet of aircraft, trucks
or locomotives, funds in different asset classes, groups of people). The attributes of
each entity or resource may be simple (the truck may be described by its location,

modeling resources
175
the money by the asset class into which it has been invested) or complex (all the
characteristics of an aircraft or pilot).
The computational implications of each problem class can be quite different. Not
surprisingly, different communities tend to focus on speciﬁc problem classes, mak-
ing it difﬁcult for people to make the connection between mathematical notation
(which can be elegant but vague) and the characteristics of a problem. Problems
involving a single, simple entity can usually be solved using the classical techniques
of Markov decision processes (Chapter 3), although even here some problems can
be difﬁcult. The artiﬁcial intelligence community often works on problems involv-
ing a single, complex entity (games, e.g., Connect-4 and backgammon, moving a
robot arm, or ﬂying an aircraft). The operations research community has major
subcommunities working on problems that involve multiple, simple entities (mul-
ticommodity ﬂow problems, inventory problems), while portions of the simulation
community and math programming community (for deterministic problems) will
work on applications with multiple, complex entities.
In this section we describe notation that allows us to evolve from simple to
complex problems in a natural way. Our goal is to develop mathematical notation
that does a better job of capturing which problem class we are working on.
5.3.1
Modeling a Single, Discrete Resource
Many problems in dynamic programming involve managing a single resource such
as ﬂying an aircraft, planning a path through a network, or planning a game of
chess or backgammon. These problems are distinctly different than those involving
the management of ﬂeets of vehicles, inventories of blood, or groups of people.
For this reason we adopt speciﬁc notation for the single entity problem.
If we are managing a single, discrete resource, we ﬁnd it useful to introduce
speciﬁc notation to model the attributes of the resource. For this purpose we use
rt = attribute vector of the resource at time t
= (rt1, rt2, . . . , rtN),
R = set of all possible attribute vectors.
Attributes might be discrete (0, 1, 2, . . . ), continuous (0 ≤ri ≤1) or categorical
(ri = red). We typically assume that the number of dimensions of rt is not too
large. For example, if we are modeling the ﬂow of product through a supply chain,
the attribute vector might consist of the product type and location. If we are playing
chess, the attribute vector would have 64 dimensions (the piece on each square of
the board).
5.3.2
Modeling Multiple Resources
Imagine that we are modeling a ﬂeet of unmanned aerial vehicles (UAVs), which
are robotic aircraft used primarily for collecting information. We can let rt be the
attributes of a single UAV at time t, but we would like to describe the collective

176
modeling dynamic programs
attributes of a ﬂeet of UAVs. There is more than one way to do this, but one way
that will prove to be notationally convenient is to deﬁne
Rtr = number of resources with attribute r at time t.
Rt = (Rtr)r∈R.
Rt is known as the resource state vector. If r is a vector, then |R| may be
quite large. It is not hard to create problems where Rt has hundreds of thou-
sands of dimensions. If elements of rt are continuous, then in theory at least, Rt
is inﬁnite-dimensional. It is important to emphasize that in such cases we would
never enumerate the entire vector of elements in Rt.
We note that we can use this notation to model a single resource. Instead of
letting rt be our state vector (for the single resource), we let Rt be the state vector,
where 
r∈R Rtr = 1. This may seem clumsy, but it offers notational advantages
we will exploit from time to time.
5.3.3
Illustration: The Nomadic Trucker
The “nomadic trucker” is a colorful illustration of a multiattribute resource that
helps us clarify some of the modeling conventions being introduced in this chapter.
We use this example to illustrate different issues that arise in approximate dynamic
programming, leading up to the solution of large-scale resource management prob-
lems later in our presentation.
The problem of the nomadic trucker arises in what is known as the truckload
trucking industry. In this industry a truck driver works much like a taxicab. A
shipper will call a truckload motor carrier and ask it to send over a truck. The
driver arrives, loads up the shipper’s freight, and takes it to the destination where
it is unloaded. The shipper pays for the entire truck, so the carrier is not allowed to
consolidate the shipment with freight from other shippers. In this sense the trucker
works much like a taxicab for people. However, as we will soon see, our context of
the trucking company adds an additional level of richness that offers some relevant
lessons for dynamic programming.
Our trucker crosses the United States, so we can assume that his location is
one of the 48 contiguous states. When he arrives in a state, he sees the customer
demands for loads to move from that state to other states. There may be none,
one, or several. He may choose a load to move if one is available; alternatively,
he has the option of doing nothing or moving empty to another state (even if a
load is available). Once he moves out of a state, all other customer demands (in
the form of loads to be moved) are assumed to be picked up by other truckers and
are therefore lost. He is not able to see the availability of loads out of states other
than where he is located.
Although truckload motor carriers can boast ﬂeets of over 10,000 drivers, our
model focuses on the decisions made by a single driver. There are in fact thousands
of trucking “companies” that consist of a single driver. In Chapter 14 we will show
that the concepts we develop here form the foundation for managing the largest and

modeling resources
177
most complex versions of this problem. For now, our “nomadic trucker” represents
a particularly effective way of illustrating some important concepts in dynamic
programming.
A Basic Model
The simplest model of our nomadic trucker assumes that his only attribute is his
location, which we assume has to be one of the 48 contiguous states. We let
I = set of “states” (locations) at which the driver can be located.
We use i and j to index elements of I. His attribute vector then consists of
r = {i}.
In addition to the attributes of the driver, we also have to capture the attributes of the
loads that are available to be moved. For our basic model, loads are characterized
only by where they are going. Let
b = vector of characteristics of a load
=
 b1
b2

=
 Origin of the load
Destination of the load

.
We let R be the set of all possible values of the driver attribute vector r, and we
let B be the set of all possible load attribute vectors b.
A More Realistic Model
We need a richer set of attributes to capture some of the realism of an actual truck
driver. To begin, we need to capture the fact that at a point in time, a driver may
be in the process of moving from one location to another. If this is the case, we
represent the attribute vector as the attribute that we expect when the driver arrives
at the destination (which is the next point at which we can make a decision). We
then have to include as an attribute the time at which we expect the driver to arrive.
Second, we introduce the dimension that the equipment may fail, requiring
some level of repair. A failure can introduce additional delays before the driver is
available.
A third important dimension covers the rules that limit how much a driver can
be on the road. In the United States, drivers are governed by a set of rules set by
the Department of Transportation (DOT). There are three basic limits: the amount
a driver can be behind the wheel in one shift, the amount of time a driver can
be “on duty” in one shift (includes time waiting), and the amount of time that a
driver can be on duty over any contiguous eight-day period. As of this writing,
these rules are as follows: a driver can drive at most 11 hours at a stretch, he may
be on duty for at most 14 continuous hours (there are exceptions to this rule), and
the driver can work at most 70 hours in any eight-day period. The last clock is
reset if the driver is off-duty for 34 successive hours during any stretch (known as
the “34-hour reset”).

178
modeling dynamic programs
A ﬁnal dimension involves getting a driver home. In truckload trucking, drivers
may be away from home for several weeks at a time. Companies work to assign
drivers that get them home in a reasonable amount of time.
If we include these additional dimensions, our attribute vector grows to
rt =


r1
r2
r3
r4
r5
r6
r7
r8


=


Current or future location of the driver
Time at which the driver is expected to arrive at his future location
Maintenance status of the equipment
Number of hours a driver has been behind the wheel during his current shift
Number of hours a driver has been on duty during his current shift
An eight-element vector giving the number of hours the driver was on
duty over each of the previous eight days
Drivers home domicile
Number of days a driver has been away from home


.
We note that element r6 is actually a vector that holds the number of hours the
driver was on duty during each calendar day over the last eight days.
A single attribute such as location (including the driver’s domicile) might have
100 outcomes, or over 1000. The number of hours a driver has been on the road
might be measured to the nearest hour, while the number of days away from home
can be as large as 30 (in rare cases). Needless to say, the number of potential
attribute vectors is extremely large.
5.4
THE STATES OF OUR SYSTEM
The most important quantity in a dynamic program is the state variable. The state
variable captures what we need to know, but just as important it is the variable
around which we construct value function approximations. Success in developing a
good approximation strategy depends on a deep understanding of what is important
in a state variable to capture the future behavior of a system.
5.4.1
Deﬁning the State Variable
Surprisingly, other presentations of dynamic programming spend little time deﬁning
a state variable. Bellman’s seminal text (Bellman 1957, p. 81) says: “ . . . we have
a physical system characterized at any stage by a small set of parameters, the state
variables.” In a much more modern treatment, Puterman ﬁrst introduces a state
variable by saying (Puterman 2005, p. 18): “At each decision epoch, the system
occupies a state.” In both cases the italics are in the original manuscript, indicating
that the term “state” is being introduced. In effect both authors are saying that
given a system, the state variable will be apparent from the context.
Interestingly different communities appear to interpret state variables in slightly
different ways. We adopt an interpretation that is fairly common in the control
theory community, but offer a deﬁnition that appears to be somewhat tighter than
what typically appears in the literature. We suggest the following deﬁnition:

the states of our system
179
Deﬁnition 5.4.1
A state variable is the minimally dimensioned function of his-
tory that is necessary and sufﬁcient to compute the decision function, the transition
function, and the contribution function.
Later in the chapter we discuss the decision function (Section 5.5), the transition
function (Section 5.7), and the objective function (Section 5.8). In plain English,
a state variable is all the information you need to know (at time t) to model the
system from time t onward. Initially it is easiest to think of the state variable in
terms of the physical state of the system (the status of the pilot, the amount of
money in our bank account), but ultimately it is necessary to think of it as the
“state of knowledge.”
This deﬁnition provides a very quick test of the validity of a state variable. If
there is a piece of data in either the decision function, the transition function, or
the contribution function that is not in the state variable, then we do not have a
complete state variable. Similarly, if there is information in the state variable that
is never needed in any of these three functions, then we can drop it and still have
a valid state variable.
We use the term “minimally dimensioned function” so that our state variable
is as compact as possible. For example, we could argue that we need the entire
history of events up to time t to model future dynamics. But this is not practical.
As we start doing computational work, we are going to want St to be as compact
as possible. Furthermore there are many problems where we simply do not need to
know the entire history. It might be enough to know the status of all our resources at
time t (the resource variable Rt). But there are examples where this is not enough.
Assume, for example, that we need to use our history to forecast the price of
a stock. Our history of prices is given by ( ˆp1, ˆp2, . . . , ˆpt). If we use a simple
exponential smoothing model, our estimate of the mean price wt can be computed
using
pt = (1 −α)pt−1 + α ˆpt,
where α is a stepsize satisfying 0 ≤α ≤1. With this forecasting mechanism we do
not need to retain the history of prices, but rather only the latest estimate pt. As a
result pt is called a sufﬁcient statistic, which is a statistic that captures all relevant
information needed to compute any additional statistics from new information. A
state variable, according to our deﬁnition, is always a sufﬁcient statistic.
Consider what happens when we switch from exponential smoothing to an
N -period moving average. Our forecast of future prices is now given by
pt = 1
N
N−1

τ=0
ˆpt−τ.
Now we have to retain the N -period rolling set of prices ( ˆpt, ˆpt−1, . . . , ˆpt−N+1)
in order to compute the price estimate in the next time period. With exponential
smoothing, we could write
St = pt.

180
modeling dynamic programs
If we use the moving average, our state variable would be
St = ( ˆpt, ˆpt−1, . . . , ˆpt−N+1).
(5.4)
Many authors say that if we use the moving average model, we no longer have
a proper state variable. Rather, we would have an example of a “history-dependent
process” where the state variable needs to be augmented with history. By our
deﬁnition of a state variable, the concept of a history-dependent process has no
meaning. The state variable is simply the minimal information required to capture
what is needed to model future dynamics. Needless to say, having to explicitly
retain history, as we did with the moving average model, produces a much larger
state variable than the exponential smoothing model.
5.4.2
The Three States of Our System
To set up our discussion, assume that we are interested in solving a relatively
complex resource management problem, one that involves multiple (possibly many)
different types of resources which can be modiﬁed in various ways (changing their
attributes). For such a problem, it is necessary to work with three types of states:
Physical state. This is a snapshot of the status of the physical resources we are
managing and their attributes. The physical state might include the amount
of water in a reservoir, the price of a stock, or the location of a sensor on a
network.
Information state. This encompasses the physical state as well as any other
information we need to make a decision, compute the transition, or compute
the objective function.
Belief/knowledge state. If the information state is what we know, the belief
state (also known as the knowledge state) captures how well we know it.
This concept is largely absent from most dynamic programs but arises in the
setting of partially observable processes (when we cannot observe a portion
of the state variable).
There are many problems in dynamic programming that involve the management of
a single resource or entity (or asset—the best terminology depends on the context),
such as using a computer to play backgammon, routing a single aircraft, controlling
a power plant, or selling an asset. There is nothing wrong with letting St be the
state of this entity. When we are managing multiple entities (which often puts us
in the domain of “resource management”), it is often useful to distinguish between
the state of a single entity, which we represent as rt, and the state of all the entities,
which we represent as Rt.
We can use St to be the state of a single resource (if this is all we are managing),
or let St = Rt be the state of all the resources we are managing. There are many
problems where the state of the system consists only of rt or Rt. We suggest using
St as a generic state variable when it is not important to be speciﬁc, but it must

the states of our system
181
be used when we may wish to include other forms of information. For example,
we might be managing resources (consumer products, equipment, people) to serve
customer demands ˆDt that become known at time t. If Rt describes the state of
the resources we are managing, our state variable would consist of St = (Rt, ˆDt),
where ˆDt represents additional information we need to solve the problem.
Alternatively, other information might include estimates of parameters of the
system (costs, speeds, times, prices). To represent this, let
θt = a vector of estimates of different problem parameters at time t.
ˆθt = new information about problem parameters that arrive during time interval t.
We can think of θt as the state of our information about different problem param-
eters at time t. We can now write a more general form of our state variable as
St = information state at time t
= (Rt, θt).
In Chapter 12 we will show that it is important to include not just the point
estimate θt, but the entire distribution (or the parameters needed to characterize
the distribution, e.g., the variance).
A particularly important version of this more general state variable arises in
approximate dynamic programming. Recall that in Chapter 4 we used an approxi-
mation of the value function to make a decision, as in
xn
t = arg max
xt∈Xnt

Ct(Rn
t , xt) +V
n−1
t
(Rx
t )

(5.5)
Here V
n−1(·) is an estimate of the value function if our decision takes us from
resource state Rt to Rx
t , and xn
t is the value of xt that solves the right-hand side
of (5.5). In this case our state variable would consist of
St = (Rt, V
n−1).
The idea that the value function is part of our state variable is quite important in
approximate dynamic programming.
5.4.3
The Post-decision State Variable
We can view our system as evolving through sequences of new information fol-
lowed by a decision followed by new information (and so on). Although we have
not yet discussed decisions, for the moment let the decisions (which may be a
vector) be represented generically using at (we discuss our choice of notation for
a decision in the next section). So a history of the process might be represented
using
ht = (S0, a0, W1, a1, W2, a2, . . . , at−1, Wt).

182
modeling dynamic programs
ht contains all the information we need to make a decision at at time t. As we
discussed before, ht is sufﬁcient but not necessary. We expect our state variable to
capture what is needed to make a decision, allowing us to represent the history as
ht = (S0, a0, W1, S1, a1, W2, S2, a2, . . . , at−1, Wt, St).
(5.6)
The sequence in equation (5.6) deﬁnes our state variable as occurring after new
information arrives and before a decision is made. For this reason we call St
the pre-decision state variable. This is the most natural place to write a state
variable because the point of capturing information from the past is to make a
decision.
For most problem classes we can design more effective computational strategies
using the post-decision state variable. This is the state of the system after a decision
at. For this reason we denote this state variable Sa
t , which produces the history
ht = (S0, a0, Sa
0, W1, S1, a1, Sa
1, W2, S2, a2, Sa
2, . . . , at−1, Sa
t−1, Wt, St).
(5.7)
We again emphasize that our notation Sa
t means that this function has access to all
the exogenous information up through time t, along with the decision at (which
also has access to the information up through time t).
The examples below provide some illustrations of pre- and post-decision states.
■
EXAMPLE 5.1
A traveler is driving through a network, where the travel time on each link of
the network is random. As she arrives at node i, she is allowed to see the travel
times on each of the links out of node i, which we represent by ˆτi = (ˆτij)j.
As she arrives at node i, her pre-decision state is St = (i, ˆτi). Assume that she
decides to move from i to k. Her post-decision state is Sa
t = (k) (note that she
is still at node i; the post-decision state captures the fact that she will next be
at node k, and we no longer have to include the travel times on the links out
of node i).
■
■
EXAMPLE 5.2
The nomadic trucker revisited. Let Rtr = 1 if the trucker has attribute vector
r at time t and 0 otherwise. Now let Dtb be the number of customer demands
(loads of freight) of type b available to be moved at time t. The pre-decision
state variable for the trucker is St = (Rt, Dt), which tells us the state of the
trucker and the demands available to be moved. Assume that once the trucker
makes a decision, all the unserved demands in Dt are lost, and new demands
become available at time t + 1. The post-decision state variable is given by
Sa
t = Ra
t , where Ra
tr = 1 if the trucker has attribute vector r after a decision
has been made.
■

the states of our system
183
■
EXAMPLE 5.3
Imagine playing backgammon where Rti is the number of your pieces on
the ith “point” on the backgammon board (there are 24 points on a board).
The transition from St to St+1 depends on the player’s decision at, the play
of the opposing player, and the next roll of the dice. The post-decision state
variable is simply the state of the board after a player moves but before his
opponent has moved.
■
The importance of the post-decision state variable, and how to use it, depends
on the problem at hand. We saw in Chapter 4 that the post-decision state variable
allowed us to make decisions without having to compute the expectation within
the max or min operator. Later we will see that this allows us to solve some very
large scale problems.
There are three ways of ﬁnding a post-decision state variable:
Decomposing Decisions and Information
There are many problems where we can create functions SM,a(·) and SM,W(·) from
which we can compute
Sa
t = SM,a(St, at),
(5.8)
St+1 = SM,W(Sa
t , Wt+1).
(5.9)
The structure of these functions is highly problem-dependent. However, there are
sometimes signiﬁcant computational beneﬁts, primarily when we face the prob-
lem of approximating the value function. Recall that the state variable captures all
the information we need to make a decision, compute the transition function, and
compute the contribution function. Sa
t only has to carry the information needed to
compute the transition function. For some applications, Sa
t has the same dimen-
sionality as St, but in many settings, Sa
t is dramatically simpler than St, simplifying
the problem of approximating the value function.
State-Action Pairs
A very generic way of representing a post-decision state is to simply write
Sa
t = (St, at).
Figure 5.2 provides a nice illustration using our tic-tac-toe example. Figure 5.2a
shows a tic-tac-toe board just before player O makes his move. Figure 5.2b shows
the augmented state-action pair, where the decision (O decides to place his move
in the upper right-hand corner) is distinct from the state. Finally, Figure 5.2c shows
the post-decision state. For this example the pre- and post-decision state spaces are
the same, while the augmented state-action pair is nine times larger.
The augmented state (St, at) is closely related to the post-decision state Sa
t
(not surprising, since we can compute Sa
t deterministically from St and at). But

184
modeling dynamic programs
(a)
Pre-decision
St
State-action
(St, at)
(b)
(c)
Post-decision
(St )
a
Figure 5.2
Pre-decision state, augmented state-action, and post-decision state for tic-tac-toe.
computationally the difference is signiﬁcant. If S is the set of possible values of
St, and A is the set of possible values of at, then our augmented state space has
size |S| × |A|, which is obviously much larger.
The augmented state variable is used in a popular class of algorithms known
as Q-learning (introduced in Chapter 6), where the challenge is to statistically
estimate Q-factors that give the value of being in state St and taking action at.
The Q-factors are written Q(St, at), in contrast to value functions Vt(St), which
provide the value of being in a state. This allows us to directly ﬁnd the best action
by solving mina Q(St, at). This is the essence of Q-learning, but the price of this
algorithmic step is that we have to estimate Q(St, at) for each St and at. It is not
possible to determine at by optimizing a function of Sa
t alone, since we generally
cannot determine which action at brought us to Sa
t .
The Post-decision as a Point Estimate
Suppose that we have a problem where we can compute a point estimate of future
information. Let W t,t+1 be a point estimate, computed at time t, of the outcome
of Wt+1. If Wt+1 is a numerical quantity, we might use W t,t+1 = E(Wt+1|St) or
W t,t+1 = 0. Wt+1 might be a discrete outcome such as the number of equipment
failures. It may not make sense to use an expectation (we may have problems
working with 0.10 failures), so in this setting Wt+1 might be the most likely out-
come. Finally, we might simply assume that Wt+1 is empty (a form of “null” ﬁeld).
For example, a taxi picking up a customer may not know the destination of the
customer before the customer gets in the cab. In this case, if Wt+1 represents the
destination, we might use W t,t+1 = ‘-’.
If we can create a reasonable estimate W t,t+1, we can compute post- and pre-
decision state variables using
Sa
t = SM(St, at, W t,t+1),
St+1 = SM(St, at, Wt+1).
Measured this way, we can think of Sa
t as a point estimate of St+1, but this does
not mean that Sa
t is necessarily an approximation of the expected value of St+1.

the states of our system
185
5.4.4
Partially Observable States*
There are many applications where we are not able to observe (or measure) the state
of the system precisely, as illustrated in the examples. These problems are referred
to as partially observable Markov decision processes, and require introducing a
new class of exogenous information representing the difference between the true
state and the observed state.
■
EXAMPLE 5.4
A retailer may have to order inventory without being able to measure the
precise current inventory. It is possible to measure sales, but theft and breakage
introduce errors.
■
■
EXAMPLE 5.5
The military has to make decisions about sending out aircraft to remove impor-
tant military targets that may have been damaged in previous raids. These
decisions typically have to be made without knowing the precise state of the
targets.
■
■
EXAMPLE 5.6
Policy makers have to decide how much to reduce CO2 emissions, and would
like to plan a policy over 200 years that strikes a balance between costs and the
rise in global temperatures. Scientists cannot measure temperatures perfectly (in
large part because of natural variations), and the impact of CO2 on temperature
is unknown and not directly observable.
■
To model this class of applications, let
˜St = true state of the system at time t,
˜Wt = errors that arise when measuring the state ˜St.
In this context we assume that our state variable St is the observed state of the
system. Now, our history is given by
ht = (S0, a0, Sa
0, W1, ˜S1, ˜W1, S1, a1, Sa
1, W2, ˜S2, ˜W2, S2, a2, Sa
2, . . . ,
at−1, Sa
t−1, Wt, ˜St, ˜Wt, St).
We view our original exogenous information Wt as representing information such
as the change in price of a resource, a customer demand, equipment failures, or
delays in the completion of a task. By contrast,
˜Wt, which captures the differ-
ence between ˜St and St, represents measurement error or the inability to observe
information. Examples of measurement error might include differences between

186
modeling dynamic programs
actual and calculated inventory of product on a store shelf (e.g., due to theft or
breakage), the error in estimating the location of a vehicle (due to errors in the GPS
tracking system), or the difference between the actual state of a piece of machine
such as an aircraft (which might have a failed part) and the observed state (we
do not yet know about the failure). A different form of observational error arises
when there are elements we simply cannot observe (e.g., we know the location of
the vehicle but not its fuel status).
It is important to realize that there are two transition functions at work here.
The “real” transition function models the dynamics of the true (unobservable) state,
as in
˜St+1 = ˜SM( ˜St, at, ˜Wt+1).
In practice, not only do we have the problem that we cannot perfectly measure ˜St,
we may not know the transition function ˜SM(·). Instead, we are working with our
“engineered” transition function
St+1 = SM(St, at, Wt+1),
where Wt+1 is capturing some of the effects of the observation error. When build-
ing a model where observability is an issue, it is important to try to model ˜St,
the transition function ˜SM(·), and the observation error ˜Wt as much as possible.
However, anything we cannot measure may have to be captured in our generic
noise vector Wt.
5.4.5
Flat versus Factored State Representations*
It is very common in the dynamic programming literature to deﬁne a discrete set
of states S = (1, 2, . . . , |S|), where s ∈S indexes a particular state. For example,
consider an inventory problem where St is the number of items we have in inventory
(where St is a scalar). Here our state space S is the set of integers, and s ∈S tells
us how many products are in inventory. This is the style we used in Chapters 3
and 4.
Now suppose that we are managing a set of K product types. The state of our
system might be given by St = (St1, St2, . . . , Stk, . . .), where Stk is the number
of items of type k in inventory at time t. Assume that Stk ≤M. Our state space S
would consist of all possible values of St, which could be as large as KM. A state
s ∈S corresponds to a particular vector of quantities (Stk)K
k=1.
Modeling each state with a single scalar index is known as a ﬂat or unstructured
representation. Such a representation is simple and elegant, and produces very
compact models that have been popular in the operations research community.
The presentation in Chapter 3 depends on this representation. However, the use of
a single index completely disguises the structure of the state variable, and often
produces intractably large state spaces.
In the arena of approximate dynamic programming, it is often essential that
we exploit the structure of a state variable. For this reason we generally ﬁnd it

modeling decisions
187
necessary to use what is known as a factored representation, where each factor
represents a feature of the state variable. For example, in our inventory example
we have K factors (or features). It is possible to build approximations that exploit
the structure that each dimension of the state variable is a particular quantity.
Our attribute vector notation, which we use to describe a single entity, is an
example of a factored representation. Each element ri of an attribute vector repre-
sents a particular feature of the entity. The resource state variable Rt = (Rtr)r∈R is
also a factored representation, since we explicitly capture the number of resources
with a particular attribute. This is useful when we begin developing approxima-
tions for problems such as the dynamic assignment problem that we introduced in
Section 2.2.10.
5.5
MODELING DECISIONS
Fundamental to dynamic programs is the characteristic that we are making decisions
over time. For stochastic problems we have to model the sequencing of decisions
and information, but there are many applications of dynamic programming that
address deterministic problems.
Virtually all problems in approximate dynamic programming have large state
spaces; it is hard to devise a problem with a small state space that is hard to solve.
But problems can vary widely in terms of the nature of the decisions we have to
make. In this section we introduce two notational styles (which are illustrated in
Chapters 2 and 4) to help communicate the breadth of problem classes.
5.5.1
Decisions, Actions, and Controls
A survey of the literature reveals a distressing variety of words used to mean “deci-
sions.” The classical literature on Markov decision process talks about choosing
an action a ∈A (or a ∈As, where As is the set of actions available when we
are in state s) or a policy (a rule for choosing an action). The optimal control
community chooses a control u ∈Ux when the system is in state x. The math pro-
gramming community wants to choose a decision represented by the vector x, and
the simulation community wants to apply a rule.
The proper notation for decisions will depend on the speciﬁc application. Rather
than use one standard notation, we use two. Following the widely used convention
in the reinforcement learning community, we use a whenever we have a ﬁnite
number of discrete actions. The action space A might have 10, 100, or perhaps
even 1000 actions, but we are not aware of actual applications with, say, 10,000
actions or more.
There are many applications where a decision is either continuous or vector-
valued. For example, in Chapters 2 and 14 we describe applications where a
decision at time t involves the assignment of resources to tasks. Let x = (xd)d∈D
be the vector of decisions, where d ∈D is a type of decision, such as assign-
ing resource i to task j, or purchasing a particular type of equipment. It is not
hard to create problems with hundreds, thousands, and even tens of thousands of
dimensions (enumerating the number of potential actions, even if xt is discrete,

188
modeling dynamic programs
is meaningless). These high-dimensional decision vectors arise frequently in the
types of resource allocation problems addressed in operations research.
There is an obvious equivalence between the “a” notation and the “x” notation.
Let d = a represent the decision to make a decision of “type” a. Then xd = 1
corresponds to the decision to take action a. We would like to argue that one
representation is better than the other, but the simple reality is that both are useful.
We could simply stay with the “a” notation, allowing a to be continuous or a
vector, as needed. However, there are many algorithms, primarily developed in the
reinforcement learning community, where we really need to insist that the action
space be ﬁnite, and we feel it is useful to let the use of a for action communicate
this property. By contrast, when we switch to x notation, we are communicating
that we can no longer enumerate the action space, and have to turn to other types
of search algorithms to ﬁnd the best action. In general, this means that we cannot
use lookup table approximations for the value function, as we did in Chapter 4.
It is important to realize that problems with small, discrete action spaces, and
problems with continuous and/or multidimensional decisions, both represent impor-
tant problem classes, and both can be solved using the algorithmic framework of
approximate dynamic programming.
5.5.2
Making Decisions
The challenge of dynamic programming is making decisions. In a determinis-
tic setting we can pose the problem as one of making a sequence of decisions
a0, a1, . . . , aT (if we are lucky enough to have a ﬁnite horizon problem). How-
ever, in a stochastic problem the challenge is ﬁnding the best policy for making a
decision.
It is common in the dynamic programming community to represent a policy
by π. Given a state St, an action would be given by at = π(St) or xt = π(St)
(depending on our notation). We feel that this notation can be a bit abstract, and
that it also limits our ability to specify classes of policies. We prefer instead to
emphasize that a policy is in fact a function that returns a decision. As a result we
let at = Aπ(St) (or xt = Xπ(St)) represent the function (or decision function) that
returns an action at given state St. We use π as a superscript to capture the fact
that Aπ(St) is one element in a family of functions that we represent by writing
π ∈.
We refer to Aπ(St) (or Xπ(St)) and π interchangeably as a policy. A policy can
be a simple function. For example, in an inventory problem, let St be the number
of items that we have in inventory. We might use a reorder policy of the form
A(St) =

Q −St
if St < q,
0
otherwise.
We might let OUT be the set of “order-up-to” decision rules. Searching for the
best policy π ∈OUT means searching for the best set of parameters (Q, q).
Finding the best policy within a particular set does not mean that we are ﬁnding
the optimal policy, but in many cases that will be the best we can do.

the exogenous information process
189
Finding good policies is the challenge of dynamic programming. We provided a
glimpse of how to do this in Chapter 4, but we defer to Chapter 6 a more thorough
discussion of policies, which sets the tone for the rest of the book.
5.6
THE EXOGENOUS INFORMATION PROCESS
An important dimension of many of the problems that we address is the arrival
of exogenous information, which changes the state of our system. While there
are many important deterministic dynamic programs, exogenous information
processes represent an important dimension in many problems in resource man-
agement.
5.6.1
Basic Notation for Information Processes
The central challenge of dynamic programs is dealing with one or more exogenous
information processes, forcing us to make decisions before all the information is
known. These might be stock prices, travel times, equipment failures, or the behav-
ior of an opponent in a game. There might be a single exogenous process (the price
at which we can sell an asset) or a number of processes. For a particular problem
we might model this process using notation that is speciﬁc to the application. Here
we introduce generic notation that can handle any problem.
Consider a problem of tracking the value of an asset. Assume the price evolves
according to
pt+1 = pt + ˆpt+1.
Here, ˆpt+1 is an exogenous random variable representing the chance in the price
during time interval t + 1. At time t, pt is a number, while (at time t) pt+1 is
random. We might assume that ˆpt+1 comes from some probability distribution.
For example, we might assume that it is normally distributed with mean 0 and
variance σ 2. However, rather than work with a random variable described by some
probability distribution, we are going to primarily work with sample realizations.
Table 5.1 shows 10 sample realizations of a price process that starts with p0 =
29.80 but then evolves according to the sample realization. Following standard
mathematical convention, we index each path by the Greek letter ω (in the example
below, ω runs from 1 to 10). At time t = 0, pt and ˆpt are random variables (for
t≥1), while pt(ω) and ˆpt(ω) are sample realizations. We refer to the sequence
p1(ω), p2(ω), p3(ω), . . . ,
as a sample path (for the prices pt).
We are going to use “ω” notation throughout this book, so it is important to
understand what it means. As a rule we will primarily index exogenous random
variables such as ˆpt using ω, as in ˆpt(ω). ˆpt′ is a random variable if we are sitting
at a point in time t < t′. ˆpt(ω) is not a random variable; it is a sample realization.
For example, if ω = 5 and t = 2, then ˆpt(ω) = −0.73. We are going to create

190
modeling dynamic programs
Table 5.1
Set of sample realizations of prices (pt) and the changes in prices ( ˆpt)
Sample Path
t = 0
t = 1
t = 2
t = 3
ω
p0
ˆp1
p1
ˆp2
p2
ˆp3
p3
1
29.80
2.44
32.24
1.71
33.95
−1.65
32.30
2
29.80
−1.96
27.84
0.47
28.30
1.88
30.18
3
29.80
−1.05
28.75
−0.77
27.98
1.64
29.61
4
29.80
2.35
32.15
1.43
33.58
−0.71
32.87
5
29.80
0.50
30.30
−0.56
29.74
−0.73
29.01
6
29.80
−1.82
27.98
−0.78
27.20
0.29
27.48
7
29.80
−1.63
28.17
0.00
28.17
−1.99
26.18
8
29.80
−0.47
29.33
−1.02
28.31
−1.44
26.87
9
29.80
−0.24
29.56
2.25
31.81
1.48
33.29
10
29.80
−2.45
27.35
2.06
29.41
−0.62
28.80
randomness by choosing ω at random. To make this more speciﬁc, we need to
deﬁne
 = set of all possible sample realizations (with ω ∈).
p(ω) = probability that outcome ω will occur.
A word of caution is needed here. We will often work with continuous random
variables, in which case we have to think of ω as being continuous. With continuous
ω, we cannot say p(ω) is the “probability of outcome ω.” However, in all of our
work, we will use discrete samples. For this purpose we can deﬁne
ˆ = a set of discrete sample observations of ω ∈.
In this case we can talk about p(ω) being the probability that we sample ω from
within the set ˆ.
For more complex problems we may have an entire family of random variables.
In such cases it is useful to have a generic “information variable” that represents
all the information that arrives during time interval t. For this purpose we deﬁne
Wt = exogenous information becoming available during interval t.
Wt may be a single variable, or a collection of variables (travel times, equipment
failures, customer demands). We note that while we use the convention of putting
hats on variables representing exogenous information ( ˆDt, ˆpt), we do not use a hat
for Wt because this is our only use for this variable, whereas Dt and pt have other
meanings. We always think of information as arriving in continuous time; hence
Wt is the information arriving during time interval t, rather than at time t. This
eliminates the ambiguity over the information available when we make a decision
at time t.
The choice of notation Wt as a generic “information function” is not standard,
but it is mnemonic (it looks like ωt). We would then write ωt = Wt(ω) as a sample

the exogenous information process
191
realization of the information arriving during time interval t. This notation adds a
certain elegance when we need to write decision functions and information in the
same equation.
Some authors use ω to index a particular sample path, where Wt(ω) is the infor-
mation that arrives during time interval t. Other authors view ω as the information
itself, as in
ω = (−0.24, 2.25, 1.48).
Obviously, both are equivalent. Sometimes it is convenient to deﬁne
ωt = information that arrives during time period t
= Wt(ω),
ω = (ω1, ω2, . . .).
We sometimes need to refer to the history of our process, for which we deﬁne
Ht = history of the process, consisting of all the information known through
time t,
= (W1, W2, . . . , Wt),
Ht = set of all possible histories through time t,
= {Ht(ω)|ω ∈},
ht = a sample realization of a history,
= Ht(ω),
(ht) = {ω ∈|Ht(ω) = ht}.
In some applications we might refer to ht as the state of our system, but this
is usually a very clumsy representation. However, we will use the history of the
process for a speciﬁc modeling and algorithmic strategy.
5.6.2
Outcomes and Scenarios
Some communities prefer to use the term scenario to refer to a sample realization
of random information. For most purposes “outcome,” “sample path,” and “sce-
nario” can be used interchangeably (although sample path refers to a sequence
of outcomes over time). The term scenario causes problems of both notation and
interpretation. First, “scenario” and “state” create an immediate competition for the
interpretation of the letter “s.” Second, “scenario” is often used in the context of
major events. For example, we can talk about the scenario that the Chinese might
revalue their currency (a major question in the ﬁnancial markets at this time).
We could talk about two scenarios: (1) the Chinese hold the current relationship
between the yuan and the dollar, and (2) they allow their currency to ﬂoat. For

192
modeling dynamic programs
each scenario we could talk about the ﬂuctuations in the exchange rates between
all currencies.
Recognizing that different communities use “outcome” and “scenario” to mean
the same thing, we suggest that we may want to reserve the ability to use both
terms simultaneously. For example, we might have a set of scenarios that deter-
mine if and when the Chinese revalue their currency (but this would be a small
set). We recommend denoting the set of scenarios by , with ψ ∈ representing
an individual scenario. Then, for a particular scenario ψ, we might have a set
of outcomes ω ∈ (or (ψ)) representing various minor events (e.g., currency
exchange rates).
■
EXAMPLE 5.7
Planning spare transformers - In the electric power sector, a certain type of
transformer was invented in the 1960s. As of this writing, the industry does not
really know the failure rate curve for these units (is their lifetime roughly 50
years? 60 years?). Let ψ be the scenario that the failure curve has a particular
shape (e.g., where failures begin happening at a higher rate around 50 years).
For a given scenario (failure rate curve), ω represents a sample realization of
failures (transformers can fail at any time, although the likelihood they will
fail depends on ψ).
■
■
EXAMPLE 5.8
Energy resource planning - The federal government has to determine energy
sources that will replace fossil fuels. As research takes place, there are random
improvements in various technologies. However, the planning of future energy
technologies depends on the success of speciﬁc options, notably whether we
will be able to sequester carbon underground. If this succeeds, we will be able
to take advantage of vast stores of coal in the United States. Otherwise, we
have to focus on options such as hydrogen and nuclear.
■
In Section 13.7 we provide a brief introduction to the ﬁeld of stochastic program-
ming where “scenario” is the preferred term to describe a set of random outcomes.
Often applications of stochastic programming apply to problems where the number
of outcomes (scenarios) is relatively small.
5.6.3
Lagged Information Processes*
There are many settings where the information about a new arrival comes before
the new arrival itself as illustrated in the examples.
■
EXAMPLE 5.9
An airline may order an aircraft at time t and expect the order to be ﬁlled at
time t′.
■

the exogenous information process
193
■
EXAMPLE 5.10
An orange juice products company may purchase futures for frozen concen-
trated orange juice at time t that can be exercised at time t′.
■
■
EXAMPLE 5.11
A programmer may start working on a piece of coding at time t with the
expectation that it will be ﬁnished at time t′.
■
This concept is important enough that we offer the following term:
Deﬁnition 5.6.1
The actionable time of a resource is the time at which a
decision may be used to change its attributes (typically generating a cost or
reward).
The actionable time is simply one attribute of a resource. For example, if at
time t we own a set of futures purchased in the past with exercise dates of t + 1,
t+2, . . . , t′, then the exercise date would be an attribute of each futures contract
(the exercise dates do not need to coincide with the discrete time instances when
decisions are made). When writing out a mathematical model, it is sometimes useful
to introduce an index just for the actionable time (rather than having it buried as an
element of the attribute vector r). As before, we let Rtr be the number of resources
that we know about at time t with attribute vector r. The attribute might capture
that the resource is not actionable until time t′ in the future. If we need to represent
this explicitly, we might write
Rt,t′r = number of resources that we know about at time t that will be actionable
with attribute vector r at time t′,
Rtt′ = (Rt,t′r)r∈R,
Rt = (Rt,t′)t′≥t.
It is important to emphasize that while t is discrete (representing when decisions
are made), the actionable time t′ may be continuous. When this is the case, it is
generally best to simply leave it as an element of the attribute vector.
5.6.4
Models of Information Processes*
Information processes come in varying degrees of complexity. Needless to say, the
structure of the information process plays a major role in the models and algorithms
used to solve the problem. Below we describe information processes in increasing
levels of complexity.

194
modeling dynamic programs
State-Independent Processes
A large number of problems involve processes that evolve independently of the
state of the system, such as wind (in an energy application), stock prices (in the
context of small trading decisions), and demand for a product (assuming inventories
are small relative to the market).
■
EXAMPLE 5.12
A publicly traded index fund has a price process that can be described (in
discrete time) as pt+1 = pt + σδ, where δ is normally distributed with mean
µ, variance 1, and σ is the standard deviation of the change over the length
of the time interval.
■
■
EXAMPLE 5.13
Requests for credit card conﬁrmations arrive according to a Poisson process
with rate λ. This means that the number of arrivals during a period of length
t is given by a Poisson distribution with mean λt, which is independent of
the history of the system.
■
The practical challenge we typically face in these applications is that we do not
know the parameters of the system. In our price process, the price may be trending
upward or downward, as determined by the parameter µ. In our customer arrival
process, we need to know the rate λ (which can also be a function of time).
State-Dependent Information Processes
The standard dynamic programming models allow the probability distribution of
new information (e.g., the chance in price of an asset) to be a function of the state
of the system (the mean change in the price might be negative if the price is high
enough, positive if the price is low enough). This is a more general model than
one with independent increments, where the distribution is independent of the state
of the system.
■
EXAMPLE 5.14
Customers arrive at an automated teller machine according to a Poisson pro-
cess. As the line grows longer, an increasing proportion decline to join the
queue (a property known as balking in the queueing literature). The appar-
ent arrival rate at the queue is a process that depends on the length of the
queue.
■
■
EXAMPLE 5.15
A market with limited information may respond to price changes. If the
price drops over the course of a day, the market may interpret the change

the exogenous information process
195
as a downward movement, increasing sales and putting further downward
pressure on the price. Conversely, upward movement may be interpreted
as a signal that people are buying the stock, encouraging more buying
behavior.
■
Interestingly many models of Markov decision processes use information pro-
cesses that do exhibit independent increments. For example, we could have a
queueing problem where the state of the system is the number of customers in
the queue. The number of arrivals may be Poisson, and the number of customers
served in an increment of time is determined primarily by the length of the queue.
It is possible, however, that our arrival process is a function of the length of the
queue itself (see the examples for illustrations).
State-dependent information processes are more difﬁcult to model and introduce
additional parameters that must be estimated. However, from the perspective of
dynamic programming, they do not introduce any fundamental complexities. As
long as the distribution of outcomes is dependent purely on the state of the system,
we can apply our standard models. In fact approximate dynamic programming
algorithms simply need some mechanism to sample information. It does not even
matter if the exogenous information depends on information that is not in the state
variable (although this will introduce errors).
It is also possible that the information arriving to the system depends on its
state, as depicted in the next set of examples.
■
EXAMPLE 5.16
A driver is planning a path over a transportation network. When the driver
arrives at intersection i of the network, he is able to determine the tran-
sit times of each of the segments (i, j) emanating from i. Thus the tran-
sit times that are observed by the driver depend on the path taken by the
driver.
■
■
EXAMPLE 5.17
A private equity manager learns information about a company only by investing
in the company and becoming involved in its management. The information
arriving to the manager depends on the state of his portfolio.
■
This is a different form of state-dependent information process. Normally an
outcome ω is assumed to represent all information available to the system. A
probabilist would insist that this is still the case with our driver; the fact that
the driver does not know the transit times on all the links is simply a matter of
modeling the information the driver uses. However, many will ﬁnd it more natural
to think of the information as depending on the state.

196
modeling dynamic programs
Action-Dependent Information Processes
Imagine that a mutual fund is trying to optimize the process of selling a large
position in a company. If the mutual fund makes the decision to sell a large number
of shares, the effect may be to depress the stock price because the act of selling
sends a negative signal to the market. Thus the action may inﬂuence what would
normally be an exogenous stochastic process.
More Complex Information Processes
Now consider the problem of modeling currency exchange rates. The change in
the exchange rate between one pair of currencies is usually followed quickly by
changes in others. If the Japanese yen rises relative to the US dollar, it is likely
that the euro will also rise relative to it, although not necessarily proportionally.
As a result we have a vector of information processes that are correlated.
In addition to correlations between information processes, we can have corre-
lations over time. An upward push in the exchange rate between two currencies
in one day is likely to be followed by similar changes for several days while
the market responds to new information. Sometimes the changes reﬂect long-term
problems in the economy of a country. Such processes may be modeled using
advanced statistical models that capture correlations between processes as well as
over time.
An information model can be thought of as a probability density function φt(ωt)
that gives the density (we would say the probability of ω if it were discrete) of an
outcome ωt in time t. If the problem has independent increments, we would write
the density simply as φt(ωt). If the information process is Markovian (dependent
on a state variable), then we would write it as φt(ωt|St−1).
In some cases with complex information models, it is possible to proceed without
any model at all. We can use instead, realizations drawn from history. For example,
we may take samples of changes in exchange rates from different periods in history
and assume that these are representative of changes that may happen in the future.
The value of using samples from history is that they capture all of the properties
of the real system. This is an example of planning a system without a model of an
information process.
5.6.5
Supervisory Processes*
We are often trying to control systems where we have access to a set of decisions
from an exogenous source. These may be decisions from history, or they may come
from a knowledgeable expert. Either way, this produces a dataset of states (Sm)n
m=1
and decisions (xm)n
m=1. In some cases we can use this information to ﬁt a statistical
model that we use to try to predict the decision that would have been made given
a state.
The nature of such a statistical model depends very much on the context, as
illustrated in the examples.

the exogenous information process
197
■
EXAMPLE 5.18
Consider our nomadic trucker where we measure his state sn (his state) and his
decision an, which we represent in terms of the destination of his next move.
We could use a historical ﬁle (sm, am)n
m=1 to build a probability distribution
ρ(s, a) that gives the probability that we make decision a given his state s.
We can use ρ(s, a) to predict decisions in the future.
■
■
EXAMPLE 5.19
A mutual fund manager adds xt dollars in cash at the end of day t (to be used to
cover withdrawals on day t + 1) when there are Rt dollars in cash left over at
the end of the day. We can use a series of observations of xtm and Rtm on days
t1, t2, . . . , tm to ﬁt a model of the form X(R) = θ0 + θ1R + θ2R2 + θ3R3. ■
We can use supervisory processes to statistically estimate a decision function
that forms an initial policy. We can then use this policy in the context of an
approximate dynamic programming algorithm to help ﬁt value functions that can
be used to improve the decision function. The supervisory process helps provide
an initial policy that may not be perfect but at least is reasonable.
5.6.6
Policies in the Information Process*
The sequence of information (ω1, ω2, . . . , ωt) is assumed to be driven by some
sort of exogenous process. However, we are generally interested in quantities that
are functions of both exogenous information as well as the decisions. It is useful
to think of decisions as endogenous information. But where do the decisions come
from? We now see that decisions come from policies. In fact it is useful to represent
our sequence of information and decisions as
H π
t = (S0, Xπ
0 , W1, S1, Xπ
1 , W2, S2, Xπ
2 , . . . , Xπ
t−1, Wt, St).
(5.10)
Now our history is characterized by a family of functions: the information variables
Wt, the decision functions (policies) Xπ
t , and the state variables St. We see that to
characterize a particular history ht, we have to specify both the sample outcome ω
as well as the policy π. Thus we might write a sample realization as
hπ
t = H π
t (ω).
We can think of a complete history H π
∞(ω) as an outcome in an expanded proba-
bility space (if we have a ﬁnite horizon, we would denote this by H π
T (ω)). Let
ωπ = H π
∞(ω)

198
modeling dynamic programs
be an outcome in our expanded space, where ωπ is determined by ω and the policy
π. Let π be the set of all outcomes of this expanded space. The probability of an
outcome in π obviously depends on the policy we are following. Thus, computing
expectations (e.g., expected costs or rewards) requires knowing the policy as well
as the set of exogenous outcomes. For this reason, if we are interested, say, in
the expected costs during time period t, some authors will write Eπ
t {Ct(St, xt)} to
express the dependence of the expectation on the policy. However, even if we do
not explicitly index the policy, it is important to understand that we need to know
how we are making decisions if we are going to compute expectations or other
quantities.
5.7
THE TRANSITION FUNCTION
The next step in modeling a dynamic system is the speciﬁcation of the transition
function. This function describes how the system evolves from one state to another
as a result of decisions and information. We begin our discussion of system dynam-
ics by introducing some general mathematical notation. While useful, this generic
notation does not provide much guidance into how speciﬁc problems should be
modeled. We then describe how to model the dynamics of some simple problems,
followed by a more general model for complex resources.
5.7.1
A General Model
The dynamics of our system are represented by a function that describes how the
state evolves as new information arrives and decisions are made. The dynamics
of a system can be represented in different ways. The easiest is through a simple
function that works as follows:
St+1 = SM(St, Xπ
t , Wt+1).
(5.11)
The function SM(·) goes by different names such as “plant model” (literally, the
model of a physical production plant), “plant equation,” “law of motion,” “transfer
function,” “system dynamics,” “system model,” “transition law,” and “transition
function.” We prefer “transition function” because it is the most descriptive. We
use the notation SM(·) to reﬂect that this is the state transition function, which
represents a model of the dynamics of the system. Below we reinforce the “M ”
superscript with other modeling devices.
The arguments of the function follow standard notational conventions in the
control literature (state, action, information), but different authors will follow one
of two conventions for modeling time. While equation (5.11) is fairly common,
many authors will write the recursion as
St+1 = SM(St, Xπ
t , Wt).
(5.12)
If we use the form in equation (5.12), we would say “the state of the system at
the beginning of time interval t + 1 is determined by the state at time t, plus the

the transition function
199
decision that is made at time t and the information that arrives during time interval
t.” In this representation, t indexes when we are using the information. We refer
to (5.12) as the actionable representation, since it captures when we can act on
the information. This representation is always used for deterministic models, and
many authors adopt it for stochastic models as well. We prefer the form in equation
(5.11) because we are measuring the information available at time t when we are
about to make a decision. If we are making a decision xt at time t, it is natural to
index by t all the variables that can be measured at time t. We refer to this style
as the informational representation.
In equation (5.11) we have written the function assuming that the function does
not depend on time (it does depend on data that depends on time). A common
notational error is to write a function, say, SM
t (St, xt) as if it depends on time,
when in fact the function is stationary but depends on data that depends on time.
If the parameters (or structure) of the function depend on time, then we would use
SM
t (St, xt, Wt+1) (or possibly SM
t+1(St, xt, Wt+1)). If not, the transition function
should be written SM(St, xt, Wt+1).
This is a very general way of representing the dynamics of a system. In many
problems the information Wt+1 arriving during time interval t + 1 depends on the
state St at the end of time interval t, but is conditionally independent of all prior
history given St. For example, a driver moving over a road network may only learn
about the travel times on a link from i to j when he arrives at node i. When this is
the case, we say that we have a Markov information process. When the decisions
depend only on the state St, then we have a Markov decision process. In this
case we can store the system dynamics in the form of a one-step transition matrix
using
P (s′|s, x) = probability that St+1 = s′ given St = s and Xπ
t = x,
P π = matrix of elements where P (s′|s, x) is the element in row s and
column s′ and where the decision x to be made in each state is
determined by a policy π.
There is a simple relationship between the transition function and the one-step
transition matrix. Let
1X =

1
X is true,
0
otherwise.
Assuming that the set of outcomes  is discrete, the one-step transition matrix can
be computed using
P (s′|s, x) = E{1{s′=SM(St,x,Wt+1)}|St = s}
=

ωt+1∈t+1
P (Wt+1 = ωt+1)1{s′=SM(St,x,Wt+1)}.
(5.13)
It is common in the ﬁeld of Markov decision processes to assume that the
one-step transition is given as data. Often it can be quickly derived (for simple

200
modeling dynamic programs
problems) using assumptions about the underlying process. For example, consider
a ﬁnancial asset selling problem with state variable St = (Rt, pt), where
Rt =

1
we are still holding the asset,
0
asset has been sold,
and where pt is the price at time t. We assume the price process is described by
pt = pt−1 + ϵt,
where ϵt is a random variable with distribution
ϵt =



+1
with probability 0.3,
0
with probability 0.6,
−1
with probability 0.1.
Assume that the prices are integer and range from 1 to 100. We can number our
states from 0 to 100 using
S = {(0, −), (1, 1), (1, 2), . . . , (1, 100)}.
We propose that our rule for determining when to sell the asset is of the form
Xπ(Rt, pt) =

sell asset
if pt < p,
hold asset
if pt ≥p.
Assume that p = 60. A portion of the one-step transition matrix for the rows and
columns corresponding to the state (0, −) and (1, 58), (1, 59), (1, 60), (1, 61),
(1, 62) looks like
P 60 =
(0, −)
(1, 58)
(1, 59)
(1, 60)
(1, 61)
(1, 62)


1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0
0.1
0.6
0.3
0
0
0
0
0.1
0.6
0.3
0
0
0
0
0.1
0.6


.
As we saw in Chapter 3, this matrix plays a major role in the theory of Markov
decision processes, although its value is more limited in practical applications. By
representing the system dynamics as a one-step transition matrix, it is possible
to exploit the rich theory surrounding matrices in general and Markov chains in
particular.
In engineering problems it is far more natural to develop the transition function
ﬁrst. Given this, it may be possible to compute the one-step transition matrix
exactly or to estimate it using simulation. The techniques in this book do not, in
general, use the one-step transition matrix, but instead use the transition function
directly. However, formulations based on the transition matrix provide a powerful
foundation for proving convergence of both exact and approximate algorithms.

the transition function
201
5.7.2
The Resource Transition Function
There are many dynamic programming problems that can be modeled in terms of
managing “resources” where the state vector is denoted Rt. We use this notation
when we want to speciﬁcally exclude other dimensions that might be in a state
variable (e.g., the challenge of making decisions to better estimate a quantity,
which was ﬁrst introduced in Section 2.3). If we are using Rt as the state variable,
the general representation of the transition function would be written
Rt+1 = RM(Rt, xt, Wt+1).
Our notation is exactly analogous to the notation for a general state variable St, but
it opens the door to other modeling dimensions. For now, we illustrate the resource
transition equation using some simple applications.
Resource Acquisition I—Purchasing Resources for Immediate Use
Let Rt be the quantity of a single resource class we have available at the end of
a time period, but before we have acquired new resources (for the following time
period). The resource may be money available to spend on an election campaign,
or the amount of oil, coal, grain, or other commodities available to satisfy a market.
Let ˆDt be the demand for the resource that occurs over time interval t, and let xt
be the quantity of the resource that is acquired at time t to be used during time
interval t + 1. The transition function would be written
Rt+1 = max{0, Rt + xt −ˆDt+1}.
Resource Acquisition II—Purchasing Futures
Assume that we are purchasing futures at time t to be exercised at time t′ > t. At
the end of time period t, we let Rtt′ be the number of futures we are holding that
can be exercised during time period t′ (where t′ > t). Now assume that we purchase
xtt′ additional futures to be used during time period t′. Our system dynamics would
look like
Rt+1,t′ =

max{0, (Rtt + Rt,t+1) + xt,t+1 −ˆDt+1},
t′ = t + 1,
Rtt′ + xtt′,
t′ ≥t + 2 .
In many problems we can purchase resources on the spot market, which means we
are allowed to see the actual demand before we make the decision. This decision
would be represented by xt+1,t+1, which means the amount purchased using the
information that arrived during time interval t + 1 to be used during time interval
t + 1 (of course, these decisions are usually the most expensive). In this case the
dynamics would be written
Rt+1,t′ =

max{0, (Rtt + Rt,t+1) + xt,t+1 + xt+1,t+1 −ˆDt+1},
t′ = t + 1,
Rtt′ + xtt′,
t′ ≥t + 2.

202
modeling dynamic programs
Planning a Path through College
Consider a student trying to satisfy a set of course requirements (number of science
courses, language courses, departmentals, etc.). Let Rtc be the number of courses
taken that satisfy requirement c at the end of semester t. Let xtc be the number of
courses the student enrolled in at the end of semester t for semester t + 1 to satisfy
requirement c. Finally, let ˆFtc(xt−1) be the number of courses in which the student
received a failing grade during semester t given xt−1. This information depends on
xt−1, since a student cannot fail a course that she was not enrolled in. The system
dynamics would look like
Rt+1,c = Rt,c + xt,c −ˆFt+1,c.
Playing Backgammon
In backgammon there are 24 points on which pieces may sit, plus a bar when you
bump your opponent’s pieces off the board, and plus the state that a piece has been
removed from the board (you win when you have removed all of your pieces).
Let i ∈{1, 2, . . . , 26} represent these positions. Let Rti be the number of your
pieces on location i at time t, where Rti < 0 means that your opponent has |Rti|
pieces on i. Let xtii′ be the number of pieces moved from point i to i ′ at time t.
If ωt = Wt(ω) is the outcome of the roll of the dice, then the allowable decisions
can be written in the general form xt ∈Xt(ω), where the feasible region Xt(ω)
captures the rules on how many pieces can be moved given Wt(ω). For example, if
we only have two pieces on a point, Xt would restrict us from moving more than
two pieces from this point. Let δxt be a column vector with element i given by
δxti =

i′
xti′i −

i′′
xtii′′.
Now let ˆyt+1 be a variable similar to xt representing the moves of the opponent
after we have ﬁnished our moves, and let δ ˆyt+1 be a column vector similar to δxt.
The transition equation would look like
Rt+1 = Rt + δxt + δ ˆyt+1.
A Portfolio Problem
Let Rtk be the amount invested in asset k ∈K, where K may be individual stocks,
mutual funds, or asset classes such as bonds and money market instruments. Sup-
pose that each month we examine our portfolio and shift money from one asset to
another. Let xtkk′ be the amount we wish to move from asset k to k ′, where xtkk
is the amount we hold in asset k. We assume the transition is made instantly (the
issue of transaction costs are not relevant here). Now let ˆρt+1,k be the return for
asset k between t and t + 1. The transition equation would be given by
Rt+1,k = ˆρt+1,k

k′∈K
xtk′k

.

the transition function
203
We note that it is virtually always the case that the returns ˆρtk are correlated
across the assets. When we use a sample realization ˆρt+1(ω), we assume that these
correlations are reﬂected in the sample realization.
5.7.3
Transition Functions for Complex Resources*
When we are managing multiple, complex resources, each of which are described
by an attribute vector r. It is useful to adopt special notation to describe the
evolution of the system. Recall that the state variable St might consist of both
the resource state variable Rt as well as other information. We need notation that
speciﬁcally describes the evolution of Rt separately from the other variables.
We deﬁne the attribute transition function that describes how a speciﬁc entity
with attribute vector r is modiﬁed by a decision of type a. This is modeled using
rt+1 = rM(rt, at, Wt+1).
The function rM(·) parallels the state transition function SM(·), but it works at
the level of a decision of type a acting on a resource of type r. It is possible that
there is random information affecting the outcome of the decision. For example,
in Section 5.3.3 we introduced a realistic version of our nomadic trucker where
the attributes include dimensions such as the estimated time that the driver will
arrive in a city (random delays can change this), and the maintenance status of the
equipment (the driver may identify an equipment problem while moving).
As with our state variable, we let rt be the attribute just before we make a
decision. Although we are acting on the resource with a decision of type a, we
retain our notation for the post-decision state and let ra
t be the post-decision attribute
vector. A simple example illustrates the pre- and post-decision attribute vectors
for our nomadic trucker. Assume our driver has two attributes: location and the
expected time at which he can be assigned to a new activity. Equation (5.14) shows
pre-and post-decision attributes for a nomadic trucker. At time t = 40, we expect
the driver to be available in St. Louis at time t = 41.4. At t = 40, we make the
decision that as soon as the driver is available, we are going to assign him to a
load going to Los Angeles, where we expect him (again at t = 40) to arrive at
time t = 65.0. At time t = 60, he is still expected to be heading to Los Angeles,
but we have received information that he has been delayed and now expect him to
be available at time t = 70.4 (the delay is the new information).
t = 40
t = 40
t = 60
pre-decision
post-decision
pre-decision
 St.Louis
41.4

 LosAngeles
65.0

 LosAngeles
70.4

(5.14)
As before, we can break down the effect of decisions and information using
ra
t = rM,a(rt, at),
rt+1 = rM,W(ra
t , Wt+1).

204
modeling dynamic programs
For algebraic purposes it is also useful to deﬁne the indicator function:
δa
r′(r, a) =

1,
ra
t = r′ = rM,a(rt, at),
0
otherwise,
a = matrix with δa
r′(r, a) in row r′ and column (r, a).
The function δa(·) (or matrix a) gives the post-decision attribute vector resulting
from a decision a (in the case of a, a set of decisions represented by at).
It is convenient to think of acting on a single resource with an action a. If
we have multiple resources (or types of resources), it also helps to let xtra be the
number of times we act on a resource of type r with action a, and let xt be the
vector of these decisions. We can then describe the evolution of an entire set of
resources, represented by the vector Rt, using
Rt+1 = RM(Rt, xt, Wt+1).
(5.15)
In this setting we would represent the random information Wt+1 as exogenous
changes to the resource vector, given by
ˆRt+1,r = change in the number of resources with attribute vector r due to
information arriving during time interval t + 1.
We can now write out the transition equation using
Rt+1,r′ =

r∈R

a∈A
δa
r′(r, a)xtra + ˆRt+1,r′
or in matrix-vector form
Rt+1 = xRt + ˆRt+1.
5.7.4
Some Special Cases*
It is important to realize when special cases offer particular modeling challenges
(or opportunities). Below we list a few we have encountered.
Deterministic Resource Transition Functions
It is quite common in resource allocation problems that the attribute transition
function is deterministic (the equipment never breaks down, there are no delays,
resources do not randomly enter and leave the system). The uncertainty may arise
not in the evolution of the resources that we are managing but in the “other infor-
mation” such as customer demands (e.g., the loads that the driver might move)
or our estimates of parameters (where we use our decisions to collect informa-
tion on random variables with unknown distributions). If the attribute transition

the transition function
205
function is deterministic, then we would write ra
t = rM(rt, at) and we would have
that rt+1 = ra
t . In this case we simply write
rt+1 = rM(rt, at).
This is an important special case because it arises quite often in practice. It does
not mean the transition function is deterministic. For example, a driver moving
over a network faces the decision, at node i, whether to go to node j. If he makes
this decision, he will go to node j deterministically, but the cost or time over the
link from i to j may be random.
Another example arises when we are managing resources (people, equipment,
blood) to serve demands that arrive randomly over time. The effect of a deci-
sion acting on a resource is deterministic. The only source of randomness is in
the demands that arise in each time period. Let Rt be the vector describing our
resources, and let ˆDt be the demands that arose during time interval t. The state
variable is St = (Rt, ˆDt) where ˆDt is purely exogenous. If xt is our decision vector
that determines which resources are assigned to each demand, then the resource
transition function Rt+1 = RM(Rt, xt) would be deterministic.
Gaining and Losing Resources
In addition to the attributes of the modiﬁed resource we sometimes have to capture
the fact that we may gain or lose resources in the process of completing a decision.
We might deﬁne
ρt+1,r,a = multiplier giving the quantity of resources with attribute vector r
available after being acted on with decision a at time t.
The multiplier may depend on the information available at time t (in which case
we would write it as ρtra), but it is often random and depends on information that
has not yet arrived (in which case we use ρt+1,r,a). Illustrations of gains and losses
are given in the next set of examples.
■
EXAMPLE 5.20
A corporation is holding money in an index fund with a 180-day holding period
(money moved out of this fund within the period incurs a 4 percent load) and
would like to transfer them into a high-yield junk bond fund. The attribute of
the resource would be r = (Type, Age). There is a transaction cost (the cost
of executing the trade) and a gain ρ, which is 1.0 for funds held more than
180 days, and 0.96 for funds held less than 180 days.
■
■
EXAMPLE 5.21
A company transporting liqueﬁed natural gas would like to purchase 500,000
tons of liqueﬁed natural gas in southeast Asia for consumption in North

206
modeling dynamic programs
America. Although in liquiﬁed form, the gas evaporates at a rate of 0.2 percent
per day, implying ρ = 0.998.
■
5.8
THE OBJECTIVE FUNCTION
The ﬁnal dimension of our model is the objective function. We divide this pre-
sentation from a summary of the contribution function (or cost function if we are
minimizing) and the function we optimize to ﬁnd the best policy.
5.8.1
The Contribution Function
We assume that we have some measure of how well we are doing. This might be
a cost that we wish to minimize, a contribution or reward if we are maximizing,
or a more generic utility (which we typically maximize). We assume that we are
maximizing a contribution. In many problems the contribution is a deterministic
function of the state and action, in which case we would write
C(St, at) = contribution (cost if we are minimizing) earned by taking action at
while in state St at time t.
We often write the contribution function as Ct(St, at) to emphasize that it is being
measured at time t and therefore depends on information in the state variable St.
Our contribution may be random. For example, we may invest a dollars in an asset
that earns a random return ρt+1 that we do not know until time t + 1. We may
think of the contribution as ρt+1at, which means that the contribution is random.
In this case we typically will write
ˆCt+1(St, at, Wt+1) = contribution at time t from being in state St, making
decision at, which also depends on the information Wt+1.
We emphasize that the decision at does not have access to the information Wt+1
(this is where our time indexing style eliminates any ambiguity about the infor-
mation content of a variable). As a result the decision at has to work with the
expected contribution, which we write
Ct(St, at) = E{ ˆCt+1(St, at, Wt+1)|St}.
The role that Wt+1 plays is problem-dependent, as illustrated in the examples below.
■
EXAMPLE 5.22
In asset acquisition problems we order at in time period t to be used to satisfy
demands ˆDt+1 in the next time period. Our state variable is St = Rt = the
product on hand after demands in period t have been satisﬁed. We pay a cost

the objective function
207
cpat in period t and receive a revenue p min(Rt + at, ˆDt+1) in period t + 1.
Our total one-period contribution function is then
ˆCt, t+1(Rt, at, ˆDt+1) = p min(Rt + at, ˆDt+1) −cpat.
The expected contribution is
Ct(St, at) = E{p min(Rt + at, ˆDt+1) −cpat}.
■
■
EXAMPLE 5.23
Consider again the asset acquisition problem above, but this time we place our
orders in period t to satisfy the known demand in period t. Our cost function
contains both a ﬁxed cost cf (which we pay for placing an order of any size)
and a variable cost cp. The cost function would look like
Ct(St, at) =

p min(Rt + at, ˆDt),
at = 0,
p min(Rt + at, ˆDt) −cf −cpat,
at > 0,
Note that our contribution function no longer contains information from the
next time period. If we did not incur a ﬁxed cost cf , then we would simply
look at the demand Dt and order the quantity needed to cover demand (as a
result there would never be any product left over). However, since we incur a
ﬁxed cost cf with each order, there is a beneﬁt to ordering enough to cover the
demand now and future demands. This beneﬁt is captured through the value
function.
■
There are many resource allocation problems where the contribution of a deci-
sion can be written using
ctra = unit contribution of acting on a resource with attribute
vector r with action a.
This contribution is incurred in period t using information available in period t. In
this case our total contribution at time t could be written
Ct(St, at) =

r∈R

a∈A
ctraxtra.
It is surprisingly common for us to want to work with two contributions. The
common view of a contribution function is that it contains revenues and costs that
we want to maximize or minimize. In many operational problems there can be a
mixture of “hard dollars” and “soft dollars.” The hard dollars are our quantiﬁable
revenues and costs. But there are often other issues that are important in an oper-
ational setting and cannot always be easily quantiﬁed. For example, if we cannot
cover all of the demand, we may wish to assess a penalty for not satisfying it.

208
modeling dynamic programs
We can then manipulate this penalty to reduce the amount of unsatisﬁed demand.
Examples of the use of soft-dollar bonuses and penalties abound in operational
problems (see examples).
■
EXAMPLE 5.24
A trucking company has to pay the cost of a driver to move a load, but wants
to avoid using inexperienced drivers for their high-priority accounts (yet has
to accept the fact that it is sometimes necessary). An artiﬁcial penalty can be
used to reduce the number of times this happens.
■
■
EXAMPLE 5.25
A charter jet company requires that in order for a pilot to land at night, he/she
has to have landed a plane at night three times in the last 60 days. If the
third time a pilot landed at night is at least 50 days ago, the company wants
to encourage assignments of these pilots to ﬂights with night landings so that
they can maintain their status. A bonus can be assigned to encourage these
assignments.
■
■
EXAMPLE 5.26
A student planning her schedule of courses has to face the possibility of failing
a course, which may require taking either an extra course one semester or a
summer course. She wants to plan out her course schedule as a dynamic pro-
gram but use a penalty to reduce the likelihood of having to take an additional
course.
■
■
EXAMPLE 5.27
An investment banker wants to plan a strategy to maximize the value of an
asset and minimize the likelihood of a very poor return. She is willing to
accept lower overall returns in order to achieve this goal and can do it by
incorporating an additional penalty when the asset is sold at a signiﬁcant
loss.
■
Given the presence of these so-called soft dollars, it is useful to think of two
contribution functions. We can let Ct(St, at) be the hard dollars and Cπ
t (St, at) be
the contribution function with the soft dollars included. The notation captures the
fact that a set of soft bonuses and penalties represents a form of policy. So we can
think of our policy as making decisions that maximize Cπ
t (St, at), but measure the
value of the policy (in hard dollars), using Ct(St, Aπ(St)).

the objective function
209
5.8.2
Finding the Best Policy
We are now ready to optimize to ﬁnd the best policy. Let Aπ
t (St) be a decision
function (equivalent to a policy) that determines what decision we make given that
we are in state St. Our optimization problem is to choose the best policy by choosing
the best decision function from the family (Aπ
t (St))π∈. We wish to choose the
best function that maximizes the total expected (discounted) contribution over a
ﬁnite (or inﬁnite) horizon. This would be written as
F ∗
0 = max
π∈ Eπ
 T

t=0
γ tCπ
t (St, Aπ
t (St))|S0

,
(5.16)
where γ discounts the money into time t = 0 values. We write the value of policy
π as
F π
0 = E
 T

t=0
γ tCπ
t (St, Aπ
t (St))|S0

.
In some communities, it is common to use an interest rate r, in which case the
discount factor is
γ =
1
1 + r .
Important variants of this objective function are the inﬁnite horizon problem (T =
∞), the undiscounted ﬁnite horizon problem (γ = 1), and the average reward,
given by
F π
0 = E

lim
T →∞
1
T
T −1

t=0
Cπ
t (St, Aπ
t (St))|S0

.
(5.17)
A word of explanation is in order when we write the expectation (5.16) as Eπ(·).
If we can pre-generate all the potential outcomes W(ω) in advance (without regard
to the dynamics of the system), then we would normally write the expectation E(·),
since the exogenous events are not affected by the policy. Of course, the policy
does affect the dynamics, but as long as it does not affect the random events, the
expectation does not depend on the policy. However, there are many problems
where the exogenous events depend on the states that we visit, and possibly the
actions we take. For this reason it is safest to express the expectation as dependent
on the policy.
Our optimization problem is to choose the best policy. In most practical appli-
cations we can write the optimization problem as one of choosing the best policy,
or
F ∗
0 = max
π∈ F π
0 .
(5.18)

210
modeling dynamic programs
It might be the case that a policy is characterized by a continuous parameter (the
speed of a car, the temperature of a process, the price of an asset). In theory,
we could have a problem where the optimal policy corresponds to a value of a
parameter being equal to inﬁnity. It is possible that F ∗
0 exists but that an optimal
“policy” does not exist (because it requires ﬁnding a parameter equal to inﬁnity).
While this is more of a mathematical curiosity, we handle these situations by
writing the optimization problem as
F ∗
0 = sup
π∈
F π
0 ,
(5.19)
where “sup” is the supremum operator, which ﬁnds the smallest number greater
than or equal to F π
0 for any value of π. If we were minimizing, we would use “inf,”
which stands for “inﬁmum,” which is the largest value less than or equal to the
value of any policy. It is common in more formal treatments to use “sup” instead of
“max” or “inf” instead of “min” since these are more general. Our emphasis is on
computation and approximation, where we consider only problems where a solution
exists. For this reason we use “max” and “min” throughout our presentation.
The expression (5.16) contains one important but subtle assumption that will
prove to be critical later and that will limit the applicability of our techniques
in some problem classes. Speciﬁcally, we assume the presence of what is known
as linear, additive utility. That is, we have added up contributions for each time
period. It does not matter if the contributions are discounted or if the contribution
functions are nonlinear. However, we will not be able to handle functions that look
like
F π = Eπ




t∈T
γ tCt(St, Aπ
t (St))
2

.
(5.20)
The assumption of linear, additive utility means that the total contribution is a
separable function of the contributions in each time period. While this works for
many problems, it certainly does not work for all of them, as presented in the
examples below.
■
EXAMPLE 5.28
We may value a policy of managing a resource using a nonlinear function
of the number of times the price of a resource dropped below a certain
amount.
■
■
EXAMPLE 5.29
Suppose that we have to ﬁnd the route through a network where the traveler is
trying to arrive at a particular point in time. The value function is a nonlinear
function of the total lateness, which means that the value function is not a
separable function of the delay on each link.
■

a measure-theoretic view of information
211
■
EXAMPLE 5.30
Consider a mutual fund manager who has to decide how much to allocate
among aggressive stocks, conservative stocks, bonds, and money market instru-
ments. Let the allocation of assets among these alternatives represent a policy
π. The mutual fund manager wants to maximize long-term return, but needs to
be sensitive to short-term swings (the risk). He can absorb occasional down-
swings, but wants to avoid sustained downswings over several time periods.
Thus his value function must consider not only his return in a given time
period but also how his return looks over one-year, three-year, and ﬁve-year
periods.
■
In some cases these apparent instances of violations of linear, additive utility
can be solved using a creatively deﬁned state variable.
5.9
A MEASURE-THEORETIC VIEW OF INFORMATION**
For researchers interested in proving theorems or reading theoretical research arti-
cles, it is useful to have a more fundamental understanding of information.
When we work with random information processes and uncertainty, it is standard
in the probability community to deﬁne a probability space, which consists of three
elements. The ﬁrst is the set of outcomes , which is generally assumed to represent
all possible outcomes of the information process (actually,  can include outcomes
that can never happen). If these outcomes are discrete, then all we would need is
the probability of each outcome p(ω).
It is nice to have a terminology that allows for continuous quantities. We want to
deﬁne the probabilities of our events, but if ω is continuous, we cannot talk about
the probability of an outcome ω. However, we can talk about a set of outcomes
E that represent some speciﬁc event (if our information is a price, the event E
could be all the prices that constitute the event that the price is greater than some
number). In this case we can deﬁne the probability of an outcome E by integrating
the density function p(ω) over all ω in the event E.
Probabilists handle continuous outcomes by deﬁning a set of events F, which is
literally a “set of sets” because each element in F is itself a set of outcomes in .
This is the reason that we resort to the script font F as opposed to our calligraphic
font for sets; it is easy to read E as “calligraphic E” and F as “script F.” The set F
has the property that if an event E is in F, then its complement  \ E is in F, and
the union of any two events EX ∪EY in F is also in F. F is called a “sigma-algebra”
(which may be written “σ-algebra”), and is a countable union of outcomes in .
An understanding of sigma-algebras is not important for computational work but
can be useful in certain types of proofs, as we see in the “why does it work”
sections at the end of several chapters. Sigma-algebras are without question one
of the more arcane devices used by the probability community, but once they are
mastered, they are a powerful theoretical tool.

212
modeling dynamic programs
Finally, it is required that we specify a probability measure denoted P, which
gives the probability (or density) of an outcome ω, which can then be used to
compute the probability of an event in F.
We can now deﬁne a formal probability space for our exogenous information
process as (, F, P). If we wish to take an expectation of some quantity that
depends on the information, say, Ef (Wt), then we would sum (or integrate) over
the set ω multiplied by the probability (or density) P.
It is important to emphasize that ω represents all the information that will
become available, over all time periods. As a rule we are solving a problem at
time t, which means we do not have the information that will become available
after time t. To handle this, we let Ft be the sigma-algebra representing events
that can be created using only the information up to time t. To illustrate, consider
an information process Wt consisting of a single 0 or 1 in each time period. Wt
may be the information that a customer purchases a jet aircraft, or the event that
an expensive component in an electrical network fails. If we look over three time
periods, there are eight possible outcomes, as shown in Table 5.2.
Let E{W1} be the set of outcomes ω that satisfy some logical condition on W1.
If we are at time t = 1, we only see W1. The event W1 = 0 would be written
E{W1=0} = {ω|W1 = 0} = {1, 2, 3, 4}.
The sigma-algebra F1 would consist of the events
{E{W1=0}, E{W1=1}, E{W1∈{0,1}}, E{W1 /∈{0,1}}}.
Now assume that we are at time t = 2 and have access to W1 and W2. With
this information, we are able to divide our outcomes  into ﬁner subsets. Our
history H2 consists of the elementary events H2 = {(0, 0), (0, 1), (1, 0), (1, 1)}.
Let h2 = (0, 1) be an element of H2. The event E{h2=(0,1)} = {3, 4}. At time t =
1, we could not tell the difference between outcomes 1, 2, 3, and 4; now that we
are at time 2, we can differentiate between ω ∈{1, 2} and ω ∈{3, 4}. The sigma-
algebra F2 consists of all the events Eh2, h2 ∈H2, along with all possible unions
and complements.
Table 5.2
Set of demand outcomes
Outcome
Time Period
ω
1
2
3
1
0
0
0
2
0
0
1
3
0
1
0
4
0
1
1
5
1
0
0
6
1
0
1
7
1
1
0
8
1
1
1

bibliographic notes
213
Another event in F2 is {ω|(W1, W2) = (0, 0)} = {1, 2}. A third event in F2 is
the union of these two events, which consists of ω = {1, 2, 3, 4} that, of course,
is one of the events in F1. In fact every event in F1 is an event in F2, but not the
other way around. The reason is that the additional information from the second
time period allows us to divide  into ﬁner set of subsets. Since F2 consists of all
unions (and complements), we can always take the union of events, which is the
same as ignoring a piece of information. By contrast, we cannot divide F1 into a
ﬁner subsets. The extra information in F2 allows us to ﬁlter  into a ﬁner set of
subsets than was possible when we only had the information through the ﬁrst time
period. If we are in time period 3, F will consist of each of the individual elements
in  as well as all the unions needed to create the same events in F2 and F1.
From this example we see that more information (i.e., the ability to see more
elements of W1, W2, . . .) allows us to divide  into ﬁner-grained subsets. For this
reason we can always write Ft−1 ⊆Ft. Ft always consists of every event in Ft−1 in
addition to other ﬁner events. As a result of this property, Ft is termed a ﬁltration.
It is because of this interpretation that the sigma-algebras are typically represented
using the script letter F (which literally stands for ﬁltration) rather the more natural
letter H (which stands for history). The fancy font used to denote a sigma-algebra
is used to designate that it is a set of sets (rather than just a set).
It is always assumed that information processes satisfy Ft−1 ⊆Ft. Interestingly
this is not always the case in practice. The property that information forms a
ﬁltration requires that we never “forget” anything. In real applications this is not
always true. Suppose that we are doing forecasting using a moving average. This
means that our forecast ft might be written as ft = (1/T ) T
t′=1 ˆDt−t′. Such a
forecasting process “forgets” information that is older than T time periods.
There are numerous textbooks on measure theory. For a nice introduction to
measure-theoretic thinking (and in particular the value of measure-theoretic think-
ing), see Pollard (2002).
5.10
BIBLIOGRAPHIC NOTES
Section 5.2
Figure 5.1, which describes the mapping from continuous to dis-
crete time, was outlined for me by Erhan Cinlar.
Section 5.3
The multiattribute notation for multiple resource classes is based
primarily on Simao et al. (2001).
Section 5.4
The deﬁnition of states is amazingly confused in the stochastic
control literature. The ﬁrst recognition of the difference between the physical
state and the state of knowledge appears to be in Bellman and Kalaba (1959),
which used the term “hyperstate” to refer to the state of knowledge. The
control literature has long used state to represent a sufﬁcient statistic (e.g.,
see Kirk, 1998), representing the information needed to model the system
forward in time. For an introduction to partially observable Markov deci-
sion processes, see White (1991). An excellent description of the modeling
of Markov decision processes from an AI perspective is given in Boutilier

214
modeling dynamic programs
et al. (1999), including a very nice discussion of factored representations.
See also Guestrin et al. (2003) for an application of the concept of factored
state spaces to a Markov decision process.
Section 5.5
Our notation for decisions represents an effort to bring together
the ﬁelds of dynamic programming and math programming. We believe this
notation was ﬁrst used in Simao et al. (2001). For a classical treatment of
decisions from the perspective of Markov decision processes, see Puterman
(2005). For examples of decisions from the perspective of the optimal control
community, see Kirk (1998) and Bertsekas (2005). For examples of treat-
ments of dynamic programming in economics, see Stokey and R. E. Lucas
(1989) and Chow (1997).
Section 5.6
Our representation of information follows classical styles in the
probability literature (see, for example, Chung (1974)). Considerable attention
has been given to the topic of supervisory control. An example includes
Werbos (1992b).
PROBLEMS
5.1
A college student must plan what courses she takes over each of eight
semesters. To graduate, she needs 34 total courses, while taking no more
than ﬁve and no less than three courses in any semester. She also needs
two language courses, one science course, eight departmental courses in her
major and two math courses.
(a) Formulate the state variable for this problem in the most compact way
possible.
(b) Give the transition function for our college student assuming that she
successfully passes any course she takes. You will need to introduce
variables representing her decisions.
(c) Give the transition function for our college student, but now allow for
the random outcome that she may not pass every course.
5.2
Suppose that we have N discrete resources to manage, where Ra is the
number of resources of type a ∈A and N = 
a∈A Ra. Let R be the set of
possible values of the vector R. Show that
|R| =
 N + |A| −1
|A| −1

,
where
 X
Y

=
X!
Y!(X −Y)!
is the number of combinations of X items taken Y at a time.

problems
215
5.3
A broker is working in thinly traded stocks. He must make sure that he does
not buy or sell in quantities that would move the price, and he feels that
if he works in quantities that are no more than 10 percent of the average
sales volume, he should be safe. He tracks the average sales volume of a
particular stock over time. Let ˆvt be the sales volume on day t, and assume
that he estimates the average demand ft using ft = (1 −α)ft−1 + α ˆvt. He
then uses ft as his estimate of the sales volume for the next day. Assuming
he started tracking demands on day t = 1, what information would constitute
his state variable?
5.4
How would your previous answer change if our broker used a 10-
day moving average to estimate his demand? That is, he would use
ft = 0.10 10
i=1 ˆvt−i+1 as his estimate of the demand.
5.5
The pharmaceutical industry spends millions managing a sales force to push
the industry’s latest and greatest drugs. Suppose that one of these salesmen
must move between a set I of customers in his district. He decides which
customer to visit next only after he completes a visit. For this exercise,
assume that his decision does not depend on his prior history of visits (that
is, he may return to a customer he has visited previously). Let Sn be his
state immediately after completing his nth visit that day.
(a) Assume that it takes exactly one time period to get from any customer
to any other customer. Write out the deﬁnition of a state variable, and
argue that his state is only his current location.
(b) Assume that τij is the (deterministic and integer) time required to move
from location i to location j. What is the state of our salesman at any
time t? Be sure to consider both the possibility that he is at a location
(having just ﬁnished with a customer) or between locations.
(c) Assume that the travel time τij follows a discrete uniform distribution
between aij and bij (where aij and bij are integers)?
5.6
Consider a simple asset acquisition problem where xt is the quantity pur-
chased at the end of time period t to be used during time interval t + 1.
Let Dt be the demand for the assets during time interval t. Let Rt be the
pre-decision state variable (the amount on hand before you have ordered xt)
and Rx
t be the post-decision state variable.
(a) Write the transition function so that Rt+1 is a function of Rt, xt, and
Dt+1.
(b) Write the transition function so that Rx
t is a function of Rx
t−1, Dt, and
xt.
(c) Write Rx
t as a function of Rt, and write Rt+1 as a function of Rx
t .
5.7
As a buyer for an orange juice products company, you are responsible for
buying futures for frozen concentrate. Let xtt′ be the number of futures you
purchase in year t that can be exercised during year t′.

216
modeling dynamic programs
(a) What is your state variable in year t?
(b) Write out the transition function.
5.8
A classical inventory problem works as follows. Assume that our state vari-
able Rt is the amount of product on hand at the end of time period t and that
Dt is a random variable giving the demand during time interval (t −1, t)
with distribution pd = P (Dt = d). The demand in time interval t must be
satisﬁed with the product on hand at the beginning of the period. We can
then order a quantity xt at the end of period t that can be used to replenish
the inventory in period t + 1. Give the transition function that relates Rt+1
to Rt.
5.9
Many problems involve the movement of resources over networks. The
deﬁnition of the state of a single resource, however, can be complicated by
different assumptions for the probability distribution for the time required
to traverse a link. For each example below, give the state of the resource:
(a) You have a deterministic, static network, and you want to ﬁnd the short-
est path from an origin node q to a destination node r. There is a known
cost cij for traversing each link (i, j).
(b) Next assume that the cost cij is a random variable with an unknown
distribution. Each time you traverse a link (i, j), you observe the cost
ˆcij, which allows you to update your estimate cij of the mean of cij.
(c) Finally assume that when the traveler arrives at node i, he sees ˆcij for
each link (i, j) out of node i.
(d) A taxicab is moving people in a set of cities C. After dropping a pas-
senger off at city i, the dispatcher may have to decide to reposition the
cab from i to j, (i, j) ∈C. The travel time from i to j is τij, which
is a random variable with a discrete uniform distribution (that is, the
probability that τij = t is 1/T, for t = 1, 2, . . . , T). Assume that the
travel time is known before the trip starts.
(e) Same as (d), but now the travel times are random with a geometric distri-
bution (i.e., the probability that τij = t is (1 −θ)θt−1 for t = 1,2,3, . . .).
5.10
In Figure 5.3, a sailboat is making its way upwind from point A to point B.
To do this, the sailboat must tack, whereby it sails generally at a 45-degree
angle to the wind. The problem is that the angle of the wind tends to shift
randomly over time. The skipper decides to check the angle of the wind each
minute and must decide whether the boat should be on port or starboard tack.
Note that the proper decision must consider the current location of the boat,
which we may indicate by an (x, y) coordinate.
(a) Formulate the problem as a dynamic program. Carefully deﬁne the state
variable, decision variable, exogenous information, and the contribution
function.

problems
217
A
B
Path of 
boat:
Wind
:
Port
tack:
Starboard
tack:
Figure 5.3
(b) Use δ to discretize any continuous variables (in practice, you might
choose different levels of discretization for each variable, but we are
going to keep it simple). In terms of δ, give the size of the state space,
the number of exogenous outcomes (in a single time period), and the
action space. If you need an upper bound on a variable (e.g., wind speed),
simply deﬁne an appropriate variable and express your answer in terms
of this variable. All your answers should be expressed algebraically.
(c) Using a maximum wind speed of 30 miles per hour and δ = 0.1, compute
the size of your state, outcome and action spaces.
5.11
Implement your model from exercise 5.10 as a Markov decision process,
and solve it using the techniques of 3 (Section 3.2). Choose a value of
δ that makes your program computationally reasonable (run times under
10 minutes). Let δ be the smallest value of δ that produces a run time
(for your computer) of under 10 minutes, and compare your solution (in
terms of the total contribution) for δ = δ
N for N = 2, 4, 8, 16. Evaluate
the quality of the solution by simulating 1000 iterations using the value
functions obtained using backward dynamic programming. Plot your average
contribution function as a function of δ.
5.12
What is the difference between the history of a process, and the state of a
process?
5.13
As the purchasing manager for a major citrus juice company, you have
the responsibility of maintaining sufﬁcient reserves of oranges for sale or
conversion to orange juice products. Let xti be the amount of oranges that
you decide to purchase from supplier i in week t to be used in week t + 1.

218
modeling dynamic programs
Each week you can purchase up to ˆqti oranges (i.e., xti ≤ˆqti) at a price
ˆpti from supplier i ∈I, where the price/quantity pairs ( ˆpti, ˆqti)i∈I ﬂuctuate
from week to week. Let s0 be your total initial inventory of oranges, and let
Dt be the number of oranges that the company needs for production during
week t (this is our demand). If we are unable to meet demand, the company
must purchase additional oranges on the spot market at a spot price ˆpspot
ti
.
(a) What is the exogenous stochastic process for this system?
(b) What are the decisions you can make to inﬂuence the system?
(c) What would be the state variable for your problem?
(d) Write out the transition equations.
(e) What is the one-period contribution function?
(f) Propose a reasonable structure for a decision rule for this problem, and
call it Xπ. Your decision rule should be in the form of a function that
determines how much to purchase in period t.
(g) Carefully and precisely, write out the objective function for this problem
in terms of the exogenous stochastic process. Clearly identify what you
are optimizing over.
(h) For your decision rule, what do we mean by the space of policies?
5.14
Customers call in to a service center according to a (nonstationary) Poisson
process. Let E be the set of events representing phone calls, where te, e ∈E
is the time that the call is made. Each customer makes a request that will
require time τe to complete and will pay a reward re to the service center.
The calls are initially handled by a receptionist who determines τe and re.
The service center does not have to handle all calls and obviously favors calls
with a high ratio of reward per time unit required (re/τe). For this reason the
company adopts a policy that the call will be refused if (re/τe) < γ . If the
call is accepted, then it is placed in a queue to wait for one of the available
service representatives. Assume that the probability law driving the process
is known, where we would like to ﬁnd the right value of γ .
(a) This process is driven by an underlying exogenous stochastic process
with element ω ∈. What is an instance of ω?
(b) What are the decision epochs?
(c) What is the state variable for this system? What is the transition function?
(d) What is the action space for this system?
(e) Give the one-period reward function.
(f) Give a full statement of the objective function that deﬁnes the Markov
decision process. Clearly deﬁne the probability space over which the
expectation is deﬁned, and what you are optimizing over.
5.15
A major oil company is looking to build up its storage tank reserves, antici-
pating a surge in prices. It can acquire 20 million barrels of oil, and it would
like to purchase this quantity over the next 10 weeks (starting in week 1).

problems
219
At the beginning of the week, the company contacts its usual sources, and
each source j ∈J is willing to provide ˆqtj million barrels at a price ˆptj. The
price/quantity pairs ( ˆptj, ˆqtj) ﬂuctuate from week to week. The company
would like to purchase (in discrete units of millions of barrels) xtj million
barrels (where xtj is discrete) from source j in week t ∈{1, 2, . . . , 10}.
Your goal is to acquire 20 million barrels while spending the least amount
possible.
(a) What is the exogenous stochastic process for this system?
(b) What would be the state variable for your problem? Give an equation(s)
for the system dynamics.
(c) Propose a structure for a decision rule for this problem and call it Xπ.
(d) For your decision rule, what do we mean by the space of policies? Give
examples of two different decision rules.
(e) Write out the objective function for this problem using an expectation
over the exogenous stochastic process.
(f) You are given a budget of $300 million to purchase the oil, but you
absolutely must end up with 20 million barrels at the end of the 10 weeks.
If you exceed the initial budget of $300 million, you may get additional
funds, but each additional $1 million will cost you $1.5 million. How
does this affect your formulation of the problem?
5.16
You own a mutual fund where at the end of each week t you must decide
whether to sell the asset or hold it for an additional week. Let ˆrt be the
one-week return (e.g., ˆrt = 1.05 means the asset gained ﬁve percent in the
previous week), and let pt be the price of the asset if you were to sell it in
week t (so pt+1 = pt ˆrt+1). We assume that the returns ˆrt are independent
and identically distributed. You are investing this asset for eventual use in
your college education, which will occur in 100 periods. If you sell the asset
at the end of time period t, then it will earn a money market rate q for each
time period until time period 100, at which point you need the cash to pay
for college.
(a) What is the state space for our problem?
(b) What is the action space?
(c) What is the exogenous stochastic process that drives this system? Give
a ﬁve time period example. What is the history of this process at
time t?
(d) You adopt a policy that you will sell if the asset falls below a price p
(which we are requiring to be independent of time). Given this policy,
write out the objective function for the problem. Clearly identify exactly
what you are optimizing over.


C H A P T E R
6
Policies
Perhaps one of the most widely used and poorly understood terms in dynamic
programming is policy. A simple deﬁnition of a policy is:
Deﬁnition 6.0.1
A policy is a rule (or function) that determines a decision given
the available information in state St.
The problem with the concept of a policy is that it refers to any method for deter-
mining an action given a state, and as a result it covers a wide range of algorithmic
strategies, each suited to different problems with different computational require-
ments. Given the vast diversity of problems that fall under the umbrella of dynamic
programming, it is important to have a strong command of the types of policies that
can be used, and how to identify the best type of policy for a particular decision.
In this chapter we review a range of policies, grouped under four broad cate-
gories:
Myopic policies. These are the most elementary policies. They optimize costs/
rewards now, but do not explicitly use forecasted information or any direct
representation of decisions in the future. However, they may use tunable
parameters to produce good behaviors over time.
Lookahead policies. These policies make decisions now. They explicitly opti-
mize over some horizon by combining an approximation of future informa-
tion, with an approximation of future actions.
Policy function approximations. These are functions which directly return an
action given a state, without resorting to any form of imbedded optimization,
and without directly using any forecast of future information.
Value function approximations. These policies are often referred to as greedy
policies. They depend on an approximation of the value of being in a future
state as a result of a decision made now. The impact of a decision now on
the future is captured purely through a value function that depends on the
state that results from a decision now.
Approximate Dynamic Programming: Solving the Curses of Dimensionality, Second Edition.
Warren B. Powell.
2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
221

222
policies
These four broad categories span a wide range of strategies for making decisions.
Further complicating the design of clean categories is the ability to form hybrids
combined of strategies from two or even three of these categories. However, we
feel that these four fundamental categories offer a good starting point. We note that
the term “value function approximation” is widely used in approximate dynamic
programming, while “policy function approximation” is a relatively new phrase,
although the idea behind policy function approximations is quite old. We use this
phrase to emphasize the symmetry between approximating the value of being in a
state, and approximating the action we should take given a state. Recognizing this
symmetry will help synthesize what is currently a disparate literature on solution
strategies.
Before beginning our discussion, it is useful to brieﬂy discuss the nature of
these four strategies. Myopic policies are the simplest, since they make no explicit
attempt to capture the impact of a decision now on the future. Lookahead policies
capture the effect of decisions now by explicitly optimizing in the future (perhaps
in an approximate way) using some approximation of information that is not known
now. A difﬁculty is that lookahead strategies can be computationally expensive. For
this reason considerable energy has been devoted to developing approximations.
The most popular strategy involves developing an approximation V(s) of the value
around the pre- or post-decision state. If we use the more convenient form of
approximating the value function around the post-decision state, we can make
decisions by solving
at = arg max
a

C(St, a) +V(SM,a(St, a))

.
(6.1)
If we have a vector-valued decision xt with feasible region Xt, we would solve
xt = arg max
x∈Xt

C(St, x) +V(SM,x(St, x))

.
(6.2)
Equations (6.1) and (6.2) both represent policies. For example, we can write
Aπ(St) = arg max
a

C(St, a) +V(SM,a(St, a))

.
(6.3)
When we characterize a policy by a value function approximation, π refers to a
speciﬁc approximation in a set  that captures the family of value function approx-
imations. For example, we might use V(s) = θ0 + θ1s + θ2s2. In this case we might
write the space of policies as V FA−LR to represent value function approximation-
based policies using linear regression. π ∈V FA−LR would represent a particular
setting of the vector (θ0, θ1, θ2).
Remark
Some authors like to write the decision function in the form
Aπ
t (St) ∈arg max
a

C(St, a) +V(SM,a(St, a))

.
The “∈” captures the fact that there may be more than one optimal solution, which
means the arg maxa(·) is really a set. If this is the case, then we need some rule for
choosing which member of the set we are going to use. Most algorithms make this

policies
223
choice at random (typically we ﬁnd a best solution, and replace it only when we
ﬁnd a solution that is even better, rather than just as good). In rare cases we do care.
However, if we use Aπ
t (St) ∈arg max, then we have to introduce additional logic
to solve this problem. We assume that the “arg max” operator includes whatever
logic we are going to use to solve this problem (since this is typically what happens
in practice).
Alternatively, we may try to directly estimate a decision function Aπ(St) (or
Xπ(St)), which does not have the embedded maxa (or maxx) within the function.
We can think of V(Sa
t ) (where Sa
t = SM,a(St, a)) as an approximation of the value
of being in (post-decision) state Sa
t (while following some policy in the future).
At the same time we can think of Aπ(St) as an approximation of the decision we
should make given that we are in (pre-decision) state St. V(s) is almost always a
real-valued function. Since we have deﬁned a to always refer to discrete actions,
Aπ(s) is a discrete function, while Xπ(s) may be a scalar continuous function,
or a vector-valued function (discrete or continuous). For example, we might use
Xπ(s) = θ0 + θ1s + θ2s2 to compute a decision x directly from a continuous state s.
Whether we are trying to estimate a value function approximation V(s), or a
decision function Aπ(s) (or Xπ(s)), it is useful to identify three categories of
approximation strategies:
Lookup tables. Also referred to as tabular functions, lookup tables mean that
we have a discrete value V(s) (or action Aπ(s)) for each discrete state s.
Parametric representations. These are explicit, analytic functions for V(s) or
Aπ(s) that generally involve a vector of parameters that we typically represent
by θ.
Nonparametric representations. Nonparametric representations offer a more
general way of representing functions, but at a price of greater complexity.
We have a separate category for lookup tables because they represent a special
case. Primarily they are the foundation of the ﬁeld of Markov decision processes
on which the original theory is based. Also lookup tables can be represented as
a parametric model (with one parameter per state), but they share important char-
acteristics of nonparametric models (since both focus on the local behavior of
functions).
Some authors in the community take the position (sometimes implicitly) that
approximate dynamic programming is equivalent to ﬁnding value function approxi-
mations. While designing policies based on value function approximations arguably
remains one of the most powerful tools in the ADP toolbox, it is virtually impossi-
ble to create boundaries between a policy based on a value function approximation,
and a policy based on direct search. Direct search is effectively equivalent to max-
imizing a value function, even if we are not directly trying to estimate the value
function. Lookahead policies are little more than an alternative method for approx-
imating the value of being in a state, and these are often used as heuristics within
other algorithmic strategies.
In the remainder of this chapter, we illustrate the four major categories of poli-
cies. For the case of value function and policy function approximations, we draw

224
policies
on the three major ways of representing functions. However, we defer to Chapter 8
an in-depth presentation of methods for approximating value functions. Chapter 7
discusses methods for ﬁnding policy function approximations in greater detail.
6.1
MYOPIC POLICIES
The most elementary class of policy does not explicitly use any forecasted infor-
mation, or any attempt to model decisions that might be implemented in the future.
In its most basic form, a myopic policy is given by
AMyopic(St) = arg max
a
C(St, a).
(6.4)
Myopic policies are actually widely used in resource allocation problems. Consider
the dynamic assignment problem introduced in Section 2.2.10, where we are trying
to assign resources (e.g., medical technicians) to tasks (equipment that needs to be
installed). Let I be the set of technicians, and let ri be the attributes of the ith
technician (location, skills, time away from home). Let Jt be the tasks that need to
be handled at time t, and let bj be the attributes of task j, which can include the
time window in which it has to be served and the reward earned from serving the
task. Finally, let xtij = 1 if we assign technician i to task j at time t and let ctij
be the contribution of assigning technician i to task j at time t that captures the
revenue we receive from serving the task minus the cost of assigning a particular
technician to the task. We might solve our problem at time t using
xt = arg max
x∈Xt

i∈I

j∈Jt
ctijxtij,
where the feasible region Xt is deﬁned by

j∈Jt
xtij ≤1
∀i ∈I,

i∈I
xtij ≤1
∀j ∈Jt,
xtij ≥0.
Note that we have no difﬁculty handling a high-dimensional decision vector xt,
since it requires only that we solve a simple linear program. The decision xt
ignores its effect on the future, hence its label as a myopic policy.
6.2
LOOKAHEAD POLICIES
Lookahead policies make a decision now by solving an approximation of the
problem over some horizon. These policies are distinguished from value function

lookahead policies
225
and policy function approximations by the explicit representation of both future
information and future decisions. In this section we review tree search, roll-out
heuristics, and rolling horizon procedures, all of which are widely used in engi-
neering practice.
6.2.1
Tree Search
Tree search is a brute force strategy that enumerates all possible actions and all
possible outcomes over some horizon. As a rule this can only be used for fairly
short horizons, and even then only when the number of actions and outcomes
is not too large. Tree search is identical to solving decision trees, as we did in
Section 2.2.1.
If a full tree search can be performed for some horizon T , we would use the
results to pick the action to take right now. The calculations are exact, subject to
the single limitation that we are only solving the problem over T time periods
rather than the full (and possibly inﬁnite) horizon.
6.2.2
Sparse Sampling Tree Search
If the action space is not too large, we may limit the explosion due to the number
of outcomes by replacing the enumeration of all outcomes with a Monte Carlo
sample of outcomes. It is possible to actually bound the error from the optimal
solution if enough outcomes are generated, but in practice this quickly produces
the same explosion in the size of the outcome tree. Instead, this is best viewed as a
potential heuristic that provides a mechanism for limiting the number of outcomes
in the tree.
6.2.3
Roll-out Heuristics
For problems where the number of actions per state is fairly large, we may replace
the explicit enumeration of the entire tree with some sort of heuristic policy to
evaluate what might happen after we reach a state. Imagine that we are trying
to evaluate if we should take action a0, which takes us to location A. Instead
of enumerating the tree out of location A, we might follow some simple policy
(perhaps a myopic policy) starting at A and extending a reasonable number of time
periods in the future. The value of this trajectory is then used purely to develop
an estimate of the value of being in location A. This process is repeated for each
action a0 so that we get a rough estimate of the downstream value of each of these
actions. From this we choose the best value for a0 given these rough estimates.
The idea is illustrated in Figure 6.1, assuming that we are using a myopic policy
to evaluate future trajectories.
The advantage of this strategy is that it is simple and fast. It provides a better
approximation than using a pure myopic policy, since in this case we are using a
myopic policy to provide a rough estimate of the value of being in a state. This can
work quite well, but it can work very poorly. The performance is very dependent
on the application.

226
policies
Procedure MyopicRollOut(a)
Step 0. Initialize: Given initial state s, increment tree depth m = m + 1.
Step 1. Find the best myopic decision using
a = arg max
a∈A
C(s, a).
Step 2. If m < M, do:
Step 3a. Sample the random information W given state s.
Step 3b. Compute s′ = SM(s, a, W)
Step 3c. Compute
V
n(s′) = MyopicRollOut(s′).
Step 3d. Find the value of the decision
ˆv = C(s, a) + γV
n(s′).
Else
ˆv = C(s, a).
End if:
Step 4. Approximate the value of the state using:
MyopicRollOut = ˆv.
Figure 6.1
Approximating the value of being in a state using a myopic policy.
It is also possible to choose a decision using the current value function approx-
imation, as in
a = arg max
a∈A

C(s, a) +V
n−1(SM(s, a))

.
We note that we are most dependent on this logic in the early iterations when the
estimates V
n may be poor, but some estimate may be better than nothing.
In approximate dynamic programming (and in particular the reinforcement learn-
ing community), it is common practice to introduce an artiﬁcial discount factor.
This factor is given its own name λ, reﬂecting the common (and occasionally
annoying) habit of the reinforcement learning community to name algorithms after
the notation used. If we introduce this factor, step 3c of Figure 6.1 would be
replaced by
ˆv = C(s, a) + γ λV
n(s′).
In this setting γ is viewed as a ﬁxed model parameter, while λ is a tunable algorith-
mic parameter. Its use in this setting reﬂects the fact that we are trying to estimate
the value of a state s′ by following some heuristic policy from s′ onward. For
example, imagine that we are using this idea to play a game, and we ﬁnd that our

lookahead policies
227
heuristic policy results in a loss. Should this loss be reﬂected in a poor value for
state s′ (because it led to a loss), or is it possible that state s′ is actually a good state
to be in, but the loss resulted from poor decisions made when we tried to search
forward from s′? Since we do not know the answer to this question, we discount
values further into the future so that a negative outcome (or positive outcome) does
not have too large of an impact on the results.
6.2.4
Rolling Horizon Procedures
Typically the reason we cannot solve our original stochastic optimization problem to
optimality is the usual explosion of problem size when we try to solve a stochastic
optimization problem over an inﬁnite or even a long horizon. As a result a natural
approximation strategy is to try to solve the problem over a shorter horizon. This
is precisely what we were doing in Section 6.2.1 using tree search, but that method
was limited to discrete (and typically small) action spaces.
Imagine that we are at time t (in state St), and that we can solve the problem
optimally over the horizon from t to t + H for a sufﬁciently small horizon H. We
can then implement the decision at (or xt if we are dealing with vectors), after
which we simulate our way to a single state St+1. We then repeat the process by
optimizing over the interval t + 1 to t + H + 1. In this sense we are “rolling” the
horizon one time period forward. This method also goes under the names receding
horizon procedure (common in operations research), or model predictive control
(common in the engineering control theory literature).
Our hope is that we can solve the problem over a sufﬁciently long horizon
that the decision at (or xt) “here and now” is a good one. The problem is that if
we want to solve the full stochastic optimization problem, we might be limited to
extremely short horizons. For this reason we typically have to resort to approxi-
mations even to solve shorter horizon problems. These strategies can be divided
based on whether we are using a deterministic forecast of the future, or a stochastic
one. A distinguishing feature of these methods is that we can handle vector-valued
problems.
Deterministic Forecasts
A popular method in practice for building a policy for stochastic, dynamic problems
is to use a point forecast of future exogenous information to create a deterministic
model over a H-period horizon. In fact the term “rolling horizon procedure” (or
model predictive control) is often interpreted to speciﬁcally refer to the use of
deterministic forecasts. We illustrate the idea using a simple example drawn from
energy systems analysis. We switch to vector-valued decisions x, since a major
feature of deterministic forecasts is that it makes it possible to use standard math
programming solvers that scale to large problems.
Consider the problem of managing how much energy we should store in a battery
to help power a building that receives energy from the electric power grid (but at
a random price) or solar panels (but with random production due to cloud cover).

228
policies
We assume that we can always purchase power from the grid, but the price may
be quite high.
Let
Rt = amount of energy stored in the battery at time t,
pt = price of electricity purchased at time t from the grid,
ht = energy production from the solar panel at time t,
Dt = demand for electrical power in the building at time t.
The state of our system is given by St = (Rt, pt, ht, Dt). The system is controlled
using
xgb
t
= amount of energy stored in the battery from the grid at price pt at time t,
xsb
t
= amount of energy stored in the battery from the solar panels at time t,
xsd
t
= amount of energy directed from the solar panels to serve demand at time t,
xbd
t
= amount of energy drawn from the battery to serve demand at time t,
xgd
t
= amount of energy drawn from the grid to serve demand at time t.
We let xt = (xgb
t , xsb
t , xsd
t , xbd
t ) be the decision vector. The cost to meet demand
at time t is given by
C(St, xt) = pt

xgb
t
+ xgd
t

.
The challenge with deciding what to do right now is that we have to think about
demands, prices, and solar production in the future. Demand and solar production
tend to follow a daily pattern, although demand rises early in the morning more
quickly than solar production, and can remain high in the evening after solar pro-
duction has disappeared. Prices tend to be highest in the middle of the afternoon,
and as a result we try to have energy stored in the battery to reduce our demand
for expensive electricity at this time.
We can make the decision xt by optimizing over the horizon from t to t + H.
While pt′, ht′ and Dt′, for t′ > t, are all random variables, we are going to replace
them with forecasts ptt′, htt′, and Dtt′, all made with information available at
time t. Since these are deterministic, we can formulate the following deterministic
optimization problem:
min
xtt,...,xt,t+H
t+H

t′=t
ptt′

xgb
tt′ + xgd
tt′

(6.5)
subject to
Rt′+1 = Rt′ + xgb
tt′ + xsb
tt′ −xbd
tt′ ,
(6.6)
xbd
tt′ ≤Rt′,
(6.7)
xgb
tt′ , xsb
tt′, xbd
tt′ , xsd
tt′ ≥0.
(6.8)

lookahead policies
229
We note that we are letting xtt′ be a type of forecast of what we think we will do
at time t′ when we solve the optimization problem at time t. It is useful to think
of this as a forecast of a decision. We project decisions over this horizon because
we need to know what we would do in the future in order to know what we should
do right now.
The optimization problem 6.5–6.8 is a deterministic linear program. We can
solve this using, say, 1 minute increments over the next 12 hours (720 time periods)
without difﬁculty. However, we are not interested in the values of xt+1, . . . , xt+H.
We are only interested in xt, which we implement at time t. As we advance the
clock from t to t + 1, we are likely to ﬁnd that the random variables have not
evolved precisely according to the forecast, giving us a problem starting at time
t + 1 (extending through t + H + 1) that is slightly different than what we thought
would be the case.
Our deterministic model offers several advantages. First, we have no difﬁculty
handling the property that xt is a continuous vector. Second, the model easily han-
dles the highly nonstationary nature of this problem, with daily cycles in demand,
prices, and solar production. Third, if a weather forecast tells us that the solar
production will be less than normal six hours from now, we have no difﬁculty
taking this into account. Thus knowledge about the future does not complicate the
problem.
At the same time, the inability of the model to handle uncertainty in the future
introduces signiﬁcant weaknesses. One problem is that we would not feel that
we need to store energy in the battery in the event that solar production might
be lower than we expect. Second, we may wish to store electricity in the battery
during periods when electricity prices are lower than normal, something that would
be ignored in a forecast of future prices, since we would not forecast stochastic
variations from the mean.
Deterministic rolling horizon policies are widely used in operations research,
where a deterministic approximation provides value in many applications. The real
value of this strategy is that it opens the door for using commercial solvers that
can handle vector-valued decisions. These policies are rarely seen in the classical
examples of reinforcement learning that focus on small action spaces but that also
focus on problems where a deterministic approximation of the future would not
produce an interesting model.
Stochastic Forecasts
The tree search and roll-out heuristics were both methods that captured, in differ-
ent ways, uncertainty in future exogenous events. In fact tree search is, technically
speaking, a rolling horizon procedure. However, neither of these methods can han-
dle vector-valued decisions, something that we had no difﬁculty handling if we
were willing to assume we knew future events were known deterministically.
We can extend our deterministic, rolling horizon procedure to handle uncertainty
in an approximate way. The techniques have evolved under the umbrella of a ﬁeld
known as stochastic programming, which describes the mathematics of introducing
uncertainty in math programs. Introducing uncertainty in multistage optimization

230
policies
problems in the presence of vector-valued decisions is intrinsically difﬁcult, and not
surprisingly, there does not exist a computationally tractable algorithm to provide
an exact solution to this problem. However, some practical approximations have
evolved. We illustrate the simplest strategy here.
We start by dividing the problem into two stages. The ﬁrst stage is time t,
representing our “here and now” decision. We assume we know the state St at time
t. The second stage starts at time t + 1, at which point we assume we know all the
information that will arrive from t + 1 through t + H. The approximation here is
that the decision xt+1 is allowed to “see” the entire future.
Even this fairly strong approximation is not enough to allow us to solve this
problem, since we still cannot compute the expectation over all possible realizations
of future information exactly. Instead, we resort to Monte Carlo sampling. Let ω
be a sample path representing a sample realization of all the random variables
(pt′, ht′, Dt′)t′ > t from t + 1 until t + H. We use Monte Carlo methods to generate
a ﬁnite sample  of potential outcomes. If we ﬁx ω, then this is like solving the
deterministic optimization problem above, but instead of using forecasts ptt′, htt′,
and Dtt′, we use sample realizations ptt′(ω), htt′(ω), and Dtt′(ω). Let p(ω) be
the probability that ω happens, where this might be as simple as p(ω) = 1/||.
We further note that ptt(ω) = ptt for all ω (the same is true for the other random
variables). We are going to create decision variables xtt(ω), . . . , xt,t+H(ω) for all
ω ∈. We are going to allow xtt′(ω) to depend on ω for t′ > t, but then we are
going to require that xtt(ω) be the same for all ω.
We now solve the optimization problem
min
xtt,xtt(ω),...,xt,t+H (ω),∀ω ptt

xgb
tt + xgd
tt

+

ω∈
p(ω)
t+H

t′=t+1
ptt′(ω)

xgb
tt′ (ω) + xgd
tt′ (ω)

subject to the constraints
Rt′+1(ω) = Rt′(ω) + xgb
tt′ (ω) + xsb
tt′(ω)xbd
tt′ (ω),
(6.9)
xbd
tt′ (ω) ≤Rt′(ω),
(6.10)
xgb
tt′ (ω), xsb
tt′(ω), xbd
tt′ (ω), xsd
tt′ (ω) ≥0.
(6.11)
If we only imposed the constraints (6.9)–(6.11), then we would have a different
solution xtt(ω) for each scenario, which means we are allowing our decision now
to see into the future. To prevent this, we impose nonanticipativity constraints
xtt(ω) −xtt = 0.
(6.12)
Equation (6.12) means that we have to choose one decision xtt regardless of the
outcome ω, which means that we are not allowed to see into the future. However,
we are allowing xt,t+1(ω) to depend on ω, which speciﬁes not only what happens
between t and t′ but also what happens for the entire future. This means that at
time t + 1, we are allowing our decision to see future energy prices, future solar
output, and future demands. However, this is only for the purpose of approximating

lookahead policies
231
the problem so that we can make a decision at time t, which is not allowed to see
into the future.
This formulation allows us to make a decision at time t while modeling the
stochastic variability in future time periods. In the stochastic programming com-
munity, the outcomes ω are often referred to as scenarios. If we model 20 scenarios,
then our optimization problem becomes roughly 20 times larger, so the introduction
of uncertainty in the future comes at a signiﬁcant computational cost. However, this
formulation allows us to capture the variability in future outcomes, which can be a
signiﬁcant advantage over using simple forecasts. Furthermore, while the problem
is certainly much larger, it is still a deterministic optimization problem that can be
solved with standard commercial solvers.
There are numerous applications where there is a desire to break the second
stage into additional stages, so we do not have to make the approximation that
xt+1 sees the entire future. However, if we have || outcomes in each stage, then
if we have K stages, we would be modeling ||K scenario paths. Needless to
say, this grows quickly with K. Modelers overcome this by reducing the number
of scenarios in the later stages, but it is hard to assess the errors that are being
introduced.
Rolling Horizon with Discounting
A common objective function in dynamic programming uses discounted costs,
whether over a ﬁnite or inﬁnite horizon. If C(St, xt) is the contribution earned
from implementing decision xt when we are in state St, our objective function
might be written
max
π
E
∞

t=0
γ tC(St, Xπ(St)),
where γ is a discount factor that captures, typically the time value of money. If
we choose to use a rolling horizon policy (with deterministic forecasts), our policy
might be written
Xπ(St) = arg max
xt,...,xt+H
t+H

t′=t
γ t′−tC(St′, xt′).
Let’s consider what a discount factor might look like if it only captures the time
value of money. Imagine that we are solving an operational problem where each
time step is one hour (there are many applications where time steps are in minutes
or even seconds). If we assume a time value of money of 10 percent per year, then
we would use γ = 0.999989 for our hourly discount factor. If we use a horizon of,
say, 100 hours, then we might as well use γ = 1. Not surprisingly, it is common
to introduce an artiﬁcial discount factor to reﬂect the fact that decisions made at
t′ = t + 10 should not carry the same weight as the decision xt that we are going
to implement right now. After all, xt′ for t′ > t is only a forecast of a decision that
might be implemented.

232
policies
In our presentation of roll-out heuristics, we introduced an artiﬁcial discount
factor λ to reduce the inﬂuence of poor decisions backward through time. In our
rolling horizon model, we are actually making optimal decisions, but only for a
deterministic approximation (or perhaps an approximation of a stochastic model).
When we introduce this new discount factor, our rolling horizon policy would be
written
Xπ(St) = arg max
xt,...,xt+H
t+H

t′=t
γ t′−tλt′−tC(St′, xt′).
In this setting, γ plays the role of the original discount factor (which may be equal
to 1.0, especially for ﬁnite horizon problems), while λ is a tunable parameter. An
obvious motivation for the use of λ in a rolling horizon setting is the recognition
that we are solving a deterministic approximation over a horizon in which events
are uncertain.
6.2.5
Discussion
Lookahead strategies have long been a popular method for designing policies for
stochastic, dynamic problems. Note that in each case we use both a model of exoge-
nous information and some sort of method for making decisions in the future. Since
optimizing over the future, even for a shorter horizon, quickly becomes compu-
tationally intractable, different strategies are used for simplifying the outcomes of
future information, and simplifying how decisions might be made in the future.
Even with these approximations, lookahead procedures can be computation-
ally demanding. It is largely as a result of a desire to ﬁnd faster algorithms that
researchers have developed methods based on value function approximations and
policy function approximations.
6.3
POLICY FUNCTION APPROXIMATIONS
It is often the case that we have a very good idea of how to make a decision, and
we can design a function (i.e., a policy) that returns a decision which captures the
structure of the problem. For example:
■
EXAMPLE 6.1
A policeman would like to give tickets to maximize the revenue from the
citations he writes. Stopping a car requires about 15 minutes to write up the
citation, and the ﬁnes on violations within 10 miles per hour of the speed
limit are fairly small. Violations of 20 miles per hour over the speed limit
are signiﬁcant, but relatively few drivers fall in this range. The policeman can
formulate the problem as a dynamic program, but it is clear that the best policy
will be to choose a speed, say s, above which he writes out a citation. The
problem is choosing s.
■

policy function approximations
233
■
EXAMPLE 6.2
A utility wants to maximize the proﬁts earned by storing energy in a battery
when prices are lowest during the day, and releasing the energy when prices
are highest. There is a fairly regular daily pattern to prices. The optimal policy
can be found by solving a dynamic program, but it is fairly apparent that the
policy is to charge the battery at one time during the day, and discharge it at
another. The problem is identifying these times.
■
Policies come in many forms, but all are functions that return a decision given
an action. In this section we are limiting ourselves to functions that return a policy
directly from the state, without solving an embedded optimization problem. As with
value function approximations, which return an estimate of the value of being in a
state, policy functions return an action given a state. Policy function approximations
come in three basic styles:
Lookup tables.
When we are in a discrete state s, the function returns a discrete
action a directly from a matrix.
Parametric representations.
These are explicit, analytic functions that are
designed by the analyst. Similar to the process of choosing basis functions
for value function approximations, designing a parametric representation of
a policy is an art form.
Nonparametric representations.
Nonparametric statistics offer the potential of
producing very general behaviors without requiring the speciﬁcation of basis
functions.
Since a policy is any function that returns an action given a state, it is important
to recognize when designing an algorithm if we are using (1) a rolling horizon
procedure, (2) a value function approximation, or (3) a parametric representation
of a policy. Readers need to understand that when a reference is made to “pol-
icy search” and “policy gradient methods,” this usually means that a parametric
representation of a policy is being used.
We brieﬂy describe each of these classes below.
6.3.1
Lookup Table Policies
We have already seen lookup table representations of policies in Chapter 3. By a
lookup table, we speciﬁcally refer to a policy where every state has its own action
(similar to having a distinct value of being in each state). This means we have one
parameter (an action) for each state. We exclude from this class any policies that
can be parameterized by a smaller number of parameters.
It is fairly rare that we would use a lookup table representation in an algorithmic
setting, since this is the least compact form. However, lookup tables are relatively
common in practice, since they are easy to understand. The Transportation Safety
Administration (TSA) has speciﬁc rules that determine when and how a passenger
should be searched. Call-in centers use speciﬁc rules to govern how a call should

234
policies
be routed. A utility will use rules to govern how power should be released to the
grid to avoid a system failure. Lookup tables are easy to understand, and easy to
enforce. But, in practice, they can be very hard to optimize.
6.3.2
Parametric Policy Models
Without question the most widely used form of policy function approximation is the
simple analytic models parameterized by a low dimensional vector. Some examples
include:
• We are holding a stock, and would like to sell it when it goes over a price θ.
• In an inventory policy, we will order new product when the inventory St falls
below θ1. When this happens, we place an order at = θ2 −St, which means
we “order up to” θ2.
• We might choose to set the output xt from a water reservoir, as a function of the
state (the level of the water) St of the reservoir, as a linear function of the form
xt = θ0 + θ1St. Or we might desire a nonlinear relationship with the water
level, and use a basis function φ(St) to produce a policy xt = θ0 + θ1φ(St).
These are known in the control literature as afﬁne policies (literally, linear in
the parameters).
Designing parametric policy function approximations is an art form. The science
arises in determining the parameter vector θ.
To choose θ, assume that we have a parametric policy Aπ(St|θ) (or Xπ(St|θ)),
where we express the explicit dependence of the policy on θ. We would then
approach the problem of ﬁnding the best value of θ as a stochastic optimization
problem, written in the form
max
θ
F(θ) = E
T

t−0
γ tC(St, Aπ(St, θ)).
Here we write maxθ whereas before we would write maxπ∈ to express the fact
that we are optimizing over a class of policies  that captures the parametric
structure. maxπ∈ is more generic, but in this setting, it is equivalent to maxθ.
The major challenge we face is that we cannot compute F(θ) in any compact
form, primarily because we cannot compute the expectation. We have to depend
instead on Monte Carlo samples. Fortunately, there is an entire ﬁeld known as
stochastic search (see Spall, 2003, for an excellent overview) to help us with this
process. We describe these algorithms in more detail in Chapter 7.
There is another strategy we can use to ﬁt a parametric policy model. LetV
n−1(s)
be our current value function approximation at state s. Assume that we are currently
at a state Sn and then use our value function approximation to compute an action
an:
an = arg max
a

C(Sn, a) +V
n−1(SM,a(Sn, a))

.

value function approximations
235
We can now view an as an “observation” of the action an corresponding to state
Sn. When we ﬁt value function approximations, we used the observation ˆvn of the
value of being in state Sn to ﬁt our value function approximation. With policies,
we can use an as the observation corresponding to state Sn to ﬁt a function that
“predicts” the action we should be taking when in state Sn.
6.3.3
Nonparametric Policy Models
The strengths and weaknesses of using parametric models for policy function
approximations is the same as we encountered when using them for approximating
value functions. Not surprisingly, there has been some interest in using nonparamet-
ric models for policies. We provide a brief introduction to nonparametric methods
in Chapter 8.
The basic idea of a nonparametric policy is to use nonparametric statistics, such
as kernel regression, to ﬁt a function that predicts the action a when we are in state
s. Just as we described above with a parametric policy model, we can compute an
using a value function approximation V(s) when we are in state s = SN. We then
use the pairs of responses an and independent variables (covariates) Sn to ﬁt the
model.
6.4
VALUE FUNCTION APPROXIMATIONS
The most powerful and visible method for solving complex dynamic programs
involves replacing the value function Vt(St) with an approximation of some form.
In fact, for many in the research community, approximate dynamic programming is
viewed as being equivalent to replacing the value function with an approximation
as a way of avoiding “the” curse of dimensionality. If we are approximating the
value around the pre-decision state, a value function approximation means that we
are making decisions using equations of the form
at = arg max
a

C(St, a) + γ E
'
V(SM(St, a, Wt+1))|St
(
.
(6.13)
In Chapter 4 we discussed the challenge of computing the expectation, and argued
that we can approximate the value around the post-decision state Sa
t , giving us
at = arg max
a

C(St, a) +V(SM,a(St, a))

.
(6.14)
For our presentation here we are not concerned with whether we are approximating
around the pre- or post-decision state. We simply want to consider instead different
types of approximation strategies. There are three broad classes of approximation
strategies that we can use:
Lookup tables. If we are in discrete state s, we have a stored value V(s) that
gives an estimate of the value of being in state s.

236
policies
Parametric models. These are analytic functions V(s|θ) parameterized by a
vector θ whose dimensionality is much smaller than the number of states.
Nonparametric models. These models draw on the ﬁeld of nonparametric statis-
tics, and that avoid the problem of designing analytic functions.
We describe each of these brieﬂy below, but defer a more complete presentation
to Chapter 8.
6.4.1
Lookup Tables
Lookup tables are the most basic way of representing a function. This strategy is
only useful for discrete states, and typically assumes a ﬂat representation where
the states are numbered s ∈{1, 2, . . . , |S|}, where S is the original set of states,
where each state might consist of a vector. Using a ﬂat representation, we have
one parameter, V(s), for each value of s (we refer to the value of being in each
state as a parameter, since it is a number that we have to estimate). As we saw in
Chapter 4, if ˆvn is a Monte Carlo estimate of the value of being in a state Sn, then
we can update the value of being in state Sn using
V
n(Sn) = (1 −α)V
n−1(Sn) + α ˆV n.
Thus we only learn about V
n(Sn) by visiting state Sn. If the state space is large,
then this means that we have a very large number of parameters to estimate.
The power of lookup tables is that if we have a discrete state space, then in
principle we can eventually approximate V (S) with a high level of precision (if
we visit all the states often enough). The down side is that visiting state s tells us
nothing about the value of being in state s′ ̸= s.
6.4.2
Parametric Models
One of the simplest ways to avoid the curse of dimensionality is to replace value
functions using a lookup table with some sort of regression model. We start by
identifying what we feel are important features. If we are in state S, a feature is
simply a function φf (S), f ∈F that draws information from S. We would say
that F (or more properly (φf (S))f ∈F) is our set of features. In the language of
approximate dynamic programming the functions φf (S) are referred to as basis
functions.
Creating features is an art form, and depends on the problem. Some examples
are as follows:
• We are trying to manage blood inventories, where Rti is the number of units
of blood type i ∈(1, 2, . . . , 8) at time t. The vector Rt is our state variable.
Since Rt is a vector, the number of possible values of Rt may be extremely
large, making the challenge of approximating V (R) quite difﬁcult. We might
create a set of features φ1
i (R) = Ri, φ2
i (R) = R2
i , φ3
ij(R) = RiRj.

value function approximations
237
• Perhaps we wish to design a computer to play tic-tac-toe. We might design
features such as (1) the number of X’s in corner points, (2) 0/1 to indicate if
we have the center point, (3) the number of side points (excluding corners)
that we have and (4) the number of rows and columns where we have X’s in
at least two elements.
Once we have a set of features, we might specify a value function approximation
using
V(S) =

f ∈F
θf φf (S),
(6.15)
where θ is a vector of regression parameters to be estimated. Finding θ is the
science within the art form. When we use an approximation of the form given in
equation (6.15), we say that the function has a linear architecture or is a linear
model, or simply linear. Statisticians will often describe (6.15) as a linear model,
but care has to be used, because the basis functions may be nonlinear functions of
the state. Just because an approximation is a linear model does not mean that the
function is linear in the state variables.
If we are working with discrete states, it can be analytically convenient to create
a matrix  with element φf (s) in row s, column f . If V is our approximate value
function, expressed as a column vector with an element for every state, then we
can write the approximation using matrix algebra as
V = θ.
There are, of course, problems where a linear architecture will not work. For
example, perhaps our value function is expressing a probability that we will win at
backgammon, and we have designed a set of features φf (S) that capture important
elements of the board, where S is the precise state of the board. We propose that
we can create a utility function U(S|θ) = 
f θf φf (S) that provides a measure of
the quality of our position. However, we do not observe utility; we do observe if
we eventually win or lose a game. If V = 1, then we win, and we might write the
probability of this using
P (V = 1|θ) =
eU(S|θ)
1 + eU(S|θ) .
(6.16)
We would describe this model as one with a nonlinear architecture, or nonlinear
in the parameters. Our challenge is estimating θ from observations of whether we
win or lose the game.
Parametric models are exceptionally powerful when they work. Their real value
is that we can estimate a parameter vector θ using a relatively small number of
observations. For example, we may feel that we can do a good job if we have 10
observations per feature. Thus, with a very small number of observations, we obtain
a model that offers the potential of approximating an entire function, even if S is

238
policies
V (S)
S
(S)
0 0
1 1
V (S)
(S)
(S)
Figure 6.2
Illustration of linear (in the state) value function approximation of a nonlinear value
function (V(S)), given the density µ(S) for visiting states.
multidimensional and continuous. Needless to say, this idea has attracted consider-
able interest. In fact many people even equate “approximate dynamic programming”
with “approximating value functions using basis functions.”
The downside of parametric models is that we may not have the right basis
functions to produce a good approximation. As an example consider the problem
of approximating the value of storing natural gas in a storage facility, shown in
Figure 6.2. Here V (S) is the true value function as a function of the state S,
which gives the amount of gas in storage, and µ(S) is the probability distribution
describing the frequency with which we visit different states. Now assume that we
have chosen basis functions where φ0(S) = 1 (the constant term) and φ1(S) = S
(linear in the state variable). With this choice, we have to ﬁt a value function that
is nonlinear in S, with a function that is linear in S. Figure 6.2 depicts the best
linear approximation given µ(S), but obviously this would change quite a bit if
the distribution describing what states we visited changed.
6.4.3
Nonparametric Models
Nonparametric models can be viewed as a hybrid of lookup tables and parametric
models, but in some key respects are actually closer to lookup tables. There is a
wide range of nonparametric models, but they all share the fundamental feature
of using simple models (typically constants or linear models) to represent small
regions of a function. They avoid the need to specify a speciﬁc structure such as
the linear architecture in (6.15) or a nonlinear architecture such as that illustrated
in (6.16). At the same time, most nonparametric models offer the additional feature
of providing very accurate approximations of very general functions, as long as we
have enough observations.
There are a variety of nonparametric methods, and the serious reader should
consult a reference such as Hastie et al. (2009). The central idea is to estimate a
function by using a weighted estimate of local observations of the function (see
Figure 6.3). Say we want to estimate the value function V(s) at a particular state
(called the query state) s. We have made a series of observations ˆv1, ˆv2, . . . , ˆvn

hybrid strategies
239
Kh (s, si)
h
s
Figure 6.3
Kernel with bandwidth h being used to estimate the value function at a query state s.
at states S1, S2, . . . , Sn. We can form an estimate V(s) using
V(s) =
n
i=1 Kh(s, si)ˆvi
n
i=1 Kh(s, si) ,
(6.17)
where Kh(s, si) is a weighting function parameterized by a bandwidth h. A com-
mon choice of weighting function is the Gaussian density, given by
Kh(s, si) = exp −
s −si
h
2
.
Here the bandwidth h serves as the standard deviation. This form of weighting is
often referred to as a radial basis function in the approximate dynamic program-
ming literature, which has the unfortunate effect of placing it in the same family of
approximation strategies as linear regression. However, parametric and nonpara-
metric approximation strategies are fundamentally different, with very different
implications in terms of the design of algorithms.
Nonparametric methods offer signiﬁcant ﬂexibility. Unlike parametric models,
where we are ﬁnding the parameters of a ﬁxed functional form, nonparametric meth-
ods do not have a formal model, since the data forms the model. Any observation
of the function at a query state s requires summing over all prior observations,
which introduces additional computational requirements. For approximate dynamic
programming, we encounter the same issue we saw in Chapter 4 of exploration.
With lookup tables, we have to visit a particular state in order to estimate the value
of a state. With kernel regression, we relax this requirement, but we still have to
visit points close to a state in order to estimate the value of a state.
6.5
HYBRID STRATEGIES
The concept of (possibly tunable) myopic policies, lookahead policies, policies
based on value function approximations, and policy function approximation rep-
resent the core tools in the arsenal for approximating solving dynamic programs.

240
policies
Given the richness of applications, it perhaps should not be surprising that we often
turn to mixtures of these strategies.
6.5.1
Myopic Policies with Tunable Parameters
Myopic policies are so effective for some problem classes that all we want to do is
to make them a little smarter, which is to say make decisions that do a better job
capturing the impact of a decision now on the future. We can create a family of
myopic policies by introducing tunable parameters that might help achieve good
behaviors over time. Imagine that task j ∈Jt needs to be handled before time τj.
If a task is not handled within the time window, it vanishes from the system and we
lose the reward that we might have earned. Now deﬁne a modiﬁed reward given by
cπ
tij(θ) = cij + θ0e−θ1(τj −t).
(6.18)
This modiﬁed contribution increases the reward (provided that θ0 and θ1 are both
positive) for covering a task as it gets closer to its due date τj. The reward is
controlled by the parameter vector θ = (θ0, θ1). We can now write our myopic
policy in the form
Xπ(St) = arg max
x∈Xt

i∈I

j∈Jt
cπ
tij(θ)xtij.
(6.19)
The policy π is characterized by the function in (6.19), which includes the choice
of the vector θ. We note that by tuning θ, we might be able to get our myopic policy
to make decisions that produce better decisions in the future. But the policy does
not explicitly use any type of forecast of future information or future decisions.
6.5.2
Rolling Horizon Procedures with Value Function Approximations
Deterministic rolling horizon procedures offer the advantage that we can solve them
optimally, and if we have vector-valued decisions, we can use commercial solvers.
Limitations of this approach are (1) they require that we use a deterministic view of
the future and (2) they can be computationally expensive to solve (pushing us to use
shorter horizons). By contrast, a major limitation of value function approximations
is that we may not be able to capture the complex interactions that are taking place
within our optimization of the future.
An obvious strategy is to combine the two approaches. For low-dimensional
action spaces, we can use tree search or a roll-out heuristics for H periods, and
then use a value function approximation. If we are using a rolling horizon procedure
for vector-valued decisions, we might solve
Xπ(St) = arg max
xt,...,xt+H
t+H−1

t′=t
γ t′−tC(St′, xt′) + γ HVt+H(St+H),
where St+H is determined by Xt+H . In this setting, Vt+H (St+H ) would have to be
some convenient analytical form (linear, piecewise linear, nonlinear) in order to

hybrid strategies
241
be used in an appropriate solver. If we use the algorithmic discount factor λ, the
policy would look like
Xπ(St) = arg max
xt,...,xt+H
t+H−1

t′=t
γ t′−tλt′−tC(St′, xt′) + γ HλHVt+H (St+H).
6.5.3
Rolling Horizon Procedures with Tunable Policies
A rolling horizon procedure using a deterministic forecast is, of course, vulnerable
to the use of a point forecast of the future. For example, in the energy application
we used in Section 6.2.4 to introduce rolling horizon procedures, we might use
a deterministic forecast of prices, energy from the solar panels, and demand. A
problem with this is that the model will optimize provided that these forecasts are
known perfectly, potentially leaving no buffer in the battery to handle surges in
demand or drops in solar energy.
This limitation will not be solved by introducing value function approximations
at the end of the horizon. It is possible, however, to perturb our deterministic
model to make the solution more robust with respect to uncertainty. For example,
we might reduce the amount of energy we expect to receive from the solar panel, or
we might inﬂate the demand. As we step forward in time, if we have overestimated
demand or underestimated the energy from the solar panel, we can put the excess
energy in the battery. Suppose that we factor demand by βD > 1, and factor solar
energy output by βS < 1. Now we have a rolling horizon policy parameterized by
(βD, βS). We can tune these parameters just as we would tune the parameters of
a policy function approximation.
Just as designing a policy function approximation is an art, designing these
adjustments to a rolling horizon policy (or any other lookahead strategy) is also
an art. Imagine optimizing the movement of a robot arm to take into considera-
tion variations in the mechanics. We can optimize over a horizon ignoring these
variations, but we might plan a trajectory that puts the arm too close to a barrier.
We can modify our rolling horizon procedure by introducing a margin of error (the
arm cannot be closer than β to a barrier), which becomes a tunable parameter.
6.5.4
Rollout Heuristics with Policy Function Approximation
Rollout heuristics can be particularly effective when we have access to a reasonable
policy to simulate decisions that we might take if an action a takes us to a state s′.
This heuristic might exist in the form of a policy function approximation Aπ(s|θ)
parameterized by a vector θ. We can tune θ, recognizing that we are still choosing
a decision based on the one period contribution C(s, a) plus the approximate value
of being in state s′ = SM(s, a, W).
6.5.5
Tree Search with Roll-out Heuristic and a Lookup Table Policy
A surprisingly powerful heuristic algorithm that has received considerable success
in the context of designing computer algorithms to play games uses a limited tree

242
policies
search, which is then augmented by a roll-out heuristic assisted by a user-deﬁned
lookup table policy. For example, a computer might evaluate all the options for
a chess game for the next four moves, at which point the tree grows explosively.
After four moves, the algorithm might resort to a roll-out heuristic, assisted by
rules derived from thousands of chess games. These rules are encapsulated in an
aggregated form of lookup table policy that guides the search for a number of
additional moves into the future.
6.5.6
Value Function Approximation with Lookup Table or
Policy Function Approximation
Suppose that we are given a policy A(St), which might be in the form of a lookup
table or a parameterized policy function approximation. This policy might reﬂect
the experience of a domain expert, or it might be derived from a large database
of past decisions. For example, we might have access to the decisions of people
playing online poker, or it might be the historical patterns of a company. We can
think of A(St) as the decision of the domain expert or the decision made in the
ﬁeld. If the action is continuous, we could incorporate it into our decision function
using
Aπ(St) = arg max
a

C(St, a) +V(SM,a(St, a)) + β(A(St) −a)2
.
The term β(A(St) −a)2 can be viewed as a penalty for choosing actions that
deviate from the external domain expert. β controls how important this term is.
We note that this penalty term can be set up to handle decisions at some level of
aggregation.
6.6
RANDOMIZED POLICIES
In Section 3.10.5 we introduced the idea of randomized policies, and showed that
under certain conditions a deterministic policy (one where the state S and a deci-
sion rule uniquely determine an action) will always do at least as well or better
than a policy that chooses an action partly at random. In the setting of approx-
imate dynamic programming, randomized policies take on a new role. We may,
for example, wish to ensure that we are testing all actions (and reachable states)
inﬁnitely often. This arises because in ADP, we need to explore states and actions
to learn how well they do.
The common notation in approximate dynamic programming for representing
randomized policies is to let π(a|s) be the probability of choosing action a given
we are in state s. This is known as notational overloading, since the policy in
this case is both the rule for choosing an action, and the probability that the rule
produces for choosing the action. We separate these roles by allowing π(a|s) to
be the probability of choosing action a if we are in state s (or, more often, the
parameters of the distribution that determines this probability), and let Aπ(s) be the

randomized policies
243
decision function, which might sample an action a at random using the probability
distribution π(a|s).
There are different reasons in approximate dynamic programming for using
randomized policies. We list three such:
• We often have to strike a balance between exploring states and actions, and
choosing actions that appear to be best (exploiting). The exploration versus
exploitation issue is addressed in depth in Chapter 12.
• In Chapter 9 (Section 9.4) we discuss the need for policies where the proba-
bility of selection for an action may be strictly positive.
• In Chapter 10 (Section 10.7) we introduce a class of algorithms that requires
that the policy be parameterized so that the probability of choosing an action is
differentiable in a parameter, which means that it also needs to be continuous.
The most widely used policy that mixes exploration and exploitation is called ϵ-
greedy. In this policy we choose an action a ∈A at random with probability ϵ,
and with probability 1 −ϵ:
an = arg max
a∈A

C(Sn, a) + γ EV
n−1 
SM 
Sn, a, W n+1 
,
which is to say we exploit our estimate of V
n−1(s′). This policy guarantees that
we will try all actions (and therefore all reachable states) inﬁnitely often, which
helps with convergence proofs. Of course, this policy may be of limited value for
problems where A is large.
One limitation of ϵ-greedy is that it keeps exploring at a constant rate. As the
algorithm progresses, we may wish to spend more time evaluating actions that we
think are best, rather than exploring actions at random. A strategy that addresses
this is to use a declining exploration probability, such as
ϵn = 1
n.
This strategy not only reduces the exploration probability, but it does so at a rate
that is sufﬁciently slow (in theory) that it ensures that we will still try all actions
inﬁnitely often. In practice, it is likely to be better to use a probability such as
1/nβ where β ∈(0.5, 1].
The ϵ-greedy policy exhibits a property known in the literature as “greedy
in the limit with inﬁnite exploration,” or GLIE. Such a property is important in
convergence proofs. A limitation of ϵ-greedy is that when we explore, we choose
actions at random without regard to their estimated value. In applications with
larger action spaces, this can mean that we are spending a lot of time evaluating
actions that are quite poor. An alternative is to compute a probability of choosing
an action based on its estimated value. The most popular of these rules uses
P (a|s) =
eβQn(s,a)

a′∈A eβQn(s,a′) ,

244
policies
whereQ
n(s, a) is an estimate of the value of being in state s and taking action a. β
is a tunable parameter, where β = 0 produces a pure exploration policy, while as
β →∞, the policy becomes greedy (choosing the action that appears to be best).
This policy is known under different names such as Boltzmann exploration,
Gibbs sampling and a soft-max policy. Boltzmann exploration is also known as a
“restricted rank-based randomized” policy (or RRR policy). The term “soft-max”
refers to the fact that it is maximizing over the actions based on the factors Q(s, a)
but produces an outcome where actions other than the best have a positive proba-
bility of being chosen.
6.7
HOW TO CHOOSE A POLICY?
Given the choice of policies, the question naturally arises, how do we choose
which policy is best for a particular problem? Not surprisingly, it depends on both
the characteristics of the problem and constraints on computation time and the
complexity of the algorithm. Below we summarize different types of problems,
and provide a sample of a policy that appears to be well suited to the application,
largely based on our own experiences with real problems.
6.7.1
A Myopic Policy
A business jet company has to assign pilots to aircraft to serve customers. In
making decisions about which jet and which pilot to use, it might make sense to
think about whether a certain type of jet is better suited given the destination of
the customer (are there other customers nearby who prefer that same type of jet)?
We might also want to think about whether we want to use a younger pilot who
needs more experience. However, the here-and-now costs of ﬁnding the pilot and
aircraft that can serve the customer at least cost dominate what might happen in
the future. Also the problem of ﬁnding the best pilot and aircraft, across a ﬂeet
of hundreds of aircraft and thousands of pilots, produces a fairly difﬁcult integer
programming problem. For this situation a myopic policy works extremely well
given the ﬁnancial objectives and algorithmic constraints.
6.7.2
A Lookahead Policy—Tree Search
You are trying to program a computer to operate a robot that is described by several
dozen different parameters describing the location, velocity, and acceleration of
both the robot and its arms. The number of actions is not too large, but choosing
an action requires considering not only what might happen next (exogenous events)
but also decisions we might make in the future. The state space is very large with
relatively little structure, which makes it very hard to approximate the value of
being in a state. Instead, it is much easier to simply step into the future, enumerating
future potential decisions with an approximation (e.g., with Monte Carlo sampling)
of random events.

how to choose a policy?
245
6.7.3
A Lookahead Policy—Deterministic Rolling Horizon Procedures
Now assume that we are managing an energy storage device for a building that has
to balance energy from the grid (where an advance commitment has been made
the previous day), an hourly forecast of energy usage (which is fairly predictable),
and energy generation from a set of solar panels on the roof. If the demand of the
building exceeds its commitment from the grid, it can turn to the solar panels and
battery, using a diesel generator as a backup. With a perfect forecast of the day, it
is possible to formulate the decision of how much to store and when, when to use
energy in the battery, and when to use the diesel generator as a single, deterministic
mixed-integer program. Of course, this ignores the uncertainty in both demand and
the sun, but most of the quantities can be predicted with some degree of accuracy.
The optimization model makes it possible to balance dynamics of hourly demand
and supply to produce an energy plan over the course of the day.
6.7.4
A Lookahead Policy—Stochastic Rolling Horizon Procedures
A utility has to operate a series of water reservoirs to meet demands for electricity,
while also observing a host of physical and legal constraints that govern how much
water is released and when. For example, during dry periods there are laws that
specify that the discomfort of low water ﬂows has to be shared. Of course, reservoirs
have maximum and minimum limits on the amount of water they can hold. There
can be tremendous uncertainty about the rainfall over the course of a season,
and as a result the utility would like to make decisions now while accounting
for the different possible outcomes. Stochastic programming enumerates all the
decisions over the entire year, for each possible scenario (while ensuring that only
one decision is made now). The stochastic program ensures that all constraints will
be satisﬁed for each scenario.
6.7.5
Policy Function Approximations
A utility would like to know the value of a battery that can store electricity when
prices are low and release them when prices are high. The price process is highly
volatile, with a modest daily cycle. The utility needs a simple policy that is easy
to implement in software. The utility chose a policy where we ﬁx two prices, and
store when prices are below the lower level and release when prices are above the
higher level. This requires optimizing these two price points. A different policy
might involve storing at a certain time of day, and releasing at another time of day,
to capture the daily cycle.
6.7.6
Value Function Approximations—Transportation
A truckload carrier needs to determine the best driver to assign to a load, which may
take several days to complete. The carrier has to think about whether it wants to
accept the load (if the load goes to a particular city, will there be too many drivers in
that city?). It also has to decide if it wants to use a particular driver, since the driver

246
policies
eventually needs to get home, and the load may take it to a destination from which
it is difﬁcult for the driver to eventually get home. Loads are highly dynamic, so
the carrier will not know with any precision which loads will be available when the
driver arrives. What has worked very well is to estimate the value of having a partic-
ular type of driver in a particular location. With this value, the carrier has a relatively
simple optimization problem to determine which driver to assign to each load.
6.7.7
Value Function Approximations—Energy
There is considerable interest in the interaction of energy from wind and hydro-
electric storage. Hydroelectric power is fairly easy to manipulate, making it an
attractive source of energy that can be paired with energy from wind. However,
this requires modeling the problem in hourly increments (to capture the variation
in wind) over an entire year (to capture seasonal variations in rainfall and the
water being held in the reservoir). It is also critical to capture the uncertainty in
both the wind and rainfall. The problem is relatively simple, and decisions can be
made using a value function approximation that captures only the value of storing
a certain amount of water in the reservoir. However, these value functions have to
depend on the time of year. Deterministic approximations provide highly distorted
results, and simple rules require parameters that depend on the time (hour) of year.
Optimizing a simple policy requires tuning thousands of parameters. Value func-
tion approximations make it relatively easy to obtain time-dependent decisions that
capture uncertainty.
6.7.8
Discussion
The examples of this section raise a series of questions that should be asked when
choosing the structure of a policy:
• Will a myopic policy solve your problem? If not, is it at least a good starting
point?
• Does the problem have structure that suggests a simple and natural deci-
sion rule? If there is an “obvious” policy (e.g., replenish inventory when it
gets too low), then more sophisticated algorithms based on value function
approximations are likely to struggle. Exploiting structure always helps.
• Is the problem fairly stationary, or highly nonstationary? Nonstationary prob-
lems (e.g., responding to hourly demand or daily water levels) mean that you
need a policy that depends on time. Rolling horizon problems can work well
if the level of uncertainty is low relative to the predictable variability. It is
hard to produce policy function approximations where the parameters vary by
time period.
• If you think about approximating the value of being in a state, does this appear
to be a relatively simple problem? If the value function is going to be very
complex, it will be hard to approximate, making value function approximations
hard to use. But if it is not too complex, value function approximations may
be a very effective strategy.
Unless you are pursuing an algorithm as an intellectual exercise, it is best to focus
on your problem and choose the method that is best suited to the application. For

problems
247
more complex problems, be prepared to use a hybrid strategy. For example, rolling
horizon procedures may be combined with adjustments that depend on tunable
parameters (a form of policy function approximation). You might use a tree search
combined with a simple value function approximation to help reduce the size of
the tree.
6.8
BIBLIOGRAPHIC NOTES
The goal of this chapter is to organize the diverse policies that have been suggested
in the ADP and RL communities into a more compact framework. In the process
we are challenging commonly held assumptions, for example, that “approximate
dynamic programming” always means that we are approximating value functions,
even if this is one of the most popular strategies. Lookahead policies and policy
function approximations are effective strategies for certain problem classes.
Section 6.2
Lookahead policies have been widely used in engineering practice
in operations research under the name of rolling horizon procedure, and in
computer science as a receding horizon procedure, and in engineering under
the name model predictive control (Camacho and Bordons, 2004). Decision
trees are similarly a widely used strategy for which there are many refer-
ences. Roll-out heuristics were introduced by Wu (1997) and Bertsekas and
Castanon (1999). Stochastic programming, which combines uncertainty with
multidimensional decision vectors, is reviewed in Birge and Louveaux (1997)
and Kall and Wallace (1994), among others. Secomandi (2008) studies the
effect of reoptimization on rolling horizon procedures as they adapt to new
information.
Section 6.3
While there are over 1000 papers that refer to “value function
approximation” in the literature (as of this writing), there were only a few
dozen papers using “policy function approximation.” However, this is a term
that we feel deserves more widespread use as it highlights the symmetry
between the two strategies.
Section 6.4
Making decisions that depend on an approximation of the value
of being in a state has deﬁned approximation dynamic programming since
Bellman and Kalaba (1959).
PROBLEMS
6.1
Following is a list of how decisions are made in speciﬁc situations. For each,
classify the decision function in terms of which of the four fundamental
classes of policies are being used. If a policy function approximation or value
function approximation is used, identify which functional class is being used:
• If the temperature is below 40 degrees F when I wake up, I put on a winter
coat. If it is above 40 but less than 55, I will wear a light jacket. Above
55, I do not wear any jacket.
• When I get in my car, I use the navigation system to compute the path I
should use to get to my destination.

248
policies
• To determine which coal plants, natural gas plants and nuclear power plants
to use tomorrow, a grid operator solves an integer program that plans over
the next 24 hours which generators should be turned on or off, and when.
This plan is then used to notify the plants who will be in operation tomor-
row.
• A chess player makes a move based on her prior experience of the proba-
bility of winning from a particular board position.
• A stock broker is watching a stock rise from $22 per share up to $36 per
share. After hitting $36, the broker decides to hold on to the stock for a
few more days because of the feeling that the stock might still go up.
• A utility has to plan water ﬂows from one reservoir to the next, while
ensuring that a host of legal restrictions will be satisﬁed. The problem can
be formulated as a linear program which enforces these constraints. The
utility uses a forecast of rainfalls over the next 12 months to determine
what it should do right now.
• The utility now decides to capture uncertainties in the rainfall by modeling
20 different scenarios of what the rainfall might be on a month-by-month
basis over the next year.
• A mutual fund has to decide how much cash to keep on hand. The mutual
fund uses the rule of keeping enough cash to cover total redemptions over
the last 5 days.
• A company is planning sales of TVs over the Christmas season. It produces
a projection of the demand on a week-by-week basis, but does not want to
end the season with zero inventories. So the company adds a function that
provides positive value for up to 20 TVs.
• A wind farm has to make commitments of how much energy it can pro-
vide tomorrow. The wind farm creates a forecast, including an estimate of
the expected amount of wind and the standard deviation of the error. The
operator then makes an energy commitment so that there is an 80 percent
probability that he will be able to make the commitment.
6.2
Earlier we considered the problem of assigning a resource i to a task j. If the
task is not covered at time t, we hold it in the hope that we can complete it
in the future. We would like to give tasks that have been delayed more higher
priority, so instead of just just maximizing the contribution cij, we add in a
bonus that increases with how long the task has been delayed, giving us the
modiﬁed contribution
cπ
tij(θ) = cij + θ0e−θ1(τj −t).
Now imagine using this contribution function, but optimizing over a time
horizon T using forecasts of tasks that might arrive in the future. Would
solving this problem, using cπ
tij(θ) as the contribution for covering task j
using resource i at time t, give you the behavior that you want?

C H A P T E R
7
Policy Search
In Chapter 6 we described four classes of policies: myopic policies, lookahead
policies, policies that depend on value function approximations, and policy function
approximations. Each of these policies may be tunable in some way.
Myopic policies. We choose the best action to maximize the contribution now,
without regard to the impact of the decision on the future. We may add
bonuses to serve a customer that has been delayed, or our myopic policy
may depend on design decisions such as the size of buffers or the location of
ambulance bases. These bonuses and constraints represent tunable parameters.
Lookahead policies. We optimize over a horizon H, possibly using a deter-
ministic representation of future events such as random customer demands
or deviations in trajectories of vehicles or robots. We might introduce buffer
stocks of resources to handle surges in demands, or boundaries to keep a
vehicle away from a cliff or barrier. The size of these buffer stocks and
boundaries represents tunable parameters, as would the horizon H.
Value function approximations. We make a decision now by maximizing
Aπ(S|θ) = arg max
a

C(S, a) + γ

f
θf φf (SM,a(S, a))

.
(7.1)
In this setting we can view the regression variables θ as tunable parameters.
Policy function approximations. Policy function approximations might be sim-
ple functions such as a rule to order Q −S units of inventory when the
inventory S < q, where Q and q are tunable parameters. Or, we may decide
that an action a can be expressed using
a = θ0 + θ1S + θ2S2.
In this setting the vector θ would be our tunable vector. Finally, it is popular
to use neural networks to represent a function that computes an action given
Approximate Dynamic Programming: Solving the Curses of Dimensionality, Second Edition.
Warren B. Powell.
2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
249

250
policy search
a state. The neural network is parameterized by a vector of weights w, which
would represent the tunable parameters.
As we can see, there is a wide range of ways that a policy can be tuned, with
different notations for the parameter vector being tuned. All of this literature falls
under the general umbrella of stochastic search, which is typically written
min
x EF(x, W).
Here x might be discrete, a continuous scalar, or a vector of binary, continuous,
or discrete variables, each introducing its own computational complexities. We use
x for consistency with the literature on stochastic search where x is a decision. In
our dynamic programming problems, x is a parameter that controls a policy that
is used to choose an action a. In a speciﬁc dynamic programming application, we
might use θ or ρ as our tunable parameter, which will help to avoid confusion
when our action is the vector x (so many things to do with so few variables!).
This chapter pursues a speciﬁc path for designing policies that is distinctly
different from the strategy that depends on approximating value functions. For
example, if our policy depends on a value function approximation (as shown in
equation (7.1)), in this chapter we are going to tune θ to produce the highest
contribution. By contrast, in Chapters 8 through 9 we are going to try to ﬁnd θ so
that the value function approximation closely approximates the value of being in
a state, with the hope that if we ﬁnd a good approximation, it will produce a good
policy. In this chapter we only care if θ produces a good policy, and not if the
resulting value function approximation approximates the value of being in a state.
Readers will (or should) reasonably ask, if the goal of this chapter is to ﬁnd
parameters to produce policies that work the best, why would we devote Chapters
8, 9, and 10 to ﬁnding value functions that predict the value of being in a state,
with the hope that this produces a good policy? One answer is that it is not always
easy to design a policy that, if properly tuned, produces good results. Another is
that the techniques in this chapter will generally work best if the parameter vector
x is fairly low dimensional. For example, it can be difﬁcult using policy search
to solve time-dependent problems, since you effectively have to optimize a policy
for each time period. This is much easier when we are using methods based on
approximating value functions. At the same time there are many problems where
the structure of a good policy is fairly self-evident. Examples include ordering
inventory when it falls below some level (we need to tune the right level), or sell a
stock when the price exceeds a limit (we need to ﬁnd the right limit). If we ignore
this structure, then we may ﬁnd that we are producing policies that underperform
simple rules.
7.1
BACKGROUND
The stochastic optimization problems that we consider in this volume can all be
written in the general form

background
251
max
π
E
T

t=0
γ tC(St, Aπ(St)).
(7.2)
This mathematical statement only makes sense when we understand the nature of
a policy, and then what we mean by ﬁnding the best policy. In this chapter we
assume we have some function (the policy) characterized by a vector of parameters
x. Our choice of the notation “x” reﬂects the roots of this theory to the communities
that work on stochastic search and stochastic optimization, where x is the decision
vector. Here x is a set of parameters that governs how the policy works. For
example, if a policy is given by
Aπ(St|θ) = arg max
a

C(St, a) + γ E

f
θf φf (St)

,
then this policy is parameterized by θ, and we would be searching for θ instead
of x. However, we might have an inventory system where St is the inventory, and
we order Q −St whenever St < q, and 0 otherwise. In this setting, x would be
the parameters (Q, q).
Now let
F(x) = EF π(x, W),
where
F π(x, W) =
T

t=0
γ tC(St, Aπ(St)).
(7.3)
Here we let W represent all the random variables that arise during our simulation.
For example, this could represent machine failures, customer demands, ﬂuctuations
in wind and prices, and deviations in robotic trajectories. We remind the reader that
W may depend on the policy or, equivalently, the parameter x, although we are
not going to make a point of this.
We note that there are many problems that can be written in the general form
min
x EF(x, W),
(7.4)
where F(x, W) is some function of a random variable where the expectation is
hard or impossible to compute, but where we can compute F(x, W), obtaining a
noisy (but unbiased) estimate of EF(x, W). If EF(x, W) were easy to compute,
we would have a deterministic optimization problem minx F(x), and we would not
need this chapter (or even this book).
Problems that can be written in the form given in equation (7.4) are often
described under a variety of names, including
Stochastic search. Stochastic search is probably the closest we can come to
a general name that covers the entire ﬁeld. Stochastic search often refers

252
policy search
to optimization of noisy functions, where x may be vector-valued and
continuous.
Ranking and selection. This represents a special case where we are searching
for x among a ﬁnite set {x1, x2, . . . , xM}, where M is “not too large.” The
size of M in this setting depends on the complexity of computing F(x, W).
Simulation optimization. Assume that we really do have a multiperiod sim-
ulation problem, such as might arise in discrete-event simulators for man-
ufacturing, simulations of call centers or simulations of the spread of a
disease. Typically a single run of a simulator can be moderately to very
time-consuming. We also have the ability to obtain more accurate estimates
by running a simulation for a longer period of time, with costs for stopping
and starting the simulator.
Optimization of expensive, black-box functions. There are applications where
a model may take hours, days, or even weeks to run. The challenge is to
tune parameters to obtain the best performance. A single run may be noisy
(stochastic), but not necessarily, and the behavior of the function is often
nonconvex.
All of these settings are mathematically equivalent, but introduce different compu-
tational issues. Our interest is primarily in multiperiod problems, but it is impossible
to ignore the mathematical equivalence between choosing a parameter to optimize
a sequential decision problem, and choosing a parameter to optimize a single-stage
problem (where you choose x, observe W and then stop).
For our purposes it is important to characterize the nature of the decision variable
x, which we can divide into four important classes:
• Scalar continuous. What is the right price of a product? What is the right
temperature of a chemical process?
• Discrete and ﬁnite. By this we mean a “not too large” number of discrete
alternatives (as described under ranking and selection above).
• Vector-valued and continuous. This means the search space is multidimen-
sional, requiring us to draw on a more general class of search algorithms that
are stochastic analogs of nonlinear programming algorithms.
• Vector-valued and discrete. These often arise in complex design problems,
such as ﬁnding the sequence of jobs to move through a machine, the best set
of proposals that should be funded, or the best locations for a set of facilities
(rather than just one facility).
We also need to separate three classes of functions:
• The function F(x, W), given W, is differentiable with respect to x. We refer
to ∇xF(x, W) as a stochastic gradient. Note that even if we can compute
derivatives, the function may be nondifferentiable. For example, we might
have F(x, W) = p min{x, W} −cx. For a given value of W,

gradient search
253
∂F(x, W)
∂x
=

p −c
if x < W,
−c
if x > W.
If x = W, the derivative is not deﬁned, but we can generally compute a
subgradient (for the example above, this is like choosing between p −c or
−c randomly), and still get provable convergence.
• We cannot compute the derivative (possibly because it is a complex simulation
and we simply do not know how to ﬁnd a derivative), but we can obtain noisy
observations of the function fairly quickly.
• We can obtain noisy observations of the function, but these are expensive,
putting special emphasis on intelligent search.
This discussion provides a peek into the rich ﬁelds that fall under the broad
umbrella of stochastic search. In this chapter we highlight some of the tools that
can be applied to approximate dynamic programming. However, we are unable to
provide a thorough introduction to the ﬁeld. For this purpose we refer readers to
the excellent survey by Span (2003), which is written at a similar mathematical
level to this book.
7.2
GRADIENT SEARCH
We can model our search for a policy parameterized by x in the following generic
form
min
x EF(x, W).
For example, assume that we are trying to solve a newsvendor problem, where
we wish to allocate a quantity x of resources (“newspapers”) before we know the
demand W. The optimization problem is given by
max
x
F(x) = Ep min{x, W} −cx.
(7.5)
If we could compute F(x) exactly (i.e., analytically), and its derivative, then we
could ﬁnd x∗by taking its derivative and setting it equal to zero. If this is not
possible, we could still use a classical steepest ascent algorithm
xn = xn−1 −αn−1∇xF(xn−1),
(7.6)
where αn−1 is a stepsize. For deterministic problems we typically choose the best
stepsize by solving the one-dimensional optimization problem
α∗= arg min
α
F

xn−1 −α∇F(xn−1)

.
(7.7)
We would then update xn using αn−1 = α∗.
We assume in our work that we cannot compute the expectation exactly. We
resort instead to an algorithmic strategy known as stochastic gradients, but also
known as stochastic approximation procedures.

254
policy search
7.2.1
A Stochastic Gradient Algorithm
For our stochastic problem we assume that we either cannot compute F(x) or
cannot compute the gradient exactly. However, there are many problems where, if
we ﬁx W = W(ω), we can ﬁnd the derivative of F(x, W(ω)) with respect to x.
Then, instead of using the deterministic updating formula in (7.6), we would use
xn = xn−1 −αn−1∇xF(xn−1, W n).
(7.8)
Here ∇xF(xn−1, W n) is called a stochastic gradient because it depends on a sample
realization of W n. It is important to note our indexing. A variable such as xn−1
or αn−1 that is indexed by n −1 is assumed to be a function of the observations
W 1, W 2, . . . , W n−1, but not W n. Thus our stochastic gradient ∇xF(xn−1, W n)
depends on our previous solution xn−1 and our most recent observation W n. To
illustrate, consider the simple newsvendor problem with the objective that
F(x, W) = p min{x, W} −cx.
In this problem we order a quantity x, and then observe a random demand W.
We earn a revenue given by p min{x, W} (we cannot sell more than we bought,
or more than the demand), but we had to pay for our order, producing a negative
cost cx. Let ∇F(x, W(ω)) be the sample gradient, taken when W = W(ω). In our
example, clearly
∂F(x, W(ω))
∂x
=

p −c
if x < W,
−c
if x > W.
(7.9)
We call ∇F(x, ω) a stochastic gradient because it is a gradient that depends on
a random variable. xn−1 is the estimate of x computed from the previous iteration
(using the sample realization ωn−1), while ωn is the sample realization in iteration
n (the indexing tells us that xn−1 was computed without ωn). When the function
is deterministic, we would choose the stepsize by solving the one-dimensional
optimization problem
min
α F

xn−1 −αn−1∇F(xn−1, ωn)

.
(7.10)
Now we face the problem of ﬁnding the stepsize αn−1. Unlike our deterministic
algorithm, we cannot consider solving a one-dimensional search to ﬁnd the best
stepsize. Part of the problem is that a stochastic gradient can even point away from
the optimal solution such that any positive stepsize actually makes the solution
worse. For example, the right order quantity might be 15. However, even if we
order x = 20, it is possible that the demand is greater 20 on a particular day.
Although our optimal solution is less than our current estimate, the algorithm tells
us to increase x.
Remark
Many authors will write equation (7.6) in the form
xn+1 = xn −αn∇F(xn, ωn).
(7.11)

gradient search
255
With this style, we would say that xn+1 is the estimate of x to be used in iteration
n + 1 (although it was computed with the information from iteration n). We use
the form in (7.6) because we will later allow the stepsizes to depend on the data,
and the indexing tells us the information content of the stepsize. For theoretical
reasons it is important that the stepsize be computed using information up through
n −1, hence our use of αn−1. We index xn on the left-hand side of (7.6) using n
because the right-hand side has information from iteration n. It is often the case
that time t is also our iteration counter, and so it helps to be consistent with our
time indexing notation.
There are many applications where the units of the gradient, and the units of
the decision variable, are different. This happens with our newsvendor example,
where the gradient is in units of dollars, while the decision variable x is in units
of newspapers. This is a signiﬁcant problem that causes headaches in practice.
Returning to our original problem of estimating the mean, we assume, when
running a stochastic gradient algorithm, that x0 is an initial guess, and that W(ω1)
is our ﬁrst observation. If our stepsize sequence uses an initial stepsize α0 = 1,
then
x1 = (1 −α0)x0 + α0W(ω1)
= W(ω1),
which means that we do not need the initial estimate for x0. Smaller initial stepsizes
would only make sense if we had access to a reliable initial guess, and in this case
the stepsize should reﬂect the conﬁdence in our original estimate (e.g., we might
be warm-starting an algorithm from a previous iteration).
We can evaluate our performance using a mean squared statistical measure. If
we have an initial estimate x0, then we would use
MSE = 1
n
n

m=1
(xm−1 −W(ωm))2.
(7.12)
However, it is often the case that the sequence of random variables W(ωn) is non-
stationary, which means that they are coming from a distribution that is changing
over time. (Recall that in Chapter 4 we would make random observations of the
value of being in a state, which we referred to as ˆvn
t , but these depended on a
value function approximation for future events which was changing over time.) In
the present case, estimating the mean squared error is similar to our problem of
estimating the mean of the random variable W, for which we should use a standard
stochastic gradient (smoothing) expression of the form
MSEn = (1 −βn−1)MSEn−1 + βn−1(xn−1 −W(ωn))2,
where βn−1 is another stepsize sequence (which could be the same as αn−1).

256
policy search
7.2.2
Derivatives of Simulations
In the previous section we illustrated a stochastic gradient algorithm in the context
of a fairly simple stochastic function. But imagine that we have a multiperiod
simulation, such as we might encounter when simulating ﬂows of jobs around
a manufacturing center. Perhaps we use a simple rule to govern how jobs are
assigned to machines once they have ﬁnished a particular step (e.g., being drilled
or painted). However, these rules have to reﬂect physical constraints such as the
size of buffers for holding jobs before a machine can start working on them. If the
buffer for a downstream machine is full, the rule might specify that a job be routed
to a different machine or to a special holding queue.
This is an example of a policy that is governed by static variables such as the
size of the buffer. We would let x be the vector of buffer sizes. It would be helpful,
then, if we could do more than simply run a simulation for a ﬁxed vector x. What
if we could compute the derivative with respect to each element of x, so that after
running a simulation, we obtain all the derivatives?
Computing these derivatives from simulations is the focus of an entire branch of
the simulation community. A class of algorithms called inﬁnitesimal perturbation
analysis was developed speciﬁcally for this purpose. It is beyond the scope of our
presentation to describe these methods in any detail, but it is important for readers
to be aware that the ﬁeld exists. A good introduction is the book Ho (1992),
which effectively invented the ﬁeld. A short tutorial is given by Fu (2008). For a
mathematically more mature treatment, see Glasserman (1991).
7.3
DIRECT POLICY SEARCH FOR FINITE ALTERNATIVES
While stochastic gradient algorithms are simple and elegant, there are many prob-
lems where derivatives are simply not available. When this is the case, we have to
depend on sophisticated hunt-and-peck algorithms that successively guess at values
of x to try while searching for the best value. A major part of the challenge, as
before, is that if we try a value xn, all we get is a noisy measurement F(xn, W n+1).
We are going to guide our search by creating a belief about the function
F(x) = EF(x, W). There are three major strategies we can use for approximating
a function:
Lookup table. We store an estimateF x for each discrete value of x.
Parametric model. We representF x using some sort of parametric model. The
parameter x may be discrete or continuous, scalar or vector-valued.
Nonparametric model. We representF x using nonparametric statistics.
We begin our presentation assuming that x can only take on values from a
set {x1, x2, . . . , xM}, where M is not too large, and using a lookup table model
for F(x). The problem of ﬁnding the best value of x from this set, using noisy
measurements, is known as the ranking and selection problem.

direct policy search for ﬁnite alternatives
257
We are going to use this basic but important problem class to establish some
useful concepts in learning. These are directly applicable in the context of policy
search (which is why we introduce the concepts here), but they also have a big
role in the context of what is famously known as the exploration versus exploita-
tion problem in dynamic programming, a topic that is covered in more depth in
Chapter 12.
7.3.1
The Ranking and Selection Problem
We assume our parameter choices consist of a ﬁnite set of alternatives chosen from
the set X = {1, 2, . . . , M}. Let µx be the true value of our function at x that is
unknown to us. At each iteration n = 0, 1, . . ., we have to choose the parameter
vector xn to measure, which may involve simulating a policy parameterized by xn.
We then make a noisy observation of µxn, which we can express as
W n+1
xn
= µxn + εn+1.
We emphasize that our indexing reﬂects the information content. Thus at iteration
n we have been allowed to see W 1
x0, W 2
x1, . . . , W n
xn−1. We choose xn before we
are allowed to see W n+1
xn . We use our observations to create estimates θ n
x .
The goal of the ranking and selection problem is to collect information
W 1
x0, W 2
x1, . . . , W N
xN−1 within a budget of N measurements to produce the best
possible design. The challenge is to design a measurement policy π that determines
x0, x1, . . . , xN−1 and gives us the best information about each set of parameters.
Note that we are using π to guide the policy of collecting information. At the
end we view the parameter vector x as determining a policy for controlling the
underlying dynamic program. Since the parameter vector x effectively determines
the policy that controls our dynamic program, we feel that we can temporarily
adopt the notation π to specify our policy for collecting information.
After we have made our measurements, we obtain the estimates θN
x
for each
choice x. We may write the problem of ﬁnding the best measurement policy as
max
π
max
x
θN
x .
(7.13)
Equation (7.13) evaluates our success based on the actual estimates θN
x , which we
can view as our best estimate of the value of x. A more ideal objective would be
to solve
max
π
µxπ ,
(7.14)
where xπ = arg max θN
x is the choice that we believe is best based on our current
estimates. Note that the objective (7.14) assumes that we know µx.
Below we present two views of our belief about µx, known as the frequentist
view and the Bayesian view. The Bayesian view shows us a way of computing
(7.14).

258
policy search
7.3.2
The Frequentist Approach
The frequentist view is arguably the approach that is most familiar to people with
an introductory course in statistics. Suppose that we are trying to estimate the mean
µ of a random variable W, which might be the performance of a device or policy.
Let W n be the nth sample observation. Also let θn be our estimate of µ, and ˆσ 2,n
be our estimate of the variance of W. We know from elementary statistics that we
can write θn and ˆσ 2,n using
θn = 1
n
n

i=1
W i,
(7.15)
ˆσ 2,n =
1
n −1
n

i=1
(W i −θn)2.
(7.16)
Since the measurements W i are random, the estimate θn is also a random variable.
ˆσ 2,n is our estimate, after n observations, of the variance of the random variable
W. The variance of the estimator θn is given by
σ 2,n = 1
n ˆσ 2,n.
We can write these expressions recursively using
θn =

1 −1
n

θn−1 + 1
nW n,
(7.17)
ˆσ 2,n =



1
n(W n −θn−1)2,
n = 2,
n −2
n −1 ˆσ 2,n−1 + 1
n(W n −θ n−1)2,
n > 2.
(7.18)
When we are collecting information about individual measurements, we would let
Nn
x be the number of times we have measured alternative x after n measurements.
The updating of θ n
x would then be given by
θn+1
xn
=

1 −
1
Nn
xn

θn
xn +
1
Nn
xn
W n+1
xn .
The updating of ˆσ 2,n
x
would be modiﬁed similarly.
The process of collecting information about µx can be viewed as a dynamic
program, since we are making decisions over time, just as we have been doing
with the other dynamic programs in this book. However, there is one important
difference. In most of our dynamic programming problems, our decisions modify
some sort of physical state (the position of a robot, the positions on a game board,
the allocation of resources). In the ranking and selection problem we do not have a
physical state, but we do have a state of knowledge (often referred to as the belief
state), which we represent using Kn. The state of knowledge captures what we

direct policy search for ﬁnite alternatives
259
know about the parameters µx after n measurements. If we use a frequentist view,
the state of knowledge is given by
Kn
f req =

θn, ˆσ 2,n, n

.
Typically we also associate a probability distribution around the estimate θn. If the
errors in our observations W n are normally distributed, then it is easy to verify
that θn will be normally distributed. However, by the central limit theorem, θn will
be closely approximated by a normal distribution even if the observations are not
normally distributed.
If we are using the frequentist view, it is most natural to evaluate the success
of our sequence of observations using the objective function in (7.13).
7.3.3
The Bayesian View
The Bayesian view is somewhat more subtle. When we adopt a Bayesian perspec-
tive, we start by assuming that we have a state of belief (known as the prior) about
µx before we have made a single measurement. For our presentation we are going
to always assume that our belief about µx is normally distributed, with mean µx
and (we assume) a known variance σ 2
x . To simplify our notation, we are going to
introduce the notion of the precision of our belief, represented by βx. The precision
is deﬁned simply as
βx = 1
σ 2x
.
It is important to recognize that in the Bayesian view, µx is viewed as a ran-
dom variable described by our prior distribution. Before we have collected any
information, we would say that µx ∝N(θ0
x, β0
x), which means that µx is normally
distributed with initial mean θ0
x and initial precision β0
x. Thus (θ0
x , β0
x) is our prior
distribution of belief about µx before we have started taking measurements. After
n measurements, we would say that (θn
x , βn
x ) is our prior distribution that we use to
choose xn, while N(θn+1
x
, βn+1
x
) would be the posterior distribution. Throughout
our presentation, µx represents the true value of x, while θn
x represents our best
estimate of the true value of µx.
Assume that we have chosen to measure xn, allowing us to observe W n+1
xn . We
assume that the measurement has a known precision βW
xn (the reciprocal of the
variance of W). If the measurements are also normally distributed, it is possible
to show (although this takes a little development) that the posterior distribution of
belief about µx after n + 1 measurements is normally distributed with mean and
precision given by
θn+1
x
=



βn
x θn
x + βW
x W n+1
x
βnx + βW
x
if xn = x,
θn
x
otherwise;
(7.19)
βn+1
x
=

βn
x + βW
x
if xn = x,
βn
x
otherwise.
(7.20)

260
policy search
As with the frequentist view, the problem of determining what measurements to
observe can be viewed as a dynamic program with the knowledge state (or belief
state) given by
Kn = (θn
x , βn
x)M
x=1.
With the Bayesian view, we can see a way of computing the objective function
in equation (7.14). This equation, however, does not recognize that µx is viewed
as a random variable. For this reason we revise the objective to
max
π
F π = EµEWµxπ .
(7.21)
Here EW is the expectation over all the potential outcomes of the measurements
W, given a particular truth µ. Then Eµ is the expectation over all potential truths
µ given the prior (θ0, β0).
A good way to envision the expectations in F π is to think about approximating
it using Monte Carlo simulation. Imagine an experiment where, for each trial, you
randomly sample a truth µ(ω) from the prior N(θ0, β0) (here µ(ω) represents a
sample realization of what the truth might be). Then follow some policy π that
speciﬁes which alternatives you should measure (more on this below). When you
want to measure alternative x, you draw a random realization
W n
x = µx(ω) + ϵn(ω),
where ϵn(ω) is a random perturbation around our assumed truth µ(ω). If we follow
policy π and sample path ω (which determines both the truth µ(ω) and the sample
observations W(ω)), this will produce estimates θπ,N(ω) and a decision xπ(ω) =
arg maxx θπ,N(ω). This in turn allows us to compute a sample realization of F π(ω)
using
F π(ω) = µ(ω)xπ (ω).
Notice how we use our method of sampling an assumed truth to evaluate how well
our measurement policy has identiﬁed the best set of parameters.
If we do K trials (i.e., k different values of ω), then we can approximate F π
using
F π ≈1
K
K

k=1
F π(ωk).
A common strategy for evaluating measurement policies is to estimate the expected
opportunity cost, EOCπ. For a sample realization ω this is given by
EOCπ(ω) = µ(ω)x∗−µ(ω)xπ(ω),
where x∗= arg maxx µ(ω)x is the true best alternative. We then average these
using
F π ≈1
K
K

k=1
EOCπ(ω).
The challenge now is to ﬁnd good measurement policies.

direct policy search for ﬁnite alternatives
261
7.3.4
Bayesian Updating with Correlated Beliefs
There are problems where our belief about µx is correlated with our belief about
µx′. For example, x might be a price, speed, or temperature, representing a dis-
cretization of a continuous variable. Imagine that we measure xn and obtain an
observation W n+1
x
that suggests that we should increase our belief about µxn. Per-
haps this single measurement suggests that we should increase our belief about µx′
for values x′ that are close to xn. We can incorporate these covariances into our
updating strategies, a powerful feature that allows us to update beliefs where the
number of measurements is much smaller than the number of alternatives we have
to evaluate.
Let
Covn(µx, µx′) = covariance between µx and µx′ after n measurements
have been made,
n = covariance matrix, with element n
xx′ = Covn(µx, µx′).
Just as we deﬁned the precision βn
x to be the reciprocal of the variance, we are
going to deﬁne the precision matrix Bn to be
Bn = (n)−1.
Let ex be a column vector of 0′s with a 1 for element x, and as before we let W n+1
be the (scalar) observation when we decide to choose x. If we choose to measure
xn, we obtain a column vector of observations given by W n+1exn. Keeping in mind
that θn is a column vector of our beliefs about the expectation of µ, the Bayesian
equation for updating this vector in the presence of correlated beliefs is given by
θn+1 = (Bn+1)−1
Bnθn + βWW n+1
xn exn
,
(7.22)
where Bn+1 is given by
Bn+1 = (Bn + βWexn(exn)T ).
(7.23)
Note that ex(ex)T is a matrix of zeroes with a one in row x, column x. βW is a
scalar giving the precision of our measurement W.
7.3.5
Some Learning Policies
We have now set up the foundation for representing our beliefs given a set of
observations, and for evaluating different policies for taking measurements. We
next face the challenge of deciding how to design policies that determine what
we should measure next. Below we list some of the most popular heuristics for
determining how to collect information.
Pure Exploitation
Pure exploitation refers to making the decision that appears to be the best given
what we know. In this case we would use
xn = arg max
x∈X
θn
x .

262
policy search
Pure Exploration
Pure exploration means simply choosing values of x to try at random.
Epsilon-Greedy Exploration
We can use a mixed strategy where we explore with probability ϵ (known as the
exploration rate) and we exploit with probability 1 −ϵ. This encourages a policy
where we spend more time evaluating what appears to be the best choice.
Boltzmann Exploration
Boltzmann exploration, also known as Gibbs’s sampling or soft-max, uses a more
nuanced strategy for testing alternatives. Epsilon-greedy will focus a certain amount
of attention on the best choice, but the second best is treated no differently than
the worst. With Boltzmann exploration we sample measurement x with probability
pn
x given by
pn
x =
exp (ρθ n
x )

x′∈X exp (ρθn
x′).
This strategy samples the choices that appear to be better more often, but continues
to sample all choices, with a probability that declines with our belief about their
attractiveness.
Interval Estimation
Boltzmann exploration improved on the previous heuristics by paying attention
to our belief about each alternative. Interval estimation takes this a step further,
by also taking into account our conﬁdence in each estimate. Interval estimation
proceeds by computing an index νn
x for each alternative x given by
νn
x = θn
x + zασ n
x ,
where σ n
x is our estimate of the standard deviation of θn
x . When we use an interval
estimation policy, we choose the measurement xn with the highest value of νn
x.
Interval estimation bases the decision to try an alternative by striking a balance
between how well we think an alternative is likely to perform (θn
x ), and our level
of uncertainty about this estimate (σ n
x ). It is unlikely to try an alternative that we
think is genuinely poor, but is likely to try alternatives that do not appear to be the
best, but about which we have considerable uncertainty.
7.4
THE KNOWLEDGE GRADIENT ALGORITHM
FOR DISCRETE ALTERNATIVES
The knowledge gradient is a simple policy that chooses the alternative that produces
the greatest value from a single measurement. This can be viewed as a myopic or
greedy policy, but there is a growing body of theoretical and empirical evidence that
shows that the policy works well, across a wide range of problems. Furthermore it
is relatively easy to implement.

the knowledge gradient algorithm
263
7.4.1
The Basic Idea
The idea is simple to explain. If we were to stop now (at iteration n), we would
make our choice using
xn = max
x′∈X θn
x′.
The value of being in knowledge state Sn = (θn, βn) is then given by
V n(Sn) = θn
xn.
If we choose to next measure xn = x allowing us to observe W n+1
x
, this would
give us knowledge state Sn+1(x), which is a random variable since we do not know
W n+1
x
. Sn+1(x) = (θn+1, βn+1) is computed using equations (12.4) and (12.5). The
value of being in state Sn+1(x) is given by
V n+1(Sn+1(x)) = max
x′∈X θn+1
x′
.
We would like to choose x at iteration n that maximizes the expected value of
V n+1(Sn+1(x)), given by
νKG,n
x
= E

V n+1(Sn+1(x)) −V n(Sn)|Sn
.
(7.24)
The index νKG,n
x
is the marginal value of information due to the measurement x,
which can be viewed as the gradient with respect to x. The knowledge gradient
policy chooses to measure the alternative x that maximizes νKG,n
x
. Alternatively,
we may write the knowledge gradient policy as
XKG,n = arg max
x∈X
E

V n+1(Sn+1(x)) −V n(Sn)|Sn
.
(7.25)
7.4.2
Computation
If we have ﬁnite alternatives with normally distributed and independent beliefs, the
knowledge gradient is especially easy to compute. We ﬁrst present the updating
formulas in terms of variances (rather than the precision), given by
σ 2,n
x
=

(σ 2,n−1
x
)−1 + (σ 2
W)−1−1
=
(σ 2,n−1
x
)
1 + σ 2,n−1
x
/σ 2
W
.
(7.26)
To illustrate our problem, let σ 2,n
x
= 16 and σW = 64. Then
σ 2,n+1
x
=
 1
16 + 1
64
−1
= (0.078125)−1
= 12.8.

264
policy search
Let Varn(·) be the variance of a random variable given what we know about the ﬁrst
n measurements. For example, Varnθn
x = 0 since, given the ﬁrst n measurements,
θn
x is deterministic. Next we need the variance in the change in our estimate of θn
x
as a result of measuring x:
˜σ 2,n
x
= Varn[θn+1
x
−θn
x ].
Keep in mind that θn+1
x
is random because, after n measurements, we have not yet
observed W n+1. We can write ˜σ 2,n
x
in the following ways:
˜σ 2,n
x
= σ 2,n
x
−σ 2,n+1
x
(7.27)
=
(σ 2,n
x
)
1 + σ 2
W/σ 2,n
x
(7.28)
= (βn
x )−1 −(βn
x + βW)−1.
(7.29)
For our example this would produce
˜σ 2,n
x
= 16 −12.8
= 3.2.
We then compute ζ n
x , which is given by
ζ n
x = −
....
θn
x −maxx′̸=x θn
x′
˜σ nx
.... .
(7.30)
ζ n
x is the number of standard deviations from the current estimate of the value of
decision x, given by θn
x , and the best alternative other than decision x. It is impor-
tant to recognize that the value of information depends on its ability to change
a decision. ζ n
x captures the distance between the choice we are thinking of mea-
suring and the next best alternative. For our example, assume that θn
x = 24 and
maxx′̸=x θn
x′ = 24.5. In this case
ζ n
x = −
....
24 −24.5
√
3.2
....
= −| −0.27951|
= −0.27951.
We next deﬁne
f (ζ) = ζ(ζ) + φ(ζ),
(7.31)
where (ζ) and φ(ζ) are, respectively, the cumulative standard normal distribution
and the standard normal density. That is,
φ(ζ) =
1
√
2π
e−ζ 2

the knowledge gradient algorithm
265
and
(ζ) =
6 ζ
−∞
φ(x)dx.
φ(ζ) is, of course, quite easy to compute, while very accurate approximations of
(ζ) are available in most computer libraries. For our numerical example
f (ζ) = −(0.27951)(0.38993) + 0.38366
= 0.27467.
Finally, the knowledge gradient is given by
νKG,n
x
= ˜σ n
x f (ζ n
x ).
(7.32)
=
	
(3.2)(0.27467)
= 0.491345.
(7.33)
Table 7.1 illustrates the calculations for a problem with ﬁve choices. The priors
θ n are shown in the second column, followed by the prior precision. The precision
of the measurement is βW = 1.
7.4.3
Knowledge Gradient for Correlated Beliefs
A particularly important feature of the knowledge gradient is that it can be adapted
to handle the important problem of correlated beliefs. Correlated beliefs arise when
we are maximizing a continuous surface (nearby points will be correlated) or choos-
ing subsets (e.g., the location of a set of facilities) that produce correlations when
subsets share common elements. If we are trying to estimate a continuous function,
we might assume that the covariance matrix satisﬁes
Cov(x, x′) ∝e−ρ∥x−x′∥,
where ρ captures the relationship between neighboring points. If x is a vector of
0′ s and 1′ s indicating elements in a subset, the covariance might be proportional
to the number of 1’s that are in common between two choices.
Table 7.1
Calculations illustrating the knowledge gradient index
Choice
µn
βn
βn+1
˜σ
maxx′x′
ζ
f (ζ)
νKG
x
1
20.0
0.0625
1.0625
3.8806
28
−2.0616
0.0072
0.0279
2
22.0
0.1111
1.1111
2.8460
28
−2.1082
0.0063
0.0180
3
24.0
0.0400
1.0400
4.9029
28
−0.8158
0.1169
0.5731
4
26.0
0.1111
1.1111
2.8460
28
−0.7027
0.1422
0.4048
5
28.0
0.0625
1.0625
3.8806
26
−0.5154
0.1931
0.7493

266
policy search
The covariance function (or matrix) is of central importance to the successful
use of the knowledge gradient. Not surprisingly, the procedure works the best when
the problem has structure that allows us to determine the covariance function in
advance. However, it is possible to estimate the covariance function from data as
the algorithm progresses.
There is a more compact way of updating our estimate of θn in the presence of
correlated beliefs. Let λW = σ 2
W = 1/βW. Let n+1(x) be the updated covariance
matrix given that we have chosen to measure alternative x, and let ˜n(x) be the
change in the covariance matrix due to evaluating x, which is given by
˜n = n −n+1,
(7.34)
˜(x) = nex(ex)T n
	
nxx + λW .
(7.35)
Now deﬁne the vector ˜σ n(x), which gives the standard deviation of the change in
our belief due to measuring x and is given by
˜σ n(x) =
nex
	
nxx + λW .
(7.36)
Let ˜σi(, x) be the component (ei)T ˜σ(x) of the vector ˜σ(x), and let Varn(·) be the
variance given what we know after n measurements. We note that if we measure
alternative xn, then
Varn 
W n+1 −θn
= Varn 
θxn + εn+1
= n
xnxn + λW.
(7.37)
Next deﬁne the random variable
Zn+1 =
W n+1 −θn
7
Varn 
W n+1 −θn.
We can now rewrite (12.4) for updating our beliefs about the mean as
θn+1 = θn + ˜σ(xn)Zn+1.
(7.38)
The knowledge gradient policy for correlated beliefs is computed using
XKG(s) = arg max
x
E
2
max
i
θn+1
i
| Sn = s, xn = x
3
(7.39)
= arg max
x
E
2
max
i
θn
i + ˜σi(xn)Zn+1 | Sn, xn = x
3
,

the knowledge gradient algorithm
267
where Z is a scalar, standard normal random variable. The problem with this
expression is that the expectation is harder to compute, but a simple algorithm can
be used to compute the expectation exactly. We start by deﬁning
h(θn, ˜σ(x)) = E
2
max
i
θn
i + ˜σi(xn)Zn+1 | Sn, xn = x
3
.
(7.40)
Substituting (7.40) into (7.39) gives us
XKG(s) = arg max
x
h(θn, ˜σ(x)).
(7.41)
Let h(a, b) = E maxi ai + biZ, where ai = θn
i , bi = ˜σi(xn), and Z is our stan-
dard normal deviate. Both a and b are M-dimensional vectors. Sort the elements
bi so that b1 ≤b2 ≤. . . so that we get a sequence of lines with increasing slopes,
as depicted in Figure 7.1. There are ranges for z over which a particular line may
dominate the other lines, and some lines may be dominated all the time (e.g.,
alternative 3).
We need to identify and eliminate the dominated alternatives. To do this, we start
by ﬁnding the points where the lines intersect. The lines ai + biz and ai+1 + bi+1z
intersect at
z = ci = ai −ai+1
bi+1 −bi
.
For the moment we are going to assume that bi+1 > bi. If ci−1 < ci < ci+1, then
we can ﬁnd a range for z over which a particular choice dominates, as depicted
in Figure 7.1. A line is dominated when ci+1 < ci, at which point the line corre-
sponding to i + 1 is dropped from the set. Once the sequence ci has been found,
we can compute (7.39) using
h(a, b) =
M

i=1
(bi+1 −bi)f (−|ci|),
where as before, f (z) = z(z) + φ(z). Of course, the summation has to be adjusted
to skip any choices i that were found to be dominated.
a1 + b1z
a2 + b2z
a3 + b3z
a4 + b4z
c1
c3 c3'
c2
z
h(z)
Figure 7.1
Regions of z over which different choices dominate. Choice 3 is always dominated.

268
policy search
It is important to recognize that there is more to incorporating correlated beliefs
than simply using the covariances when we update our beliefs after a measure-
ment. With this procedure, we anticipate the updating before we even make a
measurement.
The ability to handle correlated beliefs in the choice of what measurement to
make is an important feature that has been overlooked in other procedures. It makes
it possible to make sensible choices when our measurement budget is much smaller
than the number of potential choices we have to evaluate. There are, of course,
computational implications. It is relatively easy to handle dozens or hundreds of
measurement alternatives, but as a result of the matrix calculations, it becomes
expensive to handle problems where the number of potential choices is in the
thousands. If this is the case, then it is likely the problem has special structure. For
example, we might be discretizing a p-dimensional parameter surface. If this is the
case, then it may make sense to consider the adaptation of the knowledge gradient
for problems where the belief structure can be represented using a parametric model.
7.4.4
The Knowledge Gradient for Parametric Belief Models
There are many applications (especially in approximate dynamic programming)
where we are trying to learn a function that we are approximating using linear
regression (the simplest form of parametric model). We can do this fairly eas-
ily from our calculation of the knowledge gradient for correlated beliefs using a
lookup table belief structure. A limitation of this strategy is that it works with
the covariance matrix, which increases with the square of the number of points to
measure. Using linear regression, we reduce this problem dealing with a matrix
that is |F| × |F|, where F is the number of features in our linear model.
We start by recognizing that the knowledge gradient requires computing the
function h(a, b(j)), where a is a vector with element ai = θn
i giving the estimate
of the value of the ith alternative, and b(j) is a vector with element bi(j) =
˜σ n
i (j), which is the conditional variance of the change in θn+1
i
from measuring
alternative j. The function h(a, b) is given by
h(a, b(j)) =
M−1

i=1
(bi+1(j) −bi(j))f (−|ci(j)|)
(7.42)
where
ci(j) =
ai −ai+1
bi+1(j) −bi(j).
The major computational challenges arises when computing ˜σ n(j), which is
a vector giving the change in the variance of each alternative i if we choose
to measure alternative j. From our presentation of the knowledge gradient for
correlated beliefs, we found that
˜n(j) = nej(ej)T n
7
n
jj + λW
ϵ
,

the knowledge gradient algorithm
269
which gives us the change in the covariance matrix from measuring an alternative
j. The matrix ˜n(j) is M × M, which is quite large, but we only need the jth
row. We compute the standard deviation of the elements of the j th row using
˜σ n(j) =
nej
7
n
jj + σ 2ϵ
.
(7.43)
Now is where we use the structure of our linear regression model. If there are a
large number of choices, the matrix ˜n will be too expensive to compute. However,
it is possible to work with the covariance matrix θ of the regression vector θ,
which is dimensioned by the number of parameters in our regression function. To
compute θ, we need to set up some variables. First let
xn =


xn
1
xn
2...
xn
K


be a vector of K independent variables corresponding to the nth observation. Next
create the matrix Xn that consists of all of our measurements:
Xn =


x1
1
x2
1...
xn
1
x1
2
x2
2...
xn
2
. . .
x1
K
x2
K...
xn
K


.
We can use Xn to compute our regression vector by computing
θn = [(Xn)T Xn]−1(Xn)T Y n.
(7.44)
We do not actually use this equation directly, but we defer to Section 9.3.1 for a
more in-depth discussion of linear regression models, and methods for recursively
computing θn that avoid the use of matrix inversions. We have yet a different
purpose for the moment. We can use Xn to compute a compact form for the
covariance matrix θ,n of θ. With a bit of algebra, it is possible to show that after
n observations
θ,n = [(Xn)T Xn]−1(Xn)T Xn[(Xn)T Xn]−1σ 2
ϵ
= [(Xn)T Xn]−1σ 2
ϵ ,
where σ 2
ϵ is the variance of our measurements. Of course, this seems to imply that
we still have to invert (Xn)T Xn. In Chapter 9 (Section 9.3.1) we provide equations
for computing this update recursively.

270
policy search
Knowing that we can compute θ,n in an efﬁcient way, we can quickly compute
˜σ n. Note that we do not need the entire matrix. We can show that
n = Xθ,nXT ,
Let Xj be the jth row of X. Then
˜σ 2,n(j) = Xjθ,nXT .
We still have to multiply the K × K-dimensional matrix θ,n times the K × M-
dimensional matrix XT , after which we have to compute equation (7.42) to ﬁnd the
knowledge gradient for each alternative. Even for problems with tens of thousands
of alternatives, this can be executed in a few seconds. Now that we have an efﬁcient
way of computing ˜σ n(j), we can apply the knowledge gradient for correlated beliefs
described in Section 7.4.3.
7.5
SIMULATION OPTIMIZATION
A subcommunity within the larger stochastic search community goes by the name
simulation optimization. This community also works on problems that can be
described in the form of minx EF(x, W), but the context typically arises when
x represents the design of a physical system, which is then evaluated (noisily)
using discrete-event simulation. The number of potential designs X is typically in
the range of 5 to perhaps 100. We can evaluate a design x more accurately by
increasing the run length nx of the simulation, where nx might be the number
of time periods, the CPU time, or the number of discrete events (e.g., customer
arrivals). We assume that we have a global budget N, and we need to ﬁnd nx for
each x so that

x∈X
nx = N.
For our purposes there is no difference between a potential design of a physical
system and a policy. Searching for the best design and searching for the best policy
is, algorithmically speaking, identical as long as the set of policies is not too large.
We can tackle this problem using the strategies described above (e.g., the knowl-
edge gradient) if we break up the problem into a series of short simulations (e.g., 1
time step or 1 unit of CPU time). Then at each iteration we have to decide which
design x to measure, contributing to our estimate θn
x for design x. The problem
with this strategy is that it ignores the startup time for a simulation. It is much
easier to set a run length nx for each design x, and then run the entire simulation
to obtain an estimate of θx.
The simulation optimization problem is traditionally formulated in a frequentist
framework, reﬂecting the lack of prior information about the alternatives. A stan-
dard strategy is to run the experiments in two stages. In the ﬁrst stage a sample

simulation optimization
271
n0 is collected for each design. The information from this ﬁrst stage is used to
develop an estimate of the value of each design. We might learn, for example, that
certain designs seem to lack any promise at all, while other designs may seem
more interesting. Rather than spreading our budget across all the designs, we can
use this information to focus our computing budget across the designs that offer
the greatest potential.
From the information at the initial stage, the challenge is to ﬁnd an allocation nx
that optimizes some objective subject to the budget constraint N. There are several
objectives that we might consider:
• Expected opportunity cost. Above we described the stochastic search prob-
lem in the form of solving minx EF(x, W). If x∗is the optimal solution,
consider the equivalent optimization problem minx E

F(x, W) −F(x∗, W)

,
which gives us the cost over the optimal solution. This is called the expected
opportunity cost.
• Subset selection. Ultimately our goal is to pick the best design. Imagine that
we are willing to choose a subset of designs S, and we would like to ensure
that P (x∗∈S) ≥1 −α, where 1/|X| < 1 −α < 1. Of course, it would be
ideal if |S| = 1 or, failing this, as small as possible. Let θn
x be our estimate of
the value of x after n measurements, and assume that all measurements have
a constant and known variance σ. We include x in the subset if
θn
x ≥max
x′̸=x θn
x′ −hσ

2
n.
The parameter h is the 1 −α quantile of the random variable maxi Zn
i , where
Zn
i is given by
Zn
i = (θn
i −θn
x ) −(µi −µx)
σ√2/n
.
• Indifference zone selection. The goal is to ﬁnd with probability 1 −α the
system x such that µx ≥µx∗−δ. In other words, we are indifferent about
selecting systems that are within δ of the best. The procedure samples each
alternative n times when
n = ⌜2h2σ 2⌝
δ2
,
where ⌜z⌝means to round up, and h is chosen as it was for the subset
selection objective. Use these samples to compute θ n
x , and then choose the
best x. Note that the goal here is to ensure that we meet our goal with a
speciﬁed probability, rather than to work within a speciﬁc budget constraint.
The expected opportunity cost objective focuses on estimating the value of each
alternative, while the other two options focus on ﬁnding the best or near-best,
which is a form of ordinal optimization.

272
policy search
7.5.1
An Indifference Zone Algorithm
There are a number of algorithms that have been suggested to search for the best
design using the indifference zone criterion, which is one of the most popular in
the simulation optimization community. The algorithm in Figure 7.2 summarizes a
method that successively reduces a set of candidates at each iteration, focusing the
evaluation effort on a smaller and smaller set of alternatives. The method (under
some assumptions) uses a user-speciﬁed indifference zone of δ. Of course, as δ is
decreased, the computational requirements increase.
Step 0. Initialization
Step 0a. Select the probability of correct selection 1 −α, indifference zone parameter δ
and initial sample size n0 ≥2. Set r = n0.
Step 0b. Compute
η = 1
2
/ 2α
k −1
−2/(n0−1)
−1
0
.
Step 0c. Set h2 = 2η(n0 −1).
Step 0d. Set X 0 = X as the set of systems in contention.
Step 0e. Obtain samples W m
x , m = 1, . . . , n0 of each x ∈X 0, and let θ0
x be the resulting
sample means for each alternative computed using
θ0
x = 1
n0
n0

m=1
W m
x .
Compute the sample variances for each pair using
ˆσ 2
xx′ =
1
n0 −1
n0

m=1

W m
x −W m
x′ −

θ0
x −θ0
x′
2 .
Step 0f. Set n = 1.
Step 1. Compute
Wxx′(r) = max

0,
δ
2r

h2 ˆσ 2
xx′
δ2
−r

.
Step 2. Reﬁne the eligible set using
X n =
'
x : x ∈X n−1 and θ n
x ≥θn
x′ −Wxx′(r), x′ ̸= x
(
.
Step 3. If |X n| = 1, stop and select the best element in X n. Otherwise, perform an additional
sample W n+1
x
of each x ∈X n, set r = r + 1, and return to step 1.
Figure 7.2
Policy search algorithm using the indifference zone criterion, due to Kim and Nelson
(2001).

simulation optimization
273
7.5.2
Optimal Computing Budget Allocation
The value of the indifference zone strategy is that it focuses on achieving a speciﬁc
level of solution quality while being constrained by a speciﬁc budget. However,
it is often the case that we are trying to do the best we can within a speciﬁc
computing budget. For this purpose a line of research has evolved under the name
optimal computing budget allocation, or OCBA.
Figure 7.3 illustrates a typical version of an OCBA algorithm. The algorithm
proceeds by taking an initial sample N0
x = n0 of each alternative x ∈X, which
means we use B0 = Mn0 measurements from our budget B. Letting M = |X|, we
divide the remaining budget of measurements B −B0 into equal increments of size
 so that we do N = (B −Mn0) iterations.
Step 0. Initialization.
Step 0a. Given a computing budget B, let n0 be the initial sample size for each of the
M = |X | alternatives. Divide the remaining budget B −Mn0 into increments so that
N = (B −Mn0)/δ is an integer.
Step 0b. Obtain samples W m
x , m = 1, . . . , n0 samples of each x ∈X .
Step 0c. Initialize N1
x = n0 for all x ∈X .
Step 0d. Initialize n = 1.
Step 1. Compute
θn
x =
1
Nnx
Nnx

m=1
W m
x .
Compute the sample variances for each alternative x using
ˆσ 2,n
x
=
1
Nnx −1
Nnx

m=1

W m
x −θn
x
2 .
Step 2. Let xn = arg maxx∈X θn
x .
Step 3. Increase the computing budget by  and calculate the new allocation Nn+1
1
,
. . . , Nn+1
M
so that
Nn+1
x
Nn+1
x′
= ˆσ 2,n
x
/(θn
xn −θn
x )2
ˆσ 2,n
x′ /(θn
xn −θn
x′)2 ,
x ̸= x′ ̸= xn,
Nn+1
xn
= ˆσ n
xn
8
9
9
9
:
M

i=1,i̸=xn

Nn+1
xn
ˆσ n
i
2
.
Step 4. Perform max

Nn+1
x
−Nn
x , 0

additional simulations for each alternative x.
Step 5. Set n = n + 1. If 
x∈X Nn
x < B, go to step 1.
Step 6. Return xn = argmaxx∈Xθn
x .
Figure 7.3
Optimal computing budget allocation procedure.

274
policy search
After n iterations, we assume that we have measured alternative x Nn
x times,
and let W m
x be the mth observation of x, for m = 1, . . . , Nn
x . The updated estimate
of the value of each alternative x is given by
θn
x = 1
Nnx
Nnx

m=1
W m
x .
Let xn = arg max θn
x be the current best option.
After using Mn0 observations from our budget, at each iteration we increase our
allowed budget by Bn = Bn−1 +  until we reach BN = B. After each increment
the allocation Nn
x , x ∈X is recomputed using
Nn+1
x
Nn+1
x′
= ˆσ 2,n
x
/(θn
xn −θn
x )2
ˆσ 2,n
x′ /(θn
xn −θn
x′)2 ,
x ̸= x′ ̸= xn,
(7.45)
Nn+1
xn
= ˆσ n
xn
8
9
9
9
:
M

i=1,i̸=xn

Nn+1
x
ˆσ n
i
2
.
(7.46)
We use equations (7.45)–(7.46) to produce an allocation Nn
x such that 
x Nn
x =
Bn. Note that after increasing the budget, it is not guaranteed that Nn
x ≥Nn−1
x
for
some x. So, if this is the case, we do not measure these alternatives at all in the next
iteration. We can solve these equations by writing each Nn
x in terms of some ﬁxed
alternative (other than xn), such as Nn
1 (assuming xn ̸= 1). After writing N n
x as a
function of N n
1 for all x, we determine N n
1 so that  Nn
x ≈Bn (within rounding).
The complete algorithm is summarized in Figure 7.3.
7.6
WHY DOES IT WORK?**
Stochastic approximation methods have a rich history starting with the seminal
paper Robbins and Monro (1951) and followed by Blum (1954b) and Dvoretzky
(1956). The serious reader should see Kushner and Yin (1997) for a modern treat-
ment of the subject. Wasan (1969) is also a useful reference for fundamental results
on stochastic convergence theory. A separate line of investigation was undertaken
by researchers in eastern European community focusing on constrained stochas-
tic optimization problems (Gaivoronski, 1988; Ermoliev, 1988; Ruszczynski 1980,
1987). This work is critical to our fundamental understanding of Monte Carlo based
stochastic learning methods.
The theory behind these proofs is fairly deep and requires some mathematical
maturity. For pedagogical reasons we start in Section 7.6.1 with some probabilistic
preliminaries, after which Section 7.6.2 presents one of the original proofs that is
relatively more accessible and that provides the basis for the universal requirements
that stepsizes must satisfy for theoretical proofs. Section 7.6.3 provides a more
modern proof based on the theory of martingales.

why does it work?
275
7.6.1
Some Probabilistic Preliminaries
The goal in this section is to prove that stochastic algorithms work. But what does
this mean? The solution xn at iteration n is a random variable. Its value depends
on the sequence of sample realizations of the random variables over iterations 1 to
n. If ω = (ω1, ω2, . . . , ωn, . . .) represents the sample path that we are following,
we can ask what is happening to the limit limn→∞xn(ω). If the limit is x∗, does
x∗depend on the sample path ω?
In the proofs below we show that the algorithms converge almost surely. What
this means is that
lim
n→∞xn(ω) = x∗
for all ω ∈ that can occur with positive measure. This is the same as saying that
we reach x∗with probability 1. Here x∗is a deterministic quantity that does not
depend on the sample path. Because of the restriction p(ω) > 0, we accept that in
theory there could exist a sample outcome that can never occur and would produce
a path that converges to some other point. As a result we say that the convergence is
“almost sure,” which is universally abbreviated as “a.s.” Almost sure convergence
establishes the core theoretical property that the algorithm will eventually settle
in on a single point. This is an important property for an algorithm, but it says
nothing about the rate of convergence (an important issue in approximate dynamic
programming).
Let x ∈ℜn. At each iteration n, we sample some random variables to compute
the function (and its gradient). The sample realizations are denoted by ωn. We let
ω = (ω1, ω2, . . .) be a realization of all the random variables over all iterations.
Let  be the set of all possible realizations of ω, and let F be the σ-algebra on
 (i.e., the set of all possible events that can be deﬁned using ). We need the
concept of the history up through iteration n. Let
H n = a random variable giving the history of all random variables up through
iteration n.
A sample realization of H n would be
hn = H n(ω)
= (ω1, ω2, . . . , ωn).
We could then let n be the set of all outcomes of the history (that is, hn ∈H n)
and let Hn be the σ-algebra on n (which is the set of all events, including
their complements and unions, deﬁned using the outcomes in n). Although we
could do this, this is not the convention followed in the probability community.
So we deﬁne instead a sequence of σ-algebras F1, F2, . . . , Fn as the sequence of
σ-algebras on  that can be generated as we have access to the information through
the ﬁrst 1, 2, . . . , n iterations, respectively. What does this mean? Consider two
outcomes ω ̸= ω′ for which H n(ω) = H n(ω′). If this is the case, then any event in

276
policy search
Fn that includes ω must also include ω′. If we say that a function is Fn-measurable,
then this means that it must be deﬁned in terms of the events in Fn, which is in
turn equivalent to saying that we cannot be using any information from iterations
n + 1, n + 2, . . ..
We would say, then, that we have a standard probability space (, F, P), where
ω ∈ represents an elementary outcome, F is the σ-algebra on , and P is
a probability measure on (, F). Since our information is revealed iteration by
iteration, we would also then say that we have an increasing set of σ-algebras
F1 ⊆F2 ⊆. . . ⊆Fn (which is the same as saying that Fn is a ﬁltration).
7.6.2
An Older Proof
Enough with probabilistic preliminaries. Let F(x, ω) be a F-measurable function.
We wish to solve the unconstrained problem
max
x
E{F(x, ω)}
(7.47)
with x∗being the optimal solution. Let g(x, ω) be a stochastic ascent vector that
satisﬁes
g(x, ω)T ∇F(x, ω) ≥0.
(7.48)
For many problems, the most natural ascent vector is the gradient itself
g(x, ω) = ∇F(x, ω),
(7.49)
which clearly satisﬁes (7.48).
We assume that F(x) = E{F(x, ω)} is continuously differentiable and concave,
with bounded ﬁrst and second derivatives so that for ﬁnite M
−M ≤g(x, ω)T ∇2F(x)g(x, ω) ≤M.
(7.50)
For notational simplicity, we are letting F(x) be the expectation, while F(x, ω) is
a sample realization. A stochastic gradient algorithm (sometimes called a stochastic
approximation method) is given by
xn = xn−1 + αn−1g(xn−1, ωn).
(7.51)
We ﬁrst prove our result using the proof technique of Blum (1954b) that gen-
eralized the original stochastic approximation procedure proposed by Robbins and
Monro (1951) to multidimensional problems. This approach does not depend on
more advanced concepts such as martingales and, as a result, is accessible to a
broader audience. This proof helps the reader understand the basis for the conditions
∞
n=0 αn = ∞and ∞
n=0(αn)2 < ∞that are required of all stochastic approxima-
tion algorithms.

why does it work?
277
We make the following (standard) assumptions on stepsizes
αn ≥0,
n ≥0,
(7.52)
∞

n=0
αn = ∞,
(7.53)
∞

n=0
(αn)2 < ∞.
(7.54)
We want to show that under suitable assumptions, the sequence generated by (7.51)
converges to an optimal solution. That is, we want to show that
lim
n→∞xn = x∗
a.s.
(7.55)
We now use the mean value theorem, which says that for any continuously
differentiable function F(x), there exists a parameter 0 ≤η ≤1 that satisﬁes
F(x) = F(x0) + ∇F(x0 + η(x −x0))T (x −x0).
(7.56)
This is the ﬁrst-order Taylor series approximation. The second-order version takes
the form
F(x) = F(x 0) + ∇F(x0)T (x −x0) + 1
2(x −x0)∇2F(x0 + η(x −x0))(x −x0)
(7.57)
for some 0 ≤η ≤1. We use the second-order version. Replace x0 with xn−1, and
replace x with xn. Also we can simplify our notation by using
gn = g(xn−1, ωn).
(7.58)
This means that
x −x0 = xn −xn−1
= (xn−1 + αn−1gn) −xn−1
= αn−1gn.
From our stochastic gradient algorithm (7.51), we may write
F(xn, ωn) = F(xn−1 + αn−1gn, ωn)
= F(xn−1, ωn) + ∇F(xn−1, ωn)T (αn−1gn)
+ 1
2(αn−1gn)T ∇2F(xn−1 + γ (αn−1gn))(αn−1gn).
(7.59)

278
policy search
It is now time to use a standard mathematician’s trick. We sum both sides of (7.59)
to get
N

n=1
F(xn, ωn) =
N

n=1
F(xn−1, ωn) +
N

n=1
∇F(xn−1, ωn)T (αn−1gn)
+ 1
2
N

n=1
(αn−1gn)T ∇2F

xn−1 + x(αn−1gn)

(αn−1gn).
(7.60)
Note that the terms F(xn), n = 2, 3, . . . , N appear on both sides of (7.60). We
can cancel these. We then use our lower bound on the quadratic term (7.50) to
write
F(xN, ωN) ≥F(x0, ω1) +
N

n=1
∇F(xn−1, ωn)T (αn−1gn) + 1
2
N

n=1
(αn−1)2(−M).
(7.61)
We now want to take the limit of both sides of (7.61) as N →∞. In doing
so, we want to show that everything must be bounded. We know that F(xN)
is bounded (almost surely) because we assumed that the original function was
bounded. We next use the assumption 7.54 that the inﬁnite sum of the squares
of the stepsizes is also bounded to conclude that the rightmost term in (7.61)
is bounded. Finally, we use (7.48) to claim that all the terms in the remaining
summation (N
n=1 ∇F(xn−1, ωn)(αn−1gn)) are positive. That means that this term
is also bounded (from both above and below).
What do we get with all this boundedness? Well, if
∞

n=1
αn−1∇F(xn−1, ωn)T gn < ∞
a.s.
(7.62)
and (from (11.15))
∞

n=1
αn−1 = ∞.
(7.63)
We can conclude that
∞

n=1
∇F(xn−1, ωn)T gn < ∞.
(7.64)
Since all the terms in (7.64) are positive, they must go to zero. (Remember, every-
thing here is true almost surely; after a while, it gets a little boring to keep saying
almost surely every time. It is a little like reading Chinese fortune cookies and
adding the automatic phrase “under the sheets” at the end of every fortune.)
We are basically done except for some relatively difﬁcult (albeit important if you
are ever going to do your own proofs) technical points to really prove convergence.

why does it work?
279
At this point we would use technical conditions on the properties of our ascent
vector gn to argue that if ∇F(xn−1, ωn)T gn →0, then ∇F(xn−1, ωn) →0 (it is
okay if gn goes to zero as F(xn−1, ωn) goes to zero, but it cannot go to zero too
quickly).
This proof was ﬁrst proposed in the early 1950s by Robbins and Monro and
became the basis of a large area of investigation under the heading of stochastic
approximation methods. A separate community, growing out of the Soviet literature
in the 1960s, addressed these problems under the name of stochastic gradient (or
stochastic quasi-gradient) methods. More modern proofs are based on the use of
martingale processes, which do not start with the mean value theorem and do not
(always) need the differentiability conditions that this approach needs.
Our presentation does, however, help present several key ideas in most proofs of
this type. First, concepts of almost sure convergence are virtually standard. Second,
it is common to set up equations such as (7.59) and then take a ﬁnite sum as in
(7.60) using the alternating terms in the sum to cancel all but the ﬁrst and last
elements of the sequence of some function (in our case, F(xn−1, ωn)). We then
establish the boundedness of this expression as N →∞, which will require the
assumption that ∞
n=1(αn−1)2 < ∞. Then the assumption ∞
n=1 αn−1 = ∞is used
to show that if the remaining sum is bounded, then its terms must go to zero.
More modern proofs will use functions other than F(x). Popular is the introduc-
tion of so-called Lyapunov functions, which are artiﬁcial functions that provide a
measure of optimality. These functions are constructed for the purpose of the proof
and play no role in the algorithm itself. For example, we might let T n = ||xn −x∗||
be the distance between our current solution xn and the optimal solution. We will
then try to show that T n is suitably reduced to prove convergence. Since we do
not know x∗, this is not a function we can actually measure, but it can be a useful
device for proving that the algorithm actually converges.
It is important to realize that stochastic gradient algorithms of all forms do not
guarantee an improvement in the objective function from one iteration to the next.
First, a sample gradient gn may represent an appropriate ascent vector for a sample
of the function F(xn−1, ωn) but not for its expectation. In other words, randomness
means that we may go in the wrong direction at any point in time. Second, our
use of a nonoptimizing stepsize, such as αn−1 = 1/n, means that even with a good
ascent vector, we may step too far and actually end up with a lower value.
7.6.3
A More Modern Proof
Since the original work by Robbins and Monro, more powerful proof techniques
have evolved. Below we illustrate a basic martingale proof of convergence. The
concepts are somewhat more advanced, but the proof is more elegant and requires
milder conditions. A signiﬁcant generalization is that we no longer require that
our function be differentiable (which our ﬁrst proof required). For large classes of
resource allocation problems, this is a signiﬁcant improvement.
First, just what is a martingale? Let ω1, ω2, . . . , ωt be a set of exogenous
random outcomes, and let ht = Ht(ω) = (ω1, ω2, . . . , ωt) represent the history
of the process up to time t. We also let Ft be the σ-algebra on  generated by

280
policy search
Ht. Further let Ut be a function that depends on ht (we would say that Ut is a
Ft-measurable function), and bounded (E|Ut| < ∞, ∀t ≥0). This means that if
we know ht, then we know Ut deterministically (needless to say, if we only know
ht, then Ut+1 is still a random variable). We additionally assume that our function
satisﬁes
E[Ut+1|Ft] = Ut.
If this is the case, then we say that Ut is a martingale. Alternatively, if
E[Ut+1|Ft] ≤Ut
(7.65)
then we say that Ut is a supermartingale. If Ut is a supermartingale, then it has
the property that it drifts downward, usually to some limit point U ∗. What is
important is that it only drifts downward in expectation. That is, it could easily
be the case that Ut+1 > Ut for speciﬁc outcomes. This captures the behavior of
stochastic approximation algorithms. Properly designed, they provide solutions that
improve on average, but where from one iteration to another the results can actually
get worse.
Finally, we assume that Ut ≥0. If this is the case, we have a sequence Ut that
drifts downward but that cannot go below zero. Not surprisingly, we obtain the
following key result:
Theorem 7.6.1
Let Ut be a positive supermartingale. Then Ut converges to a ﬁnite
random variable U ∗a.s.
So what does this mean for us? We assume that we are still solving a problem
of the form
min
x E{F(x, ω)},
(7.66)
where we assume that F(x, ω) is continuous and concave (but we do not require
differentiability. (Note that we have switched to solving a minimization problem,
which better suits the proof technique here.) Let xn be our estimate of x at iteration
n (remember that xn is a random variable). Instead of watching the evolution of a
process of time, we are studying the behavior of an algorithm over iterations. Let
F n = EF(xn) be our objective function at iteration n, and let F ∗be the optimal
value of the objective function. If we are maximizing, we know that F n ≤F ∗. If
we let Un = F ∗−F n, then we know that U n ≥0 (this assumes that we can ﬁnd
the true expectation rather than some approximation of it). A stochastic algorithm
will not guarantee that F n ≥F n−1, but if we have a good algorithm, then we may
be able to show that U n is a supermartingale, which at least tells us that in the
limit, U n will approach some limit U. With additional work we might be able to
show that U = 0, which means that we have found the optimal solution.
A common strategy is to deﬁne Un as the distance between xn and the optimal
solution, which is to say
U n = (xn −x∗)2.
(7.67)

why does it work?
281
Of course, we do not know x∗. As a result, we cannot actually compute U n, but
that is not really a problem for us (we are just trying to prove convergence). Note
that we immediately get U n ≥0 (without an expectation). If we can show that U n
is a supermartingale, then we get the result that Un converges to a random variable
U∗(which means that the algorithm converges). Showing that U ∗= 0 means that
our algorithm will (eventually) produce the optimal solution.
We are solving this problem using a stochastic gradient algorithm
xn = xn−1 −αn−1gn,
(7.68)
where gn is our stochastic gradient. If F is differentiable, we would write
gn = ∇xF(xn−1, ωn).
But, in general, F may be nondifferentiable, in which case we may have multiple
subgradients at a point xn−1 (for a single sample realization). In this case we write
gn ∈∂xF(xn−1, ωn),
where ∂xF(xn−1, ωn) refers to the set of subgradients at xn−1. We assume our
problem is unconstrained, so ∇xEF(x∗, ωn) = 0 if F is differentiable. If it is
nondifferentiable, we would assume that 0 ∈∂xEF(x∗, ωn).
Throughout our presentation, we assume that x (and hence gn) is a scalar
(exercise 11.10 provides an opportunity to redo this section using vector notation).
In contrast with the previous section, we are now going to allow our stepsizes to
be stochastic. For this reason we need to slightly revise our original assumptions
about stepsizes (equations (7.52) to (7.54)) by assuming
αn ≥0
a.s.,
(7.69)
∞

n=0
αn = ∞
a.s.,
(7.70)
E
/ ∞

n=0
(αn)2
0
< ∞.
(7.71)
The requirement that αn be nonnegative “almost surely” (a.s.) recognizes that αn
is a random variable. We can write αn(ω) as a sample realization of the stepsize
(i.e., this is the stepsize at iteration n if we are following sample path ω). When
we require that αn ≥0 “almost surely” we mean that αn(ω) ≥0 for all ω where
the probability (more precisely, probability measure) of ω, p(ω), is greater than
zero; said differently, this means that the probability that P[αn ≥0] = 1. The same
reasoning applies to the sum of the stepsizes given in equation (7.70). As the proof
unfolds, we will see the reason for needing the conditions (and why they are stated
as they are).
We next need to assume some properties of the stochastic gradient gn. Speciﬁ-
cally, we need to assume the following:

282
policy search
Assumption 1
E[gn+1(xn −x∗)|Fn] ≥0.
Assumption 2
|gn| ≤Bg.
Assumption 3
For any x where |x −x∗| > δ, δ > 0, there exists ϵ > 0 such
that E[gn+1|Fn] > ϵ.
Assumption 1 implies that on average, the gradient gn points toward the opti-
mal solution x∗. This is easy to prove for deterministic, differentiable functions,
but it may be harder to establish for stochastic problems or problems where F(x)
is nondifferentiable. This is because we do not have to assume that F(x) is dif-
ferentiable. Nor do we assume that a particular gradient gn+1 moves toward the
optimal solution (for a particular sample realization, it is entirely possible that we
are going to move away from the optimal solution). Assumption 2 implies that
the gradient is bounded. Assumption 3 requires that the expected gradient cannot
vanish at a nonoptimal value of x. This assumption will be satisﬁed for any strictly
concave function.
To show that U n is a supermartingale, we start with
U n+1 −U n = (xn+1 −x∗)2 −(xn −x∗)2
=

(xn −αngn+1) −x∗2 −(xn −x∗)2
=

(xn −x∗)2 −2αngn+1(xn −x∗) + (αngn+1)2
−(xn −x∗)2
= (αngn+1)2 −2αngn+1(xn −x∗).
(7.72)
Taking conditional expectations on both sides gives
E[Un+1|Fn] −E[U n|Fn] = E[(αngn+1)2|Fn] −2E[αngn+1(xn −x∗)|Fn].
(7.73)
We note that
E[αngn+1(xn −x∗)|Fn] = αnE[gn+1(xn −x∗)|Fn]
(7.74)
≥0.
(7.75)
Equation (7.74) is subtle but important, as it explains a critical piece of notation in
this book. Keep in mind that we may be using a stochastic stepsize formula, which
means that αn is a random variable. We assume that αn is Fn-measurable, which
means that we are not allowed to use information from iteration n + 1 to compute
it. This is why we use αn−1 in updating equations such as equation (7.6) instead of
αn. When we condition on Fn in equation (7.74), αn is deterministic, allowing us to
take it outside the expectation. This allows us to write the conditional expectation
of the product of αn and gn+1 as the product of the expectations. Equation (7.75)
comes from assumption 1 and the nonnegativity of the stepsizes.
Recognizing that E[U n|Fn] = U n (given Fn), we may rewrite (7.73) as
E[Un+1|Fn] = Un + E[(αngn+1)2|Fn] −2E[αngn+1(xn −x∗)|Fn]
≤U n + E[(αngn+1)2|Fn].
(7.76)

why does it work?
283
Because of the positive term on the right-hand side of (7.76), we cannot directly
get the result that Un is a supermartingale. But hope is not lost. We appeal to a
neat little trick that works as follows. Let
W n = U n +
∞

m=n
(αmgm+1)2.
(7.77)
We are going to show that W n is a supermartingale. From its deﬁnition, we obtain
W n = W n+1 + Un −Un+1 + (αngn+1)2.
(7.78)
Taking conditional expectations of both sides gives
W n = E

W n+1|Fn
+ U n −E

U n+1|Fn
+ E

(αngn+1)2|Fn
,
which is the same as
E[W n+1|Fn] = W n −

Un + E

(αngn+1)2|Fn
−E[U n+1|Fn]




I
.
We see from equation (7.76) that I ≥0. Removing this term gives us the inequality
E[W n+1|Fn] ≤W n.
(7.79)
This means that W n is a supermartingale. It turns out that this is all we really need
because limn→∞W n = limn→∞Un. This means that
lim
n→∞Un →U∗
a.s.
(7.80)
Now that we have the basic convergence of our algorithm, we have to ask: but
what is it converging to? For this result we return to equation (7.72) and sum it
over the values n = 0 up to some number N, giving us
N

n=0
(U n+1 −Un) =
N

n=0
(αngn+1)2 −2
N

n=0
αngn+1(xn −x∗).
(7.81)
The left-hand side of (7.81) is an alternating sum (sometimes referred to as a
telescoping sum), which means that every element cancels out except the ﬁrst and
the last, giving us
UN+1 −U0 =
N

n=0
(αngn+1)2 −2
N

n=0
αngn+1(xn −x∗).
Taking expectations of both sides gives
E[UN+1 −U0] = E
/ N

n=0
(αngn+1)2
0
−2E
/ N

n=0
αngn+1(xn −x∗)
0
.
(7.82)

284
policy search
We want to take the limit of both sides as N goes to inﬁnity. To do this, we have
to appeal to the Dominated Convergence Theorem (DCT), which tells us that
lim
N→∞
6
x
f n(x)dx =
6
x

lim
N→∞f n(x)

dx
if |f n(x)| ≤g(x) for some function g(x), where
6
x
g(x)dx < ∞.
For our application the integral represents the expectation (we would use a sum-
mation instead of the integral if x were discrete), which means that the DCT gives
us the conditions needed to exchange the limit and the expectation. Above we
showed that E[Un+1|Fn] is bounded (from (7.76) and the boundedness of U0 and
the gradient). This means that the right-hand side of (7.82) is also bounded for
all n. The DCT then allows us to take the limit as N goes to inﬁnity inside the
expectations, giving us
U ∗−U 0 = E
/ ∞

n=0
(αngn+1)2
0
−2E
/ ∞

n=0
αngn+1(xn −x∗)
0
.
We can rewrite the ﬁrst term on the right-hand side as
E
/ ∞

n=0
(αngn+1)2
0
≤E
/ ∞

n=0
(αn)2(B)2
0
(7.83)
= B2 E
/ ∞

n=0
(αn)2
0
(7.84)
< ∞.
(7.85)
Equation (7.83) comes from assumption 2, which requires that |gn| be bounded
by B, and this immediately gives us equation (7.84). The requirement that
E ∞
n=0(αn)2 < ∞(equation (7.54)) gives us (7.85), which means that the ﬁrst
summation on the right-hand side of (7.82) is bounded. Since the left-hand side
of (7.82) is bounded, we can conclude that the second term on the right-hand side
of (7.82) is also bounded.
Now let
βn = E

gn+1(xn −x∗)

= E

E

gn+1(xn −x∗)|Fn
≥0,
since E[gn+1(xn −x∗)|Fn] ≥0 from Assumption 1. This means that
∞

n=0
αnβn < ∞
a.s.
(7.86)

bibliographic notes
285
But we have required that ∞
n=0 αn = ∞a.s. (equation (7.70)). Since αn ≥0 and
βn ≥0 (a.s.), we conclude that
lim
n→∞βn →0
a.s.
(7.87)
If βn →0, then E[gn+1(xn −x∗)] →0, which allows us to conclude that
E[gn+1(xn −x∗)|Fn] →0 (the expectation of a nonnegative random variable
cannot be zero unless the random variable is always zero). But what does this tell
us about the behavior of xn? Knowing that βn →0 does not necessarily imply
that gn+1 →0 or xn →x∗. There are three scenarios:
1. xn →x∗for all n, and of course all sample paths ω. If this were the case,
we are done.
2. xnk →x∗for a subsequence n1, n2, . . . , nk, . . .. For example, it might be
that the sequence x1, x3, x5, . . . →x∗, while E[g2|F1], E[g4|F3], . . . , →
0. This would mean that for the subsequence nk, U nk →0. But we already
know that U n →U ∗, where U ∗is the unique limit point, which means that
U∗= 0. But, if this is the case, then this is the limit point for every sequence
of xn.
3. There is no subsequence xnk that has x∗as its limit point. This means that
E[gn+1|Fn] →0. However, assumption 3 tells us that the expected gradient
cannot vanish at a nonoptimal value of x. This means that this case cannot
happen.
This completes the proof.
□
7.7
BIBLIOGRAPHIC NOTES
Section 7.2
The theoretical foundation for estimating value functions from
Monte Carlo estimates has its roots in stochastic approximation theory, orig-
inated by Robbins and Monro (1951), with important early contributions
made by Kiefer and Wolfowitz (1952), Blum (1954a) and Dvoretzky (1956).
For thorough theoretical treatments of stochastic approximation theory, see
Wasan (1969), Kushner and Clark (1978), and Kushner and Yin (1997). Very
readable treatments of stochastic optimization can be found in Pﬂug (1996)
and Spall (2003).
Section 7.3
A nice introduction to various learning strategies is contained
in Kaelbling (1993) and Sutton and Barto (1998). Thrun (1992) contains
a good discussion of exploration in the learning process. The discussion
of Boltzmann exploration and epsilon-greedy exploration is based on Singh
et al. (2000).
Section 7.4
The knowledge gradient policy for normally distributed rewards
and independent beliefs was introduced by Gupta and Miescke (1996),
and subsequently analyzed in greater depth by Frazier et al. (2008). The

286
policy search
knowledge gradient for correlated beliefs was introduced by Frazier et al.
(2009). The adaptation of the knowledge gradient for online problems is due
to Ryzhov and Powell (2009). The knowledge gradient for correlated beliefs
is due to Negoescu et al. (2010).
Section 7.5
There is an advanced ﬁeld of research within the simulation com-
munity that has addressed the problem of using simulation (in particular,
discrete event simulation) to ﬁnd the best setting of a set of parameters that
controls the behavior of the simulation. An early survey is given by Bech-
hofer et al. (1995); a more recent survey can be found in Fu et al. (2007).
Kim and Nelson (2006) provides a nice tutorial overview of methods based
on ordinal optimization. Other important contributions in this line include
Hong and Nelson (2006, 2007). Most of this literature considers problems
where the number of potential alternatives is not too large. Nelson et al.
(2001) consider the case where the number of designs is large. Ankenman
et al. (2009) discusses the use of a technique called kriging, which is useful
when the parameter vector x is continuous. The literature on optimal com-
puting budget allocation is based on a series of articles originating with Chen
(1995), and including Yucesan et al. (1997), Chen et al. (1997, 2000) and
Chick et al. (2000). Chick and Inoue (2001) introduce the LL(B) strategy,
which maximizes the linear loss with measurement budget B. He et al. (2007)
introduce an OCBA procedure for optimizing the expected value of a chosen
design, using the Bonferroni inequality to approximate the objective func-
tion for a single stage. A common strategy in simulation is to test different
parameters using the same set of random numbers to reduce the variance of
the comparisons. Fu et al. (2007) apply the OCBA concept to measurements
using common random numbers.
Section 7.6.2
This proof is based on Blum (1954b), which generalized the
original paper by Robbins and Monro (1951).
Section 7.6.3
The proof in Section 7.6.3 uses standard techniques drawn from
several sources, notably Wasan (1969), Chong (1991), Kushner and Yin
(1997) and, for this author, Powell and Cheung (2000).
PROBLEMS
7.1
We are going to optimize a policy for selling an asset using a spreadsheet.
Let column A be the time period t starting at t = 0, and generate a series
of prices in column B using pt = pt−1 + 2U, where U is a random vari-
able uniformly distributed between −0.5 and +0.5, generated in Excel using
U = (RAND() −0.5). Set p0 = 8. In column C, let pt = 0.9pt−1 + 0.1pt.
In column D, create a variable Rt = Rt−1 −xt−1 with R0 = 1. Rt = 1 means
that we still own the asset. In column E, program the policy Xt = Rt ∗
IF(pt −pt > β, 1, 0), which means that we will sell the asset if its current
price exceeds the smoothed price by more than β. Finally, compute the return

problems
287
if the asset is sold in column F using Ct = xtpt. Note that if xt = 1, Rt+1 = 0
and will then remain zero. Repeat the calculations for 100 rows, by which
time the asset should have sold.
(a) Program the spreadsheet and perform 10 repetitions of the simulation
using β = 1.0. Compute the average, the sample variance and the variance
of the average (by dividing the sample variance by 10).
(b) Repeat (a) using β = 2.0. Compare the conﬁdence intervals for the per-
formance of each policy, and determine if they are statistically different
at the 95 percent conﬁdence level.
(c) Create a parallel simulation to the ﬁrst one that simulates the policy using
β + δ, where δ is a perturbation parameter that you specify. The second
simulation should be run using the same prices as the ﬁrst simulation. If
F(β) is the performance of the policy using β, compute a sample gradient
using
gn = F(βn + δ) −F(βn)
δ
.
Using δ = 1, perform 10 iterations of a stochastic gradient algorithm
βn+1 = βn + αngn
Let αn = 5/(10 + n), and perform 10 iterations of this algorithm.
(d) Perform three iterations each for β = 0.5, 1.0, 1.5, 2.0, 3.0. Plot the
points and use the LINEST routine in Excel (or any other statistical
package) to ﬁt a curve to identify the best value of β.
7.2
We are going to optimize a policy for storing electricity in a battery when
prices are low, and selling the power when prices are high. Let column A
be the time period t starting at t = 0, and generate a series of prices in
column B using pt = pt−1 + 0.7(µ −pt−1) + 40U where U is a random
variable uniformly distributed between −0.5 and +0.5, generated in Excel
using U = (RAND( ) −0.5). Set p0 = 40. In column C, create a variable
Rt = max{0, Rt−1 + xt−1} with R0 = 0, where Rt represents the amount of
energy in megawatt-hours stored in the battery. In column D, program the
policy
Xt =



1,
pt ≤pL,
−1,
pt ≥pU,
0,
otherwise.
In column E, program the contribution Ct = ptxt. Sum the contributions over
200 time periods and compute the averageF(P U, P L).
(a) Program the spreadsheet and perform 10 repetitions of the simulation
using pU = 45 and P L = 25. Compute the average, the sample variance
and the variance of the average (by diving the sample variance by 10).

288
policy search
(b) Repeat (a) using pU = 50 and P L = 25. Compare the conﬁdence inter-
vals for the performance of each policy and determine if they are statis-
tically different at the 95 percent conﬁdence level.
(c) Create a parallel simulation to the ﬁrst one that simulates the policy
using P U + δ and P L + δ for the upper and lower prices, where δ is a
perturbation parameter that you specify. The second simulation should be
run using the same prices as the ﬁrst simulation. IfF(β) is the performance
of the policy using β, compute a sample gradient using
gn = F(βn + δ) −F(βn)
δ
.
Using δ = 5, perform 10 iterations of a stochastic gradient algorithm
βn+1 = βn + αngn.
Let αn = 5/(10 + n), and perform 10 iterations of this algorithm.
(d) Perform one iteration each while varying P L over the grid from 20 to
35 in increments of 5, and varying P U from 45 to 60 in increments of 5.
Plot the points using a three-dimensional plot using Excel, and from this
estimate what appear to be the best values of P U and P L.
7.3
You are trying to plan how much energy you can use from your solar panels,
and how much should be stored so that it can be used later when energy
may be expensive. At time t, let pt be the price of electricity from the
grid, and assume it evolves according to pt+1 = pt + ˆpt+1. Let Et be the
energy generated by the solar panels, where Et+1 = Et + ˆEt+1, and let Dt =
µD
t + ϵtd be the demand for energy which follows a predictable pattern µD
t
with some variation. Let Rt be the energy stored in the battery, and let xt be
the amount of solar energy that is stored (if xt > 0) or withdrawn (if xt < 0).
(a) What is the state variable for this problem?
(b) What types of policies would you consider using for a problem such this.
Discuss the strengths and weaknesses of each.
(c) Now introduce the dimension that at time t, you are given a forecastEtt′
for the energy from the solar panels that will be produced at time t′. What
is the state variable for this problem?
(d) How would your answer to (b) change with this new information?

C H A P T E R
8
Approximating Value Functions
One of the most powerful, yet difﬁcult, tools in approximate dynamic programming
is the ability to create a policy by approximating the value of being in a state,
famously known as value function approximations. If Vt(St) is our approximation
of the value of being in state St, we can make decisions using the policy
at = arg max
at∈At

C(St, at) + γ EVt+1(St+1)

,
where St+1 = SM(St, at, Wt+1). The challenge is ﬁnding a good approximation
Vt(St).
We break the problem of ﬁnding this approximation into three speciﬁc tasks,
covered in this and the following two chapters:
Approximating value functions. This ﬁrst chapter addresses the different ways
we can approximate value functions, divided along three lines: lookup tables
(with aggregation), parametric models, and nonparametric models.
Learning value function approximations. Chapter 9 describes different meth-
ods for computing ˆvn, a sampled estimate of the value of being in a state Sn,
and then using this information to update the value function approximation
for a ﬁxed policy.
Learning while optimizing.
Chapter 10 introduces the challenge of search-
ing for better policies while simultaneously estimating the value of a policy.
Approximate value iteration and policy iteration are illustrated using a mix-
ture of lookup tables, parametric models, and nonparametric models.
Readers should view this chapter as a peek into the vast ﬁeld of statistical learn-
ing (also known as machine learning). All the methods we present have been used
in the approximate dynamic programming community. However, it is important to
realize that ADP imposes special demands on statistical algorithms. One of these
Approximate Dynamic Programming: Solving the Curses of Dimensionality, Second Edition.
Warren B. Powell.
2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
289

290
approximating value functions
is the need to update statistical models recursively, since estimates of the value of
being in a state arrive in a continuous stream, and not as a single batch.
8.1
LOOKUP TABLES AND AGGREGATION
For many years researchers addressed the “curse of dimensionality” by using aggre-
gation to reduce the size of the state space. The idea was to aggregate the original
problem, solve it exactly (using the techniques of Chapter 3), and then disaggregate
it back to obtain an approximate solution to the original problem. Not only would
the value function be deﬁned over this smaller state space, so would the one-step
transition matrix. In fact, simplifying the transition matrix (since its size is given
by the square of the number of states) was even more important than simplifying
the states over which the value function was deﬁned.
In our presentation we only use aggregation in the value function approximation.
Once we have chosen an action at, we simulate our way to the next state as before
using St+1 = SM(St, at, Wt+1) using the original, disaggregate state variable. We
feel that this is a more effective modeling strategy than methods that aggregate the
dynamics of the process itself.
8.1.1
ADP and Aggregation
Perhaps one of the most powerful features of approximate dynamic programming is
that even if we use aggregation, we do not have to simplify the state of the system.
The transition function St+1 = SM(St, at, Wt+1) always uses the original, disag-
gregate (possibly continuous) state vector. Aggregation is only used to approximate
the value function. For example, in our nomadic trucker problem it is necessary
to capture location, domicile, ﬂeet type, equipment type, number of hours he has
driven that day, how many hours he has driven on each of the past seven days,
and the number of days he has been driving since he was last at home. All of
this information is needed to simulate the driver forward in time. But we might
estimate the value function using only the location of the driver, his domicile, and
ﬂeet type. We are not trying to simplify how we represent the problem; rather, we
only want to simplify how we approximate the value function for the purpose of
making decisions.
If our nomadic trucker is described by the state vector St, the transition function
St+1 = SM(St, at, Wt+1) may represent the state vector at a high level of detail
(some values may be continuous). But the decision problem
max
at∈A

C(St, at) + γ EVt+1(G(St+1))

(8.1)
uses a value function Vt+1(G(St+1)), where G(·) is an aggregation function that
maps the original (and very detailed) state S into something much simpler. The
aggregation function G may ignore a dimension, discretize it, or use any of a
variety of ways to reduce the number of possible values of a state vector. This also
reduces the number of parameters we have to estimate. In what follows we drop

lookup tables and aggregation
291
the explicit reference of the aggregation function G and simply use Vt+1(St+1).
The aggregation is implicit in the value function approximation.
Some examples of aggregation include:
Spatial. A transportation company is interested in estimating the value of truck
drivers at a particular location. Locations may be calculated at the level of a
ﬁve-digit zip code (there are about 55,000 in the United States), three-digit
zip code (about 1000), or the state level (48 contiguous states).
Temporal.
A bank may be interested in estimating the value of holding an
asset at a point in time. Time may be measured by the day, week, month, or
quarter.
Continuous parameters.
The state of an aircraft may be its fuel level; the
state of a traveling salesman may be how long he has been away from home;
the state of a water reservoir may be the depth of the water; the state of
the cash reserve of a mutual fund is the amount of cash on hand at the end
of the day. These are examples of systems with at least one dimension of
the state that is at least approximately continuous. The variables may all be
discretized into intervals.
Hierarchical classiﬁcation.
A portfolio problem may need to estimate the
value of investing money in the stock of a particular company. It may be
useful to aggregate companies by industry segment (e.g., a particular company
might be in the chemical industry), and it might be further aggregated based
on whether it is viewed as a domestic or multinational company. Similarly,
problems of managing large inventories of parts (e.g.,for cars) may beneﬁt by
organizing parts into part families (transmission parts, engine parts, dashboard
parts).
The examples below provide additional illustrations.
■
EXAMPLE 8.1
The state of a jet aircraft may be characterized by multiple attributes that
include spatial and temporal dimensions (location and ﬂying time since the last
maintenance check) as well other attributes. A continuous parameter could be
the fuel level; an attribute that lends itself to hierarchical aggregation might be
the speciﬁc type of aircraft. We can reduce the number of states (attributes) of
this resource by aggregating each dimension into a smaller number of potential
outcomes.
■
■
EXAMPLE 8.2
The state of a portfolio might consist of the number of bonds that are charac-
terized by the source of the bond (a company, a municipality, or the federal
government), the maturity (six months, 12 months, 24 months), when it was

292
approximating value functions
purchased, and its rating by bond agencies. Companies can be aggregated up by
industry segment.
■
■
EXAMPLE 8.3
Blood stored in blood banks can be characterized by type, the source (which
might indicate risks for diseases), age (it can be stored for up to 42 days), and
the current location where it is being stored. A national blood management
agency might want to aggregate the state space by ignoring the source (ignoring
a dimension is a form of aggregation), discretizing the age from days into
weeks, and aggregating locations into more aggregate regions.
■
■
EXAMPLE 8.4
The value of an asset is determined by its current price, which is continuous.
We can estimate the asset using a price discretized to the nearest dollar.
■
There are many applications where aggregation is naturally hierarchical. For
example, in our nomadic trucker problem we might want to estimate the value of
a truck based on three attributes: location, home domicile, and ﬂeet type. The ﬁrst
two represent geographical locations, which can be represented (for this example)
at three levels of aggregation: 400 sub-regions, 100 regions, and 10 zones. Table 8.1
illustrates ﬁve levels of aggregation that might be used. In this example each higher
level can be represented as an aggregation of the previous level.
Aggregation is also useful for continuous variables. Suppose that our state vari-
able is the amount of cash we have on hand, a number that might be as large as
$10 million dollars. We might discretize our state space in units of $1 million,
$100 thousand, $10 thousand, $1000, $100, and $10. This discretization produces
a natural hierarchy since 10 segments at one level of aggregation naturally group
into one segment at the next level of aggregation.
Hierarchical aggregation is often the simplest to work with, but in most cases
there is no reason to assume that the structure is hierarchical. In fact we may even
use overlapping aggregations (sometimes known as “soft” aggregation), where the
same state s aggregates into multiple elements in Sg. For example, let s represent
Table 8.1
Examples of aggregations of the state space for the
nomadic trucker problem
Aggregation
Level
Location
Fleet Type
Domicile
Size of State Space
0
Sub-region
Fleet
Region
400 × 5 × 100 = 200,000
1
Region
Fleet
Region
100 × 5 × 100 = 50,000
2
Region
Fleet
Zone
100 × 5 × 10 = 5,000
3
Region
Fleet
—
100 × 5 × 1 = 500
4
Zone
—
—
10 × 1 × 1 = 10
Note: The dash indicates that the particular dimension is ignored.

lookup tables and aggregation
293
an (x, y) coordinate in a continuous space that has been discretized into the set of
points (xi, yi)i∈I. Assume that we have a distance metric ρ((x, y), (xi, yi)) that
measures the distance from any point (x, y) to every aggregated point (xi, yi), i ∈
I. We might use an observation at the point (x, y) to update estimates at each
(xi, yi) with a weight that declines with ρ((x, y), (xi, yi)).
8.1.2
Computing Bias and Variance
Before we present our methods for hierarchical aggregation, we are going to need
some basic results on bias and variance in statistical estimation. These results are
needed both here and later in the volume. To develop these results, we assume that
we are trying to estimate a time-varying series that we denote θn (think of this as
the true value of being in a state after n iterations, just as the value of being in a
state changes as we execute the value iteration algorithm). Let θ
n be our statistical
estimate of θn, and let ˆθn be a sampled estimate that we are going to use to update
θ
n as
θ
n = (1 −αn−1)θ
n−1 + αn−1 ˆθn,
where ˆθn is an unbiased observation (i.e., E ˆθn = θn), which is assumed to be
independent of θ
n−1. We emphasize that the parameter we are trying to estimate,
θn, varies with n just as expected value functions vary with n (recall the behavior
of the value function when using value iteration from Chapter 3). We are interested
in estimating the variance of θ
n and its bias, which is deﬁned by θ
n−1 −θn.
We start by computing the variance of θ
n. We can represent our observations of
θn using
ˆθn = θn + εn,
where Eεn = 0 and Var[εn] = σ 2. Previously εn was the error between our earlier
estimate and our later observation. Here we treat this as an exogenous measurement
error. With this model we can compute the variance of θ
n using
Var[θ
n] = λnσ 2,
(8.2)
where λn can be computed from the simple recursion
λn =

(αn−1)2,
n = 1,
(1 −αn−1)2λn−1 + (αn−1)2,
n > 1.
(8.3)
To see this, we start with n = 1. For a given (deterministic) initial estimate θ
0, we
ﬁrst observe that the variance of θ
1 is given by
Var[θ
1] = Var[(1 −α0)θ
0 + α0 ˆθ1]
= (α0)2Var[ ˆθ1]
= (α0)2σ 2.

294
approximating value functions
For general θ
n, we use a proof by induction. Assume that Var[θ
n−1] = λn−1σ 2.
Then, since θ
n−1 and ˆθn are independent, we ﬁnd
Var[θ
n] = Var
4
(1 −αn−1)θ
n−1 + αn−1 ˆθn5
= (1 −αn−1)2Var
4
θ
n−15
+ (αn−1)2Var[ ˆθn]
= (1 −αn−1)2λn−1σ 2 + (αn−1)2σ 2
(8.4)
= λnσ 2.
(8.5)
Equation (8.4) is true by assumption (in our induction proof), while equation (8.5)
establishes the recursion in equation (8.3). This gives us the variance, assuming of
course that σ 2 is known.
The bias of our estimate is the difference between our current estimate and the
true value, given by
βn = E[θ
n−1] −θn.
We note that θ
n−1 is our estimate of θn computed using the information up through
iteration n −1. Of course, our formula for the bias assumes that θn is known. These
two results for the variance and bias are called the parameters-known formulas.
We next require the mean squared error, which can be computed using
E
2
θ
n−1 −θn23
= λn−1σ 2 + (βn)2.
(8.6)
See exercise 8.3 to prove this. This formula gives the variance around the known
mean, θn. For our purposes it is also useful to have the variance around the obser-
vations ˆθn. Let
νn = E
2
θ
n−1 −ˆθn23
be the mean squared error (including noise and bias) between the current estimate
θ
n−1 and the observation ˆθn. It is possible to show that (see exercise 8.4)
νn = (1 + λn−1)σ 2 + (βn)2,
(8.7)
where λn is computed using equation (8.3).
In practice, we do not know σ 2, and we certainly do not know θn. As a result
we have to estimate both parameters from our data. We begin by providing an
estimate of the bias using
β
n = (1 −ηn−1)β
n−1 + ηn−1(θ
n−1 −ˆθ n),
where ηn−1 is a (typically simple) stepsize rule used for estimating the bias and
variance. As a general rule, we should pick a stepsize for ηn−1, which produces

lookup tables and aggregation
295
larger stepsizes than αn−1, because we are more interested in tracking the true signal
than producing an estimate with a low variance. We have found that a constant
stepsize such as 0.10 works quite well on a wide range of problems, but if precise
convergence is needed, it is necessary to use a rule where the stepsize goes to zero
such as the harmonic stepsize rule (equation (11.19)).
To estimate the variance, we begin by ﬁnding an estimate of the total variation
νn. Let νn be the estimate of the total variance which we might compute using
νn = (1 −ηn−1)νn−1 + ηn−1(θ
n−1 −ˆθn)2.
Using νn as our estimate of the total variance, we can compute an estimate of σ 2
by way of
(σ n)2 = νn −(β
n)2
1 + λn−1 .
We can use (σ n)2 in equation (8.2) to obtain an estimate of the variance of θ
n.
If we are doing true averaging (as would occur if we use a stepsize of 1/n),
we can get a more precise estimate of the variance for small samples by using the
recursive form of the small sample formula for the variance
(ˆσ 2)n = n −2
n −1(ˆσ 2)n−1 + 1
n(θ
n−1 −ˆθn)2.
(8.8)
The quantity (ˆσ 2)n is an estimate of the variance of ˆθn. The variance of our estimate
θ
(n) is computed using
(σ 2)n = 1
n(ˆσ 2)n.
8.1.3
Modeling Aggregation
We begin by deﬁning a family of aggregation functions
Gg : S →S(g).
Here S(g) represents the gth level of aggregation of the state space S. Let
S(g) = Gg(s), the gth level aggregation of the state vector s.
G = set of indexes corresponding to the levels of aggregation.
In this section we assume that we have a single aggregation function G that maps
the disaggregate state s ∈S = S(0) into an aggregated space S(g). In Section 8.1.4,
we let g ∈G = {0, 1, 2, . . .} and we work with all levels of aggregation at the
same time.
To begin our study of aggregation, we ﬁrst need to characterize how we sample
different states (at the disaggregate level). For this discussion, we assume that we

296
approximating value functions
have two exogenous processes: at iteration n, the ﬁrst process chooses a state to
sample (which we denote by ˆsn), and the second produces an observation of the
value of being in state ˆsn ∈S, which we denote by ˆvn (or ˆvn
s ). There are different
ways of choosing a state ˆs; we could choose it at random, or by ﬁnding an
t by
solving (8.1) and then choosing a state ˆsn
t = SM(Sn
t , an
t , Wt+1).
We need to characterize the errors that arise in our estimate of the value function.
Let
ν(g)
s
= true value of being in state s at aggregation level g.
Here s is the original, disaggregated state vector. We let νs = ν(0)
s
be the true
(expected) value of being in state s. ν(g)
s
is the expected value of being in aggregated
state G(s). We can think of ν(g)
s
as an average over all the values of state s such that
G(s) = s. In fact there are different ways to weight all these disaggregate values,
and we have to specify the weighting in a speciﬁc application. We might weight
by how many times we actually visit a state (easy and practical, but it means the
aggregate measure depends on how you are visiting states), or we might weight all
states equally (this takes away the dependence on the policy, but we might include
states we would never visit).
We need a model of how we sample and measure values. Assume that at iteration
n, we sample a (disaggregated) state ˆsn, and we then observe the value of being
in this state with the noisy observation
ˆvn = νˆsn + εn.
We let ω be a sample realization of the sequence of states and values, given by
ω = (ˆs1, ˆv1, ˆs2, ˆv2, . . .).
Let
v(g,n)
s
= estimate of the value associated with the state vector s
at the gth level of aggregation after n observations.
Throughout our discussion, a bar over a variable means it was computed from
sample observations. A hat means the variable was an exogenous observation. If
there is nothing (e.g., ν or β), then it means this is the true value (which is not
known to us).
When we are working at the most disaggregate level (g = 0), the state s that
we measure is the observed state s = ˆsn. For g > 0, the subscript s in v(g,n)
s
refers
to Gg(ˆsn), or the gth level of aggregation of ˆsn. Given an observation (ˆsn, ˆvn),
we would update our estimate of being in state s = ˆsn using
v(g,n)
s
= (1 −α(g)
s,n−1)v(g,n−1)
s
+ α(g)
s,n−1 ˆvn.
Here we have written the stepsize α(g)
s,n−1 to explicitly represent the dependence
on the state and level of aggregation. Implicit is that this is also a function of the

lookup tables and aggregation
297
number of times that we have updated v(g,n)
s
by iteration n, rather than a function
of n itself.
To illustrate, imagine that our nomadic trucker is described by the state vec-
tor s = (Loc, Equip, Home, DOThrs, Days), where “Loc” is location, “Equip”
denotes the type of trailer (long, short, refrigerated), “Home” is the location of
where the driver lives, “DOThrs” is a vector giving the number of hours the driver
has worked on each of the last eight days, and “Days” is the number of days the
driver has been away from home. We are going to estimate the value V(s) for
different levels of aggregation of s, where we aggregate purely by ignoring certain
dimensions of s. We start with our original disaggregate observation ˆv(s), which
we are going to write as
ˆv


Loc
Equip
Home
DOThrs
Days


= max
a

C(s, a) +V(SM(s, a))

.
We now wish to use this estimate of the value of a driver in state s to produce value
functions at different levels of aggregation. We can do this by simply smoothing
this disaggregate estimate in with estimates at different levels of aggregation, as
in
v(1,n)


Loc
Equip
Home

= (1 −α(1)
s,n−1)v(1,n−1)


Loc
Equip
Home

+ α(1)
s,n−1 ˆv


Loc
Equip
Home
DOThrs
Days


,
v(2,n)

Loc
Equip

= (1 −α(2)
s,n−1)v(2,n−1)

Loc
Equip

+ α(2)
s,n−1 ˆv


Loc
Equip
Home
DOThrs
Days


,
v(3,n)  Loc 
= (1 −α(3)
s,n−1)v(3,n−1)  Loc 
+ α(3)
s,n−1 ˆv


Loc
Equip
Home
DOThrs
Days


.
In the ﬁrst equation we are smoothing the value of a driver based on a ﬁve-
dimensional state vector, given by ˆv(s), in with an approximation indexed by a

298
approximating value functions
three-dimensional state vector. The second equation does the same using value
function approximation indexed by a two-dimensional state vector, while the third
equation does the same with a one-dimensional state vector. It is important to keep
in mind that the stepsize must reﬂect the number of times a state has been updated.
We can estimate the variance of v(g,n)
s
using the techniques described in
Section 8.1.2. Let
(s2
s)(g,n) = estimate of the variance of observations made of state s,
using data from aggregation level g, after n observations.
(s2
s)(g,n) is the estimate of the variance of the observations ˆv when we observe a
state ˆsn which aggregates to state s (i.e., Gg(ˆsn) = s). We are really interested in
the variance of our estimate of the mean, v(g,n)
s
. In Section 8.1.2 we showed that
(σ 2
s)(g,n) = Var[v(g,n)
s
]
= λ(g,n)
s
(s2
s )(g,n),
(8.9)
where (s2
s )(g,n) is an estimate of the variance of the observations ˆvn
t at the gth level
of aggregation (computed below), and λ(g,n)
s
can be computed from the recursion
λ(g,n)
s
=

(α(g)
s,n−1)2,
n = 1,
(1 −α(g)
s,n−1)2λ(g,n−1)
s
+ (α(g)
s,n−1)2,
n > 1.
Note that if the stepsize α(g)
s,n−1 goes to zero, then λ(g,n)
s
will also go to zero, as will
(σ 2
s)(g,n). We now need to compute (s2
s )(g,n), which is the estimate of the variance
of observations ˆvn for states ˆsn for which Gg(ˆsn) = s (the observations of states
that aggregate up to s). Let ν(g,n)
s
be the total variation, given by
ν(g,n)
s
= (1 −ηn−1)ν(g,n−1)
s
+ ηn−1(v(g,n−1)
s
−ˆvn
s )2,
where ηn−1 follows some stepsize rule (which may be just a constant). We refer
to ν(g,n)
s
as the total variation because it captures deviations that arise due to both
measurement noise (the randomness when we compute ˆvn
s ) and bias (since v(g,n−1)
s
is a biased estimate of the mean of ˆvn
s ).
We ﬁnally need an estimate of the bias. There are two types of bias. In
Section 8.1.2 we measured the transient bias that arose from the use of smoothing
applied to a nonstationary time series using
β
(g,n)
s
= (1 −ηn−1)β
(g,n−1)
s
+ ηn−1(ˆvn −v(g,n−1)
s
).
(8.10)
This bias arises because the observations ˆvn may be steadily increasing (or decreas-
ing) with the iterations (the usual evolution of the value function that we ﬁrst saw
with value iteration). When we smooth on past observations, we obtain an estimate
v(g,n−1)
s
that tends to underestimate (overestimate if ˆvn tends to decrease) the true
mean of ˆvn.

lookup tables and aggregation
299
The second form of bias is the aggregation bias given by the difference between
the estimate at an aggregate level and the disaggregate level. We compute the
aggregation bias using
µ(g,n)
s
= v(g,n)
s
−v(0,n)
s
.
(8.11)
By the same reasoning presented in Section 8.1.2, we can separate out the effect
of bias to obtain an estimate of the variance of the error using
(ss2)(g,n) = νs(g,n) −

βs
(g,n)2
1 + λn−1
.
(8.12)
In the next section we put the estimate of aggregation bias, µs
(g,n), to work.
The relationships are illustrated in Figure 8.1, which shows a simple function
deﬁned over a single continuous state (e.g., the price of an asset). If we select
a particular state s, we ﬁnd we have only two observations for that state, versus
seven for that section of the function. If we use an aggregate approximation, we
would produce a single number over that range of the function, creating a bias
between the true function and the aggregated estimate. As the illustration shows,
the size of the bias depends on the shape of the function in that region.
One method for choosing the best level of aggregation is to choose the level
that minimizes (σ s2)(g,n) + (µs
(g,n))2, which captures both bias and variance. In the
next section we use the bias and variance to develop a method that uses estimates
at all levels of aggregation at the same time.
8.1.4
Combining Multiple Levels of Aggregation
Rather than try to pick the best level of aggregation, it is intuitively appealing to
use a weighted sum of estimates at different levels of aggregation. The simplest
Aggregated function
Original function
Aggregate value estimate
g,n
av
1
Bias
a
= Attribute selected
va
a
Figure 8.1
Illustration of a disaggregate function, an aggregated approximation, and a set of samples.
For a particular state s, we show the estimate and the bias.

300
approximating value functions
strategy is to use
vn
s =

g∈G
w(g)v(g)
s ,
(8.13)
where w(g) is the weight applied to the gth level of aggregation. We would expect
the weights to be positive and sum to one, but we can also view these simply as
coefﬁcients in a regression function. In such a setting we would normally write the
regression as
V(s|θ) = θ0 +

g∈G
θgv(g)
s
(see Section 8.2.2 for a discussion of general regression methods). The problem
with this strategy is that the weight does not depend on the state s. Intuitively it
makes sense to put a higher weight on the more disaggregate estimates of the value
of states s that have more observations, or where the estimated variance is lower.
This behavior is lost if the weight does not depend on s.
In practice, we will generally observe some states much more frequently than
others, suggesting that the weights should depend on s. To accomplish this, we
need to use
vn
s =

g∈G
w(g)
s v(g,n)
s
.
Now the weight depends on the state, allowing us to put a higher weight on the
disaggregate estimates when we have a lot of observations. This is clearly the most
natural, but when the state space is large, we face the challenge of computing
thousands (perhaps hundreds of thousands) of weights. If we are going to go this
route, we need a fairly simple method to compute the weights.
We can view the estimates (v(g,n))g∈G as different ways of estimating the same
quantity. There is an extensive statistics literature on this problem. For example, it
is well known that the weights that minimize the variance of vn
s in equation (8.13)
are given by
w(g)
s
∝

(σ 2
s)(g,n)−1 .
Since the weights should sum to one, we obtain
w(g)
s
=

1
(σ 2
s)(g,n)
 

g∈G
1
(σ 2
s)(g,n)


−1
.
(8.14)
These weights work if the estimates are unbiased, which is clearly not the case.
This is easily ﬁxed by using the total variation (variance plus the square of the

lookup tables and aggregation
301
bias), producing the weights
w(g,n)
s
=
1
(σ 2
s)(g,n) +

µ(g,n)
s
2



g′∈G
1
(σ 2
s)(g′,n) +

µ(g′,n)
s
2


−1
.
(8.15)
These are computed for each level of aggregation g ∈G. Furthermore we compute
a different set of weights for each state s. (σ 2
s)(g,n) and µ(g,n)
s
are easily computed
recursively using equations (8.9) and (8.11), which makes the approach well suited
to large scale applications. Note that if the stepsize used to smooth ˆvn goes to zero,
then the variance (σ 2
s)(g,n) will also go to zero as n →∞. However, the bias β
(g,n)
s
will in general not go to zero.
Figure 8.2 shows the average weight put on each level of aggregation (when
averaged over all the states s) for a particular application. The behavior of the
weights as the number of observation grows illustrates the intuitive property that the
weights on the aggregate level are highest when there are only a few observations,
with a shift to the more disaggregate level as the algorithm progresses. This is an
important behavior to consider when approximating value functions. It is simply
not possible to produce good value function approximations with only a few data
points, so simple functions (with only a few parameters) should be used.
The weights computed using (8.2) minimize the variance in the estimate v(g)
s
if
the estimates at different levels of aggregation are independent, but this is simply
not going to be the case. v(0)
s
(an estimate of s at the most disaggregate level) and
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0
200
400
600
800
1000
1200
Iterations
Weights
1
3
2
4
5
Aggregation level
6
7
Weight on most disaggregate level
Weight on most aggregate levels
Figure 8.2
Average weight (across all states) for each level of aggregation using equation (8.15).

302
approximating value functions
v(1)
s
will be correlated since v(1)
s
is estimated using some of the same observations
used to estimate v(0)
s . So it is fair to ask if the weights produce accurate estimates.
To get a handle on this question, consider the scalar function in Figure 8.3a. At
the disaggregate level the function is deﬁned for 10 discrete values. This range is
then divided into three larger intervals, and an aggregated function is created by
estimating the function over each of these three larger ranges. Instead of using the
(a)
(b)
High bias
Moderate bias
Zero bias
v(a)
Optimal weights
Weights assuming independence
Figure 8.3
Weight given to the disaggregate level for a two-level problem at each of 10 points,
with and without the independence assumption. (a) Scalar, nonlinear function; (b) weight given to
disaggregate level (from George et al., 2008).

lookup tables and aggregation
303
High noise
with independence assumption
without independence
assumption
Low noise
Moderate noise
Number of observations
Performance measure (θs)
Figure 8.4
Effect of ignoring the correlation between estimates at different levels of aggregation.
weights computed using (8.15), we can ﬁt a regression of the form
ˆvn = θ0v(0,n)
s
+ θ1v(1,n)
s
.
(8.16)
The parameters θ0 and θ1 can be ﬁt using regression techniques. Note that while we
would expect θ0 + θ1 to be approximately 1, there is no formal guarantee of this.
If we use only two levels of aggregation, then we can ﬁnd (θ0, θ1) using linear
regression and can compare these weights to those computed using equation (8.15)
where we assume independence.
For this example the weights are shown in Figure 8.3b. The ﬁgure illustrates that
the weight on the disaggregate level is highest when the function has the greatest
slope, which produces the highest biases. When we compute the optimal weights
(by which we capture the correlation), the weight on the disaggregate level for the
portion of the curve that is ﬂat is zero, as we would expect. Note that when we
assume independence, the weight on the disaggregate level (when the slope is zero)
is no longer zero. Clearly, a weight of zero is best because it means that we are
aggregating all the points over the interval into a single estimate, which is going
to be better than trying to produce three individual estimates.
One would expect that using the optimal weights, which captures the correlations
between estimates at different levels of aggregation, would also produce better esti-
mates of the function itself. This does not appear to be the case. We compared the
errors between the estimated function and the actual function using both methods
for computing weights and three levels of noise around the function. The results
are shown in Figure 8.4, which indicates that there is virtually no difference in the

304
approximating value functions
accuracy of the estimates produced by the two methods. This observation has held
up under a number of experiments.
8.1.5
State Aliasing and Aggregation
An issue that arises when we use aggregation is that two different states (call them
S1 and S2) may have the same behavior (as a result of aggregation), despite the
fact that the states are different, and perhaps should exhibit different behaviors.
When this happens, we refer to S1 as an alias of S2 (and vice versa). We refer to
this behavior as aliasing.
In approximate dynamic programming we do not (as a rule) aggregate the state
variable as we step forward through time. In most applications, transition functions
can handle state variables at a high level of detail. However, we may aggregate
the state for the purpose of computing the value function. In this case, if we are in
state Si and take action xi, we might expect to end up in state S′
i, where S′
1 ̸= S′
2.
But we might have V(S′
1) = V(S′
2) as a result of aggregation. So it might happen
that we make the same decision even though the difference in the state S1 and S2
might call for different decisions. Welcome to approximate dynamic programming.
8.2
PARAMETRIC MODELS
Up to now we have focused on lookup table representations of value functions,
where if we are in a (discrete) state s, we compute an approximation v(s) that is
an estimate of the value of being in state s. Using aggregation (even mixtures of
estimates at different levels of aggregation) is still a form of lookup table (we are
just using a simpler lookup table). An advantage of this strategy is that it avoids the
need to exploit specialized structure in the state variable (other than the deﬁnition
of levels of aggregation). A disadvantage is that it does not allow you to take
advantage of structure in the state variable.
There has been considerable interest in estimating value functions using regres-
sion methods. A classical presentation of linear regression poses the problem of
estimating a parameter vector θ to ﬁt a model that predicts a variable y using a set
of observations (known as covariates in the machine learning community) (xi)i∈I,
where we assume a model of the form
y = θ0 +
I

i=1
θixi + ε.
(8.17)
In the language of approximate dynamic programming, the independent variables
xi are created using basis functions that reduce potentially large state variables
into a small number of features, a term widely used in the artiﬁcial intelligence
community. If we are playing the game of tic-tac-toe, we might want to know if
our opponent has captured the center square, or we might want to know the number
of corner squares we own. If we are managing a ﬂeet of taxis, we might deﬁne for

parametric models
305
each region of the city a feature that gives the number of taxis in the region, plus
0.5 times the number of taxis in each of the neighboring regions.
Using this language, instead of an independent variable xi, we would have
a basis function φf (S), where f ∈F is a feature. φf (S) might be an indicator
variable (e.g., 1 if we have an “X” in the center square of our tic-tac-toe board),
a discrete number (the number of X’s in the corners of our tic-tac-toe board), or a
continuous quantity (the price of an asset, the amount of oil in our inventories, the
amount of AB−negative blood on hand at the hospital). Some problems might have
fewer than 10 features, others may have dozens, and some may have hundreds of
thousands. In general, however, we would write our value function in the form
V(S|θ) =

f ∈F
θf φf (S).
In a time-dependent model the parameter vector θ and basis functions φ may also
be indexed by time (but not necessarily).
In the remainder of this section, we provide a brief review of linear regression,
followed by some examples of regression models. We close with a more advanced
presentation that provides insights into the geometry of basis functions (including
a better understanding of why they are called “basis functions”). Given the tremen-
dous amount of attention this class of approximations has received in the literature,
we defer to Chapter 9 a full description of how to ﬁt linear regression models
recursively for an ADP setting.
8.2.1
Linear Regression Review
Let yn be the nth observation of our dependent variable (what we are trying to
predict) based on the observation (xn
1, xn
2, . . . , xn
I ) of our independent (or explana-
tory) variables (the xi are equivalent to the basis functions we used earlier). Our
goal is to estimate a parameter vector θ that solves
min
θ
n

m=1

ym −(θ0 +
I

i=1
θixm
i )
2
.
(8.18)
This is the standard linear regression problem. Let θ
n be the optimal solution for
this problem. Throughout this section we assume that the underlying process from
which the observations yn are drawn is stationary (an assumption that is almost
never satisﬁed in approximate dynamic programming).
If we deﬁne x0 = 1, we let
xn =


xn
0
xn
1...
xn
I



306
approximating value functions
be an I + 1-dimensional column vector of observations. Throughout this section,
and unlike the rest of the book, we use traditional vector operations, where xT x is
an inner product (producing a scalar) while xxT is an outer product, producing a
matrix of cross terms.
Letting θ be the column vector of parameters, we can write our model as
y = θT x + ε.
We assume that the errors (ε1, . . . , εn) are independent and identically distributed.
We do not know the parameter vector θ, so we replace it with an estimate θ that
gives us the predictive formula
yn = (θ)T xn,
where yn is our predictor of yn+1. Our prediction error is
ˆεn = yn −(θ)T xn.
Our goal is to choose θ to minimize the mean squared error
min
θ
n

m=1
(ym −θT xm)2.
(8.19)
It is well known that this can be solved very simply. Let Xn be the n by I + 1
matrix
Xn =


x01
x11
xI 1
x02
x12
xI 2
...
...
. . .
...
x0n
x1n
xI n


.
Next denote the vector of observations of the dependent variable as
Y n =


y1
y2
...
yn

.
The optimal parameter vector θ (after n observations) is given by
θ = [(Xn)T Xn]−1(Xn)T Y n.
(8.20)
Solving a static optimization problem such as (8.19), which produces the elegant
equations for the optimal parameter vector in (8.20), is the most common approach
taken by the statistics community. It has little direct application in approximate
dynamic programming since our problems tend to be recursive in nature, reﬂecting

parametric models
307
the fact that at each iteration we obtain new observations, which require updates
to the parameter vector. In addition our observations tend to be notoriously non-
stationary. Later we show how to overcome this problem using the methods of
recursive statistics.
8.2.2
Illustrations Using Regression Models
There are many problems where we can exploit structure in the state variable,
allowing us to propose functions characterized by a small number of parameters
that have to be estimated statistically. Section 8.1.4 represented one version where
we had a parameter for each (possibly aggregated) state. The only structure we
assumed was implicit in the ability to specify a series of one or more aggregation
functions.
The remainder of this section illustrates the use of regression models in speciﬁc
applications. The examples use a speciﬁc method for estimating the parameter
vector θ that will typically prove to be somewhat clumsy in practice.
Pricing an American Option
Consider the problem of determining the value of an American-style put option,
which gives us the right to sell an asset (or contract) at a speciﬁed price at any
of a set of discrete time periods. For example, we might be able to exercise the
option on the last day of the month over the next 12 months.
Suppose that we have an option that allows us to sell an asset at $1.20 at any of
four time periods. We assume a discount factor of 0.95 to capture the time value
of money. If we wait until time period 4, we must exercise the option, receiving
zero if the price is over $1.20. At intermediate periods, however, we may choose
to hold the option even if the price is below $1.20 (of course, exercising it if the
price is above $1.20 does not make sense). Our problem is to determine whether
to hold or exercise the option at the intermediate points.
From history we have found 10 samples of price trajectories, which are shown
in Table 8.2. If we wait until time period 4, our payoff is shown in Table 8.3,
which is zero if the price is above 1.20, and 1.20 −p4 for prices below $1.20.
At time t = 3, we have access to the price history (p1, p2, p3). Since we may
not be able to assume that the prices are independent or even Markovian (where
p3 depends only on p2), the entire price history represents our state variable, along
with an indicator that tells us if we are still holding the asset. We wish to predict
the value of holding the option at time t = 4. Let V4(S4) be the value of the option
if we are holding it at time 4, given the state (which includes the price p4) at time
4. Now let the conditional expectation at time 3 be
V3(S3) = E{V4(S4)|S3}.
Our goal is to approximate V3(S3) using information we know at time 3. We
propose a linear regression of the form
Y = θ0 + θ1X1 + θ2X2 + θ3X3,

308
approximating value functions
Table 8.2
Ten sample realizations of prices over four time periods
Stock Prices
Time Period
Outcome
1
2
3
4
1
1.21
1.08
1.17
1.15
2
1.09
1.12
1.17
1.13
3
1.15
1.08
1.22
1.35
4
1.17
1.12
1.18
1.15
5
1.08
1.15
1.10
1.27
6
1.12
1.22
1.23
1.17
7
1.16
1.14
1.13
1.19
8
1.22
1.18
1.21
1.28
9
1.08
1.11
1.09
1.10
10
1.15
1.14
1.18
1.22
Table 8.3
Payout at time 4 if we are still holding the option
Option Value at t = 4
Time Period
Outcome
1
2
3
4
1
—
—
—
0.05
2
—
—
—
0.07
3
—
—
—
0.00
4
—
—
—
0.05
5
—
—
—
0.00
6
—
—
—
0.03
7
—
—
—
0.01
8
—
—
—
0.00
9
—
—
—
0.10
10
—
—
—
0.00
where
Y = V4,
X1 = p2,
X2 = p3,
X3 = (p3)2.
The variables X1, X2, and X3 are our basis functions. Keep in mind that it is
important that our explanatory variables Xi be a function of information we have
at time t = 3, although we are trying to predict what will happen at time t = 4
(the payoff). We would then set up the data matrix given in Table 8.4.

parametric models
309
Table 8.4
Data table for our regression at time 3
Regression Data
Independent Variables
Dependent Variable
Outcome
X1
X2
X3
Y
1
1.08
1.17
1.3689
0.05
2
1.12
1.17
1.3689
0.07
3
1.08
1.22
1.4884
0.00
4
1.12
1.18
1.3924
0.05
5
1.15
1.10
1.2100
0.00
6
1.22
1.23
1.5129
0.03
7
1.44
1.13
1.2769
0.01
8
1.18
1.21
1.4641
0.00
9
1.11
1.09
1.1881
0.10
10
1.14
1.18
1.3924
0.00
We can now run a regression on these data to determine the parameters (θi)3
i=0.
It makes sense to consider only the paths that produce a positive value in the fourth
time period, since these represent the sample paths where we are most likely to still
be holding the asset at the end. The linear regression is only an approximation,
and it is best to ﬁt the approximation in the region of prices that are the most
interesting (we could use the same reasoning to include some “near misses”). We
only use the value function to estimate the value of holding the asset, so it is this
part of the function we wish to estimate. For our illustration, however, we use all
10 observations, which produces the equation
V3 ≈0.0056 −0.1234p2 + 0.6011p3 −0.3903(p3)2.
V3 is an approximation of the expected value of the price that we would receive if
we hold the option until time period 4. We can now use this approximation to help
us decide what to do at time t = 3. Table 8.5 compares the value of exercising
the option at time 3 against holding the option until time 4, computed as γV3(S3).
Taking the larger of the two payouts, we ﬁnd, for example, that we would hold the
option given samples 1–4, 6, 8, and 10 but would sell given samples 5, 7, and 9.
We can repeat the exercise to estimate V2(St). This time our dependent variable
Y can be calculated two different ways. The simplest is to take the larger of the
two columns from Table 8.5 (marked in bold). So for sample path 1 we would
have Y1 = max{0.03, 0.03947} = 0.03947. This means that our observed value is
actually based on our approximate value function V3(S3).
An alternative way of computing the observed value of holding the option in
time 3 is to use the approximate value function to determine the decision, but then
use the actual price we receive when we eventually exercise the option. Using this
method, we receive 0.05 for the ﬁrst sample path because we decide to hold the
asset at time 3 (based on our approximate value function) after which the price of

310
approximating value functions
Table 8.5
Payout if we exercise at time 3, and the expected value of holding based
on our approximation
Rewards
Decision
Outcome
Exercise
Hold
1
0.03
0.04155 × .95 = 0.03947
2
0.03
0.03662 × .95 = 0.03479
3
0.00
0.02397 × .95 = 0.02372
4
0.02
0.03346 × .95 = 0.03178
5
0.10
0.05285 × .95 = 0.05021
6
0.00
0.00414 × .95 = 0.00394
7
0.07
0.00899 × .95 = 0.00854
8
0.00
0.01610 × .95 = 0.01530
9
0.11
0.06032 × .95 = 0.05731
10
0.02
0.03099 × .95 = 0.02944
aThe best decision is indicated in bold.
the option turns out to be worth 0.05. Discounted, this is worth 0.0475. For sample
path 2 the option proves to be worth 0.07, which discounts back to 0.0665 (we
decided to hold at time 3, and the option was worth 0.07 at time 4). For sample
path 5 the option is worth 0.10 because we decided to exercise at time 3.
Regardless of which way we compute the value of the problem at time 3, the
remainder of the procedure is the same. We have to construct the independent
variables Y and regress them against our observations of the value of the option
at time 3 using the price history (p1, p2). Our only change in methodology would
occur at time 1 where we would have to use a different model (because we do not
have a price at time 0).
Playing “Lose Tic-Tac-Toe”
The game of “lose tic-tac-toe” is the same as the familiar game of tic-tac-toe, with
the exception that now you are trying to make the other person get three in a row.
This nice twist on the popular children’s game provides the setting for our next
use of regression methods in approximate dynamic programming.
Unlike our exercise in pricing options, representing a tic-tac-toe board requires
capturing a discrete state. Assume the cells in the board are numbered left to right,
top to bottom as shown in Figure 8.5a. Now consider the board in Figure 8.5b.
We can represent the state of the board after the tth play using
Sti =



1
if cell i contains an “X,”
0
if cell i is blank,
−1
if cell i contains an “O,”
St = (Sti)9
i=1.

parametric models
311
1
2
3
4
5
6
7
8
9
X
X
O
O
O
X
O
Figure 8.5
Some tic-tac-toe boards. (8.5a) Our indexing scheme; (8.5b) sample board.
We see that this simple problem has up to 39 = 19,683 states. While many of these
states will never be visited, the number of possibilities is still quite large, and seems
to overstate the complexity of the game.
We quickly realize that what is important about a game board is not the status
of every cell as we have represented it. For example, rotating the board does not
change a thing, but it does represent a different state. Also we tend to focus on
strategies (early in the game when it is more interesting) such as winning the center
of the board or a corner. We might start deﬁning variables (basis functions) such as
φ1(St) = 1 if there is an “X” in the center of the board, 0 otherwise,
φ2(St) = number of corner cells with an “X,”
φ3(St) = number of instances of adjacent cells with an “X” (horizontally,
vertically, or diagonally).
There are, of course, numerous such functions we can devise, but it is unlikely that
we could come up with more than a few dozen (if that) that appear to be useful.
It is important to realize that we do not need a value function to tell us to make
obvious moves.
Once we form our basis functions, our value function approximation is given by
Vt(St) =

f ∈F
θtf φf (St).
We note that we have indexed the parameters by time (the number of plays) since
this might play a role in determining the value of the feature being measured by
a basis function, but it is reasonable to try ﬁtting a model where θtf = θf . We
estimate the parameters θ by playing the game (and following some policy) after
which we see if we won or lost. We let Y n = 1 if we won the nth game, 0 otherwise.
This also means that the value function is trying to approximate the probability of
winning if we are in a particular state.
We may play the game by using our value functions to help determine a policy.
Another strategy, however, is simply to allow two people (ideally, experts) to play
the game and use this to collect observations of states and game outcomes. This
is an example of supervisory learning. If we lack a “supervisor” then we have to
depend on simple strategies combined with the use of slowly learned value function
approximations. In this case we also have to recognize that in the early iterations

312
approximating value functions
we are not going to have enough information to reliably estimate the coefﬁcients
for a large number of basis functions.
8.2.3
A Geometric View of Basis Functions∗
For readers comfortable with linear algebra, we can obtain an elegant perspective
on the geometry of basis functions. In Section 8.2.1 we found the parameter vector
θ for a regression model by minimizing the expected square of the errors between
our model and a set of observations. Assume now that we have a “true” value
function V (s) that gives the value of being in state s, and let p(s) be the probability
of visiting state s. We wish to ﬁnd the approximate value function that best ﬁts
V (s) using a given set of basis functions (φf (s))f ∈F. If we minimize the expected
square of the errors between our approximate model and the true value function,
we would want to solve
min
θ
F(θ) =

s∈S
p(s)

V (s) −

f ∈F
θf φf (s)


2
,
(8.21)
where we have weighted the error for state s by the probability of actually being in
state s. Our parameter vector θ is unconstrained, so we can ﬁnd the optimal value
by taking the derivative and setting this equal to zero. Differentiating with respect
to θf ′ gives
∂F(θ)
∂θf ′
= −2

s∈S
p(s)

V (s) −

f ∈F
θf φf (s)

φf ′(s).
Setting the derivative equal to zero and rearranging gives

s∈S
p(s)V (s)φf ′(s) =

s∈S
p(s)

f ∈F
θf φf (s)φf ′(s).
(8.22)
At this point it is much more elegant to revert to matrix notation. Deﬁne an |S| × |S|
diagonal matrix D where the diagonal elements are the state probabilities p(s), as
follows:
D =


p(1)
0
...
0
0
p(2)
0
...
. . .
0
0
...
p(|S|)

.
Let V be the column vector giving the value of being in each state
V =


V (1)
V (2)
...
V (|S|)

.

parametric models
313
Finally, let  be an |S| × |F| matrix of the basis functions given by
 =


φ1(1)
φ1(2)
...
φ1(|S|)
φ2(1)
φ2(2)
...
φ2(|S|)
. . .
φ|F|(1)
φ|F|(2)
...
φ|F|(|S|)

.
Recognizing that equation (8.22) is for a particular feature f ′, with some care it is
possible to see that equation (8.22) for all features is given by the matrix equation
T DV = T Dθ.
(8.23)
It helps to keep in mind that  is an |S| × |F| matrix, D is an |S| × |S| diagonal
matrix, V is an |S| × 1 column vector, and θ is an |F| × 1 column vector. The
reader should carefully verify that (8.23) is the same as (8.22).
Now pre-multiply both sides of (8.23) by (T D)−1. This gives us the optimal
value of θ as
θ = (T D)−1T DV.
(8.24)
This equation is closely analogous to the normal equations of linear regression,
given by equation (8.20), with the only difference being the introduction of the
scaling matrix D that captures the probability that we are going to visit a state.
Now pre-multiply both sides of (8.24) by , which gives
θ = V = (T D)−1T DV.
θ is, of course, our approximation of the value function, which we have denoted
by V. This, however, is the best possible value function given the set of functions
φ = (φf )f ∈F. If the vector φ formed a complete basis over the space formed by
the value function V (s) and the state space S, then we would obtain θ = V = V .
Since this is generally not the case, we can view V as the nearest point projection
(where “nearest” is deﬁned as a weighted measure using the state probabilities p(s))
onto the space formed by the basis functions. In fact, we can form a projection
operator  deﬁned by
 = (T D)−1T D
so that V = V is the value function closest to V that can be produced by the set
of basis functions.
This discussion brings out the geometric view of basis functions (and at the
same time, the reason why we use the term “basis function”). There is an extensive
literature on basis functions that has evolved in the approximation literature.

314
approximating value functions
8.3
REGRESSION VARIATIONS
Regression comes in many ﬂavors. This section serves primarily as a reminder that
there is more to regression than classical linear regression.
8.3.1
Shrinkage Methods
A common problem in regression is estimates of the regression parameters that are
nonzero, but where the corresponding explanatory variable does not provide any
predictive value. An effective strategy is to add a term to the objective function
that penalizes nonzero regression parameters in some way. Ridge regression does
this by penalizing the sum of squares of the regression coefﬁcients, giving us the
objective function ridge regression
Y(x) = min
θ
 n

i=1
(yi −θT xi)2 + η
p

i=1
θ2
i

.
(8.25)
A limitation of ridge regression is that it can still produce small but nonzero
regression parameters. A variant of the LASSO method minimizes the weighted
sum of regression coefﬁcients, lasso regression:
Y(x) = min
θ
 n

i=1
(yi −θT xi)2 + η
p

i=1
|θi|

.
(8.26)
These methods can play a role in feature selection. Using the language of basis
functions, we can write an independent variable xf = φf (S). Now imagine that
we create a large number of basis functions (independent variables), but we are not
sure if all of them are useful. Shrinkage methods can be used to help identify the
basis functions that are most important.
8.3.2
Support Vector Regression
Support vector machines (for classiﬁcation) and support vector regression (for
continuous problems) have attracted considerable interest in the machine learn-
ing community. For the purpose of ﬁtting value function approximations, we are
primarily interested in support vector regression, but we can also use regression
to ﬁt policy function approximations, and if we have discrete actions, we may be
interested in classiﬁcation. For the moment we focus on ﬁtting continuous functions.
Support vector regression, in its most basic form, is linear regression with a
different objective than simply minimizing the sum of the squares of the errors.
With support vector regression, we consider two goals. First, we wish to minimize
the absolute sum of deviations that are larger than a set amount ξ. Second, we wish
to minimize the regression parameters, to push as many as possible close to zero.
As before, we let our predictive model be given by
y = θx + ϵ.

regression variations
315
i
y
x
Figure 8.6
Penalty structure for support vector regression. Deviations within the gray area are assessed
a value of zero. Deviations outside the gray area are measured based on their distance to the gray area.
Let ϵi = yi −θxi be the error. We then choose θ by solving the following opti-
mization problem:
min
θ

η
2∥θ∥2 +
n

i=1
max{0, |ϵi| −ξ}

.
(8.27)
The ﬁrst term penalizes positive values of θ, encouraging the model to minimize
values of θ unless they contribute in a signiﬁcant way to producing a better model.
The second term penalizes errors that are greater than ξ. The parameters η and
ξ are both tunable parameters. The error ϵi and error margin ξ are illustrated in
Figure 8.6.
It can be shown by solving the dual that the optimal value of θ and the best ﬁt
Y(x) have the form
θ =
n

i=1
(β
i −αi)xi,
Y(x) =
n

i=1
(β
i −αi)(xi)T xi.
Here β
i and αi are scalars found by solving
min
βi,αi ξ
n

i=1
(β
i + αi) −
n

i=1
yi(β
i + αi) + 1
2
n

i=1
n

i′=1
(β
i + αi)(β
i′ + αi′)(xi)T xi

316
approximating value functions
subject to the constraints
0 ≤αi, β
i ≤1
η,
n

i=1
(β
i −αi) = 0,
αiβ
i = 0.
A very rich ﬁeld of study has evolved around support vector machines and
support vector regression. For a thorough tutorial, see Smola and Sch¨olkopf (2004).
A shorter and more readable introduction is contained in chapter 12 of Hastie et
al. (2009). Note that SVR does not lend itself readily to recursive updating, which
we suspect will limit its usefulness in approximate dynamic programming.
8.4
NONPARAMETRIC MODELS
The power of parametric models is matched by their fundamental weakness: they
are only effective if you can design an effective parametric model, and this remains
a frustrating art. For this reason nonparametric statistics have attracted recent atten-
tion. They avoid the art of specifying a parametric model but introduce other com-
plications. Nonparametric methods work primarily by building local approximations
to functions using observations rather than depending on functional approximations.
There is an extensive literature on the use of approximation methods for contin-
uous functions. These problems, which arise in many applications in engineering
and economics, require the use of approximation methods that can adapt to a
wide range of functions. Interpolation techniques, orthogonal polynomials, Fourier
approximations and splines are just some of the most popular techniques. Often
these methods are used to closely approximate the expectation without the use of
Monte Carlo methods. We do not cover these techniques in this book, but instead
refer readers to Chapter 6 of Judd (1998) for a very nice review of approxima-
tion methods (see also chapter 12 of the same volume for their use in dynamic
programming).
In this section we review some of the nonparametric methods that have received
the most attention within the approximate dynamic programming community. This
is an active area of research that offers tremendous potential, but signiﬁcant hurdles
remain before this approach can be widely adopted.
8.4.1
k-Nearest Neighbor
Perhaps the simplest form of nonparametric regression forms estimates of functions
by using a weighted average of the k-nearest neighbors. As above, we assume we
have a response yn corresponding to a measurement xn = (xn
1, xn
2, . . . , xn
I ). Let
ρ(x, xn) be a distance metric between a query point x (in dynamic programming,

nonparametric models
317
this would be a state), and an observation xn. Then let Nn(x) be the set of the
k-nearest points to the query point x, where clearly we require k ≤n. Finally, let
Y
n(x) be the response function, which is our best estimate of the true function Y (x)
given the observations x1, . . . , xn. When we use a k-nearest neighbor model, this
is given by
Y
n(x) = 1
k

n∈Nn(x)
yn.
(8.28)
Thus our best estimate of the function Y (x) is made by averaging the k points
nearest to the query point x.
Using a k-nearest neighbor model requires, of course, choosing k. Not surpris-
ingly, we obtain a perfect ﬁt of the data by using k = 1 if we base our error on
the training dataset.
A weakness of this logic is that the estimate Y
n(x) can change abruptly as x
changes continuously, as the set of nearest neighbors changes. An effective way of
avoiding this behavior is using kernel regression, which uses a weighted sum of
all data points.
8.4.2
Kernel Regression
Kernel regression has attracted considerable attention in the statistical learning
literature. As with k-nearest neighbor, kernel regression forms an estimate Y(x) by
using a weighted sum of prior observations, which we can write generally as
Y
n(x) =
n
m=1 Kh(x, xm)ym
n
m=1 Kh(x, xm)
(8.29)
where Kh(x, xm) is a weighting function that declines with the distance between
the query point x and the measurement xm. h is referred to as the bandwidth which
plays an important scaling role. There are many possible choices for the weighting
function Kh(x, xm). One of the most popular is the Gaussian kernel, given by
Kh(x, xm) = e−(∥x−xm∥/h)2
.
where ∥· ∥is the Euclidean norm. Here h plays the role of the standard devia-
tion. Note that the bandwidth h is a tunable parameter that captures the range of
inﬂuence of a measurement xm. The Gaussian kernel, often referred to as radial
basis functions in the ADP literature, provide a smooth, continuous estimate Y
n(x).
Another popular choice of kernel function is the symmetric beta family, given by
Kh(x, xm) = max(0, (1 −∥x −xm∥)2)h.
Here h is a nonnegative integer. h = 1 gives the uniform kernel, h = 2 gives the
Epanechnikov kernel, and h = 3 gives the biweight kernel. Figure 8.7 illustrates
each of these four kernel functions.

318
approximating value functions
(a) Gaussian
(c) Epanechnikov
(d) Biweight
(b) Uniform
0
–.5
–1
–1.5
1.5
1
.5
0
.2
.4
.6
.8
1.0
0
–.5
–1
–1.5
1.5
1
.5
0
.2
.4
.6
.8
1.0
0
–.5
–1
–1.5
1.5
1
.5
0
.2
.4
.6
.8
1.0
0
–.5
–1
–1.5
1.5
1
.5
0
.2
.4
.6
.8
1.0
Figure 8.7
Gaussian, uniform, Epanechnikov, and biweight kernel weighting functions.
We pause to brieﬂy discuss some issues surrounding k-nearest neighbors and
kernel regression. First, it is fairly common in the ADP literature to see k-nearest
neighbors and kernel regression being treated as a form of aggregation. The process
of giving a set of states that are aggregated together has a certain commonality with
k-nearest neighbor and kernel regression, where points near each other will produce
estimates of Y (x) that are similar. But this is where the resemblance ends. Simple
aggregation is actually a form of parametric regression using dummy variables, and
it offers neither the continuous approximations nor the asymptotic unbiasedness of
kernel regression.
Kernel regression is a method of approximation that is fundamentally different
from linear regression and other parametric models. Parametric models use an
explicit estimation step, where each observation results in an update to a vector of
parameters. At any point in time, our approximation consists of the pre-speciﬁed
parametric model, along with the current estimates of the regression parameters.
With kernel regression all we do is store data until we need an estimate of the
function at a query point. Only then do we trigger the approximation method,
which requires looping over all previous observation, a step that clearly can become
expensive as the number of observations grow.

nonparametric models
319
Kernel regression enjoys an important property from an old result known as Mer-
cer’s theorem. The result states that there exists a set of basis functions φf (S), f ∈
F, possibly of very high dimensionality, where
Kh(S, S′) = φ(S)T φ(S′),
as long as the kernel function Kh(S, S′) satisﬁes some basic properties (satisﬁed
by the kernels listed above). In effect this means that using appropriately designed
kernels is equivalent to ﬁnding potentially very high dimensional basis functions,
without having to actually create them.
Unfortunately, the news is not all good. First, there is the annoying dimension
of bandwidth selection, although this can be mediated partially by scaling the
explanatory variables. More seriously, kernel regression (and this includes k-nearest
neighbors), cannot be immediately applied to problems with more than about ﬁve
dimensions (and even this can be a stretch). The problem is that these methods are
basically trying to aggregate points in a multidimensional space. As the number
of dimensions grows, the density of points in the d-dimensional space becomes
quite sparse, making it very difﬁcult to use “nearby” points to form an estimate
of the function. A strategy for high-dimensional applications is to use separable
approximations. These methods have received considerable attention in the broader
machine learning community but have not been widely tested in an ADP setting.
8.4.3
Local Polynomial Regression
Classical kernel regression uses a weighted sum of responses yn to form an estimate
of Y (x). An obvious generalization is to estimate locally linear regression models
around each point xn by solving a least squares problem that minimizes a weighted
sum of least squares. Let Y
n(x|xk) be a linear model around the point xk, formed
by minimizing the weighted sum of squares given by
min
θ
 n

m=1
Kh(xk, xm)(ym −(
I

i=1
θixm
i ))2

.
(8.30)
Thus we are solving a classical linear regression problem, but we do this for
each point xk, and we ﬁt the regression using all the other points (ym, xm), m =
1, . . . , n. However, we weight deviations between the ﬁtted model and each obser-
vation ym by the kernel weighting factor Kh(xk, xm), which is centered on the point
xk.
Local polynomial regression offers signiﬁcant advantages in modeling accuracy,
but with a signiﬁcant increase in complexity.
8.4.4
Neural Networks
Neural networks represent an unusually powerful and general class of approxima-
tion strategies that have been widely used in approximate dynamic programming,

320
approximating value functions
primarily in classic engineering applications. There are a number of excellent text-
books on the topic, so our presentation is designed only to introduce the basic idea
and encourage readers to experiment with this technology if simpler models are
not effective.
Up to now we have considered approximation functions of the form
V(S) =

f ∈F
θf φf (S),
where F is our set of features, and (φf (S))f ∈F are the basis functions that extract
what are felt to be the important characteristics of the state variable that explain
the value of being in a state. We have seen that when we use an approximation
that is linear in the parameters, we can estimate the parameters θ recursively using
standard methods from linear regression. For example, if Ri is the number of
resources of type i, our approximation might look like
V(R|θ) =

i∈I

θ1iRi + θ2iR2
i

.
Now assume that we feel that the best function might not be quadratic in Ri,
but we are not sure of the precise form. We might want to estimate a function of
the form
V(R|θ) =

i∈I

θ1iRi + θ2iRθ3
i

.
So we have a function that is nonlinear in the parameter vector (θ1, θ2, θ3), where
θ1 and θ2 are vectors and θ3 is a scalar. If we have a training dataset of state-value
observations, (ˆvn, Rn)N
n=1, we can ﬁnd θ by solving
min
θ
N

n=1

ˆvn −V(Rn|θ)
2 ,
which generally requires the use of nonlinear programming algorithms. One chal-
lenge is that nonlinear optimization problems do not lend themselves to the simple
recursive updating equations that we obtained for linear (in the parameters) func-
tions. But more problematic is that we have to experiment with various functional
forms to ﬁnd the one that ﬁts best.
Neural networks offer a much more ﬂexible set of architectures, and at the same
time can be updated recursively. The technology has matured to the point that there
are a number of commercial packages available that implement the algorithms.
However, applying the technology to speciﬁc dynamic programming problems can
be a nontrivial challenge. In addition it is not possible to know in advance which
problem classes will beneﬁt most from the additional generality, in contrast with
the simpler strategies that we have covered in this chapter.
Neural networks are ultimately a form of statistical model which, for our appli-
cation, predicts the value of being in a state as a function of the state, using a

nonparametric models
321
series of observations of states and values. The simplest neural network is nothing
more than a linear regression model. If we are in post-decision state Sa
t , we are
going to make the random transition St+1 = SM(St, at, Wt+1(ω)) and then observe
a random value ˆvt+1 from being in state St+1. We would like to estimate a statis-
tical function ft(Sa
t ) (the same as Vt(Sa
t )) that predicts ˆvt+1. To write this in the
traditional notation of regression, let Xt = Sa
t where Xt = (Xt1, Xt2, . . . , XtI) (we
assume that all the components Xti are numerical). If we use a linear model, we
might write
ft(Xt) = θ0 +
I

i=1
θiXti.
In the language of neural networks, we have I inputs (we have I + 1 parameters
since we also include a constant term), which we wish to use to estimate a single
output ˆvt+1 (a random observation of being in a state). The relationships are illus-
trated in Figure 8.8 where we show the I inputs that are then “ﬂowed” along the
links to produce ft(Xt). After this we learn the sample realization ˆvt+1 that we
were trying to predict, which allows us to compute the error ϵt+1 = ˆvt+1 −ft(Xt).
We would like to ﬁnd a vector θ that solves
min
θ
E 1
2(ft(Xt) −ˆvt+1)2.
Let
F(θ) = E

0.5(ft(Xt) −ˆvt+1)2
,
and
let
F(θ, ˆvt+1(ω)) = 0.5(ft(Xt) −
ˆvt+1(ω))2, where ˆvt+1(ω) is a sample realization of the random variable ˆvt+1(ω).
Xt1
Xtl
Xt2
Xt3
ft(Xt)
t+1
t+1
ˆv
t1
t2
t3
tl
Figure 8.8
Neural networks with a single layer.

322
approximating value functions
As before, we can solve this iteratively using a stochastic gradient algorithm
θt+1 = θt −αt∇θF(θt, ˆvt+1(ω)),
where ∇θF(θt, ˆvt+1(ω)) = ϵt+1.
We illustrated our linear model by assuming that the inputs were the individual
dimensions of the state variable which we denoted Xti. We may not feel that
this is the best way to represent the state of the system (imagine representing the
states of a Connect-4 game board). We may feel it is more effective (and certainly
more compact) if we have access to a set of basis functions φf (St), f ∈F, where
φf (St) captures a relevant feature of our system. In this case we would be using
our standard basis function representation, where each basis function provides one
of the inputs to our neural network.
This was a simple illustration, but it shows that if we have a linear model, we get
the same basic class of algorithms that we have already used. A richer model, given
in Figure 8.9, illustrates a more classical neural network. Here the “input signal”
Xt (this can be the state variable or the set of basis functions) is communicated
through several layers. Let X(1)
t
= Xt be the input to the ﬁrst layer (recall that Xti
might be the ith dimension of the state variable itself, or a basis function). Let I(1)
be the set of inputs to the ﬁrst layer (e.g., the set of basis functions).
Here the ﬁrst linear layer produces J outputs given by
Y (2)
tj
=

i∈I(1)
θ(1)
ij X(1)
ti ,
j ∈I(2).
Y (2)
tj
becomes the input to a nonlinear perceptron node that is characterized by
a nonlinear function that may dampen or magnify the input. A typical functional
Xt1
Xt2
Xt3
XtI
ft(Xt)
t+1
t+1
ˆv
Figure 8.9
Three-layer neural network.

nonparametric models
323
form for a perceptron node is the logistics function given by
σ(y) =
1
1 + e−βy ,
where β is a scaling coefﬁcient. The function σ(y) is illustrated in Figure 8.10.
σ(x) introduces nonlinear behavior into the communication of the “signal” Xt.
We next calculate
X(2)
ti = σ(Y (2)
ti ),
i ∈I(2)
and use X(2)
ti
as the input to the second linear layer. We then compute
Y (3)
tj
=

i∈I(2)
θ(2)
ij X(2)
ti ,
j ∈I(3).
Finally, we compute the single output using
ft =

i∈I(3)
θ(3)
i
X(3)
ti .
As before, ft is our estimate of the value of the input Xt. This is effectively our
value function approximation Vt(Sx
t ) that we update using the observation ˆvt+1.
We update the parameter vector θ = (θ(1), θ(2), θ(3)) using the same stochastic
gradient algorithms we used for a single layer network. The only difference is that
the derivatives have to capture the fact that changing θ(1), for example, impacts the
“ﬂows” through the rest of the network. The derivatives are slightly more difﬁcult
to compute, but the logic is basically the same.
Our presentation above assumes that there is a single output, which is to say
that we are trying to match a scalar quantity ˆvt+1, the observed value of being in
y
(y)
Figure 8.10
Illustrative logistics function for introducing nonlinear behavior into neural networks.

324
approximating value functions
a state. In some settings, ˆvt+1 might be a vector. For example, in Chapter 13 (see,
in particular, Section 13.1), we describe problems where ˆvt+1 is the gradient of a
value function, which of course would be multidimensional. In this case, ft would
also be a vector which would be estimated using
ftj =

i∈I(3)
θ(3)
ij X(3)
ti ,
j ∈I(4),
where |I(4)| is the dimensionality of the output layer (that is, the dimensionality of
ˆvt+1).
This presentation should be viewed as nothing more than a very simple illus-
tration of an extremely rich ﬁeld. The advantage of neural networks is that they
offer a much richer class of nonlinear functions (“nonlinear architectures” in the
language of neural networks) that can be trained in an iterative way, consistent
with the needs of approximate dynamic programming. This said, neural networks
are no panacea (a statement that can be made about almost anything). As with our
simple linear models, the updating mechanisms struggle with scaling (the units of
Xt and the units of ˆvt+1 may be completely different) that has to be handled by
the parameter vectors θ(ℓ). Neural networks typically introduce signiﬁcantly more
parameters that can also introduce problems with stability. There are signiﬁcant
opportunities for taking advantage of problem structure, which can be reﬂected
both in the design of the inputs (as with the choice of basis functions) as well as
the density of the internal networks (these do not have to be dense).
8.4.5
Indexed Functions, Tree Structures, and Clustering
There are many problems where we feel comfortable specifying a simple set of
basis functions for some of the parameters, but we do not have a good feel for
the nature of the contribution of other parameters. For example, we may wish to
plan how much energy to hold in storage over the course of the day. Let Rt be
the amount of energy stored at time t, and let Ht be the hour of the day. Our state
variable might be St = (Rt, Ht). We feel that the value of energy in storage is a
concave function in Rt, but this value depends in a complex way on the hour of day.
It would not make sense, for example, to specify a value function approximation
using
V(St) = θ0 + θ1Rt + θ2R2
T + θ3Ht + θ4H 2
t .
There is no reason to believe that the hour of day will be related to the value of
energy storage in any convenient way. Instead, we can estimate a function V(St|Ht)
given by
V(St|h) = θ0(h) + θ1(h)Rt + θ2(h)R2
T .
What we are doing here is estimating a linear regression model for each value of
h = Ht. This is simply a form of lookup table using regression given a particular

approximations and the curse of dimensionality
325
value of the complex variables. Imagine that we can divide our state variable St into
two sets: the ﬁrst set, ft, contains variables where we feel comfortable capturing
the relationship using linear regression. The second set, gt, includes more complex
variables whose contribution is not as easily approximated. If gt is a discrete scalar
(e.g., as hour of day), we can consider estimating a regression model for each value
of gt. However, if gt is a vector (possibly with continuous dimensions), then there
will be too many possible values.
When the vector gt cannot be enumerated, we can resort to various clustering
strategies. These fall under names such as regression trees and local polynomial
regression (a form of kernel regression). These methods cluster gt (or possibly
the entire state St) and then ﬁt simple regression models over subsets of data. In
this case we would create a set of clusters Cn based on n observations of states
and values. We then ﬁt a regression function V(St|c) for each cluster c ∈Cn. In
traditional batch statistics this process proceeds in two stages: clustering and then
ﬁtting. In approximate dynamic programming we have to deal with the fact that
we may change our clusters as we collect additional data.
A much more sophisticated strategy is based on a concept known as Dirichlet
process mixtures. This is a fairly sophisticated technique, but the essential idea
is that you form clusters that produce good ﬁts around local polynomial regres-
sions. However, unlike traditional cluster-then-ﬁt methods, the idea with Dirichlet
process mixtures is that membership in a cluster is probabilistic, where the prob-
abilities depend on the query point (e.g., the state whose value we are trying to
estimate).
8.5
APPROXIMATIONS AND THE CURSE OF DIMENSIONALITY
There are many applications where state variables have multiple, possibly contin-
uous dimensions. In some applications the number of dimensions can number in
the thousands.
■
EXAMPLE 8.5
An unmanned aerial vehicle may be described by location (three dimensions),
velocity (three dimensions), and acceleration (three dimensions), in addition to
fuel level. All 10 dimensions are continuous.
■
■
EXAMPLE 8.6
A utility is trying to plan the amount of energy that should be put in storage
as a function of the wind history (six hourly measurements), the history of
electricity spot prices (six measurements), and the demand history (six mea-
surements).
■

326
approximating value functions
■
EXAMPLE 8.7
A trader is designing a policy for selling an asset that is priced against a basket
of 20 securities, creating a 20-dimensional state variable.
■
■
EXAMPLE 8.8
A car rental company has to manage its inventory of 12 different types of cars
spread among 500 car rental locations, creating an inventory vector with 6000
dimensions.
■
Each of these problems has a multidimensional state vector, and in all but the last
example the dimensions are continuous. In the car rental example, the inventories
will be discrete, but potentially fairly large (a major rental lot may have dozens of
each type of car).
If we have 10 dimensions, and discretize each dimension into 100 elements,
our state space is 10010 = 1020, which is clearly a very large number. A rea-
sonable strategy might be to aggregate. Instead of discretizing each dimension
into 100 elements, what if we discretize into 5 elements? Now our state space is
510 = 9.76 × 106, or almost 10 million states. Much smaller, but still quite large.
Figure 8.11 illustrates the growth in the state space with the number of dimensions.
Approximating high-dimensional functions is fundamentally intractable, and is
not related to the speciﬁc approximation strategy. If we use a second-order para-
metric representation, we might approximate a two-dimensional function using
V (S) ≈θ0 + θ1S1 + θ2S2 + θ11S2
1 + θ22S2
2 + θ12S1S2.
If we have N dimensions, the approximation would look like
V (S) ≈θ0 +
N

i=1
θiSi +
N

i1=1
N

i2=1
θijSiSj,
(a)
(b)
(c)
Figure 8.11
Effect of higher dimensions on the number of grids in an aggregated state space.

approximations and the curse of dimensionality
327
which means we have to estimate 1 + N + N2 parameters. As N grows, this grows
very quickly, and this is only a second-order approximation. If we allow N th-order
interactions, the approximation would look like
V (S) ≈θ0 +
N

i=1
θiSi +
N

i1=1
N

i2=1
θi1i2Si1Si2
+
N

i1=1
N

i2=1
· · ·
N

iN =1
θi1,i2,..., iN Si1Si2 · · · SiN .
The number of parameters we now have to estimate is given by 1 + N + N2 +
N3 + · · · + NN. Not surprisingly, this becomes intractable even for relatively small
values of N .
The problem follows us if we were to use kernel regression, where an estimate
of a function at a point s can be estimated from a series of observations (ˆvi, si)N
i=1
using
V (s) ≈
N
i=1 ˆvik(s, si)
N
i=1 k(s, si)
where k(s, si) might be the Gaussian kernel
k(s, si) = e−∥s−si∥2/b
where b is a bandwidth. Kernel regression is effectively a soft form of the aggre-
gation depicted in Figure 8.11c. The problem is that we would have to choose a
bandwidth that covers most of the data to get a statistically reliable estimate of a
single point.
To see this, imagine that our observations are uniformly distributed in an N -
dimensional cube that measures 1.0 on each side, which means it has a volume of
1.0. If we carve out an N -dimensional cube that measures 0.5 on a side, then this
would capture 12.5 percent of the observations in a three-dimensional cube, and 0.1
percent of the observations in a 10-dimensional cube. If we would like to choose
a cube that captures η = 0.1 of our cube, we would need a cube that measures
r = η1/N = 0.11/10 = 0.794, which means that our cube is covering almost 80
percent of the range of each input dimension.
The problem is that we have a multidimensional function, and we are trying
to capture the joint behavior of all N dimensions. If we are willing to live with
separable approximations, then we can scale to very large number of dimensions.
For example, the approximation
V (S) ≈θ0 +
N

i=1
θ1iSi +
N

i=1
θ2iS2
i ,
captures quadratic behavior but without any cross terms. The number of param-
eters is 1 + 2N, which means we may be able to handle very high-dimensional

328
approximating value functions
problems. However, we lose the ability to handle interactions among different
dimensions.
8.6
WHY DOES IT WORK?**
8.6.1
Correlations in Hierarchical Estimation
It is possible to derive the optimal weights for the case where the statistics v(g)
s
are
not independent. In general, if we are using a hierarchical strategy and have g′ > g
(which means that aggregation g′ is more aggregate than g), then the statistic v(g′,n)
s
is computed using observations ˆvn
s that are also used to compute v(g,n)
s
.
We begin by deﬁning
N(g,n)
s
= set of iterations n where Gg(ˆsn) = Gg(s) (i.e., ˆsn aggregates to the
same state as s),
N(g,n)
s
= |N(g,n)
s
|,
ε(g,n)
s
= estimate of the average error when observing state s = G(ˆsn)
=

n∈N(g,n)
s
ˆε(g,n)
s
/1/N(g,n)
s
.
The average error ε(g,n)
s
can be written
ε(g,n)
s
=
1
N (g,n)
s



n∈N(0,n)
s
εn +

n∈N(g,n)
s
\N(0,n)
s
εn


= N (0,n)
s
N (g,n)
s
ε(0)
s
+
1
N(g,n)
s

n∈N(g,n)
s
\N(0,n)
s
εn.
(8.31)
This relationship shows us that we can write the error term at the higher level
of aggregation g′ as a sum of a term involving the errors at the lower level of
aggregation g (for the same state s) and a term involving errors from other states
s′′ where Gg′(s′′) = Gg′(s), given by
ε(g′,n)
s
=
1
N(g′,n)
s



n∈N(g,n)
s
εn +

n∈N(g′,n)
s
\N(g,n)
s
εn


=
1
N(g′,n)
s

N(g,n)
s

n∈N(g,n)
s
εn
N(g,n)
s
+

n∈N(g′,n)
s
\N(g,n)
s
εn


= N (g,n)
s
N(g′,n)
s
ε(g,n)
s
+
1
N(g′,n)
s

n∈N(g′,n)
s
\N(g,n)
s
εn.
(8.32)

why does it work?
329
We can overcome this problem by rederiving the expression for the optimal
weights. For a given (disaggregate) state s, the problem of ﬁnding the optimal
weights (w(g,n)
s
)g∈G is stated by
min
w(g,n)
s
,g∈G
E


1
2


g∈G
w(g,n)
s
· v(g,n)
s
−ν(g,n)
s


2

(8.33)
subject to

g∈G
w(g,n)
s
= 1,
(8.34)
w(g,n)
s
≥0,
g ∈G.
(8.35)
Let
δ
(g,n)
s
= error in the estimate v(g,n)
s
from the true value associated with attribute
vector s,
= v(g,n)
s
−νs.
The optimal weights are computed using the following theorem:
Theorem 8.6.1
For a given state vector, s, the optimal weights, w(g,n)
s
, g ∈G,
where the individual estimates are correlated by way of a tree structure, are given
by solving the following system of linear equations in (w, λ):

g∈G
w(g,n)
s
E
4
δ
(g,n)
s
δ
(g′,n)
s
5
−λ = 0
∀g′ ∈G,
(8.36)

g∈G
w(g,n)
s
= 1,
(8.37)
w(g,n)
s
≥0
∀g ∈G.
(8.38)
Proof
The proof is not too difﬁcult, and it illustrates how we obtain the optimal
weights. We start by formulating the Lagrangian for the problem formulated in
(8.33)–(8.35), which gives us
L(w, λ) = E

1
2


g∈G
w(g,n)
s
· v(g,n)
s
−ν(g,n)
s


2
+ λ

1 −

g∈G
w(g,n)
s


= E

1
2


g∈G
w(g,n)
s

v(g,n)
s
−ν(g,n)
s



2
+ λ

1 −

g∈G
w(g,n)
s

.

330
approximating value functions
The ﬁrst order optimality conditions are
E


g∈G
w(g,n)
s

v(g,n)
s
−ν(g,n)
s
 
v(g′,n)
s
−ν(g,n)
s


−λ = 0
∀g′ ∈G,
(8.39)

g∈G
w(g,n)
s
−1 = 0.
(8.40)
To simplify equation (8.39), we note that
E


g∈G
w(g,n)
s

v(g,n)
s
−ν(g,n)
s
 
v(g′,n)
s
−ν(g,n)
s


= E


g∈G
w(g,n)
s
δ
(g,n)
s
δ
(g′,n)
s


=

g∈G
w(g,n)
s
E
4
δ
(g,n)
s
δ
(g′,n)
s
5
.
(8.41)
Combining equations (8.39) and (8.41) gives us equation (8.36) which completes
the proof.
□
Finding the optimal weights that handle the correlations among the statistics
at different levels of aggregation requires ﬁnding E
4
δ
(g,n)
s
δ
(g′,n)
s
5
. We are going
to compute this expectation by conditioning on the set of attributes ˆsn that are
sampled. This means that our expectation is deﬁned over the outcome space ε.
Let N (g,n)
s
be the number of observations of state s at aggregation level g. The
expectation is computed using:
Proposition 8.6.1
The coefﬁcients of the weights in equation (8.37) can be
expressed as follows:
E
4
δ
(g,n)
s
δ
(g′,n)
s
5
= E
4
β
(g,n)
s
β
(g′,n)
s
5
+ N(g,n)
s
N (g′,n)
s
E
4
ε(g,n)2
s
5
∀g ≤g′; g, g′ ∈G.
(8.42)
The proof is given in Section 8.6.2.
Now consider what happens when we make the assumption that the measurement
error εn is independent of the attribute being sampled, ˆsn. We do this by assuming
that the variance of the measurement error is a constant given by σε2. This gives
us the following result:
Corollary 8.6.1
For the special case where the statistical noise in the measure-
ment of the values is independent of the attribute vector sampled, equation (8.42)

why does it work?
331
reduces to
E
4
δ
(g,n)
s
δ
(g′,n)
s
5
= E
4
β
(g,n)
s
β
(g′,n)
s
5
+
σ 2
ε
N(g′,n)
s
.
(8.43)
For the case where g = 0 (the most disaggregate level), we assume that β(0)
s
= 0,
which gives us
E
4
β
(0,n)
s
β
(g′,n)
s
5
= 0.
This allows us to further simplify (8.43) to obtain
E
4
δ
(0,n)
s
δ
(g′,n)
s
5
=
σ 2
ε
N(g′,n)
s
.
(8.44)
8.6.2
Proof of Proposition 8.6.1
We start by deﬁning
δ
(g,n)
s
= β
(g,n)
s
+ ε(g,n)
s
.
(8.45)
Equation (8.45) gives us
E
4
δ
(g,n)
s
δ
(g′,n)
s
5
= E
4
(β
(g,n)
s
+ ε(g,n)
s
)(β
(g′,n)
s
+ ε(g′,n)
s
)
5
= E
4
β
(g,n)
s
β
(g′,n)
s
+ β
(g′,n)
s
ε(g,n)
s
+ β
(g,n)
s
ε(g′,n)
s
+ ε(g,n)
s
ε(g′,n)
s
5
= E
4
β
(g,n)
s
β
(g′,n)
s
5
+ E
4
β
(g′,n)
s
ε(g,n)
s
5
+ E
4
β
(g,n)
s
ε(g′,n)
s
5
+ E
4
ε(g,n)
s
ε(g′,n)
s
5
.
(8.46)
We note that
E
4
β
(g′,n)
s
ε(g,n)
s
5
= β
(g′,n)
s
E

ε(g,n)
s

= 0.
Similarly
E
4
β
(g,n)
s
ε(g′,n)
s
5
= 0.
This allows us to write equation (8.46) as
E
4
δ
(g,n)
s
δ
(g′,n)
s
5
= E
4
β
(g,n)
s
β
(g′,n)
s
5
+ E
4
ε(g,n)
s
ε(g′,n)
s
5
.
(8.47)

332
approximating value functions
We start with the second term on the right-hand side of equation (8.47). This
term can be written as
E
4
ε(g,n)
s
ε(g′,n)
s
5
= E
/
ε(g,n)
s
· N(g,n)
s
N (g′)
s
ε(g,n)
s
0
+ E

ε(g,n)
s
·
1
N (g′)
s

n∈N(g′,n)
s
\N(g,n)
s
εn


= N (g,n)
s
N(g′)
s
E

ε(g,n)
s
ε(g,n)
s

+
1
N(g′)
s
E

ε(g,n)
s
·

n∈N(g′,n)
s
\N(g,n)
s
εn

. . .



I
.
The term I can be rewritten using
E

ε(g,n)
s
·

n∈N(g′,n)
s
\N(g,n)
s
εn

= E

ε(g,n)
s

E



n∈N(g′,n)
s
\N(g,n)
s
εn


= 0,
which means that
E
4
ε(g,n)
s
ε(g′,n)
s
5
= N(g,n)
s
N(g′)
s
E
4
ε(g)2
s
5
.
(8.48)
Combining (8.47) and (8.48) proves the proposition.
□
The second term on the right-hand side of equation (8.48) can be further sim-
pliﬁed using
E
4
ε(g)2
s
5
= E




1
N (g,n)
s

n∈N(g,n)
s
εn


2

∀g′ ∈G
=
1

N(g,n)
s
2

m∈N(g,n)
s

n∈N(g,n)
s
E

εmεn
=
1

N(g,n)
s
2

n∈N(g,n)
s
E

(εn)2
=
1

N(g,n)
s
2 N(g,n)
s
σε2
=
σ 2
ε
N(g,n)
s
.
(8.49)
Combining equations (8.42), (8.48), and (8.49) gives us the result in
equation (8.43).
□

bibliographic notes
333
8.7
BIBLIOGRAPHIC NOTES
This chapter is primarily a brief tutorial into statistical learning. Readers interested
in pursuing approximate dynamic programming should obtain a good statistical
reference such as Bishop (2006) and Hastie et al. (2009). The second reference can
be downloaded from http://www-stat.stanford.edu/∼tibs/ElemStatLearn/.
Sections 8.1 Aggregation has been a widely used technique in dynamic pro-
gramming as a method to overcome the curse of dimensionality. Early work
focused on picking a ﬁxed level of aggregation (Whitt, 1978; Bean et al.,
1987), or using adaptive techniques that change the level of aggregation as the
sampling process progresses (Bertsekas and Castanon, 1989, Mendelssohn,
1982; Bertsekas and Tsitsiklis, 1996), but that still use a ﬁxed level of aggre-
gation at any given time. Much of the literature on aggregation has focused
on deriving error bounds (Zipkin 1980a,b). A recent discussion of aggrega-
tion in dynamic programming can be found in Limbert et al. (2002). For
a good discussion of aggregation as a general technique in modeling, see
Rogers et al. (1991). The material in Section 8.1.4 is based on George et
al. (2008) and George and Powell (2006). LeBlanc and Tibshirani (1996)
and Yang (2001) provide excellent discussions of mixing estimates from dif-
ferent sources. For a discussion of soft state aggregation, see Singh et al.
(1995). Section 8.1.2 on bias and variance is based on George and Powell
(2006).
Section 8.2 Basis functions have their roots in the modeling of physical pro-
cesses. A good introduction to the ﬁeld from this setting is Heuberger et al.
(2005). Schweitzer and Seidmann (1985) describe generalized polynomial
approximations for Markov decision processes for use in value iteration, pol-
icy iteration, and the linear programming method. Menache et al. (2005)
discuss basis function adaptations in the context of reinforcement learning.
For a very nice discussion of the use of basis functions in approximate
dynamic programming, see Tsitsiklis and Roy (1996b) and Van Roy (2001).
Tsitsiklis and Van Roy (1997) prove convergence of iterative stochastic algo-
rithms for ﬁtting the parameters of a regression model when the policy is held
ﬁxed. For Section 8.2.2, the ﬁrst use of approximate dynamic programming
for evaluating an American call option is given in Longstaff and Schwartz
(2001), but the topic has been studied for decades (see Taylor (1967)). Tsit-
siklis and Van Roy (2001) also provide an alternative ADP algorithm for
American call options. Clement et al. (2002) provide formal convergence
results for regression models used to price American options. This presenta-
tion on the geometric view of basis functions is based on Tsitsiklis and Van
Roy (1997).
Section 8.3 There is, of course, an extensive literature on different statistical
methods. This section provides only a sampling. For a much more thorough
treatment, see Bishop (2006) and Hastie et al. (2009).

334
approximating value functions
Section 8.4 An excellent introduction to continuous approximation techniques
is given in Judd (1998) in the context of economic systems and computa-
tional dynamic programming. Ormoneit and Sen (2002) and Ormoneit and
Glynn (2002) discuss the use of kernel-based regression methods in an
approximate dynamic programming setting, providing convergence proofs
for speciﬁc algorithmic strategies. For a thorough introduction to locally
polynomial regression methods, see Fan and Gijbels (1996). An excellent
discussion of a broad range of statistical learning methods can be found in
Hastie et al. (2009). Bertsekas and Tsitsiklis (1996) provides an excellent
discussion of neural networks in the context of approximate dynamic pro-
gramming. Haykin (1999) presents a much more in-depth presentation of
neural networks, including a chapter on approximate dynamic programming
using neural networks.
Section 8.5 See Hastie et al. (2009, section 2.5), for a very nice discussion of
the challenges of approximating high-dimensional functions.
PROBLEMS
8.1
In a spreadsheet, create a 4 × 4 grid where the cells are numbered 1, 2,
. . . , 16 starting with the upper left-hand corner and moving left to right, as
shown in the lists below. We are going to treat each number in the cell as
the mean of the observations drawn from that cell. Now assume that if we
observe a cell, we observe the mean plus a random variable that is uniformly
distributed between −1 and +1. Next deﬁne a series of aggregations where
aggregation 0 is the disaggregate level, aggregation 1 divides the grid into
four 2 × 2 cells, and aggregation 2 aggregates everything into a single cell.
After n iterations, let v(g,n)
s
be the estimate of cell “s” at the nth level of
aggregation, and let
1
9
5
13
2
10
6
14
4
12
8
16
3
11
7
15
vn
s =

g∈G
w(g)
s v(g,n)
s
be your best estimate of cell s using a weighted aggregation scheme. Com-
pute an overall error measure using
(σ 2)n =

s∈S
(vn
s −νs)2,

problems
335
where νs is the true value (taken from your grid) of being in cell s. Also
let w(g,n) be the average weight after n iterations given to the aggregation
level g when averaged over all cells at that level of aggregation (e.g., there is
only one cell for w(2,n)). Perform 1000 iterations where at each iteration you
randomly sample a cell and measure it with noise. Update your estimates at
each level of aggregation, and compute the variance of your estimate with
and without the bias correction.
(a) Plot w(g,n) for each of the three levels of aggregation at each iteration.
Do the weights behave as you would expect? Explain.
(b) For each level of aggregation, set the weight given to that level equal
to one (in other words, use a single level of aggregation), and plot the
overall error as a function of the number of iterations.
(c) Add to your plot the average error when you use a weighted average,
where the weights are determined by equation (8.14) without the bias
correction.
(d) Finally add to your plot the average error when you used a weighted
average, but now determine the weights by equation (8.15), which uses
the bias correction.
(e) Repeat the above assuming that the noise is uniformly distributed
between −5 and +5.
8.2
Show that
σ 2
s = (σ 2
s )(g) + (β(g)
s )2,
(8.50)
which breaks down the total variation in an estimate at a level of aggregation
is the sum of the variation of the observation error plus the bias squared.
8.3
Show that E
2
θ
n−1 −θn23
= λn−1σ 2 + (βn)2. [Hint: Add and subtract
Eθ
n−1 inside the expectation and expand.]
8.4
Show
that
E
2
θ
n−1 −ˆθn23
= (1 + λn−1)σ 2 + (βn)2
(which
proves
equation 8.7). [Hint: See previous exercise.]
8.5
Derive the small sample form of the recursive equation for the variance
given in (8.8). Recall that if
θ
n = 1
n
n

m=1
ˆθm,
then an estimate of the variance of ˆθ is
Var[ ˆθ] =
1
n −1
n

m=1
( ˆθm −θ
n)2.

336
approximating value functions
8.6
Show that the matrix H n in the recursive updating formula from equation
(9.80),
θ
n = θ
n−1 −H nxnˆεn,
reduces to H n = 1/n for the case of a single parameter (which means we
are using Y =constant, with no independent variables).
8.7
A general aging and replenishment problem arises as follows: Let st be the
“age” of our process at time t. At time t, we may choose between a decision
d = C to continue the process, incurring a cost g(st) or a decision d = R
to replenish the process, which incurs a cost K + g(0). Assume that g(st)
is convex and increasing. The state of the system evolves according to
st+1 =

st + Dt
if d = C,
0
if d = R,
where Dt is a nonnegative random variable giving the degree of deterioration
from one epoch to another (also called the “drift”).
(a) Prove that the structure of this policy is monotone. Clearly state the
conditions necessary for your proof.
(b) How does your answer to part (1) change if the random variable Dt
is allowed to take on negative outcomes? Give the weakest possible
conditions on the distribution of required to ensure the existence of a
monotone policy.
(c) Now assume that the action is to reduce the state variable by an amount
q ≤st at a cost of cq (instead of K). Further assume that g(s) = as2.
Show that this policy is also monotone. Say as much as you can about
the speciﬁc structure of this policy.

C H A P T E R
9
Learning Value Function
Approximations
In Chapter 6 we described a number of different ways of constructing a policy.
One of the most important ways, and the way that is most widely associated with
the term “approximate dynamic programming,” requires approximating the value of
being in a state. Chapter 8 described a number of different approximation strategies,
including lookup table, aggregated functions, parametric models and nonparametric
models. All of these statistical models are created by generating a state Sn, next
computing some observation of a value ˆvn of being in state Sn, and ﬁnally using
the pair (Sn, ˆvn) to estimate (or update the estimate) of the value of being in a
state.
In this chapter we focus primarily on the different ways of calculating ˆvn and
then use this information to estimate a value function approximation, for a ﬁxed
policy. To emphasize that we are computing values for a ﬁxed policy, we index
parameters such as the value function V π by the policy π. After we establish the
fundamentals for estimating the value of a policy, we address in Chapter 10 the
last step of searching for good policies.
9.1
SAMPLING THE VALUE OF A POLICY
At ﬁrst glance the problem of statistically estimating the value of a ﬁxed policy
should not be any different than estimating a function from noisy observations. We
start by showing that from one perspective this is correct. However, the context
of dynamic programming, even when we ﬁx a policy, introduces opportunities and
challenges as we realize that we can take advantage of the dynamics of information
that may arise in both ﬁnite and inﬁnite horizon settings.
Approximate Dynamic Programming: Solving the Curses of Dimensionality, Second Edition.
Warren B. Powell.
2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
337

338
learning value function approximations
9.1.1
Direct Policy Evaluation for Finite Horizon Problems
Imagine that we have a ﬁxed policy Aπ(s) (or Xπ(s) if we are working with vector-
valued decisions). The policy may take any of the forms described in Chapter 6.
For iteration n, if we are in state Sn
t at time t, we then choose action an
t = Aπ(Sn
t ),
after which we sample the exogenous information W n
t+1. We sometimes say that
we are following sample path ωn from which we observe W n
t+1 = Wt+1(ωn). The
exogenous information W n
t+1 may depend on both Sn
t and the action an
t . From this
we may compute our contribution (cost if we are minimizing) from
ˆCn
t = C(Sn
t , an
t , W n
t+1).
Finally, we compute our next state from our transition function
Sn
t+1 = SM(Sn
t , an
t , W n
t+1).
This process continues until we reach the end of our horizon T. The basic algorithm
is described in Figure 9.1. In step 6 we use a batch routine to ﬁt a statistical model.
It is often more natural to use some sort of recursive procedure and embed the
updating of the value function within the iterative loop. The type of recursive
procedure depends on the nature of the value function approximation. Later in this
chapter we describe several recursive procedures if we are using linear regression.
Finite horizon problems are sometimes referred to as episodic, where an episode
refers to a simulation of a policy until the end of the horizon (also known as trials).
However, the term “episodic” can also be interpreted more broadly. For example,
an emergency vehicle may repeatedly return to base where the system then restarts.
Step 0. Initialization.
Step 0a. Initialize V
0.
Step 0b. Initialize S1.
Step 0c. Set n = 1.
Step 1. Choose a sample path ωn.
Step 2. Choose a starting state Sn
0.
Step 3. Do for t = 0, 1, . . . , T:
Step 3a. an
t = Aπ(Sn
t ).
Step 3b. ˆCn
t = C(Sn
t , an
t ).
Step 3c. W n
t+1 = Wt+1(ωn).
Step 3d. Sn
t+1 = SM(Sn
t , an
t , W n
t+1).
Step 4. Compute ˆvn
0 = T
t=0 γ t ˆCn
t .
Step 5. Increment n. If n≤N , go to step 1.
Step 6. Use the sequence of state-value pairs (Si, ˆvi)N
i=1 to ﬁt a value function approximation
V
π(s).
Figure 9.1
Basic policy approximation method.

sampling the value of a policy
339
Each cycle of starting from a home base and then returning to the home base can be
viewed as an episode. As a result, if we are working with a ﬁnite horizon problem,
we prefer to refer to these speciﬁcally as such.
Evaluating a ﬁxed policy is mathematically equivalent to making unbiased obser-
vations of a noisy function. Fitting a functional approximation is precisely what
the entire ﬁeld of statistical learning has been trying to do for decades. If we are
ﬁtting a linear model, then there are some powerful recursive procedures that can
be used. These are discussed below.
9.1.2
Policy Evaluation for Inﬁnite Horizon Problems
Not surprisingly, inﬁnite horizon problems introduce a special complication, since
we cannot obtain an unbiased observation in a ﬁnite number of measurements.
Below we present some methods that have been used for inﬁnite horizon
applications.
Recurrent Visits
There are many inﬁnite horizon problems where the system resets itself periodically.
A simple example of this is a ﬁnite horizon problem where hitting the end of the
horizon and starting over (as would occur in a game) can be viewed as an episode.
A different example is a queueing system where the admission of patients to an
emergency room must be managed. From time to time the queue may become
empty, at which point the system resets and starts over. For such systems it makes
sense to estimate the value of following a policy π when starting from the base
state.
Even if we do not have such a renewal system, imagine that we ﬁnd ourselves
in a state s. Now follow a policy π until we re-enter state s again. Let Rn(s) be
the reward earned, and let τ n(s) be the number of time periods required before
re-entering state s. Here n is counting the number of times we visit state s. An
observation of the average reward earned when in state s and following policy π
would be given by
ˆvn(s) = Rn(s)
τ n(s) .
ˆvn(s) would be computed when we return to state s. We might then update the
average value of being in state s using
vn(s) = (1 −αn−1)vn−1(s) + αn−1 ˆvn(s).
Note that as we make each transition from some state s′ to some state s′′, we are
accumulating rewards in R(s) for every state s that we have visited prior to reaching
state s′. Each time we arrive at some state s′′, we stop accumulating rewards for
s′′ and compute ˆvn(s′′), and then we smooth this into the current estimate of v(s′′).
Note that we have presented this only for the case of computing the average reward
per time period.

340
learning value function approximations
Partial Simulations
While we may not be able to simulate an inﬁnite trajectory, we may simulate a
long trajectory T, long enough to ensure that we are producing an estimate that
is “good enough.” When we are using discounting, we realize that eventually γ t
becomes small enough that a longer simulation does not really matter. This idea
can be implemented in a relatively simple way.
Consider the algorithm in Figure 9.1, and insert the calculation in step 3:
ct = t −1
t
ct−1 + 1
t
ˆCn
t .
ct is an average over the time periods of the contribution per time period. As we
follow our policy over progressively more time periods, ct approaches an average
contribution per time period. Over an inﬁnite horizon we would expect to ﬁnd
ˆvn
0 = lim
t→∞
∞

t=0
γ t ˆCn
t =
1
1 −γ c∞.
Now suppose that we only progress T time periods, and let cT be our estimate of
c∞at this point. We would expect that
ˆvn
0(T ) =
T

t=0
γ t ˆCn
t
≈1 −γ T +1
1 −γ
cT .
(9.1)
The error between our T-period estimate ˆvn
0(T ) and the inﬁnite horizon estimate
ˆvn
0 is given by
δn
T =
1
1 −γ c∞−1 −γ T +1
1 −γ
cT
≈
1
1 −γ cT −1 −γ T +1
1 −γ
cT
= γ T +1
1 −γ cT .
Thus we just have to ﬁnd T to make δT small enough. This strategy is embedded
in some optimal algorithms that only require δn
T →0 as n →∞(meaning that we
have to steadily allow T to grow).
Inﬁnite Horizon Projection
The analysis above leads to another idea that has received considerable atten-
tion in the approximate dynamic programming community under the name least
squares temporal differencing (LSTD), although we present it here with a some-
what different development. We can easily see from (9.1) that if we stop after

sampling the value of a policy
341
T time periods, we will underestimate the inﬁnite horizon contribution by a fac-
tor 1 −γ T +1. Assuming that T is reasonably large (e.g., γ T +1 < 0.1), we might
introduce the correction
ˆvn
0 =
1
1 −γ T +1 ˆvn
0(T ).
In essence we are taking a sample estimate of a T-period path, and projecting it
out over an inﬁnite horizon.
9.1.3
Temporal Difference Updates
Suppose that we are in state Sn
t
and we make decision an
t
(using policy
π), after which we observe the information Wt+1
that puts us in state
Sn
t+1 = SM(Sn
t , an
t , W n
t+1). The contribution from this transition is given by
C(Sn
t , an
t ) (or C(Sn
t , an
t , W n
t+1) if the contribution depends on the outcome Wt+1).
Imagine now that we continue this until the end of our horizon T. For simplicity,
we are going to drop discounting. In this case the contribution along this path
would be
ˆvn
t = C(Sn
t , an
t ) + C(Sn
t+1, an
t+1) + · · · + C(Sn
T , an
T ).
(9.2)
This is the contribution from following the path produced by a combination of
the information from outcome ωn (which determines W n
t+1, W n
t+2, . . . , W n
T ) and
policy π. ˆvn
t is an unbiased sample estimate of the value of being in state St and
following policy π over sample path ωn. We can use a stochastic gradient algorithm
to estimate the value of being in state St:
V
n
t (Sn
t ) = V
n−1
t
(Sn
t ) −αn

V
n−1
t
(Sn
t ) −ˆvn
t

.
(9.3)
We can obtain a richer class of algorithms by breaking down our path cost in (9.2)
by using
ˆvn
t =
T

τ=t
C(Sn
τ , an
τ , W n
τ+1)
−
 T

τ=t

V
n−1
τ
(Sτ) −V
n−1
τ+1(Sτ+1)

+

V
n−1
t
(St) −V
n−1
T +1(ST +1)

.



=0
We now use the fact that V
n−1
T +1(ST +1) = 0 (this is where our ﬁnite horizon model
is useful). Rearranging gives
ˆvn
t = V
n−1
t
(St) +
T

τ=t

C(Sn
τ , an
τ , W n
τ+1) +V
n−1
τ+1(Sτ+1) −V
n−1
τ
(Sτ)

.

342
learning value function approximations
Let
δπ
τ = C(Sn
τ , an
τ , W n
τ+1) +V
n−1
τ+1(Sn
τ+1) −V
n−1
τ
(Sn
τ ).
(9.4)
The terms δπ
τ are called temporal differences. They are indexed by π because they
depend on the particular policy for choosing actions. Let ˆvn
t = C(Sn
t , an
t , W n
t+1) +
V
n−1
t+1 (Sn
t+1) be our sample observation of being in state St, while V
n−1
t
(St) is our
current estimate of the value of being in state St. This means that the temporal
difference at time t, δπ
t = ˆvn
t −V
n−1
t
(St), is the difference in our estimate of the
value of being in state St between our current estimate and the updated estimate.
The temporal difference is a sample realization of what is also known as the
Bellman error.
Using equation (9.4), we can write ˆvn
t in the more compact form:
ˆvn
t = V
n−1
t
(St) +
T

τ=t
δπ
τ .
(9.5)
Substituting (9.5) into (9.3) gives
V
n
t (St) = V
n−1
t
(St) −αn−1
/
V
n−1
t
(St) −

V
n−1
t
(St) +
T

τ=t
δπ
τ
0
= V
n−1
t
(St) + αn−1
T −1

τ=t
δπ
τ .
(9.6)
We next use this bit of algebra to build an important class of updating mechanisms
for estimating value functions.
9.1.4
TD(λ)
The temporal differences δπ
τ are the errors in our estimates of the value of being in
state Sτ. We can think of each term in equation (9.6) as a correction to the estimate
of the value function. It makes sense that updates farther along the path should not
be given as much weight as those earlier in the path. As a result it is common to
introduce an artiﬁcial discount factor λ, producing updates of the form
V
n
t (St) = V
n−1
t
(St) + αn−1
T

τ=t
λτ−tδπ
τ .
(9.7)
We derived this formula without a time discount factor. We leave as an exercise
to the reader to show that if we have a time discount factor γ , then the temporal-
difference update becomes
V
n
t (St) = V
n−1
t
(St) + αn−1
T

τ=t
(γ λ)τ−tδπ
τ .
(9.8)

sampling the value of a policy
343
Equation (9.8) shows that the discount factor γ , which is typically viewed as
capturing the time value of money, and the algorithmic discount λ, which is a
purely algorithmic device, have exactly the same effect. Not surprisingly, modelers
in operations research have often used a discount factor γ set to a much smaller
number than would be required to capture the time value of money. Artiﬁcial
discounting allows us to look into the future, but then we discount the results when
we feel that the results are not perfectly accurate. Note that our use of λ in this
setting is precisely equivalent to our discounting (also using λ) when we presented
discounted rolling horizon policies in Section 6.2.4.
Updates of the form given in equation (9.7) produce an updating procedure that
is known as TD(λ) (or, temporal-difference learning with discount λ). We have
seen this form of discounting in Section 6.2.3, where we introduced λ as a form
of algorithmic discounting.
The updating formula in equation (9.7) requires that we step all the way to the
end of the horizon before updating our estimates of the value. There is, however,
another way of implementing the updates. The temporal differences δπ
τ are com-
puted as the algorithm steps forward in time. As a result our updating formula can
be implemented recursively. Say we are at time t′ in our simulation. We would
simply execute
V
n
t (Sn
t ) := V
n−1
t
(St) + αn−1λt′−tδπ
t′
for all t ≤t′.
(9.9)
Here our notation “: = ” means that we take the current value of V
n−1
t
(St), add
αn−1λt′−tδπ
t′ to it to obtain an updated value of V
n
t (St). When we reach time t′ = T ,
our value functions would have undergone a complete update. We note that at time
t′, we need to update the value function for every t ≤t′.
9.1.5
TD(0) and Approximate Value Iteration
An important special case of TD(λ) occurs when we use λ = 0. In this case
V
n
t (Sn
t ) = V
n−1
t
(Sn
t ) + αn−1

C(Sn
t , an
t ) + γV
n−1(SM(Sn
t , an
t , W n
t+1))
−V
n−1
t
(Sn
t )

.
(9.10)
Now consider value iteration. In Chapter 3, where we did not have to deal with
Monte Carlo samples and statistical noise, value iteration (for a ﬁxed policy) looked
like
V n
t (s) = C(s, Aπ(s)) + γ

s′∈S
pπ(s′|s)V n
t+1(s′).
In steady state we would write it as
V n(s) = C(s, Aπ(s)) + γ

s′∈S
pπ(s′|s)V n−1(s′).

344
learning value function approximations
When we use classical temporal difference learning (for a ﬁxed policy), we are
following a sample path that puts us in state Sn
t , where we observe a sample
realization of a contribution ˆCn
t , after which we observe a sample realization of
the next downstream state Sn
t+1 (the action is determined by our ﬁxed policy). A
sample observation of the value of being in state Sn
t would be computed using
ˆvn
t = C(Sn
t , an
t ) + γV
n−1
t+1 (Sn
t+1),
where an
t = Aπ(Sn
t ). We can then use this to update our estimate of the value of
being in state Sn
t using
V
n
t (Sn
t ) = (1 −αn−1)V
n−1
t
(Sn
t ) + αn−1 ˆvn
t
= (1 −αn−1)V
n−1
t
(Sn
t )
+ αn−1

C(Sn
t , an
t ) + γV
n−1(SM(Sn
t , an
t , W n
t+1))

.
(9.11)
It is not hard to see that equations (9.10) and (9.11) are the same. The idea is
popular because it is particularly easy to implement. It is also well suited to high-
dimensional decision vectors x, as we illustrate in Chapters 13 and 14.
Temporal difference learning derives its name because V
n−1(S) is viewed as the
“current” value of being in state S, while C(S, a) +V
n−1(SM(S, a, W))
is
viewed
as
the
updated
value
of
being
in
state
S.
The
difference
V
n−1(S) −(C(S, a) +V
n−1(SM(S, a, W)))
is
the
difference
in
these
esti-
mates across iterations (or time), hence the name. TD(0) is a form of statistical
bootstrapping because, rather than simulate the full trajectory, it depends on the
current estimate of the value V
n−1(SM(S, a, W)) of being in the downstream
state SM(S, a, W). TD learning, Q-learning, and approximate value iteration all
use statistical bootstrapping.
While TD(0) can be very easy to implement, it can also produce very slow
convergence. The effect is illustrated in Table 9.1, where there are ﬁve steps before
earning a reward of 1 (which we always earn). In this illustration there are no
decisions and the contribution is zero for every other time period. A stepsize of
1/n was used throughout.
Table 9.1 illustrates that the rate of convergence for V0 is dramatically slower
than for V4. The reason is that as we smooth ˆvt into Vt−1, the stepsize has a
discounting effect. The problem is most pronounced when the value of being in a
state at time t depends on contributions that are a number of steps into the future
(imagine the challenge of training a value function to play the game of chess).
For problems with long horizons, and in particular those where it takes many steps
before receiving a reward, this bias can be so serious that it can appear that temporal
differencing (and algorithms that use it) simply does not work. We can partially
overcome the slow convergence by carefully choosing a stepsize rule. Stepsizes
are discussed in depth in Chapter 11.

sampling the value of a policy
345
Table 9.1
Effect of stepsize on backward learning
Iteration
V0
ˆv1
V1
ˆv2
V2
ˆv3
V3
ˆv4
V4
ˆv5
0
0.000
0.000
0.000
0.000
0.000
1
1
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
1.000
1
2
0.000
0.000
0.000
0.000
0.000
0.000
0.500
1.000
1.000
1
3
0.000
0.000
0.000
0.000
0.167
0.500
0.667
1.000
1.000
1
4
0.000
0.000
0.042
0.167
0.292
0.667
0.750
1.000
1.000
1
5
0.008
0.042
0.092
0.292
0.383
0.750
0.800
1.000
1.000
1
6
0.022
0.092
0.140
0.383
0.453
0.800
0.833
1.000
1.000
1
7
0.039
0.140
0.185
0.453
0.507
0.833
0.857
1.000
1.000
1
8
0.057
0.185
0.225
0.507
0.551
0.857
0.875
1.000
1.000
1
9
0.076
0.225
0.261
0.551
0.587
0.875
0.889
1.000
1.000
1
10
0.095
0.261
0.294
0.587
0.617
0.889
0.900
1.000
1.000
1
9.1.6
TD Learning for Inﬁnite Horizon Problems
We can perform updates using a general TD(λ) strategy as we did for ﬁnite horizon
problems. However, there are some subtle differences. With ﬁnite horizon problems,
it is common to assume that we are estimating a different function Vt for each time
period t. As we step through time, we obtain information that can be used for a
value function at a speciﬁc point in time. With stationary problems, each transition
produces information that can be used to update the value function, which is then
used in all future updates. By contrast, if we update Vt for a ﬁnite horizon problem,
then this update is not used until the next forward pass through the states.
When we move to inﬁnite horizon problems, we drop the indexing by t. Instead
of stepping forward in time, we step through iterations, where at each iteration we
generate a temporal difference
δπ,n = C(sn, an) + γV
n−1(SM,a(sn, an)) −V
n−1(sn).
To do a proper update of the value function at each state, we would have to use
an inﬁnite series of the form
V
n(s) = V
n−1(s) + αn
∞

m=0
(γ λ)mδπ,n+m,
(9.12)
where we can use any initial starting state s0 = s. Of course, we would use the
same update for each state sm that we visit, so we might write
V
n(sm) = V
n−1(sm) + αn
∞

n=m
(γ λ)(n−m)δπ,n.
(9.13)
Equations (9.12) and (9.13) both imply stepping forward in time (presumably a
“large” number of iterations) and computing temporal differences before perform-
ing an update. A more natural way to run the algorithm is to do the updates

346
learning value function approximations
incrementally. After we compute δπ,n, we can update the value function at each of
the previous states we visited. So, at iteration n, we would execute
V
n(sm) := V
n(sm) + αn(γ λ)n−mδπ,n,
m = n, n −1, . . . , 1.
(9.14)
We can now use the temporal difference δπ,n to update the estimate of the value
function for every state we have visited up to iteration n.
Step 0. Initialization.
Step 0a. Initialize V
0(S) for all S.
Step 0b. Initialize the state S0.
Step 0c. Set n = 1.
Step 1. Choose ωn.
Step 2. Solve
an = arg max
a∈An

C(Sn, a) + γV
n−1(SM,a(Sn, a))

.
(9.15)
Step 3. Compute the temporal difference for this step:
δπ,n = C(Sn, an) + γ

V
n−1(SM,a(Sn, an)) −V
n−1(Sn)

.
Step 4. Update V for m = n, n −1, . . . , 1:
V
n(Sm) = V
n−1(Sm) + (γ λ)n−mδπ,n.
(9.16)
Step 5. Compute Sn+1 = SM(Sn, an, W(ωn)).
Step 6. Let n = n + 1. If n < N, go to step 1.
Figure 9.2
TD(λ) algorithm for inﬁnite horizon problems.
Figure 9.2 outlines the basic structure of a TD(λ) algorithm for an inﬁnite
horizon problem. Step 1 begins by computing the ﬁrst post-decision state, after
which step 2 makes a single step forward. After computing the temporal difference
in step 3, we traverse previous states we have visited in step 4 to update their
value functions. The sequence of states that we traverse and the associated discount
factor (γ λ)n−m is known in the reinforcement learning community as an eligibility
trace.
In step 4 we update all the states (Sm)n
m=1 that we have visited up to then. Thus,
at iteration n, we would have simulated the partial update
V
n(S0) = V
n−1(S0) + αn−1
n

m=0
(γ λ)mδπ,m.
(9.17)
This means that at any iteration n, we have updated our values using biased sample
observations (as is generally the case in value iteration). We avoided this problem
for ﬁnite horizon problems by extending out to the end of the horizon. We can

stochastic approximation methods
347
obtain unbiased updates for inﬁnite horizon problems by assuming that all policies
eventually put the system into an “absorbing state.” For example, if we are modeling
the process of holding or selling an asset, we might be able to guarantee that we
eventually sell the asset.
One subtle difference between temporal difference learning for ﬁnite horizon and
inﬁnite horizon problems is that in the inﬁnite horizon case, we may be visiting the
same state two or more times on the same sample path. For the ﬁnite horizon case
the states and value functions are all indexed by the time that we visit them. Since
we step forward through time, we can never visit the same state at the same point
in time twice in the same sample path. By contrast, it is quite easy in a steady-
state problem to revisit the same state over and over again. For example, we could
trace the path of our nomadic trucker, who might go back and forth between the
same pair of locations in the same sample path. As a result we are using the value
function to determine what state to visit, but at the same time we are updating the
value of being in these states.
9.2
STOCHASTIC APPROXIMATION METHODS
A central idea in recursive estimation is the use of stochastic approximation
methods and stochastic gradients. We have already seen this in one setting in
Section 7.2.1. We review the idea again here, but in a different context. We begin
with the same stochastic optimization problem, which we originally introduced as
the problem
min
x EF(x, W).
Now assume that we are choosing a scalar value v to solve the problem
min
v EF(v, ˆV ),
(9.18)
where
F(v, ˆV ) = 1
2(v −ˆV )2,
and where ˆV is a random variable with unknown mean. We would like to use a
series of sample realizations ˆvn to guide an algorithm that generates a sequence vn
that converges to the optimal solution v∗that solves (9.18). We use the same basic
strategy as we introduced in Section 7.2.1 where we update vn using
vn = vn−1 −αn−1∇F(vn−1, ˆvn)
(9.19)
= vn−1 −αn−1(vn−1 −ˆvn).
Now, if we make the transition that instead of updating a scalar vn, we are updating
V
n
t (Sn
t ), we obtain the updating equation
V
n
t (Sn
t ) = V
n−1
t
(Sn
t ) −αn−1(V
n−1
t
(Sn
t ) −ˆvn).
(9.20)

348
learning value function approximations
If we use ˆvn = C(Sn
t , an
t ) + γV
n−1(Sn
t+1), we quickly see that the updating equation
produced using our stochastic gradient algorithm (9.20) gives us the same update
that we obtained using temporal difference learning (equation (9.10)) and approx-
imate value iteration (equation (9.11)). In equation (9.19), αn is called a stepsize
because it controls how far we go in the direction of ∇F(vn−1, ˆvn), and for this
reason this is the term that we adopt for αn throughout this book. In contrast to
our ﬁrst use of this idea in Section 7.2, where the stepsize had to serve a scaling
function, in this setting the units of the variable being optimized, vn, and the units
of the gradient are the same. Indeed we can expect that 0 < αn ≤1, which is a
major simpliﬁcation.
Remark
Stochastic gradient methods pervade approximate dynamic programming
and reinforcement learning, but it is important to recognize an odd problem with
signs. Recall that we have deﬁned the temporal difference (also known as the
Bellman error) using δn
t = ˆvn
t −Vt(Sn
t ). If we substitute the temporal difference
into (9.20), we would obtain
V
n
t (Sn
t ) = V
n−1
t
(Sn
t ) −αn−1(−δn
t )
= V
n−1
t
(Sn
t ) + αn−1δn
t .
This type of equation can be found throughout the ADP/RL literature. It looks like
a stochastic gradient updating equation where we are trying to minimize the error.
But instead of seeing a minus sign before the stepsize, you will see a plus sign.
This arises purely because of the disconnect between the traditional deﬁnition of
the temporal difference and the standard strategy in statistics to deﬁne errors as the
“actual” (ˆvn) minus the “predicted” (Vt(Sn−1
t
)).
Let us consider what happens when we replace the lookup table representation
V(s) that we used above with a linear regression V(s|θ) = θT φ. Now we want to
ﬁnd the best value of θ, which we can do by solving
min
θ
E 1
2(V(s|θ) −ˆv)2.
Applying a stochastic gradient algorithm, we obtain the updating step
θn = θn−1 −αn−1(V(s|θn−1) −ˆvn)∇θV(s|θn).
(9.21)
Since V(s|θn) = 
f ∈F θn
f φf (s) = (θn)T φ(s), the gradient with respect to θ is
given by
∇θV(s|θn) =


∂V(s|θn)
∂θ1
∂V(s|θn)
∂θ2...
∂V(s|θn)
∂θF


=


φ1(sn)
φ2(sn)
...
φF (sn)

= φ(sn).

recursive least squares for linear models
349
Thus the updating equation (9.21) is given by
θn = θn−1 −αn−1(V(s|θn−1) −ˆvn)φ(sn)
= θn−1 −αn−1(V(s|θn−1) −ˆvn)


φ1(sn)
φ2(sn)
...
φF(sn)

.
(9.22)
Using a stochastic gradient algorithm requires that we have some starting estimate
θ0 for the parameter vector, although θ0 = 0 is a common choice.
While this is a simple and elegant algorithm, we have reintroduced the problem
of scaling. Just as we encountered in Section 7.2, the units of θn−1 and the units of
(V(s|θn−1) −ˆvn)φ(sn) may be completely different. What we have learned about
stepsizes still applies, except that we may need an initial stepsize that is quite dif-
ferent than 1.0 (our common starting point). Our experimental work has suggested
that the following policy works well: when you choose a stepsize formula, scale
the ﬁrst value of the stepsize so that the change in θn in the early iterations of the
algorithm is approximately 20 to 50 percent (you will typically need to observe
several iterations). You want to see individual elements of θn moving consistently
in the same direction during the early iterations. If the stepsize is too large, the
values can swing wildly, and the algorithm may not converge at all. If the changes
are too small, the algorithm may simply stall out. It is very tempting to run the
algorithm for a period of time and then conclude that it appears to have converged
(presumably to a good solution). While it is important to see the individual ele-
ments moving in the same direction (consistently increasing or decreasing) in the
early iterations, it is also important to see oscillatory behavior toward the end.
9.3
RECURSIVE LEAST SQUARES FOR LINEAR MODELS
Linear regression plays a major role in approximate dynamic programming, since
it is a powerful and effective way for approximating value functions for certain
problem classes. Perhaps one of the most appealing features of linear regression is
the ease with which models can be updated recursively. Recursive methods are well
known in the statistics and machine learning communities, but these communities
often focus on batch methods. Recursive statistics is especially valuable in approx-
imate dynamic programming because we are often updating functional approxima-
tions (value functions or policy functions) iteratively as the algorithm progresses.
In Chapter 8 we reviewed linear statistical models using the classical notation
of statistics,
y = θT x + ε,
where θ = (θ1, . . . , θI)T was the vector of regression coefﬁcients. We let Xn be
the n × I matrix of observations (where n is the number of observations). Using

350
learning value function approximations
batch statistics, we can estimate θ from the normal equation
θ = [(Xn)T Xn]−1(Xn)T Y n.
(9.23)
We now make the conversion to the notation of approximate dynamic programming
and reinforcement learning. Instead of xi we use a basis function φf (S), where f ∈
F is a feature. We let φ(s) be a column vector of the features, and so φn = φ(Sn)
replaces xn. We also write our value function approximation using
V(S) =

f ∈F
θf φf (S) = φ(S)T θ.
Throughout our presentation we assume that we have access to an observation ˆvn
t
of the value of being in state Sn
t . As we have shown, there are different ways of
computing ˆv in dynamic programming applications, but at this point we are going
to simply take these estimates as data. The estimate ˆvn may be the simulation of a
policy as in
ˆvn =
T

t=0
γ tC(St, Aπ(St)),
or a simple bootstrapped estimate
ˆvn
t = C(St, Aπ(St)) + γ EV
n−1
t+1 (St+1).
Ultimately the choice of how we compute ˆv will have other algorithmic implications
in terms of convergence proofs and updating strategies. But for now, we are simply
trying to use the estimates ˆvn to estimate a value function approximation.
9.3.1
Recursive Least Squares for Stationary Data
In the setting of approximate dynamic programming, estimating the coefﬁcient
vector θ using batch methods such as equation (9.23) would be very expensive.
Fortunately, it is possible to compute these formulas recursively. The updating
equation for θ is
θn = θn−1 −H nφnˆεn,
(9.24)
where H n is a matrix computed using
H n = 1
γ n Bn−1.
(9.25)
The error ˆεn is computed using
ˆεn = Vs(θ n−1) −ˆvn.
(9.26)
Note that it is common in statistics to compute the error in a regression using
“actual minus predicted” while we are using “predicted minus actual” (see also

recursive least squares for linear models
351
equation (9.21) above). Our sign convention is motivated by the derivation from
ﬁrst principles of optimization. Bn−1 is an |F| by |F| matrix, which is updated
recursively using
Bn = Bn−1 −1
γ n (Bn−1φn(φn)T Bn−1).
(9.27)
γ n is a scalar computed using
γ n = 1 + (φn)T Bn−1φn.
(9.28)
The derivation of equations (9.24) through (9.28) is given in Section 9.10.1.
Equation (9.24) has the feel of a stochastic gradient algorithm, but it has one
signiﬁcant difference. Instead of using a typical stepsize, we have the matrix H n
to serve as a scaling matrix.
It is possible in any regression problem that the matrix (Xn)T Xn (in equation
(9.23)) is noninvertible. If this is the case, then our recursive formulas are not
going to overcome this problem. When this happens, we will observe γ n = 0.
Alternatively, the matrix may be invertible, but unstable, which occurs when γ n is
very small (e.g., γ n < ϵ for some small ϵ). When this occurs, the problem can be
circumvented by using
γ n = γ n + δ,
where δ is a suitably chosen small perturbation that is large enough to avoid
instabilities. Some experimentation is likely to be necessary, since the right value
depends on the scale of the parameters being estimated.
The only missing step in our algorithm is initializing Bn. One strategy is to
collect a sample of m observations where m is large enough to compute Bm using
full inversion. Once we have Bm, we can then proceed to update it using the
formula above. A second strategy is to use B0 = ϵI, where I is the identity matrix
and ϵ is a “small constant.” This strategy is not guaranteed to give the exact values,
but it should work well if the number of observations is relatively large.
In our dynamic programming applications the observations ˆvn will represent
estimates of the value of being in a state, and our independent variables will be
either the states of our system (if we are estimating the value of being in each
state) or the basis functions. So in this case we are estimating the coefﬁcients of
the basis functions. The equations assume implicitly that the estimates come from
a stationary series.
There are many problems where the number of basis functions can be extremely
large. In these cases even the efﬁcient recursive expressions in this section cannot
avoid the fact that we are still updating a matrix where the number of rows and
columns is the number of states (or basis functions). If we are only estimating a few
dozen or a few hundred parameters, this can be ﬁne. If the number of parameters
extends into the thousands, even this strategy would probably bog down. It is
important to work out the approximate dimensionality of the matrices before using
these methods.

352
learning value function approximations
9.3.2
Recursive Least Squares for Nonstationary Data
It is generally the case in approximate dynamic programming that our observations
ˆvn (typically updates to an estimate of a value function) come from a nonsta-
tionary process. This is true even when we are estimating the value of a ﬁxed
policy if we use TD learning, but it is always true when we introduce the dimen-
sion of optimizing over policies. Recursive least squares puts equal weight on all
prior observations, whereas we would prefer to put more weight on more recent
observations.
Instead of minimizing total errors (as we do in equation (8.18)), it makes sense
to minimize a geometrically weighted sum of errors
min
θ
n

m=1
λn−m

ˆvm −(θ0 +
I

i=1
θiφm
i )
2
,
(9.29)
where λ is a discount factor that we use to discount older observations. If we
repeat the derivation in Section 9.3.1, the only changes we have to make are in the
updating formula for Bn, which is now given by
Bn = 1
λ

Bn−1 −1
γ n (Bn−1φn(φn)T Bn−1)

,
(9.30)
and the expression for γ n, which is now given by
γ n = λ + (φn)T Bn−1φn.
(9.31)
λ works in a way similar to a stepsize, although in the opposite direction. Setting
λ = 1 means that we are putting an equal weight on all observations, whereas
smaller values of λ puts more weight on more recent observations. This way λ
plays a role similar to our use of λ in TD(λ).
We could use this logic and view λ as a tunable parameter. Of course, a constant
goal in the design of approximate dynamic programming algorithms is to avoid the
need to tune yet another parameter. Ideally we would like to take advantage of the
theory we developed for automatically adapting stepsizes to automatically adjust λ.
For the special case where our regression model is just a constant (in which case
φn = 1), we can develop a simple relationship between αn and the discount factor
(which we now compute at each iteration, so we write it as λn). Let Gn = (Bn)−1,
which means that our updating equation is now given by
θ n = θn−1 −1
γ n (Gn)−1φnˆεn.
Recall that we compute the error εn as predicted minus actual as given in equation
(9.26). This is required if we are going to derive our optimization algorithm based
on ﬁrst principles, which means that we are minimizing a stochastic function. The
matrix Gn is updated recursively using
Gn = λnGn−1 + φn(φn)T ,
(9.32)

recursive least squares for linear models
353
with G0 = 0. For the case where φn = 1 (in which case Gn is also a scalar),
(Gn)−1φn = (Gn)−1 plays the role of our stepsize, so we would like to write
αn = (Gn)−1. Assume that αn−1 =

Gn−1−1. Equation (9.32) implies that
αn = (λnGn−1 + 1)−1
=
 λn
αn−1
+ 1
−1
.
Solving for λn gives
λn = αn−1
1 −αn
αn

.
(9.33)
Note that if λn = 1, then we want to put equal weight on all the observations
(which would be optimal if we have stationary data). We know that in this setting,
the best stepsize is αn = 1/n. Substituting this stepsize into equation (9.33) veriﬁes
this identity.
The value of equation (9.33) is that it allows us to relate the discounting pro-
duced by λn to the choice of stepsize rule, which has to be chosen to reﬂect the
nonstationarity of the observations. In Chapter 11 we introduce a much broader
range of stepsize rules, some of which have tunable parameters. Using equation
(9.33) allows us to avoid introducing yet another tunable parameter.
9.3.3
Recursive Estimation Using Multiple Observations
The previous methods assume that we get one observation and use it to update the
parameters. Another strategy is to sample several paths and solve a classical least
squares problem for estimating the parameters. In the simplest implementation we
would choose a set of realizations ˆn (rather than a single sample ωn) and follow
all of them, producing a set of estimates (ˆvn(ω))ω∈ˆn that we can use to update
the value function.
If we have a set of observations, we then face the classical problem of ﬁnding a
vector of parameters ˆθn that best match all of these value function estimates. Thus
we want to solve
ˆθn = arg min
ˆθ
1
| ˆn|

ω∈ˆn
(V( ˆθ) −ˆv(ω))2.
This is the standard parameter estimation problem faced in the statistical estima-
tion community. If V(θ) is linear in θ, then we can use the usual formulas for
linear regression. If the function is more general, then we would typically resort to
nonlinear programming algorithms to solve the problem. In either case, ˆθn is still
an update that needs to be smoothed in with the previous estimate θn−1, which we
would do using
θn = (1 −αn−1)θn−1 + αn−1 ˆθn.
(9.34)

354
learning value function approximations
One advantage of this strategy is that in contrast with the updates that depend on
the gradient of the value function, updates of the form given in equation (9.34)
do not encounter a scaling problem, and therefore we return to our more familiar
territory where 0 < αn ≤1. Of course, as the sample size ˆ increases, the stepsize
should also be increased because there is more information in ˆθn. Stepsizes based
on the Kalman ﬁlter (Sections 11.4.2 and 11.4.3) will automatically adjust to the
amount of noise in the estimate.
The usefulness of this particular strategy will very much depend on the problem
at hand. In many applications the computational burden of producing multiple
estimates ˆvn(ω), ω ∈ˆn before producing a parameter update will simply be too
costly.
9.3.4
Recursive Time-Series Estimation*
Up to now we have used regression models that depend on explanatory variables
(which we sometimes refer to as basis functions) that depend on the state of the
system. The goal has been to represent the value of being in a state St with a small
number of parameters by using the structure of the state variable.
Now consider a somewhat different use of linear regression. Let ˆvn be an esti-
mate of the value of being in some state St = s (which we suppress to keep the
notation simple). By now we are well aware that the random observations ˆvn tend
to grow (or shrink) over time. If they are growing, then the standard updating
equation
vn = (1 −αn−1)vn−1 + αn−1 ˆvn
(9.35)
will generally produce an estimate vn that is less than ˆvn+1. We can minimize this
by estimating a regression model of the form
ˆvn+1 = θ0 ˆvn + θ1vn−1 + · · · + θF vn−F + ˆεn+1.
(9.36)
What we are doing is estimating ˆvn+1 as a function of the recent history of obser-
vations, which offers the potential that we will be predicting trends in the estimates
that might arise when using algorithms such as value iteration or Q-learning.
After iteration n, our estimate of the value of being in state s is
vn = θn
0 ˆvn + θn
1 vn−1 + · · · + θn
Fvn−F.
(9.37)
Contrast this update with our simple exponential smoothing (e.g., equation (9.35)).
We see that exponential smoothing is simply a form of linear regression where the
parameter vector θ is determined by the stepsize.
We can update our parameter vector θn using the techniques described earlier in
this section. We need to keep in mind that we are estimating the value of being in
a state s using a F+1-dimensional parameter vector. There are applications where
we are trying to estimate the value of being in hundreds of thousands of states.
For such large applications we have to decide if we are going to have a different
parameter vector θ for each state s or one for the entire system.

recursive least squares for linear models
355
There are many variations on this basic idea that draw on the entire ﬁeld of
time-series analysis. For example, we can use our recursive formulas to estimate a
more general time-series model. At iteration n let the elements of our basis function
be given by
φn = (φf (Sn))f ∈F,
which is the observed value of each function given the state vector Sn. If we wished
to include a constant term, we would deﬁne a basis function φ0 = 1. Let F = |F|
be the number of basis functions used to explain the value function, so φn is an
F-dimensional column vector.
We can combine the information in our basis functions (which depend on the
state at time t) with the history of observations of the updated values. This we
represent using the column vector
˜vn = (ˆvn, vn−1, . . . , vn−F)T
just as we did in Sections 9.2 and 9.3. Taken together, φn and ˜vn is our population
of potential explanatory variables that we can use to help us predict ˆvn+1. φn is an
F-dimensional column vector, while ˜vn is a F+1-dimensional column vector. We
can formulate a model using
ˆvn+1 = (θφ)T φn + (θv)T ˜vn + ˆεn+1,
where θφ and θv are, respectively, F and F+1-dimensional parameter vectors. This
gives us a prediction of the value of being in state Sn using
vn = (θφ,n)T φn + (θv,n)T ˜vn.
Again, we update our estimate of the parameter vector θn = (θφ,n, θv,n) using the
same techniques we presented above.
It is important to be creative. For example, we could use a time series model
based on differences, as in
ˆvn+1 = θ0 ˆvn + θ1(vn−1 −ˆvn) + θ2(vn−2 −vn−1) + · · ·
+θF

vn−F −vn−F+1
+ ˆεn+1.
Alternatively, we can use an extensive library of techniques developed under the
umbrella of time-series modeling.
The signal processing community has developed a broad range of techniques
for estimating models in a dynamic environment. This community refers to models
such as (9.36) and (9.38) as linear ﬁlters. Using these techniques to improve the
process of estimating value functions remains an active area of research. Our sense
is that if the variance of ˆvn is large relative to the rate at which the mean is changing,
then we are unlikely to derive much value from a more sophisticated model which
considers the history of prior estimates of the value function. However, there are

356
learning value function approximations
problems where the mean of vn is either changing quickly or changes for many
iterations (due to a slow learning process). For these problems a time-series model
that captures the behavior of the estimates of the value function over time may
improve the convergence of the algorithm.
9.4
TEMPORAL DIFFERENCE LEARNING WITH A LINEAR MODEL
We are now ready to introduce one of the most powerful and attractive algorithmic
procedures in approximate dynamic programming, which is temporal difference
learning using a value function based on a linear model. Section 9.3 has introduced
a family of recursive procedures for linear models. It is convenient to represent
these using a generic updating function that we can depict using
V
n(s) ←U V (V
n−1(s), Sn, ˆvn).
Here we are saying that we are visiting state Sn where we observe an estimate ˆvn of
the value of being in state Sn, which is then used to update our approximate value
function V
n−1(s) to obtain an updated function V
n(s). It might be the case that
V
n−1(s) = 
f θn−1
f
φf (s). The new information would allow us to use recursive
least squares to obtain θ n, giving us V
n(s) = 
f θn
f φf (s). To avoid the usual
complications of taking expectations, we are going to assume that we are estimating
the value function approximation around the post-decision state Sa.
Figure 9.3 is an illustration of a fairly typical TD(0) learning algorithm using
linear models to approximate the value function. Note that the algorithm easily
handles complex state variables (which may be multidimensional and continu-
ous). We might also be able to handle multidimensional and continuous actions
(which we normally write as x) if we can solve the maximization problem in
step 1.
The algorithm seeks to ﬁnd the best ﬁt of the linear model when we ﬁx the
policy using
Aπ(s) = arg max
a∈An

C(Sn, a) + γ

f
θπ
f φf (SM,a(Sn, a))

.
Note that a ﬁxed parameter vector θπ deﬁnes how actions are chosen, and this
is not updated as the algorithm progresses. Also note that we choose the next
state to visit using Sn+1 = SM(Sn, an, W n+1), which means that we use the action
determined by policy π to choose the next state that we visit. This is known as
on-policy learning, a term we ﬁrst saw in Section 4.3.3.
Deﬁne the true value of our policy as
V π(s) = E
∞

t=0
γ tC(s, Aπ(s)).

temporal difference learning with a linear model
357
Step 0. Initialization.
Step 0a. Initialize V
0
t , t ∈T.
Step 0b. Set n = 1.
Step 0c. Initialize the previous post-decision state Sa,0.
Step 1. Solve
ˆvn = max
a∈An

C(Sn, a) + γ

f
θπ
f φf (SM,a(Sn, a))

,
(9.38)
and let an be the value of a that solves (9.38).
Step 2. Update the value function around the previous post-decision state variable:
V
n(s) ←UV (V
n−1(s), Sa,n−1, ˆvn).
Step 3. Sample W n+1 and update the state variable:
Sn+1 = SM(Sn, an, W n+1).
Step 4. Increment n. If n ≤N, go to step 1.
Step 5. Return the value functions V
π ≈V
N.
Figure 9.3
Temporal difference learning for a ﬁxed policy.
It turns out that if we use on-policy learning, this algorithm has been proved to
produce a value function approximation that minimizes the function
min
θ

s
dπ
s


f
θf φf (s) −V π(s)


2
,
(9.39)
where dπ
s is the steady-state probability of being in state s while following policy
π. The weighting by dπ
s is an important dimension of our objective function. We
note that the result still applies even if the state variable is multidimensional and
continuous, something that the algorithm handles without modiﬁcation. The conver-
gence proof requires an approximation that is linear in the parameters; TD-learning
using approximations that are nonlinear in the parameters may not converge, and
may even diverge.
Using our policy Aπ(s) to determine the next state to visit seems like a perfectly
reasonable assumption, but it can cause problems when we embed this logic in an
algorithm that is searching for the best policy. As we discussed in Section 4.3, we
might be following a policy that avoids certain states and actions that might lead to
even better policies. Sometimes we have to force our algorithms to explore states
and actions that may not appear to be the best but that might be very attractive.
This is not an issue when we are evaluating a ﬁxed policy, but it becomes critical
if we are doing this to ﬁnd better policies.

358
learning value function approximations
For the purpose of determining the next state to visit, we might introduce an
exploration step where we choose an action a at random with probability ϵ, or
we might use the action determined by our policy Aπ(s) with probability 1 −
ϵ. This logic was essential in our Q-learning algorithm, but when we make the
transition to using linear models, not only do we lose our guarantee of convergence,
the algorithm may actually diverge! Furthermore, divergence is not restricted to
pathological cases.
The interaction of exploration policies and linear models is a subtle but critical
dimension of approximate dynamic programming, especially when we introduce
the dimension of optimizing over policies (which we address in Chapter 10). The
reinforcement learning community struggled for years with the issue of whether
TD learning converged while using linear models, with a mixture of results that
both supported and contradicted convergence. The issue was resolved in the sem-
inal paper Tsitsiklis and Van Roy (1997) that established that online learning was
a critical component in a convergent algorithm, along with the presence of the
weighting term dπ
s in the objective function (9.39).
The resolution that on-policy learning was critical to the convergence of TD
learning left unanswered the challenge of how to combine the need for convergence
with the need to explore states. One strategy is to use an ϵ-greedy exploration with ϵ
declining with the iterations so that the algorithm is actually a sequence of policies
converging on a single greedy policy. Precup et al. (2001) offered the ﬁrst version
of an off-policy TD algorithm using linear models that converged to the optimal
solution (i.e., the approximation that solves (9.39)), but the algorithm is restricted
to policies such as Boltzmann exploration so that the probability of choosing any
action is strictly positive.
Often overlooked in this debate is that if we have a good architecture (i.e., a
good set of basis functions), we should not need to explore. That is, the issue is
not so much exploration as it is identiﬁcation, which addresses the question of
whether we are collecting observations from a sufﬁciently diverse set of states that
allows us to produce statistically reliable estimates of the function.
Consider the illustration in Figure 9.4, where we are only sampling a subset of
states determined by the density dπ
s (assume that the states are continuous). The
true value function V π(s) is depicted by the solid line. If we have a good set
of basis functions, all we need is enough variation in the states that we do visit
to ﬁnd an approximation V(s) that works well for all states. For example, if we
could only sample a narrow region of the state space, we would have a difﬁcult
time estimating our function. Of course, the assumption that we have a good set
of basis functions is critical, and hard to verify. In practice, we generally can only
hope to do well in a certain region, which is why the exploration issue is important.
9.5
BELLMAN’S EQUATION USING A LINEAR MODEL
It is possible to solve Bellman’s equation for inﬁnite horizon problems by starting
with the assumption that the value function is given by a linear model V (s) =

bellman’s equation using a linear model
359
s
s
d
V (s)
V (s)
Figure 9.4
True value function V π(s) (solid line), a ﬁtted approximation V(s) (dashed line), with
observations drawn from a subset of states determined by the density dπ
s .
θT φ(s) where (s) is a column vector of basis functions for a particular state s.
Of course, we are still working with a single policy, so we are using Bellman’s
equation only as a method for ﬁnding the best linear approximation for the inﬁnite
horizon value of a ﬁxed policy π.
We begin with a derivation based on matrix linear algebra, which is more
advanced and which does not produce expressions that can be implemented in
practice. We follow this discussion with a simulation-based algorithm that can be
implemented fairly easily.
9.5.1
A Matrix-Based Derivation*
In Section 8.2.3 we provided a geometric view of basis functions, drawing on the
elegance and obscurity of matrix linear algebra. We are going to continue this
presentation and present a version of Bellman’s equation assuming linear models.
However, we are not yet ready to introduce the dimension of optimizing over poli-
cies, so we are still simply trying to approximate the value of being in a state. Also
we are only considering inﬁnite horizon models, since we have already handled the
ﬁnite horizon case. This presentation can be viewed as another method for handling
inﬁnite horizon models, while using a linear architecture to approximate the value
function.
First recall that Bellman’s equation (for a ﬁxed policy) is written
V π(s) = C(s, Aπ(s)) + γ

s′∈S
p(s′|s, Aπ(s))V π(s′).
In vector-matrix form, we let V π be a vector with element V π(s), we let cπ
be a vector with element C(s, Aπ(s)), and we let P π be the one-step transition

360
learning value function approximations
matrix with element p(s′|s, Aπ(s)) at row s, column s′. By this notation, Bellman’s
equation becomes
V π = cπ + γ P πV π,
allowing us to solve for V π using
V π = (I −γ P π)−1cπ.
This works with a lookup table representation (a value for each state). Now assume
that we replace V π with an approximation V
π = θ, where  is a |S| × |F| matrix
with element s,f = φf (s). Also let dπ
s be the steady-state probability of being
in state s while following policy π, and let Dπ be a |S| × |S| diagonal matrix
where the state probabilities (dπ
1 , . . . , dπ
|S| make up the diagonal. We would like
to choose θ to minimize the weighted sum of errors squared, where the error for
state s is given by
ϵn(s) =

f
θf φf (s) −

cπ(s) + γ

s′∈S
pπ(s′|s, Aπ)

f
θn
f φf (s′)

.
(9.40)
The ﬁrst term on the right-hand side of (9.40) is the predicted value of being in
each state given θ, while the second term on the right-hand side is the “predicted”
value computed using the one-period contribution plus the expected value of the
future, which is computed using θn. The expected sum of errors squared is then
given by
min
θ

s∈S
dπ
s


f
θf φf (s) −

cπ(s) + γ

s′∈S
pπ(s′|s, Aπ)

f
θn
f φf (s′)




2
.
In matrix form this can be written
min
θ (θ −(cπ + γ P πθn))T Dπ(θ −(cπ + γ P πθn)),
(9.41)
where Dπ is a |S| × |S| diagonal matrix with elements dπ
s that serves a scaling
role (we want to focus our attention on states we visit the most). We can ﬁnd
the optimal value of θ (given θn) by taking the derivative of the function being
optimized in (9.41) with respect to θ and setting it equal to zero. Let θn+1 be the
optimal solution, which means we can write
T Dπ
θn+1 −(cπ + γ P πθn)

= 0.
(9.42)
We can ﬁnd a ﬁxed point limn→∞θn = limn→∞θn+1 = θ∗, that allows us to write
equation (9.42) in the form
Aθ ∗= b,
(9.43)

bellman’s equation using a linear model
361
where A = T Dπ(I −γ P π) and b = T Dπcπ. This allows us, in theory at
least, to solve for θ∗using
θ∗= A−1b,
(9.44)
which can be viewed as a scaled version of the normal equations (equation (9.23)).
Equation (9.44) is very similar to our calculation of the steady-state value of being
in each state introduced in Chapter 3, given by
V π = (I −γ P π)−1cπ.
Equation (9.44) differs only in the scaling by the probability of being in each state
(Dπ) and then the transformation to the feature space by .
We note that equation (9.42) can also be written in the form
Aθ −b = T Dπ 
θ −(cπ + γ P πθ)

.
(9.45)
The term θ can be viewed as the approximate value of each state. The term
(cπ + γ P πθ) can be viewed as the one-period contribution plus the expected
value of the state that we would transition to under policy π, again computed for
each state. Let δπ be a column vector containing the temporal difference for each
state when we choose an action according to policy π. By tradition, the temporal
difference has always been written in the form C(St, a) +V(St+1) −V(St), which
can be thought of as “estimated minus predicted.” If we continue to let δπ be the
traditional deﬁnition of the temporal difference, it would be written
δπ = −

θ −(cπ + γ P πθ)

.
(9.46)
The pre-multiplication of δπ by Dπ in (9.45) has the effect of factoring each tem-
poral difference by the probability that we are in each state. Then pre-multiplying
Dπδπ by T has the effect of transforming this scaled temporal difference for each
state into the feature space.
The goal is to ﬁnd the value θ that produces Aθ −b = 0, which means we are
trying to ﬁnd the value θ that produces a scaled version of θ −(cπ + γ P πθ) =
0 but transformed to the feature space.
Linear algebra offers a compact elegance, but at the same time can be hard to
parse, and for this reason we encourage the reader to stop and think about the
relationships. One useful exercise is to think of a set of basis functions where we
have a “feature” for each state, with φf (s) = 1 if feature f corresponds to state
s. In this case,  is the identity matrix. Dπ, the diagonal matrix with diagonal
elements dπ
s giving the probability of being in state s, can be viewed as scaling
quantities for each state by the probability of being in a state. If  is the identity
matrix, then A = Dπ −γ DπP π where DπP π is the matrix of joint probabilities
of being in state s and then transitioning to state s′. The vector b becomes the
vector of the cost of being in each state (and then taking the action corresponding
to policy π) times the probability of being in the state.

362
learning value function approximations
When we have a smaller set of basis functions, then multiplying cπ or Dπ(I −
γ P π) times  has the effect of scaling quantities that are indexed by the state
into the feature space, which also transforms an |S|-dimensional space into an
|F|-dimensional space.
9.5.2
A Simulation-Based Implementation
We start by simulating a trajectory of states, actions and information,
(S0, a0, W 1, S1, a1, W 2, . . . , Sn, an, W n+1).
Recall that φ(s) is a column vector with an element φf (s) for each feature f ∈F.
Using our simulation above, we also obtain a sequence of column vectors φ(si)
and contributions C(Si, ai, W i+1). We can create a sample estimate of the |F| by
|F| matrix A in the section above using
An = 1
n
n−1

i=0
φ(Si)(φ(Si) −γ φ(Si+1))T .
(9.47)
We can also create a sample estimate of the vector b using
bn = 1
n
n−1

i=0
φ(Si)C(Si, ai, W i+1).
(9.48)
To gain some intuition, again stop and assume that there is a feature for every
state, which means that φ(Si) is a vector of 0’s with a 1 corresponding to the
element for state i, making it an indicator variable telling us what state we are
in. The term (φ(Si) −γ φ(Si+1)) is then a simulated version of Dπ(I −γ P π),
weighted by the probability that we are in a particular state, where we replace the
probability of being in a state with a sampled realization of actually being in a
particular state.
We are going to use this foundation to introduce two important algorithms for
inﬁnite horizon problems when using linear models to approximate value function
approximations. These are known as least squares temporal differences (LSTD)
and least squares policy evaluation (LSPE).
9.5.3
Least Squares Temporal Differences (LSTD)
As long as An is invertible (which is not guaranteed), we can compute a sample
estimate of θ using
θn = (An)−1bn.
(9.49)
This algorithm is known in the literature as least squares temporal differences.
As long as the number of features is not too large (as is typically the case), the
inverse is not too hard to compute. LSTD can be viewed as a batch algorithm

bellman’s equation using a linear model
363
that operates by collecting a sample of temporal differences, and then using least
squares regression to ﬁnd the best linear ﬁt.
We can see the role of temporal differences more clearly by doing a little algebra.
We use equations (9.47) and (9.48) to write
Anθn −bn = 1
n
n−1

i=0

φ(Si)(φ(Si) −γ φ(Si+1))T θn −φ(Si)C(Si, ai, W i+1)

= 1
n
n−1

i=0
φ(Si)

φ(Si)T θn −(C(Si, ai, W i+1) + γ φ(Si+1)T θn)

= −1
n
n−1

i=0
φ(Si)δi(θn),
where δi(θn) = (C(Si,ai,W i+1) + γ φ(Si+1)T θn) −φ(Si)T θn is the ith temporal
difference given the parameter vector θn, where we are using the standard deﬁnition
of temporal difference. Thus we are doing a least squares regression so that the
sum of the temporal differences over the simulation (which approximations the
expectation) is equal to zero. We would, of course, like it if θ could be chosen so
that δi(θ) = 0 for all i. However, when working with sample realizations, the best
we can expect is that the average across the observations of δi(θ) tends to zero.
9.5.4
Least Squares Policy Evaluation (LSPE)
LSTD is basically a batch algorithm, which requires collecting a sample of n
observations and then using regression to ﬁt a model. An alternative strategy uses
a stochastic gradient algorithm that successively updates estimates of θ. The basic
updating equation is
θn = θn−1 −α
n Gn
n−1

i=0
φ(Si)δi(n),
(9.50)
where Gn is a scaling matrix. Although there are different strategies for computing
Gn, the most natural is a simulation-based estimate of (T Dπ)−1 that can be
computed using
Gn =

1
n + 1
n

i=0
φ(Si)φ(Si)T
−1
.
To visualize Gn, return again to the assumption that there is a feature for every
state. In this case φ(Si)φ(Si)T is an |S| by |S| matrix with a 1 on the diagonal for
row Si and column Si. As n approaches inﬁnity, the matrix

1
n + 1
n

i=0
φ(Si)φ(Si)T


364
learning value function approximations
approaches the matrix Dπ of the probability of visiting each state, stored in elements
along the diagonal.
9.6
ANALYSIS OF TD(0), LSTD, AND LSPE USING A SINGLE STATE
A useful exercise to understand the behavior of recursive least squares, LSTD,
and LSPE is to consider what happens when they are applied to a trivial dynamic
program with a single state and a single action. Obviously we are interested in
the policy that chooses the single action. This dynamic program is equivalent to
computing the sum
F = E
∞

i=0
γ i ˆCi,
(9.51)
where ˆCi is a random variable giving the ith contribution. If we let c = E ˆCi, then
clearly F = c/(1 −γ ). But let us pretend that we do not know this, and we are
using these various algorithms to compute the expectation.
9.6.1
Recursive Least Squares and TD(0)
Let ˆvn be an estimate of the value of being in state Sn. We continue to assume that
the value function is approximated using
V(s) =

f ∈F
θf φf (s).
We wish to choose θ by solving
min
θ
n

i=1

ˆvi −


f ∈F
θf φf (Si)




2
.
Let θn be the optimal solution. We can determine this recursively using the tech-
niques presented earlier in this chapter which gives us the updating equation
θn = θn−1 −
1
1 + (xn)T Bn−1xn Bn−1xn(V
n−1(Sn) −ˆvn)
(9.52)
where xn = (φ1(Sn), . . . , φf (Sn), . . . , φF(Sn)), and the matrix Bn is computed
using
Bn = Bn−1 −
1
1 + (xn)T Bn−1xn

Bn−1xn(xn)T Bn−1
.

analysis of td(0), lstd, and lspe using a single state
365
If we have only one state and one action, we only have one basis function φ(s) = 1
and one parameter θn = V
n(s). Now the matrix Bn is a scalar and equation (9.53)
reduces to
vn = vn−1 −
Bn−1
1 + Bn−1 (vn−1 −ˆvn)
=

1 −
Bn−1
1 + Bn−1

vn−1 +
Bn−1
1 + Bn−1 .
If B0 = 1, Bn−1 = 1/n, giving us
vn = n −1
n
vn−1 + 1
n ˆvn.
(9.53)
Imagine now that we are using TD(0) where ˆvn = ˆCn + γ vn−1. In this case we
obtain
vn =

1 −(1 −γ )1
n

vn−1 + 1
n
ˆCn.
(9.54)
Equation (9.54) can be viewed as an algorithm for ﬁnding
v =
∞

n=0
γ n ˆCn,
where the solution is v∗= E ˆC/(1 −γ ).
Equation (9.54) shows us that recursive least squares, when ˆvn is computed
using temporal difference learning, has the effect of successively adding sample
realizations of costs, with a “discount factor” of 1/n. The factor 1/n arises directly
as a result of the need to smooth out the noise in ˆCn. For example, if ˆC = c is a
known constant, we could use standard value iteration, which would give us
vn = c + γ vn−1.
(9.55)
It is easy to see that vn in (9.55) will rise much more quickly toward v∗than the
algorithm in equation (9.54). We return to this topic in some depth in Chapter 11.
9.6.2
LSPE
LSPE requires that we ﬁrst generate a sequence of states Si and contributions ˆCi
for i = 1, . . . , n. We then compute θ by solving the regression problem
θn = arg min
θ
n

i=1


f
θf φf (Si) −

ˆCi + γV
n−1(Si+1)



2
.

366
learning value function approximations
For a problem with one state where θn = vn, this reduces to
vn = arg min
θ
n

i=1

θ −

ˆCi + γ vn−12
.
This problem can be solved in closed form, giving us
vn =

1
n
n

i=1
ˆCi

+ γ vn−1.
9.6.3
LSTD
Finally, we showed above that the LSTD procedure ﬁnds θ by solving the system
of equations
n

i=1
φf (Si)(φf (Si) −γ φf (Si+1))T θn =
n

i=1
φf (Si) ˆCi,
for each f ∈F. Again, since we have only one basis function φ(s) = 1 for our
single state problem, this reduces to ﬁnding the scalar θn = vn using
vn =
1
1 −γ

1
n
n

i=1
ˆCn

.
9.6.4
Discussion
This presentation illustrates three different styles for estimating an inﬁnite hori-
zon sum. In recursive least squares, equation (9.53) demonstrates the successive
smoothing of the previous estimate vn and the latest estimate ˆvn. We are, at the
same time, adding contributions over time while trying to smooth out the noise.
LSPE, by contrast, separates the estimation of the mean of the single-period
contribution, and the process of summing contributions over time. At each iteration
we improve our estimate of E ˆC, and then accumulate our latest estimate in a
telescoping sum.
LSTD, ﬁnally, updates its estimate of E ˆC, and then projects this over the inﬁnite
horizon by factoring the result by 1/(1 −γ ).
9.7
GRADIENT-BASED METHODS FOR APPROXIMATE
VALUE ITERATION*
There has been a strong desire for approximation algorithms with the following
features: (1) off-policy learning, (2) temporal-difference learning, (3) linear models
for value function approximation, and (4) complexity (in memory and computation)
that is linear in the number of features. The last requirement is primarily of interest

gradient-based methods
367
in specialized applications that require thousands or even millions of features. Off-
policy learning is desirable because it provides an important degree of control over
exploration. Temporal-difference learning is useful because it is so simple, as is the
use of linear models, that it is possible to provide an estimate of the entire value
function with a small number of measurements.
Off-policy, temporal-difference learning was ﬁrst introduced in the form of Q-
learning using a lookup table representation, where it is known to converge. But
we lose this property if we introduce value function approximations that are linear
in the parameters. In fact Q-learning can be shown to diverge for any positive
stepsize. The reason is that there is no guarantee that our linear model is accurate,
which can introduce signiﬁcant instabilities in the learning process.
Q-learning and temporal difference learning can be viewed as forms of stochastic
gradient algorithms, but the problem with earlier algorithms when we use linear
value function approximations can be traced to the choice of objective function. For
example, if we wish to ﬁnd the best linear approximation V(s|θ), a hypothetical
objective function would be to minimize the expected mean squared difference
between V(s|θ) and the true value function V (s). If dπ
s is the probability of being
in state s, this objective would be written
MSE(θ) = 1
2

s
dπ
s (V(s|θ) −V (s))2.
If we are using approximate value iteration, a more natural objective function is to
minimize the mean squared Bellman error. We use the Bellman operator Mπ (as
we did in Chapter 3) for policy π to represent
Mπv = cπ + γ P πv,
where v is a column vector giving the value of being in state s, and cπ is the
column vector of contributions C(s, Aπ(s)) if we are in state s and choose an
action a according to policy π. This allows us to deﬁne
MSBE(θ) = 1
2

s
dπ
s

V(s|θ) −(cπ(s) + γ

s′
pπ(s′|s)V(s′|θ))
2
= 1
2∥V(θ) −MV(θ)∥2
D.
We can minimize MSBE(θ) by generating a sequence of states (S1, . . . , Si,
Si+1, . . .) and then computing a stochastic gradient
∇θMSBE(θ) = −δπ,i(φ(Si) −γ φ(Si+1)),
where φ(Si) is a column vector of basis functions evaluated at state Si. The scalar
δπ,i is the temporal difference given by
−δπ,i = V(Si|θ) −(cπ(Si) + γV(Si+1|θ)).

368
learning value function approximations
We note that δπ,i depends on the policy π that affects both the single-period
contribution and the likelihood of transitioning to state Si+1. To emphasize that we
are working with a ﬁxed policy, we carry the superscript π throughout.
A stochastic gradient algorithm, then, would seek to optimize θ using
θn+1 = θn −αn∇θMSBE(θ)
(9.56)
= θn −αn(−δπ,n)(φ(Sn) −γ φ(Sn+1))
= θn + αnδπ,n(φ(Sn) −γ φ(Sn+1)).
(9.57)
We note that the sign convention for temporal differences gives us a stochastic
updating formula that seems to have the sign in front of the stepsize reversed. This
derivation tracks the reason for this sign change, which can be traced to the sign
convention for temporal differences.
A variant of this basic algorithm, called the generalized TD(0) (or, GTD(0))
algorithm, is given by
θn+1 = θn + αn(φ(Sn) −γ φ(Sn+1))φ(Sn)T un,
(9.58)
where
un+1 = un + βn(un −δπ,nφ(Sn)).
(9.59)
αn and βn are both stepsizes. un is a smoothed estimate of the product δπ,nφ(Sn).
Gradient descent methods based on temporal differences will not minimize
MSBE(θ) because there does not exist a value of θ that would allow ˆv(s) =
cπ(s) + γV(s|θ) to be represented as V(s|θ). We can ﬁx this using the mean-
squared projected Bellman error (MSPBE(θ)) which we compute as follows. It is
more compact to do this development using matrix-vector notation. We ﬁrst recall
the projection operator  given by
 = (T Dπ)−1T Dπ.
(See Section 8.2.3 for a derivation of this operator.) If V is a vector giving the value
of being in each state, V is the nearest projection of V on the space generated
by θφ(s). We are trying to ﬁnd V(θ) that will match the one-step lookahead given
by MπV(θ), but this produces a column vector that cannot be represented directly
as θ, where  is the |S| × |F| matrix of feature vectors φ. We accomplish this
by pre-multiplying MπV (θ) by the projection operator . This allows us to form
the mean-squared projected Bellman error using
MSPBE(θ) = 1
2∥V(θ) −MπV(θ)∥2
D
(9.60)
= 1
2

V(θ) −MπV(θ)
T D

V(θ) −MπV(θ)

.
(9.61)
We can now use this new objective function as the basis of an optimization algo-
rithm to ﬁnd θ. Recall that Dπ is a |S| × |S| diagonal matrix with elements dπ
s ,

gradient-based methods
369
giving us the probability that we are in state s while following policy π. We use
Dπ as a scaling matrix to give us the probability that we are in state s. We start
by noting the identities
E[φφT ] =

s∈S
dπ
s φsφT
s
= T Dπ,
E[δπφ] =

s∈S
dπ
s φs

cπ(s) + γ

s′∈S
pπ(s′|s)V(s′|θ) −V(s|θ)

= T Dπ(MπV(θ) −V(θ)).
In writing these identities, we are slightly abusing notation by thinking of φ as a
column vector that depends on a random state S, where the expectation is being
taken over the states. In the second identity, φπ is a scalar depending on a random
state S, where again the expectation is over the states.
The derivations here and below make extensive use of matrices, which can be
difﬁcult to parse. A useful exercise is to write out the matrices, assuming that there
is a feature φf (s) for each state s so that φf (s) = 1 if feature f corresponds to
state s (see exercise 9.4).
We see that the role of the scaling matrix Dπ is to enable us to take the expecta-
tion of the quantities φφT and δπφ. Below we are going to simulate these quantities,
where a state will occur with probability dπ
s . We also use
T Dπ = ((T Dπ)−1T Dπ)T Dπ((T Dπ)−1T Dπ)
= (Dπ)T (T Dπ)−1T Dπ(T Dπ)−1T Dπ
= (Dπ)T (T Dπ)−1T Dπ.
We have one last painful piece of linear algebra that gives us a more compact form
for MSPBE(θ). Pulling the 1/2 to the left-hand side (this will later vanish when
we take the derivative), we can write
2MSPBE(θ) = ∥V(θ) −MπV(θ)∥2
D
= ∥(V(θ) −MπV(θ))∥2
D
= ((V(θ) −MπV(θ)))T Dπ((V(θ) −MπV(θ)))
= (V(θ) −MπV(θ))T T Dπ(V(θ) −MπV(θ))
= (V(θ) −MπV(θ))T (Dπ)T (T (Dπ))−1T Dπ(V(θ)
−MπV(θ))

370
learning value function approximations
= (T Dπ(MπV(θ) −V(θ)))T (T Dπ)−1T Dπ(MV(θ)
−V(θ))
= E[δπφ]T E[φφT ]−1E[δπφ].
(9.62)
We next need to estimate the gradient of this error ∇θMSPBE(θ). Keep in mind
that δπ = cπ + γ P πθ −θ, where here we interpret δπ as a vector with ele-
ment δπ
s . If φ is the column vector with element φ(s), assume that s′ occurs with
probability pπ(s′|s) under policy π, and let φ′ be the corresponding column vector.
Differentiating (9.62) gives
∇θMSPBE(θ) = E[(γ φ′ −φ)φT ]E[φφT ]−1E[δπφ]
= −E[(φ −γ φ′)φT ]E[φφT ]−1E[δπφ].
We are going to use a standard stochastic gradient updating algorithm for minimiz-
ing the error given by MSP BE(θ), which is given by
θ n+1 = θn −αn∇θMSPBE(θ)
(9.63)
= θn + αnE[(φ −γ φ′)φT ]E[φφT ]−1E[δπφ].
(9.64)
We can create a linear predictor which approximates
w ≈E[φφT ]−1E[δπφ],
where w is approximated using
wn+1 = wn + βn(δπ,n −(φn)T wn)φn.
This allows us to write the gradient
∇θMSPBE(θ) = −E[(φ −γ φ′)φT ]E[φφT ]−1E[δπφ]
≈−E[(φ −γ φ′)φT ]w.
We have now created the basis for two algorithms. The ﬁrst is called generalized
temporal difference 2 (GTD2), given by
θ n+1 = θn + αn(φn −γ φn+1)((φn)T wn).
(9.65)
Here φn is the column vector of basis functions when we are in state Sn, while
φn+1 is the column vector of basis functions for the next state Sn+1. Note that if
equation (9.65) is executed right to left, all calculations are linear in the number
of features F. This linearity property is important for algorithms which use a large
number of features. For example, recent research to develop algorithms for the
Chinese game of Go might use millions of basis functions.

least squares temporal differencing
371
A variant, called temporal difference with gradient corrector (TDC) is derived
by using a slightly modiﬁed calculation of the gradient
∇θMSPBE(θ) = −E[(φ −γ φ′)φT ]E[φφT ]−1E[δπφ]
= −

E[φφT ] −γ E[φ′φT ]

E[φφT ]−1E[δπφ]
= −

E[δπφ] −γ E[φ′φT ]E[φφT ]−1E[δπφ]

≈−

E[δπφ] −γ E[φ′φT ]w

.
This gives us the TDC algorithm
θn+1 = θn + αn

δπ,nφn −γ φn′((φn)T wn)

.
(9.66)
GTD2 and TDC have both been proven to converge to the optimal value of θ
for a ﬁxed learning policy Aπ(s) that may be different than the behavior (sampling)
policy. That is, after updating θn where the temporal difference δπ,n is computed
assuming we are in state Sn and follow policy π, we are allowed to follow a
separate sampling policy to determine Sn+1. This allows us to directly control the
states that we visit, rather than depending on the decisions made by the learning
policy. As of this writing, these algorithms are quite new and there is very little
empirical work to test stability and rate of convergence.
9.8
LEAST SQUARES TEMPORAL DIFFERENCING WITH
KERNEL REGRESSION*
In Section 8.4.2 we introduced the idea of kernel regression, where we can approx-
imate the value of a function by using a weighted sum of nearby observations. If
Si is the ith observation of a state, and we observe a value ˆvi, we can approximate
the value of visiting a generic state s by using
V(s) =
n
i=1 Kh(s, Si)ˆvi
n
i=1 Kh(s, Si) ,
where Kh(s, Si) is a weighting function that declines with the distance between s
and Si. Kernel functions are introduced in Section 8.4.2.
We now use two properties of kernels. One, which we ﬁrst introduced in section
8.4.2, is that for most kernel functions Kh(s, s′), there exists a potentially high-
dimensional set of basis functions φ(s) such that
Kh(s, s′) = φ(s)T φ(s′).
There is also a result known as the kernel representer theorem that states that there
exists a vector of coefﬁcients βi, i = 0, . . . , m that allows us to write
θm =
m

i=0
φ(Si)βi.
(9.67)

372
learning value function approximations
This allows us to write our value function approximation using
V(s) = φ(s)T θm
=
m

i=0
φ(s)T φ(Si)βi
=
m

i=0
Kh(s, Si)βi.
Recall from equations (9.43), (9.47), and (9.48) that
Amθ = bm,
where

1
m
m

i=1
φ(Si)(φ(Si) −γ φ(Si+1))T

θ = 1
m
m

i=1
φ(Si) ˆCi + εi,
with εi representing the error in the ﬁt. Substituting in (9.67) gives us

1
m
m

i=1
φ(Si)(φ(Si) −γ φ(Si+1))T

m

i=1
φiβi = 1
m
m

i=1
φ(Si) ˆCi + εi.
The single step regression function is given by
φ(Si)(φ(Si) −γ φ(Si+1))T
m

i=1
φiβi = 1
m
m

i=1
φ(Si) ˆCi + εi.
(9.68)
Let m = [φ(S1), . . . , φ(Sm)]T be a m × |F| matrix, where each row is the vector
φ(Si), and let km(Si) = [Kh(S1, Si), . . . , Kh(Sm, Si)]T be a column vector of the
kernel functions capturing the weight between each observed state S1, . . . , Sm, and
a particular state Si. Multiplying both sides of (9.68) by m gives us,
mφ(Si)(φ(Si) −γ φ(Si+1))T
m

i=1
φiβi = m 1
m
m

i=1
φ(Si) ˆCi + mεi.
Keeping in mind that products of basis functions can be replaced with kernel
functions, we obtain
m

i=1
km(Si)(km(Si) −γ km(Si+1))βm =
m

i=1
km(Si) ˆCi.

value function approximations
373
Deﬁne the m × m matrix and m-vector bm
Mm =
m

i=1
km(Si)(km(si) −γ km(Si+1)),
bm =
m

i=1
km(Si) ˆCi.
Then we can solve for βm recursively using
βm = (Mm)−1bm,
Mm+1 = Mm + km+1(Sm+1)((km+1(Sm+1))T −γ (km+2(Sm+2))T ),
bm+1 = bm + km+1(Sm+1) ˆCm.
The power of kernel regression is that it does not require specifying basis func-
tions. However, this ﬂexibility comes with a price. If we are using a parametric
model, we have to deal with a vector θ with |F| elements. Normally we try to
specify a relatively small number of features, although there are applications that
might use a million features. With kernel regression we have to invert the m × m
matrix Mm, which can become very expensive as m grows. As a result practical
algorithms would have to use advanced research on sparsiﬁcation.
9.9
VALUE FUNCTION APPROXIMATIONS BASED ON
BAYESIAN LEARNING*
A different strategy for updating value functions is one based on Bayesian learning.
Suppose that we start with a prior V 0(s) of the value of being in state s. We assume
that we have a known covariance function Cov(s, s′) that captures the relationship
in our belief about V (s) and V (s′). A good example where this function would
be known might be a function where s is continuous (or a discretization of a
continuous surface), where we might use
Cov(s, s′) ∝e−∥s−s′∥2/b,
(9.69)
where b is a bandwidth. This function captures the intuitive behavior that if two
states are close to each other, their covariance is higher. So, if we make an obser-
vation that raises our belief about V (s), then our belief about V (s′) will increase
also, and will increase more if s and s′ are close to each other. We also assume
that we have a variance function λ(s) that captures the noise in a measurement
ˆv(s) of the function at state s.
Our Bayesian updating model is designed for applications where we have access
to observations ˆvn of our true function V (s), which we can view as coming from our
prior distribution of belief. This assumption effectively precludes using updating
algorithms based on approximate value iteration, Q-learning, and least squares

374
learning value function approximations
policy evaluation. We cannot eliminate the bias, but below we describe how to
minimize it. We then describe Bayesian updating using lookup tables and parametric
models.
9.9.1
Minimizing bias
We would very much like to have observations ˆvn(s) that we can view as an
unbiased observation of V (s). One way to do this is to build on the methods
described in Section 9.1.
To illustrate, suppose that we have a policy π that determines the action at we
take when in state St, generating a contribution ˆCn
t . Say we simulate this policy
for T time periods using
ˆvn(T ) =
T

t=0
γ t ˆCt.
If we have a ﬁnite horizon problem and T is the end of our horizon, then we are
done. If our problem has an inﬁnite horizon, we can project the inﬁnite horizon
value of our policy by ﬁrst approximating the one-period contribution using
cn
T = 1
T
T

t=0
ˆCn
t .
Now suppose that this estimates the average contribution per period starting at time
T+1. Our inﬁnite horizon estimate would be
ˆvn = ˆv0(T ) + γ T +1
1
1 −γ cn
T .
Finally, we use ˆvn to update our value function approximation V
n−1 to obtain V
n.
We next illustrate the Bayesian updating formulas for lookup tables and para-
metric models.
9.9.2
Lookup Tables with Correlated Beliefs
Previously when we have used lookup tables, if we update the value V
n(s) for
some state s, we do not use this information to update the values of any other
states. With our Bayesian model, we can do much more if we have access to a
covariance function such as the one we illustrated in equation (9.69).
Suppose that we have discrete states. Assume that we have a covariance function
Cov(s, s′) in the form of a covariance matrix , where Cov(s, s′) = (s, s′). Let
V n be our vector of beliefs about the value V (s) of being in each state (we use V n
to represent our Bayesian beliefs so that V
n can represent our frequentist estimates).
Also let n be the covariance matrix of our belief about the vector V . If ˆvn(Sn)

value function approximations
375
is an (approximately) unbiased sample observation of V (s), the Bayesian formula
for updating V n is given by
V n+1(s) = V n(s) +
ˆvn(Sn) −V n(s)
λ(Sn) + n(Sn, Sn)n(s, Sn).
This has to be computed for each s (or at least each s where n(s, Sn) > 0). We
update the covariance matrix using
n+1(s, s′) = n(s, s′) −n(s, Sn)n(Sn, s′)
λ(Sn) + n(Sn, Sn) .
9.9.3
Parametric Models
For most applications a parametric model (speciﬁcally, a linear model) is going
to be much more practical. Our frequentist updating equations for our regression
vector θn were given above as
θn = θn−1 −1
γ n Bn−1φnˆεn,
(9.70)
Bn = Bn−1 −1
γ n (Bn−1φn(φn)T Bn−1),
(9.71)
γ n = 1 + (φn)T Bn−1φn,
(9.72)
where ˆεn = V(θn−1)(Sn) −ˆvn is the difference between our current estimate
V(θ n−1)(Sn) of the value function at our observed state Sn and our most recent
observation ˆvn. The adaptation for a Bayesian model is quite minor. The matrix
Bn represents
Bn = [(Xn)T Xn]−1.
It is possible to show that the covariance matrix θ (which is dimensioned by the
number of basis functions) is given by
θ = Bnλ.
In our Bayesian model, λ is the variance of the difference between our observation
ˆvn and the true value function v(Sn), where we assume λ is known. This variance
may depend on the state that we have observed, in which case we would write it as
λ(s), but in practice, since we do not know the function V (s), it is hard to believe
that we would be able to specify λ(s). We replace Bn with θ,n and rescale γ n to
create the following set of updating equations:
θn = θn−1 −1
γ n θ,n−1φnˆεn,
(9.73)
θ,n = θ,n−1 −1
γ n (θ,n−1φn(φn)T θ,n−1),
(9.74)
γ n = λ + (φn)T θ,n−1φn.
(9.75)

376
learning value function approximations
9.9.4
Creating the Prior
Approximate dynamic programming has been approached from a Bayesian per-
spective in the research literature but has apparently received very little attention
otherwise. We suspect that while there exist many applications in stochastic search
where it is valuable to use a prior distribution of belief, it is much harder to build
a prior on a value function.
Lacking any speciﬁc structural knowledge of the value function, we anticipate
that the easiest strategy will be to start with V 0(s) = v0, which is a constant across
all states. There are several strategies we might use to estimate v0. We might sample
a state Si at random, and ﬁnd the best contribution ˆCi = maxa C(Si, a). Repeat
this n times and compute
c = 1
n
n

i=1
ˆCi.
Finally, let v0 = c/(1 −γ ) if we have an inﬁnite horizon problem. The hard part is
that the variance λ has to capture the variance of the difference between v0 and the
true V (s). This requires having some sense of the degree to which v0 differs from
V (s). We recommend being very conservative, which is to say choose a variance λ
such that v0 + 2
√
λ easily covers what V (s) might be. Of course, this also requires
some judgment about the likelihood of visiting different states.
9.10
WHY DOES IT WORK*
9.10.1
Derivation of the Recursive Estimation Equations
Here we derive the recursive estimation equations given by equations (9.24) through
(9.28). To begin, we note that the matrix (Xn)T Xn is an I +1 by I +1 matrix where
the element for row i, column j is given by
[(Xn)T Xn]i,j =
n

m=1
xm
i xm
j .
This term can be computed recursively using
[(Xn)T Xn]i,j =
n−1

m=1
(xm
i xm
j ) + xn
i xn
j .
In matrix form this can be written
[(Xn)T Xn] = [(Xn−1)T Xn−1] + xn(xn)T .
Keeping in mind that xn is a column vector, and xn(xn)T is an I +1 by I +1
matrix formed by the cross products of the elements of xn. We now use the

why does it work*
377
Sherman–Morrison formula (see Section 9.10.2 for a derivation) for updating the
inverse of a matrix
[A + uuT ]−1 = A−1 −A−1uuT A−1
1 + uT A−1u,
(9.76)
where A is an invertible n × n matrix, and u is an n-dimensional column vector.
Applying this formula to our problem, we obtain
[(Xn)T Xn]−1 = [(Xn−1)T Xn−1 + xn(xn)T ]−1
= [(Xn−1)T Xn−1]−1
−[(Xn−1)T Xn−1]−1xn(xn)T [(Xn−1)T Xn−1]−1
1 + (xn)T [(Xn−1)T Xn−1]−1xn
.
(9.77)
The term (Xn)T Y n can also be updated recursively using
(Xn)T Y n = (Xn−1)T Y n−1 + xn(yn).
(9.78)
To simplify the notation, let
Bn = [(Xn)T Xn]−1,
γ n = 1 + (xn)T [(Xn−1)T Xn−1]−1xn.
This simpliﬁes our inverse updating equation (9.77) to
Bn = Bn−1 −1
γ n (Bn−1xn(xn)T Bn−1).
Recall that
θ
n = [(Xn)T Xn]−1(Xn)T Y n.
(9.79)
Combining (9.79) with (9.77) and (9.78) gives
θ
n = [(Xn)T Xn]−1(Xn)T Y n
=

Bn−1 −1
γ n (Bn−1xn(xn)T Bn−1)
 
(Xn−1)T Y n−1 + xnyn
,
= Bn−1(Xn−1)T Y n−1
−1
γ n Bn−1xn(xn)T Bn−1 
(Xn−1)T Y n−1 + xnyn
+ Bn−1xnyn.
We can start to simplify by using θ
n−1 = Bn−1(Xn−1)T Y n−1. We are also going to
bring the term xnBn−1 inside the square brackets. Finally, we are going to bring the
last term Bn−1xnyn inside the brackets by taking the coefﬁcient Bn−1xn outside the

378
learning value function approximations
brackets and multiplying the remaining yn by the scalar γ n = 1 + (xn)T Bn−1xn,
giving us
θ
n = θ
n−1 −1
γ n Bn−1xn 
(xn)T (Bn−1(Xn−1)T Y n−1)
+ (xn)T Bn−1xnyn −(1 + (xn)T Bn−1xn)yn
.
Again, we use θ
n−1 = Bn−1(Xn−1)T Y n−1 and observe that there are two terms
(xn)T Bn−1xnyn that cancel, leaving
θ
n = θ
n−1 −1
γ n Bn−1xn 
(xn)T θ
n−1 −yn
.
We note that (θ
n−1)T xn is our prediction of yn using the parameter vector from iter-
ation n −1 and the explanatory variables xn. yn is, of course, the actual observation,
so our error is given by
ˆεn = yn −(θ
n−1)T xn.
Let
H n = −1
γ n Bn−1.
We can now write our updating equation using
θ
n = θ
n−1 −H nxnˆεn.
(9.80)
9.10.2
The Sherman–Morrison Updating Formula
The Sherman–Morrison matrix updating formula (also known as the Woodbury
formula or the Sherman–Morrison–Woodbury formula) assumes that we have a
matrix A and that we are going to update it with the outer product of the column
vector u to produce the matrix B, given by
B = A + uuT .
(9.81)
Pre-multiply by B−1 and post-multiply by A−1 to obtain
A−1 = B−1 + B−1uuT A−1.
(9.82)
Post-multiply by u:
A−1u = B−1u + B−1uuT A−1u
= B−1u

1 + uT A−1u

.

bibliographic notes
379
Note that uT A−1u is a scalar. Divide through by

1 + uT A−1u

:
A−1u

1 + uT A−1u
 = B−1u.
Now post-multiply by uT A−1:
A−1uuT A−1

1 + uT A−1u
 = B−1uuT A−1.
(9.83)
Equation (9.82) gives us
B−1uuT A−1 = A−1 −B−1.
(9.84)
Substituting (9.84) into (9.83) gives
A−1uuT A−1

1 + uT A−1u
 = A−1 −B−1.
(9.85)
Solving for B−1 gives us
B−1 = [A + uuT ]−1
= A−1 −A−1uuT A−1

1 + uT A−1u
,
which is the desired formula.
9.11
BIBLIOGRAPHIC NOTES
Section 9.1 This section reviews a number of classical methods for estimating
the value of a policy drawn from the reinforcement learning community.
The best overall reference for this is Sutton and Barto (1998). Least squares
temporal differencing is due to Bradtke and Barto (1996).
Section 9.2
Tsitsiklis (1994) and Jaakkola et al. (1994) were the ﬁrst to make
the connection between emerging algorithms in approximate dynamic pro-
gramming (Q-learning, temporal difference learning) and the ﬁeld of stochas-
tic approximation theory (Robbins and Monro (1951), Blum (1954a), Kushner
and Yin (2003)).
Section 9.3
Ljung and Soderstrom (1983) and Young (1984) provide nice
treatments of recursive statistics. Precup et al. (2001) gives the ﬁrst conver-
gent algorithm for off-policy temporal-difference learning by way of basis
functions using an adjustment that is based on the relative probabilities of
choosing an action from the target and behavioral policies. Lagoudakis et al.
(2002) and Bradtke and Barto (1996) presents least squares methods in the
context of reinforcement learning. Choi and Van (2006) uses the Kalman ﬁlter

380
learning value function approximations
to perform scaling for stochastic gradient updates, avoiding the scaling prob-
lems inherent in stochastic gradient updates such as equation (9.22). Bertsekas
and Nedic¸ (2003) describes the use of least squares equation with a linear
(in the parameters) value function approximation using policy iteration and
proves convergence for TD(λ) with general λ. Bertsekas et al. (2004) presents
a scaled method for estimating linear value function approximations within a
temporal-differencing algorithm. Section 9.3.4 is based on Soderstrom et al.
(1978).
Section 9.4
The issue of whether TD learning with a linear approximation
architecture was convergent was debated in the literature for some time as a
result of a mixture of supporting and contradictory results. The fundamental
question was largely resolved in Tsitsiklis and Van Roy (1997); see the ref-
erences there for more of the history. Precup et al. (2001) provides the ﬁrst
convergent off-policy version of a TD model with a linear approximation,
but this is limited to policies that retain a positive probability of selecting any
action.
Section 9.5
The development of Bellman’s equation using linear models is
based on Tsitsiklis and Van Roy (1997), Lagoudakis and Parr (2003), and
Bertsekas (2009). Tsitsiklis and Van Roy (1997) highlights the central role of
the D-norm used in this section, which also plays a central role in the design
of a simulation-based version of the algorithm.
Section 9.6
The analysis of dynamic programs with a single state is based on
Ryzhov et al. (2009).
Section 9.7
Baird (1995) provides a nice example showing that approximate
value iteration may diverge when using a linear architecture, even when the
linear model may ﬁt the true value function perfectly. Tsitsiklis and Van
Roy (1997) establishes the importance of using Bellman errors weighted by
the probability of being in a state. De Farias and Van Roy (2000) shows
that there does not necessarily exist a ﬁxed point to the projected form of
Bellman’s equation θ = Mθ, where M is the max operator. This paper
also shows that a ﬁxed point does exist for a projection operator D deﬁned
with respect to the norm ∥· ∥D that weights a state s with the probability ds
of being in this state. This result is ﬁrst shown for a ﬁxed policy, and then
for a class of randomized policies. GTD2 and TDC are due to Sutton et al.
(2009a), with material from Sutton et al. (2009b).
Section 9.8
Our adaptation of least squares temporal differencing using kernel
regression was presented in Ma and Powell (2010a).
Section 9.9
Dearden et al. (1998) introduces the idea of using Bayesian
updating for Q-learning. Dearden et al. (1999) then considers model-based
Bayesian learning. Our presentation is based on Ryzhov and Powell (2010b),
which introduces the idea of correlated beliefs.
Section 9.10.2
The Sherman–Morrison updating formulas are given in a num-
ber of references, such as Ljung and Soderstrom (1983) and Golub and Loan
(1996).

problems
381
PROBLEMS
9.1
Consider a “Markov decision process” with a single state and single policy.
Assume that we do not know the expected value of the contribution ˆC, but
each time it is sampled, we draw a sample realization from the uniform
distribution between 0 and 20. Also assume a discount factor of γ = 0.90.
Let V = ∞
t=0 γ t ˆCt. The exercises below can be formed in a spreadsheet.
(a) Estimate V using LSTD using 100 iterations.
(b) Estimate V using LSPE using 100 iterations.
(c) Estimate V using recursive least squares, executing the algorithm for 100
iterations.
(d) Estimate V using temporal differencing (approximate value iteration) and
a stepsize of 1/n.7.
(e) Repeat (d) using a stepsize of 5/(5 + n −1).
(f) Compare the rates of convergence of the different procedures.
9.2
Repeat the exercise above using a discount factor of 0.95.
9.3
Contrast the objective function used in equation (9.39) with the objective
being minimized that we used throughout Chapter 3.
9.4
We are going to walk through the derivation of the equations in Section 9.7
assuming that there is a feature for each state, where φf (s) = 1 if feature f
corresponds to state s, and 0 otherwise. When asked for a sample of a vector
or matrix, we assume there are three states and three features. As above, let
dπ
s be the probability of being in state s under policy π, and let Dπ be the
diagonal matrix consisting of the elements dπ
s .
(a) What is the column vector φ if s = 1? What does φφT look like?
(b) If dπ
s
is the probability of being in state s under policy π, write out
E[φφT ].
(c) Write out the matrix .
(d) What is the projection matrix ?
(e) Write out equation (9.62) for MSPBE(θ).
9.5
Repeat the derivation of (9.7), but this time introduce a discount factor γ .
Show that you obtain it by updating equation (9.8).


C H A P T E R
10
Optimizing While Learning
We are ﬁnally ready to tackle the problem of searching for good policies while
simultaneously trying to produce good value function approximations. Our dis-
cussion is restricted to problems where the policy is based on the value function
approximation, since Chapter 7 has already addressed the strategy of direct policy
search. The guiding principle in this chapter is that we can ﬁnd good policies if
we can ﬁnd good value function approximations.
The statistical tools presented in Chapters 8 and 9 focused on ﬁnding the best
statistical ﬁt within a particular approximation architecture. We actually did not
address whether we had chosen a good architecture. This is particularly true of
our linear models, where we used the tools of stochastic optimization and linear
regression to ensure that we obtain the best ﬁt given a model, without regard to
whether it was a good model.
In this chapter we return to the problem of trying to ﬁnd the best policy, where
we assume throughout that our policies are of the form
Aπ(S) = arg max
a∈A

C(S, a) + γ EV(SM(S, a, W))

,
if we have an inﬁnite horizon problem with discrete actions. Or, we may consider
ﬁnite horizon problems with vector-valued decisions xt, where a policy would look
like
Xπ
t (St) = arg max
xt∈Xt

C(St, xt) + γ EVt(SM(St, xt, Wt+1))

.
The point is that the policy depends on some sort of value function approximation.
When we write our generic optimization problem
max
π
E
T

t=0
γ tC(St, Aπ
t (St)),
Approximate Dynamic Programming: Solving the Curses of Dimensionality, Second Edition.
Warren B. Powell.
2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
383

384
optimizing while learning
the maximization over policies can mean choosing an architecture for Vt(St), and
choosing the parameters that control the architecture. For example, we might be
choosing between a myopic policy (see equation (6.4) in Chapter 6), or perhaps a
simple linear architecture with one basis function
V(S) = θ0 + θ1S,
(10.1)
or perhaps a linear architecture with two basis functions,
V(S) = θ0 + θ1S + θ2S2.
(10.2)
We might even use a nonlinear architecture such as
V(S) =
eθ0+θ1S
1 + eθ0+θ1S .
Optimizing over policies may consist of choosing a value function approximation
such as (10.1) or (10.2), but then we still have to choose the best parameter vector
within each class.
We begin our presentation with an overview of the basic algorithmic strategies
that we cover in this chapter, all of which are based on using value function
approximations that are intended to approximate the value of being in a state. The
remainder of the chapter is organized around covering the following strategies:
Approximate value iteration. These are policies that iteratively update the
value function approximation, and then immediately update the policy. We
strive to ﬁnd a value function approximation that estimates the value of being
in each state while following a (near) optimal policy, but only in the limit. We
intermingle the treatment of ﬁnite and inﬁnite horizon problems. Variations
include
Lookup table representations. Here we introduce three major strategies that
reﬂect the use of the pre-decision state, state-action pairs, and the post-
decision state:
AVI for pre-decision state. Approximate value iteration using the classical
pre-decision state variable.
Q-learning. Estimating the value of state-action pairs.
AVI for the post-decision state. Approximate value iteration where value
function approximations are approximated around the post-decision state.
Parametric architectures. We discuss the issues that arise when trying to use
linear models in the context of approximate value iteration.
Approximate policy iteration. These are policies that attempt to explicitly
approximate the value of a policy to some level of accuracy within an inner
loop, within which the policy is held ﬁxed.
API using lookup tables. We use this setting to present the basic idea.
API using linear models. This is perhaps one of the most important areas
of research in approximate dynamic programming.

overview of algorithmic strategies
385
API using nonparametric models. This is a relatively young area of
research, and we summarize some recent results.
Linear programming method. The linear programming method, ﬁrst intro-
duced in Chapter 3, can be adapted to exploit value function approximations.
10.1
OVERVIEW OF ALGORITHMIC STRATEGIES
The algorithmic strategies that we examine in this chapter are based on the prin-
ciples of value iteration and policy iteration, ﬁrst introduced in Chapter 3. We
continue to adapt our algorithms to ﬁnite and inﬁnite horizons. Basic value iteration
for ﬁnite horizon problems work by solving
Vt(St) = max
at

C(St, at) + γ E{Vt+1(St+1)|St}

.
(10.3)
Equation (10.3) works by stepping backward in time, where Vt(St) is computed
for each (presumably discrete) state St. This is classical “backward” dynamic pro-
gramming that suffers from the well-known curse of dimensionality because we
typically are unable to “loop over all the states.”
Approximate dynamic programming approaches ﬁnite horizon problems by solv-
ing problems of the form
ˆvn
t = max
at

C(Sn
t , at) + γV
n−1
t+1 (SM,a(Sn
t , at))

.
(10.4)
Here we have formed the value function approximation around the post-decision
state. We execute the equations by stepping forward in time. If an
t is the action that
optimizes (10.4), then we compute our next state using Sn
t+1 = SM(Sn
t , an
t , W n
t+1),
where W n
t+1 is sampled from some distribution. The process runs until we reach
the end of our horizon, at which point we return to the beginning of the horizon
and repeat the process.
Classical value iteration for inﬁnite horizon problems is centered on the basic
iteration
V n(S) = max
a

C(S, a) + γ E{V n−1(S′)|S}

.
(10.5)
Again, equation (10.5) has to be executed for each state S. After each iteration
the new estimate V n replaces the old estimate V n−1 on the right, after which n is
incremented.
When we use approximate methods, we might observe an estimate of the value
of being in a state using
ˆvn = max
a

C(Sn, a) + γV
n−1(SM,a(Sn, an))

.
(10.6)
We then use the observed state-value pair (Sn, ˆvn) to update the value function
approximation.

386
optimizing while learning
When we use approximate value iteration, ˆvn (or ˆvn
t ) cannot be viewed as a
noisy but unbiased observation of the value of being in a state. These observations
are calculated as a function of the value function V
n−1(s). While we hope the
value function approximation converges to something, we generally cannot say
anything about the function prior to convergence. This means that ˆvn does not
have any particular property. We are simply guided by the basic value iteration
update in equation (10.5) (or (10.3)), which suggests that if we repeat this step
often enough, we may eventually learn the right value function for the right policy.
Unfortunately, we have only limited guarantees that this is the case when we depend
on approximations.
Approximate value iteration embeds a policy approximation loop within an outer
loop where policies are updated. Suppose that we ﬁx our policy using
Aπ,n(S) = arg max
a∈A

C(S, a) + γV
n−1(SM,a(S, a))

.
(10.7)
Now perform the loop over m = 1, . . . , M:
ˆvn,m = max
a∈A

C(Sn,m, a) + γV
n−1(SM,a(Sn,m, a))

,
where Sn,m+1 = SM(Sn,m, an,m, W n,m+1). Note that the value function V
n−1(s)
remains constant within this inner loop. After executing this loop, we take the series
of observations ˆvn,1, . . . , ˆvn,M and use them to update V
n−1(s) to obtain V
n(s).
Typically V
n(s) does not depend on V
n−1(s), other than to inﬂuence the calculation
of ˆvn,m. If M is large enough, V
n(s) will represent an accurate approximation of the
value of being in state s while following the policy in equation (10.7). In fact it is
speciﬁcally because of this ability to approximate a policy that approximate policy
iteration is emerging as a powerful algorithmic strategy for approximate dynamic
programming. However, the cost of using the inner policy evaluation loop can be
signiﬁcant, and for this reason approximate value iteration and its variants remain
popular.
10.2
APPROXIMATE VALUE ITERATION AND Q-LEARNING
USING LOOKUP TABLES
Arguably the most natural and elementary approach for approximate dynamic pro-
gramming uses approximate value iteration. In this section we explore variations
of approximate value iteration and Q-learning.
10.2.1
Value Iteration Using a Pre-decision State Variable
Classical value iteration (for a ﬁnite-horizon problem) estimates the value of being
in a speciﬁc state Sn
t using
ˆvn
t = max
at

C(Sn
t , at) + γ E{V
n−1
t+1 (St+1)|Sn
t }

,
(10.8)

approximate value iteration and q-learning
387
where St+1 = SM(Sn
t , xt, W n
t+1), and Sn
t is the state that we are in at time t,
iteration n. We assume that we are following a sample path ωn, where we compute
W n
t+1 = Wt+1(ωn). After computing ˆvn
t , we update the value function using the
standard equation
V
n
t (Sn
t ) = (1 −αn−1)V
n−1(Sn
t ) + αn−1 ˆvn
t .
(10.9)
If we sample states at random (rather than following the trajectory) and repeat
equations (10.8) and (10.9), we will eventually converge to the correct value of
being in each state. Note that we are assuming a ﬁnite horizon model, and that
we can compute the expectation exactly. When we can compute the expectation
exactly, this is very close to classical value iteration, with the only exception that
we are not looping over all the states at every iteration.
One reason to use the pre-decision state variable is that for some problems,
computing the expectation is easy. For example, Wt+1 might be a binomial random
variable (did a customer arrive, did a component fail), which makes the expectation
especially easy. If this is not the case, then we have to approximate the expectation.
For example, we might use
ˆvn
t = max
at

C(Sn
t , at) + γ

ˆω∈ˆn
pn( ˆω)V
n−1
t+1 (SM(Sn
t , at, Wt+1( ˆω)))

.
(10.10)
Either way, using a lookup table representation we can update the value of being in
state Sn
t using equation (10.9). Keep in mind that if we can compute an expectation
(or if we approximate it using a large sample ˆ), then the stepsize should be
much larger than when we are using a single sample realization (as we did with
the post-decision formulation). An outline of the overall algorithm is given by
Figure 10.1.
At this point a reasonable question to ask is: Does this algorithm work? The
answer is possibly, but not in general. Before we get an algorithm that will work (at
least in theory), we need to deal with what is known as the exploration-exploitation
problem.
10.2.2
On-policy, Off-policy, and the Exploration–Exploitation Problem
The algorithm in Figure 10.1 uses a kind of default logic for determining the next
state to visit. Speciﬁcally, we solve the optimization problem in equation (10.10),
and from this we not only determine ˆvn
t , which we use to update the value of being
in a state, we also determine an action an
t . Then, in step 2b of the algorithm, we
use this action to help determine the next state to visit using the transition function
Sn
t+1 = SM(Sn
t , an
t , W n
t+1).

388
optimizing while learning
Step 0. Initialization
Step 0a. Initialize V
0
t , t ∈T.
Step 0b. Set n = 1.
Step 0c. Initialize S1
0.
Step 1. Sample ωn.
Step 2. Do for t = 0, 1, . . . , T :
Step 2a: Choose ˆn ⊆ and solve
ˆvn
t = max
at

Ct(Sn−1
t
, at) + γ

ˆω∈ˆn
pn( ˆω)V
n−1
t+1 (SM(Sn−1
t
, at, Wt+1( ˆω))

,
and let an
t be the value of at that solves the maximization problem.
Step 2b: Compute
Sn
t+1 = SM(Sn
t , an
t , Wt+1(ωn)).
Step 2c. Update the value function
V
n
t ←UV (V
n−1
t
, Sn
t , ˆvn
t ).
Step 3. Increment n. If n ≤N, go to step 1.
Step 4. Return the value functions (V
n
t )T
t=1.
Figure 10.1
Approximate dynamic programming using a pre-decision state variable.
Using the action an
t , which is the action determined by the policy we are trying
to optimize, means that we are using a concept known as trajectory following.
The policy that determines the action we would like to take is known in the
reinforcement learning community as the target policy, but we prefer the alternate
term learning policy. When we are optimizing policies, what we are doing is trying
to improve the learning policy.
We can encounter serious problems if we use the learning policy to deter-
mine the next state to visit. Consider the two-state dynamic program illustrated in
Figure 10.2. Assume that we start in state 1, and further assume that we initialize
the value of being in each of the two states to V
0(1) = V
0(2) = 0. We see a neg-
ative contribution of −$5 to move from state 1 to 2, but a contribution of $0 to
stay in state 1. We do not see the contribution of $20 to move from state 2 back
to state 1, so it appears to be best to stay in state 1.
We need some way to force the system to visit state 2 so that we discover the
contribution of $20. One way to do this is to adopt logic that forces the system
to explore by choosing actions at random. For example, we may ﬂip a coin and
choose an action with probability ϵ, or choose the action an
t determined by the
learning policy with probability 1 −ϵ. This policy is known in the literature as
epsilon greedy.

approximate value iteration and q-learning
389
–$5
$20
$0
$0
1
2
Figure 10.2
Two-state dynamic program, with transition contributions.
The policy that determines which action to use to determine the next state to
visit, if it is different than the learning policy, is known as the behavior policy or the
sampling policy. The name “behavior policy” arises when we are modeling a real
system such as a human playing a game or a factory assembling components. The
behavior policy is, literally, the policy that describes how the system behaves. By
contrast, if we are simply designing an algorithm, we feel that the term “sampling
policy” more accurately describes the actual function being served by this policy.
We also note that while it is common to implement a sampling policy through the
choice of action, we may also simply choose a state at random.
If the learning policy also determines the next state we visit, then we say that
the algorithm is on policy. If the sampling policy is different than the learning
policy, then we say that the algorithm is off policy, which means that the policy
that we use to determine the next state to visit does not follow the policy we are
trying to optimize.
In the remainder of this chapter, we are going to distinguish on-policy and off-
policy algorithms. However, we defer to Chapter 12 a more complete discussion
of the different policies that can be used.
10.2.3
Q-Learning
We ﬁrst introduced Q-learning and its cousin SARSA in Chapter 4, and we refer
the reader back to this presentation for an introduction to this important algorithmic
strategy. For completeness, we present a version of the full algorithm in Figure 10.3
for a time-dependent, ﬁnite horizon problem.
10.2.4
Value Iteration Using a Post-decision State Variable
For the many applications that lend themselves to a compact post-decision state
variable, it is possible to adapt approximate value iteration to value functions esti-
mated around the post-decision state variable. At the heart of the algorithm we
choose actions (and estimate the value of being in state Sn
t ) using
ˆvn
t = arg max
at∈At

C(Sn
t , at) + γV
n−1
t
(SM,a(Sn
t , at))

.

390
optimizing while learning
Step 0. Initialization.
Step 0a. Initialize an approximation for the value functionQ
0
t (St, at) for all states St and
decisions at ∈At, t = {0, 1, . . . , T }.
Step 0b. Set n = 1.
Step 0c. Initialize S1
0.
Step 1. Choose a sample path ωn.
Step 2. Do for t = 0, 1, . . . , T :
Step 2a: Determine the action using ϵ-greedy. With probability ϵ, choose an action
an at random from A. With probability 1 −ϵ, choose an using
an
t = arg max
at ∈At
Q
n−1
t
(Sn
t , at).
Step 2b. Sample W n
t+1 = Wt+1(ωn) and compute the next state Sn
t+1 = SM(Sn
t , an
t ,
W n
t+1).
Step 2c. Compute
ˆqn
t = C(Sn
t , an
t ) + γ
max
at+1∈At+1
Q
n−1
t+1 (Sn
t+1, at+1).
Step 2d. UpdateQ
n−1
t
and V
n−1
t
using
Q
n
t (Sn
t , an
t ) = (1 −αn−1)Q
n−1
t
(Sn
t , an
t ) + αn−1 ˆqn
t .
Step 3. Increment n. If n ≤N, go to step 1.
Step 4. Return the Q-factors (Q
n
t )T
t=1.
Figure 10.3
Q-learning algorithm.
The distinguishing feature when we use the post-decision state variable is that the
maximization problem is now deterministic. The key step is how we update the
value function approximation. Instead of using ˆvn
t to update a pre-decision value
function approximation V
n−1(Sn
t ), we use ˆvn
t to update a post-decision value func-
tion approximation around the previous post-decision state Sa,n
t−1. This is done using
V
n
t−1(Sa,n
t−1) = (1 −αn−1)V
n−1
t−1 (Sa,n
t−1) + αn−1 ˆvn
t .
The post-decision state not only allows us to solve deterministic optimization prob-
lems, there are many applications where the post-decision state has either the same
dimensionality as the pre-decision state, or, for some applications, a much lower
dimensionality. Examples were given in Section 4.6. A complete summary of the
algorithm is given in Figure 10.4.
Q-learning shares certain similarities with dynamic programming using a post-
decision value function. In particular, both require the solution of a deterministic
optimization problem to make a decision. However, Q-learning accomplishes this
goal by creating an artiﬁcial post-decision state given by the state/action pair (S, a).

approximate value iteration and q-learning
391
Step 0. Initialization.
Step 0a. Initialize an approximation for the value function V
0
t (Sa
t ) for all post-decision
states Sa
t , t = {0, 1, . . . , T }.
Step 0b. Set n = 1.
Step 0c. Initialize Sa,1
0 .
Step 1. Choose a sample path ωn.
Step 2. Do for t = 0, 1, . . . , T .
Step 2a: Determine the action using ϵ-greedy. With probability ϵ, choose an action an
at random from A. With probability 1 −ϵ, choose an using
ˆvn
t = arg max
at ∈At

C(Sn
t , at) + γ V
n−1
t
(SM,a(Sn
t , at))

.
Let an
t be the action that solves the maximization problem.
Step 2b. Update V
n−1
t−1 using
V
n
t−1(Sa,n
t−1) = (1 −αn−1)V
n−1
t−1 (Sa,n
t−1) + αn−1 ˆvn
t .
Step 2c. Sample W n
t+1 = Wt+1(ωn) and compute the next state Sn
t+1 = SM(Sn
t , an
t ,
W n
t+1).
Step 3. Increment n. If n ≤N, go to step 1.
Step 4. Return the value functions (V
n
t )T
t=1.
Figure 10.4
Approximate value iteration for ﬁnite horizon problems using the post-decision state
variable.
We then have to learn the value of being in (S, a), rather than the value of being
in state S alone (which is already very hard for most problems).
If we compute the value function approximationV
n(Sa) around the post-decision
state Sa = SM,a(S, a), we can create Q-factors directly from the contribution func-
tion and the post-decision value function using
Q
n(S, a) = C(S, a) + γV
n
t (SM,a(S, a)).
Viewed this way, approximate value iteration using value functions estimated
around a post-decision state variable is equivalent to Q-learning. However, if the
post-decision state is compact, then estimating V(Sa) is much easier than estimating
Q(S, a).
10.2.5
Value Iteration Using a Backward Pass
Classical approximate value iteration, which is equivalent to temporal-difference
learning with λ = 0 (also known as TD(0)), can be implemented using a pure
forward pass, which enhances its simplicity. However, there are problems where
it is useful to simulate decisions moving forward in time, and then updating value
functions moving backward in time. This is also known as temporal-difference

392
optimizing while learning
Step 0. Initialization.
Step 0a. Initialize V
0
t , t ∈T.
Step 0b. Initialize S1
0.
Step 0c. Choose an initial policy Aπ,0.
Step 0d. Set n = 1.
Step 1. Repeat for m = 1, 2, . . . , M.
Step 1. Choose a sample path ωm.
Step 2. Do for t = 0, 1, 2, . . . , T .
Step 2a. Find
an,m
t
= Aπ,n−1(Sn,m
t
).
Step 2b. Update the state variable
Sn,m
t+1 = SM(Sn,m
t
, an,m
t
, Wt+1(ωm)).
Step 3. Set ˆvn,m
T +1 = 0, and do for t = T, T −1, . . . , 1,
ˆvn,m
t
= C(Sn,m
t
, an,m
t
) + γ ˆvn,m
t+1.
Step 4. Compute the average value from starting in state S1
0:
vn
0 = 1
M
M

m=1
ˆvn,m
0
.
Step 5. Update the value function approximation by using the average values
V
n
0 ←UV (V
n−1
0
, Sa,n
0 , vn
0).
Step 6. Update the policy
Aπ,n(S) = arg max
a∈A

C(Sn
0, a) + γV
n
0(SM,a(Sn
0, a))

.
Step 6. Increment n. If n ≤N go to step 1.
Step 7. Return the value functions (V
n
t )T
t=1.
Figure 10.5
Double-pass version of the approximate dynamic programming algorithm for a ﬁnite
horizon problem.
learning with λ = 1, but we ﬁnd “backward pass” to be more descriptive. The
algorithm is depicted in Figure 10.5.
In this algorithm we step forward through time creating a trajectory of states,
actions, and outcomes. We then step backward through time, updating the value
of being in a state using information from the same trajectory in the future. We
are going to use this algorithm to also illustrate ADP for a time-dependent, ﬁnite
horizon problem. In addition we are going to illustrate a form of policy evaluation.
Pay careful attention to how variables are indexed.

approximate value iteration and q-learning
393
The idea of stepping backward through time to produce an estimate of the value
of being in a state was ﬁrst introduced in the control theory community under the
name of backpropagation through time (BTT). The result of our backward pass
is ˆvn
t , which is the contribution from the sample path ωn and a particular policy.
Our policy is, quite literally, the set of decisions produced by the value function
approximation V
n−1
t
(Sa
t ). Unlike our forward-pass algorithm (where ˆvn
t depends on
the approximation V
n−1
t
(Sa
t ), ˆvn
t is a valid, unbiased estimate of the value of being
in state Sn
t at time t and following the policy produced by V
n−1.
We introduce an inner loop so that rather than updating the value function
approximation with a single ˆvn
0, we average across a set of samples to create a
more stable estimate, vn
0.
These two strategies are easily illustrated using our simple asset-selling problem.
For this illustration we are going to slightly simplify the model we provided earlier,
where we assumed that the change in price, ˆpt, was the exogenous information.
If we use this model, we have to retain the price pt in our state variable (even
the post-decision state variable). For our illustration we are going to assume that
the exogenous information is the price itself, so that pt = ˆpt. We further assume
that ˆpt is independent of all previous prices (a pretty strong assumption). For this
model the pre-decision state is St = (Rt, pt) while the post-decision state variable
is simply Sa
t = Ra
t = Rt −at, which indicates whether we are holding the asset or
not. Further St+1 = Sa
t , since the resource transition function is deterministic.
With this model, a single-pass algorithm (approximate value iteration) is per-
formed by stepping forward through time, t = 1, 2, . . . , T . At time t we ﬁrst
sample ˆpt and we ﬁnd
ˆvn
t = max
at∈{0,1}

ˆpn
t at + (1 −at)(−ct + vn−1
t
)

.
(10.11)
Assume that the holding cost ct = 2 for all time periods.
Table 10.1 illustrates three iterations of a single-pass algorithm for a three-period
problem. We initialize v0
t = 0 for t = 0, 1, 2, 3. Our ﬁrst decision is a1 after we
see ˆp1. The ﬁrst column shows the iteration counter, while the second shows the
stepsize αn−1 = 1/n. For the ﬁrst iteration we always choose to sell because v0
t = 0,
which means that ˆv1
t = ˆp1
t . Since our stepsize is 1.0, this produces v1
t−1 = ˆp1
t for
each time period.
Table 10.1
Illustration of a single-pass algorithm
t = 0
t = 1
t = 2
t = 3
Iteration
αn−1
v0
ˆv1
ˆp1
a1
v1
ˆv2
ˆp2
a2
v2
ˆv3
ˆp3
a3
v3
0
0
0
0
0
1
1
30
30
30
1
34
34
34
1
31
31
31
1
0
2
0.50
31
32
24
0
31.5
29
21
0
29.5
30
30
1
0
3
0.3
32.3
35
35
1
30.2
27.5
24
0
30.7
33
33
1
0

394
optimizing while learning
In the second iteration our ﬁrst decision problem is
ˆv2
1 = max{ ˆp2
1, −c1 + v1
1}
= max{24, −2 + 34}
= 32,
which means that a2
1 = 0 (since we are holding the asset). We then use ˆv2
1 to update
v2
0:
v2
0 = (1 −α1)v1
0 + α1 ˆv1
1
= (0.5)30.0 + (0.5)32.0
= 31.0
Repeating this logic, we hold again for t = 2 but we always sell at t = 3, since
this is the last time period. In the third pass we again sell in the ﬁrst time period,
but hold for the second time period.
It is important to recognize that this problem is quite simple, and we do not
have to deal with exploration issues. If we sell, we are no longer holding the asset,
and the forward pass should stop (more precisely, we should continue to simulate
the process given that we have sold the asset). Instead, even if we sell the asset,
we step forward in time and continue to evaluate the state that we are holding the
asset (the value of the state where we are not holding the asset is, of course, zero).
Normally we evaluate only the states that we transition to (see step 2b), but for
this problem we are actually visiting all the states (since there is in fact only one
state that we really need to evaluate).
Now consider a double-pass algorithm. Table 10.2 illustrates the forward pass,
followed by the backward pass, where for simplicity we are going to use only
a single inner iteration (M = 1). Each line of the table only shows the numbers
determined during the forward or backward pass. In the ﬁrst pass, we always sell
(since the value of the future is zero), which means that at each time period the
value of holding the asset is the price in that period.
In the second pass, it is optimal to hold for two periods until we sell in the
last period. The value ˆv2
t for each time period is the contribution of the rest of the
Table 10.2
Illustration of a double-pass algorithm
t = 0
t = 1
t = 2
t = 3
Iteration
Pass
v0
ˆv1
ˆp1
a1
v1
ˆv2
ˆp2
a2
v2
ˆv3
ˆp3
a3
v3
0
0
0
0
0
1
Forward
→
→
30
1
→
→
34
1
→
→
31
1
1
Back
30
30
←
←
34
34
←
←
31
31
←
←
0
2
Forward
→
→
24
0
→
→
21
0
→
→
27
1
2
Back
26.5
23
←
←
29.5
25
←
←
29
27
←
←
0

approximate value iteration and q-learning
395
trajectory, which in this case is the price we receive in the last time period. So, since
a1 = a2 = 0 followed by a3 = 1, the value of holding the asset at time 3 is the $27
price we receive for selling in that time period. The value of holding the asset at time
t = 2 is the holding cost of −2 plus ˆv2
3, giving ˆv2
2 = −2 + ˆv2
3 = −2 + 27 = 25.
Similarly, holding the asset at time 1 means that ˆv2
1 = −2 + ˆv2
2 = −2 + 25 = 23.
The smoothing of ˆvn
t with vn−1
t−1 to produce vn
t−1 is the same as for the single-pass
algorithm.
The value of implementing the double-pass algorithm depends on the problem.
For example, imagine that our asset is an expensive piece of replacement equipment
for a jet aircraft. We hold the part in inventory until it is needed, which could
literally be years for certain parts. This means there could be hundreds of time
periods (if each time period is a day) where we are holding the part. Estimating
the value of the part now (which would determine whether we order the part to
hold in inventory) using a single-pass algorithm could produce extremely slow
convergence. A double-pass algorithm would work dramatically better. But if the
part is used frequently, staying in inventory for only a few days, then the single-pass
algorithm will work ﬁne.
10.2.6
Value Iteration for Multidimensional Decision Vectors
In the previous section we saw that the use of the post-decision state variable
meant that the process of choosing the best action required solving a deterministic
optimization problem. This opens the door to considering problems where the
decision is a vector xt. For example, imagine that we have a problem of assigning
an agent in a multiskill call center to customers requiring help with their computers.
An agent of type i might have a particular set of language and technical skills.
A customer has answered a series of automated questions, which we capture with
a label j. Let
Rti = number of agents available at time t with skill set i,
Dtj = number of customers waiting at time t whose queries
are characterized by j.
We let Rt = (Rti)i and Dt = (Dtj)j, and ﬁnally let our state variable be St =
(Rt, Dt). Let our decision vector be deﬁned using
xtij = number of agents of type i who are assigned to
customers of type j at time t,
xt = (xtij)i,j.
For realistic problems, it is easy to create vectors xt with hundreds or thousands
of dimensions. Finally, let
cij = estimated time required for an agent of type i
to serve a customer of type j.

396
optimizing while learning
The state variables evolve according to
Rt+1,i = Rti −

j
xtij + ˆRt+1,i,
Dt+1,j = Dtj −

i
xtij + ˆDt+1,j.
Here ˆRt+1,i represents the number of agents that were busy but that became idle
between t and t + 1 because they completed their previous assignment. ˆDt+1,j
represents arrival of new customers. We note that Rt and Dt are pre-decision state
variables. Their post-decision counterparts are given by
Rx
t+1,i = Rti −

j
xtij,
(10.12)
Dx
t+1,j = Dtj −

i
xtij.
(10.13)
We are going to have to create a value function approximation V(Sx
t ). For the
purpose of illustrating the basic idea, let us use a separable approximation, which
we can write using
Vt(Rx
t , Dx
t ) =

i
V
R
ti(Rx
ti) +

j
V
D
tj(Dx
tj).
Since we are minimizing, we intend to create scalar, convex approximations for
V
R
ti(Rx
ti) and V
D
tj(Dx
tj). Now, our VFA policy requires solving a linear (or nonlinear)
math programming problem of the form
min
xt

i

j
cijxtij +

i
V
R
t−1,i(Rx
ti) +

j
V
D
t−1,j(Dx
tj),
(10.14)
where Rx
ti and Dx
tj are given by (10.12) and (10.13), respectively. Also our opti-
mization problem has to be solved subject to the constraints

j
xtij ≤Rti,
(10.15)

i
xtij ≤Dtj,
(10.16)
xtij ≥0.
(10.17)
The optimization problem described by equations (10.14) through (10.17) is a
linear or nonlinear optimization problem. If the scalar, separable value function
approximations are convex, this is generally fairly easy to solve, even when xt has
hundreds or thousands of dimensions.

statistical bias in the max operator
397
Elsewhere we would let ˆvn
t be the value of the optimal objective function, which
we then use to update the value function approximation. For this problem class we
are going to take advantage of the fact that the optimal solution will yield dual
variables for the constraints (10.15) and (10.16). Call these dual variables ˆvR
ti and
ˆvD
tj , respectively. Thus we can interpret ˆvR
ti as an approximation of the marginal
value of Rti, while ˆvD
tj is an approximation of the marginal value of Dtj. We can
use these marginal values to update our value function approximations. However,
we would use ˆvR
t and ˆvD
t
to update the value function approximations V
R
t−1(Rx
t−1)
and V
D
t−1(Dx
t−1), which means that we are using information from the problem we
solve at time t to update value function approximations at time t −1 around the
previous, post-decision state variable.
This logic is described in much greater detail in Chapters 13 and 14. Our goal
here is simply to illustrate the ability to use the post-decision value function to
handle optimization problems with high-dimensional decision vectors.
10.3
STATISTICAL BIAS IN THE MAX OPERATOR
A subtle type of bias arises when we are optimizing because we are taking the
maximum over a set of random variables. In algorithms such as Q-learning or
approximate value iteration, we are computing ˆqn
t by choosing the best of a set of
decisions that depend onQ
n−1(S, a). The problem is that the estimatesQ
n−1(S, a)
are random variables. In the best of circumstances, assume that Q
n−1(S, a) is an
unbiased estimate of the true value Vt(Sa) of being in (post-decision) state Sa.
Because it is still a statistical estimate with some degree of variation, some of
the estimates will be too high while others will be too low. If a particular action
takes us to a state where the estimate just happens to be too high (due to statistical
variation), then we are more likely to choose this as the best action and use it to
compute ˆqn.
To illustrate, let us choose an action a ∈A, where C(S, a) is the contribution
earned by using decision a (given that we are in state S), which then takes us
to state Sa(S, a) where we receive an estimated value V(Sa(S, a)). Normally we
would update the value of being in state S by computing
ˆvn = max
a∈A

C(S, a) +V
n−1(Sa(S, a))

.
We would then update the value of being in state S using our standard update
formula
V
n(S) = (1 −αn−1)V
n−1(S) + αn−1 ˆvn.
Since V
n−1(Sa(S, a)) is a random variable, sometimes it will overestimate the true
value of being in state Sa(S, a) while other times it will underestimate the true
value. Of course, we are more likely to choose an action that takes us to a state
where we have overestimated the value.

398
optimizing while learning
We can quantify the error due to statistical bias as follows. Fix the iteration
counter n (so that we can ignore it), and let
Ua = C(S, a) +V(Sa(S, a))
be the estimated value of using action a. The statistical error, which we represent
as β, is given by
β = E{max
a∈A Ua} −max
a∈A EUa.
(10.18)
The ﬁrst term on the right-hand side of (10.18) is the expected value of V(S),
which is computed based on the best observed value. The second term is the
correct answer (which we can only ﬁnd if we know the true mean). We can get an
estimate of the difference by using a strategy known as the “plug-in principle.” We
assume that EUa = V(Sa(S, a)), which means that we assume that the estimates
V(Sa(S, a)) are correct, and then try to estimate E{maxa∈A Ua}. Thus computing
the second term in (10.18) is easy.
The challenge is computing E{maxa∈A Ua}. We assume that while we have
been computing V(Sa(S, a)), we have also been computing σ 2(a) = Var(Ua) =
Var

V(Sa(S, a))

. Using the plug-in principle, we are going to assume that the
estimates σ 2(a) represent the true variances of the value function approxima-
tions. Computing E{maxa∈A Ua} for more than a few decisions is computationally
intractable, but we can use a technique called the Clark approximation to provide
an estimate. This strategy ﬁnds the exact mean and variance of the maximum of
two normally distributed random variables, and then assumes that this maximum
is also normally distributed. Assume that the decisions can be ordered so that
A = {1, 2, . . . , |A|}. Now let
U 2 = max{U1, U2}.
We can compute the mean and variance of U 2 as follows. First compute
Var(U2) = σ 2
1 + σ 2
2 −2σ1σ2ρ12,
where σ 2
1 = Var(U1), σ 2
2 = Var(U2), and ρ12 is the correlation coefﬁcient between
U1 and U2 (we allow the random variables to be correlated, but shortly we are
going to approximate them as being independent). Next ﬁnd
z = µ1 −µ2
	
Var(U 2)
,
where µ1 = EU1 and µ2 = EU2. Now let (z) be the cumulative standard normal
distribution (i.e., (z) = P[Z ≤z] where Z is normally distributed with mean
0 and variance 1), and let φ(z) be the standard normal density function. If we
assume that U1 and U2 are normally distributed (a reasonable assumption when

statistical bias in the max operator
399
they represent sample estimates of the value of being in a state), then it is a
straightforward exercise to show that
EU 2 = µ1(z) + µ2(−z) +
7
Var(U 2)φ(z)
(10.19)
Var(U 2) =
2
(µ2
1 + σ 2
1 )(z) + (µ2
1 + σ 2
2 )(−z) + (µ1 + µ2)
7
Var(U 2)φ(z)
3
−(EU 2)2.
(10.20)
Now suppose that we have a third random variable, U3, where we wish to ﬁnd
E max{U1, U2, U3}. The Clark approximation solves this by using
U 3 = E max{U1, U2, U3}
≈E max {U3, U2},
where we ﬁnd that U 2 is normally distributed with the mean given by (10.19) and
the variance given by (10.20). For our setting, it is unlikely that we would be able
to estimate the correlation coefﬁcient ρ12 (or ρ23), so we are going to assume that
the random estimates are independent. This idea can be repeated for large numbers
of decisions by using
U a = E max{U1, U2, . . . , Ua}
≈E max{Ua, Ua−1}.
We can apply this repeatedly until we ﬁnd the mean of U |A|, which is an approxi-
mation of E{maxa∈A Ua}. We can then use this estimate to compute an estimate of
the statistical bias β given by equation (10.18).
Figure 10.6 plots β = E maxa Ua −maxa EUa as it is being computed for 100
decisions, averaged over 30 sample realizations. The standard deviation of each Ua
was ﬁxed at σ = 20. The plot shows that the error increases steadily until the set
A reaches about 20 or 25 decisions, after which it grows much more slowly. Of
course, in an approximate dynamic programming application, each Ua would have
its own standard deviation which would tend to decrease as we sample a decision
repeatedly (a behavior that the approximation above captures nicely).
This brief analysis suggests that the statistical bias in the max operator can be
signiﬁcant. However, it is highly data dependent. If there is a single dominant
decision, then the error will be negligible. The problem only arises when there are
many (as in 10 or more) decisions that are competitive, and where the standard
deviation of the estimates is not small relative to the differences between the means.
Unfortunately, this is likely to be the case in most large-scale applications (if a
single decision is dominant, then it suggests that the solution is probably obvious).
The relative magnitudes of value iteration bias over statistical bias will depend
on the nature of the problem. If we are using a pure forward pass (TD(0)), and if
the value of being in a state at time t reﬂects rewards earned over many periods
into the future, then the value iteration bias can be substantial (especially if the

400
optimizing while learning
0
5
10
15
20
25
1
7
13
19
25
31
37
43
49
55
61
67
73
79
85
91
97
Number of decisions
E max - max E 
Figure 10.6
E maxa Ua −maxa EUa for 100 decisions, averaged over 30 sample realizations. The
standard deviation of all sample realizations was 20.
stepsize is too small). Value iteration bias has long been recognized in the dynamic
programming community. By contrast, statistical bias appears to have received
almost no attention, and as a result we are not aware of any research addressing
this problem. We suspect that statistical bias is likely to inﬂate value function
approximations fairly uniformly, which means that the impact on the policy may
be small. However, if the goal is to obtain the value function itself (e.g., to estimate
the value of an asset or a contract), then the bias can distort the results.
10.4
APPROXIMATE VALUE ITERATION AND
Q-LEARNING USING LINEAR MODELS
Approximate value iteration, Q-learning, and temporal-difference learning (with
λ = 0) are clearly the simplest methods for updating an estimate of the value of
being in a state. Linear models are the simplest methods for approximating a value
function. Not surprisingly, then, there has been considerable interest in putting
these two strategies together.
Figure 10.7 depicts a basic adaptation of approximate value iteration using a
linear model to approximate the value function. The strategy is popular because
it is so easy to implement. As of this writing, the design of provably convergent
algorithms based on approximate value iteration using linear models is an active
area of research. However, general-purpose algorithms such as the one sketched
in Figure 10.7 are not convergent, and may behave badly. This is true even if the
true value function can be perfectly represented using a set of basis functions.
The problem with approximate value iteration is that the update of the coefﬁcient
vector θn can be noisy, and this immediately impacts the policy, which further
contributes to the noise. In short, the strategy is fundamentally unstable.

approximate value iteration and q-learning
401
Step 0. Initialization.
Step 0a. Initialize V
0.
Step 0b. Initialize S1.
Step 0c. Set n = 1.
Step 1. Solve
ˆvn = max
a∈An

C(Sn, a) + γ

f
θn−1
f
φf (SM,a(Sn, a))

,
(10.21)
and let an be the value of a that solves (10.21).
Step 2. Update the value function recursively using equations (9.24)–(9.28) from chapter
9 to obtain θn.
Step 3. Choose a sample W n+1 = W(ωn+1), and determine the next state using some
policy such as
Sn = SM(Sn, an, W n+1).
Step 3. Increment n. If n ≤N, go to step 1.
Step 4. Return the value functions V
N.
Figure 10.7
Approximate value iteration using a linear model.
It is possible to design algorithms using approximate value iteration that are
provably convergent, but these require special structure. For example, Chapters 13
and 14 describe the use of approximate value iteration in the context of resource
allocation problems, where we can exploit the property of concavity. This is an
incredibly powerful property and allows us to obtain algorithms that are both prac-
tical and, under certain conditions, provably convergent. However, without these
properties, there are few guarantees. It is for this reason that the research community
has focused more on approximate policy iteration (see Section 10.5 below).
Despite the potentially poor performance of this algorithm, it remains a popular
strategy because of its simplicity. In addition, while it may not work, it might work
quite well! The important point here is that it is a strategy that may be worth trying,
but caution needs to be used. Given these observations, this section is aimed at
providing some guidance to improve the chances that the algorithm will work.
The most important step whenever a linear model is used, regardless of the
setting, is to choose the basis functions carefully so that the linear model has a
chance of representing the true value function accurately. The biggest strength of
a linear model is also its biggest weakness. A large error can distort the update of
θn, which then impacts the accuracy of the entire approximation. Since the value
function approximation determines the policy (see step 1), a poor approximation
leads to poor policies, which then distorts the observations ˆvn. This can be a vicious
circle from which the algorithm may never recover.
A second step is in the speciﬁc choice of recursive least squares updating.
Figure 10.7 refers to the classic recursive least squares updating formulas in
equations (9.24) through (9.28). However, buried in these formulas is the implicit

402
optimizing while learning
use of a stepsize rule of 1/n. We show in Chapter 11 that a stepsize 1/n is par-
ticularly bad for approximate value iteration (as well as Q-learning and TD(0)
learning). While this stepsize can work well (indeed it is optimal) for stationary
data, it is very poorly suited for the backward learning that arises in approximate
value iteration. Fortunately, the problem is easily ﬁxed if we replace the updating
equations for Bn and γ , which are given as
Bn = Bn−1 −1
γ n (Bn−1φn(φn)T Bn−1),
γ n = 1 + (φn)T Bn−1φn,
in equations (9.27) and (9.28) with
Bn = 1
λ

Bn−1 −1
γ n (Bn−1φn(φn)T Bn−1)

,
γ n = λ + (φn)T Bn−1φn,
in equations (9.30) and (9.31). Here λ discounts older errors. λ = 1 produces
the original recursive formulas. When used with approximate value iteration, it
is important to use λ < 1. In Section 9.3.2 we argued that if you choose a stepsize
rule for αn such as αn = a/(a + n −1), you should set λn at iteration n using
λn = αn−1
1 −αn
αn

.
The last issue that needs special care is the rule for determining the next state
to visit. As we discussed in Section 9.4, temporal-difference learning (TD(0) is
approximate value iteration for a ﬁxed policy) is only guaranteed to converge
while using on-policy learning, and can diverge when using off-policy learning.
Off-policy learning is a cornerstone of convergence proofs when using lookup
tables, but it appears to cause considerable problems when using a linear model
(it gets worse if our model is nonlinear in the parameters). At the same time
exploration should not be needed if our basis functions are carefully chosen (for a
ﬁxed policy). Interestingly it was recently shown that approximate value iteration
may converge if we start with a policy that is close enough to the optimal policy.
However, this condition is relaxed as the discount factor is decreased, suggesting
that a good strategy is to start with a smaller discount factor, ﬁnd a good policy for
that discount factor, and then increase the discount factor toward the desired level.
10.5
APPROXIMATE POLICY ITERATION
One of the most important tools in the toolbox for approximate dynamic program-
ming is approximate policy iteration. This algorithm is neither simpler nor more
elegant than approximate value iteration, but it can offer convergence guarantees
while using linear models to approximate the value function.

approximate policy iteration
403
In this section we review several ﬂavors of approximate policy iteration, includ-
ing
1. Finite horizon problems using lookup tables.
2. Finite horizon problems using basis functions.
3. Inﬁnite horizon problems using basis functions.
Finite horizon problems allow us to obtain Monte Carlo estimates of the value
of a policy by simulating the policy until the end of the horizon. Note that a
“policy” here always refers to decisions that are determined by value function
approximations. We use the ﬁnite horizon setting to illustrate approximating value
function approximations as well as lookup tables and basis functions, which allows
us to highlight the strengths and weaknesses of the transition to basis functions.
We then present an algorithm based on least squares temporal differences
(LSTD) and contrast the steps required for ﬁnite horizon and inﬁnite horizon
problems when using basis functions.
10.5.1
Finite Horizon Problems Using Lookup Tables
A fairly general-purpose version of an approximate policy iteration algorithm is
given in Figure 10.8 for an inﬁnite horizon problem. This algorithm helps to illus-
trate the choices that can be made when designing a policy iteration algorithm in
an approximate setting. The algorithm features three nested loops. The innermost
loop steps forward and backward in time from an initial state Sn,0. The purpose of
this loop is to obtain an estimate of the value of a path. Normally we would choose
T large enough so that γ T is quite small (thereby approximating an inﬁnite path).
The next outer loop repeats this process M times to obtain a statistically reliable
estimate of the value of a policy (determined by V
π,n). The third loop, representing
the outer loop, performs policy updates (in the form of updating the value func-
tion). In a more practical implementation we might choose states at random rather
than looping over all states.
Readers should note that we have tried to index variables in a way that shows
how they are changing (do they change with outer iteration n? inner iteration m? the
forward look-ahead counter t?). This does not mean that it is necessary to store, for
example, each state or decision for every n, m, and t. In an actual implementation
the software should be designed to store only what is necessary.
We can create different variations of approximate policy iteration by our choice
of parameters. First, if we let T →∞, then we are evaluating a true inﬁnite horizon
policy. If we simultaneously let M →∞, then vn approaches the exact, inﬁnite
horizon value of the policy π determined by V
π,n. Thus, for M = T = ∞, we have
a Monte Carlo-based version of exact policy iteration.
We can choose a ﬁnite value of T that produces values ˆvn,m close to the inﬁnite
horizon results. We can also choose ﬁnite values of M, including M = 1. When
we use ﬁnite values of M, this means that we are updating the policy before we
have fully evaluated the policy. This variant is known in the literature as optimistic

404
optimizing while learning
Step 0. Initialization.
Step 0a. Initialize V
π,0.
Step 0b. Set a lookahead parameter T and inner iteration counter M.
Step 0c. Set n = 1.
Step 1. Sample a state Sn
0 and then do:
Step 2. Do for m = 1, 2, . . . , M:
Step 3. Choose a sample path ωm (a sample realization over the lookahead horizon T ).
Step 4. Do for t = 0, 1, . . . , T :
Step 4a. Compute
an,m
t
= arg max
at ∈An,m
t

C(Sn,m
t
, at) + γV
π,n−1(SM,a(Sn,m
t
, at))

.
Step 4b. Compute
Sn,m
t+1 = SM(Sn,m
t
, an,m
t
, Wt+1(ωm)).
Step 5. Initialize ˆvn,m
T +1 = 0.
Step 6. Do for t = T, T −1, . . . , 0:
Step 6a. Accumulate ˆvn,m:
ˆvn,m
t
= C(Sn,m
t
, an,m
t
) + γ ˆvn,m
t+1.
Step 6b. Update the approximate value of the policy:
vn,m =
m −1
m

vn,m−1 + 1
m ˆvn,m
0
.
Step 8. Update the value function at Sn:
V
π,n(Sn) = (1 −αn−1)V
π,n−1(Sn) + αn−1vn,M
0
.
Step 9. Set n = n + 1. If n < N, go to step 1.
Step 10. Return the value functions (V
π,N).
Figure 10.8
Policy iteration algorithm for inﬁnite horizon problems.
policy iteration because rather than wait until we have a true estimate of the value
of the policy, we update the policy after each sample (presumably, although not
necessarily, producing a better policy). We may also think of this as a form of
partial policy evaluation, not unlike the hybrid value–policy iteration described in
Section 3.6.
10.5.2
Finite Horizon Problems Using Basis Functions
The simplest demonstration of approximate policy iteration using basis functions
is in the setting of a ﬁnite horizon problem. Figure 10.9 provides an adaption of

approximate policy iteration
405
Step 0. Initialization.
Step 0a. Fix the basis functions φf (s).
Step 0b. Initialize θπ,0
tf
for all t. This determines the policy we simulate in the inner
loop.
Step 0c. Set n = 1.
Step 1. Sample an initial starting state Sn
0:
Step 2. Initialize θn,0 (if n > 1, use θn,0 = θn−1), which is used to estimate the value
of policy π produced by θπ,n. θn,0 is used to approximate the value of following
policy π determined by θπ,n.
Step 3. Do for m = 1, 2, . . . , M:
Step 4. Choose a sample path ωm.
Step 5. Do for t = 0, 1, . . . , T :
Step 5a. Compute
an,m
t
= arg max
at ∈An,m
t

C(Sn,m
t
, at) + γ

f
θπ,n−1
tf
φf (SM,a(Sn,m
t
, at))

.
Step 5b. Compute
Sn,m
t+1 = SM(Sn,m
t
, an,m
t
, Wt+1(ωm)).
Step 6. Initialize ˆvn,m
T +1 = 0.
Step 7. Do for t = T, T −1, . . . , 0:
ˆvn,m
t
= C(Sn,m
t
, an,m
t
) + γ ˆvn,m
t+1.
Step 8. Update θn,m−1
t
using recursive least squares to obtain θn,m
t
(see Section 9.3).
Step 9. Set n = n + 1. If n < N, go to step 1.
Step 10. Return the regression coefﬁcients θN
t
for all t.
Figure 10.9
Policy iteration algorithm for ﬁnite horizon problems using basis functions.
the algorithm with lookup tables for when we are using basis functions. There is
an outer loop over n where we ﬁx the policy using
Aπ
t (St) = arg max
a

C(St, a) + γ

f
θπ,n
tf φf (SM,a(Sn,m
t,a ))

.
(10.22)
We are assuming that the basis functions are not themselves time-dependent,
although they depend on the post-decision state variable Sa
t , which, of course,
is time dependent. The policy is determined by the parameters θπ,n
tf .
We update the policy Aπ
t (s) by performing repeated simulations of the policy
in an inner loop that runs m = 1, . . . , M. Within this inner loop we use recursive
least squares to update a parameter vector θn,m
tf . This step replaces step 6b in
Figure 10.8.

406
optimizing while learning
If we let M →∞, then the parameter vector θn,M
t
approaches the best possible
ﬁt for the policy Aπ
t (s) determined by θπ,n−1. However, it is important to recognize
that this is not equivalent to performing a perfect evaluation of a policy using a
lookup table representation. The problem is that (for discrete states) lookup tables
have the potential for perfectly approximating a policy, whereas this is not generally
true when we use basis functions. If we have a poor choice of basis functions, we
may be able ﬁnd the best possible value of θ n,m as m goes to inﬁnity, but we may
still have a terrible approximation of the policy produced by θπ,n−1.
10.5.3
LSTD for Inﬁnite Horizon Problems Using Basis Functions
We have built the foundation for approximate policy iteration using lookup tables
and basis functions for ﬁnite horizon problems. We now make the transition to
inﬁnite horizon problems using basis functions, where we introduce the dimension
of projecting contributions over an inﬁnite horizon. There are several ways of
accomplishing this (see Section 9.1.2). We use least squares temporal differencing,
since it represents the most natural extension of classical policy iteration for inﬁnite
horizon problems.
To begin, we let a sample realization of a one-period contribution, given state
Sm, action am and random information W m+1 be given by
ˆCm = C(Sm, am, W m+1).
As in the past we let φm = φ(Sm) be the column vector of basis functions evaluated
at state Sm. We next ﬁx a policy that chooses actions greedily based on a value func-
tion approximation given by V
n(s) = 
f θn
f φf (s) (see equation (10.22)). Imagine
that we have simulated this policy over a set of iterations i = (0, 1, . . . , m), giv-
ing us a sequence of contributions ˆCi, i = 1, . . . , m. Drawing on the foundation
provided in Section 9.5, we can use standard linear regression to estimate θm as
θm =
/
1
1 + m
m

i=0
φi(φi −γ φi+1)T
0−1 /
1
1 + m
m

i=1
φi ˆCiφi
0
.
(10.23)
As a reminder, the term φi(φi)T −γ φi(φi+1)T can be viewed as a simulated,
sample realization of I −γ P π, projected onto the feature space. Just as we would
use (I −γ P π)−1 in our basic policy iteration to project the inﬁnite-horizon value
of a policy π (for a review, see Section 3.5) we are using the term
/
1
1 + m
m

i=0
φi(φi −γ φi+1)T
0−1
to produce an inﬁnite horizon estimate of the feature-projected contribution
/
1
1 + m
m

i=1
φi ˆCiφi
0
.

approximate policy iteration
407
Equation (10.23) requires solving a matrix inverse for every observation. It is
much more efﬁcient to use recursive least squares, which is done by using
ϵm = ˆCm −(φm −γ φm+1)T θm−1,
Bm = Bm−1 −Bmφm(φm −γ φm+1)T Bm−1
1 + (φm −γ φm+1)T Bm−1φm ,
θm = θm−1 +
ϵmBm−1φm
1 + (φm −γ φm+1)T Bm−1φm .
Figure 10.10 provides a detailed summary of the complete algorithm. The algo-
rithm has some nice properties if we are willing to assume that there is a vector
θ∗such that the true value function V (s) = 
f ∈F θ∗
f φf (s) (admittedly, a pretty
strong assumption). First, if the inner iteration limit M increases as a function of n
so that the quality of the approximation of the policy gets better and better, then the
Step 0. Initialization.
Step 0a. Initialize θ0.
Step 0b. Set the initial policy:
Aπ(s|θ0) = arg max
a∈A

C(s, a) + γ φ(SM(s, a))T θ0
.
Step 0c. Set n = 1.
Step 1. Do for n = 1, . . . , N.
Step 2. Initialize Sn,0.
Step 3. Do for m = 0, 1, . . . , M:
Step 4. Initialize θn,m.
Step 5. Sample W m+1.
Step 6. Do the following:
Step 6a. Computing the action an,m = Aπ(Sn,m|θ n−1).
Step 6b. Compute the post-decision state Sa,m = SM,a(Sn,m, an,m).
Step 6c. Compute the next pre-decision state Sn,m+1 = SM(Sn,m, an,m, W m+1).
Step 6d. Compute the input variable φ(Sn,m) −γ φ(Sn,m+1) for equation (10.23).
Step 7: Do the following:
Step 7a. Compute the response variable ˆCm = C(Sn,m, an,m, W m+1).
Step 7b. Compute θn,m using equation (10.23).
Step 8. Update θn and the policy:
θn+1 = θn,M
Aπ,n+1(s|θn+1) = arg max
a∈A

C(s, a) + γ φ(SM(s, a))T θn+1
.
Step 9. Return the Aπ(s|θN) and parameter θ N.
Figure 10.10
Approximate policy iteration for inﬁnite horizon problems using least squares temporal
differencing.

408
optimizing while learning
overall algorithm will converge to the true optimal policy. Of course, this means
letting M →∞, but from a practical perspective, it means that the algorithm can
ﬁnd a policy arbitrarily close to the optimal policy.
Second, the algorithm can be used with vector-valued and continuous actions
(we normally use the notation xt in this case). There are several features of the
algorithm that allow this. First, computing the policy Aπ(s|θn) requires solving a
deterministic optimization problem. If we are using discrete actions, it means simply
enumerating the actions and choosing the best one. If we have continuous actions,
we need to solve a nonlinear programming problem. The only practical issue is
that we may not be able to guarantee that the objective function is concave (or
convex if we are minimizing). Second, note that we are using trajectory following
(also known as on-policy training) in step 6c, without an explicit exploration step.
The algorithm does not require exploration, but it does require that we be able to
solve the recursive least squares equations.
We can avoid exploration as long as there is enough variation in the states
we visit that allows us to compute θ m in equation (10.23). When we use lookup
tables, we require exploration to guarantee that we eventually will visit every state
inﬁnitely often. When we use basis functions, we only need to visit states with
sufﬁcient diversity that we can estimate the parameter vector θm. In the language
of statistics, the issue is one of identiﬁcation (i.e., the ability to estimate θ) rather
than exploration. This is a much easier requirement to satisfy, and one of the major
advantages of parametric models.
10.6
THE ACTOR–CRITIC PARADIGM
It is very popular in some communities to view approximate dynamic programming
in terms of an “actor” and a “critic.” In this setting a decision function that chooses
a decision given the state is known as an actor. The process that determines the
contribution (cost or reward) from a decision is known as the critic, from which we
can compute a value function. The interaction of making decisions and updating
the value function is referred to as an actor–critic framework. The slight change in
vocabulary brings out the observation that the techniques of approximate dynamic
programming closely mimic human behavior. This is especially true when we drop
any notion of costs or contributions and simply work in terms of succeeding (or
winning) and failing (or losing).
The policy iteration algorithm in Figure 10.11 provides one illustration of the
actor-critic paradigm. The decision function is equation (10.24), where V π,n−1
determines the policy (in this case). This is the actor. Equation (10.25), where we
update our estimate of the value of the policy, is the critic. We ﬁx the actor for
a period of time and perform repeated iterations where we try to estimate value
functions given a particular actor (policy). From time to time, we stop and use our
value function to modify our behavior (something critics like to do). In this case
we update the behavior by replacing V π with our current V.
In other settings the policy is a rule or function that does not directly use a value
function (e.g., V π or V). For example, if we are driving through a transportation

the actor–critic paradigm
409
Step 0. Initialization.
Step 0a. Initialize V π,0
t
, t ∈T.
Step 0b. Set n = 1.
Step 0c. Initialize S1
0.
Step 1. Do for n = 1, 2, . . . , N:
Step 2. Do for m = 1, 2, . . . , M:
Step 3. Choose a sample path ωm.
Step 4. Initialize ˆvm = 0
Step 5. Do for t = 0, 1, . . . , T :
Step 5a. Solve:
an,m
t
= arg max
at ∈An,m
t

Ct(Sn,m
t
, at) + γ V π,n−1
t
(SM,a(Sn,m
t
, at))

.
(10.24)
Step 5b. Compute:
Sn,m
t+1 = SM(Sa,n,m
t
, an,m, Wt+1(ωm)).
Step 6. Do for t = T, . . . , 0:
Step 6a. Accumulate the path cost (with ˆvm
T +1 = 0):
ˆvm
t = Ct(Sn,m
t
, am
t ) + γ ˆvm
t+1.
Step 6b. Update approximate value of the policy for t > 0:
V
n,m
t−1 ←UV (V
n,m−1
t−1
, Sa,n,m
t−1 , ˆvm
t ),
(10.25)
where we typically use αm−1 = 1/m.
Step 7. Update the policy value function
V π,n
t
(Sa
t ) = V
n,M
t
(Sa
t )
∀t = 0, 1, . . . , T.
Step 8. Return the value functions (V π,N
t
)T
t=0.
Figure 10.11
Approximate policy iteration using value function-based policies.
network (or traversing a graph), the policy might be of the form “when at node
i, go next to node j.” As we update the value function, we may decide the right
policy at node i is to traverse to node k. Once we have updated our policy, the
policy itself does not directly depend on a value function.
Another example might arise when determining how much of a resource we
should have on hand. We might solve the problem by maximizing a function of the
form f (x) = β0 −β1(x −β2)2. Of course, β0 does not affect the optimal quantity.
We might use the value function to update β0 and β1. Once these are determined,
we have a function that does not itself directly depend on a value function.

410
optimizing while learning
10.7
POLICY GRADIENT METHODS
Perhaps the cleanest illustration of the actor–critic framework arises when we
parameterize both the value of being in a state as well as the policy. We use a
standard strategy from the literature which uses Q-factors, and where the goal
is to maximize the average contribution per time period (see Section 3.7 for a
brief introduction using the classical derivation based on transition matrices). Our
presentation here represents only a streamlined sketch of an idea that is simple in
principle but which involves some fairly advanced principles. We assume that the
Q-factors are parameterized using
Q(s, a|θ) =

f
θf φf (s, a).
The policy is represented using a function such as
Aπ(s|η, θ) =
eηQ(s,a)

a′ eηQ(s,a′) .
This choice of policy has the important feature that the probability that an action is
chosen is greater than zero. Also Aπ(s|η, θ) is differentiable in the policy parameter
vector η.
In the language of actor–critic algorithms,Q(s, a|θ) is an approximation of the
critic parameterized by θ, while Aπ(s|η, θ) is an approximate policy parameterized
by η. We can update θ and η using standard stochastic gradient methods. We begin
by deﬁning
ψθ(s, a) = ∇θ ln Aπ(s|η, θ)
= ∇θAπ(s|η, θ)
Aπ(s|η, θ) .
Since we are maximizing the average reward per time period, we begin by esti-
mating the average reward per time period using
cn+1 = (1 −αn)cn + αnC(Sn+1, an+1).
We then compute the temporal difference in terms of the difference between the
contribution of a state-action pair, and the average contribution, using
δn = C(Sn, an) −cn + (θn)T φθn(Sn+1, an+1) −(θn)T φθn(Sn, an).
Say we are using a TD(λ) updating procedure where we assume 0 < λ < 1. We
compute the eligibility trace using
Zn+1 = λZn + φθn(Sn+1, an+1).
We can now present the updating equations for the actor (the policy) and the
critic (the Q-factors) in a simple and compact way. The actor update is given by

the linear programming method using basis functions
411
ηn+1 = ηn −βn(θn)(θn)T φθn(Sn+1, an+1)ψθn(Sn+1, an+1),
(10.26)
where (θn) is used to scale the stepsize βn in a way that depends on the parameter
estimates θn. The critic update is given by
θn+1 = θn + αnδnZn.
(10.27)
Equations (10.26) and (10.27) provide an elegant and compact illustration of an
actor–critic updating equation, where both the value function and the policy are
approximated using parametric models.
10.8
THE LINEAR PROGRAMMING METHOD USING
BASIS FUNCTIONS
In Section 3.8 we showed that the determination of the value of being in each state
can be found by solving the following linear program:
min
v

s∈S
βsv(s)
(10.28)
subject to
v(s) ≥C(s, x) + γ

s′∈S
p(s′|s, x)v(s′)
for all s and x.
(10.29)
The problem with this formulation arises because it requires that we enumerate
the state space to create the value function vector (v(s))s∈S. Furthermore we have
a constraint for each state-action pair, a set that will be huge even for relatively
small problems.
We can partially solve this problem by replacing the discrete value function with
a regression function such as
V(s|θ) =

f ∈F
θf φf (s),
where (φf )f ∈F is an appropriately designed set of basis functions. This produces
a revised linear programming formulation
min
θ

s∈S
βs

f ∈F
θf φf (s)
subject to
v(s) ≥C(s, x) + γ

s′∈S
p(s′|s, x)

f ∈F
θf φf (s′)
for all s and x.
This is still a linear program, but now the decision variables are (θf )f ∈F instead
of (v(s))s∈S. Notice that rather than use a stochastic iterative algorithm, we obtain
θ directly by solving the linear program.

412
optimizing while learning
We still have a problem with a huge number of constraints. Since we no longer
have to determine |S| decision variables (in (10.28)–(10.29) the parameter vector
(v(s))s∈S represents our decision variables), it is not surprising that we do not
actually need all the constraints. One strategy that has been proposed is to simply
choose a random sample of states and actions. Given a state space S and set of
actions (decisions) X, we can randomly choose states and actions to create a smaller
set of constraints.
Some care needs to be exercised when generating this sample. In particular, it
is important to generate states roughly in proportion to the probability that they
will actually be visited. Then, for each state that is generated, we need to randomly
sample one or more actions. The best strategy for doing this is going to be problem-
dependent.
This technique has been applied to the problem of managing a network of
queues. Figure 10.12 shows a queueing network with three servers and eight queues.
A server can serve only one queue at a time. For example, server A might be a
machine that paints components one of three colors (e.g., red, green, and blue).
It is best to paint a series of parts red before switching over to blue. There are
customers arriving exogenously (denoted by the arrival rates λ1 and λ2). Other
customers arrive from other queues (e.g., departures from queue 1 become arrivals
to queue 2). The problem is to determine which queue a server should handle after
each service completion.
If we assume that customers arrive according to a Poisson process and that all
servers have negative exponential service times (which means that all processes
are memoryless), then the state of the system is given by
St = Rt = (Rti)8
i=1,
where Rti is the number of customers in queue i. Let K = {1, 2, 3} be our set
of servers, and let at be the attribute vector of a server given by at = (k, qt),
where k is the identity of the server and qt is the queue being served at time t.
Each server can only serve a subset of queues (as shown in Figure 10.12). Let
D = {1, 2, . . . , 8} represent a decision to serve a particular queue, and let Da be
the decisions that can be used for a server with attribute a. Finally, let xtad = 1 if
we decide to assign a server with attribute a to serve queue d ∈Da.
The state space is effectively inﬁnite (i.e., too large to enumerate). But we can
still sample states at random. Research has shown that it is important to sample
1
2
1
3
8
2
6
5
7
4
B
C
A
Figure 10.12
Queueing network with three servers serving a total of eight queues, two with exogenous
arrivals (λ) and six with arrivals from other queues (from de Farias and Van Roy, 2003).

approximate policy iteration using kernel regression
413
Table 10.3
Average cost estimated using simulation
Policy
Cost
ADP
33.37
Longest
45.04
FIFO
45.71
Source: From de Farias and Van Roy (2003).
states roughly in proportion to the probability they are visited. We do not know the
probability a state will be visited, but it is known that the probability of having a
queue with r customers (when there are Poisson arrivals and negative exponential
servers) follows a geometric distribution. For this reason we have chosen to sam-
ple a state with r = 
i Rti customers with probability (1 −γ )γ r, where γ is a
discount factor (a value of 0.95 was used).
Further complicating this problem class is that we also have to sample actions.
Let X be the set of all feasible values of the decision vector x. The number of
possible decisions for each server is equal to the number of queues it serves, so
the total number of values for the vector x is 3 × 2 × 3 = 18. In the experiments
for this illustration, only 5000 states were sampled (in portion to (1 −γ )γ r) but
all the actions were sampled for each state, producing 90,000 constraints.
Once the value function is approximated, it is possible to simulate the policy
produced by this value function approximation. The results were compared against
two myopic policies: serving the longest queue, and ﬁrst-in–ﬁrst-out (i.e., serve
the customer who had arrived ﬁrst). The costs produced by each policy are given
in Table 10.3, showing that the ADP-based strategy signiﬁcantly outperforms these
other policies.
Considerably more numerical work is needed to test this strategy on more real-
istic systems. For example, for systems that do not exhibit Poisson arrivals or
negative exponential service times, it is still possible that sampling states based on
geometric distributions may work quite well. More problematic is the rapid growth
in the feasible region X as the number of servers, and queues per server, increases.
An alternative to using constraint sampling is an advanced technique known
as column generation. Instead of generating a full linear program that enumerates
all decisions (i.e., v(s) for each state), and all constraints (equation (10.29)), it is
possible to generate sequences of larger and larger linear programs, adding rows
(constraints) and columns (decisions) as needed. These techniques are beyond the
scope of our presentation, but readers need to be aware of the range of techniques
available for this problem class.
10.9
APPROXIMATE POLICY ITERATION USING
KERNEL REGRESSION*
We build on the foundation provided in Section 9.8 that describes the use of kernel
regression in the context of least squares temporal difference (LSTD) learning. As

414
optimizing while learning
we have done earlier, we let the one-period contribution be given by
ˆCm = C(Sn,m, an,m, W m+1).
Let Sa,i, i = 1, . . . , m be the sample-path of post-decision states produced by
following a policy. Let k(Sa,i, Sa,j) be the normalized kernel function given by
k(Sa,i, Sa,j) =
Kh(Sa,i, Sa,j)
m−1
i=0 Kh(Sa,i, Sa,j)
,
which means that m−1
i=0 k(Sa,i, Sa,j) = 1. Then let P π,n be a M × M matrix where
the (i, j)th entry is given by
P π,n
i,j = k(Sa,i−1, Sa,j).
By construction, P π,n is a stochastic matrix (its rows sum to 1), which means that
I −γ P π,n is invertible.
Deﬁne the kernel-based approximation of Bellman’s operator for a ﬁxed policy
ˆMπ,m from the sample path of post-decision states Sa,0, . . . , Sa,m+1 using
ˆMπ,mV (s) =
m−1

i=0
k(Sa,i, s)( ˆCi + γ V (SM,a(Si, ai, W i+1))).
We would like to ﬁnd the ﬁxed point of the kernel-based Bellman equation deﬁned
by
ˆV π = ˆMπ,m ˆV π
= P π[cπ + γ ˆV π]
= [I −γ P π]−1cπ.
We can avoid the matrix inversion by using a value iteration approximation
ˆV π,k+1 = P π(cπ + γ ˆV π,i).
The vector ˆV π has an element ˆV π(Sa,i) for each of the (post-decision) states
Sa,i that we have visited. We then extrapolate from this vector of calculated values
for the states we have visited, giving us the continuous function
V
π(s) =
m−1

i=0
k(Sa,i, s)

ˆCi + γ ˆV π(Sa,i+1)

.
This approximation forms the basis of our approximate policy iteration. The full
algorithm is given in Figure 10.13.

ﬁnite horizon approximations for steady-state
415
Step 0. Initialization.
Step 0a. Initialize the policy Aπ(s).
Step 0b. Choose the kernel function Kh(s, s′).
Step 0b. Set n = 1.
Step 1. Do for n = 1, . . . , N:
Step 2. Choose an initial state Sn
0.
Step 3. Do for m = 0, 1, . . . , M:
Step 4. Let an,m = Aπ,n(Sn,m).
Step 5. Sample W m+1.
Step 6. Compute the post-decision state Sa,m = SM,a(Sn,m, an,m) and the next state
Sm+1 = SM(Sn,m, an,m, W m+1).
Step 7. Let cπ be a vector of dimensionality M with element ˆCm = C(Sn,m, an,m,
W m+1), m = 1, . . . , M.
Step 8. Let P π,n be a M × M matrix where the (i, j)th entry is given by Kh(Sa,i−1, Sa,j)
for i, j ∈{1, . . . , m}.
Step 9. Solve for ˆvn = (I −γ P π,n)−1, where ˆvn is an m-dimensional vector with ith
element ˆvn(Sa,i) for i = 1, . . . , m. This can be approximated using value iteration.
Step 10. LetV
n(s) = m−1
i=0 Kh(Sa,i, s)( ˆCi + γ ˆvn(Sa,i)) be our kernel-based value func-
tion approximation.
Step 11. Update the policy
Aπ,n+1(s) = arg max
a

C(s, a) + γV
n(SM,a(s, a)).
Step 12. Return the Aπ,N(s) and parameter θN.
Figure 10.13
Approximate policy iteration using least squares temporal differencing and kernel regres-
sion.
10.10
FINITE HORIZON APPROXIMATIONS
FOR STEADY-STATE APPLICATIONS
It is easy to assume that if we have a problem with stationary data (i.e., all random
information is coming from a distribution that is not changing over time), then we
can solve the problem as an inﬁnite horizon problem, and use the resulting value
function to produce a policy that tells us what to do in any state. If we can in fact
ﬁnd the optimal value function for every state, then this is true.
There are many applications of inﬁnite horizon models to answer policy ques-
tions. Do we have enough doctors? What if we increase the buffer space for holding
customers in a queue? What is the impact of lowering transaction costs on the
amount of money a mutual fund holds in cash? What happens if a car rental com-
pany changes the rules allowing rental ofﬁces to give customers a better car if they
run out of the type of car that a customer reserved? These are all dynamic pro-
grams controlled by a constraint (the size of a buffer or the number of doctors), a
parameter (the transaction cost), or the rules governing the physics of the problem
(the ability to substitute cars). We may be interested in understanding the behavior

416
optimizing while learning
of such a system as these variables are adjusted. For inﬁnite horizon problems that
are too complex to solve exactly, ADP offers a way to approximate these solutions.
Inﬁnite horizon models also have applications in operational settings. Suppose
that we have a problem governed by stationary processes. We could solve the
steady-state version of the problem, and use the resulting value function to deﬁne
a policy that would work from any starting state. This works if we have in fact
found at least a close approximation of the optimal value function for any starting
state. However, if you have made it this far in this book, then that means you
are interested in working on problems where the optimal value function cannot be
found for all states. Typically we are forced to approximate the value function, and
it is always the case that we do the best job of ﬁtting the value function around
states that we visit most of the time.
When we are working in an operational setting, we start with some known
initial state S0. From this state there are a range of “good” decisions, followed by
random information, that will take us to a set of states S1 that is typically heavily
inﬂuenced by our starting state. Figure 10.14 illustrates the phenomenon. Assume
that our true, steady-state value function approximation looks like the sine function.
At time t = 1, the probability distribution of the state St that we can reach is shown
as the shaded area. Assume that we have chosen to ﬁt a quadratic function of the
value function, using observations of St that we generate through Monte Carlo
sampling. We might obtain the dotted curve labeled as V1(S1), which closely ﬁts
the true value function around the states S1 that we have observed.
For times t = 2 and t = 3, the distribution of states S2 and S3 that we actually
observe grows wider and wider. As a result the best ﬁt of a quadratic function
spreads as well. So, even though we have a steady-state problem, the best value
function approximation depends on the initial state S0 and how many time periods
into the future that we are projecting. Such problems are best modeled as ﬁnite
horizon problems, but only because we are forced to approximate the problem.
10.11
BIBLIOGRAPHIC NOTES
Section 10.2
Approximate value iteration using lookup tables encompasses the
family of algorithms that depend on an approximation of the value of a future
state to estimate the value of being in a state now, which includes Q-learning
and temporal-difference learning. These methods represent the foundation
of approximate dynamic programming and reinforcement learning, and we
already saw an introduction to lookup table versions of these algorithms in
Chapter 4.
Section 10.4
The problems with the use of linear models in the context of
approximate value iteration (TD learning) are well known in the research lit-
erature. Good discussions of these issues are found in Bertsekas and Tsitsiklis
(1996), Tsitsiklis et al. (1997), Baird (1995), and Precup et al. (2001), to name
a few. Munos and Szepesvari (2008) prove convergence for an approximate
value iteration algorithm using a linear model but impose technical condi-
tions such as requiring that the algorithm start with a policy that is close to
the optimal policy. This paper provides insights into the types of restrictions
that may be required for approximate value iteration to work.

bibliographic notes
417
S1
S2
S3
V (S)
V1 (S1)
V2 (S2)
V3 (S3)
V (S)
V (S)
t = 1
t = 1
t = 1
Figure 10.14
Exact value function (sine curve) and value function approximations for t = 1, 2, 3,
which change with the probability distribution of the states that we can reach from S0.
Section 10.5
Bradtke and Barto (1996) introduced least squares temporal dif-
ferencing as a way of approximating the one-period contribution using a linear
model, and then projecting the inﬁnite horizon performance. Lagoudakis and
Parr (2003) describe the least squares policy iteration algorithm (LSPI), which
uses a linear model to approximate the Q-factors that is then embedded in a
model-free algorithm.
Section 10.6
There is a long history of referring to policies as “actors” and
value functions as “critics” (e.g., see Barto et al., 1983; Williams and Baird,
1990; Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998). Borkar and
Konda (1997) and Konda and Borkar (1999) analyze actor–critic algorithms
as an updating process with two time scales, one for the inner iteration to
evaluate a policy and one for the outer iteration where the policy is updated.
Konda and Tsitsiklis (2003) discuss actor–critic algorithms using linear mod-
els to represent both the actor and the critic, and bootstrapping for the
critic. Bhatnagar et al. (2009) suggest several new variations of actor–critic
algorithms, and prove convergence when both the actor and the critic use
bootstrapping.
Section 10.7
Policy gradient methods have received considerable attention
in the reinforcement learning community. The material in this section is
based on Konda and Tsitsiklis (2003), but we have provided only a stream-
lined presentation, and we urge readers to consult the original article before
attempting to implement the equations given in this section. One of the earliest

418
optimizing while learning
policy-gradient algorithms is given in Williams (1992). Marbach and Tsitsik-
lis (2001) provide gradient-based algorithms for optimizing Markov reward
processes, which is a mathematically equivalent problem. Sutton et al. (2000)
provide a version of a policy-gradient algorithm, but in a form that is difﬁcult
to compute. Szepesvari (2010) provides a recent summary of policy gradient
algorithms.
Section 10.8
Schweitzer and Seidmann (1985) describe the use of basis func-
tions in the context of the linear programming method. The idea is further
developed in de Farias and Van Roy (2003) who also develops performance
guarantees. de Farias and Van Roy (2001) investigate the use of constraint
sampling and prove results on the number of samples that are needed.
Section 10.9
This material is based on Ma and Powell (2010a).
Problems
Exercise 10.1 is due to de Farias and Van Roy (2000).
PROBLEMS
10.1
Consider a Markov chain with two states, 1 and 2, and just one policy. There
is a reward from being in state 1 of $1, and if in state 1 there is a probability
0.2 of staying in state 1, and a probability 0.8 of transitioning to state 2.
The reward for being in state 2 is $2 with a probability 0.2 of transitioning
to state 1, and a probability of 0.8 of staying in state 2. Let cπ be the
column vector of rewards for being in each state, and let P be the one-step
transition matrix. Let the basis functions be given by φ(1) = 1 and φ(2) = 2,
and let  = [1, 2]T . Let  be the projection operator (derived earlier). If
we represent the value function using θ, then Bellman’s equation would
be written
θ = Mπθ
= (cπ + γ P θ),
where γ = 5/5.4.
(a) First using value iteration and generate 5 iterations of
vn+1 = cπ + γ P vn,
starting with v0 = 0. What can you say about the limiting performance
of vn if we were to repeat this indeﬁnitely?
(b) Now consider what happens when we use value iteration with basis
functions, where
θ = (T )−1T (cπ + γ P θ).
Use this function to show that value iteration for this problem would
reduce to θn+1 = 1 + θ n. What can you say about the limiting behavior
of approximate value iteration using basis functions?

C H A P T E R
11
Adaptive Estimation and Stepsizes
At the core of approximate dynamic programming is some form of iterative learn-
ing, whether we are doing policy search or approximating value functions. In
Section 9.2 we introduced a stochastic optimization problem with the generic form
min
v
EF(v, ˆv),
where
F(v, ˆv) = 1
2(v −ˆv)2.
Here v is the value of being in some state that we are trying to estimate, and ˆv is
a random variable that represents a noisy estimate of this state. An algorithm for
estimating the best value of v (i.e., that minimizes the expected value of F(v, ˆv))
is a stochastic gradient algorithm, which looks like
vn = vn−1 −αn−1∇F(vn−1, ˆvn)
(11.1)
= vn−1 −αn−1(vn−1 −ˆvn)
(11.2)
= (1 −αn−1)vn−1 + αn−1 ˆvn.
(11.3)
Equation (11.2) shows the updating in the classical form that we have seen using
temporal difference learning, and equation (11.3) shows the form most familiar
when using recursive least squares. Both of these are easily seen as directly related
to the classical stochastic gradient algorithm in equation (11.1).
This classical derivation based on stochastic optimization views the sequence
of observations ˆvn as if they are observations from some signal such as a stock
price or wind speed. In approximate dynamic programming the observations ˆvn are
somewhat more complex, although their precise nature depends on the algorithm
that we are using. For this reason we begin our presentation with a review of
Approximate Dynamic Programming: Solving the Curses of Dimensionality, Second Edition.
Warren B. Powell.
2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
419

420
adaptive estimation and stepsizes
recursive least squares, approximate value iteration, temporal difference learning
(TD(0)), least squares policy evaluation (LSPE), and least squares temporal dif-
ferences (LSTD). These algorithms are analyzed using a Markov decision process
with a single state, which makes it possible to derive the updating equations in
closed form. From these results it is possible to derive insights into the demands
on a stepsize rule.
In other chapters we often use θn as the estimate of the true value θ after n
iterations. In this chapter we have to deal with time-dependent series, and we use
θn as the true value of our series at iteration n, and θ
n is our estimate of θn based
on the observations we have made so far.
11.1
LEARNING ALGORITHMS AND STEPSIZES
A useful exercise to understand the behavior of recursive least squares, LSTD
and LSPE is to consider what happens when they are applied to a trivial dynamic
program with a single state and a single action. This dynamic program is equivalent
to computing the sum
F = E
∞

i=0
γ i ˆCi,
(11.4)
where ˆCi is a random variable giving the ith contribution. If we let c = E ˆCi, then
clearly F = c/(1 −γ ). But let us pretend that we do not know this, and we are
using these various algorithms to compute the expectation.
We ﬁrst used the single-state problem in Section 9.6 but did not focus on the
implications for stepsizes. Here we use our ability to derive analytical solutions
for the optimal value function for least squares temporal differences (LSTD), least
squares policy evaluation (LSPE), and recursive least squares and temporal differ-
ences. These expressions allow us to understand the types of behaviors we would
like to see in a stepsize formula.
In the remainder of this section, we start by assuming that the value function is
approximated using a linear model
V(s) =

f ∈F
θf φf (s).
However, we are going to then transition to a problem with a single state, and
a single basis function φ(s) = 1. We assume that ˆv is a sampled estimate of the
value of being in the single state.
11.1.1
Least Squares Temporal Differences
In Section 9.5 we showed that the LSTD method, when using a linear architecture
applied to inﬁnite horizon problems, required solving
n

i=1
φf (Si)(φf (Si) −γ φf (Si+1))T θ =
n

i=1
φf (Si) ˆCi

learning algorithms and stepsizes
421
for each f ∈F. Let θn be the optimal solution. Again, since we have only one basis
function φ(s) = 1 for our single-state problem, this reduces to ﬁnding vn = θn:
vn =
1
1 −γ

1
n
n

i=1
ˆCn

.
(11.5)
Equation (11.5) shows that we are trying to estimate E ˆC using a simple average.
If we let C
n be the average over n observations, we can write this recursively
using
C
n =

1 −1
n

C
n−1 + 1
n
ˆCn.
For the single-state (and single-action) problem, the sequence ˆCn comes from a
stationary sequence. In this case a simple average is the best possible estimator.
In a dynamic programming setting with multiple states, and where we are trying
to optimize over policies, vn would depend on the state. Also, because the policy
that determines the action we take when we are in a state that is changing over the
iterations, the observations ˆCn, even when we ﬁx a state, would be nonstationary.
In this setting simple averaging is no longer the best. Instead, it is better to use
C
n = (1 −αn−1)C
n−1 + αn−1 ˆCn,
(11.6)
and also one of the stepsizes described in Section 11.2, 11.3, or 11.4. As a rule,
these stepsize rules do not decline as quickly as 1/n.
11.1.2
Least Squares Policy Evaluation
Least squares policy evaluation, which is developed using basis functions for inﬁ-
nite horizon applications, ﬁnds the regression vector θ by solving
θn = arg max
θ
n

i=1


f
θf φf (Si) −

ˆCi + γV
n−1(Si+1)



2
.
When we have one state, the value of being in the single state is given by vn = θn,
which we can write as
vn = max
θ
n

i=1

θ −

ˆCi + γ vn−12
.
This problem can be solved in closed form, giving us
vn =

1
n
n

i=1
ˆCi

+ γ vn−1.

422
adaptive estimation and stepsizes
Similar to LSTD, LSPE works to estimate E ˆC. For a problem with a single state
and action (and therefore only one policy), the best estimate of E ˆC is a simple
average. However, as we already argued with LSTD, if we have multiple states
and are searching for the best policy, the observation ˆC for a particular state will
come from a nonstationary series. For such problems we should again adopt the
updating formula in (11.6) and use one of the stepsize rules described Section 11.2,
11.3, or 11.4.
11.1.3
Recursive Least Squares
Using our linear model, we start with the following standard least squares model
to ﬁt our approximation:
min
θ
n

i=1

ˆvi −


f ∈F
θf φf (Si)




2
.
As we have already seen in Chapter 9, we can ﬁt the parameter vector θ using
least squares, which can be computed recursively using
θn = θn−1 −
1
1 + (xn)T Bn−1xn Bn−1xn(V
n−1(Sn) −ˆvn),
where xn = (φ1(Sn), . . . , φf (Sn), . . . , φF(Sn)), and the matrix Bn is computed
using
Bn = Bn−1 −
1
1 + (xn)T Bn−1xn

Bn−1xn(xn)T Bn−1
.
For the special case of a single state, we use the fact that we have only one basis
function φ(s) = 1 and one parameter θn = V
n(s) = vn. In this case the matrix Bn
is a scalar, and the updating equation for θn (now vn), becomes
vn = vn−1 −
Bn−1
1 + Bn−1 (vn−1 −ˆvn)
=

1 −
Bn−1
1 + Bn−1

vn−1 +
Bn−1
1 + Bn−1 ˆvn.
If B0 = 1, Bn−1 = 1/n, giving us
vn =

1 −1
n

vn−1 + 1
n ˆvn.
(11.7)

learning algorithms and stepsizes
423
Now imagine we are using approximate value iteration. In this case, ˆvn = ˆCn +
γ vn. Substituting this into equation (11.7) gives us
vn =

1 −1
n

vn−1 + 1
n( ˆCn + γ ˆvn)
=

1 −1
n(1 −γ )

vn−1 + 1
n
ˆCn.
(11.8)
Recursive least squares has the behavior of averaging the observations of ˆv. The
problem is that ˆvn = ˆCn + γ vn, since ˆvn is also trying to be a discounted accumu-
lation of the costs. Assume that the contribution was deterministic, where ˆC = c.
If we were doing classical approximate value iteration, we would write
vn = c + γ vn−1.
(11.9)
Comparing (11.8) and (11.9), we see that the one-period contribution carries
a coefﬁcient of 1/n in (11.8) and a coefﬁcient of 1 in (11.8). We can view
equation (11.8) as a steepest ascent update with a stepsize of 1/n. If we change
the stepsize to 1, we obtain (11.9).
11.1.4
Bounding 1/n Convergence for Approximate Value Iteration
It is well known that a 1/n stepsize will produce a provably convergent algorithm
when used with approximate value iteration. Experimentalists know that the rate
of convergence can be quite slow, but people new to the ﬁeld can sometimes be
found using this stepsize rule. In this section we hope to present evidence that the
1/n stepsize should never be used with approximate value iteration or its variants.
Figure 11.1 is a plot of vn computed using equation (11.8) as a function of
log10(n) for γ = 0.7, 0.8, 0.9, and 0.95, where we have set ˆC = 1. For γ = 0.90,
we need 1010 iterations to get vn = 9, which means we are still 10 percent from
the optimal. For γ = 0.95, we are not even close to converging after 100 billion
iterations.
It is possible to derive compact bounds, νL(n) and νU(n) for vn where
νL(n) < vn < νU(n).
These are given by
νL(n) =
c
1 −γ

1 −

1
1 + n
1−γ 
,
(11.10)
νU(n) =
c
1 −γ

1 −1 −γ
γ n
−
1
γ n1−γ

γ 2 + γ −1

.
(11.11)
Using the formula for the lower bound (which is fairly tight when n is large
enough that vn is close to v∗), we can derive the number of iterations to achieve a

424
adaptive estimation and stepsizes
0
0
2
4
6
8
10
12
5
10
15
20
25
0.95
0.90
0.80
0.70
log10(n)
Figure 11.1
vn plotted against log10(n) when we use a 1/n stepsize rule for updating.
particular degree of accuracy. Let ˆC = 1, which means that v∗= 1/(1 −γ ). For
a value of v < 1/(1 −γ ), we would need at least n(v) to achieve v∗= v, where
n(v) is found (from (11.10)) to be
n(v) ≥[1 −(1 −γ )v]−1/(1−γ ).
(11.12)
If γ = 0.9, we would need n(v) = 1020 iterations to reach a value of v = 9.9,
which gives us a one percent error. On a 3-GHz chip, assuming we can perform
one iteration per clock cycle (i.e., 3 × 109 iterations per second), it would take
1000 years to achieve this result.
11.1.5
Discussion
We can now see the challenge of choosing stepsizes for approximate value iter-
ation, temporal-difference learning, and Q-learning, compared to algorithms such
as LSPE, LSTD, and approximate policy iteration (the ﬁnite horizon version of
LSPE). If we observe ˆC with noise, and if the discount factor γ = 0 (which means
that we are not trying to accumulate contributions over time), then a stepsize of 1/n
is ideal. We are just averaging contributions to ﬁnd the average value. As the noise
in ˆC diminishes, and as γ increases, we would like a stepsize that approaches 1.
In general, we have to strike a balance between accumulating contributions over
time (which is more important as γ increases) and averaging the observations of
contributions (for which a stepsize of 1/n is ideal).
By contrast, LSPE, LSTD, and approximate policy iteration are all trying to
estimate the average contribution per period for each state. The values ˆC(s, a)
are nonstationary because the policy that chooses the action is changing, mak-
ing the sequence ˆC(sn, an) nonstationary. But these algorithms are not trying to
simultaneously accumulate contributions over time.

deterministic stepsize recipes
425
Sections 11.2, 11.3, and 11.4 describe stepsize formulas that are especially
designed for estimating means in the presence of nonstationary data when esti-
mating value functions based on some variation of policy evaluation. Section 11.2
describes simple deterministic stepsize rules, while Section 11.3 describes heuris-
tic stochastic stepsize rules that adapt to the data. Section 11.4 presents results for
optimal stepsize rules, including results for both stationary and nonstationary data
series. Section 11.5 presents a new stepsize formula, called the “optimal stepsize for
approximate value iteration” (OSAVI) that is speciﬁcally designed for approximate
value iteration and its variants such as Q-learning and TD(0).
11.2
DETERMINISTIC STEPSIZE RECIPES
One of the challenges in Monte Carlo methods is ﬁnding the stepsize αn. We refer
to a method for choosing a stepsize as a stepsize rule, while other communities refer
to them as learning rate schedules. A standard technique in deterministic problems
(of the continuously differentiable variety) is to ﬁnd the value of αn so that θ
n
gives the smallest possible objective function value (among all possible values of
α). For a deterministic problem, this is generally not too hard. For a stochastic
problem, it means calculating the objective function, which involves computing
an expectation. For most applications expectations are computationally intractable,
which makes it impossible to ﬁnd an optimal stepsize.
Throughout our presentation, we assume that we are trying to estimate a param-
eter θn that is evolving over the iterations, just as would occur using exact value
iteration (from Chapter 3) when we can compute expectations. In a dynamic pro-
gramming setting, θ would be the value of being in some state s, but we are going
to suppress the reference to state s. We let θ
n−1 be our estimate of θn computing
using information from the ﬁrst n −1 iterations. We then let ˆθn be a random obser-
vation of θn in iteration n. (In Chapter 4, we used ˆvn as our random observation
of being in a state, as shown in Figure 4.2.)
Our updating equation looks like
θ
n = (1 −αn−1)θ
n−1 + αn−1 ˆθn.
(11.13)
Our iteration counter always starts at n = 1 (just as our ﬁrst time interval starts
with t = 1). The use of αn−1 in equation (11.13) means that we are computing
αn−1 using information available at iteration n −1 and before. Thus we have an
explicit assumption that we are not using ˆθn to compute the stepsize in iteration n.
This is irrelevant when we use a deterministic stepsize sequence, but it is critical
in convergence proofs for stochastic stepsize formulas (introduced below). In most
formulas, α0 is a parameter that has to be speciﬁed, although we will generally
assume that α0 = 1, which means that we do not have to specify θ 0. The only
reason to use α0 < 1 is when we have some a priori estimate of θ 0 that is better
than ˆθ1.
Stepsize rules can be important especially for algorithms such as approximate
value iteration and Q-learning. As you experiment with ADP, it is possible to ﬁnd

426
adaptive estimation and stepsizes
problems where provably convergent algorithms simply do not work, and the only
reason is a poor choice of stepsizes. Inappropriate choices of stepsize rules have
led many to conclude that “approximate dynamic programming does not work.”
There are two issues when designing a good stepsize rule. The ﬁrst is the
question of whether the stepsize will produce a provably convergent algorithm.
While this is primarily of theoretical interest, these conditions do provide important
guidelines to follow to produce good behavior. The second issue is whether the
rule produces the fastest rate of convergence. Both are important issues in practice.
Below we start with a general discussion of stepsize rules. Following this, we
provide a number of examples of deterministic stepsize rules. These are formulas
that depend only on the iteration counter n (or more precisely, the number of
times that we update a particular parameter). Section 11.3 then describes stochastic
stepsize rules that adapt to the data.
The deterministic and stochastic rules presented in this section and Section 11.3
are, for the most part, heuristically designed to achieve good rates of convergence,
but are not supported by any theory that they will produce the best rate of con-
vergence. Later (Section 11.4) we provide a theory for choosing stepsizes that
produce the fastest possible rate of convergence when estimating value functions
based on policy evaluation. Finally, Section 11.5 presents a new optimal stepsize
rule designed speciﬁcally for approximate value iteration.
11.2.1
Properties for Convergence
The theory for proving convergence of stochastic gradient algorithms was ﬁrst
developed in the early 1950s and has matured considerably since then (see
Section 7.6). However, all the proofs require three basic conditions
αn−1 ≥0,
n = 1, 2, . . . ,
(11.14)
∞

n=1
αn−1 = ∞,
(11.15)
∞

n=1
(αn−1)2 < ∞.
(11.16)
Equation (11.14) obviously requires that the stepsizes be nonnegative. The most
important requirement is (11.15), which states that the inﬁnite sum of stepsizes must
be inﬁnite. If this condition did not hold, the algorithm might stall prematurely.
Finally, condition (11.6) requires that the inﬁnite sum of the squares of the stepsizes
be ﬁnite. This condition in effect requires that the stepsize sequence converge
“reasonably quickly.” A good intuitive justiﬁcation for this condition is that it
guarantees that the variance of our estimate of the optimal solution goes to zero in
the limit. Sections 7.6.2 and 7.6.3 illustrate two proof techniques that both lead to
these requirements on the stepsize. However, it is possible under certain conditions
to replace equation (11.16) with the weaker requirement that limn→∞αn = 0.

deterministic stepsize recipes
427
Conditions (11.15) and (11.16) effectively require that the stepsizes decline
according to an arithmetic sequence such as
αn−1 = 1
n.
(11.17)
This rule has an interesting property. Exercise 11.3 asks you to show that a step-
size of 1/n produces an estimate θ
n that is simply an average of all previous
observations, which is to say,
θ
n = 1
n
n

m=1
ˆθm.
(11.18)
Of course, we have a nice name for equation (11.18): it is called a sample aver-
age. And we are all aware that in general (some modest technical conditions are
required) as n →∞, θ
n will converge (in some sense) to the mean of our random
variable ˆθ.
The issue of the rate at which the stepsizes decrease is of considerable practical
importance. Consider, for example, the stepsize sequence
αn = 0.5αn−1,
which is a geometrically decreasing progression. This stepsize formula vio-
lates (11.5). More intuitively, the problem is that the stepsizes would decrease so
quickly that it is likely that we would never reach the ﬁnal solution.
Surprisingly, the “1/n” stepsize formula, which works in theory, tends not to
work in practice because it drops to zero too quickly when applied to approximate
dynamic programming applications (as we discussed in Section 11.1). The reason
is that we are usually updating the value function using biased estimates that are
changing over time. For example, consider the updating expression we used for
the post-decision state variable given in Section 4.6.5, which we repeat here for
convenience
ˆvn
t = Ct(Sn
t , an
t ) +V
n−1
t
(Sa,n
t
),
V
n
t−1(Sa,n
t−1) = (1 −αn−1)V
n−1
t−1 (Sa,n
t−1) + αn−1 ˆvn
t .
ˆvn
t is our sample observation of an estimate of the value of being in state St, which
we then smooth into the current approximation V
n−1
t−1 (Sa,n
t−1). If ˆvn
t were an unbiased
estimate of the true value, then a stepsize of 1/n would be the best we could do (we
show this later). However, ˆvn
t depends on V
n−1
t
(Sa
t ), which is an imperfect estimate
of the value function for time t. What typically happens is that the value functions
undergo a transient learning phase. Since we have not found the correct estimate for
V
n−1
t
(Sa,n
t
), the estimates ˆvn
t are biased, and the 1/n rule puts the highest weights
on the early iterations when the estimates are the worst. The resulting behavior is
illustrated in Figure 11.2.

428
adaptive estimation and stepsizes
0
10
20
30
40
50
60
70
80
90
100
1
15
22
29
36
43
50
57
64
71
78
85
92
99
1/n
Data
Mean
8
Figure 11.2
Illustration of poor convergence of the 1/n stepsize rule in the presence of transient data.
The remainder of this section presents a series of deterministic stepsize formulas
designed to overcome this problem. These rules are the simplest to implement and
are typically the best starting point when designing an ADP algorithm.
Constant Stepsizes
A constant stepsize rule is simply
αn−1 =



1
if n = 1,
α
otherwise,
where α is a stepsize that we have chosen. It is common to start with a stepsize of
1 so that we do not need an initial value θ
0 for our statistic.
Constant stepsizes are popular when we are estimating not one but many param-
eters (for large scale applications, these can easily number in the thousands or
millions). In these cases no single rule is going to be right for all of the param-
eters, and there is enough noise that any reasonable stepsize rule will work well.
Constant stepsizes are easy to code (no memory requirements) and, in particular,
easy to tune (there is only one parameter). Perhaps the biggest point in their favor
is that we simply may not know the rate of convergence, which means that we
run the risk with a declining stepsize rule of allowing the stepsize to decline too
quickly, producing a behavior we refer to as “apparent convergence.”
In dynamic programming we are typically trying to estimate the value of being in
a state using observations that are not only random but also changing systematically
as we try to ﬁnd the best policy. As a rule, as the noise in the observations of

deterministic stepsize recipes
429
the values increases, the best stepsize decreases. But, if the values are increasing
rapidly, we want a larger stepsize. Choosing the best stepsize requires striking
a balance between stabilizing the noise and responding to the changing mean.
Figure 11.3 illustrates observations that are coming from a process with relatively
low noise but where the mean is changing quickly (Figure 11.3a), and observations
that are very noisy but where the mean is not changing at all (Figure 11.3b). For
0
10
20
30
40
50
60
70
80
90
1
7
13
19
25
31
37
43
49
55
61
67
73
79
85
91
97
Time
(a) Low-noise
Value
0.05
0.1
0.2
Actual
0
20
40
60
80
100
120
1
7
13
19
25
31
37
43
49
55
61
67
73
79
85
91
97
Time
(b) High-noise
Value
0.05
0.1
0.2
Actual
Figure 11.3
Illustration of the effects of smoothing using constant stepsizes. (a) Low-noise dataset
with an underlying nonstationary structure; (b) high-noise dataset from a stationary process.

430
adaptive estimation and stepsizes
the ﬁrst, the ideal stepsize is relatively large, while for the second, the best stepsize
is quite small.
Generalized Harmonic Stepsizes
A generalization of the 1/n rule is the generalized harmonic sequence given by
αn−1 =
a
a + n −1.
(11.19)
This rule satisﬁes the conditions for convergence but produces larger stepsizes
for a > 1 than the 1/n rule. Increasing a slows the rate at which the stepsize
drops to zero, as illustrated in Figure 11.4. Choosing the best value of a requires
understanding the rate of convergence of the application. Some problems converge
to good solutions in a few hundred iterations, while others require hundreds of
thousands or millions of iterations. The best value of a might be as small as
5 or 10, or as large as 10,000 (or greater). When you get a sense of how many
iterations are required, choose a so that the stepsize is less than .05 as the algorithm
approaches convergence.
Polynomial Learning Rates
An extension of the basic harmonic sequence is the stepsize
αn−1 =
1
(n)β ,
(11.20)
where β ∈( 1
2, 1]. Smaller values of β slow the rate at which the stepsizes decline,
which improves the responsiveness in the presence of initial transient conditions.
The best value of β depends on the degree to which the initial data is transient,
and as such is a parameter that needs to be tuned.
0
0.2
0.4
0.6
0.8
1
1.2
1
5
9
13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77 81 85 89 93 97
a = 1
a = 10
a = 20
a = 40
Figure 11.4
Stepsizes for a/(a + n) while varying a.

deterministic stepsize recipes
431
McClain’s Formula
McClain’s formula (McClain, 1974) is an elegant way of obtaining 1/n behavior
initially but approaching a speciﬁed constant in the limit. The formula is given by
αn =
αn−1
1 + αn−1 −α ,
(11.21)
where α is a speciﬁed parameter. Note that steps generated by this model satisfy
the following properties:
αn > αn+1 > α
if
α > α,
αn < αn+1 < α
if
α < α.
McClain’s rule, illustrated in Figure 11.5, combines the features of the “1/n”
rule that are ideal for stationary data, and constant stepsizes for nonstationary data.
If we set α = 0, then it is easy to verify that McClain’s rule produces αn−1 = 1/n.
In the limit, αn →α. The value of the rule is that the 1/n averaging generally
works quite well in the very ﬁrst iterations (this is a major weakness of constant
stepsize rules) but avoids going to zero. The rule can be effective when you are
not sure how many iterations are required to start converging, and it can also work
well in nonstationary environments.
Search–Then–Converge Learning Rule
The search–then–converge (STC) stepsize rule (Darken and Moody, 1992) is a
variation on the harmonic stepsize rule that produces delayed learning. It was
0
0.2
0.4
0.6
0.8
1
1.2
1
5
9
13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77 81 85 89 93 97
Target = 0.20
Target = 0.10
Target = 0.0
Figure 11.5
McClain stepsize rule with varying targets.

432
adaptive estimation and stepsizes
originally proposed as
αn = α0

1 + β
α0
n
τ


1 + β
α0
n
τ + n2
τ
,
(11.22)
where α0, β and τ are parameters to be determined. A more compact and slightly
more general version of this formula is
αn−1 = α0
b
n + a

b
n + a + nβ
.
(11.23)
If β = 1, then this formula is similar to the STC rule. In addition, if b = 0,
then it is the same as the a/(a + n) rule. The addition of the term b/n to the
numerator and the denominator can be viewed as a kind of a/(a + n) rule, where
a is very large but declines with n. The effect of the b/n term then is to keep the
stepsize larger for a longer period of time, as illustrated in Figure 11.6. This can
help algorithms that have to go through an extended learning phase when the values
being estimated are relatively unstable. The relative magnitude of b depends on
the number of iterations that are expected to be run, which can range from several
dozen to several million.
This class of stepsize rules is termed search–then–converge because they pro-
vide for a period of high stepsizes (while searching is taking place) after which
0
0.2
0.4
0.6
0.8
1
1.2
1
5
9
13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77 81 85 89 93 97
a = 1, b = 0
a = 1, b = 100
a = 1, b = 1000
a = 1, b = 2000
Figure 11.6
Stepsizes for (b/n + a)/(b/n + a + n) while varying b.

stochastic stepsizes
433
0
0.2
0.4
0.6
0.8
1
1.2
1
5
9
13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77 81 85 89 93 97
a = 1, b = 0, b = 1
a = 10, b = 0, b = 1.0
a = 10, b = 0, b = 0.75
a = 10, b = 0, b = 0.6
Figure 11.7
Stepsizes for (b/n + a)/(b/n + a + nβ) while varying β.
the stepsize declines (to achieve convergence). The degree of delayed learning is
controlled by the parameter b, which can be viewed as playing the same role as
the parameter a but which declines as the algorithm progresses.
The exponent β in the denominator has the effect of increasing the stepsize in
later iterations (see Figure 11.7). With this parameter it is possible to accelerate
the reduction of the stepsize in the early iterations (by using a smaller a) but then
slow the descent in later iterations (to sustain the learning process). This may be
useful for problems where there is an extended transient phase requiring a larger
stepsize for a larger number of iterations.
11.3
STOCHASTIC STEPSIZES
There is considerable appeal to the idea that the stepsize should depend on the
actual trajectory of the algorithm. For example, if we are consistently observing
that our estimate θ
n−1 is smaller (or larger) than the observations ˆθn, then the
stepsize suggests that we are trending upward (or downward). When this happens,
we typically would like to use a larger stepsize to increase the speed at which we
reach a good estimate. When the stepsizes depend on the observations ˆθn, then we
say that we are using a stochastic stepsize.
In this section we ﬁrst review the case for stochastic stepsizes, then present
the revised theoretical conditions for convergence, and ﬁnally outline a series of
recipes that have been suggested in the literature (including some that have not).

434
adaptive estimation and stepsizes
11.3.1
The Case for Stochastic Stepsizes
Suppose that our estimates are consistently under or consistently over the actual
observations. This can easily happen during early iterations because of either a poor
initial starting point or the use of biased estimates (which is common in dynamic
programming) during the early iterations. For large problems it is possible that we
have to estimate thousands of parameters. It seems unlikely that all the parameters
will approach their true value at the same rate. Figure 11.8 shows the change in
estimates of the value of being in different states, illustrating the wide variation in
learning rates that can occur within the same dynamic program.
Stochastic stepsizes try to adjust to the data in a way that keeps the stepsize
larger while the parameter being estimated is still changing quickly. Balancing
noise against the change in the underlying signal, particularly when both of these
are unknown, is a difﬁcult challenge.
11.3.2
Convergence Conditions
When the stepsize depends on the history of the process, the stepsize itself becomes
a random variable. This change requires some subtle modiﬁcations to our require-
ments for convergence (equations (11.15) and (11.16). For technical reasons our
convergence criteria change to
αn ≥0,
almost surely,
(11.24)
∞

n=0
αn = ∞
almost surely,
(11.25)
Iteration
Value function
Figure 11.8
Different parameters can undergo signiﬁcantly different initial rates.

stochastic stepsizes
435
E
 ∞

n=0
(αn)2

< ∞.
(11.26)
The condition “almost surely” (universally abbreviated “a.s.”) means that
equation (11.25) holds for every sample path ω, and not just on average. More
precisely, we mean every sample path ω that might actually happen (we exclude
sample paths where the probability that the sample path would happen is zero).
For the reasons behind these conditions, go to our “Why does it work” section
(Section 7.6). It is important to emphasize, however, that these conditions are
completely unveriﬁable and are purely for theoretical reasons. The real issue with
stochastic stepsizes is whether they contribute to the rate of convergence.
11.3.3
Recipes for Stochastic Stepsizes
The desire to ﬁnd stepsize rules that adapt to the data has become a small cottage
industry that has produced a variety of formulas with varying degrees of sophisti-
cation and convergence guarantees. This section provides a brief sample of these
efforts.
To present our stochastic stepsize formulas, we need to deﬁne a few quantities.
Recall that our basic updating expression is given by
θ
n = (1 −αn−1)θ
n−1 + αn−1 ˆθn.
θ
n−1 is our estimate of the next observation, given by ˆθ n. The difference between
the estimate and the actual can be treated as the error, given by
εn = θ
n−1 −ˆθn.
We may wish to smooth the error in the estimate, which we designate by the
function
S(εn) = (1 −β)S(εn−1) + βεn.
Some formulas depend on tracking changes in the sign of the error. This can be
done using the indicator function
1{X} =

1
if the logical condition X is true,
0
otherwise.
Thus 1εnεn−1<0 indicates if the sign of the error has changed in the last iteration.
Below we summarize three classic rules. Kesten’s rule is the oldest and is
perhaps the simplest illustration of a stochastic stepsize rule. Trigg’s formula is a
simple rule widely used in the demand-forecasting community. Finally, the stochas-
tic gradient adaptive stepsize rule enjoys a theoretical convergence proof, but is
controlled by several tunable parameters that complicate its use in practice.

436
adaptive estimation and stepsizes
Kesten’s Rule
Kesten’s rule (Kesten, 1958) was one of the earliest stepsize rules that took advan-
tage of a simple principle: If we are far from the optimal, the errors tend to all
have the same sign; as we get close, the errors tend to alternate. Exploiting this
simple observation, Kesten proposed the following simple rule:
αn−1 =
a
a + Kn −1,
(11.27)
where a is a parameter to be calibrated. Kn counts the number of times that the
sign of the error has changed, where we would use
Kn =

n
if n = 1, 2,
Kn−1 + 1{εnεn−1<0}
if n > 2.
(11.28)
Kesten’s rule is particularly well suited to initialization problems. It slows the
reduction in the stepsize as long as the error exhibits the same sign (an indication
that the algorithm is still climbing into the correct region). However, the step-
size declines monotonically. This is typically ﬁne for most dynamic programming
applications, but problems can be encountered in situations with delayed learning.
Trigg’s Formula
Trigg’s formula (Trigg and Leach, 1967) is given by
αn = |S(εn)|
S(|εn|).
(11.29)
The formula takes advantage of the simple property that smoothing on the absolute
value of the errors is greater than or equal to the absolute value of the smoothed
errors. If there is a series of errors with the same sign, that can be taken as an
indication that there is a signiﬁcant difference between the true mean and our
estimate of the mean, which means we would like larger stepsizes.
Stochastic Gradient Adaptive Stepsize Rule
This class of rules uses stochastic gradient logic to update the stepsize. We ﬁrst
compute
ψn = (1 −αn−1)ψn−1 + εn.
(11.30)
The stepsize is then given by
αn =

αn−1 + νψn−1εnα+
α−,
(11.31)
where α+ and α−are, respectively, upper and lower limits on the stepsize. [·]α+
α−
represents a projection back into the interval [α−, α+], and ν is a scaling factor.
ψn−1εn is a stochastic gradient that indicates how we should change the stepsize
to improve the error. Since the stochastic gradient has units that are the square of

optimal stepsizes for nonstationary time series
437
the units of the error, while the stepsize is unitless, ν has to perform an important
scaling function. The equation αn−1 + νψn−1εn can easily produce stepsizes that
are larger than 1 or smaller than 0, so it is customary to specify an allowable interval
(which is generally smaller than (0,1)). This rule has provable convergence, but in
practice, ν, α+, and α−all have to be tuned.
11.3.4
Experimental Notes
Throughout our presentation, we represent the stepsize at iteration n using αn−1.
For discrete, lookup table representations of value functions (as we are doing here),
the stepsize should reﬂect how many times we have visited a speciﬁc state. If n(S)
is the number of times we have visited state S, then the stepsize for updating V(S)
should be αn(S). For notational simplicity we suppress this capability, but it can
have a signiﬁcant impact on the empirical rate of convergence.
A word of caution is offered when testing out stepsize rules. It is quite easy
to test out these ideas in a controlled way in a simple spreadsheet on randomly
generated data, but there is a big gap between showing a stepsize that works well in
a spreadsheet and one that works well in speciﬁc applications. Stochastic stepsize
rules work best in the presence of transient data where the degree of noise is not
too large compared to the change in the signal (the mean). As the variance of the
data increases, stochastic stepsize rules begin to suffer and simpler (deterministic)
rules tend to work better.
11.4
OPTIMAL STEPSIZES FOR NONSTATIONARY
TIME SERIES
Given the variety of stepsize formulas we can choose from, it seems natural to ask
whether there is an optimal stepsize rule. Before we can answer such a question, we
have to deﬁne exactly what we mean by it. Suppose that we are trying to estimate
a parameter (e.g., a value of being in a state or the slope of a value function)
that may be changing over time; we denote that parameter by θn. At iteration n,
our estimate of θn, θ
n, is a random variable that depends on our stepsize rule.
To express this dependence, let α represent a stepsize rule, and let θ
n(α) be the
estimate of the parameter µ after iteration n using stepsize rule α. We would like
to choose a stepsize rule to minimize
min
α E(θ
n(α) −θn)2.
(11.32)
Here the expectation is over the entire history of the algorithm and requires, in
principle, knowing the true value of the parameter being estimated. If we could
compute the expectation, we would obtain a deterministic stepsize sequence. In
practice, we have to estimate certain parameters from data, and this gives us a
stochastic stepsize rule.

438
adaptive estimation and stepsizes
There are other objective functions we could use. For example, instead of min-
imizing the distance to an unknown parameter sequence θn, we could solve the
minimization problem
min
α E
)
(θ
n(α) −ˆθn+1)2*
,
(11.33)
where we are trying to minimize the deviation between our prediction, obtained at
iteration n, and the actual observation at n + 1. Here we are again proposing an
unconditional expectation, which means that θ
n(α) is a random variable within the
expectation. Alternatively, we could condition on our history up to iteration n:
min
α En )
(θ
n(α) −ˆθn+1)2*
,
(11.34)
where En means that we are taking the expectation given what we know at iteration
n (i.e., θ
n(α) is a constant). (For readers familiar with the material in Section
5.9, we would write the expectation as E
)
(θ
n(α) −ˆθn+1)2|Fn*
, where Fn is the
sigma-algebra generated by the history of the process up through iteration n.) In this
formulation θ
n(α) is now deterministic at iteration n (because we are conditioning
on the history up through iteration n), whereas in (11.33), θ
n(α) is random (since
we are not conditioning on the history). The difference between these two objective
functions is subtle but signiﬁcant.
We begin our discussion of optimal stepsizes in Section 11.4.1 by addressing the
case of estimating a constant parameter that we observe with noise. Section 11.4.2
considers the case where we are estimating a parameter that is changing over time,
but where the changes have mean zero. Finally, Section 11.4.3 addresses the case
where the mean may be drifting up or down with nonzero mean, a situation that
we typically face when approximating a value function.
11.4.1
Optimal Stepsizes for Stationary Data
Suppose that we observe ˆθn at iteration n and that the observations ˆθn can be
described by
ˆθn = θ + εn,
where θ is an unknown constant and εn is a stationary sequence of independent
and identically distributed random deviations with mean 0 and variance σ 2. We
can approach the problem of estimating θ from two perspectives: choosing the best
stepsize and choosing the best linear combination of the estimates. That is, we may
choose to write our estimate θ
n after n observations in the form
θ
n =
n

m=1
an
m ˆθm.
For our discussion we will ﬁx n and work to determine the coefﬁcients am (recog-
nizing that they can depend on the iteration). We would like our statistic to have

optimal stepsizes for nonstationary time series
439
two properties: it should be unbiased, and it should have minimum variance (i.e.,
it should solve (11.32)). To be unbiased, it should satisfy
E
/ n

m=1
am ˆθm
0
=
n

m=1
amE ˆθm
=
n

m=1
amθ
= θ,
which implies that we must satisfy
n

m=1
am = 1.
The variance of our estimator is given by
Var(θ
n) = Var
/ n

m=1
am ˆθm
0
.
We use our assumption that the random deviations are independent, which allows
us to write
Var(θ
n) =
n

m=1
Var[am ˆθm]
=
n

m=1
a2
mVar[ ˆθm]
= σ 2
n

m=1
a2
m.
(11.35)
Now we face the problem of ﬁnding a1, . . . , an to minimize (11.35) subject to
the requirement that 
m am = 1. This problem is easily solved using the Lagrange
multiplier method. We start with the nonlinear programming problem
min
{am}
n

m=1
a2
m
subject to
n

m=1
am = 1,
(11.36)
am ≥0.
(11.37)

440
adaptive estimation and stepsizes
We relax constraint (11.36) and add it to the objective function
min
{am} L(a, λ) =
n

m=1
a2
m −λ
 n

m=1
am −1

subject to (11.37). We are now going to try to solve L(a, λ) (known as the
“Lagrangian”) and hope that the coefﬁcients a are all nonnegative. If this is true,
then we can take derivatives and set them equal to zero
∂L(a, λ)
∂am
= 2am −λ.
(11.38)
The optimal solution (a∗, λ∗) would then satisfy
∂L(a, λ)
∂am
= 0.
This means that at optimality
am = λ
2,
which tells us that the coefﬁcients am are all equal. Combining this result with the
requirement that they sum to one gives the expected result:
am = 1
n.
In other words, our best estimate is a sample average. From this (somewhat obvious)
result we can obtain the optimal stepsize, since we already know that αn−1 = 1/n
is the same as using a sample average.
This result tells us that if the underlying data are stationary, and we have no
prior information about the sample mean, then the best stepsize rule is the basic
1/n rule. Using any other rule requires that there be some violation in our basic
assumptions. In practice, the most common violation is that the observations are
not stationary because they are derived from a process where we are searching for
the best solution.
11.4.2
Optimal Stepsizes for Nonstationary Data—I
Suppose now that our parameter evolves over time (iterations) according to the
process
θ n = θn−1 + ξ n,
(11.39)
where Eξn = 0 is a zero mean drift term with variance (σ ξ)2. As before, we
measure θn with an error according to
ˆθ n = θn + εn.

optimal stepsizes for nonstationary time series
441
We want to choose a stepsize so that we minimize the mean squared error. This
problem can be solved using the Kalman ﬁlter. The Kalman ﬁlter is a powerful
recursive regression technique, but we adapt it here for the problem of estimating a
single parameter. Typical applications of the Kalman ﬁlter assume that the variance
of ξn, given by (σ ξ)2, and the variance of the measurement error, εn, given by
σ 2, are known. In this case the Kalman ﬁlter would compute a stepsize (generally
referred to as the gain) using
αn =
(σ ξ)2
νn + σ 2 ,
(11.40)
where νn is computed recursively using
νn = (1 −αn−1)νn−1 + (σ ξ)2.
(11.41)
Remember that α0 = 1, so we do not need a value of ν0. For our application we do
not know the variances, so these have to be estimated from data. We ﬁrst estimate
the bias using
β
n = (1 −ηn−1) β
n−1 + ηn−1

θ
n−1 −ˆθn
,
(11.42)
where ηn−1 is a simple stepsize rule such as the harmonic stepsize rule or McClain’s
formula. We then estimate the total error sum of squares using
νn = (1 −ηn−1) νn−1 + ηn−1

θ
n−1 −ˆθn2
.
(11.43)
Finally, we estimate the variance of the error using
(σ n)2 = νn −(β
n)2
1 + λ
n−1 ,
(11.44)
where λ
n−1 is computed using
λn =

(αn−1)2,
n = 1,
(1 −αn−1)2λn−1 + (αn−1)2,
n > 1.
We use (σ n)2 as our estimate of σ 2. We then propose to use

β
n2
as our estimate
of (σ ξ)2. This is purely an approximation, but experimental work suggests that it
performs quite well, and it is relatively easy to implement.

442
adaptive estimation and stepsizes
11.4.3
Optimal Stepsizes for Nonstationary Data—II
In dynamic programming we are trying to estimate the value of being in a state
(call it v) by v which is estimated from a sequence of random observations ˆv. The
problem we encounter is that ˆv might depend on a value function approximation
that is steadily increasing, which means that the observations ˆv are nonstationary.
Furthermore, unlike the assumption made by the Kalman ﬁlter that the mean of ˆv
is varying in a zero-mean way, our observations of ˆv might be steadily increasing.
This would be the same as assuming that Eξ = µ > 0 in the section above. In this
section we derive the Kalman ﬁlter learning rate for biased estimates.
Our challenge is to devise a stepsize that strikes a balance between minimizing
error (which prefers a smaller stepsize) and responding to the nonstationary data
(which works better with a large stepsize). We return to our basic model
ˆθn = θn + εn,
where θn varies over time, but it might be steadily increasing or decreasing. This
would be similar to the model in the previous section (equation 11.39) but where
ξ n has a nonzero mean. As before, we assume that {εn}n=1,2,... are independent and
identically distributed with mean value of zero and variance, σ 2. We perform the
usual stochastic gradient update to obtain our estimates of the mean
θ
n(αn−1) = (1 −αn−1) θ
n−1 + αn−1 ˆθn.
(11.45)
We wish to ﬁnd αn−1 that solves
min
αn−1 F(αn−1) = E
2
θ
n(αn−1) −θn23
.
(11.46)
It is important to realize that we are trying to choose αn−1 to minimize the uncon-
ditional expectation of the error between θ
n and the true value θn. For this reason
our stepsize rule will be deterministic, since we are not allowing it to depend on
the information obtained up through iteration n.
We assume that the observation at iteration n is unbiased, which is to say,
E
4
ˆθ n5
= θn.
(11.47)
But the smoothed estimate is biased because we are using simple smoothing on
nonstationary data. We denote this bias as
βn−1 = E
4
θ
n−1 −θn5
= E
4
θ
n−15
−θn.
(11.48)
We note that βn−1 is the bias computed after iteration n −1 (i.e., after we have
computed θ
n−1). βn−1 is the bias when we use θ
n−1 as an estimate of θn.

optimal stepsizes for nonstationary time series
443
The variance of the observation ˆθn is computed as follows:
Var
4
ˆθn5
= E
2
ˆθn −θn23
= E

(εn)2
= σ 2.
(11.49)
We now have what we need to derive an optimal stepsize for nonstationary
data with a mean that is steadily increasing (or decreasing). We refer to this as
the bias-adjusted Kalman ﬁlter stepsize rule (or BAKF), in recognition of its close
relationship to the Kalman ﬁlter learning rate. We state the formula in the following
theorem:
Theorem 11.4.1
The optimal stepsizes (αm)n
m=0 that minimize the objective func-
tion in equation (11.46) can be computed using the expression
αn−1 = 1 −
σ 2

1 + λn−1
σ 2 + (βn)2 ,
(11.50)
where λ is computed recursively using
λn =

(αn−1)2,
n = 1,
(1 −αn−1)2λn−1 + (αn−1)2,
n > 1.
(11.51)
Proof
We present the proof of this result because it brings out some properties of
the solution that we exploit later when we handle the case where the variance and
bias are unknown. Let F(αn−1) denote the objective function from the problem
stated in (11.46):
F(αn−1) = E
2
θ
n(αn−1) −θn23
(11.52)
= E
2
(1 −αn−1) θ
n−1 + αn−1 ˆθn −θn23
(11.53)
= E
2
(1 −αn−1)

θ
n−1 −θn
+ αn−1

ˆθn −θn23
(11.54)
= (1 −αn−1)2 E
2
θ
n−1 −θn23
+ (αn−1)2 E
2
ˆθn −θn23
+ 2αn−1 (1 −αn−1) E
4
θ
n−1 −θn 
ˆθn −θn5



I
.
(11.55)
Equation (11.52) is true by deﬁnition, while (11.53) is true by deﬁnition of the
updating equation for θ
n. We obtain (11.54) by adding and subtracting αn−1θn. To

444
adaptive estimation and stepsizes
obtain (11.55), we expand the quadratic term and then use the fact that the stepsize
rule, αn−1, is deterministic, which allows us to pull it outside the expectations. Then
the expected value of the cross-product term, I , vanishes under the assumption
of independence of the observations and the objective function reduces to the
following form:
F(αn−1) = (1 −αn−1)2 E
2
θ
n−1 −θn23
+ (αn−1)2 E
2
ˆθn −θn23
.
(11.56)
In order to ﬁnd the optimal stepsize, α∗
n−1, that minimizes this function, we obtain
the ﬁrst-order optimality condition by setting ∂F(αn−1)/∂αn−1 = 0, which gives us
−2

1 −α∗
n−1

E
2
θ
n−1 −θn23
+ 2α∗
n−1E
2
ˆθn −θn23
= 0.
(11.57)
Solving this for α∗
n−1 gives us the result
α∗
n−1 =
E
2
θ
n−1 −θn23
E
2
θ
n−1 −θn
23
+ E
2
ˆθn −θn
23.
(11.58)
Recall that we can write (θ
n−1 −θn)2 as the sum of the variance plus the bias
squared using
E
2
θ
n−1 −θn23
= λn−1σ 2 +

βn2 .
(11.59)
Using (11.59) and E
2
ˆθn −θn23
= σ 2 in (11.58) gives us
αn−1 =
λn−1σ 2 + (βn)2
λn−1σ 2 + (βn)2 + σ 2
= 1 −
σ 2

1 + λn−1
σ 2 + (βn)2 ,
which is our desired result (equation (11.50)).
□
A brief remark is in order. We have taken considerable care to make sure that
αn−1, used to update θ
n−1 to obtain θ
n, is a function of information available up
through iteration n −1. Yet, the expression for αn−1 in equation (11.50) includes
βn. At this point in our derivation, however, βn is deterministic, which means that
it is known (in theory at least) at iteration n −1. Below we have to address the
problem that we do not actually know βn and have to estimate it from data.

optimal stepsizes for nonstationary time series
445
Before we turn to the problem of estimating the bias and variance, there are
two interesting corollaries that we can easily establish: the optimal stepsize if the
data comes from a stationary series, and the optimal stepsize if there is no noise.
Earlier we proved this case by solving a linear regression problem, and noted that
the best estimate was a sample average, which implied a particular stepsize. Here
we directly ﬁnd the optimal stepsize as a corollary of our earlier result.
Corollary 11.4.1
For a sequence with a static mean, the optimal stepsizes are
given by
αn−1 = 1
n
∀n = 1, 2, . . . .
(11.60)
Proof
In this case, the mean θn = µ is a constant. Therefore the estimates of the
mean are unbiased, which means that βn = 0, for all t = 0, 1, . . . . This allows us
to write the optimal stepsize as
αn−1 =
λn−1
1 + λn−1 .
(11.61)
Substituting (11.61) into (11.51) gives us
αn =
αn−1
1 + αn−1
.
(11.62)
If α0 = 1, it is easy to verify (11.60).
□
For the case where there is no noise (σ 2 = 0), we have the following:
Corollary 11.4.2
For a sequence with zero noise, the optimal stepsizes are
given by
αn−1 = 1
∀n = 1, 2, . . . .
(11.63)
The corollary is proved by simply setting σ 2 = 0 in equation (11.50).
As a ﬁnal result we obtain
Corollary 11.4.3
In general,
αn−1 ≥1
n
∀n = 1, 2, . . . .
Proof
We leave this more interesting proof as an exercise to the reader (see
exercise (11.11)).
□
Corollary 11.4.3 is signiﬁcant since it establishes one of the conditions needed
for convergence of a stochastic approximation method, namely that ∞
n=1 αn = ∞.

446
adaptive estimation and stepsizes
An open theoretical question, as of this writing, is whether the BAKF stepsize rule
also satisﬁes the requirement that ∞
n=1(αn)2 < ∞.
The problem with using the stepsize formula in equation (11.50) is that it
assumes that the variance σ 2 and the bias (βn)2 are known. This can be problematic
in real instances, especially the assumption of knowing the bias, since knowing the
bias is the same as knowing the real function. If we have this information, we do
not need this algorithm.
As an alternative, we can try to estimate these quantities from data. Let
(σ 2)n = estimate of the variance of the error after iteration n,
β
n
= estimate of the bias after iteration n,
νn
= estimate of the variance of the bias after iteration n.
To make these estimates, we need to smooth new observations with our current
best estimate, something that requires the use of a stepsize formula. We could
attempt to ﬁnd an optimal stepsize for this purpose, but it is likely that a reasonably
chosen deterministic formula will work ﬁne. One possibility is McClain’s formula
(equation 11.21):
ηn =
ηn−1
1 + ηn−1 −η.
A limit point such as η ∈(0.05, 0.10) appears to work well across a broad range
of functional behaviors. The property of this stepsize that ηn →η can be a strength,
but it does mean that the algorithm will not tend to converge in the limit, which
requires a stepsize that goes to zero. If this is needed, we suggest a harmonic
stepsize rule:
ηn−1 =
a
a + n −1,
where a in the range between 5 and 10 seems to work quite well for many dynamic
programming applications.
Care needs to be used in the early iterations. For example, if we let α0 = 1,
then we do not need an initial estimate for θ
0 (a trick we have used throughout).
However, since the formulas depend on an estimate of the variance, we still have
problems in the second iteration. For this reason we recommend forcing η1 to equal
1 (in addition to using η0 = 1). We also recommend using αn = 1/(n + 1) for the
ﬁrst few iterations, since the estimates of (σ 2)n, β
n, and νn are likely to be very
unreliable in the very beginning.
Figure 11.9 summarizes the entire algorithm. Note that the estimates have been
constructed so that αn is a function of information available up through itera-
tion n. Figure 11.10 illustrates the behavior of the bias-adjusted Kalman ﬁlter
stepsize rule for two signals: very low noise (Figure 11.10a) and with higher noise
(Figure 11.10b). For both cases the signal starts small and rises toward an upper
limit of 1.0 (on average). In both ﬁgures we also show the stepsize 1/n. For the

optimal stepsizes for approximate value iteration
447
Step 0. Initialization.
Step 0a. Set the baseline to its initial value, θ0.
Step 0b. Initialize the parameters −β0, ν0 and λ0.
Step 0c. Set initial stepsizes α0 = η0 = 1, and specify the stepsize rule for η.
Step 0d. Set the iteration counter, n = 1.
Step 1. Obtain the new observation, ˆθn.
Step 2. Smooth the baseline estimate.
θ
n = (1 −αn−1)θ
n−1 + αn−1 ˆθn.
Step 3. Update the following parameters:
εn = θ
n−1 −ˆθn,
β
n = (1 −ηn−1) β
n−1 + ηn−1εn,
νn = (1 −ηn−1)νn−1 + ηn−1(εn)2,
(σ 2)n = νn −(β
n)2
1 + λn−1 .
Step 4. Evaluate the stepsizes for the next iteration.
αn =



1
n + 1,
n = 1, 2,
1 −(σ 2)n
νn
,
n > 2,
ηn =
a
a + n −1
(note that this gives us η1 = 1).
Step 5. Compute the coefﬁcient for the variance of the smoothed estimate of the baseline.
λ
n = (1 −αn−1)2λ
n−1 + (αn−1)2.
Step 6. If n < N, then n = n + 1 and go to step 1; else stop.
Figure 11.9
Bias-adjusted Kalman ﬁlter Kalman ﬁlter stepsize rule.
low-noise case, the stepsize stays quite large. For the high noise case, the stepsize
roughly tracks 1/n (notice that it never goes below 1/n).
11.5
OPTIMAL STEPSIZES FOR APPROXIMATE VALUE ITERATION
All the stepsize rules that we have presented so far are designed to estimate the
mean of a nonstationary series. In this section we develop a stepsize rule that is
speciﬁcally designed for approximate value iteration. We use as our foundation a
dynamic program with a single state and single action. We use the same theoretical
foundation that we used in Section 11.4. However, given the complexity of the
derivation, we simply provide the expression.

448
adaptive estimation and stepsizes
0
0.2
0.4
0.6
0.8
1
1.2
1
21
(a) Bias-adjusted Kalman filter for a signal with low noise
41
61
81
Observed values
BAKF stepsize rule
1/n stepsize rule
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1
21
41
61
81
Observed values
BAKF stepsize rule
(b) Bias-adjusted Kalman filter for a signal with higher noise
Figure 11.10
BAKF stepsize rule for low-noise (a) and high-noise (b). Each panel shows the signal,
the BAKF stepsizes, and the stepsizes produced by the 1/n stepsize rule.
We start with the basic relationship for our single-state problem
vn(αn−1) = (1 −(1 −γ )αn−1)vn−1 + αn−1 ˆCn.
(11.64)
Let c = ˆC be the expected one-period contribution for our problem, and let
Var( ˆC) = σ 2. For the moment, we assume c and σ 2 are known. We next deﬁne
the iterative formulas for two series, λn and δn, as follows:
λn =
α2
0,
n = 1,
α2
n−1 + (1 −(1 −γ )αn−1)2λn−1,
n > 1.
δn =
α0,
n = 1
αn−1 + (1 −(1 −γ )αn−1)δn−1,
n > 1.

convergence
449
It is possible to then show that
E(vn) = δnc,
Var(vn) = λnσ 2.
Let vn(αn−1) be deﬁned as in equation (11.64). Our goal is to solve the optimization
problem
min
αn−1 E
4
vn(αn−1) −Eˆvn25
.
(11.65)
The optimal solution can be shown to be given by
αn−1 =
(1 −γ )λn−1σ 2 + (1 −(1 −γ )δn−1)2c2
(1 −γ )2λn−1σ 2 + (1 −(1 −γ )δn−1)2c2 + σ 2 .
(11.66)
We refer to equation (11.64) as the optimal stepsize for approximate value iteration
(OSAVI). Of course, it is only optimal for our single-state problem, and it assumes
that we know the expected contribution per time period c, and the variance in the
contribution ˆC, σ 2.
OSAVI has some desirable properties. If σ 2 = 0, then αn−1 = 1. Also, if γ = 0,
then αn−1 = 1/n. It is also possible to show that αn−1 ≥(1 −γ )/n for any sample
path.
All that remains is adapting the formula to more general dynamic programs with
multiple states and where we are searching for optimal policies. We suggest the
following adaptation. We propose to estimate a single constant c representing the
average contribution per period, averaged over all states. If ˆCn is the contribution
earned in period n, let
c n = (1 −νn−1)c n−1 + νn−1 ˆCn,
(σ n)2 = (1 −νn−1)(σ n−1)2 + νn−1(c n −ˆCn)2.
Here νn−1 is a separate stepsize rule. Our experimental work suggests that a constant
stepsize works well, and that the results are quite robust with respect to the value
of νn−1. We suggest a value of νn−1 = 0.2. Now let c n be our estimate of c, and
let (σ n)2 be our estimate of σ 2.
We could also consider estimating c n(s) and (σ n)2(s) for each state so that we
can estimate a state-dependent stepsize αn−1(s). There is not enough experimental
work to support the value of this strategy, and lacking this, we favor simplicity
over complexity.
11.6
CONVERGENCE
A practical issue that arises with all stochastic approximation algorithms is that we
simply do not have reliable, implementable stopping rules. Proofs of convergence

450
adaptive estimation and stepsizes
1600000
0
10
20
30
40
50
60
70
80
90
100
1650000
1700000
1750000
1800000
1850000
1900000
1950000
2000000
(a) Objective function over 100 iterations
(b) Objective function over 400 iterations
1600000
0
50
100
150
200
250
300
350
400
450
500
1650000
1700000
1750000
1800000
1850000
1900000
1950000
2000000
Figure 11.11
Objective function plotted over 100 iterations (a), showing “apparent convergence.” The
same algorithm, continued over 400 iterations (b), showing signiﬁcant improvement.
in the limit are an important theoretical property, but they provide no guidelines
or guarantees in practice. A good illustration of the issue is given in Figure 11.11.
Figure 11.11a shows the objective function for a dynamic program over 100 iter-
ations (in this application a single iteration required approximately 20 minutes of
CPU time). The ﬁgure shows the objective function for an ADP algorithm that was
run for 100 iterations, at which point it appeared to be ﬂattening out (evidence of
convergence). Figure 11.11b is the objective function for the same algorithm run
for 400 iterations. A solid line that shows the best objective function after 100
iterations is shown at the same level on the graph where the algorithm was run for
400 iterations. As we see, the algorithm was nowhere near convergence after 100
iterations.
We refer to this behavior as “apparent convergence,” and it is particularly prob-
lematic on large-scale problems where run times are long. Typically the number of

guidelines for choosing stepsize formulas
451
iterations needed before the algorithm “converges” requires a level of subjective
judgment. When the run times are long, wishful thinking can interfere with this
process.
Complicating the analysis of convergence in approximate dynamic programming
is the behavior in some problems to go through periods of stability which are simply
a precursor to breaking through to new plateaus. During periods of exploration an
ADP algorithm might discover a strategy that opens up new opportunities, moving
the performance of the algorithm to an entirely new level.
Special care has to be made in the choice of stepsize rule. In any algorithm
using a declining stepsize, it is possible to show a stabilizing objective function
simply because the stepsize is decreasing. This problem is exacerbated when using
algorithms based on value iteration, where updates to the value of being in a state
depend on estimates of the values of future states, which can be biased. We rec-
ommend that initial testing of an ADP algorithm start with inﬂated stepsizes. After
getting a sense for the number of iterations needed for the algorithm to stabilize,
decrease the stepsize (keeping in mind that the number of iterations required to
convergence may increase) to ﬁnd the right trade-off between noise and rate of
convergence.
11.7
GUIDELINES FOR CHOOSING STEPSIZE FORMULAS
Given the plethora of strategies for computing stepsizes, it is perhaps not surpris-
ing that there is a need for general guidance when choosing a stepsize formula.
Strategies for stepsizes are problem-dependent, and as a result any advice reﬂects
the experience of the individual giving the advice.
We ﬁrst suggest that the reader review the material in Section 11.1. Are you
using some variation of approximate value iteration or Q-learning, or are you
using a variation of approximate policy iteration for a ﬁnite or inﬁnite horizon
problem? The implications on stepsizes are signiﬁcant, and appear to have been
largely overlooked in the research community. Stepsizes for variations of policy
iteration tend to be closer to 1/n, while stepsizes for value iteration and Q-learning
need to be larger, especially for discount factors closer to 1.
With this in mind, we offer the following general strategies for choosing step-
sizes:
Step 1. Start with a constant stepsize α and test out different values. Problems
with a relatively high amount of noise will require smaller stepsizes. Period-
ically stop updating the value function approximation and test your policy.
Plot the results to see roughly how many iterations are needed before your
results stop improving.
Step 2. Next try the harmonic stepsize a/(a + n −1). a = 1 produces the 1/n
stepsize rule that is provably convergent but is likely to decline too quickly.
1/n absolutely should not be used for approximate value iteration or Q-
learning. To choose a, look at how many iterations seemed to be needed

452
adaptive estimation and stepsizes
when using a constant stepsize. If 100 iterations appears to be enough for a
stepsize of 0.1, then try a ≈10, as it produces a stepsize of roughly 0.1 after
100 iterations. If you need 10,000 iterations, choose a ≈1000. But you will
need to tune a. An alternative rule is the polynomial stepsize rule α = 1/nβ
with β ∈(0.5, 1] (we suggest 0.7 as a good starting point).
Step 3. Now try either the BAKF stepsize rule (Section 11.4.3) for policy
iteration, LSPE and LSTD, or the OSAVI rule (Section 11.5) for approximate
value iteration, TD learning, and Q-learning. These are both fairly robust,
stochastic stepsize rules that adapt to the data. Both have a single tunable
parameter (a stepsize), but we have found that the stepsize smoothing for
OSAVI is more robust.
There is always the temptation to do something simple. A constant stepsize, or a
harmonic rule, is extremely simple to implement. Keep in mind that either rule has a
tunable parameter, and that the constant stepsize rule will not converge to anything
(although the ﬁnal solution may be acceptable). A major issue is that the best tuning
of a stepsize not only depends on a problem but also on the parameters of a problem
such as the discount factor. BAKF and OSAVI are more difﬁcult to implement but
are more robust to the setting of the single tunable parameter. Tunable parameters
can be a major headache in the design of algorithms, and it is good strategy
to absolutely minimize the number of tunable parameters your algorithm needs.
Stepsize rules should be something you code once and forget about.
11.8
BIBLIOGRAPHIC NOTES
Sections 11.1–11.3 A number of different communities have studied the prob-
lem of “stepsizes,” including the business forecasting community (Brown,
1959; Holt et al., 1960; Brown, 1963; Gifﬁn, 1971; Trigg, 1964; Gard-
ner, 1983), artiﬁcial intelligence (Jaakkola et al., 1994; Darken and Moody,
1991; Darken et al., 1992; Sutton and Singh, 1994), stochastic programming
(Kesten, 1958; Mirozahmedov and Uryasev, 1983; Pﬂug 1988; Ruszczynski
and Syski, 1986), and signal processing (Goodwin and Sin, 1984; Benveniste
et al., 1990; Stengel, 1994; Douglas and Mathews, 1995). The neural network
community refers to “learning rate schedules”; see Haykin (1999). Even-dar
and Mansour (2003) provide a thorough analysis of convergence rates for
certain types of stepsize formulas, including 1/n and the polynomial learn-
ing rate 1/nβ, for Q-learning problems. These sections are based on the
presentation in George and Powell (2006).
Section 11.4 This section is based on the review in George and Powell (2006),
along with the development of the optimal stepsize rule. Our proof that the
optimal stepsize is 1/n for stationary data is based on Kmenta (1997).
Section 11.5 The optimal stepsize for approximate value iteration was derived
in Ryzhov et al. (2009).

problems
453
PROBLEMS
11.1
We are going to solve a classic stochastic optimization problem known
as the newsvendor problem. Assume that we have to order x assets after
which we try to satisfy a random demand D for these assets, where D is
randomly distributed between 100 and 200. If x > D, we have ordered too
much and we pay 5(x −D). If x < D, we have an underage, and we have
to pay 20(D −x).
(a) Write down the objective function in the form minx Ef (x, D).
(b) Derive the stochastic gradient for this function.
(c) Find the optimal solution analytically [Hint: Take the expectation of
the stochastic gradient, set it equal to zero and solve for the quantity
P(D ≤x∗). From this, ﬁnd x∗.]
(d) Since the gradient is in units of dollars while x is in units of the
quantity of the asset being ordered, we encounter a scaling problem.
Choose as a stepsize αn−1 = α0/n where α0 is a parameter that has
to be chosen. Use x0 = 100 as an initial solution. Plot xn for 1000
iterations for α0 = 1, 5, 10, 20. Which value of α0 seems to produce
the best behavior?
(e) Repeat the algorithm (1000 iterations) 10 times. Let ω = (1, . . . , 10)
represent the 10 sample paths for the algorithm, and let xn(ω) be the
solution at iteration n for sample path ω. Let Var(xn) be the variance
of the random variable xn where
V(xn) = 1
10
10

ω=1
(xn(ω) −x∗)2
Plot the standard deviation as a function of n for 1 ≤n ≤1000.
11.2
A customer is required by her phone company to pay for a minimum
number of minutes per month for her cell phone. She pays 12 cents per
minute of guaranteed minutes, and 30 cents per minute that she goes over
her minimum. Let x be the number of minutes she commits to each month,
and let M be the random variable representing the number of minutes she
uses each month, where M is normally distributed with mean 300 minutes
and a standard deviation of 60 minutes.
(a) Write down the objective function in the form minx Ef (x, M).
(b) Derive the stochastic gradient for this function.
(c) Let x0 = 0 and choose as a stepsize αn−1 = 10/n. Use 100 iterations to
determine the optimum number of minutes the customer should commit
to each month.
11.3
Show that if we use a stepsize rule αn−1 = 1/n, then θ
n is a simple average
of ˆθ1, ˆθ2, . . . , ˆθn (thus proving equation 11.18).

454
adaptive estimation and stepsizes
11.4
We are going to again try to use approximate dynamic programming to esti-
mate a discounted sum of random variables (we ﬁrst saw this in Chapter 4):
F T = E
T

t=0
γ tRt,
where Rt is a random variable that is uniformly distributed between 0
and 100 (you can use this information to randomly generate outcomes, but
otherwise, you cannot use this information). This time we are going to use
a discount factor of γ = 0.95. We assume that Rt is independent of prior
history. We can think of this as a single-state Markov decision process with
no decisions.
(a) Using the fact that ERt = 50, give the exact value for F 100.
(b) Propose an approximate dynamic programming algorithm to estimate
F T . Give the value function updating equation, using a stepsize
αt = 1/t.
(c) Perform 100 iterations of the approximate dynamic programming algo-
rithm to produce an estimate of F 100. How does this compare to the
true value?
(d) Compare the performance of the following stepsize rules: Kesten’s rule,
the stochastic gradient adaptive stepsize rule (use ν = 0.001), 1/nβ
with β = 0.85, the Kalman ﬁlter rule, and the optimal stepsize rule.
For each one, ﬁnd both the estimate of the sum and the variance of the
estimate.
11.5
Consider a random variable given by R = 10U (which would be uni-
formly distributed between 0 and 10). We wish to use a stochastic gradient
algorithm to estimate the mean of R using the iteration θ
n = θ
n−1 −
αn−1(Rn −θ
n−1), where Rn is a Monte Carlo sample of R in the nth
iteration. For each of the stepsize rules below, use equation (7.12) to mea-
sure the performance of the stepsize rule to determine which works best,
and compute an estimate of the bias and variance at each iteration. If the
stepsize rule requires choosing a parameter, justify the choice you make
(you may have to perform some test runs).
(a) αn−1 = 1/n.
(b) Fixed stepsizes of αn = 0.05, 0.10, and 0.20.
(c) The
stochastic
gradient
adaptive
stepsize
rule
(equations
(11.30–11.31)).
(d) The Kalman ﬁlter (equations (11.40–11.44)).
(e) The optimal stepsize rule (algorithm 11.9).
11.6
Repeat exercise 11.5 using
Rn = 10(1 −e−0.1n) + 6(U −0.5).

problems
455
11.7
Repeat exercise 11.5 using
Rn =

10
1 + e−0.1(50−n)

+ 6(U −0.5).
11.8
Let U be a uniform [0, 1] random variable, and let
µn = 1 −exp (−θ1n).
Now let ˆRn = µn + θ2(U n −.5). We wish to try to estimate µn using
R
n = (1 −αn−1)R
n−1 + αn−1 ˆRn.
In the exercises below, estimate the mean (using R
n) and compute the
standard deviation of R
n for n = 1, 2, . . . , 100 for each of the following
stepsize rules:
• αn−1 = 0.10.
• αn−1 = a/(a + n −1) for a = 1, 10.
• Kesten’s rule.
• The bias-adjusted Kalman ﬁlter stepsize rule.
For each of the parameter settings below, compare the rules based on the
average error (1) over all 100 iterations and (2) in terms of the standard
deviation ofR
100.
(a) θ1 = 0, θ2 = 10.
(b) θ1 = 0.05, θ2 = 0.
(c) θ1 = 0.05, θ2 = 0.2.
(d) θ1 = 0.05, θ2 = 0.5.
(e) Now pick the single stepsize that works the best on all four of the
exercises above.
11.9
An oil company covers the annual demand for oil using a combination of
futures and oil purchased on the spot market. Orders are placed at the end
of year t −1 for futures that can be exercised to cover demands in year t.
If too little oil is purchased this way, the company can cover the remaining
demand using the spot market. If too much oil is purchased with futures,
then the excess is sold at 70 percent of the spot market price (it is not held
to the following year—oil is too valuable and too expensive to store).
To write down the problem, model the exogenous information using
ˆDt = demand for oil during year t,
ˆps
t = spot price paid for oil purchased in year t,
ˆpf
t,t+1 = futures price paid in year t for oil to be used in year t + 1.

456
adaptive estimation and stepsizes
The demand (in millions of barrels) is normally distributed with mean 600
and standard deviation of 50. The decision variables are given by
θ
f
t,t+1 = number of futures to be purchased at the end of
year t to be used in year t + 1,
θ
s
t = spot purchases made in year t.
(a) Set up the objective function to minimize the expected total amount
paid for oil to cover demand in a year t + 1 as a function of θ
f
t . List
the variables in your expression that are not known when you have to
make a decision at time t.
(b) Give an expression for the stochastic gradient of your objective func-
tion. That is, what is the derivative of your function for a particular
sample realization of demands and prices (in year t + 1)?
(c) Generate 100 years of random spot and futures prices as follows:
ˆpf
t = 0.80 + 0.10Uf
t ,
ˆps
t,t+1 = ˆpf
t + 0.20 + 0.10Us
t ,
where U f
t and U s
t are random variables uniformly distributed between 0
and 1. Run 100 iterations of a stochastic gradient algorithm to determine
the number of futures to be purchased at the end of each year. Use θ
f
0 =
30 as your initial order quantity, and use as your stepsize αt = 20/t.
Compare your solution after 100 years to your solution after 10 years.
Do you think you have a good solution after 10 years of iterating?
11.10
The proof in Section 7.6.3 was performed assuming that µ is a scalar.
Repeat the proof assuming that µ is a vector. You will need to make
adjustments such as replacing assumption 2 with ∥gn∥< B. You will also
need to use the triangle inequality, which states that ∥a + b∥≤∥a∥+ ∥b∥.
11.11
Prove Corollary 11.4.3.

C H A P T E R
12
Exploration Versus Exploitation
A fundamental challenge with approximate dynamic programming is that our ability
to estimate a value function may require that we visit states just to estimate the
value of being in (or near) the state. Should we make a decision because we think
it is the best decision (based on our current estimate of the values of states the
decision may take us to), or do we make a decision just to try something new?
This is a decision we face in day-to-day life, so it is not surprising that we face
this problem in our algorithms.
This choice is known in the approximate dynamic programming literature as
the exploration versus exploitation problem. Do we make a decision to explore
a state? Or do we “exploit” our current estimates of downstream values to make
what we think is the best possible decision? It can cost time and money to visit a
state, so we have to consider the future value of an action in terms of improving
future decisions.
When we are using approximate dynamic programming, we always have to think
about how well we know the value of being in a state, and whether a decision will
help us learn this value better. The science of learning in approximate dynamic
programming is quite young, with many unanswered questions. This chapter pro-
vides a peek into the issues and some of the simpler results that can be put into
practice.
12.1
A LEARNING EXERCISE: THE NOMADIC TRUCKER
A nice illustration of the exploration versus exploitation problem is provided by
our nomadic trucker example. Assume that the state of our nomadic trucker is his
location Rt. From a location, our trucker is able to choose from a set of demands ˆDt.
Thus our state variable is S = {Rt, ˆDt}. Our actions At consist of the decision to
assign to one of the loads in ˆDt, or to do nothing and wait for another time period.
Let ˆC(St, at) be the contribution earned from being in location Rt and taking
Approximate Dynamic Programming: Solving the Curses of Dimensionality, Second Edition.
Warren B. Powell.
2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
457

458
exploration versus exploitation
action at. Any demands not covered in ˆDt at time t are lost. After implementing
action at, the driver will either stay in his current location (if he does nothing) or
move to a location that corresponds to the destination of the load the driver moved
in the set ˆDt. Let Ra
t be this downstream location (which corresponds to the post-
decision state variable for Rt). The post-decision state variable Sa
t = (Ra
t ). We
assume that the action a deterministically determines the downstream destination,
so Rt+1 = Ra
t .
The driver makes his decision by solving
ˆvn
t = max
a

C(St, a) + γV
n−1(Ra
t )

,
where Ra
t = SM,a(St, a) is the downstream location. Let at be the optimal action.
We update the value of our previous, post-decision state using
V
n(Ra
t−1) = (1 −αn−1)V
n−1(Ra
t−1) + αn−1 ˆvn
t .
We start by initializing the value of being in each location to zero, and use a pure
exploitation strategy. If we simulate 500 iterations of this process, we produce the
pattern shown in Figure 12.1a. Here the circles at each location are proportional to
the value V
500(R) of being in that location. The small circles indicate places where
the trucker never visited. Out of 30 cities our trucker has ended up visiting seven.
An alternative strategy is to initialize V
0(R) to a large number. For our illustra-
tion, where rewards tends to average several hundred dollars per iteration (we are
using a discount factor of 0.80), we might initialize the value function to $2000,
which is higher than we would expect the optimal solution to be. The same strat-
egy applied to visiting a state generally produces a reduction in the estimate of
the value of being in the state. Not surprisingly, the logic tends to favor visiting
locations that we have never visited before (or have visited the least). The resulting
behavior is shown in Figure 12.1b. Here the pattern of lines shows that after 500
iterations, we have visited every city.
How do these strategies compare? We also ran an experiment where we esti-
mated the value functions by using a pure exploration strategy, where we ran ﬁve
iterations of sampling every single location. Then, for all three methods of estimat-
ing the value function, we simulated the policy produced by these value functions
for 200 iterations. The results are shown in Table 12.1. Notice that for this example
pure exploitation with a high initial estimate for the value function works better
than when we use a low initial estimate, but estimating the value functions using
a pure exploration strategy works best of all.
The difﬁculty with using a high initial estimate for the value function is that it
leads us to visit every state (at least every state that we can reach). If the problem
has a large state space, this will produce very slow convergence, since we have to
visit many states just to determine that the states should not have been visited. In
effect, exploitation with a high initial value will behave (at least initially) like pure
exploration. For applications such as the nomadic trucker (where the state space is

a learning exercise: the nomadic trucker
459
(a) Low initial estimate of the value function 
(b) High initial estimate of the value function
Figure 12.1
Effect of value function initialization on search process. (a) Low initial estimate, which
produces limited exploration; (b) high initial estimate, which forces exploration of the entire state space.

460
exploration versus exploitation
Table 12.1
Comparison of pure exploitation with low and
high initial estimates to one which uses exploration
Logic
Proﬁts
Low initial estimate
120
High initial estimate
208
Explore all
248
relatively small), this works quite well. But the logic will not generalize to more
complex problems.
12.2
AN INTRODUCTION TO LEARNING
Before we tackle the full complexity of learning within approximate dynamic pro-
gramming, it helps to start with some fundamental concepts in learning, a topic
we ﬁrst saw in Chapter 7. Let x be a design, a path, a policy, or an action that we
take in a dynamic program. For now, let µx be the true value of x, and let θn
x be
our best estimate of µx after n measurements.
12.2.1
Belief Structures
The literature on learning can be separated between frequentist and Bayesian belief
models. We touched on this earlier in Chapter 7, but we provide a brief review and
a discussion for the context of approximate dynamic programming.
Frequentist Belief Model
In a frequentist model our belief about µx is formed purely from our observations.
For example, let θn
x be our estimate of µx after n measurements, and let ˆσ 2,n
x
be
our estimate of the variance of a measurement Wx of x. Also let Nn
x be the number
of times we have measured x. We would update θn
x and ˆσ 2,n
x
recursively using
θ n
x =




1 −1
Nnx

θn−1
x
+ 1
Nnx
W n
x
if xn = x,
θn−1
x
otherwise;
ˆσ 2,n
x
=






1
N nx
(W n −θn−1)2,
if xn = x and Nn
x = 2
N n
x −2
N nx −1 ˆσ 2,n−1x + 1
Nnx
(W n
x −θn−1
x
)2,
if xn = x and Nn
x > 2
ˆσ 2,n−1
x
xn ̸= x.
The statistics θn
x and ˆσ 2,n
x
are random variables only because the observations W n
x
are random. For example, the variance of θn
x is computed using
σ 2,n
x
= 1
Nnx
ˆσ 2,n
x
.

an introduction to learning
461
Bayesian Belief Model
In a Bayesian model we view µx as a random variable, where we assume that
we already know an initial distribution of belief about µx. For example, we often
assume that our initial distribution of belief is normal with mean θ 0
x and precision
β0
x (recall that the precision is the inverse of the variance). Then we make each
measurement of Wx (with precision βW
x ) using
µn+1
x
=



βn
x µn
x + βW
x W n+1
x
βnx + βW
x
if xn = x,
µn
x
otherwise;
βn+1
x
=

βn
x + βW
x
if xn = x,
βn
x
otherwise.
After n measurements, our uncertainty about µx reﬂects a mixture of our conﬁdence
in our initial belief (captured by β0
x) and the precision in the measurements W n
x .
Belief Architectures
As we saw in Chapters 7 and 8, we can represent our belief about the value of
a state, or the value of a state-action pair, using three fundamental architectures:
lookup table, parametric (especially linear models with basis functions), and non-
parametric. Each of these carries speciﬁc implications in terms of our need to
explore and learn. Lookup tables have a parameter (a value) for each state, and as
a result algorithms for learning often carry requirements that we visit each state
inﬁnitely often if we want to guarantee convergence. Parametric models require
that we visit states with sufﬁcient diversity so that we can statistically identify
the regression parameters. Nonparametric models are closer in behavior to lookup
tables, but with continuous states the requirement to visit (discrete) states inﬁnitely
often is replaced with conditions to visit neighborhoods of regions with sufﬁcient
density.
Discussion
Bayesian belief models are fairly common in the optimal learning literature, but
less so in approximate dynamic programming. One major reason is that we may
not have a natural prior distribution of belief about the value of being in a state.
While the ability to express a prior distribution of belief can be a major beneﬁt, it
can also represent a major data hurdle if this is neither readily apparent nor easy
to express. Bayesians often address this issue using a method known as empirical
Bayes, where an initial sample of observations is used to build the prior distribution.
12.2.2
Online versus Ofﬂine Learning
We begin by introducing two fundamental problem classes in learning, which we
refer to as ofﬂine learning and online learning.

462
exploration versus exploitation
Ofﬂine Learning
Imagine that we have a budget of N measurements, after which we have to use
what we have learned to pick the best design. A design could be the layout of a
manufacturing facility, the structure of a molecular compound for a new drug, or a
policy for optimizing a dynamic program. If x is a design and θπ,N
x
is our estimate
of how well x performs based on N measurements that were allocated using policy
π (in this setting, π is the rule for making measurements), then a sample estimate
of how well policy π has performed is given by
F π = max
x
µxN ,
(12.1)
where xN = arg maxxθN
x is the choice that appears to be best based on our estimate
θN
x of each alternative after N measurements. We write the problem of ﬁnding the
best measurement policy using
max
π
EπF π.
Here we write the expectation Eπ as a function of π with the understanding that
the measurement policy determines what we measure next.
Ofﬂine learning when the measurements x are discrete and ﬁnite is known as
the ranking and selection problem. If x is continuous, the problem is generally
referred to as stochastic search.
Online Learning
Imagine again that we have again a budget of N measurements, but now we have
to learn as we go. Perhaps we have to ﬁnd the best path to get from our apartment
to our new job, or we have to ﬁnd the best price for selling a product, or we have
to determine the best dosage for a new patient. In all these cases we have to learn
as we go. As a result we care about the rewards we earn (or costs we incur) as we
proceed. Let θ π,n
x
be our estimate of the value of alternative x after n measurements
while using policy π to determine what to measure next. Our objective function is
given by
max
π
Eπ
N

n=1
γ nµxn,
(12.2)
where xn is the measurement decision we make when using policy π. For example,
we could choose to measure xn = arg maxxθn
x , which is to say, we choose to
measure the alternative that appears to be best given what we know after n mea-
surements. We should also note that the expectation in equation (12,2) is over two
sets of random variables: a) the true values µx giving the value of each alternative
and b) the observations that are made.
Online learning problems are conventionally referred to as bandit problems. If
the measurements x are discrete and ﬁnite, the problem is known as the multi-armed
bandit problem, described in greater detail below.

an introduction to learning
463
Online versus Ofﬂine Dynamic Programming
Dynamic programs also arise in online and ofﬂine settings. An online setting arises
when we are trying to optimize a problem as it happens. Perhaps we want to
optimize the control of a thermostat or the storage of energy in a battery while a
system is in operation. We may be interested in testing a strategy that does not look
good to us now but might be better than we think. So it means that we have to run
the risk of incurring lower contributions (or higher costs) while learning about the
beneﬁt of a new policy.
At the same time there are many settings where we have the ability to learn a
value function approximation using simulations. During these simulations we do
not really care if we make poor decisions while learning a better value function.
Once the simulations are ﬁnished, we use the value function approximations in a
production setting.
12.2.3
Physical States versus Belief States
Imagine that we are taking measurements of a set of discrete alternatives x ∈X.
In Chapter 7 we introduced the idea of a belief state or state of knowledge Kn
that captures what we know (or believe) about the performance of each choice x.
If we are using a frequentist perspective, our knowledge state can be represented
using
Kn =

θn
x , ˆσ 2,n
x
, Nn
x

x∈X .
If we are using a Bayesian belief model, then we would represent our state of
knowledge using
Kn =

θn
x , ˆσ 2,n
x

x∈X .
If we are using a parametric model with a linear architecture (with a frequentist
perspective), our state of knowledge would be given by
Kn = (θn, Bn).
If we are using a nonparametric model, the knowledge state is literally the entire
history of observations,
Kn = (ˆvi, xi)n
i=1.
We could just as easily let Sn be our state of knowledge. However, now consider
a problem where we have a person, vehicle, sensor, or storage device that exhibits
what we would call a physical state.
In Section 12.1 the physical state of our
nomadic trucker might be its location, although it could also include other attributes
of the truck (e.g., fuel level and maintenance status). Perhaps we are moving around
a graph, and as we traverse arcs in the graph we collect information about the cost
of the arc. We may have a belief about the expected cost, but as we gain experience,

464
exploration versus exploitation
we are willing to modify our beliefs. However, if we are at node i, we can only
observe links that are emanating from node i. If we are at node i and choose link
(i, j), we observe a realization of ˆCij, but in the process we also transition to
node j.
Since the state of our system captures everything, let Rn be our physical state
after n transition. In our graph problem, Rn would capture the node at which we
are located. Let cn
ij be our estimate of the expected cost on link (i, j), and let σ 2,n
ij
be our estimate of the variance. We could say that Kn = (cn, σ 2,n)ij is our state
of knowledge about the network, while Rn is our physical state (the node where
we are currently positioned). The state of our system would be written
Sn = (Rn, Kn).
Now we have a problem with a physical state and a knowledge state. Reﬂecting the
confusion in the research community over the deﬁnition of a state variable, some
authors (including Bellman) refer to Sn as the hyperstate, but given our deﬁnitions
(see Section 5.4), this is a classic state variable since it captures everything we
need to know to model the future evolution of our system.
12.3
HEURISTIC LEARNING POLICIES
Much of the challenge of estimating a value function is identical to that facing any
statistician trying to ﬁt a model to data. The biggest difference is that in dynamic
programming we may choose what data to collect by controlling what states to
visit. Further complicating the problem is that it takes time (and may cost money)
to visit these states to collect the information. Do we take the time to visit the
state and better learn the value of being in the state? Or do we live with what
we know?
Below we review several simple strategies (which we ﬁrst saw in Chapter 7),
any of which can be effective for speciﬁc problem classes.
12.3.1
Pure Exploitation
A pure exploitation strategy assumes that we have to make decisions by solving
xn
t = arg maxxt∈Xt

C(Sn
t , xt) +V
n−1
t
(SM,x(Sn
t , xt))

.
This decision then determines the next state that we visit. Some authors refer to
this as a greedy strategy, since we are doing the best that we think we can given
what we know.
A pure exploitation strategy may be needed for practical reasons. For example,
consider a large resource allocation problem where we have a resource vector Rt
that we act on with a decision vector xt. For some applications the dimensionality

heuristic learning policies
465
of Rt may be in the thousands, while xt may be in the tens of thousands. For
problems of this size, exploration may be of little or no value (what is the point in
randomly sampling even a million states out of a population of 10100?). Exploitation
strategies focus our attention on states that offer some likelihood of being states
we are interested in.
The problem with pure exploitation is that it is easy to become stuck in a
local solution because we have poor estimates of the value of being in some
states. While it is not hard to construct small problems where this problem is
serious, the errors can be substantial on virtually any problem that lacks speciﬁc
structure that can be exploited to ensure convergence. As a rule, optimal solutions
are not available for large problems, so we have to be satisﬁed with doing the best
we can. But just because your algorithm appears to have converged, do not fool
yourself into believing that you have reached an optimal, or even near-optimal,
solution.
12.3.2
Pure Exploration
Let us use an exogenous process (e.g., random selection) to choose either a state
to visit or a state-action pair (which leads to a state). Once in a state, we sample
information and obtain an estimate of the value of being in the state, which is then
used to update our estimate.
In a pure exploration strategy we can guarantee that we visit every state, or
at least have a chance of visiting every state. This property is often used to pro-
duce convergence proofs. We need to remember that some problems have 10100
states or more, so even if we run a million iterations, we may sample only a frac-
tion of the complete state space. For problems with large state spaces, random
exploration (i.e., choosing states or actions purely at random) is unlikely to work
well. This is a reminder that provably convergent algorithms can work terribly in
practice.
The amount of exploration we undertake depends in large part on the cost of
collecting the information (how much time does it take to run each iteration) and
the value of the information we collect. It is important to explore the most important
states (of course, we may not know which states are important).
12.3.3
Persistent Excitation
In engineering applications we often encounter problems where states and actions
are continuous, and possibly vector-valued. For example, St might be the coordi-
nates (three dimensions) and velocity (three dimensions) of a robot or aircraft. An
action xt might be a three dimensional force applied to the object, where we might
be willing to assume that the effect of the action is deterministic. We might write
our transition equation using

466
exploration versus exploitation
St+1 = St + Axt,
where A is a suitably dimensioned matrix that translates our three-dimensional
force to our six-dimensional state. We can force our system to visit other states by
adding a noise term
St+1 = St + Axt + εt+1,
where εt+1 is a suitably dimensioned column vector of random variables that per-
turbs the state that we thought we would visit. This strategy is known in the control
theory community as persistent excitation.
12.3.4
Epsilon-Greedy Exploration
A common strategy is to mix exploration and exploitation. We might specify an
exploration rate ϵ where ϵ is the fraction of iterations where decisions should be
chosen at random (exploration). The intuitive appeal of this approach is that we
maintain a certain degree of forced exploration, while the exploitation steps focus
attention on the states that appear to be the most valuable.
In practice, using a mix of exploration steps only adds value for problems with
relatively small state or action spaces. The only exception arises when the problem
lends itself to an approximation that is characterized by a relatively small number
of parameters. Otherwise, performing, say, 1000 exploration steps for a problem
with 10100 states may prove to have little or no practical value.
A useful variation is to let ϵ decrease with the number of iterations. For example,
let
ϵn(s) =
c
N n(s),
where 0 < c < 1 and where Nn(s) is the number of times we have visited state s
by iteration n. When we explore, we will choose an action a with probability 1/|A|.
We choose to explore with probability ϵn(s). This means that the probability we
choose decision a when we are in state s, given by P n(s, a), is at least ϵn(s)/|A|.
This guarantees that we will visit every state inﬁnitely often since
∞

n=1
P n(s, a) =
∞
n=1 ϵn(s)
|A|
= ∞.
12.3.5
Interval Estimation
Interval estimation sets the value of a decision to the 90th or 95th percentile of the
estimate of the value of a decision. Thus our decision problem would be
max
a

θ n
a + zασ n
a

.

heuristic learning policies
467
Here σ n
a is our estimate of the standard deviation of θ n
a . As the number of times
we observe action a goes to inﬁnity, σ n
a goes to zero. The parameter zα is best
viewed as a tunable parameter, although it is common to choose values around 2
or 3. This has the effect of valuing each action at its 90th or 95th percentile.
12.3.6
Upper Conﬁdence Bound Sampling Algorithm
The upper conﬁdence bound (UCB) uses the same
idea as interval estimation,
but with a somewhat different scaling factor for the standard deviation. If n is our
iteration counter (which means the total number of samples across all decisions),
and N n
a is the number of times we have sampled action a after n iterations, then
the UCB algorithm makes a decision by solving
max
a

θn
a + Cmax
;
2 ln n
Nna

,
where Cmax is the maximum possible contribution, and Nn
a is the number of times
we have sampled action a. Instead of Cmax, we might use an estimate of the 95th
percentile for ˆC (not the 95th percentile of θn).
12.3.7
Boltzmann Exploration
The problem with exploration steps is that you are choosing an action a ∈A at
random. Sometimes this means that you are choosing really poor decisions where
you are learning nothing of value. An alternative is Boltzmann exploration where
from state s, an action a is chosen with a probability proportional to the estimated
value of an action. For example, let Q(s, a) = C(s, a) +V
n(s, a) be the value of
choosing decision a when we are in state s. Using Boltzmann exploration, if we
are in state s at iteration n we would choose action a with probability
P n(s, a) =
eQ(s,a)/T

a′∈A eQ(s,a′)/T .
T is known as the temperature, since in physics (where this idea has its origins),
electrons at high temperatures are more likely to bounce from one state to another
(we generally replace 1/T with a scaling factor β). As the parameter T increases,
the probability of choosing different actions becomes more uniform. As T →0,
the probability of choosing the best decision approaches 1.0. It makes sense to start
with T relatively large and steadily decrease it as the algorithm progresses.
It is common to write P n(s, a) in the form
P n(s, a) =
eβn(s)Q(S,a)

a′∈A eβn(s)Q(S,a′) .
(12.3)
where βn(s) is a state-speciﬁc scaling coefﬁcient. Let Nn(s) be the number of
visits to state s after n iterations. We can ensure convergence if we choose βn(s)

468
exploration versus exploitation
so that P n(s, a) ≥c/n for some c ≤1 (which we choose below). This means that
we want
eβn(s)Q(S,a)

a′∈A eβn(s)Q(S,a′) ≥
c
Nn(s),
Nn(s)eβn(s)Q(S,a) ≥c

a′∈A
eβn(s)Q(S,a′).
Let |A| be the number of actions, and let
amax(S) = arg maxa∈AQ(S, a)
be the decision that returns the highest value of Q(S, a). Then we can require that
βn(s) satisfy
Nn(s)eβn(s)Q(S,a) ≥c|A|eβn(s)Q(S,amax(S)),
NN(S)
c|A|
≥eβn(s)(Q(S,amax(S))−Q(S,a)),
ln Nn(s) −ln c|A| ≥βn(s)(Q(S, amax(S)) −Q(S, a)).
Now choose c = 1/|A| so that ln c|A| = 0. Solving for βn(s) gives us
βn(s) = N n(s)
δQ(s),
where δQ = maxa∈A |Q(S, amax(S)) −Q(S, a)|. Since δQ is bounded, this choice
guarantees that βn(s) →∞, which means that in the limit we will be only visiting
the best decision (a pure greedy policy).
Boltzmann exploration provides for a more rational choice of decision that
focuses our attention on better decisions, but provides for some degree of explo-
ration. The probability of exploring decisions that look really bad is small, limiting
the amount of time that we spend exploring poor decisions (but this assumes we
have reasonable estimates of the value of these state/action pairs). Those that appear
to be of lower value are selected with a lower probability. We focus our energy
on the decisions that appear to be the most beneﬁcial, but provide for intelligent
exploration. However, this idea only works for problems where the number of
decisions A is relatively small, and it requires that we have some sort of estimate
of the values Q(S, a).
12.3.8
Remarks
The trade-off between exploration and exploitation is illustrated in Figure 12.2
where we are estimating the value of being in each state for a small problem with
a few dozen states. For this problem we are able to compute the exact value func-
tion, which allows us to compute the value of a policy using the approximate value

heuristic learning policies
469
70
75
80
85
90
95
100
1
19
17 25 33 41 49 57 65 73 81 89 97 105 113 121 129 137 145
Pure exploration
Pure exploitation
Figure 12.2
Pure exploration outperforms pure exploitation initially, but slows as the iterations
progress.
function as a percentage of the optimal. This graph nicely shows that pure explo-
ration has a much faster initial rate of convergence, whereas the pure exploitation
policy works better as the function becomes more accurate.
This behavior, however, is very problem-dependent. The value of any explo-
ration strategy drops as the number of parameters increases. If a mixed strategy
is used, the best fraction of exploration iterations depends on the characteristics
of each problem and may be difﬁcult to ascertain without access to an optimal
solution. Tests on smaller, computationally tractable problems (where exploration
is more useful) will not tell us the right balance for larger problems.
Exploration strategies are sometimes chosen because they guarantee that, in the
limit, actions will be chosen inﬁnitely often. This property is then used to prove
that estimates will reach their true value. Unfortunately, “provably convergent”
algorithms can produce extremely poor solutions. A proof of convergence may
bring some level of comfort, but in practice, it may provide almost no guarantee
at all of a high-quality solution.
A serious problem arises when the action space is large (or inﬁnite). In particular,
consider problems where the decision is a vector xt. Even if xt is discrete, the
number of potential decisions may be extremely large. For example, it is not that
hard to create action spaces with 10100 actions (the curse of dimensionality creates
problems like these fairly easily). By contrast, running a dynamic programming
algorithm for more than 100,000 iterations can get very expensive, even for small
applications. As of this writing, the literature on effective exploration strategies is
largely limited to small action spaces.

470
exploration versus exploitation
12.4
GITTINS INDICES FOR ONLINE LEARNING
For the most
part the best balance of exploration and exploitation is ad hoc,
problem-dependent and highly experimental. There is, however, one body of theory
that offers some very important insights into how to best make the trade-off between
exploring and exploiting. This theory is often referred to as multi-armed bandits,
which is the name given to the underlying mathematical model, or Gittins Indices,
that refers to an elegant method for solving the problem.
There are two perspectives of the information acquisition problem. First, this is
a problem in its own right that can be modeled and solved as a dynamic program.
There are many situations where the problem is to collect information, and we
have to balance the cost of collecting information against the beneﬁts of using
it. The second perspective is that this is a problem that arises in virtually every
approximate dynamic programming problem. As we illustrated with our nomadic
trucking application (which is, after all, a generic discrete dynamic program), we
have to visit states to estimate the value of being in a state. We depend on our
value function approximation Vt(Sa
t ) to approximate the value of a decision that
takes us to state Sa
t . Our estimate may be low, but the only way we are going to
ﬁnd out is to actually visit the state.
In this section we begin with a pure application of information acquisition and
then make the transition to its application in a broad range of dynamic programming
applications.
12.4.1
Foundations
Consider the problem faced by a gambler playing a set of slot machines (often
referred to as “one-armed bandits”) in a casino. Now pretend that the probability
of winning is different for each slot machine, but we do not know what these
probabilities are. We can, however, obtain information about the probabilities by
playing a machine and watching the outcomes. Because our observations are ran-
dom, the best we can do is to obtain statistical estimates of the probabilities, but
as we play a machine more, the quality of our estimates improves.
Since we are looking at a set of slot machines, the problem is referred to as
the multi-armed bandit problem. This is a pure exercise in information acquisition,
since after every round, our player is faced with the same set of choices. Contrast
this situation with most dynamic programs that involve allocating an asset where
making a decision changes the attribute (state) of the asset. In the multi-armed
bandit problem, after every round the player faces the same decisions with the
same rewards. All that has changed is what she knows about the system.
This problem, which is extremely important in approximate dynamic program-
ming, provides a nice illustration of what might be called the knowledge state
(some refer to it as the information state). The difference between the state of
the resource (in this case, the player) and the state of what we know has confused
authors since Bellman ﬁrst raised the issue. The vast majority of papers in dynamic
programming implicitly assume that the state variable is the state of the resource or

gittins indices for online learning
471
the physical state of the system. This is precisely the reason that our presentation
in Chapter 5 adopted the term “resource state” to be clear about what we were
referring to. In other areas of engineering we might use the term “physical state.”
In our multi-armed bandit problem let A be the set of slot machines, and let
ˆCa be the random variable that gives the amount that we win if we play bandit a.
Most of our presentation assumes that ˆCa is normally distributed. Let µa be the
true mean of ˆCa (which is unknown), and let σ 2
a be the variance (which we may
assume is known or unknown). Let (θn
a , ˆσ 2,n
a
) be our estimate of the mean and
variance of ˆCa after n iterations. Under our assumption of normality the mean and
variance completely determine the distribution.
We next need to specify our transition equations. When we were managing
physical assets, we used equations such as Rt+1 = [Rt + at −Dt+1]+ to capture
the quantity of assets available. In our bandit problem we have to model how our
distribution of belief evolves over time.
The theory surrounding bandit problems, and much of the literature on informa-
tion collection, has evolved in a Bayesian framework, where we view µa, the true
value of action a, to be a random variable. We assume that we begin with a prior
distribution of belief where µa is normally distributed with mean θ0
a and precision
β0
a (recall that this is the inverse of the variance).
After n measurements we assume that our belief has evolved to where µa is
normally distributed with mean θ n
a and precision βn
a . We write our belief state as
Sn = (θn
a , βn
a )a∈A, with the assumption of normality implicit. We assume that we
choose to measure an = Aπ(Sn), after which we observe ˆCn+1, where we assume
that the precision of our observation of ˆC has known precision βC. We would then
use this information to update our belief using our Bayesian updating formulas
(ﬁrst presented in Section 7.3.3), given by
µn+1
a
=



βn
a µn
a + βC
a ˆCn+1
a
βna + βCa
if an = a,
µn
a
otherwise;
(12.4)
βn+1
a
=

βn
a + βC
a
if an = x,
βn
a
otherwise.
(12.5)
Our goal is to ﬁnd a policy that determines which action to take to collect
information. Let an = Aπ(Sn) be the action we take after making n observations
using policy π. Note that using our notational system, we have a0 as the ﬁrst action
(before we have any observations), and so an means that we observe ˆCn+1
an . Let µa
be the (unknown) expected true value of choosing action a. θn is our best estimate
of this expectation after n measurements, and ˆCn+1 is a random variable whose
mean is θn (which we know) that is also equal to µa (which we do not know). We
can state the problem of ﬁnding the best policy in terms of solving
max
π
E
∞

n=0
γ nµan,

472
exploration versus exploitation
where an = Aπ(Sn). We can equivalently write this as
max
π
E
∞

n=0
γ nθn
an,
keeping in mind that an must depend on the information in Sn.
One way to solve the problem is to use Bellman’s equation
V n(Sn) = max
a∈A

C(Sn, a) + γ E{V n+1(Sn+1)|Sn}

.
It is important to keep in mind that Sn is our belief state (or state of knowledge),
and that V n(Sn) is the expected value of our earnings given our current state of
belief. The problem is that if we have |A| actions, then our state variable has a
state variable with 2|A| continuous dimensions. Of course, an interesting line of
research would be to use approximate dynamic programming, which allows us to
solve the exploration–exploitation problem of approximate dynamic programming.
In a landmark paper (Gittins and Jones, 1974) it was shown that an optimal
policy can be designed using an index policy. That is, it is possible to compute
a number νn
a for each bandit a, using information from just this bandit. It is then
optimal to choose which bandit to play next by simply ﬁnding the largest νn
a for
all a ∈A. This is known as an index policy, and the values νn
a are widely known
as Gittins indices. We develop these next.
12.4.2
Basic Theory of Gittins Indices
Say we face the choice of playing a single slot machine, or stopping and converting
to a process that pays a ﬁxed reward ρ in each time period until inﬁnity. If we
choose to stop sampling and accept the ﬁxed reward, the total future reward is
ρ/(1 −γ ). Alternatively, if we play the slot machine, we not only win a random
amount ˆC, we also learn something about the parameter µ that characterizes the
distribution of ˆC (for our presentation, E ˆC = µ, but µ could be a vector of param-
eters that characterizes the distribution of ˆC). θn represents our state variable (i.e.,
our current estimate of the mean). Let C(θn) = E ˆC = θn be our expected reward
given our estimate θ n. We use the format C(θn) for consistency with our earlier
models, where we wrote the contribution as a function of the state (we do not have
an action for this simple problem). The optimality equations can now be written
V (θn, ρ) = max

ρ + γ V (θn, ρ), C(θn) + γ E
'
V (θn+1, ρ)
.. θn(
, (12.6)
where we have written the value function to express the dependence on ρ.
Since we have an inﬁnite horizon problem, the value function must satisfy the
optimality equations
V (θ, ρ) = max

ρ + γ V (θ, ρ), C(θ) + γ E
'
V (θ′, ρ)
.. θ
(
,
where θ ′ is deﬁned by equation (12.4). It can be shown that if we choose to stop
sampling in iteration n and accept the ﬁxed payment ρ, then that is the optimal

gittins indices for online learning
473
strategy for all future rounds. This means that starting at iteration n, our optimal
future payoff (once we have decided to accept the ﬁxed payment) is
V (θ, ρ) = ρ + γρ + γ 2ρ + · · ·
=
ρ
1 −γ ,
which means that we can write our optimality recursion in the form
V (θn, ρ) = max
2
ρ
1 −γ , C(θn) + γ E
'
V (θn+1, ρ)
.. θn(3
.
(12.7)
Now for the magic of Gittins indices. Let ν be the value of ρ that makes the two
terms in the brackets in (12.7) equal. That is,
ν
1 −γ = C(θ) + γ E
'
V (θ′, ν)
.. θ
(
.
(12.8)
Let νGitt(θ, σ, σW, γ ) be the solution of (12.7). The optimal solution depends on the
current estimate of the mean θ, its variance σ 2, the variance of our measurements
σ 2
W, and the discount factor γ . (For notational simplicity we are assuming that
the measurement noise σ 2
W is independent of the action a, but this assumption is
easily relaxed.) Next assume that we have a family of slot machines A, and let
νGitt,n
a
(θn
a , σ n
a, σW, γ ) be the value of ν that we compute for each slot machine
a ∈A. An optimal policy for selecting slot machines is to choose the slot machine
with the highest value for νGitt,n
a
(θn
a , σ n
a, σW, γ ). Such policies are known as index
policies, and they refer to the property that the parameter νGitt,n
a
(θn
a , σ n
a, σW, γ )
for alternative a depends only on the characteristics of alternative a. For this
problem, the parameters νGitt,n
a
(θ n
a , σ n
a, σW, γ ) are called Gittins indices.
To put this solution in more familiar notation, imagine that we are facing
the problem of choosing a decision a ∈A. Assume that at each iteration, we
face the same decisions, and we are learning the value of making a decision
just as we were learning the value of a reward from using a slot machine. Let
νGitt,n
a
(θn
a , σ n
a, σW, γ ) be the “value” of making decision a, given our current
belief (captured by (θn
a , σ 2
a)) about the potential reward we would receive from
this decision. When we ignore the value of acquiring information (as we have done
in our presentation of approximate dynamic programming algorithms up to now),
we would make a decision by solving
max
a
θn
a .
This solution might easily lead us to avoid a decision that might be quite good,
but that we currently think is poor (and we are unwilling to learn anything more
about the decision). Gittins theory tells us to solve
max
a
νGitt,n
a
(θn
a , σ n
a, σW, γ ).

474
exploration versus exploitation
The computation of Gittins indices highlights a subtle issue when computing
expectations for information-collection problems. The proper computation of the
expectation needed to solve the optimality equations requires, in theory, knowl-
edge of exactly the distribution that we are trying to compute. To illustrate, the
expected winnings are given by C(θn) = E ˆC = µ, but µ is unknown. Instead, we
adopt a Bayesian approach that our expectation is computed with respect to the
distribution we believe to be true. Thus at iteration n we believe that our win-
nings are normally distributed with mean θn, so we would use C(θn) = θn. The
term E
'
V (θn+1, ρ)
.. θn(
captures what we believe the effect of observing ˆCn+1
will have on our estimate θn+1, but this belief is based on what we think the
distribution of ˆCn+1 is, rather than the true distribution.
The beauty of Gittins indices (or any index policy) is that it reduces
N-dimensional problems into a series of one-dimensional problems. The problem
is that solving equation (12.7) (or equivalently, (12.8)) offers its own challenges.
Finding νGitt,n
a
(θ, σ, σW, γ ) requires solving the optimality equation in (12.7)
for different values of ρ until (12.8) is satisﬁed. Although algorithmic procedures
have been designed for this, they are not simple.
12.4.3
Gittins Indices for Normally Distributed Rewards
The calculation of Gittins indices is simpliﬁed for special classes of distributions. In
this section we consider the case where the observations of rewards ˆC are normally
distributed. This is the case we are most interested in, since in the next section we
are going to apply this theory to the problem where the unknown reward is in fact
the value of being in a future state V(s′). Since V(s′) is computed using averages
of random observations, the distribution of V(s′) will be closely approximated by
a normal distribution.
Students learn in their ﬁrst statistics course that normally distributed random
variables satisfy a nice property. If Z is normally distributed with mean 0 and
variance 1 and if
X = µ + σZ,
then X is normally distributed with mean µ and variance σ 2. This property sim-
pliﬁes what are otherwise difﬁcult calculations about probabilities of events. For
example, computing P[X ≥x] is difﬁcult because the normal density function can-
not be integrated analytically. We instead have to resort to numerical procedures.
But because of the above-mentioned translationary and scaling properties of nor-
mally distributed random variables, we can perform the difﬁcult computations for
the random variable Z (the “standard normal deviate”), and use this to answer
questions about any random variable X. For example, we can write
P[X ≥x] = P
2X −µ
σ
≥x −µ
σ
3
= P
2
Z ≥x −µ
σ
3
.

gittins indices for online learning
475
Thus the ability to answer probability questions about Z allows us to answer the
same questions about any normally distributed random variable.
The same property applies to Gittins indices. Although the proof requires some
development, it is possible to show that
νGitt,n(θ n, σ n, σW, γ ) = θ + 
 σ n
σW
, γ

σW,
where

 σ n
σW
, γ

= νGitt,n(0, σ, 1, γ )
is a “standard normal Gittins index” for problems with mean 0 and variance 1.
Note that σ n/σW increases with n, and that (σ n/σW, γ ) decreases toward zero
as σ n/σW increases. As n →∞, νGitt,n(θn, σ n, σW, γ ) →θn.
Unfortunately, as of this writing, there do not exist easy-to-use software utilities
for computing standard Gittins indices. The situation is similar to doing statistics
before computers when students had to look up the cumulative distribution for the
standard normal deviate in the back of a statistics book. Table 12.2 is exactly such
a table for Gittins indices. The table gives indices for both the variance-known
and variance-unknown cases, but only for the case where σ n/σW = 1/n. In the
variance-known case we assume that σ 2 is given, which allows us to calculate
the variance of the estimate for a particular slot machine just by dividing by the
number of observations.
Lacking standard software libraries for computing Gittins indices, researchers
have developed simple approximations. As of this writing, the most recent of these
works as follows. First, it is possible to show that
 (s, γ ) =
	
−log γ · b

−s2
log γ

.
(12.9)
A good approximation of b(s), which we denote by ˜b(s), is given by
˜b(s) =



s/
√
2,
s ≤1
7,
e−0.02645(log s)2+0.89106 log s−0.4873,
1
7 < s ≤100,
√s (2 log s −log log s −log 16π)1/2 ,
s > 100.
Thus the approximate version of (12.9) is
νGitt,n(θ, σ, σW, γ ) ≈θn + σW
	
−log γ · ˜b

−
σ 2,n
σ 2
W log γ

.
(12.10)

476
exploration versus exploitation
Table 12.2
Gittins indices (σ n/σW, γ ) for the case of observations that are
normally distributed with mean 0, variance 1, and where σ n/σW = 1/n
Discount Factor
Known Variance
Unknown Variance
Observations
0.95
0.99
0.95
0.99
1
0.9956
1.5758
—
—
2
0.6343
1.0415
10.1410
39.3343
3
0.4781
0.8061
1.1656
3.1020
4
0.3878
0.6677
0.6193
1.3428
5
0.3281
0.5747
0.4478
0.9052
6
0.2853
0.5072
0.3590
0.7054
7
0.2528
0.4554
0.3035
0.5901
8
0.2274
0.4144
0.2645
0.5123
9
0.2069
0.3808
0.2353
0.4556
10
0.1899
0.3528
0.2123
0.4119
20
0.1058
0.2094
0.1109
0.2230
30
0.0739
0.1520
0.0761
0.1579
40
0.0570
0.1202
0.0582
0.1235
50
0.0464
0.0998
0.0472
0.1019
60
0.0392
0.0855
0.0397
0.0870
70
0.0339
0.0749
0.0343
0.0760
80
0.0299
0.0667
0.0302
0.0675
90
0.0267
0.0602
0.0269
0.0608
100
0.0242
0.0549
0.0244
0.0554
Source: From Gittins (1989).
12.4.4
Upper Conﬁdence Bounding
A strategy that has received considerable theoretical interest is known as the upper
conﬁdence bound, which produces an easily computable policy. The idea is to
create an upper bound on the potential reward from a particular action. If θn
a is our
estimate of the value of the reward from action a after n observations, the UCB
policy is given by
AUCP = θn
a +
;
2
Nna
g
Nna
n

,
(12.11)
where Nn
a is the number of times we have measured action a after n measurements,
and g(w) is given by
g(w) = log 1
2 −1
2 log log 1
t −1
2 log 16π.
The UCB policy has been proved to be the best possible policy in terms of min-
imizing expected regret, which is the degree to which the policy underperforms
the optimal. Curiously, despite this theory, the policy seems to underperform other

the knowledge gradient policy
477
heuristics in practice. A variant of the UCB policy, dubbed the UCB1 policy, is
given by
AUCP = θn
a + 4σW
;
log n
N na
,
(12.12)
where σW is the standard deviation of the observation error.
UCB policies work on a very similar principle to Gittins indices, which is that
each action is valued based on the current estimate of the value of the action θn
a ,
plus a term that reﬂects in some way the level of uncertainty in the estimate of
the contribution. Despite the theoretical support of UCB strategies, it is common
practice to test them with a tunable parameter in front of the second term.
12.5
THE KNOWLEDGE GRADIENT POLICY
We ﬁrst saw the knowledge gradient in Chapter 7 (Section 7.4) in the context of
policy search, which is an ofﬂine application. We present the knowledge gradient
in more detail in this section, and show how it can also be adapted to online
applications. The knowledge gradient is based on a Bayesian belief model, but it
can be used for problems without a formal prior by performing an initial sample
to build an initial belief (a method known as empirical Bayes).
As with our presentation of Gittins indices, we are going to start with a pure
learning model, which means that our state variable consists purely of our belief
about the performance of a series of options. For example, we need to ﬁnd the
conﬁguration of features for a laptop that will produce the highest sales. At each
iteration we face the same set of choices and the goal is to learn the most about the
value of each choice. We defer until Section 12.6 a discussion of the much more
difﬁcult problem of learning in the presence of a physical state.
Gittins indices were derived in the context of online learning problems, which
is to say we receive rewards from choices as we learn information about the per-
formance of each choice. In Chapter 7 we introduced the concept of the knowledge
gradient in the context of ofﬂine learning, and we start by brieﬂy reviewing this
material here. But the knowledge gradient can also be adapted to online learning
problems, and we show how to do this here.
12.5.1
The Knowledge Gradient for Ofﬂine Learning
We ﬁrst introduced the knowledge gradient in Section 7.4 in the context of search-
ing over a discrete set of parameters to optimize a policy. Here we re-introduce the
knowledge gradient, but now in the context of taking an action that yields infor-
mation about the value of the action. We begin by focusing on problems without
a physical state (sometimes described as a problem with a single physical state) in
an ofﬂine setting.
The knowledge gradient seeks to learn about the value of different actions by
maximizing the value of information from a single observation. Let Sn be our state

478
exploration versus exploitation
of knowledge about the value of each action a. The knowledge gradient uses a
Bayesian model, so
Sn = (θn
a , ˆσ 2,n
a
)a∈A.
The value of being in (knowledge) state Sn is given by
V n(Sn) = µan,
where an is the choice that appears to be best given what we know after n mea-
surements, calculated using
an = max
a′∈A θn
a′.
If we choose action an = a, we observe ˆCn+1
a
, which we then use to update our
estimate of our belief about µa using our Bayesian updating equations:
θn+1
a
=



βn
a θn
a + βW
a W n+1
a
βna + βW
a
if an = x,
θn
a
otherwise;
(12.13)
βn+1
a
=

βn
a + βW
a
if an = a,
βn
a
otherwise.
(12.14)
Here βn
a is the precision (one over the variance) of our belief about µa, and βW
a
is the precision of the observation of a. Using the updating equations (12.13) and
(12.14), the value of state Sn+1(a) when we trying action an = a is given by
V n+1(Sn+1(a)) = max
a′∈A θn
a′.
The knowledge gradient is then given by
νKG,n
a
= EV n+1(SM(Sn, a, W n+1)) −V n(Sn).
Computing a knowledge gradient policy is extremely easy. We assume that all
rewards are normally distributed, and that we start with an initial estimate of the
mean and variance of the value of decision d, given by
θ0
a = initial estimate of the expected reward from making decision a,
σ 0
a = initial estimate of the standard deviation of θ0
a .
Suppose that each time we make a decision we receive a reward given by
ˆCn+1
a
= µa + εn+1,
where µa is the true expected reward from action a (which is unknown) and ε is
the measurement error with standard deviation σW (which we assume is known).

the knowledge gradient policy
479
Assume that (θn
a , σ 2,n
a ) is the mean and variance of our belief about µa after n
observations. If we take action a and observe a reward ˆCn+1
a
, we can use Bayesian
updating (which produces estimates that are most likely given what we knew before
and what we just learned) to obtain new estimates of the mean and variance for
action a.
Suppose that we try an action a where βn
a = 1/σ 2,n
a
= 1/(202) = 0.0025, and
βW = 1/σ 2
W = 1/(402) = 0.000625. Assume that θn
a = 200 and that we observe
ˆCn+1
a
= 250. The updated mean and precision are given by
θn+1
a
= βn
a θn
a + βW ˆCn+1
a
βna + βW
= (0.0025)(200) + (0.000625)(250)
0.0025 + 0.000625
= 210.
βn+1 = βn
a + βW
= 0.0025 + 0.000625
= 0.003125.
We next ﬁnd the variance of the change in our estimate of µa assuming we
choose to sample action a in iteration n. For this we deﬁne
˜σ 2,n
a
= Var[θn+1
a
−θn
a |Sn].
(12.15)
With a little work, we can write ˜σ 2,n
a
in different ways, including
˜σ 2,n
a
= σ 2,n
a
−σ 2,n+1
a
,
(12.16)
=
(σ 2,n
a )
1 + σ 2
W/σ 2,n
a
.
(12.17)
Equation (12.16) expresses the (perhaps unexpected) result that ˜σ 2,n
a
measures the
change in the estimate of the standard deviation of the reward from decision a
from iteration n −1 to n. Equation (12.17) closely parallels equation (7.26). Using
our numerical example, equations (12.16) and (12.17) both produce the result
˜σ 2,n
a
= 400 −320 = 80
=
402
1 + (102/402) = 80.
We next compute
ζ n
a = −
....
θ n
a −maxa′̸=a θn
a′
˜σ na
.... .

480
exploration versus exploitation
ζ n
a is called the normalized inﬂuence of decision a. It measures the number of
standard deviations from the current estimate of the value of decision a, given by
θn
a , and the best alternative other than decision a. We then ﬁnd
f (ζ) = ζ(ζ) + φ(ζ),
where (ζ) and φ(ζ) are, respectively, the cumulative standard normal distribution
and the standard normal density. Thus, if Z is normally distributed with mean 0,
variance 1, then (ζ) = P[Z ≤ζ] while
φ(ζ) =
1
√
2π
exp

−ζ 2
2

.
The knowledge gradient algorithm chooses the decision a with the largest value of
νKG,n
a
given by
νKG,n
a
= ˜σ n
a f (ζ n
a ).
The knowledge gradient algorithm is quite simple to implement. Table 12.3
illustrates a set of calculations for a problem with ﬁve options. θ represents the
current estimate of the value of each action, while σ is the current standard deviation
of θ. Options 1, 2, and 3 have the same value for σ, but with increasing values of
θ. The table illustrates that when the variance is the same, the knowledge gradient
prefers the decisions that appear to be the best. Decisions 3 and 4 have the same
value of θ, but decreasing values of σ, illustrating that the knowledge gradient
prefers decisions with the highest variance. Finally, decision 5 appears to be the
best of all the decisions, but has the lowest variance (meaning that we have the
highest conﬁdence in this decision). The knowledge gradient is the smallest for
this decision out of all of them.
Figure 12.3 compares the performance of the knowledge gradient algorithm
against interval estimation (using α/2 = 0.025), Boltzmann exploration, Gittins
exploration, uniform exploration and pure exploitation. The results show steady
improvement as we collect more observations. The knowledge gradient and interval
estimation work the best for this problem. However, it is possible to fool interval
estimation by providing an option where the interval is very tight (for an option that
is good but not the best). Both of these methods outperform the Gittins exploration.
Table 12.3
Calculations behind the knowledge gradient algorithm
Decision
θ
σ
˜σ
ζ
f (z)
KG index
1
1.0
2.5
1.569
−1.275
0.048
0.075
2
1.5
2.5
1.569
−0.956
0.090
0.142
3
2.0
2.5
1.569
−0.637
0.159
0.249
4
2.0
2.0
1.400
−0.714
0.139
0.195
5
3.0
1.0
0.981
−1.020
0.080
0.079

the knowledge gradient policy
481
Number of measurements (N)
Policy value
Figure 12.3
Comparison of knowledge gradient against other policies as a function of the number of
observations.
12.5.2
The Knowledge Gradient for Correlated Beliefs
The knowledge gradient is particularly simple when we are evaluating discrete
actions, and where there is no structure among the actions. There are many problems
where this would not be true. For example, an action may be a discretization
of a continuous variable such as a price, location, or drug dosage. Imagine that
we currently believe that θa = θn
0.012 = 22.7 and θn
0.014 = 22.5. Now assume we
sample ˆCn+1
0.012 = 28.1. Imagine that from this observation, we update our belief
about a = 0.012 to θ n+1
0.012 = 24.3. It is likely that the value for a = 0.012 is highly
correlated with the value for a = 0.014, and hence we would expect that our
observation of ˆCn+1
0.012 would also lead us to increase our estimate of θn
0.014.
We showed how the knowledge gradient can be adapted to problems with cor-
related beliefs in Section 7.4.3. If there are |A| actions, this logic has complexity
|A|2, which is not an issue for problems with small action spaces but can become
a serious problem when the number of actions approaches 1000.
The knowledge gradient can be applied to a variety of problems where beliefs
are represented as lookup tables, parametric models (see Section 7.4.4), and non-
parametric models. We withhold further discussion until Section 12.6 when we
consider problems with a physical state.

482
exploration versus exploitation
12.5.3
The Knowledge Gradient for Online Learning
We have derived the knowledge gradient for ofﬂine learning problems. Our choice
of action does not depend directly on how good the action is: we only care about
how much information we gain from trying an action.
There are many online applications of dynamic programming where there is an
operational system that we would like to optimize. In these settings we have to
strike a balance between the value of an action and the information we gain that
may improve our choice of actions in the future. This is precisely the trade-off that
is made by Gittins indices for the multiarmed bandit problem.
It turns out that the knowledge gradient is easily adapted for online problems. As
before, let νKG,n
a
be the ofﬂine knowledge gradient, giving the value of observing
action an = a, measured in terms of the improvement in a single decision. Now
imagine that we have a budget of N decisions. After having made n decisions
(which means n observations of the value of different actions), if we observe
an = a, which allows us to observe ˆCn+1
a
, then we received an expected reward of
En ˆCn+1
a
= θn
a and obtain information that improves the contribution from a single
decision by νKG,n
a
. However, we have N −n more decisions to make. Suppose that
we learn from the observation of ˆCn+1
a
by choosing an = a, but we do not allow
ourselves to learn anything from future decisions. This means that the remaining
N −n decisions have access to the same information.
From this analysis the knowledge gradient for online applications consists of
the expected value of the single-period contribution of the measurement plus the
improvement in all the remaining decisions in our horizon. This implies that
νOLKG,n
a
= θn
a + (N −n)νKG,n
a
.
(12.18)
If we have an inﬁnite horizon problem with discount factor γ , then
νOLKG,n
a
= θn
a +
γ
1 −γ νKG,n
a
.
(12.19)
We note that the logic for incorporating correlated beliefs for the ofﬂine knowledge
gradient can now be directly applied to online problems.
12.6
LEARNING WITH A PHYSICAL STATE
Up to now we have focused on the problem of determining how to collect infor-
mation to make a choice that returns an uncertain reward. This is an impor-
tant problem in its own right, and it also lays the foundation for solving the
exploration–exploitation problem in approximate dynamic programming when we
have a physical state. However, the presence of a physical state dramatically com-
plicates the formal study of information collection.
In Section 12.1 we used our nomadic trucker example to provide an illustration
of what happens if we ignore learning and use a pure exploitation strategy. The
results are depicted in Figure 12.1, which demonstrates how a pure exploitation

learning with a physical state
483
strategy can become stuck in a small number of states. To overcome this behavior,
we have to design a strategy that forces the model to visit a diversity of states.
We can put this problem in the setting of our multi-armed bandit problem if we
think of the contribution of an action ˆCa as consisting of
ˆCn
a = C(s, a) + γV
n(SM,a(s, a)).
Now let θn be our estimate of the mean of ˆCa.
12.6.1
Heuristic Exploration Policies
We have already reviewed a series of heuristic exploration strategies in
Section 12.3 that can be applied in the multi-armed bandit setting:
• ϵ-greedy exploration
• Persistent excitation
• Boltzmann exploration
• Interval estimation
For discrete actions, ϵ-greedy is probably the most popular, but Boltzmann explo-
ration and interval estimation are also widely used. Note that all these strategies
have tunable parameters.
There are some issues we need to address when choosing an action:
1. Bias due to learning. If we use approximate value iteration, Q-learning, or
least squares policy evaluation, the value function grows with the iterations
because we are adding contributions over time. This means that visiting a
state tends to increase the value of being in the state.
2. The problem of vectors. Imagine that our decision is a vector x, which makes
the action space (feasible region) X very large or inﬁnite in size. Choosing
an action at random teaches us virtually nothing.
3. The uncertainty in our estimate of a value. Classic exploration strategies do
not consider the variance in the estimate of the value of the state SM,a(S, a)
that we next visit as a result of our action. Interval estimation is the only
exception.
ϵ-greedy avoids the bias issue because it ignores the estimated value of an action,
θn
a , when making an exploration step. Boltzmann exploration and interval estimation
are sensitive to the estimate θn
a , reducing the likelihood of testing an action if
the estimate is too low, ignoring the fact that trying an action may also raise
its value. None of the three major heuristics—ϵ-greedy, Boltzmann, or interval
estimation—can be applied to problems with vector-valued action spaces. ϵ-greedy
and Boltzmann, which are probably the most popular heuristics used in practice for
discrete actions (along with persistent excitation for continuous problems), ignore
the uncertainty in the estimate of an action, an issue that has proved to be of central

484
exploration versus exploitation
importance in the design of more formal strategies such as Gittins indices and the
knowledge gradient, as well as the highly effective interval estimation.
It is our position that it is going to be difﬁcult to design a principled exploration
strategy for algorithms based on approximate value iteration, which includes Q-
learning and least squares policy evaluation. As of this writing, we are not aware
of any rigorous theory for learning problems such as the multi-armed bandit that
can handle not only the uncertainty in our belief about the value of each choice but
also the property that trying an action increases the expected value of the action.
12.6.2
Gittins Heuristics
There has been an attempt to apply Gittins indices to problems with a physical
state, resulting in a method called the local bandit approximation (LBA), which is
an adaptation of Gittins indices to problems with a physical state. The LBA works
roughly as follows: Imagine that you are in a state s and you are considering action
a that might take you to a state s′, which allows you to update your belief about
the value of being in state s′. Assuming an ergodic process (under whatever policy
we are following), we will eventually return to state s, which will allow us to use
our newly acquired information again. Let R(s, a) be the expected reward that we
earn during this time, and let τ(s, a) be the expected time it takes to return to state
s. The Gittins index for action a is then
νLBA
a
= R(s, a)
τ(s, a) .
This can be thought of as a bandit problem that is local to state s, hence the name.
Not surprisingly, this is fairly hard to compute.
As a simpler alternative, we might apply the concept of Gittins indices in a
purely heuristic fashion, a method we call Gittins exploration. For example, we
might choose an action an using
an = arg maxa
2
C(s, a) + γV
n(SM(s, a)) + 
 σ n
σW
, γ

σW
3
,
(12.20)
where σW is the standard deviation of an observation, and (σ n/σW, γ ) is the
standard normal Gittins index that tells us how many standard deviations away
from the mean we should use (which is a function of the number of observations
n). This is a purely heuristic use of Gittins indices, and as a result it would make
sense to replace (σ n/σW, γ ) with a simple formula that declines with n such as

 σ n
σW
, γ

=
a
b + n,
where a and b would have to be tuned for a particular application.
An advantage of equation (12.20) is that it can be used as if we are pur-
suing an exploitation strategy with a modiﬁed contribution function that cap-
tures the value of collecting information for future decisions. Unlike combined

learning with a physical state
485
exploration–exploitation policies, we never choose an action purely at random
(regardless of how bad we think it might be).
A signiﬁcant limitation of all these heuristics is the presence of tunable param-
eters. In the next section we show how the knowledge gradient can be adapted
without using any tunable parameters.
12.6.3
The Knowledge Gradient Using Lookup Tables
We can apply the concept of the knowledge gradient to inﬁnite horizon, discrete
dynamic programs using the following line of thinking. If we choose action an
while in state Sn, this will take us to the post-decision state Sa,n after which
we transition to the next pre-decision state Sn+1. The sequence of pre-decision
states, actions and post-decision states is depicted in Figure 12.4. We illustrate the
calculations in this section for a lookup table representation, since these are easiest
to understand. As we point out in the section that follows, adapting this strategy
to other belief models such as linear architectures using basis functions is fairly
minor.
Deﬁne the Q-factors using the post-decision state as
Qn(Sn, a) = C(Sn, a) + γ V n(SM,a(Sn, a)).
We need to interpret V n(s) (where s is a post-decision state) as our belief about
V n after n iterations. When we apply the knowledge gradient, we are asking about
the value of learning from a single measurement. Thus, if we choose action an, we
are going to update our belief about V n(s) to V n+1(s), after which we are going
to stop and apply our policy from that point on, using V n+1(s) to guide the choice
of decisions. This means that V n′(s) = V n+1(s) for all n′ ≥n + 1.
If we assume we are not going to do any more learning, we would make our
decision by solving
an = arg maxa

C(Sn, a) + γ V n(SM,a(Sn, a))

.
(12.21)
Sn
an
1
an
2
an
3
a1
a2
a3
Sn,a2
Sn,a2
Sn+1
Sn,a1
Sn+1,a1
Sn+1,a2
Sn+1,a3
Figure 12.4
Decision tree showing pre-decision states St, actions at, post-decision states Sa
t , and
subsequent states and actions.

486
exploration versus exploitation
However, if we choose action an, we will transition to state Sn,a = SM,a(Sn, an),
after which we observe W n+1 which takes us to state Sn+1 = SM(Sn, an, W n+1).
Once at state Sn+1, we are going to compute
ˆvn+1 = max
a′

C(Sn+1, a′) + γ V n(SM,a(Sn+1, a′))

.
Our intent is to use ˆvn+1 to update our belief V n(s) using the Bayesian updating
equations we ﬁrst introduced in Section 9.9.2. Using our (presumably known)
covariance matrix n, we can update our belief about the value V n(s) (where s is
the post-decision state) of each state as
V
n+1(s) = V n(s) +
ˆvn+1 −V n(s)
λ(Sa,n) + n(Sa,n, Sa,n)n(s, Sa,n),
(12.22)
where λ(Sa,n) is our (again, presumably known) variance of ˆvn+1. Note that while
we have only visited one (post-decision) state Sa,n, we update all the states s (or at
least all the states where n(s, Sa,n) > 0). We also update our covariance matrix
using
n+1(s, s′) = n(s, s′) −n(s, Sa,n)n(Sa,n, s′)
λ(Sa,n) + n(Sa,n, Sa,n).
(12.23)
Our idea is to replace equation (12.21) with a rule that recognizes that we
will learn from our action, but only once. That is, by transitioning to Sa,n and then
observing ˆvn+1, we are going to update our beliefs, and we would like to recognize
this before we choose our action. However, we are going to assume that we only
learn from this single decision. This means that we want to ﬁnd our action by
solving
an = arg maxa

C(Sn, a) + γ En
aV n+1(SM,a(Sn, a))

.
(12.24)
where En
a represents our expectation given the information we have up through
iteration n, and given that we choose action a. V n+1(SM,a(Sn, a)) represents our
updated value function after we observe ˆvn+1. We recognize that since W n+1 is a
random variable, so is Sn+1. For this reason we can write
Qn(Sn, a) = C(Sn, a) + γ En
aV n+1(SM,a(Sn, a))
= C(Sn, a) + γ

s′=Sn+1
P (s′|Sa,n)En
a max
a′ Qn+1(Sn+1, a′).
When we observe ˆvn+1, we are going to update V n(s) to obtain V n+1(s), which
we then use to compute
Qn+1(Sn+1, a′) = C(Sn+1, a′) + γ V n+1(SM,a(Sn+1, a′)).
In other words, by choosing action a when we are in state Sn means that we are
going to improve our solution of maxa′ Qn+1(Sn+1, a′). We would like to capture

learning with a physical state
487
the value of observing ˆvn+1 in terms of improving our choice of an+1 in the future,
when we make our decision about an. We now describe the steps for doing this.
Let Z be a normally distributed random variable with mean 0, variance 1. When
we are in state Sn, V n+1 is a random variable that can be written as (see Section
7.4.3 for more details)
V n+1 ∝V n +
n(s, Sa,n)
√λ(Sa,n) + n(Sa,n, Sa,n)Z.
This means that we can write Qn+1(Sn+1, a′) as
Qn+1(Sn+1, a′) = Qn(Sn+1, a′) + βn(a′)Z,
where
βn(a′) = γ n(SM,a(Sn+1, a′), Sa,n)
√λ(Sa,n) + n(Sa,n, Sa,n).
Next we need to compute
En max
a′ Qn+1(Sn+1, a′) = En max
a′

Qn(Sn+1, a′) + βn(a′)Z

.
We have already seen how to do this in Section 7.4.3, and we refer the reader
back to this section for details. The method requires sorting the actions a into a list
A = {1, 2, . . . , a, a + 1, . . . } where βn(a′ + 1) > βn(a′). To compute the list A,
we also ﬁnd numbers za′ using
za′ = Qn(Sn+1, a′) −Qn(Sn+1, a′ + 1)
βn(a′ + 1) −βn(a′)
.
We require that za′+1 > za′, and eliminate any actions a′ where this is not satisﬁed.
Given this construction, we can compute the knowledge gradient using
νKG,n(Sa,n, Sn+1) = marginal value of information gained by an action that takes
us to state Sa,n after which there is a transition to Sn+1,
= En max
a′ Qn+1(Sn+1, a′) −max
a′ Qn(Sn+1, a′)
= En max
a′ (Qn(Sn+1, a′) + βn(a′)Z) −max
a′ Qn(Sn+1, a′)
=

a′∈A
(βn(a′ + 1) −βn(a′))f (−|za′|),
where f (z) = z(z) + φ(z), and where (z) is the cumulative distribution func-
tion for the standard normal distribution, and φ(z) is the standard normal density
(this is standard notation for these distributions, and we regret the overlap with
our also standard notation for basis functions). We emphasize that Qn+1(Sn+1, a′)
is our estimate of the Q factor after the beneﬁts of learning from the ﬁrst n + 1

488
exploration versus exploitation
measurements, evaluated at the state-action pair (Sn+1, a′), while Qn(Sn+1, a′) is
our estimate of the Q factor after learning from the ﬁrst n measurements, but also
evaluated at the state action pair (Sn+1, a′). This also allows us to write
En max
a′ Qn+1(Sn+1, a′) = max
a′ Qn(Sn+1, a′) + νKG,n(Sa,n, Sn+1).
Of course, Sn+1 is a random variable, and as a result we have to compute the
expectation

Sn+1
P (Sn+1|Sa,n)En
a max
a′ Qn+1(Sn+1, a′) =

Sn+1
P (Sn+1|Sa,n) max
a′ Qn(Sn+1, a′)
+

Sn+1
P (Sn+1|Sa,n)νKG,n(Sa,n, Sn+1).
We note that we can write

Sn+1
P (Sn+1|Sa,n) max
a′ Qn(Sn+1, a′) = V n(Sa,n),
since V n(Sa,n) is the expected value of being in post-decision state Sa,n. This
allows us to write equation (12.24) as
AKG−online,n = arg maxa

C(Sn, a) + γ V n(Sa,n)
+ γ

Sn+1
P (Sn+1|Sa,n)νKG,n(Sa,n, Sn+1)

,
(12.25)
which we can write equivalently as
AKG−online,n = arg maxa

Qn(Sn, a) + γ

Sn+1
P (Sn+1|Sa,n)νKG,n(Sa,n, Sn+1)

.
(12.26)
Equation (12.26) is an equation that is very similar to an interval estimation, Gittins
indices, and the knowledge gradient for online learning that we ﬁrst saw in Section
12.5.3, with one small difference. In Section 12.5.2 our formula for the online
knowledge gradient was given by
νOLKG,n
a
= θn
a + (N −n)νKG,n
a
for ﬁnite horizon problems, or
νOLKG,n
a
= θn
a +
γ
1 −γ νKG,n
a
for inﬁnite horizon problems. The reason we do not factor the knowledge gradient
term in (12.26) by N −n or γ/(1 −γ ) is that we are computing the value of

learning with a physical state
489
information on the value function, which itself is the ﬁnite or inﬁnite horizon value
of being in a state.
The policy in (12.26) (or (12.25)) would be best suited for online dynamic pro-
grams, which is to say, problems where we are actually incurring contributions as
we are making decisions. Imagine now that we are simply training value functions
in a simulated environment, and our plan is to then use the value functions we
estimated to make decisions at a later time. This is ofﬂine training, and the policy
we would use is given by
AKG−off line,n = arg maxa


Sn+1
P (Sn+1|Sa,n)νKG,n(Sa,n, Sn+1)

. (12.27)
Both our online policy AKG−online,n and ofﬂine policy AKG−off line,n require
computing an expectation using the probability P (Sn+1|Sa,n). In practice, this is
likely to be difﬁcult to execute, and as a result we suggest using a simulated
approximation

Sn+1
P (Sn+1|Sa,n)νKG,n(Sa,n, Sn+1) ≈1
K
K

k=1
νKG,n(Sa,n, Sn+1
k
)
where Sn+1
k
= SM,W(Sa,n, W n+1
k
) and where W n+1
k
the kth simulated observation
of W n+1 and SM,W(Sa,n, W n+1
k
) is the function we use to capture the evolution
from post-decision state to pre-decision state.
The adaptation of the knowledge gradient to dynamic programs with a physical
state is quite new (as of this writing). Our decision to include such new research
reﬂects what is a paucity of practical algorithms to do efﬁcient learning in a prin-
cipled way. The method was ﬁrst suggested in Ryzhov et al. (2010) who report
on experiments for two applications: a newsvendor problem and an energy storage
problem. Both are fairly small dynamic programs (which make lookup tables pos-
sible) and serve only as an initial test. The online and ofﬂine knowledge gradient
policies were tested using both online and ofﬂine settings. That is, in the online
setting, we accumulated the contributions over 150 iterations, using a discount
factor of γ = 0.99. In the ofﬂine setting, we trained the value function for 150
iterations, and then evaluated the policy produced by the resulting value function
approximation using additional simulations without any subsequent updating.
These policies were compared against the following alternatives
• Bayesian Q-learning. This strategy uses equation (12.21) to make decisions,
and uses Bayesian updating after each step.
• Bayesian learning with ϵ-greedy exploration.
• ADP using ϵ-greedy exploration.
For the newsvendor problem, it was also possible to calculate the optimal policy
for the ofﬂine version of the problem as a benchmark.

490
exploration versus exploitation
Table 12.4
Comparison of different learning policies for online and ofﬂine versions
of a newsvendor problem
Ofﬂine Objective
Online Objective
Policy
Mean
Avg. Std. Dev.
Mean
Avg. Std. Dev
Optimal
10795
3.31
—
—
Ofﬂine KG
10639
3.45
−1645
59.90
Online KG
10188
3.16
3961
31.44
Bayesian Q-learning
10633
3.10
3820
46.15
Bayesian ϵ-greedy
10587
3.29
3328
35.28
ADP ϵ-greedy
9626
3.22
2449
23.98
Note: Also shown are the standard deviations in the estimates of the performance of each policy.
These policies were tested on 1000 different problem instances, where each was
run for 150 iterations. The results are shown in Table 12.4. As we would expect,
the ofﬂine KG policy worked best on the ofﬂine problems, while the online KG
policy worked best on the online problems. The KG policies outperformed the other
policies, although the Bayesian Q-learning policy for this example was close.
The policies were also compared on an energy storage problem with continuous
states, which eliminated the option of ﬁnding an optimal policy. The results are
shown in Table 12.5, which again shows that the ofﬂine KG works best on the
ofﬂine problem, while the online KG works best on the online problem. In this
case the KG policies appeared to signiﬁcantly outperform the other policies.
The major bottleneck computationally with this method is not the calculation
of the knowledge gradient but rather the updating of the beliefs using equations
(12.22) and (12.23). We circumvent this problem in the next section when we make
the transition to using a linear belief model.
12.6.4
The Knowledge Gradient with Parametric Beliefs*
It is possible to adapt the knowledge gradient to problems with other types of belief
structures. We focus our attention here on the most popular belief structure that
uses a set of pre-speciﬁed basis functions to create a linear model.
Table 12.5
Comparison of different learning policies for online and ofﬂine versions
of an energy storage problem
Ofﬂine Objective
Online Objective
Policy
Mean
Avg. Std. Dev.
Mean
Avg. Std. Dev
Ofﬂine KG
208.39
0.33
−260.59
16.86
Online KG
68.10
0.24
155.30
6.01
Bayesian Q-learning
133.65
0.31
76.03
1.87
Bayesian ϵ-greedy
85.65
0.31
67.10
3.00
ADP ϵ-greedy
154.47
0.35
7.09
2.57

learning with a physical state
491
The steps required for using a linear model are basically the same as we used
in Section 12.6.3 or a lookup table, with the following changes:
• Updating the belief V n(s). We are using a linear model with basis func-
tions, so we use the steps for updating the regression vector θn described in
Section 9.9, where it is adapted for a Bayesian setting.
• Computing the knowledge gradient νKG,n. We use the adaptation of the knowl-
edge gradient calculation presented in Section 7.4.4 when using a linear model.
The use of a linear model is computationally much faster than a lookup table,
even when dealing with small problems. As long as the number of features is not
too large, updating both θn and the covariance matrix θ is quite fast.
The online and ofﬂine variations of the knowledge gradient were again applied
to the energy storage problem, but this time using a simple linear model with ﬁve
basis functions. These policies were compared against ADP based on approximate
value iteration, and a pure exploitation policy. The results are shown in Table 12.6.
This time the knowledge gradient policy seems to be having a signiﬁcant impact.
12.6.5
Discussion
Considerable caution should be used before drawing conclusions based on the very
limited experimental work presented in Tables 12.4, 12.5, and 12.6. The results
are encouraging, and this appears to be the ﬁrst learning policy for dynamic pro-
grams with a physical state that scales to problems of realistic size (using a linear
belief model). Reﬂecting the derivation of the policy based on ﬁrst principles, the
knowledge gradient does not have any tunable parameters.
The biggest challenge with Bayesian learning, especially when using a linear
belief model, is creating the prior. If you pick a poor prior, and especially if you
do not accurately capture your conﬁdence in the prior, you will get a poor policy
regardless of your exploration policy.
This said, we feel there is considerable potential in the use of the knowledge
gradient in a Bayesian setting. In addition to not needing any tunable parame-
ters, it can be used in both online and ofﬂine settings. For example, we may use
AKG−online,n not only to determine what action to take that determines the next
Table 12.6
Comparison of different policies using a linear belief model on the
energy storage problem
Ofﬂine Objective
Online Objective
Policy
Mean
Avg. Std. Dev.
Mean
Avg. Std. Dev
Ofﬂine KG
702.62
2.58
−162.72
32.54
Online KG
320.49
1.96
6.01
39.01
ADP ϵ-greedy
−350.21
6.65
0.71
25.10
Pure exploitation
−618.78
0.85
−125.53
35.69

492
exploration versus exploitation
downstream state (the behavior or sampling policy), we can also use it to update
our estimate of the value of being in a state by computing
ˆvn = max
a

C(Sn, a) + γ V n(Sa,n)
+γ

Sn+1
P (Sn+1|Sa,n)νKG,n(Sa,n, Sn+1)

.
Although the value of information term would appear to distort the estimate of
the value of being in a state, this term tends to zero as the algorithm progresses.
However, this strategy means that we are using an online policy, thereby avoiding
the difﬁculties inherent in ofﬂine policies that arise when we are trying to evaluate
a learning policy while following a different sampling policy.
12.7
BIBLIOGRAPHIC NOTES
Section 12.3 A nice introduction to various learning strategies is contained in
Kaelbling (1993) and Sutton and Barto (1998). Thrun (1992) contains a good
discussion of exploration in the learning process. The discussion of Boltz-
mann exploration and epsilon-greedy exploration is based on Singh et al.
(2000). Interval estimation is due to Kaelbling (1993). The upper conﬁdence
bound is due to Lai and Robbins (1985). We use the version of the UCB
rule given in Lai (1987). The UCB1 policy is given in Auer et al. (2002).
Analysis of UCB policies are given in Lai and Robbins (1985) and Auer et
al. (2002), as well as Chang et al. (2007).
Section 12.4 What came to be known as “Gittins indices” was introduced in
Gittins and Jones (1974) to solve bandit problems (see DeGroot 1970, for
a discussion of bandit problems before the development of Gittins indices).
This was more thoroughly developed in Gittins (1979, 1981, 1989). Whittle
(1982) and Ross (1983) provide very clear tutorials on Gittins indices, helping
launch an extensive literature on the topic (e.g., see Lai and Robbins, 1985;
Berry and Fristedt, 1985; Weber, 1992). The work on approximating Gittins
indices is due to Brezzi and Lai (2002), Yao (2006), and Chick and Gans
(2009).
Section 12.5 The knowledge gradient policy for normally distributed rewards and
independent beliefs was introduced by Gupta and Miescke (1996), and sub-
sequently analyzed in greater depth by Powell et al. (2008). The knowledge
gradient for correlated beliefs was introduced by Frazier et al. (2009). The
adaptation of the knowledge gradient for online problems is due to Ryzhov
and Powell (2009).
Section 12.6 The problem of introducing active learning in dynamic programs
(implicitly, where the state represents a physical state) has been an area of

problems
493
interest for some time, but characterized primarily by heuristic algorithms.
Mike Duff and Andy Barto (Duff and Barto, 1997, 2003; Duff, 2002) devel-
oped an adaptation of Gittins indices for dynamic programs using the concept
of the local bandit approximation. Poupart et al. (2006) give an analytic
solution of the online learning problem and proposes an algorithm called
BEETLE, which is demonstrated on some very small problems. The adap-
tation of the knowledge gradient to problems with a physical state was ﬁrst
developed in Ryzhov and Powell (2010a). The adaptation of the knowledge
gradient for parametric beliefs was ﬁrst given in Negoescu et al. (2010) but
adapted to dynamic programming in Ryzhov and Powell (2010a).
PROBLEMS
12.1
Joe Torre, former manager of the great Yankees, had to struggle with the
constant game of guessing who his best hitters are. The problem is that he
could only observe a hitter if he put him in the order. Say he had four batters
that he was looking at. Table 12.7 shows their actual batting averages (batter
1 will produce hits 30 percent of the time, batter 2 will get hits 32 percent
of the time, etc.). Unfortunately, Joe did not know these numbers. As far as
he was concerned, these where all 0.300 hitters.
For each man at bat, Joe had to pick one of these hitters to hit. Table 12.7
shows what would have happened if each batter were given a chance to hit
(1 = hit, 0 = out). Again, Joe did not get to see all these numbers. He only
got to observe the outcome of the hitter who gets to hit.
Assume that Joe always let the batter hit with the best batting average.
Assume that he used an initial batting average of 0.300 for each hitter
Table 12.7
Data for problem 12.1
Actual Batting Average
0.300
0.320
0.280
0.260
Day
Batter
A
B
C
D
1
0
1
1
1
2
1
0
0
0
3
0
0
0
0
4
1
1
1
1
5
1
1
0
0
6
0
0
0
0
7
0
0
1
0
8
1
0
0
0
9
0
1
0
0
10
0
1
0
1

494
exploration versus exploitation
(in case of a tie, used batter 1 over batter 2 over batter 3 over batter 4).
Whenever a batter got to hit, calculate a new batting average by putting
an 80 percent weight on your previous estimate of his average plus a 20
percent weight on how he did for his at bat. By this logic, you would
choose batter 1 ﬁrst. Since he did not get a hit, his updated average would
be 0.80(0.200) + 0.20(0) = 0.240. For the next at bat, you would choose
batter 2 because your estimate of his average is still 0.300, while your
estimate for batter 1 is now 0.240.
After 10 at bats, who would you conclude is your best batter? Comment
on the limitations of this way of choosing the best batter. Do you have a
better idea? (It would be nice if your idea is practical.)
12.2
There are four paths you can take to get to your new job. On the map, they
all seem reasonable, and as far as you can tell, they all take 20 minutes,
but the actual times vary quite a bit. The value of taking a path is your
current estimate of the travel time on that path. In Table 12.8 we show the
travel time on each path if you had traveled that path. Start with an initial
estimate of each value function of 20 minutes with your tie-breaking rule to
use the lowest numbered path. At each iteration take the path with the best
estimated value, and update your estimate of the value of the path based on
your experience. After 10 iterations, compare your estimates of each path to
the estimate you obtain by averaging the “observations” for each path over
all 10 days. Use a constant stepsize of 0.20. How well did you do?
Table 12.8
Data for exercise 12.2
Paths
Day
1
2
3
4
1
37
29
17
23
2
32
32
23
17
3
35
26
28
17
4
30
35
19
32
5
28
25
21
26
6
24
19
25
31
7
26
37
33
30
8
28
22
28
27
9
24
28
31
30
10
33
29
17
29
12.3
We are going to try again to solve our asset selling problem, We assume
that we are holding a real asset and we are responding to a series of offers.
Let ˆpt be the tth offer, which is uniformly distributed between 500 and 600
(all prices are in thousands of dollars). We also assume that each offer is
independent of all prior offers. You are willing to consider up to 10 offers,

problems
495
and your goal is to get the highest possible price. If you have not accepted
the ﬁrst nine offers, you must accept the 10th offer.
(a) Write out the decision function you would use in an approximate
dynamic programming algorithm in terms of a Monte Carlo sample
of the latest price and a current estimate of the value function
approximation.
(b) Write out the updating equations (for the value function) you would use
after solving the decision problem for the tth offer.
(c) Implement an approximate dynamic programming algorithm using syn-
chronous state sampling. Using 1000 iterations, write out your estimates
of the value of being in each state immediately after each offer. For this
exercise you will need to discretize prices for the purpose of approx-
imating the value function. Discretize the value function in units of 5
dollars.
(d) From your value functions, infer a decision rule of the form “sell if the
price is greater than pt.”
12.4
Suppose that you are considering ﬁve options. The actual value θd, the initial
estimate θ
0
d, and the initial standard deviation σ 0
d of each θ
0
d are given in
Table 12.9. Perform 20 iterations of each of the following algorithms:
(a) Gittins exploration using (n) = 2.
(b) Interval exploration using α/2 = 0.025.
(c) The upper conﬁdence bound algorithm using W max = 6.
(d) The knowledge gradient algorithm.
(e) Pure exploitation.
(f) Pure exploration.
Each time you sample a decision, randomly generate an observation Wd =
θd + σ εZ, where σ ε = 1 and Z is normally distributed with mean 0 and
variance 1. [Hint: You can generate random observations of Z in Excel by
using =NORMSINV(RAND( )).]
Table 12.9
Data for exercise 12.4
Decision
µ
θ
0
σ 0
1
1.4
1.0
2.5
2
1.2
1.2
2.5
3
1.0
1.4
2.5
4
1.5
1.0
1.5
5
1.5
1.0
1.0
12.5
Repeat exercise 12.4 using the data in Table 12.10, with σ ε = 10.

496
exploration versus exploitation
Table 12.10
Data for exercise 12.5
Decision
µ
θ
0
σ 0
1
100
100
20
2
80
100
20
3
120
100
20
4
110
100
10
5
60
100
30
12.6
Repeat exercise 12.4 using the data in Table 12.11, with σ ε = 20.
Table 12.11
Data for exercise 12.6
Decision
µ
θ
0
σ 0
1
120
100
30
2
110
105
30
3
100
110
30
4
90
115
30
5
80
120
30
12.7
Nomadic trucker project revisited. We are going to again solve the
nomadic trucker problem ﬁrst posed in exercise 4.18, but this time we
are going to experiment with our more advanced information acquisition
policies. Assume that we are solving the ﬁnite horizon problem with a
discount factor of γ = 0.1. Compare the following policies for determining
the next state to visit:
(a) Pure exploitation. This is the policy that you would have implemented
in exercise 4.18.
(b) Pure exploration. After solving a decision problem (and updating the
value of being in a city), choose the next city at random.
(c) Boltzmann exploration. Use equation (12.3) to determine the next city
to visit.
(d) Gittins exploration. Use equation (12.20) to decide which city to visit
next, and use equation (12.10) to compute (n). You will have to exper-
iment to ﬁnd the best value of ρG.
(e) [(e)] Interval estimation. Use α/2 = 0.025.
(f) [(f)] The knowledge gradient policy.
12.8
Repeat exercise 12.7 using a discount factor γ = 0.8. How does this affect
the behavior of the search process? How does it change the performance of
the different methods for collecting information?

C H A P T E R
13
Value Function Approximations
for Resource Allocation Problems
In Chapter 8 we introduced general-purpose approximation tools for calculating
value functions without needing to assume any special structural properties. In this
chapter we focus on approximating value functions that arise in resource allocation
problems. For example, if R is the amount of resource available (water, oil, money,
vaccines) and V (R) is the value of having R units of our resource, we often ﬁnd
that V (R) might be linear (or approximately linear), nonlinear (concave), piecewise
linear, or in some cases, simply something continuous. Value functions with this
structure yield to special approximation strategies.
We consider a series of strategies for approximating the value function using
increasing sophistication:
Linear approximations.
These are typically the simplest nontrivial approxi-
mations that work well when the functions are approximately linear over the
range of interest. It is important to realize that we mean “linear in the state”
as opposed to the more classical “linear in the parameters” model that we
considered earlier.
Separable, piecewise linear, concave (convex if minimizing). These functions
are especially useful when we are interested in integer solutions. Separable
functions are relatively easy to estimate and offer special structural properties
when solving the optimality equations.
Auxiliary functions.
This is a special class of algorithms that ﬁxes an initial
approximation and uses stochastic gradients to adjust the function.
General nonlinear regression equations. Here we bring the full range of tools
available from the ﬁeld of regression.
Approximate Dynamic Programming: Solving the Curses of Dimensionality, Second Edition.
Warren B. Powell.
2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
497

498
value function approximations
Cutting planes. This is a technique for approximating multidimensional, piece-
wise linear functions that has proved to be particularly powerful for multistage
linear programs such those that arise in dynamic resource allocation problems.
An important dimension of this chapter will be our use of derivatives to estimate
value functions, rather than just the value of being in a state. When we want to
determine how much oil should be sent to a storage facility, what matters most is
the marginal value of additional oil. For some problem classes this is a particularly
powerful device that dramatically improves convergence.
13.1
VALUE FUNCTIONS VERSUS GRADIENTS
It is common in dynamic programming to talk about the problem of estimating the
value of being in a state. There are many applications where it is more useful to
work with the derivative or gradient of the value function. In one community, where
“heuristic dynamic programming” represents approximate dynamic programming
based on estimating the value of being in a state, “dual heuristic programming”
refers to approximating the gradient.
We are going to use the context of resource allocation problems to illustrate
the power of using the gradient. In principal, the challenge of estimating the slope
of a function is the same as that of estimating the function itself (the slope is
simply a different function). However, there can be important practical advantages
to estimating slopes. First, if the function is approximately linear, it may be possible
to replace estimates of the value of being in each state (or set of states) with a single
parameter that is the estimate of the slope of the function. Estimating constant terms
is typically unnecessary.
A second and equally important difference is that if we estimate the value of
being in a state, we get one estimate of the value of being in a state when we visit
that state. When we estimate a gradient, we get an estimate of a derivative for each
type of resource. For example, if Rt = (Rtr)r∈R is our resource vector and Vt(Rt)
is our value function, then the gradient of the value function with respect to Rt
would look like
∇Rt Vt(Rt) =


ˆvtr1
ˆvtr2...
ˆvtr|A|

,
where
ˆvtri = ∂Vt(Rt)
∂Rtri
.
There may be additional work required to obtain each element of the gradient,
but the incremental work can be far less than the work required to get the value

linear approximations
499
function itself. This is particularly true when the optimization problem naturally
returns these gradients (e.g., dual variables from a linear program), and even when
we have to resort to numerical derivatives. After we have all the calculations to
solve a problem once, solving small perturbations can be relatively inexpensive.
There is one important problem class where ﬁnding the value of being in a
state is equivalent to ﬁnding the derivative. That is the case of managing a single
resource (see Section 5.3.1). In this case the state of our system (the resource) is
the attribute vector r, and we are interested in estimating the value V (r) of our
resource being in state r. Alternatively, we can represent the state of our system
using the vector Rt, where Rtr = 1 indicates that our resource has attribute r (we
assume that 
r∈R Rtr = 1). In this case the value function can be written
Vt(Rt) =

r∈R
vtrRtr.
Here the coefﬁcient vtr is the derivative of Vt(Rt) with respect to Rtr.
In a typical implementation of an approximate dynamic programming algorithm,
we would only estimate the value of a resource when it is in a particular state (given
by the vector r). This is equivalent to ﬁnding the derivative ˆvr only for the value of
r where Rtr = 1. By contrast, computing the gradient ∇Rt Vt(Rt) implicitly assumes
that we are computing ˆvr for each r ∈R. There are some algorithmic strategies
(we will describe an example of this in Section 13.7) where this assumption is
implicit in the algorithm. Computing ˆvr for all r ∈R is reasonable if the attribute
state space is not too large (e.g., if r is a physical location among a set of several
hundred locations). If r is a vector, then enumerating the attribute space can be
prohibitive (it is, in effect, the “curse of dimensionality” revisited).
Given these issues, it is critical to ﬁrst determine whether it is necessary to
estimate the slope of the value function, or the value function itself. The result can
have a signiﬁcant impact on the algorithmic strategy.
13.2
LINEAR APPROXIMATIONS
There are a number of problems where we are allocating resources of different
types. As in the past we let r be the attributes of a resource and Rtr be the quantity
of resources with attribute r in our system at time t with Rt = (Rtr)r∈R. Rt may
describe our investments in different resource classes (growth stocks, value stocks,
index funds, international mutual funds, domestic stock funds, bond funds). Or Rt
might be the amount of oil we have in different reserves or the number of people in
a management consulting ﬁrm with particular skill sets. We want to make decisions
to acquire or sell resources of each type, and we want to capture the impact of
decisions now on the future through a value function Vt(Rt).
Rather than attempt to estimate Vt(Rt) for each value of Rt, it may make more
sense to estimate a linear approximation of the value function with respect to
the resource vector. Linear approximations can work well when the single-period
contribution function is continuous and increases or decreases monotonically over

500
value function approximations
the range we are interested in (the function may or may not be differentiable).
They can also work well in settings where the value function increases or decreases
monotonically, even if the value function is neither convex nor concave, nor even
continuous.
To illustrate, consider the problem of purchasing a commodity. Let
ˆDt = random demand during time interval t,
Rt = resources on hand at time t just before we make an ordering decision,
xt = quantity ordered at time t to be used during time interval t + 1,
Rx
t = resources available just after we make a decision,
ˆpt = market price for selling commodities during time interval t,
ct = purchase cost for commodities purchased at time t.
At time t, we know the price ˆpt and demand ˆDt for time interval t, but we have
to choose how much to order for the next time interval. The transition equations
are given by
Rx
t = Rt + xt,
Rt+1 = [Rx
t −ˆDt+1]+.
The value of being in state Rt is given by
Vt(Rt) = max
xt
E

ˆpt+1 min{Rt + xt, ˆDt+1} −ctxt + V x
t (Rt + xt)

,
(13.1)
where V x
t (Rx
t ) is the post-decision value function, while Vt(Rt) is the traditional
value function around the pre-decision state. Now assume that we introduce a linear
value function approximation
V
x
t (Rx
t ) ≈VtRx
t .
The resulting approximation can be written
˜Vt(Rt) = max
xt
E

ˆpt+1 min{Rt + xt, ˆDt+1} −ctxt + vtRx
t

= max
xt
E

ˆpt+1 min{Rt + xt, ˆDt+1} −ctxt + vt(Rt + xt)

.
(13.2)
We assume that we can compute, or at least approximate, the expectation in equation
(13.2). If this is the case, we may approximate the gradient at iteration n using a
numerical derivative, as in
ˆvt = ˜Vt(Rt + 1) −˜Vt(Rt).
We now use ˆvt to update the value function Vt−1 using
vt ←(1 −α)vt−1 + α ˆvt.

piecewise-linear approximations
501
Normally we would use ˆvt to update Vt−1(Rx
t−1) around the previous post-decision
state variable Rx
t−1. Linear approximations, however, are a special case, since the
slope is the same for all Rx
t−1, which means that it is also the same for Rt−1 =
Rx
t−1 −xt−1.
Linear approximations are useful in two settings. First, the value function may be
approximately linear over the range that we are interested in. Imagine, for example,
that you are trying to decide how many shares of stock you want to sell, where the
range is between 0 and 1000. As an individual investor, it is unlikely that selling
all 1000 shares will change the market price. However, if you are a large mutual
fund and you are trying to decide how many of your 50 million shares you want
to sell, it is likely that such a high volume would move the market price. When
this happens, we need a nonlinear function.
A second use of linear approximations arises when managing resources such
as people and complex equipment such as locomotives or aircraft. Let r be the
attributes of a resource and Rtr be the number of resources with attribute r at time
t. Then it is likely that Rtr will be 0 or 1, implying that a linear function is all we
need. For these problems a linear value function is particularly convenient because
it means that we need one parameter, vtr, for each attribute r.
13.3
PIECEWISE-LINEAR APPROXIMATIONS
There are many problems where we have to estimate the value of having a quantity
R of some resource (where R is a scalar). We might want to know the value of
having R dollars in a budget, R pieces of equipment, or R units of some inventory.
R may be discrete or continuous, but we are going to focus on problems where R
is either discrete or is easily discretized.
Suppose now that we want to estimate a function V (R) that gives the value
of having R resources. There are applications where V (R) increases or decreases
monotonically in R. There are other applications where V (R) is piecewise linear,
concave (or convex) in R, which means that the slopes of the function are monoton-
ically decreasing (if the function is concave) or increasing (if it is convex). When
the function (or the slopes of the function) is steadily increasing or decreasing, we
say that the function is monotone. If the function is increasing in the state variable,
we say that it is “monotonically increasing,” or that it is isotone (although the latter
term is not widely used). To say that a function is “monotone” can mean that it is
monotonically increasing or decreasing.
Say we have a function that is monotonically decreasing, which means that
while we do not know the value function exactly, we know that V (R + 1) ≤V (R)
(for scalar R). If our function is piecewise linear concave, then we will assume that
V (R) refers to the slope at R (more precisely, to the right of R). Assume that our
current approximation V
n−1(R) satisﬁes this property, and that at iteration n, we
have a sample observation of V (R) for R = Rn. If our function is piecewise linear
concave, then ˆvn would be a sample realization of a derivative of the function. If

502
value function approximations
we use our standard updating algorithm, we would write
V
n(Rn) = (1 −αn−1)V
n−1(Rn) + αn−1 ˆvn.
After the update, it is quite possible that our updated approximation no longer
satisﬁes our monotonicity property. In this section we review three strategies for
maintaining monotonicity:
The leveling algorithm. A simple method that imposes monotonicity by simply
forcing elements of the series that violate monotonicity to a larger or smaller
value so that monotonicity is restored.
The SPAR algorithm.
SPAR takes the points that violate monotonicity and
simply averages them.
The CAVE algorithm.
If there is a monotonicity violation after an update,
CAVE simply expands the range of the function over which the update is
applied.
The leveling algorithm and the SPAR algorithm enjoy convergence proofs, but the
CAVE algorithm is the one that works the best in practice. We report on all three
to provide a sense of algorithmic choices, and an enterprising reader may design a
modiﬁcation of one of the ﬁrst two algorithms to incorporate the features of CAVE
that make it work so well.
The Leveling Algorithm
The leveling algorithm uses a simple updating logic that can be written as follows:
V
n(y) =



(1 −αn−1)V
n−1(Rn) + αn−1 ˆvn
if y = Rn,
V
n(y) ∨
)
(1 −αn−1)V
n−1(Rn) + αn−1 ˆvn*
if y > Rn,
V
n(y) ∧
)
(1 −αn−1)V
n−1(Rn) + αn−1 ˆvn*
if y < Rn,
(13.3)
where x ∧y = max{x, y}, and x ∨y = min{x, y}. Equation (13.3) starts by updat-
ing the slope V
n(y) for y = Rn. We then want to make sure that the slopes are
declining. So, if we ﬁnd a slope to the right that is larger, then we simply bring
it down to our estimated slope for y = Rn. Similarly, if there is a slope to the
left that is smaller, then we simply raise it to the slope for y = Rn. The steps are
illustrated in Figure 13.1.
The leveling algorithm is easy to visualize, but it is unlikely to be the best
way to maintain monotonicity. For example, we may update a value at y = Rn for
which there are very few observations. But because it produces an unusually high
or low estimate, we ﬁnd ourselves simply forcing other slopes higher or lower just
to maintain monotonicity.

piecewise-linear approximations
503
v0
v1
v2
u0
u1
u2
k
it
R
n
it
v
(a)
(b)
(c)
1
ˆ
ˆ
(1
)
n
n
n
it
it
it
v
v
v
v0
v1
v2
u0
u1
u2
k
it
R
n
it
v
v0
v1
v2
u0
u1
u2
k
it
R
n
it
v
1
ˆ
ˆ
(1
)
n
n
n
it
it
it
v
v
v
a
a
a
a
Figure 13.1
Steps of the leveling algorithm. (a) The initial monotone function, with the observed R
and observed value of the function ˆv; (b) the function after updating the single segment, producing a
nonmonotone function; (c) the function after monotonicity restored by leveling the function.

504
value function approximations
The SPAR Algorithm
A more elegant strategy is the SPAR (separable projective approximation rou-
tine), which works as follows. Say that we start with our original set of values
(V
n−1(y))y≥0, and that we sample y = Rn and obtain an estimate of the slope ˆvn.
After the update we obtain the set of values (which we store temporarily in the
function yn(y)):
zn(y) =

(1 −αn−1)V
n−1(y) + αn−1 ˆvn,
y = Rn,
V
n−1(y)
otherwise.
(13.4)
If zn(y) ≥zn(y + 1) for all y, then we are in good shape. If not, then either
zn(Rn) < zn(Rn + 1) or zn(Rn −1) < zn(Rn). We can ﬁx the problem by solving
the projection problem
min
v ∥v −zn∥2
(13.5)
subject to
v(z + 1) −v(z) ≤0.
(13.6)
Solving this projection is especially easy. Imagine that after our update, we have
a violation to the left. The projection is achieved by averaging the updated cell
with all the cells to the left that create a monotonicity violation. This means that
we want to ﬁnd the largest i ≤Rn such that
zn(i −1) ≥
1
Rn −i + 1
Rn

y=i
zn(y).
In other words, we can start by averaging the values for Rn and Rn −1 and
checking to see if we now have a concave function. If not, we keep lowering the
left end of the range until we either restore monotonicity or reach y = 0. If our
monotonicity violation is to the right, then we repeat the process to the right.
The steps of the algorithm are given in Figure 13.2, with an illustration given
in Figure 13.3. We start with a monotone set of values (Figure 13.3a), then update
one of the values to produce a monotonicity violation (Figure 13.3b), and ﬁnally
average the violating values together to restore monotonicity (Figure 13.3c).
There are a number of variations of these algorithms that help with convergence.
For example, in the SPAR algorithm we can solve a weighted projection that gives
more weight to slopes that have received more observations. To do this, we weight
each value of y(r) by the number of observations that segment r has received when
computing yn(i −1).
The CAVE Algorithm
A particularly useful variation is to perform an initial update (when we compute
y) over a wider interval than just y = Rn. Say we are given a parameter δ0 that

solving a resource allocation problem
505
Step 0. Initialize V
0 and set n = 1.
Step 1. Sample Rn.
Step 2. Observe a sample of the value function ˆvn.
Step 3. Calculate the vector zn as follows
zn(y) =

(1 −αn−1)V n−1
Rn
+ αn−1 ˆvn
if y = Rn,
vn−1(y)
otherwise.
Step 4. Project the updated estimate onto the space of monotone functions,
vn = (zn),
by solving (13.5)–(13.6). Increase n by one and go to step 1.
Figure 13.2
Learning form of the separable projective approximation routine (SPAR).
has been chosen so that it is approximately 20 to 50 percent of the maximum value
that Rn might take. Now compute z(y) using
zn(y) =

(1 −αn−1)V
n−1(y) + αn−1 ˆvn,
Rn −δn ≤y ≤Rn + δn,
V
n−1(y)
otherwise.
Here we are using ˆvn to update a wider range of the interval. We then apply the
same logic for maintaining monotonicity (concavity if these are slopes). We start
with the interval Rn ± δ0, but we have to periodically reduce δ0. We might, for
example, track the objective function (call it F n), and update the range using
δn =

δn−1
if F n ≥F n−1 −ϵ,
max{1, 0.5δn−1}
otherwise.
While the rules for reducing δn are generally ad hoc, we have found that this is
critical for fast convergence. The key is that we have to pick δ0 so that it plays a
critical scaling role, since it has to be set so that it is roughly on the order of the
maximum value that Rn can take. If SPAR or the leveling algorithm are going to
be successful, then these will have to be adapted to solve these scaling problems.
13.4
SOLVING A RESOURCE ALLOCATION PROBLEM USING
PIECEWISE-LINEAR FUNCTIONS
Scalar piecewise-linear functions have proved to be an exceptionally powerful
way of solving a large number of stochastic resource allocation problems. We can
describe the algorithm with a minimum of technical details using what is known
as a “plant–warehouse–customer” model, which is a form of multidimensional
newsvendor problem. Imagine that we have the problem depicted in Figure 13.4a.

506
value function approximations
(a)
(b)
(c)
v0
v1
v2
u0
u1
u2
k
it
R
n
it
v
v0
v1
v2
u0
u1
u2
k
it
R
1
ˆ
ˆ
(1
)
n
n
n
it
it
it
v
v
v
ˆ
n
it
v
v0
v1
v2
u0
u1
u2
k
it
R
v
Figure 13.3
Steps of the SPAR algorithm. (a) The initial monotone function, with the observed R
and observed value of the function ˆv; (b) the function after updating the single segment, producing a
nonmonotone function; (c) the function after the projection operation.

solving a resource allocation problem
507
(a)
(c)
Random demands
and prices
Random costs
ˆD1
ˆD2
ˆD3
ˆD4
ˆD5
1
n
R
2
n
R
3
n
R
4
n
R
(b)
Sample realizations
of demands and prices
Sample realizations
of costs
ˆD1(w)
ˆD2(w)
ˆD3(w)
ˆD4(w)
ˆD5(w)
x,n
R1
x,n
R2
x,n
R3
x,n
R4
x,n
R5
V1(  )
w
w
w
w
w
n
V2(  )
n
V3(  )
n
V4(  )
n
V5(  )
n
Duals:
Figure 13.4
Steps in estimating separable, piecewise-linear approximations for two-stage stochastic
programs. (a) The two-stage problem with stochastic second-stage data. (b) Solving the ﬁrst stage using
a separable, piecewise-linear approximation of the second stage. (c) Solving a Monte Carlo realization
of the second stage and obtaining dual variables.

508
value function approximations
We start by shipping “product” out of the four “plant” nodes on the left, and we
have to decide how much to send to each of the ﬁve “warehouse” nodes in the
middle. After making this decision, we observe the demands at the ﬁve “customer”
nodes on the right.
We can solve this problem using separable, piecewise-linear value function
approximations. Say we have an initial estimate of a piecewise-linear value func-
tion for resources at the warehouses (setting these equal to zero is ﬁne). This gives
us the network shown in Figure 13.4b, which is a small linear program (even when
we have hundreds of plant and warehouse nodes). Solving this problem gives us a
solution of how much to send to each node.
We then use the solution to the ﬁrst stage (which gives us the resources available
at each warehouse node), take a Monte Carlo sample of each of the demands, and
solve a second linear program that sends product from each warehouse to each
customer. What we want from this stage is the dual variable for each warehouse
node, which gives us an estimate of the marginal value of resources at each node.
Note that some care needs to be used here because these dual variables are not
actually estimates of the value of one more resources, rather they are subgradients,
which means that they may be the value of the last resource or the next resource,
or something in between.
Finally, we use these dual variables to update the piecewise linear value func-
tions using the methods described above. This process is repeated until the solution
no longer seems to be improving.
Although we have described this algorithm in the context of a two-stage problem,
the same basic strategy can be applied for problems with many time periods.
Using approximate value iteration (TD(0)), we would step forward in time, and
after solving each linear program, we would stop and use the duals to update
the value functions from the previous time period (more speciﬁcally, around the
previous post-decision state). For a ﬁnite horizon problem we would proceed until
the last time period, then repeat the entire process until the solution seems to be
converging.
With more work, we can implement a backward pass (TD(1)) by avoiding any
value function updates until we reach the ﬁnal time period, but we would have to
retain information about the effect of incrementing the resources at each node by
one unit (this is best done with a numerical derivative). We would then need to step
back in time, computing the marginal value of one more resource at time t using
information about the value of one more resource at time t + 1. These marginal
values would be used to update the value function approximations.
This algorithmic strategy has some nice features:
• This is a very general model with applications that span equipment, people,
product, money, energy, and vaccines. It is ideally suited for “single-layer”
resource allocation problems (one type of resource, rather than pairs such as
pilots and aircraft, locomotives and trains, or doctors and patients), although
many two-layer problems can be reasonably approximated as single-layer
problems.

the shape algorithm
509
• The methodology scales to very large problems, with hundreds or thousands
of nodes, and tens of thousands of dimensions in the decision vector.
• We do not need to solve the exploration–exploitation problem. A pure
exploitation strategy works ﬁne. The reason has to do with the concavity
of the value function approximations, which has the effect of pushing
suboptimal value functions toward the correct solution.
• Piecewise linear value function approximations are quite robust, and avoid
making any simplifying assumptions about the shapes of the value functions.
Chapter 14 provides more details on using these ideas for resource allocation prob-
lems, and closes with some descriptions of large, industrial applications. This brief
sketch of an algorithm provides a hint of the power of separable piecewise-linear
approximations in the contextual domain of resource allocation problems.
13.5
THE SHAPE ALGORITHM
A particularly simple algorithm for approximating continuous value functions starts
with an initial approximation and then “tilts” this function to improve the approxi-
mation. The concept is most effective if it is possible to build an initial approxima-
tion, perhaps using some simpliﬁcations, that produces a “pretty good” solution.
13.5.1
The Basic Idea
The idea works as follows. Suppose that we are trying to solve the stochastic
optimization problem we ﬁrst saw in Section 7.2, which is given by
min
x∈X EF(x, W),
(13.7)
where x is our decision variable and W is a random variable. We already know that
we can solve this problem using a stochastic gradient algorithm that takes the form
xn = xn−1 −αn−1∇xF(xn−1, W(ωn)).
(13.8)
The challenge we encounter with this algorithm is that the units of the decision
variable and the gradient may be different, which means that our stepsize has to be
properly scaled. Also, while these algorithms may work in the limit, convergence
can be slow and choppy.
Now assume that we have a rough approximation of EF(x, W) as a function of
x, which we callF
0(x). For example, if x is a scalar and we think that the optimal
solution is approximately equal to a, we might letF
0(x) = (x −a)2. The SHAPE
algorithm iteratively adjusts this initial approximation by ﬁrst ﬁnding a stochastic
gradient ∇xF(xn−1, W(ωn)), and then computing an updated approximation using
F
n(x) =F
n−1(x) + αn−1

∇xF(xn−1, W(ωn)) −∇xF
n−1(xn−1)




I
x.

510
value function approximations
Our updated approximation is the original approximation plus a linear correction
term. Note that x is a variable while the term I is a constant. Since the correction
term has the same units as the approximation, we can assume that 0 < αn ≤1. The
linear correction term is simply the difference between the slope of the original
function at a sample realization W(ωn) and the exact slope of the approximation
at xn−1. Typically we would pick an approximation that is easy to differentiate.
The SHAPE algorithm is depicted graphically in Figure 13.5. Figure 13.5a
shows the initial true function and the approximation F
0(x). Figure 13.5b shows
the gradient of each function at the point xn−1, with the difference between the
two slopes. If there were no noise, and if our approximation were exact, these
two gradients would match (at least at this point). Finally, Figure 13.5c shows the
updated approximation after the correction has been added in. At this point the two
slopes will match only if the stepsize αn−1 = 1.
We can illustrate the SHAPE algorithm using a simple numerical example. Sup-
pose that our problem is to solve
max
x≥0 EF(x, W) = E
' 1
2 ln (x + W) −2(x + W)
(
,
where W represents random measurement error, which is normally distributed with
mean 0 and variance 4. Assume that we start with a convex approximation such as
F
0(x) = 6√x −2x.
We begin by obtaining the initial solution x0
x0 = arg max
x≥0

6√x −2x

.
Note that our solution to the approximate problem may be unbounded, requiring
us to impose artiﬁcial limits. Since our approximation is concave, we can set the
derivative equal to zero to ﬁnd
∇F
0(x) =
3
√x −2
= 0,
which gives us x0 = 2.25. Since x0 ≥0, it is optimal. To ﬁnd the stochastic gradi-
ent, we have to sample the random variable W . Assume that W(ω1) = 1.75. Our
stochastic gradient is then
∇F(x, W(ω1)) =
1
2(x0 + W(ω1)) −2
=
1
2(2.25 + 1.75))
= 0.1250.

the shape algorithm
511
(a) True function and initial approximation.
The exact gradient of
the approximate function
1
1
(
)
n
n
1
(
,
)
F
x n
n
The approximate gradient
of the exact function
1
n
x
Exact function
Approximate
function
1(x)
n
F
F
x
1
(
,
)
F
x n
n
1
1
(
)
n
n
F
x
1
(
,
)
F
x n
n
1
1
(
)
n
n
F
x
(b) Difference between stochastic gradient of true function
     and actual gradient of approximation.
n
X
1
(
,
)
F
x n
n
1
(
(
))
n
1
n
F
x
(x)
1
n
F
x
1
n
(c) Updated approximation.
w
w
w
w
a
Figure 13.5
Illustration of the steps of the SHAPE algorithm.

512
value function approximations
Thus, while we have found the optimal solution to the approximate problem (which
produces a zero slope), our estimate of the slope of the true function is positive,
so we updateF
1(s) with the adjustment
F
1(x) = 6√x −2x −α0(0.1250 −0)x
= 6√x −3.125x,
where we have used α0 = 1.
13.5.2
A Dynamic Programming Illustration
Consider what happens when we have to solve a dynamic program (approximately).
For this illustration, assume that Rt and xt are scalars, although everything we are
going to do works ﬁne with vectors. Still using the value function around the post-
decision state, we would start with the previous post-decision state Rx
t−1. We then
use our sample path ωn to ﬁnd the pre-decision state Rn
t = RM,W(Rx
t−1, Wt(ωn))
and solve
˜V n
t (Rn
t ) = max
xt∈Xnt

C(Rn
t , xt) +V
n−1
t
(RM,x(Rn
t , xt))

.
We need the derivative of ˜V n
t (Rn
t ). There are different ways of getting this
depending on the structure of the optimization problem, but for the moment we
will simply let
ˆvn
t = ˜V n
t (Rn
t + 1) −˜V n
t (Rn
t ).
It is important to remember that embedded in the calculation of ˜V n
t (Rn
t + 1) is the
need to reoptimize xt. Recall that we use ˆvn
t , which is computed at Rt = Rn
t , to
update Vt−1(Rx
t−1) as a function of the post-decision state Rx
t−1. Using the SHAPE
algorithm, we update our approximation by way of
V
n
t−1(Rx
t−1) = V
n−1
t−1 (Rx
t−1) + αn−1

ˆvn
t −∇RV
n−1
t−1 (Rx
t−1)

Rx
t−1.
The steps of the SHAPE algorithm are given in Figure 13.6.
It is useful to consider how decisions are made using the SHAPE algorithm
versus a stochastic gradient procedure. Using SHAPE, we would choose xt by
way of
xn
t = arg max
xt∈Xnt

C(Rn
t , xt) +V
n−1
t
(RM,x(Rn
t , xt))

.
(13.9)
Contrast determining the decision xn
t from (13.9) to equation (13.8), which depends
heavily on the choice of stepsize. This property is true of most algorithms that
involve ﬁnding nonlinear approximations of the value function.

regression methods
513
Step 0. Initialization.
Step 0a. Initialize V
0
t , ˜˜t ∈T.
Step 0b. Set n = 1.
Step 0c. Initialize R1
0.
Step 1. Choose a sample path ωn.
Step 2. Do for t = 0, 1, 2, . . . , T :
Step 2a. Solve
˜V n
t (Rn
t ) = max
xt ∈Xnt

C(Rn
t , xt) +V
n−1
t
(RM,x(Rn
t , xt))

,
and let xn
t be the value of xt that solves the maximization problem.
Step 2b. Compute the derivative
ˆvn
t = ˜V n
t (Rn
t + 1) −˜V n
t (Rn
t ).
Step 2c. If t > 0, update the value function
V
n
t−1(Rx
t−1) = V
n−1
t−1 (Rx
t−1) + αn−1

ˆvn
t −∇RV
n−1
t−1 (Rx
t−1)

Rx
t−1.
Step 2d. Update the states
Rx,n
t
= RM,x(Rn
t , xn
t ),
Rn
t+1 = RM,W(Rx,n
t
, Wt+1(ωn)).
Step 3. Increment n. If n ≤N, go to step 1.
Step 4. Return the value functions (V
N
t )T
t=1.
Figure 13.6
SHAPE algorithm for approximate dynamic programming.
13.6
REGRESSION METHODS
As in Chapter 8 we can create regression models where the basis functions are
manipulations of the number of resources of each type. For example, we might use
V(R) = θ0 +

a∈A
θ1aRa +

a∈A
θ2aR2
a,
(13.10)
where θ = (θ0, (θ1r)r∈R, (θ2r)r∈R) is a vector of parameters that are to be
determined. The choice of explanatory terms in our approximation will generally
reﬂect an understanding of the properties of our problem. For example, equation
(13.10) assumes that we can use a mixture of linear and separable quadratic terms.
A more general representation is to assume that we have developed a family F
of basis functions (φf (R))f ∈F. Examples of a basis function are
φf (R) = R2
rf ,

514
value function approximations
φf (R) =


r∈Rf
Rr


2
for some subset Rf ,
φf (R) = (Rr1 −Rr2)2,
φf (R) = |Rr1 −Rr2|.
A common strategy is to capture the number of resources at some level of
aggregation. For example, if we are purchasing emergency equipment, we may
care about how many pieces we have in each region of the country, and we may
also care about how many pieces of a type of equipment we have (regardless of
location). These issues can be captured using a family of aggregation functions
Gf , f ∈F, where Gf (r) aggregates an attribute vector r into a space R(f ) where
for every basis function f there is an element rf ∈R(f ). Our basis function might
then be expressed using
φf (R) =

r∈R
1{Gf (r)=rf }Rr.
As we originally introduced in Section 8.2.2, the explanatory variables used in
the examples above, which are generally referred to as independent variables in
the regression literature, are typically referred to as basis functions by the approxi-
mate dynamic programming community. A basis function can be linear, nonlinear
separable, nonlinear nonseparable, and even nondifferentiable, although the nondif-
ferentiable case will introduce additional technical issues. The challenge, of course,
is that it is the responsibility of the modeler to devise these functions for each appli-
cation. We have written our basis functions purely in terms of the resource vector,
but it is possible for them to be written in terms of other parameters in a more
complex state vector, such as asset prices.
Given a set of basis functions, we can write our value function approximation
as
V(R|θ) =

f ∈F
θf φf (R).
(13.11)
It is important to keep in mind that V(R|θ) (or more generally, V(S|θ)), is any
functional form that approximates the value function as a function of the state
vector parameterized by θ. Equation (13.11) is a classic linear-in-the-parameters
function. We are not constrained to this form, but it is the simplest and offers some
algorithmic shortcuts.
The issues that we encounter in formulating and estimating V(R|θ) are the same
as those that any student of statistical regression would face when modeling a
complex problem. The major difference is that our data arrive over time (iterations),
and we have to update our formulas recursively. Also it is typically the case that
our observations are nonstationary. This is particularly true when an update of a
value function depends on an approximation of the value function in the future

regression methods
515
(as occurs with value iteration or any of the TD(λ) classes of algorithms). When
we are estimating parameters from nonstationary data, we do not want to equally
weight all observations.
The problem of ﬁnding θ can be posed in terms of solving the following stochas-
tic optimization problem:
min
θ
E 1
2(V(R|θ) −ˆV )2.
We can solve this using a stochastic gradient algorithm, which produces updates
of the form
θ
n = θ
n−1 −αn−1(V(Rn|θ
n−1) −ˆV (ωn))∇θV(Rn|θn)
= θ
n−1 −αn−1(V(Rn|θ
n−1) −ˆV (ωn))


φ1(Rn)
φ2(Rn)
...
φF (Rn)

.
If our value function is linear in Rt, we would write
V(R|θ) =

r∈R
θrRr.
In this case our number of parameters has shrunk from the number of possible
realizations of the entire vector Rt to the size of the attribute space (which, for
some problems, can still be large, but nowhere near as large as the original state
space). For this problem φ(Rn) = Rn.
It is not necessarily the case that we will always want to use a linear-in-the-
parameters model. We may consider a model where the value increases with the
number of resources, but at a declining rate that we do not know. Such a model
could be captured with the representation
V(R|θ) =

r∈R
θ1rRθ2r
r ,
where we expect θ2 < 1 to produce a concave function. Now our updating formula
will look like
θn
1 = θn−1
1
−αn−1(V(Rn|θ
n−1) −ˆV (ωn))(Rn)θ2,
θn
2 = θn−1
2
−αn−1(V(Rn|θ
n−1) −ˆV (ωn))(Rn)θ2 ln Rn,
where we assume the exponentiation operator in (Rn)θ2 is performed component-
wise.
We can put this updating strategy in terms of temporal differencing. As before,
the temporal difference is given by
δτ = Cτ(Rτ, xτ+1) +V
n−1
τ+1(Rτ+1) −V
n−1
τ
(Rτ).

516
value function approximations
The original parameter updating formula (equation (9.7)) when we had one param-
eter per state now becomes
θ
n = θ
n−1
t
+ αn−1
T

τ=t
λτ−tδτ∇θV(Rn|θ
n).
It is important to understand that in contrast with most of our other applications
of stochastic gradients, updating the parameter vector using gradients of the objec-
tive function requires mixing the units of θ with the units of the value function. In
these applications the stepsize αn−1 has to also perform a scaling role.
13.7
CUTTING PLANES*
Cutting planes represent a powerful strategy for representing concave (or convex if
we are minimizing), piecewise-linear functions for multidimensional problems. This
method evolved originally not as a method for approximating dynamic programs,
but instead as a technique for solving linear programs in the presence of uncertainty.
In the 1950s the research community recognized that many optimization problems
involve different forms of uncertainty, with the most common being the challenge
of allocating resources now to serve demands in the future that have not yet been
realized. For this reason a subcommunity within math programming, known as
the stochastic programming community, has developed a rich theory and some
powerful algorithmic strategies for handling uncertainty within linear programs
and, more recently, integer programs.
Historically, dynamic programming has been viewed as a technique for small,
discrete optimization problems, whereas stochastic programming has been the ﬁeld
that handles uncertainty within math programs (which are typically characterized
by high-dimensional decision vectors and large numbers of constraints). The con-
nections between stochastic programming and dynamic programming, historically
viewed as diametrically competing frameworks, have been largely overlooked. This
section is designed to bridge the gap between stochastic programming and approxi-
mate dynamic programming. Our presentation is facilitated by notational decisions
(in particular, the use of x as our decision vector) that we made at the beginning
of the book.
The material in this section is somewhat more advanced and requires a fairly
strong working knowledge of linear programming and duality theory. Our pre-
sentation is designed primarily to establish the relationship between the ﬁelds of
approximate dynamic programming and stochastic programming.
We begin our presentation by introducing a classical model in stochastic pro-
gramming known as the two-stage resource allocation problem.
13.7.1
A Two-Stage Resource Allocation Problem
We use the same notation we have been using for resource allocation problems,
where
Rtr = number of resources with attribute r ∈R in the system at time t.

cutting planes
517
Rt = (Rtr)r∈R.
Dtb = number of demands of type b ∈B in the system at time t.
Dt = (Dtb)b∈B.
R0 and D0 capture the initial state of our system. Decisions are represented using
DD = decision to satisfy a demand with attribute b(each decision d ∈DD
corresponds to a demand attribute bd ∈B).
DM = decision to modify a resource (each decision d ∈DM
has the effect of modifying the attributes of the resource where DM
includes the decision to “do nothing”).
D = DD ∪DM.
xtrd = number of resources that initially have attribute r that we act on
with decision d.
xt = (xtrd)r∈R,d∈D.
x0 is the vector of decisions we have to make now given S0 = (R0, D0). The
post-decision state is given by
Rx
0r′ = number of resources with attribute r′ available in the second stage
after we have made our original decision x0
=

r∈R

d∈D
δr′(r, d)x0rd,
Rx
0 = x0,
where  is a matrix with δr′(r, d) in row r′, column (r, d). Before we solve the
second-stage problem (at time t = 1), we observe random changes to the resource
vector as well as random new demands. For our illustration, we assume that unsat-
isﬁed demands from the ﬁrst stage are lost. This means that
R1 = Rx
0 + ˆR1,
D1 = ˆD1.
The resource vector R1 can be used to satisfy demands that ﬁrst become known
during time interval 1. In the second stage we need to choose x1 to minimize the
cost function C1(x1). At this point we assume that our problem is ﬁnished. Thus
the problem over both stages would be written
max
x0,x1

C0(x0) + EC1(x1)

.
(13.12)

518
value function approximations
This problem has to be solved subject to the ﬁrst-stage constraints:

d∈D
x0rd = R0r,
(13.13)

r∈R
x0rd ≤ˆD0bd ,
d ∈DD,
(13.14)
x0rd ≥0,
(13.15)

r∈R

d∈D
δr′(r, d)x0rd −Rx
0r′ = 0.
(13.16)
The second-stage decisions have to be made subject to constraints for each outcome
ω ∈, given by

d∈D
x1rd(ω) = R1r(ω) = Rx
0r + ˆR1r(ω),
(13.17)

r∈R
x1rd(ω) ≤ˆD1bd (ω),
d ∈DD,
(13.18)
x1rd(ω) ≥0.
(13.19)
Constraint (13.13) limits our decisions by the resources that are initially available.
We may use these resources to satisfy initial demands, contained in ˆD0, where
ﬂows are limited by constraint (13.14). Constraint (13.15) enforces nonnegativity,
while constraint (13.16) deﬁnes the post-decision resource vector Rx
0.
The constraints for the second stage are similar to those for the ﬁrst stage, except
that there is a set of constraints for every outcome ω. Note that equation (13.12)
is written as if we are using x1 as the state variable for the second stage. Again,
this is the standard notational style of stochastic programming.
Equations (13.12) through (13.19) describe a fairly general model. It is useful
to see the model formulated at a detailed level, but for what we are about to do, it
is convenient to express it in matrix form.
First-stage constraints:
A0x0 = R0,
(13.20)
B0x0 ≤ˆD0,
(13.21)
x0 ≥0,
(13.22)
x0 −Rx
0 = 0.
(13.23)
Second-stage constraints:
A1x1(ω) = R1(ω)
∀ω ∈,
(13.24)
B1x1(ω) ≤ˆD1(ω)
∀ω ∈,
(13.25)
x1(ω) ≥0
∀ω ∈.
(13.26)

cutting planes
519
We note that in the classical language of math programming, Rx
0 is a decision
variable deﬁned by equation (13.23). In dynamic programming, we view Rx
0 =
RM,x(R0, x0) as a function that depends on R0 and x0.
The second-stage contribution function depends on the ﬁrst-stage allocation,
Rx
0(x0), so we can write it as
V0(Rx
0) = EC1(x1),
which allows us to rewrite (13.12) as
max
x0

C0(x0) + V0(Rx
0)

.
(13.27)
This shows that our two-stage problem consists of a one-period contribution func-
tion (using the information we know now) and a value function that captures the
expected contribution, which depends on the decisions that we made in the ﬁrst
stage.
We pause to note a major stylistic departure between stochastic and dynamic
programming. In writing equation (13.27), it is clear that we have written our
expected value function in terms of the resource state variable Rx
0. Of course, Rx
0
is a function of x0, which means that we could write (13.27) as
max
x0

C0(x0) + V0(x0)

.
(13.28)
This is the style favored by the stochastic programming community, where V0(x0)
is referred to as the recourse function which is typically denoted Q. Mathemat-
ically V0(x0) and V0(Rx
0) are equivalent, but computationally they can be quite
different. If |R| is the size of our resource attribute space and |D| is the number of
types of decisions, the dimensionality of x0 is typically on the order of |R| × |D|,
whereas the dimensionality of Rx
0 will be on the order of |R|. As a rule, it is
computationally much easier to approximate V0(Rx
0) than V0(x0) simply because
functions of lower dimensional variables are easier to approximate than functions
of higher dimensional ones. In some of our discussions in this chapter, we adopt
the convention of writing V0 as a function of x0 in order to clarify the relationship
with stochastic programming.
As we have seen in this volume, approximating the value function involves
exploiting structure in the state variable. The stochastic programming community
does not use the term “state variable,” but some authors will use the term tenders,
which is to say that the ﬁrst stage “tenders” Rx
0 to the second stage. However,
whereas the ADP community puts tremendous energy into exploiting the structure
of the state variable when developing an approximation, state variables play either
a minor or nonexistent role in stochastic programming.
Figure 13.4 illustrates solving the ﬁrst-stage problem using a particular approx-
imation for the second stage. The stochastic programming community has devel-
oped algorithmic strategies that solve these problems optimally (although the term
“optimal” carries different meanings). The next two sections describe algorithmic
strategies that have been developed within the stochastic programming community.

520
value function approximations
13.7.2
Benders’ Decomposition
Benders’ decomposition is most easily described in the context of our two-stage
resource allocation problem. Initially we have to solve
max
x0

c0x0 + V0(Rx
0)

(13.29)
subject to constraints (13.20) through (13.23). We note in passing that the stochastic
programming community would normally write (13.29) in the form
max
x0

c0x0 + V0(x0)

,
which is mathematically correct (after all, Rx
0 is a function of x0), but we prefer
the form in (13.29) to capture the second-stage dependence on Rx
0.
As before, the value function is given by
V0(Rx
0) =

ω∈
p(ω)V0(Rx
0, ω),
where V0(Rx
0, ω) is given by
V0(Rx
0, ω) = max
x1(ω) c1x1(ω)
(13.30)
subject to constraints (13.24) through (13.26). For ease of reference we write the
second-stage constraints as
Dual
A1x1(ω)
=
x0 + ˆR1(ω),
ˆvR
1 (ω),
B1x1(ω)
≤
ˆD1(ω),
ˆvD
1 (ω),
x1(ω)
≥
0,
where ˆvR(ω) and ˆvD(ω) are the dual variables for the resource constraints and
demand constraints under outcome (scenario) ω. For our discussion we assume that
we have a discrete set of sample outcomes , where the probability of outcome ω
is given by p(ω). The linear programming dual of the second-stage problem takes
the form
z∗(x0, ω) =
min
ˆvR
1 (ω),ˆvD
1 (ω)
(x0 + ˆR1(ω))T ˆvR
1 (ω) + ( ˆD1(ω))T ˆvD
1 (ω)
(13.31)
subject to
AT
1 ˆvR
1 (ω) + BT
1 ˆvD
1 (ω) ≥c1,
(13.32)
ˆvD
1 (ω) ≥0.
(13.33)
The dual is also a linear program, and the optimal ˆv∗
1(ω) = (ˆvR∗
1 (ω), ˆvD∗
1 (ω))
must occur at one of a set of vertices, which we denote by V1. Note that the set

cutting planes
521
of feasible dual solutions V1 is independent of ω, a property that arises because
we have assumed that our only source of randomness is in the right-hand side
constraints (we would lose this if our costs were random).
Since z∗(x0, ω) is the optimal solution of the dual, we have
z∗(x0, ω) = (x0 + ˆR1(ω))T ˆvR∗
1 (ω) + ( ˆD1(ω))T ˆvD∗
1 (ω).
Next deﬁne
z(x0, ˆv1, ω) = (x0 + ˆR1(ω))T ˆvR
1 + ( ˆD1(ω))T ˆvD
1 .
Note that while z(x0, ˆv1, ω) depends on ω, it is written for any set of duals ˆv1 ∈V1
(we do not need to index them by ω). We observe that
z∗(x0, ω) = z(x0, ˆv∗
1(ω), ω)
≤z(x0, ˆv1, ω)
∀ˆv1 ∈V1,
(13.34)
since z∗(x0, ω) is the best we can do. Furthermore equation (13.34) is true for all
ˆv1 ∈V1, and all outcomes ω. We know from the theory of linear programming that
our primal must always be less than or equal to our dual, which means that
V0(x0, ω) ≤z(x0, ˆv1, ω)
∀ˆv1 ∈V1, ω ∈,
= z∗(x0, ω),
∀ω ∈.
This means that for all ˆv1 ∈V1,
V0(x0) ≤

ω∈
p(ω)

(x0 + ˆR1(ω))T ˆvR
1 + ( ˆD1(ω))T ˆvD
1

,
(13.35)
where the inequality (13.35) is tight for ˆv1 = ˆv∗
1(ω). Now let
α1(ˆv1) =

ω∈
p(ω)
 ˆR1(ω)T ˆvR
1 + ˆD1(ω)T ˆvD
1

,
(13.36)
β1(ˆv1) =

ω∈
p(ω)T ˆvR
1 .
(13.37)
For historical reasons we use α as a coefﬁcient rather than a stepsize in our dis-
cussion of Benders’ decomposition. This allows us to write (13.35) in the more
compact form
V0(x0) ≤α1(ˆv1) + (β1(ˆv1))T x0.
(13.38)
The right-hand side of (13.38) is called a cut, since it is a plane (actually, an
n-dimensional hyperplane) that represents an upper bound on the value function.
Using these cuts, we can replace (13.29) with
max
x0

c0x0 + z

(13.39)

522
value function approximations
subject to (13.20) through (13.23) plus
z ≤α1(ˆv1) + (β1(ˆv1))T x0
∀ˆv1 ∈V1.
(13.40)
Unfortunately, equation (13.40) can be computationally problematic. The set V1
may be extremely large, so enforcing this constraint for each vertex ˆv1 ∈V1 is pro-
hibitive. Furthermore, even if  is ﬁnite, it may be quite large, making summations
over the elements in  expensive for some applications.
A simple strategy overcomes the problem of enumerating the set of dual vertices.
Assume that after iteration n −1 we have a set of cuts (αm
1 , βm
1 )n
m=1 that we have
generated from previous iterations. Using these cuts, we solve equation (13.39):
z ≤αm
1 + (βm
1 )T x0,
m = 1, 2, . . . , n −1,
(13.41)
to obtain a ﬁrst-stage solution xn
0. Given xn
0, we then solve the dual (13.31) to obtain
ˆvn
1(ω) (the optimal duals) for each ω ∈. Using this information, we obtain αn
1 and
βn
1 . Thus, instead of having to solve (13.39) subject to the entire set of constraints
(13.40), we use only the cuts in equation (13.41).
This algorithm is known as the “L-shaped” algorithm (more precisely, “L-shaped
decomposition”). For a ﬁnite  it has been proved to converge to the optimal
solution. The problem is the requirement that we calculate ˆv∗
1(ω) for all ω ∈,
which means that we must solve a linear program for each outcome at each iteration.
For most problems this is computationally pretty demanding.
13.7.3
Variations
Two variations of this algorithm that avoid the computational burden of computing
ˆv∗
1(ω) for each ω have been proposed. These algorithms vary only in how the
cuts are computed and updated. The ﬁrst is known as stochastic decomposition.
At iteration n, after solving the ﬁrst-stage problem, we would solve (for sample
realization ωn)

ˆvR,n
1
, ˆvD,n
1

=
arg min
ˆvR
1 (ωn),ˆvD
1 (ωn)
(xn
0 + ˆR1(ωn))T ˆvR
1 (ωn) + ˆD1(ωn)T ˆvD
1 (ωn)
(13.42)
for a single outcome ωn. We then update all previous cuts by ﬁrst computing for
m = 1, 2, . . . , n
(νR,n
m , νD,n
m ) = arg min
ˆvR
1 ,ˆvD
1

(xn
0 + ˆR1(ωm))T ˆvR
1 + ˆDT
1 (ωm)ˆvD
1
... (ˆvR
1 , ˆvD
1 ) ∈Vn
.
We then compute the next cut using
αn
n = 1
n
n

m=1
 ˆR1(ωm)T νR,n
m
+ ˆD1(ωm)T νD,n
m

,
(13.43)
βn
n = 1
n
n

m=1
T νR,n
m .
(13.44)

cutting planes
523
Then all the previous cuts are updated using
αn
m = n −1
n
αn−1
m
,
βn
m = n −1
n
βn−1
m
,
m = 1, . . . , n −1.
(13.45)
The complete algorithm is given in Figure 13.7. The beauty of stochastic decom-
position is that we never have to loop over all outcomes, and we solve a single
linear program for the second stage at each iteration. There is a requirement that
we loop over all the cuts that we have previously generated (in equation (13.45)),
but the calculation here is fairly trivial (contrast the need to solve a complete linear
program for every outcome in the L-shaped algorithm).
A second variation, called the CUPPS algorithm (for “cutting plane and partial
sampling” algorithm), ﬁnds new dual extreme points using
αn
n =
1
||

ω∈
 ˆR1(ω)T νR,n(ω) + ˆD1(ωm)T νD,n(ω)

,
(13.46)
Step 0. Set V0 = φ, n = 1
Step 1. Solve the following master problem:
xn
0 = arg max
x0∈X0

c0x0 + z

,
where X0 = {A0x0 = R0, B0x0 ≤ˆD0, z ≤αn
m + βn
mx0, m = 1, . . . , n −1, x0 ≥0}.
Step 2. Sample ωn ∈ and solve the second-stage problem,
max{c1x1 : A1x1 = xn
0 + ˆR1(ωn), B1x1 ≤ˆD1(ωn), x1 ≥0},
to obtain the optimal dual solution from equation (13.42) and store the dual vertex
Vn ←Vn−1 ∪(ˆvR,n
1
, ˆvD,n
1
).
Step 3. Update the cuts:
Step 3a. Compute for m = 1, . . . , n:
(νR,n
m , νD,n
m ) = arg min
ˆvR
1 ,ˆvD
1

(xn
0 + ˆR1(ωm))T ˆvR
1 + ˆDT
1 (ωm)ˆvD
1
... (ˆvR
1 , ˆvD
1 ) ∈Vn
.
Step 3b. Construct the coefﬁcients αn
n and βn
n of the nth cut to be added to the master
problem using (13.43) and (13.44).
Step 3c. Update the previously generated cuts by
αn
m = n −1
n
αn−1
m
,
βn
m = n −1
n
βn−1
m
,
m = 1, . . . , n −1.
Step 4. Check for convergence; if not, set n = n + 1 and return to step 1.
Figure 13.7
Sketch of the stochastic decomposition algorithm.

524
value function approximations
βn
n =
1
||

ω∈
T νR,n(ω),
(13.47)
where for each ω ∈, we compute the duals
(νR,n, νD,n)(ω) = arg min
ˆvR
1 ,ˆvD
1

(xn
0 + ˆR1(ωm))T ˆvR
1 + ˆDT
1 (ωm)ˆvD
1
... (ˆvR
1 , ˆvD
1 ) ∈Vn
.(13.48)
Equations (13.46) through (13.48) represent the primary computational burden
of CUPPS (outside of solving the linear program for each time period in each
iteration). These have to be computed for each sample realization in . Stochas-
tic decomposition, by contrast, requires that we update all previously generated
cuts, a step that CUPPS does not share. The steps of this algorithm are given in
Figure 13.8.
L-shaped decomposition, stochastic decomposition, and the CUPPS algorithm
offer three contrasting strategies for generating cuts, which are illustrated in
Figure 13.9. L-shaped decomposition is computationally the most demanding,
but it produces tight cuts and will, in general, produce the fastest convergence
(measured in terms of the number of iterations). The cuts produced by stochastic
decomposition are neither tight nor valid, but steadily converge to the correct
function. The cuts generated by CUPPS are valid but not tight.
Stochastic decomposition and CUPPS require roughly the same amount of work
as any of the other approximation strategies we have presented for approximate
dynamic programming, but these strategies offer a proof of convergence. Notice
Step 0. Set V 0 = φ, n = 1.
Step 1. Solve the following master problem:
xn
0 = arg max
x0∈X0

c0x0 + z

,
where X0 = {A0x0 = R0, B0x0 ≤ˆD0, z ≤αm + βmx0, m = 1, . . . , n −1, x0 ≥0}.
Step 2. Sample ωn ∈ and solve the second-stage problem,
max{c1x1 : A1x1 = xn
0 + ˆR1(ωn), B1x1 ≤ˆD1(ωn), x1 ≥0},
to obtain the optimal dual solution from equation (13.42) and store the dual vertex
V n ←V n−1 ∪(ˆvR,n
1
, ˆvD,n
1
).
Step 3. Construct the coefﬁcients of the nth cut to be added to the master problem using
(13.46) and (13.47).
Step 4. Check for convergence; if not, set n = n+1 and return to step 1.
Figure 13.8
Sketch of the CUPPS algorithm.

cutting planes
525
V0
x0
(a) Cuts generated by L-shaped decomposition.
V0
x0
(b) Cuts generated by the stochastic decomposition algorithm.
V0
x0
(c) Cuts generated by the CUPPS algorithm.
Figure 13.9
Cuts generated by (a) L-shaped decomposition, (b) stochastic decomposition, and (c)
CUPPS.

526
value function approximations
also that it does not require exploration, but it does require that we generate a dual
variable for every second-stage constraint (which requires that our attribute space
A be small enough to enumerate). The real question is rate of convergence. The
next section provides some experimental work that hints at answers to this question,
although the real answer depends on the characteristics of individual problems.
13.7.4
Experimental Comparisons
For resource allocation problems it is important to evaluate approximations two
ways. The ﬁrst is to consider “two-stage” problems (a terminology from stochastic
programming) where we make a decision, see a random outcome, make one more
decision, and then stop. For resource allocation problems the ﬁrst decision consists
of meeting known demands and then acting on resources so that they are in position
to satisfy demands that will become known in the second stage (at time t = 1). Two-
stage problems are important because we solve multiperiod (multistage) problems
as sequences of two-stage problems.
Powell et al. (2004) reports on an extensive set of experimental comparisons
of separable piecewise-linear value function approximations (denoted the “SPAR”
algorithm since it uses the projection operator for maintaining concavity described
in Section 13.3) and variations of Benders’ decomposition (which can be viewed
as nonseparable value function approximations). These variations include L-shaped
(which requires solving a linear program for every sample outcome), CUPPS (which
requires looping over all sample outcomes and performing a trivial calculation),
and stochastic decomposition (which only requires looping over the sample out-
comes that we have actually sampled, performing modest calculations for each
outcome). Stochastic decomposition can be viewed as a form of approximate
dynamic programming (without a state variable). All of our experiments were done
(by necessity) with a ﬁnite set of outcomes. L-shaped is computationally expensive,
but for smaller problems if could be generally counted on to produce the optimal
solution, which was then used to evaluate the quality of the solutions produced by
the other algorithms.
The experiments were conducted for a distribution problem where resources
were distributed to a set of locations, after which they were used to satisfy a
demand. The number of locations determines the dimensionality of Rx
0 and R1.
The problems tested used 10, 25, 50, and 100 locations. L-shaped decomposition
found the optimal solution for all the problems except the one with 100 locations.
The results are shown in Table 13.1, which shows the percentage distance from
the optimal solution for all three algorithms as a function of the number of iterations.
The results show that L-shaped decomposition generally has the fastest rate of
convergence (in terms of iterations), but the computational demand of solving linear
programs for every outcome usually makes this computationally intractable for most
applications. All the methods based on Benders’ decomposition show progressively
slower convergence as the number of locations increase, suggesting that this method
will struggle with higher dimensional state variables. As the problems became
larger, the separable approximation showed much faster convergence with near
optimal performance in the limit.

cutting planes
527
Table 13.1
Numerical results for two-stage distribution problems while varying the
number of locations
Number of Independent Demand Observations
Locations
Method
25
50
100
250
500
1000
2500
5000
10
SPAR
18.65
12.21
7.07
2.25
0.48
0.28
0.04
0.15
L-Shaped
3.30
0.19
0.00
CUPPS
5.84
4.19
0.50
0.29
0.00
SD
45.45
9.18
11.35
2.35
2.30
1.12
0.30
0.26
25
SPAR
11.73
4.04
2.92
0.89
0.34
0.13
0.19
0.06
L-Shaped
19.88
15.98
2.14
0.11
0.00
CUPPS
8.27
13.40
4.33
4.02
1.47
0.16
0.00
SD
40.55
29.79
22.22
12.80
4.24
4.80
1.06
0.95
50
SPAR
9.99
2.60
1.18
0.48
0.26
0.30
0.12
0.05
L-Shaped
42.56
20.30
6.07
1.49
0.52
0.04
0.00
CUPPS
34.93
9.91
19.30
11.71
5.09
1.38
0.32
0.00
SD
43.18
29.81
17.94
8.09
5.91
6.25
2.73
1.02
100a
SPAR
8.74
4.61
1.20
0.45
0.16
0.05
0.01
0.00
L-Shaped
74.52
29.79
26.21
7.30
2.32
0.85
0.11
0.02
CUPPS
54.59
35.54
23.99
17.58
14.68
14.13
5.36
0.91
SD
62.63
34.82
40.73
12.14
15.22
17.43
17.49
9.42
aOptimal solution not found, ﬁgures represent the deviation from the best objective value known.
Note: The table gives the percent over optimal (where available) for SPAR (separable, piecewise linear
value functions), along with three ﬂavors of Benders’ decomposition: L-shaped, CUPPS, and stochastic
decomposition (SD).
Source: From Powell et al. (2004).
It is impossible to make general conclusions about the performance of one
method over another, since this is an experimental question that depends on the
characteristics of the datasets used for testing. However, it appears from these
experiments that all ﬂavors of Benders’ decomposition become much slower as the
dimensionality of the state variable grows. Stochastic decomposition and approxi-
mate dynamic programming (SPAR) require approximately the same computational
effort per iteration. Stochastic decomposition (SD) is dramatically faster than the
L-shaped decomposition in terms of the work required per iteration, but the price is
that it will have a slower rate of convergence (in terms of iterations). However, even
L-shaped decomposition exhibits a very slow rate of convergence, especially as the
problem size grows (where problem size is measured in terms of the dimensionality
of the state variable).
Benders’ decomposition (in one of its various forms) offers signiﬁcant value
since it provides a provably convergent algorithm. SPAR (which uses piecewise-
linear separable value function approximations) has been found to work on large-
scale resource allocation problems, but the error introduced by the use of a separable
approximation will be problem-dependent.

528
value function approximations
13.8
WHY DOES IT WORK?**
13.8.1
Proof of the SHAPE Algorithm
The convergence proof of the SHAPE algorithm is a nice illustration of a mar-
tingale proof. We start with the martingale convergence theorem (Doob, 1953;
Neveu, 1975; Taylor, 1990), which has been the basis of convergence proofs for
stochastic subgradient methods (as illustrated in Gladyshev, 1965). We then list
some properties of the value function approximation ˆV (·) and produce a bound on
the difference between xn and xn+1. Finally, we prove the theorem. This section
is based on Powell and Cheung (2000).
Let
ωn
be
the
information
that
we
sample
in
iteration
n,
and
let
ω = (ω1, ω2, . . .) be an inﬁnite sequence of observations of sample infor-
mation where ω ∈. Let hn = ω1, ω2, . . . , ωn be the history up to (and
including) iteration n. Let F be the σ-algebra on , and let Fn ⊆Fn+1 be the
sequence of increasing subsigma-algebras on  representing the information we
know up through iteration n.
We assume the following:
(A.1) X is convex and compact.
(A.2) EV (x, W) is convex, ﬁnite and continuous on X.
(A.3) gn is bounded such that ∥gn∥≤c1.
(A.4) ˆV n(x) is strongly convex, meaning that
ˆV n(y) −ˆV n(x) ≥ˆvn(x)(y −x) + b∥x −y∥2,
(13.49)
where b is a positive number that is a constant throughout the optimization
process. The term b∥x −y∥2 is used to ensure that the slope ˆvn(x) is a
monotone function of x. When we interchange y and x in (13.49) and add
the resulting inequality to (13.49), we obtain
(ˆvn(y) −ˆvn(x))(y −x) ≥2b∥x −y∥2.
(13.50)
(A.5) The stepsizes αn are Fn measurable and satisfy
0 < αn < 1,
∞

n=0
E{(αn)2} < ∞
and
∞

n=0
αn = ∞
a.s.
(A.6) ˆV 0(x) is bounded and continuous, and ˆv0(x) (the derivative of ˆV 0(x)) is
bounded for x ∈X.

why does it work?
529
Note that we require that the expected sum of squares be bounded, whereas we
must impose the almost sure condition that the sum of stepsizes is inﬁnite. We now
state our primary theorem.
Theorem 13.8.1
If (A.1) through (A.6) are satisﬁed, then the sequence xn, pro-
duced by algorithm SHAPE, converges almost surely to an optimal solution x∗∈X∗
of problem (13.7).
The proof illustrates several proof techniques that are commonly used for these
problems. Students interested in doing more fundamental research in this area may
be able to use some of the devices in their own work.
To prove the theorem, we need to use the Martingale convergence theorem and
two lemmas.
Martingale Convergence Theorem
A sequence of random variables {W n} that are Fn-measurable is said to be a
supermartingale if the sequence of conditional expectations E{W n+1|Fn} exists
and satisﬁes
E{W n+1|Fn} ≤W n.
Theorem 13.8.2
(Neveu (1975), p. 26) Let W n be a positive supermartingale.
Then W n converges to a ﬁnite random variable a.s.
From the deﬁnition, W n is essentially the stochastic analog of a decreasing
sequence.
Property of Approximations
In addition to equations (13.49) and (13.50) of assumption (A.4), the optimal
solution for problem (13.7) at iteration n can be characterized by the variational
inequality
ˆvn(xn)(x −xn) ≥0
∀x ∈X.
(13.51)
Furthermore, at iteration k + 1,

ˆvn(xn+1) + αn(gn −ˆvn(xn))

(x −xn+1) ≥0
∀x ∈X.
(13.52)
The ﬁrst lemma below provides a bound on the difference between two consecutive
solutions. The second lemma establishes that V n(x) is bounded.
Lemma 13.8.1
The solutions xn produced by algorithm SHAPE satisfy
∥xn −xn+1∥≤αn
2b∥gn∥,
where b satisﬁes equation (13.49).

530
value function approximations
Proof
Substituting x by xn in (13.52), we have
αn

gn −ˆvn(xn)

(xn −xn+1) ≥ˆvn(xn+1)(xn+1 −xn).
Rearranging the terms, we obtain
αngn(xn −xn+1) ≥ˆvn(xn+1)(xn+1 −xn) −αn ˆvn(xn+1 −xn)
=

ˆvn(xn+1) −ˆvn(xn)

(xn+1 −xn)
+(1 −αn)ˆvn(xn)(xn+1 −xn).
Combining (13.50), (13.51), and 0 < αn < 1 gives us
αngn(xn −xn+1) ≥2b∥xn −xn+1∥2 + (1 −αn)ˆvn(xn)(xn+1 −xn)
≥2b∥xn −xn+1∥2.
Applying Schwarz’s inequality, we have that
αn∥gn∥· ∥xn −xn+1∥≥αngn(xn −xn+1) ≥2b∥xn −xn+1∥2.
Dividing both sides by ∥xn −xn+1∥, it follows that ∥xn −xn+1∥≤αn/2b
∥gn∥.
□
Lemma 13.8.2
The approximation function ˆV n(x) in iteration n can be written as
ˆV n(x) = ˆV 0(x) + rnx,
where rn is a ﬁnite vector.
Proof
The algorithm proceeds by adding linear terms to the original approxima-
tion. Thus, at iteration n, the approximation is the original approximations plus the
linear term
ˆV n(x) = ˆV 0(x) + rnx,
(13.53)
where rn is the sum of the linear correction terms. We just have to show that rn is
ﬁnite.
When taking the ﬁrst derivative of ˆV n(x) in equation (13.53), we have
ˆvn(x) = ˆv0(x) + rn.
With that, we can write ˆV n+1(x) in terms of ˆV 0(x)
ˆV n+1(x) = ˆV n(x) + αn(gn −ˆvn(x))x
= ˆV 0(x) + rnx + αn(gn −ˆvn(x))x
= ˆV 0(x) + rnx + αn(gn −ˆv0(xk) −rn)x.

why does it work?
531
Therefore rn+1 and rn are related as follows:
rn+1 = αn(gn −ˆv0(xk)) + (1 −αn)rn.
(13.54)
So, the total change in our initial approximation is a weighted sum of gn −ˆv0(xn)
and the current cumulative change. Since both gn and ˆv0(xn) are ﬁnite, there exists
a ﬁnite, positive vector such that
ˆd ≥max
k
|gn −ˆv0(xn)|.
(13.55)
We can now use induction to show that rn ≤ˆd for all n. For n = 1, we have
r1 = a0(g0 −ˆv0
0) ≤a0 ˆd. Since a0 < 1 and is positive, we have r1 ≤ˆd. Assuming
that rn ≤ˆd, we want to show rn+1 ≤ˆd. By using this assumption and the deﬁnition
of ˆd, equation (13.54) implies that
rn+1 ≤αn|gn −ˆv0(xk)| + (1 −αn)rn ≤αn ˆd + (1 −αn) ˆd = ˆd.
We now return to our main result.
□
Proof of Theorem 13.8.1
Our algorithm proceeds in three steps. First, we establish a supermartingale that
provides a basic convergence result. Then, we show that there is a convergent sub-
sequence. Finally, we show that the entire sequence is convergent. For simplicity,
we write ˆvn = ˆvn(xn).
Step 1. Establish a supermartingale for theorem 13.8.2.
Let T n = ˆV n(x∗) −ˆV n(xn), and consider the difference of T n+1 and T n:
T n+1 −T n
= ˆV n+1(x∗) −ˆV n+1(xn+1) −ˆV n(x∗) + ˆV n(xn)
= ˆV n(x∗) + αn(gn −ˆvn)x∗−ˆV n(xn+1) −αn(gn −ˆvn)xn+1
−ˆV n(x∗) + ˆV n(xn).
If we write x∗−xn+1 as x∗−xn + xn −xn+1, we get
T n+1 −T n
= ˆV n(xn) −ˆV n(xn+1) −αn ˆvn˜(xn −xn+1)



(I)
−αn ˆvn˜(x∗−xn)



(II)
+ αngn(x∗−xn)



(III)
+ αngn(xn −xn+1)



(IV)
.

532
value function approximations
Consider each part individually. First, by the convexity of ˆV n(x), it follows that
ˆV n(xn) −ˆV n(xn+1) ≤ˆvn˜(xn −xn+1)
= (1 −αn)ˆvn˜(xn −xn+1) + αn ˆvn˜(xn −xn+1).
From equation (13.51) and 0 < αn < 1, we know that (I) ≤0. Again, from equation
(13.51) and 0 < αn < 1, we show that (II) ≥0.
For (III), by the deﬁnition that gn ∈∂V (xn, ωn+1),
gn(x∗−xn) ≤V (x∗, ωn+1) −V (xn, ωn+1),
where V (x, ωn+1) is the recourse function given outcome ωn+1.
For (IV), lemma 13.8.1 implies that
αngn(xn −xn+1) ≤αn∥gn∥· ∥xn −xn+1∥≤(αn)2∥gn∥2
2b
≤(αn)2c2
1
2b
.
Therefore, the difference T n+1 −T n becomes
T n+1 −T n ≤−αn

V (xn, ωn+1) −V (x∗ωn+1)

+ (αn)2c2
1
2b
.
(13.56)
Taking the conditional expectation with respect to Fn on both sides, it follows that
E{T n+1|Fn} ≤T n −αn(V(xn) −V(x∗)) + (αn)2c2
1
2b
,
(13.57)
where T n, αn and xk on the right-hand side are deterministic given the conditioning
on Fn. We replace V (x, ωn+1) (for x = xn and x = x∗) with its expectation V(x)
since conditioning on Fn tells us nothing about ωn+1. Since V(xn) −V(x∗) ≥0,
the sequence
W n = T n + c2
1
2b
∞

i=k
a2
i
is a positive supermartingale. Theorem 13.8.2 implies the almost sure convergence
of W n. Thus
T n →T ∗
a.s.
Step 2. Show that there exists a subsequence nj of n such that xnj →x∗∈X∗a.s.
Summing equation (13.56) over n up to N and canceling the alternating terms of
T n gives
T N+1 −T 0 = −
N

n=0
αn(V (xn, ωn+1) −V (x∗, ωn+1)) +
N

n=0
(αn)2c2
1
2b
.

why does it work?
533
Take expectations of both sides. For the ﬁrst term on the right-hand side, we take
the conditional expectation ﬁrst conditioned on Fn and then over all Fn, giving us
E{T N+1 −T 0} = −
N

n=0
E
'
E{αn(V (xn, ωn+1) −V (x∗, ωn+1))|Fn}
(
+E
 N

n=0
(αn)2c2
1
2b

= −
N

n=0
E
'
αn(V(xn) −V(x∗))
(
+ c2
1
2b
N

n=0
E
'
(αn)2(
.
Taking the limit as N →∞and using the ﬁniteness of T n and ∞
n=0 E{α2
n}, we
have
∞

n=0
E
'
αn(V(xn) −V(x∗))
(
< ∞.
Since V(xn) −V(x∗) ≥0 and ∞
n=0 αn = ∞(a.s.), there exists a subsequence nj
of n such that
V(xnj ) →V(x∗)
a.s.
By continuity of V, this sequence converges. That is,
xnj →x∗∈X∗
a.s.
Step 3. Show that xn →x∗∈X∗a.s.
Consider the convergent subsequence xnj in step 2. By using the expression of ˆV n
in lemma 13.8.2, we can write T nj as
T nj = ˆV nj (xnj ) −ˆV nj (x∗)
= ˆV 0(xnj ) −ˆV 0(x∗) + rnj (xnj −x∗)
≤ˆV 0(xnj ) −ˆV 0(x∗) + rnj |xnj −x∗|
≤ˆV 0(xnj ) −ˆV 0(x∗) + ˆd|xnj −x∗|,
where ˆd is the positive, ﬁnite vector deﬁned in (13.55). When xnj →x∗, both the
terms ˆd|xnj −x∗| and ˆV 0(xnj ) −ˆV 0(x∗) (by continuity of ˆV 0) go to 0. Since T nj
is positive, we obtain that T nj →0 a.s. Combining this result and the result in
step 1 (that T n converges to a unique nonnegative T ∗a.s.), we have T n →T ∗= 0
a.s. Finally, we know from the strong convexity of ˆV n(·) that
T n = ˆV n(xn) −ˆV n(x∗) ≥b∥xn −x∗∥2 ≥0.
Therefore xn →x∗a.s.

534
value function approximations
13.8.2
The Projection Operation
Let vn−1
s
be the value (or the marginal value) of being in state s at iteration
n −1 and assume that we have a function where we know that we should have
vn−1
s+1 ≥vn−1
s
(we refer to this function as monotone). For example, if this monotone
is the marginal value, we would expect this if we are describing a concave function.
Now assume that we have a sample realization ˆvn
s that is the value (or marginal
value) of being in state s. We would then smooth this new observation with the
previous estimates using
zn
s =

(1 −αn−1)vn−1
s
+ αn−1 ˆvn
if s = sn,
vn
s
otherwise.
(13.58)
Since ˆvn
s is random, we cannot expect zn
s to also be monotone. In this section we
want to restore monotonicity by deﬁning an operator vn = V (z) where vn
s+1 ≥vn
s .
There are several ways to do this. In this section we deﬁne the operator v = V (z),
which takes a vector z (not necessarily monotone) and produces a monotone vector
v. If we wish to ﬁnd v that is as close as possible to z, we would solve
min 1
2∥v −z∥2
subject to
vs+1 −vs ≤0,
s = 0, . . . , M.
(13.59)
Assume that v0 is bounded above by B, and vM+1, for s < M, is bounded from
below by −B. Let λs ≥0, s = 0, 1, . . . , M be the Lagrange multipliers associated
with equation (13.59). It is easy to see that the optimality equations are
vs = zs + λs −λs−1,
s = 1, 2, . . . , M,
(13.60)
λs(vs+1 −vs) = 0,
s = 0, 1, . . . , M.
(13.61)
Let i1, . . . , i2 be a sequence of states where
vi1−1 > vi1 = vi1+1 = · · · = c = · · · = vi2−1 = vi2 > vi2+1.
We can then add equations (13.60) from i1 to i2 to yield
c =
1
i2 −i1 + 1
i2

s=i1
zs.
If i1 = 1, then c is the smaller of the above and B. Similarly, if i2 = M, then c is
the larger of the above and −B.
We also note that vn−1 ∈V and zn computed by differs from vn−1 in just one
coordinate. If zn /∈V then either zn
sn−1 < zn
sn, or zn
sn+1 > zn
sn.
If zn
sn−1 < zn
sn, then we we need to ﬁnd the largest 1 < i ≤sn, where
zn
i−1 ≥
1
sn −i + 1
sn

s=i
zn
s .

bibliographic notes
535
If i cannot be found, then we use i = 1. We proceed to compute
c =
1
sn −i + 1
sn

s=i
zn
s
and let
vn+1
j
= min(B, c),
j = i, . . . , sn.
We have λ0 = max(0, c −B), and
λs =



0,
s = 1, . . . , i −1,
λs−1 + zs −vs,
s = i, . . . , sn −1,
0,
s = sn, . . . , M.
It is easy to show that the solution found and the Lagrange multipliers satisfy
equations (13.60) and (13.61).
If zn
sn < zn
sn+1, then the entire procedure is basically the same with appropriate
inequalities reversed.
13.9
BIBLIOGRAPHIC NOTES
Section 13.1
The decision of whether to estimate the value function or its
derivative is often overlooked in the dynamic programming literature, espe-
cially within the operations research community. In the controls community,
use of gradients is sometimes referred to as dual heuristic dynamic program-
ming (see Werbos, 1992a; Venayagamoorthy et al. (2002)).
Section 13.3
The theory behind the projective SPAR algorithm is given in
Powell et al. (2004). A proof of convergence of the leveling algorithm is
given in Topaloglu and Powell (2003).
Section 13.5
The SHAPE algorithm was introduced by Powell and Cheung
(2000).
Section 13.7
The ﬁrst paper to formulate a math program with uncertainty
appears to be Dantzig and Ferguson (1956). For a broad introduction to the
ﬁeld of stochastic optimization, see Ermoliev (1988) and Pﬂug (1996). For
complete treatments of the ﬁeld of stochastic programming, see Infanger
(1994), Kall and Wallace (1994), Birge and Louveaux (1997), and Kall and
Mayer (2005). For an easy tutorial on the subject, see Sen and Higle (1999). A
very thorough introduction to stochastic programming is given in Ruszczynski
and Shapiro (2003). Mayer (1998) provides a detailed presentation of com-
putational work for stochastic programming. There has been special interest
in the types of network problems we have considered (see Wallace, 1986a,
b, 1987, Rockafellar and Wets (1991) presents specialized algorithms for
stochastic programs formulated using scenarios. This modeling framework

536
value function approximations
has been of particular interest in the are of ﬁnancial portfolios (Mulvey and
Ruszczynski, 1995). Benders’ decomposition for two-stage stochastic pro-
grams was ﬁrst proposed by Van Slyke and Wets (1969) as the “L-shaped”
method. Higle and Sen (1991) introduce stochastic decomposition, which is
a Monte Carlo based algorithm that is most similar in spirit to approximate
dynamic programming. Powell and Chen (1999) present a variation of Ben-
ders that falls between stochastic decomposition and the L-shaped method.
The relationship between Benders’ decomposition and dynamic programming
is often overlooked. A notable exception is Pereira and Pinto (1991), who use
Benders to solve a resource allocation problem arising in the management of
reservoirs. This paper presents Benders as a method for avoiding the curse of
dimensionality of dynamic programming. For an excellent review of Benders’
decomposition for multistage problems, see Ruszczynski (2003). Benders has
been extended to multistage problems in Birge (1985), Ruszczynski (1993),
and Powell and Chen (1999), which can be viewed as a form of approximate
dynamic programming using cuts for value function approximations.
Section 13.8.1
The proof of convergence of the SHAPE algorithm is based on
Powell and Cheung (2000).
Section 13.8.2
The proof of the projection operation is based on Powell et al.
(2004).
PROBLEMS
13.1
Consider a news vendor problem where we solve
max
x
EF(x, ˆD),
where
F(x, ˆD) = p min(x, ˆD) −cx.
We have to choose a quantity x before observing a random demand ˆD. For
our problem, assume that c = 1, p = 2, and that
ˆD follows a discrete
uniform distribution between 1 and 10 ( ˆD = d, d = 1, 2, . . . , 10 with
probability 0.10). Approximate EF(x, ˆD) as a piecewise linear function
using the methods described in Section 13.3, using a stepsize αn−1 = 1/n.
Note that you are using derivatives of F(x, ˆD) to estimate the slopes
of the function. At each iteration, randomly choose x between 1 and 10.
Use sample realizations of the gradient to estimate your function. Com-
pute the exact function and compare your approximation to the exact
function.

problems
537
13.2
Repeat exercise 13.6, but this time approximate EF(x, ˆD) using a linear
approximation:
F(x) = θx.
Compare the solution you obtain with a linear approximation to what you
obtained using a piecewise-linear approximation. Now repeat the exercise
using demands that are uniformly distributed between 500 and 1000. Com-
pare the behavior of a linear approximation for the two different problems.
13.3
Repeat exercise 13.3, but this time approximate EF(x, ˆD) using the
SHAPE algorithm. Start with an initial approximation given by
F
0(x) = θ0(x −θ1)2.
Use the recursive regression methods of Sections 13.6 and 9.3 to ﬁt the
parameters. Justify your choice of stepsize rule. Compute the exact function
and compare your approximation to the exact function.
13.4
Repeat exercise 13.1, but this time approximate EF(x, ˆD) using the regres-
sion function given by
F(x) = θ0 + θ1x + θ2x2.
Use the recursive regression methods of Sections 13.6 and 9.3 to ﬁt the
parameters. Justify your choice of stepsize rule. Compute the exact function
and compare your approximation to the exact function. Estimate your value
function approximation using two methods:
(a) Use observations of F(x, ˆD) to update your regression function.
(b) Use observations of the derivative of F(x, ˆD), so that F(x) becomes
an approximation of the derivative of EF(x, ˆD).
13.5
Approximate the function EF(x, ˆD) in exercise 13.1, but now assume that
the random variable ˆD = 1 (that is, it is deterministic). Using the following
approximation strategies:
(a) Use a piecewise-linear value function approximation. Try using both
left and right derivatives to update your function.
(b) Use the regressionF(x) = θ0 + θ1x + θ2x2.
13.6
We are going to solve the basic asset acquisition problem (Section 2.2.5)
where we purchase assets (at a price pp) at time t to be used in time interval
t + 1. We sell assets at a price ps to satisfy the demand ˆDt that arises during
time interval t. The problem is to be solved over a ﬁnite time horizon T.

538
value function approximations
Assume that the initial inventory is 0 and that demands follow a discrete
uniform distribution over the range [0, Dmax]. The problem parameters are
given by
γ = 0.8,
Dmax = 10,
T = 20,
pp = 5,
ps = 8.
Solve this problem by estimating a piecewise linear value function approx-
imation (Section 13.3). Choose αn+1 = a/(a + n) as your stepsize rule,
and experiment with different values of a (e.g., 1, 5, 10, and 20). Use a
single-pass algorithm, and report your proﬁts (summed over all time peri-
ods) after each iteration. Compare your performance for different stepsize
rules. Run 1000 iterations and try to determine how many iterations are
needed to produce a good solution (the answer may be substantially less
than 1000).
13.7
Repeat exercise 13.6, but this time use the SHAPE algorithm to approxi-
mate the value function. Use as your initial value function approximation
the function
V
0
t (Rt) = θ0(Rt −θ2)2.
For each of the exercises below, you may have to tweak your stepsize
rule. Try to ﬁnd a rule that works well for you (we suggest stick with
a basic a/(a + n) strategy). Determine an appropriate number of training
iterations, and then evaluate your performance by averaging results over
100 iterations (testing iterations) where the value function is not changed.
(a) Solve the problem using θ0 = 1, θ1 = 5.
(b) Solve the problem using θ0 = 1, θ1 = 50.
(c) Solve the problem using θ0 = 0.1, θ1 = 5.
(d) Solve the problem using θ0 = 10, θ1 = 5.
(e) Summarize the behavior of the algorithm with these different parame-
ters.
13.8
Repeat exercise 13.6, but this time assume that your value function approx-
imation is given by
V
0
t (Rt) = θ0 + θ1Rt + θ2R2
t .
Use the recursive regression techniques of sections 13.6 and 9.3 to deter-
mine the values for the parameter vector θ.

problems
539
13.9
Repeat exercise 13.6, but this time assume you are solving an inﬁnite
horizon problem (which means you only have one value function approx-
imation).
13.10
Repeat exercise 13.8, but this time assume an inﬁnite horizon.
13.11
Repeat exercise 13.6, but now assume the following problem parameters:
γ = 0.99,
T = 200,
pp = 5,
ps = 20.
For the demand distribution, assume that ˆDt = 0 with probability 0.95, and
that ˆDt = 1 with probability 0.05. This is an example of a problem with
low demands, where we have to hold inventory for a fairly long time.


C H A P T E R
14
Dynamic Resource
Allocation Problems
There is a vast array of problems that fall under the umbrella of “resource alloca-
tion.” We might be managing a team of medical specialists who have to respond to
emergencies, or technicians who have to provide local support for the installation
of sophisticated medical equipment. Alternatively, a transportation company might
be managing a ﬂeet of vehicles, or an investment manager might be trying to
determine how to allocate funds among different asset classes. We can even think
of playing a game of backgammon as a problem of managing a single resource (the
board), although in this chapter we are only interested in problems that involve
multiple resources where quantity is an important element of the decision variable.
Aside from the practical importance of these problems, this problem class pro-
vides a special challenge. In addition to the usual challenge of making decisions
over time under uncertainty (the theme of this entire book) we now have to deal
with the fact that our decision xt is suddenly of very high dimensionality, requiring
that we use the tools of math programming (linear, nonlinear, and integer program-
ming). For practical applications it is not hard to ﬁnd problems in this class where
xt has thousands, or even tens of thousands, of dimensions. This problem class
clearly suffers from all three “curses of dimensionality.”
We illustrate the ideas by starting with a scalar problem, after which we move
to a sequence of multidimensional (and in several cases, very high-dimensional)
resource allocation problems. Each problem offers different features, but they can
all be solved using the ideas we have been developing throughout this volume.
14.1
AN ASSET ACQUISITION PROBLEM
Perhaps one of the simplest resource allocation problems is the basic asset acqui-
sition problem where we acquire assets (money, oil, water, aircraft) to satisfy a
Approximate Dynamic Programming: Solving the Curses of Dimensionality, Second Edition.
Warren B. Powell.
2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
541

542
dynamic resource allocation problems
future, random demand. In our simple illustration we purchase assets at time t at a
price that can be used to satisfy demands in the next time period (which means we
do not know what they are). If we buy too much, the leftover inventory is held to
future time periods. If we cannot satisfy all the demand, we lose revenue, but we
do not allow ourselves to hold these demands to be satisﬁed at a later time period.
We use the simple model presented in Section 2.2.5 so that we can focus our
attention on using derivatives to estimate a piecewise linear function in the context
of a dynamic program.
14.1.1
The Model
Our model is formulated as follows:
Rt = Assets on hand at time t before we make a new ordering decision, and
before we have satisﬁed any demands arising in time interval t.
xt = Amount of product purchased at time t to be used during time interval
t + 1.
ˆDt = Random demands that arise between t −1 and t.
ˆRt = Random exogenous changes to our asset levels (donations of blood, theft of
product, leakage).
New assets are purchased at a ﬁxed price pp and are sold at a ﬁxed price ps > pp.
The contribution earned during time interval t is given by
Ct(Rt, xt) = ps min{Rt, ˆDt} −ppxt.
The transition function for Rt is given by
Rx
t = Rt −min{Rt, ˆDt} + xt,
Rt+1 = Rx
t + ˆRt+1,
where we assume that any unsatisﬁed demands are lost to the system.
This problem can be solved using Bellman’s equation. For this problem, Rt
is our state variable. Let Vt(Rx
t ) be the value of being in post-decision state Rx
t .
Following our usual approximation strategy, the decision problem at time t, given
that we are in state Rn
t , is given by
xn
t = arg max
xt∈X nt

Ct(Rn
t , xt) + γ V
n−1
t
(Rx
t )

.
(14.1)
Recall that solving (14.1) is a form of decision function that we represent by
Xπ
t (Rt), where the policy is to solve (14.1) using the value function approximation
V
n−1
t
(Rx
t ). Assume, for example, that our approximation is of the form
V
n−1
t
(Rx
t ) = θ0 + θ1Rx
t + θ2(Rx
t )2,

an asset acquisition problem
543
where Rx
t = Rt −min{Rt, ˆDt} + xt. In this case, (14.1) looks like
xn
t = arg max
xt∈X nt

ps min{Rt, ˆDt} −ppxt + γ (θ0 + θ1Rx
t + θ2(Rx
t )2)

.
(14.2)
We ﬁnd the optimal value of xn
t by taking the derivative of the objective function
and setting it equal to zero, giving us
0 = dCt(Rn
t , xt)
dxt
+ γ dV
n−1
t
(Rx
t )
dxt
= −pp + γ dV
n−1
t
(Rx
t )
dRx
t
dRx
t
dxt
= −pp + γ (θ1 + 2θ2Rx
t )
= −pp + γ (θ1 + 2θ2(Rt −min{Rt, ˆDt} + xt)),
where we used dRx
t /dxt = 1. Solving for xt gives
xn
t =
1
2θ2
pp
γ −θ1

−(Rn
t −min{Rn
t , ˆDn
t }).
Of course, we assume that xn
t ≥0.
14.1.2
An ADP Algorithm
The simplest way to compute a gradient is using a pure forward-pass implementa-
tion. At time t, we have to solve
˜V n
t (Rn
t ) = max
xt

ps min{Rt, ˆDt} −ppxt + γ V
n−1
t
(Rx
t )

.
Here ˜V n
t (Rn
t ) is just a placeholder. We are going to compute a derivative using
a ﬁnite difference. We begin by recalling that the pre-decision resource state is
given by
Rn
t = RM,W(Rx,n
t−1, Wt(ωn))
= Rx,n
t−1 + ˆRn
t .
Now let
Rn+
t
= RM,W(Rx,n
t−1 + 1, Wt(ωn))
be the pre-decision resource vector when Rx,n
t−1 is incremented by one. We next
compute a derivative using the ﬁnite difference
ˆvn
t = ˜V n
t (Rn+
t
) −˜V n
t (Rn
t ).
Note that when we solve the perturbed value ˜V n
t (Rn+
t
) we have to re-optimize xt.
For example, it is entirely possible that if we increase Rx,n
t−1 by one, then xn
t will

544
dynamic resource allocation problems
decrease by one. As is always the case with a single-pass algorithm, ˆvn
t depends
on V
n
t (Rx
t ), and as a result will typically be biased.
Once we have a sample estimate of the gradient ˆvn
t , we next have to update the
value function. We can represent this updating process generically using
V
n
t−1 = U V (V
n−1
t−1 , Rx,n
t−1, ˆvn
t ).
The actual updating process depends on whether we are using a piecewise-linear
approximation, the SHAPE algorithm, or a general regression equation. Remember
that we are using ˆvn
t to update the post-decision value function at time t−1.
The complete algorithm is outlined in Figure 14.1, which is an adaptation of
our original single-pass algorithm. A simple but critical conceptual difference is
that we are now explicitly assuming that we are using a continuous functional
approximation.
14.1.3
How Well Does It Work?
This approximation strategy is provably convergent. It also works in practice (!!).
Unfortunately, while a proof of convergence can be reassuring, it is not hard to
ﬁnd provably convergent algorithms for problems that would never actually work
(typically because the rate of convergence is far too slow).
The asset acquisition problem is simple enough that we can ﬁnd the optimal
solution using the exact methods provided in Chapter 3. For our approximation we
used a piecewise-linear value function approximation. We used a pure exploitation
strategy since the concave structure of the value function will eventually push us
toward the correct solution.
Figure 14.2 shows the solution quality for a ﬁnite horizon problem (undis-
counted). We are within 5 percent of optimal after about 50 iterations, and within
1 percent after 100 iterations. For many problems in practice (where we do not
even know the optimal solution) this is extremely good.
14.1.4
Variations
This problem class is special because we are controlling the quantity of a single
type of asset to serve random demands. Using approximate dynamic programming
to ﬁt piecewise-linear value function approximations produces an algorithm that
is provably convergent (at least for the ﬁnite horizon case) without the need for
exploration. We can use our current value function approximation to determine our
decision at time t. Even if the value function is off, it will eventually converge
to the correct value function (for the states that an optimal policy visits inﬁnitely
often).
This problem class seems quite simple, but it actually includes a number of
variations that arise in industry.
• Cash balance for mutual funds. Imagine that we want to determine how much
cash to have on hand in a mutual fund in order to handle requests for redemp-
tions. If we have too much cash, we can invest it in equities do obtain a

an asset acquisition problem
545
Step 0. Initialize:
Step 0a. Initialize V
0
t (Sx
t ) for all time periods t.
Step 0b. Initialize S1
0.
Step 0c. Let n = 1.
Step 1. Choose ωn.
Step 2. Do for t = 0, 1, 2, . . . , T:
Step 2a. Let the state variable be
Sn
t = (Rn
t , ˆDt(ωn)),
and let
Sn+
t
= (Rn
t + 1, ˆDt(ωn))
Step 2b. Solve
˜V n
t (Sn
t ) = max
xt

ps min{Rt, ˆDt(ωn)} −ppxt + γV
n−1
t
(Rx
t )

,
where Rx
t = Rt −min{Rt, ˆDt} + xt. Let xn
t be the value of xt that solves the maxi-
mization problem. Also ﬁnd ˜V n
t (Sn+
t
).
Step 2c. Compute
ˆvn
t = ˜V n
t (Sn+
t
) −˜V n
t (Sn
t ).
Step 2d. If t>0 update the value function:
V
n
t−1 = UV (V
n−1
t−1 , Sx,n
t−1, ˆvn
t ).
Step 2e. Update the state variable
Sn
t+1 = SM(Sn
t , xn
t , Wt+1(ωn)).
Step 3. Let n = n+1. If n<N , go to step 1.
Figure 14.1
Approximate dynamic programming algorithm for the asset acquisition problem.
higher rate of return, but this return ﬂuctuates and there are transaction costs
for moving money between cash and equities. The state variable has to include
not just how much cash we have on hand but also information on the state
of the stock market and interest rates. This information is part of the state
variable, but the information evolves exogenously.
• Gas storage. Companies will store natural gas in coal mines, buying gas when
the price is low and selling it when it is high. Gas can be purchased from
multiple suppliers at prices that ﬂuctuate over time. Gas may be sold at time
t to be delivered at a time t′ > t (when demand is higher).
• Managing vaccine inventories. Imagine that we have a limited supply of vac-
cines that have to be made available for the highest risk segments of the

546
dynamic resource allocation problems
0
1
2
3
4
5
6
7
8
9
10
0
50
100
150
200
250
300
Iterations
Percent from optimal
Figure 14.2
Percent from optimal for the simple asset acquisition problem using a piecewise-linear
value function approximation.
population. The strategy has to balance the degree of risk now against poten-
tial future needs for the vaccine. Instead of satisfying a single demand, we
face different types of demands with different levels of importance.
• Maintaining spares. There are many problems where we have to maintain
spares (spare engines for business aircraft, spare transformers for electric
power grids). We have to decide how many new spares to purchase, and
whether to satisfy a demand against holding inventory for future (and poten-
tially more important) demands.
Some of these problems can be solved using a minor variation of the algorithmic
strategy that we described in this section. But this is not always the case. Consider
a problem where we are managing the inventory of a single product (water, oil,
money, spares) given by a scalar Rt, but where our decisions have to consider other
variables (stock market, weather, technology parameters) that evolve exogenously.
Let ˜St be the vector of variables that inﬂuence the behavior of our system (which
means they belong to the state variable) but that evolve exogenously. Our state
variable is given by
St = (Rt, ˜St).
For example, ˜St might be temperature (if we have an energy problem), rainfall
(if we are planning the storage of water in reservoirs), or even the S & P 500
stock index and interest rates. ˜St might be a scalar, but it might consist of several
variables.
We can still approximate the problem using a piecewise linear value function,
but instead of one function, we have to estimate a family of functions that we
would represent using V t(Rt| ˜St). It is just that instead of using V t(Rx
t ), we have
to use V t(Rx
t | ˜St). The algorithm is still provably convergent, but computationally
it can be much more difﬁcult. When ˜St is a vector, we encounter a curse-of-
dimensionality problem if we estimate one function for each possible value of

the blood management problem
547
˜St (a classic lookup table representation). Instead of estimating one piecewise-
linear value function V t(Rx
t ), we might have to estimate hundreds or thousands
(or more). If ˜St is a vector, we might need to look into other types of functional
representations.
14.2
THE BLOOD MANAGEMENT PROBLEM
The problem of managing blood inventories serves as a particularly elegant illus-
tration of a resource allocation problem. We are going to start by assuming that we
are managing inventories at a single hospital, where each week we have to decide
which of our blood inventories should be used for the demands that need to be
served in the upcoming week.
We have to start with a bit of background about blood. For the purposes of
managing blood inventories, we care primarily about blood type and age. Although
there is a vast range of differences in the blood of two individuals, for most
purposes doctors focus on the eight major blood types: A+ (“A positive”), A−(“A
negative”), B+, B−, AB+, AB−, O+, and O−. While the ability to substitute
different blood types can depend on the nature of the operation, for most purposes
blood can be substituted according to Table 14.1.
A second important characteristic of blood is its age. The storage of blood is
limited to six weeks, after which it has to be discarded. Hospitals need to anticipate
if they think they can use blood before it hits this limit, as it can be transferred
to blood centers that monitor inventories at different hospitals within a region. It
helps if a hospital can identify blood it will not need as soon as possible so that
the blood can be transferred to locations that are running short.
One mechanism for extending the shelf life of blood is to freeze it. Frozen blood
can be stored up to 10 years, but it takes at least an hour to thaw, limiting its use
in emergency situations or operations where the amount of blood needed is highly
uncertain. In addition once frozen blood is thawed it must be used within 24 hours.
Table 14.1
Allowable blood substitutions for most operations
Recipient
Donor
AB+
AB−
A+
A−
B+
B−
O+
O−
AB+
X
AB−
X
X
A+
X
X
A−
X
X
X
X
B+
X
X
B−
X
X
X
X
O+
X
X
X
X
O−
X
X
X
X
X
X
X
X
Note: “X” means a substitution is allowed
Source: From Cant (2006).

548
dynamic resource allocation problems
14.2.1
A Basic Model
We can model the blood problem as a heterogeneous resource allocation problem.
We are going to start with a fairly basic model which can be easily extended with
almost no notational changes. We begin by describing the attributes of a unit of
stored blood using
b =
 b1
b2

=
 Blood type(A+, A−, . . .)
Age(in weeks)

,
B = set of all attribute types.
We will limit the age to the range 0 ≤a2 ≤6. Blood with a2 = 6 (which means
blood that is already six weeks old) is no longer usable. We assume that decision
epochs are made in one-week increments.
Blood inventories, and blood donations, are represented using
Rtb = Units of blood of type b available to be assigned or held at time t.
Rt = (Rtb)b∈B.
ˆRtb = Number of new units of blood of type b donated between t −1 and t.
ˆRt = ( ˆRtb)b∈B.
The attributes of demand for blood are given by
d =


d1
d2
d3

=


Blood type of patient
Surgery type: urgent or elective
Is substitution allowed?

,
dφ = decision to hold blood in inventory (“do nothing”),
D = set of all demand types d plus dφ.
The attribute d3 captures the fact that there are some operations where a doctor
will not allow any substitution. One example is childbirth, since infants may not
be able to handle a different blood type, even if it is an allowable substitute. For
our basic model, we do not allow unserved demand in one week to be held to a
later week. As a result we need only to model new demands, which we accomplish
with
ˆDtd = Units of demand with attribute d that arose between t −1 and t,
ˆDt = ( ˆDtd)d∈D.
We act on blood resources with decisions given by
xtbd = Number of units of blood with attribute b that we assign to a demand
of type d.
xt = (xtbd)b∈B,d∈D.

the blood management problem
549
The feasible region X t is deﬁned by the following constraints:

d∈D
xtbd = Rtb,
(14.3)

b∈B
xtbd ≤ˆDtd,
d ∈D,
(14.4)
xtbd ≥0.
(14.5)
Blood that is held simply ages one week, but we limit the age to six weeks. Blood
that is assigned to satisfy a demand can be modeled as being moved to a blood-type
sink, denoted, perhaps, using bt,1 = φ (the null blood type). The blood attribute
transition function rM(bt, dt) is given by
bt+1 =
 bt+1,1
bt+1,2

=




bt,1
min{6, bt,2 + 1}

,
dt = dφ,

φ
—

,
dt ∈D.
To represent the transition function, it is useful to deﬁne
δb′(b, d) =

1,
bx
t = b′ = rM(bt, dt),
0,
otherwise,
 = matrix with δb′(b, d) in row b′ and column (b, d).
We note that the attribute transition function is deterministic. A random element
would arise, for example, if inspections of the blood resulted in blood that was less
than six weeks old being judged to have expired. The resource transition function
can now be written
Rx
tb′ =

b∈B

d∈D
δb′(b, d)xtbd,
Rt+1,b′ = Rx
tb′ + ˆRt+1,b′.
In matrix form, these would be written
Rx
t = xt,
(14.6)
Rt+1 = Rx
t + ˆRt+1.
(14.7)
Figure 14.3 illustrates the transitions that are occurring in week t. We have to
decide either which type of blood to use to satisfy a demand (Figure 14.3a) or to
hold the blood until the following week. If we use blood to satisfy a demand, it
is assumed lost from the system. If we hold the blood until the following week, it
is transformed into blood that is one week older. Blood that is six weeks old may

550
dynamic resource allocation problems
O–,1
O–,2
AB+,1
AB+,2
O–,0
ˆDt,AB
AB+,0
AB+,1
AB+,2
O–,0
O–,1
O–,2
Rt,(AB+,0)
Rt,(AB+,1)
Rt,(AB+,2)
Rt,(O–,0)
Rt,(O–,1)
Rt,(O–,2)
AB+
AB–
A+
A–
B+
B–
O+
O–
x
Rt
AB+,0
ˆ
Satisfy a demand
Hold
ˆ
ˆ
ˆ
ˆ
ˆDt,A
Dt,AB
ˆ
Dt,A
Dt,O
Dt,O
Dt,B
Dt,B
(a)
AB+,0
AB+,1
AB+,2
O–,0
O–,1
O–,2
x
AB+,0
AB+,1
AB+,2
O–,0
O–,1
O–,2
O–,3
AB+,0
AB+,1
AB+,2
AB+,3
O–,0
O–,1
O–,2
O–,3
ˆRt+1,O–
ˆRt+1,AB+
Rt+1
Rt
Rt
ˆDt
Rt,(AB+,0)
Rt,(AB+,1)
Rt,(AB+,2)
Rt,(O–,0)
Rt,(O–,1)
Rt,(O–,2)
(b)
Figure 14.3
Assignment of different blood types (and ages) to known demands in week t (a), and
holding blood until the following week (b). (Solid lines represent assigning blood to a demand, dotted
lines represent holding blood.)

the blood management problem
551
not be used to satisfy any demands, so we can view the bucket of blood that is six
weeks old as a sink for unusable blood (the value of this blood would be zero).
Note that blood donations are assumed to arrive with an age of 0. The pre- and
post-decision state variables are given by
St = (Rt, ˆDt),
Sx
t = (Rx
t ).
There is no real “cost” to assigning blood of one type to demand of another
type: we are not considering steps such as spending money to encourage additional
donations, or transporting inventories from one hospital to another. We use instead
the contribution function to capture the preferences of the doctor. We would like
to capture the natural preference that it is generally better not to substitute, and
that satisfying an urgent demand is more important than an elective demand. For
example, we might use the contributions described in Table 14.2. Thus, if we use
O−blood to satisfy the needs for an elective patient with A+ blood, we would
pick up a −$10 contribution (penalty since it is negative) for substituting blood, a
+$5 for using O−blood (something the hospitals like to encourage), and a +$20
contribution for serving an elective demand, for a total contribution of +$15.
The total contribution (at time t) is ﬁnally given by
Ct(St, xt) =

b∈B

d∈D
ctbdxtbd.
As before, let Xπ
t (St) be a policy (some sort of decision rule) that determines
xt ∈X t given St. We wish to ﬁnd the best policy by solving
max
π∈ E
T

t=0
γ tCt(St, xt).
(14.8)
The most obvious way to solve this problem is with a simple myopic policy, where
we maximize the contribution at each point in time without regard to the effect of
our decisions on the future. We can obtain a family of myopic policies by adjusting
the one-period contributions. For example, our bonus of $5 for using O−blood (in
Table 14.2), is actually a type of myopic policy. We encourage using O−blood
Table 14.2
Contributions for different types of blood and decisions
Condition
Description
Value
if d = dφ
Holding
0
if b1 = b1 when d ∈D
No substitution
0
if b1 ̸= b1 when d ∈D
Substitution
−10
if b1 = O−when d ∈D
O−substitution
5
if d2 = Urgent
Filling urgent demand
40
if d2 = Elective
Filling elective demand
20

552
dynamic resource allocation problems
since it is generally more available than other blood types. By changing this bonus,
we obtain different types of myopic policies that we can represent by the set M,
where for π ∈M our decision function would be given by
Xπ
t (St) = arg max
xt∈X t

b∈B

d∈D
ctbdxtbd.
(14.9)
The optimization problem in (14.9) is a simple linear program (known as a “trans-
portation problem”). Solving the optimization problem given by equation (14.8)
for the set π ∈M means searching over different values of the bonus for using
O−blood.
14.2.2
An ADP Algorithm
As a traditional dynamic program the optimization problem posed in equation (14.8)
is quite daunting. The state variable St has |B| + |D| = 8 × 6 + 8 × 2 × 2 = 80
dimensions. The random variables
ˆR and
ˆD also have a combined 80 dimen-
sions. The decision vector xt has 27 × 8 = 216 dimensions. Using the classical
techniques of Chapter 3, this problem looks hopeless. Using the methods that we
have presented up to now, obtaining effective solutions to this problem is fairly
straightforward.
It is natural to use approximate value iteration to determine the allocation vector
xt using
xn
t = arg max
xt∈X nt

Ct(Sn
t , xt) + V
n−1
t
(Rx
t )

,
(14.10)
where Rx
t = RM(Rt, xt) is given by equation (14.6). The ﬁrst (and most impor-
tant) challenge we face is identifying an appropriate approximation strategy for
V
n−1
t
(Rx
t ). A simple and effective approximation is to use separable piecewise-
linear approximations, which is to say
V t(Rx
t ) =

b∈B
V tb(Rx
tb),
where V tb(Rx
tb) is a scalar piecewise-linear function. It is easy to show that the
value function is concave (as well as piecewise linear), so each V tb(Rx
tb) should
also be concave. Without loss of generality, we can assume that V tb(Rx
tb) = 0 for
Rx
tb = 0, which means that the function is completely characterized by its set of
slopes. We can write the function using
V
n−1
tb (Rx
tb) =


⌊Rx
tb⌋

r=1
V
n−1
tb (r −1) + (Rx
tb −⌊Rx
tb⌋)V
n−1
tb (⌊Rx
tb⌋)

,
(14.11)
where ⌊R⌋is the largest integer less than or equal to R. As we can see, this function
is determined by the set of slopes (V
n−1
tb (r)) for r = 0, 1, . . . , Rmax, where Rmax
is an upper bound on the number of resources of a particular type.

the blood management problem
553
AB+,0
AB+,1
AB+,2
x
Rt
Rt
O–,0
O–,1
O–,2
AB+,0
AB+,1
AB+,2
AB+,3
O–,0
O–,1
O–,2
O–,3
Rt,(AB+,0)
Rt,(AB+,1)
Rt,(AB+,2)
Rt,(O–,0)
Rt,(O–,1)
Rt,(O–,2)
ˆDt
Figure 14.4
Network model for time t with separable, piecewise linear value function approximations.
Assuming that we can estimate this function, the optimization problem we have
to solve (equation (14.10)) is the fairly modest linear program shown in Figure 14.4.
As with Figure 14.3, we have to consider both the assignment of different types
of blood to different types of demand, and the decision to hold blood. To simplify
the ﬁgure, we have collapsed the network of different demand types into a single
aggregate box with demand ˆDt. This network would actually look just like the
network in Figure 14.3a. The decision to hold blood has to consider the value of a
type of blood (including its age) in the future, which we are approximating using
separable piecewise-linear value functions. Here we use a standard modeling trick
that converts the separable piecewise-linear value function approximations into a
series of parallel links from each node representing an element of Rx
t into a super-
sink. Piecewise-linear functions are not only easy to solve (we just need access to
a linear programming solver), they are easy to estimate. For many problem classes
(but not all) they have additionally been found to produce very fast convergence
with high-quality solutions.
With this decision function, we would use approximate value iteration, following
the trajectory determined by the policy. Aside from the customization of the value
function approximation, the biggest difference is in how the value functions are
updated, a problem that is handled in the next section. Of course, we could use
other nonlinear approximation strategies, but this would mean that we would have
to solve a nonlinear programming problem instead of a linear program. While this
can, of course, be done, it complicates the strategy for updating the value function
approximation.
For most operational applications this problem would be solved over a ﬁnite
horizon (e.g., 10 weeks), giving us a recommendation of what to do right now.
Alternatively, we could solve an inﬁnite horizon version of the model by simply
dropping the t index on the value function approximation.

554
dynamic resource allocation problems
14.2.3
Updating the Value Function Approximation
Since our value function depends on slopes, we want to use derivatives to update
the value function approximation. Let
˜Vt(St) = max
xt∈X nt

Ct(Sn
t , xt) + V
n−1
t
(Rx
t )

be the value of our decision function at time t. But what we want is the derivative
with respect to the previous post-decision state Rx
t−1. Recall that St = (Rt, Dt)
depends on Rt = Rx
t−1 + ˆRt. We want to ﬁnd the gradient ∇˜Vt(St) with respect to
Rx
t−1. We apply the chain rule to ﬁnd
∂˜Vt(St)
∂Rx
t−1,b
=

b′∈B
∂˜Vt(St)
∂Rtb′
∂Rtb′
∂Rx
t−1,b
.
If we decide to hold an additional unit of blood of type bt−1, then this produces an
additional unit of blood of type bt = bM(bt−1, d), where bM(b, d) is our attribute
transition function and d is our decision to hold the blood until the next week. For
this application the only difference between bt−1 and bt is that the blood has aged
by one week. We also have to model the fact that there may be exogenous changes
to our blood supply (donations, deliveries from other hospitals, as well as blood
that has gone bad). This means that
Rtbt = Rx
t−1,bt−1 + ˆRtbt .
Notice that we start with Rx
t−1,bt−1 units of blood with attribute bt−1, and then add
the exogenous changes ˆRtbt to blood of type bt. This allows us to compute the
derivative of Rt with respect to Rx
t−1 using
∂Rtbt
∂Rx
t−1,bt−1
=

1
if bt = bM(bt−1, d),
0
otherwise.
So we can write the derivative as
∂˜Vt(St)
∂Rx
t−1,bt−1
= ∂˜Vt(St)
∂Rtbt
.
∂˜Vt(St)/∂Rtb can be found quite easily. A natural byproduct from solving the
linear program in equation (14.10) is that we obtain a dual variable ˆvn
tb for each
ﬂow conservation constraint (14.3). This is a signiﬁcant advantage over strategies
where we visit state Rt (or St) and then update just the value of being in that single
state. The dual variable is an estimate of the slope of the decision problem with
respect to Rtb at the point Rn
tb, which gives us
∂˜Vt(St)
∂Rx
t−1,bt−1
.....
Rx,n
t−1,bt−1
= ˆvn
tbt .

the blood management problem
555
x
Rtb
Range of
dual
variables
ˆ +
x,n
Rtb
Vtb (Rtb)
vtb
ˆ –
vtb
x
Figure 14.5
Range of dual variables for a nondifferentiable function.
Dual variables are incredibly convenient for estimating value function approxima-
tions in the context of resource allocation problems, partly because they approx-
imate the derivatives with respect to Rtb (which is all we are interested in) but
also because we obtain an entire vector of slopes, rather than a single estimate of
the value of being in a state. However, it is important to understand exactly which
slope the dual is (or is not) giving us.
Figure 14.5 illustrates a nondifferentiable function (any problem that can be
modeled as a sequence of linear programs has this piecewise-linear shape). If we
are estimating the slope of a piecewise-linear function Vtb(Rtb) at a point Rtb = Rn
tb
that corresponds to one of these kinks (which is quite common), then we have a
left slope, ˆv−
tb = Vtb(Rtb) −Vtb(Rtb −1), and a right slope, ˆv+
tb = Vtb(Rtb + 1) −
Vtb(Rtb). If ˆvn
tb is our dual variable, we have no way of specifying whether we
want the left or right slope. In fact the dual variable can be anywhere in the range
ˆv+
tb ≤ˆvn
tb ≤ˆv−
tb.
If we are comfortable with the approximation implicit in the dual variables, then
we get the vector (ˆvn
tb)b∈B for free from our simplex algorithm. If we speciﬁcally
want the left or right derivative, then we can perform a numerical derivative by
perturbing each element Rn
tb by plus or minus 1 and reoptimizing. This sounds
clumsy when we get dual variables for free, but it is surprisingly fast.
Once we have computed ˆvn
tb for each b ∈B, we now have to update our value
function approximation. Remember that we have to update the value function at
time t−1 at the previous post-decision state. For this problem, attributes evolve
deterministically (e.g., AB+ blood that is 3 weeks old becomes AB+ blood that is
4 weeks old). Let bt = bM(bt−1, d) describe this evolution (here, d is the decision
to hold the blood). Let ˆvn
tbt be the attribute corresponding to blood with attribute bt
at time t, and assume that Rx,n
t−1,bt−1 was the amount of blood of type bt−1 at time
t−1. Let V
n−1
t−1,bt−1(r) be the slope corresponding to the interval r ≤Rx,n
t−1,bt−1 ≤

556
dynamic resource allocation problems
r + 1 (as we did in (14.11)). A simple update is to use
V
n
t−1,bt−1(r) =

(1 −αn−1)V
n−1
t−1,bt−1(r) + αn−1 ˆvn
tbt
if r = Rx,n
t−1,bt−1,
V
n−1
t−1,bt−1(r)
otherwise.
(14.12)
Here we are only updating the element V
n−1
t−1,bt−1(r) corresponding to r = Rx,n
t−1,bt−1.
We have found that it is much better to update a range of slopes, say in the interval
(r −δ, r + δ), where δ has been chosen to reﬂect the range of possible values of
r. For example, it is best if δ is chosen initially so that we are updating the entire
range, after which we periodically cut it in half until it shrinks to 1 (or an interval
small enough that we are getting the precision we want in the decisions).
There is one problem with equation (14.12) that we have already seen before.
We know that the value functions are concave, which means that we should have
V
n
tb(r) ≥V
n
tb(r + 1) for all r. This might be true for V
n−1
tb (r), but it is entirely
possible that after our update, it is no longer true for V
n
tb(r). Nothing new here.
We just have to apply the ﬁx-up techniques that were presented in Section 13.3.
The use of separable piecewise-linear approximations has proved effective in a
number of applications, but there are open theoretical questions (how good is the
approximation?) as well as unresolved computational issues (what is the best way
to discretize functions?). What about the use of low-dimensional basis functions?
If we use a continuously differentiable approximation (which requires the use of a
nonlinear programming algorithm), we can use our regression techniques to ﬁt the
parameters of a statistical model that is continuous in the resource variable.
The best value function approximation does not just depend on the structure of
the problem. It depends on the nature of the data.
14.2.4
Extensions
We can anticipate several ways in which we can make the problem richer:
• Instead of modeling the problem in single week increments, model daily deci-
sions.
• Include the presence of blood that has been frozen, and the decision to freeze
blood.
• A hospital might require weekly deliveries of blood from a community blood
bank to make up for systematic shortages. Imagine that a ﬁxed quantity (e.g.,
100 units) of blood arrives each week, but the amount of blood of each type
and age (the blood may have already been held in inventory for several weeks)
might be random.
• We presented a model that focused only on blood inventories at a single
hospital. We can handle multiple hospitals and distribution centers by simply
adding a location attribute, and providing for a decision to move blood (at a
cost) from one location to another.
This model can also be applied to any multiproduct inventory problem where there
are different types of product and different types of demands, as long as we have

a portfolio optimization problem
557
the ability to choose which type of product is assigned to each type of demand.
We also assume that products are not reusable; once the product is assigned to a
demand, it is lost from the system.
14.3
A PORTFOLIO OPTIMIZATION PROBLEM
A somewhat different type of resource allocation problem arises in the design of
ﬁnancial portfolios. We can start with some of the same notation we used to manage
blood. Let r be an asset class (more generally, the attributes of an asset, such as the
type of investment and how long it has been in the investment), where Rtr is how
much we have invested in asset class r at time t. Let D be our set of decisions,
where each element of D represents a decision to invest in a particular asset class
(there is a one-to-one relationship between D and the asset classes R).
The resource transition functions are somewhat different. We let the indicator
function δr′(r, d) be deﬁned as before. However, now we have to account for
transaction costs. Let
crd = cost of acting on asset class r with a decision of type d,
expressed as a fraction of the assets being transferred.
The decision d might mean “hold the asset,” in which case crd = 0. However, if
we are moving from one asset class to another, we have to pay crdxrd to cover the
cost of the transaction, which is then deducted from the proceeds. This means that
the post-decision resource vector is given by
Rx
tr′ =

r∈R

d∈D
δr′(r, d)(xtrd −crdxtrd).
(14.13)
We now have to account for the market returns from changes in the value of the
asset. This is handled by ﬁrst deﬁning
ˆρtr = relative return on assets of type r ∈R between t −1 and t.
These returns are the only source of noise that we are considering at this point.
Unlike the demands for the blood problem (where we tend to assume that demands
are independent), returns tend to be correlated both across assets, and possibly
across time. Since we work only with sample paths, we have no trouble handling
complex interactions. With these data, our resource transition from Rx
t to Rt+1 is
given by
Rt+1,r = ˆρt+1,rRx
tr.
(14.14)
Normally there is one “riskless” asset class to which we can turn to ﬁnd nominal,
but safe, returns.
Finally, we have to measure how well we are doing. We could simply measure
our total wealth, given by
Rt =

r∈R
Rtr.

558
dynamic resource allocation problems
We might then set up the problem to maximize our total wealth (actually the total
expected wealth) at the end of our planning horizon. This objective ignores the fact
that we tend to be risk averse, and we want to avoid signiﬁcant losses in each time
period. So we propose instead to use a utility function that measures the relative
gain in our portfolio, such as
U(Rt) = −

Rt
Rt−1
−β
,
where β is a parameter that controls the degree of risk aversion.
Regardless of how the utility function is constructed, we approach its solution
just as we did with the blood management problem. We again write our objective
function as
˜Vt(Rt) = max
xt∈X nt

U(Rt) + γ V
n−1
t
(Rx
t )

,
(14.15)
where X t is the set of xt that satisﬁes

d∈D
xtrd = Rtr,
(14.16)
xtrd ≥0.
(14.17)
Rx
t is given by equation (14.13) (which is a function of xt), whileRt is a constant
at time t (it is determined by Rx
t−1 and ˆρt). This allows us to write (14.15) as
˜Vt(Rt) = U(Rt) + Ft(Rt),
(14.18)
where
Ft(Rt) = max
xt ∈X nt

γ V
n−1
t
(Rx
t )

.
Let ˆvn
tr be the dual variable of the ﬂow conservation constraint (14.16). We can
use ˆvn
tr as an estimate of the derivative of Ft(Rt), giving us
∂Ft(Rt)
∂Rtr
....
Rnt =Rt
= ˆvn
tr.
(14.19)
Differentiating U(Rt) gives
∂U(Rt)
∂Rtr
=
β
Rt−1

Rt
Rt−1
−β−1
,
(14.20)
where we used the fact that ∂Rt/∂Rtr = 1. We next apply the chain rule to obtain
∂˜Vt(Rt)
∂Rx
t−1,r
=

r′∈R
∂˜Vt(Rt)
∂Rtr′
∂Rtr′
∂Rx
t−1,r
.
(14.21)

a portfolio optimization problem
559
0
5
10
15
20
25
30
35
0
0.2
0.4
0.6
0.8
1
1.2
1.4
R x
t
Derivative
Figure 14.6
Sample realizations for derivatives of the objective function as a function of Rx
tr for a
particular asset class a. Vertical bars show the average within a range.
Finally, we observe that
∂Rtr′
∂Rx
t−1,r
=

ˆρtr
if r′ = r,
0
otherwise.
(14.22)
Combining (14.18) through (14.22) gives us
∂˜Vt(Rt)
∂Rx
t−1,r
=

∂U(Rt)
∂Rtr
.....
Rt=Rnt
+ ˆvn
tr

ˆρtr.
(14.23)
Figure 14.6 shows sample realizations of the derivatives computed in (14.23) for
a problem using the utility function U(Rt) = −0.1(Rt/Rt−1)−4. We would use these
sample realizations to estimate a nonlinear function of Rx
t−1,r for each resource
class r. We expect the derivatives to decline, on average, with Rx
t−1,r, which is a
pattern we see in the ﬁgure. However, it is apparent that there is quite a bit of
noise in this relationship.
Now that we have our derivatives in hand, we can use it to ﬁt a value function
approximation. Again, we face a number of strategies. One is to use the same
piecewise-linear approximations that we implemented for our blood distribution
problem, which has the advantage of being independent of the underlying distri-
butions. However, a low-order polynomial would also be expected to work quite
well, and we might even be able to estimate a nonseparable one (e.g., we might
have terms such as Rx
t−1,r1Rx
t−1,r2).
Identifying and testing different approximation strategies is one of the most
signiﬁcant challenges of approximate dynamic programming. In the case of the
portfolio problem there is a considerable amount of research into different objective
functions and the properties of these functions. For example, a popular class of

560
dynamic resource allocation problems
models minimizes a quadratic function of the variance, which suggests a particular
quadratic form for the value function. Even if these functions are not perfect, they
would represent a good starting point.
As with the blood management problem, we can anticipate several extensions
we could make to our basic model:
• Short-term bonds. Our assets could have a ﬁxed maturity, which means that
we would have to capture how long we have held the bond.
• Sales loads that depend on how long we have held an asset. An asset might be
a mutual fund, and some mutual funds impose different loads (sales charges)
depending on how long you have held the asset before selling.
• Transaction times. Our model already handles transaction costs. Some equities
(e.g., real estate) also incur signiﬁcant transaction times. We can handle this
by including an attribute indicating that the asset is in the process of being
sold, as well as how much time has elapsed since the transaction was initiated
(or the time until the transaction is expected to be completed).
• Assets that might represent stocks in different industries. We could design value
functions that capture not only the amount we have invested in each stock, but
also in each industry (we just aggregate the stocks into industry groupings).
The ﬁrst three of these extensions can be handled by introducing an attribute that
measures age in some way. In the case of ﬁxed maturities, or the time until the
asset can be sold without incurring a sales load, we can introduce an attribute that
speciﬁes the time at which the status of the asset changes.
14.4
A GENERAL RESOURCE ALLOCATION PROBLEM
The preceding sections have described different instances of resource allocation
problems. While it is important to keep the context of an application in mind
(every problem has unique features), it is useful to begin working with a general
model for dynamic resource allocation so that we do not have to keep repeating
the same notation. This problem class allows us to describe a basic algorithmic
strategy that can be used as a starting point for more specialized problems.
14.4.1
A Basic Model
The simplest model of a resource allocation problem involves a “resource” that
we are managing (people, equipment, blood, money) to serve “demands” (tasks,
customers, jobs). We describe the resources and demands using
Rtr = Number of resources with attribute r ∈R in the system at time t.
Rt = (Rtr)r∈R.
Dtb = Number of demands of type b ∈B in the system at time t.
Dt = (Dtb)b∈B.

a general resource allocation problem
561
Both r and b are vectors of attributes of resources and demands. In addition to the
resources and demands, we sometimes have to model a set of parameters (the price
of a stock, the probability of an equipment failure, the price of oil) that govern
how the system evolves over time. We model these parameters using
ρt = a generic vector of parameters that affects the behavior of costs
and the transition function.
The state of our system is given by
St = (Rt, Dt, ρt).
New information is represented as exogenous changes to the resource and demand
vectors, as well as to other parameters that govern the problem. These are modeled
using
ˆRtr = Exogenous changes to Rtr from information that arrives during time
interval t (between t −1 and t).
ˆDtb = Exogenous changes to Dtr from information that arrives during time
interval t (between t −1 and t).
ˆρt = Exogenous changes to a vector of parameters (costs, parameters
governing the transition).
Our information process then is given by
Wt = ( ˆRt, ˆDt, ˆρt).
In the blood management problem, ˆRt included blood donations. In a model of
complex equipment such as aircraft or locomotives, ˆRt would also capture equip-
ment failures or delays. In a product inventory setting, ˆRt could represent theft
of product. ˆDt usually represents new customer demands, but can also represent
changes to an existing demand or cancellations of orders. ˆρt could represent random
costs, or the random returns in our stock portfolio.
Decisions are modeled using
DD = Decision to satisfy a demand with attribute b (each decision d
∈DD corresponds to a demand attribute bd ∈B).
DM = Decision to modify a resource (each decision d ∈DM has the effect
of modifying the attributes of the resource). which includes the
decision to “do nothing.”
D = DD ∪DM.
xtrd = Number of resources that initially have attribute r that we act on
with a decision of type d.
xt = (xtrd)r∈R,d∈D.

562
dynamic resource allocation problems
The decisions have to satisfy constraints such as

d∈D
xtrd = Rtr,
(14.24)

r∈R
xtrd ≤Dtbd ,
d ∈DD,
(14.25)
xtrd ≥0.
(14.26)
We let X t be the set of xt that satisfy (14.24) through (14.26). As before, we
assume that decisions are determined by a class of decision functions
Xπ
t (St) = Function that returns a decision vector xt ∈X t, where π ∈
is an element of the set of functions (policies) .
The transition function is given generically by
St+1 = SM(St, xt, Wt+1).
We now have to deal with each dimension of our state variable. The most difﬁcult,
not surprisingly, is the resource vector Rt. This is handled primarily through the
attribute transition function
rx
t = rM,x(rt, dt),
where rx
t is the post-decision attribute (the attribute produced by action of type
d before any new information has become available). For algebraic purposes we
deﬁne the indicator function
δr′(r, d) =

1
if r′ = rx
t = rM,x(rt, dt),
0
otherwise.
Using matrix notation, we can write the post-decision resource vector Rx
t as
Rx
t = Rt,
where  is a matrix in which δr′(r, d) is the element in row r′ and column (r, d).
We emphasize that the function δr′(r, d) and matrix  are used purely for notational
convenience; in a real implementation we work purely with the transition function
rM,x(rt, dt). The pre-decision resource state vector is given by
Rt+1 = Rx
t + ˆRt+1.
We model demands in a simple way. If a resource is assigned to a demand, then
it is “served” and vanishes from the system. Otherwise, it is held to the next time

a general resource allocation problem
563
period. Let
δDtbd = number of demands of type bd that are served at time t,
=

r∈R
xtrd d ∈DD,
δDt = (δDtb)b∈B.
The demand transition function can be written
Dx
t = Dt −δDt,
Dt+1 = Dx
t + ˆDt.
Finally, we are going to assume that our parameters evolve in a purely exogenous
manner such as
ρt+1 = ρt + ˆρt+1.
We can assume any structure (additive, rule-based). The point is that ρt evolves
purely exogenously.
The last dimension of our model is the objective function. For our resource
allocation problem, we deﬁne a contribution for each decision given by
crd = Contribution earned (negative if it is a cost) from using action
d acting on resources with attribute r.
The contribution function for time period t is assumed to be linear, given by
C(St, xt) =

r∈R

d∈D
crdxtrd.
The objective function is now given by
max
π∈ E
 T

t=0
C(St, Xπ
t (St))

.
14.4.2
Approximation Strategies
The sections on blood management and portfolio optimization both follow our
general strategy of solving problems of the form
Xπ
t (Sn
t ) = arg max
xt∈X nt

Ct(Sn
t , xt) + γV
n−1
t
(Sx
t )

,
(14.27)
where Sn
t is our state at time t, iteration n, and V
n−1
t
(Sx
t ) is our value function
approximation computed in iteration n−1, evaluated at the post-decision state
Sx
t = SM,x(Sn
t , xt). In the previous sections our state variable took the form

564
dynamic resource allocation problems
St = (Rt, Dt, ρt) (for the portfolio problem, ρt captured market returns), and we
used a value function approximation of the form
Vt(St) ≈

r∈R
V tr(Rx
tr),
where Rx
tr is the (post-decision) number of assets with attribute r. A particularly
powerful approximation strategy takes advantage of the fact that when we solve
(14.27), we can easily obtain gradients of the objective function directly from
the dual variables of the resource conservation constraints (contained in X n
t ) which
state that 
d∈D xtrd = Rtr. If we do not feel that the duals are sufﬁciently accurate,
we may compute numerical derivatives fairly easily. Section 14.2.3 shows how we
can use these derivatives to estimate piecewise-linear approximations.
Linear Value Function Approximation
The simplest approximation strategy outside of a myopic policy (equivalent to
V t(Rt) = 0), is one that is linear in the resource state, as in
V t(Rt) =

r′∈R
V tr′Rtr′.
With a linear value function approximation the decision function becomes unusually
simple. We start by writing Rx
tr using
Rx
tr′ =

r∈R

d∈D
δr′(r, d)xtrd.
(14.28)
Here a decision d ∈D represents acting on a resource, which might involve serving
a demand or simply modifying the resource. Substituting this in our linear value
function approximation gives us
V t(Rx
t ) =

r′∈R
V tr′

r∈R

d∈D
δr′(r, d)xtrd.
(14.29)
The decision function is now given by
Xπ
t (St) = arg max
xt∈X t

r∈R

d∈D
crdxtrd +

r′∈R
V tr′

r∈R

d∈D
δr′(r, d)xtrd

= arg max
xt∈X t(ω)

r∈R

d∈D

crd +

r′∈R
V tr′δr′(r, d)

xtrd.
(14.30)
Recognizing that 
r′∈R δr′(r, d) = δrM(rt,dt)(r, d) = 1, we can write (14.30) as
Xπ
t (St) = arg max
xt∈X t(ω)

r∈R

d∈D

crd + γ V
n−1
t,rM(r,d)

xtrd.
(14.31)

a general resource allocation problem
565
So this is nothing more than a network problem where we are assigning resources
with attribute r to demands (for decisions d ∈DD) or we are simply modifying
resources (d ∈DM). If we were using a myopic policy, we would just use the
contribution crd to determine which decisions we should make now. When we use
a linear value function approximation, we add in the value of the resource after the
decision is completed, given by V
n−1
t,rM(r,d).
Solving Xπ(St) is a linear program, and therefore returns a dual variable ˆvtr for
each ﬂow conservation constraint. We can use these dual variables to update our
linear approximation as
V
n
tr = (1 −αn−1)V
n−1
tr
+ αn−1 ˆvn
tr.
Linear (in the resource state) approximations are especially easy to develop and
use, but they do not always provide good results. They are particularly useful
when managing complex assets such as people, locomotives, or aircraft. For such
problems the attribute space is quite large, and as a result Rtr tends to be small
(e.g., 0 or 1). In general, a linear approximation will work well if the size of the
attribute space is much larger than the number of discrete resources being managed,
although even these problems can have pockets where there are a lot of resources
of the same type.
Linear value function approximations are particularly easy to solve. With a linear
value function approximation we are solving network problems with the structure
shown in Figure 14.7a, which can be easily transformed to the equivalent network
shown in Figure 14.7b where all we have done is to take the slope of the linear
value function for each downstream resource type (e.g., the slope v1 for resources
that have attribute r′
1 in the future) and add this value to the arc assigning the
resource to the demand. So, if the cost of assigning resources with attribute r1 to
the demand has a cost of c1, then we would use a modiﬁed cost of c1 + v1.
Piecewise-Linear Separable Approximation
We solved both the blood management and the portfolio allocation problems using
separable piecewise-linear value function approximations. This technique produces
results that are more stable when the value function is truly nonlinear in the amount
of resources that are provided.
The algorithm is depicted in Figure 14.8. At each time period we solve a linear
program similar to what we used in the blood management problem (Figure 14.3),
using separable piecewise-linear approximations of the value of resources in the
future. The process steps forward in time, using value function approximations
for assets in the future. The dual variables from each linear program can be used
to update the value function for the previous time period, using the techniques
described in Section 13.3.
A critical assumption in this algorithm is that we obtain ˆvtr for all r ∈R. If
we were managing a single asset, this would be equivalent to sampling the value
of being in all possible states at every iteration. In Chapter 4, we referred to this
strategy as synchronous approximate dynamic programming (see Section 4.9.2).

566
dynamic resource allocation problems
(a) A multicommodity flow subproblem with linear value functions
r1
r2
r3
r1
,
,r2
,r3
v1
v2
v3
c1
c2
c3
r1
r2
r3
r1
,
,r2
,r3
c1 + v1
c2 + v2
c3 + v3
(b) Conversion to an equivalent problem that is a pure network
Figure 14.7
If linear value functions are used (a) the problem can be converted into an equivalent
pure network (b).
Below (in Section 14.6) we describe an application where this is not possible, and
we also describe strategies for overcoming this problem.
14.4.3
Extensions and Variations
Piecewise-linear separable approximations have worked quite well for a number
of applications, but it is important to realize that this is just one of many potential
approximations that could be used. The power of piecewise-linear functions is that
they are extremely ﬂexible and can be used to handle virtually any distribution
for demands (or other parameters). But separable approximations will not work for
every problem.
A popular approximation strategy is to use simple polynomials such as
V t(St) ≈

r∈R

r′∈R
θrr′RtrRtr′.
This is a linear-in-the-parameters regression problem that can be estimated using
the techniques of Section 9.3 for recursive least squares. This approximation can be
a challenge if the set R is fairly large. Neural networks (Section 8.4.4) offer another
avenue of investigation that, as of this writing, has not received any attention for
this problem class.

a general resource allocation problem
567
t
(a) The approximate linear program for time t
(b) The approximate linear program for time t + 1
t + 1
t + 2
(c) The approximate linear program for time t + 2.
Figure 14.8
ADP for general multistage resource allocation problems using separable value function
approximations.

568
dynamic resource allocation problems
We can retain the use of separable piecewise-linear approximations but par-
tially overcome the separability approximation by writing the value function on
an aggregated state space. For example, there may be subsets of resources which
substitute fairly easily (e.g., people or equipment located nearby). We can aggre-
gate the attribute space R into an aggregated attribute space R(g), where rg ∈R(g)
represents an aggregated attribute. We can then write
Vt(St) ≈

rg∈R(g)
V trg(Rx
trg),
where Rx
trg is the number of resources with aggregated attribute rg. When we use
aggregated value functions, we may have several gradients ˆvn
a, giving the derivative
with respect to Rtr, which need to be combined in some way to update a single
aggregated value function V trg(Rx
trg).
We may also consider using the techniques of Section 8.1.4 and multiple levels
of aggregation, which would produce an approximation that looks like
Vt(St) ≈

g∈G

rg∈R(g)
θ(g)
rg V trg(Rx
trg),
where G is a family of aggregations and θ (g)
rg is the weight we give to the function
for attribute rg for aggregation level g.
Finally, we should not forget that we can use a linear or piecewise-linear function
of one variable such as Rtr that is indexed by one or more other variables. For
example, we might feel that the value of resources of type r1 are inﬂuenced by the
number of resources of type r2. We can estimate a piecewise-linear function of Rtr1
that is indexed by Rtr2, producing a family of functions as illustrated in Figure 14.9.
This is a powerful strategy in that it does not assume any speciﬁc functional
relationship between Rtr1 and Rtr2, but the price is fairly steep if we want to have
the value function indexed by more than just one additional element such as Rtr2.
We might decide that there are 5 or 10 elements that might affect the behavior
Rtr1
Rtr2
V(Rtr1 ; Rtr2)
Figure 14.9
Family of piecewise-linear value function approximations.

a general resource allocation problem
569
of the function with respect to Rtr1. Instead of estimating one piecewise-linear
value function approximation for each Rtr1, we might ﬁnd ourselves estimating
thousands, so there is a limit to this strategy.
A powerful idea which has been suggested (but we have not seen it tested in
this setting) is the use of ridge regression. Create a scalar feature resource variable
given by
Rtf =

r∈R
θf rRtr,
f ∈F.
Now estimate a value function approximation
Vt(St) ≈

f ∈F
V tf (Rtf ).
This approximation still consists of a series of scalar, piecewise-linear functions, but
now we are using specially constructed variablesRtf made up of linear combinations
of Rtr.
14.4.4
Value Function Approximations and Problem Structure
Linear value functions have a number of nice features. They are easy to estimate
(one parameter per attribute). They offer nice opportunities for decomposition. If
city i and city j both send equipment to city k, a linear value function means that
the decisions of city i and city j do not interact (at least not through the value
function). Finally, it is often the case that the optimization problem is a special type
of linear program known as a pure network, which means that it can be completely
described in terms of nodes, links, and upper bounds on the ﬂows. Pure networks
have a nice property: if all the data (supplies, demands, upper bounds) are integer,
then the optimal solution (in terms of ﬂows) will also be integer. This is especially
useful if our resources are discrete (people or equipment). It means that if we solve
the problem as a linear program (ignoring the need for integer solutions), then the
optimal solution is still integer.
Unfortunately, linear value function approximations often do not work very well.
Nonlinear value function approximations can improve the quality and stability
of the solution considerably. We have already seen with our blood management and
portfolio problems that separable piecewise-linear value function approximations
can be particularly easy to estimate and use. However, if we are managing discrete
resources, then integrality becomes an issue. Recall that our decision problem looks
like
Xπ(Rt) = arg max
xt ∈X t




r∈R

d∈Dr
crdxtrd +

r∈R
V
n−1
tr
(Rx
tr(xt))


.
(14.32)
If our only constraints are the ﬂow conservation constraint (
d xtrd = Rtr),
nonnegativity, and possibly upper bounds on individual ﬂows (xtrd ≤utrd), then

570
dynamic resource allocation problems
r1
r2
r3
r’1
r’2
r’3
xtr1d + xtr2d + xtr3d ≤ 1
Figure 14.10
Different types of resources used to serve a demand and nonlinear value functions used
to capture the value of each type of resource in the future, as bundled constraints used to capture the
type of ﬂow serving the demand.
equation (14.32) is a pure network, which means that we can ignore integrality
(i.e., we can solve it as a continuous linear program) but still obtain integer
solutions. What may destroy this structure are constraints of the form

r∈R
xtrd ≤RD
tbd ,
d ∈DD.
Constraints such as this are known as bundle constraints, since they bundle different
decisions together (“we can use resources of type 1 or 2 or 3 to cover this demand”).
The problem is illustrated in Figure 14.10. When we introduce bundle constraints,
we are no longer able to solve the problem as a continuous linear program and
count on getting integer solutions. We have to use instead an integer programming
algorithm, which can be much slower (it depends on the structure of the problem).
We avoid this situation if our problem exhibits what we might refer to as the
Markov property, which is deﬁned as follows:
Deﬁnition 14.4.1
We say that a resource allocation problem has the Markov prop-
erty if
rM(r, d, Wt+1) = rM(d, Wt+1),
which is to say, the attributes of a transition resulting from a decision a acting on
a resource with attribute r is independent of r (the transition is “memoryless”).
An example of a problem that exhibits the Markov property is the portfolio
problem. If we decide to invest money in asset class k, we do not care if the
money came from asset class i or j. Another problem with the Markov property
arises when we are managing a ﬂeet of identical vehicles, where the attribute
vector a captures only the location of the vehicle. A decision to “move the vehicle
to location j” determines the attribute of the vehicle after the decision is completed
(the vehicle is now at location j).
If a problem exhibits the Markov property, then we obtain a network shown
in Figure 14.11. If a linear program with this structure is solved using a linear

a general resource allocation problem
571
r1
r2
r3
xtr1d + xtr2d + xtr3d ≤ 1
r
Figure 14.11
Flow, after serving a demand, with an attribute that depends only on the attributes of
the demand. All the value ends in the same value function.
programming package without imposing integrality constraints, the solution will
still be integer.
If we have a problem that does not exhibit the Markov property, we can restore
this property by aggregating resources in the future. To illustrate, imagine that
we are managing different types of medical technicians to service different types
of medical equipment in a region. The technician might be characterized by his
training, his current location, and how many hours he has been working that day.
After servicing a machine at a hospital, he has changed his location and the number
of hours he has been working, but not his training. Now assume that when we
approximate the value of the technician in the future, we ignore his training and
how many hours he has been working (leaving only the location). Now his attributes
are purely a function of the decision (or equivalently, the job he has just completed).
The good news about this problem is that commercial solvers seem to eas-
ily produce optimal (or near optimal) integer solutions even when we combine
piecewise-linear separable value function approximations in the presence of bun-
dle constraints. Integer programs come in different ﬂavors, and we have found
that this problem class is one of the easy ones. However, it is important to allow
the optimization solver to return “near optimal” solutions (all codes allow you to
specify an epsilon tolerance in the solution). Obtaining provably optimal solutions
for integer programs can be extremely expensive (the code spends most of the time
verifying optimality), while allowing an epsilon tolerance can produce run times
that are almost as fast as if we ignore integrality. The reason is that if we ignore
integrality, the solution will typically be 99.99 percent integer.
14.4.5
Applications
There is a vast range of resource allocation problems that can be modeled with
this framework. We began the chapter with three examples (asset acquisition,
blood inventories, and portfolio optimization). Some examples of other applications
include:
• Inspecting passengers at airports. The Transportation Safety Administration
(TSA) assigns each arriving passenger to a risk category that determines
the level of inspection for passenger (simple X-ray and magnetic detection,
an inspector using the hand wand, more detailed interrogation). Each level

572
dynamic resource allocation problems
requires more time from selected resources (inspectors, machines). For each
arriving passenger we have to determine the right level of inspection, taking
into account future arrivals, the capacity of each inspection resource, and the
probability of detecting a real problem.
• Multiskill call centers. People phoning in for technical support for their com-
puter have to progress through a menu that provides some information about
the nature of their problem. The challenge then is to match a phone call char-
acterized by a vector of attributes a that captures the phone options, with
a technician (each of whom has different skills). Assigning a fairly simple
request to a technician with special skills might tie up that technician for what
could be a more important call in the future.
• Energy planning. A national energy model might have to decide how much
capacity for different types of energy we should have (coal, ethanol, wind
mills, hydro, oil, natural gas) at each time period to meet future demands.
Many forms of capacity have lifetimes of several decades, but these decisions
have to be made under uncertainty about commodity prices and the evolution
of different energy technologies.
• Planning ﬂu vaccines. The Center for Disease Control (CDC) has to decide
each year which ﬂu vaccine (or vaccines) should be manufactured to respond
to potential outbreaks in the upcoming ﬂu season. It can take six months to
manufacture a vaccine for a different strain, while ﬂu viruses in the population
evolve continuously.
• Fleet planning for charter jet operators. A company has to purchase different
types of jets to serve a charter business. Customers might request a particular
type of jet. If the company does not have jets of that type available, the
customer might be persuaded to upgrade to a larger jet (possibly with the
inducement of “no extra charge”). The company has to decide the proper mix
of jets, taking into account not only the uncertainty in the customer demands
but also their willingness to accept upgrades.
• Work force planning. Large companies, and the military, have to develop
the skills in their employees to meet anticipated needs. The skills may be
training in a speciﬁc activity (ﬁlling out types of forms, inspecting machin-
ery, or working in a particular industry as a consultant) or could represent
broader experiences (spending time abroad, working in accounting or market-
ing, working in the ﬁeld). It may take time to develop a skill in an employee
who previously did not have the skill. The company has to decide which
employees to develop to meet uncertain demands in the future.
• Hospital operations. There are numerous problems requiring the management
of resources for hospitals. How should different types of patients be assigned to
beds? How should nurses be assigned to different shifts? How should doctors
and interns be managed? Which patients should be scheduled for surgery and
when?
• Queueing networks. Queueing networks arise when we are allocating a
resource to serve customers who move through the system according to a

a ﬂeet management problem
573
set of exogenous rules (i.e., we are not optimizing how the customer moves
through the network). These applications often arise in manufacturing where
the customer is a job (e.g., a computer circuit board) that has to go through
a series of steps at different stations. At each station is a machine that might
perform several tasks, but can only perform one task at a time. The resource
allocation problem involves determining what type of task a machine should
be working on at a point in time. Given a set of machines (and how they are
conﬁgured), customers move from one machine to another according to ﬁxed
rules (that may depend on the state of the system).
Needless to say, the applications are nearly endless and cut across different dimen-
sions of society (corporate applications, energy, medical, homeland security). All
of these problems can be reasonably approximated using the types of techniques
we describe in this chapter.
14.5
A FLEET MANAGEMENT PROBLEM
Consider a freight transportation company that moves loads of freight in trailers
or containers. Assume that each load ﬁlls the container (we are not consolidating
small shipments into larger containers). After the company moves a load of freight
from one location to another, the container becomes empty and may have to be
repositioned empty to a new location where it is needed. Applications such as these
arise in railroads, truckload motor carriers, and intermodal container shipping.
These companies face two types of decisions (among others). (1) If there are too
many customer demands (a fairly common problem), the company has to decide
which customers to serve (i.e., which loads of freight to move) by balancing the
contribution earned by moving the load, plus the value of the container when it
becomes empty at the destination. (2) The company might have more containers
in one location than are needed, making it necessary to move containers empty to
a location where they may be needed (possibly to serve demands that are not yet
known).
A model based on the algorithmic strategies described in this chapter was
implemented at a major railroad to manage their freight cars. This section brieﬂy
describes the modeling and algorithmic strategy used for this problem.
14.5.1
Modeling and Algorithmic Strategy
This problem can be perfectly modeled using the notation in Section 14.4. The
attribute vector r will include elements such as the location of the container, the
type of container, and, because it usually takes multiple time periods for a container
to move from one location to the next, the expected time of arrival (or, the number
of time periods until it will arrive). The time to travel is usually modeled deter-
ministically, but exercise 14.2 asks you to generalize this basic model to handle
random travel times. An important characteristic of this problem is that while the

574
dynamic resource allocation problems
attribute space is not small, it is also not too large, and in fact we will depend on
our ability to estimate values for each attribute in the entire attribute space.
The decision set DD represents decisions to assign a container to a particular
type of load. The load attributes b typically include the origin and destination of
the load, but may also include attributes such as the customer (this may determine
which types of containers are allowed to be used), the type of freight, and the
service requirements.
The problem is very similar to our blood management problem, with two major
exceptions. First, in the blood problem, we could assign blood to a demand (after
which it would vanish from the system) or hold it. With the ﬂeet management
problem, we also have the option of “modifying” a car, captured by the decision
set DM. The most common “modiﬁcation” is to move a car empty from one location
to another (we are modifying the location of the car). Other forms of modiﬁcation
include cleaning a car (freight cars may be too dirty for certain types of freight),
repair a car, or change its “ownership” (cars may be assigned to shipper pools,
where they are dedicated to the needs of a particular shipper).
The second exception is that while used blood vanishes from the system, a
car usually remains in the system after it is assigned to an order. We say usually
because a railroad in the eastern United States may pick up a car that has to be
taken to the West Coast. To complete this move, the car has to be transferred to
a railroad that serves the West Coast. Although the car will eventually return to
the ﬁrst railroad, for planning purposes the car is considered lost. At a later time
it will appear as if it is a random arrival (similar to our random blood donations).
The most widely used model in practice is an optimization model that assigns
individual cars to individual orders at a point in time (Figure 14.12a). These models
do not consider the downstream impact of a decision now on the future, although
they are able to consider cars that are projected to become available in the future,
as well as forecasted orders. However, they cannot handle a situation where an
order has to be served 10 days from now which might be served with a car after it
ﬁnishes an order that has to be served three days from now. Figure 14.12b shows
the decision function when using value function approximations, where we not only
assign known cars to known orders (similar to the myopic model in Figure 14.12a),
but we also capture the value of cars in the future (through piecewise-linear value
function approximations).
Railroads that use the basic assignment model (several major North American
railroads do this) only solve them using known cars and orders. If we use approx-
imate dynamic programming, we would ﬁrst solve the problem at time t = 0 for
known cars and orders, but we would then step forward in time and use simulated
cars and orders. By simulating over a three-week horizon, ADP produces not only
a better decision at time t = 0 but also a forecast of car activities in the future.
This proved very valuable in practice.
In Section 14.4.2 we mentioned that we can use the dual variable ˆvn
tr (or a
numerical derivative) for the resource constraint

d∈D
xtrd = Rtr

a ﬂeet management problem
575
(a) Basic car-to-order assignment problem
Cars
Orders
(b) Car assignment problem using ADP
Cars
Orders
Assignments to 
booked orders
Repositioning
movements based 
on forecasts
Figure 14.12
(a) Basic assignment model for assigning cars to orders used in engineering practice.
(b) Model that uses piecewise-linear separable value function approximations estimated by approximate
dynamic programming.

576
dynamic resource allocation problems
to update the value function approximation V
n−1
t−1,r(Rx
t−1,r). When the attribute
space is not too large (something that we assume for this problem class), we
can assume that we are calculating ˆvn
tr for each r ∈R. It turns out that this is
an extremely powerful capability, and not one that we can do for more complex
problems where the attribute space is too large to enumerate.
14.5.2
Laboratory Experiments
There are two types of laboratory experiments that help provide insights into the
quality of an ADP solution: (1) those performed on a deterministic dataset and (2)
those on a stochastic dataset. Deterministic experiments are useful because these
are problems that can be solved to optimality, providing a tight bound to compare
against. Stochastic datasets allow us to evaluate how well the method performs
under uncertainty, but we have to compare against heuristic solutions since tight
bounds are not available.
Deterministic Experiments
There are many variations of the ﬂeet management problem. In the simplest, we
have a single type of container, and we might even assume that the travel time
from one location to another is a single time period (in this case the attribute
vector r consists purely of location). We assume that there is no randomness in the
resource transition function (which means that ˆRt = 0), and that if a demand is not
satisﬁed at time t, it is lost from the system (which means that Dt = ˆDt). We may
additionally assume that a demand originating from location i can only be served
by containers that are already located at location i. In Section 14.6 below we relax
a number of these assumptions.
If the problem is deterministic (which is to say, all the demands ˆDt are known in
advance), then the problem can be modeled as a network such as the one shown in
Figure 14.13. This problem is easily solved as a linear program. Since the resulting
linear program has the structure of what is known as a pure network, it is also the
case that if the ﬂows (and upper bounds) are integer, then the resulting solution is
integer.
We can solve the problem using approximate dynamic programming for a deter-
ministic application by pretending that ˆDt is stochastic (i.e., we are not allowed to
use ˆDt+1 when we solve the problem at time t), but where every time we “observe”
ˆDt we obtain the same realization. Using approximate dynamic programming, our
decision function would be given by
Xπ
t (St) = arg max
xt∈Xt

C(St, xt) + V
n−1
t
(Rx
t )

,
where we use a separable, piecewise linear value function approximation (as we
did in the previous sections).
Experiments (reported in Powell and Godfrey, 2002) were run on problems with
20, 40, and 80 locations and with 15, 30, and 60 time periods. Since the problem
is deterministic, we can optimize the entire problem (i.e., over all time periods) as

a ﬂeet management problem
577
Time
Space
Figure 14.13
Pure network for time-staged, single commodity ﬂow problems.
Table 14.3
Percentage of the optimal deterministic solution produced
by separable piecewise-linear value function approximations
Simulation Horizon
Locations
15
30
60
20
100.00%
100.00%
100.00%
40
100.00%
99.99%
100.00%
80
99.99%
100.00%
99.99%
Source: From Powell and Godfrey (2002).
a single linear program to obtain the true optimal solution, allowing us to see how
well our approximate solution compares to optimal (on a deterministic problem).
The results are reported in Table 14.3 as percentages of the optimal solution pro-
duced by the linear programming algorithm. It is not hard to see that the results
are very near optimal. We know that separable piecewise-linear approximations do
not produce provably optimal solutions (even in the limit) for this problem class,
but it appears that the error is extremely small.
Stochastic Experiments
We now consider what happens when the demands are truly stochastic, which is
to say that we obtain different values for ˆDt each time we sample information.
For this problem we do not have an optimal solution. Although this problem is
relatively small (compared to true industrial applications), if we formulate it as a
Markov decision process we would produce a state space that is far larger than
anything we could hope to solve using the techniques of Chapter 3. The standard
approach used in engineering (which we have also found works quite well) is to
use a rolling horizon procedure where at each time t we combine the demands
that are known at time t with expectations of any random demands for future time

578
dynamic resource allocation problems
50
60
70
80
90
100
20/100 20/200 20/400 40/100 40/200 40/400 80/100 80/200 80/400
Percent of posterior optimal
Approximate dynamic programming
Dataset (locations/fleet size)
Rolling horizon procedure
Figure 14.14
Percentage of posterior bound produced by a rolling horizon procedure using a point
forecast of the future versus an approximate dynamic programming approximation.
periods. A deterministic problem is then solved over a planning horizon of length
T ph that typically has to be chosen experimentally.
For a speciﬁc sample realization of the demands we can still ﬁnd an optimal
solution using a linear programming solver, but this solution “cheats” by being able
to use information about what is happening in the future. This solution is known
as the posterior bound since it uses information that only becomes known after
the fact. Although it cheats, it provides a nice benchmark against which we can
compare our solutions from the ADP algorithm.
Figure 14.14 compares the results produced by a rolling horizon procedure
to those produced using approximate dynamic programming (with separable
piecewise-linear value function approximations). All results are shown as a
percentage of the posterior bound. The experiments were run on problems with 20,
40, and 80 locations and with 100, 200, and 400 vehicles in our ﬂeet. Problems
with 100 vehicles were able to cover roughly half of the demands, while a ﬂeet
of 200 vehicles could cover over 90 percent. The ﬂeet of 400 vehicles was much
larger than would have been necessary. The ADP approximation produced better
results across all the problems, although the difference was most noticeable for
problems where the ﬂeet size was not large enough to cover all the demands. Not
surprisingly, this was also the problem class where the posterior solution was
relatively the best.
These experiments demonstrate that piecewise-linear separable value function
approximations work quite well for this problem class. However, as with all
experimental evidence, care has to be used when generalizing these results to
other datasets. This technique has been used in several industrial applications with

a ﬂeet management problem
579
success, but without provable bounds it will be necessary to reevaluate the approx-
imation for different problem classes. We have found, however, that the technique
scales quite well to very large scale problems.
14.5.3
A Case Implementation
This strategy was implemented in an industrial application for managing freight cars
for a major railroad. Two aspects of the problem proved to be much more complex
than anticipated. First, in addition to the usual attributes of location and type of
freight car, we also had to consider estimated time of arrival (for cars that are
currently moving from one location to another), cleanliness (some shippers reject
dirty cars), repair status, and ownership. The second aspect was the complexity of
the information process. Random demands turned out to be only one of the sources
of uncertainty. When a customer calls in an order, he will specify the origin of
the order (this is where we send the car to), but not the destination (this is only
revealed after the car is loaded). Also a customer might reject a car as being too
dirty only after it is delivered to the customer. Finally, the time to load and unload
the car, as well as transit times, are all uncertain. None of these issues presented a
signiﬁcant modeling or algorithmic challenge.
Figure 14.12 described two models: (1) a basic car-to-order assignment problem
widely used in industry and (2) a model which uses approximate dynamic program-
ming to measure the value of cars in the future. So this raises the question: How
do these two methods compare? Figure 14.15 compares the performance of the
0%
10%
20%
30%
40%
50%
60%
1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77 81 85 89 93 97
Empty miles as a percent of total miles
History
Basic optimization model (engineering practice)
Using approximate dynamic programming
Figure 14.15
Empty miles as a percentage of total from an industrial application, showing historical
performance (54 percent of total miles were empty), results using a myopic assignment model and when
using approximate dynamic programming.

580
dynamic resource allocation problems
myopic model (Figure 14.12a) to the model using approximate dynamic program-
ming (Figure 14.12b) against historical performance, with empty miles treated as a
percentage of total as the performance measure. For this dataset, 54 percent of the
total mileage generated by the cars were spent moving empty (a surprising statis-
tic). In the simple assignment model (which we simulated forward in time) this
number dropped to about 48 percent empty, which is a signiﬁcant improvement.
Using approximate dynamic programming (where we show the performance after
each iteration of the algorithm), we were able to drop empty miles to approximately
35 percent of the total.
14.6
A DRIVER MANAGEMENT PROBLEM
In the previous section we managed a set of “containers” that were described
by a fairly simple set of attributes. For this problem we assumed that we could
compute gradients ˆvn
tr for each attribute r ∈R. We lose this ability when we make
the transition from managing simple resources such as a container to complex
resources such as drivers.
We use as our motivating application the problem of managing a ﬂeet of drivers
for a truckload motor carrier. The mathematical model is identical to what we
used for managing containers (which is the model we presented in Section 14.4).
The only difference now is that the attribute vector r is large enough that we can
no longer enumerate the attribute space R. A second difference we are going to
introduce is that if we do not handle a demand (to move a load of freight) at time
t, then the demand is held over to the next time period. This means that if we
cannot handle all the demands, we need to think about which demands we want to
satisfy ﬁrst.
This summary is based on an actual industrial project that required modeling
drivers (and loads) at a very high level of detail. The company ran a ﬂeet of over
10,000 drivers, and our challenge was to design a model that closely matched
actual historical performance. It turns out that experienced dispatchers do a fairly
good job of thinking about the downstream impact of decisions. For example, a
dispatcher might want to put a team of two drivers (which are normally assigned
to a long load, which takes advantage of the ability of a team to move constantly)
on a load going to Boston, but it might be the case that most of the loads out of
Boston are quite short. It might be preferable to put the team on a load going to
Atlanta if there are more long loads going out of Atlanta.
14.6.1
Working with Complex Attributes
When we were managing simple resources (“containers”), we could expect that
the resource state variable Rtr would take on integer values greater than 1 (in
some applications in transportation, they could number in the hundreds). When the
attribute vector becomes complex, then it is generally going to be the case (with
rare exception) that Rtr is going to be zero (most of the time) or 1.

a driver management problem
581
In an actual project with a truckload motor carrier the attribute vector was
given by
r =


r1
r2
r3
r4
r5
r6
r7
r8
r9
r10


=


Location
Domicile
Capacity type
Scheduled time at home
Days away from home
Available time
Geographical constraints
DOT road hours
DOT duty hours
Eight-day duty hours


.
The presence of complex attributes introduces one nice simpliﬁcation: we can
use linear (in the resource state) value function approximations (rather than the
piecewise-linear functions). This means that our value function approximation can
be written
V t(Rx
t ) =

r∈R
V trRx
tr.
Instead of estimating a piecewise-linear function, we have only to estimate V tr for
each r.
Now we have to update the value of a driver with attribute r. In theory, we would
again compute the dual variable ˆvn
tr for the resource constraint 
d∈D xtrd = Rtr
for each r. This is where we run into problems. The attribute space can be huge,
so we can only ﬁnd ˆvn
tr for some of the attributes. A common strategy is to ﬁnd
a dual only for the attributes where Rtr > 0. Even for a very large problem (the
largest trucking companies have thousands of drivers), we have no problem solving
linear programs with thousands of rows. But this means that we are not obtaining
ˆvn
tr when Rtr = 0.
This issue is very similar to the classical problem in ADP when we do not
estimate the value of states we do not visit. Imagine if we were managing a single
driver. In this case rt would be the “state” of our driver at time t. Assuming that
we get ˆvtr for each r is like assuming that we visit every state at every iteration.
This is ﬁne for small state spaces (i.e., resources with simple attributes) but causes
a problem when the attributes become complicated.
Fortunately, we already have the tools to solve this problem. In Section 8.1.4
we saw that we could estimate V tr at different levels of aggregation. Let R(g) be
the attribute space at the gth level of aggregation, and let V
g
tr be an estimate of
a driver with attribute r ∈R(g). We can then estimate the value of a driver with
(disaggregate) attribute r using
V tr =

g∈G
w(g)
r V
(g)
tr .
Section 8.1.4 provides a simple way of estimating the weights w(g)
r .

582
dynamic resource allocation problems
14.6.2
Backlogging Demands
We next address the problem of deciding which demand to serve when it is possible
to hold unserved demands to future time periods. Without backlogging, the post-
decision state variable was given by Sx
t = (Rx
t ). Equation (14.28) determines Rx
t
as a function of Rt and xt (the resource transition function). With backlogging, the
post-decision state is given by Sx
t = (Rx
t , Dx
t ), where Dx
t = (Dx
tb)b∈B is the vector
of demands that were not served at time t. Dx
t is given simply using
Dx
tb = number of loads with attribute vector b that have not been served
after decisions were made at time t,
= Dtb −

r∈R
xtrd, where b = bd, d ∈DD.
(14.33)
In equation (14.33), for a decision d ∈DD to move a load, each element in DD
references an element in the set of load attributes B. So, for d ∈DD, 
r∈R xtrd is
the number of loads of type bd that were moved at time t.
If we use a linear value function approximation, we would write
V t(Rt) =

r∈R
V
R
trRx
tr +

b∈B
V
D
tbDx
tb.
As before, V
R
tr is the value of a driver with attribute r, while V
D
tb is the value of
holding an order with attribute b. Just as V
R
tr is estimated using the dual variable
of the resource constraint

d∈D
xtrd = Rtr,
we would estimate the value of a load in the future, V
D
tb using the dual variable
for the demand constraint

r∈R
xtrd ≤Dtbd ,
d ∈DD.
Recall that each element in the set DD corresponds to a type of demand B. We
refer to V
R
t as resource gradients and V
D
t as task gradients.
Whenever we propose a value function approximation, we have to ask the fol-
lowing questions: (1) Can we solve the resulting decision function? (2) Can we
design an effective updating strategy (to estimate V )? (3) Does the approximation
work well (does it provide high-quality decisions)?
For our problem the decision function is still a linear program. The value func-
tions can be updated using dual variables (or numerical derivatives) just as we did
for the single-layer problem. So the only remaining question is whether it works
well. Figure 14.16 shows the results of a simulation of a driver management prob-
lem where we make decisions using a purely myopic policy (V
R = V
D = 0), a

a driver management problem
583
60
65
70
75
80
85
90
95
100
50
55
60
65
70
75
80
85
90
95
100
Data Set Size
Percent of posterior optimal
Myopic policy
Resource gradients
Resource and task gradients
Figure 14.16
Value of including value functions for resources alone, and resource and tasks, compared
to a myopic policy (from Spivey and Powell, 2004).
policy that ignored the value of demands (tasks) in the future (V
D = 0), and a
policy that used value functions for both resources and demands. Simulations were
run for a number of datasets of increasing size. All of the runs are evaluated as a
percentage of the posterior solution (which we can solve optimally by assuming we
knew everything that happened in the future). For these datasets the myopic pol-
icy produced an average performance that was 84 percent of the posterior bound.
Using value functions for just the ﬁrst resource layer produced a performance
of almost 88 percent, while including value functions for both resource layers
produced solutions that were 92.6 percent of the posterior optimal.
14.6.3
Extensions
This basic model is capable of handling a number of extensions and generalizations
with minor modiﬁcations.
Continuous Actionable Times versus Discrete Decision Epochs
Perhaps one of the most powerful features of approximate dynamic programming is
its ability to model activities using a very detailed state variable while maintaining a
computationally compact value function approximation. One dimension where this
is particularly important is the modeling of time. In our transportation applications
where we are often managing thousands of resources (drivers, equipment), it is
typically necessary to solve decision subproblems in discrete time, possibly with a
fairly coarse discretization (e.g., every 6 or 12 hours). However, we can still model
the fact that real activities happen in continuous time.
A common mistake in modeling is to assume that if we make decisions in
discrete time, then every activity has to be modeled using the same level of dis-
cretization. This could not be farther from the truth. We can introduce in our
attribute vector r an attribute called the actionable time, which is the time at

584
dynamic resource allocation problems
which we can actually change the status of a resource (we can do the same for the
demands, by adding an “actionable time” ﬁeld to the demand attribute vector b).
Suppose that we are making decisions at time t = 0, 100, 200, . . . (times are in
minutes). Assume that at time t = 100 we wish to consider assigning a resource
(driver) with actionable time of ractionable = 124 to a demand (load) with action-
able time bactionable = 162, and assume that it takes τ = 50 minutes for the driver
to move to the load. If we make this decision, then the earliest time that the load
can actually be picked up is at time t = 124 + 50 = 174. We can model these
activities as taking place down to the nearest minute, even though we are making
decisions at 100-minute intervals.
Relaying Loads
The simplest resource allocation problem involves a single resource layer (blood,
money, containers, people) satisfying demands. In our driver management problem
we allowed, for the ﬁrst time, unsatisﬁed demands to be held for the future, but
we still assumed that if we assign a driver to a load, then the load vanishes from
the system.
It is also possible to assume that if we act on a load with attribute b with
decision d that we produce a load with attribute b′. Previously we described the
effect of a decision d on a resource with attribute r using the attribute transition
function
rt+1 = rM(rt, d, Wt+1).
We could use a similar transition function to describe how the attributes of a load
evolve. However, we need some notational sleight of hand. When modeling the
evolution of resources, we know that the attribute transition function has access to
both the attribute of the driver, rt, and the attribute of the load, which is captured
implicitly in the decision d that tells us what type of load we are moving (the
attributes are given by bd). To know how the attributes of the load change, we can
deﬁne a load transition function
bt+1 = bM(rt, d, Wt+1).
This looks a little unusual because we include rt in the arguments but not bt. In fact
bt is determined by the action d, and we need to know both the driver attributes rt
as well as the load attributes bt = bd. Algebraically we used the function δr′(r, d)
to indicate the attribute produced by the decision d acting on the attribute r. When
we have drivers and loads, we might use δD
r′ (r, d) as the indicator function for
drivers and use δL
b′(b, d) as the indicator function for loads. All the algebra would
carry through symmetrically.
Random Travel Times
We can handle random travel times as follows. Suppose that at time t = 100, we
have a driver who is currently headed to Cleveland. We expect him to arrive at time
t = 115, at which point he will be empty (status E) and available to be reassigned.

bibliographic notes
585
Although he has not arrived yet, we make the decision to assign him to pick up a
load that will take him to Seattle, where we expect him to arrive at time t = 218.
This decision needs to reﬂect the value of the driver in Seattle at time t = 218.
By time 200, the driver has not yet arrived but we have learned that he was
delayed (our random travel time), and we now expect him to arrive at time t =
233. The sequence of pre- and post-decision states is given below.
t = 100
t = 100
t = 200
Label
pre-decision
post-decision
pre-decision


Location
Status
ET A




Cleveland
E
115




Seattle
E
218




Seattle
E
233

.
At time t = 200 we use his state (at the time) that he will be in Seattle at time 233
to make a new assignment. We use the value of the driver at this time to update
the value of the driver in Seattle at time 218 (the previous post-decision state).
Random Destinations
There are numerous applications in transportation where we have to send a vehicle
(container, taxi) to serve a customer before we know where the customer is going.
People routinely get into taxi cabs and then tell the driver where they are going.
Customers of railroads often ask for “50 box cars” without specifying where they
are going.
We can handle random destinations much as we handled random travel times.
Suppose we have the same driver headed to Cleveland, with an expected time of
arrival of 115. Assume that at time 100 we decide to move the driver empty up to
nearby Evanston to pick up a new load, but we do not know the destination of the
load. The post-decision state would be “loaded in Evanston.” By time t = 200 we
have learned that the load was headed to Seattle, with an expected arrival time (at
time 200) of 233. The sequence of pre- and post-decision states is given by
t = 100
t = 100
t = 200
Label
pre-decision
post-decision
pre-decision


Location
Status
ET A




Cleveland
E
115




Evanston
L
132




Seattle
E
233

.
We again make a new decision to assign the driver, and use the value of the driver
when we make this decision to update the value of the driver when he was loaded
in Evanston (the previous post-decision state).
14.7
BIBLIOGRAPHIC NOTES
Section 14.1
Piecewise-linear functions are an especially convenient way of
approximating concave functions. Powell and Godfrey (2001) proposed the
CAVE algorithm, which has proved successful in a number of applications

586
dynamic resource allocation problems
(see Powell and Godfrey, 2002; Powell and Topaloglu, 2006). Powell et al.
(2004) and Topaloglu and Powell (2003) provide convergence proofs for the
variations called the leveling algorithm and the SPAR algorithm. Convergence
proofs for scalar, multistage problems are provided by Nascimento and Powell
(2009) and Nascimento and Powell (2010a).
Section 14.2
The material in this section is based on the undergraduate senior
thesis of Lindsey Cant (Cant, 2006). The use of separable piecewise-linear
approximations was developed in a series of papers arising in transportation
and logistics (e.g., Powell and Godfrey 2001; Powell et al. 2004).
Section 14.3
The material in this section is based on Basler (2006).
Section 14.4
The notation in this section is based on a very general model
for resource allocation given in Simao et al. (2001). The use of separable
piecewise-linear value function approximations has been invested in Powell
and Godfrey (2001), Powell and Godrey (2002), and most thoroughly in
Powell and Topaloglu (2006). The idea of using ridge regression to create
value function approximations is due to Klabjan and Adelman (2007).
Section 14.5
This section is based on Powell and Godfrey (2002) and Powell
and Topaloglu (2006). For a discussion of approximate dynamic programming
for ﬂeet management, using the vocabulary of stochastic programming, see
Powell and Topaloglu (2003).
Section 14.6
Powell el al. (2002) and Spivey and Powell (2004) laid the
foundations for this work in the truckload trucking industry. A thorough
presentation of this project with a complete description of the algorithm is
given in Simao et al. (2009). For a more accessible presentation, see Simao
et al. (2010).
PROBLEMS
14.1
Revise the notation for the blood management problem to include the pres-
ence of testing that may show that inventories that have not yet reached
the six week limit have gone bad. State any assumptions and introduce
notation as needed. Be sure to model both pre- and post-decision states.
Carefully describe the transition function.
14.2
Consider the problem of managing a ﬂeet of vehicles as described in Section
14.5. There we assumed that the travel time required to move a container
from one location to another required one time period. Now we are going
to ask you to generalize this problem.
(a) Assume that the time required to move from location i to location j
is τij, where τij is deterministic and integer. Give two different ways
for modeling this by modifying the attribute r in a suitable way (recall
that if the travel time is one period, r = {location}).
(b) How does your answer to (a) change if τij is allowed to take on non-
integer values?

problems
587
(c) Now assume that τij is random, following a discrete uniform distribu-
tion (actually it can be any general discrete distribution). How do you
have to deﬁne the attribute a?
14.3
Orange futures project We have the opportunity to purchase contracts that
allow us to buy frozen concentrated orange juice (FCOJ) at some point in
the future. When we sign the contract in year t, we commit to purchasing
FCOJ during year t′ at a speciﬁed price. In this project t′ = 10, while
1≤t≤10. To model the problem, let
xtt′ = quantity of FCOJ ordered at time t (more speciﬁcally, with the
information up through time t), to be used during year t′,
xt = (xtt′)t′≥t,
Rtt′ = FCOJ that we know about at time t that can be used during year t′,
Rt = (Rtt′)t′≥t,
ptt′ = price paid for FCOJ purchased at time t that can be used
during year t′,
pt = (ptt′)t′≥t,
Dt = demand for FCOJ during year t.
For this problem, t′ = 10. The demand for FCOJ (Dt) does not become
known until year 10, and is uniformly distributed between 1000 and 2000
tons. If you have not ordered enough to cover the demand, you must make
up the difference by purchasing on the spot market, with price given by
p10,10.
We are going to specify a stochastic model for prices. For this purpose,
let
ρu
0 = initial upper range for prices
= 2.0,
ρℓ
0 = initial lower range for prices
= 1.0,
ρu = upper range for prices
= 1.2,
ρℓ= lower range for prices
= 0.9,
β = mean reversion parameter
= 0.5,

588
dynamic resource allocation problems
ρu,s = upper range for spot prices
= 1.15,
ρℓ,s = lower range for spot prices
= 1.03.
Let U represent a random variable that is uniformly distributed between
0 and 1 (in Excel, this is computed using RAND( )). Let ω = 0 be
the initial sample, and let ω = 1, 2, . . . be the remaining samples. Let
pt,t′(ω) be a particular sample realization. Prices are randomly generated
using
p0,10(ω) = 1.7(ρℓ
0 + (ρu
0 −ρℓ
0)U), ω ≥0,
pt,10(0) = pt−1,10(ρℓ+ (ρu −ρℓ)U), t = 1, 2, . . . , 9,
p10,10(ω) = p9,10(ρℓ,s + (ρu,s −ρℓ,s)U), ω ≥0,
pt,10(0) = pt,10(0), t ≥0,
p0,10(ω) = 0.9p0,10(ω −1) + 0.05p0,10(ω), ω ≥1,
pt,10(ω) = 0.9p(t, 10)(ω −1) + 0.05pt−1,10(ω), ω ≥1,
pt,10(ω) = pt−1,10(ρu −ρℓ)U + β(pt−1,10(ω) −pt,10(ω)), t = 1, 2, . . . ,
9, ω ≥1.
This process should produce a random set of prices that tend to trend
upward but that will move downward if they get too high (known as a
mean reversion process).
(a) Prepare a detailed model of the problem.
(b) Design an approximate dynamic programming algorithm, including a
value function approximation and an updating strategy.
(c) Implement your algorithm and describe the algorithmic tuning steps
you had to go through to obtain what appears to be a useful solution.
Be careful that your stepsize is not too small.
(d) Compare your results with different mean reversion parameters, such
as β = 0 and 1.
(e) How does your answer change if the upper range for spot prices is
increased to 2.0?
14.4
Blood management project. Implement the blood management model
described in Section 14.2. Assume that total demand averages 200 units of
blood each week, and total supply averages 150 units of blood per week.
Assume that the actual supplies and demands are normally distributed with
a standard deviation equal to 30 percent of the mean (set any negative
realizations to zero). Use the table below for the distribution of supply and

problems
589
demand (these are actuals for the United States). Use Table 14.2 for your
costs and contributions.
Blood type
AB+
AB−
A+
A−
B+
B−
O+
O−
Percentage of supply
3.40
0.65
27.94
5.17
11.63
2.13
39.82
9.26
Percentage of demand
3.00
1.00
34.00
6.00
9.00
2.00
38.00
7.00
(a) Develop a program that simulates blood inventories in steady state,
using a discount factor of 0.80, where decisions are made myopically
(the value function is equal to zero). Simulate 1000 weeks, and produce
a plot showing your shortages each week. This is easiest to implement
using a software environment such as Matlab. You will need access to
a linear programming solver.
(b) Next implement an ADP strategy using separable piecewise-linear
value function approximations. Clearly state any algorithmic choices
you have to make (e.g., the stepsize rule). Determine how many iter-
ations appear to be needed in order for the solution to stabilize. Then
run 1000 testing iterations and plot the shortages each week. Compare
your results against the myopic policy. What types of behaviors do you
notice compared to the myopic policy?
(c) Repeat part (b) assuming ﬁrst that the average supply is 200 units of
blood each week, and then again with the average set to 250. Compare
your results.
14.5
For the blood management problem, compare the results obtained using
value functions trained for an inﬁnite horizon problem against those trained
for a 10-week horizon using a ﬁxed set of starting inventories (make rea-
sonable assumptions about these inventories). Compare your results for the
ﬁnite horizon case using both sets of value functions (when using the inﬁ-
nite horizon value functions, you will be using the same value function for
each time period). Experiment with different discount factors.
14.6
Transformer replacement project. There are many industries that have
to acquire expensive equipment to meet needs over time. This problem is
drawn from the electric power industry (a real project), but the issues are
similar to airlines that have to purchase new aircraft or manufacturers that
have to purchase special machinery.
Electric power is transported from power sources over high-voltage
lines, which drop to successively lower voltages before entering your house
by using devices (familiar to all of us) called transformers. In the 1960s the
industry introduced transformers that could step power from 750,000 volts
down to 500,000 volts. These high-capacity transformers have a lifetime
that is expected to be somewhere in the 50- to 80-year range. As of this

590
dynamic resource allocation problems
writing, the oldest units are just reaching 40 years, which means there is
no data on the actual lifetime. As a result there is uncertainty about the
lifetime of the units.
The problem is that the transformers cost several million dollars each,
can take a year or more to build, and weigh over 200 tons (so they are
hard to transport). If a transformer fails, it creates bottlenecks in the grid.
If enough fail, a blackout can occur. The more common problem, however,
is that failures force the grid operators (these are distinct from the utilities
themselves) to purchase power from more expensive plants.
To model the problem, deﬁne
xt = number of transformers ordered at the end of year t that will
arrive at the beginning of year t + 1
Rtr = number of transformers at time t with age r,
RA
t = total number of active transformers at the end of year t
=

r
Rtr,
Ftr = a random variable giving the number of failures that occur during
year t of transformers with age r.
For $2.5 million, the company can order a transformer in year t to arrive
in year t + 1.
Table 14.4 gives the current age distribution (2 transformers are only 1
year old, while 10 are 39 years old). The probability f (a) that a unit that
is a years old will fail is given by
f (a) =



ρf
if age a ≤τ,
ρf (a −τ)
if age a > τ.
ρf is the base failure rate for transformers with age a ≤τ, where we
assume ρf = 0.01 and τ = 40. For transformers with age a > τ (a is an
integer number of years), the failure rate rises according to the polynomial
given above.
At an aggregate level the total network congestion costs (in millions of
dollars per year) are given approximately by
C(Rt) = 5

max(145 −RA
t ), 0
2 .
(14.34)
So, if we have RA
t ≥145 active transformers, then congestion costs are
zero.
Your challenge is to use approximate dynamic programming to deter-
mine how many transformers the company should purchase in each year.
Your performance will be measured based on total purchase and congestion

problems
591
Table 14.4
Initial age distribution of the equipment
Age
R0a
Age
R0a
Age
R0a
Age
R0a
1
2
11
2
21
1
31
12
2
1
12
2
22
1
32
0
3
0
13
1
23
2
33
8
4
9
14
0
24
7
34
10
5
2
15
1
25
2
35
16
6
4
16
1
26
3
36
6
7
2
17
0
27
1
37
2
8
7
18
4
28
1
38
6
9
1
19
2
29
2
39
10
10
6
20
1
30
7
40
0
costs over a 50-year period averaged over 1000 samples. For this project,
do the following
(a) Write up a complete model of the dynamic program.
(b) Describe at least one value function approximation strategy.
(c) Describe an updating method for your value function approximation.
Specify your stepsize rule.
(d) Implement your algorithm. Describe the results of steps to tune your
algorithm. Compare different strategies (stepsize rules, approximation
strategies) and present your conclusion of the strategy that works the
best.
(e) Now assume that you are not allowed to purchase more than 4 trans-
formers in any given year. Compare the acquisition strategy to what
you obtained without this constraint.
14.7
We now extend the previous project by introducing uncertainty in the life-
time. Repeat exercise 14.6, but now assume that the lifetime τ is a random
variable that might be 40, 45, 50, 55, or 60 with equal probability. Describe
how this changes the performance of the model.
14.8
We are going to revisit the transformer problem (holding the lifetime τ
ﬁxed at 40 years), but this time we are going to change the dynamics of
the purchasing process. In exercise 14.6 we could purchase a transformer
for $2.5 million that would arrive the following year. Now, we are going to
be able to purchase transformers for $1.5 million, but they arrive in three
years. That is, a transformer purchased at the end of year t is available to
be used in year t + 3.
This is a signiﬁcantly harder problem. For this version it will be neces-
sary to keep track of transformers that have been ordered but that have not
yet arrived. This can be handled by allowing the age a to be negative. So
Rt,−2 would be the number of transformers we know about at time t that
will ﬁrst be available in 2 time periods.

592
dynamic resource allocation problems
In addition it will be necessary to choose between ordering a transformer
quickly for a higher price, or more slowly at a lower price. You will need
to design an approximation strategy that handles the different types of
transformers (consider the use of separable, piecewise-linear value function
approximations as we did for the blood management problem).
Repeat exercise 14.6, placing special care on the modeling of the state
variable, the decision variable, and the transition function. Instead of trans-
formers arriving to a single inventory, they now arrive to the system with
different ages. Try starting with a separable piecewise-linear value func-
tion approximation that captures the value of transformers of different
ages. However, try experimenting with other functional approximations.
For example, you might have a single-value function that uses the total
number of transformers that are not only available but that also have yet
to arrive. Report your experience, and compare your results to the original
problem when a transformer arrives in the next time period.

C H A P T E R
15
Implementation Challenges
So you are ﬁnally ready to solve your own problem with approximate dynamic
programming. Of course, it is nice if you have a problem that closely matches one
that we have already discussed in this book. But you don’t. As is often the case,
you have to fall back on general principles to solve a problem that does not look
like any of the examples that we have presented.
In this chapter we describe a number of issues that tend to arise in real applica-
tions of approximate dynamic programming. These are intended to help as a rough
guide to what can be expected in the path to a successful ADP application.
15.1
WILL ADP WORK FOR YOUR PROBLEM?
The ﬁrst question you have to ask is: Do you need approximate dynamic program-
ming to solve your problem? There are many problems where myopic policies
are going to work quite well (in special cases they may be provably optimal).
For example, the optimal solution to steady-state batch replenishment problems
are known to have the structure where, if the amount of product on hand is less
than s, then we should order an amount that brings our inventory up to S (these
are known as (s, S) policies). If we are managing a group of identical repairmen
serving jobs that pop up randomly (and uniformly) over a region, then it is also
unlikely that a value function that helps us look into the future will noticeably
improve the solution.
In short, we have to ask the question: Does a value function add value? If we
estimate a value function, how do we think our solution will improve?
Problems where myopic policies work well (and in particular where they are
provably optimal) tend to be quite simple (how much product to order, when to
sell an asset). There are, of course, more complex problems where myopic policies
are known to be optimal. For example, the problem of allocating funds among
Approximate Dynamic Programming: Solving the Curses of Dimensionality, Second Edition.
Warren B. Powell.
2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
593

594
implementation challenges
a set of asset classes to balance risk can be solved with a myopic policy if we
assume that there are no transaction costs (or transaction times). In fact it should
not be surprising to ﬁnd out that if it is possible to move from any state to any
other state (instantly and with no cost), then a myopic policy will be optimal. Not
surprisingly, ADP is not going to contribute very much to these problems.
In contrast, the hardest ADP problems tend to be applications where it takes
numerous steps to obtain a reward. The easiest examples of such problems are
games (chess, checkers, Connect-4) where many steps have to be made before we
know if we won or lost. ADP works particularly well when we have access to
a policy that works “pretty well” and we can use it to train value functions. A
well-designed approximate value function can make a good policy even better. But
where we have an intractably hard problem, we may have to start with naive rules
that work poorly. We may avoid a decision to visit a state if we do not know
how to properly behave once we reach the state. It is these problems where it is
often necessary to turn to an outside supervisor who can guide the search algorithm
during initial learning stages.
15.2
DESIGNING AN ADP ALGORITHM FOR
COMPLEX PROBLEMS
There are numerous problems in discrete optimization that have to be solved over
time, under uncertainty. For example, it may be necessary to decide which jobs
should be assigned to different machines, and in which order, in the presence of
random arrivals of jobs in the future. Service vehicles have to be assigned to cus-
tomer demands (picking up packages, or providing household plumbing services)
that arise randomly over time. Transportation companies have to dispatch trucks
with shipments that arrive randomly to a terminal.
Deterministic versions of these problems (where all customer demands are
known in advance) can be extremely difﬁcult, and such problems are generally
solved using heuristics. Stochastic dynamic versions of these problems are gen-
erally solved using simulation, where at time t we would solve an optimization
problem using only what is known at time t. As new information becomes known,
the problem would be re-optimized. Solving difﬁcult integer programs in real time
can be challenging, primarily because of the time constraints on solution times.
But the solution quality can be relatively poor since the decisions do not reﬂect
what might happen in the future.
The ﬁrst step in developing an ADP strategy for these problems is to begin by
implementing a myopic policy in the form of a simulation. Consider a problem
faced by the military in the scheduling of unmanned aerial vehicles (UAVs) that
have to be routed to collect information about various targets. A target is a request
to visit an area to determine its status (Was a bridge destroyed? Is a missile bat-
tery operational? Are there people near a building?). Targets arise randomly, and
sometimes it is necessary to plan the path of a UAV through a sequence of targets
in order to determine which one should be visited ﬁrst. The problem is illustrated
in Figure 15.1, which shows UAVs both on the ground and in the air, with paths

designing an ADP algorithm for complex problems
595
?
?
?
?
?
?
?
?
?
Figure 15.1
Dynamic routing of UAVs, showing known targets and potential targets.
planned through a series of known targets. In addition there is a series of potential
targets (shown as question marks) that might arise.
One way to model this problem is to assume that we are going to optimally
route a ﬂeet of UAVs through a sequence of known targets. This problem can
be formulated as an integer program and solved using commercial optimization
packages as long as the problem is not too large. The biggest complication arises if
the tours (the number of targets that a single UAV might cover in a single solution)
become long. For example, it is possible to handle problems with thousands of
UAVs if they are restricted to covering at most one target at a time. However,
the problem becomes much more complex with tours of as few as three or four
targets. If we have tours of ﬁve or ten targets, we might have to turn to heuristic
algorithms that give good, but not necessarily optimal, solutions. These heuristics
typically involve starting with one solution, and then testing alternative solutions to
try to discover improvements. The simplest are local search algorithms (try making
one or more changes; if the solution improves, keep the new solution and proceed
from there). These algorithms fall under names such as local search, neighborhood
search, and tabu search.
Suppose that we have devised a strategy for solving the problem myopically.
It is quite likely that we will not be entirely satisﬁed with the behavior of the
solution. For example, the system might assign two UAVs to serve two targets in
an area that is not likely to produce additional targets in the future. Alternatively,
the system might try to assign one UAV to cover two targets in an area that is likely
to produce a number of additional targets in the future (too many to be covered by
a single UAV). We might also want to consider attributes such as fuel level. We
might prefer a UAV with a high fuel level if we expect other demands to arise. If

596
implementation challenges
there is little chance of new demands, we might be satisﬁed assigning a UAV that
is low on fuel with the expectation that it will ﬁnish the task and then return home.
It is very important, when designing an ADP strategy, to make sure that we have
a clear idea of the behaviors we are trying to achieve that are not being produced
with a myopic policy. With this in mind, the next step is to design a value function
approximation. The choice of function depends on the type of algorithmic strategy
we are using for the myopic problem. If we wish to solve the myopic problem using
integer programming, then a nonlinear value function approximation is going to
cause complications (now we have a nonlinear integer programming problem). If
we use a local search procedure, then the value function approximation can take
on almost any form.
Once we have designed a value function approximation, we have to make sure
that we are going to be able to estimate it using information available from the
solution procedure. For example, in Chapter 13 we showed how some value func-
tion approximations can be approximated using gradient information, which means
that we need to be able to estimate the marginal value of, say, a UAV in a partic-
ular region. If we constrain ourselves to assigning a UAV to at most one target at
a time, our myopic problem is the same as the dynamic assignment problem we
considered in Section 14.6, where it is quite easy to obtain marginal values from
dual variables. Local search heuristics, however, do not yield gradient information
very easily. As an alternative, we might approximate the value of a UAV as the
contribution earned over the tour to which it has been assigned. This can be thought
of as the average value of a UAV instead of the marginal value. Not as useful, but
perhaps better than nothing.
15.3
DEBUGGING AN ADP ALGORITHM
Imagine that you now have your algorithm up and running. There are several
strategies for deciding whether it works well:
1. Plotting the objective function over the iterations.
2. Evaluating one or more performance statistics over the iterations.
3. Subjectively evaluating the behavior of your system after the algorithm has
completed.
Ideally the objective function will show generally steady improvement (keep in
mind the behavior in Figure 15.3). However, you may ﬁnd that the objective
function gets steadily worse, or wanders around with no apparent improvement.
Alternatively, you may ﬁnd that important performance measures that you are inter-
ested in are not improving as you hoped or expected. Explanations for this behavior
can include:
1. If you are using a forward pass, you may be seeing evidence of a “bounce.”
You may need to simply run more iterations.

practical issues
597
2. You might have a problem where the value function is not actually improving
the decisions. Your myopic policy may be doing the best that is possible. You
might not have a problem that beneﬁts from looking into the future.
3. Be careful that you are not using a stepsize that is going to zero too
quickly.
4. Your value function may be a poor approximation of the true value function.
You may not be properly capturing the structure of the problem.
5. You might have a reasonable value function, but the early iterations might
have produced a poor estimate, and you are having difﬁculty recovering from
this initial estimate.
6. You may not be updating the value function properly. Carefully verify that
ˆvn
t (whether it be the value of being in a state or a vector of derivatives) is
being correctly calculated. This is a common error.
Diagnosing problems with an algorithm is often easiest when you can see speciﬁc
decisions that you simply feel are wrong. If you are solving your decision problems
properly (this is not always the case), then presumably the solution is due to an
incorrect value function approximation. One problem is that Vt(Sx
t ) may reﬂect a
number of updates over many iterations. For this reason it is best to look at ˆvn
t . If
it takes the algorithm to a poor state, is ˆvn
t small? If not, why do you think it is a
good state? If it is clearly a bad state, why does it appear that ˆvn
t is large? If ˆvn
t
is reasonable, perhaps the problem is in how the value function approximation is
being updated (or the structure of the value function itself).
15.4
PRACTICAL ISSUES
Not surprisingly, there are a number of practical issues that arise when designing
and testing approximate dynamic programming algorithms. This section provides
a brief discussion of some of these.
15.4.1
Discount Factors
One way to improve the performance of the algorithm is to reduce the discount
factor. Problems with discount factors close to 1 are simply much harder than a
problem with a discount factor that is smaller. For this reason an effective strategy
is to simply start by solving a problem with a smaller discount factor (e.g., γ ≤
0.7). If you feel that you are getting an effective solution with a smaller discount
factor, then you may use the solution as an initial solution for the algorithm at a
larger discount factor.
A similar issue arises with undiscounted problems with long horizons. You
might either start with a relatively small horizon (e.g., 5 time periods) or introduce
a discount factor that you can raise toward 1. Use the solution of the simpler
problem as an initial solution to harder problems (longer horizons, higher discount
factors).

598
implementation challenges
15.4.2
Starting and Stopping
Two challenges we face in approximate dynamic programming is getting started,
which means dealing with very poor initial approximations of the value function,
and ﬁguring out when we have converged. Solving these problems tends to be
unique to each problem, but it is important to recognize that they need to be
addressed.
15.4.3
Getting through the Early Iterations
One issue that always arises in approximate dynamic programming is that you
have to get through the early iterations when the value function approximation is
very inaccurate. While it may be possible to start with an initial value function
approximation that is fairly good, it is often the case that we have no idea what
the value function should be and we simply use zero (or some comparable default
approximation). After a few iterations we might have updated the approximation
with a few observations (allowing us to update the parameter vector θ), but the
approximation may be quite poor.
As a rule, it is better to use a relatively simple approximation in the early
iterations. The problem is that as the algorithm progresses, we may stall out at a
poor solution. We would like to design a value function approximation that allows
us to produce an accurate approximation, but this may require estimating a large
number of parameters.
In Section 8.1 we introduced the idea of estimating the value of being in a state
at different levels of aggregation. Rather than use any single level of aggregation,
we showed that we could estimate the value of being in a state by using a weighted
sum of estimates at different levels of aggregation. Figure 15.2 shows what happens
when we use a purely aggregate estimate, a purely disaggregate estimate, and a
weighted combination. The purely aggregate estimate produces much faster initial
convergence, reﬂecting the fact that an aggregate estimate of the value function is
much easier to obtain with a few observations. The problem is that this estimate
does not work as well in the long run.
Using a purely disaggregate estimate produces slow initial convergence, but
ultimately provides a much better objective function in the later iterations. However,
using a weighted combination creates faster initial convergence and the best results
over all. The lesson of this demonstration is that it can be better to use a simpler
approximation in the early iteration, and transition to a more reﬁned approximation
as the algorithm progresses.
This idea can be applied in a variety of settings. Consider the problem of mod-
eling the level of cash in a mutual fund that depends on the amount of cash we are
currently holding Rt, the current market performance ft1, and current interest rates
ft2. We can immediately produce value function approximations at three levels
of aggregation: (1) V(Rt|θ), (2) V(Rt|θ(ft1)), and (3) V(Rt|θ(ft1, ft2)). The ﬁrst
value function approximation completely ignores the two ﬁnancial statistics. The
second computes a parameter vector θ for each value of ft1. The third computes
a parameter vector θ for each combination of ft1 and ft2. Depending on how ft1

practical issues
599
1400000
1450000
1500000
1550000
1600000
1650000
1700000
1750000
1800000
1850000
1900000
0
100
200
300
400
500
600
700
800
900
1000
Iterations
Objective function
Aggregate
Disaggregate
Weighted combination 
Figure 15.2
Objective function produced with a purely aggregate value function approximation, a
purely disaggregate one, and one using a weighted combination of aggregate and disaggregate approx-
imations.
and ft2 are discretized, we could easily be computing thousands of estimates of θ.
As a result the quality of the estimate of θ(ft1, ft2) for a particular combination
of ft1 and ft2 could be very inaccurate due to statistical error. We handle this by
using a weighted sum of approximations, such as
V(St|θ) = w1V(Rt|θ) + w2V(Rt|θ(ft1)) + w3V(Rt|θ(ft1, ft2)).
The weights can be computed using the techniques described in Section 8.1.4.
Computing the weights in this way produces a natural transition from simpler to
more complex models with nominal computational effort.
It is easy to overlook the importance of using simpler functional approximations
in the early iterations. A more complex function can be hard to estimate, producing
large statistical errors in the beginning. It can be much more effective to do a better
job (statistically speaking) of estimating a simpler function than to do a poor job
of trying to estimate a more sophisticated function.
15.4.4
Convergence Issues
A signiﬁcant challenge with approximate dynamic programming is determining
when to stop. In Section 11.6 we encountered the problem of “apparent conver-
gence” when the objective function appeared to have leveled off after 100 iterations,
suggesting that we could stop the algorithm. In fact the algorithm made consider-
ably more progress when we allowed it to go 1000 iterations. This is one reason
that we have to be very careful not to allow the stepsizes to decline too quickly.
Also before we decide that “100 iterations is enough,” we should do a few runs
with substantially more iterations.

600
implementation challenges
0
2000
4000
6000
8000
10000
12000
14000
0
50
100
150
200
250
300
350
400
450
Iteration
Objective function 
“Bounce”
“Exploration”
Figure 15.3
Objective function of an approximate dynamic programming algorithm.
Apparent convergence, however, is not our only headache. Figure 15.3 shows
an example of the objective function (total costs, which we are minimizing) for an
approximate value iteration algorithm. This graph shows a behavior that is fairly
common when using a single-pass algorithm, which we refer to as the “bounce.”
When using a single-pass algorithm, the value function for time t, iteration n reﬂects
activities from time t + s at iteration n −s, since values are passed backward
one time period for each iteration. The result can be value function approximations
that are simply incorrect, producing solutions that steadily get worse for a period
of time.
Figure 15.3 also illustrates some other behaviors that are not uncommon. After
the bounce, the algorithm goes through a period of exploration where little progress
is made. While this does not always happen, it is not uncommon for algorithms to
go through periods of trying different options and slowly learning the effectiveness
of these options. This behavior can be particularly pronounced for problems where
there is little structure to guide the solution.
Finally, the algorithm starts to settle down in the ﬁnal 100 iterations, but even
here there are short bursts where the solution becomes much worse. At this stage
we might be comfortable concluding that the algorithm has probably progressed as
far as it can, but we have to be careful not to stop on one of the poor solutions.
This can be avoided in several ways. The most expensive option is to stop every
K iterations and perform repeated samples. This strategy allows us to make strong
statistical statements about the quality of the solution, but this can be expensive.
Another approach is to simply smooth the objective function using exponential
smoothing, and simultaneously compute an estimate of the standard deviation of
the solution. Then simple rules can be designed to stop the algorithm when the
solution is, for example, one standard deviation below the estimated mean (if we
want to hope for a good solution).
15.4.5
Evaluating a Policy
If you are implementing an ADP algorithm, you might start asking for some way to
evaluate the accuracy of your procedure. This question can be posed in two broad

practical issues
601
ways: (1) How good is your policy? (2) How good is your value function approxi-
mation? Section 4.9.4 addressed the issue of evaluating a policy. In this section we
turn to the question of evaluating the quality of the value function approximation.
There are applications such as pricing an asset, estimating the probability of win-
ning a game, or reaching a goal where the primary interest is in the value function
itself.
LetV(S) be an approximation of the value of being in state S. The approximation
could have a lookup table format, or it might be a regression of the form
V(S) =

f ∈F
θf φf (S).
Now imagine that we have access to the true value of being in state S, which we
denote by V (s). Typically this would happen when we have a problem that we can
solve exactly using the techniques of Chapter 3, and we are now testing an ADP
algorithm to evaluate the effectiveness of a particular approximation strategy. If
we want an aggregate measure, we could use
νavg = 1
|S|

s∈S
|V (s) −V(s)|.
νavg expresses the error as an average over all the states. The problem is that it
puts equal weight on all the states, regardless of how often they are visited. Such
an error is unlikely to be very interesting.
This measure highlights the problem of determining how to weight the states.
Suppose that while determining V (s), we also ﬁnd p∗(s), which is the steady-
state value of being in state s (under an optimal policy). If we have access to this
measure, we might use
νπ∗=

s∈S
p∗(s)|V (s) −V(s)|,
where π∗represents the fact that we are following an optimal policy. The advantage
of this error measure is that p∗(s) is independent of the approximation strategy used
to determine V(s). If we want to compare different approximation strategies, where
each produces different estimates for V(s), then we get a more stable estimate of
the quality of the value function.
Such an error measure is not as perfect as it seems. Suppose that we have two
approximation strategies that produce the approximations V
(1)(s) and V
(2)(s). Also
let p(1)(s) and p(2)(s) be, respectively, estimates of the steady-state probabilities of
being in state s produced by each policy. If a particular approximation has us visit-
ing certain states more than others, then it is easy to argue that the value functions
produced by this strategy should be evaluated primarily based on the frequency
with which we visit states under that policy. For example, if we wish to obtain an
estimate of the value of the dynamic program (using the ﬁrst approximation), we

602
implementation challenges
would use
V
(1) =

s∈S
p(1)(s)V
(1)(s).
For this reason it is fair to measure the error based on the frequency with which
we actually visit states under the approximation, leading to
ν(1) =

s∈S
p(1)|V (s) −V
(1)(s)|.
If we compute V
(2) and ν(2), then provided that ν(1) < ν(2), we could conclude that
V
(1) is a better approximation than V
(2). But it would have to be understood that
this result means that V
(1) does a better job of estimating the value of states that
are visited under policy 1 than the job that V
(2) does in estimating the value of
states that are visited under policy 2.
15.5
MODELING YOUR PROBLEM
There is a long history in the optimization community to ﬁrst present a complete
optimization model of a problem, after which different algorithms may be discussed.
A mathematical programming problem is not considered properly stated unless we
have deﬁned all the variables, constraints and the objective function. The same
cannot be said of the approximate dynamic programming community, where a more
casual presentation style has evolved. It is not unusual for an author to describe the
size of the state space without actually deﬁning a state variable. Transition functions
are often ignored, and there appear to be many even in the research community
who are unable to write down an objective function properly.
The culture of approximate dynamic programming is closer to that of simulation
where it is quite common to see papers without any notation at all. Such papers
are often describing fairly complex problems of physical processes that are well
understood by those working in the same area of application. Mathematical models
of such processes can be quite cumbersome, and have not proved necessary for
people who are familiar with the problem class. There are many papers in approxi-
mate dynamic programming which have evolved out of the simulation community,
and have apparently adopted this minimalist culture for modeling.
ADP should be viewed as a potentially powerful took for the simulation com-
munity. However, in a critical way it should be viewed quite differently. Most
simulation papers are trying to mimic a physical process rather than optimize it.
The most important part of simulation models are captured by the transition func-
tion, the details of which may not be particularly interesting. But when we use
approximate dynamic programming, we are often trying to optimize (over time)
complex problems that might otherwise be solved using simulation.
To properly describe a dynamic program (in preparation for using approximate
dynamic programming), we need to specify the value function approximation and

modeling your problem
603
the updating strategies used to estimate it. We need to evaluate the quality of a
solution by comparing objective functions. These steps require a proper description
of the state variable (by far the most important quantity in a dynamic program),
the decision variables, exogenous information and the contribution function. But as
with many simulation problems, it is not always clear that we need the transition
function in painstaking detail. If we formulate a math programming problem, the
transition function would be buried in the constraints. When modeling a dynamic
system, it is well known that the transition function captures the potentially complex
physics of a problem. For simple problems, there is no reason not to completely
specify these equations, especially if we need to take advantage of the structure
of the transition function. For more complex problems, modeling the transition
function can be cumbersome, and the physics tends to be well known by people
working with an application class.
If you wish to develop an approximate dynamic programming algorithm, it is
important to learn to express your problem mathematically. There is a strong culture
in the deterministic optimization community to write out the problem in its entirety
using linear algebra. By contrast, there is a vast array of simulation problems that
are expressed without any mathematics at all. Approximate dynamic programming
sits between these two ﬁelds. It is simply inappropriate to solve a problem using
ADP without expressing the problem mathematically. At the same time ADP allows
us to address problems with far more complexity than are traditionally handled
using tools such as linear programming. For this reason we suggest the following
guidelines when modeling a problem:
• State variable. Clearly identify the dimensions of the state variable using
explicit notation. This is the mechanism by which we can understand the
complexity and structure of your problem.
• Decision variable. We need to know what dimensions of the problem we are
controlling. It is often convenient to describe constraints on decisions that
apply at a point in time (rather than over time).
• Exogenous information process. What information is arriving to our system
from exogenous sources? What do we know (if anything) about the probability
law describing what information might arrive?
• Transition function. If the problem is relatively simple, this should be stated
mathematically in its entirety. But as the problem becomes more compli-
cated, a more descriptive presentation may be appropriate. For many problems
the details of the transition function are not central to the development of
an approximate dynamic programming algorithm. Also, as problems become
more complex, a complete mathematical model of the transition function can
become tedious. Obviously the right level of detail depends on the context
and what we are trying to accomplish.
• Contribution function. Also known as the cost function (minimizing), the
reward function (maximizing), or the utility function (usually maximizing).
We need to understand what goal we are trying to achieve.

604
implementation challenges
• Objective function. This is where you specify how you are going to evalu-
ate contributions over time (ﬁnite or inﬁnite horizon? Discounted or average
reward?).
Thus we feel that every component should be stated explicitly using mathematical
notation with the single major exception of the transition function (which captures
all the physics of the problem). Some problems are simply too complex, and for
these problems the structure of the transition function may not be relevant to the
design of the solution algorithm. A transition function might be a single simple
equation, but it can also be thousands of lines of code. The details of the transition
function are generally not that interesting if the focus is on the development of an
ADP algorithm.
When writing up your model, pay particular attention to the modeling of time.
Keep in mind that dynamic programs are solved at discrete points in time that
we call decision epochs. Variables indexed at time t are being measured at time
t, which determines the information that is available when we make a decision.
It does not imply that physical activities are also taking place at these points in
time. We can decide at time 100 to assign a technician to perform a task at time
117. It may be that we will not make the next decision until time 200 (or time
143, when the next phone call comes in). If you are modeling a deterministic
problem, these issues are less critical. But for stochastic problems the modeling of
information (what we know when we make a decision) needs to be separated from
the modeling of physical processes (when something happens).
15.6
ONLINE VERSUS OFFLINE MODELS
When testing approximate dynamic programming algorithms, it is common to per-
form the testing in an “ofﬂine” setting. In this mode (which describes virtually
every algorithm in this book) we start at time t = 0 and step forward in time,
using information realizations (which we have denoted W(ω)) that are either gen-
erated randomly, drawn from a ﬁle of sample paths (e.g., prices of stocks) or
sampled from a known probability distribution.
In an online setting the model is running in real time alongside an actual phys-
ical process. The process might be a stock price, an inventory at a store, or a
problem in engineering such as ﬂying an aircraft or controlling the temperature of
a chemical reaction. In such settings we may observe the exogenous information
process (e.g., a stock price) or we may even depend on physical observations to
observe state transitions. When we observe exogenous information, we do not need
to estimate probability distributions for the random variables (e.g., the change in
the stock price). These are sometimes referred to as distribution-free models. If
we make a decision and then observe the state transition, then we do not need a
mathematical transition function. This is referred to as model-free (or sometimes
“direct”) approximate dynamic programming.
There are two important cases of online applications: steady state and transient.
In a steady-state situation we do not have to forecast future information, we instead

online versus ofﬂine models
605
need only to use live information to update the process. We can use live information
to drive our ADP algorithm. For example, consider a steady-state version of the
asset acquisition problem we solved in Section 14.1. Assume that our asset is a
type of DVD player (a popular theft item). If we have Rt DVDs in inventory, we
use some rule to decide to order an amount xt. Suppose now that we observe that
we sold ˆDt units, allowing us to compute a proﬁt
Ct(xt) = p ˆDt −cxt,
where p is the price at which we sell the units, and c is the wholesale cost of
new units purchased at time t. Note that ˆDt is sales (rather than actual demand).
ˆDt is limited by the measured inventory Rt. We let Rx
t = Rt −ˆDt + xt be our
post-decision state, which is how much we think we have left over (but excluding
loss due to theft). After this we measure the inventory Rt+1 that differs from Rx
t
because of potential theft.
We can use the methods in Chapter 13 to help estimate an approximate value
function V(Rx
t ). We would then make decisions using
xt = arg max

Ct(xt) + γV(Rx
t )

.
Now assume that we have a nonstationary problem, where demands might be
steadily increasing or decreasing, or might follow some sort of cyclic pattern (e.g.,
hourly, weekly or monthly cycles, depending on the application). We might use a
time-series model with the form
Ftt′ = θt0 +

c∈C
θcal
tc Xt′c +
τM

τ=1
θtime
tτ
pt−τ
where C is a set of calendar effects (e.g., the seven days of the week), Xt′c is an
indicator variable (e.g., equal to 1 if t′ falls on a Monday), θcal is the calendar
parameters, pt−τ is the price τ time periods ago, and θtime
tτ
is the impact of the
price τ time periods ago on the model. The precise speciﬁcation of our forecast
model is not important right now, but we observe that it is a function of a series
of past prices, so we can deﬁne a price state variable Sp = (pt−1, . . . , pt−τM ) and
a series of parameters θt = (θt0, (θcal
tc )c∈C, (θtime
tτ
)τ), which are updated after each
time period. Suppose that we have devised a system for updating the parameter
vector θt after we observe the demands ˆDt.
In the presence of transient demands, to determine the decision xt, we have to
set up and solve a dynamic program over the time periods t, t + 1, . . . , t + τ ph,
where τ ph is a speciﬁed planning horizon. This means that we have to use our
forecast model Ftt′ over t′ = t, t + 1, . . . , t + τ ph. To run an ADP algorithm, we
have to assume a distribution (e.g., normal with mean Ftt′ and variance σ 2
t,t′−t)
to generate random observations of demands. This means that we are solving an
ADP model on a rolling horizon basis, reoptimizing over a horizon with each step
forward in time.

606
implementation challenges
15.7
IF IT WORKS, PATENT IT!
A successful ADP algorithm for an important problem class is, we believe, a
patentable invention. While we continue to search for the holy grail of a gen-
eral class of algorithms that work reliably on all problems, we suspect the future
will involve fairly speciﬁc problem classes with a well-deﬁned structure. For such
problems we can, with some conﬁdence, perform a set of experiments that can
determine whether an ADP algorithm solves the problems that are likely to come up
in that class. Of course, we encourage developers to keep these results in the public
domain, but our point is that these represent speciﬁc technological breakthroughs
that could form the basis of a patent application.

Bibliography
Andreatta, G., and Romeo, L. (1988), “Stochastic shortest paths with recourse,” Networks
18, 193–204.
Ankenman, B., Nelson, B. L., and Staum, J. (2009), “Stochastic Kriging for simulation
metamodeling,” Operations Research 58 (2), 371–382.
Antos, A., Munos, R., and Szepesvari, C. (2008a), “Fitted Q-iteration in continuous action-
space MDPs,” Advances in Neural Information Processing Systems 20, 9–16.
Antos, A., Szepesvari, C., and Munos, R. (2007), “Value-iteration based ﬁtted policy iter-
ation: Learning with a single trajectory,” 2007 IEEE International Symposium on
Approximate Dynamic Programming and Reinforcement Learning, pp. 330–337.
Antos, A., Szepesv´ari, C., and Munos, R. (2008b), “Learning near-optimal policies with
Bellman-residual minimization based ﬁtted policy iteration and a single sample path,”
Machine Learning 71 (1), 89–129.
Auer, P., Cesa-bianchi, N., and Fischer, P. (2002), “Finite time analysis of the multiarmed
bandit problem,” Machine Learning 47(2–3), 235–256.
Baird, L. C. (1995), “Residual algorithms: Reinforcement learning with function approx-
imation,” Proceedings of the Twelfth International Conference on Machine Learning
pp. 30–37.
Banks, J., Nelson, B. L., and J. S. Carson, I. I. (1996), Discrete-Event System Simulation,
Prentice-Hall, Englewood Cliffs, NJ.
Barto, A. G., and Sutton, R. S. (1981), “Landmark learning: An illustration of associative
search,” Biological Cybernetics 8, 1–8.
Barto, A. G., Bradtke, S. J., and Singh, S. (1995), “Learning to act using real-time dynamic
programming,” Artiﬁcial Intelligence 72 (1–2), 81–138.
Barto, A. G., Sutton, R. S., and Anderson, C. W. (1983), “Neuron-like elements that can
solve difﬁcult learning control problems,” IEEE Transactions on Systems, Man and
Cybernetics 13, 834–846.
Barto, A. G., Sutton, R. S., and Brouwer, P. (1981), “Associative search network: A rein-
forcement learning associative memory,” Biological Cybernetics 40 (3), 201–211.
Approximate Dynamic Programming: Solving the Curses of Dimensionality, Second Edition.
Warren B. Powell.
2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
607

608
bibliography
Basler, J. T. (2006), “Optimal portfolio rebalancing: An approximate dynamic program-
ming approach,” Technical report, Department of Operations Research and Financial
Engineering, Princeton University.
Bean, J. C., Birge, J. R., and Smith, R. L. (1987), “Aggregation in dynamic programming,”
Operations Research 35, 215–220.
Bechhofer, R. E., Santner, T. J., and Goldsman, D. M. (1995), Design and Analysis of
Experiments for Statistical Selection, Screening, and Multiple Comparisons, Wiley,
New York.
Bellman, R. (1971), Introduction to the Mathematical Theory of Control Processes, Vol. II,
Academic Press, New York.
Bellman, R., and Kalaba, R. (1959), “On adaptive control processes,” IRE Transactions on
Automatic Control 4 (2), 1–9.
Bellman, R. E. (1957), Dynamic Programming, Princeton University Press, Princeton, NJ.
Bellman, R. E., and Dreyfus, S. E. (1959), “Functional approximations and dynamic pro-
gramming,” Mathematical Tables and Other Aids to Computation 13, 247–251.
Benveniste, A., Metivier, M., and Priouret, P. (1990), Adaptive Algorithms and Stochastic
Approximations, Springer, New York.
Berry, D. A., and Fristedt, B. (1985), Bandit Problems, Chapman and Hall, London.
Bertsekas, D. P. (1982), “Distributed dynamic programming,” IEEE Transactions on Auto-
matic Control 27, 610–616.
Bertsekas, D. P. (2005), Dynamic Programming and Stochastic Control, Vol. I, Athena
Scientiﬁc, Belmont, MA.
Bertsekas, D. P. (2009), Approximate Dynamic Programming, Vol. II, 3rd ed., Athena Sci-
entiﬁc, Belmont, MA.
Bertsekas, D. P., and Castanon, D. (1989), “Adaptive aggregation methods for inﬁnite hori-
zon dynamic programming,” IEEE Transactions on Automatic Control 34, 589–598.
Bertsekas, D. P., and Castanon, D. A. (1999), “Rollout algorithms for stochastic scheduling
problems,” Journal of Heuristics 5, 89–108.
Bertsekas, D. P., and Nedic¸, A. (2003), “Least-squares policy evaluation algorithms with
linear function approximation,” Journal of Discrete Event Systems 13, 79–110.
Bertsekas, D. P., and Tsitsiklis, J. N. (1989), Parallel and Distributed Computation: Numer-
ical Methods, Prentice-Hall, Englewood Cliffs, NJ.
Bertsekas, D. P., and Tsitsiklis, J. N. (1996), Neuro-dynamic Programming, Athena Scien-
tiﬁc, Belmont, MA.
Bertsekas, D. P., Borkar, V. S., and Nedic, A. (2004), “Improved temporal difference meth-
ods with linear function approximation,” in J. Si, A. G. Barto, W. B. Powell, and D.
Wunsch, eds., Handbook of Learning and Approximate Dynamic Programming, IEEE
Press, New York, pp. 233–257.
Bertsekas, D. P., Tsitsiklis, J. N., and An (1991), “Analysis of stochastic shortest path
problems,” Mathematics of Operations Research 16 (3), 580–595.
Bertsekas, D. P., Van Roy, B., Lee, Y., and Tsitsiklis, J. N. (1997), “A neuro-dynamic
programming approach to retailer inventory management,” Proceedings of the IEEE
Conference on Decision and Control, Vol. 4, pp. 4052–4057.
Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., and Lee, M. (2009), “Natural actor–critic
algorithms,” Automatica 45 (11), 2471–2482.

bibliography
609
Birge, J. R. (1985), “Decomposition and partitioning techniques for multistage stochastic
linear programs,” Operations Research 33, 989–1007.
Birge, J. R., and Louveaux, F. (1997), Introduction to Stochastic Programming, Springer,
New York.
Bishop, C. M. (2006), Pattern Recognition and Machine Learning, Springer, New York.
Blum, J. (1954a), “Multidimensional stochastic approximation methods,” Annals of Mathe-
matical Statistics 25, 737–744.
Blum, J. R. (1954b), “Approximation methods which converge with probability one,” Annals
of Mathematical Statistics 25, 382–386.
Borkar, V., and Konda, V. (1997), “The actor–critic algorithm as multi-time-scale stochastic
approximation,” Sadhana 22 (4), 525–543.
Boutilier, C., Dean, T., and Hanks, S. (1999), “Decision-theoretic planning: Structural
assumptions and computational leverage,” Access 11 (1), 1–94.
Bradtke, S. J., and Barto, A. G. (1996), “Linear least-squares algorithms for temporal dif-
ference learning,” Machine Learning 22 (1), 33–57.
Bradtke, S. J., Barto, A. G., and Ydstie, B. (1994), “Adaptive linear quadratic control using
policy iteration,” American Control Conference 3, 3475–3479.
Brezzi, M., and Lai, T. (2002), “Optimal learning and experimentation in bandit problems,”
Journal of Economic Dynamics and Control 27 (1), 87–108.
Brown, R. G. (1959), Statistical Forecasting for Inventory Control, McGraw-Hill, New
York.
Brown, R. G. (1963), Smoothing, Forecasting and Prediction of Discrete Time Series,
Prentice-Hall, Englewood Cliffs, NJ.
Busoniu, L., Babuska, R., De Schutter, B., and Ernst, D. (2010), Reinforcement Learning
and Dynamic Programming Using Function Approximators, CRC Press, New York.
Camacho, E. F., and Bordons, C. (2004), Model Predictive Control, Springer, New York.
Cant, L. (2006), “Life saving decisions: A model for optimal blood inventory management.”
Senior thesis, Department of Operations Research and Financial Engineering, Princeton
University.
Cao, X.-R. (2007), Stochastic Learning and Optimization, Springer, New York.
Chang, H. S., Fu, M. C., Hu, J., and Marcus, S. I. (2007), Simulation-Based Algorithms for
Markov Decision Processes, Springer, New York.
Chen, C. H. (1995), “An effective approach to smartly allocate computing budget for dis-
crete event simulation,” 34th IEEE Conference on Decision and Control, Vol. 34, New
Orleans, LA, pp. 2598–2603.
Chen, H. C., Chen, C. H., and Y¨ucesan, E. (2000), “Computing efforts allocation for ordinal
optimization and discrete event simulation,” IEEE Transactions on Automatic Control
45 (5), 960–964.
Chen, H. C., Chen, C. H., Dai, L., and Y¨ucesan, E. (1997), “New development of optimal
computing budget allocation for discrete event simulation,” Proceedings of the Winter
Simulation Conference, IEEE, Piscataway, NJ, pp. 334–341.
Chick, S. E., and Gans, N. (2009), “Economic analysis of simulation selection problems,”
Management Science 55 (3), 421–437.
Chick, S. E., and Inoue, K. (2001), “New two-stage and sequential procedures for selecting
the best simulated system,” Operations Research 49 (5), 732–743.

610
bibliography
Chick, S. E., Chen, C. H., Lin, J., and Y¨ucesan, E. (2000), “Simulation budget allocation
for further enhancing the efﬁciency of ordinal optimization,” Discrete Event Dynamic
Systems 10, 251–270.
Choi, D. P., and Van Roy, B. (2006), “A generalized kalman ﬁlter for ﬁxed point approx-
imation and efﬁcient temporal-difference learning,” Discrete Event Dynamic Systems
16, 207–239.
Chong, E. K. P. (1991), “On-line stochastic optimization of queueing systems,” PhD thesis,
Department of Electrical Engineering, Princeton University.
Chow, G. (1997), Dynamic Economics, Oxford University Press, New York.
Chung, K. L. (1974), A Course in Probability Theory, Academic Press, New York.
Clement, E., Lamberton, D., and Protter, P. (2002), “An analysis of a least squares regression
method for american option pricing,” Finance and Stochastics 17, 448–471.
Dantzig, G., and Ferguson, A. (1956), “The allocation of aircrafts to routes: An example of
linear programming under uncertain demand,” Management Science 3, 45–73.
Darken, C., and Moody, J. (1991), “Note on learning rate schedules for stochastic optimiza-
tion,” in R. P. Lippmann, J. Moody, and D. S. Touretzky, eds., Advances in Neural
Information Processing Systems 3, Morgan Kaufmann, San Mateo, CA, pp. 832–838.
Darken, C., and Moody, J. (1992), “Towards faster stochastic gradient search,” in J. Moody,
D. L. Hanson, and R. P. Lippmann, eds., Advances in Neural Information Processing
Systems 4, Morgan Kaufmann, San Mateo, CA, pp. 1009–1016.
Darken, C., Chang, J., and Moody, J. (1992), “Learning rate schedules for faster stochastic
gradient search,” Neural Networks for Signal Processing 2—Proceedings of the 1992
IEEE Workshop, IEEE Press, New York, pp. 3–12.
de Farias, D. P., and Van Roy, B. (2000), “On the existence of ﬁxed points for approximate
value iteration and temporal-difference learning,” Journal of Optimization Theory and
Applications, Springer, New York 105 (3), 589–608.
de Farias, D. P., and Van Roy, B. (2003), “The linear programming approach to approximate
dynamic programming,” Operations Research 51, 850–865.
Dearden, R., Friedman, N., and Andre, D. (1999), “Model-based bayesian exploration,”
Proceedings of the Fifteenth Conference on Uncertainty in Artiﬁcial Intelligence, Stock-
holm, pp. 150–159.
Dearden, R., Friedman, N., and Russell, S. (1998), “Bayesian Q-learning,” Proceedings of
the National Conference on Artiﬁcial Intelligence, John Wiley and Sons, Hoboken, NJ,
pp. 761–768.
DeGroot, M. H. (1970), Optimal Statistical Decisions, Wiley, New York.
Deisenroth, M. P., Peters, J., Rasmussen, C. E., and Conference, A. C. (2008), “Approximate
dynamic programming with gaussian processes,” Control. American Control Confer-
ence, IEEE Press, New York, pp. 4480–4485.
Denardo, E. V. (1982), Dynamic Programming, Prentice-Hall, Englewood Cliffs, NJ.
Derman, C. (1962), “On sequential decisions and markov chains,” Management Science 9,
16–24.
Derman, C. (1970), Finite State Markovian Decision Processes, Academic Press, New York.
Doob, J. L. (1953), Stochastic Processes, Wiley, New York.
Douglas, S. C., and Mathews, V. J. (1995), “Stochastic gradient adaptive step size algorithms
for adaptive ﬁltering,” Proceedings of the International Conference on Digital Signal
Processing, Limassol, Cyprus 1, 142–147.

bibliography
611
Dreyfus, S., and Law, A. M. (1977), The Art and Theory of Dynamic Programming, Aca-
demic Press, New York.
Duff, M. (2002), “Optimal learning: Computational procedures for Bayes-adaptive Markov
decision processes,” PhD thesis. Department of Computer Science, University of Mas-
sachusetts at Amherst.
Duff, M. O., and Barto, A. G. (1997), “Local bandit approximation for optimal learning
problems,” Advances in Neural Information Processing Systems 9, 1019.
Duff, M. O., and Barto, A. G. (2003), “Local bandit approximation for optimal learning prob-
lems,” Technical report, Department of Computer Science, University of Massachusetts,
Amherst, MA.
Dvoretzky, A. (1956), “On stochastic approximation,” in J. Neyman, ed., Proceedings 3rd
Berkeley Symposium on Mathematical Statistics and Probability, University of Califor-
nia Press, Berkeley, pp. 39–55.
Dynkin, E. B., and Yushkevich, A. A. (1979), Controlled Markov Processes, Springer-Verlag,
New York.
Engel, Y., Mannor, S., and Meir, R. (2005), “Reinforcement learning with Gaussian pro-
cesses,” Proceedings of the 22nd International Conference on Machine Learning, ACM
Press, New York, pp. 201–208.
Ermoliev, Y. (1988), “Stochastic quasigradient methods,” in Y. Ermoliev and R. Wets, eds.,
Numerical Techniques for Stochastic Optimization, Springer, Berlin.
Even-dar, E., and Mansour, Y. (2003), “Learning rates for Q-learning,” Journal of Machine
Learning Research 5, 1–25.
Fan, J., and Gijbels, I. (1996), Local Polynomial Modelling and Its Applications, Chapman
and Hall, London.
Farias, D., and Roy, B. (2001), “On constraint sampling for the linear programming approach
to approximate dynamic,” Mathematics of Operations Research 29 (3), 462–478.
Farias, D. P. D., and Van Roy, B. (2003), “The linear programming approach to approximate
dynamic programming,” Operations Research 51 (6), 850–865.
Ford, L. R., and Fulkerson, D. R. (1962), Flows in Networks, Princeton University Press,
Princeton, NJ.
Frank, H. (1969), “Shortest paths in probabilistic graphs,” Operations Research 17,
583–599.
Frazier, P. I., Powell, W. B., and Dayanik, S. (2008), “A knowledge gradient policy for
sequential information collection,” SIAM Journal on Control and Optimization 47 (5),
2410–2439.
Frazier, P. I., Powell, W. B., and Dayanik, S. (2009), “The knowledge-gradient policy for
correlated normal beliefs,” INFORMS Journal on Computing 21 (4), 599–613.
Frieze, A., and Grimmet, G. (1985), “The shortest path problem for graphs with random arc
lengths,” Discrete Applied Mathematics 10, 57–77.
Fu, M. C. (2002), “Optimization for simulation: Theory vs. practice,” INFORMS Journal on
Computing 14 (3), 192–215.
Fu, M. C. (2008), “What you should know about simulation and derivatives,” Naval Research
Logistics 55 (8), 723–736.
Fu, M. C., Hu, J.-Q., Chen, C.-H., and Xiong, X. (2007), “Simulation allocation for deter-
mining the best design in the presence of correlated sampling,” INFORMS Journal on
Computing 19, 101–111.

612
bibliography
Gaivoronski, A. (1988), “Stochastic quasigradient methods and their implementation,” in
Y. Ermoliev and R. Wets, eds., Numerical Techniques for Stochastic Optimization,
Springer, Berlin.
Gardner, E. S. (1983), “Automatic monitoring of forecast errors,” Journal of Forecasting 2,
1–21.
George, A. P., and Powell, W. B. (2006), “Adaptive stepsizes for recursive estimation with
applications in approximate dynamic programming,” Journal of Machine Learning 65
(1), 167–198.
George, A., Powell, W. B., and Kulkarni, S. (2008), “Value function approximation using
multiple aggregation for multiattribute resource management,” Journal of Machine
Learning Research 9, 2079–2111.
Gifﬁn, W. C. (1971), Introduction to Operations Engineering, Irwin, Homewood, IL.
Gittins, J. (1979), “Bandit processes and dynamic allocation indices,” Journal of the Royal
Statistical Society, Series B (Methodological) 41 (2), 148–177.
Gittins, J., and Jones, D. (1974), A Dynamic Allocation Index for the Sequential Design of
Experiments, North Holland, Amsterdam, pp. 241–266.
Gittins, J. C. (1981), “Multiserver scheduling of jobs with increasing completion times,”
Journal of Applied Probability 16, 321–324.
Gittins, J. C. (1989), Multi-armed Bandit Allocation Indices, Wiley, New York.
Gladyshev, E. G. (1965), “On stochastic approximation,” Theory of Probability and Its Appli-
cations 10, 275–278.
Glasserman, P. (1991), Gradient Estimation via Perturbation Analysis, Kluwer Academic,
Boston.
Golub, G. H., and Loan, C. F. V. (1996), Matrix Computations, John Hopkins University
Press, Baltimore, MD.
Goodwin, G. C., and Sin, K. S. (1984), Adaptive Filtering and Control, Prentice-Hall, Engle-
wood Cliffs, NJ.
Gordon, G. J. (1995), “Stable function approximation in dynamic programming,” Proceed-
ings of the 12th International Conference on Machine Learning, San Francisco, Morgan
Kaufmann, San Mateo, CA, pp. 261–268.
Gordon, G. J. (2001), “Reinforcement learning with function approximation converges to a
region,” Advances in Neural Information Processing Systems 13, 1040–1046.
Gosavi, A. (2003), Simulation-Based Optimization, Kluwer Academic, Norwell, MA.
Guestrin, C., Koller, D., and Parr, R. (2003), “Efﬁcient solution algorithms for factored
MDPs,” Journal of Artiﬁcial Intelligence Research 19, 399–468.
Gupta, S. S., and Miescke, K. J. (1996), “Bayesian look ahead one-stage sampling allocations
for selection of the best population,” Journal of Statistical Planning and Inference 54,
229–244.
Hastie, T., Tibshirani, R., and Friedman, J. (2009), The Elements of Statistical Learning:
Data Mining, Inference and Prediction, Springer, New York.
Haykin, S. (1999), Neural Networks: A Comprehensive Foundation, Prentice Hall, Engle-
wood Cliffs, NJ.
He, D., Chick, S. E., and Chen, C.-h. (2007), “Opportunity cost and OCBA selection
procedures in ordinal optimization for a ﬁxed number of alternative systems,” IEEE
Transactions on Systems Man and Cybernetics Part C-Applications and Reviews 37 (5),
951–961.

bibliography
613
Heuberger, P. S. C., Van den Hov, P. M. J., and Wahlberg, B., eds. (2005), Modeling and
Identiﬁcation with Rational Orthogonal Basis Functions, Springer, New York.
Heyman, D. P., and Sobel, M. (1984), Stochastic Models in Operations Research, Vol. II:
Stochastic Optimization, McGraw-Hill, New York.
Higle, J., and Sen, S. (1991), “Stochastic decomposition: An algorithm for two-stage linear
programs with recourse,” Mathematics of Operations Research 16 (3), 650–669.
Ho, Y.-C. (1992), Discrete Event Dynamic Systems: Analyzing Complexity and Performance
in the Modern World, IEEE Press, New York.
Holt, C. C., Modigliani, F., Muth, J., and Simon, H. (1960), Planning, Production, Inventories
and Work Force, Prentice-Hall, Englewood Cliffs, NJ.
Hong, J., and Nelson, B. L. (2006), “Discrete optimization via simulation using COMPASS,”
Operations Research 54 (1), 115–129.
Hong, L., and Nelson, B. L. (2007), “A framework for locally convergent random-search
algorithms for discrete optimization via Simulation,” ACM Transactions on Modeling
and Computer Simulation 17 (4), 1–22.
Howard, R. A. (1960), Dynamic Programming and Markov Process, MIT Press, Cambridge.
Infanger, G. (1994), Planning under Uncertainty: Solving Large-Scale Stochastic Linear
Programs, Boyd and Fraser, New York.
Jaakkola, T., Jordan, M. I., and Singh, S. (1994), “On the convergence of stochastic iterative
dynamic programming algorithms,” Neural Computation 6 (6), 1185–1201.
Judd, K. L. (1998), Numerical Methods in Economics, MIT Press, Cambridge.
Kaelbling, L. P. (1993), Learning in Embedded Systems, MIT Press, Cambridge.
Kaelbling, L. P., Littman, M., and Moore, A. W. (1996), “Reinforcement learning: A survey,”
Journal of Artiﬁcial Intelligence Research 4, 237–285.
Kall, P., and Mayer, J. (2005), Stochastic Linear Programming: Models, Theory, and Com-
putation, Springer, New York.
Kall, P., and Wallace, S. W. (1994), Stochastic Programming, Wiley, New York.
Kesten, H. (1958), “Accelerated stochastic approximation,” Annals of Mathematical Statistics
29, 41–59.
Kiefer, J., and Wolfowitz, J. (1952), “Stochastic estimation of the maximum of a regression
function,” Annals of Mathematical Statistics 23, 462–466.
Kim, S.-H., and Nelson, B. L. (2001), “A fully sequential procedure for indifference-zone
selection in simulation,” ACM Transactions on Modeling Computer Simulations, 11,
251–273.
Kim, S. H., and Nelson, B. L. (2006), Selecting the Best System, Elsevier, Amsterdam.
Kirk, D. E. (1998), Optimal Control Theory: An Introduction, Dover, New York.
Klabjan, D., and Adelman, D. (2007), “An inﬁnite-dimensional linear programming algo-
rithm for deterministic semi-Markov decision processes on Borel spaces,” Mathematics
of Operations Research 32, 528–550.
Kmenta, J. (1997), Elements of Econometrics, University of Michigan Press, Ann Arbor.
Konda, V. R., and Borkar, V. S. (1999), “Actor–critic-type learning algorithms for Markov
decision processes,” SIAM Journal on Control and Optimization 38, 94.
Konda, V. R., and Tsitsiklis, J. N. (2003), “On actor–critic algorithms,” SIAM Journal of
Control and Optimization 42 (4), 1143–1166.

614
bibliography
Kushner, H. J., and Clark, S. (1978), Stochastic Approximation Methods for Constrained and
Unconstrained Systems, Springer, New York.
Kushner, H. J., and Yin, G. G. (1997), Stochastic Approximation Algorithms and Applica-
tions, Springer, New York.
Kushner, H. J., and Yin, G. G. (2003), Stochastic Approximation and Recursive Algorithms
and Applications, Springer, New York.
Lagoudakis, M., and Parr, R. (2003), “Least-squares policy iteration,” Journal of Machine
Learning Research 4, 1149.
Lagoudakis, M., Parr, R., and Littman, M. (2002), “Least-squares methods in reinforcement
learning for control,” Methods and Applications of Artiﬁcial Intelligence 752.
Lai, T. L. (1987), “Adaptive treatment allocation and the multi-armed bandit problem,”
Annals of Statistics 15, 1091–1114.
Lai, T. L., and Robbins, H. (1985), “Asymptotically efﬁcient adaptive allocation rules,”
Advances in Applied Mathematics 6, 4–22.
Lambert, T., Smith, R., and Epelman, M. (2004), “Aggregation in stochastic dynamic
programming,” Technical report 04-07, Department of Industrial and Operations Engi-
neering, University of Michigan, Ann Arbor.
Landelius, T., and Knutsson, H. (1996), “Greedy adaptive critics for LQR problems: Con-
vergence proofs,” Technical report, Department of Electrical Engineering, Linkoping
University, Linkoping, Sweden.
Law, A. M., and Kelton, E. D. (2000), Simulation Modeling and Analysis, McGraw-Hill,
New York.
LeBlanc, M., and Tibshirani, R. (1996), “Combining estimates in regression and classiﬁca-
tion,” Journal of the American Statistical Association 91, 1641–1650.
Ljung, L., and Soderstrom, T. (1983), Theory and Practice of Recursive Identiﬁcation, MIT
Press, Cambridge.
L¨ohndorf, N., and Minner, S. (2010), “Optimal day-ahead trading and storage of renewable
energies: An approximate dynamic programming approach,” Energy Systems 1 (1),
1–17.
Longstaff, F., and Schwartz, E. S. (2001), “Valuing American options by simulation: A
simple least squares approach,” Review of Financial Studies 14, 113–147.
Ma, J., and Powell, W. B. (2010a), “Convergence analysis of kernel-based on-policy approx-
imate policy iteration algorithms for Markov decision processes with continuous mul-
tidimensional states and actions,” Technical report, Princeton University.
Ma, J., and Powell, W. B. (2010b), “Convergence analysis of on-policy LSPI for multi-
dimensional continuous state and action space MDPs and extension with orthogonal
polynomial approximation,” Technical report, Princeton University.
Manne, A. S. (1960), “Linear programming and sequential decisions,” Management Science
6 (3), 259–267.
Marbach, P., and Tsitsiklis, J. N. (2001), “Simulation-based optimization of Markov reward
processes,” IEEE Transactions on Automatic Control, 46 (2), 191–209.
Mayer, J. (1998), Stochastic Linear Programming Algorithms: A Comparison Based on a
Model Management System, Springer, New York.
McClain, J. O. (1974), “Dynamics of exponential smoothing with trend and seasonal terms,”
Management Science 20, 1300–1304.

bibliography
615
Melo, F. S., and Ribeiro, M. I. (2007), “Q-Learning with linear function approximation,”
Proceedings of the 20th Annual Conference on Learning Theory, Berlin, Springer,
pp. 308–322.
Menache, I., Mannor, S., and Shimkin, N. (2005), “Basis function adaptation in tem-
poral difference reinforcement learning,” Annals of Operations Research 134 (1),
215–238.
Mendelssohn, R. (1982), “An iterative aggregation procedure for Markov decision pro-
cesses,” Operations Research 30, 62–73.
Meyn, S. P. (1997), “The policy iteration algorithm for average reward Markov decision
processes with general state space,” IEEE Transactions on Automatic Control, 42 (12),
1663–1680.
Mirozahmedov, F., and Uryasev, S. P. (1983), “Adaptive stepsize regulation for stochastic
optimization algorithm,” Zurnal vicisl. mat. i. mat. ﬁz. 6 (23), 1314–1325.
Mulvey, J. M., and Ruszczynski, A. (1995), “A new scenario decomposition method for
large scale stochastic optimization,” Operations Research 43, 477–490.
Munos, R., and Szepesvari, C. (2008), “Finite-time bounds for ﬁtted value iteration,” Journal
of Machine Learning Research 1, 815–857.
Nascimento, J. M., and Powell, W. B. (2009), “An optimal approximate dynamic program-
ming algorithm for the lagged asset acquisition problem,” Mathematics of Operations
Research 34 (1), 210–237.
Nascimento, J. M., and Powell, W. B. (2010a), “An optimal approximate dynamic program-
ming algorithm for the energy dispatch problem with grid-level storage,” Technical
report, Princeton University.
Nascimento, J. M., and Powell, W. B. (2010b), “Dynamic programming models and algo-
rithms for the mutual fund cash balance problem,” Management Science 56 (5),
801–815.
Negoescu, D. M., Frazier, P. I., and Powell, W. B. (2010), “The knowledge-gradient algo-
rithm for sequencing experiments in drug discovery,” Informs Journal on Computing,
(in press), doi 10.1287.
Nelson, B. L., Swann, J., Goldsman, D., and Song, W. (2001), “Simple procedures for select-
ing the best simulated system when the number of alternatives is large,” Operations
Research 49, 950–963.
Nemhauser, G. L. (1966), Introduction to Dynamic Programming, Wiley, New York.
Neveu, J. (1975), Discrete Parameter Martingales, North Holland, Amsterdam.
Ormoneit, D., and Glynn, P. W. (2002), “Kernel-based reinforcement learning average-cost
problems,” IEEE Transactions on Automatic Control, 1624–1636.
Ormoneit, D., and Sen, S. (2002), “Kernel-based reinforcement learning,” Machine Learning
49, 161–178.
Papavassiliou, V. A., and Russell, S. (1999), “Convergence of reinforcement learning with
general function approximators,” International Joint Conference on Artiﬁcial Intelli-
gence, pp. 748–757.
Pearl, J. (1984), Heuristics: Intelligent Search Strategies for Computer Problem Solving,
Addison-Wesley, Upper Saddle River, NJ.
Pereira, M. V. F., and Pinto, L. M. V. G. (1991), “Multistage stochastic optimization applied
to energy planning,” Mathmatical Programming 52, 359–375.

616
bibliography
Pﬂug, G. C. (1988), “Stepsize rules, stopping times and their implementation in stochas-
tic quasi-gradient algorithms,” in Numerical Techniques for Stochastic Optimization,
Springer-Verlag, New York, pp. 353–372.
Pﬂug, G. C. (1996), Optimization of Stochastic Models: The Interface Between Simulation
and Optimization, Kluwer Academic, Boston.
Pollard, D. (2002), A User’s Guide to Measure Theoretic Probability, Cambridge University
Press, Cambridge, UK.
Porteus, E. L. (1990), Handbooks in Operations Research and Management Science: Stochas-
tic Models, Vol. 2, North Holland, Amsterdam.
Poupart, P., Vlassis, N., Hoey, J., and Regan, K. (2006), “An analytic solution to discrete
Bayesian reinforcement learning,” Proceedings of the 23rd International Conference on
Machine Learning, ACM Press, New York, pp. 697–704.
Powell, W. B., and Chen, Z.-L. (1999), “A convergent cutting-plane and partial-sampling
algorithm for multistage linear programs with recourse,” Journal of Optimization Theory
and Applications 103, 497–524.
Powell, W. B., and Cheung, R. K.-M. (2000), “SHAPE: A stochastic hybrid approximation
procedure for two-stage stochastic programs,” Operations Research 48, 73–79.
Powell, W. B., and Godfrey, G. (2002), “An adaptive dynamic programming algorithm for
dynamic ﬂeet management, I: Single period travel times,” Transportation Science 36
(1), 21–39.
Powell, W. B., and Godfrey, G. A. (2001), “An adaptive, distribution-free approximation
for the newsvendor problem with censored demands, with applications to inventory and
distribution problems,” Management Science 47 (8), 1101–1112.
Powell, W. B., and Topaloglu, H. (2003), “Stochastic programming in transportation and
logistics,” Handbooks in Operations Research and Management Science: Stochastic
Programming. Elsevier, New York, pp. 555–636.
Powell, W. B., and Topaloglu, H. (2006), “Dynamic-programming approximations for
stochastic time-staged integer multicommodity-ﬂow problems,” Informs Journal on
Computing 18 (1), 31.
Powell, W. B., and Van Roy, B. (2004), “Approximate dynamic programming for high
dimensional resource allocation problems,” in J. Si, A. G. Barto, W. B. Powell, and
D. W. II, eds., Handbook of Learning and Approximate Dynamic Programming, IEEE
Press, New York.
Powell, W. B., Shapiro, J., and Simao, H. P. (2001), “A representational paradigm for
dynamic resource transformation problems,” in C. Coullard, R. Fourer, and J. H. Owen,
eds., Annals of Operations Research on Modeling, 104, 231–279.
Powell, W. B., George, A., Lamont, A., and Stewart, J. (2011), “SMART: A stochastic mul-
tiscale model for the analysis of energy resources, technology and policy.” Informs
Journal on Computing. Available on http://citeseerx.ist.psu.edu/viewdoc/download?
doi=10.1.1.157.8434&rep=rep1&type=pdf.
Powell, W. B., Ruszczynski, A., and Topaloglu, H. (2004), “Learning algorithms for sep-
arable approximations of discrete stochastic optimization problems,” Mathematics of
Operations Research 29 (4), 814–836.
Powell, W. B., Shapiro, J. A., and Simao, H. P. (2002), “An adaptive dynamic programming
algorithm for the heterogeneous resource allocation problem,” Transportation Science
36 (2), 231–249.

bibliography
617
Precup, D., and Perkins, T. (2003), “A convergent form of approximate policy iteration,”
in S. Becker, S. Thrun, and K. Obermayer, eds., Advances in Neural Information Pro-
cessing Systems, The MIT Press, Cambridge, MA, pp. 1595–1602.
Precup, D., Sutton, R. S., and Dasgupta, S. (2001), “Off-policy temporal-difference learning
with function approximation,” Proceedings of the 18th International Conference on
Machine Learning, ACM Press, New York, pp. 417–424.
Psaraftis, H. N., and Tsitsiklis, J. N. (1993), “Dynamic shortest paths in acyclic networks
with Markovian arc costs,” Operations Research 41, 91–101.
Puterman, M. L. (2005), Markov Decision Processes, 2nd Ed., Wiley, Hoboken, NJ.
Robbins, H., and Monro, S. (1951), “A stochastic approximation method,” Annals of Math-
ematical Statistics 22 (3), 400–407.
Rockafellar, R. T., and Wets, R. J.-B. (1991), “Scenarios and policy aggregation
in optimization under uncertainty,” Mathematics of Operations Research 16 (1),
119–147.
Rogers, D., Plante, R., Wong, R., and Evans, J. (1991), “Aggregation and disaggrega-
tion techniques and methodology in optimization,” Operations Research 39, 553–
582.
Ross, S. (1983), Introduction to Stochastic Dynamic Programming, Academic Press, New
York.
Ross, S. R. (2002), Simulation, Academic Press, New York.
Rust, J. (1997), “Using randomization to break the curse of dimensionality,” Econometrica
65 (3), 487–516.
Ruszczynski, A. (1980), “Feasible direction methods for stochastic programming problems,”
Mathematical Programming 19, 220–229.
Ruszczynski, A. (1987), “A linearization method for nonsmooth stochastic programming
problems,” Mathematics of Operations Research 12, 32–49.
Ruszczynski, A. (1993), “Parallel decomposition of multistage stochastic programming prob-
lems,” Mathematical Programming 58, 201–228.
Ruszczynski, A. (2003), “Decomposition methods,” in A. Ruszczynski and A. Shapiro, eds.,
Handbook in Operations Research and Management Science: Stochastic Programming,
Elsevier, Amsterdam. pp. 141–212.
Ruszczynski, A., and Shapiro, A. (2003), Handbooks in Operations Research and Manage-
ment Science: Stochastic Programming, Vol. 10, Elsevier, Amsterdam.
Ruszczynski, A., and Syski, W. (1986), “A method of aggregate stochastic subgradients
with on-line stepsize rules for convex stochastic programming problems,” Mathematical
Programming Study 28, 113–131.
Ryzhov, I. O., and Powell, W. B. (2009), “A Monte Carlo knowledge gradient method for
learning abatement potential of emissions reduction technologies,” in M. Rossetti, R.
R. Hill, A. Dunkin, and R. G. Ingals, eds., Proceedings of the 2009 Winter Simulation
Conference, pp. 1492–1502.
Ryzhov, I. O., and Powell, W. B. (2010a), “Approximate dynamic programming with corre-
lated Bayesian beliefs,” Forty-Eighth Annual Allerton Conference on Communication,
Control, and Computing, Monticello, IL, IEEE Press, New York, pp. 1360–1367.
Ryzhov, I. O., and Powell, W. B. (2010b), “Information collection on a graph,” Operations
Research 2012.

618
bibliography
Ryzhov, I. O., Frazier, P. I., and Powell, W. B. (2009), “Stepsize selection for approxi-
mate value iteration and a new optimal stepsize rule,” Technical report, Department of
Operations Research and Financial Engineering, Princeton University.
Ryzhov, I. O., Powell, W. B., and Frazier, P. I. (2010), “The knowledge gradient algo-
rithm for a general class of online learning problems,” Technical report, Department of
Operations Research and Financial Engineering, Princeton University.
Samuel, A. L. (1959), “Some studies in machine learning using the game of checkers,” IBM
Journal of Research and Development 3, 211–229.
Schweitzer, P., and Seidmann, A. (1985), “Generalized polynomial approximations in
Markovian decision processes,” Journal of Mathematical Analysis and Applications
110, 568–582.
Secomandi, N. (2008), “An analysis of the control-algorithm resolving issue in inventory
and revenue management,” Manufacturing and Service Operations Management 10 (3),
468–483.
Sen, S., and Higle, J. (1999), “An introductory tutorial on stochastic linear programming
models,” Interfaces 29 (2), 33–6152.
Si, J., Barto, A. G., Powell, W. B., and Wunsch, D. (2004), Handbook of Learning and
Approximate Dynamic Programming, Wiley-IEEE Press, Hoboken, NJ.
Sigal, C. E., Pritsker, A. B., and Solberg, J. J. (1980), “The stochastic shortest route problem,”
Operations Research 28, 1122–1129.
Simao, H. P., Day, J., George, A. P., Gifford, T., Powell, W. B., and Nienow, J. (2009),
“An approximate dynamic programming algorithm for large-scale ﬂeet management: A
case application,” Transportation Science 43 (2), 178–197.
Simao, H. P., George, A., Powell, W. B., Gifford, T., Nienow, J., and Day, J. (2010),
“Approximate dynamic programming captures ﬂeet operations for Schneider National,”
Interfaces 40 (5), 1–11.
Singh, S., Jaakkola, T., Littman, M., and Szepesvari, C. (2000), “Convergence results for
single-step on-policy reinforcement-learning algorithms,” Machine Learning 38 (3),
287–308.
Singh, S. P., Jaakkola, T., and Jordan, M. I. (1995), “Reinforcement learning with soft
state aggregation,” in Advances in Neural Information Processing Systems, Vol. 7, MIT
Press, Cambridge, pp. 361–368.
Smola, A. J., and Sch¨olkopf, B. (2004), “A tutorial on support vector regression,” Statistics
and Computing 14 (3), 199–222.
Soderstrom, T., Ljung, L., and Gustavsson, I. (1978), “A theoretical analysis of recursive
identiﬁcation methods,” Automatica 78, 231–244.
Spall, J. C. (2003), Introduction to Stochastic Search and Optimization: Estimation, Simula-
tion and Control, Wiley, Hoboken, NJ.
Spivey, M. Z., and Powell, W. B. (2004), “The dynamic assignment problem,” Transporta-
tion Science 38 (4), 399–419.
Stengel, R. F. (1994), Optimal Control and Estimation, Dover, New York.
Stokey, N. L., and R. E. Lucas, J. (1989), Recursive Methods in Dynamic Economics, Harvard
University Press, Cambridge.
Sutton, R. S., and Barto, A. G. (1981), “Toward a modern theory of adaptive networks,”
Psychological Review 88 (2), 135–170.

bibliography
619
Sutton, R. S., and Barto, A. G. (1998), Reinforcement Learning, Vol. 35, MIT Press, Cam-
bridge.
Sutton, R. S., and Singh, S. P. (1994), “On step-size and bias in temporal-difference learn-
ing,” Proceedings of the Eighth Yale Workshop on Adaptive and Learning Systems, Yale
University, pp. 91–96.
Sutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver, D., Szepesvari, C., and
Wiewiora, E. (2009a), “Fast gradient-descent methods for temporal-difference learning
with linear function approximation,” in Proceedings of the 26th Annual International
Conference on Machine Learning, ACM Press, New York, pp. 1–8.
Sutton, R. S., Mcallester, D., Singh, S., Mansour, Y., Avenue, P., and Park, F. (2000),
“Policy gradient methods for reinforcement learning with function approximation,” in
A. Solla, T. K. Leen, and K. R. Muller, eds., Advances in Neural Information Processing
Systems, MIT Press, Cambridge, pp. 1057–1063.
Sutton, R. S., Szepesvari, C., and Maei, H. R. (2009), “A convergent O(n) algorithm for
off-policy temporal-difference learning with linear function approximation,” Advances
in Neural Information Processing Systems, 21, 1609–1616.
Szepesvari, C. (2010), Algorithms for Reinforcement Learning, Morgan and Claypool, San
Rafael, CA.
Szita, I. (2007), “Rewarding excursions: Extending reinforcement learning to complex
domains,” PhD thesis. Graduate School of Computer Science, Lorand University,
Budapest.
Taylor, H. (1967), “Evaluating a call option and optimal timing strategy in the stock market,”
Management Science 12, 111–120.
Taylor, H. M. (1990), Martingales and Random Walks, Vol. 2, Elsevier Science, Amsterdam.
Thrun, S. B. (1992), “The role of exploration in learning control,” in D. A. White, and D. A.
Sofge, eds., Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches,
Van Nostrand Reinhold Company, New York.
Topaloglu, H. (2001), “Dynamic programming approximations for dynamic resource allo-
cation problems,” PhD thesis. Department of Operations Research and Financial Engi-
neering, Princeton University.
Topaloglu, H., and Powell, W. B. (2003), “An algorithm for approximating piecewise
linear concave functions from sample gradients,” Operations Research Letters 31,
66–76.
Topkins, D. M. (1978), “Minimizing a submodular function on a lattice,” Operations
Research 26, 305–321.
Trigg, D. W. (1964), “Monitoring a forecasting system,” Operations Research Quarterly 15,
271–274.
Trigg, D. W., and Leach, A. G. (1967), “Exponential smoothing with an adaptive response
rate,” Operations Research Quarterly 18, 53–59.
Tsitsiklis, J. N. (1994), “Asynchronous stochastic approximation and Q-learning,” Machine
Learning 16, 185–202.
Tsitsiklis, J. N., and Van Roy, B. (1996), “Feature-based methods for large scale dynamic
programming,” Machine Learning 22 (1), 59–94.
Tsitsiklis, J. N., and Van Roy, B. (1997), “An analysis of temporal-difference learn-
ing with function approximation,” IEEE Transactions on Automatic Control 42,
674–690.

620
bibliography
Tsitsiklis, J. N., and Van Roy, B. (2001), “Regression methods for pricing complex american-
style options,” IEEE Transactions on Neural Networks 12, 694–703.
Tsitsiklis, J. N., Van Roy, B. (1997), “An analysis of temporal-difference learning with
function approximation,” IEEE Transactions on Automatic Control 42, 674–690.
Van Roy, B. (2001), “Neuro-dynamic programming: Overview and recent trends,” in
E. Feinberg and A. Shwartz, eds., Handbook of Markov Decision Processes: Methods
and Applications, Kluwer Academic, Boston, pp. 431–460.
Van Slyke, R., and Wets, R. (1969), “L-shaped linear programs with applications to opti-
mal control and stochastic programming,” SIAM Journal of Applied Mathematics 17,
638–663.
Venayagamoorthy, G. K., Harley, R. G., and Wunsch, D. G. (2002), “Comparison of heuristic
dynamic programming and dual heuristic programming adaptive critics for neurocontrol
of a turbogenerator,” IEEE Transactions in Neural Networks 13, 764–773.
Wallace, S. W. (1986a), “Decomposing the requirement space of a transportation problem
into polyhedral cones,” Mathematical Programming Study 28, 29–47.
Wallace, S. W. (1986b), “Solving stochastic programs with network recourse,” Networks
16, 295–317.
Wallace, S. W. (1987), “A piecewise linear upper bound on the network recourse function,”
Mathematical Programming 38, 133–146.
Wasan, M. T. (1969), Stochastic Approximation, Cambridge University Press, Cambridge,
UK.
Watkins, C. (1989), “Learning from delayed rewards,” PhD thesis, King’s College, Cam-
bridge, UK.
Watkins, C., and Dayan, P. (1992), “Q-Learning,” Machine Learning 8, 279–292.
Weber, R. (1992), “On the Gittins index for multiarmed bandits,” Annals of Applied Proba-
bility 2, 1024–1033.
Werbos, P. J. (1974), “Beyond regression: New tools for prediction and analysis in the
behavioral sciences,” PhD thesis, Harvard University.
Werbos, P. J. (1989), “Backpropagation and neurocontrol: A review and prospectus,” Inter-
national Joint Conference on Neural Networks, pp. 209–216.
Werbos, P. J. (1992a), “Approximate dynamic programming for real-time control and neural
modelling,” in D. A. White and D. A. Sofge, eds., Handbook of Intelligent Con-
trol: Neural, Fuzzy, and Adaptive Approaches, Van Nostrand Reinhold, New York,
pp. 493–525.
Werbos, P. J. (1992b), “Neurocontrol and supervised learning: An overview and evaluation,”
in D. A. White and D. A. Sofge, eds., Handbook of Intelligent Control, Von Nostrand
Reinhold, New York, pp. 65–89.
Werbos, P. J., Miller, W. T., and Sutton, R. S., eds. (1990), Neural Networks for Control,
MIT Press, Cambridge.
White, C. C. (1991), “A survey of solution techniques for the partially observable Markov
decision process,” Annals of Operations Research 32, 215–230.
White, D. A., and Sofge, D. A. (1992), Handbook of Intelligent Control: Neural, Fuzzy, and
Adaptive Approaches, Van Nostrand Reinhold, New York.
White, D. J. (1969), Dynamic Programming, Holden-Day, San Francisco.
Whitt, W. (1978), “Approximations of dynamic programs I,” Mathematics of Operations
Research, 3 (3) 231–243.

bibliography
621
Whittle, P. (1982), Optimization Over Time: Dynamic Programming and Stochastic Control,
Vols I and II, Wiley, New York.
Williams, R. J. (1992), “Simple statistical gradient-following algorithms for connectionist
reinforcement learning,” Machine Learning 8, 229–256.
Williams, R. J., and Baird, L. C. (1990), “A mathematical analysis of actor–critic archi-
tectures for learning optimal controls through incremental dynamic programming,” in
Sixth Yale Workshop on Adaptive and Learning Systems, New Haven, pp. 96–101.
Wu, C. (1997), “Rollout algorithms for combinatorial optimization,” Journal of Heuristics
3, 245–262.
Wu, T. T., Powell, W. B., and Whisman, A. (2009), “The optimizing-simulator,” ACM
Transactions on Modeling and Computer Simulation 19 (3), 1–31.
Yang, Y. (2001), “Adaptive regression by mixing,” Journal of the American Statistical Asso-
ciation, 96, 574–588.
Yao, Y. (2006), “Some results on the Gittins index for a normal reward process,” in H. Ho,
C. Ing, and T. Lai, eds., Time Series and Related Topics: In Memory of Ching-Zong
Wei, Institute of Mathematical Statistics, Beachwood, OH, pp. 284–294.
Young, P. (1984), Recursive Estimation and Time-Series Analysis, Springer, Berlin.
Y¨ucesan, E., Chen, C. H., Dai, L., and Chen, H. C. (1997), “A gradient approach for smartly
allocating computing budget for discrete event simulation,” in J. Charnes, D. Morrice,
D. Brunner, and J. Swain, eds., Proceedings of the 1996 Winter Simulation Conference,
IEEE Press, Piscataway, NJ, pp. 398–405.
Zipkin, P. (1980a), “Bounds for row-aggregation in linear programming,” Operations
Research 28, 903–916.
Zipkin, P. (1980b), “Bounds on the effect of aggregating variables in linear programming,”
Operations Research 28, 403–418.


Index
Actions, 187
Actor–critic, 408
Afﬁne function, 64
Aggregation, 290
modeling, 295
multiple levels, 299
Algorithm
ADP for asset acquisition, 545
ADP with exact expectation, 120
ADP for policy iteration, 404, 405
ADP using post-decision state, 141
ADP with pre-decision state, 388
approximate expectation, 128
approximate policy iteration
kernel regression, 415
least squares temporal differencing, 407
approximate policy iteration with VFA, 409
asynchronous dynamic programming, 152
bias-adjusted Kalman ﬁlter stepsize, 447
CUPPS algorithm, 524
double-pass ADP, 392
ﬁnite horizon policy evaluation, 338
generic ADP, 147
inﬁnite horizon generic ADP, 401
policy iteration, 74
policy search
indifference zone, 272
Q-learning
ﬁnite horizon, 390, 391
real-time dynamic programming, 127
relative value iteration, 70
roll-out policy, 225
SHAPE algorithm, 513
shortest path, 26, 27
Approximate Dynamic Programming: Solving the Curses of Dimensionality, Second Edition.
Warren B. Powell.
2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
SPAR, 504
stochastic decomposition, 523
synchronous dynamic programming,
152
temporal-difference learning for inﬁnite
horizon, 346
temporal difference learning with linear
model, 357
tree-search, 238
value iteration, 68, 69
Aliasing, 304
American option, 307
Apparent convergence, 450
Approximate policy iteration
kernel regression, 415
Approximate value iteration, 341
multidimensional decision vectors, 395
post-decision state, 389
pre-decision state, 386
Asset acquisition, 37, 38
ADP algorithm, 543
lagged, 39
variations, 544
Asset valuation, 36
Asynchronous dynamic programming, 150
Attribute transition function, 203
Backpropagation through time, 393
Bandit problems, 47, 470
Basis functions, 513
approximate linear programming, 411
geometric view, 312
Longstaff and Schwartz, 308
neural network, 320
623

624
index
recursive time-series, 355
tic-tac-toe, 311
Basis functions, 165, 304
Batch replenishment, 40, 79
Behavior policy, 389, 125
Belief state, 463
Bellman
functional equation, 4
Hamilton–Jacobi, 4
optimality equation, 4
recurrence equation, 4
Bellman error, 342
Bellman’s equation, 4, 38, 58
deterministic, 59
expectation form, 60
operator form, 64
standard form, 60
vector form, 61
Benders’ decomposition, 520
CUPPS algorithm, 524
stochastic decomposition, 523
Bias, 293
statistical error in max operator,
397
Blood management
ADP algorithm, 552
model, 548
Boltzmann exploration, 467, 483
Budgeting problem, continuous, 29
Contribution function, 206
Controls, 187
Cost function, 206
Cost-to-go function, 61
CUPPS algorithm, 523
Curses of dimensionality, 112
action space, 6
outcome space, 6
state space, 5
Cutting planes, 516
Decision node, 32
Decision tree, 32
Decisions, 187
Discount factors, 597
Double-pass algorithm, 392
Dynamic assignment problem, 43
Eligibility trace, 346
Epsilon-greedy, 483
Epsilon-greedy policy, 388
Error measures, 600
Exogenous information, 38, 51, 189
lagged, 192
outcomes, 191
scenarios, 191
Expected opportunity cost, 271
Experimental issues
discount factors, 597
starting, 598
Exploitation, 464
Exploration, 465
Exploration policies
Boltzmann exploration, 262, 483
epsilon-greedy, 483
epsilon-greedy exploration, 262
interval estimation, 483
interval exploration, 262
local bandit approximation, 484
persistent excitation, 483
pure exploitation, 261
pure exploration, 262
Exploration versus exploitation, 124, 152, 242,
257, 457
Exponential smoothing, 128
Factored representation of a state, 186
Finite horizon, for inﬁnite horizon models, 415
Flat representation of a state, 186
Forward dynamic programming, 114
Gambling problem, 34
Gittins exploration, 484
Gittins indices, 470
basic theory, 472
foundations, 470
normally distributed rewards, 474
Gradients, 498
Greedy policy, 221
Greedy strategy, 116
Indifference zone selection, 271
Inﬁnite horizon, 66
ﬁnite-horizon approximations, 415
policy iteration, 403
temporal-difference learning, 345
Information acquisition, 47
Initialization, 150
Interval estimation, 466, 483
k-nearest neighbor, 316
Kernel regression, 317, 415
LSTD, 413
Knowledge gradient, 477
Knowledge state, 463
L-shaped decomposition, 522
Lagged information, 192

index
625
Lasso regression, 314
Lattice, 81
Learning policies, 125
Boltzmann exploration, 467
exploitation, 464
exploration, 465
interval estimation, 466
persistent excitation, 465
upper conﬁdence bound sampling algorithm,
467
Learning rate schedules, 425
Learning strategies
epsilon-greedy exploration, 466
Gittins exploration, 484
Gittins indices, 470
knowledge gradient, 477
upper conﬁdence bound, 467
Least squares temporal difference, 363
Leveling algorithm, 502
Linear ﬁlter, 128
Linear operator, 64
Linear programming method
approximate, 411
exact, 78
Linear regression, 305
Longstaff and Schwartz, 307
recursive estimation
derivation, 377
multiple observations, 353
time-series, 354
recursive least squares
nonstationary data, 352
stationary data, 350
recursive methods, 349
stochastic gradient algorithm, 347
Local bandit approximation, 484
Local polynomial regression, 319
Longstaff and Schwartz, 307
Lookahead policy, 221, 224
LSTD, 363, 413
Markov decision processes, 57
max operator, 64
Measure-theoretic view of information, 211
min operator, 64
Model
contribution function, 122, 206
decisions, 187
elements of a dynamic program, 168
contribution function, 168
decision variable, 168
exogenous information, 168
state variable, 168
transition function, 168
exogenous information, 122
objective function, 206
policies, 221
resources, 174
multiple, 175
single discrete, 175
state, 178
time, 170
transition function, 122, 198
Modeling dynamic programs, 50
Monotone policies, 78, 79
proof of optimality, 97
Monte Carlo sampling, 117
Multi-armed bandit problem, 462
Myopic policy, 221, 224
Neural networks, 319
Nomadic trucker, 176
learning, 457
Nonparametric models, 316
k-nearest neighbor, 316
kernel regression, 317
local polynomial regression, 319
Objective function, 51, 58, 209
Ofﬂine learning, 462
Online learning, 462
Optimal computing budget allocation, 273
Optimality equation, 58
post-decision state, 138
proof, 85
Optimistic policy iteration, 404
Ordinal optimization, 271
Outcome node, 32
Outcomes, 191
Partial policy evaluation, 404
Partially observable states, 185
Persistent excitation, 465, 483
Physical state, 463
myopic, 224
Policies, 197, 221
behavior, 125, 389
Boltzmann exploration, 244
epsilon greedy, 388
GLIE, 243
greedy, 116, 221
learning, 125
lookahead, 221, 224
myopic, 221
optimistic, 404
partial, 404
policy function approximation, 221
randomized, 242

626
index
Policies (Continued)
roll-out heuristic, 225
rolling horizon procedure, 227
deterministic, 227
discounted, 231
stochastic, 229
sampling, 125, 389
soft max, 244
target, 125
tree search, 225
value function approximation, 221
Policy function approximation, 221
Policy iteration, 74
inﬁnite horizon, 403
Post-decision state, 181
optimality equations, 138
perspective, 130
Post-decision state variable, 129
Q-learning, 122, 389
Randomized policies, 96, 242
Ranking and selection, 462
Real-time dynamic programming, 126
Resource allocation
asset acquisition, 541
blood management, 547
ﬂeet management, 573
general model, 560
portfolio optimization, 557
trucking application, 580
Resources
multiple, 175
nomadic trucker, 176
single discrete, 175
Reward function, 206
Ridge regression, 314
Roll-out heuristic, 225
Rolling horizon procedure, 227
RTDP, 126
Sample path, 116
Sampling policy, 125, 389
SARSA, 124
Scenarios, 191
SHAPE algorithm, 509
proof of convergence, 528
Sherman–Morrison, 379
Shortest path
deterministic, 2, 26
information collecting, 50
stochastic, 33
SPAR
projection, 504
weighted projection, 504
SPAR algorithm, projection operation, 534
State
alias, 304
asset pricing, 36
bandit problem, 48
belief, 463
budgeting problem, 28
deﬁnition, 50, 178
dynamic assignment problem, 46
factored, 186
ﬂat, 186
gambling problem, 34
hyperstate, 464
knowledge, 463
multidimensional, 112, 134
partially observable, 185
physical, 463
post-decision, 129, 181
pre-decision, 129
resource state, 38, 39
sampling strategies, 150
shortest path, 27
state of knowledge, 48
transformer replacement, 43
State variable, deﬁnition, 179
States of a system
belief state, 180
information state, 180
physical state, 180
Stepsize, 127
apparent convergence, 451
bias and variance, 293
bias-adjusted, 447
convergence conditions, 426
deterministic, 425
1/n, 430
constant, 428
harmonic, 430
McClain, 431
polynomial learning rate, 430
search-then-converge, 431
inﬁnite horizon, bounds, 423
optimal
nonstationary I, 440
nonstationary II, 442
stationary, 438
stochastic, 433
convergence conditions, 434
Kesten’s rule, 436
stochastic gradient adaptive stepsize, 436
Trigg, 436

index
627
Stepsize rule, 425
Stochastic approximation
Martingale proof, 279
older proof, 276
Stochastic approximation procedure, 254
Stochastic decomposition, 523
Stochastic gradient, 252
Stochastic gradient algorithm, 254
Stochastic programming, 516
Benders, 520
Submodular, 82
Subset selection, 271
Superadditive, 83
Supermodular, 82
Supervisor, 311
Supervisory learning, 311
Supervisory processes, 196
Support vector regression, 314
Synchronous dynamic programming, 150
System model, 39
Target policy, 125
Temporal difference (TD) learning, 341, 344
inﬁnite horizon, 345
linear models
off-policy, 358
on-policy, 358
nonlinear models
on-policy, 358
Tic-tac-toe, 310
Time, 170
Trajectory following, 388
Transformer replacement, 42
Transition function, 3, 28, 39, 40, 51, 198
attribute transition, 203
batch, 41
resource transition function, 201
special cases, 204
Transition matrix, 57, 63
Tree search, 225
Two-stage stochastic program, 516
Upper conﬁdence bound, 476
Upper conﬁdence bound sampling algorithm, 467
Value function, 61, 62
Value function approximation, 114, 144, 221, 235
aggregation, 290
cutting planes, 516
error measures, 600
gradients, 498
leveling, 502
linear approximation, 499
mixed strategies, 324
neural networks, 319
piecewise linear, 501
recursive methods, 349
regression, 304
regression methods, 513
SHAPE algorithm, 509
SPAR, 504
tic-tac-toe, 310
Value iteration, 68, 69
bound, 72
error bound, 95
monotonic behavior, 71
pre-decision state, 388
proof of convergence, 89
proof of monotonicity, 93
relative value iteration, 70
stopping rule, 69
Variance of estimates, 293













Robert Jonsson
Exercises in Statistical Inference
with detailed solutions
Download free books at

2 
 
Robert Jonsson
Exercises in Statistical Inference with 
detailed solutions
Download free eBooks at bookboon.com

3 
 
Exercises in Statistical Inference with detailed solutions
1st edition
© 2014 Robert Jonsson & bookboon.com
ISBN 978-87-403-XXXX-X
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
4 
Contents
Contents
	
About the author	
7
1	
Introduction	
8
1.1	
Purpose of this book	
8
1.2	
Chapter content and plan of the book	
8
1.3	
Statistical tables and facilities	
10
2	
Basic probability and mathematics	
12
2.1	
Probability distributions of discrete and continuous random variables	
12
2.2	
Some distributions	
17
2.3	
Mathematics	
27
2.4	
Final words	
33
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Exercises in Statistical Inference  
with detailed solutions
5 
Contents
3	
Sampling Distributions	
34
3.1	
Some exact sampling distributions	
35
3.2	
Sample moments	
37
3.3	
Asymptotic and approximate results in sampling theory	
39
3.4	
Final words	
44
4	
Point estimation	
46
4.1	
Concepts	
46
4.2	
Requirements on estimators	
49
4.3	
Estimation methods	
51
4.4	
Final words	
61
5	
Interval estimation	
64
5.1	
Concepts	
64
5.2	
CIs in small samples by means of pivotal statistics	
65
5.3	
Approximate CIs in large samples based on Central Limit Theorems	
71
5.4	
Some further topics	
74
5.5	
Final words	
79
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Exercises in Statistical Inference  
with detailed solutions
6 
Contents
6	
Hypothesis Testing	
82
6.1	
Concepts	
82
6.2	
Methods of finding tests	
86
6.3	
The power of normally distributed statistics	
120
6.4	
Adjusted p-values for simultaneous inference	
125
6.5	
Randomized tests	
127
6.6	
Some tests for linear models	
128
6.7	
Final wjords	
144
	
Answers to Supplementary Exercises	
158
	
References	
197
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Exercises in Statistical Inference  
with detailed solutions
7 
About the author
About the author
Robert Jonsson got his Ph.D. in Statistics from the Univ. of Gothenburg, Sweden, in 1983. He has 
been doing research as well as teaching undergraduate and graduate students at Dept. of Statistics 
(Gothenburg), Nordic School of Public Health (Gothenburg) and Swedish School of Economics (Helsinki, 
Finland). His researches cover theoretical statistics, medical statistics and econometrics that in turn have 
given rise to 14 articles in refereed international journals and some dozens of national papers. Teaching 
experience reaches from basic statistical courses for undergraduates to Ph.D. courses in Statistical 
Inference, Probability and Stochastic processes. 
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
8 
Introduction
1	 Introduction
1.1	
Purpose of this book
The book is designed for students in statistics at the master level. It focuses on problem solving in the 
field of statistical inference and should be regarded as a complement to text books such as Wackerly et al 
2007, Mathematical Statistics with Applications or Casella & Berger 1990, Statistical Inference. The author 
has noticed that many students, although being well aware of the statistical ideas, fall short when being 
faced with the task of solving problems. This requires knowledge about statistical theory, but also about 
how to apply proper methodology and useful tricks. It is the aim of the book to bridge the gap between 
theoretical knowledge and problem solving.
Each of the following chapters contains a minimum of the theory needed to solve the problems in the 
Exercises. The latter are of two types. Some exercises with solutions are interspersed in the text while 
others, called Supplementary Exercises, follow at the end of the chapter. The solutions of the latter are 
found at the end of the book. The intention is that the reader shall try to solve these problems while 
having the solutions of the preceding exercises in mind. Towards the end of the following chapters there 
is a section called ‘Final Words’. Here some important aspects are considered, some of which might have 
been overlooked by the reader. 
1.2	
Chapter content and plan of the book
Emphasis will be on the kernel areas of statistical inference: Point estimation – Confidence Intervals – 
Test of hypothesis. More specialized topics such as Prediction, Sample Survey, Experimental Design, 
Analysis of Variance and Multivariate Analysis will not be considered since they require too much space 
to be accommodated here. Results in the kernel areas are based on probability theory. Therefore we 
first consider some probabilistic results, together with useful mathematics. The set-up of the following 
chapters is as follows.
•	 Ch. 2 Basic properties of discrete and continuous (random) variables are considered and examples 
of some common probability distributions are given. Elementary pieces of mathematics are 
presented, such as rules for derivation and integration. Students who feel that their prerequisites 
are insufficient in these topics are encouraged to practice hard, while others may skip much 
of the content of this chapter.
•	 Ch. 3 The chapter is mainly devoted to sampling distributions, i.e. the distribution of quantities 
that are computed from a sample such as sums and variances. In more complicated cases 
methods are presented for obtaining asymptotic or approximate formulas. Results from this 
chapter are essential for the understanding of results that are derived in the subsequent chapters.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
9 
Introduction
•	 Ch. 4 Important concepts in point estimation are introduced, such as likelihood of a sample 
and sufficient statistics. Statistics used for point estimation of unknown quantities in the 
population are called estimators. (Numerical values of the latter are called estimates.) Some 
requirements on ‘good’ estimators are mentioned, such as being unbiased, consistent and having 
small variance. Four general methods for obtaining estimators are presented: Ordinary Least 
Squares (OLS), Moment, Best Linear Unbiased Estimator (BLUE) and Maximum Likelihood 
(ML). The performance of various estimators is compared. Due to limited space other estimation 
methods have to be omitted.
•	 Ch. 5 The construction of confidence intervals (CIs) for unknown parameters in the population 
by means of so called pivotal statistics is explained. Guide lines are given for determining the 
sample size needed to get a CI of certain coverage probability and of certain length. It is also 
shown how CIs for functions of parameters, such as probabilities, can be constructed.
•	 Ch. 6 Two alternative ways of testing hypotheses are described, the p-value approach and the 
rejection region (RR) approach. When a statistic is used for testing hypotheses it is called a test 
statistic. Two general principles for constructing test statistics are presented, the Chi-square 
principle and the Likelihood Ratio principle. Each of these gives raise to a large number of 
well-known tests. It’s therefore a sign of statistical illiteracy when referring to a test as the Chi-
Square test (probably supposed to mean the well-known test of independency between two 
qualitative variables). Furthermore, some miscellaneous methods are presented. A part of the 
chapter is devoted to nonparametric methods for testing goodness-of-fit, equality of two or 
more distributions and Fisher’s exact test for independency.
A general expression for the power (ability of a test to discriminate between the alternatives) 
is derived for (asymptotically) normally distributed test statistics and is applied to some 
special cases. 
When several hypotheses are tested simultaneously, we increase the probability of rejecting a 
hypothesis when it in fact is true. (This is one way to ‘lie’ when using statistical inference, more 
examples are given in the book.) One solution of this problem, called the Bonferroni-Holm 
correction is presented.
We finally give some tests for linear models, although this topic perhaps should require their 
own book. Here we consider the classical Gauss-Markov model and simple cases of models 
with random coefficients.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
10 
Introduction
From the above one might get the impression that statistical testing is a more ‘important’ in some sense 
than point and interval estimation. This is however not the case. It has been noticed that good point 
estimators also work well for constructing good CIs and good tests. (See e.g. Stuart et al 1999, p. 276.) A 
frequent question from students is: Which is best, to make a CI or to make a test? A nice answer to this 
somewhat controversial question can be found in an article by T. Wonnacott, 1987. He argues that in 
general a CI is to be preferred in front of a test because a CI is more informative. For the same reason he 
argues for a p-value approach in front of a RR approach. However, in practice there are situations where 
the construction of CIs becomes too complicated. Also the computation of p-values may be complicated. 
E.g. in nonparametric inference (Ch. 6.2.4) it is often much easier to make a test based on the RR approach 
than to use the p-value approach. The latter in turn being simpler than making a CI. An approach based 
on testing is also much easier to use when several parameters have to be estimated simultaneously.
1.3	
Statistical tables and facilities
A great deal of the problem solving is devoted to computation of probabilities. For continuous variables 
this means that areas under frequency curves have to be computed. To this end various statistical tables 
are available. When using these there are two different quantities of interest.
--
Given a value on the x-axis, what is the probability of a larger value, i.e. how large is the area 
under the curve above the value on the x-axis? This may be called computation of a p-value.
--
Given a probability, i.e. an area under curve, what is the value on the x-axis that produced the 
probability? This may be called computation of an inverse p-value.
Statistical tables can show lower-tail areas or upper-tail areas. Lower-tail areas are areas below values 
on the x-axis and upper-tail areas are areas above. The reader should watch out carefully whether it is 
required to search for a p-value or an inverse p-value and whether the table show lower-or upper-tail 
areas. This seems to actually be a stumbling block for many students. It may therefore be helpful to 
remember some special cases for the normal-, Student’s T-, Chi-square- and F-distributions. (These will 
be defined in Ch. 2.2.2 and Ch. 3.1.) The following will serve as hang-ups:
--
In the normal distribution the area under curve above 1.96 is 0.025. The area under curve 
below 1.96 is thus 1-0.025=0.975.
--
In Student’s T distribution one needs to know the degrees of freedom (df) in order to determine 
the areas. With df = 1 the area under curve above 12.706 is 0.025.
--
In the Chi-square distribution with df = 1 the area under curve above e
2)
96
.1(
84
.3
|
 is 
05
.0
025
.0
2
 

.
--
In the F distribution one needs to know a pair of degrees of freedoms sometimes denoted 
)
,
(
r)
denominato
numerator,
(
2
1 f
f
 
. With
2
1
1
f
f
=
=
the area under curve above 161.45
2)
706
.
12
(
|
is 0.025.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
11 
Introduction
Calculation of probabilities is facilitated by using either statistical program packages, so called ‘calculators’ 
or printed statistical tables.
•	 Statistical program packages. These are the most reliable ones to use and both p-values and 
inverse p-values can easily be computed by using programs such as SAS or SPSS, just to 
mention a few ones. E.g. in SAS the function probt can be used to find p-values for Student’s 
T distribution and the function tinv to find inverse p-values. However, read manuals carefully.
•	 ‘Calculators’. These have quite recently appeared on the internet. They are easy to use 
(enter a value and click on ‘calculate’) and they are often free. Especially the calculation 
of areas in the F-distribution may be facilitated. An example is found under the address  
http://vassarstats.net/tabs.html.
•	 Printed tables. These are often found in statistical text books. Quality can be uneven, but 
an example of an excellent table is the table over the Chi-square distribution in Wackerly 
et al, 2007. This shows both small lower-tail areas and small upper-tail areas. Many tables 
can be downloaded from the internet. One example from the University of Glasgow is  
http://www.stats.gla.ac.uk.
Throughout this book we will compute exact probabilities obtained from functions in the program packet 
SAS. However, it is frequently enough to see whether a p-value is above or below 0.05 and in such cases 
it will suffice to use printed tables.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
12 
Basic probability and mathematics
2	 Basic probability 
and mathematics
2.1	
Probability distributions of discrete and continuous random variables
A variable that is dependent on the outcome of an experiment (in a wide sense) is called a random 
variable (or just variable) and is denoted by an upper case letter, such as Y. A particular value taken by 
Y is denoted by a lower case letter y. For example, let Y = ‘Number of boys in a randomly chosen family 
with 4 children’, where Y may take any of the values y = 0,…,4. Before the ‘experiment’ of choosing such 
a family we do not know the value of y. But, as will be shown below, we can calculate the probability 
that the family has y boys. The probability of the outcome ‘Y = y’ is denoted 
)
(
y
Y
P
=
 and since it is a 
function of y it is denoted
)
(y
p
. This is called the probability function (pf) of the discrete variable Y. A 
variable that can take any value in some interval, e.g. waiting time in a queue, is called continuous. The 
latter can be described by the density (frequency function) of the continuous variable Y, 
)
(y
f
. The latter 
shows the relative frequency of values close to y.
Properties of p(y) (If not shown, summations are over all possible values of y.)
1)	
∑
=
≤
≤
1
)
(
 ,1
)
(
0
y
p
y
p
2)	 Expected value, Population mean, of Y : 
∑
⋅
=
=
)
(
)
(
y
p
y
Y
E
µ
, center of gravity.
3)	 Expected value of a function of Y: 
∑
⋅
=
)
(
)
(
))
(
(
y
p
y
g
Y
g
E
.
4)	 (Population) Variance of Y:
∑
−
=
⋅
−
=
=
2
2
2
2
)
(
)
(
)
(
)
(
µ
µ
σ
Y
E
y
p
y
Y
V
, dispersion around 
population mean. The latter expression is often simpler for calculations. Notice that (3) is used 
with
2)
(
)
(
µ
−
= y
y
g
. 
5)	 Cumulative distribution function (cdf) of Y. 
...
)1
(
)
(
)
(
)
(
+
−
+
=
≤
=
y
p
y
p
y
Y
P
y
F
and Survival 
function
)
(
1
...
)
2
(
)1
(
)
(
)
(
y
F
y
p
y
p
y
Y
P
y
S
−
=
+
+
+
+
=
>
=
.
Properties of f(y) (If not shown, integration is over all possible values of y.)
1)	
)
(
1
)
(
,
)
(
)
(
,1
)
(
,0
)
(
y
F
y
S
dx
x
f
y
F
dy
y
f
y
f
y
y

 
 
 
t ³
³
f
 
.
2)	
³

 
 
dy
y
f
y
Y
E
)
(
)
(
P
, center of gravity.
3)	 Expected value of a function of Y, g(Y): 
³

 
 
dy
y
f
y
g
Y
g
E
)
(
)
(
))
(
(
P
.
4)	 (Population) Variance of Y: 
³

 

 
 
2
2
2
2
)
(
)
(
)
(
)
(
P
P
V
Y
E
dy
y
f
y
Y
V
.
5)	 Cumulative distribution function (cdf) of Y. 
³
f

 
d
 
y
dx
x
f
y
Y
P
y
F
)
(
)
(
)
(
 and Survival function 
³
f
 
!
 
y
dx
x
f
y
Y
P
)
(
)
(
.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
13 
Basic probability and mathematics
6)	 The Population median, M, is obtained by solving the equation 
2
/
1
)
(
=
M
F
for M.
One may define a median also for a discrete variable, but this can cause problems when trying 
to obtain an unique solution. We illustrate these properties in two elementary examples. The 
mathematics needed to solve the problems is found in Section 2.2.3.
EX 1 You throw a symmetric six-sided dice and define the discrete Y = ‘Number of dots that comes up’. The pf of Y is 
obviously
6
,...,
1
 ,6
/
1
)
(
=
=
y
y
p
.
1)	
1
6
1
6
6
1
)
(
6
1
=
⋅
=
=
∑
∑
=
y
y
p
, 
2)	
2
7
2
)1
6
(
6
6
1
6
1
)
(
)
(
6
1
=
+
⋅
⋅
=
⋅
=
⋅
= ∑
∑
=
y
y
y
p
y
Y
E
3)	
6
91
6
)1
6
2
)(
1
6
(
6
6
1
6
1
)
(
)
(
6
1
2
2
2
 




 

 

 ¦
¦
 
y
y
y
p
y
Y
E
4)	
12
35
2
7
6
91
)
(
)
(
2
2
2
 
¸
¹
·
¨
©
§

 

 
P
Y
E
Y
V
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Exercises in Statistical Inference  
with detailed solutions
14 
Basic probability and mathematics
EX2 You arrive at a bus stop where buses run every ten minutes. Define the continuous variable Y = ‘Waiting time for 
the next bus’. The density can be assumed to be 
10
0
,
10
/
1
)
(
d
d
 
y
y
f
.
1)	
1
6
1
6
6
1
)
(
6
1
 

 
 
¦
¦
 
y
y
p
2)	
5
2
0
100
.
10
1
2
10
1
10
1
)
(
)
(
10
0
2
10
0
 
¸
¹
·
¨
©
§

 
»
¼
º
«
¬
ª
 

 

 ³
³
y
dy
y
dy
y
f
y
Y
E
3)	
3
100
3
0
1000
10
1
3
10
1
10
1
)
(
)
(
10
0
3
10
0
2
2
2
 
¸
¹
·
¨
©
§

 
»
¼
º
«
¬
ª
 

 

 ³
³
y
dy
y
dy
y
f
y
Y
E
4)	
3
25
5
3
100
)
(
)
(
2
2
2
 

 

 
P
Y
E
Y
V
5)	
10
10
1
)
(
0
y
dx
y
F
y
 
 ³
. So, 
5
2
1
10
)
(
 

 
 
M
M
M
F
. 
Here the median equals the mean and this is always the case when the density is symmetric around the mean.
One may calculate probabilities such as the probability of having to wait more than 8 minutes, 
5
1
)
8
10
(
10
1
10
1
)
8
(
10
8
 

 
 
!
³
dy
Y
P
More generally, 
)
(
r
r
Y
E
=
α
is the rth moment and 
(
)
r
r
Y
E
)
(
µ
µ
−
=
the rth central moment, r = 1,2,….
A bivariate random variable Y consists of a pair of variables 
)
,
(
2
1 Y
Y
. If the latter are discrete the pf of Y 
is 
)
(
)
,
(
2
2
1
1
2
1
y
Y
y
Y
P
y
y
p
=
∩
=
=
, i.e. the probability of the simultaneous outcome. Given that 
2
2
y
Y =
the conditional probability of 
1Y is (
)
(
)
2
2
1
1
2
1
y
Y
y
Y
P
y
y
p
=
=
=
.
Properties of 
2
y
y
p
,
(
1
) (If not shown, summations are over all possible values of 
2
1
 
and
 
y
y
)
1)	
∑∑
=
≤
≤
1
)
,
(
 ,1
)
,
(
0
2
1
2
1
y
y
p
y
y
p
.
2)	
 )
(
 
and
 )
(
 ,)
(
)
,
(
 ),
(
)
,
(
2
1
2
2
1
1
2
1
1
2
y
p
y
p
y
p
y
y
p
y
p
y
y
p
y
y
∑
∑
=
=
are marginal pfs.
3)	
(
)
(
)
)
(
)
,
(
 ,
)
(
)
,
(
1
2
1
1
2
2
2
1
2
1
y
p
y
y
p
y
y
p
y
p
y
y
p
y
y
p
=
=
.
4)	
(
)
1
)
(
)
(
1
)
,
(
)
(
1
2
2
2
1
2
2
1
1
1
=
⋅
=
=
∑
∑
y
p
y
p
y
y
p
y
p
y
y
p
y
y
.
5)	
2
1
 
and
 
Y
Y
are independent if 



)
(
)
(
)
,
(
or 
)
(
or 
)
(
2
1
2
1
2
1
2
1
2
1
y
p
y
p
y
y
p
y
p
y
y
p
y
p
y
y
p

 
 
 
.
6)	
(
) ∑∑
⋅
⋅
=
⋅
)
,
(
)
(
)
(
)
(
)
(
2
1
2
1
2
1
y
y
p
y
h
y
g
Y
h
Y
g
E
.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
15 
Basic probability and mathematics
7)	 Covariance between 
:
 
and
 
2
1
Y
Y
	
¦¦

 



 
 
2
1
2
1
2
1
2
2
1
1
2
1
12
)
(
)
,
(
)
)(
(
)
,
(
P
P
P
P
V
Y
Y
E
y
y
p
y
y
Y
Y
Cov
.
 Notice that 
)
,
(
1
1
11
Y
Y
Cov
 
V
 is simply the variance of 
1Y .
8)	 Correlation between 
2
1
 
and
 
Y
Y
:
)
(
and
)
(
 where
,
2
2
2
1
2
1
2
1
12
12
Y
V
Y
V
 
 

 
V
V
V
V
V
U
 Notice that 
1
1
12 d
d

U
.
9)	 The conditional expected value
(
)
(
)
∑
⋅
=
=
=
1
2
1
1
2
2
1
2
1
y
y
y
p
y
y
Y
Y
E
µ
is termed the regression 
function. If this is a linear function of 
2y the regression is linear, 
2y
⋅
+ β
α
, where α is an 
intercept and β  is the slope or regression coefficient.
10)	The conditional variance (
)
(
)
∑
=
⋅
−
=
=
2
1
2
2
1
1
2
2
1
)
(
y
y
p
y
y
Y
Y
V
µ
 
(
)
2
2
1
2
2
2
1
µ
−
= y
Y
Y
E
is the residual variance.
More generally, a n-dimensional random variable Y has n components 
)
,...,
( 1
n
Y
Y
and the pf is 
)
...
(
)
,...,
(
1
1
1
n
n
n
y
Y
y
Y
P
y
y
p
=
∩
∩
=
=
. This can represent the outcomes in a sample of n observations. 
Assume for instance that we have chosen a sample of n families, each with 4 children. Define the variable 
iY = ‘Number of boys in family i’, i = 1…n. In this case it may be reasonable to assume that the number 
of boys in one chosen family is independent of the number of boys in another family. The probability of 
the sample is thus
∏
=
=
⋅
⋅
=
n
i
i
n
n
y
p
y
p
y
p
y
y
p
1
1
1
)
(
)
(
...
)
(
)
,...,
(

(1a) 
If furthermore each 
iY  has the same pf we say that the sequence ( )n
i
iY
1
=  is identically and independently 
distributed (iid).
Similar relations hold for n-dimensional continuous variables. For n independent variables the joint 
density is
∏
=
=
n
i
i
n
y
f
y
y
f
1
1
)
(
)
,...,
(

(1b)
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
16 
Basic probability and mathematics
Linear form of random variables
Let ( )n
i
iY
1
= be variables with 
LM
M
L
LL
L
L
L
<
<
&RY
<
9
<
(
V
V
P
 
 
 




DQG






 A linear form of the 
s
Y j '
is 
∑
=
=
n
i
i
iY
a
L
1
, where the 
ia are constants. It is easy to show the following (Wackerly, Mendenhall 
&Scheaffer 2008, p. 271)
¦¦
¦
¦
d

d
 
 

 
 
Q
M
L
LM
M
L
Q
L
LL
L
Q
L
L
L
D
D
D
/
9
D
/
(









V
V
P

(2)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Exercises in Statistical Inference  
with detailed solutions
17 
Basic probability and mathematics
Consider e.g. the case n = 3 in which case 
23
3
2
13
3
1
12
2
1
3
1
V
V
V
V
a
a
a
a
a
a
a
a
j
i
ij
j
i


 
¦¦
d

d
. We illustrate 
the use of eq. (2) below.
EX3 Variance of a sum and of a difference.
>
@
>
@
12
22
11
2
1
2
1
12
22
11
2
1
2
1
2
1
,1
)
(
,
2
1
)
(
V
V
V
V
V
V


 

 
 
 



 
 
 
 

a
a
Y
Y
V
a
a
Y
Y
V
Assume further that 
2
22
11
V
V
V
 
 
 say. Then 
2
12
12
V
V
U
 
 and it follows that
)
1(
2
)
(
12
2
2
1
U
V

 
 Y
Y
V
 and 









U
V

 
<
<
9
.
This last equation is interesting because it shows that the variance in data with positively correlated observations can 
be reduced by forming differences. In fact 
0
)
(
2
1
→
−Y
Y
V
as 
1
12 o
U
. A typical example of positively correlated 
observations is in ‘before-after’ studies, e.g. when body weight is measured for each person before and after a 
slimming program.
2.2	
Some distributions
Many discrete and continuous distributions have been found to be workable models for several important 
practical situations. Such distributions have been termed ‘families of distributions’ or ‘distributional laws’. 
In this section we catalog some of these and give the basic assumptions on which they are based. We 
also give means and variances and indicate important properties and applications in following examples. 
When a certain variable Y follows a certain law L we use the notation Y~ L.
2.2.1	
Discrete distributions
1)	  
)
(
~
p
Bernoulli
Y
. Y is a variable that takes the value 1 with probability p and 0 with probability 
(1-p). The outcome Y = 1 is often termed a ‘success’ and the outcome Y = 0 is termed a ‘failure’. 
The pf is 
	
1,0
 ,
)
1(
)
(
1
=
−
=
−
y
p
p
y
p
y
y
 
with mean 
 
 variance
and
 p
=
µ
)
1(
2
p
p
−
=
σ
.
2)	
)
,
(
~
p
n
Binomial
Y
. The pf can be derived under the following assumptions: n independent 
repetitions are made of the same experiment that each time can result in one of the outcomes 
‘success’ with probability p and ‘failure’ with probability (1-p). Define the variable Y = ‘Number 
of successes that occur in n trials’. The pf is
	
n
y
p
p
y
n
y
p
y
n
y
,...,
1,0
 ,
)
1(
)
(
=
−






=
−
with 
)
1(
and
2
p
np
np

 
 
V
P
. Notice that 
∑
=
=
n
i
iY
Y
1
, where ( )n
i
iY
1
= is a sequence of iid 
variables, each 
)
(
~
p
Bernoulli
. For the meaning of 






y
n see Ch.2.3.5 below.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
18 
Basic probability and mathematics
3)	
)
(
~
p
Geometric
Y
. Assumptions: Independent repetitions are made of the same experiment 
that each time can result in one of the outcomes ‘success’ with probability p and ‘failure’ with 
probability (1-p). Define the variable Y = ‘Number of trials when a ‘success’ occurs for the 
first time’. The pf is
	
∞
=
−
=
−
,...
2,1
 ,
)
1(
)
(
1
y
p
p
y
p
y
with
2
2
/)
1(
 
and
 
/
1
p
p
p
−
=
=
σ
µ
. The survival function is 
y
p
y
Y
P
y
S
)
1(
)
(
)
(
−
=
>
=
. An 
interesting property of the Geometric distribution is the lack of memory, which means that the 
probability of a first ‘success’ in trial number (y+1), given that there has been no ‘successes’ in 
earlier trials, is the same as the probability of a ‘success’ in the first trial. Symbolically,
(
)
)1
(
)
1(
)
1(
)
(
)1
(
)
(
)
1
(
1
=
=
−
−
=
>
+
=
=
>
>
∩
+
=
=
>
+
=
Y
P
p
p
p
y
Y
P
y
Y
P
y
Y
P
y
Y
y
Y
P
y
Y
y
Y
P
y
y
4)	
)
(
~
λ
Poisson
Y
. The pf can be derived under a variety of different assumptions. One of 
the simplest way to obtain the pf is to start with a variable that is Binomial(n,p) and to let 
0
 
 time
same
 
at the
 
 while
,
→
∞
→
p
n
in such a way that 
λ
→
⋅p
n
. In practice this means 
that n is large and p is so small that the product 
λ
=
⋅p
n
is moderate, say within the interval 
(0.5, 20). The pf is
	
∞
=
=
−
,...
1,0
 ,
!
)
(
y
e
y
y
p
y
λ
λ
with 
λ
σ
λ
µ
=
=
2
 
and
 
.
A more general random quantity is
)
(t
Y
. This is a counting function that describes the number 
of events that occurs during a time interval of length t. It is called a stationary Poisson process 
of rate (intensity) λ and the pf is
	
(
)
( )
∞
=
=
=
−
,...
1,0
 ,
!
)
(
y
e
y
t
y
t
Y
P
t
y
λ
λ
with (
)
(
))
(
)
(
t
Y
V
t
t
Y
E
=
= λ
. λ can be interpreted as the expected number of events per unit 
time since 
	
(
)
(
)
t
t
Y
V
t
t
t
Y
V
t
Y
E
t
t
t
Y
E
λ
λ
=
⋅
=






=
⋅
=






)
(
1
)
(
 
Also,
 .
)
(
1
)
(
2
.
A Poisson process can be obtained under the assumption that the process is a superposition 
of a large number of independent general point processes, each of low intensity (Cox & Smith 
1954, p. 91).
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
19 
Basic probability and mathematics
Let 
)
(
 
and
 )
(
t
Y
s
X
be two independent Poisson processes of rates
Y
X
λ
λ
 
and
 
, respectively, e.g. 
number of road accidents during s and t hours on roads with and without limited speed. We 
are interested in comparing the two intensities in order to draw conclusions about the effect of 
limited speed on road accidents. One elegant way to do this is to use the Conditional Poisson 
Property (cf. Cox & Lewis 1968, p 223)
The conditional variable 
)
)
(
)
(
)
(
(
n
t
Y
s
X
t
Y
=
+
~ Binomial(n,
t
s
t
p
Y
X
Y
⋅
+
⋅
⋅
=
λ
λ
λ
)
(3)
The problem of comparing two intensities can thus be reduced to the problem of drawing 
inference about one single parameter. Notice that if 
Y
X
λ
λ
=
then
)
/(
t
s
t
p
+
=
.
The discrete variable 
)
(t
Y
 that counts the number of events in intervals of length t is related 
to another continuous variable that expresses the length between successive events. (Cf. the 
theorem (4) in Section 2.2.2.) 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Exercises in Statistical Inference  
with detailed solutions
20 
Basic probability and mathematics
5)	 Y ~. (Discrete) Uniform(N). The pf is
	
n
y
N
y
p
,...,
2,1
 ,
1
)
(
=
=
with 
12
/)1
(
and
2
/)1
(
2
2

 

 
N
N
V
P
. The distribution put equal mass on each of the 
outcomes 1,2,…,N. A typical example with N = 6 is when you throw a symmetric six-sided 
dice and count the number of dots coming up. 
6)	 (
)
k
Y
Y ,...,
1
~ Multinomial(n,
k
p
p ,...,
1
). This is the only example of a discrete many-dimensional 
variable that is considered in this book. The pf is derived under the same assumptions as for a 
Binomial variable. However, instead of two outcomes at each single trial, there are k mutually 
exclusive outcomes 
k
A
A ,...,
1
where the probability of 
1
and
is
1
 
¦
 
k
i
i
i
i
p
p
A
. The pf of the 
variables 
k
i
A
Y
i
i
,...,
1
,
occurs'
 that 
 times
of
Number 
'
 
 
 is
	
ky
k
y
k
k
p
p
y
y
n
y
y
p
⋅⋅⋅
⋅⋅⋅
=
1
1
1
1
!
!
!
)
,...,
(
with 
n
y
k
i
i =
∑
=1
Verify that k = 2 gives the Binomial distribution. Here 
i
i
i
p
n
Y
E
⋅
=
=
)
(
µ
, 
j
i
p
p
n
Y
Y
Cov
p
p
n
Y
V
j
i
j
i
ij
i
i
i
ii
z


 
 


 
 
,
)
,
(
and
)
1(
)
(
V
V
 .
EX 4 Let Y be the variable ‘Number of boys in a randomly chosen family with 4 children’. This can be assumed to be 
Binomial(n, p) with n = 4 and p = 53/103 ≈ 0.516, the latter figure being obtained from population statistics in the 
Scandinavian countries (106 born boys on 100 born girls). By using the pf in (2) above one gets
070
.0
)
103
/
50
(
)
103
/
53
(
4
4
)
4
(
,
265
.0
)
103
/
50
(
)
103
/
53
(
3
4
)
3
(
,
374
.0
)
103
/
50
(
)
103
/
53
(
2
4
)
2
(
,
235
.0
)
103
/
50
(
)
103
/
53
(
1
4
)1(
,
056
.0
)
103
/
50
(
)
103
/
53
(
0
4
)
0
(
0
4
1
3
2
2
3
1
4
0
 
¸¸
¹
·
¨¨
©
§
 
 
¸¸
¹
·
¨¨
©
§
 
 
¸¸
¹
·
¨¨
©
§
 
 
¸¸
¹
·
¨¨
©
§
 
 
¸¸
¹
·
¨¨
©
§
 
p
p
p
p
p
These probabilities are very close to the actual relative frequencies. However, it should be kept in mind that 
calculations have been based on crude figures and the results may not be true in other populations. E.g. if both 
parents are smokers the proportion born boys is only 0.451 or 82 born boys on 100 born girls (Fukada et al 2002, 
p. 1407).
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
21 
Basic probability and mathematics
EX 5 In Russian roulette a revolver with place for 6 bullets is loaded with one bullet. You spin the revolver, direct 
it towards your head and then fire. Define the variable Y = ‘Number of trials until the bullet hits your head for the 
first time (and probably the last).’ The variable can be assumed to have a Geometric distribution with p = 1/6. In this 
case it is perhaps not that interesting to compute the probability that the revolver fires after exact y trials, but the 
probability to survive y trials. From the expression above in (3), Ch. 2.2.1, we get the survival function
∞
=
=
>
=
,...
2,1
 ,
)
6
/
5
(
)
(
)
(
y
y
Y
P
y
S
y
A few values are:
y
1
2
3
4
5
6
S(y)
0.83
0.69
0.58
0.48
0.40
0.33
The median is somewhere between 3 and 4 trials which implies that after 3 successive trials most of the candidates 
will have been hit by the bullet. Russian roulette has been a motive in several films such as “The Deer Hunter”, “The 
Way of the Gun” and “Leon”, just to mention a few ones. The next time you are watching such a film you should have 
the table above in your mind.
EX 6 Let 
)
(s
X
be a Poisson process of rate 
X
λ
representing the number of road accidents on a road segment. 
During 12 months it is noticed that there has been 18 accidents, so that 
X
λ
may be put equal to 18/12 = 1.5. One 
can now calculate the probability of several outcomes such as
--
At least one accident in s months, (
)
s
x
e
p
x
p
s
X
P
⋅
−
∞
=
−
=
−
=
=
≥
∑
5.1
1
1
)
0
(
1
)
(
1
)
(
,  
which tends to 1with increasing values of s.
--
At least one accident in 1 month, (
)
777
.0
1
1
)1(
5.1
=
−
=
≥
−
e
X
P
.
--
At least two accidents in 1 month, (
)
=
−
−
=
≥
)1(
)
0
(
1
2
)1(
p
p
X
P
 
442
.0
5.1
1
5.1
5.1
=
⋅
−
−
−
−
e
e
.
--
At least two accidents in one month given that at least one accident has occurred, 
(
)
(
)
(
)
=
≥
≥
∩
≥
=
≥
≥
1
)1(
2
)1(
1
)1(
1
)1(
2
)1(
X
P
X
X
P
X
X
P
[The intersection of the two events in the numerator 
is simply
2
)1(
≥
X
]= (
)
(
)
777
.0
442
.0
1
)1(
2
)1(
=
≥
≥
X
P
X
P
= 0.569.
EX7 Assume that speed limits are introduced on the road segment in EX 6 and after this one observe 3 accidents in 3 
months. The rate of accidents has thus decreased from 1.5 to 1.0 per month. Does this imply that restricted speed has 
had an effect on accidents, or is the decrease just temporary? We will later present some ways to tackle this question 
(Cf. Ch. 6), but for the moment we just show how the problem of comparing two rates can be reformulated.
Let 
)
(t
Y
be the Poisson process of accidents during time t after the introduction of speed limits and let the rate be
Y
λ . According to formula (3) in this section the variable 

21
)3
(
)
12
(
)3
(
 
 Y
X
Y
 is Binomial (n,p) with n = 21 and 
)3
12
/(
3




 
Y
X
Y
p
O
O
O
. If 
Y
X
λ
λ
=
then p = 1/5, to be compared with the observed proportion 3/21 = 1/7. 
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
22 
Basic probability and mathematics
EX 8 (
)
3
2
1
,
,
Y
Y
Y
is a Multinomial variable (
)
3
2
1
,
,
,
p
p
p
n
. The pf is 
3
2
1
3
2
1
3
2
1
3
2
1
!
!
!
!
,
,
(
y
y
y
p
p
p
y
y
y
n
y
y
y
p
=
. The 
outcomes are often referred to as cell frequencies.
The mean and variance of 
2
1
Y
Y −
are


)
(
2
1
2
1
2
1
2
1
p
p
n
np
np
Y
Y
E


 

 

 

P
P


2
1
2
2
1
1
12
22
11
2
1
2
)
1(
)
1(
2
p
np
p
np
p
np
Y
Y
V




 


 

V
V
V
 = [After some  
re-arrangements] = 
(
))
(
1
)
(
2
1
2
1
p
p
p
p
n
+
−
+
⋅
2.2.2	
Continuous distributions
A convenient way to summarize the properties of a continuous distribution is to calculate the (symmetric) 
variation limits 
)
,
(
2
1 c
c
. These are the limits within which a certain percentage of all observations will fall. 
E.g. the 95% limits are obtained by solving the two equations
025
.0
)
(
1 =
< c
Y
P
 and 
025
.0
)
(
2 =
> c
Y
P
for 
2
1
 
and
 
c
c
. (Cf. EX 9-EX12.)
1.	 Uniform distribution on the interval [
]
[
]
b
a
Uniform
Y
b
a
,
~
 ,
,
.
Density 
°°
¯
°°
®
­
!
d
d



 
d
d
°¯
°®
­

 
b
y
b
y
a
a
b
a
y
y
y
F
b
y
a
a
b
y
f
1,
,
)
(
)
(
a
,0
)
(
cdf
,
,
otherwise
,0
)
(
1
)
(
It is easy to show that 
12
/
)
(
and
2
/)
(
2
2
a
b
a
b

 

 
V
P
.
2)	 Gamma distribution, Y ~ Gamma
)
,
(
k
λ
This is a class of distributions that is closely connected with the Gamma function 
)
(k
Γ
(Cf. 
Section 2.3.5.). The general form of the density is 
	
0
 ,0
 ,0
 ,
)
(
)
(
1
>
>
≥
Γ
=
⋅
−
−
k
y
e
y
k
y
f
y
k
k
λ
λ
λ
.
Notice that the integral of the density over all values of y is 1, a property that can be used in 
computations. Two important special cases are:
--
Exponential distribution, k = 1, Y ~Exponential
)
(λ ,with density
y
e
y
f
⋅
−
=
λ
λ
)
(
.
--
Chi-square distribution with n degrees of freedom (df) 
2
/
 
and
 2
/
1
n
k =
=
λ
,  
Y ~
)
(
2 n
χ
.
The cdf can only be expressed explicitly if k is a positive integer,
y
k
i
i
e
i
y
y
F
⋅
−
−
=∑
−
=
λ
λ
1
0
!
)
(
1
)
(
. 
In the exponential case we thus get
y
e
y
F
⋅
−
−
=
λ
1
)
(
. An important theorem that links the 
Exponential distribution to the Poisson process in Section 2.2.1 is the following:
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
23 
Basic probability and mathematics
Let (
)
i
X
be a sequence of times-between-successive events. Then we have the identity
¯
®
­

t
independen
are
X
The
l
Exponentia
X
Each
t
Y
i
i
)
2
)
(
~
)1
rate
of
process
Poisson 
a
is
)
(
O
O

(4)
This gives us a simple clue to determine whether a given sequence of events follow the Poisson 
law or not: (1) Make a histogram of 
i
X and compare it with the Exponential density, (2) Make 
a plot of each interval length versus the length of the following interval (
i
X versus 
1
+
i
X
) and 
compute the correlation. For more refined methods the reader is referred a book by Cox & 
Lewis 1966, p. 152.
For Y ~Gamma
)
,
(
k
λ
we have
2
2
/
 
and
 
/
λ
σ
λ
µ
k
k
=
=
. More generally 
)
(
)
(
1
)
(
k
r
k
Y
E
r
r
Γ
+
Γ
=
λ
which holds for r = K – 2, – 1, 0, 1, 2K. Special case: 
r
r
r
r
Y
E
k
λ
α
!
)
(
1
=
=
⇒
=
. 
The following theorem makes it possible to calculate areas under the Gamma density by using 
tables for Chi-square variables that are found in most textbooks:
)
2
(
~
2
)
,
(
~
2
k
Y
k
Gamma
Y
χ
λ
λ
⇒

(5)
An application of this is given in EX 11 below.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Exercises in Statistical Inference  
with detailed solutions
24 
Basic probability and mathematics
3)	 Weibull distribution, Y ~W(
λ
α,
). This has the density
	
0
 ,0
 ,0
 ,
)
(
1
>
>
≥
⋅
⋅
=
⋅
−
−
λ
α
λ
α
α
λ
α
y
e
y
y
f
y
Here 
α
α
λ
α
α
σ
λ
α
µ
/
2
2
2
/
1
)
/
1
1(
)
/
2
1(
 
and
 )
/
1
1(
+
Γ
−
+
Γ
=
+
Γ
=
. The cdf is 
α
λ y
e
y
F
⋅
−
−
= 1
)
(
. This 
distribution is obtained from the relation 
~
 
 where
,
/
1
X
X
Y
α
=
Exponential( λ ).
Applications can be found in survival analysis and reliability engineering.
4)	 Normal distribution, Y ~ N
)
,
(
2
σ
µ
has the density
	
∞
<
<
∞
−
⋅
=
⋅
−
−
y
e
y
f
y
 ,
2
1
)
(
2
2
2
)
(
2
σ
µ
σ
π
,
where
2
 
and
 
σ
µ
is the mean and variance, respectively. A standard normal variable is obtained 
by putting
1
 
and
 0
2 =
=
σ
µ
. The latter is denoted 
)1,0
(
~ N
Z
and will be used to compute 
areas under the normal density in a way that is described in EX 12 below. Notice that 
σ
µ /)
(
−
= Y
Z
, the transformation is called standardization.
The normal distribution can be obtained as a limiting distribution in several ways. Some of 
these are listed below in (a) to (c), where the one in (a) is formulated as a theorem due to its 
importance. A proof of (a) can be found in Casella & Berger 1990, p. 217. A proof of (c) can 
be found in Cramer 1957, p. 250.
a) Central Limit Theorem (CLT) . Let ( )n
i
iY
1
= be a sequence of independent and identically distributed (iid) variables 
with mean
2
 
 variance
and
 
σ
µ
. Then the cdf of the standardized variable 
n
Y
n
n
Y
Y
V
Y
E
Y
Z
n
i
i
n
i
i
n
i
nn
i
i
i
n
/
2
2
1
1
1
1
V
P
V
P

 



 
¸¸
¹
·
¨¨
©
§
¸¸
¹
·
¨¨
©
§

 
¦
¦
¦
¦
 
 
 
 
 tends to the cdf of 
)1,0
(
~ N
Z
as 
∞
→
n
.  
This is denoted
f
o
o

n
Z
Z
D
n
as
,
.
(6)
b)	 If 
.
as
),
1,0
(
~
)
1(
 then 
)
,
(
~
f
o
o



 
n
N
Z
p
np
np
Y
Z
p
n
Binomial
Y
D
n
c)	 If 
)
(t
Y
is a Poisson process with rate O then 
)1,0
(
~
)
(
)
(
N
Z
t
t
t
Y
t
Z
D
o


 
O
O
 as 
∞
→
t
, or 
alternatively, with t = 1
f
o
o

O
as
)1,0
(
~
)1(
N
Z
Z
D
.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
25 
Basic probability and mathematics
Comments
--
The CLT was first formulated and proved by the French mathematician Laplace about 1778 
(exact year is hard to establish). Notice that it is the standardized variable that has a normal 
distribution as a limit. In some textbooks you may find expressions like ‘Y has a limiting Normal 
distribution with mean 
n
/
 
 variance
and
 
2
σ
µ
’. But this is not true since the distribution of Y
tends to a ‘one-point’ distribution at µ with variance zero. 
--
As you might suspect, the result in (b) is simply a result of the CLT since 
)
,
(
~
p
n
Binomial
Y
can 
be expressed as 
∑
=
=
n
i
iY
Y
1
where the 
iY are iid with a Bernoulli distribution. However, this result 
was published earlier than that of the CLT, in November 12, 1733 by the French mathematician 
de Moivre and it seems to be the first time that the formula of the normal density appears.
--
Further results were later obtained by the German mathematician K.F. Gauss (1809) and 
the Russians Markov (1900) and Liapuonov (1901). It has been found that the limiting Z 
-distribution exists under less restricted assumptions than mentioned in (a) above.
--
Many distributions are related to Z ~N(0,1), e.g. 
)1(
~
2
2
χ
Z
.
--
If 
)
,
(
~
2
i
i
i
N
Y
σ
µ
then 
∑
=
N
Y
a
L
i
i
~
with mean and variance given in (2), Ch. 2.1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Exercises in Statistical Inference  
with detailed solutions
26 
Basic probability and mathematics
5)	 Laplace distribution, Laplace’s first law or the double Exponential distribution, Y~ L(
b
,
µ
). The 
density and cdf are
	



≥
⋅
−
<
⋅
=
=
−
−
−
−
−
b
y
e
b
y
e
y
F
e
b
y
f
b
y
b
y
b
y
 ,
)
2
/
1(
1
 ,
)
2
/
1(
)
(
 
and
 
2
1
)
(
)
(
µ
µ
µ
With mean µ and 
2
2
2b
=
σ
.
This distribution and its generalizations to non-symmetric casas has important applications in engineering 
and finance.
EX 9 Assume that waiting times are distributed U[0,b]. Compute the mean and the median waiting time and also the 
95% variation limits.
2
/
2
/
1
)
Put
 (
)
(
 ,2
/
b
M
b
M
M
F
b
=
⇒
=
=
=
=
µ
.
95 % variation limits are obtained from: 
b
c
b
c
c
F
c
Y
P
025
.0
2
05
.0
)
Put
(
)
(
)
(
1
1
1
1
 

 
 
 
 

, 
Put)
(
1
)
(
1
)
(
2
2
2
=
−
=
−
=
>
b
c
c
F
c
Y
P
=0.025
b
c
975
.0
2 =
⇒
. The 95 % variation limits are thus (0.025b, 
0.975b). E.g. if a bus runs every 20 minutes from a bus stop, 95 % of the waiting times will range from 0.5 to 
19.5 minutes.
EX 10 Intervals between arrivals to an intensive care are distributed
)
(λ
l
Exponentia
. Compute the mean and 
median interval and give the 95% variation limits.
O
O
O
O
P
O
O
67
.0
)
2
ln(
so
),
2
ln(
2
/
1
2
/
1
Put)
(
1
)
(
,
/
1
|
 
 


 

 
 

 
 




M
M
e
e
M
F
M
M
O
O
O
O
O
O
O
/
69
.3
/)
025
.0
ln(
025
.0
)
Put
(
)
(
1
)
(
.
/
025
.0
/)
975
,0
ln(
975
.0
025
.0
)
Put
(
1
)
(
2
2
2
1
1
2
1
1
|

 

 
 
 


 
!
|

 

 

 
 

 







c
e
c
Y
P
c
Y
P
c
e
e
c
Y
P
c
c
c
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
27 
Basic probability and mathematics
EX 11 Assume that service times (minutes) for a customer at a cash machine are distributed Gamma
)
2
,2
(
=
=
k
λ
. 
Determine the mean and median service times and give the 95 % variation limits for the service times.
.1
2
/
2
/
=
=
=
λ
µ
k
[
]
Put)
(
)
2
)
4
(
(
)
2
2
(
 trick
 the
Notice
)
(
2
=
<
=
<
=
=
<
M
P
M
Y
P
M
Y
P
λ
χ
λ
λ
=1/2. From a table of the Chi-
square distribution we get 
 
 

 
4
/
36
.3
36
.3
2
M
M
O
 0.84.
025
.0
Put)
(
)
2
)
4
(
(
)
2
2
(
)
(
1
2
1
1
=
=
<
=
<
=
<
c
P
c
Y
P
c
Y
P
λ
χ
λ
λ
. The same tables gives 2λc1 = 0.48   so  
c1 = 0.12. 
0.025
(Put)
)
2
)
4
(
(
)
2
2
(
)
(
2
2
2
2
=
=
>
=
>
=
>
c
P
c
Y
P
c
Y
P
λ
χ
λ
λ
. From this 2λc2 = 11.14, so 
79
.2
2  
c
.
In this example we have used the theorem in (5)
EX 12 
Y
N
Y
for 
limits
 variation
%
95
 the
Determine
).
,
(
~
2
V
P
.
=
<
)
(
1c
Y
P
[Notice the trick]=
⇒
=
=






−
<
=






−
<
−
025
.0
Put)
(
1
1
σ
µ
σ
µ
σ
µ
c
Z
P
c
Y
P
.
96
.1
96
.1
1
1
V
P
V
P

 


 

c
c
 Similarly we get 
V
P
96
.1
2

 
c
2.3	
Mathematics
Some mathematics will be needed when solving problems in statistical inference. Here we consider a 
few results that will be needed.
2.3.1	
Functions of a single variable
A function 
)
(x
f
y =
maps one set of x- values on one set of y- values. The function is called one-to-one 
if only one x- value correspond to a y- value. In such a case one can obtain the reversed map, the inverse 
function
)
(
1 y
f
x
−
=
. Consider the function
∞
<
<
∞
−
=
x
x
y
 ,
2
, which maps values along the whole 
x- values on the positive y- axis. It is not one-to-one since e.g. both x = -1 and x = 1 gives y =1. On the 
other hand, 
∞
<
≤
=
x
0 ,
2
x
y
is one-to-one with the inverse function
y
x =
.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
28 
Basic probability and mathematics
Some simple functions
--
Straight line, 
,x
b
a
y
⋅
+
=
a is the intercept and b is the slope.
--
Exponential,
x
ab
y  
. With a = 1 and b = e
7182
.2
≈
, 
x
e
y =
having the following properties:
(
)
2
1
2
1
2
1
2
1
 ,
 ,
/
1
x
x
x
x
x
x
x
x
x
x
e
e
e
e
e
e
e
=
=
⋅
=
+
−
--
Potense, 
b
ax
y  
--
Logarithmic (natural), 
)
ln(x
y =
having the following properties: 
,0
)1
ln(
,
)
0
ln(
=
−∞
→
 
1
)
ln(
=
e
, 
x
x
b
x
x
x
x
x
x
x
x
x
b
=
=
−
=
+
=
)
ln(e
),
ln(
)
ln(
 ),
ln(
)
ln(
)
/
ln(
 ),
ln(
)
ln(
)
ln(
x
2
1
2
1
2
1
2
1
. If 
x
e
x
y
y =
=
 then 
)
ln(
--
Logistic (S-curve), y = el / (1 + el), where l = a + b · x.
Linearization of non-linear functions
--
x
ab
y  
. Taking logarithms on both sides gives 
 

 
 
 
)
ln(
)
ln(
)
ln(
)
ln(
'
b
x
a
ab
y
y
x
x
b
a
'
'+
. So x plotted against 
)
ln(y gives a straight line.
--
b
ax
y  
.
'
'
)
ln(
)
ln(
)
ln(
)
ln(
'
bx
a
x
b
a
ax
y
y
b

 

 
 
 
. So, 
ln(y)
against 
 
plotted
 )
ln(x
gives a straight line.
--
x
b
a
l
e
e
y
l
l


 

 
 with 
),
1
/(
. Now 


x
b
a
l
y
y
y
e
y
y
l


 
 

 
 

)
1
/(
ln
'
so
,
)
1
/(
 
and thus a plot of x against ln
))
1
/(
(
y
y

 gives a straight line.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
▶▶enroll by September 30th, 2014 and 
▶▶save up to 16% on the tuition!
▶▶pay in 10 installments / 2 years
▶▶Interactive Online education
▶▶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Exercises in Statistical Inference  
with detailed solutions
29 
Basic probability and mathematics
2.3.2	
Sums and products
The sum of
∑
=
=
+
+
=
n
i
i
n
n
x
x
x
x
x
1
1
1
...
,...,
. The 
ix  are terms Sometimes we drop the lower or upper 
index in the summation sign if they are obvious. The product of 
∏
=
=
⋅⋅⋅
=
n
i
i
n
n
x
x
x
x
x
1
1
1,...,
. The 
ix
are now termed factors.
Some rules
--
2
1
2
2
2
1
2
2
1
2
)
(
x
x
x
x
x
x
+
+
=
+
. More generally: 
∑
∑
∑
≤
<
≤
=
=
+
=






n
j
i
j
i
n
i
i
n
i
i
x
x
x
x
1
1
2
2
1
2
. Notice that 
the last sum contains 
n
n −
2
terms of the form
j
ix
x
.
--
(
) ∑
∏
∏
=
∑
=
⋅⋅⋅
=
=
i
x
x
x
x
n
i
x
x
e
e
e
e
e
i
i
i
n
i
ln ,
1
1
--
∏
∏
∑
∑
=
=
=
=
=
⋅
=
⋅
n
i
i
n
n
i
n
i
i
n
i
i
x
a
x
a
x
a
x
a
1
1
i
1
1
 , 
, where a is a constant.
EX 13 
∑
=
=
n
i
ix
n
x
1
1
is termed the arithmetic mean. Obviously 
=
−
=
−
∑
∑
∑
=
=
=
n
i
n
i
i
n
i
i
x
x
x
x
1
1
1
)
(
 
0
1
=
⋅
−
∑
=
x
n
x
n
i
i
.
Let a be an arbitrary constant. Then ¦
 
 

n
i
i
x
a
a
x
1
2
.
if
minimized
is
)
(
Proof: 
[
]
(
)
+
−
+
−
=
−
+
−
=
=
−
∑
∑
∑
∑
=
=
=
=
n
i
n
i
i
n
i
i
n
i
i
a
x
x
x
a
x
x
x
a
x
1
2
1
2
1
2
1
2
)
(
)
(
)
(
)
(
 trick
 the
Notice
)
(
∑
=
−
n
i
i
x
x
1
(
2
) (
∑
∑
=
=
−
−
+
−
+
−
=
−
n
i
i
n
i
i
x
x
a
x
a
x
n
x
x
a
x
1
2
1
2
)
(
)
(
2
)
(
)
(
)
, where the last term is zero.
Notice that 
2
1
1
2
1
2
1
2
1
2
1
2
)
(






−
=
−
⋅
+
=
−
∑
∑
∑
∑
∑
=
=
=
=
=
n
i
i
n
i
i
n
i
i
n
i
i
n
i
i
x
n
x
x
x
x
n
x
x
x
. The latter expression is often simpler 
to use in calculations.
2.3.3	
Derivatives
The derivative of 
)
(x
f
y =
with respect to x is the limit 


0
h
as
/
)
(
)
(
lim
)
('
o


 
h
x
f
h
x
f
x
f
. 
Other notations for a derivative are 
.
or
,
,'
f
D
dx
df
dx
dy
y
x
 Rather than having to calculate the limit it is 
easier to use the following rules.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
30 
Basic probability and mathematics
Derivation rules
1)	 Special functions
)
(x
f
:
bx
a 
b
x
x
e
)
(x
g
e
)
ln(x
)
(' x
f
:
b
1

b
bx
x
e
)
(
)
('
x
g
e
x
g
x
/
1
2)	
)
('
)
('
)
('
)
(
)
(
)
(
x
h
x
g
x
f
x
h
x
g
x
f
±
=
⇒
±
=
3)	
)
('
)
(
)
(
)
('
)
('
)
(
)
(
)
(
x
h
x
g
x
h
x
g
x
f
x
h
x
g
x
f
⋅
+
⋅
=
⇒
⋅
=
4)	
(
)
)
(
/
)
('
)
(
)
(
)
('
)
('
)
(
/)
(
)
(
2 x
h
x
h
x
g
x
h
x
g
x
f
x
h
x
g
x
f
⋅
−
⋅
=
⇒
=
5)	
(
)
)
('
)
('
)
('
)
(
)
(
h
g
x
h
x
f
x
h
g
x
f
=
⇒
=
. This is a very useful rule that is demonstrated in
EX 14 below.
EX 14 
)
ln(
)
(
and
1
3
)
(
Put 
).
1
3
ln(
)
(
h
h
g
x
x
h
x
x
f
 

 

 
in (5) above, with
3
)
('
=
x
h
, 
h
h
g
/
1
)
('
=
. Then 
)1
3
/(
3
)
('

 
x
x
f
h
h
h
g
x
h
h
h
h
g
x
x
h
x
x
f
2
1
2
/
1
)
('
  ,2
)
('
 with 
,
)
(
 
and
 
2
)
(
Put 
 .
2
)
(
2
/
1
2
/
1
=
⋅
=
=
=
=
=
=
−
. Then 
x
x
f
2
1
)
('
=
.
)
(
2
)
(
2
)1
(
),
(
2
.
)
(
2
x
a
a
x
da
dy
a
x
dx
dy
a
x
y

 



 

 

 
. The function y can be considered as a 
function of either x or a.
>
@
¦
¦
¦
 
 
 

 



 

 
 

 
n
i
i
n
i
i
i
i
i
n
i
i
x
a
a
x
da
dy
a
x
x
dx
dy
a
x
y
1
1
1
2
)
(
2
)
(
2
)1
(
),
(
2
one
just 
is
There
.
)
(
Two important theorems about extreme values
--
If 
)
(x
f
has a local maximum (max) or minimum (min) at 
0x
x =
then this can be obtained 
by solving the equation 
 .
for 
 0
)
('
0x
x
x
f
=
=
Furthermore, from the sign of the second 
derivative 
)
('' x
f
, we draw the following conclusions:



=
⇒
<
=
⇒
>
0
0
0
at 
max 
 
local
 a 
has
 )
(
0
at 
min 
 
local
 a 
has
 )
(
0
)
(''
x
x
x
f
x
x
x
f
x
f
--
If 
0
)
(
>
x
f
then 
)
(x
f
has a local max or min at the same x- value as 

)
(
ln
x
f
 
EX 14 Does the function
2)
1
(
)
(
−
−
=
x
e
x
f
have any max/min-values? Since 
0
)
(
>
x
f
we prefer to study the 
simpler function 


2)1
(
)
(
ln
)
(


 
 
x
x
f
x
z
. Since 
0
)1
(
2
)
('
=
−
−
=
x
x
z
1
0 =
⇒x
, this must be 
a value of interest. Now, 
,0
2
)
(''
<
−
=
x
z
 from which we conclude that the function has a local maximum at 
.1
=
x
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
31 
Basic probability and mathematics
2.3.4	
Integrals
The (Riemann) integral l³
b
a
dx
x
f
)
(
 
is the area between a and b under the curve
)
(x
f
.
Integration rules
1)	
>
@
)
(
)
(
)
(
)
(
a
F
b
F
x
F
dx
x
f
b
x
a
x
b
a

 
 
 
 
³
where F is a primitive function to f. Since 
)
(
)
('
x
f
x
F
=
we can use the derivation rules above to find primitive functions.
2)	


³
³
³
r
 
r
b
a
b
a
b
a
dx
x
h
dx
x
g
dx
x
h
x
g
)
(
)
(
)
(
)
(
3)	
>
@
³
³

 

 
 
b
a
b
x
a
x
b
a
dx
x
h
x
G
h
h
x
G
dx
x
h
x
g
)
('
)
(
)
(
)
(
)
(
)
(
 (Partial integration)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Exercises in Statistical Inference  
with detailed solutions
32 
Basic probability and mathematics
EX 15 
2
1
)
0
0
(
2
1
1
2
)
1(
1
0
2
1
0
 



 
»¼
º
«¬
ª 
 

 
 
³
x
x
x
x
dx
x
.
³
³
 
 

1
0
1
0
2
/
1
1
dx
x
dx
x
2
0
2
2
/
1
1
0
2
/
1
 

 
»¼
º
«¬
ª
 
 
x
x
x
.
>
@
1
)
(
0
0
0
0
 



 

 

f
o
 

f

³
e
e
dx
e
x
x
x
x
. An area under an infinitely long interval can thus be finite. This is an 
example of a mathematical paradox since it would imply that we could paint an infinitely long fence, having an 
exponential shape, with a finite amount of paint.
2.3.5	
Some special functions and relations
Let n be any of the integers 0,1,2,…. Then n! (‘n faculty’) equals 1 for n = 0 and 
n
⋅⋅⋅
⋅2
1
 for n >0.
The combination operator
)!
(!
!
x
n
x
n
x
n

 
¸¸
¹
·
¨¨
©
§
. E.g.
10
!3
!2
!5
2
5
 

 
¸¸
¹
·
¨¨
©
§
. 
Some series
--
6
)1
2
)(
1
(
,
2
)1
(
1
2
1


 

 
¦
¦
 
 
n
n
n
i
n
n
i
n
i
n
i
.
--
Geometric
x
x
x
n
n
i
i
−
−
=
+
=∑
1
1
1
0
. 
x
x
i
i
−
=
∑
∞
=
1
1
0
, provided that -1< x <1.
--
Binomial 
n
i
n
i
n
i
b
a
b
a
i
n
)
(
0
+
=






−
=∑
--
Exponential 
x
i
i
e
i
x =
∑
∞
=0 !
--
Taylor Let 
a
x
x
f
th
i
a
f
i
 
at 
computed
)
(
of
derivative
:
 the
be
)
(
)
(
 with
)
(
)
(
)
0
(
a
f
a
f
=
. Then 
)
(
!
)
(
)
(
)
(
0
a
f
i
a
x
x
f
i
i
i
∑
∞
=
−
=
. In practice this may be used to approximate 
)
(x
f
by a 
polynomial. E.g. 
)
(''
2
)
(
)
('
)
(
)
(
)
(
2
a
f
a
x
a
f
a
x
a
f
x
f
−
+
−
+
≈
. In this case 
)
(x
f
has been 
approximated by a Taylor polynomial of order 2 about a.
EX 16
4
1
8.0
1
1
8.0
8.0
8.0
0
0
1
=
−
−
=
−
= ∑
∑
∞
=
∞
=
i
i
i
i
. 
[
]
8
8
0
)1
1(
 
1
Put 
8
+
=
=
=
=






∑
=
b
a
i
i
= 256.
Let 
1
)
1
(
)
1(
consider 
 
and
 1
0
0
=
−
+
=
−






<
<
∑
=
−
n
n
i
i
n
i
p
p
p
p
i
n
p
-
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
33 
Basic probability and mathematics
Gamma function
For any p, define the Gamma function 
³
f


 
*
0
1
)
(
dx
e
x
p
x
p
. Tables of this function can be found in 
Standard Mathematical Tables. Tables can also be produced by using program packages such SAS, SPSS or 
Statistica. The behavior of the function is quite complicated but we will only need the following properties:
--
)
(
)1
(
p
p
p
Γ
⋅
=
+
Γ
--
!
)1
(
p
p
=
+
Γ
 if p =0, 1, 2,…
Cauchy-Schwarz inequality
Let 
i
i
y
x
 
and
 
be real numbers. Then (
)
(
)(
)
∑
∑
∑
≤
2
2
2
i
i
i
i
y
x
y
x
.
2.4	
Final words
Notice the difference between a discrete and a continuous variable when calculating probabilities. For 
a continuous variable Y the probability (
)
y
Y
P
=
is always 0. This implies that (
)
(
)
y
Y
P
y
Y
P
>
=
≥
. On 
the other hand, for a discrete variable,
(
)
(
)
(
)
y
Y
P
y
Y
P
y
Y
P
>
+
=
=
≥
 .
The population median M is a value such that 
2
/
1
)
(
=
M
F
and nothing else. The sample median m is 
obtained by ranking the observations in a sample and to let m be the observation in the middle, or the 
average of the observations in the middle. m may be used as an estimate of M.
In Ch. 2 we only considered discrete bivariate distributions. Continuous bivariate distributions are 
treated analogously. The essential difference is that all summation symbols in properties (1)-(10) are 
replaced by integrals.
The reader is encouraged to use the summation symbol ∑
=
n
i
ix
1
rather than x1 + … + xn and the product 
symbol∏
=
n
i
ix
1
rather than x1 . … . xn. In the book we will use alternative symbols for division. To save 
space we write a/b instead of 
b
a . A typical example is 
f
e
d
c
b
a
/
/
/
+
.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
34 
Sampling Distributions
3	 Sampling Distributions
Data consist of observations 
n
y
y ...,
,1
(numerical values) that have been drawn from a population. 
The latter may be called a specific sample. If we want to guess, or estimate, the value of a population 
characteristic such as the population mean µ one may take the sample mean
∑
=
n
y
y
i /
. Any new 
sample of n observations drawn from the population will give rise to a new set of y – values and thus 
also of y . To understand this variation from sample to sample it is useful to introduce the concept of a 
random sample of size n, 
n
Y
Y ,...,
1
. Throughout this book it will be assumed that the latter variables are 
independent so that the probability of the sample can be expressed as in (1a) and (1b).
The appropriateness of taking the sample mean as a guess for µ can be judged by studying the distribution 
of Y and calculate the dispersion around µ . However, Y is just one possible function of 
n
Y
Y ,...,
1
, and 
there might be other functions that are better in some sense. Every function of the n-dimensional variable 
is termed a statistic with the general notation
)
,...,
( 1
n
Y
Y
g
T =
. The distribution of T is called a sampling 
distribution. If the purpose is to estimate a characteristic in the population, T is called an estimator and 
a numerical value of T is called an estimate, t. If the purpose is to find an interval 
)
,
(
2
1 T
T
that covers 
the population characteristic with a certain probability it is called a confidence interval (CI). Finally, the 
statistic is called a test-statistic if the purpose is to use it for testing a statistical hypothesis. In this chapter 
we consider some exact and approximate results of sampling distributions..
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Exercises in Statistical Inference  
with detailed solutions
35 
Sampling Distributions
3.1	
Some exact sampling distributions
Sum of variables
1)	
)
,
(
~
)
(
~
1
p
n
Binomial
Y
p
Bernoulli
Y
n
i
i
i
∑
=
⇒
2)	






⇒
∑
∑
=
=
p
n
Binomial
Y
p
n
Binomial
Y
k
i
i
k
i
i
i
i
,
~
)
,
(
~
1
1
3)	






⇒
∑
∑
=
=
n
i
i
n
i
i
i
i
Poisson
Y
Poisson
Y
1
1
~
)
(
~
λ
λ
4)	






⇒
∑
∑
∑
=
=
=
n
i
i
i
n
i
i
i
n
i
i
i
i
i
i
a
a
N
Y
a
N
Y
1
2
2
1
1
2
,
~
)
,
(
~
σ
µ
σ
µ
5)	 Special case with 
2
2
 ,
σ
σ
µ
µ
=
=
i
and 
n
ai
/
1
=
: 
)
/
,
(
~
2 n
N
Y
σ
µ
6)	












⇒
∑
∑
∑
∑
=
=
=
=
n
i
i
n
i
i
n
i
i
n
i
i
i
i
k
n
Gamma
n
Y
k
Gamma
Y
k
Gamma
Y
1
1
1
1
,
~
 ,
,
~
)
,
(
~
λ
λ
λ
7)	 Special case with 
1
=
ik
: 
)
,
(
~
)
(
~
1
n
Gamma
Y
l
Exponentia
Y
n
i
i
i
λ
λ
∑
=
⇒
8)	 Special case with 
2
/
 
and
 2
/
1
i
i
n
k =
=
λ
: 






⇒
∑
∑
=
=
k
i
i
k
i
i
i
i
n
Y
n
Y
1
2
1
2
~
)
(
~
χ
χ
Sum of quadratic forms
9)	
)
(
~
)
(
or 
 , )
(
~
)
(
)
,
(
~
2
2
1
2
2
2
1
2
2
n
Y
n
Y
N
Y
n
i
i
n
i
i
i
χ
σ
µ
χ
σ
µ
σ
µ
⋅
−
−
⇒
∑
∑
=
=
. Notice that the sign ‘~’ 
(distributed as) can be treated in the same way as the equality sign.
10)	  
(
)
(
)
)1(
~
or 
 , )1(
~
/
)
,
(
~
2
2
2
2
2
2
2
χ
σ
µ
χ
σ
µ
σ
µ
n
Y
n
Y
N
Yi
−
−
⇒
 .
An important theorem on chi-square distributed quadratic forms is the following theorem 
(Cochran, 1934)
Cochran’s Theorem: Let 
3
2
1
 
and
 
 ,
Q
Q
Q
be quadratic forms such that 
3
2
1
Q
Q
Q
+
=
then



−
⇒
t
independen
 
are
 
 
and
 
)
(
~
)
(
~
 
and
 )
(
~
3
2
2
1
2
3
2
2
2
1
2
1
Q
Q
n
n
Q
n
Q
n
Q
χ
χ
χ

(7)
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
36 
Sampling Distributions
EX 17 Prove the relations in (9) and (10) above.
)
(
~
)
(
)1(
~
)
(
)1,0
(
~
)
,
(
~
2
2
1
2
2
2
2
2
n
Y
Y
N
Y
N
Y
n
i
i
i
i
i
i
χ
σ
µ
χ
σ
µ
σ
µ
σ
µ
∑
=
−
⇒
−
⇒
−
⇒
.
)1(
~
/
)
(
)1,0
(
~
/
)
/
,
(
~
)
,
(
~
2
2
2
2
2
χ
σ
µ
σ
µ
σ
µ
σ
µ
n
Y
N
n
Y
n
N
Y
N
Yi
−
⇒
−
⇒
⇒
.
EX 18 Use Cochran’s Theorem to show that
)1
(
~
)
(
)
,
(
~
2
2
1
2
2
−
−
⇒∑
=
n
Y
Y
N
Y
n
i
i
i
χ
σ
σ
µ
.
)
(
)
(
P
P



 

Y
Y
Y
Y
i
i





 

¦
¦
¦
 
 
 
n
i
n
i
i
n
i
i
Y
Y
Y
Y
1
2
1
2
1
2
)
(
)
(
)
(
P
P
¦
 


n
i
i
Y
Y
Y
1
)
)(
(
2
P . Here the last term is 
0
)
(
)
(
2
1
=
−
−∑
=
n
i
i
Y
Y
Y
µ
(cf. EX 13). So,
3
2
1
2
2
2
1
2
2
1
2
or 
 
/
)
(
)
(
)
(
Q
Q
Q
n
Y
Y
Y
Y
n
i
i
n
i
i
+
=
−
+
−
=
−
∑
∑
=
=
σ
µ
σ
σ
µ
The result now follows from (9) and (10) above.
EX 18 (Continued) The sample variance is defined as
)1
(
)1
(
~
1
)
(
2
2
1
2
2
−
−
−
−
= ∑
=
n
n
n
Y
Y
S
n
i
i
χ
σ
. Notice that 
2
Q
is a function of 
2
S
and 
3
Q is a function of Y . Since 
3
2
 
and
 
Q
Q
are independent it follows that 
Y
and
S
 
 
2
are 
independent random variables. So, if we repeatedly compute
2
S
andY in samples from a normal distribution we will 
obtain a zero correlation. This may seem to be amazing since 
2
S
is functionally dependent of Y , but it illustrates 
that statistical dependency and functional dependency are two different concepts.
Ratios
11)	Student’s T with f degrees of freedom, 
)
( f
T
)
(
~
 
and
 )1,0
(
~
2 f
V
N
Z
χ
are independent
)
(
~
/
f
T
f
V
Z
⇒
Tables showing areas under the density of T can be found in most elementary text books.
12)	Variance ratio F with 
2
1
 
and
 
f
f
 degrees of freedom, 
)
,
(
2
1 f
f
F
)
(
~
 
and
 )
(
~
2
2
2
1
2
1
f
V
f
V
χ
χ
are independent
)
,
(
~
/
/
2
1
2
2
1
1
f
f
F
f
V
f
V
⇒
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
37 
Sampling Distributions
Tables showing areas under the density of F can also be found in elementary textbooks, but these are 
more comprehensive and seldom show areas for all values of
2
1, f
f
. Sometimes one can use the fact
)
,
(
/
1
)
,
(
1
2
2
1
f
f
F
f
f
F
=
.
Order statistics
A random sample of n independent observations
n
i
iY
1
)
(
=  can be arranged in increasing order, from 
the smallest to the largest 
)
(
)
2
(
)
1(
...
n
Y
Y
Y
<
<
<
. Here only the distribution of the smallest and largest 
observations 
)
(
)
1(
 
and
 
n
Y
Y
are considered. We also restrict ourselves to the case with continuous variables. 
The distributional properties are summarized in the following theorem:

 >
@

n
Y
i
n
i
Y
Y
y
F
Y
Y
y
F
y
F
Y
i
)
(
1
1
~
all
If
)
(
1
1
)
(
cdf
has
1
)
1(
)
1
(


 
 


 

 
.
 In the latter case 


1
)
(
1
)
(
)
(
)
1
(


 
n
Y
Y
Y
y
F
y
nf
y
f
.
 
>
@

n
Y
i
n
i
Y
Y
n
y
F
Y
Y
y
F
y
F
Y
i
n
)
(
~
all
If
)
(
)
(
cdf
has
1
)
(
)
(
 
 
 
 

(8)
 In the latter case


1
)
(
)
(
)
(
)
(

 
n
Y
Y
Y
y
F
y
nf
y
f
n
.  
EX 19 Determine the cdf and density of 
)
1
(Y
if all
)
(
~
~
λ
l
Exponentia
Y
Yi
.
(
)
y
n
Y
y
n
n
y
Y
y
Y
e
n
y
f
e
e
y
F
e
y
F
⋅
−
⋅
−
−
−
=
−
=
−
=
⇒
−
=
λ
λ
λ
λ
λ
)
(
  ,
1
1
)
(
1
)
(
)
1
(
)
1
(
. Thus, the smallest of n 
observations is
)
( λ
n
l
Exponentia
, so the expected value of 
)
1
(Y
is
λ
n
1
EX 20 Determine the cdf and density of 
)
(n
Y
if all
[
]
b
Uniform
Y
Yi
,0
~
~
.
b
y
b
ny
f
b
y
y
F
b
y
b
y
y
F
n
n
Y
n
n
Y
Y
n
n
d
d
 
 

d
d
 

0
,
,
)
(
0
,
)
(
1
)
(
)
(
.
.
)1
(
)1
(
)
(
1
0
0
1
)
(
b
n
n
n
b
b
n
dy
y
b
n
dy
b
ny
y
Y
E
n
n
b
b
n
n
n
n
n


 


 
 

 


³
³
3.2	
Sample moments
In Ch. 2.1 we introduce the population moments 
)
(
r
r
Y
E
=
α
 and the population central moments 
(
)
2)
(
µ
µ
−
=
Y
E
r
. By means of the Binomial series in Ch. 2.3.5 we can express 
r
µ  in terms of 
r
α  in 
the following way. 
(
)
∑
∑
=
=
−






=






−






=
−
=
r
i
i
r
i
i
r
i
r
r
Y
E
i
r
Y
i
r
E
Y
E
0
0
)
(
)
(
)
(
µ
µ
µ
)( 
−
−
i
r)
µ
=∑
=
−
−






r
i
i
r
i
i
r
0
)
( µ
α
. 
From this we get e.g.
2
1
2
0
2
1
2
0
2
2
α
α
µ
α
µ
α
µ
α
µ
−
=
+
−
=
.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
38 
Sampling Distributions
The corresponding sample moments are 
∑
∑
=
=
−
=
=
=
n
i
r
i
r
n
i
r
i
r
Y
Y
n
m
Y
a
Y
n
a
1
1
1
)
(
1
 
and
 
with 
1
. Instead of 
studying the properties of 
r
m  in general we confine ourselves to 
∑
=
−
−
=
n
i
i
Y
Y
n
S
1
2
2
)
(
)1
(
1
.
The following theorem gives some properties of sample moments.
If ( )n
i
iY
1
= are iid variables with mean µ and variance 
2
σ
, then
n
a
V
a
E
r
r
r
r
r
1
)
(
)
(
 ,  
)
(
2
2
α
α
α
−
=
=
 
(9a)
n
n
n
S
V
S
E
1
)1
(
)
3
(
)
(
 ,  
)
(
2
2
4
2
2
2






−
−
−
=
=
µ
µ
σ
 
(9b)
The expressions for 
)
(
2
S
V
above is proved in a book by C.R. Rao 1965, p.368. Proofs of the other relations 
are left as exercises for the reader in EX 21 below.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master’s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master’s programmes
• 33rd place Financial Times worldwide ranking: MSc 
International Business
• 1st place: MSc International Business
• 1st place: MSc Financial Economics
• 2nd place: MSc Management of Learning
• 2nd place: MSc Economics
• 2nd place: MSc Econometrics and Operations Research
• 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‘Beste Studies’ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Exercises in Statistical Inference  
with detailed solutions
39 
Sampling Distributions
EX 21 
(
)
∑
∑
=
⋅
=
=
=
r
r
r
i
r
i
r
n
n
Y
E
n
Y
E
n
a
E
α
α
1
)
(
1
1
)
(
.
(
) [
]
(
)=
+
=
=
=
∑
∑∑
∑
r
j
r
i
r
i
r
i
r
Y
Y
Y
E
n
Y
E
n
a
E
2
)
(
1
2.3.2
 
Ch.
  
Cf.
)
(
1
)
(
2
2
2
2
2
(
)
n
n
n
n
n
n
Y
E
Y
E
Y
E
n
r
r
r
r
r
r
r
j
r
i
r
i
2
2
2
2
2
2
2
2
2
)
(
2
1
)
(
)
(
2
)
(
1
α
α
α
α
α
α
−
+
=






⋅
−
+
⋅
=
+
∑
∑∑
.
So, 
n
a
E
a
E
a
V
r
r
r
r
r
1
)
(
)
(
)
(
)
(
2
2
2
2
α
α
−
=
−
=
.

 >
@



 

 

 
 

¦
¦
¦
¦
¦
2
2
2
2
2
)
(
1
)
(
/
)
(
13
EX
Cf.
)
(
i
i
i
i
i
Y
E
n
Y
E
n
Y
Y
E
Y
Y
E
[
]
(
)
)
(
)1
(
)
(
1
above
 
expression
 
Cf.
2
1
2
1
1
2
2
2
α
α
α
α
α
α
−
⋅
−
=
⋅
−
+
⋅
−
⋅
=
n
n
n
n
n
n
.
EX 22 Let ( )n
i
iY
1
= be iid and distributed
)
(λ
l
Exponentia
. Determine
)
(
2
S
V
.
2
2
1
)
(
λ
µ
=
=
Y
V
and from Ch. 2.2.2 (2) 
r
r
r
λ
α
!
=
with 
λ
µ
α
1
1
=
=
∑
=
−
=
⋅
+
−
+
−
=
−






=
4
0
4
3
2
2
3
1
4
0
4
4
1
4
6
4
)
(
4
i
i
i
i
α
µ
α
µ
α
µ
α
µ
α
µ
α
µ
 




4
3
2
2
3
4
24
1
6
4
1
2
6
1
1
4
1
O
O
O
O
O
O
O
O
4
9
O
.
Thus, from (9b) 
4
4
4
2
1
)1
(
)
6
8
(
1
1
)1
(
)
3
(
9
)
(
λ
λ
λ
−
−
=






−
−
−
=
n
n
n
n
n
n
S
V
3.3	
Asymptotic and approximate results in sampling theory
Sometimes it is not possible, or very hard, to find the exact distribution of a statistic 
n
T based on n 
observations. In such a case one may try to find the asymptotic distribution when n is large. If also this 
is a stumbling block one can try to find at least approximate expressions for expectations and variances 
of 
n
T . In this section we present some ways to handle these problems.
3.3.1	
Convergence in probability and in distribution
By convergence of 
n
T in probability towards a constant c when 
∞
→
n
 we mean that the probability for 
the event that the distance between 
n
T and c is positive, tends to zero with increasing n. In symbols this 
is expressed by 
f
o
o

n
c
T
P
n
as
,
.In practice it is often cumbersome to verify if the latter probability 
tends to zero. Then one may use the following theorem.
c
T
T
V
c
T
E
P
n
n
n
→

⇒
→
=
0
)
(
 
and
 
)
(

(10)
By convergence in distribution (or in law) we mean that the cdf of 
n
T tends to the cdf of T, say.
In symbols we express this by 
T
T
D
n
→

. An example is the CLT given in (6). 
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
40 
Sampling Distributions
Some important results 
Let g be a continuous function, then the following relations hold. (For proofs the reader is referred to 
Ch 2c.4 and Ch 6a.2 in Rao 1965.)
)
(
)
(
)
(
)
(
T
g
T
g
T
T
c
g
T
g
c
T
D
n
D
n
P
n
P
n
→

⇒
→

→

⇒
→

 
(11)





→

⋅
→

⋅
±
→

±
⇒
→

→

c
T
U
T
c
T
U
T
c
T
U
T
c
U
T
T
D
n
n
D
n
n
D
n
n
P
n
D
n
/
/
 
and
 

(12) 
Let  
θ be a parameter and let the variance of 
)
(
be
2 T
V
n
T
, a function of θ . Then
(
)
[
]
(
))
(
)
('
,0
~
)
(
)
(
))
(
,0
(
~
)
(
2
2
2
θ
σ
θ
θ
θ
σ
θ
g
N
X
g
T
g
n
N
Y
T
n
D
n
D
n
→

−
⇒
→

−

(13)
We now consider applications of (10)-(13)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Exercises in Statistical Inference  
with detailed solutions
41 
Sampling Distributions
EX 23 Let ( )n
i
iY
1
= be iid with 
)
(
~
p
Bernoulli
Yi
. Put 
∑
=
=
n
i
iY
n
p
1
1
ˆ
=Relative frequency of ‘success’ after n trials.
a)	 Show that 
f
o
o

n
p
p
P
as
,
ˆ
.
This follows from (10) since [cf. 2.2.1 (1)] 
0
)
1(
)ˆ
(
 
and
 
)ˆ
(
→
−
=
=
n
p
p
p
V
p
p
E
 as 
∞
→
n
.
The fact that 
p
p
P
→

ˆ
has been termed law of large numbers. It can be empirically verified by throwing a 
thumbtack a number of times and noticing the relative frequency of the event ‘tip of the tack is up’. The author, with 
his particular type of thumbtack found that the frequency stabilized around p = 0.6 after about 20 trials. The outcome 
is of course depending on the experimental conditions, but the reader is encouraged to repeat it, with a shoe or 
a coin. It is instructive to plot the relative frequency of the event on the Y-axis against the number of trials on the 
X-axis.
b)	 Show that
)1,0
(
~
/)
1(
)
ˆ
(
N
Z
n
p
p
p
p
D
→

−
−
, as 
∞
→
n
.
The left hand side is, after multiplication with n in both numerator and denominator,



S
QS
QS
<
Q
L
L


¦
 
 The CLT in (6) now gives the result.
c)	 Show that 
)1,0
(
~
/)
1(
)
ˆ
(
N
Z
n
p
p
p
p
D
o



, as
f
o
n
.
The left hand side can be written
1
/)
1(
/)
ˆ
1(
ˆ
)1,0
(
~
/)
1(
)
ˆ
(
→

−
−
→

−
−
P
n
n
D
n
n
p
p
n
p
p
N
Z
n
p
p
p
p
The convergence in the numerator was shown in b). To prove the convergence in the denominator, notice that 
)
1(
)
(
)
ˆ
1(
ˆ
)
ˆ
(
ˆ
p
p
p
g
p
p
p
g
p
p
P
n
n
n
P
n
−
=
→

−
=
⇒
→

 Finally, the result follows from (12).
Comment: The difference between the expressions in b) and c) is that in c) we have replaced p by an estimator
n
pˆ
 in 
the denominator. This will simplify calculations of confidence intervals (cf. Ch. 5). However, n in c) needs to be much 
larger than in b), for the approximation to normality to hold. If p is not too far from 0.5, then n about 50 is sufficient 
for normality in b), while n perhaps larger than 5000 may be required in c).
d)	 Show that 
)1,0
(
~
/)
1(
)
ln
ˆ
(ln
N
Z
p
p
p
p
n
D
⎯→
⎯
−
−
	  
Multiplying the left hand side in b) by 
)
1(
p
p
−
 and using (10) gives 
→

−
D
n
p
p
n
)
ˆ
(
))
1(
,0
(
~
)
1(
p
p
N
Z
p
p



.  Since 
x
ln
is continuous with derivative 1/x it follows from
(13) that 
¸
¸
¹
·
¨
¨
©
§


»¼
º
«¬
ª
o


)
1(
ln
,0
)
ln
ˆ
(ln
2
p
p
dp
p
d
N
p
p
n
D
n
 from which d) follows.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
42 
Sampling Distributions
EX 24 Let ( )n
i
iY
1
= be iid variables with 
2
)
(
 
and
 
)
(
σ
µ
=
=
i
i
Y
V
Y
E
. Show that 
)1(
)
(
2
2
2
χ
σ
µ
→

−
D
Y
n
This follows because
(
)
)1,0
(
~
)
N
Z
Y
n
D
→

−
σ
µ
according to the CLT in (6). From (11) 
(
)
)1(
2
2
2
2
χ
σ
µ
=
→

−
Z
Y
n
D
EX 25 Let ( )n
i
iY
1
= be iid variables with 
2
)
(
 
and
 
)
(
σ
µ
=
=
i
i
Y
V
Y
E
. Show that
f
o
o


n
N
Z
n
S
Y
D
as
)1,0
(
~
/
)
(
P
Dividing numerator and denominator by 
n
/
σ
yields
1
/
/
)1,0
(
~
/
)
(
→

=
→

−
P
D
S
n
n
S
N
Z
n
Y
σ
σ
σ
µ
, and the result follows from (12). 
Here 
1
→
P
S
σ
for two reasons: (i) 
[
]
2
2
2
(9b)
 
Cf.
 0
)
(
σ
→

⇒
→
P
S
S
V
.
 (ii) 
1
/
/
)
(
2
2
2
2
2
=
→

=
σ
σ
σ
P
S
S
g
 [Cf. (11)].
3.3.2	
Approximations of moments
Let 
2,1
 , =
i
Yi
be two random variables with means 
2
i
 
 variances
and
 
σ
µi
and with covariance 
12
V
. From 
Taylor expansions of a function g of the variables, one can show the following. (Cf. Casella & Berger 
1990, pp. 328–331.)
(
)
2
)
(''
2
1
)
(
)
(
i
i
i
i
g
g
Y
g
E
σ
µ
µ
⋅
+
≈
, (
)
[
]
2
2
)
('
)
(
i
i
i
g
Y
g
V
σ
µ
⋅
≈


12
2
2
1
1
2
2
1
1
)
(
'
)
(
'
)
(
),
(
V
P
P

|
g
g
Y
g
Y
g
Cov

(14)
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
43 
Sampling Distributions
EX 26 
a)	 Let 
)
,
(
~
k
Gamma
Y
λ
. Determine the approximate mean and variance of lnY.
From 2.2.2 (2) we know that the mean and variance is
2
2
/
 
and
 
/
λ
σ
λ
µ
k
k
=
=
, respectively. The derivatives of 
f
2
/
1
)
(''
and
/
1
)
('
are
ln
)
(
y
y
g
y
y
g
y
y
g

 
 
 
. Thus (14) gives






k
Y
V
k
k
Y
E
/
1
/
1
ln
,
2
/
1
)
/
ln(
)
/
1
(
2
1
ln
ln
2
2
2
2
 


|

 




|
V
P
O
V
P
P
.
b)	
n
i
iY
1
)
(
=  is a sequence of iid variables, each being distributed 
)
,
(
k
Gamma λ
. Determine the approximate 
mean and variance of ln
¦
 
 
n
i
i n
Y
Y
Y
1
/
usual
as
 where
,
.
From 3.1 (6) we know that 
)
,
(
~
1
nk
Gamma
G
Y
n
i
i
O
 
¦
 
. Now, 
n
G
n
G
Y
ln
ln
)
/
ln(
ln

 
 
. From a) above we 
get 
nk
G
V
nk
nk
G
E
/
1
)
(ln
and
2
/
1
)
/
ln(
)
(ln
|

|
O
. Thus,
nk
k
n
nk
n
k
n
nk
nk
Y
E
2
/
1
)
/
ln(
ln
2
/
1
ln
)
/
ln(
ln
2
/
1
)
/
ln(
)
(ln

 



 


|
O
O
O
 
nk
G
V
n
V
G
V
Y
V
/
1
0
)
(ln
)
(ln
)
(ln
)
(ln
 

 

|
Notice that, 
0
)
(ln
and
)
(
ln
)
(ln
,
as
o
o
f
o
Y
V
Y
E
Y
E
n
i
.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Exercises in Statistical Inference  
with detailed solutions
44 
Sampling Distributions
A generalization of (14) to a function of two variables is


¸¸
¹
·
¨¨
©
§






|
12
2
1
2
1
2
2
2
2
2
2
1
2
2
1
2
1
2
1
2
2
1
2
1
)
,
(
2
)
,
(
)
,
(
2
1
)
,
(
)
,
(
V
P
P
V
P
P
V
P
P
P
P
dy
dy
g
d
dy
g
d
dy
g
d
g
Y
Y
g
E


12
2
2
1
1
2
1
2
2
2
2
2
1
2
1
2
1
2
1
2
1
)
,
(
)
,
(
2
)
,
(
)
,
(
)
,
(
V
P
P
P
P
V
P
P
V
P
P
¸¸
¹
·
¨¨
©
§
¸¸
¹
·
¨¨
©
§


¸¸
¹
·
¨¨
©
§


¸¸
¹
·
¨¨
©
§
|
dy
dg
dy
dg
dy
dg
dy
dg
Y
Y
g
V
	
(15)
EX 27
Let 
2,1
 , =
i
Yi
be correlated variables with means 
2
 
 variances
and
 
i
i
σ
µ
 and with covariance 
12
V
. Derive the 
approximate mean and variance of 
2
1 /Y
Y
R =
.
We start by computing the derivatives of the function 
2
1 / y
y
g =
(cf. derivation rule (4) in Ch. 2.3.3).
2
2
2
1
2
3
2
1
2
2
2
2
2
1
2
2
1
2
2
1
1
,
2
,
,0
,
1
y
dy
dy
g
d
y
y
dy
g
d
y
y
dy
dg
dy
g
d
y
dy
dg

 
 

 
 
 
. Thus,
»
¼
º
«
¬
ª


 
»
»
¼
º
«
«
¬
ª
¸¸
¹
·
¨¨
©
§



|
¸¸
¹
·
¨¨
©
§
2
1
12
2
2
2
2
2
1
12
2
2
3
2
2
2
1
2
1
2
1
1
1
2
2
0
2
1
P
P
V
P
V
P
P
V
P
P
V
P
P
P
Y
Y
E
»
¼
º
«
¬
ª


¸¸
¹
·
¨¨
©
§
 
¸¸
¹
·
¨¨
©
§

¸¸
¹
·
¨¨
©
§

|
¸¸
¹
·
¨¨
©
§
2
1
12
2
2
2
2
2
1
2
1
2
2
1
12
2
1
2
2
2
2
2
1
2
2
2
1
2
1
2
2
P
P
V
P
V
P
V
P
P
V
P
P
P
P
V
P
P
V
Y
Y
V
3.4	
Final words
Uppercase letters or lowercase letters? Uppercase letters, such as 
2
S for a sample variance, are used for 
statistics when we want to stress that the quantity has a distribution. Lowercase letters, such as 
2
s , are 
used for specific values of a statistic.
The distribution of a statistic is called a sampling distribution. This is a creation by statisticians for the 
purpose of drawing conclusions about parameters in the population and it has nothing to do with the 
real world. Distributions that are intended to reflect facts in nature or society are called population 
distributions.
Asymptotic results are obtained as a limit, e.g. when 
0
 
and
 
→
∞
→
p
n
in the Poisson approximation of 
the Binomial distribution. Approximate results just mean that they are not exact.
Knowledge about sampling distributions is the key for understanding the content in the following 
chapters. It’s therefore important that you are comfortable with the properties in (1)-(10), and also of 
Cochran’s theorem.
We have assumed that there is a given a random sample. This can be achieved in a verity of ways. In this 
book we don’t bother how the sample has been collected. For readers interested in these matters there 
is a hugh amount of literature in the field. (See e.g. Scheaffer, et al, 2012).
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
45 
Sampling Distributions
Supplementary Exercises, Ch. 3
EX 28 Let ( )n
i
iY
1
= be iid variables.
Find cdf and density of the smallest observation 
)
1
(Y
if 
[
]
b
Uniform
Yi
,0
~
.
EX 29 Let ( )n
i
iY
1
= be iid variables.
Find cdf and density of the largest observation 
)
(
~
if
)
(
O
l
Exponentia
Y
Y
i
n
.
EX 30 Let 
n
Y
p
p
n
Binomial
Y
/
ˆ
put 
 
and
 )
,
(
~
=
so that 
.
/)
1(
)ˆ
(
 
and
 
)ˆ
(
n
p
p
p
V
p
p
E
−
=
=
As an estimator of 
)ˆ
( p
V
one may use 
n
p
p
p
V
/)ˆ
1(ˆ
)ˆ
(ˆ
−
=
. Show that the exact mean (
))ˆ
(ˆ p
V
E
 and the approximate mean obtained 
from (14) are identical.
EX 31 In medical statistics one often wants to study whether a factor F causes a disease. Data from two independent 
samples of sizes 
2
1
 
and
 
n
n
can be summarized in the following frequency table:
Diseased
Not-Diseased
Total
F is present
1Y
1
1
Y
n −
1n
F is absent
2
Y
2
2
Y
n −
2
n
Data are analyzed by comparing the Relative Risk
2,1
 ,
/
ˆ 
 where
,
ˆ
/
ˆ
ˆ
2
1
=
=
=
i
n
Y
p
p
p
R
i
i
i
with the hypothetical 
value of 1, being obtained if F does not cause the disease. The variance of Rˆ is estimated by




−
+
−
=
2
2
2
2
1
1
1
1
2
ˆ
)ˆ
(ˆ
Y
n
Y
n
Y
n
Y
n
R
R
V
Justify this expression by using the result in EX 27.
[Hint: Use the fact that 
2
1
 
and
 
Y
Y
can be treated as two independent variables that are
.2,1
 ,)
,
(
~
=
i
p
n
Binomial
i
i
]
EX 32 The sample variance 
2
S
is in general unbiased for 
2
σ
(cf. (9b)). However, S is not in general unbiased for σ . 
Determine approximate expressions for 
)
(
 
and
 )
(
S
V
S
E
 in the following cases:
a)	 ( )n
i
iY
1
= are iid with expectation 
2
 
 variance
and
 
σ
µ
with a general distribution for 
iY .
b)	 	 	
	
 -“- 	
	
	
	
with
)
,
(
~
2
σ
µ
N
Yi
.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
46 
Point estimation
4	 Point estimation
In this chapter we deal with the problem of how to estimate an unknown characteristic in the population 
based on a sample of n observations. Focus will be on the estimation of parameters, such as the variance 
2
σ in a normal distribution or the upper point b in a Uniform distribution. We briefly also consider 
the estimation of functions of parameters and other quantities such as probability and cdf. First some 
concepts are introduced and then we discuss some requirements on good estimators. Finally some 
estimation methods are presented and evaluated.
4.1	
Concepts
A statistic T is a function of the random variables 
n
Y
Y ,...,
1
in a sample. A point estimator is a statistic 
that is used to estimate the value of an unknown parameter in the population, in general denoted θ. A 
point estimate t is a numerical value of T, obtained in a specific sample. 
In (1a) and (1b) we introduced the concept of probability of a random sample of independent observations. 
This is a function of the variable values
n
y
y ,...,
1
. If we instead consider it as a function of the parameter 
θ, it is termed Likelihood 
)
,
,...,
(
)
(
1
θ
θ
n
y
y
L
L
=
. When we want to study the long-run behavior of the 
likelihood over all possible drawn samples, we use the notation
)
,
,...,
( 1
θ
n
Y
Y
L
. In the latter case L is a 
random variable. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet’s 
electricity needs. Already today, SKF’s innovative know-
how is crucial to running a large proportion of the 
world’s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Exercises in Statistical Inference  
with detailed solutions
47 
Point estimation
Intuitively the observations in a sample contain information about θ in some sense. (The statistical 
concept of information will be defined formally below.) E.g. given the body weights in kg, 75, 50, 90, 
72, 78 of five persons drawn from a certain population, we conclude that the population mean should 
be slightly larger than 70, but also that the dispersion is quite large. Sometimes all information about θ 
is contained in a single statistic T. In such a case T is termed a sufficient statistic for θ. If we have found 
a sufficient statistic T for θ we can, roughly speaking, skip the original observations and only use T for 
making inference about θ. The following factorization criterion can be used to find a sufficient statistic:
Assume that the likelihood L can be factorized into two parts such that
)
,...,
(
)
,
(
)
,
,...,
(
1
2
1
1
n
n
Y
Y
L
T
L
Y
Y
L
⋅
=
θ
θ
,
(16)
where L1 only depends on T and θ and L2 and does not depend on T and θ but possibly only on the observations, 
then T is sufficient for θ.
More generally, 
p
T
T ,...,
1
are simultaneous sufficient statistics for 
p
θ
θ ,...,
1
if (16) holds with T and θ being 
replaced by the corresponding vectors. The following results can be useful:
 Let g be a continuous function. Then: 
)
g(
for 
sufficient
is
)
(
for 
sufficient
is
T
T
T
g
T


(17)
A sufficient statistic is unique. (There can’t be several sufficient statistics for a parameter besides  
functions of T.)
(18)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Exercises in Statistical Inference  
with detailed solutions
48 
Point estimation
EX 33 ( )n
i
iY
1
= are independent variables in a random sample. Find sufficient statistics in the following cases:  
(See Ch. 2.2 to find the various distributions.)
a)	
)
,
(
~
p
m
Binomial
Y
i
i

 


¸¸
¹
·
¨¨
©
§
 
n
i
y
m
y
i
i
i
i
i
p
p
y
m
L
1
)
1(
=
¦
¦
¦
 
 
 


i
i
i
i
i
i
y
m
y
p
p
1
1
1
)
1(

 
¸¸
¹
·
¨¨
©
§

n
i
i
i
y
m
1
. From this we conclude that the statistic∑
=
n
i
iY
1
is 
sufficient for p.
b)	
)
(
~
p
Geometric
Yi
1
)
1(
)
1(
1
1
1
⋅
−
=
−
=
∑
=
−
=
−
∏
n
n
y
n
i
y
p
p
p
p
L
n
i
i
i
. Thus, ∑
=
n
i
iY
1
is sufficient for p.
c)	
)
(
~
λ
Poisson
Yi
∏
∏
=
−
=
−
⋅
=
=
∑
=
n
i
i
n
y
n
i
i
y
y
e
e
y
L
n
i
i
i
1
1
!
1
!
1
λ
λ
λ
λ
. Thus,
O
for 
sufficient
is
1¦
 
n
i
iY
.
d)	
iY has the density 
2
2
)
(
y
ye
y
f
O
O

 
. This is called the Weibull distribution and is used as a model for life 
lengths of materials.
∏
∏
=
−
−
=
⋅
=
=
n
i
i
n
y
n
y
n
i
i
y
e
e
y
L
i
i
1
1
2
2
2
2
λ
λ
λ
λ
. Thus, ∑
=
n
i
iY
1
2 is sufficient for λ .
e)	
)
,
(
~
k
Gamma
Yi
λ
1
)
(
)
(
1
1
1
1
1

¸¸
¹
·
¨¨
©
§
*
 
*
 
¦
 


 
 




n
i
i
i
y
k
n
i
i
n
nk
n
i
y
k
i
k
e
y
k
e
y
k
L
O
O
O
O
. Thus, 






∏
∑
=
=
n
i
i
n
i
i
Y
Y
1
1
,
are simultaneous sufficient for (
)
k,
λ
.
f)	
)
,
(
~
2
σ
µ
N
Yi
2
1
2
2
2
2
)
(
2
/
2
2
/
1
2
)
(
2
/
1
2
)
(
)
2
(
1
)
2
(
1
V
P
V
P
V
S
SV
¦
 


 


 
 
n
i
i
i
y
n
n
n
i
y
e
e
L
. Here 
 

¦
 
n
i
iy
1
2)
(
P


¦
¦
¦
¦
¦
 
 
 
 
 
 






 



 

n
i
i
n
i
n
i
i
n
i
i
n
i
i
y
y
y
y
y
y
y
y
y
y
1
1
2
1
2
1
2
1
2
)
)(
(
2
)
(
)
(
)
(
)
(
)
(
P
P
P
P
¦
 



n
i
i
y
n
y
y
1
2
2
)
(
)
(
P
, since the last term is zero (cf. EX 13). From this it follows that
)
(
,
1
2
1
¸¸
¹
·
¨¨
©
§

¦
¦
 
 
n
i
i
n
i
i
Y
Y
Y
 are simultaneous sufficient for 

2
,V
P
.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
49 
Point estimation
4.2	
Requirements on estimators
In order for an estimator 
T
of
n
T
, based on n observations, to be considered as good one usually requires 
the following:
--
n
T is consistent for θ. This means that the estimator converges in probability towards the 
parameter, 
θ
→
P
n
T
, as 
∞
→
n
. Remember from (10), Ch. 3.3, that a sufficient condition 
for this is that 
0
)
(
 
and
 
)
(
→
=
n
n
T
V
T
E
θ
. Estimators that are not consistent are useless in the 
sense that we do not necessarily get closer to θ by increasing n.
--
n
T is unbiased for θ which means that 
θ
=
)
(
n
T
E
. The difference 
θ
−
)
(
n
T
E
is the bias of the 
estimator, denoted 
)
(θ
bias
. The dispersion of 
θ 
around
 
n
T
can be measured by the Mean 
Squared Error (MSE) of 
[
]
(
)2
2
)
(
)
(
)
(
 ,
θ
θ
bias
T
V
T
E
T
n
n
n
+
=
−
.
--
n
T  is a minimum variance estimator (MVE) which means that 
)
(
n
T
V
 is smaller than the 
variance of all other estimators. A MVE is unique, so there can only be one estimator with 
smallest variance.
The problem of finding a MVE is rather complicated. Before treating this we consider some results about 
derivatives of the log-likelihood function. The function 
T
G
/
G OQ
 is called a score function and it plays an 
important role in statistical inference. From Eq. (1a) and Eq. (1b) it follows that
)
,
(
ln
and
case)
(discrete
)
,
(
ln
ln
1
1
¦
¦
 
 
 
 
n
i
i
n
i
i
d
y
f
d
d
y
p
d
d
L
d
T
T
T
T
T
 (continuous case) 
(19)
In order to obtain further results in the continuous case we set up the following conditions:
a)	 The range of y- values in 
)
,
(
θ
y
f
does not depend on θ.
(20)
b)	
2
2 ln
and
ln
T
T
d
f
d
d
f
d
 are continuous. 
Notice that (20) does not hold for 
[
]
b
Uniform
Y
,0
~
with density 
b
y
b
b
y
f
≤
≤
=
0 ,
/
1
)
,
(
, but for all 
other densities we have considered so far.
If the conditions (a) and (b) in (20) holds then
a)	
0
ln
 
¸
¹
·
¨
©
§
T
d
L
d
E
 
(21)
b)	
¸¸
¹
·
¨¨
©
§

 
¸
¹
·
¨
©
§
 
2
2 ln
ln
)
(
T
T
T
d
L
d
E
d
L
d
V
I
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
50 
Point estimation
The function 
)
(θ
I
is the information about θ that is contained in a sample of size n. A solution to the 
problem of finding a MVE is given by the following theorem, called the Information inequality or the 
Cramer-Rao inequality after two of its discoverers who published the result in 1945.
)
(
1
unbiased)
is
If
(
)
(
)
(
1
)
(
2
T
T
T
T
I
T
I
d
dbias
T
V
n
n
 
 
¸
¹
·
¨
©
§ 
t

(22)
The lower limit in (22) for the variance is called the Cramer-Rao (C-R) limit. The limit may not be 
attainable for a MVE, but no estimator can have smaller variance. Thus, if we have found an estimator 
with a variance that equals the C-R limit, then we have found a MVE. But, if the variance of an estimator 
is larger than the C-R limit, the estimator may still be a MVE. The search for a MVE in the latter case 
can be complicated. Some help may be obtained from a theorem of Rao and Blackwell (Casella & Berger 
1990, p. 316), but this is beyond the level of this book.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Exercises in Statistical Inference  
with detailed solutions
51 
Point estimation
EX 34 Let ( )n
i
iY
1
= be iid variables where 
)
(
~
λ
l
Exponentia
Yi
a)	 Find an unbiased estimator of 
λ
/
1
)
(
=
iY
E
that is based on the smallest observation
)
1
(Y
. Is this estimator 
consistent?
[
]
(
)
[
]
⇒
−
=
−
−
−
=
⇒
⇒
−
=
⋅
−
⋅
−
⋅
−
y
n
n
y
Y
y
Y
e
e
y
F
e
y
F
i
λ
λ
λ
1
1
1
1
)
(
(8)
 
See
1
)
(
)
1
(
λ
λ
n
Y
E
n
l
Exponentia
Y
/
1
)
(
)
(
~
)
1(
)
1(
=
⇒
. Thus, 
)
1(
nY
Tn  
 is unbiased for
λ
/
1
.
b)	 The variance of this estimator is 
2
2
2
)
1(
2
/
1
)
(
1
)
(
)
(
λ
λ
=
⋅
=
=
n
n
Y
V
n
T
V
n
which does not tend to 0 as 
∞
→
n
, so the estimator is not consistent.
c)	 Find a sufficient statistic forλ .
∑
∏
=
−
=
⋅
−
⇒
⋅
=
=
∑
=
n
i
i
y
n
n
i
y
Y
e
e
L
n
i
i
i
1
1
1
1
λ
λ
λ
λ
is sufficient for λ .
d)	 Determine the information about λ that is contained in a sample of n observations and also the Cramer-Rao 
(C-R) limit.
2
2
2
1
1
ln
ln
ln
ln
)
ln(
ln
1
O
O
O
O
O
O
O
O
n
d
L
d
y
n
d
L
d
y
n
e
L
n
i
i
n
i
i
y
n
n
i
i

 


 



 
¸¸
¸
¹
·
¨¨
¨
©
§

 
¦
¦
 
 
 ¦
 
. Thus, 
considering the latter as a random unit we get 
2
2
)
(
λ
λ
λ
n
n
I
=





−
−
=
. The information increases with n 
and decreases with increasingλ .
The C-R limit for any unbiased estimator of 
n
I
2
)
(
1
is
O
O
O
 
.
4.3	
Estimation methods
In this section we present some general methods to obtain estimators. Focus will be on the case with 
a single parameter, but examples in the multi-parameter case are also given. It is required that the 
estimators are unbiased. In this case the precision of an estimator can be measured by its variance. When 
comparing estimators it is useful to use the concept of relative efficiency of an estimator 
1T  relative to 
another
2
T , 
)
(
/)
(
2
1
T
V
T
V
RE  
. Examples of REs for estimators produced by various methods are given 
in the Supplementary Exercises of this chapter. 
4.3.1	
Method of Ordinary Least Squares (OLS)
The method originates from the works by the mathematicians Legendre (1805) and Gauss (1809). Many 
students are familiar with this method as a way to fit a straight line to data points in the plane, but the 
method can be used in more general contexts. 
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
52 
Point estimation
Given the variables 
iY with expectations
)
(
)
(
θ
i
i
g
Y
E
=
, i = 1…n, consider the sum of squares 
>
@
¦
 

 
Q
L
L
L
J
<
66



T
 and determine the value of θ that minimizes SS, say 
OLS
θˆ
. This can be 
obtained from the solution of 
0
=
θ
d
dSS
. By putting 
OLS
ˆT
 into SS one gets an estimated sum of squares 
[
]
∑
=
−
=
n
i
OLS
i
i
g
Y
SSE
1
2
)
ˆ
(θ
which can be used for estimating dispersion.
EX 35 ( )n
i
iY
1
= are iid with
)
(
~
λ
Poisson
Yi
. Find the OLS estimator
OLS
λˆ
.
>
@
>
@
>
@

 


 


 



 


 
¦
¦
¦
¦
 
 
 
 
0
0
2
2
)1
(
1
1
1
1
2
O
O
O
O
O
n
Y
Y
Y
d
dSS
Y
SS
n
i
i
n
i
i
i
n
i
n
i
i
Y
n
Y
n
i
i
OLS
=
= ∑
=
/
ˆ
1
λ
. 
Here we notice that 
λ
λ
λ
=
⋅
=
= ∑
=
n
n
Y
E
n
E
n
i
i
OLS
1
)
(
1
)
ˆ
(
1
(Unbiased.) and 
∑
=
=
n
i
i
OLS
Y
V
n
V
1
2
)
(
1
)
ˆ
(θ
=
n
n
n
λ
λ =
⋅
2
1
.
EX 36 ( )n
i
iY
1
= where
iY are independent with
i
i
x
Y
E
β
=
)
(
 and 
2
)
(
σ
=
iY
V
. This model is often called ‘Linear 
Regression trough the Origin with constant variance’. Here each 
ix is fixed while 
iY  is random. Find the OLS 
estimator of β .
>
@
>
@
¦
¦
¦
¦
¦
¦
 
 
 
 
 
 
 

 

 


 


 
n
i
i
n
i
i
i
OLS
n
i
i
i
n
i
i
n
i
i
i
i
n
i
i
i
x
Y
x
Y
x
x
x
Y
x
d
dSS
x
Y
SS
1
2
1
1
1
2
1
2
1
ˆ
0
)
(
2
E
E
E
E
E
(
)
β
β
β
=
⋅
⋅
=
⋅
=
∑
∑
∑
∑
=
=
=
=
n
i
i
i
n
i
i
n
i
i
i
n
i
i
OLS
x
x
x
Y
E
x
x
E
1
1
2
1
1
2
1
)
(
1
ˆ
(Unbiased.)
(
)
∑
∑
∑
∑
∑
=
=
=
=
=
=
⋅
⋅






=
⋅






=
n
i
i
n
i
i
n
i
i
n
i
i
i
n
i
i
OLS
x
x
x
Y
V
x
x
V
1
2
2
11
2
2
2
1
2
1
2
2
1
2
1
)
(
1
ˆ
σ
σ
β
. Notice that the variance is small, i.e. 
the precision of the estimator is high, if the 
ix -values are large. In practice this means that if we e.g. want to estimate 
the relation between 
iY = Fuel consumption and 
ix = Speed, we should measure Fuel consumption when Speed 
is high. 
When 
)
,...,
(
)
(
1
p
i
i
g
Y
E
θ
θ
=
, a function of several parameters, we put 
>
@
¦
 

 
n
i
p
i
i
g
Y
SS
1
2
1
)
,...,
(
T
T
. By 
solving the equations 
0
,...,
0
1
=
=
p
d
dSS
d
dSS
θ
θ
we get the OLS estimators of the parameters.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
53 
Point estimation
4.3.2	
Method of Moments
This method was suggested by the statistician Karl Pearson in the late 1800s. The approach is to equal 
the sample moments 
,...
 ,
2
S
Y
to the corresponding moments in the population 
,...)
,
(
)
(
2
1
1
θ
θ
g
Y
E
=
,
,...)
,
(
)
(
2
1
2
θ
θ
g
Y
V
=
,… and solve for the parameters. The method has several deficiencies, but moment 
estimates can be used when more ingenious methods require initial values in order to get iterative 
solutions. An example of this is given in EX 44.
EX 37 ( )n
i
iY
1
= are iid and 
)
(
~
λ
Poisson
Yi
. Here 
)
(
)
(
i
i
Y
V
Y
E
=
= λ
.
Obviously 
Y
Mom =
λˆ
. We might have used 
2
ˆ
S
Mom =
λ
, but this is less appropriate since 
2
S
has larger variance 
than Y .
EX 38 ( )
n
i
iY
1
= are iid and 
)
,
(
~
k
Gamma
Yi
λ
. Here 
2
/
)
(
 
and
 
/
)
(
λ
λ
k
Y
V
k
Y
E
=
=
.
Put 
¯
®
­
 
 


N





6
<
N
O
O
 from which we get
2
2
2
/
ˆ
S
Y
S
Y
k
Mom =
⇒
=
=
λ
λ
λ
. The latter inserted into (1) 
yields 
2
2 /
ˆ
S
Y
kMom =
.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT…
     RUN FASTER.
          RUN LONGER..
                RUN EASIER…
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Exercises in Statistical Inference  
with detailed solutions
54 
Point estimation
4.3.3	
Method of Best Linear Unbiased Estimator (BLUE)
The method has an unclear origin but seems to have been in use since the early 1900s. The approach is 
simply to put 
∑
=
=
n
i
i
i
n
Y
a
T
1
and determine the constants 
ia  such that 
n
T is unbiased and has minimum 
variance. This problem belongs to the field ‘minimization under restrictions’. In the examples below we 
show how solutions can be obtained in a simple way by using Lagrange’s multiplierλ .
EX 39 ( )n
i
iY
1
= are independent with
i
i
i
V
Y
V
Y
E
=
=
)
(
 
and
 
)
(
θ
. Find the BLUE of θ .
∑
∑
∑
∑
=
=
=
=
=
⇒
=
=
=
=
⇒
=
n
i
i
n
i
i
n
i
i
i
n
n
i
i
i
n
a
a
Y
E
a
T
E
Y
a
T
1
1
1
1
1
(Put)
)
(
)
(
θ
θ
or ∑
=
=
−
n
i
ia
1
0
1

(i)
We now minimize 




−
+
=




−
+
=
∑
∑
∑
=
=
=
1
1
)
(
1
1
2
1
n
i
i
i
n
i
i
n
i
i
n
a
V
a
a
T
V
Q
λ
λ
with respect to 
ia .
say
,'
2
0
2
i
i
i
i
i
i
V
V
a
V
a
da
dQ
O
O
O
 

 

 

 

(ii)
Putting this into (i) gives
∑
∑
∑
=
=
=
=
⇒
=
=
n
i
i
n
i
i
n
i
i
V
V
a
1
1
1
1
1
'
1
'
λ
λ
, which inserted into (ii) gives
∑
∑
=
=
=
=
n
i
i
i
n
i
i
i
i
V
V
V
V
a
1
1
/
1
/
1
1
1
 . So, 
∑
∑
=
=
=
n
i
i
n
i
i
i
BLUE
V
V
Y
1
1
/
1
/
ˆθ
.
The variance is
(
)
∑
∑
∑
=
=
=
=
⋅
⋅






=
n
i
i
n
i
i
i
n
i
i
BLUE
V
V
V
V
V
1
1
2
2
1
/
1
1
/
1
/
1
1
)
ˆ
(θ
.
Notice that if all variances are equal,
V
Vi =
, then 
Y
BLUE =
θˆ
. Otherwise BLUE estimates can’t be computed in 
practice without further assumptions about the variances.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
55 
Point estimation
EX 40 ( )n
i
iY
1
= are independent with 
p
i
i
i
i
x
Y
V
x
Y
E
2
)
(
 
and
 
)
(
σ
β
=
=
for some p. 
This is the same situation as in EX 36 with the exception that 
)
( iY
V
is no longer constant, but changes with
ix . Find 
the BLUE of β .

 ¦
 
n
i
i
i
n
Y
a
T
1
0
1
(Put)
)
(
)
(
1
i
1
1
 


 
 
 
 
¦
¦
¦
 
 
 
n
i
i
n
i
i
i
n
i
i
i
n
x
a
x
a
Y
E
a
T
E
E
E

(i)
∑
∑
∑
=
=
=
=
=
=
n
i
p
i
i
n
i
p
i
i
n
i
i
i
n
x
a
x
a
Y
V
a
T
V
1
2
2
1
2
2
1
2
)
(
)
(
σ
σ
. 
»
¼
º
«
¬
ª


 
»
¼
º
«
¬
ª


 
¦
¦
¦
 
 
 
n
i
i
i
n
i
p
i
i
n
i
i
i
n
x
a
x
a
x
a
T
V
Q
1
1
2
2
1
1
1
)
(
O
V
O
,

 

 
0
2
2
i
p
i
i
x
x
a
d
dQ
O
V
E
p
i
p
i
i
x
x
a
−
−
=
−
=
1
2
1
'
2
λ
σ
λ

(ii)
(ii) into (i) gives 
∑
∑
∑
=
−
=
−
=
−
=
⇒
=
=
⋅
n
i
p
i
n
i
p
i
n
i
i
p
i
x
x
x
x
1
2
1
2
1
1
1
'
1
'
'
λ
λ
λ
, which inserted into (ii) gives
∑
=
−
−
=
n
i
p
i
p
i
i
x
x
a
1
2
1
. BLUE for β is thus
∑
∑
=
−
=
−
=
n
i
p
i
n
i
i
p
i
BLUE
x
Y
x
1
2
1
1
ˆβ
.
(
)
(
)
∑
∑
∑
∑
∑
=
−
=
−
=
−
=
−
=
−
=






=






=
n
i
p
i
n
i
p
i
p
i
n
i
p
i
n
i
i
p
i
n
i
p
i
BLUE
x
x
x
x
Y
V
x
x
V
1
2
2
1
2
2
1
2
1
2
1
2
1
2
1
2
1
)
(
1
)
ˆ
(
σ
σ
β
.
Special cases
:
)
(
so
0
2
V
 
 
iY
V
p
¦
¦
¦
 
 
 
 
 
n
i
i
BLUE
n
i
i
n
i
i
i
BLUE
x
V
x
Y
x
1
2
2
1
2
1
)
ˆ
(
 with 
ˆ
V
E
E
.
¦
¦
¦
 
 
 
 
 
 
 
 
n
i
i
BLUE
n
i
i
n
i
i
BLUE
i
i
x
V
x
Y
x
Y
x
Y
V
p
1
2
1
1
2
)
ˆ
(
 with 
ˆ
:
)
(
so
,1
V
E
E
V
.
n
V
n
x
Y
x
Y
V
p
BLUE
i
i
BLUE
i
i
2
2
2
)
ˆ
(
 with 
/
ˆ
:
)
(
so
,2
V
E
E
V
 
 
 
 
¦
.
This illustrates that estimators of the same parameter can differ very much, depending on which assumptions are 
made about the data structure. In practice it is therefore important that such structures are investigated before the 
estimation is done. 
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
56 
Point estimation
4.3.4	
Method of Maximum Likelihood (ML)
The method was developed by the English statistician R.A. Fisher in a series of papers published during 
the period 1912–1922. The idea is to determine the value of θ that maximizes the likelihood 
)
(θ
L
, thereby 
finding ‘the most likely value of the parameter given the outcomes 
n
y
y ,...,
1
’. If 
0
)
(
>
θ
L
, the likelihood 
has maximum for the same value as 
)
(
ln
T
L
 (cf. Ch. 2.3.3). Since the latter function is more convenient 
to deal with, the ML estimator 
ML
Tˆ
 can be found by solving the likelihood equations
0
ln
 
T
d
L
d
 for one parameter, or 
°
°
¯
°
°
®
­
 
 
0
ln
0
ln
1
p
d
L
d
d
L
d
T
T

 for many parameters.
Some properties of ML estimators:
--
The likelihood equations give the ML estimators if the conditions in (20) Ch. 4.2 holds.
--
Let 
)
(θ
g
be a continuous function of θ. Then the ML estimator of 
)
(θ
g
is 

Ö

0/
J T

--
ML estimators are seldom unbiased for finite n, but the bias can often easily be removed.
--
If a sufficient statistic 
θ
for 
 
n
T
exists, then 
ML
Tˆ
 is a function of 
n
T .
--
ML estimators are consistent.
--
In large samples (
∞
→
n
) 
)
(
/
1
)
ˆ
(
T
T
I
V
ML  
, so ML estimators are MVEs in large samples.
--
As 
)1,0
(
~
)
(
/
1
ˆ
,
N
Z
I
n
D
ML
o


f
o
T
T
T
.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

Exercises in Statistical Inference  
with detailed solutions
57 
Point estimation
ML estimators can be MVEs, also in small samples as will be demonstrated in the Supplementary 
Exercises of this Chapter. 
EX 41 ( )n
i
iY
1
= are iid with 
)
(
~
λ
l
Exponentia
Yi
. Show that the ML estimator of λ is biased and correct it for bias. 
Determine the variance of the corrected estimator and compare it with the CR limit.

 

 


 

 
 
¦
¦

 
 

 


¦
 
0
)
(
ln
)
ln(
)
(
ln
)
(
1
1
1
1
n
i
i
n
i
i
y
n
n
i
y
y
n
d
L
d
y
n
L
e
e
L
n
i
i
i
O
O
O
O
O
O
O
O
O
O
O
y
y
n
n
i
i
ML
1
ˆ
1
 
 
¦
 
O
.  The corresponding estimator is 
Y
Y
n
n
i
i
ML
1
ˆ
1
 
 
¦
 
O
.  (1)
To compute expectation and variance, notice that 
)
,
(
~
1
n
Gamma
Y
n
i
i
λ
∑
=
and use the expression in Ch.2.2.2(2) with 
n
k
r
=
−
=
 
and
 1
:
>
@
O
O
O
O
)1
(
)1
(
)1
(
)1
(
2.3.5
Ch.
Cf.
)
(
)1
(
)
ˆ
(

 

*


*

 
 
*

*

 
n
n
n
n
n
n
n
n
n
E
ML
. . This is not unbiased, but the 
bias can easily be removed by considering
¦
 

 


 
n
i
i
ML
ML
Y
n
n
n
1
1
ˆ
)1
(
'ˆ
O
O
.
»
»
¼
º
«
«
¬
ª
¸¸
¹
·
¨¨
©
§

 

 ¦
1
1
2)1
(
)
'ˆ
(
n
i
i
ML
Y
V
n
V O
. Now, 
=














−














=














−
=
−
=
−
=
∑
∑
∑
1
1
2
2
1
1
1
n
i
i
n
i
i
n
i
i
Y
E
Y
E
Y
V
 
¸
¸
¹
·
¨
¨
©
§
¸¸
¹
·
¨¨
©
§

*


*


*



*
 
¸¸
¹
·
¨¨
©
§
*

*

*

*
2
2
2
2
)1
(
)1
(
)1
(
)
2
(
)
2
)(
1
(
)
2
(
)
(
)1
(
)
(
)
2
(
n
n
n
n
n
n
n
n
n
n
n
O
O
O
)
2
(
)1
(
1
)1
(
1
)
2
)(
1
(
1
2
2
2
2


 
¸¸
¹
·
¨¨
©
§




n
n
n
n
n
O
O
.
Thus, 
)
2
(
)
'ˆ
(
2

 n
V
ML
O
O
which is larger than the C-R limit n
2
λ
. It can be shown that the C-R limit can’t be attained 
for any estimator. In fact, 
ML
'ˆO
is an unbiased MVE.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
58 
Point estimation
EX 42 Let 
)
(t
Y
be a Poisson process of rate λ .
a)	  Find an unbiased estimator of λ and compute the variance of the estimator. Determine the C-R limit and 
compare this with the variance.
In this case the Likelihood is of a different form: 



  

 
 
 


)
(
ln
!
)
(
)
(
)
(
O
O
O
O
L
e
y
t
y
t
Y
P
L
t
y




t
t
Y
t
y
d
L
d
t
y
t
y
ML
)
(
ˆ
0
)
(
ln
)!
ln(
)
ln(
)
ln(
 

 

 




O
O
O
O
O
O
.  (Notice here that we use the 
notation for an estimator rather than the estimate t
y .)


O
O
O
 
 
 
t
t
t
Y
E
t
E
ML
1
)
(
1
)
ˆ
(
 (Unbiased), 


f
o
o
 
 
 
t
t
t
t
t
Y
V
t
V
ML
as
0
1
)
(
1
)
ˆ
(
2
2
O
O
O
.






O
O
O
O
O
O
O
O
O
O
O
t
t
t
Y
E
I
t
Y
d
L
d
t
t
Y
d
L
d
 
 
 


 


 
2
2
2
2
2
1
)
(
1
)
(
)
(
)
(
ln
)
(
)
(
ln
.  Thus, the variance 
equals the C-R limit 
)
(
/
1
λ
I
and we conclude that 
ML
Oˆ
 is an unbiased MVE.
b)	 Find the ML estimator of (
)
t
e
t
Y
P
⋅
−
=
=
λ
0
)
(
. Compute an estimate of the latter when t = 0.5 and we have 
observed that 
10
)
5
(
 
Y
.
The ML estimator is 

t
ML
e

Oˆ
 which gives the estimate 
37
.0
1
5.0
5
10
|
 



e
e
.  The latter estimator is in fact biased, 
but it can be shown that the bias tends to zero with increasing t. 
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
59 
Point estimation
EX 43 Let (
)
)
,...,
,
(
~
,...,
1
1
k
k
p
p
n
l
Multinomia
Y
Y
. Determine the ML estimators of
k
p
p ,...,
1
.
Here 
!
!
!
 
 where
,
1
1
1
1
1
1
1
1
1
1
k
y
k
i
i
y
k
y
y
k
y
y
y
n
C
p
p
p
C
p
p
C
L
k
k
k
⋅⋅⋅
=





−
⋅⋅⋅
⋅
=
⋅⋅⋅
⋅
=
∑
−
=
−
−
.
We present two solutions, one without and one with the use of Lagrange’s multiplier. Without Lagrange’s multiplier:
¸¸
¹
·
¨¨
©
§ 


 
¸¸
¹
·
¨¨
©
§ 


 
¦
¦

¦

 

 

 

 
1
1
1
1
1
1
1
1
1
ln
ln
ln
1
ln
ln
ln
ln
k
i
i
k
k
i
i
i
k
i
y
k
i
i
y
i
p
y
p
y
C
p
p
C
L
k
i
k
i
y
p
y
p
p
y
p
y
p
y
p
y
dp
L
d
k
k
i
i
k
k
i
i
k
i
i
k
i
i
i
,...,
1
,
0
1
1
0
ln
1
1
 
 

 

 
¸¸
¸
¸
¸
¹
·
¨¨
¨
¨
¨
©
§




 
¦

 
 (i)
Since 
1
1
=
∑
=
k
i
ip
we get 
n
y
p
n
y
p
y
y
p
y
p
y
k
k
k
k
k
i
i
k
k
k
i
k
k
i
=
⇒
=
=
=
∑
∑
=
=
ˆ
1
1
1
and this inserted into (i) gives 
n
y
p
i
i =
ˆ
.
With Lagrange’s multiplier:
¦
 

 
k
i
i
i
p
y
C
L
1
ln
ln
ln
 is to be maximized subject to the condition 
1
1
=
∑
=
k
i
ip
 (ii).
Put 
i
i
i
i
i
i
k
i
i
k
i
i
i
y
y
p
p
y
dp
dQ
p
p
y
C
Q
'
0
0
1
ln
ln
1
1
O
O
O
O
 

 

 


 

»
¼
º
«
¬
ª



 
¦
¦
 
 
.  (iii) Putting this 
into (ii) gives 
n
n
y
k
i
i
/
1
'
1
'
'
1
=
⇒
=
=
∑
=
λ
λ
λ
, which inserted into (iii) gives 
n
y
p
i
i
/
ˆ =
.
The difficulty in this example arises from the fact that there are just k-1 genuine (linearly independent) parameters 
to estimate.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
60 
Point estimation
EX 44 ( )
)
,
(
~
 
 where
iid
 
are
1
k
Gamma
Y
Y
i
n
i
i
λ
=
a)	 Determine the ML estimators of 
p
 
and
 
λ
.
From EX 33 e) the likelihood is 

¸¸
¹
·
¨¨
©
§
*
 
¦
 


 
n
i
iy
k
n
i
i
n
nk
e
y
k
L
1
1
1
)
(
O
O
°
°
¯
°°
®
­

*

 

 




*


 
¦
¦
¦
¦
 
 
 
 
n
i
i
n
i
i
n
i
i
n
i
i
y
dk
k
d
n
n
dk
L
d
y
nk
d
L
d
y
y
k
k
n
nk
L
1
1
1
1
ln
)
(
ln
ln
ln
ln
ln
)1
(
)
(
ln
ln
ln
O
O
O
O
O
Putting these expressions equal to zero and solving for the two parameters yields:
y
k
y
nk
ML
ML
n
i
i
ˆ
ˆ
1
 

 ¦
 
O
O
 (i) and 
0
ln
)
(
ln
ln
1
 

*

¸¸
¹
·
¨¨
©
§
¦
 
n
i
iy
dk
k
d
n
y
k
n

(ii)
Rearranging the terms in equation (ii) gives 
n
y
y
dk
k
d
k
n
i
i
∑
=
−
=
Γ
−
1
ln
ln
)
(
ln
ln
	  

(iii)
By first solving (iii) for 
ML
kˆ
 and then putting this into (i) yields the solutions. However, (iii) has to be solved iteratively. 
How this can be done is illustrated in b) below.
b)	 From a sample of n = 100 observations the following quantities are calculated:
¦
¦
¦
 
 
 
3803
.
66
ln
,
0525
.
619
,
56
.
223
2
i
i
i
y
y
y
Compute the ML estimates of
k 
and
 
λ
.
The right hand side of (iii) is ln(2.2356)-0.6638=0.1407. With 
dk
k
d
k
k
g
)
(
ln
ln
)
(
*

 
 we want to determine the value 
of k such that 
1407
.0
)
(
=
k
g
. The function 
dk
k
d
)
(
ln*
 is well known in mathematics and is called the digamma 
function. We can thus plot 
)
(k
g
against k to find a solution of k. 
Some help in the search for solution is to use the estimate obtained by the Method of Moments. In EX 38 it was 
shown that 
2
2 /
ˆ
s
y
kMom =
. Now, 


)1
100
/(
100
/
)
56
.
223
(
0525
.
619
2
2


 
s
 =1.2046 and 
2356
.2
=
y
, so 
15
.4
ˆ
 
Mom
k
.  It is felt that a search for k in the interval [3.00, 5.00] should suffice.
The following program code (written in SAS) can be used to find k.
data a;
do k=3 to 5 by 0.01; g=log(k)-digamma(k); output; end;
proc print; var k g; run;
The solution is 
71
.3
ˆ
 
ML
k
 and putting this into (i) finally gives 
69
.1
ˆ
 
ML
O
.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
61 
Point estimation
4.4	
Final words
In this chapter we have only considered estimation of parameters and function of parameters. E.g. we can 
estimate 
λ
λ
−
=
e
y
y
p
y
!
/
)
(
by plugging in an estimate of λ . It is also possible to estimate 
)
(
),
(
),
(
y
F
y
f
y
p
 
etc. directly from data without model assumptions, but such procedures are beyond the scope of this book.
The information inequality in (22) seems to have been first discovered by Aitken and Silverstone in 1942 
during the second World War. During the 1920s Fisher showed that 
)
(
/
1
)
ˆ
(
T
T
I
V
ML  
 in large samples.
Consider the estimation of µ in the normal distribution. The estimator Y is unbiased for µ and has 
variance
n
/
2
σ
. An alternative estimator is the sample median, say m. This is also unbiased and has 
variance 
n
2
/
2
SV
 in large samples (Rao (1965), p356). The relative efficiency is 
63
.0
/
2
)
(
/)
(
|
 
S
m
V
Y
V
, 
and from this it seems obvious that the sample mean is to be preferred. However, there may be other 
aspects to take account of. In some cases the median is easier to use or can be computed more rapidly. 
As an example, consider estimation of the mean life length of rats that have been exposed to some drug. 
If we use the sample mean we have to wait until all rats have died (which may take years). By using the 
sample median we only have to wait until half of the rats have died.
In some text books one can find the concepts Best Asymptotic Normal (BAN) estimator and Consistent 
Asymptotic Normal (CAN) estimator. The ML estimator is both BAN and CAN.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Exercises in Statistical Inference  
with detailed solutions
62 
Point estimation
Supplementary Exercises, Ch. 4
EX 45 ( )n
i
iY
1
= are iid with 
[
]
b
Uniform
Yi
,0
~
.
a)	 Find an unbiased estimator of b based on the largest observation
)
(n
Y
and determine the variance of the estimator.
b)	 Show that the methods of OLS and Moment give identical unbiased estimators and determine the variance of 
the estimators.
c)	 Compare the relative efficiency of the estimators in a) and b).
d)	 The waiting times at a red traffic light were recorded 10 times and gave the following values (in seconds): 8, 13, 
16, 12, 46, 4, 22, 17, 34, 28. Use the data to estimate the time for each red light period. [Chose any estimator 
you want. Which one is most reliable?]
EX 46 ( )n
i
iY
1
= are independent with 
)
,
(
~
p
n
Binomial
Y
i
i
.
a)	 Find unbiased estimators of p by using the OLS- and ML methods. Compare the variances of the estimators.
b)	 Show that the ML estimator is BLUE, in contrast to the OLS estimator, and is in fact a MVE.
c)	 To estimate p = ‘Proportion of students with back/neck pain’, a sample of students in three class-rooms were 
taken with the following result:
Room
Total number of students
Number with back/neck pain
1
30
1
2
25
3
3
35
2
 Compute the OLS- and ML estimates of p.
EX 47( )n
i
iY
1
= are iid with 
)
(
~
p
Geometric
Yi
.
a)	 Find the ML estimator of p.
b)	 Sometimes it is practical to use sequentially collected data, rather than data with a fixed sample size n. Consider 
the following (fictive) data collected from a stream of students passing by:( 0,0,0,1,1,0,0,1), where ‘1’ indicate 
that the student visited a pub last night and ‘0’ that the student did not visit a pub. Estimate the proportion of 
students who visited a pub last night.
EX 48 ( )n
i
iY
1
= are independent with 
).
,
(
~
2
σ
β i
i
x
N
Y
a)	 Find the ML estimator of β . Show that it is BLUE and determine the distribution of the estimator.
b)	 Find the ML estimator of 
2
σ
and show that it is biased. Remove the bias and determine the distribution of the 
corrected estimator.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
63 
Point estimation
EX 49 In order to estimate mean µ and variance 
2
σ
in a normally distributed population, one takes independent 
samples from different regions. Determine the BLUE of the two parameters from the following data:
Region
1
K
k
Sample size
1n
K
k
n
Sample mean
1
Y
K
k
Y
Sample variance
2
1
S
K
2
k
S
EX 50 Let (
)
(
))
3
1(,
2,
,
~
,
,
3
2
1
p
p
p
n
l
Multinomia
Y
Y
Y
−
 Find the ML estimator of p and check whether it is MVU.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Exercises in Statistical Inference  
with detailed solutions
64 
Interval estimation
5	 Interval estimation
In the previous chapter we considered the estimation of an unknown population parameter θ at a single 
point. In this chapter we will show how it is possible to construct intervals that enclose θ with a certain 
degree of confidence. This approach is more informative than that of Ch. 4 since it does not only tell 
us about the location of the parameter value, but also how confident we should be with the estimate.
5.1	
Concepts
A confidence interval (CI) is a pair of statistics 
,
ˆ
ˆ
),
ˆ
,
ˆ
(
L
U
U
L
T
T
T
T

 that encloses θ with probability  
α
−
1
, the latter being termed the confidence coefficient or confidence level. The use of 
α
−
1
is somewhat 
confusing, but its origin will be evident in Ch. 6. 
Some properties of a CI:
--
U
L
θ
θ
ˆ 
and
 
ˆ
are both functions of a random sample Y1 … Yn and therefore the location and 
length of the CI will vary randomly.
--
There is no guarantee that a specific CI, which is a function of y1 … yn, contains the true value 
of θ. All we know is that a sequence of specific CIs will contain θ in 
)%
1(
100
D

 of all cases 
in the long run.
--
It is desirable that the CI is short, in order to be informative, and also that 
α
−
1
is high, so 
that the CI is reliable. However these two aspects are incompatible. By increasing 
α
−
1
 we are 
also increasing the length of the CI.
--
There is no golden rule to solve the conflict between length and level of a CI. In practice it 
is up to the statistician to use common sense in this matter. If the sample size is small and if 
population variance 
)
(Y
V
 is large, then one should be prepared to decrease the confidence 
level rather than stick to the conventional level of 0.95. 
In Ch. 4 there were some requirements on an estimator, and especially that it should be an unbiased 
MVE. Here we define a ‘best’ interval estimator by the requirement that
)
ˆ
ˆ
(
L
U
E
θ
θ
−
is minimum for 
given n and 
α
−
1
.
An important method for finding a CI for θ is to find a pivotal statistic, i.e. a statistic with the following 
properties:
1)	 It is a function of Y1 … Yn and θ.
2)	 Its probability distribution does not depend of θ.
An example of a pivotal statistic is 
)1
(
~
/)1
(
2
2
2
−
−
n
n
S
χ
σ
(Cf. EX 18). The subsequent examples will 
show how the pivotal method works.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
65 
Interval estimation
5.2	
CIs in small samples by means of pivotal statistics
EX 51( )n
i
iY
1
= are iid where 
)
,
(
~
2
σ
µ
N
Yi
. 
a)	 Construct 95% CIs for
2
 
and
 
σ
µ
when n = 10.
b)	 Determine the specific CIs when 
81
and
80
2  
 
S
y
.
a) CI for µ .
From Ch. 3.1(11) , 
)1
(
~
)1
(
)1
(
~
/
/
)1,0
(
~
/
)
(
/
)
(
2
−
−
−
−
=
−
n
T
n
n
n
n
S
N
n
Y
n
S
Y
χ
σ
σ
µ
µ
. This quantity is pivotal. Let C be some constant, then 
since the T- distribution is symmetric around zero
(
)



<
−
<
−






+
<
<
−
=






<
−
<
−
=
−
(ii)
     
          
)1
(
(i)
   
/
1
C
n
T
C
P
n
S
C
Y
n
S
C
Y
P
C
n
S
Y
C
P
µ
µ
α
In (i) we have simply rearranged the inequality so that µ is centered. (ii) is used to determine C in which case we 
must know the confidence level 
α
−
1
and n.
With 
95
.0
1
 
D
 we get: 

262
.2
95
.0
)
9
(
 

 



C
C
T
C
P
, obtained from tables of the T- distribution. The 
95% CI for µ is thus 
¸¸
¹
·
¨¨
©
§


10
262
.2
,
10
262
.2
S
Y
S
Y
.
Notice that the interval can be constructed before the sample is taken.
CI for
2
σ
From EX 18, 
)1
(
)1
(
~
2
2
2
−
−
n
n
S
χ
σ
which is not pivotal, but
)1
(
~
)1
(
2
2
2
−
−
n
n
S
χ
σ
 is. The chi-square 
distribution is not symmetric so we consider two constants a and b such that
(
)
  
    
          
)1
(
)1
(
)1
(
)1
(
1
2
2
2
2
2
2



<
−
<






−
<
<
−
=






<
−
<
=
−
b
n
a
P
a
n
S
b
n
S
P
b
n
S
a
P
χ
σ
σ
α
With 
95
.0
1
 
D
 the area below a under the chi-square density is 0.025 and the area above b is 0.025. Thus, 


0228
.
19
,
7004
.2
95
.0
)
9
(
2
 
 

 


b
a
b
a
P
F
.  The 95% CI for 
2
σ
is 
¸¸
¹
·
¨¨
©
§
7004
.2
9
,
0228
.
19
9
2
2
S
S
.
b) 
The lower and upper limits in the specific CI for µ are 
4.6
80
10
9
262
.2
80
r
 
r
,  so the 95% CI is (73.6,86.4).
The specific CI for 
2
σ
is 


0.
270
,3.
38
or 
7004
.2
81
9
,
0228
.
19
81
9
¸
¹
·
¨
©
§


.  This interval is very wide, as it should be since 
variance is a squared quantity, such as dollar2 or kg2. It would be wise to use a confidence level lower than 95% in 
this case. 
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
66 
Interval estimation
EX 52( )n
i
iY
1
= are iid with 
[
]
b
Uniform
Yi
,0
~
a)	 Construct a 95% CI for b based on the largest observation in the sample.
b)	 Determine the specific 95% CI from the following data: 28, 19, 31, 12, 15.
a) 
From (8) in Ch. 3.1the cdf of the largest observation 
)
(n
Y
is 
b
y
b
y
y
F
n
Y n
≤
≤






=
0
  ,
)
(
)
(
.
)
(n
Y
 is 
not pivotal, but we may try to find a pivotal statistic by the following device. Consider 
)
(n
CY
 with cdf 






n
Y
n
n
CY
Cb
y
C
y
F
C
y
Y
P
y
CY
P
y
F
n
n
¸
¹
·
¨
©
§
 
 
d
 
d
 
)
/
/
)
(
)
(
)
(
)
(
)
(
.  This is not dependent on b if 
b
C
/
1
=
. 
Thus, 
b
Y n /
)
(
is pivotal with cdf
b
y
y
y
F
n
b
Y n
≤
≤
=
0 ,
)
(
/
)
(
.
We now proceed as in EX 51.






°
¯
°
®
­
 

 
 

 

 
 




 
n
n
n
n
n
n
n
c
c
c
b
Y
P
c
c
c
b
Y
P
c
b
Y
c
P
1
2
2
2
)
(
1
1
1
1
)
(
2
)
(
1
975
.0
975
.0
/
025
.0
025
.0
/
/
95
.0
It remains to put the parameter b in the center,


¸¸
¸
¹
·
¨¨
¨
©
§

¸¸
¹
·
¨¨
©
§


 


n
n
n
n
n
n
n
Y
Y
c
Y
b
c
Y
P
c
b
Y
c
P
1
)
(
1
)
(
1
)
(
2
)
(
2
)
(
1
025
.0
,
975
.0
/
 is a 95% CI for b.
b)	
Here 
)
8.
64
,2.
31
(
025
.0
31
,
975
.0
31
31
and
5
5
1
5
1
)
(
 
¸¸
¸
¹
·
¨¨
¨
©
§

 
 
n
Y
n
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
67 
Interval estimation
EX 53 Given two independent sets of iid variables (
)
( ) Y
X
n
i
i
n
i
i
Y
X
1
1
 
and
 
=
=
where 
)
,
(
~
2
X
X
i
N
X
σ
µ
and 
)
,
(
~
2
Y
Y
i
N
Y
σ
µ
.
a)	 Construct a CI for the ratio
2
2 /
Y
X σ
σ
.
b)	 Construct a CI for the difference (
)
Y
X
µ
µ
−
assuming that 
2
2
2
σ
σ
σ
=
=
Y
X
.
a)	
Consider the two unbiased estimators
2
2
2
2
ˆ
 
and
 
ˆ
Y
Y
X
X
S
S
=
=
σ
σ
. These are independent since they are based on 
independent sets of variables. From EX 18 in Ch. 3.1 we know that
)1
(
)1
(
~
2
2
2
−
−
X
X
X
X
n
n
S
χ
σ
and 
⇒
−
−
)1
(
)1
(
~
2
2
2
Y
Y
Y
Y
n
n
S
χ
σ
)1
,1
(
~
)1
/(
)1
(
)1
/(
)1
(
~
2
2
2
2
2
2
2
2
−
−
⋅
−
−
−
−
⋅
Y
X
Y
X
Y
Y
X
X
Y
X
Y
X
n
n
F
n
n
n
n
S
S
σ
σ
χ
χ
σ
σ
[Cf. the F- distribution in Ch. 3.1.]
The latter quantity is not pivotal, but 
)1
,1
(
~
2
2
2
2
−
−
⋅
Y
X
Y
X
X
Y
n
n
F
S
S
σ
σ
is.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Exercises in Statistical Inference  
with detailed solutions
68 
Interval estimation
EX 53 (Continued)
The F- distribution is not symmetric so we choose two constants 
2
1
 
and
 
c
c
as limits in the following inequality
(
)
(i)
(ii)
     
)1
,1
(
1
2
1
2
1
2
2
2
2
2
2
2
2
2
2
2
1



<
−
−
<






<
<
=






<
⋅
<
=
−
c
n
n
F
c
P
S
c
S
S
c
S
P
c
S
S
c
P
Y
X
Y
X
Y
X
Y
X
Y
X
X
Y
σ
σ
σ
σ
α
In (i) we have simply centered the variance ratio. The expression in (ii) is used to determine the two constants
2
1
 
and
 
c
c
. However, this may be cumbersome, especially since F- tables often are incomplete. We devote a few 
lines to show how this can be done.
Let 
95
.0
1
and
10
,
25
2
1
 

 
 
D
n
n
.  We want to determine 
2
1
 
and
 
c
c
so that 
(ii) holds. Since 

025
.0
)
9,
24
(
2  
! c
F
P
 we obtain 
61
.3
2  
c
 by using Table 7 in 
Wackerly et al. It is harder to find the value of 
1c from the table. The value of 
1c giving 



  

 
!
1
1
)
9,
24
(
or 
975
.0
)
9,
24
(
c
F
P
c
F
P
 0.025 is not shown. Instead we use the fact that [See Ch. 3.1 (12).] 


37
.0
70
.2
1
025
.0
1
)
24
,9
(
)
24
,9
(
1
)
9,
24
(
1
1
1
1
1
 

 

 
¸¸
¹
·
¨¨
©
§
!
 
¸¸
¹
·
¨¨
©
§

 

c
c
c
F
P
c
F
P
c
F
P
.
In this case the 95% CI for 
2
2
Y
X
σ
σ
is 
¸¸
¹
·
¨¨
©
§


2
2
2
2
37
.0
,
61
.3
Y
X
Y
X
S
S
S
S
. 
b)	
(
)
(
)
(
))
/
1
/
1(
,
~
)
(
/
,
~
 ,
/
,
~
2
2
2
Y
X
Y
X
Y
Y
X
X
n
n
N
Y
X
n
N
Y
n
N
X
+
−
−
⇒
σ
µ
µ
σ
µ
σ
µ
, since a linear function of 
normally distributed variables is itself normally distributed (Cf. Ch. 2.2.2). Thus,
(
)
)1,0
(
~
/
1
/
1
)
(
)
(
2
N
n
n
Y
X
Y
X
Y
X
+
−
−
−
σ
µ
µ
. This is pivotal, but it can’t be used since 
2
σ
is unknown. We need an estimator 
of 
2
σ
.
 From EX 49 it follows that 
1
1
)1
(
)1
(
ˆ
2
2
2
−
+
−
−
+
−
=
Y
X
Y
Y
X
X
n
n
S
n
S
n
σ
 is BLUE for
2
σ
. Since 
)1
(
~
)1
(
2
2
2
−
−
X
X
X
n
S
n
χ
σ
and 
)1
(
~
)1
(
2
2
−
−
y
Y
Y
n
S
n
χ
it follows that 
(
)
)
2
(
)
2
(
~
1
1
)1
(
)1
(
~
ˆ
2
2
2
2
2
2
−
+
−
+
−
+
−
−
+
−
Y
X
Y
X
Y
X
Y
X
n
n
n
n
n
n
n
n
χ
σ
χ
χ
σ
σ
 [Cf. Ch. 3.1 (8)]
The following statistic is pivotal and useful
)
2
(
~
)
2
(
)
2
(
)1,0
(
~
)
/
1
/
1(
)
/
1
/
1(
ˆ
)
/
1
/
1(
)
(
)
(
)
/
1
/
1(
ˆ
)
(
)
(
2
2
2
2
2
−
+
−
+
−
+
+
+
+
−
−
−
=
+
−
−
−
Y
X
Y
X
Y
X
Y
X
Y
X
Y
X
Y
X
Y
X
Y
X
n
n
T
n
n
n
n
N
n
n
n
n
n
n
Y
X
n
n
Y
X
χ
σ
σ
σ
µ
µ
σ
µ
µ
Proceeding in the same way as in EX 51 we finally obtain the lower and upper CI limits as
)
/
1
/
1(
ˆ 2
Y
X
n
n
C
Y
X
+
±
−
σ
, where C is determined from the 
)
2
(
−
+
Y
X
n
n
T
-distribution.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
69 
Interval estimation
Comment to EX 53 The results in this exercise are crucial for comparing two means. By making a CI 
for 
)
(
Y
X
µ
µ
−
the main interest is whether the CI encloses zero. If a 95% CI, or a CI of higher level, 
does not contain zero it is customary to conclude that the two means are significally different. This way 
of claiming statistical significance is different from another one based on statistical hypothesis testing 
that is considered in Ch. 6.
Notice that the first step was to make a CI for the variance ratio
2
2 /
Y
X σ
σ
. If the latter encloses 1, the 
customary conclusion is that there is no significant difference between the variances, which therefore 
could be set equal. The CI for 
)
(
Y
X
µ
µ
−
was then constructed under this assumptions. On the other 
hand, if the CI for the variance ratio does not enclose 1 we can’t claim that variances are equal and a 
different approach is required. This is called the Behrens-Fisher’s problem for which several approximate 
solutions have been suggested. The latter are however beyond the level of this book. Most statistical 
software present solutions both with and without the assumption of equal variances.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Exercises in Statistical Inference  
with detailed solutions
70 
Interval estimation
EX 54( )n
i
iY
1
= are independent with
)
,
(
~
2
σ
β i
i
x
N
Y
. Construct CIs for
2
 
and
 
σ
β
.
CI for β
From EX 48 we know that 
¦
¦
 
 
2
ˆ
ˆ
i
i
i
BLUE
ML
x
Y
x
E
E
 and that 
)1,0
(
~
/
ˆ
2
2
N
xi
ML
¦

V
E
E
. The latter 
statistic is pivotal but it can’t be used since 
2
σ
is unknown. As an unbiased estimator of 
2
σ
we take 
)1
(
)1
(
~
)1
(
)
ˆ
(
ˆ
2
2
2
2




 ¦
n
n
n
x
Y
i
ML
i
F
V
E
V
 [Cf. EX 48], where 
2
ˆ
and
ˆ
V
E ML
 are independent.
A pivotal statistic that is useful is
)1
(
~
)1
(
)1
(
)1,0
(
~
/
/
ˆ
/
ˆ
/
ˆ
ˆ
2
2
2
2
2
2
2
2
2




 

¦
¦
¦
¦
n
T
n
n
N
x
x
x
x
i
i
i
ML
i
ML
F
V
V
V
E
E
V
E
E
,  the latter distribution being symmetric around zero. Thus


°¯
°®
­




¸¹
·
¨©
§




 
¸¸
¸
¹
·
¨¨
¨
©
§




 

¦
¦
¦
C
n
T
C
P
x
C
x
C
P
C
x
C
P
i
ML
i
ML
i
ML
)1
(
/
ˆ
ˆ
/
ˆ
ˆ
/
ˆ
ˆ
1
2
2
2
2
2
2
V
E
E
V
E
V
E
E
D
The CI for 
¦
r
2
2 /
ˆ
ˆ
is
i
ML
x
C V
E
E
 where C is determined from the 
)1
( −
n
T
- distribution. To illustrate the 
computation of C, let n = 10 and assume that we want a 90% CI for β . Since the area under the T – density 
between –C and C is 0.90, the area above C is 0.05. (Most tables today show areas above C.) From the tables we get 
C = 1.833.
CI for 
2
σ
2
ˆσ
is not pivotal, but 
)1
(
~
)1
(
ˆ
2
2
2
−
−
n
n
χ
σ
σ
is. Therefore, and since the chi-square distribution is not symmetric, 
there are two constants a and b to be determined.
(
)



<
−
<






−
<
<
−
=






<
−
<
=
−
b
n
a
P
a
n
b
n
P
b
n
a
P
)1
(
)1
(
ˆ
)1
(
ˆ
)1
(
ˆ
1
2
2
2
2
2
2
χ
σ
σ
σ
σ
σ
α
Here a and b are determined as in EX 51. The CI for 
2
σ
is thus 






−
−
a
n
b
n
)1
(
ˆ
,)1
(
ˆ
2
2
σ
σ
.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
71 
Interval estimation
5.3	
Approximate CIs in large samples based on Central Limit Theorems
In Ch. 2.2.2 (a) the Central Limit Theorem (CLT) was stated for a standardized sum of iid variables, 
denoted by
n
Z , and for a Poisson process in Ch. 2.2.2 (c), denoted by
)
(t
Z
. In Ch. 4.3.4 it was stated that 
standardized ML estimators have asymptotic 
)1,0
(
N
-distributions. These results can be used to find 
CIs that holds approximately in large samples. Since the CIs only hold approximately it is important to 
understand the meaning of ‘a large sample’. Below some examples of CIs derived from CLTs are given.
EX 55 
)
,
(
~
p
n
Binomial
Y
, or equivalently ( )n
i
iY
1
=  are iid with 
)
(
~
p
Bernoulli
Yi
. Determine a CI for p based 
on
∑
=
n
Y
Y
i
 
large
 a 
and
 
. Put
n
Y
p
/
ˆ =
.
a)	 Give a justification for the formula 
n
p
p
C
p
/)ˆ
1(ˆ
ˆ
−
±
that is often found in textbooks. (Sometimes division 
by n –1 is used instead of n.).
From EX 23c)
(
)
(
)



<
<
−
−
+
<
<
−
−
=








<
−
−
<
−
=
−
C
Z
C
P
n
p
p
C
p
p
n
p
p
C
p
P
C
n
p
p
p
p
C
P
1
/)ˆ
1(ˆ
ˆ
/)ˆ
1(ˆ
ˆ
/)ˆ
1(ˆ
ˆ
1 α
where C is determined from tables of the Normal distribution
b)	 Show that a more accurate CI for p is given by the limits
(
)
(
)
(
)
n
C
n
C
p
n
C
p
n
C
p
/
1
2
/
1
ˆ
4
/
ˆ
2
/
ˆ
2
2
2
2
2
2
2
+
+
−
+
±
+
From EX 23b)
(
)



<
<
−






−
<
−
=








<
−
−
<
−
=
−
(ii)
        
          
(i)
  
)
1(
)
ˆ
(
/)
1(
ˆ
1
2
2
2
C
Z
C
P
n
p
p
C
p
p
P
C
n
p
p
p
p
C
P
α
By solving the inequality in (i) for p it can be shown that p is located between the limits stated in b). C in (ii) is 
obtained from the normal distribution. The CI in b) is more accurate since the approach to normality goes much 
faster for 
2
Z than for
1
Z . Notice that the approach to normality for 
1
Z requires not only convergence in distribution, 
but also convergence in probability as was shown in EX 23 c).
Comment to EX 55 It is important to know the difference between the expressions given in EX 55 a) 
and b). The former is often stated in textbooks as being a result of the CLT and sometimes lowest sample 
sizes of 30–50 are advocated. This may hold for the validity of the expression in b), but definitely not 
for the expression in a).
In 1963 an interesting relation was found between the Binomial – and F distributions by G.H. Jowett. 
By using this it is possible to obtain a CI for p that holds for any sample size. The latter may be hard to 
find in text books at master level in statistics, but yet we present it here since the result is very useful. 
(Cf. Casella & Berger 1990, p. 499.)
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
72 
Interval estimation
Let 
)
,
(
2
1
975
.
f
f
F
be the 97.5% percentile of the F distribution, i.e. (
)
025
.0
)
,
(
)
,
(
2
1
975
.
2
1
=
>
f
f
F
f
f
F
P
.  
Then a 95% CI for p is given by






−
+
+
+
−
−
+
+
+
−
+
−
+
))
(
2
),1
(
2
(
)1
(
))
(
2
),1
(
2
(
)1
(
,)
2
),1
(
2
(
)1
(
975
.
975
.
975
.
Y
n
Y
F
Y
Y
n
Y
n
Y
F
Y
n
Y
n
F
Y
n
Y
Y

(23)
If 
0
=
Y
then the lower limit is 0 and if 
n
Y =
then the upper limit is 1. 
The expression in (23) gives CIs that are conservative in the sense that they give CIs with a confidence 
level of at least 95%. For simplicity a 95% CI was considered. If a 99% CI was required we would instead 
search for 99.5% percentiles in the F-distribution.
EX 56 Use the expressions for a CI in (21) and in EX 55 a) and b) to calculate 95% CIs for p in the two cases 
)
100
,
10
(
and
)
20
,2
(
 
 
 
 
n
y
n
y
.
To use (23) we have to determine the 97.5% percentiles of the F-distribution. This can be a problem since F-tables 
are often incomplete and percentiles are only shown for a few degrees of freedom. The best one can do is to use 
statistical software, such as SAS or SPSS, to find the percentiles. In worst case one may be forced to use linear 
interpolation.
In the case 
)
20
,2
(
 
 
n
y
 we find 
7846
.2
)
36
,6
(
and
4191
.8
)
4,
38
(
975
.
975
.
 
 
F
F
.  (These values were 
obtained by using the function 
)
,
,
975
.0
(
2
1 f
f
finv
in SAS.) Similarly, in the case 
)
100
,
10
(
 
 
n
y
 we get 
7503
.1
)
180
,
22
(
and
1326
.2
)
20
,
182
(
975
.
975
.
 
 
F
F
.
In both cases we get the same point estimate of p, 2/20=10/100 = 10%, but the CIs are different:
CI (%)
for p
Expression in:
(23)
55 b)
55 a)
)
20
,2
(
 
 
n
y
(1.2, 31.7)
(2.8, 30.1)
(-3.1, 23.1)
)
100
,
10
(
 
 
n
y
(4.9, 17.6)
(5.5, 17.4)
(4.1, 15.7)
The CIs based on (23) are certainly wider, but they are more reliable since they are conservative as mentioned above. 
Notice that the expression in 55 a) can result in peculiar CIs in small samples.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
73 
Interval estimation
EX 57 Determine a CI for the rate λ in a Poisson process. 
In Ch. 2.2.2 (4)(c) it was seen that, if 
)
(t
Y
is a Poisson process of rate λ ,then
f
o
o


 

 
t
N
Z
t
t
t
Y
t
t
t
Y
t
Z
D
as
)1,0
(
~
/
/)
(
)
(
)
(
O
O
O
O
.  This statistic is asymptotically pivotal, but 
there are some difficulties to obtain an inequality for λ by using this fact. Instead we notice that the statistic 
.
as
,
/)
(
ˆ
f
o
o

 
t
t
t
Y
P
O
O
 [This follows from (10) in Ch. 3.2.1 since 
(
)
λ
λ
λ
=
=
=
t
t
t
Y
E
t
E
)
(
1
)ˆ
(
and 


f
o
o
 
 
 
W
W
W
W
W
<
9
W
9

DV




Ö



O
O
O
@. Thus,
1
/
/ˆ
/
ˆ
/ˆ
ˆ
→

→

−
=
−
P
D
t
t
Z
t
t
λ
λ
λ
λ
λ
λ
λ
λ
, and from (11) in Ch. 3.2.1 we get 
Z
t
D
→

−
/ˆ
ˆ
λ
λ
λ
. Now,
(
)



<
<
−




+
<
<
−
=








<
−
<
−
=
−
C
Z
C
P
t
C
t
C
P
C
t
C
P
/ˆ
ˆ
/ˆ
ˆ
/ˆ
ˆ
1
λ
λ
λ
λ
λ
λ
λ
λ
α
. So 
t
C
/ˆ
ˆ
λ
λ ±
are the CI limits forλ , 
where C is determined from tables over the normal distribution.
EX 58 ( )n
i
iY
1
= are iid variables from an unspecified distribution with mean µ and variance
2
σ
. If n is large a 95% CI 
for µ is given by
n
S
Y
96
.1
r
(This is perhaps the most cited expression in statistical inference and is found in most elementary text books. 
Sometimes 1.96 is replaced by the figure 2,)
Give a rigorous motivation for the expression!
From (9b) 
2
2
2
as
,0
)
(
V
o


f
o
o
P
S
n
S
V
 (Cf. (10) in Ch. 3.3.1)
1
)
(
2
2
2
→

=
⇒
P
S
S
g
σ
(Cf. (11) in 
Ch. 3.3.1). Thus, 
)1,0
(
~
1
/
/
)1,0
(
~
/
/
N
Z
n
n
S
N
Z
n
Y
n
S
Y
D
P
D
o

o

o


 

V
V
P
P
.
For large n, 0.95=
¸¸
¹
·
¨¨
©
§




 
¸¸
¹
·
¨¨
©
§




n
S
Y
n
S
Y
P
n
S
Y
P
96
.1
96
.1
96
.1
/
96
.1
P
P
.
The simple expression above should be used with caution. Especially if the population distribution is heavily skewed 
or has multiple peaks, a very large n would be required. 
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
74 
Interval estimation
5.4	
Some further topics
5.4.1	
Selecting the sample size
Looking back at the examples of this chapter it is seen that the bounds of a CI are functions of the sample 
size n. This opens the possibility to determine n in advance in such a way that the CI has a stipulated 
length. The problem is that the bounds of a CI are also dependent on the values of one or several statistics 
that not yet have been computed. There is no simple solution to this problem but some guide lines can 
be given when the CI has the structure 
)ˆ
(ˆ
ˆ
p
V
C
±
θ
. The term 
)ˆ
(ˆ θ
V
C
is called Bound on the Error 
(BE). We consider two cases, a proportion and a mean.
•	 Bernoulli proportion p in large samples
The CI is 
n
p
p
C
BE
n
p
p
C
p
/)ˆ
1(ˆ
so
,
/)ˆ
1(ˆ
ˆ

 

r
. (Division by n – 1 instead of n is of minor importance.) 
Values of pˆ can be obtained in several ways:
--
Worst case scenario. Choose
2
/
1
ˆ =
p
. It is easily shown that this value maximizes
)ˆ
1(ˆ
p
p
−
 for
1
ˆ
0
≤
≤p
. The maximal BE now becomes
n
C 2
/
. This solution should only be used when 
there is no information whatsoever about pˆ .
--
Qualified guess. Here one uses earlier experience to guess the value of pˆ . Notice that the 
function 
)ˆ
1(ˆ
p
p
−
 is symmetric around 
2
/
1
ˆ =
p
so e.g. 
10
.0
ˆ  
p
 gives the same BE as 
90
.0
ˆ  
p
--
Pilot study. The idea is to take a first small sample (pilot sample) to estimate p. Observations 
from the pilot sample could then be included into the final sample. The approach is appealing 
since it is free from more or less reliable assumptions. A problem is to decide how large the 
pilot sample shall be. One solution is to collect data sequentially and compute estimates 
n
pˆ
for increasing n until the estimates have stabilized. Usually this occurs for n less than 20-30.
After having determined an appropriate value of pˆ it is instructive to plot BE on the Y-axes against n 
on the X-axis for various choices of C. (Remember that 
2.575
 
1.960,
 ,
645
.1
=
C
 corresponds to the 
confidence levels 90%, 95% and 99%, respectively.) Alternatively, in the expression for BE above one can 
solve for n, giving
(
)2
/
)ˆ
1(ˆ
B
C
p
p
n
−
=
.
One should be aware that data collection in large samples can be costly. A simple expression for the total 
cost is
n
c
c
⋅
+
0
, where 
0c  is a fixed cost and c is the cost for each sample unit.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
75 
Interval estimation
EX 59 Determine the sample size needed to get a CI for p with a BE of 0.01 or alternatively 0.025. The CI levels shall be 
90%, 95% or 99%.
a)	 Use the worst case scenario.
b)	 Use a pilot sample with the data    
15
1
i
iY
 (0,0,1,0,0,0,0,1,0,0,0,0,0,0,1) where 
p
Y
P
i
=
= )1
(
.
In a) we use
2
/
1
ˆ =
p
.
In b) we conclude from the table below that 
20
.0
ˆ  
p
 may be appropriate
n
10
11
12
13
14
15
n
y
n
i
i /
1∑
=
.20
.18.
.17
.15
.14
.20
The following table illustrates how the sample size n differs between the two approaches in a) and b):
a)
p = 0.5
b)
p = 0.2
CI level
BE
n
CI level
BE
n
90%
0.01
6765
90%
0.01
4330
95%
“
9604
95%
“
6147
99%
“
16577
99%
“
10609
90%
0.025
1082
90%
0.025
693
95%
“
1537
95%
“
983
99%
“
2652
99%
“
1697
It is seen that the sample size increases with increasing CI level and decreases with increasing length of the CI. The 
approach in a) leads to unnecessary large samples compared with the approach in b).
•	
in
mean
Population
P
 large samples
In EX 58 it was shown that the limits 
n
S
C
Y
/
⋅
±
gives a CI for µ  in large samples provided that the 
observations are iid. The Bound on the Error is 
n
S
C
BE
/

 
 from which 
2
2
2
/ BE
S
C
n  
.  In the 
latter expression S can be determined in at least two ways.
--
‘Empirical rule’. Replace S2 by the true variance σ2. Since 99% of the observations are found 
within the variation limits 
V
P
58
.2
r
,  the range of y–values is roughly 
|

V
58
.2
2
V
2.5
. 
From this we get
σ
σ
2.5
/
range
S
≈
≈
. (Sometimes the figure 2.8 is replaced by 
2
96
.1
|
, 
corresponding to 95% variation limits, which gives 
σ
4
/
range
S ≈
.)
This approach has several drawbacks. There is a great amount of arbitrariness in the choice of 
coefficient, 2.58 or 2, and sometimes even 3. As a consequence there will be large differences 
in the choice of n. Furthermore, in many cases it can be hard to identify the range of possible 
y-values.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
76 
Interval estimation
--
Pilot study. As in the case with a Bernoulli proportion, we may take a first small sample to 
obtain a likely value of S2. Data are collected sequentially until the value of S2 has stabilized. 
The calculations can be performed in any of the following ways.
For 
5
>
n
, say: 
/
)
(
 
and
 
1
2
1
2
2
1
2
1






−
=
⇒
∑
∑
∑
∑
=
=
=
=
n
y
y
S
y
y
n
i
i
n
i
i
n
n
i
i
n
i
i
/ (n – 1) or from the recursive 
relations 
)1
(
)
(
)1
(
,
1
2
1
2
2
1
1
1
+
−
+
−
=
+
⋅
+
=
+
+
+
+
n
y
y
n
S
n
S
n
y
n
y
y
n
n
n
n
n
n
n
. (The latter relations are found 
in Casella & Berger, p. 244.)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Exercises in Statistical Inference  
with detailed solutions
77 
Interval estimation
EX 60 In order to construct a 95% CI for the mean Area of Optic Disc (AOD) in a group of children, one wants to 
determine the sample size n that is needed to get a Bound on the Error (BE) of about 0.10 to 0.20  
(
2
mm ). In a pilot study the following values were obtained sequentially.
n
1
2
3
4
5
6
7
8
9
10
AOD
2.39
3.33
2.12
1.90
2.66
2.53
2.30
2.98
2.59
2.70
n
11
12
13
14
15
AOD
2.20
2.77
1.86
2.72
3.28
Use the data to first calculate a value of S2 and then suggest a proper sample size.
The sequentially calculated values of S2 are, starting from n = 7.
n
7
8
9
10
11
12
13
14
15
2
n
S
0.21
0.21
0.19
0.17
0.16
0.15
0.18
0.17
0.20
An appropriate value seems to be 
20
.0
2  
S
 that gives 
2
2
/
20
.0
96
.1
BE
n

 
.  The desired sample sizes are thus 
10
.0
for 
77
 
 
BE
n
 and 
20
.0
for 
20
 
 
BE
n
.
5.4.2	
CI for a function of a parameter
Given a CI for
U
L
θ
θ
θ
θ
ˆ
ˆ
say 
 ,
<
<
, it is possible to make a CI for a function of 
)
(
 ,
θ
θ g
, provided that the 
latter is monotonous (decreasing or increasing). The approach is illustrated in the following examples.
EX 61 ( )n
i
iY
1
= are iid where
)
(
~
λ
l
Exponentia
Yi
.
a)	 Determine a 95% CI for λ based on the statistic∑
=
n
i
iY
1
. Compute the CI limits when n=50 and 
0.
65
1
 
¦
 
n
i
iy
.
Compute the corresponding CI limits for the survival function
y
e
y
Y
P
⋅
−
=
>
λ
)
(
.
Yi ~ Gamma( ,1)
Ch. 3.1
[
]
Yi
i=1
n
~ Gamma( ,n)
Ch..2.2.2
[
]
)
2
(
~
2
2
1
n
Y
n
i
i
χ
λ∑
=
. Thus, 


°
¯
°
®
­


¸
¸
¹
·
¨
¨
©
§


 
¸¸
¹
·
¨¨
©
§


 
¦
¦
¦
 
b
n
a
P
Y
b
Y
a
P
b
Y
a
P
i
i
n
i
i
)
2
(
2
2
2
95
.0
2
1
F
O
O
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
78 
Interval estimation
From tables of the Chi square distribution (e.g. in Wackerly et al 2007, pp850-851) we get, with 
50
 
n
,




¯
®
­
 

 
!
 

 
!
22
.
74
975
.0
)
100
(
56
.
129
025
.0
)
100
(
2
2
a
a
P
b
b
P
F
F
. 
The CI for λ  is thus 


00
.1
,
57
.0
65
2
56
.
129
,
65
2
22
.
74
 
¸
¹
·
¨
©
§


.
b)	
y
e
y
Y
P
⋅
−
=
>
λ
)
(
is monotonously decreasing with λ . Therefore
y
y
y
U
L
L
U
e
e
e
⋅
−
⋅
−
⋅
−
<
<
⇒
<
<
λ
λ
λ
λ
λ
λ
ˆ
ˆ
ˆ
ˆ
. It follows that the 95% CI for the survival function is 

\
\ H
H






Notice that the latter CI will cover the true value in 95% of all cases at one specific value of y. It may be tempting to 
plot the lower and upper limits against y, thereby creating a so called confidence region. The latter will however not 
contain the true values in 95% of all the cases, since we are making several confidence statements simultaneously 
which in turn will reduce the confidence level. This Multiple inference problem is discussed further in Ch. 6.4.
In Ch. 2.2.1 (3) it was stated that if 
)
(
 
and
 )
(
t
Y
s
X
are Poisson processes of rates
Y
X
λ
λ
 
and
 
, respectively, 
then the conditional variable (
)
)
,
(
~
)
(
)
(
)
(
t
s
t
p
n
Binomial
n
t
Y
s
X
t
Y
Y
X
Y
λ
λ
λ
+
=
=
+
. This can be used to 
make a CI for the ratio
X
Y
R
λ
λ /
=
. Due to its importance we formulate the solution of the problem as 
a theorem.
A CI for the ratio of two Poisson rates 
X
Y
R
λ
λ /
=
can be constructed in the following way:
a)	 First, make a CI for the Binomial proportion p giving(
)
U
L p
p
ˆ
,
ˆ
.
b)	 A CI for R is then obtained as






−
=
−
=
t
s
p
p
R
t
s
p
p
R
U
U
U
L
L
L
)
ˆ
1(
ˆ
ˆ
,
)
ˆ
1(
ˆ
ˆ
.
(24)
This follows easily from the fact that 
t
s
p
p
p
R
s
Rt
Rt
p
)
1(
)
(

 


 
 and this is a function that increases 
monotonously from 
0
)
0
(
=
R
to infinity as
1
→
p
. 
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
79 
Interval estimation
EX 62 In the snow-free period April–November there were 85 road accidents on a certain stretch of road and during 
the winter period December–March there were 65 road accidents on the same stretch. Is the rate of road accidents 
significantly higher during the winter period?
Introduce the notations 
Y
X
Y
X
O
O
rate
of
period,
r
     winte
-"-
)
4
(
rate
of
period,
free
-
snow
in the
accidents
of
Number 
)
8
(
 
 
We will answer the question about significance by making a 95% CI for
X
Y
R
λ
λ /
=
.
If the latter does not cover 1 we draw the conclusion that there is a significant difference.
The observed proportion of 
(
))
8(
)
4
(
/)
4
(
X
Y
Y
+
 is 
4333
.0
)
85
65
/(
65
ˆ
 

 
p
.  Since n is 
large we use the expression 
n
p
p
p
/)ˆ
1(ˆ
96
.1
ˆ

r
 for a 95% CI in EX 55 a). This yields the limits 


5142
.0,
3524
.0
CI
or the
0809
.0
4333
.0
r
.  The expression in (24) finally gives the CI limits for R: 
17
.2
4
8
)
5142
.0
1(
5142
.0
ˆ
,
09
.1
4
8
)
3524
.0
1(
3524
.0
ˆ
 

 
 

 
U
L
R
R
.  Since the latter interval does not cover 1and is in fact 
located above 1, the conclusion is that the rate of road accidents is significantly higher in the winter period.
In Ch. 6 we will consider other ways to claim statistical significance.
5.5	
Final words
Verify that you can find the points a and b in the 
−
−
F
 
and
 
2
χ
distributions such that 2.5% of the 
observations are smaller than a and 97.5% of the observations are larger than b. The intervals in the 
examples of Ch. 5 are 95% CIs. Change the confidence levels to 90% and 99% to study the effect on the 
lengths of the CIs.
Remember the interpretation of a CI. If you repeatedly construct 95% CIs, then in the long run there 
will be 1 interval of 20 that doesn’t cover the true parameter value.
Notice that proportions around 1/2 require the largest sample size for a given confidence level and Bound 
on the Error. Many people do not agree about this. Therefor you should go through the arguments in 
Ch. 5.4.1, so you can persuade them. 
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
80 
Interval estimation
Supplementary Exercises, Ch. 5
EX 63 The following data shows body weight (kg) of 10 males before (X) and after (Y) participating in a training 
program with the purpose to reduce weight.
Subject
1
2
3
4
5
6
7
8
9
10
X
88.3
94.6
88.4
102.5
94.3
79.3
86.3
96.9
88.5
101.8
Y
88.1
93.5
88.5
102.0
94.7
78.5
86.1
96.2
88.2
101.1
a)	 Does the training program have a significant effect on weight-loss? Answer the question by drawing 
conclusion from a 95% CI for the average weight-loss. 
[Hint: Just look at the differences within subjects. Don’t use the approach in EX 53. Why?]
b)	 When the same training program was used by a population of females it was found that the variance of the 
weight-loss was 0.7. Does the latter value differ significantly from the variance obtained for males?
c)	 Give a 95% CI for the proportion of males that loses weight. Compare the results that are obtained by using 
the expressions in EX 55 a) and in (23).
d)	 As expected, the CI in c) becomes very wide. Consider the sample above as a pilot sample and determine the 
sample size needed to get a 95% CI with a Bound on the Error that is 0.025.
EX 64 Data below summarizes measurements of Area of Optic Disk (AOD) in mm2 from two samples of children called 
FAS and Control. Children in the FAS (Fetal Alcoholic Syndrome) group had mothers who were high-consumers of 
alcohol during pregnancy.
FAS
Control
Sample size
22
30
Mean
2.01
2.55
Variance
0.3623
0.2305
Determine a 95% CI for the difference of mean AOD between the two groups.
EX 65 Let ( )n
i
iY
1
= be iid where
)
(
~
λ
l
Exponentia
Yi
.
a)	 Determine a 95% CI for λ based on the fact that (
)
)1,0
(
~
)
(
/
)
(
N
Z
Y
V
Y
E
Y
D
→

−
.
b)	 Compute the expected length of the CI in a) when 
50
 
n
.  Compare the latter with the expected length of 
the CI in X 61 a)
EX 66 During an epidemic a sample of five institutions at a university was randomly selected. These were asked how 
many of their employees who were on the sick-list. The result was
Institution
1
2
3
4
5
Sick-listed
4
10
8
2
6
Total staff
10
42
25
11
12
Give a 95% CI for the total proportion sick-listed at the university.
[Hint: Use the ML estimator in EX 46 together with the CLT.]
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
81 
Interval estimation
EX 67 The number of bacteria (per cm3) in a certain type of food varies according to a Poisson distribution. In a 
sample of 4 units one obtained the following result
Unit 
1
2
3
4
Number of bacteria
103
112
91
117
Determine a 95% CI for the mean number of bacteria.
[Hint: Use the asymptotic normality of the Poisson distribution.]
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Exercises in Statistical Inference  
with detailed solutions
82 
Hypothesis Testing
6	 Hypothesis Testing
In Ch. 5 we considered one way to claim statistical significance, namely to construct a CI for an unknown 
parameter. In this chapter we will meet another way to claim significance, by setting up hypotheses about 
parameters and to see if these are in accordance with data. There are mainly two ways to do this, the 
p-value approach and the rejection region approach. Both of these are described below.
6.1	
Concepts
6.1.1	
p-value approach
In the p-value approach a basic hypothesis, called the null hypothesis H0, is formulated about one or 
several parameters. In the next step a statistic T = T(Y1 … Yn), called a test statistic, is chosen and the 
value taken by T in a specific sample determines whether H0 shall be rejected or not. The precise way 
in which this is done is illustrated in the following example.
EX 68 Let p be the proportion of born boys in a certain population. We want to test the hypothesis 
2
/
1
:
0
=
p
H
. 
To this end we take sample of n born boys and calculate the value of the test statistic 
X
n
X
p
 
 where
,
/
ˆ =
is the 
number of born boys in the sample. If the value of pˆ deviates ‘very much’ from the value specified by H0 we should 
reject H0. But what is the meaning of ‘very much’, is e.g. X = 7 out of n = 10 enough?
For assistance in this matter we calculate the p-value = (
)
2
/
1
7
=
≥
p
X
P
where it can be assumed that 
)
2
/
1,
10
(
~ Binomial
X
. Thus (Cf. Ch. 2.2.1 (2))
 
¸¸
¹
·
¨¨
©
§

¸¸
¹
·
¨¨
©
§

¸¸
¹
·
¨¨
©
§

¸¸
¹
·
¨¨
©
§
 

0
10
1
9
2
8
3
7
)
2
/
1(
)
2
/
1(
10
10
)
2
/
1(
)
2
/
1(
9
10
)
2
/
1(
)
2
/
1(
8
10
)
2
/
1(
)
2
/
1(
7
10
value
p
1719
.0
)
2
/
1(
176
10  

.
The latter is called a one-sided(one-tailed) p-value. But there is nothing a priori that says that a deviation from H0 only 
goes in one direction in this case. We should therefor also calculate (
)
2
/
1
3
=
≤
p
X
P
= 0.1719. (The Binomial pf is 
symmetric for p = 1/2.)
The two-sided p-value is thus 0.1719+0.1719 =0.34. The latter is the probability of getting observed extreme 
deviations from H0 by mere chance, and it is quite large.
Assume now that we instead have observed 
100
of
out
70
 
 
n
X
.  Since 
)1,0
(
~
/)
1(
ˆ
N
Z
n
p
p
p
p
D
→

−
−
, we can 
calculate a two-sided p-value in the following way:






2
/
1
30
.0
ˆ
00003
.0
00
.4
400
/
1
2
/
1
70
.0
400
/
1
2
/
1
ˆ
2
/
1
70
.0
ˆ
 
d
 
 
t
|
¸¸
¹
·
¨¨
©
§

t

 
 
t
p
p
P
Z
P
p
P
p
p
P
Thus, p-value
00006
.0
00003
.0
2
=
⋅
=
, which is very small.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
83 
Hypothesis Testing
A p-value is thus the probability of obtaining a value on the test statistic as least as extreme as the one 
that is observed, provided that H0 holds. Some comments on this.
--
A p-value should normally be two-sided. Exceptions are when it is obvious that deviations 
only can go in one direction. If you present one-sided p-values in a paper that you send to a 
scientific journal, it is likely that it will be returned since referees often wants two-sided p-values.
--
It is customary to reject H0 when the p-value is less than 0.05. Here is some frequently used 
terminology for this:
0.05<p<0.10
‘Weak significance’
0.01<p<0.05
‘Significance’
*
0.001<p<0.01
‘Strong significance’
**
p<0.001
‘Very strong significance’
***
The concept of ‘Weak significance’ can be found in areas such as Psychology and Sociology 
where sometimes sample sizes are small and the p-values become large only for this reason. 
The use of stars, similar to classification of brandy, has been popular in medical studies, but 
should be avoided. It has actually happened that it has been confused with foot notes.
--
A p-value expresses the degree of evidence against H0 that is found in the present sample and 
nothing else. Hypotheses such as p = 1/2 for the proportion of heads when tossing a coin, or 
mean = 0 for the difference in means between two groups, can strictly speaking be rejected 
without data. (1/2 is not the same as 0.5 or something with more decimals, it is exactly one 
divided by two.) These hypotheses can always be rejected by choosing n sufficiently large. 
Consider a study conducted som years ago of the effect of physical activity upon on the risk 
of getting heart disease. An ‘active´ group consisting of 30 000 subjects and a ‘control’ group 
of 20 000 subjects were followed in time and the proportion of heart diseases were reported in 
each group. In the study a p-value just below 0.05 was obtained for the hypothesis ‘no difference 
between the proportion of heart disease in the two groups’, Newspapers reported that it is now 
proved that physical activity has a statistically significant positive effect on the risk of heart 
disease. The author’s personal reaction to this as a statistician is that, if such large amount of 
data were needed to get a p-value below 5%, then the true difference must be marginal.
--
The p-value concept seems to have been first used by Laplace in the 1770s when studying the 
excess of born boys compared to girls. It was later popularized by R Fisher in the 1920s and he 
invented the term test of significance for this approach. It was later displaced by the rejection 
region approach, to be described in the next section. During the last years the p-value approach 
has regained its leading position. This is probably due to the rapid development of computer 
programs by means of which the computation of p-values is easy, something that wasn’t the 
case 30–50 years ago. Today most statistical soft-ware supply their users with a variety of 
p-values, obtained by using various test statistics and under various assumptions. This in turn 
has increased the need for a higher statistical level of knowledge.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
84 
Hypothesis Testing
6.1.2	
Rejection region approach
As before there is a null hypothesis 
0
H  and a test statistic T. Now there is furthermore an alternative 
hypothesis Ha and a rejection region, RR, such that if T takes a value within RR then 
0
H  is rejected and 
Ha is accepted. (RR is sometimes called a critical region.) By using the symbol ∈ (belongs to) this can be 
expressed as ‘Reject H0’
'
'
RR
T 

. To types of errors can be made in reaching a decision. A type I error 
is made if H0 is rejected when H0 is true. The probability of this event is denoted α and it is customary 
to require that 
05
.0

D
. A type II error is made if H0 is accepted when Ha is true. The probability of 
the latter event is denoted β.
An important concept is that of a power function, which is the probability of rejecting H0. If θ is the 
parameter that is specified by H0, then the power is 


RR
T
P
Pow

 
)
(T
.  Under H0:
0
θ
θ =
, 
α
θ
=
)
(
0
Pow
. 
The latter equality is seldom possible to achieve when the test statistic has a discrete distribution and in 
that case it is required that 
α
θ <
)
(
Pow
. In general the power depends on: (i) θ, (ii) the sample size n, 
(iii) the choice of RR and (iv) the choice of test statistic T. The best test statistic is the one that maximizes 
the power for given θ,n and RR. This is often based on the best estimator. (Stuart et al 1999, Ch. 22.36.)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Exercises in Statistical Inference  
with detailed solutions
85 
Hypothesis Testing
EX 69 Consider the test statistic Y = ‘Number of born boys’ 
)
,
10
(
~
p
n
Binomial
 
 that is used for testing 
2
/
1
:
against 
 2
/
1
:
0
≠
=
p
H
p
H
a
.
Compute the power for each of the RRs: 
^
`
^
`
^
`
10
9,
8,
2,
1,
0,
)
(,
10
,9
,1
,0
)
(,
10
,0
)
(
iii
ii
i
.  Suggest a proper RR.




10
10
0
10
10
0
1
)
1(
)
1(
10
10
)
1(
0
10
10
0
)
(
)
(
p
p
p
p
p
p
Y
P
Y
P
p
Pow
i


 

¸¸
¹
·
¨¨
©
§


¸¸
¹
·
¨¨
©
§
 
 

 
 
.









 
 

 

 
t

d
 
)
(
9
1
)
(
9
1
)
(
)
(
1
1
2
p
Pow
Y
P
Y
P
p
Pow
Y
P
Y
P
p
Pow
ii


8
8
1
1
9
9
1
)
1(
)
1(
10
)
(
)
1(
9
10
)
1(
1
10
p
p
p
p
p
Pow
p
p
p
p




 

¸¸
¹
·
¨¨
©
§


¸¸
¹
·
¨¨
©
§
.




 

¸¸
¹
·
¨¨
©
§


¸¸
¹
·
¨¨
©
§

 
t

d
 
2
8
8
2
2
3
)
1(
8
10
)
1(
2
10
)
(
8
2
)
(
)
(
p
p
p
p
p
Pow
Y
P
Y
P
p
Pow
iii


6
6
2
2
2
)
1(
)
1(
45
)
(
p
p
p
p
p
Pow




.
The three power curves are shown in Figure 1. 
Of special interest is to compute the power under H0, 
.3,2,1
 ),2
/
1
(
=
=
=
i
p
Powi
i
α
1796
.0
 ,
0480
.0
 ,
0062
.0
3
2
1
=
=
=
α
α
α
. Since the latter value is larger than 0.05 we can’t use the corresponding 
RR. The RR {0, 1, 9, 10} is to be preferred since it has a power that is less than 0.05 under the null hypothesis and it is 
constantly larger than the power in (i).
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
86 
Hypothesis Testing
6.2	
Methods of finding tests
In Ch. 4.3 some methods for finding point estimators were presented. We now consider some methods 
that can guide us when testing hypotheses. These are based on the Chi-square principle, the Likelihood-
ratio (LR) principle and on miscellaneous methods.
6.2.1	
The Chi-square principle
This requires that data are classified. For measurements on a continuous variable we thus have to create 
classes. How this can be done is illustrated in EX 109 below. We thus have the following data
Class
1
2
…
k
Total
Observed frequency
1Y
2
Y
…
k
Y
∑
= n
Yi
Hypothetical probability
1p
2
p
…
k
p
∑
=1
ip
Here the hypothetical probability
ip  is the probability of belonging to class i under H0. Examples of 
such probabilities are given in the examples below. In general the null hypothesis can be formulated 
,
,
(
:
2
1
0
θ
θ
i
i
p
p
H
=
…), where 
,
,
2
1 θ
θ
… are unknown parameters that need to be estimated (by the ML 
method) giving 
,
ˆ
,
ˆ
2
1 θ
θ
…. The Chi-square statistic is
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Exercises in Statistical Inference  
with detailed solutions
87 
Hypothesis Testing


¦
 

 
k
i
i
i
i
H
np
H
np
Y
X
1
0
2
1
2
0
2
1
2
)
,
ˆ
,
ˆ
(
)
,
ˆ
,
ˆ
(


T
T
T
T
 
(25)
As 
)
1
(
 ,
2
2
a
k
X
n
D
−
−
→

∞
→
χ
under H0. a is the number of linearly independent parameters to estimate. 
In practice (25) is used in the following way: Compute the value of
2
X , giving
2
OBS
X
. Then calculate the 
p-value (
)
2
2
)
1
(
OBS
X
a
k
P
>
−
−
χ
and reject H0 if the latter is smaller than 0.05.
EX 70 Given the following data 
Class
1
2
Total
Observed frequency
1Y
2
Y
n
Probability
1-p
p
1
Test 
2
/
1
:
against 
 2
/
1
:
1
0
≠
=
p
H
p
H
.
There are no parameters to estimate under H0.
(
)
(
)
2
2
2
2
1
2
 
gives
 
2
/
1
2
/
1
2
/
1
2
/
1
OBS
X
n
n
Y
n
n
Y
X
⋅
⋅
−
+
⋅
⋅
−
=
. p-value =
)
)
0
1
2
(
(
2
2
OBS
X
P
>
−
−
χ
. Notice that
2
X
can also 
be written 
2
2
4
/
1
2
/
1
/






−
n
n
Y
.
Contingency tables (R × C cross tables). 
The following frequency table, often called a 2 × 2 table, is a convenience way to summarize data. 


)DFWRU,,






7RWDO
)DFWRU,


< 

< 

< 



< 

<



<


7RWDO


< 


<

Q
Here there are two ‘Factors’, each divided into two categories. Examples are when the factors are two 
doctors who classify the same n patients as either ‘Healthy’ (1) or ‘Diseased’ (2), or when the political 
opinion (left- or right wing) of n voters is measured at two times (Factor I and II). The table can be 
generalized to a R × C table with R rows and C columns. It can also be extended to more than two factors, 
e.g. when the political opinion of n voters about P >2 parties is measured at T > 2 times. In such a case 
the sample is called a Panel. Notice that all cell-frequencies are random, except for the fixed sample size n.
Corresponding to the 2 × 2 table of frequencies, there is a 2 × 2 table of probabilities 
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
88 
Hypothesis Testing


)DFWRU,,






7RWDO
)DFWRU,


S 

S


S




S


S



S


7RWDO


S



S


The ‘Total’ probabilities are called marginal proportions (probabilities). Notice that there are three genuine 
(or linearly independent) parameters 
ij
p  and two genuine marginal proportions, since the sum equals 
1. In the R × C table there are 
1
−
⋅C
R
 genuine parameters 
ij
p  and 
1
1
−
+
−
C
R
 genuine marginal 
proportions. 
The frequencies in a 2 x 2 table are distributed Multinomid 
)
,
,
,
,
(
22
21
12
11
p
p
p
p
n
(Cf. Ch. 2.2.1 (6)),  so 
the probability of the outcomes, or Likelihood if we are interested in the parameters, can be written 
22
21
12
11
22
21
12
11
.)
(
y
y
y
y
p
p
p
p
const
L  
.  We will consider two types of hypotheses:
Equality of marginal proportions. 
1
1
0 :
+
+ = p
p
H
. This is easily seen to be identical with 
12
21
0 :
p
p
H
 
(= p).  
(= p). Under H0 there are 2 genuine parameters to estimate.
Independency between Factor I and Factor II. In this case 
j
i
ij
p
p
p
H

 
 
:
0
. Under H0 there are 2 
genuine parameters to estimate.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
89 
Hypothesis Testing
EX 71 Test of equal marginal proportions in the 2x2 table by means of McNemar’s test. 
As just noticed we shall test t
)
(
:
:
12
21
0
1
1
0
p
p
p
H
p
p
H
 
 

 


.  Let the Likelihood under H0 be denoted
0
L . Then
22
12
21
11
)
2
1(
.)
(
11
11
0
y
y
y
y
p
p
p
p
const
L


 

. ( Notice that we only keep the genuine parameters.)
)
2
1
ln(
ln
)
(
ln
.)
ln(
ln
11
22
12
21
11
11
0
p
p
y
p
y
y
p
y
const
L






 
¸¸
¹
·
¨¨
©
§




 
)
2
1(
1
1
ln
11
22
11
11
11
0
p
p
y
p
y
dp
L
d
= 0, 
0
)
2
1(
2
1
)
(
ln
11
22
12
21
0
 
¸¸
¹
·
¨¨
©
§





 
p
p
y
p
y
y
dp
L
d
We thus have the equations
q
¯
®
­


 



 
(2)
)
2
1
/(
2
/)
(
(1)
)
2
1
/(
/
11
22
12
21
11
22
11
11
p
p
y
p
y
y
p
p
y
p
y
Since these may be somewhat tricky to solve we show an example of a solution.
Multiplying Eq. (1) by 2 and taking the difference between the left and right-hand sides in the two equations yields 
0
/
2
/)
(
11
11
12
21
 


p
y
p
y
y

)
/(
2
12
21
11
11
y
y
py
p

 
 which inserted into Eq. (1) gives 
p
y
y
py
y
p
y
y
2
)
/(
2
1
2
12
21
11
22
12
21



 

.
So, 
n
y
y
p
n
p
y
y
y
y
p
y
y
2
)
(
ˆ
)
(
2
)
(
12
21
22
12
21
11
12
21

 


 



 

 and this inserted into Eq. (1) gives 
n
y
p
/
ˆ
11
11  
 and finally 
n
y
p
p
p
/
ˆ
2
ˆ
1
ˆ
22
11
22
 


 
.
According to the Chi-square principle.


¦



 
)
ˆ
(
)
ˆ
(
0
2
0
2
H
p
n
H
p
n
Y
X
ij
ij
ij
.  Here




2
)
(
2
)
(
ˆ
,0
ˆ
21
12
12
21
12
0
12
12
11
11
0
11
11
Y
Y
n
Y
Y
n
Y
H
p
n
Y
n
Y
n
Y
H
p
n
Y

 



 


 


 


,




0
ˆ
,
2
)
(
2
)
(
ˆ
22
22
0
22
22
12
21
12
21
21
0
21
21
 


 



 



 


n
Y
n
Y
H
p
n
Y
Y
Y
n
Y
Y
n
Y
H
p
n
Y
.
Thus 








21
12
2
21
12
12
21
2
12
21
12
21
2
21
12
2
)
(!
0
2
/)
(
2
/)
(
2
/)
(
2
/)
(
0
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
X


 
 







 
.
This test statistic was derived by McNemar (McNemar 1947, p. 153) and has been termed McNemar’s Test.
Under H0 the statistic is distributed
)1(
)
2
1
4
(
2
2
χ
χ
=
−
−
in large samples with p-value = (
)
2
2
)1(
OBS
X
P
>
χ
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
90 
Hypothesis Testing
EX 72 Test of independency
In the 2 x 2 table the hypothesis of independency is 
2,1
,
for 
,
:
0
 

 


j
i
p
p
p
H
j
i
ij
.  Under H0 the likelihood 
is 







 














\
\
\
\
S
S
S
S
S
S
S
S
FRQVW
/












 
. Since we only want genuine parameters, put
1
2
1
2
1
 
and
 
1
+
+
+
+
−
=
−
=
p
p
p
p
. Taking logarithms gives 














 






1
1
21
1
1
12
1
1
11
0
ln
)
1
ln(
)
1
ln(
ln
ln
ln
.)
ln(
ln
p
p
y
p
p
y
p
p
y
const
L

)
1
ln(
)
1
ln(
1
1
22





p
p
y
.
0
)
1(
)
1(
)
(
)
1(
)
1(
ln
1
2
1
1
1
22
21
1
12
11
1
22
1
21
1
12
1
11
1
0
 


 




 





 











p
y
p
y
p
y
y
p
y
y
p
y
p
y
p
y
p
y
dp
L
d
0
)
1(
)
1(
)
(
)
1(
)
1(
ln
1
2
1
1
1
22
12
1
21
11
1
22
1
21
1
12
1
11
1
0
 


 




 





 











p
y
p
y
p
y
y
p
y
y
p
y
p
y
p
y
p
y
dp
L
d
From this we get
n
y
y
y
y
p
p
y
p
y
y
+
+
+
+
+
+
+
+
+
+
=
+
=
⇒
=
−
1
2
1
1
1
1
2
1
1
1
)
(
 and similarly 
n
y
p
+
+ =
2
2ˆ
. Also notice that 
n
y
n
y
n
n
y
p
p
+
+
+
+
+
=
−
=
−
=
−
=
2
1
1
1
1
1
ˆ
1
ˆ
 and similarly 
n
y
p
2
2
ˆ
+
+ =
In the Chi-square statistic (25) 




n
Y
Y
p
p
n
H
p
n
j
i
j
i
ij





 

 

ˆ
ˆ
ˆ
0
.  Thus we obtain the statistic


¦
 







 
2
1
,
2
2
/
/
j
i
j
i
j
i
ij
n
Y
Y
n
Y
Y
Y
X
.
 In large samples this is distributed 
)1(
)
2
1
4
(
2
2
χ
χ
=
−
−
and the p-value is (
)
2
2
)1(
OBS
X
P
>
χ
.
In the table with R rows and C columns the Chi-square statistic remains the same, but now the degrees of 
freedom is changed to 
)1
)(
1
(
)1
(
)1
(
1


 






C
R
C
R
C
R
.  The p-value is now obtained as 


2
2
))
1
)(
1
((
OBS
X
C
R
P
!


F
.
When the hypothesis of independence between the two factors is rejected, one should go further 
in the analysis and determine which combination of levels from the factors that contributes to the 
dependency. This can be done by considering 
n
Y
Y
Y
D
j
i
ij
ij
/

 

 
.  The latter is called Deviation and 
is supplied by many statistical soft-wares. If 
ij
D  is greater than zero or below zero there is an over-or 
underrepresentation, respectively, of observations in cell 
)
,
(
j
i
. Since deviations may be due merely to 
chance, one should study whether the deviation is significantly different from zero (a ‘significant deviation’). 
A statistic for this purpose is the Cell Chi-square defined by


n
Y
Y
n
Y
Y
Y
X
j
j
i
ij
ij
/
/
1
2
2







 
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
91 
Hypothesis Testing
In large samples this is distributed as a 
)1(
2
χ
variable (Cochran 1954, p. 417). This means that if the Cell 
Chi-square is larger than 3.85, then the deviation is significant at the 5% level. The single cell statistic is 
supplied by statistical soft-ware, e.g. in SAS where it is denoted ‘Cell Chi-Square’. 
When analyzing deviations in many cells one should be aware of the risk of making wrong decisions 
due to the multiple inference context. When several conclusions are to be drawn simultaneously with 
5% significance one has to adjust the individual significance level so that the global level is maintained 
at 5%. This is explained further in Ch. 6.4.
We now turn to another application of the chi-square principle, the test of fit. In this case the null 
hypothesis specifies that data have a certain distribution. In (25) 
iY  are the observed frequencies which 
are to be compared with the hypothetical ones under H0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
▶▶enroll by September 30th, 2014 and 
▶▶save up to 16% on the tuition!
▶▶pay in 10 installments / 2 years
▶▶Interactive Online education
▶▶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Exercises in Statistical Inference  
with detailed solutions
92 
Hypothesis Testing
EX 73 A test of randomness for binary data.
A sequence of digital numbers starts with 0,0,0,0,1,1,… and ends with …,0,0,1,0,0,1. We want to study whether these 
occur in a random order. There are several ways to do this, but one is the following: Define the variable Y = ‘Number 
of digits until the first ‘1’ occurs. The observations on Y are 
Y
1
2
3
4
5
6
10
Frequency
26
13
9
2
1
1
1
From Ch. 2.2.1 (3) it follows that in this case H0: ‘Digits are in random order’ is the same as 
)
(
~
:
0
p
Geometric
Y
H
, 
where p is the probability of ‘1’.
In EX 47 it is seen that the ML estimate of p is
y
p
/
1
ˆ =
. From the table above we get 
49
.0
108
53
ˆ
53
108
1
13
26
10
1
2
13
1
26
 
 

 









 
 ¦
p
n
y
y
i


.
The estimated expected frequency under H0 of the outcome 
'
'
y
Y =
is 
,...
2,1
 ,ˆ
)ˆ
1(
=
−
⋅
y
p
p
n
y
E.g. the expected 
frequency of the outcome 
2.
13
49
.0
)
49
.0
1(
53
is
'2
'
1
2
 


 

Y
.  One obtains the following table
y
1
2
3
4
5
Expected frequency
26.0
13.2
6.8
3.4
1.8
Here the expected frequencies of the outcomes Y = 5 or larger are small so we throw them together in the following 
way: 53-(26.0+13.2+6.8+3.4) = 3.6. We now get the table
y
1
2
3
4
5-
Expected frequency
26.0
13.2
6.8
3.4
3.6
Observed frequency
26
13
9
2
3


10
.0
39
.1
)1
1
5
(
value
-
p
39
.1
6.3
)
6.3
3
(
0.
26
)
0.
26
26
(
2
2
2
2
!!
!


 

 




 
F
P
X OBS

.
There is thus no reason to reject the hypothesis of randomness.
Comment to EX 73 It has been recommended that expected frequencies under H0 shall be larger than 
2 in the Chi-square test of fit (Stuart et al 1999, p. 409), earlier recommendations were larger than 5. 
In EX 73 all expected frequencies for y larger than 4 are definitely too small. At y = 3 there is an over-
representation of observed frequencies with Deviation = 2.2, but this isn’t serious since the cell-Chi-
square statistic is only 0.71.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
93 
Hypothesis Testing
EX 74 Challenge the computer in ‘thinking randomly’.
The original series in EX 73 was actually made by a random number generator. (The function ranbin(0,1,p) with 
p = 1/2 in SAS.)
It is a challenge to try to beat the computer in ‘thinking randomly’, as measured by the value of
2
OBS
X
. 
Write down a sequence of slightly more than one hundred 0’s and 1’s. Try to place them in ‘random’ order, and repeat 
the analysis made in EX 73. You will probably not beat the computer and it is even likely that the sequence you have 
created will be rejected as random.
A tip! By the time you will learn from the table of observed and expected frequencies how to improve your skill. Then 
it is time to challenge your friends in tournaments. 
EX 75 The following table shows the treatment times (minutes) for patients at a clinic.
Treatment time
0-10
10-20
20-30
30-40
40-
Total
 Frequency
10
16
13
6
5
50
Mean = 20, Variance = 140, Max.value = 45
Test whether the treatment times have a 
)
(b
Uniform
- distribution.
The cdf is 
b
y
b
y
y
F
≤
≤
=
0
  ,
/
)
(
and a ML estimate of b is 
9.
45
45
50
51
)1
(
ˆ
)
(
 

 

 
n
y
n
n
b
.
Thus, the estimated cdf is 
9.
45
/
)
(
ˆ
y
y
F
 
.  The expected frequencies are.




9.
10
)
10
(
ˆ
)
20
(
ˆ
50
)
20
10
(ˆ
50
,9.
10
)
0
(
ˆ
)
10
(
ˆ
50
)
10
0
(ˆ
50
 

 
d
d

 

 
d
d

F
F
Y
P
F
F
Y
P




9.
10
)
30
(
ˆ
)
40
(
ˆ
50
)
40
30
(ˆ
50
,9.
10
)
20
(
ˆ
)
30
(
ˆ
50
)
30
20
(ˆ
50
 

 
d
d

 

 
d
d

F
F
Y
P
F
F
Y
P


4.6
)
40
(
ˆ
1
50
)
40
(ˆ
50
 

 
t

F
Y
P
. Thus,
Treatment time
0–10
10-20
20–30
30–40
40–
Expected frequency
10.9
10.9
10.9
10.9
6.4
Observed frequency
10
16
13
6
5
,
37
.5
4.6
)
4.6
5
(
9.
10
)
9.
10
10
(
2
2
2
 




 

OBS
X
 p-value = = 
 
!


37
.5
)1
1
5
(
2
F
P
0.15.
This p-value isn’t small enough to reject the hypothesis of a 
)
(b
Uniform
-distribution. However, the sample size 
is small and there are some suspicious signs of a positive deviation for the cell 10–20 and a negative deviation 
for the cell 30–40 (although neither being significant). There seems to be reasons to search for a more realistic 
probability model.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
94 
Hypothesis Testing
EX 76 Check if a 
)
,
(
k
Gamma λ
- distribution gives a better fit to the data in EX 75.
ML estimates of this distribution are quite laborious to obtain (Cf. EX 44), therefore we confine ourselves with 
Moment estimates (Cf. EX 38) 
3
9.2
/
ˆ 
and
 7
/
1
/
ˆ
2
2
2
≈
=
=
=
=
s
y
k
s
y
λ
.
These estimates inserted into the cdf (Cf. Ch.2.2.2 (2)) 
!
/
)
(
1
)
(
1
0
i
y
e
y
F
k
i
i
y∑
−
=
−
−
=
λ
λ
 gives
(
)
2
/
)
7
/
(
7
/
1
1
)
(
ˆ
2
7
/
x
x
e
y
F
y
+
+
−
=
−
 From this the expected frequencies are






9.
12
30
20
50
,6.
18
20
10
50
,7.8
10
0
50
 



 



 



Y
P
Y
P
Y
P




8.3
40
50
,2.6
40
30
50
 
!

 



Y
P
Y
P
. Thus,
Treatment time
0–10
10–20
20–30
30–40
40–
Expected frequency
8.7
18.6
12.9
6.2
3.8
Observed frequency
10
16
13
6 
5
90
.0
8.3
)
8.3
5
(
7.8
)
7.8
10
(
2
2
2
 




 

OBS
X
,  p-value 

 
!


 
90
.0
)
2
1
5
(
2
F
P
0.64.
The gamma distribution seems to give a much better fit to data than the uniform distribution.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Exercises in Statistical Inference  
with detailed solutions
95 
Hypothesis Testing
EX 77 The production of goods by a particular method has since a long time resulted in 25% god, 62% medium and 
13% bad products. In a test with a new method 50 products were produced. Of these 20 were god, 19 were medium 
and 11 were bad.
Does the new method give products of a different quality or are the observed differences merely due to chance?
H0: The new method gives products of the same quality as the older method.
Quality
God
Medium
Bad
Expected frequency
5.
12
50
25
.0
 

0.
31
50
62
.0
 

5.6
50
13
.0
 

Observed frequency
20
19
11
 
3.
12
5.6
)
5.6
11
(
0.
31
)
0.
31
19
(
5.
12
)
5.
12
20
(
2
2
2
2
 





 
OBS
X
,  p-value= = 

002
.0
3.
12
)
0
1
3
(
2
 
!


F
P
. 
(No parameters have been estimated.) There is thus a strong reason to reject H0. 
Let’s look more closely at the differences.
Quality
God
Medium
Bad
Deviation
+7.5
-12.0
+4.5
Cell-Chi-square
4.50 p<5%
4.65 p<5%
3.12 (NS)
The total Chi-square of 12.3 above shows that there is a significant difference. The table of deviations explains in a 
way the nature of the difference. The new method involves an over-representation of god products and an under-
representation of medium products (NS is an often used abbreviation for ‘Not Significant’.)
6.2.2	
The Likelihood Ratio principle
To test the hypothesis
,
,
(:
2
1
0
θ
θ
H
 …)
,
,
(
)
0
(
2
)
0
(
1
θ
θ
=
…) against 
,
,
(:
2
1 θ
θ
a
H
…)
,
,
(
)
0
(
2
)
0
(
1
θ
θ
≠
…) we 
consider the Likelihood Ratio (LR) statistic
L
L
ˆ
/
ˆ
0
=
Λ
. Here 
0ˆL is the likelihood under H0 with ML 
estimators inserted for the parameters. Lˆ  is the correspond likelihood under both H0 and H1, i.e. 
without any restrictions on the parameters. An obvious rejection region (RR) is
c
<
Λ
, or 
'
ln
c
!
/

. 
(This follows because
1
0
<
Λ
<
.) The LR test is performed in the following way:
1.	 Compute the ML estimates of the parameters under H0 and under H1
 (26)
2.	 Compute the value of the LR statistic, say
OBS
Λ
.
3.	 Compute the p-value for H0 from the p-value = 

OBS
s
r
P
/

!

ln
2
)
(
2
F
, where r = Number of parameters 
estimated without restrictions on the parameters and s = Number of parameters estimated under H0.
Notice that this test is a large-sample test. (Strictly speaking the test should be termed estimated LR test 
(ELR), since estimates are plugged in for the parameters.)
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
96 
Hypothesis Testing
EX 78
a)	  Make a LR test based on the fictive data in EX 70 to test H0: p =1/2 against Ha:
2
/
1
≠
p
.
b)	 Compare the LR and the Chi-square tests when 
60
and
100
2  
 
Y
n
.
a)	 The likelihood is






=
−
⋅
=
−
y
n
c
p
p
c
L
y
n
y
 
 where
,
)
1(
. Under H0 we get
y
n
y
c
L
−
⋅
=
)
2
/
1(
)
2
/
1(
0
. (There are no parameters to estimate.) The unrestricted ML estimate of p is 
n
y
p
/
ˆ  
 



 
 
/



 




y
n
y
y
n
y
y
n
y
n
y
n
y
c
c
L
L
n
y
n
y
c
L
)
/
1(
)
/
(
)
2
/
1(
)
2
/
1(
ˆ
)
/
1(
)
/
(
ˆ
0


>
@


)
/
1(
2
ln
)
(
)
/
2
ln(
2
ln
2
)
/
1(
2
)
/
2
(
1
n
y
y
n
n
y
y
n
y
n
y
y
n
y



 
/




.
p-value 


OBS
P
/

!

 
ln
2
)
0
1(
2
F
.
b)	 Chi-square test


0455
.0
00
.4
)1(
value
-
p
00
.4
2
/
1
100
)
2
/
1
100
40
(
2
/
1
100
)
2
/
1
100
60
(
2
2
2
2
 
!
 

 







 
F
P
X
.
        LR test
>
@


 

 




 
/

value
-
p
03
.4
)
100
/
60
1(
2
ln
)
60
100
(
)
100
/
60
2
ln(
60
2
ln
2


0447
.0
03
.4
)1(
2
 
!
F
P
.
The two p-values are roughly the same. In practice it will suffice to just notice that the p-values are below 5%, so H0 is 
rejected at the 5% level.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Exercises in Statistical Inference  
with detailed solutions
97 
Hypothesis Testing
EX 79 Test of equality between two proportions in independent Binomial samples.
Often one is interested in comparing two proportions, e.g. the proportion of smokers among men and women. In 
such a case one takes a sample of men and a sample of women that is independent of the first sample. (A sample of 
couples is thus not appropriate.) Data can be summarized in the following way:
Sample 1
(Men)
Sample 2
(Women)
Frequency
Probability
Frequency
Probability
Total
Smokers
1Y
1p
2
Y
2
p
2
1
Y
Y +
Non-smokers
1
1
Y
n −
1
1
p
−
2
2
Y
n −
2
1
p
−
)
(
2
1
2
1
Y
Y
n
n
+
−
+
Total
1n
1
2
n
1
2
1
n
n +
We want to test H0:
)
(
2
1
p
p
p
=
=
 against Ha:
2
1
p
p ≠
The unrestricted likelihood is 
2
2
2
1
1
1
)
1(
)
1(
2
2
2
2
1
1
1
1
y
n
y
y
n
y
p
p
y
n
p
p
y
n
L
−
−
−






−






=
with two parameters to estimate.
The likelihood under H0 is 
)
(
2
2
1
1
0
2
1
2
1
2
1
)
1(
y
y
n
n
y
y
p
p
y
n
y
n
L
+
−
+
+
−












=
with one parameter to estimate.









 
)
1
ln(
)
(
ln
)
1
ln(
)
(
ln
.
ln
2
2
2
2
2
1
1
1
1
1
p
y
n
p
y
p
y
n
p
y
const
L
2
2
2
2
2
2
2
2
2
1
1
1
1
1
1
1
1
1
ˆ
0
)
1(
)
(
ln
,
ˆ
0
)
1(
)
(
ln
n
y
p
p
y
n
p
y
dp
L
d
n
y
p
p
y
n
p
y
dp
L
d
 

 



 
 

 



 








 
)
1
ln(
))
(
(
ln
)
(
.
ln
2
1
2
1
2
1
0
p
y
y
n
n
p
y
y
const
L




2
1
2
1
2
1
2
1
2
1
0
ˆ
0
)
1(
)
(
ln
n
n
y
y
p
p
y
y
n
n
p
y
y
dp
L
d


 

 






 
2
2
2
1
1
1
2
1
2
1
2
1
2
2
2
2
1
1
1
1
)
(
2
1
2
1
2
1
2
1
1
1
)
(
)
(
1
y
n
y
y
n
y
y
y
n
n
y
y
n
y
n
y
n
y
n
y
n
n
y
y
n
n
y
y
−
−
+
−
+
+





−











−












+
+
−






+
+
=
Λ
The p-value is finally obtained from 

OBS
P
/

!

ln
2
)1
2
(
2
F
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
98 
Hypothesis Testing
EX 80 LR test of equality between marginal proportions.
a)	 Show how the LR test can be used to test H0:
)
(
12
21
p
p
p
 
 
:in the 2 x 2 table.
b)	 Perform the test in the following case where two methods A and B are used to classify the same products into 
two categories 1 and 2.
Method B
1
2
Method A
1
32
19
2
21
18
c)	 Test the same hypothesis by using a Chi-square test.
a)	 From EX 71 the estimated likelihood under H0 is 
22
12
21
11
22
11
0
ˆ
ˆ
ˆ
y
y
y
y
p
p
p
c
L


 
,  where the 
estimates are 
n
y
p
n
y
y
p
n
y
p
22
22
12
21
11
11
ˆ
,
2
ˆ
,
ˆ
 

 
 
.  The unrestricted estimated likelihood 
is 
22
12
21
11
22
12
21
11
ˆ
ˆ
ˆ
ˆ
y
y
y
y
p
p
p
p
c
L

 
,  where the estimates are 
.2,1
,
,
ˆ
 
 
j
i
n
y
p
ij
ij
 The LR 
ratio is 
^
`
12
12
21
21
12
21
12
21
ˆ
ln
ˆ
ln
ˆ
ln
)
(
2
ln
2
ˆ
ˆ
ˆ
12
21
12
21
p
y
p
y
p
y
y
p
p
p
y
y
y
y




 
/


 
/

. p-value= 


OBS
P
/

!

ln
2
)
2
3
(
2
F
b)	


0462
.0
9729
.3
)1(
value
-
p
9729
.3
ln
2
2
 
!
 

 
/

F
P
OBS
. H0 is thus rejected at the 5% level. This 
conclusion can of course also be reached by noticing that the RR consists of values larger than 
2)
96
.1(
8416
.3
 
.
c)	 The Chi-square test is based on the statistic 
12
21
2
12
21
2
)
(
y
y
y
y
X


 
.  In this case 
92
.3
2
 
OBs
X
 giving the p-value


0472
.0
92
.3
)1(
2
 
!
F
P
,  very close to that obtained by the LR test.
EX 81 LR test of independency.
a)	 Show how the LR test can be used to test H0:
j
i
ij
p
p
p


 
  (independence) in the 2 x 2 table.
b)	 Apply the test to the following data illustrating the relation between left/right-handedness and type of twin 
(identical = 1, fraternal = 2).
Type of twin
1
2
Right-handed
207
228
Left-handed
41
18
c)	 Test the same hypothesis with the Chi-square test.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
99 
Hypothesis Testing
a)	 Under H0 the likelihood is 
(
)
(
)
(
)
(
) 22
21
12
11
2
2
1
2
2
1
1
1
0
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
y
y
y
y
p
p
p
p
p
p
p
p
c
L
+
+
+
+
+
+
+
+
⋅
⋅
⋅
⋅
⋅
=
,
where 
1,2
i ,
ˆ
2
1
=
+
=
+
n
y
y
p
i
i
i
and
2,1
  ,
ˆ
2
1
=
+
=
+
j
n
y
y
p
j
j
j
.
The unrestricted likelihood is 
22
21
12
11
22
21
12
11
ˆ
ˆ
ˆ
ˆ
y
y
y
y
p
p
p
p
c
L

 
,  where 
n
y
p
ij
ij  
ˆ
.
The LR statistic is 
L
L0
=
Λ
and the p-value is 

OBS
P
/

!

ln
2
)
2
3
(
2
F
. In this case the likelihood ratio can’t be 
simplified as in EX 78.
b)	 After laborious computations we obtain 
21
.
10
ln
2
 
/

OBS
 and p-value is 

0014
.0
21
.
10
)1(
2
 
!
F
P
.
3.	
97
.9
2
 
OBS
X
 and p-value is 


0016
.0
97
.9
)1(
2
 
!
F
P
.  The following table is obtained for deviations: 
Deviation/Cell Chi-square
Type of twin
1
2
Right-handed
-11.4/0.59
11.4/0.60
Left-handed
11.4/4.37
-11.4/4.41
From the table it is concluded that the rejection of the null hypothesis is mainly due to a significant over-
representation of identical twins (1) that are left-handed.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master’s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master’s programmes
• 33rd place Financial Times worldwide ranking: MSc 
International Business
• 1st place: MSc International Business
• 1st place: MSc Financial Economics
• 2nd place: MSc Management of Learning
• 2nd place: MSc Economics
• 2nd place: MSc Econometrics and Operations Research
• 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‘Beste Studies’ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Exercises in Statistical Inference  
with detailed solutions
100 
Hypothesis Testing
Comment to EX 81 The LR test for independence is laborious compared with the Chi-square test. The 
latter is also more informative since the source of the total Chi-square can be explained in terms of 
separate deviations. Today most statistical software present results for both tests, so ease of calculation 
is not a problem. It may be tempting to report the p-value that is lowest and this seems often to be the 
one obtained from the LR test, but in that case you lose the informative aspect mentioned above.
The p-values in EX 81 have been reported with too many decimals just to illustrate differences. 
EX 82 In EX 62 a CI for the ratio of two Poisson rates 
Y
 
and
 
λ
λX
was used to claim a significant difference between 
the rates. We now show how the same problem can be solved by LR testing.
Consider the hypothesis H0:
)
( λ
λ
λ
=
=
Y
X
against Ha:
Y
X
λ
λ
≠
.
Since the two samples are independent (Cf. EX 62) the unrestricted likelihood is
t
t
y
Y
s
s
x
X
Y
X
e
t
y
t
e
s
x
s
L
⋅
−
⋅
−
⋅
⋅
⋅
=
λ
λ
λ
λ
)!
(
)
(
)!
(
)
(
)
(
)
(
and the likelihood under H0 is 
)!
(
)!
(
)
(
)
(
)
(
)
(
)
(
0
t
y
s
x
e
t
s
L
t
s
t
y
s
x
t
y
s
x



 
O
O
.  From this the following estimates are easily obtained:
t
s
t
y
s
x
t
t
y
s
s
x
Y
X
+
+
=
=
=
)
(
)
(
ˆ
  ,)
(
ˆ
  ,)
(
ˆ
λ
λ
λ
.
The estimated LR reduces to 
)
(
)
(
)
(
)
(
0
ˆ
ˆ
ˆ
ˆ
ˆ
t
y
Y
s
x
X
t
y
s
x
L
L
λ
λ
λ
⋅
=
=
Λ
+
, since many factors cancel each other. The p-value is
(
)
OBS
P
Λ
−
>
−
ln
2
)1
2
(
2
´
χ
.
In EX 62, 
25
.
16
4
65
ˆ
,
63
.
10
8
85
ˆ
,5.
12
4
8
65
85
ˆ
65
)
(
85,
(8)
,4
,8
 
 
 
 
 


 

 
 
 
 
Y
X
t
y
x
t
s
O
O
O
.
This gives 
0109
.0
value
-
p
4791
.6
ln
2
 

 
/

.  The null hypothesis is rejected.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
101 
Hypothesis Testing
Ex 83( )n
i
iY
1
= are iid with 
)
,
(
~
2
σ
β
α
i
i
x
N
Y
+
. Show how we can test H0:
0
=
β
against Ha:
0
≠
β
.
(This model is the same as in EX 54 with the exception that there is a further parameter 
0
≠
α
.)
The unrestricted likelihood is 





 


 
¦
¦



2
2
2
)
(
2
1
2
/
2
2
)
(
ln
2
ln
)
2
(
1
2
2
V
E
D
V
V
S
E
D
V
i
i
x
y
n
x
y
n
c
L
e
L
i
i
x
y
x
n
y
x
y
d
L
d
i
i
i
i


 


 

 





 
¦
¦
¦
E
D
E
D
V
E
D
D
ˆ
ˆ
,
ˆ
ˆ
0
2
)
(
2
)1
(
ln
2

(i)
 

 

 





 
¦
¦
¦
¦
¦
n
y
x
x
x
y
x
x
y
x
d
L
d
i
i
i
i
i
i
i
i
i
,
ˆ
ˆ
0
2
)
(
2
)1
(
ln
2
2
E
D
V
E
D
E
n
x
x
i
∑
+
⋅
2
ˆ
ˆ
β
α

(ii)
n
x
y
x
y
n
d
L
d
i
i
i
2
2
2
2
2
2
2
)
ˆ
ˆ
(
ˆ
0
)
(
1
0
)
2
1
(
)
(
2
ln



 

 
¸¸
¹
·
¨¨
©
§






 
¦
¦
E
D
V
V
E
D
V
V

(iii)
 (i), (ii) gives 





°°
¯
°°
®
­


 
 


 
¦
¦
¦
¦
¦
x
y
S
S
n
x
x
n
y
x
y
x
XX
XY
i
i
i
i
i
i
E
D
E
ˆ
ˆ
/
/
ˆ
2
2
.  
(iv)
The likelihood under H0 is 


2
2
2
0
)
(
2
1
2
/
2
0
2
)
(
ln
2
ln
2
1
2
2
V
D
V
V
S
D
V
¦



 


 
¦


i
y
n
y
n
L
e
L
i
y
y
d
L
d
i
 

 




 

¦
D
V
D
D
ˆ
0
2
)
(
2
)1
(
ln
2
0
. 
(v)
n
y
y
n
y
y
n
d
L
d
i
i
i
¦
¦
¦

 
 

 

 
¸¸
¹
·
¨¨
©
§




 
2
2
2
2
2
2
2
2
0
)
(
(v)
From
(
)ˆ
(
ˆ
0
)
(
1
0
)
(
ln
D
V
V
D
V
V
The latter estimate, obtained under H0 is denoted
2
0ˆσ
.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
102 
Hypothesis Testing
EX 83 (Continued) Thus, 
2
/
2
0
2
0
ˆ
ˆ
n
L
L






=
=
Λ
σ
σ
(Several factors cancel each other.)
2
2
0
ˆ
ˆ
ln
ln
2
V
V
n
 
/


.  The p-value is 

OBS
P
/

!

ln
2
)1
2
(
2
F
.
The factor 
XY
S
 in (iv) can be expressed in several ways, e.g.
¦


 
)
)(
(
y
y
x
x
S
i
i
XY
. Similarly 
¦

 
2)
(
x
x
S
i
XX
.
The Λ -test statistic can be expressed more simply.
(
)
(
)
∑
∑
−
−
−
=
−
−
2
2
ˆ
)
ˆ
(
ˆ
ˆ
i
i
i
i
x
x
y
y
x
y
β
β
β
α
=


¦
¦
¦
¦
 






 



)
)(
(
ˆ
2
)
(
ˆ
)
(
)
(ˆ
)
(
2
2
2
2
y
y
x
x
x
x
y
y
x
x
y
y
i
i
i
i
i
i
E
E
E
>
@
XX
YY
XX
XY
XY
XX
YY
S
S
S
S
S
S
S
2
2
2
ˆ
ˆ
ˆ
 that 
Notice
ˆ
2
ˆ
E
E
E
E
E

 
 
 


.
Thus, 
2
2
2
2
0
2
1
1
ˆ
ˆ
ˆ
r
S
S
S
S
S
S
YY
XX
XY
YY
XX
YY

 

 

 
E
V
V
,  where r is the sample correlation coefficient.  
It follows that 
)
1
ln(
ln
2
2
r
n


 
/

.
If 
)
,
(
Y
X
has a bivariate normal distribution it can be shown that the conditional expectation is
(
)
x
x
X
Y
E
β
α +
=
=
, where 
X
Y σ
σ
ρ
β
/
⋅
=
and ρ is the population correlation coefficient. The hypothesis that 
0
=
β
is thus equivalent with the hypothesis that 
0
=
ρ
and can be tested in the same way. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Exercises in Statistical Inference  
with detailed solutions
103 
Hypothesis Testing
6.2.3	
Miscellaneous methods
In this section we consider tests that are of a ‘common sense-nature’. This means that the test statistics 
are sensitive to changes in the parameters and are used together with a proper RR. For example, when 
there is a sample of observations on a variable 
)
,
(
~
2
σ
µ
N
Y
and we want to test H0:
0
µ
µ =
against Ha:
0
µ
µ ≠
, it is obvious that we shall use a test statistic based on 
0
µ
−
Y
 and reject H0 for large values 
of the latter quantity. There may be situations where it is less obvious how to perform a test. Then one 
may use the Neyman-Pearson Lemma which states that, when testing H0:
0
θ
θ =
against Ha:
a
θ
θ ≠
, the 
test with maximal power is obtained from the LR 
c
L
L
a <
/
0
(cf. Ch. 20.10-20.13 in Stuart et al 1999.) 
We will seldom need this Lemma since in the following applications the best RR agrees with the one 
obtained by common-sense reasoning.
EX 84 Consider again the situation in EX 70 and EX 78 where we test H0:  p = 1/2  against Ha: p ≠ 1/2.
Put 
n
Y
p =
ˆ
, with 
n
p
p
p
V
p
p
E
)
1(
)ˆ
(
 
and
 
)ˆ
(
−
=
=
. Intuitively it seems reasonable to choose the test statistic 
(
)
(
)
n
p
H
p
V
H
p
E
p
T
4
/
1
2
/
1
ˆ
ˆ
ˆ
ˆ
0
0
−
=
−
=
. H0 is rejected for large values of T or equivalently, for large values of 
2
2
4
/
1
2
/
1
ˆ






−
=
n
p
T
. However, this is exactly the same test that was obtained by the Chi-square principle.
EX 85 Consider the situation in EX 79 where data were obtained from two independent Binomial samples with 
proportions 
2
1
 
and
 
p
p
and one wanted to test H0: 
)
(
2
1
p
p
p
=
=
against Ha: 
2
1
p
p ≠
.
a)	 Construct a test of ‘common sense-nature’.
b)	 In order to test a new vaccine 90 pupils from a school were vaccinated and 66 were not vaccinated. After six 
months it was noticed how many pupils who had got a flue, with the following result:
Vaccinated (1)
Not vaccinated (2)
With flu
4
18
Without flu
86
48
90
66
Test whether the vaccine has a significant preventive effect by using the test in a). Compare the result with that which 
is obtained by using the LR test in EX 79.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
104 
Hypothesis Testing
EX 85 (Continued) 
a)	 Test statistic: 
(
)
(
)
0
2
1
0
2
1
2
1
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
H
p
p
V
H
p
p
E
p
p
T
−
−
−
−
=
. Here (
)
0
ˆ
ˆ
0
2
1
=
−
=
−
p
p
H
p
p
E
,
(
)






+
−
=
−
+
−
=
−
2
1
2
1
0
2
1
1
1
)
1(
)
1(
)
1(
ˆ
ˆ
n
n
p
p
n
p
p
n
p
p
H
p
p
V
. p is unknown and has to be estimated. In EX 46 it 
was seen that if ( )n
n
i
iY
= are independent and 
)
,
(
~
p
n
Binomial
Y
i
i
then 
∑
∑
∑
∑
=






=
=
=
i
i
i
i
i
i
i
i
n
Y
n
Y
p
n
p
n
p
ˆ
ˆ
ˆ
is an ML estimator of p and also BLUE. In this case
2
1
2
1
ˆ
n
n
Y
Y
p
+
+
=
. Thus, the test statistic to be used is
(
)
2
1
2
1
/
1
/
1
)ˆ
1(ˆ
ˆ
ˆ
'
n
n
p
p
p
p
T
+
−
−
=
. What about the distribution of 
'
T ?
According to the CLT 
f
o
o

2
1,
as
)1,0
(
~
n
n
N
Z
T
D
.  By similar arguments as in EX 23 c) it then follows that 
also T’ has a limiting standard normal distribution. (Recall that 
)
,
(
~
2
1
2
1
p
n
n
Binomial
Y
Y
+
+
.) The latter 
convergence is however slower.
H0 is rejected for larges values of 
'
T , so p-value is 
(
)
OBS
T
Z
P
'
2
>
⋅
.
b)	 From the table we get 
1410
.0
66
90
18
4
ˆ
,
2727
.0
66
18
ˆ
,
0444
.0
90
4
ˆ
2
1
 


 
 
 
 
 
p
p
p
,  from which 


00006
.0
04
.4
2
value
-
p
04
.4
)
66
/
1
90
/
1(
859
.0
141
.0
2727
.0
0444
.0
'
 
!

 


 



 
Z
P
T
The LR test in EX 77 gives 

 


 


 


 
/





 
/



8547
.
16
ln
2
66
/
18
1
66
/
18
90
/
4
1
90
/
4
156
/
22
1
156
/
22
18
66
18
4
90
4
22
156
22
p-value 


00004
.0
8547
.
16
)1
2
(
2
 
!

 
F
P
[Don’t calculate Λ  directly, but instead ln
)
66
/
18
1
ln(
)
18
66
(
)
156
/
22
ln(
22




 
/

.]
Both p-values are very small and are close to each other. The conclusion is that the vaccine has significant preventive 
effect ( p-value<0.001). Avoid statements such as ‘H0 is rejected’ or ‘p-value = 0.00006’ if results are to be reported in a 
scientific journal.
EX 86 In EX 62 two rates 
Y
X
λ
λ
 
and
 
in a Poisson process were compared. A 95% CI for the ratio 
X
Y
R
λ
λ /
=
was 
(1.09, 2.17) and it was concluded that there was a significant difference between the rates.
The same data were analyzed in EX 82 by performing a LR test, giving the p-value 0.0109. The hypothesis of equal 
rates was thus rejected.
Consider now a third way to analyze the data, by using a test based on the conditional Poisson property in (3).
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
105 
Hypothesis Testing
EX 86 (Continued)
(
)






+
=
=
+
t
s
t
p
n
Binomial
n
t
Y
s
X
t
Y
Y
X
Y
λ
λ
λ
,
~
)
(
)
(
)
(
so
t
s
t
p
H
H
Y
X
+
=
⇔
=
:
:
0
0
λ
λ
. Here 
4333
.0
65
85
65
ˆ
,
65
)
4
(
,
85
)
8
(
 

 
 
 
p
Y
X
and  and 
3
/
1
:
against 
 3
/
1
:
0
≠
=
p
H
p
H
a
.
Test statistic 
6025
.2
150
1
3
2
3
1
3
/
1
4333
.0
/)
1(
ˆ
0
0
0
=
⋅
⋅
−
=
−
−
=
n
p
p
p
p
T
. Since 
)1,0
(
~ N
Z
T
D
→

the p-value is 
(
)
0094
.0
6025
.2
2
=
>
Z
P
. The null hypothesis is thus strongly rejected (p-value < 0.01).
EX 87 In EX 51 it was shown how CIs for the parameters in the normal distribution can be constructed. Assume that 
( )n
i
iY
1
= are iid where 
)
,
(
~
2
σ
µ
N
Yi
.
a)	 Show how to test
0
0
0
:
against 
 
:
µ
µ
µ
µ
≠
=
a
H
H
.
b)	 Show how to test
2
0
2
2
0
2
0
:
against 
 
:
σ
σ
σ
σ
≠
=
a
H
H
.
c)	 Apply the tests with 
4.0
and
0.
16
2
0
0
 
 
V
P
 when 
.
7312
.0
,
67
.
16
,
10
2  
 
 
s
y
n
 Compare these results with the results that are obtained using an approach based on CIs.
a)	
).
1
(
~
/
0


 
n
T
n
S
Y
T
P
 (Cf. EX 51.) p-value 
(
)
OBS
T
n
T
P
>
−
=
)1
(
2
b)	
)1
(
~
)1
(
2
2
0
2
−
−
=
n
S
n
T
χ
σ
. (Cf. EX 51.) p-value
(
)
OBS
T
n
P
>
−
=
)1
(
2
2
χ
c)	
7312
.0
,
67
.
16
,
10
2  
 
 
s
y
n
.
To test 
0.
16
:
against 
0.
16
:
0
z
 
P
P
a
H
H
 consider the test statistic 


035
.0
478
.2
)
9
(
2
value
-
p
478
.2
10
/
7312
.0
0.
16
67
.
16
 
!
 

 

 
T
P
T
.  Here it suffices to conclude from a T-table that 
p-value < 0.05.
A 95% CI for µ is given by 
n
S
C
Y ±
, where C is determined by (
)
262
.2
025
.0
)
9
(
=
⇒
=
>
C
C
T
P
. 
Thus, 
)
28
.
17
,
06
.
16
(,
10
/
7312
.0
262
.2
67
.
16
r
. Both approaches suggest that H0 is rejected at the 5% level.
To test 
4.0
:
against 
 4.0
:
2
2
0
≠
=
σ
σ
a
H
H
 consider the test statistic 

 

 
45
.
16
4.0
7312
.0
9
T
p-value 


116
.0
45
.
16
)
9
(
2
2
 
!
 
F
P
,  so H0 is not rejected.
A 95% CI for 
2
σ
is given by 
a
S
n
b
S
n
2
2
2
)1
(
)1
(
−
<
<
−
σ
, where a = 2.7004 and b =19.0228 (See EX 51 for details.) 
This gives the interval (0.36, 2.44) and neither in this case is H0 rejected.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
106 
Hypothesis Testing
EX 88 Given two independent sets of iid variables (
)
( ) Y
X
n
i
i
n
i
i
Y
X
1
1
 
and
 
=
=
 where 
)
,
(
~
2
X
X
i
N
X
σ
µ
and 
)
,
(
~
2
Y
Y
i
N
Y
σ
µ
.
a)	 Show how to test
2
2
2
2
0
against 
 
:
Y
X
Y
X
H
σ
σ
σ
σ
≠
=
.
b)	 Show how to test 
Y
X
Y
X
H
µ
µ
µ
µ
µ
≠
=
=
against 
  )
(
:
0
.
c)	 In two independent data sets one obtains


¦
¦
 
 
 
1692
,
126
,
10
2
i
i
X
x
x
n
 and (
)
∑
∑
=
=
=
2122
  ,
127
  ,8
2
i
i
Y
y
y
n
. Perform the tests in a) 
and b) above and compare the results with that which are obtained by making CIs.
a)	 Let the largest of the two sample variances be
2
Y
S
. Then (Cf. EX 53.) 
)1
,1
(
~
2
2
2
2
−
−
X
Y
X
Y
Y
X
n
n
F
S
S
σ
σ
which under 
H0 becomes
)1
,1
(
~
2
2
−
−
X
Y
X
Y
n
n
F
S
S
. The p-value (two-sided) is






>
−
−
2
2
)1
,1
(
2
X
Y
X
Y
s
s
n
n
F
P
.
Notice that, if we for some reason, want to test 
2
2
0 :
Y
X
c
H
σ
σ
⋅
=
then the test statistic is 
2
2
X
Y
S
S
c ⋅
.
b)	 A suitable test statistic is 
(
)
(
)
Y
Y
X
X
n
n
Y
X
H
Y
X
V
H
Y
X
E
Y
X
T
/
/
2
2
0
0
σ
σ
+
−
=
−
−
−
−
=
.
If 
2
2
 
and
 
Y
X
σ
σ
were known then T would be distributed
)1,0
(
N
, but in practice the variances are unknown 
and has to be estimated. If the test in a) suggests that the variances could be assumed to be equal, 
2
σ
=
, then we estimate this by 
2
)1
(
)1
(
ˆ
2
2
2
−
+
−
+
−
=
Y
X
Y
Y
X
X
n
n
S
n
S
n
σ
. (Cf. EX 53.) It follows that the test statistic to be 
used is 
)
/
1
/
1(
ˆ
'
2
Y
X
n
n
Y
X
T
+
−
=
σ
. The p-value (two-sided) is 
(
)
OBS
Y
X
T
n
n
T
P
'
)
2
(
2
>
−
+
.
If the test in a) suggests that the variances are unequal then we are faced with the Behrens-Fisher problem 
mentioned in the Comments to EX 53.
When both 
Y
X
n
n
 
and
 
are large things become simpler since we can use the fact that
)1,0
(
~
1
/
/
/
/
)1,0
(
~
/
/
/
/
''
2
2
2
2
2
2
2
2
N
Z
n
n
n
S
n
S
N
n
n
Y
X
n
S
n
S
Y
X
T
D
P
Y
Y
X
X
Y
Y
X
X
Y
Y
X
X
Y
Y
X
X
→

→

+
+
+
−
=
+
−
=
σ
σ
σ
σ
The convergence in probability in the denominator above can be motivated in the following way: From (9b) 
(
)
(
)
2
2
2
2
2
/.
 
and
 
X
P
X
X
X
X
X
S
n
const
S
V
S
E
σ
σ
→

⇒
=
=
(Cf. (10)). Similarly,
2
2
Y
P
Y
S
σ
→

 . (11a) and (11b) 
then gives the result.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
107 
Hypothesis Testing
EX 88 (Continued)
c)	
(
)
60
.
11
)1
10
(
10
/
)
126
(
1692
,
600
.
12
2
2
 


 
 
X
s
x
,
13
.
15
)1
8
(
8
/
)
127
(
2122
,
875
.
15
2
2
 


 
 
ys
y
Test of 
2
2
2
2
0
:
against 
 
:
Y
X
a
Y
X
H
H
σ
σ
σ
σ
≠
=


34
.0
30
.1
)
9,7
(
value
-
p
30
.1
60
.
11
13
.
15
2
2
 
!
 

 
 
 
F
P
s
s
T
X
y
.  We can thus assume equal variances.
Test of 
Y
X
a
Y
X
H
H
µ
µ
µ
µ
≠
=
:
against 
 
:
0
14
.
13
2
8
10
13
.
15
)1
8
(
60
.
11
)1
10
(
ˆ 2
 







 
V
,


value
-
p
91
.1
8
/
1
10
/
1
14
.
13
600
.
12
875
.
15
'

 


 
T
=  


074
.0
0371
.0
2
91
.1
)
2
8
10
(
2
 

 
!


T
P
.  We can’t reject H0 at the 5% level.
A 95% CI for the variance ratio is
1
2
2
2
2
2
2
2
 
c
S
S
c
S
S
X
Y
X
Y
X
Y
⋅
<
<
⋅
σ
σ
(Cf. EX 53.). Here 
2
1
 
and
 
c
c
 are constants that are 
determined in the following way:
P F(7,9)<c1
(
)=0.025  can't be found in most tables, but P
1
F(7,9)>1
c1
!
"
#
$
%
&=P F(9,7)>1/c1
(
) 
 
21
.0
82
.4
/
1
025
.0
1
1
=
⇒
=
⇒
=
c
c
. (
)
20
.4
025
.0
)
9,7
(
2
2
=
⇒
=
>
c
c
F
P
 
Thus, the CI is 
)
21
.6,
31
.0
(
21
.0
60
.
11
13
.
15
 ,
20
.4
60
.
11
13
.
15
=
⎟
⎠
⎞
⎜
⎝
⎛
⋅
⋅
, in accordance with the test result above.
A 95 % CI for 
X
Y
µ
µ
−
is 
(
))
/
1
/
1
ˆ
)
(
2
Y
X
n
n
C
X
Y
+
±
−
σ
(Cf. EX 53.). C is determined by 


120
.2
025
.0
)
16
(
 

 
!
C
C
T
P
.
Thus, (15.875-12.600 
65
.3
28
.3
)
8
/
1
10
/
1(
14
.3
120
.2
r
 

r
.  Since the interval covers zero the difference 
between the means is not significant.
EX 89 ( )n
i
iY
1
= are iid variables with an arbitrary distribution and with
2
)
(
 
and
 
)
(
σ
µ
=
=
i
i
Y
V
Y
E
. Show how to 
test 
0
0 :
µ
µ =
H
when n is large.
In EX 58 it was shown that 
f
o
o


n
N
Z
n
S
Y
D
as
)1,0
(
~
/
P
.  As a test statistic we thus chose 


OBS
T
Z
P
n
S
Y
T
!

 
2
is
value
-
p
and
/
0
P
 for a two-sided alternative.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
108 
Hypothesis Testing
EX 90 (
)
( ) Y
X
n
i
i
n
i
i
Y
X
1
1
 
and
 
=
=
are independent sets of iid variables with arbitrary distributions and finite means and 
variances. Show how to test 
Y
X
H
µ
µ
=
:
0
when both sample sizes are large.
The test statistic to use is 
)1,0
(
~
/
/
2
2
N
n
S
n
S
Y
X
T
Y
Y
X
X
+
−
=
.
An argument for the distribution follows from EX 88 b). In the latter case all variables were assumed to have normal 
distributions. But, looking back at the proof it is seen that the numerator tends in distribution to a 
)1,0
(
N
-variable 
also for iid variables with arbitrary distributions.
The statistic above can also be used for constructing a CI for the difference between means.
6.2.4	
Nonparametric methods
In earlier chapters inference has been based on estimated parameters in probability models. Such problems 
are said to be parametric, and others are called nonparametric. The distinction between the two methods 
is not clear-cut. Test of independency or test of equal marginal proportions are sometimes referred 
to as nonparametric methods, although a lot of parameters are involved. A tentative position is that 
nonparametric methods are less affected by unrealistic assumptions. The latter are however also based on 
assumptions, something that is often overlooked, especially that the observations are assumed to be iid.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Exercises in Statistical Inference  
with detailed solutions
109 
Hypothesis Testing
Goodness of Fit and comparison of distributions based on the sample-distribution function
We saw in Ch. 6.2.1 that the Chi-square test can be used to test the hypothesis that the observations in 
a sample come from a certain distribution. This Goodness of Fit problem was solved by first estimate the 
parameters in the hypothetical distribution and then compare observed and expected frequencies. In this 
section we consider an alternative way to test for Goodness of Fit which we call the Kolmogorov test, also 
called the Kolmogorov-Smirnov test, proposed by the Russian statistician A. Kolmogorov in 1933. An 
important difference between the two tests is that in the Chi-square test the parameters are estimated, 
but in the Kolmogorov test the parameters have to be specified (or known). A further limitation is that 
the Kolmogorov test (in its original form) can only be applied to continuous distributions. Some attempts 
have been made to use the Kolmogorov test when parameters are estimated, e.g. in the Exponential or 
the Normal distribution. In the latter case the adjusted test is called the Lilliefors test and is provided 
by many statistical soft-wares (e.g. in proc univariate in SAS).
The sample cdf , 
)
(y
Sn
, is constructed in the following way. Rank all observations from the smallest 
to the largest 
)
2
(
)
1(
y
y
≤
≤
…
)
(n
y
≤
. From the sequence (
)n
i
i
n
i
y
1
)
(
/
,
=  we then form the step function 
)
1
(
)
(
  ,
/
)
(
+
<
≤
=
i
i
n
y
y
y
n
i
y
S
. As an illustration consider the data (1,2,2,5). Then 
1
,0
)
(
4
<
=
y
y
S
5
,1
,5
3
,4
/
3
3,
2
,4
/
3
,2
1
,4
/
1
t
 

d
 

d
 

d
 
y
y
y
y
.
)
(
)
(
:
0
0
y
F
y
F
H
=
, ( the hypothetical cdf with known parameters.)
The Kolmogorov test statistic is 
)
(
)
(
max
0 y
F
y
S
D
n
n
−
=
and H0 is rejected for large values of 
n
D . In 
this case it is too complicated to compute p-values and a RR approach is simpler. The smallest values 
for which H0 is rejected (two-sided test) with the significance levels 
=
α
0.05 and 0.01 and for various 
sample sizes 
1
≥
n
, can easily be downloaded from the internet. For large n (at least larger than 100) 
the RR consists of observed values of 
n
D larger than 
n
/
36
.1
 with 
05
.0
 
D
 and larger than 
n
/
63
.1
 
with 
01
.0
 
D
.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
110 
Hypothesis Testing
EX 91 Test whether the following ranked numbers are generated by a variable that is ~ Uniform [0,1] 
.09 .20 .23 .29 .32 .34 .34 .37 .41 .45 .53 .70 .83 .87 .94 .97
 
y
y
F
y
F
H
=
=
)
(
)
(
:
0
0
.
We get the following table: (Notice that in this case
)
(
0
)
(
i
y
y
y
F
=
=
.)
)
(i
y
.09
.20
.23
.29
.32
.34
.37
n
i /
1/16=
.0625
2/16=
.1250
3/16=
.1875
4/16=
.2500
5/16=
.3125
7/16=
.4375
8/16=
.5000
.41
.45
.53
.70
.83
.87
.94
.97
9/16=
.5625
10/16=
.6250
11/16=
.6875
12/16=
.7500
13/16=
.8125
14/16=
.8750
15/16=
.9375
16/16=
1
The largest value of 
16
D  is 
175
.
45
.0
6250
.0
 

 and this is far below the rejection limit .3273 
)
05
.0
(
 
D
,  so 
there is no reason to reject the null hypothesis
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet’s 
electricity needs. Already today, SKF’s innovative know-
how is crucial to running a large proportion of the 
world’s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Exercises in Statistical Inference  
with detailed solutions
111 
Hypothesis Testing
A different problem is to compare two distributions by means of the observations in two independent 
samples. This was done several times in Ch. 6.2.2-6.2.3 by assuming a specific form of the distributions, e.g. 
)
,
(
~ 
and
 )
,
(
~
2
2
σ
µ
σ
µ
Y
X
N
N
. Now we will test the hypothesis 
)
(
)
(
:
2
1
0
y
F
x
F
H
=
 without specifying 
the form of the cdfs, which are assumed to be continuous. The latter hypothesis is also termed the 
hypothesis of homogeneity. The test that is used will be called the Smirnov two-sample test, also called 
the Kolmogorov-Smirnov two-sample test. N.V. Smirnov (1900-1966) was a great mathematician in the 
former Soviet Union who won prices in many areas. (He is said to have won “the bronze star in vodka 
distillation” in 1940, but this is may be a student jokes.) 
The Smirnov test statistic is
)
(
)
(
max
,
y
S
x
S
D
n
m
n
m
−
=
, where 
)
(x
Sm
is the sample cdf from a sample 
of size m and 
)
(y
Sn
is the sample cdf from a sample of size n. H0 is rejected for large values of 
n
m
D , . 
Tables for the test can easily be downloaded from the internet. Critical values are given for each pair of 
m,n (often denoted 
2
1,n
n
) and for 
05
.0
 
D
 For 
01
.0
 
D
.  large sample sizes, say above 25, approximate 
critical values are given by 
/
1
/
1
36
.1

n
m
 and 
/
1
/
1
63
.1
n
m 
 for 
01
.0
 
D
.
The Smirnov test shall in first place be used when very little is known about the distributional form, but 
also as a complement to parametric tests in situations where it is suspected that lack of significance in 
a test might be a result of choosing a bad probability model.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
112 
Hypothesis Testing
EX 92 Check whether the following two samples of observations on the variables 
i
X and 
iY are drawn from the 
same population.
i
X : 7.6 8.4 8.6 8.7 9.3 9.9 10.1 10.6 11.2 (m = 9)
iY  : 5.2 5.7 5.9 6.5 6.8 8.2 9.1 9.8 10.8 11.3 11.5 12.3 12.5 13.4 14.6 (n = 15)
We make the following table of ranked observations:
)
(i
X
)
(i
Y
)
(
)
(
y
S
x
S
Y
X
−
5.2
0-1/15 = -1/15
5.7
0-2/15 = -2/15
5.9
0-3/15 = -3/15
6.5
0-4/15 = -4/15
6.8
0-5/15 = -5/15
7.6
1/9-5/15 = -2/9
8.2
1/9-6/15 = -13/45
8.4
2/9-6/15 = -8/45
8.6
3/9-6/15= -1/15
8.7
4/9-6/15= 2/45
9.1
4/9-7/15= -1/45
9.3
5/9-7/15= 4/45
9.8
5/9-8/15= 1/45
9.9
6/9-8/15= 2/15
10.1
7/9-8/15= 11/45
10.6
8/9-8/15= 16/45
10.8
8/9-9/15= 13/45
11.2
1-9/15= 2/5= 0.400
 
11.3
1-10/15= 1/3
11.5
1-11/15= 4/15
12.3
1-12/15= 1/5
12.5
1-13/15= 2/15
13.4
1-14/15= 1/15
14.6
1-1= 0
We obtain 
45
/
18
400
.0
12
,9
 
 
D
.  (Tables of critical values of this test often show fractions.) Since 


20
.0
422
.0
45
/
19
15
,9
 
|
!
D
P
 it is concluded that the maximal difference isn’t large enough to reject the hypothesis 
of equal distributions. In fact 

05
.0
533
.0
15
,9
 
!
D
P
,  so a much larger maximal difference would be required to reject 
the hypothesis.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
113 
Hypothesis Testing
The Smirnov test can be used to test the more general hypothesis 
2
),
(
)
(
:
1
0
t
 
 
k
y
F
y
F
H
k

.  If all 
the k sample sizes are large, the p-value for H0 can be computed quite easily, although the computations 
may be heavy. It is wise to have a computer program that calculates the value of the test statistic. Due 
to the usability in situations where it is hard to know the population distribution, we outline the test 
procedure (following the work in Fisz 1963, p. 409).
Let the sample sizes be n1…ni…nk and define the constants 
)
/(
)
(
)
/(
)
(
,)
/(
1
1
1
3
2
1
2
1
3
3
2
1
1
2
2
k
k
k
k
n
n
n
n
n
K
n
n
n
n
n
n
K
n
n
n
n
K
+
+
+
+
=
+
+
+
=
+
=
−
…
…
…
 
Introduce the statistics 
   …
∑
∑
−
=
−
=
−
=
1
1
1
1
/)
(
)
(
max
k
i
i
k
i
i
i
k
k
n
y
S
n
y
S
D
 
Put 
(
)
k
MAX
i
i
i
A
A
A
k
i
D
K
A
…
…
2
max
 
and
 
2
 ,
=
=
=
. Then the p-value is 
where 
)
(λ
Q
is the Kolmogorov-Smirnov λ -distribution. (Cf. Table VIII in Fisz 1963.)   
)
(
)
(
)
(
(
)
(
max
 ,)
(
)
(
max
2
1
2
2
1
1
3
3
1
2
2
n
n
y
S
n
y
S
n
y
S
D
y
S
y
S
D
+
+
−
=
−
=
(
)
1
)
(
1
−
−
k
MAX
A
Q
The Sign Test and The Wilcoxon Signed-Rank test for One Sample
Let M be the population median in a continuous distribution. Then the hypothesis
)
(
)
(
:
0
0
y
F
y
F
H
=
implies the hypothesis
0
0 :
M
M
H
=
. E.g. if 
)
,
(
~
2
σ
µ
N
Y
then 
µ
=
M
. When data consist of matched 
pairs(
)n
i
i
i Y
X
1
,
= one can reduce the problem of making inference from two dependent samples to a one-
sample problem by considering the differences 
.
1
 ,
n
i
Y
X
D
i
i
i
…
=
−
=
	   In this case it is natural to test the 
hypothesis 
0
:
0
=
M
H
which is equivalent to
(
)
(
)
2
/
1
:
0
=
<
=
>
Y
X
P
Y
X
P
H
.
The Sign Test for 
0
0 :
M
M
H
=
consists of computing the value of the test statistic Y = ‘Number of 
observations below 
0
M (if suspiciously few are below) or above 
0
M (if suspiciously few are above)’. By 
suspiciously few we mean that they deviate much from the expectation n/2. Under H0 the test statistic 
is distributed
)
2
/
1
,
(
=
p
n
Binomial
.
 EX 93 Consider the following measurements of body temperature ( in degrees Celsius):
37.1 37.0 37.3 37.2 36.9 37.4 36.8 37.1 37.3 37.3 36.9 37.0 37.5 37.2 37.1
Are these data in agreement with the hypothesis that the median body temperature in the population is 37.0?
Since there are just 3 values that are below 37.0 we compute the probability (
)=
=
≤
2
/
1
3 p
Y
P
 
0176
.0
32768
576
15
)
2
/
1(
)
2
/
1(
)
2
/
1(
15
3
0
15
3
0
15
 
 
¸¸
¹
·
¨¨
©
§
 
¸¸
¹
·
¨¨
©
§
¦
¦
 
 

y
y
y
y
y
y
.  So, the (two-sided) p-value is 
035
.0
0176
.0
2
=
⋅
 
and the hypothesis is rejected.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
114 
Hypothesis Testing
The Sign Test for Matched Pairs is illustrated in the following example.
EX 94 Blood pressure measurements (in millimeters of mercury) were obtained before and after a training program 
with the following result:
Subject
1
2
3
4
5
6
Before
136.9
201.4
166.8
150.0
173.2
169.3
After
130.2
180.7
149.6
153.2
162.6
160.1
Difference
6.7
20.7
17.2
-3.2
10.6
9.2
Test the hypothesis that the median difference is zero.
Since there is just 1 difference that is negative we consider the variable Y = ‘Number of differences that are negative’ 
and calculate the probability (
)
+






=
=
≤
6
0
)
2
/
1(
)
2
/
1(
0
6
2
/
1
1 p
Y
P
109
.0
)
6
1(
)
2
/
1(
)
2
/
1(
)
2
/
1(
1
6
6
5
1
=
+
=






. The (two-sided) p-value is 0.22 so the hypothesis can’t be rejected by 
the sign test.
As a comparison we use Student’s T-test (Cf. EX 87.) for the hypothesis that the mean difference is zero.


¦
¦
 

 
 

 
 
4440
.
70
6
/
)
2.
61
(
46
.
976
5
1
,2.
10
46
.
976
,2.
61
2
2
2
d
i
i
s
d
d
d
.


0155
.0
977
.2
)
5
(
977
.2
6
/
4440
.
70
0
2.
10
 
!

 

 
T
P
T
. So, the (two-sided) p-value is 0.031 and the hypothesis 
is rejected.
Notice that the latter test is based on the assumption that the observed differences come from a normal distribution. 
It is to be expected that tests that make use of more information about the distribution are more efficient (provided 
that the distributional assumptions are valid). However, in the next example we introduce a nonparametric test that is 
more efficient than the sign test and is nearly as efficient as the T-test. 
The Wilcoxon Signed-Rank Test for Matched-Pairs
We will test 
)
(
)
(
:
0
y
F
x
F
H
Y
X
=
based on a sample of matched pairs(
)n
i
i
i Y
X
1
,
= . Proceed in the 
following steps:
--
Form the differences 
¯
®
­


!

 

 
0
if
,
0
if
,
i
i
i
i
i
D
D
Y
X
D
.  Ties, i.e. cases with 
0
=
i
D
, are eliminated. The 
‘working’ sample size after this elimination is denoted n’. It is assumed that the differences are 
continuous and have a symmetric distribution about 0.
--
Rank the absolute differences from the smallest (1) to the largest (n’) and put a + or a – sign 
above the absolute difference. If two or more absolute differences are tied for the same rank, 
then the average rank is assigned to each member of the tied group. E.g. the six observations 
6<7=7=7=7<8 are given the ranks 1, 3.5, 3.5, 3.5, 3.5, 6 since (2+3+4+5)/4=3.5.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
115 
Hypothesis Testing
--
Put 
−
T = ‘Rank sum for negative differences’ and 
+
T = ‘Rank sum for positive differences’. 
As a test statistic chose 
)
,
min(
+
−
=
T
T
T
and reject H0 if 
C
T
T ≤
, where 
C
T is the critical 
value in the table. Tables are easily downloaded from the internet. A good table can be found 
in Wackerly et al 2008, Table 9 in Appendix 3. The latter shows critical values for working 
sample sizes up to 50 together with the p-values 0.10, 0.05, 0.02 and 0.01 (two-sided tests). 
p-values can be computed in an exact way but this is complicated. For n > 50 one can use the 
fact that 
24
/)1
2
)(
1
(
4
/)1
(




 

n
n
n
n
n
T
Z
 has approximately a 
)1,0
(
N
-distribution. So, the p-value is 
)
(
2
OBS
Z
Z
P
>
.
EX 95 Consider again the data in EX 94. The following table can be constructed:
Sign
-
+
+
+
+
+
i
D
3.2
6.7
9.2
10.6
17.2
20.7
Rank
1
2
3
4
5
6
From this we get 


1
20
,1
min
20
,1
 
 

 
 


T
T
T
.  The critical value from the table in 
Wackerly et al mentioned above is 
1
=
C
T
and this corresponds to a p-value less than 0.05. The exact 
p-value is 
0313
.0
32
/
1
|
,  very close to that obtained by the T-test in EX 94.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Exercises in Statistical Inference  
with detailed solutions
116 
Hypothesis Testing
The Mann-Whitney U Test for Two Independent Samples
Also now we test the hypothesis 
)
(
)
(
:
0
y
F
x
F
H
Y
X
=
, but there are two independent samples (
) X
n
i
i
X
1
=
and ( ) Y
n
i
iY
1
=  each with iid observations. The distributions may be discrete or continuous. Assume that 
Y
X
n
n
≤
. The test proceeds in the following steps:
--
Put all the 
Y
X
n
n
+
observations together and rank them according to their magnitude, from 
smallest to largest. Compute the rank sum for the observations that belong to the X – sample 
and call this W. 
--
The test statistic is
W
n
n
n
n
U
X
X
Y
X
−
+
+
=
2
/)1
(
. Under H0 the distribution of U is symmetric 
about the expectation
2
/
)
(
Y
X n
n
U
E
=
. This in turn implies that (
) =
≤
0
u
U
P
(
)
0
u
n
n
U
P
Y
X
−
≥
.
--
H0 is rejected for extremely large or small values of U with two-sided tests. Critical values are 
obtained from tables. We will show in the example below how p-values can be computed by 
using Table 8, Appendix 3 in Wackerly et al. The latter gives values of 
(
)
0
u
U
P
≤
for sample 
sizes up to 10 and 
,1,0
0
u =
…
.2
/
,
Y
X n
n
--
For 
10
and
10
!
!
Y
X
n
n
 it can be shown that 
12
/)1
(
2
/



 
Y
X
Y
X
Y
X
n
n
n
n
n
n
U
Z
i is close to a 
)1,0
(
N
-distribution and p-values (two-sided) are obtained from 
(
)
OBS
Z
Z
P
>
⋅
2
.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
117 
Hypothesis Testing
EX 96 Consider the following two independent series of independent observations:
x:
51
32
41
57
47
38
62
44
42
35
y:
61
34
60
59
63
45
49
53
46
58
Test whether the two series come from the same population:
a)	 By using the Mann-Whitney U Test.
b)	  -“- with Normal approximation.
c)	 By using the T-test for two independent samples.
a)	 Put all observations together and rank those observations that are coming from the x-series. 
32
34
36
38
41
42
44
45
46
47
Rank
1
4
5
6
7
10
49
51
33
57
58
59
60
61
62
63
Rank
12
14
19
The rank sum is 
78
19
14
12
10
7
6
5
4
1
 








 
W
 and the test statistic is 
77
78
2
/)1
10
(
10
10
10
 





 
U
.
The latter value is larger than the expectation, 
50
2
/
10
10
 

.  The p-value is thus 




23
2
77
2
d

 
t

U
P
U
P
. [Remember the property of the U-distribution listed above.]
From the table we get 

0216
.0
23  
d
U
P
.  So, the p-value is about 0.04 and the hypothesis of similar populations 
can be rejected (p<0.05, Mann-Whitney U Test.).
b)	 The observed value of Z is 
041
.2
12
/)1
10
(
10
10
2
/
10
10
77
 





 and p-value is
(
)
0414
.0
0207
.0
2
041
.2
2
=
⋅
=
>
⋅
Z
P
, very close to the p-value obtained in a).
c)	


¦
¦
 


 
 
 
 
77
.
90
10
/
)
449
(
20977
)1
10
(
1
,9.
44
,
20977
,
449
2
2
2
X
i
i
s
x
x
x


¦
¦
 


 
 
 
 
84
.
84
10
/
)
528
(
28642
)1
10
(
1
,8.
52
,
28642
,
528
2
2
2
Y
i
i
s
y
y
y
First we test 
2
2
0 :
Y
X
H
σ
σ
=
[Cf. EX 88 a).].


92
.0
46
.0
2
07
.1
)
9,9
(
2
value
-
p
07
.1
2
2
 

 
!

 

 
F
P
s
s
y
X
.  There is no reason to reject the null 
hypothesis and we can pool the two variances 
81
.
87
)
2
10
10
(
44
.
84
)1
10
(
77
.
90
)1
10
(
ˆ 2
 







 
V
.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
118 
Hypothesis Testing
EX 96 (Continued) We then test 
Y
X
H
µ
µ
=
:
0
[Cf. EX 88 b)]




076
.0
038
.0
2
041
.2
)1
10
(
2
value
-
p
041
.2
10
/
1
10
/
1
81
.
87
8.
52
9.
44
 

 
!


 


 


T
P
.
According to this test we can’t reject H0 at the 5% level.
One reason to the failure of the T-test to reject the null hypothesis in this case, may be that the normality assumption 
is violated. A histogram of the y-values gives the following pattern:
y
25–35
35–45
45–55
55–65
Frequency
1
1
3
5
The histogram suggests that the y-values are sampled from a population with a skew distribution rather than a 
normal distribution, although the sample size is too small to verify this.
This illustrates the strength of non-parametric methods since these are not, or to less extent, dependent on the form 
of the population distribution.
Fisher’s Exact Test of Independency
In EX 70 and EX 79 it was shown how independency could be tested in contingency tables, by the chi-
square principle and the LR principle, respectively. Both these tests require that the sample size n is large 
and p-values are computed by using the asymptotic distribution. In small samples one can use a test 
termed Fisher’s exact test to test for independence. Using the same notation for the cell frequencies in 
the 2 x 2 table as in Ch. 6.2.1, we calculate the probability of a certain outcome in the four cells, given 
that the marginal are fixed, from the expression
 
























\
\
\
\
\
\
\
\
\
\
\
\
3




 

(27) 
The p-value is obtained by calculating the sum of probabilities of all outcomes in the 2 × 2 table that are 
more extreme or equal to the observed outcome. This is illustrated in EX 95 below.
This test was first suggested by R.A. Fisher who discussed an experimental investigation of a lady’s claim 
to be able to tell by taste whether the tea was added to the milk or the milk was added to the tea (‘the 
tea drinking lady experiment’). In that case all margins were fixed since there were 4 cups of each type 
and the lady was informed about this fact. However, the test is used also in situations where just one 
margin is fixed, and even when only the total n is fixed. In the last case it is an example of a conditional 
test where we condition on the margins in the present data, although we are aware of that the margins 
will vary randomly from sample to sample. 
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
119 
Hypothesis Testing
EX 97 The following 2 x 2 table shows the frequencies of the two variables lower back pain (yes/no) and sex (males 
and females).
Lower back pain
Yes
No
Sex
Male
6
2
Female
11
19
a)	 Use Fisher’s exact test to investigate whether the two variables are independent.
b)	 Answer the same question by using the Chi-square principle and the LR principle.
a)	 Construct tables with actual and more extreme outcomes given fixed margins:
6
2
8
7
1
8
8
0
8
11
19
30
10
20
30
9
21
30
17
21
38
17
21
38
17
21
38
Table A                       Table B                          Table C
According to Eq. (27) the probability of the outcome in Table A is 
38!
19!
11!
2!
!6
!
21
!
17
!
30
!8
.  The sum of the probabilities of 
the outcomes in Table A to Table C is
0620
.0
21!
9!
0!
8!
1
20!
10!
1!
!7
1
19!
11!
2!
6!
1
38!
21!
17!
30!
8!
 
¸
¹
·
¨
©
§


.  This is the p-value for a one-sided test. The 
p-value for a two-sided test is usually found by doubling the one-sided value (McCullagh & Nelder 1983, p99) and this 
is the simplest alternative, but there are also other more complicated possibilities (Rao 1965, p345). The two-sided 
p-value is thus 0.1240.
The calculation of exact probabilities is tedious, but tables can be downloaded from the internet and even 
calculators, e.g. ‘Free Fisher’s Exact Test Calculator’ which can be found on www.danielsoper.com . In SAS p-values 
are obtained from proc freq by adding /chisq (see SAS manuals for details). The p-values obtained may vary slightly 
depending on which alternative is used.
b)	 The Chi-square principle gives (cf. EX 70) 
7539
.3
3536
.0
4367
.0
3258
.1
6378
.1
2
=
+
+
+
=
X
, p-value
(
)
05027
.0
7539
.3
)1(
2
=
>
=
χ
P
. The latter is a two-
sided p-value that differs very much from the value 0.1240 obtained in a). It is obvious that the sample size n = 38 
is too small for the Chi-square approximation to be valid. One simple way to deal with the problem is to use Yates 
correction for continuity (Yates 1934, p217)




,
3635
.2
21
17
30
8
2
/
38
11
2
19
6
2
/
2
2
1
2
1
2
21
12
22
11
2
 







 


 




y
y
y
y
n
y
y
y
y
XYates
p p-value = (
)
1242
.0
3635
.2
)1(
2
=
>
χ
P
, 
very close to the p-value in a).
The LR principle gives (cf. EX 81)
,
38
/
11
ˆ
,
38
/
2
ˆ
,
38
/
6
ˆ
,
38
/
21
ˆ
,
38
/
17
ˆ
,
38
/
30
ˆ
,
38
/
8
ˆ
21
12
11
2
1
2
1
 
 
 
 
 
 
 




p
p
p
p
p
p
p
38
/
19
ˆ 22  
p


83052
.3
58980
.2
18822
.2
58646
.1
10014
.3
2
log
2
 





 
/


p-value 
(
)
05033
.0
83052
.3
)1(
2
=
>
=
χ
P
. This is close to 0.05027 obtained with the Chi-square principle. It seems to 
be unknown how to obtain a corrected test statistic in this case.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
120 
Hypothesis Testing
Rank correlation
Pearson’s correlation coefficient for the correlation between two variables (cf. the section about 
properties of 
)
,
(
2
1 y
y
p
in Ch. 2.1) is estimated by the sample correlation coefficient defined as 
∑
∑
∑
∑
−
=
−
−
=
−
=
n
y
y
s
n
n
x
x
s
n
s
s
s
r
i
i
Y
i
i
X
Y
X
XY
/
)
(
)1
(  ,
/
)
(
)1
( 
 where
,
/
2
2
2
2
2
2
 and 
¦
 

s
n
XY
)1
(
 
¦
¦
¦

n
y
x
y
x
i
i
i
i
/)
)(
(
.  However, the calculation of r requires that the scale of the variables is at 
least at an interval level, i.e. that the operations addition and subtraction make sense. For ordinal (rank 
order) data one may define other measures of correlation. The simplest is Spearman’s coefficient Sr . Let 
)
( ix
R
be the rank of xi among x1…xn and let 
)
(
iy
R
be the rank of yi among y1…yn, then 
Sr is defined as 
r but with 
i
i
y
x
 
and
 
replaced by
)
(
 
and
 )
(
i
i
y
R
x
R
. The ranks for tied observations are treated in the same 
way as was done for the Mann-Whitney U test. If there are no ties in both the x and the y observations 
the computation of 
Sr can be simplified, 
∑
−
=
−
−
=
)
(
)
(
 
 where
,
)1
(
6
1
2
2
i
i
i
i
S
y
R
x
R
d
d
n
n
r
.
Sr can be used to test the hypothesis of no association between two variables in situations where it isn’t 
possible to obtain precise measurements, but only ranked values. In two-sided tests the null hypothesis 
shall be rejected for large or small values of Sr (remember that 
1
1
≤
≤
−
Sr
). Critical values are found in 
tables, e.g. Table 11, Appendix 3 in Wackerly et al 2007. Tables can be easily downloaded from the internet.
EX 98 A person was asked to make an assessment about the ability of 10 subjects and rank them. The ability of the 
subjects was then evaluated in a formal test. The result was
Rank
Test (x)
1
2
3
4
5
6
7
8
9
10
according to
Assessment (y)
3
4
1
5
6
8
2
10
7
9
We find ¦
 




 

 
685
.0
52
)1
100
(
10
6
1
52
2
S
i
r
d
.  This is quite large, but is it large enough in order to reject 
the hypothesis of no association between the ranked series? Referring to Table 11 mentioned earlier, one finds the 
critical value 0.648 for 
025
.0
=
α
(one-sided test) and 
05
.0
 
D
 (two-sided test). The conclusion is that there is a 
significant association between the two series (p<0.05, Spearman’s rank correlation).
6.3	
The power of normally distributed statistics
Let T be a statistic with mean
n
/)
(
 
 variance
and
 
2 θ
σ
θ
. The variance is thus allowed to be a function of 
the mean. An example of this is the sample proportion
)
,
(
~
 
 where
,
/
ˆ
p
n
Binomial
Y
n
Y
p =
, with mean 
p and variance
n
p
p
/)
1( −
. In this section it is assumed that T is normally distributed or at least that 
n is so large that
n
T
Z
/)
(θ
σ
θ
−
=
can be assumed to be distributed
)1,0
(
N
. In the examples below we first 
show a general expression for the power and then we consider some special cases.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
121 
Hypothesis Testing
EX 99 A general expression for the power.
Find the RR for testing 
0
0
0
:
against 
 
:
θ
θ
θ
θ
≠
=
a
H
H
when the type I error isα , and determine the power.
The RR is obviously of the form
α
α
α
θ
θ
θ
C
T
C
T
C
T
−
<
−
>
−
>
−
)
(
or 
 
)
( 
i.e.
 ,
0
0
0
, where 
α
C
is a constant that 
depends onα .
(
)=
=
0
0
Reject 
H
H
P
α
(
)
(
)=
−
<
−
+
>
−
0
0
0
0
H
C
T
P
H
C
T
P
α
α
θ
θ
[Do the same operations on both sides 
of the inequality sign.]
=






−
<
−
+






>
−
=
n
C
n
T
P
n
C
n
T
P
/)
(
/)
(
/)
(
/)
(
0
0
0
0
0
0
θ
σ
θ
σ
θ
θ
σ
θ
σ
θ
α
α
=






−
<
+






>
n
C
Z
P
n
C
Z
P
/)
(
/)
(
0
0
θ
σ
θ
σ
α
α
[Due to symmetry.]=






>
⋅
n
C
Z
P
/)
(
2
0
θ
σ
α
.
In the sequel we choose 
05
.0
 
D
,  so 
n
C
n
C
/)
(
96
.1
96
.1
/)
(
0
05
.0
0
05
.0
T
V
T
V

 

 
.
 The RR, with 
05
.0
 
D
,  is 
n
T
/)
(
96
.1
0
0
T
V
T

!


(28a)
The power is 





 






!
 
 
n
T
P
n
T
P
H
P
Pow
/)
(
96
.1
/)
(
96
.1
Reject 
)
(
0
0
0
0
0
T
V
T
T
V
T
T
  
[Do the same operations on both sides of the inequality sign.]=
¸¸
¹
·
¨¨
©
§






¸¸
¹
·
¨¨
©
§



!

n
n
n
T
P
n
n
n
T
P
/)
(
/)
(
96
.1
/)
(
/)
(
/)
(
96
.1
/)
(
0
0
0
0
T
V
T
T
V
T
T
V
T
T
V
T
T
V
T
T
V
T
.
From this we get
¸¸
¹
·
¨¨
©
§





¸¸
¹
·
¨¨
©
§


!
 
n
Z
P
n
Z
P
Pow
)
(
)
(
)
(
)
(
96
.1
)
(
)
(
)
(
)
(
96
.1
)
(
0
0
0
0
T
V
T
T
T
V
T
V
T
V
T
T
T
V
T
V
T
 
(28b) 
In (28a) and (28b) 
05
.0
 
D
. If it is very important to not falsely reject the null hypothesis one should choose a 
lower type I error. E.g. with 
01
.0
 
D
 the figure 1.96 is replaced by 2.575.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
122 
Hypothesis Testing
EX 100 Find a RR for testing
2
/
1
:
against 
 2
/
1
:
0
≠
=
p
H
p
H
a
, where p is a Binomial proportion, and study 
the power.
In EX 23 b) it was shown that 
n
p
p
p
p
/)
1(
ˆ
−
−
can be assumed to be distributed 
)1,0
(
N
for large n. Here 
n
p
p
p
V
p
p
E
/)
1(
)ˆ
(
 
and
 
)ˆ
(
−
=
=
, so in this case 
)
1(
)
(
 
and
 
2
p
p
p
−
=
=
θ
σ
θ
. Putting this into Eq. (28a) 
gives the following RR: 
n
n
p
1
2
/
1
2
/
1
96
.1
2
/
1
ˆ
|

!

.
The power is from (28b):
¸¸
¹
·
¨¨
©
§







¸¸
¹
·
¨¨
©
§




!
 
n
p
p
p
p
p
Z
P
n
p
p
p
p
p
Z
P
p
Pow
)
1(
)
2
/
1
(
)
1(
2
/
1
96
.1
)
1(
)
2
/
1
(
)
1(
2
/
1
96
.1
)
(
.
The behavior of this power is studied when n = 50 (pow1) and when n =100 (pow2). The following program codes (in 
SAS) computes the power and depicts the shapes of the powers in Figure 2 below.

Ś
ɨʰɬɥŚɩʰɨɥɥŚ
ʰɥŜɨɥŜɰɥŜɨŚ
ʰɨŜɰɭŵɩŵſƋſɨŞƀƀŚ
ɨʰſŞɨŵɩƀƋſɨƀŵſƋſɨŞƀƀŚ
ɩʰſŞɨŵɩƀƋſɩƀŵſƋſɨŞƀƀŚ
ɨʰɨŞſŞɨƀʫſŞŞɨƀŚ
ɩʰɨŞſŞɩƀʫſŞŞɩƀŚ
ŚŚ
ŚɨɩŚ
Ś









2EV
S
SRZ
SRZ
   
   
   
   
   
   
   
   
   

Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
123 
Hypothesis Testing
Figure 2
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Exercises in Statistical Inference  
with detailed solutions
124 
Hypothesis Testing
Figure 3
EX 101 Let ( )n
i
iY
1
= be iid observations where
)
(
~
λ
l
Exponentia
Yi
. Construct a RR for testing 
0
0
0
:
against 
 
;
λ
λ
λ
λ
≠
=
a
H
H
when n is so large that the CLT is applicable. Also, study the power and notice 
what happens if n is too small for the normality approximation to be valid, say n = 10.
n
Y
V
Y
E
Y
V
Y
E
i
i
2
2
/
1
)
(
 ,
/
1
)
(
/
1
)
(
 ,
/
1
)
(
λ
λ
λ
λ
=
=
⇒
=
=
, so in this case
2
2
/
1
)
(
 
and
 
/
1
λ
θ
σ
λ
θ
=
=
. For large n, 
2
/
1
/
1
λ
λ
n
Y −
is distributed
)1,0
(
N
.
(28a) gives the RR:
n
Y
0
0
/
1
96
.1
/
1
O
O !

.
(28b) gives the power, where we notice that 
0
0
0
0
0
0
1
/
1
/
1
/
1
)
(
 
and
 
/
1
/
1
)
(
)
(
λ
λ
λ
λ
λ
θ
σ
θ
θ
λ
λ
λ
λ
θ
σ
θ
σ
−
=
−
=
−
=
=
. It turns out that 
the power is a function of 
0
/λ
λ
:




n
Z
P
n
Z
P
Pow
))
/
(
1(
)
/
(
96
.1
))
/
(
1(
)
/
(
96
.1
)
/
(
0
0
0
0
0
O
O
O
O
O
O
O
O
O
O









!
 
.
The latter expression is obtained under the assumption that n is large. When n = 10 the power is illustrated in Figure 
3 above. It is seen that the power is very weak for  
1
/
0 >
=
λ
λ
r
and perhaps more interesting is that the type I error 
can be smaller for r > 1 than for r = 1 (the value under
0
H ). Such a test is called biased.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
125 
Hypothesis Testing
6.4	
Adjusted p-values for simultaneous inference
We have been told to reject the null hypothesis when the p-value is small (less than 0.05). In this case 
there is just one hypothesis to test. When we increase the number of hypothesis we increase the chance to 
reject at least one of the hypotheses when it’s true. If α is the type I error for testing a single hypothesis, 
one has to make the p-values smaller so that the significance level of a whole family of hypotheses is 
(at most)α. When testing m hypotheses simultaneously the Italian statistician Bonferroni suggested that 
each hypothesis is tested at the level α/m. This advice had the drawback that extremely small individual 
p-values could be needed. An improved method was later suggested by Holm (1979, p. 65). The method 
can be described in the following way: If there are m simultaneous hypotheses to be tested, rank the 
p-values from the tests, from the smallest to the largest, p(1) <p(2) <… p(i)<…. Then claim simultaneous 
significance for all p-values such that
1
)
(
+
−
<
i
m
p i
α
. The method is illustrated in the following example.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT…
     RUN FASTER.
          RUN LONGER..
                RUN EASIER…
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Exercises in Statistical Inference  
with detailed solutions
126 
Hypothesis Testing
EX 102 The following table shows the durations of a sick leave for various age groups.
Duration (Weeks)
-1
1-4
4-
Total
Age 
-30
48
32
12
92
Group
30-50
35
26
40
101
(Years)
50-
12
24
52
88
Total
95
82
104
281
Is there an association between Age and Duration, and in such a case, which combinations can explain it?
 The total chi-square is 


-9
2
2
10
35
.
47
)
4
(
value
-
p
35
.
47
|
!
 

 
F
P
X
,  so the association is very strong.  
In order to search for an explanation to this we present a table with the measures Deviation / Cell Chi-Square  
(cf. Ch. 6.2.1).
Duration (Weeks)
-1
1-4
4-
Age 
-30
16.9 / 9.2
5.2 / 1.0
-22.1 / 14.3
Group
30-50
0.9 / 0.0
-3.5 / 0.4
2.6 / 0.2
(Years)
50-
-17.8 / 10.6
-1.7 / 0.1
19.4 /11.6
There are four Cell Chi-Square measures that are relatively large so we rank their corresponding p-values. The latter 
being obtained from a table showing X2 
(
)
2
2
)1(
 
and
X
P
p
>
=
χ
.
i
1
2
3
4
Cell Chi-Square, X2
14.3
11.6
10.6
9.2
p-value
0.0002
0.0007
0.0011
0.0024
1
3
3
05
.0



i
0.0056
0.0063
0.0071
0.0080
EX 102 (Continued) Here all p-values in the third row are smaller than the values in the fourth row. So, in the 
corresponding cells there are simultaneous significant deviations (at the 5% level). The conclusion is that there is an 
over-representation of members in the youngest age group with a short sick leave and also an over-representation of 
members in the oldest age group with a long sick leave.
Notice that if we want simultaneous significance at the 1% level, there are only three cells that meet the requirement. 
For the cell with X2  = 9.2 one gets the p-value e
.
0016
.0
1
4
9
01
.0
0024
.0
 


!
 
There are other ways to adjust for multiple comparisons. E.g. when testing for pairwise equality of three or 
more means, one may apply the methods of Scheffe’ or Tukey. These are used within the field of Analysis of 
Variance (ANOVA) and require that many assumptions are met. The so called Holm-Bonferroni method 
just described has the advantage that it can be used generally, although more specialized methods may 
be more efficient in certain situations.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
127 
Hypothesis Testing
There is no clear-cut answer to the question ‘How many, and which hypotheses shall be considered in the 
simultaneous inference?’. When testing for significant individual cell deviations in an R × C contingency 
table it is quite natural to set up R × C hypotheses. In other cases it may be harder to reach a decision 
on this issue.
6.5	
Randomized tests
In EX 69 it was noticed that with a discrete distribution, such as the Binomial, one can’t expect that the 
type I error is exactly α. However, this can be achieved by introducing a further random component 
to the RR. The methodology is illustrated in the following frequently cited example. (Observe that a 
randomized test is not to be confused with a randomization test.)
EX 103 Let ( )n
i
iY
1
= be iid with
)
(
~
λ
Poisson
Yi
. We want to test 
1.0
:
0
=
λ
H
against the one-sided alternative 
1.0
:
>
λ
a
H
with a type I error (α ) of 0.05 and with n = 10
As a test statistic we take ∑
iY  and reject H  
0  if ∑
> c
Yi
. This choice of RR seems obvious, but can also be 
shown to follow from the Neyman-Pearson lemma. Since ∑
)
(
~
λ
n
Poisson
Yi
(cf. (3) in Ch. 3.1) we get, since 



 

 
O
Q

(
)
∑
∑
∑
=
−
−
∞
+
=
−
=
=
>
=
c
y
c
y
i
y
e
e
y
H
c
Y
P
0
1
1
1
0
!
1
1
!
1
α
. From this we can construct the following table:
c
0
1
2
3
α
0.63
0.20
0.08
0.02
Since we can’t find a value of c which gives 
05
.0
 
D
 we reformulate the RR in the following way:
55
°¯
°®
­
 
!
¦
¦
3
\
SUREDELOLW
ZLWK
UHMHFW


,I

\
SUREDELOLW
ZLWK
UHMHFW


,I


+
<
+
<
L
L

Now, 




P
0.0613
1
0.02
P
3
1
3
05
.0
0
0



 

 


!
 
¦
¦
H
Y
P
H
Y
P
i
i
Thus 
5.0
506
.0
0613
.0
02
.0
05
.0
P
|
 

 
.
In practice this means that if ∑
> 3
iy
then 
0
H is rejected. But if ∑
= 3
iy
it is not clear if 
0
H should be rejected 
until you have tossed a coin where e.g. the outcome ‘head’ means rejection.
The above example with P = 0.5 has inspired a lot of jokers to make fun about theoretical statistical 
inference. An example: ‘Patient: – Am I going to die in cancer? Statistician: – I just got the result from 
the lab but wait, first I have to toss a coin to decide about your future’. Randomized tests are not to be 
used in practice for several apparent reasons. But, there is one important application for randomized tests, 
and that is when the power functions of several discretely distributed test statistics are to be compared. 
In that case it is important that the all the power curves start at the same level.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
128 
Hypothesis Testing
6.6	
Some tests for linear models
6.6.1	
The Gauss-Markov model
Let 
)
,
(
Y
X
be a bivariate random variable (cf. the properties (1)-(10) in Ch. 2.2.1). The conditional 
expectation (
)
x
X
Y
E
=
is called the regression function for the regression of Y on X and the conditional 
variance (
)
x
X
Y
V
=
 is called the residual variance. We will use the following notations for the population 
parameters:




constant.
if
linear,
if
n
correlatio
population
,
)
,
(
,
)
(
,
)
(
,
)
(
,
)
(
,
)
(
2
2
2
V
E
D
P
V
V
V
U
V
V
V
V
P
P
 
 


 
 
 
 
 
 
 
 
 
 
x
X
Y
V
x
x
X
Y
E
Y
X
Cov
Y
V
Y
V
X
V
Y
E
X
E
x
Y
Y
X
XY
XY
Y
Y
X
Y
X
An important special case is when 
)
,
(
Y
X
has a bivariate Normal distribution. In that case 




)
1(
,
,
 with 
,
2
2
2
2
U
V
V
P
E
P
D
V
V
V
V
U
E
E
D

 
 
 


 
 
 


 
 
Y
X
Y
X
XY
X
Y
x
X
Y
V
x
x
X
Y
E
  (29) 
In the Gauss-Markov model (Rao 1965, p179) the following assumptions are made:
(
)
n
i
i
i x
Y
1
= are independent and 
2
,
(
~
σ
β
α
ix
N
⋅
+
). This can alternatively be expressed
(
)n
i
i
i
i
i
E
E
x
Y
1
 
 where
,
=
+
⋅
+
=
β
α
are iid and 
)
,0
(
~
2
σ
N
 
(30)
The model in (30) is quite restrictive. It involves independency, linearity, constant variance and Normality. 
The model is not proper for follow-up, or panel data, where measurements are taken from several 
subjects that are followed in time. In the special case when 
0
=
α
the model is called regression through 
the origin (cf. EX 54).
Corresponding to the population moments above there are sample moments.







¦
¦
¦
¦
¦
¦
¦
¦

 

 

 

 
n
Y
X
Y
X
S
n
Y
Y
S
n
X
X
X
X
S
i
i
i
i
XY
i
i
YY
i
i
i
XX
,
,
)
(
2
2
2
2
2
,
sample correlation 
,
 where
,
XX
X
Y
X
XY
S
S
S
S
S
r
 
 
 estimators 
)
2
(
ˆ
,
ˆ
ˆ
,
ˆ
2

 


 
 
n
SSE
x
Y
S
S
XX
XY
V
E
D
E
, where 
XX
YY
i
i
S
S
x
Y
SSE
2
2
ˆ
)
ˆ
ˆ
(
E
E
D

 



 ¦
 (Cf. EX 83) is the ‘sum of square for errors’. From the assumptions 
in (30) it follows that 
)
2
(
)
2
(
~
ˆ
,
)
1
(
,
~
ˆ
),
,
(
~
ˆ
2
2
2
2
2
2


¸¸
¹
·
¨¨
©
§

n
n
S
x
n
N
S
N
XX
XX
F
V
V
V
D
D
V
E
E
. Furthermore, 
α
β
ˆ
 
and
 ˆ
are 
independent of 
2
ˆσ and 
XX
S
x
Cov
/
)ˆ
,ˆ
(
2
V
E
D

 
.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
129 
Hypothesis Testing
EX 104
a)	 Show how to test 
0
0
0
:
against 
 
:
β
β
β
β
≠
=
a
H
H
and 
0
0
0
:
against 
 
:
α
α
α
α
≠
=
a
H
H
.
b)	 The following data show the relation between Body weight in kg (x) and Body volume in liter (y) for twelve 
4-year old boys:
x
17.1
10.5
13.8
15.7
11.9
10.4
15.0
16.0
17.8
15.8
15.1
12.1
y
16.1
10.4
13.5
15.9
11.6
10.2
14.1
15.8
17.6
15.5
14.8
11.9
Test the following hypotheses 
0
:
against 
 0
:
 
and
 1
:
against 
 1
:
0
0
≠
=
≠
=
α
α
β
β
a
a
H
H
H
H
by considering 
the regression of Y on x. 
c)	 Repeat the analysis in b) but now by considering the regression of X on y.
a)	
~
)
2
(
)
2
(
~
)1,0
(
~
ˆ
/
/
ˆ
ˆ
)
2
(
)
2
(
~
ˆ
 
and
 )1,0
(
~
/
ˆ
2
2
2
2
0
2
0
2
2
2
2
0
−
−
−
=
−
⇒
−
−
−
n
n
N
S
S
n
n
N
S
XX
XX
XX
χ
σ
σ
σ
β
β
σ
β
β
χ
σ
σ
σ
β
β
)
2
( −
n
T
..
Similarly, 
)
2
(
~
)
2
(
1
ˆ
ˆ
2
2
0

¸¸
¹
·
¨¨
©
§



n
T
S
n
x
n
XX
V
D
D
.
b) ¦
¦
¦
¦
¦
 
 
 
 
 
51
.
2454
,
14
.
2400
,4.
167
,
26
.
2511
,2.
171
2
2
i
i
i
i
i
i
y
x
y
y
x
x
.
2700
.
66
12
/)
4.
167
)(
2.
171
(
51
.
2454
,
9100
.
64
12
/
)
4.
167
(
14
.
2400
,
8067
.
68
12
/
)
2.
171
(
26
.
2511
2
2
 

 
 

 
 

 
XY
YY
XX
S
S
S
1088
.0
)
2
12
(
8067
.
68
ˆ
9100
.
64
ˆ
,
21
.0
27
.
14
ˆ
95
.
13
ˆ
,
96
.0
8067
.
68
2700
.
66
ˆ
2
2
 



 
 


 
 
 
E
V
E
D
E
(
)
3.0
17
.0
2
01
.1
)
10
(
2
value
-
p
01
.1
8067
.
68
/
1088
.0
1
96
.0
   
,1
:
0
=
⋅
=
>
⋅
=
⇒
=
−
=
=
T
P
T
H
β  
No reason to reject 
0
H .
(
)
54
.0
27
.0
2
63
.0
)
10
(
2
value
-
p
63
.0
8067
.
68
)
27
.
14
(
12
1
1088
.0
0
21
.0
   
,0
:
2
0
=
⋅
=
>
⋅
=
⇒
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
−
=
=
T
P
T
H
α  
No reason to reject
0
H . We have an example of regression through the origin.
c) Now the regression function is (
)
y
y
Y
X
E
⋅
+
=
=
'
' β
α
.
(
)
(
)
.
64
.0
32
.0
2
48
.0
)
10
(
2
value
-
p
48
.0
91
.
64
/
1148
.0
1
02
.1
   
,1
:
0
=
⋅
=
>
⋅
=
⇒
=
−
=
=
T
P
T
H
β
	  
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
130 
Hypothesis Testing
EX 104 (Continued)




.
13
.0
065
.0
2
652
.1
)
10
(
2
value
-
p
652
.1
91
.
64
11
/
1148
.0
1
02
.1
,1
:
2
0
 

 
!

 

 


 
 
F
E
P
T
H
We can’t reject
0
H .
(
)
99
.0
495
.0
2
013
.0
)
10
(
2
value
-
p
013
.0
91
.
64
)
95
.
13
(
12
1
1148
.0
0
024
.0
  ,0
:
2
0
=
⋅
=
>
⋅
=
⇒
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
−
=
=
T
P
T
H
α  
No reason to reject j
0
H . Very strong reason for regression through the origin.
Comment to EX 104 Sometimes it isn’t crystal clear which of two variables that should be regarded 
as dependent. In such situations one may try to let both be dependent and check whether the two 
regression analyses give consistent results. Notice however that a regression relation is different from a 
mathematical relation. From the mathematical relation
x
y
⋅
+
=
β
α
one can solve for the inverse relation
y
y
x
⋅
+
=
⋅
+
−
=
'
'
/
1
/
β
α
β
β
α
. E.g. in EX 104 we don’t get 
but
,
22
.0
96
.0
/
21
.0
'

 

 
D
 
024
.0
'=
α
.
Several Y-observations at each x. Test of linearity.
Data is now 






 k
i
n
j
k
kj
n
j
j
k
i
n
j
i
ij
x
Y
x
Y
x
Y
1
1
1
1
1
1
,
,
,
1
 
 
 
 
 

.  The expressions for estimation and tests of parameters 
are the same as above. The data (
)2
1
1
1 ,
=
j
j x
Y
is interpreted as 
 

1
12
1
11
,
,
,
x
Y
x
Y
.  The difference is that we 
now can test whether there is one linear regression line through the data or not. 
Introduce the notations 
¦
¦
¦
¦
¦
 
 
 
 
 
 
 
 
k
i
i
k
i
i
i
k
i
i
k
i
i
i
i
n
j
ij
i
n
x
n
x
n
Y
n
Y
n
Y
Y
i
1
1
1
1
1
,
,
.  Then the parameter estimates can be 
computed as 





x
Y
n
x
n
x
n
n
x
n
Y
n
Y
x
n
i
i
i
i
i
i
i
i
i
i
i
i
i


 


 
¦
¦
¦
¦
¦
¦
¦
E
D
E
ˆ
ˆ
,
/
/
ˆ
2
2
. 
The hypothesis to test is
(
)
x
x
X
Y
E
H
⋅
+
=
=
β
α
:
0
. The test statistic for this is 


¦¦
¦
¦





 
)
/(
)
(
)
2
/(
)
ˆ
ˆ
(
2
2
k
n
Y
Y
k
x
Y
n
F
i
i
ij
i
i
i
E
D
.  The p-value for
0
H is (
)
OBS
i
F
k
n
k
F
P
>
−
−∑
)
,2
(

(31)
The test in Eq. (31) is illustrated in the following example.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
131 
Hypothesis Testing
EX 105 Test the following (artificial) data for linearity:
x
1
2
3
Y
3, 4, 5
1, 3
3, 4, 5
We first make the table:
ix
2
ix
in
i
ix
n
2
i
ix
n
Yij
iY
i
iY
n
i
i
i Y
x
n
1
1
3
3
3
3, 4, 5
4
12
12
2
4
2
4
8
1, 3
2
4
8
3
9
3
9
27
3, 4, 5
4
12
36
Total
8
16
38
28
56
From this we get 
5.3
8
28
ˆ
,0
8
/
)
10
(
38
8
/)
16
)(
28
(
56
ˆ
2
 
 
 


 
D
E
The value of the test statistic can then be computed from the following table:
L[ 
LQ 
LM
< 
L< 




Ö
Ö

L
L
L
[
<
Q
E
D 




¦


L
LM
<
<












  


 











  


 











  


 
7RWDO





The statistic is 
value
-
p
00
.5
)
3
8
/(
6
)
2
3
/(
6

 


 
F


076
.0
00
.5
)
5,1(
 
!
 
F
P
. The hypothesis of linearity 
can’t be rejected at the 5 % level. The p-value is however small and one should look for other alternatives than the 
straight line.
Regression towards the mean- how to ‘lie’ with regression analysis
When people with extreme values of the measurements, such as high blood pressure, are measured once 
more it is found that the mean of the extreme group is closer to the mean of the whole population. If 
people with extreme values are treated with some medicine the decrease may be interpreted as showing 
the effect of the treatment (a significant negative value of β). The problem is that the mean level may go 
down (significantly) even if people are not treated. This phenomenon, known as regression towards the 
mean, can be explained by measurement errors and natural biological variations (cf. Davis, p. 493). It is 
actually linked with the word “regression” used by F. Galton in a paper from 1885, who found that the 
height of children from very short or very tall parents move toward the average. This false pattern is 
more pronounced if we relate change with initial value. The following theoretical example is instructive 
if you want to make an experiment which proves that your ‘hocus pocus drug’ has a significant lowering 
effect on blood pressure, anxiety, cholesterol, body weight etc. The intention is of course that you shall 
use the knowledge to reveal others, not to use it for their own purposes.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
132 
Hypothesis Testing
EX 106 Regression of ‘Change’ on ‘Initial value’.
Introduce the following notations and assumptions:
=
1Y
Systolic Blood Pressure (SBP) at a point in time and 
=
2
Y
SBP at a time later. Put
1
2
Y
Y
D
−
=
. For simplicity it is 
assumed that
2
2
1
)
(
)
(
σ
=
=
Y
V
Y
V
. The correlation between 
2
2
1
12
)
,
(
V
U
Y
Y
Cov
 
We are interested in the relation between
D
Y
 
and
 
1
. Assume that (
)
D
Y ,
1
has a bivariate Normal distribution and 
consider the regression function (
)
1
1
1
y
y
Y
D
E
⋅
+
=
=
β
α
. From Eq. (29) we know that
)
(
)
,
(
1
1
Y
V
D
Y
Cov
=
β
and 
from Ch. 2.1 we get 
(
)
(
)
−
−
=
−
−
−
=
−
⋅
=
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
,
(
2
1
2
1
1
2
1
1
2
1
1
1
1
Y
E
Y
E
Y
Y
E
Y
E
Y
E
Y
E
Y
Y
Y
E
D
E
Y
E
D
Y
E
D
Y
Cov


)
1(
)
1(
)
(
)
,
(
)
(
)
(
12
12
2
2
12
2
1
2
1
2
1
2
1
U
E
U
V
V
U
V


 



 

 

 

Y
V
Y
Y
Cov
Y
E
Y
E
,  which in 
practice is negative.
The true regression line (
)
1
1
y
Y
D
E
=
will have a negative slope and it is thus likely that the estimated line also has a 
negative slope.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

Exercises in Statistical Inference  
with detailed solutions
133 
Hypothesis Testing
Multiple regression
The regression function is now    (
)
k
k
k
k
x
x
x
X
x
X
Y
E
β
β
α
+
+
+
=
=
=
…
…
1
1
1
1
,
,
. The assumptions about 
the variables( i x
Y
1,…
)
n
i
kx
1
,
= are analogous to those in Eq. (30). Under the latter assumptions the best 
estimators of the parameters are given by the OLS method (cf. Ch. 4.3.1). The estimators 
β
α
,
ˆ
 ,ˆ
1 …
k
βˆ
,
 
can easily be expressed in matrix form but this is beyond the scope of this book. The reader is advised to 
obtain the solutions by running some computer program, e.g. the procedures proc glm or proc reg in SAS.
Before considering the tests being of interest we make some comments about the variables X1 … Xk. The 
latter are called independent variables in contrast to Y which is the dependent variable. The independent 
variables may also be termed explanatory variables (often used by econometricians) or predictors. The 
interpretation of a single regression parameter 
iβ is that it shows how much the expectation of Y changes 
when 
ix is increased by one unit and all other independent variables are fixed. But, if the x-variables 
are inter-related, or more or less collinear, this is impossible. Collinearity may lead to biased parameter 
estimates with great variance.
One special form of independent variables is the so called dummy variable. Consider the following 
examples:
--
We want to study how Y = ‘Amount of savings’ depends on x = ‘Salary’ among men and women. 
Introduce the dummy 
for
 0
 
and
men 
for 
 1
=
=
z
z
women. The regression model can be written


¯
®
­
 

 



 




 
0
if
,
1
if
,
)
(
,
1
3
1
2
3
2
1
z
x
z
x
z
x
z
x
z
x
Y
E
E
D
E
E
E
D
E
E
E
D
. 
This is a comparison of two lines, one for men and one for women. Hypotheses of interest are 
if 
0
3 =
β
 (parallel lines), or if 
0
 
and
 0
2
3
=
=
β
β
 (identical lines). Here 
1
β is a separate salary- 
effect regardless of sex,
2
β is a separate sex- effect regardless of salary and 
3
β is a salary-effect 
connected to sex. The latter parameter measures the interaction effect. When analyzing data 
with this model in a computer the input data consists of values in 3 columns, Y, x, z. Then you 
have to specify the model. E.g. in SAS you write the lines proc glm; model y=x z x*z;
--
In the above example there was a comparison of two regression lines. When several regression 
lines are to be compared things are a bit more complicated. Assume that we want to study how 
Y = ‘Household expenditure’ depends on x = ‘Salary’ during the four seasons of the year. Since 
there are four seasons we introduce three dummies such that
Season
1z
2z
3z
Spring
0
0
0
Summer
1
0
0
Autumn
0
1
0
Winter
0
0
1
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
134 
Hypothesis Testing
The model can be written
(
)






+
+
+
+
+
+
+
=
+
+
+
+
=
 Winter
,
Autumn 
,
Summer
 ,
Spring
 ,
,
,
,
3
2
1
3
3
2
2
1
1
3
2
1
x
x
x
x
z
z
z
x
z
z
z
x
Y
E
β
β
α
β
β
α
β
β
α
β
α
β
β
β
β
α
This is a comparison of four parallel lines. If we allow the four lines to have different slopes 
we add 
3
6
2
5
1
4
xz
xz
xz
E
E
E


 to the latter regression function.
Above we have defined dummies for two sexes and four seasons. In general, with c categories 
we need c-1 dummies taking the values 1 and 0. In computer programs for estimating linear 
models there may be other choices of dummies. The definition of those specific dummies that 
have been used is seen in the beginning of the print-out. 
The first result of interest in a multiple regression study is the ANOVA (Analysis of Variance) table. 
This shows how the total variation of the Y –observations can be split up into two components. This is 
shown in the following table:
Variance source
Degrees of freedom
Sum of squares
Regression (Model)
k
SSE
SST
SSR
−
=
Error
1
−
−k
n
2
1
1
)
ˆ
ˆ
(
∑
∑
=
=








+
−
=
n
i
k
j
ij
j
i
x
Y
SSE
β
α
Corrected Total
1
−
n
(
)
∑
=
−
=
n
i
i
Y
Y
SST
1
2
From the table we get an unbiased estimator of 
2
σ ,
)1
(
)1
(
~
1
ˆ
2
2
2
−
−
−
−
−
−
=
k
n
k
n
k
n
SSE
χ
σ
σ
.
We also get a measure of the fit of the linear model, the Coefficient of Determination 
SST
SSR
R
p
x
x
Y
=
2
1K
, 
taking values in the interval [0, 1]. Values close to 1 indicates that the explanatory ability of the model 
is god. (
2
R is in fact the square of the Multiple correlation coefficient which is the correlation between Y 
and 
¦

ij
j X
E
D
ˆ
ˆ0
.) 
2
R can never become smaller when more x- variables are included in the regression 
model. As a measure of the gain in explanatory ability by including xp+1 beyond x1… xp one may use 
2
2
2
1
1
1
1
1
p
p
p
x
x
Y
x
x
Y
x
x
Y
R
R
R
K
K
K
−
−
+
, i.e. the actual increase in relation to the maximal possible increase.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
135 
Hypothesis Testing
Four classes of hypothesis to be tested
1)	
j
H
j
 ,0
 
All
 :
0
=
=
β
1…p. Test statistic is 
(
)
OBS
T
k
n
k
F
P
k
n
SSE
k
SSR
T
>
−
−
=
−
−
=
)1
,
(
value
-
p .)1
/(
/
.
This should be the first hypothesis to test and if it isn’t rejected there is no reason to continue, 
but instead try to search for better explanatory variables.
2)	
.0
 
Some
 :
0
=
j
H
β
 Test statistic is 
)
ˆ
(ˆ
ˆ
j
j
V
T
β
β
=
. p-value =
(
)
OBS
T
k
n
T
P
>
−
−
⋅
)1
(
2
.
Here 
)
ˆ
(ˆ
 
and
 
ˆ
j
j
V β
β
are found from the computer out-print under the names ‘Estimate’ and 
‘Std error of estimate’. Notice that since 
)
,1(
)
(
2
f
F
f
T
=
(cf. (11) and (12) in Ch. 3.1) the p-value 
can also be obtained from (
)
2
)1
,1(
OBS
T
k
n
F
P
>
−
−
.
In this case it is perhaps more instructive to place a 95 % CI under each estimated
j
βˆ . The 
latter is obtained from 
)
ˆ
(ˆ
ˆ
j
j
V
C
β
β ±
where C is determined from (
)
025
.0
)1
(
=
>
−
−
C
k
n
T
P
.
3.	
for 
 0
 
All
 :
0
j
H
j
=
=
β
1…
.
'
k
k <
Test 
statistic 
is
(
)
)1
/(
)'
/(
'
−
−
−
−
=
k
n
SSE
k
k
SSE
SSE
T
. 
Here 
SSE is 
the sum of square for ‘Error’ in the full model with k regression coefficients and 
'
SSE  
is corresponding sum of squares in the reduced model with 
'k
k −
 regression coefficients. 
(
)
OBS
T
k
n
k
k
F
P
>
−
−
−
=
)1
,'
(
value
-
p
.
This test is perhaps the most useful one. It enables us to see whether the model with regression 
function 
x
β
α
+
+
1
1
…
k
k x
β
+
+
'
'
…
k
k x
β
+
 can be replaced by the regression function 
k
k
x
β
α
+
+
+
+
1
'
1
'
…
k
k x
β
+
. The use of this test is illustrated in the following examples.
4.	 Tests about linear structures of the regression coefficients. Some examples are the following:
(
)
x
x
X
Y
E
H
0
0
0 :
β
α +
=
=
. Here, 
x 
and
 
 ,
0
0 β
α
 have fixed given values. E.g. 
0
0
 
and
 
β
α
 are the 
intercept and slope that has been observed during a long time for a production process and 
one wants to test whether a new process gives the same regression relation at x.
Test statistic is 
¸¸
¹
·
¨¨
©
§





 
XX
S
x
x
n
x
x
T
2
2
0
0
)
(
1
ˆ
)
(
)
ˆ
ˆ
(
V
E
D
E
D
 with p-value
(
)
OBS
T
n
T
P
>
−
=
)
2
(
.
It is instructive to derive the above expression. Obviously, 
x
x
E
D
E
D


for 
unbiased
is
ˆ
ˆ
. 
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
136 
Hypothesis Testing

 >
@
 
 
XX
XX
XX
S
x
x
S
x
S
x
n
xCov
V
x
V
x
V
2
2
2
2
2
2
2
1
)ˆ
,ˆ
(
2
ˆ
ˆ
2.1
Ch.
in
(2)
Eq.
Cf.
ˆ
ˆ
V
V
V
E
D
E
D
E
D


¸¸
¹
·
¨¨
©
§

 


 
 

=  
¸¸
¹
·
¨¨
©
§


 
XX
S
x
x
n
2
2
)
(
1
V
.  Since 
β
α
ˆ
 
and
 ˆ
 each has normal distributions it follows that
(
)
)1,0
(
~
ˆ
ˆ
)
(
)
ˆ
ˆ
(
0
0
N
x
V
x
x
β
α
β
α
β
α
+
+
−
+
. Now, dividing numerator and denominator in the expression for T 
above by 
)
ˆ
ˆ
(
x
V
β
α +
yields a statistic that is distributed as 
)
2
/(
)
2
(
)1,0
(
2
−
−
n
n
N
χ
 ~
)
2
( −
n
T
.
An alternative to testing is to construct a CI for the true regression line at x:
¸¸
¹
·
¨¨
©
§


r

XX
S
x
x
n
C
x
2
2
)
(
1
ˆ
ˆ
ˆ
V
E
D
,  
where 
C 
is determined from (
)
2
/
)
2
(
α
=
>
−
C
n
T
P
to get 
a 
)%
1(
100
D

 CI. E.g. if we want a 90% CI when n =12, then Tables over the T-distribution shows that 


05
.0
2
/
10
.0
812
.1
)
10
(
 
 
!
T
P
, so C = 1.812.  so C = 1.812. 
For several x–variables the computations are heavy and will not be shown here. Results can be obtained 
from most computer programs. E.g. in SAS the codes proc glm; model y=x1 x2 x3/clm p; will give 
you 95% CIs for the expected means (
)
3
3
2
2
1
1
3
2
1
,
,
x
x
x
x
x
x
Y
E
β
β
β
α
+
+
+
=
, together with predicted 
(estimated) values.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Exercises in Statistical Inference  
with detailed solutions
137 
Hypothesis Testing
Another type of linear structure among regression parameters is the following
)
(
:
3
2
0
β
β
β
=
=
H
in the regression function (
)
3
3
2
2
1
1
x
x
x
x
X
Y
E
β
β
β
α
+
+
+
=
=
.
This has been termed a test of aggregation, in this case aggregation of the variables
3
2
 
and
 
x
x
. A typical 
example is when Y is ‘Prices of clothing’, 
2x is ‘Price of leather’ and 
3x is ‘Price of textile’. If 
0
H is not 
rejected this means that the effects of prices of leather and can’t be separated. The simplest way to 
perform the test is to run two regression analyses, one with 
3
2
1
,
,
x
x
x
 as independent variables giving 
rise to SSE , and one with 
)
(,
3
2
1
x
x
x
+
as independent variables giving rise to SSE’. The test statistic is 
)
4
/(
)
2
3
/(
)
'
(
−
−
−
=
n
SSE
SSE
SSE
T
with p-value
(
)
OBS
T
n
F
P
>
−
=
)
4
,1(
.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
138 
Hypothesis Testing
EX 107 Data below shows a sample of 12 persons employed in a company where Y = ‘Weekly earnings (in 1000 SEK)’ 
of various ages (year) and sex (0 = Woman, 1 = Man). Assume that the Y–values are normally distributed.
Weekly earnings
5
6
8
8
6
10
8
11
9
11
13
13
Age (x)
20
30
35
35
40
40
45
45
50
55
55
60
Sex (z)
0
0
0
1
0
1
0
1
1
1
1
1
a)	 Test whether the mean salary differ between men and women. (Use an ordinary T-test.)
b)	 Study if mean salary increases with age by using the model (
)
x
x
Y
E
β
α +
=
. From the out-print you get the 
following results:
ANOVA table
df
SS
Parameter
Estimate
Std Error of Estimate
Regression
1
59.00
β
0.20
0.036
Error
10
19.00
Corrected Total
11
78.00
Test 
0
:
0
=
β
H
and compute the Coefficient of Determination. (Std Error of Estimate in the table above is simply
)ˆ
(ˆ β
V
).
c)	 Find a proper regression model that describes how mean salary depends on both age and sex.
Model: (
)
z
x
z
x
Y
E
2
1
,
β
β
α
+
+
=
ANOVA table
df
SS
Parameter
Estimate
Std Error of Estimate
Regression
2
66.2396
1
β
0.14
0.039
Error
9
11.7604
2
β
2.07
0.879
Corrected Total
11
78.00
Model: (
)
z
x
z
x
z
x
Y
E
⋅
+
+
+
=
3
2
1
,
β
β
β
α
ANOVA table
df
SS
Parameter
Estimate
Std Error of Estimate
Regression
3
67.1659
1
β
0.10
0.060
Error
8
10.8341
2
β
-0.61
3.358
Corrected Total
11
78.00
3
β
0.066
0.080
d)	 Comment on the following statement: ’It’s true that men earn more than women, but this is due to the fact that 
men at the company tend to be older than women and salary increases with age’.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
139 
Hypothesis Testing
EX 107 (Continued)
a)	 Women (z = 0): 
80
.1
)1
5
(
5
/
)
33
(
225
,
60
.6
5
33
2
2
 


 
 
 
W
W
s
y
Men (z = 1): 
5714
.3
)1
7
(
7
/
)
75
(
825
,
714
.
10
7
75
2
2
 


 
 
 
M
M
s
y
2
2
0 :
M
W
H
V
V
 
,


26
.0
984
.1
)
4,6
(
value
-
p
984
.1
80
.1
5714
.3
 
!
 

 
 
F
P
F
. No reason to reject 
0
H .
The pooled variance estimate is 
863
.2
6
4
5714
.3
6
80
.1
4
ˆ 2
 




 
V
.
M
W
H
P
P
 
:
0
,


002
.0
001
.0
2
153
.4
)
10
(
2
value
-
p
153
.4
7
1
5
1
863
.2
60
.6
714
.
10
 

 
!

 

 
¸
¹
·
¨
©
§


 
T
P
T
.
Reject 
0
H , women have significantly lower earnings.
b)	
0
:
0
 
E
H
,


0002
.0
0001
.0
2
56
.5
)
10
(
2
value
-
p
56
.5
036
.0
20
.0
 

 
!

 

 
 
T
P
T
. Reject  
Reject
0
H , there is a strong linear relation between age and earnings.
The Coefficient of Determination is 
%)
(76
756
.0
00
.
78
00
.
59
2
 
 
x
Y
R
.
c)	 Model: (
)
z
x
z
x
Y
E
2
1
,
β
β
α
+
+
=


006
.0
59
.3
)
9
(
2
value
-
p
59
.3
039
.0
14
.0
,0
:
1
0
 
!

 

 
 
 
T
P
T
H
E
. Reject
0
H .


043
.0
35
.2
)
9
(
2
value
-
p
35
.2
879
.0
07
.2
,0
:
2
0
 
!

 

 
 
 
T
P
T
H
E
.  Reject
0
H .
Both x and z has a significant effect on salary. 
%)
(85
849
.0
00
.
78
2396
.
66
2
,
 
 
z
x
Y
R
.
Model: 

z
x
z
x
xz
z
x
Y
E




 
3
2
1
,
,
E
E
E
D


13
.0
70
.1
)
8
(
2
value
-
p
70
.1
060
.0
10
.0
,0
:
1
0
 
!

 

 
 
 
T
P
T
H
E
.  There is no significant effect of x.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
140 
Hypothesis Testing










YDOXH

S










 
!

 


 

 
 
7
3
7
+
E
³]










YDOXH

S










 
!

 

 
 
 
7
3
7
+
E
³
]
[  
In this case 











 
 
[]
]
[
<
5
 The relative increase of 
2
R  by introducing 
z
x ⋅
 is only 
%
8
849
.0
1
849
.0
861
.0
=
−
−
. There is no point in assuming that there are two lines with different slopes 
d)	 It’s true that earnings increase with age, but it’s also true that there is a separate sex effect on the earnings that 
has nothing to do with the age effect.
6.6.2	
Random Coefficient models
When repeated measurements are obtained in time from a sample of persons or companies, the Gauss-
Markov model in the preceding chapter can be very poor. This type of data is called panel data by 
econometricians and longitudinal data or follow-up data by biometricians. The typical pattern in this 
data is that measurements from each chosen sampling unit have its own development. When the latter 
develop along straight lines, each line has its own intercept and possibly also its own slope. If the Gauss- 
Markov model is used and one single model is fitted to the data, the conclusions can be totally wrong. 
If each individual slope is positive, the line fitted by the Gauss-Markov model can be negative and vice 
versa. This is nicely illustrated in Diggle et al, p. 1. Models where slopes and intercepts are allowed to 
be random are called Random Coefficient models. A general exposition of these models is beyond the 
scope of this book. Here we only illustrate the inference when intercepts are random and slopes are 
fixed, Error Components Regression (ECR) models, and when both intercepts and slopes vary randomly, 
Random Coefficient Regression (RCR) models.
Assume that measurements are made on n persons at the same times i = 1…, t. (The latter assumption 
will simplify the computations considerably.) The value obtained of the j:th person,
n
j
,...,
1
=
, at time i 
is denoted 
ij
Y .  The two models are
ij
i
j
ij
E
x
A
Y
ECR



 
E
:
ij
i
j
j
ij
E
x
B
A
Y
RCR



 
:
The assumptions are: 
AB
j
j
B
j
A
j
ij
B
A
Cov
N
B
N
A
N
E
V
V
E
V
D
V
 
)
,
(
),
,
(
~
),
,
(
~
),
,0
(
~
2
2
2
.  All other components 
are uncorrelated. It seems hard to motivate all those Normality assumptions, especially that slopes have 
Normal distributions, but the assumptions are needed to reach any results in the inference. In both 
models  
i
ij
x
Y
E


 
E
D
.  In the ECR model  
2
2
V
V

 
A
ij
Y
V
 and 
 
)
,
(
' j
i
ij Y
Y
Cov
 [cf. Ch. 2.1 Properties of 
)
,
(
2
1 y
y
p
(7)] 

 











)
)(
(
)
)(
(
'
'
'
i
i
j
i
i
j
ij
i
j
x
x
E
x
A
E
x
A
E
E
D
E
D
E
E










j
i
ij
ij
i
ij
j
j
i
i
i
i
i
j
j
i
j
i
j
j
E
E
E
x
E
A
E
x
x
x
x
A
E
A
x
A
A
E
'
'
'
'
2
'
'
2
E
E
E
E
E
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
141 
Hypothesis Testing
'
2
'
2
i
i
i
i
x
x
x
x
E
DE
DE
D



 =[Many terms cancel each other out]= (
)
2
2
2
)
(
A
j
j
A
V
A
E
σ
α
=
=
−
. Thus, the 
correlation between 
j
i
ij
Y
Y
'
and
 is 
)
/(
)
(
)
(
/)
,
(
2
2
2
'
'
V
V
V

 
A
A
j
i
ij
j
i
ij
Y
V
Y
V
Y
Y
Cov
.  So, there is a constant 
correlation between measurements within each person. In the RCR model similar calculations yields 
2
''
'
2
'
2
2
2
)
(
)
,
(
and
2
)
(
B
i
i
AB
i
i
A
j
i
ij
B
i
AB
i
A
ij
x
x
x
x
Y
Y
Cov
x
x
Y
V
V
V
V
V
V
V



 


 
.  From these expressions it 
is seen that a simple test to decide whether data follows an ECR- or a RCR model is to plot estimates of 
)
( ij
Y
V
 against
ix . If the latter relationship is constant, then we have an ECR model. On the other hand, 
if the relationship shows a quadratic pattern we have a RCR model. A formal statistical test that enables 
us to choose between the two models is presented below.
The following statistics will be needed. 


¦
¦
¦
¦

 
 
 
2
2
1
,
1
,
1
j
j
YY
ij
j
i
Y
n
Y
B
Y
t
Y
x
t
x




2
2
2
2
1
,
1
,
1
¸¸
¹
·
¨¨
©
§

 
¸¸
¹
·
¨¨
©
§

 

 
¦
¦
¦
¦
¦
¦
¦
i
ij
i
ij
Y
Y
i
ij
i
j
i
xY
i
i
xx
Y
t
Y
S
Y
x
t
Y
x
S
x
t
x
S
j
j
j


xx
j
Y
Y
i
i
j
j
ij
j
j
j
j
j
j
xx
xY
j
S
S
x
Y
SSE
n
n
x
Y
S
S
j
j
j
2
2
)
ˆ
(
)
ˆ
ˆ
(
,
ˆ
1
ˆ
,
ˆ
1
ˆ
,
ˆ
ˆ
,
ˆ
E
E
D
D
D
E
E
E
D
E

 


 
 
 

 
 
¦
¦
¦
,
(This last relation is proved in EX 81.) 


¦
¦
¦

 
 
,
ˆ
1
ˆ
,
2
2
j
j
AA
j
n
S
SSE
SSE
D
D


¦
¦

 
2
2
ˆ
1
ˆ
j
j
BB
n
S
E
E
.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Exercises in Statistical Inference  
with detailed solutions
142 
Hypothesis Testing
The hypotheses to test are 
0
:
against 
 0
:
:
against 
 
:
2
2
0
0
>
=
⇒
B
a
B
a
H
H
RCR
H
ECR
H
σ
σ
. The test 
statistic for this is 
)
2
(
/
)1
/(


 
t
n
SSE
n
S
S
T
BB
xx
 with p-value
(
)
OBS
T
t
n
n
F
P
>
−
−
=
)
2
(
,1
(
. It is shown in Petzold & 
Jonsson 2003, p6 that this test statistic is identical with the test statistic 
1
F in Hsiao 2003, p15. For the 
more general model with k regression coefficients the reader is referred to the latter citations. Since 
computations can be quite heavy, the analysis with these types of models are facilitated by utilizing 
soft-ware, e.g. proc mixed in SAS.
Depending on the outcome of the latter test we go further.
ECR model: 
¸¸
¹
·
¨¨
©
§


 
 


 



 
)
1
(
ˆ
ˆ
1
)ˆ
(ˆ
,
ˆ
)ˆ
(ˆ
,
ˆ
1
ˆ
,
1
)1
(
ˆ
2
2
2
2
2
2
2
xx
A
xx
YY
A
BB
xx
S
x
t
n
V
nS
V
t
n
B
t
n
S
S
SSE
V
V
D
V
E
V
V
V
0
0
0
:
against 
 
:
β
β
β
β
≠
=
a
H
H
is tested by 
)ˆ
(ˆ
ˆ
0
β
β
β
V
T
−
=
with p-value
(
)
OBS
T
t
n
T
P
>
−
−
⋅
=
)1
)1
(
(
2
 .
RCR model: 
¸¸
¹
·
¨¨
©
§

 


 
¸¸
¹
·
¨¨
©
§



 

 
xx
B
xx
BB
B
xx
AA
A
S
n
V
S
n
S
S
x
t
n
S
t
n
SSE
2
2
2
2
2
2
2
2
ˆ
ˆ
1
)ˆ
(ˆ
,
ˆ
1
ˆ
,
1
ˆ
1
ˆ
,)
2
(
ˆ
V
V
E
V
V
V
V
V
,
)ˆ
(ˆ α
V
is the same as for the ECR model.
0
0
0
:
against 
 
:
β
β
β
β
≠
=
a
H
H
is tested by 
)ˆ
(ˆ
ˆ
0
β
β
β
V
T
−
=
with p-value
(
)
OBS
T
n
T
P
>
−
⋅
=
)1
(
2
.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
143 
Hypothesis Testing
EX 108
The table below shows the concentration of HbA1c (Glycosylated hemoglobin) measured at three points in time 
(
3,2,1
=
ix
months) on 18 patients with diabetes. The purpose of the study was to see whether it is possible to 
reduce the HbA1c level among patients by dietary advice.
j
Yij
j
Y
j
βˆ
j
αˆ
j
SSE
1
6.4 6.3 7.6
6.77
0.60
5.57
0.327
2
9.1 8.5 8.2
8.60
-0.45
9.50
0.015
3
7.6 8.2 6.8
7.53
-0.40
8.33
0.667
4
7.3 7.3 7.0
7.20
-0.15
7.50
0.015
5
9.6 9.7 8.7
9.33
-0.45
10.23
0.202
6
9.3 8.8 8.5
8.87
-0.40
9.67
0.007
7
8.3 7.5 7.8
7.87
-0.25
8.37
0.202
8
8.1 7.9 7.3
7.77
-0.40
8.57
0.027
9
8.6 7.4 7.9
7.97
-0.35
8.67
0.482
10
8.2 8.1 7.5
7.93
-0.35
8.63
0.042
11
7.4 7.0 6.7
7.03
-0.35
7.73
0.002
12
6.8 6.7 6.5
6.67
-0.15
6.97
0.002
13
8.4 8.8 7.9
8.37
-0.25
8.87
0.282
14
9.2 8.9 8.8
8.97
-0.20
9.37
0.007
15
7.9 8.2 7.4
7.83
-0.25
8.33
0.202
16
7.2 6.8 6.4
6.80
-0.40
7.60
0.000
17
8.0 7.6 7.0
7.53
-0.50
8.53
0.007
18
10.2 11.2 8.9
10.10
-0.65
11.40
1.815
Total
-5.35
153.83
4.298
127
.1
,1.
28
,
546
.8
83
.
153
18
1
ˆ
,
297
.0
)
35
.5
(
18
1
ˆ
,
1805
.
15
2,
,2
 
 
 

 

 


 
 
 
 
BB
AA
YY
xx
S
S
B
S
x
D
E
RCR
H
ECR
H
a :
against 
 
:
0
 is tested by 


88
.0
55
.0
)
18
,
17
(
value
-
p
55
.0
)
2
3
(
18
/
298
.4
)1
18
/(
127
.1
2
 
!
 

 



 
F
P
T
.  No reason to reject the ECR model.
For the ECR model, 
187
.0
1
)1
3
(
18
127
.1
2
298
.4
ˆ 2
 




 
V
,  so 
0052
.0
2
18
187
.0
)ˆ
(ˆ
 

 
E
V
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
144 
Hypothesis Testing
0
:
against 
 0
:
0
≠
=
β
β
a
H
H
is tested by 


0001
.0
12
.4
)1
)1
3
(
18
(
2
value
-
p
12
.4
0052
.0
297
.0
 
!



 


 

 
T
P
T
.  This means that 
0
H is 
strongly rejected.
The conclusion is that there is a significant reduction of the HbA1c level and this reduction is similar to all patients 
with a mean rate of about 0.3 units per month.
6.7	
Final words
Statistics is not an exact science in the sense that there are clear-cut solutions to every problem. When 
analyzing linear models you find two opposite schools. The ‘significance fundamentalists’ argues that all 
non-significant parameters must be deleted from the model. The argument is that unnecessary parameters 
‘steal’ degrees of freedom so that other parameters may not clear the 5% p-value threshold. On the other 
hand there are ‘significance liberals’ who retain all parameters in the model that they find interesting. 
The author’s personal view is close to that of a ‘significant fundamentalist’. 
The square root of the estimated variance of a statistic is called Standard Error of Estimate. This is an 
old fashion name, but is now common in computer printouts.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Exercises in Statistical Inference  
with detailed solutions
145 
Hypothesis Testing
Recall that there are two different ways to test for equality rates in a Poisson process. One is based on 
interval data, intervals between events (EX 80), and one based on counts, or frequency data (EX 84).
In this book you find several examples of tests of linearity in regression models. This is an area that has 
been overlooked. Many examples where significance can’t be established may be due to the fact that a 
linear model is used where a non-linear model would be more adequate. Closeness to linearity is often 
said to be measured by
2
R , the coefficient of determination. However, the F-test in (31) Ch. 6.6 is much 
more efficient in detecting deviation from linearity. In EX 146 where linearity was rejected by the F-test, 
one obtains
969
.0
2 =
R
, which is large.
As you have noticed, tests of hypotheses in linear models require heavy computations. It is therefore 
desirable that you supply reliable statistical software to your computer. This is of special importance 
when dealing with random coefficient models (Ch. 6.2.2) where more or less sophisticated software are 
available under the name of ‘mixed models’.
When communicating results from a statistical analysis you should avoid expressions like “
a
H
H against 
 
0
”. 
(This is for internal use among statisticians.) Instead use formulations like “The new method gives 
significantly lower values than the old method (p<0.01, two-sided Sign test), just to take an example.
Supplementary Exercises, Ch. 6
EX 109 Gregor Mendel is said to be the founder of the science of genetics. He performed a large number of 
experiments to test his theories and much of these data are still available. In one famous experiment he cross-
pollinated smooth yellow pea plants with wrinkly green peas with the following result:
(Shape, Color)
(Round, Yellow)
(Wrinkly, Yellow)
(Round, Green)
(Wrinkly, Green)
Theoretical 
proportion
9/16
3/16
3/16
1/16
Observed 
frequency
315
108
101
32
a)	 Make a 2 x 2 table of the observed frequencies in terms of the factors Color and Shape.
b)	 Test whether the observed frequencies are in accordance with Mendel’s theory.
EX 110 The number of white blood cells per cubic millimeters is known to vary according to a Poisson distribution.  
10 blood samples from the same person showed the following number of white blood cells: 81 38 63 63 50 63 69 50 
38 31.
a)	 Compare the sample mean and variance. Conclusion?
b)	 Use the Chi-square principle to test whether the observations come from the same Poisson distribution.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
146 
Hypothesis Testing
EX 111 The number of persons being on sick leave per day was recorded at a department, with the following result:
Number on sick leave
0
1
2
3
4
5-
Frequency
12
10
6
0
2
0
Determine whether the Poisson distribution is an adequate model for the outcome.
EX 112 The Normal distribution is often taken for granted without giving any support for this assumption. Show how 
the Chi-square principle can be used in order to test whether the following (ordered) data can be assumed to be 
Normally distributed:
23 23 24 27 29 31 32 33 35 36 36 37 40 42 43 43 44 45 48 48 54 54 56 57 57 58 58 58 58 59 61 61 62 63 64 65 66 68 68 
70 73 74 75 77 81 87 89 93 97 
[Hint: Use some classification, e.g. -39, 40-60, 61-80, 81-.]
EX 113 Several independent Binomial samples.
In EX 77 and EX 83 the proportion in two independent samples were compared. Consider now ( )k
i
iY
1
=  where 
)
,
(
~
i
i
i
p
n
Binomial
Y
and   
)
(
:
1
0
p
p
p
H
k =
=
=…
.
Under the null hypothesis 
∑
∑
∑
∑
=
=
i
i
i
i
i
n
Y
n
p
n
p
ˆ
ˆ
is BLUE (cf. EX 46). The statistic for testing 
0
H is  
(Rao 1965, p. 333) 
∑
−
−
−
=
0
2
2
under 
 )1
(
~
)ˆ
ˆ
(
)ˆ
1(ˆ
1
H
k
p
p
n
p
p
T
i
i
χ
.
Consider the following Norwegian data:
Season
Spring 
Summer
Autumn
Winter
Number of born boys
9251
7967
7327
7662
Number of births
17866
15408
14251
14885
Test whether the proportion born boys is the same for all seasons.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
147 
Hypothesis Testing
EX 114 Test for independency between (A, Non-A) and (B, Non-B) in the following three contingency tables  
(fictive data).
Group 1
Group 2
B
Non-B
Total
B
Non-B
Total
A
10
40
50
A
60
40
100
Non-A
20
80
100
Non-A
30
20
50
Total
30
120
150
Total
90
60
150
Group 1+2
B
Non-B
Total
A
70
80
150
Non-A
50
100
150
Total
120
180
300
Conclusions?
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Exercises in Statistical Inference  
with detailed solutions
148 
Hypothesis Testing
EX 115 In a sample of 100 couples, the husbands and wives were asked about their opinions about a politician. The 
result was:
Husband
Positive
Negative
Wife
Positive
6
10
Negative
24
60
a)	 Estimate the proportion positive husbands and wives, respectively. Determine whether the difference between 
the proportions is significant by using the Chi-square and the LR principles.
b)	 Are the opinions of husbands and wives independent?
EX 116 In a medical rehabilitation project patients with different degree of estimated working capacity (low, medium, 
high) received different types of training (physical, activation, education). The following frequencies were obtained:
Working
capacity
Low
Medium
High
Total
Physical
119
80
21
220
Type of training
Activation
363
50
34
447
Education
23
12
4
39
Total
505
142
59
706
Is there an association between estimated working capacity and the type of training? If so, investigate which 
combinations are over/under-represented.
EX 117 During a severe epidemic 40 % of the population were on sick leave. A telephone survey to five randomly 
chosen institutions at a University gave the following result:
Institution no
1
2
3
4
5
Number on sick leave
4
10
8
2
6
Total number of employees
10
42
25
11
12
Test whether University employees are on sick leave to the same extent as the rest of the population.
EX 118 In a factory there were 10 accidents during 2 weeks. After this equipment were renewed and during the 
following 3 weeks there were 5 accidents. Did the measures have a significant effect on the rate of accidents?
[Hint: Use the conditional Poisson property and compute p-values from the Binomial distribution.]
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
149 
Hypothesis Testing
EX 119 The conditional Poisson property can be generalized to k independent Poisson processes of rates λ1…
k
λ :
(
y
t
Y
P
,
)
(
1
1
1
=
…
)
p
y
y
n
n
t
Y
y
t
Y
y
k
i
i
k
k
k
!
!
!
)
(
)
(
,
1
1
1
=
=
=
∑
K
…
k
i
t
t
p
p
i
i
i
i
i
y
k
k
...
1
 ,
 
 where
,
=
= ∑λ
λ
. The 
hypothesis 
:
1
0 λ =
H
…
)
( λ
λ
=
=
k
is thus equivalent with 
1
 ,
:
0
i
t
t
p
H
i
i
i
=
= ∑
…k. It follows that the latter 
hypothesis can be tested by 
(
)
(
)
∑
∑
∑
∑
−
=
−
=
k
n
k
n
Y
t
t
n
t
t
n
Y
X
i
i
i
i
i
i
/
/
)
/
(
)
/
(
2
2
2
for a Poisson distribution  
(cf. Stuart et al 1999, p. 393).
Test whether the data 19 16 20 25 are observations on the same Poisson distributed variable.
EX 120 Let  
10
1
 
 
n
i
iY
 be iid with 
)
(
~
λ
l
Exponentia
Yi
. Derive the RR for testing  
	  
0
0
0
:
against 
 
:
λ
λ
λ
λ
≠
=
a
H
H
 
at the 5% level. Consider the case n = 10.
[Hint: Use the Neyman-Pearson Lemma, mentioned in Ch. 6.2.3 and the property (5) in Ch. 2.2.2]
EX 121 Let (
)
( ) Y
X
n
i
i
n
i
i
Y
X
1
1
 
and
 
=
=
 be two independent sets of iid variables with Exponential distributions with 
parameters 
Y
X
λ
λ
 
and
 
, respectively. Show how the LR principle can be used to test 
)
(
:
0
λ
λ
λ
=
=
Y
X
H
. Perform 
the test when 
¦
¦
 
 
 
 
40
,
60
,
20
,
40
i
Y
i
X
y
n
x
n
.
EX 122 Let ( )n
i
iY
1
= be iid variables where 
)
(
~
p
Geometric
Yi
.
a)	 Show how to test 
2
/
1
:
against 
 2
/
1
:
0
≠
=
p
H
p
H
a
by means of the LR principle.
b)	 Perform the test when 
¦
 
 
80
and
50
iy
n
.
c)	 Give examples where this test may be of interest.
EX 123 16 persons participated in a weight loss program. The body weight (in kg) of each person was measured 
initially (X) and after six months (Y). The following values were obtained of the difference D=X – Y (in increasing 
order):
-1.8, -1.7, -1.4, 0.3, 0.6, 1.6, 1.7, 1.9, 2.3, 2.4, 2.8, 3.7, 4.5, 5.8, 6.3, 6.8
Let 
)
(D
E
D =
µ
and test
0
:
against 
 0
:
0
≠
=
D
a
D
H
H
µ
µ
a)	 By assuming that differences are normally distributed.
b)	 By performing an exact Sign test based on the Binomial distribution.
c)	 By using a normality approximation of the test in b).
[Hint: In the last case the approximation can be improved by letting
(
)






+
−
<
≈
≤
)
(
2
/
1
)
(
Y
V
Y
E
y
Z
P
y
Y
P
and (
)






−
−
<
≈
<
)
(
2
/
1
)
(
Y
V
Y
E
y
Z
P
y
Y
P
]
…
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
150 
Hypothesis Testing
EX 124 The same products were classified as Bad or God by Municipal- and State authorizes. The result was
State
Bad
God
Municipal
Bad
20
10
God
20
50
a)	 Test whether the classifications agree by testing equality between marginal frequencies.  
[Proportion Bad will suffice.]
b)	 Test whether the classifications from the two authorities are independent.
EX 125 Yeast cells were counted in a hemacytometer with the following result:
Number of yeast cells per square
0
1
2
3
4
5
6
Frequency
103
143
98
42
8
4
2
Check whether the frequencies are in accordance with the Poisson distribution
EX 126 At an industry men are working in three shifts: Morning, Day and Night. From each shift a random sample 
of 200 products were chosen and the number of defective products was recorded with the following result: 12 for 
Morning, 10 for Day and 23 for Night.
Use a Chi-square test to draw conclusions from the data.
[Hint: Construct a 2 x 3 table and test for independency.]
EX 127 Use the test for a difference between two Binomial proportions (Cf. EX 85.) to draw conclusions from the data 
in the preceding example. You may have to adjust the p-values for multiple comparisons (Cf. Ch. 6.4.).
EX 128 In a study it was found that 41 of 248 identical twins were left-handed and that 18 of 246 fraternal twins were 
left-handed. Is the difference significant?
EX 129 In the middle of 1950 the SALK vaccine against polio was tested in USA in several multi-center studies. In one 
such study 20 000 children were vaccinated and among these 1 case of polio was detected, compared with 114 cases 
of polio among 473 000 unvaccinated children. What conclusion can you draw about the effect of the vaccine?
EX 130 In a sample of 300 families the standard of the electronic equipment was classified as Cheap or Expensive. 
The families were also classified according to social class as Low-Middle-High. The result was
Class
Low
Middle
High
Total
Standard
Cheap
38
88
31
160
Expensive
62
42
39
140
Total
100
130
70
300
Test whether Standard of equipment and Social class is independent and if not, try to find some significant patterns.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
151 
Hypothesis Testing
EX 131 In 2012 the yearly incidence of malignant melanoma in Sweden was 35 cases per 100 000 person. The same 
year 60 cases was observed in the city of Malmö in southern Sweden, with 110 000 inhabitants. Does this indicate 
that inhabitants in Malmö had a significantly higher risk for malignant melanoma than people in the rest of Sweden?
EX 132 A dealer takes a sample of 200 oranges from a large batch from his importer. He notices that 19 of these are 
of bad quality while the rest are acceptable. In a delivery from a new importer he finds that 10 oranges of 200 are bad 
while the rest are acceptable. Shall he prefer the new importer?
a)	 Discuss whether a one-sided or a two-sided test is preferably.
b)	 Test whether the proportion bad oranges is the same with the former and with the new importer.
c)	 Use the ordinary Chi-square test to test for independency between Quality of oranges and Importer. Compare 
the result with b).
EX 133 In a school with 156 pupils 90 were offered vaccination against a certain disease. After half a year the effect of 
the vaccination were studied, with the following result:
Diseased
Not diseased
Total
Vaccinated
4
86
90
Not vaccinated
18
48
66
Total
22
134
156
Did the vaccination have a significant effect?
[Hint: Repeat the arguments that was given in the preceding example a)–c).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Exercises in Statistical Inference  
with detailed solutions
152 
Hypothesis Testing
EX 134 A certain kind of surgical treatment may lead to complications. A comparison of two methods gave the 
following frequencies:
Old method
New method
Total
Complications
15
1
16
No complications
83
46
129
Total
98
47
145
Is the new method better than the old method?
a)	 Use the Chi-square test for independence.
b)	 Use Fisher’s exact test.
EX 135 A comparison of life lengths (hours) between two types of bulbs gave the following result:
Sample size
Mean
Stand. Dev.
Type A
20
1128
62
Type B
20
1236
83
a)	 Test whether there is a difference in quality between the two types of bulbs under the assumption that life 
lengths are normally distributed.
b)	 Repeat the test in a) but now without assuming that life lengths are normally distributed. [Hint: Use the CLT.]
EX 136 Two varieties of wheat A and B were grown in 12 different areas. The yield is summarized in the following 
table:
Area
1
2
3
4
5
6
7
8
9
10
11
12
A
24
16
21
24
26
12
17
21
25
19
29
22
B
21
17
20
25
21
13
15
19
21
22
24
22
Assume that yield in an area is normally distributed and test whether the difference in yield is significant.
EX 137 We have seen how the Chi-square principle can be used for a variety of tests. Here is another example, called 
the Median test. (There are several versions of this test.)
The population median is defined in Ch. 2.1. As an estimator of this one may take the sample median, defined as the 
middle point of the ranked data in a sample, or the average of the two middle points in case of an even sample size.
Test whether the following two series of data come from populations with the same median.
A
44 
40
46
22
51
41
48
38
58
60
28
40
B
24
54
80
35
36
23
15
21
43
18
12
29
[Hint: Rank the observations in each series, compute the sample median m for the combined series and count the 
number of observations that are above or below m in each of the two series. Then you summarize the result in a 2 x 2 
table with frequencies of the two variables (above m /below m) and (Series A/ Series B). The classical Chi-square test 
will then give you the answer.]
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
153 
Hypothesis Testing
EX 138 A beer-tasting Binomial experiment. 4 glasses of beer A and 4 glasses of beer B are served sequentially in 
random order to a person who has to decide which of the beers he is tasting.
a)	 How would you in practice arrange the experiment so that the variable Y = ‘Number of correct answers’
)
2
/
1
,8
(
~
=
=
p
n
Binomial
? Why assuming that
2
/
1
=
p
?
b)	 Assume that the person gives correct answers in y cases. What is the smallest value of y for which the hypothesis 
2
/
1
:
0
=
p
H
is rejected by a one-sided test at the 5% level?
c)	 When people are appointed as professional tasters of beer, coffee, tea, etc. they have to undergo tests in very 
long series. Assume that the person has to compare 50 glasses of beer of each kind instead of 4. Which is the 
smallest number of correct answers required to reject the hypothesis in b)?
EX 139 In 160 families with four children the number of boys (Y ) was:
Y
0
1
2
3
4
Frequency
6
38
58
47
11
Test whether the frequencies are in agreement with a variable that is distributed 
)
516
.0
,4
(
=
=
p
n
Binomial
.  
[Hint: See EX 4.]
a)	 By using the Kolmogorov test.
b)	 By using a Chi-square test.
EX 140 Independent measurements of viscosity for a certain substance were measured during two days with the 
following result:
Day1: 37.0 31.4 34.4 33.3 34.9 36.2 31.0 33.5 33.7 33.4 34.8 30.8 32.9 34.3 33.3
Day2: 28.4 31.3 28.7 32.1 31.9 32.8 30.2 30.2 32.4 30.7
Has the population distribution changed from one day to the next?
a)	 Use the Smirnov two-sample test. [Hint: Rank the observations within each sample and estimates the two 
sample cdf’s as described in CH. 6.2.4. Then search for the largest difference between the two cdf’s. It could 
help to make a plot.]
b)	 Compare the result in a) with the result that is obtained by assuming that both series are normally distributed. 
Conclusions?
EX 141 Repeat the analysis of the two varieties of wheat in EX 136 by using the sign test.
EX 142 15 student were ranked according to their results in Mathematics and Statistics with the following result:
Math.
3
5
1
12
10
8
6
9
2
15
13
7
11
4
14
Stat.
2
1
3
15
12
5
9
4
6
13
14
10
7
8
11
Is there a significant association between the two series? [Hint: Compare with EX 98.]
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
154 
Hypothesis Testing
EX 143 In a clinical trial one wants to study the effect of a drug on the concentration of a substance in blood. In 
a pilot study where the concentration was measured before and after the drug was added, the following result 
was obtained:
Before
1.10
0.98
0.95
0.99
1.05
1.20
0.96
1.07
0.96
1.06
After
1.05
0.95
0.90
0.98
1.01
1.08
0.94
1.08
0.98
1.04
The hypothesis of interest is
0
:
against 
 0
:
0
≠
=
D
a
D
H
H
µ
µ
.
a)	 Test the hypotheses by means of a p-value argument under the assumption that the mean difference is normally 
distributed. Give reasons for the assumption.
b)	 Let the estimates from the pilot study represent the true population parameters and assume that the variance 
of the difference is the same under 
a
H
H
 
and
 
0
determine a rejection region (RR) as a function of the sample 
size n under normality assumptions. The type-I error is 0.05. [Hint: Cf. Ch. 6.3.] 
c)	 Study the power function for the test in b). For which values of 
D
µ
 is the power larger than 0.90 when n = 100?
EX 144 In 2003 it was found that the proportion disabled (p) who were full-time workers in service profession was 
8%. Ten years later it was decided to plan a study to see whether this proportion had changed. In a sequentially 
collected sample it was found that the proportion stabilized around 3/20 = 0.15.
The hypothesis to test is 
08
.0
:
against 
08
.0
:
0
z
 
p
H
p
H
a
.  Determine the sample size, n, required to get a 
power of at least 0.90 when p = 0.15. Also, determine the rejection region (RR).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Exercises in Statistical Inference  
with detailed solutions
155 
Hypothesis Testing
EX 145 ( )n
i
iY
1
=  are iid with
)
,
(
~
2
σ
µ
N
Yi
. One wants to test
2
0
2
2
0
2
0
:
against 
 
:
σ
σ
σ
σ
≠
=
a
H
H
based on the 
test statistic 
∑
−
=
−
=
2
2
)
(
)1
(
Y
Y
S
n
T
i
with a type-I error of 5%.
a)	 Specify a RR of the form 
b
T
a
T
>
<
or 
 
and derive the power function.
b)	 Let n = 10 and study the power as a function of
2
2
0 /σ
σ
=
R
.
EX 146 In EX 120, where iid observations were distributed
)
(λ
l
Exponentia
, we determined the RR for testing
0
0
0
:
against 
 
:
λ
λ
λ
λ
≠
=
a
H
H
. Express the power of this test as a function of the ratio 
0
/λ
λ
=
R
. For which values of R are the power larger than 0.90?
EX 147 A new rapid method to measure concentration of a certain substance was tested against an exact method 
with the following result:
Exact method (X )
1
2
3
4
5
6
New method (Y )
1.2
1.9
3.1
4.2
4.7
5.9
a)	 Apply the model (
)
x
x
Y
E
⋅
+
=
β
α
and test the hypotheses 
1
:
against 
 1
:
0
≠
=
β
β
a
H
H
and 
0
:
against 
 0
:
0
≠
=
α
α
a
H
H
. [Hint: Cf. EX 104.]
b)	 If ‘non-significant parameters’ appear in a) formulate an alternative model and perform the test. 
EX 148 The concentration of a substance in blood (Y) was measured and compared with a known concentration (X ). 
The following result was obtained:
X
Y
1
1.1 0.7 1.8 0.4 
3
3.0 1.4 4.9 4.4 4.5
5
7.3 8.2 6.2
10
12.0 13.1 12.6 13.2
15
18.7 19.7 17.4 17.1
Test whether the model (
)
x
x
X
Y
E
⋅
+
=
=
β
α
is adequate. [Hint: Cf. Ex105.]
EX 149 In an experiment one studied the relation between x = Temperature in minus degrees Celsius needed to 
reach the freezing point and Y = Concentration of an alcohol at which the freezing point was reached. The result was:
x
0.5 
1
4
16
Y
1.2 1.5
1.9 2.1 
3.9 4.1
7.8 8.2
Can the relation be described by a linear regression function?
[WARNING! The alcohol is not ethyl so don’t put a bottle of Champagne (12–13%) in your freeze of  
about – -
C
o
16
.]
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
156 
Hypothesis Testing
EX 150 In the preceding example the linear model had to be abandoned. Plot the data and try to find a non-linear 
model that fits the data better.
[Hint: Try some of the non-linear models in Ch. 2.3.1 and test whether the linearized version can be accepted.]
EX 151 The body weight (Y ) was recorded for 64 men and women after a diet period. Let x be the 
initial weight and let z be a variable taking the value 1 for men and 0 for women. By running the model 
(
)
z
x
z
x
z
x
Y
E
⋅
+
+
+
=
3
2
1
,
β
β
β
α
one obtained the Sum of Squares for Error SSE = 18.4381.  With the model 
(
)
x
x
Y
E
1
β
α +
=
the Sum of Squares increased to SSE = 18.7454.
 Formulate and test relevant hypotheses and draw conclusions.
[Hint: See EX 107.]
EX 152 A frequently used relation in econometrics is the production function
2
1
)
,
(
β
β
α
P
I
P
I
Q
⋅
=
, where Q = 
consumed quantity, I = income level of prospective consumers and P = price of the commodity. The parameters 
2
1
 
and
 
β
β
are interpreted as Income elasticity and Price elasticity, respectively. Estimate the parameters in the 
linearized model from the following data:
(I =Total domestic private consumption (million SEK), Q = Yearly consumed quantity of strong beer (million liter), P = 
Total price of strong beer (million SEK). All prices in 1988 monetary value.)
Year
I 
P
Q
-73
421 027
778.4
24.1
-74
437 067
754.5
25.0
-75
453 748
770.7
24.9
-76
472 681
756.3
24.5
-77
485 582
1321.1
44.5
-78
491 919
2193.8
76.6
-79
507 296
2494.6
89.7
-80
497 081
2640.8
91.8
-81
489 929
2669.9
90.4
-82
506 769
2840.3
99.5
-83
507 822
3070.2
101.1
-84
515 257
3286.3
104.9
-85
527 904
3645.2
105.8
-86
554 850
4196.2
120.1
-87
584 427
4746.3
131.5
-88
563 293
5018.0
147.9
[Hint: Use a computer!]
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
157 
Hypothesis Testing
EX 153 Time to recovery after a certain disease vary according to the Weibull distribution with survival function
α
λy
e
y
S
−
=
)
(
. (Cf. Ch. 2.2.2.) Test 
1
:
against 
 1
:
0
≠
=
α
α
a
H
H
based on the following data of the proportion 
patients that are recovered at y years which is used as an estimator of 
)
(y
S
,
)
(ˆ y
S
.
y
0.47
0.64
0.89
1.08
1.41
)
(ˆ y
S
5/6
4/6
3/6
2/6
1/6
[Hint: Linearize the survival function and use the techniques for analyzing linear models in Ch. 6.6.1.]
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Exercises in Statistical Inference  
with detailed solutions
158 
Answers to Supplementary Exercises
Answers to Supplementary 
Exercises
Solutions to Supplementary Exercises Ch. 3
EX 28 
n
Y
Y
b
y
y
F
b
y
b
y
y
F
i



−
−
=
⇒
≤
≤
=
1
1
)
(
0 ,
)
(
)
1
(
. From derivation rule (5) in Ch. 2.3.3,
1
1
1
1
0
1
)
('
)
(
)
1
(
−
−



−
=











−
−





−
=
=
n
n
Y
Y
b
y
b
n
b
y
n
b
y
F
y
f
i
.
EX 29
[
]
n
y
Y
y
Y
e
y
F
y
e
y
F
n
i
⋅
−
⋅
−
−
=
⇒
≥
−
=
λ
λ
1
)
(
0
 ,
1
)
(
)
(
. From derivation rule (5) in Ch. 2.3.3,
(
)
[
]
[
]
1
1
1
1
)
(
)
('
)
(
)
(
)
(
−
⋅
−
⋅
−
−
⋅
−
⋅
−
−
=
−
−
−
=
=
n
y
y
n
y
y
Y
Y
e
e
n
e
n
e
y
F
y
f
n
n
λ
λ
λ
λ
λ
λ
.
EX 30 Exact mean: (
)
(
)
(
)
[
]
(
)=
+
−
=
−
=
−
=
−
)ˆ
(
)ˆ
(
)ˆ
(
1
)
ˆ
(
)ˆ
(
1
ˆ
ˆ
1
/)ˆ
1(ˆ
2
2
2
p
E
p
V
p
E
n
p
E
p
E
n
p
p
E
n
n
p
p
E
).
1(
)1
(
1
1
)
1(
1
)
1(
1
2
2
p
p
n
n
n
p
p
n
p
n
p
p
p
n
−
−
=





−
−
=






−
−
−
Approximate mean: 
First we notice that
2
)
(''
,
2
1
)
('
)
(
2
−
=
−
=
⇒
−
=
p
g
p
p
g
p
p
p
g
. (13) with 
n
p
p
p
)
1(
,
2
−
=
=
σ
µ
gives: 
(
)
(
)
=






−
−
+
−
≈
−
=
−
n
p
p
p
p
n
p
p
E
n
n
p
p
E
)
1(
)
2
(
2
1
1
ˆ
ˆ
1
/)ˆ
1(ˆ
2
2
)
1(
)1
(
2
p
p
n
n
−
−
.
It follows that 
)1
(
)ˆ
1(ˆ
)ˆ
(ˆ
−
−
=
n
p
p
p
V
is unbiased for 
n
p
p
p
V
)
1(
)ˆ
(
−
=
.
EX 31 
i
i
i
i
i
i
n
p
p
p
i
p
/)
1(
 
and
 
 
have
 2,1
 ,
ˆ
2
i
−
=
=
=
σ
µ
and 
0
12  
V
 (due to independence). Thus we get from 
EX 27: 






+
−
+
−






≈
0
/)
1(
/)
1(
)ˆ
(
2
2
2
2
2
2
1
1
1
1
2
2
1
p
n
p
p
p
n
p
p
p
p
R
V
= 




−
+
−






2
2
2
1
1
1
2
2
1
1
1
p
n
p
p
n
p
p
p
.  
The expression for the estimated variance is finally obtained by replacing 
i
i
i
n
Y
p
/
by 
 
.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
159 
Answers to Supplementary Exercises
EX 32 From (9b) we know that 
2
2)
(
σ
µ
=
=
S
E
and (don’t use 
2
σ
in the following to avoid confusion)
>
@
)1
(
2
)
,
(
~
If
1
)1
(
)
3
(
)
(
4
2
2
2
4
2

 
 
¸¸
¹
·
¨¨
©
§



 
n
N
Y
n
n
n
S
V
i
V
V
P
P
P
.   The latter relation follows since 
)1
(
2
)1
(
))1
(
(
)1
(
)
(
)1
(
)1
(
~
2
4
2
2
4
2
2
2
2
−
−
=
−
−
=
⇒
−
−
n
n
n
V
n
S
V
n
n
S
σ
χ
σ
χ
σ
.
Since 
2
1
2
2
)
(S
S
S
=
=
we consider the function 
2
/
3
2
/
1
2
1
4
1
)
(''
 
and
 
2
1
)
('
 with 
)
(
y
y
g
y
y
g
y
y
g
−
=
=
=
.  
Thus,
a)	
3
2
2
2
/
3
2
2
1
2
8
)
(
)
(
)
(
4
1
2
1
)
(
)
(
σ
σ
σ
σ
S
V
S
V
S
E
−
=






−
+
≈
2
2
2
2
2
/
1
2
4
)
(
)
(
)
(
2
1
)
(
σ
σ
S
V
S
V
S
V
=






≈
b)	
)1
(
4
)1
(
2
8
1
)
(
4
3
−
−
=
−
−
≈
n
n
S
E
σ
σ
σ
σ
σ
)1
(
2
)1
(
2
4
1
)
(
2
4
2
−
=
−
≈
n
n
S
V
σ
σ
σ
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Exercises in Statistical Inference  
with detailed solutions
160 
Answers to Supplementary Exercises
Ch. 4
EX 45
a)	


n
n
Y
n
n
Y
n
Y
b
ny
y
f
b
y
y
F
y
F
y
F
b
y
b
y
y
F
n
n
n
1
)
(
)
(
)
(
)
(
and
0
,
)
(
)
(
)
(
)
(

 

 

 
d
d
 


 


 
»
¼
º
«
¬
ª

 
 

 

 
 


³
³
b
n
n
n
b
b
n
n
y
b
n
dy
y
b
n
dy
b
ny
y
Y
E
n
n
b
y
y
n
n
b
b
n
n
n
n
n
)1
(
)1
(
1
)
(
1
0
1
0
0
1
)
(
)
(
)1
(
ˆ
n
Y
n
n
b
+
=
is unbiased for b.
)
(
)1
(
)ˆ
(
)
(
2
2
n
Y
V
n
n
b
V
+
=
. Now, 
2
1
2
2
)
(
)
2
(
)
(
b
n
n
dy
b
ny
y
Y
E
n
n
n


 

 

³
,  so
)
2
(
)1
(
)
2
(
1
)1
(
)1
(
)
2
(
)1
(
)ˆ
(
2
2
2
2
2
2
2
2
+
=






+
−
+
⋅
+
=














+
−
⋅
+
+
=
n
n
b
n
n
n
b
n
n
b
n
n
b
n
n
n
n
b
V
b)	 OLS estimator: 

 

 
»¼
º
«¬
ª


 

»¼
º
«¬
ª

 
¦
¦
¦
 
 
 
2
0
2
2
)
2
1
(
2
1
1
1
2
nb
Y
b
Y
db
dSS
b
Y
SS
n
i
i
n
i
n
i
i
i
Y
n
Y
b
n
i
i
OLS
2
/
2
ˆ
1
=
= ∑
=
. This is easily seen to be unbiased for b. The variance is
 
>
@
n
b
b
n
n
Y
V
n
b
V
n
i
i
OLS
3
12
4
2.2.2
Cf.
)
(
2
)
ˆ
(
2
2
2
1
2
 

 
 
¸
¹
·
¨
©
§
 
¦
 
.
 Moment estimator: 
OLS
Mom
b
Y
b
b
Y
ˆ
2
ˆ
2
/
=
=
⇒
=
.
a)	 Relative efficiency (RE) = 
1
2
3
3
/
)
2
(
/
)
ˆ
(
)ˆ
(
2
2
≤
+
=
+
=
n
n
b
n
n
b
b
V
b
V
OLS
 with equality if n = 1. In the latter case 
OLS
b
b
ˆ 
and
 ˆ
are identical.
b)	 b = Length of each red light period. From the data we obtain
0.
20
and
46
)
(
 
 
y
y n
,  so the estimates are 
0.
40
0.
20
2
ˆ
ˆ
and
6.
50
46
10
)1
10
(
ˆ
 

 
 
 


 
Mom
OLS
b
b
b
.  Of these two the first one should be more 
reliable since RE in this case is 1/4.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
161 
Answers to Supplementary Exercises
EX 46 
a)	 OLS estimator: Remember that 
)
1(
)
(
 
and
 
)
(
p
p
n
Y
V
p
n
Y
E
i
i
i
i
−
=
=
.
>
@
>
@
¦
¦
¦
¦
¦
¦
 
 
 
 
 
 
 

 


 


 


 
n
i
n
i
n
i
i
n
i
i
i
OLS
n
i
i
i
i
i
i
i
n
i
i
i
n
Y
n
p
Y
n
p
n
p
n
Y
n
dp
dSS
p
n
Y
SS
1
1
1
2
1
1
2
1
2
ˆ
0
0
2
)
(
p
p
n
n
n
Y
E
n
n
p
E
n
i
i
i
n
i
i
n
i
i
i
n
i
i
OLS
=
⋅
=
=
∑
∑
∑
∑
=
=
=
=
1
1
2
1
1
2
1
)
(
1
)
ˆ
(
(Unbiased.)
2
1
2
1
3
1
2
2
1
2
1
2
2
1
2
)
1(
)
1(
1
)
(
1
)
ˆ
(






−
=
−
⋅






=






=
∑
∑
∑
∑
∑
∑
=
=
=
=
=
=
n
i
i
n
i
i
n
i
i
i
n
i
i
i
n
i
i
n
i
i
OLS
n
n
p
p
p
p
n
n
n
Y
V
n
n
p
V
 ML estimator:
∏
=
−
−
⇒
−
⋅
=
−






=
∑
∑
∑
=
=
=
n
i
y
n
y
y
n
y
i
i
n
i
i
n
i
i
n
i
i
i
i
i
p
p
C
p
p
y
n
L
1
1
1
1
)
1(
)
1(
[C is a constant which doesn’t contain p]
EX 46 (Continued)
)
1(
)1
(
0
ln
)
1
ln(
ln
ln
ln
1
1
1
1
1
1
p
y
n
p
y
dp
L
d
p
y
n
p
y
C
L
n
i
n
i
i
i
n
i
i
n
i
n
i
i
i
n
i
i


¸¸
¹
·
¨¨
©
§



 


¸¸
¹
·
¨¨
©
§



 
¦
¦
¦
¦
¦
¦
 
 
 
 
 
 
and putting this equal to zero yields 
¦
¦
 
 
 
n
i
n
i
i
i
ML
n
y
p
1
1
/
ˆ
 
p
p
n
n
Y
E
n
p
E
n
i
i
n
i
i
n
i
i
n
i
i
ML
 
 
 
¦
¦
¦
¦
 
 
 
 
1
1
1
1
1
)
(
1
)
ˆ
(
 (Unbiased.)
¦
¦
¦
¦
¦
 
 
 
 
 

 

¸¸
¹
·
¨¨
©
§
 
¸¸
¹
·
¨¨
©
§
 
n
i
i
n
i
i
n
i
i
n
i
i
n
i
i
ML
n
p
p
p
p
n
n
Y
V
n
p
V
1
1
2
1
1
2
1
)
1(
)
1(
1
)
(
1
)
ˆ
(
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
162 
Answers to Supplementary Exercises
Comparison: 
¦
¦
¦
 
 
 

¸¸
¹
·
¨¨
©
§
 
 
n
i
i
n
i
i
n
i
i
OLS
ML
n
n
n
p
V
p
V
RE
1
3
1
2
1
2
)
ˆ
(
)
ˆ
(
1
d
. This follows from Cauchy-Schwartz inequality (Cf. Ch. 2.3.5) 
by letting
2
3
 2
1
 
and
 
i
i
i
i
n
y
n
x
=
=
. (It may suffice if one demonstrates the inequality numerically by choosing a few 
values of
in .)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
▶▶enroll by September 30th, 2014 and 
▶▶save up to 16% on the tuition!
▶▶pay in 10 installments / 2 years
▶▶Interactive Online education
▶▶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Exercises in Statistical Inference  
with detailed solutions
163 
Answers to Supplementary Exercises
(From now on we skip the upper and lower index in the summation signs.)
b)	 To show that the ML estimator is BLUE, put 
∑
∑
=
=
⇒
=
)
(
)
(
i
i
i
i
n
Y
E
a
T
E
Y
a
T
∑
∑
∑
=
−
⇒
=
=
=
⋅
0
1
Put)
(
i
i
i
i
i
i
n
a
p
n
a
p
p
n
a
 (1)
[
]
[
]
[
]
∑
∑
∑
∑
∑
⇒
−
+
−
⋅
=
−
+
=
−
+
=
1
)
1(
1
)
(
1
)
(
2
2
i
i
i
i
i
i
i
i
i
i
n
n
a
p
p
n
a
n
a
Y
V
a
n
a
T
V
Q
λ
λ
λ
,'
)
1(
2
0
)
1(
2
O
O
O
 


 

 


 
p
p
a
n
p
p
n
a
da
dQ
i
i
i
i
i
 say. (2) Putting this into (1) gives 
∑
=
in
/
1
'
λ
which inserted into (2) gives 
∑
=
i
i
n
a
/
1
. Thus, 
¦
¦
 
 
ML
i
i
BLUE
p
n
Y
p
ˆ
/
ˆ
.
 To show that the ML estimator is a MVE we determine the C-R limit. From a) above,
>
@


 
¸¸
¹
·
¨¨
©
§





¸¸
¹
·
¨¨
©
§

 
 
¦
¦
¦
2
2
2
2
)
1(
)1
(
0
1
0
2.3.3
Ch.
in
rules
derivation
Cf.
ln
p
y
n
p
y
dp
L
d
i
i
i


>
@


 



 
¸¸
¹
·
¨¨
©
§







¦
¦
¦
¦
¦
¦
2
2
2
2
2
2
)
1(
ln
by
Replacing
)
1(
p
p
n
n
p
p
n
dp
L
d
E
Y
y
p
y
n
p
y
i
i
i
i
i
i
i
i
)
(
)
1(
)
1(
p
I
p
p
n
p
n
p
n
i
i
i
=
−
=
−
+
∑
∑
∑
. Thus, 
)
(
1
)
ˆ
(
p
I
p
V
ML  
 so the ML estimator is a MVE.
c)	 Let 
=
in
Total number of students in room i, 
=
iY
Number with back/neck pain in room i
From the data we get ¦
¦
¦
¦
 
 
 
 
175
,6
,
2750
,
90
2
i
i
i
i
i
y
n
y
n
n
.  This leads to the following 
estimates: 
%)
7.6
(
067
.0
90
/
6
ˆ
%),
4.6
(
064
.0
2750
/
175
ˆ
 
 
 
 
ML
OLS
p
p
.
EX 47

¦




 


 

 
¦


p
n
p
n
y
L
p
p
p
p
L
i
n
n
y
y
i
i
ln
)
1
ln(
)
(
ln
)
1(
)
1(
1
 
¦
¦
 
 

 




 
y
y
n
p
p
n
p
n
y
dp
L
d
i
ML
i
1
ˆ
0
)
1(
)1
(
)
(
ln
Define the variable 
=
iY
Number of trials until the first ‘1’ appears. Then n = 3 and
3
,1
,4
3
2
1
=
=
=
y
y
y
.  
Thus 
.
375
.0
8
3
ˆ
 
 
ML
p
EX 48
a)	









 
 
¦







2
2
2
2
2
2
2
2
2
1
2
2
2
V
E
V
E
SV
SV
i
i
i
i
x
y
n
x
y
e
e
L






¦
¦
¦
¦
 

 



 




 
2
2
2
2
2
ˆ
0
2
2
)
(
0
ln
2
2
ln
2
ln
i
i
i
ML
i
i
i
i
i
x
Y
x
x
y
x
d
L
d
x
y
n
L
E
V
E
E
V
E
SV
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
164 
Answers to Supplementary Exercises
EX 48 (continued)
ML
Eˆ
 is simply the BLUE estimator that was found in EX 30, without any distributional assumptions.
In Ch. 2.2.2 (4) it was stated in the comments to the normal distribution that a linear form of normally distributed 
variables is itself normally distributed. Applying this to 
¦
¦
 
 
2
 with 
,
ˆ
i
i
i
i
i
ML
x
x
l
Y
l
E
,  shows that 
ML
Eˆ
 is 
exactly normally distributed with 
 
 

 ¦
¦
)
ˆ
(
and
)
ˆ
(
2
ML
i
i
i
ML
V
x
x
x
E
E
E
E
E
(
)
∑
∑
∑
=
⋅
2
2
2
2
2
2
i
i
i
x
x
x
σ
σ
.
b)	 From the expression for ln L in a) we get
>
@

 
¸
¹
·
¨
©
§




 
 
¦
0
1
2
)
(
2
!
not
,
repect to
 with 
is
derivative
The
ln
4
2
2
2
2
V
E
V
V
V
V
i
i
x
y
n
d
L
d
n
x
Y
i
ML
i
ML
¦

 
2
2
)
ˆ
(
ˆ
E
V
.  [Notice that the ML estimator has been inserted for β .] We now determine the 
distribution of this by using Cochran’s theorem (7) in Ch. 3.1.
y
g
( )




¦
¦
¦
¦



 



 

2
2
2
2
)
ˆ
(
)
ˆ
(
)
ˆ
(
)
ˆ
(
i
i
ML
i
ML
i
i
i
ML
i
ML
i
i
i
x
x
x
Y
x
x
x
Y
x
Y
E
E
E
E
E
E
E
since the cross product vanishes. In fact 

 


 


 


¦
¦
¦
¦
2
ˆ
)
ˆ
(
2
)
ˆ
(
)
ˆ
(
2
)
ˆ
)(
ˆ
(
2
i
ML
i
i
ML
i
i
ML
i
ML
i
i
i
ML
i
x
Y
x
x
x
Y
x
x
x
Y
E
E
E
E
E
E
E
E
E
 


0
)
ˆ
(
2
 


¦
¦
i
i
i
i
ML
Y
x
Y
x
E
E
. Thus, ,




¦
¦
¦



 

2
2
2
2
)
ˆ
(
ˆ
i
ML
i
ML
i
i
i
x
x
Y
x
Y
E
E
E
E
.
Divide each term in the latter expression by 
2
σ
and write the identity as 
3
2
1
Q
Q
Q
+
=
.
We now find the distributions of 
3
1
 
and
 
Q
Q
.
(
)
)
(
~
)
(
)1(
~
)
(
1,0
~
)
(
)
,
(
~
2
2
2
1
2
2
2
2
n
x
Y
Q
x
Y
N
x
Y
x
N
Y
i
i
i
i
i
i
i
i
i
χ
σ
β
χ
σ
β
σ
β
σ
β
∑
−
=
⇒
−
⇒
−
⇒
)1(
~
/
)
ˆ
(
)1,0
(
~
/
)
ˆ
(
,
~
ˆ
2
2
2
2
3
2
2
2
2
F
V
E
E
V
E
E
V
E
E
¦
¦
¦

 



¸
¸
¹
·
¨
¨
©
§
i
ML
i
ML
i
ML
x
Q
N
x
x
N
From Cochran’s theorem it follows that 





 ¦
)1
(
~
ˆ
2
2
2
2
n
x
Y
Q
i
ML
i
F
V
E


)1
(
)1
(
~
)1
(
ˆ
2
2
2
2





 ¦
n
n
n
x
Y
S
i
ML
i
F
V
E
 with 
(
)
2
2
2
2
2
)1
(
)1
(
)1
(
)1
(
)
(
σ
σ
χ
σ
=
−
−
=
−
−
=
n
n
n
E
n
S
E
.
So, 
ML
2
ˆV
 is not unbiased, but the corrected estimator 
2
S
is unbiased.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
165 
Answers to Supplementary Exercises
EX 49 
BLUE of µ . 
∑
∑
∑
∑
⇒
=
=
=
=
=
=
µ
µ
µ
(Put)
)
(
)
(
 
has
 
i
i
i
i
n
i
i
n
a
a
Y
E
a
T
E
Y
a
T
∑
=
−
)1(  0
1
ia
∑
∑
∑
=
=
=
.
/
/
)
(
)
(
2
2
2
2
2
i
i
i
i
i
i
n
n
a
n
a
Y
V
a
T
V
σ
σ
 Thus, 
[
]=
−
+
=
∑
1
)
(
i
n
a
T
V
Q
λ
>
@
¦
¦
 

 

 

 



i
i
i
i
i
i
i
i
i
n
n
a
n
a
da
dQ
a
n
a
'
2
0
/
2
1
/
2
2
2
2
O
V
O
O
V
O
V

(2)
(2) inserted into (1) gives ∑
∑
∑
=
⇒
=
=
i
i
i
n
n
n
/
1
'
1
'
'
λ
λ
λ
, which inserted into (2) gives
∑
=
i
i
i
n
n
a
/
. So, 
∑
∑
=
i
i
i
BLUE
n
Y
n
/
ˆµ
. 
 (
)
(
)
(
)
∑
∑
∑
∑
∑
=
=
=
i
i
i
i
i
i
i
BLUE
n
n
n
n
Y
V
n
n
V
2
2
2
2
2
2
1
)
(
1
ˆ
σ
σ
µ
BLUE of 
2
σ
. Notice first that 

 



2
2
2
2
16)
EX
(Cf.
)1
(
)1
(
~
i
i
i
i
S
E
n
n
S
F
V
(
)
(
)
(
)=
−
−
=
=
−
−
=
−
−
)1
(
)1
(
 
and
 
)1
(
)1
(
)1
(
)1
(
2
2
4
2
2
2
2
2
i
i
i
i
i
i
i
n
V
n
S
V
n
n
n
E
n
χ
σ
σ
σ
χ
σ
)1
(
2
)1
(
2
)1
(
4
2
4
−
=
−
−
i
i
i
n
n
n
σ
σ
.
Put 
(
)
∑
∑
∑
∑
=
−
⇒
=
=
=
=
⇒
=
0
1
(Put)
)
(
2
2
2
2
i
i
i
i
n
i
i
n
a
a
S
E
a
T
E
S
a
T
σ
σ

(3)
(
)
∑
∑
−
=
=
)1
/(
2
)
(
2
4
2
2
i
i
i
i
n
n
a
S
V
a
T
V
σ
. Thus, 
>
@
>
@

 


 




 


 
¦
¦
¦
0
)1
(
4
1
)1
(
2
1
)
(
4
2
4
O
V
O
V
O
i
i
i
i
i
i
i
n
n
a
da
dQ
a
n
a
a
T
V
Q
)1
('
4
)1
(
4
−
=
−
−
=
i
i
i
n
n
a
λ
σ
λ
 (4), which inserted into (3) yields 
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
166 
Answers to Supplementary Exercises
∑
∑
−
=
⇒
=
−
)1
(
1
'
1
)1
('
i
i
n
n
λ
λ
and this inserted into (4) gives 
∑
−
−
=
)1
(
)1
(
i
i
i
n
n
a
. So,
∑
∑
−
−
=
)1
(
)1
(
ˆ
2
2
i
i
i
BLUE
n
S
n
σ
(
)
(
)
(
)
∑
∑
∑
∑
∑
−
=
−
−
−
=
−
−
=
)1
(
2
)1
(
)1
(
2
)1
(
)1
(
)
(
)1
(
ˆ
4
2
4
2
2
2
2
2
i
i
i
i
i
i
i
BLUE
n
n
n
n
n
S
V
n
V
σ
σ
σ
EX 50 In this example the cell probabilities are specified as hypothetical proportions. In Ch.6, where tests of 
hypothesis are considered, there will be more examples of this.






 



 
)
3
1
ln(
))
ln(
)
2
(ln(
ln
ln
ln
)
3
1(
)
2
(
3
2
1
3
2
1
p
y
p
y
p
y
C
L
p
p
p
C
L
y
y
y
 



 

 



 





 
)
(
3
ˆ
3
)
3
1
)(
(
0
)
3
1(
)
3
(
0
ln
3
2
1
2
1
3
2
1
3
2
1
y
y
y
y
y
p
p
y
p
y
y
p
y
p
y
p
y
dp
L
d
ML
n
y
y
3
2
1 +
. The corresponding ML estimator is 
n
Y
Y
pML
2
1
ˆ

 
.


>
@


p
p
n
np
n
Y
E
Y
E
n
p
E
ML
 


 
 

 
2
3
1
)
6
(
2.2.1
Ch.
Cf.
)
(
)
(
3
1
)
ˆ
(
2
1
 (Unbiased.).



  







 


 
p
p
n
p
p
n
p
np
n
Y
Y
Cov
Y
V
Y
V
n
p
V
ML
2
2
)
2
1(
2
)
1(
9
1
)
,
(
2
)
(
)
(
)
3
(
1
)
ˆ
(
2
2
1
2
1
2
n
p
p
p
p
p
n
np
)
3
1(
)
4
)
2
1(
2
1(
9
2

 




.
We now find the C-R limit.
 
¸¸
¹
·
¨¨
©
§


¸¸
¹
·
¨¨
©
§




 
¸¸
¹
·
¨¨
©
§






 
2
2
2
3
2
2
1
2
3
2
2
2
1
2
2
ln
)
3
1(
9
)
3
1(
)3
(
0
3
ln
dp
L
d
E
p
y
p
y
y
p
y
p
y
p
y
dp
L
d
)
(
)
3
1(
3
)
1(
9
3
)
3
1(
)
3
1(
9
2
)
3
1(
9
2
2
2
3
2
2
1
p
I
p
p
n
p
n
p
n
p
p
n
p
p
n
np
p
Y
p
Y
Y
E
 

 


 





 
¸¸
¹
·
¨¨
©
§



Since 
)
(
/
1
)
ˆ
(
p
I
p
V
ML  
 we conclude that the ML estimator is MVE.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
167 
Answers to Supplementary Exercises
Ch. 5
EX 63 Data consist of naturally paired observations that are strongly dependent. Therefore the approach in EX 53 
can’t be used. Instead we consider the weight-loss 
Y
X
D
−
=
 for each subject which gives the series 
subject
1
2
3
4
5
6
7
8
9
10
D
0.2
1.1
-0.1
0.3
-0.4
0.8
0.2
0.7
0.3
0.7
Here we get 
)
4497
.0
ˆ
(
2022
.0
ˆ
,
40
.0
ˆ
,
10
2
 
 
 
 
D
D
D
n
V
V
P
a)	 The 95% CI for 
D
µ
is 
10
4497
.0
40
.0
C
r
,  where C is obtained from (
)
025
.0
)
9
(
=
> C
T
P
262
,2
=
⇒C
. 
Thus, the 95% CI is (0.08, 0.72), so the conclusion is that the training program has a significant positive effect 
on weight-loss.
b)	 The 95% CI for 
2
σ
is (Cf. EX 43)






⋅
⋅
a
b
9
2022
.0
,
9
2022
.0
, where a and b are determined from 
95
.0
)
)
9
(
(
2
 


b
a
P
F
.




¯
®
­
 

 
!
 

 
!
7004
.2
975
.0
)
9
(
0226
.
19
025
.0
)
9
(
2
2
a
a
P
b
b
P
F
F
This gives the CI (0.096, 0.674) and since the latter is far below 0.7 (the value for females) we conclude that the 
variation in weight-loss is significantly smaller among males.
c)	 A weight-loss was observed for Y = 8 subjects of n = 10, giving 
80
.0
ˆ  
p
. From the large-sample expression 
in EX 55 a) we get 
10
/
20
.0
80
.0
96
.1
80
.0

r
,  or (-0.05, 1.05), which is unreasonable.
In order to use the conservative expression in (23) we need the percentiles 
13
.3
)
20
,6
(
975
.
 
F
 and 
)
4,
18
(
975
.F
. The latter is hard to obtain from tables in textbooks, but one solution may be to use linear 
interpolation between 
)
4,
15
(
and
56
.8
)
4,
20
(
975
.
975
.
F
F
 
 = 8.66, yielding 
60
.8
)
4,
18
(
975
.
|
F
.  The 
latter is close to the true value 8.59 obtained by using the function finv in SAS (Cf. EX 56).
The 95% CI is 


976
.0
,
107
.0
59
.8
)1
8
(
8
10
59
.8
)1
8
(
,
13
.3
)1
8
10
(
8
8
 
¸¸
¹
·
¨¨
©
§










d)	 From Ch. 5.4.1 
(
)2
/
)ˆ
1(ˆ
B
C
p
p
n
−
=
, where C = 1.645 since we want a CI of 90%. Thus, 
(
)
693
025
.0
/
645
.1
2.0
8.0
2 =
⋅
=
n
.
The reason for choosing a 90% CI in this case is that we need to have Bound of Error small and this in turn is 
due to the fact that pˆ  is quite close to 1.
The balance between the choice of confidence level and Bound of Error can sometimes be a delicate problem.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
168 
Answers to Supplementary Exercises
EX 64 Introduce the notations 
C
F
µ
µ
 
and
 
for the population means in the two groups and 
2
2
 
and
 
C
F
σ
σ
for the 
population variances. We first make a 95% CI for the ratio
2
2 /
F
C σ
σ
.
In accordance with EX 53 a) we get
1
2
2
2
2
2
2
2
/
/
c
S
S
c
S
S
F
C
F
C
F
C
<
<
σ
σ
, where 

95
.0
)1
,1
(
2
1
 




c
n
n
F
c
P
F
C
.
6362
.0
/
2
2
=
F
C S
S
and 




¯
®
­
 

 
!
 

 
!
455
.0
975
.0
)
21
,
29
(
317
.2
025
.0
)
21
,
29
(
1
1
2
2
c
c
F
P
c
c
F
P
 gives the CI (0.27, 1.40).
Since this interval well covers 1we can assume that the two population variances are equal.
The BLUE of the common variance is
2859
.0
50
3623
.0
21
2305
.0
29
1
1
)1
(
)1
(
ˆ
2
2
2
 



 






 
F
C
F
F
C
C
n
n
S
n
S
n
V
From EX 53 b) the 95% CI for 
F
C
µ
µ
−
is 
)
/
1
/
1(
ˆ 2
F
C
F
C
n
n
C
Y
Y
+
±
−
σ
, where C is determined from 




009
.2
025
.0
)
50
(
95
.0
)
50
(
 

 
!

 



C
C
T
P
C
T
C
P
.  This gives the CI (0.24, 0.85).
Since the CI is far above 0 the conclusion is that the mean AOD in the C group is significantly larger than in the 
FAS group.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Exercises in Statistical Inference  
with detailed solutions
169 
Answers to Supplementary Exercises
EX 65 
n
Y
V
Y
E
Y
V
Y
E
l
Exponentia
Y
2
2
/
1
)
(
 
and
 
/
1
)
(
/
1
)
(
 
and
 
/
1
)
(
 
has
 
~
λ
λ
λ
λ
=
=
⇒
=
=
.
According to the CLT 
(
)
)1,0
(
~
1
/
1
/
1
2
N
Z
Y
n
n
Y
D
→

−
=
−
λ
λ
λ
.
From this we get 


96
.1
)1
(
96
.1
95
.0




 
Y
n
P
O
 and centering λ finally gives the CI
¸
¹
·
¨
©
§


)
/
96
.1
1(
1
),
/
96
.1
1(
1
n
Y
n
Y
.
In order to calculate the expected length of the CI we need to know that 
)
,
(
~
1
n
Gamma
Y
n
i
i
λ
∑
=
so (Cf. Ch. 
2.2.2 (2)) 
)1
(
)1
(
)1
(
)1
(
)
(
)1
(
1
/
1
1
1
−
=
−
Γ
−
−
Γ
=
Γ
−
Γ
=






−
=∑
n
n
n
n
n
n
Y
E
n
i
i
λ
λ
λ
. The expected length of the CI above is 
thus  
>
@
O
O
566
.0
50
96
.1
2
)1
(
 
 
 



n
n
n
n
.  This is to be compared with the CI in EX 61 a) when n = 50, 
O
O
565
.0
2
)
22
.
74
56
.
129
(
)1
(
 


n
.
EX 66 From EX 46 
¦
¦
¦

 
 
 
i
ML
ML
i
i
ML
n
p
p
p
V
p
p
E
n
Y
p
/)
1(
)
ˆ
(
and
)
ˆ
(
has
/
ˆ
.  Here∑
iY has a 
Binomial distribution, so according to the CLT 
)1,0
(
~
/)
ˆ
1(
ˆ
ˆ
N
Z
n
p
p
p
p
D
i
ML
ML
ML
o



¦
.  (Cf. EX 23 c).) This gives 
the 95% CI limits 
¦

r
i
ML
ML
ML
n
p
p
p
/)
ˆ
1(
ˆ
96
.1
ˆ
.
From the table we get 
30
.0
100
/
30
ˆ
 
 
ML
p
,  so the CI is (0.21, 0.39).
EX 67 
76
.
105
4
117
91
112
103
ˆ
 



 
O
.  (This is the OLS-, Moment-, BLUE- and ML estimate.)
n
V
n
V
E
/ˆ
)ˆ
(ˆ
 with 
,
/
)ˆ
(
 
and
 
)ˆ
(
λ
λ
λ
λ
λ
λ
=
=
=
.
Now, following the same lines as in EX 23 c) it can be shown that 
)1,0
(
~
1
/
/ˆ
)1,0
(
~
/
)
ˆ
(
/ˆ
)
ˆ
(
N
Z
n
n
N
Z
n
n
P
D
 
o

o


 

O
O
O
O
O
O
O
O
n
/ˆ
96
.1
ˆ
O
O r

 gives a 95% CI for λ  in large 
samples. The CI is in this case (96, 116).
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
170 
Answers to Supplementary Exercises
Ch. 6
EX 109 
a)	 A 2 x 2 table may look as follows:
Shape
Round
Wrinkly
Total
Yellow
315
101
416
Color
Green
108
32
140
Total
423
133
556
b)	 We obtain the following table:
Characteristic
Obs. Freq.
Exp. Freq.
Deviation
Cell Chi.square
Yellow, Round
315
312.75
2.25
0.016
Green, Round
108
104.25
3.75
0.135
Yellow, Wrinkly
102
104.25
-3.25
0.101
Green, Wrinkly
32
34.75
-2.75
0.218
Total
556
556.00
0
0.470
SYDOXH








 
!
 
F
3
No reason to reject Mendel’s theory. (It’s interesting that the famous statistician 
R. A. Fisher concluded that Mendel’s results were far too perfect, indicating that adjustments had been made to the 
data to make observations fit the hypothesis.)
EX 110 
a)	 In the Poisson distribution
)
(
)
(
Y
V
Y
E
=
= λ
. The corresponding sample estimates are 
8.
251
and
6.
54
2  
 
s
y
. The variance is more than four times larger than the mean, which 
seems strange.
b)	
)
(
~
 :
0
λ
Poisson
Y
H
The test statistic is 
5.
41
6.
54
)
6.
54
31
(
6.
54
)
6.
54
81
(
)
(
2
2
2
2
 




 

 ¦

Y
Y
Y
X
i

p-value e




!

 
005
.0
5.
41
)1
10
(
(
2
F
P
 Reject 
0
H .
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
171 
Answers to Supplementary Exercises
EX 111 Let Y = ‘Number on sick leave per day’. 
)
(
~
:
0
λ
Poisson
Y
H
The pf is 

O
O

 
 
e
y
y
Y
P
y
!
.
0.1
30
30
0
10
12
5
0
1
10
0
12
ˆ
 
 









 
 


y
O
Expected frequencies:




,
04
.
11
!1
)
0.1(
30
1
ˆ
30
,
04
.
11
!0
)
0.1(
30
0
ˆ
30
0.1
1
0.1
0
 
 
 
 
 
 


e
Y
P
e
Y
P






^
`












Ö



Ö










Ö



 



 
d

 
t
 
 
 

<
3
<
3
H
<
3

The latter is computed to avoid small expected frequencies (cf. comment to EX 71).
29
.0
40
.2
)
40
.2
2
(
52
.5
)
52
.5
6
(
04
.
11
)
04
.
11
10
(
04
.
11
)
04
.
11
12
(
2
2
2
2
2
 







 
X
p-value e


10
.0
29
.0
)1
1
4
(
2
!!
!


 
F
P
. There is no reason to reject the Poisson model.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Exercises in Statistical Inference  
with detailed solutions
172 
Answers to Supplementary Exercises
EX 112 
)
,
(
~
:
2
0
σ
µ
N
Y
H
From the n = 48 observations we obtain the estimates 
96
.
18
ˆ
and
125
.
55
ˆ
 
 
V
P
.
Approximate cell probabilities in the four cells are (Z denote a 
variable
)1,0
(
−
N
):


2125
.0
96
.
18
125
.
55
40
ˆ
40
ˆ
 
¸
¹
·
¨
©
§


 

Z
P
Y
P


3889
.0
96
.
18
125
.
55
40
ˆ
96
.
18
125
.
55
60
ˆ
60
40
ˆ
 
¸
¹
·
¨
©
§



¸
¹
·
¨
©
§


 


Z
P
Z
P
Y
P


3038
.0
96
.
18
125
.
55
60
ˆ
96
.
18
125
.
55
80
ˆ
80
60
ˆ
 
¸
¹
·
¨
©
§



¸
¹
·
¨
©
§


 


Z
P
Z
P
Y
P




0948
.0
96
.
18
125
.
55
80
ˆ
1
80
ˆ
1
80
ˆ
 
¸
¹
·
¨
©
§



 


 
!
Z
P
Y
P
Y
P
Multiplying these probabilities by n = 48 gives the expected cell frequencies.
17
.0
5.4
)
5.4
5
(
6.
14
)
6.
14
14
(
7.
18
)
7.
18
18
(
2.
10
)
2.
10
11
(
2
2
2
2
2
 







 
X
p-value= e= 

68
.0
17
.0
)1
2
4
(
2
 
!


F
P
,  No reason to reject 
0
H .
EX 113 
)
(
:
4
3
2
1
0
p
p
p
p
p
H
=
=
=
=
We get the following proportions of born boys
Season
Spring
Summer
Autumn
Winter
Over all ( pˆ )
Proportion
0.51780
0.51707
0.51414
0.51475
0.516055
∑
=
=
+
+
+
=
−
4
1
2
147982
.0
025490
.0
052297
.0
015844
.0
054351
.0
)ˆ
ˆ
(
i
i
i
p
p
n


10
.0
59
.0
)1
4
(
value
-
p
59
.0
)
516055
.0
1(
516055
.0
147982
.0
2
!!
!

 

 

 
F
P
T
.  There is no reason to 
reject 
0
H , even though the sample is very large.
EX 114 For Group1 and Group2 
0
2 =
X
, whereas for Group (1+2) (1+2)
05
.0
value
-
p
01
.5
2


 
X
.  In the 
latter case the combination of data from tables with unequal proportions and marginal frequencies has created an 
impression of association which in fact does not exist.
This example illustrates that it is possible to ‘create non-significance’ by searching for sub-groups where no 
association is found. On the other hand, you may find significant associations in sub groups while no significant 
association is found in the total group. 
The problem can be settled by a clear definition of the population to be studied. 
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
173 
Answers to Supplementary Exercises
EX 115 
a)	 The proportion positive among husbands is roughly twice as large as among wives, 0.30 and 0.16, respectively. 
But is the difference significant?
Using the Chi-square principle in the form of McNemar’s test gives 

 


 
76
.5
10
24
)
10
24
(
2
2
X
p-value < 0.05 (p-value = 0.016). There is a significant difference.
To apply the LR test (cf. EX 78) we need the estimates 
10
.0
,
24
.0
,
17
.0
100
2
10
24
ˆ
12
21
 
 
 


 
p
p
p
^
`

 




 
/

9340
.5
)
10
.0
ln(
10
)
24
.0
ln(
24
)
17
.0
ln(
)
10
24
(
2
ln
2
 p-value < 0.05  
(p-value =0.015)
b)	 The ordinary Chi Square test of independency yields 


10
.0
48
.0
)1(
value
-
p
48
.0
2
2
!!
!
 

 
F
P
X
. 
The opinions of husbands and wives are independent.
 EX 116 
:
0
H
No association between Working capacity and Type of training.
The total Chi-square measure is 
0.001
value
-
p
71
.
65
2


 
X
.  There is thus a strong association between the 
two factors.
The next step is to search for the combination of factors that are ‘most responsible’ for the high Chi-square measure. 
This can be done by considering the table of deviations and cell Chi-square measures and then apply the Bonferroni-
Holm principle described in Ch. 6.4.
Table showing Deviation / Cell Chi-square
Low
Medium
High
Physical
-38.4 / 9.35
35.75 / 28.88
2.62 / 0.37
Activation
43.26 / 5.85
-39.91 / 17.71
-3.36 / 0.30
Education
-4.90 / 0.86
4.16 / 2.20
0.74 / 0.17
From this we get the table of ranked Cell Chi-square measures
i
1
2
3
4
Cell Chi-square
28.88
17.71
9.35
5.85
p-value
<0.001
<0.001
0.0022
0.015
)1
3
3
/(
05
.0



i
0.0056
0.0063
0.0071
0.0083
Here the first three p-values are smaller than the value in the bottom row. The corresponding Cell Chi-square 
measures are thus significant after adjustment for multiple significance. There is an over-representation for the 
combination ‘Medium working capacity’ x ‘Physical training’ and also an under-representation for the combinations 
‘Medium working capacity’ x ‘Activation’ and ‘Low working capacity’ x ‘Physical training’.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
174 
Answers to Supplementary Exercises
EX 117 The hypothesis to test is 
4.0
:
0
=
p
H
.
An estimate of p is 
30
.0
12
10
6
4
ˆ
ˆ
 




 
 
 
¦
¦
¦
¦


i
i
i
i
i
n
y
n
p
n
p
 (cf. EX 46b) ) and an estimate of the variance of 
the latter is 
0021
.0
100
)
30
.0
1(
30
.0
)ˆ
1(ˆ
)ˆ
(ˆ
 

 

 ¦
in
p
p
p
V
.
As a test statistic we take 
182
.2
0021
.0
40
.0
30
.0
)ˆ
(ˆ
4.0
ˆ

 

 

 
p
V
p
T
.  From EX 23 c) it follows that we can use the 
normal distribution to compute the (two-sided) p-value 


03
.0
015
.0
2
182
.2
2
 

 
!

Z
P
. The conclusion is 
that University employees are one sick leave to a less extent than the rest of the population.
EX 118 Introduce the notations:
=
)
2
(
X
 Number of accidents during 2 weeks before the renewal, with intensity
X
λ
.
 
)
3
(
Y
                      -“-                     3      “    after                   -“-                        
Y
O .
Then 

)
3
2
3
,
15
(
~
15
)3
(
)
2
(
)3
(
Y
X
Y
p
n
Binomial
n
Y
X
Y
O
O
O

 
 
 
 

.
 We want to test
5
3
:
0
=
⇔
=
p
H
Y
X
λ
λ
 . The observed number of accidents is 5 and therefore a one-sided 
p-value is obtained by by 

0338
.0
)
5
/
2
(
)
5
/
3
(
5
15
)
5
/
2
(
)
5
/
3
(
0
15
5
)3
(
10
5
15
0
 
¸¸
¹
·
¨¨
©
§


¸¸
¹
·
¨¨
©
§
 
d

Y
P
.
The computation of the last expression is simplified by using tables over the Binomial distribution, e.g. Table 1 p839 
in Wackerly et al 2007.
For a two-tailed test we also have to compute the probability of extremely large values. Since the expected number 
of accidents is 
9
5
/
3
15
 

 
 p
n
,  we compute 
 
t13
)3
(
Y
P
0271
.0
)
5
/
2
(
)
5
/
3
(
15
15
)
5
/
2
(
)
5
/
3
(
14
15
)
5
/
2
(
)
5
/
3
(
13
15
0
15
1
14
2
13
 
¸¸
¹
·
¨¨
©
§

¸¸
¹
·
¨¨
©
§

¸¸
¹
·
¨¨
©
§
.
The p-value for a two-tailed test is 0.0338+0.0271 > 0.05 and the conclusion is that the effect of renewed equipment 
is not significant.
EX 119 
4
/
1
:
0
=
ip
H
. The sum of observations is 80, so 
20
4
/
80
/
 
 
k
n
.  The test statistic is 


10
.0
10
.2
)1
4
(
value
-
p
10
.2
20
)
20
25
(
20
)
20
20
(
20
)
20
16
(
20
)
20
19
(
2
2
2
2
2
2
!
!

 

 







 
F
P
X
0
H is not rejected, the observations may be generated by the same Poisson variable.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
175 
Answers to Supplementary Exercises
EX 120 The likelihood is 
y
y
n
e
L
i
ML
y
n
i
1
ˆ
 
 

 
¦
¦

O
O
O
 (cf. EX 41)
The LR is 
)
1(
0
0
0
0
0
0
0
)
(
)
(
)
/
1(
ˆ
y
n
n
n
y
n
n
n
n
y
n
e
y
e
y
e
y
e
L
L
i
λ
λ
λ
λ
λ
λ
−
+
−
−
−
=
=
=
=
Λ
∑
. 
0
H is rejected for small values of Λ. Which values 
of 
y
0
λ
 will make Λ small? To answer this consider the function 
⇒
=
−)
1(
)
(
x
n
ne
x
x
g


)
1(
)
ln(
)
(
ln
x
n
x
n
x
g


 
 which has max/min for the same values as 
)
(x
g
(cf. Ch. 2.3.3).







 
 

 

 
0
)
(
ln
,1
0
)
(
ln
2
2
2
x
n
dx
x
g
d
x
n
x
n
dx
x
g
d
 Local max for 
1
0
=
=
y
x
λ
. Thus 
0
H  rejected for extremely 
large or small values of 
y
0
λ
, or equivalently for large values of ∑
iy
0
λ
, but how large or small?
To this end we use property (5) in Ch. 2.2.2.
)
2
(
~
2
)
,
(
~
)
(
~
2
1
0
1
n
Y
n
Gamma
Y
l
Exponentia
Y
n
i
i
n
i
i
i
χ
λ
λ
λ
∑
∑
=
=
⇒
⇒
. By using the latter relation probabilities can 
easily be computed from the Chi-square distribution. The RR is thus of the form 
∑
∑
>
<
2
0
1
0
2
or 
 
2
C
Y
C
Y
i
i
λ
λ
, 
where 
2
1
 
and
 
C
C
are constants that are determined in the following way:
The test is two-sided at the 5% level and n = 10 which yields


5908
.9
025
.0
)
10
2
(
1
1
2
 

 


C
C
P F
 and 

1696
.
34
025
.0
)
10
2
(
2
2
2
 

 
!

C
C
P F
.
The RR is ¦
¦
!

0
0
2
1696
.
34
or 
2
5908
.9
O
O
i
i
Y
Y
.
Notice that the sample has not yet been collected and nor has the value of 
0
λ  been specified. Once this has been 
the case the test can be performed.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master’s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master’s programmes
• 33rd place Financial Times worldwide ranking: MSc 
International Business
• 1st place: MSc International Business
• 1st place: MSc Financial Economics
• 2nd place: MSc Management of Learning
• 2nd place: MSc Economics
• 2nd place: MSc Econometrics and Operations Research
• 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‘Beste Studies’ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Exercises in Statistical Inference  
with detailed solutions
176 
Answers to Supplementary Exercises
EX 121 The Likelihood without restrictions on the parameters is
∑
∑
−
−
=
=
−
−
=
⋅
=∏
∏
i
Y
i
X
Y
X
X
Y
i
Y
i
X
y
x
n
Y
n
X
n
i
n
i
y
Y
x
X
e
e
e
e
L
λ
λ
λ
λ
λ
λ
λ
λ
1
1
 and the Likelihood under
0
H is
∑
∑
+
−
+
=
)
(
)
(
0
i
i
Y
X
y
x
n
n
e
L
λ
λ
.
¦
¦
¦
¦
 

 

 




 
i
X
X
i
X
X
X
i
Y
i
X
Y
Y
X
X
x
n
x
n
d
L
d
y
x
n
n
L
O
O
O
O
O
O
O
ˆ
0
ln
ln
ln
ln
.  
Similarly, 
∑
=
i
Y
Y
y
n
λˆ
.




¦
¦
¦
¦
¦
¦


 




 




 
i
i
Y
X
i
i
Y
X
i
i
Y
X
y
x
n
n
y
x
n
n
d
L
d
y
x
n
n
L
O
O
O
O
O
ˆ
)
(
ln
ln
)
(
ln
0
0
⇒
=
⋅
=
=
Λ
+
−
−
+
−
+
Y
X
Y
X
Y
X
Y
X
Y
X
Y
X
n
Y
n
X
n
n
n
n
n
n
n
Y
n
X
n
n
e
e
e
L
L
λ
λ
λ
λ
λ
λ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
)
(
)
(
)
(
0
^
`
Y
Y
X
X
Y
X
n
n
n
n
O
O
O
ˆ
ln
ˆ
ln
ˆ
ln
)
(
2
ln
2




 
/

 which is distributed 
)1
2
(
2
−
χ
under 
0
H .
The estimates are 

 


 
 
 
 
 
67
.1
40
20
60
40
ˆ
,
50
.1
40
60
ˆ
,
00
.2
20
40
ˆ
O
O
O
Y
X


16
.0
9425
.1
)1(
value
-
p
9425
.1
)
9712
.0
(
2
ln
2
2
 
!
 

 


 
/

F
P
 > 0.05.
There is no reason to reject 
0
H
The link between the Poisson process and exponentially distributed intervals was stated in property (4)
Ch. 2.2.2. In EX 86 it was shown how to test the equality between two Poisson rates based on count data (frequency 
of occurrences). In the present example we have shown how to perform the same test based on interval data.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
177 
Answers to Supplementary Exercises
EX 122
a)	 The unrestricted Likelihood is 
⇒
−
=
−
=
∑
−
=
−
∏
n
n
y
n
i
y
p
p
p
p
L
i
i
)
1(
)
1(
1
1




y
y
n
p
p
n
p
n
y
dp
L
d
p
n
p
n
y
L
i
i
i
1
ˆ
0
)1
(
)
1(
ln
ln
)
1
ln(
ln
 
 

 




 




 
¦
¦
¦
.
The Likelihood under 
0
H is
∑
∑
=
=
−
i
i
y
n
n
y
L
)
2
/
1(
)
2
/
1(
)
2
/
1(
0
. (No parameters need to be estimated.)
n
n
y
y
y
y
L
L
i
i











−
=
=
Λ
∑
∑
−
1
1
1
)
2
/
1(
ˆ
0
=[After some simplification, but not needed.]
∑
∑
−
−
=
n
y
y
i
i
y
y
)1
(
)
2
/
(
.


^
`)1
ln(
)
2
/
ln(
2
ln
2





 
/

¦
¦
y
n
y
y
y
i
i
 which is distributed 
)
0
1(
2
−
χ
under 
0
H .
b)	
^
`
0534
.5
)1
5
/
8
ln(
)
50
80
(
)
8.0
ln(
80
2
ln
2
 




 
/

. p-value 


02
.0
0534
.5
)1(
2
 
!
 
F
P
<0.05.
0
H is rejected and since 
5.0
625
.0
80
/
50
ˆ
!
 
 
p
 we draw the conclusion that p is significantly larger than 1/2.
c)	 There are many situations where we can collect data on the variable 
iY = ‘Number of trials until an (0, 1) -event 
occurs for the first time’ in order to test the hypothesis that p =1/2. An example is a sequence of ups and downs 
on the stock market.
EX 123
a)	 From the data we get 
3265
.7
,
2375
.2
,
16
2  
 
 
D
D
s
y
n
. Thus, 


004
.0
0024
.0
2
307
.3
)1
16
(
2
value
-
p
307
.3
16
/
3265
.7
0
2375
.2
 

 
!


 

 

 
T
P
T
.  The conclusion is 
that the weight loss program had a significant positive effect. 
b)	 Define T = ‘Number of positive signs’. Under 
( )
( )
2
/
1
:
0
=
−
=
+
P
P
H
the variable T is distributed 
)
2
/
1
,
16
(
 
 
p
n
Binomial
.  In the data we observe 
13
 
T
 and a one-sided p-value is


0106
.0
16
16
15
16
14
16
13
16
)
2
/
1(
)
2
/
1(
)
2
/
1(
16
13
16
13
16
16
 
¿
¾
½
¯
®
­
¸¸
¹
·
¨¨
©
§

¸¸
¹
·
¨¨
©
§

¸¸
¹
·
¨¨
©
§

¸¸
¹
·
¨¨
©
§
 
¸¸
¹
·
¨¨
©
§
 
t
¦
 

y
y
y
y
T
P
.
However, to compute a two-sided p-value we should also consider the possibility that the outcome may be in the 
‘opposite direction’. Since the expected value of T is 16/2 = 8, the ‘opposite direction’ consists of the outcomes
3
≤
T
.


0106
.0
3
16
2
16
1
16
0
16
)
2
/
1(
)
2
/
1(
)
2
/
1(
16
3
3
0
16
16
 
¿
¾
½
¯
®
­
¸¸
¹
·
¨¨
©
§

¸¸
¹
·
¨¨
©
§

¸¸
¹
·
¨¨
©
§

¸¸
¹
·
¨¨
©
§
 
¸¸
¹
·
¨¨
©
§
 
d
¦
 

y
y
y
y
T
P
.  (This result is to be expected 
since the Binomial distribution is symmetric for p = 1/2.)
The two-sided p-value is thus 0.02, which is much larger than 0.004 in a), but still less than 0.05.
c) 




 
¸¸
¹
·
¨¨
©
§




|


 
t

 
 
4
/
16
2
/
1
2
/
16
13
1
13
1
13
)
2
/
1
,
16
(
~
Z
P
T
P
T
P
p
n
Binomial
T
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
178 
Answers to Supplementary Exercises
EX 123 (Continued)




0122
.0
25
.2
25
.2
1
 
!
 


 
Z
P
Z
P
.
Similarly 



0122
.0
25
.2
4
/
16
5.0
2
/
16
3
3
 


 
¸¸
¹
·
¨¨
©
§



|
d
Z
P
Z
P
T
P
 because of symmetry.
The two-sided p-value is 
02
.0
0122
.0
2
 

.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Exercises in Statistical Inference  
with detailed solutions
179 
Answers to Supplementary Exercises
EX 124 
a)	 Introduce the notations 
M
S
p
p
 
and
 
for the proportion of products that are classified ‘Bad’ by State authorities 
and Municipal authorities, respectively. The observed difference is 
=
−
M
S
p
p
ˆ
ˆ
10
.0
100
)
10
20
(
100
)
20
20
(
 



,  but is the difference significant?
M
S
p
p
H
=
:
0
McNemar’s test (Cf. EX 71) yields: 

 
!
 

 


 
33
.3
)1(
value
-
p
33
.3
10
20
)
10
20
(
2
2
2
F
P
X
0.068. 
We can’t reject the null hypothesis at the 5% level.
b)	
:
0
H
Independency between the two types of classifications. (It’s close to an insult to set up this hypothesis.)
The ordinary Chi-square test of independency yields:


005
.0
7.
12
)1(
value
-
p
7.
12
42
)
42
50
(
28
)
28
20
(
18
)
18
10
(
12
)
12
20
(
2
2
2
2
2
2

!
 

 







 
F
P
X
There is strong reason to reject
0
H .
EX 125 The pf. is
λ
λ
−
=
e
y
y
p
y
!
)
(
. λˆ 
3225
.1
2
143
103
2
6
143
1
103
0
=
+
+
+
⋅
+
+
⋅
+
⋅
=
…
…
. (This is simply
y
=
λˆ
.)
Now, 
=
=
−λˆ
)
0
(ˆ
e
p
0.2667,
3527
.0
ˆ
)1(ˆ
ˆ =
⋅
=
−λ
λ e
p
, and so on. In this way we get the following table, where Y = 
Observed frequency, 
=
)
(ˆ y
p
n
Expected frequency and n = 400.
y
0
1
2
3
4
5
6
Total
)
(ˆ y
p
.2667
.3527
.2332
.1028
.0337
.0090
.0019
1
)
(ˆ y
p
n
106.7
141.1
93.3
41.1
13.5
3.6
0.7
400
Y
103
143
98
42
8
4
2
400
(
)
)
(ˆ
)
(ˆ
2
y
p
n
y
p
n
Y −
0.127
0.026
0.237
0.020
2.241
0.044
2.414
5.109
p-value 


40
.0
109
.5
)1
1
7
(
2
 
!


 
F
P
,  so there is no reason to reject the Poisson distribution. The degrees of 
freedom in the Chi-square distribution is due to the fact that there are 7 cells and 1 parameter has been estimated.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
180 
Answers to Supplementary Exercises
EX 126 A 2 x 3 frequency table is
Morning
Day
Night
Total
Defective
12
10
23
45
Not defective
188
190
177
555
Total
200
200
200
600
The table with Deviation / Cell Chi-Square (Cf. Ch. 6.2.1) is
Morning
Day
Night
Defective
-3 / 0.6
-5 / 1.6667
8 / 4.2667
Not defective
3 /0.0486
5 /0.1351
-8 / 0.3459
The total sum of Cell Chi-Square is 
q


0293
.0
0631
.7
))
1
2
)(
1
3
((
value
-
p
0631
.7
3459
.0
6.0
2
2
 
!


 

 


 
F
P
X

. We reject the hypothesis 
of no association. 
In the last table there is a large excess of observations (+8) in the cell Defective x Night. The p-value for this Cell Chi-
Square is (
)
0389
.0
2667
.4
)1(
2
=
>
χ
P
< 0.05. However, there are deviations in 6 cells to take account of. Since 0.0389 
> 0.05/6 (Cf. Ch, 6.4.) we can’t claim that there is a significant over-representation of observations in the cell Defective 
x Night after having adjusted p-values for multiple comparisons.
EX 127 There are three differences between proportions to consider. The largest difference is for the proportion 
defective in Night and Day, 
065
.0
200
/
10
200
/
23
 

. The corresponding test statistic for testing that the true 
difference is zero is (Cf. EX 83 a).)


018
.0
36
.2
2
value
-
p
36
.2
200
200
10
23
ˆ
)
200
/
1
200
/
1
)(
ˆ
1(ˆ
065
.0
 
!
 

 
»¼
º
«¬
ª


 
 


 
Z
P
p
p
p
T
<0.05.
However, since there are three comparisons to make we should require that 0.018 is less than 
017
.0
3
/
05
.0
|
 
(Cf. Ch. 6.4.). The difference between defectives in Night and Day is approximately significant after adjustment for 
multiple comparisons. The other two differences are not.
EX 128 Let 
F
I
p
p
 
and
 
be the proportion left-handed among identical- and fraternal twins, respectively. We want to 
test 
)
(
:
0
p
p
p
H
F
I
=
=
. The test statistic for this is
(
)
F
I
F
I
n
n
p
p
p
p
T
/
1
/
1
)ˆ
1(ˆ
0
ˆ
ˆ
+
−
−
−
=
which is distributed 
)1,0
(
N
in large samples. 
246
18
ˆ
,
248
41
ˆ
 
 
F
I
p
p
,


002
.0
153
.3
2
value
-
p
153
.3
246
248
18
41
ˆ
|
!
 

 



 
Z
P
T
p
.  There is thus a strongly significant difference 
between the proportions.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
181 
Answers to Supplementary Exercises
EX 129 We give two types of solutions, one ‘Straight’ as in EX 85 a) and one in a ‘Bio-statistical style’.
Straight solution: The two estimated proportions to be compared are
473000
114
ˆ 
and
 
20000
1
ˆ
2
1
=
=
p
p
.
473000
20000
114
1
ˆ
+
+
=
p
. 






+
−
−
=
473000
1
20000
1
)ˆ
1(ˆ
ˆ
ˆ
2
1
p
p
p
p
T
takes the value -1.733
(
)
042
.0
733
.1
value
-
p
=
>
=
⇒
Z
P
< 0.05. (In this case it seems reasonable to use a one-sided test.)
Hint: If you have problems to perform calculations with very small numbers in both numerator and denominator in T, 
just multiply T by e.g. 
1000
1000 .
Solution in a Bio-statistical style: Let Y =’Number of polio cases among 20000 vaccinated children’.
Under the hypothesis that the SALK vaccine has no effect 
)
473000
114
,
20000
(
~
=
=
p
n
Binomial
Y
and approximately 
)
8.4
(
~
 
 np
Poisson
Y
O
. The latter variable has expected value 4.8 and therefore the 
hypothesis of no effect is rejected for small values of Y. 
p-value = (
) ∑
=
−
−
=
+
=
=
≤
1
0
8.4
8.4
046
.0
)
8.4
1(
!
8.4
1
y
y
e
e
y
Y
P
< 0.05.
In this case the group under study (vaccinated children) has been exposed to a standard value of a parameter 
(p = 114/473000) and the outcome of this is evaluated. Such an approach is very common in bio-statistical studies, 
especially in epidemiology.
EX 130 Table of Deviation / Cell Chi-Square:
Low
Middle
High
Cheap
-14.3/ 3.93
20.0/ 5.86
-5.6/ 0.9
Expensive
14.3/ 4.31
-20/ 6.44
5.6/ 1.0
The total Chi-square measure is 
05
.0
00001
.0
value
-
p
4.
22
0.1
93
.3
2

 

 


 

X
.  There is thus 
strong evidence against independency.
The largest Cell Chi-Square is 6.44 and p-value = 

.
05
.0
011
.0
44
.6
)1(
2

 
!
F
P
 For a multiple comparison in 6 cells 
it is required that the latter is less than 0.05/6 = 0.008, which is nearly true. No further significant patterns can be 
seen.
The conclusion is that there is a significant under-representation of Middle- class families with Expensive electronic 
equipment.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
182 
Answers to Supplementary Exercises
EX 131 Let Y = ‘Yearly number of cases in Malmö’
)
,
110000
(
~
p
n
Binomial
=
. We want to test 
100000
/
35
:
0
 
p
H
.  Since p is small,
)
5.
38
100000
/
35
110000
(
~
 

 
O
Poisson
Y
 under
0
H .
The expected value of Y is 38.5 and the observed value is 60. It is therefore natural to compute the p-value as 


O
O

f
 ¦
 
t
e
y
Y
P
y
y
60 !
60
,  but this is a heavy task. Instead we use the fact that for large λ , a Poisson variable can be 
approximated by a
)
,
(
λ
λ
N
-variable (Cf. Ch. 2.2.2.).
p-value 
	  
(
)
(
)
0002
.0
47
.3
5.
38
5.
38
60
60
=
>
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−
>
≈
≥
=
Z
P
Z
P
Y
P
The conclusion is that inhabitants in Malmö have a significant higher risk for malignant melanoma than the rest of 
the Swedish population.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Exercises in Statistical Inference  
with detailed solutions
183 
Answers to Supplementary Exercises
EX 132 
a)	 As the question is formulated, a one-sided test seems to be appropriate. On the other hand, if the problem 
was to find out which of the two importers is best, a two-sided test is preferably.
b)	 The two estimated proportions are 
050
.0
200
10
ˆ
and
0.095
200
19
ˆ
2
1
 
 
 
 
p
p
.  To test 
)
(
:
2
1
0
p
p
p
H
=
=
we use the test statistic 
200
200
10
19
ˆ
 where
)
/
1
/
1
)(
ˆ
1(ˆ
0
ˆ
ˆ
2
1
2
1


 




 
p
n
n
p
p
p
p
T
.
This gives 


05
.0
0413
.0
735
.1
value
-
p
735
.1

 
!
 

 
Z
P
T
. The new importer is better!
c)	 c) We construct the following 2 x 2 table:
Quality
Bad
Acceptable
Total
Importer
Former
19
181
200
New
10
190
200
Total
29
371
400
(
)
0826
.0
0114
.3
)1(
value
-
p
0114
.3
1092
.0
3966
.1
1092
.0
3966
.1
2
2
=
>
=
⇒
=
+
+
+
=
χ
P
X
.
Notice that the p-value in c) is twice that of b). The Chi-square test is by its nature a two-sided test.
It has been demonstrated that the test of equality between Binomial proportions in EX 83 a) is equivalent to the Chi-
square test of independence if the test is two-sided. The Chi-square test can also be used as a one-sided test if the 
p-value is divided by 2. However, both tests are approximate and require that sample sizes are large.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
184 
Answers to Supplementary Exercises
EX 133 We first consider one-sided vs. two-sided tests.
Most people would probably agree that vaccination could not have a negative effect on the state of health. The test 
should thus be one-sided. 
(However, there may be other opinions on this issue. Some might e.g. argue that vaccine can be contaminated.)
The test for equality of two Binomial proportions yields


000025
.0
0476
.4
value
-
p
0476
.4
66
1
90
1
156
22
1
156
22
66
/
18
90
/
4
|
!
 


 
¸
¹
·
¨
©
§

¸
¹
·
¨
©
§ 

 
Z
P
T
.
From the 2 x 2 table the following table over Deviation / Cell Chi-Square is constructed:
Diseased
Not diseased
Vaccinated
-8.7 / 5.9529
8.7 /0.98
Not vaccinated
8.7 / 8.1176
-8.7 / 1.33
From this, 
000050
.0
)
38
.
16
)1(
(
value
-
p
38
.
16
2
2
 
!
 

 
F
P
X
.  The latter is the result of a two-sided test. To 
get a one-sided p-value we simply divide it by 2.
In the last table there are two significant Cell Chi-Square. 

00125
.
4
05
.0
0044
.0
1176
.8
)1(
2
 

 
!
F
P
 and 


0166
.0
3
05
.0
0147
.0
9529
.5
)1(
2
 

 
!
F
P
 (Cf. Ch. 6.4.).
EX 134 
a)	
(
)
0178
.0
6197
.6
)1(
value
-
p
6197
.6
2
2
=
>
=
⇒
=
χ
P
X
(two-sided test). For a one-sided test the 
p-value is halved, 0.0139.
b)	 p-value = 
0123
.0
!
47
!
82
!0
!
16
1
!
46
!
83
!1
!
15
1
!
145
!
16
!
129
!
47
!
98
 
¸
¹
·
¨
©
§










 (one-sided test)
EX 135
a)	 We first test whether the population variances of the two bulbs are equal (Cf. EX 88.).


22
.0
79
.1
)
19
,
19
(
2
value
-
p
79
.1
)
62
(
)
83
(
2
2
 
!

 

 
 
F
P
T
.  The variances can be considered equal.
The pooled sample variance is 
5367
)1
20
(
)1
20
(
)
83
)(
1
20
(
)
62
)(
1
20
(
2
2
2
 






 
S
.
The test statistic for testing that the two population means are equal is


 


 
66
.4
)
20
/
1
20
/
1(
5367
1236
1128
T


00004
.0
66
.4
)1
20
1
20
(
2
value
-
p
 
!




 
T
P
.  The difference is 
clearly significant.
b)	 Referring to EX 88, we get 


00001
.0
66
.4
2
value
-
p
66
.4
20
/
)
83
(
20
/
)
62
(
1236
1128
2
2

!

 


 


 
Z
P
T
.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
185 
Answers to Supplementary Exercises
EX 136 Let 
B
A
Y
Y
 
and
 
be the yield from variety A and B, respectively. Form the difference 
B
A
Y
Y
D
−
=
and 
compute the estimates 
7882
.6
and
33
.1
2  
 
ds
d
.  The hypothesis that the difference between the means is zero 
is the same as that
0
)
(
=
D
E
. This is tested by 
77
.1
12
/
7882
.6
0
33
.1
 

 
T

 
!


 

77
.1
)1
12
(
2
value
-
p
T
P
 0.10. We can’t claim that the difference is significant.
EX 137 The ranked series are:
A
22
28
38
40
40
41
44
46
48
51
58
60
B
12
15
18
21
23
24
29
35
36
43
54
80
The combined ranked series is easily shown to have the sample median 
39
2
40
38
 

 
m
.  This leads to the 2 x 2 
frequency table
A
B
Total
>39
9
3
12
<39
3
9
12
Total
12
12
24
The hypothesis of interest is
0
H :No association between Type of series and Distribution around sample median. 
(
)
0143
.0
0.6
)1(
value
-
p
0.6
2
2
=
>
=
⇒
=
χ
P
X
. The two series have significantly different medians.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet’s 
electricity needs. Already today, SKF’s innovative know-
how is crucial to running a large proportion of the 
world’s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Exercises in Statistical Inference  
with detailed solutions
186 
Answers to Supplementary Exercises
EX 138 
a)	 Remember the conditions for the Binomial distribution in Ch. 2.2.1,’ independent repetitions of the same 
experiment’. The taster has thus to spit out the beer after each tasting. He also has to reset his memory, so that 
he can’t compare the taste of a new glass with the taste of the preceding one. Therefore, a so called wash-out 
period is needed between tastings. The arrangement of the glasses is easily done by tossing a coin.
The hypothesis 
2
/
1
:
0
=
p
H
means that we assume that the taster is merely guessing. 
b)	 From the Binomial distribution we get (
)
004
.0
2
1
2
1
2
1
8
8
8
8
0
8
=
=






=
=
Y
P
< 0.05,
(
)
(
)
(
)
(
)
035
.0
1
8
2
1
2
1
2
1
2
1
7
8
8
7
7
8
8
7
=
+
=
+






=
=
+
=
=
≥
Y
P
Y
P
Y
P
< 0.05. For y smaller than 7 we get 
probabilities that are larger than 0.05. The smallest acceptable value of y is thus 7.
c)	 We have to solve x in the relation 



 


 
¸
¸
¹
·
¨
¨
©
§


!
|
!
645
.1
5
50
05
.0
)
2
/
1
)(
2
/
1(
100
)
2
/
1(
100
x
x
Z
P
x
Y
P
59
 
x
 will be enough.
EX 139 
a)	 From EX 4 we get
y
0
1
2
3
4
Sample cdf
6/160
44/160
102/160
149/160
160/160
)
(y
p
0.056
0.235
0.374
0.265
0.070
)
(
0 y
F
0.056
0.291
0.665
0.930
1.000
Here,   
)
(
)
0
(
)
(
0
y
p
p
y
F
+
+
=
…
.
The largest absolute difference between the sample cdf and 
)
(
0 y
F
is 
028
.0
665
.0
160
/
102
160
=
−
=
D
.
The critical value for a two-sided test at the 5 % level is 
11
.0
160
/
36
.1
 
 (Cf. Ch. 6.2.4) > 0.028 so there is no 
reason to reject the Binomial distribution.
b)	  
y
0
1
2
3
4
Expected frequency
9.0
37.6
59.8
42.4
11.2
Observed frequency
6
38
58
47
11
Here, Expected frequency is 
4,3,2,1,0
 ),
(
160
=
⋅
y
y
p
.

 









 
56
.1
2.
11
)
2.
11
11
(
4.
42
)
4.
42
47
(
8.
59
)
8.
59
58
(
6.
37
)
6.
37
38
(
0.9
)
0.9
6
(
2
2
2
2
2
2
X
p-value e


67
.0
56
.1
)1
1
5
(
2
 
!


 
F
P
 and there is again no reason to reject the Binomial distribution.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
187 
Answers to Supplementary Exercises
EX 140 
a)	 The ranked series and sample cdf’s are:
Day1 (15 observations):
y
30.8
31.0
31.4
32.9
33.3
33.4
33.5
)
(
15 y
S
0.067
0.133
0.200
0.267
0.400
0.467
0.533
y
33.7
34.3
34.4
34.8
34.9
36.2
37.0
)
(
15 y
S
0.600
0.667
0.733
0.800
0.867
0.933
1.000
Here, 0.067 = 1/15, 0.133 = 2/15, and so on. Notice that there are two observations at y = 33.3.
Day2 (10 observations):
y
28.4
28.7
30.2
30.7
31.3
31.9
32.1
32.4
32.8
)
(
10 y
S
0.100
0.200
0.400
0.500
0.600
0.700
0.800
0.900
1.000
For 
20
.0
)
(
,9.
32
4.
31
15
 

d
y
S
y
 and for 
000
.1
)
(
,8.
32
10
 
t
y
S
y
. The largest absolute difference between 
the two cdf’s is 
80
.0
00
.1
20
.0
10
,
15
 

 
D
.
From tables over the Smirnov two-sample distribution it is seen that the critical values for rejecting the hypothesis of 
equal cdf’s are 15/30 (5% level) and 19/30 (1% level). The observed absolute difference of 0.80 is larger than both of 
these, so the hypothesis of equal population distributions can be rejected at least at the 1% level.
b)	 From the data we get 	
Day 1: Mean = 33.66, Variance = 3.05
	
	
	
	
Day2: Mean =30.87, Variance = 2.28
As in EX 86 we first test whether the population variances of the two series are equal.



 
!


 

 
 
34
.0
34
.1
)1
10
,1
15
(
value
-
p
34
.1
28
.2
05
.3
F
P
F
 Population variances can be assumed to 
be equal. The pooled estimate of the common variance is 
75
.2
)1
10
(
)1
15
(
28
.2
)1
10
(
05
.3
)1
15
(
2
 








 
s
.
To test whether the population means are equal,





 
!




 

 


 
0004
.0
18
.4
)1
10
1
15
(
2
value
-
p
18
.4
10
/
1
15
/
1
75
.2
87
.
30
66
.
33
T
P
T
 Population 
means differ significantly.
According to both tests in a) and b) the population distribution of viscosity has changed from one day to another. 
The test in b) gave a somewhat stronger rejection of equal means. On the other hand, the test in a) is free from 
assumptions about the population distribution. The latter is to prefer in the absence of evidence for that viscosity is 
normally distributed.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
188 
Answers to Supplementary Exercises
EX 141 Introduce the notations 
°¯
°®
­


B
for 
yield
equals
A 
for 
yield
if
0
A
n
higher tha
is
B
for 
yield
if
B
n
higher tha
is
A 
for 
yield
if
. We now get the following pattern:
Area
1
2
3
4
5
6
7
8
9
10
11
12
Sign
+
-
+
-
+
-
+
+
+
-
+
0
Let Y = ‘Number of minus signs’ of n = 11’. (The observation with a 0 is deleted.) Under the hypothesis of no difference 
in yield,
)
2
/
1
,
11
(
~
 
 
p
n
Binomial
Y
.
p-value = 

 
»
¼
º
«
¬
ª
¸¸
¹
·
¨¨
©
§

¸¸
¹
·
¨¨
©
§

¸¸
¹
·
¨¨
©
§

¸¸
¹
·
¨¨
©
§

¸¸
¹
·
¨¨
©
§
 
¸¸
¹
·
¨¨
©
§
 
d

 ¦




























\
\
\
\
<
3



27
.0
330
165
55
11
1
2048
1
 




.  A two-sided p-value is obtained by doubling the latter value. In neither case 
the difference is not significant.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Exercises in Statistical Inference  
with detailed solutions
189 
Answers to Supplementary Exercises
EX 142 We want to test the hypothesis of no association between the two series. Let 
id be the difference between 
the ranks for the i:th student.
¦
 














 
146
9
16
16
9
1
14
16
25
2
9
2
9
4
16
1
2
id
7393
.0
146
)1
15
(
15
6
1
2
 




 
Sr
.
From Table 11, Appendix 3 in Wackerly et al one gets the critical value 0.525  
(two-sided test, 
05
.0
 
D
 ). Since this is smaller than 0.7393 we reject the hypothesis of no association.
EX 143 Let 
after
 
Value
 
before,
 
Value
=
=
i
i
Y
X
for the i:th sample unit and put 
i
i
i
Y
X
D
−
=
. 
From the data we get 
)
0392
.0
(
001533
.0
,
03
.0
,
10
2
 
 
 
 
D
D
S
S
D
n
.
a)	


0156
.0
42
.2
2
value
-
p
42
.2
10
/
0392
.0
0
03
.0
 
!

 

 

 
Z
P
Z
.  There has been a significant decrease of 
the concentration due to the effect of the drug.
There are good reasons for assuming normality in this case. Notice that D involves a sum of 20 variables.
b)	 RR is 
n
D
/
0392
.0
96
.1
0

!

c)	
¸¸
¹
·
¨¨
©
§






¸¸
¹
·
¨¨
©
§



!
 
0392
.0
)
0
(
1
96
.1
0392
.0
)
0
(
1
96
.1
)
(
n
Z
P
n
Z
P
Pow
D
D
D
P
P
P
.   
For
100
=
n
 this will be larger than 0.90 if
013
.0
>
D
µ
.
EX 144 From EX 97–98 we obtain
¸
¸
¹
·
¨
¨
©
§









¸
¸
¹
·
¨
¨
©
§






!
 
)
1(
)
08
.0
(
)
1(
)
08
.0
1(
08
.0
96
.1
)
1(
)
08
.0
(
)
1(
)
08
.0
1(
08
.0
96
.1
)
(
p
p
n
p
p
p
Z
P
p
p
n
p
p
p
Z
P
p
Pow
By inserting p = 0.15 one gets the requirement 
90
.0
)
15
.0
(
 
Pow
,  an equation in n that has to be solved by ‘trial 
and error’. It is found that for n = 200, 
90031
.0
)
15
.0
(
 
Pow
.
RR is 
>
@
0376
.0
200
)
08
.0
1(
08
.0
96
.1
08
.0
ˆ
 
 
 


!

n
n
p
.
Now the study can begin and the sample is collected. In practice it would be wise to include somewhat more than 
200 persons in the sample due to non-responses or drop-outs.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
190 
Answers to Supplementary Exercises
EX 145 
)1
(
~
)1
(
2
2
2
−
−
=
n
S
n
T
χ
σ
. The RR is
0.05
 
and
 
or 
 
=
>
<
α
b
T
a
T
.
a)	
(
)
(
)
2
2
/
2
0
2
2
/
2
0
2
0
2
2
2
0
0
  ,
)1
(
)1
(
2
α
α
χ
σ
χ
σ
σ
χ
χ
σ
α
=
=
⇒






<
−
=
<
−
=
<
=
a
a
a
n
P
a
n
P
H
a
T
P
(
)
(
)
⇒






<
−
−
=






>
−
=
>
−
=
>
=
2
0
2
2
0
2
2
2
0
0
)1
(
1
)1
(
)1
(
2
σ
χ
σ
χ
χ
σ
α
b
n
P
b
n
P
b
n
P
H
b
T
P
2
2
/
1
2
0
2
2
/
1
2
0
2
0
2
  ,
2
1
)1
(
α
α
χ
σ
χ
σ
α
σ
χ
−
−
=
=
⇒
−
=






<
−
b
b
b
n
P
.
Here we have used the notation (
)
p
P
p =
<
2
2
χ
χ
.
The RR is thus
2
2
/
1
2
0
2
2
/
2
0
or 
 
α
α
χ
σ
χ
σ
−
>
<
T
T
.
(
)
(
)
(
)+
<
−
=
>
+
<
=
−
2
2
/
2
0
2
2
2
2
/
1
2
0
2
2
/
2
0
2
)1
(
)
(
α
α
α
χ
σ
χ
σ
χ
σ
χ
σ
σ
n
P
T
P
T
P
Pow
(
)






>
−
+






<
−
=
>
−
−
−
2
2
/
1
2
2
0
2
2
2
/
2
2
0
2
2
2
/
1
2
0
2
2
)1
(
)1
(
)1
(
α
α
α
χ
σ
σ
χ
χ
σ
σ
χ
χ
σ
χ
σ
n
P
n
P
n
P
, which turns out 
to be a function of 
2
2
0 /σ
σ
=
R
.
b)	 With n = 10 and 
025
.0
2
/
 
D
,
0229
.
19
and
70039
.2
2
975
.0
2
025
.0
 
 
F
F
.  The RR is 
0229
.
19
or 
70039
.2
2
0
2
0

!


V
V
T
T
.




0229
.
19
)
9
(
70039
.2
)
9
(
)
(
2
2

!



 
R
P
R
P
R
Pow
F
F
.  This is a non-symmetric function. Some values 
are:
R
0.2
0.5
1.0
1.5
2.0
5.0
)
(R
Pow
0.92
0.39
0.05
0.09
0.20
0.94
EX 146 With n = 10 the RR is ¦
¦
!

0
0
2
1696
.
34
or 
2
5908
.9
O
O
i
i
Y
Y
.
 
¸¸
¹
·
¨¨
©
§
!

¸¸
¹
·
¨¨
©
§

 
¦
¦
0
0
2
1696
.
34
2
5908
.9
)
(
O
O
O
i
i
Y
P
Y
P
Pow
 [Multiply each factor within braces with λ
2
and use the 
result that 
)
2
(
~
2
2
1
n
Y
n
i
i
F
O¦
 
] =
¸¸
¹
·
¨¨
©
§

!

¸¸
¹
·
¨¨
©
§


1696
.
34
)
2
(
5908
.9
)
2
(
0
2
0
2
O
O
F
O
O
F
n
P
n
P
.
Put
0
λ
λ
=
R
and plot the power as a function of R.It is seen that the power is larger than 0.90 for 


DQG



!

5
5

Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
191 
Answers to Supplementary Exercises
EX 147 From the data we get 
¦
¦
¦
¦
¦
 
 
 
 
 
 
90
,2.
89
,
21
,
91
,
21
,6
2
2
xy
y
y
x
x
n
,
5.
16
6
/)
21
)(
21
(
90
,7.
15
6
/
)
21
(
2.
89
,5.
17
6
/
)
21
(
91
2
2
 

 
 

 
 

 
XY
YY
XX
S
S
S
.
a)	
0757
.2
2
ˆ
,
3029
.8
91
ˆ
2.
89
,2.0
6
21
ˆ
6
21
ˆ
,
9429
.0
5.
17
5.
16
ˆ
2
2
 

 
 


 
 

 
 
 
n
SSE
SSE
V
E
E
D
E
1
:
0
=
β
H


66
.0
33
.0
)
2
6
(
2
value
-
p
,
33
.0
5.
17
)
2
6
/(
0757
.2
1
9429
.0
 
!


 

 


 
T
P
T
. Don’t reject
0
H .
0
:
0
=
α
H
237
.0
5.
17
)
2
6
(
)
6
/
21
(
6
1
0757
.2
0
2.0
2
 
¸¸
¹
·
¨¨
©
§



 
T
,


82
.0
237
.0
)
2
6
(
2
value
-
p
 
!


 
T
P
.   
Don’t reject 
0
H .
b)	 Since 
0
:
0
=
α
H
can’t be rejected we apply the model (
)
x
x
Y
E
⋅
= β
.
From EX 54, 
0378
.0
)1
(
ˆ
,
1890
.0
91
)
91
/
90
(
2.
89
,
9890
.0
91
90
ˆ
2
2
 

 
 


 
 
 
n
SSE
SSE
V
E
 (The SSE in 
the latter expression is different from the one in a).)
1
:
0
=
β
H


62
.0
54
.0
)1
6
(
2
value
-
p
,
54
.0
91
/
0378
.0
1
9890
.0
 
!


 

 

 
T
P
T
. Don’t reject 
0
H .
The conclusion is that the model in b) is to prefer.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
192 
Answers to Supplementary Exercises
EX 148 First we construct the following table:
x
2
x
n
nx
2
nx
y
y
n
y
nx
1
1
4
4
4
1.00
4.0
4.0
3
9
5
15
45
3.64
18.2
54.6
5
25
3
15
75
7.23
21.7
108.5
10
100
4
40
400
12.725
50.9
509.0
15
225
4
60
900
18.225
72.9
1093.5
Total
20
134
1424
167.7
1769.6
,
2277
.1
)
134
(
1424
20
/)
134
)(
7.
167
(
6.
1769
ˆ
2
 


 
E
1544
.0
20
134
ˆ
20
7.
167
ˆ
 

 
E
D
x
n
y
2)
ˆ
ˆ
(
x
y
n
⋅
−
−
β
α
¦
¦

i
ij
ij
n
y
y
/
)
(
2
2
1
4
1.00
0.584
1.1
4
/
)
0.4
(
1.5
2
=
−
3
5
3.64
0.195
332
.8
5
/
)
2.
18
(
58
.
74
2
 

5
3
7.23
2.653
0067
.2
3
/
)
7.
21
(
97
.
158
2
 

10
4
12.725
0.345
9075
.0
4
/
)
9.
50
(
61
.
648
2
 

15
4
18.225
0.476
3475
.4
4
/
)
9.
72
(
95
.
1332
2
 

Total
4.253
15.6612
(
)
x
x
X
Y
E
H
⋅
+
=
=
β
α
:
0


29
.0
358
.1
)
15
,3
(
value
-
p
358
.1
)
5
20
/(
6612
.
15
)
2
5
/(
253
.4
 
!
 

 


 
F
P
F
.  No reason to reject the hypothesis.
EX 149 
x
y
n
nx
2
nx
y
y
n
y
nx
0.5
1.3 1.5
2
1
0.5
1.4
2.8
1.4
1
1.9 2.1
2
2
2
2.0
4.0
4.0
4
3.9 4.1
2
8
32
4.0
8.0
32.0
16
7.8 8.2
2
32
512
8.0
16.0
256.0
Total
30.8
8
43
546.5
30.8
293.4
67
.1
8
43
ˆ
8
8.
30
ˆ
,
4054
.0
8
/
)
43
(
5.
546
8
/)
43
)(
8.
30
(
4.
293
ˆ
2
 

 
 


 
E
D
E
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
193 
Answers to Supplementary Exercises
EX 149 (Continued)
x
n
y
y
2)
ˆ
ˆ
(
x
y
n
⋅
−
−
β
α
¦

2)
(
j
ij
y
y
0.5
2
1.3 1.5
1.4
0.4469
0.01+0.01 = 0.02
1
2
1.9 2.1
2.0
0.0114
0.01+0.01 = 0.02
4
2
3.9 4.1
4.0
1.0037
0.01+0.01 = 0.02
16
2
7.8 8.2
8.0
0.0488
0.04+0.04 = 0.08
Total
1.5108
0.14


007
.0
6.
21
)
4,2
(
value
-
p
6.
21
)
4
8
/(
14
.0
)
2
4
/(
5108
.1
 
!
 

 


 
F
P
F
.  The linear model has to be rejected.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Exercises in Statistical Inference  
with detailed solutions
194 
Answers to Supplementary Exercises
EX 150 The model 
b
x
a
y
⋅
=
may be a candidate. Here, 
'
'
'
or 
 ),
ln(
)
ln(
)
ln(
x
b
a
y
x
b
a
y
⋅
+
=
+
=
.
We now test whether (
)
'
'
'
'
x
b
a
x
Y
E
⋅
+
=
 is a proper model.
x’
y’
n
nx’
2)'
(x
n
''y
''
' y
nx
'y
n
-.069315
0.2624
0.4046
2
-1.3863
0.9609
0.3339
-0.4629
0.6678
0
0.6419
0.7419
2
0
0
0.6919
0
1.3838
1.38629
1.3610
1.4110
2
2.7726
3.8436
1.3860
3.8428
2.7720
2.77259
2.0541
2.1041
2
5.5452
15.3745
2.0791
11.5291
4.1583
3.4657
8.9818
8
6.9315
20.1790
4.4909
14.9090
8.9818
687
.0
8
9315
.6
'ˆ
8
8.9818
'ˆ
,
5028
.0
8
/
)
9315
.6
(
1790
.
20
8
/)
9315
.6
)(
9818
.8
(
9090
.
14
'ˆ
2
 

 
 


 
b
a
b
From this we get a new table:
n
2)'
'ˆ
'ˆ
'
(
x
b
a
y
n
⋅
−
−
∑
−
2)
'
'
(
i
ij
y
y
2
.000044
0.0102
2
.000046
0.0050
2
.000007
0.0013
2
.000008
0.0013
Total
0.000105
0.0078


97
.0
027
.0
)
4,2
(
value
-
p
027
.0
)
4
8
/(
0078
.0
)
2
4
/(
000105
.0
 
!
 

 


 
F
P
F
.  There is definitely no reason to reject the 
new model.
EX 151 The hypothesis to test is 
0
,0
:
against 
 0
,0
:
3
2
3
2
0
≠
≠
=
=
β
β
β
β
a
H
H
, i.e. there is no effect of the 
sex on body weight beyond the initial weight.


61
.0
50
.0
)
60
,2
(
value
-
p
50
.0
)1
3
64
/(
4381
.
18
)1
3
/(
)
4381
.
18
7454
.
18
(
 
!
 

 




 
F
P
T
. No reason to reject 
0
H
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
195 
Answers to Supplementary Exercises
EX 152 The model is linearized by the transformation 
=
+
+
=
)
ln(
)
ln(
)
ln(
)
ln(
2
1
P
I
Q
β
β
α
'
'
'
2
1
P
I
β
β
α
+
+
.
From the print out we obtain:
990
.0
2 =
R
 (a good agreement).
Parameter
Estimate
T-value
(
)
value
T
T
P
−
>
'
α
9.0795
1.43
0.1765
1
β
-0.9994
-1.92
0.0766
2
β
1.0779
16.23
< 0.0001
Here the T-value and the corresponding p-value are computed under the hypothesis that the parameter is zero. 
Since 
8773
ˆ
0795
.9
=
= e
α
the estimated model is 
08
.1
1
8773
P
I
Q


 
. A ‘significance-fundamentalist’ (a person 
who argues that all non-significant parameters should be deleted in a model) would object against including I in 
the model. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT…
     RUN FASTER.
          RUN LONGER..
                RUN EASIER…
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Exercises in Statistical Inference  
with detailed solutions
196 
Answers to Supplementary Exercises
Comment to the solution of EX 152. To solve non-linear problems by linearization and using least-squares 
techniques, as in the present example, is very frequent (perhaps too frequent) among econometricians. 
One should be aware of that if βˆ is unbiased for β, it does not follow that 
βˆ
P is unbiased for
β
P . Let’s 
look at this a bit closer. 
Put
β
β
β
β
=
=
)ˆ
(
 
 where
,
)ˆ
(
ˆ
E
P
g
. According to (14) in Ch. 3.3.2, (
)
)ˆ
(
)
(''
2
1
)
(
)ˆ
(
β
β
β
β
V
g
g
g
E
⋅
+
≈
. 
Now we have to find
)
('' β
g
. For simplicity, put 

 
 

 

 
)
ln(
'
)
ln(
)
ln(
)
ln(
P
g
g
dy
g
d
P
y
g
P
g
y
 
(
)
(
)
y
P
P
g
P
g
P
g
g
P
g
2
2
)
ln(
)
ln(
'
)
ln(
''
)
ln(
'
=
⋅
=
⋅
=
⇒
⋅
=
. From this we finally obtain 
(
)
(
)
(
)
β
β
β
β
β
β
β
P
V
P
P
V
P
P
P
P
E
>






+
=
⋅
+
≈
2
)ˆ
(
)
ln(
1
)ˆ
(
)
ln(
2
1
2
2
ˆ
, so there will be a positive bias. However, 
since
n
const
V
.
)ˆ
(
=
β
, the bias can be ignored in large samples.
EX 153 






x
y
y
S
y
y
y
S
e
y
S
y


 

 

 


 

 

D
O
D
O
O
D
O D
'
)
ln(
)
ln(
)
(
ln
ln
'
)
(
ln
)
(
,  say.
We thus run a regression of 




ln(y)
on x
)
(ˆ
ln
ln
'ˆ
 

 
y
S
y
.  This yields:
Parameter
Estimate
Standard Error of Estimate
'
λ
-0.0914
0.0378
α
2.0470
0.0888
Here, Standard Error of Estimate
)
(ˆ Estimator
V
=
1
:
against 
 1
:
0
≠
=
α
α
a
H
H
is tested by 

 
!


 

 

 
79
.
11
)
2
5
(
2
value
-
p
79
.
11
0888
.0
1
0470
.2
T
P
T
 0.001. 
Reject
0
H !
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
197 
References
References
Casella, G. & Berger, R.L. 1990, Statistical Inference, Duxbury Press, Belmont, California.
Cochran, W.G. 1934,’The distribution of quadratic forms in a normal system, with applications to the 
analysis of covariance’, Proc. Camb. Phil. Soc, 30, pp. 178–191.
Cochran, W.G. 1950, ‘The Comparison of Percentages in Matched Samples’, Biometrika, 37, pp. 256–266.
Cochran, W.G. 1954, ‘Some methods for strengthening the common
2
χ test’, Biometrics, 10, pp. 417–451.
Cox, D.R. & Smith, W.L. 1954, ‘On the superposition of renewal processes’, Biometrika, 41, pp. 91–99.
Cramer, H. 1957, Mathematical Methods of Statistics, 7th edn, Princeton University Press, Princeton.
Davis, C.E. 1976, ‘The effect of regression to the mean in epidemiologic and clinical studies’,  
Am. J. Epidemiol., 104, pp. 493–498.
Diggle, P.J., Liang, K-Y, & Zeeger, S.L. 1994, Analysis of Longitudinal Data, Oxford University Press, 
New York.
Fisz, M. 1963, Probability Theory and Mathematical Statistics, 3rd edn, Wiley, New York.
Fukuda, M., Fukuda, K., Shimizu, T., Andersen, C.Y. & Byskov, A.G. 2002, ‘Parental periconceptional 
smoking and male:female ratio of new born infants’, The Lancet, 359, pp. 1407–1408.
Holm, S. 1979, ‘A Simple Sequentially Rejective Multiple Test Procedure’, Scandinavian J. of Statistics, 6, 
No 2, pp. 65–70.
Hsiao, C. 2002, Analysis of Panel Data, Cambridge University Press, Cambridge.
McNemar, Q. 1947, ‘Note on the sampling error of the difference between correlated proportions or 
percentages’, Psychometrika, 12(2), pp. 153–157.
Petzold, M. & Jonsson, R. 2003, ‘Maximum Likelihood Ratio-based small sample tests for random 
coefficients in linear regression’, Working Paper in Economics, No 110, 2003.
Rao, C.R. 1965, Linear Statistical Inference and Its Applications, Wiley, New York.
Download free eBooks at bookboon.com

Exercises in Statistical Inference  
with detailed solutions
198 
References
Scheaffer, R.L., Mendenhall, W. Ott, R.L. & Gerow, K. 2012, Survey Sampling, 7th edn, Brooks/Cale 
CENGAGE Learning.
Shuster, J.J. 1992, ‘Exact unconditional tables for significance in the 2 × 2 multinomial trial’, Statistics in 
Medicine, 11, pp. 913–922.
Stuart, A., Ord, J.K. & Arnold, S. 1999, Kendall’s Advanced Theory of Statistics, Vol 2A, Arnold, London.
Wackerly, D., Mendenhall, W. & Scheaffer, R.L. 2007, Mathematical Statistics with Applications, 7th edn, 
Thomson, Toronto.
Wonnacott, T. 1987, ‘Confidence intervals or hypothesis tests?’, J. of Applied Statistics, 14, No 3, 
pp. 195–201.
Yates, F. 1934, ‘Contingency table involving small numbers and the 
2
χ  test’, Supplement to the J. of the 
Royal Statistical Society, 1(2), 217–235.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF


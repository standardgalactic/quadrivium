
Voice Enabling Web 
Applications: VoiceXML 
and Beyond 
KEN ABBOTT 
APress Media, LLC 

Voice EnablingWeb Applications: VoiceXML and Beyond 
Copyright ©2002 by Ken Abbott 
Originally published by Apress in 2002 
All rights reserved. No part of this work may be reproduced or transmitted in any 
form or by any means, electronic or mechanical, including photocopying, recording, 
or by any information storage or retrieval system, without the prior written per-
mission ofthe copyright owner and the publisher. 
ISBN 978-1-893115-73-6 
ISBN 978-1-4302-0850-1 (eBook)
DOI 10.1007/978-1-4302-0850-1 
Trademarked names may appear in this book. Rather than use a trademark symbol 
with every occurrence of a trademarked name, we use the names only in an editorial 
fashion and to the benefit of the trademark owner, with no intention of infringement 
of the trademark. 
Editorial Directors: Dan Appleman, Gary Cornell, Iason Gilmore, Karen Watterson 
Marketing Manager: Stephanie Rodriguez 
Managing Editor: Grace Wong 
Technical Reviewer: Dennis McCarthy 
Developmental Editor: Marty Minner 
Copy Editor: Nicole LeClerc 
Production Editor: Laura Cheu 
Compositor: Impressions Book and Journal Services, Inc. 
Artists: Susan Glinert Stevens, Impressions Book and Journal Services, Inc. 
Indexer: Valerie Haynes Perry 
Cover Designer: Tom Debolski 
The information in this book is distributed on an "as is" hasis, without warranty. 
Although every precaution has been taken in the preparation of this work, neither the 
author nor Apress shall ha ve any liability to any person or entity with respect to any 
loss or damage caused or alleged to be caused directly or indirectly by the infor-
mation contained in this work. 

Contents 
Author's Note on VoiceXML 2.0 ..................................... . vii 
Preface ............................................................... . ix 
Chapter 1: The Role of Voice on the Web .............. . 3 
Using Sight and Sound Together .................................... . 3 
Chapter 2: The Convergence of Speech and 
the Web .............................................. 7 
What Is VoiceXML? .................................................... 7 
Meet the Technical Parents ......................................... 7 
Why Merge Speech and the Web? .................................... . 13 
Making Voice User Interfaces Easy to Build and Use ........... . 14 
Summary .............................................................. . 14 
Chapter 3: The Evolution of Web 
Application Architectures ................. . 15 
The Good Old Days: Browsers, Servers, and Content ............ . 15 
Sessions and Dynamic Content ..................................... . 17 
GUis, WUis, and VUis .............................................. . 18 
Summary .............................................................. . 21 
Chapter 4: Simplified Personal Information 
Manager Example ................................ . 25 
Use Case Analysis .................................................. . 26 
Object Model ........................................................ . 30 
Summary .............................................................. . 31 
iii 

Contents 
Chapter 5: VoiceXML Concepts ............................. . 33 
VoiceXML History ................................................... . 33 
Voice Web Browsing ................................................. . 35 
Elements of VoiceXML .............................................. . 38 
Summary .............................................................. .40 
Chapter 6: Outfitting Your VoiceXML Expedition ... .41 
Standalone versus Hosted Development ........................... .41 
Development Environment ........................................... .43 
VoiceXML 1.0 versus VoiceXML 2.01 .............................. .. 44 
Some Available Software Options ................................. .45 
Speech Developer Accessories ..................................... . 46 
Summary .............................................................. .47 
Chapter 7: VoiceXML Language Tutorial ................. .49 
"Hello, World!" .................................................... .49 
SPIM Menu Navigation .............................................. . 50 
SPIM Main Menu ..................................................... . 51 
Application with Multiple Dialogs ............................... . 52 
Visiting Documents ................................................. . 54 
Form Handling ....................................................... . 57 
Handling Events .................................................... . 66 
Queries and Sets .................................................. .. 74 
Telephony Features ................................................. . 84 
Summary .............................................................. . 85 
Chapter 8: VUI Design Principles and 
Techniques ....................................... . 87 
Core Principles .................................................... . 87 
Speech Design ....................................................... . 90 
Interface Design ................................................... . 94 
Summary ............................................................. . 102 
iv 

Contents 
Chapter 9: VoiceXML Programming Guide ............... . 105 
Structure of a VoiceXML Program ................................ . 105 
Input and Output .................................................. . 107 
Navigation ......................................................... . 108 
Forms and Fields .................................................. . 109 
Dialog, Document, and Application .............................. . 110 
Speech Recognition Grammars ..................................... . 111 
Speech Synthesis Markup .......................................... . 112 
Events and Handlers .............................................. . 114 
Form Items and the Form Interpretation Algorithm ............ . 117 
Mixed-Initiative Dialogs ........................................ . 119 
Executable Content ................................................ . 120 
Telephony .......................................................... . 121 
Platform and Performance Features .............................. . 121 
Summary ............................................................. . 121 
Chapter 10: Advanced VoiceXML Topics ................ . 123 
Resource Fetching ................................................. . 123 
Audit Trail ........................................................ . 125 
Accessing the Voice Gateway ..................................... . 126 
Advanced Event Handling .......................................... . 129 
Summary ............................................................. . 139 
Chapter 11: Overview of Related Web 
Technologies .................................. . 143 
XML 
................................................................. . 143 
XSL ................................................................. . 145 
Servlet ............................................................. . 148 
JavaServer Pages .................................................. . 149 
XML Publishing .................................................... . 152 
Summary ............................................................. . 155 
v 

Contents 
Chapter 12: Adding VoiceXML to Web 
Applications .................................. . 157 
One Application, Multiple User Interfaces .................... . 157 
Web Application Architectures ................................... . 162 
Summary ............................................................. . 175 
Chapter 13: The Web Application Prototype ......... . 177 
Prototype Setup and Installation ............................... . 178 
Anatomy of the Prototype ........................................ . 188 
Trying It Out ...................................................... . 201 
Tips for Dissecting the Prototype .............................. . 203 
Summary ............................................................. . 207 
Chapter 14: What's Next? .................................. . 2o9 
Changes from VoiceXML 1. o to VoiceXML 2. o .................... . 209 
Grammar and Speech Synthesis Sped fication ................... . 210 
Reusable Dialog Components ...................................... . 211 
Mul timodal Interfaces ............................................ . 213 
Architectural Issues ............................................. . 214 
Summary ............................................................. . 216 
Appendix A ........................................................ 219 
A Quick Reference to VoiceXML 1.0 Syntax ...................... . 219 
Index ................................................................ . 241 
vi 

Author's Note on 
VoiceXML 2.0 
THE FIRST VERSION ofVoiceXML, VoiceXML 1.0, was officially released in May 2000 
by the VoiceXML Forum. Subsequently, the VoiceXML Forum turned over control 
of the VoiceXML specification to the World Wide Web Consortium (W3C). The 
next version ofVoiceXML, popularly known as VoiceXML 2.0, has been pending 
throughout the writing and production of this book, but still has not been pub-
licly released as of mid -October 2001. 
As anyone who works with software technology knows, one and one half 
years between releases of a burgeoning technology is an eternity. The delay has 
been due to internal issues within the W3C regarding intellectual property rights. 
In the past, the W3C has been a strong of advocate of open -source (public 
domain) technologies. Modern reality is that many, if not most, new and evolving 
technologies are being developed by parties who hold some intellectual rights 
to the technology, so the W3C must adapt. VoiceXML is one such technology 
under the W3C's purview (but not the only one). 
As a result of this turmoil, the anxiously awaited VoiceXML 2.0 specification 
has been pending release as a W3C Working Draft for over a year. Both the internal 
deliberations of the W3C and the contents of any unreleased work-in-progress are 
closed to the public. Therefore, the specification cannot be discussed publicly, 
and there has been no firm official information from the W3C about when it can 
be. (And the W3C finds itself in the unique position of being an open standards 
body fighting fiercely to keep a much-requested standard secret.) 
People who buy technical books want information that is up-to-date and 
timely. This presented a dilemma to authors and publishers. To provide timely 
information on an infant technology such as VoiceXML, books are often rushed 
into production. On the other hand, to provide up-to-date information, books 
are often timed to appear as soon as a particular new technology is released. For 
books on VoiceXML, the choice was to rush to market with books on VoiceXML 
1.0 (already decrepit in Web time and due to be superseded by the imminent 
VoiceXML 2.0), or wait for VoiceXML 2.0 (which was making little publicly visible 
progress toward release). 
Initially, Apress and I decided that the smart thing to do was write the manu-
script, wait for VoiceXML 2.0 publication, and then follow as soon as possible with 
publication of this book, compatible with VoiceXML 2.0. However, books don't 
hold well in captivity, and as months passed with no resolution to uncertainty 
vii 

Author's Note on VoiceXML 2. 0 
viii 
concerning the schedule for release ofVoiceXML 2.0, we decided that the book 
needed to get into people's hands. 
So, strictly speaking, this book can only claim that it is compatible with the 
VoiceXML 1.0 specification and that an online supplement will reconcile any 
incompatibilities when VoiceXML 2.0 appears. However, due to the long gestation 
ofVoiceXML 2.0, a fair amount of information about VoiceXML 2.0 has become 
publicly known, whether the W3C likes it or not. So there is some good news: 
• Based on publicly known information, I have anticipated VoiceXML 2.0 
throughout the book. 
• All indications are that VoiceXML 2.0 is an incremental improvement on 
VoiceXML 1.0. For learningVoiceXML, it doesn't matter which specification 
you're using. 
• This book is not just aboutVoiceXML. It's about voice enabling Web appli-
cations, and there is a lot of valuable information herein about integrating 
voice with Web technologies and with existing applications that can be 
found nowhere else. 
• When VoiceXML 2.0 becomes available, so will information to update 
this book-you can find it online at 
http://www.apress.com/catalog/book/1893115739/. 
So indulge your interest-use this book to start voice enabling your Web 
applications right now! 
Kenneth R. Abbott 
October 2001 
Holliston, MA 
abbott@acm.org 

Preface 
This book is about two topics that I've pretty well mixed together: using voice to 
access the Web and the VoiceXML language. Of the two, the former topic is the 
bigger, more conceptual one, and it is the one that will wear the best over time. I 
believe that VoiceXML will enable the use of voice to access the Web in a big way. 
VoiceXML is a hot new enabling technology destined to live its meteoric life in 
Web time: new and brilliant today but commonplace tomorrow. However, in the 
grand tradition of computer technology, details get the attention and the major 
trends take care of themselves. 
I am a "big picture" person, and for me, the key to mastering the ever-
changing details of technology is to keep the details in context. My years in the 
computer software industry have taught me that not everyone thinks the way I 
do, and for many technically oriented people, "God is in the details." In this book 
I have attempted to strike a balance between providing context and explaining 
the current technical details. 
In the admixture of voice, computers, and the Web, I've observed the follow-
ing overlapping constituencies with strong interest in seeingVoiceXML succeed. 
• One constituency is people with backgrounds in telephony and highly cus-
tomized voice applications. They've been making telephones and 
automated voice systems work together for years using expensive, special-
purpose gateways and expensive, proprietary software. 
• Another group is voice technologists, who are grounded in the deep complexi-
ties of voice recognition, voice synthesis, and natural language processing. 
They have been working for decades on a complex and frustrating technology, 
and they feel it is now getting close to the point where the masses can use it. 
• Web enthusiasts are technically oriented people who are deeply involved with 
the development, care, and feeding ofWeb applications and the Web itself. To 
them, voice is a new technology to be quickly mastered and assimilated. 
• Finally, there are technology integrators, who occupy the shadowy realm 
between business and technology. They are interested in finding better ways 
to do business using technology. Technology integrators tend to be interested 
in markets, products, architectures, and standards-they are the architects 
and general contractors of the computer-system building industry. 
In terms of my personal background, I am part Web enthusiast and part 
technology integrator. My background is in computer software, primarily 
ix 

Preface 
X 
large-scale applications, and my current specialty is helping clients develop and 
deploy large-scale business applications on the Web. Proving the technical feasi-
bility of an architecture or design is a vital part of what I do, so also I dive in and 
develop prototypes and pilots. 
In this book, I have tried to speak to all constituencies. I introduce and 
review, but do not labor over, material that may be familiar to some and not to 
others. I've tried to give you the same mixture of abstract and concrete advice 
I give my clients. In this book, I have established a context for voice technology in 
general and VoiceXML in particular; I have architecturally "sited" voice and 
VoiceXML relative to other major technology landmarks such as XML and appli-
cation servers; and I have provided a step-by-step tutorial for hands-on 
beginners and a working prototype for advanced users. 
What does this book give you that others don't? Most books you see on the 
shelf with VoiceXML in the title will have some sort of tutorial, walk-through, or 
annotated examples ofVoiceXML programs-most will have extensive reference 
material covering all details of the VoiceXML language. This book has those 
things. However, this book also provides a thorough grounding in what it takes to 
actually use VoiceXML effectively in the complex, polyglot world of modern Web 
applications. In terms of enabling voice access to a Web site, VoiceXML is but one 
specialist niche in a much broader landscape that includes stylesheets, servlets, 
databases, and so on. In this book, these related technologies are approached as 
a professional systems designer or architect would approach them: with basic 
knowledge of the technology, with respect for unseen details, and with a desire to 
put the technology to work as quickly and painlessly as possible. 
Structure of the Book 
To achieve the balance between context and detail discussed previously, I have 
laid the book out into the following parts. 
Part I (Chapters l-3) takes a retrospective view on two key technologies 
that VoiceXML brings together: speech and the Web. This part provides 
valuable context for understanding how these technologies came to be 
converging now, and it provides a base for extrapolating how the merged 
technologies will progress together. 
Part II (Chapters 4-9) focuses on the nuts and bolts of the VoiceXML lan-
guage. Using a Simplified Personal Information Manager as a specimen 
application, Part II guides you through an initial analysis of the application 
(Chapter 4), introduces basic VoiceXML concepts (Chapter 5), helps you set 
up your own VoiceXML development environment using software from the 
companion CD (Chapter 6), and leads you through a hands-on VoiceXML 
tutorial (Chapter 7). With the tutorial mastered, Chapters 8-10 examine 

advanced, pragmatic issues concerning the effective design and develop-
ment of voice interfaces using the full power of the VoiceXML language. 
Part III (Chapters 11-13) dollies back from the details of the VoiceXML lan-
guage and explores the issues involved in building a single Web application 
that incorporates multiple access modes, such as voice and graphical 
interfaces. Chapter 11 briefly reviews major technologies that are used in 
enterprise Web applications, including XML, XSL, JavaServer Pages, and 
application servers. Chapter 12 introduces a transformational approach 
for putting together the various technologies, includingVoiceXML, into a 
scalable, multiple access mode architecture. Chapter 13 presents a working 
prototype that demonstrates the transformational architecture. Detailed 
instructions are provided for installing the prototype from the companion 
CD, and the various components are dissected. Finally, Chapter 14 
explores future directions forVoiceXML. 
Appendix A (A Quick Reference to VoiceXML 1.0 Syntax) contains condensed 
reference material on the VoiceXML language. The reference material is 
geared toward providing an experienced VoiceXML programmer exactly the 
information he or she needs while in the throes of a coding frenzy. 
Companion CD 
The companion CD contains all the software you need to begin developing 
VoiceXML applications on your PC. The CD includes IBM Web Sphere Voice 
Server SDK 1.5, IBMWebSphere Studio (trial edition), Allaire JRun 3.1 (developer 
version), Altova XML Spy IDE for XML (30-day evaluation), plus an assortment of 
goodies, such XML Quick Reference Cards from MulberryTech and a small gallery 
of computer-synthesized voices to break in your new headset. 
Building Up Your Courage 
If you're still not sure that you're ready to take the plunge into the world of the 
voice-enabled Web, I suggest you visit some of the major resource sites on 
the Web. You'll see that there are a lot of people getting excited about combining 
voice and the Web, and there are a lot of resources for newbies. I recommend 
starting with the following sites. 
• VoiceXML Forum (http: I /www. voicexml. orgl): The VoiceXML Forum is the 
industry consortium that first standardized VoiceXML. This site has lots of 
information about VoiceXML, including FAQs, technical resources, and 
details about important activities such as user group meetings. In addition, 
Preface 
xi 

Preface 
xii 
there are links to just about every business, individual, and organization 
active in the VoiceXML area. 
• TheWorldWideWeb Consortium (http:/ /www.w3.org/): TheW3C is the 
preeminent standards body for the Web. The W3C is now responsible for 
VoiceXML as well as related standards for speech markup languages, 
speech grammar languages, and so on. The W3C is also the authoritative 
source for information about Web infrastructure technologies such as 
HTTP, HTML, and MIME. The site offers FAQs and tutorials for newbies 
and concise technical documents for experts. 
• XML.org (http: I /www.xml. orgl): This site is the best jumping-offpoint for 
immersing yourself in XML. XML.org is dedicated to promoting the use of 
XML, and the site provides links to FAQs, books online, online courses, 
examples, free software tools, and much more. 
After you have surfed around a bit, I think you'll discover two things. 
• Taken individually, the component technologies involved in a voice-
enabled Web application (such as VoiceXML, XSL, server pages, and so on) 
are fairly accessible and can be tamed by any experienced technical person. 
• Quite of number of component technologies must be lashed together to 
create a working voice-enabled Web application, but there's not much doc-
umentation about the architecture and integration required to make all the 
components work together smoothly. That is why the book in your hands 
is valuable. 
Acknowledgments 
Thanks to Dennis McCarthy, both for getting me turned on to this whole 
"VoiceXML thing" and for providing valuable comments and suggestions as the 
principal technical reviewer for the manuscript. Thanks to Sue Spielman of 
Switchback Software, who provided technical feedback from the perspective of 
an expert Cocoon user and XML developer. 
Special thanks to my wife Susan, who has seen more of me around the house 
than she thought possible or tolerable, and who struggled through the first three 
pages of the manuscript and concluded, "It's wonderful, dear." Which was exactly 
the right thing to say. 
Ken Abbott 
August200l 
Holliston, MA 
abbott@acm.org 

Part One 
Retrospective on Voice 
and the Web 
THis PART INTRODUCES and reviews the key concepts that underlie voice technology 
and the World Wide Web. Voice technology and the Web have very different ori-
gins. Understanding the context and trajectory of each technology will help 
answer many of the questions that will pop up in subsequent parts as you grapple 
with the technical details: Why do things work this way? Why isn't this obvious 
feature standardized? 
Chapter 1 provides a brief introduction to voice and its significance. 
Chapter 2 explores how and why voice and the Web are converging. Chapter 3 
closes this part with a review of how the Web has evolved technically and draws 
some parallels to the future evolution ofVoiceXML. 

CHAPTER 1 
The Role of Voice on 
the Web 
PUNDITS OF THE INFORMATION AGE portray the Internet as a tidal wave of innovation 
that is sweeping human culture. Skeptics, Luddites, and technical curmudgeons 
point out that if the Internet is a revolution, it is one limited to the elite minority 
with regular access to computers. Both sides have a point. Those who encounter 
the Internet often find the experience transforming, but not everyone has 
that opportunity. 
However, very soon the Internet will be accessible to pretty much everyone 
as a result of technological advances that will enable people to access the 
Internet from their homes, workplaces, cars, and so on. Access will be through 
a low-cost, ubiquitous "Internet appliance": the telephone. The enabling tech-
nology is voice technology, which will enable people to interact with computers 
over the telephone using their voices. 
The idea of talking to computers has been around as long as computers 
have. The technology to make computers recognize voices and generate speech 
in response has been developing for decades, and it is still imperfect. Within 
a constrained conversational context, however, voice technology has recently 
become good enough that computers and people can understand each other 
tolerably well. 
VoiceXML is a new, first -cut technology that holds the promise of making 
voice interfaces as easy to build, deploy, and use as the graphical interfaces that 
currently dominate the Web. What's significant about VoiceXML is that it reaches 
an audience much larger than just the digerati who currently populate cyber-
space-it has the potential to reach everyone that uses a phone. 
Using Sight and Sound Together 
There is no doubt that human beings perceive, use, and respond to sight and 
sound very differently. Sound is the medium of music. Sight is the medium of pic-
tures, of reading, and of art. Perhaps because it's omnidirectional and works day 
or night, sound is the original form of communication between people. Perhaps 
because it's immediate and information can be absorbed quickly, sight is the 
popular medium of communication in our technological age. 
3 

Chapter 1 
Due to its technical history and orientation, the Web has favored the 
explosion of visual interfaces over auditory interfaces. Much of the thrust of new 
developments in user interfaces in the Information Age has been to increase the 
rate at which information can be exchanged and the volume of information 
available at any given moment. (Think 21" monitors set at high resolution.) This 
approach has favored the visual over the audible, because visual interfaces are 
"scalable" -to increase the amount of visual information, you simply increase 
the transfer rate and the display capacity. On the other hand, speech cannot be 
significantly speeded up without becoming incomprehensible. To increase the 
amount of information conveyed through speech, you increase the length of 
the conversation. 
As Table 1-1 shows, the different characteristics of sight and speech make 
them useful in different situations. 
Table 1-1. Characteristics of Sight and Speech 
CHARACTERISTIC 
Conveying 
information 
Robustness 
4 
SIGHT 
SPEECH 
STRENGTH 
WEAKNESS 
STRENGTH 
WEAKNESS 
Immediate and 
May require 
Good for concisely 
More information 
excellent for 
knowledge of 
conveying emotions 
requires more time. 
conveying 
symbol 
and imprecise/ 
relationships 
vocabularies. 
ambiguous 
between things. 
information. 
Pictures can be 
Degraded by 
Highly robust and 
Doesn't work between 
understood by 
poor viewing 
error tolerant. 
people speaking 
people with 
conditions (poor 
Conversations can 
different languages. 
different levels 
lighting, no moni- occur in a wide 
of knowledge 
tor available, 
variety of situations. 
and experience. 
and so on). 
Now that both sight and speech are viable modes for accessing the Web, 
there are some interesting questions to be answered. 
• What types of interactions are more appropriate for speech, and what 
types are more appropriate for sight? 
• If the same information can be accessed by either speech or sight, then 
how is the information structured so that it can be effectively rendered into 
the different media? 
• Can and should the interfaces be merged? 

The Role ofVoice on the Web 
For example, consider the adage "A picture is worth a thousand words." 
A picture that can be perceived instantaneously by the eyes may require a long 
description to convey similar information through speech. On the other hand, 
audible speech can rapidly convey emotions and shades of meaning that are lost 
in sight-mediated representations (think of sending e-mail versus talking on 
a phone). Entering names and addresses through a graphical interface requires 
manipulating mice and keyboards, as opposed to simply saying the names 
and addresses. 
At this time, we are just becoming able to use speech as a means of (limited) 
communication between people and machines. This book is about a key enabler 
of this limited capability: VoiceXML. VoiceXML, in its current form, is strictly 
concerned with speech interaction. However, it is important to bear in mind 
that sight and speech are complementary, each with its own set of strengths 
and weaknesses. 
5 

CHAPTER 2 
The Convergence of 
Speech and the Web 
What Is VoiceXML? 
VmcEXML IS A PROGRAMMING LANGUAGE for scripting voice interactions between 
a computer and a person. The basic element of interaction is a spoken dialog in 
which the computer produces spoken prompts to elicit spoken responses from 
the user. VoiceXML prompts may be recorded or generated using Text-to-Speech 
(TTS) synthesis. Spoken user responses are processed using speech recognition 
and grammars defined in the VoiceXML program. Users may also respond 
through a keypad (DTMF1), as defined in the program. 
Meet the Technical Parents 
On hearing the term "VoiceXML," people new to the technology often notice the 
Extensible Markup Language (XML) connection but fail to catch the "computer 
speech recognition and synthesis" implication. In fact, VoiceXML draws heavily 
from the lineage of both, and its greatest technical innovation is to make com-
puterized voice technology available and accessible to the masses. 
Speech Recognition and Synthesis 
The history of the research and development of computer speech recognition 
and synthesis is long and somewhat frustrating. The possibility of communicat-
ing with a computer the way people communicate with each other has seemed 
tantalizingly close since the dawn of computing, but it still has not been realized. 
In one of those unexpected paradoxes of high technology, it turns out to be easier 
for a computer to beat a person at chess than it is for it to achieve a child's ability 
to talk. 
1 DTMF stands for Dual Tone Multi-Frequency, which is techno-speak for the sounds 
a Touch-Tone phone makes when you press the keys. 
7 

Chapter2 
8 
Considerable amounts of intellectual capital have been spent developing 
techniques to automate the recognition and comprehension of language. Efforts 
have spanned many venerable fields of study, including linguistics, computer 
science, mathematics, statistics, and psychology. In the pursuit of results, tech-
niques have run from the heights of elegant, abstract mathematical theory to the 
depths of grubby, empirical pragmatism. The bottom line: Automated speech 
recognition has improved, and continues to do so, but it is still far from perfect. 
Speech synthesis has had a similar record of continued improvement. At the 
end of the day, computers still talk funny, but people can understand computers 
better than computers can understand people. 
Sound, Speech, and Meaning 
Something you hear is sound. A sequence of sounds people make with their 
voices with the intent to communicate is speech. Meaning is the message con-
veyed when speech is successfully understood. 
During a conversation, several distinct processes occur. Speaking can be 
defined as the generation of understandable sounds using the voice. Hearing is 
the perception of those sounds as speech and the chunking of sounds into units 
of speech. Cognition is the assembling of speech units into an understandable, 
meaningful message. This speech processing model is illustrated in Figure 2-1. 
Understanding 
+E---+) 
Talking 
+E---+) 
Understanding 
Hearing 
SpeaHng 
Speech 
...... 
+-
Speech 
Speaking 
Hearing 
Figure 2-1. Speech processing model 

The Convergence of Speech and the Web 
When people of a certain age hear about "talking to a computer," they often 
conjure up images of HAL from the film 2001:A Space Odyssey. HAL was an amaz-
ing computer that could talk and see. He sounded a little nerdy, but he could 
carry on a conversation with no problem whatsoever, and his eyesight was excel-
lent. In the year 2001, HAL is still amazing. Today's computers approach HAL in 
their ability to speak understandably and to recognize the words that a person 
speaks. However, in the area of cognition, HAL is still far beyond our current 
technological capabilities. Consider the fact that HAL could not only understand 
natural human language, but he could also lip-read it (despite his personal lack 
of lips!). 
HAL engenders the false expectation that you can talk to a computer and 
have a conversation, just as you can strike up a conversation with someone 
standing in line at the post office. That happens to be exactly what computers 
can't yet do. That is why VoiceXML encompasses speaking and hearing but has no 
cognition model other than standard computer programming. 
Speaker-Dependent versus Speaker-Independent 
Speech Recognition 
Speech recognition technologists make a critical distinction between speaker-
dependent and speaker-independent speech recognition. A person must enroll 
with a speaker-dependent voice system before using it. Enrolling means going 
through a process of training the computer to recognize an individual's voice-
that is, by having the person read text that the computer provides in the person's 
habitual environment. Once the computer has "imprinted" on an individual, it 
can recognize a significant portion of that person's vocabulary in any context, but 
it makes more errors recognizing other people's speech than it did before enroll-
ment. Notice that when speaker-dependent systems train, they are training not 
only on the individual's voice, but on the environment as well (for instance, your 
voice as it sounds through a headset microphone in your office). Therefore, 
a controlled environment is a necessary ingredient for speaker-dependent 
speech recognition. Some dictation systems even go so far as to suggest 
re-enrolling if you change your microphone. 
On the other hand, speaker-independent speech recognition strives to recog-
nize what a person is saying without knowing anything about the person or his or 
her environment. This means that the recognizer must be able to accommodate 
all the variables that people unconsciously adjust for: accents; differences 
between men's, women's, and children's voices; level of excitement; and so on. 
These natural voice variations are compounded by a variety of environmental 
obstacles: background noise; poor phone connections; fidelity variations 
between phones, speakerphones, and headsets; and so on. 
9 

Chapter2 
10 
Using speaker-dependent speech recognition, current systems make errors 
at tolerable rates. Dictation systems such as Dragon NaturallySpeaking and IBM 
Via Voice make errors at a rate that some people find annoying, but still useful. 
Speaker-independent speech recognition works with reasonable success when 
the recognizer is working with a constrained grammar. In other words, success is 
greatest when the recognizer is not trying to understand what someone says out 
of the blue; it works when it has the simpler job of matching what the user said 
against a predefined set of options (for instance, a list of names or a menu of 
commands). Speaker-independent speech recognition does not work in uncon-
strained situations. You still can't just walk up to a computer and strike up 
a conversation. 
VoiceXML uses speaker-independent speech recognition. VoiceXML inter-
actions are programmed, structured dialogs where the computer is always aware 
that the user is using a finite vocabulary of possible utterances. 
Markup Languages 
Markup languages had their heyday before graphical interfaces and the 
Web. Markup languages were a way to embed text-formatting instructions into 
text documents. The text and markup language were entered into a text file using 
a text editor (which in those pre-GUI days were line-at-a-time monsters). To pro-
duce a formatted document, a special formatting program (a word processor) 
read the text file, interpreted and stripped out the markup instructions, and pro-
duced a text file that would print nicely on a selected printer. Because pre-GUI 
days were also pre-laser printer days, the printer might have been a line printer, 
a dot matrix printer, or (for the utmost in quality) a daisy wheel printer. All these 
technologies-markup language-based text processors, command-line text edi-
tors, and impact printers-were authoritatively eclipsed by GUis, WYSIWYG 
(What You See Is What You Get) editors, and laser printers. 
However, the markup language approach solved some difficult publishing 
problems that WYSIWYG word processors could not. Standard Generalized 
Markup Language (SGML), a standard developed by IBM, continued to serve 
a small but loyal market, and as a result it was still vital when someone began 
envisioning a "World Wide Web." 
SGML} HTML} and XML 
HTML is a blessing and a curse to its aged parent SGML. It's a blessing because 
it breathed life into a stagnant technology. It's a curse because it did so with 
a brash, youthful disregard for the elegance and refinement of its progenitor. 

The Convergence of Speech and the Web 
SGML aimed to solve a tough information management problem in techni-
cal publishing. Designers and manufacturers of complex systems, such as 
airplanes, weapons, and electronics systems, are forced to also be publishers of 
the large volume of technical documentation that must accompany their sys-
tems: user manuals, maintenance guides, specifications, and so on. The 
complexity of the publication and maintenance processes for technical docu-
ments rivals (and sometimes exceeds) the complexity of the design and 
manufacturing processes described therein. (Consider the case where upgrading 
a single part in a deployed system may require modifications to multiple copies 
of multiple documents delivered to the customer.) 
SGML enables technical publishers to represent complex, interrelated infor-
mation electronically such that 
• The format is neutral with regards to publication technology. 
• The information is accessible at subdocument granularity. 
• The granules of information can be reused across multiple documents. 
SGML is a metalanguage, a language used to define other languages. The 
other languages actually get used to solve problems. HTML and XML are exam-
ples oflanguages defined in the SGML family. In its intended use, SGML is used 
by a publisher to define a language specific to the problem at hand. For example, 
Norton Aircraft might define a Norton Aircraft Markup Language (NAML) specific 
to its airplane part numbering system. Norton Aircraft then captures the techni-
cal information for an airplane in NAML. 
SGML and its derivatives are markup languages. That means the information 
content is stored interspersed with tags that describe the meaning of the infor-
mation. The analogy is to a publisher's markups, where an editor marks proofs 
with coded instructions that refer to adjacent text or graphics. Presentation 
markup annotates content with instructions about how to present the content in 
a given medium (for example, on a video screen). Content markup provides 
information about the meaning and logical structure of the content, which is the 
same no matter how the content is presented. An SGML language may include 
a combination of presentation and content markup tags, but in practice, lan-
guages tend to fall in one category or the other. 
HTML draws power from SGML, but it is an expedient rather than an elegant 
application of SGML in two senses. First, the syntax of HTML is almost, but not 
quite, SGML compliant (HTML tags such as <br> aren't proper SGML). Second, 
HTML muddies the distinction between content and presentation. HTML is 
a presentation markup language because it describes how things should appear 
in a Web browser. For example, the HTML <bold> tag describes how text should 
11 

Chapter2 
12 
appear visually, but it provides no hint as to why it should appear that way. Other 
tags such as <Hl> (header Ievell) sound like they are describing the logical struc-
ture of the document, but in use, they really refer to particular rendering styles. 
In contrast, content markup tags content by its meaning (for example, 
dnformalAside> .•. </informalAside>) and leaves rendering decisions to the 
renderer. For example, a voice interface might render an informal aside in a whis-
per, while a Web interface might simply italicize the text. Decisions about 
rendering into a particular medium can be made while generating presen-
tation markup from content or they can be preprogrammed into the media 
browser itself. 
Development ofXML was spurred by a desire to generalize and extend the 
success of HTML. Technically, the approach was to popularize SGML by creating 
a simplified subset that would be useful to the broad audience of businesses try-
ing to exchange data over the Web. In its full generality, SGML has some finicky 
and complex nooks and crannies that are only used to solve the hardest prob-
lems. XML removes some of the most obtrusive complexity (and hence some of 
the power) of SGML by defining a restricted family of SGML-compliant lan-
guages. This family shares a strict, but tractable, syntax that mere mortals can 
learn and use. Notice that in the grand scheme of things, SGML and XML 
are both metalanguages, while HTML is a single SGML application (that is, 
an instance). 
WML and VXML 
HTML has two younger siblings: Wireless Markup Language (WML) and 
VoiceXML (VXML). In the "family" analogy, one might say that HTML is a young 
teenager, WML is a toddler, and VXML is a newborn. Both WML and VXML are 
XML-compliant markup languages. 
The term "Wireless Markup Language" seems to imply that there is some-
thing special or unique about wireless communication that requires separate 
handling from other types of communication. In fact, WML does not rely on or 
exploit the "wireless versus wired" distinction in any essential way. WML is better 
understood as a low-bandwidth, low-resolution markup language. In other 
words, WML is targeted at being rendered in environments (devices) with low 
communications bandwidth, limited display capabilities, and limited computing 
resources. It was simply a mark of the times that in the late 1990s wireless devices 
happened to be low-bandwidth, low-resolution devices. In the coming years, 
when wireless bandwidth improves and mobile gear has small, high-resolution 
displays, wireless devices will probably render HTML (or its successor). On the 
other hand, future toasters and other relatively low-tech household devices may 
render WML on small, cheap displays, even though they are connected by a wired 
household LAN. 

The Conuergence of Speech and the Web 
The difference between WML and HTML (syntactic differences temporarily 
aside) is really one of capability rather than one of paradigm. At the 30,000-foot 
level, WML and HTML follow the same usage paradigm: computer displays infor-
mation and choices visually, user enters data and makes choices with his or her 
hands (through a mouse, keyboard, or keypad). VoiceXML implements a different 
usage paradigm than its siblings: In VoiceXML, the computer and person take 
turns speaking and listening to each other. Thus, VoiceXML differs from WML 
and HTML in that it implements a fundamentally different model for 
human/ computer interaction. 
Why Merge Speech and the Web? 
Some benefits of putting speech recognition, speech synthesis, XML, and the 
Web together are as follows: 
• Extend the reach of the Web. People can access the Web from anywhere they 
can make a phone call. People get increased access to goods and services 
and businesses get increased access to their customers. 
• Make the Web easier to use. VoiceXML relaxes the requirements on 
being able to use the Web from "literacy plus hand-eye coordination" 
to "being able to carry on a conversation." These relaxed requirements 
include people with sight disabilities (for example, blindness) as well as 
people who are illiterate or have cognitive disabilities (for example, 
dyslexia). 
• Increase the available options for computer/human interfaces. Until 
VoiceXML, developing a voice-based interface (for instance, Interactive 
Voice Response) was an expensive, programming-intensive activity 
requiring specialized hardware and software. With VoiceXML, devel-
oping a voice-based interface is inexpensive and viable even for 
low-end applications.2 
• Reduce infrastructure costs. Businesses currently maintain separate 
infrastructures for call centers and network computing. VoiceXML offers 
the possibility of saving money by merging these functions into one 
Web infrastructure. 
2 For an example of a low-end application, play the Tellme blackjack game by calling (800) 
555-8355 and saying "entertainment" at the main menu followed by "blackjack." [Tellme is a 
VoiceXML vendor, and its voice site is completely written in VoiceXML.) 
13 

Chapter2 
14 
Making Voice User Interfaces Easy to Build and Use 
Speech technology has traditionally been an expensive, high-end technology 
available only to businesses with enough money, solid technical capabilities, and 
a strong business need for it. Combining voice technology with the simplicity of 
markup languages makes it dramatically simpler and cheaper to "program" 
a Voice User Interface (VUI). Furthermore, voice technology's spread has been 
impeded by the fact that it is error-prone and still cannot handle natural lan-
guage. The Web provides a relatively simple framework for interaction with 
computers. Within this framework, current voice technology is acceptable and 
useful. Speech recognition can adequately handle the basic Web requirements of 
navigating menus, traversing links, and entering data into forms. Combining 
recordings with TTS synthesis is sufficient to develop intelligible voice interfaces 
that can handle the Web's open -ended store of content. 
Summary 
This chapter reviewed some of the technologies converging in the emergent 
voice-enabled Web. These technologies include speech recognition, speech syn-
thesis, and markup languages. Each is a powerful technology in its own right, 
and each has its own history and drivers. This background sets the stage for the 
next chapter, which explores how the architecture ofWeb applications draws on 
these underpinnings. 

CHAPTER 3 
The Evolution 
of Web Application 
Architectures 
Tms CHAPTER BRIEFLY TRACES the architectural evolution of the Web from a simple 
mechanism for sharing published information electronically to its current role as 
a public infrastructure for communication, interaction, and commerce. In the big 
picture, enabling the Web with voice is just one piece of the larger mosaic that 
comprises the Web. At a more detailed level, voice-enabling technologies such as 
VoiceXML are just beginning on a growth path that has already been taken by 
vision-enabling technologies such as HTML. 
The Good Old Days: Browsers, Servers, and Content 
Before the Web, you could share information over the Internet through tools such 
as Gopher, but the textual presentation of the information was crude. The Web 
superimposed a publishing paradigm on shared information: On the Web, infor-
mation is structured as a related set of documents that are published and 
rendered electronically. From this publishing paradigm derived some of the basic 
technologies of the Web. For example, HTML derived from SGML, a languishing 
technology for representing structured electronic documents. The stateless 
nature of the core HTTP protocol is consistent with the publishing paradigm: 
Rendered documents are viewed by readers. Does anyone "interact" with a book? 
Architecturally, the requirements to publish documents to the Web were sim-
ple. To publish on the Web, you scrounged up a server, installed some free 
software, ran a phone line to somebody that was already wired, and listened for 
HTTP requests on port 80. A document was published as one or more files placed 
in a directory tree on the file system. 
A key architectural innovation popularized by the Web is the distinction 
between content and presentation. Content is information that is stored and 
shipped around between machines on the Internet. To be useful to humans, con-
tent must be presented (rendered) in a format that people can comprehend and 
manipulate. The Web introduced the concept of a browser as the mediator between 
15 

Chapter3 
16 
a person and content fetched from the Web. In visual terms, the browser acts as 
a window in which various types of content can be rendered. The browser: 
• Provides functions needed to locate and download content from servers. 
• Does everything required by the host environment to act as a well-behaved 
graphical application. 
• Renders HTML. 
• Calls on specialized rendering software to render non textual content. 
Early on, the relationship between content and its rendition was simple: 
Content arrived in files, content was typed (by MIME types and/ or file 
extensions), and content types had renderers. Things rapidly became more 
sophisticated with plug-ins, applets, ActiveX controls, and so on, and the 
distinction between content and computer programs was blurred. 
NOTE 
The Internet Engineering Task Force (!ETFJ maintains a list of 
Multipurpose Internet Mail Extension (MIME) types. A MIME type is 
a standard identifier for a particular type of content. The identifier usu-
ally consists of a content type and a subtype-for example, "text/plain" or 
"imagelgif" MIME types are heavily used in the "Content-type" header on 
HTTP messages, and they play a vital, but largely unsung, role in enabling 
the whole multimedia Web experience. For more information, visit the 
IETF's MIME Request for Comments document at 
http://www.ietf.org/rfc/rfc1521.txt?number=1521. 
HTML is the glue that ties together related content and drives the browser. 
The browser renders its way through an HTML stream, and along the way it must 
call on other renderers to render images, sounds, spreadsheets, and so on. In 
a sense, HTML is a rendering language that is "interpreted" by the browser. 
Before the browser paradigm, developing a GUI was a programming-
intensive activity. Software developers wrote software programs that embedded 
calls to an underlying windowing system API, such as Windows, Motif, Apple, X, 
and so on. The programs were expensive to write, debug, and deploy, and they 
were specific to the underlying windowing system. With the advent of the Web 
browser, anyone with a text editor and basic knowledge of HTML could program 
a GUI that could run anywhere. 

The Evolution ofWeb Application Architectures 
Sessions and Dynamic Content 
As the Web became popular, the simple model ofWeb users "viewing" static doc-
uments became inadequate. For example, there's a lot of useful data that people 
want to view in databases, and databases are not documents in the Web sense. 
Correlated with the demand for a more inclusive notion of"document" was the 
desire to be able to tailor the information presented to the task and person at 
hand. Given that the document wasn't really rendered until it hit the browser's 
screen, why not "create" documents on demand? Why did documents have to 
live in files anyway? The needed ingredients were a way to identify the person 
doing the browsing so that information could be tailored for him or her and 
a way for the Web server to initiate some general-purpose computing that 
returned an HTML stream that could be served to the client. 
Sessions 
As originally conceived, the HTTP protocol was stateless and anonymous. 
A browser requested a document from a server, the server returned it, and the 
transaction was done. Any browser that made the same request got exactly 
the same document returned. 
Some gross but effective techniques for maintaining information about 
a user's interaction with a Web site were rapidly hacked into place. Cookies and 
URL rewriting are the most common. Both are techniques for piggybacking infor-
mation about user identity onto HTTP requests: Cookies store information in 
HTTP headers, while URL rewriting stores information in URL search strings. 
Generating Content Dynamically 
On the server side, techniques for creating HTML documents on the fly were 
quickly developed. An early and popular technology was Common Gateway 
Interface (CGI). In CGI, a certain part of the namespace of a Web server 
(http: I I . .. lcgi-bin) is treated specially by the Web server. Rather than serve 
the files in this part of the namespace as content, the Web server treats the files 
as executable (script). The Web server synthesizes a command line, passes any 
HTTP request parameters as command-line arguments, and throws it over to the 
operating system to execute. From the operating system, the Web server receives 
a handle to retrieve and pass through the HTML generated by the executed pro-
gram. As far as the browser knows, it visited a URL and got an HTML stream back. 
CGI has the advantages of being simple, flexible, and popular with all kinds 
of people, because the programs that generate content can be written in any-
thing from Perl to shell script to C++. However, CGI suffers from architectural 
17 

Chapter3 
18 
problems: poor scalability and dubious security. Relying on an OS to start, sched-
ule, and tear down a process for every hit is slow and computationally expensive. 
Security is an issue because CGI scripts often access other, security-conscious 
subsystems (for example, databases), and the issue of maintaining end-to-end 
security from a user through a browser through a Web server through a CGI script 
to a database management system (DBMS) is not addressed architecturally. 
Application Servers 
The early techniques for managing user state and generating content on 
demand, while minimally secure and far from perfect, enabled the evolution of 
Web servers from document vending machines into sophisticated environments 
for running complex distributed applications. 
Early attempts to improve on CGI focused on directly extending the capabili-
ties of the Web server. A server-side include (SSI) is a mechanism for embedding 
executable instructions into the content being served. As the server parses 
through content, it recognizes and acts on server commands, which produce the 
actual content served to the browser. Server extension APis such as ISAPI and 
NSAPI enabled programs running on the server to interact with the Web server 
directly. Unfortunately, these direct extensions were implemented directly in the 
Web server software, making Web servers proprietary. The extensions also com-
plicated Web server implementations by compounding technical issues 
concerning threading and resource management that are much simpler in a pure 
Web server. 
The development of component-based server extensions sidestepped the 
propriety issue. This approach leaves Web servers as they are but defines a com-
plementary component-hosting server that interacts with the Web server 
through existing protocols. The component hosting server, or application server, 
provides a robust, general-purpose environment for components. The appli-
cation server handles the OS-like functions of resource management, thread 
management, I/0, and so on, while the components implement transformations 
on content streams. This makes server-side components relatively easy to pro-
gram and deploy and enables Web servers to do what they do best: serve content. 
Of course, the application server is tied to the component architecture it imple-
ments, so the issue of propriety was moved but not solved. 
GUis, WUis, and VUis 
As summarized in Table 3-l, GUI, WUI, and VUI represent the major markup 
language-based browsing interfaces to the Internet. GUI is the most fully devel-
oped of the three and is exemplified by products such as Netscape Communicator 
and Microsoft Internet Explorer. WUI, the next most developed interface, is 

The Evolution ofWeb Application Architectures 
implemented by "micro browsers" embedded in wireless phones and personal 
digital assistants (PDAs). VUis are just beginning to appear as browsing interfaces 
to the Internet, and they are driven by the standardization ofVoiceXML 1.0. 
Table 3-1. Markup Languages and Browsers 
USER INTERFACE 
GUI (graphical) 
WUI (wireless) 
VUI (voice) 
BROWSING TECHNOLOGY 
HTML,XHTML 
WML 
VXML 
ORGANIZING PARADIGM 
USAGE MODE 
Page 
Monitor, keyboard, 
and mouse 
Card 
Dialog 
Portable handheld 
Phone 
The markup languages for these three types of interfaces are all based on 
XML. XML is a markup metalanguage derived from SGML. XML simplifies some 
of the complex and little-used features of SGML, but it still provides a flexible and 
extensible base for defining specialized markup languages. For more on XML, see 
Chapter 11. 
HTML 
HTML was originally conceived as the "stitching" needed to weave together the 
"World Wide Web" of multimedia documents. In this role, HTML is appropriate 
because it is simple and platform independent, and it provides a lingua franca for 
linking together documents. Basic HTML had a simple model of interaction with 
the user. The browser rendered documents and forms onto screen real estate 
owned by the browser. The user could "interact" with the browser by clicking a link, 
entering text into a form field, or pressing a form button to submit data to a server. 
Surprisingly, this simple model was rich enough to support the rapid morph-
ing of HTML into a platform for building GUis. As the Web took off, it became 
possible for anyone with a text editor to put together a rudimentary GUI in 
a matter of minutes, changing forever the economics of developing graphical 
interfaces. More sophisticated HTML-based GUis were enabled by the fol-
lowing innovations: 
• Extension of the concept of"content rendering" to include software com-
ponents (plug-ins, applets, ActiveX, and so on) invoked by the browser to 
control small pieces of the screen 
• Architectural enhancements to bind the GUI to server-side programs in 
a user session 
• Enhancements to HTML itself 
19 

Chapter3 
20 
The net result is that today it is possible to build very sophisticated GUis 
using HTML and the Web. However, you can't do it using your trusty old text edi-
tor. In fact, HTML may be on the path to becoming a purely generated language 
created by programs such as WYSIWYG GUI editors and application servers 
solely to drive browsers and looked at by humans only in the direst need. (If you 
have doubts about this trend, go to your favorite Web page and have your 
browser display the HTML source.) 
NOTE 
HTML is not, strictly speaking, a true XML application. Some syn-
tactically sloppy shortcuts in HTML violate the strict syntax rules ofXML. 
Extensible HTML (XHTML) is a slightly reformulated version ofHTML 
that does conform to XML. XHTML is an up-and-coming standard now 
and is expected to supplant HTML soon. Although there are no major con-
ceptual or functional differences between HTML and XHTML (as far as 
Web browsing goes), there are subtle but important technical differences. 
As a well-behaved member of the XMLfamily, XHTML enables a suite of 
powerful XML-based technologies that HTML does not. These technologies 
are explored in Chapter 11. For more information on XHTML, visit 
XHTML.org (http: I IWVM . xhtml. org/ ). 
WML 
WML is a modernized, "lite" derivative of HTML that is optimized for running on 
devices that have low communication bandwidth, limited computing resources, 
and small, low-resolution displays. (It sounds like a dirty job, doesn't it?) To meet 
these performance constraints, WML is compiled into a compact binary repre-
sentation before being sent to the handheld. The markup was designed to be 
renderable by a WML browser (microbrowser) that is simple and has a small 
memory footprint. To reduce the number of network exchanges, WUis are orga-
nized into "decks" of"cards." An entire deck is downloaded at once, but the 
browser displays only one card at a time. 
WML is not a subset of HTML for the following reasons: 
• There are tags in WML that are not in HTML. 
• WML is an XML application, while HTML is not. 
• WML, although a simpler language than HTML, has a more sophisticated 
model of browsing. WML has the concept of "decks" of "cards," where one 
card at a time is displayed. HTML has a simpler "one file, one page" model. 

The Evolution ofWeb Application Architectures 
Linguistic hairsplitting aside, WML WUis really work like lightweight HTML 
GUis. There's a screen, text and graphics, and links to click and buttons to push. 
The overall structure of how the user interacts with the interface is the same for 
WML and HTML. 
NOTE 
Visithttp://www.wirelessinanutshell.com/ to learn more 
aboutWML. 
VoiceXML 
Unlike HTML, which has roots in publishing, VoiceXML comes from a program-
ming language background. It has the earmarks of a programming language: 
control constructs, variables, event handlers, nested scoping, and so on. 
VoiceXML was designed from the beginning to be a lightweight, easy-to-learn, 
interpreted programming language for developingVUis. 
VoiceXML structures interactions with the user into dialogs. A dialog consists 
of a sequence of prompts spoken by the computer and responses spoken by 
a person. The person can speak responses or key them in using a keypad. VUI 
dialogs are by nature sequential and linear, in contrast to GUI windows, which 
are multi tasked and two-dimensional. 
Architecturally, VoiceXML interfaces are event -driven interfaces just like 
GUis. In a dialog, the computer speaks a prompt and then waits for the user to 
respond to it. The computer then waits until a speech recognition event occurs. 
A speech recognition event is initiated by the speech recognition engine, which is 
continuously analyzing the user's speech and attempting to match it to expected 
responses in the dialog. There are a number of possible speech recognition 
events, including "recognized response blah blah blah," "got a response but didn't 
recognize it," "no response," and so on. Therefore, unlike GUis and WUis, where 
the events that drive the interface are low-level, unambiguous occurrences (but-
ton pressed, mouse clicked, and so on), events in VoiceXML interfaces are the 
result of complex, computation-intensive, potentially erroneous processing. 
Summary 
At its inception, the Web was envisioned as a medium to publish, share, and 
interlink electronic documents. As the Web became more popular, the focus 
shifted from electronic publishing of static documents to an interactive infra-
structure for generating, processing, and displaying information. The content 
21 

ChapterS 
22 
comes from a variety of sources, including files, databases, and computer pro-
grams. A browser interprets markup language embedded in the content, renders 
it to a particular medium, and interacts with the user. HTML renders to a com-
puter display (and a sound card) and interacts with the user through a keyboard 
and mouse. WML works with low-capability handheld devices. The newcomer to 
the scene, VoiceXML, renders content as speech and interacts with the user using 
speech recognition and speech synthesis technologies. 

Part Two 
The VoiceXML Language 
HAVING ESTABLISHED A technical context for voice in general, it's time to focus on the 
specifics of the VoiceXML language. As languages go, VoiceXML is not par-
ticularly tough. However, developing interfaces for voice is quite different than 
developing graphical or text-based interfaces. Voice and graphical interfaces dif-
fer in structure, in how errors are created and perceived, and in how information 
is processed. Some of the material in this part is the usual "syntax and seman-
tics" common to all introductions to programming languages. Much of the 
material, however, is intended to help you think about and design high-quality 
voice interfaces. 
Chapter 4 introduces the Simplified Personal Information Manager, a voice-
enabled Web application that animates the tutorial in this part and the prototype 
in the next part. Chapter 5 introduces the concepts that the VoiceXML language is 
built around. Chapter 6 dives into the nuts and bolts of setting yourself up (in 
terms of hardware and software) as a VoiceXML developer. With your environ-
ment in place, Chapter 7 provides a step-by-step tutorial that acquaints you with 
all the key features ofVoiceXML 1.0. Building on your understanding of what 
VoiceXML can do, Chapter 8 offers guidance about how to design a good voice 
interface usingVoiceXML. Chapter 9 provides a reference-style discussion of the 
machinery behind features explored in the tutorial. Finally, Chapter 10 explores 
some advanced topics and issues that become apparent when you develop real-
world VoiceXML applications. 

CHAPTER 4 
Personal 
Simplified 
Information Manager 
Example 
THROUGHOUT THE REST of this book, you will work with a simplified personal infor-
mation manager (SPIM) application. A SPIM is a slimmed down personal 
information manager (PIM) with the following features: address book, appoint-
ment calendar, and to-do list. A SPIM is accessible through voice, wireless, and 
Web interfaces. As you work through the examples in the book, you will develop 
fully functioning components of your SPIM, but your intent should not be to 
implement a complete, robust PIM. 
This example was chosen for the following reasons: 
• It is simple to understand and useful. Many people are familiar with some 
kindofPIM. 
• It can be used in a variety of situations and environments. Some use cases 
naturally involve Web access, some involve wireless access, and some 
involve voice access. 
• It can be developed in useful increments, so I can illustrate points on an 
ongoing basis. 
In the following sections, I analyze the SPIM as a serious but petite appli-
cation. The core use cases are elaborated and diagrammed in Universal Markup 
Language (UML) .1 The basic diagrams used here are pretty intuitive, so you 
should be able to grasp the intent of the diagrams without knowing UML. 
1 UML is a notation for object -oriented analysis and design. It was developed by Rational 
Software and is called "universal" because it incorporates the methodologies championed 
by Booch, Jacobson, and Rumbaugh, as well as others. For more information on UML, the 
Rational Rose object -oriented design and development tool (which was used to draw 
the diagrams here), and pretty much anything to do with developing object-oriented 
software, visit the Rational Software Web site (http: I /www. rational. com/). 
25 

Chapter4 
26 
Use Case Analysis 
A full PIM can be used in so many ways that a full use case analysis would proba-
bly involve dozens, if not hundreds, of use cases. In the sample application, you 
will leave room for future growth and build menus assuming there's a greater 
variety of functions than you'll actually implement here. For pedagogical pur-
poses, you will focus on three representative use cases for the SPIM. They are 
"representative" in terms of both the underlying technology involved and how it 
is used. 
The use cases are as follows: 
• Editing information about a contact (for example, name, address, phone 
numbers, and so on) 
• Handling a scheduled appointment that you're late for 
• Reviewing your list of things to do today 
The sections that follow elaborate on these cases. 
Top-Level Use Case 
The top-level use case diagram in Figure 4-1 shows the intentionally limited 
scope of the SPIM. In a "real" PIM, there would be many more use cases at this 
level-for example, "call a contact," "schedule an appointment," "add an address 
to an existing contact," and so on. 

Simplified Personal Information Manager Example 
0 
.. 
/Ed;tcootoctl"fo 
... 
~ 
;z 
0 ;z 
~ r 
SPIMU•~""";"gloto 
Attendee 
• 
/ 
J 
0 
ReviewSchedule 
Figure 4-1. SPIM top-level use case 
During use case analysis, SPIMUser's access mode (eye or ear) is not 
assumed or implied unless the access mode is intrinsic to the requirements of 
the use case. For example, the use case in the next section (titled "Edit Contact 
Information Use Case") should be the same regardless of access mode, because it 
is an essential function of the system. On the other hand, a hypothetical "Dial 
Roadside Assistance from My Broken-Down Car" use case could, arguably, pre-
sume that the user is accessing the system through a VUI or WUI, but not a GUI. 
The sections that follow drill down another level into these scenarios. Notice 
that every case starts with the Authenticate use case. This shared case models the 
process of identifying the current user to the system. Conventionally, authenti-
cation is performed by a username/password-based login, but it may use other 
mechanisms for voice (as you will see in Chapter 12). 
27 

Chapter4 
28 
Edit Contact Information Use Case 
You receive a change of address notice in the mail from a friend. You're not going 
to call the friend right now or schedule a meeting with him or her, but you want to 
record the address change. First, you look up the existing contact information for 
your friend, and then you selectively modify it. To look it up, you can either 
browse your address book (if it's small) or search your address book by name. 
Once you have the information at hand, you can either review it in summary 
form (the types of addresses you have on file) or in detailed form (a full listing of 
address contents). Then you update the fields that have changed. Figure 4-2 illus-
trates this use case. 
0 
Authonlicalo 
lo~2 
~s:lociContac~O 
~ 
BrowseContact 
SPIMUs~~ ~0 
0 
!:mmarizeContact 
eviewContactlnfo 0 
OotoiiContoct 
0 
EditAttribute 
.., 
Figure 4-2. Edit Contact Information use case diagram 

Simplified Personal Information Manager Example 
Running Late Use Case 
Your schedule is jammed solid and you've just escaped from a meeting that ran 
over. You don't have time to fiddle around with rescheduling things, so you want 
to notify your next appointment that you'll arrive late. You access your SPIM, 
get the relevant contact information for your next appointment, and commu-
nicate the fact that you'll be late by the best means possible: e-mail, phone call, 
wireless text message, or voice mail. Figure 4-3 illustrates this use case. 
0 
Aulhenticate 
/ 
0 
~~arizeAppointment 
SPtMUi~ 
Re-.iewAppointmtnt 
0 
~ 
OetaiiAppointment 
oi'IE--.. --~ 
NotffyAJtendee 
Altendee 
Figure 4-3. Running Late use case diagram 
Review Schedule List Use Case 
To refresh your memory on what you have scheduled for the upcoming week, 
you want to review your appointments on a day-by-day basis. In your SPIM, you 
identify the day you're interested in and then review that day's planned activities. 
As you progress through your list of appointments, you want to scan quickly, 
ignore things you already recall, and selectively focus on particular items of inter-
est. Figure 4-4 illustrates this use case. 
29 

Chapter4 
30 
-t• R.uhunt.tl Ros~ - SPIMI.null- (Utii!' (.n~e Diaqranr.: u .. e Lne Vll"-W / A.evlewSth~uleOom) 
.. r.J 
r 
..J 
0 
Authenticate 
I' 
/
0 
~~-·~·~· 
SPIMUser 
0 
SelectAppointment 
0 
RevtewAppomtmenl 
T oolo Add-Ins 
Figure 4-4. Review Schedule use case diagram 
Object Model 
The core objects in a SPIM are as follows: 
• A contact is an entity that you communicate with. To initiate communi-
cation with a contact, you can use an address that is appropriate for your 
chosen communication method (a phone number for a phone call or fax, 
an e-mail address for e-mail, and so on). 
• An appointment is a scheduled event that involves you and one or more 
contacts with whom you will interact. The venue specifies how the inter-
action will occur (physical meeting, teleconference, and so on). 
• A schedule is a list of appointments that fall in a given time span (this after-
noon, today, tomorrow, and so on). 
Figure 4-5 captures the relationships between these objects. 
X 

Simplified Personal Information Manager Example 
.. t 
Contact 
Person 
omeAddress · Address 
Gbt>usinessAddress : Address 
~rstName · Stringl«---l~honeNumber : PhoneAddrss 
~a stName · String O. 1 
~AXNumber PhoMAddress 
~M ai!Address : EMai!Address 
1 n 
Time 
.~year Integer 
~on t h : EnumMonth 
~ay EnumOayOIWeek 
~our Integer 
IIPOrrunute : Integer 
• elapsedYearsO 
• elapsedMonthsO 
• elapsedOaysO 
• elapsedHoursO 
• elapsedMinutesO 
• elapsedSecondsO 
Figure 4-5. SPIM class diagram 
Summary 
In this chapter, I've laid out the skeleton of a simple, but realistic, application 
example. The core use cases were elaborated and diagrammed in UML. From the 
use cases, a basic object model was derived and diagrammed. Applying this level 
of formality to the example is intended to drive home two points. 
• The examples presented throughout the book are not "concocted" exam-
ples that show off technology features but never occur in practice. Rather, 
the examples are practical because they derive from a real application. 
• Voice-enabled applications are not a new, different kind of application. 
The best practices of application design, development, and software engi-
neering apply to voice-enabled applications. 
In the chapters that follow, you'll use the SPIM application to lead you into 
VoiceXML programming. 
31 

CHAPTER 5 
VoiceXML Concepts 
THIS CHAPTER INTRODUCES VoiceXML. It begins with a brief history ofhowVoiceXML 
was developed and provides an overview ofVoiceXML technical concepts. This 
sets the stage for subsequent chapters, which drill down into specific topics using 
the SPIM application to illustrate how speech user interfaces are expressed 
in VoiceXML. 
VoiceXML History 
The major driver for development ofVoiceXML has been the desire of the tele-
phony industry to make existing telephone networks a vital part of the 
Information Age. Obviously, telephone companies and our society as a whole 
have a strong investment in the telephone and Public Switched Telephone 
Network (PSTN). Access to the Internet over dial-up connections was and is 
a major component of the Internet's success. Technically, however, the encoding 
and transmission of digital data over telephone networks originally intended for 
analog voice communication is somewhat inefficient. On the other hand, using 
voice to access the Internet would capitalize directly on the existing, proven, and 
highly tuned capabilities of Plain Old Telephone Service (POTS). 
AT&T, Lucent Technologies, and Motorola began discussing the possibility of 
developing a common language for voice-enabled applications in 1998. The 
VoiceXML Forum was formed in 1999, and IBM became the fourth founding 
member. The initial specification, VoiceXML 0.9, was released in August 1999. 
A process of public review and response to comments culminated in the release 
of the VoiceXML 1.0 specification in March 2000. Following development of the 
language definition, the VoiceXML Forum turned custody of the specification 
over to the World Wide Web Consortium (W3C). The Forum reorganized itself 
and broadened its charter to include more of a general role as a technology and 
industry advocate for the VoiceXML community. See Figure 5-l for a detailed 
timeline of the previously described events. 
33 

ChapterS 
34 
1998 
I 
AT&T, Motorola, 
Lucent discuss 
common 
language 
Motorola 
announces VoxML 
W3C opens discussion 
regarding a voice 
browser language 
recommendation 
1999 
I 
-
I 
IBM announces 
L_ 
~sp_ee~c=hM=L=========~r---
1 
Announce 
!-
formation of 
VoiceXML Forum 
1......----..1 
'
lucent announces 
1-
TelePortal 
._____ __ 
____, 
2000 
I 
j 
VoiceXML 
-1 0.9 
2001 
2002 
I 
I 
W3C preparation 
of VoiceXML 2.0 
---1 draft & speech 
grammar ML, TTS 
ML, others 
Forum: 20+ Promoters, 
-----1 250+ Supporters; many 
platforms, services, 
applications 
Submission of 
1.0 to WJC & 
reorganize 
VoiceXML 
Forum 
I VoiceXML 1.0 I 
Figure 5-l. Time line of speech and telephony activities (courtesy of the 
VoiceXML Forum) 
The VoiceXML Web site (http: I /www. voicexml. org/ goals. html) describes the 
Forum's goal as follows: 
"The VoiceXML Forum is an industry organization founded by AT&T, IBM, 
Lucent and Motorola, and chartered with establishing and promoting the Voice 
Extensible Markup Language (VoiceXML), a new specification essential to mak-
ing Internet content and information accessible via voice and phone." 
Now the W3C is overseeing the ongoing specification of the VoiceXML lan-
guage and other speech-related technologies, including 
• Speech grammars 
• Voice dialogs 
• Voice synthesis 
• Pronunciation lexicon 
• Call control 
• Natural language representation 

• Multimodal systems 
• Reusable dialog components 
Visit the VoiceXML Forum's Web site (http: I lwww. voicexml. orgl) for infor-
mation about VoiceXML Forum activities and members. Visit the W3C's Voice 
Browser Activity area (http: I lwww. w3c. orgiVoice) for information about the 
W3C's work on various voice-related technologies. 
Voice Web Browsing 
Accessing a Web site through the telephone is different than ordinary Web brows-
ing in both user experience and technical implementation. A typical 
Web-browsing session goes something like this: 
1. 
You sit down at a computer, open a Web browser, and type in a Web 
address (URL). 
2. 
The Web browser sends a request to the Web server. 
3. 
The Web server sends a response to the Web browser, which includes an 
HTML document. 
4. 
The Web browser "renders" the HTML document onto your screen. 
5. 
You view the display and navigate the site by clicking and typing. 
Getting the same information from the same Web site using VoiceXML goes 
something like this: 
1. 
You pick up a phone and dial the Web site's phone number. 
2. 
A VoiceXML gateway answers your call and sends a request to the 
Web server. 
3. 
The Web server sends a response to the VoiceXML interpreter, which 
includes a VoiceXML document. 
4. 
The VoiceXML interpreter "renders" the VoiceXML document by speak-
ingtoyou. 
5. 
You listen and then respond by speaking or pressing keys on the tele-
phone's keypad. 
VoiceXML Concepts 
35 

ChapterS 
36 
NOTE Even though you can use your wireless phone to access the Web 
using voice (VXMLJ or Web browsing (WML), you can't do both togethe1: 
When using voice, the link is over the telephone network; when using 
wireless, it's over a WAP link. Currently, handheld devices can only com-
municate over a single type of link at a time. Even if your handheld device 
is capable of servicing both types of links simultaneously, there's no proto-
col to correlate what's happening on the phone with what's happening on 
the WAP link. 
From the user's perspective, the difference is in the medium by which input 
and output are conveyed. With a conventional Web browser, you receive the out-
put from the computer through your eyes and send input to the computer with 
your hands. With VoiceXML, you receive output from the computer through your 
ears and send input to the computer with your voice (see Figure 5-2). 
tSSSTIME 
{Dials 1-SSS-TIME} 
P(person): What time 
is it in Brussels? 
((computer): three 
oh five pm 
Figure 5-2. Voice Web browsing 
Gateway 
http://...,.. timeanddate. COil 
Web & 
Applicat ion· 
Server 

From a technical perspective, HTML Web browsing involves two computers 
(browser and server) connected by one IP network (Internet or intranet). The 
Web browser uses the display, keyboard, and mouse of its host computer for 
input and output. 
Browsing with VoiceXML involves a telephone, two computers (gateway and 
server), and two networks. An IP network connects the Web server to the 
VoiceXML gateway, and the PSTN connects your telephone to the VoiceXML gate-
way through an ordinary phone call. The VoiceXML gateway interacts with the 
Web server using the standard HTTP protocol, just like a Web browser. However, 
it "renders" the VoiceXML document by sending its output to, and receiving its 
input from, your telephone over the PSTN. 
A VoiceXML gateway has the following subsystems (shown in Figure 5-3): 
• Network Interface: Enables HTTP communication with Web servers 
• VoiceXML Interpreter: The software that carries on a conversation with the 
user, as specified in a VoiceXML document 
• Text-to-Speech (TTS): Translates text to the spoken word 
• Audio: Plays and records audio files 
• Speech Recognition (ASR): Translates user utterances into text 
• DTMF: Translates keypad input into characters 
• Telephony Interface: Enables communications with the telephone 
network (PSTN) 
VoiceXML Concepts 
37 

ChapterS 
38 
VoiceXML Interpreter 
Telephony 
Network 
Interface 
Interface 
Speech 
Text-to-
Recog-
DlMF 
Audio 
Speech 
nition 
(TTS) 
• 
Voice 
HTTP t 
8 
8 
Figure 5-3. VoiceXML gateway subsystems 
Elements of VoiceXML 
In HTML, the basic element of retrieval is a page, which is a document with a 
certain address. When a Web browser receives an HTML document from a Web 
server, it renders the entire page to the screen at once. A VoiceXML document 
specifies an entire conversation, which consists of interchanges between the 
caller and VoiceXML interpreter. In each of these interchanges, the VoiceXML 
interpreter reads or plays a prompt and the caller responds with information or 
a command. The VoiceXML interpreter thus "renders" the VoiceXML document 
one exchange at a time. (In this respect, VoiceXML is more similar to WML, where 
the WAF phone displays one card at a time.) 
Dialogs 
In VoiceXML, dialogs are the building blocks for conversations. The VoiceXML 
interpreter "renders" a VoiceXML document by carrying on a conversation with 
the person at the other end of the telephone call. The person and the VoiceXML 
interpreter take turns speaking and listening. At any point in time, the conver-
sation between the caller and gateway is "in'' one dialog. During a call, the 
conversation moves among dialogs in the application. The VoiceXML document 
contains elements that specify what the VoiceXML interpreter can say or play to 
the user and what the user can say or key to the VoiceXML interpreter. 

Navigation 
Web sites are usually organized into a shallow hierarchy of topics. Visually, this 
hierarchy is shown as a row of links at the top of each page (or column of links at 
the left of each page). Visitors click these links to navigate through the hierarchy. 
VoiceXML menus provide the audio equivalent of visual pick lists. The computer 
speaks the list of available choices, and the user responds by saying the desired 
option. VoiceXML links enable people to jump directly to a destination at any 
time simply by saying the name of the link. 
Forms and Items 
VoiceXMLforms serve the same purpose as HTML forms: They are used to collect 
information from the user and send it to a Web server. Both VoiceXML and HTML 
collect information infields. HTML field values are entered as text in the browser, 
while VoiceXML field values are spoken by the caller and translated into text 
by the VoiceXML interpreter. In both cases, when all the fields have been filled 
in, the completed form is sent to the Web server through an HTTP GET or 
POST request. 
To fill in a field, VoiceXML executes a form item. A form item may be a simple 
field elicitation that speaks a prompt (for example, "What city?") and captures the 
response. A form item may involve more complex processing, such as invoking 
another dialog, telephoning a third party, or executing a client-side script 
(ECMAScript, also known as JavaScript). 
When executing a form, the VoiceXML interpreter will repeatedly process 
form items until it determines that all fields have been filled. Notice that execut-
ing a form item by no means guarantees that the corresponding field is filled. For 
example, the user may fail to respond to a prompt, or the computer may not be 
able to recognize what the user said. 
Grammars 
At any point in a VoiceXML dialog, there is a predefined set of valid responses 
that a person can speak. A grammar specifies a set of responses that can be rec-
ognized by the computer. A simple grammar may specify some fixed phrases to 
be used as commands (for example). A more complex grammar may specify a set 
of basic words (the vocabulary) and multiple alternative rules determining the 
order in which the words may appear to form expressions or sentences. A gram-
mar is the primary input to the voice recognition technology that underlies the 
VoiceXML interpreter. 
VoiceXML Concepts 
39 

ChapterS 
40 
A grammar is specified inline, within a VoiceXML program, or in an external 
text file. VoiceXML 1.0 doesn't specify or require a particular grammar format, 
although two are widely used: JGSF (Java Grammar Specification Format) from 
Sun and GSL (Grammar Specification Language) from Nuance. The next release 
ofVoiceXML is expected to require that VoiceXML interpreters support the XML 
form of the W3C speech grammar language, but it permits the interpreter to 
accept other formats as well (see Chapter 14 for a fuller discussion of this point). 
Events 
Programming languages such as C++ and Java use exceptions to handle errors. 
VoiceXML events are based on the exception concept. Events are thrown when 
certain conditions are detected either by the VoiceXML interpreter or by the 
VoiceXML program itself. Event handlers are fragments of executable code that 
are invoked to catch an event. 
Due to the nature of speech, conversational dialogs are intrinsically real-time 
processes. Unlike graphical interfaces, where you can take a lunch break and pick 
up right where you left off, speech interfaces require responses within certain 
time periods and must explicitly accommodate real-time interruptions and dis-
tractions. In VoiceXML, events are not restricted to representing error 
conditions-they are used to represent all kinds of real-time interactions. 
Summary 
Beginning with a brief history ofVoiceXML, this chapter introduced the key con-
cepts that underlie VoiceXML. Browsing the Web by voice is a new activity 
enabled byVoiceXML that differs from using a conventional Web browser. To 
access a VoiceXML application, a person dials into a VoiceXML gateway, which 
contains all the hardware and software required to recognize speech, synthesize 
speech, and run the VoiceXML interpreter. The VoiceXML application is com-
posed of dialogs that structure the interaction between person and computer. 
VoiceXML provides links, which are the verbal equivalent of hypertext links. 
VoiceXML forms are composed of items, each of which elicits a particular piece 
of information from a person. Grammars specify what responses are valid at 
every point in a dialog. Events are used to handle errors and manage a conver-
sation in real time. 

CHAPTER 6 
Outfitting Your 
VoiceXML Expedition 
VmcEXML rs A NEW and burgeoning technology, so the landscape of available 
VoiceXML tools and products is changing at "Web speed." This chapter provides 
a quick orientation to the basic development approaches and tools. I'm not going 
to try to cover the various characteristics and features of each tool, because that 
information will undoubtedly be obsolete by the time you get it. However, I will 
provide pointers to places where you can find up-to-date information. 
Standalone versus Hosted Development 
In a deployed VoiceXML application, the VoiceXML browser runs on a VoiceXML 
gateway. The VoiceXML gateway may be a privately owned server (for example, 
one owned by your company) equipped with expensive telephony and voice 
software, or it may be a service you buy from a VoiceXML hosting vendor. In 
either case, the application developer provides the VoiceXML software and the 
host provides the gateway environment in which the software runs. You contact 
the hosted gateway by calling a telephone number that connects to the 
VoiceXML application. 
While the VoiceXML application is under development, the developer can 
run it in either a standalone or a hosted configuration. In a standalone configu-
ration, the "gateway" runs on the developer's own workstation. The developer 
uses a headset and microphone to listen and speak, and all speech synthesis and 
recognition is done on the developer's workstation (see Figure 6-1). In a hosted 
configuration, a VoiceXML hosting vendor provides the developer with access 
to a VoiceXML gateway. The vendor gives the registered developer a phone num-
ber, a personal identification number (PIN), and a way to upload VoiceXML 
source code to the gateway. To run the application, the developer uploads the 
software, calls the phone number (which may the same number for all develop-
ers), and enters the PIN. The gateway locates the uploaded software for that PIN 
and executes it (see Figure 6-2). 
41 

Chapter6 
42 
Figure 6-1. Standalone development configuration 
Figure 6-2. Hosted development configuration 
VoiceXML 
Gateway 
In practice, the application will probably morph from a standalone configu-
ration to a hosted configuration during the development cycle. In initial 
development, the standalone configuration is preferable because the code is 
changing rapidly and the tasks of uploading and phoning to try out every change 
are onerous. As the application stabilizes toward a software release, the developer 
needs to test the application in a hosted configuration (using the phone instead 
of the microphone), because that is how it will be deployed. When the appli-
cation goes into production, it will be in a hosted configuration. 

Outfitting Your VoiceXML Expedition 
Development Environment 
The basic requirements for developing a VoiceXML application are the same as 
those for developing an HTML application: an editor, a browser, and a Web server 
(for anything but the simplest apps). For developers still hoping that GUis are 
a passing fad, a text editor and a command line can fit the bill. For the program-
ming challenged, there are fully hosted Interactive Development Environments 
(IDEs) that provide sophisticated GUis. VoiceXML browsers are not as common-
place as HTML browsers (yet), but a number are available at little or no cost 
to developers. 
XML, and hence VoiceXML, is a text-based language. As a result, you can use 
any text editor to edit VoiceXML source. XML-specific editors provide niceties 
such as syntax coloring, text and tree views, and automatic validation. VoiceXML 
IDEs may provide drag-and-drop GUI-style editing. Take your pick or mix and 
match. A standard text format is a standard text format, so you don't have to 
restrict yourself to one tool. 
When selecting a VoiceXML browser for development, the major choice is 
between a hosted and a standalone configuration. At one end of the spectrum, 
there are hosted development environments that don't require you to install any 
software on your local workstation-the whole IDE runs in a Web browser against 
a remote host. On the other end of the spectrum, there are standalone configu-
rations, which require you to install all software locally and launch the VoiceXML 
browser from the command line or from within an IDE. The one you select is pri-
marily a stylistic choice based on the development approach that's comfortable 
for you. Remember that, once written, a VoiceXML program can run (in theory) 
on any VoiceXML browser. 
CAUTION 
VoiceXML gateway hosting vendors seem to be multiplying rap-
idly. Many of the sites appear to incorporate the "developer network" 
model. You, as a develope1; sign up and get access to development tools, 
documentation, forums, FAQs, samples, and so on. These nontechnical 
features can help you get up to speed quickly and draw on the experience 
of others, but beware the siren song of"free" proprietary stuff that locks 
you in with the hosting vendor. 
When you select your development environment, think about where you're 
going to use it. If you're a frequent-flying, laptop-coding developer, you'll proba-
bly want to go with a standalone configuration that can be loaded entirely onto 
your laptop. If you're a curious amateur with not much disk space left on the old 
home computer, a hosted configuration that minimizes local resource require-
ments is probably a good idea. 
43 

Chapter6 
44 
One feature to be aware of is the VoiceXML interpreter's acceptance of both 
voice and text input. When accepting text input, speech recognition is short-
circuited, so the text input is treated as output from the speech recognition 
engine. Text may be entered in interactive mode, where you hear the spoken 
prompts and respond by typing in text or picking from a menu that shows 
expected responses. Text may also be entered in batch mode, where the 
VoiceXML interpreter reads text from a script file. The ability to enter text is 
very useful in a number of situations, including when you are 
• Doing development on an airplane, in a shared office, or in a (semi-) public 
place where saying responses out loud might be embarrassing and/ or dis-
tracting to others 
• Developing a VUI for a language you are not fluent in 
• Performing automated testing (for example, regression testing) 
• Developing while you have a cold or other impediment that makes your 
voice hard to recognize 
• Going through rapid code/test sequences and you want to avoid repeating 
the same sequence of responses over and over 
• Developing on an underperforming machine whose resources shouldn't be 
squandered on speech recognition 
VoiceXML 1.0 versus VoiceXML 2.01 
The VoiceXML 1.0 specification was released in May 2000. Public release of 
the VoiceXML 2.0 specification is expected in late 2001 (according to W3C). 
At the time of this writing, there are quite a number (dozens) ofVoiceXML 
1.0-compliant browsers and IDEs available. There are a handful of tools claiming 
1 "VoiceXML 2.0" is the unofficial name, used throughout the VoiceXML community, of the 
next release of the VoiceXML language specification. The W3C doesn't use this name, and its 
web page simply states the next working draft from the Voice Browser Working Group is due 
in late 2001. Therefore, the version number assigned by the W3C may or may not be 2.0. In 
fact, the scope of changes that are anticipated to appear in the next specification may be 
more consistent with a "point-rev" (e.g. VoiceXML 1.1) than a "full-rev" (e.g. VoiceXML 2.0). 
In any case, the phrase "VoiceXML 2.0" should be understood to be an informal reference to 
the next release of the specification, whatever its official name turns out to be. 

Outfitting Your VoiceXML Expedition 
VoiceXML 2.0 compatibility (which is surprising, because there is no published 
standard to comply with). To avoid the pain associated with running at the bleed-
ing edge of technology, the code samples, examples, and tutorials in this book 
are based on VoiceXML 1.0. Where appropriate, I have called out changes 
that are expected to be included in VoiceXML 2.0. Given that the differences 
between VoiceXML 1.0 and VoiceXML 2.0 are expected to be evolutionary, not 
revolutionary, sticking with the more established and mature VoiceXML 1.0 
toolset seems prudent. 
Some Available Software Options 
For a comprehensive list of companies involved with VoiceXML (many 
of whom are vendors), visit the VoiceXML Forum's Member page 
(http: I /www. voicexml. org/member _companies. html). To find a variety 
of XML editors, visit XML.com's XML Editors page 
(http:/ /www.xml. com/pub/rg/XML_Editors) and Scripting News' XML Editors page 
(http://scriptingnews.userland.com/directory/1026/xmlEditors).Tables6-l 
and 6-2list some VoiceXML-related tools that are currently available. 
This book's companion CD contains the following software to get 
you started: 
• IBMWebSphere Voice Server SDK (trial edition): A standalone 
voice browser. 
• IBMWebSphere Studio (entry edition): A VoiceXML IDE that provides inte-
grated VoiceXML editing and integration with the Via Voice SDK for testing 
and debugging. 
• XML Spy: An XML IDE for editing XML Schemas, DTDs, XML files, and XSL 
transformations. It integrates with external XSL transformation engines for 
testing XSL transformations. 
45 

Chapter6 
Table 6-1. Selected VoiceXML Editing/Authoring Tools 
PRODUCT 
TIBCO Extensibility 
XMLSpy 
IBM Web Sphere Studio 
COMMENTS 
Full suite ofXML tools for 
editing schemas, instances, 
validation, and so on 
Integrated GUI for 
XMLIXSL development 
Web development IDE 
integrates VoiceXML support 
URL 
http://www.extensibility.com/ 
http://www.xmlspy.com/ 
http://www-4.ibm.com/software/ 
webservers/studio/ 
Table 6-2. Selected VoiceXML Browsers Available for Experimentation 
PRODUCT 
Be Vocal Cafe 
Cambridge Voice Studio 
Hey Anita FreeSpeech 
IBM WebSphere Voice Server SDK 
Informio Developer Network 
Motorola Mobile ADK 
Nuance V-Builder 
Tellme Studio 
VoiceGenie Developer Workshop 
Voxeo Designer 
CONFIGURATION 
Hosted 
Standalone 
Hosted 
Standalone 
Hosted 
Standalone 
Standalone 
Hosted 
Hosted 
Standalone 
URL 
http://cafe.bevocal.com/ 
http://www.cambridgevoicetech.com/ 
devsite/Developers.asp 
http://freespeech.heyanita.com/default.asp 
http://www-4.ibm.com/ 
software/speech/enterprise/ep_ll.html 
http://idn.informio.com/ 
http://mix.motorola.com/audiences/ 
developers/madk_intro_dev.asp 
http://www.nuance.com/products/toolkits.html 
http://studio.tellme.com/ 
http://developer.voicegenie.com/ 
http://community.voxeo.com/index. 
cfm?pageid=B1F070D5-4E49-467D-91979A961F1516C9 
Speech Developer Accessories 
46 
To look good and be effective, everyVUI developer needs one or more headsets 
(microphone plus earphones). At a minimum, you'll need a telephone headset 

Outfitting Your VoiceXML Expedition 
and a phone. You'll use this gear while testing your VoiceXML application in its 
deployed state, and you'll use it while developing in a hosted configuration. 
Using a headset is more comfortable than using a fixed microphone and it 
enables you to type while you talk. Because you'll want to test your deployed 
application in a realistic fashion, getting a top-quality telephone headset is prob-
ably not a good idea: You want to use consumer-quality equipment, just like all 
the people who will call into your site. 
If you're doing development in a standalone configuration, and/ or you're 
interested in learning about voice technologies other than VoiceXML, you'll want 
to invest in a comfortable, high-quality headset that connects to your computer. 
Don't skimp on quality here. If you want to try using other speech technologies, 
such as dictation systems (for example, IBM Via Voice, Dragon NaturallySpeaking, 
L&HVoice Xpress, or Philips FreeSpeech), the software will require you to use 
a high-quality microphone (and will refuse to enroll you unless you use one). 
TIP Strictly speaking, you could use your computer's speakers and get by 
with simply adding a microphone. However, if you do any of your develop-
ment within earshot of other human beings, they will probably appreciate 
your thoughtfulness in using a headset so they don't have to hear the end-
less playing and replaying of prompts as you build your application. 
Summary 
Getting set up to develop VoiceXML applications requires you to perform a few 
simple steps: 
• Decide whether you want to work in hosted or standalone mode. 
• Register with a gateway hosting service (hosted) or acquire and install 
VoiceXML browser software (standalone). 
• Install an XML-capable editor or IDE for entering and editing VoiceXML 
source (based on your preferences). 
• Get yourself a good-looking and comfortable headset. 
To make things easy, all the software you need to get started is included on 
the book's companion CD. 
47 

CHAPTER 7 
VoiceXML Language 
Tutorial 
IN THIS CHAPTER, You'LL develop VoiceXML code to implement some core functions 
of the SPIM application. Along the way, you'll explore most (but not all) features of 
VoiceXML (some advanced features are discussed separately in Chapter 10). The 
example files are all contained on the companion CD. Under the Tutorial directory 
on the CD, there are subdirectories labeled Step1, Step2, and so on, through 
SteplO. The tutorial refers to these steps as it progresses. 
From this tutorial, you'll learn the salient points ofVoiceXML without execut-
ing the sample code. However, if you're a hands-on type, I suggest you get outfitted 
(as described in the previous chapter) and try out the examples as you go. 
To make the tutorial examples executable in today's voice browsers, the 
examples conform to VoiceXML 1.0. Where appropriate, comments related to fea-
tures anticipated in VoiceXML 2.0 are inserted parenthetically [VXML 2: this is 
a parenthetical comment] or called out as follows: 
VXML 2: This is an example of a comment related to VoiceXML 2.0 . 
.. Hello, World! .. 
Before diving into the SPIM, let's take a look at a very basic VoiceXML program. 
The followingVoiceXML application simply speaks the phrase "Hello, World!" 
(using speech synthesis) and then exits. See Listing 7-1 (from 
SmallExamples\Example1.vxml). 
49 

Chapter 7 
50 
Listing 7-1. Hello, World! 
<?xml version="l.O"?> 
<!DOC TYPE vxml SYSTEM "http: I lwww. voicexml. org/voicexmll-o. dtd" > 
<! -- This is a simple "Hello, World!" voice application. --> 
<vxml version=" 1. o" > 
<form id="hello" > 
<block>Hello, World! </block> 
<!form> 
<lvxml> 
As with any other well-formed XML file, the first line identifies the version of 
XML. The DOCTYPE directive is an XML directive that tells the XML parser to parse 
according to the specified "schema." Following the DOCTYPE directive is a sample 
of the ghastly format of SGML (hence XML, hence VoiceXML) comments: The 
delimiters are<!-- and-->. The <vxml> tag identifies its content asVoiceXML 
1.0 code. This application is composed of a single form, which contains a single 
line of executable content. When the VoiceXML interpreter encounters text, it 
reads it aloud. As an alternative, you can replace the text with a recorded greet-
ing, as in Smal1Examples\Example2.vxml. 
<block> <audio src=" SmallExamples/HelloWorld. wav" /></block> 
SPIM Menu Navigation 
The SPIM menu structure is shown in Figure 7-1. To start a SPIM session, the user 
dials the phone number of the VoiceXML gateway that is hosting the SPIM. The 
user then identifies him- or herself to the gateway and requests the SPIM appli-
cation (if the phone number is not dedicated to the SPIM). For the purposes of 
the SPIM, assume that the user has been identified-somehow-at the point 
that the main menu becomes active. 

VoiceXML Language Tutorial 
Logout~ 
Login 
Calendar Menu: 
Link: 
o Review Appointments 
+ 
o Add Appointment 
o Schedule Appointment 
r-
o Change Appointment 
Main Menu: 
o Calendar 
o Address Book 
Address Book 
o To-Do List 
Menu: 
o Link: Running Late 
I+ 
o Look Up Contact 
o Add Contact 
o Change Contact 
To-do List Menu: 
c.-
o Review List 
o Add Item 
o Change Item 
Figure 7-1. TheSPlMmenu hierarchy 
SPIM Main Menu 
Appointment 
Context: 
o Erase 
o Cancel 
o Change 
o Reschedule 
Contact Context: 
o Erase 
o Change 
o Call/E-mail 
Task Context: 
o Erase 
o Mark Done 
o Change 
The SPIM main menu has three choices (Calendar, Address Book, To-Do List) 
and one link (for the Running Late function described in Chapter 4). The follow-
ing code implements the basic structure of this main menu in a single docu-
ment (see Listing 7-2 from Stepl \SPIMMainMenu.vxrnl). When the VoiceXML 
interpreter executes the document, it starts executing the first form or menu in 
the document. 
A <menu> consists of a spoken prompt and a set of choices. When the 
<menu> is executed, the prompt is read and then the system listens for the user 
to say one of the choices. The choices are specified in the body of the <choice> 
tag. When a choice is recognized, the system starts executing the form or menu 
identified by the value of the next attribute, which is interpreted as a URI. 
Listing 7-2. Main Menu 
<menu id= "topLevelChoices" > 
<prompt>Your choices are: <enumerate /></prompt> 
<choice next=" #calendar" >Calendar< I choice> 
51 

Chapter 7 
52 
<choice next;"#toDo"> To-Do List</ choice> 
<choice next;"#addressBook">Address Book</choice> 
</menu> 
The prompt for a menu either can be coded explicitly or can use the 
<enumerate/> tag. The <enumerate/> tag simply returns the list of all available 
choices. In the example, the line 
<prompt>Your choices are: <enumerate I> </prompt> 
is equivalent to 
<prompt>Your choices are: Calendar, To-Do List, Address Book </prompt> 
In the example, the targets of the next attribute start with the pound sign(#). 
In URI -ese, # indicates an intradocument link. In the absence of an explicit doc-
ument address, the current document is assumed, so #calendar refers to the form 
whose ID is "calendar" in the current document. For this example, the forms are 
"stubbed out," so that they will simply say a message and exit. 
A menu or form field specifies a prompt/response exchange between the 
caller and the computer. A link is a transition that the caller can activate at any 
time the computer is listening for a response. Whereas a menu choice generally 
corresponds to only one spoken phrase, a link specifies a grammar. The grammar 
may be as simple as a single word or phrase, or it may be more complex. The 
example link in Listing 7-3 will be activated when either of two phrases is recog-
nized: "late" or "I'm late." Like a menu choice, the voice interpreter goes to the 
target of the next attribute when the link is activated. The most obvious differ-
ences between menu choices and links are that links do not specify prompts and 
there is no <enumerate/> tag for links. 
Application with Multiple Dialogs 
In this step, you'll add another level to the SPIM dialog hierarchy. The first task is 
to convert the stubbed-out forms into menus (which in turn point to stubbed-out 
forms). The result is in Step2. However, the document is starting to get lengthy and 
confusing to read, so you're going to break it out into multiple documents. 
The relationship between VoiceXML dialogs and documents is analogous to 
the relationship between functions (subroutines) and files in a programming lan-
guage such as C or C++. The language does not specify any fixed relationship, but 
there are subtle considerations involving information hiding, readability, main-
tainability, and so on. In any nontrivial VoiceXML application, developers will 
need to establish coding standards that cover this issue (and other issues). 

VoiceXML Language Tutorial 
The result of breaking the dialogs out is in Step3. In Listing 7-3, notice that 
the URis associated with menu choices in the main menu have been adjusted to 
point to the new documents. 
Listing 7-3. Dialogs Broken Out into Multiple Documents 
<?xml version="l.O" encoding="UTF-8"?> 
<vxml version="l. 0" application="SPIMApplication. vxml "> 
<menu id= "toplevelChoices" > 
<prompt>Your choices are: <enumerate I> 
</prompt> 
<choice next=" /Step3/Calendar. vxml#calendarActions" >Calendar< I choice> 
<choice next=" /Step3/ToDo. vxml#toDoActions" >To-Do List</choice> 
<choice next=" /Step3/ AddressBook. vxml#addressBookActions" >Address 
Book</choice> 
</menu> 
<link next="#Late"> 
<grammar type=" application/x- jgsf"> late I I'm late</ grammar> 
<!link> 
<form id="Late"> 
<block> 
<prompt>You are late.</prompt> 
</block> 
</form> 
</vxml> 
In the spirit of reorganizing everything at once, the dialogs have also been 
grouped into a single VoiceXML application. Documents associate with appli-
cations by specifying the application attribute on the <vxml> tag. Its value is 
a URI that points to the root document of the application. Whenever a document 
is loaded by the VoiceXML interpreter, the interpreter ensures that the appli-
cation root document is loaded. Therefore, any grammars, dialogs, links, 
variables, and so on active in the root document are active whenever any other 
document in the application is loaded. The application root document in 
Listing 7-4 establishes a scope that is shared by all documents in the application. 
Listing 7-4. Basic Application Root Document 
<?xml version="l.O"?> 
<vxml version=" 1. o" application=" /SmallExamples/SPIMApplicationBasic. vxml" > 
<!--Application root document for the SPIM application 
53 

Chapter 7 
54 
contains common links that are always active--> 
<meta name="Content-Type" content="text/x-vxml" /> 
<var name="userName" expr=" 'Ken'" I> 
<form> 
<block> 
<prompt>Hi, <value expr="userName" />!</prompt> 
<goto next=" /Step3/SPIMMainMenu. vxml#mainActions" I> 
</block> 
</form> 
</vxml> 
The application root document shown in Listing 7-4 (from 
SmallExamples\SPIMApplicationBasic.vxml) contains a variable called username. 
This variable can be referenced by any document in the application as 
application. username. For this basic example, the variable is simply initialized 
to the string "Ken." The application root contains a single form that greets the 
user ("Hi, Ken!") and dispatches immediately to the main menu. It's actually 
not necessary for the root document to contain any executable forms; it 
can also function as a passive repository for shared variables and links used in 
other documents. 
Visiting Documents 
The Web is based on HTTP, and HTTP is based on the paradigm of hypertext link-
ing. As a result, it is not surprising that VoiceXML provides several tags for 
transitioning between dialogs. 
<goto> and <submit> 
The application root document oversees authentication of the user and then 
invokes the main menu using a <goto> tag: 
<goto next="SPIMMainMenu. vxml#mainActions" I> 
The next attribute specifies the URI of a dialog to execute. URis are resolved 
relative to the current document. (See Table 7-1 for examples.) 

VoiceXML Language Tutorial 
Table 7-1. Sample <goto> Tags 
TAG 
<goto next="#localForm" I> 
<goto next= "SomeDocument. vxml" I> 
MEANING 
Execute the dialog called 
"localForm" in the current 
document. 
Execute the first dialog in the 
document "SomeDocument.vxml" 
in the same directory as the current 
document. 
<goto next=" SomeDocument. vxml#dialog1" I> 
Execute dialog "dialog I" in the 
document "SomeDocument.vxml" 
in the same directory as the current 
document. 
<goto next=" .. I AnotherDocument. vxml I> 
<goto next=" http: I lwww. a press. comlvxmll 
testiTestDocument. vxml#sampleDialog" I> 
Execute the first dialog in the docu-
ment ''AnotherDocument.vxml" in 
the parent directory of the current 
document. 
Execute "sampleDialog" at the 
given Web address. 
A close relative of <goto> is <submit>. The <submit> next attribute speci-
fies a Web address to visit. The name list attribute specifies a list of names of 
variables whose values are to be passed by an HTTP GET or POST request. The fol-
lowing <submit> tag is used to send the user name and current time to the server: 
<submit namelist="application.user when" next="LateAppointment.jsp" I> 
The .jsp suffix on the URI indicates that the target of the <submit> is a (Java) 
server page. The server page processes the submitted arguments and dynami-
cally generates a VoiceXML document that is returned to the VoiceXML 
interpreter. During development of a VoiceXML application, it is best to start with 
static VoiceXML and deal with the complexities of dynamic generation once the 
basic framework of the application is firm. So, for the first run through, hand 
code all dialogs, even ones that will be generated in the working application. 
Evidence of this technique can be seen in the following pair of lines: 
<!--<submit namelist=" application. user when" next=" LateAppointment. jsp" 1>--> 
<goto next=" LateAppointmentOOl. vxml" I> 
55 

Chapter 7 
56 
During early development, the <goto> tag transfers control to the hand-
coded version of the target page. After everything works together, the < goto> 
statement will be commented out and the <submit> tag will be used instead. 
Subdialogs 
In your sample SPIM application, authentication will be performed by a utility 
program that is part of the security infrastructure rather than part of the SPIM 
application proper. To model this for the future, you'll invoke an authentication 
routine from the application root and plan on substituting the real implemen-
tation later. 
The VoiceXML < subdialog> functions like the "call" statement in some pro-
gramming languages. When another dialog is invoked using < subdialog>, the 
dialog executes and then returns control to the caller. Optionally, the caller can 
specify input and/ or output parameters to the subdialog. However, unlike pro-
gramming languages, which typically have special syntax to distinguish 
parameters from local variables in a subroutine or method, VoiceXML does not 
provide any special syntax for parameters. 
NOTE 
There is a subtle distinction between dialogs that are invoked 
through <subdialog> and those invoked through <goto>. If invoked as 
a subdialog, a form returns control to the caller by executing a <return> 
tag. However, it is a semantic error to execute a <return> tag when exe-
cuting as the result of a <goto>. So it's not clear how to code forms that 
can be invoked either way. 
In Listing 7-5 (from Step4\SPIMApplication.vxml), a < subdialog> tag is used 
to invoke the login form in the Authenticator document. The name attribute on 
the < subdialog> tag implicitly declares a local variable that will hold the output 
parameters, if any, of the subdialog. Input parameters are passed through 
< param> tag(s), which immediately follow the <subdialog> tag. The names 
specified in the < param> tags should match the names oflocal variables declared 
in the form being called, but this is not enforced by the VoiceXML interpreter. The 
< filled> tag is executed when the subdialog has placed output parameters in 
the output variable. 

VoiceXML Language Tutorial 
Listing 7-5. A Subdialog 
<subdialog name="login" src=" /Step4/ Authenticator. vxml#login" > 
<!-- Input parameters are passed to subdialog local variables--> 
<param name="application" expr='" SPIM"' /> 
<param name=" enableAnonymous" expr= "false" type=" boolean" I> 
<filled> 
<! -- When the subdialog has set values for the "login" object 
assign the id to the application variable "userName" --> 
<assign name="userName" expr= "login. userid" I> 
</filled> 
</subdialog> 
In Listing 7-6 (from Step4\Authenticator.vxrnl), there are three variables 
declared local to the form: Two correspond to input parameters and one corre-
sponds to the output parameter. When the login form is invoked as a subdialog, 
the local variables are initialized with values specified in the caller's <param> 
tags. When the <return> tag is executed, values are copied from local variables 
listed in the name list attribute into the output variable in the caller. 
Listing 7-6. Login Form 
<form id="login"> 
<var name="application" /> 
<! -- Input param --> 
<var name="enableAnonymous" I> 
<!-- Input param -- > 
<var name="userid" /> 
<!--Output param --> 
<block> 
<! -- For the time being, always set user ID to "Ken" --> 
<assign name="userid" expr=" 'Ken'" I> 
<return namelist="userid" I> 
</block> 
</form> 
Form Handling 
Forms are the workhorses ofVoiceXML for gathering information from the user. 
In StepS, you take a first crack at the form used to add a new appointment to 
the address book. This an ambitious task, because there are several data items 
to gather: 
57 

Chapter 7 
58 
• Whom the appointment is with (should be a known contact) 
• When the appointment is scheduled 
• The duration of the appointment 
• The medium of the appointment (a telephone call, a face-to-face meeting, 
and so on) 
• The location of the appointment (home, work, and so on) 
Directed Form 
The simplest approach is to go through the list of data items you need and ask 
the user for them one by one. This corresponds to the simplest kind of input 
form: a directed form. In a directed form, the computer prompts the user explic-
itly for each item and does not proceed to the next item until the current item has 
been successfully elicited. A directed form for gathering information for a new 
appointment is shown in Listing 7-7 (from StepS \AddAppointment. vxml). 
Listing 7-7. Directed Form 
<?xml version-"1.0" encoding-"UTF-8"?> 
<! DOCTYPE vxml SYSTEM "http: I /www. voicexml. org/voicexmll-0. dtd" > 
<vxml version=" 1. o" application=" /SPIMApplication. vxml" > 
<meta name="Content-Type" content="text/x-vxml" I> 
<meta name=" Source" content=" StepS/ AddAppointment. vxml" I> 
<!-- SPIM Example: Dialog to add an appointment 
to the calendar 
<form id="addAppointment"> 
<block> 
--> 
<prompt> You are adding a new appointment. </prompt> 
</block> 
<field name="appointee"> 
<prompt>Who is the appointment with?</prompt> 
<option value="ken abbott"> Ken </option> 
<option value="dennis mccarthy"> Dennis </option> 
<option value=" susan abbott"> Susan </option> 
</field> 

<field name="year"> 
<prompt>What year?</prompt> 
<grammar type="application/x-jgsf"> 
2001 
1 2002 
1 2003 
1 2004 
1 2oos 
1 2oo6 
1 2007 
</grammar> 
</field> 
<field name="month"> 
<prompt>What month?</prompt> 
<grammar type=" application/x- jgsf" > 
January 
I February 
I March 
I April 
I May 
I June 
I 
July 
I August 
I September 
I October 
I November 
I December</grammar> 
</field> 
<field name="date" type="date"> 
<prompt>What date ?</prompt> 
</field> 
<field name="time" type="time"> 
<prompt>What time?</prompt> 
</field> 
<field name="where"> 
VoiceXML Language Tutorial 
<grammar type="application/x-jgsf">home 
work I office I other</grammar> 
<prompt>Will it be at home, office, work, or other?</prompt> 
</field> 
<field name="medium"> 
<grammar type=" application/x-jgsf" > 
call 
I meet 
I teleconference 
I other 
</grammar> 
<prompt>Call, meet, teleconference, or other?</prompt> 
</field> 
<block> 
<prompt>You have a <value expr="medium" /> 
with <value expr="appointee" I> 
on <value expr="month" I> 
<value expr="date" /> 
<value expr="year" I> 
at <value expr="hour" /> 
at <value expr="where" />. 
</prompt> 
59 

Chapter 7 
60 
<!--At this point, the data for the appointment 
should be submitted to the server --> 
<clear /> 
</block> 
</form> 
</vxml> 
The form consists of a set of fields, one field for each data item. The <field> 
tag implicitly declares a variable that will contain the value of the field. Each field 
has its own prompt and its own grammar describing valid responses to the 
prompt. When the form is executed, the VoiceXML interpreter starts at the top of 
the form and works its way through the fields one at a time. The interpreter 
speaks the prompt and waits for a spoken response. If the user doesn't respond 
within a default time-out period, or the response could not be matched to the 
grammar, the interpreter informs the user of the error and prompts again for 
the same information.1 When the interpreter recognizes a valid response, it 
assigns the recognized speech to the field variable and moves on to the next field. 
The "appointee" field uses <option> tags to identify a set of alternative 
responses to the prompt. The body of the tag contains a grammar specifying one 
or more equivalent responses. When any one of the responses is recognized, the 
value from the value attribute is assigned to the field variable. If the dtmf attrib-
ute is specified, and then the keypad value is also an alternative. For example: 
<field name=" personName" > 
<option value="' ken abbott"' dtmf="l "> [Ken I Kenneth] (Abbott] </option> 
The previous code will cause the value 'ken abbott' to be assigned to the 
variable personName if any of the following are recognized: "Ken," "Kenneth," "Ken 
Abbott," "Kenneth Abbott," '~bbott," or keypress "l." The <enumerate> tag will 
work in conjunction with <option> tags, but the results may not be what you 
expect if the body of the option contains a grammar rather than a single phrase. 
Grammars are discussed in detail in Chapter 9. 
The "year" and "month" fields specify alternates using more concise field-
level grammars. When one of the alternatives is recognized, the recognized value 
(as text) is assigned to the field variable. 
The "date" and "time" fields make use of built-in grammars. There are seven: 
boolean, date, time, digits, currency, number, and phone (see Table A-6). The idea 
1 It's up to the interpreter to set the default policy for how many times it will reprompt. The 
application is also free to specify event handling and not rely on the default policy. 

VaiceXML Language Tutorial 
is that these basic building blocks are implemented by the VoiceXML interpreter 
in a locale-sensitive fashion. 2 
While directed forms are simple and effective, long ones don't stand up well 
to repeated use. A person often doesn't mind going through a list once or twice 
when it's new. Once the person understands what's expected, he or she may find 
it very frustrating to waste time going item by item. In addition, people find the 
strict "turn-taking" (computer speaks, the person responds; computer speaks, 
the person responds; and so on) conversationally awkward. In general, directed 
forms are best used for brief, structured interactions such as taking a credit card 
number or eliciting a person's address. 
A partial solution to the frustration factor is to enable bargein. This means 
that the computer listens for a response at the same time it speaks the prompt. If 
the person knows what to say, they can "barge in" by speaking over the prompt 
and interrupting the strict turn-taking. Bargein is enabled by setting the barge in 
attribute ofthe <prompt> tag to true. As a rule, bargein should be enabled when-
ever possible. However, it is not foolproof. The computer may have trouble 
recognizing the start of a response, or it may have trouble hearing the response. 
The latter situation often occurs when talking over a telephone. The echo sup-
pressors in the phone network do not work perfectly and they can make a phone 
act like a nonduplex device, where only one person can talk and be heard at the 
same time. 
Mixed-Initiative Forms 
To achieve a more conversational style of interaction than computer -directed 
forms, VoiceXML supports mixed-initiative forms. They are called "mixed-
initiative" because both the computer and the person can direct what fields 
are filled and in what order. Another significant feature is that multiple fields 
can be filled by a single utterance. 
In Listing 7-8, the directed dialog has been converted to a mixed-initiative 
dialog. This involves the followingVoiceXML constructs, which I'll cover in turn: 
2 The interpreter should provide input grammars that recognize these basic elements and 
format the responses into a canonical format, and it should provide speech synthesis that 
reads the canonical format in the appropriate local idiom. In theory, you could write stan-
dard VoiceXML that manipulates dates and currencies without regard for locale. In practice, 
VoiceXML l.O is murky and underspecified in describing how these types behave, so use of 
these constructs is at least VoiceXML interpreter dependent, if not locale dependent as well. 
61 

Chapter 7 
62 
• Form-level grammar 
• <initial> tag 
• <help> tag 
Listing 7-8. Mixed-Initiative Dialog 
<?xml version="l.O" encoding="UTF-8"?> 
<! DOCTYPE vxml SYSTEM "http: I /www. voicexml. org/voicexmll-0. dtd" > 
<vxml version="l. 0" application=" /SPIMApplication. vxml "> 
<meta name=" Source" content=" Step6/ AddAppointment. vxml" I> 
<!-- SPIM Example: Dialog to add an appointment 
to the calendar 
--> 
<form id=" addAppointment" > 
<grammar type=" application/x-jgsf" > 
<! [CDATA[ 
[call 
I meet 
I teleconference 
I videoconference]{this.medium=$} 
[Dennis 
Susan I Ken] {this.appointee=$} 
[on 
[at 
]]> 
</grammar> 
(Sunday 
I Monday 
I Tuesday 
I Wednesday 
Thursday I Friday I Saturday) 
{this.day=$}] 
((one 
I two 
I three 
I four 
I five 
I six 
seven 
I eight 
I nine 
I ten 
I eleven 
I twelve) 
[am I pm]) {this.hour=$} ] 
<initial name="appointment"> 
<prompt>Add appointment. You may say help at any time. </prompt> 
<help> 
<prompt>Say something like call Dennis on Thursday at three 
pm. </prompt> 
</help> 
</initial> 
<field name=" appointee"> 
<prompt>With whom?</prompt> 
<grammar type="application/x-jgsf">Dennis I Susan I Ken</grammar> 

VoiceXML Language Tutorial 
<help> 
<prompt> 
Say the name of the person you are making an appointment with. 
</prompt> 
</help> 
</field> 
<field name="day"> 
<prompt>What day of the week?</prompt> 
<grammar type=" application/x- jgsf" > 
Sunday 
I Monday 
I Tuesday 
I Wednesday 
I Thursday 
I Friday 
I Saturday 
</grammar> 
<help> 
<prompt>Days are Sunday through Saturday. </prompt> 
</help> 
</field> 
<field name="hour"> 
<grammar type=" application/x-jgsf" > 
(one 
I two 
I three 
I four 
I five 
I six 
seven 
I eight 
I nine 
I ten 
I eleven 
I twelve) 
[am I pm] 
</grammar> 
<help> 
<prompt>Say something like three pm. </prompt> 
</help> 
<!field> 
<field name= "where"> 
<grammar type="application/x-jgsf">home I office I other</grammar> 
<prompt>Where?</prompt> 
<help> Say work, home, office, or other. </help> 
<!field> 
<field name="medium"> 
<grammar type=" application/x-jgsf" > 
call 
I meet 
I teleconference 
I other 
</grammar> 
<prompt>Call, meet, teleconference, or other?</prompt> 
<help> 
<prompt> 
Say one of call, meet, teleconference, or videoconference. 
</prompt> 
63 

Chapter 7 
64 
</help> 
</field> 
<block> 
<prompt>You have a <value expr="medium" /> 
with <value expr="appointee" /> 
on <value expr="month" I> 
<value expr="date" /> 
<value expr="year" /> 
at <value expr="hour" I> 
at <value expr="where" I>. 
</prompt> 
<!-- At this point, the data for the appointment 
should be submitted to the server --> 
<clear I> 
</block> 
<!form> 
</vxml> 
Form-Level Grammar 
The form-level grammar is the key to mixed-initiative dialogs. It describes a set of 
valid responses and specifies how elements of the response are assigned to field 
variables. In the example, the form-level grammar is enclosed in a<! [ CDATA[ ] ] > 
tag. This impressive-looking tag is a generic XML tag that turns offXML parsing 
for its contents. It is useful for specifying inline grammars, because grammar syn-
tax includes special characters. If the special characters were parsed, they would 
have to be written using XML escape sequences (for example, "<"must be written 
as "&It;";"&" becomes "&amp;"; and so on). Using the CDATA tag avoids con-
fusion and keeps the grammar readable. 
Consider the following grammar: 
[call 
I meet 
I teleconference 
I videoconference]{this.medium=$} 
(Dennis I Susan I Ken) {this.appointee=$} 
[on (Sunday 
I Monday 
I Tuesday 
I Wednesday 
I Thursday 
I Friday 
I Saturday) {this.day=$}] 
[at ((one 
I two 
I three 
I four 
I five 
I six 
I 
seven 
I eight 
I nine 
I ten 
I eleven 
I twelve) 
[am 
I pm]) 
{this.hour=$} ] 

VoiceXML Language Tutorial 
The previous grammar will match any of the following utterances (as well 
as others): 
"Call Dennis." 
"Meet Susan at 1." 
'Teleconference Ken on Thursday." 
"Meet Dennis on Sunday at 9 p.m." 
The curly-braced entities are tags used byVoiceXML. Tags appear immedi-
ately after phrases in the grammar. When the phrase is recognized, the body of 
the tag is substituted for the recognized text and returned to the VoiceXML inter-
preter, which treats it as executable script. For example: 
1. 
The person says "Call Dennis." 
2. 
The recognizer recognizes the phrase "call." 
3. 
In the first tag body, $ is replaced by the recognized text ("call"). 
4. 
The recognizer recognizes the phrase "Dennis." 
5. 
In the second tag body, $ is replaced by the recognized text ("Dennis"). 
6. 
The recognizer returns the tags to the VoiceXML interpreter. 
7. 
The VoiceXML interpreter interprets the two tags: {this. medium=" call"} 
and {this .appointee="Dennis"}, resulting in form fields "medium" and 
"appointee" both being filled in from the utterance. 
The <initial> Tag 
The <initial> tag acts like the first field in a mixed-initiative dialog. Like 
a <field>, it has a prompt, but its grammar is the form-level grammar. After the 
initial prompt is spoken, the computer attempts to match the response against 
the form -level grammar. If one or more field variables are set as a result of recog-
nizing the response, the <initial> tag is considered to have been "filled," and 
processing continues on to the next field. However, if the response is erroneous 
or not recognized, the user will be prompted again. 
65 

Chapter 7 
66 
The <initial> prompt is a general solicitation for the user to provide as 
much information as he or she is able. If the user knows what to say and speaks 
clearly, the <initial> prompt may be the only prompt he or she responds to in 
the form. If the user provides some, but not all, of the information, the computer 
reverts to a directed form to fill in the missing pieces. 
As a result, Dialog 7-1 and Dialog 7-2 are both acceptable. 
Dialog 7-1. Mixed-Initiative Dialog (Single Utterance) 
C (computer): Add appointment. You may say help at any time. 
P (person): Call Susan on Friday at 10 a.m. at work. 
C: 
You have a call with Susan on Friday at 10 a.m. at work. 
Dialog 7-2. Mixed-Initiative Dialog (with Prompting) 
C (computer): Add appointment. You may say help at any time. 
P (person): Call Susan at 10 a.m. 
C: 
Whatday? 
P: 
Friday. 
C: 
Where? 
P: 
Work. 
C: 
You have a call with Susan on Friday at 10 a.m. at work. 
Handling Events 
So far, you've focused on the "talking" aspects ofVoiceXML. Just as a conversation 
between people involves more interaction than just the transcript of the words 
spoken, so does scripting a VoiceXML interaction involve more than just defining 
prompts and responses. A good VUI must anticipate and handle all the vagaries 
of working in the real world: interruptions, noise, misunderstandings, lack of 
attention, soft voices, foreign accents, poor phone lines, unexpected disconnects, 
nonstandard vocabularies, and so on. The more you study speech, the greater 
appreciation you develop for natural human languages as frameworks for com-
municating reliably using an inherently error-prone medium. However, as 
pointed out before, VoiceXML has no cognitive model and is not capable of natu-
rallanguage. 
What VoiceXML does have is a rich event model, which provides the basic 
hooks for dealing with many of the real-time issues in scripting a speech inter-
action. Looking back at Step6\AddAppointment.vxml, you see that the explicit 
scripting took into account two kinds of events: recognition events and help events. 
Recognition events are events triggered by the VoiceXML interpreter when 
a phrase in an active grammar is recognized with acceptable certainty. For the 

VoiceXML Language Tutorial 
most part, recognition events are handled invisibly by VoiceXML. For example, 
the language-defined semantics for the <initial>, <field>, and <choice> 
constructs rely implicitly on recognition events happening in the interpreter. 
Recognition events can also be handled explicitly by using the <filled> tag. 
For example, the fragment in Listing 7-9 simply plays back what the computer 
heard when it recognized the response to the <prompt>. 
Listing 7-9. "Filled" Event Handler 
<initial name="appointment"> 
<prompt>Add appointment. You may say help at any time. </prompt> 
<filled> 
<prompt> You said: 
<value expr= n appointment$. utterance n > 
</prompt> 
</filled> 
</initial> 
The help events coded into Step6\AddAppointment.vxml are triggered by the 
VoiceXML interpreter. Presumably, the interpreter triggers help events when it 
recognizes piteous phrases such as "help me" (the triggering phrases are not 
specified byVoiceXML 1.0). Help events are one element of a larger set of built-in 
events defined byVoiceXML (see Chapter 9 for more information). 
Let's look at sample transcripts that are supported by Step6\ 
AddAppointment.vxml. The first example is the best-case scenario, where the 
person knows what to say and responds correctly to the prompt (see Dialog 7-3). 
Dialog 7-3. Dialog with Knowledgable Person 
C (computer): Add appointment. You may say help at any time. 
P (person): Call Pete on Friday at 10 a.m. 
C: 
You have a call with Pete on Friday at 10 a.m. 
In Table 7-2, the person asks for help, the <help> prompt is read, and the 
user responds correctly. 
67 

Chapter 7 
68 
Table 7-2. Mixed-Initiative Dialog with Help 
SPEECH 
C (computer): Add appointment. 
You may say help at any time. 
P (person): Help. 
C: Say something like call 
Dennis on Thursday at three pm. 
P: Call Dennis. 
C: What day of the week? 
P: Wednesday. 
C: What time will you meet? 
P: 8 p.m. 
VOICEXML 
<initial name=" appointment"> 
<prompt> 
Add appointment. You may say help at any time. 
</prompt> 
<help> 
<prompt> 
Say something like call Dennis on Thursday at three pm. 
</prompt> 
</help> 
<grammar type=" application/x-jgsf" > 
<![CDATA[ 
[call 
I meet 
I teleconference 
I 
videoconference]{this.medium=$} 
[Dennis I Susan I Ken] {this.appointee=$} 
]]> 
</grammar> 
<field name="day"> 
<prompt>What day of the week?</prompt> 
<grammar type=" application/x-jgsf" > 
Sunday I Monday I Tuesday I Wednesday 
Thursday I Friday I Saturday 
</grammar> 
<field name="hour"> 
<prompt>What time will you meet? </prompt> 
<grammar type=" application/x-jgsf" > 
(one 
I two 
I three 
I four 
I five 
I six 
seven 
I eight 
I nine 
I ten 
I eleven 
twelve) [am I pm] 
</grammar> 

VoiceXML Language Tutorial 
Table 7-2. Mixed-Initiative Dialog with Help (continued) 
SPEECH 
C: Where will you meet? 
P: Work. 
C: You have a call with Dennis 
on Wednesday at eight pm at work. 
VOICEXML 
<field name="where"> 
<grammar type=" application/x-jgsf" > 
home I office I other 
</grammar> 
<prompt>Where will you meet?</prompt> 
<block> 
<prompt> 
You have a <value expr="medium" I> 
with <value expr="appointee" /> 
<value expr="day" I> 
at <value expr="hour" /> 
at <value expr="where" />. 
</prompt> 
<goto next="#newAppointment" I> 
</block> 
If the person doesn't speak, the prompt will generate a timeout event after 
a default amount of time. There is no explicit handler for a time-out, so the 
default handler for the timeout event simply repeats the prompt and waits some 
more (see Table 7-3). 
Table 7-3: Dialog with no input Events 
SPEECH 
C (computer): Add appointment. 
You may say help at any time. 
P (person): {silence} 
C: Add appointment. You may 
say help at any time. 
P: {silence} 
C: Add appointment. You may 
say help at any time. 
VOICEXML 
<initial name=" appointment"> 
<prompt>Add appointment. You may say 
help at any time. </prompt> 
{times out} 
{VXML default noinput handler} 
{times out} 
{VXML default noinput handler} 
69 

Chapter 7 
70 
In Table 7-4, the person responds inappropriately, so a nomatch event is gen-
erated. There is no explicit handler, so the default handler says "I didn't get that," 
repeats the prompt, and waits some more. 
Table 7-4. Dialog with nomatch Events 
SPEECH 
C (computer): Add appointment. 
You may say help at any time. 
P (person): {mumble} 
C: I didn't get that. Add appointment. 
You may say help at any time. 
VOICEXML 
<initial name="appointment"> 
<prompt>Add appointment. You may say 
help at any time. </prompt> 
{VXML nomatch event} 
{VXML default nomatch handler} 
P: 
{dtmf "1"} 
{VXML nomatch event} 
C: I didn't get that. Add appointment. 
{VXML default nomatch handler} 
You may say help at any time. 
Tapered Prompting 
The default handlers get the job done, but they do not structure a well-designed 
interaction. Mechanically repeating the same prompt in response to a recurring 
situation leads to stalemate and an angry user. Applying the technique of tapered 
prompting can provide a better experience. In tapered prompting, the prompts 
are varied based on prior experience. For example, help prompts may "taper up," 
starting very concise and getting more verbose as the user requests more help, as 
in Dialog 7-4. 
Dialog 7-4. Tapered Help 
C (computer): Add appointment. You may say help at any time. 
P (person): Help. 
C: Say something like call Dennis on Thursday at three pm. 
P: Help. 
C: 
You are adding an appointment to your calendar. Say something like call Den-
nis on Thursday at three pm. 
P: Help. 
C: 
You are adding an appointment to your calendar. An appointment involves 
another person, a scheduled time, and a location. You will be asked for these 
individually. 
C: 
Who are you meeting with? 

VoiceXML Language Tutorial 
On the other hand, interrogational prompts may get terser as the person uses 
them and learns what's expected, as in Dialog 7-5. 
Dialog 7-5. Tapered Interrogation 
C (computer): Add appointment. You may say help at any time. 
P (person): Call Dennis. 
C: What day will you meet? 
P: Wednesday. 
C: What time will you meet? 
P: 
Bp.m. 
C: Where will you meet? 
P: 
Work. 
C: 
You have a call with Dennis on Wednesday at eight pm at work. 
C: Add appointment. 
P: Call Dennis on Saturday. 
C: 
Time? 
P: 3p.m. 
C: Where? 
P: Home. 
C: 
You have a call with Dennis on Saturday at three pm at home. 
In Step7\AddAppointment.vxml, improved error handling has been 
wrapped around the form fields to handle more errors and provide some taper-
ing. nomatch, noinput, and help handlers have been added. The handlers use the 
count=" n" attribute to modify which handler is activated after at least n invo-
cations. The basic strategy is to provide two layers of help prompting, a terse level 
and a verbose level, and to permit the help handler to switch the form into 
directed mode. The nomatch and noinput handlers invoke the help handler if the 
events recur. The directed form fields provide a terse and verbose interrogational 
prompt for each field. 
See Table 7-5 for an elaboration of how the tapered help prompting is 
implemented using event counters. Notice how the dialog switches from mixed-
initiative mode to directed mode by setting the value of "directedDisabled" in 
the <help count=3> handler. Because the entry condition (cond attribute) on the 
<initial> tag fails, the interpreter does not visit the <initial> tag again. The 
interpreter looks for the next form item whose variable has not been set and con-
tinues prompting in directed mode. 
71 

Chapter 7 
72 
Table 7-5. Tapered Help Prompting 
SPEECH 
C (computer): Add appointment. 
You may say help at any time. 
P (person): Help. 
C: Say something like call Dennis 
on Thursday at three pm. 
P: Help. 
C: You are adding an appointment 
to your calendar. Say something like 
call Dennis on Thursday at three pm. 
P: Help. 
C: You are adding an appointment 
to your calendar. An appointment 
involves another person, a scheduled 
time, and a location. You will be 
asked for these individually. 
C: Who are you meeting with? 
VOICEXML 
<grammar 
src="/NewAppointment.gram#appointment" 
type="application/x-jgsf" I> 
<initial name="appointment" 
cond="document.disableDirected"> 
<prompt> 
Add appointment. You may say help at any time. 
</prompt> 
</initial> 
<help count="l"> 
<prompt> 
Say something like call Dennis on Thursday at three pm. 
</prompt> 
</help> 
<help count="2"> 
<prompt> 
You are adding an appointment to your calendar. 
Say something like call Dennis on Thursday at three pm. 
</prompt> 
</help 
<help count="3"> 
<prompt> 
You are adding an appointment to your calendar. 
An appointment involves another person, 
a scheduledtime,and a location. 
Please answer somequestions about the appointment. 
</prompt> 
<!-- Switch over to directed mode. Stay in directed 
mode until this form is reentered. --> 
<assign name="document.disableDirected" 
expr="false" I> 
<reprompt I> 
</help> 
<field name="appointee" > 
<prompt count="l"> 
Who are you meeting with? 
</prompt> 
</field> 

VoiceXML Language Tutorial 
Table 7-5. Tapered Help Prompting (continued) 
SPEECH 
P: Dennis. 
C: VVhat day? 
P: Tomorrow at 3 p.m. 
Prompt Counting 
VOICEXML 
<subdialog 
src="/Step7/GetDayAndHour.vxml#getDayAndHour" 
name="when"> 
<param name="i_day" expr="dialog.day"/> 
<param name=" i _day _prompt" expr=" 'What day? ' " I> 
<param name="i_hour" expr="dialog.hour" /> 
<param name=" i _hour _prompt" expr=" 'What time?"' I> 
<filled> 
<assign name="day" expr="when.day" /> 
<assign name="hour" expr="when. hour" /> 
</filled> 
</subdialog> 
{field by field processing continues} 
The prompt-counting mechanism may not work exactly as you expect. In the 
previous example, the person can specify as many appointments as he or she 
wants. It would be nice if the first time through, the computer read the long 
prompt and then used the terse prompt for subsequent appointments. However, 
this is not how the prompt counter works. Every time a field is cleared, its 
prompt counter is reset to zero. All fields are implicitly cleared when the form is 
reentered (by a <goto>), or when a <clear I> is executed (as in the example). 
73 

Chapter 7 
74 
Therefore, the only time the terse prompt <prompt count=2> is heard is when 
a </reprompt> is issued from within the body of the <field>. The desired 
behavior-only saying the verbose prompt once-can be achieved by keeping 
your own count with an application -scoped variable, as shown in Listing 7-10 
(from Step7\ManualCounter.vxml). The count can be used as a guard condition 
on <prompt> by explicitly testing it in the cond attribute. 
Listing 7-10. Manual Prompt Counter 
<!-- This variable should be in document or application scope--> 
<var name="myFieldCounter" expr="1" I> 
<form> 
<var name="mlname" expr=" 'VXML'" I> 
<field name="theField"> 
<grammar type="application/x-jgsf">something</grammar> 
<prompt cond="document.myFieldCounter == 1"> 
This is a verbose prompt! 
</prompt> 
<prompt cond="document. myFieldCounter ! = 1" >terse! </prompt> 
<filled> 
<script>document .myFieldCounter +=1; </script> 
<!filled> 
<!field> 
Queries and Sets 
One of the biggest challenges in designing a VUI is browsing through collections 
of data. The nature of speech forces a linear style of browsing, which can be very 
time-consuming and tedious. In the SPIM application, this problem occurs when 
reviewing appointments for a given time period. The number of appointments 
that a person has can vary widely from day to day, so some days a person may 
have more than a dozen and other days that person may only have two or three. 
To implement the Review Appointments function, you need to establish the 
time period of interest. The sample form Step8\ReviewAppointments.vxml shows 
some dialogs that perform this function. Notice that in this case, the imprecision 
of language can actually be very useful-for example, people find it comfortable 
to say something like "tomorrow afternoon," but they may find it tedious to have to 
completely spell out "March 23, 2001, 12:00 a.m. to 6:00p.m." Furthermore, peo-
ple can have different opinions about what "afternoon'' includes. You'll want to 
take advantage of these compact linguistic patterns that people use to communi-
cate effectively. 
The strategy in the VoiceXML form is simply to capture a valid specification of a 
time period from the user. The recognized text will be shipped to a server program 

VoiceXML Language Tutorial 
to interpret precisely what was intended. (This is consistent with the principle that 
VoiceXML is a presentation language, not a processing language.) The server pro-
gram will interpret what the user said, restate the time period in precise terms (that 
is, in a query language), retrieve all the appointments in that time period, and return 
the collection (as generated VoiceXML) to the user (see Figure 7 -2). 
VoiceXML 
Gateway 
<vxml> 
</vxml> 
Figure 7-2. Reviewing appointments 
Recognizing a Time Period 
Web/App 
Server 
Query 
Start: 20010323 
12:00 
End: 20010323 
18:00 
Appt 1: 20010323 
13:15 
Appt 2: 20010323 
14:15 
Appt 3: 20010323 
16:00 
To specify the period of interest for reviewing appointments, a person should be 
able to specify various permutations of the following time periods (and others): 
Today 
Tomorrow afternoon 
Next Thursday 
March twenty third two thousand one after two fifteen pm 
Saturday from three through six 
july eighteenth two thousand one from seven fifteen am through july nineteenth 
eight thirty pm 
To recognize a person's specification of a time period of interest, the 
JGSF grammar defined in SearchAppointment.gram (Listing 7-11) is used. In 
Listing 7-11, notice that the more basic productions (for example, days of the 
75 

Chapter 7 
76 
week) appear first and are followed by more complex rules that use the basic 
ones. The most comprehensive production is <timeframe>. In the course of 
recognizing an utterance as a <timeframe>, the parser also has to recognize 
component <day>s, <hour>s, <timespan>s, and so on. 
Listing 7-11. Grammar for Specifying Time Periods 
public <dayofweek> = ( today I tomorrow I 
[ a week from 
I next ] (Sunday 
I Monday 
I Tuesday 
I Wednesday 
I Thursday 
Friday I Saturday )); 
public <hour> = (((one 
I two 
I three 
I four 
I five 
I six 
I seven 
I eight 
I nine 
I ten 
I eleven 
I twelve) 
[o'clock 
I fifteen 
I thirty 
I forty-five] [am 
I pm] ) 
I 
( noon I midnight )) {this.hour=$}; 
public <month> = (January 
I February 
I March 
I April 
I May 
I June 
I July 
August 
I September 
I October 
I November 
I December ); 
public <dayofmonth> = (thirtieth 
I thirty-first 
I twentieth 
I tenth ) 
I 
[twenty] ( first 
I second 
I third 
I fourth 
I fifth 
I sixth 
seventh 
I eighth 
I ninth ) 
I 
(eleventh 
I twelfth 
I thirteenth 
I fourteenth 
I fifteenth 
I 
sixteenth 
I seventeenth 
I eighteenth 
I nineteenth); 
public <year> 
[two thousand] (one 
I two 
I three 
I four 
I five 
I six 
I seven ); 
public <date> = <month> <dayofmonth> [<year>] 
<dayofmonth> [of] <month> [year]; 
public <day> = <dayofweek> 
I <date> ; 
public <times pan> = ( afternoon 
I morning 
I evening 
I all day ) ; 
public <relativetime> = after 
I before 
I around 
I about; 
public <timeframe> = <day> {this. startday=$} [ ( 
[[from] <hour> {this.starthour=$}] 
[through [<day> {this.endday=$}] [<hour> {this.endhour=$}]] 
<relativetime> {this.relativetime=$} <hour> {this.starthour=$} 
<timespan> {this.timespan=$} )] ; 
Notice that as various component phrases of a <timeframe> are recognized, 
the phrases are assigned to form field variables (by the expressions in curly 
braces, as discussed in the "Form-Level Grammar" section). Table 7-6 shows 
some examples of accepted utterances and the values assigned to field variables 
after the utterance is recognized. 

VoiceXML Language Tutorial 
Table 7-6. Parsing Time Frame Utterances 
TIME FRAME SPECIFICATION 
Tomorrow 
March twenty third two thousand 
one after two fifteen pm 
Thursday evening 
July eighteenth two thousand one 
from seven fifteen am through July 
nineteenth eight thirty pm 
VOICEXML FIELD VARIABLES 
this.startday="tomorrow" 
this.starthour=undefined 
this.relativetime=undefined 
this.endday=undefined 
this.endhour=undefined 
this.timespan=undefined 
this.startday="March twenty third two 
thousand one" 
this.starthour="two fifteen pm" 
this.relativetime="after" 
this.endday=undefined 
this.endhour=undefined 
this.timespan=undefined 
this.startday="Thursday" 
this.starthour=undefined 
this.relativetime=undefined 
this.endday=undefined 
this.endhour=undefined 
this.timespan="evening" 
this.startday="July eighteenth two 
thousand one" 
this.starthour="seven fifteen am" 
this.relativetime=undefined 
this.endday="July nineteenth" 
this.endhour="eight thirty pm" 
this.timespan=undefined 
CAUTION 
VoiceXML 1.0 does not specify grammar formats or the mecha-
nism for communicating values from the recognizer back to the VoiceXML 
interpreter. Therefore, any VoiceXML 1.0 program has platform dependen-
cies based on the grammar format and the passing of data. The previous 
example is fairly standard in that it uses ]SGF and the tagging mechanism 
is supported by at least some voice platforms, including the IBM platform 
included with this book. 
77 

Chapter 7 
78 
VXML 2: VoiceXML 2.0 will specify the grammar format and will specify 
the mechanism for passing data from the recognizer to a VoiceXML 
program. However, at the time of this writing, the specification of the 
data-passing mechanism was still under development. The basic mecha-
nism is expected to be similar to that used previously: The grammar will 
be tagged in such a way that recognized phrases are assigned to form 
field variables. 
Formatting a Query String 
The grammar discussed in the preceding section is used by the form 
Step8\ReviewAppointments.vxml#getTimePeriod. The form works somewhat like 
the mixed-initiative dialog you used to define a new appointment (see the 
"Directed Form" section). The person can respond to the <initial> prompt 
with a full or partial specification of a time period. If the response is sufficient to 
specify a time period, the response is sent to the server for processing. If the 
response does not fully specify a time period, form getHourAndDay is invoked to 
get the precise hour and day. Notice that you are not forcing the person to be 
completely precise. If the person says "tomorrow afternoon" and nothing more, 
this is taken to be a full, valid specification, even though (formally) one could 
argue that "tomorrow afternoon" could also specify a start time that requires an 
end time (for example, "through Friday morning"). 
The following code declares field variables, but does not prompt for them: 
< field name="startday" cond="false" ></field> 
< field name=" relati vet ime" cond= "false" > < I field > 
< field name="timespan" cond="false" ><lfield> 
< field name="starthour" cond="false"></field> 
< field name="endday" cond="false" ><lfield> 
< field name="endhour" cond="false" ></field> 
The fields are either filled in by the response to the <initial> prompt 
or the <subdialog> references. The <subdialog> tag contains the gross-
looking attribute 
cond="(startday == null) I I 
(endday==null &amp;&amp;relativetime == null &amp;&amp;timespan == null)" 
The expression in parentheses uses XML escape codes, but the conditional 
expression is equivalent to the somewhat more easily read 
startday==null I I ( endday==null && relativetime == null && timespan == null) 

VoiceXML Language Tutorial 
The form item (in this case, subdialog invocation) is executed if its cond attrib-
ute evaluates to true. So, the meaning of this expression is "If the start day of the 
time period was not specified, or the start day was specified but not followed by 
an ending day, a relative time (for example, 'after 3'), or a time span (for example, 
'evening'), then execute the subdialog to elicit a specific start day and time." 
The "checkpoint" form item reads back the information gathered thus far and 
asks the user if he or she wants to review the appointments or start over. If the user 
wants to review the appointments in the time period, the query string is formatted 
using ECMAScript, as shown in Listing 7-12. The formatting puts information in 
a canonical order and makes sure that undefined values are not inserted. 
Listing 7-12. ECMAScript Formatting 
<var name="queryString" I> 
<script> 
<! [CDATA[queryString = startday + 
( timespan == null ? "" : timespan ) + 
( starthour==null ? "" : 
( relativetime==null ? "" : relativetime) + starthour ) + 
( endday ==null ? "" : "through "+ endday + (endhour==null?"":endhour) ) + 
'. ';]]> 
</script> 
This script makes use of ECMAScript and the < ! CDAT A ] ] > tag for 
compactness. It makes heavy use ofthe C-style <boolean>: <true-case>? 
<false-case> operator to avoid inserting the string "undefined" into the result-
ing query string. ("undefined" is the string representation of a null or undefined 
ECMAScript variable.) See Table 7-6 for examples of utterances and resulting 
variable values. Once the query string is formatted, it is passed to the server by 
a <submit> tag. 
Browsing the Results 
Based on results of the query, server components will generate 
VoiceXML code to browse the qualifying appointments (see Figure 7-3). 
Step8\BrowseAppointments00l.vxml is an example of generated code. (Part III 
of this book gets into the details of how it is generated-for now just assume that 
it happened somehow.) Notice that specific information-variables, prompts, 
grammars, and so on-is coded directly into the generated VoiceXML. 
79 

Chapter 7 
80 
BrowseAppointments001.vxml 
Links: Browse (first, last, next, previous) 
Links: Goto appointment# (DTMF) 
Main Choice 
H Review Appt 1: f--
Menu: 
• Appointment 1 
~Review Appt 2: ~ 
Summary 
• Appointment 2 
Summary 
_ ~Review Appt 3: ~ 
• Appointment 3 
Summary 
Appointment 
Subdialog Review: 
Context: 
Read Details 
• Erase 
• Okay 
~=:: 
• Cancel 
• Repeat 
• Change 
• Modify 
• Reschedule 
Figure 7-3. Structure of generated browsing code 
The generated code contains two kinds of prompts for each appointment. 
There is a short prompt (for example, "Dennis at thirteen hundred.") that is used 
in the browse menu and a long prompt that gives the details (for example, 
"Weekly status meeting with Dennis and Fred at work on Thursday, May third 
from thirteen hundred hours to fourteen hundred hours."). 
To implement browsing, links are specified at document scope. There are voice 
links ("first," "last," "next," and "previous") for moving relative the current appoint-
ment, and DTMF links, which go directly to the nth appointment. The links are at 
document scope so that they can be used in any of the dialogs in the generated 
voice browser. The generated code enables bargein at most prompts, so that a per-
son can cut off the computer at any time. In Dialog 7-6, the person barges in three 
times. The first time, the person uses the keypad to go directly to the last appoint-
ment and escape from the reading of summaries. The second and third time, the 
person uses voice commands to cut off reading of unwanted details. 
Dialog 7-6. Dialog with Bargein 
C (computer): You have three appointments. One: Dennis at thirteen hundred. 
Two: Dentist at . .. 
P (person): (barging in) DTMF "3." 

VoiceXML Language Tutorial 
C: Appointment three. Dinner with Susan at Robertos Frito Misto on Thursday, 
May 3 from nineteen hundred hours to twenty one hundred hours. Your 
choices are okay, repeat, or modify appointment. 
P: Repeat. 
C: Dinner with Susan at Robertos . .. 
P: 
(barging in) Previous. 
C: Appointment two. Semiannual cleaning . .. 
P: 
(barging in) First. 
C: Appointment one. Weekly meeting with Dennis . .. 
Event Links 
The navigational commands are implemented as links that throw events. This 
allows some processing to be performed when the link is activated, as shown in 
Listing7-l3. 
Listing 7-13. An Event Link 
<link event="browse.previous"> 
<grammar type="application/x-jgsf"> previous </grammar> 
<!link> 
<catch event=" browse. previous"> 
<script> current = Math.max( current-1, 1); </script> 
<goto expr=" '#review' +current" I> 
</catch> 
When a person says "previous," the link is activated and the application-
defined event "browse. previous" is thrown in the dialog that was active when the 
user spoke. If there are no other "browse. previous" event handlers in nested forms 
or subdialogs, the document -scope "browse. previous" event handler will be 
invoked in the active dialog.3 The event handler uses ECMAScript to do some sim-
ple math on a cursor variable that always identifies the current appointment, and 
then it goes to the generated review form for the new current appointment. 
The behavior of event links is somewhat counterintuitive. At first glance, you 
would assume that a document -scoped link would throw a document -scoped 
event. However, that isn't really the case, because the whole point of links is to 
make a common set of actions available in all nested dialogs. Throwing the event 
at document scope would always terminate any nested dialogs, rather than give 
them a shot at handling the event. To see how this works, consider the example in 
Listing 7-14. 
3 See Chapter 9 for a detailed discussion of how this works. 
81 

Chapter 7 
82 
Listing 7-14. Scope of Events Thrown by Links 
<vxml> 
<link event=" special. help"> 
<grammar type=" application/x- jgsf" >double help</ grammar> 
</link> 
<catch event=" special. help"> 
<prompt>Call 911! </prompt> 
<exit/> 
</catch> 
<form id=withSpecialHelp> 
<initial> 
<prompt>I am at your service. </prompt> 
</initial> 
<catch event="special.help" count="1"> 
Please tell me whatever is on your mind. 
<reprompt/> 
</catch> 
<catch event="special.help" count="2"> 
<throw event=" special. help"/> 
</catch> 
</form> 
<form id=withoutSpecialHelp> 
<initial> 
<prompt>What do you want?</prompt> 
</initial> 
<!form> 
</vxml> 
In form "withSpecialHelp," the event is handled within the nested dialog the 
first time and then explicitly propagated out to document scope the second time 
(see Dialog 7-7). 
Dialog 7-7. Nested Event Handler 
C (computer): I am at your service. 
P (person): Double help. 
C: Please tell me whatever is on your mind. I am at your service. 
P: 
Double help. 
C: Call911! (exits) 

VoiceXML Language Tutorial 
In form "withoutSpecialHelp," there is no nested event handler, so the event 
is handled by the document-level event handler (see Dialog 7-8). 
Dialog 7-8. Document-Level Event Handler 
C (computer): What do you want? 
P (person): Double help. 
C: 
Call911! (exits) 
"Punching Up" Prompts 
Synthesized speech can be hard to understand because it comes out as an unem-
phasized stream, without the normal patterns of emphasis and pause that people 
use to highlight their speech. Consider the contrast between a person reading the 
two following sentences: 
"there are two reasons why small businesses fail one insufficient invest-
ment and two lack of forecasting." 
"There are two reasons why small businesses fail: 1.) insufficient invest-
ment, and 2.) lack of forecasting." 
In the second case, a person will add pauses around and emphasis on the 
numbers to signal that he or she is labeling the following phrase. 
To achieve a similar effect in the prompts, where numbers label options, the 
generated prompt in form "mainChoice" contains embedded <break> tags. The 
size attribute of <break> can be none, small, medium, or large, or the msecs 
attribute can specify the pause length in milliseconds. The generated prompts 
look like this: 
You have three appointments: 
<break size="large"/> 
<sayas class= "number" >1 </sayas> 
<break size=" small" />Dennis at thirteen hundred, 
The breaks serve to delineate the numbers as labels, so that people intu-
itively understand that they are menu options. The <sayas> tag is used to inform 
the VoiceXML interpreter that its content should be read as a number, which is one 
of the say-as types (see Appendix A for a list of say-as types). 
83 

Chapter 7 
84 
NOTE 
It is easy to confuse built-in grammars, built-in types, and say-as 
types, but they are different. Built-in grammars and built-in types are 
used by the speech recognizer to recognize common speech patterns such 
as phone numbers, strings of digits, sequence of keypresses, and so on. 
Say-as types are markup instructions to the voice synthesizer, identifying 
how the text is to be rendered into speech (as an acronym, for example). 
Although there are both built-in types and say-as types for currency, phone 
numbers, and so on, they are unrelated. 
VXML 2: VoiceXML 2.0 is expected to use speech markup tags defined in 
the W3C~ Speech Synthesis Markup Language Specification for the Speech 
lntelface Framework (http: I lwww. W3. orgiTRispeech-synthesis). The tags 
in this specification have slightly different names and syntax than their 
VoiceXML 1.0 equivalents. For example, <sayas> from version 1.0 
becomes <say-as> in Speech Synthesis Markup Language (SSML), 
<pros > becomes < prosody> in SSML, <emp> becomes <emphasis> 
in SSML, and so on. Version 1.0~ < break msecs=" 30" I> becomes 
<break time="30ms" I> in SSML. 
Telephony Features 
In the SPIM application, when the person says ''I'm late," a link is activated and 
Step9\RunningLate.vxrnl is executed. The dialog there fetches the current time, 
reads it to the person, and submits a query to the server. A server page uses the 
submitted current time (because client and server may be in different time 
zones) and the user ID to look up the current appointment. Based on the avail-
able contact information (for instance, is there a listed fax number?), the server 
page generates a document like Step9\LateAppointmentOOl.vxrnl. The generated 
dialog provides a menu of options for contacting the person being met: phone, 
fax, or e-mail (in the example). 
The telephone call to the person is initiated through a <transfer> tag, as 
shown in Listing 7-15. 
Listing 7-15. Transfer Tag 
<transfer name="callOutcome" dest="phone://5085551234" bridge="true" 
connecttimeout="20s"> 

VoiceXML Language Tutorial 
<filled> 
<if cond="callOutcome == 'busy' II callOutcome == 'network_busy'"> 
<prompt>Sorry, the line was busy. </prompt> 
<elseif cond="noanswer" /> 
<prompt>Sorry, there was no answer. </prompt> 
<else I> 
<prompt>Your call lasted 
<value expr="callOutcome$. duration" /> seconds. 
</prompt> 
</if> 
</filled> 
</transfer> 
The dest attribute specifies a URI that is interpreted by the VoiceXML inter-
preter. If the bridge attribute is true (bridging transfer), the VoiceXML interpreter 
suspends while the call is underway and resumes execution when the call com-
pletes. If the bridge attribute is false (blind transfer), the interpreter exits as soon 
as the call connects. For bridging transfers, the end of the call causes the "call-
Outcome" variable to be set and the <filled> handler to execute. Possible 
outcomes are busy, noanswer, network_busy, near_end_disconnect, 
far_ end_ disconnect, and network_ disconnect. 
The variable "call0utcome$.duration" is an example of a VoiceXML shadow 
variable. Shadow variables are implicitly declared and set by the VoiceXML inter-
preter after executing a form item. Syntactically, they are accessed as ECMAScript 
objects named name$, where name is the form item variable. Not all tags set 
shadow variables-see the VoiceXML specification for more information. 
Summary 
In this tutorial, you've taken a rapid tour of most of the essential features of 
VoiceXML. Starting with a "Hello, World!" example, you've explored the following 
features ofVoiceXML: 
• Menus 
• Dialogs, documents, and applications 
• Dialog navigation 
• Directed forms and mixed-initiative forms 
85 

Chapter 7 
86 
• Event handling 
• Generation ofVoiceXML to browse sets 
• Telephony 
With this knowledge in hand, you should be able to design and implement 
standalone VoiceXML programs. In the next two chapters, you'll look at some 
more advanced features ofVoiceXML. After that, you'll explore some of the 
broader architectural issues involved in integratingVoiceXML with distributed 
Web applications. 

CHAPTER 8 
VUI Design Principles 
and Techniques 
THE FOLLOWING SECTIONS provide introductory guidance about best design practices 
for VUis. Comprehensive coverage of the entire VUI design process is beyond the 
scope of this book. The intent here is to make you aware of some of the key issues 
that must be addressed in VUI design and provide an overview of some of the 
techniques that are available to address those issues. 
Core Principles 
Among speech interface designers, there's a credo: A good GUI and a good VUI 
are both a pleasure to use, a bad GUI is hard to use, but a bad VUI isn't used at all. 
This will be a major issue as VoiceXML puts voice technology in the hands of 
developers everywhere. You may remember in the early days of the Web, when 
HTML tools were just becoming available, that the number of truly awful Web 
site designs took off. Well, that's where VoiceXML will be very soon, and you can 
expect many unusable VUis to be written. 
Keep It Simple} Do It Well 
Be realistic about what can be accomplished through a VUI. Focus on handling 
the easiest 80 percent of requests simply and cost -effectively. 
A famous French proverb states, "Nothing succeeds like success." It seems 
obvious: A good interface is one that gets the job done. However, who has not 
experienced the frustration of calling a customer service phone number and 
immediately being thrust into a Kafka-esque maze of voice prompts and menu 
choices, none of which meet your need? 
There is tension between the needs of people calling in ("users") and busi-
nesses that provide the service ("providers"). Users want to have their needs 
quickly and courteously met. From the user's perspective, nothing beats having 
a well-trained and empowered person on the vendor's end of the line. From the 
provider's perspective, well-trained and empowered customer service represen-
tatives are expensive. Vendors want the call center to please customers in 
87 

ChapterS 
88 
a cost-effective manner. 1 As Table 8-1 shows, users and providers do not use the 
same criteria when measuringVUI usefulness and effectiveness. 
Table 8-1. Satisfaction Criteria for VUI 
USER CRITERIA 
Can I get the information or perform 
the transaction I want? 
Is the result worth my effort to get it? 
Do I feel like I'm receiving a valuable service? 
PROVIDER CRITERIA 
Does it reduce the load on customer 
service reps? 
Are users satisfied with the experience? 
Does it increase the number of users 
I connect with? 
By these criteria, a VUI that gracefully and elegantly ends up routing most 
calls to a human operator is not meeting provider needs. On the other hand, 
a VUI that never routes a call to a human operator is not meeting the needs of 
some users. Achieving a balance between the sometimes conflicting require-
ments of users and providers is part of the design process. Skewing the balance 
too heavily to the provider's side results in a VUI that is overly comprehensive, 
loaded with options, and frustrating to use. Skewing the balance too heavily to 
the user's side results in a VUI that is expensive to build and operate. 
When designing a VUI, be realistic about its capabilities. Don't assume that 
the VUI should be able to do everything a GUI can. Use the 80/20 rule: Aim to 
simply and effectively handle the easiest 80 percent of the load, and leave the 
other 20 percent to other means (usually human operators). 
Accommodate Errors 
Errors are not exceptions in speech and cannot be eliminated. A good VUI should 
be deceptively simple-simple in its basic dialog structure, but complex in its 
capability to perform in the presence of a multitude of errors. 
Communication by speech is inherently a complicated and error-prone pro-
cess. Computer programmers are biased to treat errors as defects-failings to be 
eradicated. However, when you develop VUis, you have to overcome this bias: 
Errors are essential and inherent. The goal is not to eliminate errors, but rather to 
contain them and tolerate them. Natural languages are marvelously error 
1 All lofty rhetoric aside, a reason businesses are interested in VUis is to reduce (or eliminate) 
the cost of maintaining call centers. VoiceXML will accelerate this interest because it 
opens the possibility of collapsing two infrastructures (call center and Web) into one. 

VUI Design Principles and Techniques 
tolerant. People can communicate effectively despite mispronunciations, jack-
hammers in the background, misunderstandings, verbal stumbles, inter-
jections, false starts, incomplete sentences, poor grammar, uncouth accents, and 
so on. Developing a good VUI has a lot in common with programming a real-time 
process control system like that which runs a power plant. There's a lot going on 
in real time: There are basics laws of physics at work, relative timing of events is 
crucial, and errors that occur need to be managed so that they damp down rather 
than amplify over time. 
As mentioned previously, VoiceXML has no cognitive model, and so it is not 
an appropriate instrument for implementing natural language. More precisely, 
VoiceXML cannot match the expressiveness of natural language, though it can 
borrow profitably from the error tolerance of natural language. The goal of a good 
VoiceXML VUI is to be deceptively simple-simple in that the basic structure of 
dialogs should be simple and easy, and deceptive in that the "simple" dialogs 
should be possible in the face of a multitude of errors. In terms of the effort 
invested in developing a VUI, the minority of effort should be spent on the basic 
dialogs. The majority of effort should be spent on detecting errors, recovering 
from them, and getting the conversation back on track. 
Design for Everyone) Everywhere 
People don't all speak alike. Bear in mind that users will possess a wide variety 
of voices, speech skills, and vocabularies. The Web is a public place: A voice-
enabled Web site can expect to hear from people with all kinds of backgrounds, 
speech skills, vocabularies, and voices. In designing a VUI, keep the response 
vocabulary simple and generic. Avoid disenfranchising nonexpert users by using 
specialists' jargon. Make sure that the VUI can adapt to situations where speech 
recognition isn't working well. 
Speech recognition technology currently achieves the highest recognition rates 
for adult native speakers of English, and it achieves lower rates for children and 
non-native speakers. In addition to natural variations in people's speech, speech 
recognition professionals also refer to "sheep" and "goats"2: "Sheep" are people 
for whom speech recognition works exceptionally well, and "goats" are people for 
whom it works exceptionally poorly. Only the voice recognizer knows what sepa-
rates them. People can't predict whose voice will be recognized easily and whose 
won't. The best policy is to design the interface so it can handle all kinds of voices 
in all kinds of environments. 
2 From How to Build a Speech Recognition Application, by Bruce Balentine and David P. 
Morgan, Glossary. 
89 

ChapterB 
90 
Speech Design 
The goal of designing an effective VUI in VoiceXML is to create a small speech 
dialect that is constrained in vocabulary and phrase structure, effective for the task 
at hand, and tolerates errors. In other words, you're not trying to make the com-
puter talk at the level of a person. You're trying to get a person to speak at the level 
of the computer. 
While this may sound like interface blasphemy (making the person accom-
modate the computer!), it is not unprecedented. People comfortably and 
effectively use a variety of dialects daily, without feeling put upon or constrained. 
For example, people talk differently to a child than they do to an adult, because 
a child's speech and comprehension skills are less than those of an adult. Simi-
larly, people talk to their pets all the time, and successfully communicate 
attitudes and emotions, even though the speech is (in human terms) nonsense. 
When talking to a non-native speaker, people unconsciously adjust their speech 
patterns to communicate more effectively. A computer is less linguistically com-
petent than a person is, whether that person is a child or adult, so it makes sense 
that people adopt a restricted form of speech to facilitate communication. 
In terms of speech design, it's important that the designer make the com-
puter speak like one. People's speech expectations are based on their perceptions 
of whom they're talking to. If a person doesn't realize he or she is speaking to 
a child, for example, that person will perceive that he or she is talking to a very 
strange adult. Similarly, if a computer attempts to sound like a person, people 
will have expectations of a person rather than a computer. So the designer should 
present the computer as what it is: an automated speaker of a simplified dialect. 
Unfortunately, people's expectations of computer speech tend to spontaneously 
fly to the HAL model, but this can be addressed by good design. 
Modeling 
In talking with one another, and with computers, people tend to model their 
speech on the other party's. If the second party speaks tersely and rapidly, the 
first party will tend to speak tersely and rapidly as well. This unconscious mim-
icry applies to many aspects of speech: vocabulary choice, phrase structure, 
volume, pitch, rate, and so on. Modeling is one of the most powerful tools avail-
able to a speech designer for tacitly cluing in users to acceptable forms of speech. 
The following are dos and don'ts for using modeling in a VUI: 
• Do use computer prompts that are brief and to the point. The style of the 
computer's speech should instill the impression that the conversation is a 
professional one-not friendly or unfriendly, but directed to a purpose. 

VUI Design Principles and Techniques 
• Do start out with examples of acceptable speech when you provide help 
rather than try to explain what's going on. People are more often confused 
about the form of what they can say to the computer than they are about 
the meaning. 
• Don't use long, wordy prompts in the vernacular because people will 
respond in kind. 
Dis fluency 
One of the biggest inhibitors of continuous speech recognition is that people do 
not speak continuously. People's speech is peppered with "um"s, "ah"s, "er"s, 
pauses, and other fillers. People correct their own speech on the fly: "Give me 
two-no, three sodas," "I want-need-a cup of coffee." People may abandon 
forms of speech in midstream: "I was just wondering ... oh, never mind." People 
subconsciously filter out these disruptions in the normal flow of speech, both as 
listeners and speakers, and consequently do not realize how frequent these dis-
fluencies are. One way to gauge how much you expect disfluency in speech is to 
think about how unnatural synthesized speech sounds. It sounds unnatural for at 
least two reasons: its prosodic limitations (for example, lack of modulated pitch 
and emphasis) and its total (inhuman) fluency. 
Disfluency causes problems in speech interfaces because they are resolved at 
multiple levels. Pauses and fillers can be fairly effectively filtered by a voice recog-
nizer, but they also can lead to errors (for instance, inserting an extra word) that 
will baffle a semantic analyzer. Self-corrections occur at the semantic level and 
are hard to model in grammars. 
The occurrence of disfluencies increases rapidly as a function of the length of 
an utterance. The longer a person speaks without a break, the greater the number 
of disfluencies. Therefore, the main tool for dealing with disfluency in speech 
design is limiting the length of utterance. There are tradeoffs in this approach, 
however. When a person starts using a VUI, there is a lot of disfluency due to the 
person's uncertainty about what to say. As the person becomes more familiar with 
the VUI and becomes more adept at speaking in a way the computer can under-
stand, he or she wants to make longer utterances for efficiency's sake. Heed the 
following tips about how to design a VUI to minimize the effects of disfluency: 
• Do structure dialogs using mixed-initiative combined with directed 
forms. This enables users to convey as much information as they are able 
in a single utterance, but it elicits information in smaller snippets if there 
are problems communicating. 
91 

ChapterB 
92 
• Don't try to address disfluency through grammar design. It can greatly 
increase the grammar complexity and slow down recognition, but the 
chances of ultimate success are slim. 
Synthesized Speech 
Listening to synthesized speech requires more concentration and effort than lis-
tening to human speech. People vary their intonation, pitch, volume; introduce 
pauses; and provide any number of behavioral "clues" that cue the listener as to 
the structure and meaning of the spoken content. Without these cues from the 
computer, people are working with less information and have to try harder to 
decipher what's being communicated. This also means that lapses in attention 
are harder to recover from, and long messages are harder to comprehend. 
Although techniques for maximizing the comprehensibility and effectiveness of 
synthesized speech depend on the particular speech synthesizer being used, here 
are some general tips on using speech synthesis: 
• Do use recorded prompts wherever possible. Any fixed prompts (for 
example, menu choices) should be recorded. Speech synthesis should 
be reserved for reading information that varies (for example, a person's 
appointments today). 
• Do pay attention to prosodic features when using synthesized speech. 
For example, when reading lists, introduce pauses to demarcate indivi-
dualitems. 
• Don't use synthesized speech to read long lists to a person and expect the 
person to comprehend or remember the lists. The interface should be 
designed so that lists contain no more than four or five items. This may 
require introducing dialogs to refine the person's focus of interest (for 
example, rather than reviewing alll5 of today's appointments, start with 
five of this morning's appointments). 
NOTE 
See Table 9-1 in Chapter 9 for a list of the VoiceXML tags that affect 
how synthesized speech sounds. 

VUI Design Principles and Techniques 
Getting the Most Out of Speech Recognition 
Recognizing speech is not an exact, analytical science. It is a probabilistic art 
and incorporates elements of sophisticated guessing. There are some design 
principles that can help you make the most of this inexact but powerful tool. The 
basic principle is to make expected responses as distinct as possible. Unfortu-
nately, a person's intuition about phrases that sound "close together" or 
"different" isn't a very good predictor of how a voice recognizer will perform. As 
anyone who has used a computer dictation system knows, voice recognizers 
make the oddest mistakes. This means that grammars need to be field tested for 
recognition problems. 
Some gross properties of responses, however, do have a strong effect on 
recognition. The main property is response length. The best responses are brief 
phrases that give the recognizer some meat to chew on, but are not so long that 
disfluencies crop up. So, for example, "Speak louder" is preferable to "Louder." 
This is particularly true for background grammars. In VoiceXML, several gram-
mars may be active at once: 
l. Field grammar (awaiting a response to a prompt for information) 
2. 
Form-scoped links 
3. 
Document-scoped links 
4. 
Application-scoped links 
5. 
Voice interpreter default grammars (help, cancel, and so on) 
Lower numbered grammars are "in front of" higher numbered grammars 
in the sense that in case of a toss-up, the recognizer prefers to match to lower 
numbered grammars. The shortest prompts should be reserved for the closest 
grammars (field and form), and links with broader scope should be longer 
phrases that can be recognized in a variety of contexts. 
Numbers and letters are problematic because they are so short. For example, 
"six" (the shortest spoken digit in English) is commonly both falsely inserted (rec-
ognized but not spoken) and falsely deleted (spoken but not recognized) by 
recognizers. This does not mean that numbers and letters aren't recognizable and 
shouldn't be responses, but it does mean the designer should be aware of poten-
tial problems. Some possible solutions are to allow DTMF input where numbers 
are expected and to use the International Communications alphabet for letters 
(alpha, bravo, charlie, delta, and so on instead of a, b, c, d, and so on). The follow-
ing are some techniques for effective use of speech recognition: 
93 

ChapterS 
94 
• Do use short phrases or multisyllabic words for links (for example, "Start 
over," "Speak louder"). 
• Do reserve the shortest, commonest responses for field-level responses. 
• Do allow use of DTMF where precise input of numbers is important. 
• Don't share recognition errors with the person. While the person may find 
it reasonable to ask for clarification with a question such as "Did you say 
Austin or Boston?", the computer will spontaneously come up with some-
thing goofy such as "Did you say Austin or hippopotamus?" Instead, ask 
for clarification directly (for example, "I didn't get that. What city?"). 
• Don't make field and form grammars needlessly broad (for instance, with 
lots of synonyms), because they will interfere with recognition of back-
ground links. 
Interface Design 
When using a good VUI, a person will feel oriented, in control, and able to 
anticipate what will happen next. A person is oriented if he or she can relate the 
current dialog to the task he or she wants to perform. A person feels in control 
if the person feels he or she knows what to do to affect the way the interaction with 
the computer proceeds. To be able to anticipate what will happen next, a person 
needs a mental model of"what the computer is up to." (Notice that the computer 
has no cognitive model, and therefore is not really "up to" anything. However, 
a good interface will superimpose a model purely for the person's benefit.) 
Ironically, some of the biggest challenges in designing a good VUI bear 
a strong resemblance to challenges faced by developers in the days of command-
line interfaces and small-screen displays. Probably the biggest challenge is keeping 
the person cued about what he or she can do next. In VUis, this means knowing 
what you can say in response to a voice prompt; in command -line systems, it 
means knowing what commands (and options) you can type at the command 
prompt. The other big challenge is presenting information in small enough chunks 
that people can absorb and use it. In VUis, reading long lists of options is time-
consuming, frustrating, and ineffective. On small screen displays, printing out 
more than a screen full of data caused some of the data to scroll off the screen. In 
both cases, you have to rely on human memory to retain what's important. 
The eventual resolution of command-line issues turned out to be windowed 
GUis. Commands became menu options, options became property sheets, and 
windows became scrollable. This resolution is unfortunate for VUI designers, 

VUI Design Principles and Techniques 
because the graphical solution cannot be transferred to voice. For a variety of 
reasons, VUis must succeed as an alternative to GUis, and they must work where 
GUis don't. Hopefully, during the upcoming decade we will see the evolution of 
interface techniques and technologies for handling the shortcomings of voice as 
a medium, just as in the past decade we've seen the development of GUI technol-
ogy to supplant command lines. 
Turn-Taking and Error Amplification 
A big challenge in VUI design is handling issues related to turn-taking. Because 
there are no behavioral or visual cues accompanying the conversation, it is very 
common for the two parties to lose track of whose turn it is. Once a conversation 
goes off track, it can rapidly degenerate into chaos (see Dialog 8-1). 
Dialog 8-1. Inexperienced Use.,-'1 
P (person): Umm, I'd like to know, ahhh ... 
C (computer): I'm sorr-
P: I'd like to know . .. the number of sh-
e: I'm ... 
P: I want to sell half my shares of Motorola. 
C: I didn't understand. 
P: I'd like to SELL ... half my shares ... 
C: I didn-
P: ... of Motorola 
C: I didn't under-
P: I'd like to SELL ... Motorola. 
C: I didn't understand. 
In this example, an inexperienced user pauses because of uncertainty 
about how to proceed. The computer interprets the pauses as the end of unrec-
ognized responses and responds with an error prompt. The user pauses when 
the computer speaks, trying to yield his or her turn, but the computer is busy 
digesting the remnant of the user's previous attempt to spit out a whole sen-
tence, which it can't recognize, and so on. The dialog spirals out of control, as 
the user attempts to get through to the brain-damaged computer by increasing 
volume and overstressing "sell," which guarantees that the computer will not 
recognize the speech. At this point, the only solution is to separate the combat-
ants for a cooling-off period. 
3 From How to Build a Speech Recognition Application, by Bruce Balentine and David P. 
Morgan, pg. 99. 
95 

ChapterS 
96 
This is an example of error amplification, in which an initial small error 
rapidly escalates into a wholesale breakdown in communication. It can be 
initiated by virtually any kind of error that confuses turn-taking. This kind of 
error escalation is familiar to designers of real-time process control systems, 
and it is also known as cascading errors. A major goal of interface design is to 
structure the interface so that when anything goes astray, the interface will get 
the person to a state where he or she is once again oriented, in control, and 
knows what's coming. 
To help stabilize dialogs and prevent error amplification, build safe points 
into the interface. A safe point is a known state of the application, usually a high-
level menu, at which the person and computer can resynchronize turn-taking 
and generally get a fresh start. The tricky part about safe points is figuring out 
when to transition to them. If things are going badly, the person and computer 
may not be listening well to each other, and so normal prompting may be 
ignored. A last-ditch technique to stabilize a dialog is to prompt the user with 
a modal prompt that requires a "yes" or "no" response: 
Computer: Do you want to continue adding appointments to your calendar? Say 
yes or no after the beep. {beep} 
A "no" answer would initiate transfer to a safe point. Such a prompt can 
occur after a certain number of errors occur in a row. 
See the end of the following section for some tips that affect both turn-taking 
and orientation issues. 
Lost in Space 
Because speech is not persistent, people must rely on their memory to know 
"where they are" in a conversation. Such short-term memory is fragile and can 
easily be disturbed by any kind of distraction: avoiding a road hazard while driv-
ing, another person entering the room, a child interrupting his or her mother, 
a fire truck screaming by, and so on. When these distractions occur during a con-
versation, a person may recover by saying something like "Where was I?" The 
other person reorients the distracted one, and the conversation resumes. 
Feeling "lost in space" and out of control should be dealt with in a good VUI. 
Distraction is a common source of disorientation. Disorientation also results 
when the computer goes down a path the person was not anticipating. The per-
son is still thinking about the task at hand, but the computer's prompts have 
become irrelevant. 
A good technique for maintaining orientation is to provide auditory cues 
along the way, so that the person receives some feedback about where he or she 
is. For example, different parts of an application might use different voices. In 

VUI Design Principles and Techniques 
the SPIM example, you might use different voices for Calendar, Contact, and 
Appointment. Earcons, or auditory icons, can accentuate meaning of different 
types of prompts. For example, different tones can be used for confirmations, 
errors, and menu choices. This does not require that the conversation rely on 
tones for tum-taking (as in "Wait for the tone before speaking"). Bargein can be 
enabled so that experienced users do not have to wait, while inexperienced users 
will get greater contextual feedback. Music or other nonspeech audio can be used 
to signal that the computer is working (and hence is holding on to its turn). 
A related technique is to include orientation tips in prompts generated 
when the person is silent or can't be understood.4 For example, the user might 
receive the following prompt if they do not respond: 
Computer: You are in your address book. Adding entry for Veronica Voice. V\lhat is 
the home phone number? 
This technique should be used judiciously, in tandem with tapered prompt-
ing, because the additional orientation information increases the prompt length 
and may frustrate users who are making small errors but are not disoriented. 
The following tips apply to good design with regard to tum-taking 
and orientation: 
• Do be aware in designing all dialogs that distraction can strike at any time 
and that errors may be the result of a loss of orientation rather than a lack 
of knowledge. 
• Do include some auditory feedback to maintain orientation and signal 
when it's the computer's turn. 
• Do provide application safe points where the user can reorient him- or her-
self and resynchronize tum-taking. 
• Do incorporate last -ditch error handling to avoid runaway error 
amplification. 
• Don't use too many tunes, tones, or other nonspeech audio-it becomes 
tiring to listen to repeatedly. 
4 This is the audible equivalent of the technique used on U.S. interstate highway marker 
signs, which seem to have a formula for orientation. The signs show where you are (I95S), 
the next major destination (New York City lOOmi), and the next destination (West Nowhere 
2mi). 
97 

ChapterB 
98 
• Don't have the computer take drastic unilateral action (for example, trans-
ferring the user to an operator) without confirmation from the user. 
• Don't force a lot of contextual information on the user unless he or she 
requests it. 
Accommodating Different Experience Levels and 
Environments 
People using your VUI will have varying levels of experience with voice interfaces 
in particular and with computers in general. One of the motivators for enabling 
Web applications with voice is to reach the user base that does not use PCs, digi-
tal networks, keyboards, and mice. So, at one end of the spectrum, novice users 
require an interface that is very simple, easy to use, and undemanding of the 
user. At the other end of the spectrum, advanced users require an interface that 
performs efficiently in a car, from a public phone, and in other mouse-less and 
keyboard-less environments. 
Experience manifests itself in the following ways: 
• Experienced users don't need constant cuing about what's expected of 
them. Too many cues can make experienced users impatient with 
unneeded prompts and more aggressive about barging in. 
• Experienced users are more likely to retain their cool when errors crop up 
and to let the computer "catch up" when tum-taking errors occur. 
• Experienced users require occasional help on options available in unfamil-
iar parts of the application, but they do not tolerate long help prompts. 
• Experienced users can become disoriented as a result of distractions. 
• Experienced users learn to speak to the computer so they will be under-
stood, using constant volume, stress, and rate; enunciating fully; and so 
on. They are therefore capable oflonger productive utterances than 
novices are. 
It's tempting to think that the error rate decreases as user experience 
increases, but this may not be the case. The reason for this is that errors are 
a product not only of speech, but of environment as well. In fact, it's quite 

VUI Design Principles and Techniques 
possible that experienced users will only use voice when other options are not 
available. So, when an experienced user is in the quiet of his or her home with 
a good microphone and no interruptions, he or she will use a mouse and 
keyboard. When the same user is at an airport with flight announcements 
randomly drowning out speech, he or she will use voice. 
Environmental characteristics that affect how well the VUI performs include 
the following: 
• Background noise: Recognizers are surprisingly good at screening out 
constant background noise, but both people and computers have trouble 
dealing with "bursts" of noise (for example, airport speakers, thunder, and 
soon). 
• Other people talking: If you're sitting on your couch with your parrot 
chattering on your shoulder and Peter Jennings blaring on the TV; the com-
puter is going to try to recognize everybody's speech-not just yours. 
• Voice fidelity: The computer tries to recognize speech as received at 
the voice gateway. Anything between the larynx and gateway that degrades 
voice quality degrades performance. Low-quality microphones and poor 
telephone connections are the usual suspects. 
Good interface design attempts to make the VUI adjust to speaker and 
environmental variations by trading off efficiency for robustness. With an experi-
enced speaker and a good environment, dialogs should permit (but not require) 
longer utterances. With a naive user and a noisy environment, dialogs should col-
lapse to brief, structured interchanges that are tedious but reliable. Essentially, 
this strategy adapts the dialogs to keep the error rate down. If there are many 
problems communicating, slow down and take it a step at a time. 
Using VoiceXML mixed-initiative dialogs is a good way to permit a user to 
use longer utterances. The user can respond to the initial prompt with as much 
or as little information as he or she wants. The dialog then switches into directed 
mode and elicits information a piece at a time. This can require sophisticated 
grammar design, so that both short and long utterances are successfully recog-
nized (see Table 8-2). 
99 

ChapterS 
100 
Table 8-2. Novice and Expert Mixed-Initiative Dialog 
NOVICE USER 
C (computer): Add appointment. 
You may say help at any time. 
P (person): Help. 
C: Say something like call Joe. 
P: Call Pete. 
C: Whatday? 
P: Friday. 
C: What time? 
P: lOa.m. 
C: Where? 
P: Work. 
C: You have a call with Pete on Friday 
at ten am at work. 
EXPERT USER 
C (computer): Add appointment. You 
may say help at any time. 
P (person): Call Pete on Friday at 10 
a.m. at work. 
C: You have a call with Pete on Friday at 
ten am at work. 
As mentioned previously, falling back to a yes/no interrogation gives about as 
much reliability as is possible: The recognizer only has to match against two possi-
ble responses. People don't tolerate a "20 questions" style of interface in general, 
but it may be acceptable in extreme circumstances, as shown in Dialog 8-2. 
Dialog 8-2. Fallback to 20 Questions 
C (computer): Do you want the shirt in red, green, or blue? 
P (person): {static} 
C: Do you want the shirt in red, green, or blue? 
P: 
{static} 
C: Do you want red? Say yes or no. 
P: 
No. 
C: Do you want green? Say yes or no. 
P: 
Yes. 
C: Do you want small, medium, large, or extra large? 
Because of this adaptability, the interface may become more complex, and 
there may be many valid responses to any given prompt. So, doesn't that make it 
more complicated to cue the user as to what's expected? Not necessarily. The sup-
port structure of the interface-help prompts, event handlers, and so on-should 
be primarily geared toward the simplest way of doing things. Even if there are ten 
ways to do something, the basic prompting should describe one way: the simplest 
way. The other nine options are still available, but they are not explicitly announced 
or described. The process of turning novices into experts should be treated as a 
training issue, not a prompting issue. Possible techniques for enlightening 
advanced users include offline tutorials and toggling a mode switch that switches 

VUI Design Principles and Techniques 
the interface between "novice" mode and "expert" mode. Also, consider the follow-
ing tips for accommodating different experience levels and environments: 
• Do provide opportunities for expert users to use shortcuts. 
• Do use mixed-initiative dialogs backed up with directed prompting for 
filling out forms. 
• Do incorporate yes/no exchanges as the fallback when more complicated 
dialogs are not working. 
• Don't clutter up basic prompts with a lot of tutorial material aimed at 
expert users. 
• Don't assume that if a user encounters a lot of errors it means he or she is 
"slow" -it may mean that an expert user is in a tough environment. 
Taking Time Out 
When talking on the phone, people often engage in sidebar "mini conversations" 
with people around them, without explicitly interrupting the phone conversation. 
There are all kinds of amusing behaviors associated with these sidebars, including 
the following: 
• Exaggerating facial expressions, mouthing words, pointing at a watch, 
and soon 
• Pantomiming, as in the game charades 
• Writing notes on paper or white boards 
• Commenting in a sotto voce to someone nearby 
• Putting a hand over the phone's mouthpiece or enabling the phone's mute 
feature to engage in a full-fledged conversation with someone else 
While people seem to be able to converse through various types of conver-
sationallapses, to a computer they sound like unrecognizable speech. The 
speech may be unrecognizable because it's disfluent, because the person's 
normal speech cadence is altered, because the computer hears things a person 
would filter out, or because a person misses timing on a response and speaks too 
101 

ChapterS 
102 
soon or too late. Regardless of the reason, the person can suddenly find the 
computer reacting to "errors" and conversational turn-taking disrupted. 
The VUI should provide ways for a user to pause or suspend a dialog at any 
point. When the user is using a microphone, issuing a command to turn the 
microphone off and on works well. On a phone, the computer cannot control 
the microphone, so a voice command that causes the computer to ignore input 
for a while is required. The command should be available at any prompt, as 
shown in Dialog 8-3. 
Dialog 8-3. Taking Time Out for Distractions 
C (computer): Please say the name of the person . . . 
P (person): {barging in} Go to sleep. 
P: Wakeup. 
C: Please say the name of the person to call. 
OR 
C: Please say the name of the person to call. 
P: 
Give me thirty seconds. 
{after 30 seconds} 
C: Are you ready to continue? 
P: 
Give me thirty seconds. 
{after 30 seconds} 
C: Are you ready to continue? 
P: Yes. 
C: Please say the name of the person to call. 
CAUTION VoiceXML does not provide an easy way to temporarily suspend 
the VoiceXML interpreter as described here. On my opinion, this is a short-
coming ofVoiceXML.J Implementing the design described here requires the 
use of proprietary features of a particular voice platform. 
Summary 
Careful VUI design is not an option-it is a requirement. A poorly designed VUI 
can be not only frustrating to the user, but also outright insulting and provoca-
tive. This chapter elaborated three core principles ofVUI design: deceptive 
simplicity, error accommodation, and designing for everyone. Some basic 
behavior and psychological characteristics of speech were also introduced, 
along with tips and techniques for dealing with some of the common pitfalls 
and challenges ofVUis. 

VUI Design Principles and Techniques 
For pragmatic, hands-on tips about implementing speech recognition 
applications, I recommend reading How to Build a Speech Recognition Appli-
cation, by Bruce Balentine and David P. Morgan. For a broader view of speech 
interfaces in general, I recommend Designing Effective Speech Interfaces, by 
Susan Weinschenk and Dean T. Barker (the companion Web site address is 
http: I lwww. wiley. comlcompbookslweinschenkl). Also, the June 2001 issue of 
VoiceXML Review (http: I lwww. voicexmlreview. orgl) covers human consider-
ations for VoiceXML applications. 
103 

CHAPTER 9 
VoiceXML Programming 
Guide 
THis CHAPTER EXPLORES VoiceXML language elements individually and in 
detail. This chapter is intended to provide a conceptual elaboration of 
VoiceXML language elements for someone who is already familiar with the 
basics of programming voice applications. People who learn best "hands on" 
should go through the VoiceXML tutorial in Chapter 7 before tackling the 
material in this chapter. For a complete specification ofVoiceXML syntax, see 
Appendix A or the VoiceXML specification. 
Structure of a VoiceXML Program 
Many of the structural concepts in VoiceXML (scope, events, and variables) may 
seem familiar to programmers who have used other languages. However, the way 
VoiceXML ties together the static structure of source code with the dynamic state 
of a running application has some subtleties that knowledgeable newcomers 
tend to gloss over. Some of these subtleties are a result ofthe real-time nature of 
speech processing, and some are the result ofVoiceXML's essential nature as an 
interpreted markup language rather than a compiled programming language. 
The following sections establish some basic concepts that apply throughout the 
rest of the chapter. 
Static Structure 
The static elements of a VoiceXML program form a hierarchy. Conceptually, 
there is a single application that is composed of one or more dialogs. Each dialog 
structures an interaction with a person. Dialogs are structured sequences of 
alternating computer-generated prompts and human responses. 
Physically, a VoiceXML program consists of a set of documents. Each docu-
ment contains VoiceXML code defining one or more VoiceXML dialogs. One 
document is designated the application root document. All documents that 
specifically designate the application root document are considered part of the 
same application. Documents are usually just text files, and documents are 
the program units that are sent from a Web server to a VoiceXML browser. 
105 

Chapter9 
106 
A dialog is the basic executable unit of a VoiceXML program, akin to a sub-
routine or function in conventional languages. Dialogs are implemented by one 
or more forms or menus. A form consists of a set of form items (variables and/ or 
code blocks) and the "recipe" (control logic) for working with a person to assign 
values to items. A menu is a collection of alternatives, each with a unique 
prompt, from which a person may select one. 
Scope 
To share code and data between cooperating forms in an executing application, 
VoiceXML defines a scope hierarchy. The scope hierarchy mirrors the static struc-
ture of the program and consists of five levels: session, application, document, 
dialog, and (anonymous). 
A scope can contain variables, properties, event handlers, and grammars. 
Entities in session scope are defined by the VoiceXML interpreter and may not be 
changed by an application. These include default event handlers, default and 
platform-specific properties and variables, default event handlers, and built-in 
grammars. Entities are placed in application scope by declaring them in the 
application root document. Application-scoped entities are available whenever 
any part of the application is running. Document scope is shared by all dialogs 
within a document. Dialog scope is established by the currently active form or 
menu, and (anonymous) scope corresponds to the current item. (See Table A-2.) 
Dynamic Execution Context 
As a VoiceXML program executes, the flow of control moves from item to item 
within a form and from form to form. The VoiceXML interpreter maintains an 
execution context that corresponds to the current point of control. The execution 
context contains all entities that are currently active (in scope). Session- and 
application-scoped entities are active throughout a single invocation of an appli-
cation. As the flow of control moves between forms, the interpreter may fetch 
and load documents containing the forms, causing the execution context to 
contain different document-scoped entities at different times. Similarly, the exe-
cution context is adjusted to correspond to the currently executing form (dialog 
scope) and currently executing form item (anonymous scope). 
When a variable is in scope (in the current execution context), it can be refer-
enced by name in a VoiceXML <value> tag or embedded ECMAScript. When 
a property is set using the <property> tag, the interpreter places the value in the 
scope in which it was set. 
When the speech recognition engine tries to match an utterance, it checks 
the utterance against all active grammars, starting with the grammars active in the 
closest scope and moving up the scope hierarchy. Typically, multiple grammars 
are active at once. For example, session-scoped grammars for the default <help> 

VoiceXML Programming Guide 
and <cancel> responses are always active (although sometimes overridden). 
There is also an active grammar for every active link. 
When an event handler is active, it is available for handling events that are 
thrown in the currently executing code. If there are event handlers defined in 
multiple scopes, there may be multiple handlers available when an event is 
thrown. The interpreter chooses the closest appropriate handler to process 
the event. 
Input and Output 
The following VoiceXML elements are used to specify input and output: 
• <prompt> 
• <audio> 
• <grammar> 
• <dtmf> 
• <record> 
The <prompt> and <audio> elements are used to prompt the user for input 
and to present output to the user. The <grammar> and <dtmf> elements are used 
to specify what a user can say or key, and how the user's speech or keystrokes are 
translated into text strings that can be sent to the Web server in HTTP messages. 
Text that is to be read to the user is placed within a <prompt> element in 
a VoiceXML document. When the VoiceXML interpreter encounters a <prompt> 
element in the course of its conversation with the user, it feeds the text content of 
the element to its text-to-speech subsystem, which reads the text to the user. 
Recorded audio is specified using the <audio> element. This element must 
be a child of a <prompt> element, where it can be used in place of, or in addition 
to, text. The <audio> element has an src attribute that specifies the URI of the 
audio file. To render an <audio> element, the VoiceXML interpreter fetches 
the file from a Web server and feeds it to its audio playback subsystem. 
The <prompt> element has a Boolean attribute, bargein, that controls 
whether or not the user can interrupt a prompt. When it is set to false, the 
VoiceXML interpreter will read or play the entire prompt before responding 
to user input. When it is set to true (the default value), the user can respond 
before the VoiceXML interpreter has finished. 
The <grammar> element is used to specify what the user can say and how the 
user's utterance is mapped to an action or field value. The <grammar> element 
107 

Chapter9 
108 
can appear in a form or link. When the VoiceXML interpreter encounters a gram-
mar, it feeds it to the speech recognition subsystem. The speech recognition 
subsystem listens to what the user says and attempts to match it with the gram-
mar. When it does, it returns text to the VoiceXML interpreter. Speech grammars 
are expressed in a grammar language. The grammar can be the content of the 
<grammar> element or it can be in a separate file identified by the URI in the src 
attribute of the <grammar> element. 
The <dtmf> element is used to specify what key sequences the user can 
enter as an alternative to speaking. The <dtmf> element is used in menus and 
forms. When the VoiceXML interpreter encounters a <dtmf> element, it feeds it 
to the DTMF recognition subsystem. The DTMF recognition subsystem listens 
for keystrokes and attempts to match them with the content of the <dtmf> ele-
ment. When it does, it returns characters to the VoiceXML interpreter. 
VXML 2: VoiceXML 2.0 is expected to require support for the Speech Recog-
nition Grammar Specification for the W3C Speech Interface Framework 
(http: I /www.w3 .org/TR/speech-grammar/). This specification treats 
"mode" (voice or dtmf) as an attribute of the grammar. The VoiceXML I. 0 
<grammar> element specifies a Speech Recognition Grammar Specifi-
cation (SRG) grammar whose mode is "voice': The VoiceXML 1.0 <dtmf> 
element specifies an SRG grammar whose mode is "dtm[': 
The <record> element is used to record what the user says without inter-
pretation. It can appear in forms. When the VoiceXML interpreter encounters 
a <record> element, it uses the audio player/recorder subsystem to record what 
the user says. This audio stream can then be sent to a Web server for storage or 
further processing. 
Navigation 
VoiceXML has two elements for organizing and navigating a voice Web site: 
o <menu> 
o <link> 
The <menu> element enables the user to select from a list of choices. 
A <menu> element generally contains a <prompt> element followed by 
<choice> elements. The interpreter reads or plays the prompt and then listens 

VoiceXML Programming Guide 
for the user's response. The user can make his or her selection using speech or 
the keypad. The content of the <choice> element specifies what the user must 
say to select it. The dtmf attribute of the <choice> element specifies what key the 
user can press to select it. Each <choice> element has a next attribute that spec-
ifies the URI of a dialog or document. When the user chooses one, the VoiceXML 
interpreter transitions the conversation to the next dialog. 
The <link> element is the voice analog of the HTML hypertext link. A link 
has a trigger and an action. The trigger is a speech or DTMF grammar. When the 
user's input matches the link's grammar, the VoiceXML interpreter executes its 
action. The VoiceXML interpreter may transition to another document or dialog, 
or it may throw an event. The action is specified by the next or event attribute of 
the <link> element. 
Forms and Fields 
In VoiceXML, forms are the basic building blocks for structuring dialogs. Func-
tionally, VoiceXML forms are similar to their ancestors, HTML forms: A form 
defines a set of fields that are filled in by the user. In HTML, fields are filled in by 
typing in text or using the mouse to make selections through radio buttons, 
check boxes, or pick lists. In VoiceXML, forms are filled in by speaking responses 
to prompts. VoiceXML forms have some features not found in HTML forms. In 
VoiceXML, multiple fields may be filled in by a single utterance. Also, VoiceXML 
subdialogs allow complex forms to be composed out of simple components in 
a nested manner. 
The <form> element contains <field> elements for the form fields. The 
<field> element has a name attribute. Each field in a form has a form variable, 
whose name is the value of the field's name attribute. This variable holds the text 
value of the field. 
A field typically contains a prompt, a recognition grammar, and a filled 
action. The <prompt> element within a <field> specifies the prompt to be read 
or played to the caller when the conversation enters the field. The <grammar> or 
<dtmf> element within a <field> element specifies the range of valid 
responses from the caller and how they are mapped into text values of the field 
variable. There are built-in grammars for common data types. These are specified 
by the type attribute of the field, rather than with an explicit <grammar> element. 
A field may contain a <filled> element, which specifies an action to 
perform after the field has been filled. In simple forms, the last field contains 
a <filled> element, which in turn contains a <submit> element that sends an 
HTTP request to a Web server with the field values. A field can also contain event 
handlers such as <help>, <noinput>, and <nomatch>. 
In a simple form, the conversation between the caller and the VoiceXML 
interpreter proceeds from field to field, in the order in which the fields appear in 
109 

Chapter9 
110 
the VoiceXML document. The VoiceXML interpreter reads or plays the prompt 
in each field and compares the user response to the field's grammar. When the 
caller provides a valid response, the VoiceXML interpreter assigns a value to 
the field variable, performs the filled action, and goes on to the next field in the 
form. You can explicitly control the order in which the fields are visited by putting 
<goto> elements in the filled actions for the fields. 
A <filled> element can also be a child of the <form> element and the 
same level as the <field> elements. The <filled> element at the form level 
has a namelist attribute that specifies a list of field names. When all of the fields 
in this list have been filled, the <filled> action is executed. The <submit> ele-
ment for a form is usually placed within a form level <filled> element. In that 
way, the form will be submitted when the required fields have been filled, regard-
less of the order in which they were filled. 
Dialog, Document, and Application 
A VoiceXML document is a single file that is retrieved through HTTP from a Web 
server. The root element in a VoiceXML document is the <vxml> element. 
A VoiceXML document contains dialogs. There are two kinds of dialogs: forms 
and menus. Forms contain form items, each of which is a single interaction 
between a caller and the VoiceXML interpreter. When all of the fields have been 
filled, the form is typically submitted to the Web server, which returns another 
VoiceXML document. The conversation between the caller and VoiceXML inter-
preter transitions to that document. A menu is a single interaction between the 
caller and VoiceXML interpreter in which the caller is presented with a list of 
options and the VoiceXML interpreter transitions the conversation to another 
dialog based on the caller's choice. 
The <vxml> element has an optional application attribute whose value is 
the URI of the application root document. By default, each VoiceXML document 
runs as an isolated application. When a document that specifies an application 
root document is loaded, the VoiceXML interpreter also ensures that the appli-
cation root document is loaded. The application root document can define 
variables and links that are available for use by all documents in the application. 
The conversation between a caller and the VoiceXML interpreter is always in 
one dialog. When the VoiceXML interpreter initially answers the call, it retrieves 
a VoiceXML document from a Web server, and the conversation is in the first dia-
log in that document. The conversation can move to another dialog in the same 
document or it can move to another document. Transitions are expressed in 
VoiceXML by links (<goto>), menus (<next>), and forms (<submit>). 

VoiceXML Programming Guide 
Speech Recognition Grammars 
Speech recognition grammars are used in links, menus, and forms to translate 
what a caller says into navigational commands or field values. VoiceXML has 
several means of specifying a speech recognition grammar: 
• The type attribute of the <field> element 
• The <choice> element in a menu 
• The <option> element in a field 
• The <grammar> element in a link, form, or field 
VoiceXML interpreters have built-in speech recognition grammars for com-
mon data types: boolean, date, digits, currency, number, phone, and time. When 
a field specifies one of these as its type attribute, the VoiceXML interpreter uses 
the built-in grammar for the field. 
Each choice in a menu has a speech recognition grammar fragment that is 
specified by the content of the <choice> element. A field can also specify a list 
of choices using the <option> element. The content of each <option> element 
specifies the speech recognition grammar fragment for that choice. In both cases, 
the VoiceXML interpreter assembles the speech recognition grammar for the 
menu or field from the <choice> or <option> elements. 
The <grammar> Element 
The <grammar> element is the most general construct for explicitly specifying 
a speech recognition grammar. The <grammar> element can appear in a link, 
form, or field. The grammar specifies what a caller can say to trigger an action or 
supply information. 
The <grammar> element may specify either an inline grammar or an external 
grammar. An inline grammar is specified by the content of the <grammar> ele-
ment. Alternatively, the grammar may be placed in an external file. In this case, 
the src attribute of the <grammar> element specifies the URI of the file, and the 
element is empty. 
Grammars have scope, which determines when a grammar is active (that is, 
when it is being used by the speech recognition subsystem). If a grammar is 
inside a field, the scope of the grammar is the field. A field grammar is active only 
when the interpreter is visiting the field. If a grammar is inside a link, the scope of 
the grammar is the scope of the element that contains the link (dialog, docu-
ment, or application). If a grammar is inside a dialog, by default its scope is that 
111 

Chapter9 
112 
dialog. The scope of a dialog grammar can be extended to the entire document 
using the scope attribute of the <grammar> element. 
Grammar Formats in VoiceXML 1.0 
VoiceXML 1.0 does not specify a required grammar format. It is left up to the 
implementer of a particular platform to decide what grammar formats are recog-
nized. Two widely used formats are Sun's Java Speech Grammar Format (JSGF) 
and Nuance's Grammar Specification Language (GSL). Some VoiceXML inter-
preter implementations support JSGF, while others support GSL. When you use 
an explicit inline grammar, you should specify the MIME type for the grammar 
format in the type attribute of the <grammar> element. The MIME type values for 
the two grammar formats are as follows: 
• JSGF: "application/x-jgsf" 
• GSL: "application/x-gsl" 
Grammar Formats in VoiceXML 2.0 
VoiceXML 2.0 is expected to require support for a draft standard grammar speci-
fication language called Speech Recognition Grammar . Visit 
http: I /www. w3. org/TR/ speech-grammar I for more of the specification's W3C 
working draft. The draft describes two syntaxes for the language: an Augmented 
Backus Naur Form (ABNF) syntax and an equivalentXML-compliant syntax. Only 
the XML-compliant syntax is expected to be required byVoiceXML 2.0. At the 
time of the writing of this book, this is a situation where one standard under 
development depends on another standard under development. See the section 
titled "Grammar and Speech Synthesis Specification'' in Chapter 14 for a brief 
overview of the current state of the new grammar format. 
The MIME type for the new grammar is "application/ grammar" for the ABNF 
syntax and "application/ grammar+vxml" for the XML-compliant syntax. 
Speech Synthesis Markup 
TTS synthesis is used to render text in <choice>, <prompt>, <enumerate>, and 
<audio> elements into speech. VoiceXML provides markup tags to direct how 
the generated speech should sound. Speech synthesis markup can affect pronun-
ciation, pauses, emphasis, pitch, phrasing, and speaking rate. 

VoiceXML Programming Guide 
VoiceXML 1.0 defined its own markup tags based on a draft Java Speech Markup 
Language specification (http: I /java. sun. com/products/java-media/speech). 
VoiceXML 2.0 is expected to make the 1.0 tags obsolete in favor of reference to a sep-
arate W3C draft specification, Speech Synthesis Markup Language Specification 
for the Speech Interface Framework (http: I /www. w3. org/TR/ speech-synthesis). 
Table 9-1 summarizes the tags and their meanings. 
Table 9-1. Speech Synthesis Markup Tags 
VOICEXML 1.0 VOICEXML 1.0 
SSML' 
SSML 
USAGE 
TAG 
ATTRIBUTES 
TAGS 
ATTRIBUTES 
<break> 
msecs 
<break> 
time 
Insert a pause. 
size 
size 
<div> 
type="paragraph" <p>or 
xml:lang 
Mark paragraph 
<paragraph> 
boundary. 
<div> 
type="sentence" 
<s>or 
xml:lang 
Mark sentence 
<sentence> 
boundary. 
<emp> 
level 
<emphasis> 
level 
Speak with emphasis. 
<pros> 
rate 
<prosody> 
pitch 
Specify prosodic 
vol 
contour 
attributes such as 
pitch 
range 
pitch, speaking rate, 
range 
rate 
volume, and so on. 
duration 
volume 
<sayas> 
sub 
<say-as> 
sub 
Identify the 
class 
type 
semantic type of a 
word or phrase (for 
example, currency 
or time). See Table 
A-9 in Appendix A. 
<sayas> 
phon 
<phoneme> 
ph 
Specify phonetic 
alphabet 
pronunciation. 
<voice> 
gender 
Set voice charac-
age 
teristics of the 
category 
speaker. 
variant 
name 
<mark> 
Ignored. 
1 Based on Speech Synthesis Markup Language Specification for the Speech Interface 
Framework, W3C Working Draft 3 January 2001. 
113 

Chapter9 
114 
NOTE 
VoiceXML standardizes the syntax of speech markup, but it is still 
up to the voice platform to make decisions about how to interpret (render) 
the various markups. In fact, the platform may ignore speech markup. As 
a result, text marked up as a "teenage girl speaking emphasized phone 
number" may sound like your daughter on one voice platform and like the 
wrestler next d.oor on another. 
The details of how speech is synthesized and how markup tags are inter-
preted are technical and platform dependent. See Table A-1 in Appendix A for 
more detailed information about the syntax of markup tags. For more infor-
mation on the meaning of the speech markup language, see the VoiceXML 
specification or the Speech Synthesis Markup Language Specification for the 
Speech Interface Framework (http: I lwww. w3. org/TR/ speech-synthesis). For 
information about how markup tags are rendered into speech, see the documen-
tation for your chosen voice platform. 
Events and Handlers 
Events are named objects that represent the occurrence of a particular situation 
or condition. Events can be generated in three ways: by the VoiceXML interpreter 
itself, by executing a <throw> element, or by specifying the event attribute on 
a <link> or <return>. 
Event handlers are code fragments that are executed when an event occurs. 
As in other programming languages, event handlers nest hierarchically in scope, 
and the "closest" handler is invoked when an event is thrown. 
The simplest form of event handling is provided by the shorthand 
<catch> elements. 
• <error> is shorthand for <catch event="error">. 
• <help> is shorthand for <catch event="help">. 
• <noinput> is shorthand for <catch event="noinput">. 
• <nomatch> is shorthand for <catch event="nomatch">. 
VoiceXML specifies predefined events and errors, which are thrown by the 
VoiceXML interpreter. If you do not specify explicit handlers for these events, 
the VoiceXML interpreter invokes its default handlers. You can also make up your 

VoiceXML Programming Guide 
own events. You must explicitly throw your events using the <throw> element. 
Your event handler must specify a matching event name. 
When an event is thrown, either by the VoiceXML interpreter or by your 
application, the interpreter selects the <catch> element "best qualified" to han-
dle the event based on the following: 
Scope: The menu or form item being visited at the time the event is thrown 
must be in the scope of the event handler. 
Event name: The event name specified by the event attribute of the 
<catch> element matches the name of the event being thrown. The 
names match if they are identical, or if the <catch> name is a prefix of 
the thrown event name (see the VoiceXML specification for details). 
Count: The count of the <catch> element must be less than or equal to 
the event count. 
VoiceXML defines more predefined events than it defines special elements, so 
some predefined events can only be caught by <catch> elements (see Table 9-2). 
Table 9-2. VoiceXML 1.0 Events 
EVENT NAME 
TYPE 
info 
TRIGGERED BY 
VXML 
CAUGHT BY 
<filled> 
MEANING 
A phrase in an active 
grammar was 
recognized. 
cancel 
info 
VXML, <throw> 
<catch> 
The user has requested 
to cancel playing of the 
current prompt. 
telephone. 
disconnect.hangup 
telephone. 
disconnect. transfer 
exit 
info 
VXML, <throw> 
info 
VXML, <throw> 
info 
VXML, <throw> 
<catch> 
<catch> 
<catch> 
The user has hung up. 
The user has been 
transferred 
unconditionally to 
another line and will 
not return. 
The user has asked 
to exit. 
ll5 

Chapter9 
Table 9-2. VoiceXML 1.0 Events (continued) 
EVENT NAME 
help 
noinput 
no match 
error.badfetch 
error. semantic 
TYPE 
info 
info 
info 
error 
error 
error.noauthorization error 
116 
TRIGGERED BY 
VXML, <throw> 
VXML, <throw> 
VXML, <throw> 
VXML, <throw> 
VXML, <throw> 
VXML, <throw> 
CAUGHT BY 
<catch>, <help> 
MEANING 
The user has asked 
for help. 
<catch>, <noinput> The user has not 
responded within the 
timeout interval. 
<catch>, <nomatch> The user input 
something, but it was 
not recognized. 
<catch>, <error> 
<catch>, <error> 
<catch>, <error> 
A failed fetch. This may 
be the result, for 
example, of a missing 
document, a mal-
formed URI, a 
communications error 
during the process of 
fetching the document, 
a timeout, a security 
violation, or a 
malformed document. 
A run-time error was 
found in the VoiceXML 
document-for example, 
a divide by 0, a substring 
bounds error, or an 
undefmed variable was 
referenced. 
The user is not 
authorized to perform 
the operation requested 
(such as dialing an 
invalid telephone 
number or one the 
user is not allowed 
to call). 

Table 9-2. VoiceXML 1.0 Events (continued) 
EVENT NAME 
error. unsupported. 
format 
error. unsupported. 
element 
TYPE 
error 
error 
- application defined - app 
TRIGGERED BY 
VXML, <throw> 
VXML, <throw> 
<throw> 
CAUGHT BY 
<catch>, <error> 
<catch>, <error> 
<catch> 
VoiceXML Programming Guide 
MEANING 
The requested resource 
has a format that is not 
supported by the 
platform-for example, 
an unsupported 
grammar format, audio 
file format, object type, 
or MIME type. 
The platform does not 
support the given 
element. For instance, if 
a platform does not 
implement <record>, it 
must throw 
error. unsupported. 
record. This enables an 
author to use event 
handling to adapt to 
different platform 
capabilities. 
Application-defined 
events should follow the 
Java naming convention: 
<orgname>. 
<app name>.<event> 
(for example, 
com.company.spim. 
nocontactfound). 
Form Items and the Form Interpretation Algorithm 
A form contains fields and other form items. The VoiceXML interpreter presents 
the form items to the caller one at a time. (The specification calls this "visiting" 
a form item.) The form interpretation algorithm (FIA) specifies this process. The 
main loop of the FIA has three phases: 
117 

Chapter9 
118 
1. 
Select the next form item to visit. 
2. 
Collect an input or event. 
3. 
Process the input or event. 
Each form item has a variable and a guard condition. The form item's vari-
able and guard condition are used in the select phase to determine which form 
item will be visited next. The default behavior of a form is to visit each form item 
once in the order they occur in the form. The initial value of each form variable is 
undefined by default. The FIA selects the first form item whose value is unde-
fined. After a form item receives a value, the FIA moves on to the next form item. 
You can change the order in which form items are visited by including an explicit 
<goto next item=" ... ">element in the <filled> action of a form item. 
In the collect phase, the VoiceXML interpreter reads or plays the prompt for 
the form item and activates the field grammar. Then it waits for the user to say or 
key a response that matches any active grammar or for an event to be thrown. 
After either event occurs, the FIA moves on to the process phase. 
In the process phase, if there was a grammar match on a link, the interpreter 
executes the link's transition. If there was a grammar match on the field being 
visited or its form, values are assigned to variables and <filled> actions are exe-
cuted. If there was a grammar match on another form (document or application 
scope), the FIA transitions to that form. If an event was thrown, the interpreter 
identifies and executes the applicable <catch> element. 
The form items are subdivided into two categories: field items and control 
items. The field items are used to collect user input and store it in variables. The 
field item elements are as follows: 
<field>: Define a field in a form. 
<record>: Capture raw user input without speech recognition. 
<transfer>: See the section on telephony later in this chapter. 
<object>: Invoke platform-specific features that return values. 
<subdialog>: Analogous to a function call in a programming language. 
Invoke another dialog and return values collected by that dialog. 
There are two control elements: 
<block>: Contains procedural statements for prompting and compu-
tation, but does not gather input. The contents of a block are executed 

VoiceXML Programming Guide 
during the collect phase, and its variable is then set to true. The process 
phase is skipped for <block> elements. 
<initial>: Supports mixed-initiative forms. It usually contains a prompt 
for a form-level grammar. 
Mixed-Initiative Dialogs 
In a directed dialog (simple form), fields are filled one at a time, with a separate 
prompt and response for each field. It's called "directed" because the computer 
directs the whole conversation-the person simply supplies information. In 
a mixed-initiative dialog, the user can fill multiple fields with a single utterance, 
which resembles a normal bidirectional conversation with another person. At 
some points in a mixed-initiative dialog, the computer will be in the position of 
passively waiting for the user to speak and then taking action based on what was 
said. In the SPIM tutorial, the dialog used to add an appointment to the calendar 
is implemented as a mixed-initiative dialog: The computer prompts with an 
open-ended question ("Next Appointment?"), and the user can respond with 
a variety of utterances, from short ("Meet with Joe") to detailed ("Call Fred Tues-
day, December third at fourteen hundred hours"). The computer extracts all the 
information it can from the utterance it recognized, and then it steps through 
a directed dialog to elicit information not extracted from the initial utterance. 
A VoiceXML mixed-initiative dialog contains a form-level grammar and an 
<initial> item. The <initial> item is syntactically and functionally like the 
first field item in the form. When the form is activated, the <initial> prompt 
is read and the computer then waits for a response that satisfies the form-level 
grammar. During the recognition process, the recognizer encounters tags 
embedded in the grammar. The grammar tags associate grammatical constructs 
with data items that the VoiceXML interpreter places in VoiceXML program vari-
ables. (For example, a grammar tag may set a variable named "place" when the 
grammar matches a placeName phrase.) Because the interpreter may encounter 
multiple grammar tags in the course ofrecognizing a single utterance, it is possi-
ble to set multiple form item variables in one complex utterance. 
An <initial> element has event handlers, event counters, and catch 
clauses like other form items, except that there is no <filled> handler. The form 
item variable for <initial> is either undefined or true. If the item variable is 
undefined, the PIA will consider the <initial> item "unfilled" and will revisit 
it until the variable becomes defined. The variable is set to true by the 
VoiceXML interpreter if any other form item variable is set as a result of 
processing grammar tags while recognizing a valid utterance. The item variable 
may also be explicitly set within the body of an event handler. 
119 

Chapter9 
120 
Once the <initial> form item has been "filled," other field items are 
processed as in plain directed dialogs. However, the FIA will not visit any field 
item whose variable has been set. Thus, form items whose variables were set by 
grammar tags are considered filled and will not be visited. In this way, a user 
may respond to the <initial> prompt with information that populates one or 
more form fields, and the computer will prompt for only those fields that were 
not populated. 
Executable Content 
Executable content is procedural code that can appear in <block> form items or 
event handlers. In regard to scripting capabilities, VoiceXML is layered closely on 
ECMAScript. (A VoiceXML interpreter is, of necessity, an ECMAScript interpreter 
as well.) The VoiceXML elements for data manipulation and control flow corre-
spond closely to underlying ECMAScript data structures and control constructs 
and generally follow ECMAScript semantic rules. 
The <var> element declares and optionally initializes a VoiceXML variable. 
Based on where the declaration appears, the variable is placed in one of the 
VoiceXML scopes: application, document, dialog, or (anonymous) (local to 
a block or event handler). Once declared, a variable may be used in VoiceXML or 
ECMAScript expressions interchangeably. Variables in containing scopes may be 
qualified with their scope: application. username or document. x. A declared but 
uninitialized variable has the value ECMAScript "undefined." The <assign> ele-
ment is used to assign the result of an ECMAScript expression to a variable. 
The FIA tests if a form item variable is set to "undefined" to determine if the 
field item is eligible for a visit. The <clear> element is used to reset one or more 
form item values to "undefined"2 and reinitialize the form item prompt and event 
counters. By default, the <clear> element resets all form item variables, effec-
tively resetting the form to its unfilled state. 
ECMAScript expressions appear in the cond and expr attributes of many 
VoiceXML elements. Any ECMAScript code may appear in the body of 
a <script> element. 
Conditional logic in VoiceXML is handled by the <if> element and its 
optional subelements, <else> and <elsei f>. There is no element for looping. 
The <goto> and <submit> elements are used to unconditionally transfer control 
to another location (URI). When either of these elements is executed, the docu-
ment identified by the URI is loaded. Execution begins at either the first form in the 
document or the form identified by the fragment in the URI (that is, 
2 ECMAScript provides no syntax to denote the value "undefined," so the ECMAScript 
equivalent of <clear namelist="variable"> is something inscrutable such as 
variable = void o;. 

VoiceXML Programming Guide 
if the URI ends with "#formName"). The main difference between the two is that 
<submit> allows a list of variables to be submitted through an HTTP POST or GET. 
The <return> element is used to stop processing of a dialog called as a sub-
dialog. Control returns to the point of invocation. The <return> element 
specifies either a result (an ECMAScript variable) to pass to the caller or an event 
to throw in the caller's scope. The <exit> element returns control the VoiceXML 
interpreter, terminating the VoiceXML application. 
Telephony 
The <transfer> element is a form item used to transfer a caller to another 
phone number. In a bridging transfer, the VoiceXML interpreter suspends while 
the call takes place and resumes when the call ends. In a blind transfer, the 
VoiceXML program terminates as soon as the call connects. 
The <disconnect> element disconnects the caller and throws an event. The 
event may be caught to perform cleanup before exiting. 
Platform and Performance Features 
VoiceXML offers a number of ways to set and configure parameters in the under-
lying voice platform. These features are discussed in Chapter 10. 
Summary 
This chapter provided reference-style coverage of all the major VoiceXML language 
elements. The VoiceXML specifications, available at http: I lwww. voicexml. orgl and 
http: I lwww. w3. orgiVoice, are the ultimate source of information on the VoiceXML 
language. The specifications are quite readable, and unlike many specifications, 
they contain numerous examples that explain the intent oflanguage features. 
121 

CHAPTER 10 
Advanced VoiceXML 
Topics 
THis CHAPTER EXAMINES some of the more advanced uses ofVoiceXML. These 
advanced topics begin to explore how a VoiceXML program is constructed and 
how it interacts with its host environment. The discussion here covers some of 
the issues involved in developing and deploying a real-world VoiceXML appli-
cation.lt sets the stage for an architecture-oriented discussion ofhowVoiceXML 
interacts with other components in a distributed system. 
Resource Fetching 
In VoiceXML, a resource is a VoiceXML document, an audio file, an object, 
a grammar, or a script. Table 10-llists the VoiceXML tags that initiate fetching 
of resource content from a URI and what kind of content each tag fetches. 
Table 10-1. Tags That Fetch Resources 
TAG 
CONTENT DESCRIPTION 
MIME TYPES 
<choice> 
VoiceXML document 
text/x-vxml 
<goto> 
<link> 
<subdialog> 
<submit> 
<audio> 
Audio recording 
audio/basic, audio/x-law-basic, 
audio/wav 
<dtmf> 
Grammar file 
application/x-jgsf, application/ 
<grammar> 
x-gsl, others 
<object> 
External object 
varies 
<script> 
ECMAScript source file 
application/x-javascript 
123 

Chapter 10 
All tags that fetch resources share attributes that control how fetching works. 
Default values for these attributes are set by VoiceXML interpreter properties. The 
"Using Properties" section of this chapter provides information about how to set 
properties. See Table 10-2 for a summary of the attributes and values available. 
Table 10-2. VoiceXML 1.0 Resource-Fetching Attributes 
ATTRIBUTE 
fetchtimeout 
fetch hint 
fetchaudio 
caching 
124 
MEANING 
Specifies how long to wait 
before throwing an error. 
badfetch event. Example 
values: "lOOms," "lOs." 
An optimization hint to the 
interpreter. prefetch suggests 
that the content may be down-
loaded before it is used. safe 
suggests that the content be 
downloaded on demand. stream 
suggests that the interpreter start 
processing the content when it is 
received, rather than waiting for 
the download to complete. 
The URI of an audio clip to play while 
the document is being fetched. This is 
a good way to keep the computer's con-
versational turn when a long delay may 
confuse the user. If attribute and property 
are not set, no clip is played. 
Specifies the caching policy for the con-
tent. The VoiceXML interpreter, like 
most Web browsers, is capable of caching 
content to avoid unnecessary downloads. 
Caching policy safe is conservative and 
should be used when content is volatile. 
Caching policy fast provides the best 
performance with static content. 
PROPERTY CONTAINING DEFAULT VALUE 
fetchtimeout 
audiofetchhint 
documentfetchhint 
grammarfetchhint 
objectfetchhint 
scriptfetchhint 
fetchaudio 
caching 

Advanced VoiceXML Topics 
Audit Trail 
It's common when you reach an automated voice response system to hear 
a prompt something like this: "Please listen carefully to the following options, 
because our menu options have changed. For good service, press 1. For mediocre 
service, press .... " The change in options is good news, because it means that 
the Interactive Voice Response Unit (IVRU) operators are monitoring actual sys-
tem usage and tuning the interface for best performance. The following are some 
tuning techniques that should be applied routinely: 
• Move the most frequently used options to the front of the list. 
• Restate prompts and/ or restructure dialogs that prove prone to errors. 
• Add/remove task-specific options as demand varies. (For example, add 
a special top-level option to request tax information during February, 
March, and April.) 
These maintenance operations are only possible if the VUI is gathering his-
torical data about its own operation. This type of self-monitoring is well known to 
Web site operators, who monitor clickstreams and server performance statistics 
for a variety of purposes, including managing and predicting load, inferring cus-
tomer preferences, triggering generation of dynamic content, and so on. 
The wrinkle introduced by VoiceXML is that maintaining a high-quality VUI 
requires detailed information about low-level conversational interactions. For 
example, a VUI designer can make good use of information such as "In form XYZ, 
the count=3 handler was invoked less than 10 percent of the time for fields 
a, b, and d, but 57 percent of the time for field c." These detailed interactions are 
scripted within VoiceXML and, from an architecture perspective, occur on the 
Web client side (and therefore do not generate the verbal equivalent of click-
streams). To transfer from one HTML page to another implies loading another 
document from a Web server, because only one HTML document is loaded at 
a time. However, a VoiceXML document can contain many dialogs, so transitions 
between dialogs do not necessarily imply round-trips to the server. 
VoiceXML does not provide any explicit, standard mechanism for generating 
the voicestream equivalent of the HTML clickstream. Individual platform ven-
dors may (and usually do) build in proprietary audit trail collection features. 
The other option available is to implement the VUI so that VUI events can be 
inferred from server visits. The basic technique is to use Web-based URis when-
ever fetchingVoiceXML resources. This forces frequent interaction with the Web 
server, which the Web server logs. The same tools used to massage clickstream 
data can be used to massage voicestream data. See Table 10-3 for some tech-
niques you can use to generate voicestream events. 
125 

Chapter 10 
Table 10-3. Techniques for Generating Voicestream Events 
TECHNIQUE 
CAPTURED EVENT 
Adopt a convention of 
Dialog transitions. 
coding one dialog per 
VoiceXML document. 
Use recorded prompts. 
Executing <audio> 
tag. 
Use vendor-provided 
Varies. 
logging facilities in the 
voice platform. 
Use external grammars 
Loading documents, 
and specify on-demand executing form items. 
retrieval. 
Use external scripts. 
Executing <script> 
tag. 
PRO 
CON 
Simple and provides 
Can't make use of 
useful information about 
document-level 
structuring dialogs. 
scoping. 
Provides fine-grained 
trace information. (For 
example, it can infer 
visits to individual 
event handlers.) 
Can work without 
modifying application. 
Can infer visits to 
individual fields (even 
if they don't have 
prerecorded prompts). 
Can use do-nothing 
scripts as trace points. 
Doesn't work when TTS 
synthesis is required. 
Audio data can be 
massive and cause 
slowdownloads. 
Proprietary. 
Makes VoiceXML code 
less readable, because 
expected responses 
are not visible inline. 
Makes VXML code less 
readable, because 
script is not visible 
inline. 
Accessing the Voice Gateway 
126 
The VoiceXML interpreter is part of a voice gateway, which includes speech 
recognition and speech synthesis technology. All commercial voice systems con-
tain more features than can be used through standard VoiceXML code.1 
1 As is to be expected with young standards driven by industry groups, to a certain extent the 
VoiceXML standard represents a "least common denominator" feature set that is agreeable 
to the participating vendors. 

Advanced VoiceXML Topics 
VoiceXML provides two principal means of getting at nonstandard features 
of the environment. VoiceXML properties are a mechanism to set configuration 
data in a standard fashion. The <object> tag is a mechanism to bridge the 
VoiceXML run-time environment and external services. 
Using Properties 
Platform properties are set using the <property> tag. Properties are scoped 
according to where the <property> specification appears. The platform uses the 
conventional hierarchical lookup from narrowest to broadest scope when deter-
mining what value of a property to use at a particular point in VoiceXML code. 
Curiously, VoiceXML properties may be set but not read. 
VoiceXML defines a standard set of properties that must be supported by all 
platforms. Table A-7 in Appendix A lists the standard properties. (For further infor-
mation, see the VoiceXML specification.) Several of the predefined properties have 
to do with tuning various time-out values that parameterize the voice recognizer. 
Others have to do with resource-fetching defaults and other platform characteris-
tics. Platforms may implement additional, nonstandard properties as well. 
In Listing 10-1, the first <property> tag sets the bargein property to false at 
document scope. Therefore, when the first prompt inside the form is executed, 
barge in is disabled. The default value for the property is inherited from docu-
ment scope, because the property is not set at dialog scope. The second 
<property> tag sets the bargein property to true, but this time at field (anony-
mous) scope. The prompt for the field is therefore read with barge in enabled, 
because the (anonymous) scope overrides document scope. 
Listing 10-1. Property Tags at Different Scopes 
<vxml> 
<property name="bargein" value="false"> 
<form> 
<!- -bargein is disabled here--> 
<prompt>You cannot stop me! <prompt> 
<field name="test"> 
<property name="bargein" value="true"> 
<grammar> ... </grammar> 
<! --bargein is enabled here--> 
<prompt> Go ahead and cut me off. <prompt> 
</field> 
</form> 
</vxml> 
127 

Chapter 10 
128 
The <object> Tag 
The <object> tag can be used invoke platform-specific functionality from 
within VoiceXML. It is modeled on the HTML <object> tag, which is used to 
include an externally defined object on a page. Syntactically, <object> is a close 
relative of <subdialog> for the following reasons: 
It is a form item that invokes other code and returns to the point 
of invocation. 
It takes parameters through the <par am> tag. 
It returns results through an ECMAScript object. 
The VoiceXML specification specifies the syntactic form of the <object> tag, 
but it does not specify any standard mappings to standard object invocation 
mechanisms such as DCOM, CORBA, or JavaBeans. This means that the set of 
extensions available for use in VoiceXML is defined solely by the platform vendor. 
As a developer, you can only write extensions to VoiceXML if your chosen plat-
form provides you a way to do it. 
Listing 10-2 shows an example of how the <object> tag might be used 
to invoke a (hypothetical) standard, platform-supplied dialog to elicit a person's 
social security number. 
Listing 10-2. The <object> Tag 
<form id="getSSN"> 
<object name="SSN" classid="builtin://elicitSSN" 
data=" .. /errorPrompts" /> 
<prompt> Please say your security number as a sequence 
of nine digits, or key in the nine digits on your 
keypad." </prompt> 
<param name= "requireResponse" expr=" 'false'" I> 
</object> 
<block> 
<prompt> Your social security number is: </prompt> 
<prompt><value expr="SSN. firstThree" /> dash </prompt> 
<prompt><value expr="SSN.middleTwo" I> dash </prompt> 
<prompt><value expr="SSN .lastFour" I> </prompt> 
</block> 
</form> 
In the example, the external object is identified by the classid attribute, 
which specifies a URI that is interpreted by the platform. The data attribute spec-

Advanced VoiceXML Topics 
ifies the URI of data used by the invoked object-in this case, the location of 
recorded error prompts that can be used during the elicitation. The person's 
nine-digit social security number is returned in ECMAScript variable SSN, already 
neatly parsed into attributes firstThree, middle Two, and lastFour. 
The <object> tag accepts several more attributes, including standard form 
item attributes, codebase, resource-fetching attributes, and others. See the 
VoiceXML specification for more syntactical details, or see the documentation 
for your voice platform for details of how to invoke specific functions. 
Advanced Event Handling 
Due to the fact that speech recognition and synthesis must work in real time, event 
handling is an essential and core part ofVoiceXML. To date, real-time applications 
have tended to be high-end software (as are voice platforms). Programming lan-
guages incorporate event handling in some form, but it's usually not in real time, 
and it's used by programmers. It will be interesting to see how VoiceXMI.;s foray into 
mass-market, markup language-based event handling will fare. 
The following sections delve into the details of how event handling works in 
the VoiceXML language and looks at some practical issues relating to the writing 
and maintenance of event -driven VoiceXML code. 
"As If by Copy" Semantics 
The VoiceXML specification contains the following paragraph (Section 11, Event 
Handling), which is easy to skip over: 
"An element inherits the catch elements ('as if by copy') from its ancestor 
elements, as needed. If a field, for example, does not contain a catch ele-
ment for nomatch, but its form does, the form's nomatch element is used. 
In this way, common event handling behavior can be specified at any 
level, and it applies to all descendants." 
While seemingly innocuous in its brevity, what this paragraph means is 
that the VoiceXML event handling model works quite differently from familiar, 
syntactically similar models in programming languages such as C++ and Java. 
Programming languages typically use the concept of nested lexical scope. Every 
data object or procedure is declared in a scope, which determines where the 
object can be referenced. A scope may contain other nested scopes, forming 
a hierarchy (see Figure 1 0-1). When an event is thrown in a nested scope, the 
"nearest" event handler in the hierarchy is invoked and the event propagates into 
the scope of the event handler. For example, if event e is thrown in Block D, the 
event handler in Block D is invoked. If event e is thrown in Block C (which has no 
129 

Chapter 10 
130 
event handler), the event propagates up the hierarchy to the event handler in the 
nearest containing scope-in this case, Procedure B. This model is called lexical 
scoping, because you can tell by reading the program in which scope an event 
handler is executed. 
Figure 10-1. Lexical scoping 
Program A 
catch( e ) _ 
Procedure B 
catch{ e ) _ 
Block C 
Block D 
catch( e ) _ 
In the VoiceXML model, event handlers are declared in one scope but may 
execute in any contained scope. In other words, an event handler declared in the 
application scope may be executed in the scope of a particular form. For exam-
ple, in Figure 10-2, when event e is thrown in Form C, the event handler from 
Document B is executed inC's scope. The behavior of the VoiceXML program is 
exactly the same as if the source code text of the <catch> element in Document 
B was copied into Form C. Thus, for example, any variable references in the event 
handler code would be resolved in Form C, not Document B. 
Application A 
copy 
Application A 
<catch event•"e"> aaa ------.. <catch event•"e"> aaa 
Document 8 
Document 8 
copy 
<catch event•"e"> bbb ~ 
<catch event•"e"> bbb 
Form C 
copy"-.._ Form C 
<catch evcnt•"e"'> bbb 
Form 0 
Form 0 
<catch event• "e"> ddd --- -+ <catch event•"e"> ddd 
copy 
Declaration 
Execution 
Figure 10-2. VoiceXML 'tis if by copy" scoping 

Advanced VoiceXML Topics 
''As if by copy" semantics makes it possible to define a single event handler 
that will handle all events of a certain type by declaring the event handler in 
application scope and nowhere else. The effect will be the same as if the one 
<catch> element were copied everyplace in the application where the event 
could be thrown. This seems to be what the specification means by "common 
event handling behavior." For example, Listing 10-3 and Listing 10-4 show 
a VoiceXML application that defines a complex group of interrelated event han-
dlers and a simple form that "uses" them. 
Listing 10-3. 'fls If by Copy" Sample Application 
<?xml version="l.O"?> 
<vxml version="l.O"> 
<var name= "tersePrompt" expr=" 'The terse prompt"' I> 
<var name="helpPromptl" expr="' First help prompt'" I> 
<var name="helpPrompt2" expr=" 'Second help prompt'" I> 
<nomatch count=" 1" > 
<prompt>I didn't get that.</prompt> 
</nomatch> 
<noinput count="l"> 
<prompt> 
<value expr="tersePrompt" I> 
</prompt> 
</noinput> 
<catch event="nomatch noinput" count="2"> 
<throw event="help" /> 
</catch> 
<help count="l"> 
<prompt> 
<value expr="helpPromptl" /> 
</prompt> 
<reprompt /> 
</help> 
<help count="2"> 
<prompt> 
<value expr="helpPrompt2" /> 
</prompt> 
<reprompt I> 
</help> 
<help count="3"> 
<throw event="nomatch" I> 
</help> 
</vxml> 
<?xml version="l.O"?> 
131 

Chapter 10 
132 
Listing 10-4. 'lis If by Copy" Sample Form 
<vxml version="1.0" 
application="file:/1/H:/VoiceXML/SPIMApp/Tutorial/SmallExamples/ 
AsifByCopyApplication. vxml" > 
<form> 
<var name="mlname" expr=" 'VXML'" I> 
<field name="hello"> 
<grammar>hello< I grammar> 
<prompt>Please say hello. </prompt> 
</field> 
<field name="goodbye"> 
<grammar>good-bye</grammar> 
<prompt>Please say good-bye. </prompt> 
</field> 
<block>Good-bye to you! </block> 
</form> 
</vxml> 
When executing the form, Dialog 10-1 is valid. 
Dialog 10-l.Application-wide Event Handling 
C (computer): Please say hello. 
P (person): Help. 
C: First help prompt. Please say hello. 
P: Help. 
C: Second help prompt. Please say hello. 
P: Hello. 
C: Please say good-bye. 
P: Help. 
C: First help prompt. Please say good-bye. 
P: Help. 
C: 
Second help prompt. Please say good-bye. 
P: 
Good-bye. 
C: 
Good-bye to you! 
Note that the same sequence of event handlers is invoked for both form 
fields, and that each field maintains its own event counters. 
The bad news is that, in practice, event handlers associated with a field item 
usually perform field-specific prompting. In other words, instead of a generic 
"First help prompt," the prompt should say, "It's polite to say hello" for the first 

Advanced VoiceXML Topics 
field and "Just say good-bye" for the second field. The "as if by copy" semantics 
hold out the promise that you could assign local variables to have the prompt 
strings defined on a field-by-field basis. However, <field> elements can't contain 
executable content, so you can't declare or assign a variable on a field-by-field 
basis. Therefore, there is no way to parameterize a single event handler so it works 
differently for different field items. The only option available is to use the 
<reprompt/> element in the common event handler, as shown in the example. 
Techniques for Event Handling 
As you saw in earlier examples, VoiceXML provides powerful but verbose event 
handling features. With these features, it is possible to construct VUis that pro-
vide good prompt tapering and dialog structuring. However, the power comes at 
the cost of readability: If you look at Listing 10-3, it's hard to see the actual field 
prompting for the forest of event handlers. Furthermore, good design principles 
dictate that event handling should be consistent across fields and forms. 
Subtleties of Subdialogs 
It would be desirable to be able to factor out the event handling code from indi-
vidual form fields and define it once. This would improve readability, improve 
software maintainability, and aid in consistency. A simplistic first approach is to 
look at what data changes between various uses of the event handlers and then 
create a subdialog with parameters for data specific to each use. In other words, 
the idea is to replace the field item shown in Listing 10-5 with a subdialog invo-
cation like that in Listing 10-6. 
Listing 10-5. Field with Verbose Event Handling 
<field name="appointee" > 
<grammar src=" /Step7 /SimpleAppointment. gram#appointee" !> 
<prompt count="l">Who are you meeting with?</prompt> 
<prompt count="2">Name?</prompt> 
<nomatch count=" 1 ">I didn't get that. <reprompt/></nomatch> 
<noinput count="1"> 
<throw event="help" !> 
</noinput> 
<catch event="nomatch noinput" count="2"> 
<throw event="help" /> 
</catch> 
<help count="1"> 
<prompt>Say the name of a person. </prompt> 
133 

Chapter 10 
134 
</help> 
<help count="2"> 
<prompt> 
You are adding an appointment. 
Say the name of the person you are making an appointment with. 
</prompt> 
</help> 
</field> 
Listing 10-6. Static Subdialog Invocation 
<subdialog name=" appointeeHandler" src=" staticFieldHandler" > 
<param name="longPrompt" expr=" 'Who are you meeting with?"' I> 
<param name="tersePrompt" expr=" 'With whom?'" I> 
<param name="helpPromptl" expr="'Say the name of a person."' I> 
<param name="helpPrompt2" expr="helpPromptl" /> 
<param name="theGrammar" value="' SimpleAppointment. gram#appointee'" I> 
<filled> 
<assign name="appointee" expr="appointeeHandler. value" /> 
</filled> 
</subdialog> 
The parameters to the subdialog are the information specific to each field. 
The implementation of the dialog would be a form with a single field like that 
in Listing 10-7. Unfortunately, when you try to actually code the subdialog, 
there is a roadblock because VoiceXML doesn't allow the grammar to be treated 
as a parameter passed to a <subdialog>. The value of the src attribute of 
a <grammar> tag must be a URI (and it can't be an expression that yields a URI). 
One possible approach is to turn off caching and point the <grammar> src 
attribute to a URI that dynamically generates the grammar. This approach is 
problematic because there is no way to inform the server of which field the 
grammar is for. 
Listing 10-7. Hypothetical Reusable Static Subdialog 
<form id=" staticFieldHandler" > 
<var name="theGrammar" /> 
<var name="tersePrompt" /> 
<var name="longPrompt" /> 
<var name="helpPromptl" /> 
<var name="helpPrompt2" /> 
<field name="theValue"> 

Advanced VoiceXML Topics 
<!-- To achieve the effect of a reusable field-handling component, 
the grammar specified in the following line would need to contain 
the specific responses valid for the field being processed. 
However, there is no way in VoiceXML to parameterize the 
grammar being used. -- > 
<grammar src="????" I> 
<prompt> 
<value expr="longPrompt" I> 
</prompt> 
</form> 
The solution is to dynamically generate the form with the grammar name 
(or the grammar itself) embedded in it. The grammar is identified in the 
<subdialog> invocation by a variable that is sent to the server. The modified 
invocation is shown in Listing 10-8, and the full text of the generated form is 
shown in Listing 10-9. 
Listing 10-8. Invoking a Generated Subdialogfor Event Handling 
<var name="thisFieldsGrammar" 
value="' myGrammars/ /Step7 /SimpleAppointment. gram#appointee' "> 
<subdialog name="appointeeHandler" src="/cgi-bin/generateFieldHandler" 
namelist="thisFieldsGrammar"> 
<param name="longPrompt" expr=" 'Who are you meeting with?'" I> 
<param name="tersePrompt" expr=" 'With whom?"' I> 
<param name="helpPromptl" expr=" 'Say the name of a person.'" I> 
<param name=" helpPrompt2" expr=" helpPromptl" I> 
<param name= "theGrammar" value="' SimpleAppointment. gram#appointee'" I> 
<filled> 
<assign name="appointee" expr="appointeeHandler.value" /> 
</filled> 
</subdialog> 
Listing 10-9. Dynamically Generated Reusable Subdialog 
<form id="dynamicFieldHandler" > 
<var name="theGrammar" I> 
<var name="tersePrompt" /> 
<var name="longPrompt" /> 
<var name="helpPrompt1" I> 
<var name="helpPrompt2" I> 
<field name="theValue"> 
135 

Chapter 10 
136 
<!-- The server generates the following reference to an external grammar 
based on the namelist variable passed in the subdialog call. --> 
<grammar src=" /Step? /SimpleAppointment. gram#appointee" 
type="application/x-jgsf" I> 
<prompt> 
<value expr="longPrompt" I> 
</prompt> 
<nomatch count="l"> 
<prompt> I didn't get that. </prompt> 
</nomatch> 
<noinput count="l"> 
<prompt> 
<value expr="tersePrompt" /> 
</prompt> 
</noinput> 
<catch event="nomatch noinput" count="2"> 
<throw event="help" I> 
</catch> 
<help count="l"> 
<prompt> 
<value expr="helpPromptl" /> 
</prompt> 
</help> 
<help count="2"> 
<prompt> 
<value expr="helpPrompt2" /> 
</prompt> 
</help> 
<help count="3 "> 
<throw event="nomatch" I> 
</help> 
</field> 
</form> 
Style Sheet As Macro 
Because VoiceXML is XML, you can bring some of the transformational capabili-
ties ofXML to bear on VoiceXML source code. Style sheets are a powerful 
technology for transforming one XML representation into another. Normally, 
style sheets come into play when dynamically generating presentation code (for 
example, HTML orVoiceXML) from anXML document. When using style sheets 

Advanced VoiceXML Topics 
on source code, you're not generating code dynamically at run time-you're actu-
ally generating static code from handwritten code. 
Listing 10-10, Listing 10-11, and Listing 10-12 show the transformation from 
an "XML-ized" version of the subdialog invocation to static VoiceXML using a 
style sheet (XSL). (This is a fragmentary example, and it does not produce a full 
VoiceXML document.) 
Listing 10-10 shows a code fragment that uses the <reusableField> tag. This 
tag is not a VoiceXML tag, but it is used to generate a standard pattern of fre-
quently used VoiceXML code. Listing 10-9 is the XSL style sheet that transforms 
<reusableField> elements into VoiceXML code. You'll look more closely at style 
sheets in later chapters. Without going into much detail, the trick to reading XSL 
style sheets is knowing that tags in the "xsl" names pace (that is, those that start 
with xsl:) are XSL processor instructions that will be replaced during transfor-
mation. Other tags are passed through to the output document. Listing 10-10 
shows the result of the transformation. 
Listing 10-10. Source Code with Embedded <reusableField> Tags 
<?xml version="l.O"?> 
<reusableField name="appointee"> 
<inlineGrammar>alpha I bravo I charlie<linlineGrammar> 
<longPrompt value="Who are you meeting with?" I> 
<tersePrompt value="Wi th whom?" I> 
<helpPromptl value=" Say the name of a person. "I> 
<helpPrompt2 value=" Say the name of a person. "I> 
<lreusableField> 
In real life, this transformation would be performed at the time that the Web 
site is published into Web format (that is, deployment time). Notice that the input 
source code is concise, readable, and maintainable, but the style sheet is probably 
mysterious unless you work with style sheets a lot. This points to some tradeoffs 
here, based on experiences with using macro-based programming languages: 
• Used sparingly and well, macros can help make source code more main-
tainable and consistent. 
• Macros are themselves code that must be maintained by someone with 
specialized knowledge (in this case, knowledge of XSLT). 
• Overuse of macros can lead to dense, unmaintainable code and can 
require a significant amount of training to use. 
137 

Chapter 10 
138 
Listing 10-11. XSL Style Sheet That Expands <reusableField> Tags 
<?xml version="l.O" encoding="UTF-8"7> 
<xsl: stylesheet version="l. o" xmlns: xsl="http: I /www. w3. org/1999/XSL/Transform" > 
<xsl: output method="xml" version="l. 0" encoding="UTF-8" indent="yes" I> 
<xsl:template match="/"> 
<xsl: apply-templates select=" I /reusableField" I> 
</xsl: template> 
<xsl: template match=" I /reusableField" > 
<field name="{@name}"> 
<grammar><xsl: value-of select=" inlineGrammar" /></grammar> 
<prompt> 
<xsl:value-of select="longPrompt" I> 
</prompt> 
<nomatch count="l"> 
<prompt>! didn't get that.</prompt> 
</nomatch> 
<noinput count="l"> 
<prompt> 
<xsl: value-of select="tersePrompt/@value" /> 
</prompt> 
</noinput> 
<catch event="nomatch noinput" count="2"> 
<throw event="help" I> 
</catch> 
<help count="l"> 
<prompt> 
<xsl :value-of select= "helpPromptl/@value" I> 
</prompt> 
</help> 
<help count="2"> 
<prompt> 
<xsl: value-of select="helpPrompt2/@value" I> 
</prompt> 
</help> 
<help count="3"> 
<throw event="nomatch" I> 
</help> 
<!field> 
< /xsl: template> 
</xsl: stylesheet> 

Advanced VoiceXML Topics 
Listing 10-12. Generated VXML Source Code (with Macros Expanded) 
<?xml version="l.O" encoding="UTF-8"?> 
<field name=" appointee"> 
<grammar>alpha I bravo I charlie</grammar> 
<prompt/> 
<nomatch count="l"> 
<prompt> I didn't get that. </prompt> 
</nomatch> 
<noinput count="l"> 
<prompt>With whom?</prompt> 
</noinput> 
<catch event="nomatch noinput" count="2"> 
<throw event="help" /> 
</catch> 
<help count="1"> 
<prompt>Say the name of a person. </prompt> 
</help> 
<help count="2"> 
<prompt>Say the name of a person. </prompt> 
</help> 
<help count="3"> 
<throw event="nomatch" I> 
</help> 
</field> 
Summary 
In this chapter, you've looked at some advanced aspects ofVoiceXML that are 
important in practice. These aspects become important when you broaden the 
scope from just programming the VoiceXML language to producing, deploying, 
and maintaining VoiceXML applications in the real world. Some features are 
already in the language (for example, resource caching, properties, and 
<object> tags) and will become "seasoned" as real VoiceXML applications 
mature. Some missing features (for example, audit trails and reusable dialogs) 
will be addressed in future releases of the language. Software engineering tech-
niques and processes, such as the use ofXSL macros for code reuse, will evolve 
as more and larger VoiceXML applications are developed. 
139 

Part 3 
Incorporating Voice into 
the Web 
IN THE PRECEDING PARTS, the focus has been on how to use VoiceXML to create voice 
interfaces. In this part, the camera dollies back for a look at the bigger picture of 
the overall architecture ofWeb applications incorporating GUis, WUis, and VUis. 
In all likelihood, people will not build stand-alone Web applications for voice 
only. It's more likely that voice interfaces will be retrofitted to existing HTML-
based Web sites, or that new Web sites will be built to incorporate multiple access 
modes from inception. Any successful Web application involves the interaction 
of many technologies, and to a large extent, VoiceXML will succeed or fail based 
on how well it cooperates with other technologies. 
Chapter 11 provides a quick, general review of some the key technologies 
thatVoiceXML will be used with. Chapter 12 explores the architectural issues that 
arise when incorporating voice into applications along with wireless and stan-
dard Web interfaces. Chapter 13 presents a prototype that pulls together the 
various technologies and demonstrates a transformational architecture. Detailed 
instructions are provided for installing the prototype from the companion CD, 
and the various components are dissected. Finally, Chapter 14 explores future 
directions for VoiceXML. 

CHAPTER 11 
Overview of Related Web 
Technologies 
THis CHAPTER REVIEws some technologies that interact to form a distributed Web 
application. The following technology briefings are intended as refreshers rather 
than primers. So many technologies are involved in a distributed Web application 
that no one can be master of them all. If you are already familiar with the tech-
nologies, feel free to skip any or all sections; if the concepts are brand new to you, 
check out some of the reference links for introductory material. 
XML 
A markup language is a language that tags the contents of a document with infor-
mation about how to interpret the content. Markup is generally divided into two 
categories: pure content and presentation. Pure content markup annotates the 
content based on its meaning, without regard for how the content will be pub-
lished or presented. Presentation markup interweaves instructions and hints 
about how to present the content in a particular medium. 
Extensible Markup Language (XML) is a standardized syntax for a family of 
markup languages. A language in the family is defined by a document type defi-
nition (DTD) or an XML Schema. WML and VoiceXML are XML languages. HTML, 
although derived from SGML, does not comply completely with XML require-
ments, but its close cousin XHTML does. In addition, XML is being used to define 
the format of data interchanged between businesses, organizations, and com-
puter systems. 
Significant Features 
• Text-based. 
• Easily exchanged between platforms. 
• Can be read by humans and edited in any text editor. 
143 

Chapter 11 
144 
• Syntax is rigorously designed for machine parsing. 
• New "dialects" can be defined without programming. 
How It Is Used in Voice Enabling Web Sites 
XML is used to express information from Web applications in a presentation-
neutral format that can be shared among the XHTML, WML, and VoiceXML 
user interfaces. 
Technical Overview 
An XML document contains elements, tags, attributes, and free-form textual 
content. Elements have optional and required attributes. Elements can contain 
content and subelements. An element is a "noun" that identifies how to interpret 
its content. Attributes are "adjectives" that modify the element. Attribute values 
are always quoted. Elements are delimited by tags that appear in angle brackets.1 
An element can be delimited with a start tag and an end tag (that begins with/), 
or a single, bodiless tag that starts and ends in the same tag. The following exam-
ple shows an element that contains content and two nested elements: 
<myXMLElement name="Fred" extra="more stuff"> 
this is free form content 
<myXMLSubElement> 
nested elements may also have content 
</myXMLSubElement> 
<bodylessTag extra="information"/> 
</myXMLElement> 
XML elements must nest hierarchically. In each XML document, a single root 
element contains all the other elements in the document. Therefore, an XML 
document can be thought of as a tree of branching nodes, where each node cor-
responds to a tag in the document. 
A DTD or XML Schema defines the set of elements and attributes available in 
a particular XML language. The schema defines what order elements may appear 
1 To be hair-splittingly precise, elements are structural components of an XML language that 
are declared in the schema or DTD. Tags are the syntactic markers that are placed in XML 
document instances to delimit elements. In practice, however, the two terms are often used 
interchangeably, and I confess that I have not been rigorous in distinguishing the two 
throughout this book. 

Overview of Related Web Technologies 
in, how the elements nest, and whether elements are optional or required. For 
each element, the schema defines required and optional attributes. The following 
XML Schema fragment could be the schema for the previous example: 
<xsd: element name= "myXMLElement" > 
<xsd:attribute name="name" type="xsd:string" use="required"l> 
<xsd: attribute name="extra" type="xsd: string" use="optional" I> 
<xsd: sequence> 
<xsd:element name="myXMLSubElement" type="xsd: string" I> 
<xsd: element name="bodylessTag" type="xsd: string"> 
<xsd: attribute name=" extra" type="xsd: string" use="required" I> 
<lxsd: element> 
<xsd: sequence> 
<lxsd: element> 
An XML document is well formed if it can be parsed into a syntax tree repre-
sentation by any XML parser. A document may have an associated schema. An 
XML document is valid if it conforms to its schema-that is, it is syntactically 
correct XML and it contains elements and attributes in the right relationships 
and order, as defined in the schema. 
Where to Learn More 
A wealth of information about XML is available on the Web. A good starting 
point is XML.org (http: I lwww. xml. orgl). XML Bible, by Elliotte Rusty Harold 
(http: I /www. ibiblio.orglxml!bookslbible), is a comprehensive guide to XML. 
XSL 
XML Stylesheet Language (XSL) is an XML language that is used to transform 
a document from one XML dialect to another. The term "stylesheet" captures 
the original intent of:XSL, which was to insert presentation markup into XML 
documents that provided content in a neutral format. A stylesheet written in 
XSL contains text templates and rules for how to merge the template text with 
content from an XML document. In practice, XSL is very powerful and has many 
more uses than just defining presentation styles. 
145 

Chapter 11 
146 
Significant Features 
• XSL is itself an XML language. 
• Allows very powerful "tree-to-tree" transformations of documents. 
• Can be read by humans and edited in any text editor. 
• Stylesheets can be modified without changing application code. 
• Declarative rather than procedural. 
• Template-based. 
How It Is Used in Voice Enabling Web Sites 
XSLT is used to transform information from a presentation-neutral XML to 
XHTML, WML, and VoiceXML markup languages. 
Technical Overview 
XSL incorporates two distinct sublanguages: a formatting layout language and 
a template transformation language. The formatting layout language is concerned 
with placing text on a two-dimensional page (therefore, it isn't particularly rele-
vant to VoiceXML, so I'll skip that part). The template transformation sublanguage 
(XSLT) is a general-purpose declarative language for rewriting documents. 
An XSL document contains template rule definitions, XSLT directives for 
finding and inserting content from the input XML document, and uninterpreted 
text. To transform a document, the XSLT processor is invoked with an XSL 
document and an XML document as input. The XSLT processor parses the 
XML document and then executes the XSLT document. When the XSLT processor 
encounters text that is not an XSLT directive, it copies the text to the output file. 
When it encounters anXSLT directive, it processes it (see Figure ll-1). 

Overview of Related Web Technologies 
XML 
Stylesheet 
XSL 
Processor 
Figure 11-1. XSL transformation processing 
XML 
OUtp<Jt Docuroent 
XSL directives make use ofXPath expressions. XPath is a complex syntax that 
is capable of designating any node, or set of nodes, in the parsed XML document. 
An absolute XPath expression might designate "the third node with element type 
myXMLElement" or "all myXMLElement nodes in the document with extra attrib-
ute value 'nothing'." A relative XPath expression refers to nodes relative to the node 
currently being processed-for example, "the last bodilessTag element descended 
from the current node" or "the node after the parent of the current node." 
The workhorse XSL construct is the template ( <xsl: template> element). 
A template has a pattern that specifies when the template is invoked. The pattern 
is expressed as a restricted XPath expression that is applied to a candidate node. 
If the pattern matches, the template is invoked with the candidate node as the 
current node. Output generated by the template is inserted in the output stream. 
The output may include raw text from the XSLT document and tag or content 
data from the input XML document. 
Candidate nodes are generated by the <xsl: apply-templates> element. It 
specifies an XPath expression that generates zero, one, or many candidate nodes, 
which are then matched against all active templates. 
Where to Learn More 
Once again, a good starting point is XML.org (http: I /www. xml. or g). Excerpts 
about XSL and XPath from XML Bible, by Elliotte Rusty Harold, are online at 
http://www.ibiblio.org/xml/books/bible/updates/14.htmland 
http://www.ibiblio.org/xml/books/bible/updates/17.html. 
147 

Chapter 11 
148 
Servlet 
A servlet is a computer program that runs on the Web server. The servlet acts as 
a surrogate for a document. Instead of serving a file from the file system, the Web 
server invokes the servlet, which dynamically generates the text (or other con-
tent) for the document. 
Significant Features 
• Flexible and powerful-if you can code it, a servlet can do it. 
• Requires programming. 
• Designed to optimize multithreaded, multiuser usage. 
• Awkward to maintain static text inside program source code. 
• Template-based. 
How It Is Used in Voice Enabling Web Sites 
Servlets encapsulate code that can retrieve information from a variety of 
different sources, and then they generate a presentation-neutral XML version 
of that information. 
Technical Overview 
When a Web client requests a document from a URI that has been configured as 
a servlet address, the Web server invokes the servlet, passes it the HTTP request, 
and routes the output from the servlet back to the Web client. The servlet 
processes the request and writes text output to the output stream. Figure ll-2 
illustrates the servlet processing model. 

Overview of Related Web Technologies 
Web 
Server 
Figure 11-2. Servlet processing model 
Servlet 
Container 
Servlets are components that are hosted in a servlet container. The servlet 
container is responsible for managing the lifecycle of servlet components and the 
resources they use. Typically, the servlet container is part of the Web server, but it 
may also be a separate add-on component. 
Although servlets are commonly associated with dynamically generated 
HTML, the basic servlet model is a language-independent request/response pro-
tocol. Servlets can certainly be used to generate any XML language or any text 
output in general. The Java Servlets 2.2 specification (currently in final draft) has 
added the concept of ftlters, which can be used to incorporate stylesheet trans-
formations into the servlet processing model. 
Where to Learn More 
Visit Sun's Java Servlet Technology site (http: I /java. sun. com/products/servletl} 
for everything you need to know about servlets. 
JavaServer Pages 
A JavaServer Page (JSP) is a text document that contains interspersed static text, 
markup language, and programming language scripts. Conceptually, when a JSP 
is served, the page is processed as follows: 
1. 
Static text is copied into the output stream. 
2. 
JSP markup language directives are processed by the server. 
149 

Chapter 11 
150 
3. 
Programming scripts are executed and their results are copied into the 
output stream. 
The significance of these features is that they enable UI designers to edit 
markup tags and static content as text, without requiring programming inter-
vention. Contrast this with servlets, where the markup and static text is 
embedded in Java code, which means that content changes require Java pro-
grammers to make program changes. 
Significant Features 
• Static text is maintained and edited as text in a file. 
• Scripting requires programming. 
• Custom tags can be defined to insulate JSP authors from script programming. 
• Compatible with XML technologies. 
• Procedural. 
How It Is Used in Voice Enabling Web Sites 
JSPs can be used to dynamically generate VoiceXML documents in the same way 
that they are used to generate HTML documents. The difference is that the static 
text embedded in the JSP consists ofVoiceXML tags rather than HTML tags. 
Technical Overview 
Technically, a JSP is a species of servlet. When a Web client requests a JSP, the Web 
server invokes the "master" JSP servlet, passes it the HTTP request, and routes 
the output from the servlet back to the Web client. If the JSP has not been served 
recently, the master servlet compiles the JSP source code into Java source code, 
invokes the Java compiler to compile the resulting servlet, loads the servlet, and 
delegates the request to the servlet as described previously. If the JSP's compan-
ion servlet is already compiled, the Web server delegates requests to it just like 
any other servlet. Figure 11-3 illustrates this process. 

Overview of Related Web Technologies 
Client 
Web 
Server 
laponse 
Figure 11-3. JSP processing model 
App 
Server 
Data Sources 
The JSP specification defines a core set ofXML elements that the JSP com-
piler recognizes. The set of markup tags available can be extended by importing 
a custom tag library. A custom tag library associates JSP markup elements with 
implementations written in Java. This enables JSP authors to invoke standard 
functions simply by coding XML markup tags into the JSP, rather than having to 
code the functions using scripting in Java. 
The JSP processing model does not depend on Java as the scripting language 
(although the JSP specification does require Java to be supported as one of possi-
bly many scripting languages). The concept of scripting can be generalized to 
incorporate other scripting languages, such as ECMAScript, Tel, Perl, and so on, 
into JSPs. 
NOTE 
The scripting language required by JSP is Java (the programming 
language). The scripting language used in VoiceXML is ECMAScript, aka 
JavaScript. JavaScript and Java are not the same, and they are in many 
ways as different as they are alike. 
Where to Learn More 
Visit Sun's JavaServer Pages site (http: I /java. sun. com/products/j sp/) for every-
thing you need to know about JavaServer Pages. 
151 

Chapter 11 
152 
XML Publishing 
XML publishing is an infrastructure technology for the generation, publication, and 
processing of XML documents. All information is represented by documents, 
and the processing of information is achieved by repeated transformation of doc-
uments using stylesheets. Apache Cocoon is representative of this technology. 
Cocoon is an open-source project sponsored by Apache. Apache has anum-
ber ofXML-related projects underway. Cocoon closely integrates the servlet 
model of server-side processing with XML document processing. Cocoon pro-
cessing is based on the concept of producing an XML document (pure content) 
in response to an HTTP request and then successively transforming the con-
tent markup into presentation markup using XSL transformations. 
Cocoon is freely available and is easy to use with Web applications. However, 
because Cocoon is not a commercial product, it may not be appropriate for pro-
duction applications. Production applications will likely use a commercial 
product that implements the same technology. 
Significant Features 
• All server-side processing works on XML documents. As a final step, a for-
matter may generate XML, HTML, or text for delivery to the client. 
• Incorporates Java language scripting. 
• While being processed, content is manipulated as parsed XML nodes, 
which avoids unnecessary reparsing. 
• The Cocoon system is implemented as a J2EE servlet that will run in any 
J2EE-compliant servlet container. 
• XSL transformation model combines template data with dynamic content. 
How It Is Used in Voice Enabling Web Sites 
XML publishing is used to implement a single application having multiple user 
interfaces. Information is obtained from business applications in XML format 
and then transformed into a presentation language appropriate to the user's 
means of access (Web browser, WAP phone, or voice phone call). 

Overview of Related Web Technologies 
Technical Overview 
When a Web client requests an XML document (file type .xml), the Web server 
invokes the Cocoon servlet, passes it the HTTP request, and routes the output 
from Cocoon back to the Web client. The Cocoon servlet orchestrates a series of 
transformations (see Figure 11-4). The first step is to invoke a producer, which 
materializes an XML document. A producer may be as simple as a Cocoon-
provided program that reads and parses a static XML ftle. More typically, a pro-
ducer is a program written in XML Servlet Processor (XSP), an Apache-defined 
language that intersperses template data with JavaScript. If the XSP producer has 
not been served recently, the Cocoon servlet compiles the XSP source code into 
Java source code, invokes the Java compiler to compile the resulting servlet, loads 
the servlet, and invokes the producer. The producer creates a parsed XML docu-
ment. The parsed document is pipelined to zero or more transformers, which 
perform XSL transformations on the document. The resulting presentation 
markup is returned to the client. 
Client 
Figure 11-4. Cocoon processing model 
App 
Server 
Data Sources 
Point by point, there are strong congruencies between the J2EE servlet/JSP 
processing model and the Cocoon model (see Table 11-1). The conceptually sub-
tle, but very significant, difference between them is that the servlet/JSP model is 
based on processing text streams, while the Cocoon model is based on processing 
parsed XML documents. Because XML is far richer than plain text, this distinction 
makes Cocoon an elegant and powerful tool for serving XML documents. 
153 

Chapter 11 
154 
Table 11-1. Servleti]SP Features versus Cocoon Features 
FEATURE 
Dynamic content 
generation 
Processing model 
Content 
transformation 
SERVLET/JSP 
JSP language intersperses 
embedded tags, static content, 
and Java scripting; dynamically 
compiled into Java code 
Pipeline of chained servlets 
operating on a text stream 
Procedural, through JSP 
language 
COCOON 
XSP language intersperses 
embedded tags, static 
content, and Java scripting; 
dynamically compiled 
into Java code 
Pipeline ofXSL trans-
formations performed 
on a parsed XML document 
Declarative, through XSL 
stylesheets 
There are a number of interrelated Apache XML projects underway, which 
brings quite an alphabet soup of technologies to bear in the Cocoon environment. 
• XSP: A template language with Java scripting that is used to create XML 
documents dynamically 
• ESQL: A set ofXML tags that performs standard SQL queries and returns 
results as XML 
• Xerces: An XML parser 
• Xalan: An XSLT processor 
Where to Learn More 
Visit the Apache XML Project's site (http: I /xml. apache. org/) for everything you 
need to know about Cocoon and other Apache XML-related technologies. 

Overview of Related Web Technologies 
Summary 
In a real Web application, VoiceXML is just one of many powerful technologies 
that must interact and cooperate to do what the user wants. This chapter pro-
vided quick summaries of some selected fundamental technologies that are likely 
to be used in association with VoiceXML: XML, XSL, servlets, JavaServer Pages, 
and XML publishing. The next chapter looks at the overall architecture ofWeb 
applications and shows how these individual technologies work together. Not 
coincidentally, several of these technologies also appear in the Web prototype 
that will be dissected later in the book. 
155 

CHAPTER 12 
Adding VoiceXML to Web 
Applications 
Tms CHAPTER EXPLORES some of the "programming in the large" issues that arise 
when designing large, new, voice-enabled Web applications or when retrofitting 
existing applications with voice. Some of the issues arise because of the special 
requirements ofVoiceXML and voice; some of the issues are simply scaling issues 
that result from supporting another access mode, regardless of how it works. 
One Application, Multiple User Interfaces 
In the early days of the Web, it was clear what Web applications were about: deliv-
ering HTML to Web browsers. With the advent of wireless devices, it was necessary 
to shoehorn WML into the application. Because WML can be treated as a kind of 
"small scale" HTML, this presented tactical but not conceptual obstacles. Voice 
represents a fundamentally different access mode, and integrating VoiceXML into 
Web applications requires rethinking how the application is structured. 
Rather than functioning as a monolithic control point for scripting inter-
actions with a user, the Web application is evolving to resemble more of 
a "content clearinghouse" (see Figure 12-l) that collects content from a variety 
of sources and matches it to people in a variety of access modes. The process of 
matching content to a requesting user may involve format transformations, fll-
tering, and generation of presentation code. 
Interface 
Application 
Content 
Figure 12-1. Application as clearinghouse 
157 

Chapter 12 
158 
The Transformational Processing Model 
The transformational processing model is illustrated in Figure 12-2. The arrows 
show how information flows through the Web application and is transformed 
along the way. The basic phases of transformation are as follows: 
• Produce: Retrieve data from wherever it resides and transform it into 
a "presentation-neutral" format suitable for processing in the application. 
• Process: Provide services of value to the end user by modifying the infor-
mation and/ or executing transactions based on information from the user 
and from data sources. In business applications, this is where business 
logic rules are enforced. 
• Format: Filter and format the information content and structure so that it 
is suitable for rendering in a particular medium. 
• Render: Interact with the user in the chosen interface medium (for exam-
ple, speech or GUI). 
Render 
FoiJUt 
Process 
Produce 
Figure 12-2. Transformational processing model 
As Figure 12-2 implies, the goal is to work with multiple interaction media 
and multiple data sources, but implement the core processing only once. This is 
achieved by keeping each phase of the transformation separate and keeping pre-
sentation, processing, and persistent representation completely independent. 

Adding VoiceXML to Web Applications 
Special Needs of VXML 
In terms of general system layout, VoiceXML and HTML work in the generic Web 
infrastructure. VoiceXML imposes some special twists described in the sections 
that follow. 
Starting a Session 
From a Web browser, a session is initiated by visiting the home page of the appli-
cation, which is identified by a URL (for example, http: I /www. voicexml. orgl). 
When initiating the session, the Web server may use cookies to identify the user 
or may require a user ID/password login sequence. Cookies are a reliable way to 
recognize a user, because they are stored locally on the computer that is running 
the Web browser. The duration of a session for an HTML browser is determined 
by the Web application (often by assigning inactivity timeouts to server-side 
session objects). Applications typically store cookies that contain session identi-
fiers on user's PCs. A session may persist across multiple invocations of the 
HTML browser. 
When using VoiceXML, the same session mechanism is at work. However, the 
VoiceXML browser runs on a gateway, not a local PC. The sessions are established 
between instances of the browser running on the gateway and the Web appli-
cation server. A single VoiceXML browser instance handles one call. Therefore, 
VoiceXML sessions exist for the duration of a phone call to the gateway. 
A VoiceXML session is initiated by phoning a VoiceXML gateway. There are 
several alternatives for how the call is connected to a VoiceXML browser instance. 
There is one phone number for all applications hosted on the gateway. 
When calling the number, the person first identifies the desired application 
by voice and then is authenticated and connected to a browser instance. 
There is a distinct phone number for each hosted application. The person 
is authenticated and connected to a browser instance. 
There is a private phone number for each user. In this case, the call-in 
number can be used to identify the user and connect him or her directly 
to a VoiceXML browser instance. 
159 

Chapter 12 
160 
Authentication Techniques 
Four options that can be used to authenticate the user of a voice system are 
as follows: 
• User speaks name and password 
• User speaks or keys in user ID and keys in PIN 
• User speaks name, system challenges, and user speaks response 
• User speaks name and voice sample 
The first option corresponds most directly to a conventional username and 
password login. This option is attractive because the voice interface uses exist-
ing user authentication information (that is, it is backward compatible with 
existing Web applications). It also works well in hands-free environments. Dis-
advantages with this option are that the password is spoken aloud and may be 
overheard (think of calling in from the airport), and it is subject to the normal 
foibles of voice recognition. For example, two people's names may sound very 
similar, causing recognition problems at login. In addition, a password should 
be unusual (to minimize the chances of being guessed), but that may make it 
difficult for the person to say it and/ or for the computer to recognize it. 
The second option is familiar to everyone who's used Interactive Voice 
Response (IVR) systems over the phone. The advantage of using the keypad is 
that keypad tones can be recognized unambiguously. The disadvantage is that 
requiring keypad usage is not conducive to hands-free use of the voice system. 
The third option is a challenge/response style of authentication. Instead of 
specifying a single password, the user defines a set of challenges and responses. 
This can either be done explicitly (for example, the computer says "hello," the 
user says "goodbye"; the computer says "red," the user says "green"; and so on) or 
algorithmically (for example, the computer says "11," the user adds "43," and the 
user responds "54"). This approach overcomes the big drawback of the first 
option, because people can overhear only the response and not the challenge. As 
a practical matter, a drawback of this approach is that the challenge/response 
protocol is not widely used for logging in to computers, and hence people are not 
particularly familiar or comfortable with using it. 
The last option is an interesting option that capitalizes on the use of voice. 
A person's voice can be used as a means of biometric authentication. In this situ-
ation, the person's voice is analyzed and matched against a predefined sample. 

Adding VoiceXML to Web Applications 
This approach is a slick and easy way to use voice technology. The major draw-
back is that authentication by voice analysis is not standardized, so application 
security must be built on proprietary technology. 
Obviously, many variations and combinations of these techniques are possi-
ble. Currently, for example, you may perform financial transactions through 
a Web browser with password-based authentication. However, when you perform 
the same transactions over the phone (with a customer service representative), it 
is quite common to go through a challenge/response authentication process in 
which the customer service representative asks you two or three questions (for 
example, "What's your mother's maiden name?" and "What is your billing address 
and ZIP code?") to verify your identity. The questions are drawn from a pool of 
"personal facts" known about you, including your social security number, the 
amount of the last deposit you made, the amount of your last cleared check, 
and soon. 
Dynamic Grammar Generation 
Grammars perform the same function in VoiceXML that drop-down lists perform 
in GUis: They encode and constrain the set of valid responses the user can give. 
I'll use the term "choice lists" to generically cover both concepts. Some choice 
lists are the same for all users-for example, navigational quick links to various 
points in a Web site. Some choice lists have a fixed set of choices, which may be 
enabled or disabled on a user-by-user basis. This is common for command lists, 
where the functions a user can perform are restricted by authorization level. 
Finally, some choice lists are unique to each user. For example, the list of all con-
tacts in my address book is unique to me. 
In the SPIM example, when the person wants to place a call, the computer 
will ask for the name of the person to call. Valid responses are defined by a gram-
mar that contains the name of each known contact. The grammar is different for 
each user, and it changes as the address book is edited. The grammar, therefore, 
is generated dynamically out of the database on the server. 
Grammars may be dynamically generated as either internal or external 
grammars. Internal grammars can be used when the VoiceXML interpreter loads 
a form that is generated on the server. This occurs when the interpreter executes 
a <submit next="uri"> element and uri identifies a generated document. In this 
case, the inline grammar is another piece of dynamic content that is derived from 
the database. External grammars can be generated when a <grammar src="uri"> 
element is loaded. In this case, uri identifies a generated document. Figure 12-3 
illustrates dynamic grammar generation. 
161 

Chapter 12 
162 
Figure 12-3. Dynamic grammar generation 
Grammar tags may be used to specify the text that is returned to the 
VoiceXML interpreter when a phrase is recognized. For dynamic grammars, 
the tag can specify the database key value for the recognized item. For example: 
<grammar type=" application/x-jgs f" > 
(Ken [Abbott]) {'C15467'} I (Dennis [McCarthy]) {'C23432'} 
</grammar> 
Using the database key as tag has the advantages of uniquely identifying the 
recognized item, despite potential variations in the exact phrasing, and enabling 
the database key to be passed back to the server on a subsequent <submit>. The 
server can perform a more efficient indexed key lookup from the database rather 
than a search on name. 
Web Application Architectures 
This section examines some of the architectural approaches for incorporating 
a GUI, WUI, and VUI into a single Web application. The focus will be on explor-
ing the server-side technologies that can be applied to support all three kinds 
of interfaces. 
Application Styles 
To provide a frame of reference for categorizing various types of applications, 
I distinguish two dimensions: the processing style and the information style. 
As shown in Figure 12-4, processing styles range between transactional and 

Adding VoiceXML to Web Applications 
Collaborative 
Processing 
Style 
Transactional 
Dat a 
Figure 12-4. Application landscape 
Cr<dlt Cord 
¥rocessin g 
s•t• 
Portfolio 
•.at~age-atnt 
Information 
Style 
Tt"("hnlcal 
PuOlbhlrg 
l·Votlng 
Document 
collaborative processing. Information styles range between data and document. 
These conceptual distinctions are useful to relate what the application does to 
the underlying technologies involved. 
Data versus Documents 
When I worked at Xerox in the late 1980s and early 1990s, a lot of effort was spent 
trying to pin down what it meant to be "The Document Company," and more 
specifically, what a "document" was. A definition in vogue back then (and still 
popular with me) was ''A document is recorded information that is structured for 
human comprehension." By analogy, "Data is recorded information that is struc-
tured for machine interpretation." In other words, if the information is intended 
to be conveyed to a person, it's a document; if it's intended to be processed effi-
ciently by a machine, it's data. 
This definition captures the range of artifacts that people intuitively recog-
nize as "documents." On one hand, there are utilitarian documents that are 
primarily a means of recording something-for example, a copy of the minutes of 
a meeting. On the other hand, there are documents that result from the creative 
work of people-for example, paintings, books, films, and so on. Documents that 
are the result of a creative or artistic process are specifically structured to pro-
duce a certain state of comprehension in a person and, by the way, are recorded 
so that they can be comprehended by many people. However, a row in a data-
base, a file on disk or tape, a computer punch card, and so on are certainly data. 
Of course, there are fascinating gray areas: Is a printed bar code data or a docu-
ment? What about an encrypted, compressed, JPEG-scanned image? 
163 

Chapter 12 
164 
Translating between the two categories is not trivial. Consider what it takes 
to display the content of a row in a database on your computer screen (for 
instance, queries, data conversions, transactions, and so on). On the other hand, 
consider what's involved in converting a printed book into a machine-searchable 
electronic format (scanning, OCR, page recognition, and so on). 
In a sense, markup languages represent an attempt to reconcile documents 
and data. Information recorded using markup language (XML in particular) is 
human readable, but it also can be interpreted by machines. 
Transactional versus Collaborative Processing 
This section introduces two contrasting styles of processing that involve com put-
ers. The point of this discussion is to develop a conceptual framework for 
distinguishing situations where a conventional, transactions-and-data style of 
processing is appropriate from situations where a collaborative, document-
oriented style is appropriate. 
Transactional Processing 
Transactional processing occurs in the following steps: 
l. Elicit the type of transaction the user wants to perform. 
2. 
Gather all the information needed to process the transaction. 
3. 
Submit the information and process the transaction. 
4. 
Notify the user of the processing outcome (whether the process suc-
ceeded or failed). 
5. 
Repeat all steps. 
An example of transactional processing is when you buy stuff online. You 
implicitly state your desire to buy things (transaction type) by opening your shop-
ping basket or adding the first item to it. As you browse, you add more items 
(gathering information about what you want to buy). When you check out, you 
provide personal billing information (still gathering). When you click the Submit 
button, all the information is submitted and the transaction is processed as 
a unit. The next page you see tells you whether the transaction succeeded or failed. 
While buying and selling are the most obvious examples of transactions, 
other examples include applying for a service (for example, a credit card), 

Adding VoiceXML to Web Applications 
performing various personal finance-related actions (for example, viewing your 
checking account balance), or submitting a bug report. 
Database transactions, which are the heart and soul of most transaction pro-
cessing applications, exhibit the ACID properties: 
• Atomic: Transactions either happen or they don't-there's no 
middle ground. 
• Consistent: Transactions transform data from one consistent state 
to another. 
• Isolated (or Independent): A transaction produces the same result whether 
it is performed by itself or at the same time as others. 
• Durable: Once performed, the results of a transaction persist until another 
transaction changes them. 
Collaborative Processing 
Collaborative processing embodies the following process for transform-
ing information: 
1. 
Someone initiates the process-for example, by submitting an appli-
cation. A temporary case folder (or its ilk) is created, marked 
"in-process," and links to relevant information are placed inside. 
2. 
The case folder is routed through a series of steps. At each step, the infor-
mation is processed by a person or application, who may modify the 
information, link in more information, and/ or change the status of 
the process (for example, from pending approval to approved). 
3. 
After the process has passed through all the steps, the outcome of the 
overall process is assigned and the process stops. Final results of the pro-
cess are archived and intermediate results may be purged or archived 
as needed. 
Processing a mortgage application is a good example of collaborative pro-
cessing. An applicant submits an application form, which starts a loan approval 
process. The folder is routed to various people, beginning with the loan proces-
sor, who assembles all the relevant documents (credit reports, appraisals, and so 
on). Some of the information is gathered by other people who are part of the pro-
cess as well. Once the package is complete, it is routed to others-loan officers, 
165 

Chapter 12 
166 
underwriters, and so on-who review the package and approve or reject the loan 
application. Finally, approved applications move to the loan officers responsible 
for settlement. 
The noteworthy characteristics of collaborative processing are as follows: 
• Processing occurs in steps and completion of a step is signaled by chang-
ing the state of the process, usually by a person. 
• Processing occurs in human time (minutes, hours, days, and so on) rather 
machine time (subsecond). 
• Multiple people and applications are involved. 
• Processing rules may be applied by people and not necessarily enforced by 
the computer system. 
Multitiered Architectures 
It is common practice now to implement Web applications using a multi tiered 
architecture. In Figure 12-5, the tiers are overlaid on the transformational pro-
cessing model. The image shows that the browser tier performs rendering, the 
Web tier performs formatting for various interaction media, and the application 
server handles processing and production of data provided by data source. 
Figure 12-5. Multitiered Web architecture 
The high-level architectural view in Figure 12-5 applies no matter what the 
underlying implementation technology. However, many choices and tradeoffs 
need to be made in designing and implementing the underlying technologies. 

Adding VoiceXML to Web Applications 
You'll see some examples of the alternatives in the following sections, and you'll 
investigate how the style of the application may influence the design and tech-
nologies chosen. 
Servlet and Server Pages Architectures 
Figure 12-6 shows a typical multitier architecture for an existing HTML-based 
Web site. 
Figure 12-6. Current multitier Web architecture 
The user interacts with an HTML browser running on a PC or workstation. 
The browser communicates with a Web server over the Internet using the HTTP 
protocoL The Web server is augmented with servlet and server page components 
to manage and publish content dynamically. The extension components com-
municate with an application server through an RPC protocol (such Java RMI, 
CORBA IIOP, or Microsoft COM+) over a secured network. In the application 
server, business object components implement business logic in a presentation-
independent manner. Business objects may be persistent or nonpersistent and 
make use of"enterprise technologies" such as messaging, database management, 
and transactions. 
This architecture has been proven successful in a number of e-business Web 
sites. The architecture is flexible, it can be deployed onto a variety of hardware 
and software configurations, and it provides independent scalability ofWeb 
response, business transaction processing, and data management. 
In this architecture, servlets and server pages translate between two worlds. 
On the back end is the world of distributed, transactional processing: APis, dis-
tributed computing services, legacy computer systems, and data stores. On the 
front end is the world of the Web: a network of hyperlinked documents that are 
structured for rendering to humans, unstructured interactivity, and so on. 
167 

Chapter 12 
168 
XML Publishing Architecture 
Figure 12-7 depicts an XML-centric architecture. In this document -oriented 
approach, information flows from point to point in the form of documents. Pro-
cessing is structured on the workflow paradigm, in which documents are passed 
through a series of tasks that are performed by different people or programs. 
Documents are transformed from one schema to another as needed, and they are 
rendered as needed. 
Presentation 
WML Gateway 
VoiceXML 
Gateway 
XIII. Server 
Transformer 
+ 
Workflow 
Figure 12-7. XML publishing architecture 
Connectors are interfaces to sources and sinks of data. A connector abstracts 
access to a data source, such as a database, a legacy application, a peer docu-
ment processing application, or a real-time data feed. Connectors convert the 
data between its native format and a canonical XML format, which is the lingua 
franca used within the XML server. The canonical XML schema is purely con-
cerned with the logical structure of the information and is not oriented toward 
any particular presentation. 
During processing, documents are routed to people and rendered for them. 
At the point when the document server is requested to render a document in 
a specific format, a formatter is invoked. Formatters abstract the process of ren-
dering a document to a specific medium. A formatter transforms documents 
from their canonical form into an XML stream that is specifically tailored for pre-
sentation (for example, XHTML, WML, or VoiceXML). 
Table 12-1 contrasts some of the key features of transactional and document 
processing architectures. 

Adding VoiceXML to Web Applications 
Table 12-1. Transactional versus Document Processing Architectures 
Key Technologies 
"Sweet Spot" Applications 
Strengths/Weaknesses 
TRANSACTIONAL 
Servlets/server pages 
Distributed objects 
DBMS 
Transactions 
E-commerce 
Financial services 
Retail 
+ Integration with legacy 
systems. 
+Underlying technologies are 
"experienced" with respect to 
scalability, reliability, and so on. 
+Cross-platform processing. 
DOCUMENT PROCESSING 
Stylesheets 
XML documents 
Multimedia documents 
Workflow processing 
Case management 
Application approval 
Collaborative design 
+Automation of multi person 
business processes. 
+ Document model is 
comprehensible to people. 
+Cross-platform information 
exchange. 
-Complex. 
-Administrative tools/techniques 
not well developed. 
Formatting: The Heart of the Matter 
When retrofitting an existing Web application with voice, the hardest part is the 
formatting operation. In formatting, generic application data is structurally 
transformed into data structures that can be used by the user interface. For GUis, 
these data structures include familiar elements such as pick lists, tables, menus, 
and so on. For VUls, the data structures are dialogs, voice menus, and links and 
grammars. Because there is no cookbook translation between the data structures, 
good design is required. The following sections explore technical options for 
designing and implementing the formatting functions in a Web application. 
169 

Chapter 12 
170 
]SP andXSL: Covert Cousins? 
Because of the very different contexts in which they evolved, at first glance, JSPs 
and stylesheets seem more different than alike. However, they perform very simi-
lar functions: transforming information into a renderable document. In JSP, the 
input data is gathered through conventional computer processing: from data-
bases, generated by programs, and so on. In this world, the underlying data is 
structured for machine processing. In XSL, the input data is documents, which 
are structured for human comprehension. In both cases, the outputs (of interest 
here) are text documents that can be rendered by some sort of browser. 
The main difference between JSP and XSL as transformation languages is the 
scripting model: JSP supports a procedural style of scripting and XSL supports 
a declarative style of scripting. In the original JSP, script code was syntactically 
distinguished in enclosing <% and %>. The enclosed script is treated as Java 
source code. Because of this syntax, JSPs were not well-formed XML docu-
ments. As ofJSP 1.2, JSP containers are required to support an equivalent, XML-
compliant syntax. Furthermore, explicit scripting can be eliminated from 
JSP source code by using custom tags. 
Listing 12-1 through Listing 12-5 compare equivalent code fragments imple-
menting different approaches to iterating through a collection of items and 
generating a line of text for each. In the JSP examples, the bean "DoQuery" is 
a component that performs a query and returns a Java collection containing the 
results. In the XSL examples, the input document is an XML document that con-
tains an < i tern> element for each item in the result set. 
Listing 12-1. JSP/XSL Comparison: JSP with Embedded Script 
<jsp: usebean id="results" class="DoQuery" scope="session" I> 
<% 
for(Iterator i=results.iterator(); i.hasNext();) 
{ %> 
this is item <%= i. next(); %> 
<%} %> 
Listing 12-2. JSP/XSL Comparison: JSP in XML Syntax 
<jsp:root ... > 
<jsp:usebean id="results" class="DoQuery" scope="session" I> 
<jsp: scriptlet> 
for(Iterator i=results.iterator(); i.hasNext();) 
{ <ljsp:scriptlet> 

Adding VoiceXML to Web Applications 
this is item <j sp: expression> i. next(); <lj sp: expression> 
<jsp:scriptlet> 
} <ljsp:scriptlet> 
<lj sp: root> 
Listing 12-3. ]SPIXSL Comparison: ]SP Custom Tag1 
<jsp:root ... > 
<jsp:usebean id="results" class="DoQuery" scope="session" I> 
<customTag:forEach group="<%= results %>" item="i" > 
this is item <jsp:expression> i <ljsp:expression> 
<lcustomTag: forEach> 
<ljsp:root> 
Listing 12-4. ]SP/XSL Comparison: XSL for -each Tag 
<xsl:stylesheet ... > 
<xsl:template match=" I"> 
<xsl:for-each select="llitem"> 
this is item <xsl:value-of select="." I> 
<xsl: for-each> 
<lxsl: template> 
<lxsl: stylesheet> 
Listing 12-S.]SP/XSL Comparison: XSL Template Rule 
<xsl: stylesheet ... > 
<xsl :template match=" I"> 
<xsl:apply-templates select="llitem" mode="showltems"> 
<lxsl :template> 
<xsl :template match=" item" mode=" show Items"> 
this is item <xsl:value-of select="." I> 
1 Based on Allaire Corporation's freely available JRun 3.0 Tag Library 
(http:llwww.allaire.comldocumentslobjects1Whitepaperljruntaglib_syntax_1.pdf). 
171 

Chapter 12 
172 
</xsl: template> 
</xsl: stylesheet> 
The bottom line is that JSP and XSL are similar in power and inscrutability. 
There is no killer technical reason for favoring one over the other. Personally, 
I give an edge to XSL because its recursive, template-based approach is some-
what easier to use for complex "tree-to-tree" transformations. On the other hand, 
the procedural style and Java-related syntax ofJSPs are probably more accessible 
to existing software developers and Web designers. 
Using ]SPs andXML Together 
JavaSoft seems aware of the potential redundancy between JSPs and XML 
stylesheet-based transformations. Over time, the two technologies will become 
more integrated. Already, evidence of this direction is visible in the upcoming 
release of the Java Servlet API 2.2 and JavaServer Pages 1.2. These and other 
upcoming releases increase the interoperability of XML and Java technologies 
(see http: I /www. j avasoft. com/xml! ?front page-spotlight for more information): 
• The Servlet 2.2 specification includes the ability to specify a "processing 
pipeline" that allows stylesheet transformations on both the input to and 
the output from a servlet. 
• The JSP 1.2 specification requires JSP containers support the XML-
compliant syntax for JSPs. 
• Java API forXML Processing (JAXP l.l) defines a standard Java API for 
parsing and generating XML. 
• The Java Architecture for XML Binding (under development) will enable 
translation ofXML Schemas into Java classes. 
In a white paper titled "Developing XML Solutions with JavaServer Pages 
Technology" (http: I /java. sun. com/products/jsp/pdf!JSPXML. pdf), several archi-
tectural options for using JSPs with XML are presented. Of particular interest to us 
is the section "Generating Markup Languages Using JSP Pages," which presents 
three approaches to generating multiple markup language presentations from a 
common XML data source. The three approaches are summarized in this section. 

Adding VoiceXML to Web Applications 
Single Pipeline 
In the single pipeline approach (see Figure 12-8), the XML data is parsed under 
the control of the JSP. The output of the JSP is an XML document that contains 
the union of all information required for any of the possible presentations. For 
each presentation medium, an XSLT stylesheet transforms from the common 
XML to the markup language for the medium. 
Clients 
Web Layer 
Data 
Sources 
DHTML 
HTML 
8 
"".' '"' '""''""' -EJ 
v1a Custom Tag 
g 
WML 
XML 
t 
'"(>rr<e) 
XSL(WML) 
XSL{XML) 
Figure 12-8. Single pipeline generating multiple markup languages (diagram 
courtesy of Sun Microsystems) 
This approach is conceptually clean. Run-time costs include parsing the 
XML data, parsing the stylesheet, and applying the stylesheet. Given that 
the presentation media may be quite different and require radically different pre-
sentation flows (for example, consider the difference between presenting a table 
of data in HTML versus VXML), the transformations from the common format 
may be quite complex. This means that a lot of the UI design and implemen-
tation logic is implicitly coded into the stylesheet. 
Multiple Pipeline 
In the multiple pipeline approach shown in Figure 12-9, a different JSP is associ-
ated with each presentation medium. The JSP pages contain the static (template) 
data for each presentation markup language, and dynamic data is inserted by 
using custom tags and/ or bean properties. 
173 

Chapter 12 
174 
Clients 
Web Layer 
Data 
Sources 
DHTML 
(JsP Page)' 
HTML 
BG:::) 
WML 
.._ 
Custom 
~ 
Tag 
XML 
(JSP Page) 
Figure 12-9. Multiple pipelines generating multiple markup languages (diagram 
courtesy of Sun Microsystems) 
This approach uses JSP pages the way they were intended to be used. Page 
developers code presentation JSPs targeted for a particular presentation 
medium, and Java developers code beans and custom tags that encapsulate the 
application's data model. The main drawback to this approach is that it doesn't 
take advantage ofXML. The source data is parsed and represented as objects, and 
the output of the JSPs is generated as text streams (that happen to contain XML). 
Combined Approach 
The single and multiple pipeline approaches can be mixed and matched by apply-
ing stylesheet transformations to the output of JSPs (see Figure 12-10). JSPs are 
used to handle the transition between object representation and XML represen-
tations specific to each presentation technology. XSLT stylesheets handle the 
XML-to-XML "style" transformations between similar XML dialects. Conceptually, 
this seems like an appropriate use of JSP and XSLT technologies. Pragmatically, it's 
not clear that using two transformation techniques rather than one is a win, given 
the additional costs of developing and maintaining in two technologies. 
Clients 
B~+---
8 ~ 
GJ 
0~+----
Web Layer 
Data 
Sources 
Figure 12-10. Combination pipeline generating multiple markup languages 
(diagram courtesy of Sun Microsystems) 

Adding VoiceXML to Web Applications 
NOTE 
The diagrams from the ]avaSoft white paper depict the data sources 
as monolithic "XML," which obscures one of the major advantages of the 
multiple pipeline approach. If you are retrofitting an existing Web appli-
cation to support more presentation media, presumably there is an 
existing implementation of the application's object model. The multiple 
pipeline approach ties neatly into the existing application infrastructure, 
which may or may not involve XML. On the other hand, if the data sources 
are all nicely behaved XML producers and consumers already, what's the 
benefit of parsing it all, representing the information as objects, processing 
]SPs, and then generating XML again? In this case, an XML-based 
approach makes more sense. 
Summary 
With the addition of voice as an interface medium, it is now becoming necessary 
to think ofWeb applications as switch boxes for connecting people and content. 
The transformational processing model provides a conceptual framework for 
relating the key steps in accessing and displaying content in a variety of media. 
The steps are as follows: produce, process, format, and render. These steps are 
general enough that they can apply to applications with HTML, WML, or 
VoiceXML interfaces. 
In looking at Web application architectures, it can be helpful to categorize 
their "information style" and "processing style." Some applications manipulate 
data, which is generated and consumed by computers. Other applications 
manipulate documents, which are information packaged for human con-
sumption. Transactional processing involves gathering all the information to per-
form a particular transaction, and then performing the transaction as a single, 
computer-controlled operation. Collaborative processing involves interaction of 
multiple people and applications over a "human" time scale. 
Two multi tiered application architecture options were characterized and 
examined. The popular servlet and server pages architecture is widely used in Web 
applications today, and it can be extended to handle voice. The XML publishing 
architecture is not currently in wide production use, but it has some strong fea-
tures that are compatible with the transformational model. Although presented as 
alternative options, there are a number of subtle similarities and dualities within 
the architectures. A number of techniques for mixing and matching architectural 
elements, especially XML, server pages, and stylesheets, are possible. 
With the conceptual foundation from this chapter, in the next chapter you'll 
plunge into developing a working prototype of the SPIM application. The proto-
type is based on the transformational model and uses the Cocoon XML 
publishing framework to manage the transformation ofXML documents. 
175 

CHAPTER 13 
The Web Application 
Prototype 
IN THIS CHAPTER, you'll put together all the technology elements covered through-
out this book to implement the Running Late function of the SPIM application. 
The result of this exercise will be a prototype voice-enabled Web application. The 
prototype will demonstrate the correct architectural relationships between the 
components, but the components were selected for ease of use, not for pro-
duction (see Figure 13-1). 
The purpose of the prototype is to demonstrate the feasibility of the architec-
ture and provide a model for developing a "real" implementation. Try to keep the 
following two objectives in mind: 
Web 
Server 
Servlet 
Container 
Figure 13-1. Architecture of the Cocoon-based prototype 
• Leave the choice of production components and technologies open. Do 
not make the demonstrated architecture depend on proprietary aspects of 
a particular technology or component, unless you are already committed 
to use it in production or you know that the architecture permits the 
dependency to be removed. 
177 

Chapter 13 
178 
• Don't get lost in the details. When integrating a multi tiered distributed sys-
tem such as this one, there are a multitude of fascinating technologies to 
be explored and a multitude of bugs and unimplemented features to frus-
trate. The purpose of the prototype is to validate the architecture without 
implementing a working system. 
You can use the prototype and the information in the following sections as 
a concrete pedagogical example, or you can actually install the components and 
make it work on your computer. I have included some instructions to help you 
hands-on types install and run the prototype, but a complete, step-by-step tuto-
rial is beyond the scope of this book. Therefore, I suggest you read through the 
following sections before jumping in and trying to install things. If you decide to 
run the prototype, be prepared for the fact that you will probably spend a lot of 
your time wrestling with installation, administration, and component integration 
issues, and relatively little time actually codingVoiceXML. Welcome to the mod-
ern world of component-based development at Web speed! 
Prototype Setup and Installation 
Here's what you need to do to prepare to run the prototype: 
• Install the IBMWebSphere Voice Server SDK. 
• Install and configure JRun 3.1. In the prototype, I use JRun as both the 
Web server and the servlet container for Cocoon. It is possible to use JRun 
as just a servlet container in tandem with another Web server, such as 
Tomcat or liS. 
• Deploy the SPIM Web application (including Cocoon 1.8.2) into JRun. 
• Install an XML tool that supports editing and debugging of XML and XSL 
files. The prototype uses XML Spy. Other tool choices are possible, or you 
may use Emacs or the command line if you like. 
• Set up an ODBC data source called "SPIMApplicationDb." The prototype 
includes a Microsoft Access database that can be configured as an ODBC 
data source. 

The Web Application Prototype 
Some of the software on the companion CD is evaluation software that will 
work for 30 days after installation. After this evaluation period, you will either 
need to buy the software or convince the manufacturer to extend your evalu-
ation period. 
TIP 
Wait until you're ready to experiment with the prototype before 
installing the evaluation software. This will maximize the time you have 
for experimentation. 
The installation instructions in the following sections are based on my 
experience installing software from the companion CD on my Windows 
2000-equipped development machine. Depending on your operating system and 
the prior configuration of your machine, the installation procedures may vary. 
TIP 
My machine is well equipped with RAM (384MB) and disk (more 
than 20GB), so I didn't hit any memory or disk limits. Although I have not 
performed an analysis to determine memory and disk requirements for 
this specific software configuration, based on prior experience I estimate 
that you should have at least 128MB of RAM (256MB is preferable) and 
lGBofdisk. 
Installing the IBM WebSphere Voice Server SDK 1.5 
The IBM WebSphere Voice Server SDK supplies the voice platform and 
VoiceXML interpreter needed to run the prototype. Install it carefully 
and pay attention to any dialog boxes that pop up. For help, visit 
http://www-4.ibm.com/software/speech/enterprise/ep_11. html. 
1. 
Execute IBM WebSphere \ VoiceServerSDK\ vssdkinstall_launcher.exe. 
Make sure there is plenty of space on the drive containing the temporary 
directory for unpacking. Dismiss the "The package has been delivered 
successfully" dialog box. 
2. 
Execute IBM WebSphere\ VoiceServerSDK\ vssdkinstall_en.exe. Unpack it 
into same directory you used in Step l. Dismiss the "The package has 
been delivered successfully" dialog box. 
179 

Chapter 13 
180 
NOTE 
This installs the U.S. English version (indicated by locale "_en") of 
the voice platform. You can install other languages by repeating Step 2 
for the other available locales: "_de" (German), "Jr" (French), or "en_ GB" 
(Great Britain). 
3. 
Go to C: \temp or whatever directory you specified. 
4. If you do not have JRE1.3 installed, execute temp\install\jrel3\ 
j2re 1_3_0-win-i.exe. 
5. 
Execute temp\setup.exe. 
6. 
In Windows 2000, you will see the message shown in Figure 13-2 
(ignore it, but be aware that the software is not officially supported on 
Windows 2000). 
,t, IBM WebSphere Vo•ce Server SDK 
"~ £1 
----
----
- ----
---
----
======· 
~ 
Operllling System: 
PAS.S I 
Java Rurtlne Env: 
PASS! 
V.aVoice Rlrtimes: 
PASS/ 
Voice Server or SDK: 
\.Widows 2000 
SUn .Rf 1 .3 .0 
No Previous Version nsteletl 
Figure 13-2. IBMWebSphere Voice Server SDK System Requirements dialog box 
(Windows 2000) 
7. 
You'll be asked the usual questions about licenses and so on. Install to 
the directory of your choice. Click Next to install all features. 

The Web Application Prototype 
8. 
Reboot. 
9. 
To install the upgrade to Version 1.5.1, execute IBMWebSphere\Voice-
ServerSDKupdate.exe. 
10. Reboot. 
NOTE 
The Voice Server needs to set up your microphone and earphones 
before you use the voice browser. You can run the setup wizard from the 
Start menu (Start> Programs> IBM WebSphere Voice Server SDK> Audio 
Setup- US English). If you don't run it manually, the wizard will run 
automatically the first time you try to use the voice browser. 
Installing Allaire JRun 3.1 
JRun 3.1 is an application server that includes a Web server, a servlet/JSP 
container, and an EJB container. The following installation procedure assumes 
that a JDK is already installed on your computer. For help, visit 
http://www.allaire.com/products/jrun/index.cfm. 
1. 
Run jrun-31-win-us.exe.exe. 
2. 
Do not enter a serial number. This will install a capacity-limited devel-
oper version. When you pick an installation directory, override the 
default and pick one with a short name (for example, D:\JRun1). 
3. 
Select the full installation and click Next. 
4. 
When prompted, do not install JRun services (uncheck the box). 
5. 
Let the JRun Admin Server Port Number default to 8000. 
1 This sounds odd, but I encountered a pernicious bug caused by an internally generated 
CIASSPATH becoming too long and overrunning some string size somewhere (I don't know 
which component or OS was responsible). No error was reported, but evidently the class· 
path was silently truncated, causing JRun and Cocoon to start failing on obscure "Class 
not found" errors. Reinstalling JRun from \Program Files\Allaire\JRun to \jrun fixed 
the problem. 
181 

Chapter 13 
182 
6. 
Enter a password and confirmation. 
7. 
Answer questions about your desire to receive JRun product information. 
8. 
Select "Start the JRun Management Console, I'll configure my web server 
later" and click Finish. 
9. 
When the Management Console prompts you, log in as user admin, 
using the password you set in Step 6. (If your browser has trouble 
opening the Management Console page, wait a few seconds and 
click Refresh.) 
Installing Altova XML Spy 
XML Spy is an Interactive Development Environment (IDE) for XML. Within XML 
Spy you can edit and validate XML code, XML Schemas, and DTDs. You can also 
edit and run XSL stylesheets at the push of a button. For help, visit 
http://www.xmlspy.com/. 
l. Run XMLSpy.exe. 
2. 
Leave the key code blank for a 30-day evaluation. 
3. 
Set the installation directory. 
4. 
Choose the full installation. 
5. 
Make XMLSpy the default editor for XML file types. 
6. 
Click Yes in answer to the complicated question about XHTML. 
7. 
Reboot. 
Deploying the SPIM Application 
I found installing and configuring Cocoon in its entirety to be tricky and very time-
consuming. Because Cocoon is open-source software, there is no one responsible 
for supporting installation, so it's every person for him- or herself. To incorporate 

The Web Application Prototype 
the tips and tricks I learned the hard way while installing Cocoon on my Windows 
2000 system, I packaged the SPIM application, including Cocoon 1.8.2, as a J2EE 
WAR (Web ARchive) file. In theory, this application should be deployable into any 
J2EE-compliant servlet container. I have only tried it with JRun 3.1. 
CAUTION 
The SPIM application installs Cocoon as its required run-time 
environment. The Cocoon environment has been configured specifically to 
make the SPIM application work properly. It is not intended to be a full-
fledged, general-purpose Cocoon installation. If you are interested in using 
Cocoon without the SPIM application, I suggest you go to the Apache site 
(http://www. apache. org/) and start from there. 
To deploy the SPIM application, perform the following steps. 
l. Stop the JRun admin and default servers. 
2. 
Edit the jrun/lib/ global. properties file. Replace this line: 
java.classpath={jrun.classpath};{user.classpath};{ejb.classpath}; 
{servlet.classpath} 
with the following (all on one line): 
java.classpath= 
{jrun.rootdir}/servers/default/spim-application/WEB-INF/lib; 
{jrun.classpath};{user.classpath};{ejb.classpath}; 
{servlet.classpath} 
3. 
Start the JRun Management Console (Start> Programs> JRun3.1 >Start 
Management Console). In the left pane, select and expand "JRun Default 
Server." Click the Web Applications link. In the right pane, select "Deploy 
a Web Application." Use the Browse button to find the file 
Prototype/spim-application.war on the companion CD. Fill out the 
forms as shown in Figure 13-3, and make sure the directory name is 
jrun/ servers/ default/ spim -application. Click Deploy. 
183 

Chapter 13 
184 
• .:jlRun Admin !il!lrvrr 
Deploy 1 Web Application 
• ~ Jbft Ot'lault §ervrr 
PMMetto.e: 
Rtrurn to w~.n P19t 
E.Cklan~JJ"<. IDon 
Cte.l:t i nADi)I.Callon 
lt•i.J··~ "' .~t·iotl• .tit ,., 
R•mow ~n ,t.ppt~uton 
Y'ou~ f O I H1M111~A•Istt'Wt 
.,lfttf •oKII ol(~Jlkltllon depioyl•..-.11'. 
You t-.i"e -sut{nsfyC., de"lonci 
SPI~ Apl)llcat.lon mto 
e: I jru n/ ~I! rve r"o/ d'ef., u lt\SP llllt · 
Apptlutlon 
Wtkome 
Figure 13-3. ]Run Management Console (after deployment) 
4. 
Stop the JRun Admin Server. Rename the following files (these are Java 
libraries that ship with JRun but are superseded by the versions that 
Cocoon requires): 
/jrun /lib/ext/jaxp.jar -> /jrun/servers/lib/ext/jaxp.jar.BAK 
/jrun /lib/ext/parser.jar -> /jrun/servers/lib/ext/parser.jar.BAK 
5. 
Edit /jrun/ servers/ default/ spim-application/WEB-INF I cocoon. 
properties. Search for the text string "SPIM Resource Protocol 
Workaround." The file protocols in the ten lines that follow should be 
edited to point to files in your JRun installation. For example, if you 
installed JRun in e: I jrun, the lines should read as follows (one line for 
each assignment): 
processor.xsp.logicsheet.context.java = file:/1/E:/jrun/servers\ 
/default/spim-application/cocoon/resourcefiles/context.xsl 
processor.xsp.logicsheet.cookie.java 
= file:/1/E:/jrun/servers\ 
/default/spim-application/cocoon/resourcefiles/cookie.xsl 
processor.xsp.logicsheet.global.java 
= file:/1/E:/jrun/servers\ 
/default/spim-application/cocoon/resourcefiles/global.xsl 
processor.xsp.logicsheet.request.java 
= file:/1/E:/jrun/servers\ 
/default/spim-application/cocoon/resourcefiles/request.xsl 
processor.xsp.logicsheet.response.java = file:/1/E:/jrun/servers\ 
/default/spim-application/cocoon/resourcefiles/response.xsl 
processor.xsp.logicsheet.session.java = file:/1/E:/jrun/servers\ 

The Web Application Prototype 
/default/spim-application/cocoon/resourcefiles/session.xsl 
processor.xsp.logicsheet.util.java 
= file:/1/E:/jrun/servers\ 
/default/spim-application/cocoon/resourcefiles/util.xsl 
processor.xsp.logicsheet.sql.java 
= file:/1/E:/jrun/servers\ 
/default/spim-application/cocoon/resourcefiles/sql.xsl 
processor.xsp.logicsheet.esql.java 
= file:/1/E:/jrun/servers\ 
/default/spim-application/cocoon/resourcefiles/esql.xsl\ 
processor.xsp.logicsheet.fp.java 
= file:/1/E:/jrun/servers\ 
/default/spim-application/cocoon 
6. 
Start the JRun Admin Server and the JRun default server. From a browser, 
enter the URL http: I /localhost: 8100/SPIMApp/VoiceSnoop. Make the 
appropriate menu selections in your browser so that you are viewing 
the HTML source for the current page. You should see a VoiceXML imple-
mentation of the SnoopServlet, which echoes header and request 
information to the client. Or, run the IBMVoiceServer by entering the 
following command line: 
%IBMVS%\bin\vsaudio en us http://localhost:8100/SPIMApp/VoiceSnoop 
Copying the SPIM Application 
From the CD, copy the SPIMApp directory to a directory you create on your 
machine called "VoiceXML." When you're done, the VoiceXML directory contains 
a subdirectory called "SPIMApp." 
Configuring the ODBC Data Source 
The sample database for the SPIM prototype comes packaged as a Microsoft 
Access database. For generality, the prototype accesses the database using 
generic ODBC database drivers. The following procedure configures the 
Microsoft Access database as an ODBC data source. It is possible to configure 
the data source to use a different DBMS. 
1. 
In Windows, bring up the Control Panel. 
2. 
In Windows 2000, double-click Administrative Tools, and then double-
click Data Sources. (On older versions ofWindows, you may have to 
double-click the ODBC Administrator icon.) 
3. 
Select the System DSN tab, and then click the Configure button. 
185 

Chapter 13 
186 
4. Click the Select button, navigate to VoiceXML\SPIMapp\Db\spim.mdb, 
and click OK. You should now see the dialog box shown in Figure 13-4. 
ODBC Microsoft Access Setup 
IJ £J 
Data Source Name: SPIMApplicationDb 
Description: 
SPIM sample database 
Database -----------==--~~--::--====-==-:--:-. 
Database: 1:\VoiceXML_CD\SPIMApp\Db\spim.mdb 
I 
C::$.·~~~~(.::::~· "!1 
Create... 
Repair... 
Compact... 
r Database: 
Figure 13-4. Microsoft Windows 2000 ODBC Setup dialog box 
5. 
Click OK, exit the ODBC Administrator, and you're done. 
Tips for Cocoon Enthusiasts 
The following tips are included for those that want to try installing Cocoon and 
deploying the SPIM application themselves. This section can be skipped if you're 
not interested in tangling with Cocoon directly. 
There are three important sources of information regarding installing 
Cocoon and configuring it to run with JRun. 
• The Allaire article titled "JRun 3.0: Installing Cocoon Servlet" 
(http://www.allaire.com/Handlers/index.cfm?ID=17501&Method=Full) 
provides basic information on configuring Cocoon with JRun. 
• The Installing Cocoon page (http: I /xml. apache. org/cocoon/install. html) 
on the Apache XML Project site provides information about installing 
Cocoon as well as some tips regarding JRun. 

The Web Application Prototype 
• The Cocoon mail archive (http: //xml. apache. org/ cocoon/ 
mail-archives. html) offers a searchable source of support infor-
mation for Cocoon. 
To install Cocoon, follow the instructions on the Allaire site, plus the adden-
dum at Cocoon site, plus the following undocumented hacks. Many of these 
require modifications to cocoon. properties. 
• When defining the "Init Arguments" for the cocoon servlet mapping in 
JRun, use a relative URL. To set or view the URL, log in to the JRun Manage-
ment Console. From the tree view on the left, click JRun Default 
Server/Web Applications/SPIM Application/Servlet Definitions. In the 
right-hand window, click the Edit button. The "Init Arguments" property is 
in the rightmost column of the table of servlet properties. For the servlet 
named "Cocoon," it should look like this: 
WEB-INF/cocoon.properties 
• To work around an apparent bug in how Cocoon resolves URis with 
protocol type "resource:," unzip all the resource files (file type .xsl) 
from cocoon. jar into the new directory jrun/ servers/ default/ 
cocoon-web-pub/WEB-INF/resources, and edit cocoon. properties 
to change the resource URLs to Web URLs. For example: 
#processor.xsp.logicsheet.context.java 
= \ 
#resource://org/apache/cocoon/processor/xsp/library/java/context.xsl 
processor.xsp.logicsheet.context.java = \ 
http://localhost:8100/cocoon-web-pub/WEB-INF/resources/context.xsl 
• Add new formatter definitions to cocoon. properties as follows: 
formatter.type.application/spim = org.apache.cocoon.formatter.XMLFormatter 
formatter.type.text/vxml 
= org.apache.cocoon.formatter.XMLFormatter 
# SPIM 
formatter.application/smil.doctype-system = \ 
http://localhost:8100/cocoon/sql/spim.dtd 
formatter.application/spim.MIME-type = application/spim 
# VXML 
formatter.text/vxml.doctype-system = \ 
http://www.voicexml.org/voicexml1-0.dtd 
formatter.text/vxml.MIME-type = application/x-vxml 
187 

Chapter 13 
188 
• In cocoon. properties, replace the existing media type definitions (lines 
starting with browser.) with the following lines: 
browser.o = vxml=vxml 
browser.l = vxml=VoiceXML 
browser.2 = html=MSIE 
browser.3 = html=Mozilla 
browser.4 = html=Netscape 
To test that Cocoon is working properly with JRun, make sure that 
the JRun default server has been restarted and visit the URI 
http: //localhost: 8100/SPIMApp/Cocoon. xml. You should see the 
Cocoon status display. 
Anatomy of the Prototype 
This section provides a guided tour of the SPIM prototype. As you will see, there 
are various parts and pieces to the prototype. Each component is worthy of 
a detailed chapter of its own. In the spirit of architectural prototyping, I have 
tried to summarize the role of each component and give you the flavor of how it 
works. I leave the exploration of details to you. 
The Database 
The database has been modeled very simply. The schema is shown in 
Figure 13-5. A sample Microsoft Access database is located on the companion 
CD atVoiceXML\SPIMApp\Database\Db\spim.mdb2• There are two core tables 
corresponding to appointments and address book contacts. Each table has a gen-
erated primary key that uniquely identifies the entity. The Appointment table 
uses a Contact ID as a foreign key to the Contact table. (This models whom the 
appointment is with.) Auxiliary tables map integer-coded values for meeting 
locales and media to text strings. 
2 I also exported the tables into Comma-Separated Values (CSV) format, which is a text for-
mat that can be loaded into other databases and spreadsheets. The files are in the same 
directory as spim.mdb and have a .csv file type. 

The Web Application Prototype 
~~~-,··~--------------~~ 
lastN¥ne -
F~ --
Cel'hone 
EJ'o\ai'rtMty 
EMOISocondory 
Horne-essl 
Horne-oss2 
>lomeQy 
.....,.State 
HorneC...Vy 
Homef'ostalc:ode 
~sAdctessl 
-..-.. .z 
-.say 
-<Slate 
-.~Co~r<ty 
--oleode 
Ncl<name 
s_QJJI) 
Figure 13-5. The SPIM prototype database schema 
Refinements needed to flesh out this implementation include the following. 
• Appointments are currently modeled with just two people: the Owner and 
the Contact. In reality, there would be one owner and a set of contacts. 
• Appointments might be with people who are not registered contacts (that 
is, people who don't have an entry in the Contact table). 
• The issue of how to model meeting times needs to be carefully thought 
through. The whole business of representing time in the database, being 
able to perform time-based queries, and appropriately representing time 
values as they are transformed between SQL representation, Java represen-
tation, and XML representation is a complicated, detail-rich process. For 
the purposes of the prototype, just store times as SQL timestamps and 
avert your eyes from problematic areas. 
189 

Chapter 13 
190 
Producing Content Markup from the Database 
When data is retrieved through a database query, the data is mapped into a single 
XML format. The XML format consists of a header (which identifies the SPIM 
user) and one or more Appointment elements followed by one or more Contact 
elements. Depending on the context for the query, the SPIM application deter-
mines how to interpret the relationships between Appointments and Contacts. 
For example, if the user is reviewing his or her calendar, there may be multiple 
appointments and multiple contacts (corresponding to the appointments within 
a given time period). For the Running Late function in the prototype, there will 
be at most one Appointment (scheduled for the current time) and one corre-
sponding Contact. See Figure 13-6 for a pictorial view. File .. \SPIMApp \XML 
Samples\SPIM.xsd contains the XML Schema representation of the schema, 
while file http: I !local host: 8100/ cocoon/ sql/Spim. dtd contains the (nearly) 
equivalent DTD, which was generated from the XML Schema. 
t•······ ·····-, 
·····: __ ~-~~- : 
..... :~;:~;.;.;;~~- : 
• ................ • 
Figure 13-6. The SPIM XML Schema 
Features of the Content Representation 
The SPIM XML Schema uses some advanced features of the XML Schema lan-
guage. Data types that appear in multiple contexts (such as people's names, 

The Web Application Prototype 
phone numbers, and addresses) are modeled as complex types and simply 
referred to from elements that conform to the type. For example, the following 
code fragment declares the "address" data type. Elements Contact/HomeAddress 
and Contact/BusinessAddress are both of this type. 
<xsd: complexType name=" Address Type"> 
<xsd: sequence> 
<xsd: sequence minOccurs="o" > 
<xsd: element name="Addresslinel" type="xsd: string" I> 
<xsd: element name=" Addressline2" type="xsd: string" minOccurs="O" I> 
<lxsd: sequence> 
<xsd:element name="City" type="xsd:string" minOccurs="O"I> 
<xsd:element name="State" type="xsd:string" minOccurs="O"I> 
<xsd:element name="Country" type="xsd:string" minOccurs="O"I> 
<xsd:element name="PostalCode" type="xsd:string" minOccurs="O"I> 
<lxsd: sequence> 
<lxsd: complexType> 
An address consists of an optional one- or two-line address followed by 
a city, state, country, and postal code (any or all of which can be omitted). Most 
elements are optional to allow for partial information being stored in the data-
base. Furthermore, an integrity constraint (for example, there must be a city) 
probably should be enforced in the database, not the middle tier. 
To associate Appointments and Contacts, XML ids are used. An id is an 
intradocument link that can be used to uniquely identify an XML element within 
a single document. The following schema fragment declares that each Contact 
element has an associated id attribute: 
<xsd: element name= "Contact"> 
<xsd: complexType> 
<xsd: sequence> 
<xsd: element name= "Name" type=" PersonNameType" I> 
<xsd:attribute name="id" type="xsd:ID" use="required"l> 
<lxsd: complex Type> 
<lxsd: element> 
The following fragment shows that the id attribute of the Appointment/ 
attendeecontact element is a reference to another element in the same 
XML document. 
<xsd: element name=" Appointment"> 
<xsd: complexType> 
<xsd: element name=" attendeecontact" minOccurs= "o" > 
191 

Chapter 13 
192 
< xsd: complexType> 
<xsd:attribute name="id" type="xsd:IDREF" use="required"/> 
</xsd: complexType> 
< /xsd: element> 
< /xsd: complexType> 
< /xsd:element > 
When a result set from a database query is mapped into your XML Schema, 
it is necessary to generate a unique id for each appointment and each schema. 
In the prototype, the generated ids are derived from the unique database keys 
as follows: 
• For Appointments: ''!\' + < database_key> ("Al2345678") 
• For Contacts: "C" + < database_key> ("C987654") 
This simple scheme is necessary because XML ids follow the same syntax as 
identifiers (and therefore can't start with a number). 
NOTE 
Unfortunately, I was never able to get XML ids to work in Cocoon 
(despite lots of fruitless effort). I was able to verify that the XSL translators 
I tried handled XML ids correctly when invoked stand-alone (for example, 
from inside XML Spy), but not when invoked in the Cocoon environment. 
I don't know if this was a bug or a configuration problem. 
ESQL and the XSP Producer 
As discussed previously, XSP is a Cocoon language for producing XML. The 
language contains special tags for intermixing template static text with Java lan-
guage fragments. ESQL is an Apache XML technology for performing SQL queries 
from within an XSP logic sheet. In a nutshell, you can think of ESQL as imple-
menting XML syntax for the JDBC query interface. To see a simple example of 
XSP and ESQL, visit http: //localhost: 8100/ cocoon/ sql/RetrieveSPIMData. xml. 
This example uses the ODBC/JDBC bridge to access the SPIM database and per-
form a query that returns the first and last names of all the Contacts in the SPIM 
database.3 The basic concept is that XSP/ESQL provides a way to iterate through 
rows in a result set and embed the values of individual fields in XML markup. 
3 This is very useful to verify that the SPIM ODBC data source and ESQL database connection 
are working properly. 

The Web Application Prototype 
The actual producer used for the Running Late function is 
http://localhost:8100/cocoon/sql/getCurrentAppointment.xml.ltproducesX]JL 
like that captured in http: I /localhost: 8100/ cocoon/ sql/CurrentAppointment. xml. 
This is the pure content markup from which the presentation markup will be 
derived (see the next section). 
XSP is the Apache Cocoon approach to generating X]JL from any type of 
data source. Because XSP translates into Java, it is capable of generating X]JL 
from any data source accessible through Java, including file systems, databases, 
application servers, beans, legacy systems, messaging systems, and so on. ESQL 
specializes the XSP approach to SQL databases. However, there are other options 
for generating X]JL representations of relational result sets. In most approaches, 
the X]JL markup is derived directly from the database schema-each row is 
delineated by an element whose name is derived from the table or view name, 
and each contains subelements whose names are the same as database column 
names. This form ofX]JL, direct from the database, can easily be transformed 
into middle-tier content markup through an XSL transformation. 
Many databases vendors now supply XML extenders, which accept SQL in 
X]JL syntax and automatically format result sets into X]JL documents. The 
Oracle Technical Network also provides a free generic servlet that takes an 
SQL query as input and returns an X]JL document as the response (see 
http://otn.oracle.com/software/tech/xml/xsql_servlet/software_index.htm-
registration is required). (Despite its source, the servlet is JDBC dependent but 
DBMS independent.) 
Generating the Presentation 
The previous section showed how to generate marked-up content from database 
data. This addressed the issue of how dynamically generated content gets into 
the Web application. The following sections look at how to generate the presen-
tation code to send out to the client browser. Two cases are discussed separately: 
generating the presentation of dynamic content and generating the presentation 
of static content. 
Transforming Content Markup into Presentation Markup 
For dynamic content, the pure content X]JL produced is transformed into pre-
sentation markup by XSL transformations. The following processor instructions 
in the XSP producer specify that the output of the producer should be processed 
using one XSL stylesheet if the media type is "html" and a different sheet if the 
media type is "vxml." 
193 

Chapter 13 
194 
<?cocoon-process type="xsp"?> 
<?cocoon-process type="xslt"?> 
<?xml-stylesheet type="text/xsl" href="RunninglateVXML. xsl" media="vxml" ?> 
<?cocoon-process type="xslt"?> 
< ?xml-sty lesheet type= "text/xsl" href= "RunninglateHTML. xsl" media=" html"? > 
The media type is derived by Cocoon from the user-Agent field in the 
HTTP request header. The user-Agent field identifies the browser that submitted 
the request. In this case, you are only interested in distinguishing between HTML 
browsers and VXML browsers. The following section in cocoon. properties speci-
fies the mappings between user-Agent values and media types4: 
########################################## 
# User Agents (Browsers) 
# 
########################################## 
#NOTE: numbers indicate the search order. This is VERY VERY IMPORTANT since 
# some words may be found in more than one browser description. (MSIE is 
#presented as "Mozilla/4.0 (Compatible; MSIE 4.01; ... ") 
# 
# for example, the "explorer=MSIE" tag indicates that the XSL stylesheet 
# associated to the media type "explorer" should be mapped to those browsers 
# that have the string "MSIE" in their "user-Agent" HTTP header. 
browser.o = vxml=vxml 
browser.1 = vxml=VoiceXML 
browser.2 = html=MSIE 
browser.3 = html=Mozilla 
browser.4 = html=Netscape 
When RunningLateHTML.xsl is applied to getCurrentAppointment.xml, the 
result is LateAppointmentOOl.htrnl. When RunningLateVXML.xsl is applied, 
the result is LateAppointmentOOl.vxml. (The .xml and .xsl files are contained 
in the JRun deployment directory, JRun \servers\default\spim-application \ 
prototype. The .htrnl and .vxml files are on the companion CD at 
VoiceXML \ SPIMApp \Tutorial\ Step 10 .) The transformation can be bench-tested 
in XML Spy by loading CurrentAppointment.xml, setting the project properties 
to specify which XSL transformation to use, and then running the XSL processor. 
Notice that the previously mentioned .xml flles are snapshots of intermediate 
4 Notice that these are not the "default" Cocoon settings-they were set for the SPIM. 

The Web Application Prototype 
results that were captured for debugging and testing. In actual use, the .xml files 
would be transient files used during processing of a single Cocoon request. 
Transforming Static Content into Presentation Markup 
Although it is possible to dynamically generate all pages in a Web application, 
most applications contain at least some static content. For example, the basic 
menu structures of the SPIM application don't change, so it makes sense to treat 
them as static content. When Web applications only dealt with HTML source 
code, static content was handled by simply publishing HTML files into the Web 
server's directory tree and allowing the Web to serve the files directly without 
additional server-side processing. 
With the addition ofVXML (and WML), the situation becomes a little more 
complex. The issue is that even though the HTML and VXML presentations are 
different, the underlying static content is the same. If the corresponding HTML 
and VXML are stored in separate flies, there will be a maintenance headache 
because the Webmaster will have to remember to manually synchronize changes 
to two files. 
The SPIM prototype demonstrates an approach in which the static content is 
written in a content markup dialect that I'll refer to as "Web-Pidgin." Web-Pidgin 
is a "least common denominator" language that can be transformed into either 
HTML or VXML. It also allows pure HTML or pure VXML scripts to be inserted 
where needed. The advantage of using Web-Pidgin is that the static content is 
stored in one source file, which improves maintainability. 
Overview ofWeb-Pidgin 
The XML schema for Web-Pidgin is /jrun/servers/default/spim-application/ 
prototype/webcontent.xsd (see Figure 13-7 for a graphical representation). Root 
element <webcontent> contains subelements for menus, links, and forms. Each 
of these contains various attributes and subelements that capture all the infor-
mation needed to generate HTML or VXML. In addition, the root element also 
contains the elements <html> and <vxml>. These elements "escape" to pure 
HTML or pure VXML, allowing arbitrary HTML and VXML documents to reside 
in the same file. 
195 

Chapter 13 
196 
· ··· ~ ~-~~ ... .! 
··· ·~~-~~ ---·! 
Figure 13-7. XML Schema for Web-Pidgin 
When a Web-Pidgin file is served, it undergoes an XSL transformation. The 
transformation is determined by the media type of the requesting client, as 
described previously. There is one XSL stylesheet that transforms any Web-Pidgin 
document into HTML, and there is another stylesheet that transforms Web-
Pidgin to VXML. These stylesheets determine how the shared Web-Pidgin con-
structs map to HTML and VXML language constructs. For example, in the 
prototype, a Web-Pidgin menu is transformed into an HTML page that contains 
a sequence of anchors (<A HREF= ••• >elements), one for each choice in 
the menu. In VXML, the Web-Pidgin menu appears as a VXML form with 
<choice> elements. 
The SPIM main menu for the prototype is shown in Listing 13-1. To avoid 
confusion between Web-Pidgin elements and HTML or VXML elements, Web-
Pidgin elements are all prefixed by the content: namespace identifier. Notice that 
some data items are common to both presentation styles (for example, the target 
URI of a menu choice is the same in HTML and VXML). In cases where different 
information is required for the different presentation media, the <content: vui> 
element captures prompt/response pairs for voice, while <content :gui> cap-
tures more verbose text for GUis. 

The Web Application Prototype 
Listing 13-2 and Listing 13-3 show the result of transforming the content 
shown in Listing 13-1 into HTML and VXML, respectively. The stylesheet that 
transforms Web-Pidgin into HTML is shown in Listing 13-4. The style sheet 
that transforms Web-Pidgin into VXML is shown in Listing 13-5. 
Listing 13-1. The SPIM Main Menu (Web-Pidgin) 
<?xm1 version="1.0" encoding="UTF-8"?> 
<?cocoon-process type="xslt" ?> 
< ?xm1- stylesheet type="textlxsl" href="WebContentHTML. xsl" media=" html" ?> 
<?cocoon-process type="xslt"?> 
< ?xml- sty lesheet type= "textlxsl" href= "WebContentvXML. xsl" media=" vxml"? > 
<webcontent version=" 1. o" 
application="/SmallExamples/SPIMApplicationBasic.vxml" 
xmlns: content="H: \VoiceXML \SPIMApp\Prototype\webcontent. xsd" > 
<content:menu title="SPIM Main Menu"> 
<content:vui> 
<content: prompt> Your choices are: <enumerate/> 
</content: prompt> 
</content :vui> 
<content: gui> 
Please choose one of the following: 
</content :gui> 
<content: choice target="Calendar. xml" > 
<content: vui> 
<content: prompt>Calendar</content: prompt> 
</content :vui> 
<content :gui>View Calendar< I content :gui> 
</content: choice> 
<content:choice target="ToDo.xml"> 
<content: vui> 
<content: prompt> To-Do</ content: prompt> 
</content :vui> 
<content :gui>Review To-Do List</content :gui> 
</content: choice> 
<content:choice target="AddressBook.xml"> 
<content: vui> 
<content: prompt>Address Book</ content: prompt> 
</content:vui> 
<content :gui>Manage Address Book</content :gui> 
197 

Chapter 13 
198 
</content: choice> 
</content:menu> 
<content:link target="Lateoxml"> 
<content: gui>Running Late o o o </content: gui> 
<content: vui> 
<content:response>late I I'm late</content:response> 
</content :vui> 
<I content: link> 
</webcontent> 
Listing 13-20 The SPIM Main Menu (HTML Presentation) 
<?cocoon-format type="text/html" ?> 
<html> 
<head> 
<meta content="text/html; charset=utf-8" 
http-equiv="content-type"> 
<title>SPIM Main Menu</title> 
</head> 
<hl>SPIM Main Menu</hl> 
<br> 
<hr> 
<em>Please choose one of the following: </em> 
<br> 
<br> 
<br> 
<a href= "Calendar. xml" >View Calendar</ a> 
<br> 
<a href="ToDooxml">Review To-Do List</a> 
<br> 
<a href="AddressBookoxml">Manage Address Book</a> 
<br> 
<a href=" Late o xml" >Running Late o o o <Ia> 
</html> 
Listing 13-30 The SPIM Main Menu (VMXL Presentation) 
<?xml version="loO" encoding="utf-8"?> 
<?cocoon- format type=" text/vxml" ? > 
<vxml version="loO"> 

<menu> 
<prompt>Your choices are: <enumerate/> 
</prompt> 
<choice next="Calendar .xml ">Calendar</choice> 
<choice next="ToDo.xml"> To-Do</ choice> 
<choice next="AddressBook. xml ">Address Book</ choice> 
</menu> 
<link next="Late.xml"> 
<grammar> late I I'm late</grammar> 
</link> 
<lvxml> 
Listing 13-4. Web-Pidgin to HTML Stylesheet 
<?xml version="l.O" encoding="UTF-8"?> 
<xsl:stylesheet version="l.O" 
xmlns:xsl="http://www.w3.org/1999/XSL/Transform" 
xmlns:content="H:\VoiceXML\SPIMApp\Prototype\webcontent.xsd" 
xmlns:fo="http://www.w3.org/1999/XSL/Format" 
exclude-result- prefixes=" xsl content fo" > 
<xsl:template match="/"> 
<xsl: processing-instruction name=" cocoon-format"> 
type="text/html" 
</xsl: processing-instruction> 
<html> 
<xsl: apply-templates select="*" I> 
</html> 
</xsl: template> 
<xsl: template match=" content: menu"> 
<head> 
<title><xsl :value-of select="@title" /></title> 
</head> 
<hl><xsl: value-of select="@title" 1></hl><br/> 
<hr/> 
<em> 
<xsl:apply-templates select="content:gui/node()" 
mode="passthrough"/> 
</em><br/><br/> 
<xsl :for-each select=" content: choice"> 
<br/> 
<a href="{@target}"> 
<xsl: value-of select=" content: gui" I> 
The Web Application Prototype 
199 

Chapter 13 
200 
</a> 
</xsl :for-each> 
</xsl: template> 
<xsl: template match=" content: link"> 
<br/> 
<a href="{@target}"> 
<xsl: value-of select="content: gui" I> 
<Ia> 
<xsl: apply-templates select=" content: html" I> 
</xsl: template> 
<xsl: template match=" content: form"> 
<! -- form processing goes here --> 
</xsl :template> 
<xsl: template match=" content: html" > 
<xsl: apply-templates select="* I node()" mode="passthrough" I> 
</xsl: template> 
<xsl: template match="* I node()" mode="passthrough" > 
<xsl:copy-of select="."!> 
</xsl: template> 
<xsl: template match=" content: vxml" I> 
</xsl: stylesheet> 
Listing 13-5. Web-Pidgin to VXML Stylesheet 
<?xml version="l.O" encoding="UTF-8"?> 
<xsl:stylesheet version="l.O" 
xmlns:xsl="http://www.w3.org/1999/XSL/Transform" 
xmlns:content="H:\VoiceXML\SPIMApp\Prototype\webcontent.xsd" 
xmlns:fo="http://www.w3.org/1999/XSL/Format" 
exclude-result-prefixes="xsl content fo" > 
<xsl:template match="/"> 
<xsl: processing-instruction name=" cocoon-format"> 
type="textlvxml" 
</xsl:processing-instruction> 
<vxml version="l.O"> 
<xsl: apply-templates select="*"/> 
</vxml> 
</xsl:template> 
<xsl:template match="content:menu"> 
<menu> 
<prompt> 
<xsl: apply-templates 

select="content:vui/content:prompt/node()" 
mode="passthrough"/> 
</prompt> 
<xsl:for-each select="content:choice"> 
<choice next="{@target}"> 
<xsl: value -of 
select=" content: vui/content: prompt" I> 
</choice> 
</xsl: for-each> 
</menu> 
<xsl: apply-templates select= "following" I> 
</xsl: template> 
<xsl: template match="content: link"> 
<link next="{@target}"> 
<grammar> 
<xsl:value-of 
select="content:vui/content:response"/> 
</grammar> 
<xsl: apply-templates select= "content: vxml" I> 
</link> 
</xsl: template> 
<xsl:template match="content:form"> 
<!-- form processing goes here -- > 
</xsl: template> 
<xsl: template match=" content: vxml" > 
<xsl:apply-templates select="*lnode()" mode="passthrough"/> 
</xsl: template> 
<xsl:template match="*lnode()" mode="passthrough"> 
<xsl: copy-of select="." I> 
</xsl: template> 
<xsl :template match=" content: html" I> 
</xsl :stylesheet> 
Trying It Out 
Now that everything is set perfectly, it's time to give it a try. Visit the 
following URL from your Web browser or VoiceXML browser: 
http://localhost:8100/SPIMApp/prototype/SPIMMainMenu.xml. 
You should see a window like the one shown in Figure 13-8. 
The Web Application Prototype 
201 

Chapter 13 
202 
SPII\11\tlain 1\tlenu 
Please choose one of the following: 
View Calendar 
Review To Do List 
Manage Address Book 
Running Late .. 
Figure 13-8. TheSPIMmain menu (HTML view) 
If you're using the IBMWebSphere Voice Server SDK, type the following at 
a command-line prompt: 
%IBMVS%\bin/vsaudio_en_US http://localhost:8100/SPIMApp/prototype/SPIMMainMenu.xml 
You should hear the prompt in Dialog 13-l. 
Dialog 13-1. TheSPIM Main Menu VXMLPrompt 
C (computer): Your choices are Calendar, To-Do, Address Book. 
In your chosen medium, you can browse through the menus. Most actions 
are not implemented, except for the Running Late link. Notice that in the HTML 
browser, Running Late appears like the three menu choices above it, because 
both menu choices and links are rendered as HTML links. However, VXML does 
distinguish between menu choices and links. VXML does not prompt for links, so 
it isn't mentioned in the prompt, but saying ''I'm late" will activate the link. 
In an HTML browser, the Running Late link will take you to the page shown 
in Figure 13-9. In a VoiceXML browser, the Running Late link will lead you to 
a dialog that starts with the prompt in Dialog 13-2. 

The Web Application Prototype 
... Ul- ·-
, ..... 
Running Late For· Current Appointment 
Figure 13-9. The Running Late page (HTML view) 
Dialog 13-2. Running Late (VXML Prompt) 
C (computer): You have a scheduled appointment with Susan Abbott at 12:00. Do 
you want to call home, call business,fax a message, or send an e-mail notice? 
Tips for Dissecting the Prototype 
To view generated VoiceXML in its text form from your Web browser, 
visit the following URL: 
http://localhost:8100/SPIMApp/prototype/SPIMMainMenu.xml?user-Agent=vxml. 
This overrides the value of the HTTP user-Agent header, so to the server it 
appears that the request comes from a VoiceXML browser. When the response is 
displayed, it may or may not look like much, depending on the XML capabilities 
of your browser (see Figure 13-10 for a partial listing of how Internet Explorer 5.0 
displays the generated VoiceXML). You can view the generated VoiceXML code 
by selecting View Source (or equivalent) to display the text the browser is render-
ing. Selecting View Source from Internet Explorer 5.0 brings up a Notepad screen 
containing the unsightly, unreadable jumble ofVoiceXML that was actually 
served (see Figure 13-11). I usually cut and paste the content from Notepad into 
a temporary VXML file in XML Spy. In XML Spy, switching to the grid view and 
back to the text view has the side effect of formatting the VoiceXML code so it can 
be examined more easily (see Figure 13-12). 
203 

Chapter 13 
204 
<?xml version=•t.o" encoding=•ur F- 8" ?> 
<!DOCTYPE vxml (View Source for full doc type .. )> 
- <vxml veorsion="t .o•> 
- <menu scope='dlalog' dtmf="false'> 
- <prompt> 
Your choices are: 
<enumerate xmlns: content='H:\ voiceXML \SPIMApp \Pro tot ype\webcontent .Ksd" /> 
</prompt> 
<chooce ne• t=' Caiendar.Mmi'>Calendar</chooce> 
<chooce ne>.t='ToDo.><mi'>To Do</choice> 
<chooce next='AddressBook.><mi' >Address book</choice> 
</menu> 
- <link next =•Lote.Mmt•> 
<grammar> Running late I late I I'm late</grammar> 
</lonk> 
</vxmb 
Figure 13-10. The Running Late function in VXML text 
~ 
SPIMMatnMenu[l) - Not.,pad 
fll(ii!E'J 
<1 x•l v•t:sion• " .I. . 0 " encodinq• •utP-8 '" 1 > 
< IDOCTYPI vx•l SYSTIH '" Att.p : //vvv. voie•x•l. Ot:'~/vo i cexall-0 . dtd'" > 
<vxal v•rs ion•Ml . O'"> 
<•enu><p~o•pt>Your choic•s ar•: 
<•n~•r•t• 
xklns: eont.•nt.• "H.: \ Voic•)QIIL\SPI.KApp\ Proeot.yp•\webcont-•nt.. xsd.'" /> 
</pt:'oapt><choice ne ~t• • Caltn.dar . xal•>Calendar</c:hoice><choice 
next•"foi>o . xal•>to I>o</choi c•><choi c• n•:r.t.••Addr•ssBook. xal•>Ad.d.rtss book</choic•ll-</a•null-
<link next •"Late . xal'"><gt"aaaar>Jh.mning laee I late I I'apos;a la.~•<Jgru..aar >"'/ link >-
</vxal> 
<! -- This p&(ftl V&S Stt:'Vtd in 140 ll.i.llis•cond.s by Cocoon l. e . z 
--)o 
Figure 13-11. The Running Late function in VXML in Notepad 
..::J 

The Web Application Prototype 
<7xml ~e rs i on="l.O " encodlng,·uTF -8"'> 
<IDOCTYPE vxml SYSTEM "http:/ Jwww.voicexml.org/volcexmll -O.dtd"> 
<vxml vefiton='"l .o·> 
<menu> 
<prompt> Your choices are: <enumerate xmlns:content,"H:\ VolceXML\SPIMApp\Prototype\webcontent.xsd" I> 
<(prompt> 
<Chol ce next ="Calendar. xml" >Calendar</ choice> 
<choice next=loOo.xmi·>To Do</choice> 
<Choice next="AddressBook.xmi">Address book </chotee> 
</menu> 
<li nk n~ xt= ·Late .xm r> 
<grammar>Runnlng I ale I late ll&.apos;m lste<tgrammar> 
</link> 
</vxml> 
Figure 13-12. The Running Late function in VXML in XML Spy 
All the "live" files used in the prototype are in directory 
t0·3SAM 
jrun/ servers/ default/ spim-application/ prototype (see Table 13-1). The appli-
cation works by visiting the XML files (.xrnl). The .xsl, .xsd, and .dtd files are used 
during Cocoon processing of the XML files. Schema files are in the "spim" sub-
directory (see Table 13-2). The Samples subdirectory contains samples generated 
during development of the prototype (see Table 13-3). 
205 

Chapter 13 
206 
Table 13-1. Index of Files in the Prototype Directory 
FILE 
AddressBook.xml 
CurrentAppointment.xml 
notlmplemented.xml 
Calendar.xml 
Late.xml 
DESCRIPTION 
Web-Pidgin for menu of the SPIM Address 
Book functions. 
Sample of pure content markup generated from 
database by Cocoon XSP processor. 
Web-Pidgin file for unimplemented functions in the 
SPIM prototype. 
Web-Pidgin for menu of the SPIM Calendar functions. 
Web-Pidgin uses scripting to get the current time and 
pass it to server to process a late appointment (see 
getCurrentAppointment.xml). 
getCurrentAppointment.xml 
Server-side Cocoon XSP page that takes the current 
SPIMMainMenu.xml 
ToDo.xml 
spim.xsl 
RunningLateVxml.xsl 
RunningLateHTML.xsl 
WebContentHTML.xsl 
WebContentVXML.xsl 
time and user name, performs an ESQL database 
query to get the current appointment, and generates 
pure content markup, which is then transformed by 
RunningLateVXML.xsl or RunningLateHTML.xsl. 
Web-Pidgin for the main menu of the SPIM prototype. 
Web-Pidgin for menu of the SPIM To-Do 
List functions. 
:XSL transforms the SPIM contact markup to HTML 
(used for viewing data from the database). 
:XSL transforms pure content markup generated by 
"running late" database query to VXML. 
:XSL transforms pure content markup generated by 
"running late" database query to HTML. 
:XSL transforms Web-Pidgin to HTML. 
XSL transforms Web-Pidgin to VXML. 

The Web Application Prototype 
Table 13-2. Index of Files in the SPIM Subdirectory 
FILE 
Spim.dtd 
webcontent.xsd 
SPIM.xsd 
Address.xsd 
PhoneNumber.xsd 
DESCRIPTION 
Data '!YPe Descriptor for the SPIM pure content markup 
language (generated from SPIM.xsd) 
XML Schema for Web-Pidgin 
XML Schema for SPIM pure content markup 
XML Schema complex type definition for an address (used 
in SPIM.xsd) 
XML Schema complex type definition for a phone number 
(used in SPIM.xsd) 
Table 13-3. Index of Files in the Samples Directory 
FILE 
LateAppointmentOO l.html 
LateAppointmentOO 1. vxml 
CurrentAppointment.xml 
RetrieveContactNames.xml 
DESCRIPTION 
Sample output derived by applying 
RunningLateVXML.xsl to 
CurrentAppointment.xml. 
Sample output derived by applying 
RunningLateVXML.xsl to 
CurrentAppointment.xml. 
Sample of pure content markup generated 
from database by Cocoon XSP processor. 
Test XSP processor used to verify SPIM 
database connection. It retrieves all people's 
names and displays them in an HTML table. 
RunningLateRawVXMLOutput.vxml 
Raw output from Cocoon processor that 
names.xsl 
Summary 
would be sent to VoiceXML browser. 
XSL transforms list of names generated by 
RetrieveContactNames.xml into HTML. 
If you've made it this far and you've successfully executed the SPIM Web appli-
cation prototype, congratulations! As you've discovered, the hard work in 
prototyping is all the setup and configuration required to get all the components 
207 

Chapter 13 
208 
playing together. Problem diagnosis and resolution can be tremendously 
frustrating, because the hardest problems spring like weeds from the cracks 
between components, where there are no debugging tools. This is the "dark side" 
of component-based development: As an integrator, you spend most of your 
time on unglamorous administrative activities and very little time on the fun 
activity of writing code. 
The SPIMWeb prototype brings together representative technologies neces-
sary to create a small-scale, multi tier, working Web application. XML Spy is the 
XMLIDE used to develop and bench-testXML, XSL, and schemas. The Web 
browser is up to you to choose (I used MSIE). The VoiceXML browser is the IBM 
Via Voice SDK. JRun plays the part ofWeb server and servlet container. Microsoft 
Access, acting as a nonproprietary ODBC/JDBC data source, plays the role of 
database. Apache Cocoon provides the framework for generating and transform-
ing XML in the Web server. 
NOTE 
The set of components was selected based on capability and easy 
availability to developers. They may or may not be suitable for production. 
People who have seen the prototype often ask me anxiously about the "pro-
duction worthiness" of Cocoon. There are several parts to my answer. First, 
open-source projects such as Cocoon are usually not about making "pro-
duction grade" software. Second, the prototype uses version Cocoon 1.8.2, 
the latest "released" version, but Cocoon 2.0 is under way and has signifi-
cant changes from 1. 8.2. Last, Cocoon is not commercial software, and 
hence is not supported. However, Cocoon is not the only game in town: 
Many EAI vendors supply frameworks for XML processing, Oracle sells an 
XML processing framework, and upcoming Java standards are specifying 
increasing levels ofXML support in standard]2EE environments. The bot-
tom line is this: Don't confuse the prototype with production, and don't 
think that the expeditious choice of developer-friendly components for the 
prototype is an endorsement for use in production. 
With the prototype infrastructure in place, "coding" the SpiM application 
becomes an exercise in developing XML documents, writing XSL stylesheet 
transformations, and defining XML schemas to model the data at various steps in 
the transformational process. Samples of these various XML coding artifacts are 
available on the book's companion CD. Hopefully, these can serve as models for 
you to assemble your own application. 

CHAPTER 14 
What's Next? 
VmcEXML IS A NEW technology that has wide applicability and technical depth. As 
with any new technology, the current version is useful, but minimal. Over time, the 
standard will become richer in terms of functions and features incorporated into 
the VoiceXML standard itself, as well as its integration with other related standards. 
In trying to understand the future directions of the VoiceXML specification, it 
is important to bear in mind an important pragmatic fact of life: Any approved 
standard is a cocktail of one part technology, one part politics, and one part expe-
dience, shaken and stirred thoroughly. The fact of the matter is that technology 
standards are written by many people, often representing multiple businesses, 
who all have a vested interest in establishing a standard. However, everyone's 
interests are not the same. Vendors are interested in creating a standard because 
it establishes a market and decreases customers' risk in purchasing a product. 
However, each vendor also wants its product to be distinguished as the best. 
Therefore, sometimes there are things that obviously ought to be standardized 
from a technological standpoint but, as a pragmatic matter, can't get through the 
standards process. 
The following sections examine some of the possible future directions for 
voice applications and the VoiceXML standard in particular. 
Changes from VoiceXML 1.0 to VoiceXML 2.0 
As this book is being written, the specification for VoiceXML 2.0 is being finalized 
and is nearly ready for public release. While the details of the specification are 
considered confidential by the W3C, certain general features of the new specifi-
cation have been revealed publicly. Throughout the book, I've tried to call out 
areas which will be affected by known features ofVoiceXML 2.0. Some of the new 
features are reviewed in this section. Notice that this information is tentative-
not final-and this section is intended to show the direction that VoiceXML 
is moving. 
Anticipated changes include: 
• Incorporation of required support for the new W3C Speech Recognition 
Grammar Format and W3C Speech Synthesis Markup Language 
209 

Chapter 14 
210 
• Improved, clarified mechanism for passing of tag data from the speech rec-
ognizer to VoiceXML interpreter 
• Expanded, clarified features for controlling resource fetching and caching 
• Expanded, improved features for telephony 
From the perspective of a VoiceXML developer or user, VoiceXML 2.0 will 
probably be an evolutionary improvement on VoiceXML 1.0. Based on the scope 
of changes know at this time, VoiceXML 2.0 will offer no revolutionary new fea-
tures or vastly expanded capabilities. This is not to say that VoiceXML 2.0 will not 
be a significant step forward. The development and incorporation of the stan-
dard grammar format and speech synthesis markup will dramatically increase 
the robustness, applicability, and portability of the whole W3C voice browser 
standards suite. VoiceXML 2.0 shores up the foundations of the entire language, 
which is important but not particularly glamorous or visible to everyone. 
Grammar and Speech Synthesis Specification 
The VoiceXML 1.0 specification is silent on the topic of how grammars are speci-
fied. It mentions JGSF and GSL as candidates, but requires neither and leaves 
the choice of grammar specification language open to the implementer of 
a VoiceXML interpreter. This approach was probably a political necessity in the 
standardization process given that the VoiceXML consortium consists of multiple 
vendors with existing products based on incompatible, proprietary grammar lan-
guages. However, it does leave a gaping hole in the portability ofVoiceXML 
programs. Grammars are an essential part of a VoiceXML application, and 
the portability of grammars is as important to overall application portability as 
portability of the presentation markup code. 
The W3C has released a draft standard grammar specification language 
called Speech Recognition Grammar Specification for the W3C Speech Interface 
Framework (http: I /www. w3. org/TR/ speech-grammar/). The draft describes two 
syntaxes for the language: an Augmented Backus Naur Form (ABNF) syntax 
(familiar to language and compiler designers) and an equivalent XML-compliant 
syntax, which will be required by the next version ofVoiceXML. 
A snippet example of a grammar for saying the name of one of four cities is 
as follows: 

<?xml version="l.O"?> 
<grammar xml:lang="en" version="l.O"> 
<rule id="city" scope="public"> 
<one-of> 
<item> new york </item> 
<item> sydney </item> 
<item> boston </item> 
<item> berlin </item> 
</one-of> 
</rule> 
</grammar> 
As you can see, it looks more XMLy than the JGSF you have been 
using. For more information, read VoiceXML Review's article titled 
"Introduction to the W3C Grammar Format" 
(http://www.voicexml.org/Review/Apr2001/features/w3c-grammarl.html). 
The W3C has also released a draft specification of Speech Synthesis 
Markup Language (SSML) (http://www.w3.org/TR/speech-synthesis). This 
is an XML markup language for supporting synthesis of speech from text 
input. For a fuller description, read VoiceXML Review's article titled "The 
Speech Synthesis Markup Language for the W3C VoiceXML Standard" 
(http://www.voicexml.org/Review/Apr2001/features/ssml2.html). 
Reusable Dialog Components 
VoiceXML defines the syntax for using a handful of built-in types (for example, 
date, boolean, and so on), but it does not specify the behavior of the types. For 
example, in VoiceXML you can specify that a field contains a date, but VoiceXML 
does not specify what utterances are recognized as dates.1 Although the concept 
leads in the right direction, the data types as currently specified are not particu-
larly useful. Because their behavior is not specified, they are not portable. 
Because no grammar language is specified, the VoiceXML specification is silent 
on the issue of how built-in types are accessed from custom grammars. 
In the longer term, the goal should be to have a library of standard, portable 
dialog components that can be assembled to produce customized grammars and 
VoiceXML dialogs with a minimum of coding work. In addition to the general 
benefits of reusability, such a standard library would also be the foundation of 
localizable dialogs. 
1 For example, one voice platform might recognize "Tuesday next," "next Tuesday," and "Janu-
ary fourteen'' as valid dates, while another might accept only utterances of the form 
"January fourteen two thousand and one." 
What's Next? 
211 

Chapter 14 
212 
The W3C has already begun work on this concept2 and has produced 
a draft requirements specification for Reusable Dialog Components 
(http: I /www.w3. org/TR/reusable-dialog-reqs). Currently, the draft identifies 
the following dialog components: 
• Yes/no 
• Natural numbers 
• Simple digit string 
• Fully specified date 
• Time 
• Currency 
• Menu 
• Partially specified date 
• Simple alpha string 
• Simple alphanumeric string 
• Simple error-recovery dialog 
• Context-compensating date 
• Telephone number 
• Sectioned digit string 
• Sectioned alphanumeric string 
2 Unfortunately, at the time of this writing, work on this standard was on hold in order to 
focus on getting the VoiceXML 2.0 standard out. There are some indications that while the 
concept of reusable dialog components is technically attractive to application developers, 
it may fall into an area where vendors agree to disagree for the purpose of distinguishing 
their products. 

• Postal code 
• Spelled name 
• Spoken and spelled name 
• Credit card information 
• E-mail address 
• Time range 
• Duration 
• URL 
• Confirmation and correction dialog 
• Browsable selection list 
• Browsable action list 
• Address 
• Non-fixed alphanumeric string 
Multimodal Interfaces 
As currently conceived, HTML, WML, and VoiceXML are mutually exclusive. 
When you implement an interface, you pick one and that's it. Multimodal inter-
faces, which allow you to intermix elements of visual, voice, and even gestural 
behavior into a single interface, are a topic of study in research labs around the 
world. Surfing the Web using a multimodal interface, for example, could involve 
using a mouse or touch screen for graphical links and filling in forms by dictation 
(without requiring the keyboard). This mixed style of interaction is of particular 
interest for speakers of Asian languages, for whom the complexity of written Ian-
guage makes keyboards especially awkward to use. 
Although the result may prove easy to use, development of multimodal inter-
faces is architecturally and technically complex. A multimodal interface must 
maintain real-time synchronization between the various modes of interaction. 
New interface paradigms and protocols will have to be developed to manage a sin-
gle discourse that switches dynamically from one medium to another. So, expect to 
see multimodal interfaces in the future-but maybe not in the near future. 
What's Next? 
213 

Chapter 14 
214 
Architectural Issues 
In their desire to agree on something and produce a useful result in a short 
amount of time, the codifiers of the VoiceXML specification were understandably 
forced to focus quite narrowly on defining the presentation language. Concern-
ing bigger picture architectural issues, they were forced to collectively avert their 
eyes, because standardizing architecture is broader in scope, takes longer, and is 
harder to agree on. However, the problem with not addressing architectural 
issues is that fundamental properties of a real application, such as security, open-
ness, and performance, cannot be handled in a standard way, which dilutes the 
usefulness of the standard. 
Over time, ifVoiceXML is successful, surrounding architectural issues are 
guaranteed to be addressed, but perhaps not by a standards or industry body. For 
example, no real application is going to be deployed without some sort of built-in 
security (even though security may have been considered out of scope of the 
VoiceXML language, the requirements must be met). Options for system-wide 
capabilities for security, performance management, administration, and reliabil-
ity include the following: 
• Roll your own: Build an application-specific capability. 
• Speech vendor supplied: Use proprietary features of the underlying 
speech platform. 
• OS supplied: Use proprietary features of the operating system (for example, 
Microsoft Windows). 
• Run-time environment supplied: Use features of the run-time environment 
(for example, J2EE). 
Most likely, applications will incorporate some combination of these options. 
With both the Microsoft community and the Java community apparently 
embracing XML as a core technology, I expect to see VoiceXML become another 
component technology that is assimilated into the competing frameworks from 
most platform vendors. 
Integration with Enterprise Technologies 
By "enterprise technologies," I am referring to the entire heavy-duty computing 
infrastructure that businesses rely on to make their applications scalable, robust, 
reliable, and available. Some of these commercial-grade technologies include 
the following: 

• Database management systems 
• Transaction monitors 
• Concurrency control 
• Messaging 
• Performance and load managers 
• Replication and failover 
For the most part, these technologies have their roots in conventional, trans-
actional styles of business processing. However, the increasing use of the Web 
for both e-commerce and customer service is driving integration of these 
middleware and back-room technologies with Web front ends. Software vendors 
are starting to provide these enterprise technologies in various middleware 
packages for use by Web applications: Application servers focus on hosting com-
ponent functions in a robust environment; Enterprise Application Integration 
servers focus on providing easy, reliable connectivity to existing back-end appli-
cations; Enterprise Information Portals focus on transforming and moving 
content between producers and consumers. 
As a Web presentation medium, VoiceXML will have to become firmly tied 
in with enterprise technologies. The prototype developed earlier used Cocoon 
as a framework, which was useful in illuminating the important points in the 
transformational architecture. However, Cocoon is primarily an XML publish-
ing framework, and it currently adds no value in integration with enterprise 
technologies. 
Addressing Security Concerns 
Before VoiceXML can gain widespread acceptance, people will need to feel com-
fortable about the security of information exchanged using VoiceXML. Security is 
a complex and multilayered subject, on both technical and psychological levels. 
Curiously, the technical and psychological aspects don't necessarily reinforce one 
another in a strictly rational manner. In the psychological realm, "Perception is 
reality." Consider the following scenarios: 
• You give your credit card to a stranger, who physically disappears with it 
for about 15 minutes, and then returns with a charge slip that you assume 
is the only imprint taken of your card. 
WhatsNext? 
215 

Chapter 14 
216 
• You call an 800 number, talk to an operator, place an order, and give 
your confidential credit card details to the operator over the public 
phone network. 
• You visit an e-commerce site on the Internet, place an order, and upload 
your confidential credit card details to the vendor's computer using RSA 
encryption of transmitted and received information. 
Which scenario is the riskiest? Obviously, there's no "correct" answer, but the 
third scenario certainly has sparked extensive popular attention and debate, 
while the first and second scenarios remain relatively unexamined (and hence, 
assumed "safe"). In fact, in the first scenario, the stranger (for example, a waiter) 
has ample opportunity to make multiple imprints of your card, and that person 
will have a physical copy of your signature. In the second scenario, the inter-
action takes place over the PSTN, which guarantees no security whatsoever. 
The third scenario is the only one in which any sort of technology is applied 
specifically for the purposes of security, and it's a high-powered technology at 
that. However, that doesn't affect the perception that buying things on the Inter-
net is risky. 
So, what's the verdict on VoiceXML? VoiceXML effectively combines the sec-
ond and third scenarios. The connection between a human and a VoiceXML 
gateway is usually over the PSTN. The connection between the VoiceXML gate-
way and a Web site is (presumably) over an encrypted connection. One can argue 
that VoiceXML really doesn't add anything new to the mix, and therefore it 
shouldn't increase or decrease security concerns relative to existing procedures. 
On the other hand, from the perspective of computer security specialists, 
VoiceXML adds another way to access applications from outside. Computer 
security specialists may perceive this as a potential "hole in the firewall" and 
want to restrict public access. Restricting access runs counter to the promise of 
VoiceXML, which is to broaden and simplify access to applications. 
Will people perceive a voice-enabled application as "just a phone call" 
where the other speaker happens to be a computer? Or will people perceive it 
as a potentially threatening interaction with a computer cleverly disguised as 
a phone call? Whatever the perception, there will probably be changes to the 
VoiceXML architecture and language to address such concerns. 
Summary 
VoiceXML is an important emerging technology. As the first release of a standard 
developed through the voluntary cooperation of competing vendors under time 
pressure, VoiceXML 1.0 is minimal and understandably has some warts. However, 
it draws strength from the experience gained from years of work on speech 

recognition, speech synthesis, and XML document processing. Because control 
of the VoiceXML standard was passed to the W3C, it has the potential to mature 
into a true foundation technology over time. The W3C is expected to release the 
VoiceXML 2.0 standard in late 2001. That release will begin a process of reinforc-
ing the core VoiceXML language and integrating it with other W3C voice and Web 
technologies, as well as with Java Enterprise technologies. 
What's Next? 
217 

Appendix A 
A Quick Reference to VoiceXML 1.0 Syntax 
Information in the following tables is excerpted from the VoiceXML 1.0 specifi-
cation (http: I /www. voicexml. org/ specs/VoiceXML -100. pdf). Page numbers in 
Table A-1 refer to page numbers in the VoiceXML 1.0 specification (PDF format). 
TableA-1. VoiceXML 1.0 Tags and Attributes 
TAG 
PURPOSE 
ATTRIBUTES 
DESCRIPTION 
<assign> 
Assign a variable a value. 
name 
The name of the variable 
being assigned to. 
expr 
The new value of the variable. 
<audio> 
Play an audio clip within 
src 
The URI of the audio prompt. 
a prompt. 
See Appendix E (of the VoiceXML 
specification) for suggested 
audio file formats. 
caching 
See Table A-8. 
fetchtimeout 
See Table A-8. 
fetch hint 
See Table A-8. 
<block> 
A container of (non-
name 
The name of a form item variable 
interactive) executable 
used to track whether this block is 
code. 
eligible to execute; defaults to an 
inaccessible internal variable. 
expr 
The initial value of the form item 
variable; default is ECMAScript 
undefined. If initialized to a value, 
then the form item will not be 
visited unless the form item 
variable is cleared. 
cond 
A boolean condition that must 
also evaluate to true in order for 
the form item to be visited. 
PAGE 
71 
46 
54 
219 

Appendix A 
TableA-1. VoiceXML 1.0 Tags and Attributes (continued) 
TAG 
PURPOSE 
ATTRIBUTES 
DESCRIPTION 
PAGE 
<break> 
JSML element to insert 
msecs 
The number of milliseconds 
44 
a pause in output. 
to pause. 
size 
A relative pause duration. 
Possible values are: none, small, 
medium, or large. 
<catch> 
Catch an event. 
event 
The event or events to catch. 
38 
count 
The occurrence of the event 
(default is 1). The count allows 
you to handle different occur-
rences of the same event differently. 
Each form item and <menu> 
maintains a counter for each event 
that occurs while it is being visited; 
these counters are reset each 
time the <menu> or form item's 
<form> is re-entered. 
cond 
An optional condition to test to 
see if the event is caught by this 
element (as in <catch>). 
Defaults to true. 
<choice> 
Define a menu item. 
dtmf 
The DTMF sequence for this choice. 
28 
next 
The URI of next dialog or document. 
event 
Specify an event to be thrown 
instead of specifying a next. 
expr 
Specify an expression to evaluate 
instead of specifying a next. 
caching 
See Table A-8. 
fetchaudio 
See Table A-8. 
fetch hint 
See Table A-8. This defaults to the 
documentfetchhint property. 
fetchtimeout 
See Table A-8. 
<clear> 
Clear one or more form 
name list 
The names of the form items to 
72 
item variables. 
be reset. When not specified, all 
form items in the current form 
are cleared. 
<disconnect> Disconnect a session. 
76 
<div> 
JSML element to classify 
type 
Possible values are sentence 
44 
a region of text as a par-
or paragraph. 
ticular type. 
220 

A Quick Reference to VoiceXML 1.0 Syntax 
TableA-1. VoiceXML 1.0 Tags and Attributes (continued) 
TAG 
PURPOSE 
ATTRIBUTES 
DESCRIPTION 
PAGE 
<dtmf> 
Specify a touch-tone 
SIC 
The URI specifying the location 
35 
key grammar. 
of the grammar, if it is external. 
scope 
Either document, which makes the 
grammar active in all dialogs of 
the current document (and 
relevant application leaf docu-
ments).or dialog, to make the 
grammar active throughout 
the current form. If omitted, the 
grammar scoping is resolved by 
looking at the parent element. 
type 
The MIME type of the grammar. 
If this is omitted, the interpreter 
context will attempt to determine 
the type dynamically. 
caching 
See Table A-8. 
fetch hint 
See Table A-8. This defaults to 
the grammarfetchhint property. 
fetchtimeout 
See Table A-8. 
<else> 
Used in <if> elements. 
72 
<elseif> 
Used in <if> elements. 
72 
<emp> 
JSML element to change 
level 
Specifies the level of emphasis. 
44 
the emphasis of 
Possible values are: strong, 
speech output. 
moderate (default). none or 
reduced. 
<enumerate> 
Shorthand for enumerat-
28 
ing the choices in a menu. 
<error> 
Catch an error event. 
count 
The event count (as in <catch>). 
39 
cond 
An optional condition to test to 
see if the event is caught by this 
element (as in <catch>). 
Defaults to true. 
<exit> 
Exit a session. 
expr 
A return expression (e.g. "0", 
75 
or "oops!"). 
name list 
Variable names to be returned to 
interpreter context. The default is 
to return no variables; this means 
the interpreter context will receive 
an empty ECMAScript object. 
221 

Appendix A 
TableA-1. VoiceXML 1.0 Tags and Attributes (continued) 
TAG 
<field> 
PURPOSE 
Declares an input 
field in a form. 
ATTRIBUTES 
name 
expr 
cond 
type 
slot 
DESCRIPTION 
The field item variable in the dia-
log scope that will hold the result. 
The initial value of the form item 
variable; default is ECMAScript 
undefined. If initialized to a value, 
then the form item will not be 
visited unless the form item 
variable is cleared. 
A boolean condition that must 
also evaluate to true in order for 
the form item to be visited. 
The type of field (i.e., the name of 
an internal grammar). This name 
must be from a standard set sup-
ported by all conformant 
platforms. If not present, 
<grammar> and/or <dtmf> 
elements can be specified instead. 
The name of the grammar slot 
used to populate the variable (if it 
is absent, it defaults to the variable 
name). This attribute is useful in 
the case where the grammar format 
being used has a mechanism for 
returning sets of slot/value pairs 
and the slot names differ from the 
field item variable names. If the 
grammar returns only one slot, as 
do the built-in type grammars like 
boolean, then no matter what the 
slot's name, the field item variable 
gets the value of that slot. 
modal 
If this is false (the default) all 
222 
active grammars are turned on 
while collecting this field. If this 
is true, then only the field's 
grammars are enabled: all others 
are temporarily disabled. 
PAGE 
50 

A Quick Reference to VoiceXML 1.0 Syntax 
Table A-I. VoiceXML 1.0 Tags and Attributes (continued) 
TAG 
PURPOSE 
ATTRIBUTES 
DESCRIPTION 
PAGE 
<filled> 
An action executed when 
mode 
Either all (the default), or any. If 
64 
fields are filled. 
any, this action is executed when 
any of the specified fields is filled 
by the last user input. If all, this 
action is executed when all of the 
mentioned fields are filled, and at 
least one has been filled by the last 
user input. A <filled> element 
in a field item cannot specify 
a mode. 
name list 
The fields to trigger on. For 
a <filled> in a form, name list 
defaults to the names (explicit 
and implicit) of the form's field 
items. A <filled> element in 
a field item cannot specify 
a namelist; the namelist in this 
case is the field item name. 
<form> 
A dialog for presenting 
id 
The name of the form. 
17 
information and 
collecting data. 
scope 
The default scope of the form's 
grammars. If it is dialog then the 
form grammars are active only in 
the form. If the scope is document, 
then the form grammars are active 
during any dialog in the same 
document. If the scope is docu-
ment and the document is an 
application root document, then 
the form grammars are active 
during any dialog in any docu-
ment of this application. A form 
grammar that has dialog scope is 
active only in its form. 
<goto> 
Go to another dialog in 
next 
The URI to which to transition. 
73 
the same or different 
document. 
expr 
An ECMAScript expression that 
yields the URI. 
nextitem 
The name of the next form item 
to visit in the current form. 
223 

Appendix A 
TableA-1. VoiceXML 1.0 Tags and Attributes (continued) 
TAG 
PURPOSE 
ATTRIBUTES 
DESCRIPTION 
PAGE 
<goto> 
expritem 
An ECMAScript expression that 
(continued) 
yields the name of the next form-
item to visit. 
caching 
See Table A-8. 
fetchaudio 
See Table A-8. 
fetchhint 
See Table A-8. This defaults to the 
documentfetchhint property. 
fetchtimeout 
See Table A-8. 
Exactly one of next, expr, nexti tern, or expri tern 
must be specified. 
<grammar> 
Specify a speech recog-
src 
The URI specifying the location 
35 
nition grammar. 
of the grammar, if it is external. 
scope 
Either document, which makes the 
grammar active in all dialogs of 
the current document (and 
relevant application leaf docu-
ments), or dialog, to make the 
grammar active throughout 
the current form. If omitted, the 
grammar scoping is resolved by 
looking at the parent element. 
type 
The MIME type of the grammar. 
If this is omitted, the interpreter 
context will attempt to deter-
mine the type dynamically. 
caching 
See Table A-8. 
fetchhint 
See Table A-8. This defaults to the 
grammarfetchhint property. 
fetchtimeout 
See Table A-8. 
<help> 
Catch a help event. 
count 
The event count (as in <catch>). 
39 
cond 
An optional condition to test to 
see if the event is caught by this 
element (as in <catch>). Defaults 
to true. 
<if> 
Simple conditional logic. 
cond 
Condition to test. 
72 
<initial> 
Declares initial logic upon 
name 
The name of a form item variable 
55 
entry into a (mixed-
used to track whether the <initial> 
initiative) form. 
is eligible to execute; defaults to 
an inaccessible internal variable. 
224 

TableA-1. VoiceXML 1.0 Tags and Attributes (continued) 
TAG 
<initial> 
(continued) 
<link> 
PURPOSE 
Specify a transition 
common to all dialogs in 
the link's scope. 
ATTRIBUTES 
expr 
cond 
next 
expr 
A Quick Reference to VoiceXML 1.0 Syntax 
DESCRIPTION 
The initial value of the form 
item variable; default is 
ECMAScript undefined. If initial-
ized to a value, then the form 
item will not be visited unless the 
form item variable is cleared. 
A boolean condition that must 
also evaluate to true in order for 
the form item to be visited. 
The URI to go to. This URI is 
a document (perhaps with an 
anchor to specify the starting 
dialog), or a dialog in the current 
document (just a bare anchor). 
Like next, except that the URI is 
dynamically determined by evalu-
ating the given ECMAScript 
expression. 
PAGE 
30 
event 
The event to throw when the user 
<menu> 
A dialog for choosing 
amongst alternative 
destinations. 
caching 
fetchaudio 
fetchhint 
fetchtimeout 
id 
scope 
matches one of the link grammars. 
Note that only one of next, expr, 
or event may be specified. 
See Table A-8. 
See Table A-8. 
See Table A-8. This defaults to the 
documentfetchhint property. 
See Table A-8. 
The identifier of the menu. It 
allows the menu to be the target 
of a <goto> or a <submit>. 
The menu's grammar scope. If it 
is dialog-the default-the 
menu's grammars are only active 
when the user transitions into the 
menu. If the scope is document, its 
grammars are active over the whole 
document (or ifthe menu is in the 
application root document, any 
loaded document in the application). 
28 
225 

Appendix A 
TableA-1. VoiceXML 1.0 Tags and Attributes (continued) 
TAG 
<menu> 
(continued) 
<meta> 
<noinput> 
<nomatch> 
<object> 
226 
PURPOSE 
Define a meta data item 
as a name/value pair. 
Catch a noinput event. 
Catch a nomatch event. 
Interact with a custom 
extension. 
ATTRIBUTES 
dtmf 
name 
content 
http-equiv 
count 
cond 
count 
cond 
name 
expr 
cond 
classid 
codebase 
DESCRIPTION 
PAGE 
When set to true, any choices that 
do not have explicit DTMF 
elements are given the implicit 
ones "1," "2," etc. 
The name of the meta-data property. 
66 
The name of the meta-data property. 
The name of an HTTP response 
header. Either name or http-equiv 
must be specified, not both. 
The event count (as in <catch>). 
An optional condition to test to see 
if the event is caught by this element 
(as in <catch>). Defaults to true. 
The event count (as in <catch>). 
An optional condition to test to see 
if the event is caught by this element 
(as in <catch>). Defaults to true. 
When the object is evaluated, it 
sets this variable to an ECMAScript 
value whose type is defined by 
the object. 
The initial value of the form item 
variable; default is ECMAScript 
undefined. If initialized to a value, 
then the form item will not be 
visited unless the form item 
variable is cleared. 
A boolean condition that must 
also evaluate to true in order for 
the form item to be visited. 
The URI specifying the location of 
the object's implementation. The 
URI conventions are platform-
dependent. 
The base path used to resolve 
relative URis specified by classid, 
data, and archive. It defaults to the 
base URI of the current document. 
39 
39 
60 

A Quick Reference to VoiceXML 1.0 Syntax 
Table A -1. VoiceXML 1. 0 Tags and Attributes (continued) 
TAG 
PURPOSE 
ATTRIBUTES 
DESCRIPTION 
PAGE 
<object> 
codetype 
The content type of data expected 
(continued) 
when downloading the object 
specified by classid. When absent 
it defaults to the value of the 
type attribute. 
data 
The URI specifying the location 
of the object's data. If it is a relative 
URI, it is interpreted relative to 
the codebase attribute. 
type 
The content type of the data 
specified by the data attribute. 
archive 
A space-separated list of URis for 
archives containing resources 
relevant to the object, which may 
include the resources specified by 
the classid and data attributes. 
URis which are relative are inter-
preted relative to the codebase 
attribute. 
caching 
See Table A-8. 
fetchaudio 
See Table A-8. 
fetchhint 
See Table A-8. This defaults to the 
obj ectfetchhint property. 
fetchtimeout 
See Table A-8. 
<option> 
Specify an option in 
dtmf 
The DTMF sequence for this 
53 
a<field>. 
option. 
value 
The string to assign to the field item 
variable when a user selects this 
option, whether by speech or 
DTME The default value for this 
attribute is the CDATA content of 
the <option> element with leading 
and trailing white space removed. 
<param> 
Parameter in <object> 
name 
The name to be associated with 
69 
or <subdialog>. 
this parameter when the object 
or sub-dialog is invoked. 
expr 
An expression that computes the 
value associated with name. 
value 
Associates a literal string value 
with name. 
227 

Appendix A 
Table A-1. VoiceXML 1.0 Tags and Attributes (continued) 
TAG 
PURPOSE 
ATTRIBUTES 
DESCRIPTION 
PAGE 
<param> 
valuetype 
One of data or ref, by default data; 
(continued) 
used to indicate to an object if the 
value associated with name is data 
or a URI (ref). This is not used for 
<subdialog>. 
type 
The MIME type of the result pro-
vided by a URI if the valuetype 
is ref; only relevant for uses of 
<param> in <object>. 
<prompt> 
Queue TIS and audio 
barge in 
Control whether a user can inter-
44 
output to the user. 
rupt a prompt. Default is true. 
cond 
An expression telling if the prompt 
66 
should be spoken. Default is true. 
count 
A number that allows you to emit 
different prompts if the user is 
doing something repeatedly. If 
omitted, it defaults to "1." 
timeout 
The timeout that will be used for 
the following user input. The default 
noinput timeout is platform-specific. 
<property> 
Control implementation 
name 
The name of the property to set. 
platform settings. 
See Table A-7. 
value 
The value of the property. 
<pros> 
JSML element to change the rate 
Specifies the speaking rate. 
44 
prosody of speech output. 
val 
Specifies the output volume. 
pitch 
Specifies the pitch. 
range 
Specifies the pitch range. 
<record> 
Record an audio sample. 
name 
The field item variable that will 
61 
hold the recording. 
expr 
The initial value of the form item 
variable; default is ECMAScript 
undefined. If initialized to a value, 
then the form item will not be 
visited unless the form item 
variable is cleared. 
cond 
A boolean condition that must 
also evaluate to true in order for 
the form item to be visited. 
228 

A Quick Reference to VoiceXML 1.0 Syntax 
Table A -1. VoiceXML 1. 0 Tags and Attributes (continued) 
TAG 
PURPOSE 
ATTRIBUTES 
DESCRIPTION 
PAGE 
<record> 
modal 
If this is true (the default) all higher 
(continued) 
level speech and DTMF grammars 
are turned off while making the 
recording. If this is false, speech 
and DTMF grammars scoped to the 
form, document, application, and 
calling documents are listened for. 
Most implementations will not 
support simultaneous recognition 
and recording. 
beep 
If true, a tone is emitted just prior 
to recording. Defaults to false. 
maxtime 
The maximum duration to record. 
finalsilence 
The interval of silence that 
indicates end of speech. 
dtmfterm 
If true, a DTMF keypress termi-
nates recording. Defaults to true. 
The DTMF tone is not part of 
the recording. 
type 
The MIME format of the resulting 
recording. Defaults to a platform-
specific format. 
<reprompt> 
Play a field prompt when 
73 
a field is revisited after 
an event. 
<return> 
Return from a subdialog. 
event 
Return, then throw this event. 
75 
name list 
Variable names to be returned 
to calling dialog. The default is to 
return no variables; this means 
the caller will receive an empty 
ECMAScript object. 
<sayas> 
JSML element to modify 
phon 
The representation of the Unicode 
44 
how a word or phrase 
International Phonetic Alphabet 
is spoken. 
(IPA)characters that are to be spoken 
instead of the contained text. 
sub 
Defines substitute text to be spoken 
instead of the contained text. 
class 
Possible values are phone, date, 
digits,literal,currency,number 
and time. See Table A-9. 
229 

Appendix A 
TableA-1. VoiceXML 1.0 Tags and Attributes (continued) 
TAG 
<script> 
PURPOSE 
Specify a block of 
ECMAScript client-side 
scripting logic. 
<subdialog> 
Invoke another dialog 
as a sub dialog of the 
current one. 
230 
ATTRIBUTES 
src 
charset 
caching 
fetch hint 
fetchtimeout 
name 
expr 
cond 
modal 
namelist 
src 
method 
enctype 
DESCRIPTION 
PAGE 
The URI specifying the location of 
77 
the script, if it is external. 
The character encoding of the 
script designated by src. 
See Table A-8. 
See Table A-8. This defaults to the 
scriptfetchhint property. 
See Table A-8. 
The result returned from the sub-
dialog, an ECMAScript object 
whose properties are the ones 
defined in the namelist attribute 
of the <return> element. 
The initial value of the form item 
variable; default is ECMAScript 
undefined. If initialized to a value, 
then the form item will not be 
visited unless the form item 
variable is cleared. 
A boolean condition that must 
also evaluate to true in order for 
the form item to be visited. 
Controls which grammars are 
active during the subdialog. If true 
(the default) all grammars active 
in the calling dialog are disabled. 
If false, they remain active. 
Same as name list in <submit>, 
except that the default is to submit 
nothing. Only valid when fetching 
another document. 
The URI of the <subdialog>. 
The request method: get (the 
default) or post. 
The MIME encoding type of the sub-
mitted document. The default is 
application/x-www-form-urlencoded. 
Interpreters may support 
additional encoding types. 
56 

A Quick Reference to VoiceXML 1.0 Syntax 
Table A -1. VoiceXML 1. 0 Tags and Attributes (continued) 
TAG 
PURPOSE 
ATIRIBUTES 
DESCRIPTION 
PAGE 
<subdialog> 
caching 
See Table A-8. 
(continued) 
fetchaudio 
See Table A-8. 
fetchtimeout 
See Table A-8. 
fetchhint 
See Table A-8. 
<submit> 
Submit values to a docu-
next 
The URI to which the query is 
74 
ment server. 
submitted. 
expr 
Like next, except that the URI is 
dynamically determined by evalu-
ating the given ECMAScript 
expression. One of next or expr 
is required. 
name list 
The list of variables to submit. By 
default, all the named field item 
variables are submitted. If a 
namelist is supplied, it may contain 
individual variable references 
which are submitted with the same 
qualification used in the namelist. 
method 
The request method: get (the 
default) or post. 
enctype 
The MIME encoding type of the sub-
mitted document. The default is 
application/x-www-form-urlencoded. 
Interpreters may support 
additional encoding types. 
caching 
See Table A-8. 
fetchaudio 
See Table A-8. 
fetchhint 
See Table A-8. This defaults to the 
documentfetchhint property. 
fetchtimeout 
See Table A-8. 
If an ECMAScript object o is the target of a submit 
then all its (ECMAScript) fields fl, f2, ... are sub-
mitted using the names o.fl, o.f2, etc. 
<throw> 
Throw an event. 
event 
The event being thrown. 
38 
231 

Appendix A 
TableA-1. VoiceXML 1.0 Tags and Attributes (continued) 
TACi 
PURPOSE 
ATTRIBUTES 
DESCRIPTION 
PAGE 
<transfer> 
Transfer the caller to 
name 
The outcome of the transfer 
63 
another destination. 
attempt. (One of: busy, noanswer, 
network_busy, near_end_disconnect, 
far_end_disconnect, 
network_ disconnect.) 
expr 
The initial value of the form item 
variable; default is ECMAScript 
undefined. If initialized to a value, 
then the form item will not be visited 
unless the form item variable 
is cleared. 
cond 
A boolean condition that must 
also evaluate to true in order for 
the form item to be visited. 
dest 
The URI of the destination (phone, 
IP telephony address). 
destexpr 
An ECMAScript expression yielding 
the URI of the destination. 
bridge 
This attribute determines what to 
do once the call is connected. If 
bridge is true, document interpre-
tation suspends until the transferred 
call terminates. If it is false, as soon 
as the call connects, the platform 
throws a telephone. disconnect. 
transfer. 
connecttimeout The time to wait while trying to 
connect the call before returning 
the noanswer condition. Default is 
platform -specific. 
maxtime 
The time that the call is allowed to 
last, or 0 if it can last arbitrarily 
long. Only applies if bridge is true. 
Default is 0. 
<value> 
Insert the value of an 
expr 
The expression to render. 
46 
expression in a prompt. 
class 
The <sayas> class of the variable 
(e.g., phone, date, currency). The 
valid formats are the same as those 
supported in the <sayas> speech 
markup. SeeTableA-9. 
232 

A Quick Reference to VoiceXML 1.0 Syntax 
Table A -1. VoiceXML 1. 0 Tags and Attributes (continued) 
TAG 
<value> 
(continued) 
<var> 
<vxml> 
PURPOSE 
Declare a variable. 
ATTRIBUTES 
mode 
recsrc 
name 
expr 
Top-level element in each 
version 
VoiceXML document. 
base 
lang 
application 
DESCRIPTION 
The type ofrendering: tts (the 
default), or recorded. 
The URI of the audio files to be con-
catenated when mode is recorded. 
The name of the variable that will 
hold the result. 
The initial value of the variable 
(optional). If there is no expr 
attribute, the variable retains its 
current value, if any. Variables start 
out with the ECMAScript value 
undefined if they are not given 
initial values. 
The version ofVoiceXML of this 
document (required). The initial 
version number is 1.0. 
The base URI. 
The language and locale type for 
this document. 
The URI ofthis document's appli-
cation root document, if any. 
Table A -2. VoiceXML Variable Scopes 
VARIABLE SCOPE 
session 
application 
document 
DESCRIPTION 
These are read-only variables that pertain to an entire user 
session. They are declared and set by the interpreter context. 
New session variables cannot be declared by VoiceXML 
documents. See Table A-3. 
These are declared with <var> elements that are children of the 
application root document's <vxml> element. They are initialized 
when the application root document is loaded. They exist while 
the application root document is loaded and are visible to the root 
document and any other loaded application leaf document. 
These variables are declared with <var> elements that are 
children of the document's <vxml> element. They are initialized 
when the document is loaded. They exist while the document is 
loaded, and are visible only within that document. 
PAGE 
7l 
14 
233 

Appendix A 
234 
TableA-2. VoiceXML Variable Scopes (continued) 
VARIABLE SCOPE 
dialog 
(anonymous) 
DESCRIPTION 
Each dialog (<form> or <menu>) has a dialog scope that exists 
while the user is visiting that dialog, and which is visible to the 
element of that dialog. Dialog variables are declared by <var> 
child elements of <form>, by <var> elements inside executable 
content (e.g., <block> content or catch element content), and by 
the various form item elements. The child <var> elements of 
<form> are initialized when the form is first visited. The <var> 
elements inside executable content are initialized when the 
executable content is executed. The form item variables are 
initialized when the form item is collected. 
Each <block>, <filled>, and catch element defines a new 
anonymous scope to contain variables declared in that element. 
Table A -3. VoiceXML 1. 0 Standard Session Variables 
VARIABLE 
session.telephone.ani 
session.telephone.dnis 
DESCRIPTION 
Automatic Number Identification. This variable 
provides the result from the Automatic Number 
Identification service that provides the receiver of 
a telephone call with the number of the calling 
phone. This information is provided only if the 
service is supported, and is undefined otherwise. 
Dialed Number Identification Service. This variable 
provides the result from the Dialed Number 
Identification Service that identifies for the receiver 
of a call the number that the caller dialed. This 
information is provided only if the service is 
supported, and is undefined otherwise. 

A Quick Reference to VoiceXML 1.0 Syntax 
TableA-3. VoiceXML 1.0 Standard Session Variables (continued) 
VARIABLE 
session.telephone.iidigits 
session.telephone.uui 
DESCRIPTION 
Information Indicator Digit. This variable provides 
information about the originating line (e.g., pay 
phone, cellular service, special operator handling, 
prison) of the caller. Telecordia publishes the 
complete list of II digits in Section l of each volume 
of the "Local Exchange Routing Guide". This 
information is provided only if the service is 
supported, and is undefined otherwise. 
User to User Information. This variable returns 
supplementary information provided as part of an 
ISDN call set-up from a calling party. This 
information is provided only if the service is 
supported, and is undefined otherwise. 
TableA-4. VoiceXML 1.0 Predefined Events 
EVENT 
cancel 
telephone.disconnect.hangup 
telephone.disconnect.transfer 
exit 
help 
no input 
nomatch 
DESCRIPTION 
The user has requested to cancel playing of the 
current prompt. 
The user has hung up. 
The user has been transferred unconditionally 
to another line and will not return. 
The user has asked to exit. 
The user has asked for help. 
The user has not responded within the 
timeout interval. 
The user input something, but it was 
not recognized. 
235 

Appendix A 
236 
TableA-5. VoiceXML 1.0 Predefined Errors 
ERROR 
error.badfetch 
error.semantic 
DESCRIPTION 
A failed fetch. This may be the result, for example, of 
a missing document, a malformed URI, a com-
munications error during the process of fetching 
the document, a timeout, a security violation, or 
a malformed document. 
A run-time error was found in the VoiceXML 
document (e.g., a divide by 0, substring bounds 
error, or an undefined variable was referenced). 
error.noauthorization 
The user is not authorized to perform the operation 
requested (such as dialing an invalid telephone 
number, or one for which the user is not allowed 
to call). 
error.unsupported.format 
The requested resource has a format that is not 
supported by the platform (e.g., an unsupported 
grammar format, audio file format, object type, or 
MIME type). 
error.unsupported.element 
The platform does not support the given element. 
For instance, if a platform does not implement 
<record>, it must throw error. unsupported. record. 
This allows an author to use event handling to adapt 
to different platform capabilities. 
Table A-6. VoiceXML 1.0 Built-in Types (Grammars) 
BUILT-IN TYPE 
boolean 
date 
DESCRIPTION 
Inputs include affirmative and negative phrases appropriate to 
the current locale. DTMF 1 is yes and 2 is no. The result is 
ECMAScript true for "yes" or false for "no". The value will be 
submitted as the string "true" or the string "false". If the field value 
is subsequently used in a prompt, it will be spoken as an 
affirmative or negative phrase appropriate to the current locale. 
Valid spoken inputs include phrases that specify a date, including 
a month, day, and year. DTMF inputs are: four digits for the year, 
followed by two digits for the month, and two digits for the day. 
The result is a fixed-length date string with format yyyymmdd 
(e.g., "20000704"). If the year is not specified, yyyy is returned as 
"????";if the month is not specified mm is returned as"??"; and if 
the day is not specified dd is returned as "??". 

A Quick Reference to VoiceXML 1. 0 Syntax 
TableA-6. VoiceXML 1.0 Built-in Types (Grammars)( continued) 
BUILT-IN TYPE 
digits 
currency 
number 
phone 
time 
DESCRIPTION 
Valid spoken or DTMF inputs include one or more digits, 
0 through 9. The result is a string of digits. If the field value is 
subsequently used in a prompt, it will be spoken as a sequence of 
digits. A user can say for example "two one two seven", but not 
"twenty one hundred and twenty-seven". 
Valid spoken inputs include phrases that specify a currency 
amount. For DTMF input, the"*" key will act as the decimal point. 
The result is a string with the format UUUmm.nn, where UUU is 
the three-character currency indicator according to ISO standard 
4217:1995 or null if not spoken by the user. If the field value is 
subsequently used in a prompt, it will be spoken as a currency 
amount appropriate to the current locale. 
Valid spoken inputs include phrases that specify numbers, such as 
"one hundred twenty-three", or "five point three". Valid DTMF 
input includes positive numbers entered using digits and"*" to 
represent a decimal point. The result is a string of digits from 0 to 
9 and may optionally include a decimal point(".") and/or a plus 
or minus sign. 
Valid spoken inputs include phrases that specify a phone number. 
DTMF asterisk"*" represents "x". The result is a string containing 
a telephone number consisting of a string of digits and optionally 
containing the character "x" to indicate a phone number with an 
extension. For North America, a result could be "8005551234x789". 
Valid spoken inputs include phrases that specify a time, including 
hours and minutes. The result is a five-character string in the 
format hhmrnx, where xis one of "a" for AM, "p" for PM, "h" to 
indicate a time specified using 24 hour clock, or"?" to indicate an 
ambiguous time. Input can be via DTME Because there is no 
DTMF convention for specifying AM/PM, in the case of DTMF 
input, the result will always end with "h" or"?". If the field value is 
subsequently used in a prompt, the value will be spoken as a time 
appropriate to the current locale. 
237 

Appendix A 
238 
Table A -7. VoiceXML 1. 0 Predefined Properties 
PROPERTY 
confidencelevel 
sensitivity 
speedvsaccuracy 
DESCRIPTION 
The speech recognition confidence level, a float value in the 
range of 0.0 to 1.0. Results are rejected (a nomatch event is 
thrown) when the engine's confidence in its interpretation 
is below this threshold. A value of 0.0 means minimum 
confidence is needed for a recognition, and a value of 1.0 
requires maximum confidence. The default value is 0.5. 
Set the sensitivity level. A value of 1.0 means that it is highly 
sensitive to quiet input. A value of 0.0 means it is least sensitive 
to noise. The default value is 0.5. 
A hint specifying the desired balance between speed vs. 
accuracy. A value of 0.0 means fastest recognition. A value of 
1.0 means best accuracy. The default is value 0.5. 
completetimeout 
The speech timeout value to use when an active grammar is 
matched. The default is platform-dependent. 
incompletetimeout 
The speech timeout to use when no active grammar has been 
matched. The default is platform-dependent. 
interdigittimeout 
The inter-digit timeout value to use when recognizing DTMF 
input. The default is platform-dependent. 
termtimeout 
The terminating timeout to use when recognizing DTMF 
input. The default value is "Os". 
termchar 
barge in 
timeout 
caching 
audiofetchhint 
The terminating DTMF character for DTMF input recognition. 
The default value is"#". 
The barge in attribute to use for prompts. Setting this to true 
allows barge-in by default. Setting it to false disallows barge-in. 
The default value is "true". 
The time after which a noinput event is thrown by the 
platform. The default value is platform -dependent. 
Either safe to never trust the cache when fetching, or fast to 
always trust the cache. The default value is fast. 
This tells the platform whether or not it can attempt to 
optimize dialog interpretation by pre-fetching audio. The 
value is either safe to say that audio is only fetched when it is 
needed, never before; prefetch to permit, but not require the 
platform to pre-fetch the audio; or stream to allow it to stream 
the audio fetches. The default value is prefetch. 

A Quick Reference to VoiceXML 1.0 Syntax 
Table A -7. VoiceXML 1. 0 Predefined Properties (continued) 
PROPERTY 
DESCRIPTION 
documentfetchhint 
Tells the platform whether or not documents may be pre-
fetched. The value is either safe (the default), or prefetch. 
grammarfetchhint 
Tells the platform whether or not grammars may be pre-
fetched. The value is either prefetch (the default), or safe. 
objectfetchhint 
Tells the platform whether the URI contents for <object> may 
be pre-fetched or not. The values are prefetch (the default), or safe. 
scriptfetchhint 
Tells whether scripts may be pre-fetched or not. The values 
are prefetch (the default), or safe. 
fetchaudio 
The URI of the audio to play while waiting for a document to 
fetchtimeout 
inputmodes 
be fetched. The default is not to play any audio. There are no 
fetchaudio properties for audio, grammars, objects, and scripts. 
The timeout for fetches. The default value is platform -dependent. 
The input modes to enable: dtmf and voice. On platforms that 
support both modes, inputmodes defaults to "dtmf voice". To 
disable speech recognition, set inputmodes to "dtmf". To disable 
DTMF, set it to "voice". One use for this would be to turn off 
speech recognition in noisy environments. Another would be 
to conserve speech recognition resources by turning them off 
where the input is always expected to be DTME 
Table A -8. VoiceXML 1. 0 Resource Fetching Properties 
PROPERTY 
caching 
DESCRIPTION 
Either safe to force a query to fetch the most recent copy of the content, 
or fast to use the cached copy of the content if it has not expired. If not 
specified, a value derived from the innermost caching property is used. 
fetchtimeout The interval to wait for the content to be returned before throwing an 
fetchhint 
error. badfetch event. If not specified, a value derived from the 
innermost fetchtimeout property is used. 
Defines when the interpreter context should retrieve content from the 
server. prefetch indicates a file may be downloaded when the page is 
loaded, whereas safe indicates a file that should only be downloaded 
when actually needed. In the case of a very large file (implying long 
download times) or a streaming audio source, stream indicates to the 
interpreter context to begin processing the content as it arrives and 
should not wait for full retrieval of the content. If not specified, a value 
derived from the innermost relevant *fetch hint property is used. 
239 

Appendix A 
240 
TableA-9. VoiceXML 1.0 Say-As Text Types 
TEXT TYPE 
currency 
date 
digits 
literal 
number 
phone 
time 
DESCRIPTION 
The contained text is a currency amount. Leading and trailing 
currency symbols are ignored. 
The contained text is a date. 
The contained text is a string of digits. 
The contained text is a string literal. 
The contained text is a number. 
The contained text is a phone number. 
The contained text is a time of day. 

Index 
Symbols and Numbers 
# (pound sign) usage in URis, 52 
<!--comment delimiter in "Hello, World!" 
program, 50 
<![CDATA[ ]]>tag 
role in ECMAScript formatting, 79 
in mixed-initiative forms, 64 
-->comment delimiter in "Hello, World!" 
program, 50 
{} (curly braces), 64-65 
20 questions interfaces, stabilizing 
dialogs with, 100 
80/20 rule, applying to VUI design, 88 
A 
ABNF (Augmented Backus Naur Form) 
syntax, 112, 210 
ACID (Atomic, Consistent, Isolated, 
Durable) properties of data-
base transactions, 165 
AddressBook.xml file in SPIM prototype 
directory, 206 
addresses in SPIM XML Schema, 191 
Address.xsd file in SPIM subdirectory, 
207 
Allaire JRun 3.1, installing for SPIM 
sample application, 181-182 
Altova XML Spy, installing for SPIM 
sample application, 182 
Apache Cocoon XML publishing 
technology, 152 
Apache XML Project Web site, 154 
application attribute, role in SPIM 
sample application, 53 
- application defined - event in VXML 
1.0, 117 
application landscape, 163 
application root document 
example of, 53-54 
role in VXML programs, 105 
application servers, 18 
application styles, role in Web appli-
cation architectures, 162-166 
application-wide event handling 
example, 132 
applications 
as clearinghouses, 157 
in VXML programs, 105, 110 
appointments 
role in SPIM sample application, 30 
using directed forms with, 58-61 
"as if by copy" semantics of event 
handling, 129-133 
attributes 
inXML, 144 
resource fetching, 124 
<audio> element or tag 
resource fetching, 123 
specifying input and output with, 
107-108 
audit trails, 125-126 
auditory icons, 97 
Authenticate use case, role in SPIM 
sample application, 27 
authentication 
oversight by application root 
document, 54-56 
performance in SPIM sample 
application, 56 
techniques, 160-161 
authoring tools, 46 
B 
background noise, effect on VUI 
performance, 99 
Balentine and Morgan on speech 
recognition, 89, 95 
bargein feature 
enabling for directed forms, 61 
sample dialog with, 80-81 
supporting orientation in conver-
sations with, 97 
Be Vocal Cafe VXML browser, 46 
241 

Index 
242 
blind call transfers, 121 
<block> control element in form items, 
118 
<break> speech synthesis markup tag, 
113 
bridge attribute in transfer tags, 84-85 
bridging call transfer, 121 
browse. previous application-defined 
event, 81 
browser paradigm, 16 
browsers 
servers, and content: evolution of, 15-16 
VoiceXML, 46 
browsing code, structure of, 79-80 
browsing query results, 79-84 
built-in grammars, distinguishing from 
built-in types and say-as gram-
mars, 84 
c 
caching attribute for fetching resources, 
124 
Calendar.xml file in SPIM prototype 
directory, 206 
callers 
disconnecting and throwing events, 
121 
transferring to other phone numbers, 
121 
Cambridge Voice Studio VXML browser, 
46 
cancel event in VXML 1.0, 115 
candidate nodes, generation byXSL, 147 
canonical XML format, role in XML pub-
lishing architecture, 168 
cascading errors, 96 
<catch> elements 
role in advanced event handling, 131 
role in event handling, 114-115 
CGI (Common Gateway Interface), role 
in generating content dynami-
cally, 17-18 
challenge/response authentication, 160 
<choice> element or tag 
navigating voice Web sites with, 
108-109 
resource fetching, 123 
clickstream, HTML Web site, 125 
Cocoon-based prototype, architecture of, 
177 
Cocoon processing model, 153 
cocoon. properties file, modifying, 
187-188 
Cocoon XML publishing technology, 152 
advisory about, 182-183 
tips, 186-188 
Web site, 183 
cognition, role in speech processing, 8 
collaborative versus transactional appli-
cation processing, 162-166 
collect phase of FIA, 117-118 
combination pipeline approach to gen-
erating multiple markup 
languages, 173-174 
command-line systems and interface 
design, 94-95 
conditional logic, 120 
connectors in XML publishing architec-
ture, 168 
contacts, role in SPIM sample appli-
cation, 30 
content 
dynamic generation of, 17-18 
versus presentation, 15 
content markup 
producing for SPIM database, 190-193 
transforming to presentation markup 
for SPIM sample application, 
193-195 
control items in forms, 118 
control, transferring to other locations, 
120 
conversations 
containment in dialogs, 110 
maintaining orientation in, 96-98 
cookies, role in startingVXML sessions, 
159 
CSV (Comma-Separated Value) format, 
role in SPIM prototype, 188 
curly braces ( {}), 64-65 
CurrentAppointment.xml file, 206-207 
D 
data items, gathering for forms, 57-66 
data versus documents in applications, 
163-164 
database key values, role in dynamic 
grammar generation, 162 
database schema for SPIM prototype, 
188-189 
design principles forVUis 
keeping it simple and doing it well, 
87-88 
maintaining flexibility of, 89 
managing turn-taking, 95-96 
dest attribute in transfer tags, 84-85 
"Developing XML Solutions with 
JavaServer Pages Technology" 
white paper, 172 
development environment, 43-44 

dialog components, reusability of, 211-213 
dialog grammar, scope of, 112 
dialog hierarchy, building in SPIM 
sample application, 52-54 
dialogs, ll 0 
with bargein, 80-81 
breaking into multiple documents in 
SPIM sample application, 53 
invoking through <subdialog> versus 
<goto>, 56 
with knowledgeable person example, 
67-68 
role in VXML programs, 105-106 
stabilizing with safe points, 96 
stabilizing with yes/no interrogation, 
96,100 
transitioning between, 54-57 
and VXML, 21, 38 
dictation systems, 9-10 
directed dialogs, treatment of fields in, 119 
directed forms, 58-61 
<disconnect> element, 121 
disfluency in speech design, minimizing 
effects of, 91-92 
disorientation, managing, 96-98 
distractions, taking time out for, 101-102 
<div> speech synthesis markup tag, 113 
document versus transactional process-
ing architectures, 168-169 
documents, llO 
associating with applications, 53 
versus data in applications, 163-164 
role in VXML programs, 105 
Dragon NaturallySpeaking dictation 
system, 10 
DTDs (document type definitions) 
role inXML, 144 
dtmf attribute in directed forms, 60 
DTMF (Dual Tone Multi-Frequency) 
recognition subsystem, 7, 108 
<dtmf> element or tag 
resource fetching, 123 
specifying input and output with, 
107-108 
dynamic content and sessions, 17-18 
dynamic execution context ofVXML pro-
grams, 106-107 
dynamic grammar generation, 161-162 
E 
earcons, 97 
earphones, 46-47 
ECMAScript 
data structures and expressions, 120 
formatting example, 79 
versus Java, 151 
edit Contact Information use case, role in 
SPIM sample application, 28 
editing tools for VoiceXML, 46 
elements vs tags in XML, 144 
<else> and <elseif> elements, 120 
<emp> speech synthesis markup tag, 
ll3 
enrolling with speaker-dependent voice 
systems, 9 
enterprise technologies, integration of 
VXML with, 214-215 
<enumerate I> tag, using in SPIM 
sample application, 52 
environments and experience levels, 
accommodating in interface 
design, 98-101 
error amplification, preventing, 96 
error handling, improving, 71 
error.* events in VXML 1.0, ll6-ll7 
errors, accommodating in VUI design, 
88-89 
ESQL and XSP Producer, role in SPIM 
database, 192-193 
event handlers in VXML, ll4-117 
event handling, 66-74, 114 
advanced techniques for, 129-139 
at document level, 83 
invoking generated subdialog for, 135 
nesting, 82-83 
event links, 81-83 
events in VXML, 40, 114-117 
events thrown by links, scope of, 81-82 
executable content, 120-121 
experience levels and environments, 
accommodating in interface 
design, 98-101 
expert mode versus novice mode, 
100-101 
external grammars 
F 
versus internal grammars, 161 
specifying, 111 
fetch* attributes for fetching resources, 
124 
FIA (form interpretation algorithm), 
117-120 
field grammar, 93, 111 
<field> element and tag, 118 
versus <initial> tag, 65 
and "as if by copy" semantics, 133 
role in directed forms, 60 
fields in VXML, 39, 109-110 
controlling order of, 110 
with verbose event handling, 133-134 
filled event handler, 67 
Index 
243 

Index 
244 
<filled> element and tag 
in fields, 109-110 
using with subdialogs, 56-57 
using with transfer tags, 85 
form handling, 57-66 
form items 
changing order of, 118 
and form interpretation algorithm, 
117-119 
form-level grammar for mixed-initiative 
forms, 64-65 
Format phase of transformational 
processing model, 158 
formatters, role in XML publishing 
architecture, 168 
formatting data in Web applications, 
169-175 
formatting layout language, role in XSL, 
146 
forms in VXML, 39, 56-57, 106, 109-110 
G 
generated browsing code, structure of, 
79-80 
generated VXML, viewing, 203 
getCurrentAppointment.xrnl file in SPIM 
prototype directory, 206 
"goats" and "sheep" interacting with 
speech recognition 
technology, 89 
<goto> element and tag, 54-56 
resource fetching, 123 
transferring control to other locations 
with,120 
grammar and speech synthesis specifi-
cation, 210-211 
grammar formats 
advisory about, 77 
in VXML 1.0, 112 
in VXML 2.0, 112 
grammar generation, 161-162 
<grammar src="uri"> element, loading 
for external grammar gener-
ation, 161 
<grammar> element and tag 
resource fetching, 123 
role in speech recognition, 111-112 
specifying input and output with, 
107-108 
grammars in VXML, 39-40 
distinguishing from built-in and say-
as types, 84 
precedence during recognition, 93 
role in maximizing speech recog-
nition, 93 
scope of, 111-112 
SPIM example for specifying time 
periods, 75-76 
GSL (Grammar Specification Language), 
112 
guard condition for form items, 118 
GUis (graphical user interfaces), 18-20 
formatting Web application data for, 
169 
versus VXML interfaces, 21 
H 
HAL computer, 9 
headsets, 46-47 
hearing, role in speech processing, 8 
"Hello, World!" program, 49-50 
help event in VXML 1.0, 116 
help prompting, 67,71-73 
Hey Anita FreeSpeech VXML browser, 46 
hosted versus standalone development, 
41-42,43 
HTML (Hypertext Markup Language), 
10-13, 19-21 
HTML Web browsing, 37 
HTML, WML, and VXML: advisory about, 
213 
I 
IDE (Interactive Development 
Environments), role in VXML 
development environment, 43 
ids in SPIM XML Schema, 191-192 
IETF (Internet Engineering Task Force) 
RFC 1521 (MIME), 16 
<if> element, 120 
lnformio Developer Network VXML 
browser, 46 
<initial> control element in form items, 
118 
<initial> tag 
in mixed-initiative dialogs, 119-120 
in mixed-initiative forms, 65-66 
role in formatting query strings, 78 
inline grammar, specifying, 111 
input, VXML elements for, 107-108 
interface design for voice, 94-102 
internal versus external grammars, 161 
interpreters 
advisory about suspension of, 102 
role in handling directed forms, 
60-61 
"Introduction to the W3C Grammar 
Format" article, 211 
items in VXML forms, 39 

IVR (Interactive Voice Response) systems, 
authenticating users of, 160 
IVRU (Interactive Voice Response Unit), 
125 
J 
J2EE servlet/JSP processing model versus 
Cocoon model, 153-154 
Java Architecture for XML Binding, role 
in using JSPs and XML 
together, 172 
Java Speech Markup Language specifi-
cation, 112 
Java versus ECMAScript, 151 
JAXP (JavaAPifor XML Processing) 1.1, 
role in using JSPs and XML 
together, 172 
JSGF (Java Speech Grammar Format), 
112 
JSP 1.2 specification, role in using JSPs 
and XML together, 172 
JSP andXML, formatting Web appli-
cations with, 172-175 
JSP andXSL, formatting Web appli-
cations with, 170-172 
JSP processing model, 150-151 
.jsp suffix on URis, 55 
JSPs (JavaServer Pages) 
L 
overview of, 149-151 
Web site, 151 
LateAppointment001 *flies in Samples 
directory, 207 
Late.xml file in SPIM prototype directory, 
206 
letters and speech recognition, 93 
lexical scoping of event handlers, 130 
<link> element and tag 
for resource fetching, 123 
navigating voice Web sites with, 
108-109 
links in VXML, 39, 52 
login forms, invoking with <subdialog> 
tag, 56-57 
looping, advisory about, 120 
"lost in space", 96-98 
M 
macros, using style sheets as, 136-139 
main menu in SPIM sample application, 
51-52 
markup languages, 10-13 
meaning, speech, and sound, 8-9 
menu navigation in SPIM sample 
application, 50-51 
menu prompts, coding in SPIM sample 
application, 52 
<menu> element 
navigating voice Web sites with, 
108-109 
in SPIM sample application, 51 
menus in VXML, 39, 106 
metalanguages, SGML as, 11 
microphones, 46-47 
MIME (Multipurpose Mail Extension) 
type, 16, 112 
mixed-initiative dialogs, 119-120 
with help, 67-69 
permitting longer utterances with, 
99-102 
mixed-initiative forms, 61-66 
modeling and dialog design, 90-91 
Motorola Mobile ADKVXML browser, 46 
multimodal interfaces, 213 
multiple pipeline approach to generating 
multiple markup languages, 
173-175 
multi tiered Web architectures, 166-175 
N 
name and password authentication, 160 
namelist attribute, using with <submit> 
tag, 55 
names.xsl file in Samples directory, 207 
navigation in VXML, 39 
nested lexical scope, 129 
next attribute in SPIM sample 
application, 52, 54 
noinput events 
in mixed-initiative dialogs, 69-71 
in VXML 1.0, 116 
nomatch events 
in mixed-initiative dialogs, 70-71 
in VXML 1.0, 116 
notlmplemented.xml file in SPIM proto-
type directory, 206 
novice mode versus expert mode, 
100-101 
Nuance V-Builder VXML browser, 46 
numbers and speech recognition, 93 
0 
object model for SPIM sample 
appli-cation, 30-31 
<object> tag, 118, 123, 128-129 
ODBC data source, configuring for SPIM 
sample application, 185-186 
<option> tags in fields, 60 
Index 
245 

Index 
246 
orientation, maintaining in 
conversations, 97-98 
output, VXML elements for, 107-108 
p 
<param> tags, using with sub dialogs, 
56-57 
password and name authentication, 160 
pauses and suspended dialogs, designing 
VUis for, 102 
PDAs (personal digital assistants), 18-19 
PhoneNumber.xsd file in SPIM subdirec-
tory, 207 
pictures, perception of, 5 
PIM (personal information manager), 25 
PINs (personal identification numbers), 
role in hosted application 
development, 41 
pipeline approaches to generating 
multiple markup languages, 
173-174 
platform properties, setting, 127 
POTS (Plain Old Telephone Service), role 
in VXML history, 33 
pound sign (#) in SPIM sample appli-
cation, 52 
presentation 
versus content, 15 
generating for SPIM sample appli-
cation, 193-201 
presentation markup, 11, 143 
process phase ofFlA, 117-118 
Process phase oftransformational pro-
cessing model, 158 
Produce phase of transformational pro-
cessing model, 158 
producers, role in Cocoon processing 
model, 153 
prompt counting, role in event handling, 
73-74 
<prompt> element, specifying input 
and output with, 107-108 
prompts, optimizing, 83 
properties, 127 
<property> tags 
effect of setting properties with, 106 
setting at different scopes, 127 
<pros> speech synthesis markup tag, 
113 
PSTN (Public Switched Telephone Net-
work), role in VXML history, 33 
pure content markup, 143 
Q 
queries and sets, 74-84 
query strings, formatting, 78-79 
R 
recognition events, 67 
<record> element or tag, 107-108, 118 
Render phase of transformational 
processing model, 158 
resource fetching, 123-124 
RetrieveContactNames.xml flle in 
Samples directory, 207 
<return> element or tag, 56, 121 
reusable dialog components, 211-213 
<reusableField> tag, example using 
style sheets, 137-139 
Review Appointments function, 
implementing, 74-75 
Review Schedule list use case, role in 
SPIM sample application, 29-30 
RFC 1521 (MIME), 16 
root document example, 53-54 
Running Late function in VXML text, 
204-205 
Running Late use case, role in SPIM sam-
ple application, 29, 193 
RunningLate* files in SPIM prototype 
directory, 206-207 
s 
safe points, building into interfaces, 96 
Samples directory, files in, 207 
say-as types, distinguishing from built-in 
types and grammars, 84 
<sayas> speech synthesis markup tag, 
83, 113 
scope hierarchy ofVXML programs, 106 
scope of grammars, 111-112 
<script> tag for fetching resources, 123 
security concerns, addressing, 215-216 
select phase ofFlA, 117-118 
server extensions, 18 
server pages and servlets Web architec-
tures, 167 
Servlet 2.2 specification, role in using 
JSPs and XML together, 172 
servlet and server pages Web architec-
tures, 167 
servlets 
versus JSPs, 150 
overview of, 148-149 
sessions 
and dynamic content, 17-18 
starting in VXML, 159 
sets and queries, 74-84 
SGML (Standard Generalized Markup 
Language), 10-12 
"sheep" and "goats" interacting with 
speech recognition technol-
ogy, 89 

sight 
characteristics of, 4 
using with sound, 3-5 
single pipeline approach to generating 
multiple markup languages, 
173 
social security numbers, eliciting with 
platform-supplied dialogs, 
128-129 
software options, 45-47 
sound 
with speech and meaning, 8-9 
using with sight, 3-5 
speaker-dependent versus speaker-
independent speech recog-
nition, 9-10 
speaking, role in speech processing, 8 
speech 
characteristics of, 4 
conveying meaning with, 5 
reasons for merging with Web, 13 
speech design, 90-94 
speech developer accessories, 46-47 
speech processing model, 8 
speech recognition 
characteristics of, 89 
functionality of, 106-107 
maximizing benefits of, 93-94 
and synthesis, 7-10 
types of, 9-10 
speech recognition grammars, 
specifying, 111-112 
speech, sound, and meaning, 8-9 
speech synthesis markup, 112-114 
"The Speech Synthesis Markup Language 
for the W3C VoiceXML 
Standard" article, 211 
Speech Synthesis Markup Language 
Specification for the Speech 
Interface Framework, 84 
SPIM class diagram, 31 
SPIM database, producing content 
markup for, 190-193 
SPIM main menu 
in HTML presentation, 198, 202-203 
in VXML presentation, 198-199 
in Web-Pidgin language, 197-198 
SPIM prototype 
anatomy of, 188-201 
dissecting, 203-207 
files in directory of, 206 
SPIM (simplified personal information 
manager) sample application, 
25 
breaking dialogs into multiple docu-
ments in, 53 
building dialog hierarchy in, 52-54 
coding menu prompts in, 52 
configuring ODBC data source for, 
185-186 
copying, 185 
core objects in, 30-31 
deploying, 182-185 
generating presentation for, 193-201 
installing Allaire JRun3.1 for, 181-182 
installing Altova XML Spy for, 182 
installing WebSphere Voice Server 
SDK 1.5 for, 179-181 
main menu in, 51-52 
menu navigation in, 50-51 
objectives of, 177-178 
prototype setup and installation of, 
178-188 
testing, 201-203 
use case analysis of, 26-30 
SPIM subdirectory, files in, 207 
SPIMXMLSchema, 190-192 
Spim.dtd file in SPIM subdirectory, 
207 
SPIMMainMenu.xml file in SPIM proto-
type directory, 206 
SPIM.xsd file in SPIM subdirectory, 207 
spim.xsl file in SPIM prototype directory, 
206 
SSI (server-side includes), and CGI, 18 
standalone versus hosted development, 
41-42,43 
static content, transforming into presen-
tation markup for SPIM 
sample application, 195-201 
static structure ofVXML programs, 
105-106 
static subdialog invocation example, 
134-135 
static VXML versus dynamic generation, 
55 
style sheets as macros, 136-139 
<subdialog> tag, 118 
versus <object> tag, 128 
resource fetching, 123 
role in formatting query strings, 78 
subdialogs, 56-57 
reusing, 134-136 
role in event handling, 133-136 
<submit next ="uri"> element, role in 
dynamic grammar generation, 
161 
<submit> element and tag, 54-56 
resource fetching, 123 
transferring control to other locations 
with, 120-121 
synthesized speech, 92 
Index 
247 

Index 
248 
T 
tags 
in SGML languages, 11-12 
resource fetching, 123-124 
specifying database key values for 
dynamic grammars with, 162 
for Speech Synthesis Markup 
Language Specification for the 
Speech Interface Framework, 
84 
inXML,144 
talking 
effect on VUI performance, 99 
relationship to understanding, 8 
tapered prompting, using with event 
handling, 70-73 
telephone.* events in VXML 1.0, 115 
telephony features, 84-85, 121 
Tellme products 
blackjack game, 13 
Studio VXML browser, 46 
template transformation language, role 
inXSL,146 
text entry, usefulness of, 44 
<throw> element, role in event 
handling, 114-115 
TIBCO Extensibility XML tools, 46 
time frame utterances, parsing (SPIM), 
76-77 
time periods, recognizing (SPIM), 75-78 
<timeframe> grammar rule (SPIM). 76 
timeout event, generating in mixed-
initiative dialog, 69 
ToDo.xml file in SPIM prototype 
directory, 206 
top-level use case diagram for SPIM 
sample application, 26-27 
transactional versus collaborative appli-
cation processing, 162-166 
transactional versus document process-
ing architectures, 168-169 
<transfer> tag, 84-85, 118 
transformational processing model, 158 
transitions between dialogs, 110 
TTS (Text-to-Speech) synthesis, 7, 
112-114 
turn-taking, managing in VUI design, 
95-97 
u 
UML (Universal Markup Language), 
using with SPIM sample 
application, 25 
understanding, in speech processing 
model, 8 
URis of dialogs, specifying with next 
attribute, 54-55 
use case analysis for SPIM example, 
26-30 
user-Agent field in SPIM sample appli-
cation, deriving media type 
from, 194, 203 
username variable in application root 
document, 54 
users of voice systems, authenticating, 
160-161 
v 
valid XML documents, 145 
<value> tag, referencing variables in 
scope with, 106 
<var> element, 120 
variable condition for form items, 118 
variables in scope, referencing in VXML 
programs, 106, 120 
Via Voice dictation system, 10 
voice fidelity, effect on VUI performance, 
99 
voice gateways, accessing, 126-129 
voice Web browsing, 35-38 
voice Web sites, navigating, 108-109 
VoiceGenie Developer Workshop VXML 
browser, 46 
voicestream events, generating, 125-126 
VoiceXML Forum, formation of, 33 
VoiceXML Forum, speech and telephony 
activities timeline, 34 
VoiceXML. See VXML entries 
Voxeo DesignerVXML browser, 46 
VUI design 
accommodating errors in, 88-89 
core principles of, 87-89 
maintaining flexibility of, 89 
managing tum-taking in, 95-96 
modeling speech in, 90-91 
purpose of, 90 
user and provider satisfaction criteria 
for, 87-88 
VUis (voice user interfaces), 14, 18-19 
auditing, 125 
challenges to, 94 
characteristics of, 94 
core design principles of, 87-89 
formatting Web application data for, 
169 
guidelines for, 94 
user and provider satisfaction criteria 
for, 87-88 
VXMLl.O 
advisory about grammar formats in, 
77 

events, 115-117 
and grammar and speech specifi-
cation, 210-211 
grammar formats in, 112 
markup tags, 112 
versus VXML 2.0, 44-45 
VXML 1.0 to VXML 2.0, changes from, 210 
VXML2.0 
advisory about grammar formats in, 
78 
changes from VXML 1.0 to, 210 
grammar formats in, 112 
Speech Synthesis Markup Language 
Specification for the Speech 
Interface Framework, 113 
support for Speech Recognition 
Grammar Specification for the 
W3C Speech Interface 
Framework, 108 
use of speech markup tags in, 84 
VXML applications, running as stand-
alone or hosted 
configurations, 41-42 
VXML browsers, selection criteria for, 43 
VXML dialogs versus documents, 52 
VXML gateway subsystems, 37-38 
VXML gateways 
functionality of, 41-42 
role in initiating sessions, 159 
VXML interpreters 
advisory about suspension of, 102 
role in handling directed forms, 60-61 
VXML interpreters, advisory about, 44 
VXML programs 
specifying input and output in, 
107-108 
structure of, 105-107 
VXML specification for "as if by" copy 
semantics, 129 
VXML (VoiceXML), 3, 12-13,21 
addressing security concerns, 215-216 
architectural issues, 214-216 
authentication techniques, 160-161 
browsers for experimentation, 46 
development environment, 43-44 
elements of, 38-40 
events and handlers in, 114-117 
history of, 33-35 
integration with enterprise technolo-
gies, 214-215 
introduction to, 7-13 
software options, 45-47 
starting sessions in, 159 
text-based characteristic of, 43 
Web browsing with, 37 
<vxml> element and tag, 110 
w 
in "Hello, World!" program, 50 
in SPIM sample application, 53 
W3C (World Wide Web Consortium) 
oversight ofVXML specifi-
cation, 33-34 
web application architectures, 162-175 
Web applications, formatting operation, 
169-175 
Web browsing session example, 35 
Web-Pidgin language, 195-201 
transformation using HTML 
stylesheet, 199-200 
transformation using VXML 
stylesheet, 200-202 
Web, reasons for merging speech with, 13 
Web sites 
Allaire JRun 3.1, 181 
Apache XML Project, 154 
Be Vocal Cafe VXML browser, 46 
Cambridge Voice Studio VXML 
browser, 46 
Cocoon mail archive, 187 
Cocoon XML publishing technology, 
183 
"Developing XML Solutions with 
JavaServer Pages Technology" 
white paper, 172 
ESQLandXSP, 192 
Hey Anita FreeSpeech VXML browser, 
46 
IETF MIME RFC, 16 
Informio Developer Network VXML 
browser, 46 
Installing Cocoon page, 186 
interoperability ofXML and Java tech-
nologies, 172 
"Introduction to the W3C Grammar 
Format" article, 211 
Java Speech Markup Language specifi-
cation, 112 
"JRun 3.0: Installing Cocoon Servlet" 
article, 186 
JSPs (JavaServerPages), 151 
Motorola Mobile ADKVXML browser, 
46 
Nuance V-Builder VXML browser, 46 
Rational Software, 25 
servlets, 149 
Speech Recognition Grammar, 112 
Speech Synthesis Markup Language 
Specification for the Speech 
Interface Framework, 84 
Tellme Studio VXML browser, 46 
Index 
249 

Index 
250 
Web sites (continued) 
"The Speech Synthesis Markup 
Language for the W3C 
VoiceXML Standard" article, 211 
TIBCO Extensibility XML tools, 46 
UML (Universal Markup Language), 25 
VoiceGenie Developer Workshop 
VXML browser, 46 
VoiceXML Forum, 35 
Voxeo DesignerVXML browser, 46 
VXML software, sources for, 45 
W3CVoice Browser Activity area, 35 
WebSphere products, 46, 179 
WML (Wireless Markup Language), 
20-21 
XHTML,20 
XML editors, 45 
XML (Extensible Markup Language), 
145 
XMLSpy,46 
XSL (XML Stylesheet Language), 147 
<webcontent> root element in XML 
Schema for Web-Pidgin lan-
guage, 195 
WebContentHTML.xsl file in SPIM proto-
type directory, 206 
WebContentVXML.xsl file in SPIM proto-
type directory, 206 
webcontent.xsd file in SPIM subdirec-
tory, 207 
WebSphere products, 45, 46 
WebSphere Voice Server SDK 1.5, 
installing for SPIM sample 
application, 179-181 
well formed XML documents, 145 
wireless phones, using voice to access 
Webwith,36 
withoutSpecialHelp form, 83 
withSpecialHelp form, event handling in, 
82-83 
WML (Wireless Markup Language), 
12-13,20-21 
WUis (wireless user interfaces), 18-19 
X 
XHTML (Extensible HTML), 20 
XML and JSP, formatting data in Web 
applications with, 172-175 
XML-compliant syntax for VXML 2.0 
grammaL 112,210 
XML documents 
components of, 146-147 
valid, 145 
well formed, 145 
XML editors, sources of, 45 
XML extenders for databases, 193 
XML (Extensible Markup Language), 12, 
19 
identifying version of, 49-50 
overview of, 143-145 
and VoiceXML, 7 
Web sites, 145 
XML files, XML IDE for, 45 
XML publishing, 152-154 
XML publishing architecture, 168-169 
XML Schema, 45, 144-145, 195-196 
XML Spy, 45, 46 
XPath expressions, role in XSL directives, 
147 
XSL and JSP, formatting data in Web 
applications with, 170-172 
XSL directives, 147 
XSL transformations, XML IDE for, 45 
XSL (XML Stylesheet Language) 
overview of, 145-14 7 
Web sites, 147 
XSLT template transformation sub-
language, 146 
<xsl:template> element, 147 
XSP and ESQL Producer, role in SPIM 
database, 192-193 
y 
yes/no interrogation, stabilizing dialogs 
with, 96, 100 

Apress'" 
License Agreement (Single-User Products) 
THIS IS A LEGAL AGREEMENT BETWEEN YOU, THE END USER, AND APRESS. BY OPENING THE 
SEALED DISK PACKAGE, YOU ARE AGREEING TO BE BOUND BY THE TERMS OF THIS AGREEMENT. 
IF YOU DO NOT AGREE TO THE TERMS OF THIS AGREEMENT, PROMPTLY RETURN THE UNOPENED 
DISK PACKAGE AND THE ACCOMPANYING ITEMS (INCLUDING WRITTEN MATERIALS AND BINDERS 
AND OTHER CONTAINERS) TO THE PLACE YOU OBTAINED THEM FOR A FULL REFUND. 
APRESS SOFTWARE LICENSE 
1. GRANT OF LICENSE. Apress grants you the right to use one copy of this enclosed Apress software 
program (the "SOFTWARE") on a single terminal connected to a single computer (e.g., with a single 
CPU). You may not network the SOFTWARE or otherwise use it on more than one computer or 
computer terminal at the same time. 
2. COPYRIGHT. The SOFTWARE copyright is owned by Apress and is protected by United States 
copyright laws and international treaty provisions. Therefore, you must treat the SOFTWARE like any 
other copyrighted material (e.g., a book or musical recording) except that you may either (a) make 
one copy of the SOFTWARE solely for backup or archival purposes, or (b) transfer the SOFTWARE to a 
single hard disk, provided you keep the original solely for backup or archival purposes. You may not 
copy the written material accompanying the SOFTWARE. 
3. OTHER RESTRICTIONS. You may not rent or lease the SOFTWARE, but you may transfer the 
SOFTWARE and accompanying written materials on a permanent basis provided you retain no 
copies and the recipient agrees to the terms of this Agreement. You may not reverse engineer, 
decompile, or disassemble the SOFTWARE. If SOFTWARE is an update, any transfer must include 
the update and all prior versions. 
4. By breaking the seal on the disc package, you agree to the terms and conditions printed in the Apress 
License Agreement. If you do not agree with the terms, simply return this book with the still-sealed 
CD package to the place of purchase for a refund. 
DISCLAIMER OF WARRANTY 
NO WARRANTIES. Apress disclaims all warranties, either express or implied, including, but not limited 
to, implied warranties of merchantability and fitness for a particular purpose, with respect to the 
SOFTWARE and the accompanying written materials. The software and any related documentation is 
provided "as is." You may have other rights, which vary from state to state. 
NO LIABILITIES FOR CONSEQUENTIAL DAMAGES. In no event shall be liable for any damages 
whatsoever (including, without limitation, damages from loss of business profits, business interruption, 
loss of business information, or other pecuniary loss) arising out of the use or inability to use this Apress 
product, even if Apress has been advised of the possibility of such damages. Because some states do not 
allow the exclusion or limitation of liability for consequential or incidental damages, the above 
limitation may not apply to you. 
U.S. GOVERNMENT RESTRICTED RIGHTS 
The SOFTWARE and documentation are provided with RESTRICTED RIGHTS. Use, duplication, or 
disclosure by the Government is subject to restriction as set forth in subparagraph (c) (1) (ii) of The 
Rights in Technical Data and Computer Software clause at 52.227-7013. Contractor/manufacturer is 
Apress, 901 Grayson Street, Suite 204, Berkeley, California, 94710. 
This Agreement is governed by the laws of the State of California. 
Should you have any questions concerning this Agreement, or if you wish to contact Apress for any 
reason, please write to Apress, 901 Grayson Street, Suite 204, Berkeley, California, 94710. 


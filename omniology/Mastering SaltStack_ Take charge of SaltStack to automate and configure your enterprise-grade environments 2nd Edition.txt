
Mastering SaltStack
Second Edition
 
 
 
 
5BLFDIBSHFPG4BMU4UBDLUPBVUPNBUFBOEDPO`HVSFZPVS
FOUFSQSJTFHSBEFFOWJSPONFOUT
 
 
 
 
Joseph Hall
       BIRMINGHAM - MUMBAI

Mastering SaltStack
Second Edition
Copyright Â© 2016 Packt Publishing
 
All rights reserved. No part of this book may be reproduced, stored in a retrieval system, or
transmitted in any form or by any means, without the prior written permission of the
publisher, except in the case of brief quotations embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the
information presented. However, the information contained in this book is sold without
warranty, either express or implied. Neither the author, nor Packt Publishing, and its
dealers and distributors will be held liable for any damages caused or alleged to be caused
directly or indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the
companies and products mentioned in this book by the appropriate use of capitals.
However, Packt Publishing cannot guarantee the accuracy of this information.
First published: August 2015
Second edition: November 2016
Production reference: 1211116
1VCMJTIFECZ1BDLU1VCMJTIJOH-UE
-JWFSZ1MBDF
-JWFSZ4USFFU
#JSNJOHIBN
#1#6,
ISBN 978-1-78646-739-3
XXXQBDLUQVCDPN

Credits
Author
Joseph Hall
Copy Editor
Madhusudan Uchil
Reviewer
Peng Yao
Project Coordinator
Judie Jose
Commissioning Editor
Kartikey Pandey
Proofreader
Safis Editing
Acquisition Editor
Namrata Patil
Indexer
Francy Puthiry
Content Development Editor
Amedh Pohad
Production Coordinator
Shantanu N. Zagade
Technical Editor
Mohit Hassija

Foreword
The Mastering SaltStack book is one of my favorite Salt books. This book will get you past
the basics of Salt and into what makes it a truly powerful automation framework. Tools like
Salt Reactor, Thorium and Salt-SSH allow you to get the most out of Salt. The best ways to
take Salt to the next level are revealed in this book, in an easy-to-understand way that will
help you solve your problems.
Joseph Hall is likely the best person to write this book. He is not only a close friend, but has
also been involved with the Salt project from the very beginning, including the early design
of the Salt States system. Joseph is the second person to write code for Salt (apart from me).
He was the first engineer hired by SaltStack.
 
Thomas S. Hatch
Founder and CTO, SaltStack

About the Author
Starting as a support technician and progressing to being a web programmer, QA engineer,
systems administrator, Linux instructor, and cloud engineer, Joseph Hall has touched just
about every area of the modern technology world. He is currently a senior cloud and
integrations engineer at SaltStack. Joseph enjoys working with some of the best minds in the
business with his coworkers and SaltStack's partners. He is also the author of Extending
SaltStack, Packt Publishing.
You can find him on LinkedIn at IUUQTXXXMJOLFEJODPNJOUFDIIBU and on GitHub
at IUUQTHJUIVCDPNUFDIIBU.

About the Reviewer
Peng Yao is an operations engineer. He is the founder and coordinator of the China
SaltStack User Group. He translated Mastering SaltStack to Chinese.

www.PacktPub.com
For support files and downloads related to your book, please visit XXX1BDLU1VCDPN.
Did you know that Packt offers eBook versions of every book published, with PDF and
ePub files available? You can upgrade to the eBook version at XXX1BDLU1VCDPN and as a
print book customer, you are entitled to a discount on the eBook copy. Get in touch with us
at TFSWJDF!QBDLUQVCDPN for more details.
At XXX1BDLU1VCDPN, you can also read a collection of free technical articles, sign up for a
range of free newsletters and receive exclusive discounts and offers on Packt books and
eBooks.
IUUQTXXXQBDLUQVCDPNNBQU
Get the most in-demand software skills with Mapt. Mapt gives you full access to all Packt
books and video courses, as well as industry-leading tools to help you plan your personal
development and advance your career.
Why subscribe?
Fully searchable across every book published by Packt
Copy and paste, print, and bookmark content
On demand and accessible via a web browser

Table of Contents
Preface
1
Chapter 1: Essentials Revisited
6
Executing commands remotely
6
Master and minions
7
Targeting minions
7
Glob
7
Perl Compatible Regular Expression (PCRE)
7
List
8
Subnet
8
Grain
8
Grain PCRE
9
Pillar
9
Compound
10
Nodegroup
10
Using module functions
11
test.ping
12
test.echo
12
test.sleep
13
test.version
13
pkg.install
13
pkg.remove
13
file.replace
14
Other file functions
14
Various user and group functions
14
sys.doc
14
SLS file trees
15
SLS files
15
Tying things together with top files
15
Organizing the SLS directories
16
Using states for configuration management
17
Using include blocks
17
Ordering with requisites
18
require
18
watch
19
onchanges
20
onfail
20
use
20
prereq
21

[ ii ]
Inverting requisites
22
Extending SLS files
22
The basics of grains, pillars, and templates
23
Using grains for minion-specific data
23
Centralizing variables with pillars
25
Managing files dynamically with templates
26
A quick Jinja primer
27
Summary
29
Chapter 2: Diving into Salt Internals
30
Understanding the Salt configuration
30
Following the configuration tree
31
Looking inside /etc/salt/
31
Managing Salt keys
32
Exploring the SLS directories
32
Examining the Salt cache
33
The master job cache
33
The master-side minion cache
35
The external file server cache
36
The minion-side proc/ directory
37
External modules
38
The renderer
38
Rendering SLS files
39
Render pipes
40
Serving templated files
40
Understanding the loader
41
Dynamic modules
41
Execution modules
42
Cloud modules
43
Plunging into the state compiler
44
Imperative versus declarative
44
Requisites
45
High and low states
46
High states
46
Low states
49
Enforcing statefulness
51
name
51
result
52
changes
52
comment
52
Summary
53
Chapter 3: Managing States
54

[ iii ]
Handling multiple states
54
Including other SLS files
54
Spanning multiple environments
57
Using the base environment
57
Breaking out environments
58
Understanding master_tops
59
Using Salt Stack Formulas on GitHub
60
Examining the standard layout
60
formula name
61
pillar.example
61
README
61
LICENSE
61
FORMULA
61
Cloning formulas
62
Using the Salt Package Manager
63
Thinking of SPM as a package manager
63
Thinking of SPM as a repository manager
64
Configuring SPM repositories
65
Downloading repository metadata
65
Creating SPM repositories
66
Set aside a directory to hold packages
66
Share that directory
66
Populate the directory
67
Create the repository metadata
67
Building packages
67
version
67
release
68
summary
68
description
68
name
68
os
68
os_family
68
dependencies
69
optional
69
recommended
69
minimum_version
69
top_level_dir
70
license
70
Where to use SPM
71
SPM configuration
71
spm_conf_file
71
formula_path
71
pillar_path
72
reactor_path
72
spm_logfile
72

[ iv ]
spm_default_include
72
spm_repos_config
72
spm_cache_dir
73
spm_build_dir
73
spm_build_exclude
73
spm_db
73
Summary
73
Chapter 4: Exploring Salt SSH
75
Grappling with SSH
75
Remote shells
75
Using rosters
76
The flat roster
77
host
77
port
78
user
78
passwd
78
sudo
79
priv
79
timeout
79
thin_dir
79
Other built-in rosters
79
scan
79
cache
81
cloud
81
ansible
82
Building dynamic rosters
82
Using Salt SSH
83
Using a Saltfile
84
Salt versus Salt SSH
85
Architecture
85
Performance
86
Understanding the salt-thin agent
86
Building the thin package
87
Including extra modules
88
Deploying the thin package
88
Executing the thin package
89
The Salt SSH shim
90
Preparing for Salt states
90
Running Salt
92
Salt's running data
93
Using the raw SSH mode
94
Caching SSH connections
94
Summary
96

[ v ]
Chapter 5: Managing Tasks Asynchronously
97
Looking at the event system
97
Reviewing the basics
98
The structure of event data
98
Watching event data
99
Installing the event listener
99
Using the event listener
99
Firing custom data
101
Namespacing events
103
Namespacing guidelines
104
Some common events
105
105
106
106
106
107
107
salt/auth
salt/key
salt/minion/minion_id!/start
salt/job/job_id!/new
salt/job/job_id!/ret/minion_id!
salt/presence/present
salt/presence/change
107
Common cloud events
108
108
108
108
109
109
109
110
salt/cloud/vm_name!/creating
salt/cloud/vm_name!/requesting
salt/cloud/vm_name!/querying
salt/cloud/vm_name!/waiting_for_ssh
salt/cloud/vm_name!/deploying
salt/cloud/vm_name!/created
salt/cloud/vm_name!/destroying
salt/cloud/vm_name!/destroyed
110
Salt API events
110
salt/netapi/url_path!
110
Building reactors
111
Configuring reactors
111
Writing reactors
112
Calling execution modules
113
Calling runner modules
114
Calling wheel modules
114
Writing more complex reactors
115
Sending out alerts
115
Using webhooks
118
Reactors calling reactors
121
Using Thorium
122
A word on engines
122
Looking at Thorium basics
122
Enabling Thorium
123
Setting up the Thorium directory tree
123

[ vi ]
Writing Thorium SLS files
124
Using requisites
125
Using the register
126
Looking forward
128
Using the queue system
128
Learning how queues work
128
Adding to the queue
129
Listing queues
129
Listing items in a queue
129
Processing queue items
130
Deleting items from a queue
131
Using queues with the reactor
131
Spreading out State runs
131
Dividing tasks among minions
132
Summary
135
Chapter 6: Taking Advantage of Salt Information Systems
136
Understanding pillar_roots
136
Templating pillar_roots
137
Calling out to other modules
138
Using Salt's external pillars
139
Configuring the etcd pillar
139
Using git_pillar
140
Using the mysql pillar
142
Some final thoughts on external pillars
143
Using multiple external pillars
144
Caching pillar data
144
Understanding SDB
144
Securely storing passwords
145
Staying simple
145
Using SDB URIs
146
Configuring SDB
146
Performing SDB lookups
148
Getting data
149
Setting data
150
Deleting data
150
Comparing pillars and SDB
151
Where the data is generated
151
Differences in drivers
152
Salt Cloud and others
152
Summary
153
Chapter 7: Taking Salt Cloud to the Next Level
154

[ vii ]
Examining the Salt Cloud configuration
155
Global configurations
155
Provider and profile configuration
156
Providers
156
Profiles
157
Extending configuration blocks
158
Using SDB with Salt Cloud
160
Using SDB with OpenStack
161
Using SDB with AWS/EC2
162
Using SDB with other cloud providers
163
Building custom deploy scripts
163
Understanding the Salt Bootstrap script
164
Installing from prebuilt packages
165
Installing from Git
167
Looking back at legacy deploy scripts
168
Writing your own deploy scripts
168
Passing arguments to scripts
170
Using file maps
172
Taking a look at cloud maps
173
Using reactors with Salt Cloud
176
Setting up the event tags
176
Working with autoscale reactors
179
Cloud cache
180
Using cloud cache events
182
Setting up a schedule
182
Catching cloud cache events
183
Summary
186
Chapter 8: Using Salt with REST
187
Looking at Salt's HTTP library
187
Why a Salt-specific library?
188
Using the http.query function
189
GET versus POST
190
Decoding return data
192
Using the http.query state
193
Using http.query with reactors
195
Understanding the Salt API
201
What is the Salt API?
201
Setting up the Salt API
201
CherryPy
202
Tornado
203
WSGI
203

[ viii ]
Creating SSL certificates
204
Configuring authentication
205
The external authentication module
206
Taking your first steps with the Salt API
207
Issuing one-off commands
211
Working with webhooks
211
Reacting with Thorium
213
Security considerations
215
More complex authentication
216
Summary
217
Chapter 9: Understanding the RAET and TCP Transports
218
Comparing RAET and ZeroMQ
219
Starting with HTTP
219
SSH â€“ the old favorite
220
Using ZeroMQ
221
ZeroMQ and security
221
The need for RAET
222
Flow-based programming
223
The pieces of the puzzle
223
Black boxes
223
Shared storage
224
Concurrent scheduling
225
Driving with RAET
226
Configuring RAET
227
The RAET architecture
229
The basics
229
The RAET scheduler
230
Estates and yards
231
Looking at asynchronous programming
231
Cooks in a restaurant
232
Examining the TCP transport
233
Using the TCP transport
233
Summary
234
Chapter 10: Strategies for Scaling
235
All about syndication
235
Different folks, different strokes
235
No need for micromanaging
236
Configuring syndication
237
High availability with multiple masters
237
Built-in high-availability configuration
238

[ ix ]
Old-school high availability
238
The round-robin DNS
239
IP-based load balancing
239
Synchronizing files
240
Base configuration files
240
Synchronizing the nonexternal files
242
Using rsync
243
Using the event reactor
243
Incorporating external data sources
245
The external job cache
245
Using returners on the minions
246
Using the master job cache
247
External filesystems
248
GitFS
248
base
250
root
250
mountpoint
251
user and password
251
insecure_auth
251
pubkey, privkey, and passphrase
251
Other source-control backends
252
SVNFS
253
root and mountpoint
253
trunk
253
branches
253
tags
254
HGFS
254
S3FS
255
One environment per bucket
256
Multiple environments per bucket
256
AzureFS
257
External pillars
257
cmd_yaml/cmd_json
258
git
258
redis
259
mysql
260
Using the master API
261
The Salt keys
261
Configuration
262
The file and pillar roots
262
Using the wheel reactor
262
Using wheel with Thorium
263
Testing the load in the infrastructure
264
Using the minionswarm.py script
264
Swarm internals
265

[ x ]
Summary
265
Chapter 11: Monitoring with Salt
266
Monitoring basics
266
Establishing a baseline
266
Reading the system vitals in Salt
267
status.loadavg
267
status.cpustats
268
status.meminfo
269
status.vmstats
270
disk.usage and status.diskusage
270
status.w
271
status.all_status and status.custom
272
Monitoring with returners
273
Deciding on a returner
274
Using monitoring states
275
Defining a monitoring state
276
Monitoring with web calls
278
Working with beacons
279
Monitoring file changes
280
Beacon intervals
281
Setting up alerts
282
Alerting in state files
282
Alerting from beacons
282
Watching file changes
283
Monitoring bad logins
284
Using aggregate data with Thorium
285
Summary
287
Chapter 12: Exploring Best Practices
288
Future-proofing your infrastructure
288
Setting up your directories
289
Standard directory locations
289
.sls versus init.sls
290
Shallow versus deep
291
Subdividing further
292
The SLS efficiency
293
Includes and extends
293
Using includes
294
Using extends
296
Using templates to simplify SLS files
297
Working with loops
297
Decisions, decisions
299

[ xi ]
Using the built-in states
301
Naming conventions
304
Generic names
304
Explicit names
306
Templates and variables
306
Nested variables
307
Referring to variables in templates
308
Summary
309
Chapter 13: Troubleshooting Problems
310
What theâ€¦?
310
Addressing the problem source
311
Where is the trouble?
311
Master-to-minion communication
311
Network and CPU congestion
312
Checking minion load
313
Querying the Salt job data
315
Using debug and trace modes
317
info
317
warn
318
error
318
debug/trace
318
Running services in debug mode
319
Using salt-call locally
322
Working with YAML
324
YAML basics
324
dict
324
list
324
YAML idiosyncrasies
326
Spacing
326
Numbers
327
Booleans
327
List items
328
Troubleshooting YAML
328
Asking the community for help
330
The salt-users mailing list
330
Asking questions
331
The Salt issue tracker
332
Researching before posting
333
Formatting your issues
334
Requesting features
336
#salt on IRC
337
Final community thoughts
338

[ xii ]
Summary
338
Index
339

Preface
I'm very excited to have been given the chance to put this book together. I've been given the
rare opportunity to watch Salt grow from an idea in the brain of Tom Hatch to an award-
winning open source project, and onward to the flagship product of an award-winning
open source company. Salt has become an incredibly powerful framework, which I wish I'd
had access to years ago.
Every day, I learn something new about Salt. This book is a collection of a number of those
things, aimed at the advanced user. Don't see it as the last word on any of the topics that it
covers. Instead, see it as a guide on your journey to use this tool to its fullest potential.
As you read through this book, I hope that the ideas and examples in it inspire you to
update and innovate your infrastructure.
What this book covers
$IBQUFS, Essentials Revisited, takes a step back to a few of the basics inside of Salt. These
concepts are critical to understanding many of the concepts discussed in this book, and
many of them will be explored further in later chapters.
$IBQUFS, Diving into Salt Internals, goes in depth with the management of Saltâ€™s own
configuration files, how Saltâ€™s loader system works, before finally discussing the state
compiler.
$IBQUFS, Managing States, builds upon the state concepts from the previous chapter and
goes on to discuss how Salt states can be compiled together to form a more cohesive
solution for your organization.
$IBQUFS, Exploring Salt SSH, will provide you with a basic understanding of Saltâ€™s SSH
transport layer, before jumping into a more technical overview of the underlying
components.
$IBQUFS, Managing Tasks Asynchronously, explores some of the subsystems in Salt that are
designed for handling tasks, which interact with each other to achieve an end goal.
$IBQUFS, Taking Advantage of Salt Information Systems, looks at some of the subsystems in
Salt that are designed entirely for data management. The functionality of these subsystems
can be utilized by the master or minion, and sometimes both.

Preface
[ 2 ]
$IBQUFS, Taking Salt Cloud to the Next Level, brings you an understanding of how Salt
interacts with public clouds and how you can extend that interaction to achieve a more
cohesive infrastructure definition.
$IBQUFS, Using Salt with REST, covers using Salt to communicate over HTTP, both as a
client and as a server. With these APIs in place, Salt can either be a cog in another system or
be the system that manages the cogs.
$IBQUFS, Understanding the RAET and TCP Transports, follows a very technical discussion
of various transport layers, both inside and outside of Salt. If you are looking to move
beyond the ZeroMQ transport, this chapter is for you.
$IBQUFS, Strategies for Scaling, explores a number of subsystems designed to allow Salt to
manage large-scale data centers. Even if you are managing a small infrastructure now, I
would encourage you to take a look at the options available here: they may just help you
rethink your existing infrastructure anyway.
$IBQUFS, Monitoring with Salt, explores many of the ways that Salt can be used to extend
your own monitoring system and even bring its own monitoring tools to the party.
$IBQUFS, Exploring Best Practices, helps you know how to most effectively use Salt in a
way that will not only lead to easy management for you, but also for those who work with
or follow you.
$IBQUFS, Troubleshooting Problems, talks about some of the ways that things can go
wrong in Salt and what to do when that happens.
What you need for this book
While this book covers a lot of ground, the focus is on Salt. It is recommended that you use
the latest version of Salt available to you, though most of the information is also relevant to
older versions.
A number of other products and services are mentioned in this book and its examples.
While the examples will be useful with those products and services, they are designed to
still be generic enough to demonstrate the intended information without requiring the
purchase or installation of or subscription to third-party entities.

Preface
[ 3 ]
Any hardware or platform that can support Salt may be used for the examples in this book
that pertain to minion operations. The Salt master is not yet supported in a Windows
environment, so any master-oriented examples will require a Linux or Unix environment.
Who this book is for
This book is ideal for IT professionals and ops engineers who already manage groups of
servers but would like to expand their knowledge and gain expertise with SaltStack. This
book explains the advanced features and concepts of Salt. A basic knowledge of Salt is
required in order to get to grips with advanced Salt features.
Conventions
In this book, you will find a number of text styles that distinguish between different kinds
of information. Here are some examples of these styles and an explanation of their meaning.
Code words in text, database table names, folder names, filenames, file extensions,
pathnames, dummy URLs, user input, and Twitter handles are shown as follows: "Unix
distributions will often use the VTSMPDBMFUDTBMU directory instead, while Windows
uses the $=TBMU=EJSFDUPSZ ."
A block of code is set as follows:
OBNFBQBDIF
PT3FE)BU%FCJBO6CVOUV4VTF'SFF#4%
PT@GBNJMZ3FE)BU%FCJBO4VTF'SFF#4%
WFSTJPO
SFMFBTF
NJOJNVN@WFSTJPO
UPQ@MFWFM@EJSBQBDIF
TVNNBSZ'PSNVMBGPSJOTUBMMJOH"QBDIF
EFTDSJQUJPO'PSNVMBGPSJOTUBMMJOHUIF"QBDIFXFCTFSWFS
PQUJPOBMNPE@QFSM
SFDPNNFOEFENPE@TTM
Any command-line input or output is written as follows:
# mkdir -p /srv/salt/devenv/

Preface
[ 4 ]
New terms and important words are shown in bold. Words that you see on the screen, for
example, in menus or dialog boxes, appear in the text like this: "On the main screen of the
repository, you will see a button titled Clone or download."
Warnings or important notes appear in a box like this.
Tips and tricks appear like this.
Reader feedback
Feedback from our readers is always welcome. Let us know what you think about this
book-what you liked or disliked. Reader feedback is important for us as it helps us develop
titles that you will really get the most out of. To send us general feedback, simply e-
mail GFFECBDL!QBDLUQVCDPN, and mention the book's title in the subject of your
message. If there is a topic that you have expertise in and you are interested in either
writing or contributing to a book, see our author guide at XXXQBDLUQVCDPNBVUIPST.
Customer support
Now that you are the proud owner of a Packt book, we have a number of things to help you
to get the most from your purchase.
Errata
Although we have taken every care to ensure the accuracy of our content, mistakes do
happen. If you find a mistake in one of our books-maybe a mistake in the text or the code-
we would be grateful if you could report this to us. By doing so, you can save other readers
from frustration and help us improve subsequent versions of this book. If you find any
errata, please report them by visiting IUUQXXXQBDLUQVCDPNTVCNJUFSSBUB, selecting
your book, clicking on the Errata Submission Form link, and entering the details of your
errata. Once your errata are verified, your submission will be accepted and the errata will
be uploaded to our website or added to any list of existing errata under the Errata section of
that title.

Preface
[ 5 ]
To view the previously submitted errata, go to IUUQTXXXQBDLUQVCDPNCPPLTDPOUFO
UTVQQPSUand enter the name of the book in the search field. The required information will
appear under the Errata section.
Piracy
Piracy of copyrighted material on the Internet is an ongoing problem across all media. At
Packt, we take the protection of our copyright and licenses very seriously. If you come
across any illegal copies of our works in any form on the Internet, please provide us with
the location address or website name immediately so that we can pursue a remedy.
Please contact us at DPQZSJHIU!QBDLUQVCDPN with a link to the suspected pirated
material.
We appreciate your help in protecting our authors and our ability to bring you valuable
content.
Questions
If you have a problem with any aspect of this book, you can contact us
at RVFTUJPOT!QBDLUQVCDPN, and we will do our best to address the problem.

1
Essentials Revisited
Salt is a very powerful remote automation framework. Before we delve into the more
advanced topics that this book covers, it would be wise to go back and review a few
essentials. In this chapter, we will cover the following topics:
Using remote execution
Basic SLS file tree structure
Using states for configuration management
Basics of grains, pillars, and templates
This book assumes that you already have root access on a device with a common
distribution of Linux installed. The machine used in the examples in this book is running
Ubuntu 14.04, unless stated otherwise. Most examples should run on other major
distributions, such as recent versions of Fedora, RHEL 5/6/7, Suse, or Arch Linux.
Executing commands remotely
The underlying architecture of Salt is based on the idea of executing commands remotely.
This is not a new concept; all networking is designed around some aspect of remote
execution. This could be as simple as asking a remote web server to display a static Web
page, or as complex as using a shell session to interactively issue commands against a
remote server.
Under the hood, Salt is a more complex example of remote execution. But whereas most
Internet users are used to interacting with only one server at a time (so far as they are
aware), Salt is designed to allow users to explicitly target and issue commands to multiple
machines directly.

Essentials Revisited
[ 7 ]
Master and minions
Salt is based around the idea of a master, which controls one or more minions. Commands
are normally issued from the master to a target group of minions, which then execute the
tasks specified in the commands and return any resulting data back to the master.
Targeting minions
The first facet of the TBMU command is targeting. A target must be specified with each
execution, which matches one or more minions. By default, the type of target is a glob,
which is the style of pattern matching used by many command shells. Other types of
targeting are also available, by adding a flag. For instance, to target a group of machines
inside a particular subnet, the 4 option is used:
# salt -S 192.168.0.0/24 test.ping
The following are most of the available target types, along with some basic usage examples.
Not all target types are covered here; Range, for example, extends beyond the scope of this
book. However, the most common types are covered.
Glob
This is the default target type for Salt, so it does not have a command line option. The
minion ID of one or more minions can be specified, using shell wildcards if desired.
When the TBMU command is issued from most command shells, wildcard characters must
be protected from shell expansion:
# salt '*' test.ping
# salt \* test.ping
When using Salt from an API or from other user interfaces, quoting and escaping wildcard
characters is generally not required.
Perl Compatible Regular Expression (PCRE)
Short Option: &
Long Option: QDSF

Essentials Revisited
[ 8 ]
When more complex pattern matching is required, a Perl Compatible Regular Expression
(PCRE) can be used. This type of targeting was added to the earliest versions of Salt, and
was meant largely to be used inside shell scripts. However, its power can still be realized
from the command line:
# salt -E '^[m|M]in.[e|o|u]n$' test.ping
List
Short Option: -
Long Option: MJTU
This option allows multiple minions to be specified as a comma-separated list. The items in
this list do not use pattern matching such as globbing or regular expressions; they must be
declared explicitly:
# salt -L web1,web2,db1,proxy1 test.ping
Subnet
Short Option: 4
Long Option: JQDJES
Minions may be targeted based on a specific IPv4 or an IPv4 subnet in CIDR notation:
# salt -S 192.168.0.42 test.ping
# salt -S 192.168.0.0/16 test.ping
As of Salt version 2016.3, IPv6 addresses cannot be targeted by a specific command line
option. However, there are other ways to target IPv6 addresses. One way is to use grain
matching.
Grain
Short Version: (
Long Version: HSBJO

Essentials Revisited
[ 9 ]
Salt can target minions based on individual pieces of information that describe the machine.
This can range from the OS to CPU architecture to custom information (covered in more
detail later in this chapter). Because some network information is also available as grains, IP
addresses can also be targeted this way.
Since grains are specified as key/value pairs, both the name of the key and the value must
be specified. These are separated by a colon:
# salt -G 'os:Ubuntu' test.ping
# salt -G 'os_family:Debian' test.ping
Some grains are returned in a multi-level dictionary. These can be accessed by separating
each key of the dictionary with a colon:
# salt -G 'ip_interfaces:eth0:192.168.11.38'
Grains which contain colons may also be specified, though it may look strange. The
following will match the local IPv6 address (). Note the number of colons used:
# salt -G 'ipv6:::1' test.ping
Grain PCRE
Short Version: (not available)
Long Version: HSBJOQDSF
Matching by grains can be powerful, but the ability to match by a more complex pattern is
even more so.
# salt --grain-pcre 'os:red(hat|flag)' test.ping
Pillar
Short Option: *
Long Option: QJMMBS
It is also possible to match based on pillar data. Pillars are described in more detail later in
the chapter, but for now we can just think of them as variables that look like grains.
# salt -I 'my_var:my_val' test.ping

Essentials Revisited
[ 10 ]
Compound
Short Option: $
Long Option: DPNQPVOE
Compound targets allow the user to specify multiple target types in a single command. By
default, globs are used, but other target types may be specified by preceding the target with
the corresponding letter followed by the ! sign:
Letter Target
G
Grains
E
PCRE minion ID
P
PCRE grains
L
List
I
Pillar
J
Pillar PCRE
S
Subnet/IP address
R
SECO range
The following command will target the minions that are running Ubuntu, have the SPMF
pillar set to XFC, and are in the  subnet.
# salt -C 'G@os:Ubuntu and I@role:web and S@192.168.100.0/24' test.ping
Boolean grammar may also be used to join target types, including BOE, PS, and OPU
operators.
# salt -C 'min* or *ion' test.ping
# salt -C 'web* or *qa and G@os:Arch' test.ping
Nodegroup
Short Option: /
Long Option: OPEFHSPVQ

Essentials Revisited
[ 11 ]
While nodegroups are used internally in Salt (all targeting ultimately results in the creation
of an on-the-fly nodegroup), it is much less common to explicitly use them from the
command line. Node groups must be defined as a list of targets (using compound syntax) in
the Salt master's configuration before they can be used from the command line. Such a
configuration might look like the following:
OPEFHSPVQT
XFCEFW	*!SPMFXFCBOE(!DMVTUFSEFW	XFCRB	*!SPMFXFCBOE
(!DMVTUFSRB	XFCQSPE	*!SPMFXFCBOE(!DMVTUFSQSPE	
Once a nodegroup is defined and the master configuration reloaded, it can be targeted from
Salt:
# salt -N webdev test.ping
Using module functions
After a target is specified, a function must be declared. The preceding examples all use the
UFTUQJOH function but, obviously, other functions are available. Functions are actually
defined in two parts, separated by a period:
NPEVMF GVODUJPO 
Inside a Salt command, these follow the UBSHFU, but precede any arguments that might be
added for the function:
TBMUUBSHFU NPEVMF GVODUJPO <BSHVNFOUT>
For instance, the following Salt command will ask all minions to return the text, )FMMP
XPSME:
salt '*' test.echo 'Hello world'
A number of execution modules ship with the core Salt distribution, and it is possible to
add more. Version 2016.3 of Salt ships with around 400 execution modules. Not all modules
are available for every platform; in fact, by design, some modules will only be available to
the user if they are able to detect the required underlying functionality.
For instance, all functions in the test module are necessarily available on all platforms.
These functions are designed to test the basic functionality of Salt and the availability of
minions. Functions in the Apache module, however, are only available if the necessary
commands are located on the minion in question.

Essentials Revisited
[ 12 ]
Execution modules are the basic building blocks of Salt; other modules in Salt use them for
their heavy lifting. Because execution modules are generally designed to be used from the
command line, an argument for a function can usually be passed as a string. However,
some arguments are designed to be used from other parts of Salt. To use these arguments
from the command line, a Python-like data structure is emulated using a JSON string.
This makes sense, since Salt is traditionally configured using YAML, and all JSON is
syntactically-correct YAML. Be sure to surround the JSON with single quotes on the
command line to avoid shell expansion, and use double quotes inside the string. The
following examples will help.
A list is declared using brackets:
	<JUFNJUFNJUFN>	
A dictionary is declared using braces (that is, curly brackets):
	\LFZWBMVFLFZWBMVFLFZWBMVF^	
A list can include a dictionary, and a dictionary can include a list:
	<\LFZWBMVF^\LFZWBMVF^>	
	\MJTU<JUFNJUFN>MJTU<JUFNJUFN>^	
There are a few modules which can be considered core to Salt, and a handful of functions in
each that are widely used.
test.ping
This is the most basic Salt command. Ultimately, it only asks the minion to return True. This
function is widely used in documentation because of its simplicity, and to check whether a
minion is responding. Don't worry if a minion doesn't respond right away; that doesn't
necessarily mean it's down. A number of variables could cause a slower-than-usual return.
However, successive failed attempts may be cause for concern.
test.echo
This function does little more than the UFTUQJOH command; it merely asks the minion to
echo back a string that is passed to it. A number of other functions exist that perform similar
tasks, including UFTUBSH, UFTULXBSH, UFTUBSH@UZQF, and UFTUBSH@SFQS.

Essentials Revisited
[ 13 ]
test.sleep
A slightly more advanced testing scenario may require a minion to sleep for a number of
seconds before returning True. This is often used to test or demonstrate the utilities that
make use of the jobs system. The UFTUSBOE@TMFFQ function is also useful for test cases
where it is desirable to check the return from a large number of minions, with the return
process spread out.
test.version
In a large enough infrastructure, a number of minions are bound to be running in a
different version of Salt than others. When troubleshooting issues specific to certain
versions of Salt, it helps to be able to take a quick look at the Salt version on each minion.
This is the simplest way to check that. Checking the version of other packages that are
maintained by the system packaging system can be performed with QLHWFSTJPO.
pkg.install
Every package manager in Salt (as of version 2016.3) supports installing a package. This
function can be as simple as asking for a single package name, or as complex as passing
through a list of packages, each with a specific version. When using an execution module,
you generally do not need to specify more than just a single package name, but inside the
state module (covered later) the advanced functionality becomes more important.
pkg.remove
This matches the QLHJOTUBMl function, allowing a certain package to be removed. Because
versions are not so important when removing packages, this function doesn't get so
complex. But it does allow passing a list of packages to be removed (using the QLHT
argument) as a Python list. From the command line, this can be done using a JSON string.

Essentials Revisited
[ 14 ]
file.replace
The TFE command is one of the oldest members of the Unix administrator's toolkit. It has
been the go-to command largely for tasks that involve editing files inline, and performing
search and replace tasks. There have been a few attempts over the years to duplicate the
functionality of the TFE command. Initially, the GJMFTFE function simply wrapped the
Unix TFE command. The GJMFQTFE function provided a Python-based replacement.
However, TFE is more than just a find/replace tool; it is a full language that can be
problematic when used incorrectly. The GJMFSFQMBDF function was designed from the
ground up to provide the find/replace functionality that most users need, while avoiding
the subtle nuances that can be caused by wrapping TFE.
Other file functions
A number of common Unix commands have been added to the GJMF function. The
following functions complement the Unix command set for managing files and their
metadata: GJMFDIPXO, GJMFDIHSQ, GJMFHFU@NPEF, GJMFTFU@NPEF, GJMFMJOL,
GJMFTZNMJOL, GJMFSFOBNF, GJMFDPQZ, GJMFNPWF, GJMFSFNPWF, GJMFNLEJS,
GJMFNBLFEJST, GJMFNLOPE, and a number of others.
Various user and group functions
The Unix toolset for managing users and groups is also available in Salt and includes
VTFSBEE, VTFSEFMFUF, VTFSJOGP, HSPVQBEE, HSPVQEFMFUF, HSPVQJOGP,
VTFSDIVJE, VTFSDIHJE, VTFSDITIFMM, VTFSDIIPNF, VTFSDIHSPVQT, and many,
many more.
sys.doc
By design, every public function in every execution module must be self-documenting. The
documentation that appears at the top of the function should contain a description just long
enough to describe the general use of the function, and must include at least one CLI
example demonstrating the usage of that function.

Essentials Revisited
[ 15 ]
This documentation is available from the minion using the TZTEPD function. Without any
arguments, it will display all the functions available on a particular minion. Adding the
name of a module will show only the available functions in that module, and adding the
name of a function will show only the documentation for that function, if it is available. This
is an extremely valuable tool, both for providing simple reminders of how to use a function
and for discovering which modules are available.
SLS file trees
There are a few subsystems in Salt that use an SLS file tree. The most common one of course
is TSWTBMU, which is used for Salt states. Right after states are pillars 
TSWQJMMBS),
which use a different file format but the same directory structure. Let's take a moment to
talk about how these directories are put together.
SLS files
SLS stands for Salt State, which was the first type of file inside Salt to use this kind of file
structure. While SLS files can be rendered in a number of different formats, by far the
widest use is the default, YAML. Various templating engines are also available to help form
the YAML (or other data structure) and again, the most popular is the default, Jinja.
Keep in mind that Salt is all about data. YAML is a serialization format that in Python,
represents a data structure in a dictionary format. When thinking about how SLS files are
designed, remember that they are a key/value pair: each item has a unique key, which is
used to refer to a value. The value can in turn contain a single item, a list of items, or
another set of key/value pairs.
The key to a stanza in an SLS file is called an ID. If no name inside the stanza is explicitly
declared, the ID is copied to the name. Remember that IDs must be globally unique;
duplicate IDs will cause errors.
Tying things together with top files
Both the state and the pillar system use a file called UPQTMT to pull the SLS files together
and serve them to the appropriate minions, in the appropriate environments.

Essentials Revisited
[ 16 ]
Each key in a UPQTMT file defines an environment. Typically, a base environment is
defined, which includes all the minions in the infrastructure. Then other environments are
defined that contain only a subset of the minions. Each environment includes a list of the
SLS files that are to be included. Take the following UPQTMT file:
CBTF
		
DPNNPO
WJN
RB
	@RB	
KFOLJOT
XFC
	XFC@	
BQBDIF
With this UPQTMT, three environments have been declared: CBTF, RB, and XFC. The base
environment will execute the DPNNPO and WJN states across all minions. The RB
environment will execute the KFOLJOT state across all the minions whose ID ends with @RB.
The web environment will execute the BQBDIF state across all the minions whose ID starts
with XFC@.
Organizing the SLS directories
SLS files may be named either as an SLS file themselves (that is, BQBDIFTMT) or as an
JOJUTMT file inside a directory with the SLS name (that is, BQBDIFJOJUTMT).
Note that BQBDIFTMT will be searched for first; if it is not there, then
BQBDIFJOJUTMT will be used.
SLS files may be hierarchical, and there is no imposed limit on how deep directories may
go. When defining deeper directory structures, each level is appended to the SLS name with
a period (that is, BQBDIFTTMJOJUTMT becomes BQBDIFTTM). It is considered best
practice by developers to keep a directory more shallow; don't make your users search
through your SLS tree to find things.

Essentials Revisited
[ 17 ]
Using states for configuration management
The files inside the TSWTBMU directory define the Salt states. This is a configuration
management format that enforces the state that a minion will be in: QBDLBHF9 needs to be
installed, GJMF: needs to look a certain way, TFSWJDF; needs to be enabled and running,
and so on. For example:
BQBDIF
QLH
JOTUBMMFE
TFSWJDF
SVOOJOH
GJMF
OBNFFUDBQBDIFBQBDIFDPOG
States may be saved in a single SLS file, but it is far better to separate them into multiple
files, in a way that makes sense to you and your organization. SLS files can use include
blocks that pull in other SLS files.
Using include blocks
In a large SLS tree, it often becomes reasonable to have SLS files include other SLS files. This
is done using an JODMVEF block, which usually appears at the top of an SLS file:
JODMVEF
CBTF
FNBDT
In this example, the SLS file in question will replace the include block with the contents of
CBTFTMT (or CBTFJOJUTMT) and FNBDTTMT (or FNBDTJOJUTMT). This imposes some
important restrictions on the user. Most importantly, the SLS files that are included may not
contain IDs that already exist in the SLS file that includes them.
It is also important to remember that JODMVEF itself, being a top-level declaration, cannot
exist twice in the same file. The following is invalid:
JODMVEF
CBTF
JODMVEF
FNBDT

Essentials Revisited
[ 18 ]
Ordering with requisites
State SLS files are unique among configuration management formats in that they are both
declarative and imperative. They are imperative, as each state will be evaluated in the order
in which it appears in the SLS file. They are also declarative because states may include
requisites that change the order in which they are actually executed. For instance:
XFC@TFSWJDF
TFSWJDFSVOOJOH
OBNFBQBDIF
SFRVJSF
QLHXFC@QBDLBHF
XFC@QBDLBHF
QLHJOTUBMMFE
OBNFBQBDIF
If a service is declared, which requires a package that appears after it in the SLS file, the QLH
states will be executed first. However, if no requirements are declared, Salt will attempt to
start the service before installing the package, because its codeblock appears before the QLH
codeblock. The following will require two executions to complete properly:
XFC@TFSWJDF
TFSWJDFSVOOJOH
OBNFBQBDIF
XFC@QBDLBHF
QLHJOTUBMMFE
OBNFBQBDIF
Requisites point to a list of items elsewhere in the SLS file that affect the behavior of the
state. Each item in the list contains two components: the name of the module and the ID of
the state being referenced.
The following requisites are available inside Salt states and other areas of Salt that use the
state compiler.
require
The SFRVJSF requisite is the most basic; it dictates that the state that it is declared in is not
executed until every item in the list that has been defined for it has executed successfully.
Consider the following example:
BQBDIF
QLH
JOTUBMMFE
SFRVJSF

Essentials Revisited
[ 19 ]
GJMFBQBDIF
TFSWJDF
SVOOJOH
SFRVJSF
QLHBQBDIF
GJMF
NBOBHFE
OBNFFUDBQBDIFBQBDIFDPOG
TPVSDFTBMUBQBDIFBQBDIFDPOG
In this example, a file will be copied to the minion first, then a package installed, then the
service started. Obviously, the service cannot be started until the package that provides it is
installed. But Debian-based operating systems such as Ubuntu automatically start services
the moment they're installed, which can be problematic if the default configuration files
aren't correct. This state will ensure that Apache is properly configured before it is even
installed.
watch
In the preceding example, a new minion will be properly configured the first time.
However, if the configuration file changes, the BQBDIF service will need to be restarted.
Adding a XBUDI requisite to the service will force that state to perform a specific action
when the state that it is watching reports changes.
BQBDIF
4/*1
TFSWJDF
SVOOJOH
SFRVJSF
QLHBQBDIF
XBUDI
GJMFBQBDIF
4/*1
The XBUDI requisite is not available for every type of state module. This is because it
performs a specific action, depending on the type of module. For instance, when a service is
triggered with a XBUDI, Salt will attempt to start a service that is stopped. If it is already
running, it will attempt either a SFMPBE5SVF, TFSWJDFGVMM@SFTUBSU, or
TFSWJDFSFTUBSU, as appropriate.
As of version 2016.3, the following states modules support using the XBUDI requisite:
TFSWJDFQLH, DNE, FWFOU, NPEVMF, NPVOU, TVQFSWJTPSE, EPDLFS, EPDLFSOHFUDE,
UPNDBU, and UFTU.

Essentials Revisited
[ 20 ]
onchanges
The PODIBOHFT requisite is similar to XBUDI, except that it does not require any special
support from the state module that is using it. If changes happen, which should only occur
when a state completes successfully, then the list of items referred to with PODIBOHFT will
be evaluated.
onfail
In a simple state tree, the POGBJM requisite is less commonly used. However, a more
advanced state tree, which is written to attempt alerting the user, or to perform auto-
correcting measures, can make use of POGBJM. When a state is evaluated and fails to execute
correctly, every item listed under POGBJM will be evaluated. Assuming that the 1BHFS%VUZ
service is properly configured via Salt and an BQBDIF@GBJMVSF state has been written to
use it, the following state can notify the operations team if Apache fails to start:
BQBDIF
TFSWJDF
SVOOJOH
POGBJM
QBHFSEVUZBQBDIF@GBJMVSF
use
It is possible to declare default values in one state and then inherit them into another state.
This typically occurs when one state file has an JODMVEF statement that refers to another
file.
If an item in the state that is being used has been redeclared, it will be overwritten with the
new value. Otherwise, the item that is being used will appear unchanged. Requisites will
not be inherited with VTF; only non-requisite options will be inherited. Therefore, in the
following SLS, the NZTRM@DPOG state will safely inherit the VTFS, HSPVQ, and mode from
the BQBDIF@DPOG state, without also triggering Apache restarts:
BQBDIF@DPOG
GJMF
NBOBHFE
OBNFFUDBQBDIFBQBDIFDPOG
VTFSSPPU
HSPVQSPPU
NPEF
XBUDI@JO
TFSWJDFBQBDIF

Essentials Revisited
[ 21 ]
NZTRM@DPOG
GJMF
NBOBHFE
OBNFFUDNZTRMNZDOG
VTF
GJMFBQBDIF@DPOG
XBUDI@JO
TFSWJDFNZTRM
prereq
There are some situations in which a state does not need to run, unless another state is
expected to make changes. For example, consider a web application that makes use of
Apache. When the codebase on a production server changes, Apache should be turned off,
so as to avoid errors with the code that has not yet finished being installed.
The QSFSFR requisite was designed exactly for this kind of use. When a state makes use of
QSFSFR, Salt will first perform a test run of the state to see if the items referred to in the
QSFSFR are expected to make changes. If so, then Salt will flag the state with the QSFSFR as
needing to execute.
BQBDIF
TFSWJDF
SVOOJOH
XBUDI
GJMFDPEFCBTF
DPEFCBTF
GJMF
SFDVSTF
4/*1
TIVUEPXO@BQBDIF
TFSWJDF
EFBE
OBNFBQBDIF
QSFSFR
GJMFDPEFCBTF
In the preceding example, the TIVUEPXO@BQBDIF state will only make changes if the
DPEFCBTF state reports that changes need to be made. If they do, then Apache will
shutdown, and then the DPEFCBTF state will execute. Once it is finished, it will trigger the
BQBDIF service state, which will start up Apache again.

Essentials Revisited
[ 22 ]
Inverting requisites
Each of the aforementioned requisites can be used inversely, by adding @JO at the end. For
instance, rather than state X requiring state Y, an SLS can be written so that state X declares
that it is required by state Y, as follows:
BQBDIF
QLH
JOTUBMMFE
SFRVJSF@JO
TFSWJDFBQBDIF
TFSWJDF
SVOOJOH
It may seem silly to add inverses of each of the states but there is in fact a very good use
case for doing so: include blocks.
SLS files cannot use requisites that point to a code that does not exist inside them. However,
using an include block will cause the contents of other SLS files to appear inside the SLS file.
Therefore, generic (but valid) configuration can be defined in one SLS file, included in
another, and modified to be more specific with a VTF@JO requisite.
Extending SLS files
In addition to an include block, state SLS files can also contain an FYUFOE block that
modifies SLS files that appear in the include block. Using an FYUFOE block is similar to a
use requisite, but there are some important differences.
Whereas a VTF or VTF@JO requisite will copy defaults to or from another state, the extend
block will only modify the state that has been extended.
DBUTSWHFOFSJD@BQBDIFJOJUTMT
BQBDIF@DPOG
GJMF
NBOBHFE
OBNFFUDBQBDIFBQBDIFDPOG
TPVSDFTBMUBQBDIFBQBDIFDPOG

*OEKBOHP@TFSWFSJOJUTMT
JODMVEF
HFOFSJD@BQBDIF
FYUFOE
BQBDIF@DPOG
GJMF
TPVSDFTBMUEKBOHPBQBDIFDPOG

Essentials Revisited
[ 23 ]

*OJNBHF@TFSWFSJOJUTMT
JODMVEF
HFOFSJD@BQBDIF
FYUFOE
BQBDIF@DPOG
GJMF
TPVSDFTBMUEKBOHPBQBDIFDPOG
The preceding example makes use of a generic Apache configuration file, which will be
overridden as appropriate for either a Django server or a web server that is only serving
images.
The basics of grains, pillars, and templates
Grains and pillars provide a means of allowing user-defined variables to be used in
conjunction with a minion. Templates can take advantage of those variables to create files
on a minion that are specific to that minion.
Before we get into details, let me start off by clarifying a couple of things: grains are defined
by the minion which they are specific to, while pillars are defined on the master. Either can
be defined statically or dynamically (this book will focus on static), but grains are generally
used to provide data that is unlikely to change, at least without restarting the minion, while
pillars tend to be more dynamic.
Using grains for minion-specific data
Grains were originally designed to describe the static components of a minion, so that
execution modules could detect how to behave appropriately. For instance, minions which
contain the Debian os_family grain are likely to use the apt suite of tools for package
management. Minions which contain the RedHat os_family grain are likely to use ZVN for
package management.
A number of grains will automatically be discovered by Salt. Grains such as os, os_family,
saltversion, and pythonversion are likely to be always available. Grains such as shell, systemd,
and ps are not likely to be available on, for instance, Windows minions.
Grains are loaded when the minion process starts up, and then cached in memory. This
improves minion performance, because the Salt-minion process doesn't need to rescan the
system for every operation. This is critical to Salt, because it is designed to execute tasks
immediately, and not wait several seconds on each execution.

Essentials Revisited
[ 24 ]
To discover which grains are set on a minion, use the HSBJOTJUFNT function:
salt myminion grains.items
To look at only a specific grain, pass its name as an argument to HSBJOTJUFN:
salt myminion grains.item os_family
Custom grains can be defined as well. Previously, static grains were defined in the minion
configuration file 
FUDTBMUNJOJPO on Linux and some Unix platforms):
HSBJOT
GPPCBS
CB[RVY
However, while this is still possible, it has fallen out of favor. It is now more common to
define static grains in a file called grains (FUDTBMUHSBJOT on Linux and some Unix
platforms). Using this file has some advantages:
Grains are stored in a central, easy-to-find location
Grains can be modified by the grains execution module
That second point is important: whereas the minion configuration file is designed to
accommodate user comments, the grains file is designed to be rewritten by Salt as
necessary. Hand-editing the grains file is fine, but don't expect any comments to be
preserved. Other than not including the grains top-level declaration, the grains file looks
like the grains configuration in the minion file:
GPPCBS
CB[RVY
To add or modify a grain in the grains file, use the HSBJOTTFUWBM function:
salt myminion grains.setval mygrain 'This is the content of mygrain'
Grains can contain a number of different types of values. Most grains contain only strings,
but lists are also possible:
NZ@JUFNT
JUFN
JUFN
In order to add an item to this list, use the HSBJOTBQQFOE function:
salt myminion grains.append my_items item3

Essentials Revisited
[ 25 ]
In order to remove a grain from the grains file, use the HSBJOTEFMWBM function:
salt myminion grains.delval my_items
Centralizing variables with pillars
In most instances, pillars behave in much the same way as grains, with one important
difference: they are defined on the master, typically in a centralized location. By default, this
is the TSWQJMMBS directory on Linux machines. Because one location contains
information for multiple minions, there must be a way to target that information to the
minions. Because of this, SLS files are used.
The UPQTMT file for pillars is identical in configuration and function to the UPQTMT file for
states: first an environment is declared, then a target, then a list of SLS files that will be
applied to that target:
CBTF
		
CBTI
Pillar SLS files are much simpler than state SLS files, because they serve only as a static data
store. They define key/value pairs, which may also be hierarchical.
TLFM@EJSFUDTLFM
SPMFXFC
XFC@DPOUFOU
JNBHFT
KQH
QOH
HJG
TDSJQUT
DTT
KT
Like state SLS files, pillar SLS files may also include other pillar SLS files.
JODMVEF
VTFST
To view all pillar data, use the QJMMBSJUFNT function:
salt myminion pillar.items

Essentials Revisited
[ 26 ]
Take note that in older versions of Salt, when running this command, by default the
master's configuration data will appear as a pillar item called master. This can cause
problems if the master configuration includes sensitive data. To disable this output, add the
following line to the master configuration:
QJMMBS@PQUT'BMTF
Fortunately, this option defaults to False as of Salt version 2015.5.0. This is also a good time
to mention that, outside the master configuration data, pillars are only viewable to the
minion or minions to which they are targeted. In other words, no minion is allowed to
access another minion's pillar data, at least by default. It is possible to allow a minion to
perform master commands using the peer system, but that is outside the scope of this
chapter.
Managing files dynamically with templates
Salt is able to use templates, which take advantage of grains and pillars, to make the state
system more dynamic. A number of other templating engines are also available, including
(as of version 2016.3) the following:
jinja
mako
wempy
cheetah
genshi
These are made available via Salt's rendering system. The preceding list only contains
renderers that are typically used as templates to create configuration files and the like.
Other renderers are available as well, but are designed more to describe data structures:
yaml
yamlex
json
json5
msgpack
py
pyobjects
pydsl

Essentials Revisited
[ 27 ]
Finally, the following Renderer can decrypt GPG data stored on the master, before passing
it through another renderer:
gpg
By default, state SLS files will be sent through the Jinja renderer, and then the yaml
renderer. There are two ways to switch an SLS file to another renderer. First, if only one SLS
file needs to be rendered differently, the first line of the file can contain a shabang line that
specifies the renderer:
QZ
The shabang can also specify multiple Renderers, separated by pipes, in the order in which
they are to be used. This is known as a render pipe. To use Mako and JSON instead of Jinja
and YAML, use:
NBLP]KTPO
To change the system default, set the renderer option in the master configuration file. The
default is:
SFOEFSFSZBNM@KJOKB
It is also possible to specify the templating engine to be used on a file that created the
minion using the GJMFNBOBHFE state:
BQBDIF@DPOG
GJMF
NBOBHFE
OBNFFUDBQBDIFBQBDIFDPOG
TPVSDFTBMUBQBDIFBQBDIFDPOG
UFNQMBUFKJOKB
A quick Jinja primer
Because Jinja is by far the most commonly-used templating engine in Salt, we will focus on
it here. Jinja is not hard to learn, and a few basics will go a long way.
Variables can be referred to by enclosing them in double-braces. Assuming a grain is set
called VTFS, the following will access it:
5IFVTFS\\HSBJOT<	VTFS	>^^JTSFGFSSFEUPIFSF

Essentials Revisited
[ 28 ]
Pillars can be accessed in the same way:
5IFVTFS\\QJMMBS<	VTFS	>^^JTSFGFSSFEUPIFSF
However, if the VTFS pillar or grain is not set, the template will not render properly. A safer
method is to use the TBMU built-in to cross-call an execution module:
5IFVTFS\\TBMU<	HSBJOTHFU	>
	VTFS		MBSSZ	^^JTSFGFSSFEUPIFSF
5IFVTFS\\TBMU<	QJMMBSHFU	>
	VTFS		MBSSZ	^^JTSFGFSSFEUPIFSF
In both of these examples, if the VTFS has not been set, then MBSSZ will be used as the
default.
We can also make our templates more dynamic by having them search through grains and
pillars for us. Using the DPOGJHHFU function, Salt will first look inside the minion's
configuration. If it does not find the requested variable there, it will check the grains. Then
it will search pillar. If it can't find it there, it will look inside the master configuration. If all
else fails, it will use the default provided.
5IFVTFS\\TBMU<	DPOGJHHFU	>
	VTFS		MBSSZ	^^JTSFGFSSFEUPIFSF
Codeblocks are enclosed within braces and percent signs. To set a variable that is local to a
template (that is, not available via DPOGJHHFU), use the TFU keyword:
\TFUNZWBS	.Z7BMVF	^
Because Jinja is based on Python, most Python data types are available. For instance, lists
and dictionaries:
\TFUNZMJTU<	BQQMFT		PSBOHFT		CBOBOBT	>^
\TFUNZEJDU\	GBWPSJUFQJF		LFZMJNF		GBWPSJUFDBLF		TBDDIFS
UPSUF	^^
Jinja also offers logic that can help define which parts of a template are used, and how.
Conditionals are performed using JG blocks. Consider the following example:
\JGHSBJOT<	PT@GBNJMZ	>	%FCJBO	^
BQBDIF
\FMJGHSBJOT<	PT@GBNJMZ	>	3FE)BU	^
IUUQE
\FOEJG^
QLH
JOTUBMMFE
TFSWJDF
SVOOJOH

Essentials Revisited
[ 29 ]
The Apache package is called BQBDIF on Debian-style systems, and IUUQE on RedHat-
style systems. However, everything else in the state is the same. This template will auto-
detect the type of system that it is on, install the appropriate package, and start the
appropriate service.
Loops can be performed using GPS blocks, as follows:
\TFUCFSSJFT<	CMVF		SBTQ		TUSBX	>^
\GPSCFSSZJOCFSSJFT^
\\CFSSZ^^CFSSZ
\FOEGPS^
Summary
Salt is designed first and foremost for remote execution. Most tasks in Salt are performed as
a type of remote execution. One of the most common types of remote execution in Salt is
configuration management, using states. Minion-specific data can be declared in grains and
pillars, and used in state files and templates.
With a basic foundation of Salt behind us, let's move on to the good stuff. In the next
chapter, we will dive into the internals of Salt, and discuss why and how Salt does what it
does.

2
Diving into Salt Internals
Now that we have covered the basic concepts, it's time to start looking at how Salt works
under the hood. In this chapter, we will:
Discover how Salt manages configuration files
Look at how the renderer system works
Discuss how the loader system handles modules
Explore the state compiler, which drives so much of Salt
With a more comprehensive understanding of the internals of Salt, you will be able to craft
configurations and states that take advantage of the architectural decisions that inspired the
design of Salt.
Understanding the Salt configuration
One of the basic ideas around the Salt configuration is that a configuration management
system should require as little configuration as possible. A concerted effort has been made
by the developers to assign defaults that will apply to as many deployments as possible
while still allowing users to fine-tune the settings to their own needs.
If you are just starting with Salt, you may not need to change anything. In fact, most of the
time, the NBTUFS configuration will be exactly what is needed for a small installation, while
minions will require almost no changes, if any.

Diving into Salt Internals
[ 31 ]
Following the configuration tree
By default, most operating system (primarily Linux-based ones) will store the Salt
configuration in the FUDTBMUms (primarily Linux-based ones) will store the Salt
configuration in the FUDTBMU directory. Unix distributions will often use the
VTSMPDBMFUDTBMU directory instead, while Windows uses the $=TBMU=directory.
These locations were chosen in order to follow the design most commonly used by the
operating system in question while still using a location that was easy to make use of. For
the purpose of this book, we will refer to the FUDTBMUdirectory, but you can go ahead
and replace it with the correct directory for your operating system.
There are other paths that Salt makes use of as well. Various caches are typically stored in
WBSDBDIFTBMU, sockets are stored in WBSSVOTBMU, and state trees, pillar trees,
and reactor files are stored in TSWTBMU and TSWQJMMBS and TSWSFBDUPS,
respectively. However, as we will see later in the Exploring the SLS directories section, these
are not exactly configuration files.
Looking inside /etc/salt/
Inside the FUDTBMU directory, there will generally be one of two files: master and
minion (both will appear if you treat your master as a minion). When the documentation 
refers to master configuration, it generally means the FUDTBMUNBTUFS file, and of
course, minion configuration refers to the FUDTBMUNJOJPO file. All configuration for
these two daemons can technically go into their respective files.
However, many users find reasons to break up their configuration into smaller files. This is
often for organizational reasons, but there is a practical reason too: because Salt can manage
itself, it is often easier to have it manage smaller, templated files rather than one large,
monolithic file.
Because of this, the master can also include any file with a DPOG extension, found in the
FUDTBMUNBTUFSE directory (and the minion likewise in the NJOJPOE directory).
This is in keeping with the numerous other services that also maintain similar directory
structures.
Other subsystems inside Salt also make use of the E directory structure. Notably, Salt
Cloud makes use of a number of these directories. The FUDTBMUDMPVE,
FUDTBMUDMPVEQSPWJEFST, and FUDTBMUDMPVEQSPGJMFT files can also be
broken up into the FUDTBMUDMPVEE, FUDTBMUDMPVEQSPWJEFSTE, and
FUDTBMUDMPVEQSPGJMFTE directories, respectively. Additionally, it is
recommended to store cloud maps in the FUDTBMUDMPVENBQTE directory.

Diving into Salt Internals
[ 32 ]
While other configuration formats are available elsewhere in Salt, the format of all of these
core configuration files is YAML (except for cloud maps, which will be discussed in $IBQUFS
, Taking Salt Cloud to the Next Level). This is by necessity: Salt needs a stable starting point
from which to configure everything else. Likewise, the FUDTBMU directory is hard-coded
as the default starting point to find these files, though that may be overridden using the 
DPOGJHEJS (or $) option:
# salt-master --config-dir=/other/path/to/salt/
Managing Salt keys
Inside the FUDTBMU directory, there is also a QLJ directory, inside which is a NBTUFS
or NJOJPO directory (or both). This is where the public and private keys are stored.
The minion will only have three files inside the FUDTBMUQLJNJOJPO directory:
NJOJPOQFN The minion's private RSA key
NJOJPOQVC The minion's public RSA key, and
NJOJPO@NBTUFSQVC The master's public RSA key
The master will also keep its RSA keys (NBTUFSQFN and NBTUFSQVC) in the
FUDTBMUQLJNBTUFS directory. However, at least three more directories will also
appear in here. The NJOJPOTQSF directory contains the public RSA keys for minions that
have contacted the master but have not yet been accepted. The NJOJPOT directory contains
the public RSA keys for minions that have been accepted on the master. And the
NJOJPOT@SFKFDUFE directory will contain keys for any minion that has contacted the
master but been explicitly rejected.
There is nothing particularly special about these directories. The TBMULFZ command on
the master is essentially a convenience tool for the user that moves public key files between
directories as requested. If needed, users can set up their own tools to manage the keys on
their own, just by moving files around.
Exploring the SLS directories
As mentioned, Salt also makes use of other directory trees on the system. The most
important of these are the directories that store SLS files, which are, by default, located in
TSW.

Diving into Salt Internals
[ 33 ]
Of the SLS directories, TSWTBMU, is probably the most important. This directory stores
the state SLS files and their corresponding top files. It also serves as the default root
directory for Salt's built-in file server. There will typically be a UPQTMT file and several
accompanying TMT files and/or directories. The layout of this directory was covered in
more detail in $IBQUFS, Essentials Revisited.
A close second is the TSWQJMMBS directory. This directory maintains a copy of the static
pillar definitions, if used. Like the TSWTBMU directory, there will typically be a UPQTMT
file and several accompanying TMT files and directories. But while the UPQTMT file
matches the format used in TSWTBMU, the accompanying TMT files are merely
collections of key/value pairs. While they can use Salt's renderer (discussed later in the The
renderer section), the resulting data does not need to conform to Salt's state compiler (also
discussed later in this chapter, in the Plunging into the state compiler section).
Another directory that will hopefully find its way into your arsenal is the TSWSFBDUPS
directory. Unlike the others, there is no UPQTMT file in here. That is because the mapping is
performed inside the master configuration instead of the top system. However, the files in
this directory do have a specific format, which will be discussed in detail in $IBQUFS,
Managing Tasks Asynchronously.
Examining the Salt cache
Salt also maintains a cache directory, usually at WBSDBDIFTBMU (again, this may differ
based on your operating system). As before, both the master and the minion have their own
directory for cache data. The master cache directory contains more entries than the minion
cache, so we'll jump into that first.
The master job cache
Probably the first cache directory that you'll run across in everyday use is the KPCT
directory. In a default configuration, this contains all the data that the master stores about
the jobs that it executes.
This directory uses hashmap-style storage. That means that a piece of identifying
informationain this case, a job ID (JID)ahas been processed with a hash algorithm, and a
directory or directory structure has been created using a part or all of the hash. In this case,
a split hash model has been used, where a directory has been created using the first two
characters of the hash, and another directory under it has been created with the rest of the
hash.

Diving into Salt Internals
[ 34 ]
The default hash type for Salt is MD5. This can be modified by changing the IBTI@UZQF
value in the master configuration:
IBTI@UZQFNE
Keep in mind that IBTI@UZQF is an important value that should be decided upon when first
setting up a new Salt infrastructure if MD5 is not the desired value. If it is changed (say to
SHA1) after an infrastructure has been using another value for a while, then any part of Salt
that has been making use of it must be cleaned up manually. The rest of this book will
assume that MD5 is used.
The JID is easy to interpret: it is a date-and-time stamp. For instance, a job ID of
 refers to a job that was started on December 3, 2014, at 56 seconds
and 191,706 microeconds past 8:14 AM. The MD5 of that JID would be
GBFEEEEGCBGFEDC. Therefore, the data that describes that job would
be located at the following path:
WBSDBDIFTBMUNBTUFSKPCTGBFEEEEGCBGFEDC.
In that directory, there will be a file called KJE. This will of course contain the job ID. There
will also be a series of files with a Q extension. These files are all serialized by NTHQBDL.
Looking inside msgpack files:
If you have checked out a copy of Salt from Git, this data is easy to view.
Inside the UFTU directory in Salt's Git tree, there is a file called
QBDLEVNQQZ. This can be used to dump the contents of the NTHQBDL files
to the console.
First, there is a a file called NJOJPOTQ (notice the leading dot), which contains a list of
minions that were targeted by this job. This will look something like so:
<
NJOJPO
NJOJPO
NJOJPO
>
The job itself will be described by a file called MPBEQ:
\
BSH<

>
GVOUFTUQJOH
KJE
UHU

Diving into Salt Internals
[ 35 ]
UHU@UZQFHMPC
VTFSSPPU
^
There will also be one directory for each minion that was targeted by that job and that
contains the return information for that job on that minion. Inside that directory will be a
file called SFUVSOQ, which contains the return data, serialized by NTHQBDL. Assuming that
the job in question performed a simple UFTUQJOH operation, the SFUVSO block will look
like this:
USVF
The master-side minion cache
Once Salt has started issuing jobs, another cache directory will show up, called NJOJPOT.
This directory will contain one entry per minion, with cached data about that minion. Inside
this directory are two files: EBUBQ and NJOFQ.
The EBUBQ file contains a copy of the grains and pillar data for that minion. A (shortened)
EBUBQ file may look like the following:
\
HSBJOT\
CJPTSFMFBTFEBUF
CJPTWFSTJPO(&588

DQV@NPEFM*OUFM
3$PSF
5.J.$16!
()[
DQVBSDIY@
PT6CVOUV
PT@GBNJMZ%FCJBO
^
QJMMBS\
SPMFXFC
^
^
The NJOFQ file contains mine data. This is not covered in detail in this book, but in short, a
minion can be configured to cache the return data from specific commands in the cache
directory on the master so that other minions can look it up. For instance, if the output from
UFTUQJOH and OFUXPSLJQ@BEEST has been configured, the contents of the NJOFQ file
will look as follows:
\
OFUXPSLJQ@BEEST<


Diving into Salt Internals
[ 36 ]
>
UFTUQJOHUSVF
^
The external file server cache
In a default installation, Salt wil keep its files in the TSWTBMU keep its files in the
TSWTBMU directory. However, an external file server, by definition, maintains an
external file store. For instance, the HJUGT external file server keeps its files on a Git server,
such as GitHub. However, it is incredibly inefficient to ask the Salt master to always serve
files directly from Git. So, in order to improve efficiency, a copy of the Git tree is stored on
the master.
The contents and layout of this tree will vary among the external file server modules. For
instance, the HJUGT module doesn't store a full directory tree as one might see in a normal
Git checkout; it only maintains the information used to create that tree using whatever
branches are available. Other external file servers, however, may contain a full copy of the
external source, which is updated periodically. The full path to this cache may look like this:
WBSDBDIFTBMUNBTUFSHJUGTwhere HJUGT is the name of the file server module.
In order to keep track of the file changes, a directory called IBTI will also exist inside the
external file server's cache. Inside IBTI, there will be one directory per environment
(CBTF, EFW, QSPE, and so on). Each of those will contain what looks like a mirror image of
the file tree. However, each actual filename will be suffixed with IBTINE (or the
appropriate hash name, if different), and the contents will be the value of the checksum for
that file.
In addition to the file server cache, there will be another directory called GJMF@MJTUT that
contains one directory per enabled file server. Inside that directory will be one file per
environment, with a Q extension (such as CBTFQ for the CBTF environment). This file will
contain a list of files and directories belonging to that environment's directory tree. A
shortened version might look like this:
\
EJST<

WJN
IUUQE
>
FNQUZ@EJST<
>
GJMFT<
UPQTMT

Diving into Salt Internals
[ 37 ]
WJNJOJUTMT
IUUQEIUUQEDPOG
IUUQEJOJUTMT
>
MJOLT<>
^
This file helps Salt with a quick lookup of the directory structure without having to
constantly descend into a directory tree.
The minion-side proc/ directory
The minion doesn't maintain nearly as many cache directories as the master, but it does
have a couple. The first of these is the QSPD directory, which maintains the data for active
jobs on the minion. It is easy to see this in action. From the master, issue a TMFFQ command
to a minion:
salt myminion test.sleep 300 --async
This will kick off a process on the minion that will wait for  seconds ( minutes) before
returning 5SVF to the master. Because the command includes the BTZOD flag, Salt will
immediately return a JID to the user.
While this process is running, log in to the minion and take a look at the
WBSDBDIFTBMUNJOJPOQSPD directory. There should be a file bearing the name of
the JID. The unpacked contents of this file will look like the following:
\	BSH	<>
	GVO		UFTUTMFFQ	
	KJE			
	QJE	
	SFU			
	UHU		NZNJOJPO	
	UHU@UZQF		HMPC	
	VTFS		SPPU	^
This file will exist until the job has been completed on the minion. If you like, you can see
the corresponding file on the master. Use the IBTIVUJMNE@EJHFTU function to find the
MD5 value of the JID:
# salt myminion hashutil.md5_digest
    20150323233901672076

Diving into Salt Internals
[ 38 ]
External modules
The other directory that you are likely to see on the minion is the FYUNPET directory. If
custom modules have been synced to the minion from the master (using the @NPEVMFT,
@TUBUFT, or other such directories on the master), they will appear here.
This is also easy to see in action:
On the master, create a @NPEVMFT directory inside TSWTBMU.
1.
Inside this directory, create a file called NZUFTUQZ, with the following contents:
2.
EFGQJOH

SFUVSO5SVF
Then, from the master, use the TBMUVUJM module to sync your new module to a
3.
minion:
TBMUNZNJOJPOTBMUVUJMTZOD@NPEVMFT
After a moment, Salt will report that it has finished:
4.
NZNJOJPO
NPEVMFTNZUFTU
Log in to the minion and look inside
5.
WBSDBDIFTBMUNJOJPOFYUNPETNPEVMFT.
There will be two files: NZUFTUQZ and NZUFTUQZD. If you look at the contents
6.
of NZUFTUQZ, you will see the custom module that you created on the master.
You will also be able to execute the NZUFTUQJOH function from the master:
TBMUNZNJOJPONZUFTUQJOH
NZNJOJPO
5SVF
The renderer
While the main master and minion configuration files must necessarily be stored in YAML,
other files in Salt can take advantage of the wealth of file formats that the modern world of
technology has to offer. This is because of the rendering system built into Salt, which can
take files of arbitrary formats and render them into a structure that is usable by Salt.

Diving into Salt Internals
[ 39 ]
Rendering SLS files
By default, all SLS files in Salt are rendered twice: first through the Jinja templating engine
and then through the PyYAML library. This provides some significant advantages:
Jinja provides a fast and powerful templating system is easy to understand and
use and follows a Pythonic mindset, comfortable to many administrators. It is
particularly well suited for managing YAML files.
YAML has a very shallow learning curve, making it easy to learn and
understand. While it does support more complex syntax, such as parentheses,
brackets, and braces (JSON is technically syntactically correct YAML), it is not
required.
However, it was immediately apparent, even before any renderers were written, that there
would be some dissent among users as to which formats were best suited to their own
environments:
A popular alternative to YAML, which was already in common usage in other
software, is JSON. This format is more strict, making it somewhat harder to read
and even more difficult to write correctly. However, because JSON is more strict
concerning how data is declared, a properly formatted JSON file is more accurate
than YAML and is easier to parse safely.
Mako was also an early addition to the Salt toolkit. While Jinja adds just enough
functionality to create a dynamic toolkit, Mako is designed to bring the full
power of Python to templates. This is especially popular with a number of users
in the DevOps community who are known to mix code with content in a number
of innovative ways.
A primary design goal of Salt has always been to provide flexibility, and so the renderer
system was designed to be pluggable in the same way as the other components of Salt.
While Jinja and YAML have been made the default, either or both can be replaced, and if
necessary, even more renderers can be brought into the mix.
If your needs include changing the global renderer from ZBNM@KJOKB, you can do so in the
master configuration file:
SFOEFSFSKTPO@NBLP
However, you should consider very carefully whether this is better. Keep in mind that
community examples, repositories, and formulas are generally kept in YAML, and if any
templating needs to be done, Jinja is usually used. This will affect how you deal with the
community or act as an enterprise customer on any support issues, and it may confuse any
experienced Salt users that your company hires.

Diving into Salt Internals
[ 40 ]
That said, even with a standard base of Jinja and YAML, there are times when using a
different set of renderers for a small subset of your SLS files is appropriate.
Render pipes
As previously mentioned, SLS files will be rendered using the configured default. However,
it is possible to change how a file is rendered by adding a shebang (also known as shabang)
line to the top of the file. A file that is to be rendered only as YAML will begin with the
following line:
ZBNM
However, in the Salt world, this is generally impractical. Adding a templating engine
increases the power of an SLS file significantly. In order to use multiple renderers in a
specific order, add them to the shabang line in the desired order, separated by pipes:
KJOKB]ZBNM
This resembles the Unix method of piping smaller programs together, to create larger, more
functional programs. There is no imposed limit on how many renderers are piped together:
NBLP]QZPCKFDUT]KJOKB]ZBNM]KTPO
However, this is pretty unrealistic. You will find that, in general, no more than two
renderers need to be used. Indeed, too many renderers will create a complexity that is
unreadable and unmaintainable. Use as many as are needed and no more.
It is important to note that SLS files will ultimately result in a specific data structure. The
most accurate way to say this in simple terms is that the data generated by SLS files must be
usable by the NTHQBDL serialization package. This is the format used extensively
throughout the various subsystems inside Salt (notably, the cache system). A more detailed
description of the resulting files will be explored later in the chapter, in the Plunging into the
state compiler section, as we uncover the mysteries of the state compiler.
Serving templated files
SLS files are not the only files that can take advantage of the renderer. Any file that is
served from an SLS file may also be rendered through a templating engine. These files
aren't as specific as SLS files because they do not need to return a specific data format; they
only need to result in the arbitrary file contents that will be served by Salt.

Diving into Salt Internals
[ 41 ]
The most common usage of this is with the GJMFNBOBHFE state. Adding a UFNQMBUF
argument to this state will cause the file to be rendered accordingly:
FUDIUUQEDPOGIUUQEDPOG
GJMFNBOBHFE
TPVSDFTBMUIUUQEIUUQEDPOG
UFNQMBUFKJOKB
Because the templated file will not return data, renderers that deal exclusively with data are
not available here. But while YAML, JSON, msgpack, and the various Python-based
renderers are not available, Jinja, Mako, Cheetah, and the like can be used.
Understanding the loader
The loader system is at the heart of how Salt works. In a nutshell, Salt is a collection of
modules tied together with the loader. Even the transport mechanisms, which enable
communication between and define the master, minion, and Syndic hierarchies make use of
modules that are managed by the loader.
Dynamic modules
Salt's loader system is a bit unconventional. Traditionally, most software has been designed
to require all supported components to be installed. This is not the case with every package,
of course. The Apache web server is an example of one project that supports a number of
components that need not all be installed. Debian-based operating systems manage Apache
modules by providing their NPEVMFTBWBJMBCMF and NPEVMFTFOBCMFE directories.
Red Hat-based systems take a different approach: all components that are supported by
Apache's IUUQE package are required to be installed with it.
Making such a demand with Salt is beyond unrealistic. So many packages are supported
with the default installation of Salt, many of which compete with each other (and some of
which compete, in some ways, with Salt itself) that it could be said that to build such a
dependency tree into Salt would effectively turn Salt into its own operating system.
However, even this is not entirely accurate. Because Salt supports a number of different
Linux distributions in addition to several Unix flavors and even Windows, it would be
more accurate to say that installing every package that is supported by Salt would
effectively turn Salt into several mutually exclusive operating systems. Obviously, this is
just not possible.

Diving into Salt Internals
[ 42 ]
Salt is able to handle this using multiple approaches. First, grains (covered in $IBQUFS,
Essentials Revisited) provide critical information to Salt to help identify the platform on
which it is running. Grains such as PT and PT@GMBWPS are used often enough to help Salt 
know whether to useZVN or BQU to manage packages or TZTUFNE or VQTUBSU to manage
services.
Each module is also able to check other dependencies on the system. The bulk of Salt's
Apache module makes use of the BQBDIFDUM command (or BQBDIFDUM, as appropriate),
so its availability is dependent upon whether or not that command exists on the system.
This set of techniques enables Salt to appropriately detect, as the minion process starts,
which modules to make available to the user.
A relatively new feature of Salt's loader system is the ability to load modules on demand.
Modules that support the lazy loader functionality will not actually load until requested by
the user. This streamlines the start process for the minion and makes more effective use of
the available resources.
Execution modules
It has often been said that most of the heavy lifting in Salt is performed by the execution
modules. This is because Salt was designed originally as a remote execution system, and
most module types that have been added to the loader have been designed to extend the
functionality of remote execution.
For instance, state modules are designed with one purpose in mind: to enforce the state of a
certain aspect of a system. This could be to ensure that a package is installed or that a
service is running. The state module itself doesn't install the package or start the service; it
calls out to the execution module to do so. A state module's only job is to add idempotency
to an execution module.
One could say that an important differentiator between runner modules and execution
modules is that runners are designed to be used from the master while execution modules
are designed to execute remotely on the minion. However, runners were actually designed
with something more specific in mind:
System administrators have been using shell scripts for decades. From DTI in Unix to CBTI
in Linux and even batch files in DOS and Windows, this has been the long-running
standard.

Diving into Salt Internals
[ 43 ]
Runner modules were designed to allow Salt users to apply a scripting language to remote
executions. Because so many early Salt users were also Python users, it was not generally
difficult for them to use Python as their scripting language. As the Salt user base grew, so
too did the number of users who were not fluent in Python, but the number of other options
available for them also grew.
Reactor modules (covered in detail in $IBQUFS, Managing Tasks Asynchronously) are a type
of module that can pull together execution modules and runner modules and make them
available to users with no programming experience. And because Salt states are actually
applied using the TUBUF execution module, even states are available through reactors.
Cloud modules
Cloud modules are not typically thought of by many people as Salt modules, perhaps
because Salt Cloud (covered extensively in $IBQUFS, Taking Salt Cloud to the Next Level)
started as a project separate from Salt, but they have in fact always used the loader system.
They do, however, work a little differently.
Unlike many other modules in Salt, Cloud modules do not make use of execution modules
(although there is an execution module that makes use of Salt Cloud). This is in part
because Salt Cloud was designed to run on the Salt master. However, it does not make use
of runner modules either (though, again, there is a runner module that can make use of Salt
Cloud).
Salt Cloud's initial purpose was to create new VMs on various public cloud providers and
automatically accept their keys on the Salt master. However, it quickly grew apparent that
users wanted to control as many aspects of their cloud providers as possible, not just VM
creation.
Now, Salt Cloud is able to perform any action that is available against a cloud provider.
Some providers support more functionality than others. In some cases, this is because
demand has not been presented, and in other cases, it's because the appropriate developer
has not yet had the resources to make the addition. But often it is because the features
available on the provider itself may be limited. Whatever the situation, if a feature is
available, it can be added and made available via the loader system.

Diving into Salt Internals
[ 44 ]
Plunging into the state compiler
Salt was initially designed as a remote execution system to be used for gathering data
normally collected by monitoring systems and storing it for later analysis. However, as
functionality grew, so too did a need to manage the execution modules that were doing the
heavy lifting. Salt states were born from this need and, before long, the engine that
managed them had expanded into other areas of Salt.
Imperative versus declarative
A point of contention between various configuration management systems is the concept of
declarative versus imperative configurations. Before we discuss Salt's take on the matter,
let's take a moment to examine the two.
It may be easiest to think of imperative programming like a script: perform Task A and,
when it is finished, perform Task B; once that has finished, perform Task C. This is what
many administrators are used to, especially as it more closely resembles the shell scripts
that have been their lifelines for so many decades. Chef is an example of a configuration
management suite that is imperative in nature.
Declarative definition is a newer concept, and more representative of object oriented
programming. The basic idea is that the user declares which tasks need to be performed,
and the software performs them in whichever order it sees fit. Generally, dependencies can
also be declared that dictate that some tasks are not to be completed until others are. Puppet
is a well-known example of a configuration management platform that is declarative in
nature.
Salt is unique in that it supports both imperative ordering and declarative execution. If no
dependencies are defined, then, by default, Salt will attempt to execute states in the order in
which they appear in the SLS files. If a state fails because it requires a task that appears
later, then multiple Salt runs will be required to complete all tasks.
However, if dependencies are defined, states will be handled differently. They will still be
evaluated in the order in which they appear, but dependencies can cause them to be
executed in a different order. Consider the following Salt state:
NZTRM
TFSWJDF
SVOOJOH
QLH
JOTUBMMFE
GJMF
NBOBHFE

Diving into Salt Internals
[ 45 ]
TPVSDFTBMUNZTRMNZDOG
OBNFFUDNZTRMNZDOG
In the first several versions of Salt that supported states, this would have been evaluated
lexicographically: the file would have been copied into place first, then the package
installed, and then the service started, because in the English alphabet, F comes before P,
and P comes before S. Happily, this is also the order that is probably desired.
However, the current default ordering system in Salt is imperative, meaning states will be
evaluated in the order in which they appear. Salt will attempt to start the NZTRM service,
which will fail because the package is not installed. It will then attempt to install the NZTRM
package, which will succeed. If this is a Debian-based system, installation of the package
will also cause the service to start, in this case without the correct configuration file. Lastly,
Salt will copy the NZDOG file into place but will make no attempt to restart the service to
apply the correct changes. A second state run will report success for all three states (the
service is running, the package is installed, and the file is managed as requested), but a
manual restart of the NZTRM service will still be required.
Requisites
To accommodate ordering issues caused by such issues, Salt uses requisites. These will
affect the order in which states are evaluated and executed. Consider the following changes
to the aforementioned Salt state:
NZTRM
TFSWJDF
SVOOJOH
SFRVJSF
QBDLBHFNZTRM
XBUDI
GJMFNZTRM
QLH
JOTUBMMFE
SFRVJSF
GJMFNZTRM
GJMF
NBOBHFE
TPVSDFTBMUNZTRMNZDOG
OBNFFUDNZTRMNZDOG
Even though the states have been defined in an order that is not appropriate, they will still
be evaluated and executed correctly.

Diving into Salt Internals
[ 46 ]
The following will be the order that will be defined:
TFSWJDF: NZTRM
1.
QLH: NZTRM
2.
GJMF: NZTRM
3.
However, the NZTRM service requires that the NZTRM package be executed first. So, before
executing the NZTRM service, it will look ahead and evaluate the NZTRM package. But since
the NZTRM package requires the NZTRM file to be executed first, it will jump ahead and
evaluate the NZTRM file. Because the file state does not require anything else, Salt will
execute it. Having completed the list of requirements for the QLH state, Salt will go back and
execute it. And finally, having completed all the service requirements, Salt will go back and
execute the service.
Following successful completion of the service state, it will move onto the next state and see
whether it has already been executed. It will continue in this fashion until all states have
been evaluated and executed.
It is in this manner that Salt is able to be both imperative (by allowing statements to be
evaluated in the order in which they appear) and declarative (by allowing statements to be
executed based on requisites).
High and low states
The concept of high states has proven to be one of the most confusing things about Salt.
Users understand that the TUBUFIJHITUBUF command performs a state run, but what
exactly is a high state? And does the presence of a high state mean that there is a low state
as well?
There are two parts of the state system that are in effect. High data refers generally to data
as it is seen by the user. Low data refers generally to data as it is ingested and used by Salt.
High states
If you have worked with state files, you have already seen every aspect of this part of the
state system. There are three specific components, each of which builds upon the one before
it:
High data

Diving into Salt Internals
[ 47 ]
SLS file
High state
Each state represents a piece of high data. If the previous SLS were broken into individual
states, those would look like this, respectively (ignoring the fact that duplicate top-level 
keys would comprise an invalid YAML file):
NZTRM
TFSWJDF
SVOOJOH
SFRVJSF
QLHNZTRM
XBUDI
GJMFNZTRM
NZTRM
QLH
JOTUBMMFE
SFRVJSF
GJMFNZTRM
NZTRM
GJMF
NBOBHFE
TPVSDFTBMUNZTRMNZDOG
OBNFFUDNZTRMNZDOG
When combined with other states, they form an SLS file:
JQUBCMFT
TFSWJDF
SVOOJOH
NZTRM
TFSWJDF
SVOOJOH
SFRVJSF
QBDLBHFNZTRM
XBUDI
GJMFNZTRM
QBDLBHF
JOTUBMMFE
SFRVJSF
GJMFNZTRM
GJMF
NBOBHFE
TPVSDFTBMUNZTRMNZDOG

Diving into Salt Internals
[ 48 ]
OBNFFUDNZTRMNZDOG
When these files are tied together using includes and further glued together for use inside
an environment using a UPQTMT file, they form a high state, like so:
UPQTMT
CBTF
		
NZTRM
NZTRMTMT
JODMVEF
JQUBCMFT
NZTRM
TFSWJDF
SVOOJOH
SFRVJSF
QBDLBHFNZTRM
XBUDI
GJMFNZTRM
QBDLBHF
JOTUBMMFE
SFRVJSF
GJMFNZTRM
GJMF
NBOBHFE
TPVSDFTBMUNZTRMNZDOG
OBNFFUDNZTRMNZDOG
JQUBCMFTTMT
JQUBCMFT
TFSWJDF
SVOOJOH
When the TUBUFIJHITUBUF function is executed, Salt will compile all relevant SLS files
inside UPQTMT and any includes into a single definition, called a high state. This can be
viewed by using the TUBUFTIPX@IJHITUBUF function:
# salt myminion state.show_highstate --out yaml
NZNJOJPO
JQUBCMFT
TFSWJDF
SVOOJOH
PSEFS
@@TMT@@JQUBCMFT
@@FOW@@CBTF
NZTRM
TFSWJDF
SVOOJOH

Diving into Salt Internals
[ 49 ]
SFRVJSF
QLHNZTRM
XBUDI
GJMFNZTRM
PSEFS
QLH
JOTUBMMFE
SFRVJSF
GJMFNZTRM
PSEFS
GJMF
NBOBHFE
TPVSDFTBMUNZTRMNZDOG
OBNFFUDNZTRMNZDOG
PSEFS
@@TMT@@NZTRM
@@FOW@@CBTF
Take note of the extra fields that are included in this output. First, an order is declared. This
is something that can be explicitly declared by the user in an SLS file using either real
numbers or the GJSTU or MBTU keywords. All states that are set to be first will have their
order adjusted accordingly. Numerically ordered states will appear next. Salt will then add
 to the last defined number (which is  by default) and add any states that are not
explicitly ordered. Finally, any states set to MBTU will be added.
Salt will also add some variables that it uses internally, to know which environment
(@@FOW@@) to execute the state in and which SLS file (@@TMT@@) the state declaration came
from.
Remember that the order is still no more than a starting point; the actual high state will be
executed based first on requisites and then on order.
Low states
Once the final high state has been generated, it will be sent to the state compiler. This will
reformat the state data into a format that Salt uses internally to evaluate each declaration
and will feed data into each state module (which will in turn call the execution modules as
necessary). As with high data, low data can be broken into individual components:
Low state
Low chunks
State module
One or more execution modules

Diving into Salt Internals
[ 50 ]
The low data can be viewed using the TUBUFTIPX@MPXTUBUF function:
# salt myminion state.show_lowstate --out yaml
NZNJOJPO
@@FOW@@CBTF
@@JE@@JQUBCMFT
@@TMT@@JQUBCMFT
GVOSVOOJOH
OBNFJQUBCMFT
PSEFS
TUBUFTFSWJDF
@@FOW@@CBTF
@@JE@@NZTRM
@@TMT@@NZTRM
GVOSVOOJOH
OBNFNZTRM
PSEFS
SFRVJSF
QBDLBHFNZTRM
TUBUFTFSWJDF
XBUDI
GJMFNZTRM
@@FOW@@CBTF
@@JE@@NZTRM
@@TMT@@NZTRM
GVOJOTUBMMFE
OBNFNZTRM
PSEFS
SFRVJSF
GJMFNZTRM
TUBUFQBDLBHF
@@FOW@@CBTF
@@JE@@NZTRM
@@TMT@@NZTRM
GVONBOBHFE
OBNFFUDNZTRMNZDOG
PSEFS
TPVSDFTBMUNZTRMNZDOG
TUBUFGJMF
Together, all this comprises a low state. Each individual item is a low Chunk. The first low 
chunk in this list looks like this:
@@FOW@@CBTF
@@JE@@JQUBCMFT
@@TMT@@JQUBCMFT
GVOSVOOJOH
OBNFJQUBCMFT

Diving into Salt Internals
[ 51 ]
PSEFS
TUBUFTFSWJDF
Each low chunk maps to a state module (in this case, TFSWJDF) and a function inside that
state module (in this case, SVOOJOH). An ID is also provided at this level (@@JE@@). Salt will
map relationships (that is, requisites) between states using a combination of state and
@@JE@@ values. If a name has not been declared by the user, then Salt will automatically use
the @@JE@@ value as the name.
Once a function inside a state module has been called, it will usually map to one or more
execution modules, which actually do the work. Let's take a moment to examine what goes
down when Salt gets to that point.
Enforcing statefulness
While execution modules are somewhat looser in definition, state modules are necessarily
more precise. Certain behaviors can always be expected of a state module:
A state module will always require a name
A state module will always return the following data structures:
name
result
changes
comment
In the case of monitoring states (covered in $IBQUFS, Monitoring with Salt), a dictionary
called EBUB will also be returned.
name
The name refers to the specific piece of information that is to be managed by the state
module. In the case of a service, it is the service's name, as recognized by that minion's
service manager (that is, BQBDIF). In the case of a file, it refers to the full path on the
minion at which the file is to be located (that is, FUDBQBDIFBQBDIFDPOG).
When the state results are returned to the user, the name will be used to identify the state
that was evaluated.

Diving into Salt Internals
[ 52 ]
result
There are only three values that will ever be returned as a result: 5SVF, 'BMTF, and /POF.
A state that returns 5SVF is declaring that following its execution, Salt believes that the
resource to be configured is as desired. This may be because the resource was already
properly configured or because the state module successfully performed the steps necessary
to enforce the desired configuration.
A state that returns 'BMTF is declaring that following execution, Salt believes that despite its
attempts, the resource has not been configured as desired.
In a normal state run, no state will ever return /POF. This value is reserved for state runs
that have been executed in test mode. This is also known as a dry run. Here's an example:
salt myminion state.highstate test=True
When a state run happens in test mode, Salt will not allow changes to occur on the system.
If a resource is already configured as expected, it will still return 5SVF. If Salt detects that
changes are required in order to enforce a resource's state, it will return /POF to notify the
user.
changes
The DIBOHFT data structure is a list that will never be populated in test mode because it
only reflects the changes that have been applied to the system to bring it into compliance
with the requested configuration. This will only be populated if the result has been returned
as 5SVF. The contents of this list are dependent on the state module that is executed.
comment
Regardless of whether or not changes were made to a system or whether or not a state was
successful, the DPNNFOU field should be populated to inform the user of additional
information, in a more human-readable format, that may be helpful to them.
Following any kind of state run, each individual state will return all of these fields. After all
the information, a tally of successes and failures will appear, including a tally of how many
changes were made. A successful state tree will require no more than one state run. When
issues arise, a combination of these fields will be incredibly useful.

Diving into Salt Internals
[ 53 ]
Summary
We have discussed how Salt manages its own configuration as well as the loader and
renderer systems. We have also gone into significant details about how the state system
works.
Now that we have a solid understanding of how Salt works under the hood, we can dive
into some of the more powerful components.
Next up, we'll dive even deeper into Salt's state system, and discuss how powerful of a tool
it is for managing configuration inside of your infrastructure

3
Managing States
Configuration management is a hot topic, and the state system is what Salt uses to perform
it. It's become very powerful over time, and even if you're already well versed in states, I
expect there are still a few tricks you haven't learned yet. In this chapter, we'll cover:
Integrating multiple state files into a cohesive product
Making state trees span multiple environments
Understanding the master tops subsystem
Taking advantage of the TBMUTUBDLGPSNVMBT organization on GitHub
Using SPM, the Salt Package Manager
Handling multiple states
Using single SLS files is pretty powerful as it is, but real power comes from being able to
combine them to form a larger orchestration. We touched on some of the basics in $IBQUFS
, Essentials Revisited. Let's go ahead and expound upon them.
Including other SLS files
That little JODMVEF code block at the beginning of an SLS file is the start to pulling together
SLS files. Let's say that you have a number of formulas that are used to create a
development environment for your users. Here are a few examples:
git
vim
emacs
ack

Managing States
[ 55 ]
pip
pycharm
Let's go ahead and put together a development environment formula first. Go ahead and
create a formula directory called EFWFOW:
# mkdir -p /srv/salt/devenv/
Then, we'll create a file inside that directory called JOJUTMT that references those
formulas:
JODMVEF
HJU
WJN
FNBDT
BDL
QJQ
QZDIBSN
Of course, most of your developers will not want both vim and emacs; they'll only want
one. Let's give them the ability to choose which one gets installed. We'll add some Jinja code
that looks for a configuration value called EFW@FEJUPS. You could set that in a pillar, but
this is one time when it is more appropriate to use a grain so that developers can manage
the setting on their own systems without having to edit pillars on the master:
\EFW@FEJUPSTBMU<	DPOGJHHFU	>
	EFW@FEJUPS		WJN	
JODMVEF
HJU
\JGEFW@FEJUPS	WJN	^
WJN
\FMJGEFW@FEJUPS	FNBDT	^
FNBDT
\FOEJG^
BDL
QJQ
QZDIBSN
We'll assume that the aforementioned states are pretty generic. For instance, the WJN state
may be no more than the following:
WJN
QLHJOTUBMMFE
WJN@UPPMT
QLHJOTUBMMFE
OBNFT
HMPCBM@WJN@UIFNFT
HMPCBM@WJN@TZOUBYFT

Managing States
[ 56 ]
But different types of developers will have different needs. A web developer may need a
package that supplies certain extra syntax definitions for Vim (such as language files), and a
QA engineer might need a completely different set (such as testing file formats).
The FYUFOE declaration gives you the ability to take existing definitions and add
information to them. If we add a little Jinja code to the mix, we can do something like this:
\EFW@FEJUPSTBMU<	DPOGJHHFU	>
	EFW@FEJUPS		WJN	
\EFW@UZQFTBMU<	DPOGJHHFU	>
	EFW@UZQF		XFC	
JODMVEF
HJU
\JGEFW@FEJUPS	WJN	^
WJN
\FMJGEFW@FEJUPS	FNBDT	^
FNBDT
\FOEJG^
BDL
QJQ
QZDIBSN
FYUFOE
WJN@UPPMT
QLH
OBNFT
EFW@WJN@UIFNFT
\JGEFW@UZQF	XFC	^
XFC@WJN@TZOUBYFT
\FMJGEFW@UZQF	RB	^
RB@WJN@TZOUBYFT
\FOEJG^
When extending SLS files, there are some important rules to remember.
There is only one FYUFOE directive. If you have multiple FYUFOE sections in a
single SLS file, you will generate YAML errors regarding multiple key
definitions.
If you extend a value that already exists in the included file, it will be overwritten
by the FYUFOE directive.
The exception to this are lists such as SFRVJSF, XBUDI, and OBNFT. If you extend
one of these, then the new information will be appended to the old list.

Managing States
[ 57 ]
Spanning multiple environments
The idea behind Salt environments is to allow formulas to be distributed to multiple types
of minions while still restricting them from running in areas where they are not
appropriate. For instance, if you use LDAP as an authentication mechanism, it probably
makes sense to have it available in every environment. However, even if you're using Git
for revision control, you probably only want it in your development environment, as other
environments are often deployed via some other mechanism.
Using the base environment
By default, all minions will behave as if they are in the CBTF environment, even if you have
no environments defined. By convention, most Salt deployments make use of the base
environment anyway, so it's not a bad idea to follow suit.
The base environment is designed to provide formulas that will be useful or necessary
across all minions. Decide carefully which formulas to add to CBTF: most formulas will
have at least one environment that they do not belong in.
Consider the following scenario: your infrastructure includes four environmentsbEFW, RB,
TUBHF, and QSPE. The EFW and RB environments are similar, except that the only tools
available in the RB environment to manage code should be those that deploy code to the
environment for testing. The RB and TUBHF environments are similar, except that the TUBHF
environment should look exactly like the QSPE environment (as appropriate) except that it
is not publicly accessible. Deployment in TUBHF must behave exactly the same as in QSPE so
that a production deployment is as predictable as possible.
In this setup, you will likely have a number of developer-centric tools available in the EFW
environment that are not appropriate for any other environment. However, most of the
formulas that are available in production (QSPE) will also be available in all other
environments. The only difference between TUBHF and QSPE will be the configuration files
that maintain the environment and the versions of the packages that are being staged versus
the ones that are already in production.
In this setup, you may have a UPQTMT file that looks something like this:
CBTF
		
CBTF@GPSNVMBT
EFW
	EFW	
EFW@UPPMT

Managing States
[ 58 ]
RB
	RB	
UFTUJOH@UPPMT
UFTUJOH@DPOGJHT
TUBHF
	TUBHF	
TUBHF@QBDLBHFT
TUBHF@DPOGJHT
QSPE
	QSPE	
QSPE@QBDLBHFT
QSPE@DPOGJHT
Breaking out environments
Unfortunately, things are rarely that simple. The aforementioned formula may work for a
smaller infrastructure in which some servers perform multiple duties. But in a production
environment, where each server should only be performing one function, you will probably
want to break out into a number of other environments.
Let's say that your infrastructure makes use of web servers, database servers, and load-
balancer servers. You may be tempted to set up servers with XFCEFW, ECEFW, MCEFW,
and so in in their names. But this can quickly lead to an exponentially large set of
environments that are difficult to maintain at best.
Instead, think about setting up environments for XFC, EC, and MC in addition to your EFW,
RB, TUBHF, and QSPE environments. Minions may belong to multiple environments, which
allows you to combine resources together. For instance, the RB and XFC sections of UPQTMT
may look like this:
RB
	RB	
UFTUJOH@UPPMT
UFTUJOH@DPOGJHT
XFC
	XFC	
BQBDIF
NPE@TTM
With this kind of configuration, a server naming scheme might look like XFCRB, XFC
RB, and so on.

Managing States
[ 59 ]
There is a potential problem with this scenario, which may be immediately obvious to the
experienced admin: how do we specify the version of a package that is appropriate for the
RB environment as opposed to TUBHF or QSPE?
My recommendation relies on a solution both inside and outside of Salt. It is extremely
common for infrastructures to rely on public repositories for their packaging needs. But
when you do that, you place yourself at the mercy of factors that you cannot control, such
as bad package updates or server downtime at the host.
Rather than placing the life of your infrastructure in the hands of somebody else, take
control. Set up your own repositories, one for each environment. Repositories for EFW will
likely be as loose as any other resource in EFW, but as code and configuration move up
through RB, TUBHF, and QSPE, you will have the ability to tighten up resources as
appropriate.
This will allow you to create formulas that look like this, for instance:
BQBDIF
QLHMBUFTU
Because you control what the latest package available is, you will spend less time editing
formulas to bring them into compliance.
Understanding master_tops
Up until now, we have referred to the UPQ setup exclusively using the UPQTMT file. But just
as other resources in Salt are modular, so is the UPQ subsystem. By default, Salt will obtain
UPQ data from the filesystem, but you can also reach out to external resources. These
resources may also be referred to as master tops.
Like state files, the only real requirement of a master top is that it return data to Salt in the
same format that would be generated by the UPQTMT file. This format contains a dictionary
of environments, each of which has another dictionary containing targets, each of which
contains a list of formulas for those targets.
Some of the master tops that ship with Salt are as follows:
DPCCMFS: An infrastructure deployment tool based around machine images
NPOHP: A popular NoSQL database solution

Managing States
[ 60 ]
SFDMBTT@BEBQUFS: A community project that aims to standardize configuration
management between multiple disparate projects, including Salt
WBSTUBDL: A project that allows you to layer together configuration variables
Because Cobbler is both a well-established solution and simple to configure as a master top,
let's take a look at its configuration.
First off, master tops are set up in the NBTUFS configuration file using the NBTUFS@UPQT
directive. In this case, we'll add Cobbler to that directive:
NBTUFS@UPQT
DPCCMFS\^
As with external filesystems, master tops expect you to provide configuration on the same
line as the configuration key. However, because the Cobbler module uses the same
configuration as the Cobbler pillar, we just need to pass in an empty dictionary.
The configuration for Cobbler comes next:
DPCCMFSVSMIUUQDPCCMFSFYBNQMFDPNDPCCMFS@BQJ
DPCCMFSVTFSMBSSZ
DPCCMFSQBTTXPSEQBTT
There's not much to this module. It connects to an established Cobbler server and converts
the information about that minion to top data that the master can use.
Using Salt Stack Formulas on GitHub
A number of formulas have been built by various engineers and are compiled together on
GitHub under the Salt Stack Formulas organization. You can find these formulas at
IUUQTHJUIVCDPNTBMUTUBDLGPSNVMBT.
At the time of writing this, there are over 200 formulas hosted under this organization.
Some are better maintained than others, but a number of them see frequent maintenance by
the community.
Probably the most helpful thing about these formulas is that, outside of rare special cases,
each software package that is supported by a formula will only have one formula. This is a
vast improvement over similar user repositories, which may include dozens or even
hundreds of different examples for the exact same package.

Managing States
[ 61 ]
Examining the standard layout
Most formulas adhere to a standard layout, which has been developed and honed over time
to help encourage a consistent look and feel. You will generally find the following files and
directories:Because Cobbler is both a well-established solution and simple to configure as a
master top,
formula name
The BQBDIFGPSNVMB repository has a directory called BQBDIF, which in turn holds an
JOJUTMT file and any other supporting files. Likewise, the NZTRMGPSNVMB repository has
a NZTRM directory with its own JOJUTMT and so on.
pillar.example
Anything that can be made a variable inside a formula should be made a variable. Each of
those variables, a sane default, and a description of what they do should be contained
inside the QJMMBSFYBNQMF file. It is up to you, the user, to implement those variables in
your pillar or grain data.
README
3&"%.& is a file containing a description of the formula and any relevant documentation
that should be seen immediately. The contents of this file will be displayed on GitHub,
below the list of files in the repository.
LICENSE
All projects on GitHub should contain a -*$&/4& file, and this is no different. It is
extremely common for Salt-related projects, such as those in Salt Stack Formulas, to contain
a copy of the Apache 2 license.
FORMULA
If present, the '03.6-" file can be used to create SPM packages of this formula. Refer to the
Using the Salt Package Manager section later in this chapter.

Managing States
[ 62 ]
Cloning formulas
It is possible to use the GitFS external filesystem to make a formula available, but I would
strongly discourage using it to point directly to GitHub. Not only is it discourteous to
GitHub themselves, but it also introduces the possibility of an unexpected commit causing
issues in your infrastructure.
No, it is better to clone a repository, test it, and then implement it. As new commits appear
upstream, you can fetch, test, and deploy them as you see fit.
Go to IUUQTHJUIVCDPNTBMUTUBDLGPSNVMBTBQBDIFGPSNVMB, and you will see this:
On the main screen of the repository, you will see a button titled Clone or download. This
button will offer you the ability to choose between cloning using HTTPS or SSH. Go ahead
and set aside a location on your machine for checkouts:
# mkdir -p /src/saltstack-formulas
Move into that directory:
# cd /src/saltstack-formulas

Managing States
[ 63 ]
Then, clone the repository:
# git clone https://github.com/saltstack-formulas
    /apache-formula.git
I'm going to assume that you have a basic understanding of Git, but I will give you a couple
of pointers. First off, most formulas use a master branch as the default. When you clone a
repository from a remote source, that source will be called PSJHJO by default.
So to fetch new data from a repository, use the following two commands:
# git fetch origin
# git pull origin master
I'm not going to get into Git any more here, as that's way outside the scope of this chapter.
But even if you're not used to Git, this should still be enough to get you going, so long as
you don't make any local changes to your clone.
Using the Salt Package Manager
The Salt Package Manager (SPM), was added in version 2015.8 to address a very common
problem: the distribution of states and formulas to other systems. You may be tempted to
think of SPM only as an extension of Salt Stack Formulas, but that is far from the truth. In
fact, there is a much larger need for the distribution of Salt-specific files.
In fact, while Salt Stack Formulas was kept in mind during the development of SPM, the
bigger concern were private customers and, in particular, government, military, and other
secure organizations that have a need to distribute information in environments that by
necessity cannot have a connection to the Internet.
Thinking of SPM as a package manager
Package managers are, in theory, a simple concept. A package is created that contains a set
of files to be copied to an individual computer. Older package managers did little more
than that, while modern package managers also keep track of file metadata so that installed
packages can be queried, monitored, and removed.
Unfortunately, these modern package managers tend to carry quite a bit of complexity in
order to offer those features to their users. A well-made package manager coupled with
well-made packages hides these complexities from the bulk of the users using those
packages.

Managing States
[ 64 ]
SPM is a very new package manager and still has plenty of quirks. As with other package
managers, it will evolve based on the needs of its users and the communities that they
create. Fortunately, its design has borrowed from existing package managers in order to try
and cut down on the learning curve.
From a package-management point of view, SPM borrows heavily from Pacman, the
package manager of Arch Linux and its derivatives, and a little from RPM Package
Manager, favored by Suse and RedHat among others.
SPM uses a tarball that has been processed with the C[JQ compression format. These
decisions were made because both UBS and C[JQ are supported by Python 2.6, which is
the lowest version of Python supported by Salt.
The tarball contains a handful of metadata files, most notably '03.6-", which is analogous
to an RPM spec file or a Pacman 1,(#6*-% file. As this is a Salt tool, '03.6-" files are
stored in YAML format. We'll get into the actual layout of this and other files in the Building
packages section coming up.
The same command (TQN) is used for all SPM operations. If you have an SPM file locally
saved on your computer, you can use the following command to install it:
# spm local install /path/to/package.spm
All TQN commands that operate on a locally available package use MPDBM. For instance, to
show the metadata for a package, you'd use this command:
# spm local info /path/to/package.spm
This one lists the files inside a package:
# spm local info /path/to/package.spm
Thinking of SPM as a repository manager
The SPM commands in the previous section may look familiar to those of you who are used
to the ZVN command that is common on so many Red Hat-based systems. This is by design.
Because ZVN commands are (more or less) in plain English, rather than relying on option
flags, they tend to be much easier to remember. SPM has taken usage cues from ZVN and
tried to utilize plain-English commands as well.
This leads in well to SPM as a repository manager in addition to its function as a package
manager. While usage is based on ZVN commands, the concept of building a package
manager and a package repository manager into the same program is copied from Pacman.

Managing States
[ 65 ]
When package managers were created, the concept of a package repository was still a long
way off. By the time Pacman (and of course SPM) came about, the developers were able to
take advantage of the existing knowledge of both.
Many of the SPM commands that don't use MPDBM are designed to be used with a package
repository. For instance, the following command will install a package from a repository:
# spm install <packagename>
Of course, this command assumes that:
You have one or more SPM repositories configured
One of those repositories has the requested package available
You have downloaded the latest metadata from that repository
The second of those should be obvious; let's discuss the others in turn.
Configuring SPM repositories
SPM repositories are configured using the FUDTBMUTQNSFQPTE directory, which
contains one or more file with a SFQP extension. Users of ZVN will recognize this naming
convention. As with other Salt configuration files, these are in YAML format. One or more
repositories can be configured in these files, using the following format:
SFQPTJUPSZOBNF 
VSMSFQPTJUPSZVSM 
At the moment, the only argument available inside a repository definition is VSM. This URL
can use one of the following schemes:
http
https
ftp
file
Here's an example:
MPDBM@TQN@SFQP
VSMGJMFTSWTQN
SFNPUF@TQN@SFQP
VSMIUUQTSFQPFYBNQMFDPNTQN

Managing States
[ 66 ]
Downloading repository metadata
Once a repository has been configured, you need to download its metadata to be able to use
it. You can download the metadata for all repositories at once, like this:
spm update_repo
Or you can download the metadata for one specific repository:
spm update_repo remote_spm_repo
Creating SPM repositories
It's all well and good to be able to configure your system to download repository metadata,
but how do you create one in the first place? As packages become readily available, this
skill will become more important than building the packages in the first place (but don't
worry; we'll get to those in the next section).
Once again, if you are familiar with creating ZVN repositories, these steps will not surprise
you. It's up to you what order you want to do these steps in, of course.
Set aside a directory to hold packages
The placement of this directory will likely depend largely on the mechanism that you will
use to share the directory. For the purpose of our examples, we'll use TSWIUUQTQN:
# mkdir -p /srv/http/spm/
Share that directory
It doesn't matter what program you use to share the directory as long as it supports one of
the necessary schemes. Apache and Nginx are excellent web servers, and WTGUQE is an
excellent FTP server. For our example, we'll use EBSLIUUQE:
# darkhttpd /srv/http/spm
darkhttpd/1.12, copyright (c) 2003-2016 Emil Mikulic.
listening on: http://0.0.0.0:80/

Managing States
[ 67 ]
Populate the directory
Go ahead and copy all of the SPM files into the directory that you wish to serve to users. As
you'll see in the next section, there's a good chance those files live in TSWTQN@CVJME:
cp /srv/spm_build/*spm /srv/http/spm/
Create the repository metadata
Once that directory is populated, you can use the TQN command to turn it into a repository:
spm create_repo /srv/http/spm/
This will create a YAML file called 41..&5"%"5" in that directory. When a user issues the
TQNVQEBUF@SFQP command, this is the file that will be downloaded.
This file describes each SPM package in that directory, including the package name,
filename, and package metadata. As of Salt version 2016.3, no other information is included,
but future versions are intended to contain a list of files inside the package as well.
Building packages
Before you can put together an SPM repository, you need packages. When SPM packages
become more readily available, this won't be so much of a problem. Until then, you will
need to build packages on your own.
The package structure is reasonably simple and is based on existing repositories in the Salt
Stack Formulas GitHub repository. In fact, the one thing that allows most of those
repositories to become an SPM package is the presence of a YAML file called '03.6-".
Let's look at the fields required in all '03.6-" files.
version
The WFSTJPO field denotes the version of the formula. By convention, this is usually the
year and month that this formula was last updated, in ::::.. format.

Managing States
[ 68 ]
release
The SFMFBTF field has the iteration of the formula in a given month. The first release build
of the month is , the second is , and so on. Combined with the version, you will end up
with formula versioning that looks like .
summary
The TVNNBSZ field has a one-line description of the formula. This should be short, sweet,
and to the point.
description
The EFTDSJQUJPO field has a longer description of the formula. This field can span multiple
lines if necessary, but don't go overboard.
There are a number of other fields that can be added as well to provide further information
about the formula. Some of these fields are descriptive only; they will not affect the
installation in any way. Others will affect how the formula is built.
name
The OBNF field denotes the name that will be used to refer to the package, both in the name
of the package file and any databases (such a repository or local install metadata) that refer
to the package. This field is highly recommended, but if it's not included, then SPM will
attempt to discover the package name based on the files present.
os
The PT field has a comma-separated list of operating systems (as seen in the PT grain) that
the formula explicitly supports. This has no bearing on the machine where the formula is
installed, as that machine is likely to only be serving formula files to other machines.
os_family
Like PT, PT@GBNJMZ is a comma-separated list of operating system families (as seen in the
PT@GBNJMZ grain) that this formula explicitly supports.

Managing States
[ 69 ]
dependencies
The EFQFOEFODJFT field has a comma-separated list of SPM package names that are
required in order for this formula to work. The items in this list are enforced at installation
time. That means that if the dependencies are not already installed, SPM will attempt to
install them.
This is one of the most useful fields as it allows you to create virtual packages of a sort,
which serves only to tie together other disparate packages. For instance, you may create a
formula called XPSEQSFTTOHJOYNZTRM that contains the following line:
EFQFOEFODJFTXPSEQSFTTOHJOYNZTRM
And you might have another formula called XPSEQSFTTBQBDIFNZTRM, which contains
the following line:
EFQFOEFODJFTXPSEQSFTTBQBDIFNZTRM
Such formulas would contain few or no files, except possibly configuration files that tie the
dependencies together.
optional
The PQUJPOBM field is a comma-separated list of SPM packages related to this formula.
During the installation of this formula, SPM will display the names of these packages but
will not force them to be installed.
recommended
The SFDPNNFOEFE field has a comma-separated list of SPM packages that are not required
but are recommended for this package. For instance, an BQBDIF package may recommend
another package called NPE@TTM.
minimum_version
The NJOJNVN@WFSTJPO field holds the minimum version of Salt that is required in order for
this formula to function properly. This is not currently enforced, but as SPM was introduced
in version 2015.8, it is assumed that any SPM package supports at least that version of Salt.

Managing States
[ 70 ]
top_level_dir
Normally, a formula directory will contain another directory with the files that belong to
the formula. For instance, the BQBDIF formula contains another directory called BQBDIF.
However, you may wish to use a directory that does not match the name of the formula. For
instance, you may have an BQBDIF formula with a directory inside of it called BQBDIF. In
this case you would set UPQ@MFWFM@EJS to BQBDIF.
license
The MJDFOTF field contains the license under which the formula itself (and not any
packages managed by the states inside the formula) has been released.
A complete formula file might look like this:
OBNFBQBDIF
PT3FE)BU%FCJBO6CVOUV4VTF'SFF#4%
PT@GBNJMZ3FE)BU%FCJBO4VTF'SFF#4%
WFSTJPO
SFMFBTF
NJOJNVN@WFSTJPO
UPQ@MFWFM@EJSBQBDIF
TVNNBSZ'PSNVMBGPSJOTUBMMJOH"QBDIF
EFTDSJQUJPO'PSNVMBGPSJOTUBMMJOHUIF"QBDIFXFCTFSWFS
PQUJPOBMNPE@QFSM
SFDPNNFOEFENPE@TTM
Once you have a '03.6-" file in place, you can issue an TQNCVJME command on the
directory that it lives in, like this, for instance:
# spm build /srv/formulas/apache-formula
This command assumes that the '03.6-" file exists inside TSWGPSNVMBTBQBDIF
GPSNVMB. If that file contains the aforementioned '03.6-" content, then it is also assumed
that there is an BQBDIF folder there as well (that is, TSWGPSNVMBTBQBDIF
GPSNVMBBQBDIF).
In the case of this formula, a file will be created here:
TSWTQN@CVJMEBQBDIFTQN
Once it has been created, you may distribute it as you feel appropriate.

Managing States
[ 71 ]
Where to use SPM
Because formulas are so commonly used on the master, a common misconception has arisen
that SPM may only be run on the master. In fact, SPM can be run on any machine running
Salt 2015.8 or greater, whether the master or minion service is running or no active Salt
service is running at all.
SPM configuration
It may seem strange to wait until the end to go over SPM configuration, but I did this for
two reasons:
These options don't make a lot of sense without understanding the rest of SPM
The defaults will work well for most installations.
However, you may have a need to tweak certain settings as your usage of SPM increases.
These settings may be set in either the NJOJPO or NBTUFS configuration file or in the TQN
configuration file. The one exception is DPOG@GJMF, which we'll cover first.
spm_conf_file
Default: FUDTBMUTQN
The name and location of the SPM configuration file is held in TQN@DPOG@GJMF. Obviously,
it doesn't make sense to place this inside the SPM configuration file itself.
formula_path
Default: TSWTQNTBMU
The GPSNVMB@GJMF setting holds the directory where SPM will install formula files. This is
intentionally set outside of the standard location for formula files (TSWTBMU) as it is
common for organizations to keep the files in the standard location inside some sort of
revision control.

Managing States
[ 72 ]
pillar_path
Default: TSWTQNQJMMBS
As with GPSNVMB@QBUI, QJMMBS@QBUI complements the standard pillar path of
TSWQJMMBS. Each formula will normally contain a QJMMBSFYBNQMF file, which will be
renamed to match the formula name, and placed in this directory. For example,
QJMMBSFYBNQMF in the Apache formula will be installed as
TSWTQNQJMMBSBQBDIFTMTPSJH.
reactor_path
Default: TSWTQNSFBDUPS
When a formula's name ends in SFBDUPS, its files will be installed in the directory
specified by SFBDUPS@QBUI rather than in the GPSNVMB@QBUI.
spm_logfile
Default: WBSMPHTBMUTQN
The TQN@MPHGJMF setting has the location of SPM's log file.
spm_default_include
Default: TQNEDPOG
The TQN@EFGBVMU@JODMVEF setting complements TQN@DPOG@GJMF by specifying a
directory in which other SPM-related configuration files may live. The directory specified
here will be located in the same directory as the SPM configuration file.
spm_repos_config
Default: FUDTBMUTQNSFQPT
Repository definitions may live entirely inside TQN@EFGBVMU@JODMVEF if you wish.
However, this file is also used as the base for the E directory, where SFQP files may live.
For instance, a setting of FUDTBMUTQNSFQPT will also automatically include
FUDTBMUTQNSFQPTESFQP.

Managing States
[ 73 ]
spm_cache_dir
Default: WBSDBDIFTBMUTQN
Any cache files belonging to SPM will live in TQN@DBDIF@EJS. This includes both copies of
any repository metadata and copies of any packages that were downloaded from those
repositories to be installed.
spm_build_dir
Default: TSWTQN@CVJME
When an SPM package is built, it will be created in the TQN@CVJME@EJS directory.
spm_build_exclude
Default: <	$74		IH		HJU		TWO	>
When an SPM package is built, files ending in any of these names will be skipped. This
helps keep SPM packages clean.
spm_db
Default: WBSDBDIFTBMUTQNQBDLBHFTEC
Metadata concerning installed packages is stored inside the TQN@EC file.
Summary
One SLS file is powerful, but combining them together into multiple, connected SLS files
can make for an infrastructure definition that is complex but not complicated. The key to
combining states together is the JODMVEF directive. Data that is included like this can be
augmented using the FYUFOE directive.

Managing States
[ 74 ]
Multiple environments can also be used to merge together state trees, combining multiple
types of formulas into a cohesive infrastructure definition. Master tops can be further used
to define those environments and the targets that go in them.
There are already a number of formulas available from the community, which are provided
by the Salt Stack Formulas organization on GitHub. You can use these or your own
formulas to build SPM packages, making the distribution of formulas as easy as using other
package managers.
Next up, we'll take a deep dive into the hows, whys, and how-tos about Salt SSH.

4
Exploring Salt SSH
Salt introduced the powerful concept of using message queues as a communication-
transport mechanism. There are times when the old tools just make sense. However, there's
no reason not to give them a kick in the seat of their pants when necessary. This is why Salt
SSH was created. In this chapter, we'll cover the following topics:
Using rosters
Building dynamic rosters
Understanding the salt-thin agent
Using the raw SSH mode
Grappling with SSH
SSH is in fact based on concepts very different from the primary architecture of Salt. Salt
was designed to communicate with large numbers of remote machines at once; SSH was
designed to interact with only one at a time. Let's take a few minutes to examine some of the
differences between Salt and SSH.
Remote shells
Let's take a step back in time to when the Internet wasn't around yet and the ARPANET
was brand new. To accompany this new concept to nationally and globally interconnected
networks, a series of new protocols was introduced. Telnet, a communication mechanism to
take advantage of them, was also introduced. Internet protocols were based on telnet,
including a remote shell.

Exploring Salt SSH
[ 76 ]
As security needs grew, so did the need to secure telnet. SSH was born; eventually, the
OpenSSH project was broadly shipped and supported by a number of Unix-based
platforms. While SSH means Secure Shell, it was in fact designed to securely tunnel
applications that had traditionally communicated with telnet. The default application was a
shell, replacing traditional telnet and its kin, but many, many more possibilities also existed.
With all of this in mind, it makes sense that developers and administrators would be so
used to their shell-based remote administration tools. SSH offers security, a reasonably (but
not completely) consistent platform between remote systems, and a familiar environment to
work in. It was never designed to handle communications between multiple machines.
A number of solutions were available to address this situation. SSH password agents and
passwordless SSH keys, along with with the power of shell scripts, comprised the bulk of
the solutions for years. One particular tool called ClusterSSH allowed multiple login
windows to accept input from a single location and relay it across all connections.
Salt SSH was not the first of these. It was released by SaltStack to accommodate the needs of
some of their users, who enjoyed the principles behind the Salt framework but had a need
to use SSH in an automated fashion to manage some of their systems.
Using rosters
Salt was originally designed to operate without the traditional database that many of its
forefathers used to store remote system configuration. As its message bus could retrieve
information directly from remote machines, often faster than a database lookup, the need
for a database was minimized.
As minions connect to the master and not the other way around, in a traditional Salt
infrastructure, the master did not even have a need to store the network and host
configuration for the minions. The game changes when dealing with SSH-based connections
because the master necessarily connects to its minions via SSH.
Rosters were introduced as a means for Salt SSH to keep track of the host information. The
default roster, which uses flat text files, is enough to get the job done. More dynamic rosters
add vast depths of power.

Exploring Salt SSH
[ 77 ]
The flat roster
As its name suggests, this roster uses a flat file. This is normally stored as
FUDTBMUSPTUFS, but it can be changed with the SPTUFSGJMF option:
# salt-ssh --roster-file=/etc/salt/altroster myminion test.ping
At its most basic level, this file needs to contain only two pieces of information: the name of
a minion and a network address (IP or hostname) through which this minion is reached:
(In FUDTBMUSPTUFS)
EVGSFTOF
BESJB
BDIBU[
CMVNFOUIBM
However, more information can be added as required:
(In FUDTBMUSPTUFS)
EVGSFTOF
IPTU
VTFSXE
BESJB
IPTU
QBTTXECVMMJ
BDIBU[
IPTU
QSJWSPPUBMJOFBQFN
CMVNFOUIBM
IPTU
TVEP5SVF
The options supported in a flat roster file as of version 2016.3 are discussed in the following
subsections.
host
IPTU can be an IP address or a hostname address. It should contain only the address and no
other information, such as protocol, port, and so on.

Exploring Salt SSH
[ 78 ]
port
QPSU is normally port 22, the SSH default. In nonstandard installations, this value can be
changed as appropriate.
user
VTFS will default to the user running the TBMUTTI client, which is normally SPPU. As
system administration tasks are often carried out using the root user, this is generally okay.
If the username differs, add this field.
If there is a need to run different tasks as different users on the same machine, it may be
useful to create separate roster entries for each user.
passwd
If using password authentication, then the value in QBTTXE is the password to use. This is
normally discouraged because this file is plain text and viewable by anyone with
appropriate permissions. If passwords are unavoidable in a roster file, then the read
permissions on the file should be restricted to the very least.
This option can be avoided by specifying the password from the command line with the 
QBTTXE option:
# salt-ssh --passwd=verybadpass myminion test.ping
Alternately, Salt SSH can prompt the user for the password, eliminating the need for a plain
text password to ever appear on screen:
# salt-ssh --askpass myminion test.ping
This should only be required on the first execution. Salt will ask whether or not it should set
up its own access key for future commands (see the QSJW option). If allowed, subsequent
commands will just use Salt's own SSH key instead of the user password.

Exploring Salt SSH
[ 79 ]
sudo
In a situation where a privileged command must be performed by an unprivileged user, the
TVEP option should be set to 5SVF. The default is 'BMTF. As of version 2016.3, the user
specified must be set to not require a password. This can be accomplished by editing the
TVEPFST file (normally, FUDTVEPFST is editable with the WJTVEP command) and adding
the /01"448% flag to a user's entry:
IFTUPO"--
"--/01"448%"--
priv
In a situation where a private key is required to access a minion, the QSJW option will
specify the path to a user-defined private key. If no such key is defined, then Salt will create
one. The default location is FUDTBMUQLJTTITBMUTTISTB.
timeout
The UJNFPVU option is the number of seconds to wait for an SSH connection to be
established. The default is .
thin_dir
The UIJO@EJS option is the directory on the target minion in which Salt's thin agent will be
installed. This agent is discussed in more detail later in the chapter, in the Understanding the
salt-thin agent section.
Other built-in rosters
A number of other rosters ship with Salt, which allows a much more dynamic means of
identifying hosts and their connection parameters. To use a different roster than the
standard flat file, add the SPTUFS option to TBMUTTI:
# salt-ssh --roster=cloud myminion test.ping
There are additional rosters available as of version 2016.3, which we will look at now.

Exploring Salt SSH
[ 80 ]
scan
The TDBO roster was the first dynamic roster to ship with Salt SSH. It directs the client to
attempt to log in to a range of IP addresses in order to issue the requested command. This
roster is unique in that it does not make use of a minion ID; the IP address that is generated
in the scan is used instead.
The following command will scan a small subnet and return 5SVF for each IP address that
is able to answer:
# salt-ssh --roster=scan 10.0.0.0/24 test.ping
There are some considerations that should be kept in mind when working with the scan
roster. First of all, all the connection information (aside from the IP address) needs to be the
same across all hosts. The exception to this is SSH keys that have already been established
and stored in an SSH key agent, if applicable.
However, there are security concerns with using preexisting SSH keys. Consider a scenario
where you have deployed your public key across your entire infrastructure. Believing your
network to be secure, you assume that any machine that is accessible via your key belongs
to you and can be trusted. However, your key, public as it is, is acquired by an attacker in
your network, who proceeds to set up a bogus minion with it. As you issue what you
believe to be secure commands using the scan roster, which may include sensitive data,
their minion is busy collecting this data.
This is not an attack vector unique to Salt SSH. This attack was in use long before
automated SSH tools hit the market; users have been falling prey to it for years. Rather than
using the scan roster to issue sensitive commands, it should be used only for network
discovery.
There are two specific options that can be used with the scan roster. The TDBOQPSUT
option accepts a comma-separated list of ports to attempt to log in to the target minion. The
default is . As this may be seen as a form of port scanning in some organizations, be sure
to check your security policy before using this option. The TDBOUJNFPVU option can
also specify a timeout for the scanning process. The default is , which translates to 10
ms.

Exploring Salt SSH
[ 81 ]
cache
While Salt was not initially designed to use a database, some optimizations have since been
added that accomplish many of the same goals. In particular, the grains for each minion are
cached by default on the master. As IPv4 addresses are among the data stored with the
grains, the DBDIF roster takes advantage of it to obtain IP addresses for minions that have
already been accessed via another transport mechanism, such as Salt's ZMQ-based
transport.
This can be useful when troubleshooting a machine on which the TBMUNJOJPO client was
previously installed but is now no longer responding. So long as the IP address has not
changed, the DBDIF roster can look it up with the following command:
# salt-ssh --roster=cache myminion service.start salt-minion
As of version 2016.3, the limitations of this roster are similar to the TDBO roster. The user
will default to whichever user is issuing the TBMUTTI command. Also, if SSH keys are not
established or specified with QSJW, passwords must be supplied with either QBTTXE or
BTLQBTT.
The cache roster only supports IPv4 addresses. However, it can differentiate between
addresses that are local (), private (,  and
), or public (everything else). It can also be configured to prefer one type
over another. This is done with the SPTUFS@PSEFS option in the master configuration. The
default is this:
SPTUFS@PSEFS
QVCMJD
QSJWBUF
MPDBM
cloud
The DMPVE roster is similar to the DBDIF roster, but there are some differences. As with Salt,
Salt Cloud caches information by default about the minions that it creates. The difference is
that minions created with Salt Cloud don't need to have a connection established with the
Salt master in order to have their IP address cached; since the hostname was required to run
the deploy process in the first place, Salt Cloud already knows what it is and caches it.

Exploring Salt SSH
[ 82 ]
Keep in mind that if a minion that was created with Salt Cloud reboots, then the IP address
will likely change unless it is specifically configured not to. However, issuing a full query
(covered in $IBQUFS, Taking Salt Cloud to the Next Level) will refresh this data. Therefore,
the DMPVE roster may be able to obtain the IP information that is otherwise not accessible to
the DBDIF roster:
# salt-ssh --roster=cloud myminion service.start salt-minion
The DMPVE roster also has the ability to borrow authentication information (SSH keys,
passwords, and so on) from the provider or profile configuration that was used to create the
minion. So long as this information has not changed, it should not be necessary to specify
that information again.
Like the DBDIF roster, the DMPVE roster supports the SPTUFS@PSEFS option in the master
configuration with the same defaults.
ansible
A notable SSH automation platform is the Ansible product. This program has seen
widespread adoption (especially among developers) because of its ease of use and its
abundant suite of tools. Many users of Ansible have found a need to use both Salt and SSH
to manage their machines. Other users have decided to switch altogether.
Rather than rosters, Ansible uses inventories to maintain the host information. The BOTJCMF
roster allows Salt SSH to natively use Ansible inventories instead of roster files to obtain the
host information.
As a path to an Ansible inventory must be specified, the SPTUFSGJMF option is used in
conjunction with this roster:
# salt-ssh --roster=ansible --roster-file=/etc/salt/hosts myminion
    test.ping
Building dynamic rosters
There is no reason to restrict yourself to the rosters that ship with Salt. As with pillars, any
external source that can provide the appropriate data can be used. In the case of rosters, the
appropriate data should look like the data that is stored in flat roster files.
Consider the following entry from a flat roster file:
NZNJOJPO

Exploring Salt SSH
[ 83 ]
IPTU
VTFSMBSSZ
QBTTXPSEH[WS
If you have a data source that provides this data, then you can plug the data into it. In fact,
if you have a command that outputs this data, for instance in YAML, then you can easily
write a roster that wraps this command.
JNQPSUZBNM
EFGUBSHFUT
UHUUHU@UZQF	HMPC	LXBSHT
SFUVSOZBNMTBGF@MPBE
@@TBMU@@<	DNESVO	>
	DBUFUDTBMUSPTUFS	
This is almost identical to the code used in the DNE@ZBNM external pillar, but it can be
adapted for use with rosters. Even if you don't know Python, the preceding code can easily
be changed to wrap your own custom commands, even those written in a different
language.
Using Salt SSH
We've spent some time discussing how to configure minions using rosters. Let's take a few
minutes to discuss some basic usage.
The TBMUTTI command is very similar to the TBMU command in usage. A target is
provided, followed by a module and function, which is optionally followed by any
arguments for the function. Targets can be specified in the same way as with the TBMU
command, although not as many target types are supported. As of version 2016.3, the
following target types are supported by Salt SSH:
Glob (the default)
Perl Compatible Regular Expression (& or QDSF)
List (- or MJTU)
Nodegroup (/ or OPEFHSPVQ)
Range (3 or SBOHF)
Using a target type is the same as with the TBMU command:
# salt-ssh -L minion1,minion2 test.ping
All the outputters available in the TBMU command are also available and accessed the same
way. The command to access the outputters is as follows:
# salt-ssh myminion grains.items --out json

Exploring Salt SSH
[ 84 ]
Other options are unique to Salt SSH. For instance, in order to target by grain, the master
needs to have a copy of the minion's grain data in its cache. This can be done using the 
SFGSFTI flag:
# salt-ssh --refresh myminion test.ping
Using a Saltfile
If a number of options are commonly used with Salt SSH, it can become cumbersome to
type all of them. Fortunately, a Saltfile can be used to automate adding these options. This
is a YAML file that can contain any number of options that are normally passed on the
command line. The following is a snippet from a Saltfile:
TBMUTTI
NBY@QSPDT
XJQF@TTI5SVF
This file is normally called 4BMUGJMF. If it is present in the current working directory when
the TBMUTTI command is issued, it will be used. The TBMUTTI command can also point
directly to a Saltfile in another directory, as shown in the following code:
# salt-ssh --saltfile=/etc/salt/Saltfile myminion test.ping
If you have a global Saltfile that you want to use everywhere, you can create a shortcut to it
as well with an alias (if your shell supports it):
# alias salt-ssh='salt-ssh --saltfile=/etc/salt/Saltfile'
You can also set an environment variable called 4"-5@4"-5'*-&:
# export SALT_SALTFILE=/etc/salt/Saltfile
The following options and their command-line equivalents are available in a Salt SSH
Saltfile:
SBX@TIFMM (S, SBX, SBX@TIFMM)
SPTUFS (SPTUFS)
SPTUFS@GJMF (SPTUFSGJMF)
SFGSFTI@DBDIF (SFGSFTI, SFGSFTIDBDIF)
NBY@QSPDT (NBYQSPDT)
FYUSB@GJMFSFGT (FYUSBGJMFSFGT)
XJQF@TTI (XXJQF)
TTI@QSJW (QSJW)

Exploring Salt SSH
[ 85 ]
JHOPSF@IPTU@LFZT (J, JHOPSFIPTULFZT)
TTI@VTFS (VTFS)
TTI@QBTTXE (QBTTXE)
TTI@BTLQBTT (BTLQBTT)
TTI@LFZ@EFQMPZ (LFZEFQMPZ)
TTI@TDBO@QPSUT (TDBOQPSUT)
TTI@TDBO@UJNFPVU (TDBOUJNFPVU)
Salt versus Salt SSH
In its default mode, Salt SSH is designed to behave (as far as VTFS is concerned) exactly like
the TBMU command. Minions can be targeted just like the TBMU command, modules are
made available exactly the same way across minions, arguments are specified the same
way, and output is displayed in exactly the same way. However, there are some differences.
Architecture
The primary difference between the standard TBMU command and the TBMUTTI command
is how the communication is architected based on the transport mechanism. The TBMU
command uses a message queue whereas the TBMUTTI command uses SSH (of course).
Think of the message queue like a television station. All of your minions are watching the
television and waiting for instruction. As tasks are issued, they will be broadcast to the
minions along with information on who should perform them. When a minion finishes a
task, it will return the result to the master along a similar queue. The transmission from
master to minions is a one-to-many communication whereas the transmission back is a
many-to-one communication. In fact, since the master makes use of its own group of local
workers to receive responses, the transmission back really is more of a many-to-not-quite as-
many connection.
SSH is more like a telephone line in that it is designed for one-to-one communication.
Individual minions listen for their phone to ring, and when a call comes in with a task, they
can execute it and return a result immediately. The more minions required to perform tasks,
the more phone calls must be made. The master can use local workers to set up multiple
concurrent connections, much like a call center, but each task must still be relayed
individually.

Exploring Salt SSH
[ 86 ]
Performance
Another difference is performance. Salt SSH is very fast, but it has some overhead, some of
which is consistent with SSH in general. The following actions are performed in addition to
what Salt already does:
Building and deploying the salt-thin agent
Building and deploying the states tarball
Establishing an SSH connection to the target
The last of these will happen with any program that uses SSH under the hood. The others
may or may not happen with other frameworks. In a small infrastructure, this may be
unnoticeable, but in a larger setup, it may be problematic.
The salt-thin agent (covered in the next section) is not a problem because it will be
generated in the first connection. Then, it will be cached until the version of Salt changes on
the master.
The state tarball (also covered in the next section) will be generated each time a state run is
issued, which does cause some slowdown. However, it will not affect other execution
modules.
Establishing SSH connections may be the biggest overhead. One system can only maintain
so many connections at once. In fact, with a large enough job, Salt SSH will limit the
number of active connections to 25 by default. This can be changed with the NBYQSPDT
flag:
# salt-ssh --max-procs 100 '*' test.ping
Exercise caution here: increasing the maximum number of connections to a number that is
not supported by the available resources can cause other issues outside Salt.
Understanding the salt-thin agent
As it turns out, automating SSH commands is not as simple as it may look at first. In an
environment where every server runs exactly the same version of exactly the same
operating system and exactly the same pieces of software, executing remote commands can
be greatly simplified. However, very few environments meet this requirement, and Salt was
designed to handle multiple environments.

Exploring Salt SSH
[ 87 ]
In order to accommodate these disparate configurations, the code that performs the tasks
needs to be able to autodetect pieces of its environment and then execute the tasks required
by the user. In short, a piece of software that behaves exactly like Salt is necessary.
Fortunately, in the Salt environment, that software is already available.
The salt-thin agent was designed to be a lightweight version of Salt that could be quickly
copied to a remote system by Salt SSH in order to perform tasks. It doesn't ship with Salt
(not as such, at least). It is built as needed by Salt SSH using the Salt version and modules
already installed on the master.
Once salt-thin has been packaged, it can be copied to the target system, where it is
unpacked and then executed. Let's look at the specifics.
Building the thin package
In its default mode, Salt SSH requires the thin package. The raw mode doesn't require the
UIJO package, but we'll cover this in a bit. However, since the package doesn't ship with
Salt in a form that is usable by Salt SSH, it gets built on demand and cached for later use.
The UIJO package contains just a little more than the bare essentials to run Salt. All the files
for this package are collected from various locations on the master. Most of these files exist
inside the installation directories for Salt itself, but some belong to other packages that Salt
depends on.
Interestingly, not all of Salt's dependencies will be needed. Many of the packages that Salt
normally depends on are not necessary. As the communication will happen using SSH
instead of ZeroMQ, this will not be included. The encryption libraries that are used to
secure this communication transport are also not needed because the connection is secured
by SSH itself.
Python will also not be packed in the UIJO package because a minimum version of Python
must already be installed on the target system. Why not include Python in the UIJO
package? There are a number of answers, but the most prominent one is binary
compatibility. Python itself is compiled differently across various platforms with different
versions of the HDD compiler. A master running Enterprise Linux would not be able to
control a target running Ubuntu because the version of Python on the master would not
meet the environment requirements on the target. Likewise, a 64-bit master would not be
able to control any 32-bit targets.

Exploring Salt SSH
[ 88 ]
Once the necessary files have been collected, they are bound to a tarball called TBMU
UIJOUH[. This package will contain only files that do not depend on the binary
compatibility between the master and minion. This limits the tarball not only to scripts,
mostly written in Python, but also to shell scripts (specifically the Bourne shell, also known
as TI).
The actual construction of the UIJO package is performed by the thin runner. If necessary,
for purposes such as testing, the UIJO package can be generated manually using this
runner:
# salt-run thin.generate
The tarball will be saved in the cache directory on the master, usually in the
WBSDBDIFTBMUNBTUFSUIJO directory. If the file already exists, you will want to
make sure to tell the runner to overwrite it. The command to overwrite the file is as follows:
# salt-run thin.generate overwrite=True
If you were to unpack the tarball after it was built, you'd find a small file structure with a
handful of files in it. Libraries such as Jinja2 and PyYAML will be there along with a
directory for Salt.
Including extra modules
By default, the UIJO package will include all the modules that ship with Salt and the
dependencies for the core Salt code. However, it will not include the Python modules that
are required for noncore modules. For instance, if you are using the FUDE execution module,
which requires the FUDE Python module, you need to be sure to include it in your UIJO
package by adding it to the end of the UIJOHFOFSBUF command. The command to carry
out this action is as follows:
# salt-run thin.generate etcd
Multiple modules can be specified and separated by commas with the following code:
# salt-run thin.generate etcd,MySQLdb
Deploying the thin package
After Salt SSH packages this file, it will be copied to the remote system. By default, it will be
placed in the UNQ directory in a hidden directory with the name of the user that will be
logged in on the target and a unique ID seeded with the hostname of the target system.

Exploring Salt SSH
[ 89 ]
For instance, on a system whose FQDN is simply EVGSFTOF, the directory may be called
UNQSPPU@E@@TBMU. This directory will be owned by the user that Salt SSH
logged in as (usually root), and the permissions will be set to  so that no other users
can read it.
If you would like to see this directory in action, including the unpacked UIJO package, you
can do so by executing some introspective Salt commands:
# salt-ssh myminion cmd.run 'ls -ls /tmp'
myminion:
    drwxrwxrwt 16 root   root    420 Apr  3 16:51 .
    drwxr-xr-x 20 root   root   4096 Jul 29  2014 ..
    drwx------  8 root   root    260 Apr  3 16:50
        .root_0338d8__salt
# salt-ssh myminion cmd.run 'ls -ls /tmp/.root*'
myminon:
    drwx------  8 root   root     280 Apr  3 17:43 .
    drwxrwxrwt 17 root   root     440 Apr  3 17:46 ..
    drwxr-xr-x  2 root   root     160 Apr  3 16:50
        certifi
    drwxr-xr-x  3 root   root     880 Apr  3 16:50
        jinja2
    drwxr-xr-x  2 root   root     220 Apr  3 16:50
        markupsafe
    drwxr-xr-x  4 root   root      80 Apr  3 16:50
        running_data
    drwxr-xr-x 31 root   root    1300 Apr  3 16:50 salt
    -rw-r--r--  1 root   root      79 Apr  3 16:50
         salt-call
    -rw-r--r--  1 root   root   27591 Dec 13 21:18
        six.py
    -rw-r--r--  1 root   root       8 Apr  3 16:50
        version
    drwxr-xr-x  2 root   root     720 Apr  3 16:50 yaml
Executing the thin package
Now that the thin package has been installed on the target, it must be executed. However,
there is more work to be done before Salt is actually executed. Python can live in different
locations depending on the environment, and Salt SSH needs to find where it is before it can
call it.

Exploring Salt SSH
[ 90 ]
The Salt SSH shim
A shim is a very tiny shell script whose job is to find the Python interpreter on the target
system and then use it to start Salt. It is encoded to a Base64 string on the master, sent to the
minion, decoded, and executed.
There are certain conditions that will affect how the shim is executed. If TVEP is required on
the target system, then the necessary commands will be embedded in the shim. If debug
logging is turned on on the master, then the shim will be executed in a debug mode, the
output of which will be displayed on the master.
The way the shim is run can also vary. If the target system requires a connection with a tty,
then the shim will be copied to the remote system using TDQ and then piped to CJOTI;
otherwise, it will be executed directly as one large command over SSH.
Preparing for Salt states
To run an execution command, not much is needed. However, executing Salt States does
require a little more work. This is because even a traditional minion that runs TBMUDBMM in
local mode requires a local copy of all the necessary files in the State tree.
When a Salt SSH command is executed using the State system, another tarball called
TBMU@TUBUFUH[ will need to be created. This file will be placed in the same hidden UIJO
directory on the target as the TBMUUIJOUH[ package. This tarball contains a copy of the
necessary files from the State tree on the master so that the TBMUDBMM command will have
access to everything that it needs for a State run.
The State tarball will also contain a copy of the State data (this is converted to low chunks)
and a copy of any pillar data from the master. These files can also be viewed with a couple
of the following introspective Salt commands:
# salt-ssh myminion state.single cmd.run name='tar
    -tzvf /tmp/.root*/salt_state.tgz'
myminion:
----------
          ID: tar -tvf /tmp/.root*/salt_state.tgz
    Function: cmd.run
      Result: True
     Comment: Command "tar -tvf /tmp/.root*
         /salt_state.tgz" run
     Started: 17:53:46.683337
    Duration: 7.335 ms
     Changes:
              ----------

Exploring Salt SSH
[ 91 ]
              pid:
                  26843
              retcode:
                  0
              stderr:
              stdout:
                  -rw-r--r-- root/root     15891 2015-04-03 17:53
pillar.json
                  -rw-r--r-- root/root       128 2015-04-03 17:53
lowstate.json
Summary
------------
Succeeded: 1 (changed=1)
Failed:    0
------------
Total states run:     1
# salt-ssh myminion state.single cmd.run name='tar
    -Ozxvf /tmp/.root*/salt_state.tgz lowstate.json'
myminion:
----------
          ID: tar -Ozxvf /tmp/.root*/salt_state.tgz lowstate.json
    Function: cmd.run
      Result: True
     Comment: Command "tar -Ozxvf /tmp/.root*/salt_state.tgz
lowstate.json" run
     Started: 17:58:35.972658
    Duration: 10.14 ms
     Changes:
              ----------
              pid:
                  29014
              retcode:
                  0
              stderr:
                  lowstate.json
              stdout:
                  [{"fun": "run", "state": "cmd", "__id__": "tar -Ozxvf
/tmp/.root*/salt_state.tgz lowstate.json", "name": "tar -Ozxvf
/tmp/.root*/salt_state.tgz lowstate.json"}]
Summary
------------
Succeeded: 1 (changed=1)
Failed:    0
------------
Total states run:     1

Exploring Salt SSH
[ 92 ]
Running Salt
Once the shim has found the Python interpreter and the TBMU@TUBUFUH[ tarball is
deployed (if necessary), it is able to execute Salt. Unlike a traditional Salt setup, it will not be
run as a daemon. Instead, the TBMUDBMM command will be executed in local mode, just like
it would on a minion. The output will then be collected by the Salt SSH client on the master,
parsed, and sent to the user. We can see this information by running with the USBDF log
level:
# salt-ssh myminion test.ping --log-level trace
...SNIP...
SALT_ARGV: ['/usr/bin/python2.7',
    '/tmp/.root_0338d8__salt/salt-call', '--local',
    '--metadata', '--out', 'json', '-l', 'quiet', '-c',
    '/tmp/.root_0338d8__salt', '--', 'test.ping']
_edbc7885e4f9aac9b83b35999b68d015148
    caf467b78fa39c05f669c0ff89878
[DEBUG   ] RETCODE localhost: 0
[DEBUG   ] LazyLoaded nested.output
[TRACE   ] data = {'myminion': True}
myminion:
    True
It is also possible to take a deeper look at the TBMU@TUBUFUH[ tarball, but it will require
logging in to the minion for the final command, as shown in the following code:
master# cp /etc/services /srv/salt/
master# salt-ssh myminion state.single file.managed /tmp/services
source=salt://services
myminion:
----------
          ID: /tmp/services
    Function: file.managed
      Result: True
     Comment: File /tmp/services is in the correct state
     Started: 18:18:28.216961
    Duration: 5.656 ms
     Changes:
Summary
------------
Succeeded: 1
Failed:    0
------------
Total states run:     1
minion# tar -tzvf /tmp/.root_0338d8__salt/salt_state.tgz
-rw-r--r-- root/root     15895 2015-04-03 18:18 pillar.json
-rw-r--r-- root/root       118 2015-04-03 18:18 lowstate.json

Exploring Salt SSH
[ 93 ]
-rw------- root/root    289283 2015-04-03 18:18 base/services
It will not be possible to view using two sequential TUBUFTJOHMF commands over Salt
SSH because the second command will generate a new TBMU@TUBUFUH[UBSCBMM, which
will not include the CBTFTFSWJDFT file. In order to obtain a truly informative view of the
target with a single TBMUTTI command, a full SLS file with enough States to perform
sufficient introspection on the target will be required.
Salt's running data
One more directory that you may have noticed in the temporary directory is the
SVOOJOH@EBUB directory. One design goal is to have Salt SSH remain as nonintrusive as
possible. This means that the directory structure that Salt normally uses has to live
someplace different: the temporary directory. We can take a look at this structure with
another Salt SSH command:
# salt-ssh myminion cmd.run 'tree /tmp/.root*/running_data'
myminion:
    /tmp/.root_0338d8__salt/running_data
    |-- etc
    |   `-- salt
    |       `-- pki
    |           `-- minion
    `-- var
        |-- cache
        |   `-- salt
        |       `-- minion
        |           `-- proc
        |               `-- 20150403195105124306
        `-- log
            `-- salt
                `-- minion
    11 directories, 2 files
As you continue to issue commands against this minion, the directory structure will
continue to grow and look more like a standard minion directory structure. If you want Salt
to completely remove all its traces when finished, including this directory, you can use the 
XJQF or X flag:
# salt-ssh --wipe myminion test.ping

Exploring Salt SSH
[ 94 ]
Using the raw SSH mode
Salt SSH is very powerful in its default mode with salt-thin. However, there are some
situations where it makes more sense to issue a raw SSH command. This can be
accomplished using the SBX flag (referred to in its short form as S from here on for
brevity).
Using the raw mode will bypass all the overhead of creating and deploying the UIJO
package: just log in to the target, issue a command, and log out. The following two
commands are functionally identical:
# salt-ssh myminion cmd.run date
myminion:
    Fri Apr  3 21:07:43 MDT 2015
# salt-ssh -r myminion date
myminion:
    ----------
    retcode:
        0
    stderr:
    stdout:
        Fri Apr  3 21:07:43 MDT 2015
However, the raw command will execute faster because it has less overhead. It will also
contain more information, such as 45%&33, 45%065, and the FYJU or SFUVSO code from the
command that was issued.
This can be useful if you wrap Salt SSH with another program that depends on the output
(especially the SFUVSO code) of the command on the remote machine. Make sure to run the
command with an outputter that is consistent and easy to parse, such as JSON:
# salt-ssh -r myminion 'ping 256.0.0.0' --out json
{
    "myminion": {
        "retcode": 2,
        "stderr": "ping: unknown host 256.0.0.0\n",
        "stdout": ""
    }
}
In this example, there is no output to examine, but the error message can certainly be
checked. Also, the SFUVSO code will always be available.

Exploring Salt SSH
[ 95 ]
Caching SSH connections
The raw SSH mode makes the execution model of Salt a little clearer. When a command is
executed anywhere in Saltbbe it the TBMU command, TBMUDBMM, or the TBMUTTI modebit
will start a job, issue the command, and return the result. Depending on how it is called,
Salt may or may not have a connection already established, but it will behave (so far as the
user is concerned) as if it were creating a new connection, executing the job, and tearing
down the connection.
This is fine in most instances, but there are some notable exceptions. For instance,
configuring a network switch over SSH can be problematic. This is because a number of
switches use the following configuration models:
SSH into the switch
Switch to a privileged user mode
Execute commands that change configuration
Review changes (if necessary)
Commit changes
Exit the privileged user mode
Log out of the switch
Trying to use Salt SSH in raw mode may take it as far as switching to privileged user mode,
but then it will log back out, forcing you to start over.
If you want to use OpenSSH on your master, you can take advantage of SSH, caching to
maintain a connection to the switch as necessary. This is not something that is built in to
Salt SSH, but it can be used nonetheless. It is especially useful when scripting Salt SSH in a
bash script, for instance.
First, use the following command to set up the connection:
# ssh -fMN -o "ControlPath /tmp/salt_ssh_ctrl" myminion.com
This will tell SSH to set up a connection in NZNJOJPODPN but to do nothing with it.
However, subsequent commands to that machine will automatically use the connection,
which will be cached with a socket stored at /UNQTBMU@TTI@DUSM on the master.
This trick is useful outside Salt SSH as well, especially if you are regularly issuing one-off
SSH commands against a machine. Even Salt SSH in its default and non-raw mode will see
a slight performance increase because the overhead of establishing and tearing down each
connection disappears.

Exploring Salt SSH
[ 96 ]
When you are finished with this host, be sure to tear down the connection, as shown in the
following code:
# ssh -O exit -o "ControlPath /tmp/salt_ssh_ctrl" myminion.com
This will disconnect it from the target and remove the socket file from the master.
Summary
Salt SSH is a powerful tool. It can be very comfortable for users in smaller infrastructures.
This tool can also be useful for dealing with devices that allow SSH connections but are not
able to have Python installed or cannot allow software (such as Salt) to be installed.
Rosters are used to store information about minions which Salt SSH will connect to.You can
use a 4BMUGJMF to store extra configuration in the current working directory. Salt SSH
makes use of a specialized tarball to deploy a thin version of Salt on the remote minion.
Next, we will delve into the asynchronous nature of Salt and start to really explore how Salt
can be used as an autonomous management platform.

5
Managing Tasks
Asynchronously
Salt is commonly thought of as a configuration management system. This is fine because
Salt does an excellent job of managing various aspects of its minions. However, this is only
a small part of what Salt can do. One of the biggest components is the event system, which
forms the basis of an asynchronous execution framework.
In this chapter, we'll spend some time looking at the following topics:
Going through the event system in depth
Understanding the reactor system
Building more complex reactors
Using the Thorium engine
Using the queue system
Looking at the event system
The event system is one of the oldest components of Salt. Yet, it is now used more than
almost any other part. Most of its usage is internal to Salt, but don't worry because there are
plenty of functionalities that we can take advantage of as users and administrators.

Managing Tasks Asynchronously
[ 98 ]
Reviewing the basics
Salt is built based on a message queue. Commands that are issued from the master generate
jobs, which are posted to the queue. Minions watch the queue for anything that targets
them. When a minion picks up a job, it attempts to perform the work associated with it.
Once it has finished, it posts the return data back to another queue; this is the one that the
master listens to.
Minions also have the ability to fire information that is not associated with a job that was
generated on the master. These pieces of information form the basis of the event bus.
There are in fact two event buses: one for minions to communicate with themselves (but not
with other minions) and one for minions to communicate with the master. The minion
event bus is currently only used internally by Salt. Minions only use it to fire events to
themselves. While it is possible for a user to manually or even programmatically fire
messages along the minion event bus, there is nothing built in to Salt for the user to directly
take advantage of.
The master event bus is a different story altogether. The ability of a minion to send
messages to the master is very powerful, especially with the reactor system in place on the
master. However, we'll get to this in just a moment.
The structure of event data
In older versions of Salt, event data was very simple: there was a message and a short tag.
Tags served as a short description of the message. This changed in version 0.17.0, when
both the message and the tag were expanded.
The tag, which was previously limited to 20 characters, now has no limit imposed on its
length. However, there are restrictions on which characters may be used: tags must be
ASCII-safe strings, and no Unicode is allowed.
The message was also expanded and is now often referred to as the event data or payload.
The most notable change involved moving it from a single string to a dictionary. Depending
on which part of Salt fired the event, there are certain pieces of data that can reasonably be
expected to appear. One piece of data that should always be expected is a timestamp called
@TUBNQ. This stamp will look something similar to the following code:
5

Managing Tasks Asynchronously
[ 99 ]
Other event data will vary. For instance, when a minion authenticates with the master, an
event will be fired with a tag called TBMUBVUI. The payload for this event will include a
timestamp (@TUBNQ), an action (BDU), the ID of the minion (JE), and the minion's public key
(QVC).
Watching event data
It is easier to get a sense of what event data looks like by watching events in real time-as
they occur. There is a script in the GitHub repository designed for this purpose, called
FWFOUMJTUFOQZ. As this is part of Salt's test suite, it does not ship with any of the
packages for individual distributions. However, it can be downloaded and used on any
system with Salt installed.
Installing the event listener
If you are only interested in using the event listener, it can be downloaded directly from
GitHub:
IUUQTSBXHJUIVCVTFSDPOUFOUDPNTBMUTUBDLTBMUEFWFMPQUFTUTFWFOUMJTUFOQZ
However, there are a number of other tests also available that may be interesting to you.
Assuming that you have Git installed, you can clone a copy of the repository and use the
event listener directly from there, like this, for instance:
# cd /root
# git clone https://github.com/saltstack/salt.git
# cd salt/tests
Using the event listener
As the most common usage of the event listener is on a Salt master with the default socket
location, it will use these settings by default. Just change to the directory that it resides in,
and issue the following command:
# python eventlisten.py
Note that because of differing Python versions and command names on different systems,
you may need to change the command to one that is more appropriate for Python 2 on your
system:
# python2 eventlisten.py

Managing Tasks Asynchronously
[ 100 ]
If you are listening to the minion event bus instead of the master, you need to tell the event
listener what kind of node you are working on (the default is NBTUFS):
# python eventlisten.py -n minion
If you have changed the location of Salt's socket directory, you will need to pass that in, as
shown in the following code:
# python eventlisten.py -s /var/run/salt
By default, the event listener assumes that you're using ZeroMQ, the default transport
mechanism for Salt. If you have configured it to use RAET instead, you'll need to specify it
as the transport, as follows:
# python eventlisten.py -t raet
Once you have started the event listener, it will show you the name of the socket that it will
listen to:
JQDWBSSVOTBMUNBTUFSNBTUFS@FWFOU@QVCJQD
It will then wait for events to appear on the bus. You can trigger events by issuing Salt
commands. Even a simple UFTUQJOH will generate a series of events that contain the job
data, as shown in the following code:
&WFOUGJSFEBU4BU"QS

5BH
%BUB
\	@TUBNQ		5		NJOJPOT	<	DBOUV	>^
&WFOUGJSFEBU4BU"QS

5BHTBMUKPCOFX
%BUB
\	@TUBNQ		5	
	BSH	<>
	GVO		UFTUQJOH	
	KJE			
	NJOJPOT	<	DBOUV	>
	UHU		DBOUV	
	UHU@UZQF		HMPC	
	VTFS		TVEP@IPNBSP	^
&WFOUGJSFEBU4BU"QS

5BHTBMUKPCSFUDBOUV
%BUB
\	@TUBNQ		5	

Managing Tasks Asynchronously
[ 101 ]
	DNE		@SFUVSO	
	GVO		UFTUQJOH	
	GVO@BSHT	<>
	JE		DBOUV	
	KJE			
	SFUDPEF	
	SFUVSO	5SVF
	TVDDFTT	5SVF^
In this case, there were three events fired. The first two denoted that a new job was created
with a job ID of . The first was an older style of the event, while
the second was a newer style. The event tagged as
TBMUKPCOFX contains information about the job and the user
that created it. We can see that it wasn't just created by the root user; it was created by a
user named IPNBSP, who issued the command using TVEP. The UFTUQJOH function was
sent directly to the DBOUV minion (otherwise, the target or UHU would be ), and there were
no arguments to it.
The last event, tagged as TBMUKPCSFUDBOUV, contains the
job return data from the minion. Among other things, we can see the function again, the
arguments for this function, and the return value from the function (5SVF). We even have
an indicator that tells us whether or not the job was completed successfully.
Firing custom data
It is possible to fire custom data from a minion to the master with the TBMUDBMM
command. Of course, it is also possible to issue a command from the master, which tells the
minion to fire a message back, but this is largely only useful for test purposes and little
more than an advanced UFTUFDIP command.
To fire a custom event to the master, both the message and the tag must be supplied in that
order. Doing this from the command line requires that the message be declared in a YAML-
parsable form. As it turns out, empty data is valid YAML. Issue the following command
from a minion:
# salt-call event.fire_master '' myevent

Managing Tasks Asynchronously
[ 102 ]
Take note of the two quotes between GJSF@NBTUFS and NZFWFOU, which will denote an
empty string. After issuing this command, look at the output in the event listener:
&WFOUGJSFEBU4BU"QS

5BHNZFWFOU
%BUB
\	@TUBNQ		5	
	DNE		@NJOJPO@FWFOU	
	EBUB	\^
	JE		DBOUV	
	QSFUBH	/POF
	UBH		NZFWFOU	^
&WFOUGJSFEBU4BU"QS

5BHTBMUKPCSFUDBOUV
%BUB
\	@TUBNQ		5	
	BSH	<			NZFWFOU	>
	DNE		@SFUVSO	
	GVO		FWFOUGJSF@NBTUFS	
	GVO@BSHT	<			NZFWFOU	>
	JE		DBOUV	
	KJE			
	SFUDPEF	
	SFUVSO	5SVF
	UHU		DBOUV	
	UHU@UZQF		HMPC	^
The first is the custom event that was requested by the command that we issued. We see the
NZFWFOU tag associated with it and the data (which was empty). To make this a little more
useful, let's add some actual YAML to our command:
# salt-call event.fire_master '{"key1": "val1"}'
    myevent
Doesn't look like YAML, does it? As JSON is syntactically correct YAML and more accurate
than it, it is safest to issue command-line data as a JSON string.
With this event, we sent a dictionary with a single key (LFZ) and its associated value
(WBM). The event listener will show the following data:
&WFOUGJSFEBU4BU"QS

5BHNZFWFOU
%BUB
\	@TUBNQ		5	

Managing Tasks Asynchronously
[ 103 ]
	DNE		@NJOJPO@FWFOU	
	EBUB	\	LFZ		WBM	^
	JE		DBOUV	
	QSFUBH	/POF
	UBH		NZFWFOU	^
&WFOUGJSFEBU4BU"QS

5BHTBMUKPCSFUDBOUV
%BUB
\	@TUBNQ		5	
	BSH	<	\LFZWBM^		NZFWFOU	>
	DNE		@SFUVSO	
	GVO		FWFOUGJSF@NBTUFS	
	GVO@BSHT	<	\LFZWBM^		NZFWFOU	>
	JE		DBOUV	
	KJE			
	SFUDPEF	
	SFUVSO	5SVF
	UHU		DBOUV	
	UHU@UZQF		HMPC	^
Once again, we can see the same kind of data as before, but now, we can see that an actual
data structure was returned in the custom event. However, it is still possible to make this
event even more useful.
Namespacing events
Part of the redesign of the event system involved making use of namespaced event tags.
You can see these by looking at the preceding examples. Consider this tag:
TBMUKPCSFUDBOUV
This tag is delimited by forward slashes. Once it's split up, we will see the following
components:
TBMU: This event was fired by Salt itself
KPC: This event pertains to Salt's job system
: This is the ID of the job
SFU: This event contains the return data from the job
DBOUV: This specifies the ID of the minion that will return the data

Managing Tasks Asynchronously
[ 104 ]
Other components of Salt will use a similar convention to tag their events. For instance,
consider the following event from Salt Cloud:
&WFOUGJSFEBU4BU"QS

5BHTBMUDMPVENZNJOJPODSFBUJOH
%BUB
\	@TUBNQ		5	
	FWFOU		TUBSUJOHDSFBUF	
	OBNF		NZNJOJPO	
	QSPGJMF		DFOUPT	
	QSPWJEFS		NZFDDPOGJHFD	^
The tag for this event can be broken down as follows:
TBMU: This event was fired by Salt itself
DMPVE: This event pertains to Salt Cloud
NZNJOJPO: This specifies the name of the VM that is affected
DSFBUJOH: This denotes what will happen to that VM now
When you create custom events for your own application or infrastructure, it may be useful
to namespace your own event tags in a similar fashion. Perhaps you have an internal
application in your organization that you call NDHFF, which manages a component that
archives the server data. You can make use of a tag similar to the following code:
NDHFFBSDIJWFJODSFNFOUBMNZTFSWFSTUBSU
To declare that a server called NZTFSWFS is starting an incremental backup process, add a
tag like this:
NDHFFBSDIJWFJODSFNFOUBMNZTFSWFSGJOJTI
This is to declare that the incremental backup has completed.
Namespacing guidelines
Why use slashes to delimit event tags? Most of the examples in this book use minion IDs,
which contain single words with punctuation. What if a minion ID contains a fully qualified
domain name? Consider the following event tag with periods instead of slashes on a minion
called XFCFYBNQMFDPN:
TBMUDMPVEXFCFYBNQMFDPNDSFBUJOH

Managing Tasks Asynchronously
[ 105 ]
Which is a part of the tag, and which is a part of the minion ID? We can tell this based on
our existing knowledge of the tag and the minion, but the ambiguity makes it very difficult
to accurately parse. Using slashes makes it much more obvious:
TBMUDMPVEXFCFYBNQMFDPNDSFBUJOH
This is why Salt itself uses slashes to delimit tags. Technically, it is entirely up to you how
you namespace your tags as long as you keep your tags ASCII safe. However, you should
keep some things in mind:
Tags are like index markers. They should be reasonably unique and adequately
describe the contents of the payload.
Keep tags as short as possible, but as long as necessary.
Use tags that are both human readable and machine parsable.
Forward slashes are standard in Salt; consider whether or not a different
delimiter would be confusing to experienced Salt users.
Some common events
There are a handful of events extremely common in Salt. Some are common only to various
subcomponents of Salt. Knowing what these events mean and how they work can be very
helpful when you build reactors.
salt/auth
Periodically, minions will reauthenticate with the master using this event. The following
data is contained in the payload:
BDU: This specifies the current status of the minion
JE: This denotes the ID of the minion
QVC: This specifies the public RSA key of the minion
SFTVMU: This denotes whether or not the request was successful

Managing Tasks Asynchronously
[ 106 ]
salt/key
When a minon's key is accepted or rejected on the master, the LFZ event will be fired. The
following data is contained in the payload:
JE: This specifies the ID of the minion
BDU: This denotes the new status of the minion
salt/minion/minion_id/start
When a TBMUNJOJPOprocess begins, it has some work to do before it is available to receive
commands. Once the process has finished starting up and is ready to perform jobs, it will
fire the TUBSU event. You may also see an event called NJOJPO@TUBSU with the same
payload. The NJOJPO@TUBSU event is a remnant of the old tag system and is expected to be
removed in a future release. The data contained in the payload is as follows:
DNE: This is another indicator to tell Salt which kind of event this is. In this case, it
will be @NJOJPO@FWFOU.
EBUB: This specifies the human-readable information about when the minion was
started.
JE: This denotes the ID of the minion.
QSFUBH: This is used internally by Salt to generate the namespace.
UBH: This is a copy of the event tag.
salt/job/job_id/new
When a new job is created, an event will be fired. This contains metadata about the job. The
following data will be contained in the event payload:
BSH: This specifies any arguments that were passed to the function.
GVO: This indicates the actual function that was called (such as UFTUQJOH).
KJE: This indicates the job ID.
NJOJPOT: This denotes a list of minions that are affected by this job.
UHU: This specifies the target that was specified for this job (such as ).
UHU@UZQF: This denotes the type of target that was specified (such as HMPC).
VTFS: This specifies the user that initiated this job. If the user used TVEP, then
TVEP@ will be prepended to the username.

Managing Tasks Asynchronously
[ 107 ]
salt/job/job_id/ret/minion_id
Once a minion finishes a job, it will fire an event with the return data. The following data
will be contained in the payload:
DNE: This is another indicator to tell Salt what kind of event this is. In this case, it
will be @SFUVSO.
GVO: As with TBMUKPCKPC@JE OFX, this indicates the actual function that
was called (such as UFTUQJOH).
GVO@BSHT: Similar to the preceding BSHT, this specifies any arguments that were
passed to the function.
JE: This is the ID that is returning the data.
KJE: This specifies the job ID.
SFUDPEF: This indicates the return code from the process that was used for this
job.
SFUVSO: This is all of the return data from this job. Depending on the function,
this could be very short or very long.
TVDDFTT: This indicates whether or not the job was completed successfully.
salt/presence/present
The QSFTFOU event will only be used when QSFTFODF@FWFOUT is set to 5SVF in the master
configuration. When enabled, this event will periodically be fired with a list of minions that
are currently connected to the master. The following data will be contained in the payload:
QSFTFOU: This specifies the list of minions that are currently connected
salt/presence/change
This event will only be used when QSFTFODF@FWFOUT is set to 5SVF in the master
configuration. When enabled, this event will be fired any time a minion connects to or
disconnects from the master. The following data will be contained in the payload:
OFX: This specifies a list of minions that have connected since the last presence
event
MPTU: This denotes a list of minions that have disconnected since the last presence
event

Managing Tasks Asynchronously
[ 108 ]
Common cloud events
Salt Cloud fires a number of events when you create or destroy machines. Which events are
fired and when depends on the cloud provider driver; some even fire events for other tasks,
but there are a few events that are generally found in all cloud drivers.
Cloud events are unique in that they don't necessarily refer to an existing ID; they refer to a
VM name. By design, the VM name in Salt Cloud matches the minion ID that is used by the
master. However, some events refer to a VM that is in the process of creation (and not yet
ready to receive commands), whereas others refer to a VM that is in the process of being
destroyed or has just been destroyed.
salt/cloud/vm_name/creating
The VM in question is about to be created. At this point, no actual work has been
performed. Every cloud driver is required to use the DSFBUJOH tag. The following data will
be contained in the payload:
OBNF: This contains the name of the VM to be created
QSPWJEFS: This indicates the name of the provider configuration used
QSPGJMF: This denotes the name of the profile configuration used
salt/cloud/vm_name/requesting
All the information required to create a VM has been gathered, and Salt Cloud is about to
make a request to the cloud provider that a VM be created. The following data will be
contained in the payload:
LXBSHT: This specifies all the arguments from the provider, profile, and, if used,
cloud map, which will be used to generate this request
salt/cloud/vm_name/querying
The cloud provider has begun the process of creating a VM and has returned an ID, which
Salt Cloud can use to refer to it. However, it has not yet returned an IP address that Salt
Cloud can use to access the VM. Salt Cloud will now wait for an IP address to become
available. The following data will be contained in the payload:
JOTUBODF@JE: This specifies the ID of the VM to be created as the cloud provider
knows it. This may not match the actual VM name or the minion ID.

Managing Tasks Asynchronously
[ 109 ]
salt/cloud/vm_name/waiting_for_ssh
An IP address has been returned for the VM, but it is not necessarily available. Salt Cloud
will now wait for the VM to become available and will be able to respond to SSH
connections. The following data will be contained in the payload:
JQ@BEESFTT: This denotes the hostname or IP address that will be used to
connect to this VM
salt/cloud/vm_name/deploying
The VM is now available via SSH (or, in the case of a Windows VM, SMB or WinRM). The
deploy script (or the Windows installer) and any accompanying files (such as public and
private keys and the minion configuration) will now be uploaded. Then, the deploy script
(or the Windows installer) will be executed. The following data will be contained in the
payload:
OBNF: This specifies the name of the VM that has been created.
LXBSHT: This denotes all the arguments that will be used to deploy Salt on the
target system. This is a very long list, and some of the items (such as the contents
of the deploy script) may also be extremely long.
salt/cloud/vm_name/created
The VM has been successfully created. This does not necessarily mean that the TBMU
NJOJPO process is able to receive connections. It may still be in its starting phase. There may
be firewall issues. Also, something may have caused the deploy script or the Windows
installer to fail. If you are waiting for a minion to be available, it is far more reliable to look
for the TBMUNJOJPONJOJPO@JE TUBSU tag. Every cloud driver is required to use the
TBMUDMPVEWN@OBNF created tag. The following data will be contained in the payload:

Managing Tasks Asynchronously
[ 110 ]
OBNF: This specifies the name of the VM that has been created.
QSPWJEFS: This denotes the name of the provider configuration used.
provider: This identifies the name of the profile configuration used.
JOTUBODF@JE: This specifies the ID of the VM as it is known by the cloud
provider. This may not be the same as the VM name or the minion ID.
salt/cloud/vm_name/destroying
Salt Cloud is about to make a request that a cloud provider destroy a VM. Every cloud
driver is required to use this tag. The following data will be contained in the payload:
OBNF: This specifies the name of the VM to be destroyed
salt/cloud/vm_name/destroyed
Salt Cloud has finished destroying a VM. Every cloud driver is required to use this tag. The
following data will be contained in the payload:
OBNF: This denotes the name of the VM that was just destroyed
Salt API events
Salt API is a daemon that ships with Salt, which provides a REST interface to be used to
control Salt instead of the command line. A notable feature of the Salt API is its ability to
fire custom events from a webhook. We will cover the configuration of the Salt API later on
in this chapter.
salt/netapi/url_path
The actual URL path that will be used will depend on how Salt API is configured is
configured with TBMUOFUBQJVSM@QBUI . Often, it will contain the word IPPL to denote
that it is a webhook, followed by a slash and an arbitrary command. The following data will
be contained in the payload:
EBUB: This denotes any custom data that was POSTed to the Salt API URL

Managing Tasks Asynchronously
[ 111 ]
Building reactors
Now you have seen what events look like, but what can you do with them? One of the most
powerful features that distinguishes Salt from similar systems is not only its ability to fire
events, but also the ability for the master to initiate new jobs based on the information
contained in the event.
This reactor system serves as a platform for users to build systems that are both
asynchronous and autonomous, which can range from simple to very complex.
Configuring reactors
Reactors are master-side processes, so none of the configuration needs to happen directly
on any minions. In fact, as the reactor system needs to actively listen to an event bus in
order for it to function, it doesn't even make sense to attempt to use it in a masterless
environment based on the TBMUDBMM commands.
Before setting up the master, decide which directory will contain reactor files. By
convention, this will be TSWSFBDUPS, but this is not a hardcore requirement and is not
enforced anywhere in Salt.
Reactors are set up in the master configuration file. The SFBDUPS block contains a mapping
of tags to look for, each of which will contain a list of SLS files that will be used when that
tag is found. Consider the following reactor block:
SFBDUPS
	TBMUNJOJPOTUBSU	
TSWSFBDUPSIJHITUBUFTMT
This is a very simple reactor that waits for minions to identify that they have started and are
ready to accept commands. When they do, it responds by calling the IJHITUBUFTMT file.
There are a couple of things to note here. First of all, the tag in this example doesn't contain
a minion ID; it contains a wildcard instead. Event tags are interpreted as globs by the
reactor system, allowing it to take advantage of namespaced tags and perform generalized
jobs based on events from specific minions.
Secondly, both the tag and the SLS file that follow are part of a list. There is no imposed
limit on how many tags may be watched by the reactor or how many SLS files may
accompany a tag.

Managing Tasks Asynchronously
[ 112 ]
This is important because SFBDUPS is a top-level declaration; you may not have multiple
SFBDUPS blocks in one master. However, the following single SFBDUPS block is valid:
SFBDUPS
	TBMUNJOJPONJOJPO@JE TUBSU	
TSWSFBDUPSIJHITUBUFTMT
	TBMUOFUBQJIPPLFDBVUPTDBMF	
TSWSFBDUPSFDBVUPTDBMFTMT
	TBMUDMPVEDSFBUJOH	
TSWSFBDUPSDMPVEDSFBUFBMFSUQBHFSEVUZTMT
TSWSFBDUPSDMPVEDSFBUFBMFSUIJQDIBUTMT
This block makes use of internal Salt events as well as two subsystems that ship with Salt:
the Salt API and Salt Cloud. We can make reasonable guesses as to what each SLS does
because they were given names that are somewhat human readable.
However, mapping out relationships between tags and files is only one part of the equation.
Let's see what reactor SLS files actually look like.
Writing reactors
As with other parts of Salt, reactors are written in YAML by default. And as with other
parts of Salt, reactors can also be written in any other format that is supported by Salt's
renderer system. For the moment, we will focus on reactors written in YAML with a little
Jinja templating.
Reactor SLS files resemble State SLS files that contain a block of data, which starts with an
ID followed by a function and any arguments to the function. The following is an example
of the IJHITUBUFTMT file referenced before:
IJHITUBUF@SVO
DNETUBUFIJHITUBUF
UHU\\EBUB<	JE	>^^
The ID for this SFBDUPS block is IJHITUBUF@SVO (not to be confused with \\EBUB<	JE	>
^^, which is a piece of Jinja templating that references the minion ID). The ID for each
SFBDUPS block is entirely arbitrary. Unlike with State SLS files, the ID does not affect any
other items in the block. It does need to be unique, but beyond this, you can consider it to be
a little more than a reference to you as to what the SFBDUPS block will do.

Managing Tasks Asynchronously
[ 113 ]
There are three different kinds of reactors that can be written: those that call execution
modules, those that call runner modules, and those that manage the master via XIFFM
modules. The function name for each of these will be preceded by DNE, SVOOFS, and XIFFM,
respectively. For instance, a reactor that uses DNESVO to execute an arbitrary command on
a minion would look like this:
MPDBMDNESVO
First, let's take a look at the reactors based on execution modules to get a feel of what
reactors are like. Runner and wheel reactors are both simpler, so once you understand
execution runners, the others will be easy.
Calling execution modules
As execution modules form the basis of Salt itself, it is no surprise that these would be the
most common types of reactor. As Salt States are kicked off using execution modules, even
State runs can be initiated here.
As execution modules are run on minions, they are targeted as they would be with the TBMU
command. The target is referred to as UHU. If a target type other than HMPC is to be used, it is
declared as UHU@UZQF. The target types supported by the reactor system are as follows:
HMPC
QDSF
MJTU
HSBJO
HSBJO@QDSF
QJMMBS
OPEFHSPVQ
SBOHF
DPNQPVOE
Most execution modules require a list of arguments. In a reactor, these may be declared in
one of two ways: BSH or LXBSH.

Managing Tasks Asynchronously
[ 114 ]
The BSH argument contains a list of arguments to be sent to the function in the order in
which they are expected to appear. This is directly analogous to the concept of BSHT in
Python.
LJMSPZ
DNEDNESVO
UHU\\EBUB<	JE	>^^
BSH
	UPVDIUNQLJMSPZ@XBT@IFSF	
The LXBSH argument contains a dictionary of argument names and the associated values to
be sent to the function. The order is not important here because the arguments are named.
This is directly analogous to the concept of LXBSHT in Python.
LJMSPZ
DNEDNESVO
UHU\\EBUB<	JE	>^^
LXBSH
DNE	UPVDIUNQLJMSPZ@XBT@IFSF	
Calling runner modules
As runner modules are executed on the master, no targeting is necessary. However, both
the BSH and LXBSH arguments are still valid and behave as they do with execution modules:
XFCIPPL
SVOOFSIUUQRVFSZ
BSH
IUUQFYBNQMFDPNQBUIUPXFCIPPL
XFCIPPL
SVOOFSIUUQRVFSZ
LXBSH
VSMIUUQFYBNQMFDPNQBUIUPPUIFSXFCIPPL
Calling wheel modules
Wheel modules also do not require targeting because they are designed to manage the
master itself. By far, the most common usage of wheel reactors is to either accept or delete
minion keys on the master.
BDDFQU@NJOJPO
XIFFMLFZBDDFQU
NBUDI\\EBUB<	JE	>^^

Managing Tasks Asynchronously
[ 115 ]
Exercise extreme caution when using wheel reactors, especially those that accept minion
keys. You can tell that the preceding reactor was not designed to be kicked off by an event
from a minion. How would the minion fire an event if it wasn't yet accepted on the master?
So, instead of including the Jinja code to make use of an ID, it instead looks inside the
payload of the event for a name.
This particular example does not perform any validation on the event to ensure that it came
from a trusted source. When you accept keys via the reactor system, it may be more
appropriate to render the reactor SLS in Python rather than YAML. One example of a
reactor that uses Python to perform validation is the EC2 Autoscale reactor, which can be
found here:
IUUQTHJUIVCDPNTBMUTUBDLGPSNVMBTFDBVUPTDBMFSFBDUPS
When you write reactors in Python, try to keep them as simple as possible. Salt will only
process one reactor at a time whereas complex reactors will cause others to begin to queue
up while waiting for their turn.
Writing more complex reactors
As the number of modules that ship with Salt is vast, there is an enormous number of
functionalities that can be harnessed in the reactor system. Let's take a look at a few use
cases and ways of how reactors can be used with them.
These examples will make use of various parts of Salt that are not covered in this chapter,
but we will try to keep them simple enough to only demonstrate the use case presented.
Sending out alerts
There is a growing number of modules appearing in Salt designed to send notifications to
others. Some of these, such as the TNUQ and IUUQ execution module, are based on the
longtime standards that the Internet is based on. Others, such as the QBHFSEVUZ and
IJQDIBU module, were built for commercial services. Some of them have free components,
whereas some require a paid account.
Let's set up a simple monitoring system that checks for disk space on a minion and sends
out an alert if the specified disk is too full. First, let's set up a monitoring State to keep an
eye on disk space.

Managing Tasks Asynchronously
[ 116 ]
Create TSWTBMUNPOJUPSEJTLTTMT with the following content:
SPPU@EFWJDF@TJ[F
EJTLTUBUVT
OBNF
NBYJNVN		
POGBJM@JO
FWFOUBMFSU@BENJOT@EJTL
BMFSU@BENJOT@EJTL
FWFOUTFOE
OBNFBMFSUBENJOTEJTL
Then, we will map the event tag to the reactor in the master configuration, as follows:
SFBDUPS
BMFSUBENJOTEJTL
TSWSFBDUPSEJTL@BMFSUTMT
While we're in the master configuration file, let's also add the configuration to use the
QBHFSEVUZ service:
NZQBHFSEVUZBDDPVOU
QBHFSEVUZTVCEPNBJONZTVCEPNBJO
QBHFSEVUZBQJ@LFZ"#$%&'
Then, we will create TSWSFBDUPSEJTL@BMFSUTMT in order to create an incident with
the QBHFSEVUZ service:
OFX@JOTUBODF@BMFSU
SVOOFSQBHFSEVUZDSFBUF@FWFOU
LXBSH
EFTDSJQUJPO-PX%JTL4QBDF\\EBUB<	JE	>^^
EFUBJMT4BMUIBTEFUFDUFEMPXEJTLTQBDFPO\\
EBUB<	JE		^^
TFSWJDF@LFZ"#$%&'BCDEF
QSPGJMFNZQBHFSEVUZBDDPVOU
In order to kick off this State and only this State, we can issue the following command:
# salt myminion state.sls monitor.disks
If Salt detects that the root device is within the specified parameters, the event will not be
fired, and the reactor will not be triggered:
MPDBM

*%SPPU@EFWJDF@TJ[F

Managing Tasks Asynchronously
[ 117 ]
'VODUJPOEJTLTUBUVT
/BNF
3FTVMU5SVF
$PNNFOU%JTLJOBDDFQUBCMFSBOHF
4UBSUFE
%VSBUJPONT
$IBOHFT

*%BMFSU@BENJOT
'VODUJPOFWFOUTFOE
/BNFBMFSUBENJOTEJTL
3FTVMU5SVF
$PNNFOU4UBUFXBTOPUSVOCFDBVTFPOGBJMSFREJEOPU
DIBOHF
4UBSUFE
%VSBUJPO
$IBOHFT
4VNNBSZ

4VDDFFEFE
'BJMFE

5PUBMTUBUFTSVO
However, if Salt detects that the root device has grown to more than  capacity, we will
see a different response:
MPDBM

*%SPPU@EFWJDF@TJ[F
'VODUJPOEJTLTUBUVT
/BNF
3FTVMU'BMTF
$PNNFOU%JTLJTBCPWFNBYJNVNPGBU
4UBSUFE
%VSBUJPONT
$IBOHFT

*%BMFSU@BENJOT
'VODUJPOFWFOUTFOE
/BNFBMFSUBENJOTEJTL
3FTVMU5SVF
$PNNFOU&WFOUGJSFE
4UBSUFE
%VSBUJPONT
$IBOHFT


Managing Tasks Asynchronously
[ 118 ]
EBUB
/POF
UBH
BMFSUBENJOTEJTL
4VNNBSZ

4VDDFFEFE
DIBOHFE
'BJMFE

5PUBMTUBUFTSVO
This is what we will see from the command line, but there will be more going on under the
hood. We can see it fire the BMFSU@BENJOT@EJTL event. What we won't see is the
EJTL@BMFSU reactor getting triggered, which will create an incident in 1BHFS%VUZ. At this
point, 1BHFS%VUZ will take over and send alerts to the admins configured in that service.
We can automate this process by using the Salt scheduler. To the minion configuration, add
the following block:
TDIFEVMF
EJTL@NPOJUPSJOH
GVODUJPOTUBUFTMT
TFDPOET
BSHT
NPOJUPSEJTLT
After making this change, restart the minion. From the point at which the minion starts up
again, it will issue the NPOJUPS@EJTLT SLS every 5 minutes.
Using webhooks
As mentioned previously, the Salt API provides a REST interface that can be used to accept
webhooks on the master. These webhooks are translated into events, which are intercepted
by the reactor.
Before we can accept webhooks, we need to configure the Salt API. First, edit the master
configuration to tell the Salt API to accept webhooks:
SFTU@DIFSSZQZ
QPSU
IPTU
TTM@DSUFUDQLJUMTDFSUTMPDBMIPTUDSU
TTM@LFZFUDQLJUMTDFSUTMPDBMIPTULFZ
XFCIPPL@VSMIPPL

Managing Tasks Asynchronously
[ 119 ]
XFCIPPL@EJTBCMF@BVUI5SVF
Instructions to create the TTM@DSU and TTM@LFZ files can be found in
$IBQUFS, Using Salt with REST, under the Creating SSL Certificates
section.
Next, we need to map any events that we are expecting to the corresponding SLS files. Add
the following lines to the master configuration:
SFBDUPS
TBMUOFUBQJIPPLDVTUPNFWFOU
TSWSFBDUPSXFCIPPLTMT
Let's assume that the master's hostname is TBMUNBTUFS for our purposes. This means that
the URL for this webhook is as follows:
IUUQTTBMUNBTUFSIPPLDVTUPNFWFOU
With the master configured, restart it. The Salt API is a separate process from the master, so
go ahead and start it up too:
# systemctl restart salt-master
# systemctl start salt-api
We can trigger this event from the command line of another system with cURL:
$ curl -k https://salt-master:8080/hook/customevent -H
    'Accept: application/json' -d passphrase=soopersekrit
If you're watching the event bus with FWFOUMJTUFOQZ, you will see the following event:
&WFOUGJSFEBU4BU"QS

5BHTBMUOFUBQJIPPLDVTUPNFWFOU
%BUB
\	@TUBNQ		5	
	CPEZ			
	IFBEFST	\	"DDFQU		BQQMJDBUJPOKTPO	
	$POUFOU-FOHUI			
	$POUFOU5ZQF		BQQMJDBUJPOYXXXGPSNVSMFODPEFE	
	)PTU		MPDBMIPTU	
	3FNPUF"EES			
	6TFS"HFOU		DVSM	^
	QPTU	\	QBTTQISBTF		TPPQFSTFLSJU	^^

Managing Tasks Asynchronously
[ 120 ]
Notice that we've used a passphrase here. HTTPS may protect the data being sent, but it
doesn't protect us from any unauthorized usage. It is still up to the user to implement their
own authentication scheme.
While reactors are normally written in YAML, we need something that allows you the logic
to actually check the passphrase. Fortunately, Jinja does provide enough logic to perform
this. Create TSWSFBDUPSXFCIPPLTMT, with the following content:
\TFUQBTTQISBTFEBUB<	QPTU	>HFU
	QBTTQISBTF			^
\JGQBTTQISBTF	TPPQFSTFLSJU	^
BVUIFOUJDBUFE
DNEDNESVO
UHUNZNJOJPO
BSH
	UPVDIUNQTPNFGJMF	
\FOEJG^
Jinja provides just enough logic for a simple authentication scheme. If something more
advanced is required, it may make sense to write the reactor in pure Python instead. The
following SLS is the Python version of the preceding YAML and Jinja reactor:
QZ
EFGSVO

QBTTQISBTFEBUB<	QPTU	>HFU
	QBTTQISBTF			
JGQBTTQISBTF	TPPQFSTFLSJU	
SFUVSO\
	BVUIFOUJDBUFE	\
	DNEDNESVO	<
\	UHU		EVGSFTOF	^
\	LXBSH	\
	DNE		UPVDIUNQTPNFGJMF	
^
^
>
^
^
This example shows two different SLS files. These files give a reasonably simple reaction to
the webhook. Let's get a little more advanced.

Managing Tasks Asynchronously
[ 121 ]
Reactors calling reactors
Let's set up a new set of reactors. First of all, let's add a couple of new events to the SFBDUPS
block in the master configuration:
SFBDUPS
	TBMUOFUBQJIPPLHPOEPS	
	TSWSFBDUPSHPOEPSTMT	
	TBMUOFUBQJIPPLSPIBO	
	TSWSFBDUPSSPIBOTMT	
The TBMUNBTUFS service will need to be restarted to pick up the new mapping, but the
TBMUBQJ service will not. Go ahead and restart the master with this command:
# systemctl restart salt-master
Next, create TSWSFBDUPSHPOEPSTMT, with the following content:
BTL@SPIBO@GPS@IFMQ
SVOOFSIUUQRVFSZ
LXBSH
VSM	IUUQMPDBMIPTUIPPLSPIBO	
NFUIPE1045
EBUB
NFTTBHF	3PIBOQMFBTFIFMQ	
Then, set up TSWSFBDUPSSPIBOTMT, with the following content:
SFTQPOE@UP@HPOEPS
DNEDNESVO
UHUHBOEBMG
BSH
FDIP	3PIBOXJMMSFTQPOE	 UNQSPIBOUYU
Go ahead and get things rolling. As we have one reactor calling another via another
webhook, we'll add a slight delay before checking for the response:
# curl https://localhost:8080/hook/gondor -H 'Accept:
    application/json' -d '{}' ; sleep 3; echo; cat
    /tmp/rohan.txt
{"success": true}
Rohan will respond

Managing Tasks Asynchronously
[ 122 ]
For the purpose of this example, this set of reactors will reside on the same master.
However, there is no reason that the URL and the SPIBO reactor couldn't exist on an
entirely different Salt infrastructure altogether.
This example also shows that as reactors have the ability to call each other, minions,
masters, and even entire infrastructures can be configured to communicate with each other
autonomously using a series of asynchronous events.
Using Thorium
The Thorium system is another component of Salt with the ability to watch the event bus
and react based on what it sees there. But the ideas behind it are much different than with
the reactor.
A word on engines
Thorium is one of the engines that started shipping with Salt in version 2016.3. Engines are
a type of long-running process that can be written to work with the master or minion. Like
other module types, they have access to the Salt configuration and certain Salt subsystems.
Engines are separate processes that are managed by Salt. The event reactor runs inside the
Salt processes themselves, which means that long-running reactor operations can affect the
rest of Salt. Because Thorium is an engine, it does not suffer from this limitation.
Looking at Thorium basics
Like the reactor, Thorium watches the event bus. But unlike the reactor, which is configured
entirely via SLS files, Thorium uses its own subsystem of modules (which are written in
Python) and SLS files. Because these modules and SLS files use the state compiler, much of
the functionality has been carried over.
In order to use Thorium, there are a few steps that you must complete. These steps work
together to form the basis of your Thorium setup, so be careful not to skip any.

Managing Tasks Asynchronously
[ 123 ]
Enabling Thorium
First, as an engine, you need to enable Thorium in the NBTUFS configuration file using the
FOHJOFT directive:
FOHJOFT
UIPSJVN\^
Because Thorium is so heavily configured using its own files, no configuration needs to be
passed in at this point. However, engines do need a dictionary of some sort passed in, so we
pass in an empty one.
Setting up the Thorium directory tree
With Thorium configured, we need to create a directory to store Thorium SLS files in. By
default, this is TSWUIPSJVN. Go ahead and create that:
# mkdir /srv/thorium/
If you'd like to change this directory, you may do so in the NBTUFS configuration file:
UIPSJVN@SPPUT
CBTF
TSWUIPSJVNBMU
Like the state system, Thorium requires a UPQTMT file. This is the first of many similarities
you'll find between the two subsystems. As with TSWTBMUUPQTMT, you need to specify
an environment, a target, and a list of SLS files:
CBTF
		
UIPSJVN@UFTU
To be honest, the environment and target really don't mean much; they are artifacts from
the state system, which weren't designed to do anything special inside of Thorium. That
said, the target does actually have some useful purposes.
The target here doesn't refer to any minions. Rather, it refers to the master that this top file
applies to. For example, if your master's ID is NPF and you set a target of DVSMZ, then this
top file won't be evaluated for that master.

Managing Tasks Asynchronously
[ 124 ]
Sound confusing? In a single-master, non-syndicated environment, it probably is. In such an
environment, go ahead and set the target to . But in an environment in which multiple
masters are present, you may wish to divide the workload between them. Take a look at
this UPQTMT file:
CBTF
	NPOJUPSJOHNBTUFS	
BMFSUT
HSBQIT
	QBDLBHJOHNBTUFS	
SFEIBUQLHT
EFCJBOQLHT
In this multi-master environment, the masters may work in concert to manage jobs among
the minions, but one master will also be tasked with looking for monitoring-related events
and processing them, while the other will handle packaging-related events.
There is a component of Thorium that we haven't discussed yet called the register. We'll get
to it in a moment, but this is a good time to point out that the register is not shared. This
means that if you assign two different masters to handle the same events, any actions
performed by one master will be invisible to the other. The right hand won't know what the
left is doing, as it were.
As you may expect, the list following each target specifies a set of SLS files to be evaluated.
But whereas state SLS files are only evaluated when you kick off a state run
(TUBUFIJHITUBUF, for instance), Thorium SLS files are evaluated at regular intervals. By
default, these intervals are set to every half second. You can change that interval in the
NBTUFS configuration file:
UIPSJVN@JOUFSWBM
Once you have your UPQTMT file configured, it's time to set up some SLS files.
Writing Thorium SLS files
Let's go ahead and create TSWUIPSJVNUIPSJVNUFTUTMT with the following content
in it:
TIFMM@UFTU
MPDBMDNE
UHUNZNJOJPO
GVODDNESVO
BSH
FDIP	UIPSJVNTVDDFTT	 UNQUIPSJVNUYU

Managing Tasks Asynchronously
[ 125 ]
I wouldn't restart your master yet if I were you. First, let's talk about what we're looking at
here.
This should look very familiar to you, with a few differences. As you would expect,
TIFMM@UFTU is the ID of this code block; MPDBMDNE refers to the module and function that
will be used, and everything that follows is arguments to that function.
The local module is a Thorium-specific module. Execution, state, runner, and other modules
are not available in Thorium without using a Thorium module that wraps them. The local
module is one such wrapper, which provides access to execution modules. As such, UHU
refers to the target that the module will be executed on, GVOD is the module and function
that will be executed, and BSH is a list of ordered arguments. If you like, you may use LXBSH
instead to specify keyword arguments.
Because state modules are accessed via the state execution module, MPDBMDNE would also
be used to kick those off; SVOOFSDNE is also available to issue commands using the runner
subsystem.
Now, why did I tell you not to restart your master yet? Because if you did, this SLS file
would run every half second, writing out to UNQUIPSJVNUYU over and over again. In
order to keep it from running so often, we need to gate it somehow.
Using requisites
Because Thorium uses the state compiler, all state requisites are available, and they all
function as you would expect. Let's go ahead and add another code block and alter our first
one a little bit:
DIFDLFS
DIFDLFWFOU
OBNFTBMUUIPSJVNUFTU
DPOUBJOT@DIFDL
DIFDLDPOUBJOT
WBMVFUSJHHFS
TIFMM@UFTU
MPDBMDNE
UHUNZNJOJPO
GVODDNESVO
BSH
FDIP	UIPSJVNTVDDFTT	 UNQUIPSJVNUYU
SFRVJSF

Managing Tasks Asynchronously
[ 126 ]
DIFDLDPOUBJOT@DIFDL
The DIFDL module has a number of functions that are designed to compare a piece of data
against an event and return 5SVF if the specified conditions are met.
In this case, we're using DIFDLFWFOU, which looks at a given tag and returns 5SVF if an
event comes in and matches it. The tag that we are looking for is TBMUUIPSJVNUFTU,
which is intended to look for TBMUUIPSJVNNJOJPO@JE UFTU. This isn't an official
event of any sort, just a made-up one for the purposes of this book. The event must also
have a variable called DIFDLFS in its payload, with a value of USJHHFS.
We have also added a SFRVJSF requisite to the TIFMM@UFTU code block, which will prevent
that block from running unless the right event comes in. Now that we're set up, go ahead
and restart the master and a minion called NZNJOJPO, and issue the following command
from the minion:
# salt-call event.fire_master '{"checker":"trigger"}'
    'salt/thorium/myminion/test'
local:
    True
You may need to wait a second or two for the event to be processed and the command from
TIFMM@UFTU to be sent. But then you should be able to see a file called UNQUIPSJVNUYU
and read its contents:
# cat /tmp/thorium.txt
thorium success
This particular SLS, as it is now, mimics the functionality of the reactor system, albeit with a
slightly more complex setup. Let's take a moment now to go beyond the reactor.
Using the register
Thorium isn't just another reactor. Even if it were, just running in its own process space
makes it more valuable than the old reactor. But the true value of Thorium comes with the
register.
The register is Thorium's own in-memory database, which persists across executions. A
value that is placed in the register at one point in time is will still be there a half hour later
unless the master is restarted.

Managing Tasks Asynchronously
[ 127 ]
Is the register really that fragile? At the moment, yes. And as I stated
before, it's also not shared between systems. However, it is possible to
make a copy of the register on disk by adding the following code block:
NZSFHJTUFSGJMF
   GJMFTBWF
This will cause the register to be written to a file called NZSFHJTUFSGJMF
at the following location:
WBSDBDIFTBMUNBTUFSUIPSJVNTBWFTNZSFHJTUFSGJMF
At the time of writing this, that file will not be reloaded into memory
when the master restarts. However, it is expected that persisting the
registry to disk and autoloading it on start will be added to Salt very
shortly, possibly by the time this book is published.
We're going to go ahead and alter our SLS file. The TIFMM@UFTU code block doesn't need to
change, but the DIFDLFS code block will. Remove the OBNF field and change the function
from DIFDLFWFOU to DIFDLDPOUBJOT:
DIFDLFS
DIFDLDPOUBJOT
WBMVFUSJHHFS
We're still looking for a payload with a variable called DIFDLFS and a value called
DIFDLFS, but we're going to look at the tag somewhere else:
NZSFHJTUFS
SFHTFU
BEEDIFDLFS
NBUDITBMUUIPSJVNUFTU
In this code block, NZSFHJTUFS is the name of the register that you're going to write to. The
SFHTFU function will add a variable to that register that contains the specified piece of the
payload. In this case, it will grab the variable from the payload called DIFDLFS and add its
associated value. However, it will only add this information to the registry if the tag on the
event in question matches the NBUDI specification (TBMUUIPSJVNUFTU).
Go ahead and restart the master, and then fire the same event to the master:
# salt-call event.fire_master '{"checker":"trigger"}'
    'salt/thorium/myminion/test'
local:
    True

Managing Tasks Asynchronously
[ 128 ]
If you've added the GJMFTBWF code block from before, we can go ahead and take a look at
the register:
# cat /var/cache/salt/master/thorium/saves
    /myregisterfile
{"myregister": {"val": "set(['trigger'])"}}
Looking forward
The Thorium system is pretty new, so it's still filling out. The value of the registry is that
data can be aggregated to it and analyzed in real time. Unfortunately, at the time of writing
this, the functions to analyze that data do not yet exist.
The Carbon release of Salt is expected to have modules that allow better access to that data
and the ability to perform an action based on a series of data that is collected over time. In
the meantime, the reactor functionality does already exist (again, in the Carbon release), so
we'll bring up a few more examples with it throughout the book.
Using the queue system
The queue system is another component of Salt with the ability to fire events. This can be
used by the reactor system. However, before we get ahead of ourselves, let's go through the
basics of using the queue system.
Learning how queues work
At its most basic level, the queue is very simple. Items can be added to the queue and then
processed at a later time in the order in which they were added. Depending on the queue
module being used, items may or may not be required to be unique.
For our examples, we'll use TRMJUF, the default queue module. This module should work in
any infrastructure because TRMJUF is built in Python. It will also automatically generate
any database files if they don't already exist. Take note that TRMJUF is one of the queue
modules that require items to be unique. If you want to use a different module, just add a
backend argument to any of the queue commands. For instance, to explicitly list queues
stored in TRMJUF, use the following command:
# salt-run queue.list_queues backend=sqlite

Managing Tasks Asynchronously
[ 129 ]
The queue system is managed by a runner. This means that queue databases will be
accessed only by the master. However, as you'll see later on, it can still be used to manage
tasks on minions.
Adding to the queue
Before we can do anything with a queue, we need to have some items in it to process. For
now, we'll use a queue called NZRVFVF. The following command will add a single item to
the queue:
# salt-run queue.insert myqueue item1
True
It is also possible to add multiple items to the queue at a time by passing them as a list.
From the command line, we'll do this using a JSON string:
# salt-run queue.insert myqueue '["item2", "item3"]'
True
Listing queues
As we're using the TRMJUF module, if this queue did not exist before we issued the
command, it will be automatically created. The following command will list the queues that
are available:
# salt-run queue.list_queues
- myqueue
Listing items in a queue
Now that we have some items in the queue, let's take a quick look at them. The following
command will just list them:
# salt-run queue.list_items myqueue
- item1
- item2
- item3
To get a count of the number of items in a queue, use the following command:
# salt-run queue.list_length myqueue
3

Managing Tasks Asynchronously
[ 130 ]
Processing queue items
There are two ways to process one or more items in a queue. Simply popping the queue will
remove the first item and display it on the command line:
# salt-run queue.pop myqueue
- item1
Multiple items can also be popped at the same time:
# salt-run queue.pop myqueue 2
- item2
- item3
This can be useful for programmatic applications that make use of Salt's queue system.
However, it doesn't help us in the way of providing system automation. In order to do that,
we need to be able to fire events to the reactor system. The following command will pop an
item off the queue and fire an event with it:
# salt-run queue.process_queue myqueue
None
If you are watching the event bus when this command is issued, you will see an event that
looks like this:
&WFOUGJSFEBU4BU"QS

5BHTBMURVFVFNZRVFVFQSPDFTT
%BUB
\	@TUBNQ		5	
	CBDLFOE		TRMJUF	
	JUFNT	<	JUFN	>
	RVFVF		NZRVFVF	^
As with popping items, you can also process multiple queue items at once, and they will
appear within the same event:
&WFOUGJSFEBU4BU"QS

5BHTBMURVFVFNZRVFVFQSPDFTT
%BUB
\	@TUBNQ		5	
	CBDLFOE		TRMJUF	
	JUFNT	<	JUFN		JUFN	>
	RVFVF		NZRVFVF	^

Managing Tasks Asynchronously
[ 131 ]
Deleting items from a queue
Before we move on, there's one more function available to us-deleting an item from the
queue without popping or processing it:
# salt-run queue.delete myqueue item1
True
It is also possible to delete multiple items at once with the following code:
# salt-run queue.delete myqueue '["item2", "item3"]'
True
Using queues with the reactor
Queues were originally designed to be used with the reactor system. Minion IDs were
added to a queue, and this queue was processed as appropriate. This can be useful in
environments with large jobs, which may end up consuming resources on the master.
Spreading out State runs
Let's take a look at a use case. A master that will run on hardware that is not performant
enough, for its needs may have difficulty serving large files to all of its minions. Rather than
performing a State run on all minions at once, it makes sense to use the queue to spread
them out a little.
First, let's get the reactor set up. The queue system will always use the
TBMURVFVFNZRVFVFQSPDFTT tag, so let's go ahead and map this to a reactor SLS file in
the master configuration:
SFBDUPS
TBMURVFVFNZRVFVFQSPDFTT
TSWSFBDUPSTBMURVFVFTMT
Now, we need to set up the reactor itself. This will not be a complex reactor; it only needs to
issue the TUBUFIJHITUBUF command. Create TSWSFBDUPSTBMURVFVFTMT with the
following content:
\JGEBUB<	RVFVF	>	OFFET@IJHITUBUF	^
\GPSNJOJPOJOEBUB<	JUFNT	>^
IJHITUBUF@\\NJOJPO^^
DNETUBUFIJHITUBUF
UHU\\NJOJPO^^
\FOEGPS^

Managing Tasks Asynchronously
[ 132 ]
\FOEJG^
We will use Jinja in this example to filter out queues and only loop through the items that
appear in the queue that we want. In this case, the queue that we're looking at is called
OFFET@IJHITUBUF. For each minion ID that is delivered via the event, a reactor called
IJHITUBUF@NJOJPO@JE  will be created, which issues the TUBUFIJHITUBUF command
against that individual minion.
Now that we have our reactor set up, let's go ahead and set up a schedule that only kicks off
one State run every 5 minutes. In the master configuration, add the following code:
TDIFEVMF
IJHITUBUF@RVFVF
GVODUJPORVFVFQSPDFTT@RVFVF
NJOVUFT
BSH
OFFET@IJHITUBUF
When you restart the master, this schedule will pop a minion ID off the queue every 5
minutes, starting with the master start time, and perform a State run on it. If there are no
minions in the queue, it will wait another 5 minutes and try again.
Dividing tasks among minions
Let's take a look at another use case where a large number of jobs need to be handled by
multiple minions. In this example, we have two queues. The first queue contains pieces of
data to be posted on a URL as they are received. The second queue contains a list of minions
that will perform a job. For the sake of simplicity, we'll assume that the job is able to make
use of the data that is posted on the URL without any interaction from us. The example job
that will be run only requires that a minion issue the /VTSMPDBMCJOCJHKPC command.
First, we need to populate the CJHKPC queue, which contains data that will be used by the
minions:
# salt-run queue.insert bigjob '["data1", "data2",
    "data3", "data4"]'
Then, we will populate a XPSLFST queue, which contains the names of the minions that are
available to perform the big jobs:
# salt-run queue.insert workers '["dave", "carl",
    "jorge", "stuart"]'

Managing Tasks Asynchronously
[ 133 ]
As before, the NBTUFS configuration needs to be able to map between the event data and
the reactor SLS:
SFBDUPS
TBMURVFVFCJHKPCQSPDFTT
TSWSFBDUPSTBMURVFVFTMT
For this example, we'll create a new TSWSFBDUPSTBMURVFVFTMT file with the
following content:
\JGEBUB<	RVFVF	>	CJHKPC	^
\GPSKPCJOEBUB<	JUFNT	>^
CJHEBUB@\\KPC^^
SVOOFSIUUQRVFSZ
LXBSH
VSM	IUUQCJHEBUBFYBNQMFDPNKPCT	
NFUIPE1045
EBUB
KPC\\KPC^^
CJHKPC@\\KPC^^
SVOOFSRVFVFQSPDFTT@RVFVF
BSH
XPSLFST
\FOEGPS^
\FOEJG^
\JGEBUB<	RVFVF	>	XPSLFST	^
\GPSNJOJPOJOEBUB<	JUFNT	>^
XPSLFS@\\NJOJPO^^
DNEDNESVO
UHU\\NJOJPO^^
BSH
	VTSMPDBMCJOCJHKPC	
\FOEGPS^
\FOEJG^
There's a lot going on here, so let's jump in. The first thing that we will do is process the
CJHKPC queue. Each item in the queue will be POSTed to the
IUUQCJHEBUBFYBNQMFDPNKPCT URL. It will also trigger the worker queue to
process one item at a time.

Managing Tasks Asynchronously
[ 134 ]
The worker queue reactor is simpler: it pops a minion ID off the queue and asks it to
execute the VTSMPDBMCJOCJHKPC command. Again, we'll assume that this command
knows how to make use of the data that was posted on the URL.
There are a couple of ways to kick off this workflow. One way is to assume that once a
CJHKPC instance is finished, it can kick off an event to the reactor that processes the next
item in the CJHKPC queue. Let's go ahead and set up a webhook that accomplishes this. For
simplicity, we'll not worry about authentication this time.
First, map a new webhook to a new reactor in the NBTUFS configuration file, as follows:
SFBDUPS
TBMUOFUBQJIPPLCJHKPC
TSWSFBDUPSCJHKPCTMT
Then, we will create TSWSFBDUPSCJHKPCTMT with the following content:
QSPDFTT@CJHKPC
SVOOFSRVFVFQSPDFTT@RVFVF
BSH
CJHKPC
Now, assuming that the hostname for the master is TBMUNBTUFS, we will issue the
following cURL command:
curl https://salt-master:8080/hook/bigjob -H 'Accept:
    application/json' -d '{}'
This will kick off the process by processing one queue item. It could also be called by the
VTSMPDBMCJOCJHKPC command after the completion of the job in order to notify the
master that it is finished. Of course, the minion should also add its name back to the queue.
Let's modify TSWSFBDUPSCJHKPCTMT so that it can do this as well:
QSPDFTT@CJHKPC
SVOOFSRVFVFQSPDFTT@RVFVF
BSH
CJHKPC
BEE@XPSLFS
SVOOFSRVFVFJOTFSU
BSH
XPSLFST
\\EBUB<	NJOJPO@JE	>^^

Managing Tasks Asynchronously
[ 135 ]
We'll also change the cURL command to include the ID of the minion:
curl http://salt-master:8080/hook/bigjob -H 'Accept:
    application/json' -d minion_id=<this_minion_id>
Another option is to use the scheduler to kick off the CJHKPC queue on a regular basis:
TDIFEVMF
CJHKPC@RVFVF
GVODUJPORVFVFQSPDFTT@RVFVF
IPVST
BSH
CJHKPC
In this case, be sure to remove the QSPDFTT@CJHKPC block from
/TSWSFBDUPSCJHKPCTMT, but leave the BEE@XPSLFS block.
Summary
The event system in Salt can be extremely powerful when combined with the reactor
system. Events can be designed to trigger other events, which can in turn trigger even more
events. This moves Salt from the configuration management and automation playing fields
to a bigger area, where autonomy rules all.
In the next chapter we will take a look at some of the systems available inside of Salt to
provide information both to masters and minions.

6
Taking Advantage of Salt
Information Systems
To the newly initiated, one of the stranger aspects of Salt is the absence of a configuration
management database (CMDB). The fact is that Salt was designed to query minions
quickly to return the data normally found in a CMDB in real time.
The mechanism used to discover that information was the grains system. However, using
the minion itself to manage that information presents some limitations. The pillar system,
and later the SDB system, was added to help address these limitations. In this chapter, we'll
cover:
Templating files in QJMMBS@SPPUT
Using the external pillars that ship with Salt
Understanding SDB, the Simple Database System
Understanding the differences between pillars and SDB
Understanding pillar_roots
In the early days, pillars didn't exist in Salt. Grains were originally introduced as a
mechanism for minions to gather system information. That data was useful to execution
modules, and it wasn't long before custom grains were also introduced.
The problem with grains is that they need to be configured directly on minions. While the
HSBJOT execution module does allow easy remote editing of those files, it is often easier and
more appropriate to configure data for those minions in a centralized location, such as the
master.

Taking Advantage of Salt Information Systems
[ 137 ]
As you know, pillars are normally stored in the TSWQJMMBS directory on the master, in
static YAML files. However, those files don't need to be static.
Templating pillar_roots
By QJMMBS@SPPUT, we mean the directory that is used as the root directory for pillar files.
This setting is configured in the NBTUFS configuration file. By default, it is set to the
following:
QJMMBS@SPPUT
CBTF
TSWQJMMBS
TSWTQNQJMMBS
The second entry refers to pillars that are installed by SPM, which was covered in $IBQUFS
, Managing States. The first entry is the more traditional storage location for pillar files.
As with other Salt files, pillars are formatted in YAML by default. But as with other Salt
files, they are also run through Salt's renderer system, which means that all of the other
templating engines are available. Even the UPQTMT file can be templated, so you can do
things such as declaring variables in Jinja:
\TFUUNQWBS	WJN	^
CBTF
		
CBTFQLHT
\\UNQWBS^^
This would be rendered as follows:
CBTF
		
CBTFQLHT
WJN
You could also use the QZ renderer and write your SLS files in pure Python:
QZ
EFGSVO

SFUVSO\
	CBTF	\
		<
	CBTFQLHT	
	WJN	
>

Taking Advantage of Salt Information Systems
[ 138 ]
^
^
This would still render as the follows:
CBTF
		
CBTFQLHT
WJN
Calling out to other modules
The UPQTMT file is still somewhat limited, because it only defines a set of targets and the
formulas that are associated with those targets. Other pillar files are more dynamic, as they
are rendered specifically for the minion that will be receiving their data. Because the minion
ID is known at this point, operations can be performed that are relative to the minion itself.
In particular, three dictionaries are available inside pillars and can be referenced from the
renderer system:
PQUT: A copy of the minion's in-memory configuration
HSBJOT: The grains that have been generated for that minion
TBMU: The execution modules that are available to that minion
If you wanted to access the minion's ID, you would reference it as follows:
\\PQUT<	JE	>^^
If you wanted to generate a pillar that returned the version of Apache installed on the
minion, you would reference this:
\\TBMU<	QLHWFSTJPO	>
	BQBDIF	^^
Keep in mind that while the master does cache a copy of the minion's PQUT and HSBJOT
dictionaries, calling out to an execution module will always generate traffic between the
master and the minion. The flow of traffic will be like this:
The minion asks the master for pillar data.
1.
The master begins rendering the pillar.
2.
During the render process, the master makes a callback to the minion to execute a
3.
job.

Taking Advantage of Salt Information Systems
[ 139 ]
The minion executes the job and sends the return data back to the master.
4.
The master interpolates that data into the pillar and finishes rendering it.
5.
The pillar data is now sent back to the minion.
6.
What seems like an innocuous call to an execution module may actually be very expensive
in terms of resources. This is especially true when working with a target that matches
hundreds of minions or more. There are times when performing such a call may be the most
appropriate option, but you will need to decide that on a case-by-case basis.
Using Salt's external pillars
Pillar files can be very simple and convenient, and having the ability to template data is
very powerful, but sometimes going that route is like trying to fit a square peg into a round
hole. There are a number of situations where going outside of QJMMBS@SPPUT and using an
external pillar fits the bill much better.
External pillars are their own type of Salt module, written in Python. You can create your
own, but that falls outside the scope of this book. If you would like more information, check
out Extending SaltStack, Packt Publishing.
Configuring the etcd pillar
Fortunately, a number of valuable external pillar modules already ship with Salt. For
instance, a number of users have begun integrating the FUDE configuration database into
their infrastructures. When you use the FUDE pillar, it might feel as if Salt and FUDE were
designed for each other!
The FUDE pillar does require that you create a configuration block in your NBTUFS
configuration that specifies how to connect to the FUDE service:
NZ@FUDE@DPOGJH
FUDEIPTU
FUDEQPSU
Once that is configured, you can add a reference to that block to your FYU@QJMMBS
configuration inside the NBTUFS file:
FYU@QJMMBS
FUDENZ@FUDE@DPOGJH

Taking Advantage of Salt Information Systems
[ 140 ]
When the master is restarted, the contents of the FUDE database will be available as pillar
items. In this configuration, all minions will have access to the same pillar data. If you
would like to store pillar data that is specific to one minion, you will need to set aside a path
inside FUDE where minion data will be stored. Each minion will then have its own directory
named after itself inside that path. The configuration for this model would look more like
this:
FYU@QJMMBS
FUDENZ@FUDE@DPOGJHSPPUTBMU
NJOJPO@JET
Take note of 
NJOJPO@JET, which will be replaced with the minion's ID when that pillar
is requested. With that configuration in place, you may add minion-specific data to the
FUDE database with a command such as this:
etcdctl set /salt/myminion/foo bar
You could even use multiple FUDE servers or at least multiple external pillar definitions for
the same server at the same time. Consider the following:
FUDE@TIBSFE
FUDEIPTU
FUDEQPSU
FUDE@QSJWBUF
FUDEIPTU
FUDEQPSU
FYU@QJMMBS
FUDEFUDE@TIBSFESPPUTBMUTIBSFE
FUDEFUDE@QSJWBUFSPPUTBMUQSJWBUF
NJOJPO@JET
This configuration makes use of two different FUDE servers, each with their own type of
data. You could of course set up multiple paths inside the same server and have each pillar
definition use a different root:
FYU@QJMMBS
FUDEFUDE@DPOGJHSPPUTBMUTIBSFE
FUDEFUDE@DPOGJHSPPUTBMUQSJWBUF
NJOJPO@JET
Using git_pillar
The GitFS external file server has shown to be extremely popular. Somewhat less known is
HJU@QJMMBS, based on the same concepts. Just as HJUGT presents a virtual file system that
correlates with GJMF@SPPUT, HJU@QJMMBS presents a virtual file system that corresponds to
QJMMBS@SPPUT.

Taking Advantage of Salt Information Systems
[ 141 ]
Unlike FUDE, this pillar does not require an extra code block to be defined. All that it needs
is in the FYU@QJMMBS declaration itself. If you were hosting your pillars on GitHub, your
configuration might look like this:
FYU@QJMMBS
HJUNBTUFSIUUQTHJUIVCDPNNZVTFSNZQSPKFDU
In this example, NBTUFS refers to the branch of the Git repository to use, and the rest of
course is the URL to point to that repository.
The contents of this Git repository must resemble the standard pillar definitions that you
would find in QJMMBS@SPPUT. There needs to be a UPQTMT file and all of the static (or
templated) pillar files that it refers to.
If you find the idea of maintaining a single repository just for pillar data to be limiting, don't
worry. You can point to a specific directory inside a repository to use as the starting point
for the pillar tree. If your repository had a directory called QJMMBS@EBUB in it, then your
configuration would look like this:
FYU@QJMMBS
HJUNBTUFSIUUQTHJUIVCDPNNZVTFSNZQSPKFDU
SPPUQJMMBS@EBUB
You can also map your pillars to different Git branches based on the environment that is
specified when the pillar data is requested. Your master configuration would look like this:
FYU@QJMMBS
HJU@@FOW@@IUUQTHJUIVCDPNNZVTFSNZQSPKFDU
SPPUQJMMBS@EBUB
And your UPQTMT file might look like this:
\\FOW^^
		
\\FOW^^QLHT
The @@FOW@@ and \\FOW^^ placeholders will be replaced with the specified
environment. In this example, I've added a second \\FOW^^ block to specify a group of
packages that belong to that environment. I did this to reinforce that pillar files, even those
stored in Git, are processed through the renderer system.
A quick note on \\FOW^^: This variable is available in UPQTMT, but not inside any other
pillar SLS files. If you would like to refer to the environment in one of those, use \\
PQUT<	FOWJSPONFOU	>^^.

Taking Advantage of Salt Information Systems
[ 142 ]
I'd like to point out one more thing about HJU@QJMMBS for those of you that are worried
about crippling Git servers with excessive pillar calls. This pillar does not connect to the Git
server for every single pillar lookup; that would be way too expensive in terms of resources.
Instead, it creates a local clone of the remote repository and then queries that. This clone is
then periodically updated from that server in order to avoid using resources in a wasteful
manner.
Using the mysql pillar
Let's take a look at one final external pillar: NZTRM. This pillar differs from the first two that
we looked at in that it allows you to specify an actual query with which to look up data.
First, the MySQL server is configured using a NZTRM code block:
NZTRM
VTFSMBSSZ
QBTTQBTT
ECQJMMBS@EBUB
Then, an external pillar definition is set up, which contains the query to be run:
FYU@QJMMBS
NZTRM
RVFSZ	4&-&$5TPNFGJFME'30.TPNFUBCMF8)&3&
NJOJPO@JE-*,&T	
EFQUI
This query contains a very simple example query, with a depth of . The T in the query
will be replaced with the minion ID when the query is executed.
The depth refers to how the SQL data is converted to the dictionary that is returned inside
the pillar. Depth values start at , and the maximum value is based on how many fields are
referred to in your query. What this looks like is probably best explained by the
documentation in the code:
The depth defines how the dicts are constructed.
Essentially if you query for fields a,b,c,d for each row you'll get:
With depth 1: {a: {`ba: b, `ca: c, `da: d}}
With depth 2: {a: {b: {`ca: c, `da: d}}}

Taking Advantage of Salt Information Systems
[ 143 ]
With depth 3: {a: {b: {c: d}}}
Depth greater than 3 wouldn't be different from 3 itself.
Depth of 0 translates to the largest depth needed, so 3 in this case. (max depth == key count b 1)
Then they are merged in a similar way to plain pillar data, in the order returned by the SQL
database.
Thus subsequent results overwrite previous ones when they collide.
Okay, so it's still a little rough to understand. What happens with multiple fields is that a
dictionary is created that contains the values and, in some cases, the names of the fields. The
more fields you specify, the more complex the return result.
My solution to this is to keep my queries simple, with a low EFQUI value specified. I tend to
like my pillar data to be pretty concise anyway, so this works out well in my
implementations.
You may be alarmed by the last line there about collisions being overwritten. This is entirely
possible, depending on how loose your query is. If you are concerned about collisions
showing up in your pillar data, you can configure it to return those dictionaries in a list
format:
FYU@QJMMBS
NZTRM
RVFSZ	4&-&$5TPNFGJFME'30.TPNFUBCMF8)&3&
NJOJPO@JE-*,&T	
EFQUI
BT@MJTU5SVF
This makes it harder to search pillar data without using a loop, but it does get around the
collision problem.
Some final thoughts on external pillars
I want to give you some final notes concerning external pillars in general before we move
on to the next section.

Taking Advantage of Salt Information Systems
[ 144 ]
Using multiple external pillars
You can specify a number of external pillars to be used at a time, by adding new list items
to the FYU@QJMMBS configuration block:
FYU@QJMMBS
FUDENZ@FUDE@DPOGJHSPPUTBMU
NJOJPO@JET
HJUNBTUFSIUUQTHJUIVCDPNNZVTFSNZQSPKFDU
There is no imposed limit within Salt itself as to how many of these you use, but depending
on your environment and your pillar sources, you may still run into resource issues as you
increase the number of configurations that are used. When you specify too many sources,
you run the risk of having a lookup from a single minion slowing down the master for all of
the other minions. Use as many external pillars as are necessary, but don't go overboard.
Caching pillar data
Some pillar drivers cache data, while others do not. In the case of HJU@QJMMBS, retrieving
data from the remote source can be very expensive, so the data is cached locally. But FUDE
tends to be a very light lookup in terms of resources, so its pillar just grabs data from the
database in real time. The NZTRM pillar is more resource intensive than the FUDE pillar, but it
still queries in real time. A slow MySQL connection will definitely be problematic, as will
overly complex queries.
Understanding SDB
SDB, or Simple Database, was created to serve certain needs that the grains and pillar
systems cannot provide. Specifically, grains and pillars are constructs that exist entirely for
the benefit of minions, and as such are not available to master-side operations.
This makes SDB more useful for lookups that happen inside the NBTUFS configuration and
for Salt Cloud configuration. SDB data is still available via NJOJPO configurations, and there
are both execution and pillar modules for SDB, but in those cases, you may find it more
appropriate to store your data inside a pillar.
Before we get into all that, let's talk about the use cases that drove the initial development of
SDB.

Taking Advantage of Salt Information Systems
[ 145 ]
Securely storing passwords
SDB was originally intended as a mechanism for keeping passwords out of Salt Cloud
provider and profile files. As you can imagine, keeping the following code block around in
plaintext would constitute security risks:
B[VSFDFOUPT
QSPWJEFSMBSSZB[VSFBSN
JNBHF0QFO-PHJD]$FOU04]]
TJ[F4UBOEBSE@"
MPDBUJPOXFTUVT
TTI@VTFSOBNFMBSSZ
TTI@QBTTXPSEQBTT
If it were possible to store some of this data in a more secure system, such as with the
LFZSJOH module in PyPi, and then use a simple reference to obtain the data only when
necessary, then we could do things such as storing those files in Git without worrying about
passwords being stored in the revision history.
I want to make something very clear here. Modules such as LFZSJOH and
WBVMU do store data using secure software using trusted encryption-at-rest
techniques. But if you can access that data using a module, there is a very
good chance somebody else can too. Best practices for properly securing
your data will vary between SDB modules, and some offer no encryption
at all. Most importantly, if a minion process has access to data, then its
master also has access to that same data. Don't assume that data is secure
just because it is encrypted!
Staying simple
One point of contention that has presented itself in the past is SDB's insistence on being
simple. URIs should be short and easy to use. Complicated URIs make for complicated and
often confusing usage. Unfortunately, I cannot promise that every SDB URI that you
encounter will be simple. If you use SDB enough, you are bound to eventually run into a
driver that makes use of long, complicated configuration and URIs.
That said, the intent is to remain simple. Information on writing SDB modules can be found
in Extending SaltStack, Packt Publishing, complete with admonitions to keep drivers simple.

Taking Advantage of Salt Information Systems
[ 146 ]
Using SDB URIs
SDB is based around a very simple URI format. The scheme of course is TEC, followed
by the name of the configuration block being referenced, followed by the key to look up,
optionally followed by any options. A sample URI might look like this:
TECNZLFZSJOHNZQBTTXPSE
This particular URI would reference a configuration block inside either the NBTUFS,
NJOJPO, or DMPVE configuration files, which would look like this:
NZLFZSJOH
ESJWFSLFZSJOH
TFSWJDFTZTUFN
Of course, NZQBTTXPSE would refer to a key stored inside the LFZSJOH system called
NZQBTTXPSE. We'll get into how that configuration block looks in the next section. For the
moment, let's look at how the aforementioned DMPVE configuration would look using SDB
URIs to obscure certain data:
B[VSFDFOUPT
QSPWJEFSMBSSZB[VSFBSN
JNBHF0QFO-PHJD]$FOU04]]
TJ[F4UBOEBSE@"
MPDBUJPOXFTUVT
TTI@VTFSOBNFTECNZLFZSJOHNZVTFSOBNF
TTI@QBTTXPSETECNZLFZSJOHNZQBTTXPSE
This file can safely be stored in plaintext or revision control, and even changing the
username or password will not affect the file itself.
Configuring SDB
SDB modules all require some sort of configuration block to exist. The name of that block is
up to you, but it is best to keep the name short and simple but appropriately descriptive.
For instance, when using the LFZSJOH driver, a name such as NZLFZSJOH may be all you
need. But if you're using SDB with multiple FUDE databases, then names that describe each
one of them (FUDEQSJWBUF, FUDETIBSFE, and so on) will be easier to make use of.

Taking Advantage of Salt Information Systems
[ 147 ]
The first line inside that configuration block is ESJWFS, which refers to the module that will
be used to provide the necessary functionality, for instance, FUDE or LFZSJOH.
As far as SDB is concerned, those two lines are the bare minimum required. In fact, the FOW
driver will only ever make use of those two lines. For instance, the following configuration
is valid:
PTFOW
ESJWFSFOW
And the following URI would access data from that environment:
TECPTFOW)0.&
If you want to try this out yourself, add the aforementioned configuration to your NJOJPO
configuration and run it:
# salt-call --local sdb.get sdb://osenv/HOME
local:
    /root
Most other drivers require some other sort of configuration. Usually, that configuration will
reference a specific point inside that resource where the requested key is to be found. It may
even contain connection information. For example, FUDE supports the same connection
information that is used for pillars:
NZFUDE
ESJWFSFUDE
FUDEIPTU
FUDEQPSU
The LFZSJOH driver specifies which kind of database to perform the lookup in:
NZLFZSJOH
ESJWFSLFZSJOH
TFSWJDFTZTUFN
Each driver should contain documentation that explains what parameters are required and
how to use them.

Taking Advantage of Salt Information Systems
[ 148 ]
Performing SDB lookups
SDB URIs are always looked up at runtime, when they are requested. This means a couple
of things:
If you issue a command that performs an SDB lookup and then a separate
command that performs the same lookup, you will receive different data if the
SDB source was changed between those two commands.
Configuration files may contain SDB URIs in any place, even above the
configuration block that they use to define their source. This is because the URI
itself is stored, and it will not be translated to the actual data until requested.
In the case of configuration files, such as FUDTBMUNBTUFS or
FUDTBMUDMPVEQSPGJMFT (or, say,
FUDTBMUDMPVEQSPGJMFTETPGUMBZFSDPOG), SDB URIs are entered as is:
B[VSFDFOUPT
QSPWJEFSMBSSZB[VSFBSN
JNBHF0QFO-PHJD]$FOU04]]
TJ[F4UBOEBSE@"
MPDBUJPOXFTUVT
TTI@VTFSOBNFTECNZLFZSJOHNZVTFSOBNF
TTI@QBTTXPSETECNZLFZSJOHNZQBTTXPSE
But if you want to access the data directly, you have some other options.
First of all, if you have direct access to the data source, you can just modify it directly, using
whatever command or commands are appropriate to do so. This is in many cases the easiest
to manage those data.
However, there are both execution and runner modules that can perform the lookup for
you. The usage is the same (outside of TBMU or TBMUDBMM versus TBMUSVO), so we'll use
TBMUDBMM (with the MPDBM flag to speed things up) for our examples.

Taking Advantage of Salt Information Systems
[ 149 ]
Getting data
We'll start with retrieving data from SDB, which is also the most straightforward. The name
of the call is TECHFU, followed by the URI to look up:
# salt-call --local sdb.get sdb://osenv/HOME
This is a good time to point out that when referring to SDB data, the URI is
always used, whether getting, setting, or deleting data. It contains all of
the data necessary to perform the lookups (or at least the data necessary to
access the configuration blocks), and trying to force people to define the
location of data in a different way would be contrived and confusing at
best.
In the previous example, we are asking SDB to return the )0.& environment variable from
the minion. If you have the minion running as a special user (such as TBMU) then the
environment that has been defined for that process running as that user will be returned. If
you're issuing the command using TBMUDBMM, then you will see variables from your own
environment.
Some drivers may support functionality beyond a simple key lookup. Because SDB URIs
follow (or should follow) RFC 3986, it is possible for a driver to support URIs that contain
data that looks like HTTP GET request data:
TECNZESJWFSNZUBCMFVTFSOBNFMBSSZEFQUQBJOUFST
Most drivers do not support this functionality as of this edition of this book, but don't be
surprised if drivers like that become more commonplace.
You can look up specifics for RFC 3986 and its predecessor, RFC 2396, on
Wikipedia:
IUUQTFOXJLJQFEJBPSHXJLJ6OJGPSN@3FTPVSDF@*EFOUJGJFS.
While I normally support using underscores in Salt configuration, this is
one place not to do it. That's because while compliant URIs may contain
dashes, they may not contain underscores. Salt doesn't care (right now, at
least), but it is best practice.

Taking Advantage of Salt Information Systems
[ 150 ]
Setting data
While every SDB driver must support getting data, not every driver supports setting data.
This is in part because in many cases, it is better to manage the data being served through
some other tool native to the platform in question.
For those drivers that do support setting data, there is very little difference in the command
used:
# salt-call --local sdb.set sdb://osenv/EDITOR vim
Outside of the function called, the only difference is the presence of a second argument,
which contains the information to apply to that key.
The previous example is interesting because it sets an environment variable. However, the
scope of that environment is local to the salt worker that performed the command. Once the
command is finished, the worker will close up shop and the environment will disappear.
Why then would that driver support setting an environment variable?
This is more of a hack, really, but it can be very useful. While execution modules are
designed to open a process, execute a command, and then close, not every execution is as
shortlived as that. For instance, the TUBUF execution module can issue a set of commands
that are complex and long running.
Calling SDB to set an environment variable at the beginning of a state run (for instance,
inside Jinja code) can provide settings which may not otherwise be available in that
environment. A number of cloud providers, for instance, provide toolkits that make
extensive use of environment variables to authenticate to their services. In this case, it is
actually more useful to use SDB to set data (as temporary though it may be) than to retrieve
data.
Deleting data
Even more rare than the ability to set data is the ability to delete data. This will vary wildly
between drivers, and for some drivers it will never be appropriate for this functionality to
exist. Where it does exist, deleting data is as easy as getting it:
# salt-call --local sdb.delete sdb://myetcd/mypassword

Taking Advantage of Salt Information Systems
[ 151 ]
Because there is no data to set, there is only one argument: a pointer to the key to remove.
Be wary when using this command: Salt will not ask you whether you're sure you want to
delete the data-it will just delete it.
As of Salt version 2016.3, the only driver to support delete is the FUDE driver. But don't be
surprised if other drivers start supporting it soon as well.
Comparing pillars and SDB
It is very important to note that while pillars and SDB do provide much of the same data,
there are some differences between them. Understanding these differences will help you
decide which is more appropriate for you for each situation you encounter.
Where the data is generated
Pillar data is always generated on the master before being sent to the minion. If that data
comes from another source that the master has permission to look at but not the minions,
then the minions can still obtain the necessary data from the master without having to have
access to that resource themselves.
SDB configuration is obtained using configuration on the system that performs the lookup.
If QJMMBS@PQUT is 5SVF on the master (meaning the minions receive a copy of the master's
configuration as its own pillar dictionary), then SDB URIs in the NBTUFS configuration will
be translated on the master before being sent to the minion.
However, if it is the NJOJPO configuration that contains an SDB URI, then the minion will
perform the lookup itself. In the case of Salt Cloud, SDB lookups will be performed by Salt
Cloud itself before being used to issue cloud commands.
An important example here is the LFZSJOH driver. The LFZSJOH driver itself is in fact a
wrapper around its own set of drivers that manage secure data. Most of the drivers that it
supports are designed for desktop use rather than server use. Data for some of these drivers
cannot be accessed without physically typing in a passphrase on the machine running
LFZSJOH, and even then, that data is typically only available inside a specific session.

Taking Advantage of Salt Information Systems
[ 152 ]
If a minion has not had this password entered yet, then the master will have no way to
obtain data stored inside that database (outside of actually attempting decryption attacks
against it). Once the password has been entered, that data is still only available to that user's
session.
In the case of secure drivers such as this, SDB can safely access data that is otherwise
unavailable via a pillar, because the master has access to that data. That is, however,
contingent upon you running the minion in local mode (using TBMUDBMMMPDBM). As
soon as a minion connects to the master, the master can ask it for whatever the minion has
access to. If the minion is running in the same session as your LFZSJOH data, then the
master can look it up.
Differences in drivers
While many modules exist in both SDB and pillar form, not all modules do. The previous
discussion of the LFZSJOH driver is one such example. If a particular type of driver does
have both SDB and pillar counterparts, they may have been architected differently, as
necessity dictates.
Salt Cloud and others
Salt Cloud can be run on either master or minions, but while an execution module does
exist for it, its primary purpose is to stand apart from an infrastructure and create, manage,
and destroy resources in that infrastructure as required. As such, pillar data will not always
be appropriate or available to Salt Cloud.
In the cases of software such as Salt Cloud, runners, or other subsystems that are really
designed to be run on the master, pillars will probably not do you a whole lot of good. It is
in these cases that SDB can step up to the plate and take over those functionalities.

Taking Advantage of Salt Information Systems
[ 153 ]
Summary
There are a number of information systems inside of Salt. Grains are defined on the minions
that use them, while pillars are defined on the master. When files inside QJMMBS@SPPUT are
used, they may take advantage of templates and other modules in the renderer system.
External pillars are also available that can give access to dynamic remote sources. Some of
those external pillars, such as HJU@QJMMBS, provide a virtual filesystem that behaves like
QJMMBS@SPPUT, even allowing templating abilities.
SDB can be used to obscure data from configuration files. While it may look secure, there
are very few situations in which it can actually be treated as a completely secure storage
mechanism. Its power lies in two areas: being able to remove sensitive data from
configuration files so that they can be safely stored in plaintext or revision control and being
simple to use.
SDB and pillars overlap in some areas of functionality, but not all. With some exception,
SDB tends to be more useful for master-side configuration, while pillars are only available
to minions.
Next up, we'll take a look at Salt Cloud!

7
Taking Salt Cloud to the Next
Level
To many, Salt Cloud has become a crucial part of the stack of Salt tools. Originally designed
for little more than creating and cSaltingd virtual machines across multiple cloud hosting
providers, its functionalities have grown to become much more. In this chapter, we'll
discuss the following topics:
The basics of configuration
Extending configuration directives
Using SDB with Salt Cloud
Building custom deploy scripts
Working with cloud maps
Creating reactors to extend Salt Cloud
Building and using autoscale reactors
Take note that this chapter discusses how to manage compute nodes or instances, which
generally refers to virtual machines. However, since some cloud hosting companies also
provide cloud resources by creating an entire physical server or bare-metal instance
available to the user, we will refer to them all collectively as compute instances.

Taking Salt Cloud to the Next Level
[ 155 ]
Examining the Salt Cloud configuration
Some of the biggest evidence of how much Salt Cloud has grown resides in its configuration
files. Early versions supported several cloud providers, but only one account per provider.
It quickly became clear that a number of users did in fact make use of multiple accounts.
Let's take a few minutes to look at the basics of how Salt Cloud configuration files work
now.
Global configurations
The basis of the Salt Cloud configuration resides in the main configuration file, which is
normally found at FUDTBMUDMPVE. This file used to be very central to the operation of
Salt Cloud, but nowadays, there are very few options that can only be used here. We'll
cover these later in Global configurations; for now, we'll focus on the global aspect of this file.
Salt Cloud has been designed with a top-down configuration mindset. Configurations
defined in each type of the configuration file are available in the next configuration set,
unless overridden. The order of operation is as follows:
FUDTBMUDMPVE
1.
Provider configuration
2.
Profile configuration
3.
Cloud maps
4.
Some options are relevant to many cloud providers, while others only pertain to one. As the
configuration is compiled together to create a new compute instance, a number of options
may be made available. These aren't necessary. Don't worry, because any options that are
declared as unusable will be ignored.
Let's go through an example. Let's assume that you manage a number of compute instances
in Amazon's EC2 cloud and that they span multiple regions. As you'll see in the next
section, each region will be configured as a different cloud provider as far as Salt Cloud is
concerned. However, you have a number of EC2-specific options that you want to apply to
all regions:
# cat /etc/salt/cloud
rename_on_destroy: True
delvol_on_destroy: True

Taking Salt Cloud to the Next Level
[ 156 ]
These two options are specific to EC2. Every region that is set up as a separate provider will
inherit these options, saving the user having to specify them multiple times. Simplifying
configurations like this will also cut down on errors when broad changes need to be made,
because tedious editing of multiple files and configuration blocks will be eliminated.
Provider and profile configuration
Before we go into details on how these two types of configuration work, we should clarify
what each one is. You should already be comfortable with these configurations, so we won't
go into the details here.
Providers
Provider refers to the compute-cloud hosting company, which should be used to create the
new compute instance. A provider configuration block is also used to separate
configuration for multiple regions on the same cloud provider. For example, the following
configuration blocks refer to the same hosting company, but within two different regions:
FDFBTU
QSPWJEFSFD
JE)+(3:$*-+-,+:(
LFZ	LEKHGTHNXPPSNHMBTFSJHKLTKEIBTEGHO	
LFZOBNFUFTU
TFDVSJUZHSPVQRVJDLTUBSU
QSJWBUF@LFZSPPUUFTUQFN
MPDBUJPOVTFBTU
FDXFTU
QSPWJEFSFD
JE)+(3:$*-+-,+:(
LFZ	LEKHGTHNXPPSNHMBTFSJHKLTKEIBTEGHO	
LFZOBNFUFTU
TFDVSJUZHSPVQRVJDLTUBSU
QSJWBUF@LFZSPPUUFTUQFN
MPDBUJPOVTXFTU
Take note of the QSPWJEFS line in each of these blocks. Inside the provider
configuration blocks, this argument refers to the driver that will be used to
manage compute instances.

Taking Salt Cloud to the Next Level
[ 157 ]
Provider configuration is stored in one of two places. For simpler cloud configuration, it
may be most convenient to store it in the FUDTBMUDMPVEQSPWJEFST file. If you use a
number of cloud providers or have a need for smaller configuration files, which are
managed by a configuration management suite (such as Salt), it may be better to break them
into a number of files in the FUDTBMUDMPVEQSPWJEFSTE directory.
Take note that files inside this directory must have a DPOG extension in
order for Salt Cloud to use them
Profiles
Profile configuration is used to set up configuration blocks. These blocks define a certain
type of compute instance. For instance, an infrastructure may have a number of web servers
that share identical configuration and a number of database servers that share their own
identical configuration, which is likely to be very different from the web servers'
configurations.
A profile configuration block is used to divide the separate configurations for each of these
types of machines. Take, for example, the following two profiles:
B[VSFDFOUPT
QSPWJEFSB[VSFXFTU
JNBHF	0QFO-PHJD$FOU04	
TTI@VTFSOBNFSPPU
TTI@QBTTXPSEQBTT
TTI@QVCLFZSPPUB[VSFQVC
NFEJB@MJOL	IUUQTFYBNQMFCMPCDPSFXJOEPXTOFUWIET	
TMPUQSPEVDUJPO
TJ[F.FEJVN
B[VSFVCVOUV
QSPWJEFSB[VSFXFTU
JNBHF	6CVOUV@-54BNETFSWFSFOVT(#	
TTI@VTFSOBNFMBSSZ
TTI@QBTTXPSEQBTT
TTI@QVCLFZSPPUB[VSFQVC
NFEJB@MJOL	IUUQTFYBNQMFCMPCDPSFXJOEPXTOFUWIET	
TMPUQSPEVDUJPO
TJ[F.FEJVN
UUZ5SVF
TVEP5SVF

Taking Salt Cloud to the Next Level
[ 158 ]
These profiles are almost identical; the differences stem from the operating system. Many
CentOS images default to using root as the default user, whereas the Ubuntu philosophy
prefers to use an unprivileged user as the default. However, since privileged access is
required to be able to install Salt, additional options have been added in order for Salt
Cloud to be able to issue commands as root, using TVEP.
Take note of the QSPWJEFS argument in each block. In this case, it refers to
the configuration block defined in the QSPWJEFS configuration files, as
opposed to the name of the driver.
Similar to the provider configuration files, profile configuration may be stored either in the
FUDTBMUDMPVEQSPGJMFT file or in DPOG files in the
FUDTBMUDMPVEQSPGJMFTE directory.
Extending configuration blocks
The QSPWJEFS and QSPGJMF configuration blocks are unique among Salt configurations, in
which they support the FYUFOET configuration directive. This feature allows you to create a
generic QSPWJEFS or QSPGJMF configuration block and then use this block as a template for
other QSPWJEFS or QSPGJMF definitions.
For example, take the following profile:
FDVCVOUV
QSPWJEFSNZFD
JNBHFBNJEFFFB
TJ[FNNFEJVN
TTI@VTFSOBNFVCVOUV
TFDVSJUZHSPVQJNBHFT
TQPU@DPOGJH
TQPU@QSJDF
This profile takes advantage of the spot instance feature in Amazon's EC2, which allows
you to bid for resources at a potentially lower cost than they would normally be available
at. This profile and its spot price may be a good default for most compute instances in your
organization, but certain compute instances may need to be a little different.

Taking Salt Cloud to the Next Level
[ 159 ]
Let's say you have some web servers that only serve static images, and they don't need to be
as large or as expensive as others. You can create a new profile that inherits all the 
properties from this one and then overwrites arguments as needed:
TUBUJDJNBHFFD
TJ[FNTNBMM
TQPU@DPOGJH
TQPU@QSJDF
FYUFOETFDVCVOUV
This profile, once compiled by Salt Cloud, will actually create a new profile that looks
similar to the following code:
TUBUJDJNBHFFD
QSPWJEFSNZFD
JNBHFBNJEFFFB
TJ[FNTNBMM
TTI@VTFSOBNFVCVOUV
TFDVSJUZHSPVQJNBHFT
TQPU@DPOGJH
TQPU@QSJDF
Now is a good time to point out an important restriction with the FYUFOET block. There are
several configuration items that can be declared as lists. For instance, TTI@VTFSOBNF is a
common configuration item across multiple providers in the following example:
FDVCVOUV
QSPWJEFSNZFD
JNBHFBNJEFFFB
TJ[FNNFEJVN
TTI@VTFSOBNF
VCVOUV
FDVTFS
TFDVSJUZHSPVQJNBHFT
There are two usernames provided. When initially logging in to a compute instance, Salt
Cloud will attempt each of these in order until a valid username is found.

Taking Salt Cloud to the Next Level
[ 160 ]
If this profile were extended to also include the root user, then the entire TTI@VTFSOBNF
argument would need to be redeclared. This is because list items will be overwritten in their
entirety. The following profile would only include the root username in the final
configuration:
NFEJVNVCVOUV
TTI@VTFSOBNF
SPPU
FYUFOETFDVCVOUV
This profile would contain all the necessary usernames:
NFEJVNVCVOUV
TTI@VTFSOBNF
VCVOUV
FDVTFS
SPPU
FYUFOETFDVCVOUV
This may seem odd because there are list arguments in SLS files appended together, for
example, the SFRVJSF statement, as shown in the following code:
OHJOYTFSWJDF
TFSWJDFSVOOJOH
OBNFOHJOY
SFRVJSF
QLHOHJOYQBDLBHF
QLHNZTRMQBDLBHF
However, in the case of SLS files, the order is not actually important. So long as all the
requirements are met, the state will execute. If the requirements need to be performed in a
specific order, then each item will have its own list of requirements declared, which will
determine the order.
In the case of lists in Salt Cloud, the order is often very important, whereas in the case of
TTI@VTFSOBNF, each item will be tried in the order in which it is presented. Simply adding
lists together will not necessarily result in the order that is actually desired.
Using SDB with Salt Cloud
Salt's simple database system was originally designed with a single idea in mind: to keep
passwords out of Salt Cloud configuration files. Of course, they were immediately added to
other configuration files as well, but that doesn't diminish their use here.

Taking Salt Cloud to the Next Level
[ 161 ]
The classic examples of using SDB to store usernames and passwords still apply, of course:
NZQSPWJEFS
VTFSOBNFTECNZFUDEVTFSOBNF
QBTTXPSETECNZFUDEQBTTXPSE
But the new FOW SDB driver will prove valuable to a number of cloud users who make use
of environment variables to store their authentication information.
Using SDB with OpenStack
OpenStack users have been making use of environment variables for years to store their
personal cloud configuration. It is very common to set up a shell script that exports
OpenStack variables to be used with the standard OpenStack commands. One such script
might look like this:
FYQPSU04@64&3/".&MBSSZ
FYQPSU04@1"44803%QBTT
FYQPSU04@5&/"/5@/".&NZ1SPKFDU
FYQPSU04@"65)@63-IUUQTLFZTUPOFFYBNQMFDPNW
FYQPSU04@3&(*0/@/".&SFHJPO0OF
In Salt Cloud, these variables would be stored in a provider configuration file, such as
FUDTBMUDMPVEQSPWJEFSTEPQFOTUBDLDPOG:
NZPQFOTUBDLQSPWJEFS
ESJWFSOPWB
VTFSMBSSZ
QBTTXPSEQBTT
UFOBOUNZ1SPKFDU
JEFOUJUZ@VSMIUUQTLFZTUPOFFYBNQMFDPNW
DPNQVUF@SFHJPOSFHJPO0OF
Having to maintain two different configuration files can be a pain. What's more, if you're on
a system that is shared between multiple users, you don't necessarily want to use a single
provider file for all of your users nor do you want to maintain multiple provider files, each
with its own private user data, all of which are visible to all users. You could set up the
correct user permissions on each provider file, but users still end up relying on admins to
manage their configuration.

Taking Salt Cloud to the Next Level
[ 162 ]
No-it is much easier in many cases to make use of environment variables instead. Assuming
that users already have their environment variables set up (perhaps in their
_CBTI@QSPGJMF file), you could set up a single, global provider configuration file that
looks like this:
NZPQFOTUBDLQSPWJEFS
ESJWFSOPWB
VTFSTECFOWJSPO04@64&3/".&
QBTTXPSETECFOWJSPO04@1"44803%
UFOBOUTECFOWJSPO04@5&/"/5@/".&
JEFOUJUZ@VSMTECFOWJSPO04@"65)@63-
DPNQVUF@SFHJPOTECFOWJSPO04@3&(*0/@/".&
In order to make this work, you will also need to enable the FOW SDB driver in your
NBTUFS, NJOJPO, or DMPVE configuration file:
FOWJSPO
ESJWFSFOW
With all of this set up, each user will be able to use their own environment variables with a
single global configuration. They could even set up multiple scripts, each with their own set
of configuration variables, for different OpenStack installations. Let's say you have two
separate shell files, each for a different region of the same provider, called
PQFOTUBDL@OZDTI and PQFOTUBDL@EGXTI respectively. Each file has the
aforementioned OpenStack variables set up in them. You could then set up TBMUDMPVE
aliases in your _CBTI@QSPGJMF file to run those scripts before issuing a Salt Cloud
command:
alias salt-cloud-nyc='source ~/openstack_nyc.sh; salt-cloud'
alias salt-cloud-dfw='source ~/openstack_dfw.sh; salt-cloud'
Using SDB with AWS/EC2
OpenStack is not the only cloud provider whose users have gotten used to using 
environment variables for authentication and the like. The AWS command-line tools also
make use of these. For instance, you may have the following set in your _CBTI@QSPGJMF
file:
FYQPSU"84@"$$&44@,&:@*%"#$%&'
FYQPSU"84@4&$3&5@"$$&44@,&:BCDEFGBCDEFG

Taking Salt Cloud to the Next Level
[ 163 ]
With those variables in place, you can now set up the
FUDTBMUDMPVEQSPWJEFSTEFDDPOG file with the following lines in place:
JETECFOWJSPO"84@"$$&44@,&:@*%
LFZTECFOWJSPO"84@4&$3&5@"$$&44@,&:
Don't forget to enable the FOW SDB driver:
FOWJSPO
ESJWFSFOW
And now, each AWS user will be able to use their own environment variables with the
same global configuration file.
Using SDB with other cloud providers
The aforementioned examples make use of standard environment variables that are used by
CLI tools for OpenStack and AWS respectively. I pointed these out because they are two of
the most common cloud providers, and each has a user community that is already used to
these tools.
Some other providers also have standard environment variables. For instance, the Linode
CLI tools make use of -*/0%&@"1*@,&:, which can be pulled in using the following:
NZMJOPEFDPOGJH
BQJLFZTECFOWJSPO-*/0%&@"1*@,&:
Many other cloud providers don't make use of standard environment variables, but that
doesn't mean you can't set them up yourself. I would recommend you check the
documentation for your cloud provider first in order to see whether such configuration
already exists. Then, once you know which environment variables you're going to use, go
ahead and set up your cloud provider files and shell scripts that export the appropriate
configuration for your users.
Building custom deploy scripts
One of the most critical aspects of Salt Cloud is its ability to not only create compute
instances but also deploy Salt (or anything else) once those instances become available. The
vast majority of users will use the Salt Bootstrap script to install Salt, but there are times
when augmenting or replacing this script with your own files is appropriate.

Taking Salt Cloud to the Next Level
[ 164 ]
Understanding the Salt Bootstrap script
The Salt Bootstrap script is the default deployment method to install Salt on non-Windows
compute instances with Salt Cloud. By default, it will only install the Salt minion service. It
also has the capability to install Salt master and Salt syndic if required.
The Salt Bootstrap script has some special considerations, which dictate the way in which it
works. First, it was designed to be run on as many POSIX platforms as possible, including
various flavors of both Unix and Linux. In order to accommodate the disparate
environments, it was written to be compatible with the Bourne shell, also known as TI.
As it's very rare to find a Unix or Linux distribution that does not support the Bourne shell,
the Salt Bootstrap script can run pretty much anywhere (except for Windows). It uses this
portability to automatically detect the operating system that it's running on so that it can
install the necessary dependencies for Salt, followed by Salt itself.
This brings us to another important aspect of Salt Bootstrap. While it will execute on almost
any platform, it does need to be written specifically for a platform in order to be able to do
its job there. As of Bootstrap version 2015.03.15, the following platforms are supported:
Amazon Linux 2012.09
Arch Linux
CentOS 5/6
Debian 6.x/7.x (Git installations only)
Debian 8
Fedora 17/18
FreeBSD 9.1/9.2/10
Gentoo
Linaro
Linux Mint 13/14
OpenSUSE 12.x
Oracle Linux 5/6
Red Hat Enterprise 5/6
Scientific Linux 5/6

Taking Salt Cloud to the Next Level
[ 165 ]
SmartOS
SUSE Linux Enterprise 11 SP1 b 3
Ubuntu 10.x/11.x/12.x/13.04/13.10
Elementary OS 0.2
You've probably noticed the note about Debian Git installations. In its normal mode of
operation, Salt Bootstrap will install the latest version of Salt available for that platform
using the prebuilt packages that are available in that platform's public repositories. Let's
first talk about installing from those.
Installing from prebuilt packages
Most people who use Salt Bootstrap will do so from Salt Cloud, Salty Vagrant, or another
similar tool that will automatically place it on the compute node to be deployed and run it
for them. However, it is helpful to know how to run Salt Bootstrap manually.
A copy is distributed with Salt so that Salt Cloud can make use of it. If you are running the
most recent version of Salt, you probably have the most recent version of Salt Bootstrap as
well. If you want to be sure, you can ask Salt Cloud to update it for you with the following
code:
# salt-cloud --update-bootstrap
You can also download the latest stable version manually on the target system. There are a
number of different ways to do this from the command line, depending on the
downloading tool that your operating system has installed:
# curl -L https://bootstrap.saltstack.com -o
      bootstrap-salt.sh
# wget -O bootstrap-salt.sh
      https://bootstrap.saltstack.com
# python -m urllib "https://bootstrap.saltstack.com"
      > bootstrap-salt.sh
# fetch -o bootstrap-salt.sh
      https://bootstrap.saltstack.com
Once you have it downloaded, you can simply run it with no arguments to install the Salt
minion, as shown in the following code:
# sh bootstrap-salt.sh

Taking Salt Cloud to the Next Level
[ 166 ]
If you would like to install Salt master as well, add the . argument:
# sh bootstrap-salt.sh -M
To install Salt syndic, add 4:
# sh bootstrap-salt.sh -S
Also, if you want to install the Salt libraries but not the Salt minion, add the / argument:
# sh bootstrap-salt.sh -N
Any of these arguments can be used together as required. When any of the Salt services are
installed, they will be started automatically. If you prefer not to start them right away, you
can add the 9 argument:
# sh bootstrap-salt.sh -X
This doesn't work everywhere; Debian-based distributions-such as Ubuntu-are designed to
start services automatically as part of the package installation process. Salt Bootstrap will
warn the user about this. However, efforts have been made to support this argument
whenever possible.
Keep in mind that automatically starting a service that has not yet been configured properly
can be problematic. By default, the Salt minion will check the DNS for the Salt master at the
Salt hostname. It will also poll the minion's own hostname and use that as the minion's ID.
If there is no master at the Salt hostname or if a number of minions have the MPDBMIPTU
hostname, this will cause problems.
Before you run the Salt Bootstrap script, you can either place configuration files and keys
directly where they need to be or put them in a temporary directory and ask the Bootstrap
script to put them in place for you:
# sh bootstrap-salt.sh -c /tmp/.saltbootstrap/
This feature exists so that automated deployment programs, such as Salt Cloud, can spend
as few resources as possible getting the files to the target. A simplified version of the
commands run by Salt Cloud to create and populate this directory is as follows:
(via ssh)# mkdir /tmp/.saltcloud-<randomhash>/
(via ssh)# chmod 700 /tmp/.saltcloud-<randomhash>/
# scp minion target:/tmp/.saltcloud-<randomhash>/
# scp minion.pem target:/tmp/.saltcloud-<randomhash>/
# scp minion.pub target:/tmp/.saltcloud-<randomhash>/
# scp bootstrap-salt.sh target:/tmp/.saltcloud-
      <randomhash>/

Taking Salt Cloud to the Next Level
[ 167 ]
Once the directory has been populated, Salt Cloud will issue the command to run the
Bootstrap script. After this has finished, Salt Cloud will clean up the temporary directory on
the target and return to the user.
Installing from Git
If you do not wish to use the public repositories for your infrastructure, then you have two
options. One is to install from Git. The other is to use a custom deploy script. We'll get to
custom scripts in a moment-let's talk about installing from Git first.
To install from Git, you need to pass HJUCSBODI  to the end of the Salt Bootstrap
command:
# sh bootstrap-salt.sh git develop
By default, this will use SaltStack's own Salt repository on GitHub:
IUUQTHJUIVCDPNTBMUTUBDLTBMU
If you want to install Salt from a different Git repository, such as your own fork, you can
use the H argument:
# sh bootstrap-salt.sh -g https://github.com/myuser
      /salt.git git develop
Normally, Salt Bootstrap will use a HJU URL for Salt. If the Git port is blocked, you will
need to use the IUUQT URL for Git. Instead of manually specifying the IUUQT URL,
you can use ( to use it automatically, as shown in the following code:
# sh bootstrap-salt.sh -G git develop
If you want to install a version of Salt that is different from the version in your distribution's
repositories, using a Git-based installation is often the easiest. SaltStack uses Git tags to
keep track of different major versions of Salt. As of January 2014, Salt is versioned with the
year and the month, and the tags reflect this. To install the  branch of Salt, use this
command:
# sh bootstrap-salt.sh git 2016.11

Taking Salt Cloud to the Next Level
[ 168 ]
Looking back at legacy deploy scripts
Before the Salt Bootstrap script, there were a number of different scripts used to deploy Salt.
Each operating system had two scripts: one to install from that distribution's own
repositories and one to install from Git. Although they have fallen into disuse and are often
out of date, they are still included with Salt Cloud.
One of the reasons these scripts still ship is academic: users who are unable to use Salt
Bootstrap for some reason can examine the legacy deploy scripts and modify them for their
own purposes.
There are also some newer deploy scripts that wrap around the Salt Bootstrap script. These
scripts are designed explicitly for user modification: users can add their own commands
before or after the Salt Bootstrap script runs. If you have a lot of work that needs to be done
by the deploy script before Salt starts up and takes over, then a customized deploy script
may be ideal.
Writing your own deploy scripts
In the early days, Salt Cloud was designed to do little more than create a compute instance,
install Salt on it, and auto-accept that minion's keys on the master. As the most complex
variable here was which operating system was running on the target, this script was 
specified using the PT argument. Before long, it became clear that Salt Cloud needed to
support more complexity, so the PT argument was changed to TDSJQU.
Back in those days, custom scripts needed to be added directly to the EFQMPZ directory of
Salt Cloud's source tree. Fortunately, we can now take advantage of a simpler and more
predictable directory: FUDTBMUDMPVEEFQMPZE.
Scripts in this directory may be referred to with or without the TI extension, which is
normally associated with Bourne shell scripts. This doesn't mean that the scripts have to be
Bourne scripts, but if no extension is specified and Salt Cloud is unable to find it, then the
TI extension will be added, and Salt Cloud will look again.
The deploy script will generally perform the following tasks:
Place automatically signed keys on the minion
Place the minion's configuration file
Install the Salt minion package for that operating system
Start the TBMUNJOJPO service

Taking Salt Cloud to the Next Level
[ 169 ]
As with most files in Salt, this file can be templated using Salt's renderer system. By default,
the Jinja templating system will be used, but of course, other renderers are also available.
The purpose of using a renderer for this file is to be able to place keys and other files on the
minion. Other configuration variables from the provider and profile configuration blocks
will also be merged, if specified.
The following is a very basic script to install Salt on an Ubuntu target:
#!/bin/bash
# Install the minion's keys
mkdir -p /etc/salt/pki/minion
echo '{{ vm['priv_key'] }}' > /etc/salt/pki/minion
      /minion.pem
echo '{{ vm['pub_key'] }}' > /etc/salt/pki/minion
      /minion.pub
# Write the minion's configuration file
cat > /etc/salt/minion <<EOF
{{minion}}
EOF
# Set up Ubuntu repositories
echo deb http://ppa.launchpad.net/saltstack
      /salt/ubuntu `lsb_release -sc` main | tee /etc/apt
      /sources.list.d/saltstack.list
wget -q -O- "http://keyserver.ubuntu.com:11371
      /pks/lookup?op=get&search=0x4759FA960E27C0A6" |
      apt-key add -
apt-get update
# Install Salt
apt-get install -y -o DPkg::Options::=--force-
      confold salt-minion
# No need to start services on Ubuntu; it will be
      done by apt
You can see the template variables for the minion's private and public keys as \\
WN<	QSJW@LFZ	>^^ and \\WN<	QVC@LFZ	>^^, respectively. You can also see the
minion's configuration file as \\NJOJPO^^. Any other work that needs to be performed
can be added here as needed.

Taking Salt Cloud to the Next Level
[ 170 ]
Of course, the section of this script that sets up the Ubuntu repositories can be a little
unwieldy. Also, the Salt Bootstrap script will take care of those for you. Plus, the keys and
configuration file will be uploaded manually by Salt Cloud anyway, so unless you have a
specific reason to put them here, you can skip those steps and have a greatly simplified file,
which is much more multiplatform. The following script is much shorter and makes use of a
couple more tricks:
#!/bin/sh
wget -O - https://bootstrap.saltstack.com | sudo sh
      -s -- "$@"
This script makes use of the infamous one-line installer. It's nice because it demonstrates
how Salt can be installed with a single command even when extra work needs to be
performed, such as setting up repositories. However, it does have issues.
The biggest issue is with the source. You are trusting an arbitrary URL, which you have no
control over, to provide a script that will be directly piped to the TI command. This is
considered insecure by security professionals and should normally be avoided. However,
since Salt Cloud used to only upload a specific set of files, custom scripts that wrapped the
Bootstrap script had little other choice. Technically, the following script demonstrates a
more secure way to download and use the Salt Bootstrap script:
#!/bin/bash
wget -O bootstrap-salt.sh
      https://bootstrap.saltstack.com
sudo sh bootstrap-salt.sh "$@"
However, since this is run as a script anyway, there is no functional difference. It is much
more secure to upload your own copy of the Salt Bootstrap script in addition to the
wrapper. In a moment, we'll discuss how to do that. But, first, let's talk about the other trick
used here: passing arguments to your script.
Passing arguments to scripts
As you have already seen, the Salt Bootstrap script supports a number of arguments. Also,
some of these will be automatically added by Salt Cloud. However, you may wish to
specify some arguments directly. This can be especially important for your own custom
scripts, which support your own arguments.

Taking Salt Cloud to the Next Level
[ 171 ]
Salt Cloud allows script arguments to be passed with the TDSJQU@BSHT argument
anywhere in the Salt Cloud configuration files, which are used for the target minion,
provider, profile, and so on. The following profile configuration block will not only install
Salt as usual, but also attempt to install Apache Libcloud so that the target minion can also
be used to run Salt Cloud with a Libcloud-based provider:
HPHSJEDFOUPT
QSPWJEFSNZHPHSJE
TJ[F.#
JNBHF$FOU04
CJUX/POF
TDSJQU@BSHT-
We covered a few arguments earlier in this chapter. In addition to those, the following
arguments are also available:
W: This specifies the display script version.
O: This denotes no colors.
%: This shows debug output.
L: This specifies the temporary directory that holds the minion keys that will
preseed the master.
T: This refers to the sleep time used when waiting for daemons to start and
restart and when checking for running services. The default is
\@@%&'"6-5@4-&&1^.
$: This only runs the configuration function. This option automatically bypasses
any installation.
1: This allows installations using QJQ. On some distributions, the required Salt
packages or their dependencies are not available as a package for that
distribution. Using this flag allows the script to use QJQ as a last-resort method.
This only works for functions that actually implement
QJQ
installations.
': This allows copied files to overwrite existing files (DPOGJH, JOJUE, and so
on).
6: If this is set, the system is fully upgraded prior to bootstrapping Salt.
,: If this is set, the temporary files are kept in the temporary directories,
specified with D and L.

Taking Salt Cloud to the Next Level
[ 172 ]
*: If this is set, insecure connections are allowed when you download files, for
example, pass OPDIFDLDFSUJGJDBUF to XHFU or JOTFDVSF to DVSM.
": This passes the TBMUNBTUFS DNS name or IP and will be stored under
\@4"-5@&5$@%*3^NJOJPOENBTUFSBEESFTTDPOG.
J: This passes the TBMUNJOJPO ID and will be stored under
\@4"-5@&5$@%*3^NJOJPO@JE.
-: This installs the Apache Libcloud package if possible (required for TBMU
DMPVE).
Q: This specifies extra packages to install when you install Salt dependencies.
You are limited to one package per Q flag. You're responsible for providing the
proper package name.
): This uses the specified HTTP proxy for the installation.
;: This enables the external software source for newer ZeroMQ versions (only
available for RedHat-based distributions).
Using file maps
In the Building custom deploy scripts section, we touched on how to upload your own custom
version of the Salt Bootstrap script. You can in fact upload any number of files to a minion
before executing the deploy script. However, you may issue only one command, as
explained before, with the TDSJQU argument, and any arguments are passed using the
TDSJQU@BSHT argument.
Therefore, if you need to upload and execute a number of scripts, you will need to create
one master script that executes them for you. This is the script that will be specified with the
TDSJQU argument. The other files can be uploaded with a file map.
The GJMF@NBQ variable is a dictionary that can be added to any relevant configuration file
for the minion. Each key in the dictionary is the name of a local file that is to be uploaded,
and the value is the name of the remote path to which it should be uploaded. Consider the
following cloud profile:
FDVCVOUV
QSPWJEFSNZFD
JNBHFBNJEFFFB
TJ[FUTNBMM
TTI@VTFSOBNFVCVOUV
TFDVSJUZHSPVQEFGBVMU
TDSJQUJOTUBMMTI
GJMF@NBQ

Taking Salt Cloud to the Next Level
[ 173 ]
TSWTBMUTDSJQUTJOTUBMMTIUNQJOTUBMMTI
TSWTBMUTDSJQUTJOTUBMMQZUNQJOTUBMMQZ
TSWTBMUTDSJQUTDVTUPNQLHEFCUNQDVTUPNQBDLBHFEFC
This profile will upload a shell script, a Python script, and a package file to the UNQ
directory on the minion. It won't necessarily upload them in that order, but that's okay; the
JOTUBMMTI script (which will be uploaded from FUDTBMUDMPVEEFQMPZE on the
system running Salt Cloud) will execute them in the proper order.
Take note of the last file in GJMF@NBQ. If you specify a different filename
on the target system than on the local system, Salt Cloud will rename it for
you when it uploads it.
Taking a look at cloud maps
So far, we have only discussed how to work with compute instances on an individual level.
However, one of the earliest features of Salt Cloud was the concept of a cloud map file (not
to be confused with GJMF@NBQ). Cloud maps allow you to specify a group of machines to
create all in one shot.
This can be very useful when managing a small infrastructure or small pieces of a large
infrastructure. Not only can you declare that certain profiles be used to declare a number of
compute instances, but you can also append and override configuration in those profiles.
Let's say that you have an infrastructure that contains database servers, web servers, and
load balancers. Each type of server will have its own unique needs, but there will also be
multiple instances of each type of server. First, let's define the provider:
NZFD
JE'8&),+'4%"'%&%'
LFZ	GFXIHSF'3&'4&GSFHS'%4%4%4'EGGV	
LFZOBNFNZDPNQBOZ
QSJWBUF@LFZSPPUNZDPNQBOZQFN
TFDVSJUZHSPVQQSJWBUF
MPDBUJPOVTFBTU
QSPWJEFSFD
SFOBNF@PO@EFTUSPZ5SVF
EFMWPM@PO@EFTUSPZ5SVF
PXOFSBNB[PO
NJOJPO
NBTUFS
5IFOXF	MMEFGJOFUIFQSPGJMFTBTGPMMPXT
FDMPBECBMBODFS

Taking Salt Cloud to the Next Level
[ 174 ]
QSPWJEFSNZFD
TJ[FUNJDSP
JNBHFBNJEFFFB
TFDVSJUZ@HSPVQQVCMJD
FDXFC
QSPWJEFSNZFD
TJ[FUTNBMM
JNBHFBNJEFFFB
TFDVSJUZ@HSPVQQVCMJD
FDEBUBCBTF
QSPWJEFSNZFD
TJ[FNYMBSHF
JNBHFBNJEFFFB
To finish, we'll define a map that makes use of each of these profiles, as shown in the
following code:
FDEBUBCBTF
EC
HSBJOT
SPMFEBUBCBTF
FDMPBECBMBODFS
MC
HSBJOT
SPMFMPBECBMBODFS
OPUFQSJNBSZMPBECBMBODFS
MC
HSBJOT
SPMFMPBECBMBODFS
OPUFTFDPOEBSZMPBECBMBODFS
\TFUXFCTFSWFST
	XFC		XFC		XFC	^
\GPSTFSWFSJOXFCTFSWFST^
FDXFC
\\TFSWFS^^
HSBJOT
SPMFXFC
\FOEGPS^
Three different techniques have been used in this map to define server names. However,
only one database server has been defined: EC. It also has a custom grain set, which
declares its SPMF to be EBUBCBTF. However, two load balancers, called MC and MC,
have been defined. They both have their SPMF grains set to MPBECBMBODFS, but they also
have a unique value for their OPUF grains.

Taking Salt Cloud to the Next Level
[ 175 ]
The last profile, FDXFC, defines three different web servers, but it does so using Jinja
templating. As with other files, cloud map files use Salt's renderer system, making
templating easy. In this map, there's nothing special about each of the web servers; they will
be identical. So, we just declared a tuple in Jinja with their names and then looped through
each of them to create the final map file, which will look like the following code:
FDEBUBCBTF
EC
HSBJOT
SPMFEBUBCBTF
FDMPBECBMBODFS
MC
HSBJOT
SPMFMPBECBMBODFS
OPUFQSJNBSZMPBECBMBODFS
MC
HSBJOT
SPMFMPBECBMBODFS
OPUFTFDPOEBSZMPBECBMBODFS
FDXFC
XFC
HSBJOT
SPMFXFC
XFC
HSBJOT
SPMFXFC
XFC
HSBJOT
SPMFXFC
In order to run this map, we will use the N or NBQ command-line argument:
salt-cloud -m /etc/salt/cloud.maps.d/mymap.map
By default, Salt Cloud will create these machines serially. In order to create them in parallel,
we will add 1 or QBSBMMFM:
salt-cloud -P -m /etc/salt/cloud.maps.d/mymap.map
You may have noticed the DMPVENBQTE directory. This is a directory suggested by
SaltStack, which matches the naming scheme for other Salt Cloud directories. However, it is
not required, and in fact, Salt Cloud will not even look in this directory unless it is pointed
directly to it. If you do not specify an absolute path to a map file, Salt Cloud will look in the
current working directory.

Taking Salt Cloud to the Next Level
[ 176 ]
Using reactors with Salt Cloud
Salt Cloud is powerful on its own, but it can be made even more powerful by tying it in
Salt's own event bus. Let's go ahead and set up a workflow to kick off whenever a new
server is spun up using Salt Cloud. The steps that we need to perform, in order, are as
follows:
Request a new cloud server.
1.
Upon its creation, kick off a test suite to validate the servers.
2.
If the tests fail, fire an alert and stop.
3.
If they succeed, add the server to the load balancer.
4.
We won't create a full test suite here as that is well beyond the scope of this book. However,
because we can make use of Jinja templates, we will go ahead and perform some
configuration for the load balancer.
Setting up the event tags
We're going to use a combination of both standard cloud tags and our own custom tags in
our example. Go ahead and edit the NBTUFS configuration file, and add a SFBDUPS section:
SFBDUPS
-PPLGPSOFXXFCTFSWFST
	TBMUDMPVEXFCDSFBUFE	
4UBSUUIFUFTUTVJUF
	TSWSFBDUPSXFCUFTUTTMT	
-PPLGPSGBJMVSFT
	RBMJWFUFTUTXFCGBJM	
'JSFBOBMFSU
	TSWSFBDUPSXFCUFTUGBJMTMT	
-PPLGPSTVDDFTT
	RBMJWFUFTUTXFCTVDDFTT	
"EETFSWFSUPMPBECBMBODFS
	TSWSFBDUPSXFCIBQSPYZBEETMT	
In this example, all our web servers start with XFC. Undoubtedly, you'll want to run
different tests against different types of servers, but this will get us through our example.

Taking Salt Cloud to the Next Level
[ 177 ]
Now, we need to set up a reactor to kick off our test suite. This example assumes that there
is a custom module called NZUFTUTVJUF that has a series of functions, including one called
XFC, for testing servers. We'll use a minion called UFTUNBOBHFS that has been set aside just
for this purpose. Create this file as TSWSFBDUPSXFCUFTUTTMT:
UFTUOFXXFCTFSWFS
DNENZUFTUTVJUFXFC
UHUUFTUNBOBHFS
LXBSH
NJOJPO@JE\\EBUB<	OBNF	>^^
We'll also need to set up an alert that can notify us when a new server does not pass the test
suite. This reactor will make use of another minion, called NPOJUPSJOH, which has been
configured to fire alerts using PagerDuty. Save this file as TSWSFBDUPSXFCUFTU
GBJMTMT:
OFXXFCGBJMVSF
DNEQBHFSEVUZDSFBUF@FWFOU
UHUNPOJUPSJOH
LXBSH
EFTDSJQUJPO5FTUGBJM\\EBUB<	OBNF	>^^
EFUBJMT"OFXDMPVEJOTUBODFPO\\
EBUB<	QSPWJEFS	>^^DBMMFE\\EBUB<	OBNF	>^^IBT
GBJMFEUIFXFCUFTUTVJUF
TFSWJDF@LFZBCDEFGBCDEFG
QSPGJMFNZQBHFSEVUZBDDPVOU
We're assuming that this NZUFTUTVJUFXFC function will fire an event that looks like
RBMJWFUFTUTXFCGBJM when a server fails the test suite. We're also assuming that
that function will fire an event that looks like RBMJWFUFTUTXFCTVDDFTT when a
server passes the test suite. The reactor for this will be a little more complex, because we
need to do some specific things in sequence:
Notify the load balancer about the new server.
1.
Rewrite the load balancer configuration file.
2.
Restart the load balancer.
3.
We'll make use of grains to manage the list of active servers because there is already an
execution module to manage grains. We'll use a state to handle the configuration file and
load balancer because that can be included later in a highstate run.

Taking Salt Cloud to the Next Level
[ 178 ]
We'll set up a file at TSWSFBDUPSXFCIBQSPYZBEETMT, which adds the grain and
then kicks off a state run:
BEEHSBJO
DNEHSBJOTBQQFOE
UHU	IBQSPYZ	
LXBSH
LFZXFCTFSWFST
WBM
NJOJPO\\EBUB<	OBNF	>^^
IPTUOBNF\\EBUB<	IPTUOBNF	>^^
SVOTUBUF
DNETUBUFTMT
UHU	IBQSPYZ	
BSH
IBQSPYZ
This file assumes that two pieces of data were sent in the event from the test suite: the ID of
the minion (as OBNF) and the hostname or IP address (as IPTUOBNF).
Then, we'll set up TSWTBMUIBQSPYZTMT, to be stored on any minion whose ID starts
with IBQSPYZ:
FUDIBQSPYZIBQSPYZDPOG
GJMFNBOBHFE
TPVSDFTBMUIBQSPYZIBQSPYZDPOG
UFNQMBUFKJOKB
NPEF
VTFSSPPU
HSPVQSPPU
IBQSPYZ
TFSWJDFSVOOJOH
FOBCMF5SVF
SFMPBE5SVF
SFRVJSF
GJMFFUDIBQSPYZIBQSPYZDPOG

Taking Salt Cloud to the Next Level
[ 179 ]
Finally, we'll set up a template to be used to generate the IBQSPYZDPOG configuration file.
This will be stored at TSWTBMUIBQSPYZIBQSPYZDPOG. To avoid confusion, I'll only
show the part of the configuration file here that we're actually interested in:
MJTUFOIUUQXFCTFSWJDFT
BDMTFSWFST@EPXOOCTSW
TFSWFSTMU
NPOJUPSVSJIBQSPYZNPOJUPS
NPOJUPSGBJMJGTFSWFST@EPXO
\GPSNJOJPOJOHSBJOT<	XFCTFSWFST	>^
$POGJHVSBUJPOGPS\\NJOJPO<	NJOJPO	>^^
TFSWFS\\NJOJPO<	IPTUOBNF	>^^\\NJOJPO<	IPTUOBNF	>^^NBYDPOO
DIFDLJOUFSTSJTFGBMM
\FOEGPS^
With everything in place, we can now create new servers using the TBMUDMPVE command,
which kicks off this whole process:
# salt-cloud -p webserver web-093
If everything goes well, then each server whose name starts with IBQSPYZ will end up
having the following appear in its FUDIBQSPYZIBQSPYZDPOG file:
MJTUFOIUUQXFCTFSWJDFT
BDMTFSWFST@EPXOOCTSW
TFSWFSTMU
NPOJUPSVSJIBQSPYZNPOJUPS
NPOJUPSGBJMJGTFSWFST@EPXO
$POGJHVSBUJPOGPSXFC
TFSWFSNBYDPOODIFDLJOUFSTSJTFGBMM
Working with autoscale reactors
Some cloud providers are able to actively send updates to automated systems, such as Salt,
which Salt Cloud can act on, but even if they don't, Salt Cloud is able to poll cloud
providers for the information it needs and use that information when the cloud provider
automatically scales your infrastructure for you.

Taking Salt Cloud to the Next Level
[ 180 ]
Cloud cache
There are in fact two caches kept by Salt Cloud, but one is more of an index than anything.
Both are kept in the DMPVE directory inside Salt's own DBDIF directory. The normal
location for the cloud cache is WBSDBDIFTBMUDMPVE.
If you've used Salt Cloud to create a compute instance, then there will be a file in this
directory called JOEFYQ. This file, which is in the NTHQBDL format, contains a list of all the
compute instances that have been created by Salt Cloud (minus any that have been
subsequently destroyed by Salt Cloud). There is no configuration variable to turn this on or
off; it will be generated automatically.
If you were to open this file with NTHQBDL, you would find the list of compute instances
with a very small amount of information about them, as shown in the following code:
\
UFTUJOTUBODF\
ESJWFSFD
JEUFTUJOTUBODF
QSPGJMFDFOUPT
QSPWJEFSNZFD
^
^
This information can be used by various components, such as the DMPVE roster, for Salt SSH
to quickly determine which provider a compute instance belongs to and which profile was
used to create it.
However, it is possible to extend the DMPVE cache to include more information. Be warned
that this may slow Salt Cloud execution for some providers, but you may find it worth the
extra time. To turn on the full cloud cache, set the following value in the main cloud
configuration file:
VQEBUF@DBDIFEJS5SVF
With this option turned on, Salt Cloud will make an entry in this directory for every single
compute instance that is queried when either a GVMMRVFSZ operation is performed
against a cloud provider or when a TIPX@JOTUBODF action is performed against a single
compute instance.

Taking Salt Cloud to the Next Level
[ 181 ]
Once the cache is updated, a new set of directories will show up under
WBSDBDIFTBMUDMPVE. First, there will be a directory called BDUJWF. In this
directory, there will be another directory for each driver (FD, MJOPEF, TPGUMBZFS, and so
on) that has been queried. In each of these directories, there will be another directory for
each user-defined provider-configuration block (NZFDDPOGJH, NZMJOPEFDPOGJH,
NZTPGUMBZFSDPOGJH, and so on). In each of these directories, there will be a NTHQBDL
file with a Q extension for each node that has been queried for that provider. Such a
directory structure will look like the following code:
The content of each file is the output that will be shown for that compute instance in either a
GVMMRVFSZ or TIPX@JOTUBODF action. For example, take a look at the following code:
\
BNJ-BVODI*OEFY
BSDIJUFDUVSFY@
4/*1
UBH4FU\
JUFN\
LFZ/BNF
WBMVFUFDIIBUNBTUFS
^
^
WJSUVBMJ[BUJPO5ZQFQBSBWJSUVBM
^
The contents of these files become especially useful when Salt Cloud is instructed to fire
events when something changes. In order to do this, set the following value in the main
cloud configuration file:
EJGG@DBDIF@FWFOUT5SVF

Taking Salt Cloud to the Next Level
[ 182 ]
When this is turned on, Salt Cloud will fire events when GVMMRVFSZ is executed. If a
compute instance is found that was not previously in the cache, an event tagged
TBMUDMPVEWN@OBNF DBDIF@OPEF@OFX will be fired. If a compute instance that
previously existed in the cache is no longer there, then an event tagged
TBMUDMPVEWN@OBNF DBDIF@OPEF@NJTTJOH will be fired. Also, if a piece of
information about a compute instance has changed (such as its running status), then an
event tagged TBMUDMPVEWN@OBNF DBDIF@OPEF@EJGG will be fired.
Using cloud cache events
These cloud cache events can be used in conjunction with autoscaling systems. Many cloud
providers offer their own autoscaling solutions but do not actively issue notifications to 
users when compute instances are created or destroyed. A small handful of cloud providers
do, which can speed things up dramatically.
If you are working with one of the many cloud providers that do not actively send
notifications, do not despair; Salt Cloud can still help. If set on a schedule, it will poll the
cloud provider for you on a regular basis, firing events when it finds any changes.
Setting up a schedule
To use Salt's own scheduler on the master, add the following code to your master
configuration file:
TDIFEVMF
DMPVE@RVFSZ
GVODUJPODMPVEGVMM@RVFSZ
NJOVUFT
When this master configuration is applied, Salt Cloud will perform a full query for you
every  minutes, updating the cache.
It is recommended to keep the time value conservative: bear in mind that each time this is
run, it will query the cloud provider directly. If you are using a cloud provider that meters
API calls and you query them too often, you risk getting blocked on a number of queries.

Taking Salt Cloud to the Next Level
[ 183 ]
It may also be useful to set aside a dedicated minion to perform cloud queries. A number of
infrastructure managers have decided to go this route to keep the load off the master. If you
feel this is the right move for you, go ahead and install the necessary Salt Cloud
configuration files on your chosen minion, and set the TDIFEVMF option in the minion
configuration file, which is exactly the same as on the master:
TDIFEVMF
DMPVE@RVFSZ
GVODUJPODMPVEGVMM@RVFSZ
NJOVUFT
You don't even have to install the Salt Cloud package on the minion that will be performing
the work. This is because Salt Cloud itself is built in to the core Salt libraries; the Salt Cloud
package provides little more than the TBMUDMPVE command. However, since there is a
cloud execution module (which mirrors the cloud runner module on the master), you don't
actually need the TBMUDMPVE command to exist in order to use Salt Cloud. However, you
will still need any dependencies (Libcloud, Azure, and so on) that are required for your
cloud provider of choice.
Of course, if your infrastructure uses the cron system instead of Salt's built-in scheduler,
you can still kick off events. However, you will need the TBMUDMPVE command. Go ahead
and set the following record in your crontab:
VTSCJOTBMUDMPVEGVMMRVFSZ
This will also issue a GVMMRVFSZ operation every  minutes, which is based on clock
time rather than the time that the TBMUNBTUFS or the TBMUNJOJPO services started up.
Catching cloud cache events
Once you have events set up to fire when the cloud cache changes, you can set up a reactor
to respond to them. However, before we get into the details of how to set up the reactor,
let's talk about the workflow that is going on here.
When Salt Cloud is asked to create a compute instance using either a QSPGJMF or NBQ
argument, it will perform the following:
Request that the cloud provider create a compute instance
Wait for an IP address to become available for that compute instance
Wait for SSH/SMB to become available at that IP address

Taking Salt Cloud to the Next Level
[ 184 ]
Upload files to that IP address
Execute the deploy script or the Windows installer
Clean up temporary files
Return to the user
Some cloud providers do more than this, but every cloud provider performs at least these
steps.
When Salt Cloud detects that a new minion has appeared, we know that the first of these
steps (request a compute instance) has already been performed. We don't necessarily know
that any other steps were performed, but that's okay; each step naturally leads to the next.
All we need to do is tell Salt Cloud to skip Step 1 and start with Step 2, using the ID of the
compute instance that we provide it.
Keep in mind that not every cloud provider currently supports skipping
the first step. As of Salt version 2015.5, the EC2 driver and all OpenStack-
based drivers support it.
When Salt Cloud finds a new compute instance, it will fire an event using the
TIPX@JOTUBODF function that contains all the information that will be shown for that node:
5BHTBMUDMPVENZOFXJOTUBODFDBDIF@OPEF@OFX
%BUB
\	@TUBNQ		5	
	FWFOU		OFXOPEFGPVOE	
	OFX@EBUB	\	BNJ-BVODI*OEFY			
	BSDIJUFDUVSF		Y@	
	JE		JEFBEDBGF	
	JOTUBODF*E		JEFBEDBGF	
4/*1
	UBH4FU	\	JUFN	\	LFZ		/BNF		WBMVF	
	NZOFXJOTUBODF	^^
	WJSUVBMJ[BUJPO5ZQF		QBSBWJSUVBM	^^
The most important piece of information here is the JE field. We'll assume that the ID of the
compute instance is also the name that you'll want to refer to the minion with (which is
probably true for a compute instance that was created by an autoscaler). For simplicity,
we'll also assume that you're only working with one provider.
First, let's go ahead and create a map for the reactor in the master configuration file:
SFBDUPS
TBMUDMPVEDBDIF@OPEF@OFX
TSWSFBDUPSOFX@DPNQVUF@JOTUBODFTMT

Taking Salt Cloud to the Next Level
[ 185 ]
Then, we'll set up a simple Jinja-based reactor to kick off a Salt Cloud process, as follows:
cat /srv/reactor/new_compute_instance.sls
OFX@DPNQVUF@JOTUBODF
SVOOFSDMPVEDSFBUF
JOTUBODFT\\EBUB<	OFX@EBUB	><	JE	>^^
JOTUBODF@JE\\EBUB<	OFX@EBUB	><	JE	>^^
QSPWJEFSNZFD
Profile configuration is not necessary here because the compute instance has already been
created. However, provider configuration does need to be specified so that Salt Cloud
knows how to access the compute instance and its metadata.
However, this information is not necessary when Salt Cloud detects that a compute instance
has disappeared. Salt Cloud doesn't need to do anything by itself, including destroying the
node, because it's already been destroyed. However, the old public key for that minion
should still be cleaned up on the master. Let's add one more reactor. The event that will be
generated looks like the following code:
5BHTBMUDMPVENZNJTTJOHJOTUBODFDBDIF@OPEF@NJTTJOH
%BUB
\	@TUBNQ		5	
	FWFOU		DBDIFEOPEFNJTTJOHGSPNQSPWJEFS	
	NJTTJOHOPEF		NZNJTTJOHJOTUBODF	^
First, let's map it out in the master configuration:
SFBDUPS
TBMUDMPVEDBDIF@OPEF@NJTTJOH
TSWSFBDUPSNJTTJOH@DPNQVUF@JOTUBODFTMT
Then, we'll set up a reactor that cleans up the key from the master using the XIFFM
subsystem:
cat /srv/reactor/missing_compute_instance.sls
NJTTJOH@DPNQVUF@JOTUBODF
XIFFMLFZEFMFUF
NBUDI\\EBUB<	NJTTJOH@OPEF	>^^
Now, when Salt Cloud detects that a node is missing, its key will be cleaned up right away.

Taking Salt Cloud to the Next Level
[ 186 ]
Summary
Salt Cloud can be a very powerful tool in the hands of an experienced user. It has moved
from its humble beginnings as a simple tool for creating virtual machines to a crucial
component of many production infrastructures with complex provisioning needs.
The Salt Cloud configuration can be simple, but it also has the flexibility to be very complex
if required. The Salt Bootstrap script is also very powerful but not a one-size-fits-all
solution. Fortunately, we can replace it when another solution is better for our needs.
Cloud maps are also very useful for managing infrastructures and they bring with them the
power of Salt's renderer system. The reactor system can be used to manage tasks such as
starting test suites and updating configuration. Also, when third-party systems manage
clouds for you, it is still possible to bring Salt Cloud into the game using autoscale reactors.
In the next chapter, we'll take a look at how Salt works with REST interfaces, both as a client
and as a server.

8
Using Salt with REST
We've discussed using Salt not only to manage an infrastructure from within, but also to
grow the infrastructure with Salt Cloud. But what happens when you need to manage your
infrastructure from the outside? Or if you want your infrastructure to take advantage of
external systems? In this chapter, we'll explore how Salt makes use of REST interfaces, both
as a client and as a server. We'll be covering the following topics:
Taking advantage of Salt's HTTP library
Setting up the Salt API
Communicating as a client or a server
Parsing data
Reacting with Thorium
Looking at Salt's HTTP library
An increasing number of subsystems inside Salt are designed to make use of external APIs.
At the moment, most of these are drivers for Salt Cloud, and most use either the Apache
project's Libcloud library or the SDK maintained by the cloud provider.
Things have been changing in recent releases, though. Salt now has a library designed to
make a generic, Salty HTTP client available to modules and for direct use by users. This
library is already being used by some compute cloud providers as well as other services
that provide a REST interface to their users.

Using Salt with REST
[ 188 ]
Why a Salt-specific library?
Why go to all this trouble instead of just using an SDK? The biggest reason is portability.
Take for example PagerDuty, which is a powerful service that manages incident alerting.
The original Salt module used a community driver for PagerDuty. At the time, this driver
didn't do much, but it did allow Salt to create alerts, which was all that was needed.
However, in practical use, it was realized that if a minion wanted to create an alert in
PagerDuty, it needed that community package installed. This would mean having one more
package to maintain on each minion across the infrastructure, and with a cluster filled with
thousands of nodes, that was just unreasonable.
So Salt said farewell to the community driver, and an entirely new set of Salt modules was
written from scratch using PagerDuty's REST API directly. Now any server that has Salt
installed automatically has access to create alerts in PagerDuty.
This explains why so many modules in Salt skip the SDK and use REST APIs directly. But
why provide a library when so many other fine libraries exist and do a nice job of providing
access to HTTP?
The first reason is that while these libraries are easy to use when it comes to connecting to
Salt, they aren't really designed to take advantage of the Salt toolset. Salt provides
templating, an event bus, reactors, and a number of other subsystems that are very
powerful. The Salt HTTP library ties these components together in a manner consistent with
Salt's own mannerisms.
But which library to use? There are a number of excellent HTTP libraries available, each
with its own pros and cons. The SFRVFTUT library is easily the most popular, and even
upstream Python documentation recommends it over its own built-in VSMMJC and VSMMJC
libraries. But due to packaging issues, requests cannot be made a hard dependency for Salt.
However, Salt does have a hard dependency on the UPSOBEP web library for internal use
with the event bus. And while Tornado is primarily designed to be used as a web server, it
also has its own HTTP client built in.
The Salt HTTP library allows any of these three libraries to be used: UPSOBEP (the default),
SFRVFTUT, and VSMMJC. In order to set a system-wide default, set the backend argument
in either the master or the minion configuration, as appropriate:
CBDLFOEUPSOBEP
CBDLFOESFRVFTUT
CBDLFOEVSMMJC

Using Salt with REST
[ 189 ]
You can also declare, per execution, which backend to use, but let's not get too far ahead of
ourselves. First, let's go over the basics of how to use the client.
Using the http.query function
The Salt HTTP client is widely available throughout Salt. Using it as a runner on the master
is identical to using it as an execution module on a minion, apart from the obvious
differences between how those two types of modules operate:
# salt myminion http.query https://www.google.com/
# salt-run http.query https://www.google.com/
We'll use the runner for the rest of the examples in this section, but all the arguments are
identical in usage between the two.
By default, these functions will not return anything. If return data is expected, then you
need to tell the function what kind to return. The following will return a dictionary
containing the HTTP status code and headers and the content of the result body:
# salt-run http.query https://www.google.com/ text=True status=True
headers=True
IFBEFST

"DDFQU3BOHFT
OPOF
"MUFSOBUF1SPUPDPM
RVJDQ
$BDIF$POUSPM
QSJWBUFNBYBHF
$POOFDUJPO
DMPTF
$POUFOU5ZQF
UFYUIUNMDIBSTFU*40
%BUF
.PO.BZ(.5
&YQJSFT

4FSWFS
HXT
7BSZ
"DDFQU&ODPEJOH
TUBUVT

UFYU
EPDUZQFIUNM 4/*1

Using Salt with REST
[ 190 ]
For simplicity, our examples from here on won't include these arguments unless they are
explicitly needed for the example in question.
A cookie jar is also available if necessary. To turn it on, set DPPLJFT to 5SVF:
# salt-run http.query https://www.google.com/
    cookies=True
By default, this cookie jar will be stored as DPPLJFTUYU in the Salt DBDIF directory. This
will normally be WBSDBDIFTBMUDPPLJFTUYU. It will also be saved in the (LWP (lib-
www-perl) format by default since it is a text-based file. To change the location of the
cookie jar and set it to use (old-style) Mozilla cookies instead, use the DPPLJF@KBS and
DPPLJF@GPSNBU arguments:
GET versus POST
By default, requests made with IUUQRVFSZ will use the (&5 method. The (&5 arguments
can either be added to the URL manually or passed as QBSBNT values. The following two
arguments are functionally identical:
# salt-run http.query http://mydomain.com/?user=larry
# salt-run http.query http://mydomain.com/
    params='{"user": "larry"}'
However, it is possible to use any other valid HTTP method. 1045 is the most common
argument to be used after (&5, but these techniques are also valid with 165, 1"5$), and so
on:
# salt-run http.query http://mydomain.com/ POST
    data='{}'
If the 1045 data to be sent is stored inside a file, then that file may be specified instead:
# salt-run http.query http://mydomain.com/ POST
    data_file=/tmp/post.txt
This is where the Salt HTTP client starts to differ from other clients: if a POST data file is
used, then it may be templated using any of the renderer engines.

Using Salt with REST
[ 191 ]
For instance, a number of older web APIs are based on XML. Unlike JSON, which can be
easily generated without specialized tools, XML often requires templates to be used, with
data merged in. The Salt HTTP client makes quick work of this. Consider the following
template:
YNMWFSTJPOFODPEJOHVUG 
TPBQ&OWFMPQFYNMOTTPBQIUUQTDIFNBTYNMTPBQPSHTPBQFOWFMPQF
YNMOTYTJIUUQXXXXPSH9.-4DIFNBJOTUBODF
YNMOTYTEIUUQXXXXPSH9.-4DIFNB
YNMOTYNJNFIUUQXXXXPSHYNMNJNF
YNMOTOTIUUQTDIFNBTIQDPN4.
YNMOTDNOIUUQTDIFNBTIQDPN4.$PNNPO 
TPBQ#PEZ 
OT%FMFUF)PTU3FRVFTU 
OTNPEFM 
OTLFZT 
OT7./BNFUZQF \\NJOJPO@JE^^OT7./BNF 
OTLFZT 
OTJOTUBODF 
OTNPEFM 
OT%FMFUF)PTU3FRVFTU 
TPBQ#PEZ 
TPBQ&OWFMPQF 
It looks daunting, doesn't it? This is actually a very small request compared to a number of
XML queries, and the only piece of information that needs to be changed is the 7./BNF
value, which has been templatized in Jinja as NJOJPO@JE. To issue a request using this file
to insert a NJOJPO@JE of XFC, use this:
# salt-run http.query http://mydomain.com/ POST
    data_file=/srv/xml/delete.xml data_render=True
    template_dict='{"minion_id": "web099"}
The two important arguments here are EBUB@SFOEFS, which must be set to 5SVF in order to
use the renderer, and UFNQMBUF@EJDU, which contains the variables to be merged into the
template. The default renderer is Jinja, but you can change it by specifying a
EBUB@SFOEFSFS argument as well.
1045 data is not the only data that can make use of Salt's renderer system. Header data may
be represented as a dictionary, a properly formatted list, or a file that can be rendered if
necessary. To send a header dictionary, use IFBEFS@EJDU:
# salt-run http.query http://example.com/
    header_dict='{"Content-Type": "application/json"}'

Using Salt with REST
[ 192 ]
To send a list of headers (making sure that they are already properly formatted), use
IFBEFS@MJTU:
# salt-run http.query http://example.com/
    header_list='["Content-Type: application/json"]'
To use a file containing all the headers (again, properly formatted), use IFBEFS@GJMF:
# cat /srv/headers/headers.txt
Content-Type: application/json
# salt-run http.query http://example.com/
    header_file=/srv/headers/headers.txt
If the IFBEFS@GJMF is templated, set IFBEFS@SFOEFS to 5SVF, and pass the values using
the UFNQMBUF@EJDU argument as before:
# cat /srv/headers/headers.txt
Content-Type: {{ content_type }}
# salt-run http.query http://example.com/
    header_file=/srv/headers/headers.txt
    header_render=True template_dict='{"content_type":
    "application/json"}'
As with the 1045 data templates, if you want to use a different renderer for the headers, set
it with IFBEFS@SFOEFSFS.
Decoding return data
Another important aspect of the Salt HTTP client is the ability to automatically decode data
that is received. If EFDPEF is set to 5SVF, Salt will try to autodetect whether the return data
is XML or JSON, unless EFDPEF@UZQF is explicitly set to either YNM or KTPO:
# salt-run http.query https://api.github.com/
    decode=True decode_type=json
dict:
    ----------
    authorizations_url:
        https://api.github.com/authorizations
    code_search_url:
        https://api.github.com/search/code?q={query}
        {&page,per_page,sort,order}
    current_user_authorizations_html_url:
        https://github.com/settings/connections
        /applications{/client_id}
    current_user_repositories_url:
        https://api.github.com

Using Salt with REST
[ 193 ]
        /user/repos{?type,page,per_page,sort}
    current_user_url:
        https://api.github.com/user
    emails_url:
        https://api.github.com/user/emails
    emojis_url:
        https://api.github.com/emojis
...SNIP...
Note that when EFDPEF is set to 5SVF, the data will be returned in the
EJDU field rather than the UFYU field that is used when UFYU is 5SVF.
You should also note that while JSON is perfectly suited to being decoded as a dictionary,
XML is very poorly suited to being translated like that. Salt will do its best, but any XML
attributes won't be decoded. If you are working with XML that does not decode properly
this way, it is best to set decode to 'BMTF and use a more XML-specific program to translate
it (or use straight Python).
Now that you've seen the basics of the Salt HTTP client and some of the more advanced
features that are specific to Salt, let's take a look at how we can use that functionality with
our state files.
Using the http.query state
The IUUQRVFSZ state differs slightly from most other states in that it serves not to actually
make changes on the minion running the state but to provide assistance to the operations of
a state run. Webhooks can be called to do things such as reporting on failures or providing
status updates.
Take, for example, the following SLS:
DPEF@USFF
GJMFSFDVSTF
OBNFTSWXFCDPEF
TPVSDFTBMUDPEF
POGBJM@JO
IUUQBMFSU@BENJOT
BMFSU@BENJOT
IUUQRVFSZ
OBNFIUUQBMFSUTFYBNQMFDPNUZQFDPEF@EFQMPZ@GBJM

Using Salt with REST
[ 194 ]
This SLS will attempt to recursively copy a directory to a minion. In the event of a failure, it
will call a URL that reports to an alerting service if a failure has occurred.
Of course, multiple calls to IUUQRVFSZ can be made within a single SLS file. We can
trigger a separate webhook for each of the states if we want:
DPEF@USFF
GJMFSFDVSTF
OBNFTSWXFCDPEF
TPVSDFTBMUDPEF
POGBJM@JO
IUUQBMFSU@BENJOT@DPEF
BMFSU@BENJOT@DPEF
IUUQRVFSZ
OBNFIUUQBMFSUTFYBNQMFDPNUZQFDPEF@EFQMPZ@GBJM
XFC@TFSWJDF
TFSWJDFSVOOJOH
OBNFOHJOY
SFRVJSF
GJMFDPEF@USFF
POGBJM@JO
IUUQBMFSU@BENJOT@XFC
BMFSU@BENJOT@XFC
IUUQRVFSZ
OBNFIUUQBMFSUTFYBNQMFDPNUZQFXFC@SFTUBSU@GBJM
Alternatively, we can just provide status via a webhook as each state is called:
DPEF@USFF
GJMFSFDVSTF
OBNFTSWXFCDPEF
TPVSDFTBMUDPEF
BMFSU@BENJOT@DPEF
IUUQRVFSZ
OBNFIUUQBMFSUTFYBNQMFDPNUZQFDPEF@EFQMPZ@GJOJTIFE
SFRVJSF
GJMFDPEF@USFF
XFC@TFSWJDF
TFSWJDFSVOOJOH
OBNFOHJOY
SFRVJSF
GJMFDPEF@USFF

Using Salt with REST
[ 195 ]
BMFSU@BENJOT@XFC
IUUQRVFSZ
OBNFIUUQBMFSUTFYBNQMFDPNUZQFXFC@SFTUBSU@GJOJTIFE
SFRVJSF
TFSWJDFXFC@TFSWJDF
This may have started to look like a lot of work-why doesn't Salt just have something built
in that can send updates as each state is completed?
Using http.query with reactors
You may recall using IUUQRVFSZ in $IBQUFS, Managing Tasks Asynchronously, in
conjunction with reactors. Each time a state is completed, an event is fired to the master
containing the result data for that event. This has an advantage over using the IUUQRVFSZ
state, in that the return data from the state will also be available.
Go ahead and fire up the event listener on the master (as described in $IBQUFS, Managing
Tasks Asynchronously), and try out this set of commands:
# salt myminion state.single file.touch /root/somedir
MPDBM

*%SPPUTPNFEJS
'VODUJPOGJMFUPVDI
3FTVMU5SVF
$PNNFOU$SFBUFEFNQUZGJMFSPPUTPNFEJS
4UBSUFE
%VSBUJPONT
$IBOHFT

OFX
SPPUTPNFEJS
4VNNBSZ

4VDDFFEFE
DIBOHFE
'BJMFE

5PUBMTUBUFTSVO
TBMUNZNJOJPOTUBUFTJOHMFGJMFEJSFDUPSZSPPUTPNFEJS
MPDBM

*%SPPUTPNFEJS
'VODUJPOGJMFEJSFDUPSZ
3FTVMU'BMTF
$PNNFOU4QFDJGJFEMPDBUJPOSPPUTPNFEJSFYJTUTBOEJTBGJMF
4UBSUFE

Using Salt with REST
[ 196 ]
%VSBUJPONT
$IBOHFT
4VNNBSZ

4VDDFFEFE
'BJMFE

5PUBMTUBUFTSVO
TBMUNZNJOJPOTUBUFTJOHMFGJMFBCTFOUSPPUTPNFEJS
MPDBM

*%SPPUTPNFEJS
'VODUJPOGJMFBCTFOU
3FTVMU5SVF
$PNNFOU3FNPWFEGJMFSPPUTPNFEJS
4UBSUFE
%VSBUJPONT
$IBOHFT

SFNPWFE
SPPUTPNFEJS
4VNNBSZ

4VDDFFEFE
DIBOHFE
'BJMFE

5PUBMTUBUFTSVO
# salt myminion state.single file.directory /root/somedir
MPDBM

*%SPPUTPNFEJS
'VODUJPOGJMFEJSFDUPSZ
3FTVMU5SVF
$PNNFOU%JSFDUPSZSPPUTPNFEJSVQEBUFE
4UBSUFE
%VSBUJPONT
$IBOHFT

SPPUTPNFEJS
/FX%JS
4VNNBSZ

4VDDFFEFE
DIBOHFE
'BJMFE

5PUBMTUBUFTSVO

Using Salt with REST
[ 197 ]
Because one of the states intentionally had an error, you will see three successes overall and
one failure. In the event listener, you will see messages that look like the following:
&WFOUGJSFEBU4VO.BZ

5BHTBMUKPCSFUNZNJOJPO
%BUB
\	@TUBNQ		5	
	BSH	<	GJMFUPVDI		SPPUTPNFEJS	>
	DNE		@SFUVSO	
	GVO		TUBUFTJOHMF	
	GVO@BSHT	<	GJMFUPVDI		SPPUTPNFEJS	>
	JE		NZNJOJPO	
	KJE			
	PVU		IJHITUBUF	
	SFUDPEF	
	SFUVSO	\	GJMF@]SPPUTPNFEJS@]SPPUTPNFEJS@]UPVDI	
\	@@SVO@OVN@@	
	DIBOHFT	\	OFX		SPPUTPNFEJS	^
	DPNNFOU		$SFBUFEFNQUZGJMFSPPUTPNFEJS	
	EVSBUJPO	
	OBNF		SPPUTPNFEJS	
	SFTVMU	5SVF
	TUBSU@UJNF			^^
	UHU		NZNJOJPO	
	UHU@UZQF		HMPC	^
&WFOUGJSFEBU4VO.BZ

5BHTBMUKPCSFUNZNJOJPO
%BUB
\	@TUBNQ		5	
	BSH	<	GJMFEJSFDUPSZ		SPPUTPNFEJS	>
	DNE		@SFUVSO	
	GVO		TUBUFTJOHMF	
	GVO@BSHT	<	GJMFEJSFDUPSZ		SPPUTPNFEJS	>
	JE		NZNJOJPO	
	KJE			
	PVU		IJHITUBUF	
	SFUDPEF	
	SFUVSO	\	GJMF@]SPPUTPNFEJS@]SPPUTPNFEJS@]EJSFDUPSZ	
\	@@SVO@OVN@@	
	DIBOHFT	\^
	DPNNFOU		4QFDJGJFEMPDBUJPOSPPUTPNFEJSFYJTUTBOE
JTBGJMF	
	EVSBUJPO	
	OBNF		SPPUTPNFEJS	
	SFTVMU	'BMTF
	TUBSU@UJNF			^^

Using Salt with REST
[ 198 ]
	UHU		NZNJOJPO	
	UHU@UZQF		HMPC	^
&WFOUGJSFEBU4VO.BZ

5BHTBMUKPCSFUNZNJOJPO
%BUB
\	@TUBNQ		5	
	BSH	<	GJMFBCTFOU		SPPUTPNFEJS	>
	DNE		@SFUVSO	
	GVO		TUBUFTJOHMF	
	GVO@BSHT	<	GJMFBCTFOU		SPPUTPNFEJS	>
	JE		NZNJOJPO	
	KJE			
	PVU		IJHITUBUF	
	SFUDPEF	
	SFUVSO	\	GJMF@]SPPUTPNFEJS@]SPPUTPNFEJS@]BCTFOU	
\	@@SVO@OVN@@	
	DIBOHFT	\	SFNPWFE		SPPUTPNFEJS	^
	DPNNFOU		3FNPWFEGJMFSPPUTPNFEJS	
	EVSBUJPO	
	OBNF		SPPUTPNFEJS	
	SFTVMU	5SVF
	TUBSU@UJNF			^^
	UHU		NZNJOJPO	
	UHU@UZQF		HMPC	^
&WFOUGJSFEBU4VO.BZ

5BHTBMUKPCSFUNZNJOJPO
%BUB
\	@TUBNQ		5	
	BSH	<	GJMFEJSFDUPSZ		SPPUTPNFEJS	>
	DNE		@SFUVSO	
	GVO		TUBUFTJOHMF	
	GVO@BSHT	<	GJMFEJSFDUPSZ		SPPUTPNFEJS	>
	JE		NZNJOJPO	
	KJE			
	PVU		IJHITUBUF	
	SFUDPEF	
	SFUVSO	\	GJMF@]SPPUTPNFEJS@]SPPUTPNFEJS@]EJSFDUPSZ	
\	@@SVO@OVN@@	
	DIBOHFT	\	SPPUTPNFEJS		/FX%JS	^
	DPNNFOU		%JSFDUPSZSPPUTPNFEJSVQEBUFE	
	EVSBUJPO	
	OBNF		SPPUTPNFEJS	
	SFTVMU	5SVF
	TUBSU@UJNF			^^
	UHU		NZNJOJPO	
	UHU@UZQF		HMPC	^

Using Salt with REST
[ 199 ]
You can see the kinds of information we get along the event bus and we can make use of in
our reactors. Let's go ahead and set up something that will fire a webhook every time any
state is run. Because we're specifically interested in the return data, let's start by mapping
that out in the master configuration file. Go ahead; edit the master configuration and add
the following mapping:
SFBDUPS
	TBMUKPCSFU	
TSWSFBDUPSTUBUF@OPUJGZTMT
This will catch any job (the first ) coming from any minion (the second ). Now, let's set up
the reactor itself:
# cat /srv/reactor/state_notify.sls
KJOKB]KTPO
\JGEBUB<	GVO	>TUBSUTXJUI
	TUBUF	^
\SFBDU@UP@TUBUF
\SVOOFSIUUQRVFSZ
<
\
VSMIUUQBMFSUTFYBNQMFDPN
NFUIPE1045
EBUB\\EBUB^^
^
>
^
^
\FOEJG^
We have to be very careful here, since this reactor is going to analyze any job return data
that appears on the bus. This would be especially critical if we were calling out to an
execution module, which itself would create another return event. So, we start off by
making sure we only process the events whose function starts with state (that is,
TUBUFIJHITUBUF, TUBUFUPQ, TUBUFTMT, and so on).
With that out of the way, this reactor is actually very simple: call the IUUQRVFSZ runner
with the URL IUUQBMFSUTFYBNQMFDPN, a 1045 method, and the contents of the
return data as the 1045 data.

Using Salt with REST
[ 200 ]
Notice how we used +40/ instead of YAML here. That's because the return data may
contain characters that wouldn't translate properly in YAML. JSON is a more exacting
serialization method and much less likely to cause syntactical errors.
But let's say that we're only interested in raising an alert when we get an error. Let's add
another condition to our reactor SLS:
# cat /srv/reactor/state_notify.sls
KJOKB]KTPO
\JGEBUB<	GVO	>TUBSUTXJUI
	TUBUF	^
\TFUSFUVSO@LFZEBUB<	SFUVSO	>LFZT
<>^
\TFUSFTVMUEBUB<	SFUVSO	><SFUVSO@LFZ><	SFTVMU	>^
\JGSFTVMU'BMTF^
\SFBDU@UP@TUBUF
\SVOOFSIUUQRVFSZ
<
\
VSMIUUQBMFSUTFYBNQMFDPNJTGBMTF
NFUIPE1045
EBUB\\EBUB^^
^
>
^
^
\FOEJG^
\FOEJG^
We've left in the check to make sure that we're looking at a state, because we don't want
anything else happening if it's not. Beyond that, we've actually done a couple of things here.
First, we had to find out what the return value of the state was. You may recall that the
return dictionary has a single key in it, which looks kind of like this:
GJMF@]SPPUTPNFEJS@]SPPUTPNFEJS@]EJSFDUPSZ
That's going to be pretty much impossible to autodetect, and in our case, it doesn't matter
anyway. But Jinja still does need to know what that key is so that it can access the result
inside the return dictionary. So, we pull it out first and assign it to SFUVSO@LFZ. Then, we
use that to access the rest of the dictionary.
Once we know what the result is, we just do a check on its truth value. Python coders take
note: Jinja requires that our check, even for a boolean, use  instead of JT.

Using Salt with REST
[ 201 ]
Understanding the Salt API
We've spent some time looking at how to send requests, but many users would argue that
receiving requests is just as important, if not more so. Let's take a moment to understand
the Salt API.
What is the Salt API?
Very simply, the Salt API is a REST interface wrapped around Salt. But that doesn't tell you
the whole story. The TBMU command is really just a command-line interface for Salt. In fact,
each of the other Salt commands (TBMUDBMM, TBMUDMPVE, and so on) is really just a way
to access various parts of Salt from the command line.
The Salt API provides a way to access Salt from a different interface: HTTP (or HTTPS,
preferably). Because web protocols are so ubiquitous, the Salt API allows software, written
in any language that has the capability of interacting with web servers, to take advantage of
it.
Setting up the Salt API
So far, this book has assumed that you have a copy of Salt installed, with both a master and
a minion service running. But we're going to take a moment to talk about setting up the Salt
API, since it's somewhat less intuitive than the rest of Salt.
Being a REST interface, the Salt API acts as a web server over and above Salt. But it doesn't
actually provide the server interface itself. It uses other web frameworks to provide those
services and then acts as more of a middleman between them and Salt. The modules that
are supported for this are:
CherryPy
Tornado
WSGI
These modules are set up in the master configuration file. Each has its own set of
configuration parameters and possible dependencies. Let's take a look at each one.

Using Salt with REST
[ 202 ]
CherryPy
CherryPy is a minimalist web framework that is designed to be very Pythonic. Because it is
based around creating web code in the same way that other Python code is created, it is said
to result in code that is much smaller and more quickly developed. It has a mature codebase
and a number of notable users. It has also been the de facto module of the Salt API for some
time.
This module does require that the CherryPy package (usually called QZUIPODIFSSZQZ) be
installed.
The basic setup for CherryPy doesn't involve much configuration. At a minimum, you
should have the following:
SFTU@DIFSSZQZ
QPSU
TTM@DSUFUDQLJUMTDFSUTMPDBMIPTUDSU
TTM@LFZFUDQLJUMTDFSUTMPDBMIPTULFZ
We'll discuss creating certificates in a moment, but first let's talk about configuration in
general. There are a number of configuration parameters available for this module, but we'll
focus on the more common ones here:
QPSU: This is required. It's the port for the Salt API to listen on.
IPTU: Normally, the Salt API listens on all available interfaces (). If you
are in an an environment where you need to provide services only to one
interface, then provide the IP address (that is, ) here.
TTM@DSU: This is the path to your SSL certificate. We'll cover this in a moment.
TTM@LFZ: This is the path to the private key for the SSL certificate. Again, we'll
cover this in a moment.
EFCVH: If you are setting up the Salt API for the first time, setting this to 5SVF can
be very helpful. But once you are up and running, make sure to remove this
option or explicitly set it to 'BMTF.
EJTBCMF@TTM: It is highly recommended that the default value of 'BMTF be used
here. Even when just getting started, self-signed certificates are better than setting
this to 5SVF. Why? Because nothing is as permanent as temporary, and at least self-
signed certificates will remind you each time that you need to get a real set of
certificates in place. Don't be complacent for the sake of learning.

Using Salt with REST
[ 203 ]
SPPU@QSFGJY: Normally, the Salt API will serve from the root path of the server
(that is, IUUQTTBMUBQJFYBNQMFDPN), but if you have several applications
that you're serving from the same host or you just want to be more specific, you
can change this. The default is , but you could set it to FYBNQMFBQJ in order to
serve REST services from IUUQTTBMUBQJFYBNQMFDPNFYBNQMFBQJ, for
example.
XFCIPPL@VSM: If you are using webhooks, they need their own entry point. By
default, this is set to IPPL, which in our example would serve from
IUUQTTBMUBQJFYBNQMFDPNIPPL.
XFCIPPL@EJTBCMF@BVUI: Normally, the Salt API requires authentication, but
this is quite commonly not possible with third-party applications that need to call
it over a webhook. This allows webhooks to not require authentication. We'll go
more in depth on this in a moment.
Tornado
Tornado is a somewhat newer framework that was written by Facebook. It is also newer
than Salt but is quickly becoming the web framework of choice inside Salt itself. In fact, it is
used so much inside Salt that it is now considered a hard dependency for Salt and will be
available on all newer installations.
Tornado doesn't have as many configuration options inside the Salt API as CherryPy. The
ones that are supported (as defined in the CherryPy section) are:
QPSU
TTM@DSU
TTM@LFZ
EFCVH
EJTBCMF@TTM
While the Tornado module doesn't support nearly as much functionality as the CherryPy
module just yet, keep an eye on it; it may become the new de facto Salt API module.
WSGI
WSGI, or Web Server Gateway Interface, is a Python standard, defined in PEP 333. Direct
support for it ships with Python itself, so no external dependencies are required, but this
module is also pretty basic. The only configuration option to worry about here is QPSU.

Using Salt with REST
[ 204 ]
However, this module is useful in that it allows the Salt API to be run under any WSGI-
compliant web server, such as Apache with NPE@XTHJ or Nginx with FastCGI. Because this
module does not provide any sort of SSL-based security, it is recommended that one of
these options be used, with those third-party web servers being properly configured with
the appropriate SSL settings.
Creating SSL certificates
It is highly advisable to use an SSL certificate for the Salt API even if you currently only
plan to use it on a local, secured network. You should probably also purchase a certificate
that is signed by a certificate authority (CA). When you get to this point, the CA will
provide instructions on how to create one using their system. However, for now, we can get
by with a self-signed certificate.
There are a number of guides online for creating self-signed certificates, but finding one that
is easy to understand is somewhat more difficult. The following steps will generate both an
SSL certificate and the key to use it on a Linux system:
First, we'll need to generate the key. Don't worry about the password-just enter one for
now, take note of it, and we'll strip it out in a moment.
# openssl genrsa -des3 -out server.key 2048
(FOFSBUJOH34"QSJWBUFLFZCJUMPOHNPEVMVT


FJT
Y
&OUFSQBTTQISBTFGPSTFSWFSLFZ
7FSJGZJOH&OUFSQBTTQISBTFGPSTFSWFSLFZ
Once you have the key, you need to use it to generate a certificate signing request, or CSR.
You will be asked a number of questions about you that are important if you want a
certificate signed by a CA. On your internal network, it's somewhat less important.
# openssl req -new -key server.key -out server.csr
&OUFSQBTTQISBTFGPSTFSWFSLFZ
:PVBSFBCPVUUPCFBTLFEUPFOUFSJOGPSNBUJPOUIBUXJMMCFJODPSQPSBUFE
JOUPZPVSDFSUJGJDBUFSFRVFTU
8IBUZPVBSFBCPVUUPFOUFSJTXIBUJTDBMMFEB%JTUJOHVJTIFE/BNFPSB%/
5IFSFBSFRVJUFBGFXGJFMETCVUZPVDBOMFBWFTPNFCMBOL
'PSTPNFGJFMETUIFSFXJMMCFBEFGBVMUWBMVF
*GZPVFOUFS		UIFGJFMEXJMMCFMFGUCMBOL

$PVOUSZ/BNF
MFUUFSDPEF<"6>64
4UBUFPS1SPWJODF/BNF
GVMMOBNF<4PNF4UBUF>6UBI

Using Salt with REST
[ 205 ]
-PDBMJUZ/BNF
FHDJUZ<>4BMU-BLF$JUZ
0SHBOJ[BUJPO/BNF
FHDPNQBOZ<*OUFSOFU8JEHJUT1UZ-UE>.Z$PNQBOZ--$
0SHBOJ[BUJPOBM6OJU/BNF
FHTFDUJPO<>
$PNNPO/BNF
FHTFSWFS'2%/PS:063OBNF<>
&NBJM"EESFTT<>NF!FYBNQMFDPN
1MFBTFFOUFSUIFGPMMPXJOH	FYUSB	BUUSJCVUFT
UPCFTFOUXJUIZPVSDFSUJGJDBUFSFRVFTU
"DIBMMFOHFQBTTXPSE<>
"OPQUJPOBMDPNQBOZOBNF<>
At this point, we can go ahead and strip the password from the key.
# cp server.key server.key.org# openssl rsa -in server.key.org -out
server.key
&OUFSQBTTQISBTFGPSTFSWFSLFZPSH
XSJUJOH34"GLFZ
Finally, we'll create a self-signed certificate.
# openssl x509 -req -days 365 -in server.csr -signkey server.key -out
server.crt
4JHOBUVSFPL
TVCKFDU$64456UBI-4BMU-BLF$JUZ0.Z$PNQBOZ
--$FNBJM"EESFTTNF!FYBNQMFDPN
(FUUJOH1SJWBUFLFZ
At this point, you will have four files:
TFSWFSDSU
TFSWFSDTS
TFSWFSLFZ
TFSWFSLFZPSH
Copy TFSWFSDSU to the path specified for TTM@DSU and TFSWFSLFZ to the path specified
for TTM@LFZ.
Configuring authentication
Normally when using Salt, the TBMU command is accessed from the command line as the
root user, an unprivileged user with TVEP access, or as the user that is running the TBMU
NBTUFS daemon. However, it can be configured to allow other user accounts to be used
instead, from other authentication platforms.

Using Salt with REST
[ 206 ]
Because the Salt API is not available from the command line (outside command-line clients
such as XHFU and DVSM), it requires external authentication (often referred to as FBVUI) to
be configured. This is done in the same way as with the other areas of Salt that support
FBVUI.
The external authentication module
The external authentication module is one of the pluggable areas of Salt that does not
currently have many modules and is not likely to in the near future. Why is that so? That is
because so many of the authentication schemes that could be used here are already
supported byPAM, Linux's Pluggable Authentication Module .
For instance, even though LDAP is supported, many administrators find it easier to just use
PAM bindings, since the users they want to authenticate are already available through
PAM's LDAP bindings.
External authentication is set up inside the master configuration file with a block called
FYUFSOBM@BVUI. The module that is used is then declared, followed by users to be
provided by that module and their permissions. Here's an example:
FYUFSOBM@BVUI
QBN
MBSSZ

	!SVOOFS	
	!XIFFM	
EBSSFM
UFTU
	!SVOOFS	
	!XIFFM	
EBSSZM
UFTU
OFUXPSL
	!SVOOFS	
	!XIFFM	
The MBSSZ user has three permissions defined here:
: These are the execution modules that the user has access to. Note that this is a
regular expression.
!SVOOFS: These are the runner modules that the user has access to (in this case,
all runner modules).

Using Salt with REST
[ 207 ]
!XIFFM: These are the wheel modules that the user has access to (in this case, all
wheel modules).
The EBSSFM and EBSSZM users have slightly more restricted access: both have access to any
of the functions inside the UFTU module, and EBSSZM also has access to any of the functions
inside the OFUXPSL module.
Taking your first steps with the Salt API
Once you have the proper settings in the master configuration file, you can start up the Salt
API. For now, go ahead and set EFCVH to 5SVF, and start up the service in the foreground:
# salt-api
[11/May/2015:00:55:22] ENGINE Listening for SIGHUP.
[11/May/2015:00:55:22] ENGINE Listening for SIGTERM.
[11/May/2015:00:55:22] ENGINE Listening for SIGUSR1.
[11/May/2015:00:55:22] ENGINE Bus STARTING
[11/May/2015:00:55:22] ENGINE Started monitor thread
    '_TimeoutMonitor'.
[11/May/2015:00:55:22] ENGINE Started monitor thread
   'Autoreloader'.
[11/May/2015:00:55:23] ENGINE Serving on
    http://0.0.0.0:8080
[11/May/2015:00:55:23] ENGINE Bus STARTED
As you issue commands to the Salt API, you will see information about them printed to this
console. Because the Salt API requires extra headers and often 1045 data, we'll use DVSM for
our examples.
Before we do anything else, we need to obtain a token to issue commands. We will need to
submit the proper credentials to get this token but, once we have it, we can use it to confirm
our authentication with each request. To get a token, type the following:
# curl -ski https://localhost:8080/login \
      -H 'Accept: application/json' \
      -d username='larry' \
      -d password='123pass' \
      -d eauth='pam'
The T in this command will tell the Salt API to be silent (it shouldn't show any superfluous
messages). The J will tell it to show the headers that were returned from the server. We'll
use T in the rest of our examples, but we'll leave out J.

Using Salt with REST
[ 208 ]
The ) option allows us to send a specific header to the remote server. The Salt API requires
that the BQQMJDBUJPOKTPO header be sent; we are interested in receiving responses back
in the JSON format.
The E options are used to send POST data to the server-in this case, in a set of key-value
pairs. The FBVUI parameter explicitly specifies which FBVUI module to use to authenticate
this user. The VTFSOBNF and QBTTXPSE, of course, refer to the credentials that will be used
with that module.
This command will return something that looks like the following (the return data is
formatted here for clarity):
)5510,
$POUFOU-FOHUI
"DDFTT$POUSPM&YQPTF)FBEFST(&51045
7BSZ"DDFQU&ODPEJOH
4FSWFS$IFSSZ1Z
"MMPX(&5)&"%1045
"DDFTT$POUSPM"MMPX$SFEFOUJBMTUSVF
%BUF.PO.BZ(.5
"DDFTT$POUSPM"MMPX0SJHJO
9"VUI5PLFOCCCFEGCFFCDEDD
$POUFOU5ZQFBQQMJDBUJPOKTPO
4FU$PPLJFTFTTJPO@JECCCFEGCFFCDEDD
FYQJSFT.PO.BZ(.51BUI
\SFUVSO<\
QFSNT<!SVOOFS!XIFFM>
TUBSU
UPLFOCCCFEGCFFCDEDD
FYQJSF
VTFSMBSSZ
FBVUIQBN^>^
In the return body, you'll see a token-in this case,
CCCFEGCFFCDEDD. You'll need to pass this exact token in
each of your subsequent requests but, since it's pretty long, we'll just use UPLFO  to
represent it in our examples.

Using Salt with REST
[ 209 ]
Once we have our token, we add it to subsequent requests to the server using the 9"VUI
5PLFO header. For example, to perform a simple UFTUQJOH operation against a minion, we
will issue the following command:
# curl -sk https://localhost:8080/minions -H 'Accept: application/json' -H
'X-Auth-Token: <token>' -d client='local' -d tgt='*' -d fun='test.ping'
\
@MJOLT\KPCT<\ISFGKPCT
^>^
SFUVSO<\
KJE
NJOJPOT<NZNJOJPO>^>^
Note that this command was issued against the NJOJPOT URL. This is
used for executing a number of commands-for example, against an
execution module-over the Salt API.
There are also some extra arguments in the command that we need to look at before we get
to the return data. The first is DMJFOU	MPDBM	, which tells the Salt API to use an execution
module. The others may look more familiar. The UHU argument sets the target-in this case,
all minions. The GVO argument sets the function that will be run-UFTUQJOH in this case.
As with other components of Salt, we can also specify UHU@UZQF to change the UBSHFU type
from HMPC to something else and we can add BSH and LXBSH as needed in order to pass
arguments and keyword arguments to Salt.
There are two pieces of return data in the response body. First, we see @MJOLT, which
contains a reference path that we'll use in a moment. But we also have a return dictionary
that contains a job ID (KJE) and a list of the minions affected by this command.
What we did not get was an actual response from the minions in this command. Remember
that Salt is asynchronous by nature and, while it makes sense for the TBMU command to wait
a few seconds for the response by default, it doesn't make sense to force web clients to wait
the same amount of time.

Using Salt with REST
[ 210 ]
In order to retrieve the response data, we'll need to issue another command that includes
the job ID. This command will be performed using a (&5 method, so be sure there are no E
options in your command:
# curl -sk https://localhost:8080/jobs/20150511024432503750 -H 'Accept:
application/json' -H 'X-Auth-Token: <token>'
\
JOGP<\
'VODUJPOUFTUQJOH
KJE
5BSHFU
5BSHFUUZQFHMPC
6TFSMBSSZ
4UBSU5JNF.BZ
"SHVNFOUT<>
.JOJPOT<NZNJOJPO>
3FTVMU\NZNJOJPO
\SFUVSOUSVF^^^>
SFUVSO<\
NZNJOJPOUSVF^>^
This is more like what we expect to see! You can see a breakdown of all the options used,
the success of the command on each minion, and the return data from the command on
each minion.
Note: The KPCT path in the URL requires a job ID to be passed after it.
Just to make this a little easier to read, let's go ahead and tell the Salt API to return the data
in YAML format. Note how we change the "DDFQU header to BQQMJDBUJPOYZBNM:
# curl -s localhost:8080/jobs/20150511024432503750 -H 'Accept:
application/x-yaml' -H 'X-Auth-Token: <token>'
JOGP
"SHVNFOUT<>
'VODUJPOUFTUQJOH
.JOJPOT
NZNJOJPO
3FTVMU
NZNJOJPO
SFUVSOUSVF
4UBSU5JNF.BZ
5BSHFU		
5BSHFUUZQFHMPC
6TFSMBSSZ

Using Salt with REST
[ 211 ]
KJE		
SFUVSO
NZNJOJPOUSVF
Issuing one-off commands
So far, we've only issued commands using a token. We can in fact issue one-off commands
that authenticate on every single call. This is not the normal operation of the Salt API, but it
can be helpful in troubleshooting issues with custom modules. It may also be helpful for
working with webhooks, which we will cover in a moment.
In order to issue a one-off command, we will use the SVO path in our URL instead of the
NJOJPOT path. The rest of the command contains arguments that you're already familiar
with:
# curl -sk http://localhost:8080/run -H 'Accept: application/json' -d
username='larry' -d password='123pass' -d eauth='pam' -d client='local' -d
tgt='*' -d fun='test.ping'
\SFUVSO<\NZNJOJPOUSVF^>^
Working with webhooks
As mentioned previously, it is possible to use webhooks with the Salt API. Webhooks are
designed to be commands that can be issued over HTTP/HTTPS in a single call-no getting
tokens first. This can be problematic from a number of standpoints.
The first roadblock involves services that make use of tokens or any other authentication
scheme that requires multiple web requests to be made to a server. Since webhooks need to
be able to work in a single shot, using a Salt API token is out of the question.
As you have seen, the Salt API does allow commands to be issued in a single call as long as
all the credentials are passed along. This is okay if the service making the call allows you to
define things such as custom headers and 1045 data. In some situations, this is acceptable,
but some services do not provide that capability.
That leaves us with unauthenticated web requests. This is also doable inside the Salt API,
but the user will have to provide their own authentication mechanism. We'll see how to do
that in a moment.

Using Salt with REST
[ 212 ]
First, let's go ahead and configure the Salt master to accept webhooks in the first place. This
functionality will require the CherryPy module, so make sure you're set up with that. It will
also require a XFCIPPL@VSM value, as specified in the CherryPy section. And, for our
purposes, we'll go ahead and set XFCIPPL@EJTBCMF@BVUI to 5SVF:
SFTU@DIFSSZQZ
QPSU
TTM@DSUFUDQLJUMTDFSUTMPDBMIPTUDSU
TTM@LFZFUDQLJUMTDFSUTMPDBMIPTULFZ
XFCIPPL@VSMIPPL
XFCIPPL@EJTBCMF@BVUI5SVF
This means that all webhook URLs will have a path of IPPL, for example,
IUUQTTBMUBQJFYBNQMFDPNIPPL. We have also disabled authentication for
webhooks, because our examples will not need authentication. If you are working with a
service that does support passing custom headers and 1045 data, go ahead and leave it out,
or explicitly set it to 'BMTF.
You will not need to restart TBMUBQJ explicitly; it will do so on its own when it detects
changes in its configuration.
Go ahead and fire up the event listener. We'll issue a request to the webhook URL using
DVSM:
# curl -sk http://localhost:8080/hook -H 'Accept: application/json' -d
foo=bar
*OUIFFWFOUMJTUFOFSXF	MMTFFUIFGPMMPXJOHNFTTBHF
&WFOUGJSFEBU.PO.BZ

5BHTBMUOFUBQJIPPL
%BUB
\	@TUBNQ		5	
	CPEZ			
	IFBEFST	\	"DDFQU		BQQMJDBUJPOKTPO	
	$POUFOU-FOHUI			
	$POUFOU5ZQF		BQQMJDBUJPOYXXXGPSN
VSMFODPEFE	
	)PTU		MPDBMIPTU	
	3FNPUF"EES			
	6TFS"HFOU		DVSM	^
	QPTU	\	GPP		CBS	^^

Using Salt with REST
[ 213 ]
There are two items of interest here. First of all, the event was tagged as
TBMUOFUBQJIPPL. All events fired from the Salt API will start with TBMUOFUBQJ and
then contain the path that was used in the web request, including XFCIPPL@VSM.
The other item is the 1045 data, which was translated inside Salt to a dictionary. In this
case, the dictionary is very short: there is a key of GPP with a value of CBS.
Reacting with Thorium
Let's go ahead and take a look at a Thorium example using the popular website Slack
(IUUQTTMBDLDPN). Slack provides a communication medium in the form of a chatroom
for members of a team or a company.
While Slack is obviously useful for chatting with team members, some teams also use it to
receive messages from external sources. For instance, you may have GitHub set up to post
messages to Slack when new code is committed, or you may have Twitter set up to post
new messages from specific users.
For now, let's focus on what to do when Slack receives new messages. You can configure
Slack to post outgoing webhooks, which we can then look at with Thorium.
Information about outgoing webhooks in Slack can be found at
IUUQTBQJTMBDLDPNPVUHPJOHXFCIPPLT.
For our example we'll assume that we have a user named DPEFCPU, who will post a
message that says OFX@DPNNJU when it picks up new code. We will add messages from
DPEFCPU to the Thorium register as they come in, and then we will examine them for the
OFX@DPNNJU string. If the string is found, then we'll kick off a request to a Jenkins server
(using their own web API) to run a build.
The URL that we'll use for our Salt API server will be this:
IUUQTTBMUFYBNQMFDPNIPPLTMBDLDPNNJU
This means that when new messages come in, they will have the following tag:
TBMUOFUBQJIPPLTMBDLDPNNJU

Using Salt with REST
[ 214 ]
Slack has two fields that we'll want to look for: VTFS@OBNF and UFYU. However, we'll go
ahead and store the VTFS@JE and the UJNFTUBNQ values in the register too-create
TSWUIPSJVNTMBDLTMT with the following content:
TMBDL
SFHMJTU
BEE
VTFS@OBNF
VTFS@JE
UFYU
UJNFTUBNQ
NBUDITBMUOFUBQJIPPLTMBDLDPNNJU
TUBNQ5SVF
GJMF
TBWF
VTFS@OBNF
DIFDLDPOUBJOT
WBMVFDPEFCPU
UFYU
DIFDLDPOUBJOT
WBMVFOFX@DPNNJU
TUBSU@CVJME
SVOOFSDNE
GVODIUUQRVFSZ
LXBSH
VSMIUUQKFOLJOTFYBNQMFDPNKPCKPC@OBNF CVJME
UPLFOBCDEFG
SFRVJSF
DIFDLVTFS@OBNF
DIFDLUFYU
You can see the TMBDL code block, which adds incoming data to the register. We also have a
code block for VTFS@OBNF, which checks to see whether it contains DPEFCPU, and one for
UFYU, which looks for OFX@DPNNJU. The GJMFTBWF part of the TMBDL code block will let us
keep an eye on things while we're working. This will generate a file here:
WBSDBDIFTBMUNBTUFSUIPSJVNTBWFTTMBDL
Those blocks will always run. However, the TUBSU@CVJME code block will only run if both
the VTFS@OBNF and the CPEZ code blocks find what they're looking for. If DPEFCPU posts a
message that doesn't contain OFX@DPNNJU, it will be ignored.

Using Salt with REST
[ 215 ]
When TUBSU@CVJME does run, it will kick off the IUUQRVFSZ runner to start a new build
job on the Jenkins server. We can test this manually by faking a message from Slack. Use
DVSM to issue the following command:
# curl http://localhost:8080/hook/slack/commit \-d user_name=codebot -d
text=new_commit \
EVTFS@JE6EUJNFTUBNQ
If you check the contents of the save file, you should find something that looks like this:
\TMBDL\WBM<\VTFS@OBNFDPEFCPUUFYUOFX@DPNNJU
VTFS@JE6UJNFTUBNQUJNF
5^>^^
If you check your Jenkins server, you should find that a new build process has started.
Security considerations
Anyone who can access the Salt API port on your Salt master will be able to send messages.
There are a couple of simple means of authenticating requests, neither of which are very
secure, but they are important to take a look at.
First, you can authenticate by URL. If a user knows the correct URL to use, they can send
requests. This type of security is known as security through obscurity: it only remains
secure as long as an easily obtainable piece of information is obscured.
You can also authenticate based on the address of the remote machine that made the
request. This is known as host-based security. Unfortunately, since known proofs of
concept exist to spoof both hostnames and IP addresses, this method is also not terribly
secure.
That leaves us with using the 1045 data to send secure data. If we're using HTTPS, then this
data will be encrypted already, which mitigates man-in-the-middle attacks. In such an
attack, a user watching the communication between two parties is able to obtain enough 
information to imitate one or both of the parties.
If you are able to pass custom 1045 data, this may be enough; just set some secret data
inside the 1045, and watch for it on the Salt master.

Using Salt with REST
[ 216 ]
Let's go ahead and set up a reactor that handles this use case. First, we'll configure reactor
mapping in the master configuration:
SFBDUPS
TBMUOFUBQJIPPLTBNQMFVSM
TSWSFBDUPSXFCIPPL@TJNQMF@QPTUTMT
Then, after restarting the master, we'll set up the reactor SLS itself:
# cat /srv/reactor/webhook_simple_post.sls
\JGEBUB<	QPTU	><	GPP	>	CBS	^
TJNQMF@QPTU@BVUI
DNEGJMFUPVDI
UHUNZNJOJPO
BSH
UNQTJNQMF@QPTU@BVUIUYU
\FMTF^
TJNQMF@QPTU@BVUI@GBJMFE
DNEGJMFUPVDI
UHUNZNJOJPO
BSH
UNQTJNQMF@QPTU@BVUI@GBJMFEUYU
\FOEJG^
This time, we've not only triggered a file to be touched if the authentication succeeded, but
also a different file if authentication failed. Go ahead and try it out:
# curl -s localhost:8080/hook/sample/url -H 'Accept: application/json' -d
foo=bar# salt myminion cmd.run 'ls -l /tmp/simple_post_auth*'
NZNJOJPO
SXSSSPPUSPPU.BZUNQTJNQMF@QPTU@BVUIUYU
DVSMTMPDBMIPTUIPPLTBNQMFVSM)	"DDFQUBQQMJDBUJPOKTPO	E
GPPCB[
TBMUNZNJOJPODNESVO	MTMUNQTJNQMF@QPTU@BVUI	
NZNJOJPO
SXSSSPPUSPPU.BZUNQTJNQMF@QPTU@BVUIUYU
SXSSSPPUSPPU.BZUNQTJNQMF@QPTU@BVUI@GBJMFEUYU
More complex authentication
It may be that you're working with something more complex than simply passing through
1045 arguments. In this case, you'll probably need something more powerful than Jinja
mixed with YAML. There are three Python renderers that ship with Salt and, as long as it
doesn't take very long, writing a reactor using one of them may be appropriate for your
needs.

Using Salt with REST
[ 217 ]
Because the focus of this book is Salt and not Python, we won't go into a lot of detail here.
But I'll include a Python version of the aforementioned reactor to get the Python users
among you started:
QZ
EFGSVO

JGEBUBHFU
	QPTU	\^HFU
	GPP				CBS	
SFUVSO\	TJNQMF@QPTU@BVUI	\
	DNEGJMFUPVDI	<
\	UHU		NZNJOJPO	^
\	BSH	<	UNQTJNQMF@QPTU@BVUIUYU	>^
>
^
FMTF
SFUVSO\	TJNQMF@QPTU@BVUI@GBJMFE	\
	DNEGJMFUPVDI	<
\	UHU		NZNJOJPO	^
\	BSH	<	UNQTJNQMF@QPTU@BVUI@GBJMFEUYU	>^
>
^
Summary
Salt provides some very powerful capabilities for interacting with the REST interfaces, both
as a client and as a server. When combined with the reactor system, especially, these
capabilities change the scope of autonomous operations from being local to the internal
infrastructure to being usable with the vast majority of third-party services available today.
Now that we've spent some time looking at some of the more traditional aspects of Salt, it's
time to get really serious. The next chapter will focus on Salt's new transmission protocol,
Reliable Asynchronous Event Transport, and Salt's pure TCP transport mechanism.

9
Understanding the RAET and
TCP Transports
You may have heard of SaltStack's Reliable Asynchronous Event Transport (RAET)
protocol. However, there's a very good chance you haven't heard much about it. To the
average user, RAET may not seem like much. Salt commands haven't changed, output from
the commands hasn't changed, and you certainly don't need to update your SLS files. In
fact, if you've enabled RAET but haven't changed your workflow, you probably haven't
noticed much of anything, which is by design. So what's the big deal? In fact, RAET
introduces some interesting concepts that are new to the configuration-management game.
We will also take a look at the TCP transport that now ships with Salt. ZeroMQ already uses
TCP, but this transport uses the protocol directly. Its introduction also came with the
addition of a package to Salt called Tornado, which is an asynchronous programming
library.
In this chapter, we'll cover the following topics:
RAET versus ZeroMQ
Understanding flow-based programming
Using estates, roads, and lanes
Understanding asynchronous programming
Using the TCP transport

Understanding the RAET and TCP Transports
[ 219 ]
Comparing RAET and ZeroMQ
To understand Salt, it really does help if we understand what ZeroMQ is and why Salt was
originally based on it. Having a solid foundation of knowledge when it comes to ZeroMQ
will also help in getting a handle on RAET. It will also help if we understand HTTP and
why Salt doesn't make use of it.
It's important to understand that when ZeroMQ came out, there was nothing like it. There
are alternatives now, such as OBOPNTH, but ZeroMQ was the first, and it came on the scene
just in time for Salt.
Many of the design principles behind Salt are inspired by previously existing projects, some
of which were in use by Salt's creator at the time of its conception. However, it was not
simply a copy of a bunch of other projects hacked together. If existing projects did what Salt
was designed to do, then Salt would not have been created. The corollary here is that a
number of design principles were used because what was out there wasn't cutting it.
Starting with HTTP
A very common technology in distributed management systems is HTTP, which makes
sense from a number of points of view. It's ubiquitous and easy to understand and use.
However, its functionality is somewhat limited: you request and are served a document.
Doing so was originally a static process. Features were quickly added to support dynamic
content, but the basic premise still stands.
Security was an afterthought with HTTP and, in the beginning, competing standards
existed to supply it. In the end, HTTPS won out, but the numerous iterations of SSL,
followed by TLS, that have presented themselves only highlight a number of limitations
with the protocol. However, we will focus on HTTP and HTTPS together as a single
concept.
A configuration-management system based on HTTP necessarily requires a web server to
be in the mix. Maintaining a web server for clients opens up innumerable potential security
risks and introduces a lot of overhead, so having clients connect to a single server instead
makes a whole lot more sense.

Understanding the RAET and TCP Transports
[ 220 ]
This means that the web server must be able to handle a lot of load. Also, the more clients
that connect to it, the more concurrent connections it must handle. For instance, Puppet is
barely usable when configured to use WEBrick because WEBrick wasn't designed to handle
much traffic. Switching to Mongrel or Apache significantly improves the performance of
Puppet.
However, no matter how performant a web server is, it is still limited to pull-based
connections. It is not possible to have the server initiate a connection to the client with only
a web server. This is why web-based configuration-management systems are frequently set
to check with their master on a regular basis, such as every 30 minutes.
SSH â€“ the old favorite
Possibly the world's most common means of managing a Unix or Linux server is SSH. This
is because Telnet was the king for several decades, whereas SSH is a secure tunneling
mechanism. Its default application is Telnet. For a very long time, SSH-based management
was an entirely manual process. However, as popular as it was, it was only natural that it
would eventually acquire automation tools.
The earliest of these tools was a simple, shell-based loop to perform one-off commands.
Other tools that could perform SSH-based tasks in parallel or maintain multiple concurrent
SSH windows were introduced and gained popularity. Soon, entire frameworks were built
around SSH-based management.
However, there are architectural similarities between HTTP and SSH that demonstrate
several limitations of an SSH-based management system. Like HTTP, a server can accept
multiple simultaneous connections. However, a client can only connect to one server at
once. In order to connect to multiple machines in parallel, multiple clients must also be
started at once, and the number of parallel clients is limited by the resources on the client
machine.
Also, like HTTP, an SSH connection requires a server to exist on the other end. However, it
is extremely uncommon for machines that need to be configured to call home to a central
server over SSH. Instead, each machine that is to be configured over SSH will generally be
running an SSH server itself. This carries exactly the same risks as running an HTTP server
on each client, and the risk is more if the SSH user has root access to the machine. This will
probably be the case if the goal of the SSH connection is to configure the machine.

Understanding the RAET and TCP Transports
[ 221 ]
Using ZeroMQ
ZeroMQ was never designed for configuration management. It was designed as a faster,
simpler replacement for the (AMQPAdvanced Message Queuing Protocol (AMQP), which
its author had also designed. However, message queues work well with large-scale
environments and tend to be very performant.
Message queues are different from HTTP and SSH because rather than a single client
initiating a single connection to a single server, multiple clients can subscribe to one or more
servers (message queues) and watch for messages that apply to them. The connection is
very lightweight. As it is persistent, the overhead of successive client/server handshakes is
eliminated.
Let's look at some analogies that demonstrate the architectural differences between HTTP,
SSH, and ZeroMQ. A team of workers is headed by a manager, who will assign tasks to
workers on a regular basis.
With an HTTP-based architecture, each worker will call the manager on the phone on a
regular basis to check for new tasks. If the manager is already busy talking to other workers
when another worker calls in, then the other worker must continue to try calling until the
manager's phone is no longer busy.
With an SSH-based architecture, the manager will call each worker when they have a task
to assign. When there are few tasks to be performed, this will result in less work for
everybody involved. It also allows the manager to initiate tasks as soon as they are received
rather than waiting for the intended recipient to check in.
In a ZeroMQ-based architecture, each worker will watch a TV channel that broadcasts the
tasks as they become available. When a worker sees a task that applies to them, they
perform it and then call the manager with the results. Any TV executive can tell you that
broadcasting messages in this manner is a far more scalable way to reach an intended
audience, especially when the potential audience is quite large.
Salt actually makes use of two message queues. Port 4505 is the port that workers subscribe
to (or the channel that they watch) in order to receive messages from their manager. Port
4506 is a second queue that the manager watches to receive the result data back.
ZeroMQ and security
ZeroMQ did not originally implement any kind of security. This is because it was meant to
be run inside environments that were already secure on their own. Without the encryption
overhead, ZeroMQ was able to achieve significant performance.

Understanding the RAET and TCP Transports
[ 222 ]
However, Salt was not designed to be only run inside secure networks. It was designed to
run on networks where not every user was necessarily trusted. Because of this, it was
crucial that Salt messages be encrypted. Even today, it is not possible for Salt to
communicate on a nonencrypted channel.
However, as ZeroMQ did not have security built in at that time, a secure layer was built on
top of it, that is, inside Salt. This layer was based on the SSH standard, which is widely 
trusted worldwide. SaltStack has never used its own encryption libraries; it has always
made use of other known, vetted libraries, such as PyCrypto. Although there have been a
couple of hiccups in the past with applying these libraries, Salt's encryption layer is now
regularly audited by third-party corporations that have a vested interest in it running
securely.
The need for RAET
Salt is now used in a number of extremely large clusters. It is not uncommon for groups of
15,000 servers or more to be running on a single Salt master. As the scale of these
infrastructures increased, it became evident that a transport was needed that was designed
specifically for the needs of a tool such as Salt.
One the other hand, a ZeroMQ bus is similar to having all the workers watch a single TV
channel; RAET is more like having each worker watching their own, dedicated TV channel
that nobody else can watch.
ZeroMQ, similar to most popular Internet protocols (including HTTP and SSH), is based on
TCP, which is known for its reliability. RAET is based on UDP, which is known for its
unreliability. While TCP is commonly encrypted, UDP is nearly impossible to encrypt. So
why use it?
The biggest advantage that UDP has is speed. As it doesn't bother with things such as
handshakes and always makes sure that network packets reach their destination, the ones
that do make it tend to be very fast.
RAET itself provides the missing components, such as handshakes and reliability. Also,
rather than using the classic encryption libraries that HTTPS and SSH have traditionally
been based on, RAET uses a type of elliptic curve-based cryptography (ECC) called
Curve25519. This algorithm is considered by many to be the most secure today and is now
the default encryption method for OpenSSH. RAET doesn't manage the encryption either; it
lets a library called MJCTPEJVN (no relation to SaltStack) handle all the work.

Understanding the RAET and TCP Transports
[ 223 ]
The architecture of RAET is far less known than its older siblings-HTTP, SSH, and ZeroMQ-
but before we get into that, let's cover some of the concepts that RAET is based on.
Flow-based programming
We've talked about just a few of the differences between RAET and ZeroMQ. However, to
really understand the benefits of RAET and how it affects you, it really helps to get at least a
basic handle on flow-based programming (FBP), which RAET is designed on.
The pieces of the puzzle
All this may sound a little intimidating, but don't worry. We'll break it into smaller
components first and then look at how these pieces fit together. FBP is based on three
concepts:
Black boxes
Shared storage
Concurrent scheduling
These three types of components fit together to form a framework that can manage tasks
very quickly and efficiently. Let's take a look at them individually.
Black boxes
The first puzzle piece is the black box. More accurately, black boxes really are the puzzle
pieces themselves: they are organized by the scheduler and connected with shared storage.
Most simply, a black box is one thing that does another thing. That's not very specific, so
let's go into more detail.
A black box is a simple construct that performs an action. This action can be as complex as
necessary, but it's often better to keep it simple. There should be a simple interface to start
using the black box and a simple interface to obtain the result of its work.

Understanding the RAET and TCP Transports
[ 224 ]
An everyday example of a black box is a toaster. It has a simple interface, in which slices of
bread are inserted into slots, a timer is set to control how dark the toast is, and a lever is
pressed to warm up the heating coils; this begins the toasting process. Once the process is
complete, the timer runs out, the toast is ejected, and the heating coils are cooled.
The cook who uses this black box may also employ other black boxes to complete a larger
task. To prepare breakfast, one may employ the toaster black box and a blender black box as
well to create a smoothie, or a frying pan black box, which in turn makes use of a stove
black box. This requires more frequent input from the user.
Shared storage
Shared storage is something that just about every professional programmer and systems
administrator has dealt with in their career: a database. As the name implies, all the black
boxes have access to it in some way.
Going back to our breakfast example, we may refer to the refrigerator as our shared storage.
It stores juice, fruits, eggs, butter, and, with some users, even the bread used to make the
toast.
We may even employ a slightly more complex shared-storage solution by adding pantry
storage. You can even refer to it as archival storage, in which the user processes jams, jellies,
fruits, pickles, and so on in a boiling-water canner and then stores them until they are
needed. When they are retrieved from the archive, they are moved to the refrigerator,
where they are accessed more frequently.

Understanding the RAET and TCP Transports
[ 225 ]
Concurrent scheduling
It's nice to have shared storage to keep our food and black boxes to cook our food, but these
items are useless without something or someone to combine their functionalities. The food
cannot be cooked until it is moved from storage to black box, and doing so in the right way
at the right time is critical to the success of our breakfast. Simply putting an egg in a frying-
pan black box without properly opening it will not result in an edible product; putting the
egg in the toaster black box is likely to cause a fire.
In order to tie these elements together, we need a scheduler. The scheduler will determine
when each process needs to happen and which black boxes get which pieces of data.
In our breakfast example, the cook is the scheduler. The cook will remove the bread from
the pantry or refrigerator, unpack it from its bag, move it to the toaster, and start the timer.
The cook will also take the eggs out of the refrigerator, unpack them by cracking them open
in the frying pan, and occasionally provide input to properly cook them.
The cook, as the scheduler, will also decide when it is best to start each action. A well-tuned
scheduler (or a seasoned cook) will be able to use historical data to help make these
decisions.
Both the eggs and the toast highlight a couple of important aspects of the kinds of
processing data centers regularly deal with. There are tools to help with the processing of
the eggs, but it is still largely a manual process. However, once the data (bread) and
parameters (the darkness setting) are given to the toaster, the process of toasting the bread
happens without any further user input.

Understanding the RAET and TCP Transports
[ 226 ]
We've talked about scheduling, but what is concurrency? It's actually best to explain both
parallel processing and concurrent processing because understanding one will help us
understand the other. It's also important to understand both because they are so easy to
confuse, as the terms are often used interchangeably.
When two or more processes happen literally at the same time, they happen in parallel. For
instance, when two slices of bread are inserted into the toaster at the same time, they will be
toasted in parallel.
Concurrency looks as if two or more processes are happening at the same time, but in fact,
each step of each process will be performed individually and sequentially. As computers
are so fast and the work is hidden from the user, concurrent processing often looks like
parallel processing.
Let's go back to our kitchen example: our cook is making breakfast for his whole family. He
owns a large toaster and is able to process a dozen slices of bread at once. He also owns a
griddle and is able to cook several eggs at once. He also owns a cutting board and knife,
and he plans to add fresh fruits to the breakfast.
The cook starts by turning on the griddle. While it is warming up, he puts bread in the
toaster and turns it on. Then, he pulls a melon out and sets it on the cutting board. He goes
to the griddle and cracks a few eggs in it. Then, he goes back to the cutting board and slices
some of the melon. He goes back to the griddle and flips the eggs. Then he finishes cutting
the melon.
The toast pops up just as he finishes cutting the melon. The eggs are just about finished as
well. He grabs some plates, puts toast on each, uses the spatula to distribute the eggs
between them, and finishes up by distributing the slices of melon between them.
The breakfast in this example has in fact been prepared both in parallel and concurrently.
The cook concurrently switched between processing the eggs and processing the melon. He
also had the toast processing in parallel by another process.
Driving with RAET
We've talked about the concepts behind RAET, but now it's time to get down to business.
Let's go ahead and enable it; we can then get started.

Understanding the RAET and TCP Transports
[ 227 ]
Configuring RAET
Setting up Salt to use RAET is actually not a big deal. In the master and the minion
configuration files, set USBOTQPSU to SBFU:
USBOTQPSUSBFU
As of version 2016.3, this is still set to [FSPNR by default, but this may change in future
releases.
There are some other changes that come with RAET. As RAET uses a different encryption
scheme, it also has its own set of keys. The master used to store its minions' public keys in
the following directories (inside FUDTBMUQLJNBTUFS):
NJOJPOT@QSF: This specifies the minions that have identified themselves to the
master but whom the master has not yet accepted
NJOJPOT/: This denotes the minions that have been identified to and accepted by
the master
NJOJPOT@SFKFDUFE: This specifies the minions that have identified themselves
to the master but with whom the master has explicitly disallowed communication
Instead, RAET uses the following names for these directories respectively:
QFOEJOH
BDDFQUFE
SFKFDUFE
As ZeroMQ-based Salt uses RSA encryption, its keys look like this:
#&(*/16#-*$,&:
.**$*K"/#HLRILJ(X#"2&'""0$"H".**$$H,$"H&"Y-DV3;MCS"F"R3Z
Q+F#1M")-O6%D75Y*00+$U:CK['OHW7P19Q$*NE#C3'#Q,#(ONC,V
89CBFZNP%PCC%Q:4K(%'PSG&%W)GEKPW9WG$&+GJSSG*P:F&*E-07T8
,NQB)(JFD&MJUNEEGH"BQ(RER;Y[S.75BYW1JEN(0UJ:0Y;YIK9G+
Z&9L$+WBYC#0PSB&UC-$U);*B-B$;E0L%PQ)#'K1;'FH+(E
-1'081-R.N0ZF#N$J$Z8*T'X&V[)(I$.POPS"*ITS.NY4SYNS2V"
)+*B#"WNGK($VHHLEC"KFWW1%L5H:WRXEF*$.3"/)47:ER9UGMQT"'5,
-IVG0C*T%G':-KGD(%YL'2,W%/6MTD641Y9%.WTHZ6#DZ8
D1+ZY3-X/$$&#,1U[W#G%DBSG,N3%TDTR171K#G9%&+K[EE$
HE75'KI%DP%JT-S[)[TCEL1Z5'&V.MPZO%H5Y2[$"[)CQ/7MD;0SSW[P
V;ODJIO69J7SU6R061Q[+I7'[W:.CY+/T;FCIXK%(P8QT(R1,2E+U
6K.%*+9UNLEZXUIP3-D$"X&""2
&/%16#-*$,&:

Understanding the RAET and TCP Transports
[ 228 ]
As RAET-based Salt uses Curve25519, its keys look like this:
WFSJGZ!CCDDCCEBCEGFBFEGFBDGGECDCGQVC!
GCFBCFFFBGECEGGDCEFBDDFNJOJPO@JENZ
NJOJPO
However, beyond these differences, using Salt with RAET is not different, from a user
perspective, from using it with ZeroMQ.
While the RAET libraries ship with Salt, the dependencies needed to run it
may not, depending on your distribution. For instance, in Arch Linux,
there are two different Salt packages:
TBMU[NR
TBMUSBFU
Each pulls its own set of dependencies. As they both provide many of the
same files, they cannot both be installed at once.
Once the master and minion(s) have been properly configured, Salt should function as
normal. Go ahead and try out a couple of commands:
# salt myminion test.ping
NZNJOJPO
5SVF
TBMUNZNJOJPOTUBUVTMPBEBWH
NZNJOJPO

NJO

NJO

NJO

TBMUNZNJOJPOTUBUVTEJTLVTBHFEFWTED
NZNJOJPO

EFWTED

BWBJMBCMF

UPUBM


Understanding the RAET and TCP Transports
[ 229 ]
The RAET architecture
RAET has a slightly different internal architecture from what most people are used to. On
the surface, RAET is just a peer-to-peer connection. Salt uses it in a client/server model but,
outside Salt, it can be used for any machine to talk to any other machine. However, before
we talk about how machines address each other, we should get some terminology out of the
way.
The basics
Hosts on the RAET bus are called estates. Each estate has one or more yards. Estates are
connected to each other via roads or lanes. Lanes are used to connect estates on the same
physical machine, whereas roads are used to connect estates on separate physical machines.
On Unix and Linux, lanes are actually an abstraction of Unix domain sockets (UXD),
whereas on Windows, lanes are an abstraction of mail slots. Also, on all platforms, roads are
an abstraction of a UDP Internet connection:
When one machine needs to send a message to another machine, it needs to package this
message and address it to the recipient. The address needs to contain the name of the estate
and the yard within that estate that needs to receive the message.

Understanding the RAET and TCP Transports
[ 230 ]
A yard is like a process inside the host (but not an actual, full-fledged Unix or Windows
process). A host can have multiple yards, each of which is uniquely addressable. This
introduces an important change internal to Salt.
Traditionally, Salt commands have been treated as one-off actions. When a command is sent
to a minion, a process will be fired off to execute it. Even if this process were to stay open
indefinitely, there would be no way to guarantee that any subsequent command will be
able to make use of this same process. However, since a process in RAET must be addressed
specifically (as a yard), applications can make continued use of them:
The RAET scheduler
Like ZeroMQ, RAET is based on the concept of queues, which it calls stacks. However, the
stacks in RAET work a bit differently from what one may expect.
When the master issues a command to a minion in RAET, no work actually happens right
away; the command is placed in the appropriate stack to wait for processing purposes.
RAET's concurrent scheduler will loop through each stack and, as it finds tasks to be
completed, it will process them as appropriate. This gives RAET stacks the property of
being nonblocking. When multiple tasks come in, they are immediately sent to the
appropriate stack and then processed concurrently by the scheduler.

Understanding the RAET and TCP Transports
[ 231 ]
Estates and yards
We've already mentioned that hosts are referred to as estates. Let's take a closer look at the
relationships that are going on inside an estate.
An estate contains yards. While estates are connected to each other via roads, yards are
connected to each other via lanes.
We could say that each yard is a process, but that's a little misleading because individual
yards do not correlate directly to individual processes on the OS level. Rather than being
parallel processes, yards are concurrent threads managed by the estate.
Think back to our breakfast scenario earlier in the chapter. Our cook is a scheduler: he
manages all of the tasks that need to be done. The appliances that he is cooking with
correlate directly to yards. He flips an egg in one yard, moves to another yard, and flips
another egg. Then, he moves to the next yard and cuts a slice of melon. You may also recall
that our refrigerator is a shared-storage mechanism, which each of the yards has access to
and can make use of. Eggs are a type of data that is retrieved from the shared storage,
processed, and, if there are leftovers, sent back to the storage device.
Looking at asynchronous programming
Considering that asynchronous is in RAET's name itself, it should come as no surprise that
RAET is a type of asynchronous protocol. TCP is not an asynchronous protocol by nature,
but it can be used for asynchronous tasks. UDP isn't exactly asynchronous either; it uses
more of a cfire and forgetd methodology.
So what is asynchronous programming? First of all, let's examine what synchronous
programming is. Classical programming is synchronous by its very nature. A program
issues a command and then waits for it to complete before issuing the next command.
In terms of more complex software, a program will start with a series of commands, which
will usually end up calling subroutines or functions, which themselves may call more
subroutines and functions. Each time a piece of code calls another piece of code, it must
wait until that code completes before moving onto its next step.
This kind of code is limited by blocking. This means that if you have a number of functions
that need to run, each one is blocked by the one before it. If a queue of tasks starts to build
up, then each will be blocked by the one before it. And if one of those tasks takes a long
time, it will negatively impact the queue.

Understanding the RAET and TCP Transports
[ 232 ]
You've probably seen this effect in real life. When going to the grocery store, the bank, or
the concert hall, there is often a queue of people ahead of you. Grocery stores solve this by
having another cashier open up another register. Banks have tellers open new windows.
And concertsewell, nobody's perfect, I suppose.
Opening up new registers and teller windows is a type of parallel processing. The results
may look asynchronous, but it's a different solution. In fact, asynchronous programming
can be used in conjunction with parallel processing.
Now, before we go any further, I want to clarify something. It may start to look as if
concurrent and asynchronous processing are the same, but they are not. Remember our
cook earlier in the chapter? Let's go ahead and expand our analogy further.
Cooks in a restaurant
Our example of concurrency involved a cook that was concurrently processing multiple
tasks. If there were two cooks, both handling the same tasks in the same way, they would
be concurrently processing their tasks in parallel. But there is an essential component that is
missing from this equation: how do the cooks know what to cook and when to cook it in the
first place?
Let's introduce a new actor: the waiter or waitress, whom we'll conveniently refer to as the
server. The server processes requests from guests and delivers them to the kitchen. In our
example, they'll attach the order to an order wheel. This is a device used by many
restaurants as a means of managing orders coming in. The cooks will routinely check the
order wheel for new orders and process them as new ones come in.
Once a cook receives an order, they will prepare it as requested and then place the order
under a heat lamp for the server to pick up. The server will then deliver the order to the
customer.
The order wheel is a type of queuing system. The algorithm in this example is a first-come-
first-serve model: the task will be processed by the first cook who accepts it. If the server
directly asked the cooks to prepare the order, then the queuing system is removed from the
equation.
In either version of this example, the server (who may or may not be assisted by the queue)
represents the asynchronous aspect of a program. One scheduler (in this case, the server)
requests a task to be performed by a separate entity (the cooks) and then does other things
while waiting for the task to complete. Once the task is completed, the cooks notify the
server by placing the completed dish under the heat lamp.

Understanding the RAET and TCP Transports
[ 233 ]
Examining the TCP transport
The previous example illustrates how a complex set of processes can make use of a number
of different concepts at once. If you consider that each cook must follow a set of tasks (a
recipe) in the correct order in order to produce a dish, our example makes use of
asynchronous, parallel, concurrent, and synchronous task management, combined with
queue-based task distribution, all flowing together harmoniously.
The TCP transport makes use of most of these concepts, though the concurrent nature of
RAET has been replaced with an asynchronous model.
The TCP transport makes use of a Python library called Tornado, which is an asynchronous
networking library. With the introduction of the TCP transport, the ZeroMQ transport was
also retrofitted to be managed by the Tornado library. To sum up, this means that while
SSH is synchronous and RAET is concurrent, ZeroMQ and TCP are both asynchronous.
However, while the ZeroMQ transport uses ZeroMQ to handle queuing and
communication, the TCP transport handles those things by itself.
Under the hood, Tornado provides the TCP transport with a type of function called a
coroutine. A collection of coroutines is defined to accept a certain type of data and return an
object (called a future), which can be used to check on that coroutine's status. Coroutines
have the ability to call other coroutines ad nauseam to ensure that all of their tasks can be
accomplished without blocking any of the other tasks.
These coroutines are all managed with an event loop. Coroutines may run forever, meaning
they continue waiting for tasks until halted by some outside force, or they may run until
complete, meaning they will stop as soon as their task is complete. Both types of coroutines
are used throughout the TCP transport and inside Salt itself. The event loop manages the
coroutines themselves.
Using the TCP transport
Unlike RAET, which uses a different type of keys, the TCP transport uses the same keys as
the ZeroMQ transport. This means that you can switch from ZeroMQ to TCP and back as
you wish, bearing in mind that the master and minions must also be restarted to pick up the
change.

Understanding the RAET and TCP Transports
[ 234 ]
Both master and minion use the same setting to switch to the TCP transport:
USBOTQPSUUDQ
However, as of the Carbon release of Salt, minions may instead be configured to use a
setting of EFUFDU:
USBOTQPSUEFUFDU
When a minion is configured this way, it will check with the master while starting up to see
which transport it is configured to use. It will first check to see whether the master is
configured to use [FSPNR. If it can't connect that way, then it will attempt to connect using
UDQ. If that also fails, then the minion will behave as if the master is unavailable.
Whether you expect to use UDQ in the future or stick with [FSPNR, I recommend you give
the EFUFDU option a trial run and keep it in place if you feel comfortable with it.
Summary
RAET is a very powerful protocol that extends the functionality of Salt in unexpected ways.
We discussed other options for communication management used in other frameworks. We
also looked at why they aren't used in Salt.
Next, we'll discuss some of the many techniques that are built in to Salt to handle large-
scale infrastructures and a few techniques that can be added to the mix.

10
Strategies for Scaling
Up until now, we've generally covered topics that are relevant to any size of infrastructure,
big or small. But when an infrastructure starts getting really big, there is an entire class of
strategies that must be considered in order to handle the load. In this chapter, we'll cover
the following topics:
Building a hierarchy with TZOEJDT
Using multiple masters
Testing load with NJOJPOTXBSNQZ
Using external filesystems
Managing the master using the wheel system
Using reactors and Thorium to manage the master
All about syndication
In order to understand what syndication is all about in Salt, let's step back a few years to
when an infrastructure's size did not often go beyond a few dozen nodes. Server
management software didn't really need to handle a lot of connections, and often didn't.
Different folks, different strokes
Puppet was one of the earlier configuration-management platforms that really started
addressing scale. Since Puppet uses an HTTP-based methodology, early documentation
discussed the pros and cons and the various configurations of different web servers.

Strategies for Scaling
[ 236 ]
As we discussed in the previous chapter, Salt doesn't use HTTP and so needs to employ
different strategies to address scale. On its own, some users report using Salt to manage
over 10,000 machines. However, not everybody has the kind of beefy hardware that those
users have available for their masters.
The syndic system was designed for infrastructures where the master was not expected to
be powerful enough to handle the load from all the minions that it needed to communicate
with. Rather than using a classic architecture, where one master communicates directly with
all its minions, the syndic system allows a master to communicate directly with one or more
other masters in addition to its own minions. Each master in the hierarchy may also
communicate directly with one or more other masters in addition to its own minions. There
is no imposed limit to the depth of such a hierarchy.
While this concept was designed to mitigate the load on the master, it is now much more
commonly used as an organizational technique. For instance, an organization may maintain
data centers in Houston, New York, London, Dubai, Singapore, Tokyo, and Sydney. Each
data center may have a single master, which in turn is connected to another centralized
master.
When dealing with Salt traffic over the Internet, security is going to be a concern. Limiting
minions to only communicate with a master in their own data center will also simplify
firewall configuration, as the centralized master will only need to worry about accepting
connections from a small set of IP addresses. Additionally, administrators can perform tasks
in each data center by connecting to a single master only.
No need for micromanaging
Each master in a syndicated Salt infrastructure knows only about the minions and masters
that directly report to it. Say we have master A, which has a group of minions, plus two
syndics called master B and master C. Master A is able to issue commands that are
propagated down the line to the minions belonging to master B and master C, but it doesn't
need to store copies of the public keys for those minions or even know that they exist.
This is because the syndic system provides little more than a pass-through connection.
Commands that are published on master A's bus will be sent to master B and master C, each
of which will republish those commands to their own buses. As return data is received by
master B and master C, it will be consolidated and aggregated back up to master A.

Strategies for Scaling
[ 237 ]
This also means that minions that request other resources from their master, such as files or
pillar data, will only be able to receive that data from the master that they report directly to.
A syndic will not be able obtain data from its own master to be served to a minion that
reports to it. Fortunately, this is not a problem if all that data is provided by an external
filesystem or an external pillar, as discussed later in this chapter in Incorporating external data
sources.
Configuring syndication
Each master in the infrastructure must, of course, be running the TBMUNBTUFS daemon.
Masters that report to an upper-level master must also be running the TBMUTZOEJD
daemon (which may need to be installed separately, depending on your operating system).
Each syndic must also be configured to know which master they report to. In the master
configuration on the syndic, the host or IP address of the master is configured as a
TZOEJD@NBTUFS:
TZOEJD@NBTUFS
If necessary, you can also change the TZOEJD@NBTUFS@QPSU, which defaults to the value of
the NBTUFS@QPSU option ():
TZOEJD@NBTUFS@QPSU
The master that presides over the syndic will treat it like any other minion, meaning it will
need to accept that syndic's public key, which will show up as a minion key. However, it
will also need to know that it will be controlling syndics. To know that it will be sending
orders to other syndics (who are masters themselves), the PSEFS@NBTUFST option needs to
be 5SVF in the configuration of the master at the upper level:
PSEFS@NBTUFST5SVF
High availability with multiple masters
The traditional Salt setup only involves a single master with multiple minions. This is fine
for a number of smaller shops, and even some of the bigger ones, but definitely not for
everybody. High availability has become increasingly crucial in the modern infrastructure,
and Salt is a part of that.

Strategies for Scaling
[ 238 ]
Built-in high-availability configuration
Salt does have some built-in configuration to handle multiple masters, but it's smaller and
simpler than what one might expect. Interestingly, the actual Salt configuration is all on the
minion; the masters don't have any configuration inside Salt itself. We'll talk about that in a
moment.
First, let's talk about minion configuration. Normally, minions have a single master defined
in their configuration file:
NBTUFS
However, a list of masters may be defined instead:
NBTUFS


There is no limit imposed to how many masters may be declared for a single minion.
However, once the list of masters has changed, the minion must be restarted in order to
pick up the new configuration.
The masters themselves do not need to be aware of other masters in the infrastructure.
However, all masters should be identical in every way possible. They must all share the
same public/private key pair and should have a synchronized copy of the minions' public
keys. Any other files in UIFFUDTBMU or TSW directories must also be identical.
The process of synchronizing files among the masters is not currently supported by Salt; it
is up to the administrators to define and implement their own workflow to accomplish that.
However, most of the work can be accomplished using other subsystems of Salt, as we'll
discuss in a moment.
Old-school high availability
Before Salt got the ability to set multiple masters inside a single minion, it was still possible
to create a multi-master setup; it just required a little work outside Salt.
When a minion is pointed to a master, the address used may be either an IP address (that is,
) or a DNS-resolvable hostname address (that is, TBMUNBTUFS or
TBMUNBTUFSFYBNQMFDPN). Techniques to map either type of address to multiple servers
have been around for years, and some even for decades.

Strategies for Scaling
[ 239 ]
The round-robin DNS
DNS may be the oldest method for mapping a single address to multiple machines. It
inherently supports several IP addresses to be assigned to one name. When a client makes a
request to a DNS server for an IP address matching the specified hostname, the DNS server
serves a list of IP addresses to the client. The client can choose which IP address to use,
often the first one. When another client makes the same request, the same list is served, but
in a different order.
By cycling through a pool of configured addresses, load is effectively spread out across each
server. This technique is known as round-robin DNS and is often referred to as a poor man's
load balancer. It's nowhere near perfect, but it usually does a decent job.
Despite the diminutive nickname, round robin DNS does have its place. More advanced
configurations are still employed by a number of gigantic infrastructures, which still make
use of this style of DNS, combined with more intelligent software that can analyze current
traffic information and patterns before deciding which IP address is best to give to a client.
IP-based load balancing
One of the original design goals of Salt was for minions to not have to rely on DNS in order
to reach their master. This is because as crucial as DNS is to the modern Internet, it still has
a number of shortcomings, which are inherent to its necessarily hierarchical nature. Because
DNS is based on a series of lookups, which may require constant synchronization between
several DNS servers just to resolve one address, there are a lot of gears that may break
while a minion attempts to maintain contact with its master.
Fortunately, DNS is not the only way to map a single address to multiple hosts. A number
of open source and commercial load balancers exist that can map one or more public IP
addresses to one or more private IP addresses. Depending on the solution used, the
intelligence used may be as simple as a round-robin style or as advanced as actually
monitoring the load and delivering traffic accordingly.
This is a good time to point out that when we talk about a public IP address, we're not
necessarily talking about an Internet-facing IP address. What we're talking about is an IP
address that is visible to the clients that need to use it. It could be that Master 1 has an
address of  and Master 2 has an address of  and that they share the
public address of  through the load balancer.

Strategies for Scaling
[ 240 ]
Synchronizing files
Whether you configure minions to point to multiple masters or use a shared address
solution, or some combination of them, there is still the matter of keeping files in sync
among the masters themselves.
Before we talk about how to keep the files synchronized, let's talk about which files need to
be kept in sync, why, and which of those we actually need to worry about.
Base configuration files
Inside the FUDTBMU directory, there is a base set of files that are necessary for Salt to
even run in the first place, and they form the bulk of the files that we'll need to worry about.
FUDTBMUNBTUFS
FUDTBMUNBTUFSEDPOG
The NBTUFS file, plus any files with a DPOG extension in the NBTUFSE directory,
comprises the master configuration. None of these files technically need to exist; if they are
absent, then the default master configuration will be used. However, things such as external
pillars, caches, and filesystems are not configured by default. Because those things are
important to a high-availability setup, it's best to keep this managed.
Fortunately, once these files are dialed in, it's rarely necessary to change them. However
you manage these files, you are unlikely to need anything more than a manual process
unless you are managing them via some other external process that requires regular,
automatic changes to be made.
FUDTBMUQLJ
This is where the keys are stored, both for the master and for the minions. It is very crucial
to keep this directory synchronized among the masters so that when a key is accepted or,
even more importantly, removed from one master, the others know about it as quickly as
possible.
FUDTBMUDMPVE
FUDTBMUDMPVEQSPGJMFT
FUDTBMUDMPVEQSPGJMFTE
FUDTBMUDMPVEQSPWJEFST
FUDTBMUDMPVEQSPWJEFSTE
FUDTBMUDMPVENBQTE

Strategies for Scaling
[ 241 ]
FUDTBMUDMPVEEFQMPZE
All these files and directories belong to Salt Cloud, and their necessity and management
varies with the needs of the organization. Because Salt Cloud is designed to auto-accept
keys on the master, many organizations choose to use Salt Cloud only on the master.
However, it is possible to issue commands to Salt Cloud from a minion, and a growing
number of users have decided to go that route and handle key management outside Salt
Cloud.
If you are not using Salt Cloud at all on a master, then these files can obviously be ignored.
If you are using it on a master, keeping these files synchronized is far less crucial. They will
only be utilized when issuing Salt Cloud commands, and unless Salt Cloud is being called
from an autonomous process such as the reactor, these files can be manually synchronized
as needed.
WBSDBDIFTBMU
This directory is used by a number of different processes inside Salt, including the master,
minion, Salt Cloud, and others. Salt Cloud uses this directory primarily for increasing
performance. However, if EJGG@DBDIF@FWFOUT is set to 5SVF in FUDTBMUDMPVE and
reactors that make use of those events are configured, then keeping
WBSDBDIFTBMUDMPVE synchronized is as important as keeping FUDTBMUQLJ
synchronized.
The files in WBSDBDIFTBMUNBTUFS are important for those who make use of Salt's
various job-lookup capabilities. If you are using multiple masters, then these capabilities are
not only used, but crucial. Fortunately, the job cache can be offloaded onto another server or
service, which can be shared by multiple masters. We'll cover the external job cache later on
in this chapter, in the Incorporating external data sources section.
TSWTBMU
TSWQJMMBS
TSWSFBDUPS
How much you use these directories, if at all, depends entirely on your use cases. However,
if you use them, then keeping them synchronized among the masters is pretty important.
The good news is that TSWTBMU can be provided in its entirety by one of Salt's external
filesystem drivers, and TSWQJMMBS can be provided entirely by one of Salt's external
pillar drivers. Both are covered in the next section.

Strategies for Scaling
[ 242 ]
The bad news is that TSWSFBDUPS does not have a specific Salt driver to handle it.
However, using this location for reactors is also not required; it was a naming convention
that was recommended because it made sense. These files could just as easily be stored in,
say, TSWTBMUSFBDUPS if desired, as long as they aren't referenced by any files used by
the state system.
WBSSVOTBMU
The files in this directory are specific to an individual host and should not be synchronized.
Synchronizing the nonexternal files
As we saw, there are some files that can be managed externally, and we'll cover those in a
moment. For now, let's look at the ones that need to be managed outside Salt.
It's probably fair to say that the majority of infrastructures have a minion set up on the
master. If this is the case, then any file in the FUDTBMU directory (outside of
FUDTBMUQLJ) may be managed by a Salt state, which means that any of these files can
be stored using one of Salt's external filesystem drivers. If your infrastructure meets this
model, then this will greatly simplify the storage and management of those files.
If this model doesn't work for you, then you'll need to look at one of the options that will be
necessary for FUDTBMUQLJ to handle the rest of the files in the FUDTBMU directory.
The simplest option, in terms of configuration, may be to mount FUDTBMU (or just
FUDTBMUQLJ) using an external filesystem outside Salt, such as NFS or SMB. However,
because this strategy will be subject to network conditions, you may find yourself in a
situation wherein the stability of Salt degrades or disappears because of a network user or
process that is misbehaving.
Even though it may be more complex to set up, it's far better to maintain local copies of the
files in these directories and set up a process, be it manual or automatic, to maintain
consistency among all masters.

Strategies for Scaling
[ 243 ]
Using rsync
A periodic STZOD command among the masters will accomplish much of what is needed.
Setting this up on a regular cron will eventually make the data among the masters
consistent, which in many cases is good enough. The following cron line will synchronize
files from one master to another every  minutes:
STZODBW[FUDTBMUQLJPUIFSNBTUFSFUDTBMUQLJ
Setting up lines similar to this on each master is one step towards eventual consistency.
However, STZOD can be an expensive process to be running on a regular basis, especially
with a large set of files. Worse, mismatches between masters can cause keys to show up in
multiple directories, putting a minion in a state where it is both accepted and unaccepted.
Fortunately, Salt does give us the ability to be more intelligent with the files that we copy
between the masters.
Using the event reactor
It is best to only perform work when it needs to be performed, and ideally, that work is
performed as soon as possible. Because key management triggers events, we can use the
event listener to let us know when keys have changed so that we can propagate those
changes as quickly as possible.
If you were to fire up the event listener and accept and delete the keys for a minion, you
would see events that look like this:
&WFOUGJSFEBU5IV+VO

5BHTBMULFZ
%BUB
\	@TUBNQ		5	
	BDU		BDDFQU	
	JE		UFTUNJOJPO	
	SFTVMU	5SVF^
&WFOUGJSFEBU5IV+VO

5BHTBMULFZ
%BUB
\	@TUBNQ		5	
	BDU		EFMFUF	
	JE		UFTUNJOJPO	
	SFTVMU	5SVF^

Strategies for Scaling
[ 244 ]
Chances are that these are the only two events you'll be dealing with for key management
(unless you have an active policy of rejecting keys, which would result in an act of
rejection), so we can go ahead and start building reactors based on them.
First, we'll map the tag to a reactor file inside of FUDTBMUNBTUFS:
SFBDUPS
TBMULFZ
TSWSFBDUPSTBMULFZTMT
Then, we'll create TSWSFBDUPSTBMULFZTMT with the following content:
\TFUNJOJPOEBUB<	JE	>^
\TFUQLJEJS	FUDTBMUQLJNBTUFS	^
\JGEBUB<	BDU	>	BDDFQU	^
DPQZ@BDDFQUFE@LFZ
DNEDNESVO
UHUNBTUFS
OBNFTDQ\\QLJEJS^^NJOJPOT\\NJOJPO^^NBTUFS\\QLJEJS^^NJOJPOT
SFNPWF@VOBDDFQUFE@LFZ
DNEGJMFSFNPWF
UHUNBTUFS
OBNF\\QLJEJS^^NJOJPOT@QSF\\NJOJPO^^
\FMJGEBUB<	BDU	>	EFMFUF	^
\GPSLFZEJSJO
	NJOJPOT		NJOJPOT@QSF		NJOJPOT@SFKFDUFE	^
EFMFUF@\\LFZEJS^^@LFZ
DNEGJMFSFNPWF
UHUNBTUFS
OBNF\\QLJEJS^^\\LFZEJS^^\\NJOJPO^^
\FOEGPS^
\FOEJG^
Again, we're letting Jinja do a lot of work for us. Let's take a look at what's happening in the
preceding code.
First, we define the ID of the minion. We get that from the JE field inside the EBUB value
provided by the event. We could skip this line and just refer to EBUB<	JE	> throughout the
file, but this is a little more readable and gives us more flexibility if we decide to change the
behavior of that variable later. We've done the same thing with the PKI directory by
defining it as QLJEJS.
Then we check to see whether we're dealing with a key that has been accepted or deleted. If
it is accepted, we have two tasks to perform: copy the key to the NJOJPOT directory on the
target master and then make sure it is no longer in the NJOJPOT@QSF directory.

Strategies for Scaling
[ 245 ]
It's temping to issue a TBMULFZ command here, but don't do it! First off, the minion in
question may not have attempted to contact the other masters yet, and therefore, there
won't be a key to accept in the first place. But more importantly, since every master should
be configured with this reactor, using the TBMULFZ command on all the other masters
would trigger an infinite loop, which would be crippling for all the masters, and possibly
even the network.
If the key is not being accepted but being deleted, then we just need to make sure that it is
absent from all the other masters. There are three directories that it needs to be absent from,
so we use a loop to go through each of them. Order is important here: we want to make sure
that the minion to be deleted is unable to receive tasks before removing it from the other
two directories.
Incorporating external data sources
We've taken care of Salt keys, but we have some other directories that we need to distribute
among the masters. Let's start with a component that is used every time you issue a
command from the master to one or more minions: the job cache.
The external job cache
Before we dive into this component, let's review the master job cache:
When a command is issued, a job ID (JID) is created on the master.
1.
Information about that job is stored in the job cache, such as what the command
2.
and its arguments are and which minions will be affected.
The job data is posted to the message queue, where the affected minions pick it
3.
up and perform the requested work.
When each minion finishes the task, it sends the return data back to the master,
4.
where it is also stored in the job cache.
If the TBMU command is still running, it will pick up the job data and display it to
5.
the user.
In this workflow, the minion will always return data to the master, whether or not the TBMU
command is there to receive it. The master will always cache it so that it can be looked up
later.

Strategies for Scaling
[ 246 ]
Using returners on the minions
If we introduce a returner to the equation, then the minions will send the return data to an
external data source in addition to sending it to the master. In this case, since the minion is
connecting to the data source, the minion will need to be configured with the connection
options for that data source. For instance, to use SFEJT, one might add the following to the
minion configuration:
SFEJTECTBMUEC
SFEJTIPTUTBMUECFYBNQMFDPN
SFEJTQPSU
Then, to use that returner, specify it when issuing your TBMU command:
TBMU		EJTLVTBHFSFUVSOSFEJT
Any returner that ships with Salt can be used in this manner. All that will be sent to the
external database will be the return data for the job. This is useful for monitoring-style
tasks, where a minion is constantly being asked for information about system vitals, such as
disk or memory usage. Using an external database to store that information allows later
analysis of it by another piece of software.
The master can be configured to ask its minions to always return data to an external data
store. However, this introduces a couple of changes to the equation. When a default
returner is configured, all the job data will be sent to it, not just the return data. Not every
returner is set up to do this. As of version 2015.5, the returners that can do this are:
DPVDICBTF
DPVDIEC
FUDE
JOGMVYEC
NFNDBDIF
NPOHP
NZTRM
PECD
QHKTPOC (1PTUHSFT, with the KTPOC data type)
QPTUHSFT
SFEJT
TRMJUF

Strategies for Scaling
[ 247 ]
You may have noticed that only a handful of the many returners that ship with Salt are
suitable for use in the external job cache. That's because several returners are designed to be
used with systems that are write-only as far as Salt is concerned. When job data is sent to
the returner for the Slack service, for instance, it will be posted to a chatroom, which cannot
be queried later by Salt. When job data is sent to the returner for the /BHJPT package, it will
be used exclusively for monitoring and alerting purposes.
In order to send job data to an external data source by default, the FYU@KPC@DBDIF setting
must be set in the master configuration file. This will just be set to the same name as would
normally be passed in using the SFUVSO flag:
FYU@KPC@DBDIFSFEJT
When this is set, minions will be directed to send the data to the external data source every
time. With the master also configured with the same returner credentials as the minions, the
KPCT runner will also access the external data store to pick up its data about jobs.
Many administrators may feel squeamish about storing database credentials inside the
minion configuration files, where anyone with access to the minion can view them, and
rightfully so. Returners such SFEJT and FUDE can be even more problematic, as they
require no credentials and therefore allow unfettered access to everybody.
With some of these returners, it is possible to mitigate some of these concerns. Many of the
databases available have the ability to restrict access to reads or overwrite data inside a
database, based on the credentials used to access it. However, there is another way out.
Using the master job cache
Starting with version 2014.7, Salt has the ability for the master to store the job return data in
the external data store instead of asking the minions to do it. Administrators who are
concerned about storing database credentials on minions can set their minds at rest in
knowing that only the master has the credentials.
In order to do this, set the NBTUFS@KPC@DBDIF option in the master configuration file to the
same returner that would be used with FYU@KPC@DBDIF on the minions:
NBTUFS@KPC@DBDIFSFEJT

Strategies for Scaling
[ 248 ]
Keep in mind that, by default, minions still have access to the master configuration data
inside a pillar called master. In order to keep this information from the minions, the
QJMMBS@PQUT option can be turned off in the master configuration file:
QJMMBS@PQUT'BMTF
Keep in mind that the default for this option has changed. Previous to
version 2015.5.0, it was 5SVF. As of 2015.5.0, the default is now 'BMTF.
When these options are configured across all masters, the contents of the
WBSDBDIFTBMU directory that are important to synchronicity will be immediately (not
just eventually) available to all masters.
External filesystems
The next set of files and directories that need to be made available to all masters are those
found in the TSWTBMU directory. As it turns out, the option that many administrators
consider to be the best even with a single master is also the option that makes it possible to
share this directory structure with all other masters as well.
GitFS
The first component to offer the option of storing files outside the master's local filesystem
was the HJUGT driver. This option was so immediately popular that the entire filesystem
model of Salt was refactored in order to allow other drivers to be added as well.
GitFS was a godsend, because so many organizations anyway prefer keeping all their code
in a software versioning system such as Git, and having Salt access repositories directly cuts
out a lot of work.
As time has passed, a number of features have been added to this driver. So, let's get
through the basics first and then cover the bonus features.
External filesystems are set up in the master configuration file, using the
GJMFTFSWFS@CBDLFOE option. By default, this option is set to SPPUT, which is the driver
that manages files on the master's local filesystem:
GJMFTFSWFS@CBDLFOE
SPPUT

Strategies for Scaling
[ 249 ]
To switch over from local storage to Git-based storage, change the SPPUT line to HJUGT:
GJMFTFSWFS@CBDLFOE
HJU
Before we get into the rest of the configuration for HJUGT, this is a good time to point out
that multiple fileserver backends can be specified, in the order in which they are to be
searched:
GJMFTFSWFS@CBDLFOE
HJU
SPPUT
When a file is requested, Salt will search through each external fileserver, in the order in
which they are specified, until the requested file is found.
Once the fileserver backend is configured, Salt needs to know where to find the Git
repositories:
HJUGT@SFNPUFT
HJUHJUIVCDPNNZDPNQBOZTBMUHJU
This is the most simple configuration, which will set the root of the Git repository to act as if
it were TSWTBMU on the local filesystem.
The HJU URL is not the only protocol that is supported by GitFS. You can also use
remotes with IUUQT, GJMF, or TTI URL schemes.
As with GJMFTFSWFS@CBDLFOE, multiple Git remotes can be specified, and when a file is
requested, Salt will search through each repository in the order in which it is declared.
Starting with version 2014.7, a number of options have been added that allow individual
configurations to be specified per Git repository. The following options are available:
CBTF
SPPU
NPVOUQPJOU
VTFS
QBTTXPSE
JOTFDVSF@BVUI
QVCLFZ
QSJWLFZ
QBTTQISBTF

Strategies for Scaling
[ 250 ]
A number of different backend drivers are available to power GitFS. The base, root, and
NPVOUQPJOU options are available across all drivers. But all the other options are only
available using the QZHJU driver. To make sure that you are using the QZHJU driver,
specify it as the HJUGT@QSPWJEFS:
HJUGT@QSPWJEFSQZHJU
The purpose of some of these options may not be immediately obvious, so let's go through
them and see how they are used.
CBTF
As you know from configuring Salt states, the default environment in Salt is called CBTF.
Other environments such as EFW, RB, and QSPE are normally configured in the UPQTMT file.
With GitFS, these environments are configured via tags or branches instead. Files that are in
the QSPE branch will be served to servers in the QSPE environment, and so on.
Rather than forcing users to create a branch or tag called CBTF to serve files, the CBTF
option can be used to specify a different branch. For instance, if you needed to use files
inside the USVOL branch inside your base environment, your configuration might look like
the following:
HJUGT@SFNPUFT
HJUHJUFYBNQMFDPNNZQSPKFDUHJU
CBTFUSVOL
Note that if the base is not specified, the default will be the master branch
on the repository, whether or not it exists.
SPPU
Normally when Salt serves files from Git, the SPPU of the repository behaves as if it were
TSWTBMU. This may be impractical, depending on the organization of the repository in
question. If the directory tree in your repository is set up in a way that, say, the
DPEFTBMUTUBUFT directory needs to be treated as if it were TSWTBMU, you can
redirect the SPPU to point to that directory:
HJUGT@SFNPUFT
HJUHJUFYBNQMFDPNNZQSPKFDUHJU
SPPUDPEFTBMUTUBUFT

Strategies for Scaling
[ 251 ]
NPVOUQPJOU
Sometimes, you need the opposite of what the root option provides. Perhaps you want the
root of your repository to show up deeper inside the directory tree inside Salt. Using a
mount point will append a virtual path to the beginning of the repository root. For instance,
say your repository has a file called IUUQTDPOG in its SPPU directory, and you need it to be
served as if it were located at TSWTBMUBQBDIFGJMFTIUUQEDPOG. Your
configuration would look something like the following:
HJUGT@SFNPUFT
HJUHJUFYBNQMFDPNNZQSPKFDUHJU
NPVOUQPJOUTBMUBQBDIFGJMFT
VTFSBOEQBTTXPSE
When working with Git repositories using the IUUQT URL scheme, a username and
password may be required. These are passed using the VTFS and QBTTXPSE options:
HJUGT@SFNPUFT
IUUQTHJUFYBNQMFDPNNZQSPKFDUHJU
VTFSMBSSZ
QBTTXPSEQBTT
JOTFDVSF@BVUI
By default, Salt will refuse to authenticate against repositories using the IUUQ URL
scheme. In order to force Salt to authenticate using this insecure transfer method, set
JOTFDVSF@BVUI to 5SVF:
HJUGT@SFNPUFT
IUUQHJUFYBNQMFDPNNZQSPKFDUHJU
VTFSMBSSZ
QBTTXPSEQBTT
JOTFDVSF@BVUI5SVF
QVCLFZQSJWLFZBOEQBTTQISBTF
Normally, Git repositories that use SSH are configured using the HJU URL scheme.
However, Git can be configured to allow access using an SSH-like syntax. The following
two declarations are functionally identical:
IUUQTHJU!HJUFYBNQMFDPNVTFSNZQSPKFDUHJU
HJU!FYBNQMFDPNVTFSNZQSPKFDUHJU

Strategies for Scaling
[ 252 ]
Using Git over SSH will require authentication, and key-based security is the way to go. The
QVCLFZ and QSJWLFZ options are used to specify the locations of the public and private key
files, respectively.
HJUGT@SFNPUFT
HJUHJUFYBNQMFDPNNZQSPKFDUHJU
QVCLFZSPPUTTINZQSPKFDU@STBQVC
QSJWLFZSPPUTTINZQSPKFDU@STB
If the private key is protected by a passphrase, it may be specified using the QBTTQISBTF
option:
HJUGT@SFNPUFT
HJUHJUFYBNQMFDPNNZQSPKFDUHJU
QVCLFZSPPUTTINZQSPKFDU@STBQVC
QSJWLFZSPPUTTINZQSPKFDU@STB
QBTTQISBTFQBTT
Any of these options may also be specified globally by prepending them with HJUGT@.
When doing this, the options will be applied to all GitFS remotes, but they can be
overridden individually, as seen in the preceding code. For example, to use USVOL as the
global branch for the base environment while overriding it and using EFWFMPQ on the last
remote, your configuration might look like the following:
HJUGT@CBTFUSVOL
HJUGT@SFNPUFT
HJUHJUFYBNQMFDPNNZQSPKFDUHJU
HJUHJUFYBNQMFDPNZPVSQSPKFDUHJU
HJUHJUFYBNQMFDPNPVSQSPKFDUHJU
CBTFEFWFMPQ
Other source-control backends
By far, the most popular fileserver backend is GitFS, and so the maximum time and most
features have been spent on that filesystem. However, it is certainly not the only player in
the game. Both Subversion (SVN) and Mercurial (HG) are available using the TWOGT and
IHGT drivers, respectively. Both have many of the options that are available with GitFS, but
there are some differences.

Strategies for Scaling
[ 253 ]
SVNFS
In order to use SVNFS, there must be a fileserver backend of TWO configured:
GJMFTFSWFS@CBDLFOE
TWO
A URL to the remote SVN repository or repositories must also be configured:
TWOGT@SFNPUFT
TWOTWOFYBNQMFDPNNZQSPKFDU
The following options may also be added to any of the TWOGT@SFNPUFT:
root
mountpoint
trunk
branches
tags
As with GitFS, these options may be used globally across all SVN repositories by
prepending them with TWOGT@.
SPPUBOENPVOUQPJOU
The SPPU and NPVOUQPJOU options behave as with GitFS, but the other three options do
need some explanation.
USVOL
SVN is based on USVOL, each of which is made from other branches. This option specifies
where a USVOL is located, relative to the SVN remote URL in question. The default is USVOL:
TWOGT@USVOLUSVOL
CSBODIFT
Also, relative to the SVN remote URL is the location of the branches inside the repository.
The default is CSBODIFT:
TWOGT@CSBODIFTCSBODIFT

Strategies for Scaling
[ 254 ]
UBHT
Once again, SVN tags are also relative to the SVN remote URL. As you might expect, the
default is UBHT:
TWOGT@UBHTUBHT
As with GitFS, environments are mapped to UBHT and CSBODIFT. However, with a
repository that contains a large number of either of these, limiting the tags and branches
that are made available to Salt may increase performance.
This can be accomplished using the TWOGT@FOW@XIJUFMJTU or the TWOGT@FOW@CMBDLMJTU
options. They both function as one might expect: items that are not specified in the whitelist
are not made available, and items that are specified in the blacklist are not made available.
Items in either of these lists may be specified as either an entire name or a pattern specified
as either a glob or a regular expression:
TWOGT@FOW@XIJUFMJTU
PMEQSPKFDU
BDDPVOUJOH
	TBMFT=E	
These two options may also be used together. When doing so, the whitelist will be
evaluated first, and then items that match the blacklist will be removed.
HGFS
Of course, we would be remiss if we skipped over using Mercurial as a fileserver backend.
In order to use this driver, set IH in the GJMFTFSWFS@CBDLFOE:
GJMFTFSWFS@CBDLFOE
IH
Then, set up Mercurial repositories using the IHGT@SFNPUFT option:
IHGT@SFNPUFT
IUUQTMBSSZ!IHFYBNQMFDPNMBSSZNZQSPKFDU

Strategies for Scaling
[ 255 ]
As with SVNFS, the following options are available globally:
IHGT@SPPU
IHGT@CBTF
IHGT@NPVOUQPJOU
IHGT@FOW@XIJUFMJTU
IHGT@FOW@CMBDLMJTU
Once any of these global options are declared, SPPU, CBTF and, NPVOUQPJOU may be
overridden on a per-repo basis:
One more global option is also available, which is specific to HGFS: IHGT@CSBODI@NFUIPE.
This specifies whether CSBODI or CPPLNBSLT, or both, will be used in conjunction with tags
to provide Salt environments. The available settings for this are:
branches
bookmarks
mixed
S3FS
Version control systems are not the only external filesystem drivers that ship with Salt.
After GitFS was introduced, it did not take long for the S3FS driver to be committed. This 
driver has proven to be extremely popular with the customers of Amazon Web Services.
Before we get into the configuration of this driver, take note: it does not provide version
control. When working with text-based files, I'm going to put my foot down and
recommend that they all be checked into some sort of a revision control system. In any
production environment, this provides far more advantages than just using the master's
local filesystem.
However, binary files are impractical to be stored in version control. They make repositories
bulky and slow and cannot be properly managed as text files can. This is where using a
driver such as S3FS can really be useful.
In order to use this driver, add TGT to the list of fileserver backends:
GJMFTFSWFS@CBDLFOE
TGT

Strategies for Scaling
[ 256 ]
Credentials to connect to S3 must also be provided. Once you have received them from
Amazon, add them to the master configuration file as TLFZJE and TLFZ:
TLFZJE"#$%&'
TLFZBCDEFGHIJKLMNOPQRSTUVWXYZ[
There are two ways to set up your S3 buckets to serve files: one environment per bucket or
multiple environments per bucket.
0OFFOWJSPONFOUQFSCVDLFU
The most straightforward way to configure S3FS, and the one that requires the least amount
of thought before the bucket is created in the first place, is to treat each bucket as its own
separate environment. With this model, each environment, and the bucket (or buckets) that
belongs to it, are specified with the TCVDLFUT option:
TCVDLFUT
CBTF
DPEF
EFTJHO
QSPE
QSPE@DPEF
QSPE@EFTJHO
.VMUJQMFFOWJSPONFOUTQFSCVDLFU
It may make more sense to keep all your environments together in a single bucket or in a
group of buckets that are combined together. This requires that the buckets be set up
beforehand to include directories named after the environments. First, list the bucket or
buckets in the TCVDLFUT list:
TCVDLFUT
DPEF
EFTJHO
Then, create the buckets (in this case, one called DPEF and one called EFTJHO). Inside each
bucket, create one directory per environment (in our case, CBTF and QSPE). Then, files are
placed inside those directories, as usual.

Strategies for Scaling
[ 257 ]
Were you to abstract your directory tree out into an T URL scheme, our example
would have a file structure that looked like this:
TDPEFCBTFGJMFT 
TDPEFQSPEGJMFT 
TEFTJHOCBTFGJMFT 
TEFTJHOQSPEGJMFT 
AzureFS
Not to be outdone by Amazon, Azure also has a cloud storage solution available, which can
be used as an external filesystem with Salt.
In order to use AzureFS, add B[VSFGT to the list of fileserver backends:
GJMFTFSWFS@CBDLFOE
B[VSFGT
However, this driver is a little different because of the difference between Azure storage
and S3. First of all, Azure uses storage containers, not buckets; so we'll refer to them as
containers from here on (not to be confused with container systems such as Docker and
CoreOS RKT). Secondly, AzureFS is configured only to allow one environment per
container.
Azure configures access to each container using the name of the TUPSBHF@BDDPVOU that the
container lives in and a TUPSBHF@LFZ that authenticates access to that container.
B[VSFGT@FOWT
CBTF
TUPSBHF@BDDPVOUEFWFMPQNFOU
TUPSBHF@LFZBCDEFG"#$%&'
External pillars
The last component that can be moved into an external service is the pillar system. Let's go
over the basic configuration first. To use an external pillar driver, add it to the FYU@QJMMBS
list inside the master configuration:
FYU@QJMMBS
DNE@KTPOVTSCJONZQJMMBS

Strategies for Scaling
[ 258 ]
There are two components to every pillar declaration: the name of the module (in this case,
DNE@KTPO) and any arguments that are to be passed to that driver (in this case, the
VTSCJONZQJMMBS command, which is expected to return pillar data in JSON format).
There are quite a number of external pillar modules available-too many to go through. So,
we'll pick out a few key pillars that are likely to be of use in your organization.
The full list of pillars that are available can be found online at
IUUQEPDTTBMUTUBDLDPNFOMBUFTUSFGQJMMBSBMMJOEFYIUNM.
cmd_yaml/cmd_json
The DNE@ZBNM and DNE@KTPO aren't pillars that are likely to be useful in scaling out to
multiple masters, but they do effectively demonstrate how the pillar system works.
The argument for both of these modules is a command that returns a dictionary of data in
either the YAML or the JSON format, respectively. If this command includes a T in it, it
will be replaced with the name of the minion that is requesting the pillar data.
For instance, the following code will return data that would be provided to all of the
minions:
DNE@KTPODBUTSWQJMMBSDPNNPOKTPO
This code will search for pillar data that is specific to the requester:
DNE@KTPODBUTSWQJMMBSNJOJPOTTKTPO
These modules are good for getting a feel of how the external pillar system works.
However, to start scaling out, it's time to look at more advanced external pillars.
git
It should not be surprising that one of the most popular external pillars makes use of Git
repositories. However, the configuration of this module is a little different from the GitFS
configuration.

Strategies for Scaling
[ 259 ]
There are two arguments that are required: the branch inside the repository that is to be
used and the URL to the repository. Here's an example:
FYU@QJMMBS
HJUNBTUFSHJUHJUFYBNQMFDPNNZQSPKFDUHJU
Like GitFS, an alternate SPPU may also be specified:
FYU@QJMMBS
HJUNBTUFSHJUHJUFYBNQMFDPNNZQSPKFDUHJUSPPUDPEF
In this case, DPEF refers to a directory inside the Git repository called DPEF.
If you want to specify that the CSBODI be mapped to a different environment name, you can
specify both the branch and the environment together, separated by a colon:
FYU@QJMMBS
HJUNBTUFSCBTFHJUHJUFYBNQMFDPNNZQSPKFDUHJU
There are a couple of different ways to expose the branches as their own environment. If the
special @@FOW@@ branch is specified, then each branch will automatically be mapped to a
corresponding environment of the same name:
FYU@QJMMBS
HJU@@FOW@@HJUHJUFYBNQMFDPNNZQSPKFDUHJU
If you don't want to expose all the branches in the repository as their own environments, it's
better to define them all individually, adding a CSBODI value to the environment mappings
as appropriate:
FYU@QJMMBS
HJUNBTUFSCBTFHJUHJUFYBNQMFDPNNZQSPKFDUHJU
HJUEFWHJUHJUFYBNQMFDPNNZQSPKFDUHJU
HJUQSPEHJUHJUFYBNQMFDPNNZQSPKFDUHJU
redis
The SFEJT module is a natural fit for pillar data, because it stores data in the same kind of
key/value pairs used for pillar data.

Strategies for Scaling
[ 260 ]
If you are already using SFEJT for your external job cache, you can reuse the connection
settings inside the master configuration:
SFEJTEC
SFEJTIPTU
SFEJTQPSU
Then, configure how the data will be pulled out of SFEJT. Data can be stored in SFEJT as
either a JSON object or as a string, hash, or list.
If the data is stored as JSON, then the external pillar declaration will be as follows:
FYU@QJMMBS
SFEJT\GVODUJPOLFZ@KTPO^
Take note that unlike other examples in this book, this one is printed
exactly as it should appear in your configuration, character for character.
With this type of data, the key of the JSON object is the name of the minion that has
requested the pillar data. If no such object exists in the database, an empty dictionary will
be returned.
If the data is stored as a string, hash, or list, then the external pillar declaration will be as
follows:
FYU@QJMMBS
SFEJT\GVODUJPOLFZ@WBMVF^
Again, this is the exact configuration that is to be used. And again, the key that is used to
access the data must match the name of the minion that has requested it.
mysql
The NZTRM module may not seem as natural as SFEJT, since SQL isn't normally thought of
as having key/value pairs. But, in fact, nothing could be further from the truth. SQL is all
about key/value pairs; it just happens to refer to the keys as fields, and it organizes the data
as rows.

Strategies for Scaling
[ 261 ]
To use this module, connection details must be set up inside the master configuration:
NZTRM
VTFSTBMU
QBTTQBTT
ECTBMUEC
Then, configure the NZTRM pillar with a query that can be used to collect the data from the
database:
FYU@QJMMBS
NZTRM
GSPNEC
RVFSZ	4&-&$5SPMF'30.NJOJPOT8)&3&JE-*,&T	
Using the master API
The master API has a somewhat confusing name for users that are used to traditional Unix
terminology. Because the master is what steers a Salt infrastructure, the API that is used to
configure it is called the wheel system. To put it clearly, Salt's wheel system bears no
relation or resemblance to the wheel group found in many Unix and Linux distributions.
Like most of Salt, the wheel system is pluggable. However, while a few modules do exist,
most of them will never be used by most end users. For our purposes, we'll focus on the
ones that are used.
The Salt keys
The part of the wheel system that is most commonly seen by administrators is key
management. When keys are accepted, rejected, or deleted, that action is usually performed
by the wheel system. The LFZ module doesn't actually do much: it creates keys when
necessary, moves them between directories as requested, and fires events along the event
bus when doing so.

Strategies for Scaling
[ 262 ]
Configuration
The DPOGJH module is used to manage the master configuration files. Again, it does very
little: it can write one or more values to the master configuration file, and it can return the
contents of that file. Be warned: when this module is used to modify the master
configuration, any comments that exist in that file will be stripped. If you take comfort in
the presence of friendly comments to help you out, this module is best avoided.
One important note about this module: while it manages configuration files, it does not
manage the internal master configuration. Once changes have been made, the master must
still be restarted in order to apply those changes.
The file and pillar roots
The GJMF@SPPUT and QJMMBS@SPPUT modules behave in largely the same way, the only
functional difference being the directory they operate on. Each supports searching for files,
listing environments, and reading and writing the contents of a file. As the name implies,
they are designed to be used with local files, not external filesystems or external pillars.
Using the wheel reactor
So, what good are these modules? Again, the vast majority of commands will use the LFZ
module, though the small set of use cases surrounding the other modules may meet your
needs.
While accepting keys on the master will require some logic that is specific to your needs, in
order to securely accept keys only from minions that are considered trustworthy, deleting
keys from the master is reasonably safe, at least from a security point of view. While
accepting the wrong key may allow rogue minions into your infrastructure, deleting the
wrong key never will.
A process that is able to detect that a minion's key should be deleted can be set up to fire an
event that triggers that key's deletion. For now, we'll assume that the process in question is
able to issue a standard Salt command. For instance, a minion that has been tasked with
keeping watch may issue the following command:
salt-call event.fire_master '{"id": "myminion"}'
    custom/key/myminion/delete

Strategies for Scaling
[ 263 ]
On the master, the tag is mapped to a SFBDUPS file:
SFBDUPS
DVTUPNLFZEFMFUF
TSWSFBDUPSEFMFUFLFZTMT
Inside the TSWSFBDUPSEFMFUFLFZTMT file, the wheel system is called to delete the key
for the minion in question:
EFMFUF@NJOJPO@LFZ
XIFFMLFZEFMFUF
NBUDIEBUB<	JE	>
Using wheel with Thorium
Let's go ahead and take a look at what this reactor would look like using the Thorium
system. Instead of using the SFBDUPS section in the master configuration file, let's set up
our Thorium engine:
FOHJOFT
UIPSJVN\^
Then go ahead and create TSWUIPSJVNEFMFUFLFZTMT:
EFMFUF@LFZ@UBH
DIFDLFWFOU
OBNFDVTUPNLFZEFMFUF
EFMFUF@LFZ
XIFFMDNE
GVOLFZEFMFUF
NBUDI\\@@SFH@@<	EBUB	><	OBNF	>^^
SFRVJSF
DIFDLEFMFUF@LFZ@UBH
Lastly, we map our code in Thorium's UPQTMT file:
CBTF
		
EFMFUF@LFZ

Strategies for Scaling
[ 264 ]
Testing the load in the infrastructure
Now that we've talked about the various ways to scale out your infrastructure, you will
probably find it useful to know how to throw tests at your infrastructure to test its ability to
handle the load that will be thrown at it.
Using the minionswarm.py script
The NJOJPOTXBSNQZ script was originally used to test the performance of execution
modules as they were written. But while it is still used for that today, it can also be used to
test the performance of a state tree against a large number of minions.
The NJOJPOTXBSNQZ script does not ship inside any of the Salt packages,
but it can be downloaded from the UFTUT directory in Salt's GitHub
repository at IUUQTHJUIVCDPNTBMUTUBDLTBMU.
The NJOJPOTXBSNQZ script is designed to create a user-defined number of minions, which
can then accept commands from the master. Keep in mind that this script will only run on a
single host, so it won't help much in testing syndic architecture. However, it will effectively
test how well your external filesystems and pillars are able to interact with each other.
To use the script, copy it into a directory and start it up with Python:
# python minionswarm.py
By default, this will create five minion processes. Once they are spun up, standard Salt
commands can be issued against those minions. If you want to spin up a master as well to
talk to those minions, you can do so by specifying the . option:
# python minionswarm.py -M
The first thing you will probably want to do is create more minions to test with. The N
option will set the number of minions that will be created.
# python minionswam.py -m 500
Be careful here! Each minion that is created will leave its own footprint in
memory, and each footprint will be the size of a regular minion process.
Specifying too many minions may overload an underpowered system.

Strategies for Scaling
[ 265 ]
Swarm internals
Obviously, the swarm of minions that is created cannot be considered viable for production
use; it is for testing purposes only. This should be kept in mind should you decide to take a
look at the various files that are created during this process.
In the UNQ directory, a directory whose name starts with NTXBSNSPPU will be created.
This directory will contain a QLJ directory with entries for each minion. And if you look at
the keys for each minion, you will notice that they are all identical!
If the need presents itself, it is possible to replace keys for individual minions so long as you
make sure to keep them in sync with the temporary keys stored by the master. But the point
of the minion swarm is not to test keys or security-it's to test load!
So, go ahead and fire up a few test commands to get a feel of how load testing works with
the minion swarm:
# salt '*' test.ping
# salt '*' network.interfaces
# salt '*' disk.usage
Then, try testing out a state run, preferably in test mode:
# salt '*' state.highstate test=True
If you decide to do a state run without enabling the test mode, it will be best to do so on an
otherwise blank machine that can safely be trashed if the state run gets out of control. Don't
feel bad if you end up having to wipe it and start over several times; if you're storing all
your data in an external filesystem or an external pillar (both of which are read-only), you
won't have to do much work to get things rolling again.
Summary
In this chapter, we covered the hierarchical and failover configurations of Salt, using
syndication and multiple masters. We also talked about load testing and offloading
resources from the master to other servers. We even talked about managing the master
using the wheel system.
There are plenty of ways to help your Salt infrastructure scale to meet the demands of
thousands of servers. But once they're up-and-running, how do you keep an eye on them?
Next up, we'll discuss how we can use Salt to help monitor our systems.

11
Monitoring with Salt
Many users are unaware that Salt wasn't originally intended to be used as a configuration
management system. One of its first uses was to collect and store information on system
vitals, such as memory, CPU, and disk usage. It can still be used this way today, and in fact,
it has quite a bit more functionality now, which can be useful in monitoring. In this chapter,
we'll discuss the following topics:
Using returners to establish a historical baseline
Using monitoring states
Incorporating beacons into your workflow
Setting up alerts
Using Thorium in your monitoring systems
Monitoring basics
There are a number of different monitoring systems available today, some of which have
modules inside Salt to support them. However, different systems provide different types of
monitoring.
Establishing a baseline
Take, for example, the classic TZTTUBU monitoring package in Linux. By default, it collects
data on various system vitals every 10 minutes. Over a period of time, analysis of this data
will paint a picture of what the system looks like under normal load. Spikes or dips are
likely to occur from time to time, which may or may not be normal.

Monitoring with Salt
[ 267 ]
For instance, after monitoring a web server for a few weeks, it may become evident that
load average gradually increases throughout the morning and in the afternoon before
spiking for a few hours in the evening and dropping off before midnight. Depending on the
type of website, weekends may experience more traffic than weekdays. This will manifest
itself in tools, such as TZTTUBU. This is how the output from TZTTUBU looks:
# sar
-JOVY"3$)
EVGSFTOF@Y@@
$16
1.-*/693&45"35
$16
1.$16VTFSOJDFTZTUFNJPXBJUTUFBM
JEMF
1.BMM

1.BMM

"WFSBHFBMM

This information will form a standard baseline. Also, when unexpected abnormalities occur
that differ from the baseline, it may be a cause for concern. This is where alerting comes in,
which we'll cover in the Setting up alerts section later in the chapter.
As wonderful a tool TZTTUBU is, it doesn't tell the whole story. It reports on a predefined set
of system information, such as load average and IO wait times. It does not report on which
processes are running or how many users are logged in to the system.
Reading the system vitals in Salt
Most of the earliest modules inside Salt were designed to collect information about various
aspects of the system and return them to the user in a format that has been parsed and
formatted for ease of usability. Much of what is normally associated with system vitals is
inside the status module, but there are others as well. Let's take a look at a few of them.
status.loadavg
The TUBUVTMPBEBWH module returns load average, using the same information present in
the popular UPQ program that ships with most Unix and Linux distributions. Establishing a
baseline will help you know what specific servers normally look like. In general, so long as
the number reported in the 1-minute average is less than the total number of processors in
the system, the system is considered to be idle:
# salt myminion status.loadavg

Monitoring with Salt
[ 268 ]
NZNJOJPO

NJO

NJO

NJO

status.cpustats
Much of the information that is stored by TZTTUBU will be contained inside the return of the
TUBUVTDQVTUBUT function, although it may look a little different. Most Linux monitoring
systems, including Salt, collect information from virtual files in the QSPD directory. In this
case, the information presented is from the QSPDTUBU file. The output is likely to be
pretty long, but a shortened version would look like the following code:
# salt myminion status.cpustats
NZNJOJPO

CUJNF

DQV

JEMF

JPXBJU

JSR

OJDF

TPGUJSR

TUFBM

TZTUFN

VTFS

DQV

DQV

DQV


Monitoring with Salt
[ 269 ]
DQV

DUYU

status.meminfo
Information on memory usage is vital. Most operating systems have various strategies to
cope with running low on memory, but it is far better to not run out in the first place. The
TUBUVTNFNJOGP function will provide information about memory usage and try to be
helpful by also presenting the unit that the information is being presented in. A shortened
version looks similar to the following code:
# salt myminion status.meminfo
NZNJOJPO

"DUJWF

VOJU
L#
WBMVF

"DUJWF
BOPO

VOJU
L#
WBMVF

"DUJWF
GJMF

VOJU
L#
WBMVF

"OPO)VHF1BHFT

VOJU
L#
WBMVF

"OPO1BHFT

VOJU
L#
WBMVF


Monitoring with Salt
[ 270 ]
status.vmstats
Just as TUBUVTNFNJOGP reports on physical memory, TUBUVTWNTUBUT reports on virtual
memory. This is one of the strategies that operating systems use to cope with running out of
physical memory. In Linux, this information is pulled from QSPDWNTUBU. This is how a
shortened version of the output looks:
# salt myminion status.vmstats
NZNJOJPO

OS@BDUJWF@BOPO

OS@BDUJWF@GJMF

OS@BMMPD@CBUDI

OS@BOPO@QBHFT

OS@BOPO@USBOTQBSFOU@IVHFQBHFT

OS@CPVODF

OS@EJSUJFE

OS@EJSUZ

OS@EJSUZ@CBDLHSPVOE@UISFTIPME

OS@EJSUZ@UISFTIPME

disk.usage and status.diskusage
Disk usage is just as critical as memory usage, and in some situations, even more so. There
are two different functions inside Salt to display this information. Each behaves differently
based on where they obtain their information from. In Linux, the EJTLVTBHF function
obtains its information from the EV command, whereas the TUBUVTEJTLVTBHF function
uses information from the QSPDNPVOUT file. The command for disk usage is as follows:
# salt myminion disk.usage
NZNJOJPO



,CMPDLT

Monitoring with Salt
[ 271 ]

BWBJMBCMF

DBQBDJUZ

GJMFTZTUFN
EFWTEB
VTFE

# salt myminion status.diskusage
NZNJOJPO



BWBJMBCMF

UPUBM

status.w
The strangely-named TUBUVTX function should actually look familiar to old-school Linux
or Unix users. This calls out to the X command, which reports who is logged in to the
system and what they are doing. The command for TUBUVTX is as follows:
# salt myminion status.w
NZNJOJPO
]@

JEMF
N
KDQV

MPHJO

QDQV
T
UUZ
UUZ
VTFS
MBSSZ
XIBU
YJOJUIPNFMBSSZYJOJUSDFUD9YJOJUYTFSWFSSDWU
BVUIUNQTFSWFSBVUIU1'5W(R

Monitoring with Salt
[ 272 ]
status.all_status and status.custom
If you've been following along on your own computer and testing out these commands,
you've probably noticed that some of these functions return a lot of data. If you want a
firehose of data, try out the TUBUVTBMM@TUBUVT function, which returns everything from
the following functions:
TUBUVTDQVJOGP
TUBUVTDQVTUBUT
TUBUVTEJTLTUBUT
TUBUVTEJTLVTBHF
TUBUVTMPBEBWH
TUBUVTNFNJOGP
TUBUVTOFUEFW
TUBUVTOFUTUBUT
TUBUVTVQUJNF
TUBUVTWNTUBUT
TUBUVTX
This kind of report is useful because it returns a lot of information in only a single call.
However, chances are that it returns far more information than you actually want or need.
The TUBUVTDVTUPN function is designed to cull out the information that isn't necessary. It
returns only what is actually needed. By default, it returns nothing; you will need to specify
the functions and fields from these functions that you want to run in the minion
configuration.
To configure a function, add a line to the minion configuration, containing the name of the
function and a list of the fields that you want returned from that function. The format is as
follows:
TUBUVTGVODUJPO DVTUPN
JUFN 
JUFN 
FUD 

Monitoring with Salt
[ 273 ]
Consider the following configuration:
TUBUVTDQVTUBUTDVTUPN
	DQV	
	QSPDFTTFT	
TUBUVTMPBEBWHDVTUPN
	NJO	
This will return a custom output that looks like the following code:
# salt myminion status.custom
NZNJOJPO

NJO

DQV

JEMF

JPXBJU

JSR

OJDF

TPGUJSR

TUFBM

TZTUFN

VTFS

QSPDFTTFT

Monitoring with returners
As we discussed in the previous chapter, returners have the ability to store the job return
data from minions in an external data store. This is ideal for monitoring situations because
the external data store can be used to establish a baseline.

Monitoring with Salt
[ 274 ]
One of the best ways to set up Salt so that it starts to collect data is to use the minion's
scheduler. For our example, we'll assume that you're using the NZTRM returner. Go ahead
and add the following code to your minion configuration:
TDIFEVMF
MPBEBWH@NPOJUPSJOH
GVODUJPOTUBUVTMPBEBWH
NJOVUFT
SFUVSOFSNZTRM
EJTLVTBHF@NPOJUPSJOH
GVODUJPOTUBUVTEJTLVTBHF
NJOVUFT
SFUVSOFSNZTRM
Note that both of these have the returner set to NZTRM. If you are scheduling a lot of tasks
that use the same returner, you may just want to add a TDIFEVMF@SFUVSOFS line instead:
TDIFEVMF@SFUVSOFSNZTRM
TDIFEVMF
MPBEBWH@NPOJUPSJOH
GVODUJPOTUBUVTMPBEBWH
NJOVUFT
EJTLVTBHF@NPOJUPSJOH
GVODUJPOTUBUVTEJTLVTBHF
NJOVUFT
These configurations will set the minion to run two monitoring jobs, starting with when
TBMUNJOJPO starts up and continuing every  minutes thereafter. This time period was
chosen only because it's the default for TZTTUBU. Other monitoring software uses other
intervals, such as every  minutes. Before deciding on a time period based on what you
are used to in other software, decide whether or not this is the most appropriate interval for
your needs.
Deciding on a returner
While only a selected set of returners can be used to manage the external job cache, any
returner can be used to store the job return data. However, not all external storage
mechanisms are created equally. While the job return data will always come from Salt in
exactly the same format, it frequently needs to be massaged into something different in
order to meet the requirements of the other API.

Monitoring with Salt
[ 275 ]
NoSQL style databases are the most natural choice because they generally store data in
exactly the same format as Salt. However, not everybody uses this type of database, and in
fact, some organizations avoid them entirely.
MySQL may feel like a natural choice because it is one of the world's most popular database
servers, especially for beginners. However, its internal data format doesn't support Salt's
data structures. In order to accommodate the requirements of SQL, most SQL returners
convert the job return data to a JSON BLOB and store it in a single field. Other fields will
also be used to store metadata, but searching through data structures inside the return is
likely to be cumbersome at best.
Then, there are these special-case returners. Some, such as IJQDIBU, TMBDL, and YNQQ, are
designed to drop return data in a chat room. The TNUQ returner will send one e-mail per job
per minion with the return data. Separate integrations could be written, which make use of
these platforms, but it's definitely not a natural fit.
Lastly, there are returners designed to dump data directly to a database. This is designed
specifically for monitoring purposes. One such returner is for a piece of software called
Carbon. This is a component of the Graphite tool, which in turn can be used to generate
graphs from data such as the one that is returned from Salt.
Using monitoring states
Monitoring states are one of the less commonly known pieces of functionality inside Salt,
and that's a shame. While execution modules are superb for building and maintaining a
baseline of information about a machine, monitoring states are designed to raise a
notification when a metric falls out of the desired range.
The notification in this case is not the same as an alert. It can be used to
raise alerts, but it is an independent action.
As you may recall, there are four pieces of information that will always be returned from
each individual state:
Name
Result
Changes
Comment

Monitoring with Salt
[ 276 ]
Monitoring states differ from standard states in three ways. First of all, they are not allowed
to make changes to the system. Their job is to observe and report. Secondly, they return a
fifth piece of information:
Data
This contains a dictionary of data that was retrieved by the monitoring state. This could be a
metric involving disk usage, a particular CPU load average, or even the contents of a web
page that is being monitored.
The last difference is that when a monitoring state is called, it can be given parameters that
define what is considered acceptable for the data field. If the data falls within these
parameters or no parameters are given, the result of this state will be 5SVF. However, if
parameters are given and they fall outside what is defined to be acceptable, the result of this
state will be 'BMTF.
As monitoring states are handled during a state run, they can be used to trigger other states
to run. The triggered states may attempt to perform auto-healing or raise an alert. We'll talk
about alerts later on in the chapter; first, let's talk about how to define a monitoring state.
Defining a monitoring state
Let's take a look at a very simple monitoring state: EJTLTUBUVT. The purpose of this state
is to monitor usage on a specific filesystem. The default outputter does not show the data
output, so let's use the nested outputter instead:
[root@dufresne ~]# salt myminion state.single disk.status / --out nested
NZNJOJPO

EJTL@]@]@]TUBUVT

@@SVO@OVN@@

DIBOHFT

DPNNFOU
%JTLJOBDDFQUBCMFSBOHF
EBUB

,CMPDLT

BWBJMBCMF

DBQBDJUZ

Monitoring with Salt
[ 277 ]

GJMFTZTUFN
EFWTEB
VTFE

EVSBUJPO

OBNF

SFTVMU
5SVF
TUBSU@UJNF

If a minimum or maximum is defined as a percentage, Salt will check to see whether the
disk usage is inside this range. If it is not, it will return 'BMTF. Otherwise, it will return
5SVF:
[root@dufresne ~]# salt myminion state.single disk.status / minimum=50
maximum=90 --out nested
NZNJOJPO

EJTL@]@]@]TUBUVT

@@SVO@OVN@@

DIBOHFT

DPNNFOU
%JTLJTCFMPXNJOJNVNPGBU
EBUB

,CMPDLT

BWBJMBCMF

DBQBDJUZ

GJMFTZTUFN
EFWTEB
VTFE

EVSBUJPO

OBNF

SFTVMU
'BMTF

Monitoring with Salt
[ 278 ]
TUBSU@UJNF

Monitoring with web calls
The most unique of the monitoring states is probably IUUQRVFSZ. Rather than checking
the local system, it makes a web call and then analyzes the return from it.
There are two items that can be checked with the IUUQRVFSZ state. A match pattern may
be specified either as a block of plain text or as a regular expression. Here's an example:
IUUQFYBNQMFDPNQBHFIUNM
IUUQRVFSZ
NBUDI	5IJTJTQBHF	
IUUQFYBNQMFDPNQBHFIUNM
IUUQRVFSZ
NBUDI	5IJTJTQBHF<UXP]>	
NBUDI@UZQFQDSF
It is also possible to specify a status code. This is expected to be returned from the page.
Normally, this will be , but there are reasons to check for others. For instance, if a page
is supposed to be missing, then it is reasonable to check for a 404 page not found error. The
code for it is as follows:
IUUQFYBNQMFDPNOPU@GPVOEIUNM
IUUQRVFSZ
TUBUVT
It is also possible to check for a match pattern and a status code in the same state, as shown
in the following code:
IUUQFYBNQMFDPNKVOHMFIUNM
IUUQRVFSZ
NBUDI	8FMDPNFUPUIF+VOHMF	
NBUDI@UZQFTUSJOH
TUBUVT
Any argument that can be used with the IUUQRVFSZ execution and runner modules can
also be declared here, with two exceptions: the UFYU and TUBUVT arguments will always be
set to 5SVF because these are the items that are being checked, and the TUBUVT argument
behaves differently in the IUUQRVFSZ state.

Monitoring with Salt
[ 279 ]
In order to run a web query (which posts actual data), you would run this:
IUUQFYBNQMFDPNPSEFSQJ[[BQZ
IUUQRVFSZ
UFYUTVDDFTT
TUBUVT
NFUIPE1045
QBSBNT
UPQQJOHTQFQQFSPOJ
DSVTUQBO
This is a good time to put in a warning about the IUUQRVFSZ state. As it has no way to
check whether the parameters that are given to it are read only on the target server, it is
possible for a bad set of parameters to make changes to the target URL. It is entirely up to
the user to ensure that the parameters given are safe.
However, it is possible to run the IUUQRVFSZ state in test mode. This is another thing that
is unique about this monitoring state. Normally, monitoring states do not need to check to
see whether they are being run in test mode because they are not making any changes.
However, the IUUQRVFSZ state will allow an alternate URL to be specified, which it will
use instead if it detects that it is running in test mode.
This URL is specified as UFTU@VSM:
IUUQQSPEFYBNQMFDPNPSEFSQJ[[BQZ
IUUQRVFSZ
UFYUTVDDFTT
TUBUVT
UFTU@VSMIUUQEFWFYBNQMFDPNPSEFSQJ[[BQZ
NFUIPE1045
QBSBNT
UPQQJOHTQFQQFSPOJ
DSVTUQBO
Working with beacons
Beacons are a very new feature in Salt, but they've already gained quite a following. In past
versions of Salt, if a third-party process needed to raise an event inside Salt, it would have
to explicitly make a call to Salt to do so. Beacons overcome this by allowing events to be
triggered by third-party processes without having to perform any work inside that process
itself.

Monitoring with Salt
[ 280 ]
As you can imagine, beacons were designed for monitoring and, specifically, alerting
purposes. While monitoring states are fairly passive, in which they only run when called
explicitly or via the scheduler, beacons are very proactive, in which they are constantly
watching for changes.
Monitoring file changes
Beacons are run on a regular basis on the target minion. When they pick up important
changes, they will fire an event that describes these changes.
The first beacon that was ever added was for the JOPUJGZ system. This is built in the Linux
kernel, starting with version 2.6.13. The JOPUJGZ system can perform an operation when
certain activity happens to a file or directory. For instance, some organizations use it to
track changes to files across a set of directories and then use these change notifications to
perform incremental backups.
To use this beacon, the QZUIPOQZJOPUJGZ package must be installed on the target minion.
Once it is installed, let's go ahead and watch a file called TFSWJDFT in the UNQ directory.
Add this block to the minion configuration:
CFBDPOT
JOPUJGZ
UNQTFSWJDFT
NBTL
NPEJGZ
EFMFUF@TFMG
This will watch for UNQTFSWJDFT to be modified or deleted. This file doesn't exist yet,
but that's okay. We can still set up notifications on it. Go ahead and restart the minion, and
then start up the event listener on the master. Run the following command to put the file in
place:
# cp /etc/services /tmp
You won't see any events just yet because JOPUJGZ does not track the creation of a single
file that is being monitored. It can track the creation of a file in a directory that is being
watched, but let's stay focused on just one file at a time.
If you issue the preceding command again, it will register a change in JOPUJGZ:
# cp /etc/services /tmp

Monitoring with Salt
[ 281 ]
Go ahead and look at the event listener. You should see an event that looks similar to the
following code:
&WFOUGJSFEBU4BU+VO

5BHTBMUCFBDPONZNJOJPOJOPUJGZUNQTFSWJDFT
%BUB
\	@TUBNQ		5	
	EBUB	\	DIBOHF		*/@.0%*':		JE		NZNJOJPO		QBUI	
	UNQTFSWJDFT	^
	UBH		TBMUCFBDPONZNJOJPOJOPUJGZUNQTFSWJDFT	^
You can see a namespacing in the tag: beacon tags start with TBMUCFBDPO, followed by
the minion ID, then the name of the beacon module, and the item that is being watched.
Go ahead and delete the file with the following code:
# rm /tmp/services
Then, look at the following event listener:
&WFOUGJSFEBU4BU+VO

5BHTBMUCFBDPONZNJOJPOJOPUJGZUNQTFSWJDFT
%BUB
\	@TUBNQ		5	
	EBUB	\	DIBOHF		*/@%&-&5&@4&-'	
	JE		NZNJOJPO	
	QBUI		UNQTFSWJDFT	^
	UBH		TBMUCFBDPONZNJOJPOJOPUJGZUNQTFSWJDFT	^
The tag hasn't changed, but the data has. In the case of this beacon, we're interested in what
the change item in the data dictionary contains.
Beacon intervals
By default, beacons are run once every second. As a result, they need to be quite light, and
they perform their work as quickly as possible. However, you may not want the beacons to
run this often. Say you're using the load beacon to keep an eye on the load average for a
system. You've decided that you don't need a check every second, but that every  seconds
is reasonable.

Monitoring with Salt
[ 282 ]
You can change the beacon interval with the JOUFSWBM argument. For our example, you can
configure the load beacon with the following code:
CFBDPOT
MPBE
N


JOUFSWBM
This beacon will fire an event if the 1-minute load average ever drops below zero or goes
above .
Setting up alerts
Now that you've seen various ways to monitor minions, let's go ahead and set up some
alerts.
Alerting in state files
In $IBQUFS, Managing Tasks Asynchronously, we discussed how to use the reactor system
to file incidents in the 1BHFS%VUZ service in response to events. Our example also made use
of the EJTLTUBUVT monitoring state.
Keep in mind that any state inside an SLS file can raise an alert;
monitoring states are not alone here.
Alerting from beacons
As beacons are designed to do nothing more than send an event when a certain threshold is
reached, they are perfect for alerting purposes! Let's go ahead and set up a couple of
examples.

Monitoring with Salt
[ 283 ]
Watching file changes
Let's go back to JOPUJGZ for a moment. Say that you're using the FUDIPTUT file to
manage local DNS lookups. You may have some software that manages this file for you,
perhaps automatically adding entries as necessary; you want to be notified via e-mail when
this happens.
First off, the minion needs to be properly configured to send e-mails. The minion ID in this
example is called TNUQNJOJPO. Add the appropriate values to your minion configuration:
NZTNUQMPHJO
TNUQTFSWFSTNUQFYBNQMFDPN
TNUQUMT5SVF
TNUQTFOEFSMBSSZ!FYBNQMFDPN
TNUQVTFSOBNFMBSSZ
TNUQQBTTXPSEQBTT
Then, add a beacon to keep an eye on this file:
CFBDPOT
JOPUJGZ
FUDIPTUT
NBTL
NPEJGZ
Go ahead and restart the TBMUNJOJPO process. Then, we'll need to set up a reactor on the
master. Go ahead and add the reactor mapping with the following code:
SFBDUPS
TBMUCFBDPOEVGSFTOFJOPUJGZFUDIPTUT
TSWSFBDUPSIPTUT@DIBOHFTTMT
Finally, create the TSWSFBDUPSIPTUT@DIBOHFTTMT file with the following content:
IPTUT@DIBOHFE
DNETNUQTFOE@NTH
UHUTNUQNJOJPO
LXBSH
SFDJQJFOUMBSSZ!FYBNQMFDPN
NFTTBHF)PTUT'JMF$IBOHFEPO\\EBUB<	JE	>^^
TVCKFDU)PTUT'JMF$IBOHFEPO\\EBUB<	JE	>^^
QSPGJMFNZTNUQMPHJO
Restart the TBMUNBTUFS process. Go ahead and add an entry to your FUDIPTUT file on
this minion, and check your e-mail.

Monitoring with Salt
[ 284 ]
Monitoring bad logins
Most Linux users don't keep a watchful eye on the CUNQ file, which is unfortunate. This file
keeps track of failed login attempts on the system. On a public-facing system, this can mean
serious trouble. However, in recent years, it has become common practice for attackers to
make several dozen, or even several hundred, attempts to log in to a system at once. Using
SMTP for alerts is probably a bad idea because your inbox may get flooded from a single
attack run.
So, let's go ahead and set up our alert system to send a webhook that reports on bad logins
instead.
If you were to fire up the event listener and then attempt a bad login to the system, you
would see something similar to the following code:
&WFOUGJSFEBU4VO+VO

5BHTBMUCFBDPOEVGSFTOFCUNQ
%BUB
\	@TUBNQ		5	
	EBUB	\	1*%	
	BEES	
	FYJU@TUBUVT	
	IPTUOBNF			
	JE		EVGSFTOF	
	JOJUUBC			
	MJOF		QUT	
	TFTTJPO	
	UJNF	
	UZQF	
	VTFS		DVSMZ	^
	UBH		TBMUCFBDPONZNJOJPOCUNQ	^
Now that we know what the event looks like, we can set up an alert for it. First, we'll set up
the beacon in the minion configuration:
CFBDPOT
CUNQ\^
The configuration for this beacon is very simple; as it requires no arguments to tell it to look
at the CUNQ file, an empty configuration block is used. Go ahead and restart the TBMU
NJOJPO process.

Monitoring with Salt
[ 285 ]
Then, we will set up the reactor mapping on the master, as follows:
SFBDUPS
TBMUCFBDPOEVGSFTOFCUNQ
TSWSFBDUPSCUNQTMT
Finally, we will create TSWSFBDUPSCUNQTMT with the following content:
CUNQ@BMFSU
SVOOFSIUUQRVFSZ
LXBSH
VSM	IUUQFYBNQMFDPNBMFSUTQZ	
NFUIPE1045
QBSBNT
JE\\EBUB<	JE	>^^
VTFS\\EBUB<	VTFS	>^^
Using aggregate data with Thorium
Because Thorium can store data in its register, you can keep track of a certain amount of
aggregate data and perform operations based on it. Let's take a look at a classic example:
load average. We'll use the MPBE beacon to keep track of load on a minion and fire events
when it falls outside the bounds configured for it. Then we'll have Thorium perform
calculations and react when another threshold is reached.
First, we need to set up the MPBE beacon. Let's go ahead and use the configuration that we
looked at earlier in the chapter, with one addition. Add the following configuration to your
NJOJPO file:
CFBDPOT
MPBE
N


JOUFSWBM
PODIBOHFPOMZ5SVF
Setting PODIBOHFPOMZ to 5SVF will tell the beacon not to fire any events unless the
thresholds that we have configured have been passed. This will cut down on the amount of
traffic that is sent to the master.

Monitoring with Salt
[ 286 ]
Then we'll set up a Thorium SLS file to take a look at those beacons as they come in. Go
ahead and create TSWUIPSJVNMPBETMT with the following content:
MPBEBWH
SFH
MJTU
BEEN
NBUDI	TBMUCFBDPOMPBE	
QSVOF
GJMF
TBWF
This will store 1-minute load averages in a register called MPBEBWH. Because this register
uses a list, we can tell Thorium to keep it pruned to only save the last 10 entries. As before,
GJMFTBWF is only there so that we can watch what's happening.
This takes care of populating the register, but now we need to do something with it. There
is a Thorium module called DBMD that can perform statistical calculations on a list of data in
a register. It supports the following functions:
BEE: Add the last x values together
NVM: Multiply the last x values together
NFBO: Calculate the mean of the last x values
NFEJBO: Calculate the median of the last x values
NFEJBO@MPX: Calculate the low median of the last x values
NFEJBO@IJHI: Calculate the high median of the last x values
NFEJBO@HSPVQFE: Calculate the grouped median of the last x values
NPEF: Calculate the mode of the last x values
For now, we'll just look at the mean (or average) of the last  values in the register:
BWFSBHF
DBMDNFBO
OBNFMPBEBWH
OVN
NBYJNVN
SFGN
The OVN value is the number of entries to look at, and NBYJNVN is the maximum of the
mean. Because the MPBE beacon can return N, N, and N, we have directed Thorium to
reference (SFG) only the N value.

Monitoring with Salt
[ 287 ]
This will perform the calculations that we need, but we still need to actually do something
when the NFBO exceeds the threshold we have set. If you specify a NJOJNVN or NBYJNVN
value for any of the DBMD functions and that value is exceeded, then the code block will
return 'BMTF.
Because Thorium supports state file logic, we can add one more code block to perform an
operation when that threshold is exceeded:
BMFSU
SVOOFSDNE
GVOQBHFSEVUZDSFBUF@FWFOU
POGBJM@JO
DBMDBWFSBHF
LXBSHT
EFTDSJQUJPO)JHI-PBE"WFSBHF
EFUBJMT4BMU)BT%FUFDUFE)JHI-PBE"WFSBHF
TFSWJDF@LFZ"#$%&'BCDEF
QSPGJMFNZQBHFSEVUZBDDPVOU
You may recall our discussion of PagerDuty from $IBQUFS, Managing Tasks
Asynchronously. The call really isn't much different inside of Thorium: we use SVOOFSDNE
and GVOQBHFSEVUZDSFBUF@FWFOU instead of SVOOFSQBHFSEVUZDSFBUF@FWFOU,
and we specify LXBSHT instead of LXBSH.
Of course, if you're going to use PagerDuty, make sure to configure it in your master file:
NZQBHFSEVUZBDDPVOU
QBHFSEVUZTVCEPNBJONZTVCEPNBJO
QBHFSEVUZBQJ@LFZ"#$%&'
Summary
Establishing a historical baseline of data is critical to monitoring systems, and Salt has been
using returners to do this since the beginning. You can also use monitoring states and
beacons to collect data and perform actions based on it.
Salt has a very powerful set of tools to monitor systems in order to establish a baseline of
information and raise alerts when something goes awry.
Thorium can be used to collect aggregate data and perform operations based on statistical
analysis of it.
In the next chapter, we'll take a look at some of the best and worst practices in Salt.

12
Exploring Best Practices
Like all tools, Salt is easier to use and gives more consistent results when you use it right.
Referring to a piece of equipment in his kitchen, a chef once told me, cI've seen a number of
really ingenious ways to use this tool wrongd. This chapter aims to give guidance to help
you use Salt in the best way possible. We'll cover the following topics:
Future-proofing your infrastructure
Establishing a proper directory structure
Creating efficient SLS files
Using intuitive naming conventions
Using effective variables with templates
Future-proofing your infrastructure
One of the most aggravating things about technology is its ability to change before you can
adapt, and in some cases, before you even finish implementing what was considered new
when you started. Future-proofing refers to planning things out as far in advance as
possible. It also refers to working in a way that minimizes the amount of work that will
have to be done in the future to make the current technology still work.
One of the most famous examples of code that was not future-proofed was the Year 2000 or
the Y2K bug. For those who missed it, here's what happened. Developers needed to store
dates. It was already common usage outside computers to store two-digit years. For
example, January 1, 1970 may be stored as 1/1/70. Using a two-digit year saved space, which
was at a premium at the time.

Exploring Best Practices
[ 289 ]
Unfortunately, far more of the code that used this strategy survived into the future than was
expected. Even worse, even some code written in the 1990s was still designed to store two-
digit years instead of four-digit years.
It would have been far better to store four-digit years in the first place. In fact, a date stored
as YYYYMMDD (including zero-padded two-digit months and days, such as 01) can be
considered future-proof for a very long time; it has the added bonus of being easier to sort
chronologically. Storing timestamps in a 24-hour mode (13:00 instead of 1:00 PM) is a
similar strategy.
We will come back to future-proofing regularly in this chapter. It's something that should
always be in the back of your mind when you work on any technology.
Setting up your directories
A good directory structure is important in any platform, and Salt is no different. The default
placement of directories inside Salt was very carefully considered in order to maintain the
best balance between the Filesystem Hierarchy Standard (FHS), Linux Standard Base
(LSB), and various nuances between different Linux distributions.
As a user, you have a number of directories to contend with yourself, especially when
planning both your state files and your pillar files. There's no official standard inside these
directories, but there are some things that you can do to keep your directory trees in good
order.
Standard directory locations
Most Linux distributions place files directly in their appropriate directories. Configuration
files and directories live in FUD files, whose content is variable (logs, caches, and so on)
and belong directly in WBS, and site-specific files that belong to a network server often go
in TSW (although this can change depending on your environment). However, many Unix
and some Linux distributions prefer to extend this structure by adding a MPDBM directory.
If you are currently using one of these operating systems, you're probably already used to
its conventions.

Exploring Best Practices
[ 290 ]
One of the design decisions behind Salt was to use as few directories as possible. It's not
reasonable to force users to look all over their system for files, and Salt strives to be
reasonable. Again, some of these locations will differ depending on your platform, but the
locations that Salt normally uses are:
VTSCJO : This specifies executables
VTSMJCQZUIPOWFSTJPO TJUFQBDLBHFTTBMU: This denotes the bulk
of Salt code
FUDTBMU: This specifies the configuration and key files
WBSMPHTBMU: This denotes log files
WBSDBDIFTBMU: This specifies the cache data
WBSSVOTBMU: This denotes socket files
TSWTBMU: This specifies state files
TSWQJMMBS: This specifies pillar files
TSWSFBDUPS: This denotes reactor files
Most of these directories will never even be seen by the average user. They keep themselves
in check when necessary and require no maintenance or modifications by most
administrators.
The files in the TSW directories are what most Salt users will look at and modify on a
regular basis. It is possible to change any one of these directories both in and out of the
TSW directories, but resist the temptation. Users are used to finding these files here. Also,
moving to a nonstandard location adds an unnecessary layer of confusion to new hires and
consultants working with you.
.sls versus init.sls
Both the TSWTBMU and TSWQJMMBS directories may include a UPQTMT file, any
number of other SLS files, directories that contain an JOJUTMT file, and optionally, other
SLS files and directories. Both of these files will refer to a state called "QBDIF:
TSWTBMUBQBDIFTMT
TSWTBMUBQBDIFJOJUTMT

Exploring Best Practices
[ 291 ]
While it is perfectly acceptable to use either, consider what may happen further down the
road. Apache is the sort of service that doesn't just get installed and started; chances are you
will be modifying at least one configuration file. Therefore, it makes more sense to create a
directory called BQBDIF with its own JOJUTMT file and keep Apache-related files in there.
But what about other services? NTP is often the sort of thing that comes with perfectly good
configuration out of the box. It does not often need modification. However, it can be
modified, and the more mature your infrastructure becomes, the more likely it is that you
will need to customize something.
When that time comes, changing OUQTMT to OUQJOJUTMT may only take a couple of
extra steps. However, these are steps that can be saved up front. Also, certain other
procedures in your organization may necessitate even more work. For instance, do you
have other software, such as backup or security solutions, which is expecting files to be in a
specific place? Skip NPEVMF TMT and maintain a policy of segregating states and pillars
into their own directories up front.
Shallow versus deep
A number of different organization mindsets can be found in the modern world of system
administration. Some people like to be very specific when they organize data, creating
kingdoms, classes, and phylums, as if they were keeping track of plant and animal species.
For example, the three most popular text editors in the Linux world are arguably vim,
emacs, and nano. Two popular graphics editors in the open source world are GIMP and
Inkscape. When you manage an infrastructure that includes all of these programs, it may be
tempting to start classifying like a scientist, but this can quickly get out of hand.

Exploring Best Practices
[ 292 ]
Take a look at this directory structure:
5IJTJTTPNFXIBUPGBEFFQEJSFDUPSZTUSVDUVSF5IJTLJOEPGTUSVDUVSFNBZ
CFBFTUIFUJDBMMZQMFBTJOHUPUIFPCTFTTJWFMZPSHBOJ[FECVUJUBEETFYUSB
DPNQMFYJUZGPSPUIFSTUPIBWFUPEFBMXJUI'JOEJOHJOXIJDIEJSFDUPSJFT
GJMFTMJWFJOUBLFTFYUSBTUFQTXIFSFBTUIFDMBTTJGJDBUJPOPGTPNF
TPGUXBSFDBOCFBNCJHVPVT%JEUIFPSJHJOBMNBJOUBJOFSQVUWJNJO
DPSF@UPPMTPSEFW@UPPMT
Let's take a look at a different directory structure:
5IJTJTBNPSFTIBMMPXEJSFDUPSZTUSVDUVSF*U	TOPUTPTIBMMPXUIBUXF
XPVMESFTPSUUPWJNTMTFNBDTTMTBOETPPOCVUJUJTTIBMMPXFOPVHI
UIBUTUBUFTDBOFBTJMZCFGPVOEKVTUCZMPPLJOHJOUIFTSWTBMU
EJSFDUPSZ

Exploring Best Practices
[ 293 ]
Subdividing further
Descending further into these directories, you will find a number of different techniques to
organize files specific to one package. Some prefer to use a GJMFT directory, while others
just dump all the files for an SLS in one directory.
In most cases, putting all of your files in one directory will be the easiest to work with. The
shallow structure will make them easy to find and modify.
However, if a full directory structure is to be copied to a minion, it makes absolutely no
sense to remain completely shallow. Move those files to their own directory and use the
GJMFSFDVSTF state to provision them on the minion. The shallow structure is only a
guideline; it is up to you to decide what makes the most sense for your situation.
The SLS efficiency
When building an SLS tree, the directory structure is only part of the equation. There are a
number of strategies that can be employed in the SLS files, which will increase their ease of
use and maintainability.
Includes and extends
Like a number of modern languages and file formats, SLS files were designed to take
advantage of code reuse. Rather than creating large, monolithic files, states can be broken
down into smaller files, which can be combined together across multiple environments.
Consider the following partial SLS file:
JQUBCMFT
TFSWJDF
EFBE
IUUQE
QLH
JOTUBMMFE
TFSWJDF
SVOOJOH
PQUDPEFCBTF
GJMFSFDVSTF
TPVSDFTBMUDPEFCBTFGJMFT

Exploring Best Practices
[ 294 ]
Obviously, a production version of this would be far longer, but this short version fits our
needs.
There are three distinct components of this SLS: the firewall, the web server, and the code
base. There is an implied order here: the web server can't serve pages if the firewall is
blocking it, and the code base is useless without the web server to connect it to users.
The first problem with this SLS is that each component should be broken down into
individual files. This state tree is likely to grow, and other components may be added,
which takes advantage of the firewall configuration or the web server.
The second problem is that managing the firewall by flat out disabling it is neither
extensible nor future-proof. However, we'll talk about that in a moment.
Using includes
Let's go ahead and break this SLS into three separate files:
# cat /srv/salt/firewall/init.sls
JQUBCMFT
TFSWJDF
EFBE
# cat /srv/salt/webserver/init.sls
IUUQE
QLH
JOTUBMMFE
TFSWJDF
SVOOJOH
# cat /srv/salt/codebase/init.sls
JODMVEF
GJSFXBMM
IUUQE
PQUDPEFCBTF
GJMFSFDVSTF
TPVSDFTBMUDPEFCBTFGJMFT
See the names that we have used for our SLS files. It may be natural for Linux users to refer
to the firewall configuration as JQUBCMFT, but it's not future-proof. A number of users have
found themselves in situations where they suddenly need to support new platforms that
they did not originally plan for. Mergers or partnerships between companies and directives
from upper levels of management may dictate the use of other firewalls, such as the QG
firewall program in BSD or the Windows firewall system. Referring to this SLS as firewall
will help simplify these changes when necessary.

Exploring Best Practices
[ 295 ]
The same goes for the web server, which is currently set to use IUUQE: the package and
service name for Apache on certain Linux platforms. But what happens if the infrastructure
switches from a Red Hat-based platform to one based on Debian? Or if a change is made
from Apache to Nginx?
In such cases, using more generic names will simplify further changes to the infrastructure.
Detailed configuration is still possible of course, but it can be localized to the individual SLS
files that it pertains to, instead of affecting the set of files as a whole.
You may also have noticed that the code base SLS file includes the firewall and the web
server SLS files. Why not have the web server include the firewall and the code base only
include the web server? To answer this, let's expand our code base SLS file:
# cat /srv/salt/codebase/init.sls
JODMVEF
GJSFXBMM
IUUQE
JOTUBMMFE@DPEFCBTF
GJMFSFDVSTF
TPVSDFTBMUDPEFCBTFGJMFT
OBNFPQUDPEFCBTF
DPEFCBTFXFCDPOGJH
GJMFNBOBHFE
TPVSDFTBMUDPEFCBTFBQBDIFDPOG
OBNFFUDIUUQEDPOGEDPEFCBTFDPOG
Remember that other components may be added later, which make use of the web server,
but which include their own configuration files that are specific to that web server.
In this case, it's okay that we put "QBDIF in the name of the configuration file. In fact, it's
preferable so that we know that we're not dealing with an Nginx configuration file. We've
referred to the block of code that handles the web server configuration generically so that if
we need to change to Nginx later, we only need to change it inside this block, and not inside
the references to this block.
Speaking of generic names, we've also changed the state that handles the code base itself to
have a more generic name; just in case PQUDPEFCBTF is changed at a later point
toTSWDPEFCBTF for example.

Exploring Best Practices
[ 296 ]
Using extends
There's a little more work that we can do in the code base SLS file. Our current example
assumes that no changes need to be made to the default Apache configuration file; the file
that appears in the DPOGE directory should be enough.
However, this may not in fact be the case. In fact, Apache is historically handled very
differently across various platforms. While Red Hat's Apache automatically includes
FUDIUUQEDPOGEDPOG by default, Arch Linux does not; the IUUQEDPOG file in that
platform does not include a number of the DPOG files that ship with it by default. It will
need to be updated. Also, with Debian-based Apache installations, the entire directory
structure is different.
We will go ahead and make changes to the SLS files for both the XFCTFSWFS and the
DPEFCBTF:
# cat /srv/salt/webserver/init.sls
IUUQE
QLH
JOTUBMMFE
TFSWJDF
SVOOJOH
GJMFNBOBHFE
TPVSDFTBMUXFCTFSWFSIUUQEDPOG
OBNFFUDIUUQEDPOGIUUQEDPOG
# cat /srv/salt/codebase/init.sls
JODMVEF
GJSFXBMM
IUUQE
FYUFOE
IUUQE
GJMF
TPVSDFTBMUDPEFCBTFIUUQEDPOG
JOTUBMMFE@DPEFCBTF
GJMFSFDVSTF
TPVSDFTBMUDPEFCBTFGJMFT
OBNFPQUDPEFCBTF
DPEFCBTFXFCDPOGJH
GJMFNBOBHFE
TPVSDFTBMUDPEFCBTFDPEFCBTFBQBDIFWIPTUDPOG
OBNFFUDIUUQEDPOGEDPEFCBTFDPOG
We've added a generic configuration file to the web server configuration, which will
automatically be included along with all the other Apache code blocks in the web server
SLS file. However, we are then modifying the source of this file so that it points to a specific
IUUQEDPOG file in the DPEFCBTF directory.

Exploring Best Practices
[ 297 ]
Using IUUQEDPOG as the filename makes more sense because it's the upstream name for
that file. It is also the name that is used on pretty much every non-Debian distribution.
However, having both an IUUQEDPOG and an BQBDIFDPOG file in the same directory can
be confusing. The BQBDIFDPOG file is likely to contain little more than a 7JSUVBM)PTU 
directive that is specific to the codebase. We can see now that while "QBDIF should
probably still be in the filename, it would have been more future-proof to give more
information in the filename on the server. Fortunately, as we've already used generic names
for the block that controls this file, we only need to make two changes: one to the SLS file
and one to rename the file itself.
Using templates to simplify SLS files
Using the built-in JODMVEF and FYUFOE blocks is helpful to tie files together, but it doesn't
help much in the files. This is where templates can really shine. We can take advantage of
them to shorten some code blocks or decide whether or not the state compiler will actually
see them in the first place.
Working with loops
There are times when using a loop inside a template can seem helpful. For instance,
managing a group of users, all of which have identical settings and permissions:
\GPSVTFSJO
	MBSSZ		DVSMZ		NPF	^
\\VTFS^^
VTFSQSFTFOU
\FOEGPS^
This Jinja code block will effectively create the following SLS to be sent through the state
compiler:
MBSSZ
VTFSQSFTFOU
DVSMZ
VTFSQSFTFOU
NPF
VTFSQSFTFOU

Exploring Best Practices
[ 298 ]
However, while using template loops can save a lot of time and tedium in some places,
there are other places where it's just not appropriate. For instance, take the following SLS
snippet to install CloudStack:
\GPSQLHJO
	DMPVETUBDLBHFOU		DMPVETUBDLNBOBHFNFOU	^
\\QLH^^
QLHJOTUBMMFE
\FOEGPS^
It may look like a quick way to install packages without creating a single block per package,
but Salt already has its own ways to handle this. As each package (presumably) has
identical settings, we can create a single block to include them all:
DMPVETUBDLQLHT
QLHJOTUBMMFE
OBNFT
DMPVETUBDLBHFOU
DMPVETUBDLNBOBHFNFOU
However, the QLH state is special because it supports the state aggregation. With either of
the preceding code blocks, the minion's package manager will be called once per
declaration. With a large list of packages, this can quickly grow to be much too long of a list.
Using QLHT instead of OBNFT will cause Salt to aggregate these package names together and
call the minion's package manager only once to deal with all the declared packages.
The user state doesn't support this kind of aggregation, but it does support using the OBNFT
argument. The following code is a much simpler version of the preceding SLS for users:
NZVTFST
VTFSQSFTFOU
OBNFT
MBSSZ
DVSMZ
NPF

Exploring Best Practices
[ 299 ]
It may be starting to look like loops have no place inside the SLS files. That's not true. While
the OBNFT and QLHT arguments alleviate the need for loops inside a single state, it may be
helpful to use loops to handle repetitive code across multiple states. Let's modify our user
SLS to include a TBOECPY for each user:
\GPSVTFSJO
	MBSSZ		DVSMZ		NPF	^
\\VTFS^^
VTFSQSFTFOU
TSWTBOECPY\\VTFS^^
GJMFEJSFDUPSZ
SFRVJSF
VTFS\\VTFS^^
\FOEGPS^
This block goes beyond just managing users. It creates a directory for each user, but not
until making sure that the user exists first.
There is no functional difference between using names and using a loop to add users; unlike
with packages, the VTFSBEE command will be called once per user. However, without
using a template, there is no way to create the kind of dependency between the two states
that we have used before.
Decisions, decisions
There are times when we need our SLS files to make decisions based on certain aspects of
the minion. In the programming terminology, this is often called branching.
Let's go back to our code base example. At the moment, we are just killing the firewall in
order to open up the web port or ports that we need. However, this is sloppy at best. It is far
better to maintain a firewall with all the ports closed, except for the ones that are needed.
We'll assume for now that we're working with Red Hat-based minions, which store their
firewall configuration in the FUDTZTDPOGJHJQUBCMFT file. Rather than shutting down
the firewall, we'll take a look at the role of the minion, as declared in a HSBJO, and lay down
the appropriate file:
# cat /etc/salt/grains
SPMFXFCTFSWFS
# cat /srv/salt/firewall/init.sls
GJSFXBMMDPOGJHVSBUJPO
GJMFNBOBHFE
\JGHSBJOT<	SPMF	>	XFCTFSWFS	^
TPVSDFTBMUGJSFXBMMXFCTFSWFSJQUBCMFT
\FMTF^

Exploring Best Practices
[ 300 ]
TPVSDFTBMUGJSFXBMMXFCTFSWFSEFGBVMU
\FOEJG^
OBNFFUDTZTDPOGJHJQUBCMFT
Looking at this example, you may be considering that an JG/FOEJG block is perhaps
unnecessary; after all, since the value of the SPMF grain is also used in the filename, we
could perhaps just refer to the variable name there instead:
# cat /srv/salt/firewall/init.sls
GJSFXBMMDPOGJHVSBUJPO
GJMFNBOBHFE
TPVSDFTBMUGJSFXBMM\\HSBJOT<	SPMF	>^^JQUBCMFT
OBNFFUDTZTDPOGJHJQUBCMFT
However, this is not a future-proof solution. What happens if a minion does not have an
associated firewall file on the server? Also, what if the SPMF grain has not been defined yet
for this minion? Either will cause errors to appear in the state runs on these minions.
Rather than ignoring all but those who are explicitly set up, it is better to define what we
can and set defaults for everybody else:
# cat /srv/salt/firewall/init.sls
GJSFXBMMDPOGJHVSBUJPO
GJMFNBOBHFE
\JGHSBJOTHFU
	SPMF		XFCTFSWFS	^
TPVSDFTBMUGJSFXBMMXFCTFSWFSJQUBCMFT
\FMTF^
TPVSDFTBMUGJSFXBMMEFGBVMUJQUBCMFT
\FOEJG^
OBNFFUDTZTDPOGJHJQUBCMFT
We can take this one step further and ensure that this state is only executed on minions that
are actually running Linux and therefore have JQUBCMFT available:
# cat /srv/salt/firewall/init.sls
\JGHSBJOT<	PTDPEFOBNF	>	-JOVY	^
GJSFXBMMDPOGJHVSBUJPO
GJMFNBOBHFE
\JGHSBJOTHFU
	SPMF		XFCTFSWFS	^
TPVSDFTBMUGJSFXBMMXFCTFSWFSJQUBCMFT
\FMTF^
TPVSDFTBMUGJSFXBMMEFGBVMUJQUBCMFT
\FOEJG^
OBNFFUDTZTDPOGJHJQUBCMFT
\FOEJG^

Exploring Best Practices
[ 301 ]
Using the built-in states
In the previous example, we manually laid down files for the JQUBCMFT configuration. In
order to see what's going on, we'll actually have to view the files directly. This also makes
the files somewhat rigid.
There are a number of configuration file formats that are natively supported in Salt. When
this is the case, it is often easier to manage components of those files directly in states, rather
than create large, monolithic files with less visibility.
JQUBCMFT is a great example too because like packages, the JQUBCMFT state supports
aggregation. This means that a number of components throughout the SLS tree can define
their own JQUBCMFT rules, and when a high state is run, they will all be aggregated together
into a single JQUBCMFT configuration file.
Let's switch our firewall SLS from laying down monolithic configuration files to generating
the framework of a stateful set of firewall rules, as shown in the following code:
# cat /srv/salt/firewall/init.sls
*/165
JQUBCMFTDIBJO@QSFTFOU
UBCMFGJMUFS
GBNJMZJQW
JOQVU@QPMJDZ
JQUBCMFTTFU@QPMJDZ
DIBJO*/165
QPMJDZ%301
SFRVJSF
JQUBCMFT*/165
JDNQ@BDDFQU
JQUBCMFTJOTFSU
UBCMFGJMUFS
DIBJO*/165
KVNQ"$$&15
QSPUPJDNQ
QPTJUJPO
SFRVJSF
JQUBCMFT*/165
MP@BDDFQU
JQUBCMFTJOTFSU
UBCMFGJMUFS
DIBJO*/165
KVNQ"$$&15
JGMP
QPTJUJPO
SFRVJSF

Exploring Best Practices
[ 302 ]
JQUBCMFTJDNQ@BDDFQU
TUBUF@USBDLJOH
JQUBCMFTJOTFSU
UBCMFGJMUFS
DIBJO*/165
KVNQ"$$&15
NBUDIDPOOUSBDL
DUTUBUF3&-"5&%&45"#-*4)&%
QPTJUJPO
SFRVJSF
JQUBCMFTMP@BDDFQU
EFGBVMU@SVMF
JQUBCMFTBQQFOE
UBCMFGJMUFS
DIBJO*/165
KVNQ3&+&$5
SFKFDUXJUIJDNQQSPUPVOSFBDIBCMF
SFRVJSF
JQUBCMFTTUBUF@USBDLJOH
This is too much to handle, so let's break it down.
As JQUBCMFT is based on chains, we start off by making sure that the chain that we need is
present. The */165 chain in the filter table is normally built-in, but it doesn't hurt to be
explicit. The default policy for this chain is "$$&15, and for our purposes, we will change it
to %301, meaning that if a network packet does not match any of the rules, it will simply be
ignored.
Then, we will set up a series of rules that make JQUBCMFT behave in a stateful manner,
which, in this case, is not a Salt term; it refers to the ability of JQUBCMFT to track the
connection state. The rules that we have set up allow all traffic that originates from the
minion's local network interface and all traffic that uses the ICMP network protocol.
The next rule checks the network packet to see whether it belongs to a connection that has
already been established, or is related to a connection that has already been established. In
either case, it is assumed that the connection has already been validated and no further
checking is required.
The last rule tells JQUBCMFT to reject any network traffic that has not matched any rules in
the chain. With this current definition, no outside traffic will be allowed to this minion.
The first three rules need to exist in a specific order, so their position has been explicitly
declared using the JQUBCMFTJOTFSU state. The last rule needs to appear at the end of the
chain. Since JQUBCMFTBQQFOE does just this, it's perfect here.

Exploring Best Practices
[ 303 ]
However, we still need to open up the firewall ports for the web server. We'll go ahead and
add these rules to the code base SLS because other components in the state tree, which use
the web server, may require different ports:
# cat /srv/salt/codebase/init.sls
JODMVEF
GJSFXBMM
IUUQE
FYUFOE
IUUQE
GJMF
TPVSDFTBMUDPEFCBTFIUUQEDPOG
JOTUBMMFE@DPEFCBTF
GJMFSFDVSTF
TPVSDFTBMUDPEFCBTFGJMFT
OBNFPQUDPEFCBTF
DPEFCBTFXFCDPOGJH
GJMFNBOBHFE
TPVSDFTBMUDPEFCBTFDPEFCBTFBQBDIFWIPTUDPOG
OBNFFUDIUUQEDPOGEDPEFCBTFDPOG
QPSUGJSFXBMM
JQUBCMFTJOTFSU
UBCMFGJMUFS
DIBJO*/165
KVNQ"$$&15
NBUDITUBUF
DPOOTUBUF/&8
EQPSU
QSPUPUDQ
TBWF5SVF
QPTJUJPO
SFRVJSF
JQUBCMFTEFGBVMU@SVMF
As before, the codebase SLS requires the firewall state. However, it now explicitly requires
the last rule (the one that rejects unidentified traffic) to be run before it applies its own rule.
This is because of the position that has been specified.
Salt's JQUBCMFTJOTFSU state allows a negative position to be declared. This is not a
feature that has been built-in JQUBCMFT itself; it's a convenience feature that Salt added for
this exact use case. When a negative number is declared, Salt will start at the end of the
chain and start counting backwards. The last rule is position  (which should be declared
with JQUBCMFTBQQFOE, not JQUBCMFTJOTFSU). The rule before it is position , the rule
before that is , and so on.

Exploring Best Practices
[ 304 ]
This allows a default rule to be set at the very end of a chain and other rules to be set before
it. The advantage is that users don't need to manually track all of their rules in their SLS tree
and then set the final rules to explicitly happen last.
There is one more trick that the JQUBCMFT state module has up its sleeve. Recent versions of
JQUBCMFT are able to check to see whether a rule already exists in a chain, and if so, do not
try to add it again. This allows states to declare JQUBCMFT rules, which are both stateful to
JQUBCMFT and stateful in terms of Salt.
Naming conventions
An important aspect of the SLS organization is a sensible naming structure. As we have
seen, when components are named generically, it is less likely that they will need to be
renamed at a later point. However, when a component is named explicitly, it is more likely
that a user who is unfamiliar with the SLS tree will understand what it is trying to
accomplish.
A good naming convention strives to strike a balance between the oil and water of generic
versus explicit. To borrow from the food and chemistry worlds, a good naming convention
is the emulsifier that binds everything in a recipe or formula together.
Generic names
Before starting out with an SLS tree, let's try to plan out as many of the primal components
as possible. As an example, a modern infrastructure may reasonably include the following
components:
A load balancer
A database server
A web server
A firewall
An application code base
An e-mail server

Exploring Best Practices
[ 305 ]
We will start with names that reflect these primal components before moving on to sub
primal components. The code for the same is as follows:
Some of these primal components may be broken down into smaller components. For
instance, an organization may include both a web application server and a static content
web server, as shown in the following code:
Each of the BQQ@XFCTFSWFS and TUBUJD@XFCTFSWFS SLS files will include the web server
SLS and make their own additions and modifications to it. You can also make use of
premade formulas on the TBMUTUBDLGPSNVMBT repository on GitHub, as shown in the
following URL: IUUQTHJUIVCDPNTBMUTUBDLGPSNVMBT

Exploring Best Practices
[ 306 ]
If this is the case, then you probably have the web server SLS, which includes one or more
of these as well:
Explicit names
With generically named directories, it is important that the files in these directories are as
explicit as possible, but not to the point of being unusable. The following filename is
probably okay:
BQBDIFBDPSOWIPTUDPOG
However, this filename is just too much:
BQBDIFQSPKFDU@BDPSO@DPEFCBTFWJSUVBMIPTUTOPO@TTMDPOG
When naming files like this, try to be reasonably lazy. If a file is in a directory that implies
its purpose, then perhaps this directory name doesn't need to appear again in the file, unless
doing so would add needed clarity. However, if making a filename slightly more explicit
avoids confusion down the road, then it's worth being explicit now.
Templates and variables
When we're talking about the balance between generic and explicit, we'd better talk about
variable names. A good variable name will also strike this balance and become an
emulsifier between the minion and the state tree.

Exploring Best Practices
[ 307 ]
Nested variables
Salt allows hierarchical data structures to be used in order to define variables. In a way, this
allows variables to behave like directories, in which these structures may be either shallow
or deep.
However, unlike directory structures, searching through deep variable structures is not
necessarily as painful. When you use flat files to define a structure, it may actually be easier
to read a structure that is deeply nested. Let's turn the ingredients for a chocolate chip
cookie recipe into a set of Salt grains, as follows:
DPPLJFT
GBUT
CVUUFS
TVHBST
HSBOVMBUFETVHBS
MJHIUCSPXOTVHBS
XFU@JOHSFEJFOUT
FHHT
WBOJMMBFYUSBDU
ESZ@JOHSFEJFOUT
GMPVS
CBLJOHTPEB
CBLJOHQPXEFS
TBMU
HBSOJTI
DIPDPMBUFDIJQT
Now, imagine these variables as a shallow data structure:
DPPLJFT@GBUT
CVUUFS
DPPLJFT@TVHBST
HSBOVMBUFETVHBS
MJHIUCSPXOTVHBS
DPPLJFT@XFU@JOHSFEJFOUT
FHHT
WBOJMMBFYUSBDU
DPPLJFT@ESZ@JOHSFEJFOUT
GMPVS
CBLJOHTPEB
CBLJOHQPXEFS
TBMU
OPOJPEJ[FE
DPPLJFT@HBSOJTI
DIPDPMBUFDIJQT

Exploring Best Practices
[ 308 ]
A data structure that contains even more components can quickly get out of hand and
become cumbersome to declare inside multiple SLS files.
Referring to variables in templates
There are a number of ways to refer to variables in templates, in part because there are
multiple ways to store variables in the first place. In a default installation of Salt, a minion
may pull variables from:
The minion configuration
Grains
Pillars
The master configuration
Old-school Salt users could have stored all of their variables in grains. Early adopters of Salt
could have started stored variables in grains before migrating to pillars. Also, there are a
number of use cases where it is more appropriate to store a variable in either the master or
minion configuration.
If you want to ensure that you are looking explicitly for a grain, it's easy enough to look in
the HSBJOT dictionary in a template:
\\HSBJOT<	GPP	>^^
Using HSBJOTJUFN, you could also make a cross call to the HSBJOT execution module in a
template:
\\TBMU<	HSBJOTJUFN	>
	GPP	^^
However, it's more reliable to use HSBJOTHFU, which has the added advantage of allowing
you to supply a default when necessary:
\\TBMU<	HSBJOTHFU	>
	GPP		CBS	^^
The pillar data may also be called the same way:
\\TBMU<	QJMMBSHFU	>
	GPP		CBS	^^

Exploring Best Practices
[ 309 ]
The most versatile call to use here is DPOGJHHFU, which will look through each area where
a variable may possibly be stored in this order:
The minion configuration
1.
Grains
2.
Pillars
3.
The master configuration
4.
The call, of course, looks like this:
\\TBMU<	DPOGJHHFU	>
	GPP		CBS	^^
It's important to note that HSBJOTHFU, QJMMBSHFU, and DPOGJHHFU are the only calls
that allow retrieval of a specific item in a nested dictionary. To get a list of the wet
ingredients from our preceding cookie recipe, we will call:
\\TBMU<	DPOGJHHFU	>
	DPPLJFTXFU@JOHSFEJFOUT	<>^^
The colon () is what delimits the layers of the nested dictionary.
Summary
It is important to consider the potential future for your infrastructure. Saving time by
cutting corners can have disastrous consequences. Try to keep your directory structure
simple and easy to browse. Consider how deep your code tree really needs to be, and
design it accordingly.
Efficient SLS files makes for simpler and easier maintenance in the future. Using descriptive
variable names will help you and others keep track of what data each variable represents.
There are two recurring themes that you will find when you explore the best practices of
any project: making future-proof decisions and giving things names that are as descriptive
as they need to be (no more, no less). When these practices are followed, the results will be
easy to read and easy to maintain.
Following best practices is important in creating an environment that is easy to maintain. In
the next chapter, we'll finish up by looking at various techniques to troubleshoot our Salt
infrastructure.

13
Troubleshooting Problems
It doesn't matter what software you use, or how useful it is to you, sooner or later there will
be problems. Some of these arise from a simple misunderstanding of the software, but
sooner or later, there will be a problem with the software itself. In this chapter, we'll talk
about some of the tools that are available to users, and discuss the following:
Properly identifying problems
Using Salt in debug/trace mode
Using salt-call locally
Dealing with YAML idiosyncrasies
Using Salt's mailing list and issue tracker
What theâ€¦?
Things go wrong. And you don't always notice when they do, at least not at first. And when
you do, your first response is likely to sound like, cHey, that's weird!a
Before you can really start troubleshooting a problem, it always helps to build some context
around it, so that you know where to look for the solution. It has been said that real
programmers cook popcorn on the heat of their CPU. They can tell which jobs are running based on
the rate of the popping. You don't need to get so involved as that to work around problems in
Salt, but a little knowledge will go a long way.

Troubleshooting Problems
[ 311 ]
Addressing the problem source
A common mistake in any troubleshooting situation is to address the symptoms when they
occur, with little regard to what's actually causing the problem. For example, if a roof is
leaking during a rainstorm, the only step that some people will take is to leave out
containers under the leak to catch the water, and empty them when they get full. When the
sun comes out, far too many people won't bother to venture up onto the roof, or call a
roofing professional, to locate and fix the source of the leak.
When troubleshooting, addressing symptoms is usually also appropriate, but unless you're
also trying to figure out what is generating the symptoms, they are likely to keep appearing.
Be wary of problems that seem to fix themselves; oftentimes, they are only lying in wait to
spring on you again.
Where is the trouble?
A lot of systems are somewhat easier to troubleshoot because their complexity does not
extend beyond a single computer. Any networked system immediately suffers from the
complexity of troubleshooting multiple machines.
In its original default operation, Salt always consisted of at least two components: the
master and a minion. In the years since its introduction, this is no longer always the case. A
handful of master-side operations can be performed in the absence of minions, and a
number of minion-side operations can be performed without a master. In fact, several
organizations don't even employ a master in their infrastructure; all activities are performed
locally by the minions, orchestrated by third-party elements.
When a problem occurs in Salt, the first step is often to determine the component in which
the root of that problem lies. Let's take a look at a few examples.
Master-to-minion communication
Let's say that a job is sent from the master to a minion, and the minion doesn't respond.
There are at least two potential places that the problem could be occurring, starting with the
master and the minion.
The first thing that many users would do is to send a very simple job to the minion and see
if it responds. The most simple of jobs is a UFTUQJOH, which when functioning correctly,
will return 5SVF.

Troubleshooting Problems
[ 312 ]
If sending a UFTUQJOH does indeed return 5SVF, then we know several pieces of
information already:
The master is running
The minion is running
A network connection exists between the master and the minion
The master is able to communicate with the minion
The minion is able to communicate with the master
This implies that Salt itself, and all its basic components, are functioning properly. In fact, it
strongly implies that the master itself is functioning normally, and that the problem likely
resides with the minion.
Network and CPU congestion
Perhaps a UFTUQJOH only returns 5SVF intermittently. The fact that it does return 5SVF in
the first place strongly implies that:
There is a valid connection between the master and the minion
Any gateways, firewalls, and switches between the master and the minion are
appropriately configured
One or more of the network segments between the master and the minion may be
experiencing congestion. This is even more likely if one or more network segments are
across the Internet.
It is also possible that either the master or the minion, or both, are experiencing a high
amount of CPU load. Checking the VQUJNF command on the master will show its load
average across a time lapse of 1 minute, 5 minutes, and 15 minutes. Checking
TUBUVTMPBEBWH on a Unix or a Linux minion will show the same kind of information.
Interpreting load average can be tricky, since it doesn't reflect the percentage of CPU use.
What's more, interpreting it on a multi-processor system can be deceiving to the uninitiated,
so let's take a quick moment to explain.
The system load average is the average number of processes that are either in a runnable, or
uninterruptable state. A runnable process is one that is either currently using CPU cycles, or
is waiting to do so. An uninterruptable process is one which is waiting for some sort of I/O
access to happen (usually the disk).

Troubleshooting Problems
[ 313 ]
On a single processor system, a load average of 1 means that the CPU on that system is
busy 100% of the time. Less than 1 means that the CPU has had some idle time, and more
than 1 means that one or more processes are waiting to use the CPU.
On a multi-core or multi-processor system, increment that number for each core or
processor. For instance, on a 2-core system, a value of 2 means that both the CPUs are busy
100% of the time. On a 4-core system, a value of 4 means that all 4 CPUs are busy 100% of
the time.
Because this load average is reported in increments of 1, 5, and 15 minutes, we have a tiny
amount of historical data which can be used to tell us whether or not the system was likely
to be busy when the messages were not properly seen by the master, so long as we check as
quickly as possible.
Checking TUBUVTDQVMPBE on a Windows minion will show a CPU load as a percentage.
This is different from a load average, and should be interpreted differently. The CPU load
in Windows refers to the amount of time that the processor(s) spends doing work, as
opposed to the amount of time that it is idle.
For instance, on a single 2 GHz processor, a CPU load of 50% means that the processor is
performing one billion cycles of work per second. As with Unix and Linux, adding in
multiple cores and processors will affect this percentage, in part because switching tasks
between cores and processors will add to the percentage.
Checking minion load
If a minion is only responding intermittently, it may be more reliable to log into the minion
manually and troubleshoot. How you log in and check the load depends on whether you're
troubleshooting a Unix, Linux, or a Windows minion.
In Linux, we'll assume that you're logging in via SSH and issuing commands via a
command shell. The standard tool for checking the minion load is UPQ, which shows which
processes are consuming the most resources. By default, it refreshes every 2 seconds, but it
can be manually refreshed by hitting the space bar. However, auto-refreshing can get in the
way, and you may want a report that can be analyzed for longer than 2 seconds, or you may
want to save that report. Try this command:
# top -b -n1

Troubleshooting Problems
[ 314 ]
The C in this command starts up top in batch mode, which will perform a certain number
of iterations before exiting. The O will set that iteration count to 1, meaning that a single
report will be generated, and then UPQ will exit.
The report will look exactly like the standard output from UPQ, except that all processes will
be displayed, since it doesn't have to worry about the screen real estate.
UPQVQEBZTNJOVTFSTMPBEBWFSBHF

5BTLTUPUBMSVOOJOHTMFFQJOHTUPQQFE[PNCJF
$QV
TVTTZOJJEXBIJTJ
TU
(J#.FNUPUBMGSFFVTFECVGGDBDIF
(J#4XBQUPUBMGSFFVTFEBWBJM.FN
1*%64&313/*7*353&4$16.&.5*.&4$0.."/%
MBSSZNN49PSH
MBSSZNN4DISPNJVN
MBSSZNN4QVMTFBVEJP
MBSSZNN4DISPNJVN
MBSSZNN4DISPNJVN
SPPUNN4TBMUNBTUFS
MBSSZNN4DISPNJVN
SPPUNN4TZTUFNE
SPPUNN4LUISFBEE
SPPUNN4LTPGUJSRE
SPPUNN4LXPSLFS)
SPPUNN4SDV@QSFFNQU
SPPUNN4SDV@TDIFE
SPPUNN4SDV@CI
SPPUSUNN4NJHSBUJPO
FUD
Because UPQ performs calculations across all processes, it is able to generate percentages, as
seen in the preceding code. This information can be invaluable in troubleshooting the load.

Troubleshooting Problems
[ 315 ]
In Windows, there are similar tools for troubleshooting the CPU load, though they are
graphical, and therefore do not output text reports like UPQ can in Linux. The Task
Manager can be reached by pressing the Ctrl+Alt+Del key sequence and clicking Task
Manager:
Unlike UPQ in Unix and Linux, the Task Manager will not auto-refresh, so there's no need to
generate a report for longer viewing. The Task Manager will show the CPU percentage, just
like with TUBUVTDQVMPBE. However, it will also show application percentages, like top
will.
Querying the Salt job data
Some jobs take longer to complete than others. Assuming that there are no issues with
network and CPU congestion, and that Salt itself is functioning properly, a UFTUQJOH
should return immediately, with at least one minion.

Troubleshooting Problems
[ 316 ]
With hundreds or thousands of minions, it may take a little longer to receive the return
from every minion. Keep in mind that Salt is an asynchronous architecture, and that when
commands are published to minions, the minion will always return when their job finishes,
so long as the minion is functioning properly. The TBMU command does listen to the return
bus for a few seconds (10 by default), but if any commands take longer than the timeout,
they won't show.
The Salt job system will cache the return data as soon as it receives it, and it is still be able to
receive queries later. To see this in action, run the following from the master:
# salt --async myminion test.sleep 60
&YFDVUFEDPNNBOEXJUIKPC*%
# salt-run jobs.active


"SHVNFOUT

'VODUJPO
UFTUTMFFQ
3FUVSOFE
3VOOJOH
]@

NZNJOJPO

4UBSU5JNF
+VM
5BSHFU
NZNJOJPO
5BSHFUUZQF
HMPC
6TFS
TVEP@MBSSZ
# salt myminion saltutil.running
NZNJOJPO
]@

BSH

GVO
UFTUTMFFQ
KJE

QJE

SFU
UHU

Troubleshooting Problems
[ 317 ]
NZNJOJPO
UHU@UZQF
HMPC
VTFS
TVEP@MBSSZ
# salt myminion saltutil.find_job 20150704100203488893
NZNJOJPO

BSH

GVO
UFTUTMFFQ
KJE

QJE

SFU
UHU
NZNJOJPO
UHU@UZQF
HMPC
VTFS
TVEP@MBSSZ
Once the job has finished running, you can look at the return data with:
# salt-un jobs.lookup_jid  20150704100203488893
NZNJOJPO
5SVF
Using debug and trace modes
Every Salt command has the ability to change the amount of information displayed to the
user by changing the log level. The following log levels, and an explanation of each, are the 
most commonly used in Salt:
info
This is the default log level in every Salt command. It shows information that is considered
helpful to any user, but not part of the return output of the actual Salt command.

Troubleshooting Problems
[ 318 ]
warn
This is the level used when something has gone wrong, but not so horribly wrong that it
causes Salt to die. Often, this level is used to inform users when they are using Salt in a way
that has been deprecated. When this happens, the message will give you information on the
updated usage.
error
In a case where something has gone wrong and Salt is unable to recover from it, Salt will
usually exit without completing the task it is working on, and give you any information that
it has.
debug/trace
These modes are normally reserved for administrators and developers to give information
that is only useful when writing code or troubleshooting problems. Both modes are
extremely verbose, but the trace level is the noisiest.
The EFCVH mode may have some information that is useful to end-users, such as status
codes from HTTP calls, or the name of a shell command that was executed.
The USBDF mode should generally be avoided, unless you are writing code. It contains
information such as full HTTP responses, and the output from shell commands.
To change the log level for the information that is printed to the screen, use M, or MPH
MFWFM:
# salt -l debug myminion test.ping
<%&#6(>3FBEJOHDPOGJHVSBUJPOGSPNFUDTBMUNBTUFS
<%&#6(>6TJOHDBDIFENJOJPO*%GSPNFUDTBMUNJOJPO@JENZNJOJPO
<%&#6(>.JTTJOHDPOGJHVSBUJPOGJMFSPPUTBMUSD
<%&#6(>$POGJHVSBUJPOGJMFQBUIFUDTBMUNBTUFS
<%&#6(>3FBEJOHDPOGJHVSBUJPOGSPNFUDTBMUNBTUFS
<%&#6(>6TJOHDBDIFENJOJPO*%GSPNFUDTBMUNJOJPO@JENZNJOJPO
<%&#6(>.JTTJOHDPOGJHVSBUJPOGJMFSPPUTBMUSD
<%&#6(>.BTUFS&WFOU16#TPDLFU63*
JQDWBSSVOTBMUNBTUFSNBTUFS@FWFOU@QVCJQD
<%&#6(>.BTUFS&WFOU16--TPDLFU63*
JQDWBSSVOTBMUNBTUFSNBTUFS@FWFOU@QVMMJQD
<%&#6(>*OJUJBMJ[JOHOFX"TZOD;FSP.23FR$IBOOFMGPS

	FUDTBMUQLJNBTUFS		EVGSFTOF@NBTUFS		UDQ	
	DMFBS	

Troubleshooting Problems
[ 319 ]
<%&#6(>-B[Z-PBEFEMPDBM@DBDIFHFU@MPBE
<%&#6(>HFU@JUFS@SFUVSOTGPSKJETFOUUP
TFU
<	NZNJOJPO	>XJMMUJNFPVUBU
<%&#6(>KJESFUVSOGSPNNZNJOJPO
<%&#6(>-B[Z-PBEFEOFTUFEPVUQVU
NZNJOJPO
5SVF
<%&#6(>KJEGPVOEBMMNJOJPOTTFU
<	NZNJOJPO	>
To change the log level for information that is sent to the log file, use MPHGJMFMFWFM.
# salt --log-file-level debug myminion test.ping
To change the path to the log file to be used, use MPHGJMF:
# salt --log-file /tmp/salt.log myminion test.ping
Running services in debug mode
When troubleshooting issues between master and minion, it is often helpful to run both
services in the foreground using the EFCVH log level.
Log into the master, and shut down the TBMUNBTUFS service:
# service salt-master stop
Then, start it up in EFCVH mode:
# salt-master -l debug
A slew of information will fly past, but eventually you will see a series of lines interpolated
in the output that signify that the master has its queues up and listening:
<*/'0>8PSLFSCJOEJOHUPTPDLFUJQDWBSSVOTBMUNBTUFSXPSLFSTJQD
<%&#6(>.BTUFS&WFOU16#TPDLFU63*
JQDWBSSVOTBMUNBTUFSNBTUFS@FWFOU@QVCJQD
<%&#6(>.BTUFS&WFOU16--TPDLFU63*
JQDWBSSVOTBMUNBTUFSNBTUFS@FWFOU@QVMMJQD
Once these messages appear, the master is ready to start receiving data from its minions.
Log into the problem minion and shut down the TBMUNJOJPO service:
# service salt-minion stop
Then, start up the minion in EFCVH mode:
# salt-minion -l debug

Troubleshooting Problems
[ 320 ]
Again, a slew of information (though not much) will fly past, eventually ending with the
establishment of the minion's socket files:
<%&#6(>.JOJPO&WFOU16#TPDLFU63*
JQDWBSSVOTBMUNJOJPONJOJPO@FWFOU@CC@QVCJQD
<%&#6(>.JOJPO&WFOU16--TPDLFU63*
JQDWBSSVOTBMUNJOJPONJOJPO@FWFOU@CC@QVMMJQD
This signifies that the minion is now connected to the master, and is watching its queues for
messages for it.
Open up another shell on the master, and issue a command to the minion:
# salt myminion test.ping
Switch over to the shell running the TBMUNBTUFS process in EFCVH mode, and you will see
another information dump:
<%&#6(>4FOEJOHFWFOUEBUB\	@TUBNQ		5	
	NJOJPOT	<	NZNJOJPO	>^
<%&#6(>4FOEJOHFWFOUEBUB\	UHU@UZQF		HMPC		KJE	
			UHU		NZNJOJPO		@TUBNQ	
	5		VTFS		TVEP@MBSSZ		BSH	<>	GVO	
	UFTUQJOH		NJOJPOT	<	NZNJOJPO	>^
<%&#6(>$PVMEOPU-B[Z-PBEMPDBMTBWF@MPBE
<*/'0>6TFSTVEP@MBSSZ1VCMJTIFEDPNNBOEUFTUQJOHXJUIKJE

<%&#6(>1VCMJTIFEDPNNBOEEFUBJMT\	UHU@UZQF		HMPC		KJE	
			UHU		NZNJOJPO		SFU		MPDBM		VTFS	
	TVEP@MBSSZ		BSH	<>	GVO		UFTUQJOH	^
<%&#6(>-B[Z-PBEFEMPDBM@DBDIFQSFQ@KJE
<*/'0>(PUSFUVSOGSPNNZNJOJPOGPSKPC
<%&#6(>4FOEJOHFWFOUEBUB\	GVO@BSHT	<>	KJE	
			SFUVSO	5SVF	SFUDPEF		TVDDFTT	5SVF
	DNE		@SFUVSO		@TUBNQ		5		GVO	
	UFTUQJOH		JE		NZNJOJPO	^
If you switch over to the window running the TBMUNJOJPO process in EFCVH mode, you
will also see some information about the job:
<*/'0>6TFSTVEP@MBSSZ&YFDVUJOHDPNNBOEUFTUQJOHXJUIKJE

<%&#6(>$PNNBOEEFUBJMT\	UHU@UZQF		HMPC		KJE	
			UHU		NZNJOJPO		SFU		MPDBM		VTFS	
	TVEP@MBSSZ		BSH	<>	GVO		UFTUQJOH	^
<*/'0>4UBSUJOHBOFXKPCXJUI1*%
<%&#6(>-B[Z-PBEFEUFTUQJOH
<*/'0>3FUVSOJOHJOGPSNBUJPOGPSKPC

Troubleshooting Problems
[ 321 ]
<%&#6(>*OJUJBMJ[JOHOFX"TZOD;FSP.23FR$IBOOFMGPS

	FUDTBMUQLJNJOJPO		NZNJOJPO		UDQ		BFT	
<%&#6(>*OJUJBMJ[JOHOFX4"VUIGPS
	FUDTBMUQLJNJOJPO		NZNJOJPO	
	UDQ	
<%&#6(>-B[Z-PBEFEMPDBMSFUVSOFS
\	GVO@BSHT	<>	KJE				SFUVSO	5SVF	SFUDPEF	
	TVDDFTT	5SVF	GVO		UFTUQJOH		JE		NZNJOJPO	^
If a process on the minion that is related to this job has issues, then helpful information is
likely to be shown here. To test this, go ahead and create an execution module on the
minion with intentionally bad code:
# cat /usr/lib/python2.7/site-packages/salt/modules/mytest.py
EFGCBEDPEF

EJF

Stop the TBMUNJOJPO process with a Ctrl+C, and start it up again:
# salt-minion -l debug
Switch over to the master shell and run a command that executes the bad code:
# salt myminion mytest.badcode
In that shell window, you may see an error message about the bad code:
NZNJOJPO
5IFNJOJPOGVODUJPODBVTFEBOFYDFQUJPO5SBDFCBDL
NPTUSFDFOUDBMM
MBTU
'JMFVTSMJCQZUIPOTJUFQBDLBHFTTBMUNJOJPOQZMJOFJO
@UISFBE@SFUVSO
SFUVSO@EBUBGVOD
BSHTLXBSHT
'JMFVTSMJCQZUIPOTJUFQBDLBHFTTBMUNPEVMFTNZUFTUQZMJOF
JOCBEDPEF
EJF

/BNF&SSPSHMPCBMOBNF	EJF	JTOPUEFGJOFE
If you switch over to the TBMUNJOJPO process, you will see the USBDFCBDL again:
<*/'0>6TFSTVEP@MBSSZ&YFDVUJOHDPNNBOENZUFTUCBEDPEFXJUIKJE

<%&#6(>$PNNBOEEFUBJMT\	UHU@UZQF		HMPC		KJE	
			UHU		NZNJOJPO		SFU		MPDBM		VTFS	
	TVEP@MBSSZ		BSH	<>	GVO		NZUFTUCBEDPEF	^
<*/'0>4UBSUJOHBOFXKPCXJUI1*%
<%&#6(>-B[Z-PBEFENZUFTUCBEDPEF
<8"3/*/(>5IFNJOJPOGVODUJPODBVTFEBOFYDFQUJPO
5SBDFCBDL
NPTUSFDFOUDBMMMBTU

Troubleshooting Problems
[ 322 ]
'JMFVTSMJCQZUIPOTJUFQBDLBHFTTBMUNJOJPOQZMJOFJO
@UISFBE@SFUVSO
SFUVSO@EBUBGVOD
BSHTLXBSHT
'JMFVTSMJCQZUIPOTJUFQBDLBHFTTBMUNPEVMFTNZUFTUQZMJOF
JOCBEDPEF
EJF

/BNF&SSPSHMPCBMOBNF	EJF	JTOPUEFGJOFE
<%&#6(>4BMU&WFOU16#TPDLFU63*
JQDWBSSVOTBMUNJOJPONJOJPO@FWFOU@CC@QVCJQD
<%&#6(>4BMU&WFOU16--TPDLFU63*
JQDWBSSVOTBMUNJOJPONJOJPO@FWFOU@CC@QVMMJQD
<%&#6(>4FOEJOHFWFOUEBUB\	NFTTBHF	V	5IFNJOJPOGVODUJPODBVTFE
BOFYDFQUJPO		BSHT	
	5IFNJOJPOGVODUJPODBVTFEBOFYDFQUJPO	
	@TUBNQ		5	^
<%&#6(>)BOEMJOHFWFOU@TBMU@FSSPS=O=O=Y=YBNFTTBHF=YEB=Y	5IF
NJOJPOGVODUJPODBVTFEBOFYDFQUJPO=YBBSHT=Y=YEB=Y	5IFNJOJPOGVODUJPO
DBVTFEBOFYDFQUJPO=YB@TUBNQ=YCB5
<%&#6(>'PSXBSEJOHTBMUFSSPSFWFOUUBH@TBMU@FSSPS
<%&#6(>*OJUJBMJ[JOHOFX"TZOD;FSP.23FR$IBOOFMGPS

	FUDTBMUQLJNJOJPO		NZNJOJPO		UDQ		BFT	
<%&#6(>*OJUJBMJ[JOHOFX4"VUIGPS
	FUDTBMUQLJNJOJPO		EVGSFTOF	
	UDQ	
<*/'0>3FUVSOJOHJOGPSNBUJPOGPSKPC
<%&#6(>*OJUJBMJ[JOHOFX"TZOD;FSP.23FR$IBOOFMGPS

	FUDTBMUQLJNJOJPO		NZNJOJPO		UDQ		BFT	
<%&#6(>*OJUJBMJ[JOHOFX4"VUIGPS
	FUDTBMUQLJNJOJPO		NZNJOJPO	
	UDQ	
<%&#6(>-B[Z-PBEFEMPDBMSFUVSOFS
\	GVO@BSHT	<>	KJE				SFUVSO		5IFNJOJPO
GVODUJPODBVTFEBOFYDFQUJPO5SBDFCBDL
NPTUSFDFOUDBMMMBTU=O'JMF
VTSMJCQZUIPOTJUFQBDLBHFTTBMUNJOJPOQZMJOFJO
@UISFBE@SFUVSO=OSFUVSO@EBUBGVOD
BSHTLXBSHT=O'JMF
VTSMJCQZUIPOTJUFQBDLBHFTTBMUNPEVMFTNZUFTUQZMJOFJO
CBEDPEF=OEJF
=O/BNF&SSPSHMPCBMOBNF	EJF	JTOPUEFGJOFE=O	
	TVDDFTT	'BMTF	GVO		NZUFTUCBEDPEF		JE		NZNJOJPO		PVU	
	OFTUFE	^
Using salt-call locally
Very often, it is helpful to issue commands directly on the minion without involving the
master, or at least minimizing communication with the master. The TBMUDBMM command
can be used with or without the local mode:
# salt-call test.ping
# salt-call --local test.ping

Troubleshooting Problems
[ 323 ]
The difference between these two commands is that the first will still contact the master to
ask for data such as pillar data, files from the master file server (if needed), and so on. The
second will tell the minion to behave as if it has no master, and look for that information
locally. If data has been set up in GJMF@SPPUT or QJMMBS@SPPUT directly on the minion, it
will be used instead of contacting the master:
# salt-call mytest.badcode
<&3303>"OVOIBOEMFEFYDFQUJPOXBTDBVHIUCZTBMU	THMPCBMFYDFQUJPO
IBOEMFS
/BNF&SSPSHMPCBMOBNF	EJF	JTOPUEFGJOFE
5SBDFCBDL
NPTUSFDFOUDBMMMBTU
'JMFVTSCJOTBMUDBMMMJOFJONPEVMF 
TBMU@DBMM

'JMFVTSMJCQZUIPOTJUFQBDLBHFTTBMUTDSJQUTQZMJOFJO
TBMU@DBMM
DMJFOUSVO

'JMFVTSMJCQZUIPOTJUFQBDLBHFTTBMUDMJDBMMQZMJOFJOSVO
DBMMFSSVO

'JMFVTSMJCQZUIPOTJUFQBDLBHFTTBMUDMJDBMMFSQZMJOFJO
SVO
SFUTFMGDBMM

'JMFVTSMJCQZUIPOTJUFQBDLBHFTTBMUDMJDBMMFSQZMJOFJO
DBMM
SFU<	SFUVSO	>GVOD
BSHTLXBSHT
'JMFVTSMJCQZUIPOTJUFQBDLBHFTTBMUNPEVMFTNZUFTUQZMJOF
JOCBEDPEF
EJF

NameError: global name 'die' is not defined
If you issue a command using TBMUDBMM on the minion while running the TBMUNJOJPO
process in the foreground, you will notice that the foreground window will not respond to
your command.
This is because the TBMUDBMM command will fire up its own, single-use TBMUNJOJPO
process, perform the requested task, and then exit. It will not interact with any other TBMU
NJOJPO processes that are running.

Troubleshooting Problems
[ 324 ]
Working with YAML
YAML is a very easy language to work with. It is very easy for humans to read, and in most
cases, it is easy for computers to parse. However, there are little things inside YAML that
can cause pain to even the most experienced users.
YAML basics
Before we get into troubleshooting YAML, let's go over the basic functionality that you are
likely to use in Salt.
YAML is based on a key/value model that is very common in a number of programming
languages. In Perl and Ruby it's called a hash, in Python it's called a dictionary (or dict, for
short), and in other languages it has other names. Because Salt is written in Python, we'll
henceforth refer to it as a dict.
dict
A dict is a set of keys, each of which has a value. This value may be a number of things,
including a string, a number, a list (or array), another dict, and so on. The following is a 
very basic dict, in YAML format:
MBSSZDIFFTFDBLF
TIFNQDIPDPMBUFDBLF
NPFBQQMFQJF
The order of the items in a dict is not normally important, and in most cases will be ignored.
Salt is different in that some of its code uses what's called an OrderedDict, which maintains
the order of the keys and their associated values. One of the places where this is used is in
the state compiler, which is designed to evaluate SLS data in the order in which it appears.
list
A list is just that: a group of items in a specific order. The order will always be preserved, at
least during the phases which read in and parse the data. In YAML, items in a list are
preceded by a dash:
BQQMFT
PSBOHFT
CBOBOBT

Troubleshooting Problems
[ 325 ]
In Salt, you will usually not find lists by themselves in YAML. They are usually the value of
a key in a dict. However, a list item may in turn contain a dict, or even another list:
GBWPSJUF@EFTTFSUT
MBSSZDIFFTFDBLF
TIFNQDIPDPMBUFDBLF
NPFBQQMFQJF
GSVJUT
BQQMFT
PSBOHFT
CBOBOBT
CFSSJFT
OJHIUTIBEF
UPNBUP
DIJMF
There are a number of ways to organize these dicts and lists in YAML. The most common in
Salt is to use whitespace, as with the preceding data structure. However, YAML also
supports using braces and brackets to organize data:
GBWPSJUF@EFTTFSUT\MBSSZDIFFTFDBLFTIFNQDIPDPMBUFDBLFNPFBQQMF
QJF^
GSVJUT<BQQMFTPSBOHFTCBOBOBTCFSSJFT\OJHIUTIBEF<UPNBUPDIJMF>^>
Items in YAML may also be quoted, which makes them easier for the compiler to parse, and
in many cases, easier for humans to read:
	GBWPSJUF@EFTTFSUT	\	MBSSZ		DIFFTFDBLF		TIFNQ		DIPDPMBUFDBLF	
	NPF		BQQMFQJF	^
	GSVJUT	<	BQQMFT		PSBOHFT		CBOBOBT		CFSSJFT	\	OJHIUTIBEF	
<	UPNBUP		DIJMF	>^>
Either single quotes (	) or double quotes () may be used. It is often better to use double
quotes, for two reasons. First, it avoids having to escape apostrophes which are likely to
occur in sentences meant for humans. Second, if double quotes are used, and the entire
structure is set up as a properly-formed dict or list, the YAML can also be read by a JSON
intepreter:
\GBWPSJUF@EFTTFSUT\TIFNQDIPDPMBUFDBLFMBSSZDIFFTFDBLF
NPFBQQMFQJF^GSVJUT<BQQMFTPSBOHFTCBOBOBT\CFSSJFT
<\OJHIUTIBEF<UPNBUPDIJMF>^>^>^
This is why all JSON is syntactically-correct YAML; YAML is actually a superset of JSON.

Troubleshooting Problems
[ 326 ]
YAML idiosyncrasies
If you decide to store all your YAML data in proper JSON format, then it will always be
correctly parsed by the computer. However, it will be more difficult for humans to read and
modify. This is one reason why YAML is generally preferred for Salt states.
However, there are some nuances in YAML that can trip up even the most experienced
user, especially if they aren't paying enough attention.
Spacing
Without braces and brackets, YAML uses whitespace to determine where blocks of text
begin and end. If a dict contains another dict, then that second dict will contain spaces at the
beginning of each line. Technically, a single space is enough, but Salt has standardized on
two spaces. This is enough to determine where the lines start, without going overboard.
NZEJDU
JUFNWBMVF
JUFNWBMVF
If you spend a lot of time writing code, you may have your own preference for spacing.
Some coders use three or four spaces, and some even use as much as eight.
When working with YAML that is meant for Salt, avoid the temptation to use anything
other than two spaces. First, longer pieces of YAML start to look weird with too many
spaces b spend enough time writing YAML and you'll see what I mean. Second, the Salt
community at large tends to follow Salt's two-space model. Asking others for help, or hiring
experienced Salt users, will become that much more painful if they have to re-adjust
themselves to your style.
Technically, list items belonging to a dict do not generally need to be spaced out:
NZMJTU
POF
UXP
UISFF
But it is still a better practice to space them anyway. Not only is it easier for humans to read,
but in some situations, it is actually easier for Salt to read as well.

Troubleshooting Problems
[ 327 ]
Numbers
YAML is usually able to distinguish between text and numbers. However, there are some
situations where it needs to be forced to do the right thing.
A very common example is file modes in Unix and Linux. For example, a directory might
have a mode of 775, meaning the user and the group which own it have full (read, write,
execute) permissions, while other users have only read and execute permissions.
This number is in fact a bit-mapped set of digits, stored in an octal (base-8). It can also
contain more fields than just the 6TFS, (SPVQ, and 0UIFS fields that were shown
previously. For instance, another bit can be added to the beginning, which specifies special
attributes (SUID, SGID, and Sticky). A mode of  may look identical to 775, but it will
enforce that the special bits are not set.
When digits appear in YAML, it is assumed that they are base-10, and that any leading
zeroes are to be stripped. If you need to explicitly set a directory's permissions to , this
will be a problem. In order for Salt to see the correct value, it must be converted to a string
by placing it in quotes (		). The following SLS data shows an example of this:
TSWNZEBUB
GJMFEJSFDUPSZ
NPEF		
Booleans
Boolean values refer to things that are 5SVF or 'BMTF (or /POF, in Python). These data are
very commonly used throughout Salt, including YAML files. If you are used to quoting all
your values in your YAML, this is likely to trip you up. The following two keys do not have
the same value:
LFZ5SVF
LFZ	5SVF	
YAML will convert the second line to a string, which will not evaluate to a Boolean data
type like the first one will.
JSON adds an extra element of confusion because it does not support Booleans, and is more
strict when it finds unquoted data. The following line is a valid JSON:
\LFZ5SVF^

Troubleshooting Problems
[ 328 ]
While this is not:
\LFZ5SVF^
Salt will generally try to do the most appropriate thing based on the information that it
receives. For instance, the state compiler will attempt to properly read Booleans as
Booleans, even if they are quoted in a way that is inconsistent with what is expected.
List items
A very common mistake in YAML involves spacing with list items. Because each list item
resembles a bullet point, and because word processors don't require spaces after bullet
points, many users often forget to add the required space after a dash for a list item. The
following list is valid YAML:
POF
UXP
UISFF
While this list will not read properly:
POF
UXP
UISFF
Troubleshooting YAML
Writing YAML may seem easy to the experienced user, but it is very easy to trip it up. Very
often, mistakes are easy to see when we are able to see what our YAML will look like once it
is parsed.
An excellent tool that is available is the Online YAML Parser:
IUUQZBNMPOMJOFQBSTFSBQQTQPUDPN
This tool will take YAML input from the user, and translate it to either
JSON, Python's pretty print format, or to canonical YAML. If there are
errors in YAML, an error will instantly be thrown which attempts to
inform where the problem lies.
However, this is of no use if you are in an environment that is restricted in
its Internet access such that this site is unavailable. Fortunately, it is
possible to perform a similar test from the command line on a machine
with Python installed (such as any master or minion).

Troubleshooting Problems
[ 329 ]
Create a file called UNQZBNMZNM with the following content:
NZMJTU
POF
UXP
UISFF
Then, use the following one-line command to parse it:
# python -c 'import yaml; fh = open("/tmp/yaml.yml",
    "r"); print(yaml.safe_load(fh.read()))'
Okay, so there's a fair amount of typing involved. Fortunately, if you are using a command
shell (like CBTI or [TI) which supports command history, you can just use your Up arrow
key to navigate to the command and issue it again.
Go ahead and modify UNQZBNMZNM, and remove the leading spaces from one of the list
items:
NZMJTU
POF
UXP
UISFF
Then, issue the Python command again:
# python2 -c 'import yaml; fh = open("/tmp/yaml.yml", "r");
print(yaml.safe_load(fh.read()))'
5SBDFCBDL
NPTUSFDFOUDBMMMBTU
'JMFTUSJOH MJOFJONPEVMF 
'JMFVTSMJCQZUIPOTJUFQBDLBHFTZBNM@@JOJU@@QZMJOFJO
TBGF@MPBE
SFUVSOMPBE
TUSFBN4BGF-PBEFS
'JMFVTSMJCQZUIPOTJUFQBDLBHFTZBNM@@JOJU@@QZMJOFJO
MPBE
SFUVSOMPBEFSHFU@TJOHMF@EBUB

'JMFVTSMJCQZUIPOTJUFQBDLBHFTZBNMDPOTUSVDUPSQZMJOFJO
HFU@TJOHMF@EBUB
OPEFTFMGHFU@TJOHMF@OPEF

'JMFVTSMJCQZUIPOTJUFQBDLBHFTZBNMDPNQPTFSQZMJOFJO
HFU@TJOHMF@OPEF
EPDVNFOUTFMGDPNQPTF@EPDVNFOU

'JMFVTSMJCQZUIPOTJUFQBDLBHFTZBNMDPNQPTFSQZMJOFJO
DPNQPTF@EPDVNFOU
OPEFTFMGDPNQPTF@OPEF
/POF/POF
'JMFVTSMJCQZUIPOTJUFQBDLBHFTZBNMDPNQPTFSQZMJOFJO
DPNQPTF@OPEF
OPEFTFMGDPNQPTF@NBQQJOH@OPEF
BODIPS

Troubleshooting Problems
[ 330 ]
'JMFVTSMJCQZUIPOTJUFQBDLBHFTZBNMDPNQPTFSQZMJOFJO
DPNQPTF@NBQQJOH@OPEF
XIJMFOPUTFMGDIFDL@FWFOU
.BQQJOH&OE&WFOU
'JMFVTSMJCQZUIPOTJUFQBDLBHFTZBNMQBSTFSQZMJOFJO
DIFDL@FWFOU
TFMGDVSSFOU@FWFOUTFMGTUBUF

'JMFVTSMJCQZUIPOTJUFQBDLBHFTZBNMQBSTFSQZMJOFJO
QBSTF@CMPDL@NBQQJOH@LFZ
FYQFDUFECMPDLFOE CVUGPVOESUPLFOJEUPLFOTUBSU@NBSL
ZBNMQBSTFS1BSTFS&SSPSXIJMFQBSTJOHBCMPDLNBQQJOH
JOTUSJOH MJOFDPMVNO
NZMJTU
?
FYQFDUFECMPDLFOE CVUGPVOE		
JOTUSJOH MJOFDPMVNO
UISFF
?
The last couple of lines give some information about where the YAML parser thinks that the
problem might be. It may not be the easiest message in the world to interpret, but it will tell
you if you have poorly-formatted YAML, and where to look for the problem.
You may be interested to know that JSON content can be parsed using a similar command:
# python2 -c 'import json; fh = open("/tmp/json.json",
    "r"); print(json.loads(fh.read()))'
Asking the community for help
Salt boasts of a very large community of very friendly and helpful users. When you're
unable to figure out a problem by yourself, you can try turning to the community for help.
The salt-users mailing list
There is a very active mailing list for Salt users, hosted on Google Groups. A Google
account is not required to participate in the mailing list itself, but it is required to participate
in the web version.
The web version of the list can be found at:
IUUQTHSPVQTHPPHMFDPNGPSVNGPSVNTBMUVTFST
If you do not have a Google account and you still wish to subscribe to the
list, visit:
IUUQTHSPVQTHPPHMFDPNGPSVNGPSVNTBMUVTFSTKPJO

Troubleshooting Problems
[ 331 ]
Fill out the required fields, and a confirmation e-mail will be sent to you. Click the Join This
Group link and you will be subscribed.
If you ever decide to unsubscribe, you can do so from
IUUQTHSPVQTHPPHMFDPNGPSVNGPSVNTBMUVTFSTVOTVCTDSJCF.
Asking questions
When you have a question about Salt usage, or you're attempting to troubleshoot a
problem, the mailing list is an excellent place to ask. When posting a message, it is best to be
as informative and helpful as possible with your question, without going overboard.
It will be helpful to know which Salt version you are using, both on your master and on any
affected minions (if they differ). This can be obtained from Salt using the WFSTJPOT
SFQPSU flag:
# salt --versions-report
4BMU7FSTJPO
4BMU
%FQFOEFODZ7FSTJPOT
+JOKB
.$SZQUP
.BLP/PU*OTUBMMFE
1Z:".-
1Z;.2
1ZUIPO
EFGBVMU.BZ
3"&5
5PSOBEP
;.2
JPGMP
MJCOBDM
NTHQBDLQVSF/PU*OTUBMMFE
NTHQBDLQZUIPO
QZDSZQUP
4ZTUFN7FSTJPOT
EJTU
NBDIJOFY@
SFMFBTF"3$)

Troubleshooting Problems
[ 332 ]
If you are asking a question concerning Salt Cloud, be sure to get the WFSTJPOTSFQPSU
from it instead, as it contains additional information that is specific to Salt Cloud:
# salt-cloud --versions-report
4BMU7FSTJPO
4BMU
%FQFOEFODZ7FSTJPOT
"QBDIF-JCDMPVE
FUD
Other users will frequently ask for this information if you don't provide it, so it's best to
post it with your initial question to save a little time.
When asking your question, try to explain the situation as clearly and simply as possible. It
is extremely common for other users to experience the same sorts of issues, especially
within the same release versions, and there's a good chance that somebody has already seen
your issue, and either has a solution, or is able to collaborate to find a solution.
Do not be discouraged if you don't receive a response right away. Salt has a very
international user base, and the person who is willing to help you may not live within your
time zone. Weekends and holidays will also play a part in the amount of time it takes to
answer your message.
If you do not hear from anybody for a few days, it is not unreasonable to ask again. Perhaps
somebody saw your message and intended to respond, but got distracted. It's also possible
that the person who can help you didn't see the message the first time.
I have seen a number of messages over the years from users, followed an hour or two later
by an impatient, cIs anybody there?a e-mail. This will not expedite your message at all, and in
fact may keep somebody from answering who otherwise would have. Be friendly and
patient, and you will have much better luck.
The Salt issue tracker
When you encounter a problem that you believe to be an issue with Salt itself, the issue
tracker is the place to go to.
The Salt issue tracker can be found at
IUUQTHJUIVCDPNTBMUTUBDLTBMUJTTVFT.

Troubleshooting Problems
[ 333 ]
The occasional user does post questions in the issue tracker, and they will receive the same
sort of attention that other issues get, but the mailing list is usually the more appropriate
place.
When deciding whether a problem is an issue or not, ask yourself if it may be a problem
with your understanding of the usage of Salt, or if it is inconsistent with how you have been
led to believe that Salt should behave. A USBDFCBDL is almost always appropriate for the
issue tracker. Some examples of tracebacks appear earlier in this chapter, and here's one
again for reference:
5SBDFCBDL
NPTUSFDFOUDBMMMBTU
'JMFVTSCJOTBMUDBMMMJOFJONPEVMF 
TBMU@DBMM

'JMFVTSMJCQZUIPOTJUFQBDLBHFTTBMUTDSJQUTQZMJOFJO
TBMU@DBMM
DMJFOUSVO

'JMFVTSMJCQZUIPOTJUFQBDLBHFTTBMUDMJDBMMQZMJOFJOSVO
DBMMFSSVO

'JMFVTSMJCQZUIPOTJUFQBDLBHFTTBMUDMJDBMMFSQZMJOFJO
SVO
SFUTFMGDBMM

'JMFVTSMJCQZUIPOTJUFQBDLBHFTTBMUDMJDBMMFSQZMJOFJO
DBMM
SFU<	SFUVSO	>GVOD
BSHTLXBSHT
'JMFVTSMJCQZUIPOTJUFQBDLBHFTTBMUNPEVMFTNZUFTUQZMJOF
JOCBEDPEF
EJF

/BNF&SSPSHMPCBMOBNF	EJF	JTOPUEFGJOFE
Note that tracebacks do start with the word USBDFCBDL, and show a trail
of the pieces of code that were accessed before finding the line of code
which actually raised the error.
Researching before posting
Before posting an issue, it is important to perform a little research first. Duplicate issues are
surprisingly uncommon on Salt's issue tracker, at least in comparison to others, but they do
happen. Use the search button in the issue tracker to see if your particular issue has already
been reported.

Troubleshooting Problems
[ 334 ]
GitHub allows you to apply filters to your searches, and knowing how to use them is also
important. They will show up in the issue search box. The default filters are is:open and
is:issue, which means that only issues that are open, and not pull requests, will be searched.
If searching with the defaults yields no suitable results, try changing is:open to is:closed, or
removing it altogether. There are thousands of closed issues in GitHub, and your particular
issue may already be resolved.
If you are unable to find the issue, try putting together a list of steps which can be used to
reproduce the problem, as simply and quickly as possible. If you have access to virtual
machines that can be used to reproduce the issue with a stock version of Salt, other users
will also be much more likely to reproduce the problem.
Formatting your issues
When posting an issue to the issue tracker, it is very helpful to be able to format certain data
in a way that makes it easy to read. GitHub supports a markdown language which makes it
possible to format code appropriately.
You can find documentation on their markdown format at
IUUQTIFMQHJUIVCDPNBSUJDMFTHJUIVCGMBWPSFENBSLEPXO.
By far, the most useful formatting trick involves the grave symbol (A), also known as
backticks. On modern US keyboards, this usually shares a key with the tilde (_), located in
the top-left corner of the keyboard.

Troubleshooting Problems
[ 335 ]
Placing one or more words between backticks will cause them to be formatted as code. If
you have multiple lines that all need to be formatted as code, you can place them between
two lines, each of which contains three backticks together (AAA):
 
GitHub contains a Preview mode, which can be used to test your formatting before
submitting it to ensure that it looks the way you want it to:

Troubleshooting Problems
[ 336 ]
Requesting features
It may be that what you need is not an actual issue, but in fact a piece of functionality that is
not yet supported in Salt. One of the most powerful aspects of Salt is the willingness of the
developers to consider new ideas for adding functionality.
Before requesting a new feature, please do take a moment to think about that feature in a
way that extends beyond your own reach. Are you looking for a feature that is only useful
to you, or is it something that you feel that others can also benefit from? If its scope is very
limited, is it possible to approach it from a more generic point of view?

Troubleshooting Problems
[ 337 ]
Once you have established a feature in a way that is potentially usable to a large audience,
do not hesitate to file an issue requesting it. Be sure to state the use case clearly, and the way
in which you feel that it should be addressed. If you are unsure, it is appropriate to state the
use case and ask for ideas.
#salt on IRC
Another venue to check is the #salt channel in (IRCInternet Relay Chat (IRC). This channel
is hosted by a service called Freenode. If you already have an IRC client and know how to
configure it, connect it to the JSDGSFFOPEFOFU server, and join the #salt chatroom.
If you are unfamiliar with IRC, or don't have a standalone client, you can
try out the chatroom using the web client at
IUUQXFCDIBUGSFFOPEFOFUDIBOOFMTTBMU.
The TBMU chatroom has several hundred users at any given time, though fortunately they
are not all chatting at once. Many are users such as yourself, asking questions about Salt
usage or looking for help with specific issues. Others are Salt enthusiasts who periodically
check in to see if there is anything that they can help with.
It is useful to know about the netiquette that goes along with IRC rooms. A common phrase
is, cDon't ask to ask; just aska. This means that if you have a question, don't start by saying,
cCan I ask a question?a The answer is yes. Just ask the question, and you'll be fine.
When you do ask a question, don't be alarmed or impatient if it is not answered
immediately. Most users are not keeping a constant eye on the chatroom, but many do
check in on a fairly regular basis.
Do not log into the chatroom, ask a question, and then log out a minute or two later.
Generally, that is not enough time for your question to be appropriately answered. If you
have waited several minutes and seem to be ignored, it may be that nobody who is able to
help is online at the moment. Give it some time. If you are unable to find the help you need,
consider using the mailing list.

Troubleshooting Problems
[ 338 ]
Final community thoughts
Remember that whichever avenue you decide to turn to, some SaltStack employees may
respond, but most of the people you talk to are members of the community, just like you.
They have full-time jobs at other companies, and any time they spend helping is,
essentially, volunteer time. When they help out, they do so out of the kindness of their
hearts and not out of any sort of obligation.
With that in mind, please be friendly and helpful as you speak with them. There are some
brilliant minds in the world who have discovered Salt, and who enjoy working with others
on this tool. As you foster relationships with them, do not be surprised if some of those
relationships grow into life-long friendships. And remember that if you are unfriendly and
demanding, you may miss out on those opportunities.
Summary
There are a number of tools available, both inside Salt and from external sources, which can
be used when troubleshooting problems. Clearly identifying the problems, tracking down
their source, and asking for help when necessary are all important when trying to work
through difficulties inside of Salt.
Congratulations, you made it to the end! We're very thankful that you have decided to use
this book to help guide you in your journey to master SaltStack, and hope that it was
everything you needed and more.

Index
.
.sls file
   versus init.sls file  , 
A
Advanced Message Queuing Protocol (AMQP) 

aggregate data
   using, with Thorium  , , 
alerts, from beacons
   bad logins, monitoring  , 
   file changes, watching  
alerts
   from beacons  
   in state files  
   setting up  
apache-formula
   URL  
architecture, RAET
   basics  , 
   estates  
   scheduler  
   yards  
arguments
   passing, to scripts  , , 
asynchronous programming
   about  , 
   example  
authentication
   external authentication module  , 
autoscale reactors
   cloud cache  , 
   cloud cache events, using  
   using  
AWS
   SDB, used with  , 
B
base environment
   using  
baseline
   creating  , 
beacons
   alerts  
   file changes, monitoring  , 
   intervals  , 
   using  , 
black box  
blocking  
Bourne shell  
built-in rosters
   about  
   ansible  
   cache  
   cloud  , 
   scan  
built-in states
   using  , 
C
carbon software  
certificate authority (CA)  
cloud cache events
   schedule, setting up  , 
   setting  , 
   using  
cloud cache
   about  , , 
cloud events
   salt/cloud/vm_name/created  , 
   salt/cloud/vm_name/creating  
   salt/cloud/vm_name/deploying  
   salt/cloud/vm_name/destroyed  

[ 340 ]
   salt/cloud/vm_name/destroying  
   salt/cloud/vm_name/querying  
   salt/cloud/vm_name/requesting  
   salt/cloud/vm_name/waiting_for_ssh  
cloud maps  , 
cloud modules  
cloud providers
   SDB, used with  
ClusterSSH  
complex authentication  , 
complex reactors
   alerts, sending out  , 
   calling  , 
   webhooks, using  , 
   writing  
components, flow-based programming (FBP)
   black boxes  , 
   concurrent scheduling  , 
   shared storage  
configuration blocks
   extending  , , 
configuration management database (CMDB)  
configuration management
   Salt states, used  
Curve25519  , 
custom deploy scripts
   arguments, passing to scripts  , , 
   building  
   file maps, using  
   legacy deploy scripts  
   Salt Bootstrap script  , 
   writing  , , 
D
debug mode
   about  
   error  
   info  
   services, running in  
   using  
   warn  
declarative
   versus imperative  
deep directory structure
   versus shallow directory structure  
dict, YAML  
directories
   .sls file, versus init.sls file  , 
   locations  , 
   setting up  
   shallow directory structure, versus deep directory
structure  
   subdividing  
dynamic modules  , 
dynamic rosters
   building  
E
EC2 Autoscale reactor
   reference link  
EC2
   SDB, used with  , 
elliptic curve-based cryptography (ECC)  
etcd pillar
   configuring  , 
event data
   custom data, triggering  , 
   event listener, installing  
   event listener, using  , 
event listener
   URL, for downloading  
event system
   about  
   basics, reviewing  
   cloud events, commonly used  
   event data, structure  , 
   event data, viewing  
   Salt API, events  
event tags
   setting up  , , , 
events
   commonly used  
   namespacing  , 
   salt/auth  
   salt/job/job_id/new  
   salt/job/job_id/ret/minion_id  
   salt/key  
   salt/minion/minion_id/start  
   salt/presence/change  
   salt/presence/present  

[ 341 ]
execution modules  , 
explicit names  
external data sources
   external filesystems  
   external pillars  , 
   incorporating  
   job cache  
external filesystems
   about  
   AzureFS  
   GitFS  , , 
   HGFS  , 
   S3F3 driver  , 
   source-control backends  
   SVNFS  
external job cache
   about  
   master job cache, using  , 
   returners, using on minions  , 
external pillars
   about  
   cmd_json  
   cmd_yaml  
   etcd pillar, configuring  , 
   git  , 
   git_pillar, using  , , 
   multiple external pillars, using  
   mysql  , 
   mysql pillar, using  , 
   pillar data, caching  
   redis  , 
   using  
F
file maps
   using  
file_roots  
files
   base configuration  , 
   event reactor, using  , , 
   managing, with templates  , 
   nonexternal files, synchronizing  
   rsync, using  
Filesystem Hierarchy Standard (FHS)  
flat roster
   about  
   host  
   passwd  
   port  
   priv  
   sudo  
   thin_dir  
   timeout  
   user  
flow-based programming (FBP)  
   components  
formula
   reference link  
G
generic names  , 
git_pillar
   using  , , 
GitFS
   about  , 
   base  
   insecure_auth  
   mountpoint  
   passphrase  , 
   password  
   privkey  , 
   pubkey  , 
   root  
   username  
GitHub
   Salt Stack formulas, using  
grains
   about  
   using, for minion-specific data  , , 
graphite tool  
H
hierarchy, Salt configuration
   /etc/salt/ directory, viewing  , 
   Salt keys, managing  
   SLS directories  , 
high availability
   about  
   configuration, built-in  
   files, synchronizing  

[ 342 ]
   load balancing, IP-based  
   multiple masters  
   round-robin DNS  
high data  
high states  , 
host-based security  
HTTP library
   about  
   http.query function, using  , 
   http.query state, using  
   Salt-specific library  , 
http.query function
   GET, versus POST  , , 
   return data, decoding  
   using  , 
http.query state
   using, with reactors  , , 
HTTP
   considerations  , 
I
idempotency  
imperative
   versus declarative  
include blocks
   using  
infrastructure
   load, testing in  
   planning  , 
init.sls file
   versus .sls file  , 
Internet Relay Chat (IRC)
   about  
   URL  
issue tracker, Salt
   features, requesting  , 
   issues, formatting  , 
   posting, research  , 
   reference link  
issue
   addressing  
   CPU congestion  , 
   identifying  
   master-to-minion communication  , 
   minion load, checking  , , 
   network conjestion  
   Salt job data, querying  , 
J
Jinja  , , , 
JSON file  
L
lazy loader  
legacy deploy scripts  
Linux Standard Base (LSB)  
load
   minionswarm.py script, using  
   testing, in infrastructure  
loader
   about  
   cloud modules  
   dynamic modules  , 
   execution modules  , 
low chunk  
low data  
low state  
low states  , 
LWP (lib-www-perl)  
M
Mako  
markdown format
   reference link  
master  
master API
   configuration  
   file_roots  
   pillar_roots  
   Salt keys  
   using  
   wheel reactor, using  , 
master configuration  
master tops  , 
Mercurial (HG)  
minion configuration  
minions
   about  
   compound  
   glob  

[ 343 ]
   grain  
   grain PCRE  
   grains, used for minion-specific data  , , 
   list  
   nodegroup  
   Perl Compatible Regular Expression (PCRE)  
   pillar  
   subnet  
   targeting  
minionswarm.py script
   swarm internals  
   URL, for downloading  
   using  
module functions
   file functions  
   file.replace  
   group functions  
   pkg.install  
   pkg.remove  
   sys.doc  , 
   test.echo  
   test.ping  
   test.sleep  
   test.version  
   user functions  
   using  , 
monitoring
   baseline, creating  , 
   basics  
   file changes  , 
   returners, determining  , 
   states, defining  
   states, using  , 
   system vitals, reading in Salt  
   with returners  , 
   with web calls  , 
multiple environments
   base environment, using  
   separating  , 
   spanning  
multiple masters
   high availability  
multiple states
   handling  
   SLS files, including  , 
mysql pillar
   using  , 
N
namespacing
   guidelines  , 
naming conventions  
   explicit names  
   generic names  , 
O
Online YAML Parser
   reference link  
OpenSSH  
OpenStack
   SDB, used with  , 
P
package manager
   SPM, using  
packages, Salt Package Manager (SPM)
   dependencies field  
   description field  
   license field  
   minimum_version field  
   name field  
   optional field  
   os field  
   os_family  
   recommended field  
   release field  
   summary field  
   top_level_dir  
   version field  
Perl Compatible Regular Expression (PCRE)  
pillar_roots
   about  , , 
   modules, calling out  , 
   templating  
pillars
   about  
   data, generating  , 
   drivers, differences  
   reference link  
   Salt Cloud  

[ 344 ]
   variables, centralizing with  , 
   versus SDB  
pipes
   rendering  
Pluggable Authentication Module (PAM)  
port 4505  
port 4506  
profile configuration  , 
provider configuration  , 
Puppet  
PyCrypto  
PyYAML library  
Q
queue system
   adding to  
   items, deleting  
   items, listing  
   items, processing  
   listing  
   State runs, spreading out  
   tasks, dividing among minions  , , 
   using  
   using, with reactor  
   working  , 
R
raw SSH mode
   SSH connections, caching  , 
   using  
reactors
   about  
   building  
   complex reactors, writing  
   configuring  , 
   event tags, setting up  , , , 
   execution modules, calling  , 
   queue system, using with  
   runner modules, calling  
   using, with Salt Cloud  
   wheel modules, calling  , 
   writing  , 
reliable asynchronous event transport (RAET)
   about  , 
   architecture  
   configuring  , 
   HTTP, considerations  , 
   SSH  
   usage  , 
   versus ZeroMQ  
   ZeroMQ, using  
remote shells
   about  , 
renderer
   about  
   templated files, serving  , 
repository manager
   metadata, downloading  
   SPM repositories, configuring  
   SPM, using  
requisites
   about  , 
   for ordering  
   inverting  
   onchanges  
   onfail  
   require  
   use  
   watch  
returners
   determining  , 
   monitoring with  , 
rosters
   built-in rosters  
   dynamic rosters, building  
   flat roster  
   Salt SSH, using  
   Salt, versus Salt SSH  
   using  
round-robin DNS  
S
S3FS driver
   about  
   multiple environments per bucket  , 
   one environment per bucket  
Salt API
   about  
   authentication, configuring  , 
   CherryPy  , 

[ 345 ]
   commands, issuing  
   events  
   planning  , , , 
   salt/netapi/url_path  
   setting up  
   SSL certificates, creating  
   Tornado  
   Web Server Gateway Interface (WSGI)  , 
Salt Bootstrap script
   about  , 
   Git, installing from  
   prebuilt packages, installing from  , , 
Salt cache
   examining  
   external file server cache  
   external modules  
   master job cache  , 
   master-side minion cache  
   minion-side proc / directory  
Salt Cloud
   about  
   configuration blocks, extending  , , 
   configuration, examining  
   global configurations  , 
   profile configuration  
   provider configuration  
   reactors, used with  
   SDB, used with  , 
Salt commands
   executing, remotely  
   master  
   minions  
   minions, targeting  
   module functions, using  , 
Salt configuration
   about  
   cache, examining  
   hierarchy  
Salt keys  
Salt Package Manager (SPM)
   about  
   configuration  
   directory to hold packages  
   directory, polpulating  
   directory, sharing  
   packages, building  
   repositories, creating  
   repository metadata, creating  
   usage  
   using  
   using, as package manager  
   using, as repository manager  
Salt SSH
   Saltfile, using  
   using  
   versus Salt  
Salt Stack formulas
   formulas, cloning  
   standard layout, analyzing  
   URL  
   using, on GitHub  
Salt states
   include blocks, using  
   prereq  
   requisites, inverting  
   requisites, ordering with  
   SLS files, extending  
   using, for configuration management  
Salt, versus Salt SSH
   architecture  
   performance  
salt-call command
   using  , 
salt-thin agent
   about  , 
   thin package, building  , 
   thin package, deploying  
   thin package, executing  
Salt-users
   #salt on IRC  
   about  , 
   mailing list  , 
   URL, for Google  
   URL, for subscribing  
   URL, for unsubscribing  
Salt
   issue tracker  , 
   system vitals, reading in  
   troubleshooting  , , 
   versus Salt SSH  

[ 346 ]
Saltfile  
security considerations  , 
security through obscurity  
shallow directory structure
   versus deep directory structure  
shared storage  
simple database (SDB)
   about  
   configuring  , 
   data, deleting  , 
   data, obtaining  
   data, setting  
   lookups, performing  
   passwords, storing securely  
   URIs  
   URIs, using  
   using, with AWS  , 
   using, with cloud providers  
   using, with EC2  , 
   using, with OpenStack  , 
   using, with Salt Cloud  , 
   versus pillars  
Slack
   URL  
SLS directories
   organizing  
SLS efficiency
   about  
   built-in states, using  , 
   extends, using  , 
   includes, using  , 
   templates, using to SLS files  
SLS files
   about  
   extending  
   heirarchy  
   including  , 
   rendering  , 
   SLS directories, organizing  
   templates, using to  
   top files, merging  , 
SPM configuration
   formula_path  
   pillar_path  
   reactor_path  
   spm_build_dir  
   spm_build_exclude  
   spm_cache_dir  
   spm_conf_file  
   spm_db  
   spm_default_include  
   spm_logfile  
   spm_repos_config  
spot instance feature  
SSH
   about  , 
   connections, caching  , 
   remote shells  , 
standard layout
   analyzing  
   FORMULA file  
   formula name  
   LICENSE file  
   pillar.example file  
   README file  
state compiler
   about  
   high states  , 
   imperative, versus declarative  
   low states  , , 
   requisites  , 
   statefulness  
state files
   alerts  
statefulness
   changes  
   comment  
   enforcing  
   name  
   result  
subversion (SVN)  
SVNFS
   branches  
   mountpoint  
   root  
   tags  
   trunk  
syndication
   about  
   configuring  

[ 347 ]
   hierarchy, building  , 
   micromanaging, not needed  , 
system vitals
   disk.usage function  
   reading, in Salt  
   status.all_status function  , 
   status.cpustats function  
   status.custom function  , 
   status.diskusage function  
   status.loadavg module  
   status.meminfo function  
   status.vmstats function  
   status.w function  
T
TCP transport
   examining  
   using  , 
templated files
   serving  , 
templates
   about  , 
   branching  , 
   files, managing with  , 
   loops, working with  , 
   using, to SLS files  
   variables, referring to  , 
thin package
   building  , 
   deploying  
   executing  
   modules, adding  
   Salt SSH shim  
   Salt states, preparing  
   Salt's running data  
   Salt, running  , 
Thorium
   aggregate data, using with  , , 
   as engines  
   basics  
   directory tree, setting up  , 
   enabling  
   reacting with  , , 
   register, using  , , 
   requisites, using  
   SLS files, writing  
   using  
   viewing  
Tornado  , 
trace mode
   about  
   error  
   info  
   using  
   warn  
troubleshooting
   debug mode, using  
   issue, addressing  
   issue, identifying  
   Salt  
   trace mode, using  
U
Unix domain sockets (UXD)  
V
variables
   about  
   centralizing, with pillars  , 
   nested variables  , 
   referring, in templates  , 
W
web calls
   monitoring with  , 
webhooks
   complex authentication  , 
   reference link  
   security considerations  , 
   Thorium, reacting with  , , 
   using  , , 
WEBrick  
wheel reactor
   using  , 
   using, with Thorium  
Y
YAML
   about  

   basics  
   booleans  , 
   dict  
   idiosyncrasies  
   list  , 
   list items  
   numbers  
   spacing  
   troubleshooting  , 
   using  
Z
ZeroMQ
   about  , 
   using  
   versus RAET  


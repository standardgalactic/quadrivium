Mathematics of the Discrete Fourier Transform
(DFT)
Julius O. Smith III (jos@ccrma.stanford.edu)
Center for Computer Research in Music and Acoustics (CCRMA)
Department of Music, Stanford University
Stanford, California 94305
March 15, 2002

Page ii
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Contents
1
Introduction to the DFT
1
1.1
DFT Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Mathematics of the DFT
. . . . . . . . . . . . . . . . . .
3
1.3
DFT Math Outline . . . . . . . . . . . . . . . . . . . . . .
6
2
Complex Numbers
7
2.1
Factoring a Polynomial . . . . . . . . . . . . . . . . . . . .
7
2.2
The Quadratic Formula
. . . . . . . . . . . . . . . . . . .
8
2.3
Complex Roots . . . . . . . . . . . . . . . . . . . . . . . .
9
2.4
Fundamental Theorem of Algebra . . . . . . . . . . . . . .
11
2.5
Complex Basics . . . . . . . . . . . . . . . . . . . . . . . .
11
2.5.1
The Complex Plane
. . . . . . . . . . . . . . . . .
13
2.5.2
More Notation and Terminology
. . . . . . . . . .
14
2.5.3
Elementary Relationships . . . . . . . . . . . . . .
15
2.5.4
Euler’s Formula . . . . . . . . . . . . . . . . . . . .
15
2.5.5
De Moivre’s Theorem
. . . . . . . . . . . . . . . .
17
2.6
Numerical Tools in Matlab
. . . . . . . . . . . . . . . . .
17
2.7
Numerical Tools in Mathematica . . . . . . . . . . . . . .
23
3
Proof of Euler’s Identity
27
3.1
Euler’s Theorem
. . . . . . . . . . . . . . . . . . . . . . .
27
3.1.1
Positive Integer Exponents
. . . . . . . . . . . . .
27
3.1.2
Properties of Exponents . . . . . . . . . . . . . . .
28
3.1.3
The Exponent Zero . . . . . . . . . . . . . . . . . .
28
3.1.4
Negative Exponents
. . . . . . . . . . . . . . . . .
28
3.1.5
Rational Exponents
. . . . . . . . . . . . . . . . .
29
3.1.6
Real Exponents . . . . . . . . . . . . . . . . . . . .
30
3.1.7
A First Look at Taylor Series . . . . . . . . . . . .
31
3.1.8
Imaginary Exponents
. . . . . . . . . . . . . . . .
32
iii

Page iv
CONTENTS
3.1.9
Derivatives of f(x) = ax . . . . . . . . . . . . . . .
32
3.1.10 Back to e . . . . . . . . . . . . . . . . . . . . . . .
33
3.1.11 Sidebar on Mathematica . . . . . . . . . . . . . . .
34
3.1.12 Back to ejθ . . . . . . . . . . . . . . . . . . . . . .
34
3.2
Informal Derivation of Taylor Series
. . . . . . . . . . . .
36
3.3
Taylor Series with Remainder . . . . . . . . . . . . . . . .
37
3.4
Formal Statement of Taylor’s Theorem . . . . . . . . . . .
39
3.5
Weierstrass Approximation Theorem . . . . . . . . . . . .
40
3.6
Diﬀerentiability of Audio Signals . . . . . . . . . . . . . .
40
4
Logarithms, Decibels, and Number Systems
41
4.1
Logarithms
. . . . . . . . . . . . . . . . . . . . . . . . . .
41
4.1.1
Changing the Base . . . . . . . . . . . . . . . . . .
43
4.1.2
Logarithms of Negative and Imaginary Numbers .
43
4.2
Decibels . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
4.2.1
Properties of DB Scales . . . . . . . . . . . . . . .
45
4.2.2
Speciﬁc DB Scales . . . . . . . . . . . . . . . . . .
46
4.2.3
Dynamic Range . . . . . . . . . . . . . . . . . . . .
52
4.3
Linear Number Systems for Digital Audio . . . . . . . . .
53
4.3.1
Pulse Code Modulation (PCM) . . . . . . . . . . .
53
4.3.2
Binary Integer Fixed-Point Numbers . . . . . . . .
53
4.3.3
Fractional Binary Fixed-Point Numbers . . . . . .
58
4.3.4
How Many Bits are Enough for Digital Audio? . .
58
4.3.5
When Do We Have to Swap Bytes? . . . . . . . . .
59
4.4
Logarithmic Number Systems for Audio . . . . . . . . . .
61
4.4.1
Floating-Point Numbers . . . . . . . . . . . . . . .
61
4.4.2
Logarithmic Fixed-Point Numbers . . . . . . . . .
63
4.4.3
Mu-Law Companding
. . . . . . . . . . . . . . . .
64
4.5
Appendix A: Round-OﬀError Variance
. . . . . . . . . .
65
4.6
Appendix B: Electrical Engineering 101
. . . . . . . . . .
66
5
Sinusoids and Exponentials
69
5.1
Sinusoids
. . . . . . . . . . . . . . . . . . . . . . . . . . .
69
5.1.1
Example Sinusoids . . . . . . . . . . . . . . . . . .
70
5.1.2
Why Sinusoids are Important . . . . . . . . . . . .
71
5.1.3
In-Phase and Quadrature Sinusoidal Components .
72
5.1.4
Sinusoids at the Same Frequency . . . . . . . . . .
73
5.1.5
Constructive and Destructive Interference . . . . .
74
5.2
Exponentials
. . . . . . . . . . . . . . . . . . . . . . . . .
76
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CONTENTS
Page v
5.2.1
Why Exponentials are Important . . . . . . . . . .
77
5.2.2
Audio Decay Time (T60)
. . . . . . . . . . . . . .
78
5.3
Complex Sinusoids . . . . . . . . . . . . . . . . . . . . . .
78
5.3.1
Circular Motion
. . . . . . . . . . . . . . . . . . .
79
5.3.2
Projection of Circular Motion . . . . . . . . . . . .
79
5.3.3
Positive and Negative Frequencies
. . . . . . . . .
80
5.3.4
The Analytic Signal and Hilbert Transform Filters
81
5.3.5
Generalized Complex Sinusoids . . . . . . . . . . .
85
5.3.6
Sampled Sinusoids . . . . . . . . . . . . . . . . . .
86
5.3.7
Powers of z . . . . . . . . . . . . . . . . . . . . . .
86
5.3.8
Phasor & Carrier Components of Complex Sinusoids 87
5.3.9
Why Generalized Complex Sinusoids are Important
89
5.3.10 Comparing Analog and Digital Complex Planes . .
91
5.4
Mathematica for Selected Plots . . . . . . . . . . . . . . .
94
5.5
Acknowledgement . . . . . . . . . . . . . . . . . . . . . . .
95
6
Geometric Signal Theory
97
6.1
The DFT
. . . . . . . . . . . . . . . . . . . . . . . . . . .
97
6.2
Signals as Vectors . . . . . . . . . . . . . . . . . . . . . . .
98
6.3
Vector Addition . . . . . . . . . . . . . . . . . . . . . . . .
99
6.4
Vector Subtraction . . . . . . . . . . . . . . . . . . . . . .
100
6.5
Signal Metrics . . . . . . . . . . . . . . . . . . . . . . . . .
100
6.6
The Inner Product . . . . . . . . . . . . . . . . . . . . . .
105
6.6.1
Linearity of the Inner Product
. . . . . . . . . . .
106
6.6.2
Norm Induced by the Inner Product . . . . . . . .
107
6.6.3
Cauchy-Schwarz Inequality
. . . . . . . . . . . . .
107
6.6.4
Triangle Inequality . . . . . . . . . . . . . . . . . .
108
6.6.5
Triangle Diﬀerence Inequality . . . . . . . . . . . .
109
6.6.6
Vector Cosine . . . . . . . . . . . . . . . . . . . . .
109
6.6.7
Orthogonality . . . . . . . . . . . . . . . . . . . . .
109
6.6.8
The Pythagorean Theorem in N-Space . . . . . . .
110
6.6.9
Projection . . . . . . . . . . . . . . . . . . . . . . .
111
6.7
Signal Reconstruction from Projections
. . . . . . . . . .
111
6.7.1
An Example of Changing Coordinates in 2D
. . .
113
6.7.2
General Conditions . . . . . . . . . . . . . . . . . .
115
6.7.3
Gram-Schmidt Orthogonalization . . . . . . . . . .
119
6.8
Appendix: Matlab Examples
. . . . . . . . . . . . . . . .
120
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page vi
CONTENTS
7
Derivation of the Discrete Fourier Transform (DFT)
127
7.1
The DFT Derived
. . . . . . . . . . . . . . . . . . . . . .
127
7.1.1
Geometric Series . . . . . . . . . . . . . . . . . . .
127
7.1.2
Orthogonality of Sinusoids . . . . . . . . . . . . . .
128
7.1.3
Orthogonality of the DFT Sinusoids . . . . . . . .
131
7.1.4
Norm of the DFT Sinusoids . . . . . . . . . . . . .
131
7.1.5
An Orthonormal Sinusoidal Set . . . . . . . . . . .
131
7.1.6
The Discrete Fourier Transform (DFT)
. . . . . .
132
7.1.7
Frequencies in the “Cracks” . . . . . . . . . . . . .
133
7.1.8
Normalized DFT . . . . . . . . . . . . . . . . . . .
136
7.2
The Length 2 DFT . . . . . . . . . . . . . . . . . . . . . .
137
7.3
Matrix Formulation of the DFT . . . . . . . . . . . . . . .
138
7.4
Matlab Examples . . . . . . . . . . . . . . . . . . . . . . .
140
7.4.1
Figure 7.2 . . . . . . . . . . . . . . . . . . . . . . .
140
7.4.2
Figure 7.3 . . . . . . . . . . . . . . . . . . . . . . .
141
7.4.3
DFT Matrix in Matlab
. . . . . . . . . . . . . . .
142
8
Fourier Theorems for the DFT
145
8.1
The DFT and its Inverse . . . . . . . . . . . . . . . . . . .
145
8.1.1
Notation and Terminology . . . . . . . . . . . . . .
146
8.1.2
Modulo Indexing, Periodic Extension . . . . . . . .
146
8.2
Signal Operators . . . . . . . . . . . . . . . . . . . . . . .
148
8.2.1
Flip Operator . . . . . . . . . . . . . . . . . . . . .
148
8.2.2
Shift Operator
. . . . . . . . . . . . . . . . . . . .
148
8.2.3
Convolution . . . . . . . . . . . . . . . . . . . . . .
151
8.2.4
Correlation . . . . . . . . . . . . . . . . . . . . . .
154
8.2.5
Stretch Operator . . . . . . . . . . . . . . . . . . .
155
8.2.6
Zero Padding . . . . . . . . . . . . . . . . . . . . .
155
8.2.7
Repeat Operator . . . . . . . . . . . . . . . . . . .
156
8.2.8
Downsampling Operator . . . . . . . . . . . . . . .
158
8.2.9
Alias Operator . . . . . . . . . . . . . . . . . . . .
160
8.3
Even and Odd Functions . . . . . . . . . . . . . . . . . . .
163
8.4
The Fourier Theorems . . . . . . . . . . . . . . . . . . . .
165
8.4.1
Linearity
. . . . . . . . . . . . . . . . . . . . . . .
165
8.4.2
Conjugation and Reversal . . . . . . . . . . . . . .
166
8.4.3
Symmetry . . . . . . . . . . . . . . . . . . . . . . .
167
8.4.4
Shift Theorem
. . . . . . . . . . . . . . . . . . . .
169
8.4.5
Convolution Theorem
. . . . . . . . . . . . . . . .
171
8.4.6
Dual of the Convolution Theorem
. . . . . . . . .
173
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CONTENTS
Page vii
8.4.7
Correlation Theorem . . . . . . . . . . . . . . . . .
173
8.4.8
Power Theorem . . . . . . . . . . . . . . . . . . . .
174
8.4.9
Rayleigh Energy Theorem (Parseval’s Theorem)
.
174
8.4.10 Stretch Theorem (Repeat Theorem) . . . . . . . .
175
8.4.11 Downsampling Theorem (Aliasing Theorem)
. . .
175
8.4.12 Zero Padding Theorem . . . . . . . . . . . . . . . .
177
8.4.13 Bandlimited Interpolation in Time . . . . . . . . .
178
8.5
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . .
179
8.6
Acknowledgement . . . . . . . . . . . . . . . . . . . . . . .
179
8.7
Appendix A: Linear Time-Invariant Filters and Convolution180
8.7.1
LTI Filters and the Convolution Theorem . . . . .
181
8.8
Appendix B: Statistical Signal Processing . . . . . . . . .
182
8.8.1
Cross-Correlation . . . . . . . . . . . . . . . . . . .
182
8.8.2
Applications of Cross-Correlation . . . . . . . . . .
183
8.8.3
Autocorrelation . . . . . . . . . . . . . . . . . . . .
186
8.8.4
Coherence . . . . . . . . . . . . . . . . . . . . . . .
187
8.9
Appendix C: The Similarity Theorem
. . . . . . . . . . .
188
9
Example Applications of the DFT
191
9.1
Spectrum Analysis of a Sinusoid: Windowing, Zero-Padding,
and the FFT
. . . . . . . . . . . . . . . . . . . . . . . . .
191
9.1.1
Example 1: FFT of a Simple Sinusoid . . . . . . .
191
9.1.2
Example 2: FFT of a Not-So-Simple Sinusoid . . .
194
9.1.3
Example 3: FFT of a Zero-Padded Sinusoid . . . .
197
9.1.4
Example 4: Blackman Window . . . . . . . . . . .
199
9.1.5
Example 5: Use of the Blackman Window . . . . .
201
9.1.6
Example 6: Hanning-Windowed Complex Sinusoid
203
A Matrices
211
A.0.1
Matrix Multiplication
. . . . . . . . . . . . . . . .
212
A.0.2
Solving Linear Equations Using Matrices
. . . . .
215
B Sampling Theory
217
B.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . .
217
B.1.1
Reconstruction from Samples—Pictorial Version
.
218
B.1.2
Reconstruction from Samples—The Math . . . . .
219
B.2
Aliasing of Sampled Signals . . . . . . . . . . . . . . . . .
220
B.3
Shannon’s Sampling Theorem . . . . . . . . . . . . . . . .
223
B.4
Another Path to Sampling Theory . . . . . . . . . . . . .
225
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page viii
CONTENTS
B.4.1
What frequencies are representable by a geometric
sequence? . . . . . . . . . . . . . . . . . . . . . . .
226
B.4.2
Recovering a Continuous Signal from its Samples .
228
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Preface
This reader is an outgrowth of my course entitled “Introduction to Digital
Signal Processing and the Discrete Fourier Transform (DFT)1 which I
have given at the Center for Computer Research in Music and Acoustics
(CCRMA) every year for the past 16 years.
The course was created
primarily as a ﬁrst course in digital signal processing for entering Music
Ph.D. students. As a result, the only prerequisite is a good high-school
math background. Calculus exposure is desirable, but not required.
Outline
Below is an overview of the chapters.
• Introduction to the DFT
This chapter introduces the Discrete Fourier Transform (DFT) and
points out the elements which will be discussed in this reader.
• Introduction to Complex Numbers
This chapter provides an introduction to complex numbers, factor-
ing polynomials, the quadratic formula, the complex plane, Euler’s
formula, and an overview of numerical facilities for complex num-
bers in Matlab and Mathematica.
• Proof of Euler’s Identity
This chapter outlines the proof of Euler’s Identity, which is an im-
portant tool for working with complex numbers. It is one of the
critical elements of the DFT deﬁnition that we need to understand.
• Logarithms, Decibels, and Number Systems
This chapter discusses logarithms (real and complex), decibels, and
1http://www-ccrma.stanford.edu/CCRMA/Courses/320/
ix

Page x
CONTENTS
number systems such as binary integer ﬁxed-point, fractional ﬁxed-
point, one’s complement, two’s complement, logarithmic ﬁxed-point,
µ-law, and ﬂoating-point number formats.
• Sinusoids and Exponentials
This chapter provides an introduction to sinusoids, exponentials,
complex sinusoids, t60, in-phase and quadrature sinusoidal compo-
nents, the analytic signal, positive and negative frequencies, con-
structive and destructive interference, invariance of sinusoidal fre-
quency in linear time-invariant systems, circular motion as the vec-
tor sum of in-phase and quadrature sinusoidal motions, sampled
sinusoids, generating sampled sinusoids from powers of z, and plot
examples using Mathematica.
• The Discrete Fourier Transform (DFT) Derived
This chapter derives the Discrete Fourier Transform (DFT) as a
projection of a length N signal x(·) onto the set of N sampled
complex sinusoids generated by the N roots of unity.
• Fourier Theorems for the DFT
This chapter derives various Fourier theorems for the case of the
DFT. Included are symmetry relations, the shift theorem, convo-
lution theorem, correlation theorem, power theorem, and theorems
pertaining to interpolation and downsampling. Applications related
to certain theorems are outlined, including linear time-invariant ﬁl-
tering, sampling rate conversion, and statistical signal processing.
• Example Applications of the DFT
This chapter goes through some practical examples of FFT anal-
ysis in Matlab. The various Fourier theorems provide a “thinking
vocabulary” for understanding elements of spectral analysis.
• A Basic Tutorial on Sampling Theory
This appendix provides a basic tutorial on sampling theory. Alias-
ing due to sampling of continuous-time signals is characterized math-
ematically. Shannon’s sampling theorem is proved. A pictorial rep-
resentation of continuous-time signal reconstruction from discrete-
time samples is given.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Chapter 1
Introduction to the DFT
This chapter introduces the Discrete Fourier Transform (DFT) and points
out the elements which will be discussed in this reader.
1.1
DFT Deﬁnition
The Discrete Fourier Transform (DFT) of a signal x may be deﬁned by
X(ωk) ∆=
N−1

n=0
x(tn)e−jωktn,
k = 0, 1, 2, . . . , N −1
and its inverse (the IDFT) is given by
x(tn) = 1
N
N−1

k=0
X(ωk)ejωktn,
n = 0, 1, 2, . . . , N −1
1

Page 2
1.1.
DFT DEFINITION
where
x(tn)
∆=
input signal amplitude at time tn (sec)
tn
∆=
nT = nth sampling instant (sec)
n
∆=
sample number (integer)
T
∆=
sampling period (sec)
X(ωk)
∆=
Spectrum of x, at radian frequency ωk
ωk
∆=
kΩ= kth frequency sample (rad/sec)
Ω
∆=
2π
NT = radian-frequency sampling interval
fs
∆=
1/T = sampling rate (samples/sec, or Hertz (Hz))
N
=
number of samples in both time and frequency (integer)
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 1. INTRODUCTION TO THE DFT
Page 3
1.2
Mathematics of the DFT
In the signal processing literature, it is common to write the DFT in the
more pure form obtained by setting T = 1 in the previous deﬁnition:
X(k)
∆=
N−1

n=0
x(n)e−j2πnk/N,
k = 0, 1, 2, . . . , N −1
x(n)
=
1
N
N−1

k=0
X(k)ej2πnk/N,
n = 0, 1, 2, . . . , N −1
where x(n) denotes the input signal at time (sample) n, and X(k) denotes
the kth spectral sample.1 This form is the simplest mathematically while
the previous form is the easier to interpret physically.
There are two remaining symbols in the DFT that we have not yet
deﬁned:
j
∆=
√
−1
e
∆=
lim
n→∞

1 + 1
n
n
= 2.71828182845905 . . .
The ﬁrst, j = √−1, is the basis for complex numbers. As a result, complex
numbers will be the ﬁrst topic we cover in this reader (but only to the
extent needed to understand the DFT).
The second, e = 2.718 . . ., is a transcendental number deﬁned by the
above limit. In this reader, we will derive e and talk about why it comes
up.
Note that not only do we have complex numbers to contend with, but
we have them appearing in exponents, as in
sk(n) ∆= ej2πnk/N
We will systematically develop what we mean by imaginary exponents in
order that such mathematical expressions are well deﬁned.
With e, j, and imaginary exponents understood, we can go on to
prove Euler’s Identity:
ejθ = cos(θ) + j sin(θ)
1Note that the deﬁnition of x() has changed unless the sampling rate fs really is 1,
and the deﬁnition of X() has changed no matter what the sampling rate is, since when
T = 1, ωk = 2πk/N, not k.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 4
1.2. MATHEMATICS OF THE DFT
Euler’s Identity is the key to understanding the meaning of expressions
like
sk(tn) ∆= ejωktn = cos(ωktn) + j sin(ωktn)
We’ll see that such an expression deﬁnes a sampled complex sinusoid, and
we’ll talk about sinusoids in some detail, from an audio perspective.
Finally, we need to understand what the summation over n is doing
in the deﬁnition of the DFT. We’ll learn that it should be seen as the
computation of the inner product of the signals x and sk, so that we may
write the DFT using inner-product notation as
X(k) ∆= ⟨x, sk⟩
where
sk(n) ∆= ej2πnk/N
is the sampled complex sinusoid at (normalized) radian frequency ωk =
2πk/N, and the inner product operation is deﬁned by
⟨x, y⟩∆=
N−1

n=0
x(n)y(n)
We will show that the inner product of x with the kth “basis sinusoid”
sk is a measure of “how much” of sk is present in x and at “what phase”
(since it is a complex number).
After the foregoing, the inverse DFT can be understood as the weighted
sum of projections of x onto {sk}N−1
k=0 , i.e.,
x(n) ∆=
N−1

k=0
˜Xksk(n)
where
˜Xk
∆= X(k)
N
is the (actual) coeﬃcient of projection of x onto sk.
Referring to the
whole signal as x ∆= x(·), the IDFT can be written as
x ∆=
N−1

k=0
˜Xksk
Note that both the basis sinusoids sk and their coeﬃcients of projection
˜Xk are complex.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 1. INTRODUCTION TO THE DFT
Page 5
Having completely understood the DFT and its inverse mathemati-
cally, we go on to proving various Fourier Theorems, such as the “shift
theorem,” the “convolution theorem,” and “Parseval’s theorem.”
The
Fourier theorems provide a basic thinking vocabulary for working with
signals in the time and frequency domains. They can be used to answer
questions like
What happens in the frequency domain if I do [x] in the time
domain?
Finally, we will study a variety of practical spectrum analysis exam-
ples, using primarily Matlab to analyze and display signals and their
spectra.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 6
1.3. DFT MATH OUTLINE
1.3
DFT Math Outline
In summary, understanding the DFT takes us through the following top-
ics:
1. Complex numbers
2. Complex exponents
3. Why e?
4. Euler’s formula
5. Projecting signals onto signals via the inner product
6. The DFT as the coeﬃcient of projection of a signal x onto a sinusoid
7. The IDFT as a weighted sum of sinusoidal projections
8. Various Fourier theorems
9. Elementary time-frequency pairs
10. Practical spectrum analysis in matlab
We will additionally discuss practical aspects of working with sinu-
soids, such as decibels (dB) and display techniques.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Chapter 2
Complex Numbers
This chapter provides an introduction to complex numbers, factoring
polynomials, the quadratic formula, the complex plane, Euler’s formula,
and an overview of numerical facilities for complex numbers in Matlab
and Mathematica.
2.1
Factoring a Polynomial
Remember “factoring polynomials”? Consider the second-order polyno-
mial
p(x) = x2 −5x + 6
It is second-order because the highest power of x is 2 (only non-negative
integer powers of x are allowed in this context). The polynomial is also
monic because its leading coeﬃcient, the coeﬃcient of x2, is 1. Since it is
second order, there are at most two real roots (or zeros) of the polynomial.
Suppose they are denoted x1 and x2.
Then we have p(x1) = 0 and
p(x2) = 0, and we can write
p(x) = (x −x1)(x −x2)
This is the factored form of the monic polynomial p(x). (For a non-monic
polynomial, we may simply divide all coeﬃcients by the ﬁrst to make it
monic, and this doesn’t aﬀect the zeros.) Multiplying out the symbolic
factored form gives
p(x) = (x −x1)(x −x2) = x2 −(x1 + x2)x + x1x2
7

Page 8
2.2. THE QUADRATIC FORMULA
Comparing with the original polynomial, we ﬁnd we must have
x1 + x2
=
5
x1x2
=
6
This is a system of two equations in two unknowns. Unfortunately, it
is a nonlinear system of two equations in two unknowns.1 Nevertheless,
because it is so small, the equations are easily solved. In beginning alge-
bra, we did them by hand. However, nowadays we can use a computer
program such as Mathematica:
In[]:=
Solve[{x1+x2==5, x1 x2 == 6}, {x1,x2}]
Out[]:
{{x1 -> 2, x2 -> 3}, {x1 -> 3, x2 -> 2}}
Note that the two lists of substitutions point out that it doesn’t matter
which root is 2 and which is 3. In summary, the factored form of this
simple example is
p(x) = x2 −5x + 6 = (x −x1)(x −x2) = (x −2)(x −3)
Note that polynomial factorization rewrites a monic nth-order polyno-
mial as the product of n ﬁrst-order monic polynomials, each of which
contributes one zero (root) to the product.
This factoring business is
often used when working with digital ﬁlters.
2.2
The Quadratic Formula
The general second-order polynomial is
p(x) ∆= ax2 + bx + c
where the coeﬃcients a, b, c are any real numbers, and we assume a ̸= 0
since otherwise it would not be second order. Some experiments plotting
1“Linear” in this context means that the unknowns are multiplied only by
constants—they may not be multiplied by each other or raised to any power other
than 1 (e.g., not squared or cubed or raised to the 1/5 power). Linear systems of N
equations in N unknowns are very easy to solve compared to nonlinear systems of N
equations in N unknowns. For example, Matlab or Mathematica can easily handle
them. You learn all about this in a course on Linear Algebra which is highly recom-
mended for anyone interested in getting involved with signal processing. Linear algebra
also teaches you all about matrices which we will introduce only brieﬂy in this reader.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 2. COMPLEX NUMBERS
Page 9
p(x) for diﬀerent values of the coeﬃcients leads one to guess that the
curve is always a scaled and translated parabola. The canonical parabola
centered at x = x0 is given by
y(x) = d(x −x0)2 + e
where d determines the width and e provides an arbitrary vertical oﬀset.
If we can ﬁnd d, e, x0 in terms of a, b, c for any quadratic polynomial,
then we can easily factor the polynomial. This is called “completing the
square.” Multiplying out y(x), we get
y(x) = d(x −x0)2 + e = dx2 −2dx0x + dx2
0 + e
Equating coeﬃcients of like powers of x gives
d
=
a
−2dx0
=
b
⇒
x0 = −b/(2a)
dx2
0 + e
=
c
⇒
e = c −b2/(4a)
Using these answers, any second-order polynomial p(x) = ax2 + bx + c
can be rewritten as a scaled, translated parabola
p(x) = a

x + b
2a
2
+

c −b2
4a

.
In this form, the roots are easily found by solving p(x) = 0 to get
x = −b ±
√
b2 −4ac
2a
This is the general quadratic formula. It was obtained by simple algebraic
manipulation of the original polynomial. There is only one “catch.” What
happens when b2 −4ac is negative? This introduces the square root of
a negative number which we could insist “does not exist.” Alternatively,
we could invent complex numbers to accommodate it.
2.3
Complex Roots
As a simple example, let a = 1, b = 0, and c = 4, i.e.,
p(x) = x2 + 4
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 10
2.3. COMPLEX ROOTS
-3
-2
-1
0
1
2
3
0
1
2
3
4
5
6
7
8
9
10
x
y(x)
Figure 2.1: An example parabola deﬁned by p(x) = x2 + 4.
As shown in Fig. 2.1, this is a parabola centered at x = 0 (where p(0) = 4)
and reaching upward to positive inﬁnity, never going below 4. It has no
zeros. On the other hand, the quadratic formula says that the “roots”
are given formally by x = ±√−4 = ±2√−1. The square root of any
negative number c < 0 can be expressed as

|c|√−1, so the only new
algebraic object is √−1. Let’s give it a name:
j ∆=
√
−1
Then, formally, the roots of of x2+4 are ±2j, and we can formally express
the polynomial in terms of its roots as
p(x) = (x + 2j)(x −2j)
We can think of these as “imaginary roots” in the sense that square roots
of negative numbers don’t really exist, or we can extend the concept of
“roots” to allow for complex numbers, that is, numbers of the form
z = x + jy
where x and y are real numbers, and j2 ∆= −1.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 2. COMPLEX NUMBERS
Page 11
It can be checked that all algebraic operations for real numbers2 ap-
ply equally well to complex numers.
Both real numbers and complex
numbers are examples of a
mathematical ﬁeld. Fields are closed with
respect to multiplication and addition, and all the rules of algebra we use
in manipulating polynomials with real coeﬃcients (and roots) carry over
unchanged to polynomials with complex coeﬃcients and roots. In fact,
the rules of algebra become simpler for complex numbers because, as dis-
cussed in the next section, we can always factor polynomials completely
over the ﬁeld of complex numbers while we cannot do this over the reals
(as we saw in the example p(x) = x2 + 4).
2.4
Fundamental Theorem of Algebra
Every nth-order polynomial possesses exactly n complex roots.
This is a very powerful algebraic tool. It says that given any polynomial
p(x)
=
anxn + an−1xn−1 + an−2xn−2 + · · · + a2x2 + a1x + a0
∆=
n

i=0
aixi
we can always rewrite it as
p(x)
=
an(x −zn)(x −zn−1)(x −zn−2) · · · (x −z2)(x −z1)
∆=
an
n

i=1
(x −zi)
where the points zi are the polynomial roots, and they may be real or
complex.
2.5
Complex Basics
This section introduces various notation and terms associated with com-
plex numbers. As discussed above, complex numbers are devised by intro-
ducing the square-root of −1 as a primitive new algebraic object among
2multiplication, addition, division, distributivity of multiplication over addition,
commutativity of multiplication and addition.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 12
2.5. COMPLEX BASICS
real numbers and manipulating it symbolically as if it were a real number
itself:
j ∆=
√
−1
Mathemeticians and physicists often use i instead of j as √−1. The use
of j is common in engineering where i is more often used for electrical
current.
As mentioned above, for any negative number c < 0, we have √c =

(−1)(−c) = j

|c|, where |c| denotes the absolute value of c. Thus,
every square root of a negative number can be expressed as j times the
square root of a positive number.
By deﬁnition, we have
j0
=
1
j1
=
j
j2
=
−1
j3
=
−j
j4
=
1
· · ·
and so on. Thus, the sequence x(n) ∆= jn, n = 0, 1, 2, . . . is a periodic
sequence with period 4, since jn+4 = jnj4 = jn. (We’ll learn later that
the sequence jn is a sampled complex sinusoid having frequency equal to
one fourth the sampling rate.)
Every complex number z can be written as
z = x + jy
where x and y are real numbers.
We call x the real part and y the
imaginary part. We may also use the notation
re {z}
=
x
(“the real part of z = x + jy is x”)
im {z}
=
y
(“the imaginary part of z = x + jy is y”)
Note that the real numbers are the subset of the complex numbers having
a zero imaginary part (y = 0).
The rule for complex multiplication follows directly from the deﬁnition
of the imaginary unit j:
z1z2
∆=
(x1 + jy1)(x2 + jy2)
=
x1x2 + jx1y2 + jy1x2 + j2y1y2
=
(x1x2 −y1y2) + j(x1y2 + y1x2)
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 2. COMPLEX NUMBERS
Page 13
In some mathematics texts, complex numbers z are deﬁned as ordered
pairs of real numbers (x, y), and algebraic operations such as multipli-
cation are deﬁned more formally as operations on ordered pairs, e.g.,
(x1, y1) · (x2, y2) ∆= (x1x2 −y1y2, x1y2 + y1x2).
However, such formal-
ity tends to obscure the underlying simplicity of complex numbers as a
straightforward extension of real numbers to include j ∆= √−1.
It is important to realize that complex numbers can be treated alge-
braically just like real numbers. That is, they can be added, subtracted,
multiplied, divided, etc., using exactly the same rules of algebra (since
both real and complex numbers are mathematical ﬁelds).
It is often
preferable to think of complex numbers as being the true and proper set-
ting for algebraic operations, with real numbers being the limited subset
for which y = 0.
To explore further the magical world of complex variables, see any
textbook such as [1, 2].
2.5.1
The Complex Plane
Real Part
Imaginary Part
θ
x
y
z = x + j y
r
r  sin θ
r  cos θ
Figure 2.2: Plotting a complex number as a point in the complex
plane.
We can plot any complex number z = x + jy in a plane as an ordered
pair (x, y), as shown in Fig. 2.2. A complex plane is any 2D graph in
which the horizontal axis is the real part and the vertical axis is the
imaginary part of a complex number or function. As an example, the
number j has coordinates (0, 1) in the complex plane while the number 1
has coordinates (1, 0).
Plotting z = x + jy as the point (x, y) in the complex plane can be
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 14
2.5. COMPLEX BASICS
viewed as a plot in Cartesian or rectilinear coordinates.
We can also
express complex numbers in terms of polar coordinates as an ordered pair
(r, θ), where r is the distance from the origin (0, 0) to the number being
plotted, and θ is the angle of the number relative to the positive real
coordinate axis (the line deﬁned by y = 0 and x > 0). (See Fig. 2.2.)
Using elementary geometry, it is quick to show that conversion from
rectangular to polar coordinates is accomplished by the formulas
r
=

x2 + y2
θ
=
tan−1(y/x).
The ﬁrst equation follows immediately from the Pythagorean theorem ,
while the second follows immediately from the deﬁnition of the tangent
function. Similarly, conversion from polar to rectangular coordinates is
simply
x
=
r cos(θ)
y
=
r sin(θ).
These follow immediately from the deﬁnitions of cosine and sine, respec-
tively,
2.5.2
More Notation and Terminology
It’s already been mentioned that the rectilinear coordinates of a complex
number z = x + jy in the complex plane are called the real part and
imaginary part, respectively.
We also have special notation and various names for the radius and
angle of a complex number z expressed in polar coordinates (r, θ):
r
∆=
|z| =

x2 + y2
=
modulus, magnitude, absolute value, norm, or radius of z
θ
∆=
̸ z = tan−1(y/x)
=
angle, argument, or phase of z
The complex conjugate of z is denoted z and is deﬁned by
z ∆= x −jy
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 2. COMPLEX NUMBERS
Page 15
where, of course, z ∆= x + jy. Sometimes you’ll see the notation z∗in
place of z, but we won’t use that here.
In general, you can always obtain the complex conjugate of any ex-
pression by simply replacing j with −j. In the complex plane, this is
a vertical ﬂip about the real axis; in other words, complex conjugation
replaces each point in the complex plane by its mirror image on the other
side of the x axis.
2.5.3
Elementary Relationships
From the above deﬁnitions, one can quickly verify
z + z
=
2 re {z}
z −z
=
2j im {z}
zz
=
|z|2
Let’s verify the third relationship which states that a complex number
multiplied by its conjugate is equal to its magnitude squared:
zz ∆= (x + jy)(x −jy) = x2 −(jy)2 = x2 + y2 ∆= |z|2,
2.5.4
Euler’s Formula
Since z = x+jy is the algebraic expression of z in terms of its rectangular
coordinates, the corresponding expression in terms of its polar coordinates
is
z = r cos(θ) + j r sin(θ).
There is another, more powerful representation of z in terms of its
polar coordinates. In order to deﬁne it, we must introduce Euler’s For-
mula:
ejθ = cos(θ) + j sin(θ)
(2.1)
A proof of Euler’s identity is given in the next chapter. Just note for the
moment that for θ = 0, we have ej0 = cos(0) + j sin(0) = 1 + j0 = 1, as
expected. Before, the only algebraic representation of a complex number
we had was z = x + jy, which fundamentally uses Cartesian (rectilinear)
coordinates in the complex plane. Euler’s identity gives us an alternative
algebraic representation in terms of polar coordinates in the complex
plane:
z = rejθ
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 16
2.5. COMPLEX BASICS
This representation often simpliﬁes manipulations of complex numbers,
especially when they are multiplied together. Simple rules of exponents
can often be used in place of more diﬃcult trigonometric identities. In
the simple case of two complex numbers being multiplied,
z1z2 =

r1ejθ1
 
r2ejθ2

= (r1r2)

ejθ1ejθ2

= r1r2ej(θ1+θ2)
A corollary of Euler’s identity is obtained by setting θ = π to get
ejπ + 1 = 0
This has been called the “most beautiful formula in mathematics” due to
the extremely simple form in which the fundamental constants e, j, π, 1,
and 0, together with the elementary operations of addition, multiplica-
tion, exponentiation, and equality, all appear exactly once.
For another example of manipulating the polar form of a complex
number, let’s again verify zz = |z|2, as we did above, but this time using
polar form:
zz = rejθre−jθ = r2e0 = r2 = |z|2.
We can now easily add a fourth line to that set of examples:
z/z = rejθ
re−jθ = ej2θ = ej2̸
z
Thus, |z/z| = 1 for every z ̸= 0.
Euler’s identity can be used to derive formulas for sine and cosine in
terms of ejθ:
ejθ + ejθ
=
ejθ + e−jθ
=
[cos(θ) + j sin(θ)] + [cos(θ) −j sin(θ)]
=
2 cos(θ),
Similarly, ejθ −ejθ = 2j sin(θ), and we have
cos(θ)
= ejθ + e−jθ
2
sin(θ)
= ejθ −e−jθ
2j
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 2. COMPLEX NUMBERS
Page 17
2.5.5
De Moivre’s Theorem
As a more complicated example of the value of the polar form, we’ll prove
De Moivre’s theorem:
[cos(θ) + j sin(θ)]n = cos(nθ) + j sin(nθ)
Working this out using sum-of-angle identities from trigonometry is labo-
rious. However, using Euler’s identity, De Moivre’s theorem simply “falls
out”:
[cos(θ) + j sin(θ)]n =

ejθ	n
= ejθn = cos(nθ) + j sin(nθ)
Moreover, by the power of the method used to show the result, n can be
any real number, not just an integer.
2.6
Numerical Tools in Matlab
In Matlab, root-ﬁnding is always numerical:3
>> % polynomial = array of coefficients in Matlab
>> p = [1 0 0 0 5 7]; %
p(x) = x^5 + 5*x + 7
>> format long;
%
print double-precision
>> roots(p)
%
print out the roots of p(x)
ans =
1.30051917307206 + 1.10944723819596i
1.30051917307206 - 1.10944723819596i
-0.75504792501755 + 1.27501061923774i
-0.75504792501755 - 1.27501061923774i
-1.09094249610903
Matlab has the following primitives for complex numbers:
>> help j
J
Imaginary unit.
The variables i and j both initially have the value sqrt(-1)
for use in forming complex quantities.
For example, the
3unless you have the optional Maple package for symbolic mathematical manipula-
tion
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 18
2.6. NUMERICAL TOOLS IN MATLAB
expressions 3+2i, 3+2*i, 3+2i, 3+2*j and 3+2*sqrt(-1).
all have the same value.
However, both i and j may be
assigned other values, often in FOR loops and as subscripts.
See also I.
Built-in function.
Copyright (c) 1984-92 by The MathWorks, Inc.
>> sqrt(-1)
ans =
0 + 1.0000i
>> help real
REAL
Complex real part.
REAL(X) is the real part of X.
See also IMAG, CONJ, ANGLE, ABS.
>> help imag
IMAG
Complex imaginary part.
IMAG(X) is the imaginary part of X.
See I or J to enter complex numbers.
See also REAL, CONJ, ANGLE, ABS.
>> help conj
CONJ
Complex conjugate.
CONJ(X) is the complex conjugate of X.
>> help abs
ABS
Absolute value and string to numeric conversion.
ABS(X) is the absolute value of the elements of X. When
X is complex, ABS(X) is the complex modulus (magnitude) of
the elements of X.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 2. COMPLEX NUMBERS
Page 19
See also ANGLE, UNWRAP.
ABS(S), where S is a MATLAB string variable, returns the
numeric values of the ASCII characters in the string.
It does not change the internal representation, only the
way it prints.
See also SETSTR.
>> help angle
ANGLE
Phase angle.
ANGLE(H) returns the phase angles, in radians, of a matrix with
complex elements.
See also ABS, UNWRAP.
Note how helpful the “See also” information is in Matlab.
Let’s run through a few elementary manipulations of complex numbers
in Matlab:
>> x = 1; % Every symbol must have a value in Matlab
>> y = 2;
>> z = x + j * y
z =
1.0000 + 2.0000i
>> 1/z
ans =
0.2000 - 0.4000i
>> z^2
ans =
-3.0000 + 4.0000i
>> conj(z)
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 20
2.6. NUMERICAL TOOLS IN MATLAB
ans =
1.0000 - 2.0000i
>> z*conj(z)
ans =
5
>> abs(z)^2
ans =
5.0000
>> norm(z)^2
ans =
5.0000
>> angle(z)
ans =
1.1071
Now let’s do polar form:
>> r = abs(z)
r =
2.2361
>> theta = angle(z)
theta =
1.1071
Curiously, e is not deﬁned by default in Matlab (though it is in Oc-
tave). It can easily be computed in Matlab as e=exp(1). Below are some
examples involving imaginary exponentials:
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 2. COMPLEX NUMBERS
Page 21
>> r * exp(j * theta)
ans =
1.0000 + 2.0000i
>> z
z =
1.0000 + 2.0000i
>> z/abs(z)
ans =
0.4472 + 0.8944i
>> exp(j*theta)
ans =
0.4472 + 0.8944i
>> z/conj(z)
ans =
-0.6000 + 0.8000i
>> exp(2*j*theta)
ans =
-0.6000 + 0.8000i
>> imag(log(z/abs(z)))
ans =
1.1071
>> theta
theta =
1.1071
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 22
2.6. NUMERICAL TOOLS IN MATLAB
>>
Some manipulations involving two complex numbers:
>> x1 = 1;
>> x2 = 2;
>> y1 = 3;
>> y2 = 4;
>> z1 = x1 + j * y1;
>> z2 = x2 + j * y2;
>> z1
z1 =
1.0000 + 3.0000i
>> z2
z2 =
2.0000 + 4.0000i
>> z1*z2
ans =
-10.0000 +10.0000i
>> z1/z2
ans =
0.7000 + 0.1000i
Another thing to note about Matlab is that the transpose operator
’ (for vectors and matrices) conjugates as well as transposes. Use .’ to
transpose without conjugation:
>>x = [1:4]*j
x =
0 + 1.0000i
0 + 2.0000i
0 + 3.0000i
0 + 4.0000i
>> x’
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 2. COMPLEX NUMBERS
Page 23
ans =
0 - 1.0000i
0 - 2.0000i
0 - 3.0000i
0 - 4.0000i
>> x.’
ans =
0 + 1.0000i
0 + 2.0000i
0 + 3.0000i
0 + 4.0000i
>>
2.7
Numerical Tools in Mathematica
In Mathematica, we can ﬁnd the roots of simple polynomials in closed
form, while larger polynomials can be factored numerically in either Mat-
lab or Mathematica. Look to Mathematica to provide the most sophisti-
cated symbolic mathematical manipulation, and look for Matlab to pro-
vide the best numerical algorithms, as a general rule.
One way to implicitly ﬁnd the roots of a polynomial is to factor it in
Mathematica:
In[1]:
p[x_] := x^2 + 5 x + 6
In[2]:
Factor[p[x]]
Out[2]:
(2 + x)*(3 + x)
Factor[] works only with exact Integers or Rational coeﬃcients, not with
Real numbers.
Alternatively, we can explicitly solve for the roots of low-order poly-
nomials in Mathematica:
In[1]:
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 24
2.7. NUMERICAL TOOLS IN MATHEMATICA
p[x_] := a x^2 + b x + c
In[2]:
Solve[p[x]==0,x]
Out[2]:
{{x -> (-(b/a) + (b^2/a^2 - (4*c)/a)^(1/2))/2},
{x -> (-(b/a) - (b^2/a^2 - (4*c)/a)^(1/2))/2}}
Closed-form solutions work for polynomials of order one through four.
Higher orders, in general, must be dealt with numerically, as shown below:
In[1]:
p[x_] := x^5 + 5 x + 7
In[2]:
Solve[p[x]==0,x]
Out[2]:
{ToRules[Roots[5*x + x^5 == -7, x]]}
In[3]:
N[Solve[p[x]==0,x]]
Out[3]:
{{x -> -1.090942496109028},
{x -> -0.7550479250175501 - 1.275010619237742*I},
{x -> -0.7550479250175501 + 1.275010619237742*I},
{x -> 1.300519173072064 - 1.109447238195959*I},
{x -> 1.300519173072064 + 1.109447238195959*I}}
Mathematica has the following primitives for dealing with complex
numbers (The “?” operator returns a short description of the symbol to
its right):
In[1]:
?I
Out[1]:
I represents the imaginary unit Sqrt[-1].
In[2]:
?Re
Out[2]:
Re[z] gives the real part of the complex number z.
In[3]:
?Im
Out[3]:
Im[z] gives the imaginary part of the complex number z.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 2. COMPLEX NUMBERS
Page 25
In[4]:
?Conj*
Out[4]:
Conjugate[z] gives the complex conjugate of the complex number z.
In[5]:
?Abs
Out[5]:
Abs[z] gives the absolute value of the real or complex number z.
In[6]:
?Arg
Out[6]:
Arg[z] gives the argument of z.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 26
2.7. NUMERICAL TOOLS IN MATHEMATICA
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Chapter 3
Proof of Euler’s Identity
This chapter outlines the proof of Euler’s Identity, which is an important
tool for working with complex numbers. It is one of the critical elements
of the DFT deﬁnition that we need to understand.
3.1
Euler’s Theorem
Euler’s Theorem (or “identity” or “formula”) is
ejθ = cos(θ) + j sin(θ)
(Euler’s Identity)
To “prove” this, we must ﬁrst deﬁne what we mean by “ejθ.” (The right-
hand side is assumed to be understood.)
Since e is just a particular
number, we only really have to explain what we mean by imaginary ex-
ponents. (We’ll also see where e comes from in the process.) Imaginary
exponents will be obtained as a generalization of real exponents. There-
fore, our ﬁrst task is to deﬁne exactly what we mean by ax, where x is
any real number, and a > 0 is any positive real number.
3.1.1
Positive Integer Exponents
The “original” deﬁnition of exponents which “actually makes sense” ap-
plies only to positive integer exponents:
an ∆= a · a · a · · · · · a



n times
where a > 0 is real.
27

Page 28
3.1. EULER’S THEOREM
Generalizing this deﬁnition involves ﬁrst noting its abstract mathe-
matical properties, and then making sure these properties are preserved
in the generalization.
3.1.2
Properties of Exponents
From the basic deﬁnition of positive integer exponents, we have
(1) an1an2 = an1+n2
(2) (an1)n2 = an1n2
Note that property (1) implies property (2). We list them both explicitly
for convenience below.
3.1.3
The Exponent Zero
How should we deﬁne a0 in a manner that is consistent with the properties
of integer exponents? Multiplying it by a gives
a0 · a = a0a1 = a0+1 = a1 = a
by property (1) of exponents. Solving a0 · a = a for a0 then gives
a0 = 1 .
3.1.4
Negative Exponents
What should a−1 be? Multiplying it by a gives
a−1 · a = a−1a1 = a−1+1 = a0 = 1
Solving a−1 · a = 1 for a−1 then gives
a−1 = 1
a
Similarly, we obtain
a−M =
1
aM
for all integer values of M, i.e., ∀M ∈Z.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 3. PROOF OF EULER’S IDENTITY
Page 29
3.1.5
Rational Exponents
A rational number is a real number that can be expressed as a ratio of
two integers:
x = L
M ,
L ∈Z, M ∈Z
Applying property (2) of exponents, we have
ax = aL/M =

a
1
M
L
Thus, the only thing new is a1/M. Since

a
1
M
M
= a
M
M = a
we see that a1/M is the Mth root of a. This is sometimes written
a
1
M ∆=
M√a
The Mth root of a real (or complex) number is not unique. As we all
know, square roots give two values (e.g.,
√
4 = ±2). In the general case
of Mth roots, there are M distinct values, in general.
How do we come up with M diﬀerent numbers which when raised to
the Mth power will yield a? The answer is to consider complex numbers in
polar form. By Euler’s Identity, the real number a > 0 can be expressed,
for any integer k, as a·ej2πk = a·cos(2πk)+j·a·sin(2πk) = a+j·a·0 = a.
Using this form for a, the number a1/M can be written as
a
1
M = a
1
M ej2πk/M,
k = 0, 1, 2, 3, . . . , M −1
We can now see that we get a diﬀerent complex number for each k =
0, 1, 2, 3, . . . , M −1. When k = M, we get the same thing as when k = 0.
When k = M + 1, we get the same thing as when k = 1, and so on, so
there are only M cases using this construct.
Roots of Unity
When a = 1, we can write
1k/M = ej2πk/M,
k = 0, 1, 2, 3, . . . , M −1
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 30
3.1. EULER’S THEOREM
The special case k = 1 is called the primitive Mth root of unity, since
integer powers of it give all of the others:
ej2πk/M =

ej2π/Mk
The Mth roots of unity are so important that they are often given a
special notation in the signal processing literature:
W k
M
∆= ej2πk/M,
k = 0, 1, 2, . . . , M −1,
where WM denotes the primitive Mth root of unity. We may also call
WM the generator of the mathematical group consisting of the Mth roots
of unity and their products.
We will learn later that the Nth roots of unity are used to generate
all the sinusoids used by the DFT and its inverse. The kth sinusoid is
given by
W kn
N = ej2πkn/N ∆= ejωktn = cos(ωktn)+j sin(ωktn),
n = 0, 1, 2, . . . , N−1
where ωk
∆= 2πk/NT, tn
∆= nT, and T is the sampling interval in seconds.
3.1.6
Real Exponents
The closest we can actually get to most real numbers is to compute a
rational number that is as close as we need. It can be shown that ratio-
nal numbers are dense in the real numbers; that is, between every two
real numbers there is a rational number, and between every two rational
numbers is a real number. An irrational number can be deﬁned as any
real number having a non-repeating decimal expansion. For example,
√
2
is an irrational real number whose decimal expansion starts out as
√
2 = 1.414213562373095048801688724209698078569671875376948073176679737 . . .
(computed via N[Sqrt[2],80] in Mathematica). Every truncated, rounded,
or repeating expansion is a rational number. That is, it can be rewritten
as an integer divided by another integer. For example,
1.414 = 1414
1000
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 3. PROOF OF EULER’S IDENTITY
Page 31
and, using overbar to denote the repeating part of a decimal expansion,
x
=
0.123
⇒
1000x
=
123.123 = 123 + x
⇒
999x
=
123
⇒
x
=
123
999
Other examples of irrational numbers include
π
=
3.1415926535897932384626433832795028841971693993751058209749 . . .
e
=
2.7182818284590452353602874713526624977572470936999595749669 . . .
Let ˆxn denote the n-digit decimal expansion of an arbitrary real num-
ber x. Then ˆxn is a rational number (some integer over 10n). We can
say
lim
n→∞ˆxn = x
Since aˆxn is deﬁned for all n, it is straightforward to deﬁne
ax ∆= lim
n→∞aˆxn
3.1.7
A First Look at Taylor Series
Any “smooth” function f(x) can be expanded in the form of a Taylor
series:
f(x) = f(x0)+ f ′(x0)
1
(x−x0)+ f ′′(x0)
1 · 2 (x−x0)2 + f ′′′(x0)
1 · 2 · 3 (x−x0)3 +· · · .
This can be written more compactly as
f(x) =
∞

n=0
f (n)(x0)
n!
(x −x0)n.
An informal derivation of this formula for x0 = 0 is given in §3.2 and §3.3.
Clearly, since many derivatives are involved, a Taylor series expansion is
only possible when the function is so smooth that it can be diﬀerentiated
again and again. Fortunately for us, all audio signals can be deﬁned so
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 32
3.1. EULER’S THEOREM
as to be in that category. This is because hearing is bandlimited to 20
kHz, and any sum of sinusoids up to some maximum frequency, i.e., any
audible signal, is inﬁnitely diﬀerentiable. (Recall that sin′(x) = cos(x)
and cos′(x) = −sin(x), etc.). See §3.6 for more on this topic.
3.1.8
Imaginary Exponents
We may deﬁne imaginary exponents the same way that all suﬃciently
smooth real-valued functions of a real variable are generalized to the
complex case—using Taylor series. A Taylor series expansion is just a
polynomial (possibly of inﬁnitely high order), and polynomials involve
only addition, multiplication, and division. Since these elementary oper-
ations are also deﬁned for complex numbers, any smooth function of a
real variable f(x) may be generalized to a function of a complex variable
f(z) by simply substituting the complex variable z = x + jy for the real
variable x in the Taylor series expansion.
Let f(x) ∆= ax, where a is any positive real number. The Taylor series
expansion expansion about x0 = 0 (“Maclaurin series”), generalized to
the complex case is then
az ∆= f(0) + f ′(0)(z) + f ′′(0)
2
z2 + f ′′′(0)z3
3!
+ · · · · · ·
which is well deﬁned (although we should make sure the series converges
for every ﬁnite z). We have f(0) ∆= a0 = 1, so the ﬁrst term is no problem.
But what is f ′(0)? In other words, what is the derivative of ax at x = 0?
Once we ﬁnd the successive derivatives of f(x) ∆= ax at x = 0, we will be
done with the deﬁnition of az for any complex z.
3.1.9
Derivatives of f(x) = ax
Let’s apply the deﬁnition of diﬀerentiation and see what happens:
f ′(x0)
∆=
lim
δ→0
f(x0 + δ) −f(x0)
δ
∆=
lim
δ→0
ax0+δ −ax0
δ
= lim
δ→0 ax0 aδ −1
δ
= ax0 lim
δ→0
aδ −1
δ
Since the limit of (aδ −1)/δ as δ →0 is less than 1 for a = 2 and greater
than 1 for a = 3 (as one can show via direct calculations), and since
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 3. PROOF OF EULER’S IDENTITY
Page 33
(aδ −1)/δ is a continuous function of a, it follows that there exists a
positive real number we’ll call e such that for a = e we get
lim
δ→0
eδ −1
δ
∆= 1.
For a = e, we thus have (ax)′ = (ex)′ = ex.
So far we have proved that the derivative of ex is ex. What about ax
for other values of a? The trick is to write it as
ax = eln(ax) = ex ln(a)
and use the chain rule, where ln(a) ∆= loge(a) denotes the log-base-e of
a. Formally, the chain rule tells us how do diﬀerentiate a function of a
function as follows:
d
dxf(g(x))|x=x0 = f ′(g(x0))g′(x0)
In this case, g(x) = x ln(a) so that g′(x) = ln(a), and f(y) = ey which is
its own derivative. The end result is then (ax)′ =

ex ln a′ = ex ln(a) ln(a) =
ax ln(a), i.e.,
d
dxax = ax ln(a)
3.1.10
Back to e
Above, we deﬁned e as the particular real number satisfying
lim
δ→0
eδ −1
δ
∆= 1.
which gave us (ax)′ = ax when a = e. From this expression, we have, as
δ →0,
eδ −1
→
δ
⇒
eδ
→
1 + δ
⇒
e
→
(1 + δ)1/δ,
or,
e ∆= lim
δ→0(1 + δ)1/δ
This is one way to deﬁne e. Another way to arrive at the same deﬁnition
is to ask what logarithmic base e gives that the derivative of loge(x) is
1/x. We denote loge(x) by ln(x).
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 34
3.1. EULER’S THEOREM
3.1.11
Sidebar on Mathematica
Mathematica is a handy tool for cranking out any number of digits in
transcendental numbers such as e:
In[]:
N[E,50]
Out[]:
2.7182818284590452353602874713526624977572470937
Alternatively, we can compute (1 + δ)1/δ for small δ:
In[]:
(1+delta)^(1/delta) /. delta->0.001
Out[]:
2.716923932235594
In[]:
(1+delta)^(1/delta) /. delta->0.0001
Out[]:
2.718145926824926
In[]:
(1+delta)^(1/delta) /. delta->0.00001
Out[]:
2.718268237192297
What happens if we just go for it and set delta to zero?
In[]:
(1+delta)^(1/delta) /. delta->0
1
Power::infy: Infinite expression - encountered.
0
Infinity::indt:
ComplexInfinity
Indeterminate expression 1
encountered.
Indeterminate
3.1.12
Back to ejθ
We’ve now deﬁned az for any positive real number a and any complex
number z. Setting a = e and z = jθ gives us the special case we need for
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 3. PROOF OF EULER’S IDENTITY
Page 35
Euler’s identity. Since ez is its own derivative, the Taylor series expansion
for for f(x) = ex is one of the simplest imaginable inﬁnite series:
ex =
∞

n=0
xn
n! = 1 + x + x2
2 + x3
3! + · · ·
The simplicity comes about because f(n)(0) = 1 for all n and because we
chose to expand about the point x = 0. We of course deﬁne
ejθ ∆=
∞

n=0
(jθ)n
n!
= 1 + jθ −θ2/2 −jθ3/3! + · · ·
Note that all even order terms are real while all odd order terms are
imaginary. Separating out the real and imaginary parts gives
re

ejθ
=
1 −θ2/2 + θ4/4! −· · ·
im

ejθ
=
θ −θ3/3! + θ5/5! −· · ·
Comparing the Maclaurin expansion for ejθ with that of cos(θ) and
sin(θ) proves Euler’s identity. Recall that
d
dθ cos(θ)
=
−sin(θ)
d
dθ sin(θ)
=
cos(θ)
so that
dn
dθn cos(θ)

θ=0
=

(−1)n/2,
n even
0,
n odd
dn
dθn sin(θ)

θ=0
=

(−1)(n−1)/2,
n odd
0,
n even
Plugging into the general Maclaurin series gives
cos(θ)
=
∞

n=0
f (n)(0)
n!
θn
=
∞

n≥0
n even
(−1)n/2
n!
θn
sin(θ)
=
∞

n≥0
n odd
(−1)(n−1)/2
n!
θn
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 36
3.2. INFORMAL DERIVATION OF TAYLOR SERIES
Separating the Maclaurin expansion for ejθ into its even and odd terms
(real and imaginary parts) gives
ejθ ∆=
∞

n=0
(jθ)n
n!
=
∞

n≥0
n even
(−1)n/2
n!
θn + j
∞

n≥0
n odd
(−1)(n−1)/2
n!
θn = cos(θ) + j sin(θ)
thus proving Euler’s identity.
3.2
Informal Derivation of Taylor Series
We have a function f(x) and we want to approximate it using an nth-
order polynomial:
f(x) = f0 + f1x + f2x2 + · · · + fnxn + Rn+1(x)
where Rn+1(x), which is obviously the approximation error, is called the
“remainder term.” We may assume x and f(x) are real, but the following
derivation generalizes unchanged to the complex case.
Our problem is to ﬁnd ﬁxed constants {fi}n
i=0 so as to obtain the best
approximation possible. Let’s proceed optimistically as though the ap-
proximation will be perfect, and assume Rn+1(x) = 0 for all x (Rn+1(x) ≡
0), given the right values of fi. Then at x = 0 we must have
f(0) = f0
That’s one constant down and n −1 to go! Now let’s look at the ﬁrst
derivative of f(x) with respect to x, again assuming that Rn+1(x) ≡0:
f ′(x) = 0 + f1 + 2f2x + 3f2x2 + · · · + nfnxn−1
Evaluating this at x = 0 gives
f ′(0) = f1
In the same way, we ﬁnd
f ′′(0)
=
2 · f2
f ′′′(0)
=
3 · 2 · f3
· · ·
f (n)(0)
=
n! · fn
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 3. PROOF OF EULER’S IDENTITY
Page 37
where f (n)(0) denotes the nth derivative of f(x) with respect to x, eval-
uated at x = 0. Solving the above relations for the desired constants
yields
f0
=
f(0)
f1
=
f ′(0)
1
f2
=
f ′′(0)
2 · 1
f3
=
f ′′′(0)
3 · 2 · 1
· · ·
fn
=
f (n)(0)
n!
Thus, deﬁning 0! ∆= 1 (as it always is), we have derived the following
polynomial approximation:
f(x) ≈
n

k=0
f (k)(0)
k!
xk
This is the nth-order Taylor series expansion of f(x) about the point
x = 0. Its derivation was quite simple. The hard part is showing that
the approximation error (remainder term Rn+1(x)) is small over a wide
interval of x values. Another “math job” is to determine the conditions
under which the approximation error approaches zero for all x as the
order n goes to inﬁnity. The main point to note here is that the form of
the Taylor series expansion itself is simple to derive.
3.3
Taylor Series with Remainder
We repeat the derivation of the preceding section, but this time we treat
the error term more carefully.
Again we want to approximate f(x) with an nth-order polynomial:
f(x) = f0 + f1x + f2x2 + · · · + fnxn + Rn+1(x)
Rn+1(x) is the “remainder term” which we will no longer assume is zero.
Our problem is to ﬁnd {fi}n
i=0 so as to minimize Rn+1(x) over some
interval I containing x. There are many “optimality criteria” we could
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 38
3.3. TAYLOR SERIES WITH REMAINDER
choose. The one that falls out naturally here is called “Pad´e” approxima-
tion. Pad´e approximation sets the error value and its ﬁrst n derivatives
to zero at a single chosen point, which we take to be x = 0. Since all
n + 1 “degrees of freedom” in the polynomial coeﬃcients fi are used to
set derivatives to zero at one point, the approximation is termed “maxi-
mally ﬂat” at that point. Pad´e approximation comes up often in signal
processing. For example, it is the sense in which Butterworth lowpass
ﬁlters are optimal. (Their frequency reponses are maximally ﬂat at dc.)
Also, Lagrange interpolation ﬁlters can be shown to maximally ﬂat at dc
in the frequency domain.
Setting x = 0 in the above polynomial approximation produces
f(0) = f0 + Rn+1(0) = f0
where we have used the fact that the error is to be exactly zero at x = 0.
Diﬀerentiating the polynomial approximation and setting x = 0 gives
f ′(0) = f1 + R′
n+1(0) = f1
where we have used the fact that we also want the slope of the error to
be exactly zero at x = 0.
In the same way, we ﬁnd
f (k)(0) = k! · fk + R(k)
n+1(0) = k! · fk
for k = 2, 3, 4, . . . , n, and the ﬁrst n derivatives of the remainder term
are all zero. Solving these relations for the desired constants yields the
nth-order Taylor series expansion of f(x) about the point x = 0
f(x) =
n

k=0
f (k)(0)
k!
xk + Rn+1(x)
as before, but now we better understand the remainder term.
From this derivation, it is clear that the approximation error (remain-
der term) is smallest in the vicinity of x = 0. All degrees of freedom in the
polynomial coeﬃcients were devoted to minimizing the approximation er-
ror and its derivatives at x = 0. As you might expect, the approximation
error generally worsens as x gets farther away from 0.
To obtain a more uniform approximation over some interval I in x,
other kinds of error criteria may be employed. This is classically called
“economization of series,” but nowadays we may simply call it polynomial
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 3. PROOF OF EULER’S IDENTITY
Page 39
approximation under diﬀerent error criteria.
In Matlab, the function
polyfit(x,y,n) will ﬁnd the coeﬃcients of a polynomial p(x) of degree
n that ﬁts the data y over the points x in a least-squares sense. That is,
it minimizes
∥Rn+1 ∥2 ∆=
nx

i=1
|y(i) −p(x(i))|2
where nx
∆= length(x).
3.4
Formal Statement of Taylor’s Theorem
Let f(x) be continuous on a real interval I containing x0 (and x), and
let f(n)(x) exist at x and f(n+1)(ξ) be continuous for all ξ ∈I. Then we
have the following Taylor series expansion:
f(x) = f(x0)
+
1
1f ′(x0)(x −x0)
+
1
1 · 2f ′′(x0)(x −x0)2
+
1
1 · 2 · 3f ′′′(x0)(x −x0)3
+
· · ·
+
1
n!f (n+1)(x0)(x −x0)n
+
Rn+1(x)
where Rn+1(x) is called the remainder term. There exists ξ between x
and x0 such that
Rn+1(x) = f (n+1)(ξ)
(n + 1)! (x −x0)n+1
In particular, if |f (n+1)| ≤M in I, then
Rn+1(x) ≤M|x −x0|n+1
(n + 1)!
which is normally small when x is close to x0.
When x0 = 0, the Taylor series reduces to what is called a Maclaurin
series.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 40
3.5. WEIERSTRASS APPROXIMATION THEOREM
3.5
Weierstrass Approximation Theorem
Let f(x) be continuous on a real interval I. Then for any ϵ > 0, there
exists an nth-order polynomial Pn(f, x), where n depends on ϵ, such that
|Pn(f, x) −f(x)| < ϵ
for all x ∈I.
Thus, any continuous function can be approximated arbitrarily well
by means of a polynomial. Furthermore, an inﬁnite-order polynomial can
yield an error-free approximation. Of course, to compute the polynomial
coeﬃcients using a Taylor series expansion, the function must also be
diﬀerentiable of all orders throughout I.
3.6
Diﬀerentiability of Audio Signals
As mentioned earlier, every audio signal can be regarded as inﬁnitely
diﬀerentiable due to the ﬁnite bandwidth of human hearing. One of the
Fourier properties we will learn later in this reader is that a signal cannot
be both time limited and frequency limited. Therefore, by conceptually
“lowpass ﬁltering” every audio signal to reject all frequencies above 20
kHz, we implicitly make every audio signal last forever!
Another way
of saying this is that the “ideal lowpass ﬁlter ‘rings’ forever”. Such ﬁne
points do not concern us in practice, but they are important for fully
understanding the underlying theory.
Since, in reality, signals can be
said to have a true beginning and end, we must admit in practice that
all signals we work with have inﬁnite-bandwidth at turn-on and turn-oﬀ
transients.1
1One joke along these lines, due, I’m told, to Professor Bracewell, is that “since
the telephone is bandlimited to 3kHz, and since bandlimited signals cannot be time
limited, it follows that one cannot hang up the telephone”.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Chapter 4
Logarithms, Decibels, and
Number Systems
This chapter provides an introduction to logarithms (real and complex),
decibels, and number systems such as binary integer ﬁxed-point, frac-
tional ﬁxed-point, one’s complement, two’s complement, logarithmic ﬁxed-
point, µ-law, and ﬂoating-point number formats.
4.1
Logarithms
A logarithm y = logb(x) is fundamentally an exponent y applied to a
speciﬁc base b. That is, x = by. The term “logarithm” can be abbreviated
as “log”.
The base b is chosen to be a positive real number, and we
normally only take logs of positive real numbers x > 0 (although it is ok
to say that the log of 0 is −∞). The inverse of a logarithm is called an
antilogarithm or antilog.
For any positive number x, we have
x = blogb(x)
for any valid base b > 0. This is just an identity arising from the deﬁnition
of the logarithm, but it is sometimes useful in manipulating formulas.
When the base is not speciﬁed, it is normally assumed to be 10, i.e.,
log(x) ∆= log10(x). This is the common logarithm.
Base 2 and base e logarithms have their own special notation:
ln(x)
∆=
loge(x)
lg(x)
∆=
log2(x)
41

Page 42
4.1. LOGARITHMS
(The use of lg() for base 2 logarithms is common in computer science.
In mathematics, it may denote a base 10 logarithm.) By far the most
common bases are 10, e, and 2. Logs base e are called natural logarithms.
They are “natural” in the sense that
d
dx ln(x) = 1
x
while the derivatives of logarithms to other bases are not quite so simple:
d
dx logb(x) =
1
x ln(b)
(Prove this as an exercise.) The inverse of the natural logarithm y = ln(x)
is of course the exponential function x = ey, and ey is its own derivative.
In general, a logarithm y has an integer part and a fractional part.
The integer part is called the characteristic of the logarithm, and the
fractional part is called the mantissa. These terms were suggested by
Henry Briggs in 1624. “Mantissa” is a Latin word meaning “addition” or
“make weight”—something added to make up the weight [3].
The following Matlab code illustrates splitting a natural logarithm
into its characteristic and mantissa:
>> x = log(3)
x = 1.0986
>> characteristic = floor(x)
characteristic = 1
>> mantissa = x - characteristic
mantissa = 0.0986
>> % Now do a negative-log example
>> x = log(0.05)
x = -2.9957
>> characteristic = floor(x)
characteristic = -3
>> mantissa = x - characteristic
mantissa = 0.0043
Logarithms were used in the days before computers to perform multi-
plication of large numbers. Since log(xy) = log(x) + log(y), one can look
up the logs of x and y in tables of logarithms, add them together (which
is easier than multiplying), and look up the antilog of the result to obtain
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 4. LOGARITHMS, DECIBELS, AND NUMBER SYSTEMS
Page 43
the product xy. Log tables are still used in modern computing environ-
ments to replace expensive multiplies with less-expensive table lookups
and additions. This is a classic tradeoﬀbetween memory (for the log
tables) and computation. Nowadays, large numbers are multiplied using
FFT fast-convolution techniques.
4.1.1
Changing the Base
By deﬁnition, x = blogb(x). Taking the log base a of both sides gives
loga(x) = logb(x) loga(b)
which tells how to convert the base from b to a, that is, how to convert
the log base b of x to the log base a of x. (Just multiply by the log base
a of b.)
4.1.2
Logarithms of Negative and Imaginary Numbers
By Euler’s formula, ejπ = −1, so that
ln(−1) = jπ
from which it follows that for any x < 0, ln(x) = jπ + ln(|x|).
Similarly, ejπ/2 = j, so that
ln(j) = j π
2
and for any imaginary number z = jy, ln(z) = jπ/2 + ln(y), where y is
real.
Finally, from the polar representation z = rejθ for complex numbers,
ln(z) ∆= ln(rejθ) = ln(r) + jθ
where r > 0 and θ are real. Thus, the log of the magnitude of a complex
number behaves like the log of any positive real number, while the log of
its phase term ejθ extracts its phase (times j).
As an example of the use of logarithms in signal processing, note that
the negative imaginary part of the derivative of the log of a spectrum
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 44
4.2. DECIBELS
X(ω) is deﬁned as the group delay1 of the signal x(t):
Dx(ω) ∆= −im
 d
dω ln(X(ω))

Another usage is in Homomorphic Signal Processing [6, Chapter 10] in
which the multiplicative formants in vocal spectra are converted to ad-
ditive low-frequency variations in the spectrum (with the harmonics be-
ing the high-frequency variation in the spectrum). Thus, the lowpass-
ﬁltered log spectrum contains only the formants, and the complementarily
highpass-ﬁltered log spectrum contains only the ﬁne structure associated
with the pitch.
Exercise: Work out the deﬁnition of logarithms using a com-
plex base b.
4.2
Decibels
A decibel (abbreviated dB) is deﬁned as one tenth of a bel. The bel2 is
an amplitude unit deﬁned for sound as the log (base 10) of the intensity
relative to some reference intensity,3 i.e.,
Amplitude in bels = log10

Signal Intensity
Reference Intensity

The choice of reference intensity (or power) deﬁnes the particular choice
of dB scale. Signal intensity, power, and energy are always proportional
to the square of the signal amplitude. Thus, we can always translate these
1Group delay and phase delay are covered in the CCRMA publication [4] as well
as in standard signal processing references [5]. In the case of an AM or FM broadcast
signal which is passed through a ﬁlter, the carrier wave is delayed by the phase delay
of the ﬁlter, while the modulation signal is delayed by the group delay of the ﬁlter. In
the case of additive synthesis, group delay applies to the amplitude envelope of each
sinusoidal oscillator, while the phase delay applies to the sinusoidal oscillator waveform
itself.
2The “bel” is named after Alexander Graham Bell, the inventor of the telephone.
3Intensity is physically power per unit area. Bels may also be deﬁned in terms of
energy, or power which is energy per unit time. Since sound is always measured over
some area by a microphone diaphragm, its physical power is conventionally normal-
ized by area, giving intensity. Similarly, the force applied by sound to a microphone
diaphragm is normalized by area to give pressure (force per unit area).
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 4. LOGARITHMS, DECIBELS, AND NUMBER SYSTEMS
Page 45
energy-related measures into squared amplitude:
Amplitude in bels = log10

Amplitude2
Amplitude2
ref

= 2 log10
 |Amplitude|
|Amplituderef|

Since there are 10 decibels to a bel, we also have
AmplitudedB
=
20 log10
 |Amplitude|
|Amplituderef|

= 10 log10
 Intensity
Intensityref

=
10 log10
 Power
Powerref

= 10 log10
 Energy
Energyref

A just-noticeable diﬀerence (JND) in amplitude level is on the order
of a quarter dB. In the early days of telephony, one dB was considered a
reasonable “smallest step” in amplitude, but in reality, a series of half-dB
amplitude steps does not sound very smooth, while quarter-dB steps do
sound pretty smooth. A typical professional audio ﬁlter-design speciﬁca-
tion for “ripple in the passband” is 0.1 dB.
Exercise: Try synthesizing a sawtooth waveform which in-
creases by 1/2 dB a few times per second, and again using 1/4
dB increments. See if you agree that quarter-dB increments
are “smooth” enough for you.
4.2.1
Properties of DB Scales
In every kind of dB, a factor of 10 in amplitude gain corresponds to a 20
dB boost (increase by 20 dB):
20 log10
10 · A
Aref

= 20 log10(10)



20 dB
+20 log10
 A
Aref

and 20 log10(10) = 20, of course. A function f(x) which is proportional
to 1/x is said to “fall oﬀ” (or “roll oﬀ”) at the rate of 20 dB per decade.
That is, for every factor of 10 in x (every “decade”), the amplitude drops
20 dB.
Similarly, a factor of 2 in amplitude gain corresponds to a 6 dB boost:
20 log10
2 · A
Aref

= 20 log10(2)



6 dB
+20 log10
 A
Aref

DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 46
4.2. DECIBELS
and
20 log10(2) = 6.0205999 . . . ≈6 dB.
A function f(x) which is proportional to 1/x is said to fall oﬀ6 dB per
octave. That is, for every factor of 2 in x (every “octave”), the amplitude
drops close to 6 dB. Thus, 6 dB per octave is the same thing as 20 dB
per decade.
A doubling of power corresponds to a 3 dB boost:
10 log10

2 · A2
A2
ref

= 10 log10(2)



3 dB
+10 log10

A2
A2
ref

and
10 log10(2) = 3.010 . . . ≈3 dB.
Finally, note that the choice of reference merely determines a vertical
oﬀset in the dB scale:
20 log10
 A
Aref

= 20 log10(A) −20 log10(Aref)



constant oﬀset
4.2.2
Speciﬁc DB Scales
Since we so often rescale our signals to suit various needs (avoiding over-
ﬂow, reducing quantization noise, making a nicer plot, etc.), there seems
to be little point in worrying about what the dB reference is—we simply
choose it implicitly when we rescale to obtain signal values in the range
we want to see. Nevertheless, a few speciﬁc dB scales are worth knowing
about.
DBm Scale
One common dB scale in audio recording is the dBm scale in which the
reference power is taken to be a milliwatt (1 mW) dissipated by a 600 Ohm
resistor. (See Appendix 4.6 for a primer on resistors, voltage, current, and
power.)
DBV Scale
Another dB scale is the dBV scale which sets 0 dBV to 1 volt. Thus, a
100-volt signal is
20 log10
100V
1V

= 40 dBV
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 4. LOGARITHMS, DECIBELS, AND NUMBER SYSTEMS
Page 47
and a 1000-volt signal is
20 log10
1000V
1V

= 60 dBV
Note that the dBV scale is undeﬁned for current or power, unless the
voltage is assumed to be across a standard resistor value, such as 600
Ohms.
DB SPL
Sound Pressure Level (SPL) is deﬁned using a reference which is approx-
imately the intensity of 1000 Hz sinusoid that is just barely audible (0
“phons”). In pressure units:
0 dB SPL
∆=
0.0002 µbar (micro-barometric pressure4)
=
20 µPa (micro-Pascals)
=
2.9 × 10−9 PSI (pounds per square inch)
=
2 × 10−4 dynes
cm2
(CGS units)
=
2 × 10−5 nt
m2
(MKS units)
In intensity units:
I0 = 10−16 W
cm2
which corresponds to a root-mean-square (rms) pressure amplitude of
20.4 µPa, or about 20 µPa, as listed above. The wave impedance of air
plays the role of “resistor” in relating the pressure- and intensity-based
references exactly analogous to the dBm case discussed above.
Since sound is created by a time-varying pressure, we compute sound
levels in dB-SPL by using the average intensity (averaged over at least
one period of the lowest frequency contained in the sound).
Table 4.1 gives a list list of common sound levels and their dB equiva-
lents5 [7]:
In my experience, the “threshold of pain” is most often deﬁned
as 120 dB.
The relationship between sound amplitude and actual loudness is com-
plex [8]. Loudness is a perceptual dimension while sound amplitude is
5Adapted from S. S. Stevens, F. Warshofsky, and the Editors of Time-Life Books,
Sound and Hearing, Life Science Library, Time-Life Books, Alexandria, VA, 1965, p.
173.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 48
4.2. DECIBELS
Sound
dB-SPL
Jet engine at 3m
140
Threshold of pain
130
Rock concert
120
Accelerating motorcycle at 5m
110
Pneumatic hammer at 2m
100
Noisy factory
90
Vacuum cleaner
80
Busy traﬃc
70
Quiet restaurant
50
Residential area at night
40
Empty movie house
30
Rustling of leaves
20
Human breathing (at 3m)
10
Threshold of hearing (good ears)
0
Table 4.1: Ballpark ﬁgures for the dB-SPL level of common sounds.
physical. Since loudness sensitivity is closer to logarithmic than linear in
amplitude (especially at moderate to high loudnesses), we typically use
decibels to represent sound amplitude, especially in spectral displays.
The sone amplitude scale is deﬁned in terms of actual loudness per-
ception experiments [8]. At 1kHz and above, loudness perception is ap-
proximately logarithmic above 50 dB SPL or so. Below that, it tends
toward being more linear.
The phon amplitude scale is simply the dB scale at 1kHz [8, p. 111].
At other frequencies, the amplitude in phons is deﬁned by following the
equal-loudness curve over to 1 kHz and reading oﬀthe level there in
dB SPL. In other words, all pure tones have the same loudness at the
same phon level, and 1 kHz is used to set the reference in dB SPL. Just
remember that one phon is one dB-SPL at 1 kHz. Looking at the Fletcher-
Munson equal-loudness curves [8, p. 124], loudness in phons can be read
oﬀalong the vertical line at 1 kHz.
Classically, the intensity level of a sound wave is its dB SPL level, mea-
suring the peak time-domain pressure-wave amplitude relative to 10−16
watts per centimeter squared (i.e., there is no consideration of the fre-
quency domain here at all).
Another classical term still encountered is the sensation level of pure
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 4. LOGARITHMS, DECIBELS, AND NUMBER SYSTEMS
Page 49
tones: The sensation level is the number of dB SPL above the hearing
threshold at that frequency [8, p. 110].
For further information on “doing it right,” see, for example,
http://www.measure.demon.co.uk/Acoustics Software/loudness.html.
DB for Display
In practical signal processing, it is common to choose the maximum signal
magnitude as the reference amplitude. That is, we normalize the signal
so that the maximum amplitude is deﬁned as 1, or 0 dB. This convention
is also used by “sound level meters” in audio recording. When displaying
magnitude spectra, the highest spectral peak is often normalized to 0 dB.
We can then easily read oﬀlower peaks as so many dB below the highest
peak.
Figure 4.1b shows a plot of the Fast Fourier Transform (FFT) of ten
periods of a “Kaiser-windowed” sinusoid at 440 Hz. (FFT windows will
be discussed later in this reader. For now, just think of the window as
selecting and tapering a ﬁnite-duration section of the signal.) Note that
the peak dB magnitude has been normalized to zero, and that the plot
has been clipped at -100 dB.
Below is the Matlab code for producing Fig. 4.1. Note that it contains
several elements (windows, zero padding, spectral interpolation) that we
will not cover until later. They are included here as “forward references”
in order to keep the example realistic and practical, and to give you an
idea of “how far we have to go” before we know how to do practical spec-
trum analysis. Otherwise, the example just illustrates plotting spectra
on an arbitrary dB scale between convenient limits.
% Example practical display of the FFT of a synthesized sinusoid
fs = 44100;
% Sampling rate
f = 440;
% Sinusoidal frequency = A-440
nper = 10;
% Number of periods to synthesize
dur = nper/f;
% Duration in seconds
T = 1/fs;
% Sampling period
t = 0:T:dur;
% Discrete-time axis in seconds
L = length(t)
% Number of samples to synthesize
ZP = 5;
% Zero padding factor (for spectral interpolation)
N = 2^(nextpow2(L*ZP))
% FFT size (power of 2)
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 50
4.2. DECIBELS
0
5
10
15
20
25
-1
-0.5
0
0.5
1
Time (ms)
Amplitude
10 Periods of a 440 Hz Sinusoid, Kaiser Windowed
0
100
200
300
400
500
600
700
800
900
-100
-80
-60
-40
-20
0
Frequency (Hz)
Magnitude (dB)
Interpolated FFT of 10 Periods of 440 Hz Sinusoid
Figure 4.1: Windowed sinusoid (top) and its FFT magnitude (bot-
tom).
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 4. LOGARITHMS, DECIBELS, AND NUMBER SYSTEMS
Page 51
x = cos(2*pi*f*t);
% A sinusoid at A-440 (a "row vector")
w = kaiser(L,8);
% We’ll learn a bit about "FFT windows" later
xw = x .* w’;
% Need to transpose w to get a row vector
sound(xw,fs);
% Might as well listen to it
xzp = [xw,zeros(1,N-L)];% Zero-padded FFT input buffer
X = fft(xzp);
% Spectrum of xw, interpolated by factor ZP
Xmag = abs(X);
% Spectral magnitude
Xdb = 20*log10(Xmag);
% Spectral magnitude in dB
XdbMax = max(Xdb);
% Peak dB magnitude
Xdbn = Xdb - XdbMax;
% Normalize to 0dB peak
dBmin = -100;
% Don’t show anything lower than this
Xdbp = max(Xdbn,dBmin); % Normalized, clipped, dB magnitude spectrum
fmaxp = 2*f;
% Upper frequency limit of plot, in Hz
kmaxp = fmaxp*N/fs;
% Upper frequency limit of plot, in bins
fp = fs*[0:kmaxp]/N;
% Frequency axis in Hz
% Ok, plot it already!
subplot(2,1,1);
plot(1000*t,xw);
xlabel(’Time (ms)’);
ylabel(’Amplitude’);
title(sprintf(’a) %d Periods of a %3.0f Hz Sinusoid, Kaiser Windowed’,nper,f));
subplot(2,1,2);
plot(fp,Xdbp(1:kmaxp+1)); grid;
% Plot a dashed line where the peak should be:
hold on; plot([440 440],[dBmin,0],’--’); hold off;
xlabel(’Frequency (Hz)’);
ylabel(’Magnitude (dB)’);
title(sprintf([’b) Interpolated FFT of %d Periods of ’,...
’%3.0f Hz Sinusoid’],nper,f));
The following more compact Matlab produces essentially the same
plot, but without the nice physical units on the horizontal axes:
x = cos([0:2*pi/20:10*2*pi]); % 10 periods of a sinusoid, 20 samples/cycle
L = length(x);
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 52
4.2. DECIBELS
xw = x’ .* kaiser(L,8);
N = 2^nextpow2(L*5);
X = fft([xw’,zeros(1,N-L)]);
subplot(2,1,1); plot(xw);
xlabel(’Time (samples)’); ylabel(’Amplitude’);
title(’a) 10 Periods of a Kaiser-Windowed Sinusoid’);
subplot(2,1,2); kmaxp = 2*10*5; Xl = 20*log10(abs(X(1:kmaxp+1)));
plot([10*5+1,10*5+1],[-100,0],[0:kmaxp],max(Xl - max(Xl),-100)); grid;
xlabel(’Frequency (Bins)’); ylabel(’Magnitude (dB)’);
title(’b) Interpolated FFT of 10 Periods of Sinusoid’);
4.2.3
Dynamic Range
The dynamic range of a signal processing system can be deﬁned as the
maximum dB level sustainable without overﬂow (or other distortion) mi-
nus the dB level of the “noise ﬂoor”.
Similarly, the dynamic range of a signal can be deﬁned as its maximum
decibel level minus its average “noise level” in dB. For digital signals, the
limiting noise is ideally quantization noise.
Quantization noise is generally modeled as a uniform random variable
between plus and minus half the least signiﬁcant bit (since rounding to
the nearest representable sample value is normally used). If q denotes the
quantization interval, then the maximum quantization-error magnitude
is q/2, and its variance (“noise power”) is σ2
q = q2/12 (see Appendix 4.5
for a derivation of this value).
The rms level of the quantization noise is therefore σq = q/(2
√
3) ≈
0.3q, or about 60% of the maximum error.
The number system and number of bits chosen to represent signal
samples determines their available dynamic range. Signal processing op-
erations such as digital ﬁltering may use the same number system as the
input signal, or they may use extra bits in the computations, yielding an
increased “internal dynamic range”.
Since the threshold of hearing is near 0 dB SPL, and since the “thresh-
old of pain” is often deﬁned as 120 dB SPL, we may say that the dynamic
range of human hearing is approximately 120 dB.
The dynamic range of magnetic tape is approximately 55 dB. To in-
crease the dynamic range available for analog recording on magnetic tape,
companding is often used. “Dolby A” adds approximately 10 dB to the
dynamic range that will ﬁt on magnetic tape (by compressing the signal
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 4. LOGARITHMS, DECIBELS, AND NUMBER SYSTEMS
Page 53
dynamic range by 10 dB), while DBX adds 30 dB (at the cost of more
“transient distortion”).6 In general, any dynamic range can be mapped
to any other dynamic range, subject only to noise limitations.
4.3
Linear Number Systems for Digital Audio
This section discusses the most commonly used number formats for digital
audio.
4.3.1
Pulse Code Modulation (PCM)
The “standard” number format for sampled audio signals is oﬃcially
called Pulse Code Modulation ( PCM). This term simply means that each
signal sample is interpreted as a “pulse” (e.g., a voltage or current pulse)
at a particular amplitude which is binary encoded, typically in two’s
complement binary ﬁxed-point format (discussed below). When some-
one says they are giving you a soundﬁle in “raw binary format”, they
pretty much always mean (nowadays) 16-bit, two’s-complement PCM
data. Most mainstream computer soundﬁle formats consist of a “header”
(containing the length, etc.) followed by 16-bit two’s-complement PCM.
You can normally convert a soundﬁle from one computer’s format to
another by stripping oﬀits header and prepending the header for the new
machine (or simply treating it as raw binary format on the destination
computer). The UNIX “cat” command can be used for this, as can the
Emacs text editor (which handles binary data just ﬁne). The only is-
sue usually is whether the bytes have to be swapped (an issue discussed
further below).
4.3.2
Binary Integer Fixed-Point Numbers
Most prevalent computer languages only oﬀer two kinds of numbers,
ﬂoating-point and integer ﬁxed-point. On present-day computers, all num-
bers are encoded using binary digits (called “bits”) which are either 1 or
0.7 In C, C++, and Java, ﬂoating-point variables are declared as float
6Companders (compresser-expanders) essentially “turn down” the signal gain when
it is “loud” and “turn up” the gain when it is “quiet”. As long as the input-output
curve is monotonic (such as a log characteristic), the dynamic-range compression can
be undone (expanded).
7Computers use bits, as opposed to the more familiar decimal digits, because they
are more convenient to implement in digital hardware. For example, the decimal num-
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 54
4.3. LINEAR NUMBER SYSTEMS FOR DIGITAL AUDIO
(32 bits) or double (64 bits), while integer ﬁxed-point variables are de-
clared as short int (typically 16 bits and never less), long int (typi-
cally 32 bits and never less), or simply int (typically the same as a long
int, but sometimes between short and long). For an 8-bit integer, one
can use the char datatype (8 bits).
Since C was designed to accommodate a wide range of hardware,
including old mini-computers, some lattitude was historically allowed in
the choice of these bit-lengths. The sizeof operator is oﬃcially the “right
way” for a C program to determine the number of bytes in various data
types at run-time, e.g. sizeof(long). (The word int can be omitted
after short or long.) Nowadays, however, shorts are always 16 bits (at
least on all the major platforms), ints are 32 bits, and longs are typically
32 bits on 32-bit computers and 64 bits on 64-bit computers (although
some C/C++ compilers use long long int to declare 64-bit ints). Table
4.2 gives the lengths currently used by GNU C/C++ compilers (usually
called “gcc” or “cc”) on 64-bit processors.8
Java, which is designed to be platform independent, deﬁnes a long
int as equivalent in precision to 64 bits, an int as 32 bits, a short int
as 16 bits, and additionally a byte int as an 8-bit int. Similarly, the
“Structured Audio Orchestra Language” (SAOL9) (pronounced “sail”)—
the sound-synthesis component of the new MPEG-4 audio compression
standard—requires only that the underlying number system be at least as
accurate as 32-bit floats. All ints discussed thus far are signed integer
formats. C and C++ also support unsigned versions of all int types, and
they range from 0 to 2N −1 instead of −2N−1 to 2N−1 −1, where N is
bers 0, 1, 2, 3, 4, 5 become, in binary format, 0, 1, 10, 11, 100, 101. Each bit position
in binary notation corresponds to a power of 2, e.g., 5 = 1 · 22 + 0 · 21 + 1 · 20;
while each digit position in decimal notation corresponds to a power of 10, e.g.,
123 = 1 · 102 + 2 · 101 + 3 · 100. The term “digit” comes from the same word meaning
“ﬁnger.”
Since we have ten ﬁngers (digits), the term “digit” technically should be
associated only with decimal notation, but in practice it is used for others as well.
Other popular number systems in computers include octal which is base 8 (rarely seen
any more, but still speciﬁable in any C/C++ program by using a leading zero, e.g.,
0755 = 7 · 82 + 5 · 81 + 5 · 80 = 493 decimal = 111,101,101 binary), and hexadecimal
(or simply “hex”) which is base 16 and which employs the letters A through F to yield
16 digits (speciﬁable in C/C++ by starting the number with “0x”, e.g., 0x1ED =
1 · 162 + 15 · 161 + 14 · 160 = 493 decimal = 1,1110,1101 binary). Note, however, that
the representation within the computer is still always binary; octal and hex are simply
convenient groupings of bits into sets of three bits (octal) or four bits (hex).
8This information is subject to change without notice. Check your local compiler
documentation.
9http://sound.media.mit.edu/mpeg4
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 4. LOGARITHMS, DECIBELS, AND NUMBER SYSTEMS
Page 55
Type
Bytes
Notes
char
1
short
2
int
4
long
8
(4 bytes on 32-bit machines)
long long
8
(may become 16 bytes)
type *
8
(any pointer)
float
4
double
8
long double
8
(may become 10 bytes)
size t
8
(type of sizeof())
T* - T*
8
(pointer arithmetic)
Table 4.2: Byte sizes of GNU C/C++ data types for 64-bit architectures.
the number of bits. Finally, an unsigned char is often used for integers
that only range between 0 and 255.
One’s Complement Fixed-Point Format
One’s Complement is a particular assignment of bit patterns to numbers.
For example, in the case of 3-bit binary numbers, we have the assignments
shown in Table 4.3.
Binary
Decimal
000
0
001
1
010
2
011
3
100
-3
101
-2
110
-1
111
-0
Table 4.3: Three-bit one’s-complement binary ﬁxed-point numbers.
In general, N-bit numbers are assigned to binary counter values in
the “obvious way” as integers from 0 to 2N−1 −1, and then the negative
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 56
4.3. LINEAR NUMBER SYSTEMS FOR DIGITAL AUDIO
numbers are assigned in reverse order, as shown in the example.
The term “one’s complement” refers to the fact that negating a num-
ber in this format is accomplished by simply complementing the bit pat-
tern (inverting each bit).
Note that there are two representations for zero (all 0s and all 1s).
This is inconvenient when testing if a number is equal to zero. For this
reason, one’s complement is generally not used.
Two’s Complement Fixed-Point Format
In two’s complement, numbers are negated by complementing the bit
pattern and adding 1, with overﬂow ignored. From 0 to 2N−1 −1, positive
numbers are assigned to binary values exactly as in one’s complement.
The remaining assignments (for the negative numbers) can be carried
out using the two’s complement negation rule. Regenerating the N = 3
example in this way gives Table 4.4.
Binary
Decimal
000
0
001
1
010
2
011
3
100
-4
101
-3
110
-2
111
-1
Table 4.4: Three-bit two’s-complement binary ﬁxed-point numbers.
Note that according to our negation rule, −(−4) = −4. Logically,
what has happened is that the result has “overﬂowed” and “wrapped
around” back to itself.
Note that 3 + 1 = −4 also.
In other words,
if you compute 4 somehow, since there is no bit-pattern assigned to 4,
you get -4, because -4 is assigned the bit pattern that would be assigned
to 4 if N were larger.
Note that numerical overﬂows naturally result
in “wrap around” from positive to negative numbers (or from negative
numbers to positive numbers).
Computers normally “trap” overﬂows
as an “exception.”
The exceptions are usually handled by a software
“interrupt handler,” and this can greatly slow down the processing by
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 4. LOGARITHMS, DECIBELS, AND NUMBER SYSTEMS
Page 57
the computer (one numerical calculation is being replaced by a rather
sizable program).
Note that temporary overﬂows are ok in two’s complement; that is,
if you add 1 to 3 to get −4, adding −1 to −4 will give 3 again. This is
why two’s complement is a nice choice: it can be thought of as placing all
the numbers on a “ring,” allowing temporary overﬂows of intermediate
results in a long string of additions and/or subtractions. All that matters
is that the ﬁnal sum lie within the supported dynamic range.
Computers designed with signal processing in mind (such as so-called
“Digital Signal Processing (DSP) chips”) generally just do the best they
can without generating exceptions. For example, overﬂows quietly “sat-
urate” instead of “wrapping around” (the hardware simply replaces the
overﬂow result with the maximum positive or negative number, as ap-
propriate, and goes on). Since the programmer may wish to know that
an overﬂow has occurred, the ﬁrst occurrence may set an “overﬂow indi-
cation” bit which can be manually cleared. The overﬂow bit in this case
just says an overﬂow happened sometime since it was last checked.
Two’s-Complement, Integer Fixed-Point Numbers
Let N denote the (even) number of bits.
Then the value of a two’s
complement integer ﬁxed-point number can be expressed in terms of its
bits {bi}N−1
i=0
as
x = −b0 · 2N−1 +
N−2

i=1
bi · 2N−1−i,
bi ∈{0, 1}
(4.1)
We visualize the binary word containing these bits as
x = [b0b1 · · · bN−1]
Each bit bi is of course either 0 or 1. Check that the N = 3 table above
is computed correctly using this formula. As an example, the numer 3 is
expressed as
3 = [011] = −0 · 4 + 1 · 2 + 1 · 1
while the number -3 is expressed as
−3 = [101] = −1 · 4 + 0 · 2 + 1 · 1
and so on.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 58
4.3. LINEAR NUMBER SYSTEMS FOR DIGITAL AUDIO
The most-signiﬁcant bit in the word, b0, can be interpreted as the
“sign bit”. If b0 is “on”, the number is negative. If it is “oﬀ”, the number
is either zero or positive.
The least-signiﬁcant bit is bN−1. “Turning on” that bit adds 1 to the
number, and there are no fractions allowed.
The largest positive number is when all bits are on except b0, in which
case x = 2N−1−1. The largest (in magnitude) negative number is 10 · · · 0,
i.e., b0 = 1 and bi = 0 for all i > 0. Table 4.5 shows some of the most
common cases.
N
xmin
xmax
8
-128
127
16
-32768
32767
24
-8,388,608
8,388,607
32
-2,147,483,648
2,147,483,647
Table 4.5: Numerical range limits in N-bit two’s-complement.
4.3.3
Fractional Binary Fixed-Point Numbers
In “DSP chips” (microprocessors explicitly designed for digital signal pro-
cessing applications), the most commonly used ﬁxed-point format is frac-
tional ﬁxed point, also in two’s complement.
Quite simply, fractional ﬁxed-point numbers are obtained from integer
ﬁxed-point numbers by dividing them by 2N−1. Thus, the only diﬀerence
is a scaling of the assigned numbers. In the N = 3 case, we have the
correspondences shown in Table 4.6.
4.3.4
How Many Bits are Enough for Digital Audio?
Armed with the above knowledge, we can visit the question “how many
bits are enough” for digital audio. Since the threshold of hearing is around
0 db SPL, the “threshold of pain” is around 120 dB SPL, and each bit in
a linear PCM format is worth about 6 dB of dynamic range, we ﬁnd that
we need 120/6 = 20 bits to represent the full dynamic range of audio in
a linear ﬁxed-point format. This is a simplestic analysis because it is not
quite right to equate the least-signiﬁcant bit with the threshold of hearing;
instead, we would like to adjust the quantization noise ﬂoor to just below
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 4. LOGARITHMS, DECIBELS, AND NUMBER SYSTEMS
Page 59
Binary
Decimal
000
0
(0/4)
001
0.25
(1/4)
010
0.5
(2/4)
011
0.75
(3/4)
100
-1
(-4/4)
101
-0.75
(-3/4)
110
-0.5
(-2/4)
111
-0.25
(-1/4)
Table 4.6: Three-bit fractional ﬁxed-point numbers.
the threshold of hearing. Since the threshold of hearing is non-uniform,
we would also prefer a shaped quantization noise ﬂoor (a feat that can be
accomplished using ﬁltered error feedback10 Nevertheless, the simplistic
result gives an answer similar to the more careful analysis, and 20 bits
is a good number.
However, this still does not provide for headroom
needed in a digital recording scenario. We also need both headroom and
guard bits on the lower end when we plan to carry out a lot of signal
processing operations, especially digital ﬁltering. As an example, a 1024-
point FFT (Fast Fourier Transform) can give amplitudes 1024 times the
input amplitude (such as in the case of a constant “dc” input signal),
thus requiring 10 headroom bits. In general, 24 ﬁxed-point bits are pretty
reasonable to work with, although you still have to scale very carefully,
and 32 bits are preferable.
4.3.5
When Do We Have to Swap Bytes?
When moving a soundﬁle from one computer to another, such as from
a “PC” to a “Mac” (Intel processor to Motorola processor), the bytes
in each sound sample have to be swapped.
This is because Motorola
processors are big endian (bytes are numbered from most-signiﬁcant to
least-signiﬁcant in a multi-byte word) while Intel processors are little en-
10Normally, quantization error is computed as e(n) = x(n) −ˆx(n), where x(n) is
the signal being quantized, and ˆx(n) = Q[x(n)] is the quantized value, obtained by
rounding to the nearest representable amplitude. Filtered error feedback uses instead
the formula ˆx(n) = Q[x(n) + L{e(n −1)}], where L{ } denotes a ﬁltering operation
which “shapes” the quantization noise spectrum. An excellent article on the use of
round-oﬀerror feedback in audio digital ﬁlters is [9].
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 60
4.3. LINEAR NUMBER SYSTEMS FOR DIGITAL AUDIO
dian (bytes are numbered from least-signiﬁcant to most-signiﬁcant).11
Any Mac program that supports a soundﬁle format native to PCs (such
as .wav ﬁles) will swap the bytes for you. You only have to worry about
swapping the bytes yourself when reading raw binary soundﬁles from
a foreign computer, or when digging the sound samples out an “unsup-
ported” soundﬁle format yourself.
Since soundﬁles typically contain 16 bit samples (not for any good
reason, as we now know), there are only two bytes in each audio sample.
Let L denote the least-signiﬁcant byte, and M the most-signiﬁcant byte.
Then a 16-bit word is most naturally written [M, L] = M · 256 + L, i.e.,
the most-signiﬁcant byte is most naturally written to the left of the least-
signiﬁcant byte, analogous to the way we write binary or decimal integers.
This “most natural” ordering is used as the byte-address ordering in big-
endian processors:
M,L, M,L, M,L, ..., M,L
(Big Endian)
Little-endian machines, on the other hand, store bytes in the order
L,M, L,M, L,M, ..., L,M.
(Little Endian)
These orderings are preserved when the sound data are written to a disk
ﬁle.
Since a byte (eight bits) is the smallest addressable unit in modern day
processor families, we don’t have to additionally worry about reversing
the bits in each byte. Bits are not given explicit “addresses” in memory.
They are extracted by means other than simple addressing (such as mask-
ing and shifting operations, table look-up, or using specialized processor
instructions).
Table 4.7 lists popular present-day processors and their “endianness”:12
When compiling C or C++ programs under UNIX, there may be a
BYTE ORDER macro in endian.h or bytesex.h. In other cases, there may
be a deﬁned macro
INTEL ,
LITTLE ENDIAN ,
BIG ENDIAN ,
or the like.
11Remember that byte addresses in a big endian word start at the big end of the
word, while in a little endian architecture, they start at the little end of the word.
12Thanks to Bill Schottstaedt for help with this table.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 4. LOGARITHMS, DECIBELS, AND NUMBER SYSTEMS
Page 61
Processor Family
Endian
Pentium (Intel)
Little
Alpha (DEC/Compaq)
Little
680x0 (Motorola)
Big
PowerPC (Motorola & IBM)
Big
SPARC (Sun)
Big
MIPS (SGI)
Big
Table 4.7: Byte ordering in the major computing platforms.
4.4
Logarithmic Number Systems for Audio
Since hearing is approximately logarithmic, it makes sense to represent
sound samples in a logarithmic or semi-logarithmic number format. Floating-
point numbers in a computer are partially logarithmic (the exponent
part), and one can even use an entirely logarithmic ﬁxed-point number
system. The µ-law amplitude-encoding format is linear at small ampli-
tudes and becomes logarithmic at large amplitudes. This section discusses
these formats.
4.4.1
Floating-Point Numbers
Floating-point numbers consist of an “exponent,” “signiﬁcand”, and “sign
bit”. For a negative number, we may set the sign bit of the ﬂoating-point
word and negate the number to be encoded, leaving only nonnegative
numbers to be considered. Zero is represented by all zeros, so now we
need only consider positive numbers.
The basic idea of ﬂoating point encoding of a binary number is to
normalize the number by shifting the bits either left or right until the
shifted result lies between 1/2 and 1.
(A left-shift by one place in a
binary word corresponds to multiplying by 2, while a right-shift one place
corresponds to dividing by 2.) The number of bit-positions shifted to
normalize the number can be recorded as a signed integer. The negative
of this integer (i.e., the shift required to recover the original number) is
deﬁned as the exponent of the ﬂoating-point encoding. The normalized
number between 1/2 and 1 is called the signiﬁcand, so called because it
holds all the “signiﬁcant bits” of the number.
Floating point notation is exactly analogous to “scientiﬁc notation”
for decimal numbers, e.g., 1.2345×10−9; the number of signiﬁcant digits,
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 62
4.4. LOGARITHMIC NUMBER SYSTEMS FOR AUDIO
5 in this example, is determined by counting digits in the “signiﬁcand”
1.2345, while the “order of magnitude” is determined by the power of 10
(-9 in this case). In ﬂoating-point numbers, the signiﬁcand is stored in
fractional two’s-complement binary format, and the exponent is stored as
a binary integer.
Since the signiﬁcand lies in the interval [1/2, 1),13 its most signiﬁcant
bit is always a 1, so it is not actually stored in the computer word, giving
one more signiﬁcant bit of precision.
Let’s now restate the above a little more precisely. Let x > 0 denote
a number to be encoded in ﬂoating-point, and let ˜x = x · 2−E denote
the normalized value obtained by shifting x either E bits to the right (if
E > 0), or |E| bits to the left (if E < 0). Then we have 1/2 ≤˜x < 1,
and x = ˜x · 2E. The signiﬁcand M of the ﬂoating-point representation
for x is deﬁned as the binary encoding of ˜x.14 It is often the case that ˜x
requires more bits than are available for exact encoding. Therefore, the
signiﬁcand is typically rounded (or truncated) to the value closest to ˜x.
Given NM bits for the signiﬁcand, the encoding of ˜x can be computed by
multiplying it by 2NM (left-shifting it NM bits), rounding to the nearest
integer (or truncating toward minus inﬁnity—the as implemented by the
floor() function), and encoding the NM-bit result as a binary (signed)
integer.
As a ﬁnal practical note, exponents in ﬂoating-point formats may have
a bias. That is, instead of storing E as a binary integer, you may ﬁnd a
binary encoding of E −B where B is the bias.15
These days, ﬂoating-point formats generally follow the IEEE stan-
dards set out for them. A single-precision ﬂoating point word is 32 bits
(four bytes) long, consisting of 1 sign bit, 8 exponent bits, and 23 signif-
icand bits, normally laid out as
S EEEEEEEE MMMMMMMMMMMMMMMMMMMMMMM
13The notation [a, b) denotes a half-open interval which includes a but not b.
14Another term commonly heard for “signiﬁcand” is “mantissa.” However, this use
of the term “mantissa” is not the same as its previous deﬁnition as the fractional part
of a logarithm. We will therefore use only the term “signiﬁcand” to avoid confusion.
15By choosing the bias equal to half the numerical dynamic range of E (thus ef-
fectively inverting the sign bit of the exponent), it becomes easier to compare two
ﬂoating-point numbers in hardware: the entire ﬂoating-point word can be treated by
the hardware as one giant integer for numerical comparison purposes.
This works
because negative exponents correspond to ﬂoating-point numbers less than 1 in mag-
nitude, while. positive exponents correspond to ﬂoating-point numbers greater than 1
in magnitude.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 4. LOGARITHMS, DECIBELS, AND NUMBER SYSTEMS
Page 63
where S denotes the sign bit, E an exponent bit, and M a signiﬁcand bit.
Note that in this layout, ordinary integer comparison can be used in the
hardware.
A double-precision ﬂoating point word is 64 bits (eight bytes) long,
consisting of 1 sign bit, 11 exponent bits, and 52 signiﬁcand bits. In the
Intel Pentium processor, there is also an extended precision format, used
for intermediate results, which is 80 bits (ten bytes) containing 1 sign
bit, 15 exponent bits, and 64 signiﬁcand bits. In Intel processors, the
exponent bias is 127 for single-precision ﬂoating-point, 1023 for double-
precision, and 16383 for extended-precision. The single and double preci-
sion formats have a “hidden” signiﬁcand bit, while the extended precision
format does not. Thus, the most signiﬁcant signiﬁcand bit is always set
in extended precision.
The MPEG-4 audio compression standard (which supports compres-
sion using music synthesis algorithms) speciﬁes that the numerical calcu-
lations in any MPEG-4 audio decoder should be at least as accurate as
32-bit single-precision ﬂoating point.
4.4.2
Logarithmic Fixed-Point Numbers
In some situations it makes sense to use logarithmic ﬁxed-point.
This
number format can be regarded as a ﬂoating-point format consisting of
an exponent and no explicit signiﬁcand. However, the exponent is not
interpreted as an integer as it is in ﬂoating point.
Instead, it has a
fractional part which is a true mantissa. (The integer part is then the
“characteristic” of the logarithm.) In other words, a logarithmic ﬁxed-
point number is a binary encoding of the log-base-2 of the signal-sample
magnitude. The sign bit is of course separate.
An example 16-bit logarithmic ﬁxed-point number format suitable for
digital audio consists of one sign bit, a 5-bit characteristic, and a 10-bit
mantissa:
S CCCCC MMMMMMMMMM
The 5-bit characteristic gives a dynamic range of about 6 dB ×25 = 192
dB. This is an excellent dynamic range for digital audio. (While 120 dB
would seem to be enough for audio, consider that when digitally modeling
a brass musical instrument, say, the internal air pressure near the “virtual
mouthpiece” can be far higher than what actually reaches the ears in the
audience.)
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 64
4.4. LOGARITHMIC NUMBER SYSTEMS FOR AUDIO
A nice property of logarithmic ﬁxed-point numbers is that multiplies
simply become additions and divisions become subtractions. The hard
elementary operation are now addition and subtraction, and these are
normally done using table lookups to keep them simple.
One “catch” when working with logarithmic ﬁxed-point numbers is
that you can’t let “dc” build up. A wandering dc component will cause
the quantization to be coarse even for low-level “ac” signals. It’s a good
idea to make sure dc is always ﬁltered out in logarithmic ﬁxed-point.
4.4.3
Mu-Law Companding
A companding operation compresses dynamic range on encode and ex-
pands dynamic range on decode. In digital telephone networks and voice
modems (currently in use everywhere), standard CODEC 16 chips are
used in which audio is digitized in a simple 8-bit µ-law format (or simply
“mu-law”).
Given an input sample x(n) represented in some internal format, such
as a short, it is converted to 8-bit mu-law format by the formula [10]
ˆxµ
∆= Qµ [log2 (1 + µ |x(n)|)]
where Qµ[] is a quantizer which produces a kind of logarithmic ﬁxed-
point number with a 3-bit characteristic and a 4-bit mantissa, using a
small table lookup for the mantissa.
As we all know from talking on the telephone, mu-law sounds really
quite good for voice, at least as far as intelligibility is concerned. However,
because the telephone bandwidth is only around 3 kHz (nominally 200–
3200 Hz), there is very little “bass” and no “highs” in the spectrum above
4 kHz. This works out ﬁne for intelligibility of voice because the ﬁrst three
formants (envelope peaks) in typical speech spectra occur in this range,
and also because the diﬀerence in spectral shape (particularly at high
frequencies) between consonants such as “sss”, “shshsh”, “ﬀf”, “ththth”,
etc., are suﬃciently preserved in this range. As a result of the narrow
bandwidth provided for speech, it is sampled at only 8 kHz in standard
CODEC chips.
For “wideband audio”, we like to see sampling rates at least as high as
44.1 kHz, and the latest systems are moving to 96 kHz (mainly because
oversampling simpliﬁes signal processing requirements in various areas,
16CODEC is an acronym for “COder/DECoder”.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 4. LOGARITHMS, DECIBELS, AND NUMBER SYSTEMS
Page 65
not because we can actually hear anything above 20 kHz). In addition,
we like the low end to extend at least down to 20 Hz or so. (The lowest
note on a normally tuned bass guitar is E1 = 41.2 Hz. The lowest note
on a grand piano is A0 = 27.5 Hz.)
4.5
Appendix A: Round-OﬀError Variance
This section shows how to derive that the noise power of quantization
error is q2/12, where q is the quantization step size.
Each round-oﬀerror in quantization noise e(n) is modeled as a uniform
random variable between −q/2 and q/2. It therefore has the probability
density function (pdf)
pe(x) =
 1
q,
|x| ≤q
2
0,
|x| > q
2
Thus, the probability that a given roundoﬀerror e(n) lies in the interval
[x1, x2] is given by
 x2
x1
pe(x)dx = x2 −x1
q
assuming of course that x1 and x2 lie in the allowed range [−q/2, q/2]. We
might loosely refer to pe(x) as a probability distribution, but technically
it is a probability density function, and to obtain probabilities, we have
to integrate over one or more intervals, as above.
We use probability
distributions for variables which take on discrete values (such as dice),
and we use probability densities for variables which take on continuous
values (such as round-oﬀerrors).
The mean of a random variable is deﬁned as
µe
∆=
 ∞
−∞
xpe(x)dx = 0
In our case, the mean is zero because we are assuming the use of rounding
(as opposed to truncation, etc.).
The mean of a signal e(n) is the same thing as the expected value of
e(n), which we write as E{e(n)}. In general, the expected value of any
function f(v) of a random variable v is given by
E{f(v)} ∆=
 ∞
−∞
f(x)pv(x)dx
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 66
4.6. APPENDIX B: ELECTRICAL ENGINEERING 101
Since the quantization-noise signal e(n) is modeled as a series of inde-
pendent, identically distributed (iid) random variables, we can estimate
the mean by averaging the signal over time. Such an estimate is called a
sample mean.
Probability distributions are often be characterized by their moments.
The nth moment of the pdf p(x) is deﬁned as
 ∞
−∞
xnp(x)dx
Thus, the mean µx = E{e(n)} is the ﬁrst moment of the pdf. The second
moment is simply the expected value of the random variable squared, i.e.,
E{e2(n)}.
The variance of a random variable e(n) is deﬁned as the second central
moment of the pdf:
σ2
e
∆= E{[e(n) −µe]2} =
 ∞
−∞
(x −µe)2pe(x)dx
“Central” just means that the moment is evaluated after subtracting out
the mean, that is, looking at e(n) −µe instead of e(n). In the case of
round-oﬀerrors, the mean is zero, so subtracting out the mean has no
eﬀect. Plugging in the constant pdf for our random variable e(n) which
we assume is uniformly distributed on [−q/2, q/2], we obtain the variance
σ2
e =
 q/2
−q/2
x2 1
qdx = 1
q
1
3x3

q/2
−q/2
= q2
12
Note that the variance of e(n) can be estimated by averaging e2(n) over
time, that is, by computing the mean square. Such an estimate is called
the sample variance. For sampled physical processes, the sample variance
is proportional to the average power in the signal. Finally, the square root
of the sample variance (the rms level) is sometimes called the standard
deviation of the signal, but this term is only precise when the random
variable has a Gaussian pdf.
Some good textbooks in the area of statistical signal processing in-
clude [11, 12, 13]. EE 278 is the starting course on this topic at Stanford.
4.6
Appendix B: Electrical Engineering 101
The state of an ideal resistor is completely speciﬁed by the voltage across
it (call it V volts) and the current passing through it (I Amperes, or
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 4. LOGARITHMS, DECIBELS, AND NUMBER SYSTEMS
Page 67
simply “amps”). The ratio of voltage to current gives the value of the
resistor (V/I = R = resistance in Ohms).
The fundamental relation
between voltage and current in a resistor is called Ohm’s Law:
V (t) = R · I(t)
(Ohm’s Law)
where we have indicated also that the voltage and current may vary with
time (while the resistor value normally does not).
The electrical power in watts dissipated by a resistor R is given by
P = V · I = V 2
R = R · I2
where V is the voltage and I is the current. Thus, volts times amps gives
watts. Also, volts squared over ohms equals watts, and so on.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 68
4.6. APPENDIX B: ELECTRICAL ENGINEERING 101
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Chapter 5
Sinusoids and Exponentials
This chapter provides an introduction to sinusoids, exponentials, com-
plex sinusoids, t60, in-phase and quadrature sinusoidal components, the
analytic signal, positive and negative frequencies, constructive and de-
structive interference, invariance of sinusoidal frequency in linear time-
invariant systems, circular motion as the vector sum of in-phase and
quadrature sinusoidal motions, sampled sinusoids, generating sampled
sinusoids from powers of z, and plot examples using Mathematica.
5.1
Sinusoids
A sinusoid is any function of time having the following form:
x(t) = A sin(ωt + φ)
where all variables are real numbers, and
A
=
Peak Amplitude (nonnegative)
ω
=
Radian Frequency (rad/sec)
=
2πf (f in Hz)
t
=
Time (sec)
f
=
Frequency (Hz)
φ
=
Phase (radians)
The term “peak amplitude” is often shortened to “amplitude,” e.g.,
“the amplitude of the sound was measured to be 5 Pascals.”
Strictly
speaking, however, the “amplitude” of a signal x is its instantaneous
69

Page 70
5.1. SINUSOIDS
value x(t) at any time t.
The peak amplitude A satisﬁes x(t) ≤A.
The “instantaneous magnitude” or simply “magnitude” of a signal x(t)
is given by |x(t)|, and the peak magnitude is the same thing as the peak
amplitude.
Note that Hz is an abbreviation for Hertz which physically means
“cycles per second.” You might also encounter the older (and clearer)
notation “c.p.s.” for cycles per second.
Since sin(θ) is periodic with period 2π, the phase φ ± 2π is indistin-
guishable from the phase φ. As a result, we may restrict the range of φ
to any length 2π interval. When needed, we will choose
−π ≤φ < π,
i.e., φ ∈[−π, π). You may also encounter the convention φ ∈[0, 2π).
5.1.1
Example Sinusoids
Figure 5.1 plots the sinusoid A sin(2πft+φ), for A = 10, f = 2.5, φ = π/4,
and t ∈[0, 1]. Study the plot to make sure you understand the eﬀect of
changing each parameter (amplitude, frequency, phase), and also note the
deﬁnitions of “peak-to-peak amplitude” and “zero crossings.”
10 Sin[2 Pi 2.5 t + Pi/4]
0.2
0.4
0.6
0.8
1 Sec
-10
-5
5
10
Amp.
A Sin[φ]
f
P
=
ω
π
=
1
2
Period
Zero
Crossings
Amplitude
Peaks
Peak-to-Peak
Amplitude = 2A
...
...
Figure 5.1: An example sinusoid.
The Mathematica code for generating this ﬁgure is listed in §5.4.
A “tuning fork” vibrates approximately sinusoidally. An “A-440” tun-
ing fork oscillates at 440 cycles per second. As a result, a tone recorded
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 5. SINUSOIDS AND EXPONENTIALS
Page 71
from an ideal A-440 tuning fork is a sinusoid at f = 440 Hz. The amp-
litude A determines how loud it is and depends on how hard we strike
the tuning fork. The phase φ is set by exactly when we strike the tuning
fork (and on our choice of when time 0 is). If we record an A-440 tuning
fork on an analog tape recorder, the electrical signal recorded on tape is
of the form
x(t) = A sin(2π440t + φ)
As another example, the sinusoid at amplitude 1 and phase π/2 (90
degrees) is simply
x(t) = sin(ωt + π/2) = cos(ωt)
Thus, cos(ωt) is a sinusoid at phase 90-degrees, while sin(ωt) is a sinusoid
at zero phase. Note, however, that we could just as well have deﬁned
cos(ωt) to be the zero-phase sinusoid rather than sin(ωt). It really doesn’t
matter, except to be consistent in any given usage. The concept of a
“sinusoidal signal” is simply that it is equal to a sine or cosine function
at some amplitude, frequency, and phase. It does not matter whether we
choose sin() or cos() in the “oﬃcial” deﬁnition of a sinusoid. You may
encounter both deﬁnitions. Using sin() is nice since “sinusoid” in a sense
generalizes sin(). However, using cos() is nicer when deﬁning a sinusoid
to be the real part of a complex sinusoid (which we’ll talk about later).
5.1.2
Why Sinusoids are Important
Sinusoids are fundamental in a variety of ways.
One reason for the importance of sinusoids is that they are funda-
mental in physics. Anything that resonates or oscillates produces quasi-
sinusoidal motion. See simple harmonic motion in any freshman physics
text for an introduction to this topic.
Another reason sinusoids are important is that they are eigenfunc-
tions of linear systems (which we’ll say more about later). This means
that they are important for the analysis of ﬁlters such as reverberators,
equalizers, certain (but not all) “eﬀects”, etc.
Perhaps most importantly, from the point of view of computer music
research, is that the human ear is a kind of spectrum analyzer. That is,
the chochlea of the inner ear physically splits sound into its (near) sinu-
soidal components. This is accomplished by the basilar membrane in the
inner ear: a sound wave injected at the oval window (which is connected
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 72
5.1. SINUSOIDS
via the bones of the middle ear to the ear drum), travels along the basi-
lar membrane inside the coiled cochlea. The membrane starts out thick
and stiﬀ, and gradually becomes thinner and more compliant toward its
apex (the helicotrema). A stiﬀmembrane has a high resonance frequency
while a thin, compliant membrane has a low resonance frequency (as-
suming comparable mass density, or at least less of a diﬀerence in mass
than in compliance). Thus, as the sound wave travels, each frequency
in the sound resonates at a particular place along the basilar membrane.
The highest frequencies resonate right at the entrance, while the lowest
frequencies travel the farthest and resonate near the helicotrema. The
membrane resonance eﬀectively “shorts out” the signal energy at that
frequency, and it travels no further. Along the basilar membrane there
are hair cells which “feel” the resonant vibration and transmit an in-
creased ﬁring rate along the auditory nerve to the brain. Thus, the ear
is very literally a Fourier analyzer for sound, albeit nonlinear and using
“analysis” parameters that are diﬃcult to match exactly. Nevertheless,
by looking at spectra (which display the amount of each sinusoidal fre-
quency present in a sound), we are looking at a representation much more
like what the brain receives when we hear.
5.1.3
In-Phase and Quadrature Sinusoidal Components
From the trig identity sin(A + B) = sin(A) cos(B) + cos(A) sin(B), we
have
x(t)
=
A sin(ωt + φ) = A sin(φ + ωt)
=
[A sin(φ)] cos(ωt) + [A cos(φ)] sin(ωt)
∆=
A1 cos(ωt) + A2 sin(ωt)
From this we may conclude that every sinusoid can be expressed as the
sum of a sine function (phase zero) and a cosine function (phase π/2).
If the sine part is called the “in-phase” component, the cosine part can
be called the “phase-quadrature” component. In general, “phase quadra-
ture” means “90 degrees out of phase,” i.e., a relative phase shift of ±π/2.
It is also the case that every sum of an in-phase and quadrature com-
ponent can be expressed as a single sinusoid at some amplitude and phase.
The proof is obtained by working the previous derivation backwards.
Figure 5.2 illustrates in-phase and quadrature components overlaid.
Note that they only diﬀer by a relative 90 degree phase shift. (See §5.4
for the Mathematica code for this ﬁgure.)
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 5. SINUSOIDS AND EXPONENTIALS
Page 73
0.2
0.4
0.6
0.8
1
 Time (Sec)
-1
-0.5
0.5
1
Amplitude
Figure 5.2: In-phase and quadrature sinusoidal components.
5.1.4
Sinusoids at the Same Frequency
An important property of sinusoids at a particular frequency is that they
are closed with respect to addition. In other words, if you take a sinsusoid,
make many copies of it, scale them all by diﬀerent gains, delay them all
by diﬀerent amounts, and add them up, you always get a sinusoid at the
same original frequency. This is a nontrivial property. It obviously holds
for any constant signal x(t) = c (which we may regard as a sinusoid at
frequency f = 0), but it is not obvious for f ̸= 0 (see Fig. 5.2 and think
about the sum of the two waveforms shown being precisely a sinusoid).
Since every linear, time-invariant (LTI1) system (ﬁlter) operates by
copying, scaling, delaying, and summing its input signal(s) to create its
output signal(s), it follows that when a sinusoid at a particular frequency
is input to an LTI system, a sinusoid at that same frequency always
appears at the output. Only the amplitude and phase can be changed
by the system. We say that sinusoids are eigenfunctions of LTI systems.
Conversely, if the system is nonlinear or time-varying, new frequencies
are created at the system output.
To prove the important invariance property of sinusoids, we may sim-
ply express all scaled and delayed sinusoids in the “mix” in terms of their
in-phase and quadrature components and then add them up. For exam-
ple, consider the case of two sinusoids arbitrarily scaled by gains g1, g2
1A system S is said to be linear if for any two input signals x1(t) and x2(t), we
have S[x1(t) + x2(t)] = S[x1(t)] + S[x2(t)]. A system is said to be time invariant if
S[x(t −τ)] = y(t −τ), where y(t)
∆= S[x(t)]. This subject is developed in detail in [4].
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 74
5.1. SINUSOIDS
and arbitrarily delayed by time-delays t1, t2:
y(t)
∆=
g1x(t −t1) + g2x(t −t2)
=
g1A sin[ω(t −t1) + φ] + g2A sin[ω(t −t2) + φ]
Focusing on the ﬁrst term, we have
g1A sin[ω(t −t1) + φ]
=
g1A sin[ωt + (φ −ωt1)]
=
[g1A sin(φ −ωt1)] cos(ωt) + [g1A cos(φ −ωt1)] sin(ωt)
∆=
A1 cos(ωt) + B1 sin(ωt)
We similarly compute
g2A sin[ω(t −t2) + φ] = A2 cos(ωt) + B2 sin(ωt)
and add to obtain
y(t) = (A1 + A2) cos(ωt) + (B1 + B2) sin(ωt)
This result, consisting of one in-phase and one quadrature signal compo-
nent, can now be converted to a single sinusoid at some amplitude and
phase (and frequency ω), as discussed above.
5.1.5
Constructive and Destructive Interference
Sinusoidal signals are analogous to monochromatic laser light. You might
have seen “speckle” associated with laser light, caused by destructive in-
teference of multiple reﬂections of the light beam. In a room, the same
thing happens with sinusoidal sound. For example, play a simple sinu-
soidal tone (e.g., “A-440” which is a sinusoid at frequency f = 440 Hz)
and walk around the room with one ear plugged. If the room is reverber-
ant you should be able ﬁnd places where the sound goes completely away
due to destructive interference. In between such places (which we call
“nodes” in the soundﬁeld), there are “antinodes” at which the sound is
louder by 6 dB (amplitude doubled) due to constructive interference. In a
diﬀuse reverberant soundﬁeld, the distance between nodes is on the order
of a wavelength (the “correlation distance” within the random sound-
ﬁeld).
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 5. SINUSOIDS AND EXPONENTIALS
Page 75
The way reverberation produces nodes and antinodes for sinusoids in
a room is illustrated by the simple comb ﬁlter.2 There is also shown in
Fig. 5.3. A unit-amplitude sinusoid is present at the input, and the output
must also be sinusoidal, since the comb ﬁlter is linear and time-invariant.
The feedforward path of the comb ﬁlter has a gain of 0.5, and the delay is
one period in one case and half a period in the other. With the delay set
to one period, the unit amplitude sinusoid coming out of the delay line
constructively interferes with the amplitude 0.5 sinusoid from the feed-
forward path, and the output amplitude is therefore 1 + 0.5 = 1.5. In the
other case, with the delay set to half period, the unit amplitude sinusoid
coming out of the delay line destructively interferes with the amplitude 0.5
sinusoid from the feed-forward path, and the output amplitude therefore
drops to |−1 + 0.5| = 0.5.
0.2
0.4
0.6
0.8
1 Time (Sec)
-1.5
-1
-0.5
0.5
1
1.5
Amplitude
delay
delay = one period
delay = half period
0.2
0.4
0.6
0.8
1
-1
-0.5
0.5
1
Input Sinusoid
0.5
Output Sinusoid,
Two Cases
Figure 5.3: A comb ﬁlter with a sinusoidal input.
Consider a ﬁxed delay of τ seconds for the delay line. Constructive
interference happens at all frequencies for which an exact integer number
of periods ﬁts in the delay line, i.e., fτ = 0, 1, 2, 3, . . ., or f = n/τ, for
n = 0, 1, 2, 3, . . .. On the other hand, destructive interference happens
2Technically, this is the feedforward comb ﬁlter, also called the “inverse comb ﬁlter”
[14]. The longer names are meant to distinguish it from the feedback comb ﬁlter (deﬁned
as “the” comb ﬁlter in Dodge and Jerse [15]). In the feedback comb ﬁlter, the delay
output is fed back around the delay line and summed with the delay input instead of
the input being fed forward around the delay line and summed with its output. The
frequency response of the feedforward comb ﬁlter is the inverse of that of the feedback
comb ﬁlter (one will cancel the eﬀect of the other), hence the name “inverse comb
ﬁlter.” When the delay in the feedforward comb ﬁlter is varied slowly over time, the
ﬂanger eﬀect is obtained. Flanging was originally achieved by mixing the outputs of
two LP record turntables and changing their relative speeds by alternately touching
the “ﬂange” of each turntable to slow it down.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 76
5.2. EXPONENTIALS
at all frequencies for which number of periods in the delay line is an
integer plus a half, i.e., fτ = 1.5, 2.5, 3.5, etc., or, f = (n + 1/2)/τ, for
n = 0, 1, 2, 3, . . .. It is quick to verify that frequencies of constructive
interference alternate with frequencies of destructive interference, and
therefore the amplitude response of the comb ﬁlter (a plot of gain versus
frequency) looks as shown in Fig. 5.4.
2
4
6
8
Freq (Hz)
0.25
0.5
0.75
1
1.25
1.5
1.75
Gain
Figure 5.4: Comb ﬁlter amplitude response when delay τ = 1 sec.
The ampitude response of a comb ﬁlter has a “comb” like shape,
hence the name.3 Note that if the feedforward gain is increased from 0.5
to 1, the comb-ﬁlter gain ranges between 0 (complete cancellation) and 2.
Negating the feedforward gain inverts the gain curve, placing a minumum
at dc4 instead of a peak.
5.2
Exponentials
The canonical form of an exponential function, as typically used in signal
processing, is
a(t) = Ae−t/τ,
t ≥0
where τ is called the time constant of the exponential. A is the peak
amplitude, as before. The time constant is the time it takes to decay by
3While there is no reason it should be obvious at this point, the comb-ﬁlter gain
varies in fact sinusoidally between 0.5 and 1.5. It looks more ”comb” like on a dB
amplitude scale, which is more appropriate for audio applications.
4“dc” means “direct current” and is an electrical engineering term for “frequency
0”.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 5. SINUSOIDS AND EXPONENTIALS
Page 77
1/e, i.e.,
a(τ)
a(0) = 1
e
A normalized exponential decay is depicted in Fig. 5.5.
t = τ
t = t60
1
2
3
4
5
6
7 Sec/Tau
0.2
0.4
0.6
0.8
1
Amplitude/A
Figure 5.5: The decaying exponential Ae−t/τ.
5.2.1
Why Exponentials are Important
Exponential decay occurs naturally when a quantity is decaying at a
rate which is proportional to how much is left. In nature, all linear res-
onators, such as musical instrument strings and woodwind bores, exhibit
exponential decay in their response to a momentary excitation. As an-
other example, reverberant energy in a room decays exponentially after
the direct sound stops. Essentially all undriven oscillations decay expo-
nentially (provided they are linear and time-invariant). Undriven means
there is no ongoing source of driving energy. Examples of undriven oscil-
lations include the vibrations of a tuning fork, struck or plucked strings,
a marimba or xylophone bar, and so on. Examples of driven oscillations
include horns, woodwinds, bowed strings, and voice. Driven oscillations
must be periodic while undriven oscillations normally are not, except in
idealized cases.
Exponential growth occurs when a quantity is increasing at a rate
proportional to the current amount. Exponential growth is unstable since
nothing can grow exponentially forever without running into some kind
of limit. Note that a positive time constant corresponds to exponential
decay, while a negative time constant corresponds to exponential growth.
In signal processing, we almost always deal exclusively with exponential
decay (positive time constants).
Exponential growth and decay are illustrated in Fig. 5.6.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 78
5.3. COMPLEX SINUSOIDS
0.2
0.4
0.6
0.8
1Time (sec)
0.5
1
1.5
2
2.5
Amp
e−t
et
Figure 5.6: Growing and decaying exponentials.
5.2.2
Audio Decay Time (T60)
In audio, a decay by 1/e is too small to be considered a practical “decay
time.” In architectural acoustics (which includes the design of concert
halls), a more commonly used measure of decay is “t60” (or T60), which
is deﬁned as the time to decay by 60 dB.5 That is, t60 is obtained by
solving the equation
a(t60)
a(0) = 10−60/20 = 0.001
Using the deﬁnition of the exponential a(t) = Ae−t/τ, we ﬁnd
t60 = ln(1000)τ ≈6.91τ
Thus, t60 is about seven time constants.
See where t60 is marked on
Fig. 5.5 compared with τ.
5.3
Complex Sinusoids
Recall Euler’s Identity,
ejθ = cos(θ) + j sin(θ)
Multiplying this equation by A ≥0 and setting θ = ωt+φ, we obtain the
deﬁnition of the complex sinusoid:
s(t) ∆= Aej(ωt+φ) = A cos(ωt + φ) + jA sin(ωt + φ)
5Recall that a gain factor g is converted to decibels (dB) by the formula 20 log10(g).
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 5. SINUSOIDS AND EXPONENTIALS
Page 79
Thus, a complex sinusoid consists of an in-phase component for its real
part, and a phase-quadrature component for its imaginary part. Since
sin2(θ) + cos2(θ) = 1, we have
|s(t)| ≡A
That is, the complex sinusoid is constant modulus.
(The symbol “≡”
means “identically equal to,” i.e., for all t.) The phase of the complex
sinusoid is
̸ s(t) = ωt + φ
The derivative of the phase of the complex sinusoid gives its frequency
d
dt
̸ s(t) = ω = 2πf
5.3.1
Circular Motion
Since the modulus of the complex sinusoid is constant, it must lie on a
circle in the complex plane. For example,
x(t) = ejωt
traces out counter-clockwise circular motion along the unit circle in the
complex plane, while
x(t) = e−jωt
is clockwise circular motion.
We call a complex sinusoid of the form ejωt, where ω > 0, a positive-
frequency sinusoid. Similarly, we deﬁne a complex sinusoid of the form
e−jωt, with ω > 0, to be a negative-frequency sinusoid.
Note that a
positive- or negative-frequency sinusoid is necessarily complex.
5.3.2
Projection of Circular Motion
We have
re

ejωt
=
cos(ωt)
im

ejωt
=
sin(ωt)
Interpreting this in the complex plane tells us that sinusoidal motion
is the projection of circular motion onto any straight line.
Thus, the
sinusoidal motion cos(ωt) is the projection of the circular motion ejωt
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 80
5.3. COMPLEX SINUSOIDS
onto the x (real-part) axis, while sin(ωt) is the projection of ejωt onto the
y (imaginary-part) axis.
Figure 5.7 shows a plot of a complex sinusoid versus time, along with
its projections onto coordinate planes. This is a 3D plot showing the z-
plane versus time. The axes are the real part, imaginary part, and time.
(Or we could have used magnitude and phase versus time.)
Figure 5.7: A complex sinusoid and its projections.
Note that the left projection (onto the z plane) is a circle, the lower
projection (real-part vs. time) is a cosine, and the upper projection (imaginary-
part vs. time) is a sine. A point traversing the plot projects to uniform
circular motion in the z plane, and sinusoidal motion on the two other
planes.
5.3.3
Positive and Negative Frequencies
Earlier, we used Euler’s Identity to show
cos(θ)
= ejθ + e−jθ
2
sin(θ)
= ejθ −e−jθ
2j
Setting θ = ωt + φ, we see that both sine and cosine (and hence all
real sinusoids) consist of a sum of equal and opposite circular motion.
Phrased diﬀerently, every real sinusoid consists of an equal contribution
of positive and negative frequency components. This is true of all real
signals. When we get to spectrum analysis, we will ﬁnd that every real
signal contains equal amounts of positive and negative frequencies, i.e.,
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 5. SINUSOIDS AND EXPONENTIALS
Page 81
if X(ω) denotes the spectrum of the real signal x(t), we will always have
|X(−ω)| = |X(ω)|.
Note that, mathematically, the complex sinusoid Aej(ωt+φ) is really
simpler and more basic than the real sinusoid A sin(ωt + φ) because ejωt
consists of one frequency ω while sin(ωt) really consists of two frequencies
ω and −ω. We may think of a real sinusoid as being the sum of a positive-
frequency and a negative-frequency complex sinusoid, so in that sense real
sinusoids are “twice as complicated” as complex sinusoids. Complex si-
nusoids are also nicer because they have a constant modulus. “Amplitude
envelope detectors” for complex sinusoids are trivial: just compute the
square root of the sum of the squares of the real and imaginary parts to
obtain the instantaneous peak amplitude at any time. Frequency demod-
ulators are similarly trivial: just diﬀerentiate the phase of the complex
sinusoid to obtain its instantaneous frequency. It should therefore come
as no surprise that signal processing engineers often prefer to convert real
sinusoids into complex sinusoids before processing them further.
5.3.4
The Analytic Signal and Hilbert Transform Filters
A signal which has no negative-frequency components is called an analytic
signal.6 Therefore, in continuous time, every analytic signal z(t) can be
represented as
z(t) = 1
2π
 ∞
0
Z(ω)ejωtdω
where Z(ω) is the complex coeﬃcient (setting the amplitude and phase)
of the positive-freqency complex sinusoid exp(jωt) at frequency ω.
Any sinusoid A cos(ωt+φ) in real life may be converted to a positive-
frequency complex sinusoid A exp[j(ωt+φ)] by simply generating a phase-
quadrature component A sin(ωt + φ) to serve as the “imaginary part”:
Aej(ωt+φ) = A cos(ωt + φ) + jA sin(ωt + φ)
6In complex variables, “analytic” just means diﬀerentiable of all orders. Therefore,
one would expect an “analytic signal” to simply be any signal which is diﬀerentiable of
all orders at any point in time, i.e., one that admits a fully valid Taylor expansion about
any point in time. However, all bandimited signals (being sums of ﬁnite-frequency
sinusoids) are analytic in the complex-variables sense. Therefore, the signal processing
term “analytic signal” is somewhat of a misnomer. It is included in this chapter only
because it is a commonly used term in engineering practice.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 82
5.3. COMPLEX SINUSOIDS
The phase-quadrature component can be generated from the in-phase
component by a simple quarter-cycle time shift.7
For more complicated signals which are expressible as a sum of many
sinusoids, a ﬁlter can be constructed which shifts each sinusoidal com-
ponent by a quarter cycle. This is called a Hilbert transform ﬁlter. Let
Ht{x} denote the output at time t of the Hilbert-transform ﬁlter applied
to the signal x(·). Ideally, this ﬁlter has magnitude 1 at all frequencies and
introduces a phase shift of −π/2 at each positive frequency and +π/2 at
each negative frequency. When a real signal x(t) and its Hilbert transform
y(t) = Ht{x} are used to form a new complex signal z(t) = x(t) + jy(t),
the signal z(t) is the (complex) analytic signal corresponding to the real
signal x(t). In other words, for any real signal x(t), the corresponding
analytic signal z(t) = x(t) + jHt{x} has the property that all “negative
frequencies” of x(t) have been “ﬁltered out.”
To see how this works, recall that these phase shifts can be impressed
on a complex sinusoid by multiplying it by exp(±jπ/2) = ±j.
Con-
sider the positive and negative frequency components at the particular
frequency ω0:
x+(t)
∆=
ejω0t
x−(t)
∆=
e−jω0t
Now let’s apply a −90 degrees phase shift to the positive-frequency com-
ponent, and a +90 degrees phase shift to the negative-frequency compo-
nent:
y+(t)
=
e−jπ/2ejω0t = −jejω0t
y−(t)
=
ejπ/2e−jω0t = je−jω0t
Adding them together gives
z+(t)
∆=
x+(t) + jy+(t) = ejω0t −j2ejω0t = 2ejω0t
z−(t)
∆=
x−(t) + jy−(t) = e−jω0t + j2e−jω0t = 0
7This operation is actually used in some real-world AM and FM radio receivers
(particularly in digital radio receivers). The signal comes in centered about a high
“carrier frequency” (such as 101 MHz for radio station FM 101), so it looks very much
like a sinusoid at frequency 101 MHz.
(The frequency modulation only varies the
carrier frequency in a relatively tiny interval about 101 MHz. The total FM bandwidth
including all the FM “sidebands” is about 100 kHz. AM bands are only 10kHz wide.)
By delaying the signal by 1/4 cycle, a good approximation to the imaginary part of
the analytic signal is created, and its instantaneous amplitude and frequency are then
simple to compute from the analytic signal.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 5. SINUSOIDS AND EXPONENTIALS
Page 83
and sure enough, the negative frequency component is ﬁltered out. (There
is also a gain of 2 at positive frequencies which we can remove by deﬁning
the Hilbert transform ﬁlter to have magnitude 1/2 at all frequencies.)
For a concrete example, let’s start with the real sinusoid
x(t) = 2 cos(ω0t) = exp(jω0t) + exp(−jω0t).
Applying the ideal phase shifts, the Hilbert transform is
y(t)
=
exp(jω0t −jπ/2) + exp(−jω0t + jπ/2)
=
−j exp(jω0t) + j exp(−jω0t) = 2 sin(ω0t)
The analytic signal is then
z(t) = x(t) + jy(t) = 2 cos(ω0t) + j2 sin(ω0t) = 2ejω0t,
by Euler’s identity. Thus, in the sum x(t) + jy(t), the negative-frequency
components of x(t) and jy(t) cancel out in the sum, leaving only the
positive-frequency component. This happens for any real signal x(t), not
just for sinusoids as in our example.
Figure 5.8 illustrates what is going on in the frequency domain. While
we haven’t “had” Fourier analysis yet, it should come as no surprise
that the spectrum of a complex sinusoid exp(jω0t) will consist of a sin-
gle “spike” at the frequency ω = ω0 and zero at all other frequen-
cies.
(Just follow things intuitively for now, and revisit Fig. 5.8 after
we’ve developed the Fourier theorems.) From the identity 2 cos(ω0t) =
exp(jω0t)+exp(−jω0t), we see that the spectrum contains unit-amplitude
“spikes” at ω = ω0 and ω = −ω0. Similarly, the identity 2 sin(ω0t) =
[exp(jω0t) −exp(−jω0t)]/j = −j exp(jω0t) + j exp(−jω0t) says that we
have an amplitude −j spike at ω = ω0 and an amplitude +j spike at ω =
−ω0. Multiplying y(t) by j results in j sin(ω0t) = exp(jω0t)−exp(−jω0t)
which is a unit-amplitude “up spike” at ω = ω0 and a unit “down spike”
at ω = −ω0. Finally, adding together the ﬁrst and third plots, corre-
sponding to z(t) = x(t) + jy(t), we see that the two up-spikes add in
phase to give an amplitude 2 up-spike (which is 2 exp(jω0t)), and the
negative-frequency up-spike in the cosine is canceled by the down-spike
in j times sine at frequency −ω0. This sequence of operations illustrates
how the negative-frequency component exp(−jω0t) gets ﬁltered out by
the addition of 2 cos(ω0t) and j2 sin(ω0t).
As a ﬁnal example (and application), let x(t) = A(t) cos(ωt), where
A(t) is a slowly varying amplitude envelope (slow compared with ω). This
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 84
5.3. COMPLEX SINUSOIDS
Re{X(ω)}
ω
0
x(t) = cos(ω0 t)
Im{X(ω)}
ω0
−ω0
Re{Y(ω)}
ω
0
y(t) = sin(ω0 t)
Im{Y(ω)}
ω0
−ω0
Re{ j Y(ω)}
ω
0
j y(t) = j sin(ω0 t)
Im{ j Y(ω)}
ω0
−ω0
Re{Z(ω)}
ω
0
z(t) = x(t) + j y(t) = cos(ω0 t) +  j sin(ω0 t)
Im{Z(ω)}
ω0
a)
b)
c)
d)
(analytic)
Figure 5.8: Creation of the analytic signal z(t) = ejω0t from the
real sinusoid x(t) = cos(ω0t) and the derived phase-quadrature si-
nusoid y(t) = sin(ω0t), viewed in the frequency domain. a) Spec-
trum of x. b) Spectrum of y. c) Spectrum of jy. d) Spectrum of
z = x + jy.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 5. SINUSOIDS AND EXPONENTIALS
Page 85
is an example of amplitude modulation applied to a sinusoid at “carrier
frequency” ω (which is where you tune your AM radio).
The Hilbert
transform is almost exactly y(t) ≈A(t) sin(ωt)8, and the analytic signal
is z(t) ≈A(t)ejωt. Note that AM demodulation9 is now nothing more
than the absolute value. I.e., A(t) = |z(t)|. Due to this simplicity, Hilbert
transforms are sometimes used in making amplitude envelope followers
for narrowband signals (i.e., signals with all energy centered about a
single “carrier” frequency).
AM demodulation is one application of a
narrowband envelope follower.
5.3.5
Generalized Complex Sinusoids
We have deﬁned sinusoids and extended the deﬁnition to include complex
sinusoids.
We now extend one more step by allowing for exponential
amplitude envelopes:
y(t) ∆= Aest
where A and s are complex, and further deﬁned as
A
=
Aejφ
s
=
σ + jω
When σ = 0, we obtain
y(t) ∆= Aejωt = Aejφejωt = Aej(ωt+φ)
which is the complex sinusoid at amplitude A, radian frequency ω, and
phase φ.
More generally, we have
y(t)
∆=
Aest
∆=
Aejφe(σ+jω)t
=
Ae(σ+jω)t+jφ
=
Aeσtej(ωt+φ)
=
Aeσt [cos(ωt + φ) + j sin(ωt + φ)]
8If A(t) were constant, this would be exact.
9Demodulation is the process of recovering the modulation signal. For amplitude
modulation (AM), the modulated signal is of the form y(t) = A(t) cos(ωct), where ωc is
the “carrier frequency”, A(t) = [1+µx(t)] ≥0 is the amplitude envelope (modulation),
x(t) is the modulation signal we wish to recover (the audio signal being broadcast in
the case of AM radio), and µ is the modulation index for AM.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 86
5.3. COMPLEX SINUSOIDS
Deﬁning τ = −1/σ, we see that the generalized complex sinusoid is
just the complex sinusoid we had before with an exponential envelope:
re {y(t)}
=
Ae−t/τ cos(ωt + φ)
im {y(t)}
=
Ae−t/τ sin(ωt + φ)
5.3.6
Sampled Sinusoids
In discrete-time audio processing, such as we must do on a computer, we
work with samples of continuous-time signals. Let fs denote the sampling
rate in Hz. For audio, we typically have fs > 40 kHz, since the audio
band nominally extends to 20 kHz. For compact discs (CDs), fs = 44.1
kHz (or very close to that—I once saw Sony device using a sampling rate
of 44, 025 Hz), while for digital audio tape (DAT), fs = 48 kHz.
Let T ∆= 1/fs denote the sampling period in seconds. Then to convert
from continuous to discrete time, we replace t by nT, where n is an integer
interpreted as the sample number.
The sampled generalized complex sinusoid (which includes all other
cases) is then
y(nT)
=
AesnT = A

esT n
∆=
Aejφe(σ+jω)nT
=
AeσnT [cos(ωnT + φ) + j sin(ωnT + φ)]
=
A

eσT n [cos(ωnT + φ) + j sin(ωnT + φ)]
5.3.7
Powers of z
Choose any two complex numbers z0 and z1, and form the sequence
x(n) ∆= z0zn
1 ,
n = 0, 1, 2, 3, . . .
(5.1)
What are the properties of this signal? Expressing the two complex num-
bers as
z0
=
Aejφ
z1
=
esT = e(σ+jω)T
we see that the signal x(n) is always a discrete-time generalized complex
sinusoid, i.e., an exponentially enveloped complex sinusoid.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 5. SINUSOIDS AND EXPONENTIALS
Page 87
Figure 5.9: Exponentially decaying complex sinusoid and its pro-
jections.
Figure 5.9 shows a plot of a generalized (exponentially decaying) com-
plex sinusoid versus time.
Note that the left projection (onto the z plane) is a decaying spiral, the
lower projection (real-part vs. time) is an exponentially decaying cosine,
and the upper projection (imaginary-part vs. time) is an exponentially
enveloped sine wave.
5.3.8
Phasor & Carrier Components of Complex Sinusoids
If we restrict z1 in Eq. (5.1) to have unit modulus, then we obtain a
discrete-time complex sinusoid.
x(n) ∆= z0zn
1 =

Aejφ
ejωnT = Aej(ωnT+φ),
n = 0, 1, 2, 3, . . .
(5.2)
where we have deﬁned
z0
∆=
Aejφ,
and
z1
∆=
ejωT .
It is common terminology to call z0 = Aejφ the sinusoidal phasor, and
zn
1 = ejωnT the sinusoidal carrier.
For a real sinusoid
xr(n) ∆= A cos(ωnT + φ)
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 88
5.3. COMPLEX SINUSOIDS
the phasor is again deﬁned as z0 = Aejφ and the carrier is zn
1 = ejωnT .
However, in this case, the real sinusoid is recovered from its complex
sinusoid counterpart by taking the real part:
xr(n) = re {z0zn
1 }
The phasor magnitude |z0| = A is the amplitude of the sinusoid. The
phasor angle ̸ z0 = φ is the phase of the sinusoid.
When working with complex sinusoids, as in Eq. (5.2), the phasor
representation of a sinusoid can be thought of as simply the complex
amplitude of the sinusoid Aejφ.
I.e., it is the complex constant that
multiplies the carrier term ejωnT .
Why Phasors are Important
LTI systems perform only four operations on a signal: copying, scaling,
delaying, and adding. As a result, each output is always a linear combi-
nation of delayed copies of the input signal(s). (A linear combination is
simply a weighted sum.) In any linear combination of delayed copies of
a complex sinusoid
y(n) =
N

i=1
gix(n −di)
where gi is a weighting factor, di is the ith delay, and
x(n) = ejωnT
is a complex sinusoid, the “carrier term” ejωnT can be “factored out” of
the linear combination:
y(n)
=
N

i=1
giej[ω(n−di)T ] =
N

i=1
giejωnT e−jωdiT
=
ejωnT
N

i=1
gie−jωdiT = x(n)
N

i=1
gie−jωdiT
The operation of the LTI system on a complex sinusoids is thus reduced to
a calculation involving only phasors, which are simply complex numbers.
Since every signal can be expressed as a linear combination of complex
sinusoids, this analysis can be applied to any signal by expanding the
signal into its weighted sum of complex sinusoids (i.e., by expressing it as
an inverse Fourier transform).
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 5. SINUSOIDS AND EXPONENTIALS
Page 89
5.3.9
Why Generalized Complex Sinusoids are Important
As a preview of things to come, note that one signal y(·)10 is projected
onto another signal x(·) using an inner product. The inner product ⟨y, x⟩
computes the coeﬃcient of projection11 of y onto x. If x(n) = ejωknT , n =
0, 1, 2, . . . , N −1 (a sampled, unit-amplitude, zero-phase, complex sinu-
soid), then the inner product computes the Discrete Fourier Transform
(DFT), provided the frequencies are chosen to be ωk = 2πkfs/N. For the
DFT, the inner product is speciﬁcally
⟨y, x⟩∆=
N−1

n=0
y(n)x(n) =
N−1

n=0
y(n)e−j2πnk/N ∆= DFTk(y) ∆= Y (ωk)
Another commonly used case is the Discrete Time Fourier Transform
(DTFT) which is like the DFT, except that the transform accepts an
inﬁnite number of samples instead of only N. In this case, frequency is
continuous, and
⟨y, x⟩=
∞

n=0
y(n)e−jωnT ∆= DTFTω(y)
The DTFT is what you get in the limit as the number of samples in
the DFT approaches inﬁnity. The lower limit of summation remains zero
because we are assuming all signals are zero for negative time.
This
means we are working with unilateral Fourier transforms. There are also
corresponding bilateral transforms for which the lower summation limit
is −∞.
If, more generally, x(n) = zn (a sampled complex sinusoid with expo-
nential growth or decay), then the inner product becomes
⟨y, x⟩=
∞

n=0
y(n)z−n
and this is the deﬁnition of the z transform. It is a generalization of the
DTFT: The DTFT equals the z transform evaluated on the unit circle in
the z plane. In principle, the z transform can also be recovered from the
10The notation y(n) denotes a single sample of the signal y at sample n, while the
notation y(·) or simply y denotes the entire signal for all time.
11The coeﬃcient of projection of a signal y onto another signal x can be thought of
as a measure of how much of x is present in y. We will consider this topic in some
detail later on.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 90
5.3. COMPLEX SINUSOIDS
DTFT by means of “analytic continuation” from the unit circle to the
entire z plane (subject to mathematical disclaimers which are unnecessary
in practical applications since they are always ﬁnite).
Why have a z tranform when it seems to contain no more information
than the DTFT? It is useful to generalize from the unit circle (where
the DFT and DTFT live) to the entire complex plane (the z transform’s
domain) for a number of reasons. First, it allows transformation of grow-
ing functions of time such as unstable exponentials; the only limitation
on growth is that it cannot be faster than exponential. Secondly, the z
transform has a deeper algebraic structure over the complex plane as a
whole than it does only over the unit circle. For example, the z trans-
form of any ﬁnite signal is simply a polynomial in z. As such, it can be
fully characterized (up to a constant scale factor) by its zeros in the z
plane. Similarly, the z transform of an exponential can be characterized
by a single point of the transform (the point which generates the expo-
nential); since the z transform goes to inﬁnity at that point, it is called a
pole of the transform. More generally, the z transform of any generalized
complex sinusoid is simply a pole located at the point which generates the
sinusoid. Poles and zeros are used extensively in the analysis of recursive
digital ﬁlters. On the most general level, every ﬁnite-order, linear, time-
invariant, discrete-time system is fully speciﬁed (up to a scale factor) by
its poles and zeros in the z plane.
In the continuous-time case, we have the Fourier transform which
projects y onto the continuous-time sinusoids deﬁned by x(t) = ejωt, and
the appropriate inner product is
⟨y, x⟩=
 ∞
0
y(t)e−jωtdt ∆= Y (ω)
Finally, the Laplace transform is the continuous-time counterpart of
the z transform, and it projects signals onto exponentially growing or
decaying complex sinusoids:
⟨y, x⟩=
 ∞
0
y(t)e−stdt ∆= Y (s)
The Fourier transform equals the Laplace transform evaluated along the
“jω axis” in the s plane, i.e., along the line s = jω, for which σ = 0.
Also, the Laplace transform is obtainable from the Fourier transform via
analytic continuation. The usefulness of the Laplace transform relative
to the Fourier transform is exactly analogous to that of the z transform
outlined above.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 5. SINUSOIDS AND EXPONENTIALS
Page 91
5.3.10
Comparing Analog and Digital Complex Planes
In signal processing, it is customary to use s as the Laplace transform
variable for continuous-time analysis, and z as the z-transform variable
for discrete-time analysis. In other words, for continuous-time systems,
the frequency domain is the “s plane”, while for discrete-time systems,
the frequency domain is the “z plane.” However, both are simply complex
planes.
Figure 5.10 illustrates the various sinusoids est represented by points
in the s plane.
The frequency axis is s = jω, called the “jω axis,”
and points along it correspond to complex sinusoids, with dc at s =
0 (e0t = 1).
The upper-half plane corresponds to positive frequencies
(counterclockwise circular or corkscrew motion) while the lower-half plane
corresponds to negative frequencies (clockwise motion). In the left-half
plane we have decaying (stable) exponential envelopes, while in the right-
half plane we have growing (unstable) exponential envelopes. Along the
real axis (s = σ), we have pure exponentials. Every point in the s plane
can be said to correspond to some generalized complex sinusoids, x(t) =
Aest, t ≥0 with special cases being complex sinusoids Aejωt, exponentials
Aeσt, and the constant function x(t) = 1 (dc).
Figure 5.11 shows examples of various sinusoids zn = [esT ]n repre-
sented by points in the z plane. The frequency axis is the “unit circle”
z = ejωT , and points along it correspond to sampled complex sinusoids,
with dc at z = 1 (1n = [e0T ]n = 1).
As in the s plane, the upper-
half plane corresponds to positive frequencies while the lower-half plane
corresponds to negative frequencies. Inside the unit circle, we have de-
caying (stable) exponential envelopes, while outside the unit circle, we
have growing (unstable) exponential envelopes. Along the positive real
axis (z > 0, im {z} = 0), we have pure exponentials, but along the neg-
ative real axis (z < 0, im {z} = 0), we have exponentially enveloped
sampled sinusoids at frequency fs/2 (exponentially enveloped alternating
sequences). The negative real axis in the z plane is normally a place where
all signal z transforms should be zero, and all system responses should be
highly attenuated, since there should never be any energy at exactly half
the sampling rate (where amplitude and phase are ambiguously linked).
Every point in the z plane can be said to correspond to some sampled
generalized complex sinusoids x(n) = Azn = A[esT ]n, n ≥0, with special
cases being sampled complex sinusoids AejωnT, exponentials AeσnT , and
the constant function x = [1, 1, 1, . . .] (dc).
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 92
5.3. COMPLEX SINUSOIDS
s-plane
F o u r i e r  T r a n s f o r m
         D o m
a i n
Domain of Laplace transforms
Frequency
Decay
jω
σ
Figure 5.10: Generalized complex sinusoids represented by points
in the s plane.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 5. SINUSOIDS AND EXPONENTIALS
Page 93
z-plane
F o u r i e r  T r a n s f o r m
        D o m
a i n
Domain of z-transforms
Frequency
u n i t  c i r c l e
branch
cut:
Decay
crossing this line in
frequency will cause
aliasing
(Nyquist frequency)
Figure 5.11: Generalized complex sinusoids represented by points
in the z plane.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 94
5.4. MATHEMATICA FOR SELECTED PLOTS
In summary, the exponentially enveloped (“generalized”) complex si-
nusoid is the fundamental signal upon which other signals are “projected”
in order to compute a Laplace transform in the continuous-time case, or
a z transform in the discrete-time case. As a special case, if the expo-
nential envelope is eliminated (set to 1), leaving only a complex sinusoid,
then the projection reduces to the Fourier transform in the continuous-
time case, and either the DFT (ﬁnite length) or DTFT (inﬁnite length)
in the discrete-time case. Finally, there are still other variations, such
as short-time Fourier transforms (STFT) and wavelet transforms, which
utilize further modiﬁcations such as projecting onto windowed complex
sinusoids. Music 42012 delves into these topics.
5.4
Mathematica for Selected Plots
The Mathematica code for producing Fig. 5.1 (minus the annotations
which were done using NeXT Draw and EquationBuilder from Lighthouse
Design) is
Plot[10 Sin[2 Pi 2.5 t + Pi/4],{t,0,1},
PlotLabel->"10 Sin[2 Pi 2.5 t + Pi/4]",
PlotPoints->500,
AxesLabel->{" Sec", "Amp."}];
The Mathematica code for Fig. 5.2 is
Show[
Plot[Sin[2 Pi 2.5 t],{t,-0.1,1.1},
PlotPoints->500,
AxesLabel->{‘‘ Time (Sec)’’, ‘‘Amplitude’’}],
Plot[Cos[2 Pi 2.5 t],{t,-0.1,1.1},
PlotPoints->500,
PlotStyle->Dashing[{0.01,0.01}]
];
For the complex sinusoid plots (Fig. 5.7 and Fig. 5.9), see the Mathe-
matica notebook ComplexSinusoid.nb13 on Craig Sapp’s web page14 of
Mathematica notebooks. (The package SCMTheory.m15 is required.)
12http://www-ccrma.stanford.edu/CCRMA/Courses/420/Welcome.html
13http://www-ccrma.stanford.edu/CCRMA/Software/SCMP/SCMTheory/ComplexSinusoid.nb.gz
14http://www-ccrma/CCRMA/Software/SCMP/
15http://www-ccrma.stanford.edu/CCRMA/Software/SCMP/SCMTheory/SCMTheory.m
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 5. SINUSOIDS AND EXPONENTIALS
Page 95
5.5
Acknowledgement
Many thanks to Craig Stuart Sapp <craig@ccrma.stanford.edu> for con-
tributing the Mathematica magic for Figures 5.7, 5.9, 5.10, and 5.11.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 96
5.5. ACKNOWLEDGEMENT
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Chapter 6
Geometric Signal Theory
This chapter provides an introduction to the elements of geometric sig-
nal theory, including vector spaces, norms, inner products, orthogonality,
projection of one signal onto another, and elementary vector space oper-
ations.
6.1
The DFT
For a length N complex sequence x(n), n = 0, 1, 2, . . . , N −1, the discrete
Fourier transform (DFT) is deﬁned by
X(ωk) ∆=
N−1

n=0
x(n)e−jωktn =
N−1

n=0
x(n)e−j2πkn/N,
k = 0, 1, 2, . . . N −1
tn
∆=
nT = nth sampling instant (sec)
ωk
∆=
kΩ= kth frequency sample (rad/sec)
T
∆=
1/fs = time sampling interval (sec)
Ω
∆=
2πfs/N = frequency sampling interval (sec)
We are now in a position to have a full understanding of the transform
kernel:
e−jωktn = cos(ωktn) −j sin(ωktn)
The kernel consists of samples of a complex sinusoid at N discrete fre-
quencies ωk uniformly spaced between 0 and the sampling rate ωs
∆= 2πfs.
97

Page 98
6.2. SIGNALS AS VECTORS
All that remains is to understand the purpose and function of the summa-
tion over n of the pointwise product of x(n) times each complex sinusoid.
We will learn that this can be interpreted as an inner product opera-
tion which computes the coeﬃcient of projection of the signal x onto the
complex sinusoid cos(ωktn) + j sin(ωktn). As such, X(ωk), the DFT at
frequency ωk, is a measure of the amplitude and phase of the complex
sinusoid at that frequency which is present in the input signal x. This
is the basic function of all transform summations (in discrete time) and
integrals (in continuous time) and their kernels.
6.2
Signals as Vectors
For the DFT, all signals and spectra are length N. A length N sequence
x can be denoted by x(n), n = 0, 1, 2, . . . N −1, where x(n) may be real
(x ∈RN) or complex (x ∈CN). We now wish to regard x as a vector x1
in an N dimensional vector space. That is, each sample x(n) is regarded
as a coordinate in that space.
A vector x is mathematically a single
point in N-space represented by a list of coordinates (x0, x1, x2, . . . , xN−1)
called an N-tuple. (The notation xn means the same thing as x(n).) It
can be interpreted geometrically as an arrow in N-space from the origin
0 ∆= (0, 0, . . . , 0) to the point x ∆= (x0, x1, x2, . . . , xN−1).
We deﬁne the following as equivalent:
x ∆= x ∆= x(·) ∆= (x0, x1, . . . , xN−1) ∆= [x0, x1, . . . , xN−1] ∆= [x0 x1 · · · xN−1]
where xn
∆= x(n) is the nth sample of the signal (vector) x. From now
on, unless speciﬁcally mentioned otherwise, all signals are length N.
An Example Vector View: N = 2
Consider the example two-sample signal x = (2, 3) graphed in Fig. 6.1.
Under the geometric interpretation of a length N signal, each sample
is a coordinate in the N dimensional space. Signals which are only two
samples long are not terribly interesting to hear, but they are easy to plot
geometrically.
1We’ll use an underline to emphasize the vector interpretation, but there is no
diﬀerence between x and x. For purposes of this course, a signal is the same thing as
a vector.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 6. GEOMETRIC SIGNAL THEORY
Page 99
1
2
3
4
1
2
3
4
x = (2,3)
Figure 6.1: A length 2 signal x = (2, 3) plotted as a vector in 2D
space.
6.3
Vector Addition
Given two vectors in RN, say x = (x0, x1, . . . , xN−1) and y = (y0, y1, . . . , yN−1),
the vector sum is deﬁned by elementwise addition. If we denote the sum
by w ∆= x + y, then we have w(n) = x(n) + y(n) for n = 0, 1, 2, . . . , N −1.
The vector diagram for the sum of two vectors can be found using
the parallelogram rule, as shown in Fig. 6.2 for N = 2, x = (2, 3), and
y = (4, 1).
5
6
7
8
1
2
3
4
x = (2,3)
y = (4,1)
x+y = (6,4)
1
2
3
4
0
Figure 6.2: Geometric interpretation of a length 2 vector sum.
Also shown are the lighter construction lines which complete the par-
allelogram started by x and y, indicating where the endpoint of the sum
x + y lies.
Since it is a parallelogram, the two construction lines are
congruent to the vectors x and y. As a result, the vector sum is often
expressed as a triangle by translating the origin of one member of the
sum to the tip of the other, as shown in Fig. 6.3.
In the ﬁgure, x was translated to the tip of y. It is equally valid to
translate y to the tip of x, because vector addition is commutative, i.e.,
x + y = y + x.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 100
6.4.
VECTOR SUBTRACTION
5
6
7
8
1
2
3
4
x = (2,3)  [translated]
y = (4,1)
x+y = (6,4)
1
2
3
4
0
Figure 6.3: Vector sum with translation of one vector to the tip
of the other.
6.4
Vector Subtraction
Figure 6.7 illustrates the vector diﬀerence w = x −y between x = (2, 3)
and y = (4, 1). From the coordinates, we compute w = x −y = (−2, 2).
1
2
3
4
x = (2,3)
y = (4,1)
w = x-y = (-2,2)
w translated
-3
-2
-1
0
1
2
3
4
Figure 6.4: Geometric interpretation a diﬀerence vector.
Note that the diﬀerence vector w may be drawn from the tip of y
to the tip of x rather than from the origin to the point (−2, 2); this is
a customary practice which emphasizes relationships among vectors, but
the translation in the plot has no eﬀect on the mathematical deﬁnition
or properties of the vector. Subtraction, however, is not commutative.
To ascertain the proper orientation of the diﬀerence vector w = x−y,
rewrite its deﬁnition as x = y + w, and then it is clear that the vector
x should be the sum of vectors y and w, hence the arrowhead is on the
correct endpoint.
6.5
Signal Metrics
This section deﬁnes some useful functions of signals.
The mean of a signal x (more precisely the “sample mean”) is deﬁned
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 6. GEOMETRIC SIGNAL THEORY
Page 101
as its average value:
µx
∆= 1
N
N−1

n=0
xn
(mean of x)
The total energy of a signal x is deﬁned the sum of squared moduli:
Ex
∆=
N−1

n=0
|xn|2
(energy of x)
Energy is the “ability to do work.” In physics, energy and work are in
units of “force times distance,” “mass times velocity squared,” or other
equivalent combinations of units. The energy of a pressure wave is the
integral over time of the squared pressure divided by the wave impedance
the wave is traveling in. The energy of a velocity wave is the integral over
time of the squared velocity times the wave impedance. In audio work, a
signal x is typically a list of pressure samples derived from a microphone
signal, or it might be samples of force from a piezoelectric transducer,
velocity from a magnetic guitar pickup, and so on. In all of these cases,
the total physical energy associated with the signal is proportional to the
sum of squared signal samples. (Physical connections in signal processing
are explored more deeply in Music 4212.)
The average power of a signal x is deﬁned the energy per sample:
Px
∆= Ex
N = 1
N
N−1

n=0
|xn|2
(average power of x)
Another common description when x is real is the “mean square.” When
x is a complex sinusoid, i.e., x(n) = Aej(ωnT+φ), then Px = A2; in other
words, for complex sinusoids, the average power equals the instantaneous
power which is the amplitude squared.
Power is always in physical units of energy per unit time. It therefore
makes sense to deﬁne the average signal power as the total signal energy
divided by its length. We normally work with signals which are functions
of time. However, if the signal happens instead to be a function of distance
(e.g., samples of displacement along a vibrating string), then the “power”
as deﬁned here still has the interpretation of a spatial energy density.
Power, in contrast, is a temporal energy density.
2http://www-ccrma.stanford.edu/CCRMA/Courses/421/
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 102
6.5. SIGNAL METRICS
The root mean square (RMS) level of a signal x is simply √Px. How-
ever, note that in practice (especially in audio work) an RMS level may
be computed after subtracting out the mean value. Here, we call that the
variance.
The variance (more precisely the sample variance) of the signal x is
deﬁned as the power of the signal with its sample mean removed:
σ2
x
∆= 1
N
N−1

n=0
|xn −µx|2
(variance of x)
It is quick to show that, for real signals, we have
σ2
x = Px −µ2
x
which is the “mean square minus the mean squared.” We think of the
variance as the power of the non-constant signal components (i.e., every-
thing but dc). The terms “sample mean” and “sample variance” come
from the ﬁeld of statistics, particularly the theory of stochastic processes.
The ﬁeld of statistical signal processing [12] is ﬁrmly rooted in statistical
topics such as “probability,” “random variables,” “stochastic processes,”
and “time series analysis.” In this reader, we will only touch lightly on a
few elements of statistical signal processing in a self-contained way.
The norm of a signal x is deﬁned as the square root of its total energy:
∥x∥∆=

Ex =




N−1

n=0
|xn|2
(norm of x)
We think of ∥x∥as the length of x in N-space. Furthermore, ∥x −y∥is
regarded as the distance between x and y. The norm can also be thought
of as the “absolute value” or “radius” of a vector.3
Example: Going back to our simple 2D example x = [2, 3], we can
compute its norm as ∥x∥=
√
22 + 32 =
√
13. The physical interpretation
of the norm as a distance measure is shown in Fig. 6.5.
Example: Let’s also look again at the vector-sum example, redrawn
in Fig. 6.6.
The norm of the vector sum w = x + y is
∥w∥∆= ∥x + y∥∆= ∥(2, 3) + (4, 1)∥= ∥(6, 4)∥=

62 + 42 =
√
52 = 2
√
13
3You might wonder why the norm of x is not written as |x|. There would be no
problem with this since |x| is undeﬁned. However, the historically adopted notation is
instead ∥x∥.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 6. GEOMETRIC SIGNAL THEORY
Page 103
1
2
3
4
1
2
3
4
x = (2,3)
∥∥


=
+
=
x
3
1
3
2
2
2
Length =
Figure 6.5: Geometric interpretation of a signal norm in 2D.
5
6
7
8
1
2
3
4
x = (2,3)  [translated]
y = (4,1)
x+y = (6,4)
1
2
3
4
0
Length = ||
||

=
+ y
x
3
1
2
||
||

=
x
3
1
2
Length =
Length = ||
||

=
y
7
1
Figure 6.6: Length of vectors in sum.
while the norms of x and y are
√
13 and
√
17, respectively. We ﬁnd that
∥x+y∥< ∥x∥+∥y∥which is an example of the triangle inequality. (Equal-
ity occurs only when x and y are colinear, as can be seen geometrically
from studying Fig. 6.6.)
Example: Consider the vector-diﬀerence example diagrammed in
Fig. 6.7.
1
2
3
4
1
2
3
4
x = (2,3)
Length =
y = (4,1)
x-y
∥
∥

=
−y
x
2
2
Figure 6.7: Length of a diﬀerence vector.
The norm of the diﬀerence vector w = x −y is
∥w∥∆= ∥x −y∥∆= ∥(2, 3) −(4, 1)∥= ∥(−2, 2)∥=

(−2)2 + (2)2 = 2
√
2
Other Norms
Since our main norm is the square root of a sum of squares, we are using
what is called an L2 norm and we may write ∥x∥2 to emphasize this fact.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 104
6.5. SIGNAL METRICS
We could equally well have chosen a normalized L2 norm:
∥x∥˜2
∆=

Px =



 1
N
N−1

n=0
|xn|2
(normalized L2 norm of x)
which is simply the “RMS level” of x.
More generally, the Lp norm of x ∈CN is deﬁned
∥x∥p
∆=
N−1

n=0
|xn|p
1/p
The most interesting Lp norms are
• p = 1: The L1, “absolute value,” or “city block” norm.
• p = 2: The L2, “Euclidean,” “root energy,” or “least squares” norm.
• p = ∞: The L∞, “Chebyshev,” “supremum,” “minimax,” or “uni-
form” norm.
Note that the case p = ∞is a limiting case which becomes
∥x∥∞= max
0≤n<N |xn|
There are many other possible choices of norm. To qualify as a norm
on CN, a real-valued signal function f(x) must satisfy the following three
properties:
1. f(x) = 0 ⇔x = 0
2. f(x + y) ≤f(x) + f(y)
3. f(cx) = |c| f(x), ∀c ∈C
The ﬁrst property, “positivity,” says only the zero vector has norm zero.
The second property is “subadditivity” and is sometimes called the “tri-
angle inequality” for reasons which can be seen by studying Fig. 6.3. The
third property says the norm is “absolutely homogeneous” with respect
to scalar multiplication (which can be complex, in which case the phase
of the scalar has no eﬀect).
Mathematically, what we are working with so far is called a Banach
space which a normed linear vector space. To summarize, we deﬁned our
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 6. GEOMETRIC SIGNAL THEORY
Page 105
vectors as any list of N real or complex numbers which we interpret as
coordinates in the N-dimensional vector space. We also deﬁned vector
addition in the obvious way. It turns out we have to also deﬁne scalar
multiplication, that is, multiplication of a vector by a scalar which we
also take to be an element of the ﬁeld of real or complex numbers. This
is also done in the obvious way which is to multiply each coordinate of
the vector by the scalar. To have a linear vector space, it must be closed
under vector addition and scalar multiplication. That means given any
two vectors x ∈CN and y ∈CN from the vector space, and given any
two scalars c1 ∈C and c2 ∈C from the ﬁeld of scalars, then any linear
combination c1x + c2y must also be in the space. Since we have used the
ﬁeld of complex numbers C (or real numbers R) to deﬁne both our scalars
and our vector components, we have the necessary closure properties so
that any linear combination of vectors from CN lies in CN. Finally, the
deﬁnition of a norm (any norm) elevates a vector space to a Banach space.
6.6
The Inner Product
The inner product (or “dot product”) is an operation on two vectors
which produces a scalar. Adding an inner product to a Banach space
produces a Hilbert space (or “inner product space”).
There are many
examples of Hilbert spaces, but we will only need {CN, C} for this reader
(complex length N vectors and complex scalars).
The inner product between two (complex) N-vectors x and y is deﬁned
by
⟨x, y⟩∆=
N−1

n=0
x(n)y(n)
The complex conjugation of the second vector is done in order that a
norm will be induced by the inner product:
⟨x, x⟩=
N−1

n=0
x(n)x(n) =
N−1

n=0
|x(n)|2 ∆= Ex = ∥x∥2
As a result, the inner product is conjugate symmetric:
⟨y, x⟩= ⟨x, y⟩
Note that the inner product takes CN ×CN to C. That is, two length
N complex vectors are mapped to a complex scalar.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 106
6.6. THE INNER PRODUCT
Example: For N = 3 we have, in general,
⟨x, y⟩= x0y0 + x1y1 + x2y2
Let
x
=
[0, j, 1]
y
=
[1, j, j]
Then
⟨x, y⟩= 0 · 1 + j · (−j) + 1 · (−j) = 0 + 1 + (−j) = 1 −j
6.6.1
Linearity of the Inner Product
Any function f(x) of a vector x ∈CN (which we may call an operator
on CN) is said to be linear if for all x1 ∈CN and x2 ∈CN, and for all
scalars c1 and c2 in C, we have
f(c1x1 + c2x2) = c1f(x1) + c2f(x2)
A linear operator thus “commutes with mixing.”
Linearity consists of two component properties,
• additivity: f(x1 + x2) = f(x1) + f(x2), and
• homogeneity: f(c1x1) = c1f(x1).
The inner product ⟨x, y⟩is linear in its ﬁrst argument, i.e.
⟨c1x1 + c2x2, y⟩= c1 ⟨x1, y⟩+ c2 ⟨x2, y⟩
This is easy to show from the deﬁnition:
⟨c1x1 + c2x2, y⟩
∆=
N−1

n=0
[c1x1(n) + c2x2(n)] y(n)
=
N−1

n=0
c1x1(n)y(n) +
N−1

n=0
c2x2(n)y(n)
=
c1
N−1

n=0
x1(n)y(n) + c2
N−1

n=0
x2(n)y(n)
∆=
c1 ⟨x1, y⟩+ c2 ⟨x2, y⟩
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 6. GEOMETRIC SIGNAL THEORY
Page 107
The inner product is also additive in its second argument, i.e.,
⟨x, y1 + y2⟩= ⟨x, y1⟩+ ⟨x, y2⟩
but it is only conjugate homogeneous in its second argument, since
⟨x, c1y1⟩= c1 ⟨x, y1⟩̸= c1 ⟨x, y1⟩
The inner product is strictly linear in its second argument with respect
to real scalars:
⟨x, r1y1 + r2y2⟩= r1 ⟨x, y1⟩+ r2 ⟨x, y2⟩,
ri ∈R
Since the inner product is linear in both of its arguments for real
scalars, it is often called a bilinear operator in that context.
6.6.2
Norm Induced by the Inner Product
We may deﬁne a norm on x ∈CN using the inner product:
∥x∥∆=

⟨x, x⟩
It is straightforward to show that properties 1 and 3 of a norm hold.
Property 2 follows easily from the Schwarz Inequality which is derived in
the following subsection. Alternatively, we can simply observe that the
inner product induces the well known L2 norm on CN.
6.6.3
Cauchy-Schwarz Inequality
The Cauchy-Schwarz Inequality (or “Schwarz Inequality”) states that for
all x ∈CN and y ∈CN, we have

x, y
  ≤∥x∥· ∥y∥
with equality if and only if x = cy for some scalar c.
We can quickly show this for real vectors x ∈RN, y ∈RN, as follows:
If either x or y is zero, the inequality holds (as equality). Assuming both
are nonzero, let’s scale them to unit-length by deﬁning the normalized
vectors ˜x ∆= x/∥x∥, ˜y ∆= y/∥y∥, which are unit-length vectors lying on the
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 108
6.6. THE INNER PRODUCT
“unit ball” in RN (a hypersphere of radius 1). We have
0 ≤∥˜x −˜y∥2
=
⟨˜x −˜y, ˜x −˜y⟩
=
⟨˜x, ˜x⟩−⟨˜x, ˜y⟩−⟨˜y, ˜x⟩+ ⟨˜y, ˜y⟩
=
∥˜x∥2 −

⟨˜x, ˜y⟩+ ⟨˜x, ˜y⟩
	
+ ∥˜y∥2
=
2 −2re {⟨˜x, ˜y⟩}
=
2 −2 ⟨˜x, ˜y⟩
which implies
⟨˜x, ˜y⟩≤1
or, removing the normalization,
re

x, y
 
≤∥x∥· ∥y∥
The same derivation holds if x is replaced by −x yielding
−re

x, y
 
≤∥x∥· ∥y∥
The last two equations imply

x, y
  ≤∥x∥· ∥y∥
The complex case can be shown by rotating the components of x and y
such that re {⟨˜x, ˜y⟩} becomes equal to |⟨˜x, ˜y⟩|.
6.6.4
Triangle Inequality
The triangle inequality states that the length of any side of a triangle is
less than or equal to the sum of the lengths of the other two sides, with
equality occurring only when the triangle degenerates to a line. In CN,
this becomes
∥x + y∥≤∥x∥+ ∥y∥
We can show this quickly using the Schwarz Inequality.
∥x + y∥2
=

x + y, x + y
 
=
∥x∥2 + 2re

x, y
 
+ ∥y∥2
≤
∥x∥2 + 2

x, y
  + ∥y∥2
≤
∥x∥2 + 2∥x∥· ∥y∥+ ∥y∥2
=

∥x∥+ ∥y∥
2
=⇒
∥x + y∥
≤
∥x∥+ ∥y∥
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 6. GEOMETRIC SIGNAL THEORY
Page 109
6.6.5
Triangle Diﬀerence Inequality
A useful variation on the triangle inequality is that the length of any side
of a triangle is greater than the absolute diﬀerence of the lengths of the
other two sides:
∥x −y∥≥
∥x∥−∥y∥

Proof:
∥x −y∥2
=
∥x∥2 −2re

x, y
 
+ ∥y∥2
≥
∥x∥2 −2

x, y
  + ∥y∥2
≥
∥x∥2 −2∥x∥· ∥y∥+ ∥y∥2
=

∥x∥−∥y∥
2
=⇒
∥x −y∥
≥
∥x∥−∥y∥

6.6.6
Vector Cosine
The Cauchy-Schwarz Inequality can be written

x, y
 
∥x∥· ∥y∥≤1
In the case of real vectors x, y, we can always ﬁnd a real number θ which
satisﬁes
cos(θ) ∆=

x, y
 
∥x∥· ∥y∥
We thus interpret θ as the angle between two vectors in RN. In CN we
can similarly deﬁne |cos(θ)|.
6.6.7
Orthogonality
The vectors (signals) x and y are said to be orthogonal if ⟨x, y⟩= 0,
denoted x ⊥y. That is to say
x ⊥y ⇔⟨x, y⟩= 0.
Note that if x and y are real and orthogonal, the cosine of the angle
between them is zero. In plane geometry (N = 2), the angle between
two perpendicular lines is π/2, and cos(π/2) = 0, as expected.
More
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 110
6.6. THE INNER PRODUCT
generally, orthogonality corresponds to the fact that two vectors in N-
space intersect at a right angle and are thus perpendicular geometrically.
Example (N = 2):
Let x = [1, 1] and y = [1, −1], as shown in Fig. 6.8.
1
2
3
1
2
x = (1,1)
-1
y = (1,-1)
Figure 6.8: Example of two orthogonal vectors for N = 2.
The inner product is ⟨x, y⟩= 1 · 1 + 1 · (−1) = 0. This shows that the
vectors are orthogonal. As marked in the ﬁgure, the lines intersect at a
right angle and are therefore perpendicular.
6.6.8
The Pythagorean Theorem in N-Space
In 2D, the Pythagorean Theorem says that when x and y are orthogonal,
as in Fig. 6.8, (i.e., when the triangle formed by x, y, and x + y, with y
translated to the tip of x, is a right triangle), then we have
∥x + y∥2 = ∥x∥2 + ∥y∥2.
This relationship generalizes to N dimensions, as we can easily show:
∥x + y∥2
=
⟨x + y, x + y⟩
=
⟨x, x⟩+ ⟨x, y⟩+ ⟨y, x⟩+ ⟨y, y⟩
=
∥x∥2 + ⟨x, y⟩+ ⟨x, y⟩+ ∥y∥2
=
∥x∥2 + ∥y∥2 + 2re {⟨x, y⟩}
If x ⊥y, then ⟨x, y⟩= 0 and the Pythagorean Theorem ∥x + y∥2 =
∥x∥2 + ∥y∥2 holds in N dimensions. If, on the other hand, we assume the
Pythagorean Theorem holds, then since all norms are positive unless x
or y is zero, we must have ⟨x, y⟩= 0. Finally, if x or y is zero, the result
holds trivially.
Note that we also have an alternate version of the Pythagorean the-
orem:
∥x −y∥2 = ∥x∥2 + ∥y∥2
⇐⇒
x ⊥y.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 6. GEOMETRIC SIGNAL THEORY
Page 111
6.6.9
Projection
The orthogonal projection (or simply “projection”) of y ∈CN onto x ∈
CN is deﬁned by
Px(y) ∆=

y, x
 
∥x∥2 x
The complex scalar

x, y
 
/∥x∥2 is called the coeﬃcient of projection.
When projecting y onto a unit length vector x, the coeﬃcient of projection
is simply the inner product of y with x.
Motivation: The basic idea of orthogonal projection of y onto x is
to “drop a perpendicular” from y onto x to deﬁne a new vector along x
which we call the “projection” of y onto x. This is illustrated for N = 2
in Fig. 6.9 for x = [4, 1] and y = [2, 3], in which case
Px(y) ∆=

y, x
 
∥x∥2 x = (2 · 4 + 3 · 1)
42 + 12
x = 11
17x =
!44
17, 11
17
"
5
6
7
8
1
2
3
4
y = (2,3)
x = (4,1)
1
2
3
4
0
( )
∥∥
(
)
x
x
x
y
x
y
Px
=
=
>
<
=
7
1
1
1
,
7
1
4
4
7
1
1
1
,
2
Figure 6.9: Projection of y onto x in 2D space.
Derivation: (1) Since any projection onto x must lie along the line
colinear with x, write the projection as Px(y) = αx. (2) Since by deﬁni-
tion the projection is orthogonal to x, we must have
(y −αx)
⊥
x
⇔

y −αx, x
 
=
0
⇔

y, x
 
=
α ⟨x, x⟩
⇔α
=

y, x
 
⟨x, x⟩=

y, x
 
∥x∥2
6.7
Signal Reconstruction from Projections
We now know how to project a signal onto other signals. We now need
to learn how to reconstruct a signal x ∈CN from its projections onto N
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 112
6.7. SIGNAL RECONSTRUCTION FROM PROJECTIONS
diﬀerent vectors sk, k = 0, 1, 2, . . . , N −1. This will give us the inverse
DFT operation (or the inverse of whatever transform we are working
with).
As a simple example, consider the projection of a signal x ∈CN onto
the rectilinear coordinate axes of CN. The coordinates of the projection
onto the 0th coordinate axis are simply (x0, 0, . . . , 0).
The projection
along coordinate axis 1 has coordinates (0, x1, 0, . . . , 0), and so on. The
original signal x is then clearly the vector sum of its projections onto the
coordinate axes:
x = (x0, . . . , xN−1) = (x0, 0, . . . , 0) + (0, x1, 0, . . . , 0) + · · · (0, . . . , 0, xN−1)
To make sure the previous paragraph is understood, let’s look at the
details for the case N = 2. We want to project an arbitrary two-sample
signal x = (x0, x1) onto the coordinate axes in 2D. A coordinate axis can
be represented by any nonzero vector along its length. The horizontal
axis can be represented by any vector of the form (α, 0) while the vertical
axis can be represented by any vector of the form (0, β). For maximum
simplicity, let’s choose the positive unit-length representatives:
e0
∆=
(1, 0)
e1
∆=
(0, 1)
The projection of x onto e0 is by deﬁnition
Pe0(x) ∆= ⟨x, e0⟩
∥e0∥2 e0 = ⟨x, e0⟩e0 = ⟨[x0, x1], [1, 0]⟩e0 = (x0·1+x1·0)e0 = x0e0 = [x0, 0]
Similarly, the projection of x onto e1 is
Pe1(x) ∆= ⟨x, e1⟩
∥e1∥2 e1 = ⟨x, e1⟩e1 = ⟨[x0, x1], [0, 1]⟩e1 = (x0·0+x1·1)e1 = x1e1 = [0, x1]
The reconstruction of x from its projections onto the coordinate axes is
then the vector sum of the projections:
x = Pe0(x) + Pe1(x) = x0e0 + x1e1
∆= x0(1, 0) + x1(0, 1) = (x0, x1)
The projection of a vector onto its coordinate axes is in some sense
trivial because the very meaning of the coordinates is that they are scalars
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 6. GEOMETRIC SIGNAL THEORY
Page 113
xn to be applied to the coordinate vectors en in order to form an arbitrary
vector x ∈CN as a linear combination of the coordinate vectors:
x ∆= x0e0 + x1e1 + · · · + xN−1eN−1
Note that the coordinate vectors are orthogonal.
Since they are also
unit length, ∥en∥= 1, we say that the coordinate vectors {en}N−1
n=0 are
orthonormal.
What’s more interesting is when we project a signal x onto a set of
vectors other than the coordinate set. This can be viewed as a change
of coordinates in CN. In the case of the DFT, the new vectors will be
chosen to be sampled complex sinusoids.
6.7.1
An Example of Changing Coordinates in 2D
As a simple example, let’s pick the following pair of new coordinate vec-
tors in 2D
s0
∆=
[1, 1]
s1
∆=
[1, −1]
These happen to be the DFT sinusoids for N = 2 having frequencies
f0 = 0 (“dc”) and f1 = fs/2 (half the sampling rate). (The sampled
complex sinusoids of the DFT reduce to real numbers only for N = 1 and
N = 2.) We already showed in an earlier example that these vectors are
orthogonal. However, they are not orthonormal since the norm is
√
2 in
each case. Let’s try projecting x onto these vectors and seeing if we can
reconstruct by summing the projections.
The projection of x onto s0 is by deﬁnition
Ps0(x) ∆= ⟨x, s0⟩
∥s0∥2 s0 = ⟨[x0, x1], [1, 1]⟩
2
s0 = (x0 · 1 + x1 · 1)
2
s0 = x0 + x1
2
s0
Similarly, the projection of x onto s1 is
Ps1(x) ∆= ⟨x, s1⟩
∥s1∥2 s1 = ⟨[x0, x1], [1, −1]⟩
2
s1 = (x0 · 1 −x1 · 1)
2
s1 = x0 −x1
2
s1
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 114
6.7. SIGNAL RECONSTRUCTION FROM PROJECTIONS
The sum of these projections is then
Ps0(x) + Ps1(x)
=
x0 + x1
2
s0 + x0 −x1
2
s1
∆=
x0 + x1
2
(1, 1) + x0 −x1
2
(1, −1)
=
x0 + x1
2
+ x0 −x1
2
, x0 + x1
2
−x0 −x1
2

=
(x0, x1) ∆= x
It worked!
Now consider another example:
s0
∆=
[1, 1]
s1
∆=
[−1, −1]
The projections of x = [x0, x1] onto these vectors are
Ps0(x)
=
x0 + x1
2
s0
Ps1(x)
=
−x0 + x1
2
s1
The sum of the projections is
Ps0(x) + Ps1(x)
=
x0 + x1
2
s0 −x0 + x1
2
s1
∆=
x0 + x1
2
(1, 1) −x0 + x1
2
(−1, −1)
=
x0 + x1
2
, x0 + x1
2

̸= x
Something went wrong, but what? It turns out that a set of N vectors
can be used to reconstruct an arbitrary vector in CN from its projections
only if they are linearly independent. In general, a set of vectors is linearly
independent if none of them can be expressed as a linear combination of
the others in the set.
What this means intuituvely is that they must
“point in diﬀerent directions” in N space. In this example s1 = −s0 so
that they lie along the same line in N-space. As a result, they are linearly
dependent: one is a linear combination of the other.
Consider this example:
s0
∆=
[1, 1]
s1
∆=
[0, 1]
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 6. GEOMETRIC SIGNAL THEORY
Page 115
These point in diﬀerent directions, but they are not orthogonal. What
happens now? The projections are
Ps0(x)
=
x0 + x1
2
s0
Ps1(x)
=
x1s0
The sum of the projections is
Ps0(x) + Ps1(x)
=
x0 + x1
2
s0 + x1s1
∆=
x0 + x1
2
(1, 1) + x1(0, 1)
=
x0 + x1
2
, x0 + 3x1
2

̸=
x
So, even though the vectors are linearly independent, the sum of projec-
tions onto them does not reconstruct the original vector. Since the sum
of projections worked in the orthogonal case, and since orthogonality im-
plies linear independence, we might conjecture at this point that the sum
of projections onto a set of N vectors will reconstruct the original vector
only when the vector set is orthogonal, and this is true, as we will show.
It turns out that one can apply an orthogonalizing process, called
Gram-Schmidt orthogonalization to any N linearly independent vectors
in CN so as to form an orthogonal set which will always work. This will
be derived in Section 6.7.3.
Obviously, there must be at least N vectors in the set. Otherwise,
there would be too few degrees of freedom to represent an arbitrary x ∈
CN. That is, given the N coordinates {x(n)}N−1
n=0 of x (which are scale
factors relative to the coordinate vectors en in CN), we have to ﬁnd at
least N coeﬃcients of projection (which we may think of as coordinates
relative to new coordinate vectors sk).
If we compute only M < N
coeﬃcients, then we would be mapping a set of N complex numbers to
M < N numbers. Such a mapping cannot be invertible in general. It
also turns out N linearly independent vectors is always suﬃcient. The
next section will summarize the general results along these lines.
6.7.2
General Conditions
This section summarizes and extends the above derivations in a somewhat
formal manner (following portions of chapter 4 of [16]).
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 116
6.7. SIGNAL RECONSTRUCTION FROM PROJECTIONS
Deﬁnition: A set of vectors is said to form a vector space if given
any two members x and y from the set, the vectors x + y and cx are also
in the set, where c is any scalar.
Vectors deﬁned as a list of N complex numbers x = (x0, . . . , xN−1) ∈
CN, using elementwise addition and multiplication by complex scalars
c ∈C, form a vector space, where CN denotes the set of all length N
complex sequences. Similarly, real N-vectors x ∈RN and real scalars
α ∈R form a vector space.
Theorem: The set of all linear combinations of any set of vectors
from RN or CN forms a vector space.
Proof: Let the original set of vectors be denoted s0, . . . , sM−1, where
M can be any integer greater than zero. Then any member x0 of the
vector space is by deﬁnition a linear combination of them:
x0 = α0s0 + · · · + α0sM−1
From this we see that cx0 is also a linear combination of the original
vectors, and hence is in the vector space. Also, given any second vector
from the space x1 = α1s0 + · · · + α1sM−1, the sum is
x0 + x1
=
(α0s0 + · · · α0sM−1) + (α1s0 + · · · α1sM−1)
=
(α0 + α1)s0 + · · · + (α0 + α1)sM−1
which is just another linear combination of the original vectors, and hence
is in the vector space. It is clear here how the closure of vector addition
and scalar multiplication are “inherited” from the closure of real or com-
plex numbers under addition and multiplication.
Deﬁnition: If a vector space consists of the set of all linear combi-
nations of a ﬁnite set of vectors s0, . . . , sM−1, then those vectors are said
to span the space.
Example: The coordinate vectors in CN span CN since every vector
x ∈CN can be expressed as a linear combination of the coordinate vectors
as
x = x0e0 + x1e1 + · · · + xN−1eN−1
where xi ∈C, and e0 = (1, 0, 0, . . . , 0), e1 = (0, 1, 0, . . . , 0), e2 = (0, 0, 1, 0, . . . , 0),
and so on up to eN−1 = (0, . . . , 0, 1).
Theorem: (i) If s0, . . . , sM−1 span a vector space, and if one of them,
say sm, is linearly dependent on the others, then the vector space is
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 6. GEOMETRIC SIGNAL THEORY
Page 117
spanned by the set obtained by omitting sm from the original set. (ii)
If s0, . . . , sM−1 span a vector space, we can always select from these a
linearly independent set that spans the same space.
Proof: Any x in the space can be represented as a linear combination
of the vectors s0, . . . , sM−1. By expressing sm as a linear combination
of the other vectors in the set, the linear combination for x becomes a
linear combination of vectors other than sm. Thus, sm can be eliminated
from the set, proving (i). To prove (ii), we can deﬁne a procedure for
forming the require subset of the original vectors: First, assign s0 to the
set. Next, check to see if s0 and s1 are linearly dependent. If so (i.e., s1
is a scalar times s0), then discard s1; otherwise assign it also to the new
set. Next, check to see if s2 is linearly dependent on the vectors in the
new set. If it is (i.e., s1 is some linear combination of s0 and s1) then
discard it; otherwise assign it also to the new set. When this procedure
terminates after processing sM−1, the new set will contain only linearly
independent vectors which span the original space.
Deﬁnition: A set of linearly independent vectors which spans a vector
space is called a basis for that vector space.
Deﬁnition: The set of coordinate vectors in CN is called the natural
basis for CN, where the nth basis vector is
en = (0, . . . , 0,
1


nth
, 0, . . . , 0).
Theorem: The linear combination expressing a vector in terms of
basis vectors for a vector space is unique.
Proof: Suppose a vector x ∈CN can be expressed in two diﬀerent
ways as a linear combination of basis vectors s0, . . . , sN−1:
x
=
α0s0 + · · · αN−1sN−1
=
β0s0 + · · · βN−1sN−1
where αi ̸= βi for at least one value of i ∈[0, N −1]. Subtracting the two
representations gives
0 = (α0 −β0)s0 + · · · + (αN−1 −βN−1)sN−1
Since the vectors are linearly independent, it is not possible to cancel
the nonzero vector (αi −βi)si using some linear combination of the other
vectors in the sum. Hence, αi = βi for all i = 0, 1, 2, . . . , N −1.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 118
6.7. SIGNAL RECONSTRUCTION FROM PROJECTIONS
Note that while the linear combination relative to a particular basis
is unique, the choice of basis vectors is not. For example, given any basis
set in CN, a new basis can be formed by rotating all vectors in CN by the
same angle. In this way, an inﬁnite number of basis sets can be generated.
As we will soon show, the DFT can be viewed as a change of coordi-
nates from coordinates relative to the natural basis in CN, {en}N−1
n=0 , to co-
ordinates relative to the sinusoidal basis for CN, {sk}N−1
k=0 , where sk(n) ∆=
ejωktn. The sinusoidal basis set for CN consists of length N sampled com-
plex sinusoids at frequencies ωk = 2πkfs/N, k = 0, 1, 2, . . . , N −1. Any
scaling of these vectors in CN by complex scale factors could also be cho-
sen as the sinusoidal basis (i.e., any nonzero amplitude and any phase
will do). However, for simplicity, we will only use unit-amplitude, zero-
phase complex sinusoids as the Fourier “frequency-domain” basis set. To
summarize this paragraph, the time-domain samples of a signal are its
coordinates relative to the natural basis for CN, while its spectral coeﬃ-
cients are the coordinates of the signal relative to the sinusoidal basis for
CN.
Theorem: Any two bases of a vector space contain the same number
of vectors.
Proof: Left as an exercise (or see [16]).
Deﬁnition: The number of vectors in a basis for a particular space
is called the dimension of the space. If the dimension is N, the space is
said to be an N dimensional space, or N-space.
In this reader, we will only consider ﬁnite-dimensional vector spaces
in any detail. However, the discrete-time Fourier transform (DTFT) and
Fourier transform both require inﬁnite-dimensional basis sets, because
there is an inﬁnite number of points in both the time and frequency
domains.
Theorem: The projections of any vector x ∈CN onto any orthogonal
basis set for CN can be summed to reconstruct x exactly.
Proof: Let {s0, . . . , sN−1} denote any orthogonal basis set for CN.
Then since x is in the space spanned by these vectors, we have
x = α0s0 + α1s1 + · · · + αN−1sN−1
for some (unique) scalars α0, . . . , αN−1. The projection of x onto sk is
equal to
Psk(x) = α0Psk(s0) + α1Psk(s1) + · · · + αN−1Psk(sN−1)
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 6. GEOMETRIC SIGNAL THEORY
Page 119
(using the linearity of the projection operator which follows from linearity
of the inner product in its ﬁrst argument). Since the basis vectors are
orthogonal, the projection of sl onto sk is zero for l ̸= k:
Psk(sl) ∆= ⟨sl, sk⟩
∥sk ∥2 sk =
 0,
l ̸= k
sk,
l = k
We therefore obtain
Psk(x) = 0 + · · · + 0 + αkPsk(sk) + 0 + · · · + 0 = αksk
Therefore, the sum of projections onto the vectors sk is just the linear
combination of the sk which forms x.
6.7.3
Gram-Schmidt Orthogonalization
Theorem: Given a set of N linearly independent vectors s0, . . . , sN−1
from CN, we can construct an orthonormal set ˜s0, . . . , ˜sN−1 which are
linear combinations of the original set and which span the same space.
Proof: We prove the theorem by constructing the desired orthonormal
set {˜sk} sequentially from the original set {sk}. This procedure is known
as Gram-Schmidt orthogonalization.
1. Set ˜s0
∆=
s0
∥s0 ∥.
2. Deﬁne y1 as the s1 minus the projection of s1 onto ˜s0:
y1
∆= s1 −P˜s0(s1) = s1 −⟨s1, ˜s0⟩˜s0
The vector y1 is orthogonal to ˜s0 by construction. (We subtracted
out the part of s1 that wasn’t orthogonal to ˜s0.)
3. Set ˜s1
∆=
y1
∥y1 ∥(i.e., normalize the result of the preceding step).
4. Deﬁne y2 as the s2 minus the projection of s2 onto ˜s0 and ˜s1:
y2
∆= s2 −P˜s0(s2) −P˜s1(s2) = s2 −⟨s2, ˜s0⟩˜s0 −⟨s2, ˜s1⟩˜s1
5. Normalize: ˜s2
∆=
y2
∥y2 ∥.
6. Continue this process until ˜sN−1 has been deﬁned.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 120
6.8. APPENDIX: MATLAB EXAMPLES
The Gram-Schmidt orthogonalization procedure will construct an or-
thonormal basis from any set of N linearly independent vectors. Obvi-
ously, by skipping the normalization step, we could also form simply an
orthogonal basis. The key ingredient of this procedure is that each new
orthonormal basis vector is obtained by subtracting out the projection
of the next linearly independent vector onto the vectors accepted so far
in the set. We may say that each new linearly independent vector sk is
projected onto the subspace spanned by the vectors {˜s0, . . . , ˜sk−1}, and
any nonzero projection in that subspace is subtracted out of sk to make
it orthogonal to the entire subspace. In other words, we retain only that
portion of each new vector sk which points along a new dimension. The
ﬁrst direction is arbitrary and is determined by whatever vector we choose
ﬁrst (s0 here). The next vector is forced to be orthogonal to the ﬁrst.
The second is forced to be orthogonal to the ﬁrst two, and so on.
This chapter can be considered an introduction to some of the most
important concepts from linear algebra. The student is invited to pursue
further reading in any textbook on linear algebra, such as [16].
6.8
Appendix: Matlab Examples
Here’s how Fig. 6.1 was generated in Matlab:
>> x = [2 3];
% coordinates of x
>> origin = [0 0];
% coordinates of the origin
>> xcoords = [origin(1) x(1)]; % plot() expects coordinate lists
>> ycoords = [origin(2) x(2)];
>> plot(xcoords,ycoords);
% Draw a line from origin to x
Mathematica can plot a list of ordered pairs:
In[1]:
(* Draw a line from (0,0) to (2,3): *)
ListPlot[{{0,0},{2,3}},PlotJoined->True];
In Matlab, the mean of the row-vector x can be computed as
x * ones(size(x’))/N
or by using the built-in function mean().
In Matlab, if x = [x1 ...
xN] is a row vector, we can compute the
total energy as
Ex = x*x’.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 6. GEOMETRIC SIGNAL THEORY
Page 121
Matlab has a function orth() which will compute an orthonormal
basis for a space given any set of vectors which span the space.
>> help orth
ORTH
Orthogonalization.
Q = orth(A) is an orthonormal basis for the range of A.
Q’*Q = I, the columns of Q span the same space as the columns
of A and the number of columns of Q is the rank of A.
See also QR, NULL.
Below is an example of using orth() to orthonormalize a linearly inde-
pendent basis set for N = 3:
% Demonstration of the Matlab function orth() for
% taking a set of vectors and returning an orthonormal set
% which span the same space.
v1 = [1; 2; 3];
% our first basis vector (a column vector)
v2 = [1; -2; 3]; % a second, linearly independent column vector
v1’ * v2
% show that v1 is not orthogonal to v2
ans =
6
V = [v1,v2]
% Each column of V is one of our vectors
V =
1
1
2
-2
3
3
W = orth(V)
% Find an orthonormal basis for the same space
W =
0.2673
0.1690
0.5345
-0.8452
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 122
6.8. APPENDIX: MATLAB EXAMPLES
0.8018
0.5071
w1 = W(:,1)
% Break out the returned vectors
w1 =
0.2673
0.5345
0.8018
w2 = W(:,2)
w2 =
0.1690
-0.8452
0.5071
w1’ * w2
% Check that w1 is orthogonal to w2 (to working precision)
ans =
2.5723e-17
w1’ * w1
% Also check that the new vectors are unit length in 3D
ans =
1
w2’ * w2
ans =
1
W’ * W
% faster way to do the above checks (matrix multiplication)
ans =
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 6. GEOMETRIC SIGNAL THEORY
Page 123
1.0000
0.0000
0.0000
1.0000
% Construct some vector x in the space spanned by v1 and v2:
x = 2 * v1 - 3 * v2
x =
-1
10
-3
% Show that x is also some linear combination of w1 and w2:
c1 = x’ * w1
% Coefficient of projection of x onto w1
c1 =
2.6726
c2 = x’ * w2
% Coefficient of projection of x onto w2
c2 =
-10.1419
xw = c1 * w1 + c2 * w2
% Can we make x using w1 and w2?
xw =
-1.0000
10.0000
-3.0000
error = x - xw
error =
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 124
6.8. APPENDIX: MATLAB EXAMPLES
1.0e-14 *
0.1332
0
0
norm(error)
% typical way to summarize a vector error
ans =
1.3323e-15
% It works!
% Now, construct some vector x NOT in the space spanned by v1 and v2:
y = [1; 0; 0];
% Almost anything we guess in 3D will work
%
Try to express y as a linear combination of w1 and w2:
c1 = y’ * w1;
% Coefficient of projection of y onto w1
c2 = y’ * w2;
% Coefficient of projection of y onto w2
yw = c1 * w1 + c2 * w2
% Can we make y using w1 and w2?
yw =
0.1000
0.0000
0.3000
yerror = y - yw
yerror =
0.9000
0.0000
-0.3000
norm(yerror)
ans =
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 6. GEOMETRIC SIGNAL THEORY
Page 125
0.9487
% While the error is not zero, it is the smallest possible
% error in the least squares sense.
% That is, yw is the optimal least-squares approximation
% to y in the space spanned by v1 and v2 (w1 and w2).
% In other words, norm(yerror) <= norm(y-yw2) for any other vector yw2 made
% using a linear combination of v1 and v2.
% In yet other words, we obtain the optimal least squares approximation
% of y (which lives in 3D) in some subspace W (a 2D subspace of 3D)
% by projecting y orthogonally onto the subspace W to get yw as above.
%
% An important property of the optimal least-squares approximation
% is that the approximation error is orthogonal to the the subspace
% in which the approximation lies.
Let’s show this:
W’ * yerror % must be zero to working precision
ans =
1.0e-16 *
-0.2574
-0.0119
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 126
6.8. APPENDIX: MATLAB EXAMPLES
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Chapter 7
Derivation of the Discrete
Fourier Transform (DFT)
This chapter derives the Discrete Fourier Transform (DFT) as a projec-
tion of a length N signal x(·) onto the set of N sampled complex sinusoids
generated by the N roots of unity.
7.1
The DFT Derived
In this section, the Discrete Fourier Transform (DFT) will be derived.
7.1.1
Geometric Series
Recall that for any compex number z1 ∈C, the signal x(n) ∆= zn
1 , n =
0, 1, 2, . . ., deﬁnes a geometric sequence, i.e., each term is obtained by
multiplying the previous term by a (complex) constant.
A geometric
series is deﬁned as the sum of a geometric sequence:
SN(z1) ∆=
N−1

n=0
zn
1 = 1 + z1 + z2
1 + z3
1 + · · · + zN−1
1
If z1 ̸= 1, the sum can be expressed in closed form as
SN(z1) = 1 −zN
1
1 −z1
127

Page 128
7.1. THE DFT DERIVED
Proof: We have
SN(z1)
∆=
1 + z1 + z2
1 + z3
1 + · · · + zN−1
1
z1SN(z1)
=
z1 + z2
1 + z3
1 + · · · + zN−1
1
+ zN
1
=⇒
SN(z1) −z1SN(z1)
=
1 −zN
1
=⇒
SN(z1)
=
1 −zN
1
1 −z1
7.1.2
Orthogonality of Sinusoids
A key property of sinusoids is that they are orthogonal at diﬀerent fre-
quencies. That is,
ω1 ̸= ω2 =⇒A1 sin(ω1t + φ1) ⊥A2 sin(ω2t + φ2)
This is true whether they are complex or real, and whatever amplitude
and phase they may have. All that matters is that the frequencies be
diﬀerent. Note, however, that the sinusoidal durations must be inﬁnity.
For length N sampled sinuoidal signal segments, such as used by
the DFT, exact orthogonality holds only for the harmonics of the sam-
pling rate divided by N, i.e., only over the frequencies fk = kfs/N, k =
0, 1, 2, 3, . . . , N −1. These are the only frequencies that have an exact
integer number of periods in N samples (depicted in Fig. 7.2 for N = 8).
The complex sinusoids corresponding to the frequencies fk are
sk(n) ∆= ejωknT ,
ωk
∆= k2π
N fs,
k = 0, 1, 2, . . . , N −1
These sinusoids are generated by the N roots of unity in the complex
plane:
W k
N
∆= ejωkT ∆= ejk2π(fs/N)T = ejk2π/N,
k = 0, 1, 2, . . . , N −1
These are called the N roots of unity because each of them satisﬁes

W k
N
	N
=

ejωkT N =

ejk2π/N	N
= ejk2π = 1
The N roots of unity are plotted in the complex plane in Fig. 7.1 for
N = 8. In general, for any N, there will always be a point at z = 1, and
the points equally subdivide the unit circle. When N is even, there is a
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 7. DERIVATION OF THE DISCRETE FOURIER
TRANSFORM (DFT)
Page 129
W
z
=
=
0
N
1
e
e
e
W
z
=
=
=
=
1
j
j
T
j
N
ω
π
π
4
2
1
N
(
)
e
e
e
e
W
z
=
=
=
=
=
j
j
j
T
j
N
N
−
ω
−1
π
π
−
π
−
N
)
N
N
4
4
7
1
2
1
Re{z}
Im{z}
(
)
e
e
e
e
W
z
−
=
=
=
=
=
=
j
j
j
T
j
N
N 2
π
−
π
ω
π
N
N
N
2
2
2
1
j
e
e
e
W
z
=
=
=
=
=
2
j
j
T
j
N
ω
π
π
2
4
2
N
W
z =
3
N
W
W
z
=
=
3
5
N
N
−
j
W
W
z
−
=
=
=
2
6
N
N
−
Figure 7.1: The N roots of unity for N = 8.
point at z = −1 (corresponding to a sinusoid at exactly half the sampling
rate), while if N is odd, there is no point at z = −1.
The sampled sinusoids corresponding to the N roots of unity are plot-
ted in Fig. 7.2. These are the sampled sinusoids (W k
N)n = ej2πkn/N =
ejωknT used by the DFT. Note that taking successively higher integer
powers of the point W k
N on the unit circle generates samples of the kth
DFT sinusoid, giving [W k
N]n, n = 0, 1, 2, . . . , N −1. The kth sinusoid
generator W k
N is in turn the kth power of the primitive Nth root of unity
WN
∆= ej2π/N.
The notation WN, W k
N, and W nk
N
are common in the
digital signal processing literature.
Note that in Fig. 7.2 the range of k is taken to be [−N/2, N/2 −1] =
[−4, 3] instead of [0, N −1] = [0, 7]. This is the most “physical” choice
since it corresponds with our notion of “negative frequencies.” However,
we may add any integer multiple of N to k without changing the sinusoid
indexed by k. In other words, k ± mN refers to the same sinusoid for all
integer m.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 130
7.1. THE DFT DERIVED
0
2
4
6
8
-1
0
1
Real Part
k=3
0
2
4
6
8
-1
0
1
k=3
Imaginary Part
0
2
4
6
8
-1
0
1
k=2
0
2
4
6
8
-1
0
1
k=2
0
2
4
6
8
-1
0
1
k=1
0
2
4
6
8
-1
0
1
k=1
0
2
4
6
8
-1
0
1
k=0
0
2
4
6
8
-1
0
1
k=0
0
2
4
6
8
-1
0
1
k=-1
0
2
4
6
8
-1
0
1
k=-1
0
2
4
6
8
-1
0
1
k=-2
0
2
4
6
8
-1
0
1
k=-2
0
2
4
6
8
-1
0
1
k=-3
0
2
4
6
8
-1
0
1
k=-3
0
2
4
6
8
-1
0
1
k=-4
Time (samples)
0
2
4
6
8
-1
0
1
k=-4
Time (samples)
Figure 7.2: Complex sinusoids used by the DFT for N = 8.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 7. DERIVATION OF THE DISCRETE FOURIER
TRANSFORM (DFT)
Page 131
7.1.3
Orthogonality of the DFT Sinusoids
We now show mathematically that the DFT sinusoids are exactly orthog-
onal. Let
sk(n) ∆= ejωknT = ej2πkn/N
denote the kth complex DFT sinusoid. Then
⟨sk, sl⟩
∆=
N−1

n=0
sk(n)sl(n) =
N−1

n=0
ej2πkn/Ne−j2πln/N
=
N−1

n=0
ej2π(k−l)n/N =
1 −ej2π(k−l)
1 −ej2π(k−l)/N
where the last step made use of the closed-form expression for the sum
of a geometric series.
If k ̸= l, the denominator is nonzero while the
numerator is zero. This proves
sk ⊥sl,
k ̸= l
While we only looked at unit amplitude, zero phase complex sinusoids,
as used by the DFT, it is readily veriﬁed that the (nonzero) amplitude
and phase have no eﬀect on orthogonality.
7.1.4
Norm of the DFT Sinusoids
For k = l, we follow the previous derivation to the next-to-last step to
get
⟨sk, sk⟩=
N−1

n=0
ej2π(k−k)n/N = N
which proves
∥sk ∥=
√
N
7.1.5
An Orthonormal Sinusoidal Set
We can normalize the DFT sinusoids to obtain an orthonormal set:
˜sk(n) ∆= sk(n)
√
N
= ej2πkn/N
√
N
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 132
7.1. THE DFT DERIVED
The orthonormal sinusoidal basis signals satisfy
⟨˜sk, ˜sl⟩=
 1,
k = l
0,
k ̸= l
We call these the normalized DFT sinusoids.
7.1.6
The Discrete Fourier Transform (DFT)
Given a signal x(·) ∈CN, the spectrum is deﬁned by
X(ωk) ∆= ⟨x, sk⟩=
N−1

n=0
x(n)sk(n),
k = 0, 1, 2, . . . , N −1
or, as is most often written
X(ωk) ∆=
N−1

n=0
x(n)e−j 2πkn
N ,
k = 0, 1, 2, . . . , N −1
That is, the kth sample X(ωk) of the spectrum of x is deﬁned as the inner
product of x with the kth DFT sinusoid sk. This deﬁnition is N times
the coeﬃcient of projection of x onto sk, i.e.,
⟨x, sk⟩
∥sk ∥2 = X(ωk)
N
The projection of x onto sk itself is
Psk(x) = X(ωk)
N
sk
The inverse DFT is simply the sum of the projections:
x =
N−1

k=0
X(ωk)
N
sk
or, as we normally write,
x(n) = 1
N
N−1

k=0
X(ωk)ej 2πkn
N
In summary, the DFT is proportional to the set of coeﬃcients of
projection onto the sinusoidal basis set, and the inverse DFT is the re-
construction of the original signal as a superposition of its sinusoidal
projections.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 7. DERIVATION OF THE DISCRETE FOURIER
TRANSFORM (DFT)
Page 133
7.1.7
Frequencies in the “Cracks”
The DFT is deﬁned only for frequencies ωk = 2πkfs/N. If we are analyz-
ing one or more periods of an exactly periodic signal, where the period is
exactly N samples (or some integer divisor of N), then these really are
the only frequencies present in the signal, and the spectrum is actually
zero everywhere but at ω = ωk. However, we use the DFT to analyze ar-
bitrary signals from nature. What happens when a frequency ω is present
in a signal x that is not one of the DFT-sinusoid freqencies ωk?
To ﬁnd out, let’s project a length N segment of a sinusoid at an
arbitrary freqency ω onto the kth DFT sinusoid:
x(n)
∆=
ejωnT
sk(n)
∆=
ejωknT
Psk(x)
=
⟨x, sk⟩
⟨sk, sk⟩sk
∆= X(ωk)
N
sk
The coeﬃcient of projection is proportional to
X(ωk)
∆=
⟨x, sk⟩∆=
N−1

n=0
x(n)sk(n)
=
N−1

n=0
ejωnT e−jωknT =
N−1

n=0
ej(ω−ωk)nT = 1 −ej(ω−ωk)NT
1 −ej(ω−ωk)T
=
ej(ω−ωk)(N−1)T/2 sin[(ω −ωk)NT/2]
sin[(ω −ωk)T/2]
using the closed-form expression for a geometric series sum. As previously
shown, the sum is N at ω = ωk and zero at ωl, for l ̸= k. However, the
sum is nonzero at all other frequencies.
Since we are only looking at N samples, any sinusoidal segment can
be projected onto the N DFT sinusoids and be reconstructed exactly by
a linear combination of them. Another way to say this is that the DFT
sinusoids form a basis for CN, so that any length N signal whatsoever can
be expressed as linear combination of them. Therefore, when analyzing
segments of recorded signals, we must interpret what we see accordingly.
The typical way to think about this in practice is to consider the DFT
operation as a digital ﬁlter.1 The frequency response of this ﬁlter is what
1More precisely, DFTk() is a length N ﬁnite-impulse-response (FIR) digital ﬁlter.
See §8.7 for related discussion.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 134
7.1. THE DFT DERIVED
we just computed,2 and its magnitude is
|X(ωk)| =

sin[(ω −ωk)NT/2]
sin[(ω −ωk)T/2]

(shown in Fig. 7.3a for k = N/4). At all other integer values of k, the
response is the same but shifted (circularly) left or right so that the peak
is centered on ωk. The secondary peaks away from ωk are called sidelobes
of the DFT response, while the main peak may be called the main lobe
of the response. Since we are normally most interested in spectra from
an audio perspective, the same plot is repeated using a decibel vertical
scale in Fig. 7.3b (clipped at −60 dB). We see that the sidelobes are
really quite high from an audio perspective. Sinusoids with frequencies
near ωk±1.5, for example, are only attenuated approximately 13 dB in the
DFT output X(ωk).
We see that X(ωk) is sensitive to all frequencies between dc and the
sampling rate except the other DFT-sinusoid frequencies ωl for l ̸= k.
This is sometimes called spectral leakage or cross-talk in the spectrum
analysis. Again, there is no error when the signal being analyzed is truly
periodic and we can choose N to be exactly a period, or some multiple of
a period. Normally, however, this cannot be easily arranged, and spectral
leakage can really become a problem.
Note that spectral leakage is not reduced by increasing N.
It can
be thought of as being caused by abruptly truncating a sinusoid at the
beginning and/or end of the N-sample time window.
Only the DFT
sinusoids are not cut oﬀat the window boundaries. All other frequencies
will suﬀer some truncation distortion, and the spectral content of the
abrupt cut-oﬀor turn-on transient can be viewed as the source of the
sidelobes. Remember that, as far as the DFT is concerned, the input
signal x(n) is the same as its periodic extension. If we repeat N samples
of a sinusoid at frequency ω ̸= ωk, there will be a “glitch” every N
samples since the signal is not periodic in N samples. This glitch can be
considered a source of new energy over the entire spectrum.
To reduce spectral leakage (cross-talk from far-away frequencies), we
typically use a window function, such as a “raised cosine” window, to
taper the data record gracefully to zero at both endpoints of the window.
As a result of the smooth tapering, the main lobe widens and the sidelobes
decrease in the DFT response. Using no window is better viewed as using
2We call this the aliased sinc function to distinguish it from the sinc function
sinc(x)
∆= sin(πx)/(πx).
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 7. DERIVATION OF THE DISCRETE FOURIER
TRANSFORM (DFT)
Page 135
0
1
2
3
4
5
6
7
0
5
10
15
20
DFT Amplitude Response at k=N/4
Normalized Radian Frequency (radians per sample)
Magnitude (Linear)
a)
0
1
2
3
4
5
6
7
-60
-40
-20
0
20
40
Normalized Radian Frequency (radians per sample)
Magnitude (dB)
b)
Main Lobe
Sidelobes
Figure 7.3: Frequency response magnitude of a single DFT output
sample.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 136
7.1. THE DFT DERIVED
a rectangular window of length N, unless the signal is exactly periodic in
N samples. These topics are considered further in Music 420 and in the
“Examples using the DFT” chapter.
Since the kth spectral sample X(ωk) is properly regarded as a measure
of spectral amplitude over a range of frequencies, nominally k −1/2 to
k + 1/2, this range is sometimes called a frequency bin (as in a “storage
bin” for spectral energy). The frequency index k is called the bin number,
and |X(ωk)|2 can be regarded as the total energy in the kth bin (see
Parseval’s Theorem in the “Fourier Theorems” chapter). Similar remarks
apply to samples of any continuous bandlimited function; however, the
term “bin” is only used in the frequency domain, even though it could be
assigned exactly the same meaning mathematically in the time domain.
7.1.8
Normalized DFT
A more “theoretically clean” DFT is obtained by projecting onto the
normalized DFT sinusoids
˜sk(n) ∆= ej2πkn/N
√
N
In this case, the normalized DFT of x is
˜X(ωk) ∆= ⟨x, ˜sk⟩=
1
√
N
N−1

n=0
x(n)e−j2πkn/N
which is also precisely the coeﬃcient of projection of x onto ˜sk.
The
inverse normalized DFT is then more simply
x(n) =
N−1

k=0
X(ωk)˜sk(n) =
1
√
N
N−1

k=0
X(ωk)ej2πkn/N
While this deﬁnition is much cleaner from a “geometric signal theory”
point of view, it is rarely used in practice since it requires more compu-
tation than the typical deﬁnition. However, note that the only diﬀerence
between the forward and inverse transforms in this case is the sign of the
exponent in the kernel.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 7. DERIVATION OF THE DISCRETE FOURIER
TRANSFORM (DFT)
Page 137
7.2
The Length 2 DFT
The length 2 DFT is particularly simple, since the basis sinusoids are
real:
s0
=
(1, 1)
s1
=
(1, −1)
The DFT sinusoid s0 is a sampled constant signal, while s1 is a sampled
sinusoid at half the sampling rate.
Figure 7.4 illustrates the graphical relationships for the length 2 DFT
of the signal x = [6, 2].
s0 = (1,1)
s1 = (1,-1)
e0 = (1,0)
e1 = (0,1)
x = (6,2)
( )
(
)
x
Pe1
=
2
,0
( )
(
)
x
Ps1
−
=
2
,2
( )
(
)
x
Ps0
=
4
,4
( )
(
)
x
Pe0
=
0
,6
Figure 7.4: Graphical interpretation of the length 2 DFT.
Analytically, we compute the DFT to be
X(ω0)
∆=
Ps0(x) ∆= ⟨x, s0⟩
⟨s0, s0⟩s0 = 6 · 1 + 2 · 1
12 + 12
s0 = 4s0 = (4, 4)
X(ω1)
∆=
Ps1(x) ∆= ⟨x, s1⟩
⟨s1, s1⟩s1 = 6 · 1 + 2 · (−1)
12 + (−1)2
s0 = 2s1 = (2, −2)
Note the lines of orthogonal projection illustrated in the ﬁgure.
The
“time domain” basis consists of the vectors {e0, e1}, and the orthogo-
nal projections onto them are simply the coordinate projections (6, 0)
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 138
7.3. MATRIX FORMULATION OF THE DFT
and (0, 2). The “frequency domain” basis vectors are {s0, s1}, and they
provide an orthogonal basis set which is rotated 45 degrees relative to
the time-domain basis vectors. Projecting orthogonally onto them gives
X(ω0) = (4, 4) and X(ω1) = (2, −2), respectively. The original signal x
can be expressed as the vector sum of its coordinate projections (a time-
domain representation), or as the vector sum of its projections onto the
DFT sinusoids (a frequency-domain representation). Computing the co-
eﬃcients of projection is essentially “taking the DFT” and constructing
x as the vector sum of its projections onto the DFT sinusoids amounts
to “taking the inverse DFT.”
7.3
Matrix Formulation of the DFT
The DFT can be formulated as a complex matrix multiply, as we show
in this section. For basic deﬁnitions regarding matrices, see §A.
The DFT consists of inner products of the input signal x with sampled
complex sinusoidal sections sk:
X(ωk) ∆= ⟨x, sk⟩∆=
N−1

n=0
x(n)e−j2πnk/N,
k = 0, 1, 2, . . . , N −1
By collecting the DFT output samples into a column vector, we have


X(ω0)
X(ω1)
X(ω2)
...
X(ωN−1)


=


s0(0)
s0(1)
· · ·
s0(N −1)
s1(0)
s1(1)
· · ·
s1(N −1)
s2(0)
s2(1)
· · ·
s2(N −1)
...
...
...
...
sN−1(0)
sN−1(1)
· · ·
sN−1(N −1)




x(0)
x(1)
x(2)
...
x(N −1)


or
X = SNx
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 7. DERIVATION OF THE DISCRETE FOURIER
TRANSFORM (DFT)
Page 139
where SN denotes the DFT matrix SN[k, n] ∆= W −kn
N
∆= e−j2πkn/N, or,
SN
∆=


s0(0)
s0(1)
· · ·
s0(N −1)
s1(0)
s1(1)
· · ·
s1(N −1)
s2(0)
s2(1)
· · ·
s2(N −1)
...
...
...
...
sN−1(0)
sN−1(1)
· · ·
sN−1(N −1)


∆=


1
1
1
· · ·
1
1
e−j2π/N
e−j4π/N
· · ·
e−j2π(N−1)/N
1
e−j4π/N
e−j8π/N
· · ·
e−j2π2(N−1)/N
...
...
...
...
...
1
e−j2π(N−1)/N
e−j2π2(N−1)/N
· · ·
e−j2π(N−1)(N−1)/N


We see that the kth row of the DFT matrix is the k DFT sinusoids.
Since the matrix is symmetric, SN
T = SN (where transposition does not
include conjugation), we observe that the kth column of SN is also the
kth DFT sinusoid.
The inverse DFT matrix is simply SN/N. That is, we can perform
the inverse DFT operation simply as
x = 1
N SN ∗X
Since X = SNx, the above implies
SN ∗SN = N · I
The above equation succinctly implies that the columns of SN are or-
thogonal, which, of course, we already knew.
The normalized DFT matrix is given by
˜SN
∆=
1
√
N
SN
and the corresponding normalized inverse DFT matrix is simply ˜SN, so
that we have
˜S∗
N ˜SN = I
This implies that the columns of SN are orthonormal. Such a matrix is
said to be unitary.
When a real matrix A satisﬁes ATA = I, then A is said to be orthogo-
nal. “Unitary” is the generalization of “orthogonal” to complex matrices.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 140
7.4. MATLAB EXAMPLES
7.4
Matlab Examples
7.4.1
Figure 7.2
Below is the Matlab for Fig. 7.2:
N=8;
fs=1;
n = [0:N-1]; % row
t = [0:0.01:N]; % interpolated
k=fliplr(n)’ - N/2;
fk = k*fs/N;
wk = 2*pi*fk;
clf;
for i=1:N
subplot(N,2,2*i-1);
plot(t,cos(wk(i)*t))
axis([0,8,-1,1]);
hold on;
plot(n,cos(wk(i)*n),’*’)
if i==1
title(’Real Part’);
end;
ylabel(sprintf(’k=%d’,k(i)));
if i==N
xlabel(’Time (samples)’);
end;
subplot(N,2,2*i);
plot(t,sin(wk(i)*t))
axis([0,8,-1,1]);
hold on;
plot(n,sin(wk(i)*n),’*’)
ylabel(sprintf(’k=%d’,k(i)));
if i==1
title(’Imaginary Part’);
end;
if i==N
xlabel(’Time (samples)’);
end;
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 7. DERIVATION OF THE DISCRETE FOURIER
TRANSFORM (DFT)
Page 141
end
7.4.2
Figure 7.3
Below is the Matlab for Fig. 7.3:
% Parameters (sampling rate = 1)
N = 16;
% DFT length
k = N/4;
% bin where DFT filter is centered
wk = 2*pi*k/N;
% normalized radian center-frequency for DFT_k()
wStep = 2*pi/N;
w = [0:wStep:2*pi - wStep]; % DFT frequency grid
interp = 10;
N2 = interp*N;
% Denser grid showing "arbitrary" frequencies
w2Step = 2*pi/N2;
w2 = [0:w2Step:2*pi - w2Step]; % Extra dense frequency grid
X = (1 - exp(j*(w2-wk)*N)) ./ (1 - exp(j*(w2-wk)));
% slightly offset to avoid divide by zero at wk
X(1+k*interp) = N;
% Fix divide-by-zero point (overwrite "NaN")
% Plot spectral magnitude
clf;
magX = abs(X);
magXd = magX(1:interp:N2); % DFT frequencies only
subplot(2,1,1);
plot(w2,magX,’-’); hold on; grid;
plot(w,magXd,’*’);
% Show DFT sample points
title(’DFT Amplitude Response at k=N/4’);
xlabel(’Normalized Radian Frequency (radians per sample)’);
ylabel(’Magnitude (Linear)’);
text(-1,20,’a)’);
% Same thing on a dB scale
magXdb = 20*log10(magX);
% Spectral magnitude in dB
% Since the zeros go to minus infinity, clip at -60 dB:
magXdb = max(magXdb,-60*ones(1,N2));
magXddb = magXdb(1:interp:N2); % DFT frequencies only
subplot(2,1,2);
hold off; plot(w2,magXdb,’-’); hold on; plot(w,magXddb,’*’); grid;
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 142
7.4. MATLAB EXAMPLES
xlabel(’Normalized Radian Frequency (radians per sample)’);
ylabel(’Magnitude (dB)’);
text(-1,40,’b)’);
print -deps ’../eps/dftfilter.eps’;
hold off;
7.4.3
DFT Matrix in Matlab
The following example reinforces the discussion of the DFT matrix. We
can simply create the DFT matrix in Matlab by taking the FFT of the
identity matrix. Then we show that multiplying by the DFT matrix is
equivalent to the FFT:
>> eye(4)
ans =
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
>> S4 = fft(eye(4))
ans =
1.0000
1.0000
1.0000
1.0000
1.0000
0.0000 - 1.0000i
-1.0000
0.0000 + 1.0000i
1.0000
-1.0000
1.0000
-1.0000
1.0000
0.0000 + 1.0000i
-1.0000
0.0000 - 1.0000i
>> S4’ * S4
% Show that S4’ = inverse DFT (times N=4)
ans =
4.0000
0.0000
0
0.0000
0.0000
4.0000
0.0000
0.0000
0
0.0000
4.0000
0.0000
0.0000
0.0000
0.0000
4.0000
>> x = [1; 2; 3; 4]
x =
1
2
3
4
>> fft(x)
ans =
10.0000
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 7. DERIVATION OF THE DISCRETE FOURIER
TRANSFORM (DFT)
Page 143
-2.0000 + 2.0000i
-2.0000
-2.0000 - 2.0000i
>> S4 * x
ans =
10.0000
-2.0000 + 2.0000i
-2.0000
-2.0000 - 2.0000i
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 144
7.4. MATLAB EXAMPLES
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Chapter 8
Fourier Theorems for the
DFT
This chapter derives various Fourier theorems for the case of the DFT.
Included are symmetry relations, the shift theorem, convolution theorem,
correlation theorem, power theorem, and theorems pertaining to interpo-
lation and downsampling. Applications related to certain theorems are
outlined, including linear time-invariant ﬁltering, sampling rate conver-
sion, and statistical signal processing.
8.1
The DFT and its Inverse
Let x(n), n = 0, 1, 2, . . . , N −1 denote an n-sample complex sequence,
i.e., x ∈CN. Then the spectrum of x is deﬁned by the Discrete Fourier
Transform (DFT):
X(k) ∆=
N−1

n=0
x(n)e−j2πnk/N,
k = 0, 1, 2, . . . , N −1
The inverse DFT (IDFT) is deﬁned by
x(n) = 1
N
N−1

k=0
X(k)ej2πnk/N,
n = 0, 1, 2, . . . , N −1
Note that for the ﬁrst time we are not carrying along the sampling interval
T = 1/fs in our notation. This is actually the most typical treatment in
145

Page 146
8.1. THE DFT AND ITS INVERSE
the digital signal processing literature. It is often said that the sampling
frequency is fs = 1. However, it can be set to any desired value using the
substitution
ej2πnk/N = ej2πk(fs/N)nT ∆= ejωktn
However, for the remainder of this reader, we will adopt the more common
(and more mathematical) convention fs = 1.
In particular, we’ll use
the deﬁnition ωk
∆= 2πk/N for this chapter only. In this case, a radian
frequency ωk is in units of “radians per sample.” Elsewhere in this reader,
ωk always means “radians per second.” (Of course, there’s no diﬀerence
when the sampling rate is really 1.) Another term we use in connection
with the fs = 1 convention is normalized frequency: All normalized radian
frequencies lie in the range [−π, π), and all normalized frequencies in Hz
lie in the range [−0.5, 0.5).
8.1.1
Notation and Terminology
If X is the DFT of x, we say that x and X form a transform pair and
write
x ↔X
(“x corresponds to X”).
Another notation we’ll use is
DFT(x)
∆=
X,
and
DFTk(x)
∆=
X(k)
As we’ve already seen, time-domain signals are consistently denoted using
lowercase symbols such as “x,” while frequency-domain signals (spectra),
are denoted in uppercase (“X”).
8.1.2
Modulo Indexing, Periodic Extension
The DFT sinusoids sk(n) ∆= ejωkn are all periodic having periods which
divide N. That is, sk(n+mN) = sk(n) for any integer m. Since a length
N signal x can be expressed as a linear combination of the DFT sinusoids
in the time domain,
x(n) = 1
N

k
X(k)sk(n),
it follows that the “automatic” deﬁnition of x(n) beyond the range [0, N −
1] is periodic extension, i.e., x(n + mN) ∆= x(n) for every integer m.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 147
Moreover, the DFT also repeats naturally every N samples, since
X(k + mN) ∆=

n
⟨x, sk+mN⟩=
N−1

n=0
⟨x, sk⟩= X(k)
because sk+mN(n) = ej2πn(k+mN)/N = ej2πnk/Nej2πnm = sk(n).
(The
DFT sinusoids behave identically as functions of n and k.) Accordingly,
for purposes of DFT studies, we may deﬁne all signals in CN as be-
ing single periods from an inﬁnitely long periodic signal with period N
samples:
Deﬁnition: For any signal x ∈CN, we deﬁne
x(n + mN) ∆= x(n)
for every integer m.
As a result of this convention, all indexing of signals and spectra1 can
be interpreted modulo N, and we may write x(n mod N) to emphasize
this. Formally, “n mod N” is deﬁned as n −mN with m chosen to give
n −mN in the range [0, N −1].
As an example, when indexing a spectrum X, we have that X(N) =
X(0) which can be interpreted physically as saying that the sampling
rate is the same frequency as dc for discrete time signals. In the time
domain, we have what is sometimes called the “periodic extension” of
x(n). This means that the input to the DFT is mathematically treated
as samples of a periodic signal with period NT seconds (N samples). The
corresponding assumption in the frequency domain is that the spectrum
is zero between frequency samples ωk.
It is also possible to adopt the point of view that the time-domain
signal x(n) consists of N samples preceded and followed by zeros.
In
this case, the spectrum is nonzero between spectral samples ωk, and the
spectrum between samples can be reconstructed by means of bandlimited
interpolation.
This “time-limited” interpretation of the DFT input is
considered in detail in Music 420 and is beyond the scope of Music 320
(except in the discussion of “zero padding ↔interpolation” below).
1A spectrum is mathematically identical to a signal, since both are just sequences of
N complex numbers. However, for clarity, we generally use “signal” when the sequence
index is considered a time index, and “spectrum” when the index is associated with
successive frequency samples.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 148
8.2. SIGNAL OPERATORS
8.2
Signal Operators
It will be convenient in the Fourier theorems below to make use of the
following signal operator deﬁnitions.
8.2.1
Flip Operator
Deﬁnition: We deﬁne the ﬂip operator by
Flipn(x) ∆= x(−n)
which, by modulo indexing, is x(N −n). The Flip() operator reverses the
order of samples 1 through N −1 of a sequence, leaving sample 0 alone,
as shown in Fig. 8.1a. Thanks to modulo indexing, it can also be viewed
as “ﬂipping” the sequence about the vertical axis, as shown in Fig. 8.1b.
The interpretation of Fig. 8.1b is usually the one we want, and the Flip
operator is usually thought of as “time reversal” when applied to a signal
x or “frequency reversal” when applied to a spectrum X.
8.2.2
Shift Operator
Deﬁnition: The shift operator is deﬁned by
Shift∆,n(x) ∆= x(n −∆)
and Shift∆(x) denotes the entire shifted signal. Note that since index-
ing is modulo N, the shift is circular. However, we normally use it to
represent time delay by ∆samples. We often use the shift operator in
conjunction with zero padding (appending zeros to the signal x) in order
to avoid the “wrap-around” associated with a circular shift.
Figure 8.2 illustrates successive one-sample delays of a periodic signal
having ﬁrst period given by [0, 1, 2, 3, 4].
Example: Shift1([1, 0, 0, 0]) = [0, 1, 0, 0]
(an impulse delayed one
sample).
Example: Shift1([1, 2, 3, 4]) = [4, 1, 2, 3] (a circular shift example).
Example: Shift−2([1, 0, 0, 0]) = [0, 0, 1, 0]
(another circular shift
example).
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 149
x(n)
Flipn(x)
0
1
2
3
4
0
1
2
3
4
x(n)
Flipn(x)
-2
-1
0
1
2
-2
-1
0
1
2
a)
b)
Figure 8.1: Illustration of x and Flip(x) for N = 5 and two
diﬀerent domain interpretations:
a) n ∈[0, N −1].
b) n ∈[−(N −1)/2, (N −1)/2].
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 150
8.2. SIGNAL OPERATORS
Figure 8.2: Successive one-sample shifts of a sampled periodic
sawtooth waveform having ﬁrst period [0, 1, 2, 3, 4].
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 151
8.2.3
Convolution
Deﬁnition: The convolution of two signals x and y in CN is denoted
“x ∗y” and deﬁned by
(x ∗y)n
∆=
N−1

m=0
x(m)y(n −m)
Note that this is cyclic or “circular” convolution.2
The importance of
convolution in linear systems theory is discussed in §8.7
Convolution is commutative, i.e.,
x ∗y = y ∗x
Proof:
(x∗y)n
∆=
N−1

m=0
x(m)y(n−m) =
n−N+1

l=n
x(n−l)y(l) =
N−1

l=0
y(l)x(n−l) ∆= (y∗x)n
where in the ﬁrst step we made the change of summation variable l ∆=
n−m, and in the second step, we made use of the fact that any sum over
all N terms is equivalent to a sum from 0 to N −1.
Graphical Convolution
Note that the cyclic convolution operation can be expressed in terms of
previously deﬁned operators as
y(n) ∆= (x ∗h)n
∆=
N−1

m=0
x(m)h(n −m) = ⟨x, Shiftn(Flip(h))⟩
(h real)
where x, y ∈CN and h ∈RN.
It is instructive to interpret the last
expression above graphically.
Figure 8.3 illustrates convolution of
y
=
[1, 1, 1, 1, 0, 0, 0, 0]
h
=
[1, 0, 0, 0, 0, 1, 1, 1]
2To simulate acyclic convolution, as is appropriate for the simulation of sampled
continuous-time systems, suﬃcient zero padding is used so that nonzero samples do
not “wrap around” as a result of the shifting of y in the deﬁnition of convolution. Zero
padding is discussed later in this reader.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 152
8.2. SIGNAL OPERATORS
y(m)
h(0–m)
h(7–m)
h(6–m)
h(5–m)
h(1–m)
h(2–m)
h(3–m)
h(4–m)
4
3
2
1
0
1
2
3
(
)( )
[
]
=
∗
n
h
y
3
,2
,1
,0
,1
,2
,3
,4
	 (
) (
)
N−1
m
m
n
h
m
y
−
Figure 8.3: Illustration of convolution of y = [1, 1, 1, 1, 0, 0, 0, 0]
and its “matched ﬁlter” h=[1,0,0,0,0,1,1,1] (N = 8).
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 153
For example, y could be a “rectangularly windowed signal, zero-padded
by a factor of 2,” where the signal happened to be dc (all 1s). For the
convolution, we need
Flip(h) = [1, 1, 1, 1, 0, 0, 0, 0]
which is the same as y. When h = Flip(y), we say that h is matched
ﬁlter for y.3 In this case, h is matched to look for a “dc component,” and
also zero-padded by a factor of 2. The zero-padding serves to simulate
acyclic convolution using circular convolution. The ﬁgure illustrates the
computation of the convolution of y and h:
y ∗h ∆=
N−1

n=0
y(n) · Flip(h) = [4, 3, 2, 1, 0, 1, 2, 3]
Note that a large peak is obtained in the convolution output at time 0.
This large peak (the largest possible if all signals are limited to [−1, 1]
in magnitude), indicates the matched ﬁlter has “found” the dc signal
starting at time 0. This peak would persist even if various sinusoids at
other frequencies and/or noise were added in.
Polynomial Multiplication
Note that when you multiply two polynomials together, their coeﬃcients
are convolved. To see this, let p(x) denote the mth-order polynomial
p(x) = p0 + p1x + p2x2 + · · · + pmxm
with coeﬃcients pi, and let q(x) denote the nth-order polynomial
q(x) = q0 + q1x + q2x2 + · · · + qnxn
with coeﬃcients qi. Then we have [17]
p(x)q(x)
=
p0q0 + (p0q1 + p1q0)x + (p0q2 + p1q1 + p2q0)x2
+(p0q3 + p1q2 + p2q1 + p3q0)x3
+(p0q4 + p1q3 + p2q2 + p3q1 + p4q0)x4 + · · ·
+(p0qn+m + p1qn+m−1 + p2qn+m−2 + pn+m−1q1 + pn+mq0)xn+m
3Matched ﬁltering is brieﬂy discussed in §8.8.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 154
8.2. SIGNAL OPERATORS
Denoting p(x)q(x) by
r(x) ∆= p(x)q(x) = r0 + r1x + r2x2 + · · · + rm+nxm+n,
we see that the ith coeﬃcient can be expressed as
ri
=
p0qi + p1qi−1 + p2qi−2 + · · · + pi−1q1 + piq0
=
i

j=0
pjqi−j =
∞

j=−∞
pjqi−j
∆=
(p ∗q)(i)
where pi and qi are doubly inﬁnite sequences, deﬁned as zero for i < 0
and i > m, n, respectively.
Multiplication of Decimal Numbers
Since decimal numbers are implicitly just polynomials in the powers of
10, e.g.,
3819 = 3 · 103 + 8 · 102 + 1 · 101 + 9 · 100
it follows that multiplying two numbers convolves their digits. The only
twist is that, unlike normal polynomial multiplication, we have carries.
That is, when a convolution result exceeds 10, we subtract 10 from the
result and add 1 to the digit in the next higher place.
8.2.4
Correlation
Deﬁnition: The correlation operator for two signals x and y in CN is
deﬁned as
(x ⋆y)n
∆=
N−1

m=0
x(m)y(m + n)
We may interpret the correlation operator as
(x ⋆y)n = ⟨Shift−n(y), x⟩
which is the coeﬃcient of projection onto x of y advanced by n samples
(shifted circularly to the left by n samples). The time shift n is called the
correlation lag, and x(m)y(m+n) is called a lagged product. Applications
of correlation are discussed in §8.8.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 155
8.2.5
Stretch Operator
Unlike all previous operators, the StretchL() operator maps a length
N signal to a length M ∆= LN signal, where L and N are integers. We
use “m” instead of “n” as the time index to underscore this fact.
Figure 8.4: Illustration of Stretch3(x).
Deﬁnition: A stretch by factor L is deﬁned by
StretchL,m(x) ∆=
 x(m/L),
m/L = integer
0,
m/L ̸= integer
Thus, to stretch a signal by the factor L, insert L −1 zeros between each
pair of samples.
An example of a stretch by factor three is shown in
Fig. 8.4. The example is
Stretch3([4, 1, 2]) = [4, 0, 0, 1, 0, 0, 2, 0, 0]
The stretch operator is used to describe and analyze upsampling, i.e.,
increasing the sampling rate by an integer factor.
8.2.6
Zero Padding
Deﬁnition: Zero padding consists of appending zeros to a signal.
It
maps a length N signal to a length M > N signal, but M need not be
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 156
8.2. SIGNAL OPERATORS
an integer multiple of N:
ZeroPadM,m(x) ∆=
 x(m),
0 ≤m ≤N −1
0,
N ≤m ≤M −1
For example,
ZeroPad10([1, 2, 3, 4, 5]) = [1, 2, 3, 4, 5, 0, 0, 0, 0, 0]
The above deﬁnition is natural when x(n) represents a signal starting at
time 0 and extending for N samples. If, on the other hand, we are zero-
padding a spectrum, or we have a time-domain signal which has nonzero
samples for negative time indices, then the zero padding is normally in-
serted between samples (N −1)/2 and (N + 1)/2 for N odd (note that
(N + 1)/2 = (N + 1)/2 −N = −(N −1)/2 mod N), and similarly for
even N. I.e., for spectra, zero padding is inserted at the point z = −1
(ω = πfs). Figure 8.5 illustrates this second form of zero padding. It is
also used in conjunction with zero-phase FFT windows (discussed a bit
further below).
Using Fourier theorems, we will be able to show that zero padding in
the time domain gives bandlimited interpolation in the frequency domain.
Similarly, zero padding in the frequency domain gives bandlimited inter-
polation in the time domain. This is how ideal sampling rate conversion
is accomplished.
It is important to note that bandlimited interpolation is ideal inter-
polation in digital signal processing.
8.2.7
Repeat Operator
Like the StretchL() operator, the RepeatL() operator maps a length
N signal to a length M ∆= LN signal:
Deﬁnition: The repeat L times operator is deﬁned by
RepeatL,m(x) ∆= x(m),
m = 0, 1, 2, . . . , M −1
where M ∆= LN. Thus, the RepeatL() operator simply repeats its input
signal L times.4 An example of Repeat2(x) is shown in Fig. 8.6. The
4You might wonder why we need this since all indexing is deﬁned modulo N already.
The answer is that RepeatL() formally expresses a mapping from the space of length
N signals to the space of length M = LN signals.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 157
a)
b)
c)
d)
Figure 8.5: Illustration of frequency-domain zero padding:
a) Original spectrum X = [3, 2, 1, 1, 2] plotted over the domain
k ∈[0, N −1] where N = 5 (i.e., as the spectral array would
normally exist in a computer array).
b) ZeroPad11(X).
c) The same signal X plotted over the domain k ∈[−(N −
1)/2, (N −1)/2] which is more natural for interpreting negative
frequencies.
d) ZeroPad11(X).
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 158
8.2. SIGNAL OPERATORS
example is
Repeat2([0, 2, 1, 4, 3, 1]) = [0, 2, 1, 4, 3, 1, 0, 2, 1, 4, 3, 1]
Repeat2
Figure 8.6: Illustration of Repeat2(x).
A frequency-domain example is shown in Fig. 8.7. Figure 8.7a shows
the original spectrum X, Fig. 8.7b shows the same spectrum plotted over
the unit circle in the z plane, and Fig. 8.7c shows Repeat3(X). The z = 1
point (dc) is on the right-rear face of the enclosing box. Note that when
viewed as centered about k = 0, X is a somewhat “triangularly shaped”
spectrum.
The repeating block can be considered to extend from the
point at z = 1 to the point far to the left, or it can be considered the
triangularly shaped “baseband” spectrum centered about z = 1.
The repeat operator is used to state the Fourier theorem
StretchL ↔RepeatL
That is, when you stretch a signal by the factor L, its spectrum is repeated
L times around the unit circle.
8.2.8
Downsampling Operator
Deﬁnition: Downsampling by L is deﬁned as taking every Lth sample,
starting with sample 0:
SelectL,m(x) ∆= x(mL), m = 0, 1, 2, . . . , M −1
(N = LM, x ∈CN)
The SelectL() operator maps a length N = LM signal down to a
length M signal. It is the inverse of the StretchL() operator (but not
vice versa), i.e.,
SelectL(StretchL(x))
=
x
StretchL(SelectL(x))
̸=
x
(in general).
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 159
0
-5
-10
-15
-20
0
5
10
15
0
0.2
0.4
0.6
0.8
1
1.2
a)
b)
c)
Figure 8.7: Illustration of Repeat3(X).
a) Conventional plot of X.
b) Plot of X over the unit circle in the z plane.
c) Repeat3(X).
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 160
8.2. SIGNAL OPERATORS
The stretch and downsampling operations do not commute because they
are linear time-varying operators.
They can be modeled using time-
varying switches controlled by the sample index n.
Figure 8.8: Illustration of Select2(x).
The white-ﬁlled circles
indicate the retained samples while the black-ﬁlled circles indicate
the discarded samples.
An example of Select2(x) is shown in Fig. 8.8. The example is
Select2([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) = [0, 2, 4, 6, 8]
8.2.9
Alias Operator
Aliasing occurs when a signal is undersampled. If the signal sampling
rate fs is too low, we get frequency-domain aliasing.
The topic of aliasing normally arises in the context of sampling a
continuous-time signal. Shannon’s Sampling Theorem says that we will
have no aliasing due to sampling as long as the sampling rate is higher
than twice the highest frequency present in the signal being sampled.
In this chapter, we are considering only discrete-time signals, in order
to keep the math as simple as possible. Aliasing in this context occurs
when a discrete-time signal is decimated to reduce its sampling rate. You
can think of continuous-time sampling as the limiting case for which the
starting sampling rate is inﬁnity.
An example of aliasing is shown in Fig. 8.9.
In the ﬁgure, the high-frequency sinusoid is indistinguishable from
the lower frequency sinusoid due to aliasing. We say the higher frequency
aliases to the lower frequency. Undersampling in the frequency domain
gives rise to time-domain aliasing. If time or frequency is not speciﬁed,
the term “aliasing” normally means frequency-domain aliasing.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 161
1
2
3
4
5
6
7
8
9
-1
-0.5
0.5
1
Figure 8.9: Example of aliasing due to undersampling in time.
The aliasing operator is deﬁned by
AliasL,m(x) ∆=
N−1

l=0
x

m + lN
L

, m = 0, 1, 2, . . . , M−1
(N = LM, x ∈CN)
Like the SelectL() operator, the AliasL() operator maps a length N =
LM signal down to a length M signal.
The way to think of it is to
partition the original N samples in to L blocks of length M, with the
ﬁrst block extending from sample 0 to sample M −1, the second block
from M to 2M −1, etc. Then just add up the blocks. This process is
called aliasing. If the original signal x is a time signal, it is called time-
domain aliasing; if it is a spectrum, we call it frequency-domain aliasing,
or just aliasing. Note that aliasing is not invertible. Once the blocks are
added together, it is not possible to recover the original blocks, in general.
For example,
Alias2([0, 1, 2, 3, 4, 5])
=
[0, 1, 2] + [3, 4, 5] = [3, 5, 7]
Alias3([0, 1, 2, 3, 4, 5])
=
[0, 1] + [2, 3] + [4, 5] = [6, 9]
The alias operator is used to state the Fourier theorem
SelectL ↔AliasL
That is, when you decimate a signal by the factor L, its spectrum is
aliased by the factor L.
Figure 8.10 shows the result of Alias2 applied to Repeat3(X) from
Fig. 8.7c. Imagine the spectrum of Fig. 8.10a as being plotted on a piece
of paper rolled to form a cylinder, with the edges of the paper meeting at
z = 1 (upper right corner of Fig. 8.10a). Then the Alias2 operation can
be simulated by rerolling the cylinder of paper to cut its circumference in
half. That is, reroll it so that at every point, two sheets of paper are in
contact at all points on the new, narrower cylinder. Now, simply add the
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 162
8.2. SIGNAL OPERATORS
a)
b)
c)
d)
e)
f)
Figure 8.10: Illustration of aliasing in the frequency domain.
a) Repeat3(X) from Fig. 8.7c.
b) First half of the original unit circle (0 to π) wrapped around
the new, smaller unit circle (which is magniﬁed to the original
size).
c) Second half (π to 2π), also wrapped around the new unit
circle.
d) Overlay of components to be summed.
e) Sum of components (the aliased spectrum).
f) Both sum and overlay.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 163
values on the two overlapping sheets together, and you have the Alias2
of the original spectrum on the unit circle. To alias by 3, we would shrink
the cylinder further until the paper edges again line up, giving three layers
of paper in the cylinder, and so on.
Figure 8.10b shows what is plotted on the ﬁrst circular wrap of the
cylinder of paper, and Fig. 8.10c shows what is on the second wrap.
These are overlaid in Fig. 8.10d and added together in Fig. 8.10e. Finally,
Fig. 8.10f shows both the addition and the overlay of the two components.
We say that the second component (Fig. 8.10c) “aliases” to new frequency
components, while the ﬁrst component (Fig. 8.10b) is considered to be at
its original frequencies. If the unit circle of Fig. 8.10a covers frequencies
0 to fs, all other unit circles (Fig. 8.10b-c) cover frequencies 0 to fs/2.
In general, aliasing by the factor K corresponds to a sampling-rate
reduction by the factor K. To prevent aliasing when reducing the sam-
pling rate, an anti-aliasing lowpass ﬁlter is generally used. The lowpass
ﬁlter attenuates all signal components at frequencies outside the interval
[−fs/(2K), fs/(2K)].
If they are not ﬁltered out, they will alias, and
aliasing is generally non-invertible.
Conceptually, the unit circle is reduced by Alias2 to a unit circle half
the original size, where the two halves are summed. The inverse of alias-
ing is then “repeating” which should be understood as increasing the unit
circle circumference using “periodic extension” to generate “more spec-
trum” for the larger unit circle. In the time domain, downsampling is
the inverse of the stretch operator. All of these relationships are precise
only for integer stretch/downsampling/aliasing/repeat factors; in contin-
uous time, the restriction to integer factors is removed, and we obtain the
(simpler) similarity theorem (proved in §8.9).
8.3
Even and Odd Functions
Some of the Fourier theorems can be succinctly expressed in terms of even
and odd symmetries.
Deﬁnition: A function f(n) is said to be even if f(−n) = f(n). An
even function is also symmetric, but the term symmetric applies also to
functions symmetric about a point other than 0.
Deﬁnition: A function f(n) is said to be odd if f(−n) = −f(n). An
odd function is also called antisymmetric.
Note that every odd function f(n) must satisfy f(0) = 0. Moreover,
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 164
8.3.
EVEN AND ODD FUNCTIONS
for any x ∈CN with N even, we also have x(N/2) = 0 since x(N/2) =
−x(−N/2) = −x(−N/2 + N) = −x(N/2), i.e., N/2 and −N/2 index the
same point.
Theorem: Every function f(n) can be decomposed into a sum of its
even part fe(n) and odd part fo(n), where
fe(n)
∆=
f(n) + f(−n)
2
fo(n)
∆=
f(n) −f(−n)
2
Proof: In the above deﬁnitions, fe is even and fo is odd by construc-
tion. Summing, we have
fe(n) + fo(n) = f(n) + f(−n)
2
+ f(n) −f(−n)
2
= f(n)
Theorem: The product of even functions is even, the product of odd
functions is even, and the product of an even times an odd function is
odd.
Proof: Readily shown.
Since even times even is even, odd times odd is even, and even times
odd is odd, we can think of even as (+) and odd as (−): (+) · (+) = (+),
(−) · (−) = (+), and (+) · (−) = (−) · (+) = (−).
Example: cos(ωkn) is an even signal since cos(−θ) = cos(θ).
Example: sin(ωkn) is an odd signal since sin(−θ) = −sin(θ).
Example: cos(ωkn) · sin(ωln) is odd (even times odd).
Example: sin(ωkn) · sin(ωln) is even (odd times odd).
Theorem: The sum of all the samples of an odd signal xo in CN is
zero.
Proof: This is readily shown by writing the sum as xo(0) + [xo(1) +
xo(−1)] + · · · + x(N/2), where the last term only occurs when N is even.
Each term so written is zero for an odd signal xo.
Example: For all DFT sinusoidal frequencies ωk = 2πk/N,
N−1

n=0
sin(ωkn) cos(ωkn) = 0, k = 0, 1, 2, . . . , N −1
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 165
More generally,
N−1

n=0
xe(n)xo(n) = 0
for any even signal xe and odd signal xo in CN.
8.4
The Fourier Theorems
In this section the main Fourier theorems are stated and proved. It is
no small matter how simple these theorems are in the DFT case relative
to the other three cases (DTFT, Fourier transform, and Fourier series).
When inﬁnite summations or integrals are involved, the conditions for the
existence of the Fourier transform can be quite diﬃcult to characterize
mathematically. Mathematicians have expended a considerable eﬀort on
such questions. By focusing primarily on the DFT case, we are able to
study the essential concepts conveyed by the Fourier theorems without
getting involved with mathematical diﬃculties.
8.4.1
Linearity
Theorem: For any x, y ∈CN and α, β ∈C, the DFT satisﬁes
αx + βy ↔αX + βY
Thus, the DFT is a linear operator.
Proof:
DFTk(αx + βy)
∆=
N−1

n=0
[αx(n) + βy(n)]e−j2πnk/N
=
N−1

n=0
αx(n)e−j2πnk/N +
N−1

n=0
βy(n)e−j2πnk/N
=
α
N−1

n=0
x(n)e−j2πnk/N + β
N−1

n=0
y(n)e−j2πnk/N
∆=
αX + βY
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 166
8.4.
THE FOURIER THEOREMS
8.4.2
Conjugation and Reversal
Theorem: For any x ∈CN,
x ↔Flip(X)
Proof:
DFTk(x)
∆=
N−1

n=0
x(n)e−j2πnk/N =
N−1

n=0
x(n)ej2πnk/N
=
N−1

n=0
x(n)e−j2πn(−k)/N ∆= Flipk(X)
Theorem: For any x ∈CN,
Flip(x) ↔X
Proof: Making the change of summation variable m ∆= N −n, we get
DFTk(Flip(x))
∆=
N−1

n=0
x(N −n)e−j2πnk/N =
1

m=N
x(m)e−j2π(N−m)k/N
=
N−1

m=0
x(m)ej2πmk/N =
N−1

m=0
x(m)e−j2πmk/N ∆= X(k)
Theorem: For any x ∈CN,
Flip(x) ↔Flip(X)
Proof:
DFTk[Flip(x)]
∆=
N−1

n=0
x(N −n)e−j2πnk/N =
N−1

m=0
x(m)e−j2π(N−m)k/N
=
N−1

m=0
x(m)ej2πmk/N ∆= X(−k) ∆= Flipk(X)
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 167
Corollary: For any x ∈RN,
Flip(x) ↔X
(x real)
Proof: Picking up the previous proof at the third formula, remember-
ing that x is real,
N−1

n=0
x(n)ej2πnk/N =
N−1

n=0
x(n)e−j2πnk/N =
N−1

n=0
x(n)e−j2πnk/N ∆= X(k)
when x(n) is real.
Thus, conjugation in the frequency domain corresponds to reversal in
the time domain. Another way to say it is that negating spectral phase
ﬂips the signal around backwards in time.
Corollary: For any x ∈RN,
Flip(X) = X
(x real)
Proof: This follows from the previous two cases.
Deﬁnition: The property X(−k) = X(k) is called Hermitian sym-
metry or “conjugate symmetry.” If X(−k) = −X(k), it may be called
skew-Hermitian.
Another way to state the preceding corollary is
x ∈RN ↔X is Hermitian
8.4.3
Symmetry
In the previous section, we found Flip(X) = X when x is real. This fact
is of high practical importance. It says that the spectrum of every real
signal is Hermitian. Due to this symmetry, we may discard all negative-
frequency spectral samples of a real signal and regenerate them later if
needed from the positive-frequency samples. Also, spectral plots of real
signals are normally displayed only for positive frequencies; e.g., spectra
of sampled signals are normally plotted over the range 0 Hz to fs/2 Hz.
On the other hand, the spectrum of a complex signal must be shown,
in general, from −fs/2 to fs/2 (or from 0 to fs), since the positive and
negative frequency components of a complex signal are independent.
Theorem: If x ∈RN, re {X} is even and im {X} is odd.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 168
8.4.
THE FOURIER THEOREMS
Proof: This follows immediately from the conjugate symmetry of X
for real signals x.
Theorem: If x ∈RN, |X| is even and ̸ X is odd.
Proof: This follows immediately from the conjugate symmetry of X
expressed in polar form X(k) = |X(k)| ej̸
X(k).
The conjugate symmetry of spectra of real signals is perhaps the most
important symmetry theorem. However, there are a few more we can
readily show.
Theorem: An even signal has an even transform:
x even ↔X even
Proof: Express x in terms of its real and imaginary parts by x ∆=
xr + jxi. Note that for a complex signal x to be even, both its real and
imaginary parts must be even. Then
X(k)
∆=
N−1

n=0
x(n)e−jωkn
=
N−1

n=0
[xr(n) + jxi(n)] cos(ωkn) −j[xr(n) + jxi(n)] sin(ωkn)
=
N−1

n=0
[xr(n) cos(ωkn) + xi(n) sin(ωkn)] + j[xi(n) cos(ωkn) −xr(n) sin(ωkn)]
=
N−1

n=0
even · even +
N−1

n=0
even · odd



sums to 0
+j
N−1

n=0
even · even −j
N−1

n=0
even · odd



sums to 0
=
N−1

n=0
even · even + j
N−1

n=0
even · even = even + j · even = even
Theorem: A real even signal has a real even transform:
x real and even ↔X real and even
Proof: This follows immediately from setting xi(n) = 0 in the preced-
ing proof and seeing that the DFT of a real and even function reduces to
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 169
a type of cosine transform5,
X(k) =
N−1

n=0
x(n) cos(ωkn),
or we can show it directly:
X(k)
∆=
N−1

n=0
x(n)e−jωkn =
N−1

n=0
x(n) cos(ωkn) + j
N−1

n=0
x(n) sin(ωkn)
=
N−1

n=0
x(n) cos(ωkn)
() even · odd = ) odd = 0)
=
N−1

n=0
even · even =
N−1

n=0
even = even
Deﬁnition: A signal with a real spectrum (such as a real, even signal)
is often called a zero phase signal. However, note that when the spectrum
goes negative (which it can), the phase is really π, not 0. Nevertheless,
it is common to call such signals “zero phase, ” even though the phase
switches between 0 and π at the zero-crossings of the spectrum. Such
zero-crossings typically occur at low amplitude in practice, such as in the
“sidelobes” of the DTFT of an FFT window.
8.4.4
Shift Theorem
Theorem: For any x ∈CN and any integer ∆,
DFTk[Shift∆(x)] = e−jωk∆X(k)
5The discrete cosine transform (DCT) used often in applications is actually deﬁned
somewhat diﬀerently, but the basic principles are the same
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 170
8.4.
THE FOURIER THEOREMS
Proof:
DFTk[Shift∆(x)]
∆=
N−1

n=0
x(n −∆)e−j2πnk/N
=
N−1−∆

m=−∆
x(m)e−j2π(m+∆)k/N
(m ∆= n −∆)
=
N−1

m=0
x(m)e−j2πmk/Ne−j2πk∆/N
=
e−j2π∆k/N
N−1

m=0
x(m)e−j2πmk/N
∆=
e−jωk∆X(k)
The shift theorem says that a delay in the time domain corresponds to a
linear phase term in the frequency domain. More speciﬁcally, a delay of ∆
samples in the time waveform corresponds to the linear phase term e−jωk∆
multiplying the spectrum, where ωk
∆= 2πk/N. (To consider ωk as radians
per second instead of radians per sample, just replace ∆by ∆T so that
the delay is in seconds instead of samples.) Note that spectral magnitude
is unaﬀected by a linear phase term. That is,
e−jωk∆X(k)
 = |X(k)|.
Linear Phase Terms
The reason e−jωk∆is called a linear phase term is that its phase is a
linear function of frequency:
̸ e−jωk∆= −∆· ωk
Thus, the slope of the phase versus radian frequency is −∆. In general,
the time delay in samples equals minus the slope of the linear phase term.
If we express the original spectrum in polar form as
X(k) = G(k)ejΘ(k),
where G and Θ are the magnitude and phase of X, respectively (both
real), we can see that a linear phase term only modiﬁes the spectral phase
Θ(k):
e−jωk∆X(k) ∆= e−jωk∆G(k)ejΘ(k) = G(k)ej[Θ(k)−ωk∆]
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 171
where ωk
∆= 2πk/N. A positive time delay (waveform shift to the right)
adds a negatively sloped linear phase to the original spectral phase. A
negative time delay (waveform shift to the left) adds a positively sloped
linear phase to the original spectral phase. If we seem to be belaboring
this relationship, it is because it is one of the most useful in practice.
Deﬁnition: A signal is said to be linear phase signal if its phase is
of the form
Θ(ωk) = ±∆· ωk ± πI(ωk)
where I(ωk) is an indicator function which takes on the values 0 or 1.
Application of the Shift Theorem to FFT Windows
In practical spectrum analysis, we most often use the fast Fourier trans-
form6 (FFT) together with a window function w(n), n = 0, 1, 2, . . . , N−1.
Windows are normally positive (w(n) > 0), symmetric about their mid-
point, and look pretty much like a “bell curve.” A window multiplies the
signal x being analyzed to form a windowed signal xw(n) = w(n)x(n), or
xw = w · x, which is then analyzed using the FFT. The window serves
to taper the data segment gracefully to zero, thus eliminating spectral
distortions due to suddenly cutting oﬀthe signal in time. Windowing is
thus appropriate when x is a short section of a longer signal.
Theorem: Real symmetric FFT windows are linear phase.
Proof: The midpoint of a symmetric signal can be translated to the
time origin to create an even signal. As established previously, the DFT
of a real even signal is real and even. By the shift theorem, the DFT of the
original symmetric signal is a real even spectrum multiplied by a linear
phase term. A spectrum whose phase is a linear function of frequency
(with possible discontinuities of π radians), is linear phase by deﬁnition.
8.4.5
Convolution Theorem
Theorem: For any x, y ∈CN,
x ∗y ↔X · Y
6The FFT is just a fast implementation of the DFT.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 172
8.4.
THE FOURIER THEOREMS
Proof:
DFTk(x ∗y)
∆=
N−1

n=0
(x ∗y)ne−j2πnk/N
∆=
N−1

n=0
N−1

m=0
x(m)y(n −m)e−j2πnk/N
=
N−1

m=0
x(m)
N−1

n=0
y(n −m)e−j2πnk/N



e−j2πmk/NY (k)
=
N−1

m=0
x(m)e−j2πmk/N

Y (k)
(by the Shift Theorem)
∆=
X(k)Y (k)
This is perhaps the most important single Fourier theorem of all. It is the
basis of a large number of applications of the FFT. Since the FFT pro-
vides a fast Fourier transform, it also provides fast convolution, thanks
to the convolution theorem.
It turns out that using the FFT to per-
form convolution is really more eﬃcient in practice only for reasonably
long convolutions, such as N > 100. For much longer convolutions, the
savings become enormous compared with “direct” convolution. This hap-
pens because direct convolution requires on the order of N2 operations
(multiplications and additions), while FFT-based convolution requires on
the order of N lg(N) operations.
The following simple Matlab example illustrates how much faster con-
volution can be performed using the FFT:
>> N = 1024;
% FFT much faster at this length
>> t = 0:N-1;
% [0,1,2,...,N-1]
>> h = exp(-t);
% filter impulse reponse = sampled exponential
>> H = fft(h);
% filter frequency response
>> x = ones(1,N);
% input = dc (any example will do)
>> t0 = clock; y = conv(x,h); t1 = etime(clock,t0);
% Direct
>> t0 = clock; y = ifft(fft(x) .* H); t2 = etime(clock,t0); % FFT
>> sprintf([’Elapsed time for direct convolution = %f sec\n’,...
’Elapsed time for FFT convolution = %f sec\n’,...
’Ratio = %f (Direct/FFT)’],t1,t2,t1/t2)
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 173
ans =
Elapsed time for direct convolution = 8.542129 sec
Elapsed time for FFT convolution = 0.075038 sec
Ratio = 113.837376 (Direct/FFT)
8.4.6
Dual of the Convolution Theorem
The Dual7 of the Convolution Theorem says that multiplication in the
time domain is convolution in the frequency domain:
Theorem:
x · y ↔1
N X ∗Y
Proof: The steps are the same as in the Convolution Theorem.
This theorem also bears on the use of FFT windows.
It says that
windowing in the time domain corresponds to smoothing in the frequency
domain. That is, the spectrum of w · x is simply X ﬁltered by W, or,
W ∗X. This smoothing reduces sidelobes associated with the rectangular
window (which is the window one gets implicitly when no window is
explicitly used). FFT windows are covered in Music 420.
8.4.7
Correlation Theorem
Theorem: For all x, y ∈CN,
x ⋆y ↔X · Y
Proof:
(x ⋆y)n
∆=
N−1

m=0
x(m)y(n + m)
=
N−1

m=0
x(−m)y(n −m)
(m ←−m)
=
(Flip(x) ∗y)n
↔
X · Y
7In general, the dual of any Fourier operation is obtained by interchanging time
and frequency.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 174
8.4.
THE FOURIER THEOREMS
where the last step follows from the Convolution Theorem and the result
Flip(x) ↔X from the section on Conjugation and Reversal.
8.4.8
Power Theorem
Theorem: For all x, y ∈CN,
⟨x, y⟩= 1
N ⟨X, Y ⟩
Proof:
⟨x, y⟩
∆=
N−1

n=0
x(n)y(n) = (y ⋆x)0 = DFT−1
0 (Y · X)
=
1
N
N−1

k=0
X(k)Y (k) ∆= 1
N ⟨X, Y ⟩
Note that the power theorem would be more elegant (⟨x, y⟩= ⟨X, Y ⟩) if
the DFT were deﬁned as the coeﬃcient of projection onto the normalized
DFT sinusoid ˜sk(n) ∆= sk(n)/
√
N.
8.4.9
Rayleigh Energy Theorem (Parseval’s Theorem)
Theorem: For any x ∈CN,
∥x ∥2 = 1
N ∥X ∥2
I.e.,
N−1

n=0
|x(n)|2 = 1
N
N−1

k=0
|X(k)|2
Proof: This is a special case of the Power Theorem.
Note that again the relationship would be cleaner (∥x ∥= ∥X ∥) if
we were using the normalized DFT.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 175
8.4.10
Stretch Theorem (Repeat Theorem)
Theorem: For all x ∈CN,
StretchL(x) ↔RepeatL(X)
Proof: Recall the stretch operator:
StretchL,m(x) ∆=
 x(m/L),
m/L = integer
0,
m/L ̸= integer
Let y ∆= StretchL(x), where y ∈CM, M = LN. Also deﬁne the new
denser frequency grid associated with length M by ω′
k
∆= 2πk/M, with
ωk = 2πk/N as usual. Then
Y (k) ∆=
M−1

m=0
y(m)e−jω′
km =
N−1

n=0
x(n)e−jω′
knL
(n ∆= m/L)
But
ω′
kL ∆= 2πk
M L = 2πk
N
= ωk
Thus, Y (k) = X(k), and by the modulo indexing of X, L copies of X are
generated as k goes from 0 to M −1 = LN −1.
8.4.11
Downsampling Theorem (Aliasing Theorem)
Theorem: For all x ∈CN,
SelectL(x) ↔1
LAliasL(X)
Proof: Let k′ ∈[0, M −1] denote the frequency index in the aliased
spectrum, and let Y (k′) ∆= AliasL,k′(X). Then Y is length M = N/L,
where L is the downsampling factor. We have
Y (k′)
∆=
AliasL,k′(X) ∆=
L−1

l=0
Y (k′ + lM),
k′ = 0, 1, 2, . . . , M −1
∆=
L−1

l=0
N−1

n=0
x(n)e−j2π(k′+lM)n/N
∆=
N−1

n=0
x(n)e−j2πk′n/N
L−1

l=0
e−j2πlnM/N
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 176
8.4.
THE FOURIER THEOREMS
Since M/N = L, the sum over l becomes
L−1

l=0
e−j2πlMn/N =
L−1

l=0

e−j2πn/L	l
=
1 −e−j2πn
1 −e−j2πn/L =
 L,
n = 0
mod L
0,
n ̸= 0
mod L
using the closed form expression for a geometric series derived earlier. We
see that the sum over L eﬀectively samples x every L samples. This can
be expressed in the previous formula by deﬁning m ∆= n/L which ranges
only over the nonzero samples:
AliasL,k′(X)
=
N−1

n=0
x(n)e−j2πk′n/N
L−1

l=0
e−j2πln/L
=
L
N/L−1

m=0
x(mL)e−j2πk′(mL)/N
(m ∆= n/L)
∆=
L
M−1

m=0
x(mL)e−j2πk′m/M
∆= L
M−1

m=0
SelectL,m(x)e−j2πk′m/M
∆=
L · DFTk′(SelectL(x))
Since the above derivation also works in reverse, the theorem is proved.
Here is an illustration of the Downsampling Theorem in Matlab:
>> N=4;
>> x = 1:N;
>> X = fft(x);
>> x2 = x(1:2:N);
>> fft(x2)
% FFT(Decimate(x,2))
ans =
4
-2
>> (X(1:N/2) + X(N/2 + 1:N))/2
% (1/2) Alias(X,2)
ans =
4.0000
-2.0000
An illustration of aliasing in the frequency domain is shown in Fig. 8.10.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 177
8.4.12
Zero Padding Theorem
A fundamental tool in practical spectrum analysis is zero padding. This
theorem shows that zero padding in the time domain corresponds to ideal
interpolation in the frequency domain:
Let x ∈CN and deﬁne y = ZeroPadM(x).
Then y ∈CM with
M ≥N. Denote the original frequency index by k, where ωk
∆= 2πk/N
and the new frequency index by k′, where ωk′ ∆= 2πk′/M.
Deﬁnition: The ideal bandlimited interpolation of a spectrum X(ωk) ∆=
DFTk(x), x ∈CN, to an arbitrary new frequency ω ∈[−π, π) is deﬁned
as
X(ω) ∆=
N−1

n=0
x(n)e−jωn
Note that this is just the deﬁnition of the DFT with ωk replaced by ω.
That is, the spectrum is interpolated by projecting onto the new sinusoid
exactly as if it were a DFT sinusoid. This makes the most sense when
x is assumed to be N samples of a time-limited signal. That is, if the
signal really is zero outside of the time interval [0, N −1], then the inner
product between it and any sinusoid will be exactly as in the equation
above. Thus, for time limited signals, this kind of interpolation is ideal.
Deﬁnition: The interpolation operator interpolates a signal by an
integer factor L. That is,
InterpL,k′(X) ∆= X(ωk′),
ωk′ = 2πk′/M, k′ = 0, 1, 2, . . . , M−1, M ∆= LN
Since X(ωk) ∆= DFTN,k(x) is initially only deﬁned over the N roots of
unity, while X(ωk′) is deﬁned over M = LN roots of unity, we deﬁne
X(ωk′) for ωk′ ̸= ωk by ideal bandlimited interpolation.
Theorem: For any x ∈CN
ZeroPadLN(x) ↔InterpL(X)
Proof: Let M = LN with L ≥1. Then
DFTM,k′(ZeroPadM(x)) =
N−1

m=0
x(m)e−j2πmk′/M ∆= X(ωk′) = InterpL(X)
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 178
8.4.
THE FOURIER THEOREMS
8.4.13
Bandlimited Interpolation in Time
The dual of the Zero-Padding Theorem states formally that zero padding
in the frequency domain corresponds to ideal bandlimited interpolation in
the time domain. However, we have not precisely deﬁned ideal bandlim-
ited interpolation in the time domain. Therefore, we’ll let the dual of the
Zero-Padding Theorem provide its deﬁnition:
Deﬁnition: For all x ∈CN and any integer L ≥1,
InterpL(x) ∆= IDFT(ZeroPadLN(X))
where the zero-padding is of the frequency-domain type, as described
earlier and illustrated in Fig. 8.5.
It is instructive to interpret the Interpolation Theorem in terms of
the Stretch Theorem StretchL(x) ↔RepeatL(X). To do this, it is
convenient to deﬁne a “zero-centered rectangular window” operator:
Deﬁnition: For any X ∈CN and any odd integer M < N we deﬁne
the length M even rectangular windowing operation by
RectWinM,k(X) ∆=
 X(k),
−M−1
2
≤k ≤M−1
2
0,
M+1
2
≤|k| ≤N
2
Thus, the “zero-phase rectangular window,” when applied to a spectrum
X, sets the spectrum to zero everywhere outside a zero-centered interval
of M samples. Note that RectWinM(X) is the ideal lowpass ﬁltering op-
eration in the frequency domain, where the lowpass “cut-oﬀfrequency” in
radians per sample is ωc = 2π[(M −1)/2]/N. With this we can eﬃciently
show the basic theorem of ideal bandlimited interpolation:
Theorem: For x ∈CN,
InterpL(x) = IDFT(RectWinN(DFT(StretchL(x))))
In other words, ideal bandlimited interpolation of x by the factor L may
be carried out by ﬁrst stretching x by the factor L (i.e., inserting L −1
zeros between adjacent samples of x), taking the DFT, applying the ideal
lowpass ﬁlter, and performing the inverse DFT.
Proof: First, recall that StretchL(x) ↔RepeatL(X), that is, stretch-
ing a signal by the factor L gives a new signal y = StretchL(x) which
has a spectral grid L times the density of X, and the spectrum Y con-
tains L copies of X repeated around the unit circle. The “baseband copy”
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 179
of X can be deﬁned as the width N sequence centered about frequency
zero. Therefore, if we can use an “ideal ﬁlter” to “pass” the baseband
spectral copy and zero out all others, we can convert RepeatL(X) to
ZeroPadLN(X). I.e.,
RectWinN(RepeatL(X)) = ZeroPadLN(X) ↔InterpL(x)
where the last step is by deﬁnition of time-domain ideal bandlimited
interpolation.
Note that the deﬁnition of ideal bandlimited time-domain interpola-
tion in this section is only really ideal for signals which are periodic in N
samples. To see this, consider that the rectangular windowing operation
in the frequency domain corresponds to cyclic convolution in the time
domain,8 and cyclic convolution is only the same as acyclic convolution
when one of the signals is truly periodic in N samples. Since all spec-
tra X ∈CN are truly periodic in N samples, there is no problem with
the deﬁnition of ideal spectral interpolation used in connection with the
Zero-Padding Theorem. However, for a more practical deﬁnition of ideal
time-domain interpolation, we should use instead the dual of the Zero-
Padding Theorem for the DTFT case. Nevertheless, for signals which are
exactly periodic in N samples (a rare situation), the present deﬁnition is
ideal.
8.5
Conclusions
For the student interested in pursuing further the topics of this reader,
see [18, 19].
8.6
Acknowledgement
Thanks to Craig Stuart Sapp (craig@ccrma.stanford.edu) for contributing
Figures 8.2, 8.3, 8.4, 8.6, 8.7, 8.8, 8.9, and 8.10.
8The inverse DFT of the rectangular window here is an aliased, discrete-time coun-
terpart of the sinc function which is deﬁned as “sin(πt)/πt.”
The sinc function is
covered in Music 420.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 180
8.7. APPENDIX A: LINEAR TIME-INVARIANT FILTERS AND
CONVOLUTION
8.7
Appendix A: Linear Time-Invariant Filters
and Convolution
A reason for the importance of convolution is that every linear time-
invariant system9 can be represented by a convolution. Thus, in the con-
volution equation
y = h ∗x
we may interpret x as the input signal to a ﬁlter, y as the output signal,
and h as the digital ﬁlter, as shown in Fig. 8.11.
x(n)
y(n)
h
Figure 8.11: The ﬁlter interpretation of convolution.
The impulse or “unit pulse” signal is deﬁned by
δ(n) ∆=
 1,
n = 0
0,
n ̸= 0
For example, for N = 4, δ = [1, 0, 0, 0]. The impulse signal is the identity
element under convolution, since
(x ∗δ)n
∆=
N−1

m=0
δ(m)x(n −m) = x(n)
If we set x = δ in the ﬁlter equation above, we get
y = h ∗x = h ∗δ = h
Thus, h is the impulse response of the ﬁlter.
It turns out in general that every linear time-invariant (LTI) system
(ﬁlter) is completely described by its impulse response. No matter what
the LTI system is, we can give it an impulse, record what comes out,
call it h(n), and implement the system by convolving the input signal x
with the impulse response h. In other words, every LTI system has a
convolution representation in terms of its impulse response.
9We use the term “system” interchangeably with “ﬁlter.”
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 181
8.7.1
LTI Filters and the Convolution Theorem
Deﬁnition: The frequency response of an LTI ﬁlter is deﬁned as the
Fourier transform of its impulse response. In particular, for ﬁnite, discrete-
time signals h ∈CN, the sampled frequency response is deﬁned as
H(ωk) ∆= DFTk(h)
The complete frequency response is deﬁned using the DTFT, i.e.,
H(ω) ∆= DTFTω(ZeroPad∞(h)) ∆=
N−1

n=0
h(n)e−jωn
where we used the fact that h(n) is zero for n < 0 and n > N −1 to trun-
cate the summation limits. Thus, the inﬁnitely zero-padded DTFT can
be obtained from the DFT by simply replacing ωk by ω. In principle, the
continuous frequency response H(ω) is being obtained using “time-limited
interpolation in the frequency domain” based on the samples H(ωk). This
interpolation is possible only when the frequency samples H(ωk) are suf-
ﬁciently dense: for a length N ﬁnite-impulse-response (FIR) ﬁlter h, we
require at least N samples around the unit circle (length N DFT) in or-
der that H(ω) be suﬃciently well sampled in the frequency domain. This
is of course the dual of the usual sampling rate requirement in the time
domain.10
Deﬁnition: The amplitude response of a ﬁlter is deﬁned as the mag-
nitude of the frequency response
G(k) ∆= |H(ωk)|
From the convolution theorem, we can see that the amplitude response
G(k) is the gain of the ﬁlter at frequency ωk, since
|Y (k)| = |H(ωk)X(k)| = G(k) |X(k)|
10Note that we normally say the sampling rate in the time domain must be higher
than twice the highest frequency in the frequency domain. From the point of view of
this reader, however, we may say instead that sampling rate in the time domain must
be greater than the full spectral bandwidth of the signal, including both positive and
negative frequencies. From this simpliﬁed point of view, “sampling rate = bandwidth
supported.”
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 182
8.8. APPENDIX B: STATISTICAL SIGNAL PROCESSING
Deﬁnition: The phase response of a ﬁlter is deﬁned as the phase of
the frequency response
Θ(k) ∆= ̸ H(ωk)
From the convolution theorem, we can see that the phase response Θ(k)
is the phase-shift added by the ﬁlter to an input sinusoidal component at
frequency ωk, since
̸ Y (k) = ̸ [H(ωk)X(k)] = ̸ H(ωk) + ̸ X(k)
The subject of this section is developed in detail in [4].
8.8
Appendix B: Statistical Signal Processing
The correlation operator deﬁned above plays a major role in statistical
signal processing. This section gives a short introduction to some of the
most commonly used elements. The student interested in mastering the
concepts introduced brieﬂy below may consider taking EE 278 in the
Electrical Engineering Department. For further reading, see [12, 20].
8.8.1
Cross-Correlation
Deﬁnition: The circular cross-correlation of two signals x and y in CN
may be deﬁned by
rxy(l) ∆= 1
N (x ⋆y)(l) ∆= 1
N
N−1

n=0
x(n)y(n + l), l = 0, 1, 2, . . . , N −1
(cross-correlation)
(Note carefully above that “l” is an integer variable, not the constant
1.)
The term “cross-correlation” comes from statistics, and what we
have deﬁned here is more properly called the “sample cross-correlation,”
i.e., it is an estimator of the true cross-correlation which is a statistical
property of the signal itself. The estimator works by averaging lagged
products x(n)y(n + l).
The true statistical cross-correlation is the so-
called expected value of the lagged products in random signals x and y,
which may be denoted E{x(n)y(n + l)}. In principle, the expected value
must be computed by averaging (n)y(n + l) over many realizations of the
stochastic process x and y. That is, for each “roll of the dice” we obtain
x(·) and y(·) for all time, and we can average x(n)y(n + l) across all
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 183
realizations to estimate the expected value of x(n)y(n + l). This is called
an “ensemble average” across realizations of a stochastic process. If the
signals are stationary (which primarily means their statistics are time-
invariant), then we may average across time to estimate the expected
value. In other words, for stationary noise-like signals, time averages equal
ensemble averages. The above deﬁnition of the sample cross-correlation
is only valid for stationary stochastic processes.
The DFT of the cross-correlation is called the cross-spectral density,
or “cross-power spectrum,” or even simply “cross-spectrum.”
Normally in practice we are interested in estimating the true cross-
correlation between two signals, not the circular cross-correlation which
results naturally in a DFT setting. For this, we may deﬁne instead the
unbiased cross-correlation
ˆrxy(l) ∆=
1
N −1 −l
N−l

n=0
x(n)y(n + l), l = 0, 1, 2, . . . , L −1
where we chose L << N (e.g. L =
√
N) in order to have enough lagged
products at the highest lag that a reasonably accurate average is obtained.
The term “unbiased” refers to the fact that we are dividing the sum by
N −l rather than N.
Note that instead of ﬁrst estimating the cross-correlation between
signals x and y and then taking the DFT to estimate the cross-spectral
density, we may instead compute the sample cross-correlation for each
block of a signal, take the DFT of each, and average the DFTs to form a
ﬁnal cross-spectrum estimate. This is called the periodogram method of
spectral estimation.
8.8.2
Applications of Cross-Correlation
In this section, two applications of cross-correlation are outlined.
Matched Filtering
The cross-correlation function is used extensively in pattern recognition
and signal detection. We know that projecting one signal onto another is
a means of measuring how much of the second signal is present in the ﬁrst.
This can be used to “detect” the presence of known signals as components
of more complicated signals. As a simple example, suppose we record x(n)
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 184
8.8. APPENDIX B: STATISTICAL SIGNAL PROCESSING
which we think consists of a signal s(n) which we are looking for plus some
additive measurement noise e(n). Then the projection of x onto s is
Ps(x) = Ps(s) + Ps(e) ≈s
since the projection of any speciﬁc signal s onto random, zero-mean noise
is close to zero. Another term for this process is called matched ﬁltering.
The impulse response of the “matched ﬁlter” for a signal x is given by
Flip(x). By time reversing x, we transform the convolution implemented
by ﬁltering into a cross-correlation operation.
FIR System Identiﬁcation
Estimating an impulse response from input-output measurements is called
system identiﬁcation, and a large literature exists on this topic [21].
Cross-correlation can be used to compute the impulse response h(n)
of a ﬁlter from the cross-correlation of its input and output signals x(n)
and y = h ∗x, respectively.
To see this, note that, by the correlation theorem,
x ⋆y ↔X · Y = X · (H · X) = H · |X|2
Therefore, the frequency response is given by the input-output cross-
spectrum divided by the input power spectrum:
H = X · Y
|X|2
In terms of the cross-spectral density and the input power-spectral density
(which can be estimated by averaging X · Y and |X|2, respectively), this
relation can be written as
H(ω) = Rxy(ω)
Rxx(ω)
Multiplying the above equation by Rxx(ω) and taking the inverse DTFT
yields the time-domain relation
h ∗rxx = rxy
where rxy can also be written as x ⋆y. In words, the cross-correlation
of the ﬁlter input and output is equal to the ﬁlter’s impulse response
convolved with the autocorrelation of the input signal.
A Matlab program illustrating these relationships is listed in Fig. 8.12.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 185
% sidex.m - Demonstration of the use of FFT cross-
% correlation to compute the impulse response
% of a filter given its input and output.
% This is called "FIR system identification".
Nx = 32; % input signal length
Nh = 10; % filter length Ny = Nx+Nh-1;
% max output signal length
% FFT size to accommodate cross-correlation:
Nfft = 2^nextpow2(Nx+Ny-1); % FFT wants power of 2
x = rand(1,Nx); % input signal = noise
%x = 1:Nx;
% input signal = ramp
h = [1:Nh];
% the filter
xzp = [x,zeros(1,Nfft-Nx)]; % zero-padded input
yzp = filter(h,1,xzp); % apply the filter
X = fft(xzp);
% input spectrum
Y = fft(yzp);
% output spectrum
Rxx = conj(X) .* X; % energy spectrum of x
Rxy = conj(X) .* Y; % cross-energy spectrum
Hxy = Rxy ./ Rxx;
% should be the freq. response
hxy = ifft(Hxy);
% should be the imp. response
hxy(1:Nh)
% print estimated impulse response
freqz(hxy,1,Nfft);
% plot estimated freq response
err = norm(hxy - [h,zeros(1,Nfft-Nh)])/norm(h);
disp(sprintf([’Impulse Response Error = ’,...
’%0.14f%%’],100*err));
err = norm(Hxy-fft([h,zeros(1,Nfft-Nh)]))/norm(h);
disp(sprintf(’Frequency Response Error = ’,...
’%0.14f%%’],100*err));
Figure 8.12: FIR system identiﬁcation example in Matlab.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 186
8.8. APPENDIX B: STATISTICAL SIGNAL PROCESSING
8.8.3
Autocorrelation
The cross-correlation of a signal with itself gives the autocorrelation func-
tion
rx(l) ∆= 1
N (x ⋆x)(l) ∆= 1
N
N−1

n=0
x(n)x(n + l)
The autocorrelation function is Hermitian:
rx(−l) = rx(l)
When x is real, its autocorrelation is symmetric. More speciﬁcally, it is
real and even.
As in the case of cross-correlation, we can form an unbiased sample
autocorrelation as
ˆrx(l) ∆=
1
N −l
N−1

n=0
x(n)x(n + l), l = 0, 1, 2, . . . , L −1
The DFT of the autocorrelation function rx(n) is called the power
spectral density (PSD), or power spectrum, and is often denoted
Rx(k) ∆= DFTk(rx)
The true PSD of a “stationary stochastic process” is the Fourier transform
of the true autocorrelation function, and therefore the deﬁnition above
provides only a sample estimate of the PSD.
Periodogram Method for Power Spectrum Estimation
As in the case of the cross-spectrum, we may use the periodogram method
for computing the power spectrum estimate. That is, we may estimate
the power spectrum as the average of the DFTs of many sample auto-
correlations which are computed block by block in a long signal, rather
than taking one DFT of a single autocorrelation estimate based on all the
data we have. By the Correlation Theorem, this is the same as averaging
squared-magnitude DFTs of the signal blocks themselves. Let xm denote
the mth block of the signal x, and let M denote the number of blocks.
Then the PSD estimate is given by
ˆRx(k) = 1
M |DFTk(xm)|2
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 187
However, note that |Xm|2 ↔x ⋆x which is circular correlation. To avoid
this, we use zero padding in the time domain, i.e., we replace xm above by
[xm, 0, . . . , 0]. However, note that although the “wrap-around problem” is
ﬁxed, the estimator is still biased. To repair this, we can use a triangular
window (also called a “Bartlett window”) to apply the weighting needed
to remove the bias.
For real signals, the autocorrelation is real and even, and therefore the
power spectral density is real and even for all real signals. The PSD Rx(ω)
can interpreted as a measure of the relative probability that the signal
contains energy at frequency ω. Essentially, however, it is the long-term
average energy density vs. frequency in the random process x(n).
At lag zero, the autocorrelation function reduces to the average power
(root mean square) which we deﬁned earlier:
rx(0) ∆= 1
N
N−1

m=0
|x(m)|2 ∆= P2
x
Replacing “correlation” with “covariance” in the above deﬁnitions
gives the corresponding zero-mean versions.
For example, the cross-
covariance is deﬁned as
cxy(n) ∆= 1
N
N−1

m=0
[x(m) −µx][y(m + n) −µy]
We also have that cx(0) equals the variance of the signal x:
cx(0) ∆= 1
N
N−1

m=0
|x(m) −µx|2 ∆= σ2
x
8.8.4
Coherence
A function related to cross-correlation is the coherence function Γxy(ω),
deﬁned in terms of power spectral densities and the cross-spectral density
by
Γxy(k) ∆=
Rxy(k)

Rx(k)Ry(k)
In practice, these quantities can be estimated by averaging X(k)Y (k),
|X(k)|2 and |Y (k)|2 over successive signal blocks. Let {·} denote time
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 188
8.9. APPENDIX C: THE SIMILARITY THEOREM
averaging. Then an estimate of the coherence, the sample coherence func-
tion ˆΓxy(k), may be deﬁned by
ˆΓxy(k) ∆=

X(k)Y (k)

*
|X(k)|2
·

|Y (k)|2
The magnitude-squared coherence |Γxy(k)|2 is a real function between
0 and 1 which gives a measure of correlation between x and y at each
frequency (DFT bin number k). For example, imagine that y is produced
from x via an LTI ﬁltering operation:
y = h ∗x
=⇒
Y (k) = H(k)X(k)
Then the coherence function is
ˆΓxy(k) ∆=
X(k)Y (k)
|X(k)| · |Y (k)| =
X(k)H(k)X(k)
|X(k)| |H(k)X(k)| = |X(k)|2 H(k)
|X(k)|2 |H(k)|
= H(k)
|H(k)|
and the magnitude-squared coherence function is simply
ˆΓxy(k)
 =

H(k)
|H(k)|
 = 1
On the other hand, if x and y are uncorrelated noise processes, the co-
herence converges to zero.
A common use for the coherence function is in the validation of in-
put/output data collected in an acoustics experiment for purposes of sys-
tem identiﬁcation. For example, x(n) might be a known signal which is
input to an unknown system, such as a reverberant room, say, and y(n)
is the recorded response of the room. Ideally, the coherence should be 1
at all frequencies. However, if the microphone is situated at a null in the
room response for some frequency, it may record mostly noise at that fre-
quency. This will be indicated in the measured coherence by a signiﬁcant
dip below 1.
8.9
Appendix C: The Similarity Theorem
The similarity theorem is fundamentally restricted to the continuous-time
case. It says that if you “stretch” a signal by the factor α in the time
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 8. FOURIER THEOREMS FOR THE DFT
Page 189
domain, you “squeeze” its Fourier transform by the same factor in the
frequency domain. This is such a fundamental Fourier relationship, that
we include it here rather than leave it out as a non-DFT result.
The closest we came to the similarity theorem among the DFT the-
orems was the the Interpolation Theorem. We found that “stretching”
a discrete-time signal by the integer factor α (ﬁlling in between sam-
ples with zeros) corresponded to the spectrum being repeated α times
around the unit circle. As a result, the “baseband” copy of the spectrum
“shrinks” in width (relative to 2π) by the factor α. Similarly, stretching
a signal using interpolation (instead of zero-ﬁll) corresponded to the re-
peated spectrum with all spurious spectral copies zeroed out. The spec-
trum of the interpolated signal can therefore be seen as having been
stretched by the inverse of the time-domain stretch factor. In summary,
the Interpolation DFT Theorem can be viewed as the discrete-time coun-
terpart of the similarity Fourier Transform (FT) theorem.
Theorem: For all continuous-time functions x(t) possessing a Fourier
transform,
Stretchα(x) ↔1
|α|Stretch(1/α)(X)
where
Stretchα,t(x) ∆= x(αt)
and α is any nonzero real number (the abscissa scaling factor).
Proof:
FTω(Stretchα(x))
∆=
 ∞
−∞
x(αt)e−jωtdt =
 ∞
−∞
x(τ)e−jω(τ/α)d(τ/α)
=
1
|α|
 ∞
−∞
x(τ)e−j(ω/α)τdτ
∆=
1
|α|X
ω
α

The absolute value appears above because, when α < 0, d(τ/α) < 0,
which brings out a minus sign in front of the integral from −∞to ∞.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 190
8.9. APPENDIX C: THE SIMILARITY THEOREM
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Chapter 9
Example Applications of
the DFT
This chapter goes through some practical examples of FFT analysis in
Matlab. The various Fourier theorems provide a “thinking vocabulary”
for understanding elements of spectral analysis.
9.1
Spectrum Analysis of a Sinusoid: Window-
ing, Zero-Padding, and the FFT
The examples below give a progression from the most simplistic analysis
up to a proper practical treatment. Careful study of these examples will
teach you a lot about how spectrum analysis is carried out on real data,
and provide opportunities to see the Fourier theorems in action.
9.1.1
Example 1: FFT of a Simple Sinusoid
Our ﬁrst example is an FFT of the simple sinusoid
x(n) = cos(ωxnT)
where we choose ωx = 2π(fs/4) (frequency fs/4) and T = 1 (sampling
rate set to 1). Since we’re using the FFT, the signal length N must be a
power of 2. Here is the Matlab code:
echo on; hold off; diary off;
191

Page 192
9.1.
SPECTRUM ANALYSIS OF A SINUSOID: WINDOWING,
ZERO-PADDING, AND THE FFT
% !/bin/rm -f examples.dia; diary examples.dia
% For session log
% !mkdirs eps
% For figures
% Example 1: FFT of a DFT sinusoid
% Parameters:
N = 64;
% Must be a power of two
T = 1;
% Set sampling rate to 1
f = 0.25;
% Sinusoidal frequency in cycles per sample
A = 1;
% Sinusoidal amplitude
phi = 0;
% Sinusoidal phase
n = [0:N-1];
x = cos(2*pi*n*f*T);
% Signal to analyze
X = fft(x);
% Spectrum
Let’s plot the time data and magnitude spectrum:
% Plot time data
figure(1);
subplot(3,1,1);
plot(n,x,’*’);
ni = [0:.1:N-1];
% Interpolated time axis
hold on;
plot(ni,cos(2*pi*ni*f*T),’-’);
title(’Sinusoid Sampled at 1/4 the Sampling Rate’);
xlabel(’Time (samples)’); ylabel(’Amplitude’);
text(-8,1,’a)’);
% Plot spectral magnitude
magX = abs(X);
fn = [0:1.0/N:1-1.0/N];
% Normalized frequency axis
subplot(3,1,2);
stem(fn,magX)
xlabel(’Normalized Frequency (cycles per sample))’);
ylabel(’Magnitude (Linear)’);
text(-.11,40,’b)’);
% Same thing on a dB scale
spec = 20*log10(magX);
% Spectral magnitude in dB
subplot(3,1,3);
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 9. EXAMPLE APPLICATIONS OF THE DFT
Page 193
plot(fn,spec);
axis([0 1 -350 50]);
xlabel(’Normalized Frequency (cycles per sample))’);
ylabel(’Magnitude (dB)’);
text(-.11,50,’c)’);
print -deps eps/example1.eps; hold off;
0
10
20
30
40
50
60
70
−1
−0.5
0
0.5
1
Sinusoid Sampled at 1/4 the Sampling Rate
Time (samples)
Amplitude
a)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
10
20
30
40
Normalized Frequency (cycles per sample))
Magnitude (Linear)
b)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−300
−200
−100
0
Normalized Frequency (cycles per sample))
Magnitude (dB)
c)
Figure 9.1: Sampled sinusoid at f = fs/4. a) Time waveform. b)
Magnitude spectrum. c) DB magnitude spectrum.
The results are shown in Fig. 9.1. The time-domain signal is shown
in Fig. 9.1a, both in pseudo-continuous and sampled form. In Fig. 9.1b,
we see two peaks in the magnitude spectrum, each at magnitude 32 on a
linear scale, located at normalized frequencies f = 0.25 and f = 0.75 =
−0.25. Since the DFT length is N = 64, a spectral peak amplitude of
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 194
9.1.
SPECTRUM ANALYSIS OF A SINUSOID: WINDOWING,
ZERO-PADDING, AND THE FFT
32 = (1/2)64 is what we expect, since
DFTk(cos(ωxn)) ∆=
N−1

n=0
ejωxn + e−jωxn
2
e−jωkn =
N−1

n=0
ej0n
2
= N
2
when ωk = ±ωx. This happens at bin numbers k = (0.25/fs)N = 16 and
k = (0.75/fs)N = 48 for N = 64. However, recall that Matlab requires
indexing from 1, so that these peaks will really show up at index 17 and
49 in the magX array.
The spectrum should be exactly zero at the other bin numbers. How
accurately this happens can be seen by looking on a dB scale, as shown
in Fig. 9.1c. We see that the spectral magnitude in the other bins is on
the order of 300 dB lower, which is close enough to zero for audio work.
9.1.2
Example 2: FFT of a Not-So-Simple Sinusoid
Now let’s increase the frequency in the above example by one-half of a
bin:
% Example 2: Same as Example 1 but with a frequency between bins
f = 0.25 + 0.5/N;
% Move frequency off-center by half a bin
x = cos(2*pi*n*f*T);
% Signal to analyze
X = fft(x);
% Spectrum
% Plot time data
figure(2);
subplot(3,1,1);
plot(n,x,’*’);
ni = [0:.1:N-1];
% Interpolated time axis
hold on;
plot(ni,cos(2*pi*ni*f*T),’-’);
title(’Sinusoid Sampled at NEAR 1/4 the Sampling Rate’);
xlabel(’Time (samples)’); ylabel(’Amplitude’);
text(-8,1,’a)’); hold off;
% Plot spectral magnitude
subplot(3,1,2);
magX = abs(X);
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 9. EXAMPLE APPLICATIONS OF THE DFT
Page 195
stem(fn,magX); grid;
xlabel(’Normalized Frequency (cycles per sample))’);
ylabel(’Magnitude (Linear)’);
text(-0.11,30,’b)’);
% Same spectrum on a dB scale
subplot(3,1,3);
spec = 20*log10(magX);
% Spectral magnitude in dB
plot(fn,spec); grid
xlabel(’Normalized Frequency (cycles per sample))’);
ylabel(’Magnitude (dB)’);
text(-0.11,40,’c)’);
print -deps eps/example2.eps; hold off;
0
10
20
30
40
50
60
70
−1
−0.5
0
0.5
1
Sinusoid Sampled at NEAR 1/4 the Sampling Rate
Time (samples)
Amplitude
a)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
10
20
30
Normalized Frequency (cycles per sample))
Magnitude (Linear)
b)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−10
0
10
20
30
Normalized Frequency (cycles per sample))
Magnitude (dB)
c)
Figure 9.2: Sinusoid at Frequency f = 0.25 + 0.5/N. a) Time
waveform. b) Magnitude spectrum. c) DB magnitude spectrum.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 196
9.1.
SPECTRUM ANALYSIS OF A SINUSOID: WINDOWING,
ZERO-PADDING, AND THE FFT
The resulting magnitude spectrum is shown in Fig. 9.2b and c. We
see extensive “spectral leakage” into all the bins at this frequency.
To get an idea of where this spectral leakage is coming from, let’s look
at the periodic extension of the time waveform:
% Plot the periodic extension of the time-domain signal
plot([x,x]);
title(’Time Waveform Repeated Once’);
xlabel(’Time (samples)’);
ylabel(’Amplitude’);
print -deps eps/waveform2.eps;
% Figure 4
disp ’pausing for RETURN (check the plot). . .’; pause
0
20
40
60
80
100
120
140
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Time waveform repeated once
Time (samples)
Amplitude
Figure 9.3: Time waveform repeated to show discontinuity intro-
duced by periodic extension (see midpoint).
The result is shown in Fig. 9.3. Note the “glitch” in the middle where
the signal begins its forced repetition.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 9. EXAMPLE APPLICATIONS OF THE DFT
Page 197
9.1.3
Example 3: FFT of a Zero-Padded Sinusoid
Interestingly, looking back at Fig. 9.2c, we see there are no negative dB
values. Could this be right? To really see the spectrum, let’s use some zero
padding in the time domain to yield ideal interpolation in the freqency
domain:
% Example 3: Add zero padding
zpf = 8;
% zero-padding factor
x = [cos(2*pi*n*f*T),zeros(1,(zpf-1)*N)];
% zero-padded FFT input data
X = fft(x);
% Interpolated spectrum
% Plot time data
figure(4);
subplot(3,1,1);
plot(x);
title(’Zero-Padded Sampled Sinusoid’);
xlabel(’Time (samples)’); ylabel(’Amplitude’);
text(-30,1,’a)’); hold off;
% Plot spectral magnitude
magX = abs(X);
nfft = zpf*N;
fni = [0:1.0/nfft:1-1.0/nfft];
% Normalized frequency axis
subplot(3,1,2);
plot(fni,magX,’-’); grid; % With interpolation, we can use solid lines ’-’
% title(’Interpolated Spectral Magnitude’);
xlabel(’Normalized Frequency (cycles per sample))’);
ylabel(’Magnitude (Linear)’);
text(-.11,40,’b)’);
% Same thing on a dB scale
spec = 20*log10(magX);
% Spectral magnitude in dB
spec = max(spec,-60*ones(1,length(spec))); % clip to -60 dB
subplot(3,1,3);
plot(fni,spec,’-’); grid; axis([0 1 -60 50]);
% title(’Interpolated Spectral Magnitude (dB)’);
xlabel(’Normalized Frequency (cycles per sample))’);
ylabel(’Magnitude (dB)’);
text(-.11,50,’c)’);
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 198
9.1.
SPECTRUM ANALYSIS OF A SINUSOID: WINDOWING,
ZERO-PADDING, AND THE FFT
print -deps eps/example3.eps;
if dopause, disp ’pausing for RETURN (check the plot). . .’; pause; end
0
100
200
300
400
500
600
−1
−0.5
0
0.5
1
Zero−Padded Sampled Sinusoid
Time (samples)
Amplitude
a)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
10
20
30
40
Normalized Frequency (cycles per sample))
Magnitude (Linear)
b)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−50
0
50
Normalized Frequency (cycles per sample))
Magnitude (dB)
c)
Figure 9.4: Zero-Padded Sinusoid at Frequency f = 0.25+0.5/N.
a) Time waveform. b) Magnitude spectrum. c) DB magnitude
spectrum.
With the zero padding, we see there’s quite a bit going on. In fact, the
spectrum has a regular sidelobe structure. On the dB scale in Fig. 9.4c,
we now see that there are indeed negative dB values. This shows the
importance of using zero padding to interpolate spectral displays so that
the eye can “ﬁll in” properly between the samples.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 9. EXAMPLE APPLICATIONS OF THE DFT
Page 199
9.1.4
Example 4: Blackman Window
Finally, to ﬁnish oﬀthis sinusoid, let’s look at the eﬀect of using a Black-
man window [22] (which has good though suboptimal characteristics for
audio work). Figure 9.5a shows the Blackman window, Fig. 9.5b shows
its magnitude spectrum on a dB scale, and Fig. 9.5c introduces the use
of a more natural frequency axis which interprets the upper half of the
bin numbers as negative frequencies. Here is the Matlab for it:
% Add a "Blackman window"
% w = blackman(N); % if you have the signal processing toolbox
w = .42-.5*cos(2*pi*(0:N-1)/(N-1))+.08*cos(4*pi*(0:N-1)/(N-1));
figure(5);
subplot(3,1,1); plot(w,’*’); title(’The Blackman Window’);
xlabel(’Time (samples)’); ylabel(’Amplitude’);
text(-8,1,’a)’);
% Also show the window transform:
xw = [w,zeros(1,(zpf-1)*N)]; % zero-padded window (col vector)
Xw = fft(xw);
% Blackman window transform
spec = 20*log10(abs(Xw));
% Spectral magnitude in dB
spec = spec - max(spec);
% Usually we normalize to 0 db max
spec = max(spec,-100*ones(1,nfft)); % clip to -100 dB
subplot(3,1,2); plot(fni,spec,’-’); axis([0,1,-100,10]); grid;
xlabel(’Normalized Frequency (cycles per sample))’);
ylabel(’Magnitude (dB)’);
text(-.12,20,’b)’);
% Replot interpreting upper bin numbers as negative frequencies:
nh = nfft/2;
specnf = [spec(nh+1:nfft),spec(1:nh)];
% see also Matlab’s fftshift()
fninf = fni - 0.5;
subplot(3,1,3);
plot(fninf,specnf,’-’); axis([-0.5,0.5,-100,10]); grid;
xlabel(’Normalized Frequency (cycles per sample))’);
ylabel(’Magnitude (dB)’);
text(-.6,20,’c)’);
print -deps eps/blackman.eps;
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 200
9.1.
SPECTRUM ANALYSIS OF A SINUSOID: WINDOWING,
ZERO-PADDING, AND THE FFT
0
10
20
30
40
50
60
70
0
0.5
1
The Blackman Window
Time (samples)
Amplitude
a)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−100
−50
0
Normalized Frequency (cycles per sample))
Magnitude (dB)
b)
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
−100
−50
0
Normalized Frequency (cycles per sample))
Magnitude (dB)
c)
Figure 9.5: The Blackman Window. a) The window itself in the
time domain. b) DB Magnitude spectrum of the Blackman win-
dow. c) Blackman-window DB magnitude spectrum plotted over
frequencies [−0.5, 0.5).
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 9. EXAMPLE APPLICATIONS OF THE DFT
Page 201
9.1.5
Example 5: Use of the Blackman Window
Now let’s apply this window to the sinusoidal data:
% Use the Blackman window on the sinusoid data
xw = [w .* cos(2*pi*n*f*T),zeros(1,(zpf-1)*N)]; % windowed, zero-padded data
X = fft(xw);
% Smoothed, interpolated spectrum
% Plot time data
figure(6);
subplot(2,1,1);
plot(xw);
title(’Windowed, Zero-Padded, Sampled Sinusoid’);
xlabel(’Time (samples)’); ylabel(’Amplitude’);
text(-50,1,’a)’); hold off;
% Plot spectral magnitude in the best way
spec = 10*log10(conj(X).*X);
% Spectral magnitude in dB
spec = max(spec,-60*ones(1,nfft)); % clip to -60 dB
subplot(2,1,2);
plot(fninf,fftshift(spec),’-’); axis([-0.5,0.5,-60,40]); grid;
title(’Smoothed, Interpolated, Spectral Magnitude (dB)’);
xlabel(’Normalized Frequency (cycles per sample))’);
ylabel(’Magnitude (dB)’);
text(-.6,40,’b)’);
print -deps eps/xw.eps;
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 202
9.1.
SPECTRUM ANALYSIS OF A SINUSOID: WINDOWING,
ZERO-PADDING, AND THE FFT
0
100
200
300
400
500
600
−1
−0.5
0
0.5
1
Windowed, Zero−Padded, Sampled Sinusoid
Time (samples)
Amplitude
a)
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
−60
−40
−20
0
20
40
Smoothed, Interpolated, Spectral Magnitude (dB)
Normalized Frequency (cycles per sample))
Magnitude (dB)
b)
Figure 9.6: Eﬀect of the Blackman window on the sinusoidal data
segment.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 9. EXAMPLE APPLICATIONS OF THE DFT
Page 203
9.1.6
Example 6: Hanning-Windowed Complex Sinusoid
In this example, we’ll perform spectrum analysis on a complex sinusoid
having only a single positive frequency. We’ll use the Hanning window
which does not have as much sidelobe suppression as the Blackman win-
dow, but its main lobe is narrower. Its sidelobes “roll oﬀ” very quickly
versus frequency. Compare with the Blackman window results to see if
you can see these diﬀerences.
% Example 5: Practical spectrum analysis of a sinusoidal signal
% Analysis parameters:
M = 31;
% Window length (we’ll use a "Hanning window")
N = 64;
% FFT length (zero padding around a factor of 2)
% Signal parameters:
wxT = 2*pi/4;
% Sinusoid frequency in rad/sample (1/4 sampling rate)
A = 1;
% Sinusoid amplitude
phix = 0;
% Sinusoid phase
% Compute the signal x:
n = [0:N-1];
% time indices for sinusoid and FFT
x = A * exp(j*wxT*n+phix); % the complex sinusoid itself: [1,j,-1,-j,1,j,...]
% Compute Hanning window:
nm = [0:M-1];
% time indices for window computation
w = (1/M) * (cos((pi/M)*(nm-(M-1)/2))).^2;
% Hanning window = "raised cosine"
% (FIXME: normalizing constant above should be 2/M)
wzp = [w,zeros(1,N-M)]; % zero-pad out to the length of x
xw = x .* wzp;
% apply the window w to the signal x
% Display real part of windowed signal and the Hanning window:
plot(n,wzp,’-’); hold on;
plot(n,real(xw),’*’);
title(’Hanning Window and Windowed, Zero-Padded, Sinusoid (Real Part)’);
xlabel(’Time (samples)’); ylabel(’Amplitude’); hold off;
disp ’pausing for RETURN (check the plot). . .’; pause
print -deps eps/hanning.eps;
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 204
9.1.
SPECTRUM ANALYSIS OF A SINUSOID: WINDOWING,
ZERO-PADDING, AND THE FFT
0
10
20
30
40
50
60
70
-0.04
-0.03
-0.02
-0.01
0
0.01
0.02
0.03
0.04
Hanning Window and Windowed, Zero-Padded, Sinusoid (Real Part)
Time (samples)
Amplitude
Figure 9.7: A length 31 Hanning Window (“Raised Cosine”) and
the windowed sinusoid created using it.
Zero-padding is also
shown. The sampled sinusoid is plotted with ‘*’ using no connect-
ing lines. You must now imagine the continuous sinusoid threading
through the asterisks.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 9. EXAMPLE APPLICATIONS OF THE DFT
Page 205
% Compute the spectrum and its various alternative forms
%
Xw = fft(xw);
% FFT of windowed data
fn = [0:1.0/N:1-1.0/N];
% Normalized frequency axis
spec = 20*log10(abs(Xw));
% Spectral magnitude in dB
% Since the zeros go to minus infinity, clip at -100 dB:
spec = max(spec,-100*ones(1,length(spec)));
phs = angle(Xw);
% Spectral phase in radians
phsu = unwrap(phs);
% Unwrapped spectral phase (using matlab function)
To help see the full spectrum, we’ll also compute a heavily interpolated
spectrum which we’ll draw using solid lines. (The previously computed
spectrum will be plotted using ’*’.) Ideal spectral interpolation is done
using zero-padding in the time domain:
Nzp = 16;
% Zero-padding factor
Nfft = N*Nzp;
% Increased FFT size
xwi = [xw,zeros(1,Nfft-N)]; % New zero-padded FFT buffer
Xwi = fft(xwi);
% Take the FFT
fni = [0:1.0/Nfft:1.0-1.0/Nfft]; % Normalized frequency axis
speci = 20*log10(abs(Xwi));
% Interpolated spectral magnitude in dB
speci = max(speci,-100*ones(1,length(speci))); % clip at -100 dB
phsi = angle(Xwi);
% Phase
phsiu = unwrap(phsi);
% Unwrapped phase
% Plot spectral magnitude
%
plot(fn,abs(Xw),’*’); hold on; plot(fni,abs(Xwi));
title(’Spectral Magnitude’);
xlabel(’Normalized Frequency (cycles per sample))’);
ylabel(’Amplitude (Linear)’);
disp ’pausing for RETURN (check the plot). . .’; pause
print -deps eps/specmag.eps; hold off;
% Same thing on a dB scale
plot(fn,spec,’*’); hold on; plot(fni,speci);
title(’Spectral Magnitude (dB)’);
xlabel(’Normalized Frequency (cycles per sample))’);
ylabel(’Magnitude (dB)’);
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 206
9.1.
SPECTRUM ANALYSIS OF A SINUSOID: WINDOWING,
ZERO-PADDING, AND THE FFT
0
0.2
0.4
0.6
0.8
1
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Spectral Magnitude
Normalized Frequency (cycles per sample))
Amplitude (Linear)
Figure 9.8: Spectral Magnitude, linear scale.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 9. EXAMPLE APPLICATIONS OF THE DFT
Page 207
disp ’pausing for RETURN (check the plot). . .’; pause
print -deps eps/specmagdb.eps; hold off;
0
0.2
0.4
0.6
0.8
1
-100
-90
-80
-70
-60
-50
-40
-30
-20
-10
0
Spectral Magnitude (dB)
Normalized Frequency (cycles per sample))
Magnitude (dB)
Figure 9.9: Spectral Magnitude, dB scale.
Note that there are no negative frequency components in Fig. 9.8
because we are analyzing a complex sinusoid [1, j, −1, −j, 1, j, . . .] which
is a sampled complex sinusoid frequency fs/4 only.
Notice how diﬃcult it would be to correctly interpret the shape of
the “sidelobes” without zero padding.
The asterisks correspond to a
zero-padding factor of 2, already twice as much as needed to preserve all
spectral information faithfully, but it is clearly not suﬃcient to make the
sidelobes clear in a spectral magnitude plot.
Spectral Phase
As for the phase of the spectrum, what do we expect? We have chosen
the sinusoid phase to be zero. The window is symmetric about its middle.
Therefore, we expect a linear phase term with slope -(M-1)/2 samples.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 208
9.1.
SPECTRUM ANALYSIS OF A SINUSOID: WINDOWING,
ZERO-PADDING, AND THE FFT
Also, the window transform has sidelobes which cause a phase of π radians
to switch in and out. Thus, we expect to see samples of a straight line
with slope -15 across the main lobe of the window transform, together
with a switching oﬀset by π in every other sidelobe away from the main
lobe, starting with the immediately adjacent sidelobes.
In the plot, you can see the negatively sloped line across the main
lobe of the window transform, but the sidelobes are hard to follow.
plot(fn,phs,’*’); hold on; plot(fni,phsi); grid;
title(’Spectral Phase’);
xlabel(’Normalized Frequency (cycles per sample))’);
ylabel(’Phase - Phi (Radians)’);
disp ’pausing for RETURN (check the plot). . .’; pause
print -deps eps/specphase.eps; hold off;
0
0.2
0.4
0.6
0.8
1
-4
-3
-2
-1
0
1
2
3
4
Spectral Phase
Normalized Frequency (cycles per sample))
Phase - Phi (Radians)
Figure 9.10: Spectral phase.
To convert the expected phase slope from −15 “radians per radian-
frequency” to “radians per cycle-per-sample,” we need to multiply by
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

CHAPTER 9. EXAMPLE APPLICATIONS OF THE DFT
Page 209
“radians per cycle,” or 2π. Thus, in Fig. 9.10, we expect a slope of −94.2
radians per unit normalized frequency, or −9.42 radians per 0.1 cycles-
per-sample, and this looks about right, judging from the plot.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 210
9.1.
SPECTRUM ANALYSIS OF A SINUSOID: WINDOWING,
ZERO-PADDING, AND THE FFT
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Appendix A
Matrices
A matrix is deﬁned as a rectangular array of numbers, e.g.,
A =
+
a
b
c
d
,
which is a 2×2 (“two by two”) matrix. A general matrix may be M ×N,
where M is the number of rows, and N is the number of columns. For
example, the general 3 × 2 matrix is


a
b
c
d
e
f


(Either square brackets or large parentheses may be used.) The (i, j)th
element1 of a matrix A may be denoted by A[i, j] or A(i, j). The rows
and columns of matrices are normally numbered from 1 instead of from
0; thus, 1 ≤i ≤M and 1 ≤j ≤N. When N = M, the matrix is said to
be square.
The transpose of a real matrix A ∈RM×N is denoted by AT and is
deﬁned by
AT[i, j] ∆= A[j, i]
Note that while A is M × N, its transpose is N × M.
A complex matrix A ∈CM×N, is simply a matrix containing com-
plex numbers. The transpose of a complex matrix is normally deﬁned to
1We are now using j as an integer counter, not as √−1. This is standard notational
practice.
211

Page 212
include conjugation. The conjugating transpose operation is called the
Hermitian transpose. To avoid confusion, in this tutorial, AT and the
word “transpose” will always denote transposition without conjugation,
while conjugating transposition will be denoted by A∗and be called the
“Hermitian transpose” or the “conjugate transpose.” Thus,
A∗[i, j] ∆= A[j, i]
Example: The transpose of the general 3 × 2 matrix is


a
b
c
d
e
f


T
=
! a
c
e
b
d
f
"
while the conjugate transpose of the general 3 × 2 matrix is


a
b
c
d
e
f


∗
=
! a
c
e
b
d
f
"
A column-vector
x =
+
x0
x1
,
is the special case of an M × 1 matrix, and a row-vector
xT = [x0 x1]
(as we have been using) is a 1 × N matrix. In contexts where matrices
are being used (only this section for this reader), it is best to deﬁne all
vectors as column vectors and to indicate row vectors using the transpose
notation, as was done in the equation above.
A.0.1
Matrix Multiplication
Let AT be a general M×L matrix and let B denote a general L×N matrix.
Denote the matrix product by C = ATB or C = AT · B. Then matrix
multiplication is carried out by computing the inner product of every row
of AT with every column of B. Let the ith row of AT be denoted by aT
i ,
i = 1, 2, . . . , M, and the jth column of B by bj, j = 1, 2, . . . , L. Then the
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

APPENDIX A. MATRICES
Page 213
matrix product C = ATB is deﬁned as
C = ATB =


< aT
1, b1 >
< aT
1, b2 >
· · ·
< aT
1, bN >
< aT
2, b1 >
< aT
2, b2 >
· · ·
< aT
2, bN >
...
...
· · ·
...
< aT
M, b1 >
< aT
M, b2 >
· · ·
< aT
M, bN >


This deﬁnition can be extended to complex matrices by using a deﬁnition
of inner product which does not conjugate its second argument.2
Examples:


a
b
c
d
e
f

·
! α
β
γ
δ
"
=


aα + bγ
aβ + bδ
cα + dγ
cβ + dδ
eα + fγ
eβ + fδ


! α
β
γ
δ
"
·
! a
c
e
b
d
f
"
=
! αa + βb
αc + βd
αe + βf
γa + δb
γc + δd
γe + δf
"
! α
β
"
·

a
b
c

=
! αa
αb
αc
βa
βb
βc
"

a
b
c

·


α
β
γ

= aα + bβ + cγ
An M × L matrix A can only be multiplied on the right by an L × N
matrix, where N is any positive integer. An L × N matrix A can only be
multiplied on the left by a M ×L matrix, where M is any positive integer.
Thus, the number of columns in the matrix on the left must equal the
number of rows in the matrix on the right.
Matrix multiplication is non-commutative, in general. That is, nor-
mally AB ̸= BA even when both products are deﬁned (such as when the
matrices are square.)
The transpose of a matrix product is the product of the transposes in
reverse order:
(AB)
T = BTAT
2Alternatively, it can be extended to the complex case by writing A∗B
∆= [. . . <
bj, a∗
i > . . .], so that A∗includes a conjugation of the elements of A. This diﬃculty
arises from the fact that matrix multiplication is really deﬁned without consideration
of conjugation or transposition at all, making it unwieldy to express in terms of inner
products in the complex case, even though that is perhaps the most fundamental
interpretation of a matrix multiply.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 214
The identity matrix is denoted by I and is deﬁned as
I ∆=


1
0
0
· · ·
0
0
1
0
· · ·
0
0
0
1
· · ·
0
...
...
...
· · ·
...
0
0
0
· · ·
1


Identity matrices are always square. The N × N identity matrix I, some-
times denoted as IN, satisﬁes A · IN = A for every M × N matrix A.
Similarly, IM · A = A, for every M × N matrix A.
As a special case, a matrix AT times a vector x produces a new vector
y = ATx which consists of the inner product of every row of AT with x
ATx =


< aT
1, x >
< aT
2, x >
...
< aT
M, x >


A matrix AT times a vector x deﬁnes a linear transformation of x. In fact,
every linear function of a vector x can be expressed as a matrix multiply.
In particular, every linear ﬁltering operation can be expressed as a matrix
multiply applied to the input signal. As a special case, every linear, time-
invariant (LTI) ﬁltering operation can be expressed as a matrix multiply
in which the matrix is Toeplitz, i.e., AT[i, j] = AT[i −j] (constant along
all diagonals).
As a further special case, a row vector on the left may be multiplied
by a column vector on the right to form a single inner product:
y∗x =< x, y >
where the alternate transpose notation “∗” is deﬁned to include complex
conjugation so that the above result holds also for complex vectors. Using
this result, we may rewrite the general matrix multiply as
C = ATB =


aT
1b1
aT
1b2
· · ·
aT
1bN
aT
2b1
aT
2b2
· · ·
aT
2bN
...
...
...
...
aT
Mb1
aT
Mb2
· · ·
aT
MbN


DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

APPENDIX A. MATRICES
Page 215
A.0.2
Solving Linear Equations Using Matrices
Consider the linear system of equations
ax1 + bx2
=
c
dx1 + ex2
=
f
in matrix form:
+
a
b
d
e
, +
x1
x2
,
=
+
c
f
,
.
This can be written in higher level form as
Ax = b
where A denotes the two-by-two matrix above, and x and b denotes the
two-by-one vectors. The solution to this equation is then
x = A−1b =
+
a
b
d
e
,−1 +
c
f
,
The general two-by-two matrix inverse is given by
+
a
b
d
e
,−1
=
1
ae −bd
+
e
−b
−d
a
,
and the inverse exists whenever ae −bd (which is called the determinant
of the matrix A) is nonzero. For larger matrices, numerical algorithms
are used to invert matrices, such as used by Matlab based on LINPACK
[23]. An initial introduction to matrices and linear algebra can be found
in [16].
For a Stanford “courselet” on matrices, see
http://sll-1.stanford.edu/Courselets/Matrices/web/quiz/quizz.cgi/ask/quest.html.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 216
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Appendix B
Sampling Theory
A basic tutorial on sampling theory is presented. Aliasing due to sampling
of continuous-time signals is characterized mathematically.
Shannon’s
sampling theorem is proved.
A pictorial representation of continuous-
time signal reconstruction from discrete-time samples is given.
B.1
Introduction
Inside computers and modern “digital” synthesizers, (as well as music
CDs), sound is sampled into a stream of numbers. Each sample can be
thought of as a number which speciﬁes the position1 of a loudspeaker at a
particular instant. When sound is sampled, we call it digital audio. The
sampling rate used for CDs is 44,100 samples per second. That means
1More typically, each sample represents the instantaneous velocity of the speaker.
Here’s why: Most microphones are transducers from acoustic pressure to electrical
voltage, and analog-to-digital converters (ADCs) produce numerical samples which are
proportional to voltage. Thus, digital samples are normally proportional to acoustic
pressure deviation (force per unit area on the microphone, with ambient air pressure
subtracted out).
When digital samples are converted to analog form by digital-to-
analog conversion (DAC), each sample is converted to an electrical voltage which then
drives a loudspeaker (in audio applications). Typical loudspeakers use a “voice-coil”
to convert applied voltage to electromotive force on the speaker which applies pressure
on the air via the speaker cone. Since the acoustic impedance of air is a real number,
wave pressure is directly proportional wave velocity.
Since the speaker must move
in contact with the air during wave generation, we may conclude that digital signal
samples correspond most closely to the velocity of the speaker, not its position. The
situation is further complicated somewhat by the fact that speakers do not themselves
have a real driving-point impedance. However, for an “ideal” microphone and speaker,
we should get samples proportional to speaker velocity and hence to air pressure.
217

Page 218
B.1. INTRODUCTION
when you play a CD, the speakers in your stereo system are moved to
a new position 44,100 times per second, or once every 23 microseconds.
Controlling a speaker this fast enables it to generate any sound in the
human hearing range because we cannot hear frequencies higher than
around 20,000 cycles per second, and a sampling rate more than twice
the highest frequency in the sound guarantees that exact reconstruction
is possible from the samples.
B.1.1
Reconstruction from Samples—Pictorial Version
Figure B.1 shows how a sound is reconstructed from its samples. Each
sample can be considered as specifying the scaling and location of a
sinc function. The discrete-time signal being interpolated in the ﬁgure
is [. . . , 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, . . . ]. The sinc functions are drawn with
dashed lines, and they sum to produce the solid curve. Note the “Gibb’s
overshoot” near the corners of this continuous rectangular pulse due to
band-limiting.
Figure B.1: How sinc functions sum up to create a continuous
waveform from discrete-time samples.
Note how each sinc function passes through zero at every sample in-
stant but the one it is centered on, where it passes through 1. An isolated
sinc function is shown in Fig. B.2.
The sinc function is the famous “sine x over x” curve, deﬁned by
sinc(Fst) ∆= sin(πFst)
πFst
.
where Fs denotes the sampling rate in samples-per-second (Hz), and t
denotes time in seconds.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

APPENDIX B. SAMPLING THEORY
Page 219
-7 -6 -5 -4 -3 -2 -1
1
2
3
4
5
6
7
1
. . .
. . .
. . .. . .
Figure B.2: The sinc function.
B.1.2
Reconstruction from Samples—The Math
Let xd(n) ∆= x(nTs) denote the nth sample of the original sound x(t),
where t is time in seconds. Thus, n ranges over the integers, and Ts is
the sampling period in seconds. The sampling rate in Hertz (Hz) is just
the reciprocal of the sampling period, i.e.,
Fs
∆= 1
Ts
To avoid losing any information as a result of sampling, we must
assume x(t) is band-limited to less than half the sampling rate.
This
means there can be no energy in x(t) at frequency Fs/2 or above. We will
prove this mathematically when we prove Shannon’s Sampling Theorem
in §B.3 below.
Let X(ω) denote the Fourier transform of x(t), i.e.,
X(ω) ∆=
 ∞
−∞
x(t)e−jωtdt.
Then we can say x is band-limited to less than half the sampling rate if
and only if X(ω) = 0 for all |ω| ≥πFs. In this case, Shannon’s sampling
theorem gives us that x(t) can be uniquely reconstructed from the samples
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 220
B.2. ALIASING OF SAMPLED SIGNALS
x(nTs) by summing up shifted, scaled, sinc functions:
ˆx(t) ∆=
∞

n=−∞
x(nTs)hs(t −nTs) ≡x(t)
where
hs(t) ∆= sinc(Fst) ∆= sin(πFst)
πFst
The sinc function is the impulse response of the ideal lowpass ﬁlter. This
means its Fourier transform is a rectangular window in the frequency
domain. The particular sinc function used here corresponds to the ideal
lowpass ﬁlter which cuts oﬀat half the sampling rate. In other words, it
has a gain of 1 between frequencies 0 and Fs/2, and a gain of zero at all
higher frequencies.
The reconstruction of a sound from its samples can thus be interpreted
as follows: convert the sample stream into a weighted impulse train, and
pass that signal through an ideal lowpass ﬁlter which cuts oﬀat half
the sampling rate. These are the fundamental steps of digital to analog
conversion (DAC). In practice, neither the impulses nor the lowpass ﬁlter
are ideal, but they are usually close enough to ideal that you cannot hear
any diﬀerence. Practical lowpass-ﬁlter design is discussed in the context
of band-limited interpolation.2
B.2
Aliasing of Sampled Signals
This section quantiﬁes aliasing in the general case. This result is then
used in the proof of Shannon’s Sampling Theorem in the next section.
It is well known that when a continuous-time signal contains energy
at a frequency higher than half the sampling rate Fs/2, then sampling at
Fs samples per second causes that energy to alias to a lower frequency.
If we write the original frequency as f = Fs/2 + ϵ, then the new aliased
frequency is fa = Fs/2 −ϵ, for ϵ ≤Fs/2.
This phenomenon is also
called “folding”, since fa is a “mirror image” of f about Fs/2. As we
will see, however, this is not a fundamental description of aliasing, as it
only applies to real signals. For general (complex) signals, it is better
to regard the aliasing due to sampling as a summation over all spectral
“blocks” of width Fs.
2http://www-ccrma.stanford.edu/˜jos/resample/
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

APPENDIX B. SAMPLING THEORY
Page 221
Theorem.
(Continuous-Time Aliasing Theorem) Let x(t) denote
any continuous-time signal having a Fourier Transform (FT)
X(jω) ∆=
 ∞
−∞
x(t)e−jωtdt
Let
xd(n) ∆= x(nTs),
n = . . . , −2, −1, 0, 1, 2, . . . ,
denote the samples of x(t) at uniform intervals of Ts seconds, and denote
its Discrete-Time Fourier Transform (DTFT) by
Xd(ejθ) ∆=
∞

n=−∞
xd(n)e−jθn
Then the spectrum Xd of the sampled signal xd is related to the spectrum
X of the original continuous-time signal x by
Xd(ejθ) = 1
Ts
∞

m=−∞
X
!
j
 θ
Ts
+ m2π
Ts
"
.
The terms in the above sum for m ̸= 0 are called aliasing terms. They
are said to alias into the base band [−π/Ts, π/Ts]. Note that the summa-
tion of a spectrum with aliasing components involves addition of complex
numbers; therefore, aliasing components can be removed only if both their
amplitude and phase are known.
Proof. Writing x(t) as an inverse FT gives
x(t) = 1
2π
 ∞
−∞
X(jω)ejωtdω
Writing xd(n) as an inverse DTFT gives
xd(n) = 1
2π
 π
−π
Xd(ejθ)x(t)ejθtdθ
where θ ∆= 2πωdTs denotes the normalized discrete-time frequency vari-
able.
The inverse FT can be broken up into a sum of ﬁnite integrals, each
of length Ωs
∆= 2πFs = 2π/Ts, as follows:
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 222
B.2. ALIASING OF SAMPLED SIGNALS
x(t)
=
1
2π
 ∞
−∞
X(jω)ejωtdω
=
1
2π
∞

m=−∞
 (2m+1)π/Ts
(2m−1)π/Ts)
X(jω)ejωtdω
=
ω ←mΩs
1
2π
∞

m=−∞
 Ωs/2
−Ωs/2
X (jω + jmΩs) ejωtejΩsmtdω
=
1
2π
 Ωs/2
−Ωs/2
ejωt
∞

m=−∞
X (jω + jmΩs) ejΩsmtdω
Let us now sample this representation for x(t) at t = nTs to obtain
xd(n) ∆= x(nTs)
=
1
2π
 Ωs/2
−Ωs/2
ejωnTs
∞

m=−∞
X (jω + jmΩs) ejΩsmnTsdω
=
1
2π
 Ωs/2
−Ωs/2
ejωnTs
∞

m=−∞
X (jω + jmΩs) ej(2π/Ts)mnTsdω
=
1
2π
 Ωs/2
−Ωs/2
ejωnTs
∞

m=−∞
X (jω + jmΩs) dω
since n and m are integers. Normalizing frequency as θ′ = ωTs yields
xd(n) = 1
2π

−π
πejθ′n 1
Ts
∞

m=−∞
X
!
j
 θ′
Ts
+ m2π
Ts
"
dθ′.
Since this is formally the inverse DTFT of Xd(ejθ′) written in terms of
X(jθ′/Ts), the result follows. ✷
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

APPENDIX B. SAMPLING THEORY
Page 223
B.3
Shannon’s Sampling Theorem
Theorem.
Let x(t) denote any continuous-time signal having a
continuous Fourier transform
X(jω) ∆=
 ∞
−∞
x(t)e−jωtdt
Let
xd(n) ∆= x(nTs),
n = . . . , −2, −1, 0, 1, 2, . . . ,
denote the samples of x(t) at uniform intervals of Ts seconds.
Then
x(t) can be exactly reconstructed from its samples xd(n) if and only if
X(jω) = 0 for all |ω| ≥π/Ts.3
Proof.
From the Continuous-Time Aliasing Theorem of §B.2, we
have that the discrete-time spectrum Xd(ejθ) can be written in terms of
the continuous-time spectrum X(jω) as
Xd(ejωdTs) = 1
Ts
∞

m=−∞
X[j(ωd + mΩs)]
where ωd
∆= θ/Ts is the “digital frequency” variable. If X(jω) = 0 for all
|ω| ≥Ωs/2, then the above inﬁnite sum reduces to one term, the m = 0
term, and we have
Xd(ejωdTs) = 1
Ts
X(jωd),
ωd ∈[−π/Ts, π/Ts]
At this point, we can see that the spectrum of the sampled signal x(nTs)
coincides with the spectrum of the continuous-time signal x(t). In other
words, the DTFT of x(nTs) is equal to the FT of x(t) between plus and
minus half the sampling rate, and the FT is zero outside that range. This
makes it clear that spectral information is preserved, so it should now be
possible to go from the samples back to the continuous waveform without
error.
3Mathematically, X(jω) can be allowed to be nonzero over points |ω| ≥π/Ts pro-
vided that the set of all such points have measure zero in the sense of Lebesgue inte-
gration. However, such distinctions do not arise for practical signals which are always
ﬁnite in extent and which therefore have continuous Fourier transforms. This is why
we specialize Shannon’s Sampling Theorem to the case of continuous-spectrum signals.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 224
B.3. SHANNON’S SAMPLING THEOREM
To reconstruct x(t) from its samples x(nTs), we may simply take the
inverse Fourier transform of the zero-extended DTFT, i.e.,
x(t)
=
IFTt(X) ∆= 1
2π
 ∞
−∞
X(jω)ejωtdω = 1
2π
 Ωs/2
−Ωs/2
X(jω)ejωtdω
=
1
2π
 Ωs/2
−Ωs/2
Xd(ejθ)ejωtdω ∆= IDTFTt(Xd)
By expanding Xd(ejθ) as the DTFT of the samples x(n), the formula for
reconstructing x(t) as a superposition of sinc functions weighted by the
samples, depicted in Fig. B.1, is obtained:
x(t)
=
IDTFTt(Xd)
∆=
1
2π
 π
−π
Xd(ejθ)ejωtdω
=
Ts
2π
 π/Ts
−π/Ts
Xd(ejωdTs)ejωdtdωd
∆=
Ts
2π
 π/Ts
−π/Ts
+
∞

n=−∞
x(nTs)e−jωdnTs
,
ejωdtdωd
=
∞

n=−∞
x(nTs) Ts
2π
 π/Ts
−π/Ts
ejωd(t−nTs)dωd



∆=h(t−nTs)
∆=
∞

n=−∞
x(nTs)h(t −nTs)
∆=
(x ∗h)(t)
where we deﬁned
h(t −nTs)
∆=
Ts
2π
 π/Ts
−π/Ts
ejωd(t−nTs)dωd
=
Ts
2π
2
2j(t −nTs)

ejπ t−n
T s /Ts −e−jπ t−nTs
Ts
	
=
sin

π

t
Ts −n
	
π

t
Ts −n

∆=
sinc(t −nTs)
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

APPENDIX B. SAMPLING THEORY
Page 225
I.e.,
h(t) = sinc(t) ∆= sin(πt)
πt
The “sinc function” is deﬁned with π in its argument so that it has
zero crossings on the integers, and its peak magnitude is 1. Figure B.2
illustrates the appearance of the sinc function.
We have shown that when x(t) is band-limited to less than half the
sampling rate, the IFT of the zero-extended DTFT of its samples x(nTs)
gives back the original continuous-time signal x(t).
Conversely, if x(t) can be reconstructed from its samples xd(n) ∆=
x(nTs), it must be true that x(t) is band-limited to (−Fs/2, Fs/2), since
a sampled signal only supports frequencies up to Fs/2 (see Appendix B.4).
This completes the proof of Shannon’s Sampling Theorem. ✷
A “one-line summary” of Shannon’s sampling theorem is as follows:
x(t) = IFTt {ZeroPad∞{DTFT{xd}}}
That is, the domain of the Discrete-Time Fourier Transform of the sam-
ples is extended to plus and minus inﬁnity by zero (“zero padded”),
and the inverse Fourier transform of that gives the original signal. The
Continuous-Time Aliasing Theorem provides that the zero-padded DTFT{xd}
and the original signal spectrum FT{x} are identical, as needed.
Shannon’s sampling theorem is easier to show when applied to discrete-
time sampling-rate conversion, i.e., when simple decimation of a discrete
time signal is being used to reduce the sampling rate by an integer factor.
In analogy with the Continuous-Time Aliasing Theorem of §B.2, the Dec-
imation Theorem states that downsampling a digital signal by an integer
factor L produces a digital signal whose spectrum can be calculated by
partitioning the original spectrum into L equal blocks and then summing
(aliasing) those blocks.
If only one of the blocks is nonzero, then the
original signal at the higher sampling rate is exactly recoverable.
B.4
Another Path to Sampling Theory
Consider z0 ∈C, with |z0| = 1. Then we can write z0 in polar form as
z0
∆= ejθ0 ∆= ejω0Ts,
where θ0, ω0, and Ts are real numbers.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 226
B.4. ANOTHER PATH TO SAMPLING THEORY
Forming a geometric sequence based on z0 yields the sequence
x(tn) ∆= zn
0 = ejθ0n = ejω0tn
where tn
∆= nTs. Thus, successive integer powers of z0 produce a sampled
complex sinusoid with unit amplitude, and zero phase. Deﬁning the sam-
pling interval as Ts in seconds provides that ω0 is the radian frequency in
radians per second.
B.4.1
What frequencies are representable by a geometric
sequence?
A natural question to investigate is what frequencies ω0 are possible. The
angular step of the point zn
0 along the unit circle in the complex plane is
θ0 = ω0Ts. Since ej(θ0n+2π) = ejθ0n, an angular step θ0 > 2π is indistin-
guishable from the angular step θ0 −2π. Therefore, we must restrict the
angular step θ0 to a length 2π range in order to avoid ambiguity.
Recall that we need support for both positive and negative frequencies
since equal magnitudes of each must be summed to produce real sinusoids,
as indicated by the identities
cos(ω0tn)
=
1
2ejω0tn + 1
2e−jω0tn
sin(ω0tn)
=
−j
2ejω0tn + j
2e−jω0tn.
The length 2π range which is symmetric about zero is
θ0 ∈[−π, π],
which, since θ0 = ω0Ts = 2πf0Ts, corresponds to the frequency range
ω0
∈
[−π/Ts, π/Ts]
f0
∈
[−Fs/2, Fs/2]
However, there is a problem with the point at f0 = ±Fs/2: Both +Fs/2
and −Fs/2 correspond to the same point z0 = −1 in the z-plane. Tech-
nically, this can be viewed as aliasing of the frequency −Fs/2 on top of
Fs/2, or vice versa. The practical impact is that it is not possible in gen-
eral to reconstruct a sinusoid from its samples at this frequency. For an
obvious example, consider the sinusoid at half the sampling-rate sampled
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

APPENDIX B. SAMPLING THEORY
Page 227
on its zero-crossings: sin(ω0tn) = sin(πn) ≡0. We cannot be expected
to reconstruct a nonzero signal from a sequence of zeros! For the signal
cos(ω0tn) = cos(πn) = (−1)n, on the other hand, we sample the positive
and negative peaks, and everything looks looks ﬁne. In general, we ei-
ther do not know the amplitude, or we do not know phase of a sinusoid
sampled at exactly twice its frequency, and if we hit the zero crossings,
we lose it altogether.
In view of the foregoing, we may deﬁne the valid range of “digital
frequencies” to be
θ0
∈
[−π, π)
ω0
∈
[−π/Ts, π/Ts)
f0
∈
[−Fs/2, Fs/2)
While you might have expected the open interval (−π, π), we are free
to give the point z0 = −1 either the largest positive or largest negative
representable frequency. Here, we chose the largest negative frequency
since it corresponds to the assignment of numbers in two’s complement
arithmetic, which is often used to implement phase registers in sinusoidal
oscillators. Since there is no corresponding positive-frequency component,
samples at Fs/2 must be interpreted as samples of clockwise circular mo-
tion around the unit circle at two points. Such signals are any alternating
sequence of the form c(−1)n, where c can be be complex. The amplitude
at −Fs/2 is then deﬁned as |c|, and the phase is ̸ c.
We have seen that uniformly spaced samples can represent frequencies
f0 only in the range [−Fs/2, Fs/2), where Fs denotes the sampling rate.
Frequencies outside this range yield sampled sinusoids indistinguishable
from frequencies inside the range.
Suppose we henceforth agree to sample at higher than twice the high-
est frequency in our continuous-time signal. This is normally ensured in
practice by lowpass-ﬁltering the input signal to remove all signal energy
at Fs/2 and above. Such a ﬁlter is called an anti-aliasing ﬁlter, and it is
a standard ﬁrst stage in an Analog-to-Digital (A/D) Converter (ADC).
Nowadays, ADCs are normally implemented within a single integrated
circuit chip, such as a CODEC (for “coder/decoder”) or “multimedia
chip”.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 228
B.4. ANOTHER PATH TO SAMPLING THEORY
B.4.2
Recovering a Continuous Signal from its Samples
Given samples of a properly band-limited signal, how do we reconstruct
the original continuous waveform? I.e., given x(tn), n = 0, 1, 2, . . . , N −1,
how do we compute x(t) for any value of t?
One reasonable deﬁnition for x(t) can be based on the DFT of x(n):
X(ejωk) ∆=
N−1

n=0
x(tn)e−jωktn,
k = 0, 1, 2, . . . , N −1
Since X(ejωk) gives the magnitude and phase of the sinusoidal component
at frequency ωk, we simply construct x(t) as the sum of its constituent
sinusoids, using continuous-time versions of the sinusoids:
x(t) ∆=
N−1

k=0
X(ejωk)ejωkt
This method is based on the fact that we know how to reconstruct sam-
pled complex sinusoids exactly, since we have a “closed form” formula for
any sinusoid. This method makes perfectly ﬁne sense, but note that this
deﬁnition of x(t) is periodic with period NTs seconds. This happens be-
cause each of the sinusoids ejωkt repeats after NTs seconds. This is known
as the periodic extension property of the DFT, and it results from the
fact that we had only N samples to begin with, so some assumption must
be made outside that interval. We see that the “automatic” assumption
built into the math is periodic extension. However, in practice, it is far
more common to want to assume zeros outside the range of samples:
x(t) ∆=
 )N−1
k=0 X(ejωk)ejωkt,
0 ≤t ≤(N −1)Ts
0,
otherwise
Note that the nonzero time interval can be chosen as any length NTs
interval. Often the best choice is [−Ts(N −1)/2, Ts(N −1)/2 −1], which
allows for both positive and negative times. (“Zero-phase ﬁltering” must
be implemented using this convention, for example.)
“Chopping” the sum of the N “DFT sinusoids” in this way to zero
outside an N-sample range works ﬁne if the original signal x(tn) starts
out and ﬁnishes with zeros. Otherwise, however, the truncation will cause
errors at the edges, as can be understood from Fig. B.1.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

APPENDIX B. SAMPLING THEORY
Page 229
Does it Work?
It is straightforward to show that the “additive synthesis” reconstruction
method of the previous section actually works exactly (in the periodic
case) in the following sense:
• The reconstructed signal x(t) is band-limited to [−π, π), i.e., its
Fourier transform X(ω) is zero for all |ω| > π/Ts. (This is not
quite true in the truncated case.)
• The reconstructed signal x(t) passes through the samples x(tn) ex-
actly. (This is true in both cases.)
Is this enough? Are we done? Well, not quite. We know by construc-
tion that x(t) is a band-limited interpolation of x(tn). But are band-
limited interpolations unique? If so, then this must be it, but are they
unique? The answer turns out to be yes, based on Shannon’s Sampling
Theorem.
The uniqueness follows from the uniqueness of the inverse
Fourier transform. We still have two diﬀerent cases, however, depending
on whether we assume periodic extension or zero extension beyond the
range n ∈[0, N −1]. In the periodic case, we have found the answer;
in the zero-extended case, we need to use the sum-of-sincs construction
provided in the proof of Shannon’s sampling theorem.
Why do the DFT sinusoids suﬃce for interpolation in the periodic case
and not in the zero-extended case? In the periodic case, the spectrum
consists of the DFT frequencies and nothing else, so additive synthesis
using DFT sinusoids works perfectly. A sum of N DFT sinusoids can
only create a periodic signal (since each of the sinusoids repeats after N
samples). Truncating such a sum in time results in all frequencies being
present to some extent (save isolated points) from ω = 0 to ω = ∞.
Therefore, the truncated result is not band-limited, so it must be wrong.
It is a well known Fourier fact that no function can be both time-
limited and band-limited. Therefore, any truly band-limited interpolation
must be a function which has inﬁnite duration, such as the sinc function
sinc(Fst) used in bandlimited interpolation by a sum of sincs. Note that
such a sum of sincs does pass through zero at all sample times in the
“zero extension” region.
...
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 230
B.4. ANOTHER PATH TO SAMPLING THEORY
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Bibliography
[1] R. V. Churchill, Complex Variables and Applications, McGraw-Hill,
New York, 1960.
[2] W. R. LePage, Complex Variables and the Laplace Transform for
Engineers, Dover, New York, 1961.
[3] J. Gullberg, Mathematics From the Birth of Numbers, Norton and
Co., New York, 1997, [Qa21.G78 1996] ISBN 0-393-04002-X.
[4] J. O. Smith, “Introduction to digital ﬁlter theory”, in Digital Audio
Signal Processing: An Anthology, J. Strawn, Ed. William Kaufmann,
Inc., Los Altos, California, 1985,
(out of print). Updated version
available
online
at
http://www-ccrma.stanford.edu/˜jos/ﬁlters/.
Original version available as Stanford University Department of Mu-
sic Technical Report STAN–M–20, April 1985. A shortened version
appears in [25].
[5] L. R. Rabiner and B. Gold, Theory and Application of Digital Signal
Processing, Prentice-Hall, Inc., Englewood Cliﬀs, NJ, 1975.
[6] A. V. Oppenheim and R. W. Schafer,
Digital Signal Processing,
Prentice-Hall, Inc., Englewood Cliﬀs, NJ, 1975.
[7] A. D. Pierce,
Acoustics,
American Institute of Physics, for the
Acoustical Society of America, (516)349-7800 x 481, 1989.
[8] S. S. Stevens and H. Davis, Hearing: It’s Psychology and Physiology,
American Institute of Physics, for the Acoustical Society of America,
(516)349-7800 x 481, 1983, Copy of original 1938 edition.
[9] J. Dattorro, “The implementation of recursive digital ﬁlters for high-
ﬁdelity audio”, Journal of the Audio Engineering Society, vol. 36,
231

Page 232
BIBLIOGRAPHY
pp. 851–878, Nov. 1988, Comments, ibid. (Letters to the Editor),
vol. 37, p. 486 (1989 June); Comments, ibid. (Letters to the Editor),
vol. 38, pp. 149-151 (1990 Mar.).
[10] L. R. Rabiner and R. W. Schafer, Digital Processing of Speech Sig-
nals, Prentice-Hall, Englewood Cliﬀs, NJ, 1978.
[11] A. Papoulis,
Probability, Random Variables, and Stochastic Pro-
cesses, McGraw-Hill, New York, 1965.
[12] L. L. Sharf, Statistical Signal Processing, Detection, Estimation, and
Time Series Analysis, Addison-Wesley, Reading MA, 1991.
[13] T. Kailath, A. H. Sayed, and B. Hassibi,
Linear Estimation,
Prentice-Hall, Inc., Englewood Cliﬀs, NJ, April 2000.
[14] K. Steiglitz, A Digital Signal Processing Primer with Applications
to Audio and Computer Music, Addison-Wesley, Reading MA, 1996.
[15] C. Dodge and T. A. Jerse, Computer Music, Schirmer, New York,
1985.
[16] B. Noble, Applied Linear Algebra, Prentice-Hall, Inc., Englewood
Cliﬀs, NJ, 1969.
[17] M. Abramowitz and I. A. Stegun, Eds., Handbook of Mathematical
Functions, Dover, New York, 1965.
[18] R. Bracewell, The Fourier Transform and its Applications, McGraw-
Hill, New York, 1965.
[19] E. O. Brigham, The Fast Fourier Transform, Prentice-Hall, Inc.,
Englewood Cliﬀs, NJ, 1974.
[20] S. M. Kay, Modern Spectral Estimation, Prentice-Hall, Inc., Engle-
wood Cliﬀs, NJ, 1988.
[21] L. Ljung and T. L. Soderstrom, Theory and Practice of Recursive
Identiﬁcation, MIT Press, Cambridge, MA, 1983.
[22] F. J. Harris, “On the use of windows for harmonic analysis with the
discrete Fourier transform”, Proceedings of the IEEE, vol. 66, no. 1,
pp. 51–83, Jan 1978.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

BIBLIOGRAPHY
Page 233
[23] G. H. Golub and C. F. Van Loan, Matrix Computations, 2nd Edition,
The Johns Hopkins University Press, Baltimore, 1989.
[24] J. H. McClellan, R. W. Schafer, and M. A. Yoder, DSP First: A
Multimedia Approach,
Prentice-Hall, Englewood Cliﬀs, NJ, 1998,
Tk5102.M388.
[25] C. Roads, Ed., The Music Machine, MIT Press, Cambridge, MA,
1989.
[26] H. L. F. von Helmholtz, Die Lehre von den Tonempﬁndungen als
Physiologische Grundlage f¨ur die Theorie der Musik, F. Vieweg und
Sohn, Braunschweig, 1863.
[27] C. Roads, The Computer Music Tutorial, MIT Press, Cambridge,
MA, 1996.
[28] N. H. Fletcher and T. D. Rossing, The Physics of Musical Instru-
ments, Second Edition, Springer Verlag, New York, 1998, 485 illus-
trations, hardcover, 69.95 USD direct from Springer.
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Index
20 dB boost, 45
3 dB boost, 46
absolute value of a complex num-
ber, 14
aliased sinc function, 134
Aliasing, 160
aliasing operator, 161
amplitude response, 181
anti-aliasing lowpass ﬁlter, 163
antilog, 41
antilogarithm, 41
antisymmetric functions, 163
argument of a complex number,
14
average power, 66
bandlimited interpolation in the
frequency domain, 156
bandlimited interpolation in the
time domain, 156
base, 41
base 10 logarithm, 42
base 2 logarithm, 42
bel, 44
bin (discrete Fourier transform),
136
bin number, 136
binary, 53
binary digits, 53
bits, 53
carrier, 87
Cartesian coordinates, 14
characteristic of the logarithm, 42
circular cross-correlation, 182
CODEC, 64
coeﬃcient of projection, 132
coherence function, 187
column-vector, 212
comb-ﬁlter, 75
common logarithm, 41
commutativity of convolution, 151
companding, 52, 64
complex amplitude, 88
complex conjugate, 14
complex matrix, 211
complex matrix transpose, 211
complex multiplication, 12
complex number, 12
complex numbers, 10
argument or angle or phase,
14
modulus or magnitude or ra-
dius or absolute value or
norm, 14
complex plane, 13
conjugation in the frequency do-
main corresponds to re-
versal in the time domain,
167
convolution, 151
convolution in the frequency do-
main, 173
234

INDEX
Page 235
convolution representation, 180
correlation operator, 154
cross-correlation, circular, 182
cross-covariance, 187
cross-spectral density, 183
cross-talk, 134
cyclic convolution, 151
dB per decade, 45
dB per octave, 46
dB scale, 44
De Moivre’s theorem, 17
decibel, 44
decimal numbers, 54
DFT as a digital ﬁlter, 133
DFT matrix, 139
digit, 54
digital ﬁlter, 180
Discrete Fourier Transform, 145
Discrete Fourier Transform (DFT),
1
dynamic range, 52
dynamic range of magnetic tape,
52
energy, 44
Euler’s Formula, 15
Euler’s Theorem, 27
even functions, 163
expected value, 65
factored form, 7
fast convolution, 172
ﬂip operator, 148
Fourier Dual, 173
Fourier theorems, 145
frequency bin, 136
frequency response, 181
frequency-domain aliasing, 160,
161
fundamental theorem of algebra,
11
geometric sequence, 127
geometric series, 127
Hermitian spectra, 167
Hermitian symmetry, 167
Hermitian transpose, 212
hex, 54
hexadecimal, 54
ideal bandlimited interpolation,
177, 178
ideal bandlimited interpolation in
the time domain, 178
ideal lowpass ﬁltering operation
in the frequency domain,
178
identity matrix, 214
IDFT, 145
imaginary part, 12
impulse response, 180
impulse signal, 180
indicator function, 171
Intensity, 44
intensity level, 48
interpolation operator, 177
inverse DFT, 1, 145
inverse DFT matrix, 139
irrational number, 30
JND, 45
just-noticeable diﬀerence, 45
lag, 154
lagged product, 154
length M even rectangular win-
dowing operation, 178
linear combination, 88
linear phase FFT windows, 171
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

Page 236
INDEX
linear phase signal, 171
linear phase term, 170
linear transformation, 214
logarithm, 41
loudness, 47
magnitude of a complex number,
14
main lobe, 134
mantissa, 42
matched ﬁlter, 153
matrices, 211
matrix, 211
matrix columns, 211
matrix multiplication, 212
matrix rows, 211
matrix transpose, 211
mean, 65
mean of a random variable, 65
mean of a signal, 65
mean square, 66
mean value, 65
modulo, 147
modulus of a complex number,
14
moments, 66
monic, 7
Mth roots of unity, 29
mu-law companding, 64
multiplication in the time domain,
173
multiplication of large numbers,
42
multiplying two numbers convolves
their digits, 154
natural logarithm, 42
negating spectral phase ﬂips the
signal around backwards
in time, 167
non-commutativity of matrix mul-
tiplication, 213
nonlinear system of equations, 8
norm of a complex number, 14
normalized inverse DFT matrix,
139
normalized DFT, 174
normalized DFT matrix, 139
normalized DFT sinusoid, 174
normalized DFT sinusoids, 132,
136
normalized frequency, 146
octal, 54
odd functions, 163
orthogonal, 139
orthogonality of sinusoids, 128
orthonormal, 139
parabola, 9
PCM, 53
periodic, 146
periodic extension, 134, 146
periodogram method, 186
periodogram method for power spec-
trum estimation, 186
phase response, 182
phasor, 87
phasor angle, 88
phasor magnitude, 88
phon amplitude scale, 48
polar coordinates, 14
polar form, 29
polynomial approximation, 39
power, 44
power spectral density, 186
power spectrum, 186
pressure, 44
primitive Mth root of unity, 30
primitive Nth root of unity, 129
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.

INDEX
Page 237
quadratic formula, 9
rational, 29
real part, 12
rectangular window, 136
rectilinear coordinates, 14
rms level, 66
roots, 7
roots of unity, 29, 128
row-vector, 212
sample coherence function, 188
sample mean, 66
sample variance, 66
sampling rate, 2
second central moment, 66
sensation level, 48
shift operator, 148
sidelobes, 134
signal dynamic range, 52
similarity theorem, 188
sinc function, 134, 179
sinc function, aliased, 134
skew-Hermitian, 167
smoothing in the frequency do-
main, 173
sone amplitude scale, 48
Sound Pressure Level, 47
spectral leakage, 134
Spectrum, 2
spectrum, 132, 145
SPL, 47
square matrix, 211
standard deviation, 66
stretch by factor L, 155
Stretch Operator, 155
symmetric functions, 163
system identiﬁcation, 184, 188
Taylor series expansion, 36
time-domain aliasing, 160, 161
Toeplitz, 214
transform pair, 146
transpose of a matrix product,
213
unbiased cross-correlation, 183
unit pulse signal, 180
unitary, 139
variance, 66
window, 134
windowing in the time domain,
173
Zero padding, 155
zero padding, 177
zero padding in the frequency do-
main, 156, 178
zero padding in the time domain,
156
zero phase signal, 169
zeros, 7
DRAFT of “Mathematics of the Discrete Fourier Transform (DFT),” by J.O.
Smith, CCRMA, Stanford, Winter 2002. The latest draft and linked HTML
version are available on-line at http://www-ccrma.stanford.edu/~jos/mdft/.


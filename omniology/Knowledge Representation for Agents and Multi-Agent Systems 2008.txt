Lecture Notes in Artiﬁcial Intelligence
5605
Edited by R. Goebel, J. Siekmann, and W. Wahlster
Subseries of Lecture Notes in Computer Science

John-Jules Ch. Meyer Jan Broersen (Eds.)
KnowledgeRepresentation
for Agents
and Multi-Agent Systems
First International Workshop, KRAMAS 2008
Sydney, Australia, September 17, 2008
Revised Selected Papers
1 3

Series Editors
Randy Goebel, University of Alberta, Edmonton, Canada
Jörg Siekmann, University of Saarland, Saarbrücken, Germany
Wolfgang Wahlster, DFKI and University of Saarland, Saarbrücken, Germany
Volume Editors
John-Jules Ch. Meyer
Jan Broersen
Universiteit Utrecht, Department of Information and Computing Sciences
Padualaan 14, De Uithof, 3584 CH Utrecht, The Netherlands
E-mail: {jj,broersen}@cs.uu.nl
Library of Congress Control Number: 2009937292
CR Subject Classiﬁcation (1998): I.2.4-6, I.2, F.4.1, H.3, H.2.8, F.1
LNCS Sublibrary: SL 7 – Artiﬁcial Intelligence
ISSN
0302-9743
ISBN-10
3-642-05300-9 Springer Berlin Heidelberg New York
ISBN-13
978-3-642-05300-9 Springer Berlin Heidelberg New York
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, speciﬁcally the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting,
reproduction on microﬁlms or in any other way, and storage in data banks. Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965,
in its current version, and permission for use must always be obtained from Springer. Violations are liable
to prosecution under the German Copyright Law.
springer.com
© Springer-Verlag Berlin Heidelberg 2009
Printed in Germany
Typesetting: Camera-ready by author, data conversion by Scientiﬁc Publishing Services, Chennai, India
Printed on acid-free paper
SPIN: 12780396
06/3180
5 4 3 2 1 0

Preface
This book comprises the formal proceedings of KRAMAS 2008, a workshop on
Knowledge Representation for Agents and Multi-Agent Systems, held at KR
2008, Sydney, Australia, September 17, 2008. The initiative for KRAMAS 2008
was taken by last year’s KR Chairs to enhance cross-fertilization between the KR
(Knowledge Representation and Reasoning) and agent communities. To promote
participation in the KR schedule, the workshop was conveniently ‘sandwiched’
between days with regular KR sessions. The topics solicited included:
– Knowledge representation and reasoning aspects of agent systems (langua-
ges, ontologies, techniques)
– Reasoning about (actions of) agents
– Reasoning methods (such as non-monotonic reasoning, abduction, argumen-
tation, diagnosis, planning, decision making under uncertainty, reasoning
about preference, ...) applied to agents and multi-agent systems (MAS)
– Theory of negotiation, communication, cooperation, group decision making,
game theory for MAS
– Cognitive robotics
– Representations of other agents / opponent models
– Logics for intelligent agents and MAS
– Speciﬁcation and veriﬁcation techniques for agents
– Automated reasoning techniques for agent-based systems
– Logical foundations of agent-based systems, normative MAS and e-institutions
– Formal semantics of agent programming languages
– Formal techniques for agent-oriented programming and agent-oriented soft-
ware engineering
We originally received 14 papers. There were two review rounds: the ﬁrst
one deciding on acceptance for presentation at the workshop, and the second
one deciding on inclusion of revised, extended and resubmitted versions of the
presented papers in these proceedings. Of the original 14, 10 papers made it into
these proceedings. The workshop was a success and proved that there is indeed
much interest in the problems and issues arising at the junction of KR and MAS.
We are grateful to the participants of KRAMAS 2008 and to the authors who
submitted papers, to the members of the Program Committee for their service
in reviewing papers (twice) and to the KR organization for taking the initiative
to have KRAMAS and their support in its organization. Thanks, too, to Richard
van de Stadt whose CyberChairPRO system was a very great help to us. Finally
we are indebted to Springer, and Alfred Hofmann in particular, for their support
in getting these proceedings published.
April 2009
John-Jules Ch. Meyer
Jan Broersen

Workshop Organization
Program Chair
John-Jules Meyer
Workshop Chairs
John-Jules Meyer
Utrecht University, The Netherlands
Jan Broersen
Utrecht University, The Netherlands
Program Committee
Thomas Agotnes
Bergen, Norway
Natasha Alechina
Nottingham, UK
Jamal Bentahar
Montreal, Canada
Rafael Bordini
Durham, UK
Jan Broersen
Utrecht, The Netherlands
Mehdi Dastani
Utrecht, The Netherlands
Giuseppe De Giacomo
Rome, Italy
Hans van Ditmarsch
Otago, New Zealand
Jurgen Dix
Clausthal, Germany
Andreas Herzig
Toulouse, France
Wiebe van der Hoek
Liverpool, UK
Wojciech Jamroga
Clausthal, Germany
Catholijn Jonker
Delft, The Netherlands
Yves Lesperance
Toronto, Canada
Alessio Lomuscio
London, UK
Timothy Norman
Aberdeen, UK
Henry Prakken
Utrecht, The Netherlands
Alessandro Ricci
Cesena, Italy
Renate Schmidt
Manchester, UK
Carles Sierra
Barcelona, Spain
Francesca Toni
London, UK
Rineke Verbrugge
Groningen, The Netherlands

Table of Contents
Reasoning about Other Agents’ Beliefs under Bounded Resources . . . . . .
1
Natasha Alechina, Brian Logan, Hoang Nga Nguyen, and
Abdur Rakib
Normative Multi-agent Programs and Their Logics . . . . . . . . . . . . . . . . . . .
16
Mehdi Dastani, Davide Grossi, John-Jules Ch. Meyer, and
Nick Tinnemeier
Modal Logics for Preferences and Cooperation: Expressivity and
Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
C´edric D´egremont and Lena Kurzen
Simulation and Information: Quantifying over Epistemic Events. . . . . . . .
51
Hans van Ditmarsch and Tim French
On the Dynamics of Institutional Agreements . . . . . . . . . . . . . . . . . . . . . . .
66
Andreas Herzig, Tiago de Lima, and Emiliano Lorini
From Trust in Information Sources to Trust in Communication
Systems: An Analysis in Modal Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
Emiliano Lorini and Robert Demolombe
Pre-processing Techniques for Anytime Coalition Structure Generation
Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
Tomasz Michalak, Andrew Dowell, Peter McBurney, and
Michael Wooldridge
Cognitive Use of Artifacts: Exploiting Relevant Information Residing
in MAS Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
114
Michele Piunti and Alessandro Ricci
Information-Based Argumentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
Carles Sierra and John Debenham
Mediation = Information Revelation + Analogical Reasoning . . . . . . . . . .
145
Simeon Simoﬀ, Carles Sierra, and Ramon L´opez de M`antaras
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
161

Reasoning about Other Agents’ Beliefs under
Bounded Resources
Natasha Alechina, Brian Logan, Hoang Nga Nguyen, and Abdur Rakib⋆
School of Computer Science
University of Nottingham
Nottingham NG8 1BB, UK
{nza,bsl,hnn,rza}@cs.nott.ac.uk
Abstract. There exists a considerable body of work on epistemic logics for
bounded reasoners where the bound can be time, memory, or the amount of in-
formation the reasoners can exchange. In much of this work the epistemic logic
is used as a meta-logic to reason about beliefs of the bounded reasoners from
an external perspective. In this paper, we present a formal model of a system of
bounded reasoners which reason about each other’s beliefs, and propose a sound
and complete logic in which such reasoning can be expressed. Our formalisation
highlights a problem of incorrect belief ascription in resource-bounded reasoning
about beliefs, and we propose a possible solution to this problem, namely adding
reasoning strategies to the logic.
1
Introduction
The purpose of this paper is to investigate a multi-agent epistemic logic which results
from taking seriously the idea that agents have bounded time, memory and communi-
cation resources, and are reasoning about each other’s beliefs. The main contribution
of the paper is to generalise several existing epistemic logics for resource-bounded rea-
soners by adding an ability for reasoners to reason about each other’s beliefs. We show
that a problem of incorrect belief ascription arises as a result, and propose a possible
solution to this problem.
To give the reader an idea where the current proposal ﬁts into the existing body
of research on epistemic logics for bounded reasoners, we include a brief survey of
existing approaches, concentrating mostly on the approaches which have inﬂuenced the
work presented here.
In standard epistemic logic (see e.g. [1,2] for a survey) an agent’s (implicit) knowl-
edge is modelled as closed under logical consequence. This can clearly pose a problem
when using an epistemic logic to model resource-bounded reasoners, whose set of be-
liefs is not generally closed with respect to their reasoning rules. Various proposals to
modify possible worlds semantics in order to solve this problem of logical omniscience
(e.g., introducing impossible worlds as in [3,4], or non-classical assignment as in [5])
result in agent’s beliefs still being logically closed, but with respect to a weaker logic.
⋆This work was supported by the UK Engineering and Physical Sciences Research Council
[grant number EP/E031226].
J.-J.Ch. Meyer and J.M. Broersen (Eds.): KRAMAS 2008, LNAI 5605, pp. 1–15, 2009.
c⃝Springer-Verlag Berlin Heidelberg 2009

2
N. Alechina et al.
Our work builds on another approach to solving this problem, namely treating be-
liefs as syntactic objects rather than propositions (sets of possible worlds). In [6], Fagin
and Halpern proposed a model of limited reasoning using the notion of awareness: an
agent explicitly believes only the formulas which are in a syntactically deﬁned aware-
ness set (as well as in the set of its implicit beliefs). Implicit beliefs are still closed under
consequence, but explicit beliefs are not, since a consequence of explicit beliefs is not
guaranteed to belong to the awareness set. However, the awareness model does not give
any insight into the connection between the agent’s awareness set and the agent’s re-
source limitations, which is what we try to do in this paper.1 Konolige [7] proposed a
different model of non-omniscient reasoners, the deduction model of belief. Reasoners
were parameterised with sets of rules which could, for example, be incomplete. However,
the deduction model of belief still models beliefs of a reasoner as closed with respect to
reasoner’s deduction rules; it does not take into account the time it takes to produce this
closure, or any limitations on the agent’s memory. Step logic, introduced in [8], gives a
syntactic account of beliefs as theories indexed by time points; each application of in-
ference rules takes a unit of time. No ﬁxed bound on memory was considered, but the
issue of bounded memory was taken into account. An account of epistemic logic called
algorithmic knowledge, which treats explicit knowledge as something which has to be
computed by an agent, was introduced in [9], and further developed in e.g. [1,10]. In the
algorithmic knowledge approach, agents are assumed to possess a procedure which they
use to produce knowledge. In later work [10] this procedure is assumed to be given as a
set of rewrite rules which are applied to the agent’s knowledgeto producea closed set, so,
like Konolige’s approach,algorithmicknowledgeis concernedwith the result rather than
the process of producing knowledge. In [11,12] Duc proposed logics for non-omniscient
epistemic reasoners which will believe all consequences of their beliefs eventually, after
some interval of time. It was shown in [13] that Duc’s system is complete with respect
to semantics in which the set of agent’s beliefs is always ﬁnite. Duc’s system did not
model the agents’ reasoning about each others’ beliefs. Other relevant approaches where
epistemic logics were given a temporal dimension and each reasoning step took a unit
of time are, for example, [14], where each inference step is modelled as an action in the
style of dynamic logic, and [15] which proposes a logic for veriﬁcation of response-time
properties of a system of communicating rule-based agents (each rule ﬁring or commu-
nication takes a unit of time). In a somewhat different direction, [16] proposed a logic
where agents reason about each others beliefs, but have no explicit time or memory limit;
however there is a restriction on the depth of belief nestings (context switching by the
agents). Epistemic logics for bounded-memoryagents were investigated in, for example,
[17,18,19,20], and the interplay between bounded recall and bounded memory (ability
to store strategies of only bounded size) was studied in [21].
An epistemic logic BMCL for communicating agents with communication limits on
the number of exchanged messages (and connections to space complexity of proofs and
communication complexity) was investigated in [20]. In this paper we expand BMCL
by adding rules for reasoning about other agents’ beliefs, demonstrate that epistemic
reasoning done in resource-bounded fashion has an inherent problem of incorrect belief
ascription, and propose the use of reasoning strategies as a solution to this problem.
1 We also completely dispense with the notion of implicit beliefs.

Reasoning about Other Agents’ Beliefs under Bounded Resources
3
2
Model of Reasoning Agents
The logic BMCL presented in [20] formalises reasoning about the beliefs of a system of
reasoners who reason using propositional resolution and can exchange information to
solve a problem together. The set up is similar to, for example, [22]. BMCL models each
inference rule application as taking a single time step, introduces an explicit bound on
the set of beliefs of each reasoner, and a bound on the number of messages the reasoners
can exchange. In this paper, we generalise this approach by assuming that agents can
also reason about each other’s beliefs. Namely, they assume that other agents use a
certain set of inference rules, and they reason about what another agent may believe at
the next step. For example, if agent A believes that agent B believes two clauses c1 and
c2 and these two clauses are resolvable to a clause c, and agent A assumes that agent B
reasons using resolution, then it is reasonable for agent A to believe that agent B may
believe c at the next step.
We assume a set of n agents. Each agent i has a set of inference rules, a set of
premises KBi, and a working memory. To infer from the premises in KBi, the relevant
formulas must ﬁrst be read into working memory. We assume that each agent’s working
memory is bounded by nM, which is the maximal number of formulas an agent can
believe at the same time. We also set a limit on the possible size of a formula, or rather
on the depth of nesting of belief operators, nB, and a limit, nC, on the maximal number
of communications an agent can make. For simplicity, we assume that these bounds are
the same for all agents, but this can be easily relaxed by introducing functions nM(i),
nB(i) and nC(i) which assign a different limit to each agent i.
The set of reasoning actions is as follows:
Read KB: an agent can retrieve information from its KB and put it into its working
memory using the Read action. Since an agent has a ﬁxed size memory, adding a
formula to its memory may require erasing some belief already in memory (if the
limit nM would otherwise be exceed). The same applies to other reasoning actions
which add a new formula, in that adding a new formula may involve overwriting a
formula currently in working memory.
Resolution: an agent can derive a new clause if it has two resolvable clauses in its
memory.
Copy: an agent can communicate with another agent to request a clause from the mem-
ory of the other agent. We assume that communication is always successful if the
other agent has the requested clause. If agent A has clause c in memory, then a copy
by B will result in agent B believing that A believes c. Copy is only enabled if the
agent has performed fewer than nC copy actions in the past and the preﬁx of the
resulting belief has nesting of at most nB.
Idle: an agent may idle (do nothing) at any time step. This means that at the next time
point of the system, the agent does not change its state of memory.
Erase: an agent may remove a formula from its working memory. This action is intro-
duced for technical reasons to simplify the proofs.
In addition to the actions listed above, we introduce actions that enable agents to rea-
son about other agents’ beliefs, essentially epistemic axioms K (ascribing propositional

4
N. Alechina et al.
reasoning to the other agent) and 4 (positive introspection about the agent’s own beliefs,
and ascribing positive introspection to other agents). The reasons we do not adopt for
example KD45 are as follows. If the agent’s knowledge base is inconsistent, we want
it to be able to derive B⊥(or B[] where [] is the empty clause). Negative introspection
is also problematic in a resource-bounded setting, in that the agent may derive ¬Bα
if α is not in its current set of beliefs, and then derive α from its other beliefs, ending
up with an inconsistent set of beliefs (¬Bα and Bα by positive introspection from α),
even if its knowledge base is consistent. We could have adopted a restricted version of
negative introspection (see, e.g., [12]) but in this paper we omit it for simplicity.
In addition to the reasoning actions listed above, we therefore add the following
actions:
Other’s Resolution: an agent A can perform this action if it believes that another agent
B believes two resolvable clauses c1 and c2. Then A can conclude that B will
believe in the resolvent clause c of c1 and c2 in the next time point. As a general
case, we can extend the chain agent-believes ... agent-believes. For example, if
agent A believes that agent B believes that agent C believes two resolvable clauses
c1 and c2, then it is possible in the next time point that agent A believes that agent
B believes that agent C believes c which is the resolvent of c1 and c2.
Positive Introspection: if an agent A believes a clause c, it can perform this action to
reach a state where it believes that it believes c.
Other’s Positive Introspection: if an agent A believes that another agent B believes a
clause c, it can perform this action to reach a state where it believes that B believes
that B believes c.
The reasoning actions Positive Introspection and Other’s Positive Introspection are
only enabled if the derived formula has a depth of nesting of at most nB.
Note that the assumption that the agents reason using resolution and positive in-
trospection is not essential for the main argument of this paper. This particular set of
inference rules has been chosen to make the logic concrete; we could have, for exam-
ple, assumed that the agents reason using modus ponens and conjunction introduction
instead of resolution. In what follows, we give a formal deﬁnition of an epistemic logic
for communicating agents which reason in a step-wise, memory-bounded fashion using
some well-deﬁned set of inference rules.
3
Syntax and Semantics of ERBL
In this section, we give the syntax and semantics of the logic ERBL which formalises
the ideas sketched in the previous section. ERBL (Epistemic Resource Bounded Logic)
is an epistemic and temporal meta-language in which we can talk about beliefs ex-
pressed in the agents’ internal language.
Let the set of agents be A = {1, 2, . . ., nA}. We assume that all agents agree on a
ﬁnite set PROP of propositional variables, and that all belief formulas of the internal
language of the agents are in the form of clauses or clauses preceded by a preﬁx of
belief operators of ﬁxed length.

Reasoning about Other Agents’ Beliefs under Bounded Resources
5
From the set of propositional variables, we have the deﬁnition of all literals as fol-
lows:
LPROP = {p, ¬p | p ∈PROP}
Then, the set of all clauses is Ω = ℘(LPROP). Finally, the set of all belief formulas
is deﬁned as follows:
BΩ ::= {Bi1 . . . Bikc | c ∈Ω, 0 ≤k ≤nB},
where ij ∈A. Note that we only include in the set of belief formulas those whose belief
operator nesting is limited by nB. Therefore, BΩ is ﬁnite.
Each agent i ∈A is assumed to have a knowledge base KBi ⊆BΩ.
For convenience, the negation of a literal L is deﬁned as ¬L, where:
¬L =
¬p if L = p for some p ∈PROP
p
if L = ¬p for some p ∈PROP
The form of resolution rule which will be used in formal deﬁnitions below is as
follows: given two clauses c1 and c2 ∈Ω such that one contains a literal L and the other
has its negation ¬L, we can derive a new clause which is the union c1\{L}∪c2\{¬L}.
The syntax of ERBL is then deﬁned inductively as follows.
– ⊤is a well-formed formula (wff) of ERBL.
– start is a wff of ERBL; it is a marker for the start state.
– cp=n
i
(the number of communication actions performed by agent i) is a wff of
ERBL for all n = 0, . . . , nC, and i ∈A; it is used as a communication counter in
the language.
– If α ∈BΩ, then Biα (agent i believes α) is a wff of ERBL, i ∈A.
– If ϕ and ψ are wffs, then so are ¬ϕ, ϕ ∧ψ.
– If ϕ and ψ are wffs, then so are Xϕ (ϕ holds in the next moment of time), ϕUψ(ϕ
holds until ψ), and Aϕ (ϕ holds on all paths).
Classical abbreviations for ∨, →, ↔are deﬁned as usual. We also have ⊥≡¬⊤,
Fϕ ≡⊤Uϕ (ϕ holds some time in the future), Eϕ ≡¬A¬ϕ (ϕ holds on some path).
For convenience, let CPi = {cp=n
i
|n = {0, . . . , nC}} and CP = 
i∈A CPi.
The semantics of ERBL is deﬁned by ERBL transition systems which are based
on ω-tree structures (standard CTL∗models as deﬁned in [23]).
Let (T, R) be a pair where T is a set and R is a binary relation on T . Let the relation
< be the irreﬂexive and transitive closure of R, namely the set of pairs of states {(s, t) ∈
T × T | ∃n ≥0, t0 = s, .., tn = t ∈T such that tiRti+1 for all i = 0, . . . , n −1}.
(T, R) is a ω-tree frame iff the following conditions are satisﬁed.
1. T is a non-empty set.
2. R is total, i.e., for all t ∈T , there exists s ∈T such that tRs.
3. For all t ∈T , the past {s ∈T | s < t} is linearly ordered by <.
4. There is a smallest element called the root, denoted by t0.
5. Each maximal linearly <- ordered subset of T is order-isomorphic to the natural
numbers.

6
N. Alechina et al.
A branch of (T, R) is an ω-sequence (t0, t1, . . .) such that t0 is the root and tiRti+1
for all i ≥0. We denote by B(T, R) the set of all branches of (T, R).
A ERBL transition system M is deﬁned as a triple (T, R, V ) where:
– (T, R) is a ω-tree frame,
– V : T × A →℘(BΩ ∪CP) such that for all s ∈T and i ∈A: V (s, i) =
Q ∪{cp=n
i
} for some Q ⊆BΩ and 0 ≤n ≤nC. We denote by V ∗(s, i) the set
V (s, i) \ {cp=n
i
|0 ≤n}.
For a branch σ ∈B(T, R), σi denotes the element ti of σ and σ≤i is the preﬁx
(t0, t1, . . . , ti) of σ.
The truth of a ERBL formula at a point n of a path σ ∈B(T, R) is deﬁned induc-
tively as follows:
– M, σ, n |= ⊤,
– M, σ, n |= Biα iff α ∈V (s, i),
– M, σ, n |= start iff n = 0,
– M, σ, n |= cp=m
i
iff cp=m
i
∈V (s, i),
– M, σ, n |= ¬ϕ iff M, σ, n ̸|= ϕ,
– M, σ, n |= ϕ ∧ψ iff M, σ, n |= ϕ and M, σ, n |= ψ,
– M, σ, n |= Xϕ iff M, σ, n + 1 |= ϕ,
– M, σ, n |= ϕUψ iff ∃m ≥n such that ∀k ∈[n, m) M, σ, k |= ϕ and M, σ, m |= ψ,
– M, σ, n |= Aϕ iff ∀σ′ ∈BR such that σ′
≤n = σ≤n, M, σ′, n |= ϕ.
The set of possible transitions in a model is deﬁned as follows. Deﬁnition 1 below
describes possible outcomes of various actions. For example, performing a resolution
results in adding the resolvent to the set of beliefs. Deﬁnition 2 describes when an action
is possible or enabled. For example, resolution is enabled if the agent has two resolvable
clauses in memory.
Deﬁnition 1. Let (T, R, V ) be a tree model. The set of effective transitions Ra for
an action a is deﬁned as a subset of R and satisﬁes the following conditions, for all
(s, t) ∈R:
1. (s, t) ∈RReadi,α,β iff α ∈KBi, α /∈V (s, i) and V (t, i) = V (s, i) \ {β} ∪{α}.
This condition says that s and t are connected by agent i’s Read transition if the
following is true: α is in i’s knowledge base but not in V (s, i), α is added to the
set of i’s beliefs at t, and β ∈BΩ is removed from the agent’s set of beliefs.
The argument β stands for a formula which is overwritten in the transition. If β ∈
V (s, i) then the agent actually loses a belief in the transition, if β ̸∈V (s, i) then
the transition only involves adding a formula α without removing any beliefs.
2. (s, t)∈RResi,α1 ,α2,L,β where α1 =Bi1 . . . Bik−1Bikc1 and α2 =Bi1 . . . Bik−1Bikc2
iff α1 ∈V (s, i), α2 ∈V (s, i), L ∈c1, ¬L ∈c2, α = Bi1 . . . Bik−1Bikc /∈V (s, i)
and V (t, i) = V (s, i) \ {β} ∪{α} where c = c1 \ {L} ∪c2 \ {¬L}. This condition
says that s and t are connected by agent i’s Res transition if in s agent i believes
two resolvable clauses α1 and α2 but not α, possibly preceded by the same se-
quence of belief operators, and in t agent i believes their resolvent, preceded by the
same preﬁx. Again, β ∈BΩ is overwritten if it is in the set of agent’s beliefs in s.

Reasoning about Other Agents’ Beliefs under Bounded Resources
7
3. (s, t) ∈RCopyi,α,β iff α ∈V (s, j) for some j ∈A and j ̸= i, for any cp=n
i
∈
V (s, i) such that n < nC, Bjα /∈V (s, i) and V (t, i) = V (s, i) \ {cp=n
i
|cp=n
i
∈
V (s, i)} ∪{cp=n+1
i
|cp=n
i
∈V (s, i)} \ {β} ∪{Bjα}. s and t are connected by a
Copy transition of agent i if in t, i adds to its beliefs a formula Bjα where α is an
agent j’s belief in s, and i has previously copied fewer than nC formulas. Again
some β ∈BΩ is possibly overwritten.
4. (s, t) ∈RIdlei iff V (t, i) = V (s, i). The Idle transition does not change the state.
5. (s, t) ∈RErasei,β iff V (t, i) = V (s, i) \ {β}. Erase removes one of the agent’s
beliefs.
6. (s, t) ∈RP Ii,α,β iff α ∈V (s, i), Biα /∈V (s, i) and V (t, i) = V (s, i) \ {β} ∪
{Biα}. PI is i’s positive introspection: s and t are connected by i’s PI transition
if in s it believes α but not Biα and in t it believes Biα.
7. (s, t) ∈ROP Ii,Bi1 ...Bik−1 ,Bik α,β iff Bi1 . . . Bik−1Bikα ∈V (s, i) but not Bi1 . . .
Bik−1BikBikα, V (t, i) = V (s, i) \ {β} ∪{Bi1 . . . Bik−1BikBikα}. This corre-
sponds to ascribing positive introspection to agent ik.
This speciﬁes the effects of actions. Below, we specify when an action is possible. Note
that we only enable deriving a formula if this formula is not already in the set of the
agent’s beliefs.
Deﬁnition 2. Let (T, R, V ) be a tree model. The set Acts,i of possible actions that an
agent i can perform at a state s ∈T is deﬁned as follows:
1. Readi,α,β ∈Acts,i iff α ̸∈V (s, i), α ∈KBi and β ∈V (s, i) if |V ∗(s, i)| ≥nM.
2. Resi,α1,α2,L,β ∈Acts,i iff c=(c1\L)∪(c2\¬L)̸∈V (s, i), α1 =Bi1 . . . Bik−1Bikc1,
α2 = Bi1 . . . Bik−1Bikc2, L ∈c1, ¬L ∈c2, α1, α2 ∈V (s, i), and β ∈V (s, i) if
|V ∗(s, i)| ≥nM.
3. Copyi,α,β ∈Acts,i iff Bjα ̸∈V (s, i), α ∈V (s, j) for some j ∈A and j ̸= i,
n < nC for any cp=n
i
∈V (s, i) and β ∈V (s, i) if |V ∗(s, i)| ≥nM.
4. It is always the case that Idlei ∈Acts,i.
5. PIi,α,β ∈Acts,i iff iα ̸∈V (s, i), α ∈V (s, i) and β ∈V (s, i) if |V ∗(s, i)| ≥nM.
6. OPIi,Bi1 ...Bik−1 Bik α,β ∈Acts,i iff Bi1 . . . Bik−1BikBikα ̸∈V (s, i), Bi1 . . . Bik−1
Bikα ∈V (s, i) and β ∈V (s, i) if |V ∗(s, i)| ≥nM.
There are no speciﬁed conditions for enabling Erasei,β. This action is introduced for
technical reasons, to simplify the proofs.
Finally, the deﬁnition of the set of models corresponding to a system of reasoners is
given below:
Deﬁnition 3. M(KB1, . . . , KBnA, nB, nM, nC) is the set of models (T, R, V ) which
satisﬁes the following conditions:
1. |V ∗(s, i)| ≤nM for all s ∈T and i ∈A.
2. cp=0
i
∈V (t0, i) where t0 is the root of (T, R) for all i ∈A.
3. R = 
∀a Ra.
4. For all s ∈T , ai ∈Acts,i, there exists t ∈T such that (s, t) ∈Rai for all i ∈A.

8
N. Alechina et al.
4
Axiomatisation
In this section, we introduce an axiom system which is sound and complete with respect
to the set of models deﬁned in the previous section.
Below are some abbreviations which will be used in the axiomatisation:
– ByReadi(α, n) = ¬Biα ∧cp=n
i
. This formula describes the state before the agent
comes to believe formula α by the Read transition. n is the value of i’s communi-
cation counter.
– ByResi(α, n) = ⊥if α = Bi1 . . . Bik−1¬Bikc for some c ∈Ω and 1 ≤k ≤
nB; otherwise ByResi(α, n) = ¬Biα∧
(α1,α2)∈Res−1(α) (Biα1 ∧Biα2) where
Res−1(Bi1 . . . Bik−1Bikc) = {(Bi1 . . . Bik−1Bikc1, Bi1 . . . Bik−1Bikc2) | ∃L ∈
LPROP such that c = c1 \ {L} ∪c2 \ {¬L}}. This formula describes the state
of the system before i derives α by resolution. Note that it may not be possible to
derive an arbitrary formula α by resolution; in that case, the state is described by
falsum ⊥.
– ByCopyi(α, n) = ⊥if α ̸= Bjα′ for some j ̸= i or n ≤0;
otherwise ByCopyi(Bjα′, n) = ¬BiBjα′ ∧Bjα′ ∧cp=n−1.
– ByPIi(α, n) = ⊥if α ̸= Biα′; otherwise ByPIi(α, n) = ¬BiBiα′ ∧Biα′ ∧
cp=n.
– ByOPIi(α, n) = ⊥if α ̸= Bi1 . . . Bik−1BikBikα′;
otherwise ByOPIi(α, n) = ¬BiBi1 . . . Bik−1BikBikα′ ∧BiBi1 . . . Bik−1Bikα′
∧cp=n.
The axiomatisation is as follows.
A1. All axioms and inference rules of CT L∗[24].
A2. 
γ∈Q Biγ ∧cp=n
i
∧¬Biα →EX(
γ∈Q Biγ ∧cp=n
i
∧Biα) for all α ∈KBi,
and Q ⊆BΩ such that |Q| < nM.
Intuitively, this axiom says that it is always possible to make a transition to a state
where agent i believes a formula from its knowledge base KBi. In addition, the
communication counter of the agent does not increase, and a set of beliefs Q of
cardinality less than nM can also be carried over to the same state.
Axioms A3 - A6 similarly describe transitions made by resolution (given that re-
solvable clauses are available), copy (with communication counter increased), and
positive introspection (applied by agent i or ascribed by i to another agent).
A3. 
γ∈Q Biγ ∧BiBi1 . . . Bik−1Bikc1 ∧BiBi1 . . . Bik−1Bikc2 ∧cp=n
i
∧¬BiBi1 . . .
Bik−1 Bikc →EX(
γ∈Q Biγ ∧cp=n
i
∧BiBi1 . . . Bik−1Bikc) for all c1, c2 ∈Ω
such that L ∈c1, ¬L ∈c2 and c = c1 \ {L} ∪c2 \ {¬L}, k ≥0, and Q ⊆BΩ
such that |Q| < nM.
A4. 
γ∈Q Biγ ∧Bjα ∧cp=n
i
∧¬BiBjα →EX(
γ∈Q Biγ ∧BiBjα ∧cp=n+1
i
) for
any α ∈BΩ, j ∈A, j ̸= i, n < nC, and Q ⊆BΩ such that |Q| < nM.
A5. 
γ∈Q Biγ ∧Biα∧cp=n
i
∧¬BiBiα →EX(
γ∈Q Biγ ∧BiBiα∧cp=n
i
) for any
α ∈BΩ and Q ⊆BΩ such that |Q| < nM.
A6. 
γ∈Q Biγ ∧BiBi1 . . . Bik−1Bikα ∧cp=n
i
∧¬ BiBi1 . . . Bik−1BikBikα →
EX(
γ∈Q Biγ ∧BiBi1 . . . Bik−1BikBikα ∧cp=n
i
) for any α ∈BΩ, k ≥0 and
Q ⊆BΩ such that |Q| < nM.

Reasoning about Other Agents’ Beliefs under Bounded Resources
9
A7. EX(Biα ∧Biβ) →Biα ∨Biβ.
This axiom says that at most one new belief is added in the next state.
A8. EX(¬Biα ∧¬Biβ) →¬Biα ∨¬Biβ.
This axiom says that at most one belief is deleted in the next state.
A9. EX(Biα∧cp=n
i
) →Biα∨ByReadi(α, n)∨ByResi(α, n)∨ByCopyi(α, n)∨
ByPIi(α, n) ∨ByOPIi(α, n) for α ∈KBi.
This axiom says that a new belief which is an element of the agent’s knowledge
base can only be added by one of the valid reasoning actions.
A10. EX(Biα ∧cp=n
i
) →Biα ∨ByResi(α, n) ∨ByCopyi(α, n) ∨ByPIi(α, n) ∨
ByOPIi(α, n) for α /∈KBi.
This axiom describes possible ways in which a new belief which is not in the agent’s
knowledge base can be added.
A11. Biα1 ∧. . . ∧BiαnM →¬BiαnM+1 for all i ∈A, αj ∈BΩ where j =
1, . . . , nM + 1 and all αj are pairwise different.
This axiom states that an agent cannot have more than nM different beliefs.
A12a start →cp=0
i
for all i ∈A.
In the start state, the agent has not performed any Copy actions.
A12b ¬EXstart (start only holds at the root of the tree).
A13. 
n=0...nC cp=n
i
for all i ∈A.
There is always a number n between 0 and nC corresponding to the number of
Copy actions agent i has performed.
A14. cp=n
i
→¬cp=n′
i
for all i ∈A and n′ ̸= n.
The number of previous Copy actions by i in each state is unique.
A15. ϕ →EXϕ where ϕ does not contain start.
It is always possible to make a transition to a state where all agents have the same
beliefs and communication counter values as in the current state (essentially an
Idle transition by all agents).
A16. 
i∈A EX(
γ∈Qi Biγ ∧cp=ni
i
) →EX 
i∈A(
γ∈Qi Biγ ∧cp=ni
i
) for any
Qi ⊆BΩ such that |Qi| ≤nM.
If each agent i can separately reach a state where it believes formulas in Qi, then
all agents together can reach a state where for each i, agent i believes formulas in
Qi.
Notice that since the depth of the nesting of belief operators is restricted by nB, for
any subformula Biα appearing in any above axiom, α ∈BΩ.
Deﬁnition 4. L(KB1, . . . , KBnA, nB, nM, nC) is the logic deﬁned by the axiomati-
sation A1–A16.
We have the following result.
Theorem 1. L(KB1, . . . , KBnA, nB, nM, nC) is sound and complete with respect to
M(KB1, . . . , KBnA, nB, nM, nC).
The proof is omitted due to lack of space; it is based on the proof technique used in
[24].

10
N. Alechina et al.
5
Discussion
Systems of step-wise reasoners with bounded memory and a communication limit are
faithful models of systems of distributed resource-limited reasoners, and various re-
source requirements of such systems can be effectively veriﬁed, e.g. by model-checking,
as in for example [20]. However, adding reasoning about beliefs poses a signiﬁcant
challenge, both in the complexity of the system and in the way this reasoning is mod-
elled. The branching factor of the models is much larger when reasoning about beliefs
is considered, making model-checking less feasible. The main problem however has to
do with the correctness of an agent’s belief ascription. We describe this problem below
and propose a tentative solution.
In the system proposed in this paper, agents correctly ascribe reasoning mechanisms
to each other, and in the limit, their predictions concerning other agents’ beliefs are
correct: if agent j believes that eventually agent i will believe α, then eventually agent
i will believe α, and vice versa. More precisely, for every model M and state s,
{α : M, s |= EFBjBiα} = {α : M, s |= EFBiα}
However, in spite of this, the agents are almost bound to make wrong predictions when
trying to second-guess what other reasoners will believe in the next state. More pre-
cisely,
{α : M, s |= BjBiα} ̸⊆{α : M, s |= Biα}
i.e. agent j may believe that i believes some α when i does not believe α.
Consider the following example. Suppose there are two agents, 1 and 2, each with
a memory limit of two formulas, communication limit of one formula, belief nesting
limit of two, and knowledge bases KB1 = {p} and KB2 = {q}. A possible run of the
system is shown in Figure 1.
State
Agent 1
Agent 2
t0
{ }
{ }
transition: Read
Read
t1
{p}
{q}
transition: Copy
Copy
t2
{p, B2q} {q, B1p}
Fig. 1. A possible run of the system
Note that this is only one possible run, and other transitions are possible. For exam-
ple, in t0, one or both agents can idle. In t1, one or both agents can idle, or make a
positive introspection transition. In state t2, the agents’ beliefs about each other’s be-
liefs are correct. However, in most successor states of t2, agent 1 will have incorrect
beliefs about agent 2’s beliefs, and vice versa. Indeed, the options of agent 1 in t2 are:
read p, idle, erase p, erase B2q, apply positive introspection to derive B1p or B1B2q,
ascribe introspection to agent 2 to derive B2B2q. Agent 2 has similar choices. In only
two of these cases do the agents make non-trivial (that is, new compared to the ones al-
ready existing in t2) correct belief ascriptions, namely if agent 1 derives B1p and agent

Reasoning about Other Agents’ Beliefs under Bounded Resources
11
State
Agent 1
Agent 2
t2
{p, B2q}
{q, B1p}
transition: PI, overwrite B2q OPI, overwrite q
t3
{p, B1p}
{B1p, B1B1p}
Fig. 2. Continuing from t2: a correct ascription
2 derives B1B1p, and vice versa when agent 2 derives B2q and agent 1 derives B2B2q
(see Figure 2).
Figure 3 shows one of many possible incorrect ascriptions. Note that agent 1’s as-
cription is now incorrect because agent 2 has forgotten q, and agent 2’s ascription is in-
correct because it assumed agent 1 will use positive introspection to derive B1p, which
it did not.
State
Agent 1
Agent 2
t2
{p, B2q} {q, B1p}
transition: Idle
OPI, overwrite q
t4
{p, B2q} {B1p, B1B1p}
Fig. 3. Continuing from t2: an incorrect ascription
This suggests an inherent problem with modelling agents reasoning about each other’s
beliefs in a step-wise, memory-bounded fashion. Note that this problem is essentially
one of belief ascription, i.e., of correctly predicting what another agent will believe
given limited information about what it currently believes (of deriving correct conclu-
sions from correct premises), rather than a problem of belief revision [25], i.e., what an
agent should do if it discovers the beliefs it has ascribed to another agent are incorrect.
It is also distinct from the problem of determining the consequences of information
updates as studied in dynamic epistemic logic (e.g. [26]). Adding new true beliefs in a
syntactic approach such as ours is straightforward compared to belief update in dynamic
epistemic logic, which interprets beliefs as sets of possible worlds. Essentially, in dy-
namic epistemic logic an agent acquires a new logically closed set of beliefs at the next
‘step’ after an announcement is made, while we model the gradual process of deriving
consequences from a new piece of information (and the agent’s previous beliefs).
The disparity between agent i’s beliefs and the beliefs agent j ascribes to i at each
step is due both to the fact that at most one formula is derived by each agent at any
given step (and agent j may guess incorrectly which inference rule agent i is going
to use) and to memory limitations which cause agents to forget formulas. An obvious
alternative is to do tentative ascription of beliefs to other agents, namely conclude that
the other agent will be in one of several possible belief sets in the next state, e.g.
B2B1p →EX(B2((B1p ∧B1B1p) ∨(B1p ∧¬B1B1p) ∨. . .))
However, this implies that one of the agents (agent 2 in this case) has a much larger
(exponentially larger!) memory and a more expressive internal language to reason about
the other agent’s beliefs.

12
N. Alechina et al.
It is clearly not sufﬁcient for correct belief prediction for the reasoners to ascribe to
other agents just a set of inferences rules or a logic such as KD45. They need to be
able to ascribe to other agents a reasoning strategy, or a preference order on the set
reasoning actions used by the other agents which constrains the possible transitions of
each reasoner, and directs each agent’s reasoning about the beliefs of other agents. As
a simple example, suppose agent 2 believes that agent 1’s strategy is to apply positive
introspection to formula p in preference to all other actions. Then in state t2 agent 2 will
derive B1B1p from B1p. If agent 2’s ascription of strategy to agent 1 is correct, agent
1 will indeed derive B1p from p in the next state, making agent 2’s belief prediction
correct.
6
ERBL with strategies
In this section, we modify the semantics of ERBL to introduce reasoning strategies.
First we need to deﬁne strategies formally. A reasoning strategy for agent i, ≺i, is a
total order on the set Acti of all reasoning actions of i and their arguments:
Acti = {Readi,α,β, Resi,α1,α2,L,β, Copyi,α,β,
Erasei,β, Idlei, PIi,α,β, OPIi,Bi1 ...Bik−1 ,Bik α,β | α, β, α1, α2 ∈BΩ}
A simple example of a reasoning strategy for i would be a lexicographic order on Acti
which uses two total orders: an order on the set of transitions, e.g. Res < PI < OPI <
Copy < Read < Idle, and an order on BΩ.
Recall that in Deﬁnition 2 we speciﬁed which actions are enabled in state s, Acts,i ⊆
Acti. We required in Deﬁnition 3 that for each enabled action, there is indeed a transi-
tion by that action out of s. The simple change that we make to Deﬁnition 3 is that for
every agent i we only enable one action, namely the element of Acts,i which is minimal
in ≺i.
Deﬁnition 5. The set of reasoning strategy models M strat(KB1, . . . , KBnA, nB, nM,
nC) is the set of models (T, R, V ) which satisﬁes conditions 1-3 from Deﬁnition 3 and
the following condition:
4’. For all s ∈T , there exists a unique state t such that (s, t) ∈Rai for all i ∈A,
where ai is the minimal element with respect to ≺i in Acts,i.
Observe that in the reasoning strategy models, the transition relation is a linear order.
Finally, we give one possible deﬁnition of a correct ascription of a reasoning strategy
which allows an agent j to have a correct and complete representation of the beliefs
of another agent i, namely ensuring that Biα ↔BjBiα at each step. Such perfect
matching of i’s beliefs by j is possible if
KBj = {Biα : α ∈KBi}
and agent i does not use the Copy action (intuitively, because in order to match Copy
by i, agent j has to add two modalities in one step: when agent i derives Blα from α
being in agent l’s belief set, agent j has to derive BiBlα). Below, we also assume that
j is allowed one extra nesting of belief modalities (nB(j) = nB(i) + 1).

Reasoning about Other Agents’ Beliefs under Bounded Resources
13
Deﬁnition 6. Agent j has a strategy which matches the strategy of agent i if for every
natural number k, the following correspondence holds between the kth element of ≺j
and the kth element of ≺i:
– if the kth element of ≺i is Readi,α,β, then the kth element of ≺j is Readj,Biα,Biβ
– if the kth element of ≺i is Resi,α1,α2,L,β, then the kth element of ≺j is
Resj,Biα1,Biα2,L,Biβ
– if the kth element of ≺i is PIi,α,β, then the kth element of ≺j is OPIj,Biα,Biβ
– if the kth element of ≺i is OPIi,Blα,β, then then the kth element of ≺j is
OPIj,BiBlα,Biβ.
– if the kth element of ≺i is Erasei,β, then the kth element of ≺j is Erasej,Biβ
– if the kth element of ≺i is Idlei, then the kth element of ≺j is Idlej.
Theorem 2. If agent j’s strategy matches agent i’s strategy and agent j has complete
and correct beliefs about agent i’s beliefs in state s: M, s |= Biα ↔BjBiα, then agent
j will always have correct beliefs about agent i’s beliefs: M, s |= AG(Biα ↔BjBiα).
Other more realistic matching strategies, for example, those which allow the agent to
have a less than complete representation of other agent’s beliefs, are possible, and their
formal investigation is a subject of future work.
7
Conclusion
We presented a formal model of resource-bounded reasoners reasoning about each
other’s beliefs, and a sound and complete logic, ERBL, for reasoning about such sys-
tems. Our formalisation highlighted a problem of incorrect belief ascription, and we
showed that this problem can be overcome by extending the framework with reasoning
strategies. In future work we plan to extend the framework in a number of ways, in-
cluding producing correct belief ascription under less strict matching between agents’
strategies, and introducing reasoning about other agent’s resource limitations. At the
moment the agents have no way of forming beliefs about another agent’s memory limit
nM or belief nesting bound nB (note that we can also easily make those limits different
for different agents). If they could represent those limitations, then one agent could infer
that another agent does not believe some formula on the grounds that the latter agent’s
memory is bounded.
References
1. Fagin, R., Halpern, J.Y., Moses, Y., Vardi, M.Y.: Reasoning about Knowledge. MIT Press,
Cambridge (1995)
2. Meyer, J.J., van der Hoek, W.: Epistemic Logic for Computer Science and Artiﬁcial Intelli-
gence. Cambridge University Press, Cambridge (1995)
3. Hintikka, J.: Knowledge and belief. Cornell University Press, Ithaca (1962)
4. Rantala, V.: Impossible worlds semantics and logical omniscience. Acta Philosophica Fen-
nica 35, 106–115 (1982)

14
N. Alechina et al.
5. Fagin, R., Halpern, J.Y., Vardi, M.Y.: A non-standard approach to the logical omniscience
problem. In: Parikh, R. (ed.) Theoretical Aspects of Reasoning about Knowledge: Proceed-
ings of the Third Conference, pp. 41–55. Morgan Kaufmann, San Francisco (1990)
6. Fagin, R., Halpern, J.Y.: Belief, awareness and limited reasoning: Preliminary report. In:
Proceedings of the 9th International Joint Conference on Artiﬁcial Intelligence, pp. 491–501
(1985)
7. Konolige, K.: A Deduction Model of Belief. Morgan Kaufmann, San Francisco (1986)
8. Elgot-Drapkin, J.J., Perlis, D.: Reasoning situated in time I: Basic concepts. Journal of Ex-
perimental and Theoretical Artiﬁcial Intelligence 2, 75–98 (1990)
9. Halpern, J.Y., Moses, Y., Vardi, M.Y.: Algorithmic knowledge. In: Fagin, R. (ed.) Theoretical
Aspects of Reasoning about Knowledge: Proceedings of the Fifth Conference (TARK 1994),
pp. 255–266. Morgan Kaufmann, San Francisco (1994)
10. Pucella, R.: Deductive algorithmic knowledge. J. Log. Comput. 16(2), 287–309 (2006)
11. Duc, H.N.: Logical omniscience vs. logical ignorance on a dilemma of epistemic logic.
In: Pinto-Ferreira, C.A., Mamede, N.J. (eds.) EPIA 1995. LNCS, vol. 990, pp. 237–248.
Springer, Heidelberg (1995)
12. Duc, H.N.: Reasoning about rational, but not logically omniscient, agents. Journal of Logic
and Computation 7(5), 633–648 (1997)
13. ˚Agotnes, T., Alechina, N.: The dynamics of syntactic knowledge. Journal of Logic and Com-
putation 17(1), 83–116 (2007)
14. Sierra, C., Godo, L., de M´antaras, R.L., Manzano, M.: Descriptive dynamic logic and its
application to reﬂective architectu res. Future Gener. Comput. Syst. 12(2-3), 157–171 (1996)
15. Alechina, N., Jago, M., Logan, B.: Modal logics for communicating rule-based agents. In:
Brewka, G., Coradeschi, S., Perini, A., Traverso, P. (eds.) Proceedings of the 17th European
Conference on Artiﬁcial Intelligence (ECAI 2006), pp. 322–326. IOS Press, Amsterdam
(2006)
16. Fisher, M., Ghidini, C.: Programming resource-bounded deliberative agents. In: Proceedings
of the Sixteenth International Joint Conference on Artiﬁcial Intelligence (IJCAI 1999), pp.
200–205. Morgan-Kaufmann, San Francisco (1999)
17. ˚Agotnes, T.: A Logic of Finite Syntactic Epistemic States. Ph.D. thesis, Department of In-
formatics, University of Bergen, Norway (2004)
18. ˚Agotnes, T., Alechina, N.: Knowing minimum/maximum n formulae. In: Brewka, G.,
Coradeschi, S., Perini, A., Traverso, P. (eds.) Proceedings of the 17th European Conference
on Artiﬁcial Intelligence (ECAI 2006), pp. 317–321. IOS Press, Amsterdam (2006)
19. Albore, A., Alechina, N., Bertoli, P., Ghidini, C., Logan, B., Seraﬁni, L.: Model-checking
memory requirements of resource-bounded reasoners. In: Proceedings of the Twenty-First
National Conference on Artiﬁcial Intelligence (AAAI 2006), pp. 213–218. AAAI Press,
Menlo Park (2006)
20. Alechina, N., Logan, B., Nguyen, H.N., Rakib, A.: Verifying time, memory and communi-
cation bounds in systems of reasoning agents. In: Padgham, L., Parkes, D., M¨uller, J., Par-
sons, S. (eds.) Proceedings of the Seventh International Conference on Autonomous Agents
and Multiagent Systems (AAMAS 2008), Estoril, Portugal, May 2008, vol. 2, pp. 736–743.
IFAAMAS (2008)
21. ˚Agotnes, T., Walther, D.: Towards a logic of strategic ability under bounded memory. In:
Proceedings of the Workshop on Logics for Resource-Bounded Agents (2007)
22. Adjiman, P., Chatalic, P., Goasdou´e, F., Rousset, M.C., Simon, L.: Scalability study of peer-
to-peer consequence ﬁnding. In: Kaelbling, L.P., Safﬁotti, A. (eds.) Proceedings of the Nine-
teenth International Joint Conference on Artiﬁcial Intelligence (IJCAI 2005), Edinburgh,
Scotland, pp. 351–356. Professional Book Center (2005)
23. Emerson, E.A.: Temporal and modal logic. In: Handbook of Theoretical Computer Science.
Formal Models and Sematics (B), vol. B, pp. 995–1072. Elsevier and MIT Press (1990)

Reasoning about Other Agents’ Beliefs under Bounded Resources
15
24. Reynolds, M.: An axiomatization of full computation tree logic. Journal of Symbolic
Logic 66(3), 1011–1057 (2001)
25. Alchourr´on, C.E., G¨ardenfors, P., Makinson, D.: On the logic of theory change: Partial meet
functions for contraction and revision. Journal of Symbolic Logic 50, 510–530 (1985)
26. Baltag, A., Moss, L.S., Solecki, S.: The logic of public announcements, common knowl-
edge, and private suspicions. In: Proceedings of the 7th conference on Theoretical aspects of
rationality and knowledge, TARK 1998 (1998)

Normative Multi-agent Programs
and Their Logics
Mehdi Dastani1, Davide Grossi2, John-Jules Ch. Meyer1, and Nick Tinnemeier1
1Universiteit Utrecht
The Netherlands
2Computer Science and Communication
University of Luxembourg, Luxembourg
Abstract. Multi-agent systems are viewed as consisting of individual
agents whose behaviors are regulated by an organization artefact. This
paper presents a simpliﬁed version of a programming language that is
designed to implement norm-based artefacts. Such artefacts are speciﬁed
in terms of norms being enforced by monitoring, regimenting and sanc-
tioning mechanisms. The syntax and operational semantics of the pro-
gramming language are introduced and discussed. A logic is presented
that can be used to specify and verify properties of programs developed
in this language.
1
Introduction
In this paper, multi-agent systems are considered as consisting of individual
agents that are autonomous and heterogenous. Autonomy implies that each in-
dividual agent pursues its own objectives and heterogeneity implies that the
internal states and operations of individual agents may not be known to exter-
nal entities [19,7]. In order to achieve the overall objectives of such multi-agent
systems, the observable/external behavior of individual agents and their inter-
actions should be regulated/coordinated.
There are two main approaches to regulate the external behavior of individual
agents. The ﬁrst approach is based on coordination artefacts that are speciﬁed in
terms of low-level coordination concepts such as synchronization of processes[16].
The second approach is motivated by organizational models, normative systems,
and electronic institutions[17,13,7,10]. In such an approach, norm-based arte-
facts are used to regulate the behavior of individual agents in terms of norms
being enforced by monitoring, regimenting and sanctioning mechanisms. Gen-
erally speaking, the social and normative perspective is conceived as a way to
make the development and maintenance of multi-agent systems easier to man-
age. A plethora of social concepts (e.g., roles, social structures, organizations,
institutions, norms) has been introduced in multi-agent system methodologies
(e.g. Gaia [19]), models (e.g. OperA [6], Moise+ [11], electronic institutions and
frameworks (e.g. AMELI [7], J-Moise+ [12]).
The main contribution of this paper is twofold. On the one hand, a simpliﬁed
version of a programming language is presented that is designed to implement
J.-J.Ch. Meyer and J.M. Broersen (Eds.): KRAMAS 2008, LNAI 5605, pp. 16–31, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

Normative Multi-agent Programs and Their Logics
17
multi-agent systems in which the observable (external) behavior of individual
agents is regulated by means of norm-based artefacts. Such artefacts are imple-
mented in terms of social concepts such as norms and sanctions, monitor the
actions performed by individual agents, evaluate their eﬀects, and impose sanc-
tions if necessary. On the other hand, we devise a logic to specify and verify
properties of programs that implement norm-based artefacts.
In order to illustrate the idea of norm-based artefacts, consider the following
simple example of a simulated train station where agents ought to buy a ticket
before entering the platform or trains. To avoid the queue formation, agents are
not checked individually before allowing them to enter the platform or trains.
In this simulation, being on the platform without a ticket is considered as a
violation and getting on the train without having a ticket is considered as a
more severe violation. A norm-based artefact detects (all or some) violations by
(all or some) agents and reacts on them by issuing a ﬁne if the ﬁrst violation
occurs, for instance by charging the credit card of the defecting user, and a
higher ﬁne if the second violation occurs.
In this paper, we ﬁrst brieﬂy explain our idea of normative multi-agent sys-
tems and discuss two norm-based approaches to multi-agent systems, that is,
ISLANDER/AMELI [7] and J-MOISE+ [12]. In section 3, we present the syn-
tax and operational semantics of a programming language designed to imple-
ment normative multi-agent systems. This programming language allows the
implementation of norm-based artefacts by providing programming constructs
to represent norms and mechanisms to enforce them. In section 4, a logic is pre-
sented that can be used to specify and verify properties of norm-based artefacts
implemented in the presented programming language. Finally, in section 5, we
conclude the paper and discuss some future directions in this research area.
2
Norms and Multi-Agent Systems
Norms in multi-agent systems can be used to specify the standards of behavior
that agents ought to follow to meet the overall objectives of the system. How-
ever, to develop a multi-agent system does not boil down to state a number of
standards of behavior in the form of a set of norms, but rather to organize the
system in such a way that those standards of behavior are actually followed by
the agents. This can be achieved by regimentation [13] or enforcement mecha-
nisms, e.g., [10].
When regimenting norms all agents’ external actions leading to a violation
of those norms are made impossible. Via regimentation (e.g., gates in train sta-
tions) the system prevents an agent from performing a forbidden action (e.g.,
entering a train platform without a ticket). However, regimentation drastically
decreases agent autonomy. Instead, enforcement is based on the idea of respond-
ing after a violation of the norms has occurred. Such a response, which includes
sanctions, aims to return the system to an acceptable/optimal state. Crucial for
enforcement is that the actions that violate norms are observable by the system
(e.g., ﬁnes can be issued only if the system can detect travelers entering the

18
M. Dastani et al.
platform or trains without a ticket). Another advantage of having enforcement
over regimentation is that allowing for violations contributes to the ﬂexibility
and autonomy of the agent’s behavior [3]. These norms are often speciﬁed by
means of concepts like permissions, obligations, and prohibitions.
In the literature of multi-agent systems related work can be found on electronic
institutions. In particular, ISLANDER[8] is a formal framework for specifying
norms in institutions, which is used in the AMELI platform [7] for executing
electronic institutions based on norms provided in it. However, the key aspect
of ISLANDER/AMELI is that norms can never be violated by agents. In other
words, systems programmed via ISLANDER/AMELI make only use of regimen-
tation in order to guarantee the norms to be actually followed. Similar to some
extensions of electronic institutions (cf. [9,18]) we relax this assumption, guar-
anteeing higher autonomy to the agents, and higher ﬂexibility to the system.
In contrast to our work, however, the norms of [7,9,18] all relate to (dialogical)
actions the agents should or should not perform and ignore the issue of express-
ing more high-level norms concerning a state of the environment that should be
brought about. Such high-level norms can be used to represent what the agents
should establish – in terms of a declarative description of a state – rather than
specifying how they should establish it.
A work that is also concerned with programming multiagent systems using
(among others) normative concepts is J-MOISE+ [12], which is an organiza-
tional middleware that follows the Moise+ model[11]. This approach, like ours,
builds on programming constructs investigated in social and organizational sci-
ences. In contrast to [7,9,18], J-MOISE+ is concerned with more high-level norms
that are about declarative descriptions of a state that should be achieved. How-
ever, norms in S-MOISE+ typically lack monitoring and sanctioning mecha-
nisms for their implementation which are the focus of our proposal. It should
be noted that [14] advocates the use of artifacts to implement norm enforce-
ment mechanisms. However, it is not explained how such a mechanism is to be
realized.
To summarise, in the work on electronic institutions norms pertain to very low-
level procedures that directly refer to actions, whereas the work on J-MOISE+ is
concerned with more high-level norms pertaining to declarative descriptions of the
system, but no speciﬁc system’s response to violations is built in the framework.
Another important issue is that the above mentioned approaches lack a formal
operational semantics [15], which we deem very important for a thorough under-
standing and analysis of the proposed concepts. The present paper ﬁlls this gap
along the same lines that have been followed for the operationalization of BDI
notions in the APL-like agent programming languages [4,5]. Finally, it should be
noted that besides normative concepts MOISE+ and ISLANDER/AMELI also
provide a variety of other social and organizational concepts. Since the focus of
this paper is on the normative aspect, the above discussion is limited hereto. Fu-
ture research will focus on other social and organizational concepts.

Normative Multi-agent Programs and Their Logics
19
3
Programming Multi-Agent Systems with Norms
In this section, we present a programming language to facilitate the implemen-
tation of multi-agent systems with norms, i.e., to facilitate the implementation
of norm-based artefacts that coordinate/regulate the behavior of participating
individual agents. A normative multi-agent system (i.e., a norm-based artefact)
is considered to contain two modules: an organization module that speciﬁes
norms and sanctions, and an environment module in which individual agents
can perform actions. The individual agents are assumed to be implemented in
a programming language, not necessarily known to the multi-agent system pro-
grammer, though the programmer is required to have the reference to the (ex-
ecutable) programs of each individual agent. It is also assumed that all actions
that are performed by individual agents are observable to the multi-agent system
(i.e., norm-based artefact). Note that the reference to the (executable) programs
of individual agents are required such that multi-agent systems (i.e., normative
artefact) can observe the actions generated by the agent programs. Finally, we
assume that the eﬀect of an individual agent’s action in the external environment
is determined by the program that implements the norm-based artefact (i.e., by
the multi-agent system program). Most noticeably it is not assumed that the
agents are able to reason about the norms of the system.
The programming language for normative multi-agent systems provides pro-
gramming constructs to specify the eﬀect of an agent’s actions in the environ-
ment, norms, sanctions, and the initial state of the environment. Moreover, the
programming language is based on a monitoring and a sanctioning mechanism
that observes the actions performed by the agents, determines their eﬀects in the
shared environment, determines the violations caused by performing the actions,
and possibly, imposes sanctions. A program in this language is the implemen-
tation of a norm-based artefact. As we assume that the norm-based artefacts
determine the eﬀects of external actions in the shared environment, the pro-
gramming language should provide constructs to implement these eﬀects. The
eﬀect of an agent’s (external) actions is speciﬁed by a set of literals that should
hold in the shared environment after the external action is performed by the
agent. As external actions can have diﬀerent eﬀects when they are executed in
diﬀerent states of the shared environment, we add a set of literals that function
as the pre-condition of those eﬀect.
We consider norms as being represented by counts-as rules [17], which ascribe
“institutional facts” (e.g. “a violation has occurred”), to “brute facts” (e.g. “an
agent is on the train without ticket”). For example, a counts-as rule may express
the norm ”an agent on the train without ticket counts-as a violation”. In our frame-
work, brute facts constitute the environment shared by the agents, while insti-
tutional facts constitute the normative/institutional state of the multi-agent sys-
tem. Institutional facts are used with the explicit aim of triggering system’s reac-
tions (e.g., sanctions). As showed in [10] counts-as rules can enjoy a rather classical
logical behavior, and are here implemented as simple rules that relate brute and

20
M. Dastani et al.
normative facts. In the presented programming language, we distinguish brute
facts from normative (institutional) facts and assume two disjoint sets of propo-
sitions to denote these facts.
Brute and institutional facts constitute the (initial) state of the multi-agent
system (i.e., the state of the norm-based artefact). Brute facts are initially set by
the programmer by means of the initial state of the shared environment. These
facts can change as individual agents perform actions in the shared environment.
Normative facts are determined by applying counts-as rules in multi-agent states.
The application of counts-as rules in subsequent states of a multi-agent system
realizes a monitoring mechanism as it determines and detects norm violations
during the execution of the multi-agent system.
Sanctions are also implemented as rules, but follow the opposite direction of
counts-as rules. A sanction rule determines which brute facts will be brought
about by the system as a consequence of the normative facts. Typically, such
brute facts are sanctions, such as ﬁnes. Notice that in human systems sanctions
are usually issued by speciﬁc agents (e.g. police agents). This is not the case
in our computational setting, where sanctions necessarily follow the occurrence
of a violation if the relevant sanction rule is in place (comparable to automatic
traﬃc control and issuing tickets). It is important to stress, however, that this is
not an intrinsic limitation of our approach. We do not aim at mimicking human
institutions but rather providing the speciﬁcation of computational systems.
3.1
Syntax
In order to represent brute and institutional facts in our normative multi-agent
systems programming language, we introduce two disjoint sets of propositions to
denote these facts. The syntax of the normative multi-agent system programming
language is presented in Figure 1 using the EBNF notation. In the following,
we use <b-prop> and <i-prop> to be propositional formulae taken from two
diﬀerent disjoint sets of propositions. Moreover, we use <ident> to denote a
string and <int> to denote an integer.
In order to illustrate the use of this programming language, consider the fol-
lowing underground station example.
Agents:
passenger
PassProg
1
Facts:
{-at platform, -in train, -ticket}
Effects:
{-at platform} enter {at platform},
{-ticket} buy ticket {ticket},
{at platform, -in train} embark
{-at platform, in train}
Counts as rules: {at platform , -ticket} ⇒{viol1},
{in train , -ticket} ⇒{viol⊥}
Sanction rules: {viol1} ⇒{fined10}
This program creates one agent called passenger whose (executable) speciﬁca-
tion is included in a ﬁle with the name PassProg. The Facts, which implement

Normative Multi-agent Programs and Their Logics
21
N-MAS Prog
:= "Agents: " (<agentName> <agentProg> [<nr>])+ ;
"Facts: " <bruteFacts>
"Effects: " <effects>
"Counts-as rules: " <counts-as>
"Sanction rules: " <sanctions>;
<agentName>
:= <ident>;
<agentProg>
:= <ident>;
<nr>
:= <int>;
<bruteFacts> := <b-literals>;
<effects>
:= ({<b-literals>} <actionName> {<b-literals>})+;
<counts-as>
:= ( <literals> ⇒<i-literals> )+;
<sanctions>
:= ( <i-literals> ⇒<b-literals>)+;
<actionName> := <ident>;
<b-literals> := <b-literal> {"," <b-literal>};
<i-literals> := <i-literal> {"," <i-literal>};
<literals>
:= <literal> {"," <literal>};
<literal>
:= <b-literal> | <i-literal>;
<b-literal>
:= <b-prop> | "not" <b-prop>;
<i-literal>
:= <i-prop> | "not" <i-prop>;
Fig. 1. The EBNF syntax of the normative multi-agent programming language
brute facts, determine the initial state of the shared environment. In this case,
the agent is not at the platform (-at platform) nor in the train (-in train)
and has no ticket (-ticket). The Effects indicate how the environment can
advance in its computation. Each eﬀect is of the form {pre-condition} action
{post-condition}. The ﬁrst eﬀect, for instance, means that if the agent per-
forms an enter action when not at the platform, the result is that the agent
is on the platform (either with or without a ticket). Only those eﬀects that are
changed are thus listed in the post-condition. The Counts as rules determine
the normative eﬀects for a given (brute and normative) state of the multi-agent
system. The ﬁrst rule, for example, states that being on the platform with-
out having a ticket is a speciﬁc violation (marked by viol1). The second rule
marks states where agents are on a train without a ticket with the speciﬁcally
designated literal viol⊥. This literal is used to implement regimentation. The op-
erational semantics of the language ensures that the designated literal viol⊥can
never hold during any run of the system (see Deﬁnition 3). Intuitively, rules with
viol⊥as consequence could be thought of as placing gates blocking an agent’s
action. Finally, the aim of Sanction rules is to determine the punishments
that are imposed as a consequence of violations. In the example the violation of
type viol1 causes the sanction ﬁned10 (e.g., a 10 EUR ﬁne).
Counts-as rules obey syntactic constraints. Let l = (Φ ⇒Ψ) be a rule, we
use condl and consl to indicate the condition Φ and consequent Ψ of the rule l,
respectively. We consider only sets of rules such that 1) they are ﬁnite; 2) they
are such that each condition has exactly one associated consequence (i.e., all the
consequences of a given conditions are packed in one single set cons); and 3)
they are such that for counts-as rule k, l, if consk ∪consl is inconsistent (i.e.,

22
M. Dastani et al.
contains p and −p), then condk ∪condl is also inconsistent. That is to say, rules
trigger inconsistent conclusions only in diﬀerent states. In the rest of this paper,
sets of rules enjoying these three properties are denoted by R.
Note that in the current approach a multi-agent system consists of only one
normative artifact (environment) the agents interact with, which suggests a cen-
tralized approach. However, extending the notion of a multi-agent system to
contain a multitude of normative artifacts the agents interact with does not af-
fect the norm enforcement mechanism of the individual artifacts. Because this
paper concentrates on the norm enforcement mechanism of a normative artifact
we decided to adopt the assumption of one environment in order not to compli-
cate matters further. Our framework can be extended with a set of normative
artifacts, each of which is responsible for monitoring a speciﬁc set of actions and
sanctioning the corresponding agents.
3.2
Operational Semantics
One way to deﬁne the semantics of this programming language is by means of
operational semantics. Using such semantics, one needs to deﬁne the conﬁgu-
ration (i.e., state) of normative multi-agent systems and the transitions that
such conﬁgurations can undergo through transition rules. The state of a multi-
agent system with norms consists of the state of the external environment, the
normative state, and the states of individual agents.
Deﬁnition 1. (Normative Multi-Agent System Conﬁguration) Let Pb and Pn be
two disjoint sets of literals denoting atomic brute and normative facts (includ-
ing viol⊥), respectively. Let Ai be the conﬁguration of individual agent i. The
conﬁguration of a normative multi-agent system is deﬁned as ⟨A, σb, σn⟩where
A = {A1, . . . , An}, σb is a consistent set of literals from Pb denoting the brute
state of multi-agent system and σn is a consistent set of literals from Pn denoting
the normative state of multi-agent system.
The conﬁguration of such a multi-agent system can change for various reasons,
e.g., because individual agents perform actions in the external environment or
because the external environment can have its own internal dynamics (the state
of a clock changes independent of an individual agent’s action). In operational
semantics, transition rules specify how and when conﬁgurations can change, i.e.,
they specify which transition between conﬁgurations are allowed and when they
can be derived. In this paper, we consider only the transition rules that spec-
ify the transition of multi-agent system conﬁgurations as a result of performing
external actions by individual agents. Of course, individual agents can perform
(internal) actions that modify only their own conﬁgurations and have no inﬂu-
ence on the multi-agent system conﬁguration. The transition rules to derive such
transitions are out of the scope of this paper.
Deﬁnition 2. (Transitions of Individual Agent’s Actions) Let Ai and A′
i be
conﬁgurations of individual agent i, and α(i) be an (observable) external action
performed by agent i. Then, the following transition captures the execution of an
external action by an agent.

Normative Multi-agent Programs and Their Logics
23
Ai
α(i)
−→A′
i : agent i can perform external action α
This transition indicates that an agent conﬁguration can change by performing
an external action. The performance of the external action is broadcasted to the
multi-agent system level. Note that no assumption is made about the internals of
individual agents as we do not present transition rules for deriving internal agent
transitions (denoted as A −→A′). The only assumption is that the action of
the agent is observable. This is done by labeling the transition with the external
action name.
Before presenting the transition rule specifying the possible transitions of the
normative MAS conﬁgurations, the closure of a set of conditions under a set of
(counts-as and sanction) rules needs to be deﬁned. Given a set R of rules and
a set X of literals, we deﬁne the set of applicable rules in X as ApplR(X) =
{Φ ⇒Ψ | X |= Φ}. The closure of X under R, denoted as ClR(X), is inductively
deﬁned as follows:
B: ClR
0 (X) = X ∪(
l∈ApplR(X) consl)
S: ClR
n+1(X) = ClR
n (X) ∪(
l∈ApplR(ClR
n (X)) consl)
Because of the properties of ﬁniteness, consequence uniqueness and consistency
of R one and only one ﬁnite number m + 1 can always be found such that
ClR
m+1(X) = ClR
m(X) and ClR
m(X) ̸= ClR
m−1(X). Let such m + 1 deﬁne the
closure X under R: ClR(X) = ClR
m+1(X). Note that the closure may become
inconsistent due to the ill-deﬁned set of counts-as rules. For example, the counts-
as rule p ⇒−p (or the set of counts as rules {p ⇒q , q ⇒−p}), where p and q
are normative facts, may cause the normative state of a multi-agent system to
become inconsistent.
We can now deﬁne a transition rule to derive transitions between normative
multi-agent system conﬁgurations. In this transition rule, the function up deter-
mines the eﬀect of action α(i) on the environment σb based on its speciﬁcation
(Φ α(i) Φ′) as follows:
up(α(i), σb) = (σb ∪Φ′) \ ({p | −p ∈Φ′} ∪{−p | p ∈Φ′})
Deﬁnition 3. (Transition Rule for Normative Multi-Agent Systems) Let Rc be
the set of counts-as rules, Rs be the set of sanction rules, and (Φ α(i) Φ′) be the
speciﬁcation of action α(i). The multi-agent transition rule for the derivation of
normative multi-agent system transitions is deﬁned as follows:
Ai ∈A
&
Ai
α(i)
→A′
i
&
σb |= Φ
&
σ′
b = up(α(i), σb)
σ′
n = ClRc(σ′
b) \ σ′
b & σ′
n ̸|= viol⊥&
S = ClRs(σ′
n) \ σ′
n & σ′
b ∪S ̸|= ⊥
⟨A, σb, σn⟩−→⟨A′, σ′
b ∪S, σ′
n⟩
where A′ = (A\{Ai})∪{A′
i} and viol⊥is the designated literal for regimentation.
This transition rule captures the eﬀects of performing an external action by an
individual agent on both external environments and the normative state of the

24
M. Dastani et al.
MAS. First, the eﬀect of α on σb is computed. Then, the updated environment
is used to determine the new normative state of the system by applying all
counts-as rules to the new state of the external environments. Finally, possible
sanctions are added to the new environment state by applying sanction rules
to the new normative state of the system. In should be emphasized that other
multi-agent transition rules, such as transition rules for communication actions,
are not presented in this paper because the focus here is on how norms determine
the eﬀects of external actions.
Note that the external action of an agent can be executed only if it would
not result in a state containing viol⊥. This captures exactly the regimentation
of norms. Hence, once assumed that the initial normative state does not include
viol⊥, it is easy to see that the system will never be in a viol⊥-state. It is
important to note that when a normative state σ′
n becomes inconsistent, the
proposed transition rule cannot be applied because an inconsistent σ′
n entails
viol ⊥. Also, note that the condition σ′
b ∪S ̸|= ⊥guarantees that the environment
state never can become inconsistent. Finally, it should be emphasized that the
normative state σ′
b is not deﬁned on σn and is always computed anew.
4
Logic
In this section, we propose a logic to specify and verify liveness and safety prop-
erties of multi-agent system programs with norms. This logic, which is a variant
of Propositional Dynamic Logic (PDL, see [2]), is in the spirit of [1] and rely on
that work. It is important to note that the logic developed in [1] aims at spec-
ifying and verifying properties of single agents programmed in terms of beliefs,
goals, and plans. Here we modify the logic and apply it to multi-agent system
programs. We ﬁrst introduce some preliminaries before presenting the logic.
4.1
Preliminaries
We show how the programming constructs can be used for grounding a logical
semantics. Let P denote the set of propositional variables used to describe brute
and normative states of the system. It is assumed that each propositional vari-
able in P denotes either an institutional/normative or a brute state-of-aﬀairs:
P = Pn ∪Pb and Pn ∩Pb = ∅. A state s is represented as a pair ⟨σb, σn⟩where
σb = {(−)p1, . . . , (−)pn : pi ∈Pb} is a consistent set of literals (i.e., for no p ∈Pb
it is the case that p ∈σb and −p ∈σb), and σn is like σb for Pn.
Rules are pairs of conditions and consequences ({(−)p1, . . . , (−)pn | (−)pi ∈
X}, {(−)q1, . . . , (−)qk | (−)qi ∈Y }) with X and Y being either σb or σn when
applied in state ⟨σb, σn⟩. Following [10], if X = σb and Y = σn then the rule
is called bridge counts-as rule; if X = Y = σn then the rule is an institutional
counts-as rule; if X = σn and Y = σb then the rule is a sanction rule. Literals
p’s and q’s are taken to be disjoint. Leaving technicalities aside, bridge counts-as
rules connect brute states to normative/institutional ones, institutional counts-as
rules connect institutional facts to institutional facts, and sanction rules connect
normative states to brute ones.

Normative Multi-agent Programs and Their Logics
25
Given a set R of rules, we say a state s = ⟨σb, σn⟩to be R-aligned if for all
pairs (condk, consk) in R: if condk is satisﬁed by σb ∪σn (σb in the case of a
bridge counts-as rule and σn in the case of an institutional counts-as or a sanction
rule), then consk is satisﬁed by σn (in the case of a bridge or institutional counts-
as rule) or by σb (in the case of a sanction rule), respectively. States that are
R-aligned are states which instantiate the normative system speciﬁed by R.
Let the set of agents’ external actions Ac be the union 
i∈I Aci of the ﬁnite sets
Aci of external actions of each agent i in the set I. We denote external actions as
α(i) where α ∈Aci and i ∈I. We associate now with each α(i) ∈Aci a set of pre-
and post-conditions {(−)p1 ∈σb, . . . , (−)pn ∈σb}, {(−)q1 ∈σ′
b, . . . , (−)qk ∈σ′
b}
(where p’s and q’s are not necessarily disjoint) when α(i) is executed in a state
with brute facts set σb which satisﬁes the pre-condition then the resulting state s′
has the brute facts set σ′
b which satisﬁes the post-condition (including replacing
p with −p if necessary to preserve consistency) and it is such that the rest of σ′
b
is the same as σb. Executing an action α(i) in diﬀerent conﬁgurations may give
diﬀerent results. For each α(i), we denote the set of pre- and post-condition pairs
{(prec1, post1), . . . , (precm, postm)} by Cb(α(i)). We assume that Cb(α(i)) is
ﬁnite, that pre-conditions preck, precl are mutually exclusive if k ̸= l, and that
each pre-condition has exactly one associated post-condition. We denote the set
of all such pre- and post-conditions of all agents’ external actions by C.
Now everything is put into place to show how the execution of α(i) in a
state with brute facts set σb also univocally changes the normative facts set
σn by means of the applicable counts-as rules, and adds the resulting sanctions
by means of the applicable sanction rules. If α(i) is executed in a state ⟨σb, σn⟩
with brute facts set σb, which satisﬁes the pre-conditions, then the resulting state
⟨σ′
b ∪S, σ′
n⟩is such that σ′
b satisﬁes the brute post-condition of α(i) (including
replacing p with −p if necessary) and the rest of σ′
b is the same of σb; σ′
n is
determined by the closure of σ′
b with counts-as rules Rc; sanctions S are obtained
via closure of σ′
n with sanction rules Rs.
4.2
Language
The language L for talking about normative multi-agent system programs is just
the language of PDL built out of a ﬁnite set of propositional variables P ∪−P
(i.e., the literals built from P), used to describe the system’s normative and
brute states, and a ﬁnite set Ac of agents’ actions. Program expressions ρ are
built out of external actions α(i) as usual, and formulae φ of L are closed under
boolean connectives and modal operators:
ρ ::= α(i) | ρ1 ∪ρ2 | ρ1; ρ2 | ?φ | ρ∗
φ ::= (−)p | ¬φ | φ1 ∧φ2 | ⟨ρ⟩φ
with α(i) ∈Ac and (−)p ∈P ∪−P. Connectives ∨and →, and the modal operator
[ρ] are deﬁned as usual.

26
M. Dastani et al.
4.3
Semantics
The language introduced above is interpreted on transition systems that gener-
alize the operational semantics presented in the earlier section, in that they do
not describe a particular program, but all possible programs —according to C—
generating transitions between all the Rc and Rs-aligned states of the system.
As a consequence, the class of transition systems we are about to deﬁne will
need to be parameterized by the sets C, Rc and Rs.
A model is a structure M = ⟨S, {Rα(i)}α(i)∈Ac, V ⟩where:
– S is a set of Rc and Rs-aligned states.
– V = (Vb, Vn) is the evaluation function consisting of brute and normative
valuation functions Vb and Vn such that for s = ⟨σb, σn⟩, Vb(s) = σb and
Vn(s) = σn.
– Rα(i), for each α(i) ∈Ac, is a relation on S such that (s, s′) ∈Rα(i) iﬀfor
some (preck, postk) ∈C(α(i)), preck(s) and postk(s′), i.e., for some pair
of pre- and post-conditions of α(i), the pre-condition holds for s and the
corresponding post-condition holds for s′. Note that this implies two things.
First, an α(i) transition can only originate in a state s which satisﬁes one of
the pre-conditions for α(i). Second, since pre-conditions are mutually exclu-
sive, every such s satisﬁes exactly one pre-condition, and all α(i)-successors
of s satisfy the matching post-condition.
Given the relations corresponding to agents’ external actions in M, we can deﬁne
sets of paths in the model corresponding to any PDL program expression ρ in
M. A set of paths τ(ρ) ⊆(S × S)∗is deﬁned inductively:
– τ(α(i)) = {(s, s′) : Rα(i)(s, s′)}
– τ(φ?) = {(s, s) : M, s |= φ}
– τ(ρ1 ∪ρ2) = {z : z ∈τ(ρ1) ∪τ(ρ2)}
– τ(ρ1; ρ2) = {z1 ◦z2 : z1 ∈τ(ρ1), z2 ∈τ(ρ2)}, where ◦is concatenation of
paths , such that z1 ◦z2 is only deﬁned if z1 ends in the state where z2 starts
– τ(ρ∗) is the set of all paths consisting of zero or ﬁnitely many concatenations
of paths in τ(ρ) (same condition on concatenation as above)
Constructs such as If φ then ρ1 else ρ2 and while φ do ρ are deﬁned as
(φ?; ρ1) ∪(¬φ?; ρ2) and (φ?; ρ)∗; ¬φ, respectively. The satisfaction relation |= is
inductively deﬁned as follows:
– M, s |= (−)p iﬀ(−)p ∈Vb(s) for p ∈Pb
– M, s |= (−)p iﬀ(−)p ∈Vn(s) for p ∈Pn
– M, s |= ¬φ iﬀM, s ̸|= φ
– M, s |= φ ∧ψ iﬀM, s |= φ and M, s |= ψ
– M, s |= ⟨ρ⟩φ iﬀthere is a path in τ(ρ) starting in s which ends in a state s′
such that M, s′ |= φ.
– M, s |= [ρ]φ iﬀfor all paths τ(ρ) starting in s, the end state s′ of the path
satisﬁes M, s′ |= φ.
Let the class of transition systems deﬁned above be denoted MC,Rc,Rs where
C is the set of pre- and post-conditions of external actions, Rc is the set of
counts-as rules and Rs the set of sanction rules.

Normative Multi-agent Programs and Their Logics
27
4.4
Axiomatics
The axiomatics shows in what the logic presented diﬀers w.r.t. standard PDL. In
fact, it is a conservative extension of PDL with domain-speciﬁc axioms needed
to axiomatize the behavior of normative multi-agent system programs.
For every pre- and post-condition pair (preci, posti) we describe states sat-
isfying preci and states satisfying posti by formulas of L. More formally, we
deﬁne a formula tr(X) corresponding to a pre- or post-condition X as follows:
tr((−)p) = (−)p and tr({φ1, . . . , φn}) = tr(φ1) ∧. . . ∧tr(φn). This allows us
to axiomatize pre- and post-conditions of actions. The conditions and conse-
quences of counts-as rules and sanction rules can be deﬁned in similar way as
pre- and post-conditions of actions, respectively. The set of models MC,Rc,Rs is
axiomatized as follows:
PDL. Axioms and rules of PDL
Ax Consistency. Consistency of literals: ¬(p ∧−p)
Ax Counts-as. For every rule (condk, consk) in Rc: tr(condk) →tr(consk)
Ax Sanction. For every rule (violk, sanck) in Rs: tr(violk) →tr(sanck)
Ax Regiment. viol⊥→⊥
Ax Frame. For every action α(i) and every pair of pre- and post-conditions
(precj, postj) in C(α(i)) and formula Φ built out of Pb not containing any
propositional variables occurring in postj:
tr(precj) ∧Φ →[α(i)](tr(postj) ∧Φ)
This is a frame axiom for actions.
Ax Non-Executability. For every action α(i), where all possible pre-conditions
in C(α(i)) are prec1, . . . , preck: ¬tr(prec1) ∧. . . ∧¬tr(preck) →¬⟨α(i)⟩⊤
where ⊤is a tautology.
Ax Executability. For every action α(i) and every pre-condition precj in
C(α(i)): tr(precj) →⟨α(i)⟩⊤
Let us call the axiom system above AxC,Rc,Rs, where C is the set of brute pre-
and post-conditions of atomic actions, Rc is the set of counts-as rules, and Rs
is the set of sanction rules.
Theorem 1. Axiomatics AxC,Rc,Rs is sound and weakly complete for the class
of models MC,Rc,Rs.
Proof. Soundness is proven as usual by induction on the length of derivations.
We sketch the proof of completeness. It builds on the usual completeness proof
of PDL via ﬁnite canonical models. Given a consistent formula φ to be proven
satisﬁable, such models are obtained via the Fischer-Ladner closure of the set
of subformulae of the formula φ extended with all pre- and post-conditions of
any action α(i) occurring in φ. Let FLC(φ) denote such closure. The canonical
model consists of all the maximal AxC,Rc,Rs-consistent subsets of FLC(φ). The
accessibility relation and the valuation of the canonical model are deﬁned like in
PDL and the truth lemma follows in the standard way. It remains to be proven
that the model satisﬁes the axioms. First, since the states in the model are

28
M. Dastani et al.
maximal and consistent w.r.t. Ax Counts-as, Ax Sanction, Ax Consistency, and
AxRegiment, they are Rc- and Rs-aligned, σb and σn are consistent, and no state
is such that σn |= viol⊥. Second, it should be shown that the canonical model
satisﬁes the pre- and post-conditions of the actions occurring in φ in that: a) no
action α(i) is executable in a state s if none of its preconditions are satisﬁed by s,
and b) if they hold in s then the corresponding post-conditions hold in s′ which
is accessible byRα(i) from s. As to a), if a state s in the canonical model does
not satisfy any of the preconditions of α(i) then, by Ax Non-Executability and
the deﬁnition of the canonical accessibility relation, there is no s′ in the model
such that sRα(i)s′. As to b), if a state s in the canonical model satisﬁes one of
the preconditions precj of α(i) thentr(precj) belongs to s and, by Ax Frame,
[α(i)]tr(postj) also do. Now, Ax Executability guarantees that there exists at
least one s′ such that sRα(i)s′, and, for any s′ such that sRα(i)s′, by the deﬁnition
of such canonical accessibility relation, s′ contains tr(postj) (otherwise it would
not be the case that sRα(i)s′). On the other hand, for any literal (−)p in s not
occurring intr(postj), its value cannot change from s to s′ since, if it would,
then for Ax Frame it would not be the case that sRα(i)s′, which is impossible.
This concludes the proof.
4.5
Veriﬁcation
To verify a normative multi-agent system program means, in our perspective,
to check whether the program implementing the normative artefact is soundly
designed w.r.t. the regimentation and sanctioning mechanisms it is supposed to
realize or, to put it in more general terms, to check whether certain property
holds in all (or some) states reachable by the execution traces of the multi-agent
system program. In order to do this, we need to translate a multi-agent system
program into a PDL program expression.
As explained in earlier sections, a multi-agent system program assumes a
set of behaviors A1, . . . , An of agents 1, . . . , n, each of which is a sequence of
external actions (the agents actions observed from the multi-agent level), i.e.,
Ai = α1
i ; α2
i , . . . where αj
i ∈Ac. 1 Moreover, a multi-agent system program with
norms consists of an initial set of brute facts, a set of counts-as rules and a set
of sanction rules which together determine the initial state of the program. In
this paper, we consider the execution of a multi-agent program as interleaved
executions of the involved agents’ behaviors started at the initial state.
Given I as the set of agents’ names and Ai as the behavior of agent i ∈I,
the execution of a multi-agent program can be described as PDL expression
 interleaved({Ai|i ∈I}), where interleaved({Ai|i ∈I}) yields all possible
interleavings of agents’ behaviors, i.e., all possible interleavings of actions from
sequences Ai. It is important to notice that  interleaved({Ai|i ∈I}) corre-
sponds to the set of computations sequences (execution traces) generated by the
operational semantics.
1 Note an agent’s behavior can always be written as a (set of) sequence(s) of actions,
which in turn can be written as a PDL expressions.

Normative Multi-agent Programs and Their Logics
29
The general veriﬁcation problem can now be formulated as follows. Given a
multi-agent system program with norms in a given initial state satisfying φ ∈L,
the state reached after the execution of the program satisﬁes ψ, i.e.:
φ →⟨[

interleaved({Ai|i ∈I})]⟩ψ
In the above formulation, the modality ⟨[. . .]⟩is used to present both safety [. . .]
and liveness ⟨. . .⟩properties. We brieﬂy sketch a sample of such properties using
again the multi-agent system program with norms which implements the train
station example with one passenger agent (see Section 3).
Sanction follows violation. Entering without a ticket results in a ﬁne, i.e.,
−at platform ∧−train ∧−ticket →[enter](viol1 ∧pay10).
Norm obedience avoids sanction. Buying a ticket if you have none and
entering the platform does not result in a ﬁne, i.e.:
−at platform∧−train →⟨If−ticket then buy ticket; enter ⟩(at platform∧−pay10).
Regimentation. It is not possible for an agent to enter the platform and em-
bark the train without a ticket, i.e.:
−at platform ∧−train ∧−ticket →[enter; embark]⊥
Note that there is only one passenger agent involved in the example program.
For this property, we assume that the passenger’s behavior is enter; embark.
Note also that:

interleaved({enter; embark}) = enter; embark.
Below is the proof of the regimentation property above with respect to the multi-
agent system program with norms that implements the train station with one
passenger.
Proof. First, axiom Ax Frame using the speciﬁcation of the enter action (with
pre-condition {-at platform} and post-condition {at platform}) gives us
(1) −at platform ∧−in train ∧−ticket →
[enter] at platform ∧−in train ∧−ticket
Moreover, axiom Ax Frame using the speciﬁcation of the embark action (with
pre-condition {at platform, -in train} and post-condition {-at platform,
in train}) gives us
(2) at platform ∧−in train ∧−ticket →
[embark] −at platform ∧in train ∧−ticket
Also, axiom Ax Counts-as and the speciﬁcation of the second counts-as rule of
the program give us
(3) in train ∧−ticket →viol⊥
And axiom Ax Regiment together with formula (3) gives us

30
M. Dastani et al.
(4) in train ∧−ticket →⊥
Now, using PDL axioms together with formula (1), (2), and (4) we get ﬁrst
(5) −at platform ∧−in train ∧−ticket →[enter][embark] ⊥
and thus
(6) −at platform∧−in train∧−ticket →[enter; embark] ⊥. This completes
the derivation.
5
Conclusions and Future Work
The paper has proposed a programming language for implementing multi-agent
systems with norms. The programming language has been endowed with formal
operational semantics, therefore formally grounding the use of certain social
notions —eminently the notion of norm, regimentation and enforcement— as
explicit programming constructs. A sound and complete logic has then been
proposed which can be used for verifying properties of the multi-agent systems
with norms implemented in the proposed programming language.
We have already implemented an interpreter for the programming language
that facilitates the implementation of multi-agent systems without norms (see
http://www.cs.uu.nl/2apl/). Currently, we are working to build an inter-
preter for the modiﬁed programming language. This interpreter can be used
to execute programs that implement multi-agent systems with norms. Also, we
are working on using the presented logic to devise a semi-automatic proof checker
for veriﬁcation properties of normative multi-agent programs.
We are aware that for a comprehensive treatment of normative multi-agent
systems we need to extend our framework in many diﬀerent ways. Future work
aims at extending the programming language with constructs to support the im-
plementation of a broader set of social concepts and structures (e.g., roles, power
structure, task delegation, and information ﬂow), and more complex forms of en-
forcement (e.g., policing agents) and norm types (e.g., norms with deadlines).
Another extension of the work is the incorporation of the norm-awareness of
agents in the design of the multi-agent system. We also aim at extending the
framework to capture the role of norms and sanctions concerning the interaction
between individual agents.
The approach in its present form concerns only closed multi-agent systems.
Future work will also aim at relaxing this assumption providing similar formal
semantics for open multi-agent systems. Finally, we have focused on the so-
called ’ought-to-be’ norms which pertain to socially preferable states. We intend
to extend our programming framework with ’ought-to-do’ norms pertaining to
socially preferable actions.
References
1. Alechina, N., Dastani, M., Logan, B., Meyer, J.-J.Ch.: A logic of agent programs.
In: Proc. AAAI 2007 (2007)
2. Blackburn, P., de Rijke, M., Venema, Y.: Modal Logic. Cambridge University Press,
Cambridge (2001)

Normative Multi-agent Programs and Their Logics
31
3. Castelfranchi, C.: Formalizing the informal?: Dynamic social order, bottom-up so-
cial control, and spontaneous normative relations. JAL 1(1-2), 47–92 (2004)
4. Dastani, M.: 2apl: a practical agent programming language. International Journal
of Autonomous Agents and Multi-Agent Systems 16(3), 214–248 (2008)
5. Dastani, M., Meyer, J.-J.C.: A practical agent programming language. In: Dastani,
M.M., El Fallah Seghrouchni, A., Ricci, A., Winikoﬀ, M. (eds.) ProMAS 2007.
LNCS (LNAI), vol. 4908, pp. 107–123. Springer, Heidelberg (2008)
6. Dignum, V.: A Model for Organizational Interaction. PhD thesis, Utrecht Univer-
sity, SIKS (2003)
7. Esteva, M., Rodr´ıguez-Aguilar, J.A., Rosell, B., Arcos, J.L.: Ameli: An agent-based
middleware for electronic institutions. In: Proc. of AAMAS 2004, New York, US
(July 2004)
8. Esteva, M., Rodr´ıguez-Aguilar, J.A., Sierra, C., Garcia, P., Arcos, J.L.: On the
formal speciﬁcation of electronic institutions. In: Sierra, C., Dignum, F.P.M. (eds.)
AgentLink 2000. LNCS (LNAI), vol. 1991, pp. 126–147. Springer, Heidelberg (2001)
9. Garcia-Camino, A., Noriega, P., Rodriguez-Aguilar, J.A.: Implementing norms in
electronic institutions. In: Proc. of AAMAS 2005, pp. 667–673. ACM, New York
(2005)
10. Grossi, D.: Designing Invisible Handcuﬀs. PhD thesis, Utrecht University, SIKS
(2007)
11. H¨ubner, J.F., Sichman, J.S., Boissier, O.: Moise+: Towards a structural functional
and deontic model for mas organization. In: Proc. of AAMAS 2002. ACM, New
York (2002)
12. Hubner, J.F., Sichman, J.S., Boissier, O.: Developing organised multiagent systems
using the moise+ model: programming issues at the system and agent levels. Int.
J. Agent-Oriented Softw. Eng. 1(3/4), 370–395 (2007)
13. Jones, A.J.I., Sergot, M.: On the characterization of law and computer systems.
In: Deontic Logic in Computer Science (1993)
14. Kitio, R., Boissier, O., H¨ubner, J.F., Ricci, A.: Organisational artifacts and agents
for open multi-agent organisations: “giving the power back to the agents”. In: Sich-
man, J.S., Padget, J., Ossowski, S., Noriega, P. (eds.) COIN 2007. LNCS (LNAI),
vol. 4870, pp. 171–186. Springer, Heidelberg (2008)
15. Plotkin, G.D.: A structural approach to operational semantics. Technical Report
DAIMI FN-19, University of Aarhus (1981)
16. Ricci, A., Viroli, M., Omicini, A.: “Give agents their artifacts”: The A&A ap-
proach for engineering working environments in MAS. In: Proc. of AAMAS 2007,
Honolulu, Hawai’i, USA (2007)
17. Searle, J.: The Construction of Social Reality. Free (1995)
18. Silva, V.T.: From the speciﬁcation to the implementation of norms: an auto-
matic approach to generate rules from norms to govern the behavior of agents.
JAAMAS 17(1), 113–155 (2008)
19. Zambonelli, F., Jennings, N., Wooldridge, M.: Developing multiagent systems: the
GAIA methodology. ACM Transactions on Software Engineering and Methodol-
ogy 12(3), 317–370 (2003)

Modal Logics for Preferences and Cooperation:
Expressivity and Complexity
C´edric D´egremont and Lena Kurzen⋆
Universiteit van Amsterdam
Abstract. This paper studies expressivity and complexity of normal
modal logics for reasoning about cooperation and preferences. We iden-
tify a class of local and global notions relevant for reasoning about coop-
eration of agents that have preferences. Many of these notions correspond
to game- and social choice-theoretical concepts. We specify the expressive
power required to express these notions by determining whether they are
invariant under certain relevant operations on diﬀerent classes of Kripke
models and frames. A large class of known extended modal languages
is speciﬁed and we show how the chosen notions can be expressed in
fragments of this class. To determine how demanding reasoning about
cooperation is in terms of computational complexity, we use known com-
plexity results for extended modal logics and obtain for each local notion
an upper bound on the complexity of modal logics expressing it.
1
Introduction
Cooperation of agents is a major issue in ﬁelds such as computer science, eco-
nomics and philosophy. The conditions under which coalitions are formed occur
in various situations involving multiple agents. A single airline company for
instance cannot aﬀord the cost of an airport runway whereas a group of com-
panies can. Generally, agents can form groups in order to share complementary
resources or because as a group they can achieve better results than individu-
ally. Modal logic (ML) frameworks for reasoning about cooperation mostly focus
on what coalitions can achieve. Coalition Logic (CL) [1] uses modalities of the
form [C]φ saying that “coalition C has a joint strategy to ensure that φ”. CL
has neighborhood semantics but it has been shown how it can be simulated on
Kripke models [2].
Another crucial concept for reasoning about interactive situations is that of
preferences. It also received attention from modal logicians ([3] surveys). Recent
works (e.g. [4,5]) propose diﬀerent mixtures of cooperation and preference logics
for reasoning about cooperation. In such logics many concepts from game theory
(GT) and social choice theory (SCT) are commonly encountered. Depending on
the situations to be modelled, diﬀerent bundles of notions are important. Ability
⋆Authors are supported by a GLoRiClass fellowship of the EU Commission (Research
Training Fellowship MEST-CT-2005-020841) and by the Dutch Organization for Sci-
entic Research (NWO), TACTICS project, grant number 612.000.525, respectively.
J.-J.Ch. Meyer and J.M. Broersen (Eds.): KRAMAS 2008, LNAI 5605, pp. 32–50, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

Modal Logics for Preferences and Cooperation
33
to express these notions – together with good computational behavior – make a
logic appropriate for reasoning about the situations under consideration.
Rather than proposing a new logical framework, with speciﬁc expressivity and
complexity, we identify how social choice theory and game theory notions are
demanding for MLs in terms of expressivity and complexity. We identify notions
relevant for describing interactive situations. Some of them are local, i.e. they
are properties of pointed models.
We determine under which operations on models these properties are invari-
ant. Other properties are global, i.e. they are properties of frames. For each of
them, we check whether a class of frames having this property is closed un-
der certain operations. We refer to such results as satisﬁability invariance and
validity closure results respectively.
We also give explicit deﬁnability results for them. Given a local property P
we give a formula φ such that a pointed model M, w ⊩φ iﬀM, w has property
P. Given a global property Q we give a formula ψ such that a frame F ⊩φ
iﬀF has property P. We thus identify the natural (extended) modal languages
needed depending on the class of frames actually considered and the particular
bundle of notions of interest. We draw some consequences about the complexity
of reasoning about cooperation using ML.
Our results apply to logics interpreted on Kripke structures using a (prefer-
ence) relation for each agent and a relation for each coalition. The latter can be
interpreted in various ways. The pair (x, y) being in the relation for coalition C
can e.g. mean:
– Coalition C considers y as being at least as good as x.
– If the system is in state x, C would choose y as the next state.
– C can submit a request such that if it is the ﬁrst one received by the server
while the state is in x, then the state of the system will change from x to y.
– When the system is in state x, C considers it possible that it is in state y.
Interpreting the relation as the possibility to bring the system into a diﬀerent
state applies to scenarios where agents act sequentially (e.g. with a server treat-
ing requests in a “ﬁrst-come, ﬁrst-served” manner) rather than simultaneously
(as in ATL [6] or CL). In special cases - e.g. for turn-based [7,1] frames - the
approaches coincide. Still, the two approaches are ﬁrst of all complementary.
Our focus in this paper is on concepts bridging powers and preferences. The
same analysis is possible for powers themselves in ATL-style. Both analyses can
then be combined in an interesting way. Finally, an important alternative in-
terpretation of the coalition relation is that of group preferences, in which case
ATL models can simply be merged with the models we consider. We discuss the
possible interpretations of such models in more details in Section 2.
Structure of this Paper. Sect. 2 presents three classes of models of cooperative
situations. Sect. 3 introduces local and global notions motivated by ideas from
GT and SCT indicating local properties of a system and global properties that
characterize classes of frames. Sect. 4 presents a large class of extended modal
languages and background invariance results. In Sect. 5, we study the expressiv-
ity needed to express the local notions (to deﬁne the global properties) by giving

34
C. D´egremont and L. Kurzen
invariance results for relevant operations and relations between models (frames).
Sect. 6 completes this work by deﬁning the notions in fragments of (extended)
modal languages. We give complexity results for model checking and satisﬁa-
bility for these languages and thereby give upper bounds for the complexity of
logics that can express the introduced notions. Sect. 7 concludes.
2
The Models
Our aim is to study how demanding certain GT and SCT concepts are in terms
of expressivity and complexity. This depends on the models chosen. We consider
three classes of simple models that have many suitable interpretations. This
gives our results additional signiﬁcance. A frame refers to the relational part of
a model. For simplicity, we introduce models and assume that the domain of the
valuation is a countable set of propositional letters PROP and nominals NOM. We
focus on model theory and postpone discussion of formal languages to Sect. 4.
Deﬁnition 1 (N-LTS). A N-LTS (Labeled Transition Systems indexed by a ﬁnite
set of agents N) is of the form ⟨W, N, {
C→| C ⊆N}, { ≤i | i ∈N}, V ⟩, where
W ̸= ∅, N = {1, . . . , n} for some n ∈IN,
C→⊆W ×W for each C ⊆N, ≤j⊆W ×W
for each j ∈N, and V : PROP ∪NOM →℘W, |V (i)| = 1 for each i ∈NOM.
W is the set of states, N a set of agents and w
C→v says that coalition C can change
the state of the system from w into v. As mentioned, other interpretations are
possible. w ≤i v means that i ﬁnds the state v at least as good as w. w ∈V (p)
means that p is true at w. Preferences are usually assumed to be total pre-orders
(TPO). Let TPO-N-LTS denote the class of N −LTSs in which for each i ∈N, ≤i is
a TPO. We also consider models with strict preferences as explicit primitives.
Deﬁnition 2 (S/TPO −N −LTS). Deﬁne S/TPO −N −LTS as models of the form
⟨W, N, {
C→
| C ⊆N}, {
≤i
| i ∈N}, {
<i
| i ∈N}, V ⟩,which extend
TPO −N −LTS models by an additional relation <i⊆W × W for each i ∈N
with the constraint that for each i ∈N, w <i v iﬀw ≤i v and v ̸≤i w.
Depending on the interpretation of
C→, it can be complemented or replaced by
eﬀectivity functions (CL) or more generally transition functions as in ATL. In
the latter sense, powers of coalitions will in general not reduce to relations on
states. We leave an analysis of powers in such settings aside for now. There would
be two ways to go: drawing on the model-theory of neighborhood semantics [8]
or on a normal simulation of CL [2]. Generally, the expressive power might
depend on whether coalitional powers are taken as primitives or computed from
individual powers.
In the next section, we identify a list of notions inspired by concepts from
game theory and social choice theory for reasoning about cooperative ability and
preferences of agents. Then we will determine the expressivity required by certain
local and global notions, by giving invariance results for pointed models and
closure conditions of classes of frames, respectively. Since we are also interested

Modal Logics for Preferences and Cooperation
35
in the eﬀects of the underlying models on the expressivity required to express the
local notions, we will give invariance results with respect to the three diﬀerent
types of models we just introduced.
3
The Notions
Reasoning about cooperative interaction considers what coalitions of agents can
achieve and what individuals prefer. Using these elements, more elaborated no-
tions can be built. We consider natural counterparts of SCT and GT notions
and are interested both in local notions i.e. properties of a particular state in
a particular system, i.e. properties of pointed models M, w, and also in global
notions, which are properties of classes of systems. In other words, we are in-
terested in the class of frames a global property characterizes. With respect to
content, apart from notions describing only coalitional powers or preferences, we
consider stability and eﬀectivity concepts.
Power of Coalitions. We now present some interesting notions about coali-
tional power. Recall that w
C→v can e.g. mean “C can achieve v at w”.
Local Notions. Interesting properties of coalitional power involve the relation
between the powers of diﬀerent groups (PowL3) and the contribution of individ-
uals to a group’s power, e.g. an agent is needed to achieve something (PowL2).
– PowL1. Coalition C can achieve a state where p is true. ∃x(w
C→x ∧P(x))
– PowL2. Only groups with i can achieve p-states. 
C⊆N\i(∀x(w
C→x ⇒¬P(x)))
– PowL3. Coalition C can force every state that coalition D can force.
∀x(w
D→x ⇒w
C→x)
Global Notions. PowG1 says that each coalition can achieve exactly one result.
PowG3 expresses coalition monotonicity: it says that if a coalition can achieve
some result, then so can every superset of that coalition. In many situations, deci-
sion making in groups can only be achieved by a majority (PowG2). PowG4 and
PowG5 exemplify (mathematically natural) consistency requirements between
powers of non-overlapping coalitions.
– PowG1. In any state each coalition can achieve exactly one state.

C⊆N ∀x∃y(x
C→y ∧∀z(x
C→z ⇒z = y))
– PowG2. Only coalitions containing a majority of N can achieve something.
∀x(
C⊆N,|C|< |N|
2 (¬∃y(x
C→y)))
– PowG3. Coalition monotonicity, i.e. if for C and D, C ⊆D, then RC ⊆RD.
∀x(
C⊆N

D⊆N,C⊆D(∀y(x
C→y ⇒x
D→y)))
– PowG4. If C can achieve something, then subsets of its complement cannot
achieve anything.
∀x 
C⊆N(((∃y(x
C→y)) ⇒
D⊆N\C ¬∃z(x
D→z)))
– PowG5. If C can achieve something, then subsets of its complement cannot
achieve something C cannot achieve.
∀x 
C⊆N(((∃y(x
C→y)) ⇒
D⊆N\C ∀z(x
D→z ⇒x
C→z)))

36
C. D´egremont and L. Kurzen
Preferences. What do agents prefer? What are suitable global constraints on
preferences? w ≤i v means “i ﬁnds v at least as good (a.l.a.g.) as w”. We write
w <i v for w ≤i v ∧¬(v ≤i w), meaning that “i strictly prefers v over w”.
Local Notions.
First of all, we can distinguish between strict and nonstrict
preferences. The most basic preference relation that we consider is that of being
a.l.a.g. We can also look at the relation “at least as bad” (a.l.a.b) (PrefL4).
Agents’ preferences over states can also be seen as being based on preferences
over propositions [9]. PrefL8 (PrefL10) says the truth of a given proposition
is a suﬃcient (necessary) condition for an agent to prefer some state. In what
follows, “at least as good” (a.l.a.g) means “at least as good as the current state”.
– PrefL1. There is a state i ﬁnds a.l.a.g. where p holds. ∃x(w ≤i x ∧P(x))
– PrefL2. There is a p-state that i strictly prefers. ∃x(w <i x ∧P(x))
– PrefL3.There is a state that all agents ﬁnd a.l.a.g and that at least one
strictly prefers. ∃x(
i∈N(w ≤i x) ∧
j∈N w <j x)
– PrefL4. There is a state that i ﬁnds a.l.a.b. where p holds. ∃x(x ≤i w∧P(x))
– PrefL5. There is a state that i ﬁnds strictly worse where p is true.
∃x(x <i w ∧P(x))
– PrefL6. i ﬁnds a state a.l.a.g. a the current one iﬀj does.
∀x(w ≤i x ↔w ≤j x)
– PrefL7. There is a state only i ﬁnds a.l.a.g. ∃x(w ≤i x ∧
j∈N\{i} ¬(w ≤j x))
– PrefL8. i ﬁnds every p-state a.l.a.g. ∀x(P(x) ⇒w ≤i x)
– PrefL9. i strictly prefers every p-state. ∀x(P(x) ⇒w <i x)
– PrefL10. i considers only p-states to be a.l.a.g. ∀x(w ≤i x ⇒P(x))
– PrefL11. i strictly prefers only p-states. ∀x(w <i x ⇒P(x))
Global Notions. Capturing the intuitive idea of preferences requires several
conditions for the preference relation: reﬂexivity, transitivity and completeness
(trichotomy for strict preferences). Sometimes, it can also be appropriate to say
that for each alternative there is exactly one that is at least as good (PrefG8).
– PrefG1. “at least as good as” is reﬂexive. ∀x(
i∈N(x ≤i x))
– PrefG2. “at least as good as” is transitive.
∀x∀y∀z(
i∈N((x ≤i y ∧y ≤i z) ⇒x ≤i z))
– PrefG3. “at least as good as” is complete.
∀x∀y(
i∈N(x ≤i y ∨y ≤i x))
– PrefG4. “at least as good as” is a total pre-order.
(Conjunction of the two previous formulas.)
– PrefG5. “strictly better than” is transitive.
∀x∀y∀z((
i∈N(x <i y ∧y <i z) ⇒x <i z)))
– PrefG6. “strictly better than” is trichotomous.
∀x∀y(
i∈N(x <i y ∨y <i x ∨x = y))
– PrefG7. “strictly better than” a strict total order.
(Conjunction of the previous two formulas.)
– PrefG8. Determinacy for “at least as good as”, i.e. exactly one successor.
∀x(
i∈N(∃y(w ≤i y ∧∀z(x ≤i z ⇒z = y))))

Modal Logics for Preferences and Cooperation
37
So far, we focussed on preferences of individuals. A natural question in SCT is
how to aggregate individual preferences into group preferences. We can address
this question by interpreting
C→as a preference relation for each C ⊆N.
– PrefG9. C ﬁnds a state a.l.a.g. as the current one iﬀall its members do.
∀x∀y(
C⊆N(x
C→y ↔
i∈C x ≤i y))
– PrefG10. C ﬁnds a state at least as good as the current one iﬀat least one
member does.
∀x∀y(
C⊆N(x
C→y ↔
i∈C x ≤i y))
– PrefG11. C ﬁnds a state a.l.a.g. as the current one iﬀmost members do.
∀x∀y(
C⊆N(x
C→y ↔
D⊆C,|D|> |C|
2 (
i∈D x ≤i y)))
Combining preceding concepts. We start with the conceptually and histori-
cally important SCT notion of a dictator. d is a dictator if the group’s preferences
mimic d’s preferences. Interpreting
C→as achievement relation, we get an even
stronger notion: groups can only do what d likes. A local dictator is a dictator
who controls one state in the system, and a dictator controls all states.
Deﬁnition 3 (Local Dictatorship). i is a weak (strong) local dictator at w
iﬀany group prefers v at w only if for i, v is a.l.a.g. as (strictly better than) w.
We now introduce combinations of powers and preferences. The ﬁrst notion says
that coalition C can do something useful for i (in some cases giving i an incentive
to join) and the third notion characterizes situations in which a unanimously
desired state remains unachievable. We start with Local Notions.
– PPL1. C can achieve a state that i ﬁnds at least as good as the current one.
∃x(w
C→x ∧w ≤i x)
– PPL2. C can achieve a state that all i ∈D ﬁnd a.l.a.g. as the current one.
∃x(w
C→x ∧
i∈D w ≤i x)
– PPL3. There is a state that all agents prefers but no coalition can achieve
it. ∃x((
i∈N w ≤i x) ∧
C⊆N ¬(w
C→x))
– PPL4. C can achieve all states that agent i ﬁnds a.l.a.g. as the current one.
∀x(w ≤i x ⇒w
C→x)
– PPL5. C can achieve all states that i strictly prefers. ∀x(w <i x ⇒w
C→x)
– PPL6. i is a weak local dictator. ∀x(
C⊆N(w
C→x ⇒w ≤i x))
– PPL7. i is a strong local dictator. ∀x(
C⊆N(w
C→x ⇒w <i x))
Global Notions. PPG1 is a natural constraint on coalitional power: a group
can achieve a state iﬀit is good for all members - otherwise they would not take
part in the collective action. PPG3 is a condition of Arrow’s impossibility the-
orem. PPG4 reﬂects individual rationality: don’t join a group if you don’t gain
anything. It can be generalized to every sub-coalition or weakened to “not joining
if you lose something” (cf. core of a coalitional game [10] (Def. 268.3)). PPG5
applies to systems where an agent is indispensable to achieve anything: a unique
capitalist in a production economy or a unique server are typical examples.

38
C. D´egremont and L. Kurzen
– PPG1. Coalitions can only achieve states that all its members consider at
least as good as the current one. ∀x∀y 
C⊆N(x
C→y ⇒
i∈C(x ≤i y))
– PPG2. One agent is a weak local dictator in every state (dictator).

i∈N ∀x∀y(x
C→y ⇒x ≤i y)
– PPG3. There is no dictator. ¬(
i∈N ∀x∀y(x
C→y ⇒x ≤i y))
– PPG4. If i can achieve some state i strictly prefers then for any C containing
i: if C \ i cannot achieve some state but C can, then i strictly prefers that
state. 
i∈N ∀x(∃y(x
{i}
→y∧x <i y) ⇒
C⊆N,i∈C(∀z(x
C→z∧¬(x
C\{i}
→z)) ⇒x <i z))
– PPG5. Only groups with i can achieve something. ∀x 
C⊆N\{i} ¬∃y(x
C→y)
– PPG6. In all states, there is an i such that groups with i can achieve exactly
the states as they can without i. ∀x(
i∈N

C⊆N,i∈C ∀y(x
C→y ↔x
C\{i}
→y))
– PPG7. For any agent, there is some state in which coalitions not containing
this agent cannot achieve any state. 
i∈N ∃x(
C⊆N,i∈C ¬∃y(x
C→y))
Eﬃciency and Stability Notions. In our setting, it is natural to interpret
the state space as possible social states or allocations of goods. A criterion from
welfare economics to distinguish “good” from “bad” states is that of eﬃciency: if
we can change the allocation or social state and make an agent happier without
making anyone less happy then we are using resources more eﬃciently and it is
socially desirable to do so. E.g. PrefL3 in this respect means that the current
state is not eﬃcient: there is a state that is a Pareto-improvement of it. Importing
the notion of Pareto-eﬃciency into our framework is straightforward.
Deﬁnition 4 (Pareto-eﬃciency).
A state
is
weakly
(strongly)
Pareto-
eﬃcient iﬀthere is no state that everyone strictly prefers (ﬁnds a.l.a.g). A state
is Pareto-eﬃcient iﬀthere is no state such that everyone considers it to be at
least as good and at least one agent thinks that it is strictly better.
GT equilibrium concepts characterize stable states: given what others are doing,
I don’t have an incentive to do something that makes us leave this stable state.
Generalizing, a system is in a stable state if nobody has an incentive to change its
current state. We can think of strategy proﬁles in a strategic game as assigning
roles to the agents. Two proﬁles x = (s∗
−i, s∗
i ), y = (s∗
−i, s′
i) are related by
{i}
→
iﬀi can unilaterally change role (strategy) to s′
i in the next round of the game.
E.g. the stability of a state where an agent provides the public good on his own
depends on whether he cares enough about it to provide it on his own. A state is
stable iﬀthere is no strictly preferred state that an agent can achieve alone. Since
the idea relates to Nash equilibria (see [10]), we use the names Nash-stability,
and Nash-cooperation stability for its group version.
Deﬁnition 5 (Nash-stability). A state is (strongly) Nash-stable iﬀthere is no
state that an agent i strictly prefers (ﬁnds a.l.a.g.) and that i can achieve alone.
It is (strongly) Nash-cooperation stable iﬀthere is no state v and coalition such
C that every i ∈C strictly prefers v (ﬁnds v a.l.a.g.) and C can achieve v.

Modal Logics for Preferences and Cooperation
39
Local Notions
– EF1.The current state is weakly Pareto-eﬃcient. ¬∃x(
i∈N(w <i x))
– EF2. The current state is Pareto-eﬃcient. ¬∃x((
i∈N w ≤i x) ∧
j∈N w <i x)
– EF3. The current state is strongly Pareto-eﬃcient. ¬∃x(
i∈N w ≤i x)
– ST 1. The current state is Nash stable. ¬∃x(
i∈N(w
{i}
→x ∧w <i x))
– ST 2. The current state is strongly Nash stable. ¬∃x(
i∈N(w
{i}
→x ∧w ≤i x))
– ST 3. The current state is Nash-cooperation stable.
¬∃x(
C⊆N(w
C→x ∧
i∈C w <i x))
– ST 4. The current state is strongly Nash-cooperation stable.
¬∃x(
C⊆N(w
C→x ∧
i∈C w ≤i x))
4
Modal Languages and Their Expressivity
As will be clear from invariance results of next sections, Basic Modal Language
will generally be too weak for reasoning about cooperation. However, any notion
expressible in the FO correspondence language is expressible in the hybrid
language H(E, @, ↓) [11]. Amongst temporal logics, boolean modal logics and
the various hybrid logics, there are well-understood fragments. We introduce all
these Extended Modal Languages at once as a “super” logic.
Syntax. The syntax of this “super” logic is recursively deﬁned as follows:
α ::=
≤j | C | υ | α−1 | ?φ | α; α | α ∪α | α ∩α | α
φ ::= p | i | x | ¬φ | φ ∧φ | ⟨α⟩φ | Eφ | @iφ | @xφ | ↓x.φ |  α  φ |
where j ∈N, C ∈℘(N) −{∅}, p ranges over PROP, i ranges over NOM and
x ∈SVAR, for SVAR being a countable set of variables.
Semantics. Valuation maps propositional letters to subsets of the domain and
nominals to singleton subsets. Given a N −LTS, a program α is interpreted as
a relation as indicated on the left. Formulas are interpreted together with an
assignment g : SVAR →W as indicated (mostly) on the right. We skip booleans.
M, w, g ⊩i iﬀw ∈V (i)
M, w, g ⊩x
iﬀw = g(x)
R≤i
= ≤i
M, w, g ⊩⟨α⟩φ
iﬀ∃v : wRαv and M, v, g ⊩φ
RC
=
C→
M, w, g, ⊩Eφ
iﬀ∃v ∈W M, v, g ⊩φ
Rβ−1
= {(v, w)|wRβv}
M, w, g, ⊩@iφ
iﬀM, v, g ⊩φ where V (i) = {v}
Rβ∪γ
= Rβ ∪Rγ
M, w, g, ⊩@xφ
iﬀM, g(x), g ⊩φ
Rβ∩γ
= Rβ ∩Rγ
M, w, g, ⊩↓x.φ
iﬀM, w, g[x := w] ⊩φ
Rβ
= (W × W ) −Rβ
M, w, g ⊩ α  φ iﬀwRαv whenever M, v, g ⊩φ
Expressivity. The least expressive modal language we consider is L(N), which
is of similarity type ⟨(C)C⊆N, (≤i)i∈N⟩. Its natural extensions go along two lines:
adding program constructs and new operators. L(N, ∩, i) e.g. refers to the logic
with language: α ::=
≤j | C | α ∩α
φ ::=
p | i | ¬φ | φ ∧φ | ⟨α⟩φ.
As
language inclusion implies expressivity inclusion (indicated by “≤”), we only
indicate (some) non-obvious facts of inclusions in this space of modal languages.

40
C. D´egremont and L. Kurzen
Fact 1. L(N, ∪, ; , ?) ≤L(N).
Proof. By the facts that M, w, g ⊩⟨α∪β⟩φ iﬀM, w, g ⊩⟨α⟩φ ∨⟨β⟩φ, M, w, g ⊩
⟨α; β⟩φ iﬀM, w, g ⊩⟨α⟩⟨β⟩φ and by M, w, g ⊩⟨?ψ⟩φ iﬀM, w, g ⊩ψ ∧φ.
Fact 2. L(N, @, i) ≤L(N, E, i).
Proof. By the fact that M, w, g ⊩@iφ iﬀM, w, g ⊩E(i ∧φ).
Fact 3. L(N, ∩) ≤L(N, ↓, @, x).
Proof. By the fact that M, w, g ⊩⟨α∩β⟩φ iﬀM, w, g ⊩↓x.⟨α⟩(↓y.φ ∧@x⟨β⟩y).
Fact 4. L(N,  ) ≤L(N, ).
Proof. By the fact that M, w, g ⊩ α  φ iﬀM, w, g ⊩[α]¬φ.
Fact 5. L(N, ) ≤L(N, ↓, E, x).
Proof. By the fact that M, w, g ⊩⟨α⟩φ iﬀM, w, g ⊩↓x.E ↓y.(φ ∧¬E(x ∧⟨α⟩y)).
Fact 6. L(N, −1) ≤L(N, ↓, E, x).
Proof. By the fact that M, w, g ⊩⟨α−1⟩φ iﬀM, w, g ⊩↓x.E(φ ∧⟨α⟩x).
Fact 7. L(N, E) ≤L(N, ).
Proof. By the fact that M, w, g ⊩Eφ iﬀM, w, g ⊩⟨α⟩φ ∨⟨α⟩φ.
Expressivity of MLs is usually characterized by invariance results. Deﬁnitions
and background results follow. We ﬁrst introduce some relations between models.
Let τ be a ﬁnite modal similarity type with only binary relations. Let M =
⟨W, (Rk)k∈τ, V ⟩and M′ = ⟨W ′, (R′
k)k∈τ, V ′⟩be models of similarity type τ.
Deﬁnition 6 (Bisimulations). A bisimulation between M and M′ is a non-
empty binary relation Z ⊆W × W ′ fulﬁlling the following conditions:
AtomicHarmony For every p ∈PROP, wZw′ implies w ∈V (p) iﬀw′ ∈V ′(p).
Forth
∀k ∈τ , if wZw′ & Rkwv then ∃v′ ∈W ′ s.t. R′
kw′v′ & vZv′.
Back
∀k ∈τ , if wZw′ & R′
kw′v′ then ∃v ∈W
s.t. Rkwv & vZv′.
In a nutshell, ∩-Bisimulations (resp. CBisimulations) require that Back and
Forth also hold for the intersection (resp. the converse) of the relations.
H-Bisimulations extend AtomicHarmony to nominals. TBisimulations (H(@)-
bisimulations) are total 1 bisimulations (resp. total H-Bisimulations). H(E)-
Bisimulations are H-Bisimulations matching states “with the same name”. See
[11] for details. We now deﬁne bounded morphisms, generated subframes and
disjoint unions.
1 Z ⊆W × W ′ is total iﬀ∀w ∈W ∃w′ ∈W ′ wZw′ & ∀w′ ∈W ′ ∃w ∈W wZw′.

Modal Logics for Preferences and Cooperation
41
Deﬁnition 7 (BM). f : W →W ′ is a bounded morphism from M to M′ iﬀ:
AtomicHarmony
For every p ∈PROP, w ∈V (p) iﬀf(w) ∈V ′(p).
R −homomorphism ∀k ∈τ , if Rkwv then R′f(w)f(v).
Back
∀k ∈τ , if R′
kf(w)v′ then ∃v ∈W
s.t. f(v)=v′ and Rkwv.
Deﬁnition 8 (Generated Submodel). We say that that M′ is a generated
submodel (GSM) of M iﬀW ′ ⊆W, ∀k ∈τ , R′
k = Rk ∩(W ′ × W ′), ∀p ∈PROP ,
V ′(p) = V (p) ∩(W ′ × W ′) and if w ∈W ′ and Rwv then v ∈W ′.
Deﬁnition 9 (Disjoint Unions). Let (Mj)j∈J be a collection of models with
disjoint domains. Deﬁne their disjoint union 
j Mj = ⟨W, R, V ⟩as the union
of their domains and relations, and deﬁne for each p ∈PROP, V (p) := 
j Vj(p).
Deﬁnition 10 (Invariance). A property of pointed models Φ(X, y) is invari-
ant under λ-Bisimulations iﬀwhenever there exists a λ-bisimulation Z between
M and M′ such that (w, w′) ∈Z, then Φ(M, w′) holds iﬀΦ(M′, w′) holds.
Invariance for other operations is deﬁned similarly.
We now consider closure conditions. First, we consider bounded morphic images
(BMI) of frames. BM on frames are obtained by dropping AtomicHarmony in Def.
7. A class of frames is closed under BMI iﬀit is closed under surjective BM. Next,
we consider closure under generated subframes (GSF) – the frame-analogue to
GSM (cf. Def. 8). We also check if properties reﬂect GSF. A property φ reﬂects GSF
if whenever for every frame F, it holds that every GSF of F has property φ, then
so does F. We also consider closure under taking disjoint unions (DU) of frames,
which are deﬁned in the obvious way. Moreover, we look at closure under images
of bisimulation systems [11], which are families of partial isomorphisms.
Deﬁnition 11 (Bisimulation System). A bisimulation system from a frame
F to a frame F′ is a function Z : ℘W ′ →℘(W × W ′) that assigns to each
Y ⊆W ′ a total bisimulation Z(Y ) ⊆W × W ′ such that for each y ∈Y :
1. There is exactly one w ∈W such that (w, y) ∈Z(Y ).
2. If (w, y), (w, w′) ∈Z(Y ), then w′ = y.
Background results. We indicate three classical characterization results. For
details see [12,11]. Let φ(x) be a formula of the FO correspondence language with
at most one free variable. [13] proved that φ(x) is invariant under bisimulations
iﬀφ(x) is equivalent to the standard translation of a modal formula. While
[14,15] proved that φ(x) is invariant under taking generated submodels iﬀφ(x)
is equivalent to the standard translation of a formula of L(N, ↓, @, x). On the level
of frames [16] proved that a FO deﬁnable class of frames is modally deﬁnable iﬀit
is closed under taking BMI, GSF, disjoint unions and reﬂects ultraﬁlter extensions.
The reader might now like to see immediately how the notions can be deﬁned
in extended modal languages and go directly to Sect. 6. Of course, the choice
of the languages is only justiﬁed once we have determined the required expres-
sive power both to express the local notions and to deﬁne the class of frames
corresponding to the global ones. Thus we start by doing so in the next section.

42
C. D´egremont and L. Kurzen
5
Invariance and Closure Results
We start with satisﬁability invariance results for the classes of pointed models
deﬁned in Sect. 2. Then we turn to closure results for classes of frames deﬁned
by global notions. A “Y” in a cell means that the row notion is invariant under
the column operation. The number in the columns refer to representative proofs
for these results found below the tables. They will give the reader a concrete
idea of the meaning of these results. More of them in our technical report [17].
Overview of the Results for the General Case
Bis CBis ∩-Bis TBis H-Bis H(@)-Bis H(E)-Bis BM GSM DU
[PowL1]
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
[PowL2]
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
[PowL3]
N
N
N
N
N
N
N
N
Y
Y
[PrefL1]
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
[PrefL2]
N
N
N
N
N
N
N
N
Y
Y
[PrefL3]
N
N
N
N
N
N
N
N
Y
Y
[PrefL4]
N
Y
N
N
N
N
N
N
N(2)
Y
[PrefL5]
N
N
N
N
N
N
N
N
N
Y
[PrefL6]
N
N
N
N
N
N
N
N
Y
Y
[PrefL7]
N
N
N
N
N
N
N
N
Y
Y
[PrefL8]
N
N
N
N
N
N
N
N
N
N
[PrefL9]
N
N
N
N
N
N
N
N
N
N
[PrefL10] Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
[PrefL11] N
N
N
N
N
N
N
N
Y
Y
[PPL1]
N
N
Y
N
N
N
N
N
Y
Y
[PPL2]
N
N
Y
N
N
N
N
N
Y
Y
[PPL3]
N
N
N
N
N
N
N
N
Y
Y
[PPL4]
N
N
N
N
N
N
N
N
Y
Y
[PPL5]
N
N
N
N
N
N
N
N
Y
Y
[PPL6]
N
N
N
N
N
N
N
N
Y
Y
[PPL7]
N
N
N
N
N
N
N
N
Y
Y
[EF1]
N
N
N
N
N
N
N
N
Y
Y
[EF2]
N
N
N
N
N
N
N
N
Y
Y
[EF3]
N
N
Y
N
N
N
N
N
Y
Y
[ST1]
N
N
N
N
N
N
N
N
Y
Y
[ST2]
N
N
Y
N
N
N
N
N
Y
Y
[ST3]
N
N
N (1)
N
N
N
N
N
Y
Y
[ST4]
N
N
Y
N
N
N
N
N
Y
Y
Comments. Most of our notions are not bisimulation-invariant. The basic
modal language 2
is thus not expressive enough to describe our local notions
(without restrictions on the class of frames). Invariance under BM often fails; some
failures are due to intersections of relations, but as ∩-Bis also fails often, this
cannot be the only reason. By contrast, invariance under GSM generally holds; it
fails for properties with backward looking features. This is good news for expres-
2 of similarity type ⟨{
C→| C ⊆N}, { ≤i | i ∈N}⟩

Modal Logics for Preferences and Cooperation
43
sivity: we can expect deﬁnability in the hybrid language with ↓-binder. 3 But
not for computability, since the satisﬁability problem of the bounded fragment
is undecidable. Finally, the results are the same for hybrid and basic bisimula-
tions. No suprise: roughly speaking, at the level of local satisfaction, to exploit
the expressive power of nominals, the notions would have to refer explicitly to
some state. Here are two representative results.
Representative Proofs for the General Case
Proposition 1. On N −LTS, ST 3 is not invariant under ∩-bisimulation.
Proof. Let M = ⟨{w, v}, {1, 2}, {
C→|C ⊆{1, 2}}, {≤1, ≤2}, V ⟩, where w
{1,2}
→
v, w ≤1 v, v ≤1 w, w ≤2 v, V (p) = {w, v}. Let M′ = ⟨{s, t, u}, {1, 2}, {
C→′|C ⊆
{1, 2}}, {≤′
1, ≤′
2}, V ′⟩, where s
{1,2}
→′t, u
{1,2}
→′t, s ≤′
1 t, u ≤′
1 t, t ≤′
1 u, s ≤′
2 t, u ≤′
2
t, V ′(p) = {s, t, u}. Then, M, w ⊩ST 3 and M′, s ⊮ST 3 because s
{1,2}
→′t and
s <′
1 t, s <′
2 t. Moreover, Z = {(w, s), (w, u), (v, t)} is a ∩-bisimulation.
⊓⊔
Proposition 2. On N −LTS, PrefL4 is not invariant under GSM.
Proof. Let M = ⟨{w, v}, {1}, {
C→|C ⊆{1}}, {≤1}, V ⟩, where
{1}
→= ∅, v ≤1
w, V (p) = {v}. Then, M, w ⊩PrefL4 because v ≤1 w and v ∈V (p). But
for the submodel M′ generated by {w}, M′, w ⊮PrefL4 since v is not con-
tained in M′.
⊓⊔
Results Overview for the Total Pre-orders (TPO) Case. This table shows
rows that diﬀer from the general case. Entries that diﬀer are in boldface.
Bis CBis ∩-Bis TBis H-Bis H(@)-Bis H(E)-Bis BM GSM DU
[PrefL8] N
N
N
N
N
N
N
N
N
Y*
[PrefL9] N Y (3)
N
N
N
N
N
N
N
Y*
[EF1]
N
N
N
N
N
N
N
N
Y
Y*
Proposition 3. On TPO −N −LTS, PrefL9 is invariant under CBis.
Proof. Let M, w and M′, w′ be two pointed TPO −N −LTS such that there exists
a C-Bisimulation Z between M and M′ such that (w, w′) ∈Z. But now assume
that M, w ̸⊩PrefL9. It follows that there exists some t ∈W such that t ∈
V M(p) but w ̸<M
i
t. But then by totality of ≤M
i
we have t ≤M
i
w, i.e. w ≥M
i
t.
But then by C-Bisimulation there exists some state t′ ∈W ′ such that (t, t′) ∈Z
and therefore t′ ∈V M′(p) and w′ ≥M′
i
t′ and thus w′ ̸<M′
i
t′. By deﬁnition we
have thus M′, w′ ̸⊩PrefL9. The other direction is symmetrical.
Comments. Except for disjoint union (DU), the restriction to the TPO case brings
only slight beneﬁts. ∗marks trivial invariance: the only DU of models that is
complete is the trivial one: mapping a model to itself.
3 [15,14] have proved that all notions deﬁnable in the ﬁrst-order correspondence lan-
guage that are invariant under GSM are equivalent to a formula of the bounded
fragment, i.e. of the hybrid language with ↓-binder (which are notational variants).

44
C. D´egremont and L. Kurzen
Overview of the Results for the TPO Case with Strict Preferences. The
following table contains the rows that diﬀer from the ones in the table for total
preorders without strict preference relation.
Bis CBis ∩-Bis TBis H-Bis H(@)-Bis H(E)-Bis BM GSM DU
[PrefL2]
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
[PrefL3]
N
N
N
N
N
N
N
Y
Y
Y
[PrefL5]
N
Y
N
N
N
N
N
N
N
Y
[PrefL6]
N
N
N
N
N
N
N
Y
Y
Y
[PrefL7]
N
N
N
N
N
N
N
Y
Y
Y
[PrefL8]
N
Y
N
N
N
N
N
N
N
Y
[PrefL11] Y
Y
Y
Y
Y
Y
Y
Y (4)
Y
Y
[PPL7]
N
N
N
N
N
N
N
Y
Y
Y
[EF1]
N
N
Y
N
N
N
N
Y
Y
Y
[EF2]
N
N
Y
N
N
N
N
Y
Y
Y
[ST1]
N
N
Y
N
N
N
N
Y (5)
Y
Y
[ST3]
N
N
Y
N
N
N
N
Y
Y
Y
Proposition 4. On S/TPO-N-LTS, PrefL11 is invariant under BM.
Proof. Let M and M′ be two S/TPO −N −LTS and assume that f is a bounded
morphism from M to M′. Assume that the property PrefL11 does not hold for
M, w, i.e. there is a state v ∈Dom(M) such that w <M
i
v and v ̸∈V M(p). But
then by R −homomorphism, we have f(w) <M′
i
f(v) and by AtomicHarmony,
f(v) ̸∈V M′(p), and thus PrefL11 does not hold for M′, f(w). For the other
direction, assume that PrefL11 is not satisﬁed at M′, f(w). It follows that
there is a state v′ ∈Dom(M′) such that f(w) <M′
i
v′ and v′ ̸∈V M′(p) but
then by Back there is a state v ∈Dom(M) such that f(v) = v′ and w <M
i
v.
But by AtomicHarmony, v ̸∈V M(p) and thus PrefL11 is not satisﬁed at M, w,
concluding our proof.
Proposition 5. On S/TPO-N-LTS, ST 1 is invariant under BM.
Proof. One direction follows directly from R −homomorphism. For the other di-
rection, assume that PrefL11 is not satisﬁed at M′, f(w). It follows that there
is a state v′ ∈Dom(M′) such that f(w)
{i}
→′v′ and f(w) <M′
i
v′ (a). But then by
Back there is a state v ∈Dom(M) such that f(v) = v′ and w
{i}
→v (b). We are
in one of two cases. Case 1: v ̸≤M
i
w (c). But then by totality we have w ≤M
i
v
(d). But it follows from (b), (c), (d) that PrefL11 is not satisﬁed at M, w.
Now assume for contradiction that we are in the other case. Case 2: v ≤M
i
w
(e). But then by R −homomorphism, we have f(v) ≤M′
i
f(w) contradicting the
assumption that (a) and concluding our proof.
Comments. The failures of invariance under GSM are still present, reﬂecting the
fact that we do not have converse relations. By contrast, PrefL11 and PrefL2
are now invariant under bisimulation and a simple boolean modal logic with
intersection seems to have the right expressive power to talk about eﬃciency

Modal Logics for Preferences and Cooperation
45
and stability notions, since all of them are now invariant under ∩-Bisimulations.
We now check if the properties deﬁne classes of frames that are closed under the
diﬀerent operations introduced. The results can be read oﬀthe tables as in the
previous section.
Closure Results for class of frames deﬁned by global properties
BMI
GSF
DU
reﬂ.GSF
BisSysIm
BMI
GSF
DU
reﬂ.GSF
BisSysIm
BMI
GSF
DU
reﬂ.GSF
BisSysIm
PowG1 Y Y Y Y Y
PrefG4
Y
Y N N Y PPG1 Y Y Y Y Y
PowG2 Y Y Y Y Y
PrefG5
N
Y Y Y Y PPG2 Y Y N N Y
PowG3 Y Y Y Y Y
PrefG6
N
Y N N Y PPG3 N N N Y ?
PowG4 Y Y Y Y Y
PrefG7
N
Y N N Y PPG4 N Y Y Y Y
PowG5 Y Y Y Y Y
PrefG8
Y
Y Y Y Y PPG5 Y Y Y Y Y
PrefG1 Y Y Y Y Y
PrefG9
N
Y Y Y Y PPG6 Y Y Y Y Y
PrefG2 Y Y Y Y Y PrefG10
Y
Y Y Y Y PPG7 Y N Y Y Y
PrefG3 Y Y N N Y PrefG11 N(6) Y Y Y Y
Proposition 6. Validity of PrefG11 is not preserved under BMI.
Proof. Consider the frames F = ⟨{x, v, w}, {1, 2}, {
C→| C ⊆{1, 2}}, {≤1, ≤2}⟩,
with w
1→v, w
2→v,
{1,2}
→= ∅, w ≤1 v, w ≤2 x and F′ = ⟨{s, t}, {1, 2}, {
C→| C ⊆
{1, 2}}, {≤′
1, ≤′
2}⟩, with s
1→t, s
2→t,
{1,2}
→= ∅, s ≤1 t, s ≤2 t. Then f : W →
W ′, f(w) = s, f(v) = f(x) = t is a surjective BM. However, F ⊩PrefG11 and
F′ ⊮PrefG11 because s ≤1 t, s ≤2 and it is not the case that s
{1,2}
→t.
At the frame level, ML is a fragment of Monadic Second Order Logic. That it
does better at this level is thus not only an artifact of the chosen notions.
6
Modal Deﬁnability
The previous model-theoretic results give us information about possible deﬁn-
ability results. However, let us be more constructive and give formulas that
indeed do the job: be it for local-satisfaction or frame-deﬁnability aims. Cor-
respondence proofs can be found in the technical report. We indicate the least
expressive language we found still being able to express the property under con-
sideration. Another useful criterion is that of the computational complexity of
the logic, i.e. of its satisﬁability problem (SAT) and model checking problem
(MC). Since we lack the space to discuss these issues in depth here is how we
bridge our expressivity and complexity results: for each local (resp. global) no-
tion, ﬁnd the least expressive logic that is still able to express it locally (resp.
deﬁne the class of frames corresponding to it) and take the complexity of this
logic as an upper bound. Due to space restrictions we only indicate these upper
bounds and references for them. [17] has more details. We assume the reader to
be familiar with P, PSPACE and EXPTIME [18]. Π0
1-complete problems [19] are
undecidable but co-recursively enumerable (e.g. IN × IN tiling [20]).

46
C. D´egremont and L. Kurzen
6.1
Deﬁning Local Notions
Representative deﬁnability results
Axiom
Best
SAT
MC
Language
PowL1
⟨C⟩p
L(N)
PSPACE[21]
P[22]
PowL2

C̸∋i[C]¬p
L(N)
PSPACE[23]
P[22]
PowL3
↓x.[D] ↓y.@x⟨C⟩y
(7)
L(N, ↓, @, x)
(8) EXPTIME[24] PSPACE [25]
PrefL1
⟨≤i⟩p
L(N)
PSPACE[21]
P[22]
PrefL2
↓x.⟨≤i⟩(p ∧[≤i]¬x)
L(N, ↓, x)
EXPTIME[24]
PSPACE[25]
PrefL3
↓x.⟨
i∈N ≤i⟩(
j∈N[≤j]¬x)
L(N, ↓, ∩, x)
Π0
1[11]
PSPACE
PrefL4
⟨≤i
−1⟩p
L(N, ↓, @, x)
PSPACE
PSPACE [25]
PrefL5
↓x.⟨≤i
−1⟩(p ∧[≤i
−1]¬x)
L(N, ↓, −1, x)
Π0
1 [26]
PSPACE
PrefL6
[(≤i ∩≤j) ∪(≤j ∩≤i)]⊥
L(N, , ∩)
EXPTIME[27]
P[28]
PrefL7
⟨≤i ∩(
j∈N−{i} ≤j)⟩⊤
L(N, , ∩)
EXPTIME[27]
P[28]
PrefL8
 ≤i p
L(N,  )
EXPTIME[27]
P[28]
PrefL9
↓x.A ↓y.(¬⟨≤i⟩x ∧@x⟨≤i⟩y)
L(↓, @, x, E)
Π0
1 [26]
PSPACE[29]
PrefL10
[≤i]p
L(N)
PSPACE[23]
P[22]
PrefL11
↓x.[≤i]([≤i]¬x ⇒p)
L(↓, x)
EXPTIME[24]
PSPACE[25]
PPL1
⟨C∩≤i⟩⊤
L(N, ∩)
PSPACE [30]
P[28]
PPL2
⟨C ∩(
i∈D ≤i)⟩⊤
L(N, ∩)
PSPACE[30]
P[28]
PPL3
⟨(
i∈N ≤i) ∩(
C⊆N
C→)⟩⊤
L(N, , ∩)
EXPTIME[27]
P[28]
PPL4
[C∩≤i]⊥
L(N, , ∩)
EXPTIME[27]
P[28]
PPL5
↓x.[C∩≤i]⟨≤i⟩x
L(N ↓, , , ∩, x)
Π0
1 [26]
PSPACE
PPL6

C⊆N[C ∩≤i]⊥
L(N, , ∩)
EXPTIME[27]
P[28]
PPL7
↓x.[C] ↓y.(¬⟨≤i⟩x ∧@x⟨≤i⟩y)
L(N, ↓, @, x)
Π0
1[24]
PSPACE[25]
EF1
↓x.[
i∈N ≤i] 
i∈N⟨≤i⟩x
L(N, ↓, ∩)
Π0
1[11,24]
PSPACE
EF2
¬ ↓x.⟨
i∈N ≤i⟩(
j∈N[≤j]¬x)
L(N, ↓, ∩)
Π0
1[11,24]
PSPACE
EF3
[
i∈N ≤i]⊥
L(N, ∩)
PSPACE [30]
P[28]
ST1

i∈N ↓x.[i∩≤i]⟨≤i⟩x
L(N, ↓, ∩)
Π0
1[11]
PSPACE
ST2

i∈N[i∩≤i]⊥
L(N, ∩)
PSPACE [30]
P[28]
ST3

C⊆N ↓x.[C ∩(
i∈C ≤i)] 
j∈C⟨≤j⟩x
L(N, ↓, ∩)
Π0
1[24]
PSPACE
ST4

C⊆N[C ∩(
i∈C ≤i)]⊥
L(N, ∩)
PSPACE [30]
P[28]
Proposition 7. PowL3 is true of M, w iﬀM, w, g ⊩↓x.[D] ↓y.@x⟨C⟩y.
Proof. From right to left: Assume that M, w, g ⊩↓x.[D] ↓y.@x⟨C⟩y. Then
M, w, g[x := w], ⊩[D] ↓y.@x⟨C⟩y. But now assume there is a state v that
coalition D can force from w. By deﬁnition, w
D→v (1). But by (1) and semantics
of [D] then we have M, v, g[x := w], ⊩↓y.@x⟨C⟩y (2). (2) and semantics of ↓gives
us M, v, g[x := w, y := v] ⊩@x⟨C⟩y (3). From (3) and semantics of @x and the
fact that g(x) = w we have M, w, g[x := w, y := v] ⊩⟨C⟩y (4). But by semantics
of ⟨C⟩and the fact that g(y) = v, (4) really means that w
C→v (5). Since the v
was arbitrary, it follows from (5) that at w for any state v, if D can achieve it,
then C can do so, too. But this precisely means that PowL3 is true of M, w.
⊓⊔
Theorem 1 (ten Cate [24]). The satisﬁability problems for formulas in
ML(N, ↓, @, x) −2 ↓2 with bounded width is EXPTIME-complete.

Modal Logics for Preferences and Cooperation
47
Proposition 8. PowL3 is expressible in an extended modal language with a
satisﬁability problem in EXPTIME.
Proof. By the previous proposition, we have PowL3 is deﬁned by ↓x.[D] ↓
y.@x⟨C⟩y. But ↓x.[D] ↓y.@x⟨C⟩y does contain the 2 ↓2 scheme. Thus, PowL3
is deﬁned by a formula in ML(N, ↓, @, x) −2 ↓2 (1). But by Theorem 1 the
satisﬁability problem of ML(N, ↓, @, x) −2 ↓2 is in EXPTIME.
⊓⊔
6.2
Deﬁning Global Notions
First of all, we deﬁne what it means for a formula to be valid on a class of frames.
Deﬁnition 12 (Validity on a class of frames). We say that a formula φ is
valid on a class of frames F iﬀfor any frame F ∈F and any model M based on
F, at all states w in Dom(F), M, w ⊩φ. We write F ⊩φ.
Modal deﬁnability has again two sides: We can look for a formula φ such that
M, w ⊩φ iﬀM, w has some property, or such that F ⊩φ iﬀF has the property.
Axiom
Best
SAT
MC
Language
PowG1

C⊆N((⟨C⟩φ ⇒[C]φ) ∧⟨C⟩⊤)
L(N)
PSPACE[21] P[22]
PowG2

C:|C|<|N|/2[C]⊥
L(N)
PSPACE[23] P[22]
PowG3

C⊆N(⟨C⟩φ ⇒[C]φ)
L(N)
PSPACE[21] P[22]
PowG4

C⊆N

D⊇C(⟨C⟩φ ⇒⟨D⟩φ)
L(N)
PSPACE[23] P[22]
PowG5
⟨C⟩⊤⇒
D:C∩D=∅(⟨D⟩φ ⇒⟨C⟩φ)
L(N)
PSPACE[21] P[22]
PrefG1
φ ⇒⟨≤i⟩φ
L(N)
PSPACE[23] P[22]
PrefG2
⟨≤i⟩⟨≤i⟩φ ⇒⟨≤i⟩φ
L(N)
PSPACE[21] P[22]
PrefG3 (p ∧Eq) ⇒(E(p ∧⟨≤i⟩q) ∨E(q ∧⟨≤i⟩p))
L(N, E)
EXPTIME[31] P[28]
PrefG4
Conjunction of the 3 previous axioms
L(N, E)
EXPTIME[32] P[28]
PrefG5
see below
L(N)
PSPACE[23] P[22]
PrefG6

i∈N(@j⟨≤i⟩k ∨@kj ∨@k⟨≤i⟩j)
L(N, @, i) PSPACE[33] P[25]
PrefG7
PrefG5 ∧PrefG6 ∧(
i∈N(j ⇒¬⟨≤i⟩j)) L(N, @, i) PSPACE[33] P[25]
PrefG8

i∈N ((⟨≤i⟩φ ⇒[≤i]φ) ∧⟨≤i⟩⊤)
L(N)
PSPACE[23] P[22]
PrefG9
⟨C⟩j ↔
i∈C⟨≤i⟩j
L(N, i)
PSPACE[33] P[25]
PrefG10
⟨C⟩p ↔
i∈C⟨≤i⟩p
L(N)
PSPACE[21] P[22]
PrefG11
⟨C⟩j ↔
D⊆C&|D|> |C|
2 (
i∈D⟨≤i⟩j)
L(N, i)
PSPACE[33] P[25]
PPG1
⟨C⟩φ ⇒
i∈N⟨≤i⟩φ
L(N)
PSPACE[23] P[22]
PPG2

i∈N A 
C⊆N(⟨C⟩φ ⇒⟨≤i⟩φ)
L(N, E)
EXPTIME[31] P[28]
PPG3

i∈N

C⊆N(≤i∪≤i)⟨≤i ∩C⟩⊤
L( , ∩, ∪) EXPTIME[27] P[28]
PPG4
see below
L(N, i)
PSPACE[33] P[25]
PPG5

C̸⊇{i}[
C→]⊥
L(N)
PSPACE[21] P[22]
PPG6
⟨C⟩φ ⇒
D⊂C⟨D⟩φ
L(N)
PSPACE[23] P[22]
PPG7

i∈N E 
C̸⊇{i}[C]⊥
L(N, E)
EXPTIME[33] P[25]
	
i∈N
(j ∧⟨≤i⟩(k ∧¬⟨≤i⟩j ∧⟨≤i⟩(l ∧¬⟨≤i⟩k))) ⇒j ∧⟨≤i⟩(l ∧¬⟨≤i⟩j) (AxPrefG5)
[p ∧⟨i⟩q ∧⟨≤i⟩(q ∧⟨≤i⟩¬p)] ⇒
	
i∈C⊆N
[(⟨C⟩r ∧
	
D⊆C\i
¬⟨D⟩r) ⇒⟨≤i⟩(r ∧¬⟨≤i⟩p)] (PPG4)

48
C. D´egremont and L. Kurzen
7
Conclusion
We identiﬁed a set of natural notions for reasoning about cooperation: local no-
tions giving properties of a state of a given system and global notions deﬁning a
class of frames. We provided satisﬁability (resp. validity) invariance results for
these notions for a large class of operations and relations between models (resp.
frames). We also gave explicit deﬁnability results and observed that deﬁning
frames for cooperation logics does not seem too demanding in terms of expres-
sive power, as most of the notions considered are deﬁnable in the basic modal
language. On the other hand, our results show that local notions call for modal
logics for which satisfaction is not invariant under bounded morphisms. However,
as long as we avoid converse modalities, interesting reasoning about cooperation
can be done within GSM-invariant modal languages. Though this fact does not
directly lead to a nice upper bound on the complexity of the logic’s SAT (nor
to its decidability), our deﬁnability results show that most of the considered no-
tions can (individually) be expressed in MLs in EXPTIME. Moreover, for several
notions we only found logics with undecidable SAT that could express them.
All these notions involve the idea of a “strict” improvement (e.g. Nash-stable,
Pareto-eﬃcient). By contrast, strong notions of stability and eﬃciency (EF3,
ST2, ST4) are all expressible in logics with SAT in PSPACE. Thus, we could say
that “expressing strictness” and therefore “weak” notions are dangerous, while
“strong” notions (looking only at the weak preference relation) are safe.
Based on our current work, the following lines seem worth exploring:
– Since dealing with real coalitional powers is probably more natural using
neighborhood semantics, it will be useful to do the same work for modal
logics of the CL-type or of the type of one of its normal simulations [2].
– It would be interesting to obtain similar invariance results and upper bounds
on the complexity of the logics needed to encode concrete arguments from
SCT and (cooperative) GT, thus addressing the complexity of actual rea-
soning about cooperative situations.
– From our deﬁnability results we could obtain upper bounds (on SAT) and on
the combined complexity of model checking of logics able to express certain
notions from SCT and GT. The converse road would be to use complexity
results from computational social choice and algorithmic game theory to
obtain lower bounds on its data complexity.4 As an example: a way to go
could be to take a hardness result for the problem of determining whether a
proﬁle of strategies is a pure Nash-equilibrium of given game (with respect to
some reasonable and qualitative encoding of games) as a lower bound on the
data complexity of model-checking of a logic than can express this notion.
– In order to obtain a complete picture of the complexity of reasoning about
cooperation, we need a procedure to assess the LB of the complexity of modal
logics that can express some notion.
4 When measuring combined complexity both the formula and the model are part
of the input. While when measuring data complexity, the formula is ﬁxed and the
model is the input (see [34]).

Modal Logics for Preferences and Cooperation
49
– Our deﬁnability results made use of very big conjunctions and disjunctions.
It would be interesting to check how the length of these formulas is related
to a more reasonable input such as the number of agents. (When taking con-
junction/disjunctions over all coalitions, they will be exponentially related.)
– We could also consider the complexity eﬀects of using more succinct lan-
guages that have more modalities, e.g. a modality ⟨Most ≤⟩φ, read: “there
is a φ-state that a majority of agents ﬁnds at least as good as the current
one” (cf. e.g. [35]).
Acknowledgments. The authors are very grateful to Johan van Benthem,
Natasha Alechina, Jakub Szymanik, Eric Pacuit, Peter van Emde Boas, Balder
ten Cate, Ulle Endriss and the anonymous referees for their useful comments.
References
1. Pauly, M.: A modal logic for coalitional power in games. JLC 12(1), 149–166 (2002)
2. Broersen, J., Herzig, A., Troquard, N.: Normal Coalition Logic and its conformant
extension. In: Samet, D. (ed.) TARK 2007, PUL, pp. 91–101 (2007)
3. Girard, P.: Modal Logic for Preference Change. PhD thesis, Stanford (2008)
4. Kurzen, L.: Logics for Cooperation, Actions and Preferences. Master’s thesis, Uni-
versiteit van Amsterdam, The Netherlands (2007)
5. ˚Agotnes, T., Dunne, P.E., van der Hoek, W., Wooldridge, M.: Logics for coalitional
games. In: LORI 2007, Beijing, China (2007) (to appear)
6. Alur, R., Henzinger, T., Kupferman, O.: Alternating-time temporal logic. In: Pro-
ceedings of the 38th IEEE Symposium on Foundations of Computer Science,
Florida (October 1997)
7. Goranko, V.: Coalition games and alternating temporal logics. In: TARK 2001, pp.
259–272. Morgan Kaufmann, San Francisco (2001)
8. Hansen, H.H., Kupke, C., Pacuit, E.: Bisimulation for neighbourhood structures.
In: Mossakowski, T., Montanari, U., Haveraaen, M. (eds.) CALCO 2007. LNCS,
vol. 4624, pp. 279–293. Springer, Heidelberg (2007)
9. de Jongh, D., Liu, F.: Optimality, belief and preference. In: Artemov, S., Parikh,
R. (eds.) Proc. of the Workshop on Rationality and Knowledge, ESSLLI (2006)
10. Osborne, M.J., Rubinstein, A.: A course in game theory. MIT Press, Cambridge
(1994)
11. Cate, B.: Model theory for extended modal languages. PhD thesis, University of
Amsterdam, ILLC Dissertation Series DS-2005-01 (2005)
12. Blackburn, P., de Rijke, M., Venema, Y.: Modal Logic. Cambridge Tracts in The-
oretical Computer Science, vol. 53. Cambridge University Press, UK (2001)
13. van Benthem, J.: Modal Logic and Classical Logic. Bibliopolis, Napoli (1983)
14. Areces, C., Blackburn, P., Marx, M.: Hybrid logics: characterization, interpolation
and complexity. The Journal of Symbolic Logic 66(3), 977–1010 (2001)
15. Feferman, S.: Persistent and invariant formulas for outer extensions. Compositio
Mathematica 20, 29–52 (1969)
16. Goldblatt, R.I., Thomason, S.K.: Axiomatic classes in propositional modal logic.
In: Crossley, J.N. (ed.) Algebra and Logic: Papers 14th Summer Research Inst. of
the Australian Math. Soc. LNM, vol. 450, pp. 163–173. Springer, Berlin (1975)

50
C. D´egremont and L. Kurzen
17. D´egremont, C., Kurzen, L.: Modal logics for preferences and cooperation: Ex-
pressivity and complexity. ILLC PP-2008-39, University of Amsterdam (2008),
http://staff.science.uva.nl/~cdegremo/
18. Papadimitriou, C.M.: Computational complexity. Addison-Wesley, MA (1994)
19. Odifreddi, P.: Classical Recursion Theory. Studies in Logic and the Foundations of
Mathematics, vol. (125). North-Holland, Amsterdam (1989)
20. Harel, D.: Recurring dominoes: making the highly undecidable highly understand-
able. In: Topics in the theory of computation, New York, NY, USA, pp. 51–71.
Elsevier, Amsterdam (1985)
21. Ladner, R.E.: The computational complexity of provability in systems of modal
propositional logic. SIAM J. Comput. 6(3), 467–480 (1977)
22. Fischer, M.J., Ladner, R.E.: Propositional dynamic logic of regular programs. J.
Comput. Syst. Sci. (1979)
23. Halpern, J.Y., Moses, Y.: A guide to completeness and complexity for modal logics
of knowledge and belief. Artiﬁcial Intelligence 54(3), 319–379 (1992)
24. ten Cate, B., Franceschet, M.: On the complexity of hybrid logics with binders.
In: Ong, L. (ed.) CSL 2005. LNCS, vol. 3634, pp. 339–354. Springer, Heidelberg
(2005)
25. Franceschet, M., de Rijke, M.: Model checking for hybrid logics. In: Proceedings of
the Workshop Methods for Modalities (2003)
26. Gurevich, Y.: The classical decision problem. In: Perspectives in Mathematical
Logic. Springer, Heidelberg (1997)
27. Lutz, C., Sattler, U.: The complexity of reasoning with boolean modal logics. In:
Wolter, W., de Rijke, Z. (eds.) AiML, WS, pp. 329–348 (2000)
28. Lange, M.: Model checking pdl with all extras. J. Applied Logic 4(1), 39–49 (2006)
29. Franceschet, M., de Rijke, M.: Model checking hybrid logics (with an application
to semistructured data). J. Applied Logic 4(3), 279–304 (2006)
30. Donini, F.M., Lenzerini, M., Nardi, D., Nutt, W.: The complexity of concept lan-
guages. In: KR, pp. 151–162 (1991)
31. Spaan, E.: Complexity of modal logics. PhD thesis, ILLC Amsterdam (1993)
32. Hemaspaandra, E.: The price of universality. NDJFL 37(2), 174–203 (1996)
33. Areces, C., Blackburn, P., Marx, M.: A road-map on complexity for hybrid logics.
In: Flum, J., Rodr´ıguez-Artalejo, M. (eds.) CSL 1999. LNCS, vol. 1683, pp. 307–
321. Springer, Heidelberg (1999)
34. Vardi, M.Y.: The complexity of relational query languages (extended abstract). In:
STOC 1982: Proceedings of the fourteenth annual ACM symposium on Theory of
computing, pp. 137–146. ACM, New York (1982)
35. ˚Agotnes, T., van der Hoek, W., Wooldridge, M.: Quantiﬁed coalition logic. In:
Veloso, M.M. (ed.) Proceedings of the Twentieth International Joint Conference
on Artiﬁcial Intelligence (IJCAI 2007), California, pp. 1181–1186. AAAI Press,
Menlo Park (2007)

Simulation and Information: Quantifying over
Epistemic Events
Hans van Ditmarsch1,⋆and Tim French2
1 Philosophy, University of Sevilla, Spain
hvd@us.es
2 Computer Science and Software Engineering,
University of Western Australia, Australia
tim@csse.uwa.edu.au
Abstract. We introduce a multi-agent logic of knowledge with time
where Fϕ stands for “there is an informative event after which ϕ.” For-
mula Fϕ is true in a model iﬀit is true in all its reﬁnements (i.e., atoms
and back are satisﬁed; the dual of simulation). The logic is almost normal,
and positive knowledge is preserved. The meaning of Fϕ is also “after
the agents become aware of new factual information, ϕ is true,” and on
ﬁnite models it is also “there is an event model (M, s) after which ϕ.”
The former provides a correspondence with bisimulation quantiﬁers in a
setting with epistemic operators.
Keywords: Bisimulation quantiﬁer, modal logic, temporal epistemic
logic, multi-agent system.
1
Introduction
If you know where you are and you know what’s going to happen, you want to
know where you will end up. But it can also be that you know where you are
and know where you would like to end up, and that you want to know how to
make that happen. Or you might want to know where you can end up in the ﬁrst
place, disregarding how that may be brought about. In the setting of logics for
information update [1,2,3], knowledge of where you are and where you end up
is formalized in multi-agent epistemic logic and semantically represented by a
pointed multi-agent Kripke model, and knowledge about what’s going to happen
is formalized as a dynamic modal operation that is interpreted as a relation
between such Kripke models. The standard focus in dynamic epistemic logic
was on the ﬁrst of the three issues above: precision about a speciﬁc information
update and precision about the eﬀects of that update. In this contribution we
⋆The authors gratefully acknowledge the suggestions and insights from the anonymous
LOFT 2008 referees and from the anonymous KRAMAS 2008 referees. Hans van
Ditmarsch acknowledges support of the Netherlands Institute of Advanced Study
where he was Lorentz Fellow in 2008. Hans van Ditmarsch is also aﬃliated to the
University of Otago. Part of this work was carried out while aﬃliated to CNRS-IRIT,
Universit´e de Toulouse, France.
J.-J.Ch. Meyer and J.M. Broersen (Eds.): KRAMAS 2008, LNAI 5605, pp. 51–65, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

52
H. van Ditmarsch and T. French
focus on the other two issues instead. As this is partly about what may happen
after any event, this concerns quantiﬁcation over events. Our work is a further
generalization of works such as [4],[5] (and its ﬁnal journal version [6])and our
presentation of future event operators as temporal is motivated by works such
as [7] linking temporal epistemic logic to dynamic epistemic logic.
We introduce a very succinct logic of future events: the multi-agent logic of
knowledge with (only) an operation Gϕ that stands for “ϕ holds after all infor-
mative events” — the diamond version Fϕ stands for “there is an informative
event after which ϕ.”
Previous works [8,9] have modelled informative events using a notion of model
reﬁnement. In [9] it was shown that model restrictions were not suﬃcient to simu-
late informative events, and they introduced reﬁnement trees for this purpose—a
precursor of the semantics of dynamic epistemic logics developed later [2]. We
incorporate implicit quantiﬁcation over informative events directly into the lan-
guage using a similar notion of reﬁnement; in our case a reﬁnement is the inverse
of simulation [10].
We demonstrate that this is useful notion for informative event by a number
of technical results for this logic—the logic is almost normal, positive knowledge
is preserved—and by a number of equivalence results for alternative semantics:
Fϕ also means “there is an event model (M, s) after which ϕ,” and it also means
“after the agents become aware of new factual information, ϕ is true.” The last
provides a correspondence with bisimulation quantiﬁers [11,12] in a setting with
epistemic operators, as in [13].
2
Technical Preliminaries
Structural notions. Assume a ﬁnite set of agents A and a countably inﬁnite set
of atoms P.
Deﬁnition 1 (Structures). An epistemic model M = (S, R, V ) consists of a
domain S of (factual) states (or worlds), accessibility R : A →P(S × S), and
a valuation V : P →P(S). For s ∈S, (M, s) is an epistemic state (also known
as a pointed Kripke model).
For R(a) we write Ra; accessibility R can be seen as a set of relations Ra, and V
as a set of valuations V (p). Given two states s, s′ in the domain, Ra(s, s′) means
that in state s agent a considers s′ a possibility. We adopt the standard rules for
omission of parentheses in formulas, and we also delete them in representations
of structures such as (M, s) whenever convenient and unambiguous.
Deﬁnition 2 (Bisimulation, simulation, reﬁnement). Let two models M =
(S, R, V ) and M ′ = (S′, R′, V ′) be given. A non-empty relation R ⊆S × S′ is a
bisimulation, iﬀfor all s ∈S and s′ ∈S′ with (s, s′) ∈R:
atoms s ∈V (p) iﬀs′ ∈V ′(p) for all p ∈P
forth for all a ∈A and all t ∈S, if Ra(s, t), then there is a t′ ∈S′ such that
Ra(s′, t′) and (t, t′) ∈R

Simulation and Information: Quantifying over Epistemic Events
53
back for all a ∈A and all t′ ∈S′, if Ra(s′, t′), then there is a t ∈S such that
Ra(s, t) and (t, t′) ∈R
We write (M, s)↔(M ′, s′), iﬀthere is a bisimulation between M and M ′ linking
s and s′. Then we call (M, s) and (M ′, s′) bisimilar. We also say that (M, s) is
similar to (M ′, s′) and vice versa.
A relation that satisﬁes atoms and forth is a simulation, and in that case
(M ′, s′) is a simulation of (M, s), and (M, s) is a reﬁnement of (M ′, s), and we
write (M, s)→(M ′, s′) (or (M ′, s′)←(M, s)).
A bisimulation (simulation) that satisﬁes atoms for a subset P ′ ⊆P is a P ′-
bisimulation (P ′-simulation); we write (M, s)↔P ′(M ′, s′) ((M, s)→P ′(M ′, s′)),
etc.
Standard language notions. The languages of propositional logic (Lpl) and of
epistemic logic (Lel) — a ∈A, p ∈P, B ⊆A.
Lpl :
ϕ ::= p | ¬ϕ | (ϕ ∧ϕ)
Lel :
ϕ ::= p | ¬ϕ | (ϕ ∧ϕ) | Kaϕ
Standard abbreviations include: ϕ ∨ψ iﬀ¬(¬ϕ ∧¬ψ); ϕ →ψ iﬀ¬ϕ ∨ψ, ˆKaϕ
iﬀ¬Ka¬ϕ.
Event model logic. All the following are simultaneously deﬁned:
Deﬁnition 3. Language Leml of event model logic:
ϕ ::= p | ¬ϕ | ϕ ∧ϕ | Kaϕ | [M, s]ϕ
Deﬁnition 4 (Event model). An event model for a ﬁnite set of agents A and
a language L is a triple M = (S, R, pre) where
– domain S is a ﬁnite non-empty set of events,
– R : A →P(S × S) assigns an accessibility relation to each agent,
– pre : S →L assigns to each event a precondition,
A pair (M, s) with a distinguished actual event s ∈S is called an epistemic event.
An epistemic event with a singleton domain, accessible to all agents, and identity
postcondition, is a public announcement.
Deﬁnition 5 (Semantics of event model logic). Let a model (M, s) with
M = (S, R, V ) be given. Let a ∈A, B ⊆A, and ϕ, ψ ∈L.
(M, s) |= [M, s]ϕ iﬀ(M, s) |= pre(s) implies (M ⊗M, (s, s)) |= ϕ
Deﬁnition 6 (Execution of an event model). Given are an epistemic model
M = (S, R, V ), a state s ∈S, an event model M = (S, R, pre), and an event
s ∈S with (M, s) |= pre(s). The result of executing (M, s) in (M, s) is the model
(M ⊗M, (s, s)) = ((S′, R′, V ′), (s, s)) where
– S′ = {(t, t) | (M, t) |= pre(t)},
– R′(a) = {((t, t), (u, u)) | (t, t), (u, u) ∈S′ and (t, u) ∈R(a) and (t, u) ∈
R(a)},
– V ′(p) = {(t, t) | (M, t) |= p}.

54
H. van Ditmarsch and T. French
Bisimulation quantiﬁers and bisimulation quantiﬁed epistemic logic. The lan-
guage and semantics are as follows.
Deﬁnition 7 (Bisimulation quantiﬁed epistemic logic). Bisimulation
quantiﬁed epistemic logic augments epistemic logic by additionally allowing for-
mulas of the kind ∀pϕ in the recursive deﬁnition, where p is an atom of P, and
ϕ is a formula. This is the language Lbqel.
Given an epistemic model M = (S, R, V ) and a state s ∈S we say:
M, s |= ∀pϕ iﬀ
for every epistemic model (M ′, s′)↔P \{p}(M, s) : M ′, s′ |= ϕ.
3
Simulation and Information
In future event logic one can express what informative events can be expected in
a given information state. The language and the semantics of future event logic
are as follows.
Deﬁnition 8 (Language Lfel). Given agents A and atoms P, the language
Lfel is inductively deﬁned as
ϕ ::= p | ¬ϕ | (ϕ ∧ϕ) | Kaϕ | Gϕ
where a ∈A and p ∈P.
We write Fϕ for ¬G¬ϕ. We propose a dynamic epistemic modal way to interpret
temporal operators. This means that our future is the computable future: Fϕ is
true now, iﬀthere is an (unspeciﬁed) informative event after which ϕ is true.
In the semantics for Gϕ, now to follow, we use the structural notion of re-
ﬁnement, see Deﬁnition 2. From the requirements for bisimulation, a reﬁnement
satisﬁes atoms and back. Reﬁnement is the dual of simulation, that satisﬁes
atoms and forth. If (M ′, s′) is a reﬁnement of (M, s) we write (M, s)←(M ′, s′).
Deﬁnition 9 (Semantics of future event logic). Assume an epistemic
model M = (S, R, V ). The interpretation of ϕ ∈Lfel is deﬁned by induction.
M, s |= p
iﬀs ∈Vp
M, s |= ¬ϕ
iﬀM, s ̸|= ϕ
M, s |= ϕ ∧ψ iﬀM, s |= ϕ and M, s |= ψ
M, s |= Kaϕ iﬀfor all t ∈S : (s, t) ∈Ra implies M, t |= ϕ
M, s |= Gϕ
iﬀfor all (M ′, s′) : (M, s)←(M ′, s′) implies M ′, s′ |= ϕ
In other words, Gϕ is true in an epistemic state iﬀϕ is true in all of its re-
ﬁnements. Note the wrong direction in the deﬁnition: the future epistemic state
simulates the current epistemic state. Typical model operations that produce
a reﬁnement are: blowing up the model (to a bisimilar model) such as adding
copies that are indistinguishable from the current model and one another for
some agent(s), removing states, and removing pairs of the accessibility rela-
tion for an agent. Validity in a model, and validity, are deﬁned as usual. For
{s | M, s |= ϕ} we write [[ϕ]]M.

Simulation and Information: Quantifying over Epistemic Events
55
Example 1. Given are two agents that are uncertain about the value of a fact p,
and where this is common knowledge, and where p is true. We assume that both
accessibility relations are equivalence relations, and that the epistemic operators
model the agents’ knowledge. An informative event is possible after which a
knows that p but b does not know that: M, 1 |= F(Kap ∧¬KbKap).
In the ﬁgure, (M, 1) is the structure on the left, and its reﬁnement validating
the postcondition is on the right. In this visualization, the actual state is un-
derlined, states are suggestively named with the value of p (so on the right, the
two diﬀerent states where p is true are both named 1), states that are indistin-
guishable for an agent are linked and labeled with the name of that agent, and
transitivity is assumed (so on the right, all three states are indistinguishable for
agent b). Note that on the left, the formula F(Kap ∧¬KbKap) is true, because
on the right Kap ∧¬KbKap is true: in the underlined actual state there is no
alternative for agent a, so Kap is true, whereas agent b considers it possible that
the top-state also named 1 is the actual state, and in that state agent a consid-
ers it possible that p is false. Therefore, ¬KbKap is also true in the underlined
1-state.
0
1
ab
⇒
1
0
1
ab
b
Proposition 1. Some elementary validities are:
1. |= G(ϕ →ψ) ↔(Gϕ →Gψ)
2. |= Gϕ →ϕ
3. |= Gϕ →GGϕ
4. |= ϕ implies |= Gϕ
5. |= KaGϕ →GKaϕ
Proof.
1. Obvious.
2. A model is a reﬁnement of itself; this corresponds to the trivial event ‘an-
nounce true’.
3. Consider the diamond version FFϕ →Fϕ. The relational composition of
two simulations is again a simulation.
4. Obvious.
5. Consider the diamond version. Choose an accessible state in a reﬁnement
of a model. By back, this accessibility step can also be taken in the initial
model.
Proposition 1 makes clear that G comes close to being a normal modal operator.
But it is not a normal modal logic: the validities of the logic are not closed under
uniform substitution of atomic variables for other formulas. For example, given
some atom p, p →Gp is valid, but ¬Kp →G¬Kp is not valid. A countermodel

56
H. van Ditmarsch and T. French
of the latter is the typical two-state situation where there is uncertainty about
the value of p and where p is true. In that case, restriction to the p-state (public
announcement of p) makes it known. Another countermodel is provided by the
example above, for the knowledge of agent b.
A standard check for our bold claim that G formalizes a notion of informative
event is that
Proposition 2. Bisimilar epistemic states have the same logical theory.
Proof. This is not completely trivial, because bisimilarity is with respect to
the epistemic operators, whereas the same logical theory is with respect to the
epistemic operators and the temporal operator. Both can be established easily
by the observation that if an epistemic state is a reﬁnement of one of two given
bisimilar epistemic states, it is also a reﬁnement of the other epistemic state,
because the relational composition of a simulation relation and a bisimulation
relation is a simulation relation. The inductive case Gϕ of the proof is:
Assume R : (M, s)↔(M ′, s′), and let M, s |= Gϕ. To show that M ′, s′ |= Gϕ,
let (M ′′, s′′) be such that R′ : (M ′′, s′′)→(M ′, s′). We now have that R′ ◦R−1 :
(M ′′, s′′)→(M, s). From that and M, s |= Gϕ follows M ′′, s′′ |= ϕ.
The positive formulas are those in the inductively deﬁned fragment
ϕ ::= p|¬p|ϕ ∨ϕ|ϕ ∧ϕ|Kaϕ|Gϕ.
The preserved formulas are those for which
ϕ →Gϕ is valid.
I.e., they preserve truth under model reﬁnement as long as the reﬁnement in-
cludes an image for the actual state; the better known setting is model restriction.
The ﬁrst real corroboration that the temporal operators formalize a notion of
informative event is that they model growth of information in the sense that
positive knowledge does not get lost:
Proposition 3. Positive formulas preserve truth under reﬁnement of models.
Proof. Elementary.
Further corroboration that the temporal operators are quantifying over informa-
tive events is provided by the observation that a restricted modal product is a
reﬁnement of a model if the valuations of the states in that model are preserved
under the product operation. This entails that the execution of an event model
in an epistemic state is a reﬁnement of that epistemic state. This we will now
address.

Simulation and Information: Quantifying over Epistemic Events
57
4
Quantifying over Event Models
An informative update is the execution of an event model in an epistemic state.
We consider event models for the epistemic language Lel.
Proposition 4. An informative update is a reﬁnement.
Proof. Let (M, s) = ((S, R, pre), s) be an event model for language Lel. Let
(M, s) = ((S, R, V ), s) be an epistemic state and suppose M, s |= pre(s). Then
R(t, t) = t is a simulation between ((M ⊗M), (s, s)) and (M, s); below we assume
that (M ⊗M) = (S′, R′, V ′).
– atoms: if (t, t) ∈V ′(p) then t ∈V (p);
– forth: let ((t, t), (t′, t′)) ∈R′
a; then (t, t′) ∈Ra.
Subject to the restrictions that the epistemic models are ﬁnite, the ﬁt is exact:
reﬁnements are informative updates.
Proposition 5. (On ﬁnite epistemic models) A reﬁnement is an informative
update.
Proof. Given are a ﬁnite epistemic state ((S, R, V ), s) and a reﬁnement ((S′,
R′, V ′), s′) of that model (according to reﬁnement R). Consider the event model
that is isomorphic to that reﬁnement (according to isomorphism I). Instead of
valuations for states t, this event model has preconditions for events I(t). We
want the preconditions only to be satisﬁed in states s such that (s, t) ∈R—this
we cannot guarantee, but we can come close enough. In a ﬁnite model, every
state s can be distinguished from all other (except bisimilar) states by a formula
ϕs. This is shown in Lemma 1 below. These distinguishing formulas can now be
used to have event I(t) of the event model executable in every state s ∈S such
that (s, t) ∈R, by requiring that
pre(I(t)) =

(s,t)∈R
ϕs
This may give us pairs (s, I(t)) with (s, t) ̸∈R, but in that case s will be
bisimilar to some s′ satisfying the same distinguishing formula and such that
(s′, t) ∈R. Of course, the composition of the total bisimulation on (S, R, V )
with the reﬁnement relation R will also be a reﬁnement relation. Without loss
of generality we assume that R is maximal in the sense that it is a ﬁxed-point of
composition with that total bisimulation. This makes the structure of the proof
clearer.
We now show that the restricted modal product ((S′′, R′′, V ′′), (s, s′)) result-
ing from executing the event model ((S′, R′, pre), s′) in the given epistemic state
(S, R, V ), s) is bisimilar to its reﬁnement ((S′, R′, V ′), s′). The bisimulation R′
is as follows: all pairs (t, I(u)) in the restricted modal product are bisimilar to
the state u ∈S′ of which their second argument of the pair is the isomorphic
image:
R′(t, I(u)) = u

58
H. van Ditmarsch and T. French
Condition atoms is obvious, as reﬁnement satisﬁes atoms. Condition forth
is also obvious: if ((t, I(u)), (t′, I(u′))) ∈R′′
a, then by deﬁnition of the modal
product (I(u), I(u′)) ∈R′
a, so (u, u′) ∈R′
a (and, indeed, ((t′, I(u′)), u′) ∈R′
by deﬁnition). Condition back is not obvious but also holds. Let (u, u′) ∈R′
a
and ((t, I(u)), u) ∈R′. There must be a t′ ∈S (modulo bisimilarity) such that
(t′, u′) ∈R so that (t′, I(u′)) is in the modal product. We now have that from
(u, u′) ∈R′
a follows (I(u), I(u′)) ∈R′
a, and we also have that from (u, u′) ∈R′
a
follows (t, t′) ∈Ra (as R is a reﬁnement). From (t, t′) ∈Ra and (I(u), I(u′)) ∈
R′
a follows by deﬁnition the requested (t, I(u)), (t′, I(u′)) ∈R′′
a.
Lemma 1. For every ﬁnite multi-agent epistemic model M = (S, R, V ), for
every state t ∈S there exists a formula ϕt such that for all u ∈S, (M, u) |= ϕt
if and only if (M, u) is bisimilar to (M, t).
Proof. To show this it is suﬃcient to show that the relation
R = {(v, w)| (M, v) and (M, w) agree on the interpretation of all Lel formulae},
is a bisimulation. Clearly it satisﬁes atoms. To show that it satisﬁes forth, sup-
pose (v, w) ∈R and let a ≤m, and let v∗be an a-successor of the state v. We
note that the state w has ﬁnitely many a-successors, w1, . . . , wk. Suppose for con-
tradiction that none of these states were related to v∗by R. Thus for each such
state wa, there is a formula τa such that (M, v∗) ̸|= τa and (M, w) |= τa. However
then we have that (M, v) |= ¬Ka
k
a=1 τa

, but (M, w) |= Ka
k
a=1 τa

, con-
tradicting the fact that (v, w) ∈R. Therefore, for every a ≤m, every a successor
of v is related by R to some a-successor of w. The property back can be shown
symmetrically, so R is a bisimulation.
This establishes that for every u ∈S where (M, u) ̸ ↔(M, t), there is some
formula δu
t such that (M, t) |= δu
t and (M, u) ̸|= δu
t . We deﬁne ϕt = {δu
t | u ∈
S and (M, u) ̸ ↔(M, t)}. Since bisimilar states agree on the interpretation of all
Lel formulas, for all u ∈S, (M, u) |= ϕt if and only if (M, u)↔(M, t).
We emphasize that the notion of event model relative to a language allows for
inﬁnite event models, unlike in a logic with an inductively deﬁned language
including (ﬁnite!) event models. That is to come next. This will also allow us to
compare our proposal with a known method [5] for quantifying over events.
Deﬁnition 10 ([5]). The language Laeml of arbitrary event model logic is the
language Lfel of future event logic with an additional inductive construct [M, s]ϕ.
We can view [M, s]ϕ as an inductive construct, because, given the (enumerable)
set of event model frames, [M, s] can be seen as an operation on |D(M)| ar-
guments of type formula (similar to automata-PDL). These arguments are the
preconditions of the events in the event model. The language Laeml can also be
seen as extension with construct Gϕ of the language Leml for event model logic.
To distinguish future event logic from logics with the same language but
other semantics for Gϕ, we also write |=←instead of |= for the forcing relation

Simulation and Information: Quantifying over Epistemic Events
59
in future event logic; we (always) write |=⊗for the forcing relation in arbitrary
event model logic.
For the semantics of Gϕ in terms of event models we need to restrict the
preconditions of their events to G-free formulas, i.e. Leml formulas. This is to
avoid circularity in the deﬁnition, as Gϕ could itself be a precondition of such
an event. An event model is G-free iﬀall preconditions of its events are G-free.
Deﬁnition 11 (Semantics of arbitrary event model logic). Where the
preconditions of events in any M are G-free.
M, s |=⊗Gϕ iﬀfor all G-free (M, s) : M, s |=⊗[M, s]ϕ
There are reﬁnements of epistemic models that cannot be seen as the result of
executing an event model. This is because event models (in the language) have
by deﬁnition a ﬁnite domain. For example, given a ﬁnite epistemic model (M, s),
consider its unwinding as an inﬁnite tree (representing the bisimulation class).
This is a reﬁnement of (M, s). But the result of executing a ﬁnite event model
in a ﬁnite epistemic model cannot be an inﬁnite tree. Of course, that tree is
bisimilar to the initial epistemic state so can be seen in another sense as the
result of execution the trivial event. But:
Because of the restriction to G-free preconditions in event models, we will
still not get precise correspondence between the two semantics. The crux is
that there are more epistemic distinctions in models than can be enumerated
by epistemic formulas, see [5] for a similar matter. (However, we do not have a
counterexample.)
Restricted to the class of ﬁnite epistemic models we still have that:
Proposition 6. Let M be ﬁnite. Then: M, s |=↔ϕ iﬀM, s |=⊗ϕ.
Proof. Directly from Propositions 5 and 4.
5
Bisimulation and Information
Instead of validating Fϕ in some (M, s) by ﬁnding a reﬁnement of (M, s), we can
equivalently ﬁnd a model restriction of a bisimilar epistemic state. This alter-
native semantics |=↔is interesting because of a relationship with bisimulation
quantiﬁers [11], for which many theoretical results are known; and it is also in-
teresting because it shows that every informative update is equivalent to public
announcement of factual information “of which the agents may not have been
aware.”
Deﬁnition 12. Below, S′ is the domain of M ′, and S′′ is such that s ∈S′′:
M, s |=↔Gϕ iﬀfor all (M ′, s′)↔(M, s) and for all S′′ ⊆S′ : M ′|S′′, s′ |=↔ϕ
On ﬁrst thought it might seem that there are more reﬁnements of a given model
than domain restrictions of bisimilar models. In a reﬁnement we can both restrict
the domain (remove states) and remove links between states (delete pairs of the
accessibility relation for an agent). But removing links between states can also
be seen as a domain restriction on a even larger bisimilar model.

60
H. van Ditmarsch and T. French
Proposition 7. M, s |=←ϕ iﬀM, s |=↔ϕ.
Proof. This can be shown by induction on the complexity of formulas. As |=←
and |=↔agree on the interpretations of atoms and all operators except G, it is
suﬃcient to show that given (M, s |=←ϕ iﬀM, s |=↔ϕ) we have (M, s |=←Gϕ
iﬀM, s |=↔Gϕ). From left to right the latter is trivial, because the reﬁnements
of (M, s) include the bisimulations of (M, s). For the direction from right to left,
it suﬃces to show that any reﬁnement (M ′, s′) of model (M, s) is the restriction
of a model (M ′′, s′′) that is bisimilar to (M, s). This model is constructed as
follows:
Let M = (S, R, V ), M ′ = (S′, R′, V ′), and suppose that the reﬁnement rela-
tion is R. Consider (M ′′, s′′) = ((S ⊕S′, R′′, V ′′), (s′, 1)), where for all agents
a ∈A
((s, 0), (t, 0)) ∈R′′
a
iﬀ(s, t) ∈Ra
((s′, 1), (t′, 1)) ∈R′′
a iﬀ(s′, t′) ∈R′
a
((s′, 1), (t, 0)) ∈R′′
a iﬀ∃s ∈S : (s, s′) ∈R and (s, t) ∈Ra
We can then deﬁne the relation R′ between (M, s) and (M ′′, (s, 0)) as follows:
(s, (s′, 1)) ∈R′ iﬀ(s, s′) ∈R
(s, (s, 0)) ∈R′ iﬀs ∈S
This relation R′ is a bisimulation: it still satisﬁes back since the states of S
added to M ′ also satisfy back: any relation between them copied their relation
in the original M. But it now also satisﬁes forth:
If (s, (s, 0)) ∈R′ and (s, t) ∈Ra then by deﬁnition of the ﬁrst clause of R′ we
have (t, (t, 0)) ∈R′ and, trivially by the deﬁnition of R′′
a we have ((s, 0), (t, 0)) ∈
R′′
a. If (s, (s′, 1)) ∈R′ and (s, t) ∈Ra then we have (as before) (t, (t, 0)) ∈R′ and
((s′, 1), (t, 0)) ∈R′′
a. The latter holds because of the third clause in the deﬁnition
of R′′
a.
Since M ′′|(S′ × {1}) is isomorphic to M ′ this concludes the proof.
We proceed by explaining the stated relation of this semantics with bisimulation
quantiﬁers.
6
Bisimulation Quantiﬁers
Suppose that apart from the atoms in P we had an additional, reserved, atom r.
The future temporal operator can be seen as (existential) bisimulation quantiﬁca-
tion over r. This relation becomes clear if we consider the restricted bisimulation
version of the semantics for F:
First choose a bisimilar epistemic state, then do a model restriction in
that epistemic state that contains the actual state.
Given the class of models also valuing r we can replace this by
First choose a P-bisimilar epistemic state (but where the valuation of
r may vary wildly), then do a model restriction in that epistemic state
that contains the actual state.

Simulation and Information: Quantifying over Epistemic Events
61
Of course we can match the variation in the valuation of r, as long as it contains
the actual state, with that model restriction so we get
First choose a P-bisimilar epistemic state, then do a model restriction
to the r-states in that epistemic state, on condition that it contains the
actual state.
The part “choose a P-bisimilar epistemic state” of this informal description is
the semantics of a existential bisimulation quantiﬁcation.
Deﬁnition 13. Where V ′ is the valuation of M ′.
M, s |=∀r Gϕ iﬀfor all (M ′, s′)↔P (M, s) : s′ ∈V ′(r) implies M ′|r, s′ |=∀r ϕ
Example 2. For an example, consider again the model with common uncertainty
about the value of an atom p for agents a and b, where p is true. We now operate
on models that also value the atom r, in the ﬁgure this is the value of the second
digit: note that r is not part of the logical language! Given the bisimulation
quantiﬁcation, the initial value of r does not matter. In this model the formula
F(Kap ∧¬KaKbp) is true. The ﬁrst transition is to a model that is bisimilar
with respect to p only. The second transition is a restriction to the states where
r is true.
00
10
ab
⇒
00
11
01
11
ab
ab
b
b
⇒
11
01
11
ab
b
Proposition 8. M, s |=↔ϕ iﬀM, s |=∀r ϕ
Corollary 1. On ﬁnite models and given common knowledge in the language,
the four diﬀerent semantics for G correspond. (I.e. |=↔, |=←, |=⊗, and |=∀r.)
Note that the extra atom r does not disturb these results. As yet it is mere surplus
luggage that we’re carrying along towards the next section where it will become
more meaningful. Our fourth perspective of bisimulation quantiﬁer semantics is
useful for theoretical and for practical reasons. A theoretical consequence is that
Proposition 9. Future event logic is decidable.
Proof. Consider some ϕ ∈Lfel. Replace all occurrences of G in ϕ by ∀r[r]. It is
decidable whether ϕ∀r is satisﬁable. (The decidability of bisimulation quantiﬁed
modal logics can be generalized to multi-agent logics. Note that it also holds for
speciﬁc model classes such as KD45, S5 and the modal μ-calculus; see [14].)
This is a useful result. If we add dynamic event model operators to future event
logic (the language Laeml) we obtain arbitrary event model logic (see Deﬁnition

62
H. van Ditmarsch and T. French
10). The restriction of this arbitrary event model logic to events that are public
announcements is the logic APAL investigated in [5]. For that logic, the satisﬁa-
bility problem is undecidable (see [15]). That result also motivated this current
investigation, because it promised more decidable logics.
However, we may note that the translation given (replace all occurrences of
G in ϕ by ∀r[r]) is an accurate translation for all logics that are closed under
bisimulation quantiﬁers and announcement. From a recent result of van Benthem
and Ikegami [14] we know that the modal μ-calculus is also closed under products
with event models. Since future event logic and arbitrary event model logic agree
on the interpretation of Gϕ over ﬁnite models (Proposition 6), we can conclude
that the satisﬁability problem for Laeml restricted to ﬁnite models is reducible
to the satisﬁability problem for the μ-calculus, and hence decidable.
Note that the G-operator in arbitrary event model logic is interpreted diﬀer-
ently (see Deﬁnition 11), and it is unknown whether this logic is decidable.
Our current perspective also provides us with additional modelling insight,
namely that every informative update corresponds to the public announcement
of an atomic fact. Kind of. What kind of? So far, it is unclear how to interpret
this new perspective: we compare semantics with respect to model classes for
diﬀerent sets of atomic propositions; we did not add the fresh atom r to the
logical language Lfel. Here is where some trouble seems to start. If we merely add
r as a formula to the language, but, e.g., rule out Kar, we cannot truly interpret a
r-restriction of a model as a public announcement: what use is an announcement
of r if we cannot express that an agent a knows r after its announcement?
But if we add r as just another propositional variable to the base clause of
our inductive language deﬁnition, we run into trouble of a diﬀerent kind: an
existential bisimulation quantiﬁcation means that the value of p is scrambled.
Even with the restriction that the value of r remains unchanged in the actual
state, we may now still have that an agent a knew r before an event, but has
forgotten it afterwards, or vice versa. This is highly undesirable!
Example 3. In the previous example, we have that initially agent b knows that
r is false: Kb¬r, but after the update he apparently has forgotten that: ¬Kb¬r.
For another example: Kar →F¬Kar would be a validity.
A technical solution to this dilemma, that at least makes the public announce-
ment clear, is to
replace all occurrences of G in formulas by occurrences of ∀r[r],
where ∀r is universal bisimulation over r and where [r] stands for public an-
nouncement of r. Public announcement is a singleton event model, accessible to
all agents, where there precondition of the event is the formuma between brack-
ets, in this case: r. If we also allow r as formula, we can now interpret formulas
of form [r]ϕ in the usual sense for such events. For example, in our running
example it is initially true that ∃r⟨r⟩(Kap ∧¬KaKbp), as this is the translation
of F(Kap ∧¬KaKbp). (For ¬[ϕ]¬ψ we write ⟨ϕ⟩ψ.)
But the real solution to this seeming dilemma is to consider an existential
bisimulation as “the agents become aware of an additional fact,” about which

Simulation and Information: Quantifying over Epistemic Events
63
uncertainty is possible. From a modelling point of view this means that, before
the bisimulation operation, the value of r should be don’t care, in other words,
“the agents are unaware of r,” and the bisimulation quantiﬁcation itself then
means “the agents become aware of r.” This is now in the proper sense that we
move to a bisimilar model except for atom r, and (unlike before!) without the
restriction that r should remain true in the actual state, because maybe it was
false in the ﬁrst place. And after that it should be possible for them to know
that r, or know that ¬r: they are now aware of their uncertainty about r. Of
course after that, there might be other facts the agents might become aware of.
If we merely add r to the base of the inductive language deﬁnition we cannot
express this. We need one more step. That ﬁnal step we will now set in the next
section.
7
Becoming Aware of Factual Information
First, we add more structure: For each epistemic model M, the set of atoms P
is the disjoint union of a set of relevant facts Pr(M) and a set of irrelevant facts
Pi(M). The set of relevant facts is typically ﬁnite. Then, in a given model, the
interpretation of formulas containing irrelevant facts is undeﬁned, unless they
are bound by a bisimulation quantiﬁer: we can only interpret irrelevant facts
after they have become relevant to the agents. The bisimulation quantiﬁer makes
a fact relevant: its interpretation involves removing it from the set of irrelevant
facts and adding it to the set of relevant facts. The result of this is that the value
of irrelevant facts in any model is now truly don’t care from the perspective of
the agents. But they can still reason about the consequences of new facts after
they were to become relevant, i.e., after the agents were to become aware of
those facts.
Deﬁnition 14. The language Lqel of quantiﬁed event logic is inductively deﬁned
as
ϕ ::= p | ¬ϕ | (ϕ ∧ϕ) | Kaϕ | [M, s]ϕ | ∀pϕ
where a ∈A and p ∈P.
For the dual ∃pϕ, read, “(there exists a fact p such that) after the agents have
become aware of p, ϕ.” We emphasize that by “becoming aware of p” we do not
mean “learning that p is true”. In the information state resulting from becoming
aware of p, the agents may know that p is true, or that p is false, or have any
epistemic uncertainty about its value, e.g., they may not know whether p is true,
or one agent may know but not another, etc.
Deﬁnition 15 (Semantics). Assume an epistemic model M = (S, R, V ) for
atoms P = Pr(M) ∪Pi(M). The interpretation of ϕ ∈Lqel is deﬁned by induc-
tion. We only give the clauses for relevant atoms p and for ∀pϕ. The interpre-
tation of irrelevant atoms is undeﬁned. In the clause for ∀pϕ it is required that
(M ′, s′) is such that Pr(M ′) = Pr(M) + p and Pi(M ′) = Pi(M) −p.
M, s |= p
iﬀs ∈Vp
where p ∈Pr(M)
M, s |= ∀pϕ iﬀfor all (M ′, s′)↔P −p(M, s) : M ′, s′ |= ϕ
where p ∈Pi(M)

64
H. van Ditmarsch and T. French
We have not explored this version in greater detail yet. Unlike the logics with
temporal operators and the proposal with a reserved atom r for bisimulation
quantiﬁcation, agents may in this logic become aware of several diﬀerent facts.
Just as in Fagin and Halpern’s logic of awareness [16] agents’ awareness is about
atoms only (and not about complex formulas), unlike their approach our aware-
ness is global for all states and does not vary along diﬀerent states. The main
diﬀerence is of course that we propose a way to model becoming aware, unlike
the being aware in [16].
We think this logic may help modellers construct epistemic models in steps. In
this logic, if we say that agent a is uncertain about p, and we represent this in the
two-state epistemic model, this now means that the agent only is uncertain about
p. The value of other atoms in that epistemic state is don’t care: information on
an additional fact q might become available later, we then simply construct a p-
but-not-q bisimulation of this current epistemic state that represents the agents’
current knowledge, that includes q. This is exactly the ∃q-operation! We close
this section with a suitable illustration of this.
Example 4. Initially the agents are only uncertain about p. Then, they become
aware of q: in fact, a knows the value of q but b doesn’t. Finally, it is announced
that p∨q. In the resulting state, a knows that p but b does not know that. Initially,
the formula ∃r⟨p ∨q⟩(Kap ∧¬KbKap) is true. Observe that the bisimulation
quantiﬁcation is in this example diﬀerent from the subsequent announcement.
We can only achieve our ﬁnal information state by announcing a more complex
formula than an atom, and not by the announcement of an atom.
0
1
ab
⇒
00
10
01
11
ab
ab
b
b
⇒
10
01
11
ab
b
Further research. We are currently further exploring the modelling of awareness
using bisimulation quantiﬁcation, as in this section. We are also investigating the
axiomatization of the future event logic that forms the core of our contribution,
including its model checking complexity (relative to diﬀerent model classes, such
as S5), and expressivity issues.
References
1. Baltag, A., Moss, L.: Logics for epistemic programs. Synthese 139, 165–224 (2004);
Knowledge, Rationality & Action 1–60
2. van Ditmarsch, H., van der Hoek, W., Kooi, B.: Dynamic Epistemic Logic. Synthese
Library, vol. 337. Springer, Heidelberg (2007)
3. van Benthem, J., van Eijck, J., Kooi, B.: Logics of communication and change.
Information and Computation 204(11), 1620–1662 (2006)

Simulation and Information: Quantifying over Epistemic Events
65
4. Hoshi, T.: The logic of communication graphs for group communication and the
dynamic epistemic logic with a future operator. Philosophy Department, Stanford
University (2006)
5. Balbiani, P., Baltag, A., van Ditmarsch, H., Herzig, A., Hoshi, T., Lima, T.D.:
What can we achieve by arbitrary announcements? A dynamic take on Fitch’s
knowability. In: Samet, D. (ed.) Proceedings of TARK XI, Louvain-la-Neuve, Bel-
gium, pp. 42–51. Presses Universitaires de Louvain (2007)
6. Balbiani, P., Baltag, A., van Ditmarsch, H., Herzig, A., Hoshi, T., Lima, T.D.:
‘knowable’ as ‘known after an announcement’. Review of Symbolic Logic 1(3),
305–334 (2008)
7. van Benthem, J., Gerbrandy, J., Pacuit, E.: Merging frameworks for interaction:
DEL and ETL. In: Samet, D. (ed.) Proceedings of TARK 2007, pp. 72–81 (2007)
8. Fagin, R., Halpern, J., Moses, Y., Vardi, M.: Reasoning about Knowledge. MIT
Press, Cambridge (1995)
9. Lomuscio, A., Ryan, M.: An algorithmic approach to knowledge evolution. Artiﬁcial
Intelligence for Engineering Design, Analysis and Manufacturing (AIEDAM) 13(2)
(1998); Special issue on Temporal Logic in Engineering
10. Aczel, P.: Non-Well-Founded Sets. CSLI Lecture Notes, vol. 14. CSLI Publications,
Stanford (1988)
11. Visser, A.: Bisimulations, model descriptions and propositional quantiﬁers. Logic
Group Preprint Series 161, Department of Philosophy, Utrecht University (1996)
12. Hollenberg, M.: Logic and bisimulation. PhD thesis, University of Utrecht (1998)
13. French, T.: Bisimulation quantiﬁers for modal logic. PhD thesis, University of
Western Australia (2006)
14. van Benthem, J., Ikegami, D.: Modal ﬁxed-point logic and changing models. In:
Avron, A., Dershowitz, N., Rabinovich, A. (eds.) Pillars of Computer Science.
LNCS, vol. 4800, pp. 146–165. Springer, Heidelberg (2008); Also available as ILLC
Prepublication Series PP-2008-19
15. French, T., van Ditmarsch, H.: Undecidability for arbitrary public announcement
logic. In: Proceedings of the seventh conference “Advances in Modal Logic”, Lon-
don, pp. 23–42. College Publications (2008)
16. Fagin, R., Halpern, J.: Belief, awareness, and limited reasoning. Artiﬁcial Intelli-
gence 34(1), 39–76 (1988)

On the Dynamics of Institutional Agreements
Andreas Herzig1, Tiago de Lima2, and Emiliano Lorini1
1 IRIT, Toulouse, France
Andreas.Herzig@irit.fr,
Emiliano.Lorini@irit.fr
2 Eindhoven University of Technology, The Netherlands
T.d.Lima@tue.nl
Abstract. In this work we continue the work initiated in [1], in which
a logic of individual and collective acceptance was introduced. Our aim
in this paper is to investigate the extension of the logic of acceptance
by public announcements of formulas. The function of public announce-
ments is to diminish the space of possible worlds accepted by agents and
sets of agents while functioning as members of a given group, team, orga-
nization, institution, etc., x. If a set of agents C ends up with an empty
set of worlds that they accept while functioning as members of x, then
the agents in C do not identify themselves any longer with x. In such a
situation the agents in C should have the possibility to join x again. To
that aim we discuss at the end of the paper an operation which consists
of an agent (or set of agents) joining a given group, team, organization,
institution, etc.
1
Introduction
The concept of collective acceptance has been studied in social philosophy in op-
position to group attitudes such as common belief and common knowledge that
are popular in artiﬁcial intelligence and theoretical computer science [2,3]. As
suggested in [4], the main diﬀerence between collective acceptance and common
belief (or common knowledge) is that a collective acceptance by a set of agents
C is based on the fact that the agents in C identify and recognize themselves as
members of the same social context, such as a group, team, organization, insti-
tution, etc. Common belief (and common knowledge) does not necessarily entail
this aspect of mutual recognition and identiﬁcation with respect to a social con-
text. In this sense, according to [4,5], collective acceptance rather than common
belief is the more appropriate concept to characterize a proper notion of group
belief. For example, in the context of the organization Greenpeace the agents in
a set C (collectively) accept that their mission is to protect the Earth qua mem-
bers of Greenpeace. The state of acceptance qua members of Greenpeace is the
kind of acceptance the agents in C are committed to when they are functioning
together as members of Greenpeace.
It has been emphasized that a similar distinction between acceptance and
belief exists at the individual level. While an agent’s belief that p is an attitude
J.-J.Ch. Meyer and J.M. Broersen (Eds.): KRAMAS 2008, LNAI 5605, pp. 66–80, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

On the Dynamics of Institutional Agreements
67
of the agent constitutively aimed at the truth of p, an agent’s acceptance is not
necessarily connected to the actual truth of the proposition. In order to better
distinguish these two notions, it has been suggested in [6] that while an agent’s
beliefs are not subject to the agent’s will, its acceptances are voluntary; while its
beliefs aim at truth, its acceptances are sensitive to pragmatic considerations;
while its beliefs are shaped by evidence, its acceptances need not be; ﬁnally, while
its beliefs are context-independent, its acceptances might depend on context.
Often the acceptances of an agent depend on social contexts, that is, while
identifying itself as a member of a group (or team, organization, institution,
etc.) an agent reasons and accepts things qua member of this group. In these
situations it may happen that the agent’s acceptances are in conﬂict with its
beliefs. For instance, a lawyer who is trying to defend a client in a murder case
accepts qua lawyer that the client is innocent, even if he believes the contrary.
The aim of this paper is to continue the work initiated in [1,7]. There, a
logic of individual and collective acceptance was introduced.1 One of the notable
features of that logic is that the accessibility relation associated to the acceptance
operator is not necessarily serial: an empty set of possible worlds associated to
a group C in a context x just means that C does not identify itself with x.
Our aim here is to investigate the extension of the logic of acceptance by
public announcements of formulas, noted x!ψ. Modal operators of type [x!ϕ] are
intended to express that the members of a certain group, team, organization,
institution, etc., x learn that ϕ is true in that institution in such a way that
their acceptances, qua members of x, are updated. The function of public an-
nouncements is to diminish the space of possible worlds accepted by agents and
groups of agents. It might also happen that a given set of agents C ends up with
an empty set of possible worlds that they accept while functioning as members
of a certain social context x. As we have said, this means that C quits x: the
agents in C do not identify themselves any longer with x. In such a situation C
should have the possibility to join x again. To that aim we discuss at the end
of the paper an operation which consists of an agent (or set of agents) joining a
given social context x.
The main contribution of this paper is to extend the logic presented in [1]
to public announcements and show that, diﬀerently from common belief and
common knowledge, reduction axioms can be given. As usual, the addition of
these axioms to the Hilbert axiomatics of the logic of acceptance provides a
complete axiomatization of the logic of acceptance and announcements. As far
as we know, this constitutes the ﬁrst attempt to build up a logic of acceptance
and public announcements.
The paper is organized as follows. In Section 2 we present the syntax and
semantics of acceptance logic together with its axiomatization. In Section 3 we
extend it with announcements, and show that our extension also allows for reduc-
tion axioms and thereby a complete axiomatization. In Section 4 we formalize an
example which illustrates the dynamics of acceptance based on announcements.
1 This logic has some similarities with the logic of group belief that we have developed
in [8,9].

68
A. Herzig, T. de Lima, and E. Lorini
In Section 5 we brieﬂy discuss the operation which consists of an agent (or set of
agents) joining a social context. This section is not intended to provide a solution
to the logical characterization of this social phenomenon though. In Section 6
we draw conclusions.
2
The Logic of Acceptance AL
We now present a variant of the Acceptance Logic (AL) that was introduced in
[1]. AL enables expressing that certain agents identify themselves as members of
a social context x and, reasoning about what agents and groups of agents accept
while functioning together as members of a certain social context. The axioms
of AL clarify the relationships between individual acceptance (acceptances of
individual agents) and collective acceptance (acceptances of groups of agents).
2.1
Syntax
The syntactic primitives of AL are the following: a ﬁnite non-empty set of agents
AGT; a countable set of atomic formulas ATM ; and a ﬁnite set of labels CTXT
denoting social contexts such as groups, teams, organizations, institutions, etc.
The language LAL of the logic AL is given by the following BNF:
ϕ ::= p | ¬ϕ | ϕ ∨ϕ | AC:xϕ
where p ranges over ATM , C ranges over 2AGT, and x ranges over CTXT. The
formula AC:xϕ reads “the agents in C accept that ϕ while functioning together
as members of x”. We write i:x instead of {i} :x.
For example, AC:GreenpeaceprotectEarth expresses that the agents in C accept
that the mission of Greenpeace is to protect the Earth while functioning as
activists in the context of Greenpeace; and Ai:CatholicPopeInfallibility expresses
that agent i accepts that the Pope is infallible while functioning as a Catholic
in the context of the Catholic Church.
The intuition is that in two diﬀerent contexts the same agent may accept
contradictory propositions. For example, while functioning as a Catholic, agent
i accepts that killing is forbidden, and while functioning as a soldier i accepts
that killing is allowed. The CEO of Airbus accepts that Airbus is in good health
while functioning as a member of Airbus Industries, and privately accepts the
contrary.
The classical boolean connectives ∧, →, ↔, ⊤(tautology) and ⊥(contradic-
tion) are deﬁned from ∨and ¬ in the usual manner.
The formula AC:x⊥has to be read “agents in C are not functioning together
as members of x”, because we assume that functioning as a group member is, at
least in this minimal sense, a rational activity. Conversely, ¬AC:x⊥has to be read
“agents in C are functioning together as members of x”. Thus, ¬AC:x⊥∧AC:xϕ
stands for “agents in C are functioning together as members of x and they accept
that ϕ while functioning together as members of x” or simply “agents in C accept
that ϕ qua members of x”. This is a case of group acceptance. For the individual
case, formula ¬Ai:x⊥∧Ai:xϕ has to be read “agent i accepts that ϕ qua member
of x”. This is a case of individual acceptance.

On the Dynamics of Institutional Agreements
69
2.2
Semantics and Axiomatization
We use a standard possible worlds semantics. Let the set of all couples of non-
empty subsets of agents and social contexts be
Δ =

C:x : C ∈2AGT and x ∈CTXT

.
An acceptance model is a triple M = ⟨W , A , V ⟩where:
– W is a non-empty set of possible worlds;
– A : Δ →W × W maps every C:x ∈Δ to a relation AC:x between possible
worlds in W ; and
– V : ATM →2W is valuation function associating a set of possible worlds
V (p) ⊆W to each atomic formula p of ATM .
We write AC:x(w) for the set {w ′ : ⟨w, w ′⟩∈AC:x}. AC:x(w) is the set of
worlds that is accepted by the agents in C while functioning together as members
of x.
Given M = ⟨W , A , V ⟩and w ∈W , the couple ⟨M, w⟩is a pointed acceptance
model. The satisfaction relation |= between formulas of LAL and pointed accep-
tance models ⟨M, w⟩is deﬁned as usual for atomic propositions, negation and
disjunction. The satisfaction relation for acceptance operators is the following:
M, w |= AC:xϕ
iﬀ
M, w ′ |= ϕ for every w ′ ∈AC:x(w)
Validity of a formula ϕ (noted |=AL ϕ) is deﬁned as usual.
The axiomatization of AL is given in Fig. 1. It is not meant to reﬂect the
semantics the way it has been presented up to this point. Instead, these axioms
are meant as postulates, and we are going to present the corresponding semantic
constraints later.
As usual, the K-principles are the axioms and inference rules of the basic
modal logic K. Axioms 4* and 5* are introspection axioms: when the agents
in a set C function together as members of x, then, for all y ∈CTXT and B
such that B ⊆C, the agents in B have access to all the facts that are accepted
(or that are not accepted) by the agents in C. In particular, if the agents in C
(do not) accept that ϕ while functioning together as members of x then, while
functioning together as members of y, the agents of every subset B of C accept
that agents in C (do not) accept that ϕ.
(K)
All K-principles for the operators AC:x
(4*)
AC:xϕ →AB:yAC:xϕ, if B ⊆C
(5*)
¬AC:xϕ →AB:y¬AC:xϕ, if B ⊆C
(Inc)
(¬AC:x⊥∧AC:xϕ) →AB:xϕ, if B ⊆C
(Una)
AC:x(
i∈C Ai:xϕ →ϕ)
Fig. 1. Axiomatization of AL

70
A. Herzig, T. de Lima, and E. Lorini
Example 1. Suppose that three agents i, j, k, while functioning together as mem-
bers of the UK trade union, accept that their mission is to try to increase teachers’
wages, but they do not accept qua members of the trade union that their mis-
sion is to try to increase railway workers’ wages: A{i,j,k}:UnionIncrTeacherWage
and ¬A{i,j,k}:UnionIncrRailwayWage. By axiom 4* we infer that, while func-
tioning as a UK citizen, i accepts that i, j, k accept that their mission is to try
to increase teachers’ wages, while functioning together as members of the trade
union: Ai:UKA{i,j,k}:UnionIncrTeacherWage. By Axiom 5* we infer that, while
functioning as a UK citizen, i accepts that i, j, k do not accept, qua members of
the trade union, that their mission is to try to increase railway workers’ wages:
Ai:UK¬A{i,j,k}:UnionIncrRailwayWage.
Axiom Inc says that, if the agents in C accept that ϕ qua members of x then
every subset B of C accepts ϕ while functioning together as members of x. This
means that things accepted by the agents in C qua members of a certain social
context x are necessarily accepted by agents in all of C’s subsets with respect
to the same context x. Axiom Inc describes the top down process leading from
C’s collective acceptance to the individual acceptances of C’s members.2
Example 2. Imagine three agents i, j, k that, qua players of the game Clue,
accept that someone called Mrs. Red, has been killed:
¬A{i,j,k}:Clue⊥∧A{i,j,k}:CluekilledMrsRed.
By axiom Inc we infer that also the two agents i, j, while functioning as Clue
players, accept that someone called Mrs. Red has been killed:
A{i,j}:CluekilledMrsRed.
Axiom Una expresses a unanimity principle according to which the agents in
C, while functioning together as members of x, accept that if each of them
individually accepts that ϕ while functioning as member of x, then ϕ is the case.
This axiom describes the bottom up process leading from individual acceptances
of the members of C to the collective acceptance of the group C.
In order to make our axioms valid we impose the following constraints on
acceptance models, for any world w ∈W , context x, y ∈CTXT, and coalitions
C, B ∈2AGT such that B ⊆C:
(S.1) if w ′ ∈AB:y(w) then AC:x(w ′) = AC:x(w);
(S.2) if AC:x(w) ̸= ∅then AB:x(w) ⊆AC:x(w);
(S.3) if w ′ ∈AC:x(w) then w ′ ∈
i∈C Ai:x(w ′).
Axioms 4* and 5* together correspond to the constraint S.1; axiom Inc corre-
sponds to S.2, and axiom Una to S.3 (in the sense of correspondence theory).
As all our axioms are in the Sahlqvist class we obtain straightforwardly:
2 Note that the more general
(¬AC:x⊥∧AC:xϕ) →AB:yϕ, if B ⊆C
would lead to unwanted consequences: the group of Catholics’ acceptance qua mem-
bers of the Catholic church that the Pope is infallible does not entail that Catholics
privately accept that the Pope is infallible.
Also note that for B ⊆C, neither AC:x⊥→AB:x⊥nor AB:x⊥→AC:x⊥should
hold.

On the Dynamics of Institutional Agreements
71
Theorem 1. The axiomatization of AL of Fig. 1 is sound and complete w.r.t.
the class of AL models satisfying constraints S.1, S.2, and S.3.
Proof. It is a routine task to check that all the axioms of the logic AL correspond
to their semantic counterparts. It is routine, too, to check that all AL axioms
are in the Sahlqvist class, for which a general completeness result exists [10].
⊓⊔
Example 3. It follows from axioms 4*, 5* and Inc that if B ⊆C then |=AL
AB:yAC:xϕ ↔AB:y⊥∨AC:xϕ and |=AL AB:y¬AC:xϕ ↔AB:y⊥∨¬AC:xϕ. We
also have |=AL AC:x(AC:xϕ →ϕ).
3
The Logic of Acceptance and Public Announcements
ALA
In its nature, acceptance comes by communication: if a group accepts that one
of its members i accepts that ϕ then this is often the result of a speech act per-
formed by i. Acceptance is therefore closely related to the notion of commitment
that has been studied in agent communication languages [11,12,13].
In this paper we study the combination of acceptance logic AL with a rather
simple communicative act, viz. public announcements as deﬁned in public an-
nouncement logic (PAL) [14]. Basically, when formula ψ is publicly announced,
all agents learn that ψ is true. Our truth condition is that of Kooi [15], that is
slightly diﬀerent from the standard one in public announcement logic: it does
not require announcements to be truthful.
3.1
Language and Models
The language LALA of acceptance logic with announcements (ALA) extends
LAL by modal formulas of the form [x!ψ]ϕ, where ϕ, ψ ∈LAL. Such formulas
are read “ϕ holds after the public announcement of ψ in context x”. Modal
operators of type [x!ψ] are intended to express that the agents learn that ψ is
true in the social context x, in such a way that their acceptances, qua members
of x, are updated.
The announcement x!Ai:xψ is an event. It approximates i’s action of an-
nouncing that ψ in context x. (This is an assertion in speech act theory and in
Walton and Krabbe’s dialogue games [16].)
It is worth noting that when x denotes an institution, events of type x!ψ can
be used to describe the event of issuing or promulgating a certain norm ψ (e.g.
obligation, permission) within the context of the institution x.3
Formulas of LALA are interpreted in pointed acceptance models. The satis-
faction relation |= of Section 2 is extended by the following clause:
⟨W , A , V ⟩, w |= [x!ψ]ϕ
iﬀ
⟨W , A x!ψ, V ⟩, w |= ϕ
3 For a logical characterization of the act of proclaiming or promulgating a norm, see
also [17].

72
A. Herzig, T. de Lima, and E. Lorini
with
– A x!ψ
C:y (w) = AC:y(w), for all C:y ∈Δ, w ∈W and y ̸= x;
– A x!ψ
C:y (w) = AC:y(w) ∩||ψ||M, for all C:y ∈Δ, w ∈W and y = x,
where as usual ||ψ||M = {w
:
M, w |= ψ} is the extension of ψ in M, i.e.,
the set of worlds where ψ is true. Thus, in a way similar to [15], the agents take
into account the announcement of ψ in the social context x and modify their
acceptances qua members of x by eliminating all arrows leading to ¬ψ worlds
(instead of eliminating the worlds themselves, as in PAL). On the contrary, when
x and y are diﬀerent, the accessibility relations associated to the acceptances
qua members of y are not modiﬁed, after the announcement of ψ in the social
context x.
Validity of a formula ϕ (noted |=ALA ϕ) is deﬁned as before. For example,
|=ALA [x!p]AC:xp, and |=ALA AC:x¬p →[x!p]AC:x⊥. The latter means that
coalition C quits all social contexts within which C’s acceptances are inconsistent
with what is announced.
Note that contrarily to standard common knowledge and belief, the modi-
ﬁed accessibility relations for acceptances are not computed from the modiﬁed
accessibility relations for individuals, but are ﬁrst-class citizens here: they are
changed ‘on their own’.
Proposition 1. If M is an acceptance model then Mx!ψ is an acceptance model.
Proof. We show that Mx!ψ = ⟨W , A x!ψ, V ⟩satisﬁes S.1, S.2 and S.3. In what
follows let B ⊆C.
(S.1): Let w2 ∈A x!ψ
B:y (w1). If the latter is true then w2 ∈AB:y(w1), which
implies AC:x(w2) = AC:x(w1), because M respects S.1.
Now, we show that A x!ψ
C:x (w2) ⊆A x!ψ
C:x (w1). Consider a possible world w3 ∈
A x!ψ
C:x (w2). This means that w3 ∈AC:x(w2)∩||ψ||M. Then, in particular, w3 ∈
AC:x(w2), which implies w3 ∈A x!ψ
C:x (w1), because AC:x(w2) = AC:x(w1).
By using an analogous argument, we show that A x!ψ
C:x (w1) ⊆A x!ψ
C:x (w2).
(S.2): Let A x!ψ
C:x (w1) ̸= ∅and w2 ∈A x!ψ
B:x (w1). We show that w2 ∈A x!ψ
C:x (w1).
The hypothesis implies w2 ∈AB:x(w1) ∩||ψ||M. Then, in particular, w2 ∈
AB:x(w1). Also note that the hypothesis implies AC:x(w1) ̸= ∅. Then, w2 ∈
AC:x(w1), because M respects S.2. We conclude that w2 ∈AC:x(w1) ∩
||ψ||M. The latter is true if and only if w2 ∈A x!ψ
C:x (w1).
(S.3): Let w2 ∈A x!ψ
C:x (w1). We show that w2 ∈A x!ψ
i:x (w2) for some i ∈C. The
hypothesis is equivalent to w2 ∈AC:x(w1) ∩||ψ||M. Then, in particular,
w2 ∈AC:x(w1), which implies w2 ∈Ai:x(w2) for some i ∈C, because M
respects S.3. Then, w2 ∈Ai:x(w2) ∩||ψ||M for some i ∈C. The latter is
true if and only if w2 ∈A x!ψ
i:x (w2) for some i ∈C.
⊓⊔
3.2
Reduction Axioms and Completeness
Just as in dynamic epistemic logics without common belief, ALA has reduction
axioms for all cases (individual and collective acceptance). This contrasts with
logics having the common belief operator, for which such axioms do not exist [18].

On the Dynamics of Institutional Agreements
73
Proposition 2. The following equivalences are ALA valid.
(R.1) [x!ψ]p ↔p
(R.2) [x!ψ]¬ϕ ↔¬[x!ψ]ϕ
(R.3) [x!ψ](ϕ1 ∧ϕ2) ↔[x!ψ]ϕ1 ∧[ψ!ϕ]2
(R.4) [x!ψ]AC:yϕ ↔AC:y[x!ψ]ϕ
(if y ̸= x)
(R.5) [x!ψ]AC:yϕ ↔AC:y(ψ →[x!ψ]ϕ)
(if y = x)
Proof. (R.1):
⟨W , A , V ⟩, w |= [x!ψ]p
iﬀ⟨W , A x!ψ, V ⟩|= p
iﬀw ∈V (p)
iﬀ⟨W , A , V ⟩, w |= p.
(R.2):
⟨W , A , V ⟩, w |= [x!ψ]¬ϕ
iﬀ⟨W , A x!ψ, V ⟩, w |= ¬ϕ
iﬀ⟨W , A x!ψ, V ⟩, w ̸|= ϕ
iﬀ⟨W , A , V ⟩, w ̸|= [x!ψ]ϕ
iﬀ⟨W , A , V ⟩, w |= ¬[x!ψ]ϕ.
(R.3):
⟨W , A , V ⟩, w |= [x!ψ](ϕ1 ∧ϕ2)
iﬀ⟨W , A x!ψ, V ⟩, w |= ϕ1 ∧ϕ2
iﬀ⟨W , A x!ψ, V ⟩, w |= ϕ1 and ⟨W , A x!ψ, V ⟩, w |= ϕ2
iﬀ⟨W , A , V ⟩, w |= [x!ψ]ϕ1 and ⟨W , A , V ⟩, w |= [x!ψ]ϕ2
iﬀ⟨W , A , V ⟩, w |= [x!ψ]ϕ1 ∧[x!ψ]ϕ2.
(R.4): We show that the equivalent ¬[x!ψ]AC:yϕ ↔¬AC:y[x!ψ]ϕ is valid.
⟨W , A , V ⟩, w |= ¬[x!ψ]AC:yϕ
iﬀ⟨W , A , V ⟩, w |= [x!ψ]¬AC:yϕ, by (R.3),
iﬀ⟨W , A x!ψ, V ⟩, w |= ¬AC:yϕ
iﬀthere is w ′ ∈A x!ψ
C:y (w) such that ⟨W , A x!ψ, V ⟩, w ′ |= ¬ϕ
iﬀthere is w ′ ∈AC:y(w) such that ⟨W , A x!ψ, V ⟩|= ¬ϕ, because y ̸= x,
iﬀthere is w ′ ∈AC:y(w) such that ⟨W , A , V ⟩|= [x!ψ]¬ϕ
iﬀthere is w ′ ∈AC:y(w) such that ⟨W , A , V ⟩|= ¬[x!ψ]ϕ, by (R.3),
iﬀ⟨W , A , V ⟩|= ¬AC:y[x!ψ]ϕ.
(R.5): We show that ¬[x!ψ]AC:yϕ ↔¬AC:y(ψ →[x!ψ]ϕ) is valid.
⟨W , A , V ⟩, w |= ¬[x!ψ]AC:yϕ
iﬀ⟨W , A , V ⟩, w |= [x!ψ]¬AC:yϕ, by (R.3),
iﬀ⟨W , A x!ψ, V ⟩, w |= ¬AC:yϕ
iﬀthere is w ′ ∈A x!ψ
C:y (w) such that ⟨W , A x!ψ, V ⟩, w ′ |= ¬ϕ
iﬀthere is w ′ ∈AC:y(w) such that ⟨W , A , V ⟩|= ψ and ⟨W , A x!ψ, V ⟩|= ¬ϕ,
because y = x,
iﬀthere is w ′ ∈AC:y(w) such that ⟨W , A , V ⟩|= ψ and ⟨W , A , V ⟩|=
[x!ψ]¬ϕ
iﬀthere is w ′ ∈AC:y(w) such that ⟨W , A , V ⟩|= ψ ∧[x!ψ]¬ϕ
iﬀthere is w ′ ∈AC:y(w) such that ⟨W , A , V ⟩|= ψ ∧¬[x!ψ]ϕ, by (R.3),
iﬀ⟨W , A , V ⟩|= ¬AC:y(ψ →[x!ψ]ϕ).
⊓⊔

74
A. Herzig, T. de Lima, and E. Lorini
These equivalences are called reduction axioms because they allow to rewrite
every formula by successively eliminating the announcement operators, ending
up with a formula that contains none.
Theorem 2. For every ALA formula there is an equivalent AL formula.
Proof. The proof goes just as for public announcement logic (found in [15]): each
of the above ALA valid equivalences R.2–R.5, when applied from the left to
the right, yields a simpler formula, where ‘simpler’ roughly speaking means that
the announcement operator is pushed inwards. Once the announcement operator
attains an atom it is eliminated by the ﬁrst equivalence R.1.
⊓⊔
Theorem 3. The formulas that are valid in ALA models are completely ax-
iomatized by the axioms and inference rules of AL together with the reduction
axioms of Proposition 2.
Proof. This a straightforward consequence of Theorem 1 and Theorem 2.
⊓⊔
Here are some examples of reductions.
Example 4. The formula [x!p]AC:xp is successively rewritten as follows:
AC:x(p →[x!p])
by R.5
AC:x(p →p)
by R.1
The latter is a theorem of every normal modal logic (and therefore also of ac-
ceptance logic AL). It follows that the initial formula is valid, too.
Example 5. The formula Ai:x¬p →[x!p]Ai:x⊥is rewritten as follows:
Ai:x¬p →Ai:x(p →[x!p]⊥)
by R.5
Ai:x¬p →Ai:x(p →⊥)
by R.1
The latter is a theorem of every normal modal logic (and therefore also of ac-
ceptance logic AL).
Example 6. The formula [x!(Ai:xp)]AC:xAi:xp is rewritten as follows:
AC:x(Ai:xp →[x!(Ai:xp)]Ai:xp)
by R.5
AC:x(Ai:xp →Ai:x(Ai:xp →[x!(Ai:xp)]p))
by R.5
AC:x(Ai:xp →Ai:x(Ai:xp →p))
by R.1
The latter is an AL theorem (because Ai:x(Ai:xp →p) is an AL theorem, cf.
Example 3 of Section 2). It follows that the initial formula is valid, too.
3.3
Discussion
As said before, ALA has reduction axioms for all cases (individual and collective
acceptance), while it has been shown in [18] that logics of common belief (and
common knowledge) do not. Technically, it happens because acceptance models
have more arrows. Let M be the epistemic model in Fig. 2. Note that even
though M, w ̸|= Cij(q →[!q]p), we still have M, w |= [!q]Cijp. In words, it is not
common belief among i and j that q implies that after the public announcement
of q, we have that p, but after the public announcement of q it is common belief

On the Dynamics of Institutional Agreements
75
among i and j that p. That is, common belief may appear ‘out of the blue’, i.e.,
it was not foreseeable by the agents, it just ‘pops up’.
However, when we build the acceptance model that corresponds to the model
M, it looks like the structure in Fig. 3. Semantic constraint S.1 obliges the
corresponding acceptance model to have more arrows, in particular, it must
have the dashed arrow from w to u labelled by ij:x. Let M′ be such acceptance
model, note that M ′, w ̸|= Aij:x(q →[!q]p), and also M′, w ̸|= [x!q]Aij:xp. That
is, contrary to common belief, common acceptances cannot just ‘pop up’ without
be foreseeable by the agents.
None the less, we argue that the reduction axiom R.5 is an intuitive property
of collective acceptance. This is due to the fact that, diﬀerently from the standard
notions of common belief and common knowledge, collective acceptance entails
an aspect of mutual identiﬁcation and recognition with respect to a group.
Consider the left to right direction of the reduction axiom R.5. When the
agents in a set C identify themselves with a group x and recognize each other
as members of this group, they accept certain rules and principles to stand
for the the rules and principles of the group. That is, the agents in C share a
common body of rules and principles. Among these shared rules and principles,
there are the rules and principles which describe how the world should evolve
when an announcement occurs. They govern how the acceptance of the agents
in the group will be changed after an announcement. Suppose that a certain
fact ψ is publicly announced. After this announcement, the agents in C accept
ϕ, while identifying themselves with a group x and recognizing each other as
members of this group: [x!ψ]AC:xϕ. This collective acceptance of the agents in
C is not created from scratch after the announcement of ψ. On the contrary, the
creation of this acceptance depends on what the agents in C accepted (before
the announcement) as a principle of group x. In particular, the creation of C’s
acceptance that ϕ rests on the fact that, before ψ is announced, the agents in C,
while identifying themselves and recognizing each other as members of x, accept
a principle saying that “if ψ is true then, after ψ is announced in x, ϕ will be
true”: AC:x(ψ →[x!ψ]ϕ).
v
i
p
j
q
u
w
p,q
Fig. 2. An epistemic model
Fig. 3. The acceptance model corresponding to the epistemic model in Fig. 2

76
A. Herzig, T. de Lima, and E. Lorini
For example, imagine that the agents in a set C identify themselves and
recognize each other as members of the Lilliputian paciﬁst movement. Let ψ
denote the proposition “the government of Lilliput has decided to attack the
neighboring nation of Blefuscu”.4 After ψ is publicly announced the agents
in C accept that ϕ = “they should start to protest against the Lilliput gov-
ernment”, while functioning as members of the Lilliputian paciﬁst movement:
[LilliputPacifist!ψ]AC:LilliputP acifistϕ. This implies that (before the announce-
ment) the agents in C, while identifying themselves and recognizing each other
as members of the Lilliputian paciﬁst movement, accept a principle saying that
“if ψ is true then, after ψ is announced, ϕ will be true”: AC:LilliputPaciﬁst(ψ →
[LilliputPacifist!ψ]ϕ). That is, the creation of C’s acceptance to protest against
the Lilliput government depends on the fact that, before the announcement, the
agents in C accept to protest against the Lilliput government in case it will
announce its decision to attack the neighboring nation of Blefuscu. This means
that C’s acceptance to protest depends on the fact that, before the announce-
ment, the agents in C accept a principle which speciﬁes what to do in case the
Lilliput government will manifest its intention to attack Blefuscu.
4
An Example
Until now we only considered that group acceptances emerge from consensus,
by admitting axiom Una. One can go further and also consider other kinds of
group acceptances, as shown in the next example. The example is inspired by
Pettit [19].
Example 7. Imagine a three-member court which has to make a judgment on
whether a defendant is liable (noted l) for a breach of contract. The three judges
i, j and k accept a majority rule to decide on the issue. That is, i, j and k,
while functioning as members of the court, accept that if the majority of them
accepts that the defendant is liable (resp. not liable), then the defendant is
liable (resp. not liable). Formally, for any B such that B ⊆{i, j, k} and |B| = 2
we have:
(Maj)
A{i,j,k}:court(
	
i∈B
Ai:courtl →l)
∧
A{i,j,k}:court(
	
i∈B
Ai:court¬l →¬l)
Given the previous majority rule, we can prove that: after the announcement
that both i and j accept l (the defendant is liable) while functioning as mem-
bers of the court, the agents in {i, j, k} accept l while functioning together as
members of the court. Indeed, from the previous majority rule we can derive the
formula [court!Ai:courtl ∧Aj:courtl]A{i,j,k}:courtl. To prove this, it is suﬃcient
to note that, by means of the reduction axioms, the formula [court!Ai:courtl ∧
Aj:courtl]A{i,j,k}:courtl is successively rewritten as follows:
4 Lilliput and Blefuscu are the two ﬁctional nations, permanently at war, that appear
in the novel “Gulliver’s Travels” by Jonathan Swift.

On the Dynamics of Institutional Agreements
77
A{i,j,k}:court((Ai:courtl ∧Aj:courtl) →[court!Ai:courtl ∧Aj:courtl]l)
by R.5
A{i,j,k}:court((Ai:courtl ∧Aj:courtl) →l)
by R.1
The latter is entailed by the majority rule Maj.
In the previous example, we have considered a majority rule as a principle which
is responsible for the creation of collective acceptances from individual accep-
tances. This is stronger than the basic axiom of unanimity (Una) of AL. One
can imagine other kinds of rules. For instance, one can consider social contexts
with leaders (see also [7]). In such contexts, one can formalize the rule according
to which everything that the leaders accept is universally accepted in the social
context. Let the set of leaders of x be Lx ∈2AGT. Then one can formalize that
everything that the leaders accept is universally accepted in the social context
by:
(Leader)
AC:x(ALx :xϕ →ϕ)
5
Adding Retractions to ALA: Some General Insights
According to our semantics, Ai:x¬p →[x!p]Ai:x⊥is an ALA theorem (cf. Ex-
ample 5). In words, when p is publicly announced then i quits all contexts x
where he accepted p: agent i is no longer part of the institution, is kicked out
of the group, etc. In ALA there is no means for i to get out of that situation
and re-integrate context x. At the present stage, our logic of acceptance does
not include an operation which consists of an agent (or set of agents) joining a
certain social context.
Semantically, what we need is the opposite of the previous model restrictions:
an operation of adding arrows labelled by i:x to the model. Syntactically, what
we need is a new form of announcements i←C:x and corresponding modal oper-
ators of type [i←C:x], meaning that agent i adopts C’s acceptances in context x.
In terms of Kripke models, the accessibility relation Ai:x is identiﬁed with AC:x.
This kind operation of adding arrows is reminiscent of the logic of preference
upgrade of van Benthem and Liu [20], and the logic of granting and revoking
permissions of Pucella and Weissman [21].5 More intuitively, i←C:x represents
the operation of agent i’s joining the social context x by adopting the accep-
tances of group C of members of x. After this operation, agent i should start to
function again as members of x.
Other kinds of retraction operations can be devised. For example, one might
want to consider the operation of creating a supergroup D of a given group C,
where D takes over all of C’s acceptances. The logical form of such an operation
might be expressed by the operator [D:=C : x]. This operation should allow in
particular to express that the agents in D start to function as members of x (i.e.,
to move from AD:x⊥to ¬AD:x⊥), by taking over all acceptances of the agents
in the subgroup C.
5 See [22] for a systemic study of these operators.

78
A. Herzig, T. de Lima, and E. Lorini
We are currently working on the technical issue of providing a semantic char-
acterization and axiomatics of the previous operations i←C:x and D:=C :x and
corresponding modal operators [i←C:x] and [D:=C :x].
6
Conclusion
In this paper we continued the studies initiated in [1], where the logic AL,
intended to formalize group (and individual) acceptances, was proposed. Here
we extend AL by public announcements. As far as we know, our approach es
novel and there is no other attempt to build up a logic of acceptance and public
announcements.
The public announcement of ψ is an event that results in all agents learning
that ψ is true. The public announcement of AC:xψ can be understood as a
speech act. It simulates the announcement made by the group C itself, that
they accept ψ while functioning as members of x. Therefore, as seen in Example
7, public announcements can be used to reason about the acceptances of agents
when they express their own acceptances to each other. For instance, in that
particular example we saw that a public announcement makes one of the agents
quit the group, since he learns that the acceptances of the other agents are
contrary to his own acceptances in the same context. As noted in Section 3.1,
when the social context x denotes an institution, announcements of the form
x!ψ can be used to describe the event of issuing or promulgating a certain norm
ψ (e.g. obligation, permission) within the context of the institution x.
We also provide a complete axiomatization for the logic of acceptances and
announcements ALA. As well as for epistemic logic with public announcements,
the axiomatization given for ALA uses reduction axioms. In ALA, group accep-
tances are related to individual acceptances, but they are not computed from
them. It contrasts with epistemic logics where the concept of common knowl-
edge (or common belief) is completely deﬁned in terms of individual knowl-
edge (or belief). Due to this diﬀerence, it is possible to have reduction axioms
for group acceptances, while it is known to be impossible for common knowl-
edge. Still, in Section 3.3 we argue that this is an intuitive feature of group
acceptances.
Acknowledgements
We would like to thank the three anonymous reviewers for their very helpful
comments.
The contribution by Andreas Herzig and Emiliano Lorini was supported by
the French ANR project ‘ForTrust: Social Trust Analysis and Formalization’.
The contribution by Tiago de Lima is part of the research program Moral Re-
sponsibility in R&D Networks, supported by the Netherlands Organisation for
Scientiﬁc Research (NWO), under grant number 360-20-160.

On the Dynamics of Institutional Agreements
79
References
1. Gaudou, B., Longin, D., Lorini, E., Tummolini, L.: Anchoring institutions in agents’
attitudes: Towards a logical framework for autonomous multi-agent systems. In:
Padgham, L., Parkes, D.C. (eds.) Proceedings of AAMAS 2008, pp. 728–735. IFAA-
MAS (2008)
2. Fagin, R., Halpern, J., Moses, Y., Vardi, M.: Reasoning about Knowledge. The
MIT Press, Cambridge (1995)
3. Lewis, D.K.: Convention: a philosophical study. Harvard University Press, Cam-
bridge (1969)
4. Gilbert, M.: On Social Facts. Routledge, London (1989)
5. Tuomela, R.: The Philosophy of Sociality. Oxford University Press, Oxford
(2007)
6. Hakli, P.: Group beliefs and the distinction between belief and acceptance. Cogni-
tive Systems Research (7), 286–297 (2006)
7. Lorini, E., Longin, D.: A logical approach to institutional dynamics: from accep-
tances to norms via legislators. In: Brewka, G., Lang, J. (eds.) Proceedings of KR
2008. AAAI Press, Menlo Park (2008)
8. Gaudou, B., Herzig, A., Longin, D.: Grounding and the expression of belief. In:
Proceedings of KR 2006, pp. 211–229. AAAI Press, Menlo Park (2006)
9. Gaudou, B., Herzig, A., Longin, D., Nickles, M.: A new semantics for the FIPA
agent communication language based on social attitudes. In: Brewka, G., Corade-
schi, S., Perini, A., Traverso, P. (eds.) Proceedings of ECAI 2006, pp. 245–249. IOS
Press, Amsterdam (2006)
10. Blackburn, P., de Rijke, M., Venema, Y.: Modal Logic. Cambridge University Press,
Cambridge (2001)
11. Fornara, N., Colombetti, M.: Operational speciﬁcation of a commitment-based
agent communication language. In: Castelfranchi, C., Johnson, W.L. (eds.) Pro-
ceedings of AAMAS 2002, Bologna, pp. 535–542. ACM Press, New York (2002)
12. Verdicchio, M., Colombetti, M.: A Logical Model of Social Commitment for Agent
Communication. In: Proceedings of AAMAS 2003, pp. 528–535. ACM, New York
(2003)
13. Singh, M.P.: Agent communication languages: Rethinking the principles. IEEE
Computer 31(12), 40–47 (1998)
14. Plaza, J.: Logics of public communications. In: Emrich, M.L., Hadzikadic,
M., Pfeifer, M.S., Ras, Z.W. (eds.) Proceedings of ISMIS 1989, pp. 201–216
(1989)
15. Kooi, B.: Expressivity and completeness for public update logic via reduction ax-
ioms. Journal of Applied Non-Classical Logics 17(2), 231–253 (2007)
16. Walton, D.N., Krabbe, E.C.: Commitment in Dialogue: Basic Concepts of Inter-
personal Reasoning. State University of New-York Press (1995)
17. Gelati, J., Rotolo, A., Sartor, G., Governatori, G.: Normative autonomy and nor-
mative co-ordination: Declarative power, representation, and mandate. Artiﬁcial
Intelligence and Law 12(1-2), 53–81 (2004)
18. Kooi, B., Van Benthem, J.: Reduction axioms for epistemic actions. In: Schmidt,
R., Pratt-Hartmann, I., Reynolds, M., Wansing, H. (eds.) Proceedings AiML 2004,
pp. 197–211. King’s College Publications (2004)
19. Pettit, P.: Deliberative democracy and the discursive dilemma. Philosophical Is-
sues 11, 268–299 (2001)

80
A. Herzig, T. de Lima, and E. Lorini
20. van Benthem, J., Liu, F.: Dynamic logic of preference upgrade. Journal of Applied
Non-Classical Logics 17(2), 157–182 (2007)
21. Pucella, R., Weissman, V.: Reasoning about dynamic policies. In: Walukiewicz, I.
(ed.) FOSSACS 2004. LNCS, vol. 2987, pp. 453–467. Springer, Heidelberg (2004)
22. Aucher, G., Balbiani, P., Farias Del Cerro, L., Herzig, A.: Global and local graph
modiﬁers. In: Methods for Modalities 5 (M4M-5). ENTCS. Elsevier, Amsterdam
(2007)

From Trust in Information Sources to Trust in
Communication Systems: An Analysis in Modal Logic
Emiliano Lorini and Robert Demolombe
Institut de Recherche en Informatique de Toulouse (IRIT), France
Emiliano.Lorini@irit.fr
Robert.Demolombe@irit.fr
Abstract. We present a logical analysis of trust that integrates in the deﬁnition of
trust: the truster’s goal and the truster’s belief that the trustee has the right proper-
ties (powers, abilities, dispositions) to ensure that the goal will be achieved. The
second part of the paper is focused on the speciﬁc domain of trust in information
sources and communication systems. We provide an analysis of the properties
of information sources (validity, completeness, sincerity and cooperativity) and
communication systems (availability and privacy) and, we discuss their relation-
ships with trust.
1
Introduction
Future computer applications such as the semantic Web [3], e-business and e-commerce
[16], Web services [28] will be open distributed systems in which the many constituent
components are agents spread throughout a network in a decentralized manner. These
agents will interact between them in ﬂexible ways in order to achieve their design ob-
jectives and to accomplish the tasks which are delegated to them by the human users.
Some of them will directly interact and communicate with the human users. During
the system’s lifetime, these agents will need to manage and deal with trust. They will
need to automatically make trust judgments in order to assess the trustworthiness of
other (software and human) agents while, for example, exchanging money for a ser-
vice, giving access to a certain information, choosing between conﬂicting sources of
information. They will also need to understand how trust can be induced in a human
user in order to support his interaction with the system and to motivate him to use the
application. Consequently, these agents will need to understand the components and the
determinants of the user’s trust in the system.
Thus, to realize all their potential, future computer applications will require the de-
velopment of sophisticated formal and computational models of trust. These models
must provide clear deﬁnitions of the relevant concepts related to trust and safe reason-
ing rules which can be exploited by the agents for assessing the trustworthiness of a
given target. Moreover, these models of trust must be cognitively plausible, so that they
can be directly exploited by the agents during their interactions with the human user in
order to induce him to trust the system and the underlying Information and Communi-
cation Technology (ICT) infrastructure. With cognitively plausible models of trust, we
mean models in which the main cognitive constituents of trust as a mental attitude are
identiﬁed (e.g.beliefs, goals).
J.-J.Ch. Meyer and J.M. Broersen (Eds.): KRAMAS 2008, LNAI 5605, pp. 81–98, 2009.
c⃝Springer-Verlag Berlin Heidelberg 2009

82
E. Lorini and R. Demolombe
This paper follows our previous works [25,20] with the objective of developing a
general formal model of trust which meets the previous desiderata. It is worth noting
that it is not our aim to propose a model of trust based on statistics about past interac-
tions with a given target and reputational information. In particular, the present paper
focuses on an issue that we have neglected up to now: the issue of trust in information
sources and communication systems. We think that this issue is very relevant for fu-
ture computer applications such as the semantic Web, e-business and Web services. For
example, in a typical scenario of e-business, trust in information sources has a strong in-
ﬂuence on an agent’s decision to buy, or to sell, a speciﬁc kind of stocks. Indeed, to take
such a decision an agent has several types of information sources to consult in order to
predict the future evolution of the stock value. These information sources may be banks,
companies, consultants, etc. and the agent may believe that some of these information
sources have a good competence but are not necessarily sincere, others are reluctant to
inform about bad news, others are competent but are not necessarily informed at the
right moment, etc. In a typical scenario of Web services, an agent might want to make
a credit card transaction by means of a certain online payment system. In this case, the
agent’s trust in the communication system has a strong inﬂuence on the agent’s decision
to exploit it for the credit card transaction. In particular, the agent’s trust in the online
payment system is supported by the agent’s belief that the online payment system will
ensure the privacy of the credit card number from potential intruders.
The paper is organized as follows. We start with a presentation of a modal logic
which enables reasoning about actions, beliefs and goals of agents (Section 2). This
logic will be used during the paper for formalizing the relevant concepts of our model
of trust. Then, a general deﬁnition of trust is presented (Section 3). Section 4 is fo-
cused on the formal characterization of some important properties of an information
source: validity, completeness, sincerity and cooperativity. In Section 5 we show that
these properties are epistemic supports for trust in information sources. In Section 6 we
provide an analysis of communication systems. We deﬁne two fundamental properties
of communication systems: availability and privacy. Then, in Section 7, we show that
these properties are epistemic supports for an agent’s trust in a communication system.
We conclude with a discussion of some related works and we show some directions for
future works.
2
A Modal Logic of Beliefs, Goals and Actions
We present in this section the multimodal logic L that we use in the paper to formalize
the relevant concepts of our model of trust. L combines the expressiveness of a dynamic
logic [19] with the expressiveness of a logic of agents’ mental attitudes [9,29].
2.1
Syntax and Semantics
The syntactic primitives of the logic L are the following: a nonempty ﬁnite set of
agents AGT = {i, j, . . .}; a nonempty ﬁnite set of atomic actions AT = {a, b, . . .};
a ﬁnite set of atomic formulas Π = {p, q, . . .}. LIT is the set of literals which in-
cludes all atomic formulas and their negations, that is, LIT = {p, ¬p|p ∈Π}. We
note P, Q, . . . the elements in LIT. We also introduce speciﬁc actions of the form

From Trust in Information Sources to Trust in Communication Systems
83
infj(P) denoting the action of informing agent j that P is true. We call them infor-
mative actions. The set INFO of informative actions is deﬁned as follows: INFO =
{infj(P)|j ∈AGT, P ∈LIT}. Since the set Π is ﬁnite, the set INFO is ﬁnite as
well. The set ACT of complex actions is given by the union of the set of atomic actions
and the set of informative actions, that is: ACT = AT ∪INFO. We note α, β, . . .
the elements in ACT. The language of L is the set of formulas deﬁned by the follow-
ing BNF:
ϕ ::= p | ¬ϕ | ϕ ∨ϕ | Afteri:αϕ | Doesi:αϕ | Beliϕ | Goaliϕ
where p ranges over Π, α ranges over ACT and i ranges over AGT.
The operators of our logic have the following intuitive meaning. Beliϕ is meant to
stand for “agent i believes that ϕ”; Afteri:αϕ is meant to stand for “after agent i does
α, it is the case that ϕ” (Afteri:α⊥means “agent i cannot do action α”); Doesi:αϕ is
meant to stand for “agent i is going to do α and ϕ will be true afterward” (Doesi:α⊤
means “agent i is going to do α”); Goaliϕ is meant to stand for “agent i has the cho-
sen goal that ϕ” (which can be shortened to “agent i wants ϕ to be true”). Note that
operators Goali are used to denote an agent’s chosen goals, that is, the goals that the
agent has decided to pursue. We do not consider how an agent’s chosen goals origi-
nate through deliberation from more primitive motivational attitudes called desires (see
e.g.[30] on this issue). Since an agent’s chosen goals result from the agent’s delibera-
tion, they must satisfy two fundamental rationality principles: chosen goals have to be
consistent (i.e., a rational agent cannot decide to pursue inconsistent state of affairs);
chosen goals have to be compatible with the agent’s beliefs (i.e., a rational agent cannot
decide to pursue something that it believes to be impossible). These two principles will
be formally expressed in Section 4.
The following abbreviations are given:
Cani(α)
def
= ¬Afteri:α⊥;
Inti(α)
def
= GoaliDoesi:α⊤;
Infi,j(P)
def
= Doesi:infj(P )⊤.
Cani(α) means “agent i can do action α” (i.e. “i has the capacity to do α”). Inti(α)
means “agent i intends to do α”. Finally, Infi,j(P) means “i informs j that P is true”.
Models of the logic L are tuples M = ⟨W, R, D, B, G, V ⟩deﬁned as follows.
– W is a non empty set of possible worlds or states.
– R : AGT × ACT −→W × W maps every agent i and action α to a relation Ri:α
between possible worlds in W.
– D : AGT × ACT −→W × W maps every agent i and action α to a deterministic
relation Di:α between possible worlds in W.1
– B : AGT −→W × W maps every agent i to a serial, transitive and Euclidean2
relation Bi between possible worlds in W.
1 A relation Di:α is deterministic iff, if (w, w′) ∈Di:α and (w, w′′) ∈Di:α then w′ = w′′.
2 A relation Bi on W is Euclidean if and only if, if (w, w′) ∈Bi and (w, w′′) ∈Bi then
(w′, w′′) ∈Bi.

84
E. Lorini and R. Demolombe
– G : AGT −→W × W maps every agent i to a serial relation Gi between possible
worlds in W.
– V : W −→2Π is a truth assignment which associates each world w with the set
V (w) of atomic propositions true in w.
It is convenient to view relations on W as functions from W to 2W ; therefore we write
Di:α(w) for the set {w′ : (w, w′) ∈Di:α}, etc. Given a world w ∈W, Bi(w) is the
set of worlds which are compatible with agent i’s beliefs at w and Gi(w) is the set of
worlds which are compatible with agent i’s goals at w.
Given a world w ∈W, Ri:α(w) is the set of worlds that can be reached from w
through the occurrence of agent i’s action α; Di:α(w) is the next world of w which will
be reached from w through the occurrence of agent i’s action α. Indeed, we have two
kinds of relations for specifying the dynamic dimension of frames:
– when Di:α(w) = {w′} then at w agent i performs an action α resulting in the next
state w′;
– when w′ ∈Ri:α(w) but w′ ̸∈Di:α(w) then if at w agent i would do something dif-
ferent from what it actually does it might have produced another outcome world w′.
If Di:α(w) ̸= ∅(resp. Ri:α(w) ̸= ∅) then, we say that Di:α (resp. Ri:α) is deﬁned at w.
Note that the relation Di:α is used here to specify that an action α is going to performed
by a certain agent i and, consequently, that an agent i wants that a certain action α is
going to be performed by him (i.e. agent i intends to perform action α).
Given a model M, a world w and a formula ϕ, we write M, w |= ϕ to mean that
ϕ is true at world w in M. The rules deﬁning the truth conditions of formulas are just
standard for atomic formulas, negation and disjunction. The following are the remaining
truth conditions for Afteri:αϕ, Doesi:αϕ, Beliϕ and Goaliϕ:
– M, w |= Afteri:αϕ iff M, w′ |= ϕ for all w′ such that (w, w′) ∈Ri:α,
– M, w |= Doesi:αϕ iff ∃w′ such that (w, w′) ∈Di:α and M, w′ |= ϕ,
– M, w |= Beliϕ iff M, w′ |= ϕ for all w′ such that (w, w′) ∈Bi,
– M, w |= Goaliϕ iff M, w′ |= ϕ for all w′ such that (w, w′) ∈Gi.
Observe that the modal operator Doesi:α is of type possibility, and that all other modal
operators are of type necessity.
The following section is devoted to illustrate the additional semantic constraints over
L models and a sound and complete axiomatization of the logic L.
2.2
Semantic Constraints and Axiomatization
The axiomatization of the logic L includes all tautologies of propositional calculus and
the rule of inference modus ponens (MP).
MP
From ⊢ϕ and ⊢ϕ →ψ infer ⊢ψ
Operators for actions of type Afteri:α and Doesi:α are normal modal operators satisfy-
ing the axioms and rules of inference of system K. Operators of type Beli and Goali are
just standard normal modal operators. The former are standard doxastic operators sat-
isfying the axioms and rules of inference of system KD45, whereas the latter are modal

From Trust in Information Sources to Trust in Communication Systems
85
operators for goal similar to the operators introduced in [9] satisfying the axioms and
rules of inference of system KD. That is, the following axioms and rules of inference
for every operator Beli, Goali, Afteri:α and Doesi:α are given.
KBel
(Beliϕ ∧Beli(ϕ →ψ)) →Beliψ
KGoal
(Goaliϕ ∧Goali(ϕ →ψ)) →Goaliψ
KAfter
(Afteri:αϕ ∧Afteri:α(ϕ →ψ)) →Afteri:αψ
KDoes
(Doesi:αϕ ∧¬Doesi:α¬ψ) →Doesi:α(ϕ ∧ψ)
DBel
¬(Beliϕ ∧Beli¬ϕ)
DGoal
¬(Goaliϕ ∧Goali¬ϕ)
4Bel
Beliϕ →BeliBeliϕ
5Bel
¬Beliϕ →Beli¬Beliϕ
NecBel
From ⊢ϕ infer ⊢Beliϕ
NecGoal
From ⊢ϕ infer ⊢Goaliϕ
NecAfter From ⊢ϕ infer ⊢Afteri:αϕ
NecDoes
From ⊢ϕ infer ⊢¬Doesi:α¬ϕ
Actions and intentions. We add the following constraint over every relation Di:α and
every relation Dj:β of all L models. For every i, j ∈AGT, α, β ∈ACT and w ∈W:
S1
if Di:α and Dj:β are deﬁned at w then Di:α(w) = Dj:β(w).
Constraint S1 says that if w′ is the next world of w which is reachable from w through
the occurrence of agent i’s action α and w′′ is also the next world of w which is reach-
able from w through the occurrence of agent j’s action β, then w′ and w′′ denote the
same world. Indeed, we suppose that every world can only have one next world. Note
that S1 implies determinism of every Di:α. The semantic constraint S1 corresponds (in
the sense of correspondence theory, see e.g.[5]) to the following Axiom AltDoes.
AltDoes Doesi:αϕ →¬Doesj:β¬ϕ
Axiom AltDoes says that: if i is going to do α and ϕ will be true afterward, then it
cannot be the case that j is going to do β and ¬ϕ will be true afterward.
We also suppose that the world is never static in our framework, that is, we suppose
that for every world w there exists some agent i and action α such that i is going to
perform α at w. Formally, for every w ∈W we have that:
S2
there exist i ∈AGT and α ∈ACT such that Di:α is deﬁned at w.
The semantic constraint S2 corresponds to the following axiom of our logic.
Active 
i∈AGT,α∈ACT Doesi:α⊤
Axiom Active ensures that for every world w there is a next world of w which is reach-
able from w by the occurrence of some action of some agent. This is the reason why
the operator X for next of LTL (linear temporal logic) can be deﬁned as follows:3
3 Note that X satisﬁes the standard property Xϕ ↔¬X¬ϕ (i.e. ϕ will be true in the next state iff
¬ϕ will not be true in the next state).

86
E. Lorini and R. Demolombe
Xϕ
def
= 
i∈AGT,α∈ACT Doesi:αϕ.
The following relationship is supposed between every relation Di:α and the corre-
sponding relation Ri:α of all L models. For every i ∈AGT, α ∈ACT and w ∈W:
S3
Di:α ⊆Ri:α.
The constraint S3 says that if w′ is the next world of w which is reachable from w
through the occurrence of agent i’s action α, then w′ is a world which is possibly reach-
able from w through the occurrence of agent i’s action α. The semantic constraint S3
corresponds to the following axiom IncDoes,After.
IncDoes,After Doesi:αϕ →¬Afteri:α¬ϕ
According to IncDoes,After, if i is going to do α and ϕ will be true afterward, then it is
not the case that ¬ϕ will be true after i does α.
The following axioms relate intentions with actions.
IntAct1
(Inti(α) ∧Cani(α)) →Doesi:α⊤
IntAct2
Doesi:α⊤→Inti(α)
According to IntAct1, if i has the intention to do action α and has the capacity to do
α, then i is going to do α. According to IntAct2, an agent is going to do action α
only if he has the intention to do α. In this sense we suppose that an agent’s doing is by
deﬁnition intentional. Similar axioms have been studied in [26] in which a logical model
of the relationships between intention and action performance is proposed. IntAct1 and
IntAct2 correspond to the following semantic constraints over L models. For every
i ∈AGT, α ∈ACT and w ∈W:
S4
if Ri:α is deﬁned at w and Di:α is deﬁned at w′ for all w′ ∈Gi(w) then Di:α is
deﬁned at w;
S5
if w′ ∈Gi(w) and Di:α is deﬁned at w, then Di:α is deﬁned at w′.
As far as informative actions are concerned, we assume that they are always exe-
cutable, i.e. an agent i can always inform another agent j about a fact P. Formally:
CanInf
Cani(infj(P))
Note that different from public announcement logic (see, e.g., [17]), in our logic even
‘non-truthful’ informative actions are executable (i.e. informing that p even if p is cur-
rently false).
Axiom CanInf corresponds to the following semantic constraint over L models. For
every i ∈AGT, infj(P) ∈INFO and w ∈W:
S6
Ri:infj(P ) is deﬁned at w.
Beliefs, goals and actions. We suppose that goals and beliefs must be compatible,
that is, if an agent has the goal that ϕ then, he cannot believe that ¬ϕ. Indeed, the
notion of goal we characterize here is a notion of an agent’s chosen goal, i.e. a goal that
an agent decides to pursue. As some authors have stressed (e.g.[6]), a rational agent
cannot decide to pursue a certain state of affairs ϕ, if he believes that ¬ϕ. Thus, for any
i ∈AGT and w ∈W the following semantic constraint over L models is supposed:

From Trust in Information Sources to Trust in Communication Systems
87
S7
Gi(w) ∩Bi(w) ̸= ∅.
The constraint S7 corresponds to the following axiom WR (weak realism) of our logic.
WR
Goaliϕ →¬Beli¬ϕ
We also assume positive and negative introspection over (chosen) goals, that is:
PIntrGoal
Goaliϕ →BeliGoaliϕ
NIntrGoal
¬Goaliϕ →Beli¬Goaliϕ
Axioms PIntrGoal and NIntrGoal correspond together to the following semantic con-
straint over L models. For any i ∈AGT and w ∈W:
S8
if w′ ∈Bi(w) then Gi(w) = Gi(w′).
We suppose that agents satisfy the property of no forgetting (NF)4, that is, if an agent
i believes that after agent j does α, it is the case that ϕ, and agent i does not believe
that j cannot do action α, then after agent j does α, i believes that ϕ.
NF
(BeliAfterj:αϕ ∧¬Beli¬Canj(α)) →Afterj:αBeliϕ
Axiom NF corresponds to the following semantic constraint over L models. For any
i, j ∈AGT, α ∈ACT, and w ∈W:
S9
if there exists v such that (w, v) ∈Bi ◦Rj:α then Rj:α ◦Bi ⊆Bi ◦Rj:α.
where ◦is the standard composition operator between two binary relations. In accepting
the axiom NF, we suppose that events are always uninformative, that is, i should not
forget anything about the particular effects of j’s action α that starts at a world w. What
an agent i believes at a world w′, only depends on what i believed at the previous world
w and on the action which has occurred and which was responsible for the transition
from w to w′. Besides, the axiom NF relies on an additional assumption of complete
and correct information. It is supposed that j’s action α occurs if and only if every agent
is informed of this fact. Hence all action occurrences are supposed to be public.
Finally we have speciﬁc properties for informative actions. We suppose that if an
agent i is informed (resp. not informed) by another j that some fact P is true then i is
aware of being informed (resp. not being informed) by j.
PIntrInf
Infj,i(P) →BeliInfj,i(P)
NIntrInf
¬Infj,i(P) →Beli¬Infj,i(P)
Axioms PIntrInf and NIntrInf correspond to the following semantic constraints over
L models. For any i, j ∈AGT, infi(P) ∈INFO, and w ∈W:
S10
if Dj:infi(P ) is deﬁned at w then for all v ∈Bi(w), Dj:infi(P ) is deﬁned at v;
S11
if Dj:infi(P ) is not deﬁned at w then for all v ∈Bi(w), Dj:infi(P ) is not deﬁned
at v.
4 See, e.g., [14] for a discussion of this property.

88
E. Lorini and R. Demolombe
We call L the logic axiomatized by all propositional tautologies, and the axioms and
rules of inference MP, KBel, KGoal, KAfter, KDoes, DBel, DGoal, 4Bel, 5Bel, NecBel,
NecGoal, NecAfter, NecDoes, AltDoes, Active, IncDoes,After, IntAct1, IntAct2, Can-
Inf, WR, PIntrGoal, NIntrGoal, NF, PIntrInf and NIntrInf.
We write ⊢ϕ if formula ϕ is a theorem of L (i.e. ϕ is the derivable from the axioms
and rules of inference of the logic L). We write |= ϕ if ϕ is valid in all L models, i.e.
M, w |= ϕ for every L model M and world w in M. Finally, we say that ϕ is satisﬁable
if there exists a L model M and world w in M such that M, w |= ϕ. We can prove that
the logic L is sound and complete with respect to the class of L models. Namely:
Theorem 1. ⊢ϕ if and only if |= ϕ.
Proof. It is a routine task to check that the axioms of the logic L correspond one-
to-one to their semantic counterparts on the frames. In particular, Axioms DBel, 4Bel
and 5Bel correspond to the seriality, transitivity and Euclideanity of every relation Bi.
Axiom DGoal corresponds to the seriality of every relation Gi. Axiom AltDoes corre-
sponds to the constraint S1. Axiom Active corresponds to the constraint S2. Axiom
IncDoes,After corresponds to the constraint S3. Axioms IntAct1 and IntAct2 corre-
spond to the constraints S4 and S5. Axiom CanInf corresponds to the constraint S6.
Axiom WR corresponds to the semantic constraint S7. Axioms PIntrGoal and NIntr-
Goal correspond together to the constraint S8. Axiom NF corresponds to the constraint
S9. Axioms PIntrInf and NIntrInf correspond to the constraints S10 and S11.
It is routine, too, to check that all of our axioms are in the Sahlqvist class. This means
that the axioms are all expressible as ﬁrst-order conditions on frames and that they are
complete with respect to the deﬁned frames classes, cf. [5, Th. 2.42].
⊓⊔
3
A General Deﬁnition of Trust
In this work trust is conceived as a complex conﬁguration of mental states in which
there is both a motivational component and an epistemic component. More precisely,
we assume that an agent i’s trust in agent j necessarily involves a goal of the truster:
if agent i trusts agent j then, necessarily, i trusts j with respect to some of his goals.
The core of trust is a belief of the truster about some properties of the trustee, that
is, if agent i trusts agent j then necessarily i trusts j because i has some goal and
believes that j has the right properties to ensure that such a goal will be achieved. The
concept of trust formalized in this work is similar to the concept of trust deﬁned by
Castelfranchi & Falcone [15,8]. We agree with them that trust should not be seen as
an unitary and simplistic notion as other models implicitly suppose. For instance, there
are computational models of trust in which trust is conceived as an expectation of the
truster about a successful performance of the trustee sustained by the repeated direct
interactions with the trustee (under the assumption that iterated experiences of success
strengthen the truster’s conﬁdence) [23]. More sophisticated models of social trust have
been developed in which reputational information is added to information obtained via
direct interaction (e.g.[21,31]). All these models are in our view over-simpliﬁed since
they do not consider the beliefs supporting the truster’s evaluation of the trustee.
On this point we agree with Castelfranchi & Falcone on the fact that trust is based on
the truster’s evaluation of speciﬁc properties of the trustee (e.g.abilities, competencies,

From Trust in Information Sources to Trust in Communication Systems
89
dispositions, etc.) and of the environment in which the trustee is going to act, which
are relevant for the achievement of a goal of the truster. From this perspective, trust is
nothing more than the truster’s belief about some relevant properties of the trustee with
respect to a given goal. 5
3.1
Trust in the Trustee’s Action
The following is the concept of trust as an evaluation that interests us in this paper.
Deﬁnition 1. Trust in the Trustee’s Action. Agent i trusts j to do α with regard to his
goal that ϕ if and only if i wants ϕ to be true and i believes that:6
1. j, by doing α, will ensure that ϕ AND
2. j has the capacity to do α AND
3. j intends to do α.
The formal translation of Deﬁnition 1 is:
GoaliXϕ ∧Beli(Afterj:αϕ ∧Canj(α) ∧Intj(α)).
In our logic the conditions Canj(α) and Intj(α) together are equivalent to Doesj:α⊤
(by Axioms IncDoes,After, IntAct1 and IntAct2), so the deﬁnition of trust in the
trustee’s action can be simpliﬁed as follows:
Trust(i, j, α, ϕ)
def
= GoaliXϕ ∧Beli(Afterj:αϕ ∧Doesj:α⊤).
Trust(i, j, α, ϕ) is meant to stand for: i trusts j to do α with regard to his goal that ϕ.
Example 1. The two agents i and j are making a transaction in Internet. After having
paid j, i trusts j to send him a certain product with regard to his goal of having the
product in the next state:
Trust(i, j, send, hasProduct(i)).
This means that i wants to have the product in the next state:
GoaliX hasProduct(i).
Moreover, according to i’s beliefs, j, by sending him the product, will ensure that he
will have the product in the next state, and j is going to send the product:
Beli(Afterj:sendhasProduct(i)∧Doesj:send⊤).
The following theorem highlights the fact that if i trusts j to do α with regard to his
goal that ϕ then i has a positive expectation that ϕ will be true in the next state.
Theorem 2. Let i, j ∈AGT and α ∈ACT. Then:
⊢Trust(i, j, α, ϕ) →BeliXϕ
(1)
Proof. Trust(i, j, α, ϕ) implies Beli(Afterj:αϕ∧Doesj:α⊤). Afterj:αϕ∧Doesj:α⊤
implies Doesj:αϕ (by Axiom IncDoes,After and standard principles of the normal op-
erator Doesj:α). Doesj:αϕ implies Xϕ. We conclude that Beli(Afterj:αϕ∧Doesj:α⊤)
implies BeliXϕ (by Axiom KBel).
⊓⊔
5 In this paper we do not consider a related notion of decision to trust, that is, the truster’s
decision to bet and wager on the trustee and to rely on her for the accomplishment of a given
task. For a distinction between trust as an evaluation and trust as a decision, see [15,27].
6 In the present paper we only focus on full trust involving a certain belief of the truster. In order
to extend the present analysis to forms of partial trust, a notion of graded belief (i.e. uncertain
belief) or graded trust is needed.

90
E. Lorini and R. Demolombe
3.2
Trust in the Trustee’s Inaction
In our view trust in the trustee’s action must be distinguished from trust in the trustee’s
inaction. The former is focused on the domain of gains whereas the latter is focused on
the domain of losses. That is, in the former case the truster believes that the trustee is in
condition to further the achievement (or the maintenance) of a pleasant state of affairs,
and it will do that; in the latter case the truster believes that the trustee is in condition to
endanger the achievement (or the maintenance) of a pleasant state of affairs, but it will
refrain from doing that. Trust in the trustee’s inaction is based on the fact that, by doing
some action α, agent j can prevent i to reach his goal. In that case i expects that j will
not intend to do α.
Deﬁnition 2. Trust in the Trustee’s Inaction. Agent i trusts j not to do α with regard
to his goal ϕ if and only if i wants ϕ to be true and i believes that:
1. j, by doing α, will ensure that ¬ϕ AND
2. j has the capacity to do α AND
3. j does not intend to do α.
The formal deﬁnition of trust in the trustee’s inaction is given by the following abbre-
viation:
Trust(i, j, ∼α, ϕ)
def
= GoaliXϕ ∧Beli(Afterj:α¬ϕ ∧Canj(α) ∧¬Intj(α)).
Trust(i, j, ∼α, ϕ) stands for: i trusts j not to do α with regard to his goal that ϕ.
Example 2. Agent j is the webmaster of a public access website with ﬁnancial infor-
mation. Agent i is a regular reader of this website and he trusts j not to restrict the
access to the website with regard to his goal of having free access to the website:
Trust(i, j, ∼restrict, freeAccess(i)).
This means that, i has the goal of having free access to the website in the next state:
GoaliX freeAccess(i).
Moreover, according to i’s beliefs, j has the capacity to restrict the access to the website
and, by restricting the access to the website, j will ensure that i will not have free access
to the website in the next state, but j does not intend to restrict the access:
Beli(Afterj:restrict¬freeAccess(i) ∧Canj(restrict) ∧¬Intj(restrict)).
In this situation, i’s trust in j is based on i’s belief that j is in condition to restrict the
access to the website, but she does not have the intention to do this.
Note that, differently from agent i’s trust in agent j’s action, agent i’s trust in agent
j’s inaction with respect to the goal that ϕ does not entail i’s positive expectation that
ϕ will be true. Indeed, Trust(i, j, ∼α, ϕ) ∧¬BeliXϕ is satisﬁable in our logic. The
intuitive reason is that ¬ϕ may be the effect of another action than j : α.
In the following Sections 4 and 5 we will study the properties of information sources
and show how these properties can be evaluated by the truster in order to assess the
trustworthiness of an information source.
4
Some Basic Properties of an Information Source
We here consider four basic properties of an information source: validity, completeness,
sincerity and cooperativity.

From Trust in Information Sources to Trust in Communication Systems
91
Deﬁnition 3. Information Source Validity. Agent j is a valid information source about
P with regard to i if and only if, after j does the action of informing i about P, it is the
case that P.
Formally: Val(j, i, P)
def
= Afterj:infi(P )P.
Deﬁnition 4. Information Source Completeness. Agent j is a complete information
source about P with regard to i if and only if, if P is true then j does the action of
informing i about P.
Formally: Compl(j, i, P)
def
= P →Infj,i(P).
Deﬁnition 5. Information Source Sincerity. Agent j is a sincere information source
about P with regard to i if and only if, after j does the action of informing i about P, it
is the case that j believes P.
Formally: Sinc(j, i, P)
def
= Afterj:infi(P )BeljP.
Deﬁnition 6. Information Source Cooperativity. Agent j is a cooperative information
source about P with regard to i if and only if, if j believes that P then j informs i about
P.
Formally: Coop(j, i, P)
def
= BeljP →Infj,i(P).
Example 3. Consider an example in the ﬁeld of stocks and bonds market. The agent
BUG is the Bank of Union of Groenland. Sue Naive (SN) and Very Wise (VW) are two
BUG’s customers. BUG plays the role of an information source for the customers, for
instance for the facts p: “it is recommended to buy MicroHard stocks”, and q: “Micro-
hard stocks are dropping”. SN believes that BUG is sincere with regard to her about
p, because SN believes that BUG wants to help its customers and BUG has a long
experience in the domain. SN also believes that BUG is cooperative with regard to
her about q because q is a relevant information for customers in order to make de-
cisions. VW does not believe that BUG is sincere with regard to him about p. In-
deed, VW believes that BUG wants that VW buys Microhard stocks, even if this is
not proﬁtable for VW. This example is formally represented by the following formula:
BelSN (Sinc(BUG, SN , p) ∧Coop(BUG, SN , q)) ∧¬BelVW Sinc(BUG, VW , p).
5
Trust in Information Sources
We conceive trust in information sources as a speciﬁc instance of the general notion of
trust in the trustee’s action deﬁned in Section 3. In our view, the relevant aspect of trust
in information sources is the content of the truster’s goal. In particular, we suppose that
an agent i trusts the information source j to inform him whether the fact P is true only
if i has the epistemic goal of knowing whether P is true and believes that, due to the in-
formation transmitted by j, he will achieve this goal. In this sense, trust in information
sources is characterized by an epistemic goal of the truster and an informative action of
the trustee. The concept of epistemic goal can be deﬁned from the following standard

92
E. Lorini and R. Demolombe
deﬁnitions of knowing that (i.e. as having the correct belief that something is the case)
and knowing whether:
Kiϕ
def
= Beliϕ ∧ϕ
Kwiϕ
def
= Kiϕ ∨Ki¬ϕ
where Kiϕ stands for “agent i knows that ϕ is true” and, Kwiϕ stands for “i knows
whether ϕ is true”. An epistemic goal of an agent i is i’s goal of knowing the truth
value of a certain formula. Formally, GoaliKwiϕ denotes i’s epistemic goal of knowing
whether ϕ is true now; GoaliX Kwiϕ denotes i’s epistemic goal of knowing whether ϕ
is true in the next state.
Our aim in this section of the paper is to investigate the relationships between trust in
information sources and the properties of information sources deﬁned in Section 4. The
following Theorem 3 highlights the relationship between trust in information sources
and the properties of validity and completeness of information sources. It says that: if i
believes that j is a valid information source about p and ¬p with regard to i and that j is
a complete information source about p and ¬p with regard to i and, i has the epistemic
goal of knowing whether p is true then, either i trusts the information source j to inform
him that p is true or i trusts the information source j to inform him that ¬p is true (with
regard to his epistemic goal of knowing whether p is true).
Theorem 3. Let i, j ∈AGT and infi(p), infi(¬p) ∈INFO, then:
⊢(Beli(Val(j, i, p) ∧Val(j, i, ¬p)) ∧Beli(Compl(j, i, p) ∧Compl(j, i, ¬p))∧
GoaliX Kwip) →(Trust(i, j, infi(p), Kwip) ∨Trust(i, j, infi(¬p), Kwip)) (2)
Proof. We give a sketch of the proof. By Axioms KBel, PIntrInf and NIntrInf,
Beli(Compl(j, i, p) ∧Compl(j, i, ¬p)) implies BeliInfj,i(p) ∨BeliInfj,i(¬p).
By Axioms KBel, NecBel, CanInf, DBel, 4Bel, 5Bel and NF, BeliInfj,i(p)∨
BeliInfj,i(¬p) and Beli(Val(j, i, p) ∧Val(j, i, ¬p)) and GoaliX Kwip imply (GoaliX
Kwip∧BeliAfterj:infi(p)Kwip∧BeliInfj,i(p))∨(GoaliX Kwip∧Beli Afterj:infi(¬p)
Kwip ∧BeliInfj,i(¬p)). The latter is equivalent to Trust(i, j, infi(p), Kwip)∨
Trust(i, j, infi(¬p), Kwip).
⊓⊔
Example 4. Let us consider again the example of stocks and bonds market. SN has the
epistemic goal of knowing whether q (“Microhard stocks are dropping”) is true:
GoalSN X KwSN q.
SN believes that BUG is a valid information source with regard to her both about q and
about ¬q and that BUG is a complete information source with regard to her both about
q and about ¬q:
BelSN (Val(BUG, SN , q) ∧Val(BUG, SN , ¬q))∧
BelSN (Compl(BUG, SN , q) ∧Compl(BUG, SN , ¬q)).
Then, by Theorem 3, we can infer that either SN trusts the information source BUG to
inform her that q is true or SN trusts the information source BUG to inform her that ¬q
is true (with regard to her epistemic goal of knowing whether q is true):

From Trust in Information Sources to Trust in Communication Systems
93
Trust(SN , BUG, infSN(q), KwSN q) ∨Trust(SN , BUG, infSN(¬q), KwSN q).
Finally, by Theorem 2, we can infer that SN believes that in the next state she will
achieve her goal of knowing whether q is true:
BelSN X KwSN q.
In the following two Sections 6 and 7 we will shift the focus of analysis from infor-
mation sources to communication systems. We will study some important properties of
communication systems and show how these properties can be evaluated by the truster
in order to assess the trustworthiness of a communication system.
6
Some Basic Properties of a Communication System
We suppose that the fundamental properties of a communication system j can be de-
ﬁned in terms of two facts: the communication system j satisﬁes an agent i’s goal that a
certain information will be transmitted to another agent z or, the communication system
j satisﬁes an agent i’s goal that a certain information will not be transmitted to another
agent z. In the former case we say that the communication system j is available to i
to transmit the information. In the latter case we say that the communication system j
ensures to i the privacy of the information. For simpliﬁcation, we ignore in this work
other properties of communication systems like authentication and integrity.
Deﬁnition 7. Communication System Availability. The communication system j is
available to agent i to transmit the information P to agent z if and only if, if j be-
lieves that i wants that j informs z about P then j informs z about P.
Formally: Avail(j, i, z, P)
def
= BeljGoaliInfj,z(P) →Infj,z(P).
Deﬁnition 8. Communication System Privacy. The communication system j ensures
to agent i the privacy of information P from agent z if and only if, if j believes that i
wants that j does not inform z about P then, j does not inform z about P.
Formally: Priv(j, i, z, P)
def
= BeljGoali¬Infj,z(P) →¬Infj,z(P).
Example 5. Let us consider an example in the ﬁeld of Web services. An agent called
Bill decides to use a Hotel Booking Service (HBS) in Internet in order to book a double
room at the Hotel Colosseum (HC) in Rome. Bill’s decision is affected by two beliefs of
Bill, the belief that HBS ensures the privacy from a potential intruder of the information
r: “Bill’s credit card number is 01234567891234”, the belief that HBS is available to
inform HC about s: “Bill has made an online reservation”. According to our deﬁnitions,
this example is formally represented by:
BelBillPriv(HBS, Bill, intruder, r) ∧BelBillAvail(HBS, Bill, HC , s).
7
Trust in Communication Systems
We conceive trust in communication systems as a speciﬁc instance of the notion of trust
deﬁned in Section 3. On the one hand, we suppose that an agent i trusts the commu-
nication system j to inform agent z about P with regard to his goal that z believes P

94
E. Lorini and R. Demolombe
(trust in a communication system’s action) if and only if, i has the goal that z believes
P and i believes that, due to the information transmitted by j to z, z will believe P.
On the other hand, we suppose that an agent i trusts the communication system j not
to inform agent z about P with regard to his goal that z does not believe P (trust in a
communication system’s inaction) if and only if, i has the goal that z does not believe
P, i believes that, by informing z about P, j will ensure that z believes P but, i believes
that j does not intend to inform z about P. In this sense, i’s trust in the communication
system j’s action (resp. inaction) is characterized by i’s goal that a certain information
will be transmitted (resp. will not be transmitted) to another agent z so that z will have
access (resp. will not have access) to this information.
Our aim in this section of the paper is to investigate the relationships between trust
in communication systems and the two properties of communication systems deﬁned
in Section 6. The following Theorem 4 highlights the relationship between trust in a
communication system’s action and the availability of the communication system. It
says that: if i has the goal that in the next state z will believe P, i believes that j is
available to inform z about P, i believes that j believes that i wants j to inform z about
P and, i believes that z believes that j is a valid information source about P then, i
trusts j to inform z about P with regard to his goal that z will believe P.
Theorem 4. Let i, j, z ∈AGT and infz(P) ∈INFO, then:
⊢(GoaliX BelzP ∧BeliAvail(j, i, z, P) ∧BeliBeljGoaliInfj,z(P)∧
BeliBelzVal(j, z, P)) →Trust(i, j, infz(P), BelzP)
(3)
Proof. We give a sketch of the proof. By KBel, NecBel, DBel, CanInf, and NF,
BeliAvail(j, i, z, P) ∧BeliBeljGoaliInfj,z(P) ∧BeliBelzVal(j, z, P) implies
BeliInfj,z(P) ∧BeliAfterj:infz(P )BelzP.
The latter is equivalent to Trust(i, j, infz(P), BelzP).
⊓⊔
Example 6. Let us consider again the example of the Hotel Booking Service (HBS).
Bill has the goal that the receptionist at the Hotel Colosseum (HC) believes that s (“Bill
has made an online reservation”):
GoalBillX BelHCs.
Bill believes that HBS is available to inform the HC’s receptionist about s and that HBS
believes that Bill wants HBS to inform the HC’s receptionist about s:
BelBillAvail(HBS, Bill, HC, s) ∧BelBillBelHBSGoalBillInfHBS,HC(s).
Bill also believes that the HC’s receptionist believes that HBS is a valid information
source about s:
BelBillBelHCVal(HBS, HC, s).
Then, from Theorem 4, we can infer that Bill trusts HBS to inform the HC’s receptionist
about s with regard to his goal that the HC’s receptionist will believe s:
Trust(Bill, HBS, infHC(s), BelHCs).
The following Theorem 5 highlights the relationship between trust in a communication
system’s inaction and the fact that the communication system ensures privacy. It says
that: if i has the goal that z does not believe P, i believes that j ensures the privacy of
information P from agent z, i believes that j believes that i wants that j does not inform

From Trust in Information Sources to Trust in Communication Systems
95
z about P and, i believes that z believes that j is a valid information source about P
then, i trusts j not to inform z about P with regard to his goal that z does not believe P.
Theorem 5. Let i, j, z ∈AGT and infz(P) ∈INFO, then:
⊢(GoaliX¬BelzP ∧BeliPriv(j, i, z, P) ∧BeliBeljGoali¬Infj,z(P)∧
BeliBelzVal(j, z, P)) →Trust(i, j, ∼infz(P), ¬BelzP)
(4)
Proof. We give a sketch of the proof. By KBel, NecBel, DBel, CanInf and NF,
BeliPriv(j, i, z, P) ∧BeliBeljGoali¬Infj,z(P) ∧BeliBelzVal(j, z, P) implies
Beli¬Intj(infz(P)) ∧BeliCanj(infz(P)) ∧BeliAfterj:infz(P )BelzP.
The latter is equivalent to Trust(i, j, ∼infz(P), ¬BelzP).
⊓⊔
Example 7. In this version of the scenario of the Hotel Booking Service (HBS) Bill has
the goal that a potential intruder will not have access to the information r (“Bill’s credit
card number is 01234567891234”):
GoalBillX¬Belintruderr.
Bill believes that HBS ensures the privacy of information r from potential intruders and
that HBS believes that Bill wants that HBS does not inform a potential intruder about
r:
BelBillPriv(HBS, Bill, intruder, r) ∧BelBillBelHBSGoalBill¬InfHBS,intruder(r).
Finally, Bill believes that every potential intruder believes that HBS is a valid informa-
tion source about credit card numbers:
BelBillBelintruderVal(HBS, intruder, r).
Then, from Theorem 5, we can infer that Bill trusts HBS not to inform a potential in-
truder about r with regard to his goal that a potential intruder will not believe r:
Trust(Bill, HBS, ∼infintruder(r), ¬Belintruderr).
8
Related Works
Several logical models of trust in information sources have been proposed in the recent
literature [24,12,11]. Some of them take the concept of trust as a primitive [24], whereas
others reduce trust to a kind of belief of the truster [12]. All these logical models do
not investigate the epistemic supports for this form of trust. Moreover, they do not
consider the motivational aspect of trust. In the present paper both aspects have been
taken into account. On the one hand, we have modeled the properties of information
sources such as sincerity, validity, completeness and cooperativity and shown that some
of them are epistemic supports for an agent’s trust in an information source, that is, they
are sufﬁcient conditions for trusting an information source to inform whether a certain
fact is true (Theorem 3). On the other hand, we have modeled the truster’s epistemic
goal of knowing whether a certain fact is true.
More generally, the truster’s goal is for us a necessary constituent of the deﬁnition of
trust (see [13,15] for a similar view of trust). In the deﬁnitions of trust in the trustee’s
action and trust in the trustee’s inaction proposed in Section 3, i’s trust in j necessarily
involves a main and primary motivational component which is a goal of the truster. If
i trusts j then, necessarily i trusts j because i has some goal and thinks that j has the

96
E. Lorini and R. Demolombe
right properties to ensure that such a goal will be achieved. The goal component in the
deﬁnition of trust is for us fundamental since it allows to distinguish trust from mere
thinking and foreseeing. In this sense, our logical approach to trust is different from the
approach proposed by Jones [22] in which the motivational aspect is not considered to
be necessary for deﬁning trust, and trust is characterized only in terms of two beliefs
of the truster: the truster’s belief that a certain rule or regularity applies to the trustee
(called “rule belief”), and the truster’s belief that the rule or regularity is going to be
followed by the trustee (called “conformity belief”).
As far as communication systems are concerned, there are several logical models in
the literature which deal with the security properties of a communication system such as
privacy (or conﬁdentiality), availability, integrity, authentication (see e.g.[10,7]). Nev-
ertheless, there is still no formal analysis of the relationships between these security
properties of a communication system and trust in the communication system. In this
work such relationships have been clariﬁed (Theorems 4 and 5). It is also to be noted
that the deﬁnitions of privacy and availability given in Section 6 are based on an agent
i’s goal that another agent j will inform (resp. will not inform) a third agent z about a
certain fact P. Thus, our deﬁnitions are different from the deﬁnitions proposed in the
security domain in which privacy and availability are traditionally expressed in terms
of normative concepts like authorization and permission. According to [4] for instance
privacy (or conﬁdentiality) means that “(...) data may only be learned by authorized
users”. According to [18], privacy (or conﬁdentiality) “(...) is the concept of: 1) ensur-
ing that information is accessible for reading, listening, recording or physical removal
only to subjects entitled to it, and 2) that subjects only read or listens to the informa-
tion to the extent permitted”. A variant of conﬁdentiality based on normative concepts
and closer to the deﬁnition of conﬁdentiality proposed in the security domain could be
expressed in our logical framework. To this aim, it would be sufﬁcient to substitute the
goal modality Goali with an obligation modality Obl of Standard Deontic Logic [1] in
the deﬁnition given in Section 6. That is:
Priv(j, z, P)
def
= BeljObl¬Infj,z(P) →¬Infj,z(P).
According to this 3-argument deﬁnition, the communication system j ensures the pri-
vacy of information P from agent z if and only if, if j believes that it is obligatory that
he does not inform z about P then, j does not inform z about P. This deﬁnition can be
expressed in an equivalent form by the notion permission, the dual of obligation:
Priv(j, z, P) ↔(Belj¬Perm Infj,z(P) →¬Infj,z(P)).
That is, the communication system j ensures the privacy of information P from agent
z if and only if, if j believes that it is not permitted that he informs z about P then, j
does not inform z about P.
9
Conclusion
We have presented in a modal logical framework a model that integrates in the deﬁni-
tion of trust: the truster’s goal, the trustee’s action that ensures the achievement of the
truster’s goal, and the trustee’s ability and intention to do this action. In the same logical
framework we have deﬁned several properties of information sources (validity, com-
pleteness, sincerity and cooperativity) and discussed their relationships with an agent’s

From Trust in Information Sources to Trust in Communication Systems
97
trust in an information source. In the last part of the paper, we have investigated some
properties of communication systems (availability and privacy) and discussed their re-
lationships with an agent’s trust in a communication system. It is to be noted that, due to
the complexity of the concepts involved in our analysis of trust, we had to accept strong
simpliﬁcations. For instance, in the deﬁnitions of the properties of information sources
and communication systems entailment is formalized by a material implication, while
some form of conditional might be more adequate. Moreover, in the future we intend to
provide a more ﬁne-grained semantics of the notion of informative action. We think that
formalizing informative actions as private announcements in the sense of [2] would be
an interesting improvement of the logical theory of trust presented in this paper.
Acknowledgements
This research has been supported by the project ForTrust “Social trust analysis and for-
malization” ANR-06-SETI-006 ﬁnanced by the French institution “Agence Nationale
de la Recherche”.
References
1. ˚Aqvist, L.: Deontic logic. In: Gabbay, D.M., Geunther, F. (eds.) Handbook of Philosophical
Logic. Kluwer Academic Publishers, Dordrecht (2002)
2. Baltag, A., Moss, L., Solecki, S.: The logic of public announcements, common knowledge
and private suspicions. In: Proceedings of the Seventh Conference on Theoretical Aspects
of Rationality and Knowledge (TARK 1998), pp. 43–56. Morgan Kaufmann, San Francisco
(1998)
3. Berners-Lee, T., Hendler, J., Lassila, O.: The semantic web. Scientiﬁc American 284(5), 34–
43 (2001)
4. Bieber, P., Cuppens, F.: Expression of conﬁdentiality policies with deontic logic. In: Deontic
logic in computer science: normative system speciﬁcation, pp. 103–123. John Wiley and
Sons, Chichester (1993)
5. Blackburn, P., de Rijke, M., Venema, Y.: Modal Logic. Cambridge University Press,
Cambridge (2001)
6. Bratman, M.: Intentions, plans, and practical reason. Harvard University Press (1987)
7. Burrows, M., Abadi, M., Needham, R.M.: A logic of authentication. ACM Transactions on
Computer Systems 8(1), 18–36 (1990)
8. Castelfranchi, C., Falcone, R.: Principles of trust for MAS: Cognitive anatomy, social impor-
tance, and quantiﬁcation. In: Proceedings of the Third International Conference on Multia-
gent Systems (ICMAS 1998), pp. 72–79 (1998)
9. Cohen, P.R., Levesque, H.J.: Intention is choice with commitment. Artiﬁcial Intelligence 42,
213–261 (1990)
10. Cuppens, F., Demolombe, R.: A Deontic Logic for Reasoning about Conﬁdentiality. In:
Proceedings of Third International Workshop on Deontic Logic in Computer Science
(DEON 1996), Workshops in Computing, pp. 66–79. Springer, Heidelberg (1996)
11. Dastani, M., Herzig, A., Hulstijn, J., van der Torre, L.: Inferring trust. In: Leite, J., Torroni,
P. (eds.) CLIMA 2004. LNCS (LNAI), vol. 3487, pp. 144–160. Springer, Heidelberg (2005)
12. Demolombe, R.: To trust information sources: a proposal for a modal logical framework.
In: Castelfranchi, C., Tan, Y.-H. (eds.) Trust and Deception in Virtual Societies. Kluwer,
Dordrecht (2001)

98
E. Lorini and R. Demolombe
13. Deutsch, M.: Trust and suspicion. The Journal of Conﬂict Resolution 2(4), 265–279 (1958)
14. Fagin, R., Halpern, J., Moses, Y., Vardi, M.: Reasoning about Knowledge. MIT Press,
Cambridge (1995)
15. Falcone, R., Castelfranchi, C.: Social trust: A cognitive approach. In: Castelfranchi, C., Tan,
Y.H. (eds.) Trust and Deception in Virtual Societies, pp. 55–90. Kluwer, Dordrecht (2001)
16. Fasli, M.: Agent Technology for E-commerce. Wiley & Sons, Chichester (2007)
17. Gerbrandy, J., Groeneveld, W.: Reasoning about information change. Journal of Logic, Lan-
guage, and Information 6, 147–196 (1997)
18. Hammer, J.H., Schneider, G.: On the Deﬁnition and Policies of Conﬁdentiality. In: Pro-
ceedings of the Third International Symposium on Information Assurance and Security
(IAS 2007), pp. 337–342. IEEE Computer Society, Los Alamitos (2007)
19. Harel, D., Kozen, D., Tiuryn, J.: Dynamic Logic. MIT Press, Cambridge (2000)
20. Herzig, A., Lorini, E., Hubner, J.F., Ben-Naim, J., Castelfranchi, C., Demolombe, R., Longin,
D., Vercouter, L.: Prolegomena for a logic of trust and reputation. In: Proceedings of the Third
International Workshop on Normative Multiagent Systems, NorMAS 2008 (2008)
21. Huynh, T.G., Jennings, N.R., Shadbolt, N.R.: An integrated trust and reputation model for
open multi-agent systems. Journal of Autonomous Agent and Multi-Agent Systems 13, 119–
154 (2006)
22. Jones, A.J.I.: On the concept of trust. Decision Support Systems 33(3), 225–232 (2002)
23. Jonker, C.M., Treur, J.: Formal analysis of models for the dynamics of trust based on experi-
ences. In: Garijo, F.J., Boman, M. (eds.) MAAMAW 1999. LNCS, vol. 1647, pp. 221–231.
Springer, Heidelberg (1999)
24. Liau, C.J.: Belief, information acquisition, and trust in multi-agent systems: a modal logic
formulation. Artiﬁcial Intelligence 149, 31–60 (2003)
25. Lorini, E., Demolombe, R.: Trust and norms in the context of computer security. In: van
der Meyden, R., van der Torre, L. (eds.) DEON 2008. LNCS (LNAI), vol. 5076, pp. 50–64.
Springer, Heidelberg (2008)
26. Lorini, E., Herzig, A.: A logic of intention and attempt. Synthese 163(1), 45–77 (2008)
27. Marsh, S.: Formalising Trust as a Computational Concept. PhD thesis, University of Stirling
(1994)
28. Mcllraith, S.A., Son, T.C., Zeng, H.: Semantic web services. IEEE Intelligent Systems 16(2),
46–53 (2001)
29. Meyer, J.-J.Ch., van der Hoek, W., van Linder, B.: A logical approach to the dynamics of
commitments. Artiﬁcial Intelligence 113(1-2), 1–40 (1999)
30. Rao, A.S., Georgeff, M.P.: Modelling rational agents within a BDI-architecture. In: Proceed-
ings of the 2nd International Conference on Principles of Knowledge Representation and
Reasoning (KR 1991), pp. 473–484. Morgan Kaufmann, San Francisco (1991)
31. Sabater, J., Sierra, C.: Regret: a reputation model for gregarious societies. In: Proceedings
of the First International Joint Conference on Autonomous Agents and Multi-Agent Systems
(AAMAS 2002), pp. 475–482. ACM Press, New York (2001)

Pre-processing Techniques for Anytime Coalition
Structure Generation Algorithms
Tomasz Michalak, Andrew Dowell, Peter McBurney, and Michael Wooldridge
Department of Computer Science,
The University of Liverpool, UK
{tomasz,adowell,mcburney,mjw}@liv.ac.uk
Abstract. This paper is concerned with optimal coalition structure generation in
multi-agent systems. For characteristic function game representations, we pro-
pose pre-processing techniques, presented in the form of ﬁlter rules, that reduce
the intractability of the coalition structure generation problem by identifying
coalitions which cannot belong to any optimal structure. These ﬁlter rules can
be incorporated into many potential anytime coalition structure generation algo-
rithms but we test the effectiveness of these ﬁlter rules in the sequential applica-
tion of the distributed coalition value calculation algorithm (DCVC) [1] and the
anytime coalition structure generation algorithm of Rahwan et al. (RCSG) [2].
The distributed DCVC algorithm provides an input to the centralised RCSG al-
gorithm and we show that, for both normal and uniform distributions of coalition
values, the proposed ﬁlter rules reduce the size of this input by a considerable
amount. For example, in a system of 20 agents, fewer than 5% of all coalition
values have to be input, compared to more than 90% when ﬁlter rules are not
employed. Furthermore, for a normal distribution of coalition values, the running
time of the RCSG algorithm exponentially accelerates as a consequence of the
signiﬁcantly reduced input size. This pre-processing technique bridges the gap
between the distributed DCVC and centralised RCSG algorithms and is a natural
benchmark to develop a distributed CSG algorithm.1
1
Background and Problem Deﬁnition
In multi-agent systems (MAS), coalition formation occurs when distinct autonomous
agents group together to achieve something more efﬁciently than they could accom-
plish individually. Typically, coalition formation is often studied using characteristic
function games (CFG). These representations consist of a set of agents A and a char-
acteristic function v, which assigns a numerical value to every feasible coalition C ⊆A,
reﬂecting the effectiveness of the co-operation within each coalition of agents . In this
convenient but simpliﬁed representation, it is assumed that the performance of any one
coalition is independent from other co-existing coalitions in the system. In other words,
every coalition C has the same value in every structure CS to which it belongs. Evi-
dently, the CFG representation is a special case of the more general partition function
1 The authors are grateful for ﬁnancial support received from the UK EPSRC through the project
Market-Based Control of Complex Computational Systems (GR/T10657/01).
J.-J.Ch. Meyer and J.M. Broersen (Eds.): KRAMAS 2008, LNAI 5605, pp. 99–113, 2009.
c⃝Springer-Verlag Berlin Heidelberg 2009

100
T. Michalak et al.
game (PFG) representation in which the value of any coalition depends on other co-
operational arrangements between the agents in the whole system [3].
One of the main challenges in co-operative MAS is to determine which exhaustive
division of agents into disjoint coalitions (referred to as a coalition structure (CS) from
now on) maximizes the total payoff to the system. This complex research issue is re-
ferred to as the coalition structure generation (CSG) problem and since the number
of structures increases exponentially as the number of agents increases linearly (for ex-
ample, there are 190, 899, 322 possible structures for 14 agents compared to 1.3 billion
structures for 15 agents), the problem of computing an optimal CS becomes time con-
suming, even for a moderate number of agents. Consequently, much of the research
into the CSG problem has focused on generating an optimal CS by evaluating as few
coalition structures as possible. This line of research can be divided into three broad
categories: (i) limiting the size of coalitions that can be formed [4], (ii) reducing the
number of structures that need to be searched at the expense of accuracy [5,6], and (iii)
proposing algorithms which take the advantage of the CFG representation to vastly re-
duce the time taken to generate an optimal CS. Since we are interested in generating
optimal solutions in an unconstrained domain, in this paper, we will focus exclusively
on the third approach.
There are two general classes of CSG algorithms. Yen [7] proposed an algorithm
based on dynamic programming (DP) methods. The advantage of this approach is that
it outputs an optimal structure without comparing any of them. However, one disadvan-
tage is that it only outputs an optimal structure after it has completed its entire execution,
meaning such methods are not appropriate when the time required to return an optimal
solution is longer than the time available to the agents. To circumvent these problems,
Rahwan et al. [2] proposed an anytime CSG algorithm which divides the space of all
structures (denoted Π from now on) into sub-spaces consisting of coalition structures
which are identical w.r.t. the sizes of the coalitions involved. Using this representation,
and taking, as input, all feasible coalition values, this algorithm uses statistical informa-
tion, computed from the input coalition values, to determine which of the sub-spaces
are ‘promising’, i.e., which of them may contain an optimal structure. The algorithm
then searches these ‘promising’ subspaces, once again, using statistical information to
avoid generating structures which cannot be optimal. This methodology exploits the
fact that in CFGs, every coalition C has the same value in every structure CS to which
it belongs. Since it is possible to utilize statistical data computed from coalition values
to reason about the values of coalition structures, in this paper, we propose a number of
pre-processing techniques. These techniques are represented in the form of ﬁlter rules
which have syntax: condition →action, with the interpretation being that all coalition
values which meet the requirements of the condition cannot belong to an optimal struc-
ture and so an appropriate action is performed.
Typically, such actions involve ﬁltering coalition values from the input, or ﬁltering all
structures containing these coalitions from the search-space. Filtering coalition values
from the input is important for two reasons. Firstly, it reduces the number of coalition
values an individual agent needs to transfer if the coalition value calculation process
is distributed - as in the DCVC algorithm of Rahwan and Jennings [1]. Secondly, it
automatically reduces the search space as fewer coalition structures can be created from

Pre-processing Techniques for Anytime Coalition Structure Generation Algorithms
101
the input. To test the effectiveness of our approach, we compare the sequential operation
of the DCVC and Rahwan et al. anytime algorithm both with and without the ﬁlter
rules. Following the MAS literature, we focus on normal and uniform distributions of
coalition values [2] and show that our ﬁlter rules:
– always signiﬁcantly reduce the size of input (from 90% to 5% and 3% for normal
and uniform distributions, respectively); and,
– exponentially reduce the time needed to search promising subspaces for a normal
distribution whereas they do not affect the performance of the anytime CSG algo-
rithm for a uniform distribution.
2
A Pre-processing Approach to Solve the CSG Problem
Let A = {1, . . . , n} be the set of all agents in the system. Since more than one structure
can maximize the value of a system, the output of the CSG process may consist of a
set of optimal coalition structures, denoted by {CS∗}. As of now, the CSG literature
for CFG representations has exclusively focused on ﬁnding a single optimal coalition
structure, denoted by CS∗⊆{CS∗} [7,2]. Usually, the choice of CS∗⊆{CS∗} for
|{CS∗}| ≥2 is made in an ad hoc manner, e.g., the optimal coalition structure output is
the ﬁrst CS with maximal value which the algorithm encounters. However, there may be
other factors which, although not displayed in the characteristic function, can be used to
determine if one structure is better than another and so we consider the CSG({CS∗})
problem from now onward. We will denote the set of all feasible coalitions that can be
created in the system by ϝ and the corresponding set of all coalition values by V (ϝ).
The most important property of CFG representation is that, as opposed to the general
PFG representation, the value of any coalition is independent from the formation of
other distinct coalitions in the system. In other words, in a system of n = |A| agents,
for any coalition C ⊆A, the value of this coalition v(C) is the same in every possible
structure CS where C ∈CS. Thus, if it can be shown that the value v(C) is too small
for C to be in any optimal structure of the system then, clearly, any structure containing
C cannot be in the optimal set and can be disregarded. Similarly, if it is proven that
the combined value of a group of disjoint coalitions it too small for this group to be
in any optimal structure then any structure simultaneously containing every single one
of these coalitions can be disregarded. The above discussion can be summarized in the
following lemma which holds for CFG representations:
Lemma 1. For any non-trivial2 coalition C that can be divided into k disjoint sub-
coalitions C1, . . . , Ck where C1 ∪C2 . . . ∪Ck = C; if v(C1) + . . . + v(Ck) > v(C)
then C ̸∈CS∗and so ∀CS : C ∈CS, CS ̸∈{CS∗}.
Proof. Consider any coalition structure CS containing coalition C. If CS is an optimal
structure (belongs to {CS∗}) then no other structure can have a value greater than the
2 All coalitions of more than two agents will sometimes be referred to as non-trivial coalitions.
In contrast, singletons, i.e., agents acting on their own, will sometimes be referred to as trivial
coalitions.

102
T. Michalak et al.
value of CS. However, if C can be divided into k sub-coalitions C1, . . . , Ck where
C1 ∪C2 . . .∪Ck = C and |Ci| ≥1 ∀i = 1, . . . , k such that v(C1)+. . . v(Ck) > v(C)
then clearly the structure CS′ = CS \ {C} ∪{C1 ∪. . . ∪Ck} has a value greater
than the value of CS. Therefore CS, where CS is any structure containing coalition C,
cannot be an optimal structure and ∀CS : C ∈CS, CS does not belong to {CS∗}.
For example, in a system of ﬁve agents A = {a1, a2, a3, a4, a5} if the value of the
coalition {a1, a2} is less than the sum of the values of the individual coalitions {a1}
and {a2} then it is clear that any structure CSa = {a1, a2} ∪CS′, where CS′ is a
speciﬁc structure of the agents {a3, a4, a5}, must have value less than the structure
CSb = {a1}, {a2} ∪CS′. Consequently, any structure CS which contains coalition
{a1, a2} cannot be optimal and need not be considered when generating an optimal CS.
Existing CSG algorithms take advantage of this characteristic. For example, in the
anytime CSG algorithm of Rahwan et al. described in Section 1 some subspaces of
coalition structures are pruned away from the search space before the search process
has begun. Using statistical information obtained from the input of coalition values, it
is decided a priori if certain coalition structures cannot be optimal. Similarly, in the
process of searching promising subspaces, certain search directions are identiﬁed a
priori as those that cannot lead to an improved outcome. Examples include dynamic
programming (DP) CSG algorithms [7] in which it is evaluated whether a decompo-
sition a coalition C into exactly two smaller coalitions of all the agents in C would
be proﬁtable. In this spirit, the improved dynamic programming (IDP) algorithm pre-
sented in [8] considers more dissociations than just the dissociation of a coalition into
two disjoint coalitions. This approach, which is yet another application of Lemma 1,
turns out to be more efﬁcient in terms of both time and memory costs than conventional
DP algorithms.
Consider a system of agents where a value of a non-trivial coalition C := {a1, a2,
a3, a4} is 7. In order to prove that C cannot be in an optimal CS, it is not always
necessary to show that any partition of this coalition has a combined value greater than
7. In the spirit of Lemma 1, it may also be sufﬁcient to show that the values of a strict
subset of k disjoint coalitions are greater than v(C). For instance, it may be the case
that v({a1})+v({a3, a4}) ≥v({a1, a2, a3, a4}). Following this intuition, we can relax
some of the assumptions in Lemma 1 as follows:
Lemma 2. For any non-trivial coalition C that can be divided into k disjoint sub-
coalitions C1, . . . , Ck where C1 ∪C2 . . . ∪Ck = C; if j
i=1 v(Ci) > v(C) where
j < k then C ̸∈CS∗and so ∀CS : C ∈CS, CS does not belong to {CS∗}.
Theoretically, every feasible non-trivial coalition could be decomposed into all possible
combinations of sub-coalitions; however, this is a completely inefﬁcient approach. After
all, such a decomposition of the grand coalition yields Π. In the remainder of this
paper, we will show that an appropriate application of both Lemmas 1 and 2 may still
considerably speed up the CSG process in the state-of-the-art CSG algorithm.
Firstly, we will extend Lemma 1 so that it can be applied not only to a particular
coalition but to collections of coalitions which have been grouped together w.r.t. some
criteria. One natural criterion to group coalitions is size. Let all coalitions of the same
size be grouped together in |A| sets C1, . . . , Ci, . . . , C|A| where Ci denotes the set of

Pre-processing Techniques for Anytime Coalition Structure Generation Algorithms
103
all coalitions of size i. For example, for A = {1, . . ., 5} there will be |A| = 5 sets
C1, . . . , C5 where C1 contains all coalitions of size 1, C2 all coalitions of size 2, etc.
Additionally, in this example, suppose that the coalitions with smallest values in C1 and
C2 are {a1} and {a2, a3}, respectively. This means that any decomposition of coalition
of size 3 will not have a smaller value than v({a1})+v({a2, a3}). Consequently, if any
coalition from C3 has a value smaller than v({a1})+v({a2, a3}) then we can disregard
this coalition as, following Lemma 1, it cannot be in an optimal structure. We extend
Lemma 1 as follows:
Lemma 3. Let Ci denote the (complete) set of all coalitions of size i. For any set
Zi ⊆Ci of coalitions of size i and for a particular integer partition p = i1, . . . , ik of i,
(i.e.,i1+. . .+ik=i) if the sum of the lowest coalition values in sets Ci1, . . . , Cik (denoted
di(p)) is strictly greater than the maximum value in set Zi then no coalition from set
Zi can be in an optimal structure. More formally, if di(p) := k
i=1 min Cik > max Zi
then ∀CS : C ∈CS and C ∈Zi it holds that CS /∈{CS∗}.
Proof. Suppose that coalition Ci is the coalition with the biggest value in set Zi ⊆Ci
and coalitions Cil,. . ., Cik are the coalitions with the smallest value in lists Ci1,. . ., Cik,
respectively. Now consider any coalition structure CS which contains coalition Ci. If
CS is an optimal structure no other structure can have value greater than this. Now,
consider structure CS′ = CS \ {Ci} ∪{Ci1 ∪. . . ∪Cik} where Ci1, . . . , Cik are all
disjoint and Ci1 ∪. . .∪Cik = Ci. If the sum of the smallest values in sets Cil, . . . , Cik is
greater than the biggest value in set Zi ⊆Ci then clearly for all coalitions Cil, . . . , Cik
in Cil, . . . , Cik respectively, and any Ci ∈Zi ⊆Ci, v(Ci1) + . . . + v(Cik) > v(Ci) and
so v(CS′) > v(CS). Therefore, following Lemma 1, for any partition of i, i1, . . . , ik
such that i1 + . . . + ik = i, if the sum of the minimum values in sets Ci1, . . . , Cik is
greater than the maximum value in set Zi ⊆Ci then no coalition in Zi ⊆Ci can be in
an optimal coalition structure and so no CS which contains a coalition in Zi ⊆Ci can
belong to {CS∗}.
For any set containing non-trivial coalitions of size i, it is possible to compute the set
of all integer partitions of value i. Such a set is denoted P(i). Furthermore, for each
p′ ∈P(i) it is possible to compute a value di(p′) as in Lemma 3. Now, following the
same lemma, we can compare every coalition C of size i with di(p′) and immediately
disregard those for which v(C) < di(p′). In fact, there is no need to apply Lemma 3
to all partitions in P(i) but only to the partition p′′ ∈P(i) such that di(p′′) is maximal
in P(i). Clearly, for a coalition C of size i, if it holds that v(C) < di(p′) then, ∀p′′ ∈
P(i) \ {p′} it also holds that v(C) < di(p′) ≤di(p′′). Such a maximal value will be
referred to as the domination value of coalitions of size i. More formally:
Deﬁnition 1. For any set containing coalitions of size i and every partition p ∈P(i),
the domination value ˜di is the highest value of di(p) for all p ∈P(i), or ˜di =
maxp∈P (i) di(p).
Following Lemma 1, if the value of any coalition C of size i is less than the domination
value ˜di then there exists a dissociation of C with a value greater than v(C). In such a
case, C cannot be in any optimal structure. This property is exploited by the ﬁlter rules
presented in the next section.

104
T. Michalak et al.
3
Filter Rules
In this section, we propose ﬁlter rules that can be applied both while calculating the
values of all the coalitions in ϝ (thus, generating V (ϝ)) and while searching through the
(sub-)spaces of coalition structures. We will refer to coalitions which cannot belong to
any optimal structure as not-promising. All the other coalitions will be called promising.
We denote both of these disjoint sets by ϝnp ⊆ϝ and ϝp ⊆ϝ, respectively. Initially,
before the ﬁlter rules are applied, it is assumed that all coalitions are promising, i.e.,
ϝp = ϝ. Furthermore, we will refer to a subset of coalition values Z ⊆V (ϝp) (Z ⊆
V (ϝnp)) as promising (not promising).
3.1
Filter Rules for Input Calculation
While calculating all coalition values in the input, Lemma 2 can be applied to highlight
those coalitions which are not promising. However, dissociating every coalition into all
potential sub-coalition combinations is usually inefﬁcient for coalitions of greater size.
It should be left to a system designer to decide into how many sub-coalitions a currently
computed coalition should be disaggregated. The natural and possibly most efﬁcient
choice is to consider the singleton partition of a coalition, i.e., the decomposition of the
coalition into all of the individual agents who participate in this coalition. Clearly, in the
process of computing the value of a given coalition, all agents who co-operate in this
coalition must be known and such a disaggregation can be easily performed. Therefore,
following Lemma 2 we propose the ﬁrst ﬁlter rule:
FR1. For any non-trivial coalition C that can be divided into k disjoint singleton sub-
coalitions C1, . . . , Ck where C1 ∪C2 . . .∪Ck = C and ∀i = 1, . . . , k |Ci| = 1; if
it is the case that either (i) v(C1)+. . .+v(Ck) > v(C) or (ii) j
i=1 v(Ci) > v(C)
where j < k then all such coalitions C ∈ϝnp.
Naturally, we can relax the constraint that |Ci| = 1 to |Ci| ≤s where 1 < s < |A|
depending on into how many partitions the system designer wishes to divide coalition
C. The computational cost of applying FR1 should be balanced against potential gains.
Example 1. Consider a ﬁve agent system A = {a1, a2, a3, a4, a5} with the following
coalition values: v(C) = 14 for C = A; ∀|C| = 1 v(C) = 3; ∀|C| = 2 v(C) = 5;
∀|C| = 3 v(C) = 10;, and ∀|C| = 4 v(C) = 12. Observe that the sum of any two
coalitions of size 1 is strictly greater than the value of any coalition of size 2. Further-
more, the value of the grand coalition is smaller than the value of a non-cooperative
coalition structure. Thus, FR1 ﬁlters out all coalitions of size two as well as the grand
coalition. However, following Lemma 1 the system designer may consider all dissocia-
tions of a coalition into exactly two disjoint sub-coalitions in the same way as in the DP
algorithm presented in [7]. Since the value of any coalition of size 4 is smaller than the
value of any coalition of size 1 added to the value of a disjoint coalition of size 3, then
coalitions of size 4 are not promising.
Now, consider the domination value. Lemma 3 immediately yields the following ﬁlter
rule:

Pre-processing Techniques for Anytime Coalition Structure Generation Algorithms
105
FR2. For any subset Zs ⊆Cs of coalitions of size s, if ˜ds > max Zs then all coalition
from Zs are not promising. More formally: ∀Zs ⊆Cs, if ˜ds > max Zs then
Zs ∈ϝnp and Zs /∈ϝp.
FR2 can be applied by a system designer to every coalition C of size i immediately after
v(C) has been calculated (i.e. in the same manner as FR1). Alternatively, if |Zi| ≥2
then the value of max Zi can be recorded while calculating coalition values in this
subset and FR2 can be applied after this process has been ﬁnished. We illustrate the
functioning of FR2 with the following example:
Example 2. Suppose the coalition values for four agents A = {a1, a2, a3, a4} are as
follows: ∀|C| = 1, v(C) ∈⟨4, 7⟩, ∀|C| = 2, v(C) ∈⟨5, 7⟩, ∀|C| = 3, v(C) ∈⟨7, 11.5⟩
and v(A) = 11.3 The domination values for the coalitions of size 2 to 4 are computed as
follows: ˜d2 = min S1 +min S1 = 4+4 = 8, ˜d3, = max{3×minS1, min S1 +min S2
} = 12, ˜d4, = max{ 4 × min S1, 2 × min S1 + min S3, 2 × min S2 } = 16. Observe
that both ˜d2 > max S2 and ˜d4 > max S2 = v(A). Intuitively, this means that for
every coalition of size 2 and 4, there exists a dissociation of this coalition into sub-
coalitions which have value greater than the value of the coalition and so following
previous reasoning, no coalition of size 2 or 4 can be in an optimal structure and no
structure containing these coalitions can be in an optimal set.
3.2
Filter Rules for Search of Coalition Structure Space
As mentioned in the Introduction, the anytime algorithm of Rahwan et al. divides Π
into sub-spaces containing structures which are identical w.r.t. size of the coalitions
involved. In a system of |A| agents, let S∗:= {m1, . . . , mk} denote the (currently)
most promising subspace, where m1, . . . , mk represent sizes of the coalition involved
(k
i=1 mi = |A| and k ≥2). To search S∗, the values of all the coalition struc-
tures belonging to this sub-space should be computed unless it is proven beforehand
that they cannot belong to {CS∗}.4 The general form of a coalition structure in S∗is
{C1, . . . , Ck} such that all of C1, . . . , Ck are disjoint and ∀i = 1, . . . , k Ci ∈Cmi.
Let CS∗
N denote the coalition structure with the highest value found thus far. Rahwan
et al. propose a ﬁlter rule that, based on the statistical information gathered about the
coalition value input, avoids those structures which cannot be optimal. For k ≥3 and
k −1 ≥l ≥1 it holds that:
B&B. If l
i=1 v(Ci) + k
i=l+1 max Cmi ≤v(CS∗
N) then no structures in S∗can be
optimal, to which simultaneously belong all of C1, . . . , Cl.
This ﬁlter rule ensures that, for a particular structure under consideration {C1, . . . , Ck},
if the combined value of ﬁrst 1 ≤l ≤n −1 coalitions (l
i=1 v(Ci)) plus the value of
the sum of maximum values of coalitions in the remaining sets Cml+1, ..., Cmk is less
than the current optimum value v(CS∗
N) then no structures to which all of C1, . . . , Cl
simultaneously belong will be considered in the optimal CSG process.
3 For example, the notation v(C) ∈⟨4, 7⟩means that v(C) can be any real value higher than or
equal to 4 and lower than or equal to 7 and that the minimal value of all such coalitions is 4
and the maximal is 7.
4 Or unless it is proven that an optimal coalition structure in this sub-space has been found.

106
T. Michalak et al.
Example 3. For |A| = 9 agents let S∗:= {1, 2, 2, 2, 2} be the (currently) most promis-
ing subspace, CS∗
N = 28, v({a1})+v({a2, a3}) = 9, v({a4, a5}) = 4, v({a4, a6}) =
6 and max C2 = 7. Following B&B, since v({a1}) + v({a2, a3}) + v({a4, a5}) +
max C2 + max C2 < CS∗
N then no structures in S∗can be optimal, to which simul-
taneously belong all of {1}, {2, 3} and {4, 5}. These structures are: {{a1}, {a2, a3},
{a4, a5}, {a6, a7}, {a8, a9}},
{{a1}, {a2, a3}, {a4, a5}, {a6, a8}, {a7, a9}},
and
{{a1}, {a2, a3}, {a4, a5},{a6, a9},{a7, a8}}. Thus, in this example branch-and-bound
rule saves on calculation time by avoiding calculations that lead to three structures
which cannot be optimal.
The above branch-and-bound technique is based on basic statistical information col-
lected about Cm, namely the maximum value of coalitions in this set (max Cm). Assum-
ing that this information is known about some subsets Zm ⊆Cm for m = m1, . . . , mk
then the ﬁlter rule above can be generalized as follows:
FR3. If k
i=1 max Zmi < v(CS∗
N), where k ≥2, then no structures in S∗can be
optimal, in which simultaneously C1 ∈Zm1, . . . and Ck ∈Zmk.5
Example 4. In the system of nine agents let S∗:= {1, 2, 2, 4} be the (currently) most
promising subspace, CS∗
N = 25, max{Z1 := {{a1}, {a2}, {a3}}} = 4, max{Z2 :=
{{a2, a3}, {a4, a5}, {a6, a7}, {a8, a9}, {a1, a3}, {a1, a2}}}=6 and max{Z4 :=C4}=
7. Following FR3, since max Z1 + 2 × max Z2 + max Z4 < CS∗
N then no structures
in S∗containing all of C1, C2, C3, C4 such that C1 ∈Z1, C2 ∈Z2, C3 ∈Z2, and
C4 ∈Z4 can be optimal. In this example FR3 saves on calculation time by avoiding
{{a1}, {a2, a3},{a4, a5},{a6, a7, a8, a9}},{{a2}, {a1, a3},{a4, a5},{a6, a7, a8, a9}},
and {{a1,a2}, {a3},{a4, a5},{a6, a7, a8, a9}}.Note that there are certain combinations
of C1 ∈Z1, C2 ∈Z2, C3 ∈Z2, C4 ∈Z4 for which {C1, C2, C3, C4} is not a coalition
structure. For instance, any combination {{a1}, {a2, a3}, {a4, a5}, {a1, a2, a3, a4}} is
neither exhaustive nor disjoint. Although, we are not interested in such combinations,
it can be observed that the inequality in FR3 holds for them as well.
4
Application
To test the effectiveness of the ﬁlter rules we employ them in the state-of-the-art dis-
tributed coalition value calculation algorithm of Rahwan and Jennings [1] (DCVC from
now on) and in the state-of-the-art anytime CSG algorithm of Rahwan et al. [2] (RCSG
from now on).
Apart from the decentralised design, the key advantage of DCVC is that calculated
coalition values in V (ϝ) are ordered in a unique way ensuring that only V (ϝ) and not
ϝ must be kept in memory. Coalition values structured in the same way as in DCVC are
used as an input to RCSG. However, it is not trivial to connect both algorithms as the
former one is distributed whereas the latter one is centralised. This means that, at the
5 Note that certain combination of C1 ∈Zm1, . . . , Ck ∈Zmk are neither disjoint nor exhaus-
tive, i.e., they are not proper coalition structures as assumed in this paper. We leave them aside
as not being relevant to our analysis. See Example 4.

Pre-processing Techniques for Anytime Coalition Structure Generation Algorithms
107
moment when calculations in DCVC are complete, every agent knows only a fraction
of V (ϝ) needed in RCSG. Thus, a resource-consuming data transfer has to take place.
An application of any ﬁlter rules always requires a balance between computational
costs and potential gains. We will show that ﬁlter rules FR1, FR2 and FR3 can be
applied at a relatively low computational cost, and that using them results in:
1. A substantial decrease in the number of coalitions to be transferred; and
2. An increase in the efﬁciency of the promising structure sub-space search in RCSG.
4.1
Application of FR1, FR2 and FR3 in DCVC
In DCVC, the space of all coalitions is represented as a set of lists L1, . . . , L|A| where
list Li contains all coalitions of size i. Within each list, the coalitions are ordered w.r.t.
the agents they compromise. This ordering ensures that the agent composition of every
coalition in each list can be derived from the place in the list it occupies. In terms of
the notation introduced in Section 2 we may write Li = −→
Ci, i.e., every list is an ordered
set of the values in Ci. Such lists for a system of six agents a1, a2, ..., a6, all with equal
computational capabilities, are presented in Figure 1.6
All lists are divided into disjoint segments proportional to the agents’ computational
capabilities. Usually every agent is assigned two segments, one in the upper and one
in the lower part of the list. We will denote both segments assigned to agent ai in
list Lm by LU
m(ai) and LL
m(ai), respectively. As not all the lists are exactly divisible
by |A|, variable α, depicted in Figure 1, is used to distribute ‘left over’ coalitions as
equally as possible between agents. ‘Left over’ coalition values assigned to agent ai
will be denoted by LO
m(ai). Overall, the above methods to distribute V (ϝ) have the
advantage that all agent computations are ﬁnished (almost) simultaneously, even when
some coalitions require more arithmetic operations than others and when agents have
different processing capabilities. The allocation process is described in detail in [1].
After the allocation of segments has been performed, agents sequentially compute
values in lists starting from L1. Since there are |A| coalitions in L1, each agents is
assigned exactly one value from this list to calculate. Let us assume that every agent
transmits this value to all the other agents in the system. By doing so, agents are ready
to apply FR1 while computing coalition values in L2. Additionally, they are able to
efﬁciently compute the value of a structure consisting of every promising coalition C
and singletons, i.e., {C, {j} : j ∈A\C}.
Forexample,in Figure1,whilecomputing thevaluesofcoalitions{a4, a5} and {a3, a6}
in LU
2 (a5), agent a5 applies FR1 and compares v({a4}) + v({a5}) with v({a4, a5}) as
well as v({a3})+v({a6}) with v({a3, a6}). Furthermore, this agent calculates the val-
ues of {{a1}, {a2}, {a3}, {a4, a5}, {a6}} and {{a1}, {a2},{a3, a6} ,{a4}, {a5}}. Let
CS∗
N(ai) denote a coalition structure with the highest value that agent ai found in its
assigned segments.
6 Note that in Figure 1 numbers in lists are a shorthand notation representing agents. It should
be emphasized that every list contains coalition values from V (ϝ) and not coalitions from
ϝ themselves. Thus, the representation of lists L1, ..., L|A| in Figure 1 should be read not as
coalitions of agents from ϝ but as their numerical values from V (ϝ).

108
T. Michalak et al.
Fig. 1. Division of V (ϝ) in a system of six agents
In DCVC, when the computational capabilities of all the agents are symmetric, each
of them receives an (almost) equal fraction of ϝ to calculate V (ϝ) (as in Figure 1).
The total number of values assigned to every agent is equal to either

2|A| −1

/|A|

or

2|A| −1

/|A|

+ 1, depending on the distribution of ‘left-over’ coalitions. For
example, for 20 agents this amounts to either 52,428 or 52,429 and for 30 agents to
either 53,687,091 or 53,687,092. Clearly, transferring such numbers of coalition val-
ues from DCVC to RCSG is a potential bottleneck in the system performance. The
application of FR1 helps to reduce the transfer load but its effectiveness depends on
the values of the singletons. In some systems, these values might be lower than a value
of any coalition. In such a case, following Lemma 2, FR1 can be extended to consider
other partitions of a coalition C whose value is being computed. However, dissociations
other than into singletons may considerably increase computational cost and should be
weighted against potential gains. Consequently, we propose another approach that can
signiﬁcantly reduce the transfer load. Since RCSG searches only through promising
subspaces of Π, then only some lists of coalition values have to be transferred. This
means that the procedure to evaluate whether a given subspace is promising should be
removed from the RCSG and incorporated into DCVC.
In RCSG a subspace S is considered to be promising if its upper bound UBS :=

∀mi∈S maxLmi is no smaller than the lower bound of the entire system LB :=
max{v(CS∗
N), max{AvgS}}, where AvgS = 
∀mi∈S avgLmi is proven to be the
average value of every subspace. In DCVC, where calculations are distributed among all
of the agents, statistics such as maximum, minimum and average coalition values cannot
be computed without an exchange of information among agents. However, if agents
record maximum, minimum and average values in every segment they are assigned to
calculate then after all calculations are (almost simultaneously) ﬁnished every agent
ai can broadcast this statistical data along with CS∗
N(ai). Using this knowledge, the
agents can calculate, for every list Lm, the values of maxLm, avgLm, and minLm.
Both the maximum and average values are then used, by the agents, to determine which
sub-spaces are promising. Consequently, by exchanging some basic information about

Pre-processing Techniques for Anytime Coalition Structure Generation Algorithms
109
segments, the agents can prune the search space at the end of DCVC in the same way
as the centre would do it in RCSG.
The subspace with the maximum upper bound is the most promising, i.e., S∗=
arg maxS UBS, and is searched ﬁrst. It might happen that the unique CS∗(i.e., when
|{CS∗}| = 1) is found in this sub-space. Thus, in order to reduce potential transfer
load, we assume that only lists needed to construct coalition structures in S∗are ini-
tially passed on from DCVC to RCSG. Let the most promising subspace be S∗:=
{m1, . . . , mk}. While searching all structures in this subspace, a natural assumption is
that each agent will begin their search, starting from the segment of values allocated to
it in Lm1. For instance, let S∗= {1, 2, 3} for n = 6. Referring to Figure 1, it would
be natural to assume that agent a1 generates all the coalition structures starting with
coalition {a6}, agent a2 generates those starting with {a5}, etc. However, this means
that the coalitions in the two longer lists L2 and L3 must be transferred between agents.
Therefore, one way to minimize the transfer load is to assume agents start searching S∗
beginning with the longest list. In our example, let S∗be deﬁned not as {1, 2, 3} but as
S∗= {3, 1, 2}. With this ordering, agent a1 generates all the coalition structures start-
ing with coalitions {a4, a5, a6}, {a1, a2, a6} and {a1, a2, a5}. Thus, values from L3
need not be transferred as agents utilize only the values assigned to them in the DCVC.
Therefore, in contrast to [9], such an ordering, which starts with the longest list in S∗,
will be the one considered in our algorithm. This means agents do not have to transfer
values in Lm1. However, if Lm1, which is the longest list in S∗, appears more than once
then the list must be transferred as it will be needed by all the agents during the search
process.7
To reduce the transfer load even further, both FR2 and a version of FR3 are applied.
Before the transmission begins, agents calculate the domination value ˜dm for every list
Lm : m ∈S∗using the relevant statistics.8 Segments assigned to agent ai which meet
the condition max LU/L/O
m
(ai) < ˜dm are ﬁltered out as not promising. Furthermore, for
all segments in which max LU/L/O
m
(ai) ≥˜dm, it is determined if individual coalition
values within these segments, not ﬁltered by FR1 are not smaller than ˜dm. Individual
coalitions with such values become not promising. The version of FR2 that concerns
segments will be denoted as FR2a, whereas, the version that concerns individual coali-
tions as FR2b.
Although FR3 has been constructed to increase efﬁciency of searching Π, a ver-
sion of this ﬁlter rule can also be applied before data transfer. In S∗, every agent a1
knows max LU/L/O
m
(ai) and max Lm for all m = m1, ..., mk the following version of
FR3 (denoted as FR3a) can be applied to decide whether either LU
mj(ai), LL
mj(ai) or
LO
mj(ai), where mj ∈S∗, can be ﬁltered out as not promising:

m∈S∗\{mj}
max Lm + max LU/L/O
mj
(ai) < v(CS∗
N).
(1)
7 For more details on the search process, see Subsection 4.2.
8 We leave it to the discretion of the system designer as to the exact architecture of domination
valuecalculations. At theendofDCVC, agentscaneithercalculateall neededdomination values
individually or distribute calculations among themselves in such a way that the most efﬁcient
agents calculate domination values for the lists of coalitions with the highest cardinality.

110
T. Michalak et al.
If the segment cannot be ﬁltered out as not promising then the above ﬁlter rule can
be applied to every individual coalition C within this segment not ﬁltered out by FR1
or FR2a/b. In such a situation, max LU/L/O
mj
(ai) in formula (1) can be replaced with
v(C), where v(C) ∈LU/L/O
mj
(ai) and this version of FR3 is denoted FR3b. Finally,
after all the ﬁlter rules have been applied, the agents transfer only those values which
are promising.
To summarize, in order to reduce the transfer load from DCVC to RCSG we propose
the following extension of the former algorithm:
Step 1: Agents exchange among themselves values of singleton coalitions in L1. While
calculating value of any non-trivial coalition C they: (1.1) apply FR1; (1.2)
compute CS∗
N(ai) and (1.3) Update and store the statistics about their seg-
ments LU/L/O
m
;
Step 2: After calculating the values in all of their segments, agents exchange among
themselves the segments’ statistics and CS∗
N(ai). Using this information, they
determine the most promising sub-space S∗and domination values ˜dm for ev-
ery list Lm : m ∈S∗that needs to be transmitted;
Step 3: Just before transmission takes place each agent applies both FR2a and FR3a
to segments LU/L/O
m
, where m ∈S∗. If a segment is not ﬁltered out then
FR2b and FR3b are applied again to individual coalitions within this segment
that have not been ﬁltered out by FR1. Only the values of promising coalitions
are transferred.9 If the longest list in S∗appears only once then this list is not
transferred.
Finally, it should be observed that even if the coalition value calculation algorithm was
not distributed, then FR1, FR2b and FR3b can be still be applied to determine the not
promising coalitions. In the simulations we will show that it considerably reduces the
running time of RCSG.
4.2
Application of FR3 in RCSG
The strength of RCSG in searching the promising sub-spaces is that it avoids creat-
ing both invalid and repeated coalition structures as described in [9]. Suppose that
S∗:= {m1, . . . , mk} is the most promising subspace in a system of |A| agents and
that the value of the particular partial structure {Cm1, ..., Cml}, where l < k, has al-
ready been calculated. Let −→
A′ be the ordered set of agents which do not belong to
any coalition in this partial structure, i.e. −→
A′ := −→
A\{Cm1, ..., Cml}. The method pro-
posed by Rahwan et al. in [9] cycles through all ml+1−element combination from −→
A′
to construct all feasible coalitions Cml+1. The cycle is designed in such a way that
all ml+1−combinations which must contain the agent in −→
A′ with the lowest index are
considered ﬁrst and all ml+1−combinations which must contain the second agent in
−→
A′ but not the ﬁrst one are considered in the next step and so on. As explained in
9 To indicate the position of each promising coalition value and maintain the list structure we
propose to transmit a characteristic bit vector alongside the stream of values. In such a vector
1 indicates value of a promising coalition and 0 a not promising one.

Pre-processing Techniques for Anytime Coalition Structure Generation Algorithms
111
Subsection 3.2, after constructing every Cml+1 RCSG applies B&B ﬁlter rule to de-
cide whether any partial structure {Cm1, ..., Cml, Cml+1} is promising. A version of
FR3 can be used to identify groups rather than only individual partial structures. To
this end, agents in DCVC have to gather additional information about some particular
segments in lists L1, ..., L|A|. Let −→
Z m(i) denote a segment of list Lm such that ai is
the ﬁrst agent in every coalition in −→
Z m(i) ⊆Lm. For example, referring to Figure
1, −→
Z 3(2) is {{2, 5, 6}, {2, 4, 6}, {2, 4, 5}, {2, 3, 6}, {2, 3, 5}, {2, 3, 4}}. Assume that,
while calculating coalition values in DCVC, the agents record max −→
Z m(i) for every
combination of m = 1, ..., |A| and ai ∈A. In such a case, before cycling through all
ml+1−combinations which contain the agent in −→
A′ with the lowest index, the following
version of FR3 (denoted as FR3c) can be applied, by a centre in RCSG. If
l

i=1
v(Ci) + max −→
Z l+1(i) +
k

i=l+2
max Lmi < v(CS∗
N),
then any partial structure {Cm1, ..., Cml, Cml+1} such that Cml+1 ∈−→
Z l+1 is not
promising. FR3c is a generalization of RCSG B&B as it is applicable to groups rather
than individual coalitions.
4.3
Numerical Simulations
In the numerical simulations we compare the sequential execution of DCVC and RCSG
with and without ﬁlter rules. The following assumptions are imposed: (i) the calculation
of any particular coalition values in DCVC takes no time; and (ii) any data transfers are
instantaneous. We focus on the percentage of V (ϝ) which does not have to be trans-
mitted from DCVC to RCSG due to applied ﬁlter rules. Additionally, we demonstrate
that, for a normal distribution of coalition values, ﬁlter rules considerably improve the
performance of the CSG process in RCSG.
Following [2,5], we evaluate the algorithms under two different assumptions as to the
probability distributions of coalition values:10 (ND) Normal: v(C) = max(0, |C|×p),
where p ∈N(μ = 1,σ = 0.1); and (UD) Uniform: v(C) = max(0, |C| × p), where
p ∈U(a, b), where a = 0 and b = 1. For a system of |A| = 11 . . .20 agents, we
ran both versions of the algorithms 25 times and reported the results within a 95%
conﬁdence interval. The algorithms were implemented in MATLAB.
The results are presented in Table 1. Column 2 shows the number of all coalition
values |ϝ| and Column 3 percentage of V (ϝ) that needs to be transmitted from DCVC
to RCSG without ﬁlter rules. Column 4 shows the actual percentage of V (ϝ) transferred
when ﬁlter rules are applied. Column 5 contains the size of the characteristic bit vector
(as a percentage of V (ϝ)) that must be transferred simultaneously with the values in
Column 4 (see Footnote 8). The next ﬁve columns present the individual contribution of
ﬁlter rules FR1, FR2a, FR2b, FR3a and FR3b to the reduction of the overall transfer
load between Columns 3 and 4. The last column shows the running time of RCSG based
on the unﬁltered input and without FR3c divided by the running time of this algorithm
based on ﬁltered input and with FR3c.
10 We omit the sub- and super-additive cases as their solution is straightforward.

112
T. Michalak et al.
Table 1. Simulation results (all values, except for columns 1,2, and 11 are expressed in %)
Consider ND ﬁrst. Without ﬁlter rules, about 90% of V (ϝ) needs to be transmitted
from DCVC to RCSG (Column 3). Filter rules reduce this number to about 9% + 5.5%
for n = 11 and to around 2.5%+2.5% for n = 20 (Columns 4 and 5).11 FR1 is the most
successful in ﬁltering coalitions accounting for about 50% of them. Consequently, both
FR2a and FR2b, based on the same assumption, cannot offer any signiﬁcant improve-
ment upon this result. However, if the singleton coalitions have relatively low values,
then FR1 would not perform so well, and FR2a/b would become more proﬁtable. A
decreasing percentage of coalition values are ruled out by rule FR3a. The intuition be-
hind the decreasing effectiveness of this ﬁlter rule is as follows. The higher the value of
|A|, the longer the segments become. Consequently, there is a greater probability that
the (randomly-drawn) extreme values in these segments are similar to each other. This
reduces the effectiveness of ﬁlter rules based on segments’ maxima. In contrast, FR3b,
which focuses on the particular coalition values, has increasing success. We expect that
for n > 25 only FR1 and FR3b should be used. In conclusion, the combination of the
ﬁltered input and the application of FR3c in RCSG results in a much faster perfor-
mance of this algorithm. Experiments show that this result is to be partially attributed
to ﬁlter rules and primarily to the re-ordering of the way in which the subspaces are
searched.12 Intuitively, in the ND case, more coalitions are ﬁltered from the longest
lists than from any other. This means that the number of coalition structure to evaluate
is already signiﬁcantly decreased.
For the UD case, only about 50% of V (ϝ) needs to be transmitted from DCVC
to RCSG (Column 3) but ﬁlter rules are able to reduce this number: from about 2 ×
(3.32% ± 1.79%) for n = 11 to around 2 × (1.04% + 0.1%) for n = 20. Analysis
of the effectiveness of individual ﬁlter rules is similar to the ND case. However, in
contrast to ND, the combination of the ﬁltered input and the application of FR3c does
not signiﬁcantly outperform the standard RCSG. This is caused by the very nature of
the uniform-distribution. Intuitively, since coalition values in any lists are dispersed
11 All simulations are performed in MATLAB. In the future, the same simulations will be repro-
grammed in JAVA but this should not inﬂuence the results presented in this paper which are
based on relative comparison.
12 See Subsection 4.1 for more details.

Pre-processing Techniques for Anytime Coalition Structure Generation Algorithms
113
neither reduced input nor FR3c perform much better than B&B in standard RCSG.
However, our ﬁlter rules still prove its effectiveness by achieving large reduction in the
transfer load.
5
Conclusions
In this paper we have discussed a number of techniques designed to increase the efﬁ-
ciency of CSG algorithms. In particular, we developed ﬁlter rules which can, for CFG
representations, eliminate a signiﬁcant proportion of not promising coalitions from the
space of all coalition values. Such a (structured) space of coalition values acts as in-
put to, among others, the state-of-the-art CSG algorithm of Rahwan et al. Although
this algorithm has been demonstrated to be very effective, its particular drawback is its
centralised nature. This is especially challenging as there exist already a very efﬁcient
algorithm for distributed coalition value calculation; this means that after the DCVC
algorithm has been employed, all of the agents have to transmit all the values they have
computed to a single entity. In this paper, we demonstrated that our proposed ﬁlter rules
are extremely effective in reducing the size of this coalition value input to the CSG
algorithm. Consequently, we showed how to efﬁciently bridge the gap between the de-
centralised DCVC and the centralised RCSG. A natural follow up to our work, which
we plan to explore, is to develop a distributed CSG algorithm that can be relatively
easily constructed from the analysis in this paper.
References
1. Rahwan, T., Jennings, N.: An algorithm for distributing coalitional value calculations among
cooperating agents. Artiﬁcial Inteligence 171(8-9), 535–567 (2007)
2. Rahwan, T., Ramchurn, S., Dang, V., Giovannucci, A., Jennings, N.: Anytime optimal coali-
tion structure generation. In: Proceedings of AAAI, Vancouver, Canada, pp. 1184–1190
(2007)
3. Michalak, T., Dowell, A., McBurney, P., Wooldridge, M.: Optimal coalition structure genera-
tion in partition function games. In: ECAI, Patras, Greece (2008)
4. Shehory, O., Kraus, S.: Methods for task allocation via agent coalition formation. Artiﬁcial
Intelligence 101(1), 165–200 (1998)
5. Sandholm, T., Larson, K., Andersson, M., Shehory, O., Tohme, F.: Coalition structure gener-
ation with worst case guarantees. A.I. 111(1-2), 209–238 (1999)
6. Dang, V., Jennings, N.: Generating coalition structures with ﬁnite bound from the optimal
guarantees. In: Proceedings of AAMAS, New York, USA (2004)
7. Yeh, D.Y.: A dynamic programming approach to the complete partition problem. BIT 26(4),
467–474 (1986)
8. Rahwan, T., Jennings, N.: An improved dynamic programming algorithm for coalition struc-
ture generation. In: Proceedings of AAMAS, Estoril, Portugal, (2008)
9. Rahwan, T., Ramchurn, S., Dang, V., Giovannucci, A., Jennings, N.: Near-optimal anytime
coalition structure generation. In: Proceedings of IJCAI, Hyderabad, India (2007)

Cognitive Use of Artifacts: Exploiting Relevant
Information Residing in MAS Environments
Michele Piunti1,2 and Alessandro Ricci2
1 Institute of Cognitive Sciences and Technologies, ISTC-CNR, Rome, Italy
2 Alma Mater Studiorum, Universit`a degli studi di Bologna, DEIS, Cesena, Italy
{michele.piunti,a.ricci}@unibo.it
Abstract. Besides using language and direct communication, humans
adopt various kind of artifacts as eﬀective means to represent, store and
share information, and ﬁnally support knowledge-based cooperation in
complex work environments. Similarly to the human case, we argue that
an analogous concept can be eﬀective also in the context of multi-agent
systems (MAS) when cognitive agents are of concern. In particular, we
investigate the use of cognitive artifacts, as those computational entities
designed to store, process and make available environmental resources
which are relevant to achieve goals and coordinate their cooperative and
distributed activities. Some of the practical beneﬁts of the artifact-based
approach are discussed through an experiment based on CArtAgO and
Jason technologies. Eﬀectiveness of diﬀerent interaction strategies are
investigated for teams of goal-oriented agents using diﬀerent kind of co-
munication styles.
1
Introduction
According to conceptual frameworks such as Activity Theory [6] and Dis-
tributed/Situated Cognition, artifacts play a fundamental role in human coopera-
tive activities, deeply aﬀecting individuals problem solving and ability to perform
cooperative tasks, manage and share knowledge, cooperate and coordinate their
work. In this view, artifacts are a way to conceive, structure and organise the envi-
ronment where humans live and work. Recent agent literature highlighted the im-
portant role that the notion of environment can play in designing and engineering
Multi-Agent Systems (MAS). The environment is thus conceived as the suitable
locus to encapsulate resources, services and functionalities in order to improve
agent interaction, coordination, and cooperation [13]. Not surprisingly, coordina-
tion artifacts – like calendars, programmable blackboards, schedulers, etcetera –
have been progressively introduced in MAS for instance to enable and mediate
agent-agent interaction and communication [9]. In this view, the notion of artifact
has been recently introduced by the Agents & Artifact (A&A) conceptual model
as ﬁrst-class entity to structure and organise agent work environments, represent-
ing resources and tools1 that agents may want to use to support their activities,
analogously to artifacts in the human case [8].
1 In this context we consider the terms artifacts and tools as synonyms.
J.-J.Ch. Meyer and J.M. Broersen (Eds.): KRAMAS 2008, LNAI 5605, pp. 114–129, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

Cognitive Use of Artifacts: Exploiting Relevant Information Residing
115
Besides communication models, there are many traits relating cognitive mod-
els of agency to A&A. In fact, from a cognitive perspective, artifacts can be
viewed as external entities that agents may exploit to improve their repertoire of
actions. For knowledge representation in particular, cognitive artifacts – deﬁned
by Norman as those artifacts that maintain, display or operate upon information
in order to serve a representational function [7] – are essential tools for reason-
ing, heavily aﬀecting human behavior. Once an agent uses an artifact, he is both
relying on its functionalities and delegating part of his purposive activities on
artifact functioning. In this view, artifacts play the role of suitable services that
agent can exploit to externalise activities thus easing their computational bur-
dens [4]. An additional aspect relating cognitive agents to artifacts concerns the
nature of open systems, exacting agents with the ability to discover information
which is relevant for achieving their goals. These aspects heavily concern the
ability to learn and co-use environmental and distributed resources in terms of
subjective experiences and mental states.
The aim of this work is to enrich the abstraction tools in MAS design so
as to provide alternatives from communication and cooperation strategies so
far based solely on message passing and ACL, towards a view in which agents
live in shared (distributed) work environments, which are part of their habi-
tat, composed not only by other agents but also by dynamic sets of artifacts,
created/scrutinized/disposed by agents themselves. In particular we discuss the
abilities of agents to exploit artifacts with representational functionalities, i.e.
cognitive artifacts. This, we guess, has manifold beneﬁts, in particular in mak-
ing it more eﬀective to organise and share distributed knowledge, reasoning
about distributed work, retrieving information, ﬁnally improving the coordina-
tion/cooperation typical of complex activities.
First we discuss the role played by cognitive artifacts upon the A&A model
and CArtAgO technology (Section 2). Then, to provide some evidence of the
overall model, we describe an experiment in the RoomsWorld scenario (Section 3),
(Section 4) where speciﬁc kind of cognitive artifacts, i.e. logs, are exploited to
support the distributed work and interaction of a team of goal-oriented agents.
CArtAgO and Jason [1] are exploited as integrated technologies [11] to implement
respectively the artifact-based environment and the cognitive agents.
2
Artifacts in Multi-Agent Systems
The notion of artifact in MAS has been introduced with the Agents & Artifacts
A&A conceptual model as ﬁrst-class abstraction along with agents when model-
ing, designing, and developing MAS [8]. A&A promotes the notion of artifacts
and resources/tools playing a pivotal (mediation) role in coping with the scaling
up of complexity, in particular when social interactions and complex activities
are concerned [9]. In cognitive theories on human behavior, the use of tools can
be considered along with an evolutionary accumulation and transmission of so-
cial knowledge, habits, memes, which not only inﬂuence the nature of external
behavior, but also the mental functioning of individuals.

116
M. Piunti and A. Ricci
OpControlName
OpControlName
...
MANUAL
OBSERVABLE
EVENTS
GENERATION
<NAME,PARAMS>
OPERATION X
LINK INTERFACE
OPERATION Y
OBSERVABLE
PROPERTIES
USAGE
INTERFACE
OP CONTROL
<NAME,VALUE>
OBS PROPERTY
content
ObsPropName
content
ObsPropName
content
ObsPropName
(1) joinWorkspace(+Workspace[,Node] )
(2) quitWorkspace
(3) makeArtifact(+Artifact,+ArtifactType[,ArtifactConfig])
(4) lookupArtifact(+ArtifactDesc,?Artifact)
(5) disposeArtifact(+Artifact)
(6) use(+Artifact,+UICtrl([Params] )[,Sensor][,Timeout][,Filter])
(7) sense(+Sensor,?PerceivedEvent[,Filter] [,Timeout])
(8) focus(+Artifact[,Sensor] [,Filter])
(9) stopFocussing(+Artifact)
(10) observeProperty(+Artifact,+Property,?PropertyValue)
(?) optional parameters; (-/+) in/out parameters
Fig. 1. (Left) An abstract representation of an artifact, with in evidence the usage
interface, with operation controls and observable properties, the manual and the link
interface, used to link together artifacts. (Right) Basic set of actions integrating agent
platforms (Jason in this case) and CArtAgO.
By analogy, the basic idea of A&A is to deﬁne a notion of artifact in MAS
as ﬁrst-class computational entities, representing resources and tools that agent
can exploit to perform their tasks, in particular social ones. Then, according to
the A&A model, designers have two complementary abstraction tools to deﬁne
their MAS. Diﬀerently from agents, which can be described as autonomous,
pro- active (goal-oriented) entities encapsulating the control of their behaviour,
artifacts are a non-autonomous and function-oriented computational objects,
i.e., encapsulating some kind of service (function) to be exploited by agents.
In this view, the interaction between agents and artifacts is shaped on two
fundamental activities played by agents in their environment: action and percep-
tion. In this view, agents use artifacts through their usage interface, which is a
control interface, and gets their output by perceiving observable events generated
with artifact functioning and observable properties which are part of the arti-
fact state (Fig. 1 Left). Besides the notion artifact, A&A introduces the notion
of workspace as logic container of agents and artifacts. Workspaces can be used
to structure the overall sets of entities, deﬁning a topology of MAS environment.
In this view a MAS is conceived as a set of (distributed) workspaces, containing
dynamic sets of agents working together by both directly communicating and
sharing/co-using artifacts (Fig. 2 Right).
2.1
The Role of Cognitive Artifacts
As remarked in [7], cognitive artifacts shape the way in which human beeings
pursue their activities interacting with their surroundings. Examples of cogni-
tive artifacts used by humans are, for instance, activity lists or logs – useful
to maintain and manage a shared list of task to do – and white-boards – to
maintain shared knowledge and provide suitable operation to access and update
it. Besides the speciﬁc functionalities, in [7] Norman remarks the beneﬁts of
cognitive artifacts under two diﬀerent perspectives. From an individual point of
view, artifacts basically make it possible to enhance agent cognitive capabilities,
either by saving internal computational load or being merely memory enhancer,

Cognitive Use of Artifacts: Exploiting Relevant Information Residing
117
for instance creating observable cues in order to highlight relevant information.
Besides, agents are not forced to contemporaneously communicate by means of
message passing but they can asynchronously interact according to emerging
dynamics (situated cognition). In addition, by changing the actions required for
achieving a goal, artifacts change the way a task gets done. In so doing cognitive
artifacts can help individual agents to eliminate decision points, creating more
rational decision alternatives and reducing the fan-out of a decision tree [5].
From a system perspective, cognitive artifacts can be useful to: (i) distribute
actions across time (pre-computation), by allowing to store information and
enable information processing that can be performed even before the actual
operation usage; (ii) distribute the actions across time and space (distributed
cognition), asynchronously distributing tasks across agents and tools, allowing
social coordination and shared activities.
Analogously to the Norman deﬁnition, we refer to cognitive artifacts for MAS
those computational entities that are designed to maintain, make it observable,
or operate information in order to serve a representational function for agents
with respect to their environment and the work inside it. From an agent view-
point, artifacts can be cognitively used once their representational content can
be mapped and translated into reasoning processes. Relations from cognitive
agents to cognitive artifacts are, at least, bidirectional. On the one side artifact
interface descriptions can be matched with agents epistemic (beliefs) and motiva-
tional (goals) mental states. On the other side, the external events coming from
a cognitive artifact can be integrated at an architectural level by automatically
promoting such events as “relevant” signals to be addressed to the reasoning
processes.
Then, by adopting a functional view, from an agent’s perspective artifacts
can be viewed as devices playing the role of targets for purposive activities,
thus serving a twofold function: doxastic and operational [10]. On the one side,
artifacts play an operational (purposive) function according to which goals can be
achieved by means of operations which have been deﬁned within artifact control
interface. To this end, the A&A meta-model deﬁnes a series of general properties
by which artifact functionalities are deﬁned in terms of operations, which can be
triggered by agents via artifact’s usage interface (Fig. 1 Left). On the other side,
artifacts play a doxastic (epistemic) function, being their representational and
observable contents as external informational structures exploitable by agents.
2.2
Cognitive Agents Using Artifacts in CArtAgO
On the basis of the operational and doxastic functions of artifacts, A&A inter-
actions can be based on agent’s native abilities of action and perception. More
formally, an agent AG rely on an artifact AR for a given goal Gi, according to
a set of plans Pji if:
1. AG has Gi in its set of goals;
2. AG is not autonomous for Gi, i.e. lacks at least one of the resources (ac-
tions/information) necessary to achieve Gi;

118
M. Piunti and A. Ricci
3. There is a plan pji ∈Pji that achieves Gi where at least one resource (ac-
tion/information) used in this plan is in AR’s set of available operations or
AR’s observable state.
Analogously to usage interface of artifacts in the real world, an artifact usage
interface is composed by a set of operation controls that agents can use to trig-
ger and control operation execution. Each operation control is identiﬁed by a
label (typically equal to the operation name to be triggered) and a list of in-
put parameters. Operations are the basic unit upon which artifact functionality
is structured. The execution of an operation can result both in changes in the
artifact’s inner (i.e. non-observable) state, and in the generation of a stream of
observable events that can be perceived by agents that are using or simply ob-
serving the artifact. Besides, to support the doxastic function, the usage interface
might contain also a set of observable properties, i.e. properties whose dynamic
values can be observed by agents without necessarily acting upon operations.
Whereas A&A provide a conceptual underpinning on the agents and arti-
facts approach, CArtAgO [11] puts in practice the enabling technology to realise
artifact-based MAS. CArtAgO oﬀers a suitable integration technology supporting
the implementation and the deployment of distributed MAS based on the A&A
model, where agents dwelling in diﬀerent agent platforms can autonomously join
and work together in distributed and shared workspaces. It is worth noting that
the platform promotes the deployment of open systems by allowing artifact-
based environments and workspaces to persist – supported by the run time –
even beyond the presence of agents. In addition CArtAgO resolves low-level issues
at a platform level, for instance ruling interactions (i.e. action, perception, oper-
ation execution) and managing conﬁcts on resources (i.e. cuncurrency on shared
artifacts, distribution across several nodes). Current CArtAgO distribution of-
fers bridge mechanisms to be integrated by some well-known agent platforms
(namely Jason, Jadex, 2APL)2.
To allow cognitive agents to play in CArtAgO environments we consider basic
sensory-motor aspects in a cognitive system fashion. On the one hand agents
have been equipped with mechanisms to interact with artifacts (eﬀectors); on
the other hand they also have been enabled to perceive events generated by
artifacts during their operations (sensors). In this view, the integration approach
has been realized at the language level, i.e. the set of artifact-related actions
have been added to the repertoire of natively available actions. Therefore, the
bridge mechanism introduced the notion of agent body as that part of an agent
conceptually belonging to a workspace (once the agent is inside it) and containing
those sensory-motor capabilities to interact with artifacts. Fig. 1 (Right) shows
basic actions integrated in the body allowing agents to interact with CArtAgO
artifact-based environments. In particular, the described actions make it possible
for an agent to: join, move, and leave workspaces (1-3); use an artifact by acting
on its control interface and perceive events generated by artifacts (4-7); observe
2 CArtAgO is available as an open source technology at:
http://cartago.sourceforge.net. A detailed description of the platform – includ-
ing aspects not essential for this work – can be found in CArtAgO site.

Cognitive Use of Artifacts: Exploiting Relevant Information Residing
119
lastnote
0
putNote
Agent 1
LOG_1
ROOMSWORLD WORKSPACE
currentime
23
WATCH
LOG_2
LOG_N
Agent 2
Agents Platfoms
Work Environment
Fig. 2. (Left) The RoomsWorld scenario. Agents are engaged in cleaning thrash (red
items) spread over eight rooms relying on artifact based facilities of watch and logs.
(Right) An overview of the RoomsWorld scenario: agents perceiving and acting with
artifacts distributed in CArtAgO workspaces.
artifact properties (8). A pseudo-code ﬁrst-order logic-like notation is adopted
for the syntax, while semantics is described informally. use is the basic action
for triggering the execution of an operation, specifying operation control name
and parameters, and optionally a sensor name. Sensors are conceived as a kind
of body’s perceptual memory to keep track of observable events generated by
artifacts, possibly applying ﬁlters and speciﬁc kinds of “buﬀering” policies. If no
sensor is speciﬁed, all the observable events generated by the artifact executing
the operation are signalled as internal events to the agent. Otherwise, events
collected in sensors can be retrieved by using the sense primitive, specifying
a template (which functions as a ﬁlter) and possibly a timeout, indicating how
long the current course of action can be suspended waiting for the event to be
detected by the sensor. The basic support for artifact observation is provided by
focus action, which makes it possible to continuously be aware of the observable
events generated by the speciﬁed artifact. Finally, observeProperty is given to
observe and get the value of a given observable property. In this case no sensors or
percepts are involved: the value is directly bound to the variable speciﬁed within
the basic action. It’s worth remarking that, diﬀerently from use, focus and
observeProperty do not cause the execution of any operation or computational
work by the (observed) artifact.
To provide a practical example showing cognitive artifacts in practice, in the
next sections we discuss the RoomsWorld scenario, where teams of goal-oriented
agents exploit cognitive artifacts to perform their cooperative tasks.
3
Cognitive Artifacts in Practice
Built with CArtAgO technology, the RoomsWorld scenario realises an open system
where heterogeneous agents have the possibility to join, test and interact with
an artifact-based workspace. The workspace is composed by a number of virtual
rooms separated by walls and doors (see Fig. 2 Left). Once a room is entered,

120
M. Piunti and A. Ricci
agents should achieve the goal to ﬁnd and clean trash items which may appear in
the rooms with arbitrary frequencies. It is worth noting that, given its structure,
the global environment is partially observable. Rooms are visitable one at a time
and, to locate trash items, an agent has to enter and then perform some epistemic
action. Once they get the percept of a trash, agents can reach its location and
adopt a “clean” goal. For convenience, a global environment artifact is used
to hold physical consistence to the simulation and supporting agents in their
perceptive processes. The environment artifact is used here just as a convenient
way to represent and implement in the simulation the physical environment: for
instance, the location of the trash items are provided in the form of symbolic
percepts which are assumed to contain the symbolic location of the trash.
To investigate eﬀectiveness of artifact mediated interactions we engaged
agents pursuing diﬀerent strategies for achieving their goals in experiment se-
ries. The “normal cleaners” simply look for trash exploring the rooms based
on personal experience. Agents in this case act as if they were alone in the
environment, by autonomously updating relevant information without any co-
operation by other member of the team. The second team of agents adopt a
message based strategy: “Messenger cleaners” exploit explicit messages to coor-
dinate themselves. Once a given room has been cleaned, a messenger shares this
information by broadcasting a message to the overall society, thus indicating the
room identiﬁer and the exact time at which the clean activity has been ﬁnished.
For simplicity, messengers have been implemented using the same agent plat-
form (Jason in this case), thus exploiting their native infrastructure to process
message passing.
To support the third strategy, a set of particular cognitive artifacts are de-
ployed in the workspace. In this case agents have the possibility to purpo-
sively use special kind of artifacts which are physically placed at the entrance
of each room. Such artifacts function as logs to supply agents with additional
information which is relevant for accomplishing their work. For each Log, the
@OPERATION void putNote(Object ts) allows agents to store the time-stamp
at which a given clean activity has been performed (Table 1). Accordingly Logs
expose @OBSPROPERTY int lastnote as an observable property, namely the last
registered time-stamp, by which agents can infer the last performed clean ac-
tivity for that room. Logs make it possible uncoupled interaction between who
wants to share a given information (informing agent) and who may exploit this
information (reading agent), thus allowing agents to be acknowledged on those
particular information which is relevant to achieve the current tasks. In addition,
agents also exploit a watch artifact, functioning as a timer which provides them
Table 1. CArtAgO implementation for the Log artifact
public class Log extends Artifact {
private LinkedList<String> notes;
@OPERATION void init(String n, location l){
name=n; loc= l;
notes = new LinkedList<String>();
defineObsProperty("lastnote", 0);
}
@OPERATION void putNote( Object ts ){
notes.addLast(timestamp.toString());
lastnote =
((Integer)timestamp).intValue();
}
}

Cognitive Use of Artifacts: Exploiting Relevant Information Residing
121
with a symbolic record of the ongoing simulated time. Log strategy assumes
that, before entering a given a room, agents retrieve the actual time from the
watch and then store a time record in the related log. In the next section we
detail the model for Jason agents adopting the log strategy.
3.1
Agent’s Knowledge Model
Our proposed agent model adopts a systematic design structure for their be-
liefs. In particular agent’s belief base can be logically structured in two diﬀerent
(sub)sets. The ﬁrst set of beliefs indicates symbolic references to the generic
objects of the simulation (i.e. system entities, workspaces, identiﬁers, registered
CArtAgO sensors etc.). Belonging to this class, a nRooms(Nr) belief is updated
once the agent joins the environment, Nr indicating the current number of rooms.
Besides, agent’s cognitive process relies on a second class of beliefs, taking into
account the particular relation, in cognitive agents, between relevant informa-
tion and goal processing. Two groups of beliefs belong to this class. The former
belief set is composed by facts on the form cleaned(N,T) containing the state of
the various rooms (N indicating a room identiﬁer between 1 and Nr, T containing
the time-stamp at which this information has been retrieved). The latter belief
set is related to the room the agent has committed to clean (targetRoom(N)).
We consider these beliefs as “salient” facts, namely the information which is
relevant for agents for pursuing their goals and thus for selecting and commit-
ting the intention to clean a given room. The notion of relevance concerns here
those information required to agents for ruling over deliberation and means-end
reasoning. As in [3] we refer to this class of relevant beliefs as goal-supporting
beliefs. Diﬀerently from the ﬁrst deﬁned class of beliefs, goal-supporting beliefs
are dynamic ones: during agents’ purposive activities they need to be updated
in order to be consistent with world changes. Notice that information about the
state of each room can be considered certain only at a given time t, afterwards
the activities of other agents (or the rise of new trash items) would have modiﬁed
the room state. As explained in section Section 4, we will exploit the number of
belief update operations (pursued by agents upon these relevant facts) as a con-
crete measure for the computational load addressed by the cognitive processes.
3.2
Cleaner Agent Using Logs
For simplicity we here describe a cutout of the code for the Cleaner agent
implementing the log strategy. Agents are implemented in AgentSpeak using
the Jason platform [1] 3.
The initial goal join allows the agent to register himself to the RoomsWorld
workspace and to join the environment artifact. The agent here uses the
getNRooms operation retrieving the number of rooms n_rooms(Nr) where Nr
uniﬁes with the number of rooms signalled by the environment.
3 AgentSpeak is an agent programming language for goal-oriented, BDI agents. The
entire code of RoomsWorld experiment, along with agents implementation using al-
ternative platforms, is available on CArtAgO web site.

122
M. Piunti and A. Ricci
+!join : myNameID(N) & mySensor(S);
<- cartago.joinWorkspace("RoomsWorld","localhost:4010");
!locate_artifacts;
?artifactBel(environment, Env);
cartago.use(Env, join(N), S);
cartago.sense(S, joined(Loc));
cartago.use(Env, getNRooms, S);
cartago.sense(S, n_rooms(Nr));
+nRooms(Nr);
!explore.
+!explore : nRooms(N) & mySensor(S);
<- -targetLog(_);
roomsworld.randomInt(N,Rid);
+targetLog(Rid+1);
?artifactBel(watch, IDWatch);
cartago.observeProperty(IDWatch,
currentime(Wt) );
!analizeBel(X+1,Wt).
Notice that a particular plan (locate_artifacts) is then called to retrieve
and store the identiﬁers belonging to the artifacts that will be used dur-
ing the task. Once retrieved through the use of cartago.lookupArtifact
operation, the artifacts identiﬁers are stored in a belief set in the form
+artifactBel(artifactName, artifactId) providing a knowledge repository
associating each artifact name to the corresponding ID. Notice that those beliefs
are not considered goal supporting beliefs, being static references to locate sys-
tem resources more than information required to deliberate and make decisions.
The following explore goal is the starting point for agent’s purposive activity,
by which the agent randomly select a room Rid and prepare a new intention to
explore in it. The action cartago.use(W, whatTime, S) executed upon the
watch artifact allows the agent to retrieve the actual time: the agent observes
the watch artifact, getting the content of the time(Wt) percept (WT uniﬁes with
the percept content signalled by the watch)4.
To ﬁlter out worth intentions and thus prevent exploring places which have
been recently visited, the randomly selected room is then compared with the
relevant belief base that refers to the acknowledged state of the rooms. So far,
if the diﬀerence between the remembered time and the time at which the target
room has been previously cleaned is greater than a given threshold D, the agent
“commits” to the intention to go toward the room and then try to use the
related log (i.e. by updating relevant belief targetRoom(N)). Otherwise, the
agent abandons the current plan and readopt the explore goal to generate a
diﬀerent intention. Notice here that the agent is using his relevant information
to make a decision about his next course of actions.
+!analizeBel(N,Wt) : not cleaned(N,Lt) | (cleaned(N,Lt) & day(D) & (D< T-Wt))
<- -cleaned(N,_); -targetRoom(_); +targetRoom(N); !observeLog(N).
+!analizeBel(N,Wt) <- !explore.
So far, the agent observes the log artifact corresponding to the selected
room and reads the last recorded time-stamp. This is done by executing the
observeProperty action and perceiving the lastnote value, carrying to the
agent the relevant information about the last time-stamp. We here focus on
an interesting diﬀerence with respect to the strategies employed by other agent
teams. Whereas normal agents have to autonomously update the knowledge
about the problem domain and messenger agents have to continuously process
4 The cartago.* actions refer to CArtAgO basic primitives (see Section 2.2), while
roomsworld.* refer to the library of internal actions deﬁned to operate within the
RoomsWorld scenario.

Cognitive Use of Artifacts: Exploiting Relevant Information Residing
123
incoming messages, here the log agent ﬁnds information by simply observing the
log artifact on which he is interested. Relevant information left by some other
agent at the end of a previous cleaning activity has been stored and collected
within the artifact that makes it available for the overall society. This approach
directly enables uncoupled interactions – mediated by the log – among diﬀerent
agents during their practical behavior.
+!observeLog(N) : targetCh(N) & myRoom(MR)
<- roomsworld.goTo(MR, log(N));
-targetCh(N);
[ ... retrieve IDLog and IDWatch ... ]
cartago.observeProperty(IDLog, lastnote(LogT) );
cartago.observeProperty(IDWatch, currentime(Wt) );
!decide(N, Wt, LogT).
Once the agent has observed both information about log’s last note LogT and
the information about the current time WT provided by the watch, he can decide
what to do next. Notice that the agent is here deciding upon a situated infor-
mation (the actual state retrieved from the log corresponds to the last known
state of the room), being this information (in the case of team activities) pos-
sibly updated by some other agent of the group. If the diﬀerence between the
actual time WT and the time-stamp retrieved in the log LogT is greater than a
given threshold (say, a day’s length day(D)), the agent maintains his intention
of entering the room N, otherwise he reconsiders his intentions and adopt a new
explore goal to select a new room.
+!decide(N, CurrentTime, LastTime)
: (day(D) & (D< CurrentTime-LastTime))
| (LastTime=0)
<- !log("DECIDE TO ENTER! Put a note on Log");
[ ... retrieve sensor S and Log name LogN ...]
cartago.lookupArtifact(LogN, IDLog);
cartago.use(IDLog, putNote(CurrentTime), S);
!go(N).
+!decide(N, CurrentTime, LastTime)
<- !log("RECONSIDERED INTENTIONS:EXPLORING!") ;
-+cleaned(N,LastTime);
-targetRoom(_);
!explore.
It is worth noting that, within the decided plan, the agent updates either the
state of his belief or the state of the log. Indeed once the agent has decided to en-
ter the room, he puts a note on the log, either anticipating the information about
that room to be cleaned or preventing follower agents to waste their resources go-
ing to explore the same room. Otherwise the agent needs to update the relevant
beliefs and thus reconsider his intentions: in this case room N will be stored as
‘cleaned’ at time LastTime through the belief update +cleaned(N,LastTime).
The following action go(N) allows the agent to enter the selected room N. After
having updated the beliefs about target room and actual room, it’s time for the
agent to search for trash items: roomsworld.epistemicAction(N) is an internal
action used to perceive the objects situated in the room. The epistemic action, in
turn, encapsulates the use of the environment artifact and simulates agent’s per-
ceptive activities transforming volatile percepts (coming from CArtAgO sensors)
into agent beliefs.
+!go(N): not myRoom(N)
<- .print("enter in room ", N);
?myRoom(MR); roomsworld.goTo(MR, room(N));
-targetRoom(_);
-+myRoom(N);
roomsworld.epistemicAction(N);
!clean(N).
+!go(N) <- !explore.
If trash is found in the room, the epistemic action adds beliefs in the form
trash(N,X,Y) where N is the room ID, and X, Y are the coordinates of the

124
M. Piunti and A. Ricci
discovered trash item. Once the agent has located trash items (if any), he can
reach them with the action roomsworld.goTo(X,Y) and clean exploiting the
cartago.use(E, clean(Na, X, Y), S) operation upon the environment E. Ac-
cordingly, the agent updates the beliefs referring to the state of the room. Oth-
erwise, if the epistemic action has returned no items, the agent drops the goal,
reconsider the current intention exploring another room.
Finally,
the
action
to
clean
the
trash
item
is
realized
calling
the
cartago.use(Env, clean(Na, X, Y), S) operation upon the Env artifact.
The agent indicates his own name and the coordinates of the trash to clean
(which is then returned as a percept). It is worth noting that the clean/3 op-
eration may fail, due to the non-determinism of the environment or due to the
execution of concurrent cleaning actions performed by two or more agents on
the same trash item at the same time. To verify the outcome of the clean op-
eration the agent indicates a sensor S, which is used to collect the events sig-
nalled by the log.
4
RoomsWorld Experiment
To evaluate the diﬀerent interaction strategies for cleaner agents in RoomsWorld
described in Section 3, we ran sequences of trials measuring performances for
diﬀerent teams belonging to the diﬀerent strategies. RoomsWorld was set to 8
rooms and the total amount of trash items contemporaneously present is limited
to 4, whilst no more than one trash is generated for each room (Fig. 2). Each
reported experiment consists of 14 repeated runs for each team, using diﬀerent
randomly generated initial conditions and distribution of trash items.
The payoﬀs of the three strategies are based on speciﬁc metrics taking into
account the computational and behavioral eﬀectivenes for the various agents.
What we are interested in is a quantitative evaluation of the balance between
computational costs spent by agents to update their beliefs and the absolute
performances in terms of achieved goals (i.e., agent’s score). This allows us to
give an account about the agents reasoning processes in terms of computational
load. For each trial we deﬁned team’s goal eﬀectiveness in terms of cleaned
items (i.e. agent’s score) and in function of elapsed time. Besides, for each agent
typology, we focused on the particular belief set which is involved in updating
relevant information used to support reasoning and thus to perform the task. In
more detail, we deﬁne the belief change cost as the aggregate of belief change
operations performed by all the agents of the team upon their relevant beliefs (i.e.
the sum of all modiﬁcations of agent’s relevant belief base during all the trials).
Given the structure of relevant belief set as it has been deﬁned in Subsection 3.2,
two kinds of updates entail an increase of belief change cost:
1. The state of a given room N may be known by agents as cleaned at a certain
instant of time T ime through the addition of relevant +cleaned(N,Time)
facts. Once an agent has become aware of the new state of a room N (ei-
ther because of a message sent by another agent, or because the agent has
retrieved some log report, or because he has autonomously perceived the

Cognitive Use of Artifacts: Exploiting Relevant Information Residing
125
state of a room) he immediately updates the relative facts. According to the
deﬁnition, revising the relevant belief base with a new cleaned(N,T) fact
entails a belief change cost of +1.
2. An additional cost is accounted by updating the belief on targetRoom(N). In
so doing, the agent reconsiders his intention selecting an alternative course of
action. In turn, no more applicable plans will match the context-conditions
about the room identiﬁer. This entails the agent to reconsider his plans,
selecting a new intention and exploring a new room.
The following cutout shows how the messenger agents are supposed to spend two
units of belief change cost. Sender agents update the cleaned(N,T) belief once
a room has been cleaned (right column) and must broadcast this information
to the team; accordingly, a receiver agent updates his beliefs about the state of
that room once a message is received (left column).
+!clean(N) : not trash(N,_,_)
<-
?artifactBel(watch, W);
cartago.observeProperty(W, sim_time(Wt));
.broadcast(tell, cleaned(room(N, Wt)));
-+cleaned(N,Wt);
roomsworld.goTo(N, log(N));
!explore.
+cleaned(room(N,T))[source(Ag)] : true
<-
-cleaned(room(N,_))[source(_)];
-+cleaned(N,T);
For each team we also deﬁne the cost eﬀectiveness ratio in terms of total amount
of achieved tasks (agent’s score) divided by the belief change cost. Namely, cost
eﬀectiveness represents the unit of achieved goals for each belief change.
Conducted experiments considered the above deﬁned metrics and used the
threshold D to 12 and 6 units of simulated time respectively for teams of two
and four agents. Because of the random distributions of items, the previously
deﬁned metrics present a ﬂuctuating course before converging. Hence the length
of each individual trial has been set suﬃciently large to become stable and,
after the analysis of the courses of experiments, it has been set to 250 units of
simulated time. Fig. 3 and Fig. 4 show the performance of the diﬀerent teams by
averaging progresses of their eﬀectiveness (on the left) and their cost ratio (on
the right). In particular Fig. 3 refers for teams composed by two agents, while
Fig. 4 to teams of four agents.
4.1
Experiment Discussion
Experiment results put in evidence the role of log artifacts functioning as cog-
nitive resources for agents (a belief base enhancer in this case), clearly enabling
agents to ease their epistemic activities. Anyhow, on the basis of the deﬁned
metrics, the results show some noticeable payoﬀfor the various strategies. In
the case of teams composed by two agents, messenger agents attain the best
performance in terms of goal eﬀectiveness (Fig. 3, left). Even if goal eﬀective-
ness for messenger and log agents approximatively converge to the same value
(both reach a goal eﬀectiveness of about 0.13 cleaned items on each elapsed
time unit), messenger seems to tackle a shorter transitory phase. This evidence
clearly comes from the fact that messengers can suddenly achieve and main-
tain an updated knowledge of the global states of the various rooms. Indeed, on

126
M. Piunti and A. Ricci
Fig. 3. Experiment 1: teams are composed by two agents using the same interaction
strategy. Agents performances are measured in terms of goal eﬀectiveness (left) re-
ferred to the amount of achieved goals, and cost eﬀectiveness (right) referred to the
computational load related to the update processing of relevant beliefs.
Fig. 4. Experiment 2: teams are composed by 4 agents. As before, performances are
measured in terms of goal eﬀectiveness (left), and costs eﬀectiveness (right).
each achieved goal, message exchange allows agents to handle a more complete
knowledge of environments. By augmenting the team members the global perfor-
mance in terms of achieved goals for the various teams reach a higher value and
messenger superiority in terms of achieved goals becomes less evident (Fig. 4,
left). In this case, societies of numerous agents are better prone to make relevant
information available to the overall group. Accordingly, agents using artifacts
are more eﬀective to cooperatively use the logs to get a more up-to-date state
of the rooms. Hence, for teams composed by four agents, the goal eﬀectiveness
for messenger and log agents approximatively converge to the same value (both
the teams reach a global goal eﬀectiveness of about 0.22 cleaned items on each
elapsed time unit). Besides, the log agents outperform normal agents in all con-
ditions: whereas normal agents waste time and resources randomly looking for
trash in rooms that have just been cleaned, log agents are smarter in recognising
rooms requiring services, thus better balancing their payoﬀbetween exploration
and exploitation.
Performance result inverted when considering the cost eﬀectiveness: in this
case the winning team is the one composed by normal agents, due to the lower
amount of belief update performed upon the relevant facts and to the lower
frequency of intention reconsideration. As Fig. 3 (right) shows, for each internal

Cognitive Use of Artifacts: Exploiting Relevant Information Residing
127
belief update the teams composed by two normal agents achieve 0.25 goals (0.27
for logging agents and 0.15 for messenger agents). Similar results have been
collected for the teams composed by 4 agents (Fig. 4, right), where messenger
agents attain a very poor performance (whilst normal and log agents achieve 0.22
cleaned trash items for each belief update, messenger agents reach the value of
0.09, due to the signiﬁcant increase of message passing).
Balancing the performances for each selected metrics in each environmen-
tal condition, we here enlighten the eﬀectiveness oﬀered by the communication
strategy mediated by log artifacts. The advantages of exploiting distributed logs
to externalise information which is relevant for coordinating team activities are
more evident the more the team is numerous. Logs give a considerable con-
tribute both in terms of propagation and synchronization of the information.
Even more, logs provide a informational support in uncertain, transitory condi-
tions (i.e. when agents need to adjust behavior given environment partial observ-
ability). Although broadcasting messages to the overall society allows agents to
maintain an updated knowledge of the global environment, this requires agents
to waste their computational resources to continually process messages which
are not strictly relevant for the ongoing task. On the contrary, log agents locally
exploit information concerning their actual purposes, fully exploiting logs as
suitable belief base enhancer. Diﬀerently from other approaches exploiting me-
diated interactions as shared memories and blackboards, logs are here conceived
with the aim to improve situated cognition: agents can exploit logs to attain only
the local information which is relevant to achieve the actual goal. Furthermore,
the information coming from distant rooms is not relevant for agents because
does not aﬀect their ongoing intentions. This allows agents to deal with situated
information, and may not require agents to pay attention (nor spend compu-
tational resources) to information which is useless with respect of the ongoing
tasks. It is worth remarking that logs store the information which remains avail-
able even beyond their use. This has a pivotal importance in the context of open
systems, where diﬀerent agents may asynchronously operate, with interleaved
presence, in speciﬁc tasks. Agents using logs are deliberatively modifying their
environment with the aim to coordinate with other agents of the society and let
them know the actual state of aﬀairs. In so doing, the global behavior of the
society is governed by the emerging contents of the distributed logs, which are
cooperatively updated by agents during their activities.
5
Conclusions and Related Works
In this paper we discussed the role that cognitive artifacts, as artifacts with
speciﬁc representational functionalities, can play in multi-agent systems, analo-
gously to the human case, in supporting eﬃciently the cooperation of intelligent
agents, who can share information by cooperatively updating the state of the
artifacts, thus externalizing belief revision activities and easing their computa-
tional burdens. A ﬁrst important outcome in exploiting cognitive artifact is in
promoting mediated interaction, concerning comunication and knowledge trans-
mission. Exploiting artifact can be viewed as a complimentary approach with

128
M. Piunti and A. Ricci
respect to direct communication based on messages and ACL. For this purpose,
cognitive artifacts provide a natural support for time and space uncoupled com-
munication, being then particularly useful in loose coupled interactions and open
systems, where agents need to cooperate without necessarily being in the same
temporal and spatial context.
This aspect is in agreement with many other research works recently appeared
in literature (see [14] for a survey), remarking the role that the environment
could play in designing complex MAS, as a suitable place where to encapsu-
late functionalities and services useful for agent interaction, coordination and
cooperation. Among these, few works are speciﬁcally about cognitive MAS, in
particular about high-level environment models speciﬁcally conceived to sup-
port and promote goal-oriented behavior of cognitive agents, and related agent
reasoning techniques. A ﬁrst one is Brahms [12], a multi-agent programming lan-
guage and platform to develop and simulate multi-agent models of human and
machine behavior, based on a theory of work practice and situated cognition.
Our approach shares many points with Brahms, starting from a common refer-
ence conceptual background based on conceptual frameworks such as Activity
Theory. Diﬀerently from Brahms, our primary context is not modeling and sim-
ulation, but agent-oriented software development. A further work is GOLEM [2],
that introduces a platform for modeling situated cognitive agents in distributed
environments by declaratively describing the representation of the environment
in a logic-based form. GOLEM models (physical) environments in terms of con-
tainers where agents based on the KGP model and objects are situated. Besides
sharing the same modeling perspective, which can be traced back to our early
works on artifact-based coordination in MAS [9], we here investigate the cogni-
tive use of artifacts, focussing in particular on the role of cognitive artifacts.
References
1. Bordini, R., H¨ubner, J.: BDI agent programming in AgentSpeak using Jason. In:
Toni, F., Torroni, P. (eds.) CLIMA 2005. LNCS (LNAI), vol. 3900, pp. 143–164.
Springer, Heidelberg (2006)
2. Bromuri, S., Stathis, K.: Situating Cognitive Agents in GOLEM. In: Weyns, D.,
Brueckner, S.A., Demazeau, Y. (eds.) EEMMAS 2007. LNCS (LNAI), vol. 5049,
pp. 115–134. Springer, Heidelberg (2008)
3. Castelfranchi, C., Paglieri, F.: The role of beliefs in goal dynamics: Prolegomena
to a constructive theory of intentions. Synthese 155, 237–263 (2007)
4. Clark, A., Chalmers, D.: The extended mind. Analysis 58(1), 7–19 (1998)
5. Kirsh, D.: The intelligent use of space. Artif. Intell. 73(1-2), 31–68 (1995)
6. Nardi, B.A.: Context and Consciousness: Activity Theory and Human-Computer
Interaction. MIT Press, Cambridge (1996)
7. Norman, D.: Cognitive artifacts. In: Designing interaction: Psychology at the
human–computer interface. Cambridge University Press, New York (1991)
8. Omicini, A., Ricci, A., Viroli, M.: Artifacts in the A&A meta-model for multi-agent
systems. Autonomous Agents and Multi-Agent Systems 17(3) (2008)
9. Omicini, A., Ricci, A., Viroli, M., Castelfranchi, C., Tummolini, L.: Coordination
Artifacts: Environment-based Coordination for Intelligent Agents. In: Proceedings
of AAMAS 2004, New York, USA, vol. 1, pp. 286–293 (2004)

Cognitive Use of Artifacts: Exploiting Relevant Information Residing
129
10. Piunti, M., Ricci, A.: From Agents to Artifacts Back and Forth: Purposive and
Doxastic use of Artifacts in MAS. In: Proc. of 6th European Workshop on Multi-
Agent Systems (EUMAS 2008), Bath, UK (2008)
11. Ricci, A., Piunti, M., Acay, L.D., Bordini, R., Hubner, J., Dastani, M.: Integrating
Artifact-Based Environments with Heterogeneous Agent-Programming Platforms.
In: Proceedings of AAMAS 2008 (2008)
12. Sierhuis, M., Clancey, W.J.: Modeling and simulating work practice: A human-
centered method for work systems design. IEEE Intelligent Systems 17(5) (2002)
13. Weyns, D., Omicini, A., Odell, J.: Environment as a First-class Abstraction in
MAS. In: Autonomous Agents and Multi-Agent Systems [14], pp. 5–30
14. Weyns, D., Parunak, H.V.D.: Special issue on environments for multi-agent sys-
tems. Autonomous Agents and Multi-Agent Systems 14(1), 1–116 (2007)

Information-Based Argumentation
Carles Sierra1 and John Debenham2
1 Institut d’Investigaci´o en Intel·lig`encia Artiﬁcial – IIIA,
Spanish Scientiﬁc Research Council, CSIC
08193 Bellaterra, Catalonia, Spain
sierra@iiia.csic.es
2 University of Technology, Sydney, Australia
debenham@it.uts.edu.au
Abstract. Information-based argumentation aims to model the partner’s reason-
ing apparatus to the extent that an agent can work with it to achieve outcomes that
are mutually satisfactory and lay the foundation for continued interaction and per-
haps lasting business relationships. Information-based agents take observations at
face value, qualify them with a belief probability and build models solely on the
basis of messages received. Using augmentative dialogue that describes what is
good or bad about proposals, these agents observe such statements and aim to
model the way their partners react, and then to generate dialogue that works in
harmony with their partner’s reasoning.
1
Introduction
This paper is in the area labelled: information-based agency [1]. An information-based
agent has an identity, values, needs, plans and strategies all of which are expressed using
a ﬁxed ontology in probabilistic logic for internal representation and in an illocution-
ary language [2] for communication. All of the forgoing is represented in the agent’s
deliberative machinery.
In line with our “Information Principle” [2], an information-based agent makes no a
priori assumptions about the states of the world or the other agents in it — represented
in a world model inferred from the messages that it receives. These agents build up their
models by comparing expectation with observation — in this way we have constructed
general models of trust, honour and reliability in a single framework [1].
[2] describes a rhetorical argumentation framework that supports argumentative ne-
gotiation. It does this by taking into account: the relative information gain of a new ut-
terance and the relative semantic distance between an utterance and the dialogue history.
Then [3] considered the effect that argumentative dialogues have on the on-going rela-
tionship between a pair of negotiating agents. Neither of these contributions addressed
the relationship between argumentative utterances or strategies for argumentation. In
this paper we adress these two issues.
The basis of our approach differs from [4] who builds on the notion of one argument
“attacking” another. With the exception of a logical ‘attack’, whether one argument
attacks another or not will depend on the receiving agent’s private circumstances that are
unlikely to be fully articulated. Thus, the notion of attack is of little use to information-
based agents that build their models on the contents of utterances. This paper considers
J.-J.Ch. Meyer and J.M. Broersen (Eds.): KRAMAS 2008, LNAI 5605, pp. 130–144, 2009.
c⃝Springer-Verlag Berlin Heidelberg 2009

Information-Based Argumentation
131
how to counter the effect of the partner agent’s arguments, and aims to lead a negotiation
towards some desired outcome by persuasive argumentation.
This paper is based in rhetorical argumentation [5]. For example, suppose I am shop-
ping for a new car and have cited “suitability for a family” as a criterion. The salesman
says “This LandMonster is great value,” and I reply “My grandmother could not climb
into that.” Classical argumentation may attempt to refute the matriarch’s lack of gym-
nastic prowess or the car’s inaccessibility. Taking a less confrontational and more con-
structively persuasive view we might note that this statement impacts negatively on the
“suitability for a family” criterion, and attempt to counter that impact possibly with “It’s
been voted No 1 for children.” Although a smarter response may look for an argument
that is semantically closer: “The car’s height ensures a very comfortable ride over rough
terrain that is popular with old people.”
Information-based agents build their world models using an expectation/observation
framework; this includes a model of the negotiation partner’s behaviour. Agents form an
a priori expectation of the signiﬁcance of every event that occurs, and when the effect
of that event is ﬁnally observed they revise their expectations. The model of behaviour
includes measures of: trust, honour, reliability, intimacy, balance and disposition —
disposition attempts to model what the partner means which may not be what they say.
These measures are summarised using: temporal criteria, the structure of the ontology,
and the illocutionary category of observed utterances.
Our argumentation agent has to perform two key functions: to understand incoming
utterances and to generate responses. The approach is founded on a model of contract
acceptance that is described in Section 2. Section 3 details a scenario that provides the
context for the discussion. Sections 4 and 5 consider the scenario from each side of the
bargaining table. Reactive and proactive argumentation strategies are given in Section 6,
and Section 7 concludes.
2
Contract Acceptance
No matter what interaction strategy an agent uses, and no matter whether the communi-
cation language is that of simple bargaining or rich argumentation, a negotiation agent
will have to decide whether or not to sign each contract on the table. We will argue in
Section 4 that the buyer will be uncertain of his preferences in our Scenario described
in Section 3. If an agent’s preferences are uncertain then it may not make sense to link
the agent’s criterion for contract acceptance to a strategy that aims to optimise its util-
ity. Instead, we pose the more general question: “how certain am I that δ = (φ, ϕ) is
a good contract to sign?” — under realistic conditions this may be easy to estimate.
Pt(sign(α, β, χ, δ)) estimates the certainty, expressed as a probability, that α should
sign1 proposal δ in satisfaction of her need χ, where in (φ, ϕ) φ is α’s commitment and
ϕ is β’s. α will accept δ if: Pt(sign(α, β, χ, δ)) > c, for some level of certainty c.
To estimate Pt(sign(α, β, χ, δ)), α will be concerned about what will occur if con-
tract δ is signed. If agent α receives a commitment from β, α will be interested in any
1 A richer formulation is Pt(eval(α, β, χ, δ) = ei) where eval(·) is a function whose range is
some descriptive evaluation space containing terms such as “unattractive in the long term”.

132
C. Sierra and J. Debenham
variation between β’s commitment, ϕ, and what is actually observed, as the enactment,
ϕ′. We denote the relationship between commitment and enactment:
Pt(Observe(α, ϕ′)|Commit(β, α, ϕ))
simply as Pt(ϕ′|ϕ) ∈Mt, and now α has to estimate her belief in the acceptability of
each possible outcome δ′ = (φ′, ϕ′). Let Pt(acc(α, χ, δ′)) denote α’s estimate of her
belief that the outcome δ′ will be acceptable in satisfaction of her need χ, then we have:
Pt(sign(α, β, χ, δ)) = f(Pt(δ′|δ), Pt(acc(α, χ, δ′)))
(1)
for some function f;2 if f is the arithmetic product then this expression is mathematical
expectation. f may be more sensitive; for example, it may be deﬁned to ensure that no
contract is signed if there is a signiﬁcant probability for a catastrophic outcome.
There is no prescriptive way in which α should deﬁne Pt(acc(α, χ, δ′)), it is a matter
for applied artiﬁcial intelligence to capture the essence of what matters in the applica-
tion. In any real application the following three components at least will be required.
Pt(satisfy(α, χ, δ′)) represents α’s belief that enactment δ′ will satisfy her need χ.
Pt(obj(α, δ′)) represents α’s belief that δ′ is a fair deal against the open marketplace
— it represents α’s objective valuation. Pt(sub(α, χ, δ′)) represents α’s belief that δ′ is
acceptable in her own terms taking account of her ability to meet her commitment φ [2]
[1], and any way in which δ′ has value to her personally3 — it represents α’s subjective
valuation. That is:
Pt(acc(α, χ, δ′)) = g(Pt(satisfy(α, χ, δ′)), Pt(obj(α, δ′)), Pt(sub(α, χ, δ′)))
(2)
for some function g.
Suppose that an agent is able to estimate: Pt(satisfy(α, χ, δ′)), Pt(obj(α, δ′)) and
Pt(sub(α, χ, δ′)). The speciﬁcation of the aggregating g function will then be a strictly
subjective decision. A highly cautious agent may choose to deﬁne:
Pt(acc(α, χ, δ′)) =
⎧
⎪
⎨
⎪
⎩
1
if: Pt(satisfy(α, χ, δ′)) > η1
∧Pt(obj(α, δ′)) > η2 ∧Pt(sub(α, χ, δ′)) > η3
0
otherwise.
for some threshold constants ηi. Whereas an agent that was prepared to permit some
propagation of conﬁdence from one factor to compensate another could deﬁne:
Pt(acc(α, χ, δ′)) = Pt(satisfy(α, χ, δ′))η1 × Pt(obj(α, δ′))η2 × Pt(sub(α, χ, δ′))η3
where the ηi balance the inﬂuence of each factor.
The point of this is: if an agent aims to produce persuasive argumentative dialogue
then in the absence of any speciﬁc information concerning the structure of g the agent
2 β inﬂuences the equation in the sense that different βs yield different Pt(δ′|δ).
3 For example, when buying a new digital camera, α may give a high subjective valuation to a
camera that uses the same memory cards as her existing camera.

Information-Based Argumentation
133
should ignore g and concentrate on the three categories: Pt(satisfy(α, χ, δ′)),
Pt(obj(α, δ′)) and Pt(sub(α, χ, δ′)).
So
how
then
will
α
specify:
Pt(satisfy(α, χ, δ)),
Pt(sub(α, χ, δ))
and
Pt(obj(α, δ))? Of these three factors only Pt(obj(α, δ)) has a clear meaning, but it
may only be estimated if there is sufﬁcient market data available. In the case of selling
sardines this may well be so, but in the case of Google launching a take-over bid for
Microsoft it will not4. Concerning Pt(satisfy(α, χ, δ)) and Pt(sub(α, χ, δ)) we assume
that an agent will somehow assess each of these as some combination of the conﬁdence
levels across a set of privately-known criteria. For example, if I am buying a camera
then I may be prepared to deﬁne:
Pt(satisfy(α, χ, δ)) = h(Pt(easy-to-use(α, δ)), Pt(well-built(α, δ)))
(3)
for some function h. Any attempt to model another agent’s h function will be as difﬁcult
as modelling g above. But, it is perfectly reasonable to suggest that by observing my
argumentative dialogue an agent could form a view as to which of these two criteria
above was more important.
This paper considers how an agent may observe the argumentative dialogue with the
aim of modelling, within each of the three basic factors, the partner’s criteria and the rel-
ative importance of those criteria. In repeated dealings between two agents, this model
may be strengthened when the objects of the successive negotiations are semantically
close but not necessarily identical.
3
The Scenario
Rhetorical argumentation is freed from the rigour of classical argumentation and de-
scriptions of it can take the form of “this is how it works here” and “this is how it works
there” without describing a formal basis. We attempt to improve on this level of vagary
by using a general scenario and describing the behaviour of our agents within it.
In a general retail scenario there is a seller agent, α, and a buyer, β. The items for
sale are abstracted from: digital cameras, mobile phones, PDAs, smart video recorders,
computer software, sewing machines and kitchen mixers. The features of an item are
those that are typically listed on the last few pages of an instruction booklet. For exam-
ple, a camera’s features could include the various shutter speeds that it is capable of,
the various aperture settings, the number of years of warranty, and so on — together the
features describe the capabilities of the item. For the purpose of comparison with other
items, β will consider a particular item as a typed Boolean vector over the (possible)
features of each item available, this vector shows which feature is present. The state of
an item is then speciﬁed by identifying which of the item’s features are ‘on’. For exam-
ple, the state of a camera could be: ‘ready’ with aperture set to ‘f8’ and shutter speed
set to ‘1 500’th of a second’. In this scenario an offer is a pair (supply of a particular
item, supply of some money) being α’s and β’s commitments respectively.
β may wish to know how well an item performs certain tasks. Software agents are
not naturally endowed with the range of sensory and motor functions to enable such
4 In this example the subjective valuation will be highly complex.

134
C. Sierra and J. Debenham
an evaluation. We imagine that the seller agent has an associated tame human who will
demonstrate how the various items perform particular tasks on request, but performs
no other function. We also imagine that the buyer agent has an associated tame human
who can observe what is demonstrated, articulates an evaluation of it that is passed to
its own agent, but performs no other function.
To simplify our set up we assume that the seller, α, is β’s only source of information
about what tasks each item can perform, and, as we describe below, what sequence of
actions are necessary to make an item perform certain tasks5. That is, our multiagent
system consists only of {α, β}, and the buyer is denied access to product reviews, but
does have access to market pricing data. This restriction simpliﬁes the interactions and
focusses the discussion on the argumentation.
For example, if the item is a camera the buyer may wish to observe how to set the
camera’s states so that it may be used for ‘point-and-shoot’ photography. If the item is
a sewing machine she may wish to see how to make a button hole on a piece of cloth.
If the item is graphics software she may wish to see how to draw a polygon with a two-
pixel red line and to colour the polygon’s interior blue. These tasks will be achieved by
enacting a process that causes the item to pass though a sequence of states that will be
explained to β by α. So far our model consists of: features, states, sequences and tasks.
We assume that the object of the negotiation is clear where the object is an uninstan-
tiated statement of what both agents jointly understand as the intended outcome — e.g.
I wish to exchange a quantity of eggs of certain quality for cash. We assume that each
agent is negotiating with the aim of satisfying some goal or need that is private knowl-
edge. In determining whether a negotiation outcome is acceptable in satisfaction of a
need we assume that an agent will blend the factors in our acceptance model described
in Section 2. We assume that for each factor an agent will articulate a set of criteria that
together determine whether the factor is acceptable. The criteria may include private
information such as deadlines.
More formally, there is a set of feature names, F, a set of item names, I, a feature
mapping: feature : I →×n(B : F) where there are n feature names, and B is a
boolean variable that may be ⊤or ⊥. Each item name belongs to a unique concept —
e.g.: “Nikon123 is-a camera”. For any particular item name, ν, feature(ν) will be a
typed Boolean vector indicating which features that item ν possesses. Let Fν be the set
of nν features that item ν possesses. At any particular time t, the state of an item is a
mapping: statet : I →×nν(B : Fν) where the value ⊤denotes that the corresponding
feature of that item is ‘on’. A sequence is an ordered set of states, (wi), where succes-
sive states differ in one feature only being on and off. A sequence is normally seen as
performing a task that are linked by the mapping: to-do : T →2S where T is the set of
tasks and S the set of all possible sequences — that is, there many be several sequences
that perform a task. If a sequence is performed on an item then, with the assistance
of a human, the agent rates how well it believes the sequence performs the associated
task. The evaluation space, E, could be {good, OK, bad}. A criterion is a predicate:
criterion(ν), meaning that the item ν satisﬁes criterion ‘criterion’. The set of criteria is
5 In other words, the sort of information that is normally available in the item’s Instruction
Booklet — we assume that α conveys this information accurately.

Information-Based Argumentation
135
C. The argumentation requirements include (where x ∈V , c ∈C, v = feature(x),
y ∈T , z ∈S, and r ∈R):
– “I need an x”
– “What sort of x do you need?”
– “I need an x that satisﬁes criterion c”
– “What features does x have?”
– “x has features v”
– “How do you make x do y”
– “The sequence z performed on x does y”
– “Perform sequence z on x”
– “If sequence z is performed on x then how would you rate that?”
– “I rate the sequence z as performed on x as r”
4
The Buyer Assesses a Contract
In this Section we consider how the buyer might use the general framework in Section 2
to assess a contract6. In general an agent will be concerned about the enactment of
any contract signed as described in Equation 1. In the scenario described in Section 3,
enactment is not an issue, and so we focus on Equation 2. To simplify things we ignore
the subjective valuation factor. Before addressing the remaining two factors we argue
that the buyer will not necessarily be preference aware.
Consider a human agent with a need for a new camera who goes to a trusted camera
shop. If the agent is preference aware he will be able to place the twenty to ﬁfty cam-
eras on offer in order of preference. If is reasonable to suggest that a normal, intelligent
human agent could not achieve this with any certainty, nor could he with conﬁdence
represent his uncertainty in his preferences as a probability distribution over his prefer-
ences. This lack of awareness of preferences may be partially due to lack of information
about each camera. But, what could “perfect information” realistically mean in this ex-
ample? Even if the purchaser could borrow all the cameras for a day and had access to
trusted, skilled users of each camera even then we suggest that our human agent would
still be unable to specify a preference order with conﬁdence. The underlying reason
being the size and complexity of the issue space required to describe all of the fea-
tures of every camera on offer, and the level of subjective judgement required to relate
combinations of those features to meaningful criteria.
In large issue spaces, in terms of which an agent is unable to specify a preference
ordering, there is a useful special case when it is possible to specify preferences on each
issue individually (e.g. “I prefer to pay less than more”, “I prefer to have a feature on
the camera to not having it”). In this case the agent is individual preference aware.
4.1
Assessing Pt(satisfy(β, χ, δ))
First β must give meaning to Pt(satisfy(β, χ, δ)) by deﬁning suitable criteria and the
way that the belief should be aggregated across those criteria. Suppose one of β’s cri-
teria is Pt(ease-of-use(β, δ)). The idea is that β will ask α to demonstrate how certain
6 The seller will have little difﬁculty in deciding whether a contract is acceptable if he knows
what the items cost.

136
C. Sierra and J. Debenham
tasks are performed, will observe the sequences that α performs, and will use those
observations to revise this probability distribution until some clear verdict appears.
Suppose the information acquisition process is managed by a plan π. Let random
variable X represent Pt(ease-of-use(β, δ) = ei) where the ei are values from an eval-
uation space that could be E ={fantastic, acceptable, just OK, shocking}. Then given
a sequence s that was supposed to achieve task τ, suppose that β’s tame human rates
s as evidence for ease-of-use as e ∈E with probability z. Suppose that β attaches a
weighting Rt(π, τ, s) to s, 0 < R < 1, which is β’s estimate of the signiﬁcance of the
observation of sequence s within plan π as an indicator of the true value of X. For exam-
ple, the on the basis of the observation alone β might rate ease-of-use as e = acceptable
with probability z = 0.8, and separately give a weighting of Rt(π, τ, s) = 0.9 to the
sequence s as an indicator of ease-of-use. For an information-based agent each plan π
has associated update functions, Jπ(·), such that JX
π (s) is a set of linear constraints
on the posterior distribution for X. In this example, the posterior value of ‘acceptable’
would simply be constrained to 0.8.
Denote the prior distribution Pt(X) by p, and let p(s) be the distribution with min-
imum relative entropy7 with respect to p: p(s) = arg minr

j rj log rj
pj that satisﬁes
the constraints JX
s (s). Then let q(s) be the distribution:
q(s) = Rt(π, τ, s) × p(s) + (1 −Rt(π, τ, s)) × p
(4)
and then let:
Pt(X(s)) =

q(s)
if q(s) is more interesting than p
p
otherwise
(5)
A general measure of whether q(s) is more interesting than p is: K(q(s)∥D(X)) >
K(p∥D(X)), where K(x∥y) = 
j xj log xj
yj is the Kullback-Leibler distance between
two probability distributions x and y, and D(X) is the expected distribution in the ab-
sence of any observations — D(X) could be the maximum entropy distribution. Finally,
Pt+1(X) = Pt(X(s)). This procedure deals with integrity decay, and with two probabil-
ities: ﬁrst, the probability z in the rating of the sequence s that was intended to achieve
τ, and second β’s weighting Rt(π, τ, s) of the signiﬁcance of τ as an indicator of the
true value of X. Equation 5 is intended to prevent weak information from decreasing
the certainty of Pt+1(X). For example if the current distribution is (0.1, 0.7, 0.1, 0.1),
indicating an “acceptable” rating, then weak evidence P(X = acceptable) = 0.25 is
discarded.
Equation 4 simply adds in new evidence p(s) to p weighted with Rt(π, τ, s). This is
fairly crude, but the observations are unlikely to be independent and the idea is that π
7 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI)
subject to a set of J linear constraints g = {gj(p) = aj ·p−cj = 0}, j = 1, . . . , J (that must
include the constraint 
i pi −1 = 0) is: p = arg minr

j rj log
rj
qj . This may be calculated
by introducing Lagrange multipliers λ: L(p, λ) = 
j pj log
pj
qj + λ · g. Minimising L,
{ ∂L
∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L
∂pi =
0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference
that is convenient when the data is sparse [6] and encapsulates common-sense reasoning [7].

Information-Based Argumentation
137
will specify a “fairly comprehensive” set of tasks aimed to determine Pt(X) to a level
of certainty sufﬁcient for Equation 2.
4.2
Assessing Pt(obj(α, δ))
Pt(obj(β, δ)) estimates the belief that δ is acceptable in the open-market that β may ob-
serve in the scenario. Information-based agents model what they don’t know with cer-
tainty as probability distributions. Suppose that X is a discrete random variable whose
true value is the open-market value of an item. First, β should be able to bound X to an
interval (xmin, xmax) — if this is all the evidence that β can muster then X will be the
ﬂat distribution (with maximum entropy) in this interval, and Pt(obj(β, (item, y)) =

x≥y P(X = x). β may observe evidence, perhaps as observed sale prices for similar
items, that enables him to revise particular values in the distribution for X. A method
[2] similar to that described in Section 4.1 is used to derive the posterior distribution
— it is not detailed here. An interesting aspect of this approach is that it works equally
well when the valuation space has more than one dimension.
5
The Seller Models the Buyer
In this Section we consider how the seller might model the buyer’s contract acceptance
logic in an argumentative context. As in Section 4 we focus on Equation 2 and for
reasons of economy concentrate on the factor: Pt(satisfy(α, χ, δ)).
5.1
Modelling Contract Acceptance
Suppose that β has found an item that he wants to buy, α will be interested in how much
he is prepared to pay. In a similar way to Section 4.2, α can interpret β’s proposals as
willingness to accept the offers proposed, and counter-offers as reluctance to accept the
agent’s prior offer — all of these interpretations being qualiﬁed with an epistemic be-
lief probability. Entropy-based inference is then used to derive a complete probability
distribution over the space of offers for a random variable that represents the partner’s
limit offers. This distribution is “the least biased estimate possible on the given infor-
mation; i.e. it is maximally noncommittal with regard to missing information” [8]. If
there are n-issues then the space of limit offers will be an (n −1)-dimensional surface
through offer space. As described in [2], this method works well as long as the number
of issues is not large and as long as the agent is aware of its partner’s preferences along
each dimension of the issue space.
5.2
Estimating β’s Key Criteria
α’s world model, Mt, contains probability distributions that model the agent’s belief in
the world, including the state of β. In particular, for every criterion c ∈C α associates
a random variable C with probability mass function Pt(C = ei).
β may present information in the form of a high-level description of what is required;
e.g. “I want a camera for every-day family use”. In a sense this is nothing more than a

138
C. Sierra and J. Debenham
criterion, but it does not ﬁt comfortably within the terms of Equation 2 and may have
implications for all of them. We call such a statement as the object of the negotiation. It
is realistic to assume that the object is common knowledge to some degree. We assume
that there is a structured section of the ontology that describes negotiation objects. We
also assume that for each object there is are prior probabilities associated with each of
a set of negotiation criteria represented in another section of the ontology. For example,
the object “A camera for every-day family use” may associate the prior probability
distribution (0.6, 0.4, 0.0, 0.0) with “ease of use for point-and-shoot” in terms of the
example evaluation space given above.
The distributions that relate object to criteria may be learned from prior experience.
If Pt(C = e|O = o) is the prior distribution for criteria C over an evaluation space
given that the object is o, then given evidence from a completed negotiation with object
o we use the standard update procedure described in Section 4.1. For example, given
evidence that α believes with probability p that C = ei in a negotiation with object o
then Pt+1(C = e|O = o) is the result of applying the constraint P(C = ei|O = o) = p
with minimum relative entropy inference as described previously, where the result of
the process is protected by Equation 5 to ensure that weak evidence does not override
prior estimates.
In the absence of evidence of the form described above, the distributions, Pt(C =
e|O = o), should gradually tend to ignorance. If a decay-limit distribution [2] is known
they should tend to it otherwise they should tend to the maximum entropy distribution.
In our scenario, during a dialogue β will ask α to perform certain tasks that we
assume are represented in a structured section of the ontology. Following the reason-
ing above, if α is asked to perform task τ then this may suggest prior beliefs of what
β’s criteria are. For example, suppose α is asked to demonstrate how to photograph
a duck when the object is a “camera suitable for photographing wildlife”. If the on-
tology relates ducks to water to some degree then α may believe that β rates the
criterion “waterproof” as “I would prefer to have this” with some probability. Then
if α is subsequently asked to to demonstrate how to photograph a duck when the
object is a s′ she may believe that “waterproof” is a criterion. Using the semantic-
similarity-based method described in [2], this evidence would update the estimates for
Pt(Waterproof = e|(O = o′, T = τ)) in a way that is moderated by the semantic
distance between s′ and ‘camera suitable for photographing wildlife‘”
The discussion so far has not considered argumentative dialogue such as: “My grand-
mother could not climb into that (car)”. This statement would presumably follow a re-
quest to demonstrate the task “how to get into the car” that the observer would rate
against the criterion “suitable for an octogenarian” as “unacceptable”. So this example
at least can be accommodated in the framework as long as we can link the ‘grand-
mother’ with an ‘octogenarian’. Given two strings of ontological concepts, S1 and S2,
deﬁne their similarity as:
Π(S1, S2) =

x∈S1,y∈S2 Sim(x, y)
||S1|| × ||S2||
(6)
where ||S|| is the number of concepts in S, and Sim(·) is measures semantic distance
[9]. A semantically deeper analyses of text would be better, but we claim that this

Information-Based Argumentation
139
approach will link such the given statement with the criterion “suitable for an octoge-
narian” — particularly if the set of admissible criteria are known and agreed in advance.
5.3
Disposition: Shaping the Stance
Agent β’s disposition is the underlying rationale that he has for a dialogue. α will
be concerned with the conﬁdence in α’s beliefs of β’s disposition as this will affect
the certainty with which α believes she knows β’s key criteria. Gauging disposition in
human discourse is not easy, but is certainly not impossible. We form expectations about
what will be said next; when those expectations are challenged we may well believe that
there is a shift in the rationale.
The bargaining literature consistently advises (see for example [10]) that an agent
should change its stance (one dimension of stance being the ‘nice guy’ / ‘tough guy’
axis) to prevent other agents from decrypting their private information, and so we should
expect some sort of “smoke screen” surrounding any dialogue between competitive
agents. It would be convenient to think of disposition as the mirror-image of stance,
but what matters is the agent’s conﬁdence in its model of the partner. The problem is
to differentiate between a partner that is skilfully preventing us from decrypting their
private information, and a partner that has either had a fundamental change of heart or
has changed his mind in a way that will signiﬁcantly inﬂuence the set of contracts that
he will agree to. The ﬁrst of these is normal behaviour, and the second means that the
models of the partner may well be inaccurate.
If an agent believes that her partner’s disposition has altered then the entropy of
her model of the partner should be increased — particularly beliefs concerning the key
criteria should be relaxed to prevent the dialogue attempting to enter a “blind alley”, and
to permit the search for common ground to proceed on broader basis. The mechanics for
achieving this are simple: if an agent believes that his partner’s disposition has shifted
then his certainty of belief in the structure of the model of the partner is decreased.
α’s model of β’s disposition is DC = Pt(C = e|O = o) for every criterion in the
ontology, where o is the object of the negotiation. α’s conﬁdence in β’s disposition is
the conﬁdence he has in these distributions. Given a negotiation object o, conﬁdence
will be aggregated from H(C = e|O = o) for every criterion in the ontology. Then the
Agent 
's model of 's acceptance
satisfy
objective
subjective

market
data
strengthen
task
sequence
request
model
of key 
criteria
assess
disposition
weaken
rating
rates
performs
Fig. 1. The model of β’s acceptance criteria that lies at the heart of the argumentation strategy

140
C. Sierra and J. Debenham
idea is that if in the negotiation for a camera “for family use” α is asked to demonstrate
how to photograph a drop of water falling from a tap then this would presumably cause
a dramatic difference between Pt(C = e|(O = “family use”)) and Pt(C = e|(O =
“family use”, O′ = “photograph water drops”)). This difference causes α to revise her
belief in “family use”, to revise the disposition towards distributions of higher entropy,
and to approach the negotiation on a broader basis. A high-level diagram of α’s model
of β’s acceptance criteria that includes disposition is shown in Figure 1.
6
Strategies
In this section we describe the components of an argumentation strategy starting with
tools for valuing information revelation that are used to model the fairness of a negoti-
ation dialogue. This work compares with [11], [12] and [13].
6.1
Information Revelation: Computing Counter Proposals
Everything that an agent communicates gives away information. The simple offer “you
may purchase this wine for e3” may be intrepretd in a utilitarian sense (e.g. the proﬁt
that you could make by purchasing it), and as information (in terms of the reduc-
tion of your entropy or uncertainty in your beliefs about my limit price for the item).
Information-based agents value information exchanged, and attempt to manage the as-
sociated costs and beneﬁts.
Illocutionary categories and an ontology together form a framework in which the
value of information exchanged can be categorised. The LOGIC framework for argu-
mentative negotiation [3] is based on ﬁve illocutionary categories: Legitimacy of the
arguments, Options i.e. deals that are acceptable, Goals i.e. motivation for the negoti-
ation, Independence i.e: outside options, and Commitments that the agent has including
its assets. In general, α has a set of illocutionary categories Y and a categorising function
κ : L →P(Y). The power set, P(Y), is required as some utterances belong to multiple
categories. For example, in the LOGIC framework the utterance “I will not pay more
for a bottle of Beaujolais than the price that John charges” is categorised as both Option
(what I will accept) and Independence (what I will do if this negotiation fails).
Then two central concepts describe relationships and dialogues between a pair of
agents. These are intimacy — degree of closeness, and balance — degree of fairness.
In this general model, the intimacy of α’s relationship with β, At, measures the amount
that α knows about β’s private information and is represented as real numeric values
over G = Y × V .
Suppose α receives utterance u from β and that category y ∈κ(u). For any concept
x ∈V , deﬁne Δ(u, x) = maxx′∈concepts(u) Sim(x′, x). Denote the value of At
i in
position (y, x) by At
(y,x) then:
At
(y,x) = ρ × At−1
(y,x) + (1 −ρ) × I(u) × Δ(u, x)

Information-Based Argumentation
141
for any x, where ρ is the discount rate, and I(u) is the information8 in u. The balance
of α’s relationship with βi, Bt, is the element by element numeric difference of At and
α’s estimate of β’s intimacy on α.
We are particularly interested in the concept of intimacy in so far as it estimates what
α knows about β’s criteria, and about the certainty of α’s estimates of the random vari-
ables {Ci}. We are interested in balance as a measure of the ‘fairness’ of the dialogue.
If α shows β how to take a perfect photograph of a duck then it is reasonable to expect
some information at least in return.
Moreover, α acts proactively to satisfy her needs — that are organised in a hierarchy9
of needs, Ξ, and a function ω : Ξ →P(W) where W is the set of perceivable states,
and ω(χ) is the set of states that satisfy need χ ∈Ξ. Needs turn ‘on’ spontaneously, and
in response to triggers. They turn ‘off’ because α believes they are satisﬁed. When a
need ﬁres, a plan is chosen to satisfy that need (we do not describe plans here). If α is to
contemplate the future she will need some idea of her future needs — this is represented
in her needs model: υ : T →×|Ξ|[0, 1] where T is time, and: υ(t) = (χt
1, . . . , χt
|Ξ|)
where χt
i = P(need χi ﬁres at time t).
Given the needs model, υ, α’s relationship model (Relate(·)) determines the target
intimacy, A∗t
i , and target balance, B∗t
i , for each agent i in the known set of agents
Agents. That is, {(A∗t
i , Bt
∗i)}|Agents|
i=1
= Relate(υ, X, Y , Z) where, Xi is the trust
model, Y i is the honour model and Zi is the reliability model as described in [2].
As noted before, the values for intimacy and balance are not simple numbers but are
structured sets of values over Y × V .
When a need ﬁres α ﬁrst selects an agent βi to negotiate with — the social model of
trust, honour and reliability provide input to this decision, i.e. βi =Select(χ, X, Y , Z).
We assume that in her social model, α has medium-term intentions for the state of
the relationship that she desires with each of the available agents — these intentions
are represented as the target intimacy, A∗t
i , and target balance, B∗t
i , for each agent βi.
These medium-term intentions are then distilled into short-term targets for the intimacy,
A∗∗t
i
, and balance, B∗∗t
i
, to be achieved in the current dialogue Ψ t, i.e. (A∗∗t
i
, B∗∗t
i
) =
Set(χ, A∗t
i , B∗t
i ). In particular, if the balance target, B∗∗t
i
, is grossly exceeded by β
failing to co-operate then it becomes a trigger for α to terminate the negotiation.
6.2
Computing Arguments
For an information-based agent, an incoming utterance is only of interest if it reduces
the uncertainty (entropy) of the world model in some way. In information-based argu-
mentation we are particularly interested in the effect that an argumentative utterance
has in the world model including β’s disposition, and α’s estimate of β’s assessment of
current proposals in terms of its criteria.
Information-based argumentation attempts to counter the effect of the partner’s ar-
guments, in the simple negotiation protocol used here, an argumentative utterance, u,
8 Information is measured in the Shannon sense, if at time t, α receives an utterance u that may
alter this world model then the (Shannon) information in u with respect to the distributions in
Mt is: I(u) = H(Mt) −H(Mt+1).
9 In the sense of the well-known Maslow hierarchy [14], where the satisfaction of needs that are
lower in the hierarchy take precedence over the satisfaction of needs that are higher.

142
C. Sierra and J. Debenham
will either contain a justiﬁcation of the proposal it accompanies, a rating and justiﬁca-
tion of one of α demonstration sequences, or a counter-justiﬁcation of one of α’s prior
proposals or arguments. If u requests α to perform a task then u may modify β’s dis-
position i.e. the set of conditional estimates of the form: Pt(C = e|O = o)). If β rates
and comments on the demonstration of a sequence then this affects α’s estimate of β’s
likelihood to accept a contract as described in Equation 1 (this is concerned with how β
will apply his criteria).
Suppose that u rates and comments on the performance of a sequence then that se-
quence will have been demonstrated in response to a request to perform a task. Given a
task, τ, and a object, s, α may have estimates for P t(C = e|(O = o, T = τ)) — if so
then this suggests a link between the task and a set of one or more criteria Cu. The effect
that u has on β’s criteria (what ever they are) will be conveyed as the rating. In the spirit
of the scenario, we assume that for every criterion and object pair (C, o) α has a supply
of positive argumentativestatements L(C,o). Suppose α wishes to counter the negatively
rated u with a positively rated u′. Let Ψu be the set of all arguments exchanged between
α and β prior to u in the dialogue. Let Mu ⊆L(C,o) for any C ∈Cμ. Let Nu ⊆Mu
such that ∀x ∈Nu and ∀u′ ∈Ψu, Sim∗(concepts(x), concepts(u′)) > η for some
constant η. So Nu is a set of arguments all of which (a) have a positive effect on at least
one criterion associated with the negative u, and (b) are at ‘some distance’ (determined
by r) from arguments already exchanged. Then:
u′ =

arg minu′∈Nu Sim∗(concepts(u), concepts(u′))
if Nu ̸= ∅
arg minu′∈Mu Sim∗(concepts(u), concepts(u′))
otherwise.
So using only ‘fresh’ arguments, α prefers to choose a counter argument to u that is
semantically close to u, and if that is not possible she chooses an argument that has
some general positive effect on the criteria and may not have been used previously.
Suppose that u proposes a contract. α will either decide to accept it or to make a
counter offer. We do not describe the bargaining process here, see [2].
6.3
All Together
If βi communicates u then α responds with:
u′ = Argue(u, Mt, Ψ t, A∗∗t, B∗∗t, Cu, Nu, Mu, Du))
where:
– the negotiation mechanisms as explained in Section 6.1 sets parameters A∗∗t, B∗∗t)
(see e.g. [3] for further details);
– the argumentation process determines the parameters Nu, Mu needed to generate
the accompanying arguments to the proposal, see Section 6.2;
– the criteria modeling process determines the set of criteria Cu used by our opponent
to assess the proposals, see Section 5.2; and,
– the disposition modeling sets the distributions Du used to interpret the stance of
the opponent, see Section 5.3.

Information-Based Argumentation
143
The personality of the agent will be determined by the particular f chosen to select the
answer to send. The study of concrete functions is subject of ongoing research as well
as their application into a eProcurement setting.
7
Discussion
We have described an approach to argumentation that aims to:
– discover what the partner’s key evaluative criteria are,
– model how the partner is evaluating his key criteria given some evidence,
– inﬂuence the partner’s evaluation of his key criteria,
– inﬂuence the relative importance that the partner attaches to those criteria, and
– introduce new key criteria when it is strategic to do so.
The ideas described here are an attempt to develop an approach to argumentation that
may be used in the interests of both parties. It aims to achieve this by unearthing the
‘top layer’ of the partner’s reasoning apparatus and by attempting to work with it rather
than against it. To this end, the utterances produced aim to inﬂuence the partner to
believe what we believe to be in his best interests — although it may not be in fact.
The utterances aim to convey what is so, and not to point out “where the partner is
wrong”. In the long term, this behaviour is intended to lead to the development of
lasting relationships between agents that are underpinned both by the knowledge that
their partners “treat them well” and that their partners act as they do “for the right
reasons”.
The ideas in this paper have been developed within a highly restrictive scenario that
is deliberately asymmetric (being based on a buy / seller relationship). The structure
of the analysis is far more general and applies to any scenario in which something
has to be bought/made/designed that satisﬁes a need, and that can do various things.
The agents who try to make it do things (use-cases if you like) subjectively rate what
they see.
In previous work [3] we have advocated the gradual development of trust and inti-
macy10 through successive argumentative exchanges as a way of building relationships
between agents. The act of passing private information carries with it a sense of trust of
the sender in the receiver, and having done so the sender will wish to observe that the
receiver respects the information received. In this paper we have gone one step further
by including a modest degree of understanding in the sense that an agent attempts to
understand what her partner likes. This falls well short of a deep model of the part-
ner’s reasoning but we believe strikes a reasonable balance between being meaningful
and being achievable. This augments the tools for building social relationships through
argumentation by establishing:
– trust — my belief in the veracity of your commitments
– intimacy — my belief in the extent to which I know your private information
– understanding — my belief in the extent to which I know what you like
10 The revelation of private information.

144
C. Sierra and J. Debenham
Acknowledgements. This research has been supported by the Sabbatical Programme
of the Catalan Government BE2007, the Australian Research Council Discovery Grant
DP0557168, and by the Spanish Ministerio de Educaci´on y Ciencia project ”Agreement
Technologies” (CONSOLIDER CSD2007-0022, INGENIO 2010).
References
1. Sierra, C., Debenham, J.: Information-based agency. In: Proceedings of Twentieth Interna-
tional Joint Conference on Artiﬁcial Intelligence, IJCAI 2007, Hyderabad, India, pp. 1513–
1518 (2007)
2. Sierra, C., Debenham, J.: Trust and honour in information-based agency. In: Stone, P., Weiss,
G. (eds.) Proceedings Fifth International Conference on Autonomous Agents and Multi
Agent Systems AAMAS 2006, Hakodate, Japan, pp. 1225–1232. ACM Press, New York
(2006)
3. Sierra, C., Debenham, J.: The LOGIC Negotiation Model. In: Proceedings Sixth International
Conference on Autonomous Agents and Multi Agent Systems AAMAS 2007, Honolulu,
Hawai’i, pp. 1026–1033 (2007)
4. Dung, P.M.: On the Acceptability of Arguments and its Fundamental Role in Nonmonotonic
Reasoning, Logic Programming and n-Person Games. Artiﬁcial Intelligence 77, 321–358
(1995)
5. Rahwan, I., Ramchurn, S., Jennings, N., McBurney, P., Parsons, S., Sonenberg, E.:
Argumentation-based negotiation. Knowledge Engineering Review 18, 343–375 (2003)
6. Cheeseman, P., Stutz, J.: On The Relationship between Bayesian and Maximum Entropy In-
ference. In: Bayesian Inference and Maximum Entropy Methods in Science and Engineering,
pp. 445–461. American Institute of Physics, Melville (2004)
7. Paris, J.: Common sense and maximum entropy. Synthese 117, 75–93 (1999)
8. Jaynes, E.: Information theory and statistical mechanics: Part I. Physical Review 106, 620–
630 (1957)
9. Li, Y., Bandar, Z.A., McLean, D.: An approach for measuring semantic similarity between
words using multiple information sources. IEEE Transactions on Knowledge and Data Engi-
neering 15, 871–882 (2003)
10. Lewicki, R.J., Saunders, D.M., Minton, J.W.: Essentials of Negotiation. McGraw Hill, New
York (2001)
11. Bentahar, J., Mbarki, M., Meyer, J.J.C., Moulin, B.: Strategic agent communication: An
argumentation-driven approach. In: Baldoni, M., Son, T.C., van Riemsdijk, M.B., Winikoff,
M. (eds.) DALT 2008. LNCS (LNAI), vol. 5397, pp. 233–250. Springer, Heidelberg (2009)
12. Kakas, A., Maudet, N., Moraitis, P.: Layered strategies and protocols layered strategies and
protocols for argumentation-based agent interaction. In: Rahwan, I., Mora¨ıtis, P., Reed, C.
(eds.) ArgMAS 2004. LNCS (LNAI), vol. 3366, pp. 64–77. Springer, Heidelberg (2005)
13. Mbarki, M., Bentahar, J., Moulin, B.: Speciﬁcation and complexity of strategic-based rea-
soning using argumentation. In: Maudet, N., Parsons, S., Rahwan, I. (eds.) ArgMAS 2006.
LNCS (LNAI), vol. 4766, pp. 142–160. Springer, Heidelberg (2007)
14. Maslow, A.H.: A theory of human motivation. Psychological Review 50, 370–396 (1943)

Mediation = Information Revelation + Analogical
Reasoning
Simeon Simoff1,3, Carles Sierra2,3, and Ramon L´opez de M`antaras2
1 School of Computing and Mathematics,
University of Western Sydney, NSW 1797, Australia
s.simoff@uws.edu.au
2 Institut d’Investigaci´o en Intel·lig`encia Artiﬁcial – IIIA,
Spanish Scientiﬁc Research Council, CSIC
08193 Bellaterra, Catalonia, Spain
{sierra, mantaras}@iiia.csic.es
3 University of Technology, Sydney, Australia
Abstract. This paper presents an initial study of the relevant issues on the de-
velopment of an automated mediation agent. The work is conducted within the
‘curious negotiator’ framework [1]. The paper demonstrates that mediation is a
knowledge intensive process that integrates information revelation and analogi-
cal reasoning. The introduced formalism is used to demonstrate how via revealing
the appropriate information and reshaping the set of issues of the disputing par-
ties mediation can succeed. The paper presents MediaThor - a mediating agent
that utilises past experiences and information from negotiating parties to mediate
disputes and change the positions of negotiating parties.
1
Introduction
Negotiation is the process whereby two (or more) individual agents with conﬂicting
interests interact, aiming at reaching a mutually beneﬁcial agreement on a set of issues.
Engaging in such interactions is a daily activity — from a simple negotiation on the
price of a product we buy at the market to the complicated negotiations in dispute
resolutions on the international arena. During such interactions, participants may need
to make concessions in order to reach an agreement [2]. Individual negotiators usually
have agendas of their own, which may be incompatible — there may be no solution
that satisﬁes them all. Further the existence of a solution is unlikely to be known when
the negotiation commences [3]. So it may not be useful to consider negotiation as a
search problem because the solution space may be empty whilst the negotiating agents
may believe that it is not so. If the negotiation is a multi-issue negotiation for which the
issue set can change at any stage in the negotiation then the agendas of the individual
negotiating agents must necessarily be at a higher level than the issues because the
issues are unknown, and may even be issues that ‘had never occurred’ to one of the
agents. Therefore in such cases the agendas of the agents cannot be a high level goal
such as ‘to maximise proﬁt on the deal’ as the deal space is unknown. Environmental
conﬂict resolution is a typical example where conﬂicts involve many different types
of parties, issues and resources and the issue set can change during the process [4].
J.-J.Ch. Meyer and J.M. Broersen (Eds.): KRAMAS 2008, LNAI 5605, pp. 145–160, 2009.
c⃝Springer-Verlag Berlin Heidelberg 2009

146
S. Simoff, C. Sierra, and R. L´opez de M`antaras
Negotiation ‘table’
Mediation
agent 
Observer agent 
Information
miner IM1
Information Discovery
and Delivery System
Result
Profile
Index
……
…
…
…
Negotiation case base
Retrieves similar 
successful negotiation
Negotiator 
Profiles
Observes and analyses prior negotiations
why they worked
Analyses negative outcomes, 
diagnoses ‘sick’ negotiations
Diagnoses negotiation sequences
?
1
2
N
Negotiation 
agent 
Information
miner IMN
#
News media
Virtual communities
Companies sites
Government Sites
Negotiation 
agent 
Mediator 
Styles
Retrieves information 
about mediation style 
and negotiators profiles
Adviser
#
A
B
Fig. 1. The design of the curious negotiator and the progress of the research
As a result negotiations may reach a deadlock, taking prohibitively long time without
reaching tangible outcomes, or be terminated. This is when in real life the intervention
of a mediator can inﬂuence the process, facilitating it towards a mutual agreement.
The design of the ‘curious negotiator’ automated negotiation system, founded on the
intuition “it’s what you know that matters” [1], is an attempt to address these issues. Fig-
ure 1 shows an updated version of the overall design proposed in [1] and the progress
of the work. Negotiation agent α negotiates with agent β by sending illocutions which
represent offers and counter offers. The illocutions are represented in a communication
language C that enable persuasive negotiation [5] and argumentation [6]. The research
outcome under the shaded area A in Figure 1- the information-based agency [6], treats
negotiation as an information discovery and revelation process. Such information may
come from at least two sources: (i) from the ‘negotiation table’, e.g. from all utter-
ances agents make during a negotiation [7]; and (ii) from various external electronic
sources on the Internet. The research outcome under the shaded area B in Figure 1-
the information discovery and delivery system that services negotiating agents, extracts
information from various web sources [8] and provides recommendations to the negoti-
ation agents in requested form (for example, reﬁned exchange rate predictions [9] based
on information from latest ﬁnancial news; recommendations on product choice based
on information extracted from consumer product reviews [10]). The mechanisms for
dealing with negotiations that fail in reaching an agreement, or seemed to be leading to
a failure, remain the undeveloped part of the ‘curious negotiator’ - the unshaded part in
Figure 1. It includes the mediating agent μ, the observer agent ω and their supporting
knowledge representation structures.

Mediation = Information Revelation + Analogical Reasoning
147
This paper presents the initial work on the principles of building an automated medi-
ation agent within the ‘curious negotiator’ framework. We explore mediation as a pro-
cess of intertwined information revelation and analogical reasoning, which incorporates
past experiences. To model mediation the work draws from the research in dispute res-
olution. It speciﬁes the requirements towards the knowledge representation structures
supporting mediation. Section 2 looks at mediation, as a knowledge-driven process and
explores the changes that information revelation can make to the negotiation space and
the outcomes of negotiation. It introduces the notion of ‘mental model’ of participants
involved in the process and looks at mechanisms of how these models can be utilised
in automated mediation. Section 3 considers some aspects in utilising past experiences
and background knowledge in automated mediation. It looks also at the utilisation of
information at the diagnosis stage. Section 4 presents the architecture of MediaThor - a
mediation agent that illustrates the computational implementation of mediation mecha-
nisms discussed in the previous sections.
2
Mediation as a Knowledge Driven Process of Information
Revelation
Contemporary analysts in social and political sciences look at mediation as a process
that enables conﬂict resolution. Mediators are often indispensable in the area of dis-
pute (or conﬂict) resolutions, settling variety of disputes, spanning from conﬂicts be-
tween sovereign nations to conﬂicts between family members, friends, and colleagues.
Successful mediation can make a dramatic difference to the outcome of a negotiation
stalemate. For instance, on 14 January 1998 the President of United Nations Security
Council issued a statement demanding “that Iraq cooperate fully and immediately and
without conditions with the Special Commission in accordance with the relevant res-
olutions.”1 As all UN weapons inspections in Iraq were frozen, during the following
month all direct negotiations between the US and Iraq did not reach any agreement and
the military conﬂict seemed unavoidable. The following event sequence illustrates the
mediation process: (i) the US authorised the mediation effort; (ii) the UN secretary (the
mediator) achieved a possible deal with Iraq; (iii) the UN secretary passed it back to the
US; (iv) the US reviewed and accepted the deal. Several months later the conﬂict esca-
lated, but this time no mediation was sought and military actions started. The mediation
made a huge difference in the ﬁrst dispute resolution.
2.1
Necessary and Sufﬁcient Conditions for Mediation
This example illustrates that mediation as a process involves information revelation
and part of the mediator’s strategy is guiding the process of information revelation. The
following are the necessary (C1, C2) and sufﬁcient (C3) conditions for a mediation to
take place:
– Condition C1: Negotiating agents α and β are willing to achieve a mutually bene-
ﬁcial agreement;
1 http://daccessdds.un.org/doc/UNDOC/GEN/N98/007/84/PDF/N9800784.pdf

148
S. Simoff, C. Sierra, and R. L´opez de M`antaras
– Condition C2: Negotiating agents α and β are seeking or will accept mediation (in
the ﬁrst case, the awareness about the conﬂict and the problem with the current state
of the negotiation resides with the negotiating agents, in the second case either the
mediator agent μ or, if present, the observer agent ω diagnoses the problem);
– Condition C3: A mediating agent μ is available (this condition is by default embed-
ded in the ‘curious negotiator’ paradigm).
In the example with the 1998 Iraq crisis, in the second case condition C2 was not
present. Conﬂicts may be a result of a contradiction of interests, as in the example
with the 1998 Iraqi crisis, but can be also a result just of a different (but unknown to the
disputing parties) perception of the disputed subject.
2.2
Mediation Process within the ‘Curious Negotiator’ Framework
Further we consider the following mediation process, shown in Figure 2, where agents
α and β are in a deadlock and direct exchange of offers between them has ceased. In a
mediation session, α and β interact with messages m only with the mediating agent μ.
Mt denotes a “mental model” at time t. We use the label “mental model” to denote
the view (including related knowledge) of an agent about a dispute, about the views of
the other parties on that dispute and the expected outcomes. This knowledge is internal
to the agent. Each model is manifested to the other agents through the actions taken by
the agent. The label “mental models” has been chosen to emphasise the key role of the
mediator in the careful examination of the way negotiation parties have built their views
on the disputed issues [11]. It is also in accordance with the view that negotiation can
be conceptualised as a problem-solving enterprise in which mental models guide the
behaviour of negotiating parties [12]. Further in the text we use the term mental model
without quotation marks.
Mt
α and Mt
β denote the mental models of agents α and β, respectively. Mt
α is not
known by β and Mt
β is not known by α. None of them is known by the mediating agent
μ. Each of these agents has its own approximations of the mental models of the other
agents. Mt
agent(party) denotes the mental model that the agent has about another party.
In particular, Mt
α(β) is the mental model of α about β, i.e. about what β wants out of
the negotiation; respectively, Mt
β(α) is the mental model of β about α, i.e. the position
of α in the dispute. Further, Mt
μ(α) and Mt
μ(β) are the mental models of the mediating
agent μ about the positions of α and β in the dispute, respectively.
We use the above formalism to demonstrate some aspects of mediation that need
to be taken into account when developing automated mediators. Further we use two
examples - The Orange Dispute [13] and the Sinai Peninsula Dispute to illustrate the
role of information revelation and identiﬁcation of analogy between disputes in order
to reshape the set of issues and complete the mediation process.
2.3
The Orange Dispute - Reshaping the Problem Based on Additional
Information
In the Orange Dispute [13], two sisters want the same orange. According to Kolodner
[13] “MEDIATOR assumes they both want to eat it and solves the problem by having

Mediation = Information Revelation + Analogical Reasoning
149
Negotiation 
agent 
Negotiation 
agent 
Mediation
agent 
( )
{
}
,
t
t
α
α β
M
M
( )
{
}
,
t
β
β α
t
M
M
( )
( )
{
}
,
t
t
μ α
μ β
M
M
mα
μ
→
mμ
α
→
mμ
β
→
mβ
μ
→
Fig. 2. The mediation agent within the ‘curious negotiator’ framework
one sister cut the orange in two and the second chooses her half. When the second
sister uses her peel for baking and throws away the pulp, MEDIATOR realises it made
a mistake.”2
Further we present the two mediation attempts in terms of the agreements reached
and the information that can be passed to the mediator. Lets our agent α represent the
ﬁrst sister who wants to have the orange as a desert and agent β represent the second
sister who wants to have (only the peel of) the orange for cooking (the recipe requires
the whole peel). If our mediation agent μ happens to be the case-based MEDIATOR,
then the situation described in the Orange Dispute can be expressed through the mental
models of the individual participants in Figure 3, making explicit the wrong assumption
(the boxed expressions in Figure 3).
In these models tbreak, and tstart indicate the time when negotiation broke and when
mediation started, respectively (in the case of the MEDIATOR it has been a one step
act). The results of the agreements in terms of the outcomes - Outcome (agent, issue,
result) are presented in Table 1, where result values are denoted as follows: “+”, “+/-”
and “-” for positive, acceptable, and negative, respectively for the corresponding agents
in terms of the negotiated issue. In the original example [13], the result in the outcome
for β should be “+/-” as the second sister still used the peel from her half. Here we
added the constraint of the recipe in order to get negotiation about the orange to a halt
with an unacceptable “-” result and generate a request for mediation.
The Orange Dispute can be considered an example of a dispute over resource scarcity.
The resource in this case has a possible component-based separation (without change of
the total amount of available resource) that allows to change the structure of the dispute
through mediation, opening the space for a mutually beneﬁcial solution. It exposes two
aspects of mediation:
2 MEDIATOR [14] is one of the early case-based mediators. The focus of the work was on
the use of case-based reasoning for problem understanding, solution generation, and failure
recovery. The failure recovery ability is demonstrated with the Orange Dispute in [13].

150
S. Simoff, C. Sierra, and R. L´opez de M`antaras
α wants the orange as a dessert ∈Mt
α
(1)
β wants the peel of the orange for cooking ∈Mt
β
(2)
β wants an orange ∈Mtbreak
α(β)
(3)
α wants an orange ∈Mtbreak
β(α)
(4)
α wants the orange as a dessert ∈Mtstart
μ(α)
(5)
β wants the orange as a dessert ∈Mtstart
μ(β)
(6)
Fig. 3. The wrong initial assumption of the MEDIATOR [14] in terms of our mental models
(Boxed expressions). This initial assumption (which didn’t change as there were no mechanisms
for that) caused the failure of that mediator.
– The difference that a mediator can bring is in exploring the structure of the problem
from a broader stance;
– An initial assumption by a mediator can lead to a failure of the mediation effort.
Consequently, we formulate the following postulates for the automated mediator:
– Postulate P1: An automated mediator μ should start interaction with extracting
more information about the position of the parties on the negotiation;
– Postulate P2: An automated mediator should develop an independent “grand view”
of the problem, which is more comprehensive than the individual views of α and
β, respectively.;
– Postulate P3: An automated mediator μ should operate from the initial stance only
of conditions C1 and C2.
Starting mediation without initial assumptions means that μ either does not have a
model for each of the negotiating agents α and β, or accepts the models Mtbreak
α(β)
and
Mtbreak
β(α) these agents have about each other at the point of requesting mediation. In the
case of the Orange Dispute, μ starts mediation with the exit models of α and β:
– Mtstart
μ(α) = Mtbreak
β(α) , i.e. α wants an orange ∈Mtstart
μ(α) , and
– Mtstart
μ(β) = Mtbreak
α(β) , i.e. β wants an orange ∈Mtstart
μ(β) .
This information is not sufﬁcient for mediation, e.g. the uncertainty in the mutual mod-
els of α and β, and the model μ are the same. Research in conﬂict resolution in in-
ternational relations demonstrates that if a mediator could credibly add information to
the system of negotiators this alters the state of the system [15]. Consequently, μ takes
Table 1. Outcomes of the Orange Dispute, based on mediation with initial assumption
Agent Agreement clauses
Outcome for α
Outcome for β
α
Cuts the orange into halves Outcome(α, has orange, +/-) Outcome(β, has orange, -)
β
chooses one half
Outcome(α, has orange, +/-) Outcome(β, has orange, -)

Mediation = Information Revelation + Analogical Reasoning
151
α wants the orange as a dessert ∈Mt
α
(7)
β wants the peel of the orange for cooking ∈Mt
β
(8)
β wants an orange ∈Mtbreak
α(β)
(9)
α wants an orange ∈Mtbreak
β(α)
(10)
α wants an orange ∈Mtstart
μ(α)
(11)
β wants an orange ∈Mtstart
μ(β)
(12)
α wants the orange as a dessert ∈Mtend
μ(α)
(13)
β wants the peel of the orange for cooking ∈Mtend
μ(β)
(14)
Fig. 4. The respective mental models of α, β and μ in the mediation session of the Orange Dispute
with our proposed agent
steps in order to decrease this uncertainty. In addition, intuitively, it seems worth check-
ing whether both parties have the same understanding of the issues in the dispute, i.e.
technically, whether they operate with the same ontology or with compatible ontologies.
In the Orange Dispute, μ obtains from each party what the orange is needed for. The
Orange Dispute in terms of the mental models of the individual participants in the case
of proposed mediation agent is presented in Figure 4. In these models tbreak, tstart and
tend indicate the time when negotiation broke and when mediation started and ended,
respectively. Note the difference of Mtstart
μ(·)
for both α and β in Figure 3 and Figure
4. The steps taken by the mediating agent are described in Figure 5 (we do not use a
formal illocution based language, but the actions that the language should cater for are
shown in italic).
The Orange Dispute illustrates also another important ability that an automated me-
diator should posses — the ability to reshape or restructure the dispute, based on the
additional information about the models of each party. The outcomes of the restructured
Orange Dispute are shown in Table 2.
2.4
The Sinai Peninsula Dispute and Its Analogy with the Orange Dispute
The ability to reshape the problem is crucial for developing successful automated me-
diators. The Sinai Peninsula Dispute in the area of international relations shows similar
properties to the Orange Dispute. The Sinai Peninsula is a piece of land of about 62,000
square km that separates Israel and Egypt. With its landscape Sinai has a military value
for either side in terms of mechanised infantry transport or as a shelter for guerrilla
Table 2. Outcomes of the restructured Orange Dispute
Agent Agreement clauses Outcome for α
Outcome for β
α
Peels the orange
Outcome(α, eat, +) Outcome(β, cook, +)
β
Gets the whole peel Outcome(α, eat, +) Outcome(β, cook, +)

152
S. Simoff, C. Sierra, and R. L´opez de M`antaras
1. μ : ask α to send its ontology of the negotiated item (orange).
2. μ : ask β to send its ontology of the negotiated item (orange).
3. μ : compare ontologies received fromα and β.
4. μ : send α and β agreed ontology (orange as a fruit which has pulp and peel).
5. μ : ask α to send μ its preferences on the negotiated item in terms of agreed ontology.
6. μ : ask β to send μ its preferences on the negotiated item in terms of agreed ontology.
7. μ : advises α and β on Mt
α and Mt
β based on their preferences
8. μ : checks the case base for past cases (resource disputes)
9. μ : retrieves resource disputes with divisible components
10. μ : sends α and β action separate resource (peel the orange)
11. μ : tells α and β to complete negotiation.
12. μ : mediation completed.
Fig. 5. Mediation as information revelation aiming at decreasing uncertainty within the negotia-
tion system
forces. The perceived importance of the territory is evidenced by the fact that Israelis
and Egyptians fought in or over the Sinai Peninsula in 1948, 1956, 1967, 1968-1970,
and 1973. Since 1967 Sinai had been occupied by Israel. Figure 6 shows a very simpli-
ﬁed version of the models of the parties at the initial meeting in Jerusalem, when the
negotiations started and halted and the change of the mediators models that lead to the
outcomes. For the purpose of this paper we aim to emphasise the high level analogy
with the Orange Dispute case (see Figure 4), i.e. the need for a mediator to reframe
the problem. In fact, the need for restructuring the problem in order for a mediator
to get a “bigger picture” has been recognised in PERSUADER [2], to resolve labor-
management disputes. In recent works [16] the mediator is expected to have a complete
knowledge of the solution space.
Following the initial interaction in Jerusalem, the US President Jimmy Carter initi-
ated a third-party mediation effort that culminated in the Camp David accords. For the
purposes of this paper we consider a simpliﬁed version of the second agreement of the
Camp David accords on the future of the Sinai Peninsula. The items in the agreement
α wants security, support for economy, recognition ∈Mt
α
β wants sovereignity (restored territory), support for economy, security ∈Mt
β
β wants territory and strategic military advantage ∈Mtbreak
α(β)
α wants territory and strategic military advantage ∈Mtbreak
β(α)
α wants territory and strategic military advantage ∈Mtstart
μ(α)
β wants territory and strategic military advantage ∈Mtstart
μ(β)
α wants security, support for economy, recognition ∈Mtend
μ(α)
β wants sovereignity (restored territory), support for economy, security ∈Mtend
μ(β)
Fig. 6. The respective mental models of α, β and μ in the mediation session of the Sinai Dispute
with our proposed agent

Mediation = Information Revelation + Analogical Reasoning
153
Table 3. The Sinai Peninsula Dispute. α denotes Israel; β denotes Egypt
Agent Agreement clauses
Outcome for α
Outcome for β
α
withdraw its armed
Outcome(α, Military, -)
Outcome(β, Territory, +)
forces from the Sinai
Outcome(β, Sovereignty, +)
α
Evacuate its 4500 civilians
Outcome(α, Territory, -)
Outcome(β, Territory, +)
Outcome(β, Sovereignty, +)
α
Restore Sinai to Egypt
Outcome(α, Territory, -)
Outcome(β, Territory, +)
Outcome(β, Sovereignty, +)
α
Limit its forces within 3km Outcome(α, Military, -)
Outcome(β, Security, +)
from Egyptian Border
Outcome(α, Security, +)
α
Lost the Abu-Rudeis oil
Outcome(α, Economy, -)
Outcome(β, Economy, +)
ﬁelds in Western Sinai
β
Normal diplomatic
Outcome(α, Recognition, +) Outcome(β, Security, +)
relations with Israel
β
Freedom of passage through Outcome(α, Economy, +)
Outcome(β, Security, +)
Suez Canal
Outcome(α, Security, +)
β
Freedom of passage through Outcome(α, Economy, +)
Outcome(β, Economy, +)
nearby waters
Outcome(α, Security, +)
Outcome(β, Security, +)
β
Restricted Egyptian
Outcome(α, Security, +)
Outcome(β, Military, -)
forces in Sinai
Outcome(β, Security, +)
are presented in Table 3, in a structure, similar to the presentation of the agreements
in the Orange Dispute in Table 1 and Table 2. Without getting into the details of the
mediation steps, from Table 3 it is evidenced that the initial mutually perceived models
and about the need for territory and strategic military advantage have been transformed
by the mediation into a Security/Sovereignty trade-off, with economic beneﬁts.
The analogy with the Orange Dispute is in having the initial negotiation framed
around a common resource Territory and a similar issue of having strategic military ad-
vantage as the main goals that can enable the security. Though both territorial and mili-
tary components remain on the negotiation table, based on some background knowledge
and higher level view of the ontology of the problem, the mediator developed a view
of the set of issues and aligned the ontologies of both parties which eventually changed
their models: security and restoration may not necessarily be achieved with occupation
of a territory or with expensive military presence.
The information injected by the mediator and proposed steps leads to decreasing
the differences between perceived mental models Mt
α(β) and Mt
β(α), and the corre-
sponding actual mental models Mt
β and Mt
α of agents α and β, respectively, i.e. the
intervention of the mediator decreases the uncertainty in the negotiation system.
3
Utilising Past Experiences and Background Knowledge in
Automated Mediation
The American Bar Association deﬁnes mediation as a process by which those who
have a dispute, misunderstanding or conﬂict come together and, with the assistance of
a trained neutral mediator, resolve the issues and problems in a way that meets the

154
S. Simoff, C. Sierra, and R. L´opez de M`antaras
needs and interests of both parties.3 This deﬁnition emphasises the unbiased nature of
the mediator and the key role of its past experience. The bias of a mediator is deﬁned
as the presence of a preference towards one of the outcomes in the negotiation; or, sides
involved in the negotiation. Not having preference towards any of the outcomes of a
negotiation means also to keep open all options. For instance, the peace-loving broker’s
bias towards peaceful solutions makes his or her claims less believable compared to a
broker who is indifferent to war or peace [15]. Such bias as a result can decrease the
effectiveness of the mediation effort. Protecting automated mediation from introduction
of a bias is not seen as a problem.
Experience is, perhaps, the distinct feature between successful and less successful
mediators. Analogical reasoning (CBR + ontology) is an approach to problem solving
that emphasizes the role of prior experience during future problem solving (i.e., new
problems are solved by reusing and if necessary adapting the solutions to similar prob-
lems that were solved in the past) (see [17] for a recent review of the state-of-the-art in
the CBR ﬁeld). From a machine learning point of view, updating the case base learning
without generalisation. Some aspects of using the past experience by the tandem Me-
diation and Observation agents have been discussed in [1]. In terms of required case
representation, a starting point is the knowledge representation structure for represent-
ing negotiation cases, proposed in [18]. This structure needs to be updated for dealing
with ontologies. For the mediation, the case based will be linked to the corresponding
knowledge base of the mediation strategies used. The case structure now includes a ne-
gotiation case as its problem section and the collection of mediation steps, information
used and other knowledge, as the solution part of the case.
Important from a computational perspective is the diagnosis stage of the mediation
process [19]. The diagnostic function consists of monitoring the progress of negotiation
or related interactions intended to settle or resolve disputed issues (Druckman and co-
authors [19] refer to [20]). Monitoring provides a long view of unfolding developments,
including trends in escalating and de-escalating dynamics. Within the framework of
‘curious negotiator’ we consider this stage as a pre-mediation stage, which is executed
by the observer agent ω. To some extent it resembles similarity with OLAP4 — the
pre-data mining steps in business intelligence, where summary statistics at different
levels are generated and later provide guidance to the data mining strategies. Similar
to OLAP, monitoring should be able to provide snapshots of the negotiation process at
any moment of time at different levels of granularity. The mediator μ should be able
to estimate the difference between Mt
α(β) and Mt
β(α)from the respective actual mental
models Mt
β and Mt
α in order to deﬁne the intervention time of mediating interventions
(if we follow a proactive approach and intervene before negotiation fails).
4
MediaThor: A Powerful CBR Mediator Agent
The architecture of a mediator agent as described in the previous sections has to be
based on a clear understanding of the relationships between its models of the agents
3 http://www.abanet.org/cpr/clientpro/medpreface.html
4 Online analytical processing.

Mediation = Information Revelation + Analogical Reasoning
155
Orange
Drink
Cake
Peel
Pulp
required
required
part-of
part-of
Goals: Cake(X) and Drink(Y)
Agents: {X, Y}
Solution: Peel(X) and Pulp(Y)
Territory
control
Sovereignty
Security
Military
Civilian
brings
brings
part-of
part-of
Goals: Security(X) and Sovereignty(Y)
Agents: {X, Y}
Solution: Military(X) and Civilian(Y)
Avocado
Plant
Guacamole
Pulp
Stone
required
required
part-of
part-of
Goals: Guacamole(X) and Plant(Y)
Agents: {X, Y}
Solution: Pulp(X) and Stone(Y)
Peel
WaterUse
Produce
Drink
Morning
Evening
required
required
part-of
part-of
Goals: Drink(X) and Water(Y) and Produce(Z)
Agents: {X, Y, Z}
Solution: Morning(X) and Midday(Z) and
                  Evening(X)  and Night(Y)
Midday
part-of
Water
Night
part-of
required
required
part-of
A
B
C
D
Fig. 7. Four cases represented as: an ontology (a graph), the agents participating and the goals
and solution of the problem. Blue arrows represent a structural matching between the ontologies
of cases A and B, while green arrows represent a semantic matching between the same cases.
and the ontology that describes the problem and its solution. The retrieval of a previous
case, described in completely different ontological terms, has to rely on some alignment
process between the concepts and semantic relations of that past case and the concepts
and relations in the problem being solved. Figure 7 shows a representation of four cases.
Each case consists of an ontology representing the problem, the agents participating,
the goals each agent had, and the solution found. On Figure 7-A and 7-B one can see
a representation of the orange and Sinai peninsula disputes. We consider two types of
ontology matching in this architecture. A structural matching that aims at connecting
the nodes in the ontology from a structural point of view (arrows in blue in the picture
— or dark grey for the B&W printers) and from a semantic point of view (arrows in
green —or in light grey). The example would account for a perfect matching from a
structural point of view and for an almost perfect matching from a semantic point of
view (up to the semantic similarity between required and brings.)
Thus, assuming that the set of all possible ontologies5 is noted by O, we will there-
fore require that any mediator is provided with a Match : O ×O →[0, 1] function that
determines the level of similarity between any two ontologies (graphs). This function
will be a combination of at least structural and semantic similarities between the ontolo-
gies. A number of existing solutions can be found in [21]. We will also assume that the
ontology of a case is indeed a partial view (subgraph) of a more general ontology. Thus,
given an ontology o ∈O we will note by ¯o this general ontology. In our example, the
5 In this paper we will take the simpliﬁed view that ontologies are labelled directed graphs, with
concepts at the nodes and binary relations on the links.

156
S. Simoff, C. Sierra, and R. L´opez de M`antaras
orange case could be seen as a view of an ontology of fruits and their usage, while the
Sinai case could be seen as a view of an ontology on military affairs. We will therefore
associate to each case the ontology from where the case is drawn and moreover we will
deﬁne a subsumption relation between ontologies, ⊑⊆O × O, being o ⊑o′ true when
o is a sub-ontology of o′, i.e. a subgraph. Clearly oi ⊑¯oi
The concepts in the ontologies will all be understood as resources, and we abuse
notation by transforming them into predicates with the following intuitive meaning:
Security(X) means that agent X gets Security. The difference between goals and
solutions in the cases have to be understood from a planning perspective: a solution is
an assignment of resources to agents that logically imply their respective goals.
According to our view above, MediaThor will take a very speciﬁc view on what
Mt
μ(·) is. In particular we will assume that Mt
μ(α) = ⟨oα, gα, rα⟩where oα, gα, and rα
is the perceived ontology, goals, and reservations of α. We understand by reservations
those constraints that α requires the solution to satisfy. MediaThor is equipped with a
predicate Sat ⊆C ×2R that is true when a particular case satisﬁes a set of reservations.
We describe the architecture of MediaThor following the steps of a CBR algorithm.
Retrieval. MediaThor has a memory of past cases C
= {ci}i∈I where ci
=
⟨Ai, oi, Gi, Si⟩, Ai the set of agents involved, oi an ontology, Gi the goals of the agents
participating in the case, and Si the solution as a set or resource assignments. Me-
diaThor also is dealing with agents α and β trying to ﬁnd a solution to their problem.
The model that MediaThor has at any moment of time is a partial case where the on-
tology may be incomplete, and the goals may be wrong. Thus, the current problem is a
case c using ontology o.
MediaThor, at retrieval time, looks for cases that are similar to the current case c.
MediaThor uses a sliding parameter η that ﬁlters those cases ci that are expressed in
ontologies oi that are semantically similar to o up to the value η. By setting η = 1 we
are considering cases exactly over the same ontology, and by decreasing η we consider
increasingly farther away cases in terms of semantic similarity.
Figure 7 shows that there is a structural mapping between cases B and C, although in
this case C contains a richer structure. Ontology matchers provide a degree of matching
but also a function that maps the concepts and labels of one ontology into the concepts
and labels of the other. Thus, if the mapping function from B to C is called fB→C it
is clear that fB→C(oB) ⊑oC and that the degree of structural matching between A
and C has to be smaller than between A and B. Thus assuming the semantic matching
(S-Match) is the same between A and B and between C and B, we can conclude that if
B is the current case MediaThor would prefer case A to case C at retrieval time.
Thus, for a given η, Mt
μ(·), current case c = ⟨{α, β}, o, Gα ∪Gβ, ∅⟩and case base
C, the best case c∗∈C is deﬁned as:
c∗= arg
max
ci∈C
S−Match( ¯oi,¯o)≤η
Sat(ci,Rα∪Rβ)
Match(fci→c(oi), ˆoi)

Mediation = Information Revelation + Analogical Reasoning
157
where
ˆoi = arg max
o′
o⊑o′⊑¯o
Match(fci→c(oi), o′)
Reuse. Adaptation is solved by local exploration around the case found while satisfy-
ing the reservations of the agents in conﬂict. The exploration is made by looking for
expansions or contractions of the ontology of the case that might increase the matching
degree with the current case, and then using the so expanded ontology to generate a
solution.
oR = arg
max
o′
o∗⊑o′⊑¯
o∗
Match(fci→c(o′), o)
Revise. Here the solution generated in the step above is proposed to the participants. In
this way changes in the goal set of the agents or new reservations may appear. Also, the
ontology can be reﬁned as the adaptation of the ontology may have introduced elements
found irrelevant by the participants. For instance in the case of the water dispute (Figure
7-D) a farmer may say that watering the ﬁelds on midday is unfeasible because of sun
heat. This reservation might make a solution in which watering is done in the middle of
the day unfeasible. If no succeess is obtained, another iteration is made.
R′
α = f(Dialogue, Rα)
Retain. MediaThor follows a simple method to memorise new cases. A case will be
memorised if there is no other case in the memory of cases that has a similarity degree
(using the matching between the ontologies) with it over a threshold θ. That is, if the
memory of cases at time t is Ct and the solved case at time t is c then
Ct+1 =

Ct ∪{c}
if Sim(c, c∗) < θ
Ct
otherwise
(15)
We will update the ontologies as a fusion/combination of the ontologies of the cases. In
that way we can avoid asking for an ontology which is always annoying. The details of
the fusion/combination of ontologies are beyond the scope of this paper.
5
Conclusions
Though during the years there has been some interest in automated mediation
[14,2,22,16], the ﬁeld requires a signiﬁcant effort in research and development. This
paper has presented the initial work on the principles of building an automated me-
diation agent within the ‘curious negotiator’ framework, hence can utilise some of the
machinery developed for it, in particular: (i) the information-based agency [7,6]; (ii) the
information-mining system [8,9]; and (iii) the electronic/virtual institutions environment
[23,24], which offers means for performing negotiation and collecting the necessary
data about the negotiation sessions in order to use it in mediation.

158
S. Simoff, C. Sierra, and R. L´opez de M`antaras
We established that mediation is an information revelation process where analogies
(including ones across different domains) can play key role in reshaping the set of is-
sues. The Orange and Sinai disputes demonstrate how through the revealing of the ap-
propriate information and applying analogy to reshape a dispute mediation can succeed.
MediaThor demonstrated that computationally, the approach requires the speciﬁcation
of the introduced mental models of the agents and the mechanisms for aligning/agreeing
on the ontologies of the dispute that they use. MediaThor also demonstrated that case-
based reasoning offers a potential mechanism for the mediator for handling past experi-
ences, though the structure of the case is complex (in comparison to the attribute-value
structure), extending the already complex structure for representing negotiation cases
[18]. Automating mediation must take in account that mediation is (i) a knowledge in-
tensive process, where the mediators utilise their past experiences; and (ii) a process
that utilises information from negotiating parties and uses information for changing the
positions these parties have on the negotiation table.
As the mediator utilises information to decrease the uncertainty in the dispute, an au-
tomated mediation would require a measure of uncertainty H(Mt), allowing to quan-
tify and compare the uncertainty coming from the incomplete knowledge of the mental
models of the agents. In terms of the two party mediation in Figure 2, this decrease
should be observable, i.e. H(Mt
μ(α)) < H(Mt
β(α)) and H(Mt
μ(β)) < H(Mt
α(β)).
Within the framework of the information-theoretic approach, such measure should mea-
sure the “information gain”, as the mediator adds such gain. Viewing mediation as a di-
alogue system (see Figure 2) points also to the information-theoretic work in dialogue
management strategies in conversational case-based reasoning [25]. In terms of an au-
tomated mediation system, the mediator should have the mechanism to determine the
most informative question to ask at each stage of the interaction to each of the negoti-
ating agents. These issues remain beyond the scope of this paper.
Though beyond the scope of the paper, we are aware that mediation requires trust in
the mediator from the parties involved, as much of the information about their position
negotiating parties would not reveal to the other side.
In conclusion, we would like to note that nowadays mediation skills are taught to
students at various levels and schools spanning from elementary schools to university
schools, including the Harvard Law School. Hence, the development of an automated
mediation system is on the top priority of the research agendas.
Acknowledgements
This research has been supported by the Sabbatical Progrmme of the Spanish Minis-
terio de Educaci´on y Ciencia Grant SAB2006-0001, the Australian Research Council
Discovery Grant DP0557168, the Generalitat de Catalunya Grant 2005/SGR/00093 and
by the Spanish Ministerio de Educacion y Ciencia Consolider Grant CSD2007-0022.
References
1. Simoff, S.J., Debenham, J.: Curious negotiator. In: Klusch, M., Ossowski, S., Shehory, O.
(eds.) CIA 2002. LNCS (LNAI), vol. 2446, p. 104. Springer, Heidelberg (2002)
2. Sycara, K.P.: Problem restructuring in negotiation. Management Science 37(10), 1248–1268
(1991)

Mediation = Information Revelation + Analogical Reasoning
159
3. Lewicki, R.J., Saunders, D.M., Minton, J.W.: Essentials of Negotiation. McGraw-Hill, New
York (2001)
4. Franklin Dukes, E.: What we know about environmental conﬂict resolution: An analysis
based on research. Conﬂict Resolution Quarterly 22(1-2), 191–220 (2004)
5. Ramchurn, S.D., Sierra, C., Godo, L., Jennings, N.R.: Negotiating using rewards. Artiﬁcial
Intelligence 171, 805–837 (2007)
6. Sierra, C., Debenham, J.: Information-based agency. In: Proceedings of Twentieth Interna-
tional Joint Conference on Artiﬁcial Intelligence, IJCAI 2007, Hyderabad, India, pp. 1513–
1518 (2007)
7. Debenham, J.: Bargaining with information. In: Jennings, N.R., Sierra, C., Sonenberg, L.,
Tambe, M. (eds.) Proceedings Third International Conference on Autonomous Agents and
Multi Agent Systems AAMAS 2004, pp. 664–671. ACM Press, New York (2004)
8. Zhang, D., Simoff, S.: Informing the curious negotiator: Automatic news extraction from
the internet. In: Simoff, S., Williams, G. (eds.) Proceedings 3rd Australasian Data Mining
Conference, Cairns, Australia, December 6-7, pp. 55–72 (2004)
9. Zhang, D., Simoff, S., Debenham, J.: Exchange rate modelling for e-negotiators using text
mining techniques. In: E-Service Intelligence - Methodologies, Technologies and Applica-
tions, pp. 191–211. Springer, Heidelberg (2007)
10. Aciar, S., Zhang, D., Simoff, S., Debenham, J.: Informed recommender: Basing rec-
ommendations on consumer product reviews. IEEE Intelligent Systems, 39–47 (May/
June 2007)
11. Gentner, D., Stevens, A.L. (eds.): Mental Models. Erlbaum, Hillsdale (1983)
12. Van Boven, L., Thompson, L.: A look into the mind of the negotiator: Mental models in
negotiation. Group Processes & Intergroup Relations 6(4), 387–404 (2003)
13. Kolodner, J.: Case-Based Reasoning. Morgan Kaufmann Publishers, Inc., San Mateo (1993)
14. Kolodner, J.L., Simpson, R.L.: The mediator: Analysis of an early case-based problem solver.
Cognitive Science 13(4), 507–549 (1989)
15. Smith, A., Stam, A.: Mediation and peacekeeping in a random walk model of civil and inter-
state war. International Studies Review 5(4), 115–135 (2003)
16. Chalamish, M., Kraus, S.: Automed - an automated mediator for bilateral negotiations under
time constraints. In: Proceedings of the International Conference on Autonomous Agents and
Multi Agent Systems, AAMAS 2007, Hawaii, USA. IFAAMAS (2007)
17. De Mantaras, R.L., McSherry, D., Bridge, D., Leake, D., Smyth, B., Craw, S., Faltings, B.,
Maher, M.L., Cox, M.T., Forbus, K., Keane, M., Aamodt, A., Watson, I.: Retrieval, reuse,
revision and retention in case-based reasoning. The Knowledge Engineering Review 20(3),
215–240 (2005)
18. Matos, N., Sierra, C.: Evolutionary computing and negotiating agents. In: Noriega, P.,
Sierra, C. (eds.) AMEC 1998. LNCS (LNAI), vol. 1571, pp. 126–150. Springer, Heidelberg
(1999)
19. Druckman, D., Druckman, J.N., Arai, T.: e-mediation: Evaluating the impacts of an electronic
mediator on negotiating behavior. Group Decision and Negotiation 13, 481–511 (2004)
20. Zartman, I.W., Berman, M.R.: The Practical Negotiator. Yale University Press, New Haven
(1982)
21. Giunchiglia, F., Yatskevich, M., Shvaiko, P.: Semantic matching: Algorithms and implemen-
tation. In: Spaccapietra, S., Atzeni, P., Fages, F., Hacid, M.-S., Kifer, M., Mylopoulos, J., Per-
nici, B., Shvaiko, P., Trujillo, J., Zaihrayeu, I. (eds.) Journal on Data Semantics IX. LNCS,
vol. 4601, pp. 1–38. Springer, Heidelberg (2007)
22. Wilkenfeld, J., Kraus, S., Santmire, T.E., Frain, C.K.: The role of mediation in conﬂict man-
agement: Conditions for successful resolution. In: Multiple Paths to Knowledge in Interna-
tional Relations. Lexington Books (2004)

160
S. Simoff, C. Sierra, and R. L´opez de M`antaras
23. Esteva, M.: Electronic Institutions: From speciﬁcation to development. Phd thesis, Technical
University of Catalonia, Barcelona (2003)
24. Bogdanovych, A.: Virtual Institutions. Phd thesis, Faculty of Information Technology, Uni-
versity of Technology, Sydney, Sydney (2007)
25. Branting, K., Lester, J., Mott, B.: Dialogue management for conversational case-based rea-
soning. In: Funk, P., Gonz´alez Calero, P.A. (eds.) ECCBR 2004. LNCS (LNAI), vol. 3155,
pp. 77–90. Springer, Heidelberg (2004)

Author Index
Alechina, Natasha
1
Dastani, Mehdi
16
Debenham, John
130
D´egremont, C´edric
32
Demolombe, Robert
81
Ditmarsch, Hans van
51
Dowell, Andrew
99
French, Tim
51
Grossi, Davide
16
Herzig, Andreas
66
Kurzen, Lena
32
Lima, Tiago de
66
Logan, Brian
1
L´opez de M`antaras, Ramon
145
Lorini, Emiliano
66, 81
McBurney, Peter
99
Meyer, John-Jules Ch.
16
Michalak, Tomasz
99
Nguyen, Hoang Nga
1
Piunti, Michele
114
Rakib, Abdur
1
Ricci, Alessandro
114
Sierra, Carles
130, 145
Simoﬀ, Simeon
145
Tinnemeier, Nick
16
Wooldridge, Michael
99

